{"cell_type":{"0e188d6f":"code","32537fe7":"code","eafe4f8a":"code","edf2c276":"code","2f196cf6":"code","cd433fe6":"code","ab3919c3":"code","dca4a82a":"code","36729f7b":"code","363a33dd":"code","45cac4fa":"code","20f33297":"code","6accb424":"code","a19b2b71":"code","da7e279b":"code","0dff90c8":"code","b3d2b502":"code","6b45fd8d":"code","4d7bb93c":"code","ede31bf2":"code","2b47dbc8":"code","a9a2233e":"code","606d5229":"code","7748b322":"code","f02ebd2b":"code","30098b71":"code","0159a5c0":"code","8903def6":"code","f2c5e22a":"code","658991f5":"code","3415d6d3":"code","50f8be4e":"code","0241bf54":"code","3bbd95db":"code","131c6d35":"code","888134b3":"code","afe9a478":"code","510da6e2":"code","c8bcb80a":"code","f69b2647":"code","f7883db1":"code","ac5108a3":"code","8ceb95cc":"code","0754af66":"code","a38bb1d4":"code","69b28f23":"code","f41f033b":"code","b6c9d149":"code","bf5eb10c":"code","998c03b3":"code","ad42ded5":"code","ea9ca2dc":"code","b474d93e":"code","65d423bd":"code","286e2951":"code","f7f14cb8":"code","a48f7272":"code","fe496ce2":"code","6c0aa433":"code","e5745907":"code","57d1dc55":"code","5269d766":"code","d574a96d":"code","866002c2":"code","8cfd986f":"code","9d8e586a":"code","00304723":"code","98658fb2":"code","38e03231":"code","8febc71d":"code","6abe1cf7":"code","fa59c80d":"code","2f8274b4":"code","e4076378":"code","bb15cd77":"code","21af4022":"code","bc953fd9":"code","e63e5f47":"code","61efe823":"code","e9bbc776":"code","b523fbc4":"code","e3fb7186":"code","e3ea5e5b":"code","a6b21d27":"code","23867c98":"code","70247fad":"code","0e38ef6e":"code","3e239d63":"code","3531c62f":"code","f699486f":"markdown","8cfb33f1":"markdown","0f34aa3a":"markdown","06ca58d8":"markdown","0ea4ced7":"markdown","30a9d5af":"markdown","bff0dfff":"markdown","cc67831e":"markdown","003d1b93":"markdown","36650aa5":"markdown","cc43f660":"markdown","552d19c9":"markdown","3c651a63":"markdown","0dc70779":"markdown","8e352574":"markdown","8f40d4e9":"markdown","2f3a4ac5":"markdown","08a13d77":"markdown","259e34c7":"markdown","bfece5c9":"markdown","f56e6d42":"markdown","964ecd4b":"markdown","cdc5777c":"markdown","ce38083d":"markdown","9e2317b2":"markdown","00f6005e":"markdown","18d9bc4c":"markdown","b343f709":"markdown","85a6a482":"markdown","990a17e4":"markdown","6e85ed3e":"markdown","257f4d77":"markdown","708bf015":"markdown","fcd00710":"markdown","738d11df":"markdown","5f28241b":"markdown","9b4ede18":"markdown","5ec20a7a":"markdown","ba83b7f5":"markdown","c08bef6d":"markdown","cda7b68f":"markdown","d1e13a79":"markdown","90deed06":"markdown","dac67ea1":"markdown","5a5308a9":"markdown","8960c2bd":"markdown","61dd5977":"markdown","171160c6":"markdown","57371389":"markdown","e0e1857a":"markdown","bba4cfa1":"markdown","82da1112":"markdown","44b288a2":"markdown","99051bfa":"markdown","b24f338a":"markdown","046620ea":"markdown","d0fed813":"markdown"},"source":{"0e188d6f":"import os\nimport tarfile\nimport urllib","32537fe7":"# # from where we will download\n# DOWNLOAD_ROOT = \"https:\/\/raw.githubusercontent.com\/ageron\/handson-ml2\/master\/\"\n# # file location where we will download the file\n# HOUSING_PATH = os.path.join(\"\\datasets\", \"housing\")\n# # which file we will download\n# HOUSING_URL = DOWNLOAD_ROOT + \"datasets\/housing\/housing.tgz\"","eafe4f8a":"# def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n#     os.makedirs(housing_path, exist_ok=True)\n#     tgz_path = os.path.join(housing_path, \"housing.tgz\")\n#     urllib.request.urlretrieve(housing_url, tgz_path)\n#     housing_tgz = tarfile.open(tgz_path)\n#     housing_tgz.extractall(path=housing_path)\n#     housing_tgz.close()\n","edf2c276":"# fetch_housing_data()\n# # a folder named \"housing\" should be created at your file location, with housing.tgz file\n","2f196cf6":"import pandas as pd\n# def load_housing_data(housing_path=HOUSING_PATH):\n#     csv_path = os.path.join(housing_path, \"housing.csv\")\n#     return pd.read_csv(csv_path)","cd433fe6":"# housing = load_housing_data()\nhousing = pd.read_csv(\"..\/input\/housingchapter-2\/housing\/housing.csv\")\n# housing.head() # it will show some maiden data of our dataset","ab3919c3":"housing.info()","dca4a82a":"housing.describe()","36729f7b":"housing[\"ocean_proximity\"].value_counts()","363a33dd":"%matplotlib inline\nimport matplotlib.pyplot as plt\nhousing.hist(bins=50, figsize=(20, 15))\nplt.show()","45cac4fa":"#function\nimport numpy as np\ndef split_train_test(data, test_ratio):\n    shuffled_indices = np.random.permutation(len(data))\n    test_set_size = int(len(data) * test_ratio)\n    test_indices = shuffled_indices[:test_set_size]\n    train_indices = shuffled_indices[test_set_size:]\n    return data.iloc[train_indices], data.iloc[test_indices]","20f33297":"# use this to split housing data 80%-20%\ntrain_set, test_set = split_train_test(housing, 0.2)","6accb424":"len(train_set)","a19b2b71":"len(test_set)","da7e279b":"from zlib import crc32\n\ndef test_set_check(identifier, test_ratio):\n    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n\ndef split_train_test_by_id(data, test_ratio, id_column):\n    ids = data[id_column]\n    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n    return data.loc[~in_test_set], data.loc[in_test_set]","0dff90c8":"housing_with_id = housing.reset_index() # adds an `index` column\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")","b3d2b502":"housing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"id\")","6b45fd8d":"from sklearn.model_selection import train_test_split\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)","4d7bb93c":"housing['income_cat'] = pd.cut(housing['median_income'],\n                              bins = [0., 1.5, 3.0, 4.5, 6, np.inf],\n                              labels=[1, 2, 3, 4, 5])","ede31bf2":"housing['income_cat'].hist()","2b47dbc8":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]","a9a2233e":"strat_test_set[\"income_cat\"].value_counts() \/ len(strat_test_set)","606d5229":"def income_cat_proportions(data):\n    return data[\"income_cat\"].value_counts() \/ len(data)\n\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n\ncompare_props = pd.DataFrame({\n    \"Overall\": income_cat_proportions(housing),\n    \"Stratified\": income_cat_proportions(strat_test_set),\n    \"Random\": income_cat_proportions(test_set),\n}).sort_index()\ncompare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] \/ compare_props[\"Overall\"] - 100\ncompare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] \/ compare_props[\"Overall\"] - 100","7748b322":"compare_props","f02ebd2b":"#remove the income_cat attribute so the data is back as original\nfor set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"income_cat\", axis=1, inplace=True)","30098b71":"housing = strat_train_set.copy()","0159a5c0":"# scatterplot of all districts\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")","8903def6":"# make this little transparent to get the most dense place\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)","f2c5e22a":"housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\ns=housing[\"population\"]\/100, label=\"population\", figsize=(10,7),\nc=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\nsharex=False)\n\n# the district\u2019s population (option s)\n#the color represents the price (option c).\nplt.legend()","658991f5":"#Standard(Pearson's r) Correlation coefficient betn every pair of attributes\ncorr_matrix = housing.corr()","3415d6d3":"# find correlation with median_house_value.\ncorr_matrix['median_house_value'].sort_values(ascending=False)","50f8be4e":"#Another way to check for correlation between attributes\nfrom pandas.plotting import scatter_matrix\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n\"housing_median_age\"]\nscatter_matrix(housing[attributes], figsize=(12, 8))\n","0241bf54":"housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\nalpha=0.1)","3bbd95db":"housing[\"rooms_per_household\"] = housing[\"total_rooms\"]\/housing[\"households\"]\nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]\/housing[\"total_rooms\"]\nhousing[\"population_per_household\"]=housing[\"population\"]\/housing[\"households\"]","131c6d35":"corr_matrix = housing.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","888134b3":"housing.plot(kind=\"scatter\", x=\"rooms_per_household\", y=\"median_house_value\",\n             alpha=0.2)\nplt.axis([0, 5, 0, 520000])\nplt.show()","afe9a478":"housing.describe()","510da6e2":"housing = strat_train_set.drop(\"median_house_value\", axis=1)\nhousing_labels = strat_train_set[\"median_house_value\"].copy()","c8bcb80a":"# we can fill missing data using anyone of these\n# housing.dropna(subset=[\"total_bedrooms\"]) # option 1\n# housing.drop(\"total_bedrooms\", axis=1) # option 2\n# median = housing[\"total_bedrooms\"].median() # option 3\n# housing[\"total_bedrooms\"].fillna(median, inplace=True)","f69b2647":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy=\"median\")","f7883db1":"housing_num = housing.drop(\"ocean_proximity\", axis=1)","ac5108a3":"imputer.fit(housing_num)","8ceb95cc":"imputer.statistics_","0754af66":"housing_num.median().values","a38bb1d4":"X = imputer.transform(housing_num)","69b28f23":"housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n                          index=housing_num.index)","f41f033b":"housing_tr.head()","b6c9d149":"housing_cat = housing[[\"ocean_proximity\"]]","bf5eb10c":"housing_cat.head(5)","998c03b3":"from sklearn.preprocessing import OrdinalEncoder\nordinal_encoder = OrdinalEncoder()\nhousing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\nhousing_cat_encoded[:10]","ad42ded5":"# get the list of categories\nordinal_encoder.categories_","ea9ca2dc":"# new attributes are calles dummy attributes\nfrom sklearn.preprocessing import OneHotEncoder\ncat_encoder = OneHotEncoder()\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\nhousing_cat_1hot","b474d93e":"housing_cat_1hot.toarray()","65d423bd":"cat_encoder.categories_","286e2951":"from sklearn.base import BaseEstimator, TransformerMixin\nrooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room = True):\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    def fit(self, X, y = None):\n        return self #nothing to do\n    def transform(self, X):\n        rooms_per_household = X[:, rooms_ix] \/ X[:, households_ix]\n        population_per_household = X[:, population_ix] \/ X[:, households_ix]\n        \n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] \/ X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\n\nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\nhousing_extra_attribs = attr_adder.transform(housing.values)","f7f14cb8":"housing_extra_attribs = pd.DataFrame(\n    housing_extra_attribs,\n    columns=list(housing.columns)+[\"rooms_per_household\", \"population_per_household\"],\n    index=housing.index)\nhousing_extra_attribs.head()","a48f7272":"# pipeline for preprocessing the numerical attributes:\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nnum_pipeline = Pipeline([('imputer',\n                          SimpleImputer(strategy=\"median\")),\n                         ('attribs_adder', CombinedAttributesAdder()),\n                         ('std_scaler', StandardScaler()),])\nhousing_num_tr = num_pipeline.fit_transform(housing_num)","fe496ce2":"housing_num_tr","6c0aa433":"from sklearn.compose import ColumnTransformer\n\n# list of numerical column names\nnum_attribs = list(housing_num)\n# list of categorical column names\ncat_attribs = [\"ocean_proximity\"]\n\nfull_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, num_attribs),\n        (\"cat\", OneHotEncoder(), cat_attribs),\n    ])\n\nhousing_prepared = full_pipeline.fit_transform(housing)","e5745907":"housing_prepared","57d1dc55":"housing_prepared.shape","5269d766":"from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)","d574a96d":"# let's try the full preprocessing pipeline on a few training instances\nsome_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\n\nprint(\"Predictions:\", lin_reg.predict(some_data_prepared))","866002c2":"# compare with actual values\n\nprint(\"Labels:\", list(some_labels))","8cfd986f":"from sklearn.metrics import mean_squared_error\n\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","9d8e586a":"# Decision Tree Model\nfrom sklearn.tree import DecisionTreeRegressor\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(housing_prepared, housing_labels)","00304723":"housing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","98658fb2":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels,\nscoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)","38e03231":"def display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n\ndisplay_scores(tree_rmse_scores)","8febc71d":"lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n                             scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)","6abe1cf7":"from sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor()\nforest_reg.fit(housing_prepared, housing_labels)","fa59c80d":"housing_predictions = forest_reg.predict(housing_prepared)\nforest_mse = mean_squared_error(housing_labels, housing_predictions)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse","2f8274b4":"# evaluation\nfrom sklearn.model_selection import cross_val_score\n\nforest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n                                scoring=\"neg_mean_squared_error\", cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\ndisplay_scores(forest_rmse_scores)","e4076378":"from sklearn.svm import SVR\n\nsvm_reg = SVR(kernel=\"linear\")\nsvm_reg.fit(housing_prepared, housing_labels)\nhousing_predictions = svm_reg.predict(housing_prepared)\nsvm_mse = mean_squared_error(housing_labels, housing_predictions)\nsvm_rmse = np.sqrt(svm_mse)\nsvm_rmse","bb15cd77":"# to save models, use this code snippet.\n# import joblib\n# joblib.dump(my_model, \"my_model.pkl\")\n# # and later...\n# my_model_loaded = joblib.load(\"my_model.pkl\")","21af4022":"from sklearn.model_selection import GridSearchCV\nparam_grid = [\n{'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n{'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n]\nforest_reg = RandomForestRegressor()\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\nscoring='neg_mean_squared_error',\nreturn_train_score=True)\ngrid_search.fit(housing_prepared, housing_labels)","bc953fd9":"#The best hyperparameter combination found:\n\ngrid_search.best_params_","e63e5f47":"grid_search.best_estimator_","61efe823":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","e9bbc776":"pd.DataFrame(grid_search.cv_results_)","b523fbc4":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_distribs = {\n        'n_estimators': randint(low=1, high=200),\n        'max_features': randint(low=1, high=8),\n    }\n\nforest_reg = RandomForestRegressor(random_state=42)\nrnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\nrnd_search.fit(housing_prepared, housing_labels)","e3fb7186":"cvres = rnd_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","e3ea5e5b":"feature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances","a6b21d27":"# display these importance scores next to their corresponding attribute names\nextra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n#cat_encoder = cat_pipeline.named_steps[\"cat_encoder\"] # old solution\ncat_encoder = full_pipeline.named_transformers_[\"cat\"]\ncat_one_hot_attribs = list(cat_encoder.categories_[0])\nattributes = num_attribs + extra_attribs + cat_one_hot_attribs\nsorted(zip(feature_importances, attributes), reverse=True)","23867c98":"final_model = grid_search.best_estimator_\n\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)","70247fad":"final_rmse","0e38ef6e":"from scipy import stats\n\nconfidence = 0.95\nsquared_errors = (final_predictions - y_test) ** 2\nnp.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n                         loc=squared_errors.mean(),\n                         scale=stats.sem(squared_errors)))","3e239d63":"# Manual method\n\nm = len(squared_errors)\nmean = squared_errors.mean()\ntscore = stats.t.ppf((1 + confidence) \/ 2, df=m - 1)\ntmargin = tscore * squared_errors.std(ddof=1) \/ np.sqrt(m)\nnp.sqrt(mean - tmargin), np.sqrt(mean + tmargin)","3531c62f":"zscore = stats.norm.ppf((1 + confidence) \/ 2)\nzmargin = zscore * squared_errors.std(ddof=1) \/ np.sqrt(m)\nnp.sqrt(mean - zmargin), np.sqrt(mean + zmargin)","f699486f":"Random Forests look very promising. However, note that\nthe score on the training set is still much lower than on the validation sets, meaning\nthat the model is still overfitting the training set. **SO we can...**\n<ul>\n    <li>simplify the model<\/li>\n    <li>constrain it (i.e., regularize it)<\/li>\n    <li>get a lot more training data<\/li>","8cfb33f1":"let\u2019s convert these categories from\ntext to numbers. For this, we can use Scikit-Learn\u2019s **OrdinalEncoder class**","0f34aa3a":"#### Let's look at the score of each hyperparameter combination tested during the grid search:","06ca58d8":"###### Deal With Missing Values","0ea4ced7":"We call that function to load data","30a9d5af":"The most promising attribute to predict the median house value is the median\nincome, so let\u2019s zoom in on their correlation scatterplot","bff0dfff":"OneHotEncoder returns a sparse matrix, while the num_pipeline returns\na dense matrix.<br>\nThey returns a dense matrix","cc67831e":"### Discover and Visualize the Data","003d1b93":"### Fetch and Load Data","36650aa5":"But Scikit-Learn provides a handy class to take care of missing values: SimpleImputer.","cc43f660":"### Grid Search","552d19c9":"Since the median can only be computed on numerical attributes, you need to create a\ncopy of the data without the text attribute ocean_proximity:\n","3c651a63":"It works, although the predictions are not exactly accurate. Let\u2019s measure this regression model\u2019s RMSE on the whole training\nset using Scikit-Learn\u2019s mean_squared_error() function:","0dc70779":"### Create Test Set","8e352574":"housing dataset does not have an identifier column. The simplest\nsolution is to use the row index as the ID:","8f40d4e9":"Now we call that function to download the data to our file location","2f3a4ac5":"[![Author](https:\/\/img.shields.io\/badge\/author-utshabkg-red)](https:\/\/github.com\/utshabkg\/)\n\n**Please upvote if you like it.**\n\nReference:\nhttps:\/\/github.com\/ageron\/handson-ml2","08a13d77":"## Prepare Data","259e34c7":"red is expensive, blue is cheap, larger circles indicate\nareas with a larger population","bfece5c9":"##### Visualizing Geographical Data","f56e6d42":"use this \u201ctrained\u201d imputer to transform the training set by replacing\nmissing values with the learned medians:\n","964ecd4b":"most districts\u2019 median_hous\ning_values range between 120,000 and 265,000, so a typical prediction error of\n$68,628 is not very satisfying. This is an example of a model underfitting the training\ndata.<br>\n**So we can...**\n<ul>\n<li>select a more powerful model. <\/li>\n<li>feed the training algorithm with better features<\/li> \n<li>reduce the constraints on the model. But This model is not regularized, so it is out.<\/li>","cdc5777c":"##### One Hot Encoding\ncreate one binary attribute per category: one attribute equal to 1 when the category\nis \u201c<1H OCEAN\u201d (and 0 otherwise), another attribute equal to 1 when the category\nis \u201cINLAND\u201d (and 0 otherwise), and so on.","ce38083d":"### Randomized Search\nThe grid search approach is fine when you are exploring relatively few combinations,\nlike in the previous example, but when the hyperparameter search space is large, it is\noften preferable to use RandomizedSearchCV instead as it has benefits:\n<ul>\n    <li>If you let the randomized search run for, say, 1,000 iterations, this approach will\nexplore 1,000 different values for each hyperparameter (instead of just a few values per hyperparameter with the grid search approach).<\/li>\n    <li>Simply by setting the number of iterations, you have more control over the computing\nbudget you want to allocate to hyperparameter search.\n    <\/li><\/ul>","9e2317b2":"## Fine-Tune Model","00f6005e":"The argument sharex=False fixes a display bug (the x-axis values and legend were not displayed). This is a temporary fix (see: https:\/\/github.com\/pandas-dev\/pandas\/issues\/10611 ). Thanks to Wilmer Arellano for pointing it out.","18d9bc4c":"<h3>Some Visualization<\/h3>","b343f709":"### Transformation Pipelines\nthere are many data transformation steps that need to be executed in\nthe right order.","85a6a482":"So much accuracy!? Is is overfitted? So we need to use part of the training set for training and part of it for model validation.","990a17e4":"We take 8 and 30 as the best result.","6e85ed3e":"### Z Score","257f4d77":"## Select and Train a Model","708bf015":"### RandomForestRegressor\n##### Let\u2019s try one last model now","fcd00710":"a district\u2019s latitude and longitude are guaranteed to be stable for a few\nmillion years, so you could combine them into an ID like so","738d11df":"houses with a lower bedroom\/room ratio tend to be more expensive. The number of\nrooms per household is also more informative than the total number of rooms in a\ndistrict\u2014obviously the larger the houses, the more expensive they are.","5f28241b":"#### Better Evaluation Using Cross-Validation","9b4ede18":"but it is not perfect: if you run the program again, it will generate a different test set! Over time, you (or your Machine Learning algorithms) will get to see the whole dataset, which is what you want to avoid.\nwe can compute a hash of each instance's identifier.","5ec20a7a":"### Decision Tree\n##### let\u2019s try a more complex model to see how it does.","ba83b7f5":"We first define a function to fetch the data","c08bef6d":"we construct a Colum\nnTransformer. The constructor requires a list of tuples, where each tuple contains a\nname,22 a transformer, and a list of names (or indices) of columns that the transformer\nshould be applied to. In this example, we specify that the numerical columns\nshould be transformed using the num_pipeline that we defined earlier, and the categorical\ncolumns should be transformed using a OneHotEncoder. Finally, we apply this\nColumnTransformer to the housing data:","cda7b68f":"#### with scikit-learn, we can do all these so easily!","d1e13a79":"### Support Vector Machine\n##### Let's have a try with this too.","90deed06":"### Data Cleaning","dac67ea1":"## Feature Scaling\nThere are two common ways to get all attributes to have the same scale: **min-max scaling** and **standardization**.","5a5308a9":"## Custom Transformers\ncreate a class and implement three methods: fit()\n(returning self), transform(), and fit_transform().<br>\nYou can get the last one for free by simply adding TransformerMixin as a base class.\nIf you add BaseEstimator as a base class (and avoid *args and **kargs in your constructor),\nyou will also get two extra methods (get_params() and set_params()) that\nwill be useful for automatic hyperparameter tuning.","8960c2bd":"You might want to have an idea of how precise this estimate is. So we can compute a 95% confidence interval for the test RMSE:","61dd5977":"## Evaluate Your System on the Test Set","171160c6":"When you looked at the top five rows, you\nprobably noticed that the values in the ocean_proximity column were repetitive,\nwhich means that it is probably a categorical attribute. You can find out what categories\nexist and how many districts belong to each category by using the\nvalue_counts() method:","57371389":"##### Handling Text and Categorical Attributes","e0e1857a":"##### Experimenting with Attribute Combinations","bba4cfa1":"#### Training and Evaluating on the Training Set","82da1112":"Now we make a function lo load that data.","44b288a2":"It seems to perform\nworse than the Linear Regression model! Let\u2019s compute the same scores for the Linear Regression model just to be sure:","99051bfa":"##### Looking for Correlations","b24f338a":"let\u2019s revert to a clean training set (by copying strat_train_set once again).\nLet\u2019s also separate the predictors and the labels, since we don\u2019t necessarily want to\napply the same transformations to the predictors and the target values (note that\ndrop() creates a copy of the data and does not affect strat_train_set):","046620ea":"## Analyze the Best Models and Their Errors","d0fed813":"fit the imputer instance to the training data using the fit() method:"}}