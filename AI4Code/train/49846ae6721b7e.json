{"cell_type":{"8be23cb5":"code","6909583f":"code","1aa81213":"code","26691c1c":"code","e324e62d":"code","eba46a74":"code","49ca379c":"code","9717d77c":"code","a9e64c13":"code","91e5e005":"code","db516fb1":"code","1c96995c":"code","965731b6":"code","aa866230":"code","694dbb72":"code","5363605e":"code","70270a1c":"code","7b2c8dd5":"code","05aabff6":"code","3715a630":"code","5c603aa5":"code","bb6bb5df":"code","1b301fdb":"code","8a18e30b":"code","7168ef09":"code","dde908ee":"code","c1c980e2":"code","96f18e27":"code","53166857":"code","3787f943":"code","f6ba7d8d":"code","e4a591ef":"code","047c1aa7":"code","b7e231ed":"code","3b3e23a2":"code","b4366f96":"code","6b01ee5c":"code","dde197cf":"code","e23b7e12":"code","96047766":"code","58e32994":"code","d966140a":"code","24f8adbe":"code","32581d73":"code","f6c0fd79":"code","6344d8d6":"code","b941ef89":"code","dd9fe092":"code","18b66844":"code","44e8cd20":"markdown","81af45d0":"markdown"},"source":{"8be23cb5":"## This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6909583f":"!wget --help","1aa81213":"!wget -P mgsr\/pretrained\/ https:\/\/github.com\/libai3\/masr\/releases\/download\/m1\/gated-conv.pth","26691c1c":"!wget -P mgsr\/lm\/ https:\/\/deepspeech.bj.bcebos.com\/zh_lm\/zh_giga.no_cna_cmn.prune01244.klm","e324e62d":"!git clone https:\/\/github.com\/SeanNaren\/warp-ctc.git\nos.makedirs(\".\/warp-ctc\/build\")\nos.chdir(\".\/warp-ctc\/build\")\n! cmake ..\n! make\nprint(os.listdir(\".\/\"))\nos.chdir(\"..\/pytorch_binding\/\")\n! python setup.py install","eba46a74":"os.chdir(\"\/kaggle\/working\")\nprint(os.listdir(os.getcwd()))","49ca379c":"!git clone --recursive https:\/\/github.com\/parlance\/ctcdecode.git\nos.chdir(\"\/kaggle\/working\/ctcdecode\/\")\n!pip install wget\n!pip install .\nos.chdir(\"\/kaggle\/working\")","9717d77c":"os.system(\"pip install .\")","a9e64c13":"os.chdir(\"\/kaggle\/working\")\nprint(os.listdir(os.getcwd()))","91e5e005":"with open(\".\/requirement.txt\", \"w\") as fw:\n    fw.write(\"\"\"python-levenshtein\nlibrosa\ntensorboard\ntensorboardX\"\"\")\n! pip install -r requirement.txt","db516fb1":"import shutil\nshutil.copytree(\"..\/input\/mgcode1\/\", \".\/mgcode1\/\")","1c96995c":"!git clone https:\/\/github.com\/Ulysses0817\/MGSR-Pytorch.git .\/mgsr\/\nos.listdir(\".\/mgsr\/\")","965731b6":"!cd mgsr & ls & python train.py","aa866230":"os.getcwd()","694dbb72":"dev = pd.read_csv(\"..\/input\/speechrecon\/dev.csv\")\ntest = pd.read_csv(\"..\/input\/speechrecon\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/speechrecon\/train.csv\")","5363605e":"import jieba\n# jieba.load_userdict(\"user_dict.txt\")\nimport json\n!pip install swifter\nimport swifter\nfrom tqdm import tqdm","70270a1c":"test.head()","7b2c8dd5":"test.words.unique()","05aabff6":"test = test.loc[test.words==\" \"]","3715a630":"test[\"wav_path\"] = test[\"Unnamed: 0\"].swifter.apply(lambda x: \"\/kaggle\/input\/magicdata\/test\/test_byte(Recorder)\/\"+\"wave_%s.bin\"%x)","5c603aa5":"os.chdir(\"\/kaggle\/working\/mgsr\/\")\nprint(os.listdir(os.getcwd()))\nimport torch\nfrom models.conv import GatedConv\nimport torch.nn.functional as F\nfrom ctcdecode import CTCBeamDecoder\n\ndef load_audio(wav_path, normalize=True):  # -> numpy array\n\tif \".wav\" in wav_path:\n\t\twith wave.open(wav_path) as wav:\n\t\t\twav = np.frombuffer(wav.readframes(wav.getnframes()), dtype=\"int16\")\n\t\t\twav = wav.astype(\"float\")\n\telif \".bin\" in wav_path:\n\t\twav = np.fromfile(wav_path, dtype=\"int16\").astype(\"float\")\n\telse:\n\t\tprint(\"Error: \", wav_path)\n\t\t\n\tif normalize:\n\t\treturn (wav - wav.mean()) \/ wav.std()\n\telse:\n\t\treturn wav\n\ndef spectrogram(wav, normalize=True):\n# librosa\n\tD = librosa.stft(\n\t\twav, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=window\n\t)\n\n\tspec, phase = librosa.magphase(D)\n\tspec = np.log1p(spec)\n\t\n# handled\n\t\n\tspec = torch.FloatTensor(spec)\n\n\tif normalize:\n\t\tspec = (spec - spec.mean()) \/ spec.std()\n\n\treturn spec\n\nalpha = 0.8\nbeta = 0.3\nlm_path = \"lm\/zh_giga.no_cna_cmn.prune01244.klm\"\ncutoff_top_n = 40\ncutoff_prob = 1.0\nbeam_width = 32\nnum_processes = 4\nblank_index = 0\n\nmodel = GatedConv.load(\"pretrained\/gated-conv.pth\")\nmodel.eval()\n\ndecoder = CTCBeamDecoder(\n    model.vocabulary,\n    lm_path,\n    alpha,\n    beta,\n    cutoff_top_n,\n    cutoff_prob,\n    beam_width,\n    num_processes,\n    blank_index,\n)\n\ndef translate(vocab, out, out_len):\n    return \"\".join([vocab[x] for x in out[0:out_len]])\n\ndef predict(f):\n    wav = feature.load_audio(f)\n    spec = feature.spectrogram(wav)\n    spec.unsqueeze_(0)\n    with torch.no_grad():\n        y = model.cnn(spec)\n        y = F.softmax(y, 1)\n    y_len = torch.tensor([y.size(-1)])\n    y = y.permute(0, 2, 1)  # B * T * V\n    print(\"decoding\")\n    out, score, offset, out_len = decoder.decode(y, y_len)\n    return translate(model.vocabulary, out[0][0], out_len[0][0])","bb6bb5df":"with open(\".\/_init_path.py\", \"w+\") as f:\n    f.write(\"\")","1b301fdb":"! python beamdecode.py","8a18e30b":"with open(\".\/beamdecode.py\", \"r\") as f:\n    code = f.read()","7168ef09":"with open(\".\/beamdecode.py\", \"w\") as fr:\n    fr.write(code.replace(\"feature\", \"data\"))","dde908ee":"from beamdecode import predict","c1c980e2":"import shutil","96f18e27":"test.loc[:, [\"wav_path\", \"words\"]]","53166857":"with open(\".\/test.json\", \"w+\") as f:\n    f.write(json.dumps())","3787f943":"def lcut(x):\n    if x.startswith(\"[\"):\n        return [x]\n    return jieba.lcut(x)","f6ba7d8d":"!pip install pypinyin","e4a591ef":"from pypinyin import lazy_pinyin, Style\nstyle = Style.TONE3\nprint(lazy_pinyin('12\u4e2a[\u806a\u660e]\u7684\u5c0f\u5154\u5b50', style=style, errors=))","047c1aa7":"read_files = [\"train.csv\"]#[\"prime.txt\"]#\nwav_lst = []\npny_lst = []\nhan_lst = []\nfor file in read_files:\n    if \"csv\" in file:\n        data = pd.read_csv(\"..\/input\/speechrecon\/\"+file)\n        data[\"wav_file\"] = data[\"Unnamed: 0\"].swifter.apply(lambda x: \"..\/input\/wave_\"+str(x)+\".csv\")\n        data[\"pny\"] = data[\"words\"].swifter.apply(lambda x: lazy_pinyin(x, style=style, ))\n    else:\n        print('load ', file, ' data...')\n        sub_file = '.\/DeepSpeechRecognition-master\/data\/' + file\n        with open(sub_file, 'r', encoding='utf8') as f:\n            data = f.readlines()\n        for line in tqdm(data):\n            wav_file, pny, han = line.split('\\t')\n            wav_lst.append(wav_file)\n            pny_lst.append(pny.split(' '))\n            han_lst.append(han.strip('\\n'))","b7e231ed":"data[\"pny\"]#.value_counts()","3b3e23a2":"def mk_am_vocab(data):\n    vocab = []\n    for line in tqdm(data):\n        line = line\n        for pny in line:\n            if pny not in vocab:\n                vocab.append(pny)\n    vocab.append('_')\n    return vocab\n\ndef mk_lm_pny_vocab(data):\n    vocab = ['<PAD>']\n    for line in tqdm(data):\n        for pny in line:\n            if pny not in vocab:\n                vocab.append(pny)\n    return vocab\n\ndef mk_lm_han_vocab(data):\n    vocab = ['<PAD>', \"[*]\", \"[LAUGH]\", \"[SONANT]\", \"[ENs]\", \"[MUSIC]\", \"+\"]\n    for line in tqdm(data):\n        if line in vocab:\n            continue\n        line = ''.join(line.split(' '))\n        for han in line:\n            if han not in vocab:\n                vocab.append(han)\n    return vocab","b4366f96":"!wget -O 12.rar https:\/\/sh-ctfs.ftn.qq.com\/ftn_handler\/a3f3cb65dbba4be16f9756e5d25484360f05a6cfb41720feddbfd9091e2036eb403822d13d67402b46ed73d627dab6931b30823bdc85c12719afe07f0b39341b\/?fname=dsa%40fall2019-10up.pdf&k=28353631cc740799f8e361124266534e520452555c5f00574f025706064b555750541b500202054c5305075356510004530707556473610511547657050a0d5352040f1c555614114c455257645b&code=b561dfaa&fr=00&&txf_fid=2a6b047eb6a5481a69f08a78680812501cdb5092&xffz=127528280","6b01ee5c":"print('make am vocab...')\nam_vocab = mk_am_vocab(data.pny)\nprint('make lm pinyin vocab...')\npny_vocab = mk_lm_pny_vocab(data.pny)\nprint('make lm hanzi vocab...')\nhan_vocab = mk_lm_han_vocab(data.words)","dde197cf":"dev_han_vocab = mk_lm_han_vocab(data.words)","e23b7e12":"dev_han = set(dev_han_vocab)","96047766":"dev_han","58e32994":"dev_han.difference(set(han_vocab))","d966140a":"len(set(pny_vocab)),len(set(han_vocab))","24f8adbe":"alpha_list","32581d73":"alpha_list = []\nfor i in set(pny_vocab):\n    if i.isalpha():\n        alpha_list.append(i)","f6c0fd79":"b_id = data.pny.swifter.apply(lambda x:sum([1 if i in alpha_list else 0 for i in x]))","6344d8d6":"lazy_pinyin(\"\u55ef\u6211\u770b\u5230\u6700\u8fd1\u6296\u97f3\u4e0a\u9762\u6709\u5f88\u591a\u5173\u4e8e\u65c5\u6e38\u7684\uff0c\u5c31\u662f\u53d1\u5e03\u4e86\u5f88\u591a\u65c5\u6e38\u7684\u7f8e\u4e3d\u7684\u666f\u70b9\", style=style, strict=False)","b941ef89":"data[b_id>0]","dd9fe092":"set(han_vocab)","18b66844":"import pandas as pd\ndev = pd.read_csv(\"..\/input\/speechrecon\/dev.csv\")\ntest = pd.read_csv(\"..\/input\/speechrecon\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/speechrecon\/train.csv\")\nwave_0 = pd.read_csv(\"..\/input\/speechrecon\/wave_0.csv\")\nwave_1 = pd.read_csv(\"..\/input\/speechrecon\/wave_1.csv\")\nwave_2 = pd.read_csv(\"..\/input\/speechrecon\/wave_2.csv\")","44e8cd20":"> ### \u73af\u5883\u914d\u7f6e","81af45d0":"> ### \u51c6\u5907\u6570\u636e"}}