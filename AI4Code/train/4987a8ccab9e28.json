{"cell_type":{"9d78b6ed":"code","5fb61db1":"code","723d42d7":"code","c755ee3e":"markdown","513b0d5f":"markdown","f6b72990":"markdown","11cad3b2":"markdown","6058946d":"markdown","c4e34d0f":"markdown","3e9df884":"markdown"},"source":{"9d78b6ed":"import numpy as np\nimport pandas as pd\nimport torch\nimport math\nimport time\nimport os\nfrom tqdm import tqdm\n\nfrom torch import nn\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\nfrom transformers.modeling_utils import (\n    PreTrainedModel,\n    apply_chunking_to_forward,\n    find_pruneable_heads_and_indices,\n    prune_linear_layer,\n)\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom transformers import AlbertConfig\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold\n\nDATA_PATH = \"..\/input\/ventilator-pressure-prediction\/\"\n\nDEBUG = False\n\nif DEBUG == True:\n    train = pd.read_csv(DATA_PATH+'train.csv').head(24000)\n    test = pd.read_csv(DATA_PATH+'test.csv').head(16000)\n    submission = pd.read_csv(DATA_PATH+'sample_submission.csv').head(16000)\nelse:\n    train = pd.read_csv(DATA_PATH+'train.csv')\n    test = pd.read_csv(DATA_PATH+'test.csv')\n    submission = pd.read_csv(DATA_PATH+'sample_submission.csv')\n\n    # train = train.head(train.shape[0]\/\/10)\n    # test = test.head(test.shape[0]\/\/10)\n    # submission = submission.head(submission.shape[0]\/\/10)\n\ndef add_features(df):\n    df['area'] = df['time_step'] * df['u_in']\n    df['area'] = df.groupby('breath_id')['area'].cumsum()\n    df['cross']= df['u_in']*df['u_out']\n    df['cross2']= df['time_step']*df['u_out']\n    \n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    ###\n    df['one'] = 1\n    df['count'] = (df['one']).groupby(df['breath_id']).cumsum()\n    df['u_in_cummean'] =df['u_in_cumsum'] \/ df['count']\n    \n    ###\n    df['breath_id_lag']=df['breath_id'].shift(1).fillna(0)\n    df['breath_id_lag2']=df['breath_id'].shift(2).fillna(0)\n    df['breath_id_lag3']=df['breath_id'].shift(3).fillna(0)\n    df['breath_id_lagsame']=np.select([df['breath_id_lag']==df['breath_id']],[1],0)\n    df['breath_id_lag2same']=np.select([df['breath_id_lag2']==df['breath_id']],[1],0)\n    df['breath_id_lag3same']=np.select([df['breath_id_lag3']==df['breath_id']],[1],0)\n    df['u_in_lag'] = df['u_in'].shift(1).fillna(0)\n    df['u_in_lag'] = df['u_in_lag']*df['breath_id_lagsame']\n    df['u_in_lag2'] = df['u_in'].shift(2).fillna(0)\n    df['u_in_lag2'] = df['u_in_lag2']*df['breath_id_lag2same']\n    df['u_in_lag3'] = df['u_in'].shift(2).fillna(0)\n    df['u_in_lag3'] = df['u_in_lag3']*df['breath_id_lag3same']\n    \n    df['u_out_lag2'] = df['u_out'].shift(2).fillna(0)\n    df['u_out_lag2'] = df['u_out_lag2']*df['breath_id_lag2same']\n    df['u_out_d'] = df['u_out'].diff().fillna(0)\n    \n    df['u_sum'] = df['u_in'] + df['u_out']\n    \n    df['ts_diff'] = df['time_step'].diff().fillna(0.033)\n    df['ts_dd'] = df['ts_diff'].diff().fillna(0)\/df['ts_diff']\n    \n    df['u_in_diff'] = df['u_in'].diff().fillna(0)\n    df['u_in_d'] = df['u_in_diff']\/df['ts_diff']\n    df['u_in_d_diff'] = df['u_in_d'].diff().fillna(0)\n    df['u_in_dd'] = df['u_in_d']\/df['ts_diff']\n    df['u_in_dd_diff'] = df['u_in_dd'].diff().fillna(0)\n    \n    # gives the mean of 'u_in' with column name 'u_in_y'\n    df = df.merge(df.groupby('breath_id')['u_in'].mean(), left_on=['breath_id'], right_on=['breath_id'])\n    df['bias'] = df['u_in_x'] - df['u_in_y']\n    df['bias_diff'] = df['bias'].diff().fillna(0)\n    \n    df['bias_d'] = df['bias']\/df['ts_diff']\n    df['bias_d_diff'] = df['bias_d'].diff().fillna(0)\n    #\n    df['var'] = (df['u_in_x'] - df['u_in_y'])**2\n    df['var_diff'] = df['var'].diff().fillna(0)\n    #\n    df['var_d'] = df['var']\/df['ts_diff']\n    df['var_d_diff'] = df['var_d'].diff().fillna(0)\n    #\n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df['RC'] = df['R'] + df['C']\n    df = pd.get_dummies(df)\n    return df\n\ntrain = add_features(train)\ntest = add_features(test)\n\nmask_train = 1 - train[['u_out']].to_numpy().reshape(-1, 80)#[:,:33]\nmask_test = 1 - test[['u_out']].to_numpy().reshape(-1, 80)#[:,:33]\n\ntargets = train[['pressure']].to_numpy().reshape(-1, 80)#[:,:33]\n#targets = targets * mask_train\n\ntrain.drop(['pressure', 'id', 'breath_id', 'one','count', 'u_in_y', 'breath_id_lag','breath_id_lag2','breath_id_lagsame','breath_id_lag2same','u_out_lag2'], axis=1, inplace=True)\ntest.drop(['id', 'breath_id', 'one','count', 'u_in_y','breath_id_lag','breath_id_lag2','breath_id_lagsame','breath_id_lag2same','u_out_lag2'], axis=1, inplace=True)\n\nRS = RobustScaler()\ntrain = RS.fit_transform(train)\ntest = RS.transform(test)\n\ntrain = train.reshape(-1, 80, train.shape[-1])#[:,:33,:]\n#train = np.transpose(np.transpose(train, (2,0,1))*mask_train, (1,2,0))\n\ntest = test.reshape(-1, 80, train.shape[-1])#[:,:33,:]\n#test = np.transpose(np.transpose(test, (2,0,1))*mask_test, (1,2,0))\n","5fb61db1":"\nclass AlbertAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                f\"heads ({config.num_attention_heads}\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        self.hidden_size = config.hidden_size\n        self.attention_head_size = config.hidden_size \/\/ config.num_attention_heads\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n\n        self.attention_dropout = nn.Dropout(config.attention_probs_dropout_prob)\n        self.output_dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.pruned_heads = set()\n\n        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n            self.max_position_embeddings = config.max_position_embeddings\n            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n\n    # Copied from transformers.models.bert.modeling_bert.BertSelfAttention.transpose_for_scores\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def prune_heads(self, heads):\n        if len(heads) == 0:\n            return\n        heads, index = find_pruneable_heads_and_indices(\n            heads, self.num_attention_heads, self.attention_head_size, self.pruned_heads\n        )\n\n        # Prune linear layers\n        self.query = prune_linear_layer(self.query, index)\n        self.key = prune_linear_layer(self.key, index)\n        self.value = prune_linear_layer(self.value, index)\n        self.dense = prune_linear_layer(self.dense, index, dim=1)\n\n        # Update hyper params and store pruned heads\n        self.num_attention_heads = self.num_attention_heads - len(heads)\n        self.all_head_size = self.attention_head_size * self.num_attention_heads\n        self.pruned_heads = self.pruned_heads.union(heads)\n\n    def forward(self, hidden_states, attention_mask=None, head_mask=None, output_attentions=False):\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores = attention_scores \/ math.sqrt(self.attention_head_size)\n\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n            attention_scores = attention_scores + attention_mask\n\n        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n            seq_length = hidden_states.size()[1]\n            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n            distance = position_ids_l - position_ids_r\n            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n\n            if self.position_embedding_type == \"relative_key\":\n                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n                attention_scores = attention_scores + relative_position_scores\n            elif self.position_embedding_type == \"relative_key_query\":\n                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.attention_dropout(attention_probs)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attention_probs = attention_probs * head_mask\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n        context_layer = context_layer.transpose(2, 1).flatten(2)\n\n        projected_context_layer = self.dense(context_layer)\n        projected_context_layer_dropout = self.output_dropout(projected_context_layer)\n        layernormed_context_layer = self.LayerNorm(hidden_states + projected_context_layer_dropout)\n        return (layernormed_context_layer, attention_probs) if output_attentions else (layernormed_context_layer,)\n\nclass AlbertLayer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.config = config\n        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n        self.seq_len_dim = 1\n        self.full_layer_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.attention = AlbertAttention(config)\n        self.ffn = nn.Linear(config.hidden_size, config.intermediate_size)\n        self.ffn_output = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.activation = ACT2FN[config.hidden_act]\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(\n        self, hidden_states, attention_mask=None, head_mask=None, output_attentions=False, output_hidden_states=False\n    ):\n        attention_output = self.attention(hidden_states, attention_mask, head_mask, output_attentions)\n\n        ffn_output = apply_chunking_to_forward(\n            self.ff_chunk,\n            self.chunk_size_feed_forward,\n            self.seq_len_dim,\n            attention_output[0],\n        )\n        hidden_states = self.full_layer_layer_norm(ffn_output + attention_output[0])\n\n        return (hidden_states,) + attention_output[1:]  # add attentions if we output them\n\n    def ff_chunk(self, attention_output):\n        ffn_output = self.ffn(attention_output)\n        ffn_output = self.activation(ffn_output)\n        ffn_output = self.ffn_output(ffn_output)\n        return ffn_output\n\nclass AlbertLayerGroup(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.albert_layers = nn.ModuleList([AlbertLayer(config) for _ in range(config.inner_group_num)])\n\n    def forward(\n        self, hidden_states, attention_mask=None, head_mask=None, output_attentions=False, output_hidden_states=False\n    ):\n        layer_hidden_states = ()\n        layer_attentions = ()\n\n        for layer_index, albert_layer in enumerate(self.albert_layers):\n            layer_output = albert_layer(hidden_states, attention_mask, head_mask[layer_index], output_attentions)\n            hidden_states = layer_output[0]\n\n            if output_attentions:\n                layer_attentions = layer_attentions + (layer_output[1],)\n\n            if output_hidden_states:\n                layer_hidden_states = layer_hidden_states + (hidden_states,)\n\n        outputs = (hidden_states,)\n        if output_hidden_states:\n            outputs = outputs + (layer_hidden_states,)\n        if output_attentions:\n            outputs = outputs + (layer_attentions,)\n        return outputs  # last-layer hidden state, (layer hidden states), (layer attentions)\n\nclass AlbertTransformer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.config = config\n        self.embedding_hidden_mapping_in = nn.Linear(config.embedding_size, config.hidden_size)\n        self.albert_layer_groups = nn.ModuleList([AlbertLayerGroup(config) for _ in range(config.num_hidden_groups)])\n\n        self.logits = nn.Linear(config.hidden_size, config.hidden_size)\n        #self.logits_act = nn.Tanh()\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        output_attentions=False,\n        output_hidden_states=False,\n        return_dict=True,\n    ):\n        hidden_states = self.embedding_hidden_mapping_in(hidden_states)\n\n        all_hidden_states = (hidden_states,) if output_hidden_states else None\n        all_attentions = () if output_attentions else None\n\n        head_mask = [None] * self.config.num_hidden_layers if head_mask is None else head_mask\n\n        for i in range(self.config.num_hidden_layers):\n            # Number of layers in a hidden group\n            layers_per_group = int(self.config.num_hidden_layers \/ self.config.num_hidden_groups)\n\n            # Index of the hidden group\n            group_idx = int(i \/ (self.config.num_hidden_layers \/ self.config.num_hidden_groups))\n\n            layer_group_output = self.albert_layer_groups[group_idx](\n                hidden_states,\n                attention_mask,\n                head_mask[group_idx * layers_per_group : (group_idx + 1) * layers_per_group],\n                output_attentions,\n                output_hidden_states,\n            )\n            hidden_states = layer_group_output[0]\n\n            if output_attentions:\n                all_attentions = all_attentions + layer_group_output[-1]\n\n            if output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states,)\n\n        hidden_states = self.logits(hidden_states)\n\n        #hidden_states = self.logits_act(hidden_states)\n\n        if not return_dict:\n            return tuple(v for v in [hidden_states, all_hidden_states, all_attentions] if v is not None)\n\n        return hidden_states, all_hidden_states, all_attentions\n\nclass AlbertPreTrainedModel(PreTrainedModel):\n    config_class = AlbertConfig\n    base_model_prefix = \"albert\"\n    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n\n    def _init_weights(self, module):\n        \"\"\"Initialize the weights.\"\"\"\n        if isinstance(module, nn.Linear):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https:\/\/github.com\/pytorch\/pytorch\/pull\/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\nlogger = logging.get_logger(__name__)\ndef load_tf_weights_in_albert(model, config, tf_checkpoint_path):\n    \"\"\"Load tf checkpoints in a pytorch model.\"\"\"\n    try:\n        import re\n\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        logger.error(\n            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n            \"https:\/\/www.tensorflow.org\/install\/ for installation instructions.\"\n        )\n        raise\n    tf_path = os.path.abspath(tf_checkpoint_path)\n    logger.info(f\"Converting TensorFlow checkpoint from {tf_path}\")\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    names = []\n    arrays = []\n    for name, shape in init_vars:\n        logger.info(f\"Loading TF weight {name} with shape {shape}\")\n        array = tf.train.load_variable(tf_path, name)\n        names.append(name)\n        arrays.append(array)\n\n    for name, array in zip(names, arrays):\n        print(name)\n\n    for name, array in zip(names, arrays):\n        original_name = name\n\n        # If saved from the TF HUB module\n        name = name.replace(\"module\/\", \"\")\n\n        # Renaming and simplifying\n        name = name.replace(\"ffn_1\", \"ffn\")\n        name = name.replace(\"bert\/\", \"albert\/\")\n        name = name.replace(\"attention_1\", \"attention\")\n        name = name.replace(\"transform\/\", \"\")\n        name = name.replace(\"LayerNorm_1\", \"full_layer_layer_norm\")\n        name = name.replace(\"LayerNorm\", \"attention\/LayerNorm\")\n        name = name.replace(\"transformer\/\", \"\")\n\n        # The feed forward layer had an 'intermediate' step which has been abstracted away\n        name = name.replace(\"intermediate\/dense\/\", \"\")\n        name = name.replace(\"ffn\/intermediate\/output\/dense\/\", \"ffn_output\/\")\n\n        # ALBERT attention was split between self and output which have been abstracted away\n        name = name.replace(\"\/output\/\", \"\/\")\n        name = name.replace(\"\/self\/\", \"\/\")\n\n        # The pooler is a linear layer\n        name = name.replace(\"pooler\/dense\", \"pooler\")\n\n        # The classifier was simplified to predictions from cls\/predictions\n        name = name.replace(\"cls\/predictions\", \"predictions\")\n        name = name.replace(\"predictions\/attention\", \"predictions\")\n\n        # Naming was changed to be more explicit\n        name = name.replace(\"embeddings\/attention\", \"embeddings\")\n        name = name.replace(\"inner_group_\", \"albert_layers\/\")\n        name = name.replace(\"group_\", \"albert_layer_groups\/\")\n\n        # Classifier\n        if len(name.split(\"\/\")) == 1 and (\"output_bias\" in name or \"output_weights\" in name):\n            name = \"classifier\/\" + name\n\n        # No ALBERT model currently handles the next sentence prediction task\n        if \"seq_relationship\" in name:\n            name = name.replace(\"seq_relationship\/output_\", \"sop_classifier\/classifier\/\")\n            name = name.replace(\"weights\", \"weight\")\n\n        name = name.split(\"\/\")\n\n        # Ignore the gradients applied by the LAMB\/ADAM optimizers.\n        if (\n            \"adam_m\" in name\n            or \"adam_v\" in name\n            or \"AdamWeightDecayOptimizer\" in name\n            or \"AdamWeightDecayOptimizer_1\" in name\n            or \"global_step\" in name\n        ):\n            logger.info(f\"Skipping {'\/'.join(name)}\")\n            continue\n\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r\"[A-Za-z]+_\\d+\", m_name):\n                scope_names = re.split(r\"_(\\d+)\", m_name)\n            else:\n                scope_names = [m_name]\n\n            if scope_names[0] == \"kernel\" or scope_names[0] == \"gamma\":\n                pointer = getattr(pointer, \"weight\")\n            elif scope_names[0] == \"output_bias\" or scope_names[0] == \"beta\":\n                pointer = getattr(pointer, \"bias\")\n            elif scope_names[0] == \"output_weights\":\n                pointer = getattr(pointer, \"weight\")\n            elif scope_names[0] == \"squad\":\n                pointer = getattr(pointer, \"classifier\")\n            else:\n                try:\n                    pointer = getattr(pointer, scope_names[0])\n                except AttributeError:\n                    logger.info(f\"Skipping {'\/'.join(name)}\")\n                    continue\n            if len(scope_names) >= 2:\n                num = int(scope_names[1])\n                pointer = pointer[num]\n\n        if m_name[-11:] == \"_embeddings\":\n            pointer = getattr(pointer, \"weight\")\n        elif m_name == \"kernel\":\n            array = np.transpose(array)\n        try:\n            assert (\n                pointer.shape == array.shape\n            ), f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\"\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        print(f\"Initialize PyTorch weight {name} from {original_name}\")\n        pointer.data = torch.from_numpy(array)\n\n    return model\n\nclass AlbertModel(AlbertPreTrainedModel):\n\n    config_class = AlbertConfig\n    load_tf_weights = load_tf_weights_in_albert\n    base_model_prefix = \"albert\"\n\n    def __init__(self, config, add_pooling_layer=True):\n        super().__init__(config)\n\n        self.config = config\n        self.encoder = AlbertTransformer(config)\n        if add_pooling_layer:\n            self.pooler = nn.Linear(config.hidden_size, config.hidden_size)\n            self.pooler_activation = nn.Tanh()\n        else:\n            self.pooler = None\n            self.pooler_activation = None\n\n        self.init_weights()\n\n    def _prune_heads(self, heads_to_prune):\n        for layer, heads in heads_to_prune.items():\n            group_idx = int(layer \/ self.config.inner_group_num)\n            inner_group_idx = int(layer - group_idx * self.config.inner_group_num)\n            self.encoder.albert_layer_groups[group_idx].albert_layers[inner_group_idx].attention.prune_heads(heads)\n\n    def forward(\n        self,\n        embedding_output,\n        attention_mask=None,\n        head_mask=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        input_shape = embedding_output.shape[:-1]\n        device = embedding_output.device\n\n        if attention_mask is None:\n            attention_mask = torch.ones(input_shape, device=device)\n\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n        extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\n        encoder_outputs = self.encoder(\n            embedding_output,\n            extended_attention_mask,\n            head_mask=head_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = encoder_outputs[0]\n        hidden_states = encoder_outputs[1]\n        attentions = encoder_outputs[2]\n\n        pooled_output = self.pooler_activation(self.pooler(sequence_output[:, 0])) if self.pooler is not None else None\n\n        return sequence_output, pooled_output, hidden_states, attentions\n\nclass AlbertForPreTraining(AlbertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.albert = AlbertModel(config)\n        self.predictions = AlbertMLMHead(config)\n        self.init_weights()\n\n    def forward(\n        self,\n        embedding_output,\n        attention_mask=None,\n        head_mask=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        outputs = self.albert(\n            embedding_output, #outputs[0],# \n            attention_mask=attention_mask,\n            head_mask=head_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        prediction_scores = self.predictions(outputs[0])\n\n        return prediction_scores, #hidden_states, attentions\n\nclass AlbertMLMHead(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.conv = nn.Conv1d(config.max_position_embeddings, config.max_position_embeddings, kernel_size=3, padding='same')\n        self.LayerNorm = nn.LayerNorm(config.embedding_size)\n        self.dense = nn.Linear(config.hidden_size, config.embedding_size)\n        self.activation = ACT2FN[config.hidden_act]\n        self.decoder = nn.Linear(config.embedding_size, 1)\n        self.bias = nn.Parameter(torch.zeros(1))\n        self.decoder.bias = self.bias\n\n    def forward(self, hidden_states):\n        #hidden_states = self.conv(hidden_states)\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.activation(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states)\n        hidden_states = self.decoder(hidden_states)\n\n        prediction_scores = hidden_states\n\n        return prediction_scores\n\n    def _tie_weights(self):\n        # To tie those two weights if they get disconnected (on TPU or when the bias is resized)\n        self.bias = self.decoder.bias\n","723d42d7":"\n# optimizer\nlearning_rate = 5e-2\nloss_fct1 = nn.L1Loss()\nEPOCH = 30 if DEBUG==True else 200\nBATCH_SIZE = 20 if DEBUG == True else 200\nFOLDS = 10\nPARALLEL = False\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") if DEBUG == False else torch.device(\"cpu\")\ncfg = AlbertConfig()\ncfg.max_position_embeddings = train.shape[1]\ncfg.hidden_size = 128 #512\ncfg.intermediate_size = 256 #2048\n\ncfg.embedding_size = train.shape[2]\ncfg.vocab_size = train.shape[1]\n#cfg.num_attention_heads = 16\n#cfg.num_hidden_layers = 4\n#cfg.hidden_dropout_prob = 0.1\n\ndef data_gen(batch, num, total):\n    for i in range(total):\n        d = np.random.randint(0, num, batch)\n        yield d\n        \nMODEL_PATH = '..\/input\/bertmodel\/'\nwriter = SummaryWriter()\nkf = KFold(n_splits=FOLDS, shuffle=True, random_state=2021)\ntest_preds = []\nfor iii in range(1):\n    for fold, (train_steps, test_idx) in enumerate(kf.split(train, targets)):\n        print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n        X_train, X_valid = train[train_steps], train[test_idx]\n        y_train, y_valid = targets[train_steps], targets[test_idx]\n        best_val_loss = float('inf')\n\n        model_path = MODEL_PATH + f'Fold{fold+1}-BERTMLM-LSTM.bin'\n        model = AlbertForPreTraining(cfg).to(device=device)\n        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n        if os.path.exists(model_path):\n            checkpoint = torch.load(model_path)\n            model.load_state_dict(checkpoint['model_state'])\n            #optimizer = torch.optim.SGD(model.parameters(), lr=0.2)\n            optimizer.load_state_dict(checkpoint['optimizer_state'])\n            print(f\"=================== fold {fold+1} loaded ====================\")\n        if PARALLEL and not DEBUG:\n            model = nn.DataParallel(model)\n        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.995)\n        train_size = X_train.shape[0]\n        eval_size = X_valid.shape[0]\n        for epoch in range(EPOCH):\n            epoch_start_time = time.time()\n            model.train()\n            total_loss = 0.\n            train_steps = int(train_size * 1.2) \/\/ BATCH_SIZE\n            for batch, data in tqdm(enumerate(data_gen(BATCH_SIZE, train_size, train_steps)), total = train_steps):\n                x = torch.tensor(X_train[data], dtype=torch.float32, device=device)\n                y = torch.tensor(y_train[data], dtype=torch.float32, device=device)\n                prediction_scores = model(x)[0]\n                loss = loss_fct1(prediction_scores.view(-1), y.view(-1))\n                loss.backward()\n\n                total_loss += loss\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n                optimizer.step()\n                optimizer.zero_grad()\n\n            train_loss = total_loss\/batch\n\n            model.eval()\n            total_loss = 0.\n            with torch.no_grad():\n                eval_steps = int(eval_size * 1.2) \/\/ BATCH_SIZE\n                for batch, data in enumerate(data_gen(BATCH_SIZE, eval_size, eval_steps)):\n                    x = torch.tensor(X_valid[data], dtype=torch.float32, device=device)\n                    y = torch.tensor(y_valid[data], dtype=torch.float32, device=device)\n                    prediction_scores = model(x)[0]\n                    l1loss = loss_fct1(prediction_scores.view(-1), y.view(-1))\n                    total_loss += l1loss\n                eval_loss = total_loss\/batch\n\n            lr = scheduler.get_last_lr()[0]\n            elapsed = time.time() - epoch_start_time\n            scheduler.step()\n\n            writer.add_scalar(f'Fold{fold+1}\/train', train_loss, epoch)\n            writer.add_scalar(f'Fold{fold+1}\/eval', eval_loss, epoch)\n            print(f'| epoch {epoch:3d} | lr {lr:02.8f} | time: {elapsed:5.2f}s | loss {train_loss:3.4f} | eval {eval_loss:3.4f}')\n\n            if eval_loss < best_val_loss:\n                best_val_loss = eval_loss\n                #torch.save({'model_state': model.module.state_dict() if PARALLEL else model.state_dict() if DEBUG else model.module.state_dict(), 'optimizer_state': optimizer.state_dict(), }, model_path)\n\n        # more work needed\n        test_result = np.zeros((test.shape[0], 80, 1))\n        test_steps = test.shape[0]\/\/BATCH_SIZE + 1\n\n        if DEBUG:\n            for i in range(test_steps):\n                test_result[i*BATCH_SIZE:(i+1)*BATCH_SIZE,:80] = test_result[i*BATCH_SIZE:(i+1)*BATCH_SIZE:,:80] + \\\n                    model(torch.tensor(test[i*BATCH_SIZE:(i+1)*BATCH_SIZE], dtype=torch.float32, device=device))[0].detach().numpy()\n        else:\n            for i in range(test_steps):\n                test_result[i*BATCH_SIZE:(i+1)*BATCH_SIZE,:80] = test_result[i*BATCH_SIZE:(i+1)*BATCH_SIZE:,:80] + \\\n                    model(torch.tensor(test[i*BATCH_SIZE:(i+1)*BATCH_SIZE], dtype=torch.float32, device=device))[0].cpu().detach().numpy()\n\n        #print(test.shape, test_result.shape)\n        test_preds.append(test_result.squeeze().reshape(-1, 1).squeeze())\n        break\n\nsubmission[\"pressure\"] = sum(test_preds)\/FOLDS\nsubmission.to_csv('submission.csv', index=False)\n","c755ee3e":"![Screenshot from 2021-10-07 21-56-21.png](attachment:1f98c777-7ef5-4b52-9af6-7905664c93eb.png)","513b0d5f":"## Thanks for reading my notebook, if you enjoy it please upvote. ","f6b72990":"## Discussion\n\nNormally what I would do when training an ML model is I would train 5 folds of the model for 200 epochs each run to fit in kaggle's 9 hours restriction and save the output model; next time I will add the model as dataset and continue the training for another 9 hours, and so on and so forth till it is satisfactory.  \n\nBut for this #ALBERT model, before I have some decent TESLA A100 GPUS available, I only trained one fold of the data over and over for more than 1000 epochs. It is indeed very slow but the training curve looks very promising. \n\nI'm also working on an automated shell script to effectively use GCP GPUs, by using multiple cutting edge GPUs such as Tesla A100 you will be able train this model in just an hour or two for several thousands epochs. Check this out: https:\/\/github.com\/haroldmei\/MachineLearning\/tree\/master\/misc\n\nMachine Learning is getting more and more expensive now - it's said that they spent USD 12 million for a GPT-3 model, it really makes the democritization of ML\/AI unrealistic for hobbist today. ","11cad3b2":"## Data preparation\n\nData preparation is similar to many of the open notebooks here in this competition","6058946d":"## Training\nSince transformer based models are normally huge, the training is normally very computationally intensive. Below only trains one fold of the data, for exploration purpose.  \nAlso since this is not an NLP problem, the ALBERT model is slightly different from the original - it has no embedding component in the model, the model takes feature array directly, instead of token ids.","c4e34d0f":"## Model definition\nModel definition is copied from huggingface transformers, with a bit tweak to adapt to timeseries data. Model does not include embedding layer,and default parameters have been largely reduced to be fit in two 8G nvidia gtx gpus.   \nhttps:\/\/github.com\/huggingface\/transformers\/blob\/master\/src\/transformers\/models\/albert\/modeling_albert.py","3e9df884":"## Description\nIn this notebook, I attempted the problem with huggingface's ALBERT MLM model, trying to figure out if it's possible to make use of transformer based models for time series data.  \nI managed to train a model with my two 1070ti for about 40 hrs, the final training loss is around 0.23, and eval loss is around 0.26, and the curve is still going down.  \nTraining big models is super time consuming, so I will only share the code and resulting training curve I have here for people who are interested in exploring transformers based models.  \n"}}