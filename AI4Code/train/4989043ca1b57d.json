{"cell_type":{"1c219957":"code","0494481e":"code","f17f71fa":"code","910d02b7":"code","e5c97bb4":"code","3c8020f0":"code","6b4c117e":"code","45a088d4":"code","c98a1e4e":"code","09e783e6":"code","548ac441":"markdown","7854e42c":"markdown"},"source":{"1c219957":"#All imports her\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\")","0494481e":"data_asm_byte_final = pd.read_csv(\"..\/input\/data_asm_byte_final.csv\", index_col = 0)\ndata_asm_byte_final.head()","f17f71fa":"final_y = data_asm_byte_final[\"Class\"]\ndata_asm_byte_final = data_asm_byte_final.drop(\"Class\", axis=1)","910d02b7":"#Let's normalize the data.\ndef normalize(dataframe):\n    #print(\"Here\")\n    test = dataframe.copy()\n    for col in tqdm(test.columns):\n        if(col != \"Id\" and col !=\"Class\"):\n            max_val = max(dataframe[col])\n            min_val = min(dataframe[col])\n            test[col] = (dataframe[col] - min_val) \/ (max_val-min_val)\n    return test","e5c97bb4":"data_asm_byte_final = normalize(data_asm_byte_final)","3c8020f0":"data_asm_byte_final.head()","6b4c117e":"data_y = final_y\n# split the data into test and train by maintaining same distribution of output varaible 'y_true' [stratify=y_true]\nx_train, x_test, y_train, y_test = train_test_split(data_asm_byte_final.drop(['Id'], axis=1), data_y,stratify=data_y,test_size=0.20)\n# split the train data into train and cross validation by maintaining same distribution of output varaible 'y_train' [stratify=y_train]\nx_train, x_cv, y_train, y_cv = train_test_split(x_train, y_train,stratify=y_train,test_size=0.20)","45a088d4":"def perform_hyperparam_tuning(list_of_hyperparam, model_name,  x_train, y_train, x_cv, y_cv):\n    cv_log_error_array = []\n    for i in tqdm(list_of_hyperparam):\n        if(model_name == \"rf\"):\n            model = RandomForestClassifier(n_estimators=i,random_state=42,n_jobs=-1, class_weight='balanced')\n        model.fit(x_train, y_train)\n        caliberated_model = CalibratedClassifierCV(model, method = \"sigmoid\")\n        caliberated_model.fit(x_train, y_train)\n        predict_y = caliberated_model.predict_proba(x_cv)\n        cv_log_error_array.append(log_loss(y_cv, predict_y))\n    for i in range(len(cv_log_error_array)):\n        print ('log_loss for hyper_parameter = ',list_of_hyperparam[i],'is',cv_log_error_array[i])\n    return cv_log_error_array\n       \ndef get_best_hyperparam(list_of_hyperparam, cv_log_error_array):\n    index = np.argmin(cv_log_error_array)\n    best_hyperparameter = list_of_hyperparam[index]\n    return best_hyperparameter\n\n\ndef perform_on_best_hyperparam(model_name, best_hyperparameter, cv_log_error_array,x_train,y_train,x_cv,y_cv,x_test,y_test):\n    \n    if(model_name == \"rf\"):\n            model = RandomForestClassifier(n_estimators = best_hyperparameter,random_state = 42,n_jobs = -1,  class_weight='balanced')\n    model.fit(x_train, y_train)\n    \n    caliberated_model = CalibratedClassifierCV(model, method = \"sigmoid\")\n    caliberated_model.fit(x_train, y_train)\n\n    predicted_y = caliberated_model.predict_proba(x_train)\n    print(\"The training log-loss for best hyperparameter is\", log_loss(y_train, predicted_y))\n    predicted_y = caliberated_model.predict_proba(x_cv)\n    print(\"The cv log-loss for best hyperparameter is\", log_loss(y_cv, predicted_y))\n    predicted_y = caliberated_model.predict_proba(x_test)\n    print(\"The test log-loss for best hyperparameter is\", log_loss(y_test, predicted_y))\n\n    predicted_y = caliberated_model.predict(x_test)\n    #plot_confusion_matrix(y_test, predicted_y)\n    \n\ndef plot_cv_error(list_of_hyperparam, cv_log_error_array):\n    fig, ax = plt.subplots()\n    ax.plot(list_of_hyperparam, cv_log_error_array,c='g')\n    for i, txt in enumerate(np.round(cv_log_error_array,3)):\n        ax.annotate((list_of_hyperparam[i],np.round(txt,3)), (list_of_hyperparam[i],cv_log_error_array[i]))\n    plt.grid()\n    plt.title(\"Cross Validation Error for each hyperparameter\")\n    plt.xlabel(\"Hyperparameter\")\n    plt.ylabel(\"Error measure\")\n    plt.show()\n","c98a1e4e":"list_of_hyperparam = [10,50,100,500,1000,2000,3000]\nmodel_name = \"rf\"\ncv_log_error_array = perform_hyperparam_tuning(list_of_hyperparam, model_name,  x_train, y_train, x_cv, y_cv)","09e783e6":"best_hyperparameter = get_best_hyperparam(list_of_hyperparam, cv_log_error_array)\nperform_on_best_hyperparam(model_name, best_hyperparameter, cv_log_error_array,x_train,y_train,x_cv,y_cv,x_test,y_test)","548ac441":"# Utility Functions","7854e42c":"# Random Forest Model"}}