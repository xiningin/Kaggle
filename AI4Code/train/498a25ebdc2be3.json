{"cell_type":{"69776f12":"code","96c4ebc0":"code","0e78be32":"code","d7e3d3e6":"code","452c2dd2":"code","c7351da9":"code","36bbe1c3":"code","6fb710a0":"code","fe89b1d3":"code","a9b2f43a":"code","2d7fcc02":"code","b7f477f3":"code","6e37d22a":"code","1f8a839b":"code","2c7d3fcb":"code","a4ed31d5":"code","1cca9ae2":"code","dbcdcf7e":"code","331316aa":"code","a49d78af":"code","768171bd":"code","69da2d4e":"code","2978437d":"code","6058f565":"code","c4554e5e":"code","01fbb9e9":"code","98c26bce":"code","fbbe4e3e":"code","781183c8":"code","18cd5595":"code","e6a2020f":"code","25dfe30e":"code","9ea8a9f7":"code","29453af5":"code","abdeb8e8":"code","fc891ac0":"code","f5da2a8d":"code","33842556":"code","1f8cfcab":"code","3e8bc3da":"code","0ecb4db0":"code","967a60e0":"code","c009281e":"code","9579ec1b":"code","60508fd0":"code","ad666715":"code","c50498d2":"code","d3cfa55b":"code","60238f37":"code","7e4ee176":"code","6fa1fe5d":"code","23668a4d":"code","475982fc":"code","a45c42e6":"code","51c09008":"code","cfd9118f":"code","d1baa7c9":"code","27921694":"code","1841e366":"code","8f69a028":"code","858704cf":"code","d6b6840c":"code","f82d3b35":"markdown","3efef330":"markdown","28ca347a":"markdown","cbb76a02":"markdown","b093b1a8":"markdown","812c53a0":"markdown","1b15fcae":"markdown","64c05b81":"markdown","8b980a5a":"markdown","dad85f5a":"markdown","7d61ab64":"markdown","3667f752":"markdown","de64e34c":"markdown","214479dc":"markdown","85ddc9a9":"markdown","adb638af":"markdown","82c0ec3d":"markdown","06f857ec":"markdown","419a5e32":"markdown","6278f79f":"markdown","6a430058":"markdown","6be7e2eb":"markdown","7ab0d59c":"markdown","f83ed8c5":"markdown","230096ed":"markdown","e3f954c7":"markdown","200aa574":"markdown","c8f30ab8":"markdown","f414ea5b":"markdown","6abb25be":"markdown","e2061348":"markdown","a5526579":"markdown","d1946a7f":"markdown","d21443c1":"markdown","5992d9d0":"markdown","eaa6816a":"markdown","84152e74":"markdown","7e783eab":"markdown","0c55c4f1":"markdown","8ca0e773":"markdown","f09dea15":"markdown"},"source":{"69776f12":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import _LRScheduler\nimport torch.utils.data as data\n\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\n\nfrom sklearn import decomposition\nfrom sklearn import manifold\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport copy\nimport random\nimport time","96c4ebc0":"SEED = 1234\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","0e78be32":"\nROOT = '.data'\n\ntrain_data = datasets.CIFAR10(root = ROOT, \n                              train = True, \n                              download = True)\n\nmeans = train_data.data.mean(axis = (0,1,2)) \/ 255\nstds = train_data.data.std(axis = (0,1,2)) \/ 255\n\nprint(f'Calculated means: {means}')\nprint(f'Calculated stds: {stds}')","d7e3d3e6":"train_transforms = transforms.Compose([\n                           transforms.RandomRotation(5),\n                           transforms.RandomHorizontalFlip(0.5),\n                           transforms.RandomCrop(32, padding = 2),\n                           transforms.ToTensor(),\n                           transforms.Normalize(mean = means, \n                                                std = stds)\n                       ])\n\ntest_transforms = transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize(mean = means, \n                                                std = stds)\n                       ])","452c2dd2":"train_data = datasets.CIFAR10(ROOT, \n                              train = True, \n                              download = True, \n                              transform = train_transforms)\n\ntest_data = datasets.CIFAR10(ROOT, \n                             train = False, \n                             download = True, \n                             transform = test_transforms)","c7351da9":"VALID_RATIO = 0.8\n\nn_train_examples = int(len(train_data) * VALID_RATIO)\nn_valid_examples = len(train_data) - n_train_examples\n\ntrain_data, valid_data = data.random_split(train_data, \n                                           [n_train_examples, n_valid_examples])","36bbe1c3":"valid_data = copy.deepcopy(valid_data)\nvalid_data.dataset.transform = test_transforms","6fb710a0":"print(f'Number of training examples: {len(train_data)}')\nprint(f'Number of validation examples: {len(valid_data)}')\nprint(f'Number of testing examples: {len(test_data)}')","fe89b1d3":"def plot_images(images, labels, classes, normalize = False):\n\n    n_images = len(images)\n\n    rows = int(np.sqrt(n_images))\n    cols = int(np.sqrt(n_images))\n\n    fig = plt.figure(figsize = (10, 10))\n\n    for i in range(rows*cols):\n\n        ax = fig.add_subplot(rows, cols, i+1)\n        \n        image = images[i]\n\n        if normalize:\n            image_min = image.min()\n            image_max = image.max()\n            image.clamp_(min = image_min, max = image_max)\n            image.add_(-image_min).div_(image_max - image_min + 1e-5)\n\n        ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n        ax.set_title(classes[labels[i]])\n        ax.axis('off')","a9b2f43a":"N_IMAGES = 25\n\nimages, labels = zip(*[(image, label) for image, label in \n                           [train_data[i] for i in range(N_IMAGES)]])\n\nclasses = test_data.classes\n\nplot_images(images, labels, classes)","2d7fcc02":"plot_images(images, labels, classes, normalize = True)","b7f477f3":"def normalize_image(image):\n    image_min = image.min()\n    image_max = image.max()\n    image.clamp_(min = image_min, max = image_max)\n    image.add_(-image_min).div_(image_max - image_min + 1e-5)\n    return image","6e37d22a":"#As before, we'll check what images look like with Sobel filters applied to them.\ndef plot_filter(images, filter, normalize = True):\n\n    images = torch.cat([i.unsqueeze(0) for i in images], dim = 0).cpu()\n    filter = torch.FloatTensor(filter).unsqueeze(0).unsqueeze(0).cpu()\n    filter = filter.repeat(3, 3, 1, 1)\n    \n    n_images = images.shape[0]\n\n    filtered_images = F.conv2d(images, filter)\n\n    images = images.permute(0, 2, 3, 1)\n    filtered_images = filtered_images.permute(0, 2, 3, 1)\n\n    fig = plt.figure(figsize = (25, 5))\n\n    for i in range(n_images):\n\n        image = images[i]\n\n        if normalize:\n            image = normalize_image(image)\n\n        ax = fig.add_subplot(2, n_images, i+1)\n        ax.imshow(image)\n        ax.set_title('Original')\n        ax.axis('off')\n\n        image = filtered_images[i]\n\n        if normalize:\n            image = normalize_image(image)\n        \n        ax = fig.add_subplot(2, n_images, n_images+i+1)\n        ax.imshow(image)\n        ax.set_title(f'Filtered')\n        ax.axis('off');","1f8a839b":"N_IMAGES = 10\n\nimages = [image for image, label in [train_data[i] for i in range(N_IMAGES)]]\n\nhorizontal_filter = [[-1, -2, -1],\n                     [ 0,  0,  0],\n                     [ 1,  2,  1]]\n\nplot_filter(images, horizontal_filter)","2c7d3fcb":"vertical_filter = [[-1, 0, 1],\n                   [-2, 0, 2],\n                   [-1, 0, 1]]\n\nplot_filter(images, vertical_filter)","a4ed31d5":"#We'll also do the same for subsampling\/pooling.\ndef plot_subsample(images, pool_type, pool_size, normalize = True):\n\n    images = torch.cat([i.unsqueeze(0) for i in images], dim = 0).cpu()\n    \n    if pool_type.lower() == 'max':\n        pool = F.max_pool2d\n    elif pool_type.lower() in ['mean', 'avg']:\n        pool = F.avg_pool2d\n    else:\n        raise ValueError(f'pool_type must be either max or mean, got: {pool_type}')\n    \n    n_images = images.shape[0]\n\n    pooled_images = pool(images, kernel_size = pool_size)\n\n    images = images.permute(0, 2, 3, 1)\n    pooled_images = pooled_images.permute(0, 2, 3, 1)\n\n    fig = plt.figure(figsize = (25, 5))\n    \n    for i in range(n_images):\n\n        image = images[i]\n\n        if normalize:\n            image = normalize_image(image)\n\n        ax = fig.add_subplot(2, n_images, i+1)\n        ax.imshow(image)\n        ax.set_title('Original')\n        ax.axis('off')\n\n        image = pooled_images[i]\n\n        if normalize:\n            image = normalize_image(image)\n\n        ax = fig.add_subplot(2, n_images, n_images+i+1)\n        ax.imshow(image)\n        ax.set_title(f'Subsampled')\n        ax.axis('off');","1cca9ae2":"#As before, the higher filter sizes in the pooling layers means more information is lost, i.e. the image becomes lower resolution.\nplot_subsample(images, 'max', 2)","dbcdcf7e":"plot_subsample(images, 'max', 3)","331316aa":"plot_subsample(images, 'avg', 2)","a49d78af":"plot_subsample(images, 'avg', 3)","768171bd":"BATCH_SIZE = 256\n\ntrain_iterator = data.DataLoader(train_data, \n                                 shuffle = True, \n                                 batch_size = BATCH_SIZE)\n\nvalid_iterator = data.DataLoader(valid_data, \n                                 batch_size = BATCH_SIZE)\n\ntest_iterator = data.DataLoader(test_data, \n                                batch_size = BATCH_SIZE)","69da2d4e":"class AlexNet(nn.Module):\n    def __init__(self, output_dim):\n        super().__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, 3, 2, 1), #in_channels, out_channels, kernel_size, stride, padding\n            nn.MaxPool2d(2), #kernel_size\n            nn.ReLU(inplace = True),\n            nn.Conv2d(64, 192, 3, padding = 1),\n            nn.MaxPool2d(2),\n            nn.ReLU(inplace = True),\n            nn.Conv2d(192, 384, 3, padding = 1),\n            nn.ReLU(inplace = True),\n            nn.Conv2d(384, 256, 3, padding = 1),\n            nn.ReLU(inplace = True),\n            nn.Conv2d(256, 256, 3, padding = 1),\n            nn.MaxPool2d(2),\n            nn.ReLU(inplace = True)\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(256 * 2 * 2, 4096),\n            nn.ReLU(inplace = True),\n            nn.Dropout(0.5),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace = True),\n            nn.Linear(4096, output_dim),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        h = x.view(x.shape[0], -1)\n        x = self.classifier(h)\n        return x, h","2978437d":"OUTPUT_DIM = 10\n\nmodel = AlexNet(OUTPUT_DIM)","6058f565":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","c4554e5e":"def initialize_parameters(m):\n    if isinstance(m, nn.Conv2d):\n        nn.init.kaiming_normal_(m.weight.data, nonlinearity = 'relu')\n        nn.init.constant_(m.bias.data, 0)\n    elif isinstance(m, nn.Linear):\n        nn.init.xavier_normal_(m.weight.data, gain = nn.init.calculate_gain('relu'))\n        nn.init.constant_(m.bias.data, 0)","01fbb9e9":"model.apply(initialize_parameters)","98c26bce":"class LRFinder:\n    def __init__(self, model, optimizer, criterion, device):\n        \n        self.optimizer = optimizer\n        self.model = model\n        self.criterion = criterion\n        self.device = device\n        \n        torch.save(model.state_dict(), 'init_params.pt')\n\n    def range_test(self, iterator, end_lr = 10, num_iter = 100, \n                   smooth_f = 0.05, diverge_th = 5):\n        \n        lrs = []\n        losses = []\n        best_loss = float('inf')\n\n        lr_scheduler = ExponentialLR(self.optimizer, end_lr, num_iter)\n        \n        iterator = IteratorWrapper(iterator)\n        \n        for iteration in range(num_iter):\n\n            loss = self._train_batch(iterator)\n\n            lrs.append(lr_scheduler.get_last_lr()[0])\n\n            #update lr\n            lr_scheduler.step()\n            \n            if iteration > 0:\n                loss = smooth_f * loss + (1 - smooth_f) * losses[-1]\n                \n            if loss < best_loss:\n                best_loss = loss\n\n            losses.append(loss)\n            \n            if loss > diverge_th * best_loss:\n                print(\"Stopping early, the loss has diverged\")\n                break\n                       \n        #reset model to initial parameters\n        model.load_state_dict(torch.load('init_params.pt'))\n\n                    \n        return lrs, losses\n\n    def _train_batch(self, iterator):\n        \n        self.model.train()\n        \n        self.optimizer.zero_grad()\n        \n        x, y = iterator.get_batch()\n        \n        x = x.to(self.device)\n        y = y.to(self.device)\n        \n        y_pred, _ = self.model(x)\n                \n        loss = self.criterion(y_pred, y)\n        \n        loss.backward()\n        \n        self.optimizer.step()\n        \n        return loss.item()\n\nclass ExponentialLR(_LRScheduler):\n    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n        self.end_lr = end_lr\n        self.num_iter = num_iter\n        super(ExponentialLR, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        curr_iter = self.last_epoch\n        r = curr_iter \/ self.num_iter\n        return [base_lr * (self.end_lr \/ base_lr) ** r for base_lr in self.base_lrs]\n\nclass IteratorWrapper:\n    def __init__(self, iterator):\n        self.iterator = iterator\n        self._iterator = iter(iterator)\n\n    def __next__(self):\n        try:\n            inputs, labels = next(self._iterator)\n        except StopIteration:\n            self._iterator = iter(self.iterator)\n            inputs, labels, *_ = next(self._iterator)\n\n        return inputs, labels\n\n    def get_batch(self):\n        return next(self)","fbbe4e3e":"START_LR = 1e-7\n\noptimizer = optim.Adam(model.parameters(), lr = START_LR)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ncriterion = nn.CrossEntropyLoss()\n\nmodel = model.to(device)\ncriterion = criterion.to(device)","781183c8":"END_LR = 10\nNUM_ITER = 100\n\nlr_finder = LRFinder(model, optimizer, criterion, device)\nlrs, losses = lr_finder.range_test(train_iterator, END_LR, NUM_ITER)","18cd5595":"\ndef plot_lr_finder(lrs, losses, skip_start = 5, skip_end = 5):\n    \n    if skip_end == 0:\n        lrs = lrs[skip_start:]\n        losses = losses[skip_start:]\n    else:\n        lrs = lrs[skip_start:-skip_end]\n        losses = losses[skip_start:-skip_end]\n    \n    fig = plt.figure(figsize = (16,8))\n    ax = fig.add_subplot(1,1,1)\n    ax.plot(lrs, losses)\n    ax.set_xscale('log')\n    ax.set_xlabel('Learning rate')\n    ax.set_ylabel('Loss')\n    ax.grid(True, 'both', 'x')\n    plt.show()","e6a2020f":"plot_lr_finder(lrs, losses)","25dfe30e":"FOUND_LR = 1e-3\n\noptimizer = optim.Adam(model.parameters(), lr = FOUND_LR)","9ea8a9f7":"def calculate_accuracy(y_pred, y):\n    top_pred = y_pred.argmax(1, keepdim = True)\n    correct = top_pred.eq(y.view_as(top_pred)).sum()\n    acc = correct.float() \/ y.shape[0]\n    return acc","29453af5":"def train(model, iterator, optimizer, criterion, device):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.train()\n    \n    for (x, y) in iterator:\n        \n        x = x.to(device)\n        y = y.to(device)\n        \n        optimizer.zero_grad()\n                \n        y_pred, _ = model(x)\n        \n        loss = criterion(y_pred, y)\n        \n        acc = calculate_accuracy(y_pred, y)\n        \n        loss.backward()\n        \n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        \n    return epoch_loss \/ len(iterator), epoch_acc \/ len(iterator)","abdeb8e8":"def evaluate(model, iterator, criterion, device):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.eval()\n    \n    with torch.no_grad():\n        \n        for (x, y) in iterator:\n\n            x = x.to(device)\n            y = y.to(device)\n\n            y_pred, _ = model(x)\n\n            loss = criterion(y_pred, y)\n\n            acc = calculate_accuracy(y_pred, y)\n\n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n        \n    return epoch_loss \/ len(iterator), epoch_acc \/ len(iterator)","fc891ac0":"def epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time \/ 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","f5da2a8d":"EPOCHS = 25\n\nbest_valid_loss = float('inf')\n\nfor epoch in range(EPOCHS):\n    \n    start_time = time.time()\n    \n    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, device)\n    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, device)\n        \n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'tut3-model.pt')\n\n    end_time = time.time()\n\n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')","33842556":"model.load_state_dict(torch.load('tut3-model.pt'))\n\ntest_loss, test_acc = evaluate(model, test_iterator, criterion, device)\n\nprint(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')","1f8cfcab":"def get_predictions(model, iterator, device):\n\n    model.eval()\n\n    images = []\n    labels = []\n    probs = []\n\n    with torch.no_grad():\n\n        for (x, y) in iterator:\n\n            x = x.to(device)\n\n            y_pred, _ = model(x)\n\n            y_prob = F.softmax(y_pred, dim = -1)\n            top_pred = y_prob.argmax(1, keepdim = True)\n\n            images.append(x.cpu())\n            labels.append(y.cpu())\n            probs.append(y_prob.cpu())\n\n    images = torch.cat(images, dim = 0)\n    labels = torch.cat(labels, dim = 0)\n    probs = torch.cat(probs, dim = 0)\n\n    return images, labels, probs","3e8bc3da":"images, labels, probs = get_predictions(model, test_iterator, device)","0ecb4db0":"# Then, for each prediction we get the predicted class.\npred_labels = torch.argmax(probs, 1)","967a60e0":"# We can then find which predictions were correct and then sort the incorrect predictions in descending order of their confidence.\ncorrects = torch.eq(labels, pred_labels)\n\nincorrect_examples = []\n\nfor image, label, prob, correct in zip(images, labels, probs, corrects):\n    if not correct:\n        incorrect_examples.append((image, label, prob))\n\nincorrect_examples.sort(reverse = True, key = lambda x: torch.max(x[2], dim = 0).values)","c009281e":"def plot_most_incorrect(incorrect, classes, n_images, normalize = True):\n\n    rows = int(np.sqrt(n_images))\n    cols = int(np.sqrt(n_images))\n\n    fig = plt.figure(figsize = (25, 20))\n\n    for i in range(rows*cols):\n\n        ax = fig.add_subplot(rows, cols, i+1)\n        \n        image, true_label, probs = incorrect[i]\n        image = image.permute(1, 2, 0)\n        true_prob = probs[true_label]\n        incorrect_prob, incorrect_label = torch.max(probs, dim = 0)\n        true_class = classes[true_label]\n        incorrect_class = classes[incorrect_label]\n\n        if normalize:\n            image = normalize_image(image)\n\n        ax.imshow(image.cpu().numpy())\n        ax.set_title(f'true label: {true_class} ({true_prob:.3f})\\n' \\\n                     f'pred label: {incorrect_class} ({incorrect_prob:.3f})')\n        ax.axis('off')\n        \n    fig.subplots_adjust(hspace = 0.4)","9579ec1b":"N_IMAGES = 36\n\nplot_most_incorrect(incorrect_examples, classes, N_IMAGES)","60508fd0":"def get_representations(model, iterator, device):\n\n    model.eval()\n\n    outputs = []\n    intermediates = []\n    labels = []\n\n    with torch.no_grad():\n        \n        for (x, y) in iterator:\n\n            x = x.to(device)\n\n            y_pred, h = model(x)\n\n            outputs.append(y_pred.cpu())\n            intermediates.append(h.cpu())\n            labels.append(y)\n        \n    outputs = torch.cat(outputs, dim = 0)\n    intermediates = torch.cat(intermediates, dim = 0)\n    labels = torch.cat(labels, dim = 0)\n\n    return outputs, intermediates, labels","ad666715":"outputs, intermediates, labels = get_representations(model, train_iterator, device)","c50498d2":"# We can then perform PCA on them both and plot them.\n\ndef get_pca(data, n_components = 2):\n    pca = decomposition.PCA()\n    pca.n_components = n_components\n    pca_data = pca.fit_transform(data)\n    return pca_data\n\ndef plot_representations(data, labels, classes, n_images = None):\n    \n    if n_images is not None:\n        data = data[:n_images]\n        labels = labels[:n_images]\n        \n    fig = plt.figure(figsize = (10, 10))\n    ax = fig.add_subplot(111)\n    scatter = ax.scatter(data[:, 0], data[:, 1], c = labels, cmap = 'tab10')\n    handles, labels = scatter.legend_elements()\n    legend = ax.legend(handles = handles, labels = classes)","d3cfa55b":"output_pca_data = get_pca(outputs)\nplot_representations(output_pca_data, labels, classes)","60238f37":"intermediate_pca_data = get_pca(intermediates)\nplot_representations(intermediate_pca_data, labels, classes)","7e4ee176":"def get_tsne(data, n_components = 2, n_images = None):\n    \n    if n_images is not None:\n        data = data[:n_images]\n        \n    tsne = manifold.TSNE(n_components = n_components, random_state = 0)\n    tsne_data = tsne.fit_transform(data)\n    return tsne_data","6fa1fe5d":"N_IMAGES = 5_000\n\noutput_tsne_data = get_tsne(outputs, n_images = N_IMAGES)\nplot_representations(output_tsne_data, labels, classes, n_images = N_IMAGES)","23668a4d":"intermediate_tsne_data = get_tsne(intermediates, n_images = N_IMAGES)\nplot_representations(intermediate_tsne_data, labels, classes, n_images = N_IMAGES)","475982fc":"def imagine_image(model, classes, image, device, n_iterations = 10_000):\n\n    model.eval()\n    \n    label = classes.index(image)\n\n    best_prob = 0\n    best_image = None\n\n    with torch.no_grad():\n    \n        for _ in range(n_iterations):\n\n            x = torch.randn(256, 3, 32, 32).to(device)\n\n            y_pred, _ = model(x)\n\n            preds = F.softmax(y_pred, dim = -1)\n\n            _best_prob, index = torch.max(preds[:,label], dim = 0)\n\n            if _best_prob > best_prob:\n                best_prob = _best_prob\n                best_image = x[index]\n\n    return best_image, best_prob","a45c42e6":"IMAGE = 'frog'\n\nbest_image, best_prob = imagine_image(model, classes, IMAGE, device)","51c09008":"# We get an image that our model is ~100% confident is a frog, but just looks like random noise.\nprint(f'Best image probability: {best_prob.item()*100:.2f}%')","cfd9118f":"best_image = normalize_image(best_image)\n\nplt.imshow(best_image.permute(1,2,0).cpu().numpy())\nplt.axis('off');","d1baa7c9":"#  Next, we'll plot some images after they have been convolved with the first convolutional layer.\ndef plot_filtered_images(images, filters, n_filters = None, normalize = True):\n\n    images = torch.cat([i.unsqueeze(0) for i in images], dim = 0).cpu()\n    filters = filters.cpu()\n\n    if n_filters is not None:\n        filters = filters[:n_filters]\n\n    n_images = images.shape[0]\n    n_filters = filters.shape[0]\n\n    filtered_images = F.conv2d(images, filters)\n\n    fig = plt.figure(figsize = (30, 30))\n\n    for i in range(n_images):\n\n        image = images[i]\n\n        if normalize:\n            image = normalize_image(image)\n\n        ax = fig.add_subplot(n_images, n_filters+1, i+1+(i*n_filters))\n        ax.imshow(image.permute(1,2,0).numpy())\n        ax.set_title('Original')\n        ax.axis('off')\n\n        for j in range(n_filters):\n            image = filtered_images[i][j]\n\n            if normalize:\n                image = normalize_image(image)\n\n            ax = fig.add_subplot(n_images, n_filters+1, i+1+(i*n_filters)+j+1)\n            ax.imshow(image.numpy(), cmap = 'bone')\n            ax.set_title(f'Filter {j+1}')\n            ax.axis('off');\n\n    fig.subplots_adjust(hspace = -0.7)","27921694":"N_IMAGES = 5\nN_FILTERS = 7\n\nimages = [image for image, label in [test_data[i] for i in range(N_IMAGES)]]\nfilters = model.features[0].weight.data\n\nplot_filtered_images(images, filters, N_FILTERS)","1841e366":"def plot_filters(filters, normalize = True):\n\n    filters = filters.cpu()\n\n    n_filters = filters.shape[0]\n\n    rows = int(np.sqrt(n_filters))\n    cols = int(np.sqrt(n_filters))\n\n    fig = plt.figure(figsize = (20, 10))\n\n    for i in range(rows*cols):\n\n        image = filters[i]\n\n        if normalize:\n            image = normalize_image(image)\n\n        ax = fig.add_subplot(rows, cols, i+1)\n        ax.imshow(image.permute(1, 2, 0))\n        ax.axis('off')\n\n    fig.subplots_adjust(wspace = -0.9)","8f69a028":"# Again, nothing really interpretable here, sadly.\nplot_filters(filters)","858704cf":"import torchvision.models as models\n\nmodel = models.alexnet(pretrained = True)\n\nprint(model)","d6b6840c":"filters = model.features[0].weight.data\n\nplot_filters(filters)","f82d3b35":"# Defining Necessary functions to Train the model.","3efef330":"The filters are still 2-dimensional but they are expanded to a depth of three dimensions inside the plot_filter function.\n\nBelow is a filter which detects horizontal lines.","28ca347a":"Then, we'll actually plot the images.\n\nWe get both the images and the labels from the training set and convert the labels, which are originally stored as integers, into their human readable class by using the data's classes dictionary.\n\nWhen we plot them we see lots of warnings. This is because matplotlib is expecting the values of every pixel to be between $[0, 1]$, however our normalization will cause them to be outside this range. By default matplotlib will then clip these values into the $[0,1]$ range. This clipping causes all of the images to look a bit weird - all of the colors are oversaturated.","cbb76a02":"We can see different types of edge detection and blurring that the filters have learned that are apparently decent feature extractors for this model and task.","b093b1a8":"Next up is defining the transforms for data augmentation.\n\nThe images in the CIFAR10 dataset are significantly more complex than the MNIST dataset. They are larger, have three times the amount of pixels and are more cluttered. This makes them harder to learn and consequently means we should use less augmentation.\n\nA new transform we use is RandomHorizontalFlip. This, with a probability of 0.5 as specified, flips the image horizontally. So an image of a horse facing to the right will be flipped so it will face to the left. We couldn't do this in the MNIST dataset as we are not expecting our test set to contain any flipped digits, however natural images, such as those in the CIFAR10 dataset, can potentially be flipped as they still make visual sense.\n\nAs our means and stds are now already in lists we do not need to enclose them in lists as we did for the single channel images in the MNIST dataset.","812c53a0":"# Training the Model\nNext up, we'll initialize the parameters of our model.\n\nPyTorch's default initialization is usually fine however by manually trying different initialization schemes we can usually squeeze out a slight performance improvement.\n\nWe initialize parameters in PyTorch by creating a function that takes in a PyTorch module, checking what type of module it is, and then using the nn.init methods to actually initialize the parameters.\n\nFor our convolutional layers, we'll initialize the weights from a Normal distribution with a standard deviation given by:\n![image.png](attachment:image.png)\n","1b15fcae":"# Examining the Model\nWe will do the exact same probing into our model as we did in the previous notebooks: plotting a confusion matrix, plotting the most confident incorrect predictions, using PCA and t-SNE, and viewing the learned weights of our model.\n\nFirst, we'll collect all of the predictions.","64c05b81":"# https:\/\/www.kaggle.com\/darthmanav\/explaining-resnet-model-fine-tuning-pca-t-sne <br> <br>\n# https:\/\/www.kaggle.com\/darthmanav\/explaining-vgg-model-fine-tuning-pca-t-sne","8b980a5a":"...create a validation set from our training set...","dad85f5a":"We calculate the mean and standard deviation of our data so we can normalize it.\n\nOur dataset is made up of color images but three color channels (red, green and blue), compared to MNIST's black and white images with a single color channel. To normalize our data we need to calculate the means and standard deviations for each of the color channels independently.\n\nTo do this we pass a tuple containing the axes we want to take the means and standard deviations over to the mean and std functions and we receive a list of means and standard deviations for each of the three color channels.","7d61ab64":"The value of $\\text{gain}$ depends on the non-linearity we will be using after the convolutional layer and we simply tell the initialization function that we are using ReLU which sets the gain to $\\sqrt{2}$. The fan mode can be either fan_in or fan_out. fan_in is the number of connections coming into the layer and fan_out is the number of connections going out of the layer. For the first convolutional layer the input is from 3x3x3 filter, so the fan_in is 27 and the output is 64x3x3, so the fan_out is 576. We leave it to the default fan_in mode. This initialization scheme is called Kaiming Normal, also known as He Normal. See the paper to learn more about how it was devised.\n\nFor the linear layers we initialize with a Normal distribution but this time the standard deviation is given by:\n![image.png](attachment:image.png)","3667f752":"Now, we'll create a function to plot some of the images in our dataset to see what they actually look like.\n\nNote that by default PyTorch handles images that are arranged [channel, height, width], but matplotlib expects images to be [height, width, channel], hence we need to permute our images before plotting them.\n\nIgnore the normalize argument for now, we'll explain it shortly.","de64e34c":"Next, as standard, we'll load the dataset with our transforms...","214479dc":"Confusingly, there are two \"paths\" of processing through the network. This is due to the original AlexNet model being implemented on two GPUs in parallel. Almost all implementations of AlexNet are now on a single GPU and our implementation is too.\n\nWe are now moving on from the MNIST dataset and from now on we will be using the CIFAR10 dataset. CIFAR10 consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. The classes are: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck.\n![image.png](attachment:image.png)","85ddc9a9":"We apply the initialization by using the model's apply method. This will call the given function on every module and sub-module within the mode","adb638af":"We can now create a new optimizer with our found learning rate.\n\nIronically, the learning rate value we found, $0.001$ is actually Adam's default learning rate!","82c0ec3d":"Next, we'll get the output and intermediate (after the flatten) representations.","06f857ec":"![image.png](attachment:image.png)","419a5e32":"\n...and ensure our validation set uses the test transforms.","6278f79f":"The final bit of the data processing is creating the iterators.\n\nWe use a much larger batch size here than in previous models. Generally, when using a GPU, a larger batch size means our model trains faster. Our model has significantly more parameters and the images it is training on are much larger, than the previous notebook, so will generally take longer. We offset this as much as we can by using a batch size of 256 instead of 64.","6a430058":"# AlexNet\nIn this notebook we will be implementing a modified version of AlexNet, a neural network model that uses convolutional neural network (CNN) layers and was designed for the ImageNet challenge. AlexNet is famous for winning the ImageNet challenge in 2012 by beating the second place competitor by over 10% accuracy and kickstarting the interest in deep learning for computer vision.\n\nThe image below shows the architecture of AlexNet (yes, the official image has the top of the architecture cropped off).","6be7e2eb":"We print out the number of examples in each set of data to ensure everything has gone OK so far.","7ab0d59c":"We can do the same with the t-SNE algorithm.\n\nAgain, we only use a subset of the data as t-SNE takes a considerable amount of time to compute.\n\nWe also see that the classes are more well separated in the output representations compared to the intermediate representations","f83ed8c5":"As we can see, the loss begins flat and then begins to decrease rapidly before reaching a minimum and starting to increase.\n\nHow can we read this plot and get the optimal learning rate? According to this article, we should look for the loss begins to flatten, this is around $10^{-2}$ below, and then reduce that by a factor of 10, which gives us a found learning rate of $10^{-3}$ or $0.001$.","230096ed":"\nCan we ever learn interesting looking filters? Or are we doomed to just look at colorful blocks forever?\n\nWell, we can take a pre-trained AlexNet model and view the filters of that. This pre-trained version of AlexNet was trained by people at PyTorch and was not trained on the CIFAR10 dataset, but on the ILSVRC dataset, usually just called the ImageNet dataset. ImageNet is a dataset with over 1 million images in 1,000 classes. Torchvision provides ways of downloading different models pre-trained on ImageNet, such as AlexNet and many others.\n\nFirst, we can import the model making sure to pass pretrained = True to get a pre-trained version. Torchvision will then import the model, download the weights for it and load them into the new model.\n\nWe can see that this is similar to our AlexNet model but has considerably more parameters.\n\nOne interesting thing is that they use much larger filters in the first convolutional layer - 11x11 instead of 3x3.","e3f954c7":"Next up is the learning rate finder. The code here is taken from a stripped down and slightly modified version of the excellent pytorch-lr-finder.\n\nNote: the learning rate finder is more of an art than a science. It is not going to find an exact learning rate to 10 decimal places which will always give us 100% accuracy - but it is usually going to be better than just picking a learning rate out of thin air.\n\nThe most commonly used optimizer used is Adam. Adam's default learning rate is usually a fine choice but, much like how we manually initialized our parameters to potentially get some performance improvement, we can try and calculate an optimal learning rate manually.\n\nHow does the learning rate finder work? We give the finder our model, optimizer and criterion we want to use. However we give it an optimizer with a much lower learning rate than we are expecting to use. We then train the model on the batches of data from the training set - calculating the loss and updating the parameters. After each batch we increase the learning rate exponentially from the initial, extremely low learning rate to a learning rate we know will be too high. This repeats until our loss diverges (over 5x the best loss achieved) or we reach our defined maximum learning rate.\n\nAt each batch we are recording the learning rate and the loss achieved on that batch. By plotting them against each other we can find a suitable learning rate - but more on how to do that in a bit.\n\nThe losses calculated are usually quite noisy so we actually save the exponentially weighted average of the loss calculated.\n\nWe also want to use our initialized parameters, not the ones obtained by upgrading the parameters when performing the learning rate finder. Hence we save the model parameters to disk when initializing the finder and then they are reset to our desired initialized ones just before the range_test function returns by loading the initial values from disk.","200aa574":"Then we'll see how many trainable parameters our model has.\n\nOur LeNet architecture had ~44k, but here we have 23.2M parameters - and AlexNet is a relatively small model for computer vision.","c8f30ab8":"# Defining the Model\nNext up is defining the model.\n\nThe actual model itself is no more difficult to understand than the previous model, LeNet. It is made up of convolutional layers, pooling layers and ReLU activation functions. See the previous notebook for a refresher on these concepts.\n\nThere are only two new concepts introduced here, nn.Sequential and nn.Dropout.\n\nWe can think of Sequential as like our transforms introduced earlier for data augmentation. We provide Sequential with multiple layers and when the Sequential module is called it will apply each layer, in order, to the input. There is no difference between using a Sequential and having each module defined in the __init__ and then called in forward - however it makes the code significantly shorter.\n\nWe have one Sequential model, features, for all of the convolutional and pooling layers, then we flatten then data and pass it to the classifier, another Sequential model which is made up of linear layers and the second new concept, dropout.\n\nDropout is a form of regularization). As our models get larger, to perform more accurately on richer datasets, they start having a significantly higher number of parameters. The problem with lots of parameters is that our models begin to overfit. That is, they do not learn general image features whilst learning to classify images but instead simply memorize images within the training set. This is bad as it will cause our model to achieve poor performance on the validation\/testing set. To solve this overfitting problem, we use regularization. Dropout is just one method of regularization, other common ones are L1 regularization, L2 regularization and weight decay.\n\nDropout works by randomly setting a certain fraction, 0.5 here, of the neurons in a layer to zero. This effectively adds noise to the training of the neural network and causes neurons to learn with \"less\" data as they are only getting half of the information from a previous layer with dropout applied. It can also be thought of as causing your model to learn multiple smaller models with less parameters.\n\nDropout is only applied when the model is training. It needs to be \"turned off\" when validating, testing or using the model for inference.\n\nAs mentioned in the previous notebook, during the convolutional and pooling layers the activation function should be placed after the pooling layer to reduce computational cost.\n\nIn the linear layers, dropout should be applied after the activation function. Although when using ReLU activation functions the same result is achieved if dropout is before or after, see here.\n\nOne last thing to mention is that the very first convolutional layer has an in_channel of three. That is because we are handling color images that have three channels (red, green and blue) instead of the single channel grayscale images from the MNIST dataset. This doesn't change the way any of the convolutional filter works, it just means the first filter has a depth of three instead of a depth of one.","f414ea5b":"Next, we can plot the learning rate against the loss.\n\nAs our learning rate was scaled up exponentially we want to plot it on a logarithmic scale. We also do not want to plot the last few values as they are usually where the loss is very high and makes it difficult to examine the graph in detail. You can also skip the first few values as nothing interesting happens there.","6abb25be":"We set the random seed so all of our experiments can be reproduced.","e2061348":"A solution to this is to renormalize the images so each pixel is between $[0,1]$. This is done by clipping the pixel values between the maximum and minimum within an image and then scaling each pixel between $[0,1]$ using these maximum and minimums.\n\nAs we can see the images below look a lot more like we were expecting, along with the rotations and cropping.","a5526579":"# Training The Model.","d1946a7f":"# Thankyou for Reading ,If you liked this notebook ,do checkout some of my other notebooks on explaining other architectures,like ResNet and VGG.","d21443c1":"Interestingly the most incorrect was an example that is incorrectly labelled in the dataset itself. It is clearly a frog, which our model predicted with 100% confidence, but the label is cat.\n\nTruck and automobile seem to be confused a lot but even to humans these two classes are slightly ambiguous","5992d9d0":"We can then get the learned values of these filters the same way we did for our version of AlexNet and then plot them.\n\nAs we can see the patterns are much more interesting, though still not really interpretable.\n\nSo how come it learned these interesting looking filters? Is it just because the filters are bigger? Is it because models can only do well on ImageNet if they learn more interesting filters? Or something else?\n\nDo more interesting looking filters imply that they perform better? Or are these filters showing how the model has overfit to patterns on the images within ImageNet?\n\nIt is difficult to answer these questions as modern computer vision architectures now seem to favour smaller filter sizes with their largest filters being 5x5 - so maybe larger filters aren't that good after all?","eaa6816a":"We will also show how to intialize the weights of our neural network and how to find a suitable learning rate using a modified version of the learning rate finder.\n\nLike the previous notebooks we'll implement our model, measure its performance on the dataset, and then have a short look into seeing what the model has learned.\n\nMost of this notebook will be similar to the previous ones thus we will skim over code that has been shown before. We can look at the previous notebook for a refresher if needed.\n\n## Data Processing\nAs always, we'll import the modules we need. A new import is the _LRScheduler which we will use to implement our learning rate finder.","84152e74":"confusingly, instead of just telling the initialization function which non-linearity we want to use and have it calculate the gain for us, we have to tell it what gain we want to use. Luckily, nn.init has a calculate_gain function which does that for us, and we just tell it we are using ReLUs. This type of initialize scheme is called Xavier Normal, also known as Glorot Normal. See the paper for the theory behind it.\n\nFor both types of layer we initialize the bias terms to zeros.\n\nWhy do we even need to initialize our parameters this way? See this article for a great explanation, but the gist of it is that just like how we normalized our input data to have a mean of 0 and a standard deviation of 1, we also want the outputs of each activation function (and therefore the inputs to the subsequent layer) to also have a mean of 0 and a standard deviation of 1. These initialization schemes, by taking account the number of connections in to and out of a layer as well as the non-linearity used, help achieve this normalization effect when initializing weights.","7e783eab":"We can also imagine an image belonging to a specified class.\n\nSpoilers: this didn't work in the previous notebooks, won't work here, and won't work in the future. This is the last time we'll attempt to generate an image.\n\nIf you do know a simple method to generate images that look better than pure random noise, feel free to submit an issue on how it is done and it can be added to these tutorials.","0c55c4f1":"Next, we'll finally use the range finder.\n\nWe first create an instance of the finder class with the model, optimizer, loss function and device. Then we use range_test with the training iterator, the maximum learning rate and the number of iterations we want to use.","8ca0e773":"To prepare to use the range finder we define an initial, very low starting learning rate and then create an instance of the optimizer we want to use with that learning rate.\n\nWe then define the loss function we want to use, the device we'll use and place our model and criterion on to our device.","f09dea15":"We'll be normalizing our images by default from now on, so we'll write a function that does it for us which we can use whenever we need to renormalize an image."}}