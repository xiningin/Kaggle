{"cell_type":{"f1001c79":"code","abb38ec6":"code","0c40f7dd":"code","5c8398ea":"code","b20def4d":"code","ba8d6098":"code","3d350907":"code","dcaa7884":"code","ecd5a0ae":"code","1873ca1b":"code","e71bca5c":"code","a0769591":"markdown","a7ebbb42":"markdown","f10aabc2":"markdown","de462375":"markdown","780fd986":"markdown","5bf07508":"markdown","3108a8fb":"markdown","781785ba":"markdown","ba6f0aa0":"markdown","8b893450":"markdown","4711fa3b":"markdown","3760e39f":"markdown","aa232666":"markdown","97b68145":"markdown","e1f95ea7":"markdown","42b20a83":"markdown","38805220":"markdown","e4bc64bb":"markdown","47306a74":"markdown"},"source":{"f1001c79":"# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn import datasets\n%matplotlib inline\n\n# Load the iris dataset\ndata = pd.read_csv(\"\/kaggle\/input\/iris\/Iris.csv\")\ndata.head() # See the first 5 rows","abb38ec6":"data.shape #There are 4 features and 150 rows in the dataset","0c40f7dd":"# To check if there are any null values in the data\nsns.heatmap(data.isnull())\n# There are no missing values in the data","5c8398ea":"# Scatterplot by species\n\nsns.FacetGrid(data, hue = 'Species', size=5) \\\n    .map(plt.scatter, 'SepalLengthCm','SepalWidthCm') \\\n    .add_legend()","b20def4d":"# We can use boxplot to look at an individual feature through a boxplot\nsns.boxplot(x='Species', y='PetalLengthCm', data=data)","ba8d6098":"# The denser regions of the data are fatter, and sparser thiner in a violin plot\nsns.violinplot(x='Species',y='SepalWidthCm', data=data, size=6)","3d350907":"# Let's see pairplot. \nsns.pairplot(data.drop('Id', axis=1), hue='Species', height=3)\nplt.legend()","dcaa7884":"# We plot a heatmap with input as correlation matrix calculated by data.corr() \nsns.heatmap(data.drop(['Id'], axis=1).corr(), annot=True, cmap='cubehelix_r') ","ecd5a0ae":"# Using elbow method\nX = data.iloc[:, [0,1,2,3]].values\n\nfrom sklearn.cluster import KMeans\n\nwcss = []\n\nfor i in range(1, 10):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', \n                    max_iter = 300, n_init = 10, random_state = 0)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\n    \n# Plotting the results onto a line graph, allowing us to observe 'The elbow'\nplt.plot(range(1, 10), wcss)\nplt.title('The elbow method')\nplt.xlabel('Number of clusters')\nplt.ylabel('Within cluster sum of squares') \nplt.show()\n","1873ca1b":"kmeans = KMeans(n_clusters = 3, init = 'k-means++',\n                max_iter = 300, n_init = 10, random_state = 0)\ny_kmeans = kmeans.fit_predict(X)","e71bca5c":"# Visualising the clusters - On the first two columns\nplt.figure(figsize=(10,8))\nplt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], \n            s = 100, c = 'green', label = 'Iris-setosa')\nplt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], \n            s = 100, c = 'red', label = 'Iris-versicolour')\nplt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1],\n            s = 100, c = 'blue', label = 'Iris-virginica')\n\n# Plotting the centroids of the clusters\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], \n            s = 100, c = 'yellow', label = 'Centroids')\n\nplt.legend()","a0769591":"It is evident from the above pairplots that Iris-setosa species is separated from the other two species","a7ebbb42":"The Iris flower data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems. It is sometimes called Anderson's Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species. The data set consists of 50 samples from each of three species of Iris (Iris Setosa, Iris virginica, and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.\nThe dataset contains a set of 150 records under 5 attributes - Petal Length, Petal Width, Sepal Length, Sepal width and Class(Species).","f10aabc2":"## Visualising the data\n","de462375":"Here there are three clusters of iris flowr species each of which has a centroid marked with yellow dots. The dots in green represent 'iris-setosa', red represent 'iris-versicolor' and blue represent 'iris-virginica'","780fd986":"## Software Used: \nPython 3 ","5bf07508":"## Application\nThe given analysis can be used to identify the pattern and for prediction using a new data.","3108a8fb":"**Definition:**\nK-Means Clustering is an unsupervised machine learning algorithm that divides the observations into \"K\" number of clusters by grouping the data points together. K is the number of centroids that we need in a dataset. Every cluster has a centroid representing the center of a cluster. The algorithm allocates each data point to the nearest cluster and keeps the cluster as small as possible.\n\n**Idea:** \nThe core idea is to minimize the sum of distances between the points and their respective cluster centroid.\n\n**Example:** \nClassifying the unlabeled data of income of people as high, average or low.","781785ba":"## Determining the value of K, the optimun numbeer of clusters for K Means","ba6f0aa0":"#### Pairplots help us see the bivariate relationships between each pair of the features of the data","8b893450":"### Loading the Dataset","4711fa3b":"## Applying kmeans to the dataset","3760e39f":"# K-Means Clustering","aa232666":"## Goal\nIn this notebook we will use K-means Clustering for cluster analysis. We will use Iris dataset predict the optimum number of clusters and classify the flower as versicolor, setosa or virginica","97b68145":"#### Let's also see the correlation among all the features of the data","e1f95ea7":"## Author: Deepika Saini","42b20a83":"## Context","38805220":"![https:\/\/miro.medium.com\/max\/2550\/0*GVjzZeYrir0R_6-X.png](https:\/\/miro.medium.com\/max\/2550\/0*GVjzZeYrir0R_6-X.png)\n\nsource image:https:\/\/miro.medium.com\/max\/2550\/0*GVjzZeYrir0R_6-X.png","e4bc64bb":"## Introduction: What is K-Means Clustering?","47306a74":"The optimum cluster is where the elbow is. \nFrom this we choose the number of clusters as **3**"}}