{"cell_type":{"941cf468":"code","3aed2d5e":"code","5f1d338d":"code","e9c1dca9":"code","bde39da2":"code","1478972c":"code","8b8ff3b2":"code","a0cf6119":"code","1ca0a19b":"code","f95392c4":"code","ee4ebfad":"code","0932aacf":"code","2c8b815a":"code","154ed68a":"code","114f9081":"code","24d39e7c":"code","e083673e":"code","2388b37d":"code","ae3c1bee":"code","45682266":"code","a8931c66":"code","2ed252e0":"code","05203bea":"code","890d9482":"code","8fae1511":"code","15214e75":"code","4f674791":"code","e42b913a":"code","a7ceed29":"code","bfd7389b":"code","d7cc11b5":"code","fe160c5e":"markdown","a7c5de98":"markdown","59859152":"markdown","f68268fc":"markdown","f12c5487":"markdown","34076c2f":"markdown","e48ba8b4":"markdown","b427146b":"markdown","e4e203c8":"markdown","301e3cbb":"markdown","09fc4f11":"markdown","02550f4a":"markdown"},"source":{"941cf468":"import os\nimport random\nimport pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom mlxtend.plotting import plot_confusion_matrix\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\n\nimport lightgbm as lgb\n\ndef seed_everything(seed=2020):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    # tf.random.set_seed(seed)\nseed_everything(0)\n\nsns.set_style(\"whitegrid\")\npalette_ro = [\"#ee2f35\", \"#fa7211\", \"#fbd600\", \"#75c731\", \"#1fb86e\", \"#0488cf\", \"#7b44ab\"]\n\nROOT = \"..\/input\/breast-cancer-wisconsin-data\"","3aed2d5e":"df = pd.read_csv(ROOT + \"\/data.csv\")\n\nprint(\"Data shape: \", df.shape)\ndf.head()","5f1d338d":"df.info()","e9c1dca9":"df.isnull().sum()","bde39da2":"fig, ax = plt.subplots(1, 1, figsize=(8, 8))\nsns.countplot(x=\"diagnosis\", ax=ax, data=df, palette=palette_ro[6::-5], alpha=0.9)\n\nax.annotate(len(df[df[\"diagnosis\"]==\"M\"]), xy=(-0.05, len(df[df[\"diagnosis\"]==\"M\"])+5),\n            size=16, color=palette_ro[6])\nax.annotate(len(df[df[\"diagnosis\"]==\"B\"]), xy=(0.95, len(df[df[\"diagnosis\"]==\"B\"])+5),\n            size=16, color=palette_ro[1])\n\nfig.suptitle(\"Distribution of diagnosis\", fontsize=18);","1478972c":"scaler = StandardScaler()\ncolumns = df.columns.drop([\"id\", \"Unnamed: 32\", \"diagnosis\"])\n\ndata_s = pd.DataFrame(scaler.fit_transform(df[columns]), columns=columns)\ndata_s = pd.concat([df[\"diagnosis\"], data_s.iloc[:, 0:10]], axis=1)\ndata_s = pd.melt(data_s, id_vars=\"diagnosis\", var_name=\"features\", value_name=\"value\")\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 16))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", ax=ax1,\n               data=data_s, palette=palette_ro[6::-5], split=True,\n               scale=\"count\", inner=\"quartile\")\n\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", ax=ax2,\n              data=data_s, palette=palette_ro[6::-5])\n\nfig.suptitle(\"Mean values distribution\", fontsize=18);","8b8ff3b2":"data_s = pd.DataFrame(scaler.fit_transform(df[columns]), columns=columns)\ndata_s = pd.concat([df[\"diagnosis\"], data_s.iloc[:, 10:20]], axis=1)\ndata_s = pd.melt(data_s, id_vars=\"diagnosis\", var_name=\"features\", value_name=\"value\")\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 16))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", ax=ax1,\n               data=data_s, palette=palette_ro[6::-5], split=True,\n               scale=\"count\", inner=\"quartile\")\n\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", ax=ax2,\n              data=data_s, palette=palette_ro[6::-5])\n\nfig.suptitle(\"Standard error values distribution\", fontsize=18);","a0cf6119":"data_s = pd.DataFrame(scaler.fit_transform(df[columns]), columns=columns)\ndata_s = pd.concat([df[\"diagnosis\"], data_s.iloc[:, 20:30]], axis=1)\ndata_s = pd.melt(data_s, id_vars=\"diagnosis\", var_name=\"features\", value_name=\"value\")\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 16))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", ax=ax1,\n               data=data_s, palette=palette_ro[6::-5], split=True,\n               scale=\"count\", inner=\"quartile\")\n\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", ax=ax2,\n              data=data_s, palette=palette_ro[6::-5])\n\nfig.suptitle(\"Worst values distribution\", fontsize=18);","1ca0a19b":"df_c = df.reindex(columns=[\"radius_mean\", \"radius_se\", \"radius_worst\", \"texture_mean\", \"texture_se\", \"texture_worst\",\n                           \"perimeter_mean\", \"perimeter_se\", \"perimeter_worst\", \"area_mean\", \"area_se\", \"area_worst\",\n                           \"smoothness_mean\", \"smoothness_se\", \"smoothness_worst\", \"compactness_mean\", \"compactness_se\", \"compactness_worst\",\n                           \"concavity_mean\", \"concavity_se\", \"concavity_worst\", \"concave points_mean\", \"concave points_se\", \"concave points_worst\",\n                           \"symmetry_mean\", \"symmetry_se\", \"symmetry_worst\", \"fractal_dimension_mean\", \"fractal_dimension_se\", \"fractal_dimension_worst\",\n                           \"diagnosis\"])\ndf_c = df_c.replace({\"M\":1, \"B\":0})\n\nprint(\"Correlation coefficient against diagnosis\")\ndf_c.corr().sort_values(\"diagnosis\", ascending=False)[\"diagnosis\"]","f95392c4":"fig, ax = plt.subplots(1, 1, figsize=(18, 12))\n\nsns.heatmap(df_c.corr(), ax=ax, vmax=1, vmin=-1, center=0,\n            annot=True, fmt=\".2f\",\n            cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            mask=np.triu(np.ones_like(df_c.corr(), dtype=np.bool)))\n\n_, labels = plt.yticks()\nlabels[30].set_color(palette_ro[0])\n\nfig.suptitle(\"Diagonal correlation matrix\", fontsize=18);","ee4ebfad":"X = df.copy()\ny = X[\"diagnosis\"].replace({\"M\":1, \"B\":0})\nX = X.drop([\"id\", \"Unnamed: 32\", \"diagnosis\"], axis=1)","0932aacf":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, stratify=y, random_state=0)\nX_train.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)\nX_train.head()","2c8b815a":"NFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\nimp = np.zeros((NFOLD, len(X_train.columns)))\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = lgb.LGBMClassifier(objective=\"binary\",\n                             metric=\"binary_logloss\")\n    clf.fit(X_tr, y_tr, eval_set=[(X_va, y_va)],\n            early_stopping_rounds=10,\n            verbose=-1)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n    imp[fold_id] = clf.feature_importances_\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test, num_iteration=clf.best_iteration_)\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\n\nprint(f\"\\nOut-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\nprint(f\"Test recall:          {recall_score(y_test, y_pred)}\")\n\nfeature_imp = pd.DataFrame(sorted(zip(np.mean(imp, axis=0), X_train.columns), reverse=True), columns=[\"values\", \"features\"])\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.barplot(x=\"values\", y=\"features\", data=feature_imp, palette=\"Blues_r\")\nplt.title(\"Feature importance of default LightGBM\", fontsize=18);","154ed68a":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of default LightGBM\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1-score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16)\nplt.yticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16);","114f9081":"drop_features1 = [\"radius_mean\", \"radius_se\", \"radius_worst\", \"texture_mean\", \"texture_se\",\n                  \"perimeter_mean\", \"perimeter_se\", \"area_mean\", \"area_worst\",\n                  \"smoothness_mean\", \"smoothness_se\", \"compactness_mean\", \"compactness_se\", \"compactness_worst\",\n                  \"concavity_mean\", \"concavity_se\", \"concavity_worst\", \"concave points_worst\",\n                  \"symmetry_mean\", \"symmetry_se\", \"fractal_dimension_mean\", \"fractal_dimension_se\"]\nX_1 = X.drop(drop_features1, axis=1)\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\nsns.heatmap(pd.concat([X_1, y], axis=1).corr(), ax=ax, vmax=1, vmin=-1, center=0,\n            annot=True, fmt=\".2f\",\n            cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            mask=np.triu(np.ones_like(pd.concat([y, X_1], axis=1).corr(), dtype=np.bool)))\n\n_, labels = plt.yticks()\nlabels[8].set_color(palette_ro[0])\n\nfig.suptitle(\"Diagonal correlation matrix\", fontsize=18);\n\nX_train, X_test, y_train, y_test = train_test_split(X_1, y, test_size=0.3, shuffle=True, stratify=y, random_state=0)\nX_train.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)","24d39e7c":"NFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\nimp = np.zeros((NFOLD, len(X_train.columns)))\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = lgb.LGBMClassifier(objective=\"binary\",\n                             metric=\"binary_logloss\",\n                             min_child_samples=10,\n                             reg_alpha=0.1)\n    clf.fit(X_tr, y_tr, eval_set=[(X_va, y_va)],\n            early_stopping_rounds=10,\n            verbose=-1)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n    imp[fold_id] = clf.feature_importances_\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test, num_iteration=clf.best_iteration_)\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\ny_pred_gbm = np.mean(y_preds, axis=1)\n\nprint(f\"\\nOut-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\nprint(f\"Test recall:          {recall_score(y_test, y_pred)}\")\n\nfeature_imp = pd.DataFrame(sorted(zip(np.mean(imp, axis=0), X_train.columns), reverse=True), columns=[\"values\", \"features\"])\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.barplot(x=\"values\", y=\"features\", data=feature_imp, palette=\"Blues_r\")\nplt.title(\"Feature importance of optimized LightGBM\", fontsize=18);","e083673e":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of optimized LightGBM\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16)\nplt.yticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16);","2388b37d":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, stratify=y, random_state=0)\nX_train.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)","ae3c1bee":"NFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\nimp = np.zeros((NFOLD, len(X_train.columns)))\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = ExtraTreesClassifier(random_state=0)\n    clf.fit(X_tr, y_tr)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n    imp[fold_id] = clf.feature_importances_\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test)\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\nprint(f\"Test recall:          {recall_score(y_test, y_pred)}\")\n\nfeature_imp = pd.DataFrame(sorted(zip(np.mean(imp, axis=0), X_train.columns), reverse=True), columns=[\"values\", \"features\"])\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.barplot(x=\"values\", y=\"features\", data=feature_imp, palette=\"Blues_r\")\nplt.title(\"Feature importance of default Extremely randomized trees\", fontsize=18);","45682266":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of default Extremely randomized trees\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16)\nplt.yticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16);","a8931c66":"drop_features2 = [\"radius_mean\", \"radius_se\", \"radius_worst\", \"texture_mean\", \"texture_se\",\n                  \"perimeter_mean\", \"perimeter_se\", \"area_mean\", \"area_worst\",\n                  \"smoothness_mean\", \"smoothness_se\", \"compactness_mean\", \"compactness_se\", \"compactness_worst\",\n                  \"concavity_mean\",  \"concavity_worst\", \"concave points_mean\", \"concave points_se\",\n                  \"symmetry_mean\", \"symmetry_se\", \"fractal_dimension_mean\", \"fractal_dimension_se\"]\nX_2 = X.drop(drop_features2, axis=1)\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\nsns.heatmap(pd.concat([X_2, y], axis=1).corr(), ax=ax, vmax=1, vmin=-1, center=0,\n            annot=True, fmt=\".2f\",\n            cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            mask=np.triu(np.ones_like(pd.concat([y, X_2], axis=1).corr(), dtype=np.bool)))\n\n_, labels = plt.yticks()\nlabels[8].set_color(palette_ro[0])\n\nfig.suptitle(\"Diagonal correlation matrix\", fontsize=18);\n\nX_train, X_test, y_train, y_test = train_test_split(X_2, y, test_size=0.3, shuffle=True, stratify=y, random_state=0)\nX_train.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)","2ed252e0":"NFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\nimp = np.zeros((NFOLD, len(X_train.columns)))\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = ExtraTreesClassifier(random_state=0,\n                               n_estimators=200,\n                               min_samples_split=5)\n    clf.fit(X_tr, y_tr)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n    imp[fold_id] = clf.feature_importances_\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test)\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\ny_pred_ert = np.mean(y_preds, axis=1)\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\nprint(f\"Test recall:          {recall_score(y_test, y_pred)}\")\n\nfeature_imp = pd.DataFrame(sorted(zip(np.mean(imp, axis=0), X_train.columns), reverse=True), columns=[\"values\", \"features\"])\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.barplot(x=\"values\", y=\"features\", data=feature_imp, palette=\"Blues_r\")\nplt.title(\"Feature importance of optimized Extremely randomized trees\", fontsize=18);","05203bea":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of optimized Extremely randomized trees\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16)\nplt.yticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16);","890d9482":"scaler = StandardScaler()\nX_s = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n\nX_train, X_test, y_train, y_test = train_test_split(X_s, y, test_size=0.3, shuffle=True, stratify=y, random_state=0)\nX_train.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)","8fae1511":"NFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = LogisticRegression(random_state=0)\n    clf.fit(X_tr, y_tr)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test)\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\nprint(f\"Test recall:          {recall_score(y_test, y_pred)}\")","15214e75":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of default linear model\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16)\nplt.yticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16);","4f674791":"drop_features3 = [\"radius_mean\", \"radius_se\", \"radius_worst\", \"texture_mean\", \"texture_se\",\n                  \"perimeter_mean\", \"perimeter_se\", \"area_mean\", \"area_worst\",\n                  \"smoothness_mean\", \"smoothness_se\", \"compactness_mean\", \"compactness_se\", \n                  \"concavity_se\", \"concavity_worst\", \"concave points_mean\", \"concave points_se\",\n                  \"symmetry_mean\", \"symmetry_se\", \"fractal_dimension_mean\", \"fractal_dimension_se\", \"fractal_dimension_worst\"]\nX_3 = X_s.drop(drop_features3, axis=1)\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\nsns.heatmap(pd.concat([X_3, y], axis=1).corr(), ax=ax, vmax=1, vmin=-1, center=0,\n            annot=True, fmt=\".2f\",\n            cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            mask=np.triu(np.ones_like(pd.concat([y, X_3], axis=1).corr(), dtype=np.bool)))\n\n_, labels = plt.yticks()\nlabels[8].set_color(palette_ro[0])\n\nfig.suptitle(\"Diagonal correlation matrix\", fontsize=18);\n\nX_train, X_test, y_train, y_test = train_test_split(X_3, y, test_size=0.3, shuffle=True, stratify=y, random_state=0)\nX_train.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)","e42b913a":"NFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = LogisticRegression(random_state=0)\n    clf.fit(X_tr, y_tr)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test)\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\ny_pred_lm = np.mean(y_preds, axis=1)\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\nprint(f\"Test recall:          {recall_score(y_test, y_pred)}\")","a7ceed29":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of optimized linear model\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16)\nplt.yticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16);","bfd7389b":"y_pred_em = y_pred_gbm*2 +  y_pred_ert*2 + y_pred_lm\ny_pred_em = (y_pred_em > 3).astype(int)\n\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred_em)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred_em)}\")\nprint(f\"Test recall:          {recall_score(y_test, y_pred_em)}\")","d7cc11b5":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred_em), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of the ensembled model\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1-score={:0.4f}\".format(accuracy_score(y_test, y_pred_em), f1_score(y_test, y_pred_em)), fontsize=14)\nplt.xticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16)\nplt.yticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16);","fe160c5e":"<a id=\"load\"><\/a>\n# Load CSV data \ud83d\udcc3","a7c5de98":"<a id=\"ensemble\"><\/a>\n# Simple ensemble \ud83e\udd1d\nFor better accuracy, ensemble predictions of the three models.<br>\n<font color=\"RoyalBlue\">\u7cbe\u5ea6\u3092\u9ad8\u3081\u308b\u305f\u3081\u306b\u30013\u3064\u306e\u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3092\u884c\u3044\u307e\u3057\u3087\u3046\u3002<\/font>","59859152":"Since many of the features in this dataset have high correlation coefficients with each other, feature selection is very important.<br>\n<font color=\"RoyalBlue\">\u3053\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u7279\u5fb4\u91cf\u306b\u306f\u4e92\u3044\u306b\u76f8\u95a2\u4fc2\u6570\u306e\u9ad8\u3044\u3082\u306e\u304c\u591a\u3044\u305f\u3081\u3001\u7279\u5fb4\u91cf\u9078\u629e\u304c\u975e\u5e38\u306b\u91cd\u8981\u306b\u306a\u3063\u3066\u304d\u307e\u3059\u3002<\/font><br>\n\n<a id=\"preprocessing\"><\/a>\n# Data preprocessing \ud83e\uddf9","f68268fc":"Do the same feature selection as before.<br>\n<font color=\"RoyalBlue\">\u5148\u7a0b\u3068\u540c\u3058\u3088\u3046\u306b\u3001\u7279\u5fb4\u91cf\u9078\u629e\u3092\u884c\u3044\u307e\u3059\u3002<\/font>","f12c5487":"<a id=\"models\"><\/a>\n# Train models and make predictions \ud83d\udcad\nNow, let's create some models and check the performance measures. The performance measure for classifiers are as follows.<br>\n<font color=\"RoyalBlue\">\u3067\u306f\u3001\u3044\u304f\u3064\u304b\u306e\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3057\u3001\u6027\u80fd\u6307\u6a19\u3092\u78ba\u8a8d\u3057\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002\u5206\u985e\u5668\u306e\u6027\u80fd\u6307\u6a19\u306b\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u3082\u306e\u304c\u3042\u308a\u307e\u3059\u3002<\/font>\n\n> Referenced from Hands-On Machine Learning with Scikit-Learn and TensorFlow (Aurelien Geron, 2017).\n* accuracy - the ratio of correct predictions\n<br>\u3000<font color=\"RoyalBlue\">\u6b63\u89e3\u7387 - \u6b63\u3057\u3044\u4e88\u6e2c\u306e\u5272\u5408<\/font>\n* confusion matrix - counting the number of times instances of class A are classified as class B\n<br>\u3000<font color=\"RoyalBlue\">\u6df7\u540c\u884c\u5217 - \u30af\u30e9\u30b9\uff21\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u304c\u30af\u30e9\u30b9\uff22\u306b\u5206\u985e\u3055\u308c\u305f\u56de\u6570\u3092\u6570\u3048\u308b<\/font>\n* precision - the accuracy of the positive predictions\n<br>\u3000<font color=\"RoyalBlue\">\u9069\u5408\u7387 - \u967d\u6027\u306e\u4e88\u6e2c\u306e\u6b63\u89e3\u7387\uff08\u967d\u6027\u3067\u3042\u308b\u3068\u4e88\u6e2c\u3057\u305f\u3046\u3061\u3001\u5f53\u305f\u3063\u3066\u3044\u305f\u7387\uff09<\/font>\n* recall (sensitivity, true positive rate: TPR) - the ratio of positive instances that are correctly detected by the classifier\n<br>\u3000<font color=\"RoyalBlue\">\u518d\u73fe\u7387\uff08\u611f\u5ea6\u3001\u771f\u967d\u6027\u7387\uff09- \u5206\u985e\u5668\u304c\u6b63\u3057\u304f\u5206\u985e\u3057\u305f\u967d\u6027\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306e\u5272\u5408\uff08\u672c\u5f53\u306b\u967d\u6027\u3067\u3042\u308b\u30b1\u30fc\u30b9\u306e\u3046\u3061\u3001\u967d\u6027\u3060\u3068\u5224\u5b9a\u3067\u304d\u305f\u7387\uff09<\/font>\n* F1 score - the harmonic mean of precision and recall\n<br>\u3000<font color=\"RoyalBlue\">F1 \u30b9\u30b3\u30a2\uff08F \u5024\uff09 - \u9069\u5408\u7387\u3068\u518d\u73fe\u7387\u306e\u8abf\u548c\u5e73\u5747\uff08\u7b97\u8853\u5e73\u5747\u306b\u6bd4\u3079\u3001\u8abf\u548c\u5e73\u5747\u306f\u4f4e\u3044\u5024\u306b\u305d\u3046\u3067\u306a\u3044\u5024\u3088\u308a\u3082\u305a\u3063\u3068\u5927\u304d\u306a\u91cd\u307f\u3092\u7f6e\u304f\uff09<\/font>\n* AUC - the area under the ROC curve (plotting the true positive rate (another name for recall) against the false positive rate)\n<br>\u3000<font color=\"RoyalBlue\">AUC - ROC \u66f2\u7dda\uff08\u507d\u967d\u6027\u7387\u306b\u5bfe\u3059\u308b\u771f\u967d\u6027\u7387\uff08\u518d\u73fe\u7387\uff09\u3092\u30d7\u30ed\u30c3\u30c8\u3057\u305f\u66f2\u7dda\uff09\u306e\u4e0b\u306e\u9762\u7a4d<\/font><br>\n\nIn this notebook, we will look at their accuracy, F1 score, and confusion matrix.<br>\n<font color=\"RoyalBlue\">\u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3067\u306f\u3001\u6b63\u89e3\u7387\u3001F1 \u30b9\u30b3\u30a2\u3001\u305d\u3057\u3066\u6df7\u540c\u884c\u5217\u3092\u898b\u3066\u3044\u304d\u307e\u3059\u3002<\/font>\n\n<a id=\"gbm\"><\/a>\n## LightGBM \ud83c\udf33\nFirst, let's try a prediction with all the features using LightGBM.<br>\n<font color=\"RoyalBlue\">\u307e\u305a\u306f\u3001LightGBM \u3067\u5168\u3066\u306e\u7279\u5fb4\u91cf\u3092\u4f7f\u3063\u305f\u4e88\u6e2c\u3092\u8a66\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002<\/font>","34076c2f":"<a id=\"ert\"><\/a>\n## Extremely randomized trees \ud83c\udf33\nWe will also use the Extremely randomized trees.<br>\n<font color=\"RoyalBlue\">Extremely randomized trees \u3082\u4f7f\u3063\u3066\u307f\u307e\u3057\u3087\u3046\u3002<\/font>","e48ba8b4":"Dataset from: [Breast Cancer Wisconsin (Diagnostic) Data Set](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+%28Diagnostic%29)\n\n* `id` - ID number\n* `diagnosis` - Diagnosis (`M`: malignant, `B`: benign)\n<br>\u3000<font color=\"RoyalBlue\">\u3010\u76ee\u7684\u5909\u6570\u3011\u8a3a\u65ad\uff08\u7d50\u679c\uff09\uff08M : \u60aa\u6027\uff0cB : \u826f\u6027\uff09<\/font><br>\n\nThe following features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.<br>\n<font color=\"RoyalBlue\">\u4ee5\u4e0b\u306e\u7279\u5fb4\u91cf\u306f\u3001\u4e73\u817a\u816b\u7624\u306e\u7a7f\u523a\u5438\u5f15\u7d30\u80de\u8a3a\uff08FNA\uff09\u306e\u30c7\u30b8\u30bf\u30eb\u753b\u50cf\u304b\u3089\u8a08\u7b97\u3055\u308c\u305f\u30c7\u30fc\u30bf\u3067\u3059\u3002\u753b\u50cf\u5185\u306b\u5b58\u5728\u3059\u308b\u7d30\u80de\u6838\u306e\u7279\u5fb4\u3092\u8aac\u660e\u3057\u3066\u3044\u307e\u3059\u3002\u4ee5\u4e0b\u306e10\u306e\u5404\u5c5e\u6027\u306b\u3064\u3044\u3066\u3001\u305d\u308c\u305e\u308c\u5e73\u5747\uff08mean\uff09\u3001\u6a19\u6e96\u8aa4\u5dee\uff08se\uff09\u3001\u6700\u60aa\u5024\uff08worst\uff09\u306e3\u7a2e\u985e\u3001\u5408\u8a0830\u306e\u7279\u5fb4\u91cf\u304c\u683c\u7d0d\u3055\u308c\u3066\u3044\u307e\u3059\u3002<\/font><br>\n\n* `radius` - mean of distances from center to points on the perimeter\n<br>\u3000<font color=\"RoyalBlue\">\u534a\u5f84 - \u4e2d\u5fc3\u304b\u3089\u5916\u5468\u4e0a\u306e\u70b9\u307e\u3067\u306e\u8ddd\u96e2\u306e\u5e73\u5747<\/font>\n* `texture` - standard deviation of gray-scale values\n<br>\u3000<font color=\"RoyalBlue\">\u30c6\u30af\u30b9\u30c1\u30e3 - \u30b0\u30ec\u30fc\u30b9\u30b1\u30fc\u30eb\u5024\u306e\u6a19\u6e96\u504f\u5dee<\/font>\n* `perimeter`\n<br>\u3000<font color=\"RoyalBlue\">\u5916\u5468\u9577<\/font>\n* `area`\n<br>\u3000<font color=\"RoyalBlue\">\u9762\u7a4d<\/font>\n* `smoothness` - local variation in radius lengths\n<br>\u3000<font color=\"RoyalBlue\">\u5e73\u6ed1\u6027 - \u534a\u5f84\u306e\u9577\u3055\u306e\u5c40\u6240\u5909\u52d5<\/font>\n* `compactness` - perimeter^2 \/ area - 1.0\n<br>\u3000<font color=\"RoyalBlue\">\u30b3\u30f3\u30d1\u30af\u30c8\u6027 - \u5916\u5468\u9577^2 \/ \u9762\u7a4d - 1.0<\/font>\n* `concavity` - severity of concave portions of the contour\n<br>\u3000<font color=\"RoyalBlue\">\u51f9\u5ea6 - \u8f2a\u90ed\u306e\u51f9\u90e8\u306e\u7a0b\u5ea6<\/font>\n* `concave points` - number of concave portions of the contour\n<br>\u3000<font color=\"RoyalBlue\">\u51f9\u70b9\u6570 - \u8f2a\u90ed\u306e\u51f9\u90e8\u306e\u6570<\/font>\n* `symmetry`\n<br>\u3000<font color=\"RoyalBlue\">\u5bfe\u79f0\u6027<\/font>\n* `fractal dimension` - \"coastline approximation\" - 1\n<br>\u3000<font color=\"RoyalBlue\">\u30d5\u30e9\u30af\u30bf\u30eb\u6b21\u5143 - \u8907\u96d1\u3055\u306e\u7a0b\u5ea6\u3092\u8868\u3059\u5c3a\u5ea6\u3002\u8907\u96d1\u3067\u3042\u308c\u3070\u3042\u308b\u307b\u3069\u5024\u304c\u5927\u304d\u304f\u306a\u308b<\/font>\n\n<a id=\"explore\"><\/a>\n# Explore CSV data \ud83d\udcca\nAcknowledgements: [Feature Selection and Data Visualization](https:\/\/www.kaggle.com\/kanncaa1\/feature-selection-and-data-visualization)","b427146b":"We were able to get better accuracy by using the ensemble model. Thanks so much for reading!<br>\n<font color=\"RoyalBlue\">\u8907\u6570\u306e\u30e2\u30c7\u30eb\u3092\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3059\u308b\u3053\u3068\u3067\u3088\u308a\u826f\u3044\u7cbe\u5ea6\u3092\u51fa\u3059\u3053\u3068\u304c\u3067\u304d\u307e\u3057\u305f\u3002\u3053\u3053\u307e\u3067\u8aad\u3093\u3067\u304f\u3060\u3055\u308a\u3069\u3046\u3082\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3057\u305f\uff01<\/font>","e4e203c8":"<a id=\"overview\"><\/a>\n# Overview \ud83e\uddd0\n<img src=\"https:\/\/i.imgur.com\/HVZezzb.jpg\" width=\"600\"><br>\nIn this notebook, we are going to predict whether a breast mass is benign or malignant based on 30 features in the dataset. This prediction can be useful in diagnosing patients with suspected breast cancer.<br>\n<font color=\"RoyalBlue\">\u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3067\u306f\u3001\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u542b\u307e\u308c\u308b30\u306e\u7279\u5fb4\u91cf\u304b\u3089\u4e73\u817a\u816b\u7624\u304c\u826f\u6027\u304b\u60aa\u6027\u304b\u3092\u4e88\u6e2c\u3057\u307e\u3059\u3002\u3053\u306e\u4e88\u6e2c\u306f\u3001\u4e73\u304c\u3093\u306e\u7591\u3044\u304c\u3042\u308b\u60a3\u8005\u3092\u8a3a\u65ad\u3059\u308b\u969b\u306b\u5f79\u7acb\u3066\u3089\u308c\u308b\u3067\u3057\u3087\u3046\u3002<\/font><br>\n\nI have also run a similar analysis in R ([Breast Cancer\ud83e\udd80EDA & FA \/ PCA with R (98.2% acc)](https:\/\/www.kaggle.com\/snowpea8\/breast-cancer-eda-fa-pca-with-r-98-2-acc)), \nif you would like to take a look at it.<br>\n<font color=\"RoyalBlue\">\u540c\u69d8\u306e\u5206\u6790\u3092 R \u3067\u3082\u5b9f\u884c\u3057\u3066\u3044\u307e\u3059\u306e\u3067\u3001\u305d\u3061\u3089\u3082\u53c2\u8003\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002<\/font><br>\n\nWe will first discover and visualize the data to gain insights. Then we split the data into a training and a test set and use the training set to train some machine learning models. At the same time, we evaluate the performance of the models with cross-validation. Finally, we will ensemble each model to improve its accuracy.<br>\n<font color=\"RoyalBlue\">\u307e\u305a\u6d1e\u5bdf\u3092\u5f97\u308b\u305f\u3081\u306b\u30c7\u30fc\u30bf\u3092\u7814\u7a76\u3001\u53ef\u8996\u5316\u3057\u307e\u3059\u3002\u305d\u308c\u304b\u3089\u30c7\u30fc\u30bf\u3092\u8a13\u7df4\u7528\u3068\u30c6\u30b9\u30c8\u7528\u306b\u5206\u5272\u3057\u3001\u8a13\u7df4\u30bb\u30c3\u30c8\u3092\u4f7f\u3063\u3066\u3044\u304f\u3064\u304b\u306e\u6a5f\u68b0\u5b66\u7fd2\u30e2\u30c7\u30eb\u3092\u8a13\u7df4\u3057\u307e\u3059\u3002\u540c\u6642\u306b\u3001\u4ea4\u5dee\u691c\u8a3c\u3067\u30e2\u30c7\u30eb\u306e\u6027\u80fd\u3092\u8a55\u4fa1\u3057\u307e\u3059\u3002\u6700\u5f8c\u306b\u305d\u308c\u305e\u308c\u306e\u30e2\u30c7\u30eb\u3092\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3057\u3001\u7cbe\u5ea6\u306e\u5411\u4e0a\u3092\u76ee\u6307\u3057\u3066\u3044\u304d\u307e\u3059\u3002<\/font>\n\n# Table of contents \ud83d\udcd6\n* [Overview \ud83e\uddd0](#overview)\n* [Setup \ud83d\udcbb](#setup)\n* [Load CSV data \ud83d\udcc3](#load)\n* [Explore CSV data \ud83d\udcca](#explore)\n* [Data preprocessing \ud83e\uddf9](#preprocessing)\n* [Train models and make predictions \ud83d\udcad](#models)\n    * [LightGBM \ud83c\udf33](#gbm)\n    * [Extremely randomized trees \ud83c\udf33](#ert)\n    * [Linear model \ud83d\udcc8](#lm)\n* [Simple ensemble \ud83e\udd1d](#ensemble)\n\n<a id=\"setup\"><\/a>\n# Setup \ud83d\udcbb\nAll seed values are fixed at zero.<br>\n<font color=\"RoyalBlue\">\u30b7\u30fc\u30c9\u5024\u306f\u5168\u30660\u3067\u56fa\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002<\/font><br>","301e3cbb":"<a id=\"lm\"><\/a>\n## Linear model \ud83d\udcc8\nIn order to get diverse models, we will also try linear model as a model without decision trees.<br>\n<font color=\"RoyalBlue\">\u591a\u69d8\u6027\u306e\u3042\u308b\u30e2\u30c7\u30eb\u3092\u5f97\u308b\u305f\u3081\u306b\u3001\u6c7a\u5b9a\u6728\u3092\u4f7f\u308f\u306a\u3044\u30e2\u30c7\u30eb\u3068\u3057\u3066\u3001\u7dda\u5f62\u30e2\u30c7\u30eb\u3082\u8a66\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002<\/font>","09fc4f11":"Next, narrow down the number of features based on EDA and feature importance. Let's choose the following features.<br>\n<font color=\"RoyalBlue\">\u6b21\u306b\u3001EDA \u3084 feature importance \u3092\u3082\u3068\u306b\u7279\u5fb4\u91cf\u306e\u6570\u3092\u7d5e\u308a\u307e\u3059\u3002\u4e0b\u8a18\u306e\u3088\u3046\u306a\u7279\u5fb4\u91cf\u3092\u9078\u3093\u3067\u3044\u304d\u307e\u3057\u3087\u3046\u3002<\/font>\n* High correlation coefficient with the objective variable\n<br>\u3000<font color=\"RoyalBlue\">\u76ee\u7684\u5909\u6570\u3068\u306e\u76f8\u95a2\u4fc2\u6570\u304c\u9ad8\u3044<\/font>\n* Less mixing in data distribution for the objective variable\n<br>\u3000<font color=\"RoyalBlue\">\u76ee\u7684\u5909\u6570\u306b\u5bfe\u3059\u308b\u30c7\u30fc\u30bf\u5206\u5e03\u306b\u304a\u3044\u3066\u6df7\u5728\u304c\u5c11\u306a\u3044<\/font>\n* High feature importance\n<br>\u3000<font color=\"RoyalBlue\">feature importance \u304c\u9ad8\u3044<\/font>\n* Features are independent of each other (to eliminate multicollinearity)\n<br>\u3000<font color=\"RoyalBlue\">\u7279\u5fb4\u91cf\u540c\u58eb\u304c\u306a\u308b\u3079\u304f\u72ec\u7acb\u3057\u3066\u3044\u308b\uff08\u591a\u91cd\u5171\u7dda\u6027\u3092\u89e3\u6d88\u3059\u308b\u305f\u3081\uff09<\/font>","02550f4a":"Do feature selection.<br>\n<font color=\"RoyalBlue\">\u7279\u5fb4\u91cf\u9078\u629e\u3092\u884c\u3044\u307e\u3059\u3002<\/font>"}}