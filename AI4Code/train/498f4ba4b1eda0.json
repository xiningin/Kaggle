{"cell_type":{"24451efb":"code","2d2022e2":"code","4e70443b":"code","90b64cb8":"code","20e89ac2":"code","3a02bd15":"code","e68278fc":"code","460d7ff6":"markdown","34a355fe":"markdown","2a21bbcf":"markdown","dd0aefa5":"markdown","34c4b025":"markdown","1082fefe":"markdown","e556ba84":"markdown"},"source":{"24451efb":"import codecs\nimport re\nimport operator\nimport numpy as np  \n\n\nnum_regex = re.compile(r'^[+-]?[0-9]+\\.?[0-9]*$')\n\ndef is_number(token):\n    return bool(num_regex.match(token))\n\n\ndef create_vocab(domain, use_doc, maxlen=0, vocab_size=0):\n    file_list = [\n        '..\/input\/imn-e2e-absa\/data_preprocessed\/%s\/train\/sentence.txt' % domain,\n        '..\/input\/imn-e2e-absa\/data_preprocessed\/%s\/test\/sentence.txt' % domain,\n    ]\n\n    if use_doc:\n        file_list.append('..\/input\/imn-e2e-absa\/data_doc\/yelp_large\/text.txt')\n        file_list.append('..\/input\/imn-e2e-absa\/data_doc\/electronics_large\/text.txt')\n\n    print('Creating vocab ...')\n\n    total_words, unique_words = 0, 0\n    word_freqs = {}\n\n    for f in file_list:\n        top = 0\n        fin = codecs.open(f, 'r', 'utf-8')\n        for line in fin:\n            words = line.split()\n            if maxlen > 0 and len(words) > maxlen:\n                continue\n\n            for w in words:\n                if not is_number(w):\n                    try:\n                        word_freqs[w] += 1\n                    except KeyError:\n                        unique_words += 1\n                        word_freqs[w] = 1\n                    total_words += 1\n    fin.close()\n\n    print('%i total words, %i unique words' % (total_words, unique_words))\n    sorted_word_freqs = sorted(word_freqs.items(), key=operator.itemgetter(1), reverse=True)\n\n    vocab = {\n        '<pad>':0, \n        '<unk>':1, \n        '<num>':2\n    }\n    index = len(vocab)\n    for word, _ in sorted_word_freqs:\n        vocab[word] = index\n        index += 1\n        if vocab_size > 0 and index > vocab_size+2:\n            break\n    if vocab_size > 0:\n        print('keep the top %i words' % vocab_size)\n    return vocab\n\n\ndef read_data(vocab, maxlen, path=None, domain=None, phase=None):\n    if path:\n        f = codecs.open(path)\n    else:\n        f = codecs.open('..\/input\/imn-e2e-absa\/data_preprocessed\/%s\/%s\/sentence.txt' % (domain, phase))\n\n    data = []\n    num_hit, unk_hit, total = 0., 0., 0.\n    maxlen_x = 0\n\n    for row in f:\n        indices = []\n        tokens = row.strip().split()\n\n        if maxlen > 0 and len(tokens) > maxlen:\n            continue\n\n        if len(tokens) == 0:\n            indices.append(vocab['<unk>'])\n            unk_hit += 1\n        for word in tokens:\n            if is_number(word):\n                indices.append(vocab['<num>'])\n                num_hit += 1\n            elif word in vocab:\n                indices.append(vocab[word])\n            else:\n                indices.append(vocab['<unk>'])\n                unk_hit += 1\n            total += 1\n\n        data.append(indices)\n        if maxlen_x < len(tokens):\n            maxlen_x = len(tokens)\n\n    f.close()\n    return data, maxlen_x\n\n\ndef read_label(domain, phase):\n    f_t = codecs.open('..\/input\/imn-e2e-absa\/data_preprocessed\/%s\/%s\/target.txt' % (domain, phase))\n    f_o = codecs.open('..\/input\/imn-e2e-absa\/data_preprocessed\/%s\/%s\/opinion.txt' % (domain, phase))\n    f_p = codecs.open('..\/input\/imn-e2e-absa\/data_preprocessed\/%s\/%s\/target_polarity.txt' % (domain, phase))\n   \n    target_label = []\n    op_label = []\n    pol_label = []\n    for t, o, p in zip(f_t, f_o, f_p):\n        target_label.append([int(s) for s in t.strip().split()])\n        op_label.append([int(s) for s in o.strip().split()])\n        pol_label.append([int(s) for s in p.strip().split()])\n\n    f_t.close()\n    f_o.close()\n    f_p.close()\n    return target_label, op_label, pol_label\n\n\ndef read_label_doc(path):\n    f = codecs.open(path)\n    doc_label = []\n    for line in f:\n        score = float(line.strip())\n        if score > 3:\n            doc_label.append(0)\n        elif score < 3:\n            doc_label.append(1)\n        else:\n            doc_label.append(2)\n    f.close()\n    return doc_label\n\n\ndef prepare_data(domain, vocab_size, use_doc=True, maxlen=0):\n    assert domain in ['res', 'lt', 'res_15']\n\n    vocab = create_vocab(domain, use_doc, maxlen, vocab_size)\n\n    train_x, train_maxlen = read_data(vocab, maxlen, domain=domain, phase='train')\n    train_label_target, train_label_opinion, train_label_polarity = read_label(domain, 'train')\n    \n    test_x, test_maxlen = read_data(vocab, maxlen, domain=domain, phase='test')\n    test_label_target, test_label_opinion, test_label_polarity = read_label(domain, 'test')\n\n    overall_maxlen_aspect = max(train_maxlen, test_maxlen)\n\n    doc_res_x, doc_res_y, doc_lt_x, doc_lt_y, doc_res_maxlen, doc_lt_maxlen = None, None, None, None, None, None\n    if use_doc:\n        doc_res_x, doc_res_maxlen = read_data(vocab, maxlen, path='..\/input\/imn-e2e-absa\/data_doc\/yelp_large\/text.txt')\n        doc_res_y = read_label_doc('..\/input\/imn-e2e-absa\/data_doc\/yelp_large\/label.txt')\n        doc_lt_x, doc_lt_maxlen = read_data(vocab, maxlen, path='..\/input\/imn-e2e-absa\/data_doc\/electronics_large\/text.txt')\n        doc_lt_y = read_label_doc('..\/input\/imn-e2e-absa\/data_doc\/electronics_large\/label.txt')\n\n    return train_x, train_label_target, train_label_opinion, train_label_polarity,\\\n           test_x, test_label_target, test_label_opinion, test_label_polarity,\\\n           vocab, overall_maxlen_aspect,\\\n           doc_res_x, doc_res_y, doc_lt_x, doc_lt_y, doc_res_maxlen, doc_lt_maxlen\n\n\ndef get_statistics(label, polarity=None):\n    num = 0\n    if polarity:\n        count = {'pos':0, 'neg':0, 'neu':0, 'conf':0}\n        polarity_map = {1: 'pos', 2: 'neg', 3: 'neu', 4: 'conf'}\n\n    for i in range(len(label)):\n        if polarity:\n            assert len(label[i]) == len(polarity[i])\n        for j in range(len(label[i])):\n            if label[i][j] == 1:\n                num += 1\n                if polarity:\n                    count[polarity_map[polarity[i][j]]] += 1\n    if polarity:\n        return num, count\n    else:\n        return num","2d2022e2":"import sys\nimport os, errno\nimport logging\n\n#-----------------------------------------------------------------------------------------------------------#\n\ndef set_logger(out_dir=None):\n    console_format = BColors.OKBLUE + '[%(levelname)s]' + BColors.ENDC + ' (%(name)s) %(message)s'\n    #datefmt='%Y-%m-%d %Hh-%Mm-%Ss'\n    logger = logging.getLogger()\n    logger.setLevel(logging.DEBUG)\n    console = logging.StreamHandler()\n    console.setLevel(logging.DEBUG)\n    console.setFormatter(logging.Formatter(console_format))\n    logger.addHandler(console)\n    if out_dir:\n        file_format = '[%(levelname)s] (%(name)s) %(message)s'\n        log_file = logging.FileHandler(out_dir + '\/log.txt', mode='w')\n        log_file.setLevel(logging.DEBUG)\n        log_file.setFormatter(logging.Formatter(file_format))\n        logger.addHandler(log_file)\n\n#-----------------------------------------------------------------------------------------------------------#\n\ndef mkdir_p(path):\n    if path == '':\n        return\n    try:\n        os.makedirs(path)\n    except OSError as exc: # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else: raise\n\n\ndef get_root_dir():\n    return os.path.dirname(sys.argv[0])\n\n\ndef bincounts(array):\n    num_rows = array.shape[0]\n    if array.ndim > 1:\n        num_cols = array.shape[1]\n    else:\n        num_cols = 1\n        array = array[:, None]\n    counters = []\n    mfe_list = []\n    for col in range(num_cols):\n        counter = {}\n        for row in range(num_rows):\n            element = array[row,col]\n            if element in counter:\n                counter[element] += 1\n            else:\n                counter[element] = 1\n        max_count = 0\n        for element in counter:\n            if counter[element] > max_count:\n                max_count = counter[element]\n                mfe = element\n        counters.append(counter)\n        mfe_list.append(mfe)\n    return counters, mfe_list\n\n\n# Convert all arguments to strings\ndef ltos(*args):\n    outputs = []\n    for arg in args:\n        if type(arg) == list:\n            out = ' '.join(['%.3f' % e for e in arg])\n            if len(arg) == 1:\n                outputs.append(out)\n            else:\n                outputs.append('[' + out + ']')\n        else:\n            outputs.append(str(arg))\n    return tuple(outputs)\n\n\nimport re\n\nclass BColors:\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\n    WHITE = '\\033[37m'\n    YELLOW = '\\033[33m'\n    GREEN = '\\033[32m'\n    BLUE = '\\033[34m'\n    CYAN = '\\033[36m'\n    RED = '\\033[31m'\n    MAGENTA = '\\033[35m'\n    BLACK = '\\033[30m'\n    BHEADER = BOLD + '\\033[95m'\n    BOKBLUE = BOLD + '\\033[94m'\n    BOKGREEN = BOLD + '\\033[92m'\n    BWARNING = BOLD + '\\033[93m'\n    BFAIL = BOLD + '\\033[91m'\n    BUNDERLINE = BOLD + '\\033[4m'\n    BWHITE = BOLD + '\\033[37m'\n    BYELLOW = BOLD + '\\033[33m'\n    BGREEN = BOLD + '\\033[32m'\n    BBLUE = BOLD + '\\033[34m'\n    BCYAN = BOLD + '\\033[36m'\n    BRED = BOLD + '\\033[31m'\n    BMAGENTA = BOLD + '\\033[35m'\n    BBLACK = BOLD + '\\033[30m'\n\n    @staticmethod\n    def cleared(s):\n        return re.sub(r\"\\033\\[[0-9][0-9]?m\", \"\", s)\n\n\ndef red(message):\n    return BColors.RED + str(message) + BColors.ENDC\n\ndef b_red(message):\n    return BColors.BRED + str(message) + BColors.ENDC\n\ndef blue(message):\n    return BColors.BLUE + str(message) + BColors.ENDC\n\ndef b_yellow(message):\n    return BColors.BYELLOW + str(message) + BColors.ENDC\n\ndef green(message):\n    return BColors.GREEN + str(message) + BColors.ENDC\n\ndef b_green(message):\n    return BColors.BGREEN + str(message) + BColors.ENDC\n\n#-----------------------------------------------------------------------------------------------------------#\n\ndef print_args(args, path=None):\n    if path:\n        output_file = open(path, 'w')\n    logger = logging.getLogger(__name__)\n    logger.info(\"Arguments:\")\n    args.command = ' '.join(sys.argv)\n    items = vars(args)\n    for key in sorted(items.keys(), key=lambda s: s.lower()):\n        value = items[key]\n        if not value:\n            value = \"None\"\n        logger.info(\"  \" + key + \": \" + str(items[key]))\n        if path is not None:\n            output_file.write(\"  \" + key + \": \" + str(items[key]) + \"\\n\")\n    if path:\n        output_file.close()\n    del args.command\n\n\ndef get_args(args):\n    items = vars(args)\n    output_string = ''\n    for key in sorted(items.keys(), key=lambda s: s.lower()):\n        value = items[key]\n        if not value:\n            value = \"None\"\n        output_string += \"  \" + key + \": \" + str(items[key] + \"\\n\")\n    return output_string","4e70443b":"import numpy as np\n\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import initializers, regularizers, constraints\nfrom tensorflow.keras.layers import Conv1D, Layer\n\n\nclass Attention(Layer):\n    def __init__(self, \n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        \"\"\"\n        Keras Layer that implements an Content Attention mechanism.\n        Supports Masking.\n        \"\"\"\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n      \n        self.W = self.add_weight(shape=(input_shape[-1], ),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        if self.bias:\n            self.b = self.add_weight(shape=(1,),\n                                     initializer='zeros',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n        self.built = True\n\n    def compute_mask(self, input_tensor, mask=None):\n        return None\n\n    def call(self, input_tensor, mask=None):\n        x = input_tensor\n        query = self.W\n\n        query = K.expand_dims(query, axis=-2)\n        eij = K.sum(x*query, axis=-1)\n\n        if self.bias:\n            eij += self.b\n\n        a = K.exp(eij)\n        a_sigmoid = K.sigmoid(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n            a_sigmoid *= K.cast(mask, K.floatx())\n\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        return [a, a_sigmoid]\n\n    def compute_output_shape(self, input_shape):\n        return [(input_shape[0], input_shape[1]),\n                (input_shape[0], input_shape[1])]\n\n\nclass Self_attention(Layer):\n    def __init__(self, use_opinion,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        \"\"\"\n        Keras Layer that implements an Content Attention mechanism.\n        Supports Masking.\n        \"\"\"\n        self.use_opinion = use_opinion\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        super(Self_attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        input_dim = input_shape[0][-1]\n        self.steps = input_shape[0][-2]\n     \n        self.W = self.add_weight(shape=(input_dim, input_dim),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        if self.bias:\n            self.b = self.add_weight(shape=(input_dim,),\n                                     initializer='zeros',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, x, mask):\n        return mask\n\n    def call(self, input_tensor, mask):\n        x = input_tensor[0]\n        gold_opinion = input_tensor[1]\n        predict_opinion = input_tensor[2]\n        gold_prob = input_tensor[3]\n        mask = mask[0]\n        assert mask is not None\n\n        x_tran = K.dot(x, self.W)\n        if self.bias:\n            x_tran += self.b \n\n        x_transpose = K.permute_dimensions(x, (0,2,1))\n        weights = K.batch_dot(x_tran, x_transpose)\n     \n        location = np.abs(np.tile(np.array(range(self.steps)), (self.steps,1)) - np.array(range(self.steps)).reshape(self.steps,1))\n        loc_weights = 1.0 \/ (location+K.epsilon())\n        loc_weights *= K.cast((location!=0), K.floatx())\n        weights *= loc_weights\n\n        if self.use_opinion:\n            gold_opinion_ = gold_opinion[:,:,1]+gold_opinion[:,:,2]\n            predict_opinion_ = predict_opinion[:,:,3]+predict_opinion[:,:,4]\n            # gold_prob is either 0 or 1 \n            opinion_weights = gold_prob*gold_opinion_ + (1.-gold_prob)*predict_opinion_\n            opinion_weights = K.expand_dims(opinion_weights, axis=-2)\n            weights *= opinion_weights\n\n        weights = K.tanh(weights)\n        weights = K.exp(weights)\n        weights *= (np.eye(self.steps)==0)\n\n        if mask is not None:\n            mask  = K.expand_dims(mask, axis=-2)\n            mask = K.repeat_elements(mask, self.steps, axis=1)\n            weights *= K.cast(mask, K.floatx())\n\n        weights \/= K.cast(K.sum(weights, axis=-1, keepdims=True) + K.epsilon(), K.floatx())\n\n        output = K.batch_dot(weights, x)\n        return output\n\n\nclass WeightedSum(Layer):\n    def __init__(self, **kwargs):\n        self.supports_masking = True\n        super(WeightedSum, self).__init__(**kwargs)\n\n    def call(self, input_tensor, mask=None):\n        assert type(input_tensor) == list\n        assert type(mask) == list\n\n        x = input_tensor[0]\n        a = input_tensor[1]\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0][0], input_shape[0][-1])\n\n    def compute_mask(self, x, mask=None):\n        return None\n\n\nclass Conv1DWithMasking(Conv1D):\n    def __init__(self, **kwargs):\n        self.supports_masking = True\n        super(Conv1DWithMasking, self).__init__(**kwargs)\n    \n    def compute_mask(self, x, mask):\n        return mask\n\n\nclass Remove_domain_emb(Layer):\n    def __init__(self, **kwargs):\n        self.supports_masking = True\n        super(Remove_domain_emb, self).__init__(**kwargs)\n\n    def call(self, x, mask=None):\n        mask_ = np.ones((400,))\n        mask_[300:] = 0\n        embs = x * mask_\n        return embs\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n    def compute_mask(self, x, mask):\n        return mask","90b64cb8":"import logging\nimport numpy as np\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Embedding, Input, Concatenate, Lambda\nfrom tensorflow.keras.models import Model, Sequential\n\n\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s %(levelname)s %(message)s')\nlogger = logging.getLogger(__name__)\n\n\n# Custom CNN kernel initializer\n# Use the initialization from Kim et al. (2014) for CNN kernel\ndef my_init(shape, dtype=K.floatx()):\n    return 0.01 * np.random.standard_normal(size=shape)\n\n\ndef create_model(args, vocab, nb_class, overall_maxlen, doc_maxlen_1, doc_maxlen_2):\n\n    # Funtion that initializes word embeddings \n    def init_emb(emb_matrix, vocab, emb_file_gen, emb_file_domain):\n\n        print('Loading pretrained general-purpose word embeddings ...')\n        counter_gen = 0.\n        pretrained_emb = open(emb_file_gen)\n        for line in pretrained_emb:\n            tokens = line.split()\n            if len(tokens) != 301:\n                continue\n            word = tokens[0]\n            vec = tokens[1:]\n            try:\n                emb_matrix[0][vocab[word]][:300] = vec\n                counter_gen += 1\n            except KeyError:\n                pass\n\n        if args.use_domain_emb:\n            print('Loading pretrained domain word embeddings ...')\n            counter_domain = 0.\n            pretrained_emb = open(emb_file_domain)\n            for line in pretrained_emb:\n                tokens = line.split()\n                if len(tokens) != 101:\n                    continue\n                word = tokens[0]\n                vec = tokens[1:]\n                try:\n                    emb_matrix[0][vocab[word]][300:] = vec\n                    counter_domain += 1\n                except KeyError:\n                    pass\n\n        pretrained_emb.close()\n        logger.info('%i\/%i word vectors initialized by general embeddings (hit rate: %.2f%%)' % (counter_gen, len(vocab), 100*counter_gen\/len(vocab)))\n        \n        if args.use_domain_emb:\n            logger.info('%i\/%i word vectors initialized by domain embeddings (hit rate: %.2f%%)' % (counter_domain, len(vocab), 100*counter_domain\/len(vocab)))\n\n        return emb_matrix\n\n\n    # Build model\n    logger.info('Building model ...')\n    print('Building model ...\\n\\n')\n\n    vocab_size = len(vocab)\n\n    ###################################\n    # Inputs \n    ###################################\n    print('Input layer')\n    # sequence of token indices for aspect-level data\n    sentence_input = Input(shape=(overall_maxlen,), dtype='int32', name='sentence_input')\n    # gold opinion label for aspect-level data. \n    op_label_input = Input(shape=(overall_maxlen, 3), dtype=K.floatx(), name='op_label_input')\n    # probability of sending gold opinion labels at opinion transmission step\n    p_gold_op = Input(shape=(overall_maxlen,), dtype=K.floatx(), name='p_gold_op') \n\n    if args.use_doc:\n        # doc_input_1 denotes the data for sentiment classification\n        # doc_input_2 denotes the data for domain classification\n        doc_input_1 = Input(shape=(doc_maxlen_1,), dtype='int32', name='doc_input_1')\n        doc_input_2 = Input(shape=(doc_maxlen_2,), dtype='int32', name='doc_input_2')\n\n\n    #########################################\n    # Shared word embedding layer \n    #########################################\n    print('Word embedding layer')\n    word_emb = Embedding(vocab_size, args.emb_dim, mask_zero=True, name='word_emb')\n\n    # aspect-level inputs\n    word_embeddings = word_emb(sentence_input) \n    sentence_output = word_embeddings\n\n    # doc-level inputs\n    if args.use_doc:\n        doc_output_1 = word_emb(doc_input_1)\n        # we only use general embedding for domain classification\n        doc_output_2 = word_emb(doc_input_2)\n        if args.use_domain_emb:\n            # mask out the domain embeddings\n            doc_output_2 = Remove_domain_emb()(doc_output_2)\n\n\n    ######################################\n    # Shared CNN layers\n    ######################################\n    for i in range(args.shared_layers):\n        print('Shared CNN layer %s' % i)\n        sentence_output = Dropout(args.dropout_prob)(sentence_output)\n        if args.use_doc:\n            doc_output_1 = Dropout(args.dropout_prob)(doc_output_1)\n            doc_output_2 = Dropout(args.dropout_prob)(doc_output_2)\n\n        if i == 0:\n            conv_1 = Conv1DWithMasking(filters=args.cnn_dim\/2, kernel_size=3, \\\n              activation='relu', padding='same', kernel_initializer=my_init, name='cnn_0_1')\n            conv_2 = Conv1DWithMasking(filters=args.cnn_dim\/2, kernel_size=5, \\\n              activation='relu', padding='same', kernel_initializer=my_init, name='cnn_0_2')\n\n            sentence_output_1 = conv_1(sentence_output)\n            sentence_output_2 = conv_2(sentence_output)\n            sentence_output = Concatenate()([sentence_output_1, sentence_output_2])\n\n            if args.use_doc:\n                doc_output_1_1 = conv_1(doc_output_1)\n                doc_output_1_2 = conv_2(doc_output_1)\n                doc_output_1 = Concatenate()([doc_output_1_1, doc_output_1_2])\n\n                doc_output_2_1 = conv_1(doc_output_2)\n                doc_output_2_2 = conv_2(doc_output_2)\n                doc_output_2 = Concatenate()([doc_output_2_1, doc_output_2_2])\n\n        else:\n            conv = Conv1DWithMasking(filters=args.cnn_dim, kernel_size=5, \\\n              activation='relu', padding='same', kernel_initializer=my_init, name='cnn_%s'%i)\n\n            sentence_output = conv(sentence_output)\n\n            if args.use_doc:\n                doc_output_1 = conv(doc_output_1)\n                doc_output_2 = conv(doc_output_2)\n\n        word_embeddings = Concatenate()([word_embeddings, sentence_output])\n\n    init_shared_features = sentence_output\n\n\n    #######################################\n    # Define task-specific layers\n    #######################################\n\n    # AE specific layers\n    aspect_cnn = Sequential()\n    for a in range(args.aspect_layers):\n        print('Aspect extraction layer %s' % a)\n        aspect_cnn.add(Dropout(args.dropout_prob))\n        aspect_cnn.add(Conv1DWithMasking(filters=args.cnn_dim, kernel_size=5, \\\n                  activation='relu', padding='same', kernel_initializer=my_init, name='aspect_cnn_%s'%a))\n    aspect_dense = Dense(nb_class, activation='softmax', name='aspect_dense')\n\n    # AS specific layers\n    sentiment_cnn = Sequential()\n    for b in range(args.senti_layers):\n        print('Sentiment classification layer %s' % b)\n        sentiment_cnn.add(Dropout(args.dropout_prob))\n        sentiment_cnn.add(Conv1DWithMasking(filters=args.cnn_dim, kernel_size=5, \\\n                  activation='relu', padding='same', kernel_initializer=my_init, name='sentiment_cnn_%s'%b))\n\n    sentiment_att = Self_attention(args.use_opinion, name='sentiment_att')\n    sentiment_dense = Dense(3, activation='softmax', name='sentiment_dense')\n\n    if args.use_doc:\n        # DS specific layers\n        doc_senti_cnn = Sequential()\n        for c in range(args.doc_senti_layers):\n            print('Document-level sentiment layers %s' % c)\n            doc_senti_cnn.add(Dropout(args.dropout_prob))\n            doc_senti_cnn.add(Conv1DWithMasking(filters=args.cnn_dim, kernel_size=5, \\\n                      activation='relu', padding='same', kernel_initializer=my_init, name='doc_sentiment_cnn_%s'%c))\n\n        doc_senti_att = Attention(name='doc_senti_att')\n        doc_senti_dense = Dense(3, name='doc_senti_dense')\n        # The reason not to use the default softmax is that it reports errors when input_dims=2 due to \n        # compatibility issues between the tf and keras versions used.\n        softmax = Lambda(lambda x: K.softmax(x), name='doc_senti_softmax')\n\n        # DD specific layers\n        doc_domain_cnn = Sequential()\n        for d in range(args.doc_domain_layers):\n            print('Document-level domain layers %s' % d)\n            doc_domain_cnn.add(Dropout(args.dropout_prob))\n            doc_domain_cnn.add(Conv1DWithMasking(filters=args.cnn_dim, kernel_size=5, \\\n                      activation='relu', padding='same', kernel_initializer=my_init, name='doc_domain_cnn_%s'%d))\n\n        doc_domain_att = Attention(name='doc_domain_att')\n        doc_domain_dense = Dense(1, activation='sigmoid', name='doc_domain_dense')\n\n    # re-encoding layer\n    enc = Dense(args.cnn_dim, activation='relu', name='enc')\n\n\n    ####################################################\n    # aspect-level operations involving message passing\n    ####################################################\n    for i in range(args.interactions+1):\n        print('Interaction number ', i)\n        aspect_output = sentence_output\n        sentiment_output = sentence_output\n\n        # note that the aspet-level data will also go through the doc-level models\n        doc_senti_output = sentence_output\n        doc_domain_output = sentence_output\n\n        ### AE ###\n        if args.aspect_layers > 0:\n            aspect_output = aspect_cnn(aspect_output)\n\n        # concate word embeddings and task-specific output for prediction\n        aspect_output = Concatenate()([word_embeddings, aspect_output])\n        aspect_output = Dropout(args.dropout_prob)(aspect_output)\n        aspect_probs = aspect_dense(aspect_output)\n\n        ### AS ###\n        if args.senti_layers > 0:\n            sentiment_output = sentiment_cnn(sentiment_output)\n\n        sentiment_output = sentiment_att([sentiment_output, op_label_input, aspect_probs, p_gold_op])\n        sentiment_output = Concatenate()([init_shared_features, sentiment_output])\n        sentiment_output = Dropout(args.dropout_prob)(sentiment_output)\n        sentiment_probs = sentiment_dense(sentiment_output)\n        \n        if args.use_doc:\n            ### DS ###\n            if args.doc_senti_layers > 0:\n                doc_senti_output = doc_senti_cnn(doc_senti_output)\n\n            # output attention weights with two activation functions\n            senti_att_weights_softmax, senti_att_weights_sigmoid = doc_senti_att(doc_senti_output)\n\n            # reshape the sigmoid attention weights, will be used in message passing\n            senti_weights = Lambda(lambda x: K.expand_dims(x, axis=-1))(senti_att_weights_sigmoid)\n\n            doc_senti_output = WeightedSum()([doc_senti_output, senti_att_weights_softmax])\n            doc_senti_output = Dropout(args.dropout_prob)(doc_senti_output)\n            doc_senti_output = doc_senti_dense(doc_senti_output)\n            doc_senti_probs = softmax(doc_senti_output)\n\n            # reshape the doc-level sentiment predictions, will be used in message passing\n            doc_senti_probs = Lambda(lambda x: K.expand_dims(x, axis=-2))(doc_senti_probs)\n            doc_senti_probs = Lambda(lambda x: K.repeat_elements(x, overall_maxlen, axis=1))(doc_senti_probs)\n\n            ### DD ###\n            if args.doc_domain_layers > 0:\n                doc_domain_output = doc_domain_cnn(doc_domain_output)\n            domain_att_weights_softmax, domain_att_weights_sigmoid = doc_domain_att(doc_domain_output)\n            domain_weights = Lambda(lambda x: K.expand_dims(x, axis=-1))(domain_att_weights_sigmoid)\n\n            doc_domain_output = WeightedSum()([doc_domain_output, domain_att_weights_softmax])\n            doc_domain_output = Dropout(args.dropout_prob)(doc_domain_output)\n            doc_domain_probs = doc_domain_dense(doc_domain_output)\n            \n            # update sentence_output for the next iteration\n            sentence_output = Concatenate()([sentence_output, aspect_probs, sentiment_probs, \n                                            doc_senti_probs, senti_weights, domain_weights])\n        else:\n            # update sentence_output for the next iteration\n            sentence_output = Concatenate()([sentence_output, aspect_probs, sentiment_probs])\n\n        sentence_output = enc(sentence_output)\n\n    aspect_model = Model(inputs=[sentence_input, op_label_input, p_gold_op], \n                         outputs=[aspect_probs, sentiment_probs], \n                         name='Aspect-Model')\n\n    \n    ####################################################\n    # doc-level operations without message passing\n    ####################################################\n    if args.use_doc:\n        if args.doc_senti_layers > 0:\n            doc_output_1 = doc_senti_cnn(doc_output_1)\n        att_1, _ = doc_senti_att(doc_output_1)\n        doc_output_1 = WeightedSum()([doc_output_1, att_1])\n        doc_output_1 = Dropout(args.dropout_prob)(doc_output_1)\n        doc_output_1 = doc_senti_dense(doc_output_1)\n        doc_prob_1 = softmax(doc_output_1)\n\n        if args.doc_domain_layers > 0:\n            doc_output_2 = doc_domain_cnn(doc_output_2)\n        att_2, _ = doc_domain_att(doc_output_2)\n        doc_output_2 = WeightedSum()([doc_output_2, att_2])\n        doc_output_2 = Dropout(args.dropout_prob)(doc_output_2)\n        doc_prob_2 = doc_domain_dense(doc_output_2)\n\n        doc_model = Model(inputs=[doc_input_1, doc_input_2], \n                          outputs=[doc_prob_1, doc_prob_2],\n                          name='Doc-Model')\n    else:\n        doc_model = None\n\n\n    ####################################################\n    # initialize word embeddings\n    ####################################################\n    logger.info('Initializing lookup table')\n\n    # Load pre-trained word vectors.\n    # To save the loading time, here we load from the extracted subsets of the original embeddings, \n    # which only contains the embeddings of words in the vocab. \n    if args.use_doc:\n        emb_path_gen = '..\/input\/imn-e2e-absa\/glove\/%s_.txt' % (args.domain)\n        emb_path_domain = '..\/input\/imn-e2e-absa\/domain_specific_emb\/%s_.txt' % (args.domain)\n    else:\n        emb_path_gen = '..\/input\/imn-e2e-absa\/glove\/%s.txt' % (args.domain)\n        emb_path_domain = '..\/input\/imn-e2e-absa\/domain_specific_emb\/%s.txt' % (args.domain)\n\n    # Load pre-trained word vectors from the orginal large files\n    # If you are loading from ssd, the process would only take 1-2 mins\n    # If you are loading from hhd, the process would take a few hours at first try, \n    # and would take 1-2 mins in subsequent repeating runs (due to cache performance). \n\n    aspect_model.get_layer('word_emb').set_weights(\n        init_emb(aspect_model.get_layer('word_emb').get_weights(), vocab, emb_path_gen, emb_path_domain)\n    )\n    logger.info('Done')\n    return aspect_model, doc_model","20e89ac2":"import tensorflow.keras.optimizers as opt\n\n\ndef get_optimizer(algorithm):\n\n    clipvalue = 0\n    clipnorm = 100\n\n    if algorithm == 'rmsprop':\n        optimizer = opt.RMSprop(lr=0.0001, rho=0.9, epsilon=1e-06, clipnorm=clipnorm, clipvalue=clipvalue)\n    elif algorithm == 'sgd':\n        optimizer = opt.SGD(lr=0.1, momentum=0.0, decay=0.0, nesterov=False, clipnorm=clipnorm, clipvalue=clipvalue)\n    elif algorithm == 'adagrad':\n        optimizer = opt.Adagrad(lr=0.01, epsilon=1e-06, clipnorm=clipnorm, clipvalue=clipvalue)\n    elif algorithm == 'adadelta':\n        optimizer = opt.Adadelta(lr=1.0, rho=0.95, epsilon=1e-06, clipnorm=clipnorm, clipvalue=clipvalue)\n    elif algorithm == 'adam':\n        optimizer = opt.Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, clipnorm=clipnorm, clipvalue=clipvalue)\n    elif algorithm == 'adamax':\n        optimizer = opt.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, clipnorm=clipnorm, clipvalue=clipvalue)\n    return optimizer","3a02bd15":"import numpy as np \n\n\ndef convert_to_list(y_aspect, y_sentiment, mask):\n    y_aspect_list = []\n    y_sentiment_list = []\n    for seq_aspect, seq_sentiment, seq_mask in zip(y_aspect, y_sentiment, mask):\n        l_a = []\n        l_s = []\n        for label_dist_a, label_dist_s, m in zip(seq_aspect, seq_sentiment, seq_mask):\n            if m == 0:\n                break\n            else:\n                l_a.append(np.argmax(label_dist_a))\n                ### all entries are zeros means that it is a background word or word with conflict sentiment\n                ### which are not counted for training AS\n                ### also when evaluating, we do not count conflict examples\n                if not np.any(label_dist_s):\n                    l_s.append(0)\n                else:\n                    l_s.append(np.argmax(label_dist_s)+1)\n        y_aspect_list.append(l_a)\n        y_sentiment_list.append(l_s)\n    return y_aspect_list, y_sentiment_list\n\n\ndef score(true_aspect, predict_aspect, true_sentiment, predict_sentiment, train_op):\n    if train_op:\n        begin = 3\n        inside = 4\n    else:\n        begin = 1\n        inside = 2\n\n        # predicted sentiment distribution for aspect terms that are correctly extracted\n        pred_count = {'pos': 0, 'neg': 0, 'neu': 0}\n        # gold sentiment distribution for aspect terms that are correctly extracted\n        rel_count = {'pos': 0, 'neg': 0, 'neu': 0}\n        # sentiment distribution for terms that get both span and sentiment predicted correctly\n        correct_count = {'pos': 0, 'neg': 0, 'neu': 0}\n        # sentiment distribution in original data\n        total_count = {'pos': 0, 'neg': 0, 'neu': 0}\n\n        polarity_map = {1: 'pos', 2: 'neg', 3: 'neu'}\n\n        # count of predicted conflict aspect term\n        predicted_conf = 0\n\n    correct, predicted, relevant = 0, 0, 0\n\n    for i in range(len(true_aspect)):\n        true_seq = true_aspect[i]\n        predict = predict_aspect[i]\n        \n        for num in range(len(true_seq)):\n            if true_seq[num] == begin:\n                relevant += 1\n                if not train_op:\n                    if true_sentiment[i][num]!=0:\n                        total_count[polarity_map[true_sentiment[i][num]]]+=1\n                     \n                if predict[num] == begin:\n                    match = True \n                    for j in range(num+1, len(true_seq)):\n                        if true_seq[j] == inside and predict[j] == inside:\n                            continue\n                        elif true_seq[j] != inside  and predict[j] != inside:\n                            break\n                        else:\n                            match = False\n                            break\n\n                    if match:\n                        correct += 1\n                        if not train_op:\n                            # do not count conflict examples\n                            if true_sentiment[i][num]!=0:\n                                rel_count[polarity_map[true_sentiment[i][num]]]+=1\n                                pred_count[polarity_map[predict_sentiment[i][num]]]+=1\n                                if true_sentiment[i][num] == predict_sentiment[i][num]:\n                                    correct_count[polarity_map[true_sentiment[i][num]]]+=1\n\n                            else:\n                                predicted_conf += 1\n        for pred in predict:\n            if pred == begin:\n                predicted += 1\n\n    p_aspect = correct \/ (predicted+1e-6)\n    r_aspect = correct \/ (relevant+1e-6)\n    \n    # F1 score for aspect (opinion) extraction\n    f_aspect = 2*p_aspect*r_aspect \/ (p_aspect+r_aspect+1e-6)\n\n    acc_s, f_s, f_absa = 0, 0, 0\n\n    if not train_op:\n        num_correct_overall = correct_count['pos'] + correct_count['neg'] + correct_count['neu']\n        num_correct_aspect = rel_count['pos'] + rel_count['neg'] + rel_count['neu']\n        num_total = total_count['pos'] + total_count['neg'] + total_count['neu']\n\n        acc_s = num_correct_overall\/(num_correct_aspect+1e-6)\n       \n        p_pos = correct_count['pos'] \/ (pred_count['pos']+1e-6)\n        r_pos = correct_count['pos'] \/ (rel_count['pos']+1e-6)\n        \n        p_neg = correct_count['neg'] \/ (pred_count['neg']+1e-6)\n        r_neg = correct_count['neg'] \/ (rel_count['neg']+1e-6)\n\n        p_neu = correct_count['neu'] \/ (pred_count['neu']+1e-6)\n        r_neu= correct_count['neu'] \/ (rel_count['neu']+1e-6)\n\n        pr_s = (p_pos+p_neg+p_neu) \/ 3.0\n        re_s = (r_pos+r_neg+r_neu) \/ 3.0\n        \n        # F1 score for AS only\n        f_s = 2*pr_s*re_s\/(pr_s+re_s)\n       \n        precision_absa = num_correct_overall \/ (predicted-predicted_conf+1e-6)\n        recall_absa = num_correct_overall \/ (num_total+1e-6)\n        \n        # F1 score of the end-to-end task\n        f_absa = 2*precision_absa*recall_absa\/(precision_absa+recall_absa+1e-6)\n\n    return f_aspect, acc_s, f_s, f_absa\n\n\ndef get_metric(y_true_aspect, y_predict_aspect, y_true_sentiment, y_predict_sentiment, mask, train_op):\n    f_a, f_o = 0, 0\n    true_aspect, true_sentiment = convert_to_list(y_true_aspect, y_true_sentiment, mask)\n    predict_aspect, predict_sentiment = convert_to_list(y_predict_aspect, y_predict_sentiment, mask)\n\n    f_aspect, acc_s, f_s, f_absa = score(true_aspect, predict_aspect, true_sentiment, predict_sentiment, 0)\n\n    if train_op:\n        f_opinion, _, _, _ = score(true_aspect, predict_aspect, true_sentiment, predict_sentiment, 1)\n\n    return f_aspect, f_opinion, acc_s, f_s, f_absa","e68278fc":"import argparse\nimport codecs\nimport logging\nimport copy\nfrom time import time\nfrom tqdm import tqdm\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.utils import plot_model, to_categorical\nfrom tensorflow.keras.models import load_model\n\n\nlogging.basicConfig(filename='out.log',\n                    level=logging.INFO,\n                    format='%(asctime)s %(levelname)s %(message)s')\nlogger = logging.getLogger()\n\n\n#############################\n# Parse arguments\n#############################\n\nparser = argparse.ArgumentParser()\n\n# argument related to datasets and data preprocessing\nparser.add_argument(\"--domain\", dest=\"domain\", type=str, metavar='<str>', default='res', help=\"domain of the corpus {res, lt, res_15}\")\nparser.add_argument(\"-v\", \"--vocab-size\", dest=\"vocab_size\", type=int, metavar='<int>', default=50_000, help=\"Vocab size. '0' means no limit (default=20000)\")\n\n# hyper-parameters related to network training\nparser.add_argument(\"-a\", \"--algorithm\", dest=\"algorithm\", type=str, default='sgd', metavar='<str>', help=\"Optimization algorithm (rmsprop|sgd|adagrad|adadelta|adam|adamax) (default=adam)\")\nparser.add_argument(\"-b\", \"--batch-size\", dest=\"batch_size\", type=int, default=64, metavar='<int>', help=\"Batch size (default=16)\")\nparser.add_argument(\"--epochs\", dest=\"epochs\", type=int, default=10, metavar='<int>', help=\"Number of epochs (default=50)\")\nparser.add_argument(\"--val-ratio\", dest=\"val_ratio\", type=float, default=0.3, metavar='<float>', help=\"The percentage of training data used for validation\")\nparser.add_argument(\"--pre-epochs\", dest=\"pre_epochs\", type=int, default=5, metavar='<int>', help=\"Number of pretrain document-level epochs (default=5)\")\nparser.add_argument(\"-mr\", dest=\"mr\", type=int, metavar='<int>', default=2, help=\"#aspect-level epochs : #document-level epochs = mr:1\")\n\n# hyper-parameters related to network structure\nparser.add_argument(\"-e\", \"--embdim\", dest=\"emb_dim\", type=int, metavar='<int>', default=400, help=\"Embeddings dimension (default=dim_general_emb + dim_domain_emb = 400)\")\nparser.add_argument(\"-c\", \"--cnndim\", dest=\"cnn_dim\", type=int, metavar='<int>', default=300, help=\"CNN output dimension. '0' means no CNN layer (default=300)\")\nparser.add_argument(\"--dropout\", dest=\"dropout_prob\", type=float, metavar='<float>', default=0.5, help=\"The dropout probability. (default=0.5)\")\n\nparser.add_argument(\"--use-doc\", dest=\"use_doc\", type=bool, default=True, metavar='<bool>', help=\"whether to exploit knowledge from document-level data\")\nparser.add_argument(\"--train-op\", dest=\"train_op\", type=bool, default=True, metavar='<bool>', help=\"whether to extract opinion terms\")\nparser.add_argument(\"--use-opinion\", dest=\"use_opinion\", type=bool, default=True, metavar='<bool>', help=\"whether to perform opinion transmission\")\nparser.add_argument(\"--use-domain-emb\", dest=\"use_domain_emb\", type=bool, default=True, metavar='<bool>', help=\"whether to use domain-specific embeddings\")\n\nparser.add_argument(\"--shared-layers\", dest=\"shared_layers\", type=int, default=4, metavar='<int>', help=\"The number of CNN layers in the shared network\")\nparser.add_argument(\"--aspect-layers\", dest=\"aspect_layers\", type=int, default=1, metavar='<int>', help=\"The number of CNN layers for extracting aspect features\")\nparser.add_argument(\"--senti-layers\", dest=\"senti_layers\", type=int, default=0, metavar='<int>', help=\"The number of CNN layers for extracting aspect-level sentiment features\")\nparser.add_argument(\"--doc-senti-layers\", dest=\"doc_senti_layers\", type=int, default=2, metavar='<int>', help=\"The number of CNN layers for extracting document-level sentiment features\")\nparser.add_argument(\"--doc-domain-layers\", dest=\"doc_domain_layers\", type=int, default=0, metavar='<int>', help=\"The number of CNN layers for extracting document domain features\")\nparser.add_argument(\"--interactions\", dest=\"interactions\", type=int, default=3, metavar='<int>', help=\"The number of interactions\")\n\n# random seed that affects data splits and parameter intializations\nparser.add_argument(\"--seed\", dest=\"seed\", type=int, metavar='<int>', default=41020, help=\"Random seed (default=41020)\")\n\n# args = parser.parse_args()\nargs, unknown = parser.parse_known_args()\nprint_args(args)\n\nnp.random.seed(args.seed)\ntf.random.set_seed(args.seed)\n\nif args.use_domain_emb:\n    args.emb_dim = 400\nelse:\n    args.emb_dim = 300\n\n\n#############################\n# Prepare data\n#############################\n\ndef convert_label(label, nb_class, maxlen):\n    label_ = np.zeros((len(label), maxlen, nb_class))\n    mask = np.zeros((len(label), maxlen))\n\n    for i in range(len(label)):\n        for j in range(len(label[i])):\n            l = label[i][j]\n            label_[i][j][l] = 1\n            mask[i][j] = 1\n    return label_, mask\n\n\ndef convert_label_sentiment(label, nb_class, maxlen):\n    label_ = np.zeros((len(label), maxlen, nb_class))\n    for i in range(len(label)):\n        for j in range(len(label[i])):\n            l = label[i][j]\n            # for background word and word with conflict label, set its sentiment label to [0,0,0]\n            # such that we don't consider them in the sentiment classification loss\n            if l in [1,2,3]:\n                label_[i][j][l-1] = 1\n    return label_\n\n\ndef shuffle(array_list):\n    len_ = len(array_list[0])\n    for x in array_list:\n        assert len(x) == len_\n    p = np.random.permutation(len_)\n    return [x[p] for x in array_list]\n\n\ndef batch_generator(array_list, batch_size):\n    batch_count = 0\n    n_batch = int(len(array_list[0]) \/\/ batch_size)\n    array_list = shuffle(array_list)\n\n    while True:\n        if batch_count == n_batch:\n            array_list = shuffle(array_list)\n            batch_count = 0\n\n        batch_list = [\n            x[batch_count*batch_size:(batch_count+1)*batch_size] for x in array_list\n        ]\n        batch_count += 1\n        yield batch_list\n\n\ndef split_dev(array_list, ratio=0.2):\n    validation_size = int(len(array_list[0]) * ratio)\n    array_list = shuffle(array_list)\n    dev_sets = [x[:validation_size] for x in array_list]\n    train_sets = [x[validation_size:] for x in array_list]\n    return train_sets, dev_sets\n\n\n# load both aspect-level and document-level data\ndata_bulk = prepare_data(args.domain, args.vocab_size, args.use_doc)\ntrain_x, train_label_target, train_label_opinion, train_label_polarity = data_bulk[:4]\ntest_x, test_label_target, test_label_opinion, test_label_polarity = data_bulk[4:8]\nvocab, overall_maxlen = data_bulk[8:10]\ndoc_res_x, doc_res_y, doc_lt_x, doc_lt_y, doc_res_maxlen, doc_lt_maxlen = data_bulk[10:]\n\n\n# print aspect-level data statistics\nprint('\\n------------------ Data statistics -----------------')\ncount_target_train, count_polarity_train = get_statistics(train_label_target, train_label_polarity)\ncount_opinion_train = get_statistics(train_label_opinion)\nprint(f'Train:\\n\\t#sentences: {len(train_x)}\\n\\t#targets: {count_target_train}\\n\\t#opinion_terms: {count_opinion_train}\\n\\t#target_polarity_count: {count_polarity_train}')\ncount_target_test, count_polarity_test = get_statistics(test_label_target, test_label_polarity)\ncount_opinion_test = get_statistics(test_label_opinion)\nprint(f'Test:\\n\\t#sentences: {len(test_x)}\\n\\t#targets: {count_target_test}\\n\\t#opinion_terms: {count_opinion_test}\\n\\t#target_polarity_count: {count_polarity_test}')\n\n\n\n###################################\n# prepare aspect-level data\n###################################\n\n# combine the information of train_label_target and train_label_opinion into one sequence tags \n# denoted as train_y_aspect if train_op = True\n# 1, 2 denotes the begining of and inside of an aspect term;\n# 3, 4 denotes the begining of and inside of an opinion term;\n# 0 denotes the background tokens.\ntrain_y_aspect = copy.deepcopy(train_label_target)\ntest_y_aspect = copy.deepcopy(test_label_target)\n\nif args.train_op: \n    nb_class = 5\n    for i in range(len(train_label_target)):\n        for j in range(len(train_label_target[i])):\n            if train_label_target[i][j] == 0 and train_label_opinion[i][j] > 0:\n                train_y_aspect[i][j] = train_label_opinion[i][j] + 2 \n    for i in range(len(test_label_target)):\n        for j in range(len(test_label_target[i])):\n            if test_label_target[i][j] ==0 and test_label_opinion[i][j] > 0:\n                test_y_aspect[i][j] = test_label_opinion[i][j] + 2\nelse:\n    nb_class = 3\n\n\n# Pad sequences to the same length for mini-batch processing\ntrain_x = sequence.pad_sequences(train_x, maxlen=overall_maxlen, padding='post', truncating='post')\ntest_x = sequence.pad_sequences(test_x, maxlen=overall_maxlen, padding='post', truncating='post')\n\n# convert the labels to one-hot encodings\ntrain_y_aspect, train_y_mask = convert_label(train_y_aspect, nb_class, overall_maxlen)\ntest_y_aspect, test_y_mask = convert_label(test_y_aspect, nb_class, overall_maxlen)\n\ntrain_y_sentiment = convert_label_sentiment(train_label_polarity, 3, overall_maxlen)\ntest_y_sentiment = convert_label_sentiment(test_label_polarity, 3, overall_maxlen)\n\n# the original opinion labels will only be used for opinion transimission at training phase\ntrain_y_opinion, _ = convert_label(train_label_opinion, 3, overall_maxlen)\ntest_y_opinion, _ = convert_label(test_label_opinion, 3, overall_maxlen)\n\n# split the original training data into train and dev sets\ntrain_bulk, dev_bulk = split_dev([train_x, train_y_aspect, train_y_sentiment, train_y_opinion, train_y_mask], ratio=args.val_ratio)\ntrain_x, train_y_aspect, train_y_sentiment, train_y_opinion, train_y_mask = train_bulk\ndev_x, dev_y_aspect, dev_y_sentiment, dev_y_opinion, dev_y_mask = dev_bulk\n\n\n###################################\n# prepare document-level data\n###################################\n\nif args.use_doc:\n    # doc_x_1, doc_y_1 used for predicting the sentiment label \n    # doc_x_2, doc_y_2 used for predicting the domain label between res and lt domains \n    doc_x_2 = np.concatenate((\n        sequence.pad_sequences(doc_res_x, maxlen=max(doc_res_maxlen, doc_lt_maxlen), padding='post', truncating='post'),\n        sequence.pad_sequences(doc_lt_x, maxlen=max(doc_res_maxlen, doc_lt_maxlen), padding='post', truncating='post'),\n        ))\n    if args.domain in {'res', 'res_15'}:\n        doc_x_1 = sequence.pad_sequences(doc_res_x, maxlen=doc_res_maxlen, padding='post', truncating='post')\n        doc_y_1 = to_categorical(doc_res_y)\n        doc_y_2 = np.concatenate((np.ones((len(doc_res_y), 1)), np.zeros((len(doc_lt_y), 1))))\n        doc_maxlen_1 = doc_res_maxlen\n    else:\n        doc_x_1 = sequence.pad_sequences(doc_lt_x, maxlen=doc_lt_maxlen, padding='post', truncating='post')\n        doc_y_1 = to_categorical(doc_lt_y)\n        doc_y_2 = np.concatenate((np.zeros((len(doc_res_y), 1)), np.ones((len(doc_lt_y), 1))))\n        doc_maxlen_1 = doc_lt_maxlen\n\n    doc_maxlen_2 = max(doc_res_maxlen, doc_lt_maxlen)\n\n    [train_doc_x_1, train_doc_y_1], [dev_doc_x_1, dev_doc_y_1] = split_dev([doc_x_1, doc_y_1], ratio=0.1)\n    [train_doc_x_2, train_doc_y_2], [dev_doc_x_2, dev_doc_y_2] = split_dev([doc_x_2, doc_y_2], ratio=0.05)\n\nelse:\n    doc_maxlen_1, doc_maxlen_2 = None, None\n\n\n###################################\n# Building model\n###################################\naspect_model, doc_model = create_model(args, vocab, nb_class, overall_maxlen, doc_maxlen_1, doc_maxlen_2)\noptimizer = get_optimizer(args.algorithm)\n\n# if args.use_doc and args.interactions > 0:\n#     # fix the document-specific parameters when updating aspect model\n#     aspect_model.get_layer('doc_senti_att').trainable = False\n#     aspect_model.get_layer('doc_senti_dense').trainable = False\n#     aspect_model.get_layer('doc_domain_att').trainable = False\n\naspect_model.compile(optimizer=optimizer, \n                     loss=['categorical_crossentropy', 'categorical_crossentropy'],\n                     loss_weights=[1., 1.])\nplot_model(aspect_model, to_file='aspect_model.png', dpi=128, show_shapes=True, show_layer_names=True)\n\nif args.use_doc:\n#     doc_model.get_layer('doc_senti_att').trainable = True\n#     doc_model.get_layer('doc_senti_dense').trainable = True\n#     doc_model.get_layer('doc_domain_att').trainable = True\n#     doc_model.get_layer('doc_domain_dense').trainable = True\n\n    doc_model.compile(optimizer=optimizer, \n                      loss=['categorical_crossentropy', 'binary_crossentropy'],\n                      loss_weights=[1., 1.],\n                      metrics=['categorical_accuracy', 'accuracy'])\n    plot_model(doc_model, to_file='doc_model.png', dpi=128, show_shapes=True, show_layer_names=True)\n\n\n###################################\n# Training\n###################################\n\n# compute the probability of using gold opinion labels in opinion transmission\n#       To alleviate the problem of unreliable predictions of opinion labels \n#           sent from AE to AS at opinion transmission step \n#           in the early stage of training,\n#           we use gold labels as prediction with probability \n#           that depends on the number of current epoch.\ndef get_prob(epoch_count):\n    prob = 5 \/ (5+np.exp(epoch_count\/5))\n    return prob\n\nprint('--------------------------------------------')\n\n\n########################################\n# pre-train document-level tasks\n########################################\n\nif args.use_doc:\n    gen_doc_1 = batch_generator([train_doc_x_1, train_doc_y_1], batch_size=args.batch_size)\n    gen_doc_2 = batch_generator([train_doc_x_2, train_doc_y_2], batch_size=args.batch_size)\n    batches_per_epoch_doc = int(len(train_doc_x_2) \/\/ args.batch_size)\n\n    for ii in range(args.pre_epochs):\n        t0 = time()\n        loss, loss_sentiment, loss_domain, acc_sentiment, acc_domain = 0., 0., 0., 0., 0.\n\n        for b in tqdm(range(batches_per_epoch_doc)):\n            batch_x_1, batch_y_1 = next(gen_doc_1)\n            batch_x_2, batch_y_2 = next(gen_doc_2)\n\n            loss_, loss_sentiment_, loss_domain_, acc_sentiment_, _, _, acc_domain_ = doc_model.train_on_batch([batch_x_1, batch_x_2], [batch_y_1, batch_y_2])\n\n            loss += loss_ \/ batches_per_epoch_doc\n            loss_sentiment += loss_sentiment_ \/ batches_per_epoch_doc\n            loss_domain +=  loss_domain_ \/ batches_per_epoch_doc\n            acc_sentiment += acc_sentiment_ \/ batches_per_epoch_doc\n            acc_domain += acc_domain_ \/ batches_per_epoch_doc\n\n        tr_time = time() - t0\n        print('Pretrain doc-level model: Epoch %d, train: %is' % (ii+1, tr_time))\n        print(f'[Train]\\n\\tloss: {loss:.4f}\\n\\tloss_sentiment: {loss_sentiment:.4f}\\n\\tloss_domain: {loss_domain:.4f}\\n\\tacc_sentiment: {acc_sentiment:.4f}\\n\\tacc_domain: {acc_domain:.4f}')\n\n        val_loss, val_loss_sentiment, val_loss_domain, \\\n        val_acc_sentiment, _, _, val_acc_domain = doc_model.evaluate(\n            [dev_doc_x_1, dev_doc_x_2], [dev_doc_y_1, dev_doc_y_2], batch_size=args.batch_size, verbose=1)\n        print(f'[Validation]\\n\\tloss: {val_loss:.4f}\\n\\tloss_sentiment: {val_loss_sentiment:.4f}\\n\\tloss_domain: {val_loss_domain:.4f}\\n\\tacc_sentiment: {val_acc_sentiment:.4f}\\n\\tacc_domain: {val_acc_domain:.4f}')\n        model_name = f\"aspect_model-ep={ii+1:03d}\"\n        model_name += f\"-loss={loss:.4f}-val_loss={val_loss:.4f}\"\n        model_name += f\"-loss_sentiment={loss_sentiment:.4f}-val_loss_sentiment={val_loss_sentiment:.4f}\"\n        model_name += f\"-loss_domain={loss_domain:.4f}-val_loss_domain={val_loss_domain:.4f}\"\n        model_name += f\"-acc_sentiment={acc_sentiment:.4f}-val_acc_sentiment={val_acc_sentiment:.4f}\"\n        model_name += f\"-acc_domain={acc_domain:.4f}-val_acc_domain={val_acc_domain:.4f}\"\n        doc_model.save_weights(model_name+\".h5\")\n\n######################################################\n# train aspect model and document model alternatively\n######################################################\n\nbest_dev_metric = 0\nsave_model = False\n\ngen_aspect = batch_generator([train_x, train_y_aspect, train_y_sentiment, train_y_opinion, train_y_mask], batch_size=args.batch_size)\nbatches_per_epoch_aspect = len(train_x) \/\/ args.batch_size\nbatches_per_epoch_aspect = int(batches_per_epoch_aspect)\n\nfor ii in range(args.epochs):\n    t0 = time()\n    loss, loss_aspect, loss_sentiment = 0., 0., 0.\n\n    gold_prob = get_prob(ii)\n    rnd = np.random.uniform()\n    # as epoch increasing, the probability of using gold opinion label descreases.\n    if rnd < gold_prob:\n        gold_prob = np.ones((args.batch_size, overall_maxlen))\n    else:\n        gold_prob = np.zeros((args.batch_size, overall_maxlen))\n\n    for b in tqdm(range(batches_per_epoch_aspect)):\n        batch_x, batch_y_ae, batch_y_as, batch_y_op, batch_mask = next(gen_aspect)\n        loss_, loss_aspect_, loss_sentiment_ = aspect_model.train_on_batch([batch_x, batch_y_op, gold_prob], [batch_y_ae, batch_y_as])\n\n        loss += loss_ \/ batches_per_epoch_aspect\n        loss_aspect += loss_aspect_ \/ batches_per_epoch_aspect\n        loss_sentiment += loss_sentiment_ \/ batches_per_epoch_aspect\n\n        if b % args.mr == 0 and args.use_doc:\n            batch_x_1, batch_y_1 = next(gen_doc_1)\n            batch_x_2, batch_y_2 = next(gen_doc_2)\n            doc_model.train_on_batch([batch_x_1, batch_x_2], [batch_y_1, batch_y_2])\n\n    tr_time = time() - t0\n\n    print('Epoch %d, train: %is' % (ii+1, tr_time))\n    print(f'Train results:\\n\\tloss: {loss}\\n\\tloss_aspect: {loss_aspect}\\n\\tloss_sentiment: {loss_sentiment}')\n\n    y_pred_aspect, y_pred_sentiment = aspect_model.predict([dev_x, dev_y_opinion, np.zeros((len(dev_x), overall_maxlen))])\n\n    f1_aspect, f1_opinion, acc_s, f1_s, f1_absa \\\n         = get_metric(dev_y_aspect, y_pred_aspect, dev_y_sentiment, y_pred_sentiment, dev_y_mask, args.train_op)\n\n    print(f'\\nValidation results:\\n\\tf1_aspect: {f1_aspect:.4f}\\n\\tf1_opinion: {f1_opinion:.4f}\\n\\tacc_s: {acc_s:.4f}\\n\\tf1_s: {f1_s:.4f}\\n\\tf1_absa: {f1_absa:.4f}')   \n\n    if f1_absa > best_dev_metric and ii > args.pre_epochs:\n        best_dev_metric = f1_absa\n        save_model = True\n    else:\n        save_model = False\n\n    y_pred_aspect, y_pred_sentiment = aspect_model.predict([test_x, test_y_opinion, np.zeros((len(test_x), overall_maxlen))])\n\n    f1_aspect, f1_opinion, acc_s, f1_s, f1_absa \\\n         = get_metric(test_y_aspect, y_pred_aspect, test_y_sentiment, y_pred_sentiment, test_y_mask, args.train_op)\n    print(f'\\nTest results:\\n\\tf1_aspect: {f1_aspect:.4f}\\n\\tf1_opinion: {f1_opinion:.4f}\\n\\tacc_s: {acc_s:.4f}\\n\\tf1_s: {f1_s:.4f}\\n\\tf1_absa: {f1_absa:.4f}')   \n\n    # if save_model:\n    #     logger.info('-------------- Save model --------------\\n')\n    aspect_model.save_weights(f\"aspect_model-ep={ii+1:03d}-a_f1={f1_aspect:.4f}-o_f1={f1_opinion:.4f}-s_f1={f1_s:.4f}-absa_f1={f1_absa:.4f}.h5\")    ","460d7ff6":"# **dataset.py**","34a355fe":"# **train.py**","2a21bbcf":"# **evaluation.py**","dd0aefa5":"# **optimizers.py**","34c4b025":"# **customized_layers.py**","1082fefe":"# **utils.py**","e556ba84":"# **model.py**"}}