{"cell_type":{"0509e5b2":"code","74eaf2ce":"code","fdae4d63":"code","4f9207ab":"code","2b2ff47d":"code","ebd6887c":"code","8bcd3d22":"code","843eca59":"code","8dba8a50":"code","794ac634":"code","fd894fd9":"code","1c24028e":"code","4b78ae67":"code","d862cb89":"code","084e0b7c":"code","28417071":"markdown","a65d43b4":"markdown","a7e041c6":"markdown","33acfca4":"markdown","fe149b0d":"markdown","64da9311":"markdown"},"source":{"0509e5b2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom sklearn import preprocessing, linear_model, ensemble, metrics, model_selection, svm, pipeline, naive_bayes\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nimport nltk\nimport spacy\nimport textblob\nfrom nltk import word_tokenize          \nfrom nltk.stem import WordNetLemmatizer","74eaf2ce":"path = '\\\\kaggle\\\\input\\\\janata-hacknlp\\\\'","fdae4d63":"# Read Data\ntrain = pd.read_csv('\/kaggle\/input\/janata-hacknlp\/train_E52nqFa\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/janata-hacknlp\/test_BppAoe0\/test.csv')\nmeta = pd.read_csv('\/kaggle\/input\/janata-hacknlp\/train_E52nqFa\/game_overview.csv')\n\n# Rename Certain columns\ntrain = train.rename({'year': 'year_no', 'title': 'title_no'}, axis = 1)\ntest = test.rename({'year': 'year_no', 'title': 'title_no'}, axis = 1)\nmeta = meta.rename({'title': 'title_no', 'developer': 'developer_no'}, axis = 1)","4f9207ab":"train.head()","2b2ff47d":"meta.head()","ebd6887c":"# Add Meta Data\ntrain = train.merge(meta, on = 'title_no')\ntest = test.merge(meta, on = 'title_no')\n\ntrain_id = train.review_id\ntest_id = test.review_id\n\n# Create indices to split train and test on later\ntrain['train_ind'] = np.arange(train.shape[0])\ntest['train_ind'] = np.arange(train.shape[0], train.shape[0]+test.shape[0])\n\n# Merge Train and Test - This approach only works for competitions - not for model deployment in real projects.\ndata = pd.concat([train, test], axis = 0)","8bcd3d22":"# Create class which performs Label Encoding - if required\nclass categorical_encoder:\n    def __init__(self, columns, kind = 'label', fill = True):\n        self.kind = kind\n        self.columns = columns\n        self.fill = fill\n        \n    def fit(self, X):\n        self.dict = {}\n        self.fill_value = {}\n        \n        for col in self.columns:\n            label = preprocessing.LabelEncoder().fit(X[col])\n            self.dict[col] = label\n            \n            # To fill\n            if self.fill:\n                self.fill_value[col] = X[col].mode()[0]\n                X[col] = X[col].fillna(self.fill_value[col])\n                \n        print('Label Encoding Done for {} columns'.format(len(self.columns)))\n        return self\n    def transform(self, X):\n        for col in self.columns:\n            if self.fill:\n                X[col] = X[col].fillna(self.fill_value[col])\n                \n            X.loc[:, col] = self.dict[col].transform(X[col])\n        print('Transformation Done')\n        return X","843eca59":"# Create Lemmatizer - if required\nclass LemmaTokenizer(object):\n    def __init__(self):\n        self.wnl = WordNetLemmatizer()\n    def __call__(self, articles):\n        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]","8dba8a50":"# Function to Create CountEncoded and tf-idf features\ndef add_text_features(text_column_name, data_file, max_features = 2000, txn = 'tf-idf', min_df = 1, max_df = 1.0,\n                     ngram_range = (1, 1), lowercase = True, sparse = False, tokenizer = None):\n    if txn == 'count':\n        # Use Count Vectorizer\n        counts = CountVectorizer(max_features = max_features, min_df = min_df, \n        max_df = max_df, ngram_range = ngram_range, lowercase = lowercase, tokenizer=tokenizer).fit(data_file[text_column_name])\n    if txn == 'tf-idf':\n        counts = pipeline.make_pipeline(CountVectorizer(max_features = max_features, min_df = min_df, \n        max_df = max_df, ngram_range = ngram_range, lowercase = lowercase, tokenizer=tokenizer),\n                                        TfidfTransformer()).fit(data_file[text_column_name])\n    text_features = counts.transform(data_file[text_column_name])\n    \n    # Return for sparse output\n    if sparse: return text_features, None\n    \n    # Create Mapping\n    if txn == 'count':\n        mapping = {val: key for key, val in counts.vocabulary_.items()}\n    if txn == 'tf-idf':\n        mapping = {val: key for key, val in counts['countvectorizer'].vocabulary_.items()}\n    \n    # Create DataFrame\n    text_features_data = pd.DataFrame(text_features.toarray())\n    text_features_data = text_features_data.rename(mapping, axis = 1)\n    text_cols = text_features_data.columns.tolist()\n    \n    # Append to dataframe\n    data_copy = pd.concat([data_file.reset_index(drop = True), text_features_data.reset_index(drop = True)], axis = 1)\n    return data_copy, text_cols","794ac634":"# Label Encode Certain columns - for use later\nenc = categorical_encoder(columns = ['title_no','developer_no', 'publisher']).fit(data)\ndata_copy = enc.transform(data)","fd894fd9":"data_copy, text_cols = add_text_features(text_column_name = 'user_review', \n                                     data_file = data_copy, max_features = 120000, min_df = 5, max_df = .5,\n                                    ngram_range = (1, 4), lowercase = True, sparse = True, tokenizer = LemmaTokenizer())","1c24028e":"# Split the data back to train and test\nX_train = data_copy[:train.shape[0], :]\ny_train = data['user_suggestion'].iloc[:train.shape[0]]\n\nX_test = data_copy[train.shape[0]:, :]\ny_test = data['user_suggestion'].iloc[train.shape[0]:]","4b78ae67":"print(X_train.shape, X_test.shape)","d862cb89":"# Train model - Logistic Regression is a good option for Text classification problems\nmodel = linear_model.LogisticRegressionCV(penalty = 'l2', Cs = 10, max_iter = 5000).fit(X_train, y_train)\n","084e0b7c":"sub = pd.DataFrame()\nsub['review_id'] = test_id\n#sub['user_suggestion'] = (model.predict_proba(X_test)[:, 1]>.50).astype(int)\nsub['user_suggestion'] = model.predict(X_test).astype(int)\nsub['user_suggestion'].value_counts()\nsub.to_csv('sub.csv', index = None)","28417071":"**Finally extract tf-idf sparse dataframe**","a65d43b4":"**Create a lemmatizer class(not mandatory but can help improve the model predictions)**","a7e041c6":"**Create a class to perform Label Encoding for multiple columns at the same time. It has transform attributes.**","33acfca4":"## Submission for Logistic Regression","fe149b0d":"**Create a class that produces count and tf-idf encoded data. It has the capability to output sparse and dense data**\nIt is recommended to use the sparse data output because space taken to store the data in memory can reduce by orders of magnitude.","64da9311":"***The submission yielded a Binary F1 score of 0.8701 on the Public Leaderboard***"}}