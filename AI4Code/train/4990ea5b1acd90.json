{"cell_type":{"df85701e":"code","ceabed33":"code","4bc982cb":"code","3b59f01d":"code","3c8e5714":"code","56f3925a":"code","12917637":"code","5de07a8a":"code","8ddfac3a":"code","501422f3":"code","661578f4":"code","989683e8":"code","4d409f90":"code","8efbb1a1":"code","ed89444d":"code","d86a5929":"code","277855ad":"code","9080da07":"code","c1f4649d":"code","1035ae0a":"code","d1f22dc0":"code","15948584":"code","34a05087":"code","1213bbb2":"code","bd866cf0":"code","9153b536":"code","16fd8680":"code","c1564cc9":"code","e7d808ba":"code","083b9dcf":"code","fc4f975c":"code","438b0978":"code","76aa4763":"code","5c4182b5":"code","c4ca865e":"code","c839252d":"code","5cd7d621":"code","628ab00d":"code","0ca04029":"code","58830e04":"code","fc2c256e":"code","da108d2a":"code","6f2907f9":"code","8f8cb093":"code","4a32dfc1":"markdown","de019152":"markdown","285bc95a":"markdown","045fdbea":"markdown","262dcd80":"markdown","8fd332b1":"markdown","8b658f45":"markdown","ff5420f5":"markdown","d09ee030":"markdown","cda08739":"markdown","21de0e58":"markdown","4bea381c":"markdown","51a1c249":"markdown","4bc31c67":"markdown","9de9d8d1":"markdown","51b46785":"markdown","0e180624":"markdown","ad560ddd":"markdown","246fdda6":"markdown","5f5798b5":"markdown","04dffe5b":"markdown","9c9c365b":"markdown","3f1c600f":"markdown","dab1d781":"markdown","1895e042":"markdown","09de1284":"markdown","b9e87a20":"markdown","5cfc290d":"markdown","644d1d4c":"markdown"},"source":{"df85701e":"!pip install tweet-preprocessor 2>\/dev\/null 1>\/dev\/null","ceabed33":"import preprocessor as pcr\nimport numpy as np \nimport pandas as pd \nimport emoji\nimport keras\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom sklearn import preprocessing,  model_selection\nfrom keras.preprocessing import sequence, text\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nimport plotly.express as px\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tokenizers import Tokenizer, models \nfrom tensorflow.keras.layers import SpatialDropout1D","4bc982cb":"df_data_1 = pd.read_csv(\"..\/input\/tweetscsv\/Tweets.csv\")","3b59f01d":"df_data_1.head()","3c8e5714":"df_data = df_data_1[[\"tweet_id\",\"airline_sentiment\",\"text\"]]","56f3925a":"df_data.head()","12917637":"data_spell = pd.read_csv(\"..\/input\/spelling\/aspell.txt\",sep=\":\",names=[\"correction\",\"misspell\"])\ndata_spell.misspell = data_spell.misspell.str.strip()\ndata_spell.misspell = data_spell.misspell.str.split(\" \")\ndata_spell = data_spell.explode(\"misspell\").reset_index(drop=True)\ndata_spell.drop_duplicates(\"misspell\",inplace=True)\nmiss_corr = dict(zip(data_spell.misspell, data_spell.correction))\n\n#Sample of the dict\n{v:miss_corr[v] for v in [list(miss_corr.keys())[k] for k in range(20)]}","5de07a8a":"def correct_spell(v):\n    for a in v.split(): \n        if a in miss_corr.keys(): \n            v = v.replace(a, miss_corr[a]) \n    return v\n\ndf_data[\"clean_content\"] = df_data.text.apply(lambda a : correct_spell(a))","8ddfac3a":"contract = pd.read_csv(\"..\/input\/contractions\/contractions.csv\")\ncont_dict = dict(zip(contract.Contraction, contract.Meaning))","501422f3":"def contract_to_meaning(v): \n  \n    for a in v.split(): \n        if a in cont_dict.keys(): \n            v = v.replace(a, cont_dict[a]) \n    return v\n","661578f4":"df_data.clean_content = df_data.clean_content.apply(lambda a : contract_to_meaning(a))","989683e8":"pcr.set_options(pcr.OPT.MENTION, pcr.OPT.URL)\npcr.clean(\"hello guys @alx #sport\ud83d\udd25 1245 https:\/\/github.com\/s\/preprocessor\")","4d409f90":"df_data[\"clean_content\"]=df_data.text.apply(lambda a : pcr.clean(a))","8efbb1a1":"def punct(v): \n  \n    punct = '''()-[]{};:'\"\\,<>.\/@#$%^&_~'''\n  \n    for a in v.lower(): \n        if a in punct: \n            v = v.replace(a, \" \") \n    return v\n","ed89444d":"punct(\"test @ #ldfldlf??? !! \")","d86a5929":"df_data.clean_content = df_data.clean_content.apply(lambda a : ' '.join(punct(emoji.demojize(a)).split()))","277855ad":"def text_cleaning(v):\n    v = correct_spell(v)\n    v = contract_to_meaning(v)\n    v = pcr.clean(v)\n    v = ' '.join(punct(emoji.demojize(v)).split())\n    \n    return v","9080da07":"text_cleaning(\"isn't \ud83d\udca1 adultry @ttt good bad ... ! ? \")","c1f4649d":"df_data = df_data[df_data.clean_content != \"\"]","1035ae0a":"df_data.airline_sentiment.value_counts()","d1f22dc0":"id_for_sentiment = {\"neutral\":0, \"negative\":1,\"positive\":2}","15948584":"df_data[\"sentiment_id\"] = df_data['airline_sentiment'].map(id_for_sentiment)","34a05087":"df_data.head()","1213bbb2":"encoding_label = LabelEncoder()\nencoding_integer = encoding_label.fit_transform(df_data.sentiment_id)\n\nencoding_onehot = OneHotEncoder(sparse=False)\nencoding_integer = encoding_integer.reshape(len(encoding_integer), 1)\nY = encoding_onehot.fit_transform(encoding_integer)","bd866cf0":"X_train, X_test, y_train, y_test = train_test_split(df_data.clean_content,Y, random_state=1995, test_size=0.2, shuffle=True)","9153b536":"# using keras tokenizer here\ntkn = text.Tokenizer(num_words=None)\nmaximum_length = 160\nEpoch = 15\ntkn.fit_on_texts(list(X_train) + list(X_test))\nX_train_pad = sequence.pad_sequences(tkn.texts_to_sequences(X_train), maxlen=maximum_length)\nX_test_pad = sequence.pad_sequences(tkn.texts_to_sequences(X_test), maxlen=maximum_length)","16fd8680":"t_idx = tkn.word_index","c1564cc9":"embedding_dimension = 160\nlstm_out = 250\n\nmodel_sql = Sequential()\nmodel_sql.add(Embedding(len(t_idx) +1 , embedding_dimension,input_length = X_test_pad.shape[1]))\nmodel_sql.add(SpatialDropout1D(0.2))\nmodel_sql.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\nmodel_sql.add(keras.layers.core.Dense(3, activation='softmax'))\n#adam rmsprop \nmodel_sql.compile(loss = \"categorical_crossentropy\", optimizer='adam',metrics = ['accuracy'])\nprint(model_sql.summary())","e7d808ba":"size_of_batch = 32","083b9dcf":"model_sql.fit(X_train_pad, y_train, epochs = Epoch, batch_size=size_of_batch,validation_data=(X_test_pad, y_test))","fc4f975c":"def get_emotion(model_sql,text_1):\n    text_1 = text_cleaning(text_1)\n    #tokenize\n    tweet = tkn.texts_to_sequences([text_1])\n    tweet = sequence.pad_sequences(tweet, maxlen=maximum_length, dtype='int32')\n    emotion = model_sql.predict(tweet,batch_size=1,verbose = 2)\n    emo = np.round(np.dot(emotion,100).tolist(),0)[0]\n    rslt = pd.DataFrame([id_for_sentiment.keys(),emo]).T\n    rslt.columns = [\"sentiment\",\"percentage\"]\n    rslt=rslt[rslt.percentage !=0]\n    return rslt","438b0978":"def result_plotting(df):\n    #colors=['#D50000','#000000','#008EF8','#F5B27B','#EDECEC','#D84A09','#019BBD','#FFD000','#7800A0','#098F45','#807C7C','#85DDE9','#F55E10']\n    #fig = go.Figure(data=[go.Pie(labels=df.sentiment,values=df.percentage, hole=.3,textinfo='percent',hoverinfo='percent+label',marker=dict(colors=colors, line=dict(color='#000000', width=2)))])\n    #fig.show()\n    clrs={'neutral':'rgb(213,0,0)','negative':'rgb(0,0,0)',\n                    'positive':'rgb(0,142,248)'}\n    col={}\n    for i in rslt.sentiment.to_list():\n        col[i]=clrs[i]\n    figure = px.pie(df, values='percentage', names='sentiment',color='sentiment',color_discrete_map=col,hole=0.3)\n    figure.show()","76aa4763":"rslt =get_emotion(model_sql,\"Had an absolutely brilliant day \u00f0\u0178\u02dc\u0081 loved seeing an old friend and reminiscing\")\nresult_plotting(rslt)","5c4182b5":"rslt =get_emotion(model_sql,\"The pain my heart feels is just too much for it to bear. Nothing eases this pain. I can\u2019t hold myself back. I really miss you\")\nresult_plotting(rslt)","c4ca865e":"rslt =get_emotion(model_sql,\"I hate this game so much,It make me angry all the time \")\nresult_plotting(rslt)","c839252d":"def data_reading(file):\n    with open(file,'r') as z:\n        word_vocabulary = set() \n        word_vector = {}\n        for line in z:\n            line_1 = line.strip() \n            words_Vector = line_1.split()\n            word_vocabulary.add(words_Vector[0])\n            word_vector[words_Vector[0]] = np.array(words_Vector[1:],dtype=float)\n    print(\"Total Words in DataSet:\",len(word_vocabulary))\n    return word_vocabulary,word_vector","5cd7d621":"vocabulary, word_to_index =data_reading(\"..\/input\/glove-global-vectors-for-word-representation\/glove.6B.200d.txt\")","628ab00d":"matrix_embedding = np.zeros((len(t_idx) + 1, 200))\nfor word, i in t_idx.items():\n    vector_embedding = word_to_index.get(word)\n    if vector_embedding is not None:\n        matrix_embedding[i] = vector_embedding","0ca04029":"embedding_dimension = 200\nlstm_out = 250\n\nmodel_lstm = Sequential()\nmodel_lstm.add(Embedding(len(t_idx) +1 , embedding_dimension,input_length = X_test_pad.shape[1],weights=[matrix_embedding],trainable=False))\nmodel_lstm.add(SpatialDropout1D(0.2))\nmodel_lstm.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\nmodel_lstm.add(keras.layers.core.Dense(3, activation='softmax'))\n#adam rmsprop \nmodel_lstm.compile(loss = \"categorical_crossentropy\", optimizer='adam',metrics = ['accuracy'])\nprint(model_lstm.summary())","58830e04":"size_of_batch = 32","fc2c256e":"model_lstm.fit(X_train_pad, y_train, epochs = Epoch, batch_size=size_of_batch,validation_data=(X_test_pad, y_test))","da108d2a":"rslt =get_emotion(model_lstm,\"Had an absolutely brilliant day \u00f0\u0178\u02dc\u0081 loved seeing an old friend and reminiscing\")\nresult_plotting(rslt)","6f2907f9":"rslt =get_emotion(model_lstm,\"The pain my heart feels is just too much for it to bear. Nothing eases this pain. I can\u2019t hold myself back. I really miss you\")\nresult_plotting(rslt)","8f8cb093":"rslt =get_emotion(model_lstm,\"I hate this game so much,It make me angry all the time \")\nresult_plotting(rslt)","4a32dfc1":"# **Result of LSTM GloVe**","de019152":"### Paragraph-3","285bc95a":"### Paragraph-1","045fdbea":"# **Result of LSTM**","262dcd80":"# **LSTM with GloVe Model**","8fd332b1":"### Paragraph-3","8b658f45":"# **LSTM with GloVe 6B 200d word embedding**\n### **GloVe algorithm is an extension to the word2vec method for efficiently learning word vectors**","ff5420f5":"## **Importing Libraries**","d09ee030":"### Paragraph-2","cda08739":"# **LSTM: Long short-term memory** \n\n### **It is an artificial recurrent neural network (RNN) architecture used in the field of deep learning.**","21de0e58":"# **Removal of URLs and Mentions from dataset**","4bea381c":"# **Result of LSTM**","51a1c249":"# **LSTM Model**","4bc31c67":"# **Result of LSTM GloVe**","9de9d8d1":"# **Project Objective and Brief**\n\n## *In this project, rule-based and Deep-Learning algorithms are used with an aim to first appropriately detect different type of emotions contained in a collection of Tweets and then accurately predict the overall emotions of the Tweets is done.*","51b46785":"### Paragraph-1","0e180624":"# **Result of LSTM**","ad560ddd":"# **Using a Python library for expanding and creating common English contractions in text**","246fdda6":"## **Encoding the data and train, test and split it**","5f5798b5":"# **Removing empty comments from dataset**","04dffe5b":"## **Preprocessor is a preprocessing library used for tweet data written in Python.While building Machine Learning systems based on tweet data, a preprocessing is required. This library makes it easy to clean, parse or tokenize the tweets.The same is imported here.** ","9c9c365b":"### Paragraph-2","3f1c600f":"# **Conclusion**\n\n","dab1d781":"# **Removal of Punctuations and Emojis from dataset**","1895e042":"# **Result of LSTM GloVe**","09de1284":"#  **Correcting Spelling of data**","b9e87a20":"**Algorithms used to detect different types of emotion from paragraph are**\n\n**1- LSTM (Long Short Term Memory)**-It is an artificial recurrent neural network (RNN) architecture used in the field of deep learning.\n**2- LSTM GloVe- GloVe algorithm is an extension to the word2vec method for efficiently learning word vectors.**\n\nIt has been concluded that using LSTM algorithm it is easier to classify the Tweets and a more accurate result is obtained.","5cfc290d":"# **Data preparation**","644d1d4c":"# **Data Modeling**"}}