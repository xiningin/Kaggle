{"cell_type":{"692e1139":"code","000d9164":"code","0dc55fd9":"code","2c5c41af":"code","8c530467":"code","d01f9dff":"code","de66e432":"code","79c33b1e":"code","492e28ff":"code","0aa5abfc":"code","35b3f23c":"code","e233c2ca":"code","a58382f6":"code","889b5d5e":"code","d8aaf1ee":"code","0f1d1f52":"code","b5b93c1a":"code","a08309b2":"code","9fcc6da8":"code","f67a5d84":"code","c2cce5c3":"code","bc09ed60":"code","6dcde1d2":"code","fa2f5d4c":"code","fc1e9213":"code","b2ca7188":"code","e87eb824":"code","35d4c1f4":"code","8855d3c7":"code","2aec819c":"code","e5019b8d":"code","30ca0a03":"markdown","d4ffb699":"markdown","9f5f77ff":"markdown","9619e2c9":"markdown","2cf9d6ff":"markdown","7e830f82":"markdown","01fa004d":"markdown","e392551f":"markdown","9be47fa3":"markdown","ef9e187a":"markdown"},"source":{"692e1139":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","000d9164":"folder_path = \"\/kaggle\/input\/fptu-huynhld3-sum21\/devset_images\/devset_images\"\nlabel_file = \"\/kaggle\/input\/fptu-huynhld3-sum21\/devset_images_gt_kaggle.csv\"","0dc55fd9":"import pandas as pd \nimport cv2\nimport numpy as np\nfrom sklearn.model_selection import train_test_split","2c5c41af":"# Read data from file 'filename.csv' \n# (in the same directory that your python process is based)\n# Control delimiters, rows, column names with read_csv (see later) \ndata = pd.read_csv(label_file) \n# Preview the first 5 lines of the loaded data \ndata_array=data.values","8c530467":"X_train=[]\nY_train=[]\nk=0\nfor i in data_array:\n  image = cv2.imread(folder_path+'\/'+str(i[0])+'.jpg')\n  if image is not None:\n    #print(k)\n    k=k+1\n    image = cv2.resize(image, (224,224))\n    # print(image.shape)\n    X_train.append(image)\n    Y_train.append(i[1])","d01f9dff":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(16,10))\nfor i in range(1,11):\n    plt.subplot(2,5,i)\n    plt.grid(False)\n    plt.imshow(X_train[i])\n    plt.xlabel(\"Label \" + str(Y_train[i]))\nplt.show()","de66e432":"X_train=np.array(X_train)\nY_train= np.array(Y_train)\nprint(X_train.shape)\nprint(Y_train.shape)","79c33b1e":"x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train)","492e28ff":"from tensorflow.keras.applications import EfficientNetB7\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\n\nimg_augmentation = Sequential(\n    [\n        preprocessing.RandomRotation(factor=0.15),\n        preprocessing.RandomTranslation(height_factor=0.1, width_factor=0.1),\n        preprocessing.RandomFlip(),\n        preprocessing.RandomContrast(factor=0.1),\n    ],\n    name=\"img_augmentation\",\n)","0aa5abfc":"import matplotlib.pyplot as plt\nimport tensorflow as tf \n\nplt.figure(figsize=(16,10))\nfor i in range(1,11):\n    plt.subplot(2,5,i)\n    plt.grid(False)\n    image = tf.expand_dims(X_train[i],0)\n    image = img_augmentation(image)\n    image = tf.squeeze(image,axis=0)\n    plt.imshow(image)\n    plt.xlabel(\"Label \" + str(Y_train[i]))\nplt.show()","35b3f23c":"from keras import backend as K\n\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","e233c2ca":"IMG_SIZE = 224\nimport tensorflow as tf\nfrom tensorflow.keras.applications import EfficientNetB4,ResNet101V2\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n#from tensorflow.keras.applications.resnet_v2 import preprocess_input\ndef build_model(num_classes=1):\n    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n    x = preprocess_input(inputs)\n    x = img_augmentation(x)\n    model = EfficientNetB4(include_top=False, input_tensor=x, weights=\"imagenet\")\n\n    # Freeze the pretrained weights\n    model.trainable = False\n\n    # Rebuild top\n    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n    x = layers.BatchNormalization()(x)\n    \n    x = layers.Dense(1024, activation=\"relu\", name=\"relu\")(x)\n\n    top_dropout_rate = 0.2\n\n    x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n\n    x = layers.Dense(128, activation=\"relu\", name=\"relu_2\")(x)\n\n    outputs = layers.Dense(num_classes, activation=\"sigmoid\", name=\"pred\")(x)\n\n    # Compile\n    model = tf.keras.Model(inputs, outputs, name=\"EfficientNet\")\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\n    model.compile(\n        optimizer=optimizer, loss=tf.keras.losses.BinaryCrossentropy(), metrics=['accuracy',f1]\n    )\n    return model","a58382f6":"model = build_model(num_classes=1)","889b5d5e":"import tensorflow as tf\n\ndef outer_product(x):\n    #Einstein Notation  [batch,1,1,depth] x [batch,1,1,depth] -> [batch,depth,depth]\n    phi_I = tf.einsum('ijkm,ijkn->imn',x[0],x[1])\n    \n    # Reshape from [batch_size,depth,depth] to [batch_size, depth*depth]\n    phi_I = tf.reshape(phi_I,[-1,x[0].shape[3]*x[1].shape[3]])\n    \n    # Divide by feature map size [sizexsize]\n    size1 = int(x[1].shape[1])\n    size2 = int(x[1].shape[2])\n    phi_I = tf.divide(phi_I, size1*size2)\n    \n    # Take signed square root of phi_I\n    y_ssqrt = tf.multiply(tf.sign(phi_I),tf.sqrt(tf.abs(phi_I)+1e-12))\n    \n    # Apply l2 normalization\n    z_l2 = tf.nn.l2_normalize(y_ssqrt, axis=1)\n    return z_l2","d8aaf1ee":"from keras.models import Model\nfrom tensorflow.keras.layers import Convolution2D,Activation,GlobalAveragePooling2D,MaxPooling2D,Flatten,Dense,Dropout,Input,Reshape,Lambda\n\n\ninputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\nx = preprocess_input(inputs)\nbandau = img_augmentation(x)\n# Model 1 \n\nmodel1 = EfficientNetB4(include_top=False, input_tensor=bandau, weights=\"imagenet\")\n\n# Model 2\nmodel2 = EfficientNetB4(include_top=False, input_tensor=bandau, weights=\"imagenet\")\n\nfor i, layer in enumerate(model1.layers):\n    layer._name = 'model1_' + layer.name\n    layer.trainable = False #Freeze all layers\nfor i, layer in enumerate(model2.layers):\n    layer._name = 'model2_' + layer.name\n    layer.trainable = False #Freeze all layers\n\n\n# Rebuild top\n#x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model1.output)\nlast_output1 = layers.BatchNormalization()(model1.output)\n\n# Rebuild top\n#x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model2.output)\nlast_output2 = layers.BatchNormalization()(model2.output)\n\n##\nmodel1_ = Model(inputs=model1.input, outputs=last_output1)\nmodel2_ = Model(inputs=model2.input, outputs=last_output2)\n\nd1=model1_.output\nd2=model2_.output\n\nbilinear = Lambda(outer_product)([d1,d2])\n\nx = Dense(32, activation='relu', name='dense1')(bilinear)\n\nx = layers.Dropout(0.2, name=\"top_dropout\")(x)\n\npredictions=Dense(1, activation='sigmoid', name='predictions')(x)\n\nmodel = Model(inputs=model1.input, outputs=predictions)","0f1d1f52":"# Compile\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\nmodel.compile(\n    optimizer=optimizer, loss=tf.keras.losses.BinaryCrossentropy(), metrics=['accuracy',f1]\n)\n\nmodel.summary()","b5b93c1a":"model_checkpoint = tf.keras.callbacks.ModelCheckpoint(mode='auto', filepath='.\/dgt301-{epoch:03d}_loss-{loss:.4f}_val_loss-{val_loss:.4f}.h5', \n                     monitor='val_loss',  \n                     save_weights_only='True', \n                     period=1,\n                     verbose=1,\n                     save_best_only=True)\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                               min_delta=0.0,\n                               patience=10,\n                               verbose=1)\nreduce_learning_rate = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n                                         factor=0.2,\n                                         patience=6,\n                                         verbose=1,\n                                         epsilon=0.001,\n                                         cooldown=0,\n                                         min_lr=0.00001)","a08309b2":"history = model.fit(\n    x_train,\n    y_train,\n    batch_size=32,\n    epochs=20,\n    validation_data=(x_test, y_test),\n    callbacks=[model_checkpoint,early_stopping,reduce_learning_rate]\n)","9fcc6da8":"model.load_weights('.\/dgt301-009_loss-0.2843_val_loss-0.2887.h5')\n\ndef unfreeze_model(model):\n    # We unfreeze the top 20 layers while leaving BatchNorm layers frozen\n    for layer in model.layers[-400:]:\n        if not isinstance(layer, layers.BatchNormalization):\n            layer.trainable = True\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n    model.compile(\n        optimizer=optimizer, loss=tf.keras.losses.BinaryCrossentropy(), metrics=[\"accuracy\",f1]\n    )\n\nunfreeze_model(model)","f67a5d84":"model_checkpoint = tf.keras.callbacks.ModelCheckpoint(mode='auto', filepath='.\/dgt301_turn2-{epoch:03d}_loss-{loss:.4f}_val_loss-{val_loss:.4f}.h5', \n                     monitor='val_loss',  \n                     save_weights_only='True', \n                     period=1,\n                     verbose=1)\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                               min_delta=0.0,\n                               patience=10,\n                               verbose=1)\nreduce_learning_rate = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n                                         factor=0.2,\n                                         patience=6,\n                                         verbose=1,\n                                         epsilon=0.001,\n                                         cooldown=0,\n                                         min_lr=0.00001)","c2cce5c3":"history2 = model.fit(\n    x_train,\n    y_train,\n    batch_size=32,\n    epochs=50,\n    validation_data=(x_test, y_test),\n    callbacks=[model_checkpoint,early_stopping,reduce_learning_rate]\n)","bc09ed60":"folder_path = \"\/kaggle\/input\/fptu-huynhld3-sum21\/devset_images\/devset_images\"\nlabel_file = \"\/kaggle\/input\/fptu-huynhld3-sum21\/testset_images_submit_kaggle.csv\"","6dcde1d2":"data = pd.read_csv(label_file) \ndata_array=data.values","fa2f5d4c":"data.head()","fc1e9213":"X_train=[]\nk=0\nfor i in data_array:\n  image = cv2.imread(folder_path+'\/'+str(i[0])+'.jpg')\n  if image is not None:\n    k=k+1\n    image = cv2.resize(image, (224,224))\n    # print(image.shape)\n    X_train.append(image)\n  else:\n    pass","b2ca7188":"X_train=np.array(X_train)\nX_train.shape","e87eb824":"model.load_weights('.\/dgt301_turn2-003_loss-0.1881_val_loss-0.2369.h5')","35d4c1f4":"batch_size = 32 \n\nstacked = 0\n#Y=np.append(Y_1, Y_2, axis=0)\n\nfor i in range(len(X_train)):\n    if i % batch_size == 0:\n        if stacked != 0: # First batch\n            if stacked == 1:\n                y = model(X_train[i-32:i])\n                Y = np.array(y)\n            else:\n                y = model(X_train[i-32:i])\n                Y = np.append(Y,y,axis=0)\n            print(i-32,i,Y.shape)       \n        stacked += 1\nrest_output = model(X_train[(stacked-1)*32:])\n\nY = np.append(Y,rest_output,axis=0)\n\nprint(Y.shape)","8855d3c7":"Y = np.array(Y)","2aec819c":"import csv\n\nwith open('submission_B4_bestloss_0.98.csv', mode='w') as employee_file:\n    employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n    employee_writer.writerow(['Id'])\n    j = 0\n    for i in data_array:\n      image = cv2.imread(folder_path+'\/'+str(i[0])+'.jpg') \n      if image is not None:\n        if Y[j] > 0.98:\n          employee_writer.writerow([i[0]])\n        else:\n          pass\n        j += 1\n      else:\n        pass","e5019b8d":"all_models = list()\n\nmodel_path = ['']\n\n\nfor i in range(len(model_path)):\n    model.load_weights(model_path[i])\n    \n    all_models.apped(model)\n\nprint('Loaded %d models' % len(members))","30ca0a03":"# F1 score metric","d4ffb699":"# Submission ","9f5f77ff":"# Unfreeze weights ","9619e2c9":"# Bilinear Model","2cf9d6ff":"# Visualize data","7e830f82":"# Separate Stacking Model","01fa004d":"# Augmentation ","e392551f":"# Callback checkpoints","9be47fa3":"# Construct model based on transfer learning method","ef9e187a":"# Train model"}}