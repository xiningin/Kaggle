{"cell_type":{"8bb2bbc9":"code","d9c464fc":"code","a8fd3a3c":"code","64449976":"code","2457f1b2":"code","3b5ec4f1":"code","21d080ee":"code","9760b40d":"code","412a93dd":"code","156f1c1c":"code","25566fc9":"code","25b841ba":"code","861925ab":"code","aa2f0602":"code","108a90ec":"code","13809c79":"code","d1b9cdff":"code","ddb73976":"code","5a007428":"code","71111eaa":"markdown","8d2680ff":"markdown","7fb4550e":"markdown","11b3a81e":"markdown","858635ac":"markdown","7dbfa8e7":"markdown","9057b787":"markdown","2d3395aa":"markdown","0bb19bb1":"markdown","125433e8":"markdown","5d3a1a35":"markdown","b2334151":"markdown"},"source":{"8bb2bbc9":"import pandas as pd\nimport numpy as np","d9c464fc":"df = pd.read_csv(\"\/kaggle\/input\/dataset\/diabetes.csv\")\ndf","a8fd3a3c":"from sklearn.feature_selection import SelectKBest \nfrom sklearn.feature_selection import chi2 \n\n#classify the dataset\n\nx = df.iloc[:,:-1]#independent columns\ny = df.iloc[:,-1] #target column \n\n\n","64449976":"x","2457f1b2":"y","3b5ec4f1":" #apply SelectKBest class to extract top 5 best features \nbestfeatures = SelectKBest(score_func=chi2, k=5) \nfit = bestfeatures.fit(x,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(x.columns)\n\n #concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score'] #naming the dataframe columns\nfeatureScores\nprint(featureScores.nlargest(5,'Score'))  #print 5 best features","21d080ee":"from sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt \nmodel = ExtraTreesClassifier() \nmodel.fit(x,y)\nprint(model.feature_importances_) ","9760b40d":"feat_importances = pd.Series(model.feature_importances_, index=x.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()\n","412a93dd":"from sklearn.model_selection import train_test_split\n\nx = df.iloc[:,:-1]#independent columns\ny = df.iloc[:,-1] #target column \n\n\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n","156f1c1c":"from sklearn.preprocessing import StandardScaler\nprint(\"Before Scaling\")\nprint(X_train,\"  \",X_test,\"   \",Y_train,\"   \",Y_test)","25566fc9":"sc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nsc_y = StandardScaler()\nY_train = Y_train.values.reshape((len(Y_train), 1))\nY_train = sc_y.fit_transform(Y_train)\nY_train = Y_train.ravel()\n\n","25b841ba":"print(\"After Scaling\")\nprint(X_train,\"  \",X_test,\"   \",Y_train,\"   \",Y_test)","861925ab":"from sklearn.preprocessing import Binarizer\nbinarizer=Binarizer(threshold=0.0).fit(x)\nbinaryX=binarizer.transform(x)\nbinaryX[0:5,:]\n","aa2f0602":"df = pd.read_csv(\"\/kaggle\/input\/dataset\/diabetes.csv\")\n","108a90ec":"bin_data = df[['Age']]\nbin_data['age_range']=pd.cut(df['Age'], bins=[0, 40, 50, 100],labels=[\"Low\", \"Mid\", \"High\"])\nbin_data\n","13809c79":"from sklearn.preprocessing import LabelEncoder\nlabel_1=LabelEncoder()\nbin_data['age_range']=label_1.fit_transform(bin_data['age_range'])\n\nbin_data\n\n\n","d1b9cdff":"from sklearn.preprocessing import LabelEncoder\ndf = pd.read_csv(\"diabetes.csv\")\nlabel_1=LabelEncoder()\ndf['Age']=label_1.fit_transform(df['Age'])\n\ndf","ddb73976":"import pandas, scipy, numpy\nfrom sklearn.preprocessing import MinMaxScaler\n\ndf=pandas.read_csv('\/kaggle\/input\/dataset\/FluidQuality.csv',sep=';')\ndf\n","5a007428":"array=df.values\n\n#Separating data into input and output components\nx=array[:,0:8]\ny=array[:,8]\n\nscaler=MinMaxScaler(feature_range=(0,1))\nrescaledX=scaler.fit_transform(x)\nnumpy.set_printoptions(precision=3) #Setting precision for the output\nrescaledX[0:5,:]\n\n","71111eaa":"Under-sampling majority class\nUnder-sampling the majority class will resample the majority class points in the data to make them equal to the minority class.\n\nEx:\nOut of 100 records we have\nvalues 1,2\n\n1 in 5 places\n2 in 95 places","8d2680ff":"# Univariate Selection(Find Feature Score)","7fb4550e":"# Scaling(Scaling ensures that all data in a dataset falls in the same range.)","11b3a81e":"# Feature Score\nExtraTreesClassifier method\nhelp to give the importance of each independent feature with a dependent feature. \nFeature importance will give you a score for each feature of your data,\nthe higher the score more important or relevant to the feature towards your output variable.","858635ac":"# Splitting the Dataset into Training and Testing sets","7dbfa8e7":"Over Sampling minority class using Synthetic Minority Oversampling Technique (SMOTE)\nIn this method, synthetic samples are generated for the minority class and equal to the majority class.","9057b787":"Over Sampling Minority class by duplication\nOversampling minority class will resample the minority class points in the data to make them equal to the majority class.","2d3395aa":"# Binning\n","0bb19bb1":"# Handling imbalanced data","125433e8":"# Rescaling Data","5d3a1a35":"# Encoding:","b2334151":"# Binarizing Data"}}