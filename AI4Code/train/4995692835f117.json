{"cell_type":{"95c4e6f5":"code","2723328d":"code","0582e868":"code","063b8e84":"code","441e8115":"code","45fae567":"code","c87f872f":"code","90b0a25b":"code","2475f09f":"code","d24af841":"code","bf2f6101":"code","48629b07":"code","5c4c1376":"markdown","3015a921":"markdown"},"source":{"95c4e6f5":"# Standard python libraries\nimport io, os, time, re\n\n# Installed libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation\nfrom sklearn.model_selection import KFold\nimport tensorflow as tf\nfrom joblib import Parallel, delayed\n\npath_data = '..\/input\/optiver-realized-volatility-prediction'","2723328d":"# Create functions for common calculation use for this competition \ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))","0582e868":"# Create functions for extracting the data.  \n# Thank you Manel for your insights on this from your original notebook https:\/\/www.kaggle.com\/manels\/lgb-starter\ndef get_dataSet(stock_ids : list, dataType = 'train'):\n    stock_stat = Parallel(n_jobs=-1)(\n        delayed(get_stock_stat)(stock_id, dataType) \n        for stock_id in stock_ids\n    )\n    stock_stat_df = pd.concat(stock_stat, ignore_index = True)\n    return stock_stat_df\n\ndef get_stock_stat(stock_id : int, dataType = 'train'):\n    key = ['stock_id', 'time_id', 'seconds_in_bucket']\n    \n    #Book features\n    df_book = pd.read_parquet(os.path.join(path_data, 'book_{}.parquet\/stock_id={}\/'.format(dataType, stock_id)))\n    df_book['stock_id'] = stock_id\n    cols = key + [col for col in df_book.columns if col not in key]\n    df_book = df_book[cols]\n\n    df_book['wap1'] = (df_book['bid_price1'] * df_book['ask_size1'] +\n                                    df_book['ask_price1'] * df_book['bid_size1']) \/ (df_book['bid_size1'] + df_book['ask_size1'])\n    df_book['wap2'] = (df_book['bid_price2'] * df_book['ask_size2'] +\n                                    df_book['ask_price2'] * df_book['bid_size2']) \/ (df_book['bid_size2'] + df_book['ask_size2'])\n    \n    df_book['log_return1'] = df_book.groupby(by = ['time_id'])['wap1'].apply(log_return)\n    df_book = df_book[~df_book['log_return1'].isnull()]\n    df_book['log_return2'] = df_book.groupby(by = ['time_id'])['wap2'].apply(log_return)\n    df_book = df_book[~df_book['log_return2'].isnull()]\n    \n    features_to_apply_realized_volatility = ['log_return'+str(i+1) for i in range(2)]\n    stock_stat = df_book.groupby(by = ['stock_id', 'time_id'])[features_to_apply_realized_volatility]\\\n                        .agg(realized_volatility).reset_index()\n\n    #Trade features\n    trade_stat =  pd.read_parquet(os.path.join(path_data,'trade_{}.parquet\/stock_id={}'.format(dataType, stock_id)))\n    trade_stat = trade_stat.sort_values(by=['time_id', 'seconds_in_bucket']).reset_index(drop=True)\n    trade_stat['stock_id'] = stock_id\n    cols = key + [col for col in trade_stat.columns if col not in key]\n    trade_stat = trade_stat[cols]\n    \n    trade_stat['trade_log_return1'] = trade_stat.groupby(by = ['time_id'])['price'].apply(log_return)\n    trade_stat = trade_stat[~trade_stat['trade_log_return1'].isnull()]\n    \n    trade_stat = trade_stat.groupby(by = ['stock_id', 'time_id'])[['trade_log_return1']]\\\n                           .agg(realized_volatility).reset_index()\n    #Joining book and trade features\n    stock_stat = stock_stat.merge(trade_stat, on=['stock_id', 'time_id'], how='left').fillna(-999)\n    \n    return stock_stat","063b8e84":"# create the train dataframe\ntrain = pd.read_csv(os.path.join(path_data, 'train.csv'))\ntrain_stock_stat_df = get_dataSet(stock_ids = train['stock_id'].unique(), dataType = 'train')\ntrain = pd.merge(train, train_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left')\nprint('Train shape: {}'.format(train.shape))\nprint(train)","441e8115":"# create the test dataframe\ntest = pd.read_csv(os.path.join(path_data, 'test.csv'))\ntest_stock_stat_df = get_dataSet(stock_ids = test['stock_id'].unique(), dataType = 'test')\ntest = pd.merge(test, test_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left').fillna(0)\nprint('Test shape: {}'.format(test.shape))\nprint(test)","45fae567":"# Create new dataframe called train_data with one-hot encoding for our categorical stock_id\ntrain_data = train.sort_index(axis=1) # Sort First\n\n# Make stock_id as one hot encoded categoral features\ntrain_data = pd.concat([train_data, pd.get_dummies(train_data['stock_id'], prefix=\"stock_id\")], axis=1)\ntrain_data = train_data.drop(['time_id', 'stock_id'], axis=1)\ntrain_data","c87f872f":"# Separate out the train labels and targets and converting them to numpy array\nX = train_data.drop(['target'], axis=1).values\nY = train_data['target'].values","90b0a25b":"# Function for constructing a NN model\ndef build_model():\n    model = Sequential()\n    model.add(Dense(84, activation='relu', input_shape=(X.shape[1],)))\n    model.add(Dense(48, activation='relu'))\n    model.add(Dense(12, activation='relu'))\n    model.add(Dense(1))\n    return model","2475f09f":"# Create a kfold cross validation training and monitor on the MAPE\nkfold = KFold(n_splits=5, shuffle=True, random_state=42)\ncvscores = []\nfold_no = 1\n\nfor train, val in kfold.split(X, Y):\n    model = build_model()\n    model.compile(optimizer='adam', loss='mean_absolute_percentage_error')\n    print(f'Training for Fold {fold_no} ...')\n    history = model.fit(X[train], Y[train],\n              batch_size=128,\n              epochs=100)\n    scores = model.evaluate(X[val], Y[val], verbose=0)\n    print(f'Fold {fold_no} CV Score: {scores}')\n    cvscores.append(scores)\n    fold_no = fold_no + 1\nprint(f'Overall Average of {len(cvscores)} folds: {sum(cvscores) \/ len(cvscores)}')","d24af841":"# Process the test_data dataset just like the train_data so all columns are available & in same order\ntest_data = test.sort_index(axis=1) # Sort First\n\n# Make stock_id as one hot encoded categoral features\ntest_data = pd.concat([test_data, pd.get_dummies(test['stock_id'], prefix=\"stock_id\")], axis=1)\ntest_data = test_data.drop(['time_id', 'stock_id', 'row_id'], axis=1)\ntest_data = pd.DataFrame(data=test_data, columns = train_data.columns).fillna(0)\ntest_data = test_data.drop(['target'], axis=1)\ntest_data","bf2f6101":"# Use trained model to predict on the test_data in numpy array\npredictions = model.predict(test_data.values)\npredictions","48629b07":"# Create a submission file with the predictions against the row_ids\ntest['target'] = predictions.reshape(len(predictions)).tolist()\nsubmission = test[['row_id', 'target']]\nsubmission.to_csv('submission.csv',index = False)\nsubmission","5c4c1376":"I hope this leaves you with lots of run to make adjustments & create your own Neural Network to make predictions for the Realized Volatility in this Competition.  Best of luck !  If this helps you, please kindly let me know in comments and upvote :)","3015a921":"In this notebook, I will be showing you a baseline of what you could do with Neural Network with K Fold cross validation.  This one is demonstrated with training data of 3 numerical values (log_return_1, log_return_2, trade_log_return1) and 1 categorical feature (stock_id)."}}