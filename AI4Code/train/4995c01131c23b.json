{"cell_type":{"653ee97f":"code","ac0ceb5d":"code","e25a43df":"code","2169fccd":"code","58409604":"code","5f277c2b":"code","f30bdcd9":"code","3279ff00":"code","a799487a":"code","a070f66b":"code","29bf2cb5":"code","11485b6a":"code","e024658b":"code","27af6ef2":"code","f731e67c":"code","2aa0ed29":"code","b6396ce0":"code","e40c2655":"code","427a1104":"code","e4f22487":"code","4a6bed35":"code","8b5261cf":"code","fcc39609":"code","bf271fed":"code","14147d62":"code","901b3b7f":"code","f27c351b":"code","ff8249f7":"code","f9209c5a":"code","3f376df6":"code","c9f2832d":"code","6d2bac82":"code","3d56f392":"code","a6f5fc67":"code","31bbbbe3":"code","831c9aab":"code","e9962272":"code","399a9cbb":"code","babd406b":"code","d0170f7e":"code","44deac54":"code","c1e88ce0":"code","02a2b0e7":"code","a0552039":"code","e09f2724":"code","6081480a":"code","9e0a5310":"code","fb1ef3eb":"code","c4f7453b":"code","ce4453b2":"markdown","d3a2319f":"markdown","442d3818":"markdown","5b7eb2e0":"markdown","a0b5e107":"markdown","5b29a259":"markdown","bbc08a4a":"markdown","5ee83b5d":"markdown","c52be960":"markdown","6a68936a":"markdown","569c9e74":"markdown","9bbf908d":"markdown","c672c7d3":"markdown","0d074406":"markdown","e989a34e":"markdown","60bee72a":"markdown","3933d4e2":"markdown","80df19b4":"markdown","8171c69a":"markdown","02646a52":"markdown","de4724a6":"markdown","46f2fe19":"markdown","4443d207":"markdown","4c8d7882":"markdown","6dc6ec10":"markdown","20da621c":"markdown","1db8ac00":"markdown","dcbbdf80":"markdown","34bea872":"markdown","d670c92b":"markdown","919edf44":"markdown","597b2cf7":"markdown","5fbc1080":"markdown","1caae5d4":"markdown","1d969fb7":"markdown","2d94d90f":"markdown","43faea19":"markdown","4875468f":"markdown","62a664f7":"markdown","23d0e4a1":"markdown","a83343e4":"markdown","1d60a846":"markdown","364f540b":"markdown","7774258c":"markdown","af197032":"markdown","9d1f2670":"markdown","698f0813":"markdown","112dad03":"markdown","5b50a43f":"markdown","7d6d46ac":"markdown","9174a393":"markdown","c1439f11":"markdown","311b3431":"markdown","33e3cea6":"markdown","2f1bb41b":"markdown","5c841f09":"markdown","c80d5315":"markdown"},"source":{"653ee97f":"#basic libraries\nimport sys\nimport numpy as np\nimport pandas as pd\nimport os\nimport warnings\n\n#seed the project\nnp.random.seed(64)\n\n#ploting libraries\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport matplotlib.gridspec as gridspec\nplt.rcParams['figure.figsize'] = (16,8)\nfrom scipy import stats\nimport missingno\nfrom scipy.stats import skew\nfrom scipy.stats.stats import pearsonr\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom scipy.stats import norm\nsns.set(context='notebook', style='whitegrid', palette='pastel', font='sans-serif', font_scale=1, color_codes=False, rc=None)\n\n#warning hadle\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"Set up completed\")","ac0ceb5d":"# Import train and test data\ntrain_path = \"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\"\ntest_path = \"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\"\n\ntrain_df = pd.read_csv(train_path)\ntest_df = pd.read_csv(test_path)","e25a43df":"# Print the first rows of the training data\ntrain_df.head()","2169fccd":"# Print the first rows of the test data\ntest_df.head()","58409604":"print (f\"Train has {train_df.shape[0]} rows and {train_df.shape[1]} columns\")\nprint (f\"Test has {test_df.shape[0]} rows and {test_df.shape[1]} columns\")","5f277c2b":"# Different data types in the dataset\ntrain_df.dtypes","f30bdcd9":"# Plot graphic of missing values\nmissingno.matrix(train_df, figsize = (30,10));","3279ff00":"# We need to see the amount of missing values present in each column.\ndef missing_percentage(df):\n    \"\"\"This function takes a DataFrame(df) as input and returns two columns, total missing values and total missing values percentage\"\"\"\n    ## the two following line may seem complicated but its actually very simple. \n    total = df.isnull().sum().sort_values(ascending = False)[df.isnull().sum().sort_values(ascending = False) != 0]\n    percent = round(df.isnull().sum().sort_values(ascending = False)\/len(df)*100,2)[round(df.isnull().sum().sort_values(ascending = False)\/len(df)*100,2) != 0]\n    return pd.concat([total, percent], axis=1, keys=['Total','Percent'])\n\nmissing_percentage(train_df)","a799487a":"missing_percentage(test_df).head(10)","a070f66b":"x = train_df['SalePrice']\nf, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw={\"height_ratios\": (.15, .85)})\n\nsns.boxplot(x, ax=ax_box)\nsns.distplot(x, ax=ax_hist)\nplt.axvline(x = x.mean(), c = 'red')\nplt.axvline(x = x.median(), c = 'green')\n\nax_box.set(yticks=[])\nsns.despine(ax=ax_hist)\nsns.despine(ax=ax_box, left=True)\nplt.show()\nprint(\"Skewness: %f\" % train_df['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train_df['SalePrice'].kurt())","29bf2cb5":"x = np.log(train_df['SalePrice'])\nf, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw={\"height_ratios\": (.15, .85)})\n\nsns.boxplot(x, ax=ax_box)\nsns.distplot(x, ax=ax_hist)\nplt.axvline(x = x.mean(), c = 'red')\nplt.axvline(x = x.median(), c = 'green')\n\nax_box.set(yticks=[])\nsns.despine(ax=ax_hist)\nsns.despine(ax=ax_box, left=True)\nplt.show()","11485b6a":"## Getting the correlation of all the features with target variable. \n(train_df.corr('pearson'))['SalePrice'].sort_values(ascending = False)[1:].head()","e024658b":"plt.subplots(figsize = (30,30))\n\n## Plotting heatmap. Generate a mask for the lower triangle (taken from seaborn example gallery)\nmask = np.zeros_like(train_df.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(train_df.corr(), cmap=sns.diverging_palette(20, 220, n=200), annot=True, mask=mask, center = 0, );\nplt.title(\"Heatmap of all the Features of Train data set\", fontsize = 25);","27af6ef2":"figure, ax = plt.subplots(1,3, figsize = (20,8))\nsns.stripplot(data=train_df, x = 'OverallQual', y='SalePrice', ax = ax[0])\nsns.violinplot(data=train_df, x = 'OverallQual', y='SalePrice', ax = ax[1])\nsns.boxplot(data=train_df, x = 'OverallQual', y='SalePrice', ax = ax[2])\nplt.show()","f731e67c":"Pearson_GrLiv = 0.71\nplt.figure(figsize = (12,6))\nsns.regplot(data=train_df, x = 'GrLivArea', y='SalePrice', scatter_kws={'alpha':0.2})\nplt.title('GrLivArea vs SalePrice', fontsize = 12)\nplt.legend(['$Pearson=$ {:.2f}'.format(Pearson_GrLiv)], loc = 'best')\nplt.show()","2aa0ed29":"figure, ax = plt.subplots(1,3, figsize = (20,8))\nsns.stripplot(data=train_df, x = 'GarageCars', y='SalePrice', ax = ax[0])\nsns.violinplot(data=train_df, x = 'GarageCars', y='SalePrice', ax = ax[1])\nsns.boxplot(data=train_df, x = 'GarageCars', y='SalePrice', ax = ax[2])\nplt.show()","b6396ce0":"figure, ax = plt.subplots(1,3, figsize = (20,8))\nsns.stripplot(data=train_df, x = 'GarageArea', y='SalePrice', ax = ax[0])\nsns.violinplot(data=train_df, x = 'GarageArea', y='SalePrice', ax = ax[1])\nsns.boxplot(data=train_df, x = 'GarageArea', y='SalePrice', ax = ax[2])\nplt.show()","e40c2655":"Pearson_TBSF = 0.61\nplt.figure(figsize = (12,6))\nsns.regplot(data=train_df, x = 'TotalBsmtSF', y='SalePrice', scatter_kws={'alpha':0.2})\nplt.title('TotalBsmtSF vs SalePrice', fontsize = 12)\nplt.legend(['$Pearson=$ {:.2f}'.format(Pearson_TBSF)], loc = 'best')\nplt.show()","427a1104":"# first we will make copies of our original datasets\ntrain = train_df.copy()\ntest = test_df.copy()","e4f22487":"train.info()","4a6bed35":"target = train['SalePrice']\ntest_id = test['Id']\ntest = test.drop(['Id'],axis = 1)\ntrain_final = train.drop(['Id'], axis = 1)\n\n# Concatenating train & test set\ntrain_test = pd.concat([train_final,test], axis=0, sort=False)","8b5261cf":"# Converting non-numeric predictors stored as numbers into string\n\ntrain_test['MSSubClass'] = train_test['MSSubClass'].apply(str)\ntrain_test['YrSold'] = train_test['YrSold'].apply(str)\ntrain_test['MoSold'] = train_test['MoSold'].apply(str)","fcc39609":"# variable containing the columns that we want to remove\ncols_missing_data = ['Alley','PoolQC', 'Fence', 'MiscFeature']\nuseless = ['GarageYrBlt','YearRemodAdd']\n\n# drop columns\ntrain_test = train_test.drop(cols_missing_data, axis=1)\ntrain_test = train_test.drop(useless, axis=1)","bf271fed":"train_test['Functional'] = train_test['Functional'].fillna('Typ')\ntrain_test['Electrical'] = train_test['Electrical'].fillna(\"SBrkr\")\ntrain_test['KitchenQual'] = train_test['KitchenQual'].fillna(\"TA\")\ntrain_test['Exterior1st'] = train_test['Exterior1st'].fillna(train_test['Exterior1st'].mode()[0])\ntrain_test['Exterior2nd'] = train_test['Exterior2nd'].fillna(train_test['Exterior2nd'].mode()[0])\ntrain_test['SaleType'] = train_test['SaleType'].fillna(train_test['SaleType'].mode()[0])\ntrain_test['FireplaceQu'] = train_test['FireplaceQu'].fillna(\"None\")\n\nfor col in ('GarageArea', 'GarageCars'):\n    train_test[col] = train_test[col].fillna(0)\n        \nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    train_test[col] = train_test[col].fillna('None')\n    \nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    train_test[col] = train_test[col].fillna('None')\n    \n    # Checking the features with NaN remained out\n\nfor col in train_test:\n    if train_test[col].isna().sum() > 0:\n        print(train_test[col][0])","14147d62":"train_test_dummy = pd.get_dummies(train_test)","901b3b7f":"# Fetch all numeric features\n\n#train_test['Id'] = train_test['Id'].apply(str)\nnumeric_features = train_test_dummy.dtypes[train_test_dummy.dtypes != object].index\nskewed_features = train_test_dummy[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skew = skewed_features[skewed_features > 0.5]\nskew_index = high_skew.index\n\n# Normalize skewed features using log_transformation\n    \nfor i in skew_index:\n    train_test_dummy[i] = np.log1p(train_test_dummy[i])","f27c351b":"train_test_dummy=train_test_dummy.fillna(train_test_dummy.mean())","ff8249f7":"# SalePrice before transformation\n\nfig, ax = plt.subplots(1,2, figsize= (15,5))\nfig.suptitle(\" qq-plot & distribution SalePrice \", fontsize= 15)\n\nsm.qqplot(target, stats.t, distargs=(4,),fit=True, line=\"45\", ax = ax[0])\n\nsns.distplot(target, kde = True, hist=True, fit = norm, ax = ax[1])\nplt.show()","f9209c5a":"# SalePrice after transformation\n\ntarget_log = np.log1p(target)\n\nfig, ax = plt.subplots(1,2, figsize= (15,5))\nfig.suptitle(\"qq-plot & distribution SalePrice \", fontsize= 15)\n\nsm.qqplot(target_log, stats.t, distargs=(4,),fit=True, line=\"45\", ax = ax[0])\nsns.distplot(target_log, kde = True, hist=True, fit = norm, ax = ax[1])\nplt.show()","3f376df6":"# Train-Test separation\ntrain_final = train_test_dummy[0:1460]\ntest_final = train_test_dummy[1460:]","c9f2832d":"train_final.describe()","6d2bac82":"test_final.describe()","3d56f392":"X = train_final\nX","a6f5fc67":"X.shape","31bbbbe3":"y = target_log","831c9aab":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=64)","e9962272":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","399a9cbb":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nlm = LinearRegression()\n\nlm.fit(X_train,y_train)\ny_pred_train=lm.predict(X_train)\ny_pred_test=lm.predict(X_test)\n\nprint('Root Mean Square Error train = ' + str(np.sqrt(mean_squared_error(y_train, y_pred_train))))\nprint('Root Mean Square Error test = ' + str(np.sqrt(mean_squared_error(y_test, y_pred_test))))","babd406b":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Ridge\n\nlm_ridge=Ridge()\nparameters= {'alpha':[x for x in [0.0005,0.001,0.01,0.1,0.2,0.4,0.5,0.7,0.8,1]]}\n\nlm_ridge=GridSearchCV(lm_ridge, param_grid=parameters)\nlm_ridge.fit(X_train,y_train)\nprint(\"The best value of Alpha is: \",lm_ridge.best_params_,lm_ridge.best_score_)","d0170f7e":"lm_ridge_mod=Ridge(alpha=1)\nlm_ridge_mod.fit(X_train,y_train)\ny_pred_train=lm_ridge_mod.predict(X_train)\ny_pred_test=lm_ridge_mod.predict(X_test)\n\nprint('Root Mean Square Error train = ' + str(np.sqrt(mean_squared_error(y_train, y_pred_train))))\nprint('Root Mean Square Error test = ' + str(np.sqrt(mean_squared_error(y_test, y_pred_test)))) ","44deac54":"from sklearn.linear_model import Lasso\n\nlm_lasso =Lasso()\nparameters= {'alpha':[x for x in [0.0005,0.001,0.01,0.1,0.2,0.4,0.5,0.7,0.8,1]]}\n\nlm_lasso=GridSearchCV(lm_lasso, param_grid=parameters)\nlm_lasso.fit(X_train,y_train)\nprint(\"The best value of Alpha is: \",lm_lasso.best_params_,lm_lasso.best_score_)","c1e88ce0":"lm_lasso_mod =Lasso(alpha=0.0005)\nlm_lasso_mod.fit(X_train,y_train)\ny_pred_train=lm_lasso_mod.predict(X_train)\ny_pred_test=lm_lasso_mod.predict(X_test)\n\nprint('Root Mean Square Error train = ' + str(np.sqrt(mean_squared_error(y_train, y_pred_train))))\nprint('Root Mean Square Error test = ' + str(np.sqrt(mean_squared_error(y_test, y_pred_test)))) ","02a2b0e7":"from sklearn.linear_model import ElasticNet\n\nlm_elastic = ElasticNet()\nparameters= {'alpha':[x for x in [0.0005,0.001,0.01,0.1,0.2,0.4,0.5,0.7,0.8,1]]}\n\nlm_elastic = GridSearchCV(lm_elastic, param_grid=parameters)\nlm_elastic.fit(X_train,y_train)\nprint(\"The best value of Alpha is: \",lm_elastic.best_params_,lm_elastic.best_score_)","a0552039":"lm_elastic_mod = ElasticNet(alpha=0.0005)\nlm_elastic_mod.fit(X_train,y_train)\ny_pred_train=lm_elastic_mod.predict(X_train)\ny_pred_test=lm_elastic_mod.predict(X_test)\n\nprint('Root Mean Square Error train = ' + str(np.sqrt(mean_squared_error(y_train, y_pred_train))))\nprint('Root Mean Square Error test = ' + str(np.sqrt(mean_squared_error(y_test, y_pred_test))))","e09f2724":"test_final.shape","6081480a":"preds = lm_ridge_mod.predict(test_final)\npreds = np.exp(preds)","9e0a5310":"my_submission = pd.DataFrame({'Id': test_id, 'SalePrice': preds})","fb1ef3eb":"my_submission.head()","c4f7453b":"my_submission.to_csv(\"ridge2_sol.csv\", index = False)","ce4453b2":"This is a standard supervised regression task. Whit 79 explanatory variables describing most aspects of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n\nGoals: \n* Do a comprehensive data analysis along with visualizations.\n* Learn\/review\/explain different regression algorithms and its hyperparameters.\n* Create models that are well equipped to predict better sale price of the the house.","d3a2319f":"### Numerical Features Transformation","442d3818":"### Categorical Features Transformation","5b7eb2e0":"#### Distribution","a0b5e107":"### Exploration of our Target Feature: SalePrice","5b29a259":"To model this data we will start by using a Linear Regression model.","bbc08a4a":"The Ames Housing dataset was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset. \n\nAll the different variables come described at the file data_description.txt, said that, we have to have in mind that we need to focus on the target variable, which is SalePrice.","5ee83b5d":"### The Data","c52be960":"### Regularization Models\n\nWhat makes regression model more effective is its ability of regularizing. The term \"regularizing\" stands for models ability to structurally prevent overfitting by imposing a penalty on the coefficients.\n\nThere are three types of regularizations.\n\n* Ridge\n* Lasso\n* Elastic Net\n\nThese regularization methods work by penalizing the magnitude of the coefficients of features and at the same time minimizing the error between the predicted value and actual observed values. \n\nThe only difference between Ridge and Lasso is the way they penalize the coefficients. Elastic Net is the combination of these two. Elastic Net adds both the sum of the squares errors and the absolute value of the squared error.","6a68936a":"Now that we have some insights about data, we need to preprocess them for the modeling part. The main steps are:\n\n* Looking at potential NaN\n* Dealing with categorical features (e.g. Dummy coding)\n* Normalization\n\nNote from @marto24:\nUsually, in a real-world project, the test data are not available until the end. For this reason, test data should contain the same type of data of the training set to preprocess them in the same way. Here, the test set is available. In this case, the creation of dummy variables has risen several issues when trying to make predictions on the test set, the easiest way to solve this problem is to concatenate Train and Test sets, preprocess, and divide them again.","569c9e74":"### Missing Values\n\nAs you can see we have some missing data, let's have a look how many we have for each column at the train_df","9bbf908d":"Welcome! Let's do some regression.","c672c7d3":"#### Ridge","0d074406":"# House Prices - Linear model","e989a34e":"It is important to check which columns contain empty values, as most ML models don't work when these are present. In this case, we can see that there are some values with a great amount of missing values, such as PoolQc, MiscFeature, Alley, Fence and FireplaceQu. \n\nAs we said, to be able to continue with our analysis we can't have any missing value, so what we will do is:\n* We will drop all columns where the data missing ratio > 50%. We could observe below that \"Alley\", \"Fence\", \"MiscFeature\", \"PoolQC\" are probaly the 5 first columns we will remove. Also, \"LotFrontage\" could be the next column that we have to consider.\n\n* Dealing with the numerical missing data by replacing column's mean value, and the object columns will be considered in the next section.\n\nBefore continuing, we will how many missing values are found in the test dataset, as we should make the same changes in both.","60bee72a":"TOC:\n1. [Problem Framing](#t1.)\n2. [Assess Data Quality & Missing Values](#t2.)\n3. [Exploratory Data Analysis](#t3.)\n4. [Feature Engineering](#t4.)\n5. [Shortlisting Promising ML Models](#t5.)\n6. [Fine-Tune the System](#t6.)\n7. [Presenting the Solution](#t7.)","3933d4e2":"### SalePrice vs GrLivArea","80df19b4":"<a id=\"t1.\"><\/a>\n## 1. Problem Framing","8171c69a":"### Preparing the tools","02646a52":"### Load the Data","de4724a6":"#### Filling Categorical NaN (That we know how to fill due to the description file)","46f2fe19":"Looking at the obtained results, the model giving us better results is the lasso model using an alpha = 1 as the parameter.","4443d207":"<a id=\"t2.\"><\/a>\n## 3. Exploration Data Analysis","4c8d7882":"### Train-Test Split\n* X_train, y_train first used to train the algorithm.\n* then, X_test is used in that trained algorithms to predict outcomes.\n* Once we get the outcomes, we compare it with y_test","6dc6ec10":"### Train-Test Separation and Split","20da621c":"<a id=\"t6.\"><\/a>\n## 6. Fine-Tune the Model","1db8ac00":"This is much better. It's not perfect, but using a logarithm conversion is one of the simpliest ways to obtain a normal distribution function, so we will apply this in the next section to train the model.","dcbbdf80":"### Key Variable Correlations","34bea872":"#### Elastic Net","d670c92b":"### Fixing Data Types","919edf44":"### SalePrice vs OverallQual\n\nOverall material and finish quality","597b2cf7":"### Separate Target and Features","5fbc1080":"### Linear Regression","1caae5d4":"We can see from these observations that the SALEPRICE seem to be strongly-POSITIVE correlated to:\n\n* OverallQual: 0.790982\n* GrLivArea: 0.708624\n* GarageCars: 0.640409\n* GarageArea: 0.623431\n* TotalBsmtSF: 0.613581\n\nWhich means that as one variable increases, the SalePrice value also increases. We will stop here and select these variables to analyse, eventhough there are probably other variable that should be considered in deep.","1d969fb7":"### Data Types\nWe can see our dataset consist on different types of data. As we continue to analyze it, we'll find features which are numerical and should actually be categorial.","2d94d90f":"### SalePrice vs GarageArea","43faea19":"<a id=\"t7.\"><\/a>\n## 7. Presenting the Solution","4875468f":"From these charts we can learn a lot about our target variable.\n* Our target variable, SalePrice is not normally distributed\n* Our target variable is right-skewed (can be fixed with different types of transformation)\n* There are multiple outliers in the variable\n\nMachine learning algorithms only work properly on normal distribution data, so we want to transform our target feature so that is as \"normal\" as possible. The simplest wat to correct the distribution is by taking the logarithm of the value.\n\nLet's check a simplest way to correct the distribution of SalePrice by taking logarithm of the value.","62a664f7":"As you can see, there are two outliers in the plot above. We will get rid off them later. Let's look at another scatter plot with a different feature.","23d0e4a1":"### Preprocess the test_df","a83343e4":"References:\n    * https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models\n    * https:\/\/www.kaggle.com\/masumrumi\/a-detailed-regression-guide-with-house-pricing","1d60a846":"There is an apparent relationship between the two features. The price of the houses increases with the overall quality. Let's check out some more features to determine the outliers. Let's focus on the numerical variables this time.","364f540b":"#### Filling Numerical NaN","7774258c":"The correlation matrix is the best way to see all the numerical correlation between features. Let's see which are the feature that correlate most with our target variable.","af197032":"#### Lasso","9d1f2670":"### Dropping Features Pipeline","698f0813":"### Problem Definition","112dad03":"### SalePrice vs GarageCars","5b50a43f":"### SalePrice vs TotalBcmtSF","7d6d46ac":"#### Creating Dummy Variables","9174a393":"<a id=\"t2.\"><\/a>\n## 2. Data Quality & Missing Values Assessment","c1439f11":"### Model Result Submission","311b3431":"#### Target Feature Tranformation","33e3cea6":"We can see that the missing values are very similar in both datasets. We will remove 4 columns in both datasets. After exploring our data, we will deal with these.","2f1bb41b":"### Predict","5c841f09":"<a id=\"t4.\"><\/a>\n## 4. Feature Engineering","c80d5315":"<a id=\"t5.\"><\/a>\n## 5. Shortlisting Promising ML Models"}}