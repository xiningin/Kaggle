{"cell_type":{"0276406b":"code","646ea3e9":"code","0760bf45":"code","498c8cba":"code","c8e38ef5":"code","55066bf0":"code","c4e12f26":"code","ecf3df74":"code","6a064020":"code","ecac13e1":"code","bc8b83a7":"code","7eaeab76":"code","db9eb0a6":"code","782e5105":"code","a6406563":"code","e6304a0e":"code","e43ecbe5":"code","76ee9abe":"code","c532c1a2":"code","1c7a9605":"code","d1a64590":"code","19d20b4d":"code","c89c8ad3":"markdown","2c1f364a":"markdown","b4ed9620":"markdown","e9f71b9b":"markdown","2f5f2c47":"markdown","aadb0fd1":"markdown","606c4aaa":"markdown","4e339605":"markdown","fae93319":"markdown","fe21109a":"markdown","c29d8d65":"markdown","a38b3c7b":"markdown","d85db391":"markdown","970a26df":"markdown","14b7c4d9":"markdown","74103b5d":"markdown","401ccf4a":"markdown","011584c4":"markdown","e68806b2":"markdown","9ca503f7":"markdown","99047854":"markdown","342932e8":"markdown","3205a25c":"markdown","52255643":"markdown","79904da2":"markdown","fe6793b7":"markdown","188984e7":"markdown","84e7a9f3":"markdown","b2ce30e0":"markdown","0bbc36f1":"markdown","3cb06921":"markdown","6cf7300d":"markdown","fd5b5d5c":"markdown"},"source":{"0276406b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # Plot\n%matplotlib inline  \nimport os\nimport seaborn as sns # Plot\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nsns.set(style=\"darkgrid\") # Style seaborn\nsns.set(color_codes=True) # Color seaborn \n\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.preprocessing import StandardScaler,RobustScaler,MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error, accuracy_score\nfrom sklearn.pipeline import make_pipeline\n#Importing Confusion Matrix\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.ensemble import RandomForestClassifier\nfrom scipy.cluster.hierarchy import linkage,cophenet,dendrogram,fcluster\nfrom scipy.spatial.distance import pdist\nfrom scipy.spatial import distance_matrix\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# Cargando el la ruta de los archivos\nPath_train = \"..\/input\/learn-together\/train.csv\"\nPath_test = \"..\/input\/learn-together\/test.csv\"\n#send = pd.read_csv(\"..\/input\/comp-01\/best_submission.csv\",index_col='Id')# ->>\"test.csv\"\n#Cargando los data sets\ndf_train = pd.read_csv(Path_train,index_col='Id')# ->> \"train.csv\"\ndf_test = pd.read_csv(Path_test,index_col='Id')# ->>\"test.csv\"\ndel df_train['Soil_Type7']\ndel df_test['Soil_Type7']\ndel df_train['Soil_Type15']\ndel df_test['Soil_Type15']\ndata_train = df_train.copy()\ndata_test = df_test.copy()","646ea3e9":"def split(df, headSize):\n    hd = df.head(headSize)\n    tl = df.tail(len(df)-headSize)\n    return hd, tl\n\n# Function for comparing different approaches\ndef score_dataset(data):\n    y = data.Cover_Type.values # <- Target\n    X = data # Data\n    X.drop(['Cover_Type'], axis=1, inplace=True) # Drop target\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n      \n    X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.80, random_state=2019)\n    \n    RF = RandomForestClassifier(n_jobs =  -1, n_estimators = 1000,random_state = 1)\n    RF.fit(X=X_train, y=y_train)\n    pred = RF.predict(X_val)\n    mae = mean_absolute_error(pred,y_val ) \n    acc = round(accuracy_score(y_val, pred),4)\n    return acc\n    \n# Function for data join    \ndef data_join(x,y,z,c=0):\n    if (c == 0):\n        frames = [x,y]\n        data_join = pd.concat(frames,join='inner',axis=1)\n        return data_join\n    else:\n        frames = [x,y,z]\n        data_join = pd.concat(frames,join='inner',axis=1)\n        return data_join\n\n# Visual data comparison function \ndef compare_plot(data):\n    sns.pairplot(data, palette=\"RdBu\",diag_kind=\"kde\",hue='Cover_Type',height=2.5)\n    plt.show()\n    \n# Multiple histogram    \ndef histogram(data):\n    f, ax = plt.subplots(figsize=(10, 10))\n    data.hist(ax=ax,align='mid',orientation='vertical',bins=52)\n    plt.show()    ","0760bf45":"data_train.iloc[:,:10].head().transpose()","498c8cba":"data_train.iloc[:,10:52].head()","c8e38ef5":"group_1 = data_join(data_train.iloc[:,:10], data_train[['Cover_Type']],None)\ncompare_plot(group_1)\ndata = abs(data_train.copy())\ntraining_score = score_dataset(df_train.copy())\ntransformed_training_score = score_dataset(data)\nprint(\"Previous Accuracy :\",training_score,\"Current Accuracy :\",transformed_training_score)\nprint(\"Train :\",df_train.copy().shape)\nprint(\"Test :\",df_test.copy().shape)","55066bf0":"train = abs(df_train.copy())\ncols_1 = ['Elevation','Aspect','Slope']\ncols_2 = ['Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology']\ncols_3 = ['Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Fire_Points']\ncols_4 = ['Hillshade_9am','Hillshade_Noon','Hillshade_3pm']\n\n# Elevation\ntrain['Elevation_sin_'] = abs(np.ceil(np.sin(train['Elevation'])))\ntrain['Elevation_cos_'] = abs(np.ceil(np.cos(train['Elevation'])))\ntrain['Elevation_tanh_'] = abs(np.ceil(np.tanh(train['Elevation'])))\ntrain['Elevation_bin_round_100'] = np.array(np.floor(np.array(train['Elevation']) \/ 100))\ntrain['Elevation_bin_round_1000'] = np.array(np.floor(np.array(train['Elevation']) \/ 1000))\ntrain['Elevation_sqrt'] = np.ceil(np.sqrt(np.floor(np.array(train['Elevation']))))\ntrain['Elevation_log'] = np.ceil(np.log(np.floor(np.array(1+train['Elevation']))))\n# Aspect\ntrain['Aspect_sin_'] = abs(np.ceil(np.sin(train['Aspect'])))\ntrain['Aspect_cos_'] = abs(np.ceil(np.cos(train['Aspect'])))\ntrain['Aspect_tanh_'] = abs(np.ceil(np.tanh(train['Aspect'])))\ntrain['Aspect_bin_round_100'] = np.array(np.floor(np.array(train['Aspect']) \/ 100))\ntrain['Aspect_bin_round_1000'] = np.array(np.floor(np.array(train['Aspect']) \/ 1000))\ntrain['Aspect_sqrt'] = np.ceil(np.sqrt(np.floor(np.array(train['Aspect']))))\ntrain['Aspect_log'] = np.ceil(np.log(np.floor(np.array(1+train['Aspect']))))\n# Slope\ntrain['Slope_sin_'] = abs(np.ceil(np.sin(train['Slope'])))\ntrain['Slope_cos_'] = abs(np.ceil(np.cos(train['Slope'])))\ntrain['Slope_tanh_'] = abs(np.ceil(np.tanh(train['Slope'])))\ntrain['Slope_bin_round_100'] = np.array(np.floor(np.array(train['Slope']) \/ 100))\ntrain['Slope_bin_round_1000'] = np.array(np.floor(np.array(train['Slope']) \/ 1000))\ntrain['Slope_sqrt'] = np.ceil(np.sqrt(np.floor(np.array(train['Slope']))))\ntrain['Slope_log'] = np.ceil(np.log(np.floor(np.array(1+train['Slope']))))\n# Basic statistics - Elev_Asp_Slope\ntrain['Elev_Asp_Slope_sum'] = train[cols_2].sum(axis=1)\ntrain['Elev_Asp_Slope_mean'] = train[cols_2].mean(axis=1)\ntrain['Elev_Asp_Slope_std'] = np.round(train[cols_2].std(axis=1),3)\ntrain['Elev_Asp_Slope_min'] = train[cols_2].min(axis=1)\ntrain['Elev_Asp_Slope_max'] = train[cols_2].max(axis=1)\ntrain['Elev_Asp_Slope_median'] = train[cols_2].median(axis=1)\ntrain['Elev_Asp_Slope_quantile'] = train[cols_2].quantile(axis=1)\n# Horizontal_Distance_To_Hydrology\ntrain['Horizontal_Distance_To_Hydrology_sin_'] = abs(np.ceil(np.sin(train['Horizontal_Distance_To_Hydrology'])))\ntrain['Horizontal_Distance_To_Hydrology_cos_'] = abs(np.ceil(np.cos(train['Horizontal_Distance_To_Hydrology'])))\ntrain['Horizontal_Distance_To_Hydrology_tanh_'] = abs(np.ceil(np.tanh(train['Horizontal_Distance_To_Hydrology'])))\ntrain['Horizontal_Distance_To_Hydrology_bin_round_100'] = np.array(np.floor(np.array(train['Horizontal_Distance_To_Hydrology']) \/ 100))\ntrain['Horizontal_Distance_To_Hydrology_bin_round_1000'] = np.array(np.floor(np.array(train['Horizontal_Distance_To_Hydrology']) \/ 1000))\ntrain['Horizontal_Distance_To_Hydrology_sqrt'] = np.ceil(np.sqrt(np.floor(np.array(train['Horizontal_Distance_To_Hydrology']))))\ntrain['Horizontal_Distance_To_Hydrology_log'] = np.ceil(np.log(np.floor(np.array(1+train['Horizontal_Distance_To_Hydrology']))))\n# Vertical_Distance_To_Hydrology\ntrain['Vertical_Distance_To_Hydrology_sin_'] = abs(np.ceil(np.sin(train['Vertical_Distance_To_Hydrology'])))\ntrain['Vertical_Distance_To_Hydrology_cos_'] = abs(np.ceil(np.cos(train['Vertical_Distance_To_Hydrology'])))\ntrain['Vertical_Distance_To_Hydrology_tanh_'] = abs(np.ceil(np.tanh(train['Vertical_Distance_To_Hydrology'])))\ntrain['Vertical_Distance_To_Hydrology_bin_round_100'] = np.array(np.floor(np.array(train['Vertical_Distance_To_Hydrology']) \/ 100))\ntrain['Vertical_Distance_To_Hydrology_bin_round_1000'] = np.array(np.floor(np.array(train['Vertical_Distance_To_Hydrology']) \/ 1000))\ntrain['Vertical_Distance_To_Hydrology_sqrt'] = np.ceil(np.sqrt(np.floor(np.array(train['Vertical_Distance_To_Hydrology']))))\ntrain['Vertical_Distance_To_Hydrology_log'] = np.ceil(np.log(np.floor(np.array(1+train['Vertical_Distance_To_Hydrology']))))\n# Basic statistics - Horizontal_Distance_To_Hydrology_+_Vertical_Distance_To_Hydrology\ntrain['Horizontal_Distance_To_Hydrology_+_Vertical_Distance_To_Hydrology_sum'] = train[cols_1].sum(axis=1)\ntrain['Horizontal_Distance_To_Hydrology_+_Vertical_Distance_To_Hydrology_mean'] = train[cols_1].mean(axis=1)\ntrain['Horizontal_Distance_To_Hydrology_+_Vertical_Distance_To_Hydrology_std'] = np.round(train[cols_1].std(axis=1))\ntrain['Horizontal_Distance_To_Hydrology_+_Vertical_Distance_To_Hydrology_min'] = train[cols_1].min(axis=1)\ntrain['Horizontal_Distance_To_Hydrology_+_Vertical_Distance_To_Hydrology_max'] = train[cols_1].max(axis=1)\ntrain['Horizontal_Distance_To_Hydrology_+_Vertical_Distance_To_Hydrology_median'] = train[cols_1].median(axis=1)\ntrain['Horizontal_Distance_To_Hydrology_+_Vertical_Distance_To_Hydrology_quantile'] = train[cols_1].quantile(axis=1)\n# Horizontal_Distance_To_Roadways\ntrain['Horizontal_Distance_To_Roadways_sin_'] = abs(np.ceil(np.sin(train['Horizontal_Distance_To_Roadways'])))\ntrain['Horizontal_Distance_To_Roadways_cos_'] = abs(np.ceil(np.cos(train['Horizontal_Distance_To_Roadways'])))\ntrain['Horizontal_Distance_To_Roadways_tanh_'] = abs(np.ceil(np.tanh(train['Horizontal_Distance_To_Roadways'])))\ntrain['Horizontal_Distance_To_Roadways_bin_round_100'] = np.array(np.floor(np.array(train['Horizontal_Distance_To_Roadways']) \/ 100))\ntrain['Horizontal_Distance_To_Roadways_bin_round_1000'] = np.array(np.floor(np.array(train['Horizontal_Distance_To_Roadways']) \/ 1000))\ntrain['Horizontal_Distance_To_Roadways_sqrt'] = np.ceil(np.sqrt(np.floor(np.array(train['Horizontal_Distance_To_Roadways']))))\ntrain['Horizontal_Distance_To_Roadways_log'] = np.ceil(np.log(np.floor(np.array(1+train['Horizontal_Distance_To_Roadways']))))\n# Hillshade_9am\ntrain['Hillshade_9am_sin_'] = abs(np.ceil(np.sin(train['Hillshade_9am'])))\ntrain['Hillshade_9am_cos_'] = abs(np.ceil(np.cos(train['Hillshade_9am'])))\ntrain['Hillshade_9am_tanh_'] = abs(np.ceil(np.tanh(train['Hillshade_9am'])))\ntrain['Hillshade_9am_bin_round_100'] = np.array(np.floor(np.array(train['Hillshade_9am']) \/ 100))\ntrain['Hillshade_9am_bin_round_1000'] = np.array(np.floor(np.array(train['Hillshade_9am']) \/ 1000))\ntrain['Hillshade_9am_sqrt'] = np.ceil(np.sqrt(np.floor(np.array(train['Hillshade_9am']))))\ntrain['Hillshade_9am_log'] = np.ceil(np.log(np.floor(np.array(1+train['Hillshade_9am']))))\n# Hillshade_Noon\ntrain['Hillshade_Noon_sin_'] = abs(np.ceil(np.sin(train['Hillshade_Noon'])))\ntrain['Hillshade_Noon_cos_'] = abs(np.ceil(np.cos(train['Hillshade_Noon'])))\ntrain['Hillshade_Noon_tanh_'] = abs(np.ceil(np.tanh(train['Hillshade_Noon'])))\ntrain['Hillshade_Noon_bin_round_100'] = np.array(np.floor(np.array(train['Hillshade_Noon']) \/ 100))\ntrain['Hillshade_Noon_bin_round_1000'] = np.array(np.floor(np.array(train['Hillshade_Noon']) \/ 1000))\ntrain['Hillshade_Noon_sqrt'] = np.ceil(np.sqrt(np.floor(np.array(train['Hillshade_Noon']))))\ntrain['Hillshade_Noon_log'] = np.ceil(np.log(np.floor(np.array(1+train['Hillshade_Noon']))))\n# Hillshade_3pm\ntrain['Hillshade_3pm_sin_'] = abs(np.ceil(np.sin(train['Hillshade_3pm'])))\ntrain['Hillshade_3pm_cos_'] = abs(np.ceil(np.cos(train['Hillshade_3pm'])))\ntrain['Hillshade_3pm_tanh_'] = abs(np.ceil(np.tanh(train['Hillshade_3pm'])))\ntrain['Hillshade_3pm_bin_round_100'] = np.array(np.floor(np.array(train['Hillshade_3pm']) \/ 100))\ntrain['Hillshade_3pm_bin_round_1000'] = np.array(np.floor(np.array(train['Hillshade_3pm']) \/ 1000))\ntrain['Hillshade_3pm_sqrt'] = np.ceil(np.sqrt(np.floor(np.array(train['Hillshade_3pm']))))\ntrain['Hillshade_3pm_log'] = np.ceil(np.log(np.floor(np.array(1+train['Hillshade_3pm']))))\n# Horizontal_Distance_To_Fire_Points\ntrain['Horizontal_Distance_To_Fire_Points_sin_'] = abs(np.ceil(np.sin(train['Horizontal_Distance_To_Fire_Points'])))\ntrain['Horizontal_Distance_To_Fire_Points_cos_'] = abs(np.ceil(np.cos(train['Horizontal_Distance_To_Fire_Points'])))\ntrain['Horizontal_Distance_To_Fire_Points_tanh_'] = abs(np.ceil(np.tanh(train['Horizontal_Distance_To_Fire_Points'])))\ntrain['Horizontal_Distance_To_Fire_Points_bin_round_100'] = np.array(np.floor(np.array(train['Horizontal_Distance_To_Fire_Points']) \/ 100))\ntrain['Horizontal_Distance_To_Fire_Points_bin_round_1000'] = np.array(np.floor(np.array(train['Horizontal_Distance_To_Fire_Points']) \/ 1000))\ntrain['Horizontal_Distance_To_Fire_Points_sqrt'] = np.ceil(np.sqrt(np.floor(np.array(train['Horizontal_Distance_To_Fire_Points']))))\ntrain['Horizontal_Distance_To_Fire_Points_log'] = np.ceil(np.log(np.floor(np.array(1+train['Horizontal_Distance_To_Fire_Points']))))\n# Basic statistics - Horizontal_Distance_To_Roadways_+_Horizontal_Distance_To_Fire_Points\ntrain['Horizontal_Distance_To_Roadways_+_Horizontal_Distance_To_Fire_Points_sum'] = train[cols_3].sum(axis=1)\ntrain['Horizontal_Distance_To_Roadways_+_Horizontal_Distance_To_Fire_Points_mean'] = train[cols_3].mean(axis=1)\ntrain['Horizontal_Distance_To_Roadways_+_Horizontal_Distance_To_Fire_Points_std'] = np.round(train[cols_3].std(axis=1),3)\ntrain['Horizontal_Distance_To_Roadways_+_Horizontal_Distance_To_Fire_Points_min'] = train[cols_3].min(axis=1)\ntrain['Horizontal_Distance_To_Roadways_+_Horizontal_Distance_To_Fire_Points_max'] = train[cols_3].max(axis=1)\ntrain['Horizontal_Distance_To_Roadways_+_Horizontal_Distance_To_Fire_Points_median'] = train[cols_3].median(axis=1)\ntrain['Horizontal_Distance_To_Roadways_+_Horizontal_Distance_To_Fire_Points_quantile'] = train[cols_3].quantile(axis=1)\n# Basic statistics - Hillshade_9am_+_Hillshade_Noon_+_Hillshade_3pm\ntrain['Hillshade_9am_+_Hillshade_Noon_+_Hillshade_3pm_sum'] = train[cols_4].sum(axis=1)\ntrain['Hillshade_9am_+_Hillshade_Noon_+_Hillshade_3pm_mean'] = train[cols_4].mean(axis=1)\ntrain['Hillshade_9am_+_Hillshade_Noon_+_Hillshade_3pm_std'] = np.round(train[cols_4].std(axis=1),3)\ntrain['Hillshade_9am_+_Hillshade_Noon_+_Hillshade_3pm_min'] = train[cols_4].min(axis=1)\ntrain['Hillshade_9am_+_Hillshade_Noon_+_Hillshade_3pm_max'] = train[cols_4].max(axis=1)\ntrain['Hillshade_9am_+_Hillshade_Noon_+_Hillshade_3pm_median'] = train[cols_4].median(axis=1)\ntrain['Hillshade_9am_+_Hillshade_Noon_+_Hillshade_3pm_quantile'] = train[cols_4].quantile(axis=1)\nprint(\"Train Size : \",train.shape)\nprint(\"previous accuracy :\",training_score,\"current accuracy :\",score_dataset(train.copy()))\nprint(\"New features have been added...\")\n#d_train.iloc[:,52:].head()","c4e12f26":"test = abs(df_test.copy())\ncols_1 = ['Elevation','Aspect','Slope']\ncols_2 = ['Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology']\ncols_3 = ['Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Fire_Points']\ncols_4 = ['Hillshade_9am','Hillshade_Noon','Hillshade_3pm']\n\n# Elevation\ntest['Elevation_sin_'] = abs(np.ceil(np.sin(test['Elevation'])))\ntest['Elevation_cos_'] = abs(np.ceil(np.cos(test['Elevation'])))\ntest['Elevation_tanh_'] = abs(np.ceil(np.tanh(test['Elevation'])))\ntest['Elevation_bin_round_100'] = np.array(np.floor(np.array(test['Elevation']) \/ 100))\ntest['Elevation_bin_round_1000'] = np.array(np.floor(np.array(test['Elevation']) \/ 1000))\ntest['Elevation_sqrt'] = np.ceil(np.sqrt(np.floor(np.array(test['Elevation']))))\ntest['Elevation_log'] = np.ceil(np.log(np.floor(np.array(1+test['Elevation']))))\n# Aspect\ntest['Aspect_sin_'] = abs(np.ceil(np.sin(test['Aspect'])))\ntest['Aspect_cos_'] = abs(np.ceil(np.cos(test['Aspect'])))\ntest['Aspect_tanh_'] = abs(np.ceil(np.tanh(test['Aspect'])))\ntest['Aspect_bin_round_100'] = np.array(np.floor(np.array(test['Aspect']) \/ 100))\ntest['Aspect_bin_round_1000'] = np.array(np.floor(np.array(test['Aspect']) \/ 1000))\ntest['Aspect_sqrt'] = np.ceil(np.sqrt(np.floor(np.array(test['Aspect']))))\ntest['Aspect_log'] = np.ceil(np.log(np.floor(np.array(1+test['Aspect']))))\n# Slope\ntest['Slope_sin_'] = abs(np.ceil(np.sin(test['Slope'])))\ntest['Slope_cos_'] = abs(np.ceil(np.cos(test['Slope'])))\ntest['Slope_tanh_'] = abs(np.ceil(np.tanh(test['Slope'])))\ntest['Slope_bin_round_100'] = np.array(np.floor(np.array(test['Slope']) \/ 100))\ntest['Slope_bin_round_1000'] = np.array(np.floor(np.array(test['Slope']) \/ 1000))\ntest['Slope_sqrt'] = np.ceil(np.sqrt(np.floor(np.array(test['Slope']))))\ntest['Slope_log'] = np.ceil(np.log(np.floor(np.array(1+test['Slope']))))\n# Basic statistics - Elev_Asp_Slope\ntest['Elev_Asp_Slope_sum'] = test[cols_2].sum(axis=1)\ntest['Elev_Asp_Slope_mean'] = test[cols_2].mean(axis=1)\ntest['Elev_Asp_Slope_std'] = np.round(test[cols_2].std(axis=1),3)\ntest['Elev_Asp_Slope_min'] = test[cols_2].min(axis=1)\ntest['Elev_Asp_Slope_max'] = test[cols_2].max(axis=1)\ntest['Elev_Asp_Slope_median'] = test[cols_2].median(axis=1)\ntest['Elev_Asp_Slope_quantile'] = test[cols_2].quantile(axis=1)\n# Horizontal_Distance_To_Hydrology\ntest['Horizontal_Distance_To_Hydrology_sin_'] = abs(np.ceil(np.sin(test['Horizontal_Distance_To_Hydrology'])))\ntest['Horizontal_Distance_To_Hydrology_cos_'] = abs(np.ceil(np.cos(test['Horizontal_Distance_To_Hydrology'])))\ntest['Horizontal_Distance_To_Hydrology_tanh_'] = abs(np.ceil(np.tanh(test['Horizontal_Distance_To_Hydrology'])))\ntest['Horizontal_Distance_To_Hydrology_bin_round_100'] = np.array(np.floor(np.array(test['Horizontal_Distance_To_Hydrology']) \/ 100))\ntest['Horizontal_Distance_To_Hydrology_bin_round_1000'] = np.array(np.floor(np.array(test['Horizontal_Distance_To_Hydrology']) \/ 1000))\ntest['Horizontal_Distance_To_Hydrology_sqrt'] = np.ceil(np.sqrt(np.floor(np.array(test['Horizontal_Distance_To_Hydrology']))))\ntest['Horizontal_Distance_To_Hydrology_log'] = np.ceil(np.log(np.floor(np.array(1+test['Horizontal_Distance_To_Hydrology']))))\n# Vertical_Distance_To_Hydrology\ntest['Vertical_Distance_To_Hydrology_sin_'] = abs(np.ceil(np.sin(test['Vertical_Distance_To_Hydrology'])))\ntest['Vertical_Distance_To_Hydrology_cos_'] = abs(np.ceil(np.cos(test['Vertical_Distance_To_Hydrology'])))\ntest['Vertical_Distance_To_Hydrology_tanh_'] = abs(np.ceil(np.tanh(test['Vertical_Distance_To_Hydrology'])))\ntest['Vertical_Distance_To_Hydrology_bin_round_100'] = np.array(np.floor(np.array(test['Vertical_Distance_To_Hydrology']) \/ 100))\ntest['Vertical_Distance_To_Hydrology_bin_round_1000'] = np.array(np.floor(np.array(test['Vertical_Distance_To_Hydrology']) \/ 1000))\ntest['Vertical_Distance_To_Hydrology_sqrt'] = np.ceil(np.sqrt(np.floor(np.array(test['Vertical_Distance_To_Hydrology']))))\ntest['Vertical_Distance_To_Hydrology_log'] = np.ceil(np.log(np.floor(np.array(1+test['Vertical_Distance_To_Hydrology']))))\n# Basic statistics - Horizontal_Distance_To_Hydrology_+_Vertical_Distance_To_Hydrology\ntest['Horizontal_Distance_To_Hydrology_+_Vertical_Distance_To_Hydrology_sum'] = test[cols_1].sum(axis=1)\ntest['Horizontal_Distance_To_Hydrology_+_Vertical_Distance_To_Hydrology_mean'] = test[cols_1].mean(axis=1)\ntest['Horizontal_Distance_To_Hydrology_+_Vertical_Distance_To_Hydrology_std'] = np.round(test[cols_1].std(axis=1))\ntest['Horizontal_Distance_To_Hydrology_+_Vertical_Distance_To_Hydrology_min'] = test[cols_1].min(axis=1)\ntest['Horizontal_Distance_To_Hydrology_+_Vertical_Distance_To_Hydrology_max'] = test[cols_1].max(axis=1)\ntest['Horizontal_Distance_To_Hydrology_+_Vertical_Distance_To_Hydrology_median'] = test[cols_1].median(axis=1)\ntest['Horizontal_Distance_To_Hydrology_+_Vertical_Distance_To_Hydrology_quantile'] = test[cols_1].quantile(axis=1)\n# Horizontal_Distance_To_Roadways\ntest['Horizontal_Distance_To_Roadways_sin_'] = abs(np.ceil(np.sin(test['Horizontal_Distance_To_Roadways'])))\ntest['Horizontal_Distance_To_Roadways_cos_'] = abs(np.ceil(np.cos(test['Horizontal_Distance_To_Roadways'])))\ntest['Horizontal_Distance_To_Roadways_tanh_'] = abs(np.ceil(np.tanh(test['Horizontal_Distance_To_Roadways'])))\ntest['Horizontal_Distance_To_Roadways_bin_round_100'] = np.array(np.floor(np.array(test['Horizontal_Distance_To_Roadways']) \/ 100))\ntest['Horizontal_Distance_To_Roadways_bin_round_1000'] = np.array(np.floor(np.array(test['Horizontal_Distance_To_Roadways']) \/ 1000))\ntest['Horizontal_Distance_To_Roadways_sqrt'] = np.ceil(np.sqrt(np.floor(np.array(test['Horizontal_Distance_To_Roadways']))))\ntest['Horizontal_Distance_To_Roadways_log'] = np.ceil(np.log(np.floor(np.array(1+test['Horizontal_Distance_To_Roadways']))))\n# Hillshade_9am\ntest['Hillshade_9am_sin_'] = abs(np.ceil(np.sin(test['Hillshade_9am'])))\ntest['Hillshade_9am_cos_'] = abs(np.ceil(np.cos(test['Hillshade_9am'])))\ntest['Hillshade_9am_tanh_'] = abs(np.ceil(np.tanh(test['Hillshade_9am'])))\ntest['Hillshade_9am_bin_round_100'] = np.array(np.floor(np.array(test['Hillshade_9am']) \/ 100))\ntest['Hillshade_9am_bin_round_1000'] = np.array(np.floor(np.array(test['Hillshade_9am']) \/ 1000))\ntest['Hillshade_9am_sqrt'] = np.ceil(np.sqrt(np.floor(np.array(test['Hillshade_9am']))))\ntest['Hillshade_9am_log'] = np.ceil(np.log(np.floor(np.array(1+test['Hillshade_9am']))))\n# Hillshade_Noon\ntest['Hillshade_Noon_sin_'] = abs(np.ceil(np.sin(test['Hillshade_Noon'])))\ntest['Hillshade_Noon_cos_'] = abs(np.ceil(np.cos(test['Hillshade_Noon'])))\ntest['Hillshade_Noon_tanh_'] = abs(np.ceil(np.tanh(test['Hillshade_Noon'])))\ntest['Hillshade_Noon_bin_round_100'] = np.array(np.floor(np.array(test['Hillshade_Noon']) \/ 100))\ntest['Hillshade_Noon_bin_round_1000'] = np.array(np.floor(np.array(test['Hillshade_Noon']) \/ 1000))\ntest['Hillshade_Noon_sqrt'] = np.ceil(np.sqrt(np.floor(np.array(test['Hillshade_Noon']))))\ntest['Hillshade_Noon_log'] = np.ceil(np.log(np.floor(np.array(1+test['Hillshade_Noon']))))\n# Hillshade_3pm\ntest['Hillshade_3pm_sin_'] = abs(np.ceil(np.sin(test['Hillshade_3pm'])))\ntest['Hillshade_3pm_cos_'] = abs(np.ceil(np.cos(test['Hillshade_3pm'])))\ntest['Hillshade_3pm_tanh_'] = abs(np.ceil(np.tanh(test['Hillshade_3pm'])))\ntest['Hillshade_3pm_bin_round_100'] = np.array(np.floor(np.array(test['Hillshade_3pm']) \/ 100))\ntest['Hillshade_3pm_bin_round_1000'] = np.array(np.floor(np.array(test['Hillshade_3pm']) \/ 1000))\ntest['Hillshade_3pm_sqrt'] = np.ceil(np.sqrt(np.floor(np.array(test['Hillshade_3pm']))))\ntest['Hillshade_3pm_log'] = np.ceil(np.log(np.floor(np.array(1+test['Hillshade_3pm']))))\n# Horizontal_Distance_To_Fire_Points\ntest['Horizontal_Distance_To_Fire_Points_sin_'] = abs(np.ceil(np.sin(test['Horizontal_Distance_To_Fire_Points'])))\ntest['Horizontal_Distance_To_Fire_Points_cos_'] = abs(np.ceil(np.cos(test['Horizontal_Distance_To_Fire_Points'])))\ntest['Horizontal_Distance_To_Fire_Points_tanh_'] = abs(np.ceil(np.tanh(test['Horizontal_Distance_To_Fire_Points'])))\ntest['Horizontal_Distance_To_Fire_Points_bin_round_100'] = np.array(np.floor(np.array(test['Horizontal_Distance_To_Fire_Points']) \/ 100))\ntest['Horizontal_Distance_To_Fire_Points_bin_round_1000'] = np.array(np.floor(np.array(test['Horizontal_Distance_To_Fire_Points']) \/ 1000))\ntest['Horizontal_Distance_To_Fire_Points_sqrt'] = np.ceil(np.sqrt(np.floor(np.array(test['Horizontal_Distance_To_Fire_Points']))))\ntest['Horizontal_Distance_To_Fire_Points_log'] = np.ceil(np.log(np.floor(np.array(1+test['Horizontal_Distance_To_Fire_Points']))))\n# Basic statistics - Horizontal_Distance_To_Roadways_+_Horizontal_Distance_To_Fire_Points\ntest['Horizontal_Distance_To_Roadways_+_Horizontal_Distance_To_Fire_Points_sum'] = test[cols_3].sum(axis=1)\ntest['Horizontal_Distance_To_Roadways_+_Horizontal_Distance_To_Fire_Points_mean'] = test[cols_3].mean(axis=1)\ntest['Horizontal_Distance_To_Roadways_+_Horizontal_Distance_To_Fire_Points_std'] = np.round(test[cols_3].std(axis=1),3)\ntest['Horizontal_Distance_To_Roadways_+_Horizontal_Distance_To_Fire_Points_min'] = test[cols_3].min(axis=1)\ntest['Horizontal_Distance_To_Roadways_+_Horizontal_Distance_To_Fire_Points_max'] = test[cols_3].max(axis=1)\ntest['Horizontal_Distance_To_Roadways_+_Horizontal_Distance_To_Fire_Points_median'] = test[cols_3].median(axis=1)\ntest['Horizontal_Distance_To_Roadways_+_Horizontal_Distance_To_Fire_Points_quantile'] = test[cols_3].quantile(axis=1)\n# Basic statistics - Hillshade_9am_+_Hillshade_Noon_+_Hillshade_3pm\ntest['Hillshade_9am_+_Hillshade_Noon_+_Hillshade_3pm_sum'] = test[cols_4].sum(axis=1)\ntest['Hillshade_9am_+_Hillshade_Noon_+_Hillshade_3pm_mean'] = test[cols_4].mean(axis=1)\ntest['Hillshade_9am_+_Hillshade_Noon_+_Hillshade_3pm_std'] = np.round(test[cols_4].std(axis=1),3)\ntest['Hillshade_9am_+_Hillshade_Noon_+_Hillshade_3pm_min'] = test[cols_4].min(axis=1)\ntest['Hillshade_9am_+_Hillshade_Noon_+_Hillshade_3pm_max'] = test[cols_4].max(axis=1)\ntest['Hillshade_9am_+_Hillshade_Noon_+_Hillshade_3pm_median'] = test[cols_4].median(axis=1)\ntest['Hillshade_9am_+_Hillshade_Noon_+_Hillshade_3pm_quantile'] = test[cols_4].quantile(axis=1)\nprint(\"Test Size : \",test.shape)\nprint(\"New features have been added...\")","ecf3df74":"data_train = abs(df_train.copy())\ndata_group_2 = data_train.iloc[:,3:5]\nx = data_join(data_group_2, data_train[['Cover_Type']],None)\ncompare_plot(x)\ndata_group_2.hist(bins=60)\nfig, ax = plt.subplots()\nax.set_xlabel('Horizontal Distance To Hydrology', fontsize=12)\nax.set_ylabel('Vertical Distance To Hydrology', fontsize=12)\nax.set_title('Scatter',fontsize=18)\nx = data_train.iloc[:,3:4]\ny = data_train.iloc[:,4:5]\nplt.scatter(x,y,c=\"b\",marker='o',alpha=0.4,linewidth=2.0)\ndata_group_2.head()","6a064020":"z_1 = linkage(data_group_2, method='complete')\nc,coph_dist = cophenet(z_1,pdist(data_group_2,'minkowski'))\n\nfig, ax = plt.subplots(figsize = (30,30))\nax.set_xlabel('Horizontal Distance To Hydrology', fontsize=25)\nax.set_ylabel('Vertical Distance To Hydrology', fontsize=25)\nax.set_title('Cluster',fontsize=25)\nclusters = fcluster(z_1,2,criterion=\"distance\")\ncl_data = pd.DataFrame(np.array(clusters),columns=['Cluster_G2'])\nx = data_train.iloc[:,3:4]\ny = data_train.iloc[:,4:5]\nplt.scatter(x,y,clusters,c=\"r\",marker='o',alpha=0.4,linewidth=2.0)\nax.grid(True)\nfig.tight_layout()\nplt.show()\nfig, ax = plt.subplots()\nax.set_title('Cluster distribution',fontsize=18)\nplt.hist(clusters)\nplt.show()\n\n# Create new data\n\nm = pd.DataFrame(z_1)\nm.columns = ['d1','d2','d3','d4']\ncol = m.columns\nnew_reg = m.loc[[15118]] - 100\njoin_reg = pd.concat([m,new_reg])\njoin_reg.reset_index(drop=False, inplace = True)\njoin_reg.reindex(df_train.index, fill_value=0)\njoin_reg.set_index(df_train.index)\nnew_cluster_data = pd.DataFrame({ 'Cluster_G2' : cl_data['Cluster_G2'].values,\n                                  'Cluster_G2_Data_x' : join_reg.d1.values,\n                                  'Cluster_G2_Data_y' : join_reg.d2.values,\n                                  'Cluster_G2_Data_xy' : join_reg.d3.values,\n                                  'Cluster_G2_Data_ncl' : join_reg.d4.values,\n    \n},index=df_train.index)\n\nprint(\"Joining Accuracy :\",c)\nprint(\"Matrix distances Train :\",m.shape)\nprint(\"Cluster distances Train :\",cl_data.shape)\nprint(\"Join data_clusters Train :\",new_cluster_data.shape)\nnew_cluster_data.head()","ecac13e1":"original_data = abs(df_train.copy())\na = pd.merge(original_data,train.iloc[:,53:], right_index=True, left_index=True)\ntrain = pd.merge(a,new_cluster_data, right_index=True, left_index=True)\n\n# Horizontal_Distance_+_Vertical_Distance_To_Hydrology\ntrain['Cluster_G2_HDR_+_HDFP_sin_'] = abs(np.ceil(np.sin(train['Cluster_G2'])))\ntrain['Cluster_G2_HDR_+_HDFP_cos_'] = abs(np.ceil(np.cos(train['Cluster_G2'])))\ntrain['Cluster_G2_HDR_+_HDFP_tanh_'] = abs(np.ceil(np.tanh(train['Cluster_G2'])))\ntrain['Cluster_G2_HDR_+_HDFP_bin_round_100'] = np.array(np.floor(np.array(train['Cluster_G2']) \/ 100))\ntrain['Cluster_G2_HDR_+_HDFP_bin_round_1000'] = np.array(np.floor(np.array(train['Cluster_G2']) \/ 1000))\ntrain['Cluster_G2_HDR_+_HDFP_sqrt'] = np.ceil(np.sqrt(np.floor(np.array(train['Cluster_G2']))))\ntrain['Cluster_G2_HDR_+_HDFP_log'] = np.ceil(np.log(np.floor(np.array(1+train['Cluster_G2']))))\nprint(\"Train Size : \",train.shape)\nprint(\"previous accuracy :\",training_score,\"current accuracy :\",score_dataset(train.copy()))\nprint(\"New features have been added...\")\ntrain.head()","bc8b83a7":"Cluster_G2 = new_cluster_data\nnew_concat = pd.concat([Cluster_G2,Cluster_G2])\nnew_concat = pd.concat([new_concat,new_concat])\nnew_concat = pd.concat([new_concat,new_concat])\nnew_concat = pd.concat([new_concat,new_concat])\nnew_concat = pd.concat([new_concat,new_concat])\nnew_concat = pd.concat([new_concat,Cluster_G2])\nnew_concat = pd.concat([new_concat,Cluster_G2])\nnew_concat = pd.concat([new_concat,Cluster_G2])\nnew_concat = pd.concat([new_concat,Cluster_G2])\nnew_concat = pd.concat([new_concat,Cluster_G2])\nnew_concat = pd.concat([new_concat,Cluster_G2])\nfirst, second = split(new_concat, 565892)\nnew_test_g2 = first.sort_index(ascending=True)\nnew_test_cluster = pd.DataFrame({ 'Cluster_G2' : new_test_g2['Cluster_G2'].values,\n                                  'Cluster_G2_Data_x' : new_test_g2.Cluster_G2_Data_x.values,\n                                  'Cluster_G2_Data_y' : new_test_g2.Cluster_G2_Data_y.values,\n                                  'Cluster_G2_Data_xy' : new_test_g2.Cluster_G2_Data_xy.values,\n                                  'Cluster_G2_Data_ncl' : new_test_g2.Cluster_G2_Data_ncl.values,\n    \n},index=df_test.index)\noriginal_data = abs(df_test.copy())\na = pd.merge(original_data,test.iloc[:,52:], right_index=True, left_index=True)\ntest = pd.merge(a,new_test_cluster, right_index=True, left_index=True)\ntest['Cluster_G2_HDR_+_HDFP_sin_'] = abs(np.ceil(np.sin(test['Cluster_G2'])))\ntest['Cluster_G2_HDR_+_HDFP_cos_'] = abs(np.ceil(np.cos(test['Cluster_G2'])))\ntest['Cluster_G2_HDR_+_HDFP_tanh_'] = abs(np.ceil(np.tanh(test['Cluster_G2'])))\ntest['Cluster_G2_HDR_+_HDFP_bin_round_100'] = np.array(np.floor(np.array(test['Cluster_G2']) \/ 100))\ntest['Cluster_G2_HDR_+_HDFP_bin_round_1000'] = np.array(np.floor(np.array(test['Cluster_G2']) \/ 1000))\ntest['Cluster_G2_HDR_+_HDFP_sqrt'] = np.ceil(np.sqrt(np.floor(np.array(test['Cluster_G2']))))\ntest['Cluster_G2_HDR_+_HDFP_log'] = np.ceil(np.log(np.floor(np.array(1+test['Cluster_G2']))))\nprint(\"Join data_clusters Test :\",new_test_cluster.shape)\nprint(\"Test Size : \",test.shape)\nprint(\"New features have been added...\")\ntest.head()","7eaeab76":"data_train = abs(df_train.copy())\ndata_group_3 = data_join(data_train[['Horizontal_Distance_To_Roadways']], data_train[['Horizontal_Distance_To_Fire_Points']],None)\ndata_group_3.head()\nx = data_join(data_group_3, data_train[['Cover_Type']],None)\ncompare_plot(x)\ndata_group_3.hist(bins=60)\nfig, ax = plt.subplots(figsize = (8,8))\nax.set_xlabel('Horizontal_Distance_To_Roadways', fontsize=12)\nax.set_ylabel('Horizontal_Distance_To_Fire_Points', fontsize=12)\nax.set_title('Scatter',fontsize=18)\nx = data_group_3.iloc[:,0:1]\ny = data_group_3.iloc[:,1:2]\nplt.scatter(x,y,c=\"r\",marker='^',alpha=0.25,linewidth=2.0)","db9eb0a6":"z_2 = linkage(data_group_3, method='average')\nc,coph_dist = cophenet(z_2,pdist(data_group_3))\n\nfig, ax = plt.subplots(figsize = (30,30))\nax.set_xlabel('Horizontal_Distance_To_Roadways', fontsize=25)\nax.set_ylabel('Horizontal_Distance_To_Fire_Points', fontsize=25)\nax.set_title('Cluster',fontsize=25)\nclusters_2 = fcluster(z_2,2,criterion=\"distance\")\ncl_data = pd.DataFrame(np.array(clusters_2),columns=['Cluster_G3'])\nx = data_group_3.iloc[:,0:1]\ny = data_group_3.iloc[:,1:2]\nplt.scatter(x,y,clusters_2,c=\"r\",marker='o',alpha=0.4,linewidth=2.0)\nax.grid(True)\nfig.tight_layout()\nplt.show()\nfig, ax = plt.subplots()\nax.set_title('Cluster distribution',fontsize=18)\nplt.hist(clusters_2,bins=90)\nplt.show()\n\n# Create new data\n\nm = pd.DataFrame(z_2)\nm.columns = ['d1','d2','d3','d4']\ncol = m.columns\nnew_reg = m.loc[[15118]] - 100\njoin_reg = pd.concat([m,new_reg])\njoin_reg.reset_index(drop=False, inplace = True)\njoin_reg.reindex(df_train.index, fill_value=0)\njoin_reg.set_index(df_train.index)\nnew_cluster_data3 = pd.DataFrame({'Cluster_G3' : cl_data['Cluster_G3'].values,\n                                  'Cluster_G3_Data_x' : join_reg.d1.values,\n                                  'Cluster_G3_Data_y' : join_reg.d2.values,\n                                  'Cluster_G3_Data_xy' : join_reg.d3.values,\n                                  'Cluster_G3_Data_nc' : join_reg.d4.values,\n    \n},index=df_train.index)\n\nprint(\"Joining Accuracy :\",c)\nprint(\"Matrix distances Train :\",m.shape)\nprint(\"Cluster distances Train :\",cl_data.shape)\nprint(\"Join data_clusters Train :\",new_cluster_data3.shape)\nnew_cluster_data3.head()","782e5105":"original_data = abs(df_train.copy())\nb = pd.merge(original_data,train.iloc[:,53:], right_index=True, left_index=True)\ntrain = pd.merge(b,new_cluster_data3, right_index=True, left_index=True)\n\n# Cluster_Horizontal_Distance_To_Roadways_+_Horizontal_Distance_To_Fire_Points\ntrain['Cluster_G3_HDR_+_HDFP_sin_'] = abs(np.ceil(np.sin(train['Cluster_G3'])))\ntrain['Cluster_G3_HDR_+_HDFP_cos_'] = abs(np.ceil(np.cos(train['Cluster_G3'])))\ntrain['Cluster_G3_HDR_+_HDFP_tanh_'] = abs(np.ceil(np.tanh(train['Cluster_G3'])))\ntrain['Cluster_G3_HDR_+_HDFP_bin_round_100'] = np.array(np.floor(np.array(train['Cluster_G3']) \/ 100))\ntrain['Cluster_G3_HDR_+_HDFP_bin_round_1000'] = np.array(np.floor(np.array(train['Cluster_G3']) \/ 1000))\ntrain['Cluster_G3_HDR_+_HDFP_sqrt'] = np.ceil(np.sqrt(np.floor(np.array(train['Cluster_G3']))))\ntrain['Cluster_G3_HDR_+_HDFP_log'] = np.ceil(np.log(np.floor(np.array(1+train['Cluster_G3']))))\nprint(\"Train Size : \",train.shape)\nprint(\"previous accuracy :\",training_score,\"current accuracy :\",score_dataset(train.copy()))\nprint(\"New features have been added...\")\ntrain.head()","a6406563":"Cluster_G3 = new_cluster_data3\nnew_Cluster_G3 = pd.concat([Cluster_G3,Cluster_G3])\nnew_Cluster_G3 = pd.concat([new_Cluster_G3,new_Cluster_G3])\nnew_Cluster_G3 = pd.concat([new_Cluster_G3,new_Cluster_G3])\nnew_Cluster_G3 = pd.concat([new_Cluster_G3,new_Cluster_G3])\nnew_Cluster_G3 = pd.concat([new_Cluster_G3,new_Cluster_G3])\nnew_Cluster_G3 = pd.concat([new_Cluster_G3,Cluster_G3])\nnew_Cluster_G3 = pd.concat([new_Cluster_G3,Cluster_G3])\nnew_Cluster_G3 = pd.concat([new_Cluster_G3,Cluster_G3])\nnew_Cluster_G3 = pd.concat([new_Cluster_G3,Cluster_G3])\nnew_Cluster_G3 = pd.concat([new_Cluster_G3,Cluster_G3])\nnew_Cluster_G3 = pd.concat([new_Cluster_G3,Cluster_G3])\nfirst, second = split(new_Cluster_G3, 565892)\nnew_test_g3 = first.sort_index(ascending=True)\nnew_cluster_test3 = pd.DataFrame({'Cluster_G3' : new_test_g3['Cluster_G3'].values,\n                                  'Cluster_G3_Data_x' : new_test_g3.Cluster_G3_Data_x.values,\n                                  'Cluster_G3_Data_y' : new_test_g3.Cluster_G3_Data_y.values,\n                                  'Cluster_G3_Data_xy' : new_test_g3.Cluster_G3_Data_xy.values,\n                                  'Cluster_G3_Data_nc' : new_test_g3.Cluster_G3_Data_nc.values,\n    \n},index=df_test.index)\noriginal_data = abs(df_test.copy())\na = pd.merge(original_data,test.iloc[:,52:], right_index=True, left_index=True)\ntest = pd.merge(a,new_cluster_test3, right_index=True, left_index=True)\ntest['Cluster_G3_HDR_+_HDFP_sin_'] = abs(np.ceil(np.sin(test['Cluster_G3'])))\ntest['Cluster_G3_HDR_+_HDFP_cos_'] = abs(np.ceil(np.cos(test['Cluster_G3'])))\ntest['Cluster_G3_HDR_+_HDFP_tanh_'] = abs(np.ceil(np.tanh(test['Cluster_G3'])))\ntest['Cluster_G3_HDR_+_HDFP_bin_round_100'] = np.array(np.floor(np.array(test['Cluster_G3']) \/ 100))\ntest['Cluster_G3_HDR_+_HDFP_bin_round_1000'] = np.array(np.floor(np.array(test['Cluster_G3']) \/ 1000))\ntest['Cluster_G3_HDR_+_HDFP_sqrt'] = np.ceil(np.sqrt(np.floor(np.array(test['Cluster_G3']))))\ntest['Cluster_G3_HDR_+_HDFP_log'] = np.ceil(np.log(np.floor(np.array(1+test['Cluster_G3']))))\nprint(\"Join data_clusters Test :\",new_cluster_test3.shape)\nprint(\"Test Size : \",test.shape)\nprint(\"New features have been added...\")\ntest.head()","e6304a0e":"data_train = abs(df_train.copy())\ndata_group_4 = data_train.iloc[:,6:9]\nsns.set(style=\"darkgrid\")\nsns.pairplot(data_group_4, palette =\"b\",kind =\"scatter\",\n             height=2.5)\nplt.show()\ndata_group_4.head()","e43ecbe5":"z_3 = linkage(data_group_4, method='average')\nc,coph_dist = cophenet(z_3,pdist(data_group_4,'chebyshev'))\n\nfig, ax = plt.subplots(figsize = (30,30))\nax.set_xlabel('Horizontal_Distance_To_Roadways', fontsize=25)\nax.set_ylabel('Horizontal_Distance_To_Fire_Points', fontsize=25)\nax.set_title('Cluster',fontsize=25)\nclusters_3 = fcluster(z_3,3,criterion=\"distance\")\ncl_data = pd.DataFrame(np.array(clusters_3),columns=['Cluster_G4'])\nx = data_group_3.iloc[:,0:1]\ny = data_group_3.iloc[:,1:2]\nplt.scatter(x,y,clusters_3,c=\"orange\",marker='o',alpha=0.4,linewidth=2.0)\nax.grid(True)\nfig.tight_layout()\nplt.show()\nfig, ax = plt.subplots()\nax.set_title('Cluster distribution',fontsize=18)\nplt.hist(clusters_3,bins=90)\nplt.show()\n\n# Create new data\n\nm = pd.DataFrame(z_3)\nm.columns = ['d1','d2','d3','d4']\ncol = m.columns\nnew_reg = m.loc[[15118]] - 100\njoin_reg = pd.concat([m,new_reg])\njoin_reg.reset_index(drop=False, inplace = True)\njoin_reg.reindex(df_train.index, fill_value=0)\njoin_reg.set_index(df_train.index)\nnew_cluster_data3 = pd.DataFrame({ 'Cluster_G4' : cl_data['Cluster_G4'].values,\n                                  'Cluster_G4_Data_x' : join_reg.d1.values,\n                                  'Cluster_G4_Data_y' : join_reg.d2.values,\n                                  'Cluster_G4_Data_xy' : join_reg.d3.values,\n                                  'Cluster_G4_Data_nc' : join_reg.d4.values,\n    \n},index=df_train.index)\n\nprint(\"Joining Accuracy :\",c)\nprint(\"Matrix distances Train :\",m.shape)\nprint(\"Cluster distances Train :\",cl_data.shape)\nprint(\"Join data_clusters Train :\",new_cluster_data3.shape)\nnew_cluster_data3.head()","76ee9abe":"original_data = abs(df_train.copy())\nc = pd.merge(original_data,train.iloc[:,53:], right_index=True, left_index=True)\ntrain = pd.merge(c,new_cluster_data3, right_index=True, left_index=True)\n# Cluster_G4_HDR_+_HDFP\ntrain['Cluster_G4_HDR_+_HDFP_sin_'] = abs(np.ceil(np.sin(train['Cluster_G4'])))\ntrain['Cluster_G4_HDR_+_HDFP_cos_'] = abs(np.ceil(np.cos(train['Cluster_G4'])))\ntrain['Cluster_G4_HDR_+_HDFP_tanh_'] = abs(np.ceil(np.tanh(train['Cluster_G4'])))\ntrain['Cluster_G4_HDR_+_HDFP_bin_round_100'] = np.array(np.floor(np.array(train['Cluster_G4']) \/ 100))\ntrain['Cluster_G4_HDR_+_HDFP_bin_round_1000'] = np.array(np.floor(np.array(train['Cluster_G4']) \/ 1000))\ntrain['Cluster_G4_HDR_+_HDFP_sqrt'] = np.ceil(np.sqrt(np.floor(np.array(train['Cluster_G4']))))\ntrain['Cluster_G4_HDR_+_HDFP_log'] = np.ceil(np.log(np.floor(np.array(1+train['Cluster_G4']))))\nprint(\"Train Size : \",train.shape)\nprint(\"previous accuracy :\",training_score,\"current accuracy :\",score_dataset(train.copy()))\nprint(\"New features have been added...\")\ntrain.head()","c532c1a2":"Cluster_G4 = new_cluster_data3\nnew_Cluster_G4 = pd.concat([Cluster_G4,Cluster_G4])\nnew_Cluster_G4 = pd.concat([new_Cluster_G4,new_Cluster_G4])\nnew_Cluster_G4 = pd.concat([new_Cluster_G4,new_Cluster_G4])\nnew_Cluster_G4 = pd.concat([new_Cluster_G4,new_Cluster_G4])\nnew_Cluster_G4 = pd.concat([new_Cluster_G4,new_Cluster_G4])\nnew_Cluster_G4 = pd.concat([new_Cluster_G4,Cluster_G4])\nnew_Cluster_G4 = pd.concat([new_Cluster_G4,Cluster_G4])\nnew_Cluster_G4 = pd.concat([new_Cluster_G4,Cluster_G4])\nnew_Cluster_G4 = pd.concat([new_Cluster_G4,Cluster_G4])\nnew_Cluster_G4 = pd.concat([new_Cluster_G4,Cluster_G4])\nnew_Cluster_G4 = pd.concat([new_Cluster_G4,Cluster_G4])\nfirst, second = split(new_Cluster_G4, 565892)\nnew_test_g4 = first.sort_index(ascending=True)\n\nnew_cluster_data4 = pd.DataFrame({'Cluster_G4' : new_test_g4['Cluster_G4'].values,\n                                  'Cluster_G4_Data_x' : new_test_g4.Cluster_G4_Data_x.values,\n                                  'Cluster_G4_Data_y' : new_test_g4.Cluster_G4_Data_y.values,\n                                  'Cluster_G4_Data_xy' : new_test_g4.Cluster_G4_Data_xy.values,\n                                  'Cluster_G4_Data_nc' : new_test_g4.Cluster_G4_Data_nc.values,\n    \n},index=df_test.index)\n\noriginal_data = abs(df_test.copy())\nb = pd.merge(original_data,test.iloc[:,52:], right_index=True, left_index=True)\ntest = pd.merge(b,new_cluster_data4, right_index=True, left_index=True)\n\n# Cluster_G4_HDR_+_HDFP\ntest['Cluster_G4_HDR_+_HDFP_sin_'] = abs(np.ceil(np.sin(test['Cluster_G4'])))\ntest['Cluster_G4_HDR_+_HDFP_cos_'] = abs(np.ceil(np.cos(test['Cluster_G4'])))\ntest['Cluster_G4_HDR_+_HDFP_tanh_'] = abs(np.ceil(np.tanh(test['Cluster_G4'])))\ntest['Cluster_G4_HDR_+_HDFP_bin_round_100'] = np.array(np.floor(np.array(test['Cluster_G4']) \/ 100))\ntest['Cluster_G4_HDR_+_HDFP_bin_round_1000'] = np.array(np.floor(np.array(test['Cluster_G4']) \/ 1000))\ntest['Cluster_G4_HDR_+_HDFP_sqrt'] = np.ceil(np.sqrt(np.floor(np.array(test['Cluster_G4']))))\ntest['Cluster_G4_HDR_+_HDFP_log'] = np.ceil(np.log(np.floor(np.array(1+test['Cluster_G4']))))\nprint(\"Join data_clusters Test :\",new_cluster_data4.shape)\nprint(\"Test Size : \",test.shape)\nprint(\"New features have been added...\")\ntest.head()","1c7a9605":"original_data = abs(df_train.copy())\ncol_g5 = original_data.iloc[:,10:52].columns.values\ndata_group_5 = data_train.iloc[:,10:52]\n# Wilderness_Area1\tWilderness_Area2\tWilderness_Area3\tWilderness_Area4\ncols=['Wilderness_Area1','Wilderness_Area2','Wilderness_Area3','Wilderness_Area4']\ntrain['WA1_+_WA2_+_WA3_+_WA4_sum'] = data_group_5[cols].sum(axis=1)\ntrain['WA1_+_WA2_+_WA3_+_WA4_mean'] = data_group_5[cols].mean(axis=1)\ntrain['WA1_+_WA2_+_WA3_+_WA4_std'] = np.round(data_group_5[cols].std(axis=1))\ntrain['WA1_+_WA2_+_WA3_+_WA4_min'] = data_group_5[cols].min(axis=1)\ntrain['WA1_+_WA2_+_WA3_+_WA4_max'] = data_group_5[cols].max(axis=1)\ntrain['WA1_+_WA2_+_WA3_+_WA4_median'] = data_group_5[cols].median(axis=1)\ntrain['WA1_+_WA2_+_WA3_+_WA4_quantile'] = data_group_5[cols].quantile(axis=1)\nprint(\"Train Size : \",train.shape)\nprint(\"previous accuracy :\",training_score,\"current accuracy :\",score_dataset(train.copy()))\nprint(\"New features have been added...\")\n#output = pd.DataFrame(new_data)\n#output.to_csv('new_train.csv', index=False)\n#output.head()","d1a64590":"original_data = abs(df_test.copy())\ncol_g5 = original_data.iloc[:,10:52].columns.values\ndata_group_5 = df_test.iloc[:,10:52]\n# Wilderness_Area1\tWilderness_Area2\tWilderness_Area3\tWilderness_Area4\ncols=['Wilderness_Area1','Wilderness_Area2','Wilderness_Area3','Wilderness_Area4']\ntest['WA1_+_WA2_+_WA3_+_WA4_sum'] = data_group_5[cols].sum(axis=1)\ntest['WA1_+_WA2_+_WA3_+_WA4_mean'] = data_group_5[cols].mean(axis=1)\ntest['WA1_+_WA2_+_WA3_+_WA4_std'] = np.round(data_group_5[cols].std(axis=1))\ntest['WA1_+_WA2_+_WA3_+_WA4_min'] = data_group_5[cols].min(axis=1)\ntest['WA1_+_WA2_+_WA3_+_WA4_max'] = data_group_5[cols].max(axis=1)\ntest['WA1_+_WA2_+_WA3_+_WA4_median'] = data_group_5[cols].median(axis=1)\ntest['WA1_+_WA2_+_WA3_+_WA4_quantile'] = data_group_5[cols].quantile(axis=1)\nprint(\"Test Size : \",test.shape)\nprint(\"New features have been added...\")\n#output = pd.DataFrame(new_data)\n#output.to_csv('new_test.csv', index=False)\n#output.head()","19d20b4d":"data_train = train.copy()\ndata_test = test.copy()\ny = data_train.Cover_Type.values # <- Target\nX = data_train # Data\nX.drop(['Cover_Type'], axis=1, inplace=True) # Drop target\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX_test = scaler.transform(data_test) \nX_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.80, random_state=2019)\nRF = RandomForestClassifier(n_jobs =  -1, n_estimators = 1000,random_state = 1)\nRF.fit(X=X_train, y=y_train)\npred = RF.predict(X_test)\noutput = pd.DataFrame({'Id': df_test.index,\n                       'Cover_Type': pred})\noutput.to_csv('submission.csv', index=False)\noutput.head()","c89c8ad3":"# Stack Models \nI invite you to see the selection of the model ...   \nYou can see the development in this Kernel and how I improve the score...        \nhttps:\/\/www.kaggle.com\/azhura\/model-selection-rnf   ","2c1f364a":"### Creating a matrix of distances","b4ed9620":"### Train","e9f71b9b":"# Feature Engineering \nFocus: I will divide the task into 2 parts:   \n* Will create new characteristics for the quantitative values.   \n* Create new characteristics for binary values    \n\n### Feature generation   \nI create several groups of features:   \nBased on different techniques I divide information into groups.  ","2f5f2c47":"A test with the Random Forest model shows that we improve the model only with positive values.      \nIt was decided to remove Soil_Type7 and Soil_Type15 for cardinality discard.    ","aadb0fd1":"### Creating a matrix of distances","606c4aaa":"### Test","4e339605":"## About this notebook   \nThis notebook is part of the next competition:   \nhttps:\/\/www.kaggle.com\/azhura\/roosevelt-national-forest-234th   ","fae93319":"**Binary values**","fe21109a":"### First 10 categories\n**Continuous values**","c29d8d65":"**If you like upvote,    \nThank you for your time.**","a38b3c7b":"### Comparing the first 10 categories with the target","d85db391":"# 3rd group \n**\"Horizontal_Distance_To_Roadways + Horizontal_Distance_To_Fire_Points\"**","970a26df":"# Resources      \nhttps:\/\/towardsdatascience.com\/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b     \nhttps:\/\/towardsdatascience.com\/feature-engineering-for-machine-learning-3a5e293a5114         \nhttps:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.cluster.hierarchy.linkage.html     \nhttps:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.spatial.distance.pdist.html?highlight=pdist#scipy.spatial.distance.pdist   \nhttps:\/\/docs.scipy.org\/doc\/scipy\/reference\/spatial.distance.html   \n","14b7c4d9":"* The public table test score of this unprocessed model is of **0.69859**","74103b5d":"### Test","401ccf4a":"### Test","011584c4":"### Train","e68806b2":"### Sorting the information","9ca503f7":"# Setup Model   \nI'm going to implement the same model we used to compare accuracy.","99047854":"# Import","342932e8":"# 1st group\n* The first group is composed of the first 10 categories of quantitative values   \n**Basic statistics,rounding, binning,clusters.**    ","3205a25c":"### test","52255643":"# 2nd group  \n**\"Horizontal_Distance_To_Hydrology + Vertical_Distance_To_Hydrology\"**","79904da2":"# 5th group\n","fe6793b7":"### Sorting the information","188984e7":"### Sorting the information","84e7a9f3":"### Test","b2ce30e0":"# Conclusion   \n* There are infinities of features to add, and many applicable methods.\n* The code is not fully optimized but it is sorted.   \n* It would be missing to apply some technique of selection of characteristics for to take better cluster values for the test group.   \n* A sample could be taken from the smaller test group in order to create the condensed matrices which would slightly change the code.   \n* Return to the draft of cpa and kernel cpa to improve sample results.   ","0bbc36f1":"### Train","3cb06921":"# Functions","6cf7300d":"# 4th group \n**\"Hillshade_9am + Hillshade_Noon + Hillshade_3pm\"**","fd5b5d5c":"### Train"}}