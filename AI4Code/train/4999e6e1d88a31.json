{"cell_type":{"4f239080":"code","4d70caca":"code","5490f52f":"code","6b599cb6":"code","9901460c":"code","ee107660":"code","051aa32b":"code","327027b4":"code","e30c12fb":"code","b34ec3e7":"markdown","84d97fa7":"markdown","b3399c7d":"markdown","31356561":"markdown","1f2524fa":"markdown"},"source":{"4f239080":"!pip install 'kaggle-environments>=0.1.4'","4d70caca":"import numpy as np\n\nfrom kaggle_environments import evaluate, make","5490f52f":"env = make('connectx', debug=True)\nenv.render()","6b599cb6":"%%writefile submission.py\nimport numpy as np\n\n\ndef my_agent(obs, config):\n    board = np.array(obs.board).reshape(config.rows, config.columns)\n    return int(np.random.choice(np.where(board[0, :] == 0)[0]))","9901460c":"%run submission.py","ee107660":"env.reset()\nenv.run([my_agent, 'random'])\nenv.render(mode='ipython', width=400, height=360)","051aa32b":"env.reset()\nenv.run([my_agent, my_agent])\nenv.render(mode='ipython', width=400, height=360)","327027b4":"trainer = env.train([None, \"random\"])\n\nobservation = trainer.reset()\n\nwhile not env.done:\n    my_action = my_agent(observation, env.configuration)\n    print(\"My Action\", my_action)\n    observation, reward, done, info = trainer.step(my_action)\n    env.render(mode=\"ipython\", width=100, height=90, header=False, controls=False)\nenv.render()","e30c12fb":"def mean_reward(rewards):\n    return sum(r[0] for r in rewards) \/ sum(r[0] + r[1] for r in rewards)\n\n# Run multiple episodes to estimate it's performance.\nprint(\"My Agent vs Random Agent:\",  mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=10)))\nprint(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))","b34ec3e7":"# Test my Agent","84d97fa7":"# Debug\/Train my Agent","b3399c7d":"# Create ConnectX Environment","31356561":"# Evaluate my Agent","1f2524fa":"# Create my Agent"}}