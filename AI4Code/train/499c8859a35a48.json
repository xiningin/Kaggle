{"cell_type":{"86accb65":"code","3a32df45":"code","dbc71a8e":"code","2e72b6ee":"code","6ffc336c":"code","129c248f":"code","4d06ee97":"code","869fc9ec":"code","6061c299":"code","f5539304":"code","230159dc":"code","9f80eae2":"code","77c6acaa":"code","605485c2":"code","b9486da3":"code","2f0edcc6":"code","22354a0a":"code","b2883b2f":"code","a6553980":"code","8ff6836d":"code","d1f9b115":"code","0ed83ff8":"code","462e7b48":"code","649a910e":"code","16247fcc":"code","ee7cc94e":"code","5920a00d":"code","4a966e24":"code","e396749d":"code","23768432":"code","a4461ac6":"code","c3513af5":"code","a796221e":"code","4d62a1a1":"code","1a73e514":"code","f30a4672":"code","a5b726fd":"code","21120f7f":"markdown","9fbc98d4":"markdown","ec370bef":"markdown","72c3b9d1":"markdown","dfb109c6":"markdown","dd9a3266":"markdown"},"source":{"86accb65":"import numpy as np\nimport pandas as pd\nfrom statsmodels.tsa.seasonal import seasonal_decompose \nfrom statsmodels.tools.eval_measures import rmse\nfrom fbprophet import Prophet","3a32df45":"df = pd.read_csv('..\/input\/beer-production-data\/Beer_production.csv')\ndf.head()","dbc71a8e":"df.Month = pd.to_datetime(df.Month)","2e72b6ee":"df = df.set_index(\"Month\")\ndf.head()","6ffc336c":"df.index.freq = 'MS'","129c248f":"ax = df['Monthly beer production'].plot(figsize = (16,5), title = \"Monthly Beer Production\")\nax.set(xlabel='Dates', ylabel='Total Production');","4d06ee97":"a = seasonal_decompose(df[\"Monthly beer production\"], model = \"add\")\na.plot();","869fc9ec":"import matplotlib.pyplot as plt\nplt.figure(figsize = (12,7))\na.seasonal.plot();","6061c299":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","f5539304":"train_data = df[:len(df)-12]\ntest_data = df[len(df)-12:]","230159dc":"scaler.fit(train_data)\nscaled_train_data = scaler.transform(train_data)\nscaled_test_data = scaler.transform(test_data)","9f80eae2":"from keras.preprocessing.sequence import TimeseriesGenerator\n\nn_input = 12\nn_features= 1\ngenerator = TimeseriesGenerator(scaled_train_data, scaled_train_data, length=n_input, batch_size=1)","77c6acaa":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\n\nlstm_model = Sequential()\nlstm_model.add(LSTM(200, activation='relu', input_shape=(n_input, n_features)))\nlstm_model.add(Dense(1))\nlstm_model.compile(optimizer='adam', loss='mse')\n\nlstm_model.summary()","605485c2":"lstm_model.fit_generator(generator,epochs=20)","b9486da3":"losses_lstm = lstm_model.history.history['loss']\nplt.figure(figsize=(12,4))\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.xticks(np.arange(0,21,1))\nplt.plot(range(len(losses_lstm)),losses_lstm);","2f0edcc6":"lstm_predictions_scaled = list()\n\nbatch = scaled_train_data[-n_input:]\ncurrent_batch = batch.reshape((1, n_input, n_features))\n\nfor i in range(len(test_data)):   \n    lstm_pred = lstm_model.predict(current_batch)[0]\n    lstm_predictions_scaled.append(lstm_pred) \n    current_batch = np.append(current_batch[:,1:,:],[[lstm_pred]],axis=1)","22354a0a":"lstm_predictions = scaler.inverse_transform(lstm_predictions_scaled)\nlstm_predictions","b2883b2f":"test_data['LSTM_Predictions'] = lstm_predictions","a6553980":"test_data","8ff6836d":"test_data['Monthly beer production'].plot(figsize = (16,5), legend=True)\ntest_data['LSTM_Predictions'].plot(legend = True);","d1f9b115":"lstm_rmse_error = rmse(test_data['Monthly beer production'], test_data[\"LSTM_Predictions\"])\nlstm_mse_error = lstm_rmse_error**2\nmean_value = df['Monthly beer production'].mean()\n\nprint(f'MSE Error: {lstm_mse_error}\\nRMSE Error: {lstm_rmse_error}\\nMean: {mean_value}')","0ed83ff8":"df_pr = df.copy()\ndf_pr = df.reset_index()","462e7b48":"df_pr.columns = ['ds','y'] # To use prophet column names should be like that","649a910e":"train_data_pr = df_pr.iloc[:len(df)-12]\ntest_data_pr = df_pr.iloc[len(df)-12:]","16247fcc":"m = Prophet()\nm.fit(train_data_pr)\nfuture = m.make_future_dataframe(periods=12,freq='MS')\nprophet_pred = m.predict(future)","ee7cc94e":"prophet_pred.tail()","5920a00d":"prophet_pred = pd.DataFrame({\"Date\" : prophet_pred[-12:]['ds'], \"Pred\" : prophet_pred[-12:][\"yhat\"]})","4a966e24":"prophet_pred = prophet_pred.set_index(\"Date\")","e396749d":"prophet_pred.index.freq = \"MS\"","23768432":"prophet_pred","a4461ac6":"test_data[\"Prophet_Predictions\"] = prophet_pred['Pred'].values","c3513af5":"import seaborn as sns","a796221e":"plt.figure(figsize=(16,5))\nax = sns.lineplot(x= test_data.index, y=test_data[\"Monthly beer production\"])\nsns.lineplot(x=test_data.index, y = test_data[\"Prophet_Predictions\"]);","4d62a1a1":"prophet_rmse_error = rmse(test_data['Monthly beer production'], test_data[\"Prophet_Predictions\"])\nprophet_mse_error = prophet_rmse_error**2\nmean_value = df['Monthly beer production'].mean()\n\nprint(f'MSE Error: {prophet_mse_error}\\nRMSE Error: {prophet_rmse_error}\\nMean: {mean_value}')","1a73e514":"rmse_errors = [lstm_rmse_error, prophet_rmse_error]\nmse_errors = [lstm_mse_error, prophet_mse_error]\nerrors = pd.DataFrame({\"Models\" : [\"LSTM\", \"Prophet\"],\"RMSE Errors\" : rmse_errors, \"MSE Errors\" : mse_errors})","f30a4672":"print(f\"Mean: {test_data['Monthly beer production'].mean()}\")\nerrors","a5b726fd":"plt.figure(figsize=(16,9))\nplt.plot_date(test_data.index, test_data[\"Monthly beer production\"], linestyle=\"-\")\nplt.plot_date(test_data.index, test_data[\"LSTM_Predictions\"], linestyle=\"--\")\nplt.plot_date(test_data.index, test_data[\"Prophet_Predictions\"], linestyle=\":\")\nplt.legend()\nplt.show()","21120f7f":"**FORECAST\nRead Dataset**","9fbc98d4":"**I will be scaling down the data first**","ec370bef":"**Before creating LSTM model we should create a Time Series Generator object**","72c3b9d1":"**Prophet Forecast**\n\nNow I will be using Facebook Prophet to predict the same dataset","dfb109c6":"**LSTM Neural Network**\n* LSTM stands for long short term memory. It is a model or architecture that extends the memory of recurrent neural networks. Typically, recurrent neural networks have \u2018short term memory\u2019 in that they use persistent previous information to be used in the current neural network. Essentially, the previous information is used in the present task. That means we do not have a list of all of the previous information available for the neural node. LSTM introduces long-term memory into recurrent neural networks. It mitigates the vanishing gradient problem, which is where the neural network stops learning because the updates to the various weights within a given neural network become smaller and smaller. It does this by using a series of \u2018gates.\n* LSTM work There are three types of gates within a unit: Input Gate: Scales input to cell (write) Output Gate: Scales output to cell (read) Forget Gate: Scales old cell value (reset) Each gate is like a switch that controls the read\/write, thus incorporating the long-term memory function into the model.","dd9a3266":"**As you know we scaled our data that's why we have to inverse it to see true predictions.**"}}