{"cell_type":{"e06e93c7":"code","dba09167":"code","af91a879":"code","b5bc0b14":"code","5abf05b1":"code","08f89908":"code","f041c17d":"code","36ccc6d1":"code","304f6953":"code","83bc40fc":"code","c126a30b":"code","61e70128":"code","58657f69":"code","6065cf7c":"code","f7fd0790":"code","bef7dbde":"code","8471c3f3":"code","d16ad4fe":"code","57a60995":"code","1ba67312":"code","9e3f40c2":"code","23674d45":"code","bd0ce9be":"code","42ff4298":"code","fcea019f":"code","b595b959":"code","7fec39f7":"markdown","dc037f59":"markdown","cc9fb289":"markdown","a8c69d73":"markdown","896c8875":"markdown","9c7b7d43":"markdown","b2bb8386":"markdown","3c8c3796":"markdown","37f98e49":"markdown","f1338cf4":"markdown","0bed6fde":"markdown","fc8aa9b2":"markdown","09e5f0b6":"markdown","6673b68a":"markdown","80bb9029":"markdown","cc460261":"markdown","034526ad":"markdown","f67e3404":"markdown","7e856dda":"markdown","07ef069d":"markdown","869946bd":"markdown","935f6112":"markdown"},"source":{"e06e93c7":"#Load the librarys\nimport pandas as pd #To work with dataset\nimport numpy as np #Math library\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns #Graph library that use matplot in background\nimport matplotlib.pyplot as plt #to plot some parameters in seaborn\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom sklearn.neighbors import KNeighborsClassifier\n# Import StandardScaler from scikit-learn\nfrom sklearn.preprocessing import PowerTransformer, StandardScaler,Normalizer,RobustScaler,MaxAbsScaler,MinMaxScaler,QuantileTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import GridSearchCV\n# Import train_test_split()\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom datetime import datetime, date\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.model_selection import cross_val_score\nimport lightgbm as lgbm\nfrom catboost import CatBoostRegressor\nimport  tensorflow as tf \nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\n#import smogn\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\n# For training random forest model\nimport lightgbm as lgb\nfrom scipy import sparse\nfrom sklearn.neighbors import KNeighborsRegressor \nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans \nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression,f_classif\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom itertools import combinations\n#import smong \nfrom sklearn.linear_model import LinearRegression, RidgeCV\nimport category_encoders as ce\nimport warnings\nimport optuna \nwarnings.filterwarnings('ignore')","dba09167":"# import lux\n# Load the training data\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\n# Preview the data\ntrain.head()","af91a879":"cat_columns = train.drop(['id','target'], axis=1).select_dtypes(exclude=['int64','float64']).columns\nnum_columns = train.drop(['id','target'], axis=1).select_dtypes(include=['int64','float64']).columns","b5bc0b14":"train[train.select_dtypes(['float64']).columns] = train[train.select_dtypes(['float64']).columns].apply(pd.to_numeric)\ntrain[train.select_dtypes(['object']).columns] = train.select_dtypes(['object']).apply(lambda x: x.astype('category'))","5abf05b1":"# Create arrays for the features and the response variable\ny = train['target']\nX = train.drop(['id','target'], axis=1)","08f89908":"# select non-numeric columns\ncat_columns = train.drop(['id','target'], axis=1).select_dtypes(exclude=['int64','float64']).columns","f041c17d":"# select the float columns\nnum_columns = train.drop(['id','target'], axis=1).select_dtypes(include=['int64','float64']).columns","36ccc6d1":"num_columns=['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7',\n       'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\ncat_columns=['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8',\n       'cat9']\nall_columns = (num_columns+cat_columns)\nprint(cat_columns)\nprint(num_columns)\nprint(all_columns)","304f6953":"if set(all_columns) == set(train.drop(['id','target'], axis=1).columns):\n    print('Ok')\nelse:\n    # Let's see the difference \n    print('dans all_columns mais pas dans train  :', set(all_columns) - set(train.drop(['id','target'], axis=1).columns))\n    print('dans X.columns   mais pas dans all_columns :', set(train.drop(['id','target'], axis=1).columns) - set(all_columns))","83bc40fc":"cross_validation_design = KFold(n_splits=7,\n                                shuffle=True,\n                                random_state=77)\n\ncross_validation_design","c126a30b":"from xgboost import XGBRegressor\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression,f_classif\nfrom sklearn.preprocessing import PolynomialFeatures\n\n\n\n\n###################\nRobustscaler  = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n                      \n                        RobustScaler()\n)\nOrdinalencoder = make_pipeline(\n            SimpleImputer(strategy='most_frequent',add_indicator=True),\n            ce.ordinal.OrdinalEncoder() )\n# Preprocess Pipe : \n##################\nOrdinalEncoder_RobustScaler = make_column_transformer(\n    ( Ordinalencoder , cat_columns),\n    ( Robustscaler, num_columns))\n\n\n# Random HyperParameters\n###################\n\nxgb_params = {\n    #'tree_method':'gpu_hist',         ## parameters for gpu\n    #'gpu_id':0,                       #\n    #'predictor':'gpu_predictor',      #\n    'n_estimators': 10000,\n    'learning_rate': 0.03628302216953097,\n    'subsample': 0.7875490025178415,\n    'colsample_bytree': 0.11807135201147481,\n    'max_depth': 3,\n    'booster': 'gbtree', \n    'reg_lambda': 0.0008746338866473539,\n    'reg_alpha': 23.13181079976304,\n    'n_jobs':-1,\n    'random_state':40}\nXGBR = XGBRegressor(**xgb_params,\n                    objective='reg:squarederror', \n                    #early_stopping_rounds=100 ,\n                    #tree_method='gpu_hist',\n                    #gpu_id=0, \n                    #predictor=\"gpu_predictor\"\n                   )\nXGBRpipe= Pipeline([\n    ('preprocess', OrdinalEncoder_RobustScaler),\n    ('classifier', XGBR)]) \n############################\nLGBM_params = {'n_estimators': 7000,\n            'learning_rate':0.034923843361431936,\n            'subsample': 0.7000000000000001,\n            'colsample_bytree':  0.1,\n            'max_depth': 3,\n            'booster': 'gbtree', \n            'reg_lambda': 100.1,\n            'reg_alpha':0.5333109437994918,\n            'reg_lambda': 97.48121524546883,\n            'random_state':40}\nmodelLGBMRegressor = lgbm.LGBMRegressor(**LGBM_params,\n                                   metric = 'rmse', \n                                   objective= \"rmse\",\n                                   #boosting_type= 'gbdt',\n                                   #device_type='gpu',\n                                   n_jobs = -1,\n                                   min_child_samples =  27,\n                                   #max_bin = 520,\n                                   bagging_seed= 42\n                                ) \nLGBMpipe = Pipeline([\n        ('preprocess', OrdinalEncoder_RobustScaler),\n        ('classifier', modelLGBMRegressor)])\n\n","61e70128":"cat_columns2=['cat0', 'cat1', 'cat2']\ncat_columns1=['cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9']\nnum_columns1=[ 'cont1', 'cont2', 'cont3', 'cont4', 'cont5',  'cont7',  'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\nnum_columns2=['cont0','cont6','cont8']\nall_columns1=cat_columns1+cat_columns2+num_columns1+num_columns2\nif set(all_columns) == set(train.drop(['id','target'], axis=1).columns):\n    print('Ok')\nelse:\n    # Let's see the difference \n    print('dans all_columns mais pas dans train  :', set(all_columns) - set(train.drop(['id','target'], axis=1).columns))\n    print('dans X.columns   mais pas dans all_columns :', set(train.drop(['id','target'], axis=1).columns) - set(all_columns))","58657f69":"from sklearn import set_config\nset_config(display='diagram')\nLGBMpipe","6065cf7c":"XGBRpipe","f7fd0790":"# Split the dataset and labels into training and test sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1,random_state=0)\nprint(\"{} rows in validation set vs. {} in training set. {} Features.\".format(X_val.shape[0], X_train.shape[0], X_val.shape[1]))","bef7dbde":"def get_models(**estimators ):\n    models =[ model for name , model in estimators.items()]\n    return models ","8471c3f3":"# fit the blending ensemble\ndef fit_ensemble(models, X_train, X_val, y_train, y_val,blender):\n\t# fit all models on the training set and predict on hold out set\n\tmeta_X = np.empty((X_val.shape[0], len(models)))\n\tfor i, model in enumerate(models):\n\t\t# fit in training set\n\t\tmodel.fit(X_train, y_train)\n\t\t# predict on hold out set\n\t\ty_val_pred = model.predict(X_val)\n\t\t# store predictions as input for blending\n\t\tmeta_X[:, i] = y_val_pred\n\t# create 2d array from predictions, each set is an input feature\n\t#meta_X = hstack(meta_X)\n\t# define blending model\n\t#blender = LinearRegression()\n\t# fit on predictions from base models\n\tblender.fit(meta_X, y_val)\n\treturn blender","d16ad4fe":"# make a prediction with the blending ensemble\ndef predict_ensemble(models, blender, X_test):\n\t# make predictions with base models\n\tmeta_X = np.empty((X_test.shape[0], len(models)))\n\tfor i, model in enumerate(models):\n\t\t# predict with base model\n\t\tyhat = model.predict(X_test)\n\t\t# store prediction\n\t\tmeta_X[:, i] = yhat\n\t# predict\n\treturn blender.predict(meta_X)","57a60995":"# create the base models\nestimators={'XGBRpipe':XGBRpipe,\n           'LGBMpipe':LGBMpipe}\nmodels = get_models(**estimators)\nmodels","1ba67312":"# train the blending ensemble\n#blender = fit_ensemble(models, X_train, X_val, y_train, y_val,LinearRegression())","9e3f40c2":"#test_final= test.drop(['id'], axis=1)\n#Scratch_predictions = predict_ensemble(models, blender, X_val)\n# summarize prediction\n#rmse = np.sqrt(mean_squared_error(y_val,Scratch_predictions))\n#print(f\" |Scratch_Blender_RMSE: {rmse}\")","23674d45":"class StackingBlender(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models , blender,val_split=0.2):\n        self.base_models =base_models\n        self.blender=blender\n        self.val_split=val_split\n        \n    def fit(self, X ,y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.blender_ = clone(self.blender)\n        # Split the dataset and labels into training and test sets\n        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.val_split,random_state=0)\n        meta_X_val = np.empty((X_val.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            instance =clone(model)\n            instance.fit(X_train,y_train)\n            self.base_models_[i].append(instance)\n            # predict  on val  set\n            y_val_pred = instance.predict(X_val)\n            # store predictions as input for blending\n            meta_X_val[:,i] = y_val_pred\n        # define blending model\n        # fit on predictions from base models\n        self.blender_.fit(meta_X_val, y_val)\n        return self \n    \n    def predict(self, X):\n        # make predictions with base models\n        #meta_X_test = np.empty((X.shape[0], len(self.base_models)))\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models])\n            for base_models in self.base_models_ ])\n        # predict\n        return self.blender_.predict(meta_features)","bd0ce9be":"blender_model = StackingBlender(base_models = (XGBRpipe,LGBMpipe),\n                                                 blender= LinearRegression(),val_split=0.1)","42ff4298":"from sklearn import set_config\nset_config(display='diagram')\nblender_model.fit(X,y)","fcea019f":"preds = blender_model.predict(X_val)\nrmse = np.sqrt(mean_squared_error(y_val,preds))\nprint(f\" | OOP_Blender_RMSE: {rmse}\")","b595b959":"# Use the model to generate predictions\ntest_final= test.drop(['id'], axis=1)\n\ntest_predictions = blender_model.predict(test_final)\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': test.id,\n                       'target': test_predictions})\noutput.to_csv('test_predictions_BlenderOOP.csv', index=False)","7fec39f7":"# Submit : ","dc037f59":"## step 5 test every thing ","cc9fb289":"### Num Features ","a8c69d73":"## Compelete prerocess pipe for  Cat dara ","896c8875":"## step 3 fit the blending ensemble","9c7b7d43":"**fit() \u2013**  Cloning the base model and blender-model, Training the base model on train , and storing the val  predictions. Then training the blender-model using the val predictions.\n\n**predict()\u2013** base model predictions for X will be column stacked and then used as an input for blender-model to predict.","b2bb8386":"### Num\/Cat Features ","3c8c3796":"## step 4 make a prediction with the blending ensemble","37f98e49":"# reference : \n\nreference : \n\n\nhttps:\/\/machinelearningmastery.com\/blending-ensemble-machine-learning-with-python\/\n\nhttps:\/\/www.kaggle.com\/abhishek\/competition-part-4-hyperparameter-tuning-optuna\n\nhttps:\/\/neptune.ai\/blog\/best-tools-for-model-tuning-and-hyperparameter-optimization\n\nhttps:\/\/machinelearningmastery.com\/blending-ensemble-machine-learning-with-python\/","f1338cf4":"## check that we have all column","0bed6fde":"## step 6  train the blending ensemble","fc8aa9b2":"##  What should we do for each colmun\n### Separate features by dtype\n\nNext we\u2019ll separate the features in the dataframe by their datatype. There are a few different ways to achieve this. I\u2019ve used the select_dtypes() function to obtain specific data types by passing in np.number to obtain the numeric data and exclude=['np.number'] to return the categorical data. Appending .columns to the end returns an Index list containing the column names. For the categorical features, we don\u2019t want to include the target income column, so I\u2019ve dropped that.\n### Cat Features ","09e5f0b6":"## Create test and train groups\n\nNow we\u2019ve got our dataframe ready we can split it up into the train and test datasets for our model to use. We\u2019ll use the Scikit-Learn train_test_split() function for this. By passing in the X dataframe of raw features, the y series containing the target, and the size of the test group (i.e. 0.1 for 10%), we get back the X_train, X_test, y_train and y_test data to use in the model.","6673b68a":"|Scratch_Blender_RMSE: 0.7198237741062108\n\n    Private Score\n    0.71624\n    \n    \n    Public Score\n    0.71770","80bb9029":"# OOP Design ","cc460261":"## Blending\nBlending is an ensemble machine learning algorithm.\n\nIt is a colloquial name for stacked generalization or stacking ensemble where instead of fitting the meta-model on out-of-fold predictions made by the base model, it is fit on predictions made on a holdout dataset.\n\nBlending was used to describe stacking models that combined many hundreds of predictive models by competitors in the $1M Netflix machine learning competition, and as such, remains a popular technique and name for stacking in competitive machine learning circles, such as the Kaggle community.\n\nIn this tutorial, you will discover how to develop and evaluate a blending ensemble in python.\n\nAfter completing this tutorial, you will know:\n\n    Blending ensembles are a type of stacking where the meta-model is fit using predictions on a holdout validation dataset instead of out-of-fold predictions.\n    How to develop a blending ensemble, including functions for training the model and making predictions on new data.\n    How to evaluate blending ensembles for classification and regression predictive modeling problems.\n\n**Blending Ensemble**\n\nBlending is an ensemble machine learning technique that uses a machine learning model to learn how to best combine the predictions from multiple contributing ensemble member models.\n\nAs such, blending is the same as stacked generalization, known as stacking, broadly conceived. Often, blending and stacking are used interchangeably in the same paper or model description.\n\n    Many machine learning practitioners have had success using stacking and related techniques to boost prediction accuracy beyond the level obtained by any of the individual models. In some contexts, stacking is also referred to as blending, and we will use the terms interchangeably here.\n\n    \n","034526ad":"Best pipe from last notebook : \n\n","f67e3404":"##  step 7 Evaluation ","7e856dda":"# 1-  Blending  From Scratch : \n## Step 1 :get the dataset\n","07ef069d":"\n## Step 2: Load the data\n\nNext, we'll load the training and test data.\n\nWe set index_col=0 in the code cell below to use the id column to index the DataFrame. (If you're not sure how this works, try temporarily removing index_col=0 and see how it changes the result.)\n","869946bd":"## Define the model features and target\n### Extract X and y ","935f6112":"## Step 2 : Get list of models   "}}