{"cell_type":{"d75ded55":"code","716f4c20":"code","f55a8422":"code","d2983e95":"code","1e2d9da6":"code","ca534f52":"code","39e6d62c":"code","5e81dfd9":"code","e8d12d0e":"code","43c462e0":"code","632b3ba4":"code","4cd9adbf":"code","ef411ee9":"code","cce8cd6c":"code","df493e61":"code","0cde46cd":"markdown"},"source":{"d75ded55":"# imports\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nimport pandas as pd\nimport numpy as np \nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","716f4c20":"# read files\nsubmission = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/sample_submission.csv\",index_col='id')\ndf_train = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/train.csv\",index_col='id')\ndf_test = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/test.csv\",index_col='id')\ndf = pd.concat([df_train, df_test],axis=0,ignore_index=True) # combine training and testing data","f55a8422":"# check submission file first, which gives an idea as what to do, we need to predict probability\nsubmission.head()","d2983e95":"# function to describe variables\ndef desc(df):\n    summ = pd.DataFrame(df.dtypes,columns=['Data_Types'])\n    summ = summ.reset_index()\n    summ['Columns'] = summ['index']\n    summ = summ[['Columns','Data_Types']]\n    summ['Missing'] = df.isnull().sum().values    \n    summ['Uniques'] = df.nunique().values\n    return summ\n\n# function to analyse missing values\ndef nulls_report(df):\n    nulls = df.isnull().sum()\n    nulls = nulls[df.isnull().sum()>0].sort_values(ascending=False)\n    nulls_report = pd.concat([nulls, nulls \/ df.shape[0]], axis=1, keys=['Missing_Values','Missing_Ratio'])\n    return nulls_report","1e2d9da6":"# test data, there are no missing values\ndesc(df_test)","ca534f52":"# distribution of target variable\ndf_train['target'].value_counts(normalize = True).plot(kind='barh',title='Distribution of Target Variable')","39e6d62c":"# Binary Encdoing\n# bin_o and bin_1 need not be converted as these are already converted\n# bin_3and bin_4 are binary variables representing T\/F and Y\/N. We can convert them to 0 or 1.\ndf['bin_3'] = df['bin_3'].map({'T':1,'F':0})\ndf['bin_4'] = df['bin_4'].map({'Y':1,'N':0})","5e81dfd9":"# Ordinal Encoding\n# ord_0 need not to be converted\n# ord_1 and ord_2 has ordinal data. We can manually encode these variables.\n# ( ord_3,ord_4,ord_5 are of hight cardinality)\n\n# ord_1 and ord_2\nd1 = {'Grandmaster': 5, 'Expert': 4 , 'Novice':1 , 'Contributor':2 , 'Master': 3}\nd2 = {'Cold': 2, 'Hot':4, 'Lava Hot': 6, 'Boiling Hot': 5, 'Freezing': 1, 'Warm': 3}\ndf['ord_1'] = df['ord_1'].map(d1)\ndf['ord_2'] = df['ord_2'].map(d2)\n\n# ord_3 and ord_4\ndf['ord_3'] = df['ord_3'].astype('category')\ndf['ord_4'] = df['ord_4'].astype('category')\nd3 = dict(zip(df['ord_3'],df['ord_3'].cat.codes))\nd4 = dict(zip(df['ord_4'],df['ord_4'].cat.codes))\ndf['ord_3'] = df['ord_3'].map(d3)\ndf['ord_4'] = df['ord_4'].map(d4)\n\ndf['ord_3'] = df['ord_3'].astype(int)\ndf['ord_4'] = df['ord_4'].astype(int)\n\n#  ord_5\nli = sorted(list(set(df['ord_5'].values)))\nd5 = dict(zip(li, range(len(li))))  # mapping dict for ord_5\ndf['ord_5'] = df['ord_5'].map(d5)","e8d12d0e":"# one hot encoding for column : nom_0 to nom_4\ndf = pd.get_dummies(df, columns=['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4'],\n                        prefix=['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4'], \n                        drop_first=True)","43c462e0":"# encoding hex feature\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfeatures_hex = ['nom_5','nom_6','nom_7','nom_8','nom_9']\n\nfor col in features_hex:\n    le.fit(df[col])\n    df[col] = le.transform(df[col])","632b3ba4":"# convert cyclical features such as day and month into 2d sin-cos features\ndf['day_sin'] = np.sin(2*np.pi * df['day']\/7)\ndf['day_cos'] = np.cos(2*np.pi * df['day']\/7)\ndf['month_sin'] = np.sin(2*np.pi * df['month']\/12)\ndf['month_cos'] = np.cos(2*np.pi * df['month']\/12)\ndf.drop(columns=['day','month'],inplace=True)\n\n# plot features in 2d\ndf.sample(1000).plot.scatter('month_sin','month_cos').set_aspect('equal')","4cd9adbf":"# get training ,testing, validation and target\nfrom sklearn.model_selection import train_test_split\ny_train = df_train['target']\nX_train = df[:len(df_train)].drop(['target'],axis=1)\nX_test = df[len(df_train):].drop(['target'],axis=1)\n\n# split train and validation data\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.001, random_state=0)","ef411ee9":"# xgboost\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier(objective= 'binary:logistic',\n                    learning_rate=0.1,\n                    max_depth=3,\n                    n_estimators=200,\n                    scale_pos_weight=2,\n                    random_state=1,\n                    colsample_bytree=0.5)\n\nxgb.fit(X_train,y_train)","cce8cd6c":"#from sklearn.linear_model import LogisticRegression\n#lr=LogisticRegression(C=0.125, solver=\"lbfgs\", max_iter=500) \n#lr.fit(X_train, y_train)","df493e61":"#submission\ny_pred = xgb.predict_proba(X_test)[:, 1]\nsubmission = pd.DataFrame({'id':df_test.index,'target':y_pred})\nsubmission.to_csv('submission.csv', index=False)","0cde46cd":"## Simple XGboost model\n\nI have used XGBClassifier with simple coding that a beginner can understand easily. If you like it useful, please vote. Your comments are also welcome."}}