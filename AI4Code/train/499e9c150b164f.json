{"cell_type":{"f73422ed":"code","70b78b7a":"code","458f656a":"code","9187c430":"code","4246ccd8":"code","f1011dfd":"code","bd5650ae":"code","1c4aa437":"code","99a39656":"code","e7818db3":"code","708fe02b":"code","4f8f1322":"markdown","c668de35":"markdown","8a79205d":"markdown","7c63acdd":"markdown"},"source":{"f73422ed":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","70b78b7a":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import StandardScaler\nimport lightgbm as lgb\n\nimport gc\nimport warnings\nwarnings.simplefilter(\"ignore\")","458f656a":"# samples = 10000\ntrain_df = pd.read_pickle('..\/input\/ump-train-picklefile\/train.pkl')#[:samples]","9187c430":"\ntrain_df.head()\n\n#Selecting the features to predict with\nfeatures = [f'f_{i}' for i in range(300)]\n\ntrain_df.info()\n","4246ccd8":"print(\"Unique ids: \",train_df[\"investment_id\"].nunique(),\"\\n\")\n\ntrain_df[\"investment_id\"].value_counts()","f1011dfd":"train_df.isnull().sum().sum() #No null values","bd5650ae":"investment_ids = train_df[\"investment_id\"].unique()\nnum_ids = len(investment_ids)\ntrain_size = 0.8\n\ntrain_ids = investment_ids[:int(num_ids * train_size)]\nvalid_ids = investment_ids[int(num_ids * train_size):]\n\ntrain = train_df[train_df[\"investment_id\"].isin(train_ids)]\nvalid = train_df[train_df[\"investment_id\"].isin(valid_ids)]\n\ndel train_df;gc.collect()","1c4aa437":"train_X = train[[\"investment_id\"]+features].values\nvalid_X = valid[[\"investment_id\"]+features].values\n\ntrain_y = train.target.values\nvalid_y = valid.target.values","99a39656":"scaler = StandardScaler()\ntrain_X = scaler.fit_transform(train_X)\nvalid_X = scaler.transform(valid_X)\n\nlgb_train = lgb.Dataset(train_X, train_y, categorical_feature=[0],free_raw_data=False)\nlgb_eval = lgb.Dataset(valid_X,valid_y,categorical_feature=[0],free_raw_data=False)\n\ndel train_X; del valid_X;gc.collect()","e7818db3":"params = {\n        \"random_state\": 420, \n        \"verbosity\": -2,\n        \"metrics\": \"rmse\",\n        \"n_estimators\":1500, \n        \"learning_rate\":0.1, \n        \"subsample\":0.8,\n        \"feature_fraction\":0.6\n}\n\nmodel = lgb.train(\n            params, lgb_train, valid_sets = lgb_eval,\n            verbose_eval = False,\n            num_boost_round = 1500,\n            early_stopping_rounds = 100,\n           \n)\n\ndel lgb_train\ndel lgb_eval\ngc.collect()","708fe02b":"import ubiquant\nenv = ubiquant.make_env()                   # initialize the environment\niter_test = env.iter_test()                 # an iterator which loops over the test set and sample submission\nfor (test_df, sample_prediction_df) in iter_test:\n    preds = model.predict(test_df[[\"investment_id\"] + features].values)\n    sample_prediction_df[\"target\"] = preds  # make your predictions here\n    env.predict(sample_prediction_df)       # register your predictions","4f8f1322":"### The dataset is loaded from a smaller size pickle file due to memory constraints\n\nThanks to colum2131 for his [kernel](https:\/\/www.kaggle.com\/columbia2131\/speed-up-reading-csv-to-pickle) that reduces the size of the dataset!\n","c668de35":"### Training the model","8a79205d":"### Splitting data into train and validation","7c63acdd":"### Scaling features and creating lgbm\nDeleting old DataFrames to save memory"}}