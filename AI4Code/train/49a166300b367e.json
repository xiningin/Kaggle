{"cell_type":{"5739f63c":"code","59ae5a65":"code","7e52fb8e":"code","be8151f8":"code","f99ef99c":"code","3d04854b":"code","22ff1159":"code","342d742b":"code","42303164":"code","3edef7a8":"code","14f6ce84":"code","8cb1d6ea":"code","d71d02c5":"code","92b1fad2":"code","68c04770":"code","171dbaeb":"code","232db69b":"code","d68b1254":"code","c9c19aca":"code","ee9f818a":"markdown"},"source":{"5739f63c":"import os\nprint(os.listdir('\/kaggle\/input\/flowers-recognition\/flowers'))","59ae5a65":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","7e52fb8e":"# we cannot use ImageDataGenerator Class as we don't have the specified structure","be8151f8":"# get the paths of each and every image from the directory\nimport os\nimport cv2\nfrom glob import glob\ndaisy_path = glob('\/kaggle\/input\/flowers-recognition\/flowers\/daisy\/*')\ndandelion_path = glob('\/kaggle\/input\/flowers-recognition\/flowers\/dandelion\/*')\nrose_path = glob('\/kaggle\/input\/flowers-recognition\/flowers\/rose\/*')\nsunflower_path = glob('\/kaggle\/input\/flowers-recognition\/flowers\/sunflower\/*')\ntulip_path = glob('\/kaggle\/input\/flowers-recognition\/flowers\/tulip\/*')","f99ef99c":"images, labels = [], []","3d04854b":"# daisy\nfor image_path in daisy_path:\n    try:\n        image = cv2.imread(image_path)\n        image = cv2.resize(image, (224, 224)) # default values\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # converting to RGB\n        images.append(image)\n        labels.append(0) # 0 for daisy\n    except:\n        print(image_path)\n\n# dandelion\nfor image_path in dandelion_path:\n    try:\n        image = cv2.imread(image_path)\n        image = cv2.resize(image, (224, 224)) # default values\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # converting to RGB\n        images.append(image)\n        labels.append(1) # 1 for dandelion\n    except:\n        print(image_path)\n\n# rose\nfor image_path in rose_path:\n    try:\n        image = cv2.imread(image_path)\n        image = cv2.resize(image, (224, 224)) # default values\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # converting to RGB\n        images.append(image)\n        labels.append(2) # 2 for rose\n    except:\n        print(image_path)\n\n# sunflower\nfor image_path in sunflower_path:\n    try:\n        image = cv2.imread(image_path)\n        image = cv2.resize(image, (224, 224)) # default values\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # converting to RGB\n        images.append(image)\n        labels.append(3) # 3 for sunflower\n    except:\n        print(image_path)\n\n# tulip\nfor image_path in tulip_path:\n    try:\n        image = cv2.imread(image_path)\n        image = cv2.resize(image, (224, 224)) # default values\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # converting to RGB\n        images.append(image)\n        labels.append(4) # 4 for tulip\n    except:\n        print(image_path)","22ff1159":"# convert the list into array\ndata = np.array(images)\nlabels = np.array(labels)\n\nprint(data.shape)\nprint(labels.shape)","342d742b":"from keras.utils import to_categorical\nlabels = to_categorical(labels, num_classes = 5)\nlabels.shape","42303164":"# splitting training and testing data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data, labels,\n                                                    test_size = 0.2,\n                                                    random_state = 101,\n                                                    shuffle = True)\nprint('training:')\nprint(X_train.shape)\nprint(y_train.shape)\nprint('testing:')\nprint(X_test.shape)\nprint(y_test.shape)","3edef7a8":"# make the data that can be used by the CNN\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndatagenerator = ImageDataGenerator(horizontal_flip = True, vertical_flip = True)\naugmented_training_data = datagenerator.flow(X_train, y_train, batch_size = 32)","14f6ce84":"# preparing the convolutional neural network\nfrom keras.models import Sequential\nfrom keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense, Dropout","8cb1d6ea":"cnn = Sequential()\ncnn.add(Convolution2D(filters = 32, kernel_size = (3, 3), input_shape = (224, 224, 3), activation = 'relu'))\ncnn.add(MaxPooling2D(pool_size = (3, 3), strides = 2))\ncnn.add(Dropout(rate = 0.2))\ncnn.add(Convolution2D(filters = 64, kernel_size = (3, 3)))\ncnn.add(MaxPooling2D(pool_size = (3, 3), strides = 2))\ncnn.add(Dropout(rate = 0.2))\ncnn.add(Flatten())\n# cnn.add(Dense(units = 64, activation = 'relu'))\n# cnn.add(Dense(units = 128, activation = 'relu'))\ncnn.add(Dense(units = 5, activation = 'softmax'))\ncnn.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])","d71d02c5":"cnn.summary()","92b1fad2":"history = cnn.fit_generator(augmented_training_data, epochs = 50)","68c04770":"# plotting the graphs of epoch, accuracy and loss\n\nimport matplotlib.pyplot as plt\nplt.plot(history.epoch, history.history['accuracy'], color = 'green')\nplt.xlabel('Epoch Number')\nplt.ylabel('Accuracy')\nplt.show()","171dbaeb":"# plotting the losses\nplt.plot(history.epoch, history.history['loss'], color = 'green')\nplt.xlabel('Epoch Number')\nplt.ylabel('loss')\nplt.show()","232db69b":"cnn.evaluate(X_test, y_test)","d68b1254":"# confusion matrix\nfrom sklearn.metrics import confusion_matrix\n\ny_true = np.argmax(y_test, axis = 1) # decoding\n\npredicted_classes = cnn.predict_classes(X_test)\n\nconfusion_mat = confusion_matrix(y_true, predicted_classes)","c9c19aca":"import seaborn as sns\nsns.heatmap(confusion_mat, annot = True)\nplt.show()","ee9f818a":"Thank you for your time, please comment if you have any creative suggestions."}}