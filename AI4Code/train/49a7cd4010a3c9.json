{"cell_type":{"adf19f0a":"code","36964fd0":"code","9206b654":"code","2fc18ae0":"code","058056c6":"code","cbb9b597":"code","1d9f5599":"code","c475e70c":"code","929c931a":"code","afd97684":"code","35683235":"code","40aa7571":"code","57fd3d84":"code","94c9e120":"code","392eb969":"code","9ac82aa5":"code","0faf1232":"code","e6b48dea":"code","1ccb33a3":"code","37a01061":"code","29315f37":"code","610658fe":"code","c0d103a9":"code","d5f6a7fa":"code","c6d04c0a":"code","eca67190":"code","a47ee165":"code","d462ff85":"code","e12b7ad5":"code","08e4fe41":"code","2247efdb":"code","5135e5cf":"code","4d951da2":"code","f41487f4":"code","1774e528":"code","a7193800":"code","43dacbf7":"code","cebcd6a7":"code","746e9eb8":"code","09482fdc":"code","dbfd7f4c":"code","d6e3667e":"code","9151244a":"code","d8479d66":"code","057ff1a4":"code","ac0c45d8":"code","e1c64ca4":"code","14d3b71a":"markdown","9ac47b8d":"markdown"},"source":{"adf19f0a":"# utilities\nimport re\nimport numpy as np\nimport pandas as pd","36964fd0":"# plotting\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt","9206b654":"# nltk\nfrom nltk.stem import WordNetLemmatizer","2fc18ae0":"# sklearn\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix, classification_report","058056c6":"# Importing the dataset\nDATASET_COLUMNS=['target','ids','date','flag','user','text']\nDATASET_ENCODING = \"ISO-8859-1\"\ndf = pd.read_csv('..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', encoding=DATASET_ENCODING, names=DATASET_COLUMNS)\ndf.head()","cbb9b597":"df.sample(5)","1d9f5599":"df.columns","c475e70c":"print('length of data is', len(df))","929c931a":"df.shape","afd97684":"df.info()","35683235":"df.dtypes","40aa7571":"df.isnull().sum()","57fd3d84":"print('Count of columns in the data is:  ', len(df.columns))\nprint('Count of rows in the data is:     ', len(df))","94c9e120":"df['target'].unique()","392eb969":"df['target'].nunique()","9ac82aa5":"# Plotting the distribution for dataset.\nax = df.groupby('target').count().plot(kind='bar', title='Distribution of data',legend=True,figsize=(20, 5))\nax.set_xticklabels(['Negative','Positive'], rotation=45)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\n# Storing data in lists.\ntext, sentiment = list(df['text']), list(df['target'])","0faf1232":"import seaborn as sns\nplt.figure(figsize = (20,5))\nax=sns.countplot(x='target', data=df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))","e6b48dea":"data=df[['text','target']]\ndata.head()","1ccb33a3":"data['target'] = data['target'].replace(4,1)\ndata['target'].value_counts()","37a01061":"data['target'].unique()","29315f37":"data_pos = data[data['target'] == 1]\ndata_pos.head()","610658fe":"data_neg = data[data['target'] == 0]\ndata_neg.head()","c0d103a9":"data_pos = data_pos.iloc[:]\ndata_neg = data_neg.iloc[:]\ndataset = pd.concat([data_pos, data_neg])\ndataset.head()","d5f6a7fa":"dataset['text']=dataset['text'].str.lower()\ndataset['text'].sample(5)","c6d04c0a":"from nltk.corpus import stopwords\nSTOPWORDS=stopwords.words('english')\nSTOPWORDS[:5]","eca67190":"def cleaning_stopwords(text):\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\ndataset['text'] = dataset['text'].apply(lambda text: cleaning_stopwords(text))\ndataset['text'].sample(5)","a47ee165":"import string\nenglish_punctuations = string.punctuation\npunctuations_list = english_punctuations\ndef cleaning_punctuations(text):\n    translator = str.maketrans('', '', punctuations_list)\n    return text.translate(translator)\ndataset['text']= dataset['text'].apply(lambda x: cleaning_punctuations(x))\ndataset['text'].sample(5)","d462ff85":"def cleaning_repeating_char(text):\n    return re.sub(r'(.)1+', r'1', text)\ndataset['text'] = dataset['text'].apply(lambda x: cleaning_repeating_char(x))\ndataset['text'].head()","e12b7ad5":"def cleaning_URLs(data):\n    return re.sub('((www.[^s]+)|(https?:\/\/[^s]+))',' ',data)\ndataset['text'] = dataset['text'].apply(lambda x: cleaning_URLs(x))\ndataset['text'].head()","08e4fe41":"def cleaning_numbers(data):\n    return re.sub('[0-9]+', '', data)\ndataset['text'] = dataset['text'].apply(lambda x: cleaning_numbers(x))\ndataset['text'].head()","2247efdb":"import nltk\nst = nltk.PorterStemmer()\ndef stemming_on_text(data):\n    text = [st.stem(word) for word in data]\n    return data\ndataset['text']= dataset['text'].apply(lambda x: stemming_on_text(x))\ndataset['text'].head()","5135e5cf":"lm = nltk.WordNetLemmatizer()\ndef lemmatizer_on_text(data):\n    text = [lm.lemmatize(word) for word in data]\n    return data\ndataset['text'] = dataset['text'].apply(lambda x: lemmatizer_on_text(x))\ndataset['text'].head()","4d951da2":"X=dataset.text\nX.sample(5)","f41487f4":"y=dataset.target\ny.sample(5)","1774e528":"data_neg = dataset[dataset['target']==0]['text']\nwc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n               collocations=False).generate(\" \".join(data_neg))\nplt.figure(figsize = (20,20))\nplt.axis('off')\nplt.imshow(wc);","a7193800":"data_pos = dataset[dataset['target']==1]['text']\nwc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n              collocations=False).generate(\" \".join(data_pos))\nplt.figure(figsize = (20,20))\nplt.axis('off')\nplt.imshow(wc);","43dacbf7":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.1, random_state =42,stratify=y)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","cebcd6a7":"vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=50000)\nvectoriser.fit(X_train)\nprint('No. of feature_words: ', len(vectoriser.get_feature_names()))","746e9eb8":"X_train = vectoriser.transform(X_train)\nX_train","09482fdc":"X_test  = vectoriser.transform(X_test)\nX_test","dbfd7f4c":"def model_Evaluate(model):\n    # Predict values for Test dataset\n    y_pred = model.predict(X_test)\n    # Print the evaluation metrics for the dataset.\n    print(classification_report(y_test, y_pred))\n    # Compute and plot the Confusion matrix\n    cf_matrix = confusion_matrix(y_test, y_pred)\n    categories = ['Negative','Positive']\n    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() \/ np.sum(cf_matrix)]\n    labels = [f'{v1} : {v2}' for v1, v2 in zip(group_names,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n    xticklabels = categories, yticklabels = categories)\n    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n    plt.ylabel(\"Actual values\" , fontdict = {'size':14}, labelpad = 10)\n    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)","d6e3667e":"BNBmodel = BernoulliNB()\nBNBmodel.fit(X_train, y_train)\nmodel_Evaluate(BNBmodel)\ny_pred1 = BNBmodel.predict(X_test)","9151244a":"from sklearn.metrics import roc_curve, auc\nfpr, tpr, thresholds = roc_curve(y_test, y_pred1)\nroc_auc = auc(fpr, tpr)\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC CURVE')\nplt.legend(loc=\"lower right\")\nplt.grid()","d8479d66":"SVCmodel = LinearSVC()\nSVCmodel.fit(X_train, y_train)\nmodel_Evaluate(SVCmodel)\ny_pred2 = SVCmodel.predict(X_test)","057ff1a4":"from sklearn.metrics import roc_curve, auc\nfpr, tpr, thresholds = roc_curve(y_test, y_pred2)\nroc_auc = auc(fpr, tpr)\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC CURVE')\nplt.legend(loc=\"lower right\")\nplt.grid()","ac0c45d8":"LRmodel = LogisticRegression(C = 2, max_iter = 1000, n_jobs=-1)\nLRmodel.fit(X_train, y_train)\nmodel_Evaluate(LRmodel)\ny_pred3 = LRmodel.predict(X_test)","e1c64ca4":"from sklearn.metrics import roc_curve, auc\nfpr, tpr, thresholds = roc_curve(y_test, y_pred3)\nroc_auc = auc(fpr, tpr)\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC CURVE')\nplt.legend(loc=\"lower right\")\nplt.grid()","14d3b71a":"# Logistic Regression is best one for this usercase","9ac47b8d":"int(100000)"}}