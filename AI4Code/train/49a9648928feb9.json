{"cell_type":{"f2839c69":"code","c20b6da9":"code","233fab80":"code","de4fbd89":"code","4d38b594":"code","0db533ae":"code","d91d25c5":"code","3f1c56e2":"code","54b747bc":"code","f0f87f8b":"code","38265d20":"code","46534439":"code","ed7438f7":"code","0728d484":"code","f3ece860":"code","e2bec560":"code","7290ed19":"code","b0d4451e":"code","403caafa":"code","447089a4":"code","650faa60":"code","0b53c598":"code","c1d74e2c":"code","ce925c59":"code","bbf00623":"code","a5ede9e2":"code","8b2df842":"code","c26d62d5":"code","e434d74e":"code","a1bcc82e":"code","2b0c6339":"code","fd3e3564":"code","7ab36a77":"code","720d62b0":"code","4403bf2c":"code","3204b0b0":"code","f090933a":"code","8fc1dcf8":"code","e8f1beab":"code","7f83ef24":"code","27c2f412":"code","45a58fd4":"code","04ffff8e":"code","6bf50c62":"code","42eb0581":"code","0b227906":"code","a244a862":"code","c40147c6":"code","98e36934":"code","d48c7632":"code","f77559fd":"code","7679208b":"code","1f6bed9f":"code","fc35f7c7":"code","5eafe02d":"code","3c68fd96":"code","93ca7f1a":"code","1bee543e":"code","0b26bdc9":"code","28a87d46":"code","1ae34994":"code","31478590":"code","c6db559b":"code","d1fe8433":"code","8d2a85e6":"code","16454f85":"code","617c29c3":"code","e289d6e5":"code","1ee91102":"code","6be9096b":"code","e326bfe5":"code","fade184a":"code","dbaf94eb":"code","7f5306f5":"code","e973f825":"code","34e2326b":"code","a260f565":"code","0f239c16":"code","5073189e":"code","5fe9c4c1":"code","29441330":"code","1506b284":"code","084b1d37":"code","04475101":"code","2cbc7635":"code","0c044ef0":"code","a4aaa26c":"code","df32e1e7":"code","545869e6":"code","b72454b2":"code","b92344c5":"code","b97f3eaa":"code","8eb04de1":"code","19de14fb":"code","55de884d":"code","8d44e489":"code","b53305bc":"code","cd60ac96":"code","38605aa0":"code","3d158b0e":"code","36dcd69b":"code","76df1d28":"code","2b175bdd":"code","54dde176":"code","f04fb3f2":"code","ca2ba158":"code","ded6d667":"code","c221083b":"code","c9ecf01c":"code","a46ffa17":"code","9a13e2c9":"code","a5a5c01d":"code","67e01c36":"code","02b127a1":"code","f0a1d7c3":"code","c87e359d":"code","25c4f73a":"code","38c4c492":"code","42beaaad":"code","5362dbde":"code","c5ec74ea":"code","52d4e895":"code","6e1795a9":"code","35f1b64e":"code","75395f65":"code","35cb884b":"code","afb4c67f":"code","fe365534":"code","42de4071":"code","82ee6914":"code","4b53534a":"code","2d85ff64":"code","2c504957":"code","ecaf7da5":"code","942d8136":"code","88a24663":"code","1198c6f7":"code","07d3b179":"code","33373a6f":"code","cc11523e":"code","86fcf292":"code","5ae42b88":"code","3bba1cc3":"code","c90d74a2":"code","8666834a":"code","4b41d124":"code","11ded512":"code","37944f24":"code","e17db5e1":"code","426eb2b6":"code","84e82414":"code","5fc68584":"code","d37f4697":"code","c1c984b2":"code","ff5d1ae0":"code","21e64e34":"code","c4b44e7c":"code","089803c3":"markdown","79b8dcd1":"markdown","fb25ffc6":"markdown","03ded65e":"markdown","02743f13":"markdown","5dae5d0d":"markdown","cddc96fd":"markdown","dba545ca":"markdown","50dec378":"markdown","46fc4ca0":"markdown","3a914ff7":"markdown","f5e83d0e":"markdown","849e6a0f":"markdown","7e9fbb15":"markdown","803bedeb":"markdown","9fdc5ef8":"markdown","455f606f":"markdown","a36417b7":"markdown","897f0ec1":"markdown","85772d6b":"markdown","be7377b7":"markdown","290b5174":"markdown","212d0872":"markdown","a1b4baa0":"markdown","425b3d47":"markdown","94045d55":"markdown","e8f4a445":"markdown","04db0f36":"markdown","81864b26":"markdown","009b67b8":"markdown","13650919":"markdown","46742258":"markdown","36deb5b2":"markdown","ab72cf51":"markdown","cc203883":"markdown","4a6b7a35":"markdown","9c6f614c":"markdown","00acdedd":"markdown","cab6a02b":"markdown","1a35b75a":"markdown","9cb70e60":"markdown","d221bb5c":"markdown","11b4a3b8":"markdown","89db3d31":"markdown","2ba40c80":"markdown","4324fba4":"markdown","65346da1":"markdown"},"source":{"f2839c69":"# Supressing the warnings\nimport warnings\nwarnings.filterwarnings('ignore')","c20b6da9":"# Importing the required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport seaborn as sns\n%matplotlib inline\n\n# Styling the plot\nstyle.use('ggplot')\nsns.set_style('whitegrid')\n\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score, recall_score","233fab80":"# Adjusting Output Views\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\npd.set_option('display.expand_frame_repr', False)","de4fbd89":"# Importing the dataset\nleads = pd.read_csv('..\/input\/leads-dataset\/Leads.csv')\nleads.head()","4d38b594":"# Shape\nleads.shape","0db533ae":"# Null Value and Data Type Information\nleads.info()","d91d25c5":"# Checking Duplicate Values in 'Prospect ID'\nsum(leads.duplicated(subset = 'Prospect ID')) == 0","3f1c56e2":"# Duplicate Values in 'Lead Number'\nsum(leads.duplicated(subset = 'Lead Number')) == 0","54b747bc":"# Using this for Lead Score at the end of the notebook\nLead_Number = leads['Lead Number']","f0f87f8b":"# Droppping these columns as we don't fit these in the model\ncols_to_drop = ['Prospect ID', 'Lead Number']","38265d20":"# Replacing the 'Select' with NA values\nleads = leads.replace('Select', np.nan)","46534439":"# Missing Values Percentages\nround((leads.isnull().sum()\/len(leads)*100), 2)","ed7438f7":"# Let us plot a bar graph to look at the proportion of null values with a benchmark of 40%\nplt.figure(figsize = [25,7])\nplt.title(\"Proportion of Null Values\",fontsize=20)\nplt.xlabel(\"Column Names\", fontsize=15)\nplt.ylabel(\"Percentage of null values\", fontsize= 15)\nplt.xticks(rotation=90)\nax = sns.barplot(leads.columns,round((leads.isnull().sum()\/leads.shape[0])*100,2), color = 'blue')\nax.axhline(40, ls='--',color='red')\nplt.show()","0728d484":"# Removing the missing values with more than 40%\ncols = leads.columns\n\nfor i in cols:\n    if((100*leads[i].isnull().sum()\/len(leads.index)) >= 40):\n        leads.drop(i, axis = 1, inplace = True)","f3ece860":"# Missing Value Percentages for the remaining variables\nround((leads.isnull().sum()\/len(leads)*100), 2)","e2bec560":"# City\nleads['City'].value_counts(dropna = False)","7290ed19":"# Dropping the City column as most entries are Mumbai or not known. So, it won't help much in our analysis\ncols_to_drop.append('City')","b0d4451e":"# Specialization\nleads['Specialization'].value_counts(dropna= False)","403caafa":"leads['Specialization'] = leads['Specialization'].replace(np.nan, 'Not Specified')","447089a4":"# Grouping values with low count in order to remove bias\n# Management Specialization\nleads['Specialization'] = leads['Specialization'].replace(['Finance Management', 'Human Resource Management',\n                                                          'Marketing Management', 'Operations Management',\n                                                          'IT Projects Management', 'Supply Chain Management',\n                                                          'Healthcare Management', 'Hospitality Management',\n                                                          'Retail Management'], 'Management Specializations')\n# Business Specialization\nleads['Specialization'] = leads['Specialization'].replace(['International Business', 'Rural and Agribusiness',\n                                                          'E-Business'], 'Business Specializations')","650faa60":"# 'Tags'\nleads['Tags'].value_counts(dropna = False)","0b53c598":"# Grouping values with low count in order to remove bias\nleads['Tags'] = leads['Tags'].replace(np.nan, 'Not Specified')\nleads['Tags'] = leads['Tags'].replace(['Recognition issue (DEC approval)', 'Shall take in the next coming month', \n                                      'University not recognized', 'Lateral student',\n                                       'In confusion whether part time or DLP', 'Interested in Next batch, Still Thinking',\n                                       'Want to take admission but has financial problems', 'Lost to Others',\n                                      'in touch with EINS', 'number not provided', 'opp hangup', 'wrong number given',\n                                      'Diploma holder (Not Eligible)', 'invalid number', 'Graduation in progress',\n                                      'Interested  in full time MBA', 'Not doing further education', 'Lost to EINS', 'Busy',\n                                      'switched off'], 'Other Tags')","c1d74e2c":"# 'What matters most to you in choosing a course'\nleads['What matters most to you in choosing a course'].value_counts(dropna = False)","ce925c59":"# Dropping 'What matters most to you in choosing a course' due to the presence of high Null Values and majority of the \n# them being Better Career Prospects\ncols_to_drop.append('What matters most to you in choosing a course')","bbf00623":"# 'What is your current occupation'\nleads['What is your current occupation'].value_counts(dropna = False)","a5ede9e2":"# Grouping values with low count in order to remove bias\nleads['What is your current occupation'] = leads['What is your current occupation'].replace(np.nan, 'Not Specified')\nleads['What is your current occupation'] = leads['What is your current occupation'].replace(['Housewife', 'Businessman'],\n                                                                                           'Other')","8b2df842":"# 'Country'\nleads['Country'].value_counts(dropna = False)","c26d62d5":"# Dropping Country column since, more than 90% values are India or NA only\ncols_to_drop.append('Country')","e434d74e":"# Lead Source\nleads['Lead Source'].value_counts(dropna= False)","a1bcc82e":"# Replacing the NULL Values in 'Lead Source' with Google and grouping values with low count in order to remove bias\nleads['Lead Source'] = leads['Lead Source'].replace([np.nan, 'google'], 'Google')\nleads['Lead Source'] = leads['Lead Source'].replace(['Referral Sites', 'Facebook', 'bing', 'google', 'Click2call',\n                                                     'Live Chat', 'Social Media', 'Press_Release', 'testone',\n                                                     'Pay per Click Ads', 'WeLearn', 'blog', 'welearnblog_Home',\n                                                     'NC_EDM', 'youtubechannel'], 'Others')","2b0c6339":"# Last Activity\nleads['Last Activity'].value_counts(dropna = False)","fd3e3564":"# Replacing the NA values and other less occuring entries as Others\nleads['Last Activity'] = leads['Last Activity'].replace(np.nan, 'Email Opened')\nleads['Last Activity'] = leads['Last Activity'].replace(['Visited Booth in Tradeshow', 'Resubscribed to emails',\n                                                        'Email Marked Spam', 'Email Received',\n                                                         'View in browser link Clicked', 'Approached upfront',\n                                                         'Had a Phone Conversation', 'Unsubscribed', 'Unreachable'],\n                                                        'Others')","7ab36a77":"# 'TotalVisits'\nleads['TotalVisits'].describe()","720d62b0":"# Replacing Null Values in 'TotalVisits' with median\nleads['TotalVisits'] = leads['TotalVisits'].replace(np.nan, leads['TotalVisits'].median())","4403bf2c":"# 'Page Views Per Visit'\nleads['Page Views Per Visit'].describe()","3204b0b0":"# Replacing the Null Values with 0.0, since, this is the most common entry\nleads['Page Views Per Visit'] = leads['Page Views Per Visit'].replace(np.nan, leads['Page Views Per Visit'].median())","f090933a":"leads['Do Not Call'].value_counts()","8fc1dcf8":"leads['Search'].value_counts()","e8f1beab":"leads['Magazine'].value_counts()","7f83ef24":"leads['Newspaper Article'].value_counts()","27c2f412":"leads['Receive More Updates About Our Courses'].value_counts()","45a58fd4":"leads['X Education Forums'].value_counts()","04ffff8e":"leads['Newspaper'].value_counts()","6bf50c62":"leads['Digital Advertisement'].value_counts()","42eb0581":"leads['Through Recommendations'].value_counts()","0b227906":"leads['Receive More Updates About Our Courses'].value_counts()","a244a862":"leads['Update me on Supply Chain Content'].value_counts()","c40147c6":"leads['Get updates on DM Content'].value_counts()","98e36934":"leads['I agree to pay the amount through cheque'].value_counts()","d48c7632":"# Adding the redundant columns to cols_to_drop in order to drop them\ncols_to_drop.extend(['Do Not Call', 'Search', 'Magazine', 'Newspaper Article',\n                     'Receive More Updates About Our Courses', 'X Education Forums', 'Newspaper', 'Digital Advertisement',\n                     'Through Recommendations', 'Receive More Updates About Our Courses',\n                     'Update me on Supply Chain Content', 'Get updates on DM Content',\n                     'I agree to pay the amount through cheque'])","f77559fd":"# Dropping the Variables\nleads = leads.drop(cols_to_drop, axis = 1)","7679208b":"# Checking the info again\nleads.info()","1f6bed9f":"def cat_var(cat):\n    plt.figure(figsize=(15, 5))\n    ax = sns.countplot(x = cat, hue = leads.Converted, data=leads)\n    \n    for p in ax.patches:\n        width = p.get_width()\n        height = p.get_height()\n        x, y = p.get_xy() \n        ax.annotate(f'{height}', (x + width\/2, y + height*1.02), ha='center')\n    \n    plt.xticks(rotation = 90)\n    plt.show()","fc35f7c7":"cat_var('Lead Origin')","5eafe02d":"cat_var('Lead Source')","3c68fd96":"leads['Last Activity'] = leads['Last Activity'].replace(['Unreachable', 'Unsubscribed', 'Had a Phone Conversation',\n                                                        'View in browser link Clicked', 'Approached upfront',\n                                                        'Visited Booth in Tradeshow', 'Resubscribed to emails',\n                                                        'Email Received', 'Email Marked Spam',\n                                                        'Form Submitted on Website'], 'Other Methods')\nleads['Last Activity'] = leads['Last Activity'].replace('Others', 'Other Methods')","93ca7f1a":"cat_var('Last Activity')","1bee543e":"cat_var('Specialization')","0b26bdc9":"cat_var('What is your current occupation')","28a87d46":"# Grouping values with low count in order to remove bias\nleads['Last Notable Activity'] = leads['Last Notable Activity'].replace(['Email Bounced', 'Email Link Clicked',\n                                                                         'Unreachable', 'Had a Phone Conversation',\n                                                                        'Olark Chat Conversation', 'Approached upfront',\n                                                                        'Resubscribed to emails', 'Unsubscribed',\n                                                                         'View in browser link Clicked',\n                                                                        'Form Submitted on Website', 'Email Received',\n                                                                         'Email Marked Spam'], 'Others')","1ae34994":"cat_var('Last Notable Activity')","31478590":"# Checking the Percentages of Converted Values\nsum(leads['Converted'])\/len(leads['Converted'])*100","c6db559b":"# Checking the Numberical columns for Corelation\nplt.figure(figsize = (10,10))\nsns.heatmap(leads.corr(), annot = True, cmap= 'Reds')\nplt.show()","d1fe8433":"# 'TotalVisits'\nsns.boxplot(y= leads['TotalVisits'])\nplt.show()","8d2a85e6":"leads['TotalVisits'].describe(percentiles = [0.05, 0.25, 0.5, 0.75, 0.90, 0.95, 0.99])","16454f85":"# Removing the top 1% of the value to handle outliers\nleads = leads[(leads['TotalVisits'] <= leads['TotalVisits'].quantile(0.99))]\nsns.boxplot(y = leads['TotalVisits'])\nplt.show()","617c29c3":"# 'Total Time Spent on Website'\nsns.boxplot(y = leads['Total Time Spent on Website'])\nplt.show()","e289d6e5":"# 'Page Views Per Visit'\nleads['Page Views Per Visit'].describe(percentiles = [0.05, 0.25, 0.5, 0.75, 0.90, 0.95, 0.99])","1ee91102":"# Removing the top 1% of the values to handle the outliers\nleads = leads[(leads['Page Views Per Visit'] <= leads['Page Views Per Visit'].quantile(0.99))]\nsns.boxplot(y = leads['Page Views Per Visit'])\nplt.show()","6be9096b":"# Checking 'TotalVisits' vs 'Converted'\nsns.boxplot(x = leads['Converted'], y = leads['TotalVisits'])\nplt.show()","e326bfe5":"# Checking 'Page Views Per Visit' vs 'Converted'\nsns.boxplot(x = leads['Converted'], y = leads['Page Views Per Visit'])\nplt.show()","fade184a":"# Checking 'Page Views Per Visit' vs 'Converted'\nsns.boxplot(x = leads['Converted'], y = leads['Total Time Spent on Website'])\nplt.show()","dbaf94eb":"# Converting Yes\/No into 1\/0 to fit the model\ndef binary_map(x):\n    return x.map({'Yes': 1, 'No':0})\n\nvarlist = ['Do Not Email', 'A free copy of Mastering The Interview']\n\n# Applying the above custom function\n\nleads[varlist] = leads[varlist].apply(binary_map)","7f5306f5":"leads.head()","e973f825":"cat_cols = leads.select_dtypes(include = 'object').columns\ncat_cols","34e2326b":"# Lead Origin\nLead_Origin = pd.get_dummies(leads['Lead Origin'], prefix = 'Lead_Origin')\nLead_Origin = Lead_Origin.drop('Lead_Origin_Quick Add Form', axis = 1)\nleads = pd.concat([leads, Lead_Origin], axis = 1)","a260f565":"# Lead Source\nLead_Source = pd.get_dummies(leads['Lead Source'], prefix = 'Lead_Source')\nLead_Source = Lead_Source.drop('Lead_Source_Others', axis = 1)\nleads = pd.concat([leads, Lead_Source], axis = 1)","0f239c16":"# Last Activity\nLast_Activity = pd.get_dummies(leads['Last Activity'], prefix = 'Last_Activity')\nLast_Activity = Last_Activity.drop('Last_Activity_Other Methods', axis = 1)\nleads = pd.concat([leads, Last_Activity], axis = 1)","5073189e":"# Specialization\nSpecial = pd.get_dummies(leads['Specialization'], prefix = 'Specialization')\nSpecial = Special.drop('Specialization_Not Specified', axis = 1)\nleads = pd.concat([leads, Special], axis = 1)","5fe9c4c1":"# What is your current occupation\noccup = pd.get_dummies(leads['What is your current occupation'], prefix = 'Occupation')\noccup = occup.drop('Occupation_Other', axis = 1)\nleads = pd.concat([leads, occup], axis = 1)","29441330":"# Tags\ntags = pd.get_dummies(leads['Tags'], prefix = 'Tags')\ntags = tags.drop('Tags_Not Specified', axis = 1)\nleads = pd.concat([leads, tags], axis = 1)","1506b284":"# Last Notable Activity\nact = pd.get_dummies(leads['Last Notable Activity'], prefix = 'Last_Notable_Activity')\nact = act.drop('Last_Notable_Activity_Others', axis = 1)\nleads = pd.concat([leads, act], axis = 1)","084b1d37":"leads.head()","04475101":"leads = leads.drop(cat_cols, axis = 1)","2cbc7635":"leads.head()","0c044ef0":"y = leads['Converted']\n\nX = leads.drop('Converted', axis = 1)","a4aaa26c":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size= 0.70, random_state= 100)","df32e1e7":"X_train.head()","545869e6":"# Scaling the Numerical variables\nscaler = StandardScaler()\n\nnum_cols = ['TotalVisits', 'Total Time Spent on Website', 'Page Views Per Visit']\n\nX_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n\nX_train.head()","b72454b2":"# Model 1\n# Model building using statsmodels\n\n# Logistic regression model\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","b92344c5":"# Logistic Regression\nlogreg = LogisticRegression()\n\nrfe = RFE(logreg, 20)             \nrfe = rfe.fit(X_train, y_train)","b97f3eaa":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","8eb04de1":"# Selected Variables\ncol = X_train.columns[rfe.support_]\ncol","19de14fb":"X_train.columns[~rfe.support_]","55de884d":"# Model 2\nX_train_sm = sm.add_constant(X_train[col])\n\nlogm2 = sm.GLM(y_train, X_train_sm, family= sm.families.Binomial())\nres = logm2.fit()\nres.summary()","8d44e489":"# Dropping the variables with p-values greater than 0.05\ncol = col.drop(['Lead_Source_Reference', 'Lead_Source_Welingak Website'])\ncol","b53305bc":"# Model 3\nX_train_sm = sm.add_constant(X_train[col])\n\nlogm3 = sm.GLM(y_train, X_train_sm, family= sm.families.Binomial())\nres = logm3.fit()\nres.summary()","cd60ac96":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","38605aa0":"# Predictions on the train set\ny_train_pred = res.predict(X_train_sm).values.reshape(-1)\ny_train_pred","3d158b0e":"y_train_pred_final = pd.DataFrame({'Converted': y_train.values, 'Converted_Prob': y_train_pred})\ny_train_pred_final['Prospect ID'] = y_train.index\ny_train_pred_final.head()","36dcd69b":"# Making a new column Predicted with values which have Probability greater than 0.5\ny_train_pred_final['Predicted'] = y_train_pred_final.Converted_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","76df1d28":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_true= y_train_pred_final.Converted, y_pred= y_train_pred_final.Predicted))","2b175bdd":"# Let's take a look at the confusion matrix again \nconfusion = metrics.confusion_matrix(y_true= y_train_pred_final.Converted, y_pred= y_train_pred_final.Predicted)\nconfusion","54dde176":"# Actual\/Predicted     Not_Converted    Converted\n    # Not_Converted        3757            178\n    # Converted            380             2048  ","f04fb3f2":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","ca2ba158":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","ded6d667":"# Let us calculate specificity\nTN \/ float(TN+FP)","c221083b":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","c9ecf01c":"# positive predictive value \nprint (TP \/ float(TP+FP))","a46ffa17":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","9a13e2c9":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic Curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","a5a5c01d":"fpr, tpr, thresholds = metrics.roc_curve(y_train_pred_final.Converted, y_train_pred_final.Converted_Prob,\n                                         drop_intermediate = False)","67e01c36":"draw_roc(y_train_pred_final.Converted, y_train_pred_final.Converted_Prob)","02b127a1":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Converted_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","f0a1d7c3":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensitivity','specificity'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","c87e359d":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensitivity','specificity'])\nplt.show()","25c4f73a":"y_train_pred_final['Final_Predicted'] = y_train_pred_final.Converted_Prob.map( lambda x: 1 if x > 0.3 else 0)\n\ny_train_pred_final.head()","38c4c492":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_true= y_train_pred_final.Converted, y_pred= y_train_pred_final.Final_Predicted))","42beaaad":"# Let's take a look at the confusion matrix again \nconfusion = metrics.confusion_matrix(y_true= y_train_pred_final.Converted, y_pred= y_train_pred_final.Final_Predicted)\nconfusion","5362dbde":"# Actual\/Predicted     Not_Converted    Converted\n    # Not_Converted        3523            412\n    # Converted            213             2215  ","c5ec74ea":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","52d4e895":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","6e1795a9":"# Let us calculate specificity\nTN \/ float(TN+FP)","35f1b64e":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","75395f65":"# Positive predictive value \nprint (TP \/ float(TP+FP))","35cb884b":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","afb4c67f":"# Precision Score\nprecision_score(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted)","fe365534":"# Recall Score\nrecall_score(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted)","42de4071":"p, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Converted_Prob)","82ee6914":"plt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","4b53534a":"# Scaling the Numerical Variables\nX_test[num_cols] = scaler.transform(X_test[num_cols])","2d85ff64":"X_test = X_test[col]\nX_test.head()","2c504957":"X_test_sm = sm.add_constant(X_test)","ecaf7da5":"# Predictions on test set\ny_test_pred = res.predict(X_test_sm)\ny_test_pred.head()","942d8136":"# Converting y_pred to a DataFrame\ny_pred_1 = pd.DataFrame(y_test_pred)\ny_pred_1.head()","88a24663":"# Converting y_test to DataFrame\ny_test_df = pd.DataFrame(y_test)","1198c6f7":"# Putting 'Prospect ID' to index\ny_test_df['Prospect ID'] = y_test_df.index","07d3b179":"# Removing index for both DataFrames to concatenate them side by side\ny_pred_1.reset_index(drop= True, inplace= True)\ny_test_df.reset_index(drop= True, inplace= True)","33373a6f":"# Concatenating y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1], axis = 1)\ny_pred_final.head()","cc11523e":"# Renaming the column\ny_pred_final = y_pred_final.rename(columns = {0: 'Converted_Prob'})","86fcf292":"# Rearranging the columns\ny_pred_final = y_pred_final[['Prospect ID', 'Converted', 'Converted_Prob']]\ny_pred_final.head()","5ae42b88":"y_pred_final['Final_Predicted'] = y_pred_final.Converted_Prob.map(lambda x: 1 if x > 0.3 else 0)\ny_pred_final.head()","3bba1cc3":"# Overall Accuracy\nmetrics.accuracy_score(y_true= y_pred_final.Converted, y_pred= y_pred_final.Final_Predicted)","c90d74a2":"# Confusion Matrix\nconfusion2 = metrics.confusion_matrix(y_true= y_pred_final.Converted, y_pred= y_pred_final.Final_Predicted)\nconfusion2","8666834a":"TP = confusion2[1,1]\nTN = confusion2[0,0]\nFP = confusion2[0,1]\nFN = confusion2[1,0]","4b41d124":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","11ded512":"# Let us calculate specificity\nTN \/ float(TN+FP)","37944f24":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","e17db5e1":"# Positive predictive value \nprint (TP \/ float(TP+FP))","426eb2b6":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","84e82414":"leads_score = leads\nleads_score.head()","5fc68584":"# Scaling the numerical variables\nleads_score[num_cols] = scaler.transform(leads_score[num_cols])","d37f4697":"# Removing all the variables which were not in our final model\nleads_score = leads_score[col]\nleads_score = sm.add_constant(leads_score)\nleads_score.head()","c1c984b2":"# Making predictions\nleads_score_pred = res.predict(leads_score)\nleads_score_pred.head()","ff5d1ae0":"leads_score['Lead Score'] = round(leads_score_pred*100)\nleads_score.head()","21e64e34":"# Adding Lead Number to see which Lead has scored how much\nleads_score['Lead Number'] = Lead_Number\nleads_score.head()","c4b44e7c":"# Plotting Boxplot to check the distribution for the Lead Scores calculated\nsns.boxplot(y = leads_score['Lead Score'])\nplt.show()","089803c3":"`Inference:`\n1. Customers identified via Lead Add Form have a high conversion of 664.","79b8dcd1":"# Step 14: Precision and Recall","fb25ffc6":"## Conclusions:\n1. Variables found which contribute the most to the model are as follows:\n    - Tags_Already a student\n    - Tags_Ringing\n    - Tags_Closed by Horizzon\n    - Tags_Interested in other courses\n    - Lead_Origin_Lead Add Form\n    - Tags_Will revert after reading the email","03ded65e":"#### Precision and Recall Tradeoff","02743f13":"##### Checking VIFs","5dae5d0d":"XEducation, an education company selling online courses, needs assistance in recognising and selecting promising leads that will most likely convert to paying customers. Our aim is to analyse and identify factors that help result in high customer conversion rate. This is done by assigning lead scores to each of the leads such that customers with high lead score indicate high chance of conversion ","cddc96fd":"`Inference:` There are 38.5% Converted Leads","dba545ca":"`Inference:` Median of both Converted and Not Converted for 'Page Views Per Visit' is almost same.","50dec378":"`Inference:`\n1. Source of Lead via Google and Direct Traffic have high negative conversion as compared to others.\n2. Leads coming via Reference have the highest conversion rate.","46fc4ca0":"#### Metrics beyond Simple Accuracy with 0.3 as Optimal Cutoff point","3a914ff7":"# Step 6: Outlier Treatment","f5e83d0e":"`Inference:`\n1. Customers who worked in Management Specialization have conversion of 1922.\n2. Customers who worked in Service have the least conversion.","849e6a0f":"#### Categorical Variables","7e9fbb15":"`Inference:`\n1. It is interesting to know that out of 5600, only 2441 converted.\n2. From 706 Working Professionals, 647 converted.","803bedeb":"The following are the steps performed for the analysis:\n1. Importing the Dataset\n2. Inspecting the DataFrame\n3. Missing Value Treatment\n4. Data Preparation\n5. Univariate Analysis\n6. Outlier Treatment\n7. Creating Dummy Variables\n8. Train - Test Split\n9. Feature Scaling\n10. Model Building\n11. Feature Selection Using RFE\n12. Plotting the ROC Curve\n13. Finding Optimal Cut-Off Points\n14. Precision and Recall\n15. Making Predictions on the Test Set\n16. Assigning the Lead Score to the Data","9fdc5ef8":"##### Creating a new column Predicted with 1 if Probability > 0.5","455f606f":"`Inference:`\n1. Last Activity performed by Customers is SMS sent, then it has conversion of 1727.","a36417b7":"# Step 5: Univariate Analysis","897f0ec1":"#### Analyzing the Variables one-by-one","85772d6b":"#### Second Model\n##### Accessing the model with statsmodels","be7377b7":"##### First Model","290b5174":"# Step 10: Model Building","212d0872":"`Inference:`\n1. For individuals with Last Notable activity as SMS sent, 1508 people converted.","a1b4baa0":"##### Creating the DataFrame with Actual Converted and Predicted Probabilities","425b3d47":"# Step 2: Inspecting the DataFrame","94045d55":"Optimal cutoff probability is that probability where we get balanced sensitivity and specificity.","e8f4a445":"\n# Step 4: Data Preparation","04db0f36":"# Step 16: Assigning the Lead Score to the data","81864b26":"`Inference:` There are no duplicates in both 'Prospect ID' and 'Lead Number'","009b67b8":"`Inference:` leads_score['Lead Score'] shows the Lead Score required","13650919":"`Inference:` Leads spending more time on the website tend to be converted.","46742258":"# Step 7: Creating Dummy Variables","36deb5b2":"### Step 8: Train - Test Split","ab72cf51":"# Step 15: Making Predictions on the Test Set","cc203883":"##### Metrics beyond Simple Accuracy","4a6b7a35":"# Step 3: Missing Value Treatment","9c6f614c":"##### From the curve above, we can see that 0.3 is our Optimal Point","00acdedd":"# Step 12: Plotting the ROC Curve","cab6a02b":"# Step 1: Importing the Dataset","1a35b75a":"#### Numerical Variables","9cb70e60":"`Inference:` Median of both Converted and Not Converted for 'TotalVisits' is almost same.","d221bb5c":"#### Dropping the Repeated Variables","11b4a3b8":"# Step 13: Finding Optimal Cut-off Points","89db3d31":"# Lead Scoring Case Study","2ba40c80":"### Step 9: Feature Scaling","4324fba4":"# Step 11: Feature Selection Using RFE","65346da1":"#### Third Model"}}