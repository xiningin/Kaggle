{"cell_type":{"66a9d2ed":"code","de7d9c3e":"code","edc206f4":"code","aa3276d2":"code","57709f1c":"code","09da4e25":"code","e00b3168":"code","3bf3b2be":"code","c8bf0643":"code","786ffad7":"code","bf6f8ac8":"code","020b5284":"code","0077c1c9":"code","fa8a50b0":"code","f67307c7":"code","dff39373":"code","66fc355e":"code","32e797d0":"code","3ff4720a":"code","b1fcb6ba":"code","a02d5272":"code","357ea46c":"code","6c69efdb":"code","79a3686b":"code","219b9e55":"code","7dfb4407":"code","a8c7d9e1":"code","4370cc2b":"code","e3f994f3":"code","e555dae4":"code","63db2a30":"code","85dabe8e":"code","e4683ee1":"code","11273348":"code","beaaf0d3":"code","a47b63c8":"code","b3f104e4":"code","98dde9df":"code","ac29ed89":"code","faba43ab":"code","9749c880":"code","9cd82dc5":"code","3e1c75a5":"code","7258eede":"code","d9b6e3b9":"code","fa993d04":"markdown","39d023a1":"markdown","b785cf04":"markdown","791e4a13":"markdown","920840e2":"markdown","ea607164":"markdown","de0ab761":"markdown","916a6be9":"markdown"},"source":{"66a9d2ed":"# -*- coding: utf-8 -*-\n\n# I don't like warnings, especially user warnings at all!\nimport warnings\nwarnings.filterwarnings('ignore')","de7d9c3e":"#Resources: https:\/\/www.kaggle.com\/yassineghouzam\/introduction-to-cnn-keras-0-997-top-6\n#Dataset:https:\/\/bengali.ai\/","edc206f4":"# Import some packages that we require\nimport pandas as pd\nimport os\nimport glob\nimport umap\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport itertools\nfrom os import listdir, makedirs, getcwd, remove\nfrom os.path import isfile, join, abspath, exists, isdir, expanduser\nfrom PIL import Image\nfrom pathlib import Path\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom keras.models import Sequential, Model\nfrom keras.applications import vgg16\nfrom keras.applications import resnet50\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten\nfrom keras.layers import GlobalMaxPooling2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.merge import Concatenate\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import TSNE\nfrom skimage.io import imread, imshow\nfrom skimage.transform import resize\nimport imgaug as ia\nfrom imgaug import augmenters as iaa\nfrom keras import backend as K\nimport tensorflow as tf\nfrom collections import defaultdict, Counter\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom sklearn.metrics import confusion_matrix\nfrom keras.backend import clear_session\nprint(os.listdir(\"..\/input\"))","aa3276d2":"# For plotting within the notebook\n%matplotlib inline\n\n# Graphics in SVG format are more sharp and legible\n%config InlineBackend.figure_format = 'svg'\n\n# seaborn color palette \ncolor = sns.color_palette()\n\n# For REPRODUCIBILITY\nseed = 42\nnp.random.seed(seed)\ntf.set_random_seed(seed)","57709f1c":"input_path = Path(\"..\/input\")\n\n# Path to training images and corresponding labels provided as numpy arrays\nnumtadb_train_path = input_path\/\"NumthDB_training.npz\"\n\n# Path to the test images and corresponding labels\nnumtadb_test_path = input_path\/\"NumthDB_test.npz\"","09da4e25":"train_images = np.load(numtadb_train_path)['data']\ntrain_labels = np.load(numtadb_train_path)['label']\n\n# Load the test data from the corresponding npz files\ntest_images = np.load(numtadb_test_path)['data']","e00b3168":"print(f\"Number of training samples: {len(train_images)} where each sample is of size: {train_images.shape[1:]}\")\nprint(f\"Number of test samples: {len(test_images)} where each sample is of size: {test_images.shape[1:]}\")","3bf3b2be":"# Get the unique labelsa\nlabels = np.unique(train_labels)\nprint(\"No of uniqe lables\",labels.shape)\n# Get the frequency count for each label\nfrequency_count = np.bincount(train_labels)\n\n# Visualize \nplt.figure(figsize=(10,5))\nsns.barplot(x=labels, y=frequency_count);\nplt.title(\"Distribution of labels in KMNIST training data\", fontsize=16)\nplt.xlabel(\"Labels\", fontsize=14)\nplt.ylabel(\"Count\", fontsize=14)\nplt.show()","c8bf0643":"random_samples = []\nfor i in range(10):\n    samples = train_images[np.where(train_labels==i)][:3]\n    random_samples.append(samples)\n\n# Converting list into a numpy array\nrandom_samples = np.array(random_samples)\n\n# Visualize the samples\nf, ax = plt.subplots(10,3, figsize=(10,20))\nfor i, j in enumerate(random_samples):\n    ax[i, 0].imshow(random_samples[i][0,:,:], cmap='gray')\n    ax[i, 1].imshow(random_samples[i][1,:,:], cmap='gray')\n    ax[i, 2].imshow(random_samples[i][2,:,:], cmap='gray')\n    \n    ax[i,0].set_title(str(i))\n    ax[i,0].axis('off')\n    ax[i,0].set_aspect('equal')\n    \n    ax[i,1].set_title(str(i))\n    ax[i,1].axis('off')\n    ax[i,1].set_aspect('equal')\n    \n    ax[i,2].set_title(str(i))\n    ax[i,2].axis('off')\n    ax[i,2].set_aspect('equal')\nplt.show()","786ffad7":"def get_random_samples(nb_indices):\n    # Choose indices randomly \n    random_indices = np.random.choice(nb_indices, size=nb_indices, replace=False)\n\n    # Get the data corresponding to these indices\n    random_train_images = train_images[random_indices].astype(np.float32)\n    random_train_images \/=255.\n    random_train_images = random_train_images.reshape(nb_indices, 28*28)\n    random_train_labels = train_labels[random_indices]\n    labels = np.unique(random_train_labels)\n    return random_indices, random_train_images, random_train_labels, labels","bf6f8ac8":"#Get randomly sampled data\nnb_indices = 5000\nrandom_indices, random_train_images, random_train_labels, labels = get_random_samples(nb_indices)\n\n# Get the actual labels from the labels dictionary\nlabels_name = [x for x in labels]\n\n# Get a t-SNE instance\n#Reduced n_components=2\ntsne = TSNE(n_components=2, random_state=seed, perplexity=30,n_iter=2000)\n\n# Do the scaling\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(random_train_images)\n\n# Fit tsne to the data\nrandom_train_2D = tsne.fit_transform(random_train_images)\nprint(\"Shape of the new manifold\",random_train_2D.shape)","020b5284":"plot_num = 500\nfig = plt.figure(figsize=(10, 8))\nfor i, label in zip(labels, labels_name):\n    sns.scatterplot(random_train_2D[random_train_labels == i, 0][:plot_num], \n                random_train_2D[random_train_labels == i, 1][:plot_num], \n                label=i, s=18)\n\nplt.title(\"Visualizating NumtaDb embeddings using tSNE\", fontsize=16)\nplt.legend()\nplt.show()","0077c1c9":"# Let's try UMAP now.\nnb_indices = 50000\nrandom_indices, random_train_images, random_train_labels, labels = get_random_samples(nb_indices)\n#Reduced n_components=2\nembedding = umap.UMAP(n_components=2, metric='correlation', min_dist=0.8)\nrandom_train_2D = embedding.fit_transform(random_train_images)\n\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111) #projection='3d')\n\nfor i, label in zip(labels, labels):\n    sns.scatterplot(random_train_2D[random_train_labels == i, 0], \n                random_train_2D[random_train_labels == i, 1], \n                label=label, s=15)\nplt.title(\"Visualiza NumtaDb embeddings using UMAP \", fontsize=16)\nplt.legend()\nplt.show()","fa8a50b0":"print(train_images.shape)\ntrain_images_1d = train_images.reshape((-1,784))\nprint(train_images_1d.shape)","f67307c7":"print(test_images.shape)\ntest_images_1d = test_images.reshape((-1,28,28,1))\nprint(test_images_1d.shape)","dff39373":"train_labels_encoded = to_categorical(train_labels,num_classes=10)\nprint(train_labels_encoded.shape)","66fc355e":"X_train, X_val, Y_train, Y_val = train_test_split(train_images_1d, train_labels_encoded, test_size = 0.2, random_state=seed)","32e797d0":"#Clear last session and start a fresh session\nfrom keras import backend as K\nK.clear_session()\nsess = tf.Session()\nK.set_session(sess)","3ff4720a":"model = Sequential()\nmodel.add(Dropout(0.2, input_shape=(784,)))\nmodel.add(Dense(units=512,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(units=10,activation='softmax'))\n# Define the optimizer\noptimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)","b1fcb6ba":"# Set a learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","a02d5272":"# Compile the model\nmodel.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","357ea46c":"%%time\nepochs = 30 # Turn epochs to 30 to get 0.9967 accuracy\nbatch_size = 64\n# Without data augmentation i obtained an accuracy of 0.98114\nhistory = model.fit(\n    X_train, Y_train,\n    batch_size = batch_size, \n    epochs = epochs, \n    validation_data = (X_val, Y_val), \n    verbose = 2,\n    callbacks=[learning_rate_reduction])","6c69efdb":"print(train_images.shape)\ntrain_images_3d = train_images.reshape((-1,28,28,1))\nprint(train_images_3d.shape)","79a3686b":"print(test_images.shape)\ntest_images_3d = test_images.reshape((-1,28,28,1))\nprint(test_images_3d.shape)","219b9e55":"train_labels_encoded = to_categorical(train_labels,num_classes=10)\nprint(train_labels_encoded.shape)","7dfb4407":"X_train, X_val, Y_train, Y_val = train_test_split(train_images_3d, train_labels_encoded, test_size = 0.2, random_state=seed)","a8c7d9e1":"#Clear last session and start a fresh session\nfrom keras import backend as K\nK.clear_session()\nsess = tf.Session()\nK.set_session(sess)","4370cc2b":"model = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))","e3f994f3":"# Define the optimizer\noptimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n# Compile the model\nmodel.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","e555dae4":"# Set a learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","63db2a30":"%%time\nepochs = 30 # Turn epochs to 30 to get 0.9967 accuracy\nbatch_size = 64\n# Without data augmentation i obtained an accuracy of 0.98114\nhistory = model.fit(\n    X_train, Y_train,\n    batch_size = batch_size, \n    epochs = epochs, \n    validation_data = (X_val, Y_val), \n    verbose = 2,\n    callbacks=[learning_rate_reduction])","85dabe8e":"# Plot the loss and accuracy curves for training and validation \nfig, ax = plt.subplots(2,1)\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['acc'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_acc'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","e4683ee1":"# Look at confusion matrix \n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Predict the values from the validation dataset\nY_pred = model.predict(X_val)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(Y_val,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(10))","11273348":"# Display some error results \n# Errors are difference between predicted labels and true labels\n\nerrors = (Y_pred_classes - Y_true != 0)\n\nY_pred_classes_errors = Y_pred_classes[errors]\nY_pred_errors = Y_pred[errors]\nY_true_errors = Y_true[errors]\nX_val_errors = X_val[errors]\n\ndef display_errors(errors_index,img_errors,pred_errors, obs_errors):\n    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n    n = 0\n    nrows = 3\n    ncols = 3\n    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n    for row in range(nrows):\n        for col in range(ncols):\n            error = errors_index[n]\n            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n            n += 1\n\n# Probabilities of the wrong predicted numbers\nY_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n\n# Predicted probabilities of the true values in the error set\ntrue_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n\n# Difference between the probability of the predicted label and the true label\ndelta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n\n# Sorted list of the delta prob errors\nsorted_dela_errors = np.argsort(delta_pred_true_errors)\n\n# Top 6 errors \nmost_important_errors = sorted_dela_errors[-9:]\n\n# Show the top 6 errors\ndisplay_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)\n","beaaf0d3":"## Model is getting confused betwwen 1 and 9,which is decent","a47b63c8":"# predict results\ntest_images_3d = test_images.reshape((-1,28,28,1))\nresults = model.predict(test_images_3d)\n\n# select the indix with the maximum probability\nresults = np.argmax(results,axis = 1)\n\nresults = pd.Series(results,name=\"Label\")","b3f104e4":"#Clear and start a fresh session\nfrom keras import backend as K\nK.clear_session()\nsess = tf.Session()\nK.set_session(sess)","98dde9df":"model = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))","ac29ed89":"# Define the optimizer\noptimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n# Compile the model\nmodel.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","faba43ab":"# Set a learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","9749c880":"# With data augmentation to prevent overfitting (accuracy 0.99286)\n\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(X_train)","9cd82dc5":"epochs = 30\nbatch_size = 64\n# Fit the model\nhistory = model.fit_generator(\n    datagen.flow(X_train,Y_train, batch_size=batch_size),\n    epochs = epochs, \n    validation_data = (X_val,Y_val),\n    verbose = 2,\n    steps_per_epoch=X_train.shape[0] \/\/ batch_size,\n    callbacks=[learning_rate_reduction]\n)","3e1c75a5":"# Plot the loss and accuracy curves for training and validation \nfig, ax = plt.subplots(2,1)\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['acc'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_acc'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","7258eede":"## Save the model\nmodel.save_weights('model_weights.h5')\n\n# Save the model architecture\nwith open('model_architecture.json', 'w') as f:\n    f.write(model.to_json())","d9b6e3b9":"#Prepare test data\ntest_images_3d = test_images.reshape((-1,28,28,1))\n# predict results\nresults = model.predict(test_images_3d)\n# select the indix with the maximum probability\nresults = np.argmax(results,axis = 1)\n\nresults = pd.Series(results,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,test_images_3d.shape[0]+1),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"cnn_numta_datagen.csv\",index=False)","fa993d04":"## Label distribution","39d023a1":"### **CNN based model **\n####  CNN architechture is In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Out\n#### architecture copied  from https:\/\/www.kaggle.com\/yassineghouzam\/introduction-to-cnn-keras-0-997-top-6","b785cf04":"## **Adding data augmentations**","791e4a13":"## **Build  Models(hypotheis)**","920840e2":" ### **Single layer  neural network(Baseline) **","ea607164":"Prepare data","de0ab761":"Load data","916a6be9":"## Data Analysis"}}