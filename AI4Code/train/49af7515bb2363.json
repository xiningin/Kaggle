{"cell_type":{"f405c6bf":"code","cafee1f6":"code","1a0c2905":"code","85b32c9f":"code","2404bb20":"code","98df486c":"code","b0a6766e":"code","0df82561":"code","b4e6a42b":"code","6fbb4e65":"code","8f8ab033":"code","80a94f3d":"code","2b4d016e":"code","0c2f9d27":"code","79d1d269":"code","fd56ca67":"code","3ab00282":"code","fd5c221a":"code","42c078ef":"code","875167cb":"code","209b0df9":"code","2775043c":"code","6d1e677b":"code","4d1ca05f":"code","d6980409":"code","94583f60":"code","ddd20e78":"code","58ab7106":"code","edbcf2a4":"code","41d5b142":"code","07c5cf82":"code","252f6a82":"code","50e53333":"code","e3737c90":"code","0732168b":"code","d7541bd3":"code","386343f6":"code","65d97e4f":"markdown","d04edd98":"markdown","1f2f0fbd":"markdown","f6ddea4b":"markdown","31ee2d38":"markdown","f7aec700":"markdown","23b7ae01":"markdown","d2c05dce":"markdown","442b9af0":"markdown","adc2d228":"markdown","53ec6f85":"markdown","9207c3cb":"markdown","dc1a143d":"markdown","d53f13aa":"markdown","732181d0":"markdown","386b3539":"markdown","e4fef8f2":"markdown","04dc27cc":"markdown","86d64f56":"markdown","26cf64f4":"markdown","3b063981":"markdown","b175e581":"markdown"},"source":{"f405c6bf":"#data analysis libraries\n\nimport numpy as np\nimport pandas as pd\n\n#visualization libraries\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","cafee1f6":"#train data\nurl = '..\/input\/titanic\/train.csv'\ntrain = pd.read_csv(url)\n\nurl = '..\/input\/titanic\/test.csv'\ntest = pd.read_csv(url)","1a0c2905":"train.describe(include=\"all\")","85b32c9f":"#show the first ten rows of the dataset\ntrain.sample(10)","2404bb20":"print(pd.isnull(train).sum())","98df486c":"#barplot of survivals by sex\nsns.barplot(x=\"Sex\", y=\"Survived\", data=train)","b0a6766e":"#Barplot of Pclass.\nsns.barplot(x=\"Pclass\", y=\"Survived\", data=train)","0df82561":"train.drop(['PassengerId'], 1).hist(figsize=(25,18))\nplt.show()","b4e6a42b":"print(\"Number of people embarking in Southampton (S):\")\nsouthampton = train[train[\"Embarked\"] == \"S\"].shape[0]\nprint(southampton)\n\nprint(\"Number of people embarking in Cherbourg (C):\")\ncherbourg = train[train[\"Embarked\"] == \"C\"].shape[0]\nprint(cherbourg)\n\nprint(\"Number of people embarking in Queenstown (Q):\")\nqueenstown = train[train[\"Embarked\"] == \"Q\"].shape[0]\nprint(queenstown)","6fbb4e65":"#Filling the missing values with S (where the majority embarked).\ntrain = train.fillna({\"Embarked\": \"S\"})","8f8ab033":"test = test.fillna({\"Embarked\": \"S\"})","80a94f3d":"#Filling the missing values based on mean fare for that Pclass\n#PS. Only the test database has null values\nfor x in range(len(test[\"Fare\"])):\n    if pd.isnull(test[\"Fare\"][x]):\n        pclass = test[\"Pclass\"][x] #Pclass = 3\n        test[\"Fare\"][x] = round(train[train[\"Pclass\"] == pclass][\"Fare\"].mean(), 4)\n    ","2b4d016e":"#converting to numerical values\ntrain['FareBand'] = pd.qcut(train['Fare'], 4, labels = [1, 2, 3, 4])\ntest['FareBand'] = pd.qcut(test['Fare'], 4, labels = [1, 2, 3, 4])\n\n#drop Fare values\ntrain = train.drop(['Fare'], axis = 1)\ntest = test.drop(['Fare'], axis = 1)","0c2f9d27":"#Drop Ticket and Cabin\ntrain = train.drop(['Ticket', 'Cabin'], axis=1)\ntest = test.drop(['Ticket', 'Cabin'], axis=1)","79d1d269":"#And fill the missing Age values\ntrain['Age'] = train.groupby(['Pclass','Sex','Parch','SibSp'])['Age'].transform(lambda x: x.fillna(x.mean()))\ntrain['Age'] = train.groupby(['Pclass','Sex','Parch'])['Age'].transform(lambda x: x.fillna(x.mean()))\ntrain['Age'] = train.groupby(['Pclass','Sex'])['Age'].transform(lambda x: x.fillna(x.mean()))","fd56ca67":"test['Age'] = test.groupby(['Pclass','Sex','Parch','SibSp'])['Age'].transform(lambda x: x.fillna(x.mean()))\ntest['Age'] = test.groupby(['Pclass','Sex','Parch'])['Age'].transform(lambda x: x.fillna(x.mean()))\ntest['Age'] = test.groupby(['Pclass','Sex'])['Age'].transform(lambda x: x.fillna(x.mean()))","3ab00282":"#count the missing values of the Age feature\nprint(pd.isnull(train['Age']).sum())","fd5c221a":"print(pd.isnull(test['Age']).sum())","42c078ef":"#Here we will create a Title column\ntrain['Title'] = pd.Series((name.split('.')[0].split(',')[1].strip() for name in train['Name']), index=train.index)\ntrain['Title'] = train['Title'].replace(['Lady','the Countess','Countess','Capt','Col','Don','Dr','Major','Rev','Sir','Jonkheer','Dona'], 'Rare')\ntrain['Title'] = train['Title'].replace(['Mlle','Ms'], 'Miss')\ntrain['Title'] = train['Title'].replace('Mme','Mrs')\ntrain['Title'] = train['Title'].map({\"Mr\":1, \"Miss\":2, \"Mrs\":3, \"Master\":4, \"Rare\":5})\n\ntest['Title'] = pd.Series((name.split('.')[0].split(',')[1].strip() for name in test['Name']), index=test.index)\ntest['Title'] = test['Title'].replace(['Lady','the Countess','Countess','Capt','Col','Don','Dr','Major','Rev','Sir','Jonkheer','Dona'], 'Rare')\ntest['Title'] = test['Title'].replace(['Mlle','Ms'], 'Miss')\ntest['Title'] = test['Title'].replace('Mme','Mrs')\ntest['Title'] = test['Title'].map({\"Mr\":1, \"Miss\":2, \"Mrs\":3, \"Master\":4, \"Rare\":5})","875167cb":"#Drop name and PassengerId\ntrain = train.drop(['Name', 'PassengerId'], axis=1)\ntest = test.drop(['Name'], axis=1)","209b0df9":"#converting sex feature to a numerical value\nsex_mapping = {\"male\": 0, \"female\": 1}\ntrain['Sex'] = train['Sex'].map(sex_mapping)\n\ntest['Sex'] = test['Sex'].map(sex_mapping)\n\ntrain.head()","2775043c":"#same to Embarked feature\nembarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\ntrain['Embarked'] = train['Embarked'].map(embarked_mapping)\n\ntest['Embarked'] = test['Embarked'].map(embarked_mapping)","6d1e677b":"plt.subplots(figsize = (12, 12))\nsns.heatmap(train.corr(), annot = True, linewidths = .5)","4d1ca05f":"train.head()","d6980409":"#splitting training data to test accuracy.\nfrom sklearn.model_selection import train_test_split\n\npredictors = train.drop(['Survived'],axis = 1)\ntarget = train[\"Survived\"]\nx_train, x_val, y_train, y_val = train_test_split(predictors, target, test_size = 0.25, random_state = 0)","94583f60":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(x_train, y_train)\ny_pred = decisiontree.predict(x_val)\nacc_decisiontree = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_decisiontree)","ddd20e78":"#Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\n\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\ny_pred = gaussian.predict(x_val)\nacc_gaussian = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gaussian)","58ab7106":"#Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\ngbk.fit(x_train, y_train)\ny_pred = gbk.predict(x_val)\nacc_gbk = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gbk)","edbcf2a4":"#KNN\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_val)\nacc_knn = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_knn)","41d5b142":"#Logistic Reggression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_val)\nacc_logreg = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_logreg)","07c5cf82":"#Perceptron\nfrom sklearn.linear_model import Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(x_train, y_train)\ny_pred = perceptron.predict(x_val)\nacc_perceptron = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_perceptron)","252f6a82":"#Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(x_train, y_train)\ny_pred = randomforest.predict(x_val)\nacc_randomforest = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_randomforest)","50e53333":"#Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nsgd.fit(x_train, y_train)\ny_pred = sgd.predict(x_val)\nacc_sgd = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_sgd)","e3737c90":"#Support Vector Machines\nfrom sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_val)\nacc_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_svc)","0732168b":"#Linear SVC\nfrom sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_train, y_train)\ny_pred = linear_svc.predict(x_val)\nacc_linear_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_linear_svc)","d7541bd3":"\n\nmodels = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', 'Linear SVC', 'SVC',  \n              'Decision Tree', 'Stochastic Gradient Descent', 'Gradient Boosting Classifier'],\n    'Score': [acc_svc, acc_knn, acc_logreg, \n              acc_randomforest, acc_gaussian, acc_perceptron, acc_linear_svc, acc_svc, acc_decisiontree,\n              acc_sgd, acc_gbk]})\nmodels.sort_values(by='Score', ascending=False)\n\n","386343f6":"ids = test['PassengerId']\npredictions = gbk.predict(test.drop('PassengerId', axis = 1))\nsubmission = pd.DataFrame({'PassengerId' : ids, 'Survived' : predictions})\nsubmission.to_csv('submission.csv', index = False)","65d97e4f":"Barplot show us that females have more chance to survive.","d04edd98":"**Cleaning data**","1f2f0fbd":"Decision Tree has the higher score.","f6ddea4b":"Converting the embarked feature:","31ee2d38":"**Showing the training data.**","f7aec700":"Only about 37% of passengers survived.\n\nMajority of fare tickets are below 50.\n\nThe people have better chance to survive if they are alone (sibsp and parch).","23b7ae01":"Filling the Embarked feature missing values.","d2c05dce":"There is high percentage of values missing in the age feature, so it makes no sense to fill these gaps with the same value as we did before.\n\nWe will replace missing values by the mean age of passengers who belong to the same group of class, sex and family.","442b9af0":"**Data visualization\n**","adc2d228":"**Reading the data.**","53ec6f85":"Passengers of higher classes have better chance of survival.","9207c3cb":"**Importing libraries.\n**\n\nFirst, we will use the block below to load some Python libraries who will be used at this kernel.\n","dc1a143d":"Now that our data is numeric, We can check the correlation between the features.","d53f13aa":"We will drop Ticket feature, It may not have correlation with Survival.\n\nCabin feature contains many null values and we will drop it.\n\nName and Passengerid may not contribute to survival and will be dropped.","732181d0":"**Training models**","386b3539":"**Data analysis.**","e4fef8f2":"Now We will convert sex feature to numerical.","04dc27cc":"analysing the dataset, we find four numerical features(Age(continuous), fare(continous), SibSp(discrete), Parch(discrete)), four categorical features(Survived, Sex, Embarked, Pclass) and two alphanumeric features(Ticket and Cabin).\n\nThere are a total of 891 passengers on training data; The column 'Age' has 714 values, and consequently 177 NaN values. I think these values is very important for our model and we should try to fullfill it; Only 204 rows has data on 'Cabin' column, since more than 70% of the column doesn't have data, we should ignore it; Only 2 values is missing on Embarked feature, probably won't be a problem.\n\n","86d64f56":"**Submission file**","26cf64f4":"Verifying the data before models creation.","3b063981":"Titanic Survival prediction\n\nHello, this is my first kernel here, I am beginning my studies of data science, more focused on machine learning.\n        \nPlease feel free to comment.","b175e581":"We will test these models:\n\n    Decision Tree Classifier\n    Gaussian Naive Bayes\n    Gradient Boosting Classifier\n    KNN\n    Logistic Regression\n    Perceptron\n    Random Forest Classifier\n    Stochastic Gradient Descent\n    Support Vector Machines\n"}}