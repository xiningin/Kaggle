{"cell_type":{"fbb99301":"code","57963010":"code","efbe5d5e":"code","f597e38c":"code","68213b7f":"code","915e1f6a":"code","c85bb9fa":"code","eea76829":"code","f651043a":"code","184951cc":"code","a5dbda8d":"code","484f6713":"code","54065360":"code","2ae6269c":"code","696a7846":"markdown","28d19c43":"markdown"},"source":{"fbb99301":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","57963010":"!pip install lofo-importance","efbe5d5e":"import pandas as pd\nimport numpy as np\nimport janestreet as jane\nfrom lofo import LOFOImportance, Dataset, plot_importance\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.ensemble import RandomForestClassifier","f597e38c":"train = pd.read_csv(\"\/kaggle\/input\/jane-street-market-prediction\/train.csv\")","68213b7f":"\ntrain = train.query('date > 85').reset_index(drop = True) \ntrain = train[train['weight'] != 0]\n\ntrain.fillna(train.mean(),inplace=True)\nfeatures = [c for c in train.columns if \"feature\" in c]\n\nf_mean = np.mean(train[features[1:]].values,axis=0)\nresp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\nX_train = list(list(np.where(train.columns.str.contains(\"feature\")))[0]) \nX_train = list(train.columns[X_train])","915e1f6a":"train['Y'] = ((train['resp'].values) > 0).astype(int)","c85bb9fa":"train.head(20)","eea76829":"Y_train = list(list(np.where(train.columns.str.contains(\"Y\")))[0]) \nY_train = list(train.columns[Y_train])","f651043a":"dataset = Dataset(df=train, target=Y_train, features=X_train)","184951cc":"Imp = LOFOImportance(dataset, cv=3, scoring=\"roc_auc\")","a5dbda8d":"imp_result = Imp.get_importance()","484f6713":"imp_result.to_csv(\"importanceResults_resp.csv\", index=False)","54065360":"\nplot_importance(imp_result.head(130), figsize=(8, 70), kind=\"default\")","2ae6269c":"temp = pd.DataFrame(train.isna().sum().sort_values(ascending=False)*100\/train.shape[0],columns=['missing %']).head(20)\ntemp.style.background_gradient(cmap='Purples')","696a7846":"<hr>\n\n**LOFO_Importance:****\n\n1. It will remove one feature from the list of features and calculate the performance of the base_model that we have given with the scoring parameter.\n\n> **why lofo:**\n1. There are a lot of methods to select important features like forward feature selection, Tree-based feature selection, and Recursive Feature Elimination\n2. LOFO has several advantages compared to other:\n3. It is model agnostic. You won\u2019t need different importance types using different models. As we use weights for Logistic Regression and gain for GBDT\n4. It gives negative importance to features that hurt performance upon inclusion","28d19c43":"**Missing Value Analysis**"}}