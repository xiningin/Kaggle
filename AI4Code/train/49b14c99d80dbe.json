{"cell_type":{"db31bf31":"code","2ede575b":"code","4b609d44":"code","6bb6a12e":"code","ed99961b":"code","02acf940":"code","0cd2fc96":"code","6c6a48be":"code","fdd6b814":"code","c68633e4":"code","0b9ad205":"code","6dc6f832":"code","eed5cef8":"code","699cb874":"markdown","58aa99b1":"markdown","c9b5160f":"markdown","6179f784":"markdown","4d86087c":"markdown","a3b3257e":"markdown","12a9bada":"markdown","8faeec51":"markdown","9ac24d1b":"markdown","b0568e42":"markdown","84b17381":"markdown","3a40a630":"markdown","98293e2d":"markdown","bbe9c2f7":"markdown"},"source":{"db31bf31":"import pandas as pd\n# show images inline\n%matplotlib inline\n\nimport keras\nimport tensorflow\n\n# import miscellaneous modules\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nimport numpy as np\nimport time\n\nimport tensorflow as tf\n\n\n#Clone Git Repository\n!git clone https:\/\/github.com\/fizyr\/keras-retinanet.git\n%cd keras-retinanet\/\n!python setup.py build_ext --inplace\n\n# import keras_retinanet\nfrom keras_retinanet import models\nfrom keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\nfrom keras_retinanet.utils.visualization import draw_box, draw_caption\nfrom keras_retinanet.utils.colors import label_color\nfrom keras_retinanet import models","2ede575b":"df_train = pd.read_csv(\"\/kaggle\/input\/tensorflow-great-barrier-reef\/train.csv\")\n\ndf_train=df_train.loc[df_train[\"annotations\"].astype(str) != \"[]\"]\ndf_train['annotations'] = df_train['annotations'].apply(eval)\n\ndf_train['image_path'] = \"\/kaggle\/input\/tensorflow-great-barrier-reef\/train_images\/video_\" + df_train['video_id'].astype(str) + \"\/\" + df_train['video_frame'].astype(str) + \".jpg\"\ndf_extrain=df_train.explode('annotations') # Single annotation per row\ndf_extrain.reset_index(inplace=True)\ndf_extrain.head()","4b609d44":"df_extrain_main=pd.DataFrame(pd.json_normalize(df_extrain['annotations']), columns=['x', 'y', 'width', 'height']).join(df_extrain)\ndf_extrain_main['class']='Fish'\ndf_extrain_main=df_extrain_main[['image_path','x','y','width','height','class','video_id','video_frame']]\ndf_extrain_main.head(10)","6bb6a12e":"!pip install --upgrade git+https:\/\/github.com\/broadinstitute\/keras-resnet\nimport keras\nimport keras_resnet\nimport urllib.request\nPRETRAINED_MODEL = '.\/snapshots\/_pretrained_model.h5'\n#### OPTION 1: DOWNLOAD INITIAL PRETRAINED MODEL FROM FIZYR ####\nURL_MODEL = 'https:\/\/github.com\/fizyr\/keras-retinanet\/releases\/download\/0.5.1\/resnet50_coco_best_v2.1.0.h5'\nurllib.request.urlretrieve(URL_MODEL, PRETRAINED_MODEL)\nprint('Downloaded pretrained model to ' + PRETRAINED_MODEL)","ed99961b":"\ndef create_tf_example(rowss,data_df):\n    \"\"\"Create a tf.Example entry for a given training image.\"\"\"\n    full_path = os.path.join(rowss.image_path)\n    with tf.io.gfile.GFile(full_path, 'rb') as fid:\n        encoded_jpg = fid.read()\n    encoded_jpg_io = io.BytesIO(encoded_jpg)\n    image = Image.open(encoded_jpg_io)\n    if image.format != 'JPEG':\n        raise ValueError('Image format not JPEG')\n\n    height = image.size[1] # Image height\n    width = image.size[0] # Image width\n    #print(width,height)\n    filename = f'{rowss.video_id}:{rowss.video_frame}'.encode('utf8') # Unique id of the image.\n    encoded_image_data = None # Encoded image bytes\n    image_format = 'jpeg'.encode('utf8') # b'jpeg' or b'png'\n\n    xmins = [] \n    xmaxs = [] \n    ymins = [] \n    ymaxs = [] \n    \n    # Convert ---> [xmin,ymin,width,height] to [xmins,xmaxs,ymins,ymaxs]\n    xmin = rowss['x']\n    xmax = rowss['x']+rowss['width']\n    ymin = rowss['y']\n    ymax = rowss['y']+rowss['height']\n    \n\n    #main_data.append((rowss['image_path'],xmins,xmaxs,ymins,ymaxs))\n    return rowss['image_path'],xmin,ymin,xmax,ymax","02acf940":"import tensorflow as tf\nimport contextlib2\nimport io\nimport IPython\nimport json\nimport numpy as np\nimport os\nimport pathlib\nimport pandas as pd\nimport sys\nimport tensorflow as tf\nimport time\n\ntf_example1=[]\n\nfrom PIL import Image, ImageDraw\nfor index, row in df_extrain_main.iterrows():\n            if index % 1000 == 0:\n                print('Processed {0} images.'.format(index))\n            image_path,xmins,ymins,xmaxs,ymaxs=create_tf_example(row,df_extrain_main)\n            #print(image_path,xmins,xmaxs,ymins,ymaxs)\n            df_extrain_main.loc[index,'image_path']=image_path\n            df_extrain_main.loc[index,'x']=xmins\n            df_extrain_main.loc[index,'y']=ymins\n            df_extrain_main.loc[index,'width']=xmaxs\n            df_extrain_main.loc[index,'height']=ymaxs\n","0cd2fc96":"classes=pd.DataFrame([{'class':'Fish','label':0}])\nclasses.to_csv(\"classes.csv\",index=False,header=False)  # This CSV will be use in training\n\ndf_extrain_main['class']='Fish'\ndf_extrain_main[['image_path','x','y','width','height','class']].to_csv(\"annotation.csv\",index=False,header=False)","6c6a48be":"!keras_retinanet\/bin\/train.py --freeze-backbone --random-transform --weights {PRETRAINED_MODEL} --batch-size 1 --steps 500 --epochs 60 csv annotation.csv classes.csv","fdd6b814":"model_path = os.path.join('snapshots', sorted(os.listdir('snapshots'), reverse=True)[0])\n#print(model_path)\n\n# load retinanet model\nmodel = models.load_model(model_path, backbone_name='resnet50')  ## Use backbone as resnet50\nmodel = models.convert_model(model)\n\n# load label to names mapping for visualization purposes\nlabels_to_names = pd.read_csv('classes.csv',header=None).T.loc[0].to_dict()","c68633e4":"THRES_SCORE = 0.4  # Set Score Threshold Value\n\ndef df_plot_orinal(drawOG,img_path,df):\n    df=df[df['image_path']==img_path]\n    for i,r in df.iterrows():\n        cv2.rectangle(drawOG, (r['x'], r['y']), (r['width'], r['height']), (255,0,0),2)\n    \n\ndef img_inference(img_path):\n  image = read_image_bgr(img_path)\n\n  # copy to draw on\n  draw = image.copy()\n  draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n  drawOG = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n  # preprocess image for network\n  image = preprocess_image(image)\n  image, scale = resize_image(image)\n\n  # process image\n  start = time.time()\n  boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))\n  df_plot_orinal(drawOG,img_path,df_extrain_main)\n  # correct for image scale\n  boxes \/= scale\n  # visualize detections\n  for box, score, label in zip(boxes[0], scores[0], labels[0]):\n      # scores are sorted so we can break\n      #print(score)\n      if score < THRES_SCORE:\n          continue\n      color = label_color(label)\n      b = box.astype(int)\n      draw_box(draw, b, color=color)\n      caption = \"{} {:.3f}%\".format(labels_to_names[label], score*100)\n    \n  fig = plt.figure(figsize=(20, 20))\n  ax1=fig.add_subplot(1, 2, 1)\n  plt.imshow(draw)\n  ax2=fig.add_subplot(1, 2, 2)\n  plt.imshow(drawOG)\n\n  ax1.title.set_text('Predicted')\n  ax2.title.set_text('Actual')\n  plt.show()","0b9ad205":"data=df_extrain_main.sample(n=5)  #Predict on Random 5 Image\nfor i,r in data.iterrows():\n    img_inference(r['image_path'])","6dc6f832":"# Import the library that is used to submit the prediction result.\nimport sys\nINPUT_DIR = '\/kaggle\/input\/tensorflow-great-barrier-reef\/greatbarrierreef\/'\nsys.path.insert(0, INPUT_DIR)\nimport greatbarrierreef\nenv = greatbarrierreef.make_env()   # initialize the environment\niter_test = env.iter_test()","eed5cef8":"submission_dict = {\n    'id': [],\n    'prediction_string': [],\n}\n\nfor (image, sample_prediction_df) in iter_test:\n  print(image.shape,sample_prediction_df)\n\n  image = preprocess_image(image)\n  image, scale = resize_image(image)\n\n  # process image\n  start = time.time()\n  boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))\n  predictions=[]\n  # correct for image scale\n  boxes \/= scale\n  # visualize detections\n  for box, score, label in zip(boxes[0], scores[0], labels[0]):\n      # scores are sorted so we can break\n      if score < THRES_SCORE:\n          continue\n      x_min = int(box[0])  \n      y_min = int(box[1])\n      x_max = int(box[2])\n      y_max = int(box[3])\n\n      bbox_width = x_max - x_min\n      bbox_height = y_max - y_min\n      predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n\n  prediction_str = ' '.join(predictions)\n  sample_prediction_df['annotations'] = prediction_str\n  env.predict(sample_prediction_df)\n  print('Prediction:', prediction_str)\n\n#my_submission = pd.DataFrame(sample_prediction_df)\n# you could use any filename. We choose submission here\n#sample_prediction_df.to_csv('submission.csv', index=False)","699cb874":"<a id=\"1\"><\/a>\n# <p style=\"background-color:#000000;font-family:newtimeroman;color:#fff;font-size:120%;text-align:center;border-radius:20px 80px;\"> Great Barrier Reef API<\/p>","58aa99b1":"<a id=\"1\"><\/a>\n# <p style=\"background-color:#000000;font-family:newtimeroman;color:#fff;font-size:120%;text-align:center;border-radius:20px 80px;\">Transfoming Data Format <\/p>","c9b5160f":"> ###  Not bad\ud83e\udd14, it could be more accurate by hyperparameter tuning\u2699\ud83d\udee0\n> ### Next I will try to Fine tune the same model. \n","6179f784":"<a id=\"1\"><\/a>\n# <p style=\"background-color:#000000;font-family:newtimeroman;color:#fff;font-size:120%;text-align:center;border-radius:20px 80px;\">Load Trained Model<\/p>","4d86087c":"<p style=\"font-size:220%;text-align:center\"> If you find this notebook interesting, please do upvote :) <\/p>\n\n# <p style=\"text-align:center\"> <img src=\"https:\/\/media.giphy.com\/media\/3oEdva9BUHPIs2SkGk\/giphy.gif\"> <\/p>\n## Checkout Inference Notebook: [\ud83c\udf1f\ud83d\udc1fDetection using Keras-RetinaNet [Inference]](https:\/\/www.kaggle.com\/mahipalsingh\/detection-using-keras-retinanet-inference)","a3b3257e":"<a id=\"1\"><\/a>\n# <p style=\"background-color:#000000;font-family:newtimeroman;color:#fff;font-size:120%;text-align:center;border-radius:20px 80px;\">Training RetinaNet<\/p>","12a9bada":"<a id=\"1\"><\/a>\n# <p style=\"background-color:#000000;font-family:newtimeroman;color:#fff;font-size:120%;text-align:center;border-radius:20px 80px;\">Creating CSV for Training <\/p>","8faeec51":"<a id=\"1\"><\/a>\n# <p style=\"background-color:#000000;font-family:newtimeroman;color:#fff;font-size:120%;text-align:center;border-radius:20px 80px;\">Download Pretrained Weights<\/p>","9ac24d1b":"<a id=\"1\"><\/a>\n# <p style=\"background-color:#000000;font-family:newtimeroman;color:#fff;font-size:120%;text-align:center;border-radius:20px 80px;\">Predicted vs Actual<\/p>","b0568e42":"<a id=\"1\"><\/a>\n# <p style=\"background-color:#000000;font-family:newtimeroman;color:#fff;font-size:120%;text-align:center;border-radius:20px 80px;\">Load Data <\/p>","84b17381":"<a id=\"1\"><\/a>\n# <p style=\"background-color:#000;font-family:newtimeroman;color:#fff;font-size:120%;text-align:center;border-radius:20px 80px;\">Introduction<\/p>\n\n> ### RetinaNet is one of the best one-stage object detection models that has proven to work well with dense and small scale objects. For this reason, it has become a popular object detection model to be used with aerial and satellite imagery. [read more...](https:\/\/developers.arcgis.com\/python\/guide\/how-retinanet-works\/)\n\n> ### In this notebook, I will be training RetinaNet architecture from [fizyr](https:\/\/github.com\/fizyr\/keras-retinanet) on [tensorflow-great-barrier-reef](https:\/\/www.kaggle.com\/c\/tensorflow-great-barrier-reef) dataset.","3a40a630":"<a id=\"1\"><\/a>\n# <p style=\"background-color:#000000;font-family:newtimeroman;color:#fff;font-size:120%;text-align:center;border-radius:20px 80px;\">Import Libraries<\/p>","98293e2d":"<a id=\"1\"><\/a>\n# <p style=\"background-color:#000000;font-family:newtimeroman;color:#fff;font-size:120%;text-align:center;border-radius:20px 80px;\"> Reference <\/p>\n\n> ### [Fizyr Keras Retinanet](https:\/\/github.com\/fizyr\/keras-retinanet)\n> ### [Great Barrier Reef API Tutorial](https:\/\/www.kaggle.com\/sohier\/great-barrier-reef-api-tutorial)\n> ### [Reef- Starter Torch FasterRCNN Infer](https:\/\/www.kaggle.com\/julian3833\/reef-starter-torch-fasterrcnn-infer-lb-0-413)\n","bbe9c2f7":"### Training this model 60 epochs for demo purpose."}}