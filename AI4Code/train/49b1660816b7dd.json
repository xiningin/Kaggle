{"cell_type":{"40da57d1":"code","6a484023":"code","04bb1d31":"code","b8756d1b":"code","3c0a2240":"code","3e1458fc":"code","8ad90d8e":"code","079f4af3":"code","9f575102":"code","9ac2ecce":"code","6417a026":"code","e27129e0":"code","b48b6294":"code","a65d7811":"code","d77f2266":"code","f3694ebd":"code","a2bc7e39":"code","0688d874":"code","77a8d851":"code","e5651f0f":"code","e5f68f66":"code","931121b1":"code","afcc6d56":"code","176ce8d2":"code","e00fd128":"code","bf552402":"code","8d052d44":"code","8eb2a4b3":"code","680c8454":"code","bbac2e5a":"code","f623e835":"code","2344c278":"markdown","a44cae57":"markdown","a09c5e76":"markdown","972f3759":"markdown","3762fed6":"markdown","ad724fd0":"markdown","c9f323ce":"markdown","1ebb64cb":"markdown","e285cf71":"markdown","ee68a4e9":"markdown","2b2d91be":"markdown","71890bac":"markdown","7bc428e0":"markdown"},"source":{"40da57d1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6a484023":"data = pd.read_csv('..\/input\/used-car-dataset-ford-and-mercedes\/ford.csv')\ndata.head()","04bb1d31":"data.info()","b8756d1b":"data.model.value_counts()","3c0a2240":"data.describe()","3e1458fc":"from sklearn.model_selection import train_test_split\n\ntrain_set, test_set = train_test_split(data, test_size=0.2, random_state=42)\ndata[\"tax_cat\"] = pd.cut(data[\"tax\"], bins=[-1, 116, 232, 348, 464, np.inf], labels=[1 ,2, 3, 4, 5])\ndata.sample(5)","8ad90d8e":"data[\"tax_cat\"].value_counts().sort_index()","079f4af3":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(data, data[\"tax_cat\"]):\n    strat_train_set = data.loc[train_index]\n    strat_test_set = data.loc[test_index]","9f575102":"# Result Alternative 2:\nstrat_test_set[\"tax_cat\"].value_counts().sort_index() \/ len(strat_test_set)","9ac2ecce":"# Result Alternative 1:\ndata[\"tax_cat\"].value_counts().sort_index() \/ len(data)","6417a026":"def tax_cat_proportions(test_values):\n    return test_values[\"tax_cat\"].value_counts() \/ len(test_values)\n\ntrain_set, test_set = train_test_split(data, test_size=0.2, random_state=42)\n\ncompare_props = pd.DataFrame({\n    \"Overall\": tax_cat_proportions(data),\n    \"Stratified\": tax_cat_proportions(strat_test_set),\n    \"Random\": tax_cat_proportions(test_set),\n}).sort_index()\ncompare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] \/ compare_props[\"Overall\"] - 100\ncompare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] \/ compare_props[\"Overall\"] - 100","e27129e0":"compare_props","b48b6294":"for set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"tax_cat\", axis=1, inplace=True)","a65d7811":"# Split into independent variables and dependent variable (y)\nx_predictors = strat_train_set.drop(\"price\", axis=1)\ny_labels = strat_train_set[\"price\"].copy()\n\n# Generate variable without strings\nford_num = x_predictors.drop([\"model\", \"transmission\", \"fuelType\"], axis=1)","d77f2266":"# Pipeline for NUMERICAL values\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n    (\"std_scaler\", StandardScaler()),\n])","f3694ebd":"# Transform CATEGORICAL values\nfrom sklearn.preprocessing import OneHotEncoder\n\nford_cat = x_predictors[[\"model\", \"transmission\", \"fuelType\"]]\ncat_encoder = OneHotEncoder()\nford_cat_1hot = cat_encoder.fit_transform(ford_cat)\nford_cat_1hot","a2bc7e39":"# Do all at once (i.e. NUMERICAL\/CATAGEORICAL variables)\nfrom sklearn.compose import ColumnTransformer\n\nnum_attribs = list(ford_num)\ncat_attribs = [\"model\", \"transmission\", \"fuelType\"]\n\nfull_pipeline = ColumnTransformer([\n    (\"num\", num_pipeline, num_attribs),\n    (\"cat\", OneHotEncoder(), cat_attribs),\n])\n\nford_prepared = full_pipeline.fit_transform(x_predictors)","0688d874":"from sklearn.svm import SVR\n\nsvr_linear = SVR(kernel=\"linear\", C=100)\nsvr_linear.fit(ford_prepared, y_labels)","77a8d851":"from sklearn.metrics import mean_squared_error\n\ndata_predictions = svr_linear.predict(ford_prepared)\nsvr_mse = mean_squared_error(y_labels, data_predictions)\nsvr_rmse = np.sqrt(svr_mse)\nsvr_rmse","e5651f0f":"from sklearn.model_selection import GridSearchCV\n\nparameters = {'kernel':('linear', 'rbf'), 'C':[1, 10, 100, 1000]}\nsvr_eval = SVR()\nclf = GridSearchCV(svr_eval, parameters)\nclf.fit(ford_prepared, y_labels)\n\nsorted(clf.cv_results_.keys())","e5f68f66":"clf.best_params_","931121b1":"clf.best_estimator_","afcc6d56":"clf.best_score_","176ce8d2":"clf.best_index_","e00fd128":"svr_linear_opt = SVR(kernel=\"rbf\", C=1000)\nsvr_linear_opt.fit(ford_prepared, y_labels)\n\ndata_predictions = svr_linear_opt.predict(ford_prepared)\nsvr_mse = mean_squared_error(y_labels, data_predictions)\nsvr_rmse = np.sqrt(svr_mse)\nsvr_rmse","bf552402":"clf_results = clf.cv_results_\nfor svr_mse, params in zip(clf_results[\"mean_test_score\"], clf_results[\"params\"]):\n    print(np.sqrt(svr_mse), params)\n","8d052d44":"final_model = clf.best_estimator_\n\nX_test = strat_test_set.drop(\"price\", axis=1)\ny_test = strat_test_set[\"price\"].copy()","8eb2a4b3":"X_test_prepared = full_pipeline.transform(X_test)","680c8454":"final_predictions = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)","bbac2e5a":"final_rmse","f623e835":"from scipy import stats\nconfidence = 0.95\nsquared_errors = (final_predictions - y_test) ** 2\nnp.sqrt(stats.t.interval(confidence, len(squared_errors) - 1, loc=squared_errors.mean(), scale=stats.sem(squared_errors)))","2344c278":"# 5. Evaluation on the test set","a44cae57":"# 3. Encoding Categorical and Numerical variables","a09c5e76":"### 4.4 Overview of all combinations","972f3759":"Based on the test set, the SVM model predicts prices for Ford cars with a RMSE of ~1,247, which is off by about 10% of the average price.","3762fed6":"### 2.1 Alternative 1","ad724fd0":"# 4. Using a Support Vector Machine (SVM) regressor","c9f323ce":"### 4.1 Evaluation of results","1ebb64cb":"# 2. Stratifying sample","e285cf71":"### 2.2 Alternative 2","ee68a4e9":"### 4.3 Using optimised parameters","2b2d91be":"# 1. EDA","71890bac":"### 2.3 Compare stratification results to default","7bc428e0":"### 4.2 Testing different kernels and hyperparameter combinations with GridSearch"}}