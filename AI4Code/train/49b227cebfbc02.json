{"cell_type":{"b60f37d5":"code","1e661e0a":"code","9a304699":"code","ca9988da":"code","14b94ca3":"code","dd50c0a7":"code","564f3457":"code","6d70a63e":"code","1a95f82a":"code","2276658a":"code","5c9101a0":"code","c738328e":"code","9f683098":"code","a375bc07":"code","7d95ec06":"code","6f038dba":"code","f902f0b5":"code","c8aad533":"code","4c57090f":"markdown","f008fece":"markdown","92c98a9f":"markdown","a750be1f":"markdown","a2b8401b":"markdown"},"source":{"b60f37d5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom sklearn import model_selection, preprocessing, metrics\n\nfrom sklearn import preprocessing\nimport gc\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nimport os\n","1e661e0a":"train = pd.read_csv('..\/input\/melanoma-train-test-creator\/train_meta_size.csv')\ntest = pd.read_csv('..\/input\/melanoma-train-test-creator\/test_meta_size.csv')","9a304699":"features = test.columns","ca9988da":"target = np.hstack([np.zeros(train.shape[0],), np.ones(test.shape[0],)])\ntrain_test = np.vstack([train[features].values, test.values])\nprint(train_test.shape)\nprint(target.shape)\ndel train, test\ngc.collect()","14b94ca3":"train, test, y_train, y_test = model_selection.train_test_split(train_test, target, test_size=0.33, random_state=42, shuffle=True)\ndel target, train_test\ngc.collect()","dd50c0a7":"train = lgb.Dataset(train, label=y_train)\ntest = lgb.Dataset(test, label=y_test)","564f3457":"param = {'num_leaves': 50,\n         'min_data_in_leaf': 30, \n         'objective':'binary',\n         'max_depth': 8,\n         'learning_rate': 0.05,\n         \"min_child_samples\": 20,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9 ,\n         \"bagging_seed\": 56,\n         \"metric\": 'auc',\n         \"verbosity\": -1}","6d70a63e":"num_round = 2000\nclf = lgb.train(param, train, num_round, valid_sets = [train, test], verbose_eval=50, early_stopping_rounds = 50)","1a95f82a":"feature_imp = pd.DataFrame(sorted(zip(clf.feature_importance(),features)), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 20))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).head(5))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-01.png')","2276658a":"feature_imp.sort_values(by=\"Value\", ascending=False).head(5)","5c9101a0":"train = pd.read_csv('..\/input\/melanoma-train-test-creator\/train_meta_size_2.csv')\ntest = pd.read_csv('..\/input\/melanoma-train-test-creator\/test_meta_size_2.csv')","c738328e":"features = test.columns","9f683098":"target = np.hstack([np.zeros(train.shape[0],), np.ones(test.shape[0],)])\ntrain_test = np.vstack([train[features].values, test.values])\nprint(train_test.shape)\nprint(target.shape)\ndel train, test\ngc.collect()","a375bc07":"train, test, y_train, y_test = model_selection.train_test_split(train_test, target, test_size=0.33, random_state=42, shuffle=True)\ndel target, train_test\ngc.collect()","7d95ec06":"train = lgb.Dataset(train, label=y_train)\ntest = lgb.Dataset(test, label=y_test)","6f038dba":"param = {'num_leaves': 50,\n         'min_data_in_leaf': 30, \n         'objective':'binary',\n         'max_depth': 8,\n         'learning_rate': 0.05,\n         \"min_child_samples\": 20,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9 ,\n         \"bagging_seed\": 56,\n         \"metric\": 'auc',\n         \"verbosity\": -1}","f902f0b5":"num_round = 2000\nclf = lgb.train(param, train, num_round, valid_sets = [train, test], verbose_eval=50, early_stopping_rounds = 50)","c8aad533":"feature_imp = pd.DataFrame(sorted(zip(clf.feature_importance(),features)), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 20))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).head(5))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-02.png')","4c57090f":"So the biggest \"culprits\" for the train\/test discrepancy are the age, followed by the image width.","f008fece":"So there is a little bit of an \"improvement\", but overall it is still a fairly significant distinction.","92c98a9f":"We'll now look at the train and test sets with more \"rigorously\" impoted missing values:","a750be1f":"For raw image data we were able to get an AUC of 0.65, while here we get 0.70. Seems that the image size and image metafeatrue data has more discrepancy between the train and test sets than the raw rescaled images.\n\nLet's look at the top features and theri relative importances.","a2b8401b":"In this notebook we'll try to assess the degree of difference between the train and test sets using adversarial validation approach. We have already done the same exercise with the rescaled raw image data, and that approach can be found [here](https:\/\/www.kaggle.com\/tunguz\/adversarial-melanoma). In this notebook we'll just use the metadata and the image size data. We have already preprocessed the datasets [in this notebook](https:\/\/www.kaggle.com\/tunguz\/melanoma-train-test-creator\/), and we'll just use the output here."}}