{"cell_type":{"e1ef2684":"code","1f9d1624":"code","b6035da2":"code","f911c0dd":"code","dd9144de":"code","27c92d47":"code","0e183eaf":"code","c0fdf995":"code","fb1b85dc":"code","4cd86f69":"code","d7e22650":"code","cd1dfb45":"code","89601e34":"code","09edebd2":"code","63e2e02d":"code","16735232":"code","3930f249":"markdown","614d9070":"markdown","7dfe3283":"markdown","95a34e13":"markdown","90baf3d9":"markdown","eb8545c8":"markdown","22c1ca42":"markdown","296a42b5":"markdown","1511f0e9":"markdown","f4056ba2":"markdown","4514844e":"markdown","1a221209":"markdown","55c7437a":"markdown","b13c7757":"markdown","1adf7bb6":"markdown","7d5d2c73":"markdown","0a941027":"markdown","82a6d088":"markdown","0f3d4232":"markdown","4efa95d3":"markdown","ac9324f3":"markdown","0b624a88":"markdown","d2f12820":"markdown","a6a7bd19":"markdown","0b59b744":"markdown","fa5b9032":"markdown","1620659b":"markdown","d39e8964":"markdown","28dc1fc8":"markdown","7392de06":"markdown","51631cf8":"markdown","844f1ed4":"markdown","b12b53a4":"markdown","6700c802":"markdown","9e9dfdce":"markdown","2e8813fb":"markdown","900fbaa1":"markdown","7fe5cbac":"markdown","eb2cb241":"markdown","6fbb3ee3":"markdown","3cc6a9b5":"markdown","a29f75de":"markdown","fb19237f":"markdown","ff07a147":"markdown","9f2df878":"markdown","962c6016":"markdown","2670db6e":"markdown"},"source":{"e1ef2684":"import multiprocessing \nimport os \n  \ndef worker1(): \n    # printing process id \n    print(\"ID of process running worker1: {}\".format(os.getpid())) \n  \ndef worker2(): \n    # printing process id \n    print(\"ID of process running worker2: {}\".format(os.getpid())) \n  \nif __name__ == \"__main__\": \n    # printing main program process id \n    print(\"ID of main process: {}\".format(os.getpid())) \n  \n    # creating processes \n    p1 = multiprocessing.Process(target=worker1) \n    p2 = multiprocessing.Process(target=worker2) \n  \n    # starting processes \n    p1.start() \n    p2.start() \n\n    # process IDs \n    print(\"ID of process p1: {}\".format(p1.pid)) \n    print(\"ID of process p2: {}\".format(p2.pid)) \n  \n    # wait until processes are finished \n    p1.join() \n    p2.join() \n\n    # both processes finished \n    print(\"Both processes finished execution!\") \n  \n    # check if processes are alive \n    print(\"Process p1 is alive: {}\".format(p1.is_alive())) \n    print(\"Process p2 is alive: {}\".format(p2.is_alive()))\n    \n","1f9d1624":"# for file_chunk in file_chunks:\n#     p = Process(target=my_func, args=(file_chunk, my_other_arg))\n#     p.start()\n#     p.join()","b6035da2":"# from multiprocessing import Pool\n\n# pool = Pool(ncores)\n\n# for file_chunk in file_chunks:\n#     pool.apply_async(my_func, args=(file_chunk, arg1, arg2)) ","f911c0dd":"# # Pool example skeleton code:\n# def eval_formula...\n\n\n# p=multiprocessing.Pool(multiprocessing.cpu_count)\n# result=p.map(eval_formula, expression_list)\n# p.close()\n# p.join()","dd9144de":"# # Process example skeleton code:\n\n# def eval_formula...\n\n# for i in range (len(expression_list)):\n#     p=Process(target=proces_eval,args=(expression_list[i],))\n#     p.start()\n#     p.join()\n\n","27c92d47":"# from multiprocessing import Pool\n# from PIL import Image\n\n# SIZE = (75,75)\n# SAVE_DIRECTORY = 'thumbs'\n\n# def get_image_paths(folder):\n#   return (os.path.join(folder, f)\n#       for f in os.listdir(folder)\n#       if 'jpeg' in f)\n\n# def create_thumbnail(filename):\n#   im = Image.open(filename)\n#   im.thumbnail(SIZE, Image.ANTIALIAS)\n#   base, fname = os.path.split(filename)\n#   save_path = os.path.join(base, SAVE_DIRECTORY, fname)\n#   im.save(save_path)\n\n# if __name__ == '__main__':\n#   folder = os.path.abspath(\n#     '11_18_2013_R000_IQM_Big_Sur_Mon__e10d1958e7b766c3e840')\n#   os.mkdir(os.path.join(folder, SAVE_DIRECTORY))\n\n#   images = get_image_paths(folder)\n\n#   pool = Pool()\n#     pool.map(create_thumbnail, images)\n#     pool.close()\n#     pool.join()\n","0e183eaf":"# # only thing that we had to replace is \n# for image in images:\n#     create_thumbnail(image)\n# #and instead we used\n\n#  pool = Pool()\n#     pool.map(create_thumbnail, images)\n#     pool.close()\n#     pool.join()","c0fdf995":"# df.shape\n# # (100, 100)\n# dfs = [df.iloc[i*25:i*25+25, 0] for i in range(4)]\n# with Pool(4) as p:\n#     res = p.map(np.exp, dfs)\n# for i in range(4): df.iloc[i*25:i*25+25, 0] = res[i]\n","fb1b85dc":"# from numba import njit, jit\n# @njit      # or @jit(nopython=True)\n# def function(a, b):\n#     # your loop or numerically intensive computations\n#     return result","4cd86f69":"# @vectorize(target=\"parallel\")\n# def func(a, b):\n#     # Some operation on scalars\n#     return result","d7e22650":"# @vectorize(target=\"cuda\")\n# def func(a, b):\n#     # Some operation on scalars\n#     return result","cd1dfb45":"%time\nfrom math import sqrt\nfrom joblib import Parallel, delayed\nParallel(n_jobs=2)(delayed(sqrt)(i**2) for i in range(100000))\n","89601e34":"%time\nfrom math import sqrt\nfrom joblib import Parallel, delayed\nParallel(n_jobs=2,backend='threading')(delayed(sqrt)(i**2) for i in range(100000))\n","09edebd2":"for i in range(100000):\n    print(sqrt(i**2))","63e2e02d":"[sqrt(x**2) for x in range(1000000)]","16735232":"from math import modf\nfrom joblib import Parallel, delayed\nr = Parallel(n_jobs=1,verbose=10)(delayed(modf)(i\/2.) for i in range(100000))\nres, i = zip(*r)\n","3930f249":"# Using pooling on Pandas Dataframes\n\nThere are 4 CPU, we apply on every quarter (since 100*100 dimensions) exponential function","614d9070":"# Parallelisation in python\n\nThings covered here\n1. multiprocessing and multithreading\n2. implementation\n3. Pool or Process from multiprocessing lib \n4. Numba-JiT compilation in python for speedup\n5. Joblib parellelisation, and some comperisons","7dfe3283":"IN REALITY, it depends on the task at hand. Pool allows you to do multiple jobs per process, which may make it easier to parallelize your program. If  you have a million tasks to execute in parallel, you can create a Pool with number of processes as many as CPU cores and then pass the list of the million tasks to pool.map. The pool will distribute those tasks to the worker processes(typically same in number as available cores) and collects the return values in the form of list and pass it to the parent process. Launching separate million processes would be much less practical (it would probably break your OS).","95a34e13":"### So when to use threading and when multiprocessing for parallelisation?","90baf3d9":"SUPER fast, they are optimised...","eb8545c8":"Also be careful with functions, numpy and standard stuff is covered. But check it http:\/\/numba.pydata.org\/numba-doc\/0.17.0\/reference\/pysupported.html","22c1ca42":"Nice example at: https:\/\/www.geeksforgeeks.org\/multiprocessing-python-set-2\/","296a42b5":" By using @vectorize wrapper you can convert your functions which operates on scalars only, for example if you are using python\u2019s math library which only works on scalars, to work for arrays.","1511f0e9":"The threading module uses threads, the multiprocessing module uses processes. The difference is that threads run in the same memory space, while processes have separate memory. This makes it a bit harder to share objects between processes with multiprocessing. Since threads use the same memory, precautions have to be taken or two threads will write to the same memory at the same time. This is what the global interpreter lock is for.\n\nIn Python, because of GIL (Global Interpreter Lock) a single python process cannot run threads in parallel (utilize multiple cores). It can however run them concurrently (context switch during I\/O bound operations).","f4056ba2":"# At the end of the day if that is not enough, maybe you should use https:\/\/en.wikipedia.org\/wiki\/Cython","4514844e":"Well faster than threading (no IO), slower than multiprocessing, what about list comprehensions?","1a221209":"# Another way of speeding up execution in python\n\nNumba. Numba is a Just-in-time compiler for python.\n\nWhat does that even mean u ask?\n\nWell first we have to talk about how is implementation of python code translated into machine readable code. Usually python is interpreted language (unlike C which is compiled--->faster). More accurately it is BYTECODE interpreted. Meaning that when we generate a .py file, at the same time .pyc---> ByteCode file is generated. And then from here it can be interpreted or JIT-just in time compiled.\n\nA Just-In-Time (JIT) compiler is a feature of the run-time interpreter, that instead of interpreting bytecode every time a method is invoked, will compile the bytecode into the machine code instructions of the running machine, and then invoke this object code instead. Ideally the efficiency of running object code will overcome the inefficiency of recompiling the program every time it runs.\n\nENGLISH please, its more efficient. Standard python does not use JIT compilation, if u can use JIT.","55c7437a":"Suppose you have a function that has multiple outputs, and you want to keep track of processing (as always verbose argument)","b13c7757":"or","1adf7bb6":"# NOTICE \n Process wont work on windows GUI (we dont see output of \"ID of process running worker2\" , spyder notebook IDLE etc.. there is workaround \nhttps:\/\/stackoverflow.com\/questions\/21198857\/python-multiprocessing-example-not-working\nor use threading\n\nOR:\n\nopen the shell (on windows) and save the script in a .py document (mine was 2.py) \n and type C:\\Users\\Noah>python C:\\Users\\Noah\\Desktop\\2.py\n \n \n ","7d5d2c73":"# Another example\nfrom https:\/\/chriskiehl.com\/article\/parallelism-in-one-line","0a941027":"nopython = True argument in jit wrapper, using which it won\u2019t use the Python interpreter at all.","82a6d088":"Process makes one sub process that you can interact with. Pool is a whole set of subprocesses. Pool knows how to split work between them, and get the answers back.\n\nIn other words spawning too many processes will also negatively affect your performance as they will compete against each other for the CPU. But Pool abstracts most of the competition away.\n","0f3d4232":"So what does that have to do with number or CPU, depending on the number of CPU we will see (number_of_CPU times improvement) in regards to speed","4efa95d3":"Computations should be intensive. For example, compute intensive loop, maybe with libraries (numpy) and functions it supports, because alternatively it will be slower than the usual code.","ac9324f3":"# To conclude: 2 things are important when deciding Pool\/Process:","0b624a88":"What is cuda and how do I know if my computer supports it?\n\n1. Short: GPU are much more powerfull than CPU. The CUDA platform is a software layer that gives direct access to the GPU's virtual instruction set and parallel computational elements.\n\n2. Usually all NVIDIA GeForce, Quadro, and Tesla GPUs as well as NVIDIA GRID solutions have CUDA, alternative just gooogle your hardware specs.","d2f12820":"A lot of functionalities https:\/\/joblib.readthedocs.io\/en\/latest\/index.html, I am going to focus on Parallel (where multiprocessing and mulltithreading can be included) and restate some interesting examples there and make comaprison:","a6a7bd19":"So what does that have to do with Synchronous\/Asynchronous execution?","0b59b744":"Lets say we have a task that will be performed on seperate cores but they will share same resources, we need to make sureit does not get mixed up----race condition\n\nSolution is using locks, nice example at https:\/\/www.geeksforgeeks.org\/synchronization-pooling-processes-python\/","fa5b9032":"classes are not fully covered, yet. https:\/\/numba.pydata.org\/numba-doc\/dev\/user\/jitclass.html\nBut functions are, using decorators (as classes)","1620659b":"Couple of interestign things. \n1. First, not only DS stuff. Need to automate something tedious on your computer. There you go.\n2. Another thing, we only need to specify the function and the inputs to the function (here images, notice not list of a list)\n3. It leads to the next question, do we always use map with Pool?","d39e8964":"1. multiple processes for CPU-intensive tasks\n2. threads for (and during) IO- input output","28dc1fc8":"Parallelisation through multiprocessing or multithreading. (speaking of a single machine. Cloud or machine clusters excluded)","7392de06":"2. Size of IO\n\nThe Pool distributes the processes among the available cores in FIFO manner. On each core, the allocated process executes serially. So, if there is a long IO operation, it waits till the IO operation is completed and does not schedule another process. This leads to the increase in execution time.  The Process class suspends the process executing IO operations and schedules another process. So, in case of long IO operation, it is advisable to use process class.","51631cf8":"# NOTE","844f1ed4":"# Difference between multiprocessing and multithreading in python","b12b53a4":"Expression list is for example a list of lists that a function eval_formula needs to be applied over","6700c802":"# Communication between processes while multiprocessing","9e9dfdce":"# So what Classes to use from multiprocessing library, Pool or Process?","2e8813fb":"However there are situations where Pool wont work: http:\/\/www.oipapio.com\/question-501760 and Process will.","900fbaa1":"# How to apply Process multiprocessing\nfrom geeksforgeeks...","7fe5cbac":"multiprocessing.dummy is the library for THREADING, and multiprocessing is for multi processing.","eb2cb241":"Another thing that we could also do to increase the speed is create our own generator for example.","6fbb3ee3":"\n\nA thread is the smallest unit of processing that can be performed in an OS. In most modern operating systems, a thread exists within a process - that is, a single process may contain multiple threads.\n\nMutlithreading increase performace of a single PCU bzw single provess (word processor example in the link)\nwhere as with multiprocessing we utilise mutliple CPU bzw cores","3cc6a9b5":"1. Number of tasks\nAs we have seen, the Pool allocates only executing processes in memory and process allocates all the tasks in memory, so when the task number is small, we can use process class and when the task number is large, we can use pool. In case of large tasks, if we use process then memory problem might occur, causing system disturbance. In case of Pool, there is overhead in creating it. Hence with small task numbers, the performance is impacted when Pool is used.","a29f75de":"Thats one concrete way to apply Process, another one, more general is:","fb19237f":"# Synchronous\/Asynchronous execution is not dependent on multiprocessing\/multithreading\n\nWhen you execute something synchronously, you wait for it to finish before moving on to another task. When you execute something asynchronously, you can move on to another task before it finishes.\n\nBut multi\/single threading can be Synchronous and Asynchronous at the same time! (even though unusual)\n","ff07a147":"Now we said that only with threading we are haveing shared memory, and that divided memory is exactly one of the negatives while multiprocessing in python. Given that every new process (on every core for example) will have its own memory space, how can we communicate between them? There are two C-compactible data types (ctypes) that help us- Array and Value","9f2df878":"# Also notice \nthat we have checked if the name is main. Isnt that something that we usually do when unit testing, to ensure that our functions\/classes perform as desired in the mother\/main script? well due to the way the new processes are started, the child process needs to be able to import the script containing the target function. Wrapping the main part of the application in a check for __main__ ensures that it is not run recursively in each child as the module is imported.\n\nNOTE, use main checking with multiprocessing!","962c6016":"On the other hand, if you have a small number of tasks to execute in parallel, and you only need each task done once, it may be perfectly reasonable to use a separate multiprocessing.process for each task, rather than setting up a Pool.","2670db6e":"# Than whats joblib ?\n\n It is built on top of the multiprocessing and multithreading libraries in order to support both but has a significant portion of additional features. It is widely used also to parallelise skicit learn algos."}}