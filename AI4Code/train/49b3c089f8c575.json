{"cell_type":{"22e2a950":"code","b503bf7a":"code","594208b1":"code","b5961121":"code","9acfb221":"code","9280641e":"code","5ded63fd":"code","d447a787":"code","563953c0":"code","217df8c4":"code","21903059":"code","01a7aebd":"code","417e501b":"code","67595ac1":"code","e335c221":"code","a88ef041":"code","7b5d9f6d":"code","e7f634ea":"code","6cdb935f":"code","9c404876":"code","99d41a15":"code","fefc752c":"code","1805334e":"code","ca0f3ecc":"code","0351ab4a":"code","f1c89f69":"code","d4b989e7":"code","9b1c0cb5":"code","ce5a0f20":"code","8a2f6796":"code","8e7f6a40":"code","c804213e":"code","60af0e97":"code","26bbe8c0":"code","56f78d2a":"code","e36f66dd":"code","44717f4c":"code","404660dc":"code","77d5fdc9":"code","c7dda8b4":"code","4692a3bb":"code","d1a1b776":"code","a51588e5":"markdown","42581fc2":"markdown","68c1fe89":"markdown","3d2ac25b":"markdown","fd8b061e":"markdown","becd36c2":"markdown","c70823d6":"markdown","6e1967df":"markdown","82903c1c":"markdown","72c42f72":"markdown","41804135":"markdown","259d5a15":"markdown","ed65c285":"markdown","053fdec4":"markdown"},"source":{"22e2a950":"# Importing libraries\n\n# overall libraries\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom collections import OrderedDict\nfrom IPython.core.pylabtools import figsize\nimport re\n\n# plotting libraries\nimport seaborn as sns\nsns.set_style('white')\nimport matplotlib.pyplot as plt\nimport matplotlib.lines as mlines\n%matplotlib inline\n\n# bayesian libraries\nimport pymc3 as pm\nimport arviz as az\nimport theano\nimport theano.tensor as T\nfloatX = theano.config.floatX\nimport itertools\nfrom pymc3.theanof import set_tt_rng, MRG_RandomStreams\n\n# sklearn libraries\nimport sklearn\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import scale\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_moons\nfrom sklearn.metrics import log_loss, roc_auc_score, roc_curve, auc, precision_recall_curve, confusion_matrix","b503bf7a":"# setting the display so you can see all the columns and all the rows\n\npd.set_option(\"max_columns\", None)\npd.set_option(\"max_rows\", None)","594208b1":"# creating the DataFrame\n\ndf = pd.read_excel('..\/input\/covid19\/dataset.xlsx', encoding='utf8')","b5961121":"# Checking how the df imported\n\ndf.head()","9acfb221":"# Removing unwanted columns for the task 1\n\ndf_task1 = df.drop(['Patient addmited to regular ward (1=yes, 0=no)', 'Patient addmited to semi-intensive unit (1=yes, 0=no)', 'Patient addmited to intensive care unit (1=yes, 0=no)'], axis = 1)\ndf_task1.head()","9280641e":"# Checking the unique values for the SARS-Cov-2 exam result (our target for this task)\n\ndf_task1['SARS-Cov-2 exam result'].unique()","5ded63fd":"# Replacing negative to 0 an positive to 1 and then checking if it worked\n\ndf_task1['SARS-Cov-2 exam result'] = df_task1['SARS-Cov-2 exam result'].replace({'negative': 0, 'positive': 1})\ndf_task1['SARS-Cov-2 exam result'].unique()","d447a787":"# checking the categorical variables\n\ndf_task1.select_dtypes(include = ['object']).columns","563953c0":"# replacing the values to make them numerical, I am doing them by hand to make sure all the exams make sense.\n# This is possible because there aren't many categorical variables.\n# To do this, I checked all the variables unique values and created an unique dictionary\n\ndf_task1.loc[:,'Respiratory Syncytial Virus':'Parainfluenza 2'] = df_task1.loc[:,'Respiratory Syncytial Virus':'Parainfluenza 2'].replace({'not_detected':0, 'detected':1})\ndf_task1.loc[:,'Influenza B, rapid test':'Strepto A'] = df_task1.loc[:,'Influenza B, rapid test':'Strepto A'].replace({'negative':0, 'positive':1})\ndf_task1['Urine - Esterase'] = df_task1['Urine - Esterase'].replace({'absent':0})\ndf_task1['Urine - Aspect'] = df_task1['Urine - Aspect'].replace({'clear':0, 'cloudy':2, 'altered_coloring':3, 'lightly_cloudy':1})\ndf_task1['Urine - pH'] = df_task1['Urine - pH'].replace({'6.5':6.5, '6.0':6.0,'5.0':5.0, '7.0':7.0, '5':5, '5.5':5.5,\n       '7.5':7.5, '6':6, '8.0':8.0})\ndf_task1['Urine - Hemoglobin'] = df_task1['Urine - Hemoglobin'].replace({'absent':0, 'present':1})\ndf_task1.loc[:,'Urine - Bile pigments':'Urine - Nitrite'] = df_task1.loc[:,'Urine - Bile pigments':'Urine - Nitrite'].replace({'absent':0})\ndf_task1.loc[:,'Urine - Urobilinogen':'Urine - Protein'] = df_task1.loc[:,'Urine - Urobilinogen':'Urine - Protein'].replace({'absent':0, 'normal':1})\ndf_task1['Urine - Hemoglobin'] = df_task1['Urine - Hemoglobin'].replace({'absent':0, 'present':1, 'not_done':np.nan})\ndf_task1['Urine - Leukocytes'] = df_task1['Urine - Leukocytes'].replace({'38000':38000, '5942000':5942000, '32000':32000, '22000':22000,'<1000': 900, '3000': 3000,'16000':16000, '7000':7000, '5300':5300, '1000':1000, '4000':4000, '5000':5000, '10600':106000, '6000':6000, '2500':2500, '2600':2600, '23000':23000, '124000':124000, '8000':8000, '29000':29000, '2000':2000,'624000':642000, '40000':40000, '3310000':3310000, '229000':229000, '19000':19000, '28000':28000, '10000':10000,'4600':4600, '77000':77000, '43000':43000})\ndf_task1['Urine - Crystals'] = df_task1['Urine - Crystals'].replace({'Ausentes':0, 'Urato Amorfo --+':1, 'Oxalato de C\u00e1lcio +++':3,'Oxalato de C\u00e1lcio -++':2, 'Urato Amorfo +++':4})\ndf_task1.loc[:,'Urine - Hyaline cylinders':'Urine - Yeasts'] = df_task1.loc[:,'Urine - Hyaline cylinders':'Urine - Yeasts'].replace({'absent':0})\ndf_task1['Urine - Color'] = df_task1['Urine - Color'].replace({'light_yellow':0, 'yellow':1, 'orange':2, 'citrus_yellow':1})\ndf_task1 = df_task1.replace('not_done', np.NaN)\ndf_task1 = df_task1.replace('N\u00e3o Realizado', np.NaN)","217df8c4":"# Dropping the patient ID column\n\ndf_task1 = df_task1.drop('Patient ID', axis = 1)","21903059":"# checking if all of the categorical variables were treated\n\ndf_task1.select_dtypes(include = ['object']).columns","01a7aebd":"# checking how the data is distribuited in the dataframe\n\ndf_task1.info()","417e501b":"# let's see what are the two columns that are working with int\n\ndf_task1.select_dtypes(include = ['int64']).columns","67595ac1":"# let's create a rank of missing values\n\nnull_count = df_task1.isnull().sum().sort_values(ascending=False)\nnull_percentage = null_count \/ len(df_task1)\nnull_rank = pd.DataFrame(data=[null_count, null_percentage],index=['null_count', 'null_ratio']).T\nnull_rank","e335c221":"# dropping columns that don't have any content in it\n\ndf_task1 = df_task1.drop(['Mycoplasma pneumoniae','Urine - Nitrite', 'Urine - Sugar', 'Partial thromboplastin time\u00a0(PTT)\u00a0', 'Prothrombin time (PT), Activity', 'D-Dimer'], axis = 1)","a88ef041":"# let's see the min and max values of the variables to fill their missing values\n\ndf_task1.describe().round(2)","7b5d9f6d":"# filling missing values with 0\n\ndf_task1[['Urine - Leukocytes', 'Urine - pH']] = df_task1[['Urine - Leukocytes', 'Urine - pH']].fillna(0)","e7f634ea":"# filling missing values with -1\n\ndf_task1[['Patient age quantile', 'SARS-Cov-2 exam result', 'Respiratory Syncytial Virus', 'Influenza A', 'Influenza B', 'Parainfluenza 1', 'CoronavirusNL63', 'Rhinovirus\/Enterovirus', 'Coronavirus HKU1', 'Parainfluenza 3', 'Chlamydophila pneumoniae', 'Adenovirus', 'Parainfluenza 4', 'Coronavirus229E', 'CoronavirusOC43', 'Inf A H1N1 2009', 'Bordetella pertussis', 'Metapneumovirus', 'Parainfluenza 2', 'Influenza B, rapid test', 'Influenza A, rapid test', 'Strepto A', 'Fio2 (venous blood gas analysis)','Myeloblasts', 'Urine - Esterase', 'Urine - Hemoglobin', 'Urine - Bile pigments', 'Urine - Ketone Bodies', 'Urine - Protein', 'Urine - Crystals', 'Urine - Hyaline cylinders', 'Urine - Granular cylinders', 'Urine - Yeasts', 'Urine - Color']] = df_task1[['Patient age quantile', 'SARS-Cov-2 exam result', 'Respiratory Syncytial Virus', 'Influenza A', 'Influenza B', 'Parainfluenza 1', 'CoronavirusNL63', 'Rhinovirus\/Enterovirus', 'Coronavirus HKU1', 'Parainfluenza 3', 'Chlamydophila pneumoniae', 'Adenovirus', 'Parainfluenza 4', 'Coronavirus229E', 'CoronavirusOC43', 'Inf A H1N1 2009', 'Bordetella pertussis', 'Metapneumovirus', 'Parainfluenza 2', 'Influenza B, rapid test', 'Influenza A, rapid test', 'Strepto A', 'Fio2 (venous blood gas analysis)','Myeloblasts', 'Urine - Esterase', 'Urine - Hemoglobin', 'Urine - Bile pigments', 'Urine - Ketone Bodies', 'Urine - Protein', 'Urine - Crystals', 'Urine - Hyaline cylinders', 'Urine - Granular cylinders', 'Urine - Yeasts', 'Urine - Color']].fillna(-1)","6cdb935f":"# filling all the other missing values with 99\n\ndf_task1 = df_task1.fillna(99)","9c404876":"# let's see if there is still any missing values left\n\nnull_count = df_task1.isnull().sum().sort_values(ascending=False)\nnull_percentage = null_count \/ len(df_task1)\nnull_rank = pd.DataFrame(data=[null_count, null_percentage],index=['null_count', 'null_ratio']).T\nnull_rank","99d41a15":"# let's now see the description of the dataframe again, because i am pretty sure we will have to apply some sort of normalization technique on it\n\ndf_task1.describe()","fefc752c":"# let's see visually how our variables are behaving\n\n#df_task1.hist(bins = 50, figsize=(40,40))\n#plt.show()","1805334e":"# creating a scaler and using it, disconsidering the target column\n\nscaler = MinMaxScaler()\nexam = pd.DataFrame(df_task1['SARS-Cov-2 exam result'], columns = ['SARS-Cov-2 exam result'])\ndf_scaled = pd.DataFrame(scaler.fit_transform(df_task1.drop('SARS-Cov-2 exam result', axis = 1)), columns = (df_task1.drop('SARS-Cov-2 exam result', axis = 1).columns))","ca0f3ecc":"# concatenating all the columns again\n\ndf_total = pd.concat([exam, df_scaled], axis = 1)","0351ab4a":"# checking if the concatening worked\n\ndf_total.head()","f1c89f69":"# doing a correlation rank to see how the exams work with the result of the exam\n\ndf_total.corr()['SARS-Cov-2 exam result'].sort_values(ascending=False)","d4b989e7":"# Let's remove all special characters and spaces from the column names\n# We will also make them lowercase\n\ndf_total.columns=df_total.columns.str.replace(r'\\(|\\)|:|,|;|\\.|\u2019|\u201d|\u201c|\\?|%|>|<|(|)|\\\\','')\ndf_total.columns=df_total.columns.str.replace(r'\/','')\ndf_total.columns=df_total.columns.str.replace(' ','')\ndf_total.columns=df_total.columns.str.replace('\"','')\ndf_total.columns=df_total.columns.str.replace('-','')\ndf_total.columns=df_total.columns.str.lower()","9b1c0cb5":"# let's get a list of all the columns so we can start working on our model\n\nlist(df_total.columns)","ce5a0f20":"# renaming the columns\n\ndf_total = df_total.rename(columns={\"Meancorpuscularhemoglobinconcentration\\xa0MCHC\": \"Meancorpuscularhemoglobinconcentrationxa0MCHC\", \"Gammaglutamyltransferase\\xa0\": \"Gammaglutamyltransferasexa0\", \"Ionizedcalcium\\xa0\": \"Ionizedcalciumxa0\", \"Creatinephosphokinase\\xa0CPK\\xa0\" : \"Creatinephosphokinasexa0CPKxa0\", 'rods#': 'rods'})","8a2f6796":"# Creating X and y\n\nX = df_total.drop('sarscov2examresult', axis = 1)\ny = df_total['sarscov2examresult']","8e7f6a40":"# let's split the X and y into a test and train set\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=.3, random_state = 42)","c804213e":"def construct_nn(ann_input, ann_output):\n    n_hidden = 7\n\n    # Initialize random weights between each layer\n    init_1 = np.random.randn(X.shape[1], n_hidden).astype(floatX)\n    init_2 = np.random.randn(n_hidden, n_hidden).astype(floatX)\n    init_out = np.random.randn(n_hidden).astype(floatX)\n\n    with pm.Model() as neural_network:\n        ann_input = pm.Data('ann_input', X_train)\n        ann_output = pm.Data('ann_output', Y_train)\n\n        # Weights from input to hidden layer\n        weights_in_1 = pm.Normal('w_in_1', 0, sigma=1,\n                                 shape=(X.shape[1], n_hidden),\n                                 testval=init_1)\n\n        # Weights from 1st to 2nd layer\n        weights_1_2 = pm.Normal('w_1_2', 0, sigma=1,\n                                shape=(n_hidden, n_hidden),\n                                testval=init_2)\n\n        # Weights from hidden layer to output\n        weights_2_out = pm.Normal('w_2_out', 0, sigma=1,\n                                  shape=(n_hidden,),\n                                  testval=init_out)\n\n        # Build neural-network using tanh activation function\n        act_1 = pm.math.tanh(pm.math.dot(ann_input,\n                                         weights_in_1))\n        act_2 = pm.math.tanh(pm.math.dot(act_1,\n                                         weights_1_2))\n        act_out = pm.math.sigmoid(pm.math.dot(act_2,\n                                              weights_2_out))\n\n        # Binary classification -> Bernoulli likelihood\n        out = pm.Bernoulli('out',\n                           act_out,\n                           observed=ann_output,\n                           total_size=Y_train.shape[0]\n                          )\n    return neural_network\n\n# using the model on the train data\nneural_network = construct_nn(X_train, Y_train)","60af0e97":"# set the package-level random number generator\nset_tt_rng(MRG_RandomStreams(42))","26bbe8c0":"%%time\n\n#  ADVI variational inference algorithm\nwith neural_network:\n    inference = pm.ADVI()\n    approx = pm.fit(n=30000, method=inference)","56f78d2a":"# lets predict on the hold-out set using a posterior predictive check (PPC)\n\ntrace = approx.sample(draws=5000)","e36f66dd":"# We can get predicted probability from model\nneural_network.out.distribution.p","44717f4c":"# create symbolic input\nx = T.matrix('X')\nn = T.iscalar('n')\nx.tag.test_value = np.empty_like(X_train[:10])\nn.tag.test_value = 100\n_sample_proba = approx.sample_node(neural_network.out.distribution.p,\n                                   size=n,\n                                   more_replacements={neural_network['ann_input']: x})\n# It is time to compile the function\n# No updates are needed for Approximation random generator\n# Efficient vectorized form of sampling is used\nsample_proba = theano.function([x, n], _sample_proba)\n\n# Create bechmark functions\ndef production_step1():\n    pm.set_data(new_data={'ann_input': X_test, 'ann_output': Y_test}, model=neural_network)\n    ppc = pm.sample_posterior_predictive(trace, samples=500, progressbar=False, model=neural_network)\n\n    # Use probability of > 0.5 to assume prediction of class 1\n    pred = ppc['out'].mean(axis=0) > 0.5\n\ndef production_step2():\n    sample_proba(X_test, 500).mean(0) > 0.5","404660dc":"%timeit production_step1()\n# checking the performance of the first function","77d5fdc9":"%timeit production_step2()\n# checking the performance of the second function","c7dda8b4":"# let's create the prediction\n\npred = sample_proba(X_test, 500).mean(0) > 0.5","4692a3bb":"# let's see the accuracy of our model\n\nprint('Accuracy = {}%'.format((Y_test == pred).mean() * 100))","d1a1b776":"# Confusion Matrix\n\ndef plot_confusion_matrix(y_test, y_pred):\n    cm = confusion_matrix(y_test, y_pred)\n    df = pd.DataFrame(cm.T, index=[\"has disease\", \"no disease\"], columns=[\"has disease\", \"no disease\"])\n    ax = sns.heatmap(df, annot=True)\n    ax.set_xlabel(\"Predicted label\")\n    ax.set_ylabel(\"True label\")\n    return ax\n\nplot_confusion_matrix(Y_test, pred)","a51588e5":"# Creating the Bayesian Neural Network\nWe will now work on the neural network. I am using the documentation of the PyMC3 as a guide.","42581fc2":"Those are ok being int, so let's procceed with our analysis. First let's work with our missing data. Remember that we can't really use a mean, a mode or anything of the sorts, because not having asked for an exam is also an indication of the result of the final exam.","68c1fe89":"Since the task 1 only talks about the exam result, we can remove the ward the patient ended up in.","3d2ac25b":"For some reason I can't remove the backslash, so I will remove those by hand.","fd8b061e":"Whith this we can see that the model brings out a very high accuracy, I believe, it is over 90%. However, the confusion matrix tells us that there is a lot of false negatives. I would have to work on the model to work on that. For now this is my submission.","becd36c2":"<h1>Background<\/h1>\nThe World Health Organization (WHO) characterized the COVID-19, caused by the SARS-CoV-2, as a pandemic on March 11, while the exponential increase in the number of cases was risking to overwhelm health systems around the world with a demand for ICU beds far above the existing capacity, with regions of Italy being prominent examples.<br>\n\nBrazil recorded the first case of SARS-CoV-2 on February 26, and the virus transmission evolved from imported cases only, to local and finally community transmission very rapidly, with the federal government declaring nationwide community transmission on March 20.\n<br>\n\nUntil March 27, the state of S\u00e3o Paulo had recorded 1,223 confirmed cases of COVID-19, with 68 related deaths, while the county of S\u00e3o Paulo, with a population of approximately 12 million people and where Hospital Israelita Albert Einstein is located, had 477 confirmed cases and 30 associated death, as of March 23. Both the state and the county of S\u00e3o Paulo decided to establish quarantine and social distancing measures, that will be enforced at least until early April, in an effort to slow the virus spread.\n<br>\n\nOne of the motivations for this challenge is the fact that in the context of an overwhelmed health system with the possible limitation to perform tests for the detection of SARS-CoV-2, testing every case would be impractical and tests results could be delayed even if only a target subpopulation would be tested.\n<br><br>\n<h1>Dataset<\/h1>\nThis dataset contains anonymized data from patients seen at the Hospital Israelita Albert Einstein, at S\u00e3o Paulo, Brazil, and who had samples collected to perform the SARS-CoV-2 RT-PCR and additional laboratory tests during a visit to the hospital.\n<br>\nAll data were anonymized following the best international practices and recommendations. All clinical data were standardized to have a mean of zero and a unit standard deviation.\n<br><br>\n<h1>Task Details<\/h1>\n* <br>\n**TASK 1**<br>\n* Predict confirmed COVID-19 cases among suspected cases.\nBased on the results of laboratory tests commonly collected for a suspected COVID-19 case during a visit to the emergency room, would it be possible to predict the test result for SARS-Cov-2 (positive\/negative)?\n<br>\n**TASK 2**<br>\n* Predict admission to general ward, semi-intensive unit or intensive care unit among confirmed COVID-19 cases.\nBased on the results of laboratory tests commonly collected among confirmed COVID-19 cases during a visit to the emergency room, would it be possible to predict which patients will need to be admitted to a general ward, semi-intensive unit or intensive care unit?\n<br><br>\n<h1>Expected Submission<\/h1>\nSubmit a notebook that implements the full lifecycle of data preparation, model creation and evaluation. Feel free to use this dataset plus any other data you have available. Since this is not a formal competition, you're not submitting a single submission file, but rather your whole approach to building a model.\n<br><br>\n<h1>Evaluation<\/h1>\nThis is not a formal competition, so we won't measure the results strictly against a given validation set using a strict metric. Rather, what we'd like to see is a well-defined process to build a model that can deliver decent results (evaluated by yourself).\n<br>\nOur team will be looking at:\n<br>\n**Model Performance** - How well does the model perform on the real data? Can it be generalized over time? Can it be applied to other scenarios? Was it overfit?\n<br>\n**Data Preparation** - How well was the data analysed prior to feeding it into the model? Are there any useful visualisations? Does the reader learn any new techniques through this submission? A great entry will be informative, thought provoking, and fresh all at the same time.\n<br>\n**Documentation** - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.","c70823d6":"We can exclude the columns that have absolutely no values in it, which are the 'Mycoplasma pneumoniae','Urine - Nitrite', 'Urine - Sugar', 'Partial thromboplastin time (PTT)', 'Prothrombin time (PT), Activity' and 'D-Dimer'.","6e1967df":"I will replace the values to make them numerical. I will do so by hand, so I can check all the exam results, and if the number transformation makes sense. This is only possbible because there isn't many categorical variables. In order to do that I will check all the unique values and then do an unique replacement dictionary for the columns. I will also drop the patient ID.","82903c1c":"# Bayesian Neural Network\n\nAfter a first analysis of the dataset, checking that it deals with many different exams, I thought it would be best to go with a bayesian model, for it would take all the inputs into consideration. For that, I will use the PyMC3 library, which makes the entire proccess very easy.","72c42f72":"Since there is a lot of different values, I will fill the as follows:\n* With 0: 'Urine - Leukocytes', 'Urine - pH'\n* With -1: 'Patient age quantile', 'SARS-Cov-2 exam result', 'Respiratory Syncytial Virus', 'Influenza A', 'Influenza B', 'Parainfluenza 1', 'CoronavirusNL63', 'Rhinovirus\/Enterovirus', 'Coronavirus HKU1', 'Parainfluenza 3', 'Chlamydophila pneumoniae', 'Adenovirus', 'Parainfluenza 4', 'Coronavirus229E', 'CoronavirusOC43', 'Inf A H1N1 2009', 'Bordetella pertussis', 'Metapneumovirus', 'Parainfluenza 2', 'Influenza B, rapid test', 'Influenza A, rapid test', 'Strepto A', 'Fio2 (venous blood gas analysis)','Myeloblasts', 'Urine - Esterase', 'Urine - Hemoglobin', 'Urine - Bile pigments', 'Urine - Ketone Bodies', 'Urine - Protein', 'Urine - Crystals', 'Urine - Hyaline cylinders', 'Urine - Granular cylinders', 'Urine - Yeasts', 'Urine - Color'\n* With 99: everything else","41804135":"Let's just check if the values inside the columns that are type int 64 actually make sense being int64 (discrete) and not continuous.","259d5a15":"Let's scale all the variables to have a more normalized input. We will use the MinMaxScaler because by default it classifies all the inputs between 0-1, and it will scale it close to the target, because the exam results also come into 0-1.","ed65c285":"# Data Preparation and EDA\nBefore we can start to model anything, we have to treat all the data.","053fdec4":"Since there are only negative and positive outcomes, we can exchange those for 0 and 1"}}