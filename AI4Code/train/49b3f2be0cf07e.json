{"cell_type":{"a7bdcac5":"code","8267c368":"code","914e462a":"code","12cddedb":"code","39223108":"code","80513667":"code","597ff164":"code","e75a4d4e":"code","cc3daed4":"code","29569f96":"code","d7fcaf7d":"code","6242fca4":"code","fe93d252":"code","422aa764":"code","85ef1e87":"markdown","932e205f":"markdown","31026dc8":"markdown"},"source":{"a7bdcac5":"from collections import Counter\n\nimport h5py\nimport tensorflow as tf\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import ResNet50V2\n%matplotlib inline","8267c368":"base_path = os.path.join('..', 'input', 'food41')\ntrain_h5_path = os.path.join(base_path, 'food_c101_n10099_r64x64x3.h5')\ntest_h5_path = os.path.join(base_path, 'food_test_c101_n1000_r64x64x3.h5')\nprint(train_h5_path)\nprint(test_h5_path)","914e462a":"f_train = h5py.File(train_h5_path, 'r')\nprint(list(f_train.keys()))\nf_test = h5py.File(test_h5_path, 'r')\nprint(list(f_test.keys()))","12cddedb":"X = np.array(f_train.get('images'))\ny = np.array(f_train.get('category'))\ny_labels = np.array([raw_category.decode() for raw_category in f_train.get('category_names')])\nX_test = np.array(f_test.get('images'))\ny_test = np.array(f_test.get('category'))\ny_test_labels = np.array([raw_category.decode() for raw_category in f_test.get('category_names')])\nprint('Train\/dev shapes. X: {0} y: {1}'.format(X.shape, y.shape))\nprint('Test shapes. X: {0} y: {1}'.format(X_test.shape, y_test.shape))\nprint(np.unique(y_labels))","39223108":"sample_images = 25\ntotal_images = X.shape[0]\nread_idxs = slice(0,sample_images)\nimage_data = X[read_idxs]\nimage_label = y[read_idxs]\nfig, m_ax = plt.subplots(5, 5, figsize = (12, 12))\nfor c_ax, c_label, c_img in zip(m_ax.flatten(), image_label, image_data):\n    c_ax.imshow(c_img if c_img.shape[2]==3 else c_img[:,:,0], cmap = 'gray')\n    c_ax.axis('off')\n    c_ax.set_title(y_labels[np.argmax(c_label)])","80513667":"quantities = dict(sorted(Counter([y_labels[i][0] for i in y]).items(), key=lambda kv: kv[1]))","597ff164":"X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.2)","e75a4d4e":"batch_size = 128\nepochs = 32","cc3daed4":"train_image_generator = ImageDataGenerator(rescale=1.\/255,\n                    rotation_range=45,\n                    width_shift_range=.15,\n                    height_shift_range=.15,\n                    horizontal_flip=True,\n                    zoom_range=0.5)\n# Generator for our training data\ndev_image_generator = ImageDataGenerator(rescale=1.\/255) # Generator for our validation data","29569f96":"train_data_gen = train_image_generator.fit(X_train)\ndev_data_gen = dev_image_generator.fit(X_dev)","d7fcaf7d":"print(X_train.shape[1:])\nprint(y_train.shape[1])","6242fca4":"model = Sequential()\nmodel.add(Conv2D(16, (3,3), padding='same', activation='relu', input_shape=X_train.shape[1:]))\nmodel.add(MaxPooling2D())\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(32, (3,3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D())\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(64, (3,3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D())\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(128, (3,3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D())\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(y_train.shape[1], activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nmodel.summary()","fe93d252":"history = model.fit_generator(\n    train_image_generator.flow(X_train,y_train, batch_size=batch_size),\n    steps_per_epoch=X_train.shape[0] \/\/ batch_size,\n    epochs=epochs,\n    validation_data=dev_image_generator.flow(X_dev, y_dev, batch_size=batch_size),\n    validation_steps=X_dev.shape[0] \/\/ batch_size\n)\n","422aa764":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","85ef1e87":"The dataset have 101 different foods, so to plot isn't intersting. I want to know if the quantity of the different classes is similar.","932e205f":"## Images Vizualization","31026dc8":"# Convolutional Neural Net for Food Classification"}}