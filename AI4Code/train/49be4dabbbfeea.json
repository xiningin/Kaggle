{"cell_type":{"d8435ba8":"code","2c9a599f":"code","4a558c2f":"code","78a1a7e7":"code","21a7970b":"code","1829b0b0":"code","cf27f782":"code","ce0fa500":"code","8011c8ab":"code","6aa9c866":"code","f7545272":"code","da0a4926":"code","db6497be":"code","bd05957c":"code","17577318":"code","2fe6bed7":"code","3ce84eda":"code","61a4a675":"code","73d52872":"code","f98fead8":"code","a8599eb9":"code","b4e9c695":"code","57fdf9d2":"code","756d78f6":"code","4ab919cc":"code","28b3f6fb":"code","e91c3575":"code","5a774a72":"code","4dbb44a8":"code","b5340537":"markdown","e4b01842":"markdown","0265599a":"markdown","6f899ff3":"markdown","1d170d08":"markdown","0d79fa72":"markdown","89a725ba":"markdown","be86c616":"markdown","f131865e":"markdown","ca462d0b":"markdown","2dc24346":"markdown","efc12914":"markdown","ee7fb4c4":"markdown","fc7b4591":"markdown","fcef8d05":"markdown","dad65659":"markdown","d37562b6":"markdown","daf4433e":"markdown","cd7eb77b":"markdown","58c47fe4":"markdown","701ba946":"markdown","7cdeac90":"markdown","f1c0a1e2":"markdown","569bb403":"markdown","eee643bd":"markdown","40f5d1ac":"markdown","6ed05d8a":"markdown","2a2ee35a":"markdown","c96cbe80":"markdown","6c5cd64b":"markdown","a43fe61c":"markdown","a9649771":"markdown","26c3ca3e":"markdown","d3474c4f":"markdown","5bae1667":"markdown","c1283f8b":"markdown"},"source":{"d8435ba8":"import pandas as pd\nimport numpy as np\nimport gc\n\ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/test.csv\")\n","2c9a599f":"def cat_column_info(column):\n    num_categories = train[column].nunique()\n    print(\"------> {} <------\".format(column))\n    print(\"--: train - type {}\".format(train[column].dtype))\n    print(\"--: test  - type {}\".format(test[column].dtype))\n    print(\"--: train - # categories {}\".format(train[column].nunique()))\n    print(\"--: test  - # categories {}\".format(test[column].nunique()))\n    if num_categories < 10:\n        if train[column].dtype == \"int64\":\n            print(\"--: train - values {}\".format(np.sort(train[column].unique())))\n            print(\"--: test  - values {}\".format(np.sort(test[column].unique())))\n        else:\n            print(\"--: train - values {}\".format(train[column].unique()))\n            print(\"--: test  - values {}\".format(test[column].unique()))\n    print(\"--: train - NaN count {}\".format(train[column].isnull().values.sum()))\n    print(\"--: test  - NaN count {}\".format(test[column].isnull().values.sum()))\n    print(\"--: train - max value {}\".format(train[column].max()))\n    print(\"--: test  - max value {}\".format(test[column].max()))\n    print(\"--: train - min value {}\".format(train[column].min()))\n    print(\"--: test  - min value {}\".format(test[column].min()))\n    print(\"\")\n\ndef cont_column_info(column):\n    print(\"------> {} <------\".format(column))\n    print(\"--: train - type {}\".format(train[column].dtype))\n    print(\"--: test  - type {}\".format(test[column].dtype))\n    print(\"--: train - min {}\".format(train[column].min()))\n    print(\"--: test  - min {}\".format(test[column].min()))\n    print(\"--: train - max {}\".format(train[column].max()))\n    print(\"--: test  - max {}\".format(test[column].max()))    \n    print(\"--: train - NaN count {}\".format(train[column].isnull().values.sum()))\n    print(\"--: test  - NaN count {}\".format(test[column].isnull().values.sum()))\n    print(\"\")\n    \nprint(\": Train shape {}\".format(train.shape))\nprint(\": Test shape {}\".format(test.shape))\nprint(\"\")","4a558c2f":"features = [\n    'Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n    'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', \n    'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3',\n    'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5', \n    'Soil_Type6', 'Soil_Type7', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n    'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18', \n    'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n    'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30', \n    'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n    'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']\n\nfor feature in features:\n    cat_column_info(feature)","78a1a7e7":"for feature in features:\n    if feature.startswith(\"Soil_Type\") or feature.startswith(\"Wilderness_Area\"):\n        train[feature] = train[feature].astype(np.int8)\n        test[feature] = test[feature].astype(np.int8)\n    else:\n        train[feature] = train[feature].astype(np.int16)\n        test[feature] = test[feature].astype(np.int16)\n        \ntrain[\"Cover_Type\"] = train[\"Cover_Type\"].astype(np.int8)","21a7970b":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('whitegrid')\nsns_params = {\"palette\": \"bwr_r\"}\n\ncounts = pd.DataFrame(train[\"Cover_Type\"].value_counts())\nax = plt.subplots(figsize=[20, 5])\nax = sns.barplot(x=counts.index, y=counts.Cover_Type, **sns_params)\nfor p in ax.patches:\n    ax.text(x=p.get_x()+(p.get_width()\/2), y=p.get_height(), s=\"{:,d}\".format(round(p.get_height())), ha=\"center\")\n_ = ax.set_title(\"Class Balance\", fontsize=15)\n_ = ax.set_ylabel(\"Number of Records\", fontsize=15)\n_ = ax.set_xlabel(\"Cover_Type\", fontsize=15)\n\ndel(counts)\n_ = gc.collect()","1829b0b0":"# Count the number of null values that occur in each row\ntrain[\"null_count\"] = train.isnull().sum(axis=1)\n\n# Group the null counts\ncounts = train.groupby(\"null_count\")[\"Cover_Type\"].count().to_dict()\nnull_data = {\"{} Null Value(s)\".format(k) : v for k, v in counts.items() if k < 6}\n\n# Plot the null count results\npie, ax = plt.subplots(figsize=[5, 5])\ncolors = sns.color_palette(\"bwr_r\")[0:5]\nplt.pie(x=null_data.values(), autopct=\"%.2f%%\", explode=[0.05]*len(null_data.keys()), labels=null_data.keys(), pctdistance=0.5, colors=colors)\n_ = plt.title(\"Percentage of Null Values Per Row (Train Data)\", fontsize=14)\n\ndel(counts)\ndel(null_data)\n_ = gc.collect()","cf27f782":"# Count the number of null values that occur in each row\ntest[\"null_count\"] = test.isnull().sum(axis=1)\n\n# Group the null counts\ncounts = test.groupby(\"null_count\")[\"null_count\"].count().to_dict()\nnull_data = {\"{} Null Value(s)\".format(k) : v for k, v in counts.items() if k < 6}\n\n# Plot the null count results\npie, ax = plt.subplots(figsize=[5, 5])\nplt.pie(x=null_data.values(), autopct=\"%.2f%%\", explode=[0.05]*len(null_data.keys()), labels=null_data.keys(), pctdistance=0.5, colors=colors)\n_ = plt.title(\"Percentage of Null Values Per Row (Test Data)\", fontsize=14)\n\ndel(counts)\ndel(null_data)\n_ = gc.collect()","ce0fa500":"feature_counts = {}\nfor feature in features:\n    feature_counts[feature] = train[feature].nunique()\n\ncounts = pd.DataFrame.from_dict(feature_counts, orient=\"index\", columns=[\"counts\"])\nax = plt.subplots(figsize=[20, 10])\nax = sns.barplot(x=counts.index, y=counts.counts, **sns_params)\nfor p in ax.patches:\n    ax.text(x=p.get_x()+(p.get_width()\/2), y=p.get_height(), s=\"{:,d}\".format(round(p.get_height())), ha=\"center\")\n_ = ax.set_title(\"Number of Unique Values Per Feature\", fontsize=15)\n_ = ax.set_ylabel(\"Number of Unique Values\", fontsize=15)\n_ = ax.set_xlabel(\"Feature\", fontsize=15)\n\nax.set_xticklabels(\n    ax.get_xticklabels(), \n    rotation=45, \n    horizontalalignment='right',\n)\n\ndel(counts)\n_ = gc.collect()","8011c8ab":"cat_features = [\n    'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4',\n    'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5', 'Soil_Type6', \n    'Soil_Type8', 'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12', 'Soil_Type13', \n    'Soil_Type14', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20', \n    'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26', \n    'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32', \n    'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38', \n    'Soil_Type39', 'Soil_Type40',\n]\nfig, axs = plt.subplots(14, 3, figsize=(4*4, 11*3), squeeze=False, sharey=True)\n\nptr = 0\nfor row in range(14):\n    for col in range(3):  \n        x = train[[cat_features[ptr], \"Cover_Type\"]].value_counts().sort_index().to_frame().rename({0: \"# of Samples\"}, axis=\"columns\").reset_index()\n        g = sns.barplot(x=cat_features[ptr], y=\"# of Samples\", hue=\"Cover_Type\", data=x, ax=axs[row][col])\n        g.legend_.remove()\n        g.axvspan(0.6, 1.6, facecolor='0.1', alpha=0.3)\n        plt.xlabel(cat_features[ptr])\n        ptr += 1\n        del(x)\nhandles, labels = g.get_legend_handles_labels()\nfig.legend(handles, labels, loc='upper right')\nplt.tight_layout()  \nplt.show()\n\n_ = gc.collect()","6aa9c866":"soil_features = [x for x in features if x.startswith(\"Soil_Type\")]\ntrain[\"soil_type_count\"] = train[soil_features].sum(axis=1)\ntest[\"soil_type_count\"] = test[soil_features].sum(axis=1)\n\nx = train[[\"soil_type_count\", \"Cover_Type\"]].value_counts().sort_index().to_frame().rename({0: \"# of Samples\"}, axis=\"columns\").reset_index()\nfig, ax = plt.subplots(figsize=(20, 10))\ng = sns.barplot(x=\"soil_type_count\", y=\"# of Samples\", hue=\"Cover_Type\", data=x)\ng.axvspan(0.6, 1.6, facecolor='0.1', alpha=0.2)\ng.axvspan(2.6, 3.6, facecolor='0.1', alpha=0.2)\ng.axvspan(4.6, 5.6, facecolor='0.1', alpha=0.2)\ng.axvspan(6.6, 7.6, facecolor='0.1', alpha=0.2)\ng.legend_.remove()\nhandles, labels = g.get_legend_handles_labels()\nfig.legend(handles, labels, loc='upper right')\n_ = ax.set_title(\"Sum of Soil_Type Features vs Cover_Type\", fontsize=15)\n_ = ax.set_ylabel(\"# of Samples\", fontsize=15)\n_ = ax.set_xlabel(\"Sum of Soil_Type Binary Features = 1\", fontsize=15)\n\ndel(x)\n_ = gc.collect()","f7545272":"wilderness_features = [x for x in features if x.startswith(\"Wilderness_Area\")]\ntrain[\"wilderness_area_count\"] = train[wilderness_features].sum(axis=1)\ntest[\"wilderness_area_count\"] = test[wilderness_features].sum(axis=1)\n\nx = train[[\"wilderness_area_count\", \"Cover_Type\"]].value_counts().sort_index().to_frame().rename({0: \"# of Samples\"}, axis=\"columns\").reset_index()\nfig, ax = plt.subplots(figsize=(20, 10))\ng = sns.barplot(x=\"wilderness_area_count\", y=\"# of Samples\", hue=\"Cover_Type\", data=x)\ng.axvspan(0.6, 1.6, facecolor='0.1', alpha=0.2)\ng.axvspan(2.6, 3.6, facecolor='0.1', alpha=0.2)\ng.legend_.remove()\nhandles, labels = g.get_legend_handles_labels()\nfig.legend(handles, labels, loc='upper right')\n_ = ax.set_title(\"Sum of Wilderness_Area Features vs Cover_Type\", fontsize=15)\n_ = ax.set_ylabel(\"# of Samples\", fontsize=15)\n_ = ax.set_xlabel(\"Sum of Wilderness_Area Binary Features = 1\", fontsize=15)\n\ndel(x)\n_ = gc.collect()","da0a4926":"columns_to_check = features.copy()\ncolumns_to_check.append(\"Cover_Type\")\ncorrelation_matrix = train[columns_to_check].corr(method=\"spearman\")\n\nfrom matplotlib.colors import SymLogNorm\n\nf, ax = plt.subplots(figsize=(20, 20))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n_ = sns.heatmap(\n    correlation_matrix, \n    mask=np.triu(np.ones_like(correlation_matrix, dtype=bool)), \n    cmap=sns.diverging_palette(230, 20, as_cmap=True), \n    center=0,\n    square=True, \n    linewidths=.1, \n    norm=SymLogNorm(linthresh=0.03, linscale=0.03, vmin=-1.0, vmax=1.0, base=10),\n    cbar=False,\n)\n_ = ax.set_title(\"Spearman Correlation Between Features\", fontsize=15)","db6497be":"min_horiz_distance_to_hydrology = abs(train[\"Horizontal_Distance_To_Hydrology\"].min())\nmin_vert_distance_to_hydrology = abs(train[\"Vertical_Distance_To_Hydrology\"].min())\n\ntrain[\"horiz_distance\"] = train[\"Horizontal_Distance_To_Hydrology\"] + min_horiz_distance_to_hydrology\ntrain[\"vert_distance\"] = train[\"Vertical_Distance_To_Hydrology\"] + min_vert_distance_to_hydrology\n\ntrain[\"horiz_distance\"] = train[\"horiz_distance\"].astype(np.int16)\ntrain[\"vert_distance\"] = train[\"vert_distance\"].astype(np.int16)\n\nmax_horiz_distance_to_hydrology = train[\"horiz_distance\"].max()\nmax_vert_distance_to_hydrology = train[\"vert_distance\"].max()\n\nheatmap_array_1 = np.zeros(shape=(max_vert_distance_to_hydrology+1, max_horiz_distance_to_hydrology+1))\nheatmap_array_2 = np.zeros(shape=(max_vert_distance_to_hydrology+1, max_horiz_distance_to_hydrology+1))\nheatmap_array_3 = np.zeros(shape=(max_vert_distance_to_hydrology+1, max_horiz_distance_to_hydrology+1))\nheatmap_array_4 = np.zeros(shape=(max_vert_distance_to_hydrology+1, max_horiz_distance_to_hydrology+1))\nheatmap_array_5 = np.zeros(shape=(max_vert_distance_to_hydrology+1, max_horiz_distance_to_hydrology+1))\nheatmap_array_6 = np.zeros(shape=(max_vert_distance_to_hydrology+1, max_horiz_distance_to_hydrology+1))\nheatmap_array_7 = np.zeros(shape=(max_vert_distance_to_hydrology+1, max_horiz_distance_to_hydrology+1))\nheatmap_array_8 = np.zeros(shape=(max_vert_distance_to_hydrology+1, max_horiz_distance_to_hydrology+1))\n\nfor index, row in train[[\"vert_distance\", \"horiz_distance\", \"Cover_Type\"]].iterrows():\n    if row[\"Cover_Type\"] == 1:\n        heatmap_array_1[row[\"vert_distance\"], row[\"horiz_distance\"]] += 1\n    if row[\"Cover_Type\"] == 2:\n        heatmap_array_2[row[\"vert_distance\"], row[\"horiz_distance\"]] += 1\n    if row[\"Cover_Type\"] == 3:\n        heatmap_array_3[row[\"vert_distance\"], row[\"horiz_distance\"]] += 1\n    if row[\"Cover_Type\"] == 4:\n        heatmap_array_4[row[\"vert_distance\"], row[\"horiz_distance\"]] += 1\n    if row[\"Cover_Type\"] == 5:\n        heatmap_array_5[row[\"vert_distance\"], row[\"horiz_distance\"]] += 1\n    if row[\"Cover_Type\"] == 6:\n        heatmap_array_6[row[\"vert_distance\"], row[\"horiz_distance\"]] += 1\n    if row[\"Cover_Type\"] == 7:\n        heatmap_array_7[row[\"vert_distance\"], row[\"horiz_distance\"]] += 1","bd05957c":"heatmap_array_1 *= 255.0\/heatmap_array_1.max()\nheatmap_array_2 *= 255.0\/heatmap_array_2.max()\nheatmap_array_3 *= 255.0\/heatmap_array_3.max()\nheatmap_array_4 *= 255.0\/heatmap_array_4.max()\nheatmap_array_5 *= 255.0\/heatmap_array_5.max()\nheatmap_array_6 *= 255.0\/heatmap_array_6.max()\nheatmap_array_7 *= 255.0\/heatmap_array_7.max()\n\nfig, axs = plt.subplots(nrows=4, ncols=2, figsize=(20, 30))\naxs[0, 0].imshow(heatmap_array_1, cmap='hot', interpolation='nearest')\n_ = axs[0, 0].set_title(\"Cover_Type = 1\", fontweight=\"bold\", size=15)\n_ = axs[0, 0].grid(False)\n\naxs[0, 1].imshow(heatmap_array_2, cmap='hot', interpolation='nearest')\n_ = axs[0, 1].set_title(\"Cover_Type = 2\", fontweight=\"bold\", size=15)\n_ = axs[0, 1].grid(False)\n\naxs[1, 0].imshow(heatmap_array_3, cmap='hot', interpolation='nearest')\n_ = axs[1, 0].set_title(\"Cover_Type = 3\", fontweight=\"bold\", size=15)\n_ = axs[1, 0].grid(False)\n\naxs[1, 1].imshow(heatmap_array_4, cmap='hot', interpolation='nearest')\n_ = axs[1, 1].set_title(\"Cover_Type = 4\", fontweight=\"bold\", size=15)\n_ = axs[1, 1].grid(False)\n\naxs[2, 0].imshow(heatmap_array_5, cmap='hot', interpolation='nearest')\n_ = axs[2, 0].set_title(\"Cover_Type = 5\", fontweight=\"bold\", size=15)\n_ = axs[2, 0].grid(False)\n\naxs[2, 1].imshow(heatmap_array_6, cmap='hot', interpolation='nearest')\n_ = axs[2, 1].set_title(\"Cover_Type = 6\", fontweight=\"bold\", size=15)\n_ = axs[2, 1].grid(False)\n\naxs[3, 0].imshow(heatmap_array_7, cmap='hot', interpolation='nearest')\n_ = axs[3, 0].set_title(\"Cover_Type = 7\", fontweight=\"bold\", size=15)\n_ = axs[3, 0].grid(False)\n\naxs[3, 1].imshow(heatmap_array_8, cmap='hot', interpolation='nearest')\n_ = axs[3, 1].set_title(\"\", fontweight=\"bold\", size=15)\n_ = axs[3, 1].grid(False)","17577318":"from scipy.spatial import distance\n\norigin = (0, 0)\n\ndef euclidean_horiz_and_vert_distance(row):\n    return distance.euclidean(origin, (row[\"horiz_distance\"], row[\"vert_distance\"]))\n\ntrain[\"hydrology_euclidean_distance\"] = train.apply(euclidean_horiz_and_vert_distance, axis=1)","2fe6bed7":"fig, ax = plt.subplots(figsize=(20, 15))\nax = sns.violinplot(data=train, y=\"hydrology_euclidean_distance\", x=\"Cover_Type\")\n_ = ax.set_title(\"Hydrology Euclidean Distance vs Cover_Type\", fontsize=15)\n_ = ax.set_ylabel(\"Hydrology - Euclidean Distance\", fontsize=15)\n_ = ax.set_xlabel(\"Cover_Type\", fontsize=15)","3ce84eda":"def manhattan_horiz_and_vert_distance(row):\n    return distance.cityblock(origin, (row[\"horiz_distance\"], row[\"vert_distance\"]))\n\ntrain[\"hydrology_manhattan_distance\"] = train.apply(manhattan_horiz_and_vert_distance, axis=1)","61a4a675":"fig, ax = plt.subplots(figsize=(20, 15))\nax = sns.violinplot(data=train, y=\"hydrology_manhattan_distance\", x=\"Cover_Type\")\n_ = ax.set_title(\"Hydrology Manhattan Distance vs Cover_Type\", fontsize=15)\n_ = ax.set_ylabel(\"Hydrology - Manhattan Distance\", fontsize=15)\n_ = ax.set_xlabel(\"Cover_Type\", fontsize=15)","73d52872":"train[\"shade_delta_1\"] = train[\"Hillshade_9am\"] - train[\"Hillshade_Noon\"]\ntrain[\"shade_delta_2\"] = train[\"Hillshade_Noon\"] - train[\"Hillshade_3pm\"]\ntrain[\"shade_delta_total\"] = train[\"Hillshade_9am\"] - train[\"Hillshade_3pm\"]\n\ntest[\"shade_delta_1\"] = test[\"Hillshade_9am\"] - test[\"Hillshade_Noon\"]\ntest[\"shade_delta_2\"] = test[\"Hillshade_Noon\"] - test[\"Hillshade_3pm\"]\ntest[\"shade_delta_total\"] = test[\"Hillshade_9am\"] - test[\"Hillshade_3pm\"]","f98fead8":"fig, ax = plt.subplots(figsize=(20, 15))\nax = sns.violinplot(data=train, y=\"shade_delta_1\", x=\"Cover_Type\")\n_ = ax.set_title(\"Change in Shade from 9:00am to 12:00pm\", fontsize=15)\n_ = ax.set_ylabel(\"Shade Delta\", fontsize=15)\n_ = ax.set_xlabel(\"Cover_Type\", fontsize=15)","a8599eb9":"fig, ax = plt.subplots(figsize=(20, 15))\nax = sns.violinplot(data=train, y=\"shade_delta_2\", x=\"Cover_Type\")\n_ = ax.set_title(\"Change in Shade from 12:00pm to 3:00pm\", fontsize=15)\n_ = ax.set_ylabel(\"Shade Delta\", fontsize=15)\n_ = ax.set_xlabel(\"Cover_Type\", fontsize=15)","b4e9c695":"fig, ax = plt.subplots(figsize=(20, 15))\nax = sns.violinplot(data=train, y=\"shade_delta_total\", x=\"Cover_Type\")\n_ = ax.set_title(\"Change in Shade from 9:00am to 3:00pm\", fontsize=15)\n_ = ax.set_ylabel(\"Shade Delta\", fontsize=15)\n_ = ax.set_xlabel(\"Cover_Type\", fontsize=15)","57fdf9d2":"fig, ax = plt.subplots(figsize=(20, 15))\nax = sns.violinplot(data=train, y=\"Elevation\", x=\"Cover_Type\")\n_ = ax.set_title(\"Elevation vs Cover_Type\", fontsize=15)\n_ = ax.set_ylabel(\"Elevation\", fontsize=15)\n_ = ax.set_xlabel(\"Cover_Type\", fontsize=15)","756d78f6":"# Drop Cover_Type 5, since we only have one example of it\ntrain = train[(train[\"Cover_Type\"] != 5)]","4ab919cc":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report\nfrom lightgbm import LGBMClassifier\nfrom lightgbm import early_stopping\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\ntarget = train[\"Cover_Type\"]\ncv_rounds = 3\n\nk_fold = StratifiedKFold(\n    n_splits=cv_rounds,\n    random_state=2021,\n    shuffle=True,\n)\n\ntrain_preds = np.zeros(len(train.index), )\ntrain_probas = np.zeros(len(train.index), )\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(train[features], target)):\n    x_train = train[features].iloc[train_index]\n    y_train = target.iloc[train_index]\n\n    x_valid = train[features].iloc[test_index]\n    y_valid = target.iloc[test_index]\n\n    model = LGBMClassifier(\n        random_state=2021,\n        n_estimators=2000,\n        verbose=-1,\n        metric=\"softmax\",\n    )\n    model.fit(\n        x_train,\n        y_train,\n        eval_set=[(x_valid, y_valid)],\n        callbacks=[early_stopping(50)],\n    )\n\n    train_oof_preds = model.predict(x_valid)\n    train_preds[test_index] = train_oof_preds\n    \n    print(\"-- Fold {}:\".format(fold+1))\n    print(\"{}\".format(classification_report(y_valid, train_oof_preds)))\n    print(\"-- Accuracy: {}\".format(accuracy_score(y_valid, train_oof_preds)))\n\nprint(\"-- Overall:\")\nprint(\"{}\".format(classification_report(target, train_preds)))\nprint(\"-- Accuracy: {}\".format(accuracy_score(target, train_preds)))\n\ntrain[\"unmodified_preds\"] = train_preds\n\n# Show the confusion matrix\nconfusion = confusion_matrix(train[\"Cover_Type\"], train[\"unmodified_preds\"])\ncover_labels = [1, 2, 3, 4, 6, 7]\nfig, ax = plt.subplots(figsize=(15, 15))\nax = sns.heatmap(confusion, annot=True, fmt=\",d\", xticklabels=cover_labels, yticklabels=cover_labels)\n_ = ax.set_title(\"Confusion Matrix for LGB Classifier (Unmodified Dataset)\", fontsize=15)\n_ = ax.set_ylabel(\"Actual Class\")\n_ = ax.set_xlabel(\"Predicted Class\")\n\ndel(train_preds)\ndel(confusion)\n_ = gc.collect()","28b3f6fb":"target = train[\"Cover_Type\"]\ncv_rounds = 3\n\nk_fold = StratifiedKFold(\n    n_splits=cv_rounds,\n    random_state=2021,\n    shuffle=True,\n)\n\nfeatures.append(\"wilderness_area_count\")\nfeatures.append(\"soil_type_count\")\n\ntrain_preds = np.zeros(len(train.index), )\ntrain_probas = np.zeros(len(train.index), )\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(train[features], target)):\n    x_train = train[features].iloc[train_index]\n    y_train = target.iloc[train_index]\n\n    x_valid = train[features].iloc[test_index]\n    y_valid = target.iloc[test_index]\n\n    model = LGBMClassifier(\n        random_state=2021,\n        n_estimators=2000,\n        verbose=-1,\n        metric=\"softmax\",\n    )\n    model.fit(\n        x_train,\n        y_train,\n        eval_set=[(x_valid, y_valid)],\n        callbacks=[early_stopping(50)],\n    )\n\n    train_oof_preds = model.predict(x_valid)\n    train_preds[test_index] = train_oof_preds\n    \n    print(\"-- Fold {}:\".format(fold+1))\n    print(\"{}\".format(classification_report(y_valid, train_oof_preds)))\n    print(\"-- Accuracy: {}\".format(accuracy_score(y_valid, train_oof_preds)))\n\nprint(\"-- Overall:\")\nprint(\"{}\".format(classification_report(target, train_preds)))\nprint(\"-- Accuracy: {}\".format(accuracy_score(target, train_preds)))\n\ntrain[\"type_count_preds\"] = train_preds\n\n# Show the confusion matrix\nconfusion = confusion_matrix(train[\"Cover_Type\"], train[\"type_count_preds\"])\ncover_labels = [1, 2, 3, 4, 6, 7]\nfig, ax = plt.subplots(figsize=(15, 15))\nax = sns.heatmap(confusion, annot=True, fmt=\",d\", xticklabels=cover_labels, yticklabels=cover_labels)\n_ = ax.set_title(\"Confusion Matrix for LGB Classifier (Type Counts)\", fontsize=15)\n_ = ax.set_ylabel(\"Actual Class\")\n_ = ax.set_xlabel(\"Predicted Class\")\n\ndel(train_preds)\ndel(confusion)\n_ = gc.collect()\n\nfeatures.remove(\"wilderness_area_count\")\nfeatures.remove(\"soil_type_count\")","e91c3575":"target = train[\"Cover_Type\"]\ncv_rounds = 3\n\nk_fold = StratifiedKFold(\n    n_splits=cv_rounds,\n    random_state=2021,\n    shuffle=True,\n)\n\nfeatures.append(\"shade_delta_1\")\nfeatures.append(\"shade_delta_2\")\nfeatures.append(\"shade_delta_total\")\n\ntrain_preds = np.zeros(len(train.index), )\ntest_preds = np.zeros(len(test.index), )\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(train[features], target)):\n    x_train = train[features].iloc[train_index]\n    y_train = target.iloc[train_index]\n\n    x_valid = train[features].iloc[test_index]\n    y_valid = target.iloc[test_index]\n\n    model = LGBMClassifier(\n        random_state=2021,\n        n_estimators=2000,\n        verbose=-1,\n        metric=\"softmax\",\n    )\n    model.fit(\n        x_train,\n        y_train,\n        eval_set=[(x_valid, y_valid)],\n        callbacks=[early_stopping(50)],\n    )\n\n    train_oof_preds = model.predict(x_valid)\n    train_preds[test_index] = train_oof_preds\n    \n    print(\"-- Fold {}:\".format(fold+1))\n    print(\"{}\".format(classification_report(y_valid, train_oof_preds)))\n    print(\"-- Accuracy: {}\".format(accuracy_score(y_valid, train_oof_preds)))\n\nprint(\"-- Overall:\")\nprint(\"{}\".format(classification_report(target, train_preds)))\nprint(\"-- Accuracy: {}\".format(accuracy_score(target, train_preds)))\n\ntrain[\"shade_delta_preds\"] = train_preds\n\n# Show the confusion matrix\nconfusion = confusion_matrix(train[\"Cover_Type\"], train[\"shade_delta_preds\"])\ncover_labels = [1, 2, 3, 4, 6, 7]\nfig, ax = plt.subplots(figsize=(15, 15))\nax = sns.heatmap(confusion, annot=True, fmt=\",d\", xticklabels=cover_labels, yticklabels=cover_labels)\n_ = ax.set_title(\"Confusion Matrix for LGB Classifier (Shade Delta)\", fontsize=15)\n_ = ax.set_ylabel(\"Actual Class\")\n_ = ax.set_xlabel(\"Predicted Class\")\n\ndel(train_preds)\ndel(confusion)\n_ = gc.collect()\n\nfeatures.remove(\"shade_delta_1\")\nfeatures.remove(\"shade_delta_2\")\nfeatures.remove(\"shade_delta_total\")","5a774a72":"target = train[\"Cover_Type\"]\ncv_rounds = 3\n\nk_fold = StratifiedKFold(\n    n_splits=cv_rounds,\n    random_state=2021,\n    shuffle=True,\n)\n\nfeatures.append(\"hydrology_manhattan_distance\")\n\ntrain_preds = np.zeros(len(train.index), )\ntest_preds = np.zeros(len(test.index), )\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(train[features], target)):\n    x_train = train[features].iloc[train_index]\n    y_train = target.iloc[train_index]\n\n    x_valid = train[features].iloc[test_index]\n    y_valid = target.iloc[test_index]\n\n    model = LGBMClassifier(\n        random_state=2021,\n        n_estimators=2000,\n        verbose=-1,\n        metric=\"softmax\",\n    )\n    model.fit(\n        x_train,\n        y_train,\n        eval_set=[(x_valid, y_valid)],\n        callbacks=[early_stopping(50)],\n    )\n\n    train_oof_preds = model.predict(x_valid)\n    train_preds[test_index] = train_oof_preds\n    \n    print(\"-- Fold {}:\".format(fold+1))\n    print(\"{}\".format(classification_report(y_valid, train_oof_preds)))\n    print(\"-- Accuracy: {}\".format(accuracy_score(y_valid, train_oof_preds)))\n\nprint(\"-- Overall:\")\nprint(\"{}\".format(classification_report(target, train_preds)))\nprint(\"-- Accuracy: {}\".format(accuracy_score(target, train_preds)))\n\ntrain[\"hydrology_manhattan_preds\"] = train_preds\n\n# Show the confusion matrix\nconfusion = confusion_matrix(train[\"Cover_Type\"], train[\"hydrology_manhattan_preds\"])\ncover_labels = [1, 2, 3, 4, 6, 7]\nfig, ax = plt.subplots(figsize=(15, 15))\nax = sns.heatmap(confusion, annot=True, fmt=\",d\", xticklabels=cover_labels, yticklabels=cover_labels)\n_ = ax.set_title(\"Confusion Matrix for LGB Classifier (Hydrology Manhattan Distance)\", fontsize=15)\n_ = ax.set_ylabel(\"Actual Class\")\n_ = ax.set_xlabel(\"Predicted Class\")\n\ndel(train_preds)\ndel(confusion)\n_ = gc.collect()\n\nfeatures.remove(\"hydrology_manhattan_distance\")","4dbb44a8":"bar, ax = plt.subplots(figsize=(20, 10))\nax = sns.barplot(\n    x=[\"Unmodified\", \"Binary Counts\", \"Shade Delta\", \"Hydrology Manhattan Dist\"],\n    y=[\n        float(accuracy_score(target, train[\"unmodified_preds\"])),\n        accuracy_score(target, train[\"type_count_preds\"]),\n        accuracy_score(target, train[\"shade_delta_preds\"]),\n        accuracy_score(target, train[\"hydrology_manhattan_preds\"]),\n    ],\n)\n_ = ax.set_title(\"Accuracy Score Based on Approach\", fontsize=15)\n_ = ax.set_xlabel(\"Approach\")\n_ = ax.set_ylabel(\"Accuracy Score\")\n_ = ax.set(ylim=(0.90, 1.0))\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(\n        x=p.get_x()+(p.get_width()\/2),\n        y=height,\n        s=\"{:.4f}\".format(height),\n        ha=\"center\"\n    )","b5340537":"Adding the counts has given a definite boost to `Cover_Type` `7` accuracy. This in turn impacts our entire accuracy score.","e4b01842":"A few interesting things to point out:\n\n* `Cover_Type` of `1` and `2` are predominant regardless of how many binary features are set to `1`. However, we see that the majority of cases where none are set, we have a `Cover_Type` of `1`, `2`, or `3` as the most likely candidates.\n* When there are between `1` and `3` of the binary features set, we see cover types `5` and `6` begin to appear.\n\nWe would very likely gain some information by creating a column that contains the sum of the `Soil_Type` features.","0265599a":"## 3.5 Comparison of Approaches","6f899ff3":"# More to come...","1d170d08":"## 1.1 Training and Testing Files\n\nOur input data consists of:\n\n* `train.csv` - 523 MB in size, containing 56 columns and 4,000,000 rows\n* `test.csv` - 129 MB in size, containing 55 columns and 1,000,000 rows\n\nThe main observation is that while it will fit in memory, model training may exert pressure on the Kaggle 16 GB CPU memory and GPU memory limitations. We should definitely explore what column formats are at play, and whether running functions to [reduce memory usage](https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage) on Pandas dataframes can ease pressure on memory. \n\nInstead of running the reduce memory usage function, we can dig a little deeper into the feature columns (see output from hidden cell above). We can see that `Soil_Type` features and `Wilderness_Area` features are binary. This means they can be recast to `int8` without problems. The `Cover_Type` feature can also be recast to `int8`. Furthermore, the max and minimum values for all remaining features suggest they can be recast to `int16`. This should reduce dataframe usage, and is simple to implement (see output from hidden cell below).","0d79fa72":"A few observations:\n    \n* Both `Cover_Type` of `1` and `2` look very similar in distribution.\n* As discussed, `Cover_Type` of `4`, `5`, and `6` are difficult to see, simply due to rarity of those classes.\n* When comparing `Cover_Type` of `7` to `1`, we can see that `7` is more spread out vertically, and horizontally. This suggests that separation between `1` & `2` and `7` is possible to some degree.\n\nIn order to make use of this relationship however, we need to transform it a little into a single continuous feature. We'll calculate the Euclidean distance of each point from the origin of (0, 0) as well as the Manhattan distance. Then we'll look to see if we can separate them a little more.","89a725ba":"The change in shade looks similar across various `Cover_Type` classes. Although in the case of `Cover_Type` `4`, there is a shorter tail to the extreme ends of the shade change spectrum. ","be86c616":"General observation is that the `Cover_Type` of `4` has a very different hydrology distribution than the other types. This again may help us separate out that particular class. Let's look at the Manhattan distance as well.","f131865e":"Similar to the `Soil_Type` feature sums, the `Wilderness_Area` sum shows that we have `Cover_Type` values of `6` and `7` showing when there is at least 1 `Wilderness_Area` set. ","ca462d0b":"With this competition, we're not seeing any missing values. This means we don't have to worry about imputing or creating new features based on null values.","2dc24346":"A few observations:\n    \n* As expected, `Soil_Type7` and `Soil_Type15` both appear in white. Since they have a single value, they do not have any correlation with any other variable.\n* The `Elevation` feature appears to be strongly correlated to `Cover_Type`.\n* Other correlations to `Cover_Type` include:\n   * `Horizontal_Distance_To_Roadways`\n   * `Horizontal_Distance_To_Fire_Points`\n   * `Wilderness_Area1`\n   * `Wilderness_Area2`","efc12914":"## 2.4 Feature Value Counts\n\nLooking at the column `dtypes`, it appears that every column is an `int64`. This would suggest we may have categorical features. Let's take a look at the counts of each feature to see if we have anything interesting.","ee7fb4c4":"`Cover_Type` `2` is the most highly represented, followed by `Cover_Type` `1`. Rare `Cover_Type` values include `4`, `5` and `6`. With only a single example of `5`, we may be able to safely ignore it, or discover standout features that make it easy to identify, however it will likely have minimal impact on the classifier. More likely, `Cover_Type` `1` and `2` will be easy to identify, and the majority of the competition will focus on the other rare types.","fc7b4591":"# 3 Simple Models\n\nGiven we know a little about the distribution of data, we should establish a set of baseline models to understand what kind of performance we can get from models.","fcef8d05":"Here we can see quite clearly that most soil types with a value of `0` will have a corresponding `Cover_Type` of `1` or `2`. This isn't unexpected, given that the class balance is very tipped towards those two types of covers. In other words, we don't see any magic binary feature, but we weren't really expecting to see one. Perhaps looking at second order properites will give us more information.","dad65659":"## 2.7 `Wilderness_Area` Features\n\nLet's look a little closer at some of the `Wilderness_Area` features. More specifically, let's look at what happens when 1 or more of them are set.","d37562b6":"## 3.3 Shade Delta\n\nOur hypothesis above was that the delta for the shade measurements may impact `Cover_Type` `4`. Let's add in all the delta computations.","daf4433e":"## 2.5 Binary Column Exploration\n\nLet's take a look to see if there is any affinity for binary values to be associated with any particular class.","cd7eb77b":"A few interesting observations here:\n    \n* The following may all be continuous in nature, given that there are many different discrete values:\n  * `Elevation`\n  * `Aspect`\n  * `Slope`\n  * `Horizontal_Distance_To_Hydrology`\n  * `Vertical_Distance_To_Hydrology`\n  * `Horizontal_Distance_To_Roadways`\n  * `Hillshade_9am`\n  * `Hillshade_Noon`\n  * `Horizontal_Distance_To_Fire_Points`\n* The following may be binary in nature, given there are only 2 discrete values:\n  * `Wilderness_Area1`\n  * `Wilderness_Area2`\n  * `Wilderness_Area3`\n  * `Wilderness_Area4`\n  * `Soil_Type1`\n  * `Soil_Type2`\n  * `Soil_Type3`\n  * `Soil_Type4`\n  * `Soil_Type5`\n  * `Soil_Type6`\n  * `Soil_Type8`\n  * `Soil_Type9`\n  * `Soil_Type10`\n  * `Soil_Type11`\n  * `Soil_Type12`\n  * `Soil_Type13`\n  * `Soil_Type14`\n  * `Soil_Type16`\n  * `Soil_Type17`\n  * `Soil_Type18`\n  * `Soil_Type19`\n  * `Soil_Type20`\n  * `Soil_Type21`\n  * `Soil_Type22`\n  * `Soil_Type23`\n  * `Soil_Type24`\n  * `Soil_Type25`\n  * `Soil_Type26`\n  * `Soil_Type27`\n  * `Soil_Type28`\n  * `Soil_Type29`\n  * `Soil_Type30`\n  * `Soil_Type31`\n  * `Soil_Type32`\n  * `Soil_Type33`\n  * `Soil_Type34`\n  * `Soil_Type35`\n  * `Soil_Type36`\n  * `Soil_Type37`\n  * `Soil_Type38`\n  * `Soil_Type39`\n  * `Soil_Type40`\n* The following columns can be dropped altogether as they only ever have a single value, and no nulls are present:\n  * `Soil_Type7`\n  * `Soil_Type15`","58c47fe4":"## 3.2 `Wilderness_Area` and `Soil_Type` Counts\n\nLet's add type counts for the binary features as described above to see if there is any lift to our model.","701ba946":"## 2.8 Spearman Correlation\n\nWe should also check to see what variables are correlated to one another. We'll check the Spearman correlation first, since it does not make assumptions about distribution types or linearity. With Spearman correlation, we have values that range from -1 to +1. Values around either extreme end mean a neagative or positive correlation, while those around 0 mean no correlation exists. In the case of the heatmap below, any values near 0 will appear white, while negative or positive correlations will appear darker blue or darker red accordingly.","7cdeac90":"## 2.11 `Elevation`\n\nGiven that the corrleation plot showed that `Elevation` is highly correlated with the `Cover_Type`, we should look to see what the relationship looks like. ","f1c0a1e2":"## 2.3 Null Values\n\nWe should also check to see if we are missing any values in the columns.","569bb403":"As expected, we do really well with `Cover_Type` classes of `1`, `2`, and `3`.","eee643bd":"## 3.4 Manhattan Distance of Hydrology Features","40f5d1ac":"Similar to the first shade changes we saw, in this case, we see the same lack of tail for `Cover_Type` `4`. ","6ed05d8a":"## 3.1 LightGBM\n\nWe'll start with a simple LightGBM model and see how our features work out from there.","2a2ee35a":"Looks like Manhattan distance may give slightly better separation to `Cover_Type` of `4`.","c96cbe80":"## 2.6 `Soil_Type` Features\n\nLet's look a little closer at some of the `Soil_Type` features. More specifically, let's look at what happens when 1 or more of them are set.","6c5cd64b":"Again, we see a small boost to our overall accuracy, this time in `Cover_Type` `4`. ","a43fe61c":"# 1 Introduction\n\nThis EDA explores the data available for the Tabular Playground Series - December 2021 competition. Simple data exploration is performed, as well as preliminary modeling.","a9649771":"## 2.10 Shade Delta\n\nOne interesting feature is the `Hillshade` profile. There are 3 times of day that the shade is given:\n\n* 9:00 am\n* 12:00 pm\n* 3:00 pm\n\nWhile the shade itself may not be informative, we should look at the change in the amount of shade over the course of the day. ","26c3ca3e":"From this we can clearly see there is good separation between `Cover_Type` classes based purely on `Elevation`.  ","d3474c4f":"# 2 Features\n\n## 2.1 `id` Column\n\nThe `id` column is a `int64` integer column that contains unique record indicators ranging from 0 to 3,999,999. Like most Tabular Series, this is simply an identifier for the record and is likely not going to be of use for modelling purposes.\n\n## 2.2 `Cover_Type` Column\n\nThe `Cover_Type` column contains the class targets we are attempting to predict. This is a multi-class classification problem. We should look first to see what class breakdown we have.","5bae1667":"## 2.9 Hydrology Map\n\nSome features may provide `Cover_Type` separation when combined. Of these, the `Distance_To_Hydrology` provides an intuitive 2D mapping for us to examine the separation of `Cover_Type` values.","c1283f8b":"Overall, `Cover_Type` `4` presents a slightly different shade change distribution when compared to the shade changes in the other classes. This may provide a small amount of signal to our classifier. "}}