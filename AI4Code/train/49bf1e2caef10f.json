{"cell_type":{"6a049b52":"code","70611c4f":"code","52c4f153":"code","415b72d9":"code","1680d289":"code","ac9f61c3":"code","0ddc601f":"code","c91f08c6":"code","65468c50":"code","e818683b":"code","31e1a8b1":"code","64e1aab7":"code","b0f5b2db":"code","55ab4149":"code","39562af0":"code","c6bf29a5":"code","c66c6939":"code","2bd244b4":"code","43b2e48a":"code","9fb8b58b":"code","bf86686b":"code","9b5a55f6":"code","539d6985":"code","1c770123":"code","8ad836ec":"code","d2c075cb":"code","9cce0523":"code","3310c172":"code","5c3b0127":"code","cd1a1319":"code","ff276c62":"markdown","64bc3206":"markdown","416db0c3":"markdown","b15b95dc":"markdown","cd089ee0":"markdown","ffc97efd":"markdown","0c596583":"markdown","80ab8705":"markdown","df977572":"markdown","1bacd8be":"markdown","a517ae27":"markdown","85cbdcb3":"markdown","d39f2e58":"markdown","5630b81b":"markdown","c7d5f5ac":"markdown","13c3a2a5":"markdown","cdd02a39":"markdown","807a85a7":"markdown","4d39bb1f":"markdown","c6c59eea":"markdown","abf5f07a":"markdown","aa9fc095":"markdown"},"source":{"6a049b52":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","70611c4f":"import json\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.model_selection import GroupKFold","52c4f153":"df_train = pd.read_csv(filepath_or_buffer=\"..\/input\/train.csv\",dtype={'date': str, 'fullVisitorId': str, 'sessionId':str}, nrows=None)\ndf_test = pd.read_csv(filepath_or_buffer=\"..\/input\/test.csv\",dtype={'date': str, 'fullVisitorId': str, 'sessionId':str}, nrows=None)","415b72d9":"df_train = df_train.drop([\"date\", \"sessionId\",\"socialEngagementType\", \"visitId\"],axis = 1)\ndf_test = df_test.drop([\"date\", \"sessionId\",\"socialEngagementType\", \"visitId\"],axis = 1)","1680d289":"df_train.shape, df_test.shape","ac9f61c3":"totals_columns = [\"transactionRevenue\", \"newVisits\", \"bounces\", \"pageviews\", \"hits\"]\ntmp_totals_df = pd.DataFrame(df_train.totals.apply(json.loads).tolist())[totals_columns]\ndf_train = pd.concat([df_train,tmp_totals_df] , axis=1)\n\ntotals_columns.remove(\"transactionRevenue\")\ntmp_totals_df = pd.DataFrame(df_test.totals.apply(json.loads).tolist())[totals_columns]\ndf_test = pd.concat([df_test,tmp_totals_df],axis = 1 )","0ddc601f":"df_train[\"transactionRevenue\"] = df_train.transactionRevenue.fillna(0.0)","c91f08c6":"df_train = df_train.drop([\"totals\"],axis=1)\ndf_test = df_test.drop([\"totals\"],axis=1)\ndel tmp_totals_df\ndf_train.shape, df_test.shape","65468c50":"geo_columns = [\"continent\",\"subContinent\",\"country\", \"city\",\"region\", \"metro\" , \"networkDomain\"]\ntmp_geo_df = pd.DataFrame(df_train.geoNetwork.apply(json.loads).tolist())[geo_columns]\ndf_train = pd.concat([df_train, tmp_geo_df],axis=1)\n\ntmp_geo_df = pd.DataFrame(df_test.geoNetwork.apply(json.loads).tolist())[geo_columns]\ndf_test = pd.concat([df_test, tmp_geo_df],axis=1)","e818683b":"del tmp_geo_df\ndf_train = df_train.drop([\"geoNetwork\"],axis=1)\ndf_test = df_test.drop([\"geoNetwork\"],axis=1)\ndf_train.shape, df_test.shape","31e1a8b1":"devices_columns = [\"browser\",\"deviceCategory\",\"isMobile\",\"operatingSystem\"]\ntmp_device_df = pd.DataFrame(df_train.device.apply(json.loads).tolist())[devices_columns]\ndf_train=pd.concat([df_train,tmp_device_df],axis = 1)\n\ntmp_device_df = pd.DataFrame(df_test.device.apply(json.loads).tolist())[devices_columns]\ndf_test=pd.concat([df_test,tmp_device_df],axis = 1)","64e1aab7":"del tmp_device_df \ndf_train = df_train.drop([\"device\"], axis = 1)\ndf_test = df_test.drop([\"device\"], axis = 1)\ndf_train.shape, df_test.shape","b0f5b2db":"trafficSource_columns = [\"campaign\",\"medium\" , \"source\",\"adContent\",\"isTrueDirect\", \"keyword\",\"referralPath\"]\ntmp_traffic_df = pd.DataFrame(df_train.trafficSource.apply(json.loads).tolist())[trafficSource_columns]\ndf_train = pd.concat([df_train,tmp_traffic_df] , axis = 1 )\n\ntmp_traffic_df = pd.DataFrame(df_test.trafficSource.apply(json.loads).tolist())[trafficSource_columns]\ndf_test = pd.concat([df_test,tmp_traffic_df] , axis = 1 )","55ab4149":"del tmp_traffic_df \ndf_train = df_train.drop([\"trafficSource\"] , axis = 1 )\ndf_test = df_test.drop([\"trafficSource\"] , axis = 1 )\ndf_train.shape , df_test.shape","39562af0":"df_train = df_train.fillna(0)\ndf_test = df_test.fillna(0)","c6bf29a5":"for df in [df_train, df_test]:\n    df['date'] = pd.to_datetime(df['visitStartTime'], unit='s')\n    df['day_of_week'] = df['date'].dt.dayofweek\n    df['hour_of_day'] = df['date'].dt.hour\n    df['day_of_month'] = df['date'].dt.day","c66c6939":"df_train = df_train.drop([\"date\",\"visitStartTime\"],axis=1)\ndf_test = df_test.drop([\"date\",\"visitStartTime\"],axis=1)","2bd244b4":"df_train[\"transactionRevenue\"] = df_train.transactionRevenue.astype(np.float)\ny_reg = df_train['transactionRevenue']","43b2e48a":"features_type = [\n    (f,df_train[f].dtype) for f in df_train.columns\n]\nfeatures_type","9fb8b58b":"df_train[\"isMobile\"] = df_train.isMobile.astype('str')\ndf_test[\"isMobile\"] = df_test.isMobile.astype('str')","bf86686b":"df_train.head()","9b5a55f6":"categorical_features = [\n    f for f in df_train.columns if (df_train[f].dtype == 'object') & (f != \"fullVisitorId\")\n]\ncategorical_features","539d6985":"for f in categorical_features:\n    df_train[f], indexer = pd.factorize(df_train[f])\n    df_test[f] = indexer.get_indexer(df_test[f])","1c770123":"df_train.shape, df_test.shape","8ad836ec":"df_train_rf = df_train.copy()\na_class = np.percentile(a=list(df_train_rf[\"transactionRevenue\"]) ,q=98.7),\nb_class = np.percentile(a=list(df_train_rf[\"transactionRevenue\"]) ,q=98.8),\nc_class = np.percentile(a=list(df_train_rf[\"transactionRevenue\"]) ,q=98.9),\nd_class = np.percentile(a=list(df_train_rf[\"transactionRevenue\"]) ,q=99),\ne_class = np.percentile(a=list(df_train_rf[\"transactionRevenue\"]) ,q=99.1),\nf_class = np.percentile(a=list(df_train_rf[\"transactionRevenue\"]) ,q=99.3),\ng_class = np.percentile(a=list(df_train_rf[\"transactionRevenue\"]) ,q=99.5),\nh_class = np.percentile(a=list(df_train_rf[\"transactionRevenue\"]) ,q=99.6),\ni_class = np.percentile(a=list(df_train_rf[\"transactionRevenue\"]) ,q=99.9),\nmax_revenue = np.percentile(a=list(df_train_rf[\"transactionRevenue\"]) ,q=100),\n\n\"\"\"percentiles are: 98.7% percentile: {}, 98.8% percentile: {}, 98.9% percentile: {}, 99.1% percentile: {}, 99.3% percentile: {}, 60% percentile: {},\n99.5% percentile: {}, 99.6% percentile: {}, 99.9% percentile: {}, 100% percentile: {}\"\"\".format(\n    np.percentile(a=a_class ,q=98.7),\n    np.percentile(a=b_class ,q=98.8),\n    np.percentile(a=c_class ,q=98.9),\n    np.percentile(a=d_class ,q=99),\n    np.percentile(a=e_class ,q=99.1),\n    np.percentile(a=f_class ,q=99.3),\n    np.percentile(a=g_class ,q=99.5),\n    np.percentile(a=h_class ,q=99.6),\n    np.percentile(a=i_class ,q=99.9),\n    np.percentile(a=max_revenue ,q=100),\n)\n\ndf_train_rf.loc[(df_train_rf[\"transactionRevenue\"]<a_class) , \"rate\"] = 1\ndf_train_rf.loc[(df_train_rf[\"transactionRevenue\"]<b_class) & (df_train_rf[\"transactionRevenue\"]>=a_class) , \"rate\"] = 2\ndf_train_rf.loc[(df_train_rf[\"transactionRevenue\"]<c_class) & (df_train_rf[\"transactionRevenue\"]>=b_class) , \"rate\"] = 3\ndf_train_rf.loc[(df_train_rf[\"transactionRevenue\"]<d_class) & (df_train_rf[\"transactionRevenue\"]>=c_class) , \"rate\"] = 4\ndf_train_rf.loc[(df_train_rf[\"transactionRevenue\"]<e_class) & (df_train_rf[\"transactionRevenue\"]>=d_class) , \"rate\"] = 5\ndf_train_rf.loc[(df_train_rf[\"transactionRevenue\"]<f_class) & (df_train_rf[\"transactionRevenue\"]>=e_class) , \"rate\"] = 6\ndf_train_rf.loc[(df_train_rf[\"transactionRevenue\"]<g_class) & (df_train_rf[\"transactionRevenue\"]>=f_class) , \"rate\"] = 7\ndf_train_rf.loc[(df_train_rf[\"transactionRevenue\"]<h_class) & (df_train_rf[\"transactionRevenue\"]>=g_class) , \"rate\"] = 8\ndf_train_rf.loc[(df_train_rf[\"transactionRevenue\"]<i_class) & (df_train_rf[\"transactionRevenue\"]>=h_class) , \"rate\"] = 9\ndf_train_rf.loc[(df_train_rf[\"transactionRevenue\"]>=i_class) , \"rate\"] = 10\n\n\ndf_train_rf[\"rate\"].astype(int)\n\ndf_train_rf = df_train_rf.drop([\"transactionRevenue\"],axis=1)","d2c075cb":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100, max_depth=20,random_state=0)\ntrain, test = train_test_split(df_train_rf, test_size=0.2)\nrf.fit(train.loc[:, train.columns != \"rate\"], train[[\"rate\"]])\nfeature_importance_rev = dict(sorted(zip(map(lambda x: round(x, len(train.columns)), rf.feature_importances_), train.columns),\n                                     reverse=True))\nfeature_importance = dict()\nfor i, j in feature_importance_rev.items():\n    feature_importance[j] = i\nfeature_importance\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfeature_importance_df = pd.DataFrame.from_dict(feature_importance,orient=\"index\")\nfeature_importance_df = feature_importance_df.rename(index=str, columns={0: \"importance\"})\nax = feature_importance_df.plot(kind='bar', figsize = (12,8))\nplt.bar(feature_importance_df.index,feature_importance_df[\"importance\"])\nplt.show()","9cce0523":"def get_folds(df=None, n_splits=5):\n    \"\"\"Returns dataframe indices corresponding to Visitors Group KFold\"\"\"\n    # Get sorted unique visitors\n    unique_vis = np.array(sorted(df['fullVisitorId'].unique()))\n\n    # Get folds\n    folds = GroupKFold(n_splits=n_splits)\n    fold_ids = []\n    ids = np.arange(df.shape[0])\n    for trn_vis, val_vis in folds.split(X=unique_vis, y=unique_vis, groups=unique_vis):\n        fold_ids.append(\n            [\n                ids[df['fullVisitorId'].isin(unique_vis[trn_vis])],\n                ids[df['fullVisitorId'].isin(unique_vis[val_vis])]\n            ]\n        )\n    return fold_ids","3310c172":"df_train.fullVisitorId = df_train[\"fullVisitorId\"].astype('str')\nfolds = get_folds(df=df_train, n_splits=5)\ntrain_features = [f for f in df_train.columns if f not in  [\"fullVisitorId\",\"transactionRevenue\"]]\ntrain_features","5c3b0127":"importances = pd.DataFrame()\noof_reg_preds = np.zeros(df_train.shape[0])\nsub_reg_preds = np.zeros(df_test.shape[0])\nfor fold_, (trn_, val_) in enumerate(folds):\n    trn_x, trn_y = df_train[train_features].iloc[trn_], y_reg.iloc[trn_]\n    val_x, val_y = df_train[train_features].iloc[val_], y_reg.iloc[val_]\n    reg = lgb.LGBMRegressor(\n        num_leaves=31,\n        learning_rate=0.03,\n        n_estimators=1000,\n        subsample=.9,\n        colsample_bytree=.9,\n        random_state=1\n    )\n    reg.fit(X=trn_x,y=np.log1p(trn_y.astype(np.float)),\n        eval_set=[(val_x, np.log1p(val_y.astype(np.float)))],\n        early_stopping_rounds=50,\n        verbose=100,\n        eval_metric='rmse'\n    )\n    imp_df = pd.DataFrame()\n    imp_df['feature'] = train_features\n    imp_df['gain'] = reg.booster_.feature_importance(importance_type='gain')\n    \n    imp_df['fold'] = fold_ + 1\n    importances = pd.concat([importances, imp_df], axis=0)\n    \n    oof_reg_preds[val_] = reg.predict(val_x, num_iteration=reg.best_iteration_)\n    oof_reg_preds[oof_reg_preds < 0] = 0\n    _preds = reg.predict(df_test[train_features], num_iteration=reg.best_iteration_)\n    _preds[_preds < 0] = 0\n    sub_reg_preds += np.expm1(_preds) \/ len(folds)\n    \nmean_squared_error(np.log1p(y_reg.astype(np.float)), oof_reg_preds) ** .5","cd1a1319":"df_test['PredictedLogRevenue'] = np.log1p(sub_reg_preds)\npre_submission = df_test[[\"fullVisitorId\" , \"PredictedLogRevenue\"]].groupby(by=[\"fullVisitorId\"]).mean()\npre_submission.to_csv(path_or_buf=\"regression_final.csv\", index=True)","ff276c62":"GC","64bc3206":"PREPARING TARGETS ","416db0c3":"**TOTALS**","b15b95dc":"The data is unbalanced based on the transactionRevenue\nsubsampling and balancing data can improve the accuracy.","cd089ee0":"GC","ffc97efd":"prepration for submission ...","0c596583":"**GEONETWORK**","80ab8705":"**TRAFFIC**","df977572":"Ensembling with 5 subsets ...\nAbsolutely increasing the folds can increase the performance.","1bacd8be":"GC","a517ae27":"LABELING FOR GBM COMPATIBILITY","85cbdcb3":"In this kernel, we extract features, categorize target space to target classess and do a simple feature evaluation using random forest.\nFinally we will do a regression with ensemble of lgb regressions.\nIf you want complete EDA on data, check my another kernel in [link](http:\/\/www.kaggle.com\/smasar\/tutorial-preprocessing-processing-evaluation).\nDuring the kernel development we periodically call garbage collector manually for reducing the memory usage.\nAny comment will be appreciated in advance.","d39f2e58":"GETTING FEATURES TYPES","5630b81b":"GC","c7d5f5ac":"**SYMPLE REGRESSION**\n\nOn the following simple regression ensembling will be deployed based on the features extracted.","13c3a2a5":"FILLING NaN","cdd02a39":"final dataframe before starting regression","807a85a7":"**VISIT_DATE**","4d39bb1f":"**DEVICES**","c6c59eea":"DROPPING EXTRA FEATURES","abf5f07a":"change booleans to categorical (lgb works with categorical features and numerical features only)","aa9fc095":"GC"}}