{"cell_type":{"0bce162d":"code","cc0f6e6a":"code","8f77c3bd":"code","83fb4a37":"code","fa16a4a8":"code","5b3337a5":"code","848d1caa":"code","686c5bd5":"code","faf4d862":"code","5eb3b28f":"code","98e49d18":"code","b83f2958":"code","9b2fa525":"code","05dda5f5":"code","9fc82f90":"code","bc164aec":"code","55184e43":"code","392bfd42":"code","95915da4":"code","7bbbf087":"code","619a2296":"code","47017988":"code","32a92682":"code","01c7c7d5":"code","38ad23c8":"code","7697578c":"code","4dbc1f99":"code","7ccbaa92":"code","be05b6b2":"code","66e528b7":"code","afe83c2b":"code","40038f88":"code","792008c3":"markdown","296ae010":"markdown","340227c3":"markdown","9c751f4d":"markdown","fd9e9390":"markdown"},"source":{"0bce162d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import minmax_scale, StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cc0f6e6a":"pd.set_option('display.max_columns', 100)\npd.set_option('display.width', 1000)","8f77c3bd":"df = pd.read_csv('\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndf.head()","83fb4a37":"df.info()","fa16a4a8":"df.isnull().sum()","5b3337a5":"for i in df.columns:\n    print(f'unique values in column \"{i}\" is \\n {df[i].value_counts()} ')\n    print('----------------------------------------------------------')","848d1caa":"for i in df.columns:\n    print (f'{i} : {df[i].unique()} \\n \"{df[i].dtype}\"')\n    print('-----------------------------------------')","686c5bd5":"df.shape","faf4d862":"df.duplicated().sum()\n","5eb3b28f":"df['TotalCharges'] = pd.to_numeric(df['TotalCharges'] , errors='coerce')\ndf.TotalCharges.dtype","98e49d18":"df.skew()","b83f2958":"df.describe()","9b2fa525":"df.columns","05dda5f5":"df.drop(['customerID'],inplace=True, axis=1)","9fc82f90":"plt.style.use('fivethirtyeight')","bc164aec":"object_col=[]\nnum_col=[]\nfor i in df.columns:\n    if df[i].dtype == 'object':\n        object_col.append(i)\n    else:\n        num_col.append(i)","55184e43":"num_col\nobject_col","392bfd42":"ax = sns.countplot(x=df.Churn)\nplt.show()","95915da4":"for i in ['tenure', 'MonthlyCharges', 'TotalCharges']:\n    plt.figure(figsize=(5,7))\n    sns.boxplot(x=df.Churn, y=df[i], data=df , linewidth=1)\n    plt.show()","7bbbf087":"for i in object_col:\n    plt.figure(figsize=(5,7))\n    sns.countplot(x=df[i], hue=df.Churn, data=df , linewidth=0.3)\n    plt.show()","619a2296":"fig, ax = plt.subplots(1, 4, figsize=(28, 8))\ndf[df.Churn == 'No'][num_col].hist(bins=20 , color= 'blue', alpha=0.5 , ax=ax)\ndf[df.Churn == 'Yes'][num_col].hist(bins=20 , color= 'red', alpha=0.5 , ax=ax)\n","47017988":"sns.pairplot(df, hue='Churn')","32a92682":"object_col","01c7c7d5":"def change_yes_no(x):\n    if x =='Yes' or x == 'Male':\n        return 1\n    elif x == 'No' or x == 'Female' :\n        return 0\n    else:\n        return x\n    \n    \nfor i in object_col:\n    if len(df[i].unique()) == 2:\n        df[i] = df[i].map(change_yes_no)","38ad23c8":"df_dummies = pd.get_dummies(df,drop_first=True )\ndf_dummies","7697578c":"plt.figure(figsize=(28,8))\ndf_dummies.corr()['Churn'].sort_values().plot(kind='bar')","4dbc1f99":"df_dummies.dropna(inplace=True)\ndf_dummies.shape","7ccbaa92":"from sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# first, initialize the classificators\ntree= DecisionTreeClassifier(random_state=42) # using the random state for reproducibility\nforest= RandomForestClassifier(random_state=42)\nknn= KNeighborsClassifier()\nsvm= SVC(random_state=42)\nxboost= XGBClassifier(random_state=42)\n\nX = df_dummies.drop('Churn' , axis=1)\ny = df_dummies['Churn']\n\n\nmodels= [tree, forest, knn, svm, xboost]\n","be05b6b2":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2 , random_state=0)\n\nfor model in models:\n    model.fit(X_train, y_train) # fit the model\n    y_pred= model.predict(X_test) # then predict on the test set\n    accuracy= accuracy_score(y_test, y_pred) # this gives us how often the algorithm predicted correctly\n    clf_report= classification_report(y_test, y_pred) # with the report, we have a bigger picture, with precision and recall for each class\n    print(f\"The accuracy of model {type(model).__name__} is {accuracy:.2f}\")\n    print(clf_report)\n    print(\"\\n\")","66e528b7":"X_scaler = StandardScaler().fit_transform(X) \n\nX_train, X_test, y_train, y_test = train_test_split(X_scaler, y, test_size = 0.2 , random_state=0)\nfor model in models:\n    model.fit(X_train, y_train) # fit the model\n    y_pred= model.predict(X_test) # then predict on the test set\n    accuracy= accuracy_score(y_test, y_pred) # this gives us how often the algorithm predicted correctly\n    clf_report= classification_report(y_test, y_pred) # with the report, we have a bigger picture, with precision and recall for each class\n    print(f\"The accuracy of model {type(model).__name__} is {accuracy:.2f}\")\n    print(clf_report)\n    print(\"\\n\")","afe83c2b":"from imblearn.over_sampling import RandomOverSampler\nros = RandomOverSampler(random_state=0)\nX_resampled, y_resampled = ros.fit_resample(X, y)","40038f88":"X_resampled_scaler = StandardScaler().fit_transform(X_resampled) \n\nX_train, X_test, y_train, y_test = train_test_split(X_resampled_scaler, y_resampled, test_size = 0.2 , random_state=0)\nfor model in models:\n    model.fit(X_train, y_train) # fit the model\n    y_pred= model.predict(X_test) # then predict on the test set\n    accuracy= accuracy_score(y_test, y_pred) # this gives us how often the algorithm predicted correctly\n    clf_report= classification_report(y_test, y_pred) # with the report, we have a bigger picture, with precision and recall for each class\n    print(f\"The accuracy of model {type(model).__name__} is {accuracy:.2f}\")\n    print(clf_report)\n    print(\"\\n\")","792008c3":"## Data is unbalanced so I will use OverSampling","296ae010":"## With StanderScaler","340227c3":"# Prepere data for modeling","9c751f4d":"# EDA","fd9e9390":"# Modeling"}}