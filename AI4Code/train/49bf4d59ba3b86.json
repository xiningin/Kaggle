{"cell_type":{"ef8dd057":"code","4c153dfa":"code","5c57ff2f":"code","1a28b324":"code","12ecd580":"code","c9b894cd":"code","2d0263fd":"code","db6a75e2":"code","7e929851":"code","a55824f2":"code","71e26033":"code","6166efaf":"code","d739ebf5":"code","f7305f9c":"code","74721176":"code","ec1b4aba":"code","a8e8fd2a":"code","b34200aa":"code","0fa48be8":"code","2cb899ef":"code","fe9cb516":"code","04971175":"code","7bb4aff7":"code","9cd41e9b":"code","7086919a":"code","aeed64e1":"code","3febb6a5":"code","e67ab186":"code","6b27e20f":"code","45dea564":"code","22431c24":"code","d4511e35":"code","56476362":"code","37cf5bb2":"code","5fe5bf40":"code","0def4032":"code","479f01f8":"code","41328d73":"code","dee23e08":"code","df1678b4":"code","e825c2e2":"code","181834d8":"code","18ae63f9":"code","109adfb5":"code","83df85f0":"code","551098da":"code","75ae649c":"code","a9180e9e":"code","dcec391a":"markdown","49990a06":"markdown","02a27da5":"markdown","0a92113b":"markdown","f729b1e2":"markdown"},"source":{"ef8dd057":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4c153dfa":"df0 = pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv')\ndf1 = pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/test.csv')","5c57ff2f":"df0.target = df0.target.map({val:i for i, val in enumerate(sorted(df0.target.unique()),1)})","1a28b324":"df0.target[:5]","12ecd580":"import sklearn\nfrom sklearn.manifold import TSNE, Isomap, LocallyLinearEmbedding, SpectralEmbedding, MDS\nfrom sklearn.model_selection import train_test_split, StratifiedKFold","c9b894cd":"sub_train = df0.iloc[:, 1:].sample(10000, random_state = 1)\nsub_target = sub_train.target\nsub_train_only = sub_train.iloc[:,:-1]\nsub_train_only.shape, sub_target.shape","2d0263fd":"from umap import UMAP","db6a75e2":"\numap = UMAP(random_state=1)\ndr = umap.fit_transform(sub_train_only, sub_target)","7e929851":"#fig, ax = plt.subplots(figsize = (10,10))\n#sns.scatterplot(dr[:,0], dr[:,1], hue = sub_target , palette = sns.color_palette()[:4] )","a55824f2":"px.scatter(x=dr[:,0], y=dr[:,1], color = sub_target  )","71e26033":"umap0 = UMAP(random_state=1)\ndr0 = umap0.fit_transform(df0.iloc[:,1:-1], df0.target)","6166efaf":"px.scatter(x=dr0[:,0], y=dr0[:,1], color = df0.target  )","d739ebf5":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n","f7305f9c":"x, y = df0.iloc[:, 1:-1], df0.iloc[:, -1]\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1, stratify = y)\nx_train.shape, x_test.shape, y_train.shape, y_test.shape","74721176":"gbc = GradientBoostingClassifier(n_estimators = 300 , verbose = 1)\n","ec1b4aba":"gbc.fit(x_train, y_train)","a8e8fd2a":"y_pred_gbc = gbc.predict(x_test)","b34200aa":"gbc.score(x_test, y_test)","0fa48be8":"cm = confusion_matrix(y_test, y_pred_gbc, labels = gbc.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = gbc.classes_)\ndisp.plot()","2cb899ef":"print(classification_report(y_test, y_pred_gbc, labels = gbc.classes_))","fe9cb516":"umap2 = UMAP(random_state=1)\nx_train_umap_2 = umap2.fit_transform(x_train, y_train)\nx_test_umap_2 = umap2.transform(x_test)","04971175":"gbc_umap_2 = GradientBoostingClassifier(n_estimators = 300 , verbose = 1)\ngbc_umap_2.fit(x_train_umap_2, y_train)\n","7bb4aff7":"y_pred_gbc_umap2 = gbc_umap_2.predict(x_test_umap_2)","9cd41e9b":"gbc_umap_2.score(x_test_umap_2, y_test)","7086919a":"cm = confusion_matrix(y_test, y_pred_gbc_umap2 )\ndisp = ConfusionMatrixDisplay(cm, display_labels = gbc_umap_2.classes_)\ndisp.plot()","aeed64e1":"print(classification_report(y_test, y_pred_gbc_umap2, labels = gbc_umap_2.classes_))","3febb6a5":"x_train_umap_2.shape","e67ab186":"#df_x_train_umap_2 = pd.DataFrame(x_train_umap_2, columns = ['umap2_1','umap2_2'])\n#df_x_train_umap_2.shape","6b27e20f":"#x_train = x_train.iloc[:,:-2]\nx_train.shape","45dea564":"#x_train_add_umap2 = pd.concat([x_train, df_x_train_umap_2 ], axis=1 ,ignore_index= True) #strange, extra rows???\n#x_train_add_umap2 = pd.concat([x_train, df_x_train_umap_2 ], axis=1 ,ignore_index=False) #strange, extra rows???\n#x_train_add_umap2 = x_train.join(df_x_train_umap_2) #strange result in NAN\nx_train_add_umap2 = x_train.copy()          \nx_train_add_umap2['umap2_1'] = x_train_umap_2[:,0]\nx_train_add_umap2['umap2_2'] = x_train_umap_2[:,1]                           \n                           \n                           ","22431c24":"x_train_add_umap2.shape","d4511e35":"gbc_add_umap_2 = GradientBoostingClassifier(n_estimators = 300 , verbose = 1)\ngbc_add_umap_2.fit(x_train_add_umap2, y_train)\n\n\n\n","56476362":"x_test_add_umap2 = x_test.copy()       \nx_test_add_umap2['umap2_1'] = x_test_umap_2[:,0] \nx_test_add_umap2['umap2_2'] = x_test_umap_2[:,1]    ","37cf5bb2":"y_pred_gbc_add_umap2 = gbc_add_umap_2.predict(x_test_add_umap2)","5fe5bf40":"gbc_add_umap_2.score(x_test_add_umap2, y_test)","0def4032":"cm = confusion_matrix(y_test, y_pred_gbc_add_umap2 )\ndisp = ConfusionMatrixDisplay(cm, display_labels = gbc_add_umap_2.classes_)\ndisp.plot()","479f01f8":"x_train.head()","41328d73":"umap6 = UMAP(n_components = 6, random_state = 1)\nx_train_umap6 = umap6.fit_transform(x_train, y_train)\nx_test_umap6 = umap6.transform(x_test)","dee23e08":"gbc_umap6_only = GradientBoostingClassifier(n_estimators = 300,  verbose = 1)\ngbc_umap6_only.fit(x_train_umap6, y_train)","df1678b4":"y_pred_gbc_umap6_only = gbc_umap6_only.predict(x_test_umap6)","e825c2e2":"disp = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(y_test, y_pred_gbc_umap6_only), display_labels = gbc_umap6_only.classes_)\ndisp.plot()","181834d8":"#x_train_add_umap6 = pd.concat([x_train, pd.DataFrame(x_train_umap6, columns=['umap6_'+str(i) for i in range(1,7)])], axis = 1 , ignore_index = True)\n#x_train_add_umap6.head()\nx_train_add_umap6 = x_train.copy()\nx_test_add_umap6 = x_test.copy()       \nfor i in range(6):\n    yvar = 'umap6_'+str(i+1)\n    x_train_add_umap6[yvar] = x_train_umap6[:,i]\n    x_test_add_umap6[yvar] = x_test_umap6[:,i]\nx_train_add_umap6.head()","18ae63f9":"x_train_add_umap6.shape, x_test_add_umap6.shape","109adfb5":"gbc_add_umap_6 = GradientBoostingClassifier(n_estimators = 300 , verbose = 1)\ngbc_add_umap_6.fit(x_train_add_umap6, y_train)","83df85f0":"y_pred_gbc_umap6 = gbc_add_umap_6.predict(x_test_add_umap6)","551098da":"disp = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(y_test, y_pred_gbc_umap6), display_labels = gbc_add_umap_6.classes_)\ndisp.plot()","75ae649c":"x_train.head()","a9180e9e":"umap_only = {}\nraw_add_umap = {}\nfor n_components in range(3,5):\n    print('%'*30)\n    print(f'Umap n_components = {n_components}')\n    print(f'Only {n_components} Umap components as feature for classification')\n    umap_only[n_components] = UMAP(n_components = n_components, random_state = 1)\n    umap_only['x_train_'+str(n_components)] = umap_only[n_components].fit_transform(x_train, y_train)\n    umap_only['x_test_'+str(n_components)] = umap_only[n_components].transform(x_test)\n    umap_only['gbc_umap'+str(n_components)] = GradientBoostingClassifier(n_estimators = 300, verbose = 1)\n    umap_only['gbc_umap'+str(n_components)].fit(umap_only['x_train_'+str(n_components)], y_train)\n    umap_only['y_pred_gbc_umap'+str(n_components)] = umap_only['gbc_umap'+str(n_components)].predict(umap_only['x_test_'+str(n_components)])\n    #print(umap_only['y_pred_gbc_umap'+str(n_components)].shape, y_test.shape)\n    print(umap_only['gbc_umap'+str(n_components)].score( umap_only['x_test_'+str(n_components)] , y_test))\n    disp = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(y_test, umap_only['y_pred_gbc_umap'+str(n_components)]), display_labels = umap_only['gbc_umap'+str(n_components)].classes_)\n    disp.plot()\n    print(classification_report(y_test, umap_only['y_pred_gbc_umap'+str(n_components)], labels = umap_only['gbc_umap'+str(n_components)].classes_))\n    \n    print('='*20)\n    print(f'Original Feature +  {n_components} Umap components as feature for classification')\n    raw_add_umap['x_train_umap'+str(n_components)] = x_train.copy()\n    raw_add_umap['x_test_umap'+str(n_components)]  = x_test.copy()       \n    for i in range(n_components):\n        yvar = 'umap'+str(n_components)+'_'+str(i+1)\n        raw_add_umap['x_train_umap'+str(n_components)][yvar] = umap_only['x_train_'+str(n_components)][:,i]\n        raw_add_umap['x_test_umap'+str(n_components)][yvar] = umap_only['x_test_'+str(n_components)][:,i]\n    raw_add_umap['gbc_umap'+str(n_components)] = GradientBoostingClassifier(n_estimators = 300, verbose = 1)\n    raw_add_umap['gbc_umap'+str(n_components)].fit(raw_add_umap['x_train_umap'+str(n_components)], y_train)\n    raw_add_umap['y_pred_gbc_umap'+str(n_components)] = raw_add_umap['gbc_umap'+str(n_components)].predict(raw_add_umap['x_test_umap'+str(n_components)])\n    print(raw_add_umap['gbc_umap'+str(n_components)].score( raw_add_umap['x_test_umap'+str(n_components)] , y_test))\n    disp = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(y_test, raw_add_umap['y_pred_gbc_umap'+str(n_components)]), display_labels = raw_add_umap['gbc_umap'+str(n_components)].classes_)\n    disp.plot()\n    \n    print(classification_report(y_test, raw_add_umap['y_pred_gbc_umap'+str(n_components)], labels = raw_add_umap['gbc_umap'+str(n_components)].classes_))\n    ","dcec391a":"## test component of 3 and 4","49990a06":"## Note\n\nIt seems due to class imbalance, GBC always predict class_2 which gives overall good result. GBC on UMAP 2 dimension alone, shows underfit. GBC with raw data + UMAP 2 as extra feature does not give good result. Considering UMAP for reduced dimension of 6 instead of 2. ","02a27da5":"## UMAP data clustering as additional feature\n\nExperiment it as addtional features or just using that clustering as feature for classificaiton. \nExperiment UMAP with different number of dimension output . \n\nShould try KNN classification with clustering results as input. ","0a92113b":"## Manifold Learning \/ Dimension Reduction","f729b1e2":"Umap6 alone is rubbish. Try add this to raw data and see. "}}