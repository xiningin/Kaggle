{"cell_type":{"9f7514e3":"code","7ac5c95c":"code","1a2b21b0":"code","1ddab88e":"code","9d19409a":"code","986f19ac":"code","3b9dffae":"code","cdc11958":"code","cdeb891c":"code","46653639":"code","d62e070c":"code","ee35d94a":"code","71801da9":"code","85a28954":"code","4283ad3f":"code","c61ca641":"markdown","daa3a2cf":"markdown","520f5bd3":"markdown","90e899eb":"markdown","e8ca93e8":"markdown","2042f833":"markdown","5fa8d388":"markdown"},"source":{"9f7514e3":"import pandas as pd\nimport numpy as np\n\ntrue, fake = pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/True.csv\"), pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/Fake.csv\")\ntrue.head()","7ac5c95c":"fake['target'] = 0 \ntrue['target'] = 1","1a2b21b0":"fake","1ddab88e":"news = pd.concat([true,fake],axis=0,ignore_index=True)\nnews = news.sample(frac = 1).reset_index(drop = True)\nnews.head()","9d19409a":"import tensorflow as tf\nimport transformers\nimport tqdm\nfrom keras.preprocessing import sequence\n\n#creating a function\ndef func_tokenizer(tokenizer_name, docs):\n    features = []\n    for doc in tqdm.tqdm(docs, desc = 'converting documents to features'):\n        tokens = tokenizer_name.tokenize(doc)\n        ids = tokenizer_name.convert_tokens_to_ids(tokens)\n        features.append(ids)\n    return features\nprint(\"The function is created successfully\")","986f19ac":"from sklearn.model_selection import train_test_split\nX, y = news['text'], news['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","3b9dffae":"#Initialize bert tokenizer\nroberta_tokenizer = transformers.RobertaTokenizer.from_pretrained('roberta-base-openai-detector')","cdc11958":"roberta_train_features = func_tokenizer(roberta_tokenizer, X_train)\nroberta_test_features = func_tokenizer(roberta_tokenizer, X_test)","cdeb891c":"roberta_trg = sequence.pad_sequences(roberta_train_features, maxlen = 500)\nroberta_test = sequence.pad_sequences(roberta_test_features, maxlen = 500)","46653639":"import warnings\nwarnings.filterwarnings(action='ignore', category=UserWarning)\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier(n_estimators = 1000, learning_rate = 0.15, max_depth = 9,\n                    eval_metric = 'auc', use_label_encoder=False,objective = 'binary:logistic')\nxgb.fit(roberta_trg, y_train)","d62e070c":"from sklearn.metrics import accuracy_score, roc_auc_score\n\nxgb_pred = xgb.predict(roberta_test)\nxgb_score = accuracy_score(y_test, xgb_pred)\nxgb_roc = roc_auc_score(y_test, xgb_pred)\nprint(\"The accuracy of XGBOOST is: %0.2f\" %xgb_score)\nprint(\"The roc_auc score of XGBOOST is: %0.2f\" %xgb_roc)","ee35d94a":"from sklearn.metrics import classification_report\ncr = classification_report(y_test, xgb_pred)\nprint(cr)","71801da9":"from catboost import CatBoostClassifier\ncb = CatBoostClassifier(eval_metric = 'Accuracy', iterations = 2000, learning_rate = 0.2)","85a28954":"cb.fit(roberta_trg, y_train, verbose = 0)\ncb_pred = cb.predict(roberta_test)\ncb_score = accuracy_score(y_test, cb_pred)\ncb_roc = roc_auc_score(y_test, cb_pred)\nprint(\"The accuracy of CatBoost is: %0.2f\" %cb_score)\nprint(\"The roc_auc score of CatBoost is: %0.2f\" %cb_roc)","4283ad3f":"from sklearn.metrics import classification_report\ncr2 = classification_report(y_test, cb_pred)\nprint(cr2)","c61ca641":"# CatBoost","daa3a2cf":"# All hail Roberta from hugging face library of transformers \ud83e\udd17","520f5bd3":"![](https:\/\/www.txstate.edu\/cache78a0c25d34508c9d84822109499dee61\/imagehandler\/scaler\/gato-docs.its.txstate.edu\/jcr:21b3e33f-31c9-4273-aeb0-5b5886f8bcc4\/fake-fact.jpg?mode=fit&width=1600)","90e899eb":"## We will be using RoBERTa algorithm from Hugging Face library of transformers for NLP. Transformers are cutting edge technology now in NLP.","e8ca93e8":"# Defining the tokenizer function","2042f833":"# Upvote if you like it or fork it. This helps us motivate to produce more notebooks for the community.","5fa8d388":"# XGBoost"}}