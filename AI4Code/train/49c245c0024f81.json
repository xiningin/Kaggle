{"cell_type":{"d4a249ff":"code","e9264540":"code","a960b9e6":"code","939aed0f":"code","bf3af122":"code","aa9ed1a8":"code","1b30e470":"code","2a5ad158":"code","fe249916":"code","034a48eb":"code","2122fab3":"code","e8f4e247":"code","d27482b2":"code","a58d6aa9":"code","e9dc0e30":"code","34df9719":"code","1edb6b2a":"code","28f57a7d":"code","ffee1e8d":"code","ec27e0b0":"code","db2655e9":"code","eb8edd93":"code","6e7abc89":"code","425449b4":"code","a1db2558":"code","f6118cd3":"code","9984b8d4":"code","2ab49b26":"code","c5ba11a2":"code","5d8824e0":"code","0499743c":"code","5f35a43c":"code","3cd174b3":"code","f37e334f":"code","7fdf21f7":"markdown","868efe69":"markdown","706c95a3":"markdown","b28d7197":"markdown","2f2a0934":"markdown","c8398e7a":"markdown","1981329b":"markdown","1566e1f1":"markdown","f2bb8799":"markdown","78704d34":"markdown","024dd887":"markdown","d02eb514":"markdown","2311eec0":"markdown","a5d1c168":"markdown","3160b679":"markdown","6eadaa77":"markdown","11475884":"markdown","4442743c":"markdown","dc976d10":"markdown","6f27fd47":"markdown","fc2458c4":"markdown","38aa31b4":"markdown","716e18df":"markdown","b0fb76be":"markdown","b376b89b":"markdown","84b847ae":"markdown","013d4356":"markdown","dbba38f8":"markdown","f4cfd710":"markdown","5788045d":"markdown","8a0679c1":"markdown","23503d56":"markdown","b94206a7":"markdown","ce53caa7":"markdown","6b5e2484":"markdown","d835bd9c":"markdown","385cfd7e":"markdown","e86c4f87":"markdown","967b2ea6":"markdown","57fe0c1f":"markdown"},"source":{"d4a249ff":"import numpy             as np\nimport pandas            as pd\nimport matplotlib.pyplot as plt\nimport seaborn           as sns\nimport plotly.graph_objs as go\nimport plotly.express    as px \n\n!pip install millify\nfrom millify     import millify\nfrom scipy.stats import norm\nfrom wordcloud   import WordCloud, STOPWORDS\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","e9264540":"df=pd.read_csv(\"..\/input\/covid19-tweets\/covid19_tweets.csv\")\ndf.head()","a960b9e6":"print(\"There are {} rows and {} columns in the dataset.\".format(df.shape[0],df.shape[1]))","939aed0f":"df.info()","bf3af122":"missing_values = pd.DataFrame()\nmissing_values['column'] = df.columns\n\nmissing_values['percent'] = [round(100* df[col].isnull().sum() \/ len(df), 2) for col in df.columns]\nmissing_values = missing_values.sort_values('percent')\nmissing_values = missing_values[missing_values['percent']>0]","aa9ed1a8":"plt.figure(figsize=(15, 5))\nsns.set(style='whitegrid', color_codes=True)\nsplot=sns.barplot(x='column', y='percent', data=missing_values)\nfor p in splot.patches:\n    splot.annotate(format(p.get_height(), '.2f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), ha = 'center',\n                   va = 'center', xytext = (0, 9), textcoords = 'offset points')\nplt.xlabel(\"Column_Name\", size=14, weight=\"bold\")\nplt.ylabel(\"Percentage\", size=14, weight=\"bold\")\nplt.title(\"Percentage of missing values in column\",fontweight=\"bold\",size=17)\nplt.show()","1b30e470":"plt.figure(figsize=(17, 5))\nsns.heatmap(df.isnull(), cbar=True, yticklabels=False)\nplt.xlabel(\"Column_Name\", size=14, weight=\"bold\")\nplt.title(\"Places of missing values in column\",fontweight=\"bold\",size=17)\nplt.show()","2a5ad158":"unique_df = pd.DataFrame()\nunique_df['Features'] = df.columns\nunique=[]\nfor i in df.columns:\n    unique.append(df[i].nunique())\nunique_df['Uniques'] = unique\n\nf, ax = plt.subplots(1,1, figsize=(15,7))\n\nsplot = sns.barplot(x=unique_df['Features'], y=unique_df['Uniques'], alpha=0.8)\nfor p in splot.patches:\n    splot.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), ha = 'center',\n                   va = 'center', xytext = (0, 9), textcoords = 'offset points')\nplt.title('Bar plot for number of unique values in each column',weight='bold', size=15)\nplt.ylabel('#Unique values', size=12, weight='bold')\nplt.xlabel('Features', size=12, weight='bold')\nplt.xticks(rotation=90)\nplt.show()","fe249916":"df_username_count = df['user_name'].value_counts().reset_index().rename(columns={\n    'user_name':'tweet_count','index':'user_name'})\n\nplt.figure(figsize=(15, 17))\nsns.barplot(y='user_name',x='tweet_count',data=df_username_count.head(50))\ny=df_username_count['tweet_count'].head(50)\nfor index, value in enumerate(y):\n    plt.text(value, index, str(value),fontsize=12)\nplt.title('Top50 users by number of tweets',weight='bold', size=15)\nplt.ylabel('User_name', size=12, weight='bold')\nplt.xlabel('Tweet_count', size=12, weight='bold')\nplt.show()","034a48eb":"fig = px.treemap(df_username_count.head(50), path=['user_name'], values='tweet_count',\n                title=\"<b>TreeMap for Top50 users by number of tweets<\/b>\") \n\nfig.show()","2122fab3":"df = pd.merge(df, df_username_count, on='user_name')","e8f4e247":"data = df.sort_values('user_followers',ascending=False).drop_duplicates(subset='user_name', keep=\"first\")\ndata = data[['user_name', 'user_followers', 'tweet_count']]\ndata.sort_values('user_followers',ascending=False)\n\ndata1 = data.head(50).reset_index().copy()\nfor i in range(50):\n    data1['user_followers'][i] = millify(data1['user_followers'][i],precision=2)\n    \ndata1['user_followers'] = data1['user_followers'].str[:-1].astype(float) # To remove 'M'\n\nplt.figure(figsize=(15, 17))\nsns.barplot(y='user_name',x='user_followers',data=data1.head(50))\ny=data1['user_followers'].head(50)\nfor index, value in enumerate(y):\n    plt.text(value, index, str(value),fontsize=12)\nplt.title('Top50 users by number of followers',weight='bold', size=15)\nplt.ylabel('User_name', size=12, weight='bold')\nplt.xlabel('Followers_count( in Millions )', size=12, weight='bold')\nplt.show()","d27482b2":"fig = px.treemap(data1.head(50), path=['user_name'], values='user_followers',\n                title=\"<b>TreeMap for Top50 users by number of followers<\/b>\",\n                 color='tweet_count') \nfig.show()","a58d6aa9":"data = df.sort_values('user_friends',ascending=False).drop_duplicates(subset='user_name', keep=\"first\")\ndata = data[['user_name', 'user_friends', 'tweet_count']]\ndata.sort_values('user_friends',ascending=False)\n\ndata1 = data.head(50).reset_index().copy()\nfor i in range(50):\n    data1['user_friends'][i] = millify(data1['user_friends'][i],precision=2)\n    \ndata1['user_friends'] = data1['user_friends'].str[:-1].astype(float) # To remove 'k'\n\nplt.figure(figsize=(15, 17))\nsns.barplot(y='user_name',x='user_friends',data=data1.head(50))\ny=data1['user_friends'].head(50)\nfor index, value in enumerate(y):\n    plt.text(value, index, str(value),fontsize=12)\nplt.title('Top50 users by number of friends',weight='bold', size=15)\nplt.ylabel('User_name', size=12, weight='bold')\nplt.xlabel('Friends_count( in Thousand )', size=12, weight='bold')\nplt.show()","e9dc0e30":"fig = px.treemap(data1.head(50), path=['user_name'], values='user_friends',\n                 title=\"<b>TreeMap for Top50 users by number of friends<\/b>\",\n                 color='tweet_count')\n\nfig.show()","34df9719":"df['user_created'] = pd.to_datetime(df['user_created'])\ndf['year_created'] = df['user_created'].dt.year\n\ndata2 = df['year_created'].value_counts().reset_index().rename(columns = {\n    'year_created' : 'count', 'index' : 'year_added'}).sort_values('year_added')[1:] # Since twitter was started from 2006\ndata2['percent'] = data2['count'].apply(lambda x : 100*x\/sum(data2['count']))\n\nt1 = go.Scatter(x=data2['year_added'], y=data2[\"count\"], marker=dict(color=\"#a678de\"))\n\nlayout = go.Layout(title=\"<b>Users added over the years<\/b>\", legend=dict(x=0.1, y=1.1, orientation=\"h\"))\nfig = go.Figure(t1, layout=layout)\nfig.update_layout(xaxis_title=\"Year\", yaxis_title=\"Users\",title_x=0.5)\nfig.show()","1edb6b2a":"def plot_frequency_charts(df, feature, title, pallete):\n    freq_df = pd.DataFrame()\n    freq_df[feature] = df[feature]\n    \n    f, ax = plt.subplots(1,1, figsize=(16,4))\n    total = float(len(df))\n    g = sns.countplot(df[feature], order = df[feature].value_counts().index[:20], palette=pallete)\n    g.set_title(\"Number and percentage of {}\".format(title))\n\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height\/total),\n                ha=\"center\") \n\n    plt.title('Frequency of {} tweeting about Corona'.format(feature),weight='bold', size=15)\n    plt.ylabel('Frequency', size=12, weight='bold')\n    plt.xlabel(title, size=12, weight='bold')\n    plt.xticks(rotation=90)\n    plt.show()","28f57a7d":"plot_frequency_charts(df, 'user_name', 'User Names','inferno')","ffee1e8d":"plot_frequency_charts(df, 'user_location', 'User Locations', 'winter')","ec27e0b0":"plot_frequency_charts(df, 'source','Source', 'brg')","db2655e9":"loc_analysis = pd.DataFrame(df['user_location'].value_counts().sort_values(ascending=False))\nloc_analysis = loc_analysis.rename(columns={'user_location':'count'})","eb8edd93":"data = {\n   \"values\": loc_analysis['count'][:15],\n   \"labels\": loc_analysis.index[:15],\n   \"domain\": {\"column\": 0},\n   \"name\": \"Location Name\",\n   \"hoverinfo\":\"label+percent+name\",\n   \"hole\": .4,\n   \"type\": \"pie\"\n}\nlayout = go.Layout(title=\"<b>Ratio on Location<\/b>\", legend=dict(x=0.1, y=1.1, orientation=\"h\"))\n\ndata = [data]\nfig = go.Figure(data = data, layout = layout)\nfig.update_layout(title_x=0.5)\nfig.show()","6e7abc89":"df1 = df.copy()\ndf1 = df1[df1['hashtags'].notna()] # to get non null values \ndf1['hashtag_count'] = df1['hashtags'].apply(lambda x: len(x.split(',')))","425449b4":"plt.figure(figsize=(15,7))\nsns.distplot(df1['hashtag_count'],fit=norm,kde=True, color='red')\nplt.title('Distplot with Normal distribution for hashtag_count',fontweight=\"bold\",size=15)\nplt.show()","a1db2558":"df['datedt'] = pd.to_datetime(df['date'])\n\ndf['year'] = df['datedt'].dt.year\ndf['month'] = df['datedt'].dt.month\ndf['day'] = df['datedt'].dt.day\ndf['day_name'] = df['datedt'].dt.day_name()\ndf['hour'] = df['datedt'].dt.hour\ndf['minute'] = df['datedt'].dt.minute\ndf['dayofyear'] = df['datedt'].dt.dayofyear\ndf['date_only'] = df['datedt'].dt.date","f6118cd3":"def countplot(feature, title, df, size=1, ordered=True):\n    f, ax = plt.subplots(1,1, figsize=(15,4))\n    total = float(len(df))\n    if ordered:\n        g = sns.countplot(df[feature], order = df[feature].value_counts().index[:20], palette='Set3')\n    else:\n        g = sns.countplot(df[feature], palette='Set3')\n    g.set_title(\"Number and percentage of {}\".format(title),weight='bold', size=17)\n    if(size > 2):\n        plt.xticks(rotation=90, size=10)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height\/total),\n                ha=\"center\") \n    plt.xlabel(feature, size=12, weight='bold')\n    plt.ylabel('Count', size=12, weight='bold')\n    plt.show() ","9984b8d4":"countplot(\"day_name\", \"tweets \/ day of week\", df, size=3, ordered=False)","2ab49b26":"countplot(\"day\", \"tweets \/ day of month\", df, size=3, ordered=False)","c5ba11a2":"countplot(\"hour\", \"tweets \/ hour\", df,size=4, ordered=False)","5d8824e0":"df = df.fillna('None')\n\ndef wordcloud(string,title,color):\n    wc = WordCloud(background_color=color, width=1200,height=600,mask=None,random_state=1,\n                   max_font_size=200,stopwords=stop_words,collocations=False).generate(string)\n    fig=plt.figure(figsize=(20,8))\n    plt.axis('off')\n    plt.title('--- WordCloud for {} --- '.format(title),weight='bold', size=30)\n    plt.imshow(wc)","0499743c":"stop_words=set(STOPWORDS)\nsource_string = \" \".join(df['source'].astype('str'))\nhastage_string = \" \".join(df['hashtags'].astype('str'))\nlocation_string = \" \".join(df['user_location'].astype('str'))","5f35a43c":"wordcloud(source_string,'source','black')","3cd174b3":"wordcloud(hastage_string,'Hashtag','grey')","f37e334f":"wordcloud(location_string,'location','white')","7fdf21f7":"## Happy Learning!!","868efe69":"**HOW IT SPREADS**\n- The virus that causes COVID-19 is **mainly transmitted** through droplets generated when an **infected person coughs, sneezes, or exhales**. These droplets are too heavy to hang in the air, and quickly fall on floors or surfaces.\n- You can be **infected by breathing in the virus** if you are within close proximity of someone who has COVID-19, or **by touching a contaminated surface and then your eyes, nose or mouth.**","706c95a3":"#  Top50 users by number of followers - TreeMap:","b28d7197":"- The above plot shows top15 locations and the ration of tweets from the location ","2f2a0934":"## Frequency of source:","c8398e7a":"# Top50 users by tweet_count - TreeMap:","1981329b":"# Background on COVID-19:","1566e1f1":"![MzY0MTIwMAa.jpeg](attachment:MzY0MTIwMAa.jpeg)","f2bb8799":"# Importing important libraries:","78704d34":"- The above plot gives us top 20 locations from where the tweets have been tweeted\n- It also shows us the ration of contributions of tweets by these locations from all the tweest in the dataset","024dd887":"## Frequency of user_name:","d02eb514":"# Top15 ratio on locations:","2311eec0":"# Reading the data and printing first five entries:","a5d1c168":"# Missing Values:","3160b679":"#  Top50 users by number of followers - Barplot:","6eadaa77":"# $$ \\color{blue}{EDA, Visualization\\ and\\ Wordcloud} $$ \n","11475884":"- The above heatmap shows where the missing values are located w.r.t. the other columns","4442743c":"# Heatmap for missing values:","dc976d10":"- From the above plot we can say that almost **48% of tweets** are **tweeted on weekends**","6f27fd47":"# Top50 users by number of friends - Barplot:","fc2458c4":"**Coronavirus disease (COVID-19)** is an **infectious disease** caused by a **newly discovered coronavirus**.\nMost people who fall sick with COVID-19 will experience mild to moderate symptoms and recover without special treatment.","38aa31b4":"## Frequency of user_location:","716e18df":"- Number of tweets tweeted are more on **22nd**, **25th**, and **30th** of a month\n- Number of tweets tweeted are less on **7th**, **10th**, and **24th** of a month\n- Remaining days the number of tweets are relatively same","b0fb76be":"# Top50 users by tweet_count - Barplot:","b376b89b":"- The above plot gives us top 20 source from which tweets have been sent\n- It also shows us the ration of contributions of tweets by these source from all the tweest in the dataset","84b847ae":"# Users created per year:","013d4356":"# Dtypes of features:","dbba38f8":"- In the dataset we could find that one column had the values less than 2006\n- We know that Twitter was created in the year 2006\n- So, we have to select records from 2006","f4cfd710":"# Frequency of tweets on Corona - Top20:","5788045d":"- The above plot gives us top 20 users who hav tweeted the most number of times\n- It also shows us the ration of contributions of tweets by these users from all the tweest in the dataset","8a0679c1":"# Analysis on time of tweets:","23503d56":"- The above barplot shows us that there are only **four columns** with missing values \n- Percentage of missing values are also shown in the plot","b94206a7":"- Followers_count are in **Millions**","ce53caa7":"# Word Cloud:","6b5e2484":"# Top50 users by number of friends - TreeMap:","d835bd9c":"# Unique value in each feature:","385cfd7e":"- Friends_count are in **Thousands**","e86c4f87":"# Distribution for hashtag_count:","967b2ea6":"# Number of rows and columns in the dataset:","57fe0c1f":"- From the above graph we can see that there are two sepearte clusters of tweets "}}