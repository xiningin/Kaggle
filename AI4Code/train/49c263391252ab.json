{"cell_type":{"a2935f2c":"code","d17afb82":"code","86405836":"code","484afce7":"code","bbd71661":"code","2b20522e":"code","d885cc29":"code","21419044":"code","00ae8f1e":"code","fd8a0c5a":"markdown","e5555c22":"markdown","71a287a5":"markdown","22952a49":"markdown","448655ab":"markdown","e0590c24":"markdown"},"source":{"a2935f2c":"import pandas as pd\nimport re\nfrom nltk.util import ngrams\nfrom collections import Counter\nimport csv\nimport matplotlib.pyplot as plt; plt.rcdefaults()\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nimport nltk","d17afb82":"def cleanReviews(documents):\n    cleanedReviews = []\n    for document in documents:\n        s = re.sub(r'[^a-zA-Z0-9\\s]', '', document)\n        s = re.sub('\\s+',' ', s)\n        s = str(s).lower()\n        tokens = [token for token in s.split(\" \") if token != \"\"]\n        tokens = [word for word in tokens if word not in stopwords.words('english')]\n        tokens = [word for word in tokens if word not in ['alexa', 'echo', 'dot']]\n        review = ' '.join(tokens)\n        cleanedReviews.append(review)\n    return(cleanedReviews)","86405836":"def documentNgrams(documents, size):\n    ngrams_all = []\n    for document in documents:\n        tokens = document.split()\n        if len(tokens) <= size:\n            continue\n        else:\n            output = list(ngrams(tokens, size))\n        for ngram in output:\n            ngrams_all.append(\" \".join(ngram))\n    cnt_ngram = Counter()\n    for word in ngrams_all:\n        cnt_ngram[word] += 1\n    df = pd.DataFrame.from_dict(cnt_ngram, orient='index').reset_index()\n    df = df.rename(columns={'index':'words', 0:'count'})\n    df = df.sort_values(by='count', ascending=False)\n    df = df.head(15)\n    df = df.sort_values(by='count')\n    return(df)","484afce7":"def plotNgrams(documents):\n    unigrams = documentNgrams(documents, 1)\n    bigrams = documentNgrams(documents, 2)\n    trigrams = documentNgrams(documents, 3)\n    \n    # Set plot figure size\n    fig = plt.figure(figsize = (20, 7))\n    plt.subplots_adjust(wspace=.5)\n\n    ax = fig.add_subplot(131)\n    ax.barh(np.arange(len(unigrams['words'])), unigrams['count'], align='center', alpha=.5)\n    ax.set_title('Unigrams')\n    plt.yticks(np.arange(len(unigrams['words'])), unigrams['words'])\n    plt.xlabel('Count')\n\n    ax2 = fig.add_subplot(132)\n    ax2.barh(np.arange(len(bigrams['words'])), bigrams['count'], align='center', alpha=.5)\n    ax2.set_title('Bigrams')\n    plt.yticks(np.arange(len(bigrams['words'])), bigrams['words'])\n    plt.xlabel('Count')\n\n    ax3 = fig.add_subplot(133)\n    ax3.barh(np.arange(len(trigrams['words'])), trigrams['count'], align='center', alpha=.5)\n    ax3.set_title('Trigrams')\n    plt.yticks(np.arange(len(trigrams['words'])), trigrams['words'])\n    plt.xlabel('Count')\n\n    plt.show()","bbd71661":"def textTrends(documents):\n    cleanedReviews = cleanReviews(documents)\n    plotNgrams(cleanedReviews)","2b20522e":"data = pd.read_csv('..\/input\/amazon_alexa.tsv', sep = '\\t')\ndata.head()","d885cc29":"posReviews = data[data['rating'] > 3]\nnegReviews = data[data['rating'] < 3]","21419044":"textTrends(posReviews['verified_reviews'])","00ae8f1e":"textTrends(negReviews['verified_reviews'])","fd8a0c5a":"Second function, documentNgrams, takes the clean reviews and computes [n-grams](https:\/\/en.wikipedia.org\/wiki\/N-gram) of any size specified by the user. This function then packages the 15 most common n-grams into a pandas dataframe. ","e5555c22":"First function, cleanReviews, cleans the reviews by stripping punctuation and whitespace, converting to lowercase, and removing stopwords (you'll see I added some domain specific stopwords).","71a287a5":"The third function, plotNgrams, computes n-grams of size 1, 2, and 3. Using each of the n-gram dataframes, it then creates three horizontal bar charts for each n-gram size.","22952a49":"The final function, textTrends, puts the previous functions together. Call this to start understanding what people are talking about.","448655ab":"I've put together a couple different functions that, when used in conjunction, can help analyze and understand trends in textual data such as product reviews. The output of these functions will be the most common single word occurences, groups of two words (bigrams) and groups of three words (trigrams). \n\nThe power of such a function can really be seen when comparing documents of different topics. For example, using this function you can compare the types of things people are saying about Alexa products vs what people are saying about Google assistants. To go more granular, you can filter for only positive\/negative reviews to better understand the strengths and weknessess of the two types of products. ","e0590c24":"After breaking out and analyzing the reviews for both positive and negative reviews, we see some very interesting leads. One is that Amazon's customer service seems to be a common problem area in negative reviews (although it is important to note there are a low number of samples here). Also, suprisingly, sound quality seems to appear all over the place, in both positive and negative reviews. I see this as meaning two things: First, it would be valuable to dig into which variations of alexa products are recieving the positive reviews and which are recieiving the negative reviews and Secondly: Sound quality is very important to people looking for AI powered voice assistants. I hope you have enjoyed. "}}