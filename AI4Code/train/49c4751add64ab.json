{"cell_type":{"b268bd65":"code","c02758f0":"code","8be7dd91":"code","f0631840":"code","f161cc07":"code","c30b0f04":"code","0e8d275f":"code","e42c89df":"code","9d2e6d3e":"code","d2063868":"code","082bbe83":"code","37d8a51c":"code","d6137bca":"code","d6d2d332":"code","dca9d7b4":"code","71db66fd":"code","2cd3b0ff":"code","3454f6fd":"code","5328f930":"code","68adc480":"code","382ec09a":"code","70ccbf5e":"code","827a3146":"code","db4c4eec":"code","97df4e18":"code","60c149d4":"code","f1bcda91":"markdown","b48b0e73":"markdown","57ef53af":"markdown","cb43e7b0":"markdown","bdee59f6":"markdown","15f5ab9f":"markdown","72f686b1":"markdown","65680006":"markdown","02ccd355":"markdown","3284593d":"markdown","efc7e141":"markdown","e1e34e2e":"markdown"},"source":{"b268bd65":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Data Viz\nimport matplotlib.pyplot as plt\nimport seaborn as sns # statistical data vizualization\n\n# Sklearn\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Config\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","c02758f0":"data = pd.read_csv('..\/input\/data.csv')\n\n# shape of the data\nprint(data.shape)","8be7dd91":"# printing the first rows of the data\ndata.head()","f0631840":"# describing the data\ndata.describe()","f161cc07":"# getting the info of the data\ndata.info()","c30b0f04":"# checking if the dataset contains any NULL Values\nnull_counts = data.isna().sum()\nnull_counts = null_counts[null_counts > 0]\n\nprint(null_counts)","0e8d275f":"# checking for unique values of features\ndata.nunique()","e42c89df":"# removing the id column as it is unique\ndata = data.drop('id', axis = 1)\n\nprint(data.shape)","9d2e6d3e":"# removing the last column as it is empty\ndata = data.drop('Unnamed: 32', axis = 1)\n\nprint(data.shape)","d2063868":"# interesting features to plot\ntarget_vars = [\n    'radius_mean',\n    'texture_mean',\n    'area_mean',\n    'perimeter_mean',\n    'smoothness_mean'\n]\nsns.pairplot(\n    data, \n    hue = 'diagnosis', \n    vars = target_vars\n)","082bbe83":"sns.countplot(data.diagnosis)","37d8a51c":"sns.lmplot(\n    'area_mean',\n    'smoothness_mean',\n    hue ='diagnosis',\n    data = data,\n    fit_reg=False\n)","d6137bca":"# plots the given feature with respect to the target feature\ndef plot_feature(df, feature, target_feature):\n    fig = sns.FacetGrid(df, hue=target_feature, aspect=4)\n    fig.map(sns.kdeplot, feature, shade= True)\n    fig.add_legend()","d6d2d332":"plot_feature(data, 'radius_mean', 'diagnosis')","dca9d7b4":"plot_feature(data, 'texture_mean', 'diagnosis')","71db66fd":"plot_feature(data, 'area_mean', 'diagnosis')","2cd3b0ff":"plot_feature(data, 'perimeter_mean', 'diagnosis')","3454f6fd":"plot_feature(data, 'smoothness_mean', 'diagnosis')","5328f930":"fig = plt.figure(figsize = (20, 10))\n\ncorr = data.corr()\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(\n    corr,\n    mask = mask,\n    cmap = 'RdYlGn', \n    annot = True, \n    fmt=\".1f\")","68adc480":"# label encoding of the dependent variable\nle = LabelEncoder()\ndata.diagnosis = le.fit_transform(data.diagnosis)\n\ndata.diagnosis.value_counts()","382ec09a":"# splitting the dependent and independent variables from the dataset\nX = data.iloc[:,1:]\ny = data.iloc[:,0]\n\nprint(X.shape)\nprint(y.shape)","70ccbf5e":"# splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n\nprint(X_train.shape)\nprint(y_train.shape) \n\nprint(X_test.shape)\nprint(y_test.shape)","827a3146":"# data normalization\nsc = StandardScaler()\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","db4c4eec":"# Support Vector Machine\nfrom sklearn.svm import SVC\n\n# HYPER PARAMETER TUNING FOR SVM\n# using grid search to find the best parameters for svm\n\nparam = {\n    'C': [0.1, 1, 10, 100],\n    'kernel':['linear', 'rbf'],\n    'gamma' :[0.1, 0.8, 0.9, 1, 1.1, 1.2, 1.3, 1.4]\n}\ngrid_svc = GridSearchCV(\n    SVC(),\n    param_grid = param,\n    scoring = 'accuracy',\n    cv = 10\n)","97df4e18":"# Feeding the training data to the grid model\n# and also finding the best parameters\n\ngrid_svc.fit(X_train, y_train)\nprint(\"Best Parameters: \", grid_svc.best_params_)\nprint(\"Best Accuarcy: \", grid_svc.best_score_)","60c149d4":"# creating a new SVC model with these best parameters\nmodel = grid_svc.best_estimator_\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\nprint(\"Testing accuarcy :\", model.score(X_test, y_test))\n\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(\n    cm,\n    annot = True,\n    cmap = 'copper',\n    fmt='d'\n)","f1bcda91":"## Exploratory Data Analysis","b48b0e73":"Pearson\u2019s Correlation Coefficient helps you find out the relationship between two features. It gives you the measure of the strength of association between two variables. The value of Pearson\u2019s Correlation Coefficient can be between -1 to +1.\n\n**1** means that they are **highly correlated** and 0 means no correlation. -1 means that there is a negative correlation. Think of it as an inverse proportion.\n\nObservations:\n* malign cells have overall higher values compared to benign cells\n* mean area and perimeter are correlated\n* mean area and radius are correlated","57ef53af":"![](https:\/\/storage.googleapis.com\/kaggle-datasets-images\/180\/384\/3da2510581f9d3b902307ff8d06fe327\/dataset-cover.jpg)","cb43e7b0":"### Support Vector Classifier\n\nAn explanation on how Support Vector Machines work: https:\/\/www.kdnuggets.com\/2016\/07\/support-vector-machines-simple-explanation.html","bdee59f6":"## Data Vizualization","15f5ab9f":"\n# Breast Cancer Classification\nPredicting if the breast cancer diagnosis is benign or malignant based on several observations:\n\n* radius (mean of distances from center to points on the perimeter)\n* texture (standard deviation of gray-scale values)\n* perimeter\n* area\n* smoothness (local variation in radius lengths)\n* compactness (perimeter^2 \/ area - 1.0)\n* concavity (severity of concave portions of the contour)\n* concave points (number of concave portions of the contour)\n* symmetry","72f686b1":"## Reading the data","65680006":"## Modeling","02ccd355":"## Feature Engineering","3284593d":"## Conclusion\nSVM was able to classify tumors into Malignant \/ Benign with a preety good accuracy, humans only got it right arround 70% of the time.\n\nIf you liked this kernel please up vote :)","efc7e141":"## Data Cleaning","e1e34e2e":"## Imports"}}