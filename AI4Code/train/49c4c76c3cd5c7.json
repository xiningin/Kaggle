{"cell_type":{"76c809a7":"code","553d6bbb":"code","dc60a47b":"code","a71e2db7":"code","8d26c6f3":"code","8e3db275":"code","bf8be195":"code","ef8aad3f":"code","98362233":"code","64097153":"code","481fe47b":"code","d8c74ada":"code","26ddb3f1":"code","04de6bc8":"code","8c78a756":"code","0906ae7f":"code","b3fbeed1":"code","b99c1c79":"code","c1b44e63":"code","a8b4dd42":"code","1fae11bb":"code","267f659b":"code","9ee19366":"code","f0e686a9":"code","75aaed9e":"code","beca151d":"code","76782ed5":"code","eed0960b":"code","f00b1344":"code","d2c643a2":"code","0fb98e44":"markdown","96f4b841":"markdown","168b8d6b":"markdown"},"source":{"76c809a7":"import torch\nfrom torch import nn","553d6bbb":"# #code to free up gpu\n# !pip install GPUtil\n\n# from GPUtil import showUtilization as gpu_usage\n# from numba import cuda\n\n# def free_gpu_cache():\n#     print(\"Initial GPU Usage\")\n#     gpu_usage()                             \n\n#     torch.cuda.empty_cache()\n\n#     cuda.select_device(0)\n#     cuda.close()\n#     cuda.select_device(0)\n\n#     print(\"GPU Usage after emptying the cache\")\n#     gpu_usage()\n\n# free_gpu_cache()         ","dc60a47b":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","a71e2db7":"# all these numbers and layers are defined in alexnet implementation\nalexnet = nn.Sequential(\n            nn.Conv2d(1, 96, kernel_size=11, stride=4), nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(96, 256, kernel_size=5, padding=2),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(256, 386, kernel_size=3, padding=1),nn.ReLU(),\n            nn.Conv2d(386, 386, kernel_size=3, padding=1),nn.ReLU(),\n            nn.Conv2d(386, 256, kernel_size=3, padding=1),nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2),nn.Flatten(),\n            \n            # best way to get this 6400 is after adding flatten just check for the size of tensor returned after flatten layer\n            nn.Linear(6400, 4096), nn.ReLU(), nn.Dropout(p=0.5),\n            nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(p=0.5),\n            nn.Linear(4096, 10)\n)","8d26c6f3":"X =  torch.randn(1,1,224,224)\nX.shape","8e3db275":"alexnet(X).detach()","bf8be195":"# lets look at each layer\n\ndef look_at_net(net, X):\n    \n    for layer in net:\n        X = layer(X)\n        print(f' For {layer.__class__.__name__}, shape : {X.shape}')","ef8aad3f":"look_at_net(alexnet, X)","98362233":"# lets get the fashionmnist dataset\n\nfrom torch.utils.data import Dataset, DataLoader","64097153":"import pandas as pd\nimport torchvision.transforms as transforms\n\ndf = pd.read_csv(\"..\/input\/fashionmnist\/fashion-mnist_train.csv\").head()","481fe47b":"X = df.iloc[:, 1:]\ny = df.iloc[:, 0]\n\nX.shape, y.shape","d8c74ada":"X.iloc[0], y.iloc[0]","26ddb3f1":"# lets implement some helper functions for training this network\n\n\n\nmy_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n])\n\nclass fashion_dataset(Dataset):\n    def __init__(self,data_path=\"..\/input\/fashionmnist\/fashion-mnist_train.csv\", transforms=my_transforms, subset_length=2000):\n        if subset_length:\n            self.input_df = pd.read_csv(data_path)[:subset_length]\n        else :\n            self.input_df = pd.read_csv(data_path)\n        self.len = len(self.input_df)\n        self.transform = transforms\n\n        self.x  = torch.from_numpy(self.input_df.iloc[:,1:].values)\n        self.x = self.x.reshape(self.x.size(0), 1, 28,28).float()\n        \n        self.y = torch.from_numpy(self.input_df.iloc[:,0].values).long()\n        \n    def __len__(self):\n        return self.len\n    \n    def __getitem__(self, index):\n        x_item = self.x[index]\n        y_item = self.y[index]\n        \n        if (self.transform):\n            x_item = self.transform(x_item)\n               \n        return x_item, y_item  \n        ","04de6bc8":"train_dataset = fashion_dataset()","8c78a756":"train_dataset.__len__()","0906ae7f":"X, y = train_dataset[0]\n\nprint(X.shape, y)\n\nimport matplotlib.pyplot as plt\n\nprint(y)\n\nimg = X.permute(1,2,0)\nplt.imshow(img)","b3fbeed1":"test_dataset = fashion_dataset(data_path=\"..\/input\/fashionmnist\/fashion-mnist_test.csv\")","b99c1c79":"batch_size=224\ntrain_dataloader = DataLoader(train_dataset, batch_size=128,shuffle=True)\ntest_dataloader = DataLoader(test_dataset,batch_size=128, shuffle=False)","c1b44e63":"# helper function to make training easy\n\ndef accuracy(y_hat,y):\n    return (y_hat.argmax(1)==y).sum()","a8b4dd42":"def full_accuracy(net, data_iter):\n    net.eval()\n#     device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'\n    net = net.to(device)\n    \n    total_acc = 0\n    total_num = 0\n    with torch.no_grad():\n        for X, y in data_iter:\n            X = X.to(device)\n            y = y.to(device)\n\n            y_hat = net(X)\n\n            total_acc += accuracy(y_hat, y)\n            total_num += y.numel()\n    \n    return total_acc\/total_num\n            ","1fae11bb":"def train_net(net, train_dataloader, test_dataloader, lr, num_apochs=10):\n    \n    def init_weights(m):\n        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n            nn.init.xavier_uniform_(m.weight)\n    net.apply(init_weights)\n    \n#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    print(f'training on {device}')\n    net.train()\n    \n    net.to(device)\n    \n    loss = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n    \n    train_loss = []\n    train_acc = []\n    test_acc = []\n    \n    for epoch in range(num_epochs):\n        acc_value = 0\n        total_number = 0\n        total_loss= 0\n        for X,y in train_dataloader:\n            X = X.to(device)\n            y = y.to(device)\n            \n            y_hat = net(X)\n            l = loss(y_hat, y)\n            \n            optimizer.zero_grad()\n            l.backward()\n            optimizer.step()\n            \n            total_loss += l \n            acc_value += accuracy(y_hat, y)\n            total_number += y.numel()\n            \n#             print(l * X.shape[0], y.numel())\n        \n        with torch.no_grad():\n            \n            print(f\"\\tEpoch {epoch} : Statistics: \")\n            print(f'\\tcurrent train loss : {float(total_loss\/total_number)}')\n            print(f'\\tcurrent train acc : {float(acc_value\/total_number)}')\n            print(f'\\tcurrent test acc : {float(full_accuracy(net, test_dataloader))}')\n    \n            \n            train_loss.append(float(total_loss\/total_number))\n            test_acc.append(float(full_accuracy(net, test_dataloader)))\n            train_acc.append(float(acc_value\/total_number))\n    \n#     plt.plot(range(num_epochs), train_loss, label = 'train loss')\n    plt.plot(range(num_epochs), train_acc, label = 'train acc')\n    plt.plot(range(num_epochs), test_acc, label = 'test acc')\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n            \n        ","267f659b":"# %%time\n# lr , num_epochs = 0.5, 10\n# train_net(alexnet, train_dataloader, test_dataloader, num_epochs, lr)","9ee19366":"# all these numbers and layers are defined in alexnet implementation\nalexnet_new = nn.Sequential(\n            nn.Conv2d(1, 96, kernel_size=11, stride=4), nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2),nn.Flatten(),\n            \n            # best way to get this 6400 is after adding flatten just check for the size of tensor returned after flatten layer\n            nn.Linear(384, 120), nn.ReLU(), nn.Dropout(p=0.5),\n            nn.Linear(120, 10)\n)","f0e686a9":"X = torch.randn(1,1,28,28)","75aaed9e":"look_at_net(alexnet_new, X)","beca151d":"train_dataset = fashion_dataset(transforms=None)\ntest_dataset = fashion_dataset(data_path=\"..\/input\/fashionmnist\/fashion-mnist_test.csv\", transforms=None)\nbatch_size=128\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True)\ntest_dataloader = DataLoader(test_dataset,batch_size=batch_size, shuffle=False)","76782ed5":"%%time\nlr , num_epochs = 0.5, 10\ntrain_net(alexnet_new, train_dataloader, test_dataloader, num_epochs, lr)","eed0960b":"\n%%time\nlr , num_epochs = 0.5, 100\ntrain_net(alexnet_new, train_dataloader, test_dataloader, num_epochs, lr)","f00b1344":"# 5\n\nlenet_5 = nn.Sequential(nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.ReLU(),\n                    nn.AvgPool2d(kernel_size=2, stride=2),\n                    nn.Conv2d(6, 16, kernel_size=5), nn.ReLU(),\n                    nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),\n                    nn.Linear(16 * 5 * 5, 120), nn.ReLU(), nn.Dropout(p=0.5),\n                    nn.Linear(120, 84), nn.ReLU(), nn.Linear(84, 10))","d2c643a2":"\n%%time\nlr , num_epochs = 0.5, 10\ntrain_net(alexnet_new, train_dataloader, test_dataloader, num_epochs, lr)","0fb98e44":"## Exercises\n1. Try increasing the number of epochs. Compared with LeNet, how are the results different? Why?\n\n* The result is a very noisy function, it is like its not getting trained at all.\n\n2. AlexNet may be too complex for the Fashion-MNIST dataset.\n    1. Try simplifying the model to make the training faster, while ensuring that the accuracy\n    does not drop significantly.\n\n    * done but accuracy has still not improved.\n\n    2. Design a better model that works directly on 28 \u00d7 28 images.\n\n    * done. \n\n3. Modify the batch size, and observe the changes in accuracy and GPU memory.\n\n* more batch size more consumption\n\n4. Analyze computational performance of AlexNet.\n\n    1. What is the dominant part for the memory footprint of AlexNet?\n        * Linear network and 3 cnns\n    2. What is the dominant part for computation in AlexNet?\n        * the neuralnetwork 3 cnns\n    3. How about memory bandwidth when computing the results?\n        * more than 100 mb\n5. Apply dropout and ReLU to LeNet-5. Does it improve? How about preprocessing?\n   ```python\n    \n    lenet_5 = nn.Sequential(nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.ReLU(),\n                    nn.AvgPool2d(kernel_size=2, stride=2),\n                    nn.Conv2d(6, 16, kernel_size=5), nn.ReLU(),\n                    nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),\n                    nn.Linear(16 * 5 * 5, 120), nn.ReLU(), nn.Dropout(p=0.5),\n                    nn.Linear(120, 84), nn.ReLU(), nn.Linear(84, 10))\n   ```","96f4b841":"# Problems and approach\n\n1. Alexnet is too complex it takes more than 64 gb of space\n2. So I tried training only 2000 images instead.\n\n3. lets trying simplyfyingalex net\n","168b8d6b":"it is definitely training faster but very innacurate."}}