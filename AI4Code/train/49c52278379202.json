{"cell_type":{"fc660320":"code","bddf8174":"code","16bf1aef":"code","14493ea5":"code","5da7958e":"code","a5313039":"code","660f5d97":"code","e39664dd":"code","5fdd0a7d":"code","155718d3":"code","3530b725":"code","9e8a843f":"code","959c4325":"code","a173c8b0":"code","b756350a":"code","6c28318f":"code","d05b7da3":"code","4d8f04b5":"code","c1c6824a":"code","1fa71bed":"code","d88aa1a1":"code","3d22fc55":"code","888f7a47":"code","b8ad2c3e":"code","8ea6e169":"code","ca1f2717":"code","34ffa2e9":"code","1c9c0273":"code","aef62e8a":"code","47825e2a":"markdown","b92eb3ec":"markdown","2a51e34d":"markdown","ee2078ab":"markdown","9a1d7637":"markdown","21ff41ef":"markdown","4cebcf33":"markdown","1daf4fcb":"markdown","7694b9fe":"markdown","0e9c987b":"markdown","a57fec6f":"markdown","bf3682f6":"markdown","15f9d3e2":"markdown","35388b5c":"markdown","9471a2be":"markdown","62740eb0":"markdown","8f805273":"markdown","209e7783":"markdown","4df6aa36":"markdown","41ae99e8":"markdown","b7e3304e":"markdown","85f61af1":"markdown"},"source":{"fc660320":"from IPython.display import HTML\nHTML('''<script>\n  function code_toggle() {\n    if (code_shown){\n      $('div.input').hide('500');\n      $('#toggleButton').val('Show Code')\n    } else {\n      $('div.input').show('500');\n      $('#toggleButton').val('Hide Code')\n    }\n    code_shown = !code_shown\n  }\n  $( document ).ready(function(){\n    code_shown=false;\n    $('div.input').hide()\n  });\n<\/script>\n<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"><\/form>''')","bddf8174":"# connecting packeges\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n# decision tree packeges\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.tree import export_graphviz \n# boosting\nimport xgboost\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import accuracy_score\n# PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression","16bf1aef":"# Hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')","14493ea5":"# load data\n# surviving passengers by id\ngender_submission = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\n# train data\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\n# test data\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","5da7958e":"print('gender_submission')\ngender_submission.head()\nprint('Train data')\ntrain.head()\nprint('Test data')\ntest.head()","a5313039":"# remove the passenger's name\ndel train['Name']\ndel test['Name']\ndel train['Cabin']\ndel test['Cabin']\ndel train['Ticket']\ndel test['Ticket']","660f5d97":"# Let's look at the number of NaN values in the columns\nprint('Train data')\ntrain.isnull().sum()\nprint('Test data')\ntest.isnull().sum()","e39664dd":"# replace empty values with averages\ntrain = train.fillna({'Age':train.Age.median()})\ntest = test.fillna({'Age':test.Age.median()})\n# drop all anither NaN values\ntrain.dropna(inplace=True)\ntest.dropna(inplace=True)","5fdd0a7d":"train.Embarked = train.Embarked.replace({'S':'Southampton', 'C':'Cherbourg',  'Q':'Queenstown'})\ntest.Embarked = test.Embarked.replace({'S':'Southampton', 'C':'Cherbourg',  'Q':'Queenstown'})","155718d3":"train = pd.get_dummies(train)\ntest = pd.get_dummies(test)","3530b725":"# Look at ready data\nprint('gender_submission')\ngender_submission.head()\nprint('Train data')\ntrain.head()\nprint('Test data')\ntest.head()","9e8a843f":"X = train.drop(['PassengerId','Survived'], axis=1)\nX.head()","959c4325":"Y = train.Survived","a173c8b0":"clf = DecisionTreeClassifier()\nfrom sklearn.model_selection import GridSearchCV\nparametrs = {'criterion': ['gini', 'entropy'], 'max_depth':range(1, 100)}\ngrid_search_cv_clf = GridSearchCV(DecisionTreeClassifier(), parametrs, iid=True, cv=5)\nbest_params = grid_search_cv_clf.fit(X, Y)\nbest_criterion = best_params.best_params_['criterion']\nbest_depth = best_params.best_params_['max_depth']\nprint('Best criterion:', best_criterion)\nprint('Best depth of the tree:', best_depth)","b756350a":"clf = DecisionTreeClassifier(criterion=best_criterion, max_depth=best_depth)\nimp_tree = pd.DataFrame(clf.fit(X, Y).feature_importances_, \n              index=X.columns, columns=['importance'])\nax = imp_tree.sort_values('importance').plot(kind='barh', figsize=(5, 5))","6c28318f":"best_clf = grid_search_cv_clf.fit(X, Y).best_estimator_\nX_test = test.merge(gender_submission, how='left', left_on='PassengerId', right_on='PassengerId').drop(['PassengerId','Survived'], axis=1)\nY_test = test.merge(gender_submission, how='left', left_on='PassengerId', right_on='PassengerId').Survived\nres_tree = best_clf.score(X_test, Y_test)\nprint('Percentage of correctly predicted passengers who survived %.2f%%:' % (round(res_tree*100, 2)))","d05b7da3":"xmodel = xgboost.XGBClassifier()\nxmodel.fit(X, Y)","4d8f04b5":"imp_xgb = pd.DataFrame(xmodel.feature_importances_, \n              index=X.columns, columns=['importance'])\nax = imp_xgb.sort_values('importance').plot(kind='barh', figsize=(5, 5))","c1c6824a":"predictions = [round(value) for value in xmodel.predict(X_test)]","1fa71bed":"accuracy = accuracy_score(Y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","d88aa1a1":"sc = StandardScaler() \n\npca_train = sc.fit_transform(X) \npca_test = sc.transform(X_test) ","3d22fc55":"pca = PCA(n_components = 2) \n \npca_train = pca.fit_transform(pca_train) \npca_test = pca.transform(pca_test) \n \nexplained_variance = pca.explained_variance_ratio_ ","888f7a47":"classifier = LogisticRegression(random_state = 0) \nclassifier.fit(pca_train, Y)","b8ad2c3e":"y_pred = classifier.predict(pca_test) ","8ea6e169":"accuracy = accuracy_score(Y_test, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","ca1f2717":"class Perceptron():\n    def __init__(self, rate = 0.01, niter = 10):\n        self.rate = rate\n        self.niter = niter\n\n    def fit(self, X, y):\n        self.weight = np.zeros(1 + X.shape[1])\n        self.errors = []\n\n        for i in range(self.niter):\n            err = 0\n            for xi, target in zip(X, y):\n                delta_w = self.rate * (target - self.predict(xi))\n                self.weight[1:] += delta_w * xi\n                self.weight[0] += delta_w\n                err += int(delta_w != 0.0)\n            self.errors.append(err)\n        return self\n\n    def net_input(self, X):\n\n        return np.dot(X, self.weight[1:]) + self.weight[0]\n\n    def predict(self, X):\n        \"\"\"Return class label after unit step\"\"\"\n        return np.where(self.net_input(X) >= 0.0, 1, -1)","34ffa2e9":"pn = Perceptron(0.1, niter=100)\npn.fit(np.array(X), np.array(Y))","1c9c0273":"ax = plt.plot(range(1, len(pn.errors) + 1), pn.errors, marker='o')\nax = plt.xlabel('Epochs')\nax = plt.ylabel('Number of misclassifications')\nax = plt.show()\nax","aef62e8a":"y_pred = pn.predict(np.array(X_test)) \naccuracy = accuracy_score(Y_test, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","47825e2a":"<p style=\"text-indent: 25px;\">Let's look at the change of errors with each era.<\/p>","b92eb3ec":"### <center>Summary:<\/center> ###\n\n<p style=\"text-indent: 25px;\">Among all the presented methods of classification, logical regression with the method of principal components proved to be the best. Boosting and decision tree also performed well. While the perceptron has shown itself bad. So the principal components method showed $94%$ prediction accuracy, boosting $\\approx 90%$, decision tree showed $87%$ and perceptron showed $36%$. This is probably due to the fact that in the sample there are problems with which the perceptron does not cope well, for example, the exclusive or (XOR).<\/p>","2a51e34d":"<p style=\"text-indent: 25px;\">Let's see how the factors allocated by The XGBoost module are distributed.<\/p> ","ee2078ab":"# <center>Data analysis of the Titanic with the use of the classification.<\/center> #\n#### <center>10.12.2019<\/center> ####\n\n### <center>Introduction<\/center> ###\n\n<p style=\"text-indent: 25px;\">Titanic analysis is a classic example of the use of machine learning (ML). With ML, you can predict whether a given passenger will survive or not, depending on the characteristics that are inherent in this passenger.<\/p>\n<p style=\"text-indent: 25px;\">The data obtained from <a href=\"https:\/\/www.kaggle.com\/c\/titanic\/data\"> $Kaggle$ <\/a> will be used for the analysis.<\/p>\n<p style=\"text-indent: 25px;\">The data is divided into training and test samples.<\/p>\n<p style=\"text-indent: 25px;\">The analysis will use such types ML as: Decision Tree; Boosting, Principal component method and Single layer perceptron.<\/p>","9a1d7637":"<p style=\"text-indent: 25px;\">Let's test our model on test data.<\/p>\n<p style=\"text-indent: 25px;\">Let's test our model on test data.<\/p>","21ff41ef":"<p style=\"text-indent: 25px;\">The de tree does a good job of predicting the Titanic passenger's survival.<\/p>\n<p style=\"text-indent: 25px;\">The main factor of survival is: female sex, cabin class, ticket price, age, as well as the presence of brothers or sisters on the ship.<\/p>","4cebcf33":"<p style=\"text-indent: 25px;\">Let's train the perceptron on the training data.<\/p>","1daf4fcb":"<p style=\"text-indent: 25px;\">As you can see, the minimum error value occurs 2 times at 38 and 81 epochs.<\/p>","7694b9fe":"### <center>Decision tree<\/center> ###\n<p style=\"text-indent: 25px;\">A decision tree is a classifier using a visual and analytical decision support tool where the expected values (or expected utility) of competing alternatives are calculated. The decision tree consists of three types of nodes:<\/p>\n\n- The nodes of the decision\n- Probabilistic nodes\n- Closing nodes\n\n<p style=\"text-indent: 25px;\">Let's prepare the data for the decision tree: first, replace the \"gender\" fields from the string with an integer, and replace the passenger boarding place with the vessel (C-Cherbourg, S-Southampton, Q = Queenstown), remove the passenger name, and replace the empty age values with the average values. Also, it should be noted that the cabin number depends on the class of the passenger, so to avoid codependency it is necessary to delete the cabin number and the passenger ticket.<\/p>","0e9c987b":"### <center>Boosting<\/center> ###\n<p style=\"text-indent: 25px;\">Boosting is an improved decision tree using gradient boosting.<\/p> \n\nImprovements:\n\n- Regularization: to reduce the overfitting of the model;\n- Working with sparse data: filling missing values depending on the loss value;\n- Weighted quantile Method: finds optimal separation points most efficiently;\n- Cross-validation.\n\n<p style=\"text-indent: 25px;\">For boosting, we use the XGBoost module for python.<\/p> ","a57fec6f":"Remove the passenger number and split the sample into training and target data.","bf3682f6":"<p style=\"text-indent: 25px;\">Perceptron shows a low result in comparison with other presented classifiers.<\/p>","15f9d3e2":"### <center>Principal component method<\/center> ###\n<p style=\"text-indent: 25px;\">By the method of principal components is a transformation of the data to find variables which highly correlated factors of the sample. After finding these variables, you can use them to build a prediction model.By the method of principal components is a transformation of the data to find variables which highly correlated factors of the sample. After finding these variables, you can use them to build a prediction model. The search is carried out by explaining the maximum variance.<\/p> \n<p style=\"text-indent: 25px;\">For the principal component method, you first need to unify the data relative to each other.<\/p> \n<p style=\"text-indent: 25px;\">Use PCA for predict survival of the passengers.<\/p>","35388b5c":"<p style=\"text-indent: 25px;\">Let's construct a logical regression using PCA.<\/p>","9471a2be":"<p style=\"text-indent: 25px;\">The boosting method uses more factors to build the model.<\/p> ","62740eb0":"<p style=\"text-indent: 25px;\">Create decision tree. When using the decision tree, it is possible to retrain the model. to avoid this, we will select the optimal parameters of the decision tree.<\/p>","8f805273":"<p style=\"text-indent: 25px;\">Let's look at what facts affect whether the passenger will survive or not.<\/p>","209e7783":"<p style=\"text-indent: 25px;\">The method of boosting showed a better, though slightly better result than the usual decision tree..<\/p> ","4df6aa36":"### <center>Single layer perceptron<\/center> ###\n<p style=\"text-indent: 25px;\">A single-layer perceptron is a model that consists of: a neuron that is a sum with a given offset of weights of incoming vectors and an activation function.<\/p> \n<img src='https:\/\/im0-tub-ru.yandex.net\/i?id=518207d565d0df119682d4a5d346b1a1&n=13&exp=1'>\n\n<center>Single layer perceptron example: <a href=\"https:\/\/im0-tub-ru.yandex.net\/i?id=518207d565d0df119682d4a5d346b1a1&n=13&exp=1\"> $Source$ <\/a> <\/center>\n\n\n\n<p style=\"text-indent: 25px;\">When creating a single-layer perceptron, you must specify the number of incoming nodes (this is the number of incoming factors), the offset (in this case, the learning rate), and the number of epochs (training cycles).<\/p>\n<p style=\"text-indent: 25px;\">Let's create a class for perceptron.<\/p>","41ae99e8":"<p style=\"text-indent: 25px;\">PCA showed even better result than decision tree and boosting.<\/p> ","b7e3304e":"<p style=\"text-indent: 25px;\">Remove the columns with the name, cabin number andthe passenger ticket data. And replace the passenger boarding point.<\/p>","85f61af1":"### Let's Looking to data"}}