{"cell_type":{"1cc00bf4":"code","dcebf634":"code","df555444":"code","615c3b54":"code","e913f747":"code","e36112d3":"code","3759133d":"code","53ba7617":"code","b71b58dd":"code","c32ca75c":"code","89bd1310":"code","f8aac134":"code","4df71d65":"code","bc9b997c":"code","78c06efb":"code","4264a4a1":"code","d768d193":"code","1ae95cd7":"code","0a6e9a44":"code","17890ae3":"code","64e51f14":"code","8ac0501c":"code","7f69e472":"code","891ea335":"code","1ee4750c":"code","9033cc85":"code","a03c40f2":"code","8ed6d798":"code","98a36d8e":"code","07f9c0ae":"code","a2cf89c3":"code","6e21cd36":"code","85835c9c":"code","5fd3de01":"code","dfdb2d78":"code","c6b0fdea":"code","e8c92c51":"code","afb51e27":"code","69df807c":"code","5c8f00bd":"code","5ad71feb":"code","fd257d04":"code","0d57b03e":"code","41b07e1d":"code","a39b7e3c":"code","b7bca9e1":"code","4087bcf8":"code","ee4f7ac7":"code","58679493":"code","84fb8d79":"code","c30fe0d8":"code","d1b7092b":"code","c26c5e5c":"code","6987dcdb":"code","a379ad7a":"code","054560a5":"code","1de12961":"code","991b70ec":"code","9b5d08a8":"code","150439b6":"code","51fb8077":"code","297ec84f":"code","943428ef":"code","fb551956":"code","1ccb6901":"code","3dd2ccdc":"code","d6194792":"code","5e3ff067":"code","176fc7e5":"code","38c40364":"code","023050ea":"code","c2ddf640":"code","e83b9d29":"code","c287aa01":"code","b875c9d8":"code","256ca530":"code","9393458b":"code","7b05482a":"code","b53d4c51":"code","70bb3e88":"code","5beca2bf":"code","d1c6ec62":"code","ef44319a":"code","e5ed7bc8":"code","4e0ca3d9":"code","9f3340eb":"code","b36583ee":"code","9c138538":"code","0b38072a":"code","d50a1372":"code","7a09d07a":"code","0ae98ad3":"code","a5ee0987":"code","21a154b9":"code","69ff4e69":"code","5d9c8613":"code","17ff3457":"code","76c2fea1":"code","799edff5":"markdown","f9783008":"markdown","4bf96a88":"markdown","b7e64356":"markdown","2fd68885":"markdown","253aa319":"markdown","9ff03546":"markdown","e544b312":"markdown","8674ff13":"markdown","2e3121bb":"markdown","36b7617e":"markdown","eb76dfbe":"markdown","63be8565":"markdown","674ed6e3":"markdown","374fbb6c":"markdown","d47e767a":"markdown"},"source":{"1cc00bf4":"#importing basic libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","dcebf634":"from sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression","df555444":"house= pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')","615c3b54":"house.head()","e913f747":"house.shape","e36112d3":"house.info()","3759133d":"house.describe()","53ba7617":"#lets loook at the missing value percentage\nround(100*house.isnull().sum()\/len(house.index),2).sort_values(ascending=False)","b71b58dd":"#columns with morethan 45% missing value\nhouse.columns[100*house.isnull().sum()\/len(house.index)>45]","c32ca75c":"# based on data dictionary na in PoolQC means 'No Pool'\nhouse.loc[house['PoolQC'].isnull(),['PoolQC']] = 'No Pool'\n# based on data dictionary na in Fence means 'No Fence'\nhouse.loc[house['Fence'].isnull(),['Fence']] = 'No Fence'\n# based on data dictionary na in MiscFeature means 'none'\nhouse.loc[house['MiscFeature'].isnull(),['MiscFeature']] = 'none'","89bd1310":"# based on data dictionary na in Alley means 'No alley access'\nhouse.loc[house['Alley'].isnull(),['Alley']] = 'No alley access'\n# based on data dictionary na in FireplaceQu means 'No Fireplace'\nhouse.loc[house['FireplaceQu'].isnull(),['FireplaceQu']] = 'No Fireplace'","f8aac134":"# based on data dictionary na in BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2 means ''No Basement'\nhouse.loc[house['BsmtQual'].isnull(),['BsmtQual']] = 'No Basement'\nhouse.loc[house['BsmtCond'].isnull(),['BsmtCond']] = 'No Basement'\nhouse.loc[house['BsmtExposure'].isnull(),['BsmtExposure']] = \"No Basement\"\nhouse.loc[house['BsmtFinType1'].isnull(),['BsmtFinType1']] = 'No Basement'\nhouse.loc[house['BsmtFinType2'].isnull(),['BsmtFinType2']] = 'No Basement'","4df71d65":"# based on data dictionary na in MasVnrType means 'none'\nhouse.loc[house['MasVnrType'].isnull(),['MasVnrType']] = 'none'\n# based on data dictionary where MasVnrType type is 'none' area will 0\nhouse.loc[house['MasVnrArea'].isnull(),['MasVnrArea']] = 0","bc9b997c":"100*house['LotFrontage'].isnull().sum()\/len(house.index)\n#replacing the missing values with mean\nhouse[\"LotFrontage\"].replace(np.nan, house[\"LotFrontage\"].mean(),inplace=True)","78c06efb":"# based on data dictionary na in GarageType,GarageFinish,GarageQual,GarageCond means 'No Garage'\nhouse.loc[house['GarageType'].isnull(),['GarageType']] = 'No Garage'\nhouse.loc[house['GarageFinish'].isnull(),['GarageFinish']] = 'No Garage'\nhouse.loc[house['GarageQual'].isnull(),['GarageQual']] = 'No Garage'\nhouse.loc[house['GarageCond'].isnull(),['GarageCond']] = 'No Garage'","4264a4a1":"# replacing with mode value of the column\nhouse.loc[house['Electrical'].isnull(),['Electrical']] = \"SBrkr\"","d768d193":"# impute GarageYrBlt with 2019 so that while calculating age it will it will turn to 0\n# age = 2019-2019 = 0\nhouse.loc[house['GarageYrBlt'].isnull(),['GarageYrBlt']] = 2019","1ae95cd7":"#lets again look if there are any columns\nhouse.columns[100*house.isnull().sum()\/len(house.index)>0]","0a6e9a44":"house.shape","17890ae3":"#drop duplicates if any\nhouse=house.drop_duplicates()\nhouse.shape","64e51f14":"house['SalePrice'].describe()","8ac0501c":"#lets drop the ID column as it is redundant in model builiding\nhouse.drop(['Id'],axis=1,inplace=True)","7f69e472":"##Derived variables from the dataset\n#New variable creation TotalSF i.e combination of TotalBsmtSF, 1stFlrSF, 2ndFlrSF\nhouse['TotalSF'] = house['TotalBsmtSF'] + house['1stFlrSF'] + house['2ndFlrSF']\n#house[\"house_age_when_sold_in_month\"] = (((house[\"YrSold\"]-1) - house[\"YearBuilt\"])*12) + house[\"MoSold\"]\n# derive house age = 2019 - year build\nhouse[\"house_age\"] = 2019 - house[\"YearBuilt\"]\n# derive garage age = 2019 - year build\nhouse[\"garage_age\"] = 2019 - house[\"GarageYrBlt\"]\n# derive gap b\/w house build and remodel = YearRemodAdd - year build\nhouse[\"gap_between_build_remodel\"] = house[\"YearRemodAdd\"] - house[\"YearBuilt\"]","891ea335":"#converting to correct datatype for some variables\nhouse['MSSubClass'] = house['MSSubClass'].astype('object')\nhouse['OverallCond'] = house['OverallCond'].astype('object')\nhouse['YrSold'] = house['YrSold'].astype('object')\nhouse['MoSold'] = house['MoSold'].astype('object')","1ee4750c":"#dividing the variables to numeric and categorical\nhouse_numeric=house.select_dtypes(include=['float64','int64'])\nhouse_numeric.head()","9033cc85":"house_numeric.columns","a03c40f2":"house_categorical=house.select_dtypes(include=['object'])\nhouse_categorical.columns","8ed6d798":"print(len(house_categorical.columns))\nprint(len(house_numeric.columns))","98a36d8e":"# correlation matrix\ncorr = house_numeric.corr()\ncorr","07f9c0ae":"# plotting a heatmap\nplt.figure(figsize = (20, 15))\n# heatmap\nsns.heatmap(corr, cmap=\"coolwarm\", annot=True)\nplt.show()","a2cf89c3":"#plotting scatter plot for some of the numeric variables\nsns.set()\nplt.figure(figsize=(40, 30))\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt',\n        'KitchenAbvGr','Fireplaces','WoodDeckSF','PoolArea','TotalSF']\nsns.pairplot(house[cols], size = 2.5)\nplt.show();","6e21cd36":"#distplot of the target variables\nsns.distplot(house['SalePrice'])","85835c9c":"#These are the columns form the above distplot that doesnot follow normal distribution, lets drop few of them\n#'GarageYrBlt','YearBuilt','YrSold','YearRemodAdd','MoSold','2ndFlrSF','BsmtFullBath','FullBath','HalfBath','Fireplaces','GarageCars'\nhouse_numeric.drop(['YearBuilt', 'YearRemodAdd','Fireplaces'], axis=1,inplace=True)\nhouse_numeric.head()","5fd3de01":"house_numeric.shape","dfdb2d78":"#boxplots for numeric varaibles\nplt.figure(figsize=(24, 12))\nplt.subplot(3,3,1)\nsns.violinplot(house.LotFrontage, fill='#A4A4A4', color=\"red\")\nplt.subplot(3,3,2)\nsns.violinplot(house.LotArea, fill='#A4A4A4', color=\"red\")\nplt.subplot(3,3,3)\nsns.violinplot(house.MasVnrArea, fill='#A4A4A4', color=\"red\")\nplt.subplot(3,3,4)\nsns.violinplot(house.BsmtUnfSF, fill='#A4A4A4', color=\"red\")\nplt.subplot(3,3,5)\nsns.violinplot(house.TotalSF, fill='#A4A4A4', color=\"red\")\nplt.subplot(3,3,6)\nsns.violinplot(house['1stFlrSF'], fill='#A4A4A4', color=\"red\")\nplt.subplot(3,3,7)\nsns.violinplot(house['2ndFlrSF'], fill='#A4A4A4', color=\"red\")\nplt.subplot(3,3,8)\nsns.violinplot(house.LowQualFinSF, fill='#A4A4A4', color=\"red\")\nplt.subplot(3,3,9)\nsns.violinplot(house.GrLivArea, fill='#A4A4A4', color=\"red\")\nplt.show()","c6b0fdea":"#boxplots for numeric variables\nplt.figure(figsize=(24, 12))\nplt.subplot(3,3,1)\nsns.violinplot(house.GrLivArea, fill='#A4A4A4', color=\"red\")\nplt.subplot(3,3,2)\nsns.violinplot(house.TotRmsAbvGrd, fill='#A4A4A4', color=\"red\")\nplt.subplot(3,3,3)\nsns.violinplot(house.house_age, fill='#A4A4A4', color=\"red\")\nplt.subplot(3,3,4)\nsns.violinplot(house.garage_age, fill='#A4A4A4', color=\"red\")\nplt.subplot(3,3,5)\nsns.violinplot(house.PoolArea, fill='#A4A4A4', color=\"red\")\nplt.subplot(3,3,6)\nsns.violinplot(house.MiscVal, fill='#A4A4A4', color=\"red\")\nplt.subplot(3,3,7)\nsns.violinplot(house.EnclosedPorch, fill='#A4A4A4', color=\"red\")\nplt.subplot(3,3,8)\nsns.violinplot(house.GarageArea, fill='#A4A4A4', color=\"red\")\nplt.subplot(3,3,9)\nsns.violinplot(house.SalePrice, fill='#A4A4A4', color=\"red\")\nplt.show()","e8c92c51":"#since data available is very less we will treat the outliers only for few variables\n# outlier treatment for LotFrontage\nQ1 = house.LotFrontage.quantile(0.25)\nQ3 = house.LotFrontage.quantile(0.75)\nIQR = Q3 - Q1\nhouse = house[(house.LotFrontage >= Q1 - 1.5*IQR) & (house.LotFrontage <= Q3 + 1.5*IQR)]","afb51e27":"# outlier treatment for LotArea\nQ1 = house.LotArea.quantile(0.25)\nQ3 = house.LotArea.quantile(0.75)\nIQR = Q3 - Q1\nhouse = house[(house.LotArea >= Q1 - 1.5*IQR) & (house.LotArea <= Q3 + 1.5*IQR)]","69df807c":"Q1 = house.PoolArea.quantile(0.25)\nQ3 = house.PoolArea.quantile(0.75)\nIQR = Q3 - Q1\nhouse = house[(house.PoolArea >= Q1 - 1.5*IQR) & (house.PoolArea <= Q3 + 1.5*IQR)]","5c8f00bd":"Q1 = house.MiscVal.quantile(0.25)\nQ3 = house.MiscVal.quantile(0.75)\nIQR = Q3 - Q1\nhouse = house[(house.MiscVal >= Q1 - 1.5*IQR) & (house.MiscVal <= Q3 + 1.5*IQR)]","5ad71feb":"house.shape","fd257d04":"# split into X and y\nX = house.drop(['SalePrice'], axis=1)\ny = house['SalePrice']","0d57b03e":"#replacing with binary values \nhouse[\"CentralAir\"]=house[\"CentralAir\"].map({'Y': 1, \"N\": 0})","41b07e1d":"#lets include the categorical columns\nhouse_categorical_df=X.select_dtypes(include=['object'])","a39b7e3c":"house_categorical_df.columns","b7bca9e1":"#creating dummy variables for categorical columns\nhouse_df_dummies = pd.get_dummies(house_categorical_df, drop_first=True)\nhouse_df_dummies.head()","4087bcf8":"# dropping the original categorical variables after creating dummies\nX = X.drop(list(house_categorical_df.columns), axis=1)","ee4f7ac7":"# concat dummy variables with X\nX = pd.concat([X, house_df_dummies], axis=1)","58679493":"X.shape","84fb8d79":"#scaling\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscale_var = X.columns\nX[scale_var] = scaler.fit_transform(X[scale_var])","c30fe0d8":"X.describe()","d1b7092b":"# split data into test and train\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.7, \n                                                    test_size = 0.3, \n                                                    random_state = 100)","c26c5e5c":"# let's build a Linear regression model first\nfrom sklearn import metrics\n# linear regression\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\n# predict\ny_train_pred = lm.predict(X_train)\nmetrics.r2_score(y_true=y_train, y_pred=y_train_pred)","6987dcdb":"y_test_pred = lm.predict(X_test)\nmetrics.r2_score(y_true=y_test, y_pred=y_test_pred)","a379ad7a":"lm.intercept_","054560a5":"# model coefficients\nmodel_parameters = list(lm.coef_)\nmodel_parameters.insert(0, lm.intercept_)\nmodel_parameters = [round(x,3) for x in model_parameters]\ncols = X.columns\ncols = cols.insert(0, \"constant\")\nlist(zip(cols, model_parameters))","1de12961":"#Importing the lasso and ridge regressions\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV","991b70ec":"# lasso regression without tuning hyper parameter\nlm = Lasso(alpha=0.001)\nlm.fit(X_train, y_train)\n\n# predict\ny_train_pred = lm.predict(X_train)\nprint(metrics.r2_score(y_true=y_train, y_pred=y_train_pred))\ny_test_pred = lm.predict(X_test)\nprint(metrics.r2_score(y_true=y_test, y_pred=y_test_pred))","9b5d08a8":"# lasso model parameters without tuning hyper parameter\nmodel_parameters = list(lm.coef_)\nmodel_parameters.insert(0, lm.intercept_)\nmodel_parameters = [round(x,3) for x in model_parameters]\ncols = X.columns\ncols = cols.insert(0, \"constant\")\nlist(zip(cols, model_parameters))","150439b6":"# Now let's do k fold with r2 score and tune hyper parameter\nfrom sklearn.model_selection import KFold\n\nfolds = KFold(n_splits = 5, shuffle = True, random_state = 4)\n\n# specify range of hyperparameters\nparams = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 200]}\n\nmodel = Lasso()\nmodel_cv = GridSearchCV(estimator = model, param_grid = params, \n                        scoring= 'r2', \n                        cv = folds, \n                        return_train_score=True, verbose = 1)            \nmodel_cv.fit(X_train, y_train)","51fb8077":"# results data frame \ncv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results.head()","297ec84f":"model_cv.best_params_","943428ef":"#plotting the values of r2 score to choose the optimal value of alpha\ncv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('r2 score')\nplt.title(\"r2 score and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","fb551956":"# Now let's do k fold with neg_mean_absolute_error score and tune hyper parameter\nparams = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 200]}\nlasso = Lasso()\n\nmodel_cv = GridSearchCV(estimator = lasso, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \n\nmodel_cv.fit(X_train, y_train)","1ccb6901":"# results data frame\ncv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results.head()","3dd2ccdc":"model_cv.best_params_","d6194792":"# plotting mean test and train scoes with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\n\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","5e3ff067":"# model with optimal alpha = 200\nlm = Lasso(alpha=200)\nlm.fit(X_train, y_train)\n\n\n# predict\ny_train_pred = lm.predict(X_train)\nprint(metrics.r2_score(y_true=y_train, y_pred=y_train_pred))\ny_test_pred = lm.predict(X_test)\nprint(metrics.r2_score(y_true=y_test, y_pred=y_test_pred))","176fc7e5":"# lasso model parameters with alpha = 200\nmodel_parameters = list(lm.coef_)\nmodel_parameters.insert(0, lm.intercept_)\nmodel_parameters = [round(x, 3) for x in model_parameters]\ncols = X.columns\ncols = cols.insert(0, \"constant\")\nlist(zip(cols, model_parameters))\nfinal_var=dict(zip(cols, model_parameters))\n{k: v for k, v in sorted(final_var.items(), key=lambda item: item[1])}","38c40364":"# ridge regression without tuning hyper parameter\nlm = Ridge(alpha=0.001)\nlm.fit(X_train, y_train)\n\n# predict\ny_train_pred = lm.predict(X_train)\nprint(metrics.r2_score(y_true=y_train, y_pred=y_train_pred))\ny_test_pred = lm.predict(X_test)\nprint(metrics.r2_score(y_true=y_test, y_pred=y_test_pred))","023050ea":"# Now let's do k fold with r2 score and tune hyper parameter\nparams = {'alpha': [0.001, 0.01, 1.0, 5.0, 10.0]}\n\nridge = Ridge()\nfolds = 5\nmodel_cv = GridSearchCV(estimator = ridge, \n                        param_grid = params, \n                        scoring= 'r2', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \nmodel_cv.fit(X_train, y_train)","c2ddf640":"# results data frame\ncv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results.head()","e83b9d29":"model_cv.best_params_","c287aa01":"# plotting mean test and train scoes with alpha to tune hyper parameter\ncv_results['param_alpha'] = cv_results['param_alpha'].astype('int32')\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('r2 score')\nplt.title(\"r2 score and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","b875c9d8":"# Now let's do k fold with neg_mean_absolute_error score and tune hyper parameter\nparams = {'alpha': [0.001, 0.01, 1.0, 5.0, 10.0]}\n\nridge = Ridge()\nfolds = 5\nmodel_cv = GridSearchCV(estimator = ridge, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \nmodel_cv.fit(X_train, y_train)","256ca530":"# results data frame\ncv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results.head()","9393458b":"model_cv.best_params_","7b05482a":"# plotting mean test and train scoes with alpha to tune hyper parameter\ncv_results['param_alpha'] = cv_results['param_alpha'].astype('int32')\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","b53d4c51":"# model with optimal alpha = 10\n# ridge regression\nlm = Ridge(alpha=10)\nlm.fit(X_train, y_train)\ny_train_pred = lm.predict(X_train)\nprint(metrics.r2_score(y_true=y_train, y_pred=y_train_pred))\ny_test_pred = lm.predict(X_test)\nprint(metrics.r2_score(y_true=y_test, y_pred=y_test_pred))","70bb3e88":"# Only the variables with no zero coefficient values are selected\nX_new=cols.drop(['LotFrontage','BsmtUnfSF','1stFlrSF','BsmtFullBath','GarageYrBlt','GarageArea','PoolArea','MiscVal','gap_between_build_remodel',\n          'MSSubClass_50','MSSubClass_60','MSSubClass_190','Alley_Pave','Neighborhood_MeadowV','Neighborhood_SWISU','Neighborhood_SawyerW','Condition2_Norm',\n          'Condition2_RRAn','Condition2_PosA','BldgType_TwnhsE','HouseStyle_1Story','HouseStyle_SLvl','OverallCond_6','RoofStyle_Hip','RoofMatl_Roll','Exterior1st_AsphShn','Exterior1st_Stucco',\n'Exterior2nd_HdBoard', 'Exterior2nd_MetalSd','Exterior2nd_Plywood','MasVnrType_none','ExterCond_TA','BsmtQual_No Basement','BsmtCond_Gd','BsmtCond_Po','BsmtCond_No Basement','BsmtExposure_No Basement','BsmtFinType1_No Basement','BsmtFinType1_No Basement',\n'BsmtFinType2_GLQ','BsmtFinType2_No Basement','Functional_Min1','FireplaceQu_Gd','GarageType_Attchd','GarageType_CarPort','GarageFinish_No Garage','GarageQual_Gd','GarageQual_No Garage','GarageCond_No Garage','MiscFeature_Shed','MiscFeature_none','SaleType_ConLD','SaleType_ConLw','SaleType_New'     ])","5beca2bf":"#lets look at the length of the variables after elimination by lasso regression\nlen(X_new)","d1c6ec62":"#lets move the varibles in to a list\nreduced_cols=list(X_new)\n#removing the constant \nreduced_cols.remove('constant')","ef44319a":"#creating the dataframe of the varibles extracted above\nX_NEW=X[reduced_cols]","e5ed7bc8":"X_NEW.head()","4e0ca3d9":"# Split test and train again using new X\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_NEW, y, train_size=0.7,\n                                                    test_size = 0.3, \n                                                    random_state = 100)","9f3340eb":"# list of alphas to tune\nparams = {'alpha': [0.001, 0.01, 1.0, 5.0, 10.0]}\n\nridge = Ridge()\n\n# cross validation\nfolds = 5\nmodel_cv = GridSearchCV(estimator = ridge, \n                        param_grid = params, \n                        scoring= 'r2', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \nmodel_cv.fit(X_train, y_train)","b36583ee":"# results data frame\ncv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results.head()","9c138538":"model_cv.best_params_","0b38072a":"cv_results['param_alpha'] = cv_results['param_alpha'].astype('int32')\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('r2 score')\nplt.title(\"r2 score and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","d50a1372":"# Now let's do k fold with neg_mean_absolute_error score and tune hyper parameter\nparams = {'alpha': [0.001, 0.01, 1.0, 5.0, 10.0]}\n\nridge = Ridge()\n\n# cross validation\nfolds = 5\nmodel_cv = GridSearchCV(estimator = ridge, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \nmodel_cv.fit(X_train, y_train)","7a09d07a":"# results data frame\ncv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results.head()","0ae98ad3":"model_cv.best_params_","a5ee0987":"# plotting mean test and train scoes with alpha to tune hyper parameter\ncv_results['param_alpha'] = cv_results['param_alpha'].astype('int32')\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","21a154b9":"# model with optimal alpha = 10\nlm = Ridge(alpha=10)\nlm.fit(X_train, y_train)\n\n# predict\ny_train_pred = lm.predict(X_train)\nprint(metrics.r2_score(y_true=y_train, y_pred=y_train_pred))\ny_test_pred = lm.predict(X_test)\nprint(metrics.r2_score(y_true=y_test, y_pred=y_test_pred))","69ff4e69":"#lets create a dictionary with coeffiecients\nmodel_parameters = list(lm.coef_)\nmodel_parameters.insert(0, lm.intercept_)\nmodel_parameters = [round(x, 3) for x in model_parameters]\ncols = X.columns\ncols = cols.insert(0, \"constant\")\nfinal_var=dict(zip(cols, model_parameters))","5d9c8613":"#sort the dict\n{k: v for k, v in sorted(final_var.items(), key=lambda item: item[1])}","17ff3457":"test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')","76c2fea1":"my_submission = pd.DataFrame({'Id': test.Id, 'SalePrice':final_var})\nmy_submission.to_csv('submission.csv', index=False)","799edff5":"### Missing value treatment","f9783008":"### Model Builiding","4bf96a88":"Builing the Ridge regression after variable shrinkage done by lasso","b7e64356":"It is clearly visible that the model is overfit since the tarining accracy is very high compared to test data","2fd68885":"### loading the dataset","253aa319":"### Data understanding","9ff03546":"### Univariate analysis","e544b312":"Optimal value of alpha of lasso regression is 200 and r2 score for optimal value of alpha is given below R2 score for train : 0.941828259003052 R2 score for test : 0.9194833870877548\n\nOptimal value of alpha of ridge regression is 10 and r2 score for optimal value of alpha is given below R2 score for train : 0.9464820405083926 R2 score for test : 0.9075797172829855 \n\nOptimal value of alpha is 10 for ridge regression on variables selected by lasso regession and r2 score for optimal value of alpha is given below R2 score for train : 0.9445110204167058 R2 score for test : 0.9132677230470666 Lasso has successfully reduced variables by shrinking the variable coefficient to 0.\n","8674ff13":"lets tune the hyperparameter","2e3121bb":"With optimal value of alpha=200 we got r2 values for training and test data as 0.94 and 0.91 which is good indication that model is not overfit","36b7617e":"### Outlier Analysis","eb76dfbe":"### Converting the categorical columns by creating dummy variables","63be8565":"### Ridge regression","674ed6e3":"With optimal value of alpha = 10 for ridge regression on variables selected by lasso regession, we got following score for train and test data set. R2 for train : 0.9445110204167058 R2 for test : 0.9132677230470666 \n\nThis score looks good and we can conclude that model is not overfitting. And this score is pretty close to previous ridge regression model.","374fbb6c":"### Importing the basic libraries","d47e767a":"#### These are the top features in the ridge regression after using the non zero coefficient variables from the lasso regression.\n\nGrLivArea,\nLotFrontage,\nOverallQual,\nLotArea,\nGarageArea,\nNeighborhood_Crawfor,\nMSSubClass_45,\nMasVnrArea,\n1stFlrSF,\nExterQual,\nExterior1st_CBlock,\nExterior1st_CemntBd,\nFoundation_Stone,\nFoundation_Wood,\nExterior1st_WdShing,\nWoodDeckSF,\nExterior2nd_AsphShn,\nGarageType_Basment,\nExterior2nd_BrkFace,\n"}}