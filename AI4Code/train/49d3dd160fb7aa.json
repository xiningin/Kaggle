{"cell_type":{"7ac51a2b":"code","2c341c67":"code","99ad711f":"code","e80689ce":"code","a2275ad2":"code","a3fd0847":"code","7f82a0c2":"code","17b85680":"code","a7637fd1":"code","36935bad":"code","bb073a5e":"code","86e7d43e":"code","c21beb22":"code","5f0e34fb":"code","9cd6e717":"code","fc59b233":"code","29545155":"code","4806e0d1":"code","e7806245":"code","83d3c4e9":"code","bbea79e5":"code","a77ca242":"code","1f67e375":"code","1e9845fd":"code","18a3ca2b":"code","b60fde23":"code","6596470d":"code","28d61467":"code","0f9a7b8f":"code","7cf1a0b4":"code","c2c7fda3":"code","8bd0afcc":"code","d3621dc5":"code","ed0f1b9b":"code","b8036347":"code","0e8709bf":"code","95811648":"code","4a53ecf3":"code","375b8327":"code","5753d97e":"code","8ad3a823":"code","fed159b8":"code","97fe2416":"code","9f090f00":"code","3f4f4eb6":"code","80120a75":"code","25881938":"code","085b5522":"code","a7bc724c":"code","0e2d1d28":"code","6b419fc7":"code","ea2868d1":"code","e056370e":"code","671dbfbc":"code","9fb27129":"code","3275bc54":"code","7c90e522":"code","cafdd593":"code","a14d85d2":"code","5f621df3":"code","748a7ff4":"code","3b9c360c":"code","fbe5bcd9":"code","585920de":"code","23885195":"code","a3263ff8":"code","d0c64610":"code","cb5a5906":"code","52316e39":"code","5c0b9475":"code","bf6b5eaa":"code","9cc60847":"code","74412fcf":"code","8517957d":"code","bd50b8d3":"code","bcfb3c43":"code","c2476fb4":"code","d20ff6b7":"code","f6012faa":"code","63ec5409":"code","f88d19e5":"code","4c9d1363":"code","bbf08553":"code","f915c566":"code","b07fc3cb":"markdown","5e6e69d4":"markdown","64ee9120":"markdown","f403b45e":"markdown","9a3304bf":"markdown","937e81eb":"markdown","ade88e15":"markdown","c97e81fd":"markdown","f35f2577":"markdown","c3405c9c":"markdown","0bad8af7":"markdown","6e20c173":"markdown","00bfc84d":"markdown","157d4e7e":"markdown","05a83feb":"markdown","52344436":"markdown","38adaf35":"markdown","f43598ac":"markdown","2928be6f":"markdown","9e529e07":"markdown"},"source":{"7ac51a2b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2c341c67":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","99ad711f":"df_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","e80689ce":"df_train.head()","a2275ad2":"df_train.info()","a3fd0847":"df_train.describe().T","7f82a0c2":"df_train.isnull().sum()*100\/len(df_train)","17b85680":"df_train.nunique()","a7637fd1":"# Drop the identity columns\nid_column = ['PassengerId', 'Name', 'Ticket']","36935bad":"df_train = df_train.drop(id_column, axis = 1)\ndf_test = df_test.drop(id_column, axis = 1)","bb073a5e":"# Drop 'cabin' column as out of present 204 values, 147 are distinct\ndf_train = df_train.drop(['Cabin'], axis = 1)\ndf_test = df_test.drop(['Cabin'], axis = 1)","86e7d43e":"var_categorical = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']\nvar_numerical = list(set(df_train.columns) - set(var_categorical) - set(['Survived']))","c21beb22":"# Function to label the count on top of each bar in graph\ndef label_values(ax, spacing=5):\n    total = 0\n    for rect in ax.patches:\n        total += rect.get_height()\n\n    for rect in ax.patches:\n        y_value = rect.get_height()\n        x_value = rect.get_x() + rect.get_width() \/ 2\n\n        space = spacing\n        \n        va = 'bottom'\n        \n        if y_value < 0:\n            space *= -1\n            va = 'top'\n        label = \"{:.2f}, {:.2f}\".format(y_value, y_value\/total*100)\n        ax.annotate(\n            label,                      \n            (x_value, y_value),         \n            xytext=(0, space),          \n            textcoords=\"offset points\", \n            ha='center',                \n            va=va)    ","5f0e34fb":"ax = sns.countplot(x = df_train[\"Survived\"])\nlabel_values(ax, spacing = -15)\nplt.show()","9cd6e717":"i = 1\nfor column in var_numerical:\n    print(column.title())\n    plt.subplots(figsize=(16, 16))\n    plt.subplot(len(var_numerical) + 1, 3, i)\n    sns.boxplot(x = df_train[column])\n    i += 1\n    plt.subplot(len(var_numerical) + 1, 3, i)\n    sns.distplot(x = df_train[column])\n    i += 1\n    plt.subplot(len(var_numerical) + 1, 3, i)\n    sns.boxplot(x = df_train[\"Survived\"], y = df_train[column])\n    i += 1\n    plt.show()","fc59b233":"for column in var_categorical:\n    plt.figure(figsize=(15, 6))\n    print(column.title())\n    ax = sns.countplot(x = df_train[column], hue=df_train[\"Survived\"])\n    label_values(ax)\n    plt.show()","29545155":"df_train[\"Parch\"] = df_train[\"Parch\"].apply(lambda x: 4 if x>=4 else x)\ndf_train[\"SibSp\"] = df_train[\"SibSp\"].apply(lambda x: 5 if x>=5 else x)\ndf_train[\"Fare\"] = df_train[\"Fare\"].apply(lambda x: 0 if np.log(x)<0 else np.log(x))\n\ndf_test[\"Parch\"] = df_test[\"Parch\"].apply(lambda x: 4 if x>=4 else x)\ndf_test[\"SibSp\"] = df_test[\"SibSp\"].apply(lambda x: 5 if x>=5 else x)\ndf_test[\"Fare\"] = df_test[\"Fare\"].apply(lambda x: 0 if np.log(x)<0 else np.log(x))\n","4806e0d1":"df_train[\"Embarked\"] = df_train[\"Embarked\"].fillna(\"S\")\ndf_train[\"Age\"] = df_train[\"Age\"].fillna(df_train[\"Age\"].median())\n\ndf_test[\"Embarked\"] = df_test[\"Embarked\"].fillna(\"S\")\ndf_test[\"Age\"] = df_test[\"Age\"].fillna(df_test[\"Age\"].median())\ndf_test[\"Fare\"] = df_test[\"Fare\"].fillna(df_test[\"Fare\"].median())","e7806245":"df_train[\"Sex\"] = df_train[\"Sex\"].apply(lambda x: 1 if x==\"female\" else 0)\ndf_test[\"Sex\"] = df_test[\"Sex\"].apply(lambda x: 1 if x==\"female\" else 0)","83d3c4e9":"# Make dummy variables for the nominal columns\ndf_train = pd.get_dummies(df_train, columns=[\"Embarked\"], drop_first=True, prefix=[\"Embarked\"])\ndf_test = pd.get_dummies(df_test, columns=[\"Embarked\"], drop_first=True, prefix=[\"Embarked\"])","bbea79e5":"from sklearn.preprocessing import StandardScaler, MinMaxScaler","a77ca242":"scaler = StandardScaler()\nminmax_scaler = MinMaxScaler()","1f67e375":"df_train[\"Age\"] = scaler.fit_transform(np.array(df_train[\"Age\"]).reshape(-1, 1))\ndf_train[\"Fare\"] = minmax_scaler.fit_transform(np.array(np.round(df_train[\"Fare\"],2)).reshape(-1, 1))","1e9845fd":"df_test[\"Age\"] = scaler.transform(np.array(df_test[\"Age\"]).reshape(-1, 1))\ndf_test[\"Fare\"] = minmax_scaler.transform(np.array(np.round(df_test[\"Fare\"],2)).reshape(-1, 1))","18a3ca2b":"sns.heatmap(df_train.corr(), annot=True)\nplt.show()","b60fde23":"# visualizing numerical columns\nsns.pairplot(data = df_train)\nplt.show()","6596470d":"y_train = df_train.pop('Survived')\nX_train = df_train\n\nX_test = df_test","28d61467":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.model_selection import KFold\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix","0f9a7b8f":"lr = LogisticRegression()","7cf1a0b4":"# Create the param grid for logistic regression\nlr_param_grid = {\n    'max_iter': [75, 100, 125],\n    'penalty': ['l1', 'l2'],\n    'C': [0.5 , 1, 1.5],\n    'solver': ['liblinear', 'lbfgs']\n}\nprint(lr_param_grid)","c2c7fda3":"clf_lr = GridSearchCV(lr, param_grid=lr_param_grid, scoring='accuracy', cv=4)\n\nclf_lr.fit(X_train, y_train)","8bd0afcc":"print(clf_lr.best_estimator_)\nprint(clf_lr.best_score_ )\nprint(clf_lr.best_params_)","d3621dc5":"y_train_lr = clf_lr.predict(X_train)\ny_test_lr = clf_lr.predict(X_test)\nprint(accuracy_score(y_train, y_train_lr))","ed0f1b9b":"sns.heatmap(confusion_matrix(y_train, y_train_lr), annot=True)\nplt.show()","b8036347":"svc = SVC(probability=True)","0e8709bf":"# Create the param grid for logistic regression\nparams_grid_svc = {\n    \"kernel\": [\"linear\", \"rbf\"],\n    \"degree\": [1, 2, 3]\n}","95811648":"clf_svc = GridSearchCV(svc, param_grid=params_grid_svc, scoring='accuracy', cv=4, verbose=1)\n\nclf_svc.fit(X_train, y_train)","4a53ecf3":"print(clf_svc.best_estimator_)\nprint(clf_svc.best_score_ )\nprint(clf_svc.best_params_)","375b8327":"y_train_svc = clf_svc.predict(X_train)\ny_test_svc = clf_svc.predict(X_test)\nprint(accuracy_score(y_train, y_train_svc))","5753d97e":"sns.heatmap(confusion_matrix(y_train, y_train_svc), annot=True)\nplt.show()","8ad3a823":"rf = RandomForestClassifier(n_jobs=-1, oob_score=True)","fed159b8":"# Create the param grid for logistic regression\nparams_grid_rf = {\n    'n_estimators': [30, 50, 75, 100],\n   'max_depth': [3, 5],\n    \"min_samples_leaf\": [20, 35, 50]\n}","97fe2416":"clf_rf = GridSearchCV(rf, param_grid=params_grid_rf, scoring='accuracy', cv=4, verbose=1)\n\nclf_rf.fit(X_train, y_train)","9f090f00":"print(clf_rf.best_estimator_)\nprint(clf_rf.best_score_ )\nprint(clf_rf.best_params_)","3f4f4eb6":"y_train_rf = clf_rf.predict(X_train)\ny_test_rf = clf_rf.predict(X_test)\nprint(accuracy_score(y_train, y_train_rf))","80120a75":"sns.heatmap(confusion_matrix(y_train, y_train_rf), annot=True)\nplt.show()","25881938":"gb = GradientBoostingClassifier()","085b5522":"# Create the param grid for Gradient Boosting Classifier\nparams_grid_gb = {\n    \"n_estimators\": [30, 50, 75, 100],\n    \"learning_rate\": [0.05, 0.1, 0.15],\n    'max_depth': [3, 5],\n    \"min_samples_leaf\": [20, 35, 50],\n}","a7bc724c":"clf_gb = GridSearchCV(gb, param_grid=params_grid_gb, scoring='accuracy', cv=4, verbose=1)\n\nclf_gb.fit(X_train, y_train)","0e2d1d28":"print(clf_gb.best_estimator_)\nprint(clf_gb.best_score_ )\nprint(clf_gb.best_params_)","6b419fc7":"y_train_gb = clf_gb.predict(X_train)\ny_test_gb = clf_gb.predict(X_test)\nprint(accuracy_score(y_train, y_train_gb))","ea2868d1":"sns.heatmap(confusion_matrix(y_train, y_train_gb), annot=True)\nplt.show()","e056370e":"xgb = XGBClassifier()","671dbfbc":"# Create the param grid for logistic regression\nparams_grid_xgb = {\n    \"n_estimators\": [30, 50, 75, 100],\n    \"learning_rate\": [0.05, 0.1, 0.15],\n    'max_depth': [3, 5],\n    \"min_samples_leaf\": [20, 35, 50],\n}","9fb27129":"clf_xgb = GridSearchCV(xgb, param_grid=params_grid_xgb, scoring='accuracy', cv=4, verbose=1)\n\nclf_xgb.fit(X_train, y_train)","3275bc54":"print(clf_xgb.best_estimator_)\nprint(clf_xgb.best_score_ )\nprint(clf_xgb.best_params_)","7c90e522":"y_train_xgb = clf_xgb.predict(X_train)\ny_test_xgb = clf_xgb.predict(X_test)\nprint(accuracy_score(y_train, y_train_xgb))","cafdd593":"sns.heatmap(confusion_matrix(y_train, y_train_xgb), annot=True)\nplt.show()","a14d85d2":"knn = KNeighborsClassifier(n_jobs=-1)","5f621df3":"# Create the param grid for logistic regression\nparams_grid_knn = {\n    \"n_neighbors\": [3, 5, 7]\n}","748a7ff4":"clf_knn = GridSearchCV(knn, param_grid=params_grid_knn, scoring='accuracy', cv=4)\n\nclf_knn.fit(X_train, y_train)","3b9c360c":"print(clf_knn.best_estimator_)\nprint(clf_knn.best_score_ )\nprint(clf_knn.best_params_)","fbe5bcd9":"y_train_knn = clf_knn.predict(X_train)\ny_test_knn = clf_knn.predict(X_test)\nprint(accuracy_score(y_train, y_train_knn))","585920de":"sns.heatmap(confusion_matrix(y_train, y_train_knn), annot=True)\nplt.show()","23885195":"from sklearn.ensemble import StackingClassifier","a3263ff8":"print(clf_rf.best_estimator_)\nprint(clf_lr.best_estimator_)\nprint(clf_gb.best_estimator_)\nprint(clf_xgb.best_estimator_)\nprint(clf_svc.best_estimator_)\nprint(clf_knn.best_estimator_)","d0c64610":"# Create Base Learners\nbase_learners = [\n                 ('rf_1', RandomForestClassifier(max_depth= 5, min_samples_leaf =20, n_estimators =30)),\n                 ('rf_2', SVC(degree= 1)),\n                ('rf_3', GradientBoostingClassifier(learning_rate= 0.15, max_depth= 5, min_samples_leaf= 35, n_estimators = 50)),\n                ('rf_4', XGBClassifier(learning_rate= 0.15, max_depth= 5, min_samples_leaf= 20, n_estimators= 75)),\n                ('rf_5', KNeighborsClassifier(n_neighbors=5))\n                ]\n\n# Initialize Stacking Classifier with the Meta Learner\nclf = StackingClassifier(estimators=base_learners, final_estimator=LogisticRegression(C= 0.5, max_iter= 75))\n\n# Extract score\nclf.fit(X_train, y_train)","cb5a5906":"y_train_pred = clf.predict(X_train)\ny_test = clf.predict(X_test)\nprint(accuracy_score(y_train, y_train))","52316e39":"sns.heatmap(confusion_matrix(y_train, y_train_pred), annot=True)\nplt.show()","5c0b9475":"# Create Base Learners\nbase_learners = [\n                 ('rf_1', RandomForestClassifier(max_depth= 5, min_samples_leaf =20, n_estimators =30)),\n                 ('rf_2', SVC(degree= 1, probability=True)),\n              ('rf_3', GradientBoostingClassifier(learning_rate= 0.15, max_depth= 5, min_samples_leaf= 35, n_estimators = 50)),\n                ('rf_4', XGBClassifier(learning_rate= 0.15, max_depth= 5, min_samples_leaf= 20, n_estimators= 75)),\n                ('rf_5', KNeighborsClassifier(n_neighbors=5)),\n                ('rf_6', LogisticRegression(C= 0.5, max_iter= 75))\n                ]","bf6b5eaa":"c_train, c_test = [], []\nfor model in base_learners:\n    model[1].fit(X_train, y_train)\n    c_test.append(model[1].predict_proba(X_test))\n    c_train.append(model[1].predict_proba(X_train))","9cc60847":"y_pred_prob_train = []\nfor i in range(len(X_train)):\n    sum1 = 0\n    for j in range(len(base_learners)):\n        sum1 += c_train[j][i]\n    y_pred_prob_train.append(sum1\/len(base_learners))\ny_pred_train = np.argmax(y_pred_prob_train, axis = 1)","74412fcf":"y_pred_prob = []\nfor i in range(len(X_test)):\n    sum1 = 0\n    for j in range(len(base_learners)):\n        sum1 += c_test[j][i]\n    y_pred_prob.append(sum1\/len(base_learners))\ny_pred_vote = np.argmax(y_pred_prob, axis = 1)","8517957d":"print(accuracy_score(y_train, y_pred_train))\nsns.heatmap(confusion_matrix(y_train, y_pred_train), annot=True)\nplt.show()","bd50b8d3":"df_sub = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\ndf_sub.head()","bcfb3c43":"test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","c2476fb4":"df_sub_lr = pd.DataFrame({\n    \"PassengerId\": test[\"PassengerId\"],\n    \"Survived\": y_test_lr\n})\ndf_sub_lr.to_csv('submission_lr.csv', index = False)","d20ff6b7":"df_sub_rf = pd.DataFrame({\n    \"PassengerId\": test[\"PassengerId\"],\n    \"Survived\": y_test_rf\n})\ndf_sub_rf.to_csv('submission_rf.csv', index = False)","f6012faa":"df_sub_svc = pd.DataFrame({\n    \"PassengerId\": test[\"PassengerId\"],\n    \"Survived\": y_test_svc\n})\ndf_sub_svc.to_csv('submission_svc.csv', index = False)","63ec5409":"df_sub_gb = pd.DataFrame({\n    \"PassengerId\": test[\"PassengerId\"],\n    \"Survived\": y_test_gb\n})\ndf_sub_gb.to_csv('submission_gb.csv', index = False)","f88d19e5":"df_sub_xgb = pd.DataFrame({\n    \"PassengerId\": test[\"PassengerId\"],\n    \"Survived\": y_test_xgb\n})\ndf_sub_xgb.to_csv('submission_xgb.csv', index = False)","4c9d1363":"df_sub = pd.DataFrame({\n    \"PassengerId\": test[\"PassengerId\"],\n    \"Survived\": y_test\n})\ndf_sub.to_csv('submission.csv', index = False)","bbf08553":"df_sub_vote = pd.DataFrame({\n    \"PassengerId\": test[\"PassengerId\"],\n    \"Survived\": y_pred_vote\n})\ndf_sub_vote.to_csv('submission_vote.csv', index = False)","f915c566":"df_sub_knn = pd.DataFrame({\n    \"PassengerId\": test[\"PassengerId\"],\n    \"Survived\": y_test_knn\n})\ndf_sub_knn.to_csv('submission_knn.csv', index = False)","b07fc3cb":"# Model Building","5e6e69d4":"# Encoding categorical variables","64ee9120":"## Categorical Variables","f403b45e":"# Prediction","9a3304bf":"# ii. SVC with hyperparameter tuning","937e81eb":"# Loading Dataset","ade88e15":"# Handle Missing values","c97e81fd":"# vi. KNN","f35f2577":"## Numerical Variables","c3405c9c":"# v. XGBoost Classifier with hyperparameter tuning","0bad8af7":"# vii. Stacking Classifier","6e20c173":"# iv. Gradient Boosting with hyperparameter tuning","00bfc84d":"# iii. Random Forest Classifier with hyperparameter tuning","157d4e7e":"# Scaling numerical variables","05a83feb":"## Target variable","52344436":"# Exploratory Data Analysis","38adaf35":"# Split data into X and y\n","f43598ac":"# i. Logistic Regression with hyperparameter tuning","2928be6f":"Target variable is not highly biased.","9e529e07":"# viii. Aggregate Voting"}}