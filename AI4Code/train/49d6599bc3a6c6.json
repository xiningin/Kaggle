{"cell_type":{"c981b9fb":"code","93b602a7":"code","4b64b88c":"code","979913a2":"code","754ba79e":"code","61c5f46c":"code","f093c6a2":"code","08f69ec4":"code","3e8f30ab":"code","2e869239":"code","56a028a5":"code","1014ad31":"code","424c0767":"code","14d0f9a5":"code","dbe517cc":"code","44b8ba8f":"code","e716596c":"code","52cdf8dd":"code","e827efcd":"code","fd9a1d89":"code","afa10b5c":"code","605c770a":"code","0cd40c58":"code","dba65cf6":"code","57fce859":"code","5e7cdec1":"code","6d02757b":"code","524ed8ea":"code","63565c14":"code","ecdca219":"code","bcd41408":"code","6efc643c":"code","e13f1a97":"code","f2ad197f":"code","4acb8eb7":"code","6c906063":"code","e4cb176a":"code","b0f90c66":"code","50067b4d":"code","28e9af8e":"code","ba792607":"code","5bd9635d":"code","2f1daa4e":"code","656cd90b":"code","cdec3126":"code","57069ece":"code","426744a3":"code","1bc4edc5":"code","e9e110ad":"markdown","c2cd38d8":"markdown","86214274":"markdown","43578cf0":"markdown","3eb9c08e":"markdown","e5811228":"markdown","a343dcbd":"markdown","d5ba6a8e":"markdown","1b9c11bc":"markdown","cc266606":"markdown","6e2233a7":"markdown","79638052":"markdown","16e918f1":"markdown","78cf163c":"markdown","fc97165b":"markdown","df4eaeb2":"markdown","45df71cb":"markdown","dbb46ae7":"markdown"},"source":{"c981b9fb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","93b602a7":"import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot\n\nimport nltk\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom wordcloud import WordCloud, STOPWORDS\n\nimport nltk\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom wordcloud import WordCloud, STOPWORDS\nfrom string import punctuation\nfrom bs4 import BeautifulSoup\nimport re,string,unicodedata\nfrom nltk import pos_tag\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import wordnet, stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport collections","4b64b88c":"df_train = pd.read_csv('\/kaggle\/input\/asap-aes\/training_set_rel3.tsv', sep='\\t', encoding='ISO-8859-1')\ndf_test = pd.read_csv('\/kaggle\/input\/asap-aes\/test_set.tsv', sep='\\t', encoding='ISO-8859-1')","979913a2":"df_train.head()","754ba79e":"df_test.head()","61c5f46c":"# Getting more info about the test info\n\ndf_test.info()","f093c6a2":"df_train.shape","08f69ec4":"# Getting more info about the train dataset\ndf_train.info()","3e8f30ab":"# Let's check how many values are none in each row of the train dataset\ndf_train.isnull().sum()","2e869239":"# Since a large part of the dataset has columns with more than 70-80 percent missing values so Deleting those columns\ndf_train.dropna(axis = 1, inplace = True)\ndf_train.head()","56a028a5":"# Describing the train set\ndf_train.describe()","1014ad31":"# Checking how many unique essay id were given\nprint(df_train['essay_set'].nunique())\ndf_train['essay_set'].unique()","424c0767":"# Counting the number of eacy essay_set\n\nprint(df_train.groupby('essay_set').size())\n","14d0f9a5":"# Lets see the unique ratings which are being given by the rater1\n\nprint(df_train['rater1_domain1'].nunique())\ndf_train['rater1_domain1'].unique()","dbe517cc":"# Counting the number of rates of each rates given by rater1\n\nprint(df_train.groupby('rater1_domain1').size())","44b8ba8f":"# Lets see the unique ratings which are being given by the rater2\n\nprint(df_train['rater2_domain1'].nunique())\ndf_train['rater2_domain1'].unique()","e716596c":"# Counting the number of rates of each rates given by rater1\n\nprint(df_train.groupby('rater1_domain1').size())","52cdf8dd":"# Maximum domain score obtained by any essay\n\ndf_train['domain1_score'].max()","e827efcd":"# Minimum domain score obtained by any essay\n\ndf_train['domain1_score'].min()","fd9a1d89":"# Visualizing the percentage wise share of the various sets of essay \n\nlabels = df_train['essay_set'].value_counts().index\nvalues = df_train['essay_set'].value_counts().values\n\ncolors = df_train['essay_set']\n\nfig = go.Figure(data = [go.Pie(labels = labels, values = values, textinfo = \"label+percent\",\n                              marker = dict(colors = colors, line=dict(color='#000000', width=2)), \n                              title = \"Distribution of sets of essay\")])\n\nfig.show()","afa10b5c":"# Visualizing the percentage wise share of top 10 grades given by rater 1 and their percentage\n\nlabels = df_train['rater1_domain1'].value_counts()[:10].index\nvalues = df_train['rater1_domain1'].value_counts()[:10].values\n\ncolors = df_train['rater1_domain1']\n\nfig = go.Figure(data = [go.Pie(labels = labels, values = values, textinfo = \"label+percent\",\n                              marker = dict(colors = colors), \n                              title = \"Top 10 grades given by rater 1 and their percentage\")])\n\nfig.show()","605c770a":"# Visualizing the percentage wise share of top 10 grades given by rater 1 and their percentage\n\nlabels = df_train['rater2_domain1'].value_counts()[:10].index\nvalues = df_train['rater2_domain1'].value_counts()[:10].values\n\ncolors = df_train['rater2_domain1']\n\nfig = go.Figure(data = [go.Pie(labels = labels, values = values, textinfo = \"label+percent\",\n                              marker = dict(colors = colors), \n                              title = \"Top 10 grades given by rater2 and their percentage\")])\n\nfig.show()","0cd40c58":"# Defining function to clean the text\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","dba65cf6":"# Applying clean text function on short_description to clean the text of train set\n\ndf_train['essay'] = df_train['essay'].apply(lambda x: clean_text(x))","57fce859":"# Now checking whether the text of the essay columns have been changed or not\n\ndf_train.head()","5e7cdec1":"word_cloud = WordCloud(\n                       width=1600,\n                       height=800, \n                       margin=0,\n                       max_words=500, # Maximum numbers of words we want to see \n                       max_font_size=150, min_font_size=30,  # Font size range\n                       background_color=\"white\"\n            ).generate(\" \".join(df_train['essay']))\n\nplt.figure(figsize=(10, 16))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.title('WordCloud of essay', fontsize = 40)\nplt.axis(\"off\")\nplt.show()","6d02757b":"# Applying clean text function on short_description to clean the text of test set\n\ndf_test['essay'] = df_test['essay'].apply(lambda x: clean_text(x))","524ed8ea":"word_cloud = WordCloud(\n                       width=1600,\n                       height=800, \n                       margin=0,\n                       max_words=500, # Maximum numbers of words we want to see \n                       max_font_size=150, min_font_size=30,  # Font size range\n                       background_color=\"white\"\n            ).generate(\" \".join(df_test['essay']))\n\nplt.figure(figsize=(10, 16))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.title('WordCloud of essay in test set', fontsize = 40)\nplt.axis(\"off\")\nplt.show()","63565c14":"print()\ntext = \"I love you, don't you\"\n\n# instantiate tokenizer class\ntokenizer1 = nltk.tokenize.WhitespaceTokenizer()\ntokenizer2 = nltk.tokenize.TreebankWordTokenizer()\ntokenizer3 = nltk.tokenize.WordPunctTokenizer()\ntokenizer4 = nltk.tokenize.RegexpTokenizer(r'\\w+')\n\nprint(\"Example Text: \", text)\nprint(\"Tokenization by whitespace: \", tokenizer1.tokenize(text))\nprint(\"Tokenization by words using Treebank Word Tokenizer: \", tokenizer2.tokenize(text))\nprint(\"Tokenization by punctuation: \", tokenizer3.tokenize(text))\nprint(\"Tokenization by regular expression: \", tokenizer4.tokenize(text))\n","ecdca219":"# Tokenizing the training and test set\n\n# instantiate the tokenizer class\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n\n# Tokenizing the training set\ndf_train['essay'] = df_train['essay'].apply(lambda x: tokenizer.tokenize(x))\n\n# Tokenizing the test set\ndf_test['essay'] = df_test['essay'].apply(lambda x: tokenizer.tokenize(x))","bcd41408":"# Printing the tokenized string of the training set\nprint()\nprint('Tokenized String:')\ndf_train['essay'].head()","6efc643c":"# Printing the tokenized string of the testing set\n\nprint()\nprint('Tokenized String:')\ndf_test['essay'].head()","e13f1a97":"# Defining function to remove the stopwords\n\ndef remove_stopwords(text):\n    \n    words = [word for word in text if word not in stopwords.words('english')]\n    return words","f2ad197f":"# Removing the stopwords from the training set\n\ndf_train['essay'] = df_train['essay'].apply(lambda x: remove_stopwords(x))","4acb8eb7":"# Removing the stopwords from the test set\n\ndf_test['essay'] = df_test['essay'].apply(lambda x: remove_stopwords(x))","6c906063":"# lets now look at the training set\n\ndf_train.head()","e4cb176a":"# lets now look at the test set\n\ndf_test.head()","b0f90c66":"# Stemming and Lemmatization examples\n\ntext = \"How is the Josh\"\n\ntokenizer = nltk.tokenize.TreebankWordTokenizer()\ntokens = tokenizer.tokenize(text)\n\n# Stemmer \nstemmer = nltk.stem.PorterStemmer()\nprint(\"Stemming the sentence: \", \" \".join(stemmer.stem(token) for token in tokens))\n\n# Lemmatizer \nlemmatizer = nltk.stem.WordNetLemmatizer()\nprint(\"Lemmatizing the sentence: \", \" \".join(lemmatizer.lemmatize(token) for token in tokens))","50067b4d":"# After preprocessing the text format\n\ndef combine_text(list_of_text):\n    \n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\n","28e9af8e":"# Preprocessing the train set\n\ndf_train['essay'] = df_train['essay'].apply(lambda x: combine_text(x))\n\ndf_train.head()","ba792607":"# Preprocessing the test set\n\ndf_test['essay'] = df_test['essay'].apply(lambda x: combine_text(x))\ndf_test.head()","5bd9635d":"# text preprocessing functions \ndef text_preprocessing(text):\n    \n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    \n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    remove_stopwords = [word for word in tokenized_text if word not in stopwords.words('english')]\n    combined_text = ' '.join(remove_stopwords)\n    return combined_text","2f1daa4e":"# CountVectorizer can do all the above task of preprocessing, tokenization, and stop words removal\n\ncount_vectorizer = CountVectorizer()\ntrain_vectors = count_vectorizer.fit_transform(df_train['essay'])\ntest_vectors = count_vectorizer.transform(df_test['essay'])\n\n# Keeping only non-zero elements to preserve spaces\nprint(train_vectors[0].todense())","656cd90b":"# TfidfVectorizer\n\ntfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\ntrain_tfidf = tfidf.fit_transform(df_train['essay'])\ntest_tfidf = tfidf.transform(df_test['essay'])","cdec3126":"# Let's implement simple classifiers\n\nclassifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(n_neighbors=1),\n    \"Support Vector Classifier\": SVC(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n    \"MultinimialNB\": MultinomialNB()\n}","57069ece":"# Using the KNeighbors Classifiers\n\nfrom sklearn.model_selection import cross_val_score\n\nclassifier = KNeighborsClassifier()\n\nclassifier.fit(train_vectors, df_train[\"domain1_score\"])\ntraining_score = cross_val_score(classifier, train_vectors, df_train[\"domain1_score\"], cv=5)\nprint(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","426744a3":"# Using the Logistic Regression\n\nfrom sklearn.model_selection import cross_val_score\n\nclassifier2 = LogisticRegression()\n\nclassifier2.fit(train_vectors, df_train[\"domain1_score\"])\ntraining_score = cross_val_score(classifier2, train_vectors, df_train[\"domain1_score\"], cv=5)\nprint(\"Classifiers: \", classifier2.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","1bc4edc5":"# Using the XGBoost\n\nimport xgboost as xgb\nfrom sklearn import model_selection\nclf_xgb = xgb.XGBClassifier(\n    learning_rate=0.1,\n    n_estimators=3000,\n    max_depth=15,\n    min_child_weight=1,\n    gamma=0,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    objective='multi:softmax',\n    nthread=42,\n    scale_pos_weight=1,\n    seed=27)\n\nscores = model_selection.cross_val_score(clf_xgb, train_vectors, df_train[\"domain1_score\"], cv=5, scoring=\"f1\")","e9e110ad":"There is 30 rating points ranging from 0 to 30 excluding the 29 which have been given by rater1 to differen essay sests","c2cd38d8":"### Cleaning the Text","86214274":"## Data Preprocessing","43578cf0":"There is 29 rating points ranging from 0 to 30 excluding the 28 and 29 which have been given by rater2 to differen essay sests","3eb9c08e":"The minimum score that was scored by any essay is 0","e5811228":"So there is 8 unique set of essay in the given training set","a343dcbd":"## Visualization","d5ba6a8e":"The maximum score that an essay scored is 60","1b9c11bc":"From the above Pie-Chart we can see that set 8 was the least distributed set ","cc266606":"From the above plot we can see that a large section of the essay rated by rater1 received just rating as 3,2,4 and 1","6e2233a7":"Data preprocessing is one of the critical steps in any machine learning project. It includes cleaning and formatting the data before feeding into a machine learning algorithm. For NLP, the preprocessing steps are comprised of the following tasks:\n\n* Tokenizing the string\n* Lowercasing\n* Removing stop words and punctuation\n* Stemming\n* Lemmatization","79638052":"From the above plot we can see that a large section of the essay rated by rater2 received just rating as 3,2,4 and 1","16e918f1":"## Normalizing the Tokens and Stemming","78cf163c":"## Transforming tokens to Vector","fc97165b":"## Stopwords\n\nThe next step is to remove stop words. Stop words are words that don't add significant meaning to the text.","df4eaeb2":"## Tokenizing","45df71cb":"### Plotting the WordCloud","dbb46ae7":"## Building the Final Model"}}