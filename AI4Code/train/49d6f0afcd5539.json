{"cell_type":{"cc41b761":"code","7bf79551":"code","1fc15089":"code","238798ca":"code","7f6cc2d4":"code","6e174283":"code","0c6be4b6":"code","e0cdc71c":"code","569a325a":"code","9c3208d0":"code","47a70406":"markdown","d71e2ab7":"markdown","f8122946":"markdown","debe76db":"markdown","f82777af":"markdown","ec27ad49":"markdown","b886acaa":"markdown","7416070b":"markdown","79c03104":"markdown","e7c47101":"markdown"},"source":{"cc41b761":"from __future__ import print_function, division\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport os\nimport time\nimport copy","7bf79551":"data_transforms = {\n    'train': transforms.Compose([ \n        transforms.RandomResizedCrop(256),              # It crops the images to a square of 256 px a side.\n        transforms.RandomHorizontalFlip(),              # It flips images horizontally.\n        transforms.RandomAffine(90, scale = (1, 1.5)),  # RandomAffine rotates images to some certain degrees, it fills space with black gradient.\n        transforms.ToTensor(),                                   \n        transforms.Normalize([.485, .456, .406], [ .229, .224, .225]) # This normalizes the Tensors of the Images using ImageNets' presets.\n    ]),\n    'val': transforms.Compose([\n        transforms.RandomResizedCrop(256),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomAffine(90, scale = (1, 1.5)),\n        transforms.ToTensor(),\n        transforms.Normalize([.485, .456, .406], [ .229, .224, .225])\n    ]),\n}","1fc15089":"DataDir = '..\/input\/cotton-disease-dataset\/Cotton Disease'\n\nimages = {x: datasets.ImageFolder(os.path.join(DataDir, x), data_transforms[x])\n          for x in ['train', 'val']}\n\n# This loads data for the model:\ndataloaders = {x: torch.utils.data.DataLoader(images[x], batch_size = 4, shuffle = True, num_workers = 4)\n               for x in ['train', 'val']}\n\n# These are the number of images in both, training and validation sets:\ndataset_sizes = {x: len(images[x]) for x in ['train', 'val']}\n\n# These refer to the categories in the data:\nclass_names = images['train'].classes\n","238798ca":"# Defining a function to visualize the model:\ndef visualize_model(model, num_images=6):\n    was_training = model.training\n    model.eval()\n    images_so_far = 0\n    fig = plt.figure()\n\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(dataloaders['val']):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n\n            for j in range(inputs.size()[0]):\n                images_so_far += 1\n                ax = plt.subplot(num_images\/\/2, 2, images_so_far)\n                ax.axis('off')\n                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n                imshow(inputs.cpu().data[j])\n\n                if images_so_far == num_images:\n                    model.train(mode=was_training)\n                    return\n        model.train(mode=was_training)","7f6cc2d4":"def imshow(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)\n\ninputs, classes = next(iter(dataloaders['train']))\n\n\nout = torchvision.utils.make_grid(inputs)\n\n\nimshow(out, title = [class_names[x]  for x in classes])","6e174283":"# def a function for the latter:\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss \/ dataset_sizes[phase]\n            epoch_acc = running_corrects.double() \/ dataset_sizes[phase]\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model","0c6be4b6":"# Getting our beloved CUDA:\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","e0cdc71c":"# Using transfer learning or using a pretrained ResNet50, same thing.\nmodel_ft = models.resnet50(pretrained=True)\n\nnum_ftrs = model_ft.fc.in_features\n\nmodel_ft.fc = nn.Linear(num_ftrs, 4)\n\n# Handing the model to our Device:\nmodel_ft = model_ft.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that all parameters are being optimized:\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs:\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)","569a325a":"#TRAINING THE MODEL FINALLY:\nmodel_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n                      num_epochs = 10)","9c3208d0":"visualize_model(model_ft)","47a70406":"# Defining Directories:\nThe directories are defined as where the images are.","d71e2ab7":"# Visualizing:\nOkay, so we have to define a function which predicts on new images that the model has not seen after it is done training on the given data, lets make that and lets visualize what we have here in the data too.","f8122946":"![](https:\/\/hdwallpaperim.com\/wp-content\/uploads\/2017\/08\/23\/459593-flowers-plants-leaves-748x468.jpg)\n\nI had never tried coding up a deep learning model using PyTorch. I had always stuck to FastAI Library for producing best in class computer vision models. That was the usual for me for mainly two reasons:\n   1. It takes way less code to get a model to work\n   2. It works insanely good with state-of-the-art accuracies.\n\nBut keep this in mind, FastAI is built on top of PyTorch. It is consists of perfect combination of image transformation presets. These presets are highly generalised, that is, they work for virtually any sort of dataset which is being used under Computer Vision. I really wanted to see why is PyTorch so hyped after all.","debe76db":"# Table of Content\n\n1. Importing Libararies\n2. Defining Transformations\n3. Defining the Directories\n4. Visualizing\n5. Training the model\n6. Predicting","f82777af":"Thank you for going through my note book! :D \n\nShameless self promotion ahead:\nIf you liked this, maybe upvote or give me a follow on kaggle?\nYou can hit me up at these places too:\n\n[**LinkedIn**](https:\/\/www.linkedin.com\/in\/vyom-bhatia-40ba79181\/)\n                               \n\n\n[**Instagram**](https:\/\/www.instagram.com\/vyombhatia\/)","ec27ad49":"# Predicting:\nI mean why not we got 98.02% accuracy:","b886acaa":"# Defining Transformations\nTransformations here refer to playing with images. Like, seriously the pytorch library is like: it seems like you don't have many images so what if I flip them around a bit to make it challenging for the model to overfit on this data? This way, it helps us get more images.","7416070b":"## Training the Model:\nFinally we are training the model!","79c03104":"# Importing Libararies:\n\nImporting necessary libraries for very obvious reasons:","e7c47101":"Finally lets end the suffering and take a look at the images, a bit of angry text there but bear with me:"}}