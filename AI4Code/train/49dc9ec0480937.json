{"cell_type":{"2642aa33":"code","00959851":"code","668285a2":"code","e5615cde":"code","84ede818":"code","93613b06":"code","54879df6":"code","ea7afd3e":"code","3b43fd5e":"code","0b9d8e46":"code","a6eec48c":"code","df1e919d":"code","4e0f4726":"code","7e28eee6":"code","691e477b":"code","726143db":"code","765f620b":"code","e364218e":"code","ef5c3fb0":"code","d140b8bc":"code","74982782":"code","e36c08ac":"code","f3b6b154":"code","cc3873e3":"code","72169196":"code","10d0aed6":"code","ad69b874":"code","eaf74da2":"code","1110297d":"code","7dff890e":"code","9ccf75a9":"code","ca34458a":"code","b5d55eea":"code","743ea675":"code","bf9fcda9":"code","e3da6885":"code","ec69e445":"code","69f03a65":"code","7dd454b1":"code","6a34ffcd":"code","bd9c6beb":"code","b3d90d23":"code","708d47bd":"code","018e1230":"code","52515d3d":"code","ef35b214":"code","02b08885":"code","52717261":"code","b3c56d54":"code","6ec5c121":"code","73986e69":"markdown","d442ffff":"markdown","dc83e54d":"markdown","523110a1":"markdown","dbe57674":"markdown","c0fb487e":"markdown","7322ce80":"markdown","103205c5":"markdown"},"source":{"2642aa33":"# Fehim Alt\u0131n\u0131\u015f\u0131k\n# fehim.altinisik@gmail.com\n# 160201010","00959851":"import os\nimport sys\nimport math\n\nfrom operator import attrgetter\n\nimport numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.python.client import device_lib\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import Normalizer","668285a2":"\nprint(device_lib.list_local_devices())","e5615cde":"os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"","84ede818":"item_categories = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")\nitems = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/items.csv\")\nsales_train = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\nsample_submissions = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv\")\nshops = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/shops.csv\")\ntest = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/test.csv\")","93613b06":"print(item_categories.shape)\nprint(items.shape)\nprint(sales_train.shape)\nprint(sample_submissions.shape)\nprint(shops.shape)\nprint(test.shape)","54879df6":"with pd.option_context('display.max_rows', 5, 'display.max_columns', 185):\n    display(sales_train.sample(5))","ea7afd3e":"print(test[\"shop_id\"].nunique())\nprint(test[\"item_id\"].nunique())","3b43fd5e":"print(len(test[\"shop_id\"].unique()))\nprint(len(sales_train[\"shop_id\"].unique()))\nprint(len(set(test[\"shop_id\"]).intersection(set(sales_train[\"shop_id\"]))))\n\nprint(len(test[\"item_id\"].unique()))\nprint(len(sales_train[\"item_id\"].unique()))\nprint(len(set(test[\"item_id\"]).intersection(set(sales_train[\"item_id\"]))))","0b9d8e46":"sales_train = sales_train.assign(\n    date_to_datetime=pd.to_datetime(sales_train[\"date\"])\n)","a6eec48c":"sales_train = sales_train.assign(\n    date_to_datetime_year=sales_train[\"date_to_datetime\"].dt.year,\n    date_to_datetime_month=sales_train[\"date_to_datetime\"].dt.month,\n    date_to_datetime_day=sales_train[\"date_to_datetime\"].dt.day\n)","df1e919d":"sales_train = sales_train.assign(\n    item_cnt_day_as_int=sales_train[\"item_cnt_day\"].astype(np.int32)\n)","4e0f4726":"sales_train = sales_train.merge(items.loc[:, [\"item_id\", \"item_category_id\"]], on=\"item_id\")","7e28eee6":"sales_train.head()","691e477b":"print(sales_train[\"item_category_id\"].nunique())","726143db":"def shop_feature(shop_data):\n    features = {}\n    \n    first_record = shop_data[\"date_to_datetime\"].min()\n    last_record = shop_data[\"date_to_datetime\"].max()\n    \n    features[\"first_record\"] = first_record\n    features[\"last_record\"] = last_record\n    \n    lifetime = last_record - first_record\n    features[\"shop_lifetime\"] = int(lifetime.days \/ 30)\n    \n    return pd.Series(features, index=[\"first_record\", \"last_record\", \"shop_lifetime\"])\n\nshop_features = sales_train.groupby(\"shop_id\").apply(shop_feature)","765f620b":"print(shop_features.head())","e364218e":"with pd.option_context('display.max_rows', 10, 'display.max_columns', 185):\n    display(sales_train.set_index([\"shop_id\", \"item_category_id\"]).sort_values(\"date_to_datetime\").head())","ef5c3fb0":"block_based_grouping = pd.pivot_table(sales_train, index=[\"shop_id\", \"item_category_id\"], columns=\"date_block_num\", values=\"item_cnt_day\", aggfunc=\"mean\")","d140b8bc":"print(block_based_grouping.shape)","74982782":"block_based_grouping = block_based_grouping.fillna(method='ffill', axis=\"columns\")","e36c08ac":"block_based_grouping = block_based_grouping.fillna(0)","f3b6b154":"with pd.option_context('display.max_rows', 10, 'display.max_columns', 185):\n    display(block_based_grouping.sample(10))\n    # display(testgroup.head())","cc3873e3":"def generate_training_series(data, size_of_sets):\n        \n    # index =  pd.Index((data.name, ) * 5)\n    index =  np.array((data.name, ) * 7)\n    index_extension =  np.arange(\n                (\n                    math.floor(data.shape[0] \/ size_of_sets) + 1\n                )\n        )\n    \n    new_index = np.column_stack((index, index_extension))\n    \n    # print(index)\n    # print(index_extension)\n    # print(new_index)\n    \n    # print(np.transpose(new_index))# new_index = pd.MultiIndex(new_index)\n    new_index = pd.MultiIndex.from_arrays(np.transpose(new_index), names=('shop_id', 'item_id', \"period_id\"))\n    # print(new_index)\n    \n    samples = pd.DataFrame(np.nan, index=new_index, columns=np.arange(size_of_sets).tolist() + [\"y\"])\n    \n    # print(samples)\n    \n    cursor = 0\n    counter = 0\n    \n    while cursor < data.shape[0]:\n        \n        if cursor + size_of_sets > data.shape[0]:\n            # print(\"x: {}:{}, y: {}\".format(data.shape[0] - size_of_sets - 2, data.shape[0] - 2, data.shape[0] - 1))\n            samples.loc[samples.index.get_level_values('period_id') == counter, samples.columns[0: size_of_sets]] = data.loc[data.shape[0] - size_of_sets - 1: data.shape[0] - 2].values\n            samples.loc[samples.index.get_level_values('period_id') == counter, samples.columns[size_of_sets]] = data.loc[data.shape[0] - 1]\n            break\n            \n        # print(samples.loc[samples.index.get_level_values('period_id') == counter, samples.columns[0: size_of_sets]])\n        # print(data.loc[cursor: cursor + size_of_sets - 1])\n        \n        samples.loc[samples.index.get_level_values('period_id') == counter, samples.columns[0: size_of_sets]] = data.loc[cursor: cursor + size_of_sets - 1].values\n        samples.loc[samples.index.get_level_values('period_id') == counter, samples.columns[size_of_sets]] = data.loc[cursor + size_of_sets]\n            \n        # print(\"x: {}:{}, y: {}\".format(cursor, cursor + size_of_sets, cursor + size_of_sets + 1))\n        \n        cursor += size_of_sets\n        counter += 1\n    \n    return samples\n    ","72169196":"training_set = pd.concat({i: generate_training_series(row, 5) for i, row in block_based_grouping.iterrows()})","10d0aed6":"training_set= training_set.reset_index((0, 1), drop=True)","ad69b874":"normalizer = Normalizer().fit(training_set[training_set.columns[0:5]])","eaf74da2":"training_set_normalized = normalizer.transform(training_set[training_set.columns[0:5]])","1110297d":"training_set_multi_input = np.concatenate((training_set_normalized, training_set.index.get_level_values(0).values.reshape(-1, 1), training_set.index.get_level_values(1).values.reshape(-1, 1)), axis=1)","7dff890e":"x_train, x_test, y_train, y_test = train_test_split(training_set_multi_input, training_set[training_set.columns[5]], test_size=0.33, random_state=42)","9ccf75a9":"embedding_size = len(sales_train[\"shop_id\"].unique()) * len(sales_train[\"item_id\"].unique())\n\nlstm_input = keras.Input(shape=(5, 1), name=\"sales\")\nmemory = layers.LSTM(120, input_shape=(5, 1))\nx_memory = memory(lstm_input)\n# memory_outputs = layers.Dense(1)(x_memory)\n\nembedding_input= keras.Input(shape=(None,), name=\"shop_item\")\nembedding_features = layers.Embedding(embedding_size, 128)(embedding_input)\nembedding_features = layers.LSTM(256)(embedding_features)\n\nconcat_x = layers.concatenate([embedding_features, x_memory])\n\noutputs = layers.Dense(1, name=\"sale_count\")(concat_x)\n\nmodel = keras.Model(inputs=[lstm_input, embedding_input], outputs=outputs)\n\nmodel.summary()\nkeras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)","ca34458a":"model.compile(\n    loss=keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mean_squared_error\"),\n    optimizer=keras.optimizers.Adam(lr=0.005),\n    metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')],\n)\n\nhistory = model.fit(\n    {\"sales\": np.expand_dims(x_train[:, 0:5], 2), \"shop_item\": (x_train[:, 5:7])},\n    {\"sale_count\": y_train},\n    batch_size=8,\n    epochs=3,\n    validation_split=0.2\n)\n\ntest_scores = model.evaluate(\n    {\"sales\": np.expand_dims(x_test[:, 0:5], 2), \"shop_item\": (x_test[:, 5:7])},\n    y_test, verbose=2\n)\nprint(\"Test loss:\", test_scores[0])\nprint(\"Test mse:\", test_scores[1])","b5d55eea":"# model.save(\"sales_forecast_model\")","743ea675":"### Fetch Categories\n\ntest_cases = test.merge(items.loc[:, [\"item_id\", \"item_category_id\"]], on=\"item_id\")\nprint(test_cases.shape)\nprint(test_cases.head())","bf9fcda9":"test_cases_fetch_data = test_cases.merge(\n    block_based_grouping.loc[:, block_based_grouping.columns[28:33]],\n    left_on=[\"shop_id\", \"item_category_id\"],\n    right_index=True,\n    how=\"left\"\n)","e3da6885":"print(test_cases_fetch_data.shape)","ec69e445":"with pd.option_context('display.max_rows', 10, 'display.max_columns', 185):\n    display(test_cases_fetch_data.head(10))","69f03a65":"print(test_cases[[\"shop_id\", \"item_category_id\"]].duplicated().any())","7dd454b1":"test_cases_fetch_data = test_cases_fetch_data.fillna(method='ffill', axis=\"columns\")","6a34ffcd":"test_cases_fetch_data = test_cases_fetch_data.fillna(0)","bd9c6beb":"prediction_block_data = normalizer.transform(test_cases_fetch_data[test_cases_fetch_data.columns[4:9]])","b3d90d23":"prediction_data = np.concatenate((prediction_block_data, test_cases_fetch_data[[\"shop_id\", \"item_category_id\"]].values), axis=1)","708d47bd":"print(prediction_data.shape)","018e1230":"test_scores = model.predict(\n    {\"sales\": np.expand_dims(prediction_data[:, 0:5], 2), \"shop_item\": (prediction_data[:, 5:7])}\n)","52515d3d":"print(test_scores.shape)","ef35b214":"submissions = pd.DataFrame(data=test_scores, index=test_cases_fetch_data[\"ID\"].values, columns=[\"item_cnt_month\"])","02b08885":"submissions.index.name = \"ID\"","52717261":"print(submissions.head())","b3c56d54":"submissions.index = submissions.index.astype(np.int64)","6ec5c121":"# submissions.to_csv(\"submission.csv\", index=True, index_label=\"ID\", float_format=\"%.3f\")","73986e69":"### Step 1: Data Loading ond Assurance","d442ffff":"Select GPU","dc83e54d":"### Step 5: Evaluation","523110a1":"Inspecting Test Cases","dbe57674":"Following training operation takes more then 2 hours on a NVidia Pascal Architecture GPU.","c0fb487e":"### Step 4: Training","7322ce80":"### Step 2: Data Inspection","103205c5":"### Step 3: Data Transformation"}}