{"cell_type":{"e0330c80":"code","d0e69a1a":"code","e434df64":"code","9edb5804":"code","8d8cb00c":"code","ebb9f884":"code","dcfb6dcb":"code","e5fa7ba0":"code","4a3a3c4a":"code","46a6b7f8":"code","f6eab5ee":"code","70c872cf":"code","a7d5829d":"code","de72d3f4":"code","7280d2f0":"code","d90f69a0":"code","17002e42":"code","b1817941":"code","3bc25da2":"code","8bc010ed":"code","312e0c1e":"code","470ccac9":"code","754ac5f2":"code","89e0b6f6":"code","d6b9a3f1":"code","3255fb37":"code","1d6443af":"code","643a53bd":"code","cc36a77c":"code","4274c2a7":"code","fae59011":"code","7378d1b8":"code","ba43e7fa":"code","50adf659":"code","41e1c597":"code","e07d1a59":"code","19e756a0":"code","0c08c53a":"code","b5d8f80a":"code","1c092ac8":"code","94d6162c":"code","db48793c":"code","f9159050":"code","f66c58d1":"code","99db85a7":"code","99971d1e":"code","75634fca":"code","3b3a8e5e":"markdown","c62260ad":"markdown","1f204492":"markdown","20faf000":"markdown","7a883b8e":"markdown","c6583d02":"markdown","d5bd6aff":"markdown","76abc4d2":"markdown","132ead28":"markdown","24939816":"markdown","4c22da01":"markdown","3f330f61":"markdown","9c7c7c0a":"markdown","568a207f":"markdown","e105b98b":"markdown","a1713412":"markdown","7943e77a":"markdown","e1b65edb":"markdown","0813b633":"markdown","9f18e8d5":"markdown","bed9a96f":"markdown","fae688de":"markdown","740d7701":"markdown","ca078b55":"markdown","dc21592d":"markdown","6752e1ae":"markdown","a3c5b9ba":"markdown","b2bc0629":"markdown","6dd9134c":"markdown","8d3bb03b":"markdown","a7f83545":"markdown","8a0dd045":"markdown","a1c28c42":"markdown","75615d57":"markdown","62b8626b":"markdown","5d477d0f":"markdown","598f5297":"markdown","a17cc20b":"markdown","dfc402a2":"markdown","c8af3871":"markdown","81e48944":"markdown","b9d926dc":"markdown","c423ac26":"markdown","26a2a2b2":"markdown"},"source":{"e0330c80":"import pandas as pd\nfrom pandas import DataFrame\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score","d0e69a1a":"data = pd.read_csv('..\/input\/iphone-purchase-records\/iphone_purchase_records.csv')","e434df64":"data.head()","9edb5804":"X = data.iloc[:,:-1].values\ny = data.iloc[:, 3].values","8d8cb00c":"#converting gender to number\nlabelEncoder_gender =  LabelEncoder()\nX[:,0] = labelEncoder_gender.fit_transform(X[:,0])","ebb9f884":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)","dcfb6dcb":"dt = DecisionTreeClassifier(random_state=42)","e5fa7ba0":"dt.fit(X_train,y_train)","4a3a3c4a":"y_pred_dt = dt.predict(X_test)","46a6b7f8":"y_pred_dt","f6eab5ee":"f1_score_dt = metrics.f1_score(y_test,y_pred_dt,average='macro')\nprint('F1 score: {0:f}'.format (f1_score_dt))\nprint(classification_report(y_test,y_pred_dt))\nprint(confusion_matrix(y_test,y_pred_dt))\nsdt = precision_score(y_test, y_pred_dt, average='macro')\nprint('precision score: {0:f}'.format (sdt))\ns_dt = recall_score(y_test,y_pred_dt, average='macro')\nprint('recall score: {0:f}'.format (s_dt))\nscore_dt = metrics.accuracy_score(y_test,dt.predict(X_test))\nprint('Accuracy:{0:f}'.format(score_dt))","70c872cf":"rf = RandomForestClassifier(n_estimators=70, oob_score=True, n_jobs=-1, min_samples_leaf=30)","a7d5829d":"rf.fit(X_train, y_train)","de72d3f4":"y_pred_rf = rf.predict(X_test)","7280d2f0":"y_pred_rf","d90f69a0":"f1_score_rf = metrics.f1_score(y_test,y_pred_rf,average='macro')\nprint('F1 score: {0:f}'.format (f1_score_rf))\nprint(classification_report(y_test,y_pred_rf))\nprint(confusion_matrix(y_test,y_pred_rf))\nsrf = precision_score(y_test, y_pred_rf, average='macro')\nprint('precision score: {0:f}'.format (srf))\ns_rf = recall_score(y_test,y_pred_rf, average='macro')\nprint('recall score: {0:f}'.format (s_rf))\nscore_rf = metrics.accuracy_score(y_test,rf.predict(X_test))\nprint('Accuracy:{0:f}'.format(score_rf))","17002e42":"#feature scaling\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","b1817941":"lr = LogisticRegression(random_state=0, solver=\"liblinear\")\nlr.fit(X_train, y_train)","3bc25da2":"y_pred_lr = lr.predict(X_test)","8bc010ed":"y_pred_lr","312e0c1e":"f1_score_lr = metrics.f1_score(y_test,y_pred_lr,average='macro')\nprint('F1 score: {0:f}'.format (f1_score_lr))\nprint(classification_report(y_test,y_pred_lr))\nprint(confusion_matrix(y_test,y_pred_lr))\nslr = precision_score(y_test, y_pred_lr, average='macro')\nprint('precision score: {0:f}'.format (slr))\ns_lr = recall_score(y_test,y_pred_lr, average='macro')\nprint('recall score: {0:f}'.format (s_lr))\nscore_lr = metrics.accuracy_score(y_test,lr.predict(X_test))\nprint('Accuracy:{0:f}'.format(score_lr))","470ccac9":"nb = GaussianNB()\nnb.fit(X_train, y_train)","754ac5f2":"y_pred_nb = nb.predict(X_test)","89e0b6f6":"y_pred_nb","d6b9a3f1":"f1_score_nb = metrics.f1_score(y_test,y_pred_nb,average='macro')\nprint('F1 score: {0:f}'.format (f1_score_nb))\nprint(classification_report(y_test,y_pred_nb))\nprint(confusion_matrix(y_test,y_pred_nb))\nsnb = precision_score(y_test, y_pred_nb, average='macro')\nprint('precision score: {0:f}'.format (snb))\ns_nb = recall_score(y_test,y_pred_nb, average='macro')\nprint('recall score: {0:f}'.format (s_nb))\nscore_nb = metrics.accuracy_score(y_test,nb.predict(X_test))\nprint('Accuracy:{0:f}'.format(score_nb))","3255fb37":"sgd = SGDClassifier(loss='modified_huber', shuffle=True, random_state=42)","1d6443af":"sgd.fit(X_train,y_train)","643a53bd":"y_pred_sgd = sgd.predict(X_test)","cc36a77c":"y_pred_sgd","4274c2a7":"f1_score_sgd = metrics.f1_score(y_test,y_pred_sgd,average='macro')\nprint('F1 score: {0:f}'.format (f1_score_sgd))\nprint(classification_report(y_test,y_pred_sgd))\nprint(confusion_matrix(y_test,y_pred_sgd))\nssgd = precision_score(y_test, y_pred_sgd, average='macro')\nprint('precision score: {0:f}'.format (ssgd))\ns_sgd = recall_score(y_test,y_pred_sgd, average='macro')\nprint('recall score: {0:f}'.format (s_sgd))\nscore_sgd = metrics.accuracy_score(y_test,sgd.predict(X_test))\nprint('Accuracy:{0:f}'.format(score_sgd))","fae59011":"knn = KNeighborsClassifier(n_neighbors=15)","7378d1b8":"knn.fit(X_train,y_train)","ba43e7fa":"y_pred_knn = knn.predict(X_test)","50adf659":"y_pred_knn","41e1c597":"f1_score_knn = metrics.f1_score(y_test,y_pred_knn,average='macro')\nprint('F1 score: {0:f}'.format (f1_score_knn))\nprint(classification_report(y_test,y_pred_knn))\nprint(confusion_matrix(y_test,y_pred_knn))\nsknn = precision_score(y_test, y_pred_knn, average='macro')\nprint('precision score: {0:f}'.format (sknn))\ns_knn = recall_score(y_test,y_pred_knn, average='macro')\nprint('recall score: {0:f}'.format (s_knn))\nscore_knn = metrics.accuracy_score(y_test,knn.predict(X_test))\nprint('Accuracy:{0:f}'.format(score_knn))","e07d1a59":"svm = SVC(kernel='linear', C=0.025, random_state=42)","19e756a0":"svm.fit(X_train,y_train)","0c08c53a":"y_pred_svm = svm.predict(X_test)","b5d8f80a":"y_pred_svm","1c092ac8":"f1_score_svm = metrics.f1_score(y_test,y_pred_svm,average='macro')\nprint('F1 score: {0:f}'.format (f1_score_svm))\nprint(classification_report(y_test,y_pred_svm))\nprint(confusion_matrix(y_test,y_pred_svm))\nssvm = precision_score(y_test, y_pred_svm, average='macro')\nprint('precision score: {0:f}'.format (ssvm))\ns_svm = recall_score(y_test,y_pred_svm, average='macro')\nprint('recall score: {0:f}'.format (s_svm))\nscore_svm = metrics.accuracy_score(y_test,svm.predict(X_test))\nprint('Accuracy:{0:f}'.format(score_svm))","94d6162c":"algo = ['RF','DT','LR','NB','SGD','KNN','SVM']\nscore = [score_rf, score_dt, score_lr, score_nb, score_sgd, score_knn, score_svm]\nc = ['Red','Blue','Green', 'Brown', 'Black', 'Purple', 'Olive']\nplt.figure(figsize=(10,6))\nplt.bar(algo,score,width=0.5, color=c),\nplt.title('Evaluating accuracies')\nplt.ylabel('Accuracy score')\nplt.ylim(0,1)\nplt.show()","db48793c":"algo = ['RF','DT','LR','NB','SGD','KNN','SVM']\nscore = [f1_score_rf, f1_score_dt, f1_score_lr, f1_score_nb, f1_score_sgd, f1_score_knn, f1_score_svm]\nc = ['Red','Blue','Green', 'Brown', 'Black', 'Purple', 'Olive']\nplt.figure(figsize=(10,6))\nplt.bar(algo,score,width=0.5, color=c)\nplt.title('Evaluating F1 score')\nplt.ylabel('F1 score')\nplt.ylim(0,1)\nplt.show()","f9159050":"algo = ['RF','DT','LR','NB','SGD','KNN','SVM']\nscore = [srf,sdt,slr,snb,ssgd,sknn,ssvm]\nc = ['Red','Blue','Green', 'Brown', 'Black', 'Purple', 'Olive']\nplt.figure(figsize=(10,6))\nplt.bar(algo,score,width=0.5, color=c)\nplt.title('Evaluating Precision')\nplt.ylabel('Precision score')\nplt.ylim(0,1)\nplt.show()","f66c58d1":"algo = ['RF','DT','LR','NB','SGD','KNN','SVM']\nscore = [s_rf,s_dt,s_lr,s_nb,s_sgd,s_knn,s_svm]\nc = ['Red','Blue','Green', 'Brown', 'Black', 'Purple', 'Olive']\nplt.figure(figsize=(10,6))\nplt.bar(algo,score,width=0.5, color=c)\nplt.title('Evaluating Recall score')\nplt.ylabel('Recall score')\nplt.ylim(0,1)\nplt.show()","99db85a7":"from sklearn.metrics import roc_curve, auc","99971d1e":"fp1, tp1, thresholds1 = roc_curve(y_test, y_pred_rf[:])\nroc_auc_model1 = auc(fp1, tp1)\nfp2, tp2, thresholds2 = roc_curve(y_test, y_pred_dt[:])\nroc_auc_model2 = auc(fp2, tp2)\nfp3, tp3, thresholds3 = roc_curve(y_test, y_pred_lr[:])\nroc_auc_model3 = auc(fp3, tp3)\nfp4, tp4, thresholds4 = roc_curve(y_test, y_pred_nb[:])\nroc_auc_model4 = auc(fp4, tp4)\nfp5, tp5, thresholds5 = roc_curve(y_test, y_pred_sgd[:])\nroc_auc_model5 = auc(fp5, tp5)\nfp6, tp6, thresholds6 = roc_curve(y_test, y_pred_knn[:])\nroc_auc_model6 = auc(fp6, tp6)\nfp7, tp7, thresholds7 = roc_curve(y_test, y_pred_svm[:])\nroc_auc_model7 = auc(fp7, tp7)\n\nprint(\"AUC for Random Forest Model : \",roc_auc_model1)\nprint(\"AUC for Decision Tree Model:\", roc_auc_model2)\nprint(\"AUC for Logistic Regression Model :\" ,roc_auc_model3)\nprint(\"AUC for Naive Bayes Model :\" ,roc_auc_model4)\nprint(\"AUC for Stochastic Gradient Descent Model :\" ,roc_auc_model5)\nprint(\"AUC for K-Nearest Neighbours Model :\" ,roc_auc_model6)\nprint(\"AUC for Support Vector Machine Model :\" ,roc_auc_model7)","75634fca":"plt.figure(figsize=(10,6))\nplt.clf()\nplt.plot(fp1, tp1, label='Random Forest Model (area = %0.2f)' % roc_auc_model1)\nplt.plot(fp2, tp2, label='Decision Tree Model (area = %0.2f)' % roc_auc_model2)\nplt.plot(fp3, tp3, label='Logistic Regression Model (area = %0.2f)' %roc_auc_model3)\nplt.plot(fp4, tp4, label='Naive Bayes Model (area = %0.2f)' %roc_auc_model4)\nplt.plot(fp5, tp5, label='Stochastic Gradient Descent Model (area = %0.2f)' %roc_auc_model5)\nplt.plot(fp6, tp6, label='K-Nearest Neighbours Model (area = %0.2f)' %roc_auc_model6)\nplt.plot(fp7, tp7, label='Support Vector Machine Model (area = %0.2f)' %roc_auc_model7)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Performance Plot')\nplt.legend(loc=\"lower right\")\nplt.show()","3b3a8e5e":"Performance Matrics","c62260ad":"Comparing recall score","1f204492":"Coparing precision score","20faf000":"# Naive Bayes","7a883b8e":"Let's cunvert gender to number","c6583d02":"\n            Comapring 7 most commonly used Classification Algorithums\n            \n            \n            Karthik B S\n            IIIT Kottayam, Kerala, India\n                                                                                                ","d5bd6aff":"Let's plot the ROC-AUC graph to check the best performing model.","76abc4d2":"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html?highlight=liblinear","132ead28":"# K-Nearest Neighbours","24939816":"Dataset: https:\/\/www.kaggle.com\/rohitphadke\/iphone-purchase-records","4c22da01":"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_curve.html?highlight=roc_curve#sklearn.metrics.roc_curve","3f330f61":"Our goal in this project is to impliment different classification algorithms namely Logistic Regression, Naive Bayes, Stochastic Gradient Desent, K-Nearest Neighbours, Decision Tree, Random Forest and Support Vector Machine on records of customers who purchased or did not purchase an iPhone and comparing the performance of these algorithms using Performance metrics and ROC-AUC Curve.","9c7c7c0a":"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier","568a207f":"# Plotting the performance","e105b98b":"Performance Matrics","a1713412":"From the above graph the Random Forest model captures the highest AUC and can be considered as the best performing model among the other 7 models. This way we can compute and compare different predictive models.","7943e77a":"https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#classification-metrics\n    \nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.classification_report.html?highlight=classification_report#sklearn.metrics.classification_report","e1b65edb":"In this assignment, we saw how to implement and compare different classification models. We first implemented the 3 different algorithms on the iPhone purchase records dataset and compared these different models","0813b633":"Comparing Accuracies","9f18e8d5":"Comparing F1 Scores","bed9a96f":"For comparing the performance, we make use of the Receiver Characteristics Curve \u2013 Area Under Curve that is plotted between True positive and False positive rates, where true positive is totally positive and false positive is a total negative. The area under the curve (AUC) is the summary of this curve that tells about how good a model is when we talk about its ability to generalize.","fae688de":"https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#classification-metrics\n    \nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.classification_report.html?highlight=classification_report#sklearn.metrics.classification_report","740d7701":"# Decision Tree","ca078b55":"            7 Types of Classification Algorithms :-\n                * Logistic Regression\n                * Naive Bayes\n                * Stochastic Gradient Descent\n                * K-Nearest Neighbours\n                * Decision Tree\n                * Random Forest\n                * Support Vector Machine","dc21592d":"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html?highlight=train_test_split#sklearn.model_selection.train_test_split","6752e1ae":"# Logistic regression","a3c5b9ba":"https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#classification-metrics\n    \nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.classification_report.html?highlight=classification_report#sklearn.metrics.classification_report","b2bc0629":"Performance Metrics","6dd9134c":"Spliting data into training and test set","8d3bb03b":"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html?highlight=decision%20tree#sklearn.tree.DecisionTreeClassifier","a7f83545":"Performance Metrics","8a0dd045":"# Stochastic Gradient Descent","a1c28c42":"https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#classification-metrics\n    \nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.classification_report.html?highlight=classification_report#sklearn.metrics.classification_report","75615d57":"# Support Vector Machine","62b8626b":"Apart from the Decision Tree and Random Forest classifiers, for the other classifiers we have to do feature scaling. We will use Standard Scaler for this purpose","5d477d0f":"https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#classification-metrics\n    \nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.classification_report.html?highlight=classification_report#sklearn.metrics.classification_report","598f5297":"\n# Performance Matrics Evaluation","a17cc20b":"Performance Metrics","dfc402a2":"Performance Metrics","c8af3871":"https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#classification-metrics\n    \nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.classification_report.html?highlight=classification_report#sklearn.metrics.classification_report","81e48944":"Performance Metrics","b9d926dc":"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html?highlight=randomforestclassifier#sklearn.ensemble.RandomForestClassifier","c423ac26":"# Random Forest","26a2a2b2":"This Dataset is the records of customers who purchased or did not purchase an iPhone along with their respective Gender, Age & Salary. The dataset has around 398 entries. The dataset can be accessed here."}}