{"cell_type":{"a3d5be6b":"code","fd560cd7":"code","97009094":"code","a5402c38":"code","d65a5464":"code","af040625":"code","71156970":"code","7a406b2a":"code","e831033a":"code","ebc1b1dd":"code","d4ea12b3":"code","1ce77792":"code","ce55ca9a":"code","d86f48fb":"code","6b7ef6ca":"code","098baa8f":"code","3cbc4489":"code","1c9ff1d6":"code","fb36fc25":"code","f09ddd99":"code","d8c78f38":"code","b672ae82":"code","6e576235":"code","01784deb":"code","a760d11f":"code","303621ac":"code","51db086b":"code","9eda43f6":"code","c306a63d":"code","ebe81887":"code","cbb95efb":"code","5a79dc9b":"code","463b3850":"code","0b4957c1":"code","51849e10":"code","c69d2275":"code","1196e079":"code","4a91acf3":"code","6b2cf955":"code","ce04780c":"code","1e6bf5b9":"code","8f90f11a":"code","7905ee7e":"code","b3773761":"code","c227e133":"code","5f3739cf":"code","02073b72":"code","52f9664d":"code","9c8c3584":"code","afa39f07":"code","ad7e3fed":"code","74e1db25":"code","626c8300":"code","464ca787":"code","0135664d":"code","92b07095":"code","ad6ead06":"code","1af6d87b":"code","0beca7e2":"code","6fc69aa6":"code","5b0c6763":"code","fde1dedb":"code","634a9cb8":"code","8bb12af4":"code","a099c821":"code","5a60ca59":"code","fc484e88":"code","f3d8977b":"code","54170a93":"code","92f9e215":"code","1707d7c8":"code","2e0f7db2":"code","18fc28ff":"code","ba529aca":"code","48fe35f2":"code","5c47acc6":"code","822a7e9b":"code","ad381138":"code","5cba9b0b":"code","407c0ebf":"code","8fbb74ad":"code","031594a7":"code","ffa84e41":"markdown","8b3c4997":"markdown","2ce5f265":"markdown","6192310e":"markdown","ec11059a":"markdown","4ee5a548":"markdown","4c4ec74e":"markdown","dc7cca53":"markdown","1aade57f":"markdown","15ab1b14":"markdown","9d0fcfca":"markdown","06eea7ad":"markdown","cdcb8a74":"markdown"},"source":{"a3d5be6b":"#Necessary Imports\nimport pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\npd.options.display.float_format = '{:.2f}'.format #Turning off scientific notations\n# import sweetviz","fd560cd7":"# Text Analysis related imports\nimport nltk \nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom wordcloud import WordCloud, STOPWORDS ","97009094":"# Model building Imports\nimport statsmodels.api as sm\nfrom sklearn import preprocessing\nfrom sklearn import tree, metrics\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\n# from sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \n# import pydotplus","a5402c38":"#Data Ingestion\n\ndf=pd.read_csv(\"..\/input\/zomato-restaurants-in-india\/zomato_restaurants_in_India.csv\")\nprint(df.shape)\ndf.head(1).T","d65a5464":"# report = sweetviz.analyze(df)\n# report.show_html('full_data.html')","af040625":"df.duplicated().sum() ","71156970":"df['res_id'].duplicated().sum()","7a406b2a":"df=df.drop_duplicates(subset='res_id') \n#Dropped duplicates on the basis of res_id as res_id is unique for every restaurant and for each branch, after this zipcode will be removed\ndf.shape","e831033a":"(df.isnull().sum() \/ len(df)).sort_values(ascending=False)","ebc1b1dd":"# Zip Code, has to be dropped as 80% values are missing and it is not much of a contributor to the analysis.\n# Locality can very well be used inplace of Zip Code.\n\n# Also, res_id now is just an identifier, and isn't of much use, so dropping this as well.\n\n# country_id is redundant as all restaurants pertain to India only, so dropping it.\n\n# url isn't much help here either, customers will order from Zomato, and Zomato already has all info, so dropping it.\n\n# Address and locality have extra info which isn't required as such because locality_verbose variable is here.\n\n# Dropping city_id, city name is available here\n\n# Currency field has to go, currency is INR only!\n\n# opentable_support has all 0 values, so this should be dropped too","d4ea12b3":"df=df.drop(['res_id','url','country_id','currency', 'address', 'locality','city_id', 'zipcode', 'opentable_support'],axis=1)","1ce77792":"df.describe(include='all').T","ce55ca9a":"df.info()","d86f48fb":"#delivery, takeaway and price_range columns are categorical and are stored as int, so this needs to be fixed.\ndf['delivery'] = df['delivery'].astype(object)\ndf['takeaway'] = df['takeaway'].astype(object)\ndf['price_range'] = df['price_range'].astype(object)","6b7ef6ca":"df['delivery'].value_counts(dropna=False)","098baa8f":"df['takeaway'].value_counts(dropna=False)","3cbc4489":"# Assuming -1 is a data entry error, so -1 is to be encoded as 1 for both delivery and takeaway\n\ndf['delivery'] = df['delivery'].replace(-1, 1)","1c9ff1d6":"df['takeaway'] = df['takeaway'].replace(-1,1)","fb36fc25":"print(df['delivery'].value_counts(normalize=True)) #99% offer delivery, 1% don't\nprint('*****************************************')\nprint(df['takeaway'].value_counts(normalize=True)) #100% offer takeaway","f09ddd99":"plt.figure(figsize=(7,7))\nsns.heatmap(df.corr(), cmap='coolwarm', annot=True)\nplt.show()","d8c78f38":"# Photo_count and votes are highly correlated\n# Rest all seem to be in acceptable range","b672ae82":"df['aggregate_rating'].describe()","6e576235":"df['aggregate_rating'] = df['aggregate_rating'].replace(0, np.nan)\ndf['average_cost_for_two'] = df['average_cost_for_two'].replace(0, np.nan)","01784deb":"df[df['aggregate_rating'] == 0]","a760d11f":"# Extracting Not Null values only\ndf_nn = df[pd.notnull(df['aggregate_rating'])]","303621ac":"plt.hist(df_nn['aggregate_rating'])\nplt.title('Histogram of aggregate_rating (not null)')\nplt.xlabel('aggregate_rating')\nplt.ylabel('Counts')\nplt.show()","51db086b":"# Zomato has a rating scale of 1-5 only, so restaurants rated 0 seem incorrect.\n# 0 is missing value here (could be unrated as rating_text for these values says not rated)","9eda43f6":"# Imputing missing values for average_cost_for_two\ndf_nn['average_cost_for_two'].fillna(df_nn['average_cost_for_two'].median(), inplace=True)","c306a63d":"sns.relplot(y = 'average_cost_for_two', x = 'aggregate_rating', size='average_cost_for_two', hue = 'aggregate_rating',\n            sizes= (15,200), data = df_nn)\nplt.title(\"Average Cost for Two vs. AggregateRating\")\n\nplt.show()","ebe81887":"# # # Checking Sweetviz report again on this trimmed and cleaned version of the data\n\n# report_trim = sweetviz.analyze(df)\n# report_trim.show_html('data_trim.html')","cbb95efb":"# Establishment\ndf_nn.establishment[0].replace('[','').replace(']','').replace(\"'\",'')\n\ndf_nn.establishment=df_nn.establishment.apply(lambda x:x.replace('[','').replace(']','').replace(\"'\",''))","5a79dc9b":"est_wc = ' '.join(df_nn['establishment'])","463b3850":"# Word Cloud \nwordcloud = WordCloud(width = 3000, height = 3000, \n                background_color ='black', \n                min_font_size = 10, random_state=100).generate(est_wc) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\")\nplt.xlabel('Word Cloud')\nplt.tight_layout(pad = 0) \n\nprint(\"Word Cloud of Establishment!!\")\nplt.show()","0b4957c1":"df_nn['establishment'].value_counts(dropna=False).head()","51849e10":"# Quick Bites, Casual Dining, Cafe are the dominant establishment types","c69d2275":"# Highlights\ndf_nn.highlights[0].replace('[','').replace(']','').replace(\"'\",'')\n\ndf_nn.highlights=df_nn.highlights.apply(lambda x:x.replace('[','').replace(']','').replace(\"'\",''))","1196e079":"df_nn['highlights'].value_counts().head()","4a91acf3":"# Since I want to do some analysis on the highlights vs ratings,\n# it is better tosplit the values of each record to extract different words, \n# Else, the whole data frame will become really cluttered\nsubset = df_nn[['highlights', 'aggregate_rating']]","6b2cf955":"high_split = subset['highlights'].str.get_dummies(sep = \",\")\n\nhigh_split","ce04780c":"subset = pd.concat([subset, high_split], axis=1).drop('highlights', axis = 1)\nsubset","1e6bf5b9":"subset.columns","8f90f11a":"# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df\nX = subset.drop(['aggregate_rating'], axis=1)\n\n# Declare a response variable, called y, and assign it the AdultWeekend column of the df \ny = subset['aggregate_rating'] \n\n# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X \nscaler = preprocessing.StandardScaler().fit(X)\n\n# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X \nX_scaled=scaler.transform(X)","7905ee7e":"y = y.ravel()","b3773761":"gini_model = tree.DecisionTreeRegressor(criterion = 'mse', random_state=5)\n\ngini_model.fit(X, y)","c227e133":"feature_imp=pd.Series(gini_model.feature_importances_,index=X.columns)\na = feature_imp.sort_values(ascending=False).head(20)\na","5f3739cf":"# Table booking recommended, Credit Card, Digital Payments Accepted, Outdoor seating, etc are the top highlights\n# affecting Ratings (out of all highlights)","02073b72":"subset2 = df_nn[['cuisines', 'aggregate_rating']]","52f9664d":"cuisines = subset2['cuisines'].str.get_dummies(sep = ',')\ncuisines","9c8c3584":"subset2 = pd.concat([subset2, cuisines], axis=1).drop('cuisines', axis = 1)\nsubset2","afa39f07":"#Declare an explanatory variable, called X,and assign it the result of dropping 'aggregate_rating' from the df\nX2 = subset2.drop(['aggregate_rating'], axis=1)\n\n# Declare a response variable, called y, and assign it the aggregate_rating column of the df \ny2 = subset2['aggregate_rating'] \n\n# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X \nscaler2 = preprocessing.StandardScaler().fit(X2)\n","ad7e3fed":"# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X \nX_scaled2=scaler2.transform(X2)\n\ny2 = y2.ravel()","74e1db25":"dt2 = tree.DecisionTreeRegressor(criterion = 'mse', random_state=5)\n\ndt2.fit(X2, y2)","626c8300":"feature_imp=pd.Series(dt2.feature_importances_,index=X2.columns)\nb = feature_imp.sort_values(ascending=False).head(20)\nb","464ca787":"# Restaurants having Italian, Asian, Chinese cuisines are better rated than those that don't!","0135664d":"df_nn.columns # Using the subset without NULL Values for Aggregate_Rating","92b07095":"X = df_nn.drop(['aggregate_rating', 'name', 'establishment', 'locality_verbose', 'cuisines', 'timings', 'highlights',\n               'rating_text'], axis=1)\ny = df_nn['aggregate_rating']","ad6ead06":"X = pd.get_dummies(X)\nX","1af6d87b":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=5)","0beca7e2":"# Decision Tree Regressor\ndt_model = tree.DecisionTreeRegressor(criterion = 'mse', random_state=5, max_features='sqrt')\n\ndt_model.fit(X_train, y_train)","6fc69aa6":"# dot_data = StringIO()\n\n\n# tree.export_graphviz(dt_model, out_file=dot_data,  \n#                 filled=True, rounded=True,\n#                 special_characters=True, feature_names=X_train.columns) \n\n\n# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n# Image(graph.create_png())","5b0c6763":"y_pred_dt = pd.Series(dt_model.predict(X_test))\n\nfrom sklearn.model_selection import cross_val_score\n\nnp.mean(cross_val_score(dt_model, X_test, y_test, cv=10))","fde1dedb":"# RMSE - DT Model\nfrom sklearn.metrics import mean_squared_error\nmse_dt = mean_squared_error(y_test, y_pred_dt)\nrmse_dt = np.sqrt(mse_dt)\nrmse_dt","634a9cb8":"# RandomForestRegressor\n\nreg_model = RandomForestRegressor(criterion = 'mse', random_state=5, max_features='sqrt')\n\nreg_model.fit(X_train, y_train)","8bb12af4":"y_pred_reg = pd.Series(reg_model.predict(X_test))\n\nnp.mean(cross_val_score(reg_model, X_test, y_test, cv=10))","a099c821":"# RMSE - RF Model\nmse_rf = mean_squared_error(y_test, y_pred_reg)\nrmse_rf = np.sqrt(mse_rf)\nrmse_rf","5a60ca59":"\n# from lazypredict.Supervised import LazyRegressor\n\n# reg = LazyRegressor(verbose=0, predictions=False, custom_metric='None')\n# models,predictions = reg.fit(X_train, X_test, y_train, y_test)\n\n# models_c","fc484e88":"# GB Regressor\n\nfrom sklearn.ensemble import GradientBoostingRegressor\ngbr = GradientBoostingRegressor(criterion = 'mse', random_state=5, max_features='sqrt')\n\ngbr.fit(X_train, y_train)","f3d8977b":"y_pred_gbr = pd.Series(gbr.predict(X_test))\n\nnp.mean(cross_val_score(gbr, X_test, y_test, cv=10))","54170a93":"# RMSE - GBR Model\nmse_gbr = mean_squared_error(y_test, y_pred_gbr)\nrmse_gbr = np.sqrt(mse_gbr)\nrmse_gbr","92f9e215":"# AdaBoost Regressor\nfrom sklearn.ensemble import AdaBoostRegressor\n\nadr = AdaBoostRegressor(random_state=5)\nadr.fit(X_train, y_train)","1707d7c8":"y_pred_adr = pd.Series(adr.predict(X_test))\n\nnp.mean(cross_val_score(adr, X_test, y_test, cv=10))","2e0f7db2":"mse_adr = mean_squared_error(y_test, y_pred_adr)\nrmse_adr = np.sqrt(mse_adr)\nrmse_adr","18fc28ff":"from scipy.stats import uniform, truncnorm, randint\n\nmodel_params = {\n    # randomly sample numbers from 4 to 110 estimators\n    'n_estimators': randint(1,110),\n#     # normally distributed max_features, with mean .25 stddev 0.1, bounded between 0 and 1\n#     'max_features': truncnorm(a=0, b=1, loc=0.25, scale=0.1),\n#     # uniform distribution from 0.01 to 0.2 (0.01 + 0.199)\n#     'min_samples_split': uniform(0.01, 0.199)\n}","ba529aca":"from sklearn.model_selection import RandomizedSearchCV\n\nmod=RandomizedSearchCV(reg_model, model_params, n_iter=100, cv=5, random_state=5, n_jobs=-1) \n\nmod.fit(X_train,y_train)","48fe35f2":"from pprint import pprint\npprint(mod.best_estimator_.get_params())","5c47acc6":"# Building a tuned model with Best Parameters\nrf_t = RandomForestRegressor(criterion = 'mse', random_state=5, \n                             max_features='sqrt', \n                             min_samples_split=2,\n                            n_estimators=109, verbose=0,\n                            min_samples_leaf=1)\n\nrf_t.fit(X_train, y_train)","822a7e9b":"y_pred_rft = pd.Series(rf_t.predict(X_test))\n\nnp.mean(cross_val_score(rf_t, X_test, y_test, cv=5))","ad381138":"mse_rft = mean_squared_error(y_test, y_pred_rft)\nrmse_rft = np.sqrt(mse_rft)\nrmse_rft","5cba9b0b":"print(\"MODEL_METRICS_RMSE\")\nprint(\"RMSE for Decision Tree Regressor : \" + str(rmse_dt))\nprint(\"RMSE for Random Forest Regressor : \" + str(rmse_rf))\nprint(\"RMSE for Gradient Boosting Regressor : \" + str(rmse_gbr))\nprint(\"RMSE for AdaBoost Regressor : \" + str(rmse_adr))\nprint(\"RMSE for RandomForest_pruned Model : \" +str(rmse_rft))","407c0ebf":"print(\"MODEL_METRICS_R2\")\nprint(\"R2 for Decision Tree Regressor : \" +str(np.mean(cross_val_score(dt_model, X_test, y_test, cv=5))))\nprint(\"R2 for RandomForest Regressor : \" +str(np.mean(cross_val_score(reg_model, X_test, y_test, cv=5))))\nprint(\"R2 for Gradient Boosting Regressor : \" + str(np.mean(cross_val_score(gbr, X_test, y_test, cv=5))))\nprint(\"R2 for Adaboost Regressor : \" +str(np.mean(cross_val_score(adr, X_test, y_test, cv=5))))\nprint(\"R2 for RandomForest Pruned Model : \" + str(np.mean(cross_val_score(rf_t, X_test, y_test, cv=5))))","8fbb74ad":"# RMSE is the least for RandomForest (Pruned) Model\n# R2 is maximum for RandomForest (Pruned) Model\n\n#So, finalising this model!","031594a7":"# Checking Feature Importances\n\nfi_rft=pd.Series(rf_t.feature_importances_,index=X_train.columns)\nd = fi_rft.sort_values(ascending=False).head(20)\nd\n\n# Votes and photo_count contribute max to aggregate_rating","ffa84e41":"#### Exploring the text columns","8b3c4997":"#### Visualising the Dependent Variable (Aggregate Rating)","2ce5f265":"#### Missing value treatment","6192310e":"#### Feature Importances","ec11059a":"#### Data Prep : Fixing Incorrect Data Types","4ee5a548":"#### Data Ingestion","4c4ec74e":"#### Please note some codes are commented out because of high computation requirements.\n\n## Data Wrangling\n\n\n#### Standard Imports","dc7cca53":"#### Model Metrics and Comparison","1aade57f":"#### Checking if cuisine has an effect on rating ","15ab1b14":"#### Checking correlations","9d0fcfca":"#### Check for duplicates","06eea7ad":"#### Checking EDA Report through Sweetviz Package on the whole data","cdcb8a74":"#### Developing train test datasets"}}