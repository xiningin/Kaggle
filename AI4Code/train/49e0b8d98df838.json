{"cell_type":{"f85db3a9":"code","3811b1cc":"code","8312f1f0":"code","b38d10aa":"code","9fd252af":"code","ef546b6b":"code","a5f99de7":"code","25b7d816":"code","d84fbc35":"code","2e94499b":"code","2d6e267b":"code","94b5d834":"code","611aa530":"code","93812487":"code","b6097a4a":"code","d8fc9dd8":"code","ae1aaf87":"code","05e5682e":"code","79d5f10b":"code","359c57ec":"code","acc385a0":"code","2c5fbf9a":"code","d827ac47":"code","d7890433":"code","23ad0def":"code","e16518d8":"code","815553e4":"code","14176c02":"code","2dab7a4b":"code","6464f450":"code","56dbb7a1":"code","aa04bff1":"code","4f15263b":"code","5409ad82":"code","9d21537b":"code","64231c8f":"code","812dbfd2":"code","18d81bb9":"code","fa6627c4":"code","d6ff326f":"code","19c95ea6":"code","dcd95e95":"code","721194f9":"code","6f938975":"code","b789d423":"code","c932950e":"code","8c18d6cb":"code","6ae7ebf4":"code","c659abdc":"code","31791135":"code","49e63392":"code","5669e979":"code","491666ec":"code","9a169f01":"code","268271b1":"code","002e7b60":"code","0ab868fb":"code","79b58e31":"code","12a21fcc":"code","0e7d7f45":"code","ab43e018":"code","2511194a":"code","0e679179":"code","f26d039e":"markdown","50861647":"markdown","208d6622":"markdown","38883cbd":"markdown","0e7b39f1":"markdown","1664c7b3":"markdown"},"source":{"f85db3a9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n#print(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","3811b1cc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8312f1f0":"# !pip install plot_model\n# !pip install keras\n# !pip install --upgrade tensorflow\n# !pip install --upgrade tensorflow-gpu","b38d10aa":"from tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.utils import to_categorical\n\nimport tensorflow as tf\nprint(\"You are using TensorFlow version\", tf.__version__)\nif len(tf.config.list_physical_devices('GPU')) > 0:\n  print(\"You have a GPU enabled.\")\nelse:\n  print(\"Enable a GPU before running this notebook.\")\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential  # initial NN\nfrom tensorflow.keras.layers import Input, Dense, Activation, Dropout, InputLayer, BatchNormalization # construct each layer\nfrom tensorflow.keras.layers import Conv2D # swipe across the image by 1\nfrom tensorflow.keras.layers import MaxPool2D, GlobalMaxPool2D # swipe across by pool size\nfrom tensorflow.keras.layers import Flatten, GlobalAveragePooling2D\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau,LearningRateScheduler, TensorBoard, EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import categorical_crossentropy, binary_crossentropy, mean_squared_error,sparse_categorical_crossentropy\n\nfrom tensorflow.keras.optimizers import Adam\n#from keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n#from keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n\nimport pandas as pd\nimport numpy as np\nfrom random import shuffle\nimport os\nimport cv2\nimport pickle\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg \n%matplotlib inline\nfrom tqdm.notebook import trange,tqdm\nfrom IPython.display import Image, display, Markdown, clear_output\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import svm, metrics\nfrom sklearn.decomposition import PCA\nfrom skimage import io\n\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score","9fd252af":"import cv2\n# from keras import layers\n# from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n# from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n# from keras.models import Model\n# from keras.preprocessing import image\n# from keras.utils import layer_utils\n# from keras.utils.data_utils import get_file\n# from keras.applications.imagenet_utils import preprocess_input\n# import pydot\n# from IPython.display import SVG\n# from keras.utils.vis_utils import model_to_dot\n# #from keras.utils import plot_model\n# #from keras.utils import to_categorical\n# import matplotlib.pyplot as plt \nfrom random import shuffle\n# np.set_printoptions(suppress=True)\n# np.set_printoptions(threshold=np.inf)","ef546b6b":"tf.config.list_physical_devices()","a5f99de7":"# from google.colab import drive\n# drive.mount('\/content\/drive')","25b7d816":"import zipfile\n\nwith zipfile.ZipFile('\/kaggle\/input\/dogs-vs-cats-redux-kernels-edition\/train.zip','r') as z:\n    z.extractall('.')\n    \nwith zipfile.ZipFile('\/kaggle\/input\/dogs-vs-cats-redux-kernels-edition\/test.zip','r') as z:\n    z.extractall('.')\n","d84fbc35":"# !pwd\n# !ls -l","2e94499b":"# Create image \nIMG_SIZE = 100\nX_Train_orig = []\nY_Train_orig = []\nfor i in os.listdir('\/kaggle\/working\/train\/'):\n    label = i.split('.')[-3]\n    if label == 'cat':\n        label = 0\n    elif label == 'dog':\n        label = 1\n    img = cv2.imread('\/kaggle\/working\/train\/'+i, cv2.IMREAD_COLOR)\n    img = cv2.resize(img,(IMG_SIZE,IMG_SIZE), interpolation=cv2.INTER_CUBIC)\n    #img = img \/ 255\n    X_Train_orig.append([np.array(img), np.array(label)])\n\nnp.save('Training_Data.npy', X_Train_orig)","2d6e267b":"shuffle(X_Train_orig)","94b5d834":"X = np.array([i[0] for i in X_Train_orig]).reshape(-1,IMG_SIZE,IMG_SIZE,3)\ny = np.array([i[1] for i in X_Train_orig])","611aa530":"#print(X)","93812487":"#print(y)","b6097a4a":"print(\"Shape of X is: \",X.shape)\nprint(\"Shape of y is: \",y.shape)","d8fc9dd8":"# Spilt Training and test data\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.50, stratify=y)","ae1aaf87":"print('Shape of X_train is :', X_train.shape)\nprint('Shape of y_train is :', y_train.shape)\nprint('Shape of X_val is :', X_val.shape)\nprint('Shape of y_val is :', y_val.shape)","05e5682e":"print(pd.Series(y_train).value_counts(1))\nprint(pd.Series(y_val).value_counts(1))","79d5f10b":"# X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.25, stratify=Y_train)","359c57ec":"# print('Shape of X_train is :', X_train.shape)\n# print('Shape of Y_train is :', Y_train.shape)\n# print('Shape of X_val is :', X_val.shape)\n# print('Shape of Y_val is :', Y_val.shape)","acc385a0":"# print(pd.Series(Y_train).value_counts(1))\n# print(pd.Series(Y_val).value_counts(1))","2c5fbf9a":"import matplotlib.pyplot as plt \nplt.figure(figsize=(20,20))   # to fix a shape for each image print\nfor i in range(50):          # using a for loop to display a number of images\n    plt.subplot(5, 10, i+1) # we need to use this function to print an array of pictures \n    plt.imshow(X_val[i,:,:,:]) # this will call the images from train set one by one\n    plt.title('DOG' if y_val[i] == 1 else 'CAT')  # Lets also look into the labels \n    plt.axis('off') ","d827ac47":"def cnn_model(input_shape):    \n    \n    X_input = Input(input_shape)\n\n    X = Conv2D(64, (3, 3), strides = (1, 1), padding = 'same', name = 'conv1')(X_input) \n    X = BatchNormalization(axis = 3, name = 'bn1')(X)\n    X = Activation('relu')(X) \n    \n    X = Conv2D(64, (3, 3), strides = (1, 1), padding = 'same', name = 'conv2')(X_input) \n    X = BatchNormalization(axis = 3, name = 'bn2')(X)\n    X = Activation('relu')(X) \n\n    X = MaxPooling2D((3, 3), name='max_pool_0')(X)\n    X = Dropout(0.3)(X)\n\n    X = Conv2D(128, (3, 3), strides = (1, 1), padding = 'same', name = 'conv3')(X) \n    X = BatchNormalization(axis = 3, name = 'bn3')(X)\n    X = Activation('relu')(X)\n    \n    X = Conv2D(128, (3, 3), strides = (1, 1), padding = 'same', name = 'conv4')(X) \n    X = BatchNormalization(axis = 3, name = 'bn4')(X)\n    X = Activation('relu')(X)\n    \n    X = Conv2D(128, (3, 3), strides = (1, 1), padding = 'same', name = 'conv5')(X) \n    X = BatchNormalization(axis = 3, name = 'bn5')(X)\n    X = Activation('relu')(X)\n   \n    X = MaxPooling2D((3, 3), name='max_pool_1')(X)\n    X = Dropout(0.3)(X)\n    \n    # Flatten the data.\n    X = Flatten()(X)\n    # Dense Layer\n    X = Dense(4096, activation='relu', name='fc1')(X)\n    X = Dropout(0.1)(X)\n    X = Dense(1024, activation='relu', name='fc2')(X)\n    X = Dropout(0.1)(X)\n    X = Dense(256, activation='relu', name='fc3')(X)\n    # Using softmax function to get the output\n    X = Dense(1, activation='sigmoid', name='fc4')(X)\n    \n    model = Model(inputs = X_input, outputs = X, name='model')\n    \n    return model","d7890433":"cnn_model = cnn_model(X_train.shape[1:4])","23ad0def":"epochs = 30\nbatch_size = 64\nlearning_rate = 0.001\ndecay = learning_rate\/epochs\noptimizer = Adam(learning_rate=learning_rate, epsilon=1e-08, decay = decay)","e16518d8":"cnn_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])","815553e4":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=1, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.0001)","14176c02":"early_stopping = EarlyStopping(monitor='val_accuracy', patience=30, verbose=1, restore_best_weights=True, mode='auto')","2dab7a4b":"cnn_history = cnn_model.fit(x = X_train, y = y_train, batch_size = batch_size, \n                        epochs=epochs, verbose=1, \n                        validation_data = (X_val, y_val),\n                          shuffle = True, \n                          steps_per_epoch= None, validation_steps=None,\n                                  callbacks=[learning_rate_reduction, early_stopping] )","6464f450":"y_train","56dbb7a1":"preds = cnn_model.evaluate(X_train, y_train)\nprint (\"Loss = \" + str(preds[0]))\nprint (\"Train set Accuracy = \" + str(preds[1]))","aa04bff1":"preds_val = cnn_model.evaluate(X_val, y_val)\nprint (\"Loss = \" + str(preds_val[0]))\nprint (\"Validation Set Accuracy = \" + str(preds_val[1]))","4f15263b":"history_df = pd. DataFrame(cnn_history.history)\nhistory_df.loc[:,['loss', 'val_loss']].plot()\nhistory_df.loc[:,['accuracy', 'val_accuracy']].plot()","5409ad82":"history_dict = cnn_history.history\nhistory_dict.keys()","9d21537b":"val_loss = history_dict['val_loss']\nval_acc = history_dict['val_accuracy']\nloss = history_dict['loss']\nacc = history_dict['accuracy']\nepochs = range(1,len(history_dict['val_loss'])+1)","64231c8f":"plt.plot(epochs,acc,'b-')\nplt.title('Accuracy of Model')\nplt.xlabel('epochs')\nplt.ylabel('Accuracy')\n\nplt.plot(epochs,val_acc,'b-', color = 'red')\nplt.title('Accuracy of Model')\nplt.xlabel('epochs')\nplt.ylabel('Accuracy')\nplt.show()","812dbfd2":"plt.plot(epochs,loss,'b-')\nplt.title('loss function')\nplt.xlabel('epochs')\nplt.ylabel('Loss')\n\nplt.plot(epochs,val_loss,'b-', color = 'red')\nplt.title('loss function')\nplt.xlabel('epochs')\nplt.ylabel('val_loss')\nplt.show()","18d81bb9":"predicted_val_probability = cnn_model.predict(X_val, batch_size=64)","fa6627c4":"predicted_val_probability","d6ff326f":"y_val","19c95ea6":"y_val_pred_label = np.round(predicted_val_probability)\nl = []\nfor i in range(len(y_val_pred_label)):\n    if y_val[i] != y_val_pred_label[i]:\n        l.append(i)","dcd95e95":"m = []\nfor t in l:\n    if predicted_val_probability[t] >= 0.5:\n        m.append(t)\n    elif predicted_val_probability[t] < 0.5:\n        m.append(t)\n    ","721194f9":"predicted_val_probability[predicted_val_probability >= 0.5] = 1\npredicted_val_probability[predicted_val_probability < 0.5] = 0","6f938975":"print(metrics.confusion_matrix(y_val, predicted_val_probability))         ## confusion_matrix\nprint(metrics.classification_report(y_val, predicted_val_probability))","b789d423":"#ROC curve\ncnn_fpr, cnn_tpr, cnn_threshold = roc_curve(y_val, predicted_val_probability)\nroc_auc_score_cnn=roc_auc_score(y_val, predicted_val_probability)\n\ndef graph_roc_curve_multiple(cnn_fpr, cnn_tpr):\n    plt.figure(figsize=(10,10))\n    plt.title('ROC Curve', fontsize=14)\n    plt.plot(svm_fpr, svm_tpr, label='roc_auc_score_svm:%.5s' % roc_auc_score_cnn)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=13)\n    plt.ylabel('True Positive Rate', fontsize=13)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend()\n    \ngraph_roc_curve_multiple(cnn_fpr, cnn_tpr)\nplt.show()","c932950e":"predicted_val_probability.flatten()","8c18d6cb":"len(predicted_val_probability)","6ae7ebf4":"type(predicted_val_probability)","c659abdc":"# plt.figure(figsize=(20,20))\n# c = 1\n# for i in m[:]:\n#     plt.subplot(5,5,c)\n#     plt.imshow(X_val[i])\n#     plt.title('DOG:{}\\nTrue label:{}'.format(predicted_val_probability[i], y_val[i])\n#               if predicted_val_probability[i]>= 0.5 else 'CAT:{}\\nTrue label:{}'.format(predicted_val_probability[i],y_val[i]))\n#     plt.axis('off')\n#     c = c+1","31791135":"X_Test_orig = []\nfor i in os.listdir('\/kaggle\/working\/test\/'):\n    label = i.split('.')[-2]\n    img = cv2.imread('\/kaggle\/working\/test\/'+i, cv2.IMREAD_COLOR)\n    img = cv2.resize(img,(IMG_SIZE,IMG_SIZE), interpolation = cv2.INTER_CUBIC)\n    #img = img \/ 255\n    X_Test_orig.append([np.array(img), np.array(label)])\n\nnp.save('Test_Data.npy', X_Train_orig)","49e63392":"X_test = np.array([i[0] for i in X_Test_orig]).reshape(-1,IMG_SIZE, IMG_SIZE, 3)\nLabel = np.array([i[1] for i in X_Test_orig])","5669e979":"X_test = X_test \/ 255","491666ec":"classes = cnn_model.predict(X_test, batch_size = batch_size), \nclasses","9a169f01":"print(max(classes[0]))\nprint(min(classes[0]))","268271b1":"prediction = pd.DataFrame()\nprediction['id'] = Label\nprediction['label'] = classes[0].flatten()\nprediction['id'] = prediction['id'].astype(int)","002e7b60":"type(prediction)\nprediction.shape\nprediction.info()","0ab868fb":"\n#prediction.loc[prediction['label'] >= 0.5, 'label'] = 0.99\n#prediction.loc[prediction['label'] < 0.5, 'label'] = 0.01\n\nprediction.sort_values(by='id',ascending=True,inplace=True)\nprediction.to_csv('submission_v5.csv', index = False)\nprediction.head()","79b58e31":"#Model 2\ndef cnn_model(input_shape):    \n    \n    X_input = Input(input_shape)\n    \n    X = Conv2D(64, (3, 3), strides = (1, 1), padding = 'same', name = 'conv2')(X_input) \n    X = BatchNormalization(axis = 3, name = 'bn2')(X)\n    X = Activation('relu')(X) \n\n    X = MaxPooling2D((3, 3), name='max_pool_0')(X)\n    X = Dropout(0.3)(X)\n\n    X = Conv2D(128, (3, 3), strides = (1, 1), padding = 'same', name = 'conv3')(X) \n    X = BatchNormalization(axis = 3, name = 'bn3')(X)\n    X = Activation('relu')(X)\n    \n    X = Conv2D(256, (3, 3), strides = (1, 1), padding = 'same', name = 'conv4')(X) \n    X = BatchNormalization(axis = 3, name = 'bn4')(X)\n    X = Activation('relu')(X)\n    \n   \n    X = MaxPooling2D((3, 3), name='max_pool_1')(X)\n    X = Dropout(0.3)(X)\n    \n    # Flatten the data.\n    X = Flatten()(X)\n    # Dense Layer\n    X = Dense(4096, activation='relu', name='fc1')(X)\n    X = Dropout(0.1)(X)\n    X = Dense(1024, activation='relu', name='fc2')(X)\n    X = Dropout(0.1)(X)\n    X = Dense(256, activation='relu', name='fc3')(X)\n    # Using softmax function to get the output\n    X = Dense(1, activation='sigmoid', name='fc4')(X)\n    \n    model = Model(inputs = X_input, outputs = X, name='model')\n    \n    return model\n\n\n\n\ncnn_model = cnn_model(X_train.shape[1:4])\nepochs = 30\nbatch_size = 64\nlearning_rate = 0.001\ndecay = learning_rate\/epochs\noptimizer = Adam(learning_rate=learning_rate, epsilon=1e-08, decay = decay)\ncnn_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=1, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.0001)\nearly_stopping = EarlyStopping(monitor='val_accuracy', patience=10, verbose=1, restore_best_weights=True, mode='auto')\ncnn_history = cnn_model.fit(x = X_train, y = y_train, batch_size = batch_size, \n                        epochs=epochs, verbose=1, \n                        validation_data = (X_val, y_val),\n                          shuffle = True, \n                          steps_per_epoch= None, validation_steps=None,\n                                  callbacks=[learning_rate_reduction, early_stopping] )\n\npreds = cnn_model.evaluate(X_train, y_train)\nprint (\"Loss = \" + str(preds[0]))\nprint (\"Train set Accuracy = \" + str(preds[1]))\n\npreds_val = cnn_model.evaluate(X_val, y_val)\nprint (\"Loss = \" + str(preds_val[0]))\nprint (\"Validation Set Accuracy = \" + str(preds_val[1]))\n\nhistory_df = pd. DataFrame(cnn_history.history)\nhistory_df.loc[:,['loss', 'val_loss']].plot()\nhistory_df.loc[:,['accuracy', 'val_accuracy']].plot()\nhistory_dict = cnn_history.history\nhistory_dict.keys()\n\nval_loss = history_dict['val_loss']\nval_acc = history_dict['val_accuracy']\nloss = history_dict['loss']\nacc = history_dict['accuracy']\nepochs = range(1,len(history_dict['val_loss'])+1)\n\nplt.plot(epochs,acc,'b-')\nplt.title('Accuracy of Model')\nplt.xlabel('epochs')\nplt.ylabel('Accuracy')\n\nplt.plot(epochs,val_acc,'b-', color = 'red')\nplt.title('Accuracy of Model')\nplt.xlabel('epochs')\nplt.ylabel('Accuracy')\nplt.show()\n\nplt.plot(epochs,loss,'b-')\nplt.title('loss function')\nplt.xlabel('epochs')\nplt.ylabel('Loss')\n\nplt.plot(epochs,val_loss,'b-', color = 'red')\nplt.title('loss function')\nplt.xlabel('epochs')\nplt.ylabel('val_loss')\nplt.show()\n\npredicted_val_probability = cnn_model.predict(X_val, batch_size=64)\n\n","12a21fcc":"predicted_val_probability","0e7d7f45":"predicted_val_probability[predicted_val_probability >= 0.5] = 1\npredicted_val_probability[predicted_val_probability < 0.5] = 0\nprint(metrics.confusion_matrix(y_val, predicted_val_probability))         ## confusion_matrix\nprint(metrics.classification_report(y_val, predicted_val_probability))\n\n#ROC curve\ncnn_fpr, cnn_tpr, cnn_threshold = roc_curve(y_val, predicted_val_probability)\nroc_auc_score_cnn=roc_auc_score(y_val, predicted_val_probability)\n\ndef graph_roc_curve_multiple(cnn_fpr, cnn_tpr):\n    plt.figure(figsize=(10,10))\n    plt.title('ROC Curve', fontsize=14)\n    plt.plot(svm_fpr, svm_tpr, label='roc_auc_score_svm:%.5s' % roc_auc_score_cnn)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=13)\n    plt.ylabel('True Positive Rate', fontsize=13)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend()\n    \ngraph_roc_curve_multiple(cnn_fpr, cnn_tpr)\nplt.show()","ab43e018":"#Model 3\ndef cnn_model(input_shape):    \n    \n    X = Conv2D(128, (3, 3), strides = (1, 1), padding = 'same', name = 'conv3')(X) \n    X = BatchNormalization(axis = 3, name = 'bn3')(X)\n    X = Activation('relu')(X)\n    \n    X = Conv2D(128, (3, 3), strides = (1, 1), padding = 'same', name = 'conv4')(X) \n    X = BatchNormalization(axis = 3, name = 'bn4')(X)\n    X = Activation('relu')(X)\n    \n    X = Conv2D(128, (3, 3), strides = (1, 1), padding = 'same', name = 'conv5')(X) \n    X = BatchNormalization(axis = 3, name = 'bn5')(X)\n    X = Activation('relu')(X)\n    \n    X = MaxPooling2D((3, 3), name='max_pool_1')(X)\n    X = Dropout(0.3)(X)\n    \n    # Flatten the data.\n    X = Flatten()(X)\n    # Dense Layer\n    X = Dense(4096, activation='relu', name='fc1')(X)\n    X = Dropout(0.1)(X)\n    X = Dense(1024, activation='relu', name='fc2')(X)\n    X = Dropout(0.1)(X)\n    X = Dense(256, activation='relu', name='fc3')(X)\n    # Using softmax function to get the output\n    X = Dense(1, activation='sigmoid', name='fc4')(X)\n    \n    model = Model(inputs = X_input, outputs = X, name='model')\n    \n    return model\n\n\n\n\ncnn_model = cnn_model(X_train.shape[1:4])\nepochs = 30\nbatch_size = 64\nlearning_rate = 0.001\ndecay = learning_rate\/epochs\noptimizer = Adam(learning_rate=learning_rate, epsilon=1e-08, decay = decay)\ncnn_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=1, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.0001)\nearly_stopping = EarlyStopping(monitor='val_accuracy', patience=30, verbose=1, restore_best_weights=True, mode='auto')\ncnn_history = cnn_model.fit(x = X_train, y = y_train, batch_size = batch_size, \n                        epochs=epochs, verbose=1, \n                        validation_data = (X_val, y_val),\n                          shuffle = True, \n                          steps_per_epoch= None, validation_steps=None,\n                                  callbacks=[learning_rate_reduction, early_stopping] )\n\npreds = cnn_model.evaluate(X_train, y_train)\nprint (\"Loss = \" + str(preds[0]))\nprint (\"Train set Accuracy = \" + str(preds[1]))\n\npreds_val = cnn_model.evaluate(X_val, y_val)\nprint (\"Loss = \" + str(preds_val[0]))\nprint (\"Validation Set Accuracy = \" + str(preds_val[1]))\n\nhistory_df = pd. DataFrame(cnn_history.history)\nhistory_df.loc[:,['loss', 'val_loss']].plot()\nhistory_df.loc[:,['accuracy', 'val_accuracy']].plot()\nhistory_dict = cnn_history.history\nhistory_dict.keys()\n\nval_loss = history_dict['val_loss']\nval_acc = history_dict['val_accuracy']\nloss = history_dict['loss']\nacc = history_dict['accuracy']\nepochs = range(1,len(history_dict['val_loss'])+1)\n\nplt.plot(epochs,acc,'b-')\nplt.title('Accuracy of Model')\nplt.xlabel('epochs')\nplt.ylabel('Accuracy')\n\nplt.plot(epochs,val_acc,'b-', color = 'red')\nplt.title('Accuracy of Model')\nplt.xlabel('epochs')\nplt.ylabel('Accuracy')\nplt.show()\n\nplt.plot(epochs,loss,'b-')\nplt.title('loss function')\nplt.xlabel('epochs')\nplt.ylabel('Loss')\n\nplt.plot(epochs,val_loss,'b-', color = 'red')\nplt.title('loss function')\nplt.xlabel('epochs')\nplt.ylabel('val_loss')\nplt.show()\n\npredicted_val_probability = cnn_model.predict(X_val, batch_size=64)\n\npredicted_val_probability[predicted_val_probability >= 0.5] = 1\npredicted_val_probability[predicted_val_probability < 0.5] = 0\nprint(metrics.confusion_matrix(y_val, predicted_val_probability))         ## confusion_matrix\nprint(metrics.classification_report(y_val, predicted_val_probability))\n\n#ROC curve\ncnn_fpr, cnn_tpr, cnn_threshold = roc_curve(y_val, predicted_val_probability)\nroc_auc_score_cnn=roc_auc_score(y_val, predicted_val_probability)\n\ndef graph_roc_curve_multiple(cnn_fpr, cnn_tpr):\n    plt.figure(figsize=(10,10))\n    plt.title('ROC Curve', fontsize=14)\n    plt.plot(svm_fpr, svm_tpr, label='roc_auc_score_svm:%.5s' % roc_auc_score_cnn)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=13)\n    plt.ylabel('True Positive Rate', fontsize=13)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend()\n    \ngraph_roc_curve_multiple(cnn_fpr, cnn_tpr)\nplt.show()","2511194a":"# Swapping train and test data to fulfil 2x2 completely crossed design\ntemp=X_train.copy()\nX_train = X_val.copy()\nX_val = temp.copy()\n\ntemp=y_train.copy()\ny_train=y_val.copy()\ny_val=temp.copy()\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_val.shape)\nprint(y_val.shape)","0e679179":"#Doing cross validation\ndef cnn_model(input_shape):    \n    \n    X_input = Input(input_shape)\n\n    X = Conv2D(64, (3, 3), strides = (1, 1), padding = 'same', name = 'conv1')(X_input) \n    X = BatchNormalization(axis = 3, name = 'bn1')(X)\n    X = Activation('relu')(X) \n    \n    X = Conv2D(64, (3, 3), strides = (1, 1), padding = 'same', name = 'conv2')(X_input) \n    X = BatchNormalization(axis = 3, name = 'bn2')(X)\n    X = Activation('relu')(X) \n\n    X = MaxPooling2D((3, 3), name='max_pool_0')(X)\n    X = Dropout(0.3)(X)\n\n    X = Conv2D(128, (3, 3), strides = (1, 1), padding = 'same', name = 'conv3')(X) \n    X = BatchNormalization(axis = 3, name = 'bn3')(X)\n    X = Activation('relu')(X)\n    \n    X = Conv2D(128, (3, 3), strides = (1, 1), padding = 'same', name = 'conv4')(X) \n    X = BatchNormalization(axis = 3, name = 'bn4')(X)\n    X = Activation('relu')(X)\n    \n    X = Conv2D(128, (3, 3), strides = (1, 1), padding = 'same', name = 'conv5')(X) \n    X = BatchNormalization(axis = 3, name = 'bn5')(X)\n    X = Activation('relu')(X)\n   \n    X = MaxPooling2D((3, 3), name='max_pool_1')(X)\n    X = Dropout(0.3)(X)\n    \n    # Flatten the data.\n    X = Flatten()(X)\n    # Dense Layer\n    X = Dense(4096, activation='relu', name='fc1')(X)\n    X = Dropout(0.1)(X)\n    X = Dense(1024, activation='relu', name='fc2')(X)\n    X = Dropout(0.1)(X)\n    X = Dense(256, activation='relu', name='fc3')(X)\n    # Using softmax function to get the output\n    X = Dense(1, activation='sigmoid', name='fc4')(X)\n    \n    model = Model(inputs = X_input, outputs = X, name='model')\n    \n    return model\n\n\n\n\ncnn_model = cnn_model(X_train.shape[1:4])\nepochs = 30\nbatch_size = 64\nlearning_rate = 0.001\ndecay = learning_rate\/epochs\noptimizer = Adam(learning_rate=learning_rate, epsilon=1e-08, decay = decay)\ncnn_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=1, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.0001)\nearly_stopping = EarlyStopping(monitor='val_accuracy', patience=30, verbose=1, restore_best_weights=True, mode='auto')\ncnn_history = cnn_model.fit(x = X_train, y = y_train, batch_size = batch_size, \n                        epochs=epochs, verbose=1, \n                        validation_data = (X_val, y_val),\n                          shuffle = True, \n                          steps_per_epoch= None, validation_steps=None,\n                                  callbacks=[learning_rate_reduction, early_stopping] )\n\npreds = cnn_model.evaluate(X_train, y_train)\nprint (\"Loss = \" + str(preds[0]))\nprint (\"Train set Accuracy = \" + str(preds[1]))\n\npreds_val = cnn_model.evaluate(X_val, y_val)\nprint (\"Loss = \" + str(preds_val[0]))\nprint (\"Validation Set Accuracy = \" + str(preds_val[1]))\n\nhistory_df = pd. DataFrame(cnn_history.history)\nhistory_df.loc[:,['loss', 'val_loss']].plot()\nhistory_df.loc[:,['accuracy', 'val_accuracy']].plot()\nhistory_dict = cnn_history.history\nhistory_dict.keys()\n\nval_loss = history_dict['val_loss']\nval_acc = history_dict['val_accuracy']\nloss = history_dict['loss']\nacc = history_dict['accuracy']\nepochs = range(1,len(history_dict['val_loss'])+1)\n\nplt.plot(epochs,acc,'b-')\nplt.title('Accuracy of Model')\nplt.xlabel('epochs')\nplt.ylabel('Accuracy')\n\nplt.plot(epochs,val_acc,'b-', color = 'red')\nplt.title('Accuracy of Model')\nplt.xlabel('epochs')\nplt.ylabel('Accuracy')\nplt.show()\n\nplt.plot(epochs,loss,'b-')\nplt.title('loss function')\nplt.xlabel('epochs')\nplt.ylabel('Loss')\n\nplt.plot(epochs,val_loss,'b-', color = 'red')\nplt.title('loss function')\nplt.xlabel('epochs')\nplt.ylabel('val_loss')\nplt.show()\n\npredicted_val_probability = cnn_model.predict(X_val, batch_size=64)\n\npredicted_val_probability[predicted_val_probability >= 0.5] = 1\npredicted_val_probability[predicted_val_probability < 0.5] = 0\n\nprint(metrics.confusion_matrix(y_val, predicted_val_probability))         ## confusion_matrix\nprint(metrics.classification_report(y_val, predicted_val_probability))\n\n#ROC curve\ncnn_fpr, cnn_tpr, cnn_threshold = roc_curve(y_val, predicted_val_probability)\nroc_auc_score_cnn=roc_auc_score(y_val, predicted_val_probability)\n\ndef graph_roc_curve_multiple(cnn_fpr, cnn_tpr):\n    plt.figure(figsize=(10,10))\n    plt.title('ROC Curve', fontsize=14)\n    plt.plot(svm_fpr, svm_tpr, label='roc_auc_score_svm:%.5s' % roc_auc_score_cnn)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=13)\n    plt.ylabel('True Positive Rate', fontsize=13)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend()\n    \ngraph_roc_curve_multiple(cnn_fpr, cnn_tpr)\nplt.show()","f26d039e":"## Running Some other models","50861647":"## 4. Model Performance","208d6622":"##5 Model Prediction and Kaggle submission","38883cbd":"#2 EDA","0e7b39f1":"#1. Import Libraries","1664c7b3":"#3 CNN Model Development and Training"}}