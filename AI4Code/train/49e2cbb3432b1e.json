{"cell_type":{"c7359189":"code","c19f51b8":"code","513e5a3c":"code","60742fc2":"code","2a03744a":"code","017757a9":"code","931e8711":"code","2708a9f9":"code","a7598860":"code","6da4bb10":"code","03df81b2":"code","07c06241":"code","481be8f8":"code","6512ff4f":"code","9b5b988f":"code","022061b3":"code","d2314c55":"code","992bb263":"code","3df56d3d":"code","5e665eb5":"code","e700eabc":"code","6be6b1a1":"code","6234c3ae":"code","4f20fbce":"code","be2be292":"code","96b8e990":"code","c6a505d1":"code","c457d105":"code","d4b0f6cb":"code","2efe0944":"code","6bd98dc0":"code","21b90eb8":"code","93aeb598":"code","0e78806d":"code","6525e767":"markdown","b918f904":"markdown","56d9f47d":"markdown","b0b5f7be":"markdown","e6f7067f":"markdown","99edd90c":"markdown","ec626a29":"markdown","b3ffa5af":"markdown","9123b9eb":"markdown","f3d4952e":"markdown","689ac1f6":"markdown","09f56698":"markdown","91ada45f":"markdown","040b3845":"markdown","0523c39b":"markdown","85013660":"markdown","d01da556":"markdown","c415c334":"markdown","eb3f3145":"markdown","d3818193":"markdown","307b9154":"markdown","8b356d4e":"markdown"},"source":{"c7359189":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n# Standard machine learning models\nfrom sklearn.linear_model import LogisticRegressionCV\n\n# Scikit-learn utilities\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, confusion_matrix, roc_auc_score, roc_curve","c19f51b8":"# PyMC3 for Bayesian Inference\nimport pymc3 as pm\nprint(pm.__version__)\nimport arviz\n","513e5a3c":"import matplotlib.pyplot as plt\n\n%matplotlib inline\nplt.style.use('seaborn-darkgrid')\nfrom IPython.core.pylabtools import figsize\nimport matplotlib.lines as mlines\n\nimport seaborn as sns\nimport itertools\n\npd.options.mode.chained_assignment = None\n\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","60742fc2":"telcom = pd.read_csv(r\"\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n#first few rows\ntelcom.head()","2a03744a":"print (\"Rows     : \" ,telcom.shape[0])\nprint (\"Columns  : \" ,telcom.shape[1])\nprint (\"\\nFeatures : \\n\" ,telcom.columns.tolist())\nprint (\"\\nMissing values :  \", telcom.isnull().sum().values.sum())\nprint (\"\\nUnique values :  \\n\",telcom.nunique())","017757a9":"for i in telcom.columns:\n    if len(telcom[i].unique())<10:\n        print(\"Column:{},Unique values:{}\".format(i,telcom[i].unique()))\n    else:\n        print(\"Column:{}Unique values:{}\".format(i,len(telcom[i].unique())))\n","931e8711":"\ntelcom_dummies=pd.DataFrame()\nprint(\"Total number of rows before starting copying:{}\".format(len(telcom_dummies)))\n# len(telcom_dummies[telcom_dummies['TotalCharges'] == \" \"])\ntelcom_dummies = pd.get_dummies(telcom[['gender','PaymentMethod','Contract']], columns=['gender','PaymentMethod','Contract'])\ntelcom_dummies['SeniorCitizen'] =telcom['SeniorCitizen']\ntelcom_dummies['Partner'] = telcom['Partner'].map(lambda s :1  if s =='Yes' else 0)\ntelcom_dummies['Dependents'] = telcom['Dependents'].map(lambda s :1  if s =='Yes' else 0)\ntelcom_dummies['tenure']=telcom['tenure']\ntelcom_dummies['PhoneService'] = telcom['PhoneService'].map(lambda s :1  if s =='Yes' else 0)\ntelcom_dummies['MultipleLines'] = telcom['MultipleLines'].map(lambda s :1  if s =='Yes' else 0)\ntelcom_dummies['Has_InternetService'] = telcom['InternetService'].map(lambda s :0  if s =='No' else 1)\ntelcom_dummies['Fiber_optic'] = telcom['InternetService'].map(lambda s :1  if s =='Fiber optic' else 0)\ntelcom_dummies['DSL'] = telcom['InternetService'].map(lambda s :1  if s =='DSL' else 0)\ntelcom_dummies['OnlineSecurity'] = telcom['OnlineSecurity'].map(lambda s :1  if s =='Yes' else 0)\ntelcom_dummies['OnlineBackup'] = telcom['OnlineBackup'].map(lambda s :1  if s =='Yes' else 0)\ntelcom_dummies['DeviceProtection'] = telcom['DeviceProtection'].map(lambda s :1  if s =='Yes' else 0)\ntelcom_dummies['TechSupport'] = telcom['TechSupport'].map(lambda s :1  if s =='Yes' else 0)\ntelcom_dummies['StreamingTV'] = telcom['StreamingTV'].map(lambda s :1  if s =='Yes' else 0)\ntelcom_dummies['StreamingMovies'] = telcom['StreamingMovies'].map(lambda s :1  if s =='Yes' else 0)\ntelcom_dummies['PaperlessBilling'] = telcom['PaperlessBilling'].map(lambda s :1  if s =='Yes' else 0)\ntelcom_dummies['MonthlyCharges']=telcom['MonthlyCharges']\ntelcom_dummies['TotalCharges'] = pd.to_numeric(telcom['TotalCharges'],errors='coerce')\nprint(\"Total number of rows after  copying:{}\".format(len(telcom_dummies)))\n      #Counting number of na\nprint(\"Number of NA\")\nprint(len(telcom_dummies) - telcom_dummies.count())\ntelcom_dummies.dropna(axis=0,inplace=True)\nprint(\"Total number of rows after removing NA:{}\".format(len(telcom_dummies)))\ntelcom_dummies['Churn']=telcom['Churn'].map(lambda s :1  if s =='Yes' else 0)\ntelcom_dummies.rename(columns={\"PaymentMethod_Bank transfer (automatic)\" :\"paymnt_mthd_bank_auto\",\n\"PaymentMethod_Credit card (automatic)\"  : \"paymnt_mthd_cc_auto\",\n\"PaymentMethod_Electronic check\"   :\"paymnt_mthd_elc_check\",\n\"PaymentMethod_Mailed check\"       :\"paymnt_mthd_mailed_check\",         \n\"Contract_Month-to-month\":\"cont_mnth_to_mnth\",                    \n\"Contract_One year\"  :\"cont_1_yr\",                       \n\"Contract_Two year\"    :\"cont_2_yr\" },inplace=True)\ntelcom_dummies.columns","2708a9f9":"print(\"Checking if columns are ready to apply ML algorithm\")\nfor i in telcom_dummies.columns:\n    if len(telcom_dummies[i].unique())<10:\n        print(\"Column:{},Unique values:{},Type:{}\".format(i,telcom_dummies[i].unique(),telcom_dummies[i].dtypes))\n    else:\n        print(\"Column:{}Unique values:{},Type:{}\".format(i,len(telcom_dummies[i].unique()),telcom_dummies[i].dtypes))","a7598860":"y = telcom_dummies['Churn'].values\nX = telcom_dummies.loc[:, telcom_dummies.columns != 'Churn']\nfrom sklearn.preprocessing import MinMaxScaler\nfeatures = X.columns.values\nscaler = MinMaxScaler(feature_range = (0,1))\nscaler.fit(X)\nX = pd.DataFrame(scaler.transform(X))\nX.columns = features\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100)","6da4bb10":"# Calculate the accuracy and f1 score of a model\ndef calc_metrics(predictions, y_test):\n    accuracy = np.mean(predictions == y_test)\n    f1_metric = f1_score(y_test, predictions)\n\n    print('Accuracy of Model: {:.2f}%'.format(100 * accuracy))\n    print('F1 Score of Model: {:.4f}'.format(f1_metric))\nbaseline_pred = [0 for _ in range(len(y_test))]\ncalc_metrics(baseline_pred, y_test)\n","03df81b2":"lr = LogisticRegressionCV(Cs= 20, cv = 3, scoring = 'f1', \n                          penalty = 'l2', random_state = 42)\nlr.fit(X_test, y_test)\n\n# Make predictions and evaluate\nlr_pred = lr.predict(X_test)\ncalc_metrics(lr_pred, y_test)","07c06241":"# Build up a formula\nformula = [' %s + ' % variable for variable in X_test.columns]\nformula.insert(0, 'y ~ ')\nformula = ' '.join(''.join(formula).split(' ')[:-2])\nformula","481be8f8":"print('Intercept: {:0.4f}'.format(lr.intercept_[0]))\nfor feature, weight in zip(X_test.columns, lr.coef_[0]):\n    print('Feature: {:30} Weight: {:0.4f}'.format(feature, weight))","6512ff4f":"X_with_labels = X_train.copy()\nX_with_labels['y'] = y_train\nwith pm.Model() as logistic_model:\n    priors=dict()\n    \n    for variable in X_test.columns:\n        priors[variable]=pm.Uniform.dist(0,1)\n    priors['Intercept']=pm.Normal.dist(mu=0., sigma=100.)\n    priors['MonthlyCharges']=pm.Normal.dist(mu=0., sigma=100.)\n    priors['TotalCharges'] = pm.Normal.dist(mu=0., sigma=100.)\n    # Build the model using the formula and specify the data likelihood \n    pm.GLM.from_formula(formula, data = X_with_labels, family = pm.glm.families.Binomial(),priors=priors)\n    \n    # Using the no-uturn sampler\n    sampler = pm.NUTS()\n    \n    # Sample from the posterior using NUTS\n    trace_log = pm.sample(draws=2000, step = sampler, chains=1, tune=1000, random_seed=100,init='adapt_diag')","9b5b988f":"import pickle\nfileObject = open(\"all_parameters.pickle\",'wb')  \npickle.dump(trace_log, fileObject)\nfileObject.close()","022061b3":"trace_log_from_file= pickle.load(open(\"all_parameters.pickle\",'rb')  )\n#trace_log=trace_log_from_file   #Uncomment this line if we don't want to run model again","d2314c55":"figsize(10, 12)\npm.forestplot(trace_log);","992bb263":"pm.plot_posterior(trace_log);","3df56d3d":"pm.summary(trace_log)","5e665eb5":"def evaluate_trace(trace, data, print_model = False):\n    means_dict = {}\n    std_dict = {}\n    \n    for var in trace.varnames:\n        means_dict[var] = np.mean(trace[var])\n        std_dict[var] = np.std(trace[var])\n    \n    model = 'logit = %0.4f + ' % np.mean(means_dict['Intercept'])\n    \n    for var in data.columns:\n        model += '%0.4f * %s + ' % (means_dict[var], var)\n    \n    model = ' '.join(model.split(' ')[:-2])\n    if print_model:\n        print('Final Equation: \\n{}'.format(model))\n    \n    return means_dict, std_dict","e700eabc":"means_dict, std_dict = evaluate_trace(trace_log, X_train, print_model=True)","6be6b1a1":"# Find a single probabilty estimate using the mean value of variables in a trace\ndef find_probs(trace, data):\n    \n    # Find the means and std of the variables\n    means_dict1, std_dict = evaluate_trace(trace, data)\n          \n    probs = []\n       \n    \n    # Need an intercept term in the data\n    data['Intercept'] = 1\n    l_means_dict=dict()\n    for c in data.columns:\n        \n        l_means_dict[c]=means_dict1[c]\n    \n    data = data[list(l_means_dict.keys())]\n    mean_array = np.array(list(l_means_dict.values()))\n    # Calculate the probability for each observation in the data\n    for _, row in data.iterrows():\n        # First the log odds\n        logit = np.dot(row, mean_array)\n        # Convert the log odds to a probability\n        probability = 1 \/ (1 + np.exp(-logit))\n        probs.append(probability)\n        \n    return probs","6234c3ae":"blr_probs = find_probs(trace_log, X_test.copy())\n\n# Threshold the values at 0.5\npredictions = (np.array(blr_probs) > 0.5)\ncalc_metrics(predictions, y_test)","4f20fbce":"X_test2=X_test[X_test.columns.difference(['MonthlyCharges', 'paymnt_mthd_cc_auto', 'cont_1_yr', 'cont_2_yr', 'Partner', 'Dependents', 'Tenure', 'PhoneService', 'DSL', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport'])]\n# Build up a formula\nformula1 = [' %s + ' % variable for variable in X_test2.columns]\nformula1.insert(0, 'y ~ ')\nformula1 = ' '.join(''.join(formula1).split(' ')[:-2])\nformula1","be2be292":"with pm.Model() as logistic_model1:\n    \n    # Build the model using the formula and specify the data likelihood \n    priors=dict()\n    for variable in X_test2.columns:\n        priors[variable]=pm.Uniform.dist(0,1)\n    priors['Intercept']=pm.Normal.dist(mu=0., sigma=100.)\n    priors['MonthlyCharges']=pm.Normal.dist(mu=0., sigma=100.)\n    priors['TotalCharges'] = pm.Normal.dist(mu=0., sigma=100.)\n              \n    pm.GLM.from_formula(formula1, data = X_with_labels, family = pm.glm.families.Binomial(),priors=priors)\n    \n    # Using the no-uturn sampler\n    sampler = pm.NUTS()\n    \n    # Sample from the posterior using NUTS\n    trace_log1 = pm.sample(draws=2000, step = sampler, chains=1, tune=1000, random_seed=100,init='adapt_diag')","96b8e990":"pm.plot_posterior(trace_log);","c6a505d1":"pm.summary(trace_log1)","c457d105":"fileObject = open(\"sign_parameters.pickle\",'wb')  \npickle.dump(trace_log1, fileObject)\nfileObject.close()","d4b0f6cb":"trace_log1_frm_file= pickle.load(open(\"sign_parameters.pickle\",'rb')  )\n#trace_log1=trace_log1_frm_file #Uncomment this line if we want to load the model from static file","2efe0944":"means_dict_sign, std_dict_sign = evaluate_trace(trace_log1, X_test2, print_model=True)","6bd98dc0":"blr1_probs = find_probs(trace_log1, X_test2)\n\n# Threshold the values at 0.5\npredictions = (np.array(blr1_probs) > 0.5)\ncalc_metrics(predictions, y_test)","21b90eb8":"logistic_model.name='all_parm'\nlogistic_model1.name='sign_parm'\nmodel_trace_dict = {'all_parm':trace_log,\n                   'sign_parm':trace_log1}\ndfwaic = pm.compare(model_trace_dict)\npm.compareplot(dfwaic);","93aeb598":"dfwaicloo = pm.compare(model_trace_dict, ic='LOO')\npm.compareplot(dfwaicloo);","0e78806d":"print(dfwaic)\nprint(dfwaicloo)","6525e767":"Preparing dataframe for ML related algoithms by changing categorical variable in to dummies variable and changing \"YES\"\/\"NO\" to 1\/0 columns respectively","b918f904":"# 2. Read data","56d9f47d":"Validating the transformation","b0b5f7be":"Defining helper function and calculating the baseline model","e6f7067f":"## 1.2 Import Bayesian related libaries","99edd90c":"# Conclusion:\nBayesian models performed more or less similar to frequentist model and with model tuning it can be made to perform better.\nFollowing are next steps:\n* With additional feature engineering Bayesian model can be fine tuned\n* Based on feature distribution can look to add more relevant prior\n* Can try different model apart from logit\n","ec626a29":"## Bayesian Logistic regression after removing insignificant features like:\nFollowing ingnisficant features are removed:\n* MonthlyCharges\n* paymnt_mthd_cc_auto\n* cont_1_yr\n* cont_2_yr\n* Partner\n* Dependents\n* Tenure\n* PhoneService\n* DSL\n* OnlineSecurity\n* OnlineBackup\n* DeviceProtection\n* TechSupport\n\n","b3ffa5af":"Saving the model to pickle file(in case we don't want to run model again)\n","9123b9eb":"## 5.1 Custom function to calculate the model metrics ","f3d4952e":"# 3. Feature Engineering","689ac1f6":"Saving the model to pickle file(in case we don't want to run model again)","09f56698":"Intecept ,Monthly charges,Total charges,Gender_Male shows variation most variation in estimation","91ada45f":"# 1. Import required libaries\n## 1.1 Import computation libaries","040b3845":"  ## 5.3 Bayesian Logistic Regression","0523c39b":"## 2.1 Basic data statistics","85013660":"Appying normal non-informative prior to Intercept,MonthlyCharges and TotalCharges.Applying Uniform prior to rest of the indicator variables","d01da556":"## References:\n1.\tDataset reference:\nhttps:\/\/www.kaggle.com\/blastchar\/telco-customer-churn\n2.\tFor EDA and problem description:\nhttps:\/\/www.kaggle.com\/pavanraj159\/telecom-customer-churn-prediction\n3.\tBayesian Logistic regression using PyMC3\nhttps:\/\/docs.pymc.io\/notebooks\/GLM-logistic.html\n4.\tBayesian model selection\nhttps:\/\/docs.pymc.io\/notebooks\/GLM-model-selection.html\n\n","c415c334":"## 1.3 Import visualisation libraries","eb3f3145":"   ## 5.2 Logistic regression","d3818193":"**There is very marginal improvement between the two models.Second model perform better**","307b9154":"# 5.  Modeling ","8b356d4e":"# 4. Creating test train datset\nUsing test train split and creating dataframe with X_train,y_train,X_test and y_test"}}