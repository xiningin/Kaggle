{"cell_type":{"b7cc84c7":"code","971334a6":"code","cc112a60":"code","22cacb34":"code","848a05d9":"code","5c73c847":"code","8ccae9d1":"code","9bd3b93b":"code","fbd052e8":"code","5bd331aa":"code","e25f61de":"markdown","805a0e69":"markdown","e2737b5a":"markdown","0398293a":"markdown","6285f372":"markdown","0b78157a":"markdown","b9d7eb2c":"markdown"},"source":{"b7cc84c7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, CuDNNLSTM, SpatialDropout1D\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\nimport re\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory","971334a6":"cols = ['sentiment','id','date','query_string','user','text']\ndata = pd.read_csv('..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', encoding='latin-1', names=cols)\ndata = data[['text','sentiment']]","cc112a60":"data['text'] = data['text'].apply(lambda x: x.lower())\ndata['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n\nfor idx,row in data.iterrows():\n    row[0] = row[0].replace('rt',' ')\n    \nmax_fatures = 2000\ntokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(data['text'].values)\nX = tokenizer.texts_to_sequences(data['text'].values)\nX = pad_sequences(X)","22cacb34":"embed_dim = 128\nlstm_out = 196\n\nmodel = Sequential()\nmodel.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(CuDNNLSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(2,activation='softmax'))\nmodel.compile(loss = 'sparse_categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","848a05d9":"Y = pd.get_dummies(data['sentiment']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","5c73c847":"batch_size = 32\nmodel.fit(X_train, Y_train, epochs = 7, batch_size=batch_size, verbose = 2)\n\nmodel.save_weights(\"model.h5\")","8ccae9d1":"validation_size = 1500\n\nX_validate = X_test[-validation_size:]\nY_validate = Y_test[-validation_size:]\nX_test = X_test[:-validation_size]\nY_test = Y_test[:-validation_size]\nscore,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\nprint(\"score: %.2f\" % (score))\nprint(\"acc: %.2f\" % (acc))","9bd3b93b":"pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\nfor x in range(len(X_validate)):\n    \n    result = model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n   \n    if np.argmax(result) == np.argmax(Y_validate[x]):\n        if np.argmax(Y_validate[x]) == 0:\n            neg_correct += 1\n        else:\n            pos_correct += 1\n       \n    if np.argmax(Y_validate[x]) == 0:\n        neg_cnt += 1\n    else:\n        pos_cnt += 1\n\n\n\nprint(\"pos_acc\", pos_correct\/pos_cnt*100, \"%\")\nprint(\"neg_acc\", neg_correct\/neg_cnt*100, \"%\")","fbd052e8":"twt = ['life is good']\n#vectorizing the tweet by the pre-fitted tokenizer instance\ntwt = tokenizer.texts_to_sequences(twt)\n#padding the tweet to have exactly the same shape as `embedding_2` input\ntwt = pad_sequences(twt, maxlen=28, dtype='int32', value=0)\nsentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\nprint(sentiment[0])\nif np.argmax(round(sentiment[0])) == 0:\n    print(\"negative\")\nelse:\n    print(\"positive\")","5bd331aa":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, CuDNNLSTM, SpatialDropout1D\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\nimport re\n\ncols = ['sentiment','id','date','query_string','user','text']\ndata = pd.read_csv('..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', encoding='latin-1', names=cols)\ndata = data[['text','sentiment']]\n\nmax_fatures = 2000\ntokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(data['text'].values)\nX = tokenizer.texts_to_sequences(data['text'].values)\nX = pad_sequences(X)\n\nembed_dim = 128\nlstm_out = 196\n\nmodel = Sequential()\nmodel.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(CuDNNLSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(2,activation='softmax'))\nmodel.compile(loss = 'sparse_categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n\nY = pd.get_dummies(data['sentiment']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)\n\nbatch_size = 32\nmodel.fit(X_train, Y_train, epochs = 7, batch_size=batch_size, verbose = 2)\n\ntwt = ['life is good']\n#vectorizing the tweet by the pre-fitted tokenizer instance\ntwt = tokenizer.texts_to_sequences(twt)\n#padding the tweet to have exactly the same shape as `embedding_2` input\ntwt = pad_sequences(twt, maxlen=28, dtype='int32', value=0)\nsentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\nprint(sentiment[0])\nif np.argmax(round(sentiment[0])) == 0:\n    print(\"negative\")\nelse:\n    print(\"positive\")","e25f61de":"Hereby I declare the train and test dataset.","805a0e69":"Here we train the Network. We should run much more than 7 epoch, but I would have to wait forever for kaggle, so it is 7 for now.","e2737b5a":"As it was requested by the crowd, I extended the kernel with a prediction example, and also updated the API calls to Keras 2.0. Please note that the network performs poorly. Its because the training data is very unbalanced (pos: 4472, neg: 16986), you should get more data, use other dataset, use pre-trained model, or weight classes to achieve reliable predictions.\n\nI have created this kernel when I knew much less about LSTM & ML. It is a really basic, beginner level kernel, yet it had a huge audience in the past year. I had a lot of private questions and requests regarding this notebook and I tried my best to help and answer them . In the future I am not planning to answer custom questions and support\/enhance this kernel in any ways. Thank you my folks :)","0398293a":"Finally measuring the number of correct guesses.  It is clear that finding negative tweets goes very well for the Network but deciding whether is positive is not really. My educated guess here is that the positive training set is dramatically smaller than the negative, hence the \"bad\" results for positive tweets.","6285f372":"**Created by Peter Nagy February 2017 ** <br\/>\n[Github][1] <br\/>\n[Linkedin](https:\/\/www.linkedin.com\/in\/peternagyjob\/) <br\/>\n**Sentiment Analysis:** the process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, product, etc. is positive, negative, or neutral.\n\n\n  [1]: https:\/\/github.com\/nagypeterjob","0b78157a":"As an improvement to my previous [Kernel][1], here I am trying to achieve better results with a Recurrent Neural Network. <br\/>\nYou may want to [check out](https:\/\/www.kaggle.com\/ngyptr\/multi-class-classification-with-lstm) my latest kernel on an LSTM multi-class classification problem.\n\n  [1]: https:\/\/www.kaggle.com\/ngyptr\/d\/crowdflower\/first-gop-debate-twitter-sentiment\/python-nltk-sentiment-analysis","b9d7eb2c":"Extracting a validation set, and measuring score and accuracy."}}