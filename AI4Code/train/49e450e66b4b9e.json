{"cell_type":{"fe0a5522":"code","8ce24422":"code","f4a89ca5":"code","507d2325":"code","e86fb536":"code","04373757":"code","16e7736b":"code","c1697e6d":"code","fb6daf2b":"code","e4676eff":"code","d75cc442":"code","a593c44c":"code","0aef583c":"code","53d617f6":"code","977ffd47":"code","a59fd635":"code","1fd9cc9b":"code","2a0ad73f":"code","b9ee26e1":"code","e2ec95bd":"code","447e90ee":"code","96be5e8c":"code","3f84e391":"code","67e35906":"code","5833259c":"code","5d188ba4":"code","0893ca90":"code","4551c07e":"code","b0c9f0ff":"markdown","7adc44c5":"markdown","d4f8a497":"markdown","c2af8374":"markdown","dd04ce8f":"markdown","bbb31d91":"markdown","ac1ad15f":"markdown","60eb0567":"markdown","cdbf834b":"markdown","da5b896e":"markdown","16c2b797":"markdown","adbe067a":"markdown"},"source":{"fe0a5522":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8ce24422":"# Import all necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Import all necessary files\niowa_file_train = '\/kaggle\/input\/home-data-for-ml-course\/train.csv'\niowa_file_test = '\/kaggle\/input\/home-data-for-ml-course\/test.csv'\n\nhome_data_train = pd.read_csv(iowa_file_train) # This is all train data\nhome_data_test = pd.read_csv(iowa_file_test) # This is all test data\n\ny = home_data_train.SalePrice # This is what we want to predict\nhome_data_train.drop(['SalePrice', 'Id'], axis=1, inplace=True ) # This is all the rest (all features) without 'Id' in train data\nhome_data_test.drop(['Id'], axis=1, inplace=True ) # This is all the rest (all features) without 'Id' in test data\nX_train = home_data_train # For shortly access\nX_test = home_data_test","f4a89ca5":"# Find columns with missing\ncols_w_m_n_train = [col for col in X_train.columns if X_train[col].isnull().any()] #Missing in train\ncols_w_m_n_test = [col for col in X_test.columns if X_test[col].isnull().any()] #Missing in test\n\n\nprint(cols_w_m_n_train, '\\n',cols_w_m_n_test)","507d2325":"# Find all features that have categorical data\ns = (X_train.dtypes == 'object')\ncols_s = list(s[s].index)\n\nss = (X_test.dtypes == 'object')\ncols_ss = list(s[s].index)\n\nprint(\"Categorical variables in train data:\", cols_s)\nprint(\"Categorical variables in test data:\", cols_ss)","e86fb536":"n_unique_train = list(map(lambda col: X_train[col].nunique(), cols_s))\nd = dict(zip(cols_s, n_unique_train))\nsorted(d.items(), key = lambda x: x[1])","04373757":"n_unique_test = list(map(lambda col: X_test[col].nunique(), cols_ss))\ndd = dict(zip(cols_ss, n_unique_test))\nsorted(dd.items(), key = lambda x: x[1])","16e7736b":"print(X_train.shape, X_test.shape,'\\n','__________________')\nmissing_v_count_by_column_train = (X_train.isnull().sum())\nprint(missing_v_count_by_column_train[missing_v_count_by_column_train > 0],'\\n','__________________')\nmissing_v_count_by_column_test = (X_test.isnull().sum())\nprint(missing_v_count_by_column_test[missing_v_count_by_column_test > 0])","c1697e6d":"# First of all let's impute the missing values.\n\nX_train = X_train.fillna(value = {'Alley': 'N','MasVnrType': 'N','BsmtQual': 'N',\n                                  'BsmtCond': 'N', 'BsmtExposure': 'No',\n                                  'BsmtFinType1': 'N', 'BsmtFinType2': 'N', \n                                  'Electrical': 'SBrkr','FireplaceQu': 'N', \n                                  'GarageType': 'N', 'GarageFinish': 'N', \n                                  'GarageQual': 'N', 'GarageCond': 'N', \n                                  'PoolQC': 'N', 'Fence': 'N', 'MiscFeature': 'N', \n                                  'LotFrontage': X_train['LotFrontage'].mean(), \n                                  'MasVnrArea': 0, \n                                  'GarageYrBlt': X_train['GarageYrBlt'].mean()})\n\nX_test = X_test.fillna(value = {'LotFrontage': X_test['LotFrontage'].mean(), \n                                'MasVnrArea': 0, \n                                'GarageYrBlt': X_test['GarageYrBlt'].mean(),\n                                'BsmtFinSF1': X_test['LotFrontage'].mean(),\n                                'BsmtFinSF2' : X_test['LotFrontage'].mean(),\n                                'BsmtUnfSF': X_test['LotFrontage'].mean(), \n                                'TotalBsmtSF' : X_test['LotFrontage'].mean(), \n                                'BsmtFullBath': X_test['LotFrontage'].median(),\n                                'BsmtHalfBath' : X_test['LotFrontage'].median(),\n                                'GarageCars': X_test['LotFrontage'].median(),\n                                'GarageArea' : X_test['LotFrontage'].mean(), \n                                'MSZoning': 'RL','Alley': 'N','Utilities': 'N',\n                                'Exterior1st': 'N','Exterior2nd': 'N',\n                                'MasVnrType': 'N','BsmtQual': 'N','BsmtCond': 'N',\n                                'BsmtExposure': 'N','BsmtFinType1': 'N',\n                                'BsmtFinType2': 'N','KitchenQual': 'TA',\n                                'Functional': 'Typ','FireplaceQu': 'N', \n                                'GarageType': 'N','GarageFinish': 'N',\n                                'GarageQual': 'N','GarageCond': 'N','PoolQC': 'N',\n                                'Fence': 'N','MiscFeature': 'N','SaleType': 'WD',})","fb6daf2b":"X_train.head()","e4676eff":"X_test.head()","d75cc442":"# Secondly, we need to label features, that: categorical, have cardinality 2 or more than 9. As we found earlier it is:\n# in train [('Street', 2), ('Alley', 2), ('Utilities', 2), ('CentralAir', 2), ('Exterior1st', 15), ('Exterior2nd', 16), ('Neighborhood', 25)]\n# in test [('Street', 2), ('Alley', 2), ('Utilities', 2), ('CentralAir', 2), ('Exterior1st', 15), ('Exterior2nd', 16), ('Neighborhood', 25)] the same.\n\n#Make copy to avoid changing original data\nlabel_X_train = X_train.copy()\nlabel_X_test = X_test.copy()\n\n#Apply label encoding to each column with a such data:\nlabel_encoder = LabelEncoder()\nfor col in ['Street', 'Alley', 'Utilities', 'CentralAir', 'Exterior1st', 'Exterior2nd', \n            'Neighborhood', 'MiscFeature', 'Electrical', 'Heating', 'Condition2', \n            'HouseStyle', 'RoofMatl', 'MSZoning', 'KitchenQual', \n            'Functional', 'SaleType']:\n    label_X_train[col] = label_encoder.fit_transform(X_train[col])\n    label_X_test[col] = label_encoder.fit_transform(X_test[col])\n    \n# As it would found later, we got different cardinaluty in train and test data in the following columns, \n# among those wich we would like to do with OH_encoding:\n#[('GarageFinish', 3),('MasVnrType', 4),('Fence', 4),('Electrical', 5),('FireplaceQu', 5),('GarageCond', 5),('BsmtFinType1', 6),\n#('BsmtFinType2', 6),('Heating', 6),('GarageType', 6),('Condition2', 8),('HouseStyle', 8),('RoofMatl', 8). \n# So, we need to label them here too.\n\n# We will separately control PoolQC since here the order of numbering is really important for us depending on the quality.\nlabel_X_train['PoolQC'] = label_X_train['PoolQC'].map({'N': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, \n                                                       'Ex': 4})\nlabel_X_test['PoolQC'] = label_X_test['PoolQC'].map({'N': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, \n                                                     'Ex': 4})\n\n# Similarly, the rest of the features:\nlabel_X_train['ExterQual'] = label_X_train['ExterQual'].map({'Po': 1, 'Fa': 2, 'TA': 3, \n                                                             'Gd': 4, 'Ex': 5})\nlabel_X_test['ExterQual'] = label_X_test['ExterQual'].map({'Po': 1, 'Fa': 2, 'TA': 3, \n                                                           'Gd': 4, 'Ex': 5})\n\nlabel_X_train['ExterCond'] = label_X_train['ExterCond'].map({'Po': 1, 'Fa': 2, 'TA': 3, \n                                                             'Gd': 4, 'Ex': 5})\nlabel_X_test['ExterCond'] = label_X_test['ExterCond'].map({'Po': 1, 'Fa': 2, 'TA': 3, \n                                                           'Gd': 4, 'Ex': 5})\n\nlabel_X_train['BsmtQual'] = label_X_train['BsmtQual'].map({'N': 0, 'Po': 1, 'Fa': 2, \n                                                           'TA': 3, 'Gd': 4, 'Ex': 5})\nlabel_X_test['BsmtQual'] = label_X_test['BsmtQual'].map({'N': 0, 'Po': 1, 'Fa': 2, \n                                                         'TA': 3, 'Gd': 4, 'Ex': 5})\n\nlabel_X_train['BsmtCond'] = label_X_train['BsmtCond'].map({'N': 0, 'Po': 1, 'Fa': 2, \n                                                           'TA': 3, 'Gd': 4, 'Ex': 5})\nlabel_X_test['BsmtCond'] = label_X_test['BsmtCond'].map({'N': 0, 'Po': 1, 'Fa': 2, \n                                                         'TA': 3, 'Gd': 4, 'Ex': 5})\n\nlabel_X_train['BsmtExposure'] = label_X_train['BsmtExposure'].map({'N': 0, 'No': 1, 'Mn': 2, \n                                                                   'Av': 3, 'Gd': 4})\nlabel_X_test['BsmtExposure'] = label_X_test['BsmtExposure'].map({'N': 0, 'No': 1, 'Mn': 2, \n                                                                 'Av': 3, 'Gd': 4})\n\nlabel_X_train['GarageQual'] = label_X_train['GarageQual'].map({'N': 0, 'Po': 1, 'Fa': 2, \n                                                               'TA': 3, 'Gd': 4, 'Ex': 5})\nlabel_X_test['GarageQual'] = label_X_test['GarageQual'].map({'N': 0, 'Po': 1, 'Fa': 2, \n                                                             'TA': 3, 'Gd': 4, 'Ex': 5})\n\nlabel_X_train['GarageCond'] = label_X_train['GarageCond'].map({'N': 0, 'Po': 1, 'Fa': 2, \n                                                               'TA': 3, 'Gd': 4, 'Ex': 5})\nlabel_X_test['GarageCond'] = label_X_test['GarageCond'].map({'N': 0, 'Po': 1, 'Fa': 2, \n                                                             'TA': 3, 'Gd': 4, 'Ex': 5})\n\nlabel_X_train['Fence'] = label_X_train['Fence'].map({'N': 0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, \n                                                     'GdPrv': 4})\nlabel_X_test['Fence'] = label_X_test['Fence'].map({'N': 0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, \n                                                   'GdPrv': 4})\n\nlabel_X_train['FireplaceQu'] = label_X_train['FireplaceQu'].map({'N': 0, 'Po': 1, 'Fa': 2, \n                                                                 'TA': 3, 'Gd': 4, 'Ex': 5})\nlabel_X_test['FireplaceQu'] = label_X_test['FireplaceQu'].map({'N': 0, 'Po': 1, 'Fa': 2, \n                                                               'TA': 3, 'Gd': 4, 'Ex': 5})","a593c44c":"label_X_train.head()","0aef583c":"label_X_test.head()","53d617f6":"pd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\npx.imshow(label_X_train.corr(),\n         height = 600, width = 600)","977ffd47":"# Simple feature engineering according to information on correlations between features.\nlabel_X_train['Exterior'] = label_X_train['Exterior1st'] + label_X_train['Exterior2nd']\nlabel_X_train['Pool'] = label_X_train['PoolQC'] * label_X_train['PoolArea']\nlabel_X_train['GarageScore'] = label_X_train['GarageQual'] + label_X_train['GarageCond']\nlabel_X_train['FireplaceScore'] = label_X_train['FireplaceQu'] * label_X_train['Fireplaces']\ndrop_list = [\n    'Exterior1st', 'Exterior2nd',\n    'TotalBsmtSF', 'GarageYrBlt',\n    'TotRmsAbvGrd', 'GarageCars',\n    'PoolQC', 'PoolArea', \n    'GarageQual', 'GarageCond',\n    'FireplaceQu', 'Fireplaces'\n]\nlabel_X_train.drop(drop_list,axis=1, inplace=True)\n\nlabel_X_test['Exterior'] = label_X_test['Exterior1st'] + label_X_test['Exterior2nd']\nlabel_X_test['Pool'] = label_X_test['PoolQC'] * label_X_test['PoolArea']\nlabel_X_test['GarageScore'] = label_X_test['GarageQual'] + label_X_test['GarageCond']\nlabel_X_test['FireplaceScore'] = label_X_test['FireplaceQu'] * label_X_test['Fireplaces']\nlabel_X_test.drop(drop_list,axis=1, inplace=True)","a59fd635":"# 1 - the ratio of the living area to the area of the entire plot of land.\n# turned out to have a negative effect on performance\n#label_X_train['LivLotRatio'] = label_X_train['GrLivArea'] \/ label_X_train['LotArea']\n#label_X_test['LivLotRatio'] = label_X_test['GrLivArea'] \/ label_X_test['LotArea']\n\n# 2 - spaciousness - total living area divided by the number of rooms.\n# turned out to have a negative effect on performance\n#label_X_train['Spaciousness'] = (label_X_train['1stFlrSF'] + label_X_train['2ndFlrSF']) \/ X_train['TotRmsAbvGrd']\n#label_X_test['Spaciousness'] = (label_X_test['1stFlrSF'] + label_X_test['2ndFlrSF']) \/ X_test['TotRmsAbvGrd']\n\n# 3 - total area of external objects on a land plot.\nlabel_X_train['TotalOutsideSF'] = (label_X_train['WoodDeckSF'] + label_X_train['OpenPorchSF'] + \n                                   label_X_train['EnclosedPorch'] + label_X_train['3SsnPorch'] + \n                                   label_X_train['ScreenPorch'])\nlabel_X_test['TotalOutsideSF'] = (label_X_test['WoodDeckSF'] + label_X_test['OpenPorchSF'] + \n                                  label_X_test['EnclosedPorch'] + label_X_test['3SsnPorch'] + \n                                  label_X_test['ScreenPorch'])","1fd9cc9b":"label_X_train.head()","2a0ad73f":"label_X_test.head()","b9ee26e1":"px.imshow(label_X_train.corr(),\n         height = 600, width = 600)","e2ec95bd":"# Now, 'WoodDeckSF' and 'TotalOutsideSF' seem to be quite strongly correlated (>0.74) let's drop 'WoodDeckSF'\n\nlabel_X_train.drop('WoodDeckSF',axis=1, inplace=True)\nlabel_X_test.drop('WoodDeckSF',axis=1, inplace=True)","447e90ee":"#Thirdly, we'll use OH_encoding but just with get dummies, now, for all date, because we do what we wanted before, \n#and now there are just categorical features that must be divided by one-hot encoding. \n\nlabel_X_train = pd.get_dummies(label_X_train)\nlabel_X_test = pd.get_dummies(label_X_test)","96be5e8c":"label_X_train.head()","3f84e391":"label_X_test.head()","67e35906":"from sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import cross_val_score","5833259c":"#RFR = RandomForestRegressor(n_estimators = 900, random_state=0) \n#accuracies_RFR = -1*cross_val_score(estimator= RFR ,X= label_X_train, y=y, cv=5, scoring = 'neg_mean_absolute_error')\n#print('All scoring_RFR:', accuracies_RFR ,'\\n', 'Mean score_RFR:', accuracies_RFR.mean())","5d188ba4":"#XGB = XGBRegressor(n_estimators = 10000, learning_rate = 0.095)\n#XGB.fit(label_X_train, y, early_stopping_rounds = 55, eval_set = [(label_X_train, y)], verbose = False)\n#accuracies_XGB = -1*cross_val_score(estimator= XGB ,X = label_X_train, y=y, cv=5, scoring = 'neg_mean_absolute_error')\n#print('All scoring_XGB:', accuracies_XGB ,'\\n', 'Mean score_XGB:', accuracies_XGB.mean())","0893ca90":"from xgboost import XGBRegressor\n# Testing the model\n# Split the data\ns_X_train, s_X_test, s_y_train, s_y_test = train_test_split(label_X_train, y, train_size=0.80, test_size=0.20, random_state=0)\n\n# Now, let's test our model!\nmodel = XGBRegressor(n_estimators = 10000, learning_rate = 0.095)\nmodel.fit(s_X_train, s_y_train, early_stopping_rounds = 55, eval_set = [(s_X_test, s_y_test)], verbose = False)\n\nval_predictions = model.predict(s_X_test)\nval_mae = mean_absolute_error(val_predictions, s_y_test)\nprint(val_mae)\n","4551c07e":"# Now, let's submit our model!\n\nmodel.fit(label_X_train, y, early_stopping_rounds = 55, eval_set = [(label_X_train, y)], verbose = False)\ntest_preds = model.predict(label_X_test)\n\noutput = pd.DataFrame({'Id': pd.read_csv(iowa_file_test).Id,\n                       'SalePrice': test_preds})\noutput.to_csv('submission.csv', index=False)","b0c9f0ff":"### Introduction\n\nHello everyone! I am glad to present a version of solving the problem [Housing Prices Competition for Kaggle Learn Users](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course) by basic methods, which will be interesting for novice and contributor participants, the course of the solution is presented in detail and with my thoughts and comments\ud83d\ude44.\n\n\nThe analysis is based on:\n1. XGBoost Early Stop Model;\n2. Simple feature engineering based on a heatmap of feature correlations;\n3. Train-test split method.\n\nI would be glad if this work will help you in your decision. You can freely use the various blocks of this work (or copy the entire notebook)\ud83d\ude4c.\n\nYour view is extremely important for me, please leave any questions, proposals for the improvement of model, comments or opinions, thank you\ud83d\ude0a! ","7adc44c5":"If you wish, you can test these models using cross-validation, but this can take up to 10 minutes. ","d4f8a497":"So, we got the following result:\nin train data there are 242 columns\nin test data there are 228 columns\n\nThe model wouldn't accept it. Well, why we get different amount of columns? There were following columns, that get_dummies:\n1. ('Alley', 3),\n2. ('LandSlope', 3),\n3. ('PavedDrive', 3),\n4. ('PoolQC', 3),\n5. ('LotShape', 4),\n6. ('LandContour', 4),\n7. ('RoofMatl', 4),\n8. ('ExterQual', 4),\n9. ('BsmtQual', 4),\n10. ('BsmtCond', 4),\n11. ('BsmtExposure', 4),\n12. ('Heating', 4),\n13. ('Electrical', 4),\n14. ('KitchenQual', 4),\n15. ('GarageFinish', 4),\n16. ('MiscFeature', 4),\n17. ('MSZoning', 5),\n18. ('LotConfig', 5),\n19. ('Condition2', 5),\n20. ('BldgType', 5),\n21. ('MasVnrType', 5),\n22. ('ExterCond', 5),\n23. ('HeatingQC', 5),\n24. ('GarageQual', 5),\n25. ('Fence', 5),\n26. ('RoofStyle', 6),\n27. ('Foundation', 6),\n28. ('FireplaceQu', 6),\n29. ('GarageCond', 6),\n30. ('SaleCondition', 6),\n31. ('HouseStyle', 7),\n32. ('BsmtFinType1', 7),\n33. ('BsmtFinType2', 7),\n34. ('Functional', 7),\n35. ('GarageType', 7),\n36. ('Condition1', 9),\n37. ('SaleType', 9).\n\nAnd we have 242 - 228 = 14 different columns, what is it? Well, there are at least 2 problems:\n1. Some columns has different cardinality in train and test.\n2. Some of these columns have NONE in test and have no missing values in train and vise versa.\nWhat can we do?\n1. We can find these columns and drop them,\n2. or just label them too, like others.\n\nOk, done. Now we have in train 173 columns and 170 in test data, still why?\nThat's because there are some NONE that were inpute like 'N' in train data, and in test data there were no empty (in such columns).\n\nSo, finally we get in both data 179 columns with following label features:\n['Street','Alley','Utilities','CentralAir','Exterior1st','Exterior2nd','Neighborhood','Utilities','PoolQC','MiscFeature',\n'Electrical','GarageQual','Heating','Condition2','HouseStyle','RoofMatl','Exterior1st','Exterior2nd','MSZoning','KitchenQual','Functional', 'SaleType']","c2af8374":"Great, now we see that according to Pearson's correlation, we have the following pairs of features with a correlation greater than 0.75:\n1. *Exterior1st*: Exterior covering on house and *Exterior2nd*: Exterior covering on house (if more than one material). \n<br>Instead, we use one column with the sum of the values of these columns \n2. *1stFlrSF*: First Floor square feet and *TotalBsmtSF*: Total square feet of basement area. These are very similar parameters, so let's drop *TotalBsmtSF*.\n3. *YearBuilt*: Original construction date and *GarageYrBlt*: Year garage was built. Given that the year the garage was built is not the only feature containing information about the garage, we will drop exactly *GarageYrBlt*.\n4. *TotRmsAbvGrd*: Total rooms above grade (does not include bathrooms) and *GrLivArea*: Above grade (ground) living area square feet. An interesting moment of choosing between these parameters, in my opinion, for the final cost, it is not the number of rooms that is more important, but the living area. So we will drop *TotRmsAbvGrd*.\n5. *GarageCars*: Size of garage in car capacity and *GarageArea*: Size of garage in square feet. Same as previous. We will drop *GarageCars*.\n6. *PoolQC*: Pool quality and *PoolArea*: Pool area in square feet. They are highly correlated only because most of the data is zero and the quality of such pools is zero. Let's create a feature equal to the product of the pool quality by its area.\n7. Similarly with: *GarageQual* and *GarageCond*\n8. Similarly with: *FireplaceQu* and *Fireplaces*","dd04ce8f":"The following is the reasoning about the number of columns in the test and training DF, after applying transformations to numerical values (hidden by default). ","bbb31d91":"Now, after adjusting and introducing again new features, let's once again check the data for Pearson's correlation.","ac1ad15f":"<br>UPD 22.06.2021: \n<br>-Added new synthetic features, including those based on the [Kaggle Feature Engineering Course](https:\/\/www.kaggle.com\/learn\/feature-engineering)\n<br>-Applied Label Encoding to more variables\n<br>-New correlated features removed\n<br>-Improved model quality from 14635.77 to 14562.81 Public Score \ud83c\udf89\n<br>UPD.0: \n<br>-Added commentary on pool-related features \n<br>-Improved model quality from 14840.11 to 14635.77 Public Score \ud83c\udf89","60eb0567":"__________________________________\nBefore we apply OHE method (which will greatly increase the number of our columns), let's see the correlation between our features. \n<br> Instead of the traditional *sns.heatmap( )* we use an interactive heat map:","cdbf834b":"\u0421ounting the number of missing values:","da5b896e":"Now I would like to introduce a separate block for creating synthetic features. These variables will be based on knowledge of the *subject area*. I will use the new features in accordance with a similar recommendation that was shown in the [Kaggle Feature Engineering Course](https:\/\/www.kaggle.com\/learn\/feature-engineering), namely in the practical section of the lesson [Creating Features](https:\/\/www.kaggle.com\/ryanholbrook\/creating-features).","16c2b797":"So, we simply have to pay attention to features with a large amount of missing values.\n<br> ***Alley*** - Type of alley access has the following possible meanings: *Grvl* -> Gravel; *Pave* -> Paved; *NA* -> No alley access.\n<br> That is why we do not need to drop this column, we just need to fill in the gaps with a certain value, meaning \"the absence of alley access\", for example, like 'N'.\n<br> The same thing with ***FireplaceQu***, ***PoolQC***,***Fence*** and ***MiscFeature***. We will fill it the same way as 'N'. \n<br> In fact, it doesn't matter at all what name we assign to the missing values, it is only important to translate it into a categorical variable, later we will still change this value to a numeric one. ","adbe067a":"It's ok, we have the same amount of categorical data in train and test both.\n<br\/>\u0421ounting the number of unique values:"}}