{"cell_type":{"367b0060":"code","da76d1d3":"code","ba6d7485":"code","5cf03364":"code","0107bd0a":"code","73186071":"code","e91e0ce6":"code","8252beeb":"markdown","6f618fa7":"markdown","85a5fe19":"markdown","33ac504f":"markdown"},"source":{"367b0060":"from sklearn.datasets import fetch_20newsgroups\n\ncategories = ['alt.atheism', 'soc.religion.christian',\n              'comp.graphics', 'sci.med']\ntwenty_train = fetch_20newsgroups(\n    subset='train',\n    categories=categories,\n    shuffle=True,\n    random_state=42,\n    remove=('headers', 'footers'),\n)\ntwenty_test = fetch_20newsgroups(\n    subset='test',\n    categories=categories,\n    shuffle=True,\n    random_state=42,\n    remove=('headers', 'footers'),\n)","da76d1d3":"print(twenty_train.DESCR)\n","ba6d7485":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom keras.models import Model, Input\nfrom keras.layers import Dense, LSTM, Dropout, Embedding, SpatialDropout1D, Bidirectional, concatenate\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.metrics import accuracy_score\nfrom eli5.lime import TextExplainer\nimport regex as re\nimport numpy as np\nimport eli5\n\n\nclass KerasTextClassifier(BaseEstimator, TransformerMixin):\n    '''Wrapper class for keras text classification models that takes raw text as input.'''\n    \n    def __init__(self, max_words=30000, input_length=100, emb_dim=20, n_classes=4, epochs=5, batch_size=32):\n        self.max_words = max_words\n        self.input_length = input_length\n        self.emb_dim = emb_dim\n        self.n_classes = n_classes\n        self.epochs = epochs\n        self.bs = batch_size\n        self.model = self._get_model()\n        self.tokenizer = Tokenizer(num_words=self.max_words+1,\n                                   lower=True, split=' ', oov_token=\"UNK\")\n    \n    def _get_model(self):\n        input_text = Input((self.input_length,))\n        text_embedding = Embedding(input_dim=self.max_words + 2, output_dim=self.emb_dim,\n                                   input_length=self.input_length, mask_zero=False)(input_text)\n        text_embedding = SpatialDropout1D(0.5)(text_embedding)\n        bilstm = Bidirectional(LSTM(units=32, return_sequences=True, recurrent_dropout=0.5))(text_embedding)\n        x = concatenate([GlobalAveragePooling1D()(bilstm), GlobalMaxPooling1D()(bilstm)])\n        x = Dropout(0.7)(x)\n        x = Dense(512, activation=\"relu\")(x)\n        x = Dropout(0.6)(x)\n        x = Dense(512, activation=\"relu\")(x)\n        x = Dropout(0.5)(x)\n        out = Dense(units=self.n_classes, activation=\"softmax\")(x)\n        model = Model(input_text, out)\n        model.compile(optimizer=\"adam\",\n                      loss=\"sparse_categorical_crossentropy\",\n                      metrics=[\"accuracy\"])\n        return model\n    \n    def _get_sequences(self, texts):\n        seqs = self.tokenizer.texts_to_sequences(texts)\n        return pad_sequences(seqs, maxlen=self.input_length, value=0)\n    \n    def _preprocess(self, texts):\n        return [re.sub(r\"\\d\", \"DIGIT\", x) for x in texts]\n    \n    def fit(self, X, y):\n        '''\n        Fit the vocabulary and the model.\n        \n        :params:\n        X: list of texts.\n        y: labels.\n        '''\n        \n        self.tokenizer.fit_on_texts(self._preprocess(X))\n        self.tokenizer.word_index = {e: i for e,i in self.tokenizer.word_index.items() if i <= self.max_words}\n        self.tokenizer.word_index[self.tokenizer.oov_token] = self.max_words + 1\n        seqs = self._get_sequences(self._preprocess(X))\n        self.model.fit(seqs, y, batch_size=self.bs, epochs=self.epochs, validation_split=0.1)\n    \n    def predict_proba(self, X, y=None):\n        seqs = self._get_sequences(self._preprocess(X))\n        return self.model.predict(seqs)\n    \n    def predict(self, X, y=None):\n        return np.argmax(self.predict_proba(X), axis=1)\n    \n    def score(self, X, y):\n        y_pred = self.predict(X)\n        return accuracy_score(y, y_pred)","5cf03364":"text_model = KerasTextClassifier(epochs=15, max_words=25000, input_length=200)\n","0107bd0a":"text_model.fit(twenty_train.data, twenty_train.target)\n","73186071":"text_model.score(twenty_test.data, twenty_test.target)\n\n","e91e0ce6":"doc = twenty_test.data[40]\nte = TextExplainer(random_state=42)\nte.fit(doc, text_model.predict_proba)\nte.show_prediction(target_names=twenty_train.target_names)","8252beeb":"## News Data from Sklearn##\n\n### We will build a Deep Neural Network to classify a new group article which has  20 categories , but for illustration we are selecting only 4 categories as mentioned below\n\n### 'alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian'\n\n### Later we will see how model explanation are performed - in this which words were helps in predicting the target class.\n\n### Model explanation is a key component in real world implementation of Deep Neural Network","6f618fa7":"## Lets test the model ##","85a5fe19":"## Lets check the model prediction with an explanation ##","33ac504f":"### ELI5 gives the probability for each category and also highlight the keys words which hints towards the prediction.\n\n### These Keywords provide a high level explanation of the model.\n\n### Hope you like this example , please upvote it  !!!"}}