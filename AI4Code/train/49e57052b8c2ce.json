{"cell_type":{"350f099d":"code","845b0abb":"code","12818169":"code","44f1bb16":"code","e2b2a886":"code","0fbf0433":"code","093ad8fa":"code","3ae7a59d":"code","6c6369e3":"code","834a6a95":"code","8860085b":"code","65fee95e":"code","31779715":"code","0e770a1f":"code","e1d67126":"markdown","9db3f5fc":"markdown"},"source":{"350f099d":"from keras.applications.vgg16 import VGG16\nfrom keras.preprocessing import image\nfrom keras.applications.vgg16 import preprocess_input\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\nimport scipy.misc","845b0abb":"model = VGG16(weights='imagenet')","12818169":"img_path = \"..\/input\/train\/images\/1bd1c8c771.png\"\nimg = image.load_img(img_path, target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)\n\npreds = model.predict(x)","44f1bb16":"\nplt.plot(preds.ravel())","e2b2a886":"#https:\/\/keras.io\/getting-started\/faq\/#how-can-i-obtain-the-output-of-an-intermediate-layer\nfrom keras import backend as K\nfrom keras.preprocessing.image import array_to_img, img_to_array, load_img\n# with a Sequential model\nget_feature = K.function([model.layers[0].input],\n                                  [model.layers[2].output])\n#layer_output = get_feature([x])[0]\nfeat = get_feature([x])[0]\nprint(feat.shape)\n","0fbf0433":"conv_outputs = feat[0, :, :, :]\nconv_outputs.shape","093ad8fa":"feat_map = conv_outputs.transpose((2,0,1))\nfeat_map.shape","3ae7a59d":"#showing 3rd feature map\nplt.imshow(feat_map[2])","6c6369e3":"from skimage.transform import resize\ndef extract_hypercolumn(model, layer_indexes, instance):\n    layers = [model.layers[li].output for li in layer_indexes]\n    get_feature = K.function([model.layers[0].input],layers)\n    for layer in layers:\n        print(layer.name)\n\n    feature_maps = get_feature(instance)\n    hypercolumns = []\n    for convmap in feature_maps:\n        print(convmap.shape)\n        conv_out = convmap[0, :, :, :]\n        feat_map = conv_out.transpose((2,0,1))\n        print(\"F sz :\",feat_map.shape)\n        for fmap in feat_map: \n            #print(fmap.shape)\n            upscaled =resize(fmap, (224, 224), mode='constant', preserve_range=True)\n            hypercolumns.append(upscaled)\n    return np.asarray(hypercolumns)","834a6a95":"for idx, layer in enumerate(model.layers):\n    if \"conv\" in layer.name:\n        print(idx, layer.name)    ","8860085b":"layers_extract = [1]\nhc = extract_hypercolumn(model, layers_extract, [x])\nprint(hc.shape)","65fee95e":"avg_hc=np.average(hc, axis=0)\nprint(avg_hc.shape)\nplt.imshow(avg_hc)\n","31779715":"layers_extract = [5,8,12]\nhc2 = extract_hypercolumn(model, layers_extract, [x])","0e770a1f":"import sklearn.cluster as cluster\nm = hc2.transpose(1,2,0).reshape(224*224, -1)\nkmeans = cluster.KMeans(n_clusters=2, max_iter=300, n_jobs=5, precompute_distances=True)\ncluster_labels = kmeans .fit_predict(m)\nimcluster = np.zeros((224,224))\nimcluster = imcluster.reshape((224*224,))\nimcluster = cluster_labels\nplt.imshow(imcluster.reshape(224, 224), cmap=\"hot\")","e1d67126":"**Extracting hypercolumns**","9db3f5fc":"**Hyper columns**\n\nMany algorithms using features from CNNs (Convolutional Neural Networks) usually use the last FC (fully-connected) layer features in order to extract information about certain input. However, the information in the last FC layer may be too coarse spatially to allow precise localization (due to sequences of maxpooling, etc.), on the other side, the first layers may be spatially precise but will lack semantic information. To get the best of both worlds, the authors of the hypercolumn paper define the hypercolumn of a pixel as the vector of activations of all CNN units \u201cabove\u201d that pixel.\nhttp:\/\/arxiv.org\/pdf\/1411.5752v2.pdf\n\n![img](http:\/\/blog.christianperone.com\/wp-content\/uploads\/2016\/01\/hypercolumn.png)"}}