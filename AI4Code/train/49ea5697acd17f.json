{"cell_type":{"d7ce814f":"code","78dc655c":"code","5ced1aff":"code","a4d48306":"code","168f1ead":"code","5774880b":"code","cfae2e7f":"code","3eaff413":"code","245b15b9":"code","2379e20c":"code","7ce76f69":"code","b5533c5d":"code","2f7a809c":"code","51b7475d":"code","aa99f07b":"code","bccb5793":"code","bee3f550":"code","c4fb819c":"code","1414970f":"code","64b6508a":"code","d2316730":"code","772a4840":"code","9f951e52":"code","6ff70ee5":"code","53fc7894":"code","c02c6ae8":"code","01dda085":"code","8405554b":"code","cd3e31db":"code","f5c16fb4":"markdown","1360d916":"markdown","e3e3612b":"markdown","9095f1de":"markdown","4b33996f":"markdown","7d5a9f6f":"markdown","6f75cc23":"markdown","4e734070":"markdown","f1692977":"markdown","36163140":"markdown","eef4d73a":"markdown","05c4f933":"markdown","8e95e509":"markdown","c74bebfa":"markdown","20c2fb35":"markdown","3f6b8622":"markdown","229ed232":"markdown","e595cb9a":"markdown","62457fd2":"markdown","02fe5d76":"markdown","3e29efd7":"markdown","e3ecc1a0":"markdown","b7b85273":"markdown","606e7c8b":"markdown","6b0667b2":"markdown","d3077823":"markdown","b5bf02ca":"markdown","fffe7a64":"markdown","09aab76b":"markdown","19e8ad60":"markdown"},"source":{"d7ce814f":"!pip install -U git+https:\/\/github.com\/sberbank-ai-lab\/LightAutoML.git","78dc655c":"# Standard python libraries\nimport os\nimport time\n\n# Essential DS libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import train_test_split\nimport torch\n\n# LightAutoML presets, task and report generation\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\nfrom lightautoml.tasks import Task\nfrom lightautoml.report.report_deco import ReportDeco","5ced1aff":"N_THREADS = 4\nN_FOLDS = 5\nRANDOM_STATE = 42\nTEST_SIZE = 0.2\nTIMEOUT = 600\nTARGET_NAME = 'price'","a4d48306":"np.random.seed(RANDOM_STATE)\ntorch.set_num_threads(N_THREADS)","168f1ead":"data = pd.read_csv('..\/input\/housesalesprediction\/kc_house_data.csv')\ndata['date'] = pd.to_datetime(data['date']).astype(str)\ndata.head()","5774880b":"data.shape","cfae2e7f":"tr_data, te_data = train_test_split(\n    data, \n    test_size=TEST_SIZE, \n    random_state=RANDOM_STATE\n)\n\nprint(f'Data splitted. Parts sizes: tr_data = {tr_data.shape}, te_data = {te_data.shape}')\n\ntr_data.head()","3eaff413":"task = Task('reg', loss = 'rmsle', metric = 'rmsle')","245b15b9":"roles = {\n    'target': TARGET_NAME,\n    'drop': ['id']\n}","2379e20c":"automl = TabularAutoML(\n    task = task, \n    timeout = TIMEOUT,\n    cpu_limit = N_THREADS,\n    reader_params = {'n_jobs': N_THREADS, 'cv': N_FOLDS, 'random_state': RANDOM_STATE}\n)","7ce76f69":"%%time \noof_pred = automl.fit_predict(tr_data, roles = roles, verbose = 1)","b5533c5d":"%%time\n\nte_pred = automl.predict(te_data)\nprint(f'Prediction for te_data:\\n{te_pred}\\nShape = {te_pred.shape}')","2f7a809c":"print(f'OOF score: {mean_squared_log_error(tr_data[TARGET_NAME].values, oof_pred.data[:, 0])**0.5}')\nprint(f'HOLDOUT score: {mean_squared_log_error(te_data[TARGET_NAME].values, te_pred.data[:, 0])**0.5}')","51b7475d":"print(automl.create_model_str_desc())","aa99f07b":"RD = ReportDeco(output_path = 'tabularAutoML_model_report')\n\nautoml_rd = RD(\n    TabularAutoML(\n        task = task, \n        timeout = TIMEOUT,\n        cpu_limit = N_THREADS,\n        reader_params = {'n_jobs': N_THREADS, 'cv': N_FOLDS, 'random_state': RANDOM_STATE}\n    )\n)","bccb5793":"%%time\noof_pred = automl_rd.fit_predict(tr_data, roles = roles, verbose = 1)","bee3f550":"!ls tabularAutoML_model_report","c4fb819c":"%%time\n\nte_pred = automl_rd.predict(te_data)\nprint(f'Prediction for te_data:\\n{te_pred}\\nShape = {te_pred.shape}')","1414970f":"print(f'OOF score: {mean_squared_log_error(tr_data[TARGET_NAME].values, oof_pred.data[:, 0])**0.5}')\nprint(f'HOLDOUT score: {mean_squared_log_error(te_data[TARGET_NAME].values, te_pred.data[:, 0])**0.5}')","64b6508a":"%%time\n\n# Fast feature importances calculation\nfast_fi = automl_rd.model.get_feature_scores('fast')\nfast_fi.set_index('Feature')['Importance'].plot.bar(figsize = (30, 10), grid = True)","d2316730":"%%time\n\n# Accurate feature importances calculation (Permutation importances) -  can take long time to calculate\naccurate_fi = automl_rd.model.get_feature_scores('accurate', te_data, silent = False)","772a4840":"accurate_fi.set_index('Feature')['Importance'].plot.bar(figsize = (30, 10), grid = True)","9f951e52":"utilized_automl = TabularUtilizedAutoML(\n    task = task, \n    timeout = 2 * TIMEOUT,\n    cpu_limit = N_THREADS,\n    reader_params = {'n_jobs': N_THREADS, 'cv': N_FOLDS, 'random_state': RANDOM_STATE},\n)","6ff70ee5":"%%time \n\noof_pred = utilized_automl.fit_predict(tr_data, roles = roles, verbose = 1)","53fc7894":"print('oof_pred:\\n{}\\nShape = {}'.format(oof_pred, oof_pred.shape))","c02c6ae8":"print(utilized_automl.create_model_str_desc())","01dda085":"%%time\n\n# Fast feature importances calculation\nfast_fi = utilized_automl.get_feature_scores('fast')\nfast_fi.set_index('Feature')['Importance'].plot.bar(figsize = (30, 10), grid = True)","8405554b":"%%time\n\nte_pred = utilized_automl.predict(te_data)\nprint(f'Prediction for te_data:\\n{te_pred}\\nShape = {te_pred.shape}')","cd3e31db":"print(f'OOF score: {mean_squared_log_error(tr_data[TARGET_NAME].values, oof_pred.data[:, 0])**0.5}')\nprint(f'HOLDOUT score: {mean_squared_log_error(te_data[TARGET_NAME].values, te_pred.data[:, 0])**0.5}')","f5c16fb4":"- [Official LightAutoML github repo](https:\/\/github.com\/sberbank-ai-lab\/LightAutoML)\n- [LightAutoML documentation](https:\/\/lightautoml.readthedocs.io\/en\/latest)","1360d916":"Feature importances calculation for `TabularUtilizedAutoML`:","e3e3612b":"# 5. Spending more from TIMEOUT - `TabularUtilizedAutoML` usage\n\nUsing `TabularAutoML` we do not spend the whole `TIMEOUT` equal to 10 minutes. To spend (almost) all the `TIMEOUT` we can use `TabularUtilizedAutoML` preset instead of `TabularAutoML`, which has the same API:","9095f1de":"# 4. Model analysis","4b33996f":"### 1.2. Feature roles setup","7d5a9f6f":"# Additional materials","6f75cc23":"Also for this purposes LightAutoML have ReportDeco, use it to build reports:","4e734070":"# 3. Prediction on holdout and model evaluation","f1692977":"### 1.3. LightAutoML model creation - TabularAutoML preset","36163140":"So the report is available in tabularAutoML_model_report folder","eef4d73a":"<img src=\"https:\/\/github.com\/sberbank-ai-lab\/LightAutoML\/raw\/master\/imgs\/LightAutoML_logo_big.png\" alt=\"LightAutoML logo\" style=\"width:70%;\"\/>","05c4f933":"## Bonus: where is the automatic report?\n\nAs we used `automl_rd` in our training and prediction cells, it is already ready in the folder we specified - you can check the output kaggle folder and find the `tabularAutoML_model_report` folder with `lama_interactive_report.html` report inside (or just [click this link](tabularAutoML_model_report\/lama_interactive_report.html) for short). It's interactive so you can click the black triangles on the left of the texts to go deeper in selected part.","8e95e509":"In next the cell we are going to create LightAutoML model with `TabularAutoML` class - preset with default model structure like in the image below:\n\n<img src=\"https:\/\/github.com\/sberbank-ai-lab\/LightAutoML\/raw\/master\/imgs\/tutorial_blackbox_pipeline.png\" alt=\"TabularAutoML preset pipeline\" style=\"width:85%;\"\/>\n\nin just several lines. Let's discuss the params we can setup:\n- `task` - the type of the ML task (the only **must have** parameter)\n- `timeout` - time limit in seconds for model to train\n- `cpu_limit` - vCPU count for model to use\n- `reader_params` - parameter change for Reader object inside preset, which works on the first step of data preparation: automatic feature typization, preliminary almost-constant features, correct CV setup etc. For example, we setup `n_jobs` threads for typization algo, `cv` folds and `random_state` as inside CV seed.\n\n**Important note**: `reader_params` key is one of the YAML config keys, which is used inside `TabularAutoML` preset. [More details](https:\/\/github.com\/sberbank-ai-lab\/LightAutoML\/blob\/master\/lightautoml\/automl\/presets\/tabular_config.yml) on its structure with explanation comments can be found on the link attached. Each key from this config can be modified with user settings during preset object initialization. To get more info about different parameters setting (for example, ML algos which can be used in `general_params->use_algos`) please take a look at our [article on TowardsDataScience](https:\/\/towardsdatascience.com\/lightautoml-preset-usage-tutorial-2cce7da6f936).\n\nMoreover, to receive the automatic report for our model we will use `ReportDeco` decorator and work with the decorated version in the same way as we do with usual one. ","c74bebfa":"# 1. Task definition","20c2fb35":"### 1.1. Task type\n\nOn the cell below we create Task object - the class to setup what task LightAutoML model should solve with specific loss and metric if necessary (more info can be found [here](https:\/\/lightautoml.readthedocs.io\/en\/latest\/generated\/lightautoml.tasks.base.Task.html#lightautoml.tasks.base.Task) in our documentation):","3f6b8622":"### 0.1. Import libraries\n\nHere we will import the libraries we use in this kernel:\n- Standard python libraries for timing, working with OS etc.\n- Essential python DS libraries like numpy, pandas, scikit-learn and torch (the last we will use in the next cell)\n- LightAutoML modules: presets for AutoML, task and report generation module","229ed232":"### 0.4. Data loading\nLet's check the data we have:","e595cb9a":"# 2. AutoML training","62457fd2":"### 0.5. Data splitting for train-holdout\nAs we have only one file with target values, we can split it into 80%-20% for holdout usage:","02fe5d76":"You can obtain the description of the resulting pipeline:","3e29efd7":"## 4.1. Reports","e3ecc1a0":"### 0.0. install LightAutoML","b7b85273":"## 4.2 Feature importances calculation \n\nFor feature importances calculation we have 2 different methods in LightAutoML:\n- Fast (`fast`) - this method uses feature importances from feature selector LGBM model inside LightAutoML. It works extremely fast and almost always (almost because of situations, when feature selection is turned off or selector was removed from the final models with all GBM models). no need to use new labelled data.\n- Accurate (`accurate`) - this method calculate *features permutation importances* for the whole LightAutoML model based on the **new labelled data**. It always works but can take a lot of time to finish (depending on the model structure, new labelled dataset size etc.).\n\nIn the cell below we will use `automl_rd.model` instead `automl_rd` because we want to take the importances from the model, not from the report. But **be carefull** - everything, which is calculated using `automl_rd.model` will not go to the report.","606e7c8b":"To run autoML training use fit_predict method:\n- `train_data` - Dataset to train.\n- `roles` - Roles dict.\n- `verbose` - Controls the verbosity: the higher, the more messages.\n        <1  : messages are not displayed;\n        >=1 : the computation process for layers is displayed;\n        >=2 : the information about folds processing is also displayed;\n        >=3 : the hyperparameters optimization process is also displayed;\n        >=4 : the training process for every algorithm is displayed;\n\nNote: out-of-fold prediction is calculated during training and returned from the fit_predict method","6b0667b2":"### 0.3. Imported models setup\n\nFor better reproducibility fix numpy random seed with max number of threads for Torch (which usually try to use all the threads on server):","d3077823":"# Tutorial 1: Basics\n\n\nIn this tutorial you will learn how to:\n* run LightAutoML training on tabular data\n* obtain feature importances and reports\n* configure resource usage in LightAutoML\n\nOfficial LightAutoML github repository is [here](https:\/\/github.com\/sberbank-ai-lab\/LightAutoML)","b5bf02ca":"## 0. Prerequisites","fffe7a64":"To solve the task, we need to setup columns roles. The **only role you must setup is target role**, everything else (drop, numeric, categorical, group, weights etc.) is up to user - LightAutoML models have automatic columns typization inside:","09aab76b":"Prediction on holdout and metric calculation","19e8ad60":"### 0.2. Constants\n\nHere we setup the constants to use in the kernel:\n- `N_THREADS` - number of vCPUs for LightAutoML model creation\n- `N_FOLDS` - number of folds in LightAutoML inner CV\n- `RANDOM_STATE` - random seed for better reproducibility\n- `TEST_SIZE` - houldout data part size \n- `TIMEOUT` - limit in seconds for model to train\n- `TARGET_NAME` - target column name in dataset"}}