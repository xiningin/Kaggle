{"cell_type":{"2d770e13":"code","f28926f1":"code","e67ed20f":"code","40f4f5f7":"code","cce046cd":"code","600d9901":"code","d656bfb9":"code","c8a8b938":"code","d71dc1a6":"code","fbe64a5e":"code","09420dd8":"code","2bb0349f":"code","a99e1aac":"code","1ae0675c":"code","11511064":"code","d655ff84":"code","c0844cb6":"code","a1623be8":"code","82c727df":"code","732b4efb":"code","f101f974":"code","19f158b6":"code","4b4e95c3":"code","cb2bb133":"code","14c861ee":"code","61cb7250":"code","2822629a":"code","d3e03345":"code","7b90f87e":"code","55b24c1d":"code","58223bfe":"code","3712d535":"code","aae00486":"code","e00400d6":"code","266e62d8":"code","cdce70a7":"code","c752814a":"code","7e45b4bd":"code","911c3363":"code","7fd38590":"markdown","ec135554":"markdown","56a26358":"markdown","1ef4ac4c":"markdown","4d63cbad":"markdown","c34c3f11":"markdown","e03bfcc1":"markdown","dd3e01d3":"markdown","9cf84900":"markdown","0eea4e86":"markdown","5674b9d8":"markdown","ee882db7":"markdown","b6db22d8":"markdown","13fe3687":"markdown","de8965de":"markdown","2b6e090b":"markdown","4d366374":"markdown","edbcf486":"markdown","325b42fe":"markdown","612ba801":"markdown","440d85cf":"markdown","4723b34a":"markdown","ea5d6f11":"markdown","18f3531b":"markdown","e6a97ee0":"markdown","c6377b35":"markdown","746f4fb2":"markdown","fdd45db0":"markdown"},"source":{"2d770e13":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom statsmodels.graphics.gofplots import qqplot\nimport seaborn as sns\nfrom scipy.stats import binom\nfrom scipy.stats import bernoulli\nfrom scipy.stats import norm\nfrom scipy.stats import uniform\nfrom scipy.stats import poisson\nfrom scipy.stats import sem\nimport scipy","f28926f1":"#genrate test dataset\nnp.random.seed(1)\ndata =  5 * np.random.randn(100) + 50\nnp.mean(data) , np.std(data)","e67ed20f":"#HIST PLOT\nplt.hist(data)\nplt.show()","40f4f5f7":"#QQ PLOT\n#https:\/\/www.youtube.com\/watch?v=okjYjClSjOg\nqqplot(data,line='s')#line='s' means standarrized line to match\nplt.show()","cce046cd":"# H0 - sample was drawn from a Gaussian distribution\n# H1 - sample was not \n# p <= alpha: reject H0, not normal.\n# p > alpha: fail to reject H0, normal.","600d9901":"#Shapiro-Wilk Test\n    #suitable for smaller samples of data, e.g. thousands of observations or fewer\n#TEST STATASTIC -> how far a sample estimate is from what we expected if H0 is true\nfrom scipy.stats import shapiro\n\ntest_stat , p = shapiro(data)\nprint(\"test staistic\",test_stat),print(\"p_value\",p)\nalpha = 0.05\nif p <=alpha:print(\"reject H0\")\nelse:print(\"fail to reject H0\")","d656bfb9":"#D\u2019Agostino\u2019s K^2 Test\nfrom scipy.stats import normaltest\ntest_stat , p = normaltest(data) #test_stat = s^2 + k^2 where s=skew and k=Kurtosis\nprint(\"test staistic\",test_stat),print(\"p_value\",p)\nalpha = 0.05\nif p <=alpha:print(\"reject H0\")\nelse:print(\"fail to reject H0\")","c8a8b938":"#Anderson-Darling Test\nfrom scipy.stats import anderson\nresult = anderson(data,dist='norm')\nprint(\"test statistic\",result.statistic)\nprint(\"sl\",\"  | \",\"cv\")\nfor i in range(len(result.critical_values)):\n    sl,cv = result.significance_level[i],result.critical_values[i]\n    if result.statistic < cv:\n        print(sl,\"|\",cv,\"fail to reject H0\")\n    else:\n        print(sl,cv,\"reject H0\")","d71dc1a6":"#PMF(discrete variable)\nn=10\np=0.5\n#X = np.arange(binom.ppf(0.01,n,p),binom.ppf(0.99,n,p))\nX=range(11)\nplt.plot(X,binom.pmf(X, n, p),'bo',ms=8,label='binom pmf')\nplt.vlines(X,0,binom.pmf(X,n,p),colors='b', lw=5, alpha=0.5)\nplt.legend()\nplt.show()","fbe64a5e":"#x = np.arange(norm.ppf(0.01),norm.ppf(0.99),0.01)\nx = np.linspace(3,-3,100)\nplt.plot(x,norm.pdf(x),'r-')\nr = norm.rvs(size=1000)\nplt.hist(r,density=True,histtype='stepfilled', alpha=0.2)\nplt.show()","09420dd8":"# For one reason because we made the assumption that the [-3,3] interval fits well to describe\n#the normal distribution (and we were right!).But what if we don't really know \n#the practical limits of the x values for a specific distribution\n#for any distribution the ppf() function returns a x value that corresponds to the probability that this value appears.\nrv = norm(loc=0, scale=1)           # Freeze the norm distribution at mean 0 and std 1\nx = np.linspace(rv.ppf(0.0001), rv.ppf(0.9999), 100) #give a value of x where cdf value is 0.0001 and 0.9999\ny = norm.pdf(x)\nplt.plot(x,y)\nplt.show()","2bb0349f":"#CDF\nstd=2\nmean=0\nx = np.linspace(-2*std+mean,2*std+mean,100)\ny = norm.pdf(x,loc=mean,scale=std)\nplt.plot(x, y, label='PDF')\nplt.plot(x,norm.cdf(x), 'r', label='CDF')\nplt.legend(loc='best')\nplt.show()","a99e1aac":"#UNIFORM DISTRIBUTION\ndist = np.random.random(1000)\nsns.scatterplot(data=dist)\nplt.title('Scatterplot for uniform distribution')","1ae0675c":"#plt.hist(dist)\nsns.distplot(dist)\nplt.title('uniform distribution')","11511064":"#Bernoulli distribution\n#head 1 tail 0\np=0.6 #head with prob p=0.6\nb_dist = bernoulli.rvs(p,size=1000)\ndf = pd.DataFrame({'value':b_dist})\ndf[df==0].count().value,df[df==1].count().value#no of head and no of tail","d655ff84":"df_count = df['value'].value_counts()\ndf_count.plot(kind='bar', rot=0)","c0844cb6":"#UNIFORM\nuniform_d = uniform.rvs(size=100,loc=0,scale=10)\nsns.distplot(uniform_d,bins=10,kde=True)\nplt.xlabel('Uniform')\nplt.ylabel('Frequency')","a1623be8":"#NORMAL\nnorm_d = norm.rvs(size=10000,loc=0,scale=2)\nsns.distplot(norm_d,bins=50,kde=True)\nplt.xlabel('Normal')\nplt.ylabel('Frequency')","82c727df":"#Bernouli\ndata_bern = bernoulli.rvs(size=10000,p=0.6)\nax = sns.distplot(data_bern,kde=False)\nplt.xlabel('bernoulli')\nplt.ylabel('Frequency')","732b4efb":"#Binomial\nbinom_d = binom.rvs(n=10,p=0.5,size=1000)\nsns.distplot(binom_d,kde=False,color='blue')\nplt.xlabel('Binomal')\nplt.ylabel('Frequency')","f101f974":"#poisson\npoisson_d = poisson.rvs(mu=3,size=10000)\nsns.distplot(poisson_d,kde=False,label='rate=3')\nplt.xlabel('Possion')\nplt.ylabel('Frequency')\nplt.legend()","19f158b6":"norm_data = pd.DataFrame({'Value':norm_d})\nprint(norm_data.describe())\nprint()\nprint('Mean',norm_d.mean())\nprint('var',norm_d.var())\nprint('std',norm_d.std())\nprint('Skewness',norm_data['Value'].skew())\nprint('Kurtosis',norm_data['Value'].kurtosis())","4b4e95c3":"print('Standard error of uniform sample',sem(uniform_d))\nprint('Standard error of norm sample',sem(norm_d))\nprint('Standard error of binomial sample',sem(binom_d))","cb2bb133":"np.random.seed(1)\ndata1 =  20 * np.random.randn(1000) + 100 #100 mean and 20 std\ndata2 = data1 + 10 * np.random.randn(1000) + 50\nprint('data1-> mean: ',data1.mean(),',std: ',data1.std())\nprint('data2-> mean: ',data2.mean(),',std: ',data2.std())","14c861ee":"sns.scatterplot(data1,data2)","61cb7250":"#covariance matrix\n#cov(X, Y) = (sum (x - mean(X)) * (y - mean(Y))) * 1\/(n-1)\nnp.cov(data1,data2)\n# positive corelate two variable -> 389.7545618\n#Problem -> hard to interprate","2822629a":"#Pearson correlation coefficient\n#normalization of the covariance between the two variables to give an interpretable score\n#Pearson's correlation coefficient = cov(X, Y) \/ (stdv(X) * stdv(Y))\ncor , _ =scipy.stats.pearsonr(data1,data2)\ncor","d3e03345":"#Spearman\u2019s correlation coefficient\n#measure nonlinear relationship and variable may not have gaussian distribution\n#Spearman's correlation coefficient = covariance(rank(X), rank(Y)) \/ (stdv(rank(X)) * stdv(rank(Y)))","7b90f87e":"np.random.seed(1)\ndata1 = np.random.rand(1000)*20#not normal distribution\ndata2 = data1 + (np.random.rand(1000)*10)\n#data2 =(np.random.rand(1000)*10) check for both data2 one by one\nsns.scatterplot(data1,data2)","55b24c1d":"# calculate spearman's correlation\ncoef, p = scipy.stats.spearmanr(data1, data2)\nprint(\"coef\",coef)\n\nalpha=0.05\nif p>alpha:\n    print(\"uncorelated(Failed to reject H0) with confidence\",int((1-p)*100))\nelse:\n    print(\"corelated(reject H0) with confidence\",int((1-p)*100))\n","58223bfe":"#Kendall\u2019s correlation\n# it calculates a normalized score for the number of matching or concordant rankings between the two samples\n\ncoef , p = scipy.stats.kendalltau(data1,data2)\nprint(\"coef\",coef)\n\nalpha=0.05\nif p>alpha:\n    print(\"uncorelated(Failed to reject H0) with confidence\",int((1-p)*100))\nelse:\n    print(\"corelated(reject H0) with confidence\",int((1-p)*100))\n","3712d535":"# p <= alpha: reject null hypothesis, different distribution\n# p > alpha: fail to reject null hypothesis, same distribution\n# To test we drwan a sample from two diffrent normal distribution","aae00486":"np.random.seed(1)\ndata1 = 20 * np.random.randn(1000)+50.1\ndata2 = 10 * np.random.randn(1000)+50\nprint('Data1 mean is {0} and std is {1}'.format(data1.mean(),data1.std()))\nprint('Data2 mean is {0} and std is {1}'.format(data2.mean(),data2.std()))","e00400d6":"#The Student\u2019s t-test is a statistical hypothesis test that two independent data samples \n#known to have a Gaussian distribution, have the same Gaussian distribution\nstat , p = scipy.stats.ttest_ind(data1,data2)\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\nalpha=0.05\nif p>alpha:\n    print(\"same distribution(Failed to reject H0) with confidence\",int((1-p)*100))\nelse:\n    print(\"diff distribution(reject H0) with confidence\",int((1-p)*100))","266e62d8":"np.random.seed(1)\ndef independent_t_test(data1,data2,alpha=0.05):\n    n1=len(data1)\n    n2=len(data2)\n    mean_data1 = np.mean(data1)\n    mean_data2 = np.mean(data2)\n    se1=np.std(data1)\/(n1**0.5)\n    se2=np.std(data2)\/(n2**0.5)\n    sed=np.sqrt((se1**2)+(se2**2))\n    t_stat = (mean_data1-mean_data2)\/sed\n\n    degree_fr = n1+n2-2\n    critical_value = scipy.stats.t.ppf(1.0-alpha,degree_fr)\n    p = (1.0-scipy.stats.t.cdf(abs(t_stat),degree_fr))*2.0\n    return t_stat,critical_value,p\ndata1 = 20 * np.random.randn(1000)+50.1\ndata2 = 10 * np.random.randn(1000)+50\nt_stat , cv , p = independent_t_test(data1,data2)\nprint('t_stat: ',t_stat,'critical value:',cv,'p value',p)\nif p>alpha:\n    print(\"same distribution(Failed to reject H0) with confidence\",int((1-p)*100))\nelse:\n    print(\"diff distribution(reject H0) with confidence\",int((1-p)*100))","cdce70a7":"#H0: Paired sample distributions are equal.\n#H1: Paired sample distributions are not equal.\nnp.random.seed(1)\ndata1 = 10 * np.random.randn(1000) + 50.055\ndata2 = 20 * np.random.randn(1000) + 50\nstat , p =scipy.stats.ttest_rel(data1,data2)\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\nalpha=0.05\nif p>alpha:\n    print(\"same distribution(Failed to reject H0) with confidence\",int((1-p)*100))\nelse:\n    print(\"diff distribution(reject H0) with confidence\",int((1-p)*100))","c752814a":"np.random.seed(1)\ndef dependent_t_test(data1,data2,alpha=0.05):\n    n=len(data1)# number of paired samples\n    mean_data1 = np.mean(data1)\n    mean_data2 = np.mean(data2)\n    d1 = sum([(data1[i]-data2[i])**2 for i in range(n)])\n    d2 = sum([data1[i]-data2[i] for i in range(n)])\n    sd=np.sqrt((d1-(d2**2)\/n)\/(n-1))\n    sed=sd\/np.sqrt(n)\n    t_stat = (mean_data1-mean_data2)\/sed\n    degree_fr = n-1\n    critical_value = scipy.stats.t.ppf(1.0-alpha,degree_fr)\n    p = (1.0-scipy.stats.t.cdf(abs(t_stat),degree_fr))*2.0\n    return t_stat,critical_value,p\ndata1 = 10 * np.random.randn(1000) + 50.055\ndata2 = 20 * np.random.randn(1000) + 50\nalpha=0.05\nt_stat,critical_value,p = dependent_t_test(data1,data2,alpha=0.05)\nprint('t_stat: ',t_stat,'critical value:',cv,'p value',p)\nalpha=0.05\nif p>alpha:\n    print(\"same distribution(Failed to reject H0) with confidence\",int((1-p)*100))\nelse:\n    print(\"diff distribution(reject H0) with confidence\",int((1-p)*100))","7e45b4bd":"data1 = 10 * np.random.randn(1000)+51#this is from diff normal distribution\ndata2 = 10 * np.random.randn(1000)+50\ndata3 = 10 * np.random.randn(1000)+50\nstat,p = scipy.stats.f_oneway(data1,data2,data3)\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\nalpha=0.05\nif p>alpha:\n    print(\"same distribution(Failed to reject H0) with confidence\",int((1-p)*100))\nelse:\n    print(\"one of the distribution differ(reject H0) with confidence\",int((1-p)*100))","911c3363":"#Mann-Whitney U Test\n#Wilcoxon Signed-Rank Test\n#Kruskal-Wallis H Test\n#Friedman Test","7fd38590":"Statistical Normality Tests\n","ec135554":"# <a id='sec1'>Hypothesis Tests<\/a>","56a26358":"# <a id='sec1.2'>Correlation Test<\/a>","1ef4ac4c":"<a id='sec1.3.5'>Analysis of Variance Test<\/a>\n* There are sometimes situations where we may have multiple independent data samples.(The population standard deviations of the groups are all equal)\n* Fail to Reject H0: All sample distributions are equal.\n* Reject H0: One or more sample distributions are not equal.","4d63cbad":"<a id='sec2.4'>Binomial Distribution<a>","c34c3f11":"<a id='sec2.1'>Uniform Distribution<a>","e03bfcc1":"<a id='sec1.3.3'>Paired Student\u2019s t-Test<\/a>\n* The \"paired\" way would be to measure the heart rate of 10 people before they drink the energy drink and then measure the heart rate of the same 10 people after drinking the energy drink. These two samples consist of the same test subjects, so you would perform a paired t-test on the means of both samples\n","dd3e01d3":"<a id='sec1.2.1'>parametric method<\/a>","9cf84900":"#PMF(discrete variable)\n![image.png](attachment:image.png)","0eea4e86":"<a id='sec2.5'>Poission Distribution<a>","5674b9d8":"<a id='sec1.2.2'>NonParametric method<\/a>","ee882db7":"<a id='sec1.4'>Nonparametric Statistical Hypothesis<\/a>\n* If the data does not have the familiar Gaussian distribution, we must resort to nonparametric version of the significance tests.These tests operate in a similar manner, but are distribution free, requiring that real valued data be first transformed into rank data before the test can be performed.","b6db22d8":"<a id='sec1.3.1'>UnPaired Student\u2019s t-Test<\/a>\n* The \"unpaired\" way would be to measure the heart rate of 10 people before drinking an energy drink and then measure the heart rate of some other group of peoplewho have drank energy drinks. These two samples consist of different test subjects, so you would perform an unpaired t-test on the means of both samples.","13fe3687":"**1) Normality test**","de8965de":"1.[Hypothesis Tests](#sec1)<br>\n  &emsp;&emsp;1.1 [Normality Test](#sec1.1)<br>\n  &emsp;&emsp;1.2 [Correlation Test](#sec1.2)<br>\n  &emsp;&emsp;&emsp;&emsp;1.2.1 [parametric method](#sec1.2.1)<br>\n  &emsp;&emsp;&emsp;&emsp;1.2.2 [NonParametric method](#sec1.2.2)<br>\n  &emsp;&emsp;1.3 [Parametric Statistical Hypothesis Tests](#sec1.3)<br>\n  &emsp;&emsp;&emsp;&emsp;1.3.1 [UnPaired Student\u2019s t-Test](#sec1.3.1)<br>\n  &emsp;&emsp;&emsp;&emsp;1.3.2 [UnPaired Student\u2019s t-Test from scratch](#sec1.3.2)<br>\n  &emsp;&emsp;&emsp;&emsp;1.3.3 [Paired Student\u2019s t-Test](#sec1.3.3)<br>\n  &emsp;&emsp;&emsp;&emsp;1.3.4 [Paired Student\u2019s t-Test from scratch](#sec1.3.4)<br>\n  &emsp;&emsp;&emsp;&emsp;1.3.5 [ANOVA](#sec1.3.5)<br>\n  &emsp;&emsp;1.4 [NonParametric Statistical Hypothesis Tests](#sec1.4)<br>\n2.[Distribution](#sec2)<br>\n  &emsp;&emsp;2.1 [Uniform](#sec2.1)<br>\n  &emsp;&emsp;2.2 [Normal](#sec2.2)<br>\n  &emsp;&emsp;2.3 [Bernouli](#sec2.3)<br>\n  &emsp;&emsp;2.4 [Binomial](#sec2.4)<br>\n  &emsp;&emsp;2.5 [Poisson](#sec2.5)<br>","2b6e090b":"# Summary Statistics","4d366374":"# Distribution","edbcf486":"<a id='sec2.3'>Bernouli Distribution<a>","325b42fe":"<a id='sec1.3.3'>Paired Student\u2019s t-Test from scratch<\/a>","612ba801":"**#A Binomial Distribution has a countable number of outcomes and is therefore discrete.**","440d85cf":"Visual Normality Checks","4723b34a":"#PDF(continous variable)\n![image.png](attachment:image.png)","ea5d6f11":"Standard Error (SE) measures how spread the distribution is from the sample mean.\nThe formula can also be defined as the standard deviation divided by the square root of the number of samples","18f3531b":"QQ plot showing the scatter plot of points in a diagonal line, closely fitting the expected diagonal pattern for a sample from a Gaussian distribution.","e6a97ee0":"<a id='sec2.2'>Normal Distribution<a>","c6377b35":"# <a id='sec1.3'>Parametric Statistical Hypothesis Tests<\/a>","746f4fb2":"<a id='sec1.3.2'>UnPaired(Independent) Student\u2019s t-Test from scratch<\/a>","fdd45db0":"Poisson random variable is typically used to model the number of times an event happened in a time interval. For example, number of users visited your website in an interval can be thought of a Poisson process. Poisson distribution is described in terms of the rate (mu) at which the events happen"}}