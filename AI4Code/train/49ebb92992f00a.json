{"cell_type":{"df1515db":"code","1e2d85b6":"code","bb4e1da6":"code","42102b05":"code","646733a0":"code","2d6ef79f":"code","fa0e5baa":"code","07e17396":"code","0c905762":"code","de191184":"code","75891afd":"code","de0b50e8":"code","54add6dd":"code","78b10de2":"code","6be4ab8e":"code","69c4737b":"code","3b19ae42":"code","eaf6d667":"code","09d85114":"code","5f6a80b1":"code","a8ffb455":"code","09257a2c":"code","17cfa78c":"code","e10295f5":"code","471da026":"code","ac4143e7":"code","982e0f61":"code","5fe596e9":"code","eeb52a41":"code","ee66d1b3":"code","eeb8476b":"code","8943460b":"code","caf3e60e":"code","b894e03b":"markdown","f8ebf394":"markdown","461e38dd":"markdown","a17d77e7":"markdown"},"source":{"df1515db":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1e2d85b6":"import pandas as pd\ndata =  pd.read_csv(\"..\/input\/pump-sensor-data\/sensor.csv\")","bb4e1da6":"data.shape","42102b05":"data.columns","646733a0":"data.describe().transpose()","2d6ef79f":"data.isnull().sum()","fa0e5baa":"data.drop(['Unnamed: 0', 'timestamp','sensor_00','sensor_15','sensor_50','sensor_51'],axis=1, inplace=True)","07e17396":"import matplotlib.pyplot as plt\nprint(data.plot(subplots =True, sharex = True, figsize = (20,50)))","0c905762":"data['machine_status'].value_counts()","de191184":"import numpy as np\nconditions = [(data['machine_status'] =='NORMAL'), (data['machine_status'] =='BROKEN'), (data['machine_status'] =='RECOVERING')]\nchoices = [1, 0, 0.5]\ndata['Operation'] = np.select(conditions, choices, default=0)","75891afd":"import matplotlib.pyplot as plt\ndata.plot(subplots =True, sharex = True, figsize = (20,50))","de0b50e8":"data.columns","54add6dd":"df0 = pd.DataFrame(data, columns=['Operation','sensor_04', 'sensor_06', 'sensor_07', 'sensor_08', 'sensor_09'])","78b10de2":"df1 = pd.DataFrame(data, columns=['Operation','sensor_01', 'sensor_04', 'sensor_10', 'sensor_14', 'sensor_19', 'sensor_25'])","6be4ab8e":"df2 = pd.DataFrame(data, columns = ['Operation','sensor_02', 'sensor_05', 'sensor_11', 'sensor_16', 'sensor_20', 'sensor_26'])","69c4737b":"df3 = pd.DataFrame(data, columns = ['Operation','sensor_03', 'sensor_06', 'sensor_12', 'sensor_17', 'sensor_21', 'sensor_28'])","3b19ae42":"df0.plot(subplots =True, sharex = True, figsize = (20,20))","eaf6d667":"df = df0\ndf.shape","09d85114":"def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n    n_vars = 1 if type(data) is list else data.shape[1]\n    dff = pd.DataFrame(data)\n    cols, names = list(), list()\n    for i in range(n_in, 0, -1):\n        cols.append(dff.shift(-i))\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n    for i in range(0, n_out):\n        cols.append(dff.shift(-i))\n        if i==0:\n            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j+1)) for j in range(n_vars)]        \n        agg = pd.concat(cols, axis=1)\n        agg.columns = names\n        if dropnan:\n            agg.dropna(inplace=True)\n        return agg","5f6a80b1":"from sklearn.preprocessing import MinMaxScaler\n\nvalues = df.values\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(values)\nreframed = series_to_supervised(scaled, 1, 1)\nr = list(range(df.shape[1]+1, 2*df.shape[1]))\nreframed.drop(reframed.columns[r], axis=1, inplace=True)\nreframed.head()\n\n# Data spliting into train and test data series.\nvalues = reframed.values\nn_train_time = 50000\ntrain = values[:n_train_time, :]\ntest = values[n_train_time:, :]\ntrain_x, train_y = train[:, :-1], train[:, -1]\ntest_x, test_y = test[:, :-1], test[:, -1]\ntrain_x = train_x.reshape((train_x.shape[0], 1, train_x.shape[1]))\ntest_x = test_x.reshape((test_x.shape[0], 1, test_x.shape[1]))","a8ffb455":"from keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.layers import Dense\nfrom sklearn.metrics import mean_squared_error,r2_score\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nmodel = Sequential()\nmodel.add(LSTM(100, input_shape=(train_x.shape[1], train_x.shape[2])))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\n\n# Network fitting\nhistory = model.fit(train_x, train_y, epochs=50, batch_size=70, validation_data=(test_x, test_y), verbose=2, shuffle=False)\n\n# Loss history plot\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()\n\nsize = df.shape[1]\n\n# Prediction test\nyhat = model.predict(test_x)\ntest_x = test_x.reshape((test_x.shape[0], size))\n\n# invert scaling for prediction\ninv_yhat = np.concatenate((yhat, test_x[:, 1-size:]), axis=1)\ninv_yhat = scaler.inverse_transform(inv_yhat)\ninv_yhat = inv_yhat[:,0]\n\n# invert scaling for actual\ntest_y = test_y.reshape((len(test_y), 1))\ninv_y = np.concatenate((test_y, test_x[:, 1-size:]), axis=1)\ninv_y = scaler.inverse_transform(inv_y)\ninv_y = inv_y[:,0]\n\n# calculate RMSE\nrmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\nprint('Test RMSE: %.3f' % rmse)","09257a2c":"e = 100 - round(sum(abs(inv_y[:]-inv_yhat[:]))\/len(inv_y[:])*100,2)\naa=[x for x in range(160000)]\nplt.figure(figsize=(25,10)) \nplt.plot(aa, inv_y[:160000], marker='.', label=\"actual\")\nplt.plot(aa, inv_yhat[:160000], 'r', label=\"prediction with precision of {} %\".format(e))\nplt.ylabel(df.columns[0], size=15)\nplt.xlabel('Time', size=15)\nplt.legend(fontsize=15)\nplt.show()","17cfa78c":"df = df2\ndf.shape","e10295f5":"df2.plot(subplots =True, sharex = True, figsize = (20,20))","471da026":"def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n    n_vars = 1 if type(data) is list else data.shape[1]\n    dff = pd.DataFrame(data)\n    cols, names = list(), list()\n    for i in range(n_in, 0, -1):\n        cols.append(dff.shift(-i))\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n    for i in range(0, n_out):\n        cols.append(dff.shift(-i))\n        if i==0:\n            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j+1)) for j in range(n_vars)]        \n        agg = pd.concat(cols, axis=1)\n        agg.columns = names\n        if dropnan:\n            agg.dropna(inplace=True)\n        return agg","ac4143e7":"from sklearn.preprocessing import MinMaxScaler\n\nvalues = df.values\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(values)\nreframed = series_to_supervised(scaled, 1, 1)\nr = list(range(df.shape[1]+1, 2*df.shape[1]))\nreframed.drop(reframed.columns[r], axis=1, inplace=True)\nreframed.head()\n\n# Data spliting into train and test data series.\nvalues = reframed.values\nn_train_time = 50000\ntrain = values[:n_train_time, :]\ntest = values[n_train_time:, :]\ntrain_x, train_y = train[:, :-1], train[:, -1]\ntest_x, test_y = test[:, :-1], test[:, -1]\ntrain_x = train_x.reshape((train_x.shape[0], 1, train_x.shape[1]))\ntest_x = test_x.reshape((test_x.shape[0], 1, test_x.shape[1]))","982e0f61":"from keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.layers import Dense\nfrom sklearn.metrics import mean_squared_error,r2_score\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nmodel = Sequential()\nmodel.add(LSTM(100, input_shape=(train_x.shape[1], train_x.shape[2])))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\n\n# Network fitting\nhistory = model.fit(train_x, train_y, epochs=50, batch_size=70, validation_data=(test_x, test_y), verbose=2, shuffle=False)\n\n# Loss history plot\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()\n\nsize = df.shape[1]\n\n# Prediction test\nyhat = model.predict(test_x)\ntest_x = test_x.reshape((test_x.shape[0], size))\n\n# invert scaling for prediction\ninv_yhat = np.concatenate((yhat, test_x[:, 1-size:]), axis=1)\ninv_yhat = scaler.inverse_transform(inv_yhat)\ninv_yhat = inv_yhat[:,0]\n\n# invert scaling for actual\ntest_y = test_y.reshape((len(test_y), 1))\ninv_y = np.concatenate((test_y, test_x[:, 1-size:]), axis=1)\ninv_y = scaler.inverse_transform(inv_y)\ninv_y = inv_y[:,0]\n\n# calculate RMSE\nrmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\nprint('Test RMSE: %.3f' % rmse)","5fe596e9":"e = 100 - round(sum(abs(inv_y[:]-inv_yhat[:]))\/len(inv_y[:])*100,2)\naa=[x for x in range(170000)]\nplt.figure(figsize=(25,10)) \nplt.plot(aa, inv_y[:170000], marker='.', label=\"actual\")\nplt.plot(aa, inv_yhat[:170000], 'r', label=\"prediction with precision of {} %\".format(e))\nplt.ylabel(df.columns[0], size=15)\nplt.xlabel('Time', size=15)\nplt.legend(fontsize=15)\nplt.show()","eeb52a41":"print(rmse)","ee66d1b3":"corr = data.corr()\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\ncorr80 = corr[abs(corr)> 0.8]\nsns.heatmap(corr80)\n\n","eeb8476b":"sns.pairplot(df)\nplt.show()","8943460b":"def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) \/ nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()","caf3e60e":"plotPerColumnDistribution(df, 10, 5)","b894e03b":"Set 3: sensors 3, 6, 12, 17, 21, 28, 40","f8ebf394":"Set 2: sensors 2, 5, 11, 16, 20, 26, 39","461e38dd":"Set 1: sensors 1, 4, 10, 14, 19, 25, 34, 38","a17d77e7":"Set 0: sensors 4, 6, 7, 8, 9"}}