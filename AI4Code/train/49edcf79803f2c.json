{"cell_type":{"4f095d8e":"code","8b9e2df9":"code","5dea4b19":"code","a51e99f5":"code","9e761b2a":"code","c5bacd4f":"code","6fa4a273":"code","bfda33b3":"code","cfef65ab":"code","28bff874":"code","88dea2fd":"code","1ef04b6e":"markdown","6099209c":"markdown","ba618c5d":"markdown","e05b5014":"markdown","556208f0":"markdown","3dbbef1d":"markdown","96f1f8ff":"markdown","9c909bdc":"markdown","c5a73320":"markdown","d796fabe":"markdown","36eb9763":"markdown"},"source":{"4f095d8e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n\n#print(os.listdir(\"la_jobs\/CityofLA\/Job Bulletins\/\"))\n\n# Any results you write to the current directory are saved as output.","8b9e2df9":"jobs_path=\"..\/input\/cityofla\/CityofLA\/Job Bulletins\/\"\nadds_path=\"..\/input\/cityofla\/CityofLA\/Additional data\/\"\njobs=os.listdir(jobs_path)\nadds=os.listdir(adds_path)\nprint(len(jobs))","5dea4b19":"from nltk.stem import WordNetLemmatizer \n\n# Usefull throughout the kernel \ndef get_name_by_index(index):\n    name = jobs[index]\n    return name\n\ndef transform_line(l, lemmatize=False): # process each line \n    lemmatizer = WordNetLemmatizer()\n    l = l.rstrip()\n    l = l.strip()\n    l = l.replace(':','')\n    if lemmatize:\n        l = lemmatizer.lemmatize(l)\n    return l\n\ndef remove_fields(l): # remove elements in the list that are not going to be used as fields\n    l=[word for word in l if any(i.isdigit() for i in word)==False] \n    l=list(set(l))\n    return l","a51e99f5":"fields=[] # list of fields that divide the document\n\nfor j in jobs:\n    file = open(jobs_path+j,'r',encoding = \"ISO-8859-1\")\n    counter = 0  # The counter is used to avoid adding the job name which is uppercase also, to the list of fields \n    for line in file.readlines(): \n        if line not in ['\\n', '\\r\\n'] and line.isupper(): # if upper then its a field title\n            if counter!=0:\n                fields.append(transform_line(line))  \n            counter+=1\nfields=[word for word in fields if any(i.isdigit() for i in word)==False] \nfields=list(set(fields)) # To remove duplicates\nlen(fields)","9e761b2a":"fields_dict={}\n\nimport operator\n\nfor i in jobs:\n    file = open(jobs_path+i,'r',encoding = \"ISO-8859-1\")\n    for line in file.readlines(): \n        if line not in ['\\n', '\\r\\n'] and line.isupper(): # if upper then its a field title                \n            line=transform_line(line)\n            if line in fields and line in fields_dict.keys():\n                value=fields_dict.get(line)\n                value+=1\n                fields_dict.update({line:value})\n            if line in fields and line not in fields_dict.keys():\n                fields_dict[line]=1\n\nfields_dict = dict(sorted(fields_dict.items(), key=operator.itemgetter(1), reverse=True)) #order the dictionary \nfields_dict","c5bacd4f":"job_names=[]\n\nfor j in jobs:\n    file = open(jobs_path+j,'r',encoding = \"ISO-8859-1\")\n    counter = 0  \n    l = [transform_line(s) for s in file.readlines() if s not in ['','\\n', '\\r\\n']] \n    job_names.append(l[0])\n    file.seek(0) # reset file\n    \ndata = pd.DataFrame({'job name':job_names})\ndata.head()","6fa4a273":"# How to extract the values here?\n# Probably some documents are going to be discarded\n\nimport re\n\ndef field_extracter(field): # for extracting highly populated fields\n    l=[]\n    print(\"Extraction of {}\\n\".format(field))\n    for j,i in enumerate(jobs):\n        file = open(jobs_path+i,'r',encoding = \"ISO-8859-1\")\n        try:\n            split1=file.read().split(field)\n            upper_words=re.findall(r\"([A-Z]+\\s?[A-Z]+[^a-z0-9\\W])\",split1[1]) # Get all caps words\n            split2=split1[1].split(upper_words[0]) # THIS APPROACH MIGHT CREATE SOME PROBLEMS WHEN HAVING ORG NAMES (EX: 1st\n                                                   # doc ITA is used for split, so we loose some info.)\n            l.append(transform_line(split2[0]))\n        except:\n            print(\"Could not split doc: {}\".format(j)) # print documents that was not possible to extract field\n            l.append('')\n    print('')\n    return l\n\nsalary_text=field_extracter('ANNUAL SALARY')\nselection_process=field_extracter('SELECTION PROCESS')\nwhere_to_apply=field_extracter('WHERE TO APPLY')\nduties=field_extracter('DUTIES')","bfda33b3":"data['salary']=salary_text\ndata['selection_process']=selection_process\ndata['where_to_apply']=where_to_apply\ndata['duties']=duties\ndata.head()","cfef65ab":"requirements=[]\nfor j in jobs:\n    file = open(jobs_path+j,'r',encoding = \"ISO-8859-1\")\n    file_text = file.read()\n    file.seek(0)\n    for line in file.readlines(): \n        if line not in ['\\n', '\\r\\n'] and line.isupper() and \"REQUIRE\" in line: # if upper then its a field title  \n            try:\n                split1=file_text.split(line)\n                upper_words=re.findall(r\"([A-Z]+\\s?[A-Z]+[^a-z0-9\\W])\",split1[1]) #THE BEST WAY TO FIND ALL THE ALL CAPSS WORDS!!!!!!!!!\n                split2=split1[1].split(upper_words[0])\n                requirements.append(transform_line(split2[0]))\n                break\n            except:\n                requirements.append('')\n                break\n\ndata['requirements']=requirements\ndata.head()","28bff874":"max_salary=[]\nmin_salary=[]\nall_salaries=[]\nfor i,text in enumerate(data['salary']):\n    salaries=[]\n    text_list=[s.replace(\",\",\".\") for s in text.split()]\n    for word in text_list:\n        counter=0\n        for char in word:\n            if not char.isdigit(): #after seeing a . no more \n                if counter==1 and not char.isdigit():\n                    salary=word.replace(char,\"\")\n                if char==\".\":\n                    counter=1\n                else:\n                    salary=word.replace(char,\"\")\n        if salary.replace(\".\",\"\").isdigit():\n            salaries.append(float(salary))\n    if not salaries: salaries=[0]\n    max_salary.append(max(salaries)) \n    min_salary.append(min(salaries))\n    all_salaries.append(salaries)","88dea2fd":"data['max_salary']=max_salary\ndata['min_salary']=min_salary\ndata['all_salaries']=all_salaries\ndata.head()","1ef04b6e":"There are some fields that can be extracted","6099209c":"Lets create 2 variables for the path of each file type and check how many jobs bulettins are available in the folder","ba618c5d":"The Annual Salary column can also be further decomposed into maximum salary, minimum salary and all salaries involved in this field ","e05b5014":"Lets create auxiliary functions to \n- Help us to get the name of the document by index - this is going to be useful during the kernell to check the inspect specific documents\n- Transform lines when reading the document - useful to clean file lines and cleaning each document field identifier","556208f0":"We found 123 different fields in all the documents. Now lets count which are the most frequent ones ","3dbbef1d":"## Data extraction\n\nIn this section we are going to start to extract information to populate the dataset. To create the structure of it, we are going to follow the above dictionary that counts how many times each encountred field is available in the provided documents. \n\nFirst lets extract each job name","96f1f8ff":"There are some fields like requirements that require a special treatment, since that are many different fields that relate to it. Looking into the fields_dictionary we have:\n- REQUIREMENTS\/MINIMUM QUALIFICATIONS\n- REQUIREMENT\/MINIMUM QUALIFICATION\n- REQUIREMENTS\n- REQUIREMENT\n- REQUIREMENTS\/ MINIMUM QUALIFICATIONS\n- .... and more","9c909bdc":"# Turning Los Angeles into Job Angels\n\n\nThe intent of this notebook is to explore the opportunities available in the city of LA.\n\n<img src=\"https:\/\/buzzsouthafrica.com\/wp-content\/uploads\/Find_Jobs.jpg\" width=\"400px\" >\n\n## Problem Statement\n\nThe goal is to convert a folder full of plain-text job postings into a single structured CSV file and then to use this data to:\n\n- identify language that can negatively bias the pool of applicants\n- improve the diversity and quality of the applicant pool\n- make it easier to determine which promotions are available to employees in each job class\n\nThe first step is to create a dataset that integrates all the information available on the txt job offers.","c5a73320":"Lets add the new columns to the dataframe","d796fabe":"Lets add the created lists to the dataframe","36eb9763":"## Job bulettins field inspection\n\nTo create the structured CSV we have to find some sort of structure in the given job bulettins. Since we are talking about job offers, there are always some fields that are similar in each file (job duties, qualifications, etc..). The structure is not always the same but many fields should be repeated, with same\/similar name.\n\nThe first step then is to count how many times fields appear in all the documents. Also fields generally appear with uppercase in every document, so we are going to follow that pattern in the field extraction.  "}}