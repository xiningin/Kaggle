{"cell_type":{"3d04bf60":"code","19b15c41":"code","7cfa9dbc":"code","89040a3a":"code","c5ba331c":"code","7e99ebe4":"code","24511abd":"code","a2a8241b":"code","f39f87d1":"code","e3569963":"code","488084a8":"code","0f47e82c":"code","a02622c3":"code","31861f5f":"code","515c0751":"code","b0449257":"code","0ffe1511":"code","676a117d":"code","9a3c3bbb":"code","fe30ecbb":"code","6141f023":"code","48cb111b":"code","362590bf":"code","934dd4a3":"code","eac17432":"code","b48d5922":"code","c1ad45a5":"code","eae05109":"code","634f6397":"code","0cc0c0aa":"code","ffe2daa3":"code","9f78f085":"code","e21dd3b8":"code","0245fc15":"code","f00692b8":"code","b427d5b9":"code","0a4fb230":"code","61306b4f":"code","84238b15":"code","b451e723":"code","8584a9fe":"code","6562fb79":"code","dc5d7e18":"code","258f0295":"code","824e05a1":"code","5239976e":"code","f86b2b23":"code","68030486":"code","bc5964f9":"code","141516a1":"code","70d7ff09":"code","d8d795ec":"code","208027db":"code","0af1bbc2":"code","a6b06408":"code","4ed1ecf7":"code","cf1b78a1":"code","b1a7d53a":"code","c38e4857":"code","6552395a":"code","861eed12":"code","a2b5376b":"code","00555051":"code","208a8e93":"code","5dbacb27":"code","8cd87d4d":"code","7bfe1767":"code","67d59d5b":"code","ffc45e37":"code","b6d1f730":"code","288eaeba":"code","c4831497":"code","be5e89ad":"code","6a390fd7":"code","46601a1c":"code","1f10fd73":"code","c5e91d00":"code","f80b778f":"code","02f545ec":"code","f623dd06":"code","62db0eb9":"markdown","855f82dd":"markdown","e72d1b4a":"markdown","68902d19":"markdown","84edcecd":"markdown","31c1dc12":"markdown","ad2d2fc1":"markdown","b0b5ac02":"markdown","a636297c":"markdown","32d760bc":"markdown","8f8e86da":"markdown","8dbf6c3d":"markdown","ae1c1cb6":"markdown","accbe062":"markdown","77c7b9a3":"markdown","cd720b84":"markdown","ba71f99c":"markdown","8e816a1f":"markdown","230bfc32":"markdown","f0699013":"markdown","29ee1b3e":"markdown","77f0ee8a":"markdown","3be0a668":"markdown","0f8244ff":"markdown","c66eba2f":"markdown","0ac70e8a":"markdown","7f78ddae":"markdown","44d060d2":"markdown","817d2eae":"markdown","45466962":"markdown","4e3bef18":"markdown","81e7b0c3":"markdown","9dbdff3d":"markdown","2f69748c":"markdown","16af459a":"markdown","1eac4ab6":"markdown","0310d0b6":"markdown"},"source":{"3d04bf60":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.feature_selection import VarianceThreshold\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","19b15c41":"data = pd.read_csv(\"\/kaggle\/input\/employee-absenteeism-prediction\/Absenteeism-data.csv\")\ndata.head()","7cfa9dbc":"data.describe()","89040a3a":"data.isnull().sum()","c5ba331c":"# if you want to see all columns and raws:\n# pd.options.display.max_columns = None\n# pd.options.display.max_rows = None\n# display(data)","7e99ebe4":"data.info()","24511abd":"data.shape","a2a8241b":"# here for simplicity I will use only numerical variables\n# select numerical columns:\n\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerical_vars = list(data.select_dtypes(include=numerics).columns)\ndata_numerical = data[numerical_vars]\ndata_numerical.shape","f39f87d1":"# remove constant features\nconstant_features = [\n    feat for feat in data_numerical.columns if data_numerical[feat].std() == 0\n]\n\nlen(constant_features)","e3569963":"# remove quasi-constant features\nsel = VarianceThreshold(threshold=0.01) # 0.1 indicates 99% of observations approximately\n\nsel.fit(data_numerical)  # fit finds the features with low variance\n\nsum(sel.get_support()) # how many not quasi-constant?","488084a8":"data.duplicated().sum()","0f47e82c":"data[data.duplicated()]","a02622c3":"data = data.drop_duplicates(keep=\"first\").reset_index()","31861f5f":"data.shape","515c0751":"data.head()","b0449257":"def correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr","0ffe1511":"corr_features = correlation(data, 0.8)\nlen(set(corr_features))","676a117d":"data.drop([\"ID\",\"index\"],axis=1,inplace=True)\ndata.head()","9a3c3bbb":"# Reason for Absence\ndata[\"Reason for Absence\"].unique()","fe30ecbb":"reason_columns = pd.get_dummies(data[\"Reason for Absence\"],drop_first=True)\nreason_columns.head()","6141f023":"# Make Groups For Reasons\nreason_type_1 = reason_columns.loc[:,:14].max(axis=1)\nreason_type_2 = reason_columns.loc[:,15:17].max(axis=1)\nreason_type_3 = reason_columns.loc[:,18:21].max(axis=1)\nreason_type_4 = reason_columns.loc[:,22:].max(axis=1)","48cb111b":"# concat:\ndf = pd.concat([data.drop(\"Reason for Absence\",axis=1), reason_type_1, reason_type_2, reason_type_3, reason_type_4],axis=1)\ndf.head()","362590bf":"df.columns.values","934dd4a3":"columns_names = ['Date', 'Transportation Expense', 'Distance to Work', 'Age',\n       'Daily Work Load Average', 'Body Mass Index', 'Education',\n       'Children', 'Pets', 'Absenteeism Time in Hours', \"Reason_1\", \"Reason_2\", \"Reason_3\", \"Reason_4\"]\ndf.columns = columns_names\ndf.head()","eac17432":"df.columns.values","b48d5922":"# reorder columns\ncolumn_names_reordered = ['Reason_1','Reason_2', 'Reason_3', 'Reason_4', 'Date', 'Transportation Expense', 'Distance to Work', 'Age',\n                           'Daily Work Load Average', 'Body Mass Index', 'Education',\n                           'Children', 'Pets', 'Absenteeism Time in Hours']\ndf = df[column_names_reordered]\ndf.head()","c1ad45a5":"# create a chechpoint: to reduce the risk of losing data\ndf_reason_mod = df.copy()","eae05109":"# date:\n# df_reason_mod[\"Date\"].apply(lambda x: x.split(\"\/\"))\ndf_reason_mod[\"Date\"] = pd.to_datetime(df_reason_mod[\"Date\"], format=\"%d\/%m\/%Y\")\ndf_reason_mod[\"Date\"][0:5]","634f6397":"# extract month\nlist_months = []\nfor i in range(len(df_reason_mod[\"Date\"])):\n    list_months.append(df_reason_mod[\"Date\"][i].month)","0cc0c0aa":"df_reason_mod[\"Month Value\"] = list_months\ndf_reason_mod.head()","ffe2daa3":"# extract the day of the week:0,1,2,3,4,5,6\ndf_reason_mod[\"Date\"][0].weekday()","9f78f085":"def day_to_weekday(date_value):\n    return date_value.weekday()","e21dd3b8":"df_reason_mod[\"Day of the Week\"] = df_reason_mod[\"Date\"].apply(day_to_weekday)\ndf_reason_mod.head()","0245fc15":"# Education:\ndf_reason_mod[\"Education\"].unique()","f00692b8":"df_reason_mod[\"Education\"].value_counts()","b427d5b9":"# 1 => 0\n# 2,3,4 => 1\ndf_reason_mod[\"Education\"] = df_reason_mod[\"Education\"].map({1:0,2:1,3:1,4:1})","0a4fb230":"df_reason_mod[\"Education\"].unique()","61306b4f":"df_reason_mod[\"Education\"].value_counts()","84238b15":"# create a checkpoint\ndf_model = df_reason_mod.copy()","b451e723":"# use median cut-off hours and making targets \ndf_model[\"Absenteeism Time in Hours\"].median()","8584a9fe":"targets = np.where(df_model[\"Absenteeism Time in Hours\"] > 3, 1, 0)\ntargets[0:10]","6562fb79":"# add to df\ndf_model[\"Excessive Absenteeism\"] = targets\ndf_model.head()","dc5d7e18":"targets = pd.Series(targets)\ntargets.value_counts()","258f0295":"# drop Absenteeism Time in Hours\ndf_model.drop([\"Absenteeism Time in Hours\",\"Date\"],axis=1,inplace=True)","824e05a1":"df_model is df_reason_mod","5239976e":"unscaled_inputs = df_model.iloc[:,:-1]","f86b2b23":"# this class is just for selecting features to standardization:\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler\n\nclass CustomScaler(BaseEstimator,TransformerMixin): \n    \n    def __init__(self,columns,copy=True,with_mean=True,with_std=True):\n        self.scaler = StandardScaler(copy,with_mean,with_std)\n        self.columns = columns\n        self.mean_ = None\n        self.var_ = None\n\n    def fit(self, X, y=None):\n        self.scaler.fit(X[self.columns], y)\n        self.mean_ = np.mean(X[self.columns])\n        self.var_ = np.var(X[self.columns])\n        return self\n\n    def transform(self, X, y=None, copy=None):\n        init_col_order = X.columns\n        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]), columns=self.columns)\n        X_not_scaled = X.loc[:,~X.columns.isin(self.columns)]\n        return pd.concat([X_not_scaled, X_scaled], axis=1)[init_col_order]\n","68030486":"unscaled_inputs.columns.values","bc5964f9":"columns_to_scale = ['Month Value','Day of the Week', 'Transportation Expense', 'Distance to Work',\n                    'Age', 'Daily Work Load Average', 'Body Mass Index', 'Children', 'Pets']\n\n# Because these features are dummy variable\n# columns_to_omit = ['Reason_1', 'Reason_2', 'Reason_3', 'Reason_4','Education'] ","141516a1":"sc = CustomScaler(columns_to_scale)","70d7ff09":"scaled_inputs = sc.fit_transform(unscaled_inputs)\nscaled_inputs.head()","d8d795ec":"from sklearn.model_selection import train_test_split","208027db":"x_train, x_test, y_train, y_test = train_test_split(scaled_inputs,targets,train_size=0.8,shuffle=True,random_state=7)","0af1bbc2":"x_train.shape, y_train.shape","a6b06408":"x_test.shape, y_test.shape","4ed1ecf7":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics","cf1b78a1":"reg = LogisticRegression()","b1a7d53a":"reg.fit(x_train,y_train)","c38e4857":"reg.score(x_train,y_train)","6552395a":"model_outputs = reg.predict(x_train)","861eed12":"model_outputs == y_train","a2b5376b":"np.sum(model_outputs == y_train)","00555051":"model_outputs.shape","208a8e93":"np.sum(model_outputs == y_train) \/ model_outputs.shape[0]","5dbacb27":"np.sum(model_outputs == y_train) \/ model_outputs.shape[0] == reg.score(x_train,y_train)","8cd87d4d":"reg.intercept_","7bfe1767":"reg.coef_.T","67d59d5b":"# Make df to show better:\nfeature_name = unscaled_inputs.columns\nsummary_table = pd.DataFrame(data=feature_name,columns=[\"Feature Name\"])\nsummary_table[\"Coefficient\"] = reg.coef_.T\nsummary_table","ffc45e37":"# add intercept:\nsummary_table.index = summary_table.index + 1\nsummary_table.loc[0] = [\"Intercept\", reg.intercept_[0]]\nsummary_table = summary_table.sort_index()\nsummary_table","b6d1f730":"summary_table[\"Odds_ratio\"] = np.exp(summary_table.Coefficient)\nsummary_table","288eaeba":"summary_table.sort_values(\"Odds_ratio\", ascending=False)","c4831497":"reg.score(x_test, y_test)","be5e89ad":"# predict_proba(x) : returns the probability estimates for all possible outputs\npredict_proba = reg.predict_proba(x_test)\npredict_proba","6a390fd7":"# see sum is 1\n0.72033435 + 0.27966565, 0.87854892 + 0.12145108","46601a1c":"# 1. column: probality of being 0\n# 2. column: probality of being 1\npredict_proba.shape","1f10fd73":"predict_proba[:,1]","c5e91d00":"import pickle","f80b778f":"# model: file name \/ wb: write bites \/ dump:save\nwith open(\"model\", \"wb\") as file:\n    pickle.dump(reg,file)","02f545ec":"with open(\"model_2\", \"wb\") as file_2:\n    pickle.dump(sc,file_2)","f623dd06":"from IPython.display import FileLink, FileLinks\nFileLinks('.') #lists all downloadable files on server","62db0eb9":"<a id = \"13\"><\/a><br>\n## Standardize The Data\n* Omit Dummy Features","855f82dd":"### Test The Model\n* Often the test accuracy is 10-20% lower than the train accuracy (due to overfitting) ","e72d1b4a":"<a id = \"6\"><\/a><br>\n## Remove Quasi-Constant Features\n* There is no Quasi-Constant Feature","68902d19":"<a id = \"7\"><\/a><br>\n## Remove Duplicate Features","84edcecd":"* Whichever weights\/coefficient is bigger, its correspending feature is more important.\n\n<hr> ","31c1dc12":"### A feature is not particularly important:\n* if its coeff is around 0\n* if its odds ratio is around 1\n\n#### So you can consider to drop these features: Education, Month Value, Distance to Work\n<hr>","ad2d2fc1":"* if the probality is:\n  * below 0.5, it places a 0\n  * above 0.5, it places a 1","b0b5ac02":"<font color = 'red'>\n<h1>Analysis of Absenteeism<h1>\n    \n<hr>","a636297c":"<a id = \"17\"><\/a><br>\n## Save The Model","32d760bc":"<a id = \"3\"><\/a><br>\n## Basic Data Analysis","8f8e86da":"* log(odds) = intercept + b1x1 + b2x2 + b3x3 + .... + b14x14","8dbf6c3d":"<a id = \"5\"><\/a><br>\n## Remove Constant Features","ae1c1cb6":"### Moderately Absent <= 3\n### Excessively Absent > 4","accbe062":"* 0: 'Unknown',\n* 1: 'Certain infectious and parasitic diseases',\n* 2: 'Neoplasms',\n* 3: 'Diseases of the blood and blood-forming organs and certain disorders involving the immune mechanism',\n* 4: 'Endocrine, nutritional and metabolic diseases',\n* 5: 'Mental and behavioural disorders',\n* 6: 'Diseases of the nervous system',\n* 7: 'Diseases of the eye and adnexa',\n* 8: 'Diseases of the ear and mastoid process',\n* 9: 'Diseases of the circulatory system',\n* 10: 'Diseases of the respiratory system',\n* 11: 'Diseases of the digestive system',\n* 12: 'Diseases of the skin and subcutaneous tissue',\n* 13: 'Diseases of the musculoskeletal system and connective tissue',\n* 14: 'Diseases of the genitourinary system',\n* 15: 'Pregnancy, childbirth and the puerperium',\n* 16: 'Certain conditions originating in the perinatal period',\n* 17: 'Congenital malformations, deformations and chromosomal abnormalities',\n* 18: 'Symptoms, signs and abnormal clinical and laboratory findings, not elsewhere classified',\n* 19: 'Injury, poisoning and certain other consequences of external causes',\n* 20: 'External causes of morbidity and mortality',\n* 21: 'Factors influencing health status and contact with health services',\n* 22: 'Patient follow-up',\n* 23: 'Medical consultation',\n* 24: 'Blood donation',\n* 25: 'Laboratory examination',\n* 26: 'Unjustified absence',\n* 27: 'Physiotherapy',\n* 28: 'Dental consultation'","77c7b9a3":"<a id = \"12\"><\/a><br>\n## Logistic Regression","cd720b84":"### Create Targets","ba71f99c":"### Save The Scale","8e816a1f":"<a id = \"16\"><\/a><br>\n## Finding The Intercept & Coefficients","230bfc32":"<a id = \"4\"><\/a><br>\n## Feature Engineering","f0699013":"## Education\n* 1:High School\n* 2:Graduate\n* 3:Postgraduate\n* 4:Master or Doctor","29ee1b3e":"<a id = \"10\"><\/a><br>\n## Observe Reason for Absence","77f0ee8a":"### Select Inputs","3be0a668":"<a id = \"14\"><\/a><br>\n## Train-Test Split of Data","0f8244ff":"<a id = \"8\"><\/a><br>\n## Remove Correlation Feature\n* Drop features if they have 80% correlation ","c66eba2f":"### Manually Check The Accuracy","0ac70e8a":"<a id = \"11\"><\/a><br>\n## Feature Engineering Part-2","7f78ddae":"* Reason_1 : various diseases\n* Reason_2 : pragnancy and giving birth\n* Reason_3 : poising\n* Reason_4 : light diseases","44d060d2":"<a id = \"2\"><\/a><br>\n## Load Dataset","817d2eae":"### Training The Model","45466962":"<a id = \"1\"><\/a><br>\n## Load Libraries","4e3bef18":"* One Hot Encoding","81e7b0c3":"### Interpreting The Logistic Regression Coeff.","9dbdff3d":"### See The Files","2f69748c":"<a id = \"15\"><\/a><br>\n## Model","16af459a":"# Introduction\n* Feature Engineering Technics\n\n<font color = 'red'>\n* <h4>If you like, Please don't forget to UPVOTE <h4>\n\n<br>\n<br>\n<font color = 'blue'>\n<b>Content: <\/b>\n\n1. [Load Libraries](#1)\n1. [Load Dataset](#2)\n1. [Basic Data Analysis](#3)\n1. [Feature Engineering Part-1](#4)\n    * [Remove Constant Features](#5)\n    * [Remove Quasi-Constant Features](#6)\n    * [Remove Duplicate Features](#7)\n    * [Remove Correlation Feature](#8)\n    * [Remove Unnecessary Features](#9)   \n1. [Observe Reason for Absence](#10)\n1. [Feature Engineering Part-2](#11)\n1. [Logistic Regression](#12)\n    * [Standardize The Data](#13)\n    * [Train-Test Split of Data](#14)\n    * [Model](#15)\n    * [Finding The Intercept & Coefficients](#16)\n    * [Save The Model](#17)  \n\n\n    \n<hr>\n   ","1eac4ab6":"<a id = \"9\"><\/a><br>\n## Remove Unnecessary Features","0310d0b6":"### Interpreting The Important Features"}}