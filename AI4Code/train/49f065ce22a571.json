{"cell_type":{"a25ff2b3":"code","913ebe57":"code","8cfe2ec1":"code","9d9ca1fe":"code","f92d9a4b":"code","866e9401":"code","a19132f6":"code","18164479":"code","644823f0":"code","63ecba68":"code","20885cfe":"code","55459eb0":"code","e7d20664":"code","8f393635":"code","41009814":"code","0fce4cb3":"code","86a82dac":"code","030b7fe8":"code","6b0a71f0":"code","673443ab":"code","9ed4694b":"code","6fffe739":"code","c6c0a99f":"code","ca2a92cd":"code","2746520e":"code","94727f70":"code","451ddd64":"code","cb0038f6":"code","b8c2b9e4":"code","02e457e8":"code","9f04faef":"code","3d89d408":"code","ed80bf02":"code","1f5067e9":"code","d66d2278":"code","eeaa167c":"code","7490fd79":"code","f957225b":"code","f5a63a46":"code","9dccaac4":"code","72f06fbc":"code","76b4253c":"code","66db0505":"code","d7e79a2a":"markdown","56ab15d7":"markdown","2c744f8a":"markdown","a730c9ad":"markdown","4944d1f2":"markdown","9ef62b3d":"markdown","a9f94d26":"markdown","b110cddb":"markdown","f7af3cba":"markdown","5d97869c":"markdown","89a01d73":"markdown"},"source":{"a25ff2b3":"import numpy as np \nimport pandas as pd \n\nimport itertools\nimport os\nimport re\n\nfrom collections import Counter\nfrom dask import delayed, compute\nfrom dask.diagnostics import ProgressBar\nfrom fuzzywuzzy import fuzz, process\nfrom IPython.core.display import display\nfrom itertools import cycle, islice\nfrom sklearn.datasets import fetch_20newsgroups\n\nProgressBar().register()\n\nchunk_size = 300\npd.options.display.max_columns = chunk_size\npd.options.display.max_rows = chunk_size","913ebe57":"train_p = fetch_20newsgroups(subset='train')\ntest_p = fetch_20newsgroups(subset='test')","8cfe2ec1":"df_p = pd.concat([pd.DataFrame(data = np.c_[train_p['data'], train_p['target']],\n                                   columns= ['text','target']),\n                      pd.DataFrame(data = np.c_[test_p['data'], test_p['target']],\n                                   columns= ['text','target'])],\n                     axis=0).reset_index(drop=True)","9d9ca1fe":"df_p['target'] = df_p['target'].astype(np.int8)","f92d9a4b":"df_p['text'] = df_p['text'].map(lambda x: x.replace('\\r\\n','\\n').replace('\\r','\\n').replace('\\n','\\n '))\ndf_p.loc[df_p['text'].str.endswith('\\n '),'text'] = df_p.loc[df_p['text'].str.endswith('\\n '),'text'].map(lambda x: x[:-1])","866e9401":"p_text_chunk_list = []\np_text_index_list = []\n\nfor p_index, p_row in df_p.iterrows():\n    p_text = p_row['text']\n    p_text_len = len(p_text)\n    if p_text_len > chunk_size:\n        for j in range(p_text_len \/\/ chunk_size):\n            p_text_chunk_list.append(p_text[chunk_size*j:chunk_size*(j+1)])\n            p_text_index_list.append(p_index)\n        if p_text_len%chunk_size > 0:\n            p_text_chunk_list.append(p_text[chunk_size*(p_text_len \/\/ chunk_size):(chunk_size*(p_text_len \/\/ chunk_size)+p_text_len%chunk_size)])\n            p_text_index_list.append(p_index)\n    else:\n        p_text_chunk_list.append(p_text)\n        p_text_index_list.append(p_index)","a19132f6":"df_p_chunked = pd.DataFrame({'text' : p_text_chunk_list, 'p_index' : p_text_index_list})\ndf_p_chunked = pd.merge(df_p_chunked, df_p.reset_index().rename(columns={'index' : 'p_index'})[['p_index','target']],on='p_index',how='left')\n\ndf_p_chunked_list = []\nfor i in np.sort(df_p_chunked['target'].unique()):\n    df_p_chunked_list.append(df_p_chunked[df_p_chunked['target'] == i])","18164479":"competition_path = '..\/input\/20-newsgroups-ciphertext-challenge\/'","644823f0":"train = pd.read_csv(competition_path + 'train.csv').rename(columns={'ciphertext' : 'text'})\ntest = pd.read_csv(competition_path + 'test.csv').rename(columns={'ciphertext' : 'text'})","63ecba68":"cipher_path = '..\/input\/cipher-1-cipher-2-full-solutions\/'\n\ncipher2_map = pd.read_csv(cipher_path + '\/cipher2_map.csv')\n\ncipher2_map = pd.concat([cipher2_map,pd.DataFrame(data=[['D','\\x10']],columns=['cipher','plain'])],axis=0,ignore_index=True)\n#Cheating a bit with this update, the cipher character D has to be added to the cipher_2 map because a cipher #4 text, it would have appeared as missing in our decryption matching at the end of this kernel\n\ntranslation_2_pt = str.maketrans(''.join(cipher2_map['plain']),''.join(cipher2_map['cipher'])) # cipher #2 encryption\ntranslation_2_ct = str.maketrans(''.join(cipher2_map['cipher']), ''.join(cipher2_map['plain'])) # cipher #2 decryption\n\ndef shift_char(c,shift):\n    if c.islower():\n        return(chr((ord(c) - ord('a') + shift) % 26 + ord('a')))\n    else:\n        return(chr((ord(c) - ord('A') + shift) % 26 + ord('A')))\n\ndef replace_alpha(l,l_alpha_new):\n    res = []\n    i_alpha = 0\n    for i in range(len(l)):\n        if l[i].isalpha():\n            res.append(l_alpha_new[i_alpha])\n            i_alpha += 1\n        else:\n            res.append(l[i])\n    return(res)\n\ndef fractional_vigenere(s,key):\n    l = list(s)\n    l_alpha = [x for x in l if x.isalpha()]\n    l_alpha_shifted = [shift_char(c,-shift) for c, shift in zip(l_alpha,list(islice(cycle(key), len(l_alpha))))]\n    return(''.join(replace_alpha(l,l_alpha_shifted)))\n\nkey_ord_3 = [7, 4, 11, 4, 13, -1, 5, 14, 20, 2, 7, 4, -1, 6, 0, 8, 13, 4, 18] # cipher #3 decryption key\nkey_ord_3_n = [-x for x in key_ord_3] # cipher #3 encryption key","20885cfe":"p_counts = pd.Series(Counter(''.join(df_p['text']))).rename(\"counts\").to_frame().sort_values(\"counts\", ascending = False)\np_counts = 1000000 * p_counts \/ p_counts.sum()\np_counts = p_counts.reset_index().rename(columns = {\"index\":\"p_char\"})\n\nc_counts = []\nfor i in range(1,5):\n    counts = pd.Series(Counter(''.join(pd.concat([train[train['difficulty'] == i][['text']],test[test['difficulty'] == i][['text']]],axis=0)['text']))).rename('counts').to_frame().sort_values('counts', ascending = False)\n    counts = 1000000 * counts \/ counts.sum()\n    counts = counts.reset_index().rename(columns = {'index':'c_{}_char'.format(i)})\n    c_counts.append(counts)","55459eb0":"pd.concat([p_counts] + c_counts, axis = 1).head(20)","e7d20664":"df_p_chunked['text_len'] = df_p_chunked['text'].map(len) \ndf_p_chunked['pt_text'] = df_p_chunked['text'].map(lambda x: fractional_vigenere(x.translate(translation_2_pt),key_ord_3_n))\ndf_p_chunked['pt_counter'] = df_p_chunked['pt_text'].map(lambda x: Counter(x))\ndf_p_chunked = df_p_chunked.reset_index().rename(columns={'index' : 'p_chunk_index'})","8f393635":"difficulty_level = 4\n\ntrain = train[train['difficulty'] == difficulty_level]\ntrain['text_len'] = train['text'].map(len) \n\ntest = test[test['difficulty'] == difficulty_level]","41009814":"res = []\nfor i in train.index[:]:\n    c_text = train.loc[i]['text']\n    c_target = train.loc[i]['target']\n    c_len = train.loc[i]['text_len']\n    c_counter = Counter(c_text)\n    p_chunk_indexes = list(df_p_chunked[(df_p_chunked['text_len'] == c_len) & (df_p_chunked['target'] == c_target) & (df_p_chunked['pt_counter'] == c_counter)]['p_chunk_index'].values)\n    if len(p_chunk_indexes) == 1:\n        res.append((i,p_chunk_indexes[0]))","0fce4cb3":"df_crib2 = pd.DataFrame(res,columns=['train_index','p_chunk_index'])\ndf_crib2['c_text'] = df_crib2['train_index'].map(lambda x: train.loc[x,'text'])\ndf_crib2['pt_text'] = df_crib2['p_chunk_index'].map(lambda x: df_p_chunked.loc[x,'pt_text'])","86a82dac":"df_crib2['text_len'] = df_crib2['c_text'].map(len)","030b7fe8":"def trans_mapper(x):\n    pt_text = x['pt_text']\n    c_text = x['c_text']\n    \n    pt_series = pd.Series(Counter(pt_text)).rename('pt_counts').to_frame().sort_values('pt_counts', ascending = False)\n    c_series = pd.Series(Counter(c_text)).rename('c_counts').to_frame().sort_values('c_counts', ascending = False)\n    ptc_series = pd.merge(pt_series, c_series, left_index=True,right_index=True,how='outer')\n\n    if len(ptc_series[ptc_series['pt_counts'] != ptc_series['c_counts']]) > 0:\n        return(np.nan)\n\n    trans_map = ptc_series[ptc_series['pt_counts'] == 1]\n    trans_map = trans_map.reset_index().rename(columns={'index' : 'char'})\n    trans_map['p_char_index'] = trans_map['char'].map(lambda x: pt_text.find(x))\n    trans_map['c_char_index'] = trans_map['char'].map(lambda x: c_text.find(x))\n\n    return(dict(zip(trans_map['p_char_index'].values,trans_map['c_char_index'].values)))","6b0a71f0":"df_crib2['trans_map'] = df_crib2.apply(lambda x: trans_mapper(x),axis=1)","673443ab":"trans_len = 300\ntrans_dict = {}\nfor i in range(trans_len):\n    temp = set()\n    for j in df_crib2[df_crib2['text_len'] == trans_len].index:\n        t_map = df_crib2.loc[j,'trans_map']\n        if i in t_map:\n            temp = temp.union([t_map[i]])\n    trans_dict[i] = temp","9ed4694b":"trans_s = pd.Series(trans_dict)","6fffe739":"print(trans_s.map(len).min())\nprint(trans_s.map(len).max())","c6c0a99f":"trans_s = trans_s.map(lambda x: list(x)[0] if len(list(x))>0 else np.nan)","ca2a92cd":"def rowcol(i,n_cols):\n    row = i \/\/ n_cols\n    col = i % n_cols\n    return((row,col))\n\ndef draw_square(n,n_cols):\n    n_rows = rowcol(n,n_cols)[0]\n    df = pd.DataFrame(data=[[-1]*n_cols]*n_rows,index=range(n_rows),columns=range(n_cols))\n    for i in range(n):\n        df.loc[rowcol(i,n_cols)] = i\n    return(df)","2746520e":"trans_s.rename('c_char_index').to_frame().reset_index().rename(columns={'index':'p_char_index'}).sort_values(by='c_char_index').T.style.format(\"{:.0f}\")","94727f70":"draw_square(300,24).style.format(\"{:.0f}\")","451ddd64":"def read_col(col,square):\n    n_rows = len(square)\n    res = []\n    for i in range(n_rows):\n        x_col = square.loc[i,col]\n        if ~np.isnan(x_col):\n            res = res + [x_col]\n    return(res)\n\ndef alternate_cols(col_1,col_2,square):\n    n_rows = len(square)\n    res = []\n    for i in range(n_rows):\n        x_col1 = square.loc[i,col_1]\n        x_col2 = square.loc[i,col_2]\n        if ~np.isnan(x_col1):\n            res = res + [x_col1]\n        if ~np.isnan(x_col2):\n            res = res + [x_col2]\n    return(res)","cb0038f6":"encipher_trans_dict = {}\ndecipher_trans_dict = {}\nfor i in range(1,301):\n    df = draw_square(i,24)\n    res = []\n    res = res + read_col(0,df)\n    for j in range(1,12):\n        res = res + alternate_cols(j,24-j,df)\n    res = res + read_col(12,df)\n    encipher_trans_dict[i] = res\n    decipher_trans_dict[i] = np.argsort(res)","b8c2b9e4":"def encipher4(p_text):\n    p_len = len(p_text)\n    res = encipher_trans_dict[p_len]\n    return(''.join([p_text[int(res[i])] for i in range(p_len)]))\n\ndef decipher4(c_text):\n    c_len = len(c_text)\n    res = decipher_trans_dict[c_len]\n    return(''.join([c_text[int(res[i])] for i in range(c_len)]))","02e457e8":"train.head()","9f04faef":"train['ct_text'] = train['text'].map(lambda x: fractional_vigenere(decipher4(x),key_ord_3).translate(translation_2_ct))","3d89d408":"target_list = np.sort(df_p_chunked['target'].unique())","ed80bf02":"p_indexes_dict = {}\nfor i in target_list[:]:\n    df = df_p_chunked_list[i]\n    for j in train[train['target'] == i].index[:]:\n        ct_text = train.loc[j,'ct_text']\n        new_p_indexes = set(df[df['text'] == ct_text]['p_index'])\n        if len(new_p_indexes) > 0:\n            p_indexes_dict[j] = p_indexes_dict.get(j,set()).union(new_p_indexes)","1f5067e9":"train_p_indexes = pd.DataFrame(pd.Series(data=list(p_indexes_dict.values()), index = p_indexes_dict.keys(),dtype=object)).rename(columns={0:'p_indexes'})","d66d2278":"print(train.shape[0])\nprint(train_p_indexes.shape[0])","eeaa167c":"train = train.join(train_p_indexes)","7490fd79":"train.to_pickle('train_4.pkl')","f957225b":"test['ct_text'] = test['text'].map(lambda x: fractional_vigenere(decipher4(x),key_ord_3).translate(translation_2_ct))","f5a63a46":"p_indexes_dict = {}\nfor i in target_list[:]:\n    df = df_p_chunked_list[i]\n    for j in test.index[:]:\n        t_text = test.loc[j,'ct_text']\n        new_p_indexes = set(df[df['text'] == ct_text]['p_index'])\n        if len(new_p_indexes) > 0:\n            p_indexes_dict[j] = p_indexes_dict.get(j,set()).union(new_p_indexes)","9dccaac4":"test_p_indexes = pd.DataFrame(pd.Series(data=list(p_indexes_dict.values()), index = p_indexes_dict.keys(),dtype=object)).rename(columns={0:'p_indexes'})","72f06fbc":"print(test.shape[0])\nprint(test_p_indexes.shape[0])","76b4253c":"test = test.join(test_p_indexes)","66db0505":"test.to_pickle('test_4.pkl')","d7e79a2a":"## *2.2* Generating and Using a Crib (matched plaintexts and ciphertexts)","56ab15d7":"Now if we look closer at this transposition, we can see a pattern:\n* The plaintext is written in rows of 24 columns (since the plaintext is of length 300, there is at most 12 such rows)\n* Then it is read \n    * by column 0 from top to bottom\n    * then by alternating reading from top to bottom in columns j and 24-j for j between 1 and 11\n    * and finally by column 12 from top to bottom     ","2c744f8a":"# 3. Matching Ciphertexts and Plaintexts\n## *3.1.* For the Train Set","a730c9ad":"If we focus on the cipher &  plaintexts of length 300 (the most frequent ones):","4944d1f2":"Now we look at each ciphertext&plaintext pair and infer the transposition using characters in the message that have a count equal to 1.","9ef62b3d":"Since character frequencies are almost equal for cipher #3 and cipher #4, it seems plausible that cipher #4 is merely a transposition. \nTo identify this transposition, we shall generate a crib by matching plaintexts and ciphertexts by target and length and character counts.","a9f94d26":"## *3.2* For the Test Set","b110cddb":"This kernel presents a full solution for cipher #4. Thank you very much to Phil and all kagglers who have made this competition so much fun.\n\n# 1. Loading the Data\n## *1.1* Loading, Preprocessing and Chunking the Plaintexts","f7af3cba":"# *2* Decrypting Cipher #4\n## *2.1* Looking at Character Frequencies","5d97869c":"## *1.3* Loading the Cipher #3 Encryption and Decryption Functions","89a01d73":"## *1.2* Loading the Competition Train and Test Sets"}}