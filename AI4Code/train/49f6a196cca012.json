{"cell_type":{"6ac0b32e":"code","04fe17da":"code","7c9b0041":"code","4cd6fc98":"code","e4042ba7":"code","ee5d4150":"code","b0938bb9":"code","3e1035e7":"code","e5f76514":"code","66660055":"code","304369f5":"code","738acaf7":"code","1a078ce0":"code","d19840a8":"code","226b753b":"code","552f674d":"code","343aaad4":"code","4e568484":"code","00e4b0f2":"code","bdf9c82a":"code","cf438c01":"code","d2e97f7d":"code","9c50e15f":"code","11bf5b95":"code","14649db3":"code","e26d1719":"code","36b92255":"code","5ebd8b4c":"code","e634d018":"code","fe54bccd":"code","f0561b70":"markdown"},"source":{"6ac0b32e":"version = \"gpu_model\" #@param {type: \"string\"}\nlr =  1e-3#@param {type: \"number\"} #testar 8e-4\nbs =    8#@param {type: \"integer\"} \nseq_len =  200#@param {type: \"integer\"}\npatience =  1#@param {type: \"integer\"}\nmax_epochs =  1#@param {type: \"integer\"}\ndropout_rate = 0.3#@param {type: \"number\"}\ndrop_connect_rate = 0.5#@param {type: \"number\"}\ndebug = False #@param {type: \"boolean\"}\n\n\n# Define hyperparameters\nhparams = {\"version\": version,\n           \"lr\": lr,\n           \"bs\": bs,\n           \"seq_len\": seq_len,\n           \"patience\": patience,\n           \"max_epochs\": max_epochs,\n           \"dropout_rate\": dropout_rate,\n           \"drop_connect_rate\": drop_connect_rate,\n           \"debug\": debug, \n           \"enet\": \"efficientnet-b0\",\n           \"t5\": \"t5-small\"}\nhparams             ","04fe17da":"import gc","7c9b0041":"myfile1 = open('\/kaggle\/input\/qsocrlarge25\/qsocrlarge\/text_train.txt', \"r\")\nnum_lines1 = sum(1 for line in myfile1)\nprint('len of train:', num_lines1)\nmyfile1.close()\n\nmyfile2 = open('\/kaggle\/input\/qsocrlarge25\/qsocrlarge\/text_val.txt', \"r\")\nnum_lines2 = sum(1 for line in myfile2)\nprint('len of val:', num_lines2)\nmyfile2.close()\n\nmyfile3 = open('\/kaggle\/input\/qsocrlarge25\/qsocrlarge\/text_test.txt', \"r\")\nnum_lines3 = sum(1 for line in myfile3)\nprint('len of test:', num_lines3)\nmyfile3.close()","4cd6fc98":"data_size = 0.45","e4042ba7":"del myfile1, myfile2, myfile3\ngc.collect()","ee5d4150":"!pip install sacrebleu pytorch-lightning transformers efficientnet-pytorch --quiet     ","b0938bb9":"import matplotlib.pyplot as plt\nfrom PIL import Image\nimport random\nimport pytorch_lightning as pl\nfrom efficientnet_pytorch import EfficientNet\nimport os\nimport json\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.transforms import Lambda, ToTensor, Compose\nimport torchvision.transforms as transforms\nfrom torchvision.utils import make_grid\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nimport sacrebleu","3e1035e7":"os.environ['PYTHONHASHSEED'] = '0'\n# Important: Fix seeds so we can replicate results\nseed_value = 123\ntorch.manual_seed(seed_value)\nseed = np.random.seed(seed_value)\n\nif torch.cuda.is_available():\n  torch.cuda.manual_seed(seed_value)\n  torch.cuda.manual_seed_all(seed_value)\n  torch.backends.cudnn.deterministic = True\n  torch.backends.cudnn.benchmark = False\n\n  print(torch.cuda.device_count())\n  print(torch.cuda.get_device_name(0))\n\n# Define default device, we should use the GPU (cuda) if available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","e5f76514":"def random_lines(fname, k):\n    lines = fname.read().splitlines()\n    return random.sample(lines, k)","66660055":"MODES = [\"train\", \"val\", \"test\"]\n\nclass OCR_DATA(Dataset):\n    '''\n    Abstra\u00e7\u00e3o do OCR_DATA com o dataset RVL CDIP  \n    '''\n    def __init__(self, mode, seq_len, width=300, height=400, t5_model=\"t5-small\",image_transform=Compose([transforms.ToTensor(), Lambda(lambda img: 2.0*(img - 1.0))])):\n      '''\n      mode: um de \"train\", \"val\", test\".\n      seq_len: tamanho m\u00e1ximo de sequ\u00eancia. 64 padr\u00e3o para alinhar com feature 8x8 da efficientnet.\n      transform: transformadas para serem aplicadas somente na imagem.\n      '''\n      assert mode in MODES\n      self.mode = mode\n      self.seq_len = seq_len\n\n      self.TOKENIZER = T5Tokenizer.from_pretrained(t5_model)\n\n      self.image_transform = image_transform\n      self.reshape_trans = transforms.Compose([transforms.Resize((height, width))])\n        \n      myfile = open('\/kaggle\/input\/qsocrlarge25\/qsocrlarge\/text_' + mode + '.txt', \"r\") \n      num_lines = sum(1 for line in myfile)\n      myfile.close()\n        \n      self.targets = list()\n      self.images_path = list()\n      \n      random.seed(123)\n      file = open('\/kaggle\/input\/qsocrlarge25\/qsocrlarge\/text_' + mode + '.txt', \"r\")  \n  \n      for txt_file in random_lines(file, int(num_lines*data_size)):\n          self.images_path.append('\/kaggle\/input\/rvlcdip25\/rvlcdip\/images\/' + txt_file.split(' ')[0][:-4]+'.tif')\n          with open('\/kaggle\/input\/qsocrlarge25\/qsocrlarge\/' + txt_file.split(' ')[0], 'r') as target_file:\n                line = target_file.read().replace(\"\\n\", \" \")\n                self.targets.append(line)\n      \n      file.close()\n      self.len = len(self.targets)  \n\n\n    def __len__(self):\n      return self.len\n\n    def __getitem__(self, i):\n        '''\n        Imagens s\u00e3o extra\u00eddas do .tif, convertidas para o formato [H, W, C], float32.\n        Transformadas padr\u00e3o s\u00e3o normaliza\u00e7\u00e3o para efficientnet com advprop e totensor.\n        Frase original tamb\u00e9m \u00e9 retornada para c\u00e1lculo do bleu.\n        '''\n        caption = self.targets[i]\n\n        target = self.TOKENIZER.encode(caption,\n                                    padding='max_length',\n                                    truncation=True,\n                                    max_length=self.seq_len,\n                                    return_tensors='pt')[0]                          \n        \n        im_rgb = Image.open(self.images_path[i]).convert('RGB')\n        image = self.reshape_trans(im_rgb)\n\n        if self.image_transform is not None:\n          image = self.image_transform(image)\n\n        return image, caption, target\n\n    def get_dataloader(self, batch_size, shuffle, num_workers=4):\n        return DataLoader(self, batch_size=batch_size, shuffle=shuffle, pin_memory=True, num_workers=num_workers)","304369f5":"train_transforms = Compose(\n    [transforms.ToTensor(),\n     Lambda(lambda img: 2.0*(img - 1.0))])\n\ntest_transforms = Compose(\n    [transforms.ToTensor(),\n     Lambda(lambda img: 2.0*(img - 1.0))])\n\n# train_transforms = Compose(\n#     [transforms.ToTensor(),\n#      Lambda(lambda img: 2.0*img - 1.0)])\n\n# test_transforms = Compose(\n#     [transforms.ToTensor(),\n#      Lambda(lambda img: 2.0*img - 1.0)])","738acaf7":"datasets = {\"train\": OCR_DATA(\"train\", hparams['seq_len'], image_transform=train_transforms), \"val\": OCR_DATA(\"val\", hparams['seq_len'], image_transform=train_transforms),\n            \"test\": OCR_DATA(\"test\", hparams['seq_len'], image_transform=test_transforms)}","1a078ce0":"dataset_lens = {mode: len(datasets[mode]) for mode in MODES}\nprint(f\"Lens: {dataset_lens}\")","d19840a8":"TOKENIZER = datasets[\"train\"].TOKENIZER","226b753b":"image = datasets[\"train\"][2][0]\nprint('max image', image.max())\nprint('min image', image.min())\nprint('Image shape:', image.shape)\nprint('Image .transpose(1, 2, 0) shape:', image.cpu().numpy().transpose(1, 2, 0).shape)\n\n\nplt.figure(figsize=(5, 10))\nplt.imshow((image.cpu().numpy().transpose(1, 2, 0)+1)\/2)\n\ncompany = datasets[\"train\"][2][1]\nprint('company:', company)\n\ntarget = datasets[\"train\"][2][2]\nprint('target:', target)","552f674d":"display_dataloaders = {mode: datasets[mode].get_dataloader(batch_size=4, shuffle=True) for mode in MODES}\n\n\n# Test-load batchs \ntrain_batch, val_batch, test_batch = next(iter(display_dataloaders[\"train\"])), next(iter(display_dataloaders[\"val\"])), next(iter(display_dataloaders[\"test\"]))\n\nprint(\"Train batch\")\nfor image, label, target in zip(train_batch[0], train_batch[1], train_batch[2]):\n  plt.figure(figsize=(4, 10))\n  print(f\"Label: {label}\")\n  plt.imshow((image.squeeze(0).cpu().numpy().transpose(1, 2, 0)+1)\/2)  \n  # plt.imshow((image.squeeze(0).cpu().numpy().transpose(1, 2, 0)\/2+1))\n  plt.show()  \nprint('----------------------------------------')\n\n\nprint(\"Val batch\")\nfor image, label, target in zip(val_batch[0], val_batch[1], val_batch[2]):\n  plt.figure(figsize=(4, 10))\n  print(f\"Label: {label}\")\n  plt.imshow((image.squeeze(0).cpu().numpy().transpose(1, 2, 0)+1)\/2)\n  plt.show()\n    \nprint('----------------------------------------')    \n\nprint(\"Test batch\")\nfor image, label, target in zip(test_batch[0], test_batch[1], train_batch[2]):\n  plt.figure(figsize=(4, 10))\n  print(f\"Label: {label}\")\n  plt.imshow((image.squeeze(0).cpu().numpy().transpose(1, 2, 0)+1)\/2)\n  plt.show()","343aaad4":"del display_dataloaders, train_batch, val_batch, test_batch\ngc.collect()","4e568484":"torch.cuda.empty_cache()","00e4b0f2":"import collections\n\ndef normalize_answer(s):\n    \"\"\"Lower text and remove extra whitespace.\"\"\"\n\n    def white_space_fix(text):\n        return ' '.join(text.split())\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(lower(s))\n\ndef get_tokens(s):\n    if not s: return []\n    return normalize_answer(s).split()\n\ndef compute_exact(a_gold, a_pred):\n    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n\ndef compute_f1(a_gold, a_pred):\n    gold_toks = get_tokens(a_gold)\n    pred_toks = get_tokens(a_pred)\n    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n    num_same = sum(common.values())\n    if len(gold_toks) == 0 or len(pred_toks) == 0:\n        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n        return int(gold_toks == pred_toks)\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same \/ len(pred_toks)\n    recall = 1.0 * num_same \/ len(gold_toks)\n    f1 = (2 * precision * recall) \/ (precision + recall)\n    return f1","bdf9c82a":"\"\"\"\nCode borrowed with thanks from:\nhttps:\/\/github.com\/ndrplz\/ConvLSTM_pytorch\nhttps:\/\/github.com\/shreyaspadhy\/UNet-Zoo\/blob\/master\/CLSTM.py\nhttps:\/\/gist.github.com\/halochou\/acbd669af86ecb8f988325084ba7a749\n\"\"\"\n\nimport torch.nn as nn\nimport torch\n\n\nclass ConvGRUCell(nn.Module):\n    \"\"\"\n    Basic CGRU cell.\n    \"\"\"\n\n    def __init__(self, in_channels, hidden_channels, kernel_size, bias):\n\n        super(ConvGRUCell, self).__init__()\n\n        self.input_dim  = in_channels\n        self.hidden_dim = hidden_channels\n\n        self.kernel_size = kernel_size\n        self.padding = kernel_size[0] \/\/ 2, kernel_size[1] \/\/ 2\n        self.bias = bias\n        self.update_gate = nn.Conv2d(in_channels=self.input_dim+self.hidden_dim, out_channels=self.hidden_dim,\n                                     kernel_size=self.kernel_size, padding=self.padding,\n                                     bias=self.bias)\n        self.reset_gate = nn.Conv2d(in_channels=self.input_dim+self.hidden_dim, out_channels=self.hidden_dim,\n                                    kernel_size=self.kernel_size, padding=self.padding,\n                                    bias=self.bias)\n\n        self.out_gate = nn.Conv2d(in_channels=self.input_dim+self.hidden_dim, out_channels=self.hidden_dim,\n                                  kernel_size=self.kernel_size, padding=self.padding,\n                                  bias=self.bias)\n\n    def forward(self, input_tensor, cur_state):\n\n        h_cur = cur_state\n        # data size is [batch, channel, height, width]\n        x_in = torch.cat([input_tensor, h_cur], dim=1)\n        update = torch.sigmoid(self.update_gate(x_in))\n        reset = torch.sigmoid(self.reset_gate(x_in))\n        x_out = torch.tanh(self.out_gate(torch.cat([input_tensor, h_cur * reset], dim=1)))\n        h_new = h_cur * (1 - update) + x_out * update\n\n        return h_new\n\n    def init_hidden(self, b, h, w):\n        return torch.zeros(b, self.hidden_dim, h, w).cuda()\n\n\nclass ConvGRU(nn.Module):\n\n    def __init__(self, in_channels, hidden_channels, kernel_size, num_layers,\n                 batch_first=False, bias=True, return_all_layers=False):\n        super(ConvGRU, self).__init__()\n\n        self._check_kernel_size_consistency(kernel_size)\n\n        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n        hidden_channels = self._extend_for_multilayer(hidden_channels, num_layers)\n        if not len(kernel_size) == len(hidden_channels) == num_layers:\n            raise ValueError('Inconsistent list length.')\n\n        self.input_dim  = in_channels\n        self.hidden_dim = hidden_channels\n        self.kernel_size = kernel_size\n        self.num_layers = num_layers\n        self.batch_first = batch_first\n        self.bias = bias\n        self.return_all_layers = return_all_layers\n\n        cell_list = []\n        for i in range(0, self.num_layers):\n            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i-1]\n\n            cell_list.append(ConvGRUCell(in_channels=cur_input_dim,\n                                          hidden_channels=self.hidden_dim[i],\n                                          kernel_size=self.kernel_size[i],\n                                          bias=self.bias))\n\n        self.cell_list = nn.ModuleList(cell_list)\n\n    def forward(self, input_tensor, hidden_state=None):\n        \"\"\"\n        Parameters\n        ----------\n        input_tensor: todo\n            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n        hidden_state: todo\n            None. todo implement stateful\n        Returns\n        -------\n        last_state_list, layer_output\n        \"\"\"\n        if not self.batch_first:\n            # (t, b, c, h, w) -> (b, t, c, h, w)\n            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n\n        # Implement stateful ConvGRU\n        if hidden_state is not None:\n            raise NotImplementedError()\n        else:\n            b, _, _, h, w = input_tensor.shape\n            hidden_state = self._init_hidden(b, h, w)\n\n        layer_output_list = []\n        last_state_list   = []\n\n        seq_len = input_tensor.size(1)\n        cur_layer_input = input_tensor\n\n        for layer_idx in range(self.num_layers):\n\n            h = hidden_state[layer_idx]\n            output_inner = []\n            for t in range(seq_len):\n\n                h = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n                                                 cur_state=h)\n                output_inner.append(h)\n\n            layer_output = torch.stack(output_inner, dim=1)\n            cur_layer_input = layer_output\n\n            layer_output_list.append(layer_output)\n            last_state_list.append(h)\n\n        if not self.return_all_layers:\n            layer_output_list = layer_output_list[-1:]\n            last_state_list = last_state_list[-1:]\n\n        return layer_output_list, last_state_list\n\n    def _init_hidden(self, b, h, w):\n        init_states = []\n        for i in range(self.num_layers):\n            init_states.append(self.cell_list[i].init_hidden(b, h, w))\n        return init_states\n\n    @staticmethod\n    def _check_kernel_size_consistency(kernel_size):\n        if not (isinstance(kernel_size, tuple) or\n                    (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n            raise ValueError('`kernel_size` must be tuple or list of tuples')\n\n    @staticmethod\n    def _extend_for_multilayer(param, num_layers):\n        if not isinstance(param, list):\n            param = [param] * num_layers\n        return param\n\n\nclass ConvBGRU(nn.Module):\n    # Constructor\n    def __init__(self, in_channels, hidden_channels,\n                 kernel_size, num_layers, bias=True, batch_first=False):\n\n        super(ConvBGRU, self).__init__()\n        self.forward_net = ConvGRU(in_channels, hidden_channels\/\/2, kernel_size,\n                                    num_layers, batch_first=batch_first, bias=bias)\n        self.reverse_net = ConvGRU(in_channels, hidden_channels\/\/2, kernel_size,\n                                    num_layers, batch_first=batch_first, bias=bias)\n\n    def forward(self, xforward, xreverse):\n        \"\"\"\n        xforward, xreverse = B T C H W tensors.\n        \"\"\"\n\n        y_out_fwd, _ = self.forward_net(xforward)\n        y_out_rev, _ = self.reverse_net(xreverse)\n\n        y_out_fwd = y_out_fwd[-1] # outputs of last CGRU layer = B, T, C, H, W\n        y_out_rev = y_out_rev[-1] # outputs of last CGRU layer = B, T, C, H, W\n\n        reversed_idx = list(reversed(range(y_out_rev.shape[1])))\n        y_out_rev = y_out_rev[:, reversed_idx, ...] # reverse temporal outputs.\n        ycat = torch.cat((y_out_fwd, y_out_rev), dim=2)\n\n        return ycat","cf438c01":"class EfficientT5(pl.LightningModule):\n\n    def __init__(self, params):\n        super(EfficientT5, self).__init__()\n\n        self.params = params\n        \n        override_params = {'dropout_rate':params['dropout_rate'],'drop_connect_rate':params['drop_connect_rate']}\n\n        # Features da efficient net atuar\u00e3o como as features do encoder do T5\n        self.encoder = EfficientNet.from_pretrained(params['enet'], advprop=True, **override_params)\n        self.decoder = T5ForConditionalGeneration.from_pretrained(params['t5'])\n\n        \n        # self.bridge = ConvGRU(in_channels=1280, hidden_channels=self.decoder.config.d_model, kernel_size=(3, 3), num_layers=1, batch_first=True)\n        self.bridge = ConvGRU(in_channels=112, hidden_channels=self.decoder.config.d_model, kernel_size=(3, 3), num_layers=1, batch_first=True)\n\n        self.tokenizer = TOKENIZER\n\n    def _get_efn_embeddings(self, images):\n\n        # Shape (N, 112, 16, 16)\n        # features = self.encoder.extract_features(images)\n        features = self.encoder.extract_endpoints(images)[\"reduction_4\"]\n        # print('features shape', features.shape)\n\n        features = features.unsqueeze(1)\n\n        # features = features.reshape(features.shape[0], 1, 112, 19, 13)\n        features = features.reshape(features.shape[0], 1, 112, 25, 19)\n\n        # Shape: (N, 512, -, -)\n        features, last_state_list = self.bridge(features)\n        \n        features = features[0]\n\n        # Shape: (N, -1, 512) -> Pra ficar do shape de entrada do decoder que \u00e9 (N, seq_len, d_model)\n        efn_embeddings = features \\\n            .permute(0, 1, 3, 4, 2) \\\n            .reshape(features.shape[0], -1, self.decoder.config.d_model)\n\n        return efn_embeddings\n\n    \n    def _generate_tokens(self, efn_embeddings):\n        '''\n        Token generation\n        '''\n        max_length = self.params['seq_len']\n\n        # Add start of sequence token\n        decoded_ids = torch.full((efn_embeddings.shape[0], 1),\n                                 self.decoder.config.decoder_start_token_id,\n                                 dtype=torch.long).to(efn_embeddings.device)\n        \n        encoder_hidden_states = self.decoder.get_encoder()(inputs_embeds=efn_embeddings)\n\n        for step in range(max_length-1):\n            logits = self.decoder(decoder_input_ids=decoded_ids,\n                                  encoder_outputs=encoder_hidden_states)[0]\n            next_token_logits = logits[:, -1, :]\n\n            # decoding\n            next_token_id = next_token_logits.argmax(1).unsqueeze(-1)\n            \n            # Check if output is end of senquence for all batches\n            if torch.eq(next_token_id[:, -1], self.tokenizer.eos_token_id).all():\n                break\n\n            # Concatenate past ids with new id, keeping batch dimension\n            decoded_ids = torch.cat([decoded_ids, next_token_id], dim=-1)\n\n        return decoded_ids\n\n    def forward(self, batch): # TODO pro futuro: colocar os dados necess\u00e1rios individualmente e n\u00e3o o batch inteiro\n        # Aqui os labels s\u00e3o strings e os tokens s\u00e3o os labels a serem inseridos no decoder\n        # print(len(batch))\n        images, label, token = batch\n\n        # Output do efn atua como embedding do encoder\n        efn_embeddings = self._get_efn_embeddings(images)\n        # print('efn_embeddings', efn_embeddings.shape)\n\n        if self.training:\n            outputs = self.decoder(inputs_embeds=efn_embeddings,\n                                   decoder_input_ids=None, \n                                   labels=token,\n                                   return_dict=True)\n            return outputs.loss\n        else:\n            return self._generate_tokens(efn_embeddings)\n\n    def training_step(self, batch, batch_idx): \n        loss = self(batch)\n        self.log('loss', loss, on_epoch=True, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        pred_tokens = self(batch)\n        # Tokens -> String\n        decoded_pred = [self.tokenizer.decode(tokens) for tokens in pred_tokens]\n        return {\"pred\": decoded_pred, \"target\": batch[1]}\n\n    def test_step(self, batch, batch_idx):\n        pred_tokens = self(batch)\n        # Tokens -> String\n        decoded_pred = [self.tokenizer.decode(tokens) for tokens in pred_tokens]\n        return {\"pred\": decoded_pred, \"target\": batch[1]}\n\n    def validation_epoch_end(self, outputs):\n        # Flatten dos targets e preds para arrays\n        trues = sum([list(x['target']) for x in outputs], [])\n        preds = sum([list(x['pred']) for x in outputs], [])\n\n        # n = random.choice(range(len(trues)))\n        n_samples = random.sample(range(len(trues)), 10)\n        for n in n_samples:\n            print(f\"\\nSample Target: {trues[n]}\\nPrediction: {preds[n]}\\n\")\n        \n        f1 = []\n        exact = []\n        for true, pred in zip(trues, preds):\n            f1.append(compute_f1(a_gold=true, a_pred=pred))\n            exact.append(compute_exact(a_gold=true, a_pred=pred))\n        f1 = np.mean(f1)\n        exact = np.mean(exact)\n\n        bleu = sacrebleu.corpus_bleu(preds, [trues])\n\n        self.log(\"val_bleu_score\", bleu.score, prog_bar=True)\n        self.log(\"val_bleu_1\", bleu.precisions[0], prog_bar=True)\n        self.log(\"val_bleu_4\", bleu.precisions[3], prog_bar=True)\n        self.log(\"val_f1\", f1, prog_bar=True)\n        self.log(\"val_exact\", exact, prog_bar=True)\n\n    def test_epoch_end(self, outputs):\n        # Flatten dos targets e preds para arrays\n        trues = sum([list(x['target']) for x in outputs], [])\n        preds = sum([list(x['pred']) for x in outputs], [])\n\n        n = random.choice(range(len(trues)))\n        print(f\"\\nSample Target: {trues[n]}\\nPrediction: {preds[n]}\\n\")\n\n        f1 = []\n        exact = []\n        for true, pred in zip(trues, preds):\n            f1.append(compute_f1(a_gold=true, a_pred=pred))\n            exact.append(compute_exact(a_gold=true, a_pred=pred))\n        f1 = np.mean(f1)\n        exact = np.mean(exact)\n\n        bleu = sacrebleu.corpus_bleu(preds, [trues])\n\n        self.log(\"test_bleu_score\", bleu.score, prog_bar=True)\n        self.log(\"test_bleu_1\", bleu.precisions[0], prog_bar=True)\n        self.log(\"test_bleu_4\", bleu.precisions[3], prog_bar=True)\n        self.log(\"test_f1\", f1, prog_bar=True)\n        self.log(\"test_exact\", exact, prog_bar=True)\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.params['lr'])\n    \n    def train_dataloader(self):\n        return datasets[\"train\"].get_dataloader(batch_size=self.params['bs'], shuffle=True)\n    def val_dataloader(self):\n        return datasets[\"val\"].get_dataloader(batch_size=self.params['bs'], shuffle=False)   \n    def test_dataloader(self):\n        return datasets[\"test\"].get_dataloader(batch_size=self.params['bs'], shuffle=False)","d2e97f7d":"torch.cuda.empty_cache()","9c50e15f":"!mkdir '\/kaggle\/working\/aula10_checkpoints'","11bf5b95":"model = EfficientT5(hparams)\n\ntrainer = pl.Trainer(gpus=1,\n                     max_epochs=150,\n                     accumulate_grad_batches=2,\n                     check_val_every_n_epoch=10,\n                     checkpoint_callback=False,  # Disable checkpoint saving.\n                     overfit_batches=2)\n\n# detect any anomaly in the gradients \n# Essa fun\u00e7\u00e3o foi importante para detectar um bug que tinha para precision=16\n# torch.autograd.set_detect_anomaly(True)\n\ntrainer.fit(model)\n\ntrainer.test(model)\ndel model  # Para n\u00e3o ter estouro de m\u00e9moria da GPU\ngc.collect()\ntorch.cuda.empty_cache()","14649db3":"# Initialize Lightning Module with parameters defined above.\nmodel = EfficientT5(hparams)\nprint(model)\n\n# Configure callbacks\nif hparams[\"debug\"]:\n  checkpoint_callback = None\n  logger = None\n  early_stop_callback = None\nelse:\n  checkpoint_path = '\/kaggle\/working\/aula10_checkpoints\/epoch=10.ckpt'\n  checkpoint_dir = os.path.dirname(os.path.abspath(checkpoint_path))\n  print(f'Files in {checkpoint_dir}: {os.listdir(checkpoint_dir)}')\n  print(f'Saving checkpoints to {checkpoint_dir}')\n  checkpoint_callback = pl.callbacks.ModelCheckpoint(prefix=hparams[\"version\"],\n                                                      filepath=checkpoint_dir,\n                                                      save_top_k=-1, \n                                                      monitor=\"val_bleu_score\", mode=\"max\")\n  early_stop_callback = pl.callbacks.EarlyStopping(monitor='val_bleu_score', patience=hparams[\"patience\"], mode='max')\n\ntrainer = pl.Trainer(gpus=1,\n                     fast_dev_run=hparams[\"debug\"],\n                     accumulate_grad_batches=2,\n                     checkpoint_callback=checkpoint_callback, \n                     callbacks=[early_stop_callback],\n                     max_epochs=hparams[\"max_epochs\"])","e26d1719":"# detect any anomaly in the gradients \n# torch.autograd.set_detect_anomaly(True)\ntrainer.fit(model)","36b92255":"best_model = \"\/kaggle\/working\/aula10_checkpoints\/gpu_model-epoch=0.ckpt\"\ntest_model = EfficientT5(hparams).load_from_checkpoint(best_model, params=hparams).cuda().eval()","5ebd8b4c":"test_dl = datasets[\"test\"].get_dataloader(batch_size=16, shuffle=False, num_workers=4)","e634d018":"trainer.test(test_model, test_dl)","fe54bccd":"batch = next(iter(test_dl))\nbatch[0] = batch[0].cuda()\n\ntest_model.eval()\n# test_model = test_model.cuda()\n\nwith torch.no_grad():\n  pred_tokens = test_model(batch)\n\ndecoded_preds = [test_model.tokenizer.decode(t) for t in pred_tokens]\n\nimages, labels, tokens = batch\n\nfor image, label, pred in zip(images, labels, decoded_preds):\n  plt.imshow(((image.squeeze(0).cpu().numpy().transpose(1, 2, 0)\/2+1)))\n  plt.show()\n  print(f'Labels: {label}\\nPred: {pred}\\n')","f0561b70":"# Definir a classe do modelo usado EfficientT5"}}