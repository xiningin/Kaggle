{"cell_type":{"ad989bca":"code","1a8f7259":"code","f68e0cc2":"code","0618bfca":"code","0dd1f3c2":"code","3a9b285c":"code","7c9cf493":"code","b16530b8":"code","525273d5":"code","124854c7":"code","99bde1d7":"code","f5dc90c3":"code","8216cf23":"code","87d50ebd":"code","f9b50f4f":"code","f0a0770e":"code","ea2c61e8":"code","c2978633":"code","944e8799":"code","bdce9b1b":"code","e3cc4830":"code","7692f90c":"code","8bc90529":"code","b54d8706":"code","fff36109":"code","b8309fbb":"code","65c22931":"code","b26f04ea":"code","e065126f":"code","4ef1b4b3":"code","a8416e26":"code","125689df":"code","d7d1af8c":"code","e43e41b4":"code","04e59a3c":"markdown","939fbd24":"markdown","e5a8598d":"markdown","f862b192":"markdown","21bdaf31":"markdown","caef0bb5":"markdown","5279a6a8":"markdown","87b21444":"markdown","dc4785fd":"markdown","9eea1b96":"markdown","456f5d51":"markdown","921c3517":"markdown","c0915487":"markdown"},"source":{"ad989bca":"import torch\nimport random\nimport os\nimport pandas as pd\nimport numpy as np\nfrom torch import nn\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nfrom kfoldds_py import get_kfold_datasets2\nfrom titanic_nn_py import TitanicDataset, LinearRegression, DNN1, DNN2, train, predict","1a8f7259":"def validate(model, valloader):\n    with torch.no_grad():\n        predicts = []\n        labels = []\n        for X, y in valloader:\n            pred = model(X.float())\n            pred[pred >= 0.5] = 1\n            pred[pred < 0.5] = 0\n            pred = pred.reshape(-1)\n            predicts.extend(pred)\n            labels.extend(y)\n        result = [p == t for p, t in zip(predicts, labels)]\n        accuracy = sum(result) \/ len(predicts)\n        return {'accuracy': accuracy, 'predicts': predicts, 'lables': labels}","f68e0cc2":"train_csv = '..\/input\/my-titanic-data\/mytrain.csv'\ntest_csv = '..\/input\/my-titanic-data\/mytest.csv'","0618bfca":"df1 = pd.read_csv(train_csv)\ndft = pd.read_csv(test_csv)","0dd1f3c2":"drop_columns = ['Sex_male', 'Title_another']","3a9b285c":"df1 = df1.drop(drop_columns, axis=1)\ndft = dft.drop(drop_columns, axis=1)","7c9cf493":"X_train = df1.drop(['Survived'], axis=1)\ny_train = df1['Survived']\nX_test = dft\n\ntorch.manual_seed(2)\nrandom.seed(2)\nnp.random.seed(2)","b16530b8":"X_test","525273d5":"dataset = TitanicDataset(X_train, y_train)\ndatasets = get_kfold_datasets2(dataset, 3)\nloaders = [(DataLoader(t, batch_size=32), DataLoader(v, batch_size=32)) for t, v in datasets]","124854c7":"INPUT_SIZE = X_train.shape[1]\nHIDDEN_SIZE1 = 32\nHIDDEN_SIZE2 = 16\nOUTPUT_SIZE = 1\n\nm1 = LinearRegression(INPUT_SIZE, OUTPUT_SIZE)\nm2 = DNN1(INPUT_SIZE, HIDDEN_SIZE1, OUTPUT_SIZE)\nm3 = DNN2(INPUT_SIZE, HIDDEN_SIZE1, HIDDEN_SIZE2, OUTPUT_SIZE)\nmodels = [m1, m2, m3]","99bde1d7":"train_loss = {}\nval_results = {}\nfor cnt, (model, (t_loader, v_loader)) in enumerate(zip(models, loaders)):\n    print(f'train #{cnt} start.')\n    train_loss[cnt] = train(model, t_loader, epochs=500)\n    print(f'validate #{cnt} start.')\n    val_results[cnt] = validate(model, v_loader)","f5dc90c3":"for idx, d in val_results.items():\n    print(f'k: {idx}, accuracy: {d[\"accuracy\"]:%}')","8216cf23":"with torch.no_grad():\n    results = [predict(model, dft) for model in models]","87d50ebd":"for item in results:\n    item[item >= 0.5] = 1\n    item[item < 0.5] = 0\n\nresult = results[0] + results[1] + results[2]\nresult[result < 2] = 0\nresult[result > 1] = 1\n\nresult = result.squeeze().int().tolist()","f9b50f4f":"pid = pd.read_csv('..\/input\/titanic\/test.csv')['PassengerId']\nsubmission_wo_hypartune = pd.DataFrame({'PassengerId': pid, 'Survived': result})\n#submission_wo_hypartune.to_csv('submission_wo_hypartune.csv', index=False)\nsubmission_wo_hypartune.to_csv(f'submission_wo_hypartune_{INPUT_SIZE}inputs.csv', index=False)","f0a0770e":"import ray\nfrom ray import tune\nimport numpy as np","ea2c61e8":"def get_datasets():\n    df1 = pd.read_csv('\/kaggle\/input\/my-titanic-data\/mytrain.csv')\n    X_train = df1.drop(drop_columns + ['Survived'], axis=1)\n    y_train = df1['Survived']\n    dataset = TitanicDataset(X_train, y_train)\n    datasets = get_kfold_datasets2(dataset, 3)\n    return datasets","c2978633":"from torch.utils.data import DataLoader","944e8799":"if not os.path.isdir('.\/checkpoint'):\n    os.mkdir('.\/checkpoint')","bdce9b1b":"def hypartune_DNN1(config, checkpoint_dir=None):\n    torch.manual_seed(2)\n    random.seed(2)\n    global INPUT_SIZE, OUTPUT_SIZE\n\n    if checkpoint_dir:\n        checkpoint = os.path.join(checkpoint_dir, \"checkpoint\")\n        model_state, optimizer_state = torch.load(checkpoint)\n        net.load_state_dict(model_state)\n        optimizer.load_state_dict(optimizer_state)\n\n    datasets = get_datasets()\n    train_sets, val_sets = datasets[1]  # [0] for Regression, [2] for DNN2\n    trainloader = DataLoader(train_sets, batch_size=config['batch_size'])\n    valloader = DataLoader(val_sets, batch_size=config['batch_size'])\n\n    net = DNN1(INPUT_SIZE, config['l1'], OUTPUT_SIZE)\n\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(net.parameters(), lr=config['lr'])\n\n    for epoch in range(config['epochs']):\n        running_loss = 0.0\n        \n        for cnt, (X, y) in enumerate(trainloader, 1):\n            optimizer.zero_grad()\n\n            pred = net(X.float())           \n            loss = criterion(pred.reshape(-1), y.float())\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        print(f'[{epoch:4d}] loss: {running_loss \/ cnt:.4f}')\n\n        val_loss = 0.0\n        predicts = []\n        labels = []\n        with torch.no_grad():\n            for cnt, (val_X, val_y) in enumerate(valloader, 0):\n                pred = net(val_X.float())\n                loss = criterion(pred.reshape(-1), val_y.float())\n                val_loss += loss.item()\n                pred[pred >= 0.5] = 1\n                pred[pred < 0.5] = 0\n                predicts.extend(pred)\n                labels.extend(val_y)\n\n            result = [p == t for p, t in zip(predicts, labels)]\n            accuracy = sum(result) \/ len(predicts)\n            val_loss_avg = val_loss \/ cnt\n\n        with tune.checkpoint_dir(step=epoch) as checkpoint_dir:  # using default dir\n            path = os.path.join(checkpoint_dir, 'checkpoint')\n            torch.save(\n                (net.state_dict(), optimizer.state_dict()), path)\n\n        tune.report(loss=val_loss_avg, accuracy=accuracy)\n\n    print('Finished Training')","e3cc4830":"def tune_DNN1(num_samples=10):\n    config = {\n        'l1': tune.sample_from(lambda _: 2 ** np.random.randint(2, 6)),\n        #'l1': tune.choice([4, 8, 16, 32])\n        'lr': tune.loguniform(1e-4, 1e-1),\n        'batch_size': tune.choice([4, 8, 16, 32]),\n        'epochs': tune.choice([100, 300, 500])\n    }\n    result = tune.run(\n        tune.with_parameters(hypartune_DNN1),\n        config=config,\n        metric='loss',\n        mode='min',\n        num_samples=num_samples,\n        verbose=1\n    )\n\n    best_trial = result.get_best_trial('loss', 'min', 'last')\n    \n    print(f'Best trial config: {best_trial.config}')\n    print(f'Best trial final val loss: {best_trial.last_result[\"loss\"]}')\n    print(f'Best trial final val accuracy: {best_trial.last_result[\"accuracy\"]}')\n\n    global INPUT_SIZE, OUTPUT_SIZE\n    hidden = best_trial.config['l1']\n    model = DNN1(INPUT_SIZE, hidden, OUTPUT_SIZE)\n    checkpoint_path = os.path.join(best_trial.checkpoint.value, 'checkpoint')\n    model_state, optimizer_state = torch.load(checkpoint_path)\n    model.load_state_dict(model_state)\n    \n    torch.save(model.state_dict(), '\/kaggle\/working\/best_dnn1.pth')\n    return model, best_trial\n","7692f90c":"best_dnn1_model, best_dnn1 = tune_DNN1(16)","8bc90529":"def hypartune_DNN2(config, checkpoint_dir=None):\n    torch.manual_seed(2)\n    random.seed(2)\n    global INPUT_SIZE, OUTPUT_SIZE\n\n    if checkpoint_dir:\n        checkpoint = os.path.join(checkpoint_dir, \"checkpoint\")\n        model_state, optimizer_state = torch.load(checkpoint)\n        net.load_state_dict(model_state)\n        optimizer.load_state_dict(optimizer_state)\n\n    datasets = get_datasets()\n    train_sets, val_sets = datasets[2]  # [0] for Regression, [2] for DNN2\n    trainloader = DataLoader(train_sets, batch_size=config['batch_size'])\n    valloader = DataLoader(val_sets, batch_size=config['batch_size'])\n\n    net = DNN2(INPUT_SIZE, config['l1'], config['l2'], OUTPUT_SIZE)\n\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(net.parameters(), lr=config['lr'])\n\n    for epoch in range(config['epochs']):\n        running_loss = 0.0\n        \n        for cnt, (X, y) in enumerate(trainloader, 1):\n            optimizer.zero_grad()\n\n            pred = net(X.float())           \n            loss = criterion(pred.reshape(-1), y.float())\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        print(f'[{epoch:4d}] loss: {running_loss \/ cnt:.4f}')\n\n        val_loss = 0.0\n        predicts = []\n        labels = []\n        with torch.no_grad():\n            for cnt, (val_X, val_y) in enumerate(valloader, 0):\n                pred = net(val_X.float())\n                loss = criterion(pred.reshape(-1), val_y.float())\n                val_loss += loss.item()\n                pred[pred >= 0.5] = 1\n                pred[pred < 0.5] = 0\n                predicts.extend(pred)\n                labels.extend(val_y)\n\n            result = [p == t for p, t in zip(predicts, labels)]\n            accuracy = sum(result) \/ len(predicts)\n            val_loss_avg = val_loss \/ cnt\n\n        with tune.checkpoint_dir(step=epoch) as checkpoint_dir:\n            path = os.path.join(checkpoint_dir, 'checkpoint')\n            torch.save(\n                (net.state_dict(), optimizer.state_dict()), path)\n\n        tune.report(loss=val_loss_avg, accuracy=accuracy)\n    print('Finished Training')","b54d8706":"def tune_DNN2(num_samples=10):\n    config = {\n        'l1': tune.sample_from(lambda _: 2 ** np.random.randint(2, 6)),\n        'l2': tune.sample_from(lambda _: 2 ** np.random.randint(2, 6)),\n        'lr': tune.loguniform(1e-4, 1e-1),\n        'batch_size': tune.choice([4, 8, 16, 32]),\n        'epochs': tune.choice([100, 300, 500])\n    }\n    result = tune.run(\n        tune.with_parameters(hypartune_DNN2),\n        config=config,\n        metric='loss',\n        mode='min',\n        num_samples=num_samples,\n        verbose=1\n    )\n\n    best_trial = result.get_best_trial('loss', 'min', 'last')\n    print(f'Best trial config: {best_trial.config}')\n    print(f'Best trial final val loss: {best_trial.last_result[\"loss\"]}')\n    print(f'Best trial final val accuracy: {best_trial.last_result[\"accuracy\"]}')\n\n    global INPUT_SIZE, OUTPUT_SIZE\n    hidden1 = best_trial.config['l1']\n    hidden2 = best_trial.config['l2']\n    model = DNN2(INPUT_SIZE, hidden1, hidden2, OUTPUT_SIZE)\n    checkpoint_path = os.path.join(best_trial.checkpoint.value, 'checkpoint')\n    model_state, optimizer_state = torch.load(checkpoint_path)\n    model.load_state_dict(model_state)\n    \n    torch.save(model.state_dict(), '\/kaggle\/working\/best_dnn2.pth')\n    return model, best_trial","fff36109":"best_dnn2_model, best_dnn2 = tune_DNN2(24)","b8309fbb":"def hypartune_LinearRegression(config, checkpoint_dir=None):\n    torch.manual_seed(2)\n    random.seed(2)\n    global INPUT_SIZE, OUTPUT_SIZE\n\n    if checkpoint_dir:\n        checkpoint = os.path.join(checkpoint_dir, \"checkpoint\")\n        model_state, optimizer_state = torch.load(checkpoint)\n        net.load_state_dict(model_state)\n        optimizer.load_state_dict(optimizer_state)\n\n    datasets = get_datasets()\n    train_sets, val_sets = datasets[0]  # [1] for DNN1, [2] for DNN2\n    trainloader = DataLoader(train_sets, batch_size=config['batch_size'])\n    valloader = DataLoader(val_sets, batch_size=config['batch_size'])\n\n    net = LinearRegression(INPUT_SIZE, OUTPUT_SIZE)\n\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(net.parameters(), lr=config['lr'])\n\n    for epoch in range(config['epochs']):  # loop over the dataset multiple times\n        running_loss = 0.0\n        \n        for cnt, (X, y) in enumerate(trainloader, 1):\n            optimizer.zero_grad()\n\n            pred = net(X.float())           \n            loss = criterion(pred.reshape(-1), y.float())\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        print(f'[{epoch:4d}] loss: {running_loss \/ cnt:.4f}')\n\n        val_loss = 0.0\n        predicts = []\n        labels = []\n        with torch.no_grad():\n            for cnt, (val_X, val_y) in enumerate(valloader, 0):\n                pred = net(val_X.float())\n                loss = criterion(pred.reshape(-1), val_y.float())\n                val_loss += loss.item()\n                pred[pred >= 0.5] = 1\n                pred[pred < 0.5] = 0\n                predicts.extend(pred)\n                labels.extend(val_y)\n\n            result = [p == t for p, t in zip(predicts, labels)]\n            accuracy = sum(result) \/ len(predicts)\n            val_loss_avg = val_loss \/ cnt\n\n        with tune.checkpoint_dir(step=epoch) as checkpoint_dir:\n            path = os.path.join(checkpoint_dir, 'checkpoint')\n            torch.save(\n                (net.state_dict(), optimizer.state_dict()), path)\n\n        tune.report(loss=val_loss_avg, accuracy=accuracy)\n    print('Finished Training')","65c22931":"def tune_LinearRegression(num_samples=10):\n    config = {\n        'lr': tune.loguniform(1e-4, 1e-1),\n        'batch_size': tune.choice([4, 8, 16, 32]),\n        'epochs': tune.choice([100, 300, 500])\n    }\n    result = tune.run(\n        tune.with_parameters(hypartune_LinearRegression),\n        config=config,\n        metric='loss',\n        mode='min',\n        num_samples=num_samples,\n        verbose=1\n    )\n\n    best_trial = result.get_best_trial('loss', 'min', 'last')\n    print(f'Best trial config: {best_trial.config}')\n    print(f'Best trial final val loss: {best_trial.last_result[\"loss\"]}')\n    print(f'Best trial final val accuracy: {best_trial.last_result[\"accuracy\"]}')\n\n    global INPUT_SIZE, OUTPUT_SIZE\n    model = LinearRegression(INPUT_SIZE, OUTPUT_SIZE)\n    checkpoint_path = os.path.join(best_trial.checkpoint.value, 'checkpoint')\n    model_state, optimizer_state = torch.load(checkpoint_path)\n    model.load_state_dict(model_state)\n    \n    torch.save(model.state_dict(), '\/kaggle\/working\/best_regression.pth')\n    return model, best_trial","b26f04ea":"best_lr_model, best_lr = tune_LinearRegression(12)","e065126f":"best_models = [best_lr_model, best_dnn1_model, best_dnn2_model]\nval_results = {}\nfor cnt, (model, (_, v_loader)) in enumerate(zip(best_models, loaders)):\n    val_results[cnt] = validate(model, v_loader)\n\nfor idx, d in val_results.items():\n    print(f'k: {idx}, accuracy: {d[\"accuracy\"]:%}')","4ef1b4b3":"with torch.no_grad():\n    results = [predict(model, X_test) for model in best_models]\n\nfor item in results:\n    item[item >= 0.5] = 1\n    item[item < 0.5] = 0\n\nresult = results[0] + results[1] + results[2]\nresult[result < 2] = 0\nresult[result > 1] = 1\n\nresult = result.squeeze().int().tolist()\n\nprint(f'{sum(result)} \/ {len(result)} = {sum(result) \/ len(result)}')","a8416e26":"pid = pd.read_csv('..\/input\/titanic\/test.csv')['PassengerId']\nsubmission_with_hypartuning = pd.DataFrame({'PassengerId': pid, 'Survived': result})\n#submission_with_hypartuning.to_csv('submission_with_hypartuning.csv', index=False)\nsubmission_with_hypartuning.to_csv(f'submission_with_hypartuning_{INPUT_SIZE}inputs.csv', index=False)","125689df":"submissions = [pd.DataFrame({'PassengerId': pid, 'Survived': item}) for item in results]\nfor idx, sbm in enumerate(submissions):\n    sbm.to_csv(f'submission_with_hypertune{idx}.csv', index=False)","d7d1af8c":"INPUT_SIZE = X_train.shape[1]\nDNN1_HIDDEN_SIZE = best_dnn1.config['l1']\nDNN2_HIDDEN1_SIZE = best_dnn2.config['l1']\nDNN2_HIDDEN2_SIZE = best_dnn2.config['l2']\nOUTPUT_SIZE = 1\n\nm1 = LinearRegression(INPUT_SIZE, OUTPUT_SIZE)\nm2 = DNN1(INPUT_SIZE, DNN1_HIDDEN_SIZE, OUTPUT_SIZE)\nm3 = DNN2(INPUT_SIZE, DNN2_HIDDEN1_SIZE, DNN2_HIDDEN2_SIZE, OUTPUT_SIZE)\nmodels = [m1, m2, m3]\n\nbs_lr = best_lr.config['batch_size']\nbs_dnn1 = best_dnn1.config['batch_size']\nbs_dnn2 = best_dnn2.config['batch_size']\nbatch_sizes = [bs_lr, bs_dnn1, bs_dnn2]\n\nloaders = []\nfor (t_set, v_set), bs in zip(datasets, batch_sizes):\n    t_loader = DataLoader(t_set, batch_size=bs)\n    v_loader = DataLoader(v_set, batch_size=bs)\n    loaders.append((t_loader, v_loader))\n\nLRs = [best_lr.config['lr'], best_dnn1.config['lr'], best_dnn2.config['lr']]\noptimizers = [optim.Adam(m.parameters(), lr=lr) for m, lr in zip(models, LRs)]\n\nep_lr = best_lr.config['epochs']\nep_dnn1 = best_dnn1.config['epochs']\nep_dnn2 = best_dnn2.config['epochs']\nepochs = [ep_lr, ep_dnn1, ep_dnn2]\n\nparams = zip(models, loaders, optimizers, epochs)\ntrain_loss = {}\nval_results = {}\nfor cnt, (model, (t_loader, v_loader), opt, ep) in enumerate(params):\n    print(f'train #{cnt} start.')\n    train_loss[cnt] = train(model, t_loader, optimizer=opt, epochs=ep)\n    print(f'validate #{cnt} start.')\n    val_results[cnt] = validate(model, v_loader)\n\nfor idx, d in val_results.items():\n    print(f'k: {idx}, accuracy: {d[\"accuracy\"]:%}')","e43e41b4":"models = [m1, m2, m3]\nwith torch.no_grad():\n    results = [predict(model, X_test) for model in models]\n\nfor item in results:\n    item[item >= 0.5] = 1\n    item[item < 0.5] = 0\n\nresult = results[0] + results[1] + results[2]\nresult[result < 2] = 0\nresult[result > 1] = 1\n\nresult = result.squeeze().int().tolist()\n\n\nprint(f'{sum(result)} \/ {len(result)} = {sum(result) \/ len(result)}')\n\npid = pd.read_csv('..\/input\/titanic\/test.csv')['PassengerId']\nsubmission_with_hypartuning = pd.DataFrame({'PassengerId': pid, 'Survived': result})\n#submission_with_hypartuning.to_csv('submission_with_hypartuning4.csv', index=False)\nsubmission_with_hypartuning.to_csv(f'submission_with_hypartuning4_{INPUT_SIZE}inputs.csv', index=False)","04e59a3c":"\u5f97\u3089\u308c\u305f\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u306e\u5024\u3092\u4f7f\u3063\u3066\u4f5c\u308a\u76f4\u3057\u305f\u30e2\u30c7\u30eb\u3067\u63a8\u6e2c\u3092\u884c\u3046\u3002","939fbd24":"\u691c\u8a3c\u30c7\u30fc\u30bf\u304b\u3089\u751f\u6b7b\u3092\u63a8\u6e2c\u3057\u3066\u3001\u7cbe\u5ea6\u3092\u8abf\u3079\u305f\u3002\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u30c1\u30e5\u30fc\u30f3\u306e\u524d\u306f\u3053\u3093\u306a\u611f\u3058\u3002\n\n+ `k: 0, accuracy: 78.451180%`\n+ `k: 1, accuracy: 80.134678%`\n+ `k: 2, accuracy: 78.114480%`","e5a8598d":"`validate`\u95a2\u6570\u306f\u6b21\u5143\u306e\u5c55\u958b\u306e\u4ed5\u65b9\u3067\u30a8\u30e9\u30fc\u3068\u306a\u308b\u3053\u3068\u304c\u3042\u3063\u305f\u306e\u3067\u3001\u4e00\u6642\u7684\u306b\u66f8\u304d\u76f4\u3057\u3066\u3044\u308b\u3002","f862b192":"\u4ee5\u4e0b\u306f\u4ea4\u5dee\u691c\u8a3c\u3067\u4f7f\u7528\u3057\u3066\u3044\u308b3\u3064\u306e\u30e2\u30c7\u30eb\u305d\u308c\u305e\u308c\u306e\u63a8\u6e2c\u7d50\u679c\u3092\u30b5\u30d6\u30df\u30c3\u30b7\u30e7\u30f3\u7528\u306bCSV\u306b\u307e\u3068\u3081\u308b\u30b3\u30fc\u30c9\u3002\u6094\u3057\u3044\u7d50\u679c\u306b\u7d42\u308f\u3063\u305f\u3068\u304d\u306b\u306f\u3001\u3053\u306e3\u3064\u306e\u4e2d\u306b\u3088\u308a\u7cbe\u5ea6\u304c\u9ad8\u3044\u7d50\u679c\u304c\u3042\u308b\u306e\u3067\u306f\u3068\u30b5\u30d6\u30df\u30c3\u30c8\u3059\u308b\u3093\u3060\u3051\u3069\u3001\u305d\u308c\u3084\u308b\u30681\u65e5\u306b\u6319\u3052\u3089\u308c\u308b\u30b5\u30d6\u30df\u30c3\u30b7\u30e7\u30f3\u306e\u4e0a\u9650\u306b\u3059\u3050\u9054\u3057\u3066\u3057\u307e\u3046\u306e\u3067\u3084\u3081\u305f\u65b9\u304c\u3088\u3044\u3002\u51fa\u3055\u305a\u306b\u6e08\u3093\u3060w","21bdaf31":"`DNN2`\u3068\u540c\u69d8\u3001`LinearRegression`\u3092\u30c1\u30e5\u30fc\u30f3\u3059\u308b\u30b3\u30fc\u30c9","caef0bb5":"\u5c11\u306a\u304f\u3068\u3082\u691c\u8a3c\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u7cbe\u5ea6\u306f\u4e0a\u304c\u3063\u3066\u3044\u308b\u3002\n\n\u30c1\u30e5\u30fc\u30f3\u306b\u3088\u308a\u6700\u9069\u5316\u3055\u308c\u305f\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u6301\u3064\u30e2\u30c7\u30eb\uff083\u500b\uff09\u3092\u4f7f\u3044\u3001\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u304b\u3089\u751f\u6b7b\u3092\u63a8\u6e2c\u3059\u308b\u30b3\u30fc\u30c9\u3002","5279a6a8":"[Ray Tune\u306e\u30b5\u30f3\u30d7\u30eb](https:\/\/docs.ray.io\/en\/latest\/tune\/tutorials\/tune-pytorch-cifar.html)\u3092\u53c2\u8003\u306b\u8a18\u8ff0\u3057\u305f`DNN1`\u306e\u5168\u7d50\u5408\u5c64\u306e\u30ce\u30fc\u30c9\u6570\u3001learning rate\u3001batch_size\u3092\u30c1\u30e5\u30fc\u30f3\u3059\u308b\u305f\u3081\u306e\u95a2\u6570\u3002\u3053\u308c\u3092\u30b3\u30d4\u30da\u3057\u3066\u3001`DNN2`\u3068`LinearRegression`\u306e\u30c1\u30e5\u30fc\u30f3\u3092\u5b9f\u884c\u3059\u308b\u30b3\u30fc\u30c9\u3082\u66f8\u3044\u305f\u3002\n\n\u3068\u3053\u308d\u3069\u3053\u308d\u306b\u898b\u3089\u308c\u308b`config[\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u540d]`\u304c\u30c1\u30e5\u30fc\u30f3\u306e\u5bfe\u8c61\u3002`config['l1']`\u306a\u3089\u3001\u5168\u7d50\u5408\u5c64\u306e\u30ce\u30fc\u30c9\u6570\u3092\u5dee\u3057\u66ff\u3048\u306a\u304c\u3089\u3001\u5b66\u7fd2\u3084\u691c\u8a3c\u3092\u884c\u3044\u3001\u305d\u308c\u3092Ray Tune\u306b\u5831\u544a\u3059\u308b\u3053\u3068\u3067\u3001Ray Tune\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u306e\u6700\u9069\u5316\u3092\u884c\u3063\u3066\u304f\u308c\u308b\u3002\n\u524d\u534a\u306f\u901a\u5e38\u306e\u5b66\u7fd2\u3068\u540c\u69d8\u306a\u3053\u3068\u3092\u884c\u3044\u3001\u5f8c\u534a\u3067\u30e2\u30c7\u30eb\u306e\u691c\u8a3c\u3068\u691c\u8a3c\u6642\u306e\u640d\u5931\u306e\u8a08\u7b97\u3001Ray Tune\u3078\u306e\u5831\u544a\u306a\u3069\u3092\u884c\u3063\u3066\u3044\u308b\u3002","87b21444":"\u524d\u56de\u306b\u7279\u5fb4\u91cf\u30a8\u30f3\u30b8\u30cb\u30a2\u30ea\u30f3\u30b0\u306e\u307e\u306d\u3054\u3068\u3092\u3057\u3066\u5f97\u3089\u308c\u305f\u3082\u306e\u3092CSV\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u3057\u3066\u3001\u305d\u308c\u3092\u8aad\u307f\u8fbc\u3080\u3088\u3046\u306b\u3057\u305f\u3002","dc4785fd":"\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u306e\u30c1\u30e5\u30fc\u30f3\u3092\u3057\u306a\u3044\u307e\u307e\u3001\u3068\u308a\u3042\u3048\u305a\u3001\u30b5\u30d6\u30df\u30c3\u30b7\u30e7\u30f3\u3092\u4f5c\u6210\u3057\u3066\u63d0\u51fa\u3057\u3066\u304a\u304f\u3002","9eea1b96":"\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u524d\u306e\u30b5\u30d6\u30df\u30c3\u30b7\u30e7\u30f3\u306e\u30b9\u30b3\u30a2\u306f0.77990\u3002\u3053\u308c\u304c\u5411\u4e0a\u3059\u308b\u304b\u3069\u3046\u304b\u3002","456f5d51":"\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u57fa\u306b\u3057\u305f\u63a8\u6e2c\u7d50\u679c\u3092\u30b5\u30d6\u30df\u30c3\u30b7\u30e7\u30f3\u306b\u307e\u3068\u3081\u308b\u3002\u4ea4\u5dee\u691c\u8a3c\u3092\u884c\u3063\u305f\u3068\u304d\u306e\u30b9\u30b3\u30a2\u306f0.78229\u3002\u3053\u308c\u307e\u3067\u306bk-fold\u4ea4\u5dee\u691c\u8a3c\u3067\u306f\u51fa\u3066\u3044\u306a\u304b\u3063\u305f\u6700\u9ad8\u30b9\u30b3\u30a2\uff01","921c3517":"`DNN1`\u306e\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u30c1\u30e5\u30fc\u30f3\u3068\u540c\u69d8\u306b2\u3064\u306e\u95a2\u6570\u3067`DNN2`\u306e\u30c1\u30e5\u30fc\u30f3\u3092\u884c\u3046\u3002\n\n\u5408\u8a086\u500b\u3042\u308b\u95a2\u6570\u306f\u4f55\u3089\u304b\u306e\u65b9\u6cd5\u30672\u3064\u306b\u307e\u3068\u3081\u3089\u308c\u305d\u3046\u3060\u3051\u3069\u3001\u305d\u3053\u307e\u3067\u306f\u624b\u304c\u56de\u3089\u305a\u3002","c0915487":"\u4e0a\u8a18\u95a2\u6570\u3092\u5b9f\u884c\u3059\u308b\u305f\u3081\u306e\u30b3\u30fc\u30c9\u3002\u3053\u306e\u95a2\u6570\u5185\u3067\u3001`config`\u306b\u30c1\u30e5\u30fc\u30f3\u5bfe\u8c61\u306e\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u6307\u5b9a\u3059\u308b\u3002\n\u3067\u3001`tune.run`\u3092\u547c\u3073\u51fa\u3059\uff08\u540d\u524d\u4ed8\u304d\u5f15\u6570\u306f\u5b9f\u884c\u6642\u306e\u632f\u308b\u821e\u3044\u3092\u6307\u5b9a\u3059\u308b\u3002\u4f8b\u3048\u3070\u3001`verbose=1`\u306f\u6700\u5c0f\u9650\u306e\u9032\u6357\u60c5\u5831\u3092\u8868\u793a\u3059\u308b\u306a\u3069\uff09\u3002\n\n\u623b\u308a\u5024`result`\u304b\u3089`best_trial`\u3092\u53d6\u308a\u51fa\u3057\u3066\u3001\u6700\u826f\u306e\u7d50\u679c\u306b\u95a2\u3059\u308b\u60c5\u5831\u3092\u8868\u793a\u3057\u305f\u308a\u3001\u305d\u3053\u304b\u3089\u30e2\u30c7\u30eb\u3092\u518d\u751f\u3057\u3066\u4fdd\u5b58\u3057\u305f\u308a\u3057\u3066\u3044\u308b\u3002"}}