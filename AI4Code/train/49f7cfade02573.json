{"cell_type":{"301cf58e":"code","1ebb6379":"code","4fe0c2ec":"code","d8034834":"code","da54e76c":"code","f9337c2b":"code","cecd00a1":"code","891a4639":"code","255e831f":"code","79fce58d":"code","d34f0bac":"code","ccc32395":"code","ed935383":"code","94ba44df":"code","e6c00ab1":"code","f437d09f":"code","6e3331a6":"code","0cd19b45":"code","fa2d24a8":"code","04a97132":"code","8b81cbad":"code","b7236a16":"code","bac87c39":"code","d1bb6470":"code","f2ef2909":"code","e4bbc01c":"code","bd1c3d71":"code","ab78f1dd":"code","bde8892b":"code","01321cbe":"code","68bf5917":"code","d8a17edd":"code","34927c07":"code","09c1d212":"code","c96efc80":"code","45320a5b":"code","0eb8d366":"code","700cd03d":"code","348a4c1c":"code","08391e94":"code","8bea4d1e":"code","b41ad800":"code","48293a20":"code","73b454d4":"code","1169c716":"code","42d634c1":"code","e582614a":"markdown","51d979cd":"markdown","2a814399":"markdown","2b49f14d":"markdown","59c83a7e":"markdown","8d1cdf64":"markdown","fb2054bd":"markdown","394fd424":"markdown","b3c96330":"markdown","66979fb2":"markdown","68b85398":"markdown","ec7a3c23":"markdown","91277d8f":"markdown"},"source":{"301cf58e":"import os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n","1ebb6379":"train=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","4fe0c2ec":"train.info()","d8034834":"print(train.isnull().sum())\nfor col in train.columns:\n    pct_missing = np.mean(train[col].isnull())\n    print('{} - {}%'.format(col, round(pct_missing*100)))","da54e76c":"test.isnull().sum()\n","f9337c2b":"test.info()","cecd00a1":"# creating an additional column to distinguish between the datasets after concating\ntrain[\"train\/test\"]= \"train\"\ntest[\"train\/test\"]= \"test\"\n\n# concating\ncombined= pd.concat([train, test], axis=0)\nprint(\"combined data shape :\", combined.shape)\ncombined.head()","891a4639":"combined.describe()","255e831f":"import matplotlib.pyplot as plt\nimport seaborn as sns\n#get correlations of each features in dataset\ncorrmat = combined.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(combined[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","79fce58d":"\nprint(combined.isnull().sum())\nfor col in combined.columns:\n    pct_missing = np.mean(combined[col].isnull())\n    print('{} - {}%'.format(col, round(pct_missing*100)))","d34f0bac":"combined=combined.drop([\"Alley\",\"FireplaceQu\",\"PoolQC\",\"Fence\",\"MiscFeature\"],axis=1)","ccc32395":"#check again how many missing values left in data frame\n\nprint(combined.isnull().sum())\nfor col in combined.columns:\n    pct_missing = np.mean(combined[col].isnull())\n    print('{} - {}%'.format(col, round(pct_missing*100)))","ed935383":"# Treating LotFrontage column for missing values with mean\ncombined.loc[:, 'LotFrontage']\ncombined.LotFrontage.mean()\n# replacing null values with mean\ncombined.LotFrontage = combined.LotFrontage.fillna(combined.LotFrontage.mean())","94ba44df":"#To show all columns \npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', None)","e6c00ab1":"combined.isna().sum()","f437d09f":"#Dropping ID column and creating dummies for Categorical variables\ncombined.drop(['Id'], axis=1, inplace=True)\ncombined = pd.get_dummies(combined,drop_first=True)\n","6e3331a6":"na_col =combined.columns\nna_col=na_col.drop(\"SalePrice\")\nfor element in na_col:\n    combined[element] = combined[element].fillna(combined[element].mean())","0cd19b45":"#check again how many missing values left in data frame\n\nprint(combined.isnull().sum())\nfor col in combined.columns:\n    pct_missing = np.mean(combined[col].isnull())\n    print('{} - {}%'.format(col, round(pct_missing*100)))","fa2d24a8":"#values of correlation\nabs(combined.corr()['SalePrice']).nlargest(10)\n","04a97132":"#you can see the distribution by plotting it\n\n\n\nax1=sns.displot(combined['SalePrice']);\nfig_saleprice = plt.plot(figsize=(12,5))\n\n\nax2=sns.displot(combined['GrLivArea']);\nfig_GrLivArea = plt.figure(figsize=(12,5))\n\n\nax3=sns.displot(combined['OverallQual']);\nfig_OverallQual = plt.figure(figsize=(12,5))\n\nax4=sns.displot(combined['TotalBsmtSF']);\nfig_TotalBsmtSF = plt.figure(figsize=(12,5))","8b81cbad":"#then apply this and again do plotting you will see the difference\ncombined['SalePrice'] = np.sqrt(combined['SalePrice'] )\ncombined['GrLivArea'] = np.sqrt(combined['GrLivArea'] )\ncombined['OverallQual'] = np.sqrt(combined['OverallQual'] )\n#combined['TotalBsmtSF'] = np.sqrt(combined['TotalBsmtSF'] )","b7236a16":"#you can see the distribution by plotting it\n\n\n\n#ax1=sns.displot(combined['SalePrice']);\n#fig_saleprice = plt.plot(figsize=(12,5))\n\n\nax2=sns.displot(combined['GrLivArea']);\nfig_GrLivArea = plt.figure(figsize=(12,5))\n\n\nax3=sns.displot(combined['OverallQual']);\nfig_OverallQual = plt.figure(figsize=(12,5))\n\nax4=sns.displot(combined['TotalBsmtSF']);\nfig_TotalBsmtSF = plt.figure(figsize=(12,5))","bac87c39":"Target=combined['SalePrice']\ncombined=combined.drop(['SalePrice'], axis=1)\nTarget.isna().sum()\nTarget=Target.dropna(axis=0)","d1bb6470":"combined.info()","f2ef2909":"#Convert object to categorical variables\n\ncombined.loc[:, combined.dtypes == 'object'] =\\\n    combined.select_dtypes(['object'])\\\n    .apply(lambda x: x.astype('category'))\n","e4bbc01c":"correlated_features = set()\ncorrelation_matrix = combined.corr()\nfor i in range(len(correlation_matrix .columns)):\n    for j in range(i):\n        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n            colname = correlation_matrix.columns[i]\n            correlated_features.add(colname)\nlen(correlated_features)\nprint(correlated_features)\ncombined.drop(labels=correlated_features, axis=1, inplace=True)\n\n            ","bd1c3d71":"combined.columns","ab78f1dd":"combined.head(2)","bde8892b":"train_data= combined[combined[\"train\/test_train\"] == 1]\ntrain_data_copy=train_data.copy()\ntest_data= combined[combined[\"train\/test_train\"] == 0]","01321cbe":"test_data.shape","68bf5917":"train_data.shape","d8a17edd":"print(Target.isnull().sum())","34927c07":"train_data.head(2)","09c1d212":"#All features in a list for later use\nfeatures = train_data.columns","c96efc80":"from sklearn.preprocessing import StandardScaler\nscalar = StandardScaler()\ntrain_data = scalar.fit_transform(train_data)\ntest_data = scalar.fit_transform(test_data)\n\n","45320a5b":"import xgboost\nfrom sklearn.metrics import mean_squared_error\nfrom mlxtend.regressor import StackingCVRegressor\nimport xgboost\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import GradientBoostingRegressor\n#xgboost regressor\nxgboost = xgboost.XGBRegressor(learning_rate=0.05,\n                               colsample_bytree=0.5,\n                               subsample=0.8,\n                               n_estimators=700,\n                               max_depth=5,\n                               gamma=5)\nxgboost.fit(train_data,Target)\ny_pred1 = xgboost.predict(test_data)","0eb8d366":"#Random forest regressor\nrf = RandomForestRegressor(n_estimators=500, max_depth=2, criterion='mse', max_features='sqrt', bootstrap=False,\n                           n_jobs=-1, random_state=0, min_samples_leaf=200)\nrf.fit(train_data,Target)\ny_pred3 = rf.predict(test_data)","700cd03d":"#support vector\n#Fitting SVR to the dataset\nfrom sklearn.svm import SVR\nsvr_reg = SVR(kernel = 'rbf')\nsvr_reg.fit(train_data, Target)\ny_pred5 = svr_reg.predict(test_data)","348a4c1c":"#rmse of all models \ny_test=Target[:-1]\nfrom math import sqrt\nprint('xgb rmse', sqrt(mean_squared_error(y_test, y_pred1)))\nprint('rf rmse', sqrt(mean_squared_error( y_test,y_pred3)))\n#print('Linear rmse', sqrt(mean_squared_error(y_test, y_predL)))\nprint('svr rmse:', sqrt(mean_squared_error(y_test,y_pred5 )))\n","08391e94":"# linear regression feature importance\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\nfrom matplotlib import pyplot\n# define the model\nmodel = LinearRegression()\n# fit the model\nmodel.fit(train_data, Target)\n# get importance\nimportance = model.coef_\n# summarize feature importance\nfor i,v in enumerate(importance):\n    print('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\n#pyplot.bar([x for x in range(len(importance))], importance)\n#pyplot.show()","8bea4d1e":"pd.DataFrame({\"Feature\":features,\"Coefficients\":model.coef_})\n","b41ad800":"train_data_new= combined[combined[\"train\/test_train\"] == 1]\ntest_data_new= combined[combined[\"train\/test_train\"] == 0]\ntrain_data_new=train_data_new.drop([\"train\/test_train\"],axis=1)\ntest_data_new=test_data_new.drop([\"train\/test_train\"],axis=1)\n#get dummies for Test data and Train data \ntrain_data_new= pd.get_dummies(train_data_new,drop_first=True)\ntest_data_new= pd.get_dummies(test_data_new,drop_first=True)\ntest_final=test_data_new[[\"MSSubClass\",\"LotFrontage\",\"LotArea\",\"OverallQual\",\"OverallCond\",\"YearBuilt\",\"YearRemodAdd\",\"MasVnrArea\",\"BsmtFinSF1\",\"BsmtUnfSF\",\"TotalBsmtSF\",\"2ndFlrSF\",\"GrLivArea\",\"BedroomAbvGr\",\"Fireplaces\",\"GarageYrBlt\",\"GarageCars\",\"WoodDeckSF\",\"OpenPorchSF\",\"MoSold\",\"YrSold\"]]\ntrain_final=train_data_new[[\"MSSubClass\",\"LotFrontage\",\"LotArea\",\"OverallQual\",\"OverallCond\",\"YearBuilt\",\"YearRemodAdd\",\"MasVnrArea\",\"BsmtFinSF1\",\"BsmtUnfSF\",\"TotalBsmtSF\",\"2ndFlrSF\",\"GrLivArea\",\"BedroomAbvGr\",\"Fireplaces\",\"GarageYrBlt\",\"GarageCars\",\"WoodDeckSF\",\"OpenPorchSF\",\"MoSold\",\"YrSold\"]]","48293a20":"#Random forest regressor\nrf = RandomForestRegressor()\nrf.fit(train_final,Target)\ny_pred_new = rf.predict(test_final)\n\n#rmse\ny_test1=Target[:-1]\nfrom math import sqrt\nprint('xgb rmse', sqrt(mean_squared_error(y_test, y_pred_new)))\n# Performance metrics\nerrors = abs(y_pred_new - y_test)\nprint('Metrics for Random Forest Trained on Expanded Data')\nprint('Average absolute error:', round(np.mean(errors), 2), 'degrees.')\n# Calculate mean absolute percentage error (MAPE)\nmape = np.mean(100 * (errors \/ y_test))\n\n# Calculate and display accuracy\naccuracy = 100 - mape\nprint('Accuracy:', round(accuracy, 2), '%.')\n","73b454d4":"#support vector\n#Fitting SVR to the dataset\nfrom sklearn.svm import SVR\nsvr_reg = SVR(kernel = 'rbf')\nsvr_reg.fit(train_final, Target)\ny_pred_new1 = svr_reg.predict(test_final)\ny_test2=Target[:-1]\nfrom math import sqrt\nprint('xgb rmse', sqrt(mean_squared_error(y_test2, y_pred_new1)))\n# Performance metrics\nerrors = abs(y_pred_new1 - y_test2)\nprint('Metrics for Random Forest Trained on Expanded Data')\nprint('Average absolute error:', round(np.mean(errors), 2), 'degrees.')\n# Calculate mean absolute percentage error (MAPE)\nmape = np.mean(100 * (errors \/ y_test2))\n\n# Calculate and display accuracy\naccuracy = 100 - mape\nprint('Accuracy:', round(accuracy, 2), '%.')","1169c716":"## As we have logging of Saleprice , So lets generate original numbers again.\n# Add the exponential of the yhat column as a new column\npred_data=np.square(y_pred_new)\nprint(pred_data)","42d634c1":"# generate submission\noutput = pd.DataFrame({'Id': test.Id,\n                       'SalePrice': pred_data})\noutput.to_csv('submission_Final.csv', index=False)","e582614a":"## Now check how  many  missing values","51d979cd":"# So from above we have Top most important features,(Ran model Many times and take average features which have most impact on target variable) we will comapre both's final feature and Will make our model again and will compare our performane .","2a814399":"#### Checking and dropping if there are any correlated features","2b49f14d":"#### Treat missing values drop all those columns which have missing values > 50%.\n\n","59c83a7e":"Now we can data is now cleaned from Null value perspective . Leave SalePrice as this contain misisng values due to concat of test data","8d1cdf64":"### dimenstionality reduction using Different approaches .","fb2054bd":"# Combined data Concating both train and test dataset so that  preprocessing before giving it to model can be done simultaneously on both of them. ","394fd424":"## now we will be going to see the distribution using sns.distplot of Saleprice, GrLivArea,OverallQual,TotalBsmtSF, \n\n","b3c96330":"# overview of data","66979fb2":"### Applying different algorithm and checking their performance for final one model","68b85398":"##### So finalising SVM for this problem","ec7a3c23":"#### separation of data Test and Train","91277d8f":"#### Apply log transformation to fix skewness and Distribution issue so that error will be equal "}}