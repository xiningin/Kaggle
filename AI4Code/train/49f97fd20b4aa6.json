{"cell_type":{"550cb1a1":"code","008fce18":"code","646c1f19":"code","4aa4965f":"code","9457e10d":"code","99492949":"code","37ed2fa2":"code","543f0482":"code","28625581":"code","73d97dca":"code","e0031b46":"code","41c1a2de":"code","00c89fcd":"code","7c5fdd8f":"code","c2ebc442":"code","344d7d33":"code","ac71468e":"code","c29f54f9":"code","9a62b80e":"markdown","914aa0c2":"markdown","62c2f566":"markdown","91ae2ecb":"markdown","1994b9a9":"markdown","2c910019":"markdown","d688bf44":"markdown","9db475dc":"markdown","8d81cb3c":"markdown","9456662d":"markdown","411b6a41":"markdown","9076c9bf":"markdown","f3bec2c4":"markdown","253b934e":"markdown","0a7cb3fd":"markdown"},"source":{"550cb1a1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tensorflow import keras \nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport keras.backend as K","008fce18":"pd.read_excel(r\"..\/input\/female-anime-characters-anime-dataset\/Waifus.xlsx\")","646c1f19":"import os \nmain_path = r\"..\/input\/female-anime-characters-anime-dataset\/waifus_images\/waifus_images_train\"\npaths = os.listdir(main_path)\ndict_clases = {}\nfor path in paths:\n  dict_clases[path] = [len(os.listdir(os.path.join(main_path, path)))]\n\n\ndf_classes = pd.DataFrame(dict_clases)  \ndf_classes.head()","4aa4965f":"df_classes.plot.bar()","9457e10d":"pass","99492949":"import tensorflow as tf\nimport os\n\ndevice_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","37ed2fa2":"# define the path were the images are\ntrain_dir = r\"..\/input\/female-anime-characters-anime-dataset\/waifus_images\/waifus_images_train\"\nval_dir = r\"..\/input\/female-anime-characters-anime-dataset\/waifus_images\/waifus_images_val\"\nclass_names = os.listdir(main_path)\nclass_names","543f0482":"# define general variables\nIMG_SIZE = 224\nbatch_size = 32\nNUM_CLASSES = 7","28625581":"Train_Image_generator = ImageDataGenerator(\n    rotation_range = 40, # Degree range for random rotations.\n    width_shift_range=0.2,  # Randomly zoom inputs width in 20%.\n    height_shift_range=0.2, # Randomly zoom inputs height in 20%.\n    horizontal_flip = True, # Randomly flip inputs horizontally.\n    rescale=1.\/255    \n)\n\nTest_Image_generator = ImageDataGenerator(    \n    rescale=1.\/255\n)\n\n\ntrain_ds = Train_Image_generator.flow_from_directory(    \n    directory=train_dir, #main directory where the images are\n    target_size=(IMG_SIZE,IMG_SIZE), #desire size for the images\n    class_mode=\"categorical\",     \n    shuffle=True,\n    seed = 2)\n\ntest_ds = Test_Image_generator.flow_from_directory(    \n    directory=val_dir, #main directory where the images are\n    target_size=(IMG_SIZE,IMG_SIZE), #desire size for the images\n    class_mode=\"categorical\",     \n    shuffle=True,\n    seed = 2)","73d97dca":"from tensorflow.keras import applications\nfrom tensorflow.keras import layers\n\ndef build_model(num_classes):\n    \n    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))       \n    model = applications.EfficientNetB0(include_top=False, input_tensor=inputs, weights=\"imagenet\")\n\n    # Freeze the pretrained weights\n    model.trainable = False\n\n    # Rebuild top\n    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n    x = layers.BatchNormalization()(x)\n\n    top_dropout_rate = 0.2\n    x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n    outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\", name=\"pred\")(x)\n\n    # Compile\n    model = tf.keras.Model(inputs, outputs, name=\"EfficientNet\")   \n    \n    # We unfreeze the top 20 layers while leaving BatchNorm layers frozen\n    for layer in model.layers[-20:]:\n        if not isinstance(layer, layers.BatchNormalization):\n            layer.trainable = True\n    return model","e0031b46":"#with tf.device(r\"\/device:GPU:0\"):\nmodel = tf.keras.Sequential([     \n  layers.Conv2D(16,(3,3),input_shape=(IMG_SIZE,IMG_SIZE,3),activation=\"relu\"),    \n  layers.MaxPooling2D(),\n  layers.Conv2D(32, 3, activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(32, 3, activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Flatten(),\n  layers.Dense(128, activation='relu'),\n  layers.Dense(7, activation=\"softmax\")\n])\n\n\noptimizer = keras.optimizers.Adam(learning_rate=0.0001)\n\nmodel.compile(\n  optimizer= optimizer,\n  loss=\"categorical_crossentropy\",  \n  metrics=['accuracy'])\n\n","41c1a2de":"from tensorflow.keras.callbacks import EarlyStopping,  ReduceLROnPlateau, ModelCheckpoint\n\n\ncheck_point = r\".\/weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n\n\nmy_callbacks = [  \n  ReduceLROnPlateau(monitor=\"accuracy\",factor=0.1,patience=3,min_delta=0.01),\n  ModelCheckpoint(filepath=check_point,save_best_only=True),\n  tf.keras.callbacks.EarlyStopping(patience=5)\n]","00c89fcd":"history = model.fit(\n  train_ds,\n  validation_data=test_ds,\n  epochs=50,\n  callbacks= my_callbacks,  \n)","7c5fdd8f":"print(K.eval(model.optimizer.lr))","c2ebc442":"import matplotlib.pyplot as plt\n\n\n\nplt.plot(history.history[\"accuracy\"])\nplt.plot(history.history[\"val_accuracy\"])\nplt.title(\"model accuracy\")\nplt.ylabel(\"accuracy\")\nplt.xlabel(\"epoch\")\nplt.legend([\"train\", \"validation\"], loc=\"upper left\")\nplt.show()","344d7d33":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","ac71468e":"import itertools\ndef plot_confusion_matrix(cm, classes,\n                        normalize=False,\n                        title='Confusion matrix',\n                        cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n            horizontalalignment=\"center\",\n            color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","c29f54f9":"from sklearn.metrics import classification_report, confusion_matrix\nvalidation_generator = test_ds\nclasses = paths\n\nY_pred = model.predict_generator(validation_generator)\ny_pred = np.argmax(Y_pred, axis=1)\ncm = confusion_matrix(validation_generator.classes, y_pred)\nplot_confusion_matrix(cm, classes)\n                        \n                        \n                        \n","9a62b80e":"In the previous log console, you could see the training time of the model which is in average 170s per epoch. It is important to take into account that this model was trained using a GPU Nvidia Tesla k80 with 16gb of ram and a cpu intel xeon with 16 cores.\n\nIn the previous log console, you could see the training time of the model which is in average 170s per epoch. It is important to take into account that this model was trained using a GPU Nvidia Tesla k80 with 16gb of ram and a cpu intel xeon with 16 cores.\n\nIn the next cell you could see the loss and accuracy of the model after training.\n","914aa0c2":"Here we provide our [callbacks functions](https:\/\/keras.io\/api\/callbacks\/), wich are usefull functions to control our model when in training time.\n\n* The first one is ReduceLROnPlateau and is used to reduce the models learning rate when ther is no signifiaclly imporvemente in the selected metric after $n$ epochs.\n* The second one is ModelCheckpoint and is used to create check points of our model.\n\n","62c2f566":"In the next cell, we will provide data augmentation to our images and create and organize the data in a way that kereas can understand it. For the data augmentation purpose, we will use the module [Image data preprocessing](https:\/\/keras.io\/api\/preprocessing\/image\/#flowfromdataframe-method) . A useful module that allows us to create new images in base with the ones that we already have and control how to create the new one with parameters. ","91ae2ecb":"# Introduction:\n\nIn anime (japanese animation)  the main characters present personality archetypes that are represented in drawn features and characteristics. For example a character called \u201ctsundere\u201d are represented for being cold, reserved, temperamental and sometimes even may be hostile, but with time, she will turns in a really  warm, sweet, kind and tender person. The general appearance of these anime characters are: innocent looking, angry faces, upset expressions and usually they have long hair.\n\n![Sin%20t%C3%ADtulo2.png](attachment:Sin%20t%C3%ADtulo2.png)\n\nThe main objective of our project is to create a neural network that can classify a given image into their personality archetype. For example if we present the following image, the model should classify it as a tsundere.\n\n![Sin%20t%C3%ADtulo.jpg](attachment:Sin%20t%C3%ADtulo.jpg)\n\nFor that pourpuse, we divided the main goal in the followings tasks:\n* **Construction of waifus dere-type Database:** It is necessary to create a database with the main characters of every dere-type. \n* **Images Mining:** It is necessary to create our own images dataset because there is no one avalive of this topic on the internet. For that purpose we will mine 200 images for every waifu in the previously created database.\n* **Data augmentation:** There are not enough images of this topic available for mining. So it is necessary to increase the data set for other methods.\n* **Model construction:** It's necessary to create a model taking into account the small data set and the class imbalance presented in the dataset.","1994b9a9":"Finaly we train the model for 30 epochs.","2c910019":"As you can see in the table, there are classes with more representantes so they have more data.","d688bf44":"As you can see the model didnt achive the desired performance so to improve this we will try with another approach using transfer learning from the $x-net$.\n","9db475dc":"Examples of the data set:","8d81cb3c":"import some general libraries and configure the GPU.","9456662d":"## Data:\nThe data set is composed by 7 classes w*hich are Yandere, Deredere, Dandere, Himedere, Yangire, Tsundere and Kuudere.* As you can see in table 1 you can see every representative of each class. Each class has between 15 and 30 main character  and for each character there are 200 images for a total of 15.000 images. The data was divided in two sets, the train, and the validation set  with 80%  and 20% of the data respectively.  ","411b6a41":"In the next cell we will create the architecture for our model.\nwe defined a sequential model structure wich means that the layers are organized like a stack, one over the other:\n* The first layer is useed for resizing the size of the input image into a fixed size.\n* The second layer is used for normalize the input image into a range between 0 and 1. \n* The next 6 layers are a combination betwen Convolutional layers and Pooling Layers\n* The flaatten layer is used to reduce the dimension of the previus layer to 1 and this layer is conected with a fully conecte layer (Dense) with 128 units\n* The Final layer is a fully coneceted layer with $n$ units, one for every class and is the output for our model.","9076c9bf":"# Materials and methods\n","f3bec2c4":"if the previus cell is the models skelton, this part is the model muscles.\nFirst we used *[adam](http:\/\/https:\/\/arxiv.org\/abs\/1412.6980)* optimizer wich is one of the best optimizacion methods in machine learning.\nloss....\nFinally, the evaluation metrics is set to accuracy. A numerical value that provide use a general metric of the models peroformance.\n\n\n","253b934e":"# Algorithm","0a7cb3fd":"## Methods "}}