{"cell_type":{"baa42a90":"code","c0362efe":"code","2c094f21":"code","15056658":"code","0eefbd21":"code","c53ffbd0":"code","17eaa272":"code","ce9502bb":"code","3f56c5a1":"code","bd61a038":"code","f71933f5":"code","a6dbb1c6":"code","0eae6b3d":"code","9f880d1d":"code","53c6f8e4":"code","5b4154c7":"code","abb9be34":"code","8b1bea4f":"code","fcf5f4ec":"code","5ac20f7b":"code","2eedd1d0":"code","8b36710d":"code","95541cbd":"code","07074563":"code","d91adfcb":"code","40f58cc0":"code","a5c1820c":"code","932e2b95":"code","6a6f68d2":"code","fec54364":"code","fd3704ff":"code","09f0db1b":"code","4f752949":"code","ee518a67":"code","06456712":"code","0f7f6109":"code","b02814d4":"code","edaae4ce":"code","32da45eb":"code","de449917":"code","0bce9951":"code","a45f817c":"code","7da0490c":"code","03863d8d":"code","3d67038d":"code","ef275067":"code","e40e2aa5":"code","2e00d167":"code","89765782":"code","cc4b0514":"markdown","b79a5940":"markdown","bc41b15b":"markdown","ccfc567d":"markdown","54e64987":"markdown","6617be1a":"markdown","0cc2c170":"markdown","48e8002d":"markdown","873e911d":"markdown","ea90a9f3":"markdown","22e75180":"markdown","0e93ee2a":"markdown","ac45aa62":"markdown","12a4e83d":"markdown","713b56a5":"markdown","dd71d914":"markdown","dffad87c":"markdown","0a3828a0":"markdown","8302e01f":"markdown","258fd248":"markdown","2e0f7fe9":"markdown","5ecf5551":"markdown","b0f6beba":"markdown","7f46ae0c":"markdown","990d6566":"markdown","2534fef2":"markdown","26a3ed2f":"markdown","e4433f90":"markdown","64048695":"markdown","801870b3":"markdown","d4b0829d":"markdown"},"source":{"baa42a90":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom os import listdir\n","c0362efe":"# data must be downloaded and path provided\ndata_path = '..\/input\/db-sjsj\/mit-bih-arrhythmia-database-1.0.0\/'","2c094f21":"# list of patients\npts = ['100','101','102','103','104','105','106','107',\n       '108','109','111','112','113','114','115','116',\n       '117','118','119','121','122','123','124','200',\n       '201','202','203','205','207','208','209','210',\n       '212','213','214','215','217','219','220','221',\n       '222','223','228','230','231','232','233','234']","15056658":"import wfdb","0eefbd21":" print(wfdb.__version__)","c53ffbd0":"df = pd.DataFrame()\n\nfor pt in pts:\n    file = data_path + pt\n    annotation = wfdb.rdann(file, 'atr')\n    sym = annotation.symbol\n\n    values, counts = np.unique(sym, return_counts=True)\n    df_sub = pd.DataFrame({'sym': values, 'val': counts, 'pt': [pt] * len(counts)})\n    df = pd.concat([df, df_sub], axis=0)","17eaa272":"df.groupby('sym').val.sum().sort_values(ascending = False)","ce9502bb":"# list of nonbeat and abnormal\nnonbeat = ['[','!',']','x','(',')','p','t','u','`',\n           '\\'','^','|','~','+','s','T','*','D','=','\"','@','Q','?']\nabnormal = ['L','R','V','\/','A','f','F','j','a','E','J','e','S']","3f56c5a1":"# break into normal, abnormal or nonbeat\ndf['cat'] = -1\ndf.loc[df.sym == 'N','cat'] = 0\ndf.loc[df.sym.isin(abnormal), 'cat'] = 1","bd61a038":"df.groupby('cat').val.sum()","f71933f5":"def load_ecg(file):\n    # load the ecg\n    # example file: 'mit-bih-arrhythmia-database-1.0.0\/101'\n    \n    # load the ecg\n    record = wfdb.rdrecord(file)\n    # load the annotation\n    annotation = wfdb.rdann(file, 'atr')\n    \n    # extract the signal\n    p_signal = record.p_signal\n    \n    # verify frequency is 360\n    assert record.fs == 360, 'sample freq is not 360'\n    \n    # extract symbols and annotation index\n    atr_sym = annotation.symbol\n    atr_sample = annotation.sample\n    \n    return p_signal, atr_sym, atr_sample ","a6dbb1c6":"file = data_path + pts[0]","0eae6b3d":"p_signal, atr_sym, atr_sample = load_ecg(file)","9f880d1d":"values, counts = np.unique(sym, return_counts=True)\nfor v,c in zip(values, counts):\n    print(v,c)","53c6f8e4":"# get abnormal beat index\nab_index = [b for a,b in zip(atr_sym,atr_sample) if a in abnormal][:10]\nab_index","5b4154c7":"x = np.arange(len(p_signal))","abb9be34":"left = ab_index[1]-1080\nright = ab_index[1]+1080\n\nplt.plot(x[left:right],p_signal[left:right,0],'-',label='ecg',)\nplt.plot(x[atr_sample],p_signal[atr_sample,0],'go',label ='normal')\nplt.plot(x[ab_index],p_signal[ab_index,0],'ro',label='abnormal')\n\nplt.xlim(left,right)\nplt.ylim(p_signal[left:right].min()-0.05,p_signal[left:right,0].max()+0.05)\nplt.xlabel('time index')\nplt.ylabel('ECG signal')\nplt.legend(bbox_to_anchor = (1.04,1), loc = 'upper left')\nplt.show()","8b1bea4f":"def make_dataset(pts, num_sec, fs, abnormal):\n    # function for making dataset ignoring non-beats\n    # input:\n    # pts - list of patients\n    # num_sec = number of seconds to include before and after the beat\n    # fs = frequency\n    # output: \n    #   X_all = signal (nbeats , num_sec * fs columns)\n    #   Y_all = binary is abnormal (nbeats, 1)\n    #   sym_all = beat annotation symbol (nbeats,1)\n    \n    # initialize numpy arrays\n    num_cols = 2*num_sec * fs\n    X_all = np.zeros((1,num_cols))\n    Y_all = np.zeros((1,1))\n    sym_all = []\n    \n    # list to keep track of number of beats across patients\n    max_rows = []\n    \n    for pt in pts:\n        file = data_path + pt\n        \n        p_signal, atr_sym, atr_sample = load_ecg(file)\n        \n        # grab the first signal\n        p_signal = p_signal[:,0]\n        \n        # make df to exclude the nonbeats\n        df_ann = pd.DataFrame({'atr_sym':atr_sym,\n                              'atr_sample':atr_sample})\n        df_ann = df_ann.loc[df_ann.atr_sym.isin(abnormal + ['N'])]\n        \n        X,Y,sym = build_XY(p_signal,df_ann, num_cols, abnormal)\n        sym_all = sym_all+sym\n        max_rows.append(X.shape[0])\n        X_all = np.append(X_all,X,axis = 0)\n        Y_all = np.append(Y_all,Y,axis = 0)\n    # drop the first zero row\n    X_all = X_all[1:,:]\n    Y_all = Y_all[1:,:]\n    \n    # check sizes make sense\n    assert np.sum(max_rows) == X_all.shape[0], 'number of X, max_rows rows messed up'\n    assert Y_all.shape[0] == X_all.shape[0], 'number of X, Y rows messed up'\n    assert Y_all.shape[0] == len(sym_all), 'number of Y, sym rows messed up'\n\n    return X_all, Y_all, sym_all\n\n\n\ndef build_XY(p_signal, df_ann, num_cols, abnormal):\n    # this function builds the X,Y matrices for each beat\n    # it also returns the original symbols for Y\n    \n    num_rows = len(df_ann)\n\n    X = np.zeros((num_rows, num_cols))\n    Y = np.zeros((num_rows,1))\n    sym = []\n    \n    # keep track of rows\n    max_row = 0\n\n    for atr_sample, atr_sym in zip(df_ann.atr_sample.values,df_ann.atr_sym.values):\n\n        left = max([0,(atr_sample - num_sec*fs) ])\n        right = min([len(p_signal),(atr_sample + num_sec*fs) ])\n        x = p_signal[left: right]\n        if len(x) == num_cols:\n            X[max_row,:] = x\n            Y[max_row,:] = int(atr_sym in abnormal)\n            sym.append(atr_sym)\n            max_row += 1\n    X = X[:max_row,:]\n    Y = Y[:max_row,:]\n    return X,Y,sym\n    ","fcf5f4ec":"num_sec = 3\nfs = 360","5ac20f7b":"X_all, Y_all, sym_all = make_dataset(pts, num_sec, fs, abnormal)","2eedd1d0":"from sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_all, Y_all, test_size=0.33, random_state=42)","8b36710d":"from keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Dropout\nfrom keras.utils import to_categorical","95541cbd":"# build the same model\n# lets test out relu (a different activation function) and add drop out (for regularization)\nmodel = Sequential()\nmodel.add(Dense(32, activation = 'relu', input_dim = X_train.shape[1]))\nmodel.add(Dropout(rate = 0.25))\nmodel.add(Dense(1, activation = 'sigmoid'))","07074563":"# compile the model - use categorical crossentropy, and the adam optimizer\nmodel.compile(\n                loss = 'binary_crossentropy',\n                optimizer = 'adam',\n                metrics = ['accuracy'])","d91adfcb":"model.fit(X_train, y_train, batch_size = 32, epochs= 5, verbose = 1)","40f58cc0":"from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\ndef calc_prevalence(y_actual):\n    return (sum(y_actual)\/len(y_actual))\ndef calc_specificity(y_actual, y_pred, thresh):\n    # calculates specificity\n    return sum((y_pred < thresh) & (y_actual == 0)) \/sum(y_actual ==0)\ndef print_report(y_actual, y_pred, thresh):\n    \n    auc = roc_auc_score(y_actual, y_pred)\n    accuracy = accuracy_score(y_actual, (y_pred > thresh))\n    recall = recall_score(y_actual, (y_pred > thresh))\n    precision = precision_score(y_actual, (y_pred > thresh))\n    specificity = calc_specificity(y_actual, y_pred, thresh)\n    print('AUC:%.3f'%auc)\n    print('accuracy:%.3f'%accuracy)\n    print('recall:%.3f'%recall)\n    print('precision:%.3f'%precision)\n    print('specificity:%.3f'%specificity)\n    print('prevalence:%.3f'%calc_prevalence(y_actual))\n    print(' ')\n    return auc, accuracy, recall, precision, specificity","a5c1820c":"y_train_preds_dense = model.predict_proba(X_train,verbose = 1)\ny_valid_preds_dense = model.predict_proba(X_valid,verbose = 1)","932e2b95":"thresh = (sum(y_train)\/len(y_train))[0]\nthresh","6a6f68d2":"print('Train');\nprint_report(y_train, y_train_preds_dense, thresh)\nprint('Valid');\nprint_report(y_valid, y_valid_preds_dense, thresh);","fec54364":"import random\nrandom.seed( 42 )\npts_train = random.sample(pts, 36)\npts_valid = [pt for pt in pts if pt not in pts_train]\nprint(len(pts_train), len(pts_valid))","fd3704ff":"X_train, y_train, sym_train = make_dataset(pts_train, num_sec, fs, abnormal)\nX_valid, y_valid, sym_valid = make_dataset(pts_valid, num_sec, fs, abnormal)\nprint(X_train.shape, y_train.shape, len(sym_train))\nprint(X_valid.shape, y_valid.shape, len(sym_valid))","09f0db1b":"# build the same model\n# lets test out relu (a different activation function) and add drop out (for regularization)\nmodel = Sequential()\nmodel.add(Dense(32, activation = 'relu', input_dim = X_train.shape[1]))\nmodel.add(Dropout(rate = 0.25))\nmodel.add(Dense(1, activation = 'sigmoid'))\n\n# compile the model - use categorical crossentropy, and the adam optimizer\nmodel.compile(\n                loss = 'binary_crossentropy',\n                optimizer = 'adam',\n                metrics = ['accuracy'])\n\nmodel.fit(X_train, y_train, batch_size = 32, epochs= 5, verbose = 1)","4f752949":"y_train_preds_dense = model.predict_proba(X_train,verbose = 1)\ny_valid_preds_dense = model.predict_proba(X_valid,verbose = 1)","ee518a67":"thresh = (sum(y_train)\/len(y_train))[0]\nthresh","06456712":"print('Train');\nprint_report(y_train, y_train_preds_dense, thresh)\nprint('Valid');\nprint_report(y_valid, y_valid_preds_dense, thresh);","0f7f6109":"aucs_train = []\naucs_valid = []\n\nn_pts = [1,18,36]\nfor n_pt in n_pts:\n    \n    print(n_pt)\n    pts_sub = pts_train[:n_pt]\n    X_sub, y_sub, sym_sub = make_dataset(pts_sub, num_sec, fs,abnormal)\n\n    # build the same model\n    # lets test out relu (a different activation function) and add drop out (for regularization)\n    model = Sequential()\n    model.add(Dense(32, activation = 'relu', input_dim = X_train.shape[1]))\n    model.add(Dropout(rate = 0.25))\n    model.add(Dense(1, activation = 'sigmoid'))\n\n    # compile the model - use categorical crossentropy, and the adam optimizer\n    model.compile(\n                    loss = 'binary_crossentropy',\n                    optimizer = 'adam',\n                    metrics = ['accuracy'])\n\n    model.fit(X_sub, y_sub, batch_size = 32, epochs= 5, verbose = 0)\n    y_sub_preds_dense = model.predict_proba(X_sub,verbose = 0)\n    y_valid_preds_dense = model.predict_proba(X_valid,verbose = 0)\n    \n    auc_train = roc_auc_score(y_sub, y_sub_preds_dense)\n    auc_valid = roc_auc_score(y_valid, y_valid_preds_dense)\n    print('-',auc_train, auc_valid)\n    aucs_train.append(auc_train)\n    aucs_valid.append(auc_valid)","b02814d4":"plt.plot(n_pts, aucs_train, 'o-',label = 'Train')\nplt.plot(n_pts, aucs_valid, 'o-',label = 'Valid')\nplt.xlabel('Number Training Pts')\nplt.ylabel('AUC')\nplt.legend(bbox_to_anchor = (1.04,1), loc = 'upper left')\nplt.show()","edaae4ce":"# reshape input to be [samples, time steps, features = 1]\nX_train_cnn = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\nX_valid_cnn = np.reshape(X_valid, (X_valid.shape[0], X_valid.shape[1], 1))\n\nprint(X_train_cnn.shape)\nprint(X_valid_cnn.shape)\n","32da45eb":"from keras.layers import Conv1D","de449917":"model = Sequential()\nmodel.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu', input_shape = (2160,1)))\nmodel.add(Dropout(rate = 0.25))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation = 'sigmoid'))\n\n# compile the model - use categorical crossentropy, and the adam optimizer\nmodel.compile(\n                loss = 'binary_crossentropy',\n                optimizer = 'adam',\n                metrics = ['accuracy'])","0bce9951":"model.fit(X_train_cnn, y_train, batch_size = 32, epochs= 2, verbose = 1)","a45f817c":"y_train_preds_cnn = model.predict_proba(X_train_cnn,verbose = 1)\ny_valid_preds_cnn = model.predict_proba(X_valid_cnn,verbose = 1)","7da0490c":"print('Train');\nprint_report(y_train, y_train_preds_cnn, thresh)\nprint('Valid');\nprint_report(y_valid, y_valid_preds_cnn, thresh);","03863d8d":"from keras.layers import Bidirectional, LSTM","3d67038d":"model = Sequential()\nmodel.add(Bidirectional(LSTM(64, input_shape=(X_train_cnn.shape[1], X_train_cnn.shape[2]))))\nmodel.add(Dropout(rate = 0.25))\nmodel.add(Dense(1, activation = 'sigmoid'))\nmodel.compile(\n                loss = 'binary_crossentropy',\n                optimizer = 'adam',\n                metrics = ['accuracy'])","ef275067":"model.fit(X_train_cnn[:10000], y_train[:10000], batch_size = 32, epochs= 1, verbose = 1)","e40e2aa5":"y_train_preds_lstm = model.predict_proba(X_train_cnn[:10000],verbose = 1)\ny_valid_preds_lstm = model.predict_proba(X_valid_cnn,verbose = 1)","2e00d167":"print('Train');\nprint_report(y_train[:10000], y_train_preds_lstm, thresh)\nprint('Valid');\nprint_report(y_valid, y_valid_preds_lstm, thresh);","89765782":"from sklearn.metrics import roc_curve, roc_auc_score\n\n\nfpr_valid_cnn, tpr_valid_cnn, t_valid_cnn = roc_curve(y_valid, y_valid_preds_cnn)\nauc_valid_cnn = roc_auc_score(y_valid, y_valid_preds_cnn)\n\nfpr_valid_dense, tpr_valid_dense, t_valid_dense = roc_curve(y_valid, y_valid_preds_dense)\nauc_valid_dense = roc_auc_score(y_valid, y_valid_preds_dense)\n\nfpr_valid_lstm, tpr_valid_lstm, t_valid_lstm = roc_curve(y_valid, y_valid_preds_lstm)\nauc_valid_lstm = roc_auc_score(y_valid, y_valid_preds_lstm)\n\nplt.plot(fpr_valid_cnn, tpr_valid_cnn, 'g-', label = 'CNN AUC:%.3f'%auc_valid_cnn)\nplt.plot(fpr_valid_dense, tpr_valid_dense, 'r-', label = 'Dense AUC:%.3f'%auc_valid_dense)\nplt.plot(fpr_valid_lstm, tpr_valid_lstm, 'b-', label = 'LSTM AUC:%.3f'%auc_valid_lstm)\n\nplt.plot([0,1],[0,1], 'k--')\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.legend(bbox_to_anchor = (1.04,1), loc = 'upper left')\nplt.title('Validation Set')\nplt.show()","cc4b0514":"Validation score is much different now! Makes sense since we had data leakage before. ","b79a5940":"## CNN","bc41b15b":"We will use the MIH-BIH Arrythmia dataset from https:\/\/physionet.org\/content\/mitdb\/1.0.0\/. This is a dataset with 48 half-hour two-channel ECG recordings measured at 360 Hz. The recordings have annotations from cardiologists for each heart beat. The symbols for the annotations can be found at https:\/\/archive.physionet.org\/physiobank\/annotations.shtml","ccfc567d":"# Dataset","54e64987":"To simplify the problem, we will assume that a QRS detector is capable of automatically identifying the peak of each heart beat. We will ignore any non-beat annotations and any heart beats in the first or last 3 seconds of the recording due to reduced data. We will use a window of 6 seconds so we can compare the current beat to beats just before and after. This decision was based after talking to a physician who said it is easier to identify if you have something to compare it to. ","6617be1a":"We can try this again by splitting on patients instead of samples. ","0cc2c170":"Let's check out what abnormal beats are in a patient's ecg:","48e8002d":"# Make a dataset","873e911d":"# Project Definition","ea90a9f3":"Predict if a heart beat from the first ECG signal has an arrhythmia for each 6 second window centered on the peak of the heart beat. ","22e75180":"Reduce dataset to make this feasible for weekend project","0e93ee2a":"Now we are ready to build our first dense NN. We will do this in Keras for simplicity. ","ac45aa62":"Let's load all the annotations and see the distribution of heart beat types across all files. ","12a4e83d":"Imagine we naively just decided to randomly split our data by samples into a train and validation set. ","713b56a5":"Let's start by processing all of our patients.","dd71d914":"Let's make a plot of these, zooming in on one of the abnormal beats","dffad87c":"# Lesson 3: test multiple types of deep learning models","0a3828a0":"# Lesson 1: split on patients not on samples","8302e01f":"# Data Preparation","258fd248":"## LSTM","2e0f7fe9":"Let's start by making a CNN. Here we will use a 1 dimensional CNN (as opposed to the 2D CNN for images). ","5ecf5551":"Let's write a function for loading a single patient's signals and annotations. Note the annotation values are the indices of the signal array. ","b0f6beba":"# Introduction","7f46ae0c":"Let's make a dataset that is centered on beats with +- 3 seconds before and after. ","990d6566":"# Lesson 2: learning curve can tells us we should get more data! ","2534fef2":"Given the overfitting between training and validation. Let's make a simple learning curve to see if we should go collect more data. ","26a3ed2f":"Amazing! Not that hard! But wait, will this work on new patients? Perhaps not if each patient has a unique heart signature. Technically the same patient can show up in both the training and validation sets. This means that we may have accidentally leaked information across the datasets. ","e4433f90":"More data appears to add extra value to the model. ","64048695":"Here we will use a pypi package wfdb for loading the ecg and annotations.  ","801870b3":"A CNN is a special type of deep learning algorithm which uses a set of filters and the convolution operator to reduce the number of parameters. This algorithm sparked the state-of-the-art techniques for image classification. Essentially, the way this works for 1D CNN is to take a filter (kernel) of size `kernel_size` starting with the first time stamp. The convolution operator takes the filter and multiplies each element against the first `kernel_size` time steps. These products are then summed for the first cell in the next layer of the neural network. The filter then moves over by `stride` time steps and repeats. The default `stride` in Keras is 1, which we will use. In image classification, most people use `padding` which allows you pick up some features on the edges of the image by adding 'extra' cells, we will use the default padding which is 0. The output of the convolution is then multiplied by a set of weights W and added to a bias b and then passed through a non-linear activation function as in dense neural network. You can then repeat this with addition CNN layers if desired. Here we will use Dropout which is a technique for reducing overfitting by randomly removing some nodes. ","d4b0829d":"Recently, I was reviewing Andrew Ng's team's work(https:\/\/stanfordmlgroup.github.io\/projects\/ecg\/) on heart arrhythmia detector with convolutional neural networks (CNN). I found this quite fascinating especially with emergence of wearable products (e.g. Apple Watch and portable EKG machines) that are capable of monitoring your heart while at home. As such, I was curious how to build a machine learning algorithm that could detect abnormal heart beats. Here we will use an ECG signal (continuous electrical measurement of the heart) and train 3 neural networks to predict heart arrythmias: dense neural network, CNN, and LSTM.  "}}