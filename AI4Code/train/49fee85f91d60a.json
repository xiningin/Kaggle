{"cell_type":{"c2965e4d":"code","54980273":"code","b770b63a":"code","bc91209c":"code","55cca449":"code","145a8612":"code","4b4c86d7":"code","bdd71c80":"code","1e309b03":"code","aa73cfbe":"code","d45e2864":"code","f1430dfa":"code","93890693":"code","6d4e623e":"code","0da02e1b":"code","551658dc":"code","87be22e7":"code","f576cd16":"code","26983081":"code","acd50a06":"code","19fe01c0":"code","8233740d":"code","5d8bfd0d":"code","eec77aff":"code","8880a494":"code","e891ff16":"code","8fa7313f":"code","eb73040a":"code","725c9392":"code","a9281373":"code","00236f9f":"code","aaa188b4":"code","be828fdb":"code","5689e64f":"code","78902d5e":"code","ffab6279":"code","cd3a15d1":"code","6dc8696f":"markdown"},"source":{"c2965e4d":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport nltk\nfrom sklearn.model_selection import train_test_split\nfrom wordcloud import WordCloud, STOPWORDS \nimport operator\nfrom nltk.corpus import stopwords\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.decomposition import TruncatedSVD","54980273":"data = pd.read_csv('\/kaggle\/input\/quora-question-pairs\/train.csv.zip')","b770b63a":"data.head()","bc91209c":"print(\"Length of the dataset is {}\".format(len(data)))","55cca449":"data.describe()","145a8612":"data.info()","4b4c86d7":"positive_count = len(data[data['is_duplicate']==1])\nnegative_count = len(data[data['is_duplicate']==0])\n\nprint(\"total {} positive samples  \".format(positive_count))\nprint(\"total {} negative samples  \".format(negative_count))","bdd71c80":"fig = plt.figure(1,(10,10))\nlabels = ['Positive','Negative']\nplt.pie([positive_count,negative_count],labels= labels)\nplt.show()","1e309b03":"data.drop(['id','qid1','qid2'],inplace=True,axis = 1)","aa73cfbe":"data.tail()","d45e2864":"data = data.sample(frac=1).reset_index(drop=True)","f1430dfa":"data.head()","93890693":"train = data[:100000]","6d4e623e":"validation = data[100000:150000]","0da02e1b":"train.head()","551658dc":"def generate_word_cloud(data,max_words=100):\n    stopwords = set(STOPWORDS)\n    wordcloud = WordCloud(max_words=max_words,stopwords=stopwords).generate(str(data))\n    fig = plt.figure(1,(15,10))\n    plt.axis(\"off\")\n    plt.imshow(wordcloud)\n    plt.show()","87be22e7":"generate_word_cloud(train['question1']+train['question2'],max_words=400)","f576cd16":"def plot_length_feature(text,top_k_words = 10):\n    length_dict = {}\n    for i in range(len(text)):\n        split_words = str(text[i]).split()\n        for j in range(len(split_words)):\n            if split_words[j] not in set(stopwords.words('english')):\n                if not length_dict.get(len(split_words[j])):\n                    length_dict[len(split_words[j])]=1\n                else:\n                    length_dict[len(split_words[j])]+=1\n    \n    length_dict = sorted(length_dict.items(),key=operator.itemgetter(1),reverse=True)\n    lengths = [length for length,frequency in length_dict][:top_k_words]\n    freq = [frequency for length,frequency in length_dict][:top_k_words]\n    \n    \n    plt.figure(1,(9,7))\n    plt.title(\"Length v\/S Frequency for top {} lengths for words\".format(top_k_words))\n    plt.xlabel('Lengths')\n    plt.ylabel('Frequency')\n    plt.bar(lengths,freq,align='center', alpha=0.5)\n    plt.show()","26983081":"def clean_text(text):\n    text = str(text)\n    text = text.lower()\n    text = text.replace('\u201c','')\n    text = text.replace('\u201d','')\n    text = re.sub(\"hasn\u2019t\",\"has not\",text)\n    text = re.sub(\"can\u2019t\",\"can not\",text)\n    text = re.sub(\"wouldn\u2019t\",\"would not\",text)\n    text = re.sub(\"couldn\u2019t\",\"could not\",text)\n    text = re.sub(\"won\u2019t\",\"will not\",text)\n    text = re.sub(\"isn\u2019t\",\"is not\",text)\n    text = re.sub(\"i\u2019ll\",\"i will\",text)\n    text = re.sub(\"he\u2019ll\",\"he will\",text)\n    text = re.sub(\"she\u2019ll\",\"she will\",text)\n    text = re.sub(\"i\u2019m\",\"i am\",text)\n    text = re.sub(\"you\u2019ll\",\"you will\",text)\n    text = re.sub(\"hadn\u2019t\",\"had not\",text)\n    text = re.sub(\"don\u2019t\",\"do not\",text)\n    text = re.sub(\"here\u2019s\",\"here is\",text)\n    text = re.sub(\"where\u2019s\",\"where is\",text)\n    text = re.sub(\"that\u2019s\",\"that is\",text)\n    text = re.sub(\"it\u2019s\",\"it is\",text)\n    text = re.sub(\"he\u2019s\",\"he is\",text)\n    text = re.sub(\"she\u2019s\",\"she is\",text)\n    text = re.sub(\"what\u2019s\",\"what is\",text)\n    text = re.sub(\"i\u2019ve\",\"i have\",text)\n    text = re.sub(\"they\u2019re\",\"they are\",text)\n    text = re.sub(\"you\u2019re\",\"you are\",text)\n    text = re.sub(\"we\u2019d\",\"we would\",text)\n    text = re.sub(\"i\u2019d\",\"i would\",text)\n    text = re.sub(r'[^A-Za-z]',' ',text)\n    text = text.split()\n    text = [word for word in text if word not in set(stopwords.words('english'))]\n    text = ' '.join(text)\n    return text","acd50a06":"def clean_data(data):\n    for index,row in data.iterrows():\n        data.at[index,'question1'] = clean_text(row['question1'])\n        data.at[index,'question2'] = clean_text(row['question2'])\n        if index%10000==0:\n            print(index)\n    return data ","19fe01c0":"train = clean_data(train)","8233740d":"validation = clean_data(validation)","5d8bfd0d":"train.head()","eec77aff":"train['question_comb'] = train['question1'] +\" \"+train['question2']","8880a494":"validation['question_comb'] = validation['question1'] +\" \"+validation['question2']","e891ff16":"train.head()","8fa7313f":"validation.head()","eb73040a":"question_list = train['question_comb'].values\nvalid_question_list = validation['question_comb'].values\n\nvectorizer = TfidfVectorizer(max_features = 10000,stop_words = set(stopwords.words('english')))\nx_train = vectorizer.fit_transform(question_list)\nx_test = vectorizer.transform(valid_question_list)\ny_train = train['is_duplicate'] \ny_test = validation['is_duplicate'] ","725c9392":"logistic_model = LogisticRegression()\nlogistic_model.fit(x_train,y_train)\ny_predicted = logistic_model.predict(x_test)","a9281373":"print(accuracy_score(y_test,y_predicted))","00236f9f":"xgbclassifier = XGBClassifier()\nxgbclassifier.fit(x_train,y_train)\ny_predicted = xgbclassifier.predict(x_test)","aaa188b4":"print(accuracy_score(y_test,y_predicted))","be828fdb":"svd = TruncatedSVD(n_components = 1000)\nx_train_svd = svd.fit_transform(x_train)\nx_valid_svd = svd.transform(x_test)","5689e64f":"logistic_model = LogisticRegression()\nlogistic_model.fit(x_train_svd,y_train)\ny_predicted = logistic_model.predict(x_valid_svd)","78902d5e":"print(accuracy_score(y_test,y_predicted))","ffab6279":"xgbclassifier = XGBClassifier()\nxgbclassifier.fit(x_train_svd,y_train)\ny_predicted = xgbclassifier.predict(x_valid_svd)","cd3a15d1":"print(accuracy_score(y_test,y_predicted))","6dc8696f":"**Data Pre-Processing(for Baseline ML)**"}}