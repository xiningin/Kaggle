{"cell_type":{"90d95e0d":"code","d7872d33":"code","111270df":"code","701f2f7e":"code","d3e5ce14":"code","6240fd8f":"code","f67f5443":"code","969fc21c":"code","5947a2b8":"code","51299a81":"code","459e5e4d":"code","cafe6fe4":"code","637099e9":"code","28910a16":"code","871d0796":"code","302d4060":"code","1f6754a5":"code","e083ae6b":"code","59d9abd4":"code","c6c9fcb9":"code","c2b107c5":"code","29a7a234":"code","42e657b3":"code","7bfb189e":"code","ce2b6aaa":"markdown","6d948279":"markdown","cd1d62ee":"markdown","58a5d828":"markdown","97aa2fab":"markdown","b6f717d4":"markdown","954fe17b":"markdown","7293de6c":"markdown","67b86a60":"markdown","2f335136":"markdown","dafcc70b":"markdown","3b9cca94":"markdown","4968d641":"markdown","061e503a":"markdown","2f011b05":"markdown","e5ac01ba":"markdown","4ca222ca":"markdown","f32cdcdc":"markdown","e07710ce":"markdown","5e71ec1b":"markdown","f5385394":"markdown","d34481b4":"markdown","8f60fe13":"markdown","938d20de":"markdown","1a4ea7e1":"markdown","73869424":"markdown","59adf7c1":"markdown","005da30d":"markdown","6d7d1f0c":"markdown","bd63eea5":"markdown","738fcb04":"markdown","c4dc8faa":"markdown","c4f351ef":"markdown","b6658197":"markdown","ec00fecf":"markdown","e53c0424":"markdown","d75833c1":"markdown","1d6c9af7":"markdown","9e3a1140":"markdown","bef9a8b9":"markdown","03c400bd":"markdown","0c7d3f86":"markdown","111f244b":"markdown","6ba98734":"markdown","811666d5":"markdown","b39f2022":"markdown","1b297dd4":"markdown","71937b55":"markdown","86313264":"markdown","84841331":"markdown","f3a0f795":"markdown","902c87ab":"markdown","ceb2278a":"markdown","b8be66a1":"markdown","0b11fd2c":"markdown","651198d0":"markdown","7349ced1":"markdown","1a9f9c14":"markdown","28d51a25":"markdown","f73d24c5":"markdown","f676a275":"markdown","a93dc4b5":"markdown","06b36b96":"markdown","f44fc90b":"markdown","0976d9b1":"markdown","041dd5b8":"markdown","124dcfde":"markdown","f4e79ac1":"markdown","091bdcd1":"markdown"},"source":{"90d95e0d":"import matplotlib.pyplot as plt\nimport numpy as np\nimport warnings \nwarnings.filterwarnings('ignore')","d7872d33":"from sklearn.datasets import make_classification\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\n# Build a synthetic dataset\nX, y = make_classification(\n    n_samples=1000, n_features=5, n_informative=4, n_redundant=1, n_classes=4\n)\n\n# Train\/test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=1121218\n)\n\n# Fit\/predict\netc = ExtraTreesClassifier()\n_ = etc.fit(X_train, y_train)\ny_pred = etc.predict(X_test)\n\n# Plot confusion matrix\nfig, ax = plt.subplots(figsize=(8, 5))\ncmp = ConfusionMatrixDisplay(\n    confusion_matrix(y_test, y_pred),\n    display_labels=[\"class_1\", \"class_2\", \"class_3\", \"class_4\"],\n)\n\ncmp.plot(ax=ax)\nplt.show();","111270df":"from sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.multiclass import OneVsOneClassifier\nfrom sklearn.svm import SVC\n\n# Don't have to set `multi_class` argument if used with OVOClassifier\novo = OneVsOneClassifier(estimator=GaussianProcessClassifier())\n\novo.fit(X_train, y_train)","701f2f7e":"# Print the number of estimators created\nprint(len(ovo.estimators_))","d3e5ce14":"from sklearn.linear_model import Perceptron\nfrom sklearn.multiclass import OneVsRestClassifier\n\n# Init\/fit\novr = OneVsRestClassifier(estimator=Perceptron())\n_ = ovr.fit(X_train, y_train)","6240fd8f":"print(len(ovr.estimators_))","f67f5443":"import pandas as pd\n\ndiamonds = pd.read_csv(\"..\/input\/diamonds\/diamonds.csv\").drop(\"Unnamed: 0\", axis=1)\ndiamonds.head()","969fc21c":"diamonds.shape","5947a2b8":"diamonds.describe().T.round(3)","51299a81":"diamonds.cut.value_counts()","459e5e4d":"diamonds.hist(figsize=(16, 12));","cafe6fe4":"from sklearn.model_selection import train_test_split\n\n# Build feature\/target arrays\nX, y = diamonds.drop(\"cut\", axis=1), diamonds[\"cut\"].values.flatten()\n\n# Create train\/test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state=1121218, test_size=0.33, stratify=y\n)","637099e9":"from sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import OneHotEncoder, PowerTransformer, StandardScaler\n\n# Build categorical preprocessor\ncategorical_cols = X.select_dtypes(include=\"object\").columns.to_list()\ncategorical_pipe = make_pipeline(OneHotEncoder(sparse=False, handle_unknown=\"ignore\"))\n\n\n# Build numeric processor\nto_log = [\"price\", \"carat\"]\nto_scale = [\"x\", \"y\", \"z\", \"depth\", \"table\"]\nnumeric_pipe_1 = make_pipeline(PowerTransformer())\nnumeric_pipe_2 = make_pipeline(StandardScaler())\n\n# Full processor\nfull = ColumnTransformer(\n    transformers=[\n        (\"categorical\", categorical_pipe, categorical_cols),\n        (\"power_transform\", numeric_pipe_1, to_log),\n        (\"standardization\", numeric_pipe_2, to_scale),\n    ]\n)\n\n# Final pipeline combined with RandomForest\npipeline = Pipeline(\n    steps=[\n        (\"preprocess\", full),\n        (\n            \"base\",\n            RandomForestClassifier(max_depth=13),\n        ),\n    ]\n)","28910a16":"# Fit\n_ = pipeline.fit(X_train, y_train)","871d0796":"from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n\ny_pred = pipeline.predict(X_test)\n\n# Plot the confusion matrix\nfig, ax = plt.subplots(figsize=(12, 8))\n# Create the matrix\ncm = confusion_matrix(y_test, y_pred)\ncmp = ConfusionMatrixDisplay(cm, display_labels=pipeline.classes_)\ncmp.plot(ax=ax)\n\nplt.show();","302d4060":"from sklearn.metrics import roc_auc_score\n\n# Generate membership scores with .predict_proba\ny_pred_probs = pipeline.predict_proba(X_test)\n\n# Calculate ROC_AUC\nroc_auc_score(y_test, y_pred_probs, multi_class=\"ovr\", average=\"weighted\")","1f6754a5":"# GENERATE ROC_AUC SCORE FOR 'IDEAL' CLASS DIAMONDS\n\n# Find the index of Ideal class diamonds\nidx = np.where(pipeline.classes_ == \"Ideal\")[0][0]\n\n# Don't have to set multiclass and average params\nroc_auc_score(y_test == \"Ideal\", y_pred_probs[:, idx])","e083ae6b":"# Plot the confusion matrix\nfig, ax = plt.subplots(figsize=(12, 8))\n# Create the matrix\ncm = confusion_matrix(y_test, y_pred)\ncmp = ConfusionMatrixDisplay(cm, display_labels=pipeline.classes_)\ncmp.plot(ax=ax)\n\nplt.show();","59d9abd4":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred))","c6c9fcb9":"from sklearn.metrics import f1_score\n\n# Weighed F1 across all classes\nf1_score(y_test, y_pred, average=\"weighted\")","c2b107c5":"# F1 score for Ideal and Premium with weighted average\nf1_score(y_test, y_pred, labels=[\"Premium\", \"Ideal\"], average=\"weighted\")","29a7a234":"n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\nmax_features = [\"auto\", \"sqrt\"]\nmax_depth = [int(x) for x in np.linspace(10, 110, num=11)] + [None]\nmin_samples_split = [2, 5, 7, 10]\nmin_samples_leaf = [1, 2, 3, 4]\nbootstrap = [True, False]\n\nparam_grid = {\n    \"base__n_estimators\": n_estimators,\n    \"base__max_features\": max_features,\n    \"base__max_depth\": max_depth,\n    \"base__min_samples_split\": min_samples_split,\n    \"base__min_samples_leaf\": min_samples_leaf,\n    \"base__bootstrap\": bootstrap,\n}","42e657b3":"from sklearn.metrics import make_scorer\n\ncustom_f1 = make_scorer(f1_score, average=\"weighted\", labels=[\"Ideal\", \"Premium\"])\n\ncustom_f1","7bfb189e":"from sklearn.experimental import enable_halving_search_cv\nfrom sklearn.model_selection import HalvingGridSearchCV, HalvingRandomSearchCV\n\n\n# hgs = HalvingRandomSearchCV(\n#     estimator=pipeline,\n#     param_distributions=param_grid,\n#     scoring=custom_f1,\n#     cv=3,\n#     n_candidates=\"exhaust\",\n#     factor=5,\n#     n_jobs=-1,\n# )\n\n# _ = hgs.fit(X, y)\n\n# model = hgs.best_estimator_\n\n# _ = model.fit(X_train, y_train)\n# preds = model.predict(X_test)\n\n# f1_score(y_test, preds, average=\"weighted\", labels=[\"Ideal\", \"Premium\"])","ce2b6aaa":"For imbalanced classification tasks such as these, you rarely choose averaged precision, recall of F1 scores. Again, choosing one metric to optimize for a particular class depends on your business problem. For our case, we will choose to optimize the F1 score of Ideal and Premium classes (yes, you can choose multiple classes at the same time). First, let's see how to calculate weighted F1 across all class:","6d948279":"As we did in the last section, we passed the metric function along with custom values for `average` and `labels` parameters.\n\nFinally, let's initialize the HGS and fit it to the full data with 3-fold cross-validation:","cd1d62ee":"Sklearn also provides a wrapper estimator for the above models under `sklearn.multiclass.OneVsOneClassifier`:","58a5d828":"Before we feed the grid to HGS, let's create a custom scoring function. In binary case, we could pass string values as the names of the metrics we wanted to use such as 'precision' or 'recall'. But in multiclass case, those functions accept additional parameters and we cannot do that if we pass the function names as strings. To solve this, Sklearn provides `make_scorer` function:","97aa2fab":"Depending on the model you choose, Sklearn approaches multiclass classification problems in 3 different ways. In other words, Sklearn estimators are grouped into 3 categories by their strategy to deal with multi-class data.\n\nThe first and the biggest group of estimators are the ones that support multi-class classification natively:","b6f717d4":"The first version of our pipeline uses `RandomForestClassifier`. Let's look at its confusion matrix by generating predictions:","954fe17b":"You can check that our calculations for the Ideal class were correct. The last column of the table, `support` shows how many samples are there for each class. Also, last 2 rows shows averaged scores for the 3 metrics. We already covered what macro and weighted averages are in the example of ROC AUC.","7293de6c":"This section is only about the nitty-gritty details of how Sklearn calculates common metrics for multiclass classification. Specifically, we will peek under the hood of 4 most common metrics: ROC_AUC, precision, recall and f1 score. Even though I will give a brief overview of what each metric is, I will mostly focus on how you can use them in your own workflow. If you want a deeper explanation of what each metric measures, please refer to this [article](https:\/\/towardsdatascience.com\/multi-class-metrics-made-simple-part-i-precision-and-recall-9250280bddc2).","67b86a60":"### Hyperparameter tuning to optimize model performance for a custom metric","2f335136":"Your model is only as good as the metric you choose to evaluate it with. Hyperparameter tuning will be time-consuming but assuming you did everything right until this point and gave a good enough parameter grid, everything will turn out as expected. If not, it is an iterative process, so take your time by tweaking the preprocessing steps, take a second look at your chosen metrics and maybe widen your search grid. Thank you for reading!","dafcc70b":"### Binary classifiers that support mutliclass classification with One-vs-Rest (OVR) strategy","3b9cca94":"### How Sklearn computes multiclass classification metrics - ROC AUC score","4968d641":"Optimizing the model performance for some metric is almost the same as when we did for the binary case. The only difference is how we pass a scoring function to hyperparamter tuner like GridSearch. \n\nUp until now, we were using RandomForestClassifier pipeline, so we will create a hyperparameter grid for this estimator:","061e503a":"- [Classification Metrics](https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#classification-metrics)\n- [Multiclass and multioutput algorithms](https:\/\/scikit-learn.org\/stable\/modules\/multiclass.html)","2f011b05":"Even though multi-class classification is not as common, it certainly poses a much bigger challenge than binary classification problems. You can literally take my word for it because this article has been the most challenging post I have ever written (have written close to 70).\n\nI found that the topic of multiclass classification is deep and full of nuances. I have read so many articles, read multiple StackOverflow threads, created a few of my own, and spent several hours exploring the Sklearn user guide and doing experiments. The core topics of multiclass classification such as\u00a0\n\n- choosing a strategy to binarize the problem\n- choosing a base mode\n- understanding excruciatingly many metrics\u00a0\n- filtering out a single metric that solves your business problem and customizing it\n- tuning hyperparameters for this custom metric\n- and finally putting all the theory into practice with Sklearn\n\nhave all been scattered in the dark, sordid corners of the Internet. This was enough to conclude that no single resource shows an end-to-end workflow of dealing with multiclass classification problems on the Internet (maybe, I missed it).\n\nFor this reason, this article will be a comprehensive tutorial on how to solve any multiclass supervised classification problem using Sklearn. You will learn both the theory and the implementation of the above core concepts. It is going to be a long and technical read, so get a coffee!","e5ac01ba":"In multiclass case, these 3 metrics are calculated *per-class* basis. For example, let's look at the confusion matrix again:","4ca222ca":"We will focus on multiclass confusion matrices later in the tutorial.","f32cdcdc":"Price and carat shows skewed distributions. We will use a logarithmic transformer to make them as normally-distributed as possible. For the rest, simple standardization is enough. If you are not familiar with numeric transformations, check out my [article](https:\/\/towardsdatascience.com\/how-to-differentiate-between-scaling-normalization-and-log-transformations-69873d365a94?source=your_stories_page-------------------------------------) on the topic. Also, below code contains an example of Sklearn pipelines and you can learn all about them from [here](https:\/\/towardsdatascience.com\/how-to-use-sklearn-pipelines-for-ridiculously-neat-code-a61ab66ca90d?source=your_stories_page-------------------------------------). Let's get to work:","e07710ce":"- Classifier 1: lung vs. \\[breast, kidney, brain\\]\u200a-\u200a(lung cancer, not lung cancer)\n- Classifier 2: breast vs. \\[lung, kidney, brain\\]\u200a-\u200a(breast cancer, not breast cancer)\n- Classifier 3: kidney vs. \\[lung, breast, brain\\]\u200a-\u200a (kidney cancer, not kidney cancer)\n- Classifier 4: brain vs. \\[lung, breast kidney\\]\u200a-\u200a(brain cancer, not brain cancer)","5e71ec1b":"Sklearn suggests these classifiers to work best with OVO approach:","f5385394":"### Introduction","d34481b4":"Even though this strategy significantly lowers the computational cost, the fact that only one class is considered positive and the rest as negative makes each binary problem an imbalanced classification. This problem is even more pronounced for classes with low proportions in the target.","8f60fe13":"Above, we calculated ROC AUC for our diamond classification problem and got a very good score. Don't forget to set the `multi_class` and `average` parameters properly when using `roc_auc_score`. If you want to to generate the score for a particular class, here is how you do it:","938d20de":"In both approaches, depending on the passed estimator, the results of all binary classifiers can be summarized in two ways:\n- majority of the vote: each binary classifier predicts one class and the class that got the most votes from all classifiers is chosen\n- depending on the argmax of class membership probability scores: classifiers such as LogisticRegression computes probability scores for each class (`.predict_proba()`). Then, the argmax of the sum of the scores is chosen.","1a4ea7e1":"Alternatively, the OVR strategy creates an individual classifier for each class in the target. Essentially, each binary classifier chooses a single class and marks it as positive, encoding it as 1. The rest of the classes are considered negative labels and, thus, encoded with 0. For classifying 4 types of cancer:","73869424":"As an example, let's say there are 100 samples in the target - class 1 (45), class 2 (30), class 3 (25). OVR creates 3 binary classifiers, 1 for each class and their ROC AUC scores are 0.75, 0.68, 0.84, respectively. The weighted ROC AUC score across all classes will be:\n\n**ROC AUC (weighted): ((45 \\* 0.75) + (30 \\* 0.68) + (25 \\* 0.84)) \/ 100 = 0.7515**","59adf7c1":"Recall is calculated in a similar manner. We know the number of true positives - 6626. False negatives would be any cells that count the number of times the classifier predicted Ideal type of diamonds belonging to any other negative class. These would be the cells right and left to the center of the matrix (3 + 9 + 363 + 111 = 486). Using the formula of recall, we calculate it to be:\n\n**Recall (Ideal) = TP \/ (TP + FN) = 6626 \/ (6626 + 486) = 0.93**","005da30d":"Up to this point, we calculated the 3 metrics only for the Ideal class. But in multiclass classification, Sklearn computes them for all classes. You can use `classification_report` to calculate these metrics for all classes:","6d7d1f0c":"Finally, let's see how to optimize these metrics with hyperparameter tuning.","bd63eea5":"### Binary classifiers that support mutliclass classification with One-vs-One (OVO) strategy","738fcb04":"### Estimators that support mutliclass classification natively","c4dc8faa":"The above output shows the features are on different scales, suggesting we use some type of normalization. This step is essential for many linear-based models to perform well.","c4f351ef":"# Ultimate Guide to Multiclass Classification With Sklearn\n## Model selection, developing strategy and evaluation metrics\n![](https:\/\/cdn-images-1.medium.com\/max\/1200\/1*vlfX6cRLxZKhn4uN4MFHrg.jpeg)\n<figcaption style=\"text-align: center;\">\n    <strong>\n        Photo by \n        <a href='https:\/\/www.pexels.com\/@sergiu-iacob-10475786?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels'>Sergiu Iacob<\/a>\n        on \n        <a href='https:\/\/www.pexels.com\/photo\/wave-dark-abstract-motion-7868341\/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels'>Pexels<\/a>\n    <\/strong>\n<\/figcaption>","b6658197":"- [svm.NuSVC](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC)\n- [svm.SVC](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html#sklearn.svm.SVC)\n- [gaussian_process.GaussianProcessClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.GaussianProcessClassifier.html#sklearn.gaussian_process.GaussianProcessClassifier) (setting multi_class = \u201cone_vs_one\u201d)","ec00fecf":"Precision tells us what proportion of predicted positives is truly positive. If we want to calculate precision for Ideal diamonds, true positives would be the number of Ideal diamonds predicted correctly (the center of the matrix, 6626). False positives would be any cells that count the number of times our classifier predicted other type of diamonds as Ideal. These would be the cells above and below the center of the matrix (1013 + 521 + 31 + 8 = 1573). Using the formula of precision, we calculate it to be:\n\n**Precision (Ideal) = TP \/ (TP + FP) = 6626 \/ (6626 + 1573) = 0.808**","e53c0424":"In practice, One-vs-Rest strategy is much preferred because of this disadvantage.","d75833c1":"Always list out the terms of your matrix in this manner and the rest of your workflow will be much easier, as you will see in the next section.","1d6c9af7":"1. Each binary classifier created using OVR finds the ROC AUC score for its own class using the above steps.\n2. ROC AUC scores of all classifiers is then averaged using either of these 2 methods:\n - \"macro\": this is simple the arithmetic mean of the scores\n - \"weighted\": this takes class imbalance into account by finding a weighted average. Each ROC AUC is multiplied by their class weight and summed, then divided by the total number of samples.","9e3a1140":"So, the F1 score for the Ideal class would be:\n\n**F1 (Ideal) = 2 * (0.808 * 0.93) \/ (0.808 + 0.93) = 0.87**","bef9a8b9":"In lines 8 and 9, we are creating the matrix and using a special Sklearn function to plot it. `ConfusionMatrixDisplay` also has `display_labels` argument, which we are passing the class names accessed by `pipeline.classes_` attribute.","03c400bd":"ROC AUC score is only a good metric to see how the classifier differentiates between classes. A higher ROC AUC score does not necessarily mean a better model. On top of that, we care more about our model's ability to classify Ideal and Premium diamonds, so a metric like ROC AUC is not a good option for our case.","0c7d3f86":"Sklearn suggests these classifiers to work best with OVR approach:\n\n- [`ensemble.GradientBoostingClassifier`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier)\n- [`gaussian_process.GaussianProcessClassifier`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.GaussianProcessClassifier.html#sklearn.gaussian_process.GaussianProcessClassifier) (setting multi_class = \u201cone_vs_rest\u201d)\n- [`svm.LinearSVC`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) (setting multi_class=\u201dovr\u201d)\n- [`linear_model.LogisticRegression`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) (setting multi_class=\u201dovr\u201d)\n- [`linear_model.LogisticRegressionCV`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV) (setting multi_class=\u201dovr\u201d)\n- [`linear_model.SGDClassifier`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier)\n- [`linear_model.Perceptron`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron)","111f244b":"The above is consistent with the output of `classification_report`. To choose the F1 scores for Ideal and Premium classes, specify the `labels` parameter:","6ba98734":"We will talk more about how to score each of these strategies later in the tutorial.","811666d5":"> Don't forget to prepend each hyperparameter name with the step name you chose in the pipeline for your estimator. When we created our pipeline, we specified RandomForests as 'base'. See [this](https:\/\/stackoverflow.com\/a\/66344804\/11922237) discussion for more info.","b39f2022":"- [`naive_bayes.BernoulliNB`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB)\n- [`tree.DecisionTreeClassifier`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)\n- [`tree.ExtraTreeClassifier`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.ExtraTreeClassifier.html#sklearn.tree.ExtraTreeClassifier)\n- [`ensemble.ExtraTreesClassifier`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier)\n- [`naive_bayes.GaussianNB`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)\n- [`neighbors.KNeighborsClassifier`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)\n- [`svm.LinearSVC`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) (setting multi_class=\u201dcrammer_singer\u201d)`\n- [`linear_model.LogisticRegression`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) (setting multi_class=\u201dmultinomial\u201d)\n- [`linear_model.LogisticRegressionCV`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV) (setting multi_class=\u201dmultinomial\u201d)\n\nFor N-class problem, they produce N by N [confusion matrix](https:\/\/towardsdatascience.com\/how-to-tune-models-like-a-puppet-master-based-on-confusion-matrix-fd488f9b5e65?source=your_stories_page-------------------------------------) and most of the evaluation metrics are derived from it:","1b297dd4":"Here is the implementation of all this in Sklearn:","71937b55":"### Discussions\n- [How to choose between ROC AUC and F1 score?](https:\/\/stats.stackexchange.com\/questions\/210700\/how-to-choose-between-roc-auc-and-f1-score)\n- [What are the differences between AUC and F1-score?](https:\/\/stats.stackexchange.com\/questions\/123036\/what-are-the-differences-between-auc-and-f1-score)","86313264":"### Sample classification problem and preprocessing pipeline","84841331":"So, how do we choose between recall and precision for the Ideal class? It depends on the type of the problem you are trying to solve. If you want to minimize the instances where other, cheaper type of diamonds are predicted as Ideal, you should optimize precision. As a jewelry store owner, you might be sued for fraud for selling cheaper diamonds as expensive Ideal diamonds.\n\nOn the other hand, if you want to minimize the instances where you accidentally sell Ideal diamonds for a cheaper price, you should optimize for recall of the Ideal class. It is true that you won't get sued but you might lose money. ","f3a0f795":"Other supervised classification algorithms were mainly designed for the binary case. However, Sklearn implements two strategies called One-vs-One (OVO) and One-vs-Rest (OVR, also called One-vs-All) to convert a multi-class problem into a series of binary tasks.","902c87ab":"### Precision, Recall and F1 scores for multiclass classification","ceb2278a":"For multiclass classification, you can calculate the ROC AUC for all classes using either OVO or OVR strategies. Since we agreed that OVR is a better option, here is how ROC AUC is calculated for OVR classification:","b8be66a1":"After the search is done, you can get the best score and estimator with `.best_score_` and `.best_estimator_` attributes, respectively.","0b11fd2c":"A major downside of this strategy is its computation workload. As each pair of classes require a separate binary classifier, targets with high cardinality may take too long to train. To compute the number of classifiers that will be built for an N-class problem, the following formula is used:","651198d0":"![](https:\/\/cdn-images-1.medium.com\/max\/800\/1*uU3BK3dicsDL6ajnpVX60Q.png)","7349ced1":"The first metric we will discuss is the ROC AUC score or area under the *receiver operating characteristic curve*. It is mostly used when we want to measure a classifier's performance to differentiate between each class. This means that ROC AUC is better suited for balanced classification tasks. In essence, ROC AUC score is used for binary classification and with models that can generate class membership probabilities based on some threshold. Here is the brief overview of the steps to calculate ROC AUC for binary classification:\n1. A binary classifier that can generate class membership probabilities such as LogisticRegression with its `predict_proba` method.\n2. An initial, close to 0 decision threshold is chosen. For example, if the probability is higher than 0.1, the class is predicted negative else positive.\n3. Using this threshold, a confusion matrix is created.\n4. True positive rate (TPR) and false positive rate (FPR) is found.\n5. A new threshold is chosen and steps 3-4 are repeated.\n6. Repeat the steps 2-5 for various thresholds between 0 and 1 to create a set of TPRs and FPRs.\n7. Plot all TPRs vs FPRs to generate the receiver operating characteristic curve.\n8. Calculate the area under this curve.","1a9f9c14":"### API and User Guides","28d51a25":"The dataset contains a mixture of numeric and categorical features. I covered preprocessing steps for binary classification in my [last article](https:\/\/towardsdatascience.com\/how-to-tune-models-like-a-puppet-master-based-on-confusion-matrix-fd488f9b5e65?source=your_stories_page-------------------------------------) in detail. You can easily apply the ideas to the multi-class case, so I will keep the explanations here nice and short.\n\nThe target is 'cut', which has 5 classes: Ideal, Premium, Very Good, Good, and Fair (descending quality). We will encode the textual features with OneHotEncoder.\u00a0\n\nLet's take a quick look at the distributions of each numeric feature to decide what type of normalization to use:","f73d24c5":"If you read my [other article](https:\/\/towardsdatascience.com\/how-to-tune-models-like-a-puppet-master-based-on-confusion-matrix-fd488f9b5e65?source=your_stories_page-------------------------------------) on binary classification you know that confusion matrices are the holy grail of supervised classification problems. In a 2 by 2 matrix, the matrix terms are easy to interpret and locate. \n\nEven though it gets more difficult to interpret the matrix as the number of classes increase, there are sure-fire ways to find your way around any matrix of any shape. \n\nThe first step is always identifying your positive and negative classes. This depends on the problem you are trying to solve. As a jewelry store owner, I may want my classifier to differentiate Ideal and Premium diamonds better than other types, which makes these type of diamonds my positive class. Other classes will be considered negative. \n\nEstablishing positive and negative classes early on is very important in evaluating model performance and in hyperparameter tuning. After doing this, you should define what are your true positives, true negatives, false positives and false negatives. In our case:\n\n* Positive classes: Ideal and Premium diamonds\n* Negative classes: Very Good, Good and Fair diamonds\n\n- True Positives, type 1: actual Ideal, predicted Ideal\n- True Positives, type 2: actual Premium, predicted Premium\n- True Negatives: the rest of the diamond types predicted correctly\n- False Positives: actual value belongs any of the 3 negative classes but predicted either Ideal or Premium\n- False Negatives: actual value is either Ideal or Premium but predicted any of the 3 negative classes.","f676a275":"Alternatively, you can use the above models with the default `OneVsRestClassifier`:","a93dc4b5":"As an example problem, we will be predicting the quality of diamonds using the [Diamonds dataset](https:\/\/www.kaggle.com\/shivam2503\/diamonds) from Kaggle:","06b36b96":"We will use the HalvingGridSeachCV (HGS) which was proven to be much faster than a regular GridSearch. You can read this article to see my experiments:\n\nhttps:\/\/towardsdatascience.com\/11-times-faster-hyperparameter-tuning-with-halvinggridsearch-232ed0160155","f44fc90b":"### Related Articles","0976d9b1":"OVO splits a multi-class problem into a single binary classification task for each pair of classes. In other words, for each pair, a single binary classifier will be built. For example, a target with 4 classes\u200a-\u200abrain, lung, breast, and kidney cancer, uses 6 individual classifiers to binarize the problem:\n\n- Classifier 1: lung vs breast\n- Classifier 2: lung vs kidney\n- Classifier 3: lung vs brain\n- Classifier 4: breast vs kidney\n- Classifier 5: breast vs brain\n- Classifier 6: kidney vs brain","041dd5b8":"The third option is to have a model that is equally good at the above 2 scenarios. In other words, a model with high precision and recall. Fortunately, there is a metric that measures just that: F1 score. F1 score takes the harmonic mean of precision and recall and produces a value between 0 and 1:\n![image.png](attachment:4f725e46-c774-4cd1-930d-81240dc3b585.png)","124dcfde":"### Interpreting N by N confusion matrix","f4e79ac1":"- [Multi-Class Metrics Made Simple, Part I: Precision and Recall](https:\/\/towardsdatascience.com\/multi-class-metrics-made-simple-part-i-precision-and-recall-9250280bddc2)\n- [Multi-Class Metrics Made Simple, Part II: the F1-score](https:\/\/towardsdatascience.com\/multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1)\n- [How to Calculate Precision, Recall, and F-Measure for Imbalanced Classification](https:\/\/machinelearningmastery.com\/precision-recall-and-f-measure-for-imbalanced-classification\/)","091bdcd1":"A better metric to measure our pipeline's performance would be using either precision, recall and F1 scores. For the binary case, they are easy and intuitive to understand:\n\n![](https:\/\/cdn-images-1.medium.com\/max\/400\/1*KWZHeEuBGhDfw5CTE_PRfQ.png)\n![](https:\/\/cdn-images-1.medium.com\/max\/400\/1*LmttOOk86tXBGlaC_73Xag.png)\n![](https:\/\/cdn-images-1.medium.com\/max\/600\/1*XH-bmDiJ50rWfqBR82NOdQ.png)"}}