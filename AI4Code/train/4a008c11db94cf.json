{"cell_type":{"5ffd9d32":"code","fd1d219b":"code","4446dfbf":"code","008c62c4":"code","9cdd6851":"code","3ddc0ae2":"code","d2968c1c":"code","35a0ed29":"code","3afc079e":"code","07de9ad6":"code","9f8d78f9":"code","edf1bfdb":"code","940af35d":"code","2cec0301":"code","2b511259":"code","a16a9792":"code","2f6171a6":"code","12563bba":"code","13af8dcf":"code","265a77c3":"code","39e2da03":"code","d5f8f678":"code","4588e821":"code","0e3802a9":"code","07b9c4f5":"code","20eaf95f":"code","58696a19":"code","68841f15":"code","b083be68":"code","f4e7b654":"markdown","b92d977f":"markdown","44c02b0c":"markdown","6d0616b9":"markdown","873047bf":"markdown","a5842cdb":"markdown","cda92e0e":"markdown"},"source":{"5ffd9d32":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fd1d219b":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Conv2D,Flatten,MaxPooling2D,Dropout,BatchNormalization,Activation\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split","4446dfbf":"filepath_train = \"\/kaggle\/input\/digit-recognizer\/train.csv\"\nfilepath_test = \"\/kaggle\/input\/digit-recognizer\/test.csv\"","008c62c4":"df = pd.read_csv(filepath_train)\ndf.head()","9cdd6851":"df.isnull().sum().sort_values(ascending = False) ","3ddc0ae2":"pd.read_csv(filepath_test).head()","d2968c1c":"def import_data(filepath):\n    \n    X = pd.read_csv(filepath).drop('label',axis= 1)\n    y = pd.read_csv(filepath).loc[:,'label']\n    \n    return (np.array(X), np.array(y))\n\nX_train,y_train = import_data(filepath_train)\n# X_test,y_test = import_data(filepath_test) # since there is no label column in test.csv ","35a0ed29":"X_test = pd.read_csv(filepath_test)\nX_test = np.array(X_test)","3afc079e":"X_test.shape","07de9ad6":"X_train.shape","9f8d78f9":"y_train.shape","edf1bfdb":"np.unique(y_train)","940af35d":"plt.figure(figsize = (20,15))\n\nplt.xticks(size=15)\nsns.countplot(y_train,linewidth = 3,edgecolor=sns.color_palette(\"Set2\"))\nplt.title('Distribution of labels in the train dataset', fontdict={'color' : 'Black' , 'fontsize' : 30})\n\nplt.show()","2cec0301":"print(\"Before Reshaping : \")\nprint(\"Shape of X_train :\" ,X_train.shape)\nprint(\"Shape of y_train :\" ,y_train.shape)\nprint(\"Shape of X_test :\" ,X_test.shape)","2b511259":"X_train = X_train.reshape(len(X_train), 28,28,1)\nX_test = X_test.reshape(len(X_test), 28,28,1)\n\ny_train = tf.keras.utils.to_categorical(y_train)","a16a9792":"print(\"After Reshaping : \")\nprint(\"Shape of X_train :\" ,X_train.shape)\nprint(\"Shape of y_train :\" ,y_train.shape)\nprint(\"Shape of X_test :\" ,X_test.shape)","2f6171a6":"X_train = X_train\/255\nX_test = X_test\/255","12563bba":"L = 5\nW = 5\nfig, axes = plt.subplots(L, W, figsize = (15,15))\naxes = axes.ravel()\n\nfor i in range(0, L * W):  \n    axes[i].imshow(X_train[i],cmap='gray')\n    axes[i].set_title(\"Digit = \"+str(i))\n    axes[i].axis('off')\nplt.subplots_adjust(wspace=0.5)","13af8dcf":"model = Sequential()\nmodel.add(Conv2D(filters = 16,kernel_size = (3,3),activation = 'relu', padding = 'same', input_shape = (28,28,1)))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(filters = 32,kernel_size = (3,3),activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D((2,2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(filters = 128,kernel_size = (3,3),activation = 'relu'))\nmodel.add(MaxPooling2D((2,2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(filters = 256,kernel_size = (3,3),activation = 'relu'))\nmodel.add(MaxPooling2D((2,2)))\nmodel.add(Dropout(0.2))","265a77c3":"model.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(10,activation='softmax')) \nmodel.summary()","39e2da03":" X_train.shape[0]\/\/128","d5f8f678":"model.compile(optimizer='adam',metrics = ['accuracy'],loss = 'categorical_crossentropy')\nhistory = model.fit( \n    X_train, y_train ,\n    batch_size = 300  , \n    epochs = 30\n)","4588e821":"#Visualizing the training performance\nplt.figure(figsize=(12, 8))\n\nplt.subplot(2, 2, 1)\nplt.plot(history.history['loss'], label='Loss')\nplt.plot(history.history['accuracy'], label='accuracy')\n\nplt.legend()\nplt.grid()\nplt.title('Training Acc and Loss evolution')","0e3802a9":"predictions = model.predict(X_test)","07b9c4f5":"predictions[0]","20eaf95f":"results = np.argmax(predictions, axis= 1)","58696a19":"results[0]","68841f15":"results = pd.Series(results, name=\"Label\")\nresults.head(10)","b083be68":"submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"submission.csv\", index=False)","f4e7b654":"## No null values :)","b92d977f":"# Loading and Preprocessing ","44c02b0c":"## Visualize some data ","6d0616b9":"## Thank you !","873047bf":"# Constructing a CNN model ","a5842cdb":"## Data Distribution","cda92e0e":"## Reshaping Images"}}