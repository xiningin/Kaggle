{"cell_type":{"580785b6":"code","97fc2d4a":"code","d8c769bd":"code","c17e9efa":"code","42839f91":"code","4c57ec4d":"code","c8ee83f4":"code","42e97af9":"code","86e605ab":"code","9ba4187c":"code","523f94c3":"code","ec18088c":"code","7a8690c9":"code","47229a97":"code","e8ed0ada":"code","33b1c451":"code","1b77102e":"code","3e68b263":"code","f47e98c6":"code","a7532f79":"code","0daffa2f":"code","3bcd1c12":"code","dc4482ca":"code","5cdaea2b":"code","5c47601e":"code","fd62ae07":"code","493016c0":"code","406e6b14":"code","271b0659":"code","c1d28aa3":"code","b49958a8":"code","8292ba7a":"code","c0559582":"code","81e24bbd":"code","82b7354f":"code","1b5c7ee8":"code","4032c3c4":"code","721e0ffb":"code","89e8720b":"code","dbf04725":"code","f108d214":"code","a6a2dcf8":"code","ff77ff91":"code","1a4dee22":"code","a0f33db3":"code","d2510410":"code","3659a2aa":"code","d62f6ae3":"markdown","15e46185":"markdown","38a1541a":"markdown","e9b9e15f":"markdown","026ed361":"markdown","1886890c":"markdown","99a59619":"markdown","62f8ece2":"markdown","3060da4c":"markdown","3cde83cf":"markdown","fc1042a8":"markdown","3caa533c":"markdown"},"source":{"580785b6":"# libraries\n#%matplotlib notebook\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib\nimport seaborn\nimport matplotlib.dates as md\nfrom matplotlib import pyplot as plt\n\nfrom sklearn import preprocessing\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.covariance import EllipticEnvelope\n#from pyemma import msm # not available on Kaggle Kernel\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.svm import OneClassSVM","97fc2d4a":"# some function for later\n\n# return Series of distance between each point and his distance with the closest centroid\ndef getDistanceByPoint(data, model):\n    distance = pd.Series()\n    for i in range(0,len(data)):\n        Xa = np.array(data.loc[i])\n        Xb = model.cluster_centers_[model.labels_[i]-1]\n        distance.set_value(i, np.linalg.norm(Xa-Xb))\n    return distance\n\n# train markov model to get transition matrix\ndef getTransitionMatrix (df):\n\tdf = np.array(df)\n\tmodel = msm.estimate_markov_model(df, 1)\n\treturn model.transition_matrix\n\ndef markovAnomaly(df, windows_size, threshold):\n    transition_matrix = getTransitionMatrix(df)\n    real_threshold = threshold**windows_size\n    df_anomaly = []\n    for j in range(0, len(df)):\n        if (j < windows_size):\n            df_anomaly.append(0)\n        else:\n            sequence = df[j-windows_size:j]\n            sequence = sequence.reset_index(drop=True)\n            df_anomaly.append(anomalyElement(sequence, real_threshold, transition_matrix))\n    return df_anomaly","d8c769bd":"df = pd.read_csv(\"..\/input\/realKnownCause\/realKnownCause\/ambient_temperature_system_failure.csv\")","c17e9efa":"print(df.info())","42839f91":"# check the timestamp format and frequence \nprint(df['timestamp'].head(10))","4c57ec4d":"# check the temperature mean\nprint(df['value'].mean())","c8ee83f4":"# change the type of timestamp column for plotting\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n# change fahrenheit to \u00b0C (temperature mean= 71 -> fahrenheit)\ndf['value'] = (df['value'] - 32) * 5\/9\n# plot the data\ndf.plot(x='timestamp', y='value')","42e97af9":"# the hours and if it's night or day (7:00-22:00)\ndf['hours'] = df['timestamp'].dt.hour\ndf['daylight'] = ((df['hours'] >= 7) & (df['hours'] <= 22)).astype(int)","86e605ab":"# the day of the week (Monday=0, Sunday=6) and if it's a week end day or week day.\ndf['DayOfTheWeek'] = df['timestamp'].dt.dayofweek\ndf['WeekDay'] = (df['DayOfTheWeek'] < 5).astype(int)\n# An estimation of anomly population of the dataset (necessary for several algorithm)\noutliers_fraction = 0.01","9ba4187c":"# time with int to plot easily\ndf['time_epoch'] = (df['timestamp'].astype(np.int64)\/100000000000).astype(np.int64)","523f94c3":"# creation of 4 distinct categories that seem useful (week end\/day week & night\/day)\ndf['categories'] = df['WeekDay']*2 + df['daylight']\n\na = df.loc[df['categories'] == 0, 'value']\nb = df.loc[df['categories'] == 1, 'value']\nc = df.loc[df['categories'] == 2, 'value']\nd = df.loc[df['categories'] == 3, 'value']\n\nfig, ax = plt.subplots()\na_heights, a_bins = np.histogram(a)\nb_heights, b_bins = np.histogram(b, bins=a_bins)\nc_heights, c_bins = np.histogram(c, bins=a_bins)\nd_heights, d_bins = np.histogram(d, bins=a_bins)\n\nwidth = (a_bins[1] - a_bins[0])\/6\n\nax.bar(a_bins[:-1], a_heights*100\/a.count(), width=width, facecolor='blue', label='WeekEndNight')\nax.bar(b_bins[:-1]+width, (b_heights*100\/b.count()), width=width, facecolor='green', label ='WeekEndLight')\nax.bar(c_bins[:-1]+width*2, (c_heights*100\/c.count()), width=width, facecolor='red', label ='WeekDayNight')\nax.bar(d_bins[:-1]+width*3, (d_heights*100\/d.count()), width=width, facecolor='black', label ='WeekDayLight')\n\nplt.legend()\nplt.show()","ec18088c":"# Take useful feature and standardize them\ndata = df[['value', 'hours', 'daylight', 'DayOfTheWeek', 'WeekDay']]\nmin_max_scaler = preprocessing.StandardScaler()\nnp_scaled = min_max_scaler.fit_transform(data)\ndata = pd.DataFrame(np_scaled)\n# reduce to 2 importants features\npca = PCA(n_components=2)\ndata = pca.fit_transform(data)\n# standardize these 2 new features\nmin_max_scaler = preprocessing.StandardScaler()\nnp_scaled = min_max_scaler.fit_transform(data)\ndata = pd.DataFrame(np_scaled)","7a8690c9":"# calculate with different number of centroids to see the loss plot (elbow method)\nn_cluster = range(1, 20)\nkmeans = [KMeans(n_clusters=i).fit(data) for i in n_cluster]\nscores = [kmeans[i].score(data) for i in range(len(kmeans))]\nfig, ax = plt.subplots()\nax.plot(n_cluster, scores)\nplt.show()","47229a97":"# Not clear for me, I choose 15 centroids arbitrarily and add these data to the central dataframe\ndf['cluster'] = kmeans[14].predict(data)\ndf['principal_feature1'] = data[0]\ndf['principal_feature2'] = data[1]\ndf['cluster'].value_counts()","e8ed0ada":"#plot the different clusters with the 2 main features\nfig, ax = plt.subplots()\ncolors = {0:'red', 1:'blue', 2:'green', 3:'pink', 4:'black', 5:'orange', 6:'cyan', 7:'yellow', 8:'brown', 9:'purple', 10:'white', 11: 'grey', 12:'lightblue', 13:'lightgreen', 14: 'darkgrey'}\nax.scatter(df['principal_feature1'], df['principal_feature2'], c=df[\"cluster\"].apply(lambda x: colors[x]))\nplt.show()","33b1c451":"# get the distance between each point and its nearest centroid. The biggest distances are considered as anomaly\ndistance = getDistanceByPoint(data, kmeans[14])\nnumber_of_outliers = int(outliers_fraction*len(distance))\nthreshold = distance.nlargest(number_of_outliers).min()\n# anomaly21 contain the anomaly result of method 2.1 Cluster (0:normal, 1:anomaly) \ndf['anomaly21'] = (distance >= threshold).astype(int)","1b77102e":"# visualisation of anomaly with cluster view\nfig, ax = plt.subplots()\ncolors = {0:'blue', 1:'red'}\nax.scatter(df['principal_feature1'], df['principal_feature2'], c=df[\"anomaly21\"].apply(lambda x: colors[x]))\nplt.show()","3e68b263":"# visualisation of anomaly throughout time (viz 1)\nfig, ax = plt.subplots()\n\na = df.loc[df['anomaly21'] == 1, ['time_epoch', 'value']] #anomaly\n\nax.plot(df['time_epoch'], df['value'], color='blue')\nax.scatter(a['time_epoch'],a['value'], color='red')\nplt.show()","f47e98c6":"# visualisation of anomaly with temperature repartition (viz 2)\na = df.loc[df['anomaly21'] == 0, 'value']\nb = df.loc[df['anomaly21'] == 1, 'value']\n\nfig, axs = plt.subplots()\naxs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])\nplt.legend()\nplt.show()","a7532f79":"# creation of 4 differents data set based on categories defined before\ndf_class0 = df.loc[df['categories'] == 0, 'value']\ndf_class1 = df.loc[df['categories'] == 1, 'value']\ndf_class2 = df.loc[df['categories'] == 2, 'value']\ndf_class3 = df.loc[df['categories'] == 3, 'value']","0daffa2f":"# plot the temperature repartition by categories\nfig, axs = plt.subplots(2,2)\ndf_class0.hist(ax=axs[0,0],bins=32)\ndf_class1.hist(ax=axs[0,1],bins=32)\ndf_class2.hist(ax=axs[1,0],bins=32)\ndf_class3.hist(ax=axs[1,1],bins=32)","3bcd1c12":"# apply ellipticEnvelope(gaussian distribution) at each categories\nenvelope =  EllipticEnvelope(contamination = outliers_fraction) \nX_train = df_class0.values.reshape(-1,1)\nenvelope.fit(X_train)\ndf_class0 = pd.DataFrame(df_class0)\ndf_class0['deviation'] = envelope.decision_function(X_train)\ndf_class0['anomaly'] = envelope.predict(X_train)\n\nenvelope =  EllipticEnvelope(contamination = outliers_fraction) \nX_train = df_class1.values.reshape(-1,1)\nenvelope.fit(X_train)\ndf_class1 = pd.DataFrame(df_class1)\ndf_class1['deviation'] = envelope.decision_function(X_train)\ndf_class1['anomaly'] = envelope.predict(X_train)\n\nenvelope =  EllipticEnvelope(contamination = outliers_fraction) \nX_train = df_class2.values.reshape(-1,1)\nenvelope.fit(X_train)\ndf_class2 = pd.DataFrame(df_class2)\ndf_class2['deviation'] = envelope.decision_function(X_train)\ndf_class2['anomaly'] = envelope.predict(X_train)\n\nenvelope =  EllipticEnvelope(contamination = outliers_fraction) \nX_train = df_class3.values.reshape(-1,1)\nenvelope.fit(X_train)\ndf_class3 = pd.DataFrame(df_class3)\ndf_class3['deviation'] = envelope.decision_function(X_train)\ndf_class3['anomaly'] = envelope.predict(X_train)","dc4482ca":"# plot the temperature repartition by categories with anomalies\na0 = df_class0.loc[df_class0['anomaly'] == 1, 'value']\nb0 = df_class0.loc[df_class0['anomaly'] == -1, 'value']\n\na1 = df_class1.loc[df_class1['anomaly'] == 1, 'value']\nb1 = df_class1.loc[df_class1['anomaly'] == -1, 'value']\n\na2 = df_class2.loc[df_class2['anomaly'] == 1, 'value']\nb2 = df_class2.loc[df_class2['anomaly'] == -1, 'value']\n\na3 = df_class3.loc[df_class3['anomaly'] == 1, 'value']\nb3 = df_class3.loc[df_class3['anomaly'] == -1, 'value']\n\nfig, axs = plt.subplots(2,2)\naxs[0,0].hist([a0,b0], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])\naxs[0,1].hist([a1,b1], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])\naxs[1,0].hist([a2,b2], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])\naxs[1,1].hist([a3,b3], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])\naxs[0,0].set_title(\"WeekEndNight\")\naxs[0,1].set_title(\"WeekEndLight\")\naxs[1,0].set_title(\"WeekDayNight\")\naxs[1,1].set_title(\"WeekDayLight\")\nplt.legend()\nplt.show()","5cdaea2b":"# add the data to the main \ndf_class = pd.concat([df_class0, df_class1, df_class2, df_class3])\ndf['anomaly22'] = df_class['anomaly']\ndf['anomaly22'] = np.array(df['anomaly22'] == -1).astype(int) ","5c47601e":"# visualisation of anomaly throughout time (viz 1)\nfig, ax = plt.subplots()\n\na = df.loc[df['anomaly22'] == 1, ('time_epoch', 'value')] #anomaly\n\nax.plot(df['time_epoch'], df['value'], color='blue')\nax.scatter(a['time_epoch'],a['value'], color='red')\nplt.show()","fd62ae07":"# visualisation of anomaly with temperature repartition (viz 2)\na = df.loc[df['anomaly22'] == 0, 'value']\nb = df.loc[df['anomaly22'] == 1, 'value']\n\nfig, axs = plt.subplots()\naxs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])\nplt.legend()\nplt.show()","493016c0":"# definition of the different state\nx1 = (df['value'] <=18).astype(int)\nx2= ((df['value'] > 18) & (df['value']<=21)).astype(int)\nx3 = ((df['value'] > 21) & (df['value']<=24)).astype(int)\nx4 = ((df['value'] > 24) & (df['value']<=27)).astype(int)\nx5 = (df['value'] >27).astype(int)\ndf_mm = x1 + 2*x2 + 3*x3 + 4*x4 + 5*x5\n\n# getting the anomaly labels for our dataset (evaluating sequence of 5 values and anomaly = less than 20% probable)\n# I USE pyemma NOT AVAILABLE IN KAGGLE KERNEL\n#df_anomaly = markovAnomaly(df_mm, 5, 0.20)\n#df_anomaly = pd.Series(df_anomaly)\n#print(df_anomaly.value_counts())","406e6b14":"\"\"\"\n# add the data to the main \ndf['anomaly24'] = df_anomaly\n\n# visualisation of anomaly throughout time (viz 1)\nfig, ax = plt.subplots()\n\na = df.loc[df['anomaly24'] == 1, ('time_epoch', 'value')] #anomaly\n\nax.plot(df['time_epoch'], df['value'], color='blue')\nax.scatter(a['time_epoch'],a['value'], color='red')\nplt.show()\n\"\"\"","271b0659":"\"\"\"\n# visualisation of anomaly with temperature repartition (viz 2)\na = df.loc[df['anomaly24'] == 0, 'value']\nb = df.loc[df['anomaly24'] == 1, 'value']\n\nfig, axs = plt.subplots()\naxs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'])\nplt.legend()\nplt.show()\n\"\"\"","c1d28aa3":"# Take useful feature and standardize them \ndata = df[['value', 'hours', 'daylight', 'DayOfTheWeek', 'WeekDay']]\nmin_max_scaler = preprocessing.StandardScaler()\nnp_scaled = min_max_scaler.fit_transform(data)\ndata = pd.DataFrame(np_scaled)\n# train isolation forest \nmodel =  IsolationForest(contamination = outliers_fraction)\nmodel.fit(data)\n# add the data to the main  \ndf['anomaly25'] = pd.Series(model.predict(data))\ndf['anomaly25'] = df['anomaly25'].map( {1: 0, -1: 1} )\nprint(df['anomaly25'].value_counts())","b49958a8":"# visualisation of anomaly throughout time (viz 1)\nfig, ax = plt.subplots()\n\na = df.loc[df['anomaly25'] == 1, ['time_epoch', 'value']] #anomaly\n\nax.plot(df['time_epoch'], df['value'], color='blue')\nax.scatter(a['time_epoch'],a['value'], color='red')\nplt.show()","8292ba7a":"# visualisation of anomaly with temperature repartition (viz 2)\na = df.loc[df['anomaly25'] == 0, 'value']\nb = df.loc[df['anomaly25'] == 1, 'value']\n\nfig, axs = plt.subplots()\naxs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'], label = ['normal', 'anomaly'])\nplt.legend()\nplt.show()","c0559582":"# Take useful feature and standardize them \ndata = df[['value', 'hours', 'daylight', 'DayOfTheWeek', 'WeekDay']]\nmin_max_scaler = preprocessing.StandardScaler()\nnp_scaled = min_max_scaler.fit_transform(data)\n# train one class SVM \nmodel =  OneClassSVM(nu=0.95 * outliers_fraction) #nu=0.95 * outliers_fraction  + 0.05\ndata = pd.DataFrame(np_scaled)\nmodel.fit(data)\n# add the data to the main  \ndf['anomaly26'] = pd.Series(model.predict(data))\ndf['anomaly26'] = df['anomaly26'].map( {1: 0, -1: 1} )\nprint(df['anomaly26'].value_counts())","81e24bbd":"# visualisation of anomaly throughout time (viz 1)\nfig, ax = plt.subplots()\n\na = df.loc[df['anomaly26'] == 1, ['time_epoch', 'value']] #anomaly\n\nax.plot(df['time_epoch'], df['value'], color='blue')\nax.scatter(a['time_epoch'],a['value'], color='red')\nplt.show()","82b7354f":"# visualisation of anomaly with temperature repartition (viz 2)\na = df.loc[df['anomaly26'] == 0, 'value']\nb = df.loc[df['anomaly26'] == 1, 'value']\n\nfig, axs = plt.subplots()\naxs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])\nplt.legend()\nplt.show()","1b5c7ee8":"#select and standardize data\ndata_n = df[['value', 'hours', 'daylight', 'DayOfTheWeek', 'WeekDay']]\nmin_max_scaler = preprocessing.StandardScaler()\nnp_scaled = min_max_scaler.fit_transform(data_n)\ndata_n = pd.DataFrame(np_scaled)\n\n# important parameters and train\/test size\nprediction_time = 1 \ntestdatasize = 1000\nunroll_length = 50\ntestdatacut = testdatasize + unroll_length  + 1\n\n#train data\nx_train = data_n[0:-prediction_time-testdatacut].as_matrix()\ny_train = data_n[prediction_time:-testdatacut  ][0].as_matrix()\n\n# test data\nx_test = data_n[0-testdatacut:-prediction_time].as_matrix()\ny_test = data_n[prediction_time-testdatacut:  ][0].as_matrix()","4032c3c4":"#unroll: create sequence of 50 previous data points for each data points\ndef unroll(data,sequence_length=24):\n    result = []\n    for index in range(len(data) - sequence_length):\n        result.append(data[index: index + sequence_length])\n    return np.asarray(result)\n\n# adapt the datasets for the sequence data shape\nx_train = unroll(x_train,unroll_length)\nx_test  = unroll(x_test,unroll_length)\ny_train = y_train[-x_train.shape[0]:]\ny_test  = y_test[-x_test.shape[0]:]\n\n# see the shape\nprint(\"x_train\", x_train.shape)\nprint(\"y_train\", y_train.shape)\nprint(\"x_test\", x_test.shape)\nprint(\"y_test\", y_test.shape)","721e0ffb":"# specific libraries for RNN\n# keras is a high layer build on Tensorflow layer to stay in high level\/easy implementation\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.recurrent import LSTM\nfrom keras.models import Sequential\nimport time #helper libraries\nfrom keras.models import model_from_json\nimport sys","89e8720b":"# Build the model\nmodel = Sequential()\n\nmodel.add(LSTM(\n    input_dim=x_train.shape[-1],\n    output_dim=50,\n    return_sequences=True))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(\n    100,\n    return_sequences=False))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(\n    units=1))\nmodel.add(Activation('linear'))\n\nstart = time.time()\nmodel.compile(loss='mse', optimizer='rmsprop')\nprint('compilation time : {}'.format(time.time() - start))","dbf04725":"# Train the model\n#nb_epoch = 350\n\nmodel.fit(\n    x_train,\n    y_train,\n    batch_size=3028,\n    nb_epoch=30,\n    validation_split=0.1)\n","f108d214":"# save the model because the training is long (1h30) and we don't want to do it every time\n\"\"\"\n# serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"model2.json\", \"w\") as json_file:\n    json_file.write(model_json)\n# serialize weights to HDF5\nmodel.save_weights(\"model2.h5\")\nprint(\"Saved model to disk\")\n\"\"\"","a6a2dcf8":"# load json and create model\n\"\"\"\njson_file = open('model.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n# load weights into new model\nloaded_model.load_weights(\"model.h5\")\nprint(\"Loaded model from disk\")\n\"\"\"","ff77ff91":"# create the list of difference between prediction and test data\nloaded_model = model\ndiff=[]\nratio=[]\np = loaded_model.predict(x_test)\n# predictions = lstm.predict_sequences_multiple(loaded_model, x_test, 50, 50)\nfor u in range(len(y_test)):\n    pr = p[u][0]\n    ratio.append((y_test[u]\/pr)-1)\n    diff.append(abs(y_test[u]- pr))","1a4dee22":"# plot the prediction and the reality (for the test data)\nfig, axs = plt.subplots()\naxs.plot(p,color='red', label='prediction')\naxs.plot(y_test,color='blue', label='y_test')\nplt.legend(loc='upper left')\nplt.show()","a0f33db3":"# select the most distant prediction\/reality data points as anomalies\ndiff = pd.Series(diff)\nnumber_of_outliers = int(outliers_fraction*len(diff))\nthreshold = diff.nlargest(number_of_outliers).min()\n# data with anomaly label (test data part)\ntest = (diff >= threshold).astype(int)\n# the training data part where we didn't predict anything (overfitting possible): no anomaly\ncomplement = pd.Series(0, index=np.arange(len(data_n)-testdatasize))\n# # add the data to the main\ndf['anomaly27'] = complement.append(test, ignore_index='True')\nprint(df['anomaly27'].value_counts())","d2510410":"# visualisation of anomaly throughout time (viz 1)\nfig, ax = plt.subplots()\n\na = df.loc[df['anomaly27'] == 1, ['time_epoch', 'value']] #anomaly\n\nax.plot(df['time_epoch'], df['value'], color='blue')\nax.scatter(a['time_epoch'],a['value'], color='red')\nplt.axis([1.370*1e7, 1.405*1e7, 15,30])\nplt.show()","3659a2aa":"# visualisation of anomaly with temperature repartition (viz 2)\na = df.loc[df['anomaly27'] == 0, 'value']\nb = df.loc[df['anomaly27'] == 1, 'value']\n\nfig, axs = plt.subplots()\naxs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'])\nplt.legend()\nplt.show()","d62f6ae3":"## 2.8 Collective and sequential anomalies (Ordered)\nThis class is most general and consider ordering as well as value combinations. We usually use combination of algorithm like cluster+markov model.\n## 3 Result comparison\n(may be later)\n## 4 Conclusion\nFor this case, the contextual anomaly detection (categories+elliptique enveloppe) seem a good solution. ","15e46185":"# 1 Data\n## 1.1 Extract data\nThe dataset is from https:\/\/www.kaggle.com\/boltzmannbrain\/nab \nIn realKnownCause\/ambient_temperature_system_failure.csv","38a1541a":"Detect unusual sequence but not extreme value. More difficult to evaluate the relevance on this example. The sequence size (5) should be match with some interesting cycle.\n## 2.5 Isolation Forest\n#### Use for collective anomalies (unordered).\nSimple, works well with different data repartition and efficient with high dimention data.","e9b9e15f":"Give result similar to isolation forest but find some anomalies in average values. Difficult to know if it's relevant.\n## 2.7 RNN\n#### Use for  sequential anomalies (ordered)\nRNN learn to recognize sequence in the data and then make prediction based on the previous sequence. We consider an anomaly when the next data points are distant from RNN prediction. Aggregation, size of sequence and size of prediction for anomaly are important parameters to have relevant detection.  \nHere we make learn from 50 previous values, and we predict just the 1 next value.","026ed361":"## 1.3 Feature engineering\nExtracting some useful features","1886890c":"# Motivation : \nI read an interesting article about anomaly detection: https:\/\/iwringer.wordpress.com\/2015\/11\/17\/anomaly-detection-concepts-and-techniques\/.  \nI wanted to try a few of these techniques to better understand them. I searched an interesting dataset on Kaggle about anomaly detection with simple exemples. I choose one exemple of NAB datasets (thanks for this datasets) and I implemented a few of these algorithms. The goal of this Notebook is just to implement these techniques and understand there main caracteristics. Sometimes, they are not adapted to this datasets. I add some visualizations to understand what the algorithm detect. Hope it can help some people.\nNotebook available (with Markov Chain) here: https:\/\/github.com\/Vicam\/Unsupervised_Anomaly_Detection\n\n# Algorithm implemented :\n- Cluster based anomaly detection (K-mean)\n- Repartition of data into categories then Gaussian\/Elliptic Enveloppe on each categories separately\n- Markov Chain\n- Isolation Forest\n- One class SVM\n- RNN (comparison between prediction and reality)","99a59619":"We can see that the temperature is more stable during daylight of business day.\n# 2 Models\n## 2.1 Cluster only\n#### Use for collective anomalies (unordered). \n\nWe group together the usual combination of features. The points that are far from the cluster are points with usual combination of features.We consider those points as anomalies.","62f8ece2":"## 1.2 Understand data","3060da4c":"## 2.4 Markov chains\n#### Use for  sequential anomalies (ordered)\nWe need discretize the data points in define states for markov chain. We will just take 'value' to define state for this example and define 5 levels of value (very low, low, average, high, very high)\/(VL, L, A, H, VH).\nMarkov chain will calculate the probability of sequence like (VL, L, L, A, A, L, A). If the probability is very weak we consider the sequence as an anomaly.","3cde83cf":"Cluster method detects the low temperature around the end of record as unusually low. It doesn't detect the highest temperature pic.\n## 2.2 Categories + Gaussian\n#### Use for contextual data and collective anomalies (unordered).  \nWe will separate data by (what we think of) important categories. Or we can separate data based on different cluster (method 2.3). Then we find outliers (gaussian repartition, unimodal) by categories independently.   ","fc1042a8":"Good detections of extreme values and context separation add some precision to the detection.\n## 2.3 Cluster+Gaussian\nSimilar to 2.2 solution but with cluster to separate data in different group.","3caa533c":"## 2.6 One class SVM\n#### Use for collective anomalies (unordered).\nGood for novelty detection (no anomalies in the train set). This algorithm performs well for multimodal data."}}