{"cell_type":{"3b655f1f":"code","bc13ce3e":"code","7d2e026f":"code","171e98e6":"code","d287ba2a":"code","7d3cc28f":"code","e2979092":"code","95ad1439":"code","c31f9adf":"code","8a6d14e7":"code","3ba0695c":"code","6509569e":"code","c08956cf":"code","81c618fd":"code","4cd10d53":"markdown","da0ac4f1":"markdown","b4eda37d":"markdown","b0abc46d":"markdown","214af58e":"markdown","848a8719":"markdown","5f9ffe99":"markdown","af55bb43":"markdown","3986f836":"markdown","9d3ba2ba":"markdown","9f006d35":"markdown"},"source":{"3b655f1f":"import pandas as pd\n\nreview_df = pd.read_csv(\"\/kaggle\/input\/kumarmanoj-bag-of-words-meets-bags-of-popcorn\/labeledTrainData.tsv\",\n                       header = 0,\n                       sep ='\\t',\n                       quoting = 3)\nreview_df.head(5)\n","bc13ce3e":"review_df.review[0]","7d2e026f":"import re\n\n# replace <br\/ > into blank\nreview_df['review'] = review_df['review'].replace('<br \/>',' ')\n\n# replace special letter and number into blank\nreview_df['review'] = review_df['review'].apply(lambda x : re.sub(\"[^a-zA-Z]\",\" \",x))\n","171e98e6":"#split the data\n\nfrom sklearn.model_selection import train_test_split\n\nclass_df = review_df['sentiment']\nfeature_df = review_df.drop(['id','sentiment'],axis=1, inplace=False)\n\nX_train, X_test, y_train, y_test = train_test_split(feature_df,class_df, \n                                                    test_size = 0.3,\n                                                    random_state = 156)\n\nX_train.shape, X_test.shape","d287ba2a":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, roc_auc_score\n\n# count vectorizer\npipeline = Pipeline([('cnt_vect',CountVectorizer(stop_words = 'english',ngram_range=(1,2))),\n                     ('lr_clf', LogisticRegression())])\n\npipeline.fit(X_train['review'],y_train)\npred = pipeline.predict(X_test['review'])\npred_probs = pipeline.predict_proba(X_test['review'])[:,1]\n\nprint('Accuracy of count Vectorizer : {0:.4f}'.format(accuracy_score(y_test,pred)))","7d3cc28f":"# TF-IDF vectorizer\npipeline = Pipeline([('tfidf_vect',TfidfVectorizer(stop_words = 'english',ngram_range=(1,2))),\n                    ('lr_clf',LogisticRegression(C=10))])\n\npipeline.fit(X_train['review'],y_train)\npred = pipeline.predict(X_test['review'])\npred_probs = pipeline.predict_proba(X_test['review'])[:,1]\n\nprint('Accuracy of TF-IDF Vectorizer : {0:.4f}'.format(accuracy_score(y_test,pred)))","e2979092":"max_df = [0.9,1.0]\nC = [10,11,12]\n\nfor df in max_df:\n    for c in C:\n        pipeline = Pipeline([('tfidf_vect',TfidfVectorizer(stop_words = 'english',\n                                                           ngram_range=(1,2),\n                                                           max_df = df)),\n                             ('lr_clf',LogisticRegression(C=c))])\n        pipeline.fit(X_train['review'],y_train)\n        pred = pipeline.predict(X_test['review'])\n        \n        print('max_df :{0}, c :{1}, Accuracy : {2}'.format(df,c,accuracy_score(y_test,pred)))","95ad1439":"test_df = pd.read_csv(\"\/kaggle\/input\/kumarmanoj-bag-of-words-meets-bags-of-popcorn\/testData.tsv\",\n                             header=0, \n                             sep=\"\\t\", \n                             quoting=3)\n\n\ntest_df['review'] = test_df['review'].str.replace('<br \/>',' ')\ntest_df['review'] = test_df['review'].apply(lambda x : re.sub(\"[^a-zA-Z]\",\" \",x))\n\npred = pipeline.predict(test_df['review'])\noutput = pd.DataFrame( data={\"id\":test_df[\"id\"], \"sentiment\":pred } )\noutput.to_csv( \"submission.csv\", index=False, quoting=3 )\n","c31f9adf":"from nltk.corpus import wordnet as wn\n\n\n# function that word class tag\ndef penn_to_wn(tag):\n    if tag.startswith('J'):\n        return wn.ADJ\n    elif tag.startswith('N'):\n        return wn.NOUN\n    elif tag.startswith('R'):\n        return wn.ADV\n    elif tag.startswith('V'):\n        return wn.VERB\n    ","8a6d14e7":"import nltk\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus import sentiwordnet as swn\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import sentiwordnet as swn\nfrom nltk import sent_tokenize, word_tokenize, pos_tag\n\ndef swn_polarity(text):\n    sentiment = 0.0\n    tokens_count = 0\n    \n    lemmatizer = WordNetLemmatizer()\n    raw_sentences = sent_tokenize(text)\n    \n    for raw_sentence in raw_sentences:\n        tagged_sentence = pos_tag(word_tokenize(raw_sentence))\n        for word, tag in tagged_sentence:\n            wn_tag = penn_to_wn(tag)\n            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n                continue\n            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n            if not lemma:\n                continue\n                \n            synsets = wn.synsets(lemma, pos=wn_tag)\n            \n            if not synsets:\n                continue\n            synset = synsets[0]\n            swn_synset = swn.senti_synset(synset.name())\n            sentiment += (swn_synset.pos_score()-swn_synset.neg_score())\n            tokens_count += 1\n        if not tokens_count:\n            return 0\n        \n        if sentiment >= 0 :\n            return 1\n        \n    return 0","3ba0695c":"review_df['preds'] = review_df['review'].apply(lambda x : swn_polarity(x))\ny_target = review_df['sentiment'].values\npreds = review_df['preds'].values","6509569e":"from sklearn.metrics import accuracy_score, confusion_matrix, precision_score\nfrom sklearn.metrics import recall_score, f1_score, roc_auc_score\nimport numpy as np\n\nprint(confusion_matrix(y_target,preds))\nprint(np.round(accuracy_score(y_target, preds),4))\nprint(np.round(precision_score(y_target, preds),4))\nprint(np.round(recall_score(y_target, preds),4))","c08956cf":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nsenti_analyzer = SentimentIntensityAnalyzer()\nsenti_score = senti_analyzer.polarity_scores(review_df['review'][0])\nprint(senti_score)","81c618fd":"def vader_polarity(review, threshold = 0.1):\n    analyzer = SentimentIntensityAnalyzer()\n    score = senti_analyzer.polarity_scores(review)\n    \n    agg_score = score['compound']\n    final_sentiment = 1 if agg_score >= threshold else 0\n    return final_sentiment\n\nreview_df['vader_preds'] = review_df['review'].apply(lambda x : vader_polarity(x, 0.1))\ny_target = review_df['sentiment'].values\nvader_preds = review_df['vader_preds'].values\n\nprint(confusion_matrix(y_target,vader_preds))\nprint(np.round(accuracy_score(y_target, vader_preds),4))\nprint(np.round(precision_score(y_target, vader_preds),4))\nprint(np.round(recall_score(y_target, vader_preds),4))","4cd10d53":"# VADER","da0ac4f1":"# data preprocessiong\n\nCheck one of the review. You can see there are many interuped things in it. Like <br\/ > tag, special letter and number. We want to make the model based on the word. So remove it ! It is some kinds of data cleansing. and then we will split data into train and test.","b4eda37d":"# introduction\n\n[Sentiment analysis](https:\/\/en.wikipedia.org\/wiki\/Sentiment_analysis) is the way that identify feeling, opinion and sentiment of the text data. It is used in many area like SNS, opinion poll, review , etc. \n\nWe can divide this sentiment analysis into two type :  supervised, unsupervised.\n![2](https:\/\/ifh.cc\/g\/fHijsQ.png)\n","b0abc46d":"As you can see, TF-IDF vectorizer is more accurable than count vectorizer method.","214af58e":"# Sentiment Analysis\n\n**Let's do the sentiment analysis using movie review data !**\n![1](https:\/\/ifh.cc\/g\/K60kmo.jpg)","848a8719":"We are going to do some supervised learning using this data!","5f9ffe99":"# check data\n![mm](https:\/\/ifh.cc\/g\/uGJuLQ.jpg)\n\nWhat we gonna use in this analysis is the movie review data. We want to make the model that predict the review is positive or negative. So let's download the data.\n\nFile extention is '.tsv' This file is seperated by tap (\\t). You can easily transfer into dataframe using pandas.\n\nThe feature of this data is this.\n\n* 'id' : ID of each data\n* 'sentiment' : target label, 1 means positive review and 0 means negative review\n* 'review' : movie reviews","af55bb43":"# Bag of Words\n\nBag of Words is the model that make the feature according to the frequency of the word and ignore the sequence. \n\nYou can see more detail explaination in this : [feature vector](https:\/\/www.kaggle.com\/yejining99\/text-classification-20newsgroups#feature-vector)\n\nWe will apply both count and TF-IDF vectorizer into logistic model.","3986f836":"# SentiWordNet ","9d3ba2ba":"# hyperparameter\n\nLet's tune the parameter to upgrade performance of logistic regression model! We find that TF-IDF vectorizer is more accurate so we will use it. We gonna do hyperparameter about two of the parameter.\n\n* max_df of TF-IDF\n* C of logistic regression\n\n[max_df](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html?highlight=tf%20idf#sklearn.feature_extraction.text.TfidfVectorizer) : When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float in range [0.0, 1.0], the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n\n[C](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html?highlight=logistic#sklearn.linear_model.LogisticRegression) : Inverse of regularization strength. Smaller values specify stronger regularization.","9f006d35":"# conclusion"}}