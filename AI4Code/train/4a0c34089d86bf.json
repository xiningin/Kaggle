{"cell_type":{"6cf513cf":"code","14aec975":"code","e8108136":"code","a2fc5bc1":"code","fd14234a":"code","b9fa8d30":"code","ecd5bf0e":"code","e1d1903d":"code","a1b2400b":"code","86984929":"code","a8d929eb":"code","f9a0b106":"code","4e7e3313":"code","d95ea41b":"code","9b28aaff":"code","d1ff1711":"code","1519d314":"code","91a0f787":"code","bfe90365":"code","5a08453e":"code","024b6af7":"markdown"},"source":{"6cf513cf":"import torch\nimport torch.nn as nn\nimport torch.functional as F\nfrom torchvision import *\nfrom torch.utils.data import DataLoader, sampler, random_split\nfrom torch.optim import *","14aec975":"import pandas as pd\n\nimport os\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport xml.etree.ElementTree as ET","e8108136":"import pathlib\n\nimport time\n\nimport copy\n\nimport numpy as np\n\nfrom PIL import Image\nfrom PIL import ImageFile\nimport albumentations","a2fc5bc1":"def crop_image(breed, dog, data_dir):\n  img = plt.imread(data_dir + 'images\/Images\/' + breed + '\/' + dog + '.jpg')\n  tree = ET.parse(data_dir + 'annotations\/Annotation\/' + breed + '\/' + dog)\n  xmin = int(tree.getroot().findall('object')[0].find('bndbox').find('xmin').text)\n  xmax = int(tree.getroot().findall('object')[0].find('bndbox').find('xmax').text)\n  ymin = int(tree.getroot().findall('object')[0].find('bndbox').find('ymin').text)\n  ymax = int(tree.getroot().findall('object')[0].find('bndbox').find('ymax').text)\n  img = img[ymin:ymax, xmin:xmax, :]\n  return img","fd14234a":"data_dir = '..\/input\/stanford-dogs-dataset\/'\nbreed_list = os.listdir(data_dir + 'images\/Images\/')\n\nplt.figure(figsize=(20, 20))\nfor i in range(4):\n  plt.subplot(421 + (i*2))\n  breed = np.random.choice(breed_list)\n  dog = np.random.choice(os.listdir(data_dir + 'annotations\/Annotation\/' + breed))\n  img = plt.imread(data_dir + 'images\/Images\/' + breed + '\/' + dog + '.jpg')\n  plt.imshow(img)  \n  \n  tree = ET.parse(data_dir + 'annotations\/Annotation\/' + breed + '\/' + dog)\n  xmin = int(tree.getroot().findall('object')[0].find('bndbox').find('xmin').text)\n  xmax = int(tree.getroot().findall('object')[0].find('bndbox').find('xmax').text)\n  ymin = int(tree.getroot().findall('object')[0].find('bndbox').find('ymin').text)\n  ymax = int(tree.getroot().findall('object')[0].find('bndbox').find('ymax').text)\n  plt.plot([xmin, xmax, xmax, xmin, xmin], [ymin, ymin, ymax, ymax, ymin])\n  crop_img = crop_image(breed, dog, data_dir)\n  plt.subplot(422 + (i*2))\n  plt.imshow(crop_img)","b9fa8d30":"if 'data' not in os.listdir():\n    os.mkdir('data')\nfor breed in breed_list:\n    os.mkdir('data\/' + breed)\nprint('Created {} folders to store cropped images of the different breeds.'.format(len(os.listdir('data'))))","ecd5bf0e":"for breed in os.listdir('data'):\n    for file in os.listdir(data_dir + 'annotations\/Annotation\/' + breed):\n        img = Image.open(data_dir + 'images\/Images\/' + breed + '\/' + file + '.jpg')\n        tree = ET.parse(data_dir + 'annotations\/Annotation\/' + breed + '\/' + file)\n        xmin = int(tree.getroot().findall('object')[0].find('bndbox').find('xmin').text)\n        xmax = int(tree.getroot().findall('object')[0].find('bndbox').find('xmax').text)\n        ymin = int(tree.getroot().findall('object')[0].find('bndbox').find('ymin').text)\n        ymax = int(tree.getroot().findall('object')[0].find('bndbox').find('ymax').text)\n        img = img.crop((xmin,ymin,xmax,ymax))\n        img = img.convert('RGB')\n        img.save('data\/' + breed + '\/' + file + '.jpg')","e1d1903d":"img_count = 0\nfor folder in os.listdir('data'):\n    for _ in os.listdir('data\/' + folder):\n        img_count += 1\nprint('No. of Images: {}'.format(img_count))","a1b2400b":"def get_default_device():\n    if torch.cuda.is_available():\n        return torch.device(\"cuda\")\n    else:\n        return torch.device(\"cpu\")\ndevice = get_default_device()\ndevice","86984929":"train_transform = transforms.Compose([\n    transforms.RandomRotation(degrees=11),\n    transforms.RandomHorizontalFlip(0.5),\n    transforms.Resize(size=224),\n    transforms.CenterCrop(size=224), \n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n])\nvalidation_transform = transforms.Compose([\n    transforms.Resize(size=224),\n    transforms.CenterCrop(size=224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])","a8d929eb":"all_data = datasets.ImageFolder(root='data') \ntrain_data_len = int(len(all_data)*0.8) \ntest_data_len = int((len(all_data) - train_data_len)) \ntrain_data, val_data = random_split(all_data, [train_data_len, test_data_len])\n\n\n\ntrain_data.dataset.transform = train_transform\nval_data.dataset.transform = validation_transform\n\n\nprint(len(train_data), len(val_data))\n\n\ntrain_loader = DataLoader(train_data, batch_size=128, shuffle=True) \nval_loader = DataLoader(val_data, batch_size=128, shuffle=True)","f9a0b106":"root = pathlib.Path('data\/')\nclasses = sorted([j.name.split('\/')[-1] for j in root.iterdir()])\nprint(classes)","4e7e3313":"class Model(nn.Module):\n    def __init__(self):\n        super(Model,self).__init__()\n        \n        self.backbone =   models.resnet50(pretrained=True)\n        self.num_features = self.backbone.fc.in_features\n        self.backbone.fc = nn.Sequential(nn.Linear(self.num_features,len(classes)),\n                                        nn.LogSoftmax(dim=1))\n                                        \n        \n    def forward(self,image):\n        x = self.backbone(image)\n        return x\n    \n    def freeze(self):\n        # To freeze the residual layers\n        for param in self.backbone.parameters():\n            param.require_grad = False\n        for param in self.backbone.fc.parameters():\n            param.require_grad = True\n    \n    def unfreeze(self):\n        # Unfreeze all layers\n        for param in self.backbone.parameters():\n            param.require_grad = True","d95ea41b":"model = Model()\nmodel = model.to(device)\n\ncriterion = nn.NLLLoss()\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.00001)","9b28aaff":"def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for inputs, labels in dataloaders[phase]:\n                \n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss \/ dataset_sizes[phase]\n            epoch_acc = running_corrects.double() \/ dataset_sizes[phase]\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model","d1ff1711":"def prediciton(net, data_loader):\n    test_pred = torch.LongTensor()\n    for i, data in enumerate(data_loader):\n        if torch.cuda.is_available():\n          pass\n\n        output = net(data)\n        pred = output.cpu().data.max(1, keepdim=True)[1]\n        test_pred = torch.cat((test_pred, pred), dim=0)\n    \n    return test_pred","1519d314":"dataloaders = {'train':train_loader,\"val\":val_loader}","91a0f787":"dataset_sizes = {'train':len(train_data),'val':len(val_data)}","bfe90365":"model.freeze()\nmodel_ft = train_model(model, criterion, optimizer, exp_lr_scheduler,num_epochs=25)","5a08453e":"model.freeze()\nmodel_ft = train_model(model, criterion, optimizer, exp_lr_scheduler,num_epochs=25)","024b6af7":"# Total number of cropped Images"}}