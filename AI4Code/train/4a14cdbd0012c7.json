{"cell_type":{"f88cca83":"code","573bbf3f":"code","108cbe60":"code","5a46482c":"code","d0d8af1a":"code","4b95027e":"code","8db06769":"code","a24ca8c4":"code","d8fd218c":"code","d8b2f68e":"code","331e7427":"code","cd761b0d":"code","94b4b911":"code","a7c49ff0":"code","a3fe19cf":"code","09fd5512":"code","c861b4e3":"code","3ea05d17":"code","6973dc6d":"code","e15c74f3":"code","93cd0797":"code","9aa37243":"code","ab43f366":"code","98b6e4dd":"code","341a2c06":"code","7c538328":"code","27551d29":"code","0c7b0941":"code","05403759":"code","e285eb11":"code","e169f0ca":"code","ca740309":"code","2c002d34":"code","df58190f":"code","c102b958":"code","8292a55f":"code","75a38be0":"code","e34803cd":"code","01735c49":"code","997e3c62":"code","1bd5f91d":"code","61793e69":"code","150c9ebe":"code","8a58116b":"code","46a69dd8":"code","87256f9f":"code","92f2da99":"code","de2e68dc":"code","3dd0d463":"code","01304091":"code","669f9d7b":"code","0650af8d":"code","7ea862ea":"code","a4ba45c6":"code","44d4d5e7":"code","308de2fb":"code","46b22bdf":"code","c498c1c6":"code","32356de0":"code","e5134289":"code","0df221c9":"code","ab07921c":"code","da054710":"code","c6369c10":"code","51f9eb74":"code","9f9f67ff":"code","b565bf7d":"code","ba4c2328":"code","bbb67579":"code","de9114ce":"markdown","ca674ea4":"markdown","2cd48e98":"markdown","2471b025":"markdown","3f61d862":"markdown","1485224f":"markdown","83edc7df":"markdown","0b2166fb":"markdown","49193420":"markdown","7f84c0fa":"markdown","ac49b32a":"markdown","b28abd8d":"markdown","e8d2e543":"markdown","520d05d8":"markdown","3b01313d":"markdown"},"source":{"f88cca83":"import numpy             as np \nimport pandas            as pd \nimport matplotlib.pyplot as plt\nimport seaborn           as sns \n%matplotlib inline \n\nfrom sklearn.preprocessing   import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics         import roc_curve\nfrom sklearn.ensemble        import RandomForestClassifier\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","573bbf3f":"#Importing train & test data \ndf_train = pd.read_csv(r'..\/input\/tabular-playground-series-apr-2021\/train.csv')\ndf_test  = pd.read_csv(r'..\/input\/tabular-playground-series-apr-2021\/test.csv')\ntest_id  = pd.read_csv(r'..\/input\/tabular-playground-series-apr-2021\/test.csv')","108cbe60":"#Analyzing the train & test dataset\nprint(df_train.info())\nprint(df_test.info())","5a46482c":"#Analyzing the null values in train, test dataframe\nprint((df_train.isnull().sum()\/len(df_train))*100)\nprint((df_test.isnull().sum()\/len(df_test))*100)","d0d8af1a":"#As we  can see in the column Cabin, more than 50% of the values are missing,we will delete the column \ndf_train.drop(columns='Cabin',axis=1,inplace=True)\ndf_test.drop(columns='Cabin',axis=1,inplace=True)","4b95027e":"#In some of the columns there are null values are less than 5% so we will delete these null values \ndf_train.dropna(axis=0,inplace=True)","8db06769":"#Function to impute the null values with most frequent values \ndef impute(df):\n    from sklearn.impute import SimpleImputer\n    my_imputer           = SimpleImputer(strategy='most_frequent')\n    imputed_data         = pd.DataFrame(my_imputer.fit_transform(df))\n    imputed_data.columns = df.columns\n    return imputed_data","a24ca8c4":"df_test = impute(df_test)","d8fd218c":"#Checking the dataframe again for null values \nprint((df_train.isnull().sum()\/len(df_train))*100)\nprint((df_test.isnull().sum()\/len(df_test))*100)","d8b2f68e":"#Checking the dataframe \ndf_train.head()","331e7427":"#Converting the categorical columns into numerical data \ndf_train['Sex'] = df_train['Sex'].apply(lambda x:1 if x=='male' else 0)\ndf_test['Sex']  = df_test['Sex'].apply(lambda x:1 if x=='male' else 0)","cd761b0d":"#Analysing other categorical columns like 'EMBARKED'\ndf_train['Embarked'].value_counts()","94b4b911":"#Converting the 'EMBARKED' column into a dummy columns as it has more than 2 uniquie value \ntrain_dummy = pd.get_dummies(df_train['Embarked'],drop_first=True)\ntest_dummy  = pd.get_dummies(df_test['Embarked'],drop_first=True)","a7c49ff0":"#Merging the dummies with the main dataframe\ndf_train = pd.concat([df_train,train_dummy],axis=1)\ndf_test  = pd.concat([df_test,test_dummy],axis=1)","a3fe19cf":"#Dropping the main column 'Embarked'\ndf_train.drop(columns='Embarked',axis=1,inplace=True)\ndf_test.drop(columns='Embarked',axis=1,inplace=True)","09fd5512":"#As we can see name,passengerID,Ticket column can be dropped as they wont have any effect on survival\ndf_train.drop(columns=['PassengerId','Name','Ticket'],axis=1,inplace=True)\ndf_test.drop(columns=['PassengerId','Name','Ticket'],axis=1,inplace=True)","c861b4e3":"#Checking if any outliers exits in the data \nsns.boxplot(df_train['Age']);\n#We can conclude that there is no outlier \n#Make a point we won't be analysing the test data for outliers as we have to consider it as hidden ","3ea05d17":"#Preprocessing the data using MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler\nscaling                = MinMaxScaler()\nscaling_col            = ['Age','Fare']\ndf_train[scaling_col]  = scaling.fit_transform(df_train[scaling_col])\ndf_test[scaling_col]   = scaling.transform(df_test[scaling_col])","6973dc6d":"#Checking the dataframe one last time\ndf_train.head()","e15c74f3":"#Checking the correlation between the features \nsns.heatmap(df_train.corr(),annot=True)\n#Features which are highly correlated with the target variables are \n#  ------>>> Sex,Pclass,S,Fare,Age","93cd0797":"#Checking if the data is balanced or imbalanced\nsns.countplot(df_train['Survived']);","9aa37243":"(df_train['Survived'].value_counts(normalize=True))*100\n#The dataset is not balanced but we cannot deam it to the category of imbalance too","ab43f366":"#Function to check the VIF of the df\ndef vif_validation(X_train):\n    from statsmodels.stats.outliers_influence import variance_inflation_factor\n    # Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n    vif = pd.DataFrame()\n    vif['Features']  = X_train.columns\n    vif['VIF']       = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\n    vif['VIF']       = round(vif['VIF'], 2)\n    vif              = vif.sort_values(by = \"VIF\", ascending = False)\n    return vif","98b6e4dd":"#Function to create a table with pred values for logistic regression \n#Function will return a dataframe with predicted values\ndef prediction(model_name,x_test,y_test,thres):\n    y_pred                        = model_name.predict(x_test)\n    y_pred_final                  = pd.DataFrame({'Prob':y_pred})\n    y_pred_final['Survived']      = y_test\n    y_pred_final['pred']          = y_pred_final['Prob'].apply(lambda x:1 if x>thres else 0)\n    return y_pred_final","341a2c06":"#Function to create a table with pred values for logistic regression \n#Function will return a dataframe with predicted values\ndef test_prediction(model_name,x_test,thres):\n    y_pred                        = model_name.predict(x_test)\n    y_pred_final                  = pd.DataFrame({'Prob':y_pred})\n    y_pred_final['pred']          = y_pred_final['Prob'].apply(lambda x:1 if x>thres else 0)\n    return y_pred_final","7c538328":"#function to test the logistic Regression model \ndef validating_lr(y_real,y_pred):\n    from sklearn.metrics import confusion_matrix, accuracy_score\n    print('Confusion Matrix')\n    confusion = confusion_matrix(y_pred,y_real)\n    print(confusion)\n    print('\\n')\n    print('Accuracy Score',(accuracy_score(y_pred,y_real)*100))\n    TP = confusion[1,1] # true positive \n    TN = confusion[0,0] # true negatives\n    FP = confusion[0,1] # false positives\n    FN = confusion[1,0] # false negatives\n    print('\\n')\n    print('Sensitivity:',(TP \/ float(TP+FN)*100))\n    print('\\n')\n    print('specificity:',(TN \/ float(TN+FP)*100))\n    print('\\n')\n    print('false postive rate - predicting 1 when its 0:',(FP\/ float(TN+FP)*100))\n    print('\\n')\n    print('Positive predictive value:',(TP \/ float(TP+FP)*100))\n    print('\\n')\n    print('Negative predictive value:',(TN \/ float(TN+ FN)*100))","27551d29":"#Function to get the probablities for all possible threshold\ndef optimum_threshold(y_pred):\n    numbers = [float(x)\/10 for x in range(10)]\n    for i in numbers:\n        y_pred[i]= y_pred.Prob.map(lambda x: 1 if x > i else 0)\n    return y_pred","0c7b0941":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ndef optimum_accuracy(df,op):\n    cutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\n    from sklearn.metrics import confusion_matrix\n\n    # TP = confusion[1,1] # true positive \n    # TN = confusion[0,0] # true negatives\n    # FP = confusion[0,1] # false positives\n    # FN = confusion[1,0] # false negatives\n\n    num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n    for i in num:\n        cm1              = confusion_matrix(df[op],df[i] )\n        total1           = sum(sum(cm1))\n        accuracy         = (cm1[0,0]+cm1[1,1])\/total1\n        speci            = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n        sensi            = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n        cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\n    return cutoff_df","05403759":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ndef Roc_plotting(df):\n    df.plot.line(x='prob', y=['accuracy','sensi','speci'])\n    plt.show()","e285eb11":"#Splitting the data into different parts \nX_train = df_train.drop(columns='Survived',axis=1).copy()\ny_train = df_train['Survived']","e169f0ca":"#Building a Logistic Regression model using Statsmodel\nimport statsmodels.api as sm\nlr    = sm.GLM(y_train,sm.add_constant(X_train),family=sm.families.Binomial())\nlr_1  = lr.fit()\nprint(lr_1.summary())","ca740309":"#Analyzing if Multicolinearlity exits in the data \nvif_validation(X_train)","2c002d34":"#Dropping the column 'Pclass' from the training data set \nX_train.drop(columns='Pclass',axis=1,inplace=True)\ndf_test.drop(columns='Pclass',axis=1,inplace=True)","df58190f":"#Rebuilding the model again with new_training dataframe \nlr    = sm.GLM(y_train,sm.add_constant(X_train),family=sm.families.Binomial())\nlr_2  = lr.fit()\nprint(lr_2.summary())","c102b958":"#Analyzing if Multicolinearlity exits in the data \nvif_validation(X_train)","8292a55f":"#Getting the prediction using train data & validating the model \ny_pred = prediction(lr_2,sm.add_constant(X_train),y_train,0.5)\ny_pred.head()","75a38be0":"#Evaluting the model accuracy \nvalidating_lr(y_train,y_pred['pred'])","e34803cd":"#Getting prediction for all the thresholds\ny_pred = optimum_threshold(y_pred)\ny_pred.head()","01735c49":"#Plotting the ROC to analyse & choose the best threshold to maximize the accuracy\ncutoff_df = optimum_accuracy(y_pred,'Survived')\ncutoff_df","997e3c62":"#Analysing the ROC \nRoc_plotting(cutoff_df)","1bd5f91d":"#Lets try threhold value somewhere around 0.45-0.48\n#Getting the prediction using train data & validating the model \ny_pred = prediction(lr_2,sm.add_constant(X_train),y_train,0.5)\ny_pred.head()\n#Evaluting the model accuracy \nvalidating_lr(y_train,y_pred['pred'])","61793e69":"df_test_sm      = sm.add_constant(df_test)\ndf_test_sm.head()","150c9ebe":"df_test[['Parch','SibSp']] = df_test[['Parch','SibSp']].astype('float64')","8a58116b":"#Concatenanting the passgerID & Survival rate \ndf_test_sm      = sm.add_constant(df_test)\nlr_2_test_pred  = test_prediction(lr_2,df_test_sm,0.5)\nsubmission_file = pd.DataFrame({'PassengerID':test_id['PassengerId'],'Survived':lr_2_test_pred['pred']})\nsubmission_file.to_csv('submission_1.csv', index=False)","46a69dd8":"#Building a Decision Tree & then fitting it to the RandomForest model \nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(max_depth=5,min_samples_split=150,min_samples_leaf=150)\ndt.fit(X_train, y_train)","87256f9f":"#Checking the accuracy of the Decision Tree \nfrom sklearn.metrics import confusion_matrix, accuracy_score\ny_train_pred = dt.predict(X_train)\nprint(accuracy_score(y_train, y_train_pred))\nconfusion_matrix(y_train, y_train_pred)\n\n#We can observe that we are getting almost similar accuracy so we will do some hyperparameter tunning","92f2da99":"#Tunning the Hyperparameters \nfrom sklearn.model_selection import GridSearchCV\ndt = DecisionTreeClassifier(random_state=42)\n# Create the parameter grid based on the results of random search \nparams = {\n    'max_depth': [2, 3, 4, 5, 6, 8, 10],\n    'min_samples_leaf': [30,50,100,150,200,250,300],\n    'min_samples_split': [30,50,100,150,200,250,300],\n    'criterion': [\"gini\", \"entropy\"]\n}\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator=dt, \n                           param_grid=params, \n                           cv=4, n_jobs=-1, verbose=1, scoring = \"accuracy\")\ngrid_search.fit(X_train, y_train)","de2e68dc":"#Building a RandomForest model \nscore_df = pd.DataFrame(grid_search.cv_results_)\nscore_df.nlargest(5,\"mean_test_score\")","3dd0d463":"grid_search.best_estimator_","01304091":"#Training the DecisionTree with best hyperparameter to get maximum accuracy \ndt = DecisionTreeClassifier(max_depth=10,min_samples_split=250,min_samples_leaf=100,criterion='entropy',random_state=42)\ndt.fit(X_train, y_train)\ny_train_pred = dt.predict(X_train)\nprint(accuracy_score(y_train, y_train_pred))\nconfusion_matrix(y_train, y_train_pred)\n#Our accuracy has slightly increased","669f9d7b":"#Concatenanting the passgerID & Survival rate \ndt_test_pred    = dt.predict(df_test)\nsubmission_file = pd.DataFrame({'PassengerID':test_id['PassengerId'],'Survived':dt_test_pred})\nsubmission_file.to_csv('submission_2.csv', index=False)","0650af8d":"#Building a RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=30, max_depth=10, max_features=5, random_state=100, oob_score=True)\nrf.fit(X_train, y_train)\nrf.oob_score_","7ea862ea":"#Hyperparameter tunning for RandomForest\nrf = RandomForestClassifier(random_state=42, n_jobs=-1)\nparams = {\n    'max_depth'        : [5,10,20],\n    'min_samples_leaf' : [50,100,150,200,250,300],\n    'min_samples_split': [100,150,200,250,300],\n    'n_estimators'     : [10, 25, 50, 100]\n}\ngrid_search = GridSearchCV(estimator=rf,\n                           param_grid=params,\n                           cv = 4,\n                           n_jobs=-1, verbose=1, scoring=\"accuracy\")\ngrid_search.fit(X_train, y_train)","a4ba45c6":"#Getting the best hyperparameters \ngrid_search.best_estimator_","44d4d5e7":"#Fitting the model to new hyperparameters\nrf = RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=250,\n                       n_estimators=50, n_jobs=-1, random_state=42,oob_score=True)\nrf.fit(X_train, y_train)\nrf.oob_score_","308de2fb":"#Checking the accuracy on the train dataset\ny_train_pred = rf.predict(X_train)\nprint(accuracy_score(y_train, y_train_pred))\nconfusion_matrix(y_train, y_train_pred)\n#We can clearly observe that there is a slight improvement in the overall accuracy of the model","46b22bdf":"#Checking the model accuracy on the test dataset \n#Concatenanting the passgerID & Survival rate \nrf_test_pred         = rf.predict(df_test)\nsubmission_file      = pd.DataFrame({'PassengerID':test_id['PassengerId'],'Survived':rf_test_pred})\nsubmission_file.to_csv('submission_3.csv', index=False)","c498c1c6":"#We will create an ensemble using Logistic Regression & RandomForest (Decision Tree)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree         import DecisionTreeClassifier\nfrom sklearn.ensemble     import StackingClassifier\nfrom sklearn.metrics      import r2_score,accuracy_score","32356de0":"#Creating the different models stack \nestimators = [\n    ('lr', LogisticRegression()),\n    ('dt', RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=250,\n                       n_estimators=50, n_jobs=-1, random_state=42,oob_score=True))\n]","e5134289":"#Stacking the models together\nstack_reg = StackingClassifier(estimators=estimators)\nstack_reg.fit(X_train, y_train)","0df221c9":"#Testing the accuracy of the model \ny_train_pred = stack_reg.predict(X_train)\naccuracy_score(y_train,y_train_pred)","ab07921c":"#Checking the model accuracy on the test dataset \n#Concatenanting the passgerID & Survival rate \nrf_test_pred         = stack_reg.predict(df_test)\nsubmission_file      = pd.DataFrame({'PassengerID':test_id['PassengerId'],'Survived':rf_test_pred})\nsubmission_file.to_csv('submission_4.csv', index=False)","da054710":"from sklearn.neighbors import KNeighborsClassifier\nKnn = KNeighborsClassifier(4)\nKnn.fit(X_train,y_train)","c6369c10":"#Testing the accuracy of the model \ny_train_pred = Knn.predict(X_train)\naccuracy_score(y_train,y_train_pred)","51f9eb74":"#Checking the model accuracy on the test dataset \n#Concatenanting the passgerID & Survival rate \nrf_test_pred         = Knn.predict(df_test)\nsubmission_file      = pd.DataFrame({'PassengerID':test_id['PassengerId'],'Survived':rf_test_pred})\nsubmission_file.to_csv('submission_5.csv', index=False)","9f9f67ff":"#Creating the different models stack \nestimators = [\n    ('lr', LogisticRegression()),\n    ('dt', RandomForestClassifier(max_depth=10, min_samples_leaf=50, min_samples_split=250,\n                       n_estimators=50, n_jobs=-1, random_state=42,oob_score=True)),\n    ('Knn',KNeighborsClassifier(4))\n]","b565bf7d":"#Stacking the models together\nstack_reg = StackingClassifier(estimators=estimators)\nstack_reg.fit(X_train, y_train)","ba4c2328":"#Testing the accuracy of the model \ny_train_pred = stack_reg.predict(X_train)\naccuracy_score(y_train,y_train_pred)","bbb67579":"#Checking the model accuracy on the test dataset \n#Concatenanting the passgerID & Survival rate \nrf_test_pred         = stack_reg.predict(df_test)\nsubmission_file      = pd.DataFrame({'PassengerID':test_id['PassengerId'],'Survived':rf_test_pred})\nsubmission_file.to_csv('submission_6.csv', index=False)","de9114ce":"#### 1. We have two situations here wherin we have column 'Pclass' with high VIF & low P-value.\n#### 2. Second column is 'Age' with high p-value & low VIF.\n#### 3. We will eliminate the column 'Pclass' as it has high VIF & also it is redundant as 'Fare' column is already present in the training dataset.","ca674ea4":"* ##### If you are in the intermmediate level & trying to hone your skills in algorithims such as Random forest & Logistic Regression this notebook is for you.\n* #####  I will be using Logistic Regression,Random forest at the first and then use ensembles to improvise the accuracy further.\n\n**Topics that we will cover in this notebook are:-**\n1. Data cleaning & Preprocessing by creating dummies, Scaling.\n2. Model building using Logistic Regression, Random Forest & Ensembles.\n3. Using RFE(Recursive feature elimination) for feature elimination.\n4. VIF (Variation inflation Factor) to detect multicolinearlity.\n5. Automatic Hyperparameter tunning using Sklearn.\n6. Model evalution using accuracy, F1 score, Recall,precision.","2cd48e98":"#### 4. EXPLORATORY DATA ANALYSIS ","2471b025":"#### Th accuracy using the DecisionTree model is only 77.58% so lets try to build a RandomForest model","3f61d862":"#### 2. DATA CLEANING","1485224f":"#### 3. DATA PREPROCESSING ","83edc7df":"#### 1. Let's build an ensemble using LogisticRegression, KNN, RandomForest algorithm","0b2166fb":"#### 1. DATA IMPORTING & ANALYZING","49193420":"#### If you are curious to know why we did ony tranform for test data set & fit_tranform to train.\nvisit the link below [https:\/\/towardsdatascience.com\/what-and-why-behind-fit-transform-vs-transform-in-scikit-learn-78f915cf96fe](http:\/\/)","7f84c0fa":"##### * 1. As we can observe that the Multicolinearlity has been reduced significantly & the p-value of all the features is less & they all are sinificant.*","ac49b32a":"* #### Now, that we have used RandomForest as well and did not see any significant improvement in the model accuracy lets use Ensembles to predict the survival rate","b28abd8d":"#### 1. We are ready for the End game now, I will be using the below approach to build the best model.\n#### *     a. Building the best 'Logistic Regression model', submit pred & check accuracy.\n#### *     b. Building the best ' Random forest' model, submit pred & check accuracy.\n#### *     c. Finally, building a 'Ensemble' to check if we can maximize the accuarcy. ","e8d2e543":"* #### Here we getting the test accuracy of 77.69.\n* #### We will also try knn for classifiying.","520d05d8":"* #### As of now lr_2 is the best model we are getting with 76% train_accuracy, Let's submit & check the test accuracy of around 79.38% I am not sure how this is possible but lets move on to build a Random forest model to further improve the accuracy","3b01313d":"#### 5. MODEL BUILDING & Evaluation"}}