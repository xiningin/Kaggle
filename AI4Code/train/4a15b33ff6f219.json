{"cell_type":{"c051541c":"code","f38df538":"code","52a2bc24":"code","422dffc3":"code","1bf22d32":"code","95bd90ca":"code","05f49831":"code","91038da2":"code","29f01128":"code","a1695427":"code","b4f04401":"code","de10a0d1":"code","616d6c70":"code","8d9aedae":"code","263c8f03":"code","4923e23e":"code","c22da91d":"code","ce1514a8":"code","5942f234":"code","1a07ff1a":"code","d0ff582d":"markdown","70c97fc4":"markdown","b8625ed4":"markdown","d0e40325":"markdown","4753e646":"markdown","76da6125":"markdown","19e5126c":"markdown","128c5064":"markdown","25773d5e":"markdown","f01462a9":"markdown","0c9c185f":"markdown","11435ac8":"markdown","4a8b2596":"markdown","ed1fecd2":"markdown","c4b4c1cb":"markdown","255aa6bb":"markdown","6a76547d":"markdown","ed01a7b8":"markdown","2a57525c":"markdown","9a979bd4":"markdown","0b41abbe":"markdown","dfbe660c":"markdown","c2e4951f":"markdown"},"source":{"c051541c":"import time\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntrain = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ndisplay(train)\n\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\ndisplay(test)","f38df538":"duplicates = pd.concat(x for _, x in train.groupby([\"text\"]) if len(x) > 1)\nwith pd.option_context(\"display.max_rows\", None, \"max_colwidth\", 240):\n    display(duplicates[[\"id\", \"target\", \"text\"]])","52a2bc24":"train.drop(\n    [\n        6449, 7034, 3589, 3591, 3597, 3600, 3603, 3604, 3610, 3613, 3614, 119, 106, 115,\n        2666, 2679, 1356, 7609, 3382, 1335, 2655, 2674, 1343, 4291, 4303, 1345, 48, 3374,\n        7600, 164, 5292, 2352, 4308, 4306, 4310, 1332, 1156, 7610, 2441, 2449, 2454, 2477,\n        2452, 2456, 3390, 7611, 6656, 1360, 5771, 4351, 5073, 4601, 5665, 7135, 5720, 5723,\n        5734, 1623, 7533, 7537, 7026, 4834, 4631, 3461, 6366, 6373, 6377, 6378, 6392, 2828,\n        2841, 1725, 3795, 1251, 7607\n    ], inplace=True\n)\nduplicates = pd.concat(x for _, x in train.groupby([\"text\"]) if len(x) > 1)\nwith pd.option_context(\"display.max_rows\", None, \"max_colwidth\", 240):\n    display(duplicates[[\"id\", \"target\", \"text\"]])","422dffc3":"train.drop(\n    [\n        4290, 4299, 4312, 4221, 4239, 4244, 2830, 2831, 2832, 2833, 4597, 4605, 4618, 4232, 4235, 3240,\n        3243, 3248, 3251, 3261, 3266, 4285, 4305, 4313, 1214, 1365, 6614, 6616, 1197, 1331, 4379, 4381,\n        4284, 4286, 4292, 4304, 4309, 4318, 610, 624, 630, 634, 3985, 4013, 4019, 1221, 1349, 6091, 6094, \n        6103, 6123, 5620, 5641\n    ], inplace=True\n)","1bf22d32":"from gensim.parsing.preprocessing import stem_text\n\ndef clean_location_keyword(df):\n    df[\"location\"] = df[\"location\"].astype(\"string\").str.lower()\n    df[\"location\"].fillna(\"\", inplace=True)\n    df[\"keyword\"] = df[\"keyword\"].astype(\"string\").str.lower()\n    df[\"keyword\"].replace(regex=r\"\\%20\", value=\" \", inplace=True)\n    df[\"keyword\"].fillna(\"\", inplace=True)\n    df[\"keyword\"] = df[\"keyword\"].apply(stem_text)\n\nclean_location_keyword(train)\nclean_location_keyword(test)","95bd90ca":"counts = pd.DataFrame(train[\"target\"].value_counts())\ncounts.rename(columns={\"target\": \"Samples\"}, index={0: \"Not Real\", 1: \"Real\"}, inplace=True)\nax = sns.barplot(x=counts.index, y=counts.Samples)\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(\n        x=p.get_x()+(p.get_width()\/2),\n        y=height,\n        s=round(height),\n        ha=\"center\"\n    )","05f49831":"with pd.option_context(\"display.max_rows\", None):\n    display(train[\"keyword\"].unique())","91038da2":"def collapse_keywords(x):\n    if x == \"arsonist\":\n        return \"arson\"\n    if x == \"blaze\":\n        return \"ablaz\"\n    if x == \"bloodi\":\n        return \"blood\"\n    if x == \"build burn\" or x == \"burn build\":\n        return \"build on fire\"\n    if x == \"blew up\":\n        return \"blown up\"\n    if x == \"colli\":\n        return \"collid\"\n    if x == \"explo\":\n        return \"explod\"\n    if x == \"hailstorm\":\n        return \"hail\"\n    if x == \"injuri\":\n        return \"injur\"\n    if x == \"panick\":\n        return \"panic\"\n    if x == \"suicid bomber\":\n        return \"suicid bomb\"\n    if x == \"wildfir\":\n        return \"wild fire\"\n    return x\n\ntrain[\"keyword\"] = train[\"keyword\"].apply(lambda x: collapse_keywords(x))\ntest[\"keyword\"] = test[\"keyword\"].apply(lambda x: collapse_keywords(x))\nwith pd.option_context(\"display.max_rows\", None):\n    display(train[\"keyword\"].unique())","29f01128":"with pd.option_context(\"display.max_rows\", None):\n    display(pd.DataFrame(data=train[[\"id\", \"keyword\", \"target\"]].groupby([\"keyword\", \"target\"]).count()).rename(columns={\"id\": \"count\"}).head(50))","a1695427":"train[\"keyword\"] = train[\"keyword\"].astype(\"category\")\ntrain[\"keyword_cat\"] = train[\"keyword\"].cat.codes\ntest[\"keyword_cat\"] = test[\"keyword\"].apply(lambda x: train[\"keyword\"].cat.categories.get_loc(x) if x in train[\"keyword\"].cat.categories.to_list() else 0)\ntrain","b4f04401":"print([location for location in train[\"location\"]][:500])","de10a0d1":"import re\n\nfrom pycountry import subdivisions\n\ndef clean_state_country(df):\n    subs = [subdivision.name.lower() for subdivision in subdivisions]\n    countries = [subdivision.country_code for subdivision in subdivisions]\n    country = []\n    state = []\n    location_spam = []\n    for _, row in df.iterrows():\n        match_found = False\n        is_spam = 0\n        if row[\"location\"] == \"\":\n            country.append(\"\")\n            state.append(\"\")\n        else:\n            for index, subdivision in enumerate(subs):\n                if subdivision in row[\"location\"]:\n                    country.append(countries[index])\n                    state.append(subdivision)\n                    match_found = True\n                    break\n            if not match_found:\n                country.append(\"\")\n                state.append(\"\")\n                split_data = row[\"location\"].replace(\" \", \"\").split(\",\")\n                is_spam = 1\n                if len(split_data) == 2:\n                    if re.match(r\"[\\-]*[0-9]+\\.[0-9]+\", split_data[0]) and re.match(r\"[\\-]*[0-9]+\\.[0-9]+\", split_data[0]):\n                        is_spam = 0\n        location_spam.append(is_spam)\n    df[\"country\"] = country\n    df[\"state\"] = state\n    df[\"location_spam\"] = location_spam\n    \nclean_state_country(train)\nclean_state_country(test)\n\ntrain[\"country\"] = train[\"country\"].astype(\"category\")\ntrain[\"country_cat\"] = train[\"country\"].cat.codes\ntrain[\"state\"] = train[\"state\"].astype(\"category\")\ntrain[\"state_cat\"] = train[\"state\"].cat.codes\n\n# Apply the same category codes to the testing dataset\ntest[\"state_cat\"] = test[\"state\"].apply(lambda x: train[\"state\"].cat.categories.get_loc(x) if x in train[\"state\"].cat.categories.to_list() else 0)\ntest[\"country_cat\"] = test[\"country\"].apply(lambda x: train[\"country\"].cat.categories.get_loc(x) if x in train[\"country\"].cat.categories.to_list() else 0)\ndisplay(train[(train[\"location_spam\"] == 1)])\ndisplay(train[(train[\"country_cat\"] != 0)])","616d6c70":"with pd.option_context(\"display.max_rows\", None):\n    display(pd.DataFrame(data=train[[\"id\", \"country\", \"target\"]].groupby([\"country\", \"target\"]).count()).rename(columns={\"id\": \"count\"}).head(50))","8d9aedae":"with pd.option_context(\"display.max_rows\", None):\n    display(pd.DataFrame(data=train[[\"id\", \"state\", \"target\"]].groupby([\"state\", \"target\"]).count()).rename(columns={\"id\": \"count\"}).head(50))","263c8f03":"import re\n\ndef engineer_features(df):\n    df[\"total_length\"] = df[\"text\"].apply(len)\n    df[\"avg_word_length\"] = df[\"text\"].apply(lambda x: round(sum(len(word) for word in x.split()) \/ len(x.split())))\n    df[\"num_ats\"] = df[\"text\"].apply(lambda x: x.count(\"@\"))\n    df[\"num_hashtags\"] = df[\"text\"].apply(lambda x: x.count(\"#\"))\n    df[\"num_numeric\"] = df[\"text\"].apply(lambda x: len(re.findall(r\"\\w[0-9,]+\\w\", x)))\n    df[\"num_urls\"] = df[\"text\"].apply(lambda x: x.count(\"http\"))\n    df[\"num_timestamps\"] = df[\"text\"].apply(lambda x: len(re.findall(r\"[0-9]+:[0-9]+\", x)))\n\nengineer_features(train)\nengineer_features(test)\ntrain","4923e23e":"from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation, strip_multiple_whitespaces, strip_numeric, stem_text\n\ndef normalize_text(df):\n    normalized_text = []\n\n    for _, row in df.iterrows():\n        new_text = row[\"text\"].lower()\n        new_text = remove_stopwords(new_text)\n        new_text = re.sub(r\"http\\S+\", \"\", new_text)\n        new_text = strip_punctuation(new_text)\n        new_text = strip_numeric(new_text)\n        new_text = strip_multiple_whitespaces(new_text)\n        new_text = stem_text(new_text)\n        normalized_text.append(new_text.strip())\n\n    df[\"normalized_text\"] = normalized_text\n\nnormalize_text(train)\nnormalize_text(test)\ntrain","c22da91d":"import gc\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\n\nvectorizer = TfidfVectorizer()\nskf = StratifiedKFold(n_splits=5, random_state=2020, shuffle=True)\n\nfeatures = [\n    \"keyword_cat\", \"country_cat\", \"state_cat\", \"location_spam\", \"total_length\", \"avg_word_length\", \n    \"num_ats\", \"num_hashtags\", \"num_numeric\", \"num_urls\", \"num_timestamps\"\n]\n\nparams = {\n    \"learning_rate\": 0.01, \n    \"verbose\": 100, \n    \"random_state\": 2020, \n    \"metric\": \"average_precision\", \n    \"verbose\": -1,\n    \"boosting_type\": \"dart\",\n    \"num_leaves\": 40,\n    \"cat_column\": \"0,1,2,3\", # array features 0 (keyword_cat), 1 (country_cat), 2 (state_cat), 3 (location_spam) are categorical\n    \"num_iterations\": 200\n}\n\nimportances = pd.DataFrame()\nbest_score = 0.0\nbest_model = None\nbest_sgd = None\n\nfor fold, (train_index, test_index) in enumerate(skf.split(train, train[\"target\"])):\n    print(\"-------> fold {} <--------\".format(fold + 1))\n    print(\": Build regression model\")\n    x_train, x_valid = pd.DataFrame(train.iloc[train_index]), pd.DataFrame(train.iloc[test_index])\n    y_train, y_valid = train[\"target\"].iloc[train_index], train[\"target\"].iloc[test_index]\n    \n    x_train_tfidf = vectorizer.fit_transform(x_train[\"normalized_text\"]).astype(np.float32)\n    x_valid_tfidf = vectorizer.transform(x_valid[\"normalized_text\"]).astype(np.float32)\n    \n    regressor = LogisticRegression(random_state=2020, max_iter=1000)\n    regressor.fit(x_train_tfidf, y_train)\n    \n    x_train_features = pd.DataFrame(x_train[features])\n    x_train_features[\"sgd_score\"] = regressor.predict(x_train_tfidf)\n    x_valid_features = pd.DataFrame(x_valid[features])\n    x_valid_features[\"sgd_score\"] = regressor.predict(x_valid_tfidf)\n    \n    print(\": Build GBM model\")\n    model = lgb.LGBMClassifier(\n        **params,\n    )\n    model.fit(\n        x_train_features, \n        y_train,\n        eval_set=[(x_valid_features, y_valid)],\n        verbose=100,\n    )\n\n    train_predictions = model.predict(x_valid_features)\n    \n    imp_df = pd.DataFrame()\n    imp_df['feature'] = x_valid_features.columns\n    imp_df['gain'] = model.booster_.feature_importance(importance_type='gain')\n    imp_df['fold'] = fold + 1\n    importances = pd.concat([importances, imp_df], axis=0, sort=False)\n    print(classification_report(y_valid, train_predictions, target_names=[\"Not Real\", \"Real\"]))\n    score = model.score(x_valid_features, y_valid)\n    if score > best_score:\n        print(\"--> This model is the best so far {:0.5}\".format(score))\n        best_model = model\n        best_score = score\n        # Best is 0.82565 - Logistic Regression default","ce1514a8":"import warnings\nwarnings.simplefilter('ignore', FutureWarning)\n\nimportances['gain_log'] = np.log1p(importances['gain'])\nmean_gain = importances[['gain', 'feature']].groupby('feature').mean()\nimportances['mean_gain'] = importances['feature'].map(mean_gain['gain'])\n\nplt.figure(figsize=(14, 35))\n_ = sns.barplot(x='gain_log', y='feature', data=importances.sort_values('mean_gain', ascending=False))","5942f234":"vectorizer = TfidfVectorizer(binary=True)\n\nprint(\": Build regression model\")\ntrain_tfidf = vectorizer.fit_transform(train[\"normalized_text\"]).astype(np.float32)\n\nregressor = LogisticRegression(random_state=2020, max_iter=1000)\nregressor.fit(train_tfidf, train[\"target\"])\n\ntrain_features = pd.DataFrame(train[features])\ntrain_features[\"sgd_score\"] = regressor.predict(train_tfidf)\n\nprint(\": Build GBM model\")\nmodel = lgb.LGBMClassifier(\n    **params,\n)\nmodel.fit(\n    train_features, \n    train[\"target\"],\n    verbose=100,\n)","1a07ff1a":"test_tfidf = vectorizer.transform(test[\"normalized_text\"]).astype(np.float32)\ntest_features = pd.DataFrame(test[features])\ntest_features[\"sgd_score\"] = regressor.predict(test_tfidf)\npredictions = model.predict(test_features)\nsubmission = pd.DataFrame({\"id\": test[\"id\"], \"target\": predictions})\nsubmission.to_csv(\"submission.csv\", index=False)","d0ff582d":"It looks like we have quite a few duplicates. In some instances, the duplicates resolve to the same target class, but in others such as duplicate indexes `5620` and `5641`, we have the same tweet belonging to two different classes. For those instances where the tweet belongs to the same class, we can simply delete the duplicates.","70c97fc4":"# 7. Training and Validating the Classifier\n\nNow it's time to acutally build a classifier and see how well it does. To evaluate how well our features are working, we're going to split up our training data into a training set and a validation set. We'll do this 5 times for a 5-fold cross validation. Our difference between sets gives us an idea how robust our model actually is to variations in training data. To handle our textual data, we're going to treat it as a bag-of-words and use a regression model. To represent the words, we'll use a TF\/IDF vector (term frequency \/ inverse document frequency). Once we build the regressor, we'll use that to make predictions, and use the predicted outputs, along with our other features in a light GBM model to make actual predictions on the test data. ","b8625ed4":"# 4. Looking at Location\n\nLet's take look at the first 500 entries in the location field and see what we're working with.","d0e40325":"# 6. Pre-processing Text\n\nFor our textual analysis to be useful, we'll have to perform some pre-processing on the text first to make it easier to work with. Here's what we're going to do:\n\n* Remove words that don't have any value, such as `the`, `of`, `and` (stopword removal)\n* We're going to strip out any links since they are `tco` encoded for Twitter\n* We'll remove all punctuation\n* We'll remove the numerics\n* We'll remove multiple whitespaces\n* Stem the text","4753e646":"Now let's look at the keyword in relation to whether their target is real `1` or not real `0`. Let's just take a look at the first 50 rows or so.","76da6125":"# 2. Looking at Class Imbalance\n\nIt looks like we have 7,613 training samples. Let's see how many tweets we have that are examples of disaster versus those that are not. What we're looking at is whether or not we have a balance between samples that are both real examples of disasters, and those that are not.","19e5126c":"Now we're facing a challenge. We could keep one duplicate with one target class, but we don't have access to the method by which the dataset creators used to mark up real versus not real disaster tweets. They may have had access to more information than us, so we have to be careful if we alter the dataset - we could introduce personal bias. While it may be tempting to try to keep some of the data (e.g. `that horrible sinking feeling when you've been at home on your phone for a while and you realise its been on 3G this whole time` seems like it should be marked as `not real`), the better approach is to simply delete the offending duplicates. While this cuts our training size down, we ensure we haven't inadventently introduced bias to the dataset.","128c5064":"Here is the code to run the predictions on the test data, and build the submission file.","25773d5e":"# 3. Looking at Keywords\n\nLet's take a closer look at what kind of information we have in the `keyword` field, specifically what unique values we have.","f01462a9":"# 9. Building and Submitting the Final Model\n\nLet's go ahead and build a model that uses all of the data, and takes all the features we've examined so far. Once the model is built, we can submit the result.","0c9c185f":"# 1.2 Keyword and Location Normalization\n\nIt looks like we need to do a little cleanup here. Both `keyword` and `location` fields are meant to be interpreted as strings. While we're at it, we should probably convert them all to lowercase for ease of processing. We'll also fill all missing values (`<NA>` values) in `keyword` and `location` with the empty string. If we look at the `keyword` strings, we find that some entries have `%20` instead of a space. We should also stem the `keyword` field so we can collapse similar keywords into a single keyword (for example, `death` and `deaths` would become `death`). Let's go ahead and make those changes to the dataframe. ","11435ac8":"There are quite a number of entries for which we have no country information. The first row shows us the empty string, which is the number of rows without country information. The total is 5,353 entries, which is more than half of our available training data. For entries with countries, we're seeing somewhat equal splits between real and not real disasters. This is to be expected, as people geotag tweets from all countries whether or not they are actually disasters. It's unlikely that real disasters would exclusively be geotagged. This is probably the same for state information. Let's take a look.","4a8b2596":"# 1. Importing the Data\n\nThe first step in the process is to import our training data so we can see what kinds of information we have to work with. For this project, we'll start by importing the entire training dataset into a single Pandas dataframe.","ed1fecd2":"# 8. Feature Performance\n\nWe can take a look and see how our various features are performing.","c4b4c1cb":"There are a few things we can collapse. For example, `arson` and `arsonist` can be collapsed to `arson`. ALet's go ahead and make a few of these changes.","255aa6bb":"In some instances we have countries, others include states, and some include cities. Yet others include junk data such as `global` as well as `Twitter Lockout in progress`. We'll need a way to clean and normalize this data so that it's a little more useful to us. Normalizing this data may turn out to be beneficial, but based on how messy the field is, it may not be worthwhile to spend huge amounts of time trying to clean it. As it stands, let's see if we can use the Python package `pycountry` to help us sort out some of this data. What we'll do is try and sort out real locations from ones that are not real. We can compare what is in the `location` field to subdivision data from `pycountry`. If we get a match, we'll save the state and country to some new columns on the dataframe. If we don't get a match, we'll try and do a little more processing. If we have two floating point numbers, we probably have a set of geo coordinates, so we can mark that as not being location spam. Other than that, we can't do much with the data, so we'll flag it as probable spam.","6a76547d":"We can see that there are certain keywords that are strongly tied to one class. For example, `airplan accid` is very strongly associated with the real disaster target - we see it appear 35 times, and 30 of those times it is a real disaster, while only 5 times it is not. This is good news, since it suggests there are likely keywords here that will provide separation between classes.\n\nOnce last thing we will have to do with the keywords is categorize them. Most machine learning algorithms expect categorical data to be in the form of an numeric. We'll convert the keyword into a category type, and save the category code as a new column, and use that for our machine learning technique.","ed01a7b8":"For this particular set of data, it looks like we have a slightly skewed distribution between the two classes. In this instance, we'll have to be careful with any machine learning algorithm we use, since we have more tweets that do not pertain to disasters than we do that contain real disasters. ","2a57525c":"# Introduction\n\nThe *Real or Not? NLP with Disaster Tweets* competitions offers a neat opportunity to see how different approaches to natural language processing work when compared to one another. In this notebook, we'll look at how to start examining NLP data and performing some rudimentary second-order feature engineering. Here's a breakdown of what this notebook covers:\n\n1. Perform an initial exploration of some simple fields.\n2. Clean and normalize the data set.\n3. Extract first-order features and examine how useful they are.\n4. Perform rudimentary natural language processing on the text field.\n5. Evaluate our natural language model.\n6. Use the model and make predictions that we can submit to the competition.","9a979bd4":"Now that we have country and state information, we should be able to look at those fields the same way we examined keywords. Let's take a look what happens.","0b41abbe":"As predicted, we're missing the same amount of state information. Looking at the low counts for each state, we're probably not going to get very useful information from this field, but we'll keep it intact for now.","dfbe660c":"# 1.1 Eliminating Duplicates\n\nOne thing we should do is check to see if we have duplicated or conflicting data. Here's an easy way to check for textual duplicates against the `target` - which is the class we're trying to predict.","c2e4951f":"# 5. Simple Feature Engineering\n\nBefore we look directly at the text as a feature, let's think about some of the other first-order information we can extract from it. Here are a few features that may be informative:\n\n* Total length of the text\n* Average word length\n* Number of `@` mentions\n* Number of hashtags\n* Number of numeric values in the text (excluding timestamps)\n* Number of URLs in the text\n* Number of timestamps in the text\n\nLet's go ahead and extract these fields."}}