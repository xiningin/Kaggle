{"cell_type":{"06b50961":"code","befaa277":"code","50770635":"code","e1a20f2c":"code","68f3b3c3":"code","880d282f":"code","8c3387ac":"code","7883c20e":"code","6b6b0cdc":"code","25eef160":"code","0addb871":"code","861e4f52":"code","c3757420":"code","df0e8f3c":"code","dc2a0dc3":"code","27618421":"code","14fda769":"code","0595f96a":"code","f7fbd3ec":"markdown","5f85ce8e":"markdown","9fba3714":"markdown","94b8ebf9":"markdown","51525b8a":"markdown","81e30466":"markdown","5ab703a5":"markdown","d8ce4292":"markdown","c4132a0b":"markdown","2ead9f0b":"markdown","e1a898e7":"markdown","3aa03a88":"markdown","92e379b7":"markdown","ed494c7c":"markdown","0929b64d":"markdown","6f4cea53":"markdown","45043ccc":"markdown","67754734":"markdown","f96f945b":"markdown"},"source":{"06b50961":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\ndon = pd.read_csv(\"..\/input\/Donations.csv\")\n# Any results you write to the current directory are saved as output.","befaa277":"#Find Donors who gave donations to multiple projects \nimport base64\nimport numpy as np\nimport pandas as pd\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom collections import Counter\nfrom scipy.misc import imread\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nprojects = pd.read_csv(\"..\/input\/Projects.csv\")\n\nds = projects.loc[projects['Project Current Status'].isin(['Fully Funded','Expired','Live'])]\ndf3 = ds.merge(don, on = \"Project ID\" )\ndf3['Donor ID'] = df3['Donor ID'].str.strip()\n\n#consider only donations made to different projects, as we are interested in what makes the donor open his wallet\n\ndf4 = df3.drop_duplicates(subset=['Project ID', 'Donor ID'])\ndf4['Donor ID'].value_counts()","50770635":"# vectorizer takes care of stopwords, but we seek lemmatisation as well... hence this class to override the function inside the vectoriser\n\nfrom nltk.stem import WordNetLemmatizer\nlemm = WordNetLemmatizer()\nclass LemmaCountVectorizer(CountVectorizer):\n    def build_analyzer(self):\n        analyzer = super(LemmaCountVectorizer, self).build_analyzer()\n        return lambda doc: (lemm.lemmatize(w) for w in analyzer(doc))\n","e1a20f2c":"\n# create a sparse matrix for lda \ndf4_sample = df4.sample(n = 100000, random_state = 69)\ntext = list(df4_sample[\"Project Essay\"])\n# Calling our overwritten Count vectorizer\ntf_vectorizer = LemmaCountVectorizer(max_df=0.65, \n                                     min_df=2,\n                                     stop_words='english',\n                                     decode_error='ignore')\ntf = tf_vectorizer.fit_transform(text)","68f3b3c3":"feature_names = tf_vectorizer.get_feature_names()\ncount_vec = np.asarray(tf.sum(axis=0)).ravel()\nzipped = list(zip(feature_names, count_vec))\nx, y = (list(x) for x in zip(*sorted(zipped, key=lambda x: x[1], reverse=True)))\n# Now I want to extract out on the top 15 and bottom 15 words\nY = np.concatenate([y[0:15], y[-16:-1]])\nX = np.concatenate([x[0:15], x[-16:-1]])\n\n# Plotting the Plot.ly plot for the Top 50 word frequencies\ndata = [go.Bar(\n            x = x[0:50],\n            y = y[0:50],\n            marker= dict(colorscale='Jet',\n                         color = y[0:50]\n                        ),\n            text='Word counts'\n    )]\n\nlayout = go.Layout(\n    title='Top 50 Word frequencies after Preprocessing'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')\n\n# Plotting the Plot.ly plot for the Top 50 word frequencies\ndata = [go.Bar(\n            x = x[-100:],\n            y = y[-100:],\n            marker= dict(colorscale='Portland',\n                         color = y[-100:]\n                        ),\n            text='Word counts'\n    )]\n\nlayout = go.Layout(\n    title='Bottom 100 Word frequencies after Preprocessing'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')","880d282f":"lda = LatentDirichletAllocation(n_components=63, max_iter=5,\n                                learning_method = 'online',\n                                learning_offset = 50.,\n                                random_state = 0)\nlda.fit(tf)","8c3387ac":"text[890]","7883c20e":"tf1 = tf_vectorizer.transform([text[890]])\ndoc_topic = lda.transform(tf1)\ndoc_topic\nimport numpy   \ntopic_high = numpy.where(doc_topic > 0.05)\nnumpy.where(doc_topic > 0.05)\n\ntopic_high = list(topic_high)[1]\ntop_tup = tuple(map(lambda x:(x,doc_topic[0,x]),topic_high))\nsor_ry = sorted(top_tup, key=lambda tup: tup[1], reverse = True)\n","6b6b0cdc":"from wordcloud import WordCloud, STOPWORDS\n\ntf_feature_names = tf_vectorizer.get_feature_names()\n\nfirst_topic = lda.components_[sor_ry[0][0]]\nfirst_topic_words = [tf_feature_names[i] for i in first_topic.argsort()[:-50 - 1 :-1]]\n\nfirstcloud = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(first_topic_words))\nplt.imshow(firstcloud)\nplt.axis('off')\nplt.show()","25eef160":"sixtieth_topic = lda.components_[sor_ry[1][0]]\nsixtieth_topic_words = [tf_feature_names[i] for i in sixtieth_topic.argsort()[:-50 - 1 :-1]]\n\nscloud = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(sixtieth_topic_words))\nplt.imshow(scloud)\nplt.axis('off')\nplt.show()","0addb871":"supp_essays = df4[df4['Donor ID'] == df4_sample.iloc[890]['Donor ID']]\nsupp_essays","861e4f52":"text_test = list(supp_essays['Project Essay'])\n\ntf1 = tf_vectorizer.transform([text_test[1]])\n\ndoc_topic = lda.transform(tf1)\nprint(doc_topic)\nimport numpy   \nnumpy.where(doc_topic > 0.05)","c3757420":"text_test = list(supp_essays['Project Essay'])\n\ntf1 = tf_vectorizer.transform([text_test[2]])\n\ndoc_topic = lda.transform(tf1)\nprint(doc_topic)\nimport numpy   \nnumpy.where(doc_topic > 0.05)\ntype(doc_topic)","df0e8f3c":"supp_essays['Project Essay'].loc[supp_essays['Donation Amount'].idxmax()]","dc2a0dc3":"# To test our hypothesis, we are going to break down the essays into text topics, and then see if there is a relation between the amount donated and the topics relevance\n\ntf1 = tf_vectorizer.transform(text_test)\ndoc_topic = lda.transform(tf1)\nsupp_cp = supp_essays\nd_copy = doc_topic\ndf = pd.DataFrame(d_copy)\nsupp_cp = supp_cp.reset_index(drop=True)\ndataset = pd.concat([supp_cp['Donation Amount'],df],axis = 1)\n\n","27618421":"dataset","14fda769":"topics = dataset.drop(['Donation Amount'], axis = 1)","0595f96a":"topics.mean().plot(kind='bar', figsize=(20,10))","f7fbd3ec":"<h1> Ranking the Essays to Send <\/h1>\n\n\n","5f85ce8e":"<h1><a id='3'> Analysis <\/a> <\/h1>","9fba3714":"Now, have a look at the topics LDA is picking up ( 0.05 implies, the essay containts at least 5% of words that come from that topic's bag of words)","94b8ebf9":"Sadly we do not have the essays the donor did not donate to; i.e; saw the email but chose not to donate. \n\nSo as a poor substitute to test our hypothesis, we are going to assume the amount donated reflects the interest in the essay and higher the amount donated, greater the passion for the cause.\n\nSo at this moment, we have a couple of Essays, We are going to rank the essays by the similarity to the essay that received the highest donation; And then we can see if they received donations in the similar order","51525b8a":"More than a text analytics problem, this is a collaborative filtering problem; where the objective is to identify which topics are relevant to the donor. Also, It's very intutive to assume that we are passionate about simlar things. Eg. Chivas and Jamesons, Frost and Plath, LOTR and Harry Potter.\n\nSo we are going to test the hypothesis that Donors donate for causes that have some similarity in terms of their topics \n. \nTo test this hypothesis:\n1. Shortlist donors who have contributed to multiple projects\n2. Is there any relation at all between the topics donated to by the same donor?\n\nIdeally we should have information to compare to essays the donor did not contribute to, but we don't.","81e30466":"To test our hypothesis, lets pick up 3 essays the same donor ( who donated for project 889) has donated...","5ab703a5":"<h1><a id='1'> Context\/Pre Processing <\/a><\/h1>","d8ce4292":"Topic #3","c4132a0b":"\nSo we have above the multiple projects which received donations from the same donor. So the question we are trying to answer is what's the relation between those projects? So this is a two step proces; first use LDA to break down the essays into individual topics. Second, try to understand the relation between the projects if the donors contributed multiple times. ","2ead9f0b":"Lets randomly pick up topic no. 867\n","e1a898e7":"Topic #2","3aa03a88":"**LDA**\n(from wikipedia)\n\nIn natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics. LDA is an example of a topic model and was first presented as a graphical model for topic discovery by David Blei, Andrew Ng, and Michael I. Jordan in 2003.\n\n","92e379b7":"<h1><a id='2'> LDA <\/a><\/h1>","ed494c7c":"Once we have the sparse matrix with the necessary preprocessing done, time to fit the LDA. Run the code below and go pour yourself a glass of Chivas and contemplate the meaninglessness of life.... ","0929b64d":"Based on content-proportion:\nTopic#1","6f4cea53":"**Path**\n - <a href='#1'>Context\/Pre Processing<\/a>  \n - <a href='#2'> LDA Model<\/a>\n - <a href='#3'>Analysis<\/a>\n - <a href='#4'>Further Reading<\/a>\n \n P.S This is a work in Progress, will be updating whenever I have time. This notebook borrows code from https:\/\/www.kaggle.com\/arthurtok\/spooky-nlp-and-topic-modelling-tutorial\n \n P.P.S I am not a programmer ( if that's not obvious by how ugly this notebook looks), some pieces of this code might not be the most efficient way.","45043ccc":"<h1><a id='4'> Further reading <\/a><\/h1>\nhttp:\/\/blog.echen.me\/2011\/08\/22\/introduction-to-latent-dirichlet-allocation\/\n\nhttp:\/\/www.jmlr.org\/papers\/volume3\/blei03a\/blei03a.pdf\n","67754734":"So for our model dataset, our dependent variable is the amount donated, and our independent variables are the LDA topic distribution; \nBefore we jump into any kind of conclusion, let us look at the mean distribution across the topics,;","f96f945b":"So it's very apparent that some topics the donor has donated to more than the remaining topics. Let us try assigning a monetary value to the topics. The simplest way would be to run a linear regression, where the coefficient would imply the increase in donation amount with a 1% increase in the topic proportion.  Will be adding this and similarity to the highest donated essay soon.\n"}}