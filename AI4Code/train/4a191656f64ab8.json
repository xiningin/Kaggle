{"cell_type":{"293c2c17":"code","ac52baa1":"code","cf13cf1c":"code","cd9d0cbe":"code","c99d6dbb":"code","f5d88700":"code","08f80ff0":"code","e30090a8":"code","d9aea372":"code","5ba8e9eb":"code","e8973997":"code","2e2ed1da":"code","f6325ff9":"code","1e46bc2f":"code","e2a552c3":"code","0dfc4c6b":"code","4cce8f4e":"code","6dca5b70":"code","c0c49560":"code","faa92096":"code","0e3f3927":"code","1072dcfd":"code","d6422833":"code","1668eabe":"code","bbc1e746":"code","01cd471d":"code","f0f026d3":"code","dfd7c1ed":"code","5323236f":"code","15676aba":"code","bc08018c":"code","275573e5":"markdown","5c6cc5d1":"markdown","83f87f3d":"markdown","092db21b":"markdown","7f60eb4d":"markdown","9a622610":"markdown","30ab56a7":"markdown","3e369fae":"markdown","4b1812ea":"markdown","a340547c":"markdown","fe54d558":"markdown","e583c145":"markdown","39b822e8":"markdown","4d5a7d5b":"markdown","565737d7":"markdown","552556a6":"markdown","2d5759e4":"markdown","1700a9ec":"markdown","690fb3b1":"markdown","8e3a764a":"markdown","04d66329":"markdown","71c93e38":"markdown","af12e094":"markdown","27ade77d":"markdown","f684af2c":"markdown","84518d94":"markdown","03f29e4c":"markdown","e5dd0642":"markdown","2dfc7722":"markdown","34eed1ec":"markdown","17ffc3b9":"markdown","7041ffe5":"markdown","e7090e50":"markdown","79da6d96":"markdown","35977488":"markdown","ce753f9e":"markdown","b32e0265":"markdown","8382b6ef":"markdown","9651a26f":"markdown","49e27626":"markdown","9986f46f":"markdown"},"source":{"293c2c17":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ac52baa1":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom keras.regularizers import l2\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","cf13cf1c":"dataset = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')","cd9d0cbe":"dataset","c99d6dbb":"dataset.info()","f5d88700":"dataset.isnull().sum()","08f80ff0":"dataset.describe()","e30090a8":"# Compute the correlation matrix\ncorr = dataset.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","d9aea372":"sns.set(style=\"whitegrid\")\nlabels = ['No Fraud', 'Fraud']\nsizes = dataset['Class'].value_counts(sort = True)\n\ncolors = [\"lightblue\",\"red\"]\nexplode = (0.05,0) \n \nplt.figure(figsize=(7,7))\nplt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=90,)\n\nplt.title('Frauds in the dataset')\nplt.legend()\nplt.show()","5ba8e9eb":"#Source: https:\/\/www.kaggle.com\/laowingkin\/credit-card-fraud-eda-ml\n\nV = dataset[[col for col in dataset.columns if 'V' in col]+['Class']]\n\nf, ax = plt.subplots(ncols = 4, nrows = 7, figsize=(15,2*len(V.columns)))\n\nfor i, c in zip(ax.flatten(), V.columns):\n    sns.distplot(V[c][V['Class'] == 0],color='#87bd75', ax = i) #Genuine\n    sns.distplot(V[c][V['Class'] == 1],color='#b94646', ax = i) #Fraud   \nf.tight_layout()","e8973997":"plt.style.use(\"classic\")\nplt.figure(figsize=(10,10))\n\nsns.distplot(dataset[dataset['Class'] == 0][\"Time\"], color='green') # Genuine - green\nsns.distplot(dataset[dataset['Class'] == 1][\"Time\"], color='red') # Fraud - Red\n\nplt.title('Genuine vs Fraud by Time(in sec)', fontsize=15)\nplt.xlim([-10000,180000])\nplt.grid(linewidth = 0.7)\nplt.legend(['Genuine','Fraud'])\nplt.show()","2e2ed1da":"plt.style.use(\"classic\")\nplt.figure(figsize=(10,10))\n\nsns.distplot(dataset[dataset['Class'] == 0][\"Amount\"], color='green') # Genuine - green\nsns.distplot(dataset[dataset['Class'] == 1][\"Amount\"], color='red') # Fraud - Red\n\nplt.title('Genuine vs Fraud by Amount', fontsize=15)\nplt.xlim([-10,3700])\nplt.grid(linewidth = 0.7)\nplt.legend(['Genuine','Fraud'])\nplt.show()","f6325ff9":"plt.figure(figsize=[25,15])\ndataset.boxplot(column= ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount'])\nplt.xticks(rotation=45)\nplt.show()","1e46bc2f":"for i in dataset.iloc[:, 1:30]:\n    q1 = dataset[i].quantile(0.25)\n    q3 = dataset[i].quantile(0.75)\n    iqr = q3-q1\n    Lower_tail = q1 - 1.5 * iqr\n    Upper_tail = q3 + 1.5 * iqr\n    med = np.median(dataset[i])\n    for j in dataset[i]:\n        if j > Upper_tail or j < Lower_tail:\n            dataset[i] = dataset[i].replace(j, med)","e2a552c3":"plt.figure(figsize=[25,15])\ndataset.boxplot(column= ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount'])\nplt.xticks(rotation=45)\nplt.show()","0dfc4c6b":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\ndataset[['Time', 'Amount']] = sc.fit_transform(dataset[['Time', 'Amount']])","4cce8f4e":"x = dataset.drop('Class', axis=1).values\ny = dataset['Class']","6dca5b70":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=0)","c0c49560":"print(\"Number transactions x_train dataset: \", x_train.shape)\nprint(\"Number transactions y_train dataset: \", y_train.shape)\nprint(\"Number transactions x_test dataset: \", x_test.shape)\nprint(\"Number transactions y_test dataset: \", y_test.shape)","faa92096":"from imblearn.over_sampling import BorderlineSMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\n\nfrom imblearn.pipeline import Pipeline\n\nfrom collections import Counter","0e3f3927":"over = BorderlineSMOTE(sampling_strategy=0.4)\nunder = RandomUnderSampler(sampling_strategy=0.7)\n\nsteps = [('o', over), ('u', under)]","1072dcfd":"pipeline = Pipeline(steps=steps)\n\n# transform the dataset\nx_sm, y_sm = pipeline.fit_resample(x_train, y_train)\n\nprint(Counter(y_train))\nprint(Counter(y_sm))","d6422833":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score","1668eabe":"ann = tf.keras.models.Sequential()\nann.add(tf.keras.layers.Dense(units= 6, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu'))\ntf.keras.layers.Dropout(0.6)\nann.add(tf.keras.layers.Dense(units= 6, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu'))\ntf.keras.layers.Dropout(0.6)\nann.add(tf.keras.layers.Dense(units= 1, activation='sigmoid'))","bbc1e746":"ann.compile(optimizer= 'adam', loss= 'binary_crossentropy', metrics= ['accuracy'])","01cd471d":"ann_history = ann.fit(x_sm, y_sm, batch_size= 32, epochs= 50, validation_split= 0.3)","f0f026d3":"loss_train = ann_history.history['loss']\nloss_val = ann_history.history['val_loss']\nepochs = range(1,51)\nplt.plot(epochs, loss_train, 'g', label='Training loss')\nplt.plot(epochs, loss_val, 'b', label='validation loss')\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","dfd7c1ed":"loss_train = ann_history.history['accuracy']\nloss_val = ann_history.history['val_accuracy']\nepochs = range(1,51)\nplt.plot(epochs, loss_train, 'g', label='Training accuracy')\nplt.plot(epochs, loss_val, 'b', label='validation accuracy')\nplt.title('Training and Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","5323236f":"from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay","15676aba":"y_pred = ann.predict(x_test)\ny_pred = (y_pred > 0.5) \ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(accuracy_score(y_test, y_pred))","bc08018c":"# Visualizing Confusion Matrix\nplt.figure(figsize = (6, 6))\nsns.heatmap(cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['Genuine', 'Fraud'], xticklabels = ['Predicted Genuine', 'Predicted Fraud'])\nplt.yticks(rotation = 0)\nplt.show()","275573e5":"\ud83d\udccc We can see a interesting different distribuition in some of our features.","5c6cc5d1":"## **Outliers** <a id='3.4' ><\/a>","83f87f3d":"\ud83d\udccc An outlier is an observation that lies an abnormal distance from other values in a random sample from a population.","092db21b":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:LightSlateGray;\n           font-size:150%;\n           text-align:center;\n           letter-spacing:0.5px\"> <a id = '2'><\/a>\n    \n    Importing Dataset \n<\/div>","7f60eb4d":"<div style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           background-color:Beige;\n           font-size:110%;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:black;\">\n    \nHello Kagglers, <br>\n\nIn this notebook, I am going to predict credit card fraud. But, first I am going to perform exploratory data analysis and learn more about the features. And, also it seems our dataset is imbalance. So, I am going to take care of it with the oversampling and undersampling of our dataset. Then, I am going to use ANN on our dataset and select the best performing one. <br>\n    So, let's get started.\n<\/p>\n<\/div> ","9a622610":"## **Splitting data into Train and Test Set** <a id='4.2'><\/a>","30ab56a7":"\ud83d\udccc From above graph, we can see that there are a lot of transactions with amount less than 500.","3e369fae":"### *V* Features <a id='3.3.1' ><\/a>","4b1812ea":"## **Visualizing Confusion Matrix** <a id = '5.6'><\/a>","a340547c":"\ud83d\udccc From the above graph, we can see that most frauds have happened in the early mornings.","fe54d558":"### Detection <a id='3.4.1' ><\/a>","e583c145":"### Removal <a id='3.4.2' ><\/a>","39b822e8":"## **Pie Chart** <a id='3.2' ><\/a>","4d5a7d5b":"<div style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           border:2px solid DodgerBlue;\n           background-color:white;\n           font-size:150%;\n           text-align:center;\n           letter-spacing:0.5px\">\n    \n    Thank You!\n<\/div>","565737d7":"\ud83d\udccc After extensive data analysis of the features, I got to know more about how features are correlated to each other. <br>\n\ud83d\udccc I implemented BorderlineSMOTE and RandomUnderSampler techniques on our features to deal with the imbalance data. <br>\n\ud83d\udccc Then, I used Keras ANN with regularizers to see how it performs on the dataset. I got pretty good results with accuracy and also the visualization.","552556a6":"### **Table of Contents:** \n1. [Importing Libraries](#1)<a href='1' ><\/a> <br>\n2. [Importing Dataset](#2)<a href='2' ><\/a> <br>\n3. [Data Visualization and Analysis](#3)<a href='3' ><\/a> <br>\n    3.1. [Heat Map Correlation](#3.1)<a href='3.1' ><\/a> <br>\n    3.2. [Pie Chart](#3.2)<a href='3.2' ><\/a> <br>\n    3.3. [Distribution Plot](#3.3)<a href='3.3' ><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; a. [*V* Features](#3.3.1)<a href='3.3.1' ><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; b. [Genuine vs Fraud by Time](#3.3.2)<a href='3.3.2' ><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; c. [Genuine vs Fraud by Amount](#3.3.3)<a href='3.3.3' ><\/a> <br>\n    3.4. [Outliers](#3.4)<a href='3.4' ><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; a. [Detection](#3.4.1)<a href='3.4.1' ><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; b. [Removal](#3.4.2)<a href='3.4.2' ><\/a> <br>\n4. [Data Preprocessing](#4)<a href='4' ><\/a> <br>\n    4.1. [Feature Scaling](#4.1)<a href='4.1' ><\/a> <br>\n    4.2. [Splitting data into Train and Test Set](#4.2)<a href='4.2' ><\/a> <br>\n    4.3. [BorderlineSMOTE and RandomUnderSampler](#4.3)<a href='4.3' ><\/a> <br>\n5. [Keras ANN](#5)<a href='5' ><\/a> <br>\n    5.1. [Building ANN](#5.1)<a href='5.1' ><\/a> <br>\n    5.2. [Training ANN](#5.2)<a href='5.2' ><\/a> <br>\n    5.3. [Visualizing Training and Validation Loss](#5.3)<a href='5.3' ><\/a> <br>\n    5.4. [Visualizing Training and Validation Accuracy](#5.4)<a href='5.4' ><\/a> <br>\n    5.5. [Making Confusion Matrix](#5.5)<a href='5.5' ><\/a> <br>\n    5.6. [Visualizing Confusion Matrix](#5.6)<a href='5.6' ><\/a> <br>\n6. [Conclusion](#6)<a href='6' ><\/a> <br>","2d5759e4":"## **Heat Map Correlation** <a id='3.1' ><\/a>","1700a9ec":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:LightSlateGray;\n           font-size:150%;\n           text-align:center;\n            letter-spacing:0.5px\"> <a id='3'><\/a>\n    \n    Data Visualization and Analysis \n<\/div>","690fb3b1":"\ud83d\udccc In this notebook, we are using Box Plot to detect the outliers of each features in our dataset, where any point above or below the whiskers represent an outlier.","8e3a764a":"## **BorderlineSMOTE and RandomUnderSampler** <a id='4.3'><\/a>","04d66329":"\ud83d\udccc StandardScaler standardizes a feature by subtracting the mean and then scaling to unit variance. Unit variance means dividing all the values by the standard deviation. StandardScaler results in a distribution with a standard deviation equal to 1.","71c93e38":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:LightSlateGray;\n           font-size:150%;\n           text-align:center;\n           letter-spacing:0.5px\"> <a id = '4'><\/a>\n    \n    Data Preprocessing \n<\/div>","af12e094":"### Genuine vs Fraud by Amount <a id='3.3.3' ><\/a>","27ade77d":"## **Training the ANN** <a id = '5.2'><\/a>","f684af2c":"### Genuine vs Fraud by Time <a id='3.3.2' ><\/a>","84518d94":"## **Visualizing Training and Validation Loss** <a id = '5.3'><\/a>","03f29e4c":"\ud83d\udccc It seems our dataset is highly imbalance. I'll take care of it later.","e5dd0642":"## **Visualizing Training and Validation Accuracy** <a id = '5.4'><\/a>","2dfc7722":"\ud83d\udccc We have 284807  rows and 31 columns in our dataset. <br>\n\ud83d\udccc We can see that the dataset contains *numerical* variables.","34eed1ec":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:LightSlateGray;\n           font-size:150%;\n           text-align:center;\n           letter-spacing:0.5px\"> <a id = '5'><\/a>\n\n    Keras ANN \n<\/div>","17ffc3b9":"## **Feature Scaling** <a id='4.1'><\/a>","7041ffe5":"\ud83d\udccc *BorderlineSMOTE* is a popular extension to *SMOTE* involves selecting those instances of the minority class that are misclassified, such as with a k-nearest neighbor classification model. <br>\n\ud83d\udccc Instead of generating new synthetic examples for the minority class blindly, we would expect the Borderline-SMOTE method to only create synthetic examples along the decision boundary between the two classes.","e7090e50":" <div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:LightSlateGray;\n           font-size:150%;\n           text-align:center;\n           letter-spacing:0.5px\">\n    \n     Welcome\n<\/div>","79da6d96":"## **Distribution Plot** <a id='3.3' ><\/a>","35977488":"## **Building the ANN** <a id = '5.1'><\/a>","ce753f9e":"\ud83d\udccc There are no NULL values in our dataset.","b32e0265":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:LightSlateGray;\n           font-size:150%;\n           text-align:center;\n            letter-spacing:0.5px\"> <a id='1'><\/a>\n    \n    Importing Libraries \n<\/div>","8382b6ef":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:LightSlateGray;\n           font-size:150%;\n           text-align:center;\n           letter-spacing:0.5px\"> <a id = '6'><\/a>\n\n    Conclusion\n<\/div>","9651a26f":"\ud83d\udccc *RandomUnderSampler* involves randomly selecting examples from the majority class to delete from the training dataset. <br>\n\ud83d\udccc This has the effect of reducing the number of examples in the majority class in the transformed version of the training dataset. This process can be repeated until the desired class distribution is achieved, such as an equal number of examples for each class.","49e27626":"\ud83d\udccc After detecting, we are using Median Imputation to take care of outliers. In this technique, we replace the extreme values with median values. <br>\n\ud83d\udccc It is represented by the formula IQR = Q3 \u2212 Q1. The lines of code below calculate and print the interquartile range for each of the variables in the dataset. <br>\n\ud83d\udccc It is advised to not use mean values as they are affected by outliers.","9986f46f":"## **Making the Confusion Matrix** <a id = '5.5'><\/a>"}}