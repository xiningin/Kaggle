{"cell_type":{"88c83279":"code","632ec43d":"code","ea3d5802":"code","47eb6b7d":"code","2c0d20af":"code","445ed6d5":"code","1fbb4560":"code","f1a33d38":"code","5b664b5a":"code","d4e56dfa":"code","3ba38e71":"code","7c75a40d":"code","92d6ccad":"code","0558e5e9":"code","e7fd79b7":"code","0325b7bd":"code","8df815d9":"code","91dfbbfe":"code","2b55df31":"code","af4a7ae7":"code","e1f6d4a6":"code","8c1e2b56":"code","068d0ace":"code","08435a65":"code","510f6545":"code","e198c17a":"code","6af446cd":"code","af5da17b":"code","3f9a5f1c":"code","a822e41a":"code","47939a6d":"code","1dc18206":"code","2e09c0bd":"code","d0bf703f":"code","fc77688c":"code","76bdb9be":"code","f962bef5":"code","6de7bbed":"code","fda172bf":"code","1abdad6f":"markdown","21c716ed":"markdown","6b2eb3bb":"markdown","14424fe6":"markdown","5b281f84":"markdown","9f82cb01":"markdown","d435cf9d":"markdown","db718fe4":"markdown","f9a440d2":"markdown","24b37fe9":"markdown","10527ca5":"markdown","955f09c4":"markdown","2ed346f9":"markdown","dbac91ef":"markdown","4bbf8f3f":"markdown","2764211d":"markdown","7a2331c0":"markdown","182dc1cc":"markdown","e2a732f2":"markdown","3be9b1a8":"markdown","c6c63ea4":"markdown","b6c363ae":"markdown","dd25c671":"markdown","5ef9a2ef":"markdown","50c11f19":"markdown","66a64c42":"markdown","edc15c87":"markdown","0bbcaccb":"markdown"},"source":{"88c83279":"from pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport torch\nimport time\nimport copy\nfrom torch import nn\nfrom torch import optim\nfrom torchvision import models\nfrom torch.utils.data import Dataset, DataLoader, Sampler\nfrom PIL import Image\nfrom skimage import io\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\ntqdm.pandas()","632ec43d":"csv_file = Path('..\/input\/spacenet-7-chips-annotation-csv-file-generator\/output_csvs\/chip_csv_file.csv')\nroot_dir = Path('..\/input\/spacenet-7-change-detection-chips-and-masks\/chip_dataset\/change_detection')","ea3d5802":"df = pd.read_csv(csv_file)","47eb6b7d":"df.head()","2c0d20af":"def create_target_column(df):\n    df['blank'] = df['blank'].fillna(\"change\")\n    # dict that we are going to use to create the targets column\n    encoding_dict = {'change':1,'blank':0}\n    # loop through blank column and replace values accordingly in new column\n    df['target'] = df['blank'].map(lambda x: encoding_dict[x])\n    \n    return df","445ed6d5":"df = create_target_column(df)","1fbb4560":"df['target'].value_counts()","f1a33d38":"class ChangeDetectionChipDataset(Dataset):\n    '''SpaceNet 7 Chip Change Detection Dataset'''\n    def __init__(self,csv_file, root_dir, aug=None, dataset=None, test_size=0.25):\n        \"\"\"\n        Args:\n            csv_file (Path): Path to the csv file with annotations\n            root_dir (Path): Parent directory containing all other directories.\n            aug (callable, optional): Optional augmentations to be applied on a sample.\n            dataset (string, optional): Optional argument to split the dataset to train and test set\n            test_size (float): Argument specifying the percentage of the test dataset \n        \"\"\"\n        \n        df = pd.read_csv(csv_file)\n        \n        self.annotations = self.create_target_column(df)\n        self.test_size = test_size\n        self.root_dir = root_dir\n        self.aug = aug\n        \n        if dataset:\n            # the code below uses stratify to ensure that our train and test dataframes have the same class distribution\n            df_train, df_test, __, _ = train_test_split(self.annotations, self.annotations['target'],test_size=self.test_size ,random_state=0, stratify=self.annotations['target'])\n            \n            if dataset == 'train':\n                self.annotations = df_train.reset_index(drop=True)\n            elif dataset == 'valid':\n                self.annotations = df_test.reset_index(drop=True)\n            else:\n                raise NotImplementedError\n                \n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self,idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        # get chip path for corresponding index\n        chip_path = Path(self.root_dir\/self.annotations.loc[idx,'chip_path'])\n        # read chip into numpy array\n        chip = io.imread(chip_path)\n        # get target for corresponding chip\n        target = self.annotations.loc[idx,'target']\n\n        sample = {'chip':chip,'target':target}\n\n        # apply transforms to sample\n        if self.aug is not None:\n            sample = self.aug(image=chip, category_id=target)\n            sample['image'] = sample['image'].float()\n\n        return sample\n    \n    def create_target_column(self,df):\n        df['blank'] = df['blank'].fillna(\"change\")\n        # dict that we are going to use to create the targets column\n        encoding_dict = {'change':1,'blank':0}\n        # loop through blank column and replace values accordingly in new column\n        df['target'] = df['blank'].map(lambda x: encoding_dict[x])\n\n        return df\n\n        ","5b664b5a":"datasets = {x:ChangeDetectionChipDataset(csv_file,root_dir,dataset=x) for x in ['train','valid']}","d4e56dfa":"len(datasets['valid'])\/len(datasets['train'])","3ba38e71":"params = {\n    'batch_size': 512,\n    'learning_rate': 0.001,\n}","7c75a40d":"dataloader = DataLoader(dataset=datasets['train'],batch_size=params['batch_size'])\nbatch = next(iter(dataloader))","92d6ccad":"batch['target']","0558e5e9":"class BalancingSampler(Sampler):\n    def __init__(self,dataset,change_pct=0.5):\n        \"\"\"\n        Args:\n            dataset (PyTorch Dataset): Instance of PyTorch Dataset Class\n            change_pct (float): Percentage of the batch containing chips with change in them; Val between 0 and 1\n        \"\"\"\n        \n        assert 0 <= change_pct <= 1,'change_pct must be a value between 0 and 1'\n        \n        self.dataset = dataset\n        self.change_pct = change_pct\n        self.len_ = len(dataset)\n        \n    def __len__(self):\n        return self.len_\n    \n    def __iter__(self):\n        # get indices for blank chips and change chips\n        change_chip_idxs = np.where(self.dataset.annotations['target'] == 1)[0]\n        blank_chip_idxs = np.where(self.dataset.annotations['target'] == 0)[0]\n        # randomly sample from the incides of each class\n        change_chip_idxs = np.random.choice(change_chip_idxs,int(self.len_ * self.change_pct), replace=True)\n        blank_chip_idxs = np.random.choice(blank_chip_idxs,int(self.len_ * (1 - self.change_pct))+1, replace=False)\n        # stack the sampled class indices and shuffle\n        all_idxs = np.hstack([change_chip_idxs,blank_chip_idxs])\n        np.random.shuffle(all_idxs)\n        all_idxs[:]\n        \n        return iter(all_idxs)","e7fd79b7":"sampler = BalancingSampler(datasets['train'],change_pct=0.5)\nbalanced_loader = DataLoader(dataset=datasets['train'],batch_size=params['batch_size'],sampler=sampler)","0325b7bd":"balanced_batch = next(iter(balanced_loader))","8df815d9":"balanced_batch['target'].unique(return_counts=True)","91dfbbfe":"# Data augmentation and normalization for training\n# Just normalization for validation\nchip_dimension = 64\ndata_transforms = {\n    'train': A.Compose([\n        A.PadIfNeeded(min_height=chip_dimension,min_width=chip_dimension,value=0,p=1),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        ToTensorV2()\n        # A.Normalize([],[])\n    ]),\n    'valid': A.Compose([\n        A.PadIfNeeded(min_height=chip_dimension,min_width=chip_dimension,value=0,p=1),\n        ToTensorV2()\n        # A.Normalize([], [])\n    ]),\n}","2b55df31":"datasets = {x:ChangeDetectionChipDataset(csv_file,root_dir,dataset=x,aug = data_transforms[x]) for x in ['train','valid']}","af4a7ae7":"dataloaders = {x: DataLoader(dataset=datasets[x],batch_size=params['batch_size'],sampler=BalancingSampler(datasets[x],change_pct=0.5),num_workers=4,pin_memory=True) for x in ['train','valid']}","e1f6d4a6":"# loading pretrained weights\nmodel_ft = models.resnet50(pretrained=True)","8c1e2b56":"model_ft.conv1","068d0ace":"model_ft.conv1.in_channels = 6","08435a65":"model_ft.conv1","510f6545":"weights = model_ft.conv1.weight.clone()","e198c17a":"weights.shape","6af446cd":"updated_weights = torch.cat((weights,weights),1)","af5da17b":"updated_weights.shape","3f9a5f1c":"model_ft.conv1.weight = nn.Parameter(updated_weights)","a822e41a":"model_ft.conv1.weight.shape","47939a6d":"model_ft.fc","1dc18206":"# Here the size of each output sample is set to 2, as we have 2 classes 'change' and 'no change'.\nmodel_ft.fc.out_features = 2","2e09c0bd":"model_ft.fc","d0bf703f":"# setting device on GPU if available, else CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\nprint()","fc77688c":"model_ft = model_ft.to(device)","76bdb9be":"criterion = nn.CrossEntropyLoss()\n\n# Observe that all parameters are being optimized\noptimizer_ft = optim.Adam(model_ft.parameters(), lr=params['learning_rate'])\n\n# Decay LR if the loss did not improve after 3 epochs\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer_ft, patience=3, verbose=True)","f962bef5":"def train_model(model, criterion, optimizer, scheduler, dataloaders, num_epochs=25):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch}\/{num_epochs-1}')\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'valid']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            num_iterations_per_epoch = len(dataloaders[phase])\n            for i,batch in enumerate(tqdm(dataloaders[phase],total = num_iterations_per_epoch)):\n                # get the input and target values from dictionary\n                inputs, labels = batch.values()\n                # set the device\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n                # print internal epoch statistics every 100 iterations\n                if i % 100 == 0:\n                    print(loss.item())\n                    print(torch.sum(preds == labels.data)\/512)\n                    print()\n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss \/ dataset_sizes[phase]\n            epoch_acc = running_corrects.double() \/ dataset_sizes[phase]\n\n            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n\n            # deep copy the model\n            if phase == 'valid' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print(f'Training complete in {time_elapsed \/\/ 60:.0f}m {time_elapsed % 60:.0f}s')\n    print(f'Best val Acc: {best_acc:4f}')\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model","6de7bbed":"train_model(model_ft, criterion, optimizer_ft, scheduler, dataloaders, num_epochs=4)","fda172bf":"torch.save(model_ft, 'resnet50.pt')","1abdad6f":"Let's load the paths to our files and directories below.","21c716ed":"Let's have a quick look at our initial shape.","6b2eb3bb":"Notice how our dataset is highly unbalanced, as mentioned earlier we are going to create a custom sampler class that will give us the ability to choose the percentage of each class that we would like. One thing we can do is, we can initially start with a balanced sampler at 0.5 and then we can slowly revert back to the original sampling distribution as the number of training epochs increases.","14424fe6":"### With Custom Sampler","5b281f84":"Let's make sure that the number of channels has been updated.","9f82cb01":"Let's double check the shape of our concatinated tensors.","d435cf9d":"## Creating Sampler and DataLoader Class\nNow that we created our dataset class, we move onto the next step which is creating our DataLoader class, which will make it easier to load our batches as we train our network. \n\nOne thing that we must consider is that our dataset is highly imbalanced, in order to remedy this problem we will be creating a custom data PyTorch Sampler class that we will feed into our PyTorch DataLoader Class.","db718fe4":"Finally, let's update the weights of the model with our new concatinated weights.","f9a440d2":"### Creating Custom Sampler","24b37fe9":"## Model Settings","10527ca5":"We are going to use the blank column to create a column containing the whether change occurs or not. The function below replaces the nan values by a placeholder value named change, then creates a new column with the target labels 1 for change and 0 for no change.","955f09c4":"# SpaceNet 7 Chips Starter Notebook\nThe aim of this notebook is to provide a base for the different architectures for experimentation purposes. In this notebook we will go over the following points:\n\n* Manipulating the dataset csv file (created in our previous [notebook](https:\/\/www.kaggle.com\/amerii\/spacenet-7-chips-annotation-csv-file-generator)\n* Creating PyTorch Dataset Class\n* Creating PyTorch Sampler to address the dataset imbalance\n* Creating Learning Rate Schedular \n* Creating PyTorch Dataloader Class\n* Creating Helper Function to visualize our dataset","2ed346f9":"### Updating Input Number of Channels\nBefore Training our model we have to update the number input number of channels for our pretrained model. Let's start by having a look at the initial number of input channels that our pretrained model accepts.","dbac91ef":"## Import Dependencies","4bbf8f3f":"## Training Loop","2764211d":"### Updating Outputs\nWe need to do the same thing for the output of the fully connected layer of the model, this is because our pretrained model was intitailly trained on a dataset that contained a total of 1000 classes. We will update that value to 2, since our dataset only has 2 output classes.","7a2331c0":"## Loading Model with Pre-Trained Weights and Updating Model Architecture","182dc1cc":"As you can see it accepts, a total of 3 channels. Our input is comprised of 2 images stacked together, therefore our input number of channels needs to be 6.","e2a732f2":"Great! It's the right size!","3be9b1a8":"## Creating Dataset PyTorch Class","c6c63ea4":"## Data Augmentations and Transformations\nWe will use Albumentations to transform and augment our dataset. Below are 2 dictionaries containing the transformations and augmentations for each of the training and validation dataset.","b6c363ae":"### Updating Model Weights\nWe also need to update the corresponding weights to account for the increase in channels. We will duplicate the channel weights,In order to use the pretrained weights. ","dd25c671":"We will now create a simple PyTorch Dataset Class that reads directly from the pdf that we saw earlier. We will also incorporate the function that we created earlier as a method inside of our class.","5ef9a2ef":"As you may have noticed our dataset is imbalanced, in order to deal with that problem let's create a custom datasampler, that balances our dataset during training time.\n\nOne thing that we may want to consider is that as we start the training we may want the data to be completely balanced, and as our classifier improves we can have the data gradually revert back to the original sampling distribution.","50c11f19":"## Creating Training and Validation Data Loaders\nFinally let's recreate our datasets and dataloaders this time to include the transformations that we would like to perform on our dataset.","66a64c42":"Notice how our dataset is much more balanced than earlier, this is because we set our change_pct at 0.5, we can now vary this percentage as we like.\n\nNote that, our entire dataset is now balanced, however our individual batches will still have some randomness in them, because we are shuffling without stratifying our data.","edc15c87":"### Without Custom Sampler","0bbcaccb":"We are going to stack the pretrained weights as mentioned earlier."}}