{"cell_type":{"9b1657bf":"code","e35eac4d":"code","bf6c7d4a":"code","63dc3251":"code","dd24dc96":"code","1a8eb026":"code","b7a1f291":"code","1ff0c227":"code","41d78aee":"code","fe07e00a":"code","71e3b360":"code","63ae9887":"code","c3030adb":"code","f888128b":"code","e7b4a759":"code","51303cac":"code","ed04b420":"code","13bcbac0":"code","c8672885":"code","155abb94":"code","078d5619":"code","a93acdca":"code","81f46ac9":"markdown"},"source":{"9b1657bf":"MAKE_SUBMISSION = True          # Generate output file.\nCV_ONLY = False                 # Do validation only; do not generate predicitons.\nFIT_FULL_TRAIN_SET = True       # Fit model to full training set after doing validation.\nFIT_2017_TRAIN_SET = False      # Use 2017 training data for full fit (no leak correction)\nFIT_COMBINED_TRAIN_SET = True   # Fit combined 2016-2017 training set\nUSE_SEASONAL_FEATURES = True\nVAL_SPLIT_DATE = '2016-09-15'   # Cutoff date for validation split\nLEARNING_RATE = 0.007           # shrinkage rate for boosting roudns\nROUNDS_PER_ETA = 20             # maximum number of boosting rounds times learning rate\nOPTIMIZE_FUDGE_FACTOR = False   # Optimize factor by which to multiply predictions.\nFUDGE_FACTOR_SCALEDOWN = 0.3    # exponent to reduce optimized fudge factor for prediction","e35eac4d":"import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_absolute_error\nimport datetime as dt\nfrom datetime import datetime\nimport gc\nimport patsy\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.regression.quantile_regression import QuantReg","bf6c7d4a":"properties16 = pd.read_csv('..\/input\/properties_2016.csv', low_memory = False)\nproperties17 = pd.read_csv('..\/input\/properties_2017.csv', low_memory = False)\n\n# Number of properties in the zip\nzip_count = properties16['regionidzip'].value_counts().to_dict()\n# Number of properties in the city\ncity_count = properties16['regionidcity'].value_counts().to_dict()\n# Median year of construction by neighborhood\nmedyear = properties16.groupby('regionidneighborhood')['yearbuilt'].aggregate('median').to_dict()\n# Mean square feet by neighborhood\nmeanarea = properties16.groupby('regionidneighborhood')['calculatedfinishedsquarefeet'].aggregate('mean').to_dict()\n# Neighborhood latitude and longitude\nmedlat = properties16.groupby('regionidneighborhood')['latitude'].aggregate('median').to_dict()\nmedlong = properties16.groupby('regionidneighborhood')['longitude'].aggregate('median').to_dict()\n\ntrain = pd.read_csv(\"..\/input\/train_2016_v2.csv\")\nfor c in properties16.columns:\n    properties16[c]=properties16[c].fillna(-1)\n    if properties16[c].dtype == 'object':\n        lbl = LabelEncoder()\n        lbl.fit(list(properties16[c].values))\n        properties16[c] = lbl.transform(list(properties16[c].values))","63dc3251":"train_df = train.merge(properties16, how='left', on='parcelid')\nselect_qtr4 = pd.to_datetime(train_df[\"transactiondate\"]) >= VAL_SPLIT_DATE\nif USE_SEASONAL_FEATURES:\n    basedate = pd.to_datetime('2015-11-15').toordinal()\n","dd24dc96":"del train\ngc.collect()","1a8eb026":"# Inputs to features that depend on target variable\n# (Ideally these should be recalculated, and the dependent features recalculated,\n#  when fitting to the full training set.  But I haven't implemented that yet.)\n\n# Standard deviation of target value for properties in the city\/zip\/neighborhood\ncitystd = train_df[~select_qtr4].groupby('regionidcity')['logerror'].aggregate(\"std\").to_dict()\nzipstd = train_df[~select_qtr4].groupby('regionidzip')['logerror'].aggregate(\"std\").to_dict()\nhoodstd = train_df[~select_qtr4].groupby('regionidneighborhood')['logerror'].aggregate(\"std\").to_dict()","b7a1f291":"def calculate_features(df):\n    # Nikunj's features\n    # Number of properties in the zip\n    df['N-zip_count'] = df['regionidzip'].map(zip_count)\n    # Number of properties in the city\n    df['N-city_count'] = df['regionidcity'].map(city_count)\n    # Does property have a garage, pool or hot tub and AC?\n    df['N-GarPoolAC'] = ((df['garagecarcnt']>0) & \\\n                         (df['pooltypeid10']>0) & \\\n                         (df['airconditioningtypeid']!=5))*1 \n\n    # More features\n    # Mean square feet of neighborhood properties\n    df['mean_area'] = df['regionidneighborhood'].map(meanarea)\n    # Median year of construction of neighborhood properties\n    df['med_year'] = df['regionidneighborhood'].map(medyear)\n    # Neighborhood latitude and longitude\n    df['med_lat'] = df['regionidneighborhood'].map(medlat)\n    df['med_long'] = df['regionidneighborhood'].map(medlong)\n\n    df['zip_std'] = df['regionidzip'].map(zipstd)\n    df['city_std'] = df['regionidcity'].map(citystd)\n    df['hood_std'] = df['regionidneighborhood'].map(hoodstd)\n    \n    if USE_SEASONAL_FEATURES:\n        df['cos_season'] = ( (pd.to_datetime(df['transactiondate']).apply(lambda x: x.toordinal()-basedate)) * \\\n                             (2*np.pi\/365.25) ).apply(np.cos)\n        df['sin_season'] = ( (pd.to_datetime(df['transactiondate']).apply(lambda x: x.toordinal()-basedate)) * \\\n                             (2*np.pi\/365.25) ).apply(np.sin)\n","1ff0c227":"dropvars = ['airconditioningtypeid', 'buildingclasstypeid',\n            'buildingqualitytypeid', 'regionidcity']\ndroptrain = ['parcelid', 'logerror', 'transactiondate']\ndroptest = ['ParcelId']","41d78aee":"calculate_features(train_df)\n\nx_valid = train_df.drop(dropvars+droptrain, axis=1)[select_qtr4]\ny_valid = train_df[\"logerror\"].values.astype(np.float32)[select_qtr4]\n\nprint('Shape full training set: {}'.format(train_df.shape))\nprint('Dropped vars: {}'.format(len(dropvars+droptrain)))\nprint('Shape valid X: {}'.format(x_valid.shape))\nprint('Shape valid y: {}'.format(y_valid.shape))\n\ntrain_df=train_df[ train_df.logerror > -0.4 ]\ntrain_df=train_df[ train_df.logerror < 0.419 ]\nprint('\\nFull training set after removing outliers, before dropping vars:')     \nprint('Shape training set: {}\\n'.format(train_df.shape))\n\nif FIT_FULL_TRAIN_SET:\n    full_train = train_df.copy()\n\ntrain_df=train_df[~select_qtr4]\nx_train=train_df.drop(dropvars+droptrain, axis=1)\ny_train = train_df[\"logerror\"].values.astype(np.float32)\ny_mean = np.mean(y_train)\nn_train = x_train.shape[0]\nprint('Training subset after removing outliers:')     \nprint('Shape train X: {}'.format(x_train.shape))\nprint('Shape train y: {}'.format(y_train.shape))\n\nif FIT_FULL_TRAIN_SET:\n    x_full = full_train.drop(dropvars+droptrain, axis=1)\n    y_full = full_train[\"logerror\"].values.astype(np.float32)\n    n_full = x_full.shape[0]\n    print('\\nFull trainng set:')     \n    print('Shape train X: {}'.format(x_train.shape))\n    print('Shape train y: {}'.format(y_train.shape))","fe07e00a":"if not CV_ONLY:\n    # Generate test set data\n    \n    sample_submission = pd.read_csv('..\/input\/sample_submission.csv', low_memory = False)\n    \n    # Process properties for 2016\n    test_df = pd.merge( sample_submission[['ParcelId']], \n                        properties16.rename(columns = {'parcelid': 'ParcelId'}), \n                        how = 'left', on = 'ParcelId' )\n    if USE_SEASONAL_FEATURES:\n        test_df['transactiondate'] = '2016-10-31'\n        droptest += ['transactiondate']\n    calculate_features(test_df)\n    x_test = test_df.drop(dropvars+droptest, axis=1)\n    print('Shape test: {}'.format(x_test.shape))\n\n    # Process properties for 2017\n    for c in properties17.columns:\n        properties17[c]=properties17[c].fillna(-1)\n        if properties17[c].dtype == 'object':\n            lbl = LabelEncoder()\n            lbl.fit(list(properties17[c].values))\n            properties17[c] = lbl.transform(list(properties17[c].values))\n    zip_count = properties17['regionidzip'].value_counts().to_dict()\n    city_count = properties17['regionidcity'].value_counts().to_dict()\n    medyear = properties17.groupby('regionidneighborhood')['yearbuilt'].aggregate('median').to_dict()\n    meanarea = properties17.groupby('regionidneighborhood')['calculatedfinishedsquarefeet'].aggregate('mean').to_dict()\n    medlat = properties17.groupby('regionidneighborhood')['latitude'].aggregate('median').to_dict()\n    medlong = properties17.groupby('regionidneighborhood')['longitude'].aggregate('median').to_dict()\n\n    test_df = pd.merge( sample_submission[['ParcelId']], \n                        properties17.rename(columns = {'parcelid': 'ParcelId'}), \n                        how = 'left', on = 'ParcelId' )\n    if USE_SEASONAL_FEATURES:\n        test_df['transactiondate'] = '2017-10-31'\n    calculate_features(test_df)\n    x_test17 = test_df.drop(dropvars+droptest, axis=1)\n\n    del test_df","71e3b360":"del train_df\ndel select_qtr4\ngc.collect()","63ae9887":"xgb_params = {  # best as of 2017-09-28 13:20 UTC\n    'eta': LEARNING_RATE,\n    'max_depth': 7, \n    'subsample': 0.6,\n    'objective': 'reg:linear',\n    'eval_metric': 'mae',\n    'lambda': 5.0,\n    'alpha': 0.65,\n    'colsample_bytree': 0.5,\n    'base_score': y_mean,'taxdelinquencyyear'\n    'silent': 1\n}\n\ndtrain = xgb.DMatrix(x_train, y_train)\ndvalid_x = xgb.DMatrix(x_valid)\ndvalid_xy = xgb.DMatrix(x_valid, y_valid)\nif not CV_ONLY:\n    dtest = xgb.DMatrix(x_test)\n    dtest17 = xgb.DMatrix(x_test17)\n    del x_test","c3030adb":"del x_train\ngc.collect()","f888128b":"num_boost_rounds = round( ROUNDS_PER_ETA \/ xgb_params['eta'] )\nearly_stopping_rounds = round( num_boost_rounds \/ 20 )\nprint('Boosting rounds: {}'.format(num_boost_rounds))\nprint('Early stoping rounds: {}'.format(early_stopping_rounds))","e7b4a759":"evals = [(dtrain,'train'),(dvalid_xy,'eval')]\nmodel = xgb.train(xgb_params, dtrain, num_boost_round=num_boost_rounds,\n                  evals=evals, early_stopping_rounds=early_stopping_rounds, \n                  verbose_eval=10)","51303cac":"valid_pred = model.predict(dvalid_x, ntree_limit=model.best_ntree_limit)\nprint( \"XGBoost validation set predictions:\" )\nprint( pd.DataFrame(valid_pred).head() )\nprint(\"\\nMean absolute validation error:\")\nmean_absolute_error(y_valid, valid_pred)","ed04b420":"if OPTIMIZE_FUDGE_FACTOR:\n    mod = QuantReg(y_valid, valid_pred)\n    res = mod.fit(q=.5)\n    print(\"\\nLAD Fit for Fudge Factor:\")\n    print(res.summary())\n\n    fudge = res.params[0]\n    print(\"Optimized fudge factor:\", fudge)\n    print(\"\\nMean absolute validation error with optimized fudge factor: \")\n    print(mean_absolute_error(y_valid, fudge*valid_pred))\n\n    fudge **= FUDGE_FACTOR_SCALEDOWN\n    print(\"Scaled down fudge factor:\", fudge)\n    print(\"\\nMean absolute validation error with scaled down fudge factor: \")\n    print(mean_absolute_error(y_valid, fudge*valid_pred))\nelse:\n    fudge=1.0","13bcbac0":"if FIT_FULL_TRAIN_SET and not CV_ONLY:\n    if FIT_COMBINED_TRAIN_SET:\n        # Merge 2016 and 2017 data sets\n        train16 = pd.read_csv('..\/input\/train_2016_v2.csv')\n        train17 = pd.read_csv('..\/input\/train_2017.csv')\n        train16 = pd.merge(train16, properties16, how = 'left', on = 'parcelid')\n        train17 = pd.merge(train17, properties17, how = 'left', on = 'parcelid')\n        train17[['structuretaxvaluedollarcnt', 'landtaxvaluedollarcnt', 'taxvaluedollarcnt', 'taxamount']] = np.nan\n        train_df = pd.concat([train16, train17], axis = 0)\n        # Generate features\n        citystd = train_df.groupby('regionidcity')['logerror'].aggregate(\"std\").to_dict()\n        zipstd = train_df.groupby('regionidzip')['logerror'].aggregate(\"std\").to_dict()\n        hoodstd = train_df.groupby('regionidneighborhood')['logerror'].aggregate(\"std\").to_dict()\n        calculate_features(train_df)\n        # Remove outliers\n        train_df=train_df[ train_df.logerror > -0.4 ]\n        train_df=train_df[ train_df.logerror < 0.419 ]\n        # Create final training data sets\n        x_full = train_df.drop(dropvars+droptrain, axis=1)\n        y_full = train_df[\"logerror\"].values.astype(np.float32)\n        n_full = x_full.shape[0]     \n    elif FIT_2017_TRAIN_SET:\n        train = pd.read_csv('..\/input\/train_2017.csv')\n        train_df = train.merge(properties17, how='left', on='parcelid')\n        # Generate features\n        citystd = train_df.groupby('regionidcity')['logerror'].aggregate(\"std\").to_dict()\n        zipstd = train_df.groupby('regionidzip')['logerror'].aggregate(\"std\").to_dict()\n        hoodstd = train_df.groupby('regionidneighborhood')['logerror'].aggregate(\"std\").to_dict()\n        calculate_features(train_df)\n        # Remove outliers\n        train_df=train_df[ train_df.logerror > -0.4 ]\n        train_df=train_df[ train_df.logerror < 0.419 ]\n        # Create final training data sets\n        x_full = train_df.drop(dropvars+droptrain, axis=1)\n        y_full = train_df[\"logerror\"].values.astype(np.float32)\n        n_full = x_full.shape[0]     \n    dtrain = xgb.DMatrix(x_full, y_full)\n    num_boost_rounds = int(model.best_ntree_limit*n_full\/n_train)\n    full_model = xgb.train(xgb_params, dtrain, num_boost_round=num_boost_rounds, \n                           evals=[(dtrain,'train')], verbose_eval=10)","c8672885":"del properties16\ndel properties17\ngc.collect()","155abb94":"if not CV_ONLY:\n    if FIT_FULL_TRAIN_SET:\n        pred = fudge*full_model.predict(dtest)\n        pred17 = fudge*full_model.predict(dtest17)\n    else:\n        pred = fudge*model.predict(dtest, ntree_limit=model.best_ntree_limit)\n        pred17 = fudge*model.predict(dtest17, ntree_limit=model.best_ntree_limit)\n        \n    print( \"XGBoost test set predictions for 2016:\" )\n    print( pd.DataFrame(pred).head() )\n    print( \"XGBoost test set predictions for 2017:\" )\n    print( pd.DataFrame(pred17).head() )    ","078d5619":"if MAKE_SUBMISSION and not CV_ONLY:\n   y_pred=[]\n   y_pred17=[]\n\n   for i,predict in enumerate(pred):\n       y_pred.append(str(round(predict,4)))\n   for i,predict in enumerate(pred17):\n       y_pred17.append(str(round(predict,4)))\n   y_pred=np.array(y_pred)\n   y_pred17=np.array(y_pred17)\n\n   output = pd.DataFrame({'ParcelId': sample_submission['ParcelId'].astype(np.int32),\n           '201610': y_pred, '201611': y_pred, '201612': y_pred,\n           '201710': y_pred17, '201711': y_pred17, '201712': y_pred17})\n   # set col 'ParceID' to first col\n   cols = output.columns.tolist()\n   cols = cols[-1:] + cols[:-1]\n   output = output[cols]\n\n   output.to_csv('sub{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)","a93acdca":"print(\"Mean absolute validation error without fudge factor: \", )\nprint( mean_absolute_error(y_valid, valid_pred) )\nif OPTIMIZE_FUDGE_FACTOR:\n    print(\"Mean absolute validation error with fudge factor:\")\n    print( mean_absolute_error(y_valid, fudge*valid_pred) )","81f46ac9":"\n\nVersion 24: Added Nikunj's features and retuned<br>\nVersion 25: Added more Nikunj features and retuned again. <br>\nVersion 26: Deleted some of Nikunj features and retuned again.<br>\nVersion 27: Remove Niknuj features and go to tuning that was optimal without them, as baseline<br>\nVersion 28: Same as version 27 but after having tested some Nikunj features individually<br>\nVersion 29: Add 2 best Nikunj features (zip_count, city_count)<br>\nVersion 30: Add 3rd feature (GarPoolAC), and some cleanup<br>\nVersion 32: Retune: colsample .7 -> .8<br>\nVersion 33: Retune: lambda=10, subsample=.55<br>\nVersion 34: Revert subsample=.5<br>\nVersion 35: Fine tune: lambda=9<br>\nVersion 36: Revert: colsample .7<br>\nVersion 37: Cleanup<br>\nVersion 38: Make boosting rounds and stopping rounds inversely proportional to learning rate<br>\nVersion 40: Add city_mean and zip_mean features<br>\nVersion 41: Fix comments (Previously mis-stated logerror as \"sale price\" in feature descriptions)<br>\nVersion 42: Fix bug in city_mean definition<br>\nVersion 43: Get rid of city_mean<br>\nVersion 44: Retune: alpha=0.5<br>\nVersion 45: fine tune: lambda=9.5<br>\nVersion 46: Roll back to version 39 model, because zip_mean had a data leak, and the corrected version doesn't help<br>\nVersion 47: Add additional aggregation features, including by neighborhood<br>\nVerison 48: Put test set features in the correct order<br>\nVersion 49: Retune: lambda=5, colsample=.55<br>\nVersion 50: Retune: alpha=.65, colsample=.50<br>\nVersion 51: Retune: max_depth=7<br>\nVersion 52: Make it optional to generate submission file when running full notebook<br>\nVersion 53. Option to do validation only<br>\nVersion 54. Starting to clean up the code<br>\nVersion 55. Option to fit final model to full training set<br>\nVersion 56. Optimize fudge factor<br>\nVersion 57. Allow change to validation set cutoff date<br>\nVersion 59. Try September 15 as validation cutoff<br>\nVersion 62. Allow final fit on 2017 (no correction for data leak)<br>\nVersion 68. Add seasonal features<br>\n &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(Turns out the seasonal features make the fudge factor largely irrelevant,<br>\n &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;but that's partly because I chose the basedate to fit the fudge factors.)<br>\n Version 71. Make separate predictions for 2017 using 2017 properties data<br>\n Version 72. Run with FIT_2017_TRAIN_SET = False<br>\n Version 73. Remove outliers from 2017 data and set FIT_2017_TRAIN_SET = True<br>\n Version 74. Set FIT_2017_TRAIN_SET = False again<br>\n  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(Removing outliers helps, but 2017 data still generate bad 2016 predictions.)<br>\n  Version 76. Allow fitting combined training set<br>"}}