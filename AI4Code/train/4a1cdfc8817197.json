{"cell_type":{"259be161":"code","720a353d":"code","a8708d3c":"code","2bae558c":"code","8d8757b4":"code","9a7f7596":"code","4eacf09e":"code","1b9a69c7":"code","ca6e4f96":"code","0d0afc15":"code","a7940e4b":"code","aebc8a80":"code","b423d0ef":"code","a3886e7b":"code","e016bd1b":"code","3ef28f6d":"code","29d70b1c":"code","cdfd30e1":"code","cf7bfcac":"markdown","f6bc9223":"markdown","95a49d88":"markdown","9a7deb3f":"markdown","59a01428":"markdown","faaf4b9b":"markdown","2201da91":"markdown","38173900":"markdown","332ee2e5":"markdown","6511a14a":"markdown","8623e57c":"markdown","dc92d04f":"markdown","becb15c7":"markdown","ff2090bf":"markdown","212772c6":"markdown","a7dbc043":"markdown","7cb613d1":"markdown"},"source":{"259be161":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","720a353d":"train = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv')","a8708d3c":"fig,ax = plt.subplots()\nplt.bar(train['target'].unique(), train['target'].value_counts(), color = ['red','blue','green','yellow'])\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\nplt.title('Counting target class for training data')","2bae558c":"train['target'].value_counts()","8d8757b4":"data = [train,test]\n\nfor d in data:\n    d.drop('id', axis = 1, inplace = True)","9a7f7596":"train_null_perc = pd.DataFrame(np.round(train[train == 0].count()\/len(train),2)*100, columns = ['Train_null_perc'])","4eacf09e":"train_null_perc.sort_values(by = 'Train_null_perc',ascending = False, inplace = True)\ntrain_null_perc.head()","1b9a69c7":"fig , ax = plt.subplots()\nsns.barplot(x = train_null_perc.index, y= train_null_perc['Train_null_perc'], ax= ax, dodge = False)\nplt.xticks(rotation=90)\nplt.title('Zero values for training data')\nsns.despine()","ca6e4f96":"X_cols = [col for col in train.columns if col not in ('target','id')]\ny_col = 'target'","0d0afc15":"from sklearn.pipeline import Pipeline\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.preprocessing import OneHotEncoder","a7940e4b":"X = train[X_cols].iloc[:3000,:]\ny = np.array(train[y_col])[:3000]","aebc8a80":"pipe = Pipeline([('ohe',OneHotEncoder(sparse = False)),('LDA', LinearDiscriminantAnalysis())])","b423d0ef":"Xt = pipe.fit_transform(X,y)","a3886e7b":"Xt.shape","e016bd1b":"Xt","3ef28f6d":"df = pd.DataFrame(Xt, columns = ['Component_1','Component_2','Component_3'])\ndf['target'] = train['target'][:3000]","29d70b1c":"df.head()","cdfd30e1":"g = sns.PairGrid(df , hue = 'target', palette = ['red','blue','green','yellow'])\ng.map(sns.scatterplot)\ng.add_legend()","cf7bfcac":"As you can see different components are useful to distinguish the classes.","f6bc9223":"Now every class is associated to a different combination of the 3 components.","95a49d88":"Component_1 and Component_2 seem useful to distinguish between Class_2 and Class_3 while Component_3 and Component_1 seem useful distinguish Class_1 and Component_3 and Component_2 to distinguish Class_4.","9a7deb3f":"Let's now visualize components and classes!","59a01428":"The features have a lot of zeros values in it. Let's count them.","faaf4b9b":"### LinearDiscriminantAnalysis for data visualization","2201da91":"Training dataset has 100000 rows and 50 features.\nClass_2 is the most represented class in the dataset while Class_3 it is the least represented class.","38173900":"A use of dimensionality reduction is visualization of datasets with a high numbers od features.\nThis dataset deals with predicting the category on an eCommerce product given various attributes about the listing. There are four different classes to predict and 50 different features.\nDimensionality reduction can be useful to reduce the number of features while preserving the variance of the original dataset. By reducing the numbers of features you can also make plot easier and find the components that enable to differentiate the different classes.\n","332ee2e5":"# Dimension reduction for EDA","6511a14a":"LDA works with no sparse data so we need to set sparse = False in OneHotEncoding.","8623e57c":"95% of values in features_13 are 0, followed by 93% of feature_2.\nLet's plot all percentages.","dc92d04f":"LDA can be applied not only to visualiza data but also to reduce numbers of features before developing a model.","becb15c7":"Let's now drop 'id' column.","ff2090bf":"I will apply Linear Discriminant Analysis to the first 3000 rows of the training dataset, so it will be faster.","212772c6":"Before applying LDA we need to HotEncode variables. We can create a pipe to apply OneHotEncoding and LinearDiscriminantAnalysis sequantially to training data.","a7dbc043":"After applying LDA we get an array with 3 components that we can convert in a dataframe for data visualization.","7cb613d1":"First let's count the number of times each class is present in the training data."}}