{"cell_type":{"a3711bd4":"code","75ae153b":"code","1a4b602c":"code","7f5d8dcc":"code","d1737fd4":"code","b7367bed":"code","171fee75":"code","b22b8a17":"code","d1fbc265":"code","97f1710b":"code","49bfb85a":"code","a4bd51ab":"code","1dc2023e":"code","1df7981f":"code","90054eec":"code","83f9e7e2":"code","13071bdd":"code","83e0aca4":"code","9f33601a":"code","5e03a90b":"code","1dc9082e":"code","3d50c6ed":"code","74ec28ca":"code","48b205ae":"code","bfd49146":"code","d87452c7":"code","6ede95a9":"code","11f1b75a":"code","16885c7d":"code","33807047":"code","1a1e25ae":"code","7286da3a":"code","5180bf09":"code","45067946":"code","a16ed74c":"code","a18bae4e":"code","cc530a1d":"code","32392025":"code","97b6d514":"markdown","5505d2c7":"markdown","4f206f93":"markdown","7f190f8b":"markdown","49157e36":"markdown","ad37106e":"markdown","6d00ab69":"markdown","b9ac2067":"markdown","57d1e352":"markdown","7676d10d":"markdown","ac80f1b5":"markdown","f16cc765":"markdown","b9e854e2":"markdown","41ad4b54":"markdown","dc96977f":"markdown","a2d2e27e":"markdown","54245708":"markdown","5e158903":"markdown","3299933d":"markdown","d48bdaea":"markdown","17df375b":"markdown","0a645d34":"markdown","079b2092":"markdown","a451bc73":"markdown","cac7feaa":"markdown","ce334e43":"markdown","ea6f1384":"markdown","27c35def":"markdown","6f7ce2b2":"markdown","4174b82d":"markdown","31e25612":"markdown"},"source":{"a3711bd4":"#%% Importing Libraries\n\n# Basic Imports \nimport numpy as np\nimport pandas as pd\nfrom IPython.display import display, HTML\n\n# Plotting \nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\n%matplotlib inline\n\n# Preprocessing\nfrom sklearn.model_selection import train_test_split, StratifiedKFold,cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\n\n# Metrics \nfrom sklearn.metrics import roc_auc_score\n\n# ML Models\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb\n\n# Model Tuning \nfrom bayes_opt import BayesianOptimization\n\n# Feature Importance \nimport shap \n\n# Ignore Warnings \nimport warnings\nwarnings.filterwarnings('ignore')","75ae153b":"#%% Read train.csv\ntrain_csv = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/train.csv')\n\n# Initial glance at train.csv\nprint(train_csv.info(verbose = True,show_counts=True))","1a4b602c":"#%% Read train.csv\ntest_csv = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/test.csv')\n\n# Initial glance at train.csv\nprint(test_csv.info(verbose = True,show_counts=True))","7f5d8dcc":"#%% PlotMultiplePie \n# Input: df = Pandas dataframe, categorical_features = list of features , dropna = boolean variable to use NaN or not\n# Output: prints multiple px.pie() \n\ndef PlotMultiplePie(df,categorical_features = None,dropna = False):\n    # set a threshold of 30 unique variables, more than 50 can lead to ugly pie charts \n    threshold = 30\n    \n    # if user did not set categorical_features \n    if categorical_features == None: \n        categorical_features = df.select_dtypes(['object','category']).columns.to_list()\n        \n    print(\"The Categorical Features are:\",categorical_features)\n    \n    # loop through the list of categorical_features \n    for cat_feature in categorical_features: \n        num_unique = df[cat_feature].nunique(dropna = dropna)\n        num_missing = df[cat_feature].isna().sum()\n        # prints pie chart and info if unique values below threshold \n        if num_unique <= threshold:\n            print('Pie Chart for: ', cat_feature)\n            print('Number of Unique Values: ', num_unique)\n            print('Number of Missing Values: ', num_missing)\n            fig = px.pie(df[cat_feature].value_counts(dropna = dropna), values=cat_feature, \n                 names = df[cat_feature].value_counts(dropna = dropna).index,title = cat_feature,template='ggplot2')\n            fig.show()\n        else: \n            print('Pie Chart for ',cat_feature,' is unavailable due high number of Unique Values ')\n            print('Number of Unique Values: ', num_unique)\n            print('Number of Missing Values: ', num_missing)\n            print('\\n')","d1737fd4":"#%% Use PlotMultiplePie to see the distribution of the categorical variables \nPlotMultiplePie(train_csv.drop([\"id\"],axis = \"columns\"))","b7367bed":"#%% Print the continous features in the dataset \ncontinous_features = train_csv.drop([\"id\",\"target\"],axis = \"columns\").select_dtypes(['float64']).columns.to_list()\n\nfor cont_feature in continous_features: \n    plt.figure()\n    plt.title(cont_feature)\n    ax = sns.histplot(train_csv[cont_feature])","171fee75":"PlotMultiplePie(train_csv,categorical_features = [\"target\"])","b22b8a17":"# save the 'id' for Train and Test \ntrain_csv_id = train_csv['id'].to_list()\ntest_csv_id = test_csv['id'].to_list()\n\n# Seperate train_csv into target and features \ny_train_csv = train_csv['target']\nX_train_csv = train_csv.drop('target',axis = 'columns')\n\n# Save the index for X_train_csv \nX_train_csv_index = X_train_csv.index.to_list()\n\n# Row bind train.csv features with test.csv features \n# this makes it easier to apply label encoding onto the entire dataset \nX_train_test = X_train_csv.append(test_csv,ignore_index = True)\n\n# save the index for test.csv \nX_test_csv_index = np.setdiff1d(X_train_test.index.to_list() ,X_train_csv_index) \n\n# drop id from X_total\nX_train_test = X_train_test.drop('id',axis = 'columns')\n\n# X_train_test.info()","d1fbc265":"#%% MultiColumnLabelEncoder\n# Code snipet found on Stack Exchange \n# https:\/\/stackoverflow.com\/questions\/24458645\/label-encoding-across-multiple-columns-in-scikit-learn\n# from sklearn.preprocessing import LabelEncoder\n\nclass MultiColumnLabelEncoder:\n    def __init__(self,columns = None):\n        self.columns = columns # array of column names to encode\n\n    def fit(self,X,y=None):\n        return self # not relevant here\n\n    def transform(self,X):\n        '''\n        Transforms columns of X specified in self.columns using\n        LabelEncoder(). If no columns specified, transforms all\n        columns in X.\n        '''\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                # convert float NaN --> string NaN\n                output[col] = output[col].fillna('NaN')\n                output[col] = LabelEncoder().fit_transform(output[col])\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = LabelEncoder().fit_transform(col)\n        return output\n\n    def fit_transform(self,X,y=None):\n        return self.fit(X,y).transform(X)\n\n# store the catagorical features names as a list      \ncat_features = X_train_test.select_dtypes(['object']).columns.to_list()\n\n# use MultiColumnLabelEncoder to apply LabelEncoding on cat_features \n# uses NaN as a value , no imputation will be used for missing data\nX_train_test_encoded = MultiColumnLabelEncoder(columns = cat_features).fit_transform(X_train_test)","97f1710b":"##% Split X_train_clean_encoded \nX_train_csv_encoded = X_train_test_encoded.iloc[X_train_csv_index, :]\nX_test_csv_encoded = X_train_test_encoded.iloc[X_test_csv_index, :].reset_index(drop = True) ","49bfb85a":"##% Before and After LabelEncoding for train.csv \ndisplay(X_train_csv.head().drop(\"id\",axis = 'columns'))\ndisplay(X_train_csv_encoded.head())","a4bd51ab":"##% Before and After LabelEncoding for test.csv \ndisplay(test_csv.head().drop(\"id\",axis = 'columns'))\ndisplay(X_test_csv_encoded.head())","1dc2023e":"# Create test and train set 80-20\n#%%  train-test stratified split using a 80-20 split\nX_train, X_test, y_train, y_test = train_test_split(X_train_csv_encoded, y_train_csv, test_size=0.2, shuffle = True, stratify = y_train_csv, random_state=0)\n\nfor df in [X_train, X_test, y_train, y_test]:\n    df.reset_index(drop = True,inplace = True)\n    \nprint(\" Training Target\")\nprint(y_train.value_counts())\nprint(\"\\n\")\nprint(\" Test Target\")\nprint(y_test.value_counts())","1df7981f":"display(X_train)\ndisplay(X_test)","90054eec":"#% Initial Models\nRFC = RandomForestClassifier(n_estimators = 50, max_depth = 10, n_jobs = -1,random_state = 0).fit(X_train, y_train)\nLGBMC = lgb.LGBMClassifier(num_leaves = 50,max_depth = 10,random_state=0).fit(X_train,y_train)","83f9e7e2":"print(\"     Random Forest Classifier\")\nprint(\"Training Dataset\")\nprint(\"ROC_AUC_SCORE: \",roc_auc_score(y_train,RFC.predict_proba(X_train)[:,1]))\nprint(\"Test Dataset\")\nprint(\"ROC_AUC_SCORE: \",roc_auc_score(y_test,RFC.predict_proba(X_test)[:,1]))\n\nprint(\"\\n\")\n\nprint(\"     LGBMClassifier\")\nprint(\"Training Dataset\")\nprint(\"ROC_AUC_SCORE: \",roc_auc_score(y_train,LGBMC.predict_proba(X_train)[:,1]))\nprint(\"Test Dataset\")\nprint(\"ROC_AUC_SCORE: \",roc_auc_score(y_test,LGBMC.predict_proba(X_test)[:,1]))\n","13071bdd":"# https:\/\/github.com\/fmfn\/BayesianOptimization\n# https:\/\/github.com\/fmfn\/BayesianOptimization\/blob\/master\/bayes_opt\/bayesian_optimization.py\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_val_score.html\n# https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n# https:\/\/tech.ovoenergy.com\/bayesian-optimisation\/\n# https:\/\/www.kdnuggets.com\/2019\/07\/xgboost-random-forest-bayesian-optimisation.html\n#crash\ndef search_best_param_rf(X, y):\n    def rf_cv(X, y, **kwargs):\n        estimator = RandomForestClassifier(**kwargs)\n        cval = cross_val_score(\n            estimator,\n            X,\n            y,\n            scoring=\"roc_auc\",\n            cv=5,\n            verbose=0,\n            n_jobs=-1,\n            error_score=0,\n        )\n        return cval.mean()\n\n    def rf_crossval(n_estimators, max_depth, min_samples_split, min_samples_leaf):\n        return rf_cv(\n            X=X,\n            y=y,\n            n_estimators=int(n_estimators),\n            max_depth=int(max(max_depth, 1)),\n            min_samples_split=int(max(min_samples_split, 2)),\n            min_samples_leaf=int(max(min_samples_leaf, 1)),\n        )\n    \n    RFC_BO_params = {\n        \"n_estimators\": (10, 100),\n        \"max_depth\": (1, 100),\n        \"min_samples_split\": (2, 10),\n        \"min_samples_leaf\": (1, 5),\n    }\n\n    RFC_Bo = BayesianOptimization(rf_crossval, \n                                  RFC_BO_params, \n                                  random_state=0, \n                                  verbose=2\n                                 )\n    np.random.seed(1)\n    \n    RFC_Bo.maximize(init_points=2, n_iter=2)\n    # n_iter: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n    # init_points: How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space.\n    # more iterations more time spent searching \n    \n    params_set = RFC_Bo.max['params']\n    \n    params_set['n_estimators'] = int(round(params_set['n_estimators']))\n    params_set['max_depth'] = int(round(params_set['max_depth']))\n    params_set['min_samples_split'] = int(round(params_set['min_samples_split']))\n    params_set['min_samples_leaf'] = int(round(params_set['min_samples_leaf']))\n    \n    params_set.update({'n_jobs': -1})\n    params_set.update({'random_state': 0})\n    \n    return params_set","83e0aca4":"# Random Forest Cross Validation\n\ndef K_Fold_RandomForest(X_train,y_train, params_set = [], num_folds = 5):\n    model_num = 0 # model number \n    models = [] # model list\n    folds = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=0) # create folds\n\n        # num_folds times ; default is 5\n    for n_fold, (train_idx, valid_idx) in enumerate (folds.split(X_train, y_train)):\n        \n        print(f\"     model{model_num}\")\n        \n        train_X, train_y = X_train.iloc[train_idx], y_train.iloc[train_idx]\n        valid_X, valid_y = X_train.iloc[valid_idx], y_train.iloc[valid_idx]\n        \n        if (params_set == []): # if param_set is empty\n            # find best param_set in each fold, can lead to overfitting\n            params_set = search_best_param(train_X,train_y,cat_features) \n        \n        # fit RFC based of param_set and current fold\n        CV_RF = RandomForestClassifier(**params_set).fit(train_X, train_y)\n        \n        # append RF model to model list \n        models.append(CV_RF)\n        \n        # model metrics for current fold \n        print(\"Training Dataset\")\n        print(\"ROC_AUC_SCORE: \",roc_auc_score(y_train,models[model_num].predict_proba(X_train)[:,1]))\n        print(\"Test Dataset\")\n        print(\"ROC_AUC_SCORE: \",roc_auc_score(y_test,models[model_num].predict_proba(X_test)[:,1]))\n        print(\"\\n\")\n        \n        model_num = model_num + 1\n        \n    return models","9f33601a":"best_params_rf_cv = search_best_param_rf(X_train_csv_encoded,y_train_csv)","5e03a90b":"# Print best_params_rf_cv\nfor key, value in best_params_rf_cv.items():\n    print(key, ' : ', value)","1dc9082e":"rf_models = K_Fold_RandomForest(X_train_csv_encoded,y_train_csv,params_set = best_params_rf_cv,num_folds = 5)","3d50c6ed":"# Predict y_prds using models from RFC cross validation \ndef predict_models_RFC(models_cv,X):\n    y_preds = np.zeros(shape = X.shape[0])\n    for model in models_cv:\n        y_preds += model.predict_proba(X)[:,1]\n        \n    return y_preds\/len(models_cv)","74ec28ca":"# RFC Cross Validation Model Performance\nprint(\"     RFC Cross Validation\")\nprint(\"Total Dataset\")\nprint(\"ROC_AUC_SCORE: \",roc_auc_score(y_train_csv,predict_models_RFC(rf_models,X_train_csv_encoded)))","48b205ae":"##% parameter tuning for lightgbm \n# store the catagorical features names as a list      \ncat_features = X_train_test.select_dtypes(['object']).columns.to_list()\n# print(cat_features)\n\n# Create the LightGBM data containers\n# Make sure that cat_features are used\ntrain_lgbdata=lgb.Dataset(X_train,label=y_train, categorical_feature = cat_features,free_raw_data=False)\ntest_lgbdata=lgb.Dataset(X_test,label=y_test, categorical_feature = cat_features,free_raw_data=False)","bfd49146":"# https:\/\/github.com\/fmfn\/BayesianOptimization\ndef search_best_param(X,y,cat_features):\n    \n    trainXY = lgb.Dataset(data=X, label=y,categorical_feature = cat_features,free_raw_data=False)\n    # define the lightGBM cross validation\n    def lightGBM_CV(max_depth, num_leaves, n_estimators, learning_rate, subsample, colsample_bytree, \n                lambda_l1, lambda_l2, min_child_weight):\n    \n        params = {'boosting_type': 'gbdt', 'objective': 'binary', 'metric':'auc', 'verbose': -1,\n                  'early_stopping_round':100}\n        \n        params['max_depth'] = int(round(max_depth))\n        params[\"num_leaves\"] = int(round(num_leaves))\n        params[\"n_estimators\"] = int(round(n_estimators))\n        params['learning_rate'] = learning_rate\n        params['subsample'] = subsample\n        params['colsample_bytree'] = colsample_bytree\n        params['lambda_l1'] = max(lambda_l1, 0)\n        params['lambda_l2'] = max(lambda_l2, 0)\n        params['min_child_weight'] = min_child_weight\n    \n        score = lgb.cv(params, trainXY, nfold=5, seed=1, stratified=True, verbose_eval =False, metrics=['auc'])\n        return np.mean(score['auc-mean']) # maximize auc-mean\n\n    # use bayesian optimization to search for the best hyper-parameter combination\n    lightGBM_Bo = BayesianOptimization(lightGBM_CV, \n                                       {\n                                          'max_depth': (5, 50),\n                                          'num_leaves': (20, 100),\n                                          'n_estimators': (50, 500),\n                                          'learning_rate': (0.01, 0.3),\n                                          'subsample': (0.7, 0.8),\n                                          'colsample_bytree' :(0.5, 0.99),\n                                          'lambda_l1': (0, 5),\n                                          'lambda_l2': (0, 3),\n                                          'min_child_weight': (2, 50) \n                                      },\n                                       random_state = 1,\n                                       verbose = 1\n                                      )\n    np.random.seed(1)\n    \n    lightGBM_Bo.maximize(init_points= 2, n_iter=2) # 2 + 2, 4 iterations \n    # n_iter: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n    # init_points: How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space.\n    # more iterations more time spent searching \n    \n    params_set = lightGBM_Bo.max['params']\n    \n    # get the params of the maximum target     \n    max_target = -np.inf\n    for i in lightGBM_Bo.res: # loop thru all the residuals \n        if i['target'] > max_target:\n            params_set = i['params']\n            max_target = i['target']\n    \n    params_set.update({'verbose': -1})\n    params_set.update({'metric': 'auc'})\n    params_set.update({'boosting_type': 'gbdt'})\n    params_set.update({'objective': 'binary'})\n    \n    params_set['max_depth'] = int(round(params_set['max_depth']))\n    params_set['num_leaves'] = int(round(params_set['num_leaves']))\n    params_set['n_estimators'] = int(round(params_set['n_estimators']))\n    params_set['seed'] = 1 #set seed\n    \n    return params_set","d87452c7":"best_params = search_best_param(X_train,y_train,cat_features)","6ede95a9":"# Print best_params\nfor key, value in best_params.items():\n    print(key, ' : ', value)","11f1b75a":"# Train lgbm_best using the best params found from Bayesian Optimization\nlgbm_best = lgb.train(best_params,\n                 train_lgbdata,\n                 num_boost_round = 100,\n                 valid_sets = test_lgbdata,\n                 early_stopping_rounds = 100,\n                 verbose_eval = 50\n                 )","16885c7d":"print(\"     LGBM Tuned\")\nprint(\"Training Dataset\")\nprint(\"ROC_AUC_SCORE: \",roc_auc_score(y_train,lgbm_best.predict(X_train)))\nprint(\"Test Dataset\")\nprint(\"ROC_AUC_SCORE: \",roc_auc_score(y_test,lgbm_best.predict(X_test)))","33807047":"##% Feature Importance \n# https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_forest_importances.html\nlgb.plot_importance(lgbm_best,figsize=(25,20),max_num_features = 10)","1a1e25ae":"##% Feature Importance using shap package \n# import shap\nshap_values = shap.TreeExplainer(lgbm_best).shap_values(X_test[:2500])\nshap.summary_plot(shap_values, X_test[:2500])","7286da3a":"# Cross Validation with LightGBM\n\ndef K_Fold_LightGBM(X_train, y_train , cat_features, num_folds = 5, params_set = []):\n    num = 0 # model number\n    models = [] # list of models \n    folds = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=0) # create folds \n\n        # num_folds times \n    for n_fold, (train_idx, valid_idx) in enumerate (folds.split(X_train, y_train)):\n        \n        print(f\"     model{num}\")\n        train_X, train_y = X_train.iloc[train_idx], y_train.iloc[train_idx]\n        valid_X, valid_y = X_train.iloc[valid_idx], y_train.iloc[valid_idx]\n        \n        train_data=lgb.Dataset(train_X,label=train_y, categorical_feature = cat_features,free_raw_data=False)\n        valid_data=lgb.Dataset(valid_X,label=valid_y, categorical_feature = cat_features,free_raw_data=False)\n        \n        \n        # params_set = search_best_param(train_X,train_y,cat_features) # find best param_set in each fold\n        \n        CV_LGBM = lgb.train(params_set,\n                            train_data,\n                            num_boost_round = 100,\n                            valid_sets = valid_data,\n                            early_stopping_rounds = 100,\n                            verbose_eval = 50\n                           )\n        # increase early_stopping_rounds can lead to overfitting \n        \n        # append LGBM model to models list \n        models.append(CV_LGBM)\n        \n        # model metrics for each fold \n        print(\"Training Dataset\")\n        print(\"ROC_AUC_SCORE: \",roc_auc_score(y_train,models[num].predict(X_train)))\n        print(\"Test Dataset\")\n        print(\"ROC_AUC_SCORE: \",roc_auc_score(y_test,models[num].predict(X_test)))\n        print(\"\\n\")\n        \n        num = num + 1\n        \n    return models","5180bf09":"best_params_cv = search_best_param(X_train_csv_encoded,y_train_csv,cat_features)","45067946":"lgbm_models = K_Fold_LightGBM(X_train_csv_encoded,y_train_csv,cat_features,5,params_set = best_params_cv)","a16ed74c":"# Predict y_prds using models from cross validation \ndef predict_models_LGBM(models_cv,X):\n    y_preds = np.zeros(shape = X.shape[0])\n    for model in models_cv:\n        y_preds += model.predict(X)\n        \n    return y_preds\/len(models_cv)","a18bae4e":"# LightGBM Cross Validation Model Performance\nprint(\"     LGBM Cross Validation\")\nprint(\"Total Dataset\")\nprint(\"ROC_AUC_SCORE: \",roc_auc_score(y_train_csv,predict_models_LGBM(lgbm_models,X_train_csv_encoded)))","cc530a1d":"# Prediction for Test.csv using LightGBM CV \npredictLGBM = predict_models_LGBM(lgbm_models,X_test_csv_encoded) \n\nsubmissionLGBM = pd.DataFrame({'id':test_csv_id,'target':predictLGBM})\n\ndisplay(submissionLGBM.head())\n\n# Prediction for Test.csv using RFC CV \npredictRFC = predict_models_RFC(rf_models,X_test_csv_encoded) \n\nsubmissionRFC = pd.DataFrame({'id':test_csv_id,'target':predictRFC})\n\ndisplay(submissionRFC.head())","32392025":"#% Submit Predictions \nsubmissionLGBM.to_csv('submissionCV_LGBM4.csv',index=False)\nsubmissionRFC.to_csv('submissionCV_RFC4.csv',index=False)","97b6d514":"<a id=\"Bayesian-Optimization\"><\/a>\n## Bayesian Optimization","5505d2c7":"<a id=\"Train.csv\"><\/a>\n## Train.csv","4f206f93":"<a id=\"Table-Of-Contents\"><\/a>\n# Table Of Contents\n* [Table Of Contents](#Table-Of-Contents)\n* [Introduction](#Introduction)\n* [Importing Libraries](#Importing-Libraries)\n* [Task Details](#Task-Details)\n* [Read in Data](#Read-in-Data)\n    - [Train.csv](#Train.csv)\n    - [Test.csv](#Test.csv)\n    - [Notes](#Notes)\n* [Data Visualization](#Data-Visualization)\n    - [Categorical Features](#Categorical-Features)\n    - [Continuous Features](#Continuous-Features)\n    - [Target](#Target)\n* [Preprocessing Data](#Preprocessing-Data)\n    - [Label Encoding](#Label-Encoding)\n    - [Train-Test Stratified Split](#Train-Test-Stratified-Split)\n* [Random Forest Classifier](#Random-Forest-Classifier)\n    - [Random Forest Bayesian Optimization](#Random-Forest-Bayesian-Optimization)\n    - [Random Forest Cross Validation](#Random-Forest-Cross-Validation)\n    - [Random Forest CV Model Peformance](#Random-Forest-CV-Model-Peformance)\n* [LightGBM Classifier](#LightGBM-Classifier)\n    - [Bayesian Optimization](#Bayesian-Optimization)\n    - [Tuning LightGBM](#Tuning-LightGBM)\n    - [Feature Importance](#Feature-Importance)\n    - [Cross Validation](#Cross-Validation)\n    - [LightGBM CV Model Peformance](#LightGBM-CV-Model-Peformance)\n* [Prediction for Test.csv](#Prediction-for-Test.csv)\n* [Conclusion](#Conclusion)","7f190f8b":"<a id=\"Categorical-Features\"><\/a>\n## Categorical Features","49157e36":"<a id=\"LightGBM-Model-Peformance \"><\/a>\n## LightGBM Model Peformance ","ad37106e":"<a id=\"Random-Forest-Bayesian-Optimization\"><\/a>\n## Random Forest Bayesian Optimization","6d00ab69":"<a id=\"Random-Forest-Classifier\"><\/a>\n# Random Forest Classifier","b9ac2067":"<a id=\"Label-Encoding\"><\/a>\n## Label Encoding","57d1e352":"<a id=\"Tuning-LightGBM\"><\/a>\n## Tuning LightGBM","7676d10d":"<a id=\"Continuous-Features\"><\/a>\n## Continuous Features","ac80f1b5":"<a id=\"Initial Models\"><\/a>\n# Initial Models\nI applied different machine learning algorthims to test which model perform better on this dataset. I've listed below various machine learning techniques applied in this section.\n \n1. Random Forest Classifier \n2. LightGBM Classifier","f16cc765":"<a id=\"Test.csv\"><\/a>\n## Test.csv","b9e854e2":"<a id=\"Prediction-for-Test.csv\"><\/a>\n# Prediction for Test.csv","41ad4b54":"<a id=\"Task-Details\"><\/a>\n# Task Detail \n\n## Goal\nFor this competition, you will be predicting a **binary target** based on a number of feature columns given in the data. All of the feature columns, **cat0** - **cat18** are **categorical**, and the feature columns **cont0** - **cont10** are continuous.\n\n## Metric\nSubmissions are evaluated on [area under the ROC curve](https:\/\/en.wikipedia.org\/wiki\/Receiver_operating_characteristic) between the predicted probability and the observed target.","dc96977f":"<a id=\"Conclusion\"><\/a>\n# Conclusion\n\n**Conclusion**\n* LightGBM is a great ML algorithm that handles categorical features and missing values \n* Cross Validation is useful to combat overfitting \n* Bayesian Optimization is necessary to get hyper parameters when building an initial model\n* This is a great dataset to work on and lots of knowledge can be gain from withing with this dataset \n* Researching and reading other Kaggle notebooks is essential for becoming a better data scientist\n\n**Challenges**\n* Due to the size of the dataset my algorithms took a while to run \n* Overfitting might have occurred which reduces the model performance on the test set\n\n**Closing Remarks**  \n* Please comment and like the notebook if it of use to you! Have a wonderful year! \n\n\n**Other Notebooks** \n* [https:\/\/www.kaggle.com\/josephchan524\/studentperformanceregressor-rmse-12-26-r2-0-26](https:\/\/www.kaggle.com\/josephchan524\/studentperformanceregressor-rmse-12-26-r2-0-26)\n* [https:\/\/www.kaggle.com\/josephchan524\/bankchurnersclassifier-recall-97-accuracy-95](https:\/\/www.kaggle.com\/josephchan524\/bankchurnersclassifier-recall-97-accuracy-95)\n* [https:\/\/www.kaggle.com\/josephchan524\/housepricesregressor-using-lightgbm](https:\/\/www.kaggle.com\/josephchan524\/housepricesregressor-using-lightgbm)\n* [https:\/\/www.kaggle.com\/josephchan524\/tabularplaygroundregressor-using-lightgbm-feb2021](https:\/\/www.kaggle.com\/josephchan524\/tabularplaygroundregressor-using-lightgbm-feb2021)\n\n\n3-12-2020\nJoseph Chan ","a2d2e27e":"<a id=\"Data-Visualization\"><\/a>\n# Data Visualization ","54245708":"<a id=\"Random-Forest-Cross-Validation\"><\/a>\n## Random Forest Cross Validation ","5e158903":"<a id=\"Notes\"><\/a>\n## Notes\n\nTrain.csv and Test.csv have no missing values so imputation is not needed. Since there aren't many features in this dataset I can do a quick explanatory data analysis on the features and target.","3299933d":"<a id=\"Conclusion\"><\/a>\n# Conclusion","d48bdaea":"<a id=\"Importing-Libraries\"><\/a>\n# Importing Libraries","17df375b":"<a id=\"Target\"><\/a>\n## Target","0a645d34":"<a id=\"LightGBM-CV-Model-Peformance \"><\/a>\n## LightGBM CV Model Peformance ","079b2092":"<a id=\"Cross-Validation \"><\/a>\n## Cross Validation ","a451bc73":"<a id=\"Introduction\"><\/a>\n# Introduction\nThis is my third competition notebook on Kaggle. I hope to learn more about working with tabular data and I hope anyone who reads this learns more as well! This notebook will be working with a classfication task. If you have any questions or comments please leave below! ","cac7feaa":"# <center>TabularPlaygroundClassifier MAR2021<\/center>\n<img src= \"https:\/\/wallpaperaccess.com\/full\/1782494.jpg\" height=\"200\" align=\"center\"\/>\n","ce334e43":"<a id=\"LightGBM-Classifier\"><\/a>\n# LightGBM Classifier","ea6f1384":"<a id=\"Feature-Importance \"><\/a>\n## Feature Importance ","27c35def":"<a id=\"#Train-Test-Stratified-Split\"><\/a>\n## Train-Test Stratified Split","6f7ce2b2":"<a id=\"Random-Forest-CV-Model-Peformance\"><\/a>\n## Random Forest CV Model Peformance ","4174b82d":"<a id=\"Preprocessing-Data\"><\/a>\n# Preprocessing Data\nBecause Train.csv and Test.csv have no missing data imputation is not needed.  \nLabel encoding is still require as this dataset has categorical features ","31e25612":"<a id=\"Read-in-Data\"><\/a>\n# Read in Data"}}