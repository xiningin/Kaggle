{"cell_type":{"8d6b70d4":"code","054fbb30":"code","dcaeec4d":"code","e5efc972":"code","9a81961f":"code","ba5e07ad":"code","592f3510":"code","e0fdcb19":"code","af9496ed":"code","97a1c1be":"code","667b6ea4":"code","939b4b24":"code","b7f3bfc8":"code","56eb4d93":"markdown","f5e961e9":"markdown","0677c7a5":"markdown","9c793ded":"markdown"},"source":{"8d6b70d4":"! pip install mglearn\nimport os\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nimport mglearn\nimport numpy as np\nimport pandas as pd\nimport seaborn as sn\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.corpus import stopwords \nfrom string import punctuation\nfrom gensim.sklearn_api import W2VTransformer\n\nfrom subprocess import check_output\nfrom wordcloud import WordCloud, STOPWORDS","054fbb30":"data = pd.read_csv(\"..\/input\/SPAM text message 20170820 - Data.csv\")","dcaeec4d":"def strip_punctuation(s):\n    return ''.join(c for c in s if c not in punctuation)","e5efc972":"def digit_punctuation(s):\n    return ''.join([i for i in s if not i.isdigit()])","9a81961f":"def cleanupDoc(s):\n stopset = set(stopwords.words('english'))\n tokens = nltk.word_tokenize(s)\n cleanup = [token.lower() for token in tokens if token.lower() not in stopset and  len(token)>2]\n return cleanup","ba5e07ad":"texts1 = []\nlabels = []\nfor i, label in enumerate(data['Category']):\n    strip_no_punt=strip_punctuation(data['Message'][i].lower())\n    sttrip_nodigit=digit_punctuation(strip_no_punt)\n    texts1.append(cleanupDoc(sttrip_nodigit))\n    if label == 'ham':\n        labels.append(0)\n    else:\n        labels.append(1)","592f3510":"wordcloud = WordCloud(background_color='white',max_words=200,max_font_size=200,width=1000, height=860, random_state=42).generate(str(texts1))\nfig = plt.figure(1)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()\nfig.savefig(\"word1.png\", dpi=1000)","e0fdcb19":"texts2 = []\nlabels = []\nfor i, label in enumerate(data['Category']):\n    strip_no_punt=strip_punctuation(data['Message'][i].lower())\n    sttrip_nodigit=digit_punctuation(strip_no_punt)\n    texts2.append(sttrip_nodigit)\n    if label == 'ham':\n        labels.append(0)\n    else:\n        labels.append(1)","af9496ed":"texts2 = np.asarray(texts2)\nlabels = np.asarray(labels)\nX_train, X_test, y_train, y_test = train_test_split(texts2 , labels, test_size=0.2, random_state=42)","97a1c1be":"vect = CountVectorizer().fit(np.asarray(X_train))\nX_train_cv_without_stop_word = vect.transform(np.asarray(X_train))\nX_test_cv_without_stop_word = vect.transform(np.asarray(X_test))","667b6ea4":"logreg = LogisticRegression()\nparam_grid = {'C': [0.01, 0.1, 1, 10, 100]}\ngrid = GridSearchCV(logreg, param_grid, cv=5)\nlogreg_train = grid.fit(X_train_cv_without_stop_word, y_train)\npred_logreg = logreg_train.predict(X_test_cv_without_stop_word )\nconfusion = confusion_matrix(y_test, pred_logreg)\ndf_cm = pd.DataFrame(confusion, ['ham','spam'],['ham','spam'])\nsn.set(font_scale=1.4)#for label size\nsn.heatmap(df_cm, annot=True,annot_kws={\"size\": 16})# font size","939b4b24":"max_value = X_train_cv_without_stop_word.max(axis=0).toarray().ravel()\nsorted_by_tfidf = max_value.argsort()\n\nfeature_names = np.array(vect.get_feature_names())\n\nprint(\"features with lowest cv_with_stop_word\")\nprint(feature_names[sorted_by_tfidf[:20]], '\\n')\n\nprint(\"features with highest cv_with_stop_word\")\nprint(feature_names[sorted_by_tfidf[-20:]])","b7f3bfc8":"mglearn.tools.visualize_coefficients(grid.best_estimator_.coef_, feature_names, n_top_features=20)\nplt.title(\"TfidfVectorizer_with_stop_word-cofficient\")","56eb4d93":"**Part 2 Create Word Cloud **\nIn the second module, you will learn how to create word cloud\n","f5e961e9":"**Part 1 Import Dataset and Create Clean Dataset **\nIn the first module, you will learn how to import dataset and perform some basic clean steps\n","0677c7a5":"**Part 3 Create M.L. pipeline**\nIn the third module, you will learn how to build text classification pipeline","9c793ded":"**Part 4 Using mglearn to create visualization to realize the pattern of word**\nIn the third module, you will learn how to mglearn to create visualization"}}