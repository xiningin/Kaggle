{"cell_type":{"c8156f2b":"code","e34d0b7e":"code","de36882b":"code","831f6a11":"code","553f3cab":"code","6352c449":"code","83b55c73":"code","757c24ce":"code","08f3b4c8":"code","65f46142":"code","46f53078":"code","60177984":"markdown","2b687129":"markdown","17bde8d6":"markdown","805ca313":"markdown","b22ea6eb":"markdown","16ad21b0":"markdown","fc303cce":"markdown","752158fa":"markdown","45cf147f":"markdown","65158cff":"markdown","4592acf1":"markdown","91cc65b2":"markdown","0ed05eb4":"markdown"},"source":{"c8156f2b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader as DL\nfrom torch.nn.utils import weight_norm as WN\nimport torch.nn.functional as F\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n\nimport os\nimport gc\nfrom time import time\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\n\nsi_mean = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\nseed = 0","e34d0b7e":"def breaker():\n    print(\"\\n\" + 50*\"-\" + \"\\n\")\n    \ndef head(x=None, no_of_ele=5):\n    print(x[:no_of_ele])\n    \ndef getCol(x=None):\n    return [col for col in x.columns]\n\ndef getAccuracy(y_pred=None, y_true=None):\n    y_pred, y_true = torch.sigmoid(y_pred).detach(), y_true.detach()\n\n    y_pred[y_pred <= 0.5] = 0\n    y_pred[y_pred > 0.5] = 1\n\n    return torch.count_nonzero(y_pred == y_true).item() \/ len(y_pred)\n\ndef preprocess(x=None, *args):\n    df = x.copy()\n    df[args[0]] = df[args[0]].map({\"Other\" : 0, \"Female\" : 1, \"Male\" : 2})\n    df[args[1]] = df[args[1]].map({\"No\" : 0, \"Yes\" : 1})\n    df[args[2]] = df[args[2]].map({\"children\" : 0, \"Never_worked\" : 1, \"Govt_job\" : 2, \"Self-employed\" : 3, \"Private\" : 4})\n    df[args[3]] = df[args[3]].map({\"Urban\" : 0, \"Rural\" : 1})\n    df[args[4]] = df[args[4]].map({\"smokes\" : 0, \"formerly smoked\" : 1, \"never smoked\" : 2, \"Unknown\" : 3})\n    return df","de36882b":"data = pd.read_csv(\"..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\", engine=\"python\")\n\nbreaker()\nprint(\"Dataset Shape : {}\".format(data.shape))\nbreaker()\nfor col in getCol(data):\n    print(col + \" - \" + repr(data[col].nunique()))\nbreaker()","831f6a11":"plt.figure(figsize=(20, 9))\nplt.subplot(2, 5, 1)\nsns.countplot(data=data, x=\"gender\")\nplt.subplot(2, 5, 2)\nsns.histplot(data=data, x=\"age\", kde=True) \nplt.subplot(2, 5, 3)\nsns.countplot(data=data, x=\"hypertension\")\nplt.subplot(2, 5, 4)\nsns.countplot(data=data, x=\"heart_disease\")\nplt.subplot(2, 5, 5)\nsns.countplot(data=data, x=\"ever_married\")\nplt.subplot(2, 5, 6)\nsns.countplot(data=data, x=\"work_type\")\nplt.subplot(2, 5, 7)\nsns.countplot(data=data, x=\"Residence_type\")\nplt.subplot(2, 5, 8)\nsns.histplot(data=data, x=\"avg_glucose_level\", kde=True) \nplt.subplot(2, 5, 9)\nsns.histplot(data=data, x=\"bmi\", kde=True)\nplt.subplot(2, 5, 10)\nsns.countplot(data=data, x=\"smoking_status\")\nplt.show()\n\nsns.countplot(data=data, x=\"stroke\")\nplt.show()","553f3cab":"data = preprocess(data, \"gender\", \"ever_married\", \"work_type\", \"Residence_type\", \"smoking_status\")\n\ndata = si_mean.fit_transform(data)\n\ntest_features,   test_labels = data[5000:, 1:-1], data[5000:, -1] \ntrain_features, train_labels = data[:5000, 1:-1], data[:5000, -1]","6352c449":"class DS(Dataset):\n    def __init__(self, X=None, y=None, mode=\"train\"):\n        self.mode = mode\n        self.X = X\n        if self.mode == \"train\" or self.mode == \"valid\":\n            self.y = y\n    \n    def __len__(self):\n        return self.X.shape[0]\n    \n    def __getitem__(self, idx):\n        if self.mode == \"train\" or self.mode == \"valid\":\n            return torch.FloatTensor(self.X[idx]), torch.FloatTensor(self.y[idx])\n        else:\n            return torch.FloatTensor(self.X[idx])","83b55c73":"class CFG:\n    ts_batch_size = 128\n    IL = 10\n    OL = 1\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    def __init__(self, HL=None, epochs=None, n_folds=None, batch_size=None, tr_va_split=0.2):\n        self.HL = HL\n        self.epochs = epochs\n        self.n_folds = n_folds\n        self.tr_batch_size = batch_size\n        self.va_batch_size = batch_size\n        self.tr_va_split = tr_va_split","757c24ce":"class Classifier(nn.Module):\n    def __init__(self, IL=None, HL=None, OL=None, use_DP=False, DP=0.5):\n\n        super(Classifier, self).__init__()\n\n        self.use_DP = use_DP\n        if self.use_DP:\n            self.DP_ = nn.Dropout(p=DP)\n\n        self.HL = HL\n\n        if len(self.HL) == 1:\n            self.BN1 = nn.BatchNorm1d(num_features=IL, eps=1e-5)\n            self.FC1 = WN(nn.Linear(in_features=IL, out_features=HL[0]))\n\n            self.BN2 = nn.BatchNorm1d(num_features=HL[0], eps=1e-5)\n            self.FC2 = WN(nn.Linear(in_features=HL[0], out_features=OL))\n\n        elif len(self.HL) == 2:\n            self.BN1 = nn.BatchNorm1d(num_features=IL, eps=1e-5)\n            self.FC1 = WN(nn.Linear(in_features=IL, out_features=HL[0]))\n\n            self.BN2 = nn.BatchNorm1d(num_features=HL[0], eps=1e-5)\n            self.FC2 = WN(nn.Linear(in_features=HL[0], out_features=HL[1]))\n\n            self.BN3 = nn.BatchNorm1d(num_features=HL[1], eps=1e-5)\n            self.FC3 = WN(nn.Linear(in_features=HL[1], out_features=OL))\n\n        elif len(self.HL) == 3:\n            self.BN1 = nn.BatchNorm1d(num_features=IL, eps=1e-5)\n            self.FC1 = WN(nn.Linear(in_features=IL, out_features=HL[0]))\n\n            self.BN2 = nn.BatchNorm1d(num_features=HL[0], eps=1e-5)\n            self.FC2 = WN(nn.Linear(in_features=HL[0], out_features=HL[1]))\n\n            self.BN3 = nn.BatchNorm1d(num_features=HL[1], eps=1e-5)\n            self.FC3 = WN(nn.Linear(in_features=HL[1], out_features=HL[2]))\n\n            self.BN4 = nn.BatchNorm1d(num_features=HL[2], eps=1e-5)\n            self.FC4 = WN(nn.Linear(in_features=HL[2], out_features=OL))\n\n    def getOptimizer(self, lr=1e-3, wd=0):\n        return optim.Adam(self.parameters(), lr=lr, weight_decay=wd)\n\n    def getPlateauLR(self, optimizer=None, patience=5, eps=1e-6):\n        return optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, patience=patience, eps=eps, verbose=True)\n\n    def forward(self, x):\n        if not self.use_DP:\n            if len(self.HL) == 1:\n                x = F.relu(self.FC1(self.BN1(x)))\n                x = self.FC2(self.BN2(x))\n\n                return x\n\n            elif len(self.HL) == 2:\n                x = F.relu(self.FC1(self.BN1(x)))\n                x = F.relu(self.FC2(self.BN2(x)))\n                x = self.FC3(self.BN3(x))\n\n                return x\n\n            elif len(self.HL) == 3:\n                x = F.relu(self.FC1(self.BN1(x)))\n                x = F.relu(self.FC2(self.BN2(x)))\n                x = F.relu(self.FC3(self.BN3(x)))\n                x = self.FC4(self.BN4(x))\n\n                return x\n        else:\n            if len(self.HL) == 1:\n                x = F.relu(self.DP_(self.FC1(self.BN1(x))))\n                x = self.FC2(self.BN2(x))\n\n                return x\n\n            elif len(self.HL) == 2:\n                x = F.relu(self.DP_(self.FC1(self.BN1(x))))\n                x = F.relu(self.DP_(self.FC2(self.BN2(x))))\n                x = self.FC3(self.BN3(x))\n\n                return x\n\n            elif len(self.HL) == 3:\n                x = F.relu(self.DP_(self.FC1(self.BN1(x))))\n                x = F.relu(self.DP_(self.FC2(self.BN2(x))))\n                x = F.relu(self.DP_(self.FC3(self.BN3(x))))\n                x = self.FC4(self.BN4(x))\n\n                return x","08f3b4c8":"def fit_(model=None, optimizer=None, scheduler=None, epochs=None, early_stopping_patience=None,\n         trainloader=None, validloader=None,\n         criterion=None, device=None,\n         save_to_file=False,\n         path=None, verbose=False):\n\n    breaker()\n    print(\"Training ...\")\n    breaker()\n\n    model.to(device)\n\n    DLS = {\"train\": trainloader, \"valid\": validloader}\n    bestLoss = {\"train\": np.inf, \"valid\": np.inf}\n    bestAccs = {\"train\": 0.0, \"valid\": 0.0}\n\n    Losses = []\n    Accuracies = []\n    \n    if save_to_file:\n        file = open(os.path.join(path, \"Metrics.txt\"), \"w\")\n\n    start_time = time()\n    for e in range(epochs):\n        e_st = time()\n\n        epochLoss = {\"train\": 0.0, \"valid\": 0.0}\n        epochAccs = {\"train\": 0.0, \"valid\": 0.0}\n\n        for phase in [\"train\", \"valid\"]:\n            if phase == \"train\":\n                model.train()\n            else:\n                model.eval()\n\n            lossPerPass = []\n            accsPerPass = []\n\n            for X, y in DLS[phase]:\n                X = X.to(device)\n                if y.dtype == torch.int64:\n                    y = y.to(device).view(-1)\n                else:\n                    y = y.to(device)\n\n                optimizer.zero_grad()\n                with torch.set_grad_enabled(phase == \"train\"):\n                    output = model(X)\n                    loss = criterion(output, y)\n                    if phase == \"train\":\n                        loss.backward()\n                        optimizer.step()\n                lossPerPass.append(loss.item())\n                accsPerPass.append(getAccuracy(output, y))\n            epochLoss[phase] = np.mean(np.array(lossPerPass))\n            epochAccs[phase] = np.mean(np.array(accsPerPass))\n        Losses.append(epochLoss)\n        Accuracies.append(epochAccs)\n\n        torch.save({\"model_state_dict\": model.state_dict(),\n                    \"optim_state_dict\": optimizer.state_dict()},\n                    os.path.join(path, \"Epoch_{}.pt\".format(e + 1)))\n\n        if early_stopping_patience:\n            if epochLoss[\"valid\"] < bestLoss[\"valid\"]:\n                bestLoss = epochLoss\n                bestLossEpoch = e + 1\n                torch.save({\"model_state_dict\": model.state_dict(),\n                            \"optim_state_dict\": optimizer.state_dict()},\n                            os.path.join(path, \"Epoch_{}.pt\".format(e + 1)))\n                early_stopping_step = 0\n            else:\n                early_stopping_step += 1\n                if early_stopping_step > early_stopping_patience:\n                    print(\"Early Stopping at Epoch {}\".format(e + 1))\n                    break\n\n        if epochLoss[\"valid\"] < bestLoss[\"valid\"]:\n            bestLoss = epochLoss\n            bestLossEpoch = e + 1\n\n        if epochAccs[\"valid\"] > bestAccs[\"valid\"]:\n            bestAccs = epochAccs\n            bestAccsEpoch = e + 1\n\n        if verbose:\n            print(\"Epoch: {} | Train Loss: {:.5f} | Valid Loss: {:.5f} | Train Accs : {:.5f} | \\\nValid Accs : {:.5f} | Time: {:.2f} seconds\".format(e + 1,\n                                                  epochLoss[\"train\"], epochLoss[\"valid\"],\n                                                  epochAccs[\"train\"], epochAccs[\"valid\"],\n                                                  time() - e_st))\n\n        if save_to_file:\n            text = \"Epoch: {} | Train Loss: {:.5f} | Valid Loss: {:.5f} | Train Accs : {:.5f} | \\\nValid Accs : {:.5f} | Time: {:.2f} seconds\\n\".format(e + 1,\n                                                     epochLoss[\"train\"], epochLoss[\"valid\"],\n                                                     epochAccs[\"train\"], epochAccs[\"valid\"],\n                                                     time() - e_st)\n            file.write(text)\n\n        if scheduler:\n            scheduler.step(epochLoss[\"valid\"])\n\n    breaker()\n    print(\"-----> Best Validation Loss at Epoch {}\".format(bestLossEpoch))\n    breaker()\n    print(\"-----> Best Validation Accs at Epoch {}\".format(bestAccsEpoch))\n    breaker()\n    print(\"Time Taken [{} Epochs] : {:.2f} minutes\".format(epochs, (time() - start_time) \/ 60))\n    breaker()\n    print(\"Training Complete\")\n    breaker()\n\n    if save_to_file:\n        text_1 = \"\\n-----> Best Validation Loss at Epoch {}\\n\".format(bestLossEpoch)\n        text_2 = \"-----> Best Validation Accs at Epoch {}\\n\".format(bestAccsEpoch)\n        text_3 = \"Time Taken [{} Epochs] : {:.2f} minutes\\n\".format(epochs, (time() - start_time) \/ 60)\n\n        file.write(text_1)\n        file.write(text_2)\n        file.write(text_3)\n\n    return Losses, Accuracies, bestLossEpoch, bestAccsEpoch\n\n\ndef predict_(model=None, dataloader=None, device=None, path=None):\n    if path:\n        model.load_state_dict(torch.load(path, map_location=device)[\"model_state_dict\"])\n    \n    model.eval()\n    \n    y_pred = torch.zeros(1, 1).to(device)\n    for X in dataloader:\n        X = X.to(cfg.device)\n        with torch.no_grad():\n            output = torch.sigmoid(model(X))\n        y_pred = torch.cat((y_pred, output), dim=0)\n    \n    return y_pred[1:].detach().cpu().numpy()","65f46142":"cfg = CFG(HL=[128], epochs=10, batch_size=512, tr_va_split=0.2)\n\nX_train, X_valid, y_train, y_valid = train_test_split(train_features, train_labels, test_size=cfg.tr_va_split, shuffle=True, random_state=seed)\n\ntr_data_setup = DS(X=X_train, y=y_train.reshape(-1, 1), mode=\"train\")\nva_data_setup = DS(X=X_valid, y=y_valid.reshape(-1, 1), mode=\"valid\")\ntr_data = DL(tr_data_setup, batch_size=cfg.tr_batch_size, shuffle=True, pin_memory=True, generator=torch.manual_seed(seed),)\nva_data = DL(va_data_setup, batch_size=cfg.va_batch_size, shuffle=False, pin_memory=True)\n\ntorch.manual_seed(seed)\nmodel = Classifier(IL=cfg.IL, HL=cfg.HL, OL=cfg.OL, use_DP=True, DP=0.5)\noptimizer = model.getOptimizer(lr=1e-3, wd=0)\n\nL, A, BLE, BAE = fit_(model=model, optimizer=optimizer, scheduler=None, epochs=cfg.epochs,\n                      trainloader=tr_data, validloader=va_data, device=cfg.device,\n                      criterion=nn.BCEWithLogitsLoss(),\n                      save_to_file=True,\n                      path=\".\/\", verbose=True)\n\nTL, VL, TA, VA = [], [], [], []\n\nfor i in range(len(L)):\n    TL.append(L[i][\"train\"])\n    VL.append(L[i][\"valid\"])\n    TA.append(A[i][\"train\"])\n    VA.append(A[i][\"valid\"])\n\nx_Axis = np.arange(len(L))\nplt.figure(figsize=(12, 6))\nplt.title(\"Metrics for M1\")\nplt.subplot(1, 2, 1)\nplt.plot(x_Axis, TL, \"r\", label=\"Training Loss\")\nplt.plot(x_Axis, VL, \"b--\", label=\"validation Loss\")\nplt.legend()\nplt.grid()\nplt.subplot(1, 2, 2)\nplt.plot(x_Axis, TA, \"r\", label=\"Training Accuracy\")\nplt.plot(x_Axis, VA, \"b--\", label=\"validation Accuracy\")\nplt.legend()\nplt.grid()\nplt.show()","46f53078":"ts_data_setup = DS(X=test_features, y=None, mode=\"test\")\nts_data = DL(ts_data_setup, batch_size=cfg.ts_batch_size, shuffle=False)\n\ny_pred = predict_(model=model, dataloader=ts_data, device=cfg.device, path=\".\/Epoch_{}.pt\".format(BLE))\n\ny_pred[y_pred > 0.5] = 1\ny_pred[y_pred <= 0.5] = 0\n\nprecision, recall, f1, _ = precision_recall_fscore_support(y_pred, test_labels)\n\nbreaker()\nprint(\"Accuracy  : {:.5f}\".format(accuracy_score(y_pred, test_labels)))\nprint(\"F1 Score  : {:.5f}\".format(f1[0]))\nprint(\"Precision : {:.5f}\".format(precision[0]))\nprint(\"Recall    : {:.5f}\".format(recall[0]))\nbreaker()","60177984":"**Config**","2b687129":"**Setup**","17bde8d6":"# Helpers","805ca313":"# ANN Config and Setup","b22ea6eb":"**Data Input**","16ad21b0":"# Training and Validation","fc303cce":"# Library Imports","752158fa":"# Predictions","45cf147f":"**ANN Helpers**","65158cff":"**Simple EDA**","4592acf1":"**Custom Pytorch Dataset Template**","91cc65b2":"**Preprocessing**","0ed05eb4":"# Data Handling and Analysis"}}