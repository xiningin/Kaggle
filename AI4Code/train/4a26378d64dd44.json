{"cell_type":{"948e61d3":"code","1f5e17b9":"code","f9763844":"code","7157b897":"code","82391d0d":"code","42e4a8dc":"code","195e49de":"code","2066ab3c":"code","aa66ebae":"code","031fe248":"code","0d38f4ed":"code","794ee776":"code","29eae95f":"code","75c013f3":"code","b9e12b2a":"code","bb67a136":"code","472737c2":"code","a2e0b8c4":"code","c967dc4a":"code","a3b4bded":"code","f2e66d02":"code","d283dc0d":"code","af3b998c":"code","576791d7":"code","583cd717":"code","d0272eee":"code","717a4f93":"code","19aa6c6c":"code","b5f96e6a":"code","f611ee27":"code","413a4a41":"code","cca6debe":"code","153e9778":"code","2f0a1ccc":"code","746121b8":"code","c4910c85":"code","4bd25bcb":"code","4b8c13fa":"code","84b901a5":"code","de6134dd":"code","e25503e5":"code","7024e51e":"code","2fc1d90f":"code","ec149e48":"code","4963b1b7":"code","250b7b55":"code","62f8c228":"code","5b4c2be8":"code","e67a7ac6":"code","610a2948":"code","4c53df89":"code","a68a73d3":"code","df18d975":"code","3f54d27d":"code","106a5654":"code","4b89f740":"code","0ca76df2":"markdown","c2a41d61":"markdown","8f354eef":"markdown","f0bc6745":"markdown","d1c30498":"markdown","ebb2bb72":"markdown","f9c9e694":"markdown","3db7d118":"markdown","9d165fc5":"markdown","97689690":"markdown","d9d0013b":"markdown","9aa52726":"markdown","9ccc34df":"markdown","f3b5252f":"markdown","40028e6b":"markdown","97daafa3":"markdown","4f1037af":"markdown","f4b452f3":"markdown","35f3d2f2":"markdown","0ccebbdc":"markdown","b334e265":"markdown","10c006a6":"markdown","04cc9b70":"markdown","f45f7ee8":"markdown","e55585c0":"markdown","83a07152":"markdown","6ab1f51e":"markdown","d30bb298":"markdown","94e021b5":"markdown","6435c62b":"markdown","2aecae61":"markdown","11a243db":"markdown","97601f4e":"markdown","444209a5":"markdown","0be2800e":"markdown","f37b727b":"markdown","01c5190f":"markdown","4b566cab":"markdown"},"source":{"948e61d3":"#Data setup\nimport pandas as pd\n\ndf = pd.read_csv('iris.csv')\ndf.head()","1f5e17b9":"from sklearn.preprocessing import Imputer\nimputer = Imputer(missing_values='NaN', strategy='median', axis=0)\nimputer = imputer.fit(df.iloc[:,:-1])\nimputed_data = imputer.transform(df.iloc[:,:-1].values)\ndf.iloc[:,:-1] = imputed_data\n\niris = df","f9763844":"iris.iloc[:,5].unique()","7157b897":"iris.head()","82391d0d":"from sklearn.preprocessing import LabelEncoder\nclass_label_encoder = LabelEncoder()\n\niris.iloc[:,-1] = class_label_encoder.fit_transform(iris.iloc[:,-1])","42e4a8dc":"iris.head()","195e49de":"iris.corr()","2066ab3c":"iris.var()","aa66ebae":"splt = pd.plotting.scatter_matrix(iris, c=iris.iloc[:,-1], figsize=(20, 20), marker='o')","031fe248":"import numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# Transform data into features and target\nX = np.array(iris.ix[:, 1:5]) \ny = np.array(iris['Species'])\n\n# split into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)\n\n","0d38f4ed":"print(X_train.shape)\nprint(y_train.shape)","794ee776":"print(X_test.shape)\nprint(y_test.shape)","29eae95f":"# loading library\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\n# instantiate learning model (k = 3)\nknn = KNeighborsClassifier(n_neighbors = 3)\n\n# fitting the model\nknn.fit(X_train, y_train)\n\n# predict the response\ny_pred = knn.predict(X_test)\n\n# evaluate accuracy\nprint(accuracy_score(y_test, y_pred))\n\n# instantiate learning model (k = 5)\nknn = KNeighborsClassifier(n_neighbors=5)\n\n# fitting the model\nknn.fit(X_train, y_train)\n\n# predict the response\ny_pred = knn.predict(X_test)\n\n# evaluate accuracy\nprint(accuracy_score(y_test, y_pred))\n\n# instantiate learning model (k = 9)\nknn = KNeighborsClassifier(n_neighbors=9)\n\n# fitting the model\nknn.fit(X_train, y_train)\n\n# predict the response\ny_pred = knn.predict(X_test)\n\n# evaluate accuracy\nprint(accuracy_score(y_test, y_pred))","75c013f3":"# creating odd list of K for KNN\nmyList = list(range(1,25))\n\n# subsetting just the odd ones\nneighbors = list(filter(lambda x: x % 2 != 0, myList))\n","b9e12b2a":"\n# empty list that will hold accuracy scores\nac_scores = []\n\n# perform accuracy metrics for values from 1,3,5....19\nfor i,k in enumerate(neighbors):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    # predict the response\n    y_pred = knn.predict(X_test)\n    # evaluate accuracy\n    scores = accuracy_score(y_test, y_pred)\n    ac_scores.append(scores)\n\n# changing to misclassification error\nMSE = [1 - x for x in ac_scores]\n\n# determining best k\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors is %d\" % optimal_k)","bb67a136":"import matplotlib.pyplot as plt\n# plot misclassification error vs k\nplt.plot(neighbors, MSE)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.show()","472737c2":"#Load all required library\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nfrom sklearn import datasets\nfrom sklearn.decomposition import PCA\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB","a2e0b8c4":"# Load using input file\niris=pd.read_csv(\"iris.csv\")\niris.head(5)","c967dc4a":"# Check dimension of data\niris.shape","a3b4bded":"#Check shape of data\niris.info()","f2e66d02":"# check for missing values","d283dc0d":"iris.isna().sum()","af3b998c":"iris = iris.dropna()","576791d7":"iris.isna().sum()","583cd717":"X=iris.iloc[:,:4].values\ny=iris['Species'].values","d0272eee":"#Check the dataset\nprint(y)\nprint(X)","717a4f93":"iris[\"Species\"].value_counts()","19aa6c6c":"pd.value_counts(iris[\"Species\"]).plot(kind=\"bar\")","b5f96e6a":"spd = pd.plotting.scatter_matrix(iris, figsize=(20,20), diagonal=\"kde\")","f611ee27":"corr = iris.corr()\ncorr\n#Please note, it's Require to remove correlated features because they are voted twice in the model and it can lead to over inflating importance.We will ignore it here","413a4a41":"iris","cca6debe":"from sklearn.datasets import load_iris\niris = load_iris()\n\nfrom matplotlib import pyplot as plt\n\n# The indices of the features that we are plotting\nx_index = 0\ny_index = 1\n\n# this formatter will label the colorbar with the correct target names\nformatter = plt.FuncFormatter(lambda i, *args: iris.target_names[int(i)])\n\nplt.figure(figsize=(8, 8))\nplt.scatter(iris.data[:, x_index], iris.data[:, y_index], c=iris.target)\nplt.colorbar(ticks=[0, 1, 2], format=formatter)\nplt.xlabel(iris.feature_names[x_index])\nplt.ylabel(iris.feature_names[y_index])\n\nplt.tight_layout()\nplt.show()","153e9778":"### SPLITTING INTO TRAINING AND TEST SETS\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20,random_state=22)","2f0a1ccc":"### NORMALIZTION \/ FEATURE SCALING\nfrom sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","746121b8":"### WE WILL FIT THE THE CLASSIFIER TO THE TRAINING SET\nnaiveClassifier=GaussianNB()\nnaiveClassifier.fit(X_train,y_train)","c4910c85":"y_pred = naiveClassifier.predict(X_test)","4bd25bcb":"#Keeping the actual and predicted value side by side\ny_compare = np.vstack((y_test,y_pred)).T\n#Actual->LEFT\n#predicted->RIGHT\n#Number of values to be print\ny_compare[:20,:]","4b8c13fa":"# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)","84b901a5":"#finding accuracy from the confusion matrix.\na = cm.shape\ncorrectPrediction = 0\nfalsePrediction = 0\n\nfor row in range(a[0]):\n    for c in range(a[1]):\n        if row == c:\n            correctPrediction +=cm[row,c]\n        else:\n            falsePrediction += cm[row,c]\nprint('Correct predictions: ', correctPrediction)\nprint('False predictions', falsePrediction)\nprint ('\\n\\nAccuracy of the Naive Bayes Clasification is: ', correctPrediction\/(cm.sum()))","de6134dd":"from sklearn import metrics\nprint(metrics.classification_report(y_pred, y_test))","e25503e5":"#Import library\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","7024e51e":"diabetes = pd.read_csv('pima-indians-diabetes.csv')\nprint(diabetes.columns)","2fc1d90f":"# Eye ball the imported dataset\ndiabetes.head()","ec149e48":"print(\"dimension of diabetes data: {}\".format(diabetes.shape))\n#The diabetes dataset consists of 768 data points, with 9 features","4963b1b7":"print(diabetes.groupby('class').size())","250b7b55":"import seaborn as sns\n\nsns.countplot(diabetes['class'],label=\"Count\")","62f8c228":"diabetes.info()","5b4c2be8":"colormap = plt.cm.viridis # Color range to be used in heatmap\nplt.figure(figsize=(15,15))\nplt.title('Pearson Correlation of attributes', y=1.05, size=19)\nsns.heatmap(diabetes.corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)\n#There is no strong correlation between any two variables.\n#There is no strong correlation between any independent variable and class variable.","e67a7ac6":"spd = pd.plotting.scatter_matrix(diabetes, figsize=(20,20), diagonal=\"kde\")","610a2948":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(diabetes.loc[:, diabetes.columns != 'class'], diabetes['class'], stratify=diabetes['class'], random_state=11)","4c53df89":"X_train.shape","a68a73d3":"from sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(X_train, y_train)\n\nprint(\"Accuracy on training set: {:.2f}\".format(svc.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.2f}\".format(svc.score(X_test, y_test)))","df18d975":"#The model overfits substantially with a perfect score on the training set and only 65% accuracy on the test set.\n\n#SVM requires all the features to be on a similar scale. We will need to rescale our data that all the features are approximately on the same scale and than see the performance","3f54d27d":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.fit_transform(X_test)","106a5654":"svc = SVC()\nsvc.fit(X_train_scaled, y_train)\n\nprint(\"Accuracy on training set: {:.2f}\".format(svc.score(X_train_scaled, y_train)))\nprint(\"Accuracy on test set: {:.2f}\".format(svc.score(X_test_scaled, y_test)))","4b89f740":"svc = SVC(C=1000)\nsvc.fit(X_train_scaled, y_train)\n\nprint(\"Accuracy on training set: {:.3f}\".format(\n    svc.score(X_train_scaled, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(svc.score(X_test_scaled, y_test)))","0ca76df2":"## Problem statement\n\n### Dataset\n\nThe data set we\u2019ll be using is the Iris Flower Dataset which was first introduced in 1936 by the famous statistician Ronald Fisher and consists of 50 observations from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals.\n\n**Source:** https:\/\/archive.ics.uci.edu\/ml\/datasets\/Iris\n\n**Train the KNN algorithm to be able to distinguish the species from one another given the measurements of the 4 features.**","c2a41d61":"This dataset describes the medical records for Pima Indians and whether or not each patient will have an onset of diabetes within \fve years.\n\nFields description follow:\n\npreg = Number of times pregnant\n\nplas = Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n\npres = Diastolic blood pressure (mm Hg)\n\nskin = Triceps skin fold thickness (mm)\n\ntest = 2-Hour serum insulin (mu U\/ml)\n\nmass = Body mass index (weight in kg\/(height in m)^2)\n\npedi = Diabetes pedigree function\n\nage = Age (years)\n\nclass = Class variable (1:tested positive for diabetes, 0: tested negative for diabetes)","8f354eef":"## Question 1\n\n#### Read the iris.csv file","f0bc6745":"## Question 2 - Estimating missing values\n\n*Its not good to remove the records having missing values all the time. We may end up loosing some data points. So, we will have to see how to replace those missing values with some estimated values (median) *","d1c30498":"### Question 7\n#### Scale the data points using MinMaxScaler","ebb2bb72":"## Question 10\n\n*Plot misclassification error vs k (with k value on X-axis) using matplotlib.*","f9c9e694":"### Question 2\n#### Check the dimensions of dataset","3db7d118":"### Question 8\n#### Fit SVM Model on scaled data and give your observation","9d165fc5":"### Question 3\n#### Find Correlation among all variables and give your insights ","97689690":"### Question 4\n#### Split data in Training and Validation in 80:20","d9d0013b":"### Plot Scatter Matrix to understand the distribution of variables and give insights from it( 1 Marks)","9aa52726":"### Question 7\n#### Print Accuracy and Confusion Matrix and Conclude your findings","9ccc34df":"## Question 9 - Cross Validation\n\nRun the KNN with no of neighbours to be 1,3,5..19 and *Find the **optimal number of neighbours** from the above list using the Mis classification error","f3b5252f":"### Question 6\n#### Train Support Vector Machine Model","40028e6b":"# Naive Bayes","97daafa3":"# SVM","4f1037af":"## Question 4\n\n*Observe the association of each independent variable with target variable and drop variables from feature set having correlation in range -0.1 to 0.1 with target variable.*","f4b452f3":"### Question 9\n#### Try improving the model accuracy using C=1000","35f3d2f2":"## Question 5\n\n*Observe the independent variables variance and drop such variables having no variance or almost zero variance(variance < 0.1). They will be having almost no influence on the classification.*","0ccebbdc":"### Question 5\n#### Do train and test split with stratify sampling on Outcome variable to maintain the distribution of dependent variable","b334e265":"### Question 3\n#### Check distribution of dependent variable 'class' and plot it","10c006a6":"## Question 8 - Model\n\n*Build the model and train and test on training and test sets respectively using **scikit-learn**. Print the Accuracy of the model with different values of **k=3,5,9**.*\n\n**Hint:** For accuracy you can check **accuracy_score()** in scikit-learn","04cc9b70":"### Plot the distribution of target variable using histogram","f45f7ee8":"## Split the dataset into training and test sets\n\n## Question 7\n\n*Split the dataset into training and test sets with 80-20 ratio.*","e55585c0":"## Question 3 - Dealing with categorical data\n\nChange all the classes to numericals (0to2).","83a07152":"## Data Pre-processing","6ab1f51e":"### Question 4\n#### Do correlation analysis and bivariate viualization with Insights","d30bb298":"### Question 5\n#### Do Feature Scaling ","94e021b5":"KNN falls in the supervised learning family of algorithms. Informally, this means that we are given a labelled dataset consiting of training observations (x,y) and would like to capture the relationship between x and y. More formally, our goal is to learn a function h:X\u2192Y so that given an unseen observation x, h(x) can confidently predict the corresponding output y.\n\nIn this module we will explore the inner workings of KNN, choosing the optimal K values and using KNN from scikit-learn.","6435c62b":"## Question 3\n#### Find the distribution of target variable (Class)\n#### And, Plot the distribution of target variable using histogram","2aecae61":"### Question 2\n#### Slice data set for Independent variables and dependent variables\n#### Please note 'Species' is my dependent variables, name it y and independent set data as X","11a243db":"### Question 6 \n#### Train and Fit NaiveBayes Model","97601f4e":"Hint:\n\nMisclassification error (MSE) = 1 - Test accuracy score. Calculated MSE for each model with neighbours = 1,3,5...19 and find the model with lowest MSE","444209a5":"## K-Nearest-Neighbors","0be2800e":"### Out of  768 data points, 500 are labeled as 0 and 268 as 1.\n### Class 0 means No diabetes, outcome 1 means diabetes","f37b727b":"### Question 1\n#### Import Iris.csv","01c5190f":"### Question 1\n#### Read the input file 'Diabetes.csv' using Pandas and check it's column names(1 Marks)","4b566cab":"## Question 6\n\n*Plot the scatter matrix for all the variables.*"}}