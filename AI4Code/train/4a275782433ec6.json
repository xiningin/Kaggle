{"cell_type":{"7328784f":"code","2c984cfb":"code","5c2f6ff1":"code","e8f6f0af":"code","9f794dcb":"code","3a71ed1c":"code","616e466f":"code","77490fe5":"code","c2bf6c9a":"code","05e0528b":"code","5b43c5b6":"code","e224fb65":"code","971dda28":"code","a5cf4ac8":"code","fd58f965":"code","3ecc9289":"code","8a3210cb":"code","e200fc2e":"code","3947b9cf":"code","44df2564":"code","0d812825":"code","f16e1d4f":"code","4cfc62d1":"code","82e88ee7":"code","34d84849":"code","eb4d0f98":"code","c4760af4":"code","69631fa8":"code","7f49fd8c":"code","2acf6521":"code","4d757dde":"code","45df7593":"code","bf345049":"code","89d2106d":"code","5c27ed82":"code","dc91eca5":"code","11517dd8":"code","63b50fd6":"code","6782d039":"code","d79792ef":"code","bca088b2":"code","8afcf96d":"markdown","097e36e6":"markdown","720f5f2c":"markdown","c1a92c16":"markdown"},"source":{"7328784f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport time\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2c984cfb":"#1.2 Keras libraries\nfrom keras.layers import Input, Dense\nfrom keras.layers import Conv1D, MaxPooling1D, Dropout, Flatten,Embedding, GRU\nfrom keras.layers.merge import concatenate\nfrom keras.models import Model\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","5c2f6ff1":"from keras.utils import  plot_model\n\n# 1.4 sklearn libraries\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\n# 1.4 For model plotting\nimport matplotlib.pyplot as plt\nimport pydot\nfrom skimage import io\n","e8f6f0af":"%%time\nMIMIC_PATH = \"..\/input\/mimic3d\"\nprint (os.path.join(MIMIC_PATH,'mimic3d.csv'))\n#mimic_df = pd.read_csv(\"\/kaggle\/input\/mimic3d\/mimic3d.csv\")\nmimic_df = pd.read_csv(os.path.join(MIMIC_PATH,'mimic3d.csv'))","9f794dcb":"mimic_df.shape","3a71ed1c":"mimic_df.head()","616e466f":"mimic_df.describe()","77490fe5":"def plot_features_distribution(features, title,isLog=False):\n    plt.figure(figsize=(12,6))\n    plt.title(title)\n    for feature in features:\n        if(isLog):\n            sns.distplot(np.log1p(mimic_df[feature]),kde=True,hist=False, bins=120, label=feature)\n        else:\n            sns.distplot(mimic_df[feature],kde=True,hist=False, bins=120, label=feature)\n    plt.xlabel('')\n    plt.legend()\n    plt.show()","c2bf6c9a":"def plot_count(feature, title,size=1,df=mimic_df):\n    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n    total = float(len(df))\n    g = sns.countplot(df[feature], order = df[feature].value_counts().index[:30], palette='Set3')\n    g.set_title(\"Number and percentage of {}\".format(title))\n    if(size > 2):\n        plt.xticks(rotation=90, size=8)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height\/total),\n                ha=\"center\") \n    plt.show()","05e0528b":"plot_count('gender','gender')","5b43c5b6":"plot_features_distribution(['age'],'Patient age distribution')","e224fb65":"plot_count('admit_type','admission type',2)","971dda28":"plot_count('insurance','patient insurance',2)","a5cf4ac8":"mimic_df.head()","fd58f965":"mimic_df.tail()","3ecc9289":"mimic_df.columns.values","8a3210cb":"#Any missing value?\nmimic_df.isnull().values.sum() ","e200fc2e":"mimic_df.columns[mimic_df.isnull().sum()  > 0] ","3947b9cf":"mimic_df.AdmitDiagnosis = mimic_df.AdmitDiagnosis.fillna(\"missing\")\nmimic_df.religion = mimic_df.religion.fillna(\"missing\")\nmimic_df.marital_status = mimic_df.marital_status.fillna(\"missing\")\n#Want to make sure is there any missing values?\nmimic_df.isnull().values.sum()  ","44df2564":"dtrain,  dtest = train_test_split(mimic_df, test_size=0.25)","0d812825":"#Which columns are 'object'\nobj_columns = mimic_df.select_dtypes(include = ['object']).columns.values\nobj_columns","f16e1d4f":"#Which columns have numeric data\nnum = mimic_df.select_dtypes(include = ['int64', 'float64']).columns.values\nnum","4cfc62d1":"#Final seven obj_columns for One Hot Encoding\nobj_cols = [\"gender\", \"admit_type\", \"admit_location\", \"insurance\" ,\"marital_status\", 'ExpiredHospital', 'LOSgroupNum']\nohe = OneHotEncoder()\n# Traing on dtrain\nohe = ohe.fit(dtrain[obj_cols])\n# Transform train (dtrain) and test (dtest) data\ndtrain_ohe = ohe.transform(dtrain[obj_cols])\ndtest_ohe = ohe.transform(dtest[obj_cols])\n\ndtrain_ohe.shape       \ndtest_ohe.shape ","82e88ee7":"#  Label encode relegion and ethnicity\n#  First 'religion'\nle = LabelEncoder()\nle.fit(dtrain[\"religion\"])\ndtrain[\"re\"] = le.transform(dtrain['religion'])    # Create new column in dtrain\ndtest[\"re\"] = le.transform(dtest['religion'])      #   and in dtest","34d84849":"# Now 'ethnicity'\nle = LabelEncoder()\nle.fit(dtrain[\"ethnicity\"])\ndtrain[\"eth\"]= le.transform(dtrain['ethnicity'])   # Create new column in dtrain\ndtest[\"eth\"]= le.transform(dtest['ethnicity'])     #   and in dtest","eb4d0f98":"# Finally transform two obj_columns for tokenization\nte_ad = Tokenizer()\n#Train tokenizer on train data ie 'dtrain'\nte_ad.fit_on_texts(mimic_df.AdmitDiagnosis.values)\n#Transform both dtrain and dtest and create new columns\ndtrain[\"ad\"] = te_ad.texts_to_sequences(dtrain.AdmitDiagnosis)\ndtest[\"ad\"] = te_ad.texts_to_sequences(dtest.AdmitDiagnosis)\n\ndtrain.head(3)\ndtest.head(3)","c4760af4":"# Similarly for column: AdmitProcedure\nte_ap = Tokenizer(oov_token='<unk>')\nte_ap.fit_on_texts(mimic_df.AdmitProcedure.values)\ndtrain[\"ap\"] = te_ap.texts_to_sequences(dtrain.AdmitProcedure)\ndtest[\"ap\"] = te_ap.texts_to_sequences(dtest.AdmitProcedure)\n\ndtrain.head(3)\ndtest.head(3)","69631fa8":"# Standardize numerical data\nse = StandardScaler()\n# Train om dtrain\nse.fit(dtrain.loc[:,num])\n# Then transform both dtrain and dtest\ndtrain[num] = se.transform(dtrain[num])\ndtest[num] = se.transform(dtest[num])\ndtest.loc[:,num].head(3)","7f49fd8c":"#  Get max length of the sequences\n#    in dtrain[\"ad\"], dtest[\"ad\"]\nmaxlen_ad = 0\nfor i in dtrain[\"ad\"]:\n    if maxlen_ad < len(i):\n        maxlen_ad = len(i)\n\nfor i in dtest[\"ad\"]:\n    if maxlen_ad < len(i):\n        maxlen_ad = len(i)\n\nmaxlen_ad ","2acf6521":"#  Get max length of the sequences\n#    in dtrain[\"ap\"], dtest[\"ap\"]\n\nmaxlen_ap = 0\nfor i in dtrain[\"ap\"]:\n    if maxlen_ap < len(i):\n        maxlen_ap = len(i)\n\nmaxlen_ap      \n\nfor i in dtest[\"ap\"]:\n    if maxlen_ap < len(i):\n        maxlen_ap = len(i)\n\nmaxlen_ap  ","4d757dde":"#  Get max vocabulary size ie value of highest\n#    integer in dtrain[\"ad\"] and in dtest[\"ad\"]\n\none = np.max([np.max(i) for i in dtrain[\"ad\"].tolist() ])\ntwo = np.max([np.max(i) for i in dtest[\"ad\"].tolist() ])\nMAX_VOCAB_AD = np.max([one,two])","45df7593":"# Get max vocabulary size ie value of highest\n#     integer in dtrain[\"ap\"] and in dtest[\"ap\"]\none = np.max([np.max(i) for i in dtrain[\"ap\"].tolist() ])\ntwo = np.max([np.max(i) for i in dtest[\"ap\"].tolist() ])\nMAX_VOCAB_AP = np.max([one,two])","bf345049":"MAX_VOCAB_RE = len(dtrain.religion.value_counts())\nMAX_VOCAB_ETH = len(dtrain.ethnicity.value_counts())","89d2106d":"#  Let us put our data in a dictionary form\n#     Required when we have multiple inputs\n#     to Deep Neural network. Each Input layer\n#     should also have the corresponding 'key'\n#     name\n\n#  Training data\nXtr = {\n    \"num\" : dtrain[num].values,          # Note the name 'num'\n    \"ohe\" : dtrain_ohe.toarray(),        # Note the name 'ohe'\n    \"re\"  : dtrain[\"re\"].values,\n    \"eth\" : dtrain[\"eth\"].values,\n    \"ad\"  : pad_sequences(dtrain.ad, maxlen=maxlen_ad),\n    \"ap\"  : pad_sequences(dtrain.ap, maxlen=maxlen_ap )\n      }","5c27ed82":"# Test data\nXte = {\n    \"num\" : dtest[num].values,\n    \"ohe\" : dtest_ohe.toarray(),\n    \"re\"  : dtest[\"re\"].values,\n    \"eth\" : dtest[\"eth\"].values,\n    \"ad\"  : pad_sequences(dtest.ad, maxlen=maxlen_ad ),\n    \"ap\"  : pad_sequences(dtest.ap, maxlen=maxlen_ap )\n      }","dc91eca5":"#Design a simple model now\n\ndr_level = 0.1\n\nnum = Input(\n                      shape= (Xtr[\"num\"].shape[1], ),\n                      name = \"num\"            # Name 'num' should be a key in the dictionary for numpy array input\n                                              #    That is, this name should be the same as that of key in the dictionary\n                      )\n\n\nohe =   Input(\n                      shape= (Xtr[\"ohe\"].shape[1], ),\n                      name = \"ohe\"\n                      )\n\n\nre =   Input(\n                      shape= [1],  # 1D shape or one feature\n                      name = \"re\"\n                      )\n\neth =   Input(\n                      shape= [1],  # 1D shape or one feature\n                      name = \"eth\"\n                      )\n\nad =   Input(\n                      shape= (Xtr[\"ad\"].shape[1], ),\n                      name = \"ad\"\n                      )\n\nap =   Input(\n                      shape= (Xtr[\"ap\"].shape[1],),\n                      name = \"ap\"\n                      )","11517dd8":"#  Embedding layers for each of the two of the columns with sequence data\n#     Why add 1 to vocabulary?\n#     See: https:\/\/stackoverflow.com\/questions\/52968865\/invalidargumenterror-indices127-7-43-is-not-in-0-43-in-keras-r\n\nemb_ad  =      Embedding(MAX_VOCAB_AD+ 1 ,      32  )(ad )\nemb_ap  =      Embedding(MAX_VOCAB_AP+ 1 ,      32  )(ap)\n# Embedding layers for the two categorical variables\nemb_re  =      Embedding(MAX_VOCAB_RE+ 1 ,      32  )(re)\nemb_eth =      Embedding(MAX_VOCAB_ETH+ 1 ,      32  )(eth)\n\n#  RNN layers for sequences\nrnn_ad = GRU(16) (emb_ad)          # Output of GRU is a vector of size 8\nrnn_ap = GRU(16) (emb_ap)","63b50fd6":"rnn_re = GRU(16) (emb_re) \nrnn_eth = GRU(16) (emb_eth)","6782d039":"#Interim model summary.\n#      For 'output' we have all the existing (unterminated) outputs\nmodel = Model([num, ohe, re, eth, ad,ap], [rnn_ad, rnn_ap, emb_re, emb_eth, num, ohe])\nmodel.summary()","d79792ef":"#  Concatenate all outputs\nclass_l = concatenate([\n                      rnn_ad,        # GRU output is already 1D\n                      rnn_ap,\n                      \n                      num,                # 1D output. No need to flatten. See model summary\n                      ohe,           # 1D output\n                      Flatten()(emb_re),   # Why flatten? See model summary above\n                      Flatten()(emb_eth)\n                      ]\n                     )\n\n\n#  Add classification layer\nclass_l = Dense(64) (class_l)\nclass_l = Dropout(0.1)(class_l)\nclass_l = Dense(32) (class_l)\nclass_l = Dropout(0.1) (class_l)\n\n#  Output neuron. Activation is linear\n#      as our output is continous\noutput = Dense(1, activation=\"linear\") (class_l)\n\n#  Formulate Model now\nmodel = Model(\n              inputs= [num, ohe, re, eth, ad, ap],\n              outputs= output\n             )\n\nmodel.summary()\n\n#  Model plot uisng keras plot_model()\nplt.figure(figsize = (14,14))\nplot_model(model, to_file = \"model.png\")\nio.imshow(\"model.png\")","bca088b2":"#Compile model\nmodel.compile(loss=\"mse\",\n              optimizer=\"adam\",\n              metrics=[\"mae\"]\n              )\n\nBATCH_SIZE = 5000\nepochs = 20\n\n######\n\nstart = time.time()\nhistory= model.fit(Xtr,\n                   dtrain.LOSdays,\n                   epochs=epochs,\n                   batch_size=BATCH_SIZE,\n                   validation_data=(Xte, dtest.LOSdays),\n                   verbose = 1\n                  )\nend = time.time()\nprint((end-start)\/60)","8afcf96d":"> **Load data**","097e36e6":"**The original data**\n\nMIMIC-III (Medical Information Mart for Intensive Care III) is a large, freely-available database comprising deidentified health-related data associated with over forty thousand patients who stayed in critical care units of the Beth Israel Deaconess Medical Center between 2001 and 2012. \n\n**This dataset**\n\nThis dataset contains admission data file from MIMIC dataset. It was added by @drscarlat.","720f5f2c":"Adding addition RNN layers\u00b6","c1a92c16":"> **Data exploration**"}}