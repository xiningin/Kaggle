{"cell_type":{"6c6d7216":"code","b28a11e1":"code","6e298c60":"code","68a6ca93":"code","17abab2b":"code","38131ece":"code","753b741e":"markdown","21acf916":"markdown","9cc87f72":"markdown","b307b147":"markdown","e1a104ef":"markdown"},"source":{"6c6d7216":"%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","b28a11e1":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nbatch_size=32\nimg_height=224\nimg_width=224\n\ndata_gen_args_train = dict(\n        rescale=1.\/255,\n        rotation_range=30,\n        zoom_range=0.3,\n        shear_range=0.3,\n        width_shift_range=0.3, \n        height_shift_range=0.3,\n        horizontal_flip=True,\n        vertical_flip=False,\n    )\n\ndata_gen_args_base = dict(\n        rescale=1.\/255\n    )\n\n\ntrain_dir = '\/kaggle\/input\/covid-face-mask-detection-dataset\/New Masks Dataset\/Train'\nval_dir = '\/kaggle\/input\/covid-face-mask-detection-dataset\/New Masks Dataset\/Validation'\ntest_dir = '\/kaggle\/input\/covid-face-mask-detection-dataset\/New Masks Dataset\/Test'\n\n\ntrain_image_generator = ImageDataGenerator(**data_gen_args_base) # Generator for our training data\ntrain_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n                                                           directory=train_dir,\n                                                           shuffle=True,\n                                                           target_size=(img_height, img_width),\n                                                           class_mode='binary')\n\nval_image_generator = ImageDataGenerator(**data_gen_args_base) # Generator for our training data\nval_data_gen = val_image_generator.flow_from_directory(batch_size=batch_size,\n                                                           directory=test_dir,\n                                                           shuffle=True,\n                                                           target_size=(img_height, img_width),\n                                                           class_mode='binary')\n\ntest_image_generator = ImageDataGenerator(**data_gen_args_base) # Generator for our training data\ntest_data_gen = test_image_generator.flow_from_directory(batch_size=batch_size,\n                                                           directory=test_dir,\n                                                           shuffle=True,\n                                                           target_size=(img_height, img_width),\n                                                           class_mode='binary')","6e298c60":"# This function will plot images in the form of a grid with 1 row and 5 columns where images are placed in each column.\ndef plotImages(images_arr):\n    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n    axes = axes.flatten()\n    for img, ax in zip( images_arr, axes):\n        ax.imshow(img)\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()","68a6ca93":"sample_training_images, _ = next(train_data_gen)\nplotImages(sample_training_images[:5])","17abab2b":"class Tuner(object):\n\n    def __init__(self, data, architecture, hidden_layers, classes, epochs, batch_size):\n        self.input_shape = (224, 224, 3)\n\n        self.base_arch = architecture\n        self.nn = self.download_network()\n        self.nn.trainable = False\n\n        self.hidden_layers = hidden_layers\n        \n        self.classes = classes\n\n        self.train_ds = data[0]\n        self.val_ds   = data[1]\n        self.test_ds  = data[2]\n\n        self.EPOCHS = epochs\n        self.BATCH_SIZE = batch_size\n\n        self.model = self.build()\n        self.predictions = None\n        self.score = None\n\n        self.best_weights = None\n\n    def download_network(self):\n        '''\n        Download the requested CNN with imagenet weights\n        '''\n        nn = None\n\n        if self.base_arch == 'VGG16':\n            nn = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=self.input_shape)\n        elif self.base_arch == 'VGG19':\n            nn = tf.keras.applications.VGG19(weights='imagenet', include_top=False, input_shape=self.input_shape)\n        elif self.base_arch == 'InceptionV3':\n            nn = tf.keras.applications.InceptionV3(weights='imagenet', include_top=False, input_shape=self.input_shape)\n        elif self.base_arch == 'Xception':\n            nn = tf.keras.applications.Xception(weights='imagenet', include_top=False, input_shape=self.input_shape)\n        elif self.base_arch == 'DenseNet121':\n            nn = tf.keras.applications.DenseNet121(weights='imagenet', include_top=False, input_shape=self.input_shape)\n        elif self.base_arch == 'DenseNet201':\n            nn = tf.keras.applications.DenseNet201(weights='imagenet', include_top=False, input_shape=self.input_shape)\n        elif self.base_arch == 'ResNet152V2':\n            nn = tf.keras.applications.ResNet152V2(weights='imagenet', include_top=False, input_shape=self.input_shape)\n        elif self.base_arch == 'MobileNet':\n            nn = tf.keras.applications.MobileNet(weights='imagenet', include_top=False, input_shape=self.input_shape)\n        elif self.base_arch == 'MobileNetV2':\n            nn = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=self.input_shape)\n\n        return nn\n    \n    def run(self):\n        '''\n        Main driver for Learner object\n        '''\n        self.fine_tune()\n\n\n    def build(self):\n        '''\n        Build model. Add Dense layer to topless base CNN.\n        '''\n\n        model = tf.keras.models.Sequential()\n        model.add(self.nn)\n        model.add(tf.keras.layers.Flatten())\n        model.add(tf.keras.layers.Dropout(0.25))\n        \n        for layer in self.hidden_layers:\n            model.add(tf.keras.layers.Dense(layer, activation='relu'))\n            model.add(tf.keras.layers.BatchNormalization())\n            model.add(tf.keras.layers.Dropout(0.4))  \n\n        model.add(tf.keras.layers.Dense(self.classes, activation='softmax'))\n        \n        print (model.summary())\n\n        return model\n    \n    def load_weights(self, name):\n        '''\n        Load the best checkpointed weights.\n        '''\n        print('\\nLoading best accuracy weights.')\n        self.model.load_weights(name)\n\n             \n    def fine_tune(self):\n        '''\n        Fine-tune network in 2 phases\n        '''\n\n        print (\"\\nPhase A - Training Fully Connected Layers\\n\")\n        self.model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=0.001), metrics=['accuracy'])\n\n        # Define checkpoint to save best Phase 1 weights\n        best_weights_ph1 = self.base_arch + \"_ph1_weights.hdf5\"\n        checkpoint = tf.keras.callbacks.ModelCheckpoint(best_weights_ph1, monitor=\"val_loss\", mode=\"min\", save_best_only=True, verbose=1)\n        \n        history = self.model.fit(\n            self.train_ds,\n            epochs=self.EPOCHS,\n            validation_data=self.val_ds,\n            callbacks=[checkpoint])\n\n        print('\\nRestoring best weights and predicting validation set.')\n        self.load_weights(best_weights_ph1)\n\n        # Make predictions based on best phase 1 weights\n        self.predict()\n\n        # Plot only meaningful, when more than 1 epochs\n        if self.EPOCHS > 1:\n            self.plot_loss(history, self.EPOCHS, '\\n Transfer Learning: ' + self.base_arch + ' Ph A')\n        \n    def predict(self):\n        '''\n        Get predictions and score for validation set.\n        '''\n        print('\\nPredicting test set classes.')\n        self.score = self.model.evaluate(self.test_ds, verbose=0)\n        print('Test set score:', self.score)\n        self.predictions = self.model.predict(self.test_ds)\n        print('Done')\n\n    def plot_loss(self, history, epochs, name):\n        print('\\n\\n')\n        plt.figure(figsize=(12,8))\n        plt.plot(np.arange(0, epochs), history.history[\"loss\"], label=\"train_loss\")\n        plt.plot(np.arange(0, epochs), history.history[\"val_loss\"], label=\"val_loss\")\n        plt.plot(np.arange(0, epochs), history.history[\"accuracy\"], label=\"train_acc\")\n        plt.plot(np.arange(0, epochs), history.history[\"val_accuracy\"], label=\"val_acc\")\n        plt.title(\"Training Loss and Accuracy - {}\".format(name))\n        plt.xlabel(\"Epoch #\")\n        plt.ylabel(\"Loss\/Accuracy\")\n        plt.legend()\n        plt.show()","38131ece":"NET = 'MobileNetV2'\nHIDDEN_LAYERS = [512, 128]\nCLASSES = 2\nEPOCHS = 1\nBATCH_SIZE = 32\n\ndata = (train_data_gen, val_data_gen, test_data_gen)\n\nmodel = Tuner(data, NET, HIDDEN_LAYERS, CLASSES, EPOCHS, BATCH_SIZE)\nmodel.run()","753b741e":"# Show sample images","21acf916":"# Fine-Tune","9cc87f72":"This notebook implements CNN Transfer Learning to classify faces with mask or not.\n\nTransfer Learning is a powerful way to leverage the performance of large pre-trained CNN models.\nWith Transfer Learning the final dense Layers of the pre-trained CNNs used for image classification\nare replaced by (a) new dense layer(s) where the new final layer implements a Softmax activation over\nthe new classes to predict. This way the extremely strong feature extractors of the pre-trained CNNs\ncan be used to predict the new classes. General consensus exists that image features used to classify\ncars, airplanes, bicycles, etc. are also useful to e.g. classify between dog breeds.\n\nTraining the CNN with the new dense layers should be executed in 2 phases:\n\n- Phase 1: Only train the new dense layers. This way the output of the CNN convolutions remains stable\nto allows the dense layers learn to classify the extracted features to classes. The original trained\nCNN layers will remain \u201cfrozen\u201d.\n- Phase 2: In this phase one additionally \u201cfine-tunes\u201d either the full CNN or selected layers only\nto further increase predictive accuracy of the network. A lower learning-rate is used to prevent too\ndrastic changes to the feature extractors.\n\n\n**Notes**\n\n1. As a single epoch of transfer learning already achieves 100% accuracy on test, Phase 2 is omitted.\n2. The dataset includes only frontal face shots. 100% test accuracy is achieved even without image data augmentation.","b307b147":"---\n\n# Load images\n\nLoad the images in train, validation and test datasets. \nAll classes only include only frontal face shots. 100% test accuracy is achieved even without image data augmentation.","e1a104ef":"The **Tuner** class is taken from my github repo:  \nhttps:\/\/github.com\/mikeleske\/cnn"}}