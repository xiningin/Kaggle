{"cell_type":{"6fff5065":"code","f18bf392":"code","32d5ebc5":"code","7ee896c6":"code","01f2c08c":"code","7e7c4548":"code","556414c0":"code","898c99e6":"code","13309a2a":"code","4c6cb159":"code","980a036a":"code","100cda36":"code","0e957798":"code","1fedffde":"code","f471c0e0":"code","881d2283":"code","c1be3d10":"code","5e069d32":"code","9d434cc5":"code","3750d5f0":"code","c00b468c":"code","0874845b":"code","f1422495":"code","75ce1cc3":"code","ce9d51c0":"code","547b3edd":"code","871bd747":"code","1d020ef0":"code","fd15c3c0":"code","d1169570":"code","4a85ea44":"code","1a1df454":"code","2907bcf6":"code","5fb0f890":"code","6d3811b8":"code","42eca397":"code","2a714a7c":"code","7bfbc155":"code","51844677":"code","08184578":"code","e9750242":"code","fec743de":"code","d0a69177":"code","313fe451":"code","84dbc09d":"code","cf5f98f9":"code","2a580d76":"code","4be89622":"code","694ef50d":"code","e37819fd":"code","ca199bba":"code","2ad534eb":"code","6942f8bc":"code","b888ab94":"code","8311f182":"code","d895c69a":"code","11343cb0":"code","68374fd9":"code","f0217dcf":"code","b459e3e1":"code","dfba8949":"code","0122b604":"code","32b7006a":"code","400e3ffc":"code","263ac198":"code","624699f1":"code","973a9db9":"code","a3d898bc":"code","94330579":"code","f4434324":"code","ec461052":"code","b504c706":"code","64c0928e":"code","00d908be":"code","4305a10a":"code","3fd50241":"code","d6c7494a":"markdown","36f3ffca":"markdown","8db94c1e":"markdown","a4d86248":"markdown","6a8888cd":"markdown","f4ffa6de":"markdown","d2243725":"markdown","e0741be4":"markdown","cf5a314c":"markdown","17650ba1":"markdown","7a71bf95":"markdown","8853113c":"markdown","04411dfb":"markdown","fb111058":"markdown","a07421fc":"markdown","7e2027c4":"markdown","2f953248":"markdown","8e145cb0":"markdown","d017bc67":"markdown","b116c54a":"markdown","e87e4f2d":"markdown","4efd0b83":"markdown","dfbd6618":"markdown","7fa9825d":"markdown","59eda6cd":"markdown","e27176ab":"markdown","12507765":"markdown","64b58671":"markdown","2d2a32b8":"markdown","56b10c9a":"markdown","82c6726f":"markdown","9abf98cf":"markdown","a781d265":"markdown","6566d192":"markdown","00f1f28a":"markdown","356d884c":"markdown","ded37cc5":"markdown","e700422e":"markdown","d73a5890":"markdown","7026dbdf":"markdown","61406704":"markdown","8ac103c7":"markdown"},"source":{"6fff5065":"#importing the necessary modules\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression,RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n#import lightgbm as lgb\n#import xgboost as xgb\n","f18bf392":"#Read the necessay train and test .csv files\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\nprint('The size of the train dataset is {}'.format(train.shape))\n\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nprint('The size of the test dataset is {}'.format(test.shape))","32d5ebc5":"#display th first 5 rows of the train and test sets\ntrain.head()","7ee896c6":"test.head()","01f2c08c":"#extracting the id columns form train and test datasets\nid_train = train['Id']\nid_test = test['Id']","7e7c4548":"#removing them form the train and test sets\ntrain.drop('Id', axis = 1, inplace= True)\ntest.drop('Id', axis = 1, inplace = True)\n#we check to see that they are gone","556414c0":"test.head()\n","898c99e6":"test.shape","13309a2a":"# We take a look at the states\ntrain.describe()","4c6cb159":"#we take a look at the different data types present in the train data\ntrain.dtypes.value_counts()","980a036a":"test.dtypes.value_counts()","100cda36":"#we keep this for when we will be separating DATA_ALL back into train and test \n\ntrain_rowsize = train.shape[0]\ntest_rowsize = test.shape[0]\ntest_rowsize   ","0e957798":"train_rowsize","1fedffde":"import warnings\nwarnings.filterwarnings('ignore')\n\ndata_all = pd.concat((train, test))\ndata_all.drop('SalePrice', axis = 1, inplace = True)\n","f471c0e0":"data_all.head()","881d2283":"#we check the size of the new dataframe \nprint('The shape of the data_all is:  {} '.format(data_all.shape))","c1be3d10":"#Here is a list of all the features with Nans and the number of null for each features\nnull_values = data_all.columns[data_all.isnull().any()]\nnull_features = data_all[null_values].isnull().sum().sort_values(ascending = False)\nmissing_data = pd.DataFrame({'No of Nulls' :null_features})\nmissing_data","5e069d32":"%matplotlib inline\nsns.set_context('talk')\nsns.set_style('ticks')\nsns.set_palette('dark')\n\nplt.figure(figsize= (16, 8))\nplt.xticks(rotation='90')\nax = plt.axes()\nsns.barplot(null_features.index, null_features)\nax.set(xlabel = 'Features', ylabel = 'Number of missing values', title = 'Missing data');\n","9d434cc5":"# Correlation between the features and the predictor- SalePrice\npredictor = train['SalePrice']\nfields = [x for x in train.columns if x != 'SalePrice']\ncorrelations = train[fields].corrwith(predictor)\ncorrelations = correlations.sort_values(ascending = False)\n# correlations\ncorrs = (correlations\n            .to_frame()\n            .reset_index()\n            .rename(columns={'level_0':'feature1',\n                                0:'Correlations'}))\ncorrs","3750d5f0":"plt.figure(figsize= (16, 8))\nax = correlations.plot(kind = 'bar')\nax.set(ylabel = 'Pearson Correlation', ylim = [-0.2, 1.00]);","c00b468c":"# Get the absolute values for sorting\ncorrs['Abs_correlation'] = corrs.Correlations.abs()\ncorrs","0874845b":"plt.figure(figsize= (16, 8))\nsns.set_context('talk')\nsns.set_style('white')\nsns.set_palette('dark')\n\nax = corrs.Abs_correlation.hist(bins= 35)\n\nax.set(xlabel='Absolute Correlation', ylabel='Frequency');\n","f1422495":"# Most correlated features wrt the abs_correlations\ncorrs.sort_values('Correlations', ascending = False).query('Abs_correlation>0.45')","75ce1cc3":"missing_data = ['PoolQC',\"MiscFeature\",\"Alley\", \"Fence\", \"FireplaceQu\", 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\n                  'GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', \n                  'BsmtHalfBath', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', \"MasVnrType\", \"MasVnrArea\",\n                  'MSSubClass']\n\nfor x in missing_data:\n    data_all[x] = data_all[x].fillna(0)","ce9d51c0":"# null_values_2 = data_all.columns[data_all.isnull().any()]\n# null_features_2 = data_all[null_values_2].isnull().sum().sort_values(ascending = False)\n# missing_data_2 = pd.DataFrame({'No of Nulls' :null_features_2})\n# missing_data_2","547b3edd":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\ndata_all[\"LotFrontage\"] = data_all.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n","871bd747":"data_all['MSZoning'].value_counts(normalize = True)  \n#we see that in the MsZoning 77% of the data is RL. so we replace the Missing values here with RL","1d020ef0":"data_all['MSZoning'] = data_all['MSZoning'].fillna('RL')\ndata_all['MSZoning'].isnull().any()    # to see if the MSZoning has any missing values(NaNs)","fd15c3c0":"# Utilities\ndata_all['Utilities'].value_counts(normalize = True)","d1169570":"data_all['Utilities'] = data_all['Utilities'].fillna('AllPub')","4a85ea44":"# Functional\ndata_all['Functional'].value_counts(normalize = True)","1a1df454":"data_all['Functional'] = data_all['Functional'].fillna('Typ')","2907bcf6":"data_all['Electrical'] = data_all['Electrical'].fillna(data_all['Electrical'].mode()[0])\ndata_all['KitchenQual'] = data_all['KitchenQual'].fillna(data_all['KitchenQual'].mode()[0])\ndata_all['Exterior1st'] = data_all['Exterior1st'].fillna(data_all['Exterior1st'].mode()[0])\ndata_all['Exterior2nd'] = data_all['Exterior2nd'].fillna(data_all['Exterior2nd'].mode()[0])\ndata_all['SaleType'] = data_all['SaleType'].fillna(data_all['SaleType'].mode()[0])","5fb0f890":"null_values_2 = data_all.columns[data_all.isnull().any()]\nnull_features_2 = data_all[null_values_2].isnull().sum().sort_values(ascending = False)\nmissing_data_2 = pd.DataFrame({'No of Nulls' :null_features_2})\nmissing_data_2\n\nprint('|\\t\\t NO MORE MISSING VALUES REMAINING. \\n\\n\\t\\t\\t...IMPUTING COMPLETED ...')","6d3811b8":"data_all.dtypes.value_counts()","42eca397":"train_new = data_all[:train_rowsize]\ntest_new = data_all[train_rowsize:]\ntest_new.shape","2a714a7c":"train_new.dtypes.value_counts()","7bfbc155":"test_new.dtypes.value_counts()","51844677":"train_new.head()","08184578":"#This is the separation of features into numerical and catergorical features, to do\n#feature engineering on each class of data.\n\n#isolating all the object\/categorical feature and converting them to numeric features\n\ntrain_numericals = train[train_new.select_dtypes(exclude = ['object']).columns]\ntest_numericals = test[test_new.select_dtypes(exclude = ['object']).columns]\n\n#takeoutthe salesprice from the numerical features\n#train_numericals = train_numericals.drop(\"SalePrice\")\ntrain_categcols = train_new.select_dtypes(include = ['object']).columns\ntest_categcols = test_new.select_dtypes(include = ['object']).columns\n\ntrain_categoricals = train[train_categcols]\ntest_categoricals = test[test_categcols]\n\n# train_numeric = train[numerical_cols]\n# test_numeric = test[numerical_cols2]\n\nprint(\"Shape of Train Categoricals features : {}\".format(train_categoricals.shape))\nprint(\"Shape of Train Numerical features : {}\\n\".format(train_numericals.shape) )\n\nprint(\"Shape of Test Categoricals features : {}\".format(test_categoricals.shape))\nprint(\"Shape of Test Numerical features : {}\".format(test_numericals.shape) )","e9750242":"# Do the one hot encoding on the categorical features\ntrain_dummies = pd.get_dummies(train_new, columns = train_categcols)\ntest_dummies = pd.get_dummies(test_new, columns = test_categcols)\n#align your test and train data\ntrain_encoded, test_encoded = train_dummies.align(test_dummies, join = 'left', axis = 1)\nprint('\\t\\tShape of the new encoded train: {}'.format(train_encoded.shape))\nprint('\\n\\t\\tShape of the new encoded test: {}'.format(test_encoded.shape))\nprint('\\n\\t\\t\\t....Encoding completed.....')","fec743de":"train_encoded.dtypes.value_counts()","d0a69177":"#we check for skewness in the float data\n\nskew_limit = 0.75\nskew_vals = train_numericals.skew()\n\nskew_cols = (skew_vals\n             .sort_values(ascending=False)\n             .to_frame()\n             .rename(columns={0:'Skewness'})\n            .query('abs(Skewness) > {0}'.format(skew_limit)))\n\nskew_cols ","313fe451":"tester = 'LotArea'\nfig, (ax_before, ax_after) = plt.subplots(1, 2, figsize=(16,5))\n#before normalisation\ntrain_new[tester].hist(ax = ax_before)\nax_before.set(title = 'Before nplog1p', ylabel = 'Frequency', xlabel = 'Value')\n\n#After normalisation\ntrain_new[tester].apply(np.log1p).hist(ax = ax_after)\nax_after.set(title = 'After nplog1p', ylabel = 'Frequency', xlabel = 'Value')\n\nfig.suptitle('Field \"{}\"'.format(tester));","84dbc09d":"print(skew_cols.index.tolist()) #returns a list of the values","cf5f98f9":"#Log transfrom all the numerical features except the Salepice column\nfor col in skew_cols.index.tolist():\n    train_encoded[col] = np.log1p(train_encoded[col])\n    test_encoded[col]  = test_encoded[col].apply(np.log1p)  # same thing\nprint(test_encoded.dtypes.value_counts())\nprint ('\\n\\t\\t:) Skewed data Transformation Completed :)')","2a580d76":"#plotting the distribution curve for the SalePrice\nf, ax = plt.subplots(figsize=(12, 6))\n#plt.xticks(rotation='90')\nsns.distplot(train['SalePrice']);","4be89622":"predictor = np.log1p(train.SalePrice)\n#plotting the distribution curve for the SalePrice\nf, ax = plt.subplots(figsize=(8, 6))\nsns.distplot(predictor);","694ef50d":"from sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(train_encoded, predictor, \n                                                    test_size=0.3, random_state=42)\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","e37819fd":"#We creat a function for calculating the mean_squared_erros\nfrom sklearn.metrics import mean_squared_error\n\ndef rmse (true_data, predicted_data):\n    return np.sqrt(mean_squared_error(true_data, predicted_data))","ca199bba":"#now to the most fun part. Feature engineering is over!!!\n#i am going to use linear regression, L1 regularization, L2 regularization and ElasticNet(blend of L1 and L2)\n\n#LinearRegression\nlinearRegression = LinearRegression().fit(X_train, y_train)\nprediction1 = linearRegression.predict(X_test)\nLR_score = linearRegression.score(X_test, y_test)\nLR_rmse = rmse(y_test, prediction1)\nprint('The scoring and root mean squared error for Linear Regression in percentage\\n')\nprint('\\t\\tThe score is: ',LR_score*100)\nprint('\\t\\tThe rmse is : ',LR_rmse*100)","2ad534eb":"#choose some values of alpha for cross validation.\nalphas = [0.005, 0.05, 0.1, 1, 5, 10, 50, 100]","6942f8bc":"\n#ridge\nridgeCV = RidgeCV(alphas=alphas).fit(X_train, y_train)\nprediction2 = ridgeCV.predict(X_test)\nR_score = ridgeCV.score(X_test, y_test)\nR_rmse = rmse(y_test, prediction2)\nprint('The scoring and root mean squared error for Linear Regression in percentage\\n')\nprint('\\tThe parameter used for here was alpha = {}\\n'.format(ridgeCV.alpha_))\nprint('\\t\\tThe score is: ',R_score*100)\nprint('\\t\\tThe rmse is : ',R_rmse*100)","b888ab94":"#lasso\nlassoCV = LassoCV(alphas=[0.005, 0.001, 0.05, 0.01,1, 5], max_iter=1e2).fit(X_train, y_train)\nprediction3 = lassoCV.predict(X_test)\nL_score = lassoCV.score(X_test, y_test)\nL_rmse = rmse(y_test, prediction3)\nprint('The scoring and root mean squared error for Linear Regression in percentage\\n')\nprint('\\tThe parameter used for here was alpha = {}'.format(lassoCV.alpha_))\nprint('\\n\\t\\tThe score is: ',L_score*100)\nprint('\\t\\tThe rmse is : ',L_rmse*100)","8311f182":"#elasticNetCV\nl1_ratios = np.linspace(0.1, 0.9, 9)\nelasticnetCV = ElasticNetCV(alphas=alphas, l1_ratio=l1_ratios, max_iter=1e2).fit(X_train, y_train)\nprediction4 = elasticnetCV.predict(X_test)\nEN_score = elasticnetCV.score(X_test, y_test)\nEN_rmse = rmse(y_test, prediction4)\nprint('The scoring and root mean squared error for Linear Regression in percentage\\n')\nprint('\\tThe parameter used for here was alpha = {} and l1_ratios = {} \\n'.format(elasticnetCV.alpha_, elasticnetCV.l1_ratio_))\nprint('\\t\\tThe score is: ',EN_score*100)\nprint('\\t\\tThe rmse is : ',EN_rmse*100)","d895c69a":"randfr = RandomForestRegressor(random_state = 42) #random_state to avoid the result from fluctuating","11343cb0":"param_grid = { \n    'n_estimators': [50,250,500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [2, 4, 6, 8, 10],\n}","68374fd9":"randfr_cv = GridSearchCV(estimator=randfr, param_grid=param_grid, cv= 5)  #cv = 5 to specify the number of folds(5 in this case)  in a stratified Kfold\nrandfr = randfr_cv.fit(X_train, y_train)","f0217dcf":"prediction5 = randfr.predict(X_test)\n#print(prediction5.shape)\nRF_score = randfr.score(X_test, y_test)\nRF_rmse = rmse(y_test, prediction5)\nprint('The scoring and root mean squared error for Linear Regression in percentage\\n')\nprint('\\tThe parameter used for here were = {}\\n'.format(randfr_cv.best_params_))\nprint('\\t\\tThe score for random forest is: {} '.format(RF_score*100))\nprint('\\t\\tThe rmse is: {} '.format (RF_rmse*100))","b459e3e1":"#putting it lall together\n\nscore_vals = [LR_score, R_score, L_score, EN_score, RF_score]\nrmse_vals = [LR_rmse, R_rmse, L_rmse, EN_rmse, RF_rmse]\nlabels = ['Linear', 'Ridge', 'Lasso', 'ElasticNet', 'RandomForest']\n\nrmse_df = pd.Series(score_vals, index=labels).to_frame()\nrmse_df.rename(columns={0: 'SCORES'}, inplace=1)\nrmse_df['RMSE'] = rmse_vals\nrmse_df","dfba8949":"rmse_df = rmse_df.sort_values(['RMSE'], ascending=True)\nrmse_df","0122b604":"from datetime import datetime\n\nstart_xgb = datetime.now()\n\nxgb = XGBRegressor().fit(X_train, y_train)\n\nend_xgb = datetime.now()\n\nxgb_time = end_xgb - start_xgb\nprint('Duration for XGBoost: {}'.format(xgb_time))","32b7006a":"prediction6 = xgb.predict(X_test)\nxgb_score = xgb.score(X_test, y_test)\nxgb_rmse = rmse(y_test, prediction6)\nprint('The scoring and root mean squared error for XGBoost in percentage\\n')\nprint('\\t\\tThe score is: ',xgb_score*100)\nprint('\\t\\tThe rmse is : ',xgb_rmse*100)","400e3ffc":"Adding_xgboost = pd.Series({'SCORES': xgb_score, 'RMSE': xgb_rmse}, name = 'XGBoost')\nrmse_df = rmse_df.append(Adding_xgboost)\nrmse_df","263ac198":"start_lgbm = datetime.now()\n\nlgb = LGBMRegressor().fit(X_train, y_train)\n\nend_lgbm = datetime.now()\n\nlgbm_time = end_lgbm - start_lgbm\nprint('Duration for Light GBM: {}'.format(lgbm_time))","624699f1":"prediction7 = lgb.predict(X_test)\nlgb_score = lgb.score(X_test, y_test)\nlgb_rmse = rmse(y_test, prediction7)\nprint('The scoring and root mean squared error for light GBM in percentage\\n')\nprint('\\t\\tThe score is: ',lgb_score*100)\nprint('\\t\\tThe rmse is : ',lgb_rmse*100)","973a9db9":"Adding_lgbm = pd.Series({'SCORES': lgb_score, 'RMSE': lgb_rmse}, name = 'Light GBM')\nrmse_df.append(Adding_lgbm)","a3d898bc":"print('\\t\\tComparing the 2 model durations:\\n')\nprint('XGBOOST : {} \\t\\t LIGHT GBM : {}'.format(xgb_time, lgbm_time))","94330579":"comparisons = {'Scores': (lgb_score, xgb_score), 'RMSE': (lgb_rmse, xgb_rmse), 'Execution Time' : (lgbm_time, xgb_time)}\ncomparisons_df = pd.DataFrame(comparisons)","f4434324":"comparisons_df.index= ['LightGBM','XGBOOST'] \ncomparisons_df","ec461052":"test_encoded.isnull().sum()","b504c706":"null_values = test_encoded.columns[test_encoded.isnull().any()]\nnull_features = test_encoded[null_values].isnull().sum().sort_values(ascending = False)\nmissing = pd.DataFrame({'No of Nulls' :null_features})\nmissing","64c0928e":"test_encoded = test_encoded.fillna(0)","00d908be":"prediction = lassoCV.predict(test_encoded) # WE USE THE BEST RMSE which is tht for Lasso\nfinal_prediction = np.exp(prediction) #undoing the np log we did on the saleprices else the resu","4305a10a":"\nHouse_submission = pd.DataFrame({'Id': id_test, 'SalePrice': final_prediction})\nprint(House_submission.shape)\nHouse_submission.to_csv('House_prediction.csv', index = False)\n","3fd50241":"print(House_submission.sample(6))","d6c7494a":"# 2 - Analysis and Feature engineering","36f3ffca":"These blogs explain the 2 concept very well. Take a look to learn more:\n* https:\/\/towardsdatascience.com\/catboost-vs-light-gbm-vs-xgboost-5f93620723db\n* https:\/\/www.analyticsvidhya.com\/blog\/2017\/06\/which-algorithm-takes-the-crown-light-gbm-vs-xgboost\/\n\n    Another useful paper about this by Microsoft research: **You definately want to read this**\n* https:\/\/papers.nips.cc\/paper\/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf\n\n**Below is a little summary i could gather from reading the paper:**","8db94c1e":"Do you see the difference?. Now the distribution in more symmetric, and more towards the center.","a4d86248":"Separatiig the features into numerical and catergorical data will help encode the categorical data easily. We are going to isolate all the object\/categorical feature and converting them to numeric features","6a8888cd":"Getting started: \n* What Will i do in this notebook - \n\nFirst things First: **snacks and chip checked, Coffee checked**\n\nPlan:\n* Introduction\n* Loading the data\n* Feature engineering\n* Finding correlation with predictor\n    * Finding the most correlated features\n* Imputting missing values\n* One hot encoding or pd.Dummies\n* Scaling \n* Checking skewness","f4ffa6de":"We can observe that, LightGBM significantly outperforms XGBoost in terms of speed but XGBoost has a slightly better score than LightGBM for this experiement. \n\n**So which do you prefer? Speed  or efficiency? i would like to hear from you so please let me know in the comment section and please add a reason for your choice. **\n\n **:-) Thanks for reading till the end. If you found this kernel helpful, an upvote would be highly appreciated. :-)**","d2243725":"#  ** Introduction**","e0741be4":"ElasticnetCV is the combinaision of L1 and L2 rgularisation. We need to set another parameter called the l1_ratio. \n\n** Note** that a good choice of list of values for l1_ratio is often to put more values close to 1 (i.e. Lasso) and less close to 0 (i.e. Ridge) \n\ncheck [https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.ElasticNetCV.html](http:\/\/) for more info","cf5a314c":"**LassoCV uses a L1 regularization to reduce the magnitudes of the coefficients. L1 regularization will selectively shrink some coefficients, effectively performing feature elimination. LASSO IS VERY SLOW SO I WILL USE JUST A FEW ALPHAS**","17650ba1":"# References\/ useful kernels\n* https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\n* https:\/\/www.kaggle.com\/agodwinp\/stacking-house-prices-walkthrough-to-top-5\n* https:\/\/www.kaggle.com\/chocozzz\/beginner-challenge-house-prices?scriptVersionId=7418555\n* https:\/\/www.kaggle.com\/sociopath00\/random-forest-using-gridsearchcv\n* https:\/\/towardsdatascience.com\/catboost-vs-light-gbm-vs-xgboost-5f93620723db","7a71bf95":"#Now we deal with the missing data in the lotFrontage\n* ** LotFrontage** : Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can fill in missing values by the median LotFrontage of the neighborhood.\n\n","8853113c":"I will use GridSearchCV here just to be consistent and also because random forest does not have a built in cross validation.","04411dfb":"Now that we have done the imputing on all the data. We now separate the data into their original train and test columns","fb111058":"For the remaining features with missing values, we will replace the missing values with the most frequent values in that column.","a07421fc":"Obsevations:  #Let me know of other observations which i missed :)\n\n We see that the\n * The counts for features are not the same meaning, and we know that the total count for the train data should be 1460 rows. This means some of the features have missing values or NaNs.\n * The difference betwenn the min and max values is way too large for some features, meaning these features have lots of outliers.  When outliers are left in a dataset, the models may because of hyperensitivity to these points resulting in an over or under fit model. But we have to be smart about the way we handle outlier. It is not always safe to remove them!!!","7e2027c4":"# One Hot Encoding and Scaling","2f953248":"Lets take a look at the predictor('SalePrice')","8e145cb0":"The RidegeCV, LassoCV, ElasticnetCV are Regression models with built-in cross validation so by default, it performs generalised cross-validation meaning it is going to go through all the alphas and choose the one value that perfromed best.","d017bc67":"The before normalisation shows a right skewed distribution or positive skewed feature. After nplog1p, the distribution is more symmetrical.\n\n**BTW: You can change the tester and see the different features before and after nplog1p**\n","b116c54a":"The Saleprice is positively skewed- it is tilting more towards the right. It is usally recommended to fix the skewness of such distribution to get a better model. Some machine learning algorithms work better with features that are normally distributed i.e symmetrical and bell-shaped like the one below. Also,  It is usually adviced to use the same transformation used in the train data on the predictor","e87e4f2d":"Here i want to do a comparative study of XGboost And the Light GBM. \n\nAs you may already know, XGBoost is one of the top models uses in competitions and it is the model that usually produces the best and top score here on Kaggle. This is because it is fast effective and very powerful. \n\nWhat about LightGBM?\n\nSincerely, i had never heard of it before until i stumble upon it here [https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard](http:\/\/). So i decided to dig a little deeper, and try to understand their differences with respect to this dataset. \n\n","4efd0b83":"Having  missing values or NaNs in your dataset can couse error with come machine learning algorithms. We could try to replace the missing values with \n* A constant value that has meaning within the domain like a 0, or \n* We could replace them with the mean, median or mode of values \n* With another values selected from a random record or another predicted model\n\nWe will start imputing the missing values in our dataset(DATA):","dfbd6618":"# Predictive modelling","7fa9825d":"We will use the pd.Dummies method to encode the features. why? I prefer it ;)","59eda6cd":"# Imputing the values.  https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard does an excellent job- check it out!","e27176ab":"# Modelling (part 1)","12507765":"I will be using a numpy  log1p to transform  the very skewed data. Just to give a sense of what numpy log1p tranformation does in a skewed dataset, We are going to design a before and after log1p .","64b58671":"A Histogram of absolute correlations","2d2a32b8":"For some of these features, we are going to replace the mssing values by 0. For example: \n* **PoolQc** data description says NA means \"No Pool\". Which is common for homes to not have pools so we replace with 0. Same for **Alley, Fences, fireplaceQu, Misc, etc...**\n","56b10c9a":"This notebook is a very simple and basic introductory to some concepts in machine learning and also introduce a relatively new algorithm that hasn't got a lot of reading resources on the internet except for its documentation. \n\nIn a nut shell, Xgboost( eXtreme Gradient BOOSTing) is the predominant model here on Kaggle and it is also responsible for most of the top scores in many competitions due to its model perfromance. It is an implementation of the gradient boosting decision trees and is really fast when compared to other implementations of  gradient boosting. \n\nOn the other hand is the Light GBM which was release by Microsoft in January 2017. It is  prefix ''Light'' because of its high speed. It is getting more popular because as the size of data get bigger everyday, it is becoming difficult for more traditional data science algorithms to handle these large dataset, produce faster results, use less memory and also give us good prediction accuracy.  \n\nI am also quite a newcomer to the Kaggle scene and this is my first implementation of Light GBM. Therefore, i am pretty sure there is room for improvement so please feel freee to leave any comment or suggestions on how it can be improved. \\\n\nThe materials in this notebook( part 1 especially) borrows heavily form some awesome notebooks i stumble upon. I use traditional models in this part. Part 2 is the comparative study of the Light GBM and xgboost.","82c6726f":"Th facilitate the feature engineering, lets merge the train and test data into 1 dataFrame which we will call 'DATA_ALL' but we exclude the Saleprice columns since we do not want to alter our Predictor \n","9abf98cf":"*** XGBoost**:\n\nXGBoost is one of the most popular algorithms cout there and it has even become the de-facto algorithm for winning competitions here on Kaggle. But its efficientcy and scalability is still insatisfactory especially  when dealing with larger datasets and large feature dimensionality. The reason being that in **XGBoost, for each feature, all the data instances(observations\/samples) need to be scanned  to estimate the information gain of all possible split points, and this can be very time consuming especially when handling big data as \ncomputational complexity is propartional to the number of features and the number of samples.**\n\n\n*** How Does XGBoost work:**\n\nIt works by using both the  pre-sorting algorithm and the histogram based algorithms for finding the best split. the pre sorting algorithm is one of the most popular algorithms for finding split points but it is inefficient in both training speed and memory consumption. Pre-sorting algorthm works by \n\n1.  It enumerates all the features for each node  \n2. Sort the instance or samples for eeach features by feature values\n3. Use a linear scan to decide the best split along that feature basis \n4. Finally, it takes the best split solution along all the features\n\n\n*** Light GBM:**\n\nMicrosoft released it first stable version of Light GBM in Jan 2017. To remedie this problem, Light GBM uses a technique called GOSS( Gradient-based One-Side Sampling) which besically achieves a good balance between reducing the number of instances while maintainig the accuracy for learned decision trees.\n\n*** How does Light GBM work**\n\nWhile there is no native weight for data instance in Gradient Boosting Decision Trees, data instances with different gradients play different roles in the computation of information gain. In particular, according to the definition of information gain, those instances with larger gradients (i.e., under-trained instances) will contribute more to the information gain. Therefore, when down sampling the data instances, in order to retain the accuracy of information gain estimation, we should better keep those instances with large gradients (e.g., larger than a pre-defined threshold, or among the top percentiles), and only randomly drop those instances with small gradients\n\n**In short: GOSS keeps all the instances with large gradients and performs random sampling on the instances with small gradients.**","a781d265":"# SKEWNESS","6566d192":"This means we have 43 categorical features (object = 43) and the rest are numerical features for the train and test datasets. ","00f1f28a":"In the firstt part of modelling, i will be using the Linear regression, ridgecv, lassocv, elasticnetcv and  random forest. In the part 2, i really want to focus on applying XGBoost and Gradient Boosting Algorithms on tis data and see how both models differ from each other. ","356d884c":"# Modelling part 2","ded37cc5":"Lets take alook at how skewed our dataset is. Just a little background:\n\n* In statistics, skewness is a measure of the asymmetry of the probability distribution of a random variable about its mean. In other words, skewness tells you the amount and direction of skew (departure from horizontal symmetry). The skewness value can be positive or negative, or even undefined. If skewness is 0, the data are perfectly symmetrical, although it is quite unlikely for real-world data. As a general rule of thumb:\n\n* If skewness is less than -1 or greater than 1, the distribution is highly skewed.\n* If skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is moderately skewed.\n* If skewness is between -0.5 and 0.5, the distribution is approximately symmetric.\n\nreferences:https:\/\/help.gooddata.com\/display\/doc\/Normality+Testing+-+Skewness+and+Kurtosis","e700422e":"# 1 - LOADING THE DATA","d73a5890":"# * Light GBM Versus XGBoost","7026dbdf":"**RidgeCV uses a L2 regularization to reduce the magnitudes of the coefficients whichcan be helpful in situations where there is high variance **","61406704":"We see that in both the training and the test sets, we have an 'Id- column' which we do not need right now. So we are going to save these columns and use them later in the submission files. Then we drop the id columns from boht datasets.","8ac103c7":"As we can see from above the  new train and test sets have 43 categorical data and the rest of the data is numerical. We are going to encode the categorical features alone. So we need to separate the categoricals from the numerocal features."}}