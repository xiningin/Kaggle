{"cell_type":{"721a5cf6":"code","c724fce1":"code","d1eef851":"code","714adf14":"code","fff66f5a":"code","176540b8":"code","c71f6f98":"code","07e7749f":"code","984ed62e":"code","df296878":"code","d5643aa9":"code","7cfb82e8":"code","7315a3ce":"code","83c76811":"code","81e44a1a":"code","46150bd7":"code","5db941cb":"code","43f4c179":"code","555fc83b":"code","f8c8009a":"code","b5c50a53":"code","c95567d8":"code","3da2969d":"code","e0badbfc":"code","03e2610b":"code","197c96cb":"code","b547bbb5":"code","1ed82797":"code","f03bbc7d":"code","fbd5643f":"code","716b9077":"code","042300bc":"code","ac97c3e3":"code","9ea42318":"code","dd6743c7":"code","032e3d5d":"code","1526aef0":"code","b2673ed3":"code","54573c51":"code","09dc1afa":"code","d80a738f":"code","235dc452":"code","be844eb4":"markdown","ab6a7314":"markdown","3deac33b":"markdown","5d6fc6f6":"markdown","7367a4bb":"markdown","d14fc902":"markdown","93ba7aff":"markdown","14da4ab1":"markdown","c7dc06cd":"markdown","3aa46da4":"markdown","c889b3a2":"markdown","5442b471":"markdown","ff233222":"markdown","65c0cbae":"markdown","1e99369c":"markdown","05911345":"markdown","79a1eba0":"markdown","1ea92cf1":"markdown","ef3e4619":"markdown","5116fbd2":"markdown","15c40f77":"markdown","d9503e08":"markdown","120bf370":"markdown"},"source":{"721a5cf6":"!pip install -q efficientnet","c724fce1":"import re\nimport gc  \nimport os\nimport math\nimport json\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport tensorflow as tf\nfrom pathlib import Path\nfrom sklearn.cluster import DBSCAN\nimport efficientnet.tfkeras as efn\nfrom collections import defaultdict\nfrom matplotlib import pyplot as plt\nfrom kaggle_datasets import KaggleDatasets\n\nprint(\"Tensorflow version \" + tf.__version__)\nAUTO = tf.data.experimental.AUTOTUNE","d1eef851":"def seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    tf.random.set_seed(seed)","714adf14":"MODEL='EffNB4'\n\nHEIGHT=512\nWIDTH=512\nIMAGE_SIZE = [HEIGHT, WIDTH] # At this size, a GPU will run out of memory. Use the TPU.\n                             # For GPU training, please select 224 x 224 px image size.    \nDESCRIPTION='embeddings'\nNAME=f'{MODEL}_{HEIGHT}x{WIDTH}_{DESCRIPTION}'\nSEED=311\nBATCH_SIZE_FACTOR = 32\n\nmodel_selector={'EffNB0': efn.EfficientNetB0,\n                'EffNB1': efn.EfficientNetB1,\n                'EffNB2': efn.EfficientNetB2,\n                'EffNB3': efn.EfficientNetB3,\n                'EffNB4': efn.EfficientNetB4,\n                'EffNB5': efn.EfficientNetB5,\n                'EffNB6': efn.EfficientNetB6,\n                'EffNB7': efn.EfficientNetB7,\n               }\n\nPATH=Path('\/kaggle\/input\/cassava-leaf-disease-classification\/')\ntrain=pd.read_csv(PATH\/'train.csv')\n              \nseed_everything(SEED)\nwarnings.filterwarnings('ignore')\nprint(f\"Model name: {NAME}.\")","fff66f5a":"print(f\"The shape of the training set is {train.shape}.\")\nprint(f\"The columns in `train`:\\n {list(train.columns)}.\\n\")","176540b8":"train.head()","c71f6f98":"labels=np.sort(train['label'].unique())\nlabels","07e7749f":"with open(os.path.join(PATH, \"label_num_to_disease_map.json\")) as file:\n    label_mapping = json.loads(file.read())\n    \nlabel_mapping","984ed62e":"label_mapping={0: 'Cassava Bacterial Blight',\n               1: 'Cassava Brown Streak Disease',\n               2: 'Cassava Green Mottle',\n               3: 'Cassava Mosaic Disease',\n               4: 'Healthy',\n              }","df296878":"SHORT_CLASSES=['CBB', 'CBSD', 'CGM', 'CMD', 'Healthy']","d5643aa9":"train['desease'] = train['label'].map(label_mapping)\ntrain['desease'].value_counts()","7cfb82e8":"%%time\n\nCLASSES = [label_mapping[i] for i in range(len(label_mapping))]\nCLASSES","7315a3ce":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment \n    # variable is set. On Kaggle this is always the case.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n    \nBATCH_SIZE = BATCH_SIZE_FACTOR * strategy.num_replicas_in_sync\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\nprint(\"BATCH SIZE: \", BATCH_SIZE)","83c76811":"GCS_PATH = KaggleDatasets().get_gcs_path('cassava-leaf-disease-classification')\nALL_TFRECS=np.array(tf.io.gfile.glob(GCS_PATH + '\/train_tfrecords\/*.tfrec'))\n\nprint(GCS_PATH)","81e44a1a":"ALL_TFRECS","46150bd7":"def count_data_items(filenames):\n    # the number of data items is written in the name of the .tfrec files, \n    # i.e. test10-687.tfrec = 687 data items\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    \n    return np.sum(n)","5db941cb":"%%time\n\nprint(f\"The total number of images is {count_data_items(ALL_TFRECS)}.\")","43f4c179":"def decode_image(image_data):\n    \"\"\"\n        1. Decode a JPEG-encoded image to a uint8 tensor.\n        2. Cast the tensor to float and normalizes (range between 0 and 1).\n        3. Reshape the image to the expected shape.\n    \"\"\"\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    # we normalize our inputs by subtracting ImageNet mean of 0.449 \n    # and dividing by ImageNet standard deviation of 0.226. \n    # We have to do it because we won't be fine-tuning our model.\n    image = ((tf.cast(image, tf.float32) \/ 255.0) - 0.449) \/ 0.226\n    image = tf.reshape(image, [HEIGHT, WIDTH, 3])\n    return image","555fc83b":"def read_tfrecord(example, labeled=True):\n    \"\"\"\n        1. Parse data based on the 'TFREC_FORMAT' map.\n        2. Decode image.\n        3. If 'labeled' returns (image, label) if not (image, name).\n    \"\"\"\n    if labeled:\n        TFREC_FORMAT = {\n            'image': tf.io.FixedLenFeature([], tf.string), \n            'target': tf.io.FixedLenFeature([], tf.int64), \n        }\n    else:\n        TFREC_FORMAT = {\n            'image': tf.io.FixedLenFeature([], tf.string), \n            'image_name': tf.io.FixedLenFeature([], tf.string), \n        }\n    example = tf.io.parse_single_example(example, TFREC_FORMAT)\n    image = decode_image(example['image'])\n    if labeled:\n        label_or_name = tf.cast(example['target'], tf.int32)\n    else:\n        label_or_name =  example['image_name']\n    return image, label_or_name","f8c8009a":"def load_dataset(filenames, labeled=True, ordered=False):\n    \"\"\"\n        Create a Tensorflow dataset from TFRecords.\n    \"\"\"\n    ignore_order = tf.data.Options()\n    if not ordered:\n        # disable order, increase speed. Makes sense to do\n        # if we are going to shuffle the data anyway\n        ignore_order.experimental_deterministic = False\n    \n    # automatically interleaves reads from multiple files\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n    # if ordered=False uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.with_options(ignore_order)\n    # returns a dataset of (image, label) pairs if labeled=True \n    # or (image, id) pairs if labeled=False\n    dataset = dataset.map(lambda x: read_tfrecord(x, labeled=labeled), num_parallel_calls=AUTO)\n    return dataset","b5c50a53":"%%time\n\ndataset = load_dataset(ALL_TFRECS)\n\nprint(\"Example of the training data:\")\nfor image, label in dataset.take(1):\n    print(\"The image batch size:\", image.numpy().shape)\n    print(\"Label:\", label.numpy())","c95567d8":"def get_dataset(FILENAMES):\n    \"\"\"\n        Return a Tensorflow dataset ready for training or inference.\n    \"\"\"     \n    dataset = load_dataset(FILENAMES, labeled=False, ordered=True)\n    dataset = dataset.cache()\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    \n    return dataset","3da2969d":"# numpy and matplotlib defaults\nnp.set_printoptions(threshold=15, linewidth=80)","e0badbfc":"def batch_to_numpy_images_and_labels(databatch):\n    images, labels = databatch\n    numpy_images = images.numpy()\n    numpy_labels = labels.numpy()\n\n    if numpy_labels.dtype == object: # binary string in this case, these are image ID strings\n        # If no labels, only image IDs, return None for labels (this is the case for test data)\n        numpy_labels = [None for _ in enumerate(numpy_images)]\n\n    return numpy_images, numpy_labels","03e2610b":"def title_from_label_and_target(label, correct_label):\n    if correct_label is None:\n        return SHORT_CLASSES[label], True\n    correct = (label == correct_label)\n    return \"{} [{}{}{}]\".format(SHORT_CLASSES[label], 'OK' if correct else 'NO', u\"\\u2192\" \n                                if not correct else '', \n                                SHORT_CLASSES[correct_label] if not correct else ''), correct","197c96cb":"def display_one_image(image, title, subplot, red=False, titlesize=16):\n    plt.subplot(*subplot)\n    plt.axis('off')\n    plt.imshow(image)\n    if len(title) > 0:\n        plt.title(title, fontsize=int(titlesize) if not red else int(titlesize\/1.2), \n                  color='red' if red else 'black', fontdict={'verticalalignment':'center'}, \n                  pad=int(titlesize\/1.5)\n                 )\n    return (subplot[0], subplot[1], subplot[2]+1)","b547bbb5":"def display_batch_of_images(databatch, show_class_names=True, predictions=None):\n    \"\"\" This will work with:\n        display_batch_of_images(images)\n        display_batch_of_images(images, predictions)\n        display_batch_of_images((images, labels))\n        display_batch_of_images((images, labels), predictions)\n    \"\"\"\n    # data\n    images, labels = batch_to_numpy_images_and_labels(databatch)\n    if not any(l is None for l in labels):\n        labels = np.argmax(labels, axis=-1)\n        \n    # auto-squaring: this will drop data that does  \n    # not fit into square or square-ish rectangle\n    rows = int(math.sqrt(len(images)))\n    cols = len(images)\/\/rows\n        \n    # size and spacing\n    FIGSIZE = 13.0\n    SPACING = 0.1\n    subplot=(rows,cols,1)\n    if rows < cols:\n        plt.figure(figsize=(FIGSIZE,FIGSIZE\/cols*rows))\n    else:\n        plt.figure(figsize=(FIGSIZE\/rows*cols,FIGSIZE))\n    \n    # display\n    for i, (image, label) in enumerate(zip(images[:rows*cols], labels[:rows*cols])):\n        if show_class_names:\n            title = '' if label is None else SHORT_CLASSES[label]\n        else:\n            title = ''\n        correct = True\n        if predictions is not None:\n            title, correct = title_from_label_and_target(predictions[i], label)\n        # magic formula tested to work from 1x1 to 10x10 images\n        dynamic_titlesize = FIGSIZE*SPACING\/max(rows,cols)*40+3\n        subplot = display_one_image(image, title, subplot, \n                                     not correct, titlesize=dynamic_titlesize)\n    \n    #layout\n    plt.tight_layout()\n    if label is None and predictions is None:\n        plt.subplots_adjust(wspace=0, hspace=0)\n    else:\n        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n    plt.show()","1ed82797":"# Peek at training data\n\ndataset = get_dataset(ALL_TFRECS)\ndataset = dataset.unbatch().batch(20)\nimg_batch = iter(dataset)","f03bbc7d":"# run this cell again for next set of images\ndisplay_batch_of_images(next(img_batch))","fbd5643f":"del dataset, img_batch\ngc.collect()","716b9077":"# EXTRACT LAST LAYER OF EFFICIENT NET WITH GLOBAL AVERAGE POOLING\n\ndef build_model():\n    with strategy.scope():\n        pretrained_model = model_selector[MODEL](input_shape=(*IMAGE_SIZE, 3),\n                                                      weights='imagenet',\n                                                      include_top=False\n                                                      )\n        inp = tf.keras.layers.Input(shape=(*IMAGE_SIZE, 3))\n        x = pretrained_model(inp)\n        out = tf.keras.layers.GlobalAveragePooling2D()(x)\n        model = tf.keras.Model(inputs=inp, outputs=out)\n        return model","042300bc":"%%time\n\nmodel = build_model()","ac97c3e3":"def retrieve_image(image, image_id_or_label):\n    return image","9ea42318":"def retrieve_id_or_label(image, image_id_or_label):\n    return image_id_or_label","dd6743c7":"%%time\n\nds = get_dataset(ALL_TFRECS)\n\nds_imgs = ds.map(retrieve_image, num_parallel_calls=AUTO) \nds_ids = ds.map(retrieve_id_or_label, num_parallel_calls=AUTO).unbatch()\n\nembs = model.predict(ds_imgs,verbose=1)\n\nimage_ids=next(iter(ds_ids.batch(count_data_items(ALL_TFRECS)))).numpy().astype('U')","032e3d5d":"print(f\"The shapes of the embeddings and image ID's are {embs.shape} and {image_ids.shape}, respectively.\")","1526aef0":"%%time\n\nnp.save(f'embeddings_train_{MODEL}_{HEIGHT}x{WIDTH}', embs)\nnp.save(f'image_ids_train_{MODEL}_{HEIGHT}x{WIDTH}', image_ids)","b2673ed3":"%%time\n\nclusters = defaultdict(list)\nfor image_name, cluster_id in zip(image_ids, DBSCAN(eps=3.0, min_samples=1, n_jobs=4).fit_predict(embs)):\n    clusters[cluster_id].append(image_name)","54573c51":"potential_duplicates = np.array([c for c in clusters.values() if len(c)>1])\npotential_duplicates","09dc1afa":"def plot_duplicates(dups, h_factor=7, v_factor=6, font_size=14):\n    n_dups_max=max([len(d) for d in dups])\n    n_rows=len(dups)\n\n    fig, ax = plt.subplots(n_rows, n_dups_max, figsize=(n_dups_max*h_factor, n_rows*v_factor))\n\n    for j, image_ids in enumerate(dups):\n        for i, image_id in enumerate(image_ids):\n            \n            PATH_IMG = PATH\/'train_images'\n            label = str(train.loc[train['image_id'] == image_id, 'label'].values[0])\n                \n            image = Image.open(PATH_IMG\/image_id)\n\n            ax[j][i].imshow(image)\n            ax[j][i].set_title(\"image_id: \" + image_id + \";  label: \" + label, \n                               fontsize=font_size)\n            ax[j][i].axis('off')","d80a738f":"plot_duplicates(potential_duplicates)","235dc452":"np.save('duplicates', potential_duplicates)","be844eb4":"## Visualization utilities\n\nAnd here are the standard visualization utilities that we will use to visualize the dataset.","ab6a7314":"## A brief outline\n\nIn this notebook, we show how to search for duplicated images using image embeddings and the DBSCAN clustering algorithm. The embeddings are extracted with the help of a pre-trained EfficientNet model. We find 5 pairs of images in the Cassava train set which are potential duplicates. Visual inspection of the images in each pair reveals that all of them are true duplicates of one another, so the precission of our method on the training dataset is 100% (the recall is unknown). We find that one pair of duplicates contains images which are labeled differently which is a clear demonstration of the presence of noissy labels in the competition dataset.\n\nThis method can also be applied to search for the overlap between the current competition dataset and the [2019 data](https:\/\/www.kaggle.com\/c\/cassava-disease\/data) (we are leaving this as an excercise for an interested reader). This is very important to keep all duplicates under control when doing cross-validation because we do not want to validate our model on the same data that the model was shown during the training phase. Identifying duplicated images will help us to enforce the uniqueness of the images in the training and the validation sets.\n\nThe discussion topic is here: [Searching for duplicated images with DBSCAN](https:\/\/www.kaggle.com\/c\/cassava-leaf-disease-classification\/discussion\/210723).","3deac33b":"Thank you for reading! Please kindly upvote this notebook if you find it helpful!","5d6fc6f6":"## Finding duplicates with DBSCAN\n\nTo search for duplicated images, we apply the method suggested by Alex Shonenkov in his Melanoma compentition [public kernenl](https:\/\/www.kaggle.com\/shonenkov\/dbscan-clustering-check-marking). The idea is to tune the parameters of the `DBSCAN` clustering algorithm to split the dataset into clusters of similar images. Then we will visually inspect the resulting clusters to see whether or not they contain true duplicates.\n\nIf you want to learn more about DBSCAN and how it works please refer to the following scikit-learn page: [DBSCAN](https:\/\/scikit-learn.org\/stable\/modules\/clustering.html#dbscan).","7367a4bb":"Saving the embeddings and the image ID's.","d14fc902":"## Saving the duplicate filenames","93ba7aff":"## Configuration","14da4ab1":"Now, let's download the mapping between the label numbers and the disease names.","c7dc06cd":"Let's remove the abbreviations at the end of each decesase name and turn the keys of the dictionary into integers.","3aa46da4":"## Dataset visualizations","c889b3a2":"We found 5 pairs of potential duplicates. Let's plot them for visual inspection.","5442b471":"## Datasets utility functions\n\nBelow are the standard functions that we will be using to read and process the data from the `.tfrec` files. ","ff233222":"## Brief discussion of the result\n\nVisual inspection reveals that all of these pairs contain duplicated images. The method yields zero number of false positives, so it's *precision* $ = TP\/(TP + FP) = 1$, where $TP$ and $FP$ stand for the true and false positives, respectively. (Of course, this is the precision on the competition training set -- I cannot guarantee that the method's performance will be the same on some arbitrary data.)\n\nAlso, note that the `['1562043567.jpg', '3551135685.jpg']` pair contains duplicated images carrying different labels. This is a great examples of the noissy labels that we have to deal with in this competition.","65c0cbae":"## Data access","1e99369c":"Save the abbreviated class names as follows:","05911345":"## TPU or GPU detection","79a1eba0":"## Loading libraries","1ea92cf1":"Find clusters with more than one element.","ef3e4619":"Let's take a quick look at one example of data:","5116fbd2":"We won't be training our model, so our `get_dataset` function is very simple.","15c40f77":"## Loading data","d9503e08":"Computing the embeddings and the corresponding image ID's.","120bf370":"## Extract image features\n\nTo extract image embeddings, we follow the procedure suggested by Chris Deotte in [one of his Melanoma competition public kernels](https:\/\/www.kaggle.com\/cdeotte\/rapids-cuml-knn-find-duplicates). The idea is very simple -- we add a `GlobalAveragePooling2D` layer to a pre-trained EfficientNet model and then send every image through the resulting neural net making predictions. The output of the `GlobalAveragePooling2D` layer is a very long vector wiht thousands of components. The exact number of these components depends on the type of the EfficientNet model. This vector will be used to represent the corresponding image. In other words, we will be using it as an embedding for the image. "}}