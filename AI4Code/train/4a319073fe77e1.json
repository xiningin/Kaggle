{"cell_type":{"73925b3a":"code","09221d53":"code","5813a7e5":"code","5fee44e1":"code","4cea6945":"code","57c9f133":"code","8d5d690c":"code","8eff6415":"code","a8b89c27":"code","3ccb8354":"code","e2f683bd":"code","723a4541":"code","dac756a4":"code","523f64a9":"markdown","b62d8ac9":"markdown","85322601":"markdown","b0132791":"markdown"},"source":{"73925b3a":"# 1. Prepare Problem\n# 1.a) Load libraries\nimport numpy as np\nfrom numpy import arange\nfrom matplotlib import pyplot as plt\nfrom pandas import read_csv\nfrom pandas import set_option\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.metrics import mean_squared_error\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# 1.b) Load dataset\nfilename = '\/kaggle\/input\/boston-house-prices\/housing.csv'\nnames = ['CRIM', 'ZN' , 'INDUS' , 'CHAS' , 'NOX' , 'RM' , 'AGE' , 'DIS' , 'RAD' , 'TAX' , 'PTRATIO' , 'B' , 'LSTAT' , 'MEDV' ]\ndataset = read_csv(filename, delim_whitespace=True, names=names)","09221d53":"# 2. Summarize Data\n# shape, type & head\nset_option('display.width', 160)\nset_option('precision', 6)\nprint('dimension :', dataset.shape,'\\nType :\\n', dataset.dtypes,'\\nHead :\\n', dataset.head(20))\n# 2.a) Descriptive statistics\n# summarizing the distribution of each attribute\nset_option('precision', 3)\nprint('Statistics :\\n', dataset.describe())","5813a7e5":"# correlation between attributes\nprint('Correlations :\\n', dataset.corr(method='pearson'))\n# many attributes have a strong correlation","5fee44e1":"# 2.b) Data visualizations\n# histogram of individual attributes\ndataset.hist(sharex = False, sharey = False,xlabelsize=1,ylabelsize=1,figsize=(18,12))\nplt.show()","4cea6945":"# density\ndataset.plot(kind='density', subplots=True,  layout=(4,4),figsize=(18,12), sharex=False, legend=True,fontsize=1)\nplt.show()","57c9f133":"# box and whisker plots\ndataset.plot(kind= 'box', subplots=True, layout=(4,4), sharex=False, sharey=False,fontsize=8,figsize=(18,12))\nplt.show()","8d5d690c":"# visualizations of the interactions between variables : scatter matrix\nscatter_matrix(dataset,figsize=(18,12))\nplt.show()","8eff6415":"# correlation matrix\nfig = plt.figure()\nax = fig.add_axes([0,0,2.5,2.5])\ncax = ax.matshow(dataset.corr(), vmin=-1, vmax=1, interpolation= 'none',cmap ='coolwarm')\nfig.colorbar(cax)\nticks = np.arange(0,14,1)\nax.set_xticks(ticks)\nax.set_yticks(ticks)\nax.set_xticklabels(names)\nax.set_yticklabels(names)\nplt.show()","a8b89c27":"# 3. Evaluate Algorithms\n# Split-out validation dataset\narray = dataset.values\nX = array[:,0:13]\nY = array[:,13]\ntest_size = 0.20\nseed = 7\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y,test_size=test_size, random_state=seed)\n# Standarize the data & spot-Check Algorithms\n# using pipelines to avoid data leakage when we transform the data\npipelines = []\npipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR', LinearRegression())])))\npipelines.append(('ScaledLASSO', Pipeline([('Scaler', StandardScaler()),('LASSO', Lasso())])))\npipelines.append(('ScaledEN', Pipeline([('Scaler', StandardScaler()),('EN', ElasticNet())])))\npipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsRegressor())])))\npipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeRegressor())])))\npipelines.append(('ScaledSVR', Pipeline([('Scaler', StandardScaler()),('SVR', SVR())])))\n# Test options and evaluation metric\nnum_folds = 10\nseed = 7\nscoring = 'neg_mean_squared_error'\n# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in pipelines:\n    kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n    \n# Compare Algorithms\nfig2 = plt.figure()\nax2 = fig2.add_axes([0,0,2,2])\nax2.boxplot(results, labels=names, showmeans=True, meanline=True, meanprops = dict(linestyle='--', linewidth=2.5, color='green'))\nax2.yaxis.grid(True)\nax2.set_title('Algorithm Comparison')\nplt.show()","3ccb8354":"# 4. Improve Accuracy\n# a) Algorithm Tuning : iterate on the nbr of neighbors \n# KNN Algorithm tuning\nk_values = np.array([1,3,5,7,9,11,13,15,17,19,21])\nparam_grid = dict(KNN__n_neighbors = k_values) # tunning parameter : n_neighbors\nmodel = Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsRegressor())])\nkfold = KFold(n_splits=num_folds, random_state=seed,shuffle=True)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\ngrid_result = grid.fit(X_train, Y_train)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))   ","e2f683bd":"# b) Ensembles\nseed2 = 8\nensembles = []\nensembles.append(('ScaledAB',Pipeline([('Scaler', StandardScaler()),('AB',AdaBoostRegressor(random_state=seed2))])))\nensembles.append(('ScaledGBM',Pipeline([('Scaler', StandardScaler()),('GBM',GradientBoostingRegressor(random_state=seed2))])))\nensembles.append(('ScaledRF',Pipeline([('Scaler', StandardScaler()),('RF',RandomForestRegressor(random_state=seed2))])))\nensembles.append(('ScaledET',Pipeline([('Scaler', StandardScaler()),('ET',ExtraTreesRegressor(random_state=seed2))])))\nresults = []\nnames = []\nfor name, model in ensembles:\n    kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n    \n# Compare Ensemble Algorithms\nfig3 = plt.figure()\nax3 = fig3.add_axes([0,0,2,2])\nax3.boxplot(results, labels=names, showmeans=True, meanline=True, meanprops = dict(linestyle='--', linewidth=2.5, color='green'))\nax3.yaxis.grid(True)\nax3.set_title('Scaled Ensemble Algorithm Comparison')\nplt.show()","723a4541":"# ET Algorithm tuning\nparam_grid = dict(ET__n_estimators = np.array([50,60,80,100,150,200,250,300])) # tunning parameter : n_estimators\nmodel = Pipeline([('Scaler', StandardScaler()),('ET', ExtraTreesRegressor(random_state=seed2))])\nkfold = KFold(n_splits=num_folds, random_state=seed,shuffle=True)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\ngrid_result = grid.fit(X_train, Y_train)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","dac756a4":"from sklearn.metrics import r2_score\n# 5. Finalize Model\n# a) Predictions on validation dataset\n# prepare the model : training the model on the entire training dataset\nsc = StandardScaler()\nrescaledX = sc.fit_transform(X_train)\nmodel = ExtraTreesRegressor(random_state=seed2, n_estimators=250)\nmodel.fit(rescaledX, Y_train)\n# transform the validation dataset\nrescaledTestX = sc.transform(X_test)\npredictions = model.predict(rescaledTestX)\nprint('- mean squared error: {}, r-squared: {}' .format(-mean_squared_error(Y_test, predictions), r2_score(Y_test, predictions)))","523f64a9":"The best for k (n_neighbors) is 1 providing a mean squared error of -19.493.","b62d8ac9":"We can see that KNN has both a tight distribution of error and has the lowest score, can it perform better more by parameter tunning? \n","85322601":"It looks like ExtraTrees has a better mean score, it also looks like GradientBoostingRegressor has a similar distribution and perhaps a better median score.\nTuning the ExtraTrees to further lift the performance.","b0132791":"We can see that the best con\ufb01guration was n estimators=250 resulting in a mean squared error of -8.995152."}}