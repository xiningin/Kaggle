{"cell_type":{"a48743b3":"code","efb5b57d":"code","18015c90":"code","c42bdc26":"code","b4dc4569":"code","9b4b3621":"code","d02bf306":"code","42cc9cba":"code","6780642e":"code","10794226":"code","da1243b2":"code","f2ab5a03":"code","1f47e049":"markdown","a4a45466":"markdown"},"source":{"a48743b3":"import glob\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom keras import Sequential\nfrom keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization, GaussianNoise\nfrom keras.applications import DenseNet121\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom sklearn.model_selection import KFold\nfrom keras.models import load_model","efb5b57d":"image_path = glob.glob('..\/input\/flowers-recognition\/flowers\/*\/*.jpg')\n\nlabels = glob.glob('..\/input\/flowers-recognition\/flowers\/*')\nlabels[1].split('\/')[4]\n\nlabel_to_index = dict((label.split('\/')[4], index) for index, label in enumerate(labels))\nindex_to_label = dict((index, name) for (name, index) in label_to_index.items())\n\nimage_labels = [label_to_index.get(label.split('\/')[4]) for label in image_path]\n\nnp.random.seed(2021)\n\nindex = np.random.permutation(len(image_path))\nimage_path = np.array(image_path)[index]\nimage_labels = np.array(image_labels)[index]","18015c90":"def argument_image(image, label):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    return image, label\n\n\ndef load_image(path, label):\n    image = tf.io.read_file(path)\n    image = tf.image.decode_jpeg(image)\n    image = tf.image.resize(image, (220, 220))\n    image = tf.cast(image, tf.float32)\n    image = image \/ 255\n    return image, label","c42bdc26":"AUTOTUNE = tf.data.experimental.AUTOTUNE","b4dc4569":"BATCH_SIZE = 32\n\ntrain_count = int(len(image_path) * 0.8)\ntest_count = len(image_path) - train_count\n\nkfold = KFold(n_splits=5, shuffle=False)\n              \nmodels = []\nhistorys = []\n              \nfor train, test in kfold.split(image_path, image_labels):\n    train_image = image_path[train]\n    train_label = image_labels[train]\n\n    test_image = image_path[test]\n    test_label = image_labels[test]\n\n\n    ds_train = tf.data.Dataset.from_tensor_slices((train_image, train_label))\n    ds_test = tf.data.Dataset.from_tensor_slices((test_image, test_label))\n\n    ds_train = ds_train.map(load_image, num_parallel_calls=AUTOTUNE).map(argument_image, num_parallel_calls=AUTOTUNE).repeat().shuffle(500).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n    ds_test = ds_test.map(load_image, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n              \n    conv_base = DenseNet121(weights='imagenet', include_top=False, input_shape=(220, 220, 3))\n    conv_base.trainable = True\n\n    model = Sequential()\n    model.add(conv_base)\n    model.add(Dropout(0.2))\n    model.add(GlobalAveragePooling2D())\n    model.add(Dense(128, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n    model.add(Dense(5, activation='softmax'))\n\n    model.compile(\n            optimizer=Adam(learning_rate=0.0001),\n            loss='sparse_categorical_crossentropy',\n            metrics=['acc']\n        )\n\n\n    step_per_epoch = train_count \/\/ BATCH_SIZE\n    validation_step = test_count \/\/ BATCH_SIZE\n\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1)\n    Early_sp = EarlyStopping(monitor = 'val_acc', patience = 5,restore_best_weights = True, verbose=1)\n\n    history = model.fit(\n        ds_train, \n        epochs=50,\n        steps_per_epoch=step_per_epoch, \n        validation_data=ds_test, \n        validation_steps=validation_step,\n        callbacks=[reduce_lr, Early_sp]\n    )\n              \n    models.append(model)\n    historys.append(history)\n    print('')\n    print('-------------------------------------------')\n    print('Model{} training is finished...'.format(len(models)))\n    print('-------------------------------------------')\n    print('')","9b4b3621":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc)+1)\n\nplt.plot(epochs, acc, '#21466C', label='Training acc')\nplt.plot(epochs, val_acc, '#ff0051', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, '#21466C', label='Training loss')\nplt.plot(epochs, val_loss, '#ff0051', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","d02bf306":"# Saliency Map\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimg = '..\/input\/flowers-recognition\/flowers\/rose\/*.jpg'\nfirst_img = glob.glob(img)[13]\n\n_img = keras.preprocessing.image.load_img(first_img,target_size=(220,220))\nplt.imshow(_img)\nplt.show()\n\nimg = keras.preprocessing.image.img_to_array(_img)\nimg = img.reshape((1, *img.shape))\ny_pred = model.predict(img)\n\nimages = tf.Variable(img, dtype=float)\n\nwith tf.GradientTape() as tape:\n    pred = model(images, training=False)\n    class_idxs_sorted = np.argsort(pred.numpy().flatten())[::-1]\n    loss = pred[0][class_idxs_sorted[0]]\n    \ngrads = tape.gradient(loss, images)\ndgrad_abs = tf.math.abs(grads)\ndgrad_max_ = np.max(dgrad_abs, axis=3)[0]\n\narr_min, arr_max  = np.min(dgrad_max_), np.max(dgrad_max_)\ngrad_eval = (dgrad_max_ - arr_min) \/ (arr_max - arr_min + 1e-18)\n\nfig, axes = plt.subplots(1,2,figsize=(14,5))\naxes[0].imshow(_img)\ni = axes[1].imshow(grad_eval,cmap=\"jet\",alpha=0.8)\nfig.colorbar(i)\n\nindex_to_label.get(4)","42cc9cba":"models = []\nfor i in range(5):\n    weights_file = '..\/input\/modelf\/models\/model' + str(i + 1) + '.h5'\n    model = load_model(weights_file)\n    models.append(model)","6780642e":"# Argument the predict image\nfrom keras.preprocessing.image import ImageDataGenerator\n\ndata_dir = '..\/input\/flowers-recognition\/flowers\/'\ntotal_length = len(image_path)\n\ntest_datagen = ImageDataGenerator(\n        rescale=1.\/255,\n        horizontal_flip = True,\n        vertical_flip = True\n)\n\nargumentation_times = 4\n\nfor index in range(argumentation_times):\n    print('{}th augmentation ...'.format(index))\n\n    test_generator = test_datagen.flow_from_directory(\n            data_dir,\n            target_size=(220, 220),\n            batch_size=32,\n            shuffle = False,\n            classes = None,\n            class_mode = None\n    )\n    \n    if index == 0:\n        count = 0\n        for model in models:\n            if count == 0:\n                predictions = model.predict(test_generator, total_length)\n            else:\n                predictions += model.predict(test_generator, total_length)\n            count += 1\n    else:\n        for model in models:\n            predictions += model.predict(test_generator, total_length)\n \npredictions = predictions \/ (argumentation_times * len(models))\npredictions = [np.argmax(prediction) for prediction in predictions]","10794226":"dandelion_array = [0 for i in range(1052)]\ndaisy_array = [1 for i in range(764)]\nsunflower_array = [2 for i in range(733)]\ntulip_array = [3 for i in range(984)]\nrose_array = [4 for i in range(784)]\n\ntrue_label = daisy_array + dandelion_array + rose_array + sunflower_array + tulip_array\n\ntotal_count = len(predictions)\ncorrect_count = 0\n\nfor i in range(total_count):\n    if predictions[i] == true_label[i]:\n        correct_count += 1\n        \nacc = correct_count \/ total_count\nacc","da1243b2":"count=1\nfor model in models:\n    file_name='model' + str(count) + '.h5'\n    model.save(file_name)\n    count += 1","f2ab5a03":"total_count","1f47e049":" # Load Model","a4a45466":"# **Prediction**"}}