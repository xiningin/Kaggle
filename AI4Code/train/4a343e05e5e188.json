{"cell_type":{"314649ef":"code","3d4e97d3":"code","4975bd1d":"code","9c603202":"code","a66b59a5":"code","b533c9ba":"code","d9df0ee2":"code","7ec9a8c4":"code","cf34c8b8":"code","d7cc2a62":"code","65803b48":"code","aa58b24c":"code","db474f3e":"code","81bfc96a":"code","afe6b6cd":"code","e1729a60":"code","2c8949bd":"code","840a2c73":"code","4da9fa14":"code","9c21369e":"code","1979aee4":"code","1b94be7b":"code","4bbf2aa0":"code","35689b86":"code","fe01e6b4":"code","bdc2fb39":"code","a7866228":"code","a76bd9c0":"code","38bb5a3b":"code","2ad5d671":"code","7401155a":"markdown","966f2658":"markdown","a1f2d267":"markdown","4518a245":"markdown","a316a7c6":"markdown","e87bab28":"markdown","e3758ef8":"markdown","c63fa78f":"markdown","6253ff66":"markdown","cdb5b1e4":"markdown","a839ef5c":"markdown","c5056586":"markdown","a7fa80e7":"markdown","1e2d4f22":"markdown","3c521699":"markdown","37db782a":"markdown","e356743e":"markdown","75cfcd2b":"markdown","cee43fcc":"markdown","ddecf6de":"markdown","0e8aa641":"markdown","d574b54b":"markdown","9d468c6b":"markdown","4462a1de":"markdown","fe45e99e":"markdown","1a58e3f8":"markdown"},"source":{"314649ef":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport time\nimport datetime\n\n#matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.pylab import rcParams\n%matplotlib inline\nrcParams['figure.figsize'] = 15, 6\n\n#seaborn\nimport seaborn as sns\n\n#ploty\nimport plotly\nimport plotly.express as px\nplotly.offline.init_notebook_mode(connected=True)\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\n#bokeh\nimport holoviews as hv\nfrom holoviews import opts, dim\n\n#sklearn\nfrom sklearn.preprocessing import MinMaxScaler\n\n#statsmodels\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import acf\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.arima_model import ARMA\nfrom statsmodels.tsa.arima_process import ArmaProcess\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n!pip install pmdarima\nfrom pmdarima.arima import auto_arima","3d4e97d3":"prices_split_adjusted = pd.read_csv('\/kaggle\/input\/nyse\/prices-split-adjusted.csv',index_col='date', parse_dates=['date'])\nprices_split_adjusted.head()\n#prices_split_adjusted.info()\n#prices_split_adjusted.describe()","4975bd1d":"#Removing data with less than 1762 entries\nsymbol = pd.DataFrame(prices_split_adjusted['symbol'].value_counts())\nomit = symbol[symbol['symbol'] < 1762].index\nprices_split_adjusted = prices_split_adjusted[~prices_split_adjusted['symbol'].isin(omit)]\n\n#Working with just amazon stock data\nstock_name = 'AAPL'\nstock = prices_split_adjusted[prices_split_adjusted['symbol'] == stock_name]\nstock.describe()","9c603202":"fig = px.line(stock, y=\"close\", title=stock_name+' Close Value')\nfig.show()","a66b59a5":"stock_returns = stock['close'].pct_change()\nfig = px.line(stock_returns, title=stock_name+' Stocks returns')\nfig.show()","b533c9ba":"#rolling mean\nplt.figure(figsize=(15,5))\nplt.subplot(1,1,1)\nstock_rolling = stock['close'].rolling(window=60,min_periods=5).mean()\nstock['close'].plot()\nstock_rolling.plot()\nplt.legend(['close','Rolling Mean'])\nplt.show()","d9df0ee2":"#expanding\nplt.figure(figsize=(15,5))\nplt.subplot(1,1,1)\nstock_expanding = stock['close'].expanding(min_periods=5,center=True).mean()\nstock['close'].plot()\nstock_expanding.plot()\nplt.legend(['close','Expanding'])\nplt.show()","7ec9a8c4":"# Coorelation code\n# timestamp1 = time.time()\n# symbols = prices_split_adjusted['symbol'].unique()[0:5]\n# corr = [[0]*len(symbols) for i in range(len(symbols))]\n# for i in range(len(symbols)):\n#     stock1_returns = prices_split_adjusted.loc[prices_split_adjusted['symbol'] == symbols[i],'close'].pct_change()\n#     for j in range(len(symbols)):\n#         stock2_returns = prices_split_adjusted.loc[prices_split_adjusted['symbol'] == symbols[j],'close'].pct_change()\n#         corr[i][j] = stock1_returns.corr(stock2_returns)\n        \n# timestamp2 = time.time()\n# print(\"This took %.2f seconds\" % (timestamp2 - timestamp1))\n# np.savetxt(\"correlationmatrix.csv\", corr, delimiter=\",\")\n# sns.heatmap(corr,xticklabels=symbols,yticklabels=symbols)\n# plt.savefig('testplot.png')","cf34c8b8":"nyse = pd.read_csv('..\/input\/nyse1216cm\/new_york_stock_exchange_2012_2016_correlation_matrix.csv')\nnyse.set_index('idx',inplace=True)\n\nHighCorrelation_threshold = 0.85\nhighcorr = []\nsymbols = nyse.index\n\nfor i in range(len(nyse)):\n    for k in range(len(nyse)):\n        if (nyse.loc[symbols[i],symbols[k]] > HighCorrelation_threshold) & (symbols[i] != symbols[k]) :\n            highcorr.append([symbols[i],symbols[k]])\n            \nhighcorrdf = pd.DataFrame(highcorr,columns=['source','target'])\nhighcorrdf['value'] = 1\n\ndf_nodes_ = pd.DataFrame([pd.unique(highcorrdf[[\"source\", \"target\"]].values.ravel())], [\"source\"]).T\ndf_nodes_[\"name\"] = df_nodes_[\"source\"]\ndf_nodes_.set_index(df_nodes_[\"source\"].values, inplace = True)\ndf_nodes_.drop(\"source\", axis = 1, inplace = True)\ndf_nodes = hv.Dataset(pd.DataFrame(df_nodes_['name']), 'index')\n\nhv.extension('bokeh')\nhv.output(size = 200)\nchord = hv.Chord((highcorrdf, df_nodes)).select(value = (1, None))\nchord.opts(\n    opts.Chord(\n            cmap = 'Category20',  # select color map\n            edge_cmap = 'Category20', # select color map\n            edge_color = dim('source').str(), # generate unique colors for edges\n            labels = 'name', # use this column from the df_nodes df as the edge name\n            node_color = dim('index').str() # generate unique colors for edges\n    )\n)","d7cc2a62":"sns.kdeplot(stock['close'],shade=True)","65803b48":"#Dicky fuller test to see if the returns is a random walk\n\nstock_returns = stock['close'].pct_change()\nstock_returns = stock_returns.dropna()\nresults = adfuller(stock_returns)\nprint(results)\nprint('The p-value of the test on prices is: ' + str(results[1]))","aa58b24c":"#autocorrelation\nprint(stock_returns.autocorr())\nweekly_returns = stock.resample(rule='W').mean()['close'].pct_change()\nprint(weekly_returns.autocorr())\nmonthly_returns = stock.resample(rule='M').mean()['close'].pct_change()\nprint(monthly_returns.autocorr())\nyearly_returns = stock.resample(rule='A').mean()['close'].pct_change()\nprint(yearly_returns.autocorr())\n#Negative autocorrelation meaning its mean reverting","db474f3e":"#acf\n#https:\/\/towardsdatascience.com\/significance-of-acf-and-pacf-plots-in-time-series-analysis-2fa11a5d10a8\nplot_acf(stock.resample(rule='W').mean()['close'],alpha=0.05,lags=25)\nplt.show()","81bfc96a":"#pacf\nplot_pacf(stock.resample(rule='W').mean()['close'],alpha=0.05,lags=25)\nplt.show()","afe6b6cd":"# Auto Regressive Moving Average\n#AR(1)\namzn = stock.copy()\namzn['close'].index = pd.DatetimeIndex(amzn['close'].index).to_period('D')\naic = []\nbic = []\nresult = []\nfor i in range(1,7):\n    mod = ARMA(amzn['close'], order=(i,0))\n    res = mod.fit()\n    aic.append(res.aic)\n    bic.append(res.bic)\n    result.append(res)\n\nbest_bic = bic.index(min(bic))+1\nbest_aic = aic.index(min(aic))+1\nprint('best value for p with respect to BIC : %10.3f' % best_bic)\nprint('best value for p with respect to AIC : %10.3f' % best_aic)\nprint(result[best_bic-1].summary())\nprint(result[best_bic-1].params)","e1729a60":"fig1, ax2 = plt.subplots()\nax2 = amzn.loc['2016','close'].plot(ax=ax2)\n\nprediction_start_index = amzn.index[-1]+pd.DateOffset(weeks=0)\nprediction_end_index = amzn.index[-1]+pd.DateOffset(weeks=20)\n\nfig1 = result[best_bic-1].plot_predict(start=amzn.index[-1]-pd.DateOffset(weeks=10), end=amzn.index[-1], ax=ax2, plot_insample=False)#amzn.index[-1]-pd.DateOffset(weeks=4)\n\npredictions = result[best_bic-1].forecast(steps=20,alpha=0.05)\nprediction_index = pd.date_range(start=prediction_start_index,end=prediction_end_index,freq=\"W\")\n\npred = pd.DataFrame({'predicted':predictions[0],\n                     'lower':[predictions[2][i][0] for i in range(len(predictions[2]))],\n                     'upper':[predictions[2][i][1] for i in range(len(predictions[2]))]},\n                    index=prediction_index)\n\nfig1 = pred['predicted'].plot(ax=ax2,color=\"green\",label='predictions')\nplt.fill_between(pred.index,pred['lower'],pred['upper'],color=\"#CECECE\",label='Confidence Interval')\nrcParams[\"legend.loc\"] = 'lower right'\nplt.legend()\nplt.show()","2c8949bd":"#MA model\naic = []\nbic = []\nbic_aic_sum = []\nresult = []\nfor i in range(1,7):\n    newarray = amzn['close']-amzn['close'].shift(i)\n    ma_mod = ARMA(newarray.dropna(), order=(0,i))\n    ma_res = ma_mod.fit()\n    aic.append(ma_res.aic)\n    bic.append(ma_res.bic)\n    bic_aic_sum.append(ma_res.aic + ma_res.bic)\n    result.append(ma_res)\n\nbest_idx = bic_aic_sum.index(min(bic_aic_sum))+1\nprint('best value for q with respect to BIC & AIC: %10.3f' % (best_idx))\nprint(result[best_idx-1].summary())\nprint(result[best_idx-1].params)","840a2c73":"fig2, ax3 = plt.subplots()\nax2 = amzn.loc['2016','close'].plot(ax=ax3)\n\nprediction_start_index = amzn.index[-1]+pd.DateOffset(weeks=0)\nprediction_end_index = amzn.index[-1]+pd.DateOffset(weeks=20)\n\nfig2 = result[best_bic-1].plot_predict(start=amzn.index[-1]-pd.DateOffset(weeks=10), end=amzn.index[-1], ax=ax3, plot_insample=False)#amzn.index[-1]-pd.DateOffset(weeks=4)\n\npredictions = result[best_bic-1].forecast(steps=20,alpha=0.05)\nprediction_index = pd.date_range(start=prediction_start_index,end=prediction_end_index,freq=\"W\")\n\npred = pd.DataFrame({'predicted':predictions[0],\n                     'lower':[predictions[2][i][0] for i in range(len(predictions[2]))],\n                     'upper':[predictions[2][i][1] for i in range(len(predictions[2]))]},\n                    index=prediction_index)\n\nfig2 = pred['predicted'].plot(ax=ax3,color=\"green\",label='predictions')\nplt.fill_between(pred.index,pred['lower'],pred['upper'],color=\"#CECECE\",label='Confidence Interval')\nrcParams[\"legend.loc\"] = 'lower right'\nplt.legend()\nplt.show()","4da9fa14":"#Combining both AR and MA model\n\nbic_aic_sum = []\nresult = []\nfor i in range(1,3):\n    for k in range(1,3):\n        newarray = amzn['close']-amzn['close'].shift(k)\n        ma_mod = ARMA(newarray.dropna(), order=(i,k))\n        ma_res = ma_mod.fit()\n        result.append({'aic':ma_res.aic,'bic':ma_res.bic,'bic_aic_sum':ma_res.aic+ma_res.bic,'res':ma_res,'ar_index':i,'ma_index':k})\n        ","9c21369e":"result","1979aee4":"print(result[0]['res'].summary())\nprint(result[0]['res'].params)","1b94be7b":"fig3, ax4 = plt.subplots()\nax4 = amzn.loc['2016','close'].plot(ax=ax4)\n\nprediction_start_index = amzn.index[-1]+pd.DateOffset(weeks=0)\nprediction_end_index = amzn.index[-1]+pd.DateOffset(weeks=20)\n\nfig3 = result[0]['res'].plot_predict(start=amzn.index[-1]-pd.DateOffset(weeks=10), end=amzn.index[-1], ax=ax4, plot_insample=False)#amzn.index[-1]-pd.DateOffset(weeks=4)\n\npredictions = result[0]['res'].forecast(steps=20,alpha=0.05)\nprediction_index = pd.date_range(start=prediction_start_index,end=prediction_end_index,freq=\"W\")\n\npred = pd.DataFrame({'predicted':predictions[0],\n                     'lower':[predictions[2][i][0] for i in range(len(predictions[2]))],\n                     'upper':[predictions[2][i][1] for i in range(len(predictions[2]))]},\n                    index=prediction_index)\n\nfig3 = pred['predicted'].plot(ax=ax4,color=\"green\",label='predictions')\nplt.fill_between(pred.index,pred['lower'],pred['upper'],color=\"#CECECE\",label='Confidence Interval')\nrcParams[\"legend.loc\"] = 'lower right'\nplt.legend()\nplt.show()","4bbf2aa0":"#https:\/\/medium.com\/@sigmundojr\/seasonality-in-python-additive-or-multiplicative-model-d4b9cf1f48a7\nplt.rcParams.update({'figure.figsize': (15,10)})\nmultiplicative = sm.tsa.seasonal_decompose(stock[\"close\"].to_frame(),freq=360,model='multiplicative')\nmultiplicative.plot().suptitle('Multiplicative Decompose', fontsize=22)\nplt.show()\n\nadditive = sm.tsa.seasonal_decompose(stock[\"close\"].to_frame(),freq=360,model='additive')\nadditive.plot().suptitle('Additive Decompose', fontsize=22)\nplt.show()\nplt.rcParams.update({'figure.figsize': (15,5)})","35689b86":"#Checking if the residuals follow a normal distribution and there is no pattern, comparing it with generated normal distribution\n\nresiduals = multiplicative.resid\nnormal_distribution = np.random.normal(loc=multiplicative.resid.mean(), scale=multiplicative.resid.std(),size=len(multiplicative.resid))\n\nimport plotly.graph_objects as go\n\nhistfig = go.Figure()\nhistfig.add_trace(go.Histogram(x=residuals))\nhistfig.add_trace(go.Histogram(x=normal_distribution))\n\nhistfig.update_layout(barmode='overlay')\nhistfig.update_traces(opacity=0.5)\nhistfig.show()","fe01e6b4":"#Seasonal Autocorrelation\namzn_seasonal = multiplicative.seasonal\n\n#acf\nplot_acf(amzn_seasonal.resample(rule='W').mean(),alpha=0.05,lags=25)\nplt.show()","bdc2fb39":"#seasonal pacf\nplot_pacf(amzn_seasonal.resample(rule='W').mean(),alpha=0.05,lags=25)\nplt.show()","a7866228":"amzn_m2 = amzn.copy()\namzn_m2['mean'] = (amzn_m2['low'] + amzn_m2['high'])\/2\n\nsteps=-1\namzn_m2['actual']=amzn_m2['mean'].shift(steps)\namzn_m2.dropna(inplace=True)\n\nsc_in = MinMaxScaler(feature_range=(0, 1))\nscaled_input = sc_in.fit_transform(amzn_m2[['low', 'high','open', 'close', 'volume']])\nX = pd.DataFrame(scaled_input)\n\nsc_out = MinMaxScaler(feature_range=(0, 1))\nscaler_output = sc_out.fit_transform(amzn_m2['actual'].values.reshape(-1,1))\ny = pd.DataFrame(scaler_output)\n\nsplit_size = 0.7\ntrain_size=int(len(amzn['close']) * split_size)\ntest_size = int(len(amzn['close'])) - train_size\ntrain_X, train_y = X[:train_size], y[:train_size]\ntest_X, test_y = X[train_size:], y[train_size:]\n\nstep_wise = auto_arima(train_y, exogenous= train_X,\n                       start_p=1, start_q=1, \n                       max_p=7, max_q=7, d=1, max_d=7, \n                       trace=True, error_action='ignore', suppress_warnings=True, stepwise=True) #seasonal=True\/ trend='c'\n","a76bd9c0":"from statsmodels.tsa.statespace.sarimax import SARIMAX\n\n#d=7 only 7th order differencing\nmodel = SARIMAX(train_y, exog=train_X[3], order=(1,1,3),seasonal_order=(1,0,0,12),seasonal=True, enforce_invertibility=False, enforce_stationarity=False)\n\nsarimax_results = model.fit()\n\npredictions = sarimax_results.predict(start=train_size, end=train_size+test_size+(steps)-1, exog=test_X[3])\n\nforecast = sarimax_results.forecast(steps=test_size-1, exog=test_X[3])\n","38bb5a3b":"#model evaluation\nfrom statsmodels.tools.eval_measures import rmse\nerror=rmse(predictions.values, test_y[0].values)\nerror","2ad5d671":"\nfig4 = plt.figure(figsize=(20,5))\nax5 = plt.plot(amzn.index[0:-1],X[3])\n\nprediction_start_index = amzn.index[-1]+pd.DateOffset(weeks=0)\nprediction_end_index = amzn.index[-1]+pd.DateOffset(weeks=20)\n\ntest_predictions = sarimax_results.predict(start=train_size, end=train_size+test_size+(steps)-1, exog=test_X[3])#amzn.index[-1]-pd.DateOffset(weeks=4)\nax5 = plt.plot(amzn.index[train_size:train_size+test_size+(steps)],test_predictions.values,color=\"orange\",label=\"training\")\n\nprediction_index = pd.date_range(start=prediction_start_index,end=prediction_end_index,freq=\"W\")\npredictions = sarimax_results.forecast(steps=20,exog=test_X[3][0:20])\nfcast = sarimax_results.get_forecast(20,exog=test_X[3][0:20])\nconfidence_intervals = fcast.conf_int()\n\npred = pd.DataFrame({'predicted':predictions.values,\n                     'lower':confidence_intervals['lower y'].values,\n                     'upper':confidence_intervals['upper y'].values},\n                    index=prediction_index)\n\nax5 = plt.plot(pred.index,pred['predicted'],color=\"green\",label='predictions')\nplt.fill_between(pred.index,pred['lower'],pred['upper'],color=\"#CECECE\",label='Confidence Interval')\nrcParams[\"legend.loc\"] = 'lower right'\nplt.legend()\nplt.show()\n","7401155a":"<a id=\"connect-seasonalacf\"><\/a>\n### Seasonal ACF","966f2658":"<a id='connect-7'><\/a>\n### Amazon stock returns","a1f2d267":"# Table of contents\n\n- [Objective](#connect-1)\n- [Future Work](#connect-2)\n- [Import Libraries](#connect-3)\n- [Data Cleaning](#connect-4)\n- [Exploratory Data Analysis](#connect-5)\n    - [Amazon Stock Prices](#connect-6)\n    - [Amazon Stock Returns](#connect-7)\n    - [Rolling mean](#connect-8)\n    - [Expanding](#connect-9)\n    - [Highly Correlated Stocks](#connect-10)\n    - [Correlation vs Cointegration](#connect-11)\n    - [Stock Density Distribution](#connect-12)\n- [Modeling](#connect-13)\n    - [Agumented Dicky Fuller Test](#connect-14)\n    \n    - [Autocorrelation](#connect-16)\n    - [ACF](#connect-17)\n    - [PACF](#connect-18)\n    - [ARMA](#connect-19)\n    - [AR Model](#connect-20)\n    - [MA Model](#connect-21)\n    - [Combining AR and MA](#connect-22)\n    - [Seasonal Decompose](#connect-15)\n    - [Residual Test](#connect-residualtest)\n    - [Seasonal ACF](#connect-seasonalacf)\n    - [Seasonal PACF](#connect-seasonalpacf)\n    - [Auto Arima](#connect-autoarima)\n    - [SARIMAX](#connect-sarimax)\n- [Conclusion](#connect-23)\n\n\n<div style=\"background-color: lightgrey; padding: 10px 10px 10px 10px;\"><p>This kernel is a work in progress. If you like my work, be sure to upvote this kernel so it looks more relevant and meaningful to the community.<\/p><\/div>\n\n\n<br>\n\n<a id='connect-1'><\/a>\n### Objective:\n\nOne day ahead prediction: Rolling Linear Regression, ARMA.\n<a id='connect-2'><\/a>\n### Future work\nNeural Networks, LSTM\nMomentum\/Mean-Reversion Strategies\nSecurity clustering, portfolio construction\/hedging\nWhich company has biggest chance of being bankrupt? Which one is undervalued (how prices behaved afterwards), what is Return on Investment?\n<a id='connect-3'><\/a>\n### Import libraries","4518a245":"With the params being const = 415.954903 & ar.L1.close = 0.999653, our model equation is<br><br>\nRt = \u03bc + \u03d5Rt-1 + \u03b5t<br><br>\nModel:<br>\n<div style=\"background-color: rgb(0, 253, 206); padding: 10px 10px 10px 10px;\"><p>\nRt(Today's returns) = 415.954903(Overall Mean) + 0.999653*(Yesterday's return) + Noise<\/p><\/div><br>\nThis is while using only the autoregressive term. We can add mean average terms too.<br><br>\n\n<a id='connect-21'><\/a>\n### MA Model","a316a7c6":"<a id='connect-5'><\/a>\n## Exploratory Data Analysis","e87bab28":"<a id='connect-16'><\/a>\n### Autocorrelation","e3758ef8":"<a id='connect-12'><\/a>\n### Stock Density Distribution","c63fa78f":"<a id='connect-4'><\/a>\n### Data Cleaning","6253ff66":"p-value is 0, for AMZN stock, so we can reject the hypothesis that the returns follow a random walk. Also we can reject the hypothesis that the series is non-stationary. Returns follow a stationary time series. A stationary time series is required for ARMA models to work. Stationary being constant mean and variance. Taking log or diff() transformations can make the series stationary.","cdb5b1e4":"<a id='connect-17'><\/a>\n### ACF","a839ef5c":"Seasonal ACF tails off and seasonal pacf cuts off at p=1. So seasonal order (1,0,0) makes sense","c5056586":"<a id='connect-23'><\/a>\n### Conclusion:\n\nSARIMAX(1,1,3)(1,0,0,12)\n\n<div style=\"background-color: lightgrey; padding: 10px 10px 10px 10px;\"><p>This kernel is a work in progress. If you like my work, be sure to upvote this kernel so it looks more relevant and meaningful to the community.<\/p><\/div><br><br>","a7fa80e7":"<a id='connect-15'><\/a>\n### Seasonal Decompose\n\nThe additive model is Y[t] = T[t] + S[t] + e[t]\n\nThe multiplicative model is Y[t] = T[t] * S[t] * e[t]","1e2d4f22":"<a id='connect-9'><\/a>\n### Expanding","3c521699":"<a id='connect-10'><\/a>\n### Highly Correlated stocks","37db782a":"#### Correlation dosen't mean causation.\n<a id='connect-11'><\/a>\n### Correlation vs cointegration\n\nhttps:\/\/medium.com\/analytics-vidhya\/cointegration-for-time-series-analysis-1d0a758a20f1\nhttps:\/\/corporatefinanceinstitute.com\/resources\/knowledge\/other\/cointegration\/\n","e356743e":"<a id='connect-13'><\/a>\n# Modeling\nAutocorrelation represents the degree of similarity between a given time series and a lagged version of itself over successive time intervals. Autocorrelation measures the relationship between a variable's current value and its past values.\n\nMean reversion is synonymous with negative auto- or serial-correlation; i.e., mean reversion is the \"opposite\" of positive autocorrelation of returns. For example, if the return today is +3%, then positive autocorrelation implies a higher probability of another high return tomorrow while mean reversion (negative serial correlation) implies a higher probability of a low\/negative return tomorrow.\n<a id='connect-14'><\/a>\n### Augumented Dicky Fuller test","75cfcd2b":"<a id=\"connect-autoarima\"><\/a>\n### Auto Arima","cee43fcc":"From the result, its clear that the lowest aic and bic is for ar parameter p=1 and ma parameter q=1.<br><br>\nRt = \u03bc + \u03d5Rt-1 + \u03f5t + \u03b8\u03f5t-1<br>\n<br>Model:<br>\n<div style=\"background-color: rgb(0, 253, 206); padding: 10px 10px 10px 10px;\"><p>\nRt(Today's returns) = 0.349746(Overall mean of returns) + 0.030264*Rt-1(Yesterday's Returns) + (Today's Noise) -0.025727*\u03f5t-1(Yesterday's Noise)<\/p><\/div><br>","ddecf6de":"MA model won't work as we saw earlier that PACF cuts of at p=1 and ACF tails off. But including the code here as it could be needed for other stocks.\nWith constant coef being 0.349746 and the one and only ma term's coef being 0.004471, the following is the equation.<br><br>\nRt = \u03bc + \u03f5t1 + \u03b8\u03f5t-1<br><br>\nModel:<br>\n<div style=\"background-color: rgb(0, 253, 206); padding: 10px 10px 10px 10px;\"><p>\nRt (Today's returns) = 0.349746(general mean) + (Today's Noise) + 0.004471*(Yesterday's Noise)<\/p><\/div><br>\n\n<a id='connect-22'><\/a>\n### Combining AR and MA","0e8aa641":"<a id='connect-8'><\/a>\n### Rolling mean","d574b54b":"<a id='connect-6'><\/a>\n### Amazon stock prices","9d468c6b":"<a id='connect-18'><\/a>\n### Partial ACF","4462a1de":"\nACF amplitude tails off and PACF amlitude cuts off, which means a AR model would best describe the time series, specifically AR(1) model as the PACF cuts off at 1. If ACF cuts off and PACF tails off, the time series would be best defined by a MA model.<br>\n<a id='connect-19'><\/a>\n### ARMA\n\nTo know more about it visit: http:\/\/people.duke.edu\/~rnau\/411arim.htm\n<br>\nAIC vs BIC\nhttps:\/\/www.methodology.psu.edu\/resources\/AIC-vs-BIC\/\n#https:\/\/people.duke.edu\/~rnau\/411arim3.htm\n<a id='connect-20'><\/a>\n### AR Model","fe45e99e":"<a id=\"connect-residualtest\"><\/a>\n### Residual test","1a58e3f8":"<a id=\"connect-seasonalpacf\"><\/a>\n### Seasonal PACF"}}