{"cell_type":{"462f06fd":"code","0cebe1ac":"code","9f714425":"code","582a6305":"code","9b90cf1c":"code","56fb0a6a":"code","99dfcd0c":"code","df7e0101":"code","5217c1c4":"code","834911b0":"code","5d2c9232":"code","7280cd3f":"code","82cd77bd":"code","9678e6ee":"code","6562c7b4":"code","9f5a5826":"code","d6122779":"code","64dccb46":"code","e963bd7f":"code","2e1282e8":"code","e9717af3":"code","356ceab4":"code","9b6f8a38":"code","d5b019a1":"code","07220bfe":"code","b2fb590b":"code","a6d45011":"code","9b2c12e0":"code","91fcf302":"code","69b0d35b":"code","3b533722":"code","70ae97f4":"code","92524977":"code","dcd14123":"code","d2873b59":"code","3709d3c2":"code","46922c34":"code","d1035a18":"code","6bf407f7":"code","5b3a60ab":"code","dffedcf7":"code","11d15397":"code","742e7fcb":"code","c77c7f93":"code","9c6eb05c":"code","2fd1d30a":"code","4578ae5c":"code","5697cec9":"code","88671bd0":"code","874cf8df":"code","52f9fccb":"code","7b408f63":"code","21c12396":"code","facf4e58":"code","e3c7689f":"code","e08142db":"code","8db4cd6b":"markdown","fc967e51":"markdown","062e6a9e":"markdown","aa19e75f":"markdown","8cd1fe6c":"markdown","a006cfda":"markdown","7771e808":"markdown","a6b2a21e":"markdown","d4889402":"markdown","6e19323c":"markdown","5ada0e25":"markdown","32495c9e":"markdown","d4fcbd6e":"markdown","6118c020":"markdown","fa945b17":"markdown"},"source":{"462f06fd":"import pandas as pd \nimport numpy as np \nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom pprint import pprint \nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV","0cebe1ac":"#Used iexcloud api to get the stock prices of AAPL.\n#You may need to edit this path as per your dataset path. \ndf = pd.read_csv('..\/input\/stocks\/AAPL2.csv') ","9f714425":"df.tail(5)","582a6305":"df.date.dtype","9b90cf1c":"s = pd.to_datetime(df['date'])\ndf['date'] = s.dt.strftime(\"%m\")          #This returns 2020-10-2 as 202010 (YearMonth)","56fb0a6a":"df.date = df.date.astype('int64')\ndf.date.dtype","99dfcd0c":"df.tail(5)","df7e0101":"drop_columns = ['volume', 'uOpen', 'uClose', 'uHigh', 'uLow', \n                'uVolume', 'change', 'changePercent', 'label', 'changeOverTime', 'Unnamed: 0']\ndf.drop(drop_columns, axis = 1, inplace = True)\ndf.rename(columns = {'date':'YearMonth'})","5217c1c4":"display(df.info())\ndisplay(df.head())","834911b0":"corr = abs(df.corr())\nsns.heatmap(corr)","5d2c9232":"df.date = (df.date-df.date.mean())\/df.date.std()        #Normalizing the YearMonth column","7280cd3f":"Y = df.open                                          #Separating the taget variable from the features \ndf.drop('open', axis = 1, inplace = True)\nX = df","82cd77bd":"#Plotting history of opening prices for AAPL\nplt.figure(figsize=(15,8))\nplt.plot(Y, color=\"black\")\nplt.title('Opening Price history')\nplt.ylabel('Opening Price')\nplt.xlabel('Days')\nplt.show()","9678e6ee":"xtrain, xtest, ytrain, ytest = train_test_split(X, Y, random_state = 14, test_size = .2)","6562c7b4":"models = [KNeighborsRegressor(), GradientBoostingRegressor(),\n          LinearRegression(), SVR(), DecisionTreeRegressor()]\nfor model in models:\n    kfold = KFold(n_splits=5, random_state= 14, shuffle=True)\n    result = cross_val_score(model, xtrain, ytrain, \n                             cv=kfold, scoring = 'r2')\n    print(\"For\", model, \"the r2 score is\",result)","9f5a5826":"lr = LinearRegression(normalize=True, copy_X=True)\nlr.fit(xtrain,ytrain)\npredict = lr.predict(X)","d6122779":"lr.coef_","64dccb46":"from sklearn import metrics\nprint('R^2:',metrics.r2_score(Y, predict))","e963bd7f":"fig, ax1 = plt.subplots(figsize=(13,6))\nax1.set_xlabel('Days')\nax1.set_ylabel('Actual Price')\nax1.plot(Y, color = 'black')\n\nax2 = ax1.twinx()\nax2.set_ylabel('Predicted Price', color ='red')\nax2.tick_params(axis='y', colors='red')\nax2.plot(predict, color = 'red')\nplt.show()","2e1282e8":"gbr = GradientBoostingRegressor()\npprint(gbr.get_params())                    #Getting the hyperparameters which can be tuned for a better model","e9717af3":"random_grid = {'n_estimators': [25,50,100,150,200,300],\n               'max_depth': [3,5,10,15],\n               'min_samples_leaf': [2, 3, 4],\n               'learning_rate' : [.01,.03,.3,1],\n               'criterion' : ['friedman_mse','mse']}\npprint(random_grid)","356ceab4":"gbr_random = RandomizedSearchCV(estimator = gbr, param_distributions = random_grid,\n                                n_iter = 100, cv = 5, random_state=14, verbose = 0)","9b6f8a38":"gbr_random.fit(xtrain, ytrain)","d5b019a1":"gbr_random.best_params_                     #Best parameters found by the RandomSearchCV","07220bfe":"grid_param = {'n_estimators': [150,200,100],\n               'max_depth': [10,15,17,20],\n               'min_samples_leaf': [3, 4,5],\n               'learning_rate' : [.1,0.1,0.3],\n               'criterion' : ['friedman_mse']}\npprint(grid_param)","b2fb590b":"grid_search =  GridSearchCV(estimator = gbr, param_grid = grid_param, \n                          cv = 3, verbose = 0)","a6d45011":"grid_search.fit(xtrain, ytrain)","9b2c12e0":"grid_search.best_params_                     #These are the best parameters for the Gradient Boosting Regressor ","91fcf302":"best_params = grid_search.best_params_ ","69b0d35b":"gbr_final = GradientBoostingRegressor(n_estimators=best_params['n_estimators'], \n                               criterion=best_params['criterion'], learning_rate = best_params['learning_rate'], \n                               max_depth=best_params['max_depth'], min_samples_leaf=best_params['min_samples_leaf'])","3b533722":"gbr_final.fit(xtrain,ytrain)","70ae97f4":"predict = gbr_final.predict(X)","92524977":"from sklearn import metrics\nprint('R^2:',metrics.r2_score(Y, predict))","dcd14123":"fig, ax1 = plt.subplots(figsize=(12,6))\nax1.set_xlabel('Days')\nax1.set_ylabel('Actual Prices')\nax1.plot(Y, color = 'black')\n\nax2 = ax1.twinx()\nax2.set_ylabel('Predicted Price', color ='red')\nax2.plot(predict, color = 'red')\nax2.tick_params(axis='y', colors='red')\nplt.show()","d2873b59":"import math\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.layers import *\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom keras.callbacks import EarlyStopping\nfrom keras.optimizers import Adam","3709d3c2":"df = pd.read_csv('..\/input\/stocks\/MSFT.csv')        #Stock prices from 2015-11-25 to 2020-11-23. 5 years. \ndf= df['Close']                     #Using Close prices for predictions        ","46922c34":"plt.plot(df)\nplt.xlabel(\"TIME INTERVAL (DAYS)\")\nplt.ylabel(\"PRICE OF STOCK\")","d1035a18":"#Normalising the input values to feed into LSTM\nscaler=MinMaxScaler(feature_range=(0,1))\ndf=scaler.fit_transform(np.array(df).reshape(-1,1))","6bf407f7":"#df = np.array(df)\ntrain_size = int(len(df)*0.70)                        #Using 70 percent of dataset for training and rest for testing\ntest_size = (len(df)-train_size)\ntraining = df[:train_size, :]\ntest = df[train_size:len(df), :]","5b3a60ab":"training.shape,test.shape","dffedcf7":"time_step = 50                                                #Using the last 50 days to predict price for the 51st day\nX_train, Y_train, X_test, Y_test = [],[],[],[]\n#Preprocessing \n#Preparing training and testing datasets compatible with LSTM\nfor i in range(len(training)-time_step):\n    x = training[i:(i+time_step), 0]        \n    X_train.append(x)\n    Y_train.append(training[i+time_step,0])\nfor i in range(len(test)-time_step):\n    x = test[i:(i+time_step), 0]\n    X_test.append(x)\n    Y_test.append(test[i+time_step,0])\nX_train = np.array(X_train);\nY_train = np.array(Y_train);\nX_test = np.array(X_test);\nY_test = np.array(Y_test);","11d15397":"X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)\nX_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)","742e7fcb":"X_train.shape, Y_train.shape","c77c7f93":"model = Sequential()\n#Adding the first LSTM layer and some Dropout regularisation\nmodel.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\nmodel.add(Dropout(0.2))\n# Adding a second LSTM layer and some Dropout regularisation\nmodel.add(LSTM(units = 50, return_sequences = True))\nmodel.add(Dropout(0.2))\n# Adding a third LSTM layer\nmodel.add(LSTM(units = 50))\n# Adding the output layer\nmodel.add(Dense(units = 1))\n\nopt = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, decay=0.01)\n# Compiling the RNN\nmodel.compile(optimizer = opt,  loss = 'mean_squared_error')","9c6eb05c":"model.summary()","2fd1d30a":"model.fit(X_train, Y_train, epochs = 100, batch_size = 64)","4578ae5c":"import tensorflow as tf\ntf.__version__\n","5697cec9":"testpred=model.predict(X_test)\ntestpred=scaler.inverse_transform(testpred)\ntrainpred = model.predict(X_train)\ntrainpred = scaler.inverse_transform(trainpred)","88671bd0":"math.sqrt(mean_squared_error(Y_train,trainpred))\nmath.sqrt(mean_squared_error(Y_test,testpred))","874cf8df":"testPredictPlot = np.empty_like(df)\ntestPredictPlot[:, :] = np.nan\ntestPredictPlot[len(Y_train)-2+(50*2)+1:len(df)-1, :] = testpred\ntrainPredictPlot = np.empty_like(df)\ntrainPredictPlot[:, :] = np.nan\ntestPredictPlot[50:len(Y_train)+50, :] = trainpred\nplt.plot(scaler.inverse_transform(df))\nplt.plot(testPredictPlot)\nplt.plot(trainPredictPlot)\nplt.show()","52f9fccb":"f=test[(len(test)-50):,:].reshape(1,-1)\nprint(f.shape)\nX=list(f)\nX=X[0].tolist()","7b408f63":"output= []\nfor i in range(30):\n    if (len(X)>50):\n        f = np.array(X[-50:])\n        f = f.reshape(1,-1)\n        f = f.reshape((1, 50, 1))\n        yhat = model.predict(f) \n        print(f)\n        print(yhat)\n        X.extend(yhat[0].tolist())\n        output.extend(yhat.tolist())\n    else:\n        f = f.reshape((1,50,1))\n        yhat = model.predict(f) \n        print(f)\n        print(yhat)\n        X.extend(yhat[0].tolist())\n        output.extend(yhat.tolist())\nprint(output)","21c12396":"new = np.arange(1,31)\nplt.plot(new,scaler.inverse_transform(output))\nplt.title(\"PREDICTED STOCK PRICES (30 DAYS)\", fontsize=8, fontweight='bold')\nplt.xlabel(\"TIME INTERVAL (DAYS)\")\nplt.ylabel(\"PRICE OF STOCK\")","facf4e58":"len(df)","e3c7689f":"#Previous prices in Blue\n#Predicted prices in Orange\nold=np.arange(1,51)\nnew=np.arange(51,81)\nplt.plot(old, scaler.inverse_transform(df[1209:]))\nplt.plot(new, scaler.inverse_transform(output))\nplt.title(\"STOCK PRICES\", fontsize=8, fontweight='bold')\nplt.xlabel(\"TIME INTERVAL (DAYS)\")\nplt.ylabel(\"PRICE OF STOCK\")","e08142db":"#Appending the predicted prices to the original dataset\ndf_graph = df.tolist()\ndf_graph.extend(output)\nplt.plot(df_graph)\nplt.title(\"STOCK PRICES\", fontsize=8, fontweight='bold')\nplt.xlabel(\"TIME INTERVAL (DAYS)\")\nplt.ylabel(\"PRICE OF STOCK\")","8db4cd6b":"### You might have to take a look at your data before executing the next cell. It may not contain all the columns mentioned below. ","fc967e51":"## The target value to be predicted is going to be the \u201cClose\u201d stock price value.","062e6a9e":"## Here we have the dataset which contains the opening and closing prices of Apple's stocks for the a year before the current day, day wise. \n## \"high\" and \"low\" are the highest and the lowest prices for the stocks for that particular day. \n## \"volume\" represents the number of stocks that were bought that day. \n## \"change percentage\" is percentage change of the closing price with respect to the last day's closing price.  ","aa19e75f":"### With small datasets and limited number of features GridSearchCV provides a very accurate model. ","8cd1fe6c":"### The cell code below takes the last 50 days and uses it to predict 51st day. It then appends the 51st day prediction to the dataset and predicts the 52nd day using last 50 days from the 52nd day, this includes the predicted price of the 51st day. \n### This happens for as many days as you want to predict, here, 30. ","a006cfda":"# Using LSTM to predict stock prices","7771e808":"# Stock price prediction. \n## This notebook uses data from an api provided by iexcloud for free, with which you can call data for **any** company with any duration needed (till present date) which adds a lot of flexibility to this notebook.\n## You can also edit it to feed the dataset by changing the inputs with a single line of code, very conveniently.\n## Plotting real-time stock data and predicting stock fluctuations using standard algorithms.\n### If this helps you learn, an upvote would be huge!","a6b2a21e":"# Gradient Boosting Regressor ","d4889402":"## The correlation map doesn't really give any useful insights apart from what we expeceted from the basic understandings stocks and terms related to it.","6e19323c":"# Linear Regresssion ","5ada0e25":"### Using RandomSearchCV to get range of optimum values of hyperparameters ","32495c9e":"# Predicting for the next 30 days using the previous time step of 50","d4fcbd6e":"## Our model predicts prices pretty well based on the history of our chosen features. ","6118c020":"### The datetime datatype cannot be passed into a linear regression model. This happens becuase Linear Regression cannot assign coefficient to the date feature as it doesn't comprehend the datetime64 datatype well. Therefore we need is to make the datetime object as numerical value and then feed that into the model. \n### So I would take months of the year, from intuition, as my feature which relates to the date column. ","fa945b17":"### Here the datatype of \"date\" column is datetime64. "}}