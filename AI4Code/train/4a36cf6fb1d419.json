{"cell_type":{"ab4dfbdf":"code","7c58e7c6":"code","04be3804":"code","d8fe70f1":"code","d2380b60":"code","baa42a0b":"code","52deca19":"code","0f5bd189":"code","c4785d4e":"code","67d0760f":"code","36cc9e04":"code","77b8b1f0":"code","484d3fb3":"markdown","2c5ad6b3":"markdown","07e73de6":"markdown","ffb75a9a":"markdown"},"source":{"ab4dfbdf":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\n\nfrom sklearn.cluster import KMeans\nfrom scipy.cluster.hierarchy import linkage,dendrogram\nfrom sklearn.cluster import AgglomerativeClustering\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","7c58e7c6":"#first we will create our data (class) by using numpy (gaussian variable)\n\n#class 1\nx1=np.random.normal(25,5,1000)\ny1=np.random.normal(25,5,1000)\n#class 2\nx2=np.random.normal(55,5,1000)\ny2=np.random.normal(60,5,1000)\n#class 3\nx3=np.random.normal(35,5,1000)\ny3=np.random.normal(15,5,1000)\n\nx=np.concatenate((x1,x2,x3),axis=0)\ny=np.concatenate((y1,y2,y3),axis=0)\n\ndictionary={\"x\":x,\"y\":y}\n\n#now we have our data \ndata=pd.DataFrame(dictionary)\ndata.head()","04be3804":"#data visualization\n\nplt.scatter(x1,y1)\nplt.scatter(x2,y2)\nplt.scatter(x3,y3)\nplt.xlabel(\"\")\nplt.ylabel(\"\")\nplt.show()","d8fe70f1":"#kmeans algorithm will see our data as it is shown below\n\nplt.scatter(x1,y1,color=\"black\")\nplt.scatter(x2,y2,color=\"black\")\nplt.scatter(x3,y3,color=\"black\")\n\nplt.show()","d2380b60":"#Kmeans\nwcss=[]\nfor k in range(1,20):\n    kmeans=KMeans(n_clusters=k)\n    kmeans.fit(data)\n    wcss.append(kmeans.inertia_) #for every k values, it will find kmeans\n    \n#now we will see our kmeans and wcss values by ploting graph\nplt.plot(range(1,20),wcss)\nplt.xlabel(\"number of k (cluster) values\")\nplt.ylabel(\"wcss values\")\nplt.show()\n\n#in this plot we will find elbow point for kmeans's k value\n#and we will see from the graph our k value should be 3","baa42a0b":"#kmeans model for k=3\nkmeans=KMeans(n_clusters=3)\nclusters=kmeans.fit_predict(data) #first it will fit our data and later apply prediction on our data\ndata[\"label\"]=clusters\n\n#we will see our data, kmeans divided 3 parts \nplt.scatter(data.x[data.label==0],data.y[data.label==0],color=\"red\")\nplt.scatter(data.x[data.label==1],data.y[data.label==1],color=\"green\")\nplt.scatter(data.x[data.label==2],data.y[data.label==2],color=\"blue\")\n\n#and we can see center of clusters, plt.scatter(x,y,color=\"\")\nplt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],color=\"orange\")\n\nplt.show()\n\n","52deca19":"#image reading and converting color\n\nimg=cv2.imread(\"..\/input\/minion\/mn.jpg\")\nplt.imshow(img)\n\n#convert to gray scale\nimg_gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n#show the gray scale image\nplt.imshow(img_gray,cmap=\"gray\")\n\n#visualize  images\nimages=[img,img_gray]\ntitles=[\"Original image\",\"Gray image\"]\nfor i in range(2):\n        plt.subplot(2, 2, i+1)\n        plt.imshow(images[i])\n        plt.title(titles[i])\n        plt.xticks([])\n        plt.yticks([])\nplt.show()\n","0f5bd189":"#Kmeans with k=5 \nkmeans=KMeans(n_clusters=5)\nkmeans.fit(img_gray.reshape(img_gray.shape[0]*img_gray.shape[1],1))\n\nplt.figure(figsize=(12,6))\nplt.subplot(1,2,1)\nplt.imshow(kmeans.labels_.reshape(179,281))\nplt.title('K-Means k=5')\nplt.show()","c4785d4e":"#Kmeans clustering with differen k values\ndef main():\n    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n    #Input parameters\n#1.samples : It should be of np.float32 data type, and each feature should be put in a single column.\n#2.clusters(K) : Number of clusters required at end\n#3.criteria : It is the iteration termination criteria. When this criteria is satisfied, \n#algorithm iteration stops. Actually, it should be a tuple of 3 parameters. They are ( type, max_iter, epsilon ):\n    \n    Z = img_gray.reshape((-1,3))\n    Z = np.float32(Z)\n    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n    \n    #output parameters\n#1.compactness : It is the sum of squared distance from each point to their corresponding centers.\n#2.labels : This is the label array (same as \u2018code\u2019 in previous article) where each element marked \u20180\u2019, \u20181\u2019.....\n#3.centers : This is array of centers of clusters.\n    K=2\n    ret, label1, center1 = cv2.kmeans(Z, K, None,criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n    center1 = np.uint8(center1)\n    res1 = center1[label1.flatten()]\n    output1 = res1.reshape((img_gray.shape))\n    \n    K=4\n    ret, label1, center1 = cv2.kmeans(Z, K, None,criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n    center1 = np.uint8(center1)\n    res1 = center1[label1.flatten()]\n    output2 = res1.reshape((img_gray.shape))\n    \n    K=12\n    ret, label1, center1 = cv2.kmeans(Z, K, None,criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n    center1 = np.uint8(center1)\n    res1 = center1[label1.flatten()]\n    output3 = res1.reshape((img_gray.shape))\n\n    output = [img, output1, output2, output3]\n    titles = ['Original Image', 'K=2', 'K=4', 'K=12']\n    \n    for i in range(4):\n        plt.subplot(2, 2, i+1)\n        plt.imshow(output[i])\n        plt.title(titles[i])\n        plt.xticks([])\n        plt.yticks([])\n    plt.show()\n#run\nif __name__ == \"__main__\":\n    main()","67d0760f":"#we will use imgg data which is created from minion image\n#dendogram\nmerg=linkage(img_gray,method=\"ward\")\ndendrogram(merg,leaf_rotation=90)\n\n#visualize\nplt.xlabel(\"data points\")\nplt.ylabel(\"Euclidean distance\")\nplt.show()\n","36cc9e04":"#dendogram for data which is created before \nmerg=linkage(data,method=\"ward\")\ndendrogram(merg,leaf_rotation=90)\n\n#visualize\nplt.xlabel(\"data points\")\nplt.ylabel(\"Euclidean distance\")\nplt.show()\n\n#we can see from this figure, number of cluster should be 3","77b8b1f0":"#Hierarcyhcal clustering with sklearn library\n\nhierarcy_clustering=AgglomerativeClustering(n_clusters=3,affinity=\"euclidean\",linkage=\"ward\")\n\ncluster=hierarcy_clustering.fit_predict(data)\n\ndata[\"label\"]=cluster\n\n#visualization\nplt.scatter(data.x[data.label==0],data.y[data.label==0],color=\"red\")\nplt.scatter(data.x[data.label==1],data.y[data.label==1],color=\"green\")\nplt.scatter(data.x[data.label==2],data.y[data.label==2],color=\"yellow\")\n\nplt.show()","484d3fb3":"**Hieararchical  Clustering**","2c5ad6b3":"**Thank you** for looking my kernel and thank you in advance for your comment and votes\n\nThanks to DATAI Team","07e73de6":"**EXAMPLE**\n\nKmeans clustering with opencv for images","ffb75a9a":"**UNSUPERVISED ALGORITHMS**\n\n\n**1.Kmeans Algorithm**\n\n**2.Hieararchical  Clustering**\n\n\n"}}