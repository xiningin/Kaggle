{"cell_type":{"10195480":"code","6843c260":"code","c18bcf80":"code","f2615876":"code","15287293":"code","8d878c23":"code","ea85cd6e":"code","1d1dd27b":"code","422886a4":"code","d0424259":"code","f078ccc1":"code","57f98491":"code","9e96ff45":"code","9f29acc2":"code","1e64bc54":"code","e59949c0":"code","796171f2":"code","b74d39d6":"code","a1579f4a":"code","e5b4e6f3":"code","b8f4c333":"code","bcb76459":"code","e2c93363":"code","fe31f534":"code","2db44a16":"code","a5815f4e":"code","184eab83":"code","0c74abc4":"code","2b55b48e":"code","6eaaa4ae":"code","652d10e6":"markdown","ca38e313":"markdown","03e9a4cd":"markdown","4b176f78":"markdown","4a57bea5":"markdown","23344d5f":"markdown","bdb343e8":"markdown","f179e6c1":"markdown","a283ae51":"markdown","efc4ca9d":"markdown","806be22a":"markdown","1bd5a1fb":"markdown","75a3d960":"markdown","1c146e14":"markdown","da055db8":"markdown","6e39a88c":"markdown","d3539873":"markdown","981a8176":"markdown","97ceaa08":"markdown","60e360f6":"markdown","b8e20815":"markdown","36b70a4a":"markdown","ab215b85":"markdown","b7725290":"markdown","b8adf483":"markdown","7b971071":"markdown","780211e1":"markdown","be99e447":"markdown","4996fc00":"markdown","d50c39e5":"markdown","c94445f7":"markdown","7f863bcd":"markdown","5e97361b":"markdown","de6592e5":"markdown","7789cd35":"markdown","7440a89e":"markdown","88685a7f":"markdown"},"source":{"10195480":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6843c260":"import matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport seaborn as sns\nsns.set()\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression","c18bcf80":"raw_data = pd.read_csv('\/kaggle\/input\/diamonds\/diamonds.csv')\nraw_data.head()","f2615876":"raw_data.info()","15287293":"raw_data = raw_data.drop(['Unnamed: 0'], axis=1)\nraw_data.head()","8d878c23":"raw_data.describe(include='all')","ea85cd6e":"print('Count of x=0:',(raw_data['x']==0).sum(), '\\nCount of y=0:',(raw_data['y']==0).sum(),\\\n      '\\nCount of z=0:',(raw_data['z']==0).sum())","1d1dd27b":"data_no_null = raw_data.drop(raw_data[raw_data['x']==0].index)\ndata_no_null = data_no_null.drop(data_no_null[data_no_null['y']==0].index)\ndata_no_null = data_no_null.drop(data_no_null[data_no_null['z']==0].index)","422886a4":"print('Count of x=0:',(data_no_null['x']==0).sum(), '\\nCount of y=0:',(data_no_null['y']==0).sum(),\\\n      '\\nCount of z=0:',(data_no_null['z']==0).sum())","d0424259":"print('Min of x=0:',min(data_no_null['x']), '\\nMin of y=0:',min(data_no_null['y']),\\\n      '\\nMin of z=0:',min(data_no_null['z']))","f078ccc1":"sns.pairplot(data=data_no_null,\n             x_vars=(data_no_null.drop(['clarity', 'cut', 'color', 'price'], axis=1)).columns,\n             y_vars=['price'],\n             kind='scatter')","57f98491":"log_price = np.log(data_no_null['price'])\ndata_no_null['log_price'] = log_price\n\nsns.pairplot(data=data_no_null,\n             x_vars=(data_no_null.drop(['clarity', 'cut', 'color', 'price','log_price'], axis=1)).columns,\n             y_vars=['price', 'log_price'],\n             kind='scatter')","9e96ff45":"data_linear = data_no_null.drop(['price'], axis=1)","9f29acc2":"data_with_dummies = pd.get_dummies(data_linear, drop_first=True)\ndata_with_dummies.head()","1e64bc54":"data_preprocessed = data_with_dummies.reset_index(drop=True)","e59949c0":"x = data_preprocessed.drop(['log_price'], axis=1)\ny = data_preprocessed['log_price']","796171f2":"scaler = StandardScaler()\nscaler.fit(x)\nx_scaled = scaler.transform(x)","b74d39d6":"x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.2, random_state=420)","a1579f4a":"reg = LinearRegression()\nreg.fit(x_train, y_train)","e5b4e6f3":"print('R-squared =',reg.score(x_train, y_train))","b8f4c333":"print('Intercept =',reg.intercept_)","bcb76459":"weights = pd.DataFrame(x.columns, columns=['Feature'])\nweights['Weight'] = reg.coef_\nprint('Weight of each variable is:')\nweights","e2c93363":"y_hat_train = reg.predict(x_train)","fe31f534":"plt.scatter(np.exp(y_train), np.exp(y_hat_train), alpha=0.2)\nplt.xlabel('Actual log_Price', size=20)\nplt.ylabel('Predicted log_Price', size=20)\nplt.ylim(0,30000)\nplt.xlim(0,20000)\nplt.plot([0,30000], [0,30000], c='orange', label='45\u00b0 line')\nplt.show()","2db44a16":"sns.displot(y_train-y_hat_train, kde=True)","a5815f4e":"test = pd.DataFrame()\ny_test.reset_index(inplace=True, drop=True)\ntest['Actual'] = np.exp(y_test)\ntest['Predicted'] = np.exp(reg.predict(x_test))\ntest","184eab83":"plt.scatter(test['Actual'], test['Predicted'], alpha=0.2)\nplt.xlabel('Actual log_Price', size=20)\nplt.ylabel('Predicted log_Price', size=20)\nplt.ylim(0,25000)\nplt.ylim(0,25000)\nplt.plot([0,25000], [0,25000], c='orange', label='45\u00b0 line')\nplt.show()","0c74abc4":"sns.displot(test['Actual']-test['Predicted'], kde=True)","2b55b48e":"pd.set_option('display.max_row', None)\npd.set_option('display.float_format', lambda x: '%.2f' %x)\ntest['Residual'] = test['Actual']-test['Predicted']\ntest['Difference%'] = np.absolute(test['Residual']\/test['Actual']*100)\ntest.sort_values(by=['Difference%'])","6eaaa4ae":"np.mean(test['Difference%'])","652d10e6":"### Creating Dummy Variables","ca38e313":"data_linear.head()","03e9a4cd":"**5. No Multi-Collinearity**","4b176f78":"### Dropping additional index column","4a57bea5":"# EDA and Preprocessing","23344d5f":"The variance of the error should be constant throughout. Here the error seems to be Hetroscedastic and violates the Homoscedaticity assumsption of the OLS algorithm we are using for Linear Regression. We'll need to further work on the model to resolve this.","bdb343e8":"data_linear.columns","f179e6c1":"### Regression","a283ae51":"### Exploring descriptive statistics of the data","efc4ca9d":"# Prediction and Inferences based on Train Data","806be22a":"# Importing relevant libraries","1bd5a1fb":"Our model is over-estimating values as we can see negative residuals. While there seems to be no issue of under-estimation for the model.","75a3d960":"### Delaing with 0 in 'x', 'y' and 'z'","1c146e14":"**Inferences based on descriptive statistics**\n* count of all the variables is same, hence there are no Null values.\n* However, the min values for 'x', 'y' and 'z' is 0 which is practically impossible.\n* The mean of 'carat' is 0.79 and 75% of values are under 1.04, while the max is 5.01. This implies that there might be some outliers.\n* Similarly, the max value for variables 'x', 'y' and 'z' is very high than their respective 75% quantile value which affects the mean and implies that we might have outliers.","da055db8":"### Splitting the data in train-test","6e39a88c":"**1. Linearity**","d3539873":"# Linear Regression","981a8176":"**3. Normality and Homoscedasticity**","97ceaa08":"**Ideally we want the predicted values to be same as the actual observation. Hence, a scatter plot of actual values VS predicted values should be formed around a 45\u00b0 line.**","60e360f6":"The total number of rows with value as 0 for 'x', 'y' and 'z' is very low copared to the observations in original dataset. Hence, dropping the observations having value 0.","b8e20815":"The observations are not coming from a Time Series or panel data, hence there is no auto-correlation.\nEach observation represents a different diamond and hence they are not correlated.","36b70a4a":"**The residuals should be normally distributed with a mean of 0.**","ab215b85":"vif_var = data_linear.drop(['cut', 'color', 'clarity', 'log_price'], axis=1)\n\nvif = pd.DataFrame()\nvif['Features'] = vif_var.columns\nvif['VIF'] = [variance_inflation_factor(vif_var.values, i) for i in range(vif_var.shape[1])]\n\nvif","b7725290":"The relation between carat-->price and x-->price is exponential. Hence, we'll perform logarithmic transformation on price to obtain linearity.","b8adf483":"### Exploring PDFs of all the numerical variables","7b971071":"vif_var.corr()","780211e1":"**2. No Endogeneity**","be99e447":"The Difference% is relatively low for the lower obersavation values of price but gradually increases as the observation's price increases. We'll need to work on this to improve the model.","4996fc00":"sns.displot(data_no_null['carat'], kde=True)","d50c39e5":"No Endogeneity refers to the prohibition of correlation between the error term and the independent variables. We should take of Omitted Variable Bias(OBS) as any omitted variable eventually adds to the error term resulting in correlation between error term and the independent variables.","c94445f7":"### Scaling the data","7f863bcd":"### Declaring dependent and independent variables","5e97361b":"# Testing of the test data","de6592e5":"**4. No auto-correlation**","7789cd35":"* Normality is assumed for big samples following Central Limit Theorem\n* Zero mean of distribution of error is accomplished by the intercept\n* We can have verified normality of error term and homoscedasticity later while predicting the results","7440a89e":"### Checking and Relaxing OLS assumptions for Linear Regression","88685a7f":"# Loading Data"}}