{"cell_type":{"230eaf73":"code","feda0496":"code","7d475696":"code","697d5caf":"markdown","a7c383b9":"markdown","d4c9c429":"markdown","194adef9":"markdown","bb832ccb":"markdown","2240ae95":"markdown","d572c689":"markdown","8dfeda47":"markdown"},"source":{"230eaf73":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn import linear_model\nfrom numpy.linalg import inv,det,multi_dot,norm\nfrom sklearn.metrics import confusion_matrix\nfrom itertools import combinations\nimport warnings\nwarnings.filterwarnings('ignore')","feda0496":"class LogisticRegression:  \n    def __init__(self):\n        self.x=None\n        self.y=None        \n        self.weights = None\n        self.bias = None\n        self.cost= [ ]        \n        \n    def Error_logit(self,x,y):\n        r,c=x.shape\n        x=np.array(x)\n        y=np.array(y).reshape(1,r)\n        for i in range(len(self.bias)):\n            fx = np.dot(x, self.weights[i]) + self.bias[i]         \n        y_pred = self.sigmoid_func(fx)        \n        y_cal = np.array([1 if i > 0.5 else 0 for i in y_pred]).reshape(1,r)  \n        \n        MSE=(1\/len(x))* np.sum(np.square(y_cal - y))       \n        return MSE   \n\n    def kfold_logistic(self,x,y):\n        kf = KFold(n_splits=5) \n        x=np.array(x)\n        y=np.array(y)      \n        \n        MinErr=[ ]\n        alpha_range = [10**i for i in range(-6,2)]\n        for j in alpha_range :\n            l=[ ]\n            for train_index,test_index in kf.split(x,y):\n                #print(x.size,y.size)\n                self.Logistic_Train(x,y,j,1000,10**-10)                \n                pred= self.Error_logit(x[test_index],y[test_index])\n                l.append(pred)\n                \n            \n            MinErr.append(sum(l)\/len(l))\n            print(f\"Learning rate: {j} mean error is : {sum(l)\/len(l)}\")\n        \n        k= np.argmin(MinErr)        \n        optimal_alpha=(alpha_range[k])\n        print(\"optimal Learning rate is-->{}\".format(optimal_alpha))\n        return optimal_alpha\n   \n         \n    def sigmoid_func(self,z):\n        return 1\/(1+np.exp(-(z)))\n    \n    def Logistic_Train(self,x,y,alpha,itr,eps):  \n        r,c=x.shape\n        x=np.array(x)\n        y=np.array(y).reshape(r,1)\n        w=np.zeros((c,1))\n        w_list=[w]\n        bias=[ ]\n        cost_list=[ ]\n        w0=0\n        for i in range(itr):\n            fx = np.dot(x,w)+ w0\n            h0=self.sigmoid_func(fx)\n            Err = h0 - y  \n            w = w - (alpha\/r)* np.dot(x.T,Err)            \n            w0 = w0 - alpha * np.sum(Err)            \n            cost = - (np.sum(np.log(h0)*y+ np.log(1-h0)*(1-y)))            \n            cost_list.append(cost)            \n            w_list.append(w)\n            bias.append(w0)\n            if cost < eps:\n                break \n                \n        self.weights={ }\n        self.bias={ }  \n        K=np.unique(y)                          #for hyperplane parameters\n        m=[*range(len(K))]\n        \n        # for multiclass classification , we run the binary classification with permutations of p(M,2)\n        \n        p=list(combinations(m, 2))         \n        for i in range(len(p)):           \n            self.weights[i]=w\n            self.bias[i]=w0\n            \n        self.cost=cost_list        \n        \n\n    def confusion_mat(self,y_test,y_pred):        \n        cm = confusion_matrix(y_test,y_pred)\n        return cm   \n\n    def Logistic_Test(self,x,y):\n        r,c=x.shape\n        x=np.array(x)\n        y=np.array(y).reshape(r,1)\n        for i in range(len(self.bias)):\n            z= np.dot(x, self.weights[i]) + self.bias[i]\n        y_pred = self.sigmoid_func(z)      \n        y_cal = np.array([1 if i > 0.5 else 0 for i in y_pred]).reshape(r,1) \n        MSE=(1\/len(x))* np.sum(np.square(y_cal - y))          \n        print('Mean Square Error is-->{}'.format(MSE))\n        cm=self.confusion_mat(y,y_cal)        \n        self.performance(cm)","7d475696":"def plot_decison_boundary(x_test,y_test, weight, bias):\n        r,c = x_test.shape\n        if(c<3):           \n            #  2D hyperplane plotting\n            x=x_test.iloc[:,0]\n            for i in range(len(bias)):\n                y_cal=-(bias[i]+ x *weight[i][0])\/weight[i][1]\n                plt.plot(x,y_cal)\n            C=['red' if l==1 else 'blue' for l in y_test]\n            plt.scatter(x_test.iloc[:,[0]], x_test.iloc[:,[1]], color= C ,label=\"Actual data\")\n            plt.xlabel('x_1',fontsize = 10)\n            plt.ylabel('x_2',fontsize = 10)\n            plt.legend(loc='best')\n            plt.show()\n            \n        elif(c==3):\n            # 3D hyperplane plotting\n            fig=plt.figure(figsize=(8, 6))\n            ax = fig.add_subplot(111, projection = '3d')\n            C=['red' if l==1 else 'blue' for l in y_test]                    \n            x1=x_test.iloc[:,0]          \n            x2=x_test.iloc[:,1]\n            tmp = np.linspace(-40,40,3)\n            for i in range(len(bias)):\n                x1,x2 = np.meshgrid(tmp,tmp)\n                z = lambda x1,x2: -(bias[i]+(x1*weight[i][0])+(x2*weight[i][1]))\/weight[i][2]\n                ax.plot_surface(x1, x2, z(x1, x2))  \n                    \n            \n            ax.scatter3D(x_test.iloc[:,[0]], x_test.iloc[:,[1]], x_test.iloc[:,[2]], color= C ,label=\"Actual data\")\n            ax.set_xlabel('X1',fontsize = 10)\n            ax.set_ylabel('X2',fontsize = 10)\n            ax.set_zlabel('X3',fontsize = 10)\n            plt.legend(loc='best')\n            plt.show()\n        \n        \n        else:\n            # plot is in higher dimenstion\n            pass","697d5caf":"# Providing Workflow Of the Logistic Regression","a7c383b9":"# Importing Libraires","d4c9c429":"![logistic%20flow_with%20ROC](https:\/\/raw.githubusercontent.com\/DASHANANT\/ML_Algorithms\/main\/Logistic%20Regression\/logistic%20flow_with%20ROC.png)","194adef9":"# Model Class Logistic Regression","bb832ccb":"# How Sigmoid Function is used for logistic-regression?\n\nIt is used to Map the results into 0 to 1 Domain - which we called as probability of event being to specific class \nthen the classification is done using a threshold which is decided by ROC AUC Curve\n\nTo get the Knowledge ROC AUC curve look at this notebook\n[understanding-roc-auc-curve](https:\/\/www.kaggle.com\/anantdashpute\/understanding-roc-auc-curve)","2240ae95":"![logistic-regression-in-machine-learning](https:\/\/static.javatpoint.com\/tutorial\/machine-learning\/images\/logistic-regression-in-machine-learning.png)","d572c689":"# Logistic regression\nLogistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression).","8dfeda47":"# Plotting Function for Hyperplane of Logistic Classification"}}