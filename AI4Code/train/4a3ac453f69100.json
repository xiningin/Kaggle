{"cell_type":{"27317cf8":"code","a425f577":"code","aa36276f":"code","3e49008a":"code","f9a4b335":"code","2f7605e8":"code","156734a5":"code","3eb6d9dd":"code","e40087f2":"code","b3fad51f":"code","e0a8f873":"code","906ef53d":"code","1c83f384":"code","dda16193":"code","1ff1e570":"code","2b4fb409":"code","765cf596":"code","a3ca155b":"code","cb02aa48":"code","d14b63a6":"code","06dce24f":"code","968376b8":"code","2285031e":"code","aec7597d":"markdown","d94843d8":"markdown","122fe3da":"markdown","7b935519":"markdown"},"source":{"27317cf8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a425f577":"#import statements\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, learning_curve, KFold\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\nimport random\nfrom sklearn.svm import SVC\nimport sklearn.metrics as sk\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree","aa36276f":"#change the dataset location\ndf = pd.read_csv('\/kaggle\/input\/bank-marketing\/bank-additional-full.csv', sep = ';')\ndf.shape","3e49008a":"#viewing data\ndf.head()","f9a4b335":"#data info\ndf.info()\n#No null values in the data","2f7605e8":"#Removing non-relevant variables\ndf1=df.drop(columns=['day_of_week','month','contact','poutcome','pdays'],axis=1)\ndf1","156734a5":"#Replacing all the binary variables to 0 and 1\ndf1.y.replace(('yes', 'no'), (1, 0), inplace=True)\ndf1.default.replace(('yes', 'no'), (1, 0), inplace=True)\ndf1.housing.replace(('yes', 'no'), (1, 0), inplace=True)\ndf1.loan.replace(('yes', 'no'), (1, 0), inplace=True)\ndf1","3eb6d9dd":"#creating Dummies for categorical variables\ndf2 = pd.get_dummies(df1)\ndf2.head()","e40087f2":"#Removing extra dummy variables & checking descriptive stats\ndf3=df2.drop(columns=['job_unknown','marital_divorced','education_unknown'],axis=1)\ndf3.describe().T","b3fad51f":"#Correlation plot\nplt.figure(figsize=(14,8))\ndf3.corr()['y'].sort_values(ascending = False).plot(kind='bar')","e0a8f873":"#Creating binary classification target variable\ndf_target=df3[['y']].values\ndf_features=df3.drop(columns=['y'],axis=1).values\nx1_train, x1_test, y1_train, y1_test = train_test_split(df_features, df_target, test_size = 0.3, random_state = 0)","906ef53d":"sc = StandardScaler()\nx1_train = sc.fit_transform(x1_train)\nx1_test = sc.transform(x1_test)","1c83f384":"#Linear SVM\nprint('Linear Model',end='\\n')\nlsvclassifier = SVC(kernel='linear')\nlsvclassifier.fit(x1_train, y1_train)\n\n#Applying k-Fold Cross Validation\naccuracies = cross_val_score(estimator = lsvclassifier, X = x1_train, y = y1_train, cv = 5)\nmean_svm_linear=accuracies.mean()\nstd_svm_linear=accuracies.std()\n\n#After using 5 fold cross validation\nprint('After 5 fold cross validation:')\nprint('Mean of Accuracies: ',mean_svm_linear*100,end='\\n')\nprint('Standard deviation of Accuracies',std_svm_linear*100,end='\\n')\n\n#Predict SVM\ny_predl = lsvclassifier.predict(x1_test)\n\n#Confusion Matrix\nprint('Test Output:')\nprint('Confusion Matrix:')\nprint(sk.confusion_matrix(y1_test,y_predl))\nprint('Classification Report:')\nprint(sk.classification_report(y1_test,y_predl))\nprint('Accuracy: ',sk.accuracy_score(y1_test, y_predl, normalize=True, sample_weight=None))","dda16193":"#Polynomial SVM\nprint('Polynomial Model',end='\\n')\npsvclassifier = SVC(kernel='poly')\npsvclassifier.fit(x1_train, y1_train)\n\n#Applying k-Fold Cross Validation\naccuracies = cross_val_score(estimator = psvclassifier, X = x1_train, y = y1_train, cv = 5)\nmean_svm_poly=accuracies.mean()\nstd_svm_poly=accuracies.std()\n\n#After using 5 fold cross validation\nprint('After 5 fold cross validation:')\nprint('Mean of Accuracies: ',mean_svm_poly*100,end='\\n')\nprint('Standard deviation of Accuracies',std_svm_poly*100,end='\\n')\n\n#Predict SVM\ny_predp = psvclassifier.predict(x1_test)\n\n#Confusion Matrix\nprint('Test Output:')\nprint('Confusion Matrix:')\nprint(sk.confusion_matrix(y1_test,y_predp))\nprint('Classification Report:')\nprint(sk.classification_report(y1_test,y_predp))\nprint('Accuracy: ',sk.accuracy_score(y1_test, y_predp, normalize=True, sample_weight=None))","1ff1e570":"#RBF SVM\nprint('RBF Model',end='\\n')\nrsvclassifier = SVC(kernel='rbf')\nrsvclassifier.fit(x1_train, y1_train)\n\n#Applying k-Fold Cross Validation\naccuracies = cross_val_score(estimator = rsvclassifier, X = x1_train, y = y1_train, cv = 5)\nmean_svm_rbf=accuracies.mean()\nstd_svm_rbf=accuracies.std()\n\n#After using 5 fold cross validation\nprint('After 5 fold cross validation:')\nprint('Mean of Accuracies: ',mean_svm_rbf*100,end='\\n')\nprint('Standard deviation of Accuracies',std_svm_rbf*100,end='\\n')\n\n#Predict SVM\ny_predr = rsvclassifier.predict(x1_test)\n\n#Confusion Matrix\nprint('Test Output:')\nprint('Confusion Matrix:')\nprint(sk.confusion_matrix(y1_test,y_predr))\nprint('Classification Report:')\nprint(sk.classification_report(y1_test,y_predr))\nprint('Accuracy: ',sk.accuracy_score(y1_test, y_predr, normalize=True, sample_weight=None))","2b4fb409":"from sklearn.model_selection import ShuffleSplit\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import learning_curve\ncv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\ntrain_sizes, train_scores, test_scores = learning_curve(rsvclassifier, \n                                                        df_features, \n                                                        df_target,\n                                                        # Number of folds in cross-validation\n                                                        cv=cv,\n                                                        # Evaluation metric\n                                                        scoring='accuracy',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 50))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()\n","765cf596":"#Entropy Model\neclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\neclassifier.fit(x1_train, y1_train)\n\n#Applying k-Fold Cross Validation\naccuracies = cross_val_score(estimator = eclassifier, X = x1_train, y = y1_train, cv = 5)\nmean_dt_e=accuracies.mean()\nstd_dt_e=accuracies.std()\n\n#After using 5 fold cross validation\nprint('After 5 fold cross validation:')\nprint('Mean of Accuracies: ',mean_dt_e*100,end='\\n')\nprint('Standard deviation of Accuracies',std_dt_e*100,end='\\n')\n\n#predict y\ny_pred = eclassifier.predict(x1_test)\n\n#Confusion Matrix\nprint('Test Output:')\nprint('Confusion Matrix:')\nprint(sk.confusion_matrix(y1_test, y_pred))\nprint('Classification Report:')\nprint(sk.classification_report(y1_test, y_pred))\nprint('Accuracy: ',sk.accuracy_score(y1_test,y_pred))","a3ca155b":"#Gini Model\ngclassifier = DecisionTreeClassifier(criterion = 'gini', random_state = 0)\ngclassifier.fit(x1_train, y1_train)\n\n#Applying k-Fold Cross Validation\naccuracies = cross_val_score(estimator = gclassifier, X = x1_train, y = y1_train, cv = 5)\nmean_dt_g=accuracies.mean()\nstd_dt_g=accuracies.std()\n\n#After using 5 fold cross validation\nprint('After 5 fold cross validation:')\nprint('Mean of Accuracies: ',mean_dt_g*100,end='\\n')\nprint('Standard deviation of Accuracies',std_dt_g*100,end='\\n')\n\n#predict y\ny_pred = gclassifier.predict(x1_test)\n\n#Confusion Matrix\nprint('Test Output:')\nprint('Confusion Matrix:')\nprint(sk.confusion_matrix(y1_test, y_pred))\nprint('Classification Report:')\nprint(sk.classification_report(y1_test, y_pred))\nprint('Accuracy: ',sk.accuracy_score(y1_test,y_pred))","cb02aa48":"#Pruning the better tree - Entropy Tree\nparameters = [{'criterion': ['entropy'],'min_samples_leaf':[5,10,20,50,100],'max_depth':[5,10,20,50,100]}] \ngrid_search = GridSearchCV(estimator = eclassifier,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(x1_train, y1_train)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\n\nprint('Accuracy: ',best_accuracy,end='\\n')\nprint('Best Parameters: ',best_parameters,end='\\n')","d14b63a6":"cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\ntrain_sizes, train_scores, test_scores = learning_curve(grid_search, \n                                                        df_features, \n                                                        df_target,\n                                                        # Number of folds in cross-validation\n                                                        cv=cv,\n                                                        # Evaluation metric\n                                                        scoring='accuracy',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 50))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()\n","06dce24f":"# Boosting via Gradient Boost\nfrom sklearn.ensemble import GradientBoostingClassifier\nclassifiergb = GradientBoostingClassifier(learning_rate=0.01,random_state=1)\nclassifiergb.fit(x1_train, y1_train)\n\n# Applying k-Fold Cross Validation\naccuracies = cross_val_score(estimator = classifiergb, X = x1_train, y = y1_train, cv = 10,n_jobs=-1)\nmean_boosting=accuracies.mean()\nstd_boosting=accuracies.std()\n\n#After using 5 fold cross validation\nprint('After 5 fold cross validation:')\nprint('Mean of Accuracies: ',mean_boosting*100,end='\\n')\nprint('Standard deviation of Accuracies',std_boosting*100,end='\\n')\n\n# Predicting the Test set results\ny_predgb = classifiergb.predict(x1_test)\n\n#Confusion Matrix\nprint('Test Output:')\nprint('Confusion Matrix:')\nprint(sk.confusion_matrix(y1_test, y_predgb))\nprint('Classification Report:')\nprint(sk.classification_report(y1_test, y_predgb))\nprint('Accuracy: ',sk.accuracy_score(y1_test,y_predgb))\n","968376b8":"#playing around with the pruning to get the best boosting tree\n# Applying Grid Search to find the best model and the best parameters\nfrom sklearn.ensemble import AdaBoostClassifier\nclassifier_AdaBoost = AdaBoostClassifier(random_state=1)\nclassifier_AdaBoost.fit(x1_train, y1_train)\nfrom sklearn.model_selection import GridSearchCV\nparameters = [{'n_estimators': [50,100,200,300,500,1000,1500]}] \ngrid_search = GridSearchCV(estimator = classifier_AdaBoost,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(x1_train, y1_train)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\n\nprint('Accuracy: ',best_accuracy,end='\\n')\nprint('Best Parameters: ',best_parameters,end='\\n')","2285031e":"cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\ntrain_sizes, train_scores, test_scores = learning_curve(classifier_AdaBoost, \n                                                        df_features, \n                                                        df_target,\n                                                        # Number of folds in cross-validation\n                                                        cv=cv,\n                                                        # Evaluation metric\n                                                        scoring='accuracy',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 50))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()\n","aec7597d":"#Boosting","d94843d8":"#**Data** **Preprocessing**","122fe3da":"#Run SVM\n","7b935519":"#Decision Trees\n"}}