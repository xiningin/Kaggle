{"cell_type":{"e7d4d033":"code","fe63c381":"code","2100ae20":"code","8f411706":"code","e172f672":"code","7de4491e":"code","9615c750":"code","a7d71f52":"code","c9d0ea45":"code","b8fc40f4":"code","07cc833c":"code","184f9aa2":"code","16fce596":"code","0150d826":"code","f9fbec26":"code","d510e23e":"code","826a5f74":"code","25a5b9bf":"code","dcd45f4e":"code","5eba34cc":"code","556e16b3":"code","ed417377":"code","d5ff7e4f":"code","dbb4929a":"code","f0ddd67a":"code","805895a5":"code","db507d35":"code","c239e166":"markdown","d1b80891":"markdown","c9455544":"markdown","cd1798f5":"markdown","f14b1e36":"markdown","a226e4c7":"markdown","88adc206":"markdown","3a51ab95":"markdown","66d81931":"markdown","98805394":"markdown","12f28747":"markdown","b3b08297":"markdown","3af106f5":"markdown","0ac5ee3d":"markdown","05134348":"markdown"},"source":{"e7d4d033":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fe63c381":"# Loading the required libraries in one place\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nsns.set(color_codes=True) # adds a nice background to the graphs\n%matplotlib inline\nfrom scipy.stats import ttest_ind, levene, shapiro\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_recall_curve","2100ae20":"# Loading the 2C_Weka file with normal and abnormal classification into a dataset\ndf_master = pd.read_csv(\"\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv\", encoding=\"utf-8\", engine='c')\n\n# Displaying common stats of all columns\ndf_master.describe(include=\"all\")","8f411706":"# Displaying master dataset shape and size\nprint(\"Shape of dataset: \", df_master.shape)\nprint(\"Size of dataset: \", df_master.size)","e172f672":"# Displaying common stats of all columns\ndf_master.describe(include=\"all\")","7de4491e":"# Checking attribute information to verify if there are any Null values\ndf_master.info()","9615c750":"# Displaying the correlation map\nf, ax = plt.subplots(figsize=(20, 20))\nsns.heatmap(df_master.corr(), annot=True, cmap='Reds', ax=ax)\nplt.show()","a7d71f52":"# Displaying all correlations with ascending sort.\ndf_master.corr().unstack().sort_values().drop_duplicates()","c9d0ea45":"# Creating a function which will return required outputs after performing welch_ttest\ndef welch_ttest(x, y):\n    ## Welch-Satterthwaite Degrees of Freedom ##\n    dof = (x.var()\/x.size + y.var()\/y.size)**2 \/ ((x.var()\/x.size)**2 \/ (x.size-1) + (y.var()\/y.size)**2 \/ (y.size-1))\n   \n    # Welch's Test\n    t, p = ttest_ind(x, y, equal_var = False)\n    \n    # Displaying the results found\n    print(\"\\n\",\n          f\"Welch's t-test= {t:.4f}\", \"\\n\",\n          f\"p-value = {p:.4f}\", \"\\n\",\n          f\"Welch-Satterthwaite Degrees of Freedom= {dof:.4f}\")","b8fc40f4":"# Performing Welch's t test on normal and abnormal groups for all independent variables\n# Running a for loop to extract each attribute name individually\nfor col in df_master.columns[:-1]:\n    # Creating a 2 groups based on Dependent variable labels Normal and Abnormal (Type_H and Type_S)\n    group1 = df_master[col][df_master[\"class\"]==\"Normal\"]\n    group2 = df_master[col][df_master[\"class\"]!=\"Normal\"]\n    \n    # Printing newline for cosmetic purposes\n    print(\"\\n\", col)\n    \n    # If Shapiro test clears both groups (Confidence of 95%) then perform welch test else display appropriate message\n    if (shapiro(group1)[1]<0.05) and (shapiro(group2)[1]<0.05):\n        welch_ttest(group1, group2)\n    else:\n        print(\"\\n At least one of the classes (Normal and Abnormal) is not normally distributed\")","07cc833c":"# Performing Tukey's hsd test on different groups, based on Class variable, for all independent variables\n# Running a for loop to extract each attribute name individually\nfor col in df_master.columns[:-1]:\n    # Printing newline for cosmetic purposes\n    print(\"\\n\", col, \"\\n\")\n    # Display Tuhey hsd test results\n    print(pairwise_tukeyhsd(df_master[col], df_master[\"class\"], alpha=0.05))","184f9aa2":"# Displaying a multivariate analysis\nsns.pairplot(data=df_master, hue=\"class\", palette=\"bright\")","16fce596":"df_normal = df_master[df_master[\"class\"]==\"Normal\"]\ndf_abnormal = df_master[df_master[\"class\"]!=\"Normal\"]\nprint(\"Normal\")\ndf_normal.info()\nprint()\nprint('Abnormal')\ndf_abnormal.info()","0150d826":"# Displaying boxplots for distinguishing between the classes\ng = sns.catplot(data=df_master, col=\"class\", kind=\"box\")\ng.set_xticklabels(rotation=45)\nplt.ylim(-50, 175)\n\nplt.show()","f9fbec26":"# Displaying the scatterplots for all combination of attributes\nfor y in range(len(df_master.columns[:-1])):\n    for x in range(len(df_master.columns[:-1])):\n        if x>y:\n            g = sns.FacetGrid(df_master, col=\"class\")\n            g.map(sns.scatterplot, df_master.columns[:-1][x], df_master.columns[:-1][y])\n            # Using log scale to exxagarate the differences\n#             g.set(xscale=\"log\")\n#             g.set(yscale=\"log\")\n            plt.show()","d510e23e":"# Seperating Predictor variables, we know the target variable \"Class\" is the last column\nX = df_master.iloc[:, :-1]\n\n# Seperating Target variables, we know the target variable is called \"Class\"\ny = df_master[\"class\"]\n\n# Scaling the data so to give equal importance to all attributes\nX = (X - np.min(X))\/(np.max(X) - np.min(X))\nX.describe()","826a5f74":"# Encoding Target variables\nle = preprocessing.LabelEncoder()\ny = le.fit_transform(y)\n\n# Displaying the order of the classes\nle.classes_","25a5b9bf":"# Performing the train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Verifying the proportions of y_train and y_test is the same as original dataset\nprint(\"y_train proportions\")\nprint(np.unique(y_train, return_counts=True)[1]\/len(y_train))\n\nprint(\"y_test proportions\")\nprint(np.unique(y_test, return_counts=True)[1]\/len(y_test))","dcd45f4e":"# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'p': [1, 2],\n              'n_neighbors': range(3, 51)}\n\n# instantiate the model\nknn = KNeighborsClassifier(metric=\"minkowski\")\n\n# fit tree on training data\nknn_grid = GridSearchCV(knn, parameters, \n                    cv=n_folds, \n                   scoring='precision')\nknn_grid.fit(X_train, y_train)\n\n###### scores of GridSearch CV\nscores = pd.DataFrame(knn_grid.cv_results_)\nscores","5eba34cc":"# printing the optimal accuracy score and hyperparameters\nprint('We can get accuracy of',knn_grid.best_score_,'using',knn_grid.best_params_)","556e16b3":"# Create a function which will plot the ROC curve when false positive and true positive rates are fed to it\ndef plot_roc_curve(fpr, tpr):\n    plt.plot(fpr, tpr, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()","ed417377":"# Building the Final Model\nclassifier = KNeighborsClassifier(n_neighbors=4, metric=\"minkowski\", p=1)\nclassifier.fit(X_train, y_train)\n\n# Predicting the results against the test set\ny_pred = classifier.predict(X_test)\n\n# Building a confusion matrix for evaluation\ndf_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), \n             index=[\"T_Abnormal\", \"T_Normal\"], columns=[\"P_Abnormal\", \"P_Normal\"])\n                     \ndf_cm","d5ff7e4f":"# Calculating Precision, recall and F1 Score\nprecision = df_cm.iloc[1, 1]\/sum(df_cm.iloc[:, 1])\nrecall = df_cm.iloc[1, 1]\/sum(df_cm.iloc[1, :])\nf_score = 2 * (precision * recall)\/(precision+recall)\n\nprint(\"Precision: {}\\nRecall: {}\\nF Score: {}\".format(precision, recall, f_score))","dbb4929a":"# Loading the probability values for positive class\ny_prob = classifier.predict_proba(X_test)[:,1]\n\n# Calculating False positive rate, true positive rate and the threshold values\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_prob, pos_label=1)\n\n# Plotting the ROC curve\nplot_roc_curve(fpr, tpr)\n\n# Calculating the area under the curve\nprint(\"Area under the curve: \", metrics.auc(fpr, tpr))","f0ddd67a":"# Calculating the area under the curve\nprint(\"Area under the curve: \", metrics.auc(fpr, tpr))\n\n# Calculating the Precision, Recall and the threshold values\nprecision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n\n# Calculate the f score for all threshold values\nfscore = (2 * precision * recall) \/ (precision + recall)\n\n# Locate the index of the largest f score\nix = np.argmax(fscore)\n\n# Display the Best cutoff point based on the best f score\nprint('Best Threshold=%f, F-Score=%.3f' %(thresholds[ix], fscore[ix]))\n\n# Plot the precision-recall curve for the model\nno_skill = len(y_test[y_test==1]) \/ len(y_test)\nplt.plot([0,1], [no_skill,no_skill], linestyle='--', label='No Skill')\nplt.plot(recall, precision, marker='.', label='Logistic')\nplt.scatter(recall[ix], precision[ix], marker='o', color='black', label='Best')\n# axis labels\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.legend()\n# show the plot\nplt.show()","805895a5":"# Create and display the dataframe of all metric values\ndf_scores_knn = pd.DataFrame({'precision' : pd.Series(precision),\n                    'recall' : pd.Series(recall), \n                    'fscore' : pd.Series(fscore), \n                    'thresholds' : pd.Series(thresholds)})\ndf_scores_knn","db507d35":"# Displaying the resulting confusion Matrix at best precision to recall cutoff\n\nthreshold = thresholds[ix]\ndf_cm = pd.DataFrame(confusion_matrix(y_test, list(map(lambda x: 1 if x >= threshold else 0, y_prob))), \n             index=[\"T_Depositor\", \"T_Asset\"], columns=[\"P_Depositor\", \"P_Asset\"])\ndf_cm","c239e166":"**<font color=#B40404>Let's look at a pairplot first as it covers a multivariate analysis where the diagonal is an univariate analysis and the rest is bivariate analysis.**","d1b80891":"**<font color=#B40404>Now let us apply Tukeys hsd test to compare the means between all labels of Class. Normal vs Abnormal**\n    \n**<font color=#B40404>Ho is \u03bc(Normal) = \u03bc(Abnormal)**\n**<font color=#B40404>Ha is \u03bc(Normal) \u2260 \u03bc(Abnormal)**\n\n**<font color=#B40404>Confidence of 95% or alpha as 0.05**","c9455544":"**<font color=#B40404>Now let's visualise if we can separate Normal and Abnormal through bivariate analysis.**","cd1798f5":"# **Data pre-processing**","f14b1e36":"# **Final Model**","a226e4c7":"<font color=#B40404>**No Null Values found and target variable class has 2 unique values. Let's explore the dataset more to see what else we can find.**","88adc206":"**<font color=#B40404>Any values greater and 0.5 or less than -0.5 is a highly correlated pair. Some examples are all Pelvic_incidence to any other attribute pair, except for Pelvic_radius. Pelvic_radius is not correlated to any other attribute except itself. Sacral_Slope is correlated with all attributes other than Pelvic_Radius and Pelvic_tilt. Other than the above mentioned pairs Lumbar_lordosis_angle is highly correlated to Sacral_slope.**","3a51ab95":"# **Grid Search**","66d81931":"**<font color=#B40404>We can see for Pelvic_incidence, Lumbar_lordosis_angle and Degree_spondylolisthesis we should reject our Null Hypothesis and hence we can safely assume that the means of the values of these attibutes, when seperated based on the labels Normal and Abnormal of the dependent variables class, are different.**\n    \n**<font color=#B40404>Thus these attributes will do a good job of classifying whether a Class is Normal or Abnormal.**","98805394":"# **Data Cleaning**","12f28747":"# **Exploratory Data Analysis (EDA)**","b3b08297":"**<font color=#B40404>First let us look at correlation between all independent variables**","3af106f5":"**<font color=#B40404>Before jumping into the test let us set our Null Hypothesis and Alternate Hypothesis.**\n\n**Ho is \u03bc(Normal) = \u03bc(Abnormal)**\n    \n**Ha is \u03bc(Normal) \u2260 \u03bc(Abnormal)**","0ac5ee3d":"**<font color=#B40404>Train and Test set is ready**","05134348":"<font color=#B40404>**Let's evaluate our final model and see if we can improve the cutoff**"}}