{"cell_type":{"cf13d618":"code","2362d38b":"code","d0f08ef9":"code","c0687d52":"code","3c9948ec":"code","4cef1c28":"code","a8c62776":"code","8fdeb74b":"code","5c88020e":"code","95efc064":"code","e55402e2":"code","d3af694f":"code","f21340f7":"code","66d9857d":"code","ea4eaf34":"code","4c913ca4":"code","67907b4f":"code","9c9d73b0":"code","26202c2b":"code","e847dedd":"code","1d2baab6":"code","ad3e31b7":"code","78f594d9":"code","10210601":"code","85aa98cd":"code","7ecee4fc":"code","826c61e5":"code","4c1baded":"code","ccb173ee":"code","17440a34":"code","38164f62":"code","1e1fce8c":"code","d53af9b7":"code","8b5c13e2":"code","0cfe078e":"code","0442bbc4":"code","ea58b4ee":"code","a419b06e":"code","cd28f79c":"code","c9ae5619":"code","e0fa372a":"code","2b7b4584":"code","a7e828e3":"code","2ff8c2a5":"code","7898c50a":"code","e4f77945":"code","932a24cc":"code","56344305":"code","71bf3eaf":"code","7eae76b6":"code","b2b6c077":"code","fa672c2a":"code","bdd6b86d":"code","4f96c7b2":"code","ca44e25e":"markdown","331795a9":"markdown","7d6ce6dc":"markdown","1e7de96b":"markdown","86c3ff8d":"markdown","5dd0e515":"markdown","8f2a6a01":"markdown","5c71ccab":"markdown","89085fdb":"markdown","8da83ecc":"markdown","cdd65b0d":"markdown","1ff23160":"markdown","cb6b680f":"markdown","e327497f":"markdown","fadac299":"markdown","72c7110b":"markdown","8e0dd14f":"markdown","91f3b455":"markdown","d15370d0":"markdown","85a9e306":"markdown","a14f68de":"markdown","7d0c44d1":"markdown","e81cef16":"markdown","7cdec4b4":"markdown","673ff65f":"markdown","3cfd0322":"markdown"},"source":{"cf13d618":"# pip install --upgrade pip","2362d38b":"# pip install tensorflow","d0f08ef9":"# pip install imblearn","c0687d52":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# import tensorflow as tf\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport matplotlib.patches as mpatches\nimport time\n\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import precision_score,recall_score,f1_score,roc_auc_score,accuracy_score,classification_report\nfrom collections import Counter\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nimport collections","3c9948ec":"# importing the dataset\ncredit = pd.read_csv(r\"..\/input\/creditcardfraud\/creditcard.csv\")\ncredit.head()","4cef1c28":"# describe the data we see that the transaction amount is really small. the mean of all the amount made is approximately\n# 88 USD\ncredit.describe()","a8c62776":"# Checking for Null values\ncredit.isnull().sum().max()\n\n# Good no null values","8fdeb74b":"credit.columns","5c88020e":"# The classes are heavily skewed we need to solve this issue later\n# but lets check the percentage of fraud to non-fraud\nprint('No Fraud', round(credit[\"Class\"].value_counts()[0]\/len(credit) * 100,2), '% of the dataset')\nprint(\"Fraud\", round(credit[\"Class\"].value_counts()[1]\/len(credit) * 100,2), '% of the dataset')","95efc064":"colors = [\"#0101DF\", \"#DF0101\"]\n\nsns.countplot(\"Class\", data = credit, palette = colors)\nplt.title(\"Class Distribution \\n (0: No Fraud || 1: Fraud)\", fontsize=14)","e55402e2":"fig, ax=plt.subplots(1,2, figsize=(18,4))\n\namount_val = credit[\"Amount\"].values\ntime_val = credit[\"Time\"].values\n\nsns.distplot(amount_val, ax = ax[0], color='r')\nax[0].set_title(\"Distribution of Transaction Amount\", fontsize=14)\nax[0].set_xlim([min(amount_val), max(amount_val)])\n\nsns.distplot(time_val, ax= ax[1], color='b')\nax[1].set_title(\"Distribution of Transaction Time\", fontsize=14)\nax[0].set_xlim([min(time_val), max(time_val)])\n\nplt.show()","d3af694f":"# scaling Time and Amount\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\n\n# RobustScaler is less prone to outliers\n\nstd_scaler= StandardScaler()\nrob_scaler = RobustScaler()\n\ncredit['scaled_amount'] = rob_scaler.fit_transform(credit['Amount'].values.reshape(-1,1))\ncredit['scaled_time'] = rob_scaler.fit_transform(credit['Time'].values.reshape(-1,1))\n\ncredit.drop(['Time','Amount'], axis=1, inplace=True)\n","f21340f7":"credit.head()","66d9857d":"scaled_amount = credit['scaled_amount']\nscaled_time = credit['scaled_time']\n\n# bringing scaled_amount and time to the first columns\ncredit.drop(['scaled_amount','scaled_time'], axis=1, inplace=True)\ncredit.insert(0, 'scaled_amount', scaled_amount)\ncredit.insert(1, 'scaled_time', scaled_time)\n\n# Amount and Time are scaled\n\ncredit.head()","ea4eaf34":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nprint('No Frauds', round(credit['Class'].value_counts()[0]\/len(credit) * 100,2), '% of the dataset')\nprint('Fraud', round(credit['Class'].value_counts()[1]\/len(credit) * 100,2), '% of the dataset')\n\n# splitting the data\nX = credit.drop('Class', axis=1)\ny = credit['Class']\n\nsss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\nfor train_index, test_index in sss.split(X,y):\n    print('Train:', train_index, 'Test:', test_index)\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n    \n    # we already have X_train and y_train in undersample data thats why i am using original to distinguish\n    # original_Xtrain,original_Xtest,original_ytrain,original_ytest = train_test_split(X,y test_size=0.3, random_state=100)\n    \n    # Check the distribution and turn into an array\noriginal_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values\n\n# see if the train and test label are distributed similarly\ntrain_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\nprint('-' * 100)\n\nprint('Label Distribution \\n')\nprint(train_counts_label\/len(original_ytrain))\nprint(test_counts_label\/len(original_ytest))\n    ","4c913ca4":"# since our classes are highly skewed we should make them equivalent in order to have a normal distribution\n\n# shuffle the data before creating the subsamples\n\ncredit = credit.sample(frac=1)\n\n# amount of fraud classes 492 rows\nfraudu = credit.loc[credit['Class'] ==1]\nnon_fraudu = credit.loc[credit['Class'] ==0][:492]\n\nnormal_distributed_df = pd.concat([fraudu, non_fraudu])\n\n# shuffle dataframe rows\nNew = normal_distributed_df.sample(frac=1, random_state=42)\n\nNew.head()","67907b4f":"print('Distribution of the classes in the subsample dataset')\nprint(New['Class'].value_counts()\/len(New))\n\n\nsns.countplot('Class', data=New, palette=colors)\nplt.title('Equally Distributed Classes', fontsize=14)\nplt.show()","9c9d73b0":"f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))\n\n# Entire dataframe\ncorr = credit.corr()\nsns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)\nax1.set_title(\"Imbalanced Correlation Matrix \\n (dont use for reference)\", fontsize=14)\n\nsub_sample_corr = New.corr()\nsns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax2)\nax2.set_title(\"Sub-sample correlation matrix \\n (use for reference)\", fontsize=14)\n\nplt.show()\n","26202c2b":"f, axes = plt.subplots(ncols=4, figsize=(20,4))\n\n# Negatively correlated with our class (the lower it is the more likely its a fraud)\nsns.boxplot(x=\"Class\", y=\"V17\", data=New, palette=colors, ax=axes[0])\naxes[0].set_title('V17 vs Class Negative Correlations')\n\nsns.boxplot(x='Class', y='V14', data=New, palette=colors, ax=axes[1])\naxes[1].set_title('V14 vs Class Negative Correlations')\n\nsns.boxplot(x='Class', y='V12', data=New, palette=colors, ax=axes[2])\naxes[2].set_title('V12 vs Class Negative Correlations')\n\nsns.boxplot(x='Class', y='V10', data=New, palette=colors, ax=axes[3])\naxes[3].set_title('V10 vs Class Negative Correlatios')\n\nplt.show()","e847dedd":"f, axes = plt.subplots(ncols=4, figsize=(20,4))\n\n# Negatively correlated with our class (the lower it is the more likely its a fraud)\nsns.boxplot(x=\"Class\", y=\"V11\", data=New, palette=colors, ax=axes[0])\naxes[0].set_title('V17 vs Class Negative Correlations')\n\nsns.boxplot(x='Class', y='V4', data=New, palette=colors, ax=axes[1])\naxes[1].set_title('V14 vs Class Negative Correlations')\n\nsns.boxplot(x='Class', y='V2', data=New, palette=colors, ax=axes[2])\naxes[2].set_title('V12 vs Class Negative Correlations')\n\nsns.boxplot(x='Class', y='V19', data=New, palette=colors, ax=axes[3])\naxes[3].set_title('V10 vs Class Negative Correlatios')\n\nplt.show()","1d2baab6":"from scipy.stats import norm\n\nf,(ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 6))\n\nv14_fraud_dist = New['V14'].loc[New['Class'] == 1].values\nsns.distplot(v14_fraud_dist, ax=ax1, fit=norm, color='#FB8861')\nax1.set_title('v14 Distribution \\n (Fraud Trasaction)', fontsize=14)\n\nv12_fraud_dist = New['V12'].loc[New['Class'] == 1].values\nsns.distplot(v12_fraud_dist, ax=ax2, fit=norm, color='#56F9BB')\nax2.set_title('v12 Distribution \\n (Fraud Trasaction)', fontsize=14)\n\nv10_fraud_dist = New['V10'].loc[New['Class'] == 1].values\nsns.distplot(v10_fraud_dist, ax=ax3, fit=norm, color='#C5B3F9')\nax3.set_title('v10 Distribution \\n (Fraud Trasaction)', fontsize=14)\n\nplt.show()","ad3e31b7":"# v14 removiing outliers(highest negatively correlated with labels)\nv14_fraud = New['V14'].loc[New['Class'] == 1].values\nq25, q75 = np.percentile(v14_fraud, 25), np.percentile(v14_fraud, 75)\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\n# determining the interquartile Range (IQR)\nv14_iqr = q75 - q25\nprint('iqr: {}'.format(v14_iqr))\n\n# Determining the Treshold\nv14_cut_off = v14_iqr * 1.5\n\n# determing the upper and lower treshold\nv14_lower, v14_upper = q25 - v14_cut_off, q75 + v14_cut_off\nprint('Cut Off: {}'.format(v14_cut_off))\nprint('V14 Lower: {}.'.format(v14_lower))\nprint('V14 Upper: {}'.format(v14_upper))\n\n# condition dropping stating if the treshold is exceed in both extrems the instance would be removed\noutliers = [x for x in v14_fraud if x < v14_lower or x > v14_upper]\nprint('Feature V14 Outliers for Fraud Cases: {}'.format(len(outliers)))\nprint('V14 Outliers: {}'.format(outliers))\n\nNew = New.drop(New[(New['V14'] > v14_upper) | (New['V14'] < v14_lower)].index)\nprint('----' * 44)\n\n\n# V12 removing outliers from fraud\nv12_fraud = New['V12'].loc[New['Class'] == 1].values\nq25, q75 = np.percentile(v12_fraud, 25), np.percentile(v12_fraud, 75)\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\nv12_iqr = q75 - q25\n\nv12_cut_off = v12_iqr * 1.5\n\nv12_lower, v12_upper = q25 - v12_cut_off, q75 + v12_cut_off\nprint('V12 Lower: {}'.format(v12_lower))\nprint('V12 Upper: {}'.format(v12_upper))\n\noutliers = [x for x in v12_fraud if x < v12_lower or x > v14_upper]\nprint('Feature V12 Outliers for Fraud Cases: {}'.format(len(outliers)))\nprint('V12 Outliers: {}'.format(outliers))\n\nNew = New.drop(New[(New['V12'] > v12_upper) | (New['V12'] < v12_lower)].index)\nprint('----' * 44)\n\n# V10 removing outliers form fraud\nv10_fraud = New['V10'].loc[New['Class'] == 1].values\nq25, q75 = np.percentile(v10_fraud, 25), np.percentile(v10_fraud, 75)\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\nv10_iqr = q75 - q25\n\nv10_cut_off = v10_iqr * 1.5\n\nv10_lower, v10_upper = q25 - v10_cut_off, q75 + v10_cut_off\nprint('V10 Lower: {}'.format(v10_lower))\nprint('V10 Upper: {}'.format(v10_upper))\n\noutliers = [x for x in v10_fraud if x < v10_lower or x > v10_upper]\nprint('Feature V10 Outliers for Fraud Cases: {}'.format(len(outliers)))\nprint('V10 Outliers: {}'.format(outliers))\n\nNew = New.drop(New[(New['V10'] > v10_upper) | (New['V10'] < v10_lower)].index)\nprint('----' * 44)\n","78f594d9":"f,(ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20,6))\n\ncolors = ['#B3F9C5', '#f9c5b3']\n# Creating the boxplot with removed outliers\n\n# feature v14\nsns.boxplot(x=\"Class\", y=\"V14\", data=New, ax=ax1, palette=colors)\nax1.set_title(\"V14 Feature \\n Removal of  outliers\", fontsize=14)\nax1.annotate(\"Fewer extreme \\n outliers\", xy=(0.98, -17.5), xytext=(0, -12),arrowprops=dict(facecolor=\"black\"), fontsize=14)\n\n# feature v12\nsns.boxplot(x=\"Class\", y=\"V12\", data=New, ax=ax2, palette=colors)\nax2.set_title(\"V12 Feature \\n Removal of  outliers\", fontsize=14)\nax2.annotate(\"Fewer extreme \\n outliers\", xy=(0.98, -17.3), xytext=(0, -12),arrowprops=dict(facecolor=\"black\"), fontsize=14)\n\n# feature v10\nsns.boxplot(x=\"Class\", y=\"V10\", data=New, ax=ax3, palette=colors)\nax3.set_title(\"V10 Feature \\n Removal of  outliers\", fontsize=14)\nax3.annotate(\"Fewer extreme \\n outliers\", xy=(0.95, -16.5), xytext=(0, -12),arrowprops=dict(facecolor=\"black\"), fontsize=14)\n\nplt.show()","10210601":"X = New.drop('Class', axis=1)\ny = New['Class']\n\n\n# T-SNE implementation\nt0 = time.time()\nX_reduced_tsne =TSNE(n_components=2, random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint(\"T-SNE took {:.2} s\".format(t1-t0))\n\n# PCA implementation\nt0 = time.time()\nX_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(X.values)\nt1= time.time()\nprint(\"PCA took {:.2} s\".format(t1-t0))\n\n# TruncatedSVD\nt0 = time.time()\nX_reduced_svd = TruncatedSVD(n_components=2, algorithm='randomized' ,random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint(\"SVD took {:.2} s\".format(t1-t0))","85aa98cd":"# Plots to show how well the data is clustered\n\nf, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(24,6))\n# labels = ['No-Fraud', 'Fraud']\nf.suptitle(\"Clustering using Dimensionality Reduction\", fontsize=14)\n\nblue_patch = mpatches.Patch(color='#0A0AFF', label=\"No Fraud\")\nred_patch = mpatches.Patch(color='#AF0000', label=\"Fraud\")\n\n\n# t-SNE scatter plot\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 0), cmap=\"coolwarm\", label=\"No Fraud\", linewidths=2)\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 1), cmap=\"coolwarm\", label=\"Fraud\", linewidths=2)\nax1.set_title('t-SNE', fontsize=14)\nax1.grid(True)\nax1.legend(handles=[blue_patch,red_patch])\n\n# PCA scatter plot\nax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 0), cmap=\"coolwarm\", label=\"No Fraud\", linewidths=2)\nax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 1), cmap=\"coolwarm\", label=\"Fraud\", linewidths=2)\nax2.set_title('t-SNE', fontsize=14)\nax2.grid(True)\nax2.legend(handles=[blue_patch,red_patch])\n\n# TruncatedSVD Scatter plot\nax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == 0), cmap=\"coolwarm\", label=\"No Fraud\", linewidths=2)\nax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == 1), cmap=\"coolwarm\", label=\"Fraud\", linewidths=2)\nax3.set_title('t-SNE', fontsize=14)\nax3.grid(True)\nax3.legend(handles=[blue_patch,red_patch])\n                    \nplt.show()            ","7ecee4fc":"# Undersampling before cross validating, prone to overfit\nX = New.drop('Class', axis=1)\ny = New['Class']","826c61e5":"# splitting into test and train\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size= 0.30, random_state=42, )","4c1baded":"# we turning the values into an array to feed into the classification algorithim\n\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values","ccb173ee":"# applying the simple classifiers\n\nclassifiers = {\"LogisticRegression\": LogisticRegression(),\n              \"KNearest\": KNeighborsClassifier(),\n              \"Support Vector Classifier\": SVC(),\n              \"DecisionTreeClassifier\": DecisionTreeClassifier()}","17440a34":"from sklearn.model_selection import cross_val_score\n\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) *\n         100, \"% accuracy score\")","38164f62":"# using Gridsearch CV to find the optimum parameter\nfrom sklearn.model_selection import GridSearchCV\n\n# For logistics Regression'\nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001,0.01,0.1,1,10,100,1000]}\n\ngrid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\ngrid_log_reg.fit(X_train,y_train)\n\n# we get the best fit\nlog_reg = grid_log_reg.best_estimator_\n\n# for Knearest Neighbor\nknears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm':['auto','ball_tree','kd_tree','brute']}\ngrid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\ngrid_knears.fit(X_train, y_train)\n\n# best fit\nknears_neighbors = grid_knears.best_estimator_\n\n# for Support vector Classifier\nsvc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\ngrid_svc = GridSearchCV(SVC(), svc_params)\ngrid_svc.fit(X_train, y_train)\n\n# best fit\nsvc = grid_svc.best_estimator_\n\n# for Decision Tree\ntree_params = {\"criterion\": ['gini','entropy'], \"max_depth\": list(range(2,4,1)), \"min_samples_leaf\": list(range(5,7,1))}\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\ngrid_tree.fit(X_train, y_train)\n\n# best fit\ntree_clf = grid_tree.best_estimator_","1e1fce8c":"# overfitting Case\n\nlog_reg_score = cross_val_score(log_reg, X_train, y_train, cv=5)\nprint('Logistic Regression Cross Validation Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + \"%\")\n\nknears_score = cross_val_score(knears_neighbors, X_train, y_train, cv=5)\nprint('Knears Neighbors Cross Validation Score:', round(knears_score.mean() * 100, 2).astype(str) + \"%\")\n\nsvc_score = cross_val_score(svc, X_train, y_train, cv=5)\nprint('Support Vector Classifier :', round(svc_score.mean() * 100, 2).astype(str) + \"%\")\n\ntree_score = cross_val_score(tree_clf, X_train, y_train, cv=5)\nprint('Decision Tree Classifier :', round(tree_score.mean() * 100, 2).astype(str) + \"%\")","d53af9b7":"# We will undersample during Cross Validation\nundersample_X = credit.drop('Class', axis=1)\nundersample_y = credit['Class']\n\nfor train_index, test_index in sss.split(undersample_X, undersample_y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    undersample_Xtrain, undersample_Xtest = undersample_X.iloc[train_index], undersample_X.iloc[test_index]\n    undersample_ytrain, undersample_ytest = undersample_y.iloc[train_index], undersample_y.iloc[test_index]\n    \n# converting to array\nundersample_Xtrain = undersample_Xtrain.values\nundersample_Xtest = undersample_Xtest.values\nundersample_ytrain = undersample_ytrain.values\nundersample_ytest = undersample_ytest.values\n\nundersample_accuracy = []\nundersample_precision = []\nundersample_recall = []\nundersample_f1 = []\nundersample_auc = []\n\n# implementation of the NearMiss Technique\nX_nearmiss, y_nearmiss = NearMiss().fit_resample(undersample_X.values, undersample_y.values)\nprint('NearMiss Label Distribution: {}'.format(Counter(y_nearmiss)))\n\n# now cross validating the right way\nfor train, test in sss.split(undersample_Xtrain, undersample_ytrain):\n    undersample_pipeline = imbalanced_make_pipeline(NearMiss(sampling_strategy='majority'), log_reg)\n    # SMOTE happens during Cross validation not before\n    undersample_model = undersample_pipeline.fit(undersample_Xtrain[train], undersample_ytrain[train])\n    undersample_prediction = undersample_model.predict(undersample_Xtrain[test])\n    \n    undersample_accuracy.append(undersample_pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    undersample_precision.append(precision_score(original_ytrain[test], undersample_prediction))\n    undersample_recall.append(recall_score(original_ytrain[test], undersample_prediction))\n    undersample_f1.append(f1_score(original_ytrain[test], undersample_prediction))\n    undersample_auc.append(roc_auc_score(original_ytrain[test], undersample_prediction))\n    ","8b5c13e2":"# lets plot the learning Curve for the classifiers\n# logistics regression\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(estimator1, estimator2, estimator3, estimator4, X,y, ylim=None, cv=None, n_jobs=1,\n                       train_sizes=np.linspace(.1, 1.0, 5)):\n    f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(20,14), sharey=True)\n    if ylim is not None:\n        plt.ylim(*ylim)\n        \n        # First estimator\n        train_sizes, train_scores, test_scores = learning_curve(estimator1, X,y, cv=cv, n_jobs=n_jobs,\n                                                                train_sizes=train_sizes)\n        train_scores_mean = np.mean(train_scores, axis=1)\n        train_scores_std = np.std(train_scores, axis=1)\n        test_scores_mean = np.mean(test_scores, axis=1)\n        test_scores_std = np.std(test_scores, axis=1)\n        ax1.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean+train_scores_std,\n                        alpha=0.1, color = \"#ff9124\")\n        ax1.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std,\n                        alpha=0.1, color= \"#2492ff\")\n        ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\", label=\"Training Score\")\n        ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\", label=\"Cross-validation score\")\n        ax1.set_title(\"Logistic Regression Learnining Curve\", fontsize=14)\n        ax1.set_xlabel('Training Size (m)')\n        ax1.set_ylabel('Score')\n        ax1.grid(True)\n        ax1.legend(loc=\"best\")\n        \n        # Second Estimator\n        train_sizes, train_scores, test_scores = learning_curve(estimator2, X,y, cv=cv, n_jobs=n_jobs,\n                                                                train_sizes=train_sizes)\n        train_scores_mean = np.mean(train_scores, axis=1)\n        train_scores_std = np.std(train_scores, axis=1)\n        test_scores_mean = np.mean(test_scores, axis=1)\n        test_scores_std = np.std(test_scores, axis=1)\n        ax2.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean+train_scores_std,\n                        alpha=0.1, color = \"#ff9124\")\n        ax2.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std,\n                        alpha=0.1, color= \"#2492ff\")\n        ax2.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\", label=\"Training Score\")\n        ax2.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\", label=\"Cross-validation score\")\n        ax2.set_title(\"Knears Neighbors Learnining Curve\", fontsize=14)\n        ax2.set_xlabel('Training Size (m)')\n        ax2.set_ylabel('Score')\n        ax2.grid(True)\n        ax2.legend(loc=\"best\")\n        \n        # third estimator\n        train_sizes, train_scores, test_scores = learning_curve(estimator3, X,y, cv=cv, n_jobs=n_jobs,\n                                                                train_sizes=train_sizes)\n        train_scores_mean = np.mean(train_scores, axis=1)\n        train_scores_std = np.std(train_scores, axis=1)\n        test_scores_mean = np.mean(test_scores, axis=1)\n        test_scores_std = np.std(test_scores, axis=1)\n        ax3.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean+train_scores_std,\n                        alpha=0.1, color = \"#ff9124\")\n        ax3.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std,\n                        alpha=0.1, color= \"#2492ff\")\n        ax3.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\", label=\"Training Score\")\n        ax3.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\", label=\"Cross-validation score\")\n        ax3.set_title(\"Support Vector Classifier Learnining Curve\", fontsize=14)\n        ax3.set_xlabel('Training Size (m)')\n        ax3.set_ylabel('Score')\n        ax3.grid(True)\n        ax3.legend(loc=\"best\")\n        \n        # Fourth Estimator\n        train_sizes, train_scores, test_scores = learning_curve(estimator4, X,y, cv=cv, n_jobs=n_jobs,\n                                                                train_sizes=train_sizes)\n        train_scores_mean = np.mean(train_scores, axis=1)\n        train_scores_std = np.std(train_scores, axis=1)\n        test_scores_mean = np.mean(test_scores, axis=1)\n        test_scores_std = np.std(test_scores, axis=1)\n        ax1.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean+train_scores_std,\n                        alpha=0.1, color = \"#ff9124\")\n        ax4.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std,\n                        alpha=0.1, color= \"#2492ff\")\n        ax4.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\", label=\"Training Score\")\n        ax4.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\", label=\"Cross-validation score\")\n        ax4.set_title(\"Decision Tree Learnining Curve\", fontsize=14)\n        ax4.set_xlabel('Training Size (m)')\n        ax4.set_ylabel('Score')\n        ax4.grid(True)\n        ax4.legend(loc=\"best\")\n        return plt\n        ","0cfe078e":"cv = ShuffleSplit(n_splits=100, test_size=0.30, random_state=42)\nplot_learning_curve(log_reg, knears_neighbors, svc, tree_clf,\n                   X_train, y_train, (0.87, 1.01), cv=cv, n_jobs=4)","0442bbc4":"from sklearn.metrics import roc_curve\nfrom sklearn.model_selection import cross_val_predict\n# create a Dataframe with all the scores and classifier name\n\nlog_reg_pred = cross_val_predict(log_reg, X_train, y_train,\n                                cv=5, method = \"decision_function\")\nknears_pred = cross_val_predict(knears_neighbors, X_train, y_train,\n                                cv=5)\nsvc_pred = cross_val_predict(svc, X_train, y_train,\n                             cv=5, method=\"decision_function\")\ntree_pred = cross_val_predict(tree_clf, X_train, y_train,\n                              cv = 5)","ea58b4ee":"from sklearn.metrics import roc_auc_score\n\nprint('Logistic Regression:', roc_auc_score(y_train, log_reg_pred))\nprint('KNears Neighbors:', roc_auc_score(y_train, knears_pred))\nprint('Support Vector Classifier:', roc_auc_score(y_train, svc_pred))\nprint('Decision Tree Classifier:', roc_auc_score(y_train, tree_pred))\n","a419b06e":"log_fpr, log_tpr, log_thresold = roc_curve(y_train, log_reg_pred)\nknear_fpr, knear_tpr, knear_threshold = roc_curve(y_train, knears_pred)\nsvc_fpr, svc_tpr, svc_threshold = roc_curve(y_train, svc_pred)\ntree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, tree_pred)\n\n\ndef graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr):\n    plt.figure(figsize=(16,8))\n    plt.title('ROC Curve \\n Top 4 Classifiers', fontsize=18)\n    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train, log_reg_pred)))\n    plt.plot(knear_fpr, knear_tpr, label='KNears Neighbors Classifier Score: {:.4f}'.format(roc_auc_score(y_train, knears_pred)))\n    plt.plot(svc_fpr, svc_tpr, label='Support Vector Classifier Score: {:.4f}'.format(roc_auc_score(y_train, svc_pred)))\n    plt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y_train, tree_pred)))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend()\n    \ngraph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr)\nplt.show()","cd28f79c":"def logistic_roc_curve(log_fpr, log_tpr):\n    plt.figure(figsize=(12,8))\n    plt.title('Logistic Regression Roc Curve', fontsize=16)\n    plt.plot(log_fpr, log_tpr, 'b-', linewidth=2)\n    plt.plot([0,1], [0,1], 'r--')\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.axis([-0.01,1,0,1])\n    \nlogistic_roc_curve(log_fpr, log_tpr)\nplt.show()","c9ae5619":"from sklearn.metrics import precision_recall_curve\n\nprecision, recall, threshold = precision_recall_curve(y_train, log_reg_pred)","e0fa372a":"from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\ny_pred = log_reg.predict(X_train)\n\n# Overfitting Case\nprint('---' * 45)\nprint('Overfitting: \\n')\nprint('Recall Score: {:.2f}'.format(recall_score(y_train, y_pred)))\nprint('Precision Score: {:.2f}'.format(precision_score(y_train, y_pred)))\nprint('F1 Score: {:.2f}'.format(f1_score(y_train, y_pred)))\nprint('Accuracy Score: {:.2f}'.format(accuracy_score(y_train, y_pred)))\nprint('---' * 45)\n\n# How it should look like\nprint('---' * 45)\nprint('How it should be:\\n')\nprint(\"Accuracy Score: {:.2f}\".format(np.mean(undersample_accuracy)))\nprint(\"Precision Score: {:.2f}\".format(np.mean(undersample_precision)))\nprint(\"Recall Score: {:.2f}\".format(np.mean(undersample_recall)))\nprint(\"F1 Score: {:.2f}\".format(np.mean(undersample_f1)))\nprint('---' * 45)","2b7b4584":"undersample_y_score = log_reg.decision_function(original_Xtest)","a7e828e3":"from sklearn.metrics import average_precision_score\n\nundersample_average_precision = average_precision_score(original_ytest, undersample_y_score)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      undersample_average_precision))","2ff8c2a5":"from sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(12,6))\n\nprecision, recall, _ = precision_recall_curve(original_ytest, undersample_y_score)\n\nplt.step(recall, precision, color='#004a93', alpha=0.2,\n         where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.2,\n                 color='#48a6ff')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('UnderSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n          undersample_average_precision), fontsize=16)","7898c50a":"from imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\n\n\nprint('Length of X (train): {} | Length of y (train): {}'.format(len(original_Xtrain), len(original_ytrain)))\nprint('Length of X (test): {} | Length of y (test): {}'.format(len(original_Xtest), len(original_ytest)))\n\n# List to append the score and then find the average\naccuracy_lst = []\nprecision_lst = []\nrecall_lst = []\nf1_lst = []\nauc_lst = []\n\n# Classifier with optimal parameters\n# log_reg_sm = grid_log_reg.best_estimator_\nlog_reg_sm = LogisticRegression()\n\n\n\n\nrand_log_reg = RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=4)\n\n\n# Implementing SMOTE Technique \n# Cross Validating the right way\n# Parameters\nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\nfor train, test in sss.split(original_Xtrain, original_ytrain):\n    pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_log_reg) # SMOTE happens during Cross Validation not before..\n    model = pipeline.fit(original_Xtrain[train], original_ytrain[train])\n    best_est = rand_log_reg.best_estimator_\n    prediction = best_est.predict(original_Xtrain[test])\n    \n    accuracy_lst.append(pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    precision_lst.append(precision_score(original_ytrain[test], prediction))\n    recall_lst.append(recall_score(original_ytrain[test], prediction))\n    f1_lst.append(f1_score(original_ytrain[test], prediction))\n    auc_lst.append(roc_auc_score(original_ytrain[test], prediction))\n    \nprint('---' * 45)\nprint('')\nprint(\"accuracy: {}\".format(np.mean(accuracy_lst)))\nprint(\"precision: {}\".format(np.mean(precision_lst)))\nprint(\"recall: {}\".format(np.mean(recall_lst)))\nprint(\"f1: {}\".format(np.mean(f1_lst)))\nprint('---' * 45)","e4f77945":"labels = ['No Fraud', 'Fraud']\nsmote_prediction = best_est.predict(original_Xtest)\nprint(classification_report(original_ytest, smote_prediction, target_names=labels))","932a24cc":"y_score = best_est.decision_function(original_Xtest)","56344305":"average_precision = average_precision_score(original_ytest, y_score)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","71bf3eaf":"fig = plt.figure(figsize=(12,6))\n\nprecision, recall, _ = precision_recall_curve(original_ytest, y_score)\n\nplt.step(recall, precision, color='r', alpha=0.2,\n         where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.2,\n                 color='#F59B00')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('OverSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n          average_precision), fontsize=16)","7eae76b6":"# SMOTE Technique (OverSampling) After splitting and Cross Validating\nsm = SMOTE(random_state=42, sampling_strategy=0.6)\n# Xsm_train, ysm_train = sm.fit_sample(X_train, y_train)\n\n\n# This will be the data were we are going to \nXsm_train, ysm_train = sm.fit_resample(original_Xtrain, original_ytrain)","b2b6c077":"# We Improve the score by 2% points approximately \n# Implement GridSearchCV and the other models.\n\n# Logistic Regression\nt0 = time.time()\nlog_reg_sm = grid_log_reg.best_estimator_\nlog_reg_sm.fit(Xsm_train, ysm_train)\nt1 = time.time()\nprint(\"Fitting oversample data took :{} sec\".format(t1 - t0))","fa672c2a":"from sklearn.metrics import confusion_matrix\n\n# Logistic Regression fitted using SMOTE technique\ny_pred_log_reg = log_reg_sm.predict(X_test)\n\n# Other models fitted with UnderSampling\ny_pred_knear = knears_neighbors.predict(X_test)\ny_pred_svc = svc.predict(X_test)\ny_pred_tree = tree_clf.predict(X_test)\n\n\nlog_reg_cf = confusion_matrix(y_test, y_pred_log_reg)\nkneighbors_cf = confusion_matrix(y_test, y_pred_knear)\nsvc_cf = confusion_matrix(y_test, y_pred_svc)\ntree_cf = confusion_matrix(y_test, y_pred_tree)\n\nfig, ax = plt.subplots(2, 2,figsize=(22,12))\n\n\nsns.heatmap(log_reg_cf, ax=ax[0][0], annot=True, cmap=plt.cm.copper)\nax[0, 0].set_title(\"Logistic Regression \\n Confusion Matrix\", fontsize=14)\nax[0, 0].set_xticklabels(['', ''], fontsize=14, rotation=90)\nax[0, 0].set_yticklabels(['', ''], fontsize=14, rotation=360)\n\nsns.heatmap(kneighbors_cf, ax=ax[0][1], annot=True, cmap=plt.cm.copper)\nax[0][1].set_title(\"KNearsNeighbors \\n Confusion Matrix\", fontsize=14)\nax[0][1].set_xticklabels(['', ''], fontsize=14, rotation=90)\nax[0][1].set_yticklabels(['', ''], fontsize=14, rotation=360)\n\nsns.heatmap(svc_cf, ax=ax[1][0], annot=True, cmap=plt.cm.copper)\nax[1][0].set_title(\"Suppor Vector Classifier \\n Confusion Matrix\", fontsize=14)\nax[1][0].set_xticklabels(['', ''], fontsize=14, rotation=90)\nax[1][0].set_yticklabels(['', ''], fontsize=14, rotation=360)\n\nsns.heatmap(tree_cf, ax=ax[1][1], annot=True, cmap=plt.cm.copper)\nax[1][1].set_title(\"DecisionTree Classifier \\n Confusion Matrix\", fontsize=14)\nax[1][1].set_xticklabels(['', ''], fontsize=14, rotation=90)\nax[1][1].set_yticklabels(['', ''], fontsize=14, rotation=360)\n\n\nplt.show()","bdd6b86d":"from sklearn.metrics import classification_report\n\n\nprint('Logistic Regression:')\nprint(classification_report(y_test, y_pred_log_reg))\n\nprint('KNears Neighbors:')\nprint(classification_report(y_test, y_pred_knear))\n\nprint('Support Vector Classifier:')\nprint(classification_report(y_test, y_pred_svc))\n\nprint('Support Vector Classifier:')\nprint(classification_report(y_test, y_pred_tree))","4f96c7b2":"# Final Score in the test set of logistic regression\nfrom sklearn.metrics import accuracy_score\n\n# Logistic Regression with Under-Sampling\ny_pred = log_reg.predict(X_test)\nundersample_score = accuracy_score(y_test, y_pred)\n\n\n\n# Logistic Regression with SMOTE Technique (Better accuracy with SMOTE t)\ny_pred_sm = best_est.predict(original_Xtest)\noversample_score = accuracy_score(original_ytest, y_pred_sm)\n\n\nd = {'Technique': ['Random UnderSampling', 'Oversampling (SMOTE)'], 'Score': [undersample_score, oversample_score]}\nfinal_df = pd.DataFrame(data=d)\n\n# Move column\nscore = final_df['Score']\nfinal_df.drop('Score', axis=1, inplace=True)\nfinal_df.insert(1, 'Score', score)\n\n# Note how high is accuracy score it can be misleading! \nfinal_df","ca44e25e":"Here we want to know if there are features that influence heavily in whether a transaction is a fraud or Non-fraud, we will be using the subsample cause its already balanced.","331795a9":"#### Correlation Matrices","7d6ce6dc":"#### Equally Distributing and Correlating\nNow the dataframe is correctly balanced, we can go further with our analysis and data perprocessing","1e7de96b":"Terms:\n\n. True Positives : Correctly classified Fraud Transactions   \n. False Positives : Incorrectly classified Fraud Transactions\n\n. True Negatives: Correctly classified Non-fraud Transactions  \n. False Negatives: incorrectly classifed Non-fraud Transactions \n\n. Precision : True Positives\/(true positives + false positives)\n\n. Recall : is the amount of fraud cases our model is able to detect,\n        so we want a high recall score","86c3ff8d":"99.83% of the datset were non-fraud while 0.17% of the data were fraud this tells us that our original dataset is imbalanced\nand if we use this dataframe for our predictive model and analysis we would get a lot of errors and our algorithim will overfit\nsince it will \"assume\"  that most of the transaction are not fraud. But we want our model to detect patterns that gives sign of fraud","5dd0e515":"Confusion Matrix\npositive\/Negative: Type of class (label) [\"No\",\"yes\"]True\/False:\n        correctly of incorrectly classified by the model\nTrueNegatives(Top-Left Square): Number of correctly classified No-fraud class\n\nFalse Negatives (Top-Right Square): This is the number of incorrectly classifications of the \"No\"(No Fraud Detected) class.\n\n\nFalse Positives (Bottom-Left Square): This is the number of incorrectly classifications of the \"Yes\" (Fraud Detected) class\n\n\nTrue Positives (Bottom-Right Square): This is the number of correctly classifications of the \"Yes\" (Fraud Detected) class.        ","8f2a6a01":"we can see how skewed these features are","5c71ccab":"SMOTE stands for Synthetic Minority Over_sampling Techinque. Unlike random Undersampling, SMOTE creates new synthetic points in order to have an equal balance of the classes\n\"Another alternative for soving class imbalance problems\"","89085fdb":"Anomally Detection","8da83ecc":"In this section we will train Four Types of classifiers and decide which classifier will be more effective in detecting Fraud transactions.\nso we will split our data into train and test and seperate the features from the labels","cdd65b0d":"Our Goals:\n    1. Understand the distribution of the data provided\n    2. create a 50\/50 subdataset ration of Fraud and Non-Fraud transactions\n    3. Determine the classifier to be used and finding out which has the highest accuracy\n    4. Create a Neural network and compare the efficiency to our best classifier\n    5. Understand common mistakes make with imblanced dataset.","1ff23160":"#### Importing the Libaries","cb6b680f":"#### Splitting the Data( Original DataFrame )","e327497f":"In this kernel we will be using different predictive models to see how accurately they are in detecting whether a transaction\nis a normal payment or a fraud. so lets analyze some important aspect of the dataset","fadac299":"Analysis on the most positively and negatively correlated\n\n","72c7110b":"#### Dimensionallity Reduction and Clustering","8e0dd14f":"The main goal is to fit the model either with the dataframe that were undersample and oversample (in order for our models to detect the pattern)\nand test it on the original testing set.","91f3b455":"t-SNE algorithm can pretty accurately cluster the cases that were fraud and non-fraud in our dataset even if the subsample is \nsmall it is able to detect clusters in every scenario","d15370d0":"Removing outliers from features that have high correlation with our classes.. We have to be careful as to how far we want the treshhold for removing the outliers, the higher the treshold the less outliers it will detect, the lower the treshold the more the outliers will detect, so we will be removing only extreme outliers to prevent the risk of information loss which leads to lower accuracy.","85a9e306":"##### SMOTE Technique(over sampling)\n","a14f68de":"Note:\n\n    .Never test on the oversampled or undersampled dataset\n    .to implement cross validation, remember to oversample or undersample your training data during cross-validation not before\n     .Dont use accuracy score as a metrics with imblanced datasets(will be usually high and misleading) instead use f1-score,confusion matrix or precision\/recall score","7d0c44d1":"#### Implementing Random under Sampling ","e81cef16":"#### Classifier","7cdec4b4":"##### We dive into the logistic Regression classifier\n","673ff65f":"that is removing data to have a more balanced dataset so as to prevent overfitting\n\n. we determing how imblanced our data is(by using the \"value_counts()\" on the class)\n\n. determine how many instances are considered fraud transactions and bring the non-fraud transaction to the same amount\n\n. after implementing the techinque we shuffle the data","3cfd0322":"we are going to scale the columns comprising Time and Amount,\n\nwe also need to create sub sample (i.e a dataframe with a 50\/50 ratio of fraud and non-fraud) in order to have an equal amount of Fraud and Non-Fraud cases\nwhich helps our algorithim better understand patterns that helps determine whether a trasaction is a fraud or not.\n\ncreating these sub samples prevents overfitting and wrong correlation."}}