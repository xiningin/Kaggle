{"cell_type":{"ac6f52b0":"code","feea1d22":"code","21aa07fa":"code","faa9f0a4":"code","fcffd8c9":"code","ce9d76c3":"code","b88befdd":"code","9dd2511a":"code","ab786697":"code","b885331e":"code","4be342d2":"code","e0038a1c":"code","23a439a0":"code","5fd1e7c9":"code","396fd53a":"code","90f73819":"code","35195a42":"code","ba35afca":"code","b07da042":"code","6c331254":"code","3a99f9cc":"code","6048feb6":"code","ab8924f0":"code","098c90d0":"code","653c5220":"code","acdaf1e1":"code","f1ddaf97":"code","7f8bfdc7":"code","a0f8dedc":"code","66edb9e8":"code","bf6333a9":"code","0857a40e":"code","3d8b56e9":"code","1725330e":"code","ff6cc7ab":"code","dbf3bebd":"code","f838ec7d":"code","712575d4":"code","ea213c04":"code","72819872":"code","d19a3406":"code","2ccaa145":"code","13a555ae":"code","b89bfeec":"code","c96c5230":"code","910218ba":"code","eec9fcd5":"markdown","79382523":"markdown","f9fe2780":"markdown","7a2e9775":"markdown","3d646a9c":"markdown","ca34e371":"markdown","07813037":"markdown","ba0e1dee":"markdown","1c1d198d":"markdown","f1033033":"markdown","f4c8f21b":"markdown","70f3ab4e":"markdown","ca7f7865":"markdown","94478629":"markdown","c95e1d86":"markdown","510f6d1f":"markdown","0d038082":"markdown","983066ff":"markdown","b1ab8891":"markdown","e2fb419b":"markdown","0ed19265":"markdown","cfc7641e":"markdown","244b194a":"markdown","f8b8d789":"markdown","b00ce302":"markdown","4d320ba9":"markdown","fa32ed9a":"markdown","c1a99ea7":"markdown"},"source":{"ac6f52b0":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport time\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\nimport os","feea1d22":"class Config:\n    \n    embed_dim = 64  # Embedding size for each token.\n    num_heads = 2  # Number of attention heads\n    ff_dim = 32  # Hidden layer size in feedforward network.\n    num_experts = 10  # Number of experts used in the Switch Transformer.\n    batch_size = 50  # Batch size.\n    learning_rate = 1e-4  # Learning rate.\n    dropout_rate = 0.25  # Dropout rate.\n    num_epochs = 3  # Number of epochs.\n    vocab_size = 5000  # Only consider the top 5000 words\n    num_tokens_per_example = 30 \n    num_tokens_per_batch= (\n        batch_size * num_tokens_per_example\n    )  # Total number of tokens per batch.\n    is_kaggle_platform = os.path.exists(\"\/kaggle\/input\")\n    \n    dataset_name = \"nlp-getting-started\"\n\n    data_path = \"\/kaggle\/input\/%s\/\"%(dataset_name) if is_kaggle_platform else \"\"\n    \n    submit_filename = \"submission.csv\"\nconfig = Config()","21aa07fa":"if not config.is_kaggle_platform:\n  try:\n    import kaggle\n  except:\n    !pip install kaggle\n  if not os.path.exists(\"\/root\/.kaggle\/kaggle.json\"):\n    # Replace this place to your user name and API key\n    !echo \"{\\\"username\\\":\\\"{Your user name}\\\",\\\"key\\\":\\\"{Your API Key}\\\"}\" >> \/root\/.kaggle\/kaggle.json\n    !chmod 600 \/root\/.kaggle\/kaggle.json\n  !kaggle competitions download -c $config.dataset_name","faa9f0a4":"train = pd.read_csv(config.data_path + \"train.csv\")\ntrain.head()","fcffd8c9":"test = pd.read_csv(config.data_path + \"test.csv\")\ntest.head()","ce9d76c3":"test.shape","b88befdd":"train.isnull().sum()","9dd2511a":"test.isnull().sum()","ab786697":"train[\"keyword\"].replace(np.NAN, \"\", inplace=True)\ntrain[\"location\"].replace(np.NAN, \"\", inplace=True)\ntest[\"keyword\"].replace(np.NAN, \"\", inplace=True)\ntest[\"location\"].replace(np.NAN, \"\", inplace=True)","b885331e":"contents = []\nfor data in [train, test]:\n    for i in range(data.shape[0]):\n        item = data.iloc[i]\n        sentence = item[\"keyword\"] + \" \" + item[\"text\"] + \" \" + item[\"location\"]\n        contents.append(sentence.lower())","4be342d2":"tokenizer = tf.keras.preprocessing.text.Tokenizer()\n\ntokenizer.fit_on_texts(contents)","e0038a1c":"tokens = tokenizer.texts_to_sequences(contents)","23a439a0":"word_counter = dict()\nfor token in tokens:\n    for item in token:\n        key = tokenizer.index_word[item]\n        if key in word_counter:\n            word_counter[key] += 1\n        else:\n            word_counter[key] = 1","5fd1e7c9":"word_freq = pd.DataFrame({\"word\": word_counter.keys(), \"count\": word_counter.values()})\nword_freq.sort_values(ascending=False, by=\"count\", inplace=True)\nword_freq.head(10)","396fd53a":"len(word_counter)","90f73819":"word_freq[\"count\"].plot(kind=\"hist\")","35195a42":"word_freq[word_freq[\"count\"] < 100].plot(kind=\"hist\")","ba35afca":"word_freq[word_freq[\"count\"] <= 10].plot(kind=\"hist\")","b07da042":"word_freq[word_freq[\"count\"] <= 3].plot(kind=\"hist\")","6c331254":"lower_thresold = 3\nword_appear_less = list(word_freq[word_freq[\"count\"] <= lower_thresold][\"word\"])","3a99f9cc":"len(word_appear_less)","6048feb6":"stopword_candidates = list(word_freq[\"word\"][:100])\nprint(stopword_candidates)\n#stopword_candidate_indices = set([tokenizer.word_index[word] for word in stopword_candidates])","ab8924f0":"stop_words = ['co', 't', 'http', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is', 'on', 'for', 'you', 'my', 'it', 'with', 'that', 'by', 'at', 'this', 'new', 'from', 'https', 'are', 'be', 'was', 'have', 'like', 'as', 'up', 'just', 'your', 'not', 'but', 'me', 'so', 'no', 'all', 'will', 'after', 'an', 'we', \"i'm\", 'if', 'when', 'has', 'via', 'get', 'or', '2', 'more', 'about', 'now', 'he', 'how', 'they', 'one', 'people', 'what', \"it's\", 'who', 'news', 'over', 'been', 'do', 'ca', 'into', 'can', 'there', 'video', 'u', '3', 'would', 'world', 'her', 'us', 's', 'his', 'than', \"'\", '1', 'still', 'some']\nprint(stop_words)","098c90d0":"exclude_set = set(word_appear_less + stop_words)","653c5220":"new_sentences = []\nfor token in tokens:\n    new_token = []\n    for item in token:\n        word = tokenizer.index_word[item]\n        if not word in exclude_set:\n            new_token.append(word)\n    new_sentences.append(\" \".join(new_token))","acdaf1e1":"new_sentences[:10]","f1ddaf97":"new_tokenizer = tf.keras.preprocessing.text.Tokenizer()\nnew_tokenizer.fit_on_texts(new_sentences)","7f8bfdc7":"new_tokens = new_tokenizer.texts_to_sequences(new_sentences)","a0f8dedc":"new_tokens_lengths = [len(token) for token in new_tokens]","66edb9e8":"lengths = pd.DataFrame({\"length\":new_tokens_lengths})","bf6333a9":"lengths.describe()","0857a40e":"padding_tokens = tf.keras.preprocessing.sequence.pad_sequences(new_tokens, maxlen=30, padding='post', truncating='post')","3d8b56e9":"x_train = padding_tokens[:len(train)]\ny_train = train[\"target\"]\nx_test = padding_tokens[len(train):]","1725330e":"train[\"target\"].value_counts()","ff6cc7ab":"class TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super(TokenAndPositionEmbedding, self).__init__()\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return x + positions","dbf3bebd":"def create_feedforward_network(ff_dim, name=None):\n    return keras.Sequential(\n        [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(ff_dim)], name=name\n    )","f838ec7d":"def load_balanced_loss(router_probs, expert_mask):\n    # router_probs [tokens_per_batch, num_experts] is the probability assigned for\n    # each expert per token. expert_mask [tokens_per_batch, num_experts] contains\n    # the expert with the highest router probability in one\u2212hot format.\n\n    num_experts = tf.shape(expert_mask)[-1]\n    # Get the fraction of tokens routed to each expert.\n    # density is a vector of length num experts that sums to 1.\n    density = tf.reduce_mean(expert_mask, axis=0)\n    # Get fraction of probability mass assigned to each expert from the router\n    # across all tokens. density_proxy is a vector of length num experts that sums to 1.\n    density_proxy = tf.reduce_mean(router_probs, axis=0)\n    # Want both vectors to have uniform allocation (1\/num experts) across all\n    # num_expert elements. The two vectors will be pushed towards uniform allocation\n    # when the dot product is minimized.\n    loss = tf.reduce_mean(density_proxy * density) * tf.cast(\n        (num_experts ** 2), tf.dtypes.float32\n    )\n    return loss","712575d4":"class Router(layers.Layer):\n    def __init__(self, num_experts, expert_capacity):\n        self.num_experts = num_experts\n        self.route = layers.Dense(units=num_experts)\n        self.expert_capacity = expert_capacity\n        super(Router, self).__init__()\n\n    def call(self, inputs, training=False):\n        # inputs shape: [tokens_per_batch, embed_dim]\n        # router_logits shape: [tokens_per_batch, num_experts]\n        router_logits = self.route(inputs)\n\n        if training:\n            # Add noise for exploration across experts.\n            router_logits += tf.random.uniform(\n                shape=router_logits.shape, minval=0.9, maxval=1.1\n            )\n        # Probabilities for each token of what expert it should be sent to.\n        router_probs = keras.activations.softmax(router_logits, axis=-1)\n        # Get the top\u22121 expert for each token. expert_gate is the top\u22121 probability\n        # from the router for each token. expert_index is what expert each token\n        # is going to be routed to.\n        expert_gate, expert_index = tf.math.top_k(router_probs, k=1)\n        # expert_mask shape: [tokens_per_batch, num_experts]\n        expert_mask = tf.one_hot(expert_index, depth=self.num_experts)\n        # Compute load balancing loss.\n        aux_loss = load_balanced_loss(router_probs, expert_mask)\n        self.add_loss(aux_loss)\n        # Experts have a fixed capacity, ensure we do not exceed it. Construct\n        # the batch indices, to each expert, with position in expert make sure that\n        # not more that expert capacity examples can be routed to each expert.\n        position_in_expert = tf.cast(\n            tf.math.cumsum(expert_mask, axis=0) * expert_mask, tf.dtypes.int32\n        )\n        # Keep only tokens that fit within expert capacity.\n        expert_mask *= tf.cast(\n            tf.math.less(\n                tf.cast(position_in_expert, tf.dtypes.int32), self.expert_capacity\n            ),\n            tf.dtypes.float32,\n        )\n        expert_mask_flat = tf.reduce_sum(expert_mask, axis=-1)\n        # Mask out the experts that have overflowed the expert capacity.\n        expert_gate *= expert_mask_flat\n        # Combine expert outputs and scaling with router probability.\n        # combine_tensor shape: [tokens_per_batch, num_experts, expert_capacity]\n        combined_tensor = tf.expand_dims(\n            expert_gate\n            * expert_mask_flat\n            * tf.squeeze(tf.one_hot(expert_index, depth=self.num_experts), 1),\n            -1,\n        ) * tf.squeeze(tf.one_hot(position_in_expert, depth=self.expert_capacity), 1)\n        # Create binary dispatch_tensor [tokens_per_batch, num_experts, expert_capacity]\n        # that is 1 if the token gets routed to the corresponding expert.\n        dispatch_tensor = tf.cast(combined_tensor, tf.dtypes.float32)\n\n        return dispatch_tensor, combined_tensor","ea213c04":"class Switch(layers.Layer):\n    def __init__(self, num_experts, embed_dim, num_tokens_per_batch, capacity_factor=1):\n        self.num_experts = num_experts\n        self.embed_dim = embed_dim\n        self.experts = [\n            create_feedforward_network(embed_dim) for _ in range(num_experts)\n        ]\n        self.num_tokens_per_batch = num_tokens_per_batch\n        self.expert_capacity = num_tokens_per_batch \/\/ self.num_experts\n        self.router = Router(self.num_experts, self.expert_capacity)\n        super(Switch, self).__init__()\n\n    def call(self, inputs):\n        batch_size = tf.shape(inputs)[0]\n        num_tokens_per_example = tf.shape(inputs)[1]\n\n        # inputs shape: [num_tokens_per_batch, embed_dim]\n        #self.num_tokens_per_batch\n        inputs = tf.reshape(inputs, [-1, self.embed_dim])\n        # dispatch_tensor shape: [expert_capacity, num_experts, tokens_per_batch]\n        # combine_tensor shape: [tokens_per_batch, num_experts, expert_capacity]\n        dispatch_tensor, combine_tensor = self.router(inputs)\n        # expert_inputs shape: [num_experts, expert_capacity, embed_dim]\n        expert_inputs = tf.einsum(\"ab,acd->cdb\", inputs, dispatch_tensor)\n        expert_inputs = tf.reshape(\n            expert_inputs, [self.num_experts, self.expert_capacity, self.embed_dim]\n        )\n        # Dispatch to experts\n        expert_input_list = tf.unstack(expert_inputs, axis=0)\n        expert_output_list = [\n            self.experts[idx](expert_input)\n            for idx, expert_input in enumerate(expert_input_list)\n        ]\n        # expert_outputs shape: [expert_capacity, num_experts, embed_dim]\n        expert_outputs = tf.stack(expert_output_list, axis=1)\n        # expert_outputs_combined shape: [tokens_per_batch, embed_dim]\n        expert_outputs_combined = tf.einsum(\n            \"abc,xba->xc\", expert_outputs, combine_tensor\n        )\n        # output shape: [batch_size, num_tokens_per_example, embed_dim]\n        outputs = tf.reshape(\n            expert_outputs_combined,\n            [batch_size, num_tokens_per_example, self.embed_dim],\n        )\n        return outputs","72819872":"class TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ffn, dropout_rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        # The ffn can be either a standard feedforward network or a switch\n        # layer with a Mixture of Experts.\n        self.ffn = ffn\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(dropout_rate)\n        self.dropout2 = layers.Dropout(dropout_rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)","d19a3406":"def get_model(config):\n    switch = Switch(config.num_experts, config.embed_dim, config.num_tokens_per_batch)\n    transformer_block = TransformerBlock(config.ff_dim, config.num_heads, switch)\n\n    inputs = layers.Input(shape=(config.num_tokens_per_example,))\n    embedding_layer = TokenAndPositionEmbedding(\n        config.num_tokens_per_example, config.vocab_size, config.embed_dim\n    )\n    x = embedding_layer(inputs)\n    x = transformer_block(x)\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dropout(config.dropout_rate)(x)\n    x = layers.Dense(config.ff_dim, activation=\"relu\")(x)\n    x = layers.Dropout(config.dropout_rate)(x)\n    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n\n    classifier = keras.Model(inputs=inputs, outputs=outputs)\n    return classifier","2ccaa145":"config.vocab_size = len(new_tokenizer.index_word) + 1\nprint(config.vocab_size)","13a555ae":"model = get_model(config)\nkeras.utils.plot_model(model, show_shapes=True)","b89bfeec":"model.summary()","c96c5230":"models = []\ntf.keras.backend.clear_session()\nfor index, (train_indices, val_indices) in enumerate(StratifiedKFold(5, shuffle=True, random_state=42).split(x_train, y_train)):\n    print(\"Fold %d\" %(index))\n    train_idx = len(train_indices) \/\/ config.batch_size * config.batch_size\n    train_indices = train_indices[:train_idx]\n    val_idx = len(val_indices) \/\/ config.batch_size * config.batch_size\n    val_indices = val_indices[:val_idx]\n    train_features, train_targets = x_train[train_indices], y_train[train_indices]\n    validation_features, validation_targets = x_train[val_indices], y_train[val_indices]\n    model_checkpoint_path = \"model%d.h5\"%(index)\n    model = get_model(config)\n    loss =\"binary_crossentropy\"\n    postive_rate = train_targets.mean()\n    class_weight = {0: postive_rate, 1: 1 - postive_rate}\n    adam = tf.keras.optimizers.Adam(config.learning_rate)\n    model.compile(loss=loss, optimizer=adam, metrics=[\"accuracy\"])\n    early_stop = tf.keras.callbacks.EarlyStopping(patience=5)\n    recuce_Lr = tf.keras.callbacks.ReduceLROnPlateau(patience=2)\n    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(model_checkpoint_path, monitor=\"val_accuracy\", save_best_only=True, save_weights_only=True)\n    history = model.fit(train_features, train_targets, \n                        validation_data=(validation_features, validation_targets), \n                        batch_size=config.batch_size, epochs=100, \n                        callbacks=[early_stop, model_checkpoint], class_weight=class_weight\n                       )\n    pd.DataFrame(history.history).plot(kind=\"line\")\n    plt.title(\"Performance of Fold %d\"%(index))\n    plt.show()\n    model.load_weights(model_checkpoint_path)\n    y_val_pred = np.array(model.predict(validation_features) > 0.5, dtype=\"int\").reshape(-1)\n    cm = confusion_matrix(validation_targets, y_val_pred)\n    sns.heatmap(cm)\n    plt.show()\n    print(\"Classification Report: \\n\")\n    print(classification_report(validation_targets, y_val_pred))\n    acc_score = accuracy_score(validation_targets, y_val_pred)\n    print(\"Accuracy Score: %.2f\"%(acc_score))\n    models.append(model)","910218ba":"y_test = np.mean([model.predict(x_test).reshape(-1) for model in models], axis=0)\ny_test = np.array(y_test > 0.5, dtype=int)\nsubmission = pd.DataFrame({\"id\": test[\"id\"], \"target\": y_test})\nsubmission.to_csv(config.submit_filename, index=False)\nif not config.is_kaggle_platform:\n  !kaggle competitions submit $config.dataset_name -m \"Submission\" -f $config.submit_filename","eec9fcd5":"### Text Classification Model","79382523":"###  Switch layer","f9fe2780":"Let's see first 100 words. Choose stop words based on that. But remove some words related to disasters.","7a2e9775":"## References\n\n* [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https:\/\/arxiv.org\/abs\/2101.03961)\n* [Text classification with Switch Transformer]( https:\/\/keras.io\/examples\/nlp\/text_classification_with_switch_transformer\/)","3d646a9c":"### Transformer Block","ca34e371":"## Lengths","07813037":"### Remove stop words","ba0e1dee":"# Disaster Tweets Classification: Switch Transformer","1c1d198d":"### Number of words","f1033033":"### Implement the feedforward network","f4c8f21b":"### Implement the load-balanced loss\nThis is an auxiliary loss to encourage a balanced load across experts.","70f3ab4e":"### KFold Training","ca7f7865":"## Submission","94478629":"### Implement the router as a layer","c95e1d86":"### Remove words that seldom appears","510f6d1f":"### Visulize architecture of the Model","0d038082":"### Implement token & position embedding layer\nIt consists of two seperate embedding layers, one for tokens, one for token index (positions).","983066ff":"# Overview\nIn this notebook I will build a Disaster Tweets Classification Model with Switch Transformer. I also make this notebook compatible with Colab and maybe other platforms. When you use this notebook in Colab, it's even more convinient that you can submit your result automatically.","b1ab8891":"## Setup","e2fb419b":"## EDA & Preprocessing\n- Tokenize Texts\n- Remove words that seldom appears\n- Remove stop words","0ed19265":"### Calculate Vocabulary Size","cfc7641e":"## Data Wrangling\nLet's see null values for each column.","244b194a":"## Model Development","f8b8d789":"## Table of Contents\n- Overview\n- Configuration\n- Import Packages and Datasets\n- Data Wrangling\n- EDA & Preprocessing\n- Model Development\n- Model Evaluation\n- Submission\n- References","b00ce302":"Create a new tokenizer to preprocess these texts again.","4d320ba9":"## Tokenize texts","fa32ed9a":"There's slightly Class Imbalance Problem.","c1a99ea7":"There are 25000 words just appear once and 3000 words appear twice. It would be hard for us to find patterns in them without prior knowledge. Machine can't learn from words just appear once or twice."}}