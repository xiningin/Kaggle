{"cell_type":{"4855ebef":"code","02cd9454":"code","43a36369":"code","df75aec1":"code","366b12e4":"code","86ec163d":"code","58b4adf6":"code","9b33851b":"code","16c68c2d":"code","6d4da87f":"code","31afb742":"code","11e1bec4":"code","7b0bcc5b":"code","8c4abd11":"code","7c0c0f3d":"code","0a6e692b":"code","d4e31e5a":"code","ae894c3a":"code","248a8a60":"code","0e221dcf":"code","da860a12":"code","a2de4433":"code","83093b4e":"code","a941ddd4":"code","1be4067e":"code","582c2bfd":"code","5e0213c5":"markdown","7180217b":"markdown","f49d2fa4":"markdown","6a70772d":"markdown","64548a8e":"markdown","5f5958a9":"markdown"},"source":{"4855ebef":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom scipy.stats import norm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error\nimport xgboost as xgb\n\nimport time\nimport warnings\nwarnings.simplefilter('ignore')","02cd9454":"# adjust Jupyter views\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\npd.set_option('display.expand_frame_repr', False)","43a36369":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","df75aec1":"# Read and Load data\nallstate=pd.read_csv('\/kaggle\/input\/allstate-claims-severity\/train.csv', index_col='id')\ntest=pd.read_csv('\/kaggle\/input\/allstate-claims-severity\/test.csv', index_col='id')\nsubmission=pd.read_csv('\/kaggle\/input\/allstate-claims-severity\/sample_submission.csv', index_col='id')\nprint(allstate.shape, test.shape, submission.shape)","366b12e4":"allstate.head()","86ec163d":"allstate.info(verbose=True)","58b4adf6":"allstate.describe()","9b33851b":"# evaluate whether any value is missing in train\nallstate.isnull().values.any()","16c68c2d":"# evaluate whether any value is missing in test\ntest.isnull().values.any()","6d4da87f":"# asses target\nsns.distplot(allstate['loss'], kde=False, fit=norm)","31afb742":"allstate.describe()['loss']","11e1bec4":"percent_outliers = sum(allstate['loss']>20000)\/len(allstate['loss'])*100\nprint('Loss less than 20000 accounts for {0:.2f}% of data'.format(percent_outliers))","7b0bcc5b":"# remove data where loss is more than 20000\ntrain=allstate.drop(allstate[allstate['loss']>20000].index)\nlen(train)","8c4abd11":"X = train.copy()\ny = np.log(X.pop('loss'))","7c0c0f3d":"non_scalar=list(X.select_dtypes(np.object))\n\nfor i in non_scalar:\n    X[i] = X[i].astype('category').cat.codes\n\nX.head()","0a6e692b":"scaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)","d4e31e5a":"X_train, X_val, y_train, y_val = train_test_split(X_scaled,y,test_size=.1, random_state=43)","ae894c3a":"print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\nprint(test.shape, submission.shape)","248a8a60":"# Loading data into DMatrices\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndval = xgb.DMatrix(X_val, label=y_val)","0e221dcf":"# Build a baseline model\n\nmean_train = np.mean(y_train)\nbaseline_predictions = np.ones(y_val.shape) * mean_train\n\nmae_baseline = mean_absolute_error(y_val, baseline_predictions)\nprint(\"Baseline MAE is {:.2f}\".format(mae_baseline))","da860a12":"# Let\u2019s define it with default values for the moment\nparams = {\n    # Parameters that we are going to tune.\n    'max_depth':6,\n    'min_child_weight': 1,\n    'eta':.3,\n    'subsample': 1,\n    'colsample_bytree': 1,\n    'objective':'reg:squarederror',\n}","a2de4433":"params['eval_metric'] = \"mae\"\nnum_boost_round = 999   # set it to large value","83093b4e":"model = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dval, \"Val\")],\n    early_stopping_rounds=10\n)","a941ddd4":"print(\"Best MAE: {:.5f} with {} rounds\".format(\n                 model.best_score,\n                 model.best_iteration+1))","1be4067e":"# cross-validation score with our current default parameters\ncv_results = xgb.cv(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    seed=43,\n    nfold=5,\n    metrics={'mae'},\n    early_stopping_rounds=10\n)\ncv_results","582c2bfd":"cv_results['test-mae-mean'].min()","5e0213c5":"### Train Test split","7180217b":"### Importing Libraries","f49d2fa4":"### Problem Statement\n\nCreating an algorithm which accurately predicts claims severity","6a70772d":"[](http:\/\/)","64548a8e":"### Reading the dataset","5f5958a9":"### Building a baseline model"}}