{"cell_type":{"430554b3":"code","ff2f169e":"code","57d5d316":"code","ea656d27":"code","41e5565c":"code","306a235c":"code","f6603533":"code","6e211e8e":"code","910fce21":"code","2adffe82":"code","1900aba0":"code","699e0eca":"code","e935aa68":"code","98d50fd6":"code","fccdc5f1":"code","7af6ba92":"code","3168037e":"code","22c626b7":"code","37dfd597":"code","b066fa6a":"code","7363efa6":"code","d8a36ff7":"code","9cb5703f":"code","d72ecc18":"code","54e266b2":"code","e3046078":"code","53ad716d":"code","37ac683a":"code","b2a5806a":"markdown","c3531d1e":"markdown","1269e95f":"markdown","7f8c6e82":"markdown","5750b008":"markdown","12eeb5c2":"markdown","becacb7f":"markdown","8f99e704":"markdown","1d58ddf3":"markdown","606872f9":"markdown","64118903":"markdown","1bb8c1d6":"markdown","059cae81":"markdown","5facac04":"markdown","4c49e1c8":"markdown","86632a1c":"markdown","9a2fd47c":"markdown","8fca1a13":"markdown","61dae3b0":"markdown","14d93409":"markdown","c868aa36":"markdown","d8366175":"markdown","5ac345d3":"markdown","e6b8f19b":"markdown"},"source":{"430554b3":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz","ff2f169e":"from xgboost import XGBClassifier","57d5d316":"X = np.linspace(-2, 2, 7)\ny = X ** 3\n\nplt.scatter(X, y)\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$');","ea656d27":"xx = np.linspace(-2, 2, 100)\npredictions = [np.mean(y) for x in xx]\n\nX = np.linspace(-2, 2, 7)\ny = X ** 3\n\nplt.scatter(X, y);\nplt.plot(xx, predictions, c='red');","41e5565c":"xx = np.linspace(-2, 2, 200)\npredictions = [np.mean(y[X < 0]) if x < 0 else np.mean(y[X >= 0])\n              for x in xx]\n\nX = np.linspace(-2, 2, 7)\ny = X ** 3\n\nplt.scatter(X, y);\nplt.plot(xx, predictions, c='red');","306a235c":"def regression_var_criterion(X, y, t):\n    X_left, X_right = X[X < t], X[X >= t]\n    y_left, y_right = y[X < t], y[X >= t]\n    return np.var(y) - X_left.shape[0] \/ X.shape[0] * \\\n           np.var(y_left) - X_right.shape[0] \/ X.shape[0] * \\\n            np.var(y_right)","f6603533":"thresholds = np.linspace(-1.9, 1.9, 100)\ncrit_by_thres = [regression_var_criterion(X, y, thres) for thres in thresholds]\n\nplt.plot(thresholds, crit_by_thres)\nplt.xlabel('threshold')\nplt.ylabel('Regression criterion');","6e211e8e":"xx = np.linspace(-2, 2, 200)\n\ndef prediction(x, X, y):\n    if x >= 1.5:\n        return np.mean(y[X >= 1.5])\n    elif x < 1.5 and x >= 0:\n        return np.mean(y[(X >= 0) & (X < 1.5)])\n    elif x >= -1.5 and x < 0:\n        return np.mean(y[(X < 0) & (X >= -1.5)])\n    else:\n        return np.mean(y[X < -1.5])\n    \n    \npredictions = [prediction(x, X, y) for x in xx]\n\nX = np.linspace(-2, 2, 7)\ny = X ** 3\n\nplt.scatter(X, y);\nplt.plot(xx, predictions, c='red');","910fce21":"df = pd.read_csv('..\/input\/mlbootcamp5_train.csv',index_col='id')#, sep=';')","2adffe82":"df.head()","1900aba0":"df['age_years'] = (df.age \/ 365.25).astype('int')","699e0eca":"train_df = pd.get_dummies(df, columns=['cholesterol', \n                                       'gluc']).drop(['cardio'],\n                                                     axis=1)\ntarget = df.cardio","e935aa68":"train_df.head()","98d50fd6":"X_train, X_valid, y_train, y_valid = train_test_split(train_df.values, \n                                                      target.values,\n                                                     test_size=.3, \n                                                      random_state=17)","fccdc5f1":"tree = DecisionTreeClassifier(max_depth=3, random_state=17).fit(X_train, \n                                                                y_train)","7af6ba92":"from ipywidgets import Image\nfrom io import StringIO\nimport pydotplus ","3168037e":"dot_data = StringIO()\nexport_graphviz(tree, feature_names=train_df.columns, \n                out_file=dot_data, filled=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(value=graph.create_png())","22c626b7":"tree_pred_valid = tree.predict(X_valid)","37dfd597":"tree_acc_valid = accuracy_score(y_valid, tree_pred_valid)\ntree_acc_valid","b066fa6a":"%%time\ntree_params = {'max_depth': list(range(2, ))}\n\ntree_grid = GridSearchCV(DecisionTreeClassifier(random_state=17), \n                         tree_params, cv=5, scoring='accuracy') \n\ntree_grid.fit(X_train, y_train)","7363efa6":"plt.plot(tree_params['max_depth'], \n         tree_grid.cv_results_['mean_test_score'])\nplt.xlabel('Max depth')\nplt.ylabel('Mean CV accuracy');","d8a36ff7":"print(\"Best params:\", tree_grid.best_params_)\nprint(\"Best cross validaton score\", tree_grid.best_score_)","9cb5703f":"tuned_tree_acc_valid = accuracy_score(y_valid, \n                                      tree_grid.predict(X_valid))\ntuned_tree_acc_valid","d72ecc18":"(tuned_tree_acc_valid \/ tree_acc_valid - 1) * 100","54e266b2":"sub_df = pd.DataFrame(df.smoke.copy())\nsub_df['male']  = df.gender - 1\n\nsub_df['age_40_50'] = ((df.age_years >= 40) \n                       & (df.age_years < 50) ).astype('int')\nsub_df['age_50_55'] = ((df.age_years >= 50) \n                       & (df.age_years < 55) ).astype('int')\nsub_df['age_55_60'] = ((df.age_years >= 55) \n                       & (df.age_years < 60) ).astype('int')\nsub_df['age_60_65'] = ((df.age_years >= 60) \n                       & (df.age_years < 65) ).astype('int')\n\nsub_df['ap_hi_120_140'] = ((df.ap_hi >= 120) \n                           & (df.ap_hi < 140)).astype('int')\nsub_df['ap_hi_140_160'] = ((df.ap_hi >= 140) \n                           & (df.ap_hi < 160)).astype('int')\nsub_df['ap_hi_160_180'] = ((df.ap_hi >= 160) \n                           & (df.ap_hi < 180)).astype('int')\n\nsub_df['chol=1'] = (df.cholesterol == 1).astype('int')\nsub_df['chol=2'] = (df.cholesterol == 2).astype('int')\nsub_df['chol=3'] = (df.cholesterol == 3).astype('int')","e3046078":"sub_df.head()","53ad716d":"tree = DecisionTreeClassifier(max_depth=3, \n                              random_state=17).fit(sub_df, target)","37ac683a":"dot_data = StringIO()\nexport_graphviz(tree, feature_names=sub_df.columns, \n                out_file=dot_data, filled=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(value=graph.create_png())","b2a5806a":"<center>\n<img src=\"..\/..\/img\/ods_stickers.jpg\" \/>\n    \n## [mlcourse.ai](mlcourse.ai) \u2013 Open Machine Learning Course \nAuthor: [Yury Kashnitskiy](https:\/\/yorko.github.io) (@yorko). Edited by Anna Tarelina (@feuerengel). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/) license. Free use is permitted for any non-commercial purpose.","c3531d1e":"In the decision tree algorithm, the feature and the threshold for splitting are chosen according to some criterion. The commonly used criterion for regression is based on variance: $$\\large Q(X, y, j, t) = D(X, y) - \\dfrac{|X_l|}{|X|} D(X_l, y_l) - \\dfrac{|X_r|}{|X|} D(X_r, y_r),$$\nwhere $\\large X$ and $\\large y$ are a feature matrix and a target vector (correspondingly) for training instances in a current node, $\\large X_l, y_l$ and $\\large X_r, y_r$ are splits of samples $\\large X, y$ into two parts w.r.t. $\\large [x_j < t]$ (by $\\large j$-th feature and threshold $\\large t$), $\\large |X|$, $\\large |X_l|$, $\\large |X_r|$ (or, the same, $\\large |y|$, $\\large |y_l|$, $\\large |y_r|$) are sizes of appropriate samples, and $\\large D(X, y)$ is variance of answers $\\large y$ for all instances in $\\large X$:\n$$\\large D(X) = \\dfrac{1}{|X|} \\sum_{j=1}^{|X|}(y_j \u2013 \\dfrac{1}{|X|}\\sum_{i = 1}^{|X|}y_i)^2$$\nHere $\\large y_i = y(x_i)$ is the answer for the $\\large x_i$ instance. Feature index $\\large j$ and threshold $\\large t$ are chosen to maximize the value of criterion  $\\large Q(X, y, j, t)$ for each split.\n\nIn our 1D case,  there's only one feature so $\\large Q$ depends only on threshold $\\large t$ and training data $\\large X$ and $\\large y$. Let's designate it $\\large Q_{1d}(X, y, t)$ meaning that the criterion no longer depends on feature index $\\large j$, i.e. in 1D case $\\large j = 0$.\n\nCreate the plot of criterion $\\large Q_{1d}(X, y, t)$  as a function of threshold value $t$ on the interval $[-1.9, 1.9]$.","1269e95f":"Split data into train and holdout parts in the proportion of 7\/3 using `sklearn.model_selection.train_test_split` with `random_state=17`.","7f8c6e82":"Take a look at the SCORE table to estimate ten-year risk of fatal cardiovascular disease in Europe. [Source paper](https:\/\/academic.oup.com\/eurheartj\/article\/24\/11\/987\/427645).\n\n<img src='..\/..\/img\/SCORE2007-eng.png' width=70%>\n\nCreate binary features according to this picture:\n- $age \\in [40,50), \\ldots age \\in [60,65) $ (4 features)\n- systolic blood pressure: $ap\\_hi \\in [120,140), ap\\_hi \\in [140,160), ap\\_hi \\in [160,180),$ (3 features)\n\nIf the values of age or blood pressure don't fall into any of the intervals then all binary features will be equal to zero. Then we create decision tree with these features and additional ``smoke``, ``cholesterol``  and ``gender`` features. Transform the ``cholesterol`` to 3 binary features according to it's 3 unique values ( ``cholesterol``=1,  ``cholesterol``=2 and  ``cholesterol``=3). This method is called dummy-encoding or One Hot Encoding (OHE). Transform the ``gender`` from 1 and 2 into 0 and 1. It is better to rename it to ``male`` (0 \u2013 woman, 1 \u2013 man). In general, this is typically done with ``sklearn.preprocessing.LabelEncoder`` but here in case of only 2 unique values it's not necessary.\n\nFinally the decision tree is built using 12 binary features (without original features).\n\nCreate a decision tree with the limitation `max_depth=3` and train it on the whole train data. Use the `DecisionTreeClassifier` class with fixed `random_state=17`, but all other arguments (except for `max_depth` and `random_state`) should be set by default.\n\n**<font color='red'>Question 5.<\/font> What binary feature is the most important for heart disease detection (it is placed in the root of the tree)?**\n- Systolic blood pressure from 160 to 180 (mmHg)\n- Gender male \/ female\n- Systolic blood pressure from 140 to 160 (mmHg) **[+]**\n- Age from 50 to 55 (years)\n- Smokes \/ doesn't smoke\n- Age from 60 to 65 (years)","5750b008":"Let's make several steps to build the decision tree. Let's choose the symmetric thresholds equal to 0, 1.5 and -1.5 for partitioning. In the case of a regression task, the leaf outputs mean answer for all observations (instances) in this leaf.","12eeb5c2":"**In this assignment, we will find out how a decision tree works in a regression task, then will build and tune classification decision trees for identifying heart diseases.\nFill in the missing code in the cells marked \"Your code here\" and answer the questions in the [web form](https:\/\/docs.google.com\/forms\/d\/1hsrNFSiRsvgB27gMbXfQWpq8yzNhLZxuh_VSzRz7XhI).**","becacb7f":"Although it is possible to make figures in `sklearn` better, it is clear that the decision tree first checks whether the systolic blood pressure is in range from 140 to 160 mmHg.","8f99e704":"Draw the plot to show how mean accuracy is changing in regards to `max_depth` value on cross-validation.","1d58ddf3":"Train the decision tree on the dataset `(X_train, y_train)` with max depth equals to 3 and `random_state=17`. Plot this tree with `sklearn.tree.export_graphviz`, `dot` and `pydot`. You don't need to use quotes in the file names in order to make it work in a jupyter notebook. The commands starting from the exclamation mark are terminal commands that are usually run in terminal\/command line.","606872f9":"Then let's make splitting in each of the leaves' nodes. In the left branch (where previous split was $x < 0$) using the criterion $[x < -1.5]$, in the right branch (where previous split was $x \\geqslant 0$) with the following criterion $[x < 1.5]$. It gives us the tree of depth 2 with 7 nodes and 4 leaves. Create the plot of these tree predictions for $x \\in [-2, 2]$.","64118903":"Transform the features: create \"age in years\" (full age) and also create 3 binary features based on `cholesterol` and 3 more on `gluc`, where they are equal to 1, 2 or 3. This method is called dummy-encoding or One Hot Encoding (OHE). It is more convenient to use `pandas.get_dummmies.`. There is no need to use the original features `cholesterol` and `gluc` after encoding.","1bb8c1d6":"Make predictions for holdout data `(X_valid, y_valid)` with the trained decision tree. Calculate accuracy.","059cae81":"## 1. A simple example of regression using decision trees","5facac04":"## 2. Building a decision tree for predicting heart diseases\nLet's read the data on heart diseases. The dataset can be downloaded from the course repo from [here](https:\/\/github.com\/Yorko\/mlcourse.ai\/blob\/master\/data\/mlbootcamp5_train.csv) by clicking on `Download` and then selecting `Save As` option.\n\n**Problem**\n\nPredict presence or absence of cardiovascular disease (CVD) using the patient examination results.\n\n**Data description**\n\nThere are 3 types of input features:\n\n- *Objective*: factual information;\n- *Examination*: results of medical examination;\n- *Subjective*: information given by the patient.\n\n| Feature | Variable Type | Variable      | Value Type |\n|---------|--------------|---------------|------------|\n| Age | Objective Feature | age | int (days) |\n| Height | Objective Feature | height | int (cm) |\n| Weight | Objective Feature | weight | float (kg) |\n| Gender | Objective Feature | gender | categorical code |\n| Systolic blood pressure | Examination Feature | ap_hi | int |\n| Diastolic blood pressure | Examination Feature | ap_lo | int |\n| Cholesterol | Examination Feature | cholesterol | 1: normal, 2: above normal, 3: well above normal |\n| Glucose | Examination Feature | gluc | 1: normal, 2: above normal, 3: well above normal |\n| Smoking | Subjective Feature | smoke | binary |\n| Alcohol intake | Subjective Feature | alco | binary |\n| Physical activity | Subjective Feature | active | binary |\n| Presence or absence of cardiovascular disease | Target Variable | cardio | binary |\n\nAll of the dataset values were collected at the moment of medical examination.","4c49e1c8":"**<font color='red'>Question 4.<\/font> Is there a local maximum of accuracy on the built validation curve? Did `GridSearchCV` help to tune `max_depth` so that there's been at least 1% change in holdout accuracy?**\n(check out the expression (acc2 - acc1) \/ acc1 * 100%, where acc1 and acc2 are accuracies on holdout data before and after tuning `max_depth` with `GridSearchCV` respectively)?\n- yes, yes\n- yes, no **[+]**\n- no, yes\n- no, no","86632a1c":"**<font color='red'>Question 1.<\/font> Is the threshold value $t = 0$ optimal according to the variance criterion?**\n- Yes\n- No **[+]**","9a2fd47c":"# <center>Assignment #3. Fall 2018. Solution\n## <center> Decision trees for classification and regression","8fca1a13":"Let's split the data according to the following condition $[x < 0]$. It gives us the tree of depth 1 with two leaves. Let's create a similar plot for predictions of this tree.","61dae3b0":"Print the best value of `max_depth` where the mean value of cross-validation quality metric reaches maximum. Also compute accuracy on holdout data. All these computations are possible to make using the trained instance of the class `GridSearchCV`.","14d93409":"**<font color='red'>Question 2.<\/font> How many segments are there on the plot of tree predictions in the interval [-2, 2] (it is necessary to count only horizontal lines)?**\n- 2\n- 3\n- 4 **[+]**\n- 5","c868aa36":"Let's start from tree of depth 0 that contains all train instances. How will predictions of this tree look like for $x \\in [-2, 2]$? Create the appropriate plot using a pen, paper and Python if it is needed (without using `sklearn`).","d8366175":"Set up the depth of the tree using cross-validation on the dataset `(X_train, y_train)` in order to increase quality of the model. Use `GridSearchCV` with 5 folds. Fix `random_state=17` and change  `max_depth` from 2 to 10.","5ac345d3":"**<font color='red'>Question 3.<\/font> What 3 features are used to make predictions in the created decision tree?**\n- weight, height, gluc=3\n- smoke, age, gluc=3\n- age, weight, chol=3\n- age, ap_hi, chol=3 **[+]**","e6b8f19b":"Let's consider the following one-dimensional regression problem. It is needed to build the function $a(x)$ to approximate original dependency $y = f(x)$ using mean-squared error $min \\sum_i {(a(x_i) - f(x_i))}^2$."}}