{"cell_type":{"97e92dc6":"code","ad32fe6b":"code","0dff1b13":"code","5d875c88":"code","083b5846":"code","abb632b5":"code","cfd1012d":"code","88b6f2fb":"code","d0759d85":"code","df2cd8c9":"code","dc6ef0f8":"code","6b221f45":"code","ffdd06f1":"code","a65226be":"code","b1e3d746":"code","5bdc0966":"code","c0ed09bb":"code","683950e0":"code","678edf65":"code","de21ecf6":"code","058b1fed":"code","698f2fba":"code","a8a3bccc":"code","e519c955":"code","189820bf":"code","3dd63a90":"code","a77b17c6":"code","4d21e133":"code","fa8878ec":"code","1474a251":"code","ed5d81b3":"code","d5464613":"code","ab489a1c":"code","67997438":"code","87f3ea15":"code","97ae396b":"code","88bb3198":"code","e01fab83":"code","6ae2dd9a":"code","4127a6d6":"code","b57a6132":"code","ed8137b5":"code","a75be606":"markdown","295285a1":"markdown","35aa8a38":"markdown","3eb8db0d":"markdown","f762175d":"markdown","39799bd4":"markdown","6dafd838":"markdown","e3e96e66":"markdown","e069c22a":"markdown","bacbc2d9":"markdown"},"source":{"97e92dc6":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split","ad32fe6b":"## creating the datata for plynolimal regression","0dff1b13":"np.random.seed(0)\nn = 15\nx = np.linspace(0,10,n) + np.random.randn(n)\/5\ny = np.sin(x)+x\/6 + np.random.randn(n)\/10","5d875c88":"x","083b5846":"y","abb632b5":"X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=0)","cfd1012d":"def part1_scatter():\n    import matplotlib.pyplot as plt\n    %matplotlib notebook\n    plt.figure()\n    plt.scatter(X_train, y_train, label='training data')\n    plt.scatter(X_test, y_test, label='test data')\n    plt.legend(loc=4);\n   ","88b6f2fb":"part1_scatter()","d0759d85":"def part1_plot():\n    import matplotlib.pyplot as plt\n    %matplotlib notebook\n    plt.figure()\n    plt.plot(x,y)\n    plt.legend(loc=4);","df2cd8c9":"part1_plot()","dc6ef0f8":"## create a polynomial regression","6b221f45":"from sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures","ffdd06f1":"from sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures","a65226be":"X_train.reshape(11,1)\n## you have to convert this array into 2D array because polynomial feture wants the data that way","b1e3d746":"print(X_test)\n## 4 elements to amke 2d array we ned to do 4,1 reshape\nprint (X_test.reshape(4,1))","5bdc0966":"\n## we are iterating this for creating the poly nolimal function for different degree\n## like first degree polynomial third degree polynoimial etc\n\ndata_we_wil_try_predict = np.linspace(0,10,100)\n#print(data_we_wil_try_predict)\n## but to fit it we need to reshape just like we reshape the train data\ndata_we_wil_try_predict = data_we_wil_try_predict.reshape(100,1)\nres = np.zeros((4, 100))\n\npr=[]\nfor i,degree in enumerate([1,3,6,9]):\n    pol = PolynomialFeatures(degree)\n    #print (pol)\n    X_poly = pol.fit_transform(X_train.reshape(11,1))\n    ## you have to reshape this just like we did in deep learnng in nural network\n    #print (X_poly)\n    ##now for every degree we nee to predict\n    ## the value and store in a array\n    linreg = LinearRegression().fit(X_poly, y_train)\n    #print(linreg)\n    test = pol.fit_transform(data_we_wil_try_predict)\n    y = linreg.predict(test)\n    print(y.shape)\n    pr.append(y)\npr = np.array(pr)\npr.shape","c0ed09bb":"np.array(pr).shape","683950e0":"def plot_one(degree_predictions):\n    import matplotlib.pyplot as plt\n    %matplotlib notebook\n    plt.figure(figsize=(10,5))\n    plt.plot(X_train, y_train, 'o', label='training data', markersize=10)\n    plt.plot(X_test, y_test, 'o', label='test data', markersize=10)\n    for i,degree in enumerate([1,3,6,9]):\n        plt.plot(np.linspace(0,10,100), degree_predictions[i], alpha=0.8, lw=2, label='degree={}'.format(degree))\n    plt.ylim(-1,2.5)\n    plt.legend(loc=4)","678edf65":"plot_one(pr)","de21ecf6":"from sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics.regression import r2_score\n\ndef two():\n    r2_train = []\n    r2_test = []\n\n    for i in range(10):\n        pol = PolynomialFeatures(degree=i)\n\n        X_poly = pol.fit_transform(X_train.reshape(11,1))\n        linreg = LinearRegression().fit(X_poly, y_train)        \n        r2_train.append(linreg.score(X_poly, y_train))\n\n        X_test_poly = pol.fit_transform(X_test.reshape(4,1))\n        r2_test.append(linreg.score(X_test_poly, y_test))\n    print(np.array(r2_train).shape)\n    print(np.array(r2_test).shape)\n\n    return (np.array(r2_train), np.array(r2_test))","058b1fed":"two()","698f2fba":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import Lasso, LinearRegression\nfrom sklearn.metrics.regression import r2_score\n\npoly = PolynomialFeatures(degree=12)\n\nX_train_poly = poly.fit_transform(X_train.reshape(11,1))\nX_test_poly = poly.fit_transform(X_test.reshape(4,1))\n\nlin = LinearRegression()\nlin_fit = lin.fit(X_train_poly, y_train)\nlin_test = lin_fit.score(X_test_poly, y_test)\nprint(lin_test)","a8a3bccc":"las = Lasso(alpha=0.01, max_iter = 10000)\nlas_fit = las.fit(X_train_poly, y_train)\nlas_test = las_fit.score(X_test_poly, y_test)","e519c955":"las_test","189820bf":"!wget https:\/\/raw.githubusercontent.com\/msivalenka\/Mushroom-Dataset\/master\/mushrooms.csv","3dd63a90":"import pandas as pd","a77b17c6":"df = pd.read_csv('mushrooms.csv')","4d21e133":"df.head()","fa8878ec":"df2 = pd.get_dummies(df)","1474a251":"df2","ed5d81b3":"X_df2 = df2.drop('class_p',axis=1)","d5464613":"y_df2 = df2[['class_p']]","ab489a1c":"X_train2, X_test2, y_train2, y_test2 = train_test_split(X_df2, y_df2, random_state=0)","67997438":"X_subset = X_test2\ny_subset = y_test2","87f3ea15":"from sklearn.tree import DecisionTreeClassifier\n\ndtc = DecisionTreeClassifier(random_state=0)\ntree = dtc.fit(X_train2, y_train2)\nf_names = []\n\nfor i, importance in enumerate(tree.feature_importances_):\n    f_names.append([importance, X_train2.columns[i]])\n\nf_names.sort(reverse=True)\nf_names = np.array(f_names)\n#f_names = f_names[:5,1]\n","97ae396b":"f_names.tolist()","88bb3198":"from sklearn.svm import SVC\nfrom sklearn.model_selection import validation_curve\n\nsvc = SVC(kernel='rbf', C=1, random_state=0)\ngamma = np.logspace(-4,1,6)\ntrain_scores, test_scores = validation_curve(\n                        svc, X_subset, y_subset,\n                        param_name='gamma',\n                        param_range=gamma,\n                        scoring='accuracy'\n                        )\n\n#return (train_scores.mean(axis=1), test_scores.mean(axis=1))","e01fab83":"train_scores.mean(axis=1)","6ae2dd9a":"test_scores.mean(axis=1)","4127a6d6":"import matplotlib.pyplot as plt","b57a6132":"plt.plot(train_scores.mean(axis=1),test_scores.mean(axis=1))","ed8137b5":"def answer_seven():\n        \n    return (0.0001, 10.0, 0.1)","a75be606":"# Question 6\nFor this question, we're going to use the validation_curve function in sklearn.model_selection to determine training and test scores for a Support Vector Classifier (SVC) with varying parameter values. Recall that the validation_curve function, in addition to taking an initialized unfitted classifier object, takes a dataset as input and does its own internal train-test splits to compute results.\n\nBecause creating a validation curve requires fitting multiple models, for performance reasons this question will use just a subset of the original mushroom dataset: please use the variables X_subset and y_subset as input to the validation curve function (instead of X_mush and y_mush) to reduce computation time.\n\nThe initialized unfitted classifier object we'll be using is a Support Vector Classifier with radial basis kernel. So your first step is to create an SVC object with default parameters (i.e. kernel='rbf', C=1) and random_state=0. Recall that the kernel width of the RBF kernel is controlled using the gamma parameter.\n\nWith this classifier, and the dataset in X_subset, y_subset, explore the effect of gamma on classifier accuracy by using the validation_curve function to find the training and test scores for 6 values of gamma from 0.0001 to 10 (i.e. np.logspace(-4,1,6)). Recall that you can specify what scoring metric you want validation_curve to use by setting the \"scoring\" parameter. In this case, we want to use \"accuracy\" as the scoring metric.\n\nFor each level of gamma, validation_curve will fit 3 models on different subsets of the data, returning two 6x3 (6 levels of gamma x 3 fits per level) arrays of the scores for the training and test sets.\n\nFind the mean score across the three models for each level of gamma for both arrays, creating two arrays of length 6, and return a tuple with the two arrays.\n\ne.g.\n\nif one of your array of scores is\n\narray([[ 0.5,  0.4,  0.6],\n       [ 0.7,  0.8,  0.7],\n       [ 0.9,  0.8,  0.8],\n       [ 0.8,  0.7,  0.8],\n       [ 0.7,  0.6,  0.6],\n       [ 0.4,  0.6,  0.5]])\n\nit should then become\n\narray([ 0.5,  0.73333333,  0.83333333,  0.76666667,  0.63333333, 0.5])\n\nThis function should return one tuple of numpy arrays (training_scores, test_scores) where each array in the tuple has shape (6,).","295285a1":"Training models on high degree polynomial features can result in overly complex models that overfit, so we often use regularized versions of the model to constrain model complexity, as we saw with Ridge and Lasso linear regression.\n\nFor this question, train two models: a non-regularized LinearRegression model (default parameters) and a regularized Lasso Regression model (with parameters alpha=0.01, max_iter=10000) on polynomial features of degree 12. Return the $R^2$ score for both the LinearRegression and Lasso model's test sets.\n\nThis function should return one tuple (LinearRegression_R2_test_score, Lasso_R2_test_score)","35aa8a38":"## we have to encoded this with get_dummy() but it will create a long column","3eb8db0d":"Write a function that fits a polynomial LinearRegression model on the training data X_train for degrees 0 through 9. For each model compute the $R^2$ (coefficient of determination) regression score on the training data as well as the the test data, and return both of these arrays in a tuple.\n\nThis function should return one tuple of numpy arrays (r2_train, r2_test). Both arrays should have shape (10,)","f762175d":"# Question 5\nUsing X_train2 and y_train2 from the preceeding cell, train a DecisionTreeClassifier with default parameters and random_state=0. What are the 5 most important features found by the decision tree?\n\nAs a reminder, the feature names are available in the X_train2.columns property, and the order of the features in X_train2.columns matches the order of the feature importance values in the classifier's feature_importances_ property.\n\nThis function should return a list of length 5 containing the feature names in descending order of importance.","39799bd4":"Write a function that fits a polynomial LinearRegression model on the training data X_train for degrees 1, 3, 6, and 9. (Use PolynomialFeatures in sklearn.preprocessing to create the polynomial features and then fit a linear regression model) For each model, find 100 predicted values over the interval x = 0 to 10 (e.g. np.linspace(0,10,100)) and store this in a numpy array. The first row of this array should correspond to the output from the model trained on degree 1, the second row degree 3, the third row degree 6, and the fourth row degree 9","6dafd838":"## which feture is important find out","e3e96e66":"# this is a high degree polynomial to avoid over fitting we apply some penalty . so we use the Lasso","e069c22a":"# Part 2 - Classification\nHere's an application of machine learning that could save your life! For this section of the assignment we will be working with the UCI Mushroom Data Set stored in mushrooms.csv. The data will be used to train a model to predict whether or not a mushroom is poisonous. The following attributes are provided:\n\nAttribute Information:\n\ncap-shape: bell=b, conical=c, convex=x, flat=f, knobbed=k, sunken=s\ncap-surface: fibrous=f, grooves=g, scaly=y, smooth=s\ncap-color: brown=n, buff=b, cinnamon=c, gray=g, green=r, pink=p, purple=u, red=e, white=w, yellow=y\nbruises?: bruises=t, no=f\nodor: almond=a, anise=l, creosote=c, fishy=y, foul=f, musty=m, none=n, pungent=p, spicy=s\ngill-attachment: attached=a, descending=d, free=f, notched=n\ngill-spacing: close=c, crowded=w, distant=d\ngill-size: broad=b, narrow=n\ngill-color: black=k, brown=n, buff=b, chocolate=h, gray=g, green=r, orange=o, pink=p, purple=u, red=e, white=w, yellow=y\nstalk-shape: enlarging=e, tapering=t\nstalk-root: bulbous=b, club=c, cup=u, equal=e, rhizomorphs=z, rooted=r, missing=?\nstalk-surface-above-ring: fibrous=f, scaly=y, silky=k, smooth=s\nstalk-surface-below-ring: fibrous=f, scaly=y, silky=k, smooth=s\nstalk-color-above-ring: brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e, white=w, yellow=y\nstalk-color-below-ring: brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e, white=w, yellow=y\nveil-type: partial=p, universal=u\nveil-color: brown=n, orange=o, white=w, yellow=y\nring-number: none=n, one=o, two=t\nring-type: cobwebby=c, evanescent=e, flaring=f, large=l, none=n, pendant=p, sheathing=s, zone=z\nspore-print-color: black=k, brown=n, buff=b, chocolate=h, green=r, orange=o, purple=u, white=w, yellow=y\npopulation: abundant=a, clustered=c, numerous=n, scattered=s, several=v, solitary=y\nhabitat: grasses=g, leaves=l, meadows=m, paths=p, urban=u, waste=w, woods=d\n\n\n# The data in the mushrooms dataset is currently encoded with strings. These values will need to be encoded to numeric to work with sklearn. We'll use pd.get_dummies to convert the categorical variables into indicator variables.","bacbc2d9":"## Validation Curve. Model validation is used to determine how effective an estimator is on data that it has been trained on as well as how generalizable it is to new input. ... Note that any estimator that implements fit() and predict() and has an appropriate scoring mechanism can be used with this visualizer."}}