{"cell_type":{"ec3739af":"code","d184c223":"code","e7df4aaa":"code","f18093cb":"code","550ac8ae":"code","31f17fe6":"code","c20c0d1e":"code","572bfe15":"code","ea62da28":"code","fa63ebd1":"code","d3b2eb13":"code","0d5dac30":"code","82694654":"code","af8810f2":"code","d6ad7389":"code","7645aa25":"code","febede1f":"code","f5199ae4":"code","fe5e9d99":"code","d0195aaa":"code","9d2444db":"code","f640b30b":"code","1b2cbe87":"code","63878097":"code","24b0d4ad":"code","edce5bd1":"code","4d74d140":"code","d6d60bf9":"code","e6381037":"code","a6286c67":"markdown","2dc86687":"markdown","318c332d":"markdown","495ca5d7":"markdown","7c5d7778":"markdown","60a9c6fb":"markdown","0a4e8f05":"markdown","92b065e1":"markdown","f7d92321":"markdown","706542f7":"markdown","ce997c0c":"markdown","efa48232":"markdown","6fb0ccc5":"markdown","dd51c090":"markdown","413df509":"markdown","aba1bf3e":"markdown","e31b68f3":"markdown","5a9f4f9d":"markdown","a8971b3e":"markdown","bfd92f6a":"markdown","0d97aa19":"markdown","8dfdbec5":"markdown"},"source":{"ec3739af":"# GENERAL LIBRARIES\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# PRINT FILE PATH\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d184c223":"raw_data = pd.read_csv('\/kaggle\/input\/beginners-classification-dataset\/classification.csv')\ndf = raw_data.copy()\ndf.head()","e7df4aaa":"# Summary table\ndf.describe()","f18093cb":"# Visualization of age and interest vs success\nfig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (18,4))\n# age vs success\nax[0].scatter(df['age'], df['success'])\nax[0].set_title('Age vs. Success')\nax[0].set_xlabel('Age')\nax[0].set_ylabel('Success')\n# interest vs success\nax[1].scatter(df['interest'], df['success'])\nax[1].set_title('Interest vs. Success')\nax[1].set_xlabel('Interest')\nax[1].set_ylabel('Success')\n# Show plot\nplt.show()","550ac8ae":"# Checkpoint 1\nunsplitted_data = df.copy()\ninputs = unsplitted_data.drop('success', axis = 1)\ntargets = unsplitted_data['success']","31f17fe6":"# Splitting data library\nfrom sklearn.model_selection import train_test_split","c20c0d1e":"x_train_unscaled, x_test_unscaled, y_train, y_test = train_test_split(inputs, targets, train_size = 0.8, shuffle = True,\n                                                                     random_state = 42)\nx_train_unscaled = x_train_unscaled.reset_index(drop = True)\nx_test_unscaled = x_test_unscaled.reset_index(drop = True)\ny_train = y_train.reset_index(drop = True)\ny_test = y_test.reset_index(drop = True)\n\nprint(\"X-train shape {}, X-test shape {}, Y-train shape {}, Y-test shape {}\".format(x_train_unscaled.shape, \n                                                                                    x_test_unscaled.shape,\n                                                                                    y_train.shape, y_test.shape))","572bfe15":"# Scaling library\nfrom sklearn.preprocessing import StandardScaler","ea62da28":"scaler = StandardScaler()\nscaler.fit(x_train_unscaled)\nx_train_scaled = scaler.transform(x_train_unscaled)\nx_test_scaled = scaler.transform(x_test_unscaled)\nprint(\"X-train shape {}, X-test shape {}\".format(x_train_scaled.shape, x_test_scaled.shape))","fa63ebd1":"# Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics","d3b2eb13":"log_model = LogisticRegression()\nlog_model.fit(x_train_scaled, y_train)","0d5dac30":"bias_weights = [np.round(log_model.intercept_[0],4), np.round(log_model.coef_[0,0],4), np.round(log_model.coef_[0,1],4)]\ncol_names = inputs.columns.values.tolist()\ncol_names = [\"Bias\"] + col_names\nlog_model_summary = pd.DataFrame(data = bias_weights, index = col_names, columns = [\"Weights\"])\nlog_model_summary","82694654":"log_model.score(x_train_scaled, y_train)","af8810f2":"log_model_train_accuracy = round(log_model.score(x_train_scaled, y_train),4)\nlog_model_test_accuracy = round(log_model.score(x_test_scaled, y_test),4)\nprint(\"Train accuracy: {} \\nTest accuracy: {}\".format(log_model_train_accuracy, log_model_test_accuracy))","d6ad7389":"# Libraries\nfrom sklearn.ensemble import RandomForestClassifier","7645aa25":"rand_for = RandomForestClassifier()\nrand_for.fit(x_train_unscaled, y_train)","febede1f":"features = x_train_unscaled.columns.values.tolist()\nrand_for_summary = pd.DataFrame(rand_for.feature_importances_, index = features, columns = ['Importance'])\nrand_for_summary","f5199ae4":"rand_for_train_accuracy = round(rand_for.score(x_train_unscaled, y_train),4)\nrand_for_test_accuracy = round(rand_for.score(x_test_unscaled, y_test),4)\nprint(\"Train accuracy: {} \\nTest accuracy: {}\".format(rand_for_train_accuracy, rand_for_test_accuracy))","fe5e9d99":"# Libraries\nfrom sklearn.svm import SVC","d0195aaa":"svm_model = SVC()\nsvm_model.fit(x_train_scaled, y_train)","9d2444db":"svm_model_train_accuracy = round(svm_model.score(x_train_scaled, y_train),4)\nsvm_model_test_accuracy = round(svm_model.score(x_test_scaled, y_test),4)\nprint(\"Train accuracy: {} \\nTest accuracy: {}\".format(svm_model_train_accuracy, svm_model_test_accuracy))","f640b30b":"# Libraries\nfrom sklearn.neighbors import KNeighborsClassifier","1b2cbe87":"knn_model = KNeighborsClassifier()\nknn_model.fit(x_train_scaled, y_train)","63878097":"knn_model_train_accuracy = round(knn_model.score(x_train_scaled, y_train), 4)\nknn_model_test_accuracy = round(knn_model.score(x_test_scaled, y_test), 4)\nprint(\"Train accuracy: {} \\nTest accuracy: {}\".format(knn_model_train_accuracy, knn_model_test_accuracy))","24b0d4ad":"# Libraries\nimport tensorflow as tf","edce5bd1":"# Separate a validation set from training set\ntrain_inputs, validation_inputs, train_targets, validation_targets = train_test_split(x_train_scaled, y_train, \n                                                                                      train_size = 0.8, \n                                                                                      shuffle = True, random_state = 42)\ntrain_inputs = tf.convert_to_tensor(train_inputs)\nvalidation_inputs = tf.convert_to_tensor(validation_inputs)\ntrain_targets = tf.convert_to_tensor(train_targets)\nvalidation_targets = tf.convert_to_tensor(validation_targets)","4d74d140":"input_size = 2\noutput_size = 2\nhidden_layer_size = 300\n\nNN_model = tf.keras.Sequential([\n                               tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n                               tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n                               tf.keras.layers.Dense(output_size, activation = 'softmax')\n                               ])\n\nNN_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n\nbatch_size = 100\nmax_epochs = 50\nearly_stopping = tf.keras.callbacks.EarlyStopping(patience=2) \n\nNN_model.fit(train_inputs, train_targets,\n         #batch_size = batch_size,\n         epochs = max_epochs,\n         callbacks = [early_stopping],\n         validation_data = (validation_inputs, validation_targets),\n         verbose = 2)","d6d60bf9":"NN_model_train_accuracy = round(NN_model.evaluate(x_train_scaled, y_train)[1],4)\nNN_model_test_accuracy = round(NN_model.evaluate(x_test_scaled, y_test)[1],4)\nprint(\"Train accuracy: {} \\nTest accuracy: {}\".format(NN_model_train_accuracy, NN_model_test_accuracy))","e6381037":"accuracy_values = {'Train Accuracy':[log_model_train_accuracy, rand_for_train_accuracy, svm_model_train_accuracy, \n                         knn_model_train_accuracy, NN_model_train_accuracy], \n                   'Test Accuracy': [log_model_test_accuracy, rand_for_test_accuracy, svm_model_test_accuracy, \n                         knn_model_test_accuracy, NN_model_test_accuracy]}\nindex_values = ['Logistic Regression', 'Random Forest', 'Support Vector Machine', 'K-Nearest Neighbors', 'Neural Network']\nsummary_accuracy = pd.DataFrame(accuracy_values, index = index_values)\nsummary_accuracy = summary_accuracy.sort_values(by = 'Test Accuracy', ascending = False)\nsummary_accuracy","a6286c67":"**Splitting data into train and test**","2dc86687":"**Standardize data**","318c332d":"## Compare between different classification algorithms:\n1. **Logistic Regression**\n2. **Random Forest**\n3. **Support Vector Machine**\n4. **K-Nearest Neighbors**\n5. **Neural Network**","495ca5d7":"**Accuracy for train and test sets**","7c5d7778":"## 3. Support Vector Machine","60a9c6fb":"**Fitting the model**","0a4e8f05":"**Accuracy for train and test sets**","92b065e1":"**Accuracy for train and test sets**","f7d92321":"**Fitting the Model**","706542f7":"**Fitting the model**","ce997c0c":"## 5. Neural Network","efa48232":"From the summary table, we can note:\n* We don't have null values (all counts are 297)\n* Success samples are balanced (0.57)\n* Ranges of age and interest variables","6fb0ccc5":"## Summary","dd51c090":"### Loading the data and data overview","413df509":"**Fitting the model**","aba1bf3e":"**Accuracy for train and test sets**","e31b68f3":"## 1. Logistic Regression","5a9f4f9d":"**Accuracy for train and test sets**","a8971b3e":"### Data Preprocessing","bfd92f6a":"## 4. K-Nearest Neighbors","0d97aa19":"## 2. Random Forest","8dfdbec5":"**Fitting the model**"}}