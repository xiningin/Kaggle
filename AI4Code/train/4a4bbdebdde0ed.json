{"cell_type":{"a053f64b":"code","4b083d92":"code","6050be88":"code","a4d37e84":"code","917c9897":"code","01c7b187":"code","005db038":"code","23383c11":"code","fb69d126":"code","9ce58d18":"code","eae9a51b":"code","14045387":"code","df0bfe2e":"code","e7c838de":"code","112b2044":"code","8803d85a":"code","c07d9184":"code","61a3f49d":"code","32fef226":"code","c4c0a76f":"code","5f61337d":"code","1a94f4aa":"code","71e0e3b1":"code","f89136ef":"code","446fc86c":"code","3cdfbd42":"code","503a8b5c":"code","d097146b":"code","12b134e1":"code","1a3f1c31":"code","0098629e":"code","592fd267":"code","40d6cf15":"code","e0ebb666":"code","1612257c":"code","9392b65a":"code","45b78ae8":"markdown","38b6d35f":"markdown","46a81061":"markdown","32b607c0":"markdown","51fdf341":"markdown","ba838242":"markdown","3294e903":"markdown","01980179":"markdown","235fe636":"markdown","2fe1e473":"markdown","d18d3b2f":"markdown","3ade1546":"markdown","2722089c":"markdown","6518d55f":"markdown","038e1e9a":"markdown","aded6f06":"markdown","58be9fdf":"markdown","1443fb1f":"markdown","913fa03c":"markdown","6c484e71":"markdown","5b5b0590":"markdown","826ab12b":"markdown","2c3a450e":"markdown","5a3bba7f":"markdown","a913a58a":"markdown","74e253f3":"markdown","0f17a63b":"markdown","9e8a75b5":"markdown","f3aae984":"markdown","782a342d":"markdown","5341493d":"markdown","6d21de80":"markdown","4108be6e":"markdown","db811518":"markdown","1484abc1":"markdown","66f1b706":"markdown","f1671500":"markdown","6f4051cd":"markdown","7d62a6be":"markdown","a6271b2f":"markdown","fb534d30":"markdown","be959051":"markdown"},"source":{"a053f64b":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","4b083d92":"import warnings\nwarnings.filterwarnings('ignore')","6050be88":"data= pd.read_excel('\/kaggle\/input\/covid19\/dataset.xlsx')\nprint('Dimensions: ',data.shape[0],'rows','x',data.shape[1],'columns')\ndata.head()","a4d37e84":"sns.set(font_scale=1.5)\nsns.set(rc={'axes.facecolor':'white', 'figure.facecolor':'white','figure.figsize':(10,5)})\nsns.set_style(\"white\")\n\ntg_values=data['SARS-Cov-2 exam result'].value_counts()\ntg_values.plot.barh(color=tuple([\"r\", \"black\"]))\nplt.title('SARS-Cov-2 exam result')\nprint(\"Negative exam results: \"+\"{:.2%}\".format(tg_values[0]\/tg_values.sum())+' ('+str(tg_values[0])+' records)')\nprint(\"Positive exam results: \"+\"{:.2%}\".format(tg_values[1]\/tg_values.sum())+'  ('+str(tg_values[1])+' records)')\nprint('')","917c9897":"#Labeling encode the target variable\ndef positive_bin(x):\n    if x == 'positive':\n        return 1\n    else:\n        return 0\ndata['SARS-Cov-2 exam result_bin']=data['SARS-Cov-2 exam result'].map(positive_bin)","01c7b187":"nulls=(data.isnull().sum()\/len(data))*100\nprint('Percentage (%) of nulls for each feature:')\nnulls.sort_values(ascending=False)\n","005db038":"sns.set(font_scale=1.5)\nsns.set(rc={'axes.facecolor':'white', 'figure.facecolor':'white','figure.figsize':(8,5)})\nsns.set_style(\"white\")\n\npos=data[data['SARS-Cov-2 exam result_bin']==1]\nneg=data[data['SARS-Cov-2 exam result_bin']==0]\n\nnulls_neg=(neg.isnull().sum().sort_values(ascending=False)\/len(neg))*100\nnulls_pos=(pos.isnull().sum().sort_values(ascending=False)\/len(pos))*100\n\nax=sns.distplot(nulls_pos[nulls_pos>0],color='red',bins=20,kde_kws={\"color\": \"red\", \"label\": \"Positive\"})\nax=sns.distplot(nulls_neg[nulls_neg>0],color='black',bins=20,kde_kws={\"color\": \"black\", \"label\": \"Negative\"})\n# ax=sns.distplot(nulls_neg[nulls_neg>0],color='black',kde=False, norm_hist=False,bins=20) # histogram\nax.set(xlabel='% of Nulls',title='Features nulls KDE (% Nulls > 0)',label='3')\nplt.grid(False)\nplt.show()","23383c11":"ax=sns.distplot(nulls[nulls>0],color='blue',bins=20,kde_kws={\"color\": \"blue\", \"label\": \"All Exam Results\"})\nax.set(xlabel='% of Nulls',title='Variables Nulls KDE (% Nulls > 0)')\nplt.grid(False)\nplt.show()","fb69d126":"variables_corr=nulls.loc[nulls<90].index.tolist()","9ce58d18":"corr = data[variables_corr].corr()['SARS-Cov-2 exam result_bin'].abs().sort_values(ascending=False)\ncorr","eae9a51b":"corr=data[variables_corr].corr().abs()\n\nfig, ax = plt.subplots(figsize=(15, 15))\ncolormap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, cmap=colormap, annot=True, fmt='.2f')\nplt.xticks(range(len(corr.columns)), corr.columns);\nplt.yticks(range(len(corr.columns)), corr.columns)\nplt.show()","14045387":"data['SARS-Cov-2 exam result_Baseline']=0\nprint(\"Baseline accuracy: \"+\"{:.2%}\".format((data['SARS-Cov-2 exam result_Baseline']==data['SARS-Cov-2 exam result_bin']).sum()\/len(data['SARS-Cov-2 exam result_Baseline'])))","df0bfe2e":"nulls.drop(['SARS-Cov-2 exam result','Patient ID','SARS-Cov-2 exam result_bin'],inplace=True)","e7c838de":"selecting_features=nulls.loc[nulls<90].index.tolist()\nfeatures=selecting_features\nfeatures.append('SARS-Cov-2 exam result_bin')","112b2044":"print(features)","8803d85a":"df=data[features]\n\ndef bins(x):\n    if x == 'detected' or x=='positive':\n        return 1\n    elif x=='not_detected' or x=='negative':\n        return 0\n    else:\n        return x\n    \nfor col in df.columns:\n    df[col]=df[col].apply(lambda row: bins(row))","c07d9184":"pd.set_option('display.max_columns', None)\ndf.describe()","61a3f49d":"from sklearn.impute import KNNImputer\n\nX=df.drop(['SARS-Cov-2 exam result_bin'],axis=1)\n\ntemp = X\nimputer = KNNImputer(n_neighbors=3)\ntemp = imputer.fit_transform(X.values)\n\nX = pd.DataFrame(temp, columns=X.columns)\ny = df['SARS-Cov-2 exam result_bin']\nX.head()","32fef226":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2,random_state=5)\nprint('Train shape:',X_train.shape)\nprint('Test  shape:',X_test.shape)","c4c0a76f":"from sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.metrics import log_loss, accuracy_score\n\nresultados1=[]\nkf=RepeatedKFold(n_splits=10, n_repeats=1, random_state=5)\nfor train,valid in kf.split(X_train):\n    \n    Xtr, Xvld = X_train.iloc[train], X_train.iloc[valid]\n    ytr, yvld = y_train.iloc[train], y_train.iloc[valid]\n    \n    rf= RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=0)\n    rf.fit(Xtr,ytr)\n    \n    p=rf.predict(Xvld)\n    acc=accuracy_score(yvld,p)\n    resultados1.append(acc)","5f61337d":"from sklearn.linear_model import LogisticRegression\n\nresultados2=[]\nkf=RepeatedKFold(n_splits=10, n_repeats=1, random_state=5)\nfor train,valid in kf.split(X_train):\n    \n    Xtr, Xvld = X_train.iloc[train], X_train.iloc[valid]\n    ytr, yvld = y_train.iloc[train], y_train.iloc[valid]\n    \n    lr= LogisticRegression(max_iter=50)\n    lr.fit(Xtr,ytr)\n    \n    p=lr.predict(Xvld)\n    acc=accuracy_score(yvld,p)\n    resultados2.append(acc)","1a94f4aa":"from sklearn.tree import DecisionTreeClassifier\n\nresultados3=[]\nkf=RepeatedKFold(n_splits=10, n_repeats=1, random_state=5)\nfor train,valid in kf.split(X_train):\n    \n    Xtr, Xvld = X_train.iloc[train], X_train.iloc[valid]\n    ytr, yvld = y_train.iloc[train], y_train.iloc[valid]\n    \n    dt= DecisionTreeClassifier(random_state=3, max_depth=8)\n    dt.fit(Xtr,ytr)\n    \n    p=dt.predict(Xvld)\n    acc=accuracy_score(yvld,p)\n    resultados3.append(acc)\n","71e0e3b1":"print('')\np1=rf.predict(X_test)\np1[:]=rf.predict(X_test)\nacc=accuracy_score(y_test,p1)\nprint(\"Vanilla Random Forest          Test accuracy: \"+\"{:.2%}\".format(acc)+' \/ Train accuracy: '+\"{:.2%}\".format(np.mean(resultados1)))\np2=lr.predict(X_test)\np2[:]=lr.predict(X_test)\nacc=accuracy_score(y_test,p2)\nprint(\"Vanilla Logistic Regression    Test accuracy: \"+\"{:.2%}\".format(acc)+' \/ Train accuracy: '+\"{:.2%}\".format(np.mean(resultados2)))\np3=dt.predict(X_test)\np3[:]=dt.predict(X_test)\nacc=accuracy_score(y_test,p3)\nprint(\"Vanilla Decision Tree          Test accuracy: \"+\"{:.2%}\".format(acc)+' \/ Train accuracy: '+\"{:.2%}\".format(np.mean(resultados3)))\nprint('')","f89136ef":"visual=pd.concat([X_test,y_test],axis=1)\nvisual['predict']=p2\nvisual2=visual[visual['SARS-Cov-2 exam result_bin']==visual['predict']]\n\nprint('Positive results in the test sample: ',visual[visual['SARS-Cov-2 exam result_bin']==1].shape[0])\nprint('Positive results correctly predicted: ',visual2[visual2['predict']==1].shape[0])\nprint('Only positives accuracy: ',\"{:.2%}\".format(visual2[visual2['predict']==1].shape[0]\/visual[visual['SARS-Cov-2 exam result_bin']==1].shape[0]))","446fc86c":"sns.set(font_scale=1.5)\nsns.set(rc={'axes.facecolor':'white', 'figure.facecolor':'white','figure.figsize':(8,5)})\nsns.set_style(\"white\")\n\npred_prob=lr.predict_proba(X_test)\n\nfrom sklearn.metrics import precision_recall_curve,roc_curve\n\nscores=pred_prob[:,1]\nprecision, recall, thresholds = precision_recall_curve(y_test, scores)\n\nplt.rcParams[\"axes.grid\"] = True\n\nplt.plot([1,0],[0,1],linestyle = '--',lw = 2,color = 'grey')\nplt.plot(recall[:-1],precision[:-1],label='Logistic Regression', color='red',lw=2, alpha=1)\nplt.xlim(0, 1)\nplt.ylim(0, 1)\nplt.grid(True)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision - Recall curve')\nplt.legend(loc=\"upper right\")\nplt.show()\n\nplt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'grey')\nscores=pred_prob[:,1]\nfpr, tpr, thresholds = roc_curve(y_test,scores)\nplt.plot(fpr,tpr,label='Logistic Regression', color='black',lw=2, alpha=1)\nplt.xlim(0, 1)\nplt.ylim(0, 1)\nplt.grid(True)\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc=\"lower right\")\nplt.show()","3cdfbd42":"from sklearn.metrics import plot_confusion_matrix\n\ndisp = plot_confusion_matrix(lr, X_test, y_test,display_labels=('Negative','Positive'),cmap=plt.cm.Reds,values_format='.00f')\ndisp.ax_.set_title('Confusion Matrix - Exam Results')\ndisp.ax_.grid(False)","503a8b5c":"from imblearn.over_sampling import SMOTE\n\nsm = SMOTE(random_state=2,n_jobs=-1,sampling_strategy=1)\nX_train_res, y_train_res = sm.fit_resample(X_train, y_train)","d097146b":"resultados3=[]\nkf=RepeatedKFold(n_splits=10, n_repeats=1, random_state=5)\nfor train,valid in kf.split(X_train_res):\n    \n    Xtr, Xvld = X_train_res.iloc[train], X_train_res.iloc[valid]\n    ytr, yvld = y_train_res.iloc[train], y_train_res.iloc[valid]\n    \n    rf_os= RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=0)\n    rf_os.fit(Xtr,ytr)\n    \n    p=rf_os.predict(Xvld)\n    acc=accuracy_score(yvld,p)\n    resultados3.append(acc)","12b134e1":"from imblearn.under_sampling import RandomUnderSampler\n\nrus = RandomUnderSampler(random_state=27,sampling_strategy=1)\nX_train_res2, y_train_res2 = rus.fit_resample(X_train, y_train)","1a3f1c31":"resultados3=[]\nkf=RepeatedKFold(n_splits=10, n_repeats=1, random_state=5)\nfor train,valid in kf.split(X_train_res2):\n    \n    Xtr, Xvld = X_train_res2.iloc[train], X_train_res2.iloc[valid]\n    ytr, yvld = y_train_res2.iloc[train], y_train_res2.iloc[valid]\n    \n    rf_us= RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=0)\n    rf_us.fit(Xtr,ytr)\n    \n    p=rf_us.predict(Xvld)\n    acc=accuracy_score(yvld,p)\n    resultados3.append(acc)\np1=rf_us.predict(X_test)\np1[:]=rf_us.predict(X_test)\nacc=accuracy_score(y_test,p1)","0098629e":"disp = plot_confusion_matrix(rf_us, X_test, y_test,display_labels=('Negative','Positive'),cmap=plt.cm.Reds,values_format='.00f')\ndisp.ax_.set_title('UnderSampling Confusion Matrix - Exam Results')\ndisp.ax_.grid(False)\n\ndisp = plot_confusion_matrix(rf_os, X_test, y_test,display_labels=('Negative','Positive'),cmap=plt.cm.Reds,values_format='.00f')\ndisp.ax_.set_title('OverSampling Confusion Matrix - Exam Results')\ndisp.ax_.grid(False)","592fd267":"pred_prob=rf_us.predict_proba(X_test)\n\nscores=pred_prob[:,1]\nprecision, recall, thresholds = precision_recall_curve(y_test, scores)\n\nplt.rcParams[\"axes.grid\"] = True\n\nplt.plot([1,0],[0,1],linestyle = '--',lw = 2,color = 'grey')\nplt.plot(recall[:-1],precision[:-1],label='Random Forest + Random Undersampling', color='red',lw=2, alpha=1)\nplt.xlim(0, 1)\nplt.ylim(0, 1)\nplt.grid(True)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision Recall curve')\nplt.legend(loc=\"upper right\")\nplt.show()\n\npred_prob=rf_os.predict_proba(X_test)\n\nscores=pred_prob[:,1]\nprecision, recall, thresholds = precision_recall_curve(y_test, scores)\n\nplt.plot([1,0],[0,1],linestyle = '--',lw = 2,color = 'grey')\nplt.plot(recall[:-1],precision[:-1],label='Random Forest + SMOTE Oversampling', color='black',lw=2, alpha=1)\nplt.xlim(0, 1)\nplt.ylim(0, 1)\nplt.grid(True)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision Recall curve')\nplt.legend(loc=\"upper right\")\nplt.show()","40d6cf15":"from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n\nmodel=sfs(RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=0),k_features=8,forward=False,verbose=2,cv=10,n_jobs=-1,scoring='accuracy')\nmodel.fit(X_train_res2,y_train_res2)","e0ebb666":"var=list(model.k_feature_names_)\nvar","1612257c":"X_train_res3=X_train_res2[var]\ny_train_res3=y_train_res2\n\nresultados3=[]\nkf=RepeatedKFold(n_splits=10, n_repeats=1, random_state=5)\nfor train,valid in kf.split(X_train_res3):\n    \n    Xtr, Xvld = X_train_res3.iloc[train], X_train_res3.iloc[valid]\n    ytr, yvld = y_train_res3.iloc[train], y_train_res3.iloc[valid]\n    \n    dt= RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=0)\n    dt.fit(Xtr,ytr)\n    \n    p=dt.predict(Xvld)\n    acc=accuracy_score(yvld,p)\n    resultados3.append(acc)","9392b65a":"disp = plot_confusion_matrix(dt, X_test[var], y_test,display_labels=('Negative','Positive'),cmap=plt.cm.Reds,values_format='.00f')\ndisp.ax_.set_title('Confusion Matrix - Exam Results')\ndisp.ax_.grid(False)","45b78ae8":"## Two teechniques were used:\n","38b6d35f":"## Stay safe!","46a81061":"### As a baseline, we'll be using a prediction off all exams as Negative. That give us 90,11% accuracy","32b607c0":"## Switching our thresold will bring different scenarios, but always trading-off between FN and Accuracy","51fdf341":"-  In this study, we should choose what is the most important metric;\n-  Maybe reducing FN is more important than improving the accuracy, but we can't have a very low accuracy: will be the same as predicting all pacients as positive.\n-  With more data and model improvings, I think we can make a very helpful model that will assist doctors to make decisions.","ba838242":"<a id='t7'><\/a>\n# <div>7. Final Conclusions<\/div>\n[Summary](#top)\n\n[Back](#t6)","3294e903":"<a id='t1'><\/a>\n# <div>1. Importings<\/div>\n[Summary](#top)\n\n[Next](#t2)","01980179":"### Positive and negative,separatedly","235fe636":"## Trying to improve the model using Backward selection\n### It should work better with a Custom Cost function, balancing accuracy and the recall","2fe1e473":"## Kernel Density Estimate (KDE) - Probability density function of % of nulls of features","d18d3b2f":"## Based on the **KDE**, I'm selecting features that have a maximum of 90% as missing values","3ade1546":" ### Variables with too long range, not good to substitute **NaNs** with 0.\n ### Using KNN imputer to substitute then, another approach could be substituting the nulls with the feature mean:","2722089c":"<a id='t4'><\/a>\n# <div>4. Model Selection<\/div>\n[Summary](#top)\n\n[Back](#t3)\n\n[Next](#t5)","6518d55f":"### Platelets and Leukocytes shows higher correlation, than the others features, with the target variable. Even so, that isn't a strong correlation","038e1e9a":"## In spite of having a high accuracy, we are dealing with a very low recall\n### This is a real problem. In my opinion, the worse result that we should mitigate are the **False Negatives(FN)**, as they can bring thee COVID back to society instead of putting them in quarentine","aded6f06":"<a id='top'><\/a>\n\n1. [Importings](#t1)\n\n2. [Exploratory Data Analysis](#t2)\n    \n3. [Feature Engineering](#t3)\n\n4. [Model Selection](#t4)\n\n5. [Resampling and fitting the train data](#t5)\n\n6. [Sequential Feature Selection](#t6)\n\n7. [Final Conclusions](#t7)\n\n\n\n\n","58be9fdf":"<a id='t5'><\/a>\n# <div>5. Resampling and fitting the train data<\/div>\n[Summary](#top)\n\n[Back](#t4)\n\n[Next](#t6)","1443fb1f":"## First of all, the dataset will be splitted in Train(80%) and Test(20%)","913fa03c":"## Both of the resampling techniques improves our recall (less FN) and worsens the accuracy","6c484e71":"## Analysing some statistical attributes from each feature, to solve out what to do with the NaNs values","5b5b0590":"### Similar results to the original model","826ab12b":"### All exam results","2c3a450e":"![](https:\/\/miro.medium.com\/max\/1400\/1*ENvt_PTaH5v4BXZfd-3pMA.png)\n[source](https:\/\/miro.medium.com\/max\/1400\/1*ENvt_PTaH5v4BXZfd-3pMA.png)","5a3bba7f":"<a id='t2'><\/a>\n# <div>2. Exploratory Data Analysis<\/div>\n[Summary](#top)\n\n[Back](#t1)\n\n[Next](#t3)","a913a58a":"## Training a Random Forest Classifier, Logistic Regression and a Decision Tree (10 folds CV)","74e253f3":"## Looking at the precision - recall curve, we can see that changing our threshold can improve our recall, but it will reduce the accuracy and the precision will fall very hardly.\n## What is the most important metric here?","0f17a63b":"## Corrrelation with SARS-Cov-2 exam result, for features with less than 90% nulls","9e8a75b5":"<a id='t6'><\/a>\n# <div>6. Sequential Feature Selection<\/div>\n[Summary](#top)\n\n[Back](#t5)\n\n[Next](#t7)","f3aae984":"<a id='t3'><\/a>\n# <div>3. Feature Engineering<\/div>\n[Summary](#top)\n\n[Back](#t2)\n\n[Next](#t4)","782a342d":"### Conclusions from this section:\n- As we can see there a lot of missing records;\n- Most of the variables have at least 80% of NaNs;\n- We are dealing with a very unbalanced dataset 9:1 negative\/positive results.","5341493d":"# Diagnosis of COVID-19 - Analysis of the trade-off between FN and Accuracy\n> ## AI and Data Science supporting clinical decisions (Task #1)\n> #### Rodrigo Fragoso","6d21de80":"*  **Only the train set will be resampled**\n*  Oversampling with Synthetic Minority Oversampling Technique(SMOTE)\n*  Undersampling with Random UnderSampler\n*  1:1 Positive \/ Negative Ratio\n*  Random Forest Classifier","4108be6e":"> #### Assuming all results as negative will be a good baseline","db811518":"## Conclusions:\n- In spite of having a \"high\" accuracy, there are lots of False Negatives(FN);\n- In this case, accuracy could not a good metric, but it still is important;\n- Maybe this is also happening because the data is too unbalanced;\n- Looking at the precision\/recall curve, we can see that changing the threshold will bring different results.","1484abc1":"### Correlation Heat Map, for features with less than 90% nulls","66f1b706":"#### Obs: we can see a very unbalanced dataset","f1671500":"### Results:","6f4051cd":"### The confusion matrix reforces that this model has lots of FNs","7d62a6be":"#### The main objective on this notebook is to explore and find how to deal with the NaN values and implement some Classifiers, sampling and feature engineering techniques to understand how the unbalance results can affect our models.","a6271b2f":"#### With a brief look, we can see that are lots of missing values on this dataset","fb534d30":"## Encode categorical features using label encoder","be959051":"## Conclusions:\n- Only features that have a maximum of 90% as missing values were selected;\n- Label encoder was used for categorical values;\n- KNN imputer was used to fill the NaN values."}}