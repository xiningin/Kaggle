{"cell_type":{"5f61072d":"code","5a58df5f":"code","8082e59f":"code","f906c3a5":"code","144a0298":"code","e102d490":"code","126a584d":"code","3e9d8df1":"code","02b35606":"code","a398c59d":"code","eb8080bc":"code","e84287dc":"code","170cad3c":"code","792eb444":"code","2b497876":"code","704403ff":"code","5bb5266f":"code","b4ddd0e6":"code","ef0b05f3":"code","18f48879":"code","ea476ce5":"code","f293e14d":"markdown","b3b80ed6":"markdown","990c5462":"markdown"},"source":{"5f61072d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nfrom xgboost import XGBRegressor\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5a58df5f":"# Read csv files. And keep a copy of the original dataframes. tran_df and test_df will be used throughout this notebook for preprocessing and scaling.we need original test dataframe in the end \nor_train_df = pd.read_csv('\/kaggle\/input\/neolen-house-price-prediction\/train.csv')\nor_test_df = pd.read_csv('\/kaggle\/input\/neolen-house-price-prediction\/test.csv')\n\ntrain_df = or_train_df\ntest_df = or_test_df","8082e59f":"# Dropping 'Id' column as it has no use in training\ntrain_df = train_df.drop(['Id'], axis=1)\ntest_df = test_df.drop(['Id'], axis=1)","f906c3a5":"print(f\"Shape of train set: {train_df.shape}\")\nprint(f\"Shape of test set: {test_df.shape}\")","144a0298":"str_columns = []\nnum_columns = []\n\ndef convert_to_str(df):\n    for col in df.columns:\n        if (df[col].dtype == np.int64 or df[col].dtype == np.float64):\n            df[col] = df[col].fillna(method='ffill')\n            if col not in num_columns:\n                num_columns.append(col)\n        else:\n            df[col] = df[col].fillna(method='ffill')\n            if col not in str_columns:\n                str_columns.append(col)\n    return df\n\ntrain_df = convert_to_str(train_df)\ntest_df = convert_to_str(test_df)","e102d490":"train_df['type'] = 'train'\ntest_df['type'] = 'test'\n\n# Add a dummy SalePrice column to test dataframe to make number of columns equal \ntest_df['SalePrice'] = train_df['SalePrice'].iloc[:201]\n\ndf = train_df.append(test_df, ignore_index=True)","126a584d":"for col in str_columns:\n    one_hot = pd.get_dummies(df[col])\n\n    replace_cols = {}\n    for one_col in one_hot.columns:\n        replace_cols[one_col] = f\"{col}_{one_col}\"\n    one_hot = one_hot.rename(columns=replace_cols)\n\n    df = df.drop(col, axis = 1)\n    df = df.join(one_hot)","3e9d8df1":"train_df = df[df['type'] == 'train']\ntest_df = df[df['type'] == 'test']\n\ntrain_df = train_df.drop(['type'], axis=1)\ntest_df = test_df.drop(['type'], axis=1)\n\ntest_df = test_df.reset_index(drop=True)","02b35606":"print(f\"Shape of train set: {train_df.shape}\")\nprint(f\"Shape of test set: {test_df.shape}\")","a398c59d":"scaler = StandardScaler()\nscaler.fit(train_df[num_columns])","eb8080bc":"train_df[num_columns] = scaler.transform(train_df[num_columns])\ntest_df[num_columns] = scaler.transform(test_df[num_columns])","e84287dc":"print(f\"Shape of train set: {train_df.shape}\")\nprint(f\"Shape of test set: {test_df.shape}\")","170cad3c":"test_df = test_df.drop(['SalePrice'], axis=1)","792eb444":"train_labels = train_df['SalePrice']\ntrain_data = train_df.drop(['SalePrice'], axis=1)","2b497876":"model = XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=0.7, colsample_bytree=0.8)","704403ff":"model.fit(train_data, train_labels)","5bb5266f":"predictions = model.predict(test_df)","b4ddd0e6":"test_df['SalePrice'] = predictions\ntest_df[num_columns] = scaler.inverse_transform(test_df[num_columns])\ntest_df = pd.DataFrame(test_df, columns=train_df.columns)","ef0b05f3":"# Create results dataframe\nresults = pd.DataFrame()\nresults['Id'] = or_test_df['Id']\nresults['SalePrice'] = test_df['SalePrice']","18f48879":"results.head()","ea476ce5":"results.to_csv('submissions.csv', index=False)","f293e14d":"## XGBoost regressor model ","b3b80ed6":"## Replace NaNs\n**While replacing we have to make sure to replace it with '0' (character) in case of a string column and 0 (number) if otherwise**","990c5462":"## Standardization of values"}}