{"cell_type":{"eab4ea6d":"code","ffe40c7b":"code","c4973859":"code","2ed41185":"code","5cca06ee":"code","b2faa670":"code","d5ac6e12":"code","8605aa92":"code","6affd7c1":"code","efd9c0f1":"code","887f0121":"code","304e969a":"code","5cf0676f":"markdown","b6452417":"markdown","5c25c7f2":"markdown","054f8e31":"markdown","52ab8ffa":"markdown","8b3c0a42":"markdown","e774aeaa":"markdown","93758200":"markdown","f23e2c9a":"markdown","622a8810":"markdown"},"source":{"eab4ea6d":"from typing import List, Tuple\nfrom operator import add\nfrom itertools import chain\n\nimport tensorflow as tf\nimport pandas as pd\nimport holoviews as hv\nfrom sklearn.datasets import load_digits, load_boston\nfrom sklearn.metrics import confusion_matrix\nfrom toolz.curried import *\n\n\nhv.extension('bokeh')\n\ndigits = load_digits()\n\ndigits_X = tf.convert_to_tensor(digits.data.astype('float32'))\ndigits_y = tf.convert_to_tensor(digits.target.astype('float32'))\ndigits_y_sparse = tf.one_hot(tf.dtypes.cast(digits_y, 'int32'), 9)","ffe40c7b":"class Node(tf.keras.layers.Layer):\n\n    def __init__(self, units:int=1, alpha:float = 0.1, l1:float = 0.1):\n        super(Node, self).__init__()\n        self.units = units\n        self.alpha = alpha\n        self.l1 = l1\n\n    def build(self, input_shape):\n        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n                                 initializer='random_normal',\n                                 trainable=True)\n        \n        self.b = self.add_weight(shape=(1,self.units),\n                                 initializer='random_normal',\n                                 trainable=True)\n\n    def call(self, inputs, probability):\n        \n        pr = tf.nn.sigmoid(tf.matmul(inputs, self.w) + self.b)\n        pr_one = tf.reshape(pr, (-1,))\n\n        \n        l1 = tf.reduce_sum(tf.abs(self.w))\n        self.add_loss(self.l1 * l1)\n        \n        p = tf.reduce_mean(pr)\n        self.add_loss(self.alpha * tf.keras.losses.binary_crossentropy([0.5], p))\n\n        \n        return (pr * inputs, pr_one * probability), ( (1 - pr) * inputs, (1-pr_one) * probability)\n\n    def get_config(self):\n        return {'units': self.units}\n\nclass Leaf(tf.keras.layers.Layer):\n\n    def __init__(self, units:int =32, l1:float = 0.1):\n        super(Leaf, self).__init__()\n        self.units = units\n        self.l1 = l1\n\n    def build(self, input_shape):\n        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n                                 initializer='random_normal',\n                                 trainable=True)\n        \n        self.b = self.add_weight(shape=(1,self.units),\n                                 initializer='random_normal',\n                                 trainable=True)\n\n    def call(self, inputs: tf.Tensor, probability):\n        \n        pr = tf.matmul(inputs, self.w) + self.b\n        \n        l1 = tf.reduce_sum(0. * tf.abs(self.w))\n        \n        self.add_loss(l1)\n        \n        return pr, probability\n\n    def get_config(self):\n        return {'units': self.units}\n\n    \nclass RegressionHead(tf.keras.layers.Layer):\n\n    def __init__(self, units:int = 8):\n        super(RegressionHead, self).__init__()\n\n    def build(self, input_shape):\n        pass\n\n    def call(self, inputs: tf.Tensor):\n        \n        weighted_inputs = [x[0] * tf.reshape(x[1], (-1,1)) for x in inputs]\n        \n        output = reduce(tf.add, weighted_inputs)\n        \n        return (output)\n\n    def get_config(self):\n        return {'units': self.units}\n\nclass ClassificationHead(tf.keras.layers.Layer):\n\n    def __init__(self, units:int = 8):\n        super(ClassificationHead, self).__init__()\n\n    def build(self, input_shape):\n        pass\n\n    def call(self, inputs: tf.Tensor):\n        \n        weighted_inputs = [tf.nn.softmax(x[0]) * tf.reshape(x[1], (-1,1)) for x in inputs]\n        \n        output = reduce(tf.add, weighted_inputs)\n        \n        return (output)\n\n    def get_config(self):\n        return {'units': self.units}\n\nclass SoftTree(tf.keras.Model):\n\n    def __init__(self, max_depth:int = 3, classes: int = 1, alpha:float = 0.01, l1:float=0., head = ClassificationHead(), **kwargs):\n        super(SoftTree, self).__init__(**kwargs)\n        \n        self.max_depth = max_depth\n            \n        self.nodes = [[Node(1, alpha=alpha, l1=l1) for _ in range(2**layer)] for layer in range(self.max_depth)]\n        self.leaves = [[Leaf(classes, l1=l1) for _ in range(2**(self.max_depth))]]\n        \n        self.tree = self.nodes + self.leaves\n        \n        self.head = ClassificationHead() if head is None else head\n        \n    def prototype(self, inputs: tf.Tensor):\n        input_to_layers = [ [[( inputs, tf.ones((tf.shape(inputs)[0],)) )]] ] + self.tree\n        proto_output = reduce(lambda x, f: self.forward(x,f), input_to_layers[:-1])\n        \n        return [x[0] for x in list(chain(*proto_output))]\n    \n    def leaf_probabilty(self, inputs: tf.Tensor):\n        input_to_layers = [ [[( inputs, tf.ones((tf.shape(inputs)[0],)) )]] ] + self.tree\n        proto_output = reduce(lambda x, f: self.forward(x,f), input_to_layers)\n        \n        return [x[1] for x in proto_output]\n    \n    def leaf(self, inputs: tf.Tensor):\n        \n        input_to_layers = [ [[( inputs, tf.ones((tf.shape(inputs)[0],)) )]] ] + self.tree\n        proto_output = reduce(lambda x, f: self.forward(x,f), input_to_layers)\n        \n        leaf_preductions = [x[0] for x in proto_output]\n    \n        return list(map(tf.nn.softmax, leaf_preductions))\n        \n    def forward(self, inputs: List[Tuple[tf.Tensor]], layer: List[tf.keras.Model]):\n        inputs = list(chain(*inputs))\n        joined = zip(inputs, layer)\n        return [f(x[0], x[1]) for x, f in joined]\n    \n    def call(self, inputs: tf.Tensor):\n        input_to_layers = [ [[( inputs, tf.ones((tf.shape(inputs)[0],)) )]] ] + self.tree\n        \n        leaf_output = reduce(lambda x, f: self.forward(x,f), input_to_layers)\n        \n        return self.head(leaf_output)\n    \nclass SoftRegressionTree(SoftTree):\n    def __init__(self,  max_depth:int = 3, classes: int = 1, alpha:float = 0.01, l1:float=0.,**kwargs):\n        super(SoftRegressionTree, self).__init__(max_depth, classes, alpha, l1, head=RegressionHead(), **kwargs)\n        \nclass SoftClassificationTree(SoftTree):\n    def __init__(self,  max_depth:int = 3, classes: int = 1, alpha:float = 0.01, l1:float=0.,**kwargs):\n        super(SoftClassificationTree, self).__init__(max_depth, classes, alpha, l1, head=ClassificationHead(), **kwargs)","c4973859":"digits_tree = SoftClassificationTree(max_depth=4, classes=9, alpha=0.025)\n\ndigits_tree.compile(loss='categorical_crossentropy', optimizer='adam')\n\ndigits_tree.fit(digits_X, digits_y_sparse, epochs=250, validation_split=0.1, batch_size=200)\n\ndigits_y_hat = digits_tree.predict(digits_X)","2ed41185":"# Leaf probabilities for first datapoint\nlead_prob = digits_tree.leaf_probabilty(digits_X)\n\nP = tf.concat([tf.reshape(x, (-1,1)) for x in lead_prob], 1)\n\npd.np.random.choice(range(1000), 5)\n\nhv.Bars(tf.random.shuffle(P)[1,:].numpy())","5cca06ee":"pd.DataFrame(confusion_matrix(digits_y_sparse.numpy().argmax(axis=1), \n                              digits_y_hat.argmax(axis=1)),\n             index = range(1,10),\n             columns = range(1,10))","b2faa670":"prototype_labels = []\nfor s, p in zip(digits_tree.leaf(digits_X), digits_tree.leaf_probabilty(digits_X)):\n    prototype_labels.append(s * tf.reshape(p, (-1,1)))\n\nprototype_labels = list(map(lambda x: (tf.reduce_mean(x, 0)\n                                 .numpy()\n                                 .argmax(0)), \n                            prototype_labels))\nprototypes = list(map(lambda x: tf.reduce_mean(x, 0), digits_tree.prototype(digits_X)))","d5ac6e12":"images = []\nfor i, (proto, label) in enumerate(zip(prototypes, prototype_labels)):\n    grid = (proto\n            .numpy()\n            .reshape((8,8)))\n    standard_grid = (grid - grid.min())\/grid.max()\n    \n    images.append(hv.Image(standard_grid).opts(title=f'Digit: {str(label)} ------------------------------ Leaf : {i+1}',\n                                               xlabel='', ylabel='', \n                                               xaxis=None, yaxis=None))\n\nreduce(add, images)","8605aa92":"boston = load_boston()\n\nboston_X = tf.convert_to_tensor(boston.data.astype('float32'))\nboston_X_z_score = tf.math.divide_no_nan(boston_X - tf.reduce_mean(boston_X, 0), tf.math.reduce_std(boston_X, 0))\nboston_y = tf.convert_to_tensor(boston.target.astype('float32'))\n","6affd7c1":"boston_tree = SoftRegressionTree(max_depth=1, classes=1, alpha=0.1, l1=1.)\n\nboston_tree.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(0.025))\nboston_tree.fit(boston_X_z_score, boston_y, epochs=50, validation_split=0.1, batch_size=200)\n\ny_hat = boston_tree.predict(boston_X_z_score)","efd9c0f1":"def weight_plots(index: int, leaf: tf.keras.layers.Layer) -> hv.plotting.Plot:\n    return (pipe(zip(boston.feature_names, \n                    leaf.w\n                        .numpy()\n                        .reshape((-1,))\n                        .tolist()),\n                hv.Bars)\n            .opts(xrotation=90, \n                  xlabel='Features', \n                  ylabel='Weights', \n                  title=f'Feature Importances at Leaf {index}'))","887f0121":"pipe(enumerate(boston_tree.tree[-1]), \n     map(lambda leaf: weight_plots(*leaf)),\n     reduce(add))","304e969a":"pipe(boston_X_z_score, # the data\n     boston_tree.leaf_probabilty, # compose leaf probabilities\n     map(lambda x: tf.reshape(x, (-1,1))), # reshape probabilities\n     list,\n     partial(tf.concat, axis=1), # contatenate vector\n     partial(tf.reduce_mean, axis=0), # get mean accross index\n     lambda x: x.numpy(), # convert to numpy\n     hv.Bars # gen polts\n    ).opts(title='% sample per leaf', \n           xlabel='Leaves', \n           ylabel='%')","5cf0676f":"One of the most interesting papers I read last year, was a paper by Nicholas Frosst and Geoffrey Hinton entitled \"Distilling a Neural Network Into a Soft Decision\". This paper extends on work by Irsoy et al. on a type of Neural Network most people have not heard of called a ***Soft Decicion Tree***.  What Soft Decision Trees are, are a series of *attention-like layers* follower by a *simple linear model* which are trained using stochastic gradient descent and backpropogation.  What makes these attention layers unique is that they are are structured like a decision tree, with parent nodes branching left and right into child nodes, masking the featue-space to send a fraction of the data either left or right. If you are to sum the left and right branches they will always end up the same as the original data, but if we have the price of a good in a given columns being \\$1, the network will learn weights to send \\$0.5 right and \\$0.5 left, or may learn to send a columns representing age as 3 yrs old, 2.7 years to the right branch and 0.3 years to the left branch.  This may seem rather unintuitive and may distort the data heavily, if we started with these feature representing age and price at the inputs to the model, we end at the final linear layer of the model with two columns representing age and price, and this means that at the leaves the linear model estimating the probabilities of the output can be directly.  If the leaf with the highest confidence makes a has a positive coefficient on age, we know for this prediction, age is most likely the important feature driving that prediction.  Whats the nodes of the Soft Decision Tree are effectively doing is learning the best way to partition the data, for the linear models at the leaves. This means that at each leaf we only describe a small neighbourhood of data and learn a model to make predictions on it. \n\nWhile Hinton and Frosst use this network architecture for 'Distilling' a neural network and explaining its predictions, having such a 'non-linear' linear model, which remains in neighbourhoods local but globally non-linear can provide amazing insights into your data and its latent properties. Which we will discuss!\n\nFor those interested in the code, there is a nice Tensorflow 2.0 (TF2) implementation I have written which may make for a great read for those getting into TF2, which shows how to create custom layers, use keras' new subclassing API and define custom regularization functions.  I will keep then code hidden by default to prevent anyone scared or having to scoll too much, but please have a look.  \n\n\n[1] Frosst, N. and Hinton, G., 2017. Distilling a neural network into a soft decision tree. arXiv preprint arXiv:1711.09784.\n[2] Irsoy, O., Y\u0131ld\u0131z, O.T. and Alpayd\u0131n, E., 2012, November. Soft decision trees. In Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012) (pp. 1819-1822). IEEE.\n","b6452417":"While we have focussed our attention up until this point on multiclass classification, we now turn out attention to a regression problem. In this problem, I have chosen to look at the Boston House Price dataset offer in Scikit-learn. This is a more challenging dataset to visualize, but which may present more realistic real-world application. Those those serving insights clients, it may be of interest to know why there house was prices in a particular way and possibly what they can do to change that.  Similarly for policy makes, this style of model may provide insights into latent factors which may mean one group of houses are positively correlated with number of bedrooms and other houses are negatively correlated. In this case, we may need to collect more data in investigate whether these factors may be driven by enironmental factors, distance to public facilities etc.  \n\nFor save of simplicity, I have opted to fit a very shallow Soft Decision Tree with just two leaves.  This makes our estimates more stable on this small dataset and makes interpretation easier. ","5c25c7f2":"# Boston Housing Prices","054f8e31":"While some of these weight-spaces look quite strange, other seem to correspond quite nicely with their most confident class.  What is intersting to observe is where the model has many leaves which point most confidently to the same digit. In this case you can often see the different way these digits are written. ","52ab8ffa":"The first dataset we will look at is Scikit-learn's default digits dataset. This is a slightly smaller dataset, similar in many ways to the famous MNIST digits dataset. This dataset size of the dataset and the fact that the digits are much lower resolution, make it a great prototyping dataset for model development.  One property I really like to this dataset, is not only its ability to be easily visualized, but also in that there are many ways to write particular digits in different parts of the world and in different schooling systems - some people write ones that look like sevens and others write ones to look like single long vertical lines.  Using Soft Decision Trees, we can hopefully pick up these differences by looking at the prototypes our model learns to identify how many ones there are.  \n\nFor this excercide I decided to go for a depth of 4 in our decision tree meaning we have 16 linear models a our leaves.  The model itself does take a little patience so I decided to opt for a very concervative learning rate and large batch size, which is recommended.  ","8b3c0a42":"In the graphs below, I visualize the weight-space of the linear models at the leaves of the Soft Decision Tree to try get a sense for what each leaf is looking for in making a prediction. As this is a multiclass classification problem, I have chosen to visualize the weights for the class each leaf predicts most confidently on average accross the training dataset.  In the top right of each plot I have the leaf number and in the top left I have the class that leaf most confidently predicts.  ","e774aeaa":"Below, I have decided to look, again, at the weight space of the two leaves. Interesting these weights present very different characteristics, suggesting some regions of the feature space may have a different relationship to house price to other regions. ","93758200":"I am definately to experiment with this method more to understand its applications. I think over the coming years, as the Machine Learning industry machures, explainability will remain increasingly important in companies strategic and product offerings.  I think that while post-hoc model-agnostic and gradient-based explanations are great, it will be important to find an efficient middle ground and have suite of tools for different applications. \n\nI would love to hear your feedback on the value of model explainability and the applications you see for Soft Decision Trees.  ","f23e2c9a":"While the aim of this model is not wholly predictive accuracy, after trianing our model we seem get fairly strong and consistent results for all digits except 1.  ","622a8810":"# Digits"}}