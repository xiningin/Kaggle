{"cell_type":{"5a80a97f":"code","c699277d":"code","c4c17ac2":"code","bec775b0":"code","f9621994":"code","d4588b9c":"code","e4179f03":"code","500c98a8":"code","bf6c99cb":"code","95214eb6":"code","081cc453":"code","54e2806a":"code","6735b63d":"code","ddd5376a":"code","7b737aff":"code","3421cf9a":"code","c2e39508":"code","5a8fdbd8":"code","2e2277ec":"code","235e47a1":"code","f7976f85":"code","a0d5ef31":"code","43f752bc":"code","265acc29":"code","87fcd8ae":"code","4d1b872b":"code","bca7c63f":"code","44674de0":"code","4601185f":"code","7c44e466":"code","e593c570":"code","95a2ba24":"code","67bb26ea":"code","8c4da28d":"code","b2dcaad5":"code","fa8b41c6":"code","6d19f3d1":"code","d1b4b59f":"code","be300928":"code","1d9bb581":"code","57eda4d1":"code","d5f4c4ec":"code","7bb3f000":"markdown","13930187":"markdown","4e557b84":"markdown","4a202c7b":"markdown","7b84e5ad":"markdown","56185139":"markdown","c20101c6":"markdown","201e9ea5":"markdown","c79090f1":"markdown","00fa633c":"markdown","9182ad9f":"markdown","2aedd7fb":"markdown","4bdf331d":"markdown","148d70ff":"markdown","f9a1bdc5":"markdown","cdea98ff":"markdown","43699ff1":"markdown","079f30de":"markdown","d703f954":"markdown","6553500c":"markdown","fda3753d":"markdown","dae0d58d":"markdown","d51c7778":"markdown","31885059":"markdown","7bbe77db":"markdown","2dd731ed":"markdown","97f38bbb":"markdown","f6cfb1d7":"markdown","f6b2944f":"markdown","1f6575c8":"markdown","4e45f442":"markdown"},"source":{"5a80a97f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# turn off warnings for final notebook\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_columns', None)\n%matplotlib inline\nsns.set_context('notebook')\nsns.set_palette('Set2')\nsns.set_style('darkgrid')","c699277d":"#Importing the dataset\ndf=pd.read_csv('..\/input\/students-performance-in-exams\/StudentsPerformance.csv')\ndf.head()","c4c17ac2":"# general information about the dataset\ndf.info()\ndf.isnull().sum().sort_values(ascending=False)","bec775b0":"num = df.select_dtypes(include=np.number)\nnum.columns","f9621994":"fig,ax = plt.subplots(figsize=(10,5))\n\nbplot=ax.boxplot(num,\n                 patch_artist=True,\n                labels=num.columns)\n\ncolors=['blue','red','green']\nfor patch, color in zip(bplot['boxes'], colors):\n    patch.set_facecolor(color)","d4588b9c":"# Frequency distribution\nfig, axs = plt.subplots(nrows=1,ncols=3, figsize=(20,5), sharey=True, tight_layout=True)\n\nbin_num=30\ncolors=['blue','red','green']\nprops = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n\nfor i in range(0,3):\n    n, bins, patches = axs[i].hist(num[num.columns[i]],bins=bin_num,color=colors[i])\n    axs[i].set_title(num.columns[i], size=20,fontweight='bold')\n    axs[i].axvline(x=num[num.columns[i]].mean())\n    mu = num[num.columns[i]].mean()\n    sigma = num[num.columns[i]].std()\n    textstr = '\\n'.join((\n        r'$\\mu=%.2f$' % (mu, ),\n        r'$\\sigma=%.2f$' % (sigma, )))\n    axs[i].text(0.82*i, 0.95, textstr, transform=ax.transAxes, fontsize=14,\n        verticalalignment='top', bbox=props)\n    \naxs[0].set_ylabel('Frequency', size=20,fontweight='bold') ","e4179f03":"num.describe()","500c98a8":"fig, ax = plt.subplots(figsize=(10, 8))\nax.set_title(\"Correlation Matrix\\n\", size=20,fontweight='bold')\nsns.heatmap(num.corr(), annot=True,ax=ax,);","bf6c99cb":"plt.subplots(figsize=(7, 7))\nsns.kdeplot(data=num, x=\"reading score\", y=\"writing score\", levels=50, color=\"b\",thresh=0,cmap=\"rocket\",fill=True)\nsns.kdeplot(data=num, x=\"math score\", y=\"writing score\", levels=50, color=\"b\",thresh=0,cmap=\"rocket\",fill=True)","95214eb6":"with sns.axes_style(\"white\"):\n    g = sns.pairplot(df, diag_kind=\"kde\",height= 4,corner=True,diag_kws={\"linewidth\": 0, \"shade\": False})\n    g.map_lower(sns.kdeplot,  levels=50, color=\"b\",thresh=0,cmap=\"rocket\",fill=True)","081cc453":"g = sns.pairplot(df, diag_kind=\"kde\", height=4,hue='gender',corner=False)\ng.map_lower(sns.kdeplot,  levels=50,hue=None,thresh=0,cmap=\"rocket\",fill=True)\n\ng.fig.text(0.33, 1.02,'Distribution of Test Scores by Gender', fontsize=20)","54e2806a":"from scipy.stats import ttest_ind\n'''\nFail to Reject H0: Sample distributions are equal.\nReject H0: Sample distributions are not equal.\n'''\nstat, p = ttest_ind(df['math score'][df['gender']=='female'], df['math score'][df['gender']=='male'])\nprint('Statistics=%.3f, p=%.3f \\n' % (stat, p))\n# interpret\nalpha = 0.05\nprint(\"Comparison between the math score of male and female students: \")\nif p > alpha:\n    print('Same distributions (fail to reject H0)')\nelse:\n    print('Different distributions (reject H0)')","6735b63d":"cat = df.select_dtypes(exclude=np.number)\nprint(\"List of categorical variables:\")\ncat.columns","ddd5376a":"def label_function(val):\n    return f'{val \/ 100 * len(df):.0f}\\n{val:.0f}%'\n\nfig, axs = plt.subplots(3,2, figsize=(15, 15))\nfor i in range(0,5):\n    cat.groupby(cat.columns[i]).size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 13}, cmap='tab20c', ax=axs[int((i-i%2)\/2),i%2])\n    axs[int((i-i%2)\/2),i%2].set_title(cat.columns[i], size=20,fontweight='bold')\n    axs[int((i-i%2)\/2),i%2].set_ylabel(None)\n\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()","7b737aff":"fig,ax = plt.subplots(5,3,figsize=(30, 20))\nfor i in range(0,5):\n    for j in range(0,3):\n        sns.violinplot(ax=ax[i,j], x=cat[cat.columns[i]], y=num[num.columns[j]], data=df)","3421cf9a":"fig,ax = plt.subplots(figsize=(15, 10))\nedu_order=order= [ \n        'some high school',\n        'high school',\n        'some college',\n        \"associate's degree\",\n        \"bachelor's degree\",  \n        \"master's degree\"]\nax = sns.violinplot(x=\"parental level of education\", y=\"writing score\", data=df, inner=None, order= edu_order)\nax = sns.swarmplot(x=\"parental level of education\", y=\"writing score\", data=df,\n                   color=\"white\", edgecolor=\"gray\",order= edu_order)\nt = ax.text(\n    0.2, 2, \"Small improvement in the writing scores along parental education\", ha=\"left\", va=\"center\", rotation=0, size=13,\n    bbox=dict(boxstyle=\"rarrow,pad=0.3\", fc=\"#CBC3E3\", ec=\"#301934\", lw=1))","c2e39508":"g = sns.FacetGrid(df, col=\"race\/ethnicity\", hue=\"gender\")\ng.map(sns.histplot, \"math score\", alpha=0.5)\ng.add_legend()","5a8fdbd8":"# We are going to process data\n#First we have to give different treatment to three classes: ordinal Categorical features, non-ordinal Categorical features and Numerical features.\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\n\ncat = df.select_dtypes(exclude=np.number)\nnum = df.drop(columns=cat.columns) # Numerical features\nprint(\"Number of unique values per categorical feature:\\n\", cat.nunique())\n\ncat_ord = cat[['parental level of education','gender']] # ordinal categorical features\ncat.drop(columns=['parental level of education','gender'],inplace=True) #non-ordinal categorical features\n\n\n#Encoding ordinal categorical features\n\n# define order\norder_1 = [ \n        'some high school',\n        'high school',\n        'some college',\n        \"associate's degree\",\n        \"bachelor's degree\",  \n        \"master's degree\"]\norder_2 =['male','female']\n\n# define ordinal encoding\nencoder = OrdinalEncoder(categories=[order_1,order_2])\n# transform data\nencoder.fit(cat_ord[['parental level of education','gender']])\ncat_ord_encoded = pd.DataFrame(encoder.transform(cat_ord[['parental level of education','gender']]))\n\ncat_ord_encoded.columns = ['parental level of education','gender']\n\n\n#Encoding  non-ordinal categorical features\n\nenc = OneHotEncoder(sparse=False).fit(cat)\ncat_encoded = pd.DataFrame(enc.transform(cat))\ncat_encoded.columns = enc.get_feature_names(cat.columns)\n\n# Numerical features will be standardized\nfrom sklearn.preprocessing import StandardScaler\nnum.iloc[:, 0:3] = StandardScaler().fit_transform(num.iloc[:, 0:3])","2e2277ec":"# merge numeric and categorical data\ndf2 = pd.concat([cat_encoded,cat_ord_encoded, num], axis=1)\ndf2.head()","235e47a1":"from sklearn.model_selection import train_test_split\n\nX = df2.drop(columns='gender')\ny = df2['gender']\n\n\nx_train , x_test , y_train, y_test = train_test_split(X,y,test_size = 0.2 , random_state = 23)","f7976f85":"score_list = []\nfrom sklearn.ensemble import RandomForestClassifier\nfor each in range (1,100):\n    rf = RandomForestClassifier(n_estimators = each,random_state = 7,bootstrap = \"False\",criterion=\"gini\",\n                                min_samples_split = 10 , min_samples_leaf = 1)\n    rf.fit(x_train,y_train)\n    score_list.append(rf.score(x_test,y_test))\n    \nrf_max = np.max(score_list)\nprint(\"RF Max Score : \",rf_max)","a0d5ef31":"plt.subplots(figsize=(7, 7))\nplt.plot(score_list)\nplt.title(\"Accuracy x estimators\\n\", size=20,fontweight='bold')","43f752bc":"#Training with the best number of estimators\n\nbest = score_list.index(max(score_list)) + 1\n\nrf = RandomForestClassifier(n_estimators = best,random_state = 7,bootstrap = \"False\",criterion=\"gini\",\n                            min_samples_split = 10 , min_samples_leaf = 1)\nrf.fit(x_train,y_train)\n\n\nfrom sklearn.metrics import plot_confusion_matrix\n\nwith sns.axes_style(\"white\"):\n    titles_options = [(\"Confusion matrix, without normalization\", None),\n                      (\"Normalized confusion matrix\", 'true')]\n    class_names = ['Male','Female']\n    for title, normalize in titles_options:\n        fig, ax = plt.subplots(figsize=(7, 7))\n        disp = plot_confusion_matrix(rf, x_test, y_test,\n                                     display_labels=class_names,\n                                     cmap='rocket',\n                                     normalize=normalize,\n                                    ax=ax)\n        disp.ax_.set_title(title)\n\n        print(title)\n        print(disp.confusion_matrix)\n    \nplt.show()","265acc29":"from sklearn.svm import SVC\nsvm1 = SVC(gamma = 0.01 , C = 500 , kernel = \"rbf\")\nsvm1.fit(x_train,y_train)\nsvm1_score = svm1.score(x_test,y_test)\nprint(\"SVM Max Score = : \", svm1_score)","87fcd8ae":"with sns.axes_style(\"white\"):\n    titles_options = [(\"Confusion matrix, without normalization\", None),\n                      (\"Normalized confusion matrix\", 'true')]\n    class_names = ['Male','Female']\n    for title, normalize in titles_options:\n        fig, ax = plt.subplots(figsize=(7, 7))\n        disp = plot_confusion_matrix(svm1, x_test, y_test,\n                                     display_labels=class_names,\n                                     cmap='rocket',\n                                     normalize=normalize,\n                                    ax=ax)\n        disp.ax_.set_title(title)\n\n        print(title)\n        print(disp.confusion_matrix)\n    \nplt.show()","4d1b872b":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(rf, random_state=1).fit(x_test, y_test)\neli5.show_weights(perm, feature_names = x_test.columns.tolist(), top=7)","bca7c63f":"from sklearn.ensemble import RandomForestRegressor\nbest = score_list.index(max(score_list)) + 1\n\nrf_reg = RandomForestRegressor(n_estimators = best,random_state = 7,bootstrap = \"False\",criterion=\"mse\",\n                            min_samples_split = 10 , min_samples_leaf = 1)\nrf_reg.fit(x_train,y_train)","44674de0":"import shap\n\n# calculate shap values \nex = shap.Explainer(rf_reg)\nshap_val = ex(x_test)\n\n","4601185f":"shap.plots.bar(shap_val, show=False)\nplt.title('Mean SHAP value per feature\\n Gender Analysis',size=20,fontweight='bold')","7c44e466":"# plot\n\nplt.title('SHAP summary for Gender prediction', size=20)\nshap.plots.beeswarm(shap_val, max_display=5,show=False)\nfig = plt.gcf()\nfig.set_figheight(7)\nfig.set_figwidth(12)\nax = plt.gca()\nax.set_xlabel(r'Average SHAP values', fontsize=16)\nax.set_ylabel('Parameters', fontsize=16)\nleg = ax.legend()\nt = ax.text(\n    0.05, -0.6, \"Predict female\", ha=\"left\", va=\"center\", rotation=0, size=13,\n    bbox=dict(boxstyle=\"rarrow,pad=0.3\", fc=\"#CBC3E3\", ec=\"#301934\", lw=1))\nt = ax.text(\n    -0.05, -0.6, \"Predict male\", ha=\"right\", va=\"center\", rotation=0, size=13,\n    bbox=dict(boxstyle=\"larrow,pad=0.3\", fc=\"#CBC3E3\", ec=\"#301934\", lw=1))\nplt.show()","e593c570":"shap.initjs()\nshap.plots.force(shap_val[10])","95a2ba24":"class color:\n   PURPLE = '\\033[95m'\n   CYAN = '\\033[96m'\n   DARKCYAN = '\\033[36m'\n   BLUE = '\\033[94m'\n   GREEN = '\\033[92m'\n   YELLOW = '\\033[93m'\n   RED = '\\033[91m'\n   BOLD = '\\033[1m'\n   UNDERLINE = '\\033[4m'\n   END = '\\033[0m'\n\nprint(color.BOLD + \"Individual number 280\\n\" + color.END)\nprint(x_test.iloc[10])\nprint(color.BOLD + y_test.map({1:'female',0:\"male\"}).iloc[10] + color.END)","67bb26ea":"explainer = shap.TreeExplainer(rf_reg)\nshap_values = explainer.shap_values(x_test)\nshap.dependence_plot(\"writing score\", shap_values, x_test,show=False)\nfig = plt.gcf()\nfig.set_figheight(7)\nfig.set_figwidth(10)","8c4da28d":"X = df2.drop(columns='math score')\ny = df2['math score']\n\n\nx_train , x_test , y_train, y_test = train_test_split(X,y,test_size = 0.2 , random_state = 23)","b2dcaad5":"from sklearn.ensemble import RandomForestRegressor\nbest = score_list.index(max(score_list)) + 1\n\nrf_reg = RandomForestRegressor(n_estimators = best,random_state = 7,bootstrap = \"False\",criterion=\"mse\",\n                            min_samples_split = 10 , min_samples_leaf = 1)\nrf_reg.fit(x_train,y_train)","fa8b41c6":"score_list = []\nfor each in range (1,100):\n    rf = RandomForestRegressor(n_estimators = each,random_state = 7,bootstrap = \"False\",criterion=\"mse\",\n                                min_samples_split = 10 , min_samples_leaf = 1)\n    rf.fit(x_train,y_train)\n    score_list.append(rf.score(x_test,y_test))\n    \nrf_max = np.max(score_list)\nprint(\"RF Max Score : \",rf_max)","6d19f3d1":"plt.subplots(figsize=(7, 7))\nplt.plot(score_list)","d1b4b59f":"best = score_list.index(max(score_list)) + 1\n\nrf = RandomForestRegressor(n_estimators = best, random_state = 7 ,criterion=\"mse\",\n                            min_samples_split = 10 , min_samples_leaf = 1)\nrf.fit(x_train,y_train)","be300928":"import shap\n\n# calculate shap values \nex = shap.Explainer(rf)\nshap_val = ex(x_test)\n\n# plot\n\nplt.title('SHAP summary for math score prediction', size=16)\nshap.plots.beeswarm(shap_val, max_display=5,show=False)\nfig = plt.gcf()\nfig.set_figheight(7)\nfig.set_figwidth(12)\nax = plt.gca()\nax.set_xlabel(r'Average SHAP values', fontsize=16)\nax.set_ylabel('Parameters', fontsize=16)\nleg = ax.legend()\nt = ax.text(\n    0.05, -0.6, \"Predict higher math score\", ha=\"left\", va=\"center\", rotation=0, size=13,\n    bbox=dict(boxstyle=\"rarrow,pad=0.3\", fc=\"#CBC3E3\", ec=\"#301934\", lw=1))\nt = ax.text(\n    -0.05, -0.6, \"Predict lower math score\", ha=\"right\", va=\"center\", rotation=0, size=13,\n    bbox=dict(boxstyle=\"larrow,pad=0.3\", fc=\"#CBC3E3\", ec=\"#301934\", lw=1))\nplt.show()","1d9bb581":"select = range(5)\nfeatures = x_test.iloc[select]\nfeatures_display = x_test.loc[features.index]\nshap.decision_plot(ex.expected_value,ex.shap_values(features)[0:5] , features_display) ","57eda4d1":"shap.plots.force(shap_val[2],figsize=(10,5))","d5f4c4ec":"class color:\n   PURPLE = '\\033[95m'\n   CYAN = '\\033[96m'\n   DARKCYAN = '\\033[36m'\n   BLUE = '\\033[94m'\n   GREEN = '\\033[92m'\n   YELLOW = '\\033[93m'\n   RED = '\\033[91m'\n   BOLD = '\\033[1m'\n   UNDERLINE = '\\033[4m'\n   END = '\\033[0m'\n\nprint(color.BOLD + \"Individual number 167\\n\" + color.END)\nprint(rf.predict(x_test[16:17]))\nprint(  df['math score'].describe().loc['mean']   + (rf.predict(x_test[10:11]))[0]*df['math score'].describe().loc['std'])\n\n\nprint(x_test.iloc[16])\nprint(color.BOLD + str(df['math score'].loc[705]) + color.END)\nprint(color.BOLD + str(df['math score'].describe().loc['mean'] +y_test.loc[705]*df['math score'].describe().loc['std']) + color.END)","7bb3f000":"<html>\n    <h1 style='background: #D3D3D3;  color:black; font-size:30px; padding:10px; border:8px solid black;'><center><b>Categorical Variables<\/b><\/center><\/h1>\n<\/html>","13930187":"Here we see that the most important variables when predicting the target feature, namely \"gender\",are the writing, math and reading scores.  \nAll other variables are dispensable.","4e557b84":"<html>\n    <h1 style='background: #D3D3D3;  color:black; font-size:45px; padding:10px; border:8px solid black;'><center><b>\ud83d\ude09 Upvote if you liked the content.<\/b><\/center><\/h1>\n<\/html>","4a202c7b":"This beeswarp graph shows the dispersion of the SHAP values along the variables. As expected the three test scores contribute with higher absolute values, giving more certainty to the model prediction.","7b84e5ad":"# 5.Relative Relevance of features\n<html>\n    <h1 style='background: #D3D3D3;  color:black; font-size:45px; padding:10px; border:8px solid black;'><center><b>\ud83e\udd47 Most relevant features<\/b><\/center><\/h1>\n<\/html>","56185139":"The frequency distribution, also doesn't show anything unexpected, but there seems to be some bias towards the score of 70 in all exams.","c20101c6":"**We can expect the random forest Classifier to have an accuracy around 85%.**","201e9ea5":"At the boxplot, we see some outliers, but nothing unexpected considering a school exam.","c79090f1":"# 2.Exploring and Preparing\n<html>\n    <h1 style='background: #D3D3D3;  color:black; font-size:45px; padding:10px; border:8px solid black;'><center><b>\ud83d\udd0e Exploring and Preparing<\/b><\/center><\/h1>\n<\/html>","00fa633c":"Here, we split the dataset so we can test it for accuracy after modelling.","9182ad9f":"**With the support vector machine model, we can expect a accuracy of 89,5%.**","2aedd7fb":"<html>\n    <h1 style='background: #D3D3D3;  color:black; font-size:30px; padding:10px; border:8px solid black;'><center><b>Numerical Variables<\/b><\/center><\/h1>\n<\/html>","4bdf331d":"Here we can see a small but clear improvement in the writing scores as the parental level of education increases.","148d70ff":"Here we see the distribution of the students among the categorical variables. No anomalities perceived.","f9a1bdc5":"Here we see the distribution of the scores, now also in respect to the gender of the students.\n* There is overlap among the distributions, but we can see that the female students have a better mean performance in writing and reading, the male students in the other hand, have a slight edge in math\n* When combined, the writing and math scores, make distinct areas when it comes to gender. Those probably will be relevant predictors in the model.\n* Due to the overlap, it is important to check for the statistical relevance of the differences, especially in regard to math scores, where the distributions seems to be somewhat similar.","cdea98ff":"Here we apply the SHAP waterfall to a single individual. This allow us to follow the path the model takes and helps to understand why the top three variables are determinant in the assertion","43699ff1":"# 1.Importing\n<html>\n    <h1 style='background: #D3D3D3;  color:black; font-size:45px; padding:10px; border:8px solid black;'><center><b>\ud83d\udcda Importing Libraries and Data<\/b><\/center><\/h1>\n<\/html>","079f30de":"Now the dataset is ready to be inserted in a model","d703f954":"*No apparent absent data.  \n*The datatypes seem to be correct.","6553500c":"Here, we see the correlation matrix of the numerical variables, namely the exams scores.  \nWe observe a very strong correlation between \"writing score\" and the \"reading score\".  \nWe also see a strong correlation between \"writing score\" and \"math score\".\nIt can be concluded that the performance of the stundents tend to be rather linear, if they have higher( or lower) score in one domain, it is expected the other domains to also be higher(or lower) and vice versa.","fda3753d":"After the general view, we can inspect particulary interesting plots with the following graph.\nLet's take for instance the writing scores and the parental level of education:","dae0d58d":"## 4.1 Random Forest","d51c7778":"The confusion matrix show us that given the data available, it is easier to predict female students(90% accuracy). Male students incur in more false labelling as females(17%).","31885059":"To establish the relative relevance of the features in predicting the gender of the student, we are going to model a Random Forest Regressor, so that the target variable become continuous. Afterward we are going to apply the **SHAP (SHapley Additive exPlanations)** to rank order the features. ","7bbe77db":"## BONUS: This method can also be applied to predict numerical values\n<html>\n    <h1 style='background: #D3D3D3;  color:black; font-size:45px; padding:10px; border:8px solid black;'><center><b>Math Score Prediction<\/b><\/center><\/h1>\n<\/html>","2dd731ed":"<html>\n    <h1 style='background: #D3D3D3;  color:black; font-size:30px; padding:10px; border:8px solid black;'><center><b>Categorical X Numerical Variables<\/b><\/center><\/h1>\n<\/html>","97f38bbb":"The following plot allow us to have a bird-eye view of the distribution of the numerical variables along the categorical variables. The main point is to detect anomalies or behaviours of interest.","f6cfb1d7":"## 4.2 Support Vector Machine","f6b2944f":"# 0.Stablishing the goal\n<html>\n    <h1 style='background: #D3D3D3;  color:black; font-size:45px; padding:10px; border:8px solid black;'><center><b>\ud83c\udfc1 Goal<\/b><\/center><\/h1>\n<\/html>\n\nIn this notebook, we will explore a dataset(https:\/\/www.kaggle.com\/spscientist\/students-performance-in-exams), which contains 1000 entries of students.  \nThe main goal will be to stablish a **gender prediction** with ML based on the available features and conclude about the most important features in this prediction.","1f6575c8":"# 4.Prediction\n<html>\n    <h1 style='background: #D3D3D3;  color:black; font-size:45px; padding:10px; border:8px solid black;'><center><b>\ud83d\udd2e Prediction<\/b><\/center><\/h1>\n<\/html>","4e45f442":"# 3.Processing\n<html>\n    <h1 style='background: #D3D3D3;  color:black; font-size:45px; padding:10px; border:8px solid black;'><center><b>\ud83d\udee0 Processing<\/b><\/center><\/h1>\n<\/html>"}}