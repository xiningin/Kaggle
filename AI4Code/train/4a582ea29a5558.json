{"cell_type":{"26be5687":"code","76273096":"code","0c64c95b":"code","dbe59c66":"code","ce469cbc":"code","3e2d9666":"code","d0ef1c07":"code","eeb99261":"code","1bcb3d22":"code","6c0d2fca":"code","b67c705d":"code","9c72a768":"code","86f6f3fc":"code","b07bd997":"code","5075de47":"code","993eadd9":"code","8241544a":"code","c3dc3678":"code","b66b2848":"code","569738bf":"code","95705e49":"code","01665ca7":"code","ce2fffc9":"code","0e7abd76":"code","188c2960":"code","8ad2a507":"code","39ae949c":"code","631775db":"code","d9ef915d":"code","0cf32440":"markdown","aed0201a":"markdown","8ccd9386":"markdown","9ac83acf":"markdown","3c623c07":"markdown"},"source":{"26be5687":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","76273096":"import os\nimport sys\nimport scipy.io\nimport imageio\nimport scipy.misc\nfrom scipy import io\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\nfrom PIL import Image\nimport numpy as np\nimport tensorflow as tf\nimport pprint                  #to format printing of the vgg model\n%matplotlib inline","0c64c95b":"class CONFIG:                  #Class is like an object constructor\n    IMAGE_WIDTH = 1280\n    IMAGE_HEIGHT = 720\n    COLOR_CHANNELS = 3\n    NOISE_RATIO = 0.5\n    MEANS = np.array([123.68, 116.779, 103.939]).reshape((1,1,1,3)) ","dbe59c66":"def load_vgg_model(path):\n    vgg = io.loadmat(path)\n    vgg_layers = vgg['layers']\n    def _weights(layer, expected_layer_name):\n        wb = vgg_layers[0][layer][0][0][2]\n        W = wb[0][0]\n        b = wb[0][1]\n        layer_name = vgg_layers[0][layer][0][0][0][0]\n        assert layer_name == expected_layer_name\n        return W, b\n        return W, b\n    def _relu(conv2d_layer):\n        return tf.nn.relu(conv2d_layer)\n    def _conv2d(prev_layer, layer, layer_name):    \n        W, b = _weights(layer, layer_name)\n        W = tf.constant(W)\n        b = tf.constant(np.reshape(b, (b.size)))\n        return tf.nn.conv2d(prev_layer, filters=W, strides=[1, 1, 1, 1], padding='SAME') + b\n    def _conv2d_relu(prev_layer, layer, layer_name):\n        return _relu(_conv2d(prev_layer, layer, layer_name))\n    \n    def _avgpool(prev_layer):\n        return tf.nn.avg_pool(prev_layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n    \n    # Constructs the graph model.\n    graph = {}\n    graph['input']   = tf.Variable(np.zeros((1, CONFIG.IMAGE_HEIGHT, CONFIG.IMAGE_WIDTH, CONFIG.COLOR_CHANNELS)), dtype = 'float32')\n    graph['conv1_1']  = _conv2d_relu(graph['input'], 0, 'conv1_1')\n    graph['conv1_2']  = _conv2d_relu(graph['conv1_1'], 2, 'conv1_2')\n    graph['avgpool1'] = _avgpool(graph['conv1_2'])\n    graph['conv2_1']  = _conv2d_relu(graph['avgpool1'], 5, 'conv2_1')\n    graph['conv2_2']  = _conv2d_relu(graph['conv2_1'], 7, 'conv2_2')\n    graph['avgpool2'] = _avgpool(graph['conv2_2'])\n    graph['conv3_1']  = _conv2d_relu(graph['avgpool2'], 10, 'conv3_1')\n    graph['conv3_2']  = _conv2d_relu(graph['conv3_1'], 12, 'conv3_2')\n    graph['conv3_3']  = _conv2d_relu(graph['conv3_2'], 14, 'conv3_3')\n    graph['conv3_4']  = _conv2d_relu(graph['conv3_3'], 16, 'conv3_4')\n    graph['avgpool3'] = _avgpool(graph['conv3_4'])\n    graph['conv4_1']  = _conv2d_relu(graph['avgpool3'], 19, 'conv4_1')\n    graph['conv4_2']  = _conv2d_relu(graph['conv4_1'], 21, 'conv4_2')\n    graph['conv4_3']  = _conv2d_relu(graph['conv4_2'], 23, 'conv4_3')\n    graph['conv4_4']  = _conv2d_relu(graph['conv4_3'], 25, 'conv4_4')\n    graph['avgpool4'] = _avgpool(graph['conv4_4'])\n    graph['conv5_1']  = _conv2d_relu(graph['avgpool4'], 28, 'conv5_1')\n    graph['conv5_2']  = _conv2d_relu(graph['conv5_1'], 30, 'conv5_2')\n    graph['conv5_3']  = _conv2d_relu(graph['conv5_2'], 32, 'conv5_3')\n    graph['conv5_4']  = _conv2d_relu(graph['conv5_3'], 34, 'conv5_4')\n    graph['avgpool5'] = _avgpool(graph['conv5_4'])\n    \n    return graph","ce469cbc":"datapath = \"..\/input\/imagenetvggverydeep19mat\/imagenet-vgg-verydeep-19.mat\"\npp = pprint.PrettyPrinter(indent=4)\nmodel = load_vgg_model(datapath)\npp.pprint(model)","3e2d9666":"import imageio\ncontent_image = imageio.imread(\"https:\/\/www.economist.com\/img\/b\/1280\/720\/90\/sites\/default\/files\/20200905_BKP503.jpg\")\nplt.figure(figsize=(10,10))\nplt.imshow(content_image);\n","d0ef1c07":"content_image.shape","eeb99261":"# Compute Content Cost\n\ndef compute_content_cost(a_C,a_G):\n    #Retrieve dimensions from a_G\n    m,n_H,n_W,n_C = a_G.get_shape().as_list()\n    \n    #Unrolled a_C and a_G\n    a_C_unrolled = tf.reshape(a_C,shape=[m,-1,n_C])\n    a_G_unrolled = tf.reshape(a_G,shape=[m,-1,n_C])\n    \n    #Compute the Content Cost\n    j_content = (1\/(4*n_H*n_W*n_C))*(tf.reduce_sum(tf.square(tf.subtract(a_C_unrolled,a_G_unrolled))))\n\n    return j_content","1bcb3d22":"style_image = imageio.imread(\"https:\/\/www.thevintagenews.com\/wp-content\/uploads\/2020\/04\/van_gogh_painting-1280x720.jpg\")\nplt.figure(figsize=(10,10))\nplt.imshow(style_image)\nstyle_image.shape\n","6c0d2fca":"# Style Matrix is also known as Gram Matrix, Gij which is the dot product of np.dot(vi,vj). Gij compares how similar vi to vj if it is similar then dot product to be very high.\n# In Neural Style Transfer (NST), you can compute the Style matrix by multiplying the \"unrolled\" filter matrix with its transpose.\n# The result is a matrix of dimension (n_C,n_C) where n_C is the number of filters (channels).\n# The diagonal elements  G(gram)ii  measure how \"active\" a filter  i  is.\n# For example, suppose filter i is detecting vertical textures in the image. Then  G(gram)i  measures how common vertical textures are in the image as a whole.\n# If  G(gram)ii is large, this means that the image has a lot of vertical texture.","b67c705d":"### Computing Cost for a single layer\ndef compute_style_cost_layer(a_S,a_G):\n    #Retrieve dimensions from a_G\n    m,n_H,n_W,n_C = a_G.get_shape().as_list()\n    \n    #unroll a_S and a_G\n    a_S = tf.transpose(tf.reshape(a_S,shape=[n_H*n_W,n_C]))\n    a_G = tf.transpose(tf.reshape(a_G,shape=[n_H*n_W,n_C]))\n    \n    #Computing gram matrics\n    S = tf.matmul(a_S,tf.transpose(a_S))\n    G = tf.matmul(a_G,tf.transpose(a_G))\n    \n    #Compute the Style Cost\n    j_style_layer = (1\/(4*(n_C**2)*((n_H*n_W)**2)))*(tf.reduce_sum(tf.reduce_sum(tf.square(tf.subtract(S,G)))))\n    \n    return j_style_layer","9c72a768":"# We have captured the style from only one layer.\n# We'll get better results if we \"merge\" style costs from several different layers.\n# Each layer will be given weights (\u03bb[l]) that reflect how much each layer will contribute to the style.","86f6f3fc":"style_layers = [('conv1_2',0.3),\n               ('conv3_2',0.3),\n               ('conv3_3',0.3),\n               ('conv4_2',0.3),\n               ('conv5_1',0.3)]","b07bd997":"#Compute Style Cost\ndef compute_style_cost(model, style_layers):\n    \n    #initialize overall cost\n    j_style=0\n    \n    for layer_name,coeff in style_layers:\n        #select the output tensor of the currently selected layer\n        out = model[layer_name]\n        \n        #set a_S to be the activaltion of the currently selected layer by running the session on out\n        a_S = sess.run(out)\n        \n        #set a_G to be the activaltion from the same layer\n        a_G = model[layer_name]\n        \n        #compute style cost for the current layer\n        j_style_layer = compute_style_cost_layer(a_S,a_G)\n        \n        #Add coeff to j_style layer to compute the cost from overall layers\n        j_style += coeff*j_style_layer\n        \n    return j_style\n    ","5075de47":"#compute total cost\n#alpha and beta are the hypermeters that control the weights between content and style. \ndef total_cost(j_content, j_style, alpha=10, beta=40):\n    \n    j = (alpha*j_content) + (beta*j_style)\n    \n    return j ","993eadd9":"tf.compat.v1.disable_eager_execution()\ntf.compat.v1.reset_default_graph()\nsess = tf.compat.v1.InteractiveSession()","8241544a":"#  Reshape and normalize the input image (content or style)\ndef reshape_and_normalize_image(image):\n    image = np.reshape(image, ((1,) + image.shape))\n    image = image - CONFIG.MEANS\n    return image","c3dc3678":"content_image = imageio.imread(\"https:\/\/www.economist.com\/img\/b\/1280\/720\/90\/sites\/default\/files\/20200905_BKP503.jpg\")\ncontent_image = reshape_and_normalize_image(content_image)\nstyle_image = imageio.imread(\"https:\/\/www.thevintagenews.com\/wp-content\/uploads\/2020\/04\/van_gogh_painting-1280x720.jpg\")\nstyle_image = reshape_and_normalize_image(style_image)","b66b2848":"style_image.shape","569738bf":"# Now we initialize generated image as a noisy image by adding random noise to the content image\n\ndef generate_noisy_image(content_image, noise= CONFIG.NOISE_RATIO):\n    \n    noisy_image  = np.random.uniform(-20,20,(1,CONFIG.IMAGE_HEIGHT, CONFIG.IMAGE_WIDTH, CONFIG.COLOR_CHANNELS)).astype('float32')\n    input_image = (noisy_image * noise) + (content_image * (1-noise))\n    \n    return input_image","95705e49":"generated_image = generate_noisy_image(content_image)\nplt.imshow(generated_image[0])","01665ca7":"model = load_vgg_model(datapath)","ce2fffc9":"##CONTENT COST\n#Assign content image to the input of vgg model\nsess.run(model['input'].assign(content_image))\n\n# Select the output tensor of layer conv4_2\nout = model['conv4_2']\n\n# Set a_C to be the hidden layer activation from the layer we have selected\na_C = sess.run(out)\n\n# Set a_G to be the hidden layer activation from same layer. Here, a_G references model['conv4_2'] \n# and isn't evaluated yet. Later in the code, we'll assign the image G as the model input, so that\n# when we run the session, this will be the activations drawn from the appropriate layer, with G as input.\na_G = out\n\n# Compute the content cost\nj_content = compute_content_cost(a_C, a_G)","0e7abd76":"##STYLE COST\nsess.run(model['input'].assign(style_image))\nj_style = compute_style_cost(model, style_layers)","188c2960":"#TOTAL COST\nj = total_cost(j_content,j_style,alpha=10,beta=40)","8ad2a507":"#OPTIMIZER\n#Using Adam Optimizer to minimize the cost\noptimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.3)\ntrain_set = optimizer.minimize(j)","39ae949c":"def save_image(path, image):  \n    # Un-normalize the image so that it looks good\n    image = image + CONFIG.MEANS\n    \n    # Clip and Save the image\n    image = np.clip(image[0], 0, 255).astype('uint8')\n    imageio.imsave(path, image)","631775db":"def model_nn(sess, input_image, num_iterations = 150):\n    sess.run(tf.compat.v1.global_variables_initializer())\n    # Run the noisy input image\n    generated_image = sess.run(model[\"input\"].assign(input_image))\n    for i in range(num_iterations):\n        # Run the session on the train_step to minimize the total cost\n        sess.run(train_set)\n        generated_image = sess.run(model[\"input\"])\n        # Print every 20 iteration.\n        if i%20 == 0:\n            J, Jc, Js = sess.run([j, j_content, j_style])\n            print(\"Iteration \" + str(i) + \" :\")\n            print(\"total cost = \" + str(J))\n            print(\"content cost = \" + str(Jc))\n            print(\"style cost = \" + str(Js))\n            save_image(\"\" + str(i) + \".png\", generated_image)\n    \n    # save last generated image\n    save_image('generated_image.jpg', generated_image)\n    \n    return generated_image","d9ef915d":"model_nn(sess,generated_image)","0cf32440":"### Computing the Content Cost","aed0201a":"* Make generated image G match the content of image C.\n* The shallower layer of CovNet tends to detect low level features whereas deeper layers tend to detect high level features.\n* We would like the generated image G have the same content as C. To get the visually pleasing result we choose middle layer neither too shallow nor deeper\n\n### Forward Propagation image C\n - Set the image C input to the pretrained VGG network and run forward propagation.\n - Let a_C be the hiddle layer activation the layer you have choosen. \n\n### Forward Propagation image G\n - Set the image G input to the pretrained VGG network and run forward propagation.\n - Let a_G be the hiddle layer activation.","8ccd9386":"### Solving Optimization Problem","9ac83acf":"#### We measure how different activation (a_C and a_G) of hidden layer are. We tried to minimize the content cost so as G has same content as that of C.","3c623c07":"### Computing Style Cost"}}