{"cell_type":{"bcd88e1c":"code","8c48be39":"code","e93458a8":"code","33f3503c":"code","e40fa24d":"code","1bcdf947":"code","74dfb51f":"code","86cffdb1":"code","b2612d58":"markdown","6a615934":"markdown","df480c37":"markdown","3631827e":"markdown","b7415a73":"markdown","fcf174a4":"markdown","35c91bf9":"markdown","006a7fad":"markdown"},"source":{"bcd88e1c":"# Import libraries necessary for this project\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import display # Allows the use of display() for DataFrames\n\n# Pretty display for notebooks\n%matplotlib inline\n\n# Set a random seed\nimport random\nrandom.seed(42)\n\n# Load the dataset\nin_file = '..\/input\/data-science-day1-titanic\/DSB_Day1_Titanic_train.csv'\nfull_data = pd.read_csv(in_file)\n\n# Print the first few entries of the RMS Titanic data\ndisplay(full_data.head())","8c48be39":"# Store the 'Survived' feature in a new variable and remove it from the dataset\noutcomes = full_data['Survived']\nfeatures_raw = full_data.drop('Survived', axis = 1)\n\n# Show the new dataset with 'Survived' removed\ndisplay(features_raw.head())","e93458a8":"# Removing the names\nfeatures_no_name = features_raw.drop(['Name'], axis=1)\n\n# One-hot encoding\nfeatures = pd.get_dummies(features_no_name)","33f3503c":"features = features.fillna(0.0)\ndisplay(features.head())","e40fa24d":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features, outcomes, test_size=0.2, random_state=42)","1bcdf947":"# Import the classifier from sklearn\nfrom sklearn.tree import DecisionTreeClassifier\n\n# TODO: Define the classifier, and fit it to the data\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)","74dfb51f":"# Making predictions\ny_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\n\n# Calculate the accuracy\nfrom sklearn.metrics import accuracy_score\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\nprint('The training accuracy is', train_accuracy)\nprint('The test accuracy is', test_accuracy)","86cffdb1":"# Training the model\nmodel = DecisionTreeClassifier(max_depth=6, min_samples_leaf=6, min_samples_split=10)\nmodel.fit(X_train, y_train)\n\n# Making predictions\ny_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\n\n# Calculating accuracies\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\n\nprint('The training accuracy is', train_accuracy)\nprint('The test accuracy is', test_accuracy)","b2612d58":"## Testing the model\nNow, let's see how our model does, let's calculate the accuracy over both the training and the testing set.","6a615934":"The very same sample of the RMS Titanic data now shows the **Survived** feature removed from the DataFrame. Note that `data` (the passenger data) and `outcomes` (the outcomes of survival) are now *paired*. That means for any passenger `data.loc[i]`, they have the survival outcome `outcomes[i]`.\n\n## Preprocessing the data\n\nNow, let's do some data preprocessing. First, we'll remove the names of the passengers, and then one-hot encode the features.\n\n**Question:** Why would it be a terrible idea to one-hot encode the data without removing the names?\n\n**Answer:** If we one-hot encode the names columns, then there would be one column for each name, and the model would be learn the names of the survivors, and make predictions based on that. This would lead to some serious overfitting!","df480c37":"## (TODO) Training the model\n\nNow we're ready to train a model in sklearn. First, let's split the data into training and testing sets. Then we'll train the model on the training set.","3631827e":"And now we'll fill in any blanks with zeroes.","b7415a73":"# Lab: Titanic Survival Exploration with Decision Trees","fcf174a4":"## Getting Started\nIn this lab, you will see how decision trees work by implementing a decision tree in sklearn.\n\nWe'll start by loading the dataset and displaying some of its rows.","35c91bf9":"Recall that these are the various features present for each passenger on the ship:\n- **Survived**: Outcome of survival (0 = No; 1 = Yes)\n- **Pclass**: Socio-economic class (1 = Upper class; 2 = Middle class; 3 = Lower class)\n- **Name**: Name of passenger\n- **Sex**: Sex of the passenger\n- **Age**: Age of the passenger (Some entries contain `NaN`)\n- **SibSp**: Number of siblings and spouses of the passenger aboard\n- **Parch**: Number of parents and children of the passenger aboard\n- **Ticket**: Ticket number of the passenger\n- **Fare**: Fare paid by the passenger\n- **Cabin** Cabin number of the passenger (Some entries contain `NaN`)\n- **Embarked**: Port of embarkation of the passenger (C = Cherbourg; Q = Queenstown; S = Southampton)\n\nSince we're interested in the outcome of survival for each passenger or crew member, we can remove the **Survived** feature from this dataset and store it as its own separate variable `outcomes`. We will use these outcomes as our prediction targets.  \nRun the code cell below to remove **Survived** as a feature of the dataset and store it in `outcomes`.","006a7fad":"# Exerise: Improving the model\n\nOk, high training accuracy and a lower testing accuracy. We may be overfitting a bit.\n\nSo now it's your turn to shine! Train a new model, and try to specify some parameters in order to improve the testing accuracy, such as:\n- `max_depth`\n- `min_samples_leaf`\n- `min_samples_split`\n\nYou can use your intuition, trial and error, or even better, feel free to use Grid Search!\n\n**Challenge:** Try to get to 85% accuracy on the testing set. If you'd like a hint, take a look at the solutions notebook in this same folder."}}