{"cell_type":{"d4fc9444":"code","7f302970":"code","534185cc":"code","d4c736a5":"code","ab129445":"code","d88968a4":"code","e7b9a002":"code","92304c2b":"code","aa5ff9d9":"code","903f1364":"code","049c485d":"code","207fc183":"code","45b13592":"code","d6a9920d":"code","99727ddd":"code","4614fec6":"code","8d78ef75":"code","5645ac1b":"code","63c7da96":"code","5efa750e":"code","da4c3f8b":"code","5100eed5":"code","6cdbf1f0":"code","566dfe1e":"code","273f8965":"markdown","0d6e6caf":"markdown"},"source":{"d4fc9444":"# Importing necessary libraries as always\nimport numpy as np\nimport pandas as pd\nimport itertools\nimport hashlib\nimport copy\n\nfrom catboost import CatBoostClassifier, cv\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom sklearn.utils.class_weight import compute_class_weight\n\nfrom scipy.stats import kstest\n\nfrom matplotlib import pyplot as plt","7f302970":"# Viewing input data\ndata = pd.read_csv('..\/input\/is-this-a-good-customer\/clients.csv')\ndata","534185cc":"# There are some numerical and categorical features; we don't have to care about label ecnoding\n# as CatBoost handles categoricals out-of-the-box, but we'll have to specify their list\n# before algorithm training\ndata.info()","d4c736a5":"# Visualizing distributions of features of good and bad clients which will allow us\n# to find out the features that are distributed significantly differently\n# for classes\ndata_good = data[data.bad_client_target == 0].drop(columns=['bad_client_target'])\ndata_bad = data[data.bad_client_target == 1].drop(columns=['bad_client_target'])","ab129445":"for col in data_good.columns:\n    plt.figure(figsize=(20, 10))\n    plt.hist(data_good[col], bins=100)\n    plt.hist(data_bad[col], bins=100)\n    plt.title(f'{col} distribution')\n    plt.legend(['Good clients', 'Bad clients'])","d88968a4":"# Simple feature engineering: adding threshold values for features that match bad clients only below the threshold\ndata['risky_amount'] = [1 if amt <= 100000 else 0 for amt in data.credit_amount]\ndata['risky_age'] = [1 if age <= 65 else 0 for age in data.age]\ndata['risky_edu'] = [1 if edu not in ['Incomplete secondary education', 'PhD degree'] else 0 for edu in data.education]","e7b9a002":"# We'll use one more feature trick that involves some statistics, so let's copy our DF for further experiments\ndata_2FE = data.copy()","92304c2b":"# Get list of categorical features for CatBoost\ncategoricals = []\n\nfor col in data.drop(columns=['bad_client_target']).columns:\n    if data[col].dtype == 'object':\n        print(f'{col} is categorical (object type)')\n        categoricals.append(col)\n    elif data[col].nunique() < 25:\n        print(f'{col} is categorical (numeric type)')\n        categoricals.append(col)","aa5ff9d9":"# First of all, let's try a brute-force fit-predict to get our basic score\n# I use the F1 for the bad client class (its label is 1) for scoring as our task is to identify\n# the risky customers\nX_train, X_test, Y_train, Y_test = train_test_split(data.drop(columns=['bad_client_target']),\n                                                    data.bad_client_target, random_state=42,\n                                                    train_size=0.8, stratify=data.bad_client_target)","903f1364":"# Our simple solution gives us... 0.05 of 1. Bad news, CatBost can't do the whole work itself :)\ncbc = CatBoostClassifier(random_state=42, verbose=False, cat_features=categoricals)\ncbc.fit(X_train,Y_train)\nY_pred = cbc.predict(X_test)\nprint(classification_report(Y_test, Y_pred))\nprint(confusion_matrix(Y_test, Y_pred))","049c485d":"# Another trick for binary classification: we can vary the positive class probability threshold\n# to change the distribution of class labels\n# Such a trick gives us F1=0.3291 >> 0.05\nY_pred_proba = cbc.predict_proba(X_test)\ntop_thr = 0\ntop_f1 = 0\n\nfor thr in [round(0.05*k, 2) for k in range(20)]:\n    cur_f1 = round(f1_score(Y_test, Y_pred_proba[:, 1] >= thr), 4)\n    print(f'For threshold={thr} F1={cur_f1}')\n    if cur_f1 > top_f1:\n        top_f1 = cur_f1\n        top_thr = thr\n        \nprint(\"***\")\nprint(f\"Threshold={top_thr}: top F1={top_f1}\")","207fc183":"# The first idea that comes to mind while working with imbalanced data is to give\n# the weights to the classes to emphaize that the minority class is more important\ncw = compute_class_weight(class_weight='balanced', classes=np.unique(data.bad_client_target), y=data.bad_client_target)\nprint(f\"Class weights are {cw}\")","45b13592":"# CatBoost with class weights performes significantly better (target F1=0.29), but it's far not enough for\n# a good classifier\ncbc = CatBoostClassifier(random_state=42, verbose=False, cat_features=categoricals,\n                         class_weights=cw)\ncbc.fit(X_train,Y_train)\nY_pred = cbc.predict(X_test)\nprint(classification_report(Y_test, Y_pred))\nprint(confusion_matrix(Y_test, Y_pred))","d6a9920d":"# Class threshold variation gives us F1=0.411, which is a significant boost from our baseline\nY_pred_proba = cbc.predict_proba(X_test)\ntop_thr = 0\ntop_f1 = 0\n\nfor thr in [round(0.05*k, 2) for k in range(20)]:\n    cur_f1 = round(f1_score(Y_test, Y_pred_proba[:, 1] >= thr), 4)\n    print(f'For threshold={thr} F1={cur_f1}')\n    if cur_f1 > top_f1:\n        top_f1 = cur_f1\n        top_thr = thr\n        \nprint(\"***\")\nprint(f\"Threshold={top_thr}: top F1={top_f1}\")","99727ddd":"# Let's make a statistical feature engineering trick friendly suggested by @ellaktozar\n# First of all, we need to filter out all numerical features\ndata_num = data_2FE.select_dtypes(include=['int16', 'int32', 'int64', 'float16', 'float32', 'float64'])\ndata_num","4614fec6":"# Here, we make pairwise sums and products of pairs of numerical columns\n# If the values of these features appear to be distributed differently for good and bad class\n# (Kolmogorov-Smirnov test gives p-value <= 0.05), then the feature can be a possible\n# determinant for a class; we add such features to the dataset\ndata_num_good = data_num[data_num.bad_client_target == 0]\ndata_num_bad = data_num[data_num.bad_client_target == 1]\n\nfor col_pair in list(itertools.combinations(data_num.drop(columns=['bad_client_target']).columns, 2)):\n    col_sum = data_num[col_pair[0]].values + data_num[col_pair[1]].values\n    col_prod = data_num[col_pair[0]].values * data_num[col_pair[1]].values\n    \n    col_sum_good = data_num_good[col_pair[0]].values + data_num_good[col_pair[1]].values\n    col_sum_bad = data_num_bad[col_pair[0]].values + data_num_bad[col_pair[1]].values\n    \n    col_prod_good = data_num_good[col_pair[0]].values * data_num_good[col_pair[1]].values\n    col_prod_bad = data_num_bad[col_pair[0]].values * data_num_bad[col_pair[1]].values\n    \n    if kstest(col_sum_good, col_sum_bad).pvalue <= 0.05:\n        print(f\"Sum of {col_pair[0]} and {col_pair[1]} is good\")\n        data_2FE[col_pair[0] + '_' + col_pair[1] + '_sum'] = col_sum\n    if kstest(col_prod_good, col_prod_bad).pvalue <= 0.05:\n        print(f\"Product of {col_pair[0]} and {col_pair[1]} is good\")\n        data_2FE[col_pair[0] + '_' + col_pair[1] + '_prod'] = col_prod","8d78ef75":"# Finally, we increased the number of features from 13 to 62\ndata_2FE","5645ac1b":"# Let's try to detect the bad clients with an extended set of features\nX_train, X_test, Y_train, Y_test = train_test_split(data_2FE.drop(columns=['bad_client_target']), data_2FE.bad_client_target, random_state=42,\n                                                   train_size=0.8, stratify=data_2FE.bad_client_target)","63c7da96":"# F1=0.28 became slightly worse than the previous one (0.29); maybe we should remove some features\n# that are useless to our model\ncbc = CatBoostClassifier(random_state=42, verbose=False, cat_features=categoricals,\n                         class_weights=cw)\ncbc.fit(X_train,Y_train)\nY_pred = cbc.predict(X_test)\nprint(classification_report(Y_test, Y_pred))\nprint(confusion_matrix(Y_test, Y_pred))","5efa750e":"# Threshold variation result is also inferior to the previous solution\nY_pred_proba = cbc.predict_proba(X_test)\ntop_thr = 0\ntop_f1 = 0\n\nfor thr in [round(0.05*k, 2) for k in range(20)]:\n    cur_f1 = round(f1_score(Y_test, Y_pred_proba[:, 1] >= thr), 4)\n    print(f'For threshold={thr} F1={cur_f1}')\n    if cur_f1 > top_f1:\n        top_f1 = cur_f1\n        top_thr = thr\n        \nprint(\"***\")\nprint(f\"Threshold={top_thr}: top F1={top_f1}\")","da4c3f8b":"# To reduce the dimensionality, we identify the significance of each feature for our current model\ncatboost_feat_import = pd.DataFrame(columns=['Feature', 'Score'])\ncatboost_feat_import.Feature = data_2FE.drop(columns=['bad_client_target']).columns\ncatboost_feat_import.Score = cbc.get_feature_importance()\ncatboost_feat_import = catboost_feat_import.sort_values(by='Score', ascending=False)\ncatboost_feat_import","5100eed5":"# Simple feature selection: on each step, we add one feature to our set in order of their importance score descending,\n# train and test the classifier and find the feature subset which gives the highest F1\ntop_feats = []\ntop_f1 = 0\ntop_categoricals = []\ncol_list = []\ncur_categoricals = []\n\nY = data_2FE.bad_client_target\n\nfor col in catboost_feat_import.Feature:\n    col_list.append(col)\n    if data_2FE[col].dtype == 'object':\n        cur_categoricals.append(col)\n        \n    X = data_2FE[col_list]\n    print(f'Training with {col_list[0]}...{col_list[-1]}')\n    \n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=42, train_size=0.8, stratify=Y)\n    cbc = CatBoostClassifier(random_state=42, verbose=False, class_weights=cw, cat_features=cur_categoricals)\n    cbc.fit(X_train, Y_train)\n    Y_pred = cbc.predict(X_test)\n    cur_f1 = round(f1_score(Y_test, Y_pred), 4)\n    print(f\"For cols {col_list[0]}...{col_list[-1]} F1 score of CatBoost is {cur_f1}\")\n    if cur_f1 > top_f1:\n        top_f1 = cur_f1\n        top_feats = copy.deepcopy(col_list) # copying (not pointing to) the lists is available only with deepcopy\n        top_categoricals = copy.deepcopy(cur_categoricals)\n        \nprint('***')\nprint(f'Max F1={top_f1} is reached with cols {top_feats}')","6cdbf1f0":"# The final set gives only F1=0.3717 which is still far from good; but it's far better than the baseline 0.05 \nX_train, X_test, Y_train, Y_test = train_test_split(data_2FE[top_feats], Y, random_state=42, train_size=0.8, stratify=Y)\ncbc = CatBoostClassifier(random_state=42, verbose=False, class_weights=cw, cat_features=top_categoricals)\ncbc.fit(X_train, Y_train)\nY_pred = cbc.predict(X_test)\nprint(classification_report(Y_test, Y_pred))","566dfe1e":"# And the threshold-varied score is also 0.3717; seems like new features became redundant\nY_pred_proba = cbc.predict_proba(X_test)\ntop_thr = 0\ntop_f1 = 0\n\nfor thr in [round(0.05*k, 2) for k in range(20)]:\n    cur_f1 = round(f1_score(Y_test, Y_pred_proba[:, 1] >= thr), 4)\n    print(f'For threshold={thr} F1={cur_f1}')\n    if cur_f1 > top_f1:\n        top_f1 = cur_f1\n        top_thr = thr\n        \nprint(\"***\")\nprint(f\"Threshold={top_thr}: top F1={top_f1}\")","273f8965":"**Greetings!**\n\nToday we'll try to identify whether the loaner is reliable or not using CatBoost and some imbalanced learning techniques - class weighting and feature selection.\n\n*Spoiler:* our try was not as successful as it could be... But it's not a complete fail though :)","0d6e6caf":"The trained models gives only a maximum of F1=0.411 which is still far from good; but it's far better than the baseline value 0.05. I also tried some imbalanced learning tricks like resampling with SMOTE and ADASYN and PCA dimensionality reduction, but they gave only F1=0.34 and are not so native for CatBoost.\n\n**Thanks for your attention! If you have any ideas on how to improve the solution, feel free to start a chat :)**"}}