{"cell_type":{"65241bdf":"code","303c35ab":"code","af10113c":"code","a5196729":"code","3f2f55a5":"code","2ce7025b":"code","2b9860a5":"code","69c5f6b2":"code","93d49c17":"code","7594af10":"code","fa221d79":"code","fd4bebfe":"code","4940f067":"code","a30a8d11":"code","09e46718":"code","11f29471":"code","0a3f1d13":"code","1294d394":"code","976a8aae":"code","7d8b6821":"code","230a9fd8":"code","2e12aa8d":"code","a73d161c":"code","805bf534":"code","a3215ca4":"code","5ff74e55":"code","8876ab12":"code","2ab8566b":"code","73674153":"code","0a5bcad5":"code","a957ac6a":"code","9f4041f7":"code","3a2e9292":"code","db06da0d":"code","4fc3dcf0":"code","e59e09b8":"code","3c81f72d":"code","09270839":"code","43323025":"code","4c647edb":"code","4185b7e1":"code","8ee30429":"code","14962499":"code","6e8bf2b2":"code","b70a6392":"code","fdc2714a":"code","630ead66":"code","7973ed09":"code","03127b4c":"code","b014301c":"code","34452d37":"code","e3406723":"code","44c88d5a":"code","5c44bb5d":"code","fbfdb9a8":"code","f14095e0":"code","b9b00b10":"code","39fc9439":"code","5ef4b546":"code","dd69c1e7":"code","33912fad":"code","3de12ed1":"code","af068e5d":"code","9b0897ee":"code","b8ce4920":"code","ded14ff3":"code","e8a4a83f":"code","2c068890":"code","a3da95d4":"code","1aee9dcd":"code","d99c0f9b":"code","ffae03f5":"code","8289119f":"code","5e415667":"code","53eb6dd0":"code","91438e0b":"code","6991acdb":"code","c94a82dd":"code","28a1cc40":"code","0ad6e91b":"code","c15ec10b":"code","de62ab67":"code","33949d15":"code","5783410a":"code","29734ad3":"code","667f91b5":"code","5e188ff1":"code","aca54c07":"code","e4d326e8":"code","cee66890":"code","af9275cb":"code","3c2d487d":"code","724aacef":"markdown","5cf0fd75":"markdown","7e0459a0":"markdown","ddd18943":"markdown","0de49a6e":"markdown","f27a7a9b":"markdown","cdae06a5":"markdown","a87520d5":"markdown","1ad0efd2":"markdown","0bd21b5d":"markdown","e17e5af9":"markdown","61d1325e":"markdown","9bffd053":"markdown","7b94c74d":"markdown","bd7a4b56":"markdown","5623a3d2":"markdown","47e4f490":"markdown","a4272155":"markdown","97aac44d":"markdown","0c33ab39":"markdown","137a602d":"markdown","49d7525d":"markdown","73c52c3a":"markdown","786b321f":"markdown","4f9d09f6":"markdown","9baddab1":"markdown","ae2b93db":"markdown","f882b89a":"markdown","82884986":"markdown","21ebef29":"markdown","8d6b1a68":"markdown","facd2866":"markdown","17851c9f":"markdown","297057e6":"markdown","335c5b52":"markdown","f972ed2b":"markdown","6480dbc0":"markdown","6f7e97a2":"markdown","bc7b3ba8":"markdown","2f458bce":"markdown","4832f312":"markdown","cf8467d1":"markdown","0c948ba6":"markdown","791877b3":"markdown","607cdfb1":"markdown","d74acd94":"markdown","e1194dc5":"markdown","12de75c7":"markdown","33923bfb":"markdown","e5a62901":"markdown"},"source":{"65241bdf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","303c35ab":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport missingno as msno\nimport sklearn\nimport category_encoders as ce\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n","af10113c":"house_price = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\n","a5196729":"house_price_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","3f2f55a5":"house_price.head()","2ce7025b":"house_price.shape","2b9860a5":"house_price.info()","69c5f6b2":"house_price.describe()","93d49c17":"null_check = pd.Series(round(100*(house_price.isnull().sum()\/house_price.shape[0]),2))\nnull_check.sort_values(ascending=False)","7594af10":"msno.matrix(house_price)\nplt.show()","fa221d79":"null_check[null_check>30.00]","fd4bebfe":"house_price.drop(['Alley','FireplaceQu','PoolQC','Fence','MiscFeature'],axis=1,inplace=True)","4940f067":"msno.matrix(house_price)\nplt.show()","a30a8d11":"null_check[(null_check>0.00) & (null_check<30.00)]","09e46718":"house_price = house_price[~house_price['GarageType'].isnull()]","11f29471":"null_check_new = pd.Series(round(100*(house_price.isnull().sum()\/house_price.shape[0]),2))\nnull_check_new.sort_values(ascending=False)","0a3f1d13":"null_check_new[(null_check_new>0.00)&(null_check<30.00)]","1294d394":"impute_list = ['LotFrontage','MasVnrType','MasVnrArea','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','Electrical']\nfor i in impute_list:\n    if (house_price[i].dtype=='float64'):\n        house_price[i].fillna(house_price[i].mean() , inplace = True)\n    else:\n        house_price[i].fillna(house_price[i].mode().values[0] , inplace = True)\n","976a8aae":"cols = list(house_price.columns)\nnull_sum = 0\nfor i in cols:\n    null_sum = null_sum+house_price[i].isnull().sum()\nprint(\"Null Valuse in DataFrame : \",null_sum)\n    ","7d8b6821":"num_cols = []\ncat_cols = []\nfor i in cols[1:len(cols)]:\n    if (house_price[i].dtype=='float64')|(house_price[i].dtype=='int64'):\n        num_cols.append(i)\n    else:\n        cat_cols.append(i)\n","230a9fd8":"plt.figure(figsize=(30,20))\n\nviolin_plots = []\ncountplot_cols = []\n\nfor i in num_cols:\n    if (house_price[i].nunique()>25):\n        violin_plots.append(i)\n    else:\n        countplot_cols.append(i)\n#print(len(boxplot_cols))\n        \nfor i in enumerate(violin_plots):\n        #print(i[1])\n    plt.subplot(5,4,i[0]+1)\n    ax =sns.violinplot(house_price[i[1]]) ## KDE with narrow bandwidth to show individual probability lumps\n    #print(i[0]+1)\n    ax.set_xlabel(i[1],fontsize=15)\nplt.tight_layout()\nplt.show()","2e12aa8d":"plt.figure(figsize=(30,30))\nfor i in enumerate(countplot_cols):\n    plt.subplot(6,3,i[0]+1)\n    ax = sns.countplot(x=i[1],data=house_price)\n    ax.set_xlabel(i[1],fontsize=15)\nplt.tight_layout()\nplt.show()","a73d161c":"plt.figure(figsize=(30,30))\n\nfor i in enumerate(violin_plots):\n        #print(i[1])\n    plt.subplot(5,4,i[0]+1)\n    ax =sns.scatterplot(x=i[1],y='SalePrice',data=house_price) ## KDE with narrow bandwidth to show individual probability lumps\n    #print(i[0]+1)\n    ax.set_xlabel(i[1],fontsize=15)\n    ax.set_ylabel(\"Sale Price\",fontsize=15)\n\nplt.tight_layout()\nplt.show()","805bf534":"plt.figure(figsize=(30,40))\nfor i in enumerate(countplot_cols):\n    plt.subplot(6,3,i[0]+1)\n    ax = sns.boxplot(x=i[1],y=\"SalePrice\",data=house_price)\n    ax.set_xlabel(i[1],fontsize=15)\nplt.tight_layout()\nplt.show()","a3215ca4":"plt.figure(figsize = (30, 30))\n\n# ----------------------------------------------------------------------------------------------------\n# plot the data\n# the idea is to iterate over each class\n# extract their data ad plot a sepate density plot\nlarge_cat_cols = []\nsmall_cat_cols = []\nfor j in cat_cols:\n    if (house_price[j].nunique()>5):\n        large_cat_cols.append(j)\n    else:\n        small_cat_cols.append(j)\n\nfor i in enumerate(small_cat_cols):\n    for cyl_ in house_price[i[1]].unique():\n    # extract the data\n        x = house_price[house_price[i[1]] == cyl_][\"SalePrice\"]\n    # plot the data using seaborn\n        plt.subplot(6,4,i[0]+1)\n        ax = sns.kdeplot(x, shade=True, label = \"{}\".format(cyl_))\n        ax.set_xlabel(i[1],fontsize=15)\n        plt.setp(ax.get_legend().get_texts(), fontsize='20') # for legend text\n        #plt.setp(ax.get_legend().get_title(), fontsize='32') # for legend title\n# set the title of the plot\nplt.tight_layout()\nplt.show()\n","5ff74e55":"plt.figure(figsize=(40,30))\nfor i in enumerate(large_cat_cols):\n    plt.subplot(4,4,i[0]+1)\n    ax = sns.scatterplot(x=i[1],y='SalePrice',hue=i[1],data=house_price)\n    ax.set_xlabel(i[1],fontsize=15)\n    ax.tick_params(axis=\"x\", labelsize=15 , rotation=45)\n    #plt.setp(ax.get_legend().get_texts(), fontsize='15') # for legend text\n\n\nplt.tight_layout()\nplt.show()","8876ab12":"y_train = house_price.pop(\"SalePrice\")\nX_train = house_price","2ab8566b":"encoder = ce.OneHotEncoder(cols=cat_cols)\n\nX_train = encoder.fit_transform(X_train) ## one hot encoding on all variables","73674153":"X_test = house_price_test.copy() ## also done one hot encoding on test set as well\n\nX_test = encoder.fit_transform(X_test)","0a5bcad5":"X_test.shape ## check shape of test","a957ac6a":"X_train.shape ## check shape of train","9f4041f7":"sel = VarianceThreshold(threshold=0.1)\nsel.fit(X_train)  # fit finds the features with zero variance","3a2e9292":"# if we sum over get_support, we get the number of features that are not constant\nsum(sel.get_support())","db06da0d":"X_train = X_train[X_train.columns[sel.get_support()]] ## select variables with proper distribution of values","4fc3dcf0":"# Importing RFE and LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n","e59e09b8":"lm = LinearRegression().fit(X_train,y_train)\n## created linear regression model and fit our train data\nrfe = RFE(lm,42).fit(X_train,y_train)\n## select 42 features after running rfe","3c81f72d":"rfe_cols = X_train.columns[rfe.support_]\n## choose features provided by rfe","09270839":"## check column names provided by rfe \nrfe_cols","43323025":"X_train = X_train[rfe_cols] ## X_train contains only features selected by rfe.","4c647edb":"plt.figure(figsize=(30,15))\nsns.heatmap(X_train.corr(),annot=True)\nplt.show()","4185b7e1":"X_train.drop(['LotShape_2','LotConfig_3','ExterQual_2','BsmtQual_2','KitchenQual_2','Exterior2nd_1','Exterior2nd_2'],axis=1,inplace=True)","8ee30429":"X_train.shape ## checking shape","14962499":"# Calculate the VIFs for the new model\nimport statsmodels\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","6e8bf2b2":"X_train.drop('TotRmsAbvGrd',axis=1,inplace=True) ## drop feature ","b70a6392":"vif = pd.DataFrame() ## again compute VIF\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","fdc2714a":"X_train.drop('OverallQual',axis=1,inplace=True)","630ead66":"vif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","7973ed09":"X_train.drop('OverallCond',axis=1,inplace=True)","03127b4c":"vif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","b014301c":"X_train.drop('BedroomAbvGr',axis=1,inplace=True)","34452d37":"vif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","e3406723":"X_train.drop('FullBath',axis=1,inplace=True)","44c88d5a":"vif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","5c44bb5d":"X_train.drop('SaleType_1',axis=1,inplace=True)","fbfdb9a8":"vif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","f14095e0":"X_train.drop('GarageCars',axis=1,inplace=True)","b9b00b10":"vif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","39fc9439":"X_train.drop('Foundation_1',axis=1,inplace=True)","5ef4b546":"vif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","dd69c1e7":"X_train.drop('Condition1_1',axis=1,inplace=True)","33912fad":"vif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","3de12ed1":"X_train.drop('BldgType_1',axis=1,inplace=True)","af068e5d":"vif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","9b0897ee":"X_train.drop('SaleCondition_1',axis=1,inplace=True)","b8ce4920":"vif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","ded14ff3":"X_train.shape ","e8a4a83f":"X_test = X_test[X_train.columns]","2c068890":"X_test.shape ## check shape","a3da95d4":"X_test.isnull().sum() ## check for null value","1aee9dcd":"X_test['BsmtFullBath'].fillna(X_test['BsmtFullBath'].mode().values[0] , inplace = True) ## impute null value of BsmtFullBath with most frequent value","d99c0f9b":"null_count = 0\nfor i in X_test.columns:\n    null_count = null_count+X_test[i].isnull().sum()\nprint(\"Null Values in X_test : \",null_count)\n    ","ffae03f5":"from sklearn.ensemble import RandomForestRegressor ## import libraries\n\nrfr = RandomForestRegressor(random_state=1).fit(X_train,y_train)","8289119f":"y_train_pred = rfr.predict(X_train) ## predict sale price","5e415667":"from sklearn.metrics import r2_score\nr2_score_default = r2_score(y_train,y_train_pred) ## check r2 score of the model","53eb6dd0":"r2_score_default","91438e0b":"from sklearn.metrics import mean_squared_error\n\nmse = mean_squared_error(y_train,y_train_pred)","6991acdb":"mse","c94a82dd":"# Create the parameter grid based on the results of random search \nparams = {\n    'max_depth': [1, 2, 5, 10, 20 ],\n    'min_samples_leaf': [10, 20, 50, 100 , 200 , 400],\n    'max_features': [4 , 8 , 15 , 20],\n    'n_estimators': [10, 30, 50, 100, 200]\n}","28a1cc40":"from sklearn.model_selection import GridSearchCV\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator=rfr, param_grid=params, \n                          cv=4, n_jobs=-1, verbose=1, scoring = \"r2\")","0ad6e91b":"%%time\ngrid_search.fit(X_train,y_train)","c15ec10b":"rf_best = grid_search.best_estimator_","de62ab67":"rf_best","33949d15":"rf_best = rf_best.fit(X_train,y_train)","5783410a":"y_train_pred_tune = rf_best.predict(X_train)","29734ad3":"r2_score_best = r2_score(y_train,y_train_pred_tune)","667f91b5":"r2_score_best","5e188ff1":"mse_best = mean_squared_error(y_train,y_train_pred_tune)","aca54c07":"mse_best","e4d326e8":"test_pred = rf_best.predict(X_test)","cee66890":"house_price_test['SalePrice'] = test_pred","af9275cb":"house_price_test = house_price_test[['Id','SalePrice']]","3c2d487d":"house_price_test.to_csv(\"Submission_house_price.csv\",index=False)\n","724aacef":"# Data Preprocessing","5cf0fd75":"Check final shape of X_train","7e0459a0":"We calculate VIF and remove high vif features and again check the vifs of all features . Do this iteratively unless got all the features less than 5.","ddd18943":"Remove variables more than 30% of null values.","0de49a6e":"check info about our variables.","f27a7a9b":"**Recursive Feature Elemination**\n\nIt is a greedy optimization algorithm which aims to find the best performing feature subset. It repeatedly creates models and keeps aside the best or the worst performing feature at each iteration. It constructs the next model with the left features until all the features are exhausted. It then ranks the features based on the order of their elimination.\n\nRecursive feature elimination performs a greedy search to find the best performing feature subset. It iteratively creates models and determines the best or the worst performing feature at each iteration. It constructs the subsequent models with the left features until all the features are explored. It then ranks the features based on the order of their elimination. In the worst case, if a dataset contains N number of features RFE will do a greedy search for 2N combinations of features.","cdae06a5":"Two different lists are created one contains object type variables and another list contains float and int type variables.","a87520d5":"**Remove Constant Features**\n\nConstant features are those that show the same value, just one value, for all the observations of the dataset. This is, the same value for all the rows of the dataset. These features provide no information that allows a machine learning model to discriminate or predict a target.\nVariance threshold from sklearn is a simple baseline approach to feature selection. It removes all features which variance doesn\u2019t meet some threshold. By default, it removes all zero-variance features, i.e., features that have the same value in all samples.","1ad0efd2":"Check null values of the dataframe.","0bd21b5d":"**Comments:**\n* LotFrontage: Linear feet of street connected to property variable is basically left skewered and belongs from range between 50-100.\n* LotArea: Lot size in square feet is also left skewered and maximum observation gathered in range of 0-2000 (approx).\n* YearBuilt: Original construction date and GarageYrBlt: Year garage was built are overall distributed but maximum data belongs to 2000-2020 yr.\n* YearRemodAdd: Remodel date overall distributed.\n* MasVnrArea: Masonry veneer area in square feet , BsmtFinType1 & 2: Quality of basement finished area are basically left skewered with some outliers.\n* TotalBsmtSF: Total square feet of basement area is distributed but maximum observations beongs from lower range with vary small lower range outliers.\n* 1stFlrSF: First Floor square feet and 2ndFlrSF: Second floor square feet are mainly left skewered but 2ndFlrSF gathered around two different ranges.\n* GarageArea: Size of garage in square feet are mostly gathered in two ranges 400-600 and 200-400.\n* WoodDeckSF: Wood deck area in square feet left skewered mostly with some out liers.\n* OpenPorchSF: Open porch area in square feet left skewered mostly with some out liers.\n* EnclosedPorch: Enclosed porch area in square feet left skewered mostly with some out liers.\n* 3SsnPorch: Three season porch area in square feet left skewered mostly with some out liers.\n* ScreenPorch: Screen porch area in square feet left skewered mostly with some out liers.\n","e17e5af9":"Check r2 score and root mean squared error.","61d1325e":"# Data Inspection","9bffd053":"# Hyper parameter Tuning","7b94c74d":"Check variables those have null values greater than 30%.","bd7a4b56":"Visualise null values of different variables.White lines are denoted as null values ","5623a3d2":"Check variables with top null values.","47e4f490":"Used violin plots to check the distribution of those numerical variables contains more than 25 unique values less than that can be considered as categorical numerical variable for those I used count plots.","a4272155":"**Comments:**\n* Sale price increasing with OverallQual: Overall material(9) and finish quality but few high selling houses don't have max OverallQual: Overall material and finish quality (5)\n* Sale price increasing with max FullBath: Full bathrooms above grade.\n* Sale price increasing with more fireplace and more rooms.\n* Sale price is also increasing with more cars in garages.","97aac44d":"Import libraries like numpy,pandas,matplotlib and seaborn for visualization,warnings,missingo for visualization of missing values.","0c33ab39":"**Comments:**\n* Sale price increasing with recent YearBuilt: Original construction date same for GarageYrBlt: Year garage was built\n* Sale price increasing with 1stFlrSF: First Floor square feet and 2ndFlrSF: Second floor square feet althogh there are some houses with no 2nd floor but high selling price.\n* Sale price also increasing with GrLivArea: Above grade (ground) living area square feet.","137a602d":"**violin plots** are a method of plotting numeric data and can be considered a combination of the box plot with a kernel density plot.\nFor more information about violin plots see the link provided : [https:\/\/towardsdatascience.com\/violin-plots-explained-fb1d115e023d](http:\/\/)","49d7525d":"# Objective \nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges is to predict the final price of each home.\n\n# Data Description\nHere's a brief version of what you'll find in the data description file.\n\nSalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\nMSSubClass: The building class\nMSZoning: The general zoning classification\nLotFrontage: Linear feet of street connected to property\nLotArea: Lot size in square feet\nStreet: Type of road access\nAlley: Type of alley access\nLotShape: General shape of property\nLandContour: Flatness of the property\nUtilities: Type of utilities available\nLotConfig: Lot configuration\nLandSlope: Slope of property\nNeighborhood: Physical locations within Ames city limits\nCondition1: Proximity to main road or railroad\nCondition2: Proximity to main road or railroad (if a second is present)\nBldgType: Type of dwelling\nHouseStyle: Style of dwelling\nOverallQual: Overall material and finish quality\nOverallCond: Overall condition rating\nYearBuilt: Original construction date\nYearRemodAdd: Remodel date\nRoofStyle: Type of roof\nRoofMatl: Roof material\nExterior1st: Exterior covering on house\nExterior2nd: Exterior covering on house (if more than one material)\nMasVnrType: Masonry veneer type\nMasVnrArea: Masonry veneer area in square feet\nExterQual: Exterior material quality\nExterCond: Present condition of the material on the exterior\nFoundation: Type of foundation\nBsmtQual: Height of the basement\nBsmtCond: General condition of the basement\nBsmtExposure: Walkout or garden level basement walls\nBsmtFinType1: Quality of basement finished area\nBsmtFinSF1: Type 1 finished square feet\nBsmtFinType2: Quality of second finished area (if present)\nBsmtFinSF2: Type 2 finished square feet\nBsmtUnfSF: Unfinished square feet of basement area\nTotalBsmtSF: Total square feet of basement area\nHeating: Type of heating\nHeatingQC: Heating quality and condition\nCentralAir: Central air conditioning\nElectrical: Electrical system\n1stFlrSF: First Floor square feet\n2ndFlrSF: Second floor square feet\nLowQualFinSF: Low quality finished square feet (all floors)\nGrLivArea: Above grade (ground) living area square feet\nBsmtFullBath: Basement full bathrooms\nBsmtHalfBath: Basement half bathrooms\nFullBath: Full bathrooms above grade\nHalfBath: Half baths above grade\nBedroom: Number of bedrooms above basement level\nKitchen: Number of kitchens\nKitchenQual: Kitchen quality\nTotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\nFunctional: Home functionality rating\nFireplaces: Number of fireplaces\nFireplaceQu: Fireplace quality\nGarageType: Garage location\nGarageYrBlt: Year garage was built\nGarageFinish: Interior finish of the garage\nGarageCars: Size of garage in car capacity\nGarageArea: Size of garage in square feet\nGarageQual: Garage quality\nGarageCond: Garage condition\nPavedDrive: Paved driveway\nWoodDeckSF: Wood deck area in square feet\nOpenPorchSF: Open porch area in square feet\nEnclosedPorch: Enclosed porch area in square feet\n3SsnPorch: Three season porch area in square feet\nScreenPorch: Screen porch area in square feet\nPoolArea: Pool area in square feet\nPoolQC: Pool quality\nFence: Fence quality\nMiscFeature: Miscellaneous feature not covered in other categories\nMiscVal: $Value of miscellaneous feature\nMoSold: Month Sold\nYrSold: Year Sold\nSaleType: Type of sale\nSaleCondition: Condition of sale","73c52c3a":"Check head of our dataframe","786b321f":" Remove some variables those are co realted with another variable like 'LotShape_2','LotConfig_3','ExterQual_2','BsmtQual_2','KitchenQual_2','Exterior2nd_1','Exterior2nd_2'","4f9d09f6":"**Variance inflation factor (VIF)** is a measure of the amount of multicollinearity in a set of multiple regression variables. Mathematically, the VIF for a regression model variable is equal to the ratio of the overall model variance to the variance of a model that includes only that single independent variable.","9baddab1":"Let's identify some predictor variables that are co realated and remove one of them.","ae2b93db":"Model looks like overfitted Hence it is a very unstable model.Let's do some hyper parameter tuning to make a stable model.","f882b89a":"**Comments:**\n* MSSubClass: The building class max count 20.\n* OverallCond: Overall condition rating max count 5,6.\n* LowQualFinSF: Low quality finished square feet (all floors) max counts 0, BsmtFullBath: Basement full bathrooms max counts 0 and BsmtHalfBath: Basement half bathrooms mostly counts 0.\n* TotRmsAbvGrd: Total rooms above grade (does not include bathrooms) mostly 6.\n* Kitchen: Number of kitchens and Fireplaces: Number of fireplaces mostly 1.\n* GarageCars: Size of garage in car capacity max 2 \n* FullBath: Full bathrooms above grade and Bedroom: Number of bedrooms above basement level max counts2.\n\n","82884986":"Again visualise null values.","21ebef29":"Read our data in 'house_price'","8d6b1a68":"Check shape of our dataframe.","facd2866":"Chose the best estimator and fit it on the data.","17851c9f":"Divide X_train and y_train.","297057e6":"**One Hot Encoding** refers to splitting the column which contains numerical categorical data to many columns depending on the number of categories present in that column. Each column contains \u201c0\u201d or \u201c1\u201d corresponding to which column it has been placed.","335c5b52":"Prepare X_test with the same feature as X_train.","f972ed2b":"Check for null value percentage in our dataframe.","6480dbc0":"Recheck null value percentage of variables.","6f7e97a2":"* There is a pattern in null values those houses which don't have garage have null values in GarageType,GarageYrBlt,GarageFinish,GarageQual,GarageCond.\n* Those houses which don't have basements have null values in BsmtQual,BsmtCond,BsmtExposure,BsmtFinType1,BsmtFinSF1,BsmtFinSF2.\n* LotFrontage having randomly missing values.\n* Those houses which don't have Masonry veneer type for them has missing values in MasVnrType and area.","bc7b3ba8":"Distribution of different features by their values.","2f458bce":"# Model Build","4832f312":"Read our test data","cf8467d1":"Remove Those records who don't have garage.","0c948ba6":"# EDA","791877b3":"Check statistical description of our different numerical variables.","607cdfb1":"check r2 score and root mean squared error of our best estimator random forest regressor model.","d74acd94":"# Conclusion\nWe can use other proccess to make more better prediction we can also use different set of parameters to tune hyper parameters of our model and also chose different proccess to reduce dimensionality of the data set.","e1194dc5":"* Impute missing values of numerical columns with their mean\/average values.\n* Impute missing values of categorical columns with their mode\/most frequent values.\n","12de75c7":"**Comments:**\n* high Sale price depends on following type of features\n* Condition1: Proximity to various conditions Norm\n* HouseStyle: Style of dwelling 2Story\t\n* RoofStyle: Type of roof Hip and Gable\n* RoofMatl: Roof material CompShg\tStandard (Composite) Shingle\n* Foundation: Type of foundation PConc\tPoured Contrete\t\n* Heating: Type of heating GasA\tGas forced warm air furnace\n* GarageType attached and sale codition normal is more preferable.","33923bfb":"predict on test data using our bet model.","e5a62901":"A **random forest** is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n\nFor more information : [https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html](http:\/\/)"}}