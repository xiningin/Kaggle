{"cell_type":{"2cc0f344":"code","2d6fcc2d":"code","648f2753":"code","84cfa4c8":"code","fa9752bc":"code","3c1632c2":"code","3f301052":"code","379eb321":"code","dda70e8c":"code","be327b12":"code","f18c9bb5":"code","7d270d27":"code","4ac9eb8f":"code","c3a7d909":"code","c000c1c1":"code","237d1c4c":"code","e788d942":"code","f7ed919b":"code","43322cd9":"code","421d3af3":"code","230ca739":"code","f9935235":"markdown","f1b54674":"markdown","ad3101ca":"markdown","03ea7ced":"markdown","40549a50":"markdown","28d0021e":"markdown","91e8e9a8":"markdown","854a3451":"markdown","79f99248":"markdown","44f8a1ad":"markdown","f519a9d5":"markdown","230e3208":"markdown","f234b7aa":"markdown","7582e315":"markdown","51599019":"markdown"},"source":{"2cc0f344":"# Import necessary libraries and dependencies\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.cluster import KMeans\nimport sklearn\n%matplotlib inline","2d6fcc2d":"# Read data\noriginal_data = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")\noriginal_data.head()","648f2753":"#Display Pearson correlation HeatMap for all variables\nplt.figure(figsize=(30,20))\ncor = original_data.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.show()","84cfa4c8":"sns.scatterplot(x=original_data['V13'], y=original_data['V17'], hue=original_data['Class'])","fa9752bc":"sns.scatterplot(x=original_data['V13'], y=original_data['V14'], hue=original_data['Class'])","3c1632c2":"sns.scatterplot(x=original_data['V13'], y=original_data['V12'], hue=original_data['Class'])","3f301052":"# Plot KDE for V17 values that belong to Class 0 (Normal)\nsns.kdeplot(data=original_data[original_data['Class'] == 0]['V17'], label=\"Class 0\", shade=True)\n# Plot KDE for V17 values that belong to Class 1 (Fraud)\nsns.kdeplot(data=original_data[original_data['Class'] == 1]['V17'], label=\"Class 1\", shade=True)","379eb321":"sns.kdeplot(data=original_data[original_data['Class'] == 0]['V14'], label=\"Class 0\", shade=True)\nsns.kdeplot(data=original_data[original_data['Class'] == 1]['V14'], label=\"Class 1\", shade=True)","dda70e8c":"sns.kdeplot(data=original_data[original_data['Class'] == 0]['V12'], label=\"Class 0\", shade=True)\nsns.kdeplot(data=original_data[original_data['Class'] == 1]['V12'], label=\"Class 1\", shade=True)","be327b12":"sns.kdeplot(data=original_data[original_data['Class'] == 0]['V25'], label=\"Class 0\", shade=True)\nsns.kdeplot(data=original_data[original_data['Class'] == 1]['V25'], label=\"Class 1\", shade=True)","f18c9bb5":"# Values of the variable V14 that belong to Class 0 (Normal)\noriginal_data[original_data['Class'] == 0]['V14'].describe()","7d270d27":"# Values of the variable V14 that belong to Class 1 (Fraud)\noriginal_data[original_data['Class'] == 1]['V14'].describe()","4ac9eb8f":"# The true lables\ny = original_data.Class\n\n# This is our classifier \nhigh_accuracy_y = [0 if i>-4 and i<4 else 1 for i in original_data['V14']]\n\n# Calculate accuracy\naccuracy_score(y, high_accuracy_y)","c3a7d909":"# Display confusion matrix for our high accuracy classifier\nsns.set(font_scale=3)\nconfusion_matrix = sklearn.metrics.confusion_matrix(y, high_accuracy_y)\n\nplt.figure(figsize=(16, 14))\nsns.heatmap(confusion_matrix, annot=True, fmt=\"d\", annot_kws={\"size\": 20});\nplt.title(\"Confusion matrix\", fontsize=30)\nplt.ylabel('True label', fontsize=25)\nplt.xlabel('Clustering label', fontsize=25)\nplt.show()","c000c1c1":"TN = confusion_matrix[0][0] # True Negative\nFP = confusion_matrix[0][1] # False Positive\nFN = confusion_matrix[1][0] # False Negative\nTP = confusion_matrix[1][1] # True Positive\n# Recall\nTP\/(TP+FN)","237d1c4c":"# This is our high recall classifier \nhigh_recall_y = [0 if i>-1.05 and i<3 else 1 for i in original_data['V14']]\n\naccuracy_score(y, high_recall_y)","e788d942":"# Display confusion matrix for our high recall classifier\nsns.set(font_scale=3)\nconfusion_matrix = sklearn.metrics.confusion_matrix(y, high_recall_y)\n\nplt.figure(figsize=(16, 14))\nsns.heatmap(confusion_matrix, annot=True, fmt=\"d\", annot_kws={\"size\": 20});\nplt.title(\"Confusion matrix\", fontsize=30)\nplt.ylabel('True label', fontsize=25)\nplt.xlabel('Clustering label', fontsize=25)\nplt.show()","f7ed919b":"TN = confusion_matrix[0][0] # True Negative\nFP = confusion_matrix[0][1] # False Positive\nFN = confusion_matrix[1][0] # False Negative\nTP = confusion_matrix[1][1] # True Positive\n# Recall\nTP\/(TP+FN)","43322cd9":"X = original_data[['V17', 'V14', 'V12']]\nkmeans = KMeans(n_clusters=2, max_iter=3000, n_init=20)\n\n# Fit and then store predictions in y_pred_kmeans\ny_pred_kmeans = kmeans.fit_predict(X)\n# Calculate accuracy\naccuracy_score(y, y_pred_kmeans)","421d3af3":"# Display confusion matrix for K-means\nsns.set(font_scale=3)\nconfusion_matrix = sklearn.metrics.confusion_matrix(y, y_pred_kmeans)\n\nplt.figure(figsize=(16, 14))\nsns.heatmap(confusion_matrix, annot=True, fmt=\"d\", annot_kws={\"size\": 20});\nplt.title(\"Confusion matrix\", fontsize=30)\nplt.ylabel('True label', fontsize=25)\nplt.xlabel('Clustering label', fontsize=25)\nplt.show()","230ca739":"TN = confusion_matrix[0][0] # True Negative\nFP = confusion_matrix[0][1] # False Positive\nFN = confusion_matrix[1][0] # False Negative\nTP = confusion_matrix[1][1] # True Positive\n# Recall\nTP\/(TP+FN)","f9935235":"### 1. Loading data\n\nLet's start by loading our dataset and display the first rows.","f1b54674":"### 6. Conclusion\n\nBy a careful exploration of the dataset, we successfully identified the most important independent variables. We also understood their relationship with the dependent variable, and that led us to a simple solution for this dataset using only one variable and one line of code. Moreover, we showed that our elegant solution outperformed a KMeans model built using the top three independent variables. ","ad3101ca":"We got a Recall of **76%**, which is not that bad, but I claim that we can do better. But how can we classify more of the positive transactions correctly?\n\nThe idea is to narrow down the interval for the negative transactions. By doing that, we are going to miss classify some of the negative transactions which will decrease the accuracy. But in return, we will increase the number of correctly classified positive transactions which increases the recall. It's a tradeoff! If you look at the KDE of V14 this will make much more sense. ","03ea7ced":"### Great! We have a fair trade-off with 90% for both accuracy and recall. You can try to find a better interval!","40549a50":"**Did I just blow your mind?!**\n\nLet's slow down for a minute and think carefully of what I just did. But first, I have to come clean. I did hide something from you, and if you are already familiar with this dataset, you probably know what I'm talking about. This data set is highly imbalanced, and the positive class (frauds) account for only 0.172% of all transactions.\n\n**What does that mean?**\n\nWell, the accuracy metric is not a fair measurement of the performance of my classifier, because if I classified all negative (normal) transactions correctly, then I will achieve a very high accuracy since almost all data points belong to this class, and it does not matter how many data points from the other class I classified correctly. That is exactly what I did, I figured out from the KDE that most normal transactions have a V14 value between 4 and -4. \n\n**Solution?**\n\nWe should consider another metric to measure how many data points from the positive class did we classify correctly, and that metric is called Recall. So let's display the confusion matrix and calculate the Recall.\n","28d0021e":"**And for contrasting, let's plot one of the boring variables.**","91e8e9a8":"We can notice in all three plots we can draw a horizontal line that classifies almost all data points into their correct Class value. In other words, if we look at the y-axis that corresponds to one of the interesting variables **{V17, V14, V12}**, then we can see that most fraudulent data points are located below the value of *-5* and normal data points are located above.\n\nIn contrast, if we look at the *x-axis* where the boring variable is, then both fraudulent and normal data points are almost equally distributed between *-4* and *4*, which means that we can't draw a vertical line to separate the two groups.\n\n**Is that good enough?**\n\nNo. Although scatter plots gave us a general idea of what is going on, but they are not precise. So how we can take a better view?","854a3451":"### 3. Scatter plots\n\nThe next step is to visualise the interesting relations that we obtained from the correlation matrix using scatter plots.\n\nWhat I'm going to do here is to plot each one of the interesting variables against a very boring variable *V13* and color each data point with corresponding to it's label (Fraud or Normal)","79f99248":"Now we now the top 3 independent variables **{V17, V14, V12}** that should contains important information about whether a giving credit card transaction is fraudulent or not. We will call them from now on the interesting variables. What next?","44f8a1ad":"### 2. Correlation matrix  HeatMap\n\nNow let's start our exploration with my best tool, the Pearson correlation matrix  HeatMap!\n\nThe Pearson correlation matrix is a great tool to take a first look at your data. It's the best way to start looking for interesting things. It displays the correlation coefficient between any two variables in the dataset. The correlation coefficient takes values between -1 and 1. A value of 1 indicating a very strong positive relation between the two variables, while a value of -1 indicating a very strong negative relation, and a value of 0 indicates no relation.\n\n**So what we are looking for here?**\n\nWe are interested in looking into the relation between the independent variables and the dependent variable *Class*, and see which variables is highly correlated with our target variable the *Class*. Look at the bottom row of the matrix.","f519a9d5":"Let's summarize our findings in the following points: \n- In every interesting variable, the distribution of the normal transaction takes a shape that very close to the standard normal distribution.\n- In every interesting variable, the distribution of the fraudulent transaction takes a shape that very close to a normal distribution with height standard deviation (highly spread).\n- In the boring variable, both fraudulent and normal transactions have the same distribution. close to standard normal distribution.\n\nNow let's display some statistical measurements to support our findings.","230e3208":"### 5. KMeans clustering\n\nI want to build and train a KMeans model using all three of our interesting variables and compare its performance with our little classifier.","f234b7aa":"**How useful our findings are?**\n\nI claim that by only using the variable V14 and only one line of code I can achieve an accuracy of 99% predicting whether a credit card transaction is fraudulent or normal..","7582e315":"### 4. Kernel density estimation (KDE) plots\n\nNow what I'm about to do here is really awesome!\n\nI will take each one of the interesting variables and approximate the underlying probability density function for each *Class* value (Frauds Vs. Normal) using kernel density estimation. This should give us a clear idea of how fraudulent and normal datapoints (credit card transactions) are distributed along each variable.","51599019":"# The power of Exploratory Data Analysis (EDA)\n\nThe goal of this notebook is to demonstrate the significance of EDA. If you are looking for a state-of-the-art solution to fraud detection, then you are not going to find it here. On the other hand, if you are here to learn useful tools to understand your data, then buckle up!\n\nI can't stress enough how important it is to always start with EDA for any data science project. I know that it's very tempting to jump right away into trying fancy machine learning algorithms on your data, instead it's recommended that you plot your data and look at it and try to extract as much useful information as you can.\n\nI claim that by only looking at my data, I came up with a very simple yet powerful solution to this dataset **with no machine learning**, and this solution defeated a k-means clustering model."}}