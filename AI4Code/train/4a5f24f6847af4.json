{"cell_type":{"85e69b8d":"code","e8c2b49a":"code","71bb532d":"code","a1040008":"code","5c9113fa":"code","32f2030c":"code","5f0ab31c":"code","a763c7b6":"code","7bf3fbfb":"code","108f0bbf":"code","e797a9ef":"code","9c5d06a0":"code","62c9454c":"code","d0c25f26":"code","5d23c8ce":"code","908c0bdd":"code","a4889577":"code","46d18e7b":"code","62cb3a39":"code","d46f0991":"code","61edb717":"code","feb1b9c3":"code","809de195":"code","f68006aa":"code","95f8a2e5":"code","f0685258":"code","e5c25d3d":"code","9490be0f":"code","02849451":"code","c76b4f90":"code","b4ef85c6":"code","0325fb9b":"code","fe04982d":"code","2a01c3e5":"code","6e14db58":"code","1529db04":"code","06117ab0":"code","a54f7823":"code","8b29a93d":"code","8219474e":"code","30e7a4fc":"code","3c3bfca0":"code","37c43da7":"code","6ecb2e81":"code","d5a864d2":"markdown","bf96f5a3":"markdown","dedce247":"markdown","0609ce7b":"markdown","ecc815ad":"markdown","9aa1e5e9":"markdown","5c149b1b":"markdown","fe898087":"markdown","0e6b75bf":"markdown","b32e6585":"markdown","3065d46a":"markdown","de5b372b":"markdown","1b8c8cbb":"markdown","b937ccdb":"markdown","3f995702":"markdown","f9a6cd8c":"markdown","0680f9c8":"markdown","362ef54a":"markdown","e7332d44":"markdown","6dc4db73":"markdown","17d72ea0":"markdown","545da060":"markdown","2c0a1207":"markdown","9b6a6731":"markdown"},"source":{"85e69b8d":"# Let's ensure that we install some packages on our Kaggle VM\n!pip install tweepy nltk spacy python-louvain python-igraph","e8c2b49a":"# Installs spacy portuguese model\n!python -m spacy download pt_core_news_sm","71bb532d":"import numpy as np\nimport pandas as pd\nimport networkx as nx\nimport community as community_louvain\nfrom igraph import *\n\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\n\nimport os, tweepy, sys, unicodedata, re, csv, spacy, ast\nfrom time import sleep, time, strptime, strftime\nfrom datetime import date, datetime, timedelta\nfrom unidecode import unidecode\n\nfrom nltk.corpus import stopwords","a1040008":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5c9113fa":"import pt_core_news_sm","32f2030c":"# Loads SpaCy's model\nnlp = pt_core_news_sm.load(disable=['tagger', 'ner', 'textcat'])\n#nlp = English().from_disk(\"\/model\", disable=[\"ner\"])","5f0ab31c":"# Getting secrets from Kaggle\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nCONSUMER_KEY = user_secrets.get_secret(\"CONSUMER_KEY\")\nCONSUMER_SECRET = user_secrets.get_secret(\"CONSUMER_SECRET\")\nACCESS_TOKEN = user_secrets.get_secret(\"ACCESS_TOKEN\")\nACCESS_TOKEN_SECRET = user_secrets.get_secret(\"ACESS_TOKEN_SECRET\")","a763c7b6":"auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\nauth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)","7bf3fbfb":"# And we may call the REST API only to check if our credentials are ok!\napi = tweepy.API(auth)\nImage(api.me()._json['profile_image_url_https'])","108f0bbf":"class CustomStreamListener(tweepy.StreamListener):\n    def __init__(self, time_limit=25):\n        # For now, we won't want this class to be running forever\n        \n        self.start_time = time()\n        \n        self.limit = time_limit\n        \n        super(CustomStreamListener, self).__init__()\n\n    def on_status(self, status): # First, we'll deal with the incoming statuses\n        \n        # A little note to limit the streaming time!\n        if (time() - self.start_time) >= self.limit:\n            return False\n\n        # We'll check if retweet\n        is_retweet =  hasattr(status, 'retweeted_status')\n\n        # Then, we'll check if extended tweet\n        if hasattr(status, 'extended_tweet'):\n            \n            text = status.extended_tweet['full_text']\n            \n        else:  # if neither of them\n            \n            text = status.text\n           \n\n        # BUT let's check if quote tweet\n        is_quote_tweet = hasattr(status, 'quoted_status')\n        quoted_text = ''\n\n        if is_quote_tweet:\n            # And if it is, let's check if quote tweet has been truncated\n            \n            if hasattr(status.quoted_status, 'extended_tweet'):\n                \n                quoted_text = status.quoted_status.extended_tweet['full_text']\n                \n            else:\n                \n                quoted_text = status.quoted_status.text\n                \n        # Then, let's define what we're going to get\n        id_str = status.id_str\n        created_at = status.created_at\n        user_screen_name = status.user.screen_name\n        user_location = status.user.location\n        followers_count = status.user.followers_count\n        retweet_count = status.retweet_count\n        status_source = status.source\n        mentions = status.entities['user_mentions']\n        \n        print(str(id_str), \n              str(created_at), \n              str(user_screen_name), \n              str(user_location), \n              str(followers_count), \n              str(retweet_count), \n              str(status_source), \n              str(text),\n             '\\n -> User mentions: {}'.format(str(mentions)))","e797a9ef":"SUBJECTS = ['Lady Gaga', 'Katy Perry', 'Dua Lipa', 'Beyonce', \n            'Taylor Swift', 'Ariana Grande', 'Miley Cyrus', 'Selena Gomez']","9c5d06a0":"# Then, we create the streaming_api object\nstreaming_api = tweepy.streaming.Stream(auth, CustomStreamListener(), \n                                        timeout=25, tweet_mode='extended')\n\n# Here, we also define the portuguese language (hence the pt comes from)\nstreaming_api.filter(track=SUBJECTS, languages=['pt'], is_async=False) ","62c9454c":"class CustomStreamListener(tweepy.StreamListener):\n    def __init__(self, time_limit=25):\n        \n        # For now, we won't want this class to be running forever\n        self.start_time = time()\n        \n        self.limit = time_limit\n        \n        super(CustomStreamListener, self).__init__()\n\n    def on_status(self, status): # First, we'll deal with the incoming statuses\n        \n        # A little note to limit the streaming time!\n        if (time() - self.start_time) >= self.limit:\n            return False # Return False kills the stream\n\n        # We'll check if retweet\n        is_retweet =  hasattr(status, 'retweeted_status')\n\n        # Then, we'll check if extended tweet\n        if hasattr(status, 'extended_tweet'):\n            \n            text = status.extended_tweet['full_text']\n            \n        else:  # if neither of them\n            \n            text = status.text\n           \n\n        # BUT let's check if quote tweet\n        is_quote_tweet = hasattr(status, 'quoted_status')\n        quoted_text = ''\n\n        if is_quote_tweet:\n            \n            # And if it is, let's check if quote tweet has been truncated            \n            if hasattr(status.quoted_status, 'extended_tweet'):\n                \n                quoted_text = status.quoted_status.extended_tweet['full_text']\n                \n            else:\n                \n                quoted_text = status.quoted_status.text\n                \n        # Then, let's define what we're going to get\n        id_str = status.id_str\n        created_at = status.created_at\n        user_screen_name = status.user.screen_name\n        user_location = status.user.location\n        followers_count = status.user.followers_count\n        retweet_count = status.retweet_count\n        status_source = status.source\n        mentions = status.entities['user_mentions']\n\n        # It's useful to clean some undesired characters \n        # From https:\/\/stackoverflow.com\/questions\/33404752\/removing-emojis-from-a-string-in-python\/49146722#49146722\n        emoji_pattern = re.compile(\"[\"\n         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n         u\"\\U00002702-\\U000027B0\"\n         u\"\\U000024C2-\\U0001F251\"\n         \"]+\", flags=re.UNICODE)\n\n        text = emoji_pattern.sub(r'', text)\n        quoted_text = emoji_pattern.sub(r'', quoted_text)\n      \n        try: # save to csv \n            \n            # We can convert, optionally, the created_at timezone to \n            # our own (this is mine, at Brazil)\n            conv_created_at = created_at - timedelta(hours=3)\n\n            # this block saves effectively to csv, appending to the \n            # file with the respective date\n            with open('{}.csv'\n                      .format(conv_created_at\n                              .date()\n                              .strftime('%Y%m%d')),\n                      mode='a', \n                      newline='', \n                      encoding='utf-8') as csvfile:\n\n                csvwriter = csv.writer(csvfile, delimiter=',')\n\n                csvwriter.writerow([id_str, \n                                    created_at, \n                                    conv_created_at, \n                                    text,\n                                    quoted_text, \n                                    is_retweet, \n                                    user_screen_name, \n                                    user_location, \n                                    followers_count,\n                                    retweet_count,\n                                    status_source,\n                                    mentions])\n            \n        except Exception as e:\n                print(\"Error saving to csv: \", e)\n                return\n            \n    #####                #####\n    ##### ERROR HANDLING #####\n    #####                #####\n    \n    def on_error(self, status_code):  # Here, we'll deal with general errors. \n        # We'll append the error to a log.txt file\n        \n        with open('log.txt', 'a') as f:\n            \n            f.write('Encountered error with status code: ' + \n                    str(status_code) + ',' + str(datetime.now()) + '\\n')\n        \n        return True # Returning True doesn't kill the stream\n    \n    \n    def on_limit(self, status_code): # Here, we'll deal with the most \n                                            # feared error: Rate Limits\n        \n        with open('log.txt', 'a') as f:\n            \n            f.write('Rate Limit Exceeded... ' + str(status_code) + ',' + str(datetime.now()) + '\\n')\n        \n        return True  # Fun stuff: when you return True here, Tweepy takes care by\n                        # exponentially increasing the time between each call\n                        # when you Rate Limit, as per Twitter docs here: \n                        # https:\/\/developer.twitter.com\/en\/docs\/basics\/rate-limiting\n\n            \n    def on_disconnect(self, notice): \n        # Sometimes, twitter may disconnect \n        # you, for some reason.        \n        # You may read more about it here \n        # https:\/\/developer.twitter.com\/en\/docs\/tutorials\/consuming-streaming-data#disconnections\n        \n        with open('log.txt', 'a') as f:  \n            \n            f.write('Disconnected: ' + str(notice) + ',' + str(datetime.now()) + '\\n')\n        \n        sleep(2)\n        \n        return False # Obviously, the stream is killed\n\n    \n    def on_timeout(self): # And other times, you may only find a timeout.\n        \n        with open('log.txt', 'a') as f:\n            \n            f.write('Timeout... ' + ',' + str(datetime.now()) \n                    + '\\n') # We log the incident...\n        \n        sleep(60) # We wait...\n        \n        return True # We try again (don't kill the stream)","d0c25f26":"# Ensure we're on this folder\nos.chdir('\/kaggle\/working')","5d23c8ce":"# Then, we create the streaming_api object\nstreaming_api = tweepy.streaming.Stream(auth, CustomStreamListener(), \n                                        timeout=25, tweet_mode='extended')\n\n# Here, we also define the portuguese language (hence the pt comes from)\nstreaming_api.filter(track=SUBJECTS, languages=['pt'], is_async=False) ","908c0bdd":"cols = ['id_str',\n        'created_at',\n        'conv_created_at',\n        'text',\n        'quoted_text',\n        'is_retweet',\n        'user_screen_name',\n        'user_location',\n        'followers_count',\n        'retweet_count',\n        'status_source',\n        'mentions']\n\n# To automatically read our data from the current day, we'd use this. \n# But we're going to use a previously saved file today.\n# df = pd.read_csv((datetime.today() - timedelta(hours=3)).strftime('%Y%m%d') + '.csv',\n#           names = cols, encoding='utf-8')\n# df.head()","a4889577":"# This is the previously saved file\ndf = pd.read_csv('\/kaggle\/input\/sample_data.csv', \n                 names=['id_str', 'created_at','conv_created_at', 'text',\n                        'quoted_text', 'is_retweet', 'user_screen_name', \n                        'user_location', 'followers_count', 'retweet_count', \n                        'status_source', 'mentions'])","46d18e7b":"# Taking a sample\ndf = df.iloc[:500,:]","62cb3a39":"print('Dataframe shape: ' + str(df.shape))","d46f0991":"df.head()","61edb717":"# This is an example of Portuguese stopwords\nsr = stopwords.words('portuguese')\nprint(sr)\nprint(len(sr))","feb1b9c3":"# Remove RT, links, special characters, \n# hashtags, mentions (these are on entities object)\n\n# Makes everything lowercase\ndf['text'] = df['text'].str.lower()\ndf['quoted_text'] = df['quoted_text'].str.lower()\n\n# Removes the word RT\ndf['replaced_text'] = df['text'].str.replace(r'\\s*rt\\s', '',\n                                             case=True, \n                                             regex=True)\n\n# Replacing @s and #s terms - Or you can use this regex \n# as a way to find and store these values\n# Or you can use the Twitter's Entities object: \n# https:\/\/developer.twitter.com\/en\/docs\/tweets\/data-dictionary\/overview\/entities-object\ndf['replaced_text'] = df['replaced_text'].str.replace(r'\\s([@#][\\w_-]+)', '',\n                                                      case=False,\n                                                      regex=True)\n\n# Replaces URLs in any position (start, middle or end)- Or you can use this regex as a way to find and store these values\ndf['replaced_text'] = df['replaced_text'].str.replace(r'http\\S+\\s|\\swww.\\S+\\s|http\\S+|www.\\S+|\\shttp\\S+|\\swww.\\S+', '', \n                                                      case=False)\n\n# Removes special characters from words. Ex. amanh\u00e3 -> amanha\ndf['replaced_text'] = df['replaced_text'].apply(lambda text: unidecode(text))\n#df['replaced_quoted_text'] = df['replaced_quoted_text'].apply(lambda text: unidecode(text))\n\n\n# Removes any remaining special characters\ndf['replaced_text'] = df['replaced_text'].str.replace(r'[^0-9a-zA-Z ]+', '', \n                                                      case=False, \n                                                      regex=True)\n\n# Same as before for the the quoted text\n#df['replaced_quoted_text'] = df['replaced_quoted_text'].str.replace(r'(^|[ ])rt', '', case=False)\n#df['replaced_quoted_text'] = df['quoted_text'].str.replace(r'\\s([@#][\\w_-]+)', '', case=False)\n#df['replaced_quoted_text'] = df['replaced_quoted_text'].str.replace(r'http\\S+\\s|\\swww.\\S+\\s|http\\S+|www.\\S+|\\shttp\\S+|\\swww.\\S+', '', case=False)\n#df['replaced_quoted_text'] = df['replaced_quoted_text'].str.replace(r'[^0-9a-zA-Z ]+', '', case=False)","809de195":"df[['text', 'replaced_text']].head()","f68006aa":"# See https:\/\/www.kaggle.com\/caractacus\/thematic-text-analysis-using-spacy-networkx\ntokens = []\nparsed_doc = [] \ncol_to_parse = 'replaced_text'\n\nfor doc in nlp.pipe(df[col_to_parse].astype('unicode').values, batch_size=50,\n                        n_threads=3):\n    if doc.is_parsed:\n        parsed_doc.append(doc)\n        tokens.append([n.text for n in doc if n.text not in sr])\n    else:\n        # We want to make sure that the lists of parsed results have the\n        # same number of entries of the original Dataframe, \n        # so add some blanks in case the parse fails\n        parsed_doc.append(None)\n        tokens.append(None)\n\n\ndf['parsed_doc'] = parsed_doc\ndf['tokenized'] = tokens\n\n# Ensure that we won't have any whitespace on tokens\ndf['tokenized'] = df['tokenized'].apply(lambda text: [word for word in text if word != ' '])","95f8a2e5":"df.head()","f0685258":"# We could can all the words that are related with the\n# code snippet below.\ndf['edges'] = df['tokenized'].apply(lambda col: tuple(((x,y) for x in col for y in col if x != y)))\n\ndf['edges'].head()","e5c25d3d":"# Adapted from https:\/\/www.kaggle.com\/caractacus\/thematic-text-analysis-using-spacy-networkx\nG = nx.Graph() # undirected\nn = 0\n\nfor row in df.iterrows():\n    \n    for tup in row[1]['edges']:\n        \n        G.add_edge(tup[0], tup[1])\n        \n        n += 1        ","9490be0f":"print(G.number_of_nodes(), \"nodes, and\", G.number_of_edges(), \"edges created.\")","02849451":"# https:\/\/stackoverflow.com\/questions\/40941264\/how-to-draw-a-small-graph-with-community-structure-in-networkx\npartition = community_louvain.best_partition(G)","c76b4f90":"# Assigning a position to the nodes\npos=nx.spring_layout(G)","b4ef85c6":"# To check edges:\n# G.edges()","0325fb9b":"# Check partition values\npartition.values()","fe04982d":"fig = plt.figure(figsize=(15,15))\n\n# Draws a graph and defines color by nodes partitions\nnx.draw_networkx_nodes(G, pos, node_size=20, cmap=plt.cm.RdYlBu, node_color=list(partition.values()))\nnx.draw_networkx_edges(G, pos, alpha=0.1, edge_color='lightgray', width=0.1)\n\nplt.show()","2a01c3e5":"top_degree_sequence = sorted([(d, n)for n, d in G.degree()], reverse=True)\nbottom_degree_sequence = sorted([(d, n)for n, d in G.degree()], reverse=False)","6e14db58":"pd.DataFrame(top_degree_sequence, columns=['Occurences', 'Terms']).head()","1529db04":"pd.DataFrame(bottom_degree_sequence, columns=['Occurences', 'Terms']).head()","06117ab0":"# Let's list mentioned users\ndf['mentions'] = df['mentions'].apply(lambda row: ast.literal_eval(row))\ndf['mentions'] = df['mentions'].apply(lambda row: [item['screen_name'] for item in row if item != None])\ndf['mentions'].head()","a54f7823":"# Let's get every mention and explode it on rows\ntst = df[['user_screen_name', 'mentions']].explode('mentions')\n\nprint(tst.shape)","8b29a93d":"tst = tst.dropna() # Drops if any NaN row\n\nprint(tst.shape)","8219474e":"# Assign the number 1 for every row -- to count and assign weights\ntst['count'] = 1\n\n# We group every mention, and use the above column to count how many mentions a user\n# has mentioned another\ntst = tst.groupby(by=['user_screen_name', 'mentions'], as_index=False).sum().sort_values(by=['count'], ascending=False)\n\n# Then, we rename this column as weight\ntst.rename(columns={\"count\": \"weight\"}, inplace=True)","30e7a4fc":"# Here, we define a directed Graph with TupleList. Obviously, it expects tuples as input\n# Then, we convert the dataframe with the itertuples method. You may check a bit more on \n    # https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.itertuples.html\n# Besides that, the TupleList method expects the edges and a weight column\n    # which we defined earlier. So, when we define weights=True, it will use\n    # this field\ng = Graph.TupleList(tst.itertuples(index=False), directed=True, weights=True)","3c3bfca0":"out = plot(g, bbox = (600, 600), vertex_size=3, edge_width=1, edge_arrow_size=0.3)\nout.save('nodes_no_communities.png')\nout","37c43da7":"# We'll create an empty list called hubs\nhubs = []\n\n# This returns a list of degrees for each vertex\nin_degrees = g.degree(mode=IN, loops=True)\n\n# For each vertex, it will analyze if it has 8 or more in degrees\n# If so, it will add its name as label, if not, it will add an\n# empty string\nfor index, item in enumerate(in_degrees):\n    if item >= 8:\n        hubs.append(g.vs[index][\"name\"])\n    else:\n        hubs.append('')\n\n# Finally, we set the labels. This is because the plot method\n# Automatically search for a \"label\" attribute to plot\ng.vs[\"label\"] = hubs","6ecb2e81":"# https:\/\/stackoverflow.com\/questions\/9471906\/what-are-the-differences-between-community-detection-algorithms-in-igraph\n# https:\/\/stackoverflow.com\/questions\/37855553\/python-igraph-community-cluster-colors\ni = g.community_infomap(edge_weights=None, vertex_weights=tst['weight'], trials=10)\n\npal = drawing.colors.ClusterColoringPalette(len(i))\ng.vs['color'] = pal.get_many(i.membership)\n\nout_comunnities = plot(g, bbox = (600, 600), vertex_size=5, edge_width=1, edge_arrow_size=0.2)\nout_comunnities.save('nodes_communities.png')\nout_comunnities","d5a864d2":"We may see that there's a bit of misclassifying that would need further investigation. I'll let this one for the next version of this notebook. I hope you like it! Hopefully, this is it! A gentle intro to API calls, NLP and Graph Analysis with NetworkX and igraph!","bf96f5a3":"# 1. First things first\n\nWe're going to use a twitter library called Tweepy to get data from twitter first. Then, we'll process this data and finally generate some graphs analysis based on the words of each tweet.\n\nSo, it's handy to know that Twitter may operate with two main kinds of API: a REST API and a Streaming API. If you're retroactively trying to get data from the past few months, you're going to use the former. But if you think that you might find yourself dealing with Rate Limits (and you know what you're going to search) -- or you just don't care -- you're going for the latter.\n\nThere are several little rules that we have to deal with when calling their API. We could build our own solution from scratch, but, actually, we'll be using a little helper here: **Tweepy**.\n\nReferences: https:\/\/www.kaggle.com\/caractacus\/thematic-text-analysis-using-spacy-networkx","dedce247":"We then plot the graph:","0609ce7b":"All right, it looks like our class and our print statement are functioning correctly! But we missed a lot of things here, like error handling and, more important than anything: rate limiting. We don't want to be blocked by Twitter, right?\n\nSo, let's improve our CustomStreamListener class:","ecc815ad":"Now, we'll see a little bit about graphs. First, we'll see how the texts are usually structured. In other words, we'll see how words relate to each other.","9aa1e5e9":"This is the structure, now...","5c149b1b":" # **2. Graph Analysis on text using NetworkX**","fe898087":"## **1.1. Tweepy**","0e6b75bf":"Here, the purpose is to find communities based on text structure!","b32e6585":"Let's structure our data as dataframe.","3065d46a":"After this, we'll define the subjects we want to filter from the stream, instantiate the streaming_api object and finally start to filter things out. Trying not to have an anxiety attack about the world, I'll stick to pop singers.","de5b372b":"<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/e\/ec\/Happy_smiley_face.png\/600px-Happy_smiley_face.png\" width=300px height=300px\/>","1b8c8cbb":"For the tweets structure this is it, for now. Let's see how users interact between them.","b937ccdb":"Interesting! We can see that there's a lot of small groups in here! So finally, we may plot our graph again, this time identifying communities by color, and also plotting the names for profiles that are central in their communities: here, we're plotting labels for profiles that have 8 or more _in degrees!_","3f995702":"Now, we'll use Spacy over our replaced text.","f9a6cd8c":"Now, we'll want to process the text using a NLP pipeline. We'll tokenize the text and remove stopwords -- words that are too frequent in a language.","0680f9c8":"Here, we'll try to do a comprehensive guide through some topics over getting data from Twitter's API, parsing the texts with some NLP packages and finally trying to do some graph analysis.\n\nSometimes, the approach may seen simplistic, but this is the very basic in order to start to analyze text-based data.\n\nWhat we'll use:\n\n- Tweepy\n- nltk\n- SpaCy\n- Networkx\n- igraph\n- Pandas (of course!)\n- Some other helper libraries\n\nWithout further comments... Let's do it!","362ef54a":"All right! My photo is a happy monkey!\n\n\nNow, we have to work with the Streaming API. The rule here is:\n\n* We have to inherit a class from tweepy.StreamListener\n* Then, we have to override a few methods in order to define how to deal with the data and errors (and RATE LIMITS)\n* Finally, we'll instantiate our streaming API object and...\n* We'll call its filter method","e7332d44":"# Intro","6dc4db73":"### 1.1.1. StreamListener\n\nThere are some peculiarities when we're dealing with tweets, retweets and extended tweets (the ones > 140 characters). So we'll *try* to figure it out.","17d72ea0":"We'll take care of our authentication process. Supposing that you already have generated your credentials\/secrets at https:\/\/developer.twitter.com\/, we're going to set our credentials like this:","545da060":"# 3. Directed Graph Analysis with igraph\n\nNow, we'll consider the mentions between users in order to analyse their relations. Here, we have a directional graph: user A can mention user B, which do not imply that user B has also mentioned user A.\n\nFor this, we'll change our graph library to igraph.","2c0a1207":"# **2. NLP**","9b6a6731":"Well, there's a bit of curious structure there! Let's see the highest and lowest degree nodes!"}}