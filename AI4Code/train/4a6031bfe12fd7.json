{"cell_type":{"4b12fa8a":"code","4b1c9d09":"code","114f9d32":"code","1f7ec091":"code","1e80797b":"code","100a5d41":"code","3e72aac8":"code","6a19bc8d":"code","2da9a394":"code","912fec48":"code","11ab297f":"code","3e83c27a":"code","9aebd19d":"code","604df0ef":"code","a28709ee":"code","271c7ce3":"code","1d0e13c2":"code","72ed1f70":"code","3c92efd9":"code","b4dba00c":"code","ddf30b12":"code","f1761c8d":"code","7e63c0de":"code","f41764b5":"code","310a1255":"code","12e5333f":"code","7c2c1ff0":"markdown","8433d1a8":"markdown","199a3a1b":"markdown","e5090fbe":"markdown","0a6638ea":"markdown","5720417e":"markdown","65b3723e":"markdown","61f74067":"markdown"},"source":{"4b12fa8a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport torch \nimport torch.nn as nn \nimport torch.nn.parallel \nimport torch.optim as optim \nimport torch.utils.data \nfrom torch.autograd import Variable","4b1c9d09":"anime =  pd.read_csv('..\/input\/anime-recommendation-database-2020\/anime.csv')","114f9d32":"anime.head()","1f7ec091":"rating_complete = pd.read_csv('..\/input\/anime-recommendation-database-2020\/rating_complete.csv')","1e80797b":"rating_complete.info()","100a5d41":"rating_complete.head()","3e72aac8":"user_anime = rating_complete.groupby('user_id').size().reset_index()\nuser_anime.columns = ['user_id', 'anime_count']\nuser_anime.head()","6a19bc8d":"filtered_users = user_anime[user_anime['anime_count'] > 1000]\nusers = set(filtered_users['user_id'])\nlen(users)","2da9a394":"query = anime['Type'] == 'TV'\nanime_tv = anime[query]\nanimes = set(anime_tv['MAL_ID'])\nlen(animes)","912fec48":"query = rating_complete['user_id'].isin(users) & rating_complete['anime_id'].isin(animes)\nrating_data = rating_complete[query]\nrating_data.shape","11ab297f":"mask = np.random.rand(rating_data.shape[0]) < 0.8\ndf_train = rating_data[mask]\nprint('train', df_train.shape[0])\ndf_test = rating_data[~mask]\nprint('test', df_test.shape[0])","3e83c27a":"user_rating = rating_data.groupby('user_id').mean()['rating'].reset_index()\nuser_rating.columns = ['user', 'avg_rating']\nuser_rating.head()","9aebd19d":"user_rating['avg_rating'].describe()","604df0ef":"unique_users = {int(x): i for i,x in enumerate(df_train['user_id'].unique())}\nunique_items = {int(x): i for i,x in enumerate(anime_tv['MAL_ID'].unique())}\n\nprint(len(unique_users), len(unique_items))\nnb_users = len(unique_users)\ntrain_set = np.full((len(unique_users), len(unique_items)), -1)\n\nfor user_id, anime_id, rating in df_train.values:\n    if rating >= 8:\n        train_set[unique_users[user_id], unique_items[anime_id]] = 1\n    else:\n        train_set[unique_users[user_id], unique_items[anime_id]] = 0\n    \ntrain_set.shape","a28709ee":"unique_users = {int(x): i for i,x in enumerate(df_test['user_id'].unique())}\nunique_items = {int(x): i for i,x in enumerate(anime_tv['MAL_ID'].unique())}\n\nprint(len(unique_users), len(unique_items))\ntest_set = np.full((len(unique_users), len(unique_items)), -1)\n\nfor user_id, anime_id, rating in df_test.values:\n    if rating >= 7:\n        test_set[unique_users[user_id], unique_items[anime_id]] = 1\n    else:\n        test_set[unique_users[user_id], unique_items[anime_id]] = 0\n    \ntest_set.shape","271c7ce3":"# convert data into Torch tensor\ntrain_set = torch.FloatTensor(train_set)\ntest_set = torch.FloatTensor(test_set)","1d0e13c2":"fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,4))\n\nsns.heatmap(train_set, cmap=\"coolwarm\", cbar_kws={\"ticks\":np.arange(-1,2)}, ax=ax1)\nax1.set_xlabel('anime')\nax1.set_ylabel('user')\nax1.set_title(\"training data set\")\n\nsns.heatmap(test_set, cmap=\"coolwarm\", cbar_kws={\"ticks\":np.arange(-1,2)}, ax=ax2)\nax2.set_xlabel('anime')\nax2.set_ylabel('user')\nax2.set_title(\"test data set\")\nplt.suptitle(\"Rating: -1=no data, 0=low rating, 1=high rating\")","72ed1f70":"class RBM():\n    def __init__(self, num_visible_nodes, num_hidden_nodes):\n        ##initialize all weights \n        ##a tensor with size of num_hidden_nodes, num_visible_nodes in normal dis mean 0 var 1\n        self.W = torch.randn(num_hidden_nodes, num_visible_nodes)\n        self.a = torch.randn(1, num_hidden_nodes)  #bias for hidden nodes - #1st dimension is batch, 2nd is num of hidden nodes\n        self.b = torch.randn(1, num_visible_nodes) #bias for visible nodes\n        \n    #activate the hidden nodes by sampling all hiddens node, given values of visible nodes \n    def sample_hidden_nodes(self, x):\n        #x is values of visible nodes\n        #probablity of hiddens h to be activated, given values of visible  nodes v\n        wx = torch.mm(x, self.W.t())\n        #use sigmoid fuc to activate visible node\n        ## a is bias for hidden nodes\n        activation = wx + self.a.expand_as(wx)\n        ##ith of the vector is the probability of ith hidden nodes to be activated, \n        ##given visible values\n        p_h_given_v =torch.sigmoid(activation)\n        #samples of all hiddens nodes\n        return p_h_given_v, torch.bernoulli(p_h_given_v)\n\n    def sample_visible_nodes(self, y):\n        #y is hidden nodes\n        #probablity of visible h to be activated, given hidden  nodes v\n        wy = torch.mm(y, self.W)\n        #use sigmoid fuc to activate hiddens nodes\n        activation = wy + self.b.expand_as(wy)\n        ##ith of the vector is the probability of ith visible nodes to be activated, \n        ##given hidden values\n        p_v_given_h =torch.sigmoid(activation)\n        #samples of all hiddens nodes\n        return p_v_given_h, torch.bernoulli(p_v_given_h)\n        \n    #visible nodes after kth interation\n    #probablity of hidden nodes after kth iteration\n    def train(self, v0, vk, ph0, phk):\n        # self.W += torch.mm(v0.t(), ph0) - torch.mm(vk.t(), phk)\n        self.W += (torch.mm(v0.t(), ph0) - torch.mm(vk.t(), phk)).t()\n        #add zero to keep b as a tensor of 2 dimension\n        self.b += torch.sum((v0 - vk), 0)\n        self.a += torch.sum((ph0 - phk), 0)\n    \n    # for prediction, input pass hidden nodes and reconstruct back to visible nodes\n    def predict(self, x): # x is visible nodes\n        _, h = self.sample_hidden_nodes(x)\n        _, v = self.sample_visible_nodes(h)\n        return v","3c92efd9":"# define model parameters\nprint(len(train_set[0]))\nnum_visible_nodes = len(train_set[0]) #number of anime\nnum_hidden_nodes = 1000 #number of hidden nodes or num of features\nbatch_size = 2500\n\n# call RBM model\nrbm = RBM(num_visible_nodes, num_hidden_nodes)","b4dba00c":"##train the RBM\nnb_epoch = 10\ntrain_loss_list = []\nfor epoch in range(1, nb_epoch+1):\n    ##loss function\n    train_loss = 0\n    #normalize the loss, define a counter\n    s = 0.\n    #implement a batch learning, \n    for id_user in range(0, nb_users - batch_size, 100):\n        #input batch values\n        vk = train_set[id_user: id_user+batch_size]\n        #target used for loss mesarue: rating \n        v0 = train_set[id_user: id_user+batch_size]\n        ##initilize probablity\n        #pho: given real rating at begining, probablity of hidden nodes\n        ph0, _ = rbm.sample_hidden_nodes(v0)\n        #k step of constrative divergence\n        for k in range(10):\n            _, hk = rbm.sample_hidden_nodes(vk)\n            _, vk = rbm.sample_visible_nodes(hk)\n            #training on rating that do exist, rating as -1\n            vk[v0<0] = v0[v0<0]\n        phk, _ = rbm.sample_hidden_nodes(vk)\n        #update weights and bias\n        rbm.train(v0, vk, ph0, phk)\n        #update train loss\n        train_loss += torch.mean(torch.abs(v0[v0>0]-vk[v0>0]))\n        s += 1\n    print('epoch: '+str(epoch)+' loss: '+str(train_loss\/s))\n    train_loss_list.append ( train_loss )","ddf30b12":"# plot loss\nplt.plot(train_loss_list)\nplt.ylabel(\"loss\")\nplt.xlabel(\"epoch\")\nplt.title(\"training\")","f1761c8d":"##loss function\ntest_loss = 0\n#normalize the loss, define a counter\ns = 0.\n#implement a batch learning, \npredicted_v_input = []\ntest_input = []\nfor id_user in range(0, nb_users):\n    #use input of train set to activate RBM\n    v_input = train_set[id_user: id_user+1]\n    #target used for loss mesarue: rating \n    v_target = test_set[id_user: id_user+1]\n    #use only 1 step to make better prediction, though used 10 steps to train\n    if len(v_target[v_target>=0]):\n        # predict data \n        v_input = rbm.predict(v_input)\n        #update test loss\n        test_loss += torch.mean(torch.abs(v_target[v_target>0]-v_input[v_target>0]))\n        predicted_v_input.append ( v_input.detach().numpy()[0] )\n        test_input.append ( v_target.detach().numpy()[0] )\n        s += 1\npredicted_v_input = np.array(predicted_v_input)\nprint('test loss: ' +str(test_loss\/s))","7e63c0de":"# visualise test input and output rating data\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,4))\nsns.heatmap(test_input, cmap=\"coolwarm\", vmin=-1, cbar_kws={\"ticks\":np.arange(-1,2)}, ax=ax1)\nax1.set_xlabel('movies')\nax1.set_ylabel('users')\nax1.set_title(\"test input\")\nsns.heatmap(predicted_v_input, cmap=\"coolwarm\", vmin=-1,  cbar_kws={\"ticks\":np.arange(-1,2)}, ax=ax2)\nax2.set_xlabel('movies')\nax2.set_ylabel('users')\nax2.set_title(\"test output\")\nplt.suptitle(\"Rating: -1=no data, 0=low rating, 1=high rating\")","f41764b5":"# select tested dataframe comparing predicted and original data\ndf_test = pd.DataFrame({'predict': np.concatenate(predicted_v_input), \n                        'original': np.concatenate(test_input)})\n\n# select only data that original data is available\ndf_select = df_test[df_test[\"original\"] != -1.0]\nprint (\"number of predicted data\", len(df_test))\nprint (\"number of original data\", len(df_select))","310a1255":"from sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\n\ndef plot_confusion_matrix(true_labels, predicted_labels, title):\n    # get_metrics\n    print('Accuracy:', metrics.accuracy_score(true_labels, predicted_labels))\n    print('Precision:', metrics.precision_score(true_labels, predicted_labels, average='weighted'))\n    print('Recall:', metrics.recall_score(true_labels, predicted_labels, average='weighted'))\n    print('F1 Score:', metrics.f1_score(true_labels, predicted_labels,average='weighted'))\n\n    # confusion matrix\n    labels = list(set(true_labels))\n    cm = confusion_matrix(true_labels, predicted_labels, labels=labels)\n    cm_labeled = pd.DataFrame(cm, columns=labels, index=labels)\n    sns.heatmap(cm_labeled, annot=True, cmap='Greens', fmt='g')\n    plt.xlabel(\"predict\")\n    plt.ylabel(\"actual\")\n    plt.title(title)\n    return","12e5333f":"# check accuracy of test data comparing to rating available test data\ntrue_labels = df_select[\"original\"]\npredicted_labels = df_select[\"predict\"]\nplot_confusion_matrix(true_labels, predicted_labels, \"rated test data\")","7c2c1ff0":"# visualize data","8433d1a8":"## The rating by each user","199a3a1b":"# Restricted Boltzmann Machines (RBM)","e5090fbe":"## train datasets","0a6638ea":"# set up parameters and call RBM model","5720417e":"## RBM Model","65b3723e":"## test datasets","61f74067":"# pre-processing"}}