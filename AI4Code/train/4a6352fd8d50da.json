{"cell_type":{"f473bc2f":"code","e2c19268":"code","0a78d6a1":"code","71ec5582":"code","ba051431":"code","3366ca8d":"code","47cfea07":"code","b2590155":"code","d0840600":"code","54182a14":"code","68896d33":"code","df0b7b4e":"code","105ca62d":"code","6d7224f3":"code","6bd09957":"code","56b16cdb":"code","5c67fbc1":"code","a3a8da87":"code","4a746b11":"code","579869de":"code","78e46d34":"code","b2ec3c45":"code","1c17df7c":"code","9e62d2c4":"code","15582c5c":"code","77d6690c":"code","5d16b8ee":"code","c9df71e3":"code","65519807":"code","dfed80b6":"code","c268accc":"code","e3ae6544":"code","9104270d":"code","04ebb09f":"code","552f8a42":"code","b7da0a71":"code","ff7005c3":"code","950a2fcb":"code","9b39f430":"code","6668c2c4":"code","b88b3c8a":"code","0461d2d2":"code","1646e1cf":"code","3a1bc4a6":"code","2647378a":"code","09c0f2ff":"code","a35c4463":"code","1030c02c":"code","ac539a5a":"code","364e30c6":"code","27c82c19":"code","6d5cbb93":"code","09e7f446":"code","86452d31":"code","f84b24f4":"code","a776a95e":"code","d316672f":"code","5bdfb585":"code","7493432e":"code","329502e4":"code","daebc790":"code","f40195d6":"code","8e3b6aa3":"code","3a127036":"code","2750e765":"code","f8797b98":"code","0bd416d5":"code","cf7a139b":"code","c49522cc":"code","729bfa23":"code","bc49010d":"code","231d48ce":"code","0fcb00cb":"code","c73acc6d":"code","633f6775":"code","ff273f15":"code","158748d4":"code","53629a14":"code","4f6a3543":"code","3ab24877":"code","3fff3b8a":"code","564b500a":"code","105aab57":"code","1dea79c7":"code","e6cd0f76":"code","35f832d8":"code","da0b0733":"code","26eee6dd":"code","9830568d":"code","553e3d80":"code","b612b5f2":"code","31d9a00e":"code","48524056":"code","3b6fc188":"code","40f898af":"code","cebb84d1":"code","6e260643":"code","82a9c7fe":"code","c925e651":"code","a7ed3adf":"code","44ba55d9":"code","04aee04e":"code","daca8fc6":"code","e79ad041":"code","14252ee7":"code","975bdf9f":"code","7624601e":"code","16094f65":"code","34e753ec":"code","822e8212":"code","370d0147":"code","6aef8e88":"code","18fd80af":"code","fcb54794":"code","de08e2d2":"code","114c4331":"code","4fce1d4e":"code","359952ba":"code","3a42bc32":"code","e0cd6904":"code","c210cbae":"code","e1851df7":"code","215587eb":"code","6b0948d3":"code","f400b61e":"code","4f642d87":"code","f8dece91":"code","ad31fd42":"code","e228f905":"code","fff3782b":"code","90fb4fa0":"code","25c4b2f6":"code","817c1132":"code","d42af383":"code","440a815f":"code","b02cc99b":"code","7d310339":"code","4f1b8258":"code","249ab19a":"code","c25b6223":"code","42d4a1c6":"code","aed9306e":"code","6430908d":"code","47a41328":"code","0ec53827":"code","167fb266":"code","21a15b5a":"code","208f70eb":"code","8183e4f2":"code","65eefbc2":"code","fd8e2002":"code","ce3682f4":"code","7a1ed27c":"code","a4b360ce":"code","0cfe2751":"code","2ecb5970":"code","52985b9a":"code","fa8dbafc":"code","9c01aa8a":"code","40c04caa":"code","3869cd38":"code","39e686d0":"code","75b114cb":"code","5c0c808d":"code","fd591d99":"code","de25cad4":"code","e2194122":"code","812642ee":"code","e31e145f":"code","22a6e52e":"code","1b3e98f3":"code","64b99dae":"code","752fe9e4":"code","2a915270":"code","45889dfd":"code","3748089a":"code","438e0d77":"code","48bb6be9":"code","cbdf2de8":"code","100b74f2":"code","93dea875":"code","e304e0f2":"code","9661f34b":"code","1bb1d20a":"code","9122ef13":"code","8c521881":"code","c66a0ecc":"code","5e7850fb":"code","99b8a671":"code","b5a7934e":"code","b27a3745":"code","fad27bcd":"code","c9dabd6c":"code","555ed9b4":"code","e64e2343":"code","a907a2d6":"code","8689b877":"code","727f6665":"code","5bea4a26":"code","15cb43b4":"code","cf619d18":"code","072ae0af":"code","e2c189e9":"code","2a2c21b0":"code","a33342ee":"code","e01e8762":"code","516514d4":"code","9053dca5":"code","c81a0f44":"code","08b3df7f":"code","a6a096f6":"code","471a86df":"code","4a9ed2c2":"code","5375d8d9":"code","c05bad58":"code","80359c69":"code","e72175cf":"code","03e9fd81":"code","24916a9f":"code","c6759260":"code","a5168ba1":"code","cd9bd403":"code","79dda632":"code","d61a6c21":"markdown","4b9bfd93":"markdown","bac35a25":"markdown","1869b475":"markdown","0f976398":"markdown","aa2b7689":"markdown","15cefdd5":"markdown","9980a9c6":"markdown","e7e46b4e":"markdown","7c14a800":"markdown","28ed3c41":"markdown","8b8242d1":"markdown","14c1be40":"markdown","45b025d3":"markdown","be978cc1":"markdown","935e8e92":"markdown","d6574726":"markdown","1daf7c87":"markdown","632dcf83":"markdown","27cc204d":"markdown","b02f5f31":"markdown","4c24d96b":"markdown","cc9ce6f3":"markdown","251ed29a":"markdown","9a0ec21f":"markdown","4954c0f3":"markdown","12d809af":"markdown","e43dd2de":"markdown","34ac3078":"markdown","11675ebe":"markdown","ff85bd65":"markdown","d01a04d3":"markdown","2bfb77f0":"markdown","38d024d8":"markdown","8a5c9c7d":"markdown","38d1bf8c":"markdown","642fd8f5":"markdown","51c64e2e":"markdown","126c841f":"markdown","2e015964":"markdown","52c354df":"markdown","c6844feb":"markdown","7fefb1cf":"markdown","95959231":"markdown","95a2fed7":"markdown","42f6d1e4":"markdown","8ffeacfd":"markdown","d5b14f3b":"markdown","2b5a743e":"markdown","c61211c9":"markdown","67a566c5":"markdown","a4d3c83f":"markdown","795b9ba5":"markdown","5e1f8444":"markdown","5f0cbf9e":"markdown","b848d644":"markdown","0662ce60":"markdown","85c95bc0":"markdown","01133a20":"markdown","1c7a48d1":"markdown","cc224e7f":"markdown","ee81b91e":"markdown","e56d247a":"markdown","c039b70a":"markdown","a27f5845":"markdown","2d4f03f9":"markdown","a5f28289":"markdown","1ac87965":"markdown","cecdf90c":"markdown","14aa3732":"markdown","6da445f9":"markdown","edb5ff82":"markdown","fc082e00":"markdown","4dcdfd3d":"markdown","b0616f35":"markdown","f1e1e283":"markdown","6b5436af":"markdown","bcf6dc57":"markdown","6f1602dd":"markdown","1f7afab8":"markdown","3a6dbc17":"markdown","798c3c98":"markdown","e39be950":"markdown","57da71a6":"markdown","c5711016":"markdown","bd306d20":"markdown","8c8508e5":"markdown","075bd8e1":"markdown","13741add":"markdown","9c2e9491":"markdown","52a5913b":"markdown","0597bcfc":"markdown","c94d25f2":"markdown","8b09ab41":"markdown","2c3d4c88":"markdown","1d915492":"markdown","28983ac6":"markdown","575fb41e":"markdown","4fd763b1":"markdown","02002f4c":"markdown","f3ddad4b":"markdown","3533a844":"markdown","7e72dc66":"markdown","9be11fdb":"markdown","0372c850":"markdown","720df5eb":"markdown","42cd07b5":"markdown","fbeec3e4":"markdown","aa409b3c":"markdown","105aa8ed":"markdown","e010e436":"markdown","430ff5ce":"markdown","0064959c":"markdown","5dd4c2f3":"markdown","d17f600c":"markdown","d84b883f":"markdown","daec076e":"markdown"},"source":{"f473bc2f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e2c19268":"#import warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.filterwarnings(\"always\")","0a78d6a1":"#import all required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as sm\n\nfrom sklearn.preprocessing import StandardScaler\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn import metrics\n\nfrom sklearn.metrics import confusion_matrix\n\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import precision_recall_curve","71ec5582":"# to display all the columns \npd.set_option('display.max_columns',None)\nleads=pd.read_csv(\"\/kaggle\/input\/leadscore\/Leads.csv\")\nleads.head()","ba051431":"#check shape\nleads.shape","3366ca8d":"#check types\nleads.info()","47cfea07":"#check all the columns\nleads.columns","b2590155":"#checking Lead Number is unique\nleads[\"Lead Number\"].is_unique","d0840600":"#check Prospect_ID is unique\nleads[\"Prospect ID\"].is_unique","54182a14":"#check for null values\nleads.isnull().sum()","68896d33":"#percentage of null values\nround(100*(leads.isnull().sum()\/len(leads.index)),2)","df0b7b4e":"#checking values for \nleads.Specialization.value_counts()","105ca62d":"#converting 'Select' in np.nan because it is equivalent to null values\nleads['Specialization']=leads['Specialization'].replace({'Select': np.nan})","6d7224f3":"#counting values of this categorical column\nleads['How did you hear about X Education'].value_counts()","6bd09957":"#convert select into np.nan\nleads['How did you hear about X Education']=leads['How did you hear about X Education'].replace({'Select': np.nan})","56b16cdb":"# counting values\nleads['What is your current occupation'].value_counts()","5c67fbc1":"# counting values of this categorical column\nleads['What matters most to you in choosing a course'].value_counts()","a3a8da87":"# counting value of this categorical column\nleads['Lead Profile'].value_counts()","4a746b11":"#convert Lead profile \"select\" field into np.nan\nleads['Lead Profile']=leads['Lead Profile'].replace({'Select': np.nan})","579869de":"#converting \"select\" into np.nan.\nleads['City']=leads['City'].replace({'Select': np.nan})","78e46d34":"# counting values of \"Country\" column\nleads['Country'].value_counts()","b2ec3c45":"# count values of \"Last Notable Activity\" in percentage\nleads['Last Notable Activity'].value_counts(normalize=True)","1c17df7c":"#Finding null percentage including \"select\" fields\nround(100*(leads.isnull().sum()\/len(leads.index)),2)","9e62d2c4":"#Dropping columns\nleads.drop(['Asymmetrique Activity Index','Asymmetrique Profile Index','Asymmetrique Activity Score',\n                  'Asymmetrique Profile Score','Lead Profile','Lead Quality','Tags',\n                  'How did you hear about X Education','Last Notable Activity'],axis=1,inplace=True)\nprint(\"Sucessfully Dropped\")","15582c5c":"#checking the shape again\nleads.shape","77d6690c":"#dropping rows having all null values\nleads=leads.dropna(axis=0,how='all')","5d16b8ee":"#again checking shape\nleads.shape","c9df71e3":"#checking percentage of missing values after dropping many columns\nround(100*(leads.isnull().sum()\/len(leads.index)),2)","65519807":"leads['Do Not Email'].value_counts(normalize=True)","dfed80b6":"leads['Do Not Call'].value_counts(normalize=True)","c268accc":"leads['Search'].value_counts(normalize=True)","e3ae6544":"leads['Magazine'].value_counts(normalize=True)","9104270d":"leads['Newspaper Article'].value_counts(normalize=True)","04ebb09f":"leads['X Education Forums'].value_counts(normalize=True)","552f8a42":"leads['Newspaper'].value_counts(normalize=True)","b7da0a71":"leads['Digital Advertisement'].value_counts(normalize=True)","ff7005c3":"leads['Through Recommendations'].value_counts(normalize=True)","950a2fcb":"leads['Receive More Updates About Our Courses'].value_counts(normalize=True)","9b39f430":"leads['Update me on Supply Chain Content'].value_counts(normalize=True)","6668c2c4":"leads['Get updates on DM Content'].value_counts(normalize=True)","b88b3c8a":"leads['I agree to pay the amount through cheque'].value_counts(normalize=True)","0461d2d2":"leads['What matters most to you in choosing a course'].value_counts(normalize=True)","1646e1cf":"# Dropping all skewed columns\nleads.drop(['Prospect ID','Do Not Email','Do Not Call','Search','Magazine','Newspaper Article','X Education Forums','Newspaper',\n                  'Digital Advertisement','Through Recommendations','Receive More Updates About Our Courses',\n                  'Update me on Supply Chain Content','Get updates on DM Content',\n                  'I agree to pay the amount through cheque','Country','What matters most to you in choosing a course'],axis=1,inplace=True)\nprint(\"successfully dropped\")","3a1bc4a6":"#checking shape\nleads.shape","2647378a":"#checking counts of \"specialization\"\nleads['Specialization'].value_counts(normalize=True)","09c0f2ff":"# assigning \"other\" to all the null values\nleads.loc[pd.isnull(leads['Specialization']),['Specialization']]='Other'","a35c4463":"# checking counts of specialization after adding \"other\"\nleads['Specialization'].value_counts(normalize=True)","1030c02c":"#null count check for \"specialization\"\nleads['Specialization'].isnull().sum()","ac539a5a":"#null percentage of variables\nround(100*(leads.isnull().sum()\/len(leads.index)),2)","364e30c6":"# checking value counts of \"What is your current occupation\"\nleads['What is your current occupation'].value_counts(normalize=True)","27c82c19":"# assigning \"other\" to null values\nleads.loc[pd.isnull(leads['What is your current occupation']),['What is your current occupation']]='Other'","6d5cbb93":"#checking null after imputing\nleads['What is your current occupation'].isnull().sum()","09e7f446":"#checking value counts of \"What is your current occupation\".\nleads['What is your current occupation'].value_counts()","86452d31":"#checking null percentage after imputing\nround(100*(leads.isnull().sum()\/len(leads.index)),2)","f84b24f4":"# checking value counts of \"Lead Source\"\nleads['Lead Source'].value_counts(normalize=True)","a776a95e":"# here we decide to impute the missing values with mode because it is very less in percentage\nleads['Lead Source'].fillna(leads['Lead Source'].mode()[0],inplace=True)","d316672f":"# checking null counts after imputing\nleads['Lead Source'].isnull().sum()","5bdfb585":"#checking null percentge after imputing\nround(100*(leads.isnull().sum()\/len(leads.index)),2)","7493432e":"# Checking  value counts of \"TotalVisits\"\nleads['TotalVisits'].value_counts(normalize=True)","329502e4":"# imputing missing values with mode\nleads['TotalVisits'].fillna(leads['TotalVisits'].mode()[0],inplace=True)","daebc790":"#checking null values after imputing\nleads['TotalVisits'].isnull().sum()","f40195d6":"# null percentage\nround(100*(leads.isnull().sum()\/len(leads.index)),2)","8e3b6aa3":"# checking value counts \"Page Views Per Visit\"\nleads['Page Views Per Visit'].value_counts(normalize=True)","3a127036":"# number of null values\nleads['Page Views Per Visit'].isnull().sum()","2750e765":"# imputing null values with median because we see that it is continous column so it is safe to impute with median\nleads['Page Views Per Visit'].fillna(leads['Page Views Per Visit'].median(),inplace=True)","f8797b98":"# checking null values after imputing\nleads['Page Views Per Visit'].isnull().sum()","0bd416d5":"# checking null percentage\nround(100*(leads.isnull().sum()\/len(leads.index)),2)","cf7a139b":"# checking value counts of \"Last Activity\"\n100*leads['Last Activity'].value_counts(normalize=True)","c49522cc":"# imputing missing values with mode\nleads['Last Activity'].fillna(leads['Last Activity'].mode()[0],inplace=True)","729bfa23":"#checking null values after imputing\nleads['Last Activity'].isnull().sum()","bc49010d":"# checking null percentage\nround(100*(leads.isnull().sum()\/len(leads.index)),2)","231d48ce":"# checking value counts of \"City\".\n100*leads['City'].value_counts(normalize=True)","0fcb00cb":"# we are putting null values as \"Unknown\"\nleads.loc[pd.isnull(leads['City']),['City']]='Unknown'","c73acc6d":"#after imputing null values\n100*leads['City'].value_counts(normalize=True)","633f6775":"#chcking null percentage again\nround(100*(leads.isnull().sum()\/len(leads.index)),2)","ff273f15":"# After data cleaning \nleads.head()","158748d4":"# making pairplots\nsns.pairplot(leads,hue=\"Converted\")\nplt.show","53629a14":"ax=sns.heatmap(leads.corr(),annot=True)\nbottom,top=ax.get_ylim()\nax.set_ylim(bottom+0.5,top-0.5)\nplt.show()","4f6a3543":"# plotting barplot \nsns.barplot(x=\"Last Activity\",y=\"Converted\",data=leads)\nplt.xticks(rotation=90)","3ab24877":"# plotting barplot\nsns.barplot(x=\"A free copy of Mastering The Interview\",y=\"Converted\",data=leads)\nplt.xticks(rotation=90)","3fff3b8a":"# plotting barplot\nsns.barplot(x=\"City\",y=\"Converted\",data=leads)\nplt.xticks(rotation=90)","564b500a":"sns.barplot(x=\"What is your current occupation\",y=\"Converted\",data=leads)\nplt.xticks(rotation=90)","105aab57":"# plotting bar plot\nsns.barplot(x=\"Specialization\",y=\"Converted\",data=leads)\nplt.xticks(rotation=90)","1dea79c7":"sns.barplot(x=\"Lead Source\",y=\"Converted\",data=leads)\nplt.xticks(rotation=90)","e6cd0f76":"# plotting barplot\nsns.barplot(x=\"Lead Origin\",y=\"Converted\",data=leads)\nplt.xticks(rotation=90)","35f832d8":"# Plotting distplot \"Time spent on Website by customer\"\nsns.distplot(leads['Total Time Spent on Website'],kde=False)\nplt.title(\"Time spent on Website by customer\")\nplt.show()","da0b0733":"# plotting distplot \"Page Views Per Visit\"\nsns.distplot(leads['Page Views Per Visit'],kde=False)\nplt.title(\"Page Views Per Visit\")\nplt.xlim(0,20)\nplt.show()","26eee6dd":"# plotting distplot \"TotalVisits\"\nsns.distplot(leads['TotalVisits'],kde=False)\nplt.title(\"TotalVisits\")\nplt.xlim(0,30)\nplt.show()","9830568d":"# PLotting \"boxplot\" for \"TotalVisits\"\nplt.boxplot(leads['TotalVisits'])\nplt.show()","553e3d80":"# handling the outliers , to handle the upper end with 0.99 quantile.\nq1=leads['TotalVisits'].quantile(0.99)\nleads['TotalVisits'][leads['TotalVisits']>=q1] = q1","b612b5f2":"# plotting boxplot for \"Page Views Per Visit\"\nplt.boxplot(leads['Page Views Per Visit'])\nplt.show()","31d9a00e":"q2=leads['Page Views Per Visit'].quantile(0.99)\nleads['Page Views Per Visit'][leads['Page Views Per Visit']>=q2] = q2","48524056":"# plotting boxplot for \"Total Time Spent on Website\"\nplt.boxplot(leads['Total Time Spent on Website'])\nplt.show()","3b6fc188":"# plotting barplot\nsns.barplot(x=\"Lead Origin\",y=\"Page Views Per Visit\",hue=\"Converted\",data=leads)\nplt.xticks(rotation=90)","40f898af":"#plotting barplot\nsns.barplot(x=\"What is your current occupation\",y=\"TotalVisits\",hue=\"Converted\",data=leads)\nplt.xticks(rotation=90)","cebb84d1":"# plotting barpot\nsns.barplot(x=\"What is your current occupation\",y=\"Page Views Per Visit\",hue=\"Converted\",data=leads)\nplt.xticks(rotation=90)","6e260643":"# putting yes=1,No=0\nleads['A free copy of Mastering The Interview']=leads['A free copy of Mastering The Interview'].map({'Yes': 1, \"No\": 0})","82a9c7fe":"# checking head\nleads.head()","c925e651":"# creating dummy variable and concatenating it with out original dataset\ndummy1 = pd.get_dummies(leads[['Lead Origin', 'Lead Source', 'Last Activity', 'Specialization',\n                               'What is your current occupation','City']], drop_first=True)\nleads = pd.concat([leads, dummy1], axis=1)","a7ed3adf":"# checkinge head\nleads.head()","44ba55d9":"#checking shape\nleads.shape","04aee04e":"# Dropping the original ones after creating dummy variables\nleads=leads.drop(['Lead Origin','Lead Source','Last Activity','Specialization','What is your current occupation','City'],1)","daca8fc6":"# chcking shape again\nleads.shape","e79ad041":"# checking head\nleads.head()","14252ee7":"# checking datatypes\nleads.info()","975bdf9f":"# checking statistics\nleads.describe","7624601e":"# splitting data into X and Y.\nX = leads.drop(['Converted','Lead Number'], axis=1)\n\nX.head()","16094f65":"# getting Y\ny = leads['Converted']\n\ny.head()","34e753ec":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","822e8212":"#checking train head\nX_train.head()","370d0147":"# Instantiate scaler and perform scaler on Train data\nscaler = StandardScaler()\nX_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.fit_transform(X_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])\n\nX_train.head()","6aef8e88":"# checking how converted is distributed.\nConverted = (sum(leads['Converted'])\/len(leads['Converted'].index))*100\nConverted","18fd80af":"# checking correlation\nleads.corr()","fcb54794":"# Logistic regression model\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","de08e2d2":"# Instantiate object\nlogreg = LogisticRegression()","114c4331":"rfe = RFE(logreg, 25)             # running RFE with 25 variables as output\nrfe = rfe.fit(X_train, y_train)","4fce1d4e":"# RFE chosen variables\nrfe.support_","359952ba":"# RFE supported and non supported varibles with rankings\nlist(zip(X_train.columns, rfe.support_, rfe.ranking_))","3a42bc32":"# assinging the variables that RFE supports to \"col\"\ncol = X_train.columns[rfe.support_]","e0cd6904":"# variables that RFE doesn't support\nX_train.columns[~rfe.support_]","c210cbae":"# buliding model\nX_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","e1851df7":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","215587eb":"# Reshping \ny_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","6b0948d3":"# making dataframe having Convert and \"Convert_prob\"\ny_train_pred_final = pd.DataFrame({'Convert':y_train.values, 'Convert_Prob':y_train_pred})\ny_train_pred_final['Lead Number'] = y_train.index\ny_train_pred_final.head()","f400b61e":"# going with cutoff 0.5\ny_train_pred_final['predicted'] = y_train_pred_final.Convert_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","4f642d87":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Convert, y_train_pred_final.predicted )\nprint(confusion)","f8dece91":"# Accuracy\nprint(metrics.accuracy_score(y_train_pred_final.Convert, y_train_pred_final.predicted))","ad31fd42":"vif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","e228f905":"# Drop\ncol = col.drop('Lead Source_NC_EDM', 1)\ncol","fff3782b":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","90fb4fa0":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","25c4b2f6":"# reshaping\ny_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","817c1132":"# making dataframe having Convert and \"Convert_prob\"\ny_train_pred_final = pd.DataFrame({'Convert':y_train.values, 'Convert_Prob':y_train_pred})\ny_train_pred_final['Lead Number'] = y_train.index\ny_train_pred_final.head()","d42af383":"# Taking cut off as 0.5\ny_train_pred_final['predicted'] = y_train_pred_final.Convert_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","440a815f":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Convert, y_train_pred_final.predicted )\nprint(confusion)","b02cc99b":"print(metrics.accuracy_score(y_train_pred_final.Convert, y_train_pred_final.predicted))","7d310339":"vif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","4f1b8258":"# dropping variable\ncol = col.drop('Last Activity_Resubscribed to emails', 1)\ncol","249ab19a":"# Building model\nX_train_sm = sm.add_constant(X_train[col])\nlogm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","c25b6223":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]\n\ny_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]\n\ny_train_pred_final = pd.DataFrame({'Convert':y_train.values, 'Convert_Prob':y_train_pred})\ny_train_pred_final['Lead Number'] = y_train.index\ny_train_pred_final.head()\n\n\ny_train_pred_final['predicted'] = y_train_pred_final.Convert_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()\n\n\n# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Convert, y_train_pred_final.predicted )\nprint(confusion)\n# Accuracy\nprint(metrics.accuracy_score(y_train_pred_final.Convert, y_train_pred_final.predicted))","42d4a1c6":"vif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","aed9306e":"col = col.drop('What is your current occupation_Housewife', 1)\ncol","6430908d":"X_train_sm = sm.add_constant(X_train[col])\nlogm5 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm5.fit()\nres.summary()","47a41328":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]\n\ny_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]\n\ny_train_pred_final = pd.DataFrame({'Convert':y_train.values, 'Convert_Prob':y_train_pred})\ny_train_pred_final['Lead Number'] = y_train.index\ny_train_pred_final.head()\n\n\ny_train_pred_final['predicted'] = y_train_pred_final.Convert_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()\n\n\n# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Convert, y_train_pred_final.predicted )\nprint(confusion)\n\nprint(metrics.accuracy_score(y_train_pred_final.Convert, y_train_pred_final.predicted))","0ec53827":"vif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","167fb266":"col = col.drop('Lead Source_Social Media', 1)\ncol","21a15b5a":"X_train_sm = sm.add_constant(X_train[col])\nlogm6 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm6.fit()\nres.summary()","208f70eb":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]\n\ny_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]\n\ny_train_pred_final = pd.DataFrame({'Convert':y_train.values, 'Convert_Prob':y_train_pred})\ny_train_pred_final['Lead Number'] = y_train.index\ny_train_pred_final.head()\n\n\ny_train_pred_final['predicted'] = y_train_pred_final.Convert_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()\n\n\n# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Convert, y_train_pred_final.predicted )\nprint(confusion)\n\nprint(metrics.accuracy_score(y_train_pred_final.Convert, y_train_pred_final.predicted))","8183e4f2":"vif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","65eefbc2":"col = col.drop('Specialization_Rural and Agribusiness', 1)\ncol","fd8e2002":"X_train_sm = sm.add_constant(X_train[col])\nlogm7 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm7.fit()\nres.summary()","ce3682f4":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]\n\ny_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]\n\ny_train_pred_final = pd.DataFrame({'Convert':y_train.values, 'Convert_Prob':y_train_pred})\ny_train_pred_final['Lead Number'] = y_train.index\ny_train_pred_final.head()\n\n\ny_train_pred_final['predicted'] = y_train_pred_final.Convert_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()\n\n\n# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Convert, y_train_pred_final.predicted )\nprint(confusion)\n\nprint(metrics.accuracy_score(y_train_pred_final.Convert, y_train_pred_final.predicted))","7a1ed27c":"vif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","a4b360ce":"col = col.drop('Specialization_Retail Management', 1)\ncol","0cfe2751":"X_train_sm = sm.add_constant(X_train[col])\nlogm8 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm8.fit()\nres.summary()","2ecb5970":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]\n\ny_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]\n\ny_train_pred_final = pd.DataFrame({'Convert':y_train.values, 'Convert_Prob':y_train_pred})\ny_train_pred_final['Lead Number'] = y_train.index\ny_train_pred_final.head()\n\n\ny_train_pred_final['predicted'] = y_train_pred_final.Convert_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()\n\n\n# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Convert, y_train_pred_final.predicted )\nprint(confusion)\n\nprint(metrics.accuracy_score(y_train_pred_final.Convert, y_train_pred_final.predicted))","52985b9a":"vif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","fa8dbafc":"col = col.drop('Lead Source_Facebook', 1)\ncol","9c01aa8a":"X_train_sm = sm.add_constant(X_train[col])\nlogm9 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm9.fit()\nres.summary()","40c04caa":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]\n\ny_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]\n\ny_train_pred_final = pd.DataFrame({'Convert':y_train.values, 'Convert_Prob':y_train_pred})\ny_train_pred_final['Lead Number'] = y_train.index\ny_train_pred_final.head()\n\n\ny_train_pred_final['predicted'] = y_train_pred_final.Convert_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()\n\n\n# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Convert, y_train_pred_final.predicted )\nprint(confusion)\n\nprint(metrics.accuracy_score(y_train_pred_final.Convert, y_train_pred_final.predicted))","3869cd38":"vif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","39e686d0":"col = col.drop('Last Activity_Email Link Clicked', 1)\ncol","75b114cb":"X_train_sm = sm.add_constant(X_train[col])\nlogm10 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm10.fit()\nres.summary()","5c0c808d":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]\n\ny_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]\n\ny_train_pred_final = pd.DataFrame({'Convert':y_train.values, 'Convert_Prob':y_train_pred})\ny_train_pred_final['Lead Number'] = y_train.index\ny_train_pred_final.head()\n\n\ny_train_pred_final['predicted'] = y_train_pred_final.Convert_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()\n\n\n# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Convert, y_train_pred_final.predicted )\nprint(confusion)\n\nprint(metrics.accuracy_score(y_train_pred_final.Convert, y_train_pred_final.predicted))","fd591d99":"vif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","de25cad4":"col = col.drop('Last Activity_Form Submitted on Website', 1)\ncol","e2194122":"X_train_sm = sm.add_constant(X_train[col])\nlogm11 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm11.fit()\nres.summary()","812642ee":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]\n\ny_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]\n\ny_train_pred_final = pd.DataFrame({'Convert':y_train.values, 'Convert_Prob':y_train_pred})\ny_train_pred_final['Lead Number'] = y_train.index\ny_train_pred_final.head()\n\n\ny_train_pred_final['predicted'] = y_train_pred_final.Convert_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()\n\n\n# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Convert, y_train_pred_final.predicted )\nprint(confusion)\n\nprint(metrics.accuracy_score(y_train_pred_final.Convert, y_train_pred_final.predicted))","e31e145f":"vif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","22a6e52e":"col = col.drop('City_Unknown', 1)\ncol","1b3e98f3":"X_train_sm = sm.add_constant(X_train[col])\nlogm12 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm12.fit()\nres.summary()","64b99dae":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]\n\ny_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]\n\ny_train_pred_final = pd.DataFrame({'Convert':y_train.values, 'Convert_Prob':y_train_pred})\ny_train_pred_final['Lead Number'] = y_train.index\ny_train_pred_final.head()\n\n\ny_train_pred_final['predicted'] = y_train_pred_final.Convert_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()\n\n\n# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Convert, y_train_pred_final.predicted )\nprint(confusion)\n\nprint(metrics.accuracy_score(y_train_pred_final.Convert, y_train_pred_final.predicted))","752fe9e4":"vif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","2a915270":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","45889dfd":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","3748089a":"# Let us calculate specificity\nTN \/ float(TN+FP)","438e0d77":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","48bb6be9":"# positive predictive value \nprint (TP \/ float(TP+FP))","cbdf2de8":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","100b74f2":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","93dea875":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Convert, y_train_pred_final.Convert_Prob, drop_intermediate = False )","e304e0f2":"# Drawing Roc curve\ndraw_roc(y_train_pred_final.Convert, y_train_pred_final.Convert_Prob)","9661f34b":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Convert_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","1bb1d20a":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Convert, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","9122ef13":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\n#plt.xlim(0,0.35)\nplt.show()","8c521881":"# getting \"final_predicted\" taking 0.35 cut off.\ny_train_pred_final['final_predicted'] = y_train_pred_final.Convert_Prob.map( lambda x: 1 if x > 0.35 else 0)\n\ny_train_pred_final.head()","c66a0ecc":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.Convert, y_train_pred_final.final_predicted)","5e7850fb":"# confusion matrix for final_predicted.\nconfusion2 = metrics.confusion_matrix(y_train_pred_final.Convert, y_train_pred_final.final_predicted )\nconfusion2","99b8a671":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","b5a7934e":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","b27a3745":"# Let us calculate specificity\nTN \/ float(TN+FP)","fad27bcd":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","c9dabd6c":"# Positive predictive value \nprint (TP \/ float(TP+FP))","555ed9b4":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","e64e2343":"confusion[1,1]\/(confusion[0,1]+confusion[1,1])","a907a2d6":"confusion[1,1]\/(confusion[1,0]+confusion[1,1])","8689b877":"# Precision score of model\nprecision_score(y_train_pred_final.Convert, y_train_pred_final.predicted)","727f6665":"# Recall score of model\nrecall_score(y_train_pred_final.Convert, y_train_pred_final.predicted)","5bea4a26":"y_train_pred_final.Convert, y_train_pred_final.predicted","15cb43b4":"#Ploting the precision, recall and threshold\np, r, thresholds = precision_recall_curve(y_train_pred_final.Convert, y_train_pred_final.Convert_Prob)","cf619d18":"#plotting the curve\nplt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.xlim(0,0.8)\nplt.show()","072ae0af":"#prform scaling on test set\nX_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.transform(X_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])\n","e2c189e9":"X_test = X_test[col]\nX_test.head()","2a2c21b0":"# add constant\nX_test_sm = sm.add_constant(X_test)","a33342ee":"# predict the y values\ny_test_pred = res.predict(X_test_sm)","e01e8762":"# top 10 value of y_pred on test data\ny_test_pred[:10]","516514d4":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)","9053dca5":"# Let's see the head\ny_pred_1.head()","c81a0f44":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","08b3df7f":"# Putting Lead Number to index\ny_test_df['Lead Number'] = y_test_df.index","a6a096f6":"# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","471a86df":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","4a9ed2c2":"y_pred_final.head()","5375d8d9":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Convert_Prob'})","c05bad58":"# Rearranging the columns\ny_pred_final = y_pred_final.reindex(['Lead Number','Convert_Prob','Converted'], axis=1)","80359c69":"# Let's see the head of y_pred_final\ny_pred_final.head()","e72175cf":"# applying cut off\ny_pred_final['final_predicted'] = y_pred_final.Convert_Prob.map(lambda x: 1 if x > 0.35 else 0)","03e9fd81":"# calculating the score \ny_pred_final['Lead Score']=y_pred_final['Convert_Prob']*100\ny_pred_final.head()","24916a9f":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_pred_final.Converted, y_pred_final.final_predicted)","c6759260":"confusion2 = metrics.confusion_matrix(y_pred_final.Converted, y_pred_final.final_predicted )\nconfusion2","a5168ba1":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","cd9bd403":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","79dda632":"# Let us calculate specificity\nTN \/ float(TN+FP)","d61a6c21":"#### Update me on Supply Chain Content","4b9bfd93":"dropping \"What is your current occupation_Housewife'\" on the basis of p values.","bac35a25":"#### Do Not Call","1869b475":"# Train Test Split","0f976398":"#### Outlier Analysis","aa2b7689":"#### 'Receive More Updates About Our Courses","15cefdd5":"As Both are unique we have decided to drop \"Prospect ID\" because of its readability.","9980a9c6":"#### Newspaper Article","e7e46b4e":"Not skewed, Relevant to go ahead and perform the logistic regression.","7c14a800":"#### Nineth Model","28ed3c41":"#### Last Notable Activity","8b8242d1":"#### X Education Forums","14c1be40":"# Making Predictions on Test Set","45b025d3":"#### Heatmap","be978cc1":"#### now we have final model, with all the variables having p values < 0.05 and VIF <5 and accuracy = 82.12%.","935e8e92":"### Multivariate Analysis","d6574726":"### RFE","1daf7c87":"#### What matters most to you in choosing a course","632dcf83":"#### Country","27cc204d":"#### How did you hear about X Education","b02f5f31":"dropping \"Specialization_Rural and Agribusiness\" on the basis of p values.","4c24d96b":"**Suggestions are welcome :)**","cc9ce6f3":"#### Taking optimal cut off as 0.35","251ed29a":"#### VIF","9a0ec21f":"# Finding cutoff optimal point","4954c0f3":"# Model Building","12d809af":"This column is highly skewed.","e43dd2de":"This \"Page Views Per Visit\" also has outliers. So we need to handle them. we can do upper end capping in this column also.","34ac3078":"we have good accuracy of 82.15 % but still we have variables having high p values.","11675ebe":"#### Sixth Model","ff85bd65":"#### Lead Source","d01a04d3":"#### Do Not Email","2bfb77f0":"#### Thank you for reading","38d024d8":"# Understanding and Importing Libraries","8a5c9c7d":"#### Now we need to change the np. nan into \"other\" so as to remove the null values from columns having less percentage of missing values to impute them.","38d1bf8c":"#### Through Recommendations","642fd8f5":"\"Investment and Insurance\",\"Healthcare Management\" are more likely to convert","51c64e2e":"#### Third Model","126c841f":"#### Eleventh Model","2e015964":"#### TotalVisits","52c354df":"#### Search","c6844feb":"here we do not have any outliers in \"Total Time Spent on Website\".","7fefb1cf":"#### VIF","95959231":"\"Thane $ Outskirts\" and \"Other Cities of Maharashtra\" have higher Converted rate.","95a2fed7":"we see that \"TotalVisits\" is in the range 0-20 more likely.","42f6d1e4":"#### Specialization","8ffeacfd":"\"Welingak website\" lead source are more likely to convert","d5b14f3b":"#### Twelveth Model","2b5a743e":"#### Fourth Model","c61211c9":"# Precision and Recall","67a566c5":"#### Page Views Per Visit","a4d3c83f":"#### Outlier Handling","795b9ba5":"### Recall","5e1f8444":"### Precision","5f0cbf9e":"This column is highly skewed.","b848d644":"#### Pairplot","0662ce60":"#### Digital Advertisement","85c95bc0":"#### Lead Profile","01133a20":"#### distplot","1c7a48d1":"dropping \"Specialization_Retail Management\" on the basis of p values.","cc224e7f":"we see that this column is highly skewed","ee81b91e":"An education company named X Education sells online courses to industry professionals. The company markets its courses on several websites and search engines like Google. Once these people land on the website, they might browse the courses or fill up a form for the course or watch some videos. When these people fill up a form providing their email address or phone number, they are classified to be a lead. Moreover, the company also gets leads through past referrals.\nAlthough X Education gets a lot of leads, its lead conversion rate is very poor.\nThe typical lead conversion rate at X education is around 30%.\u00a0\nFor example,\u00a0if, say, they acquire 100 leads in a day, only about 30 of them are converted.\nWe have to build a model wherein we need to assign a lead score to each of the leads such that the customers with higher lead score have a higher conversion chance and the customers with lower lead score have a lower conversion chance.\nIf they successfully identify set of leads with higher conversion chance, the lead conversion rate should go up as the sales team will now be focusing more on communicating with the potential leads rather than making calls to everyone.","e56d247a":"# EDA","c039b70a":"dropping \"Lead Source_Facebook\" on the basis of p values.","a27f5845":"#### Second Model","2d4f03f9":"#### VIF","a5f28289":"we see that \"Time spent on Website by customer\" is more in the range 0-500","1ac87965":"# Data Cleaning","cecdf90c":"#### Get updates on DM Content","14aa3732":"So we are handling the outlier by handling the upper end. whatever the values are above 0.99 are assigned 0.99 values.","6da445f9":"#### What is your current occupation","edb5ff82":"#### What is your current occupation","fc082e00":"#### I agree to pay the amount through cheque","4dcdfd3d":"#### Fifth Model","b0616f35":"#### Boxplot","f1e1e283":"#### VIF","6b5436af":"#### Newspaper","bcf6dc57":"#### VIF","6f1602dd":"\"Working Professional\" and \"Housewife\" have more chance of getting converted.","1f7afab8":"#### What matters most to you in choosing a course","3a6dbc17":"#### City","798c3c98":"We saw that all these categorical columns are highly skewed. We are not going to get any insights from it so we decide to drop them.","e39be950":"#### Magazine","57da71a6":"#### barplot","c5711016":"# Plotting ROC Curve","bd306d20":"Now we have 75 columns finally to move ahead.","8c8508e5":"#### VIF","075bd8e1":"# Conclusion","13741add":"# Data Preparation","9c2e9491":"#### Specialization","52a5913b":"we see that area is 0.89 which is quite good.","0597bcfc":"#### VIF","c94d25f2":"#### First Model","8b09ab41":"We see that when \"Lead Origin\" is Landing Page Submission \"Page views per visit is more.","2c3d4c88":"#### Tenth Model","1d915492":"\"Lead Source_NC_EDM\" having highes p value so decided to drop it.","28983ac6":"#### City","575fb41e":"#### One Hot Encoding","4fd763b1":"**PROBLEM STATEMENT**","02002f4c":"here we do no have any Select field.","f3ddad4b":"This column doesn't have any \"select\" field","3533a844":"Here also we are doing upper end capping.","7e72dc66":"#### Eighth Model","9be11fdb":"This is having 'Select ' field so need to convert into np.nan","0372c850":" when \"Lead Origin\" is \"Lead Add Form\" and \"Quick Add Form\" are more likely to convert","720df5eb":"dropping \"Last Activity_Resubscribed to emails'\" on the basis of p values>0.05","42cd07b5":"Now we can see actual number of null values including \"Select\" field","fbeec3e4":" we do not find very much multicollinearity between the variables. we decide to move with these variables.","aa409b3c":"# Feature Scaling","105aa8ed":"dropping \"Lead Source_Social Media\" on the basis of p values.","e010e436":"#### Seventh Model","430ff5ce":"we see that \"Page Views Per Visit\" is between 0-12 more likely.","0064959c":"Now we have some score variables that we can see from data dictionary , that it filled by sales team, so we have decided to drop\nthem and also the columns having high percentage of missing values that is > 40-45%","5dd4c2f3":"From pair plot we can see that how the variables are related to each other. we can see that \"Page Views Per Visit\" ,\"TotalVisits\" \"Total Time Spent on Website\" has some outliers.","d17f600c":"we see that \"TotalVisits\" has outliers, so we will handle the outliers.","d84b883f":"Converted seems same for both but those who said \"No\" are most converted.","daec076e":"Our Final model having Sensitivity of 80.41%  on Train Set.\nand Accuracy score = 80.75 % on Train set\n\nOur Final model having Sensitivity of 79.90%  on Test Set.\nand Accuracy score = 80.3 % on Test set"}}