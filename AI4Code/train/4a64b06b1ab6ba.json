{"cell_type":{"d4777dcf":"code","43325e7f":"code","23edae51":"code","513cf745":"code","9c732d02":"code","a16de2f2":"code","aa964436":"code","aa3d79e2":"code","8b431cbc":"code","3cf4695d":"code","2edb7972":"code","7b9d65ca":"code","e0c7306d":"code","64ba6599":"code","d93e9976":"code","6ddf379f":"code","f9bb2cd2":"code","79e60567":"code","b2c3f65f":"code","281025d7":"code","b909216f":"code","5367ca53":"code","ec6cf8f1":"code","062302da":"code","762b3b73":"code","a3cf48d1":"code","7c3c9fe9":"code","1ff58fa2":"code","19d6485b":"code","9c92b66b":"code","ae4cd5f7":"code","4f78e680":"code","11672b86":"code","11ce281b":"code","4b84875e":"code","83ad62a2":"code","903713e3":"markdown","4bee1997":"markdown","40202bde":"markdown","d9c82f65":"markdown","2e251317":"markdown","99b1abbf":"markdown","8dabfe33":"markdown","5ee7fb27":"markdown","c24d6bb1":"markdown","15d2207f":"markdown","f39479fb":"markdown","7f02f9ad":"markdown","fb87cb04":"markdown","c7c07e1a":"markdown","13a24f98":"markdown","7abd592e":"markdown","9725fe66":"markdown"},"source":{"d4777dcf":"#checking gpu status\n!nvidia-smi","43325e7f":"import numpy as np\nimport os\nimport scipy\nimport datetime\nimport os\nimport imageio\nimport random as rn\nimport skimage\nfrom skimage import data, color\nfrom skimage import io\nfrom skimage.transform import rescale, resize, downscale_local_mean\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\nfrom tensorflow.python.keras.layers.convolutional import Conv3D, MaxPooling3D,MaxPooling2D,Conv2D\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten,LSTM\nfrom tensorflow.keras import backend as K\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nimport warnings\nwarnings.filterwarnings(\"ignore\")","23edae51":"IMAGE_FRAMES=30\nIMAGE_HEIGHT=120\nIMAGE_WIDTH=120\nIMAGE_CHANNELS=3\nUNIVERSAL_SIZE=(IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_CHANNELS)\nGREY_SCALE_MIN=0\nGREY_SCALE_MAX=255.0\nRANDOM_SEED=30\nBATCH_SIZE=15\nNUM_EPOCS=15","513cf745":"np.random.seed(RANDOM_SEED)\nrn.seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)","9c732d02":"! conda install -y gdown\n#!conda install -y gdown\nimport gdown\nurl = 'https:\/\/drive.google.com\/uc?id=1sxwomT_goGtMC2uF1vON6JHeq0OZuTAo'\n\n\noutput = 'dataset.zip'\n\ngdown.download(url, output, quiet=False)\n!ls\n!unzip dataset.zip -d dataset","a16de2f2":"# Initialization of source directories and data available to us\nsource_dir = 'dataset\/Project_data'\ntrain_doc = np.random.permutation(open(source_dir+'\/train.csv').readlines())\nval_doc = np.random.permutation(open(source_dir+'\/val.csv').readlines())\nbatch_size = BATCH_SIZE # experiment with the batch size","aa964436":"# Building our own image resizer function to use -Reason : Code Reusability\ndef image_resizer (image,size,action):\n    my_image = image\n    if action == 'resize':\n        resized_image = resize(my_image,size)\n    else:\n        resized_image = my_image[0:size[0],0:size[1]]\n    # Convert the image to a 0-255 scale.\n    rescaled_image = GREY_SCALE_MAX * resized_image\n    # Convert to integer data type pixels.\n    final_image = rescaled_image.astype(np.uint8)\n    return final_image","aa3d79e2":"size = (IMAGE_HEIGHT,IMAGE_WIDTH)\n# Taking 1st sample image of size 120*160*3 -Here we will crop the image size to 120*120\nsample_image_path = source_dir + '\/train' + '\/' + 'WIN_20180925_17_08_43_Pro_Left_Swipe_new'\nsample_image = os.listdir(sample_image_path)[0]\nsample = sample_image_path + '\/' + sample_image\nprint (\"This is our 1st Sample Image->\",sample)\nsample = imageio.imread(sample)\nresized_image=image_resizer(sample,size,'crop')\nprint (\"+++++++++++++++++++++++++++++++++++++++\")\nfor value in range(3):\n     plt.imshow(sample[:, : , value])\n     plt.show()\nprint( \"new preprocessed image with crop and rescale: \", resized_image)   \n# Taking 2nd Sample Image of Size 360 * 360 *3 -Here we will resize the image size to 120*120\nsample_image_path = source_dir + '\/train' + '\/' + 'WIN_20180907_16_43_39_Pro_Thumbs Up_new'\nsample_image = os.listdir(sample_image_path)[0]\nsample = sample_image_path + '\/' + sample_image\nsample = imageio.imread(sample)\nresized_image=image_resizer(sample,size,'resize')\n\nfor value in range(3):\n     plt.imshow(sample[:, : , value])\n     plt.show()\nprint( \"new preprocessed image with resize and rescale: \", resized_image) ","8b431cbc":"def generator(source_path, folder_list, batch_size):\n    print( 'Source path = ', source_path, '; batch size =', batch_size)\n    img_idx = [i for i in range(0, IMAGE_FRAMES)] #create a list of image numbers you want to use for a particular video\n    while True:\n        t = np.random.permutation(folder_list)\n        num_batches = int(len(t)\/batch_size) # calculate the number of batches\n        for batch in range(num_batches): # we iterate over the number of batches\n            batch_data = np.zeros((batch_size,IMAGE_FRAMES,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_CHANNELS)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n            for folder in range(batch_size): # iterate over the batch_size\n                imgs = os.listdir(source_path+'\/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n                for idx,item in enumerate(img_idx): #  Iterate iver the frames\/images of a folder to read them in\n                    # imread(source_path+'\/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'\/'+imgs[item]).astype(np.float32)\n                    image = imageio.imread(source_path+'\/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'\/'+imgs[item]).astype(np.float32)\n\n                    #crop the images and resize them. Note that the images are of 2 different shape \n                    #and the conv3D will throw error if the inputs in a batch have different shapes\n                    universal_size = UNIVERSAL_SIZE\n                    if image.shape[0] == 360:\n                      # in case of 360 X 360 , resize the image to 120X120\n                      resized_image = image_resizer(image,universal_size,'resize')\n                    else:\n                      # in case of 120 X 160 , crop the image to 120X120\n                      resized_image = image_resizer(image,universal_size,'crop')\n\n                    batch_data[folder,idx,:,:,0] = resized_image[:, : , 0]\/GREY_SCALE_MAX #normalise and feed in the image\n                    batch_data[folder,idx,:,:,1] = resized_image[:, : , 1]\/GREY_SCALE_MAX #normalise and feed in the image\n                    batch_data[folder,idx,:,:,2] = resized_image[:, : , 2]\/GREY_SCALE_MAX #normalise and feed in the image\n                    \n                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n\n        \n        # write the code for the remaining data points which are left after full batches\n        remaining_dp = len(t) - (num_batches * batch_size)\n        if remaining_dp > 0:\n            batch_data = np.zeros((remaining_dp,IMAGE_FRAMES,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_CHANNELS))\n            batch_labels = np.zeros((remaining_dp,5))\n            for folder in range(remaining_dp):\n                imgs = os.listdir(source_path+\"\/\"+t[num_batches*batch_size + folder].split(';')[0])\n                for idx,item in enumerate(img_idx):\n                    image = imageio.imread(source_path+\"\/\"+t[num_batches*batch_size + folder].split(';')[0]+'\/'+imgs[item]).astype(np.float32)\n                    if image.shape[0] == 360:\n                      # in case of 360 X 360 , resize the image to 120X120\n                      resized_image = image_resizer(image,universal_size,'resize')\n                    else:\n                      # in case of 120 X 160 , crop the image to 120X120\n                      resized_image = image_resizer(image,universal_size,'crop')\n                    \n                    batch_data[folder,idx,:,:,0] = resized_image[:, : , 0]\/GREY_SCALE_MAX\n                    batch_data[folder,idx,:,:,1] = resized_image[:, : , 1]\/GREY_SCALE_MAX\n                    batch_data[folder,idx,:,:,2] = resized_image[:, : , 2]\/GREY_SCALE_MAX\n\n                batch_labels[folder, int(t[folder + num_batches*batch_size].strip().split(';')[2])] = 1\n            yield batch_data, batch_labels","3cf4695d":"curr_dt_time = datetime.datetime.now()\ntrain_path = source_dir + '\/train'\nval_path = source_dir + '\/val'\nnum_train_sequences = len(train_doc)\nprint('# training sequences =', num_train_sequences)\nnum_val_sequences = len(val_doc)\nprint('# validation sequences =', num_val_sequences)\nprint ('# epochs =', NUM_EPOCS)","2edb7972":"t = np.random.permutation(val_doc)\nobj = os.listdir(val_path+'\/'+ t[1 + (1*1)].split(';')[0])\nprint(obj)\nprint(train_path)\nprint(val_path)\ntrain_generator = generator(train_path, train_doc, batch_size)\nval_generator = generator(val_path, val_doc, batch_size)","7b9d65ca":"if (num_train_sequences%batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences\/batch_size)\nelse:\n    steps_per_epoch = (num_train_sequences\/\/batch_size) + 1\nif (num_val_sequences%batch_size) == 0:\n    validation_steps = int(num_val_sequences\/batch_size)\nelse:\n    validation_steps = (num_val_sequences\/\/batch_size) + 1\nprint(\"++++++++++++++++++++++++++++++++++++\")\nprint ('Steps\/EPOCH -> %d' % steps_per_epoch)\nprint ('validation Steps -> %d' % validation_steps)\nprint(\"++++++++++++++++++++++++++++++++++++\")","e0c7306d":"model = Sequential()\nmodel.add(Conv3D(16, kernel_size = (3,3,3), activation='relu', input_shape=(IMAGE_FRAMES,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_CHANNELS)))\nmodel.add(Conv3D(16, (3,3,3), activation='relu'))\n\nmodel.add(MaxPooling3D(pool_size=(2, 2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv3D(32, (3,3,3), activation='relu'))\nmodel.add(Conv3D(32, (3,3,3), activation='relu'))\n\nmodel.add(MaxPooling3D(pool_size=(2, 2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.25))\n\n# softmax layer\nmodel.add(Dense(5, activation='softmax'))\n\n# model summary\nmodel.summary()","64ba6599":"optimiser =Adam()\nmodel.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model.summary())","d93e9976":"# Generic Function to train the model - we will use it from 2nd model onwards\ndef model_builder():\n\n    #++++++++++++++++++++++ enable to execute with physical Nvdia GPU + Win10 spec++++\n    #os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n    #os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\" \n    # Spec for execution\n    #- enable to work with physical gpu -Nvdia Quadro - M 2000 4GB \n    # - with Cuda 10.0.1, \n    #- CudNN 7.x.x + RTX device drivers\n    #++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n    model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '\/'\n    #check if the model directory exits\n    if not os.path.exists(model_name):\n        os.mkdir(model_name)\n    \n    filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n    LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)\n    callbacks_list = [checkpoint, LR]\n\n    model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=NUM_EPOCS, verbose=1, \n                    callbacks=callbacks_list, validation_data=val_generator, \n                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)","6ddf379f":"#start training the model\nmodel_builder()","f9bb2cd2":"model = Sequential()\n\nmodel.add(Conv3D(\n    8, (3,3,3), activation='relu', input_shape=(IMAGE_FRAMES,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_CHANNELS),\n    padding='same'\n))\nmodel.add(MaxPooling3D(pool_size=(2, 2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv3D(16, (3,3,3), activation='relu', padding='same'))\nmodel.add(MaxPooling3D(pool_size=(2, 2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv3D(32, (3,3,3), activation='relu', padding='same'))\nmodel.add(MaxPooling3D(pool_size=(2, 2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv3D(64, (3,3,3), activation='relu', padding='same'))\nmodel.add(MaxPooling3D(pool_size=(2, 2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.25))\n\n# softmax layer\nmodel.add(Dense(5, activation='softmax'))\n\n# model summary\noptimiser = Adam()\nmodel.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model.summary())","79e60567":"#start training the model\nmodel_builder()","b2c3f65f":"model = Sequential()\nmodel.add(Conv3D( 8, (3,3,3), activation='relu', input_shape=(IMAGE_FRAMES,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_CHANNELS),\npadding='same'\n))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling3D(pool_size=(2, 2, 2)))\n\nmodel.add(Conv3D(32, (2, 2, 2), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling3D(pool_size=(2, 2, 2)))\n\nmodel.add(Conv3D(64, (2, 2, 2), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling3D(pool_size=(2, 2, 2)))\n\nmodel.add(Conv3D(128, (2, 2, 2), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling3D(pool_size=(2, 2, 2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(256,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(128,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(5,activation='softmax'))\n\n# model summary\noptimiser = Adam()\nmodel.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model.summary())","281025d7":"#start training the model\nmodel_builder()","b909216f":"model = Sequential()\nmodel.add(Conv3D( 8, (3,3,3), activation='relu', input_shape=(IMAGE_FRAMES,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_CHANNELS),\npadding='same'\n ))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling3D(pool_size=(2, 2, 2)))\n\nmodel.add(Conv3D(16, (3,3,3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling3D(pool_size=(2, 2, 2)))\n\nmodel.add(Conv3D(32, (3,3,3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling3D(pool_size=(2, 2, 2)))\n\n\nmodel.add(Flatten())\nmodel.add(Dense(256,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Dense(5,activation='softmax'))\n\n# model summary\noptimiser = Adam()\nmodel.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model.summary())","5367ca53":"#start training the model\nmodel_builder()","ec6cf8f1":"model = Sequential()\nmodel.add(Conv3D(16, (3,3,3), padding='same',\n         input_shape= (IMAGE_FRAMES,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_CHANNELS)))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv3D(16, (3,3,3), padding='same',\n         input_shape=(30,120,120,3)))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(MaxPooling3D(pool_size=(2, 2, 2)))\n\nmodel.add(Conv3D(32, (3,3,3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv3D(32, (3,3,3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(MaxPooling3D(pool_size=(2, 2, 2)))\n\nmodel.add(Conv3D(64, (3,3,3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv3D(64, (3,3,3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(MaxPooling3D(pool_size=(2, 2, 2)))\n\nmodel.add(Conv3D(128, (3,3,3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv3D(128, (3,3,3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(MaxPooling3D(pool_size=(2, 2, 2)))\n\n\nmodel.add(Flatten())\nmodel.add(Dense(64,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(64,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Dense(5,activation='softmax'))\n\n# model summary\noptimiser = Adam()\nmodel.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model.summary())","062302da":"#start training the model\nmodel_builder()","762b3b73":"model = Sequential()\n\nmodel.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),\n                                  input_shape=(IMAGE_FRAMES,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_CHANNELS)))\n\nmodel.add(TimeDistributed(BatchNormalization()))\nmodel.add(TimeDistributed(MaxPooling2D((2, 2))))\n\nmodel.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\nmodel.add(TimeDistributed(BatchNormalization()))\nmodel.add(TimeDistributed(MaxPooling2D((2, 2))))\n\nmodel.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\nmodel.add(TimeDistributed(BatchNormalization()))\nmodel.add(TimeDistributed(MaxPooling2D((2, 2))))\n\nmodel.add(TimeDistributed(Conv2D(128, (3,3) , padding='same', activation='relu')))\nmodel.add(TimeDistributed(BatchNormalization()))\nmodel.add(TimeDistributed(MaxPooling2D((2, 2))))\n\nmodel.add(TimeDistributed(Conv2D(256, (3, 3) , padding='same', activation='relu')))\nmodel.add(TimeDistributed(BatchNormalization()))\nmodel.add(TimeDistributed(MaxPooling2D((2, 2))))\n\nmodel.add(TimeDistributed(Flatten()))\n\n\nmodel.add(LSTM(64))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dropout(0.25))\n                  \nmodel.add(Dense(5,activation='softmax'))\n\n# model summary\noptimiser = Adam()\nmodel.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model.summary())","a3cf48d1":"#start training the model\nmodel_builder()","7c3c9fe9":"model = Sequential()\n\nmodel.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),\n                          input_shape=(IMAGE_FRAMES,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_CHANNELS)))\nmodel.add(TimeDistributed(BatchNormalization()))\nmodel.add(TimeDistributed(MaxPooling2D((2, 2))))\n\nmodel.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\nmodel.add(TimeDistributed(BatchNormalization()))\nmodel.add(TimeDistributed(MaxPooling2D((2, 2))))\n\nmodel.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\nmodel.add(TimeDistributed(BatchNormalization()))\nmodel.add(TimeDistributed(MaxPooling2D((2, 2))))\n\nmodel.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\nmodel.add(TimeDistributed(BatchNormalization()))\nmodel.add(TimeDistributed(MaxPooling2D((2, 2))))\n\n\nmodel.add(TimeDistributed(Flatten()))\n\n\nmodel.add(GRU(64))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(5,activation='softmax'))\n\n# model summary\noptimiser = Adam()\nmodel.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model.summary())","1ff58fa2":"#start training the model\nmodel_builder()","19d6485b":"batch_size = 25","9c92b66b":"if (num_train_sequences%batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences\/batch_size)\nelse:\n    steps_per_epoch = (num_train_sequences\/\/batch_size) + 1\nif (num_val_sequences%batch_size) == 0:\n    validation_steps = int(num_val_sequences\/batch_size)\nelse:\n    validation_steps = (num_val_sequences\/\/batch_size) + 1\nprint(\"++++++++++++++++++++++++++++++++++++\")\nprint ('Steps\/EPOCH -> %d' % steps_per_epoch)\nprint ('validation Steps -> %d' % validation_steps)\nprint(\"++++++++++++++++++++++++++++++++++++\")","ae4cd5f7":"model = Sequential()\nmodel.add(Conv3D( 8, (3,3,3), activation='relu', input_shape=(IMAGE_FRAMES,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_CHANNELS),\npadding='same'\n))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling3D(pool_size=(2, 2, 2)))\n\nmodel.add(Conv3D(32, (2, 2, 2), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling3D(pool_size=(2, 2, 2)))\n\nmodel.add(Conv3D(64, (2, 2, 2), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling3D(pool_size=(2, 2, 2)))\n\nmodel.add(Conv3D(128, (2, 2, 2), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling3D(pool_size=(2, 2, 2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(256,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(128,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(5,activation='softmax'))\n\n# model summary\noptimiser = Adam()\nmodel.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model.summary())","4f78e680":"#start training the model\nmodel_builder()","11672b86":"batch_size=40","11ce281b":"if (num_train_sequences%batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences\/batch_size)\nelse:\n    steps_per_epoch = (num_train_sequences\/\/batch_size) + 1\nif (num_val_sequences%batch_size) == 0:\n    validation_steps = int(num_val_sequences\/batch_size)\nelse:\n    validation_steps = (num_val_sequences\/\/batch_size) + 1\nprint(\"++++++++++++++++++++++++++++++++++++\")\nprint ('Steps\/EPOCH -> %d' % steps_per_epoch)\nprint ('validation Steps -> %d' % validation_steps)\nprint(\"++++++++++++++++++++++++++++++++++++\")","4b84875e":"model = Sequential()\nmodel.add(Conv3D( 8, (3,3,3), activation='relu', input_shape=(IMAGE_FRAMES,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_CHANNELS),\npadding='same'\n ))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling3D(pool_size=(2, 2, 2)))\n\nmodel.add(Conv3D(16, (3,3,3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling3D(pool_size=(2, 2, 2)))\n\nmodel.add(Conv3D(32, (3,3,3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling3D(pool_size=(2, 2, 2)))\n\n\nmodel.add(Flatten())\nmodel.add(Dense(256,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Dense(5,activation='softmax'))\n\n# model summary\noptimiser = Adam()\nmodel.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model.summary())","83ad62a2":"#start training the model\nmodel_builder()","903713e3":"\n### Problem Statement:\nEach gesture corresponds to a specific command:\n1.\tThumbs up:  Increase the volume\n2.\tThumbs down: Decrease the volume\n3.\tLeft swipe: 'Jump' backwards 10 seconds\n4.\tRight swipe: 'Jump' forward 10 seconds  \n5.\tStop: Pause the movie\n\n\n### Data:\nData consists of sample video recorded as images with 5 different gestures. \n1.\tTraining \n2.\tValidation\nEach training and validation set consists of several sub folders. Each subfolder consists of frames of one video (30 frames for a gesture). Also, we have been provided with two csv files which consist of the video metadata (i.e. path of the video and its label).\nAll images\/frames of a video are of same dimension, there are two variants of the video image dimensions: 360x360 and 120x160 as we can see from our data exploration.\n\n### Solution Approach\nTo build model which can detect gesture in the video, we have followed following steps:\n1.\tData Understanding\n2.\tPreprocessing\/Cleaning\n3.\tModel Architectures\n4.\tModel performance comparison\n5.\tFinal model selection\n\n1.\tData Understanding:  \nWe have input data given in zip format (Project_data). Data consists of Training and validation sets. Each set consists of several sub folders, each sub folder having 30 frames.\nInput video frame dimensions: \n360x360x3\n120x160x3\n\n2.\tPreprocessing: \nFollowing two steps are performed in preprocessing:\n1.\tResize\/Crop: In the preprocessing step we have converted two different size of frames into one dimension of size 120x120 using two techniques \u2013 resize and crop.\n2.\tNormalization: Image data is normalized by dividing pixel values by 255.\nAlong with the data normalization and resizing, we need to implement generator function.  which will feed data to the model for each EPOC. We need to implement generator function which can feed equal size batches of sequences and also take care of remaining sequences. \n\n3.\tModel Architecture: \nThere are two architectures which can be used to build model for gesture recognition.\n\n1.\t3D Convolution Network or Conv3D: 3D convolutions are a natural extension to the 2D convolutions you are already familiar with. Just like in 2D conv, you move the filter in two directions (x and y), in 3D conv, you move the filter in three directions (x, y and z). In this case, the input to a 3D conv is a video (which is a sequence of 30 RGB images). If we assume that the shape of each image is 120x120x3, for example, the video becomes a 4-D tensor of shape 120x120x3x30 which can be written as (120x120x30)x3 where 3 is the number of channels. Hence, deriving the analogy from 2-D convolutions where a 2-D kernel\/filter (a square filter) is represented as (fxf)xc where f is filter size and c is the number of channels, a 3-D kernel\/filter (a 'cubic' filter) is represented as (fxfxf)xc (here c = 3 since the input images have three channels). This cubic filter will now '3D-convolve' on each of the three channels of the (120x120x30) tensor.\n\n2.\tCNN + RNN: This is the standard architecture for processing videos. In this architecture, video frames are passed through a CNN layer which extracts features from the images and then these feature vectors are fed to a RNN network to simulate sequence behavior of the video. Output of RNN is regular SoftMax function\/\na.\tWe can use transfer learning in 2D CNN layer instead of training own network. \nb.\tLSTM or GRU can be used in RNN\n\n","4bee1997":"## Model 2 -  model with three hidden layers and 1 dense layer for same batch size 15","40202bde":"## Model 5: conv3D with 8 Hidden Layers","d9c82f65":"## Model 4: Conv3D with 2 Hidden Layers,Dense Neuron = 256","2e251317":"We set the random seed so that the results don't vary drastically.","99b1abbf":" ## Model 9 - Conv3D with 2 hidden Layers  and Dense Layer with 256 Neurons and Batch Size 40","8dabfe33":"## Model 6: LSTM with 64 neurons ","5ee7fb27":"## Model 1 -  model with drop out and Dense layer","c24d6bb1":"# Croping and Resizing ","15d2207f":"# Spliting Training and Validation data ","f39479fb":"# Analysis of models\n\n<style type=\"text\/css\">\n\ttable.tableizer-table {\n\t\tfont-size: 12px;\n\t\tborder: 1px solid #CCC; \n\t\tfont-family: Arial, Helvetica, sans-serif;\n\t} \n\t.tableizer-table td tr {\n\t\tpadding: 4px;\n\t\tmargin: 3px;\n\t\tborder: 5px solid #CCC;\n\t}\n\t.tableizer-table th {\n\t\tbackground-color: #104E8B; \n\t\tcolor: #000;\n\t\tfont-weight: bold;\n\t}\n<\/style>\n<table class=\"tableizer-table\" border=1>\n<thead><tr class=\"tableizer-firstrow\"><th>Experiment Number<\/th><th>Model Architecture<\/th><th>Result <\/th><th>Decision + Explanation<\/th><\/tr><\/thead><tbody>\n <tr><td>1<\/td><td>\"Input image = 120 x 120<\/td><td>Training accuracy: 77%<\/td><td>Model is not able to learn.<BR> Needs to change architecture.<BR><\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Conv3D (16 -> 32)<\/td><td>Validation accuracy: 28%<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>hidden layers = 2<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Dense Neuron = 64<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Dropout = 0.25<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Batch Size =15<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Optimizer: Adam<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>2<\/td><td>Input image = 120 x 120<\/td><td>Training accuracy: 77%<\/td><td>Adding hidden layer and dense neuron<BR> seemed to have descent impact on accuracy and loss also started reducing in each epoch.<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Conv3D (16 ->32 -> 64)<\/td><td>Validation accuracy: 45%<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>hidden layers = 3<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Dense Neuron = 256<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Drop out = 0.25<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Batch Size =15<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>padding = same<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Optimizer: Adam <\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>3<\/td><td>Input image = 120 x 120<\/td><td>Training accuracy: 74%<\/td><td> Got good result here <BR> improve on accuracy bit more.<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Conv3D (32-> 64 ->128)<\/td><td>Validation accuracy: 46% <\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>hidden layers = 3<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Dense Neuron = 128<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Drop out = 0.25<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>padding = same<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Batch Size =15<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Batch Normalization<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Optimizer: Adam <\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>4<\/td><td>Input image = 120 x 120<\/td><td>Training accuracy: 99%<\/td><td>The model looks overfitting. Let's try other combinations <\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Conv3D (16 ->32)<\/td><td>Validation accuracy: 52%<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>hidden layers = 2<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Dense Neuron = 256<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>padding = same<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Optimizer: Adam<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Batch Size =15<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>5<\/td><td>  Input image = 120 x 120<\/td><td>  Training accuracy: 95%<\/td><td>Overfitting reduced but validation accuracy is not as expected<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Conv3D (16 ->16->32->32>64->64->128->128)<\/td><td>Validation accuracy: 62%<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Hidden layers = 8<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Dense Neuron = 64+64 = 128<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Padding = same<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Augmentation<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Optimizer = Adam<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Batch Size =15<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>6<\/td><td>Input image = 120 x 120<\/td><td>Training Accuracy :50%<\/td><td>Did not get better result here, hence would  try out GRU as well<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Conv2D(16->32->64->128->256)+LSTM<\/td><td>Validation Accuracy :48%<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Hidden layers = 5<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Dense Neuron = 64<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Optimizer = Adam<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Batch Size =15<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>7<\/td><td>Input image = 120 x 120<\/td><td>Training Accuracy: 68%<\/td><td>Model with GRU failed to produce expected validation accuracy<BR> compared to previous models<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Conv2D(16->32->64->128)+GRU<\/td><td>Validation Accuracy: 57%<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Hidden Layers = 4<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Dense Neuron = 64<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Optimizer = Adam<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Batch Size =15<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>8<\/td><td>Input image = 120 x 120<\/td><td>Training Accuracy :75%<\/td><td>Model failed to produce expected validation accuracy<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Conv3D (32-> 64 ->128)<\/td><td>Validation Accuracy:28%<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>hidden layers = 3<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Dense Neuron = 256,128<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Drop out = 0.25<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>padding = same<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Batch Size =25<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Batch Normalization<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Optimizer: Adam<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>9<\/td><td>Input image = 120 x 120<\/td><td>Training Accuracy: 50%<\/td><td>This model looks like overfitted. We will stop here.<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Conv3D (16 ->32)<\/td><td>Validation Accuracy: 20%<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>hidden layers = 2<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Dense Neuron = 256<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Drop out = 0.25<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Batch Size = 40<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>padding = same<\/td><td>&nbsp;<\/td><td>&nbsp;<\/td><\/tr>\n <tr><td>&nbsp;<\/td><td>Optimizer: Adam<\/td><td>&nbsp;<\/td><td><\/td><\/tr>\n<\/tbody><\/table>","7f02f9ad":"<h1>Gesture Recognition Case Study<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Problem-Statement\" data-toc-modified-id=\"Problem-Statement-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Problem Statement<\/a><\/span><\/li><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Data<\/a><\/span><\/li><li><span><a href=\"#Gesture-Recognition\" data-toc-modified-id=\"Gesture-Recognition-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Gesture Recognition<\/a><\/span><\/li><li><span><a href=\"#Initialisation-of-Input-Variable\" data-toc-modified-id=\"Initialisation-of-Input-Variable-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Initialisation of Input Variable<\/a><\/span><\/li><li><span><a href=\"#Croping-and-Resizing\" data-toc-modified-id=\"Croping-and-Resizing-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Croping and Resizing<\/a><\/span><\/li><li><span><a href=\"#Generator\" data-toc-modified-id=\"Generator-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Generator<\/a><\/span><\/li><li><span><a href=\"#Spliting-Training-and-Validation-data\" data-toc-modified-id=\"Spliting-Training-and-Validation-data-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>Spliting Training and Validation data<\/a><\/span><\/li><li><span><a href=\"#Model-Creation\" data-toc-modified-id=\"Model-Creation-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;<\/span>Model Creation<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Model-1----model-with-drop-out-and-Dense-layer\" data-toc-modified-id=\"Model-1----model-with-drop-out-and-Dense-layer-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;<\/span>Model 1 -  model with drop out and Dense layer<\/a><\/span><\/li><li><span><a href=\"#Model-2----model-with-three-hidden-layers-and-1-dense-layer-for-same-batch-size-15\" data-toc-modified-id=\"Model-2----model-with-three-hidden-layers-and-1-dense-layer-for-same-batch-size-15-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;<\/span>Model 2 -  model with three hidden layers and 1 dense layer for same batch size 15<\/a><\/span><\/li><li><span><a href=\"#Model-3----model-with-4-hidden-layers-and-1-dense-layer-with-batch-normalization\" data-toc-modified-id=\"Model-3----model-with-4-hidden-layers-and-1-dense-layer-with-batch-normalization-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;<\/span>Model 3 -  model with 4 hidden layers and 1 dense layer with batch normalization<\/a><\/span><\/li><li><span><a href=\"#Model-4:-Conv3D-with-2-Hidden-Layers,Dense-Neuron-=-256\" data-toc-modified-id=\"Model-4:-Conv3D-with-2-Hidden-Layers,Dense-Neuron-=-256-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;<\/span>Model 4: Conv3D with 2 Hidden Layers,Dense Neuron = 256<\/a><\/span><\/li><li><span><a href=\"#Model-5:-conv3D-with-8-Hidden-Layers\" data-toc-modified-id=\"Model-5:-conv3D-with-8-Hidden-Layers-8.5\"><span class=\"toc-item-num\">8.5&nbsp;&nbsp;<\/span>Model 5: conv3D with 8 Hidden Layers<\/a><\/span><\/li><li><span><a href=\"#Model-6:-LSTM-with-64-neurons\" data-toc-modified-id=\"Model-6:-LSTM-with-64-neurons-8.6\"><span class=\"toc-item-num\">8.6&nbsp;&nbsp;<\/span>Model 6: LSTM with 64 neurons<\/a><\/span><\/li><li><span><a href=\"#Model-7:-GRU-Model-with-64-neurons\" data-toc-modified-id=\"Model-7:-GRU-Model-with-64-neurons-8.7\"><span class=\"toc-item-num\">8.7&nbsp;&nbsp;<\/span>Model 7: GRU Model with 64 neurons<\/a><\/span><\/li><li><span><a href=\"#Model-8---Conv3D-with-3-Hidden-Layers--and-2-Dense-Layer-with-256-and-128-Neurons-each-and-Batch-Size-25\" data-toc-modified-id=\"Model-8---Conv3D-with-3-Hidden-Layers--and-2-Dense-Layer-with-256-and-128-Neurons-each-and-Batch-Size-25-8.8\"><span class=\"toc-item-num\">8.8&nbsp;&nbsp;<\/span>Model 8 - Conv3D with 3 Hidden Layers  and 2 Dense Layer with 256 and 128 Neurons each and Batch Size 25<\/a><\/span><\/li><li><span><a href=\"#Model-9---Conv3D-with-2-hidden-Layers--and-Dense-Layer-with-256-Neurons-and-Batch-Size-40\" data-toc-modified-id=\"Model-9---Conv3D-with-2-hidden-Layers--and-Dense-Layer-with-256-Neurons-and-Batch-Size-40-8.9\"><span class=\"toc-item-num\">8.9&nbsp;&nbsp;<\/span>Model 9 - Conv3D with 2 hidden Layers  and Dense Layer with 256 Neurons and Batch Size 40<\/a><\/span><\/li><\/ul><\/li><\/ul><\/div>","fb87cb04":"# Final Model Selection:\nModel 5 is our best model. It has provided an accuracy of 62% on the validation set consistently over multiple epochs. Also, it has not overfit the training data (unlike the previous models) with a training accuracy between 95%.\nAlternatively, model 4 also has a validation accuracy of 52% and training accuracy stayed at 99%.which is highly overfitting the training data","c7c07e1a":"## Model 7: GRU Model with 64 neurons","13a24f98":"## Model 3 -  model with 4 hidden layers and 1 dense layer with batch normalization","7abd592e":" ## Model 8 - Conv3D with 3 Hidden Layers  and 2 Dense Layer with 256 and 128 Neurons each and Batch Size 25","9725fe66":"#  Initialisation of Input Variable"}}