{"cell_type":{"8a871ab4":"code","a607bbe7":"code","1e95f996":"code","036ac758":"code","1fd5efe6":"code","f70df89f":"code","032ef75c":"code","db568237":"code","05a1aabc":"code","0b155db5":"code","ec8e759f":"code","76145ffa":"code","3ba143ea":"code","b238fd00":"code","0365c28a":"code","4c111c99":"code","16df7b8c":"code","a26e2bf6":"code","997d1cf9":"code","aaccfb69":"code","67d2e985":"code","58c009f2":"code","64389bd5":"markdown","631d8ef4":"markdown","09a8e986":"markdown","8346073d":"markdown","019acc21":"markdown","4d1db140":"markdown","02ab1899":"markdown","984e8d8d":"markdown","c8e39e2f":"markdown","a4c9812e":"markdown","d3d8a326":"markdown","a9943cb6":"markdown","9b2fc7c7":"markdown","c25ac3b6":"markdown","ff4fa7ff":"markdown","63c62e1b":"markdown","98036cc9":"markdown","6d83ce07":"markdown","94528001":"markdown","9551cdaa":"markdown","dc94ab27":"markdown","c8991487":"markdown","eb622e7a":"markdown","5d2ef74d":"markdown","46230f4e":"markdown","d42c0e89":"markdown","89419a45":"markdown","f32d8c86":"markdown","0dcdc49c":"markdown","163dd10b":"markdown","be0ea98c":"markdown","98874c78":"markdown","6302cb56":"markdown","468f2cb9":"markdown","e3fd9ead":"markdown","d54c5f62":"markdown","c09b1e45":"markdown","aacc1b14":"markdown","b611eee5":"markdown","f1cfed0f":"markdown"},"source":{"8a871ab4":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport string\nimport re\nfrom nltk.tokenize import word_tokenize\nimport json\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a607bbe7":"#Utility function to handle the two classes' probability outputs of ELMo's model. \ndef return_low(x):\n    high = x[\"1\"] - 1\n    if(x[\"0\"] > x[\"1\"]): return (1-x[\"0\"])\n    else : return x[\"1\"]","1e95f996":"#Loading predictions from ELMo and BERT models.\n#Loading training and test datasets.\nimport pandas as pd\n\ntest = pd.read_csv(\"..\/input\/predictions-datasets\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/predictions-datasets\/train.csv\")","036ac758":"def clean_tweets(tweet):\n    \"\"\"Removes links and non-ASCII characters\"\"\"\n    \n    tweet = ''.join([x for x in tweet if x in string.printable])\n    \n    # Removing URLs\n    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n    \n    return tweet\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef remove_punctuations(text):\n    punctuations = '@#!?+&*[]-%.:\/();$=><|{}^' + \"'`\"\n    \n    for p in punctuations:\n        text = text.replace(p, f' {p} ')\n\n    text = text.replace('...', ' ... ')\n    \n    if '...' not in text:\n        text = text.replace('..', ' ... ')\n    \n    return text\n\ndef convert_abbrev(word):\n    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word\n\ndef convert_abbrev_in_text(text):\n    tokens = word_tokenize(text)\n    tokens = [convert_abbrev(word) for word in tokens]\n    text = ' '.join(tokens)\n    return text","1fd5efe6":"ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\ntrain.at[train['id'].isin(ids_with_target_error),'target'] = 0\ntrain[train['id'].isin(ids_with_target_error)]\n\ntrain = train.drop(train[train[\"text\"].duplicated()].index)\n\nwith open('..\/input\/abbreviation\/abbreviation.json') as json_file:\n    abbreviations = json.load(json_file)\n\ntrain[\"text\"] = train[\"text\"].apply(lambda x: clean_tweets(x))\ntest[\"text\"] = test[\"text\"].apply(lambda x: clean_tweets(x))\n\ntrain[\"text\"] = train[\"text\"].apply(lambda x: remove_emoji(x))\ntest[\"text\"] = test[\"text\"].apply(lambda x: remove_emoji(x))\n\ntrain[\"text\"] = train[\"text\"].apply(lambda x: remove_punctuations(x))\ntest[\"text\"] = test[\"text\"].apply(lambda x: remove_punctuations(x))\n\ntrain[\"text\"] = train[\"text\"].apply(lambda x: convert_abbrev_in_text(x))\ntest[\"text\"] = test[\"text\"].apply(lambda x: convert_abbrev_in_text(x))\n","f70df89f":"ELMo_full_proba = pd.read_csv(\"..\/input\/predictions-datasets\/ELMo_full_proba.csv\")[[\"0\", \"1\"]]\nelmo_full = ELMo_full_proba.apply(return_low, axis = 1)\nelmo_full_around = elmo_full.apply(lambda x: np.int(np.around(x)))\n\nbert_full = pd.read_csv(\"..\/input\/predictions-datasets\/newTrainPredict.csv\")[\"0\"]\nbert_full_around = bert_full.apply(lambda x: np.around(x))\n\nbert_test = pd.read_csv(\"..\/input\/predictions-datasets\/newTestPredict.csv\")[\"0\"]\nbert_test_around = bert_test.apply(lambda x: np.around(x))","032ef75c":"Predict = bert_full_around\nPredict.index = train.index\nbert_evaluationOnes = (Predict.loc[Predict == 1] == train.loc[Predict == 1][\"target\"])\nbert_evaluationZeros = (Predict.loc[Predict == 0] == train.loc[Predict == 0][\"target\"])\n\ntot_false = (bert_evaluationOnes[bert_evaluationOnes == False]).shape[0] + (bert_evaluationZeros[bert_evaluationZeros == False]).shape[0]\nbert_acc = 1-tot_false\/train.index.shape[0]\nprint(\"accuracy is \", bert_acc)\n\nFP_bert = train.loc[bert_evaluationOnes[bert_evaluationOnes == False].index]\nFN_bert = train.loc[bert_evaluationZeros[bert_evaluationZeros == False].index]\nprint(np.shape(FP_bert), np.shape(FN_bert))","db568237":"Predict = elmo_full_around\nPredict.index = train.index\nelmo_evaluationOnes = (Predict.loc[Predict == 1] == train.loc[Predict == 1][\"target\"])\nelmo_evaluationZeros = (Predict.loc[Predict == 0] == train.loc[Predict == 0][\"target\"])\n\ntot_false = (elmo_evaluationOnes[elmo_evaluationOnes == False]).shape[0] + (elmo_evaluationZeros[elmo_evaluationZeros == False]).shape[0]\nelmo_acc = 1-tot_false\/train.index.shape[0]\nprint(\"accuracy is \", elmo_acc)\n\nFP_elmo = train.loc[elmo_evaluationOnes[elmo_evaluationOnes == False].index]\nFN_elmo = train.loc[elmo_evaluationZeros[elmo_evaluationZeros == False].index]\nprint(np.shape(FP_elmo), np.shape(FN_elmo))","05a1aabc":"combined_pd = pd.DataFrame({'bert': bert_full, 'elmo':elmo_full})\ncombined_pd = combined_pd.apply(lambda x: np.average(x), axis = 1)\ncombined_around_pd = combined_pd.apply(lambda x: np.int(np.around(x)))\n\nPredict = combined_around_pd\nPredict.index = train.index\ncombined_evaluationOnes = (Predict.loc[Predict == 1] == train.loc[Predict == 1][\"target\"])\ncombined_evaluationZeros = (Predict.loc[Predict == 0] == train.loc[Predict == 0][\"target\"])\n\ntot_false = (combined_evaluationOnes[combined_evaluationOnes == False]).shape[0] + (combined_evaluationZeros[combined_evaluationZeros == False]).shape[0]\n\nFP_comb = train.loc[combined_evaluationOnes[combined_evaluationOnes == False].index]\nFN_comb = train.loc[combined_evaluationZeros[combined_evaluationZeros == False].index]\n\ncomb_acc = 1-tot_false\/train.index.shape[0]\nprint(\"accuracy is \", comb_acc)\nprint(np.shape(FP_comb), np.shape(FN_comb))\n","0b155db5":"fig = plt.figure(figsize = (30,10))\n\nax = fig.add_subplot(131)\nax.axis([0, 10, 0, 18])\nplt.xticks(size = 20)\nplt.yticks(size = 15)\nplt.title('FP_elmo')\nFP_elmo[\"keyword\"].value_counts().head(10).plot.bar(ax = ax)\n\nax = fig.add_subplot(132)\nax.axis([0, 10, 0, 18])\nplt.xticks(size = 20)\nplt.yticks(size = 15)\nplt.title('FP_bert')\nFP_bert[\"keyword\"].value_counts().head(10).plot.bar(ax = ax)\n\nax = fig.add_subplot(133)\nax.axis([0, 10, 0, 18])\nplt.xticks(size = 20)\nplt.yticks(size = 15)\nplt.title('FP_comb')\nFP_comb[\"keyword\"].value_counts().head(10).plot.bar(ax = ax)","ec8e759f":"#Don't hesitate to test different keywords such as [detonate, pandemonium, tsunami, ...]\nkeyword = \"windstorm\"\n\nfor idx in FP_bert.index:  \n    if(FP_bert[\"keyword\"][idx] == keyword):\n        print(FP_bert[\"id\"][idx], FP_bert[\"keyword\"][idx], \" : \")\n        print(FP_bert[\"text\"][idx])\n        print('--'*20)","76145ffa":"train[train[\"keyword\"] == \"windstorm\"][\"target\"].value_counts()","3ba143ea":"#Don't hesitate to test different keywords such as [eyewitnessed, destroyed, demolition, ...]\nkeyword = \"trapped\"\n\nfor idx in FP_elmo.index:  \n    if(FP_elmo[\"keyword\"][idx] == keyword):\n        print(FP_elmo[\"id\"][idx], FP_elmo[\"keyword\"][idx], \" : \")\n        print(FP_elmo[\"text\"][idx])\n        print('--'*20)\n        \nprint(train[train[\"keyword\"] == keyword][\"target\"].value_counts())","b238fd00":"keyword = \"windstorm\"\n\nfor idx in FP_elmo.index:  \n    if(FP_elmo[\"keyword\"][idx] == keyword):\n        print(FP_elmo[\"id\"][idx], FP_elmo[\"keyword\"][idx], \" : \")\n        print(FP_elmo[\"text\"][idx])\n        print('--'*20)","0365c28a":"keyword = \"windstorm\"\n\nfor idx in FP_comb.index:  \n    if(FP_comb[\"keyword\"][idx] == keyword):\n        print(FP_comb[\"id\"][idx], FP_comb[\"keyword\"][idx], \" : \")\n        print(FP_comb[\"text\"][idx])\n        print('--'*20)","4c111c99":"fig = plt.figure(figsize = (30,10))\n\nax = fig.add_subplot(131)\nax.axis([0, 10, 0, 12])\nplt.title('FN_elmo')\nplt.xticks(size = 20)\nplt.yticks(size = 15)\nFN_elmo[\"keyword\"].value_counts().head(10).plot.bar(ax = ax)\n\nax = fig.add_subplot(132)\nax.axis([0, 10, 0, 12])\nplt.xticks(size = 20)\nplt.yticks(size = 15)\nplt.title('FN_bert')\nFN_bert[\"keyword\"].value_counts().head(10).plot.bar(ax = ax)\n\nax = fig.add_subplot(133)\nax.axis([0, 10, 0, 12])\nplt.xticks(size = 20)\nplt.yticks(size = 15)\nplt.title('FN_comb')\nFN_comb[\"keyword\"].value_counts().head(10).plot.bar(ax = ax)","16df7b8c":"def confidenceCalc(x):\n    if x<0.5 : \n        return (0.5-x)*2\n    else : \n        return (x-0.5)*2","a26e2bf6":"Predict = elmo_full_around\nPredict.index = train.index\nelmo_full.index = train.index\nTrueValues = elmo_full[train[\"target\"] == Predict]\nFalseValues = elmo_full[train[\"target\"] != Predict]\nmeanTrueConf = TrueValues.apply(confidenceCalc).mean()\nmeanFalseConf = FalseValues.apply(confidenceCalc).mean()\n\nprint(\"ELMo's accuracy :\", elmo_acc)\nprint(\"True confidence :\", meanTrueConf, \"False confidence :\", meanFalseConf)\nprint(\"Value diff :\", meanTrueConf - meanFalseConf)","997d1cf9":"Predict = bert_full_around\nPredict.index = train.index\nbert_full.index = train.index\nTrueValues = bert_full[train[\"target\"] == Predict]\nFalseValues = bert_full[train[\"target\"] != Predict]\nmeanTrueConf = TrueValues.apply(confidenceCalc).mean()\nmeanFalseConf = FalseValues.apply(confidenceCalc).mean()\n\nprint(\"BERT's accuracy :\", bert_acc)\nprint(\"True confidence :\", meanTrueConf, \"False confidence :\", meanFalseConf)\nprint(\"Value diff :\", meanTrueConf - meanFalseConf)","aaccfb69":"Predict = combined_around_pd\nPredict.index = train.index\ncombined_pd.index = train.index\nTrueValues = combined_pd[train[\"target\"] == Predict]\nFalseValues = combined_pd[train[\"target\"] != Predict]\nmeanTrueConf = TrueValues.apply(confidenceCalc).mean()\nmeanFalseConf = FalseValues.apply(confidenceCalc).mean()\n\nprint(\"combined's accuracy :\", comb_acc)\nprint(\"True confidence :\", meanTrueConf, \"False confidence :\", meanFalseConf)\nprint(\"Value diff :\", meanTrueConf - meanFalseConf)","67d2e985":"submission_combined = pd.read_csv(\"..\/input\/submission\/submission.csv\")","58c009f2":"submission_combined.to_csv('\/kaggle\/working\/submission.csv', index=False)","64389bd5":"# Error analysis between ELMo and BERT prediction","631d8ef4":"```python\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=10,\n    batch_size=16\n)\n```","09a8e986":"```python\ndef bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n\ndef build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    \n    if Dropout_num == 0:\n        # Without Dropout\n        out = Dense(1, activation='sigmoid')(clf_output)\n    else:\n        # With Dropout(Dropout_num), Dropout_num > 0\n        x = Dropout(Dropout_num)(clf_output)\n        out = Dense(1, activation='sigmoid')(x)\n\n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model\n```","8346073d":"### BERT","019acc21":"## Result Analysis","4d1db140":"We now can show the 10 most mistakenly labeled **keywords** by both model and their averaged prediction. ","02ab1899":"We have an interesting effect here !\nAlthough the True confidence of the combined prediction isn't averaged between both model's True Confidence, there is a large drop of the False confidence.\nThis \"drop\" is simply a consequence of the avering of both predictions. If for an example, ELMo predicted a `0.05` (confidence of **90**%) and BERT predicted a `0.80` (confidence of **60**%), the average will be `0.425` (confidence **15**%). If the boundary is set at `0.5`, than the final predicted class will be 0 (because **0.425 < 0.5**) but the confidence will be much lower than any of the other two models. \n\nThat means a handling of errors that is in a much better direction in the combined probabilities model, at the cost of loosing a bit of confidence in the correctly labeled predictions.\n\nIn the design of programs to analyse tweets, the handling of errors is an important specificity that can have a lot of influence on the machine learning model. Having a model to have a very low percentage of False Negative might be a requirement when dealing with strong consequences. ","984e8d8d":"## Imports","c8e39e2f":"The same effect is found with the False Negatives groups where the combined predictions the model to get less *stuck* on some keywords.","a4c9812e":"```python\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nembed = hub.Module('https:\/\/tfhub.dev\/google\/elmo\/3', trainable=True)\n\nembeddings = embed(\n    np.array(train[\"text\"]),\n    signature=\"default\",\n    as_dict=True)[\"default\"]\n```","d3d8a326":"let's do the same with ELMo's model and the keyword \"**trapped**\". ","a9943cb6":"```python\nmodel = build_model(bert_layer, max_len=160)\nmodel.summary()\n```","9b2fc7c7":"## Acknowledgments\n\nThis kernel uses the following kernels, their great explanations and inspirations:\n\n- [NLP EDA bag-of-words tfidf GloVe BERT (vbmokin)](https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert)\n- [Basic EDA Cleaning and GloVe (shahules)](https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove)\n- [Disaster NLP Keras BERT using TfHub (xhulu)](https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub)","c25ac3b6":"Here we handle errors found in the dataset and the cleaning of the sentences stopwords and unwanted characters.","ff4fa7ff":"```python\ntrain_input = bert_encode(train.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(test.text.values, tokenizer, max_len=160)\ntrain_labels = train.target.values\n```","63c62e1b":"```python\n!pip uninstall tensorflow\n!pip install tensorflow==1.15\n```","98036cc9":"One trick analysed here is to average the predictions' probabilities of both models. This allows for the most \"**confident**\" model to take over. But why would that be something we want as the BERT model being more performant, would be a intrinsically \"confident\". While that is right for **True** values, it is not exactly the same for **False** values. \n\n\"*Confidence*\" is simply \"How close to a 100% is the model when predicting a value\". A model that has a precision of 98%, but an average confidence of 70% gives a different impression than a confidence of 90%.\n\nThe more a model is confident when it is right, usually the better. Inversly, the less confident the model usually is in its mistakes, the better. It shows an \"hesitation\" and the possibility of a doubt. ","6d83ce07":"Calculation of the average confidence in the predictions of each model. \nFor rightfully labeled inputs, we want to see the confidence as high as possible (*True confidence*). \n> This would show that, when it's right, it is confident it is right.\n\nFor wrongfully labeled inputs, we want to see the confidence as low as possible (*False confidence*).\n> This would show that, when it's wrong, it was closer to doubt.\n\n","94528001":"We rapidly see here that BERT gets stuck on the formulation of \"**Windstorm Insurer**\" and labels those examples as actual threats. It probably learns to see an association with threats and cataclysm from other tweets as there are 16 positively labeled examples with the keyword \"**windstorm**\".","9551cdaa":"First remark is that the two models have very distinct errors. Although BERT has better accuracy, it seems to get more *stuck* on certain formulations. Let's look at the keyword \"**windstorm**\" and see why it could label it wrong. ","dc94ab27":"## Models\n\n","c8991487":"## Submissions","eb622e7a":"```python\nimport tensorflow_hub as hub\n\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)\n```","5d2ef74d":"A first submission from the BERT model was made with a test accuracy of **0.82822** which is not that suprising being really close to the scores from other BERT models in some notebooks on Kaggle and from the notebook this model was extracted from. ","46230f4e":"```python\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n```","d42c0e89":"Let's compare ELMo's and the combined's predictions on the classification of tweets with the keyword \"**windstorm**\"","89419a45":"Note that the Model section won't be able to work here as ELMo only works tf version 1.15 and the BERT model is designed with tf version 2.1.\nYou can jump directly to the \"Result Analysis\" section after reading the Models section.","f32d8c86":"## Ending note","0dcdc49c":"This notebooks aims to give a look out error analysis from two famous NLP machine learning models : ELMo and BERT. \n\nA combinaison of their predictions is also looked out and then submitted for the competition.","163dd10b":"Thank you for reading ! \nHope this little analysis might inspire some of you in improving your models or your comprehension ! Good luck with the competition !! ","be0ea98c":"```python\nfrom sklearn.model_selection import train_test_split\n\n#Need to clean \"train[label]\" for actual labels\nxtrain, xvalid, ytrain, yvalid = train_test_split(emb_train, \n                                                  train[\"target\"],  \n                                                  random_state=42, \n                                                  test_size=0.15)\n```","98874c78":"## Cleaning","6302cb56":"The combined prediction model performs an accuracy on the test set of **0.82719** which is really close to BERT's model performance. This is pretty much in line with the accuracy difference on the validation dataset used in training between both models (although the score is a bit higher than expected). ","468f2cb9":"A first cleaning of ELMo's prediction is needed because the model has a MLP layer on top for predictions which outputs through [proba_predict()](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier.predict_proba) (which outputs a probability of the sample for each class of the model). ","e3fd9ead":"### ELMo\n\nFirst model is a prediction from ELMo's [module]('https:\/\/tfhub.dev\/google\/elmo\/3') (non fine-tuned).","d54c5f62":"## Confidence","c09b1e45":"ELMo only has only 6 predictions in the False Positive group for the keyword \"**windstorm**\". Although the misconception of \"windstorm insurer\" is still present, it is way less frequent. This has a lot of chance to involve the architecture of ELMo that is not built on transformers but on LSTM and that is specialized on single sentence comprehension.\nWe see that it has more confidence in certain predictions than BERT allowing the combined prediction to reduce the number of mislabelled \"**windstorm**\" to 13. ","aacc1b14":"Analysis of ELMo's and BERT's predictions to divide values into confusion matrix's subsets (*True Positive, False Positives, False Negatives, True Negatives*)\n> [Confusion Matrix Wiki](https:\/\/en.wikipedia.org\/wiki\/Confusion_matrix)\n\nHere the focus is kept on **False** values to focus on errors made from the models.\n\n","b611eee5":"```python\nfrom sklearn.neural_network import MLPClassifier\nfrom itertools import product\n\n\nmlp_clf = MLPClassifier(50, learning_rate = \"constant\", max_iter=100, random_state = 42)\nmlp_clf.fit(xtrain, ytrain)\n\nelmo_result_train = mlp_clf.predict(ytrain)\nelmo_f1_sco_train = f1_score(elmo_result_train, train[\"target\"].values, average='weighted')\n\nfull_predict_mlp = mlp_clf.predict_proba(emb_train)\n```","f1cfed0f":"Although the idea of a cataclysm is present inside this repeating set of tweets, ELMo fails to grab the context of the Hollywood movie, that is clearly stated, and that doesn't represent a treat. "}}