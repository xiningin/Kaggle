{"cell_type":{"3e859a0c":"code","3306f379":"code","67849c4a":"code","caafcba3":"code","5f878fbd":"code","91108fe6":"code","82487418":"code","648f0d27":"code","2e16738d":"code","975837ee":"code","a27a6703":"code","68058c96":"code","8d005045":"code","d386ca6e":"code","867912aa":"code","b95b38e4":"code","138c5db5":"code","7766de5a":"code","8c0f0edd":"markdown","aa0bab5e":"markdown","4fb2723a":"markdown","9ec688da":"markdown","5688854e":"markdown","a0671e99":"markdown","411f89bd":"markdown","6681c533":"markdown","9d3b401b":"markdown"},"source":{"3e859a0c":"!cp -r ..\/input\/vittutorialillustrations\/* .\/ \n\n!pip install nb_black\n%load_ext nb_black","3306f379":"!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version 1.7\n!pip install timm","67849c4a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nplt.style.use(\"ggplot\")\n\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\n\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.distributed.parallel_loader as pl\n\nimport timm\n\nimport gc\nimport os\nimport time\nimport random\nfrom datetime import datetime\n\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom sklearn import model_selection, metrics","caafcba3":"# For parallelization in TPUs\nos.environ[\"XLA_USE_BF16\"] = \"1\"\nos.environ[\"XLA_TENSOR_ALLOCATOR_MAXSIZE\"] = \"100000000\"","5f878fbd":"def seed_everything(seed):\n    \"\"\"\n    Seeds basic parameters for reproductibility of results\n    \n    Arguments:\n        seed {int} -- Number of the seed\n    \"\"\"\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\nseed_everything(1001)","91108fe6":"# general global variables\nDATA_PATH = \"..\/input\/cassava-leaf-disease-classification\"\nTRAIN_PATH = \"..\/input\/cassava-leaf-disease-classification\/train_images\/\"\nTEST_PATH = \"..\/input\/cassava-leaf-disease-classification\/test_images\/\"\nMODEL_PATH = (\n    \"..\/input\/vit-base-models-pretrained-pytorch\/jx_vit_base_p16_224-80ecf9dd.pth\"\n)\n\n# model specific global variables\nIMG_SIZE = 224\nBATCH_SIZE = 16\nLR = 2e-05\nGAMMA = 0.7\nN_EPOCHS = 10","82487418":"df = pd.read_csv(os.path.join(DATA_PATH, \"train.csv\"))\ndf.head()","648f0d27":"df.info()","2e16738d":"df.label.value_counts().plot(kind=\"bar\")","975837ee":"train_df, valid_df = model_selection.train_test_split(\n    df, test_size=0.1, random_state=42, stratify=df.label.values\n)","a27a6703":"class CassavaDataset(torch.utils.data.Dataset):\n    \"\"\"\n    Helper Class to create the pytorch dataset\n    \"\"\"\n\n    def __init__(self, df, data_path=DATA_PATH, mode=\"train\", transforms=None):\n        super().__init__()\n        self.df_data = df.values\n        self.data_path = data_path\n        self.transforms = transforms\n        self.mode = mode\n        self.data_dir = \"train_images\" if mode == \"train\" else \"test_images\"\n\n    def __len__(self):\n        return len(self.df_data)\n\n    def __getitem__(self, index):\n        img_name, label = self.df_data[index]\n        img_path = os.path.join(self.data_path, self.data_dir, img_name)\n        img = Image.open(img_path).convert(\"RGB\")\n\n        if self.transforms is not None:\n            image = self.transforms(img)\n\n        return image, label","68058c96":"# create image augmentations\ntransforms_train = transforms.Compose(\n    [\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n        transforms.RandomHorizontalFlip(p=0.3),\n        transforms.RandomVerticalFlip(p=0.3),\n        transforms.RandomResizedCrop(IMG_SIZE),\n        transforms.ToTensor(),\n        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n    ]\n)\n\ntransforms_valid = transforms.Compose(\n    [\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n    ]\n)","8d005045":"print(\"Available Vision Transformer Models: \")\ntimm.list_models(\"vit*\")","d386ca6e":"class ViTBase16(nn.Module):\n    def __init__(self, n_classes, pretrained=False):\n\n        super(ViTBase16, self).__init__()\n\n        self.model = timm.create_model(\"vit_base_patch16_224\", pretrained=False)\n        if pretrained:\n            self.model.load_state_dict(torch.load(MODEL_PATH))\n\n        self.model.head = nn.Linear(self.model.head.in_features, n_classes)\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\n    def train_one_epoch(self, train_loader, criterion, optimizer, device):\n        # keep track of training loss\n        epoch_loss = 0.0\n        epoch_accuracy = 0.0\n\n        ###################\n        # train the model #\n        ###################\n        self.model.train()\n        for i, (data, target) in enumerate(train_loader):\n            # move tensors to GPU if CUDA is available\n            if device.type == \"cuda\":\n                data, target = data.cuda(), target.cuda()\n            elif device.type == \"xla\":\n                data = data.to(device, dtype=torch.float32)\n                target = target.to(device, dtype=torch.int64)\n\n            # clear the gradients of all optimized variables\n            optimizer.zero_grad()\n            # forward pass: compute predicted outputs by passing inputs to the model\n            output = self.forward(data)\n            # calculate the batch loss\n            loss = criterion(output, target)\n            # backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n            # Calculate Accuracy\n            accuracy = (output.argmax(dim=1) == target).float().mean()\n            # update training loss and accuracy\n            epoch_loss += loss\n            epoch_accuracy += accuracy\n\n            # perform a single optimization step (parameter update)\n            if device.type == \"xla\":\n                xm.optimizer_step(optimizer)\n\n                if i % 20 == 0:\n                    xm.master_print(f\"\\tBATCH {i+1}\/{len(train_loader)} - LOSS: {loss}\")\n\n            else:\n                optimizer.step()\n\n        return epoch_loss \/ len(train_loader), epoch_accuracy \/ len(train_loader)\n\n    def validate_one_epoch(self, valid_loader, criterion, device):\n        # keep track of validation loss\n        valid_loss = 0.0\n        valid_accuracy = 0.0\n\n        ######################\n        # validate the model #\n        ######################\n        self.model.eval()\n        for data, target in valid_loader:\n            # move tensors to GPU if CUDA is available\n            if device.type == \"cuda\":\n                data, target = data.cuda(), target.cuda()\n            elif device.type == \"xla\":\n                data = data.to(device, dtype=torch.float32)\n                target = target.to(device, dtype=torch.int64)\n\n            with torch.no_grad():\n                # forward pass: compute predicted outputs by passing inputs to the model\n                output = self.model(data)\n                # calculate the batch loss\n                loss = criterion(output, target)\n                # Calculate Accuracy\n                accuracy = (output.argmax(dim=1) == target).float().mean()\n                # update average validation loss and accuracy\n                valid_loss += loss\n                valid_accuracy += accuracy\n\n        return valid_loss \/ len(valid_loader), valid_accuracy \/ len(valid_loader)","867912aa":"def fit_tpu(\n    model, epochs, device, criterion, optimizer, train_loader, valid_loader=None\n):\n\n    valid_loss_min = np.Inf  # track change in validation loss\n\n    # keeping track of losses as it happen\n    train_losses = []\n    valid_losses = []\n    train_accs = []\n    valid_accs = []\n\n    for epoch in range(1, epochs + 1):\n        gc.collect()\n        para_train_loader = pl.ParallelLoader(train_loader, [device])\n\n        xm.master_print(f\"{'='*50}\")\n        xm.master_print(f\"EPOCH {epoch} - TRAINING...\")\n        train_loss, train_acc = model.train_one_epoch(\n            para_train_loader.per_device_loader(device), criterion, optimizer, device\n        )\n        xm.master_print(\n            f\"\\n\\t[TRAIN] EPOCH {epoch} - LOSS: {train_loss}, ACCURACY: {train_acc}\\n\"\n        )\n        train_losses.append(train_loss)\n        train_accs.append(train_acc)\n        gc.collect()\n\n        if valid_loader is not None:\n            gc.collect()\n            para_valid_loader = pl.ParallelLoader(valid_loader, [device])\n            xm.master_print(f\"EPOCH {epoch} - VALIDATING...\")\n            valid_loss, valid_acc = model.validate_one_epoch(\n                para_valid_loader.per_device_loader(device), criterion, device\n            )\n            xm.master_print(f\"\\t[VALID] LOSS: {valid_loss}, ACCURACY: {valid_acc}\\n\")\n            valid_losses.append(valid_loss)\n            valid_accs.append(valid_acc)\n            gc.collect()\n\n            # save model if validation loss has decreased\n            if valid_loss <= valid_loss_min and epoch != 1:\n                xm.master_print(\n                    \"Validation loss decreased ({:.4f} --> {:.4f}).  Saving model ...\".format(\n                        valid_loss_min, valid_loss\n                    )\n                )\n            #                 xm.save(model.state_dict(), 'best_model.pth')\n\n            valid_loss_min = valid_loss\n\n    return {\n        \"train_loss\": train_losses,\n        \"valid_losses\": valid_losses,\n        \"train_acc\": train_accs,\n        \"valid_acc\": valid_accs,\n    }","b95b38e4":"model = ViTBase16(n_classes=5, pretrained=True)","138c5db5":"def _run():\n    train_dataset = CassavaDataset(train_df, transforms=transforms_train)\n    valid_dataset = CassavaDataset(valid_df, transforms=transforms_valid)\n\n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n        train_dataset,\n        num_replicas=xm.xrt_world_size(),\n        rank=xm.get_ordinal(),\n        shuffle=True,\n    )\n\n    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n        valid_dataset,\n        num_replicas=xm.xrt_world_size(),\n        rank=xm.get_ordinal(),\n        shuffle=False,\n    )\n\n    train_loader = torch.utils.data.DataLoader(\n        dataset=train_dataset,\n        batch_size=BATCH_SIZE,\n        sampler=train_sampler,\n        drop_last=True,\n        num_workers=8,\n    )\n\n    valid_loader = torch.utils.data.DataLoader(\n        dataset=valid_dataset,\n        batch_size=BATCH_SIZE,\n        sampler=valid_sampler,\n        drop_last=True,\n        num_workers=8,\n    )\n\n    criterion = nn.CrossEntropyLoss()\n    #     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    device = xm.xla_device()\n    model.to(device)\n\n    lr = LR * xm.xrt_world_size()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    xm.master_print(f\"INITIALIZING TRAINING ON {xm.xrt_world_size()} TPU CORES\")\n    start_time = datetime.now()\n    xm.master_print(f\"Start Time: {start_time}\")\n\n    logs = fit_tpu(\n        model=model,\n        epochs=N_EPOCHS,\n        device=device,\n        criterion=criterion,\n        optimizer=optimizer,\n        train_loader=train_loader,\n        valid_loader=valid_loader,\n    )\n\n    xm.master_print(f\"Execution time: {datetime.now() - start_time}\")\n\n    xm.master_print(\"Saving Model\")\n    xm.save(\n        model.state_dict(), f'model_5e_{datetime.now().strftime(\"%Y%m%d-%H%M\")}.pth'\n    )","7766de5a":"# Start training processes\ndef _mp_fn(rank, flags):\n    torch.set_default_tensor_type(\"torch.FloatTensor\")\n    a = _run()\n\n\n# _run()\nFLAGS = {}\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method=\"fork\")","8c0f0edd":"## Vision Transformer Implementation in PyTorch\n\u30d3\u30b8\u30e7\u30f3\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u3092\u7406\u89e3\u3057\u305f\u3068\u3053\u308d\u3067\u3001  [this competition](https:\/\/www.kaggle.com\/c\/cassava-leaf-disease-classification)\u306e\u30d9\u30fc\u30b9\u30e9\u30a4\u30f3\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\n\n\u307e\u305a\u3001TPU \u3068 torch-image-models (timm) \u3092\u4f7f\u3048\u308b\u3088\u3046\u306b\u3059\u308b\u305f\u3081\u306b torch-xla \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002","aa0bab5e":"# <font size=4 color='blue'>\u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u304c\u4fbf\u5229\u3060\u3068\u601d\u3063\u305f\u3089\u3001\u79c1\u306f\u3082\u3063\u3068\u305d\u306e\u3088\u3046\u306a\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3092\u66f8\u304f\u305f\u3081\u306b\u3084\u308b\u6c17\u306b\u3055\u305b\u308bUpvote\u3092\u6b8b\u3057\u3066\u304f\u3060\u3055\u3044\u3002<\/font>","4fb2723a":"# <font size=4 color='blue'>\u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u304c\u4fbf\u5229\u3060\u3068\u601d\u3063\u305f\u3089\u3001\u79c1\u306f\u3082\u3063\u3068\u305d\u306e\u3088\u3046\u306a\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3092\u66f8\u304f\u305f\u3081\u306b\u3084\u308b\u6c17\u306b\u3055\u305b\u308bUpvote\u3092\u6b8b\u3057\u3066\u304f\u3060\u3055\u3044\u3002<\/font>","9ec688da":"# Vision Transformers: A gentle introduction\n\nVision Transformers\u306f\u30012020\u5e7410\u6708\u4e0b\u65ec\u306bGoogle Brain\u30c1\u30fc\u30e0\u304c\u767a\u8868\u3057\u305f\u8ad6\u6587 [AN IMAGE IS WORTH 16X16 WORDS:\nTRANSFORMERS FOR IMAGE RECOGNITION AT SCALE](https:\/\/arxiv.org\/pdf\/2010.11929.pdf) \u3067\u521d\u3081\u3066\u7d39\u4ecb\u3055\u308c\u307e\u3057\u305f\u3002\n\nViT\u306e\u4ed5\u7d44\u307f\u3092\u7406\u89e3\u3059\u308b\u305f\u3081\u306b\u306f\u3001\u5f53\u7136\u306a\u304c\u3089transformer\u304c\u3069\u306e\u3088\u3046\u306b\u6a5f\u80fd\u3057\u3001\u3069\u306e\u3088\u3046\u306a\u554f\u984c\u3092\u89e3\u6c7a\u3057\u305f\u306e\u304b\u306b\u3064\u3044\u3066\u306e\u4e88\u5099\u77e5\u8b58\u304c\u5fc5\u8981\u3067\u3059\u3002\u3053\u3053\u3067\u306f\u3001transformer\u304c\u3069\u306e\u3088\u3046\u306b\u6a5f\u80fd\u3059\u308b\u306e\u304b\u3092\u7c21\u5358\u306b\u7d39\u4ecb\u3057\u3066\u304b\u3089\u3001\u76ee\u306e\u524d\u306e\u30c8\u30d4\u30c3\u30af\u3067\u3042\u308bViT\u306e\u8a73\u7d30\u306b\u5165\u3063\u3066\u3044\u3053\u3046\u3068\u601d\u3044\u307e\u3059\u3002\n\n![ViT-Illustration](vision-transformer.png)\n\n\u3082\u3057\u3042\u306a\u305f\u304cNLP\uff08\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\uff09\u3092\u521d\u3081\u3066\u77e5\u308a\u3001\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u30e2\u30c7\u30eb\u306b\u3064\u3044\u3066\u3082\u3063\u3068\u77e5\u308a\u305f\u3044\u3068\u601d\u3063\u3066\u3044\u3066\u3001\u5b9f\u969b\u306b\u3069\u306e\u3088\u3046\u306b\u6a5f\u80fd\u3059\u308b\u304b\u306b\u3064\u3044\u3066\u306e\u516c\u6b63\u306a\u76f4\u89b3\u3092\u5f97\u305f\u3044\u3068\u601d\u3063\u3066\u3044\u308b\u306a\u3089\u3001 [Jay Allamar](https:\/\/jalammar.github.io\/)\u306e\u7d20\u6674\u3089\u3057\u3044\u30d6\u30ed\u30b0\u8a18\u4e8b\u3092\u30c1\u30a7\u30c3\u30af\u3057\u3066\u307f\u308b\u3053\u3068\u3092\u304a\u52e7\u3081\u3057\u307e\u3059\u3002\u4e0a\u306e\u753b\u50cf\u3082\u5f7c\u306e\u30d6\u30ed\u30b0\u8a18\u4e8b\u304b\u3089\u30a4\u30f3\u30b9\u30d4\u30ec\u30fc\u30b7\u30e7\u30f3\u3092\u5f97\u3066\u3044\u307e\u3059\u3002","5688854e":"# Introduction\n\n\u3053\u306e\u30ce\u30fc\u30c8\u306f\u3001\u30bf\u30a4\u30c8\u30eb\u304b\u3089\u3082\u308f\u304b\u308b\u3088\u3046\u306b\u3001\u57fa\u672c\u7684\u306b\u306f2\u3064\u306e\u30d1\u30fc\u30c8\u306b\u5206\u304b\u308c\u3066\u3044\u307e\u3059\u3002\n\n<a href=\"#Vision-Transformers:-A-gentle-introduction\">1. Vision Transformer: \u512a\u3057\u3044\u7d39\u4ecb<\/a> <br>\n<a href=\"#Vision-Transformer-Implementation-in-PyTorch\">2. PyTorch\u3067\u306e\u5b9f\u88c5<\/a>\n\n**\u4eca\u56de\u306e\u30b3\u30f3\u30c6\u30b9\u30c8\u306e\u305f\u3081\u306b\u3001PyTorch \u3067\u306e ViT \u306e\u5b9f\u88c5\u306b\u5165\u308b\u524d\u306b\u3001Vision Transformers \u306e\u57fa\u672c\u7684\u306a\u8003\u3048\u65b9\u3068\u305d\u306e\u4ed5\u7d44\u307f\u306b\u3064\u3044\u3066\u7c21\u5358\u306b\u8aac\u660e\u3057\u307e\u3059\u3002**\n\n\u30b3\u30fc\u30c9\u3060\u3051\u306b\u8208\u5473\u304c\u3042\u308b\u65b9\u306f\u3001\u3053\u306e\u30ce\u30fc\u30c8\u306e\u7b2c\u4e8c\u7ae0\u3092\u8aad\u307f\u98db\u3070\u3057\u3066\u3082\u69cb\u3044\u307e\u305b\u3093\u3002 \u5b9f\u88c5\u306f\u3001[rwightman\/pytorch-image-models](https:\/\/github.com\/rwightman\/pytorch-image-models) \u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u304a\u9670\u3067\u5927\u304d\u304f\u5909\u308f\u308b\u3053\u3068\u306f\u3042\u308a\u307e\u305b\u3093\u304c\u3001\u3053\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u306b\u306f\u3001\u4e8b\u524d\u5b66\u7fd2\u3055\u308c\u305f\u91cd\u307f\u3092\u542b\u3080\u3059\u3079\u3066\u306e\u30e2\u30c7\u30eb\u306e\u5b9f\u88c5\u304c\u542b\u307e\u308c\u3066\u3044\u307e\u3059\u3002","a0671e99":"## Transformers: A brief overview\n\n> **\u3059\u3067\u306bTransformer\u3092\u7406\u89e3\u3055\u308c\u3066\u3044\u308b\u65b9\u306f\u3001\u3054\u81ea\u7531\u306b\u8aad\u307f\u98db\u3070\u3057\u3066\u304f\u3060\u3055\u3044\u3002**\n\n\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u30e2\u30c7\u30eb\u306f\u3001\u79c1\u305f\u3061\u304c\u77e5\u3063\u3066\u3044\u308b\u3088\u3046\u306b\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306b\u9769\u547d\u3092\u3082\u305f\u3089\u3057\u307e\u3057\u305f\u3002\u6700\u521d\u306b\u5c0e\u5165\u3055\u308c\u305f\u3068\u304d\u3001\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u30e2\u30c7\u30eb\u306f\u8907\u6570\u306eNLP\u8a18\u9332\u3092\u66f4\u65b0\u3057\u3001\u5f53\u6642\u306eState of the Art\u3092\u62bc\u3057\u9032\u3081\u3066\u3044\u307e\u3057\u305f\u3002\u4eca\u3067\u306f\u3001\u73fe\u4ee3\u306eNLP\u30bf\u30b9\u30af\u306e\u30c7\u30d5\u30a1\u30af\u30c8\u30b9\u30bf\u30f3\u30c0\u30fc\u30c9\u3068\u306a\u3063\u3066\u304a\u308a\u3001LSTM\u3084GRU\u306e\u3088\u3046\u306a\u524d\u4e16\u4ee3\u306e\u30e2\u30c7\u30eb\u3068\u6bd4\u8f03\u3059\u308b\u3068\u3001\u9a5a\u304f\u307b\u3069\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u5411\u4e0a\u3092\u3082\u305f\u3089\u3057\u307e\u3059\u3002\n\nNLP\u306e\u98a8\u666f\u3092\u4e00\u5909\u3055\u305b\u305f\u6700\u3082\u91cd\u8981\u306a\u8ad6\u6587\u306f\u3001[\"Attention is all you need\"](https:\/\/arxiv.org\/pdf\/1706.03762.pdf) \u3068\u3044\u3046\u8ad6\u6587\u3067\u3059\u3002\u3053\u306e\u8ad6\u6587\u3067\u7d39\u4ecb\u3055\u308c\u305f\u306e\u304c\u3001\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3067\u3059\u3002\n\n### **Motivations:**\n\n\u5f53\u6642\u3001\u7cfb\u5217\u3084NLP\u30bf\u30b9\u30af\u306e\u305f\u3081\u306e\u65e2\u5b58\u306e\u30e2\u30c7\u30eb\u306f\u3001\u307b\u3068\u3093\u3069\u304cRNN\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3057\u305f\u3002 **\u3053\u308c\u3089\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u554f\u984c\u70b9\u306f\u3001\u9577\u671f\u7684\u306a\u4f9d\u5b58\u95a2\u4fc2\u3092\u6349\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u306a\u3044\u3053\u3068\u3067\u3057\u305f\u3002** \n\nLSTM\u3084GRU - RNN\u306e\u4e9c\u7a2e\u306f\u4f9d\u5b58\u95a2\u4fc2\u3092\u30ad\u30e3\u30d7\u30c1\u30e3\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3057\u305f\u304c\u3001\u305d\u308c\u306b\u3082\u9650\u754c\u304c\u3042\u308a\u307e\u3057\u305f\u3002 \n\n\u305d\u3053\u3067\u3001\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306e\u80cc\u5f8c\u306b\u3042\u308b\u4e3b\u306a\u30a4\u30f3\u30b9\u30d4\u30ec\u30fc\u30b7\u30e7\u30f3\u306f\u3001\u3053\u306e\u518d\u5e30\u3092\u53d6\u308a\u9664\u304d\u3001\u307b\u307c\u3059\u3079\u3066\u306e\u4f9d\u5b58\u6027\u3092\u30ad\u30e3\u30d7\u30c1\u30e3\u3059\u308b\u3053\u3068\u3067\u3057\u305f\u3002\u3053\u308c\u306f\u3001self-attention\uff08\u30de\u30eb\u30c1\u30d8\u30c3\u30c9\uff09\u3068\u547c\u3070\u308c\u308b\u6ce8\u610f\u30e1\u30ab\u30cb\u30ba\u30e0\u306e\u5909\u5f62\u3092\u4f7f\u7528\u3057\u3066\u9054\u6210\u3055\u308c\u305f\u3082\u306e\u3067\u3001\u6210\u529f\u306b\u306f\u975e\u5e38\u306b\u91cd\u8981\u3067\u3059\u3002\u30c8\u30e9\u30f3\u30d5\u30a9\u30fc\u30de\u30fc\u30e2\u30c7\u30eb\u306e\u3082\u3046\u4e00\u3064\u306e\u5229\u70b9\u306f\u3001\u9ad8\u5ea6\u306a\u4e26\u5217\u5316\u304c\u53ef\u80fd\u3067\u3042\u308b\u3053\u3068\u3067\u3059\u3002  \n\n\n### Transformer Architecture\n**\u6ce8: \u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u56f3\u306b\u306f\u3001\u8aac\u660e\u4e2d\u306e\u5bfe\u5fdc\u3059\u308b\u30b9\u30c6\u30c3\u30d7\u304c\u6ce8\u8a18\u3055\u308c\u3066\u3044\u307e\u3059\u3002**\n\n![TranformerArchitecture](transformer-arch.png)\n\n- Transformer\u306b\u306f\u3001\u4e0a\u56f3\u306e\u5de6\u5074\u306b\u3042\u308b\u30c7\u30b3\u30fc\u30c0\u3068\u53f3\u5074\u306b\u3042\u308b\u30a8\u30f3\u30b3\u30fc\u30c0\u306e2\u3064\u306e\u90e8\u5206\u304c\u3042\u308a\u307e\u3059\u3002 \n- \u3053\u3053\u3067\u306f\u6a5f\u68b0\u7ffb\u8a33\u3092\u3057\u3066\u3044\u308b\u3068\u60f3\u50cf\u3057\u3066\u304f\u3060\u3055\u3044\u3002 \n- \u30a8\u30f3\u30b3\u30fc\u30c0\u306f\u5165\u529b\u30c7\u30fc\u30bf\uff08\u6587\uff09\u3092\u53d7\u3051\u53d6\u308a\u3001\u5165\u529b\u306e\u4e2d\u9593\u8868\u73fe\u3092\u751f\u6210\u3057\u307e\u3059\u3002 \n- \u30c7\u30b3\u30fc\u30c0\u306f\u3053\u306e\u4e2d\u9593\u8868\u73fe\u3092\u6bb5\u968e\u7684\u306b\u30c7\u30b3\u30fc\u30c9\u3057\u3001\u51fa\u529b\u3092\u751f\u6210\u3057\u307e\u3059\u3002\u3057\u304b\u3057\u3001\u9055\u3044\u306f\u3053\u308c\u3092\u3069\u306e\u3088\u3046\u306b\u884c\u3063\u3066\u3044\u308b\u304b\u306b\u3042\u308a\u307e\u3059\u3002 \n- ViT\u3067\u306f\u3001\u30a8\u30f3\u30b3\u30fc\u30c0\u306e\u30bb\u30af\u30b7\u30e7\u30f3\u3092\u7406\u89e3\u3059\u308b\u3060\u3051\u3067\u5341\u5206\u3067\u3059\u3002 \n\n> **\u6ce8: \u3053\u3053\u3067\u306e\u8aac\u660e\u306f\u3001\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306e\u80cc\u5f8c\u306b\u3042\u308b\u76f4\u611f\u306b\u3064\u3044\u3066\u306e\u3082\u306e\u3067\u3059\u3002\u3088\u308a\u591a\u304f\u306e\u6570\u5b66\u7684\u306a\u8a73\u7d30\u306b\u3064\u3044\u3066\u306f\u3001\u4ee3\u308f\u308a\u306b\u305d\u308c\u305e\u308c\u306e\u7814\u7a76\u8ad6\u6587\u3092\u30c1\u30a7\u30c3\u30af\u3057\u3066\u304f\u3060\u3055\u3044\u3002**\n\n### Tranformers: Step by step overview\n**(1)** \u5165\u529b\u30c7\u30fc\u30bf\u306f\u6700\u521d\u306b\u30d9\u30af\u30c8\u30eb\u306b\u57cb\u3081\u8fbc\u307e\u308c\u307e\u3059\u3002\u57cb\u3081\u8fbc\u307f\u5c64(embedding layer)\u306f\u3001\u5404\u5358\u8a9e\u306e\u305f\u3081\u306b\u5b66\u7fd2\u3055\u308c\u305f\u30d9\u30af\u30c8\u30eb\u8868\u73fe\u3092\u3064\u304b\u3080\u306e\u306b\u5f79\u7acb\u3064\u3002\n\n**(2)** \u6b21\u306e\u6bb5\u968e\u3067\u306f\u3001\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u304c\u5165\u529bembedding\u306b\u6ce8\u5165\u3055\u308c\u308b\u3002\u3053\u308c\u306f\u3001Tranformer\u304c\u5165\u529b\u3068\u3057\u3066\u6e21\u3055\u308c\u308b\u30b7\u30fc\u30b1\u30f3\u30b9\u306e\u9806\u5e8f\uff08\u4f8b\u3048\u3070\u6587\uff09\u3092\u77e5\u3089\u306a\u3044\u304b\u3089\u3067\u3059\n\n**(3)** \u3053\u3053\u3067\u3001 multi-headed attention\u304c\u5c11\u3057\u5909\u308f\u3063\u3066\u304d\u307e\u3059\u3002\n\n**Multi-headed-attention architecture:**\n![multi-headed-attn](multi-headed-attention.png)\n\n**(4)** Multi-Headed Attention\u306f3\u3064\u306e\u5b66\u7fd2\u53ef\u80fd\u306a\u30d9\u30af\u30c8\u30eb\u3067\u69cb\u6210\u3055\u308c\u3066\u3044\u307e\u3059\u3002Query, Key\u3001Value\u306e3\u3064\u306e\u30d9\u30af\u30c8\u30eb\u3067\u3042\u308b\u3002\u3053\u308c\u306f\u3001\u691c\u7d22\uff08\u30af\u30a8\u30ea\uff09\u3059\u308b\u3068\u3001\u691c\u7d22\u30a8\u30f3\u30b8\u30f3\u304c\u30af\u30a8\u30ea\u3068\u30ad\u30fc\u3092\u6bd4\u8f03\u3057\u3001\u5024\u3067\u5fdc\u7b54\u3059\u308b\u3068\u3044\u3046\u60c5\u5831\u306e\u518d\u5229\u7528\u306b\u7531\u6765\u3059\u308b\u3068\u8a00\u308f\u308c\u3066\u3044\u307e\u3059\u3002\n\n**(5)** Q \u3068 K \u306e\u8868\u73fe\u306f\u3001\u30c9\u30c3\u30c8\u7a4d\u884c\u5217\u306e\u4e57\u7b97\u3092\u7d4c\u3066\u3001\u3042\u308b\u5358\u8a9e\u304c\u4ed6\u306e\u3059\u3079\u3066\u306e\u5358\u8a9e\u306b\u3069\u308c\u3060\u3051\u6ce8\u610f\u3092\u6255\u308f\u306a\u3051\u308c\u3070\u306a\u3089\u306a\u3044\u304b\u3092\u8868\u3059\u30b9\u30b3\u30a2\u884c\u5217\u3092\u751f\u6210\u3057\u307e\u3059\u3002\u30b9\u30b3\u30a2\u304c\u9ad8\u3051\u308c\u3070\u9ad8\u3044\u307b\u3069\u6ce8\u76ee\u5ea6\u304c\u9ad8\u304f\u3001\u9006\u3082\u307e\u305f\u7136\u308a\u3067\u3059\u3002 \n\n**(6)** \u305d\u306e\u5f8c\u3001\u30b9\u30b3\u30a2\u884c\u5217\u306f\u3001Q \u3068 K \u30d9\u30af\u30c8\u30eb\u306e\u6b21\u5143\u306b\u5fdc\u3058\u3066\u30b9\u30b1\u30fc\u30eb\u30c0\u30a6\u30f3\u3055\u308c\u308b\u3002\u3053\u308c\u306f\uff0c\u4e57\u7b97\u304c\u7206\u767a\u7684\u306a\u52b9\u679c\u3092\u3082\u305f\u3089\u3059\u53ef\u80fd\u6027\u304c\u3042\u308b\u305f\u3081\uff0c\u3088\u308a\u5b89\u5b9a\u3057\u305f\u52fe\u914d\u3092\u78ba\u4fdd\u3059\u308b\u305f\u3081\u3067\u3059\uff0e \n\n(\u30de\u30b9\u30af\u306e\u90e8\u5206\u306b\u3064\u3044\u3066\u306f\u3001\u30c7\u30b3\u30fc\u30c0\u306e\u30bb\u30af\u30b7\u30e7\u30f3\u306b\u5230\u9054\u3057\u305f\u3068\u304d\u306b\u8aac\u660e\u3057\u307e\u3059)\n\n**(7)** \u6b21\u306b\u3001\u6ce8\u76ee\u5ea6\u30b9\u30b3\u30a2\u3092\u78ba\u7387\u306b\u5909\u63db\u3059\u308b\u305f\u3081\u306b\u3001\u30b9\u30b3\u30a2\u884c\u5217\u3092\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u5316\u3057\u307e\u3059\u3002\u660e\u3089\u304b\u306b\u3001\u30b9\u30b3\u30a2\u304c\u9ad8\u3044\u307b\u3069\u9ad8\u304f\u306a\u308a\u3001\u4f4e\u3044\u307b\u3069\u4f4e\u304f\u306a\u308a\u307e\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u30e2\u30c7\u30eb\u304c\u3069\u306e\u5358\u8a9e\u306b\u6ce8\u76ee\u3059\u3079\u304d\u304b\u3092\u78ba\u5b9f\u306b\u5224\u65ad\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002 \n\n**(8)** \u6b21\u306b\u3001\u78ba\u7387\u3092\u542b\u3080\u7d50\u679c\u306e\u884c\u5217\u306b\u5024\u30d9\u30af\u30c8\u30eb\u3092\u4e57\u7b97\u3057\u307e\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u30e2\u30c7\u30eb\u304c\u5b66\u7fd2\u3057\u305f\u78ba\u7387\u30b9\u30b3\u30a2\u306e\u9ad8\u3044\u5358\u8a9e\u304c\u3088\u308a\u91cd\u8981\u306b\u306a\u308b\u3002\u30b9\u30b3\u30a2\u306e\u4f4e\u3044\u5358\u8a9e\u306f\u52b9\u679c\u7684\u306b\u304b\u304d\u6d88\u3055\u308c\u3066\u7121\u95a2\u4fc2\u306b\u306a\u308b\u3002 \n\n**(9)** \u305d\u3057\u3066\u3001QK\u30d9\u30af\u30c8\u30eb\u3068V\u30d9\u30af\u30c8\u30eb\u306e\u9023\u7d50\u51fa\u529b\u3092Linear\u5c64\u306b\u9001\u308a\u8fbc\u307f\u3001\u3055\u3089\u306b\u51e6\u7406\u3092\u884c\u3046\u3002 \n\n**(10)** \u30b7\u30fc\u30b1\u30f3\u30b9\u5185\u306e\u5404\u5358\u8a9e\u306b\u5bfe\u3057\u3066Self-Attention\u304c\u884c\u308f\u308c\u308b\u30021\u3064\u306f\u4ed6\u306e\u3082\u306e\u306b\u4f9d\u5b58\u3057\u306a\u3044\u306e\u3067\u3001Self-Attention\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u30b3\u30d4\u30fc\u3092\u4f7f\u7528\u3057\u3066\u3001\u3053\u308c\u3092**multi-headed**\u5316\u3057\u3066\u540c\u6642\u306b\u3059\u3079\u3066\u3092\u51e6\u7406\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \n\n**(11)** \u305d\u306e\u5f8c\u3001\u51fa\u529b\u5024\u30d9\u30af\u30c8\u30eb\u3092\u9023\u7d50\u3057\u3001\u5165\u529b\u5c64\u304b\u3089\u306e\u6b8b\u5dee\u63a5\u7d9a\u306b\u52a0\u7b97\u3057\u3001\u305d\u306e\u7d50\u679c\u306e\u518d\u8868\u73fe\u3092LayernNorm\u306b\u6e21\u3057\u3066\u6b63\u898f\u5316\u3059\u308b\u3002(\u6b8b\u5dee\u63a5\u7d9a\u306f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u6d41\u308c\u308b\u52fe\u914d\u3092\u52a9\u3051\u3001LayernNorm\u306f\u5b66\u7fd2\u6642\u9593\u3092\u308f\u305a\u304b\u306b\u77ed\u7e2e\u3057\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u5b89\u5b9a\u5316\u3055\u305b\u308b\u306e\u306b\u5f79\u7acb\u3061\u307e\u3059)\n\n**(12)** \u3055\u3089\u306b\u3001\u51fa\u529b\u306f\u3001\u3088\u308a\u8c4a\u304b\u306a\u8868\u73fe\u3092\u5f97\u308b\u305f\u3081\u306b\u3001point-wise feed forward network\u306b\u6e21\u3055\u308c\u307e\u3059\u3002  \n\n**(13)**  \u51fa\u529b\u306f\u518d\u3073\u30ec\u30a4\u30e4\u30fc\u30ce\u30eb\u30e0\u5316\u3055\u308c\u3001\u524d\u306e\u30ec\u30a4\u30e4\u30fc\u304b\u3089\u6b8b\u5dee\u304c\u8ffd\u52a0\u3055\u308c\u307e\u3059\u3002 \n\n\n**\u6ce8\u610f: \u3053\u308c\u3067\u30a8\u30f3\u30b3\u30fc\u30c0\u306e\u30bb\u30af\u30b7\u30e7\u30f3\u306f\u7d42\u308f\u308a\u3067\u3059\u304c\u3001Vision Transformer \u3092\u5b8c\u5168\u306b\u7406\u89e3\u3059\u308b\u306b\u306f\u3053\u308c\u3067\u5341\u5206\u3067\u3059\u3002\u30c7\u30b3\u30fc\u30c0\u90e8\u5206\u306f\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u30ec\u30a4\u30e4\u30fc\u3068\u975e\u5e38\u306b\u4f3c\u3066\u3044\u308b\u306e\u3067\u3001\u7406\u89e3\u3059\u308b\u306e\u306f\u3042\u306a\u305f\u306b\u304a\u4efb\u305b\u3057\u307e\u3059\u3002**\n\n**(14)** \u30a8\u30f3\u30b3\u30fc\u30c0\u304b\u3089\u306e\u51fa\u529b\u306f\u3001\u524d\u306e\u6642\u9593\u30b9\u30c6\u30c3\u30d7\/\u30ef\u30fc\u30c9\u304b\u3089\u306e\u5165\u529b\uff08\u3082\u3057\u3042\u308c\u3070\uff09\u3068\u5171\u306b\u30c7\u30b3\u30fc\u30c0\u306b\u9001\u3089\u308c\u3001\u51fa\u529b\u306f\u30a8\u30f3\u30b3\u30fc\u30c0\u304b\u3089\u306e\u51fa\u529b\u3068\u5171\u306b\u6b21\u306eattention layer\u306b\u9001\u3089\u308c\u308b\u524d\u306b\u3001\u30de\u30b9\u30af\u3055\u308c\u305fmulti headed attention\u3092\u53d7\u3051\u307e\u3059\u3002 \n\n**(15)** Masked multi headed attention \u306f\u30ea\u30fc\u30af\u304c\u306a\u3044\u3053\u3068\u3092\u78ba\u5b9f\u306b\u3059\u308b\u305f\u3081\u306b\u3001\u30c7\u30b3\u30fc\u30c9\u4e2d\u306b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u30b7\u30fc\u30b1\u30f3\u30b9\u5185\u3067\u5f8c\u304b\u3089\u6765\u308b\u5358\u8a9e\u3078\u306e\u53ef\u8996\u6027\u3092\u6301\u3064\u3079\u304d\u3067\u306f\u306a\u3044\u305f\u3081\u306b\u5fc5\u8981\u3067\u3042\u308b\u3002\u3053\u308c\u306f\u3001\u30b9\u30b3\u30a2\u30de\u30c8\u30ea\u30af\u30b9\u306e\u7cfb\u5217\u5185\u3067\u5f8c\u304b\u3089\u6765\u308b\u5358\u8a9e\u306e\u30a8\u30f3\u30c8\u30ea\u3092\u30de\u30b9\u30af\u3059\u308b\u3053\u3068\u306b\u3088\u3063\u3066\u884c\u308f\u308c\u307e\u3059\u3002\u30b7\u30fc\u30b1\u30f3\u30b9\u5185\u306e\u73fe\u5728\u306e\u5358\u8a9e\u3068\u524d\u306e\u5358\u8a9e\u306f 1 \u3067\u8ffd\u52a0\u3055\u308c\u3001\u672a\u6765\u306e\u5358\u8a9e\u306e\u30b9\u30b3\u30a2\u306f -inf \u3067\u8ffd\u52a0\u3055\u308c\u307e\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u78ba\u7387\u3092\u5f97\u308b\u305f\u3081\u306bsoftmax\u3092\u5b9f\u884c\u3059\u308b\u969b\u306b\u3001\u7cfb\u5217\u5185\u306e\u5c06\u6765\u306e\u5358\u8a9e\u304c0\u306b\u304b\u304d\u6d88\u3055\u308c\u3001\u6b8b\u308a\u306e\u5358\u8a9e\u306f\u4fdd\u6301\u3055\u308c\u307e\u3059\u3002 \n\n**(16)** \u3053\u3053\u306b\u3082\u6b8b\u5dee\u63a5\u7d9a\u304c\u3042\u308a\u3001\u52fe\u914d\u306e\u6d41\u308c\u3092\u6539\u5584\u3057\u3066\u3044\u307e\u3059\u3002\u6700\u5f8c\u306b\u3001\u51fa\u529b\u306f\u7dda\u5f62\u5c64\u306b\u9001\u3089\u308c\u3001\u78ba\u7387\u306e\u51fa\u529b\u3092\u5f97\u308b\u305f\u3081\u306b\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u3055\u308c\u307e\u3059\u3002 ","411f89bd":"## How Vision Tranformers works?\n\nTranformer\u306e\u5185\u90e8\u306e\u50cd\u304d\u3092\u9ad8\u3044\u30ec\u30d9\u30eb\u3067\u30ab\u30d0\u30fc\u3057\u305f\u3068\u3053\u308d\u3067\u3001\u3044\u3088\u3044\u3088Vision Tranformers\u306b\u53d6\u308a\u7d44\u3080\u6e96\u5099\u304c\u6574\u3044\u307e\u3057\u305f\u3002 \n\nTranformer\u3092\u753b\u50cf\u306b\u9069\u7528\u3059\u308b\u3053\u3068\u306f\u3001\u4ee5\u4e0b\u306e\u7406\u7531\u304b\u3089\u5e38\u306b\u56f0\u96e3\u306a\u3053\u3068\u3067\u3057\u305f\u3002\n- \u5358\u8a9e\/\u6587\u7ae0\/\u6bb5\u843d\u3068\u306f\u7570\u306a\u308a\u3001\u753b\u50cf\u306f\u57fa\u672c\u7684\u306b\u306f\u30d4\u30af\u30bb\u30eb\u306e\u5f62\u3067\u3088\u308a\u591a\u304f\u306e\u60c5\u5831\u3092\u542b\u3093\u3067\u3044\u307e\u3059\u3002 \n- \u73fe\u5728\u306e\u30cf\u30fc\u30c9\u30a6\u30a7\u30a2\u3067\u3082\u3001\u753b\u50cf\u5185\u306e\u3059\u3079\u3066\u306e\u30d4\u30af\u30bb\u30eb\u306b\u6ce8\u76ee\u3059\u308b\u3053\u3068\u306f\u975e\u5e38\u306b\u56f0\u96e3\u3067\u3059\u3002 \n- \u305d\u306e\u4ee3\u308f\u308a\u306b\u3001\u4eba\u6c17\u306e\u3042\u308b\u4ee3\u66ff\u6848\u306f\u3001\u5c40\u6240\u7684\u306aattention\u3092\u5229\u7528\u3059\u308b\u3053\u3068\u3067\u3057\u305f\u3002 \n- \u5b9f\u969b\u3001CNN\u306f\u7573\u307f\u8fbc\u307f\u306b\u3088\u3063\u3066\u975e\u5e38\u306b\u4f3c\u305f\u3088\u3046\u306a\u3053\u3068\u3092\u3057\u3066\u3044\u3066\u3001\u30e2\u30c7\u30eb\u306e\u5c64\u3092\u6df1\u304f\u3057\u3066\u3044\u304f\u3068\u53d7\u5bb9\u91ce\u306f\u672c\u8cea\u7684\u306b\u5927\u304d\u304f\u306a\u308a\u307e\u3059\u304c\u3001Tranformer\u306f\u300cTranformer\u300d\u306e\u6027\u8cea\u4e0a\u3001\u5e38\u306bCNN\u3088\u308a\u3082\u8a08\u7b97\u91cf\u304c\u591a\u304f\u306a\u308a\u307e\u3059\u3002\u3082\u3061\u308d\u3093\u3001CNN\u304c\u73fe\u5728\u306e\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30d3\u30b8\u30e7\u30f3\u306e\u9032\u6b69\u306b\u3069\u308c\u3060\u3051\u8ca2\u732e\u3057\u3066\u3044\u308b\u304b\u306f\u77e5\u3063\u3066\u3044\u308b\u3002\n\n\u30b0\u30fc\u30b0\u30eb\u306e\u7814\u7a76\u8005\u305f\u3061\u306f\u8ad6\u6587\u306e\u4e2d\u3067\u3001\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30fb\u30d3\u30b8\u30e7\u30f3\u306e\u6b21\u306e\u5927\u304d\u306a\u4e00\u6b69\u3068\u306a\u308a\u3046\u308b\u3001\u3053\u308c\u307e\u3067\u3068\u306f\u7570\u306a\u308b\u3082\u306e\u3092\u63d0\u6848\u3057\u3066\u3044\u308b\u3002\u5f7c\u3089\u306fCNN\u3078\u306e\u4f9d\u5b58\u306f\u3082\u3046\u5fc5\u8981\u306a\u3044\u304b\u3082\u3057\u308c\u306a\u3044\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u308b\u306e\u3060\u3002\u3067\u306f\u3001Vision Tranformer\u306b\u3064\u3044\u3066\u3082\u3063\u3068\u8a73\u3057\u304f\u898b\u3066\u3044\u3053\u3046\u3002\n\n### Vision Transformer Architecture\n\n![vit-architecture](vit-arch.png)\n\n**(1)** \u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306e\u30a8\u30f3\u30b3\u30fc\u30c0\u90e8\u5206\u3060\u3051\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u304c\u3001\u753b\u50cf\u3092\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u9001\u308a\u8fbc\u3080\u65b9\u6cd5\u306b\u9055\u3044\u304c\u3042\u308a\u307e\u3059\u3002\n\n\n**(2)** \u753b\u50cf\u3092\u56fa\u5b9a\u30b5\u30a4\u30ba\u306e\u30d1\u30c3\u30c1\u306b\u5206\u89e3\u3057\u3066\u3044\u307e\u3059\u3002\u305d\u306e\u305f\u3081\u3001\u3053\u308c\u3089\u306e\u30d1\u30c3\u30c1\u306e1\u3064\u306f\u3001\u8ad6\u6587\u3067\u63d0\u6848\u3055\u308c\u3066\u3044\u308b\u3088\u3046\u306b\u300116x16\u307e\u305f\u306f32x32\u306e\u5bf8\u6cd5\u306b\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u30d1\u30c3\u30c1\u304c\u591a\u3051\u308c\u3070\u591a\u3044\u307b\u3069\u3001\u30d1\u30c3\u30c1\u81ea\u4f53\u304c\u5c0f\u3055\u304f\u306a\u308b\u306e\u3067\u3001\u3053\u308c\u3089\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u8a13\u7df4\u3059\u308b\u306e\u304c\u3088\u308a\u7c21\u5358\u306b\u306a\u308a\u307e\u3059\u3002\u3057\u305f\u304c\u3063\u3066\u3001\u79c1\u305f\u3061\u306f\u30bf\u30a4\u30c8\u30eb\u306b\u3042\u308b\u3088\u3046\u306b\u3001\u300c\u753b\u50cf\u306f16x16\u30ef\u30fc\u30c9\u306e\u4fa1\u5024\u304c\u3042\u308b\u300d\u3068\u3044\u3046\u3053\u3068\u306b\u306a\u308a\u307e\u3059\u3002 \n\n**(3)** \u305d\u306e\u5f8c\u3001\u30d1\u30c3\u30c1\u306f\u5c55\u958b\u3055\u308c\uff08\u5e73\u5766\u5316\u3055\u308c\uff09\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3078\u306e\u66f4\u306a\u308b\u51e6\u7406\u306e\u305f\u3081\u306b\u9001\u3089\u308c\u307e\u3059\u3002\n\n**(4)** \u3053\u3053\u3067\u306eNN\u3068\u306f\u7570\u306a\u308a\u3001\u30e2\u30c7\u30eb\u306f\u30b7\u30fc\u30b1\u30f3\u30b9\u5185\u306e\u30b5\u30f3\u30d7\u30eb\u306e\u4f4d\u7f6e\u306b\u3064\u3044\u3066\u4f55\u3082\u8003\u3048\u3066\u3044\u307e\u305b\u3093\u304c\u3001\u3053\u3053\u3067\u306f\u5404\u30b5\u30f3\u30d7\u30eb\u306f\u5165\u529b\u753b\u50cf\u304b\u3089\u306e\u30d1\u30c3\u30c1\u3067\u3059\u3002 \u305d\u306e\u305f\u3081\u3001\u753b\u50cf\u306f**\u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f\u30d9\u30af\u30c8\u30eb\u3068\u4e00\u7dd2\u306b**\u30a8\u30f3\u30b3\u30fc\u30c0\u306b\u9001\u308a\u8fbc\u307e\u308c\u307e\u3059\u3002\u3053\u3053\u3067\u6ce8\u610f\u3057\u306a\u3051\u308c\u3070\u306a\u3089\u306a\u3044\u3053\u3068\u306f\u3001\u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f\u3082\u5b66\u7fd2\u53ef\u80fd\u306a\u306e\u3067\u3001\u5b9f\u969b\u306b\u306f\u4f4d\u7f6e\u306b\u95a2\u4fc2\u306a\u304f\u30cf\u30fc\u30c9\u30b3\u30fc\u30c9\u3055\u308c\u305f\u30d9\u30af\u30c8\u30eb\u3092\u9001\u308a\u8fbc\u3080\u5fc5\u8981\u306f\u306a\u3044\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002\n\n**(5)** BERT\u306e\u3088\u3046\u306b\u958b\u59cb\u6642\u306b\u7279\u5225\u306a\u30c8\u30fc\u30af\u30f3\u3082\u3042\u308a\u307e\u3059\u3002\n\n**(6)** \u5404\u753b\u50cf\u30d1\u30c3\u30c1\u306f\u3001\u6700\u521d\u306b\u5927\u304d\u306a\u30d9\u30af\u30c8\u30eb\u306b\u5c55\u958b\uff08\u5e73\u5766\u5316\uff09\u3055\u308c\u3001\u5b66\u7fd2\u53ef\u80fd\u306a\u57cb\u3081\u8fbc\u307f\u884c\u5217\u3068\u639b\u3051\u5408\u308f\u3055\u308c\u3001\u57cb\u3081\u8fbc\u307f\u30d1\u30c3\u30c1\u304c\u4f5c\u6210\u3055\u308c\u307e\u3059\u3002\u305d\u3057\u3066\u3001\u3053\u308c\u3089\u306e\u57cb\u3081\u8fbc\u307f\u30d1\u30c3\u30c1\u306f\u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f\u30d9\u30af\u30c8\u30eb\u3068\u7d50\u5408\u3055\u308c\u3001\u305d\u308c\u304c\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306b\u4f9b\u7d66\u3055\u308c\u307e\u3059\u3002 \n\n> **\u6ce8\u610f\uff1a\u3053\u3053\u304b\u3089\u5148\u306f\u3059\u3079\u3066\u6a19\u6e96\u7684\u306a\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u3068\u540c\u3058\u3067\u3059\u3002**\n\n**(7)** \u552f\u4e00\u306e\u9055\u3044\u306f\u3001\u30c7\u30b3\u30fc\u30c0\u306e\u4ee3\u308f\u308a\u306b\u30a8\u30f3\u30b3\u30fc\u30c0\u304b\u3089\u306e\u51fa\u529b\u304c\u76f4\u63a5\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u6e21\u3055\u308c\u3001\u5206\u985e\u51fa\u529b\u3092\u5f97\u308b\u3053\u3068\u3067\u3059\u3002 \n\n### Things to note:\n- \u3053\u306e\u8ad6\u6587\u306f\u3001\u307b\u3068\u3093\u3069\u306e\u5834\u5408\u3001\u30b3\u30f3\u30dc\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u3092\u5b8c\u5168\u306b\u7121\u8996\u3057\u3066\u3044\u307e\u3059\u3002 \n- \u3057\u304b\u3057\u3001\u5f7c\u3089\u306f\u3001\u753b\u50cf\u30d1\u30c3\u30c1\u306e\u30b3\u30f3\u30dc\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u57cb\u3081\u8fbc\u307f\u3092\u4f7f\u7528\u3059\u308bViT\u306e\u3044\u304f\u3064\u304b\u306e\u30d0\u30ea\u30a8\u30fc\u30b7\u30e7\u30f3\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002\u3057\u304b\u3057\u3001\u305d\u308c\u306f\u6027\u80fd\u306b\u3042\u307e\u308a\u5f71\u97ff\u3092\u4e0e\u3048\u3066\u3044\u306a\u3044\u3088\u3046\u3067\u3059\u3002  \n- \u3053\u308c\u3092\u66f8\u3044\u3066\u3044\u308b\u6642\u70b9\u3067\u306f\u3001Vision Transformers\u306fImageNet\u306e\u753b\u50cf\u5206\u985e\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3067\u30c8\u30c3\u30d7\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002 \n\n<img src=\"benchmarks-chart.png\" width=\"700\">\n<!-- ![BenchmarksChart](benchmarks-charpng) -->\n\n- \u3053\u306e\u8ad6\u6587\u306b\u306f\u4ed6\u306b\u3082\u8208\u5473\u6df1\u3044\u3053\u3068\u304c\u305f\u304f\u3055\u3093\u3042\u308a\u307e\u3059\u304c\u3001\u79c1\u306b\u3068\u3063\u3066\u76ee\u7acb\u3063\u3066\u3044\u3066\u3001\u6f5c\u5728\u7684\u306bCNN\u3088\u308a\u3082\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306e\u529b\u3092\u793a\u3057\u3066\u3044\u308b\u306e\u306f\u3001\u4e0b\u306e\u753b\u50cf\u306e\u3088\u3046\u306b\u3001\u30ec\u30a4\u30e4\u30fc\u306b\u5bfe\u3059\u308battention distance\u3092\u793a\u3057\u3066\u3044\u308b\u3053\u3068\u3067\u3059\u3002 \n\n\n<img src=\"attn-distance.png\" width=\"300\" height=\"300\">\n<br>\n\n- \u4e0a\u306e\u30b0\u30e9\u30d5\u306f\u3001\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u304c\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u958b\u59cb\u5c64\u304b\u3089\u96e2\u308c\u305f\u9818\u57df\u306b\u3059\u3067\u306battention\u3092\u6255\u3046\u80fd\u529b\u3092\u6301\u3063\u3066\u3044\u308b\u3053\u3068\u3092\u793a\u5506\u3057\u3066\u3044\u307e\u3059\u3002","6681c533":"## Thanks a lot for reading all the way\n\n# <font size=4 color='blue'>If you find this notebook useful, leave an upvote, that motivates me to write more such notebooks.<\/font>","9d3b401b":"## \u53c2\u8003\nhttps:\/\/www.kaggle.com\/abhinand05\/vision-transformer-vit-tutorial-baseline"}}