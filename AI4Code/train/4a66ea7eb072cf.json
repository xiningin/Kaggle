{"cell_type":{"317ddb7e":"code","05686f37":"code","35298589":"code","14d77642":"code","76df2d1b":"code","10d3cd84":"code","2062c72a":"code","2a42cb7a":"code","21580a87":"code","25736e83":"code","4887cb76":"code","2ae37e72":"code","8860e677":"code","43933efb":"code","b71ee959":"code","bbb9520b":"code","609b4eb9":"code","147970c9":"code","14620a26":"code","5bb826cc":"code","0856cf49":"code","fdae897b":"code","017e5a69":"code","2007cdb3":"code","ab47b34f":"code","89795cc4":"code","528da039":"code","8fef1a93":"code","79ea8c8f":"code","d4737e7a":"code","4e534cee":"code","1fc89d76":"code","7713fe46":"code","fb771f1a":"code","962fa35e":"code","1f441b06":"code","9d6873b1":"code","24c4e747":"code","310071b7":"code","60dddd1e":"code","eb22dcfa":"code","06a4fdf8":"code","ff34759f":"code","0a0986c0":"code","63b53e18":"code","fe3389b4":"code","609dab6f":"code","e08e98d7":"code","53cf4299":"code","8a408e9f":"code","2a1bb2e2":"code","10e4b749":"markdown","ef42d696":"markdown","2cd3648f":"markdown","35318d6f":"markdown","4f6667a3":"markdown","e3ca48b4":"markdown","2c7ef03d":"markdown","2b9d046e":"markdown","7386097f":"markdown","9bb8e071":"markdown"},"source":{"317ddb7e":"!pip install bert-for-tf2","05686f37":"import numpy as np\nimport math\nimport re\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nimport random\nimport tensorflow_hub as hub\nfrom tensorflow.keras import layers\nimport bert\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport plotly.express as px","35298589":"data = pd.read_csv('..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv', engine=\"python\",\n    encoding=\"latin1\")\ndata","14d77642":"data.drop(['UserName', 'ScreenName', 'Location','TweetAt'], axis = 1, inplace=True)\ndata","76df2d1b":"sns.heatmap(data.isnull());","10d3cd84":"data.Sentiment.unique()","2062c72a":"data['sentiment'] = data['Sentiment'].map({'Positive': 1,'Negative': -1,'Neutral': 0, 'Extremely Negative': -2, 'Extremely Positive' : 2},\n                             na_action=None)","2a42cb7a":"data","21580a87":"data.drop(['Sentiment'], axis = 1, inplace=True)\ndata","25736e83":"positive = data[data['sentiment'] >= 1 ]\nnegative = data[data['sentiment'] <= -1]\nneutral = data[data['sentiment'] == 0]","4887cb76":"plt.rcParams['figure.figsize'] = (10, 10)\nplt.style.use('fast')\n\nwc = WordCloud(background_color = 'orange', width = 1500, height = 1500).generate(str(positive['OriginalTweet']))\nplt.title('Description Positive', fontsize = 15)\n\nplt.imshow(wc)\nplt.axis('off')\nplt.show()","2ae37e72":"plt.rcParams['figure.figsize'] = (10, 10)\nplt.style.use('fast')\n\nwc = WordCloud(background_color = 'orange', width = 1500, height = 1500).generate(str(negative['OriginalTweet']))\nplt.title('Description Negative', fontsize = 15)\n\nplt.imshow(wc)\nplt.axis('off')\nplt.show()","8860e677":"plt.rcParams['figure.figsize'] = (10, 10)\nplt.style.use('fast')\n\nwc = WordCloud(background_color = 'orange', width = 1500, height = 1500).generate(str(neutral['OriginalTweet']))\nplt.title('Description Neutral', fontsize = 15)\n\nplt.imshow(wc)\nplt.axis('off')\nplt.show()","43933efb":"fig2 = px.histogram(data,x='sentiment',color='sentiment',template='plotly_dark')\nfig2.show()","b71ee959":"total = len(data)\nax1 = plt.figure(figsize=(12,5))\n\ng = sns.countplot(x='sentiment', data=data)\ng.set_title(\"Numbers in percentages\", fontsize=20)\ng.set_xlabel(\"Evaluation\", fontsize=17)\ng.set_ylabel(\"Values\", fontsize=17)\nsizes = []\nfor p in g.patches:\n    height = p.get_height()\n    sizes.append(height)\n    g.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/total*100),\n            ha=\"center\", fontsize=10) \ng.set_ylim(0, max(sizes) * 1.1)","bbb9520b":"data['sentiment'] = data['sentiment'].apply(lambda x: 1 if x >= 0 else 0) ","609b4eb9":"def clean_t(t):\n  t = BeautifulSoup(t, 'lxml').get_text()\n  t = re.sub(r\"@[A-Za-z0-9]+\", ' ', t)\n  t = re.sub(r\"https?:\/\/[A-Za-z0-9.\/]+\", ' ', t)\n  t = re.sub(r\"[^a-zA-Z.!?]\", ' ', t)\n  t = re.sub(r\" +\", ' ', t)\n  return t","147970c9":"data_clean = [clean_t(t) for t in data.OriginalTweet]","14620a26":"data_labels = data.iloc[:,1].values\ndata_labels","5bb826cc":"FullTokenizer = bert.bert_tokenization.FullTokenizer\nbert_layer = hub.KerasLayer('https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1', trainable=False)\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = FullTokenizer(vocab_file, do_lower_case)","0856cf49":"def encode_sentence(sent):\n  return [\"[CLS]\"] + tokenizer.tokenize(sent) + [\"[SEP]\"]","fdae897b":"encode_sentence(\"My dog likes strawberries.\")","017e5a69":"data_inputs = [encode_sentence(sentence) for sentence in data_clean]","2007cdb3":"print(data_inputs[0:2])","ab47b34f":"def get_ids(tokens):\n  return tokenizer.convert_tokens_to_ids(tokens)","89795cc4":"get_ids(tokenizer.tokenize(\"My dog likes strawberries.\"))","528da039":"np.char.not_equal(\"[PAD]\", \"[PAD]\")","8fef1a93":"def get_mask(tokens):\n  return np.char.not_equal(tokens, \"[PAD]\").astype(int)","79ea8c8f":"get_mask(tokenizer.tokenize(\"My dog likes strawberries.\"))","d4737e7a":"def get_segments(tokens):\n  seg_ids = []\n  current_seg_id = 0\n  for tok in tokens:\n    seg_ids.append(current_seg_id)\n    if tok == \"[SEP]\":\n      current_seg_id = 1 - current_seg_id\n  return seg_ids","4e534cee":"print(data_inputs[0])","1fc89d76":"get_segments(data_inputs[0])","7713fe46":"my_sent = [\"[CLS]\"] + tokenizer.tokenize(\"Roses are red.\") + [\"[SEP]\"]\nmy_sent","fb771f1a":"import tensorflow as tf","962fa35e":"bert_layer([\n            tf.expand_dims(tf.cast(get_ids(my_sent), tf.int32), 0),\n            tf.expand_dims(tf.cast(get_mask(my_sent), tf.int32), 0),\n            tf.expand_dims(tf.cast(get_segments(my_sent), tf.int32), 0) \n           ])","1f441b06":"data_with_len = [[sent, data_labels[i], len(sent)]\n                 for i, sent in enumerate(data_inputs)]\nrandom.shuffle(data_with_len)\ndata_with_len.sort(key = lambda x: x[2])\nsorted_all = [([get_ids(sent_lab[0]),\n               get_mask(sent_lab[0]),\n               get_segments(sent_lab[0])],\n              sent_lab[1])\n              for sent_lab in data_with_len if sent_lab[2] > 7]","9d6873b1":"all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,\n                                             output_types=(tf.int32, tf.int32))","24c4e747":"BATCH_SIZE = 32\nall_batched = all_dataset.padded_batch(BATCH_SIZE,\n                                       padded_shapes=((3, None), ()),\n                                       padding_values=(0, 0))","310071b7":"NB_BATCHES = len(sorted_all) \/\/ BATCH_SIZE\nNB_BATCHES_TEST = NB_BATCHES \/\/ 10\nall_batched.shuffle(NB_BATCHES)\ntest_dataset = all_batched.take(NB_BATCHES_TEST)\ntrain_dataset = all_batched.skip(NB_BATCHES_TEST)","60dddd1e":"class DCNNBERTEmbedding(tf.keras.Model):\n    \n    def __init__(self,\n                 nb_filters=50,\n                 FFN_units=512,\n                 nb_classes=2,\n                 dropout_rate=0.1,\n                 name=\"dcnn\"):\n        super(DCNNBERTEmbedding, self).__init__(name=name)\n        \n        self.bert_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1\",\n                                         trainable = False)\n\n        self.bigram = layers.Conv1D(filters=nb_filters,\n                                    kernel_size=2,\n                                    padding=\"valid\",\n                                    activation=\"relu\")\n        self.trigram = layers.Conv1D(filters=nb_filters,\n                                     kernel_size=3,\n                                     padding=\"valid\",\n                                     activation=\"relu\")\n        self.fourgram = layers.Conv1D(filters=nb_filters,\n                                      kernel_size=4,\n                                      padding=\"valid\",\n                                      activation=\"relu\")\n        self.pool = layers.GlobalMaxPool1D()\n        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n        self.dropout = layers.Dropout(rate=dropout_rate)\n        if nb_classes == 2:\n            self.last_dense = layers.Dense(units=1,\n                                           activation=\"sigmoid\")\n        else:\n            self.last_dense = layers.Dense(units=nb_classes,\n                                           activation=\"softmax\")\n    \n    def embed_with_bert(self, all_tokens):\n      _, embs = self.bert_layer([all_tokens[:, 0, :],\n                                 all_tokens[:, 1, :],\n                                 all_tokens[:, 2, :]])\n      return embs\n\n    def call(self, inputs, training):\n        x = self.embed_with_bert(inputs)\n        \n        x_1 = self.bigram(x)\n        x_1 = self.pool(x_1)\n        x_2 = self.trigram(x)\n        x_2 = self.pool(x_2)\n        x_3 = self.fourgram(x)\n        x_3 = self.pool(x_3)\n        \n        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)\n        merged = self.dense_1(merged)\n        merged = self.dropout(merged, training)\n        output = self.last_dense(merged)\n        \n        return output","eb22dcfa":"NB_FILTERS = 100\nFFN_UNITS = 256\nNB_CLASSES = 2\nDROPOUT_RATE = 0.2\nBATCH_SIZE = 32\nNB_EPOCHS = 5","06a4fdf8":"Dcnn = DCNNBERTEmbedding(nb_filters=NB_FILTERS,\n                         FFN_units=FFN_UNITS,\n                         nb_classes=NB_CLASSES,\n                         dropout_rate=DROPOUT_RATE)","ff34759f":"if NB_CLASSES == 2:\n    Dcnn.compile(loss=\"binary_crossentropy\",\n                 optimizer=\"adam\",\n                 metrics=[\"accuracy\"])\nelse:\n    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n                 optimizer=\"adam\",\n                 metrics=[\"sparse_categorical_accuracy\"])","0a0986c0":"history = Dcnn.fit(train_dataset,\n                   epochs=NB_EPOCHS)","63b53e18":"results = Dcnn.evaluate(test_dataset)\nprint(results)","fe3389b4":"def get_prediction(sentence):\n  tokens = encode_sentence(sentence)\n\n  input_ids = get_ids(tokens)\n  input_mask = get_mask(tokens)\n  segment_ids = get_segments(tokens)\n\n  inputs = tf.stack(\n      [\n       tf.cast(input_ids, dtype=tf.int32),\n       tf.cast(input_mask, dtype=tf.int32),\n       tf.cast(segment_ids, dtype=tf.int32),\n      ], axis = 0)\n  inputs = tf.expand_dims(inputs, 0)\n\n  output = Dcnn(inputs, training=False)\n\n  sentiment = math.floor(output*2)\n\n  if sentiment == 0:\n    print(\"Output of the model: {}\\nPredicted sentiment: negative\".format(output))\n  elif sentiment == 1:\n    print(\"Output of the model: {}\\nPredicted sentiment: positive\".format(output))","609dab6f":"get_prediction(\"This actor is very bad.\")","e08e98d7":"get_prediction(\"I love dogs.\")","53cf4299":"get_prediction(\"I hate you.\")","8a408e9f":"get_prediction(\"I'm good today.\")","2a1bb2e2":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['loss'])\nplt.title('Model accuracy and loss progress during training')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(['Accuracy', 'Loss'])","10e4b749":"**Numbers in quantities**","ef42d696":"# Database creation","2cd3648f":"As our focus will only be the texts so we will delete the attributes that will not be necessary for this study base.","35318d6f":"# Import from libraries","4f6667a3":"# Model building","e3ca48b4":"# **If you find this notebook useful, support with an upvote** \ud83d\udc4d","2c7ef03d":"# Processing","2b9d046e":"# Tokenization","7386097f":"https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge?select=metadata.csv\n","9bb8e071":"# Training"}}