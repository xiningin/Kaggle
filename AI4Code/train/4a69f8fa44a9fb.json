{"cell_type":{"fbaf3d24":"code","846c76f7":"code","4c2e5664":"code","0d5cc5c8":"code","8cbf41cc":"code","3fb2259a":"code","a8374b7f":"code","b0522bc9":"code","19868652":"code","6f3aa3e1":"code","a739b304":"code","c1457d9b":"code","9f827c88":"code","80e51f1d":"code","c63afc8c":"code","d162e12a":"code","6a550400":"code","94e70dce":"code","32b3d5c4":"code","5907b074":"code","1461f722":"code","6f562c58":"code","6b99653c":"code","b1b1d532":"code","bc321461":"code","c4158800":"code","5b7ac135":"code","93fa661c":"code","6c216a15":"code","83c57025":"code","65b230f5":"code","0c41f174":"code","7deabaa8":"code","877362c2":"code","e7cb0618":"code","1119fee3":"code","fdbea177":"code","27afe2fb":"code","c94937ef":"code","419becfe":"code","f2598515":"code","69acb611":"code","035a470c":"code","3236683e":"code","61f4d728":"markdown","259cd5b1":"markdown","5ae552c8":"markdown","bca37fc8":"markdown","d310c4fe":"markdown","4634a507":"markdown","e52badcc":"markdown","1243119f":"markdown","18c944c8":"markdown","aa34059d":"markdown","e4b14c94":"markdown","b7fe51e9":"markdown","0c967e4d":"markdown","e9a6f349":"markdown","eaba8f19":"markdown"},"source":{"fbaf3d24":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","846c76f7":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction import DictVectorizer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score , plot_roc_curve\n\nfrom joblib import dump,load\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline  ","4c2e5664":"df = pd.read_csv(\"\/kaggle\/input\/bank-marketing\/bank-additional-full.csv\",sep=';')","0d5cc5c8":"df.head()","8cbf41cc":"df.info()","3fb2259a":"# renaming columns\ndf.columns = df.columns.str.lower().str.replace('.', '_')","a8374b7f":"# matplotlib and seaborn setting\nplt.rcParams['figure.dpi'] = 150\nplt.rcParams['axes.spines.top'] = False\nplt.rcParams['axes.spines.right'] = False\n\nsns.set(rc={'figure.figsize':(8,8)})\nsns.set_theme(style=\"darkgrid\")\n# sns.set(font_scale=0.7)","b0522bc9":"fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n\ntarget_cnt = df['y'].value_counts().sort_index()\n\nax.bar(target_cnt.index, target_cnt, color=['#d4dddd' if i%2==0 else '#fafafa' for i in range(9)],\n       width=0.30, \n       edgecolor='black', \n       linewidth=0.5)\n\nax.margins(0.02, 0.05)\n\nfor i in range(2):\n    ax.annotate(f'{target_cnt[i]\/len(df)*100:.3}', xy=(i, target_cnt[i]+1000),\n                   va='center', ha='center',\n               )\n\nax.set_title('Target Distribution', weight='bold', fontsize=15)\nax.grid(axis='y', linestyle='-', alpha=0.4)\n\nfig.tight_layout()\nplt.show()","19868652":"df_numerical = df.select_dtypes(exclude=\"object\") \ndf_catgorical = df.select_dtypes(include=\"object\")","6f3aa3e1":"print(f'Number of Numerical features: {len(df_numerical.columns)}')\nprint(f'Number of Categorical features: {len(df_catgorical.columns)}')\n\nplt.pie([len(df_numerical.columns), len(df_catgorical.columns)], \n        labels=['Numerical', 'Categorical'],\n        colors=['#76D7C4', '#F5B7B1'],\n        textprops={'fontsize': 13},\n        autopct='%1.1f%%')\nplt.show()","a739b304":"df_numerical.describe().round().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')","c1457d9b":"discrete_features = []\n\nfor col in df_numerical.columns:\n    if np.array_equal(df_numerical[col].values, df_numerical[col].values.astype(int)):\n        discrete_features.append(col)\n\nprint(f'Total {len(discrete_features)} : ')\nprint(discrete_features)","9f827c88":"print(f'Unique values as follows:-')\nfor dcol in discrete_features:\n    print(f'{dcol}: {df_numerical[dcol].nunique()}')","80e51f1d":"fig = plt.figure(figsize = (20, 25))\nfor idx, i in enumerate(df_numerical.columns):\n    fig.add_subplot(4, 4, idx+1)\n#     sns.kdeplot(data = df_numerical.iloc[:, idx],\n#                fill = True)\n    df_numerical.iloc[:, idx].hist(bins=20,color='b',alpha=0.9)\n    plt.subplots_adjust(hspace=0.2)\n    plt.title(i)\nplt.show()","c63afc8c":"# pdays feature\n# clearly 999 value counts for 96% of total distribution, \n# dataset notes suggested 999 means 0,replacing 999 with 0.\n\nprint(df_numerical.pdays.value_counts())\ndf_numerical.pdays.value_counts().plot(kind=\"bar\")","d162e12a":"df_numerical['pdays'] = df_numerical['pdays'].replace(999,0)\ndf_numerical.pdays.value_counts()","6a550400":"cor_matrix = df_numerical.corr().abs()\nsns.heatmap(cor_matrix , annot= True,annot_kws={\"size\": 7})","94e70dce":"upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(np.bool))\nsns.heatmap(upper_tri , annot= True,annot_kws={\"size\": 7})","32b3d5c4":"to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.90)]\nprint(to_drop)","5907b074":"# Clealry nr_employed is highly correlated with euribo3m and emp_var_rate\n# trying to drop euribor3m and emp_var_rate\ndf_numerical = df_numerical.drop(['euribor3m','nr_employed'],axis = 1)\ndf_numerical","1461f722":"# we need to remove duration feature as mentioned in the above notes because it's highly correlated with the target.\ndf_numerical = df_numerical.drop('duration', axis = 1)\ndf_numerical","6f562c58":"fig = plt.figure(figsize = (50, 60))\nfor idx, i in enumerate(df_catgorical.columns):\n    fig.add_subplot(5, 3, idx+1)\n    sns.countplot(x = df_catgorical.iloc[:, idx],)\n    plt.subplots_adjust(hspace=0.6)\n    locs, labels = plt.xticks(fontsize=25)\n    plt.setp(labels, rotation=90)\n    plt.title(i,fontsize=25)\nplt.show()","6b99653c":"target = (df.y == 'yes').astype(int)\ndf_catgorical.drop('y',axis = 1 , inplace = True)","b1b1d532":"df_numerical.corrwith(target).to_frame('correlation')","bc321461":"# combine the numerical and catgorical features first before one hot encoding\n\ndf_combined = pd.concat([df_numerical,df_catgorical],axis = 1)\ndf_combined","c4158800":"# one encoding using DictVectorizer\ndf_dict = df_combined.to_dict(orient=\"records\")","5b7ac135":"dv = DictVectorizer(sparse=False)\ndf_combined_encoded = pd.DataFrame(dv.fit_transform(df_dict))\ndf_combined_encoded.columns = dv.get_feature_names()\ndf_combined_encoded","93fa661c":"X, X_test, y, y_test = train_test_split(df_combined_encoded, target, test_size=0.20,stratify = target,random_state=42)","6c216a15":"clf = RandomForestClassifier(n_estimators=40, min_samples_leaf=3, max_features=0.5,\n                          n_jobs=-1, oob_score=True)\n   \nclf.fit(X, y)\npreds = clf.predict_proba(X_test)[:,1]\nroc_auc_score(y_test, preds)\n","83c57025":"def rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)\n\nfi = rf_feat_importance(clf, X)\nfi[:40]","65b230f5":"def plot_fi(fi):\n    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\nplot_fi(fi[:40]);","0c41f174":"to_keep = fi[fi.imp>0.0050].cols;\nlen(to_keep)","7deabaa8":"clf = RandomForestClassifier(n_estimators=40, min_samples_leaf=3, max_features=0.5,\n                          n_jobs=-1, oob_score=True)\n   \nclf.fit(X[to_keep], y)\npredicitions = clf.predict_proba(X_test[to_keep])[:,1]\nroc_auc_score(y_test,predicitions)","877362c2":"import scipy\nfrom scipy.cluster import hierarchy as hc\n\ndef cluster_columns(df, figsize=(20,12), font_size=12):\n    corr = np.round(scipy.stats.spearmanr(df).correlation, 4)\n    corr_condensed = hc.distance.squareform(1-corr)\n    z = hc.linkage(corr_condensed, method='average')\n    fig = plt.figure(figsize=figsize)\n    hc.dendrogram(z, labels=df.columns, orientation='left', leaf_font_size=font_size)\n    plt.show()\n\ncluster_columns(X[to_keep])","e7cb0618":"from sklearn.model_selection import RandomizedSearchCV , GridSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(start = 1, stop = 30, num = 15)]\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2,3,5,7,9,10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","1119fee3":"# # Use the random grid to search for best hyperparameters\n# # First create the base model to tune\n# rf = RandomForestClassifier()\n# # Random search of parameters, using 3 fold cross validation, \n# # search across 100 different combinations, and use all available cores\n# rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=5,scoring = \"roc_auc\" ,random_state=42, n_jobs = -1)\n# # Fit the random search model\n# rf_random.fit(X,y)","fdbea177":"# rf_random.best_params_","27afe2fb":"# # Create the parameter grid based on the results of random search \n# param_grid = {\n#     'bootstrap': [False],\n#     'max_depth': [10,11,12],\n#     'max_features': ['auto'],\n#     'min_samples_leaf': [1,2,3],\n#     'min_samples_split': [2,3,4],\n#     'n_estimators': [100,200,300]\n# }\n\n# # Create a based model\n# rf = RandomForestClassifier()\n# # Instantiate the grid search model\n# grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n#                           cv = 3, n_jobs = -1, verbose = 2,scoring='roc_auc')\n# grid_search.fit(X[to_keep],y)","c94937ef":"# grid_search.best_params_","419becfe":"params = {\n    'bootstrap': False,\n     'max_depth': 10,\n     'max_features': 'auto',\n     'min_samples_leaf': 2,\n     'min_samples_split': 3,\n     'n_estimators': 200\n}","f2598515":"folds = StratifiedKFold(n_splits = 5, random_state = 228, shuffle = True)\nauc_score = 0\noverall_auc = 0\n\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X,y)):\n    \n    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n    classifier = RandomForestClassifier(**params)\n   \n    classifier.fit(X_train, y_train)\n    preds = classifier.predict_proba(X_val)[:,1]\n   \n    auc_score = roc_auc_score(y_val, preds)\n    print(f\"The AUC of {fold + 1} fold is {auc_score}\")\n    overall_auc += auc_score \n\nprint(f\"The over all AUC is {overall_auc \/ 5}\")","69acb611":"predicitions = classifier.predict_proba(X_test)[:,1]\nroc_auc_score(y_test,predicitions)","035a470c":"rfc_disp = plot_roc_curve(classifier, X_test, y_test)\nrfc_disp.figure_.suptitle(\"ROC curve\")\n\nplt.show()","3236683e":"sns.displot(y_test-predicitions , kde = True)","61f4d728":"### Target Variable","259cd5b1":"### Target Distribution","5ae552c8":"A total of 5 features have no decimal point.\n* Age\n* Duration\n* Campaign\n* Pdays\n* Previous","bca37fc8":"### Modeling","d310c4fe":"### Statictics","4634a507":"### Numerical Feature Distribution","e52badcc":"# About the Dataset\nThe data is related with direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict if the client will subscribe a term deposit (variable y). The following website provides more details of the dataset used in this notebook.\n\nhttp:\/\/archive.ics.uci.edu\/ml\/datasets\/Bank+Marketing\n\n# Deployment\nThe web app has been deployed at the following address, which return the probabilities of whether a client will deposit in the bank.\nhttps:\/\/huggingface.co\/spaces\/mohramzan\/bank_marketing","1243119f":"### Tuning Hyperparameters","18c944c8":"### Categorical Feature Distribution","aa34059d":"### Data Types","e4b14c94":"### Feature Importance","b7fe51e9":"### One Hot Encoding","0c967e4d":"* The sclae of data is diverse. And data needs to be normalised\/scaled depending on the model, tree based models tends to to handle outliers nicely.\n*  Mix data of both int and floating type.\n* The range of data is diverse\n* pdays feature needs to be looked into detail. 999 needs replacing with 0 as dataset notes suggests.\n* duration will be dropped as per dataset notes because of predictive influence","e9a6f349":"* Clealy imbalanced data here where 'no' account for 88.7% whole 'yes' only for 11.3.","eaba8f19":"### Discrete Features\n\nAs above we found data some data of integer type, let's try to dig in a bit."}}