{"cell_type":{"8c9bce3c":"code","b79112bf":"code","dab568b2":"code","0dd106da":"code","2570c3bd":"code","c198b54f":"code","64c98e7b":"code","68b38e30":"code","91010b74":"code","be6cbcb0":"code","e5c20381":"markdown","ade670ba":"markdown"},"source":{"8c9bce3c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.metrics import f1_score, roc_auc_score\n\nimport gc\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom scipy.sparse import hstack\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import KFold\nfrom scipy.sparse import hstack\nfrom scipy.sparse import coo_matrix\nfrom tqdm import tqdm\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b79112bf":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\ntrain.head()","dab568b2":"train['target'] = 0\ntest['target'] = 1","0dd106da":"train_test = pd.concat([train, test], axis =0)","2570c3bd":"train_test.tail()","c198b54f":"target = train_test['target'].values","64c98e7b":"target = train_test['target'].values\n\ntext = train_test['question_text']\n\n\ndel train, test, train_test\ngc.collect()\n\n\nword_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    stop_words='english',\n    ngram_range=(1, 1),\n    max_features=8000)\nword_vectorizer.fit(text)\nword_features = word_vectorizer.transform(text)\n\ndel word_vectorizer\ngc.collect()\n","68b38e30":"kf = KFold(n_splits=5, shuffle=True, random_state=43)\noof_pred = np.zeros([target.shape[0],])\n\nfor i, (train_index, val_index) in tqdm(enumerate(kf.split(target))):\n    x_train, x_val = word_features[list(train_index)], word_features[list(val_index)]\n    y_train, y_val = target[train_index], target[val_index]\n    classifier = LogisticRegression(C=5, solver='sag')\n    classifier.fit(x_train, y_train)\n    val_preds = classifier.predict_proba(x_val)[:,1]\n    oof_pred[val_index] = val_preds\n    print(f1_score(y_val, val_preds > 0.1))\n","91010b74":"score = 0\nthresh = .5\nfor i in np.arange(0.1, 1.001, 0.01):\n    temp_score = f1_score(target, (oof_pred > i))\n    if(temp_score > score):\n        score = temp_score\n        thresh = i\n\nprint(\"CV: {}, Threshold: {}\".format(score, thresh))","be6cbcb0":"roc_auc_score(target, oof_pred)","e5c20381":"So with F1 score at about 0.01 and AUC at almost 0.5, it would seem that the two distributions are pretty similar, at least as far as can be determined by the distribution of individual words.","ade670ba":"One of the most important things to establish for every Kaggle competition is whether there is a significatn differnece in distributions of the train and test sets. So far the CV validation scores for kernels and for public LB have been pretty close, suggesting that the two distributions are pretty similar. However, it would be interesting, and potentially very valuable, to find out in a more quantitative and specific way how do these distributions compare. For that purpose we'll build an adverserial validation scheme - we'll run a CV classifier that tries to predict if any given question belongs to the train or the test set. "}}