{"cell_type":{"b5ae7d00":"code","26aa73d0":"code","c5cd7733":"code","f8f30a31":"code","c39e0f6b":"code","f3cae584":"code","428556cb":"code","4b7f5697":"code","dfa94a87":"code","f8da1c41":"code","12927e67":"code","1a89dab3":"code","bde496c1":"code","f21f773d":"code","523e6f93":"code","48d47d22":"code","4d1d071e":"code","be919170":"code","2c252ea8":"code","cf71a35b":"code","01edb6f4":"code","92cc93b6":"code","c327ff42":"code","e337e8b6":"code","ff745adb":"code","db1febbc":"code","cda02a30":"code","477cdd7a":"code","1019a1e9":"code","edc73633":"code","9ee0aae3":"code","3f988173":"code","c3ff1fb2":"code","a8743f26":"markdown","506b3135":"markdown","3e103c71":"markdown","4b0ea1d6":"markdown","4e0e2e6b":"markdown","ef5b90a5":"markdown","34e0817b":"markdown","5de13bcf":"markdown","48e494d0":"markdown","3d070212":"markdown","2c3181ab":"markdown","a893b604":"markdown","3791798e":"markdown","dfff9c82":"markdown","1850827d":"markdown","60abb879":"markdown","a6a59b16":"markdown","ecf02d98":"markdown"},"source":{"b5ae7d00":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","26aa73d0":"df = pd.read_csv('..\/input\/customer-segmentation-tutorial-in-python\/Mall_Customers.csv')\ndf.head()","c5cd7733":"df.isnull().sum()","f8f30a31":"df.describe()","c39e0f6b":"df.info()","f3cae584":"df.drop('CustomerID', axis =1, inplace=True)\ndf.head()","428556cb":"sns.countplot(x= df.Gender);","4b7f5697":"sns.histplot(df.Age, kde=True);","dfa94a87":"df.rename({'Annual Income (k$)': 'Income'}, axis =1, inplace = True)","f8da1c41":"sns.histplot(df.Income, kde=True);","12927e67":"sns.scatterplot(y= df.Income, x= df.Age, hue= df.Gender);","1a89dab3":"df.rename({'Spending Score (1-100)': 'Score'}, axis =1, inplace = True)","bde496c1":"sns.histplot(df.Score, kde=True);","f21f773d":"sns.scatterplot(y= df.Income, x= df.Score);","523e6f93":"sns.scatterplot(y= df.Age, x= df.Score);","48d47d22":"sns.scatterplot(y= df.Income, x= df.Score);","4d1d071e":"X= df[['Income', 'Score']].values","be919170":"from sklearn.cluster import KMeans\n\nkm5 = KMeans(n_clusters= 5, init='k-means++', random_state=0)\ny_pred = km5.fit_predict(X)","2c252ea8":"#Visualizing all the clusters \nplt.figure(figsize=(10,10))\nplt.scatter(X[y_pred == 0, 0], X[y_pred == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\nplt.scatter(X[y_pred == 1, 0], X[y_pred == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\nplt.scatter(X[y_pred == 2, 0], X[y_pred == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\nplt.scatter(X[y_pred == 3, 0], X[y_pred == 3, 1], s = 100, c = 'yellow', label = 'Cluster 4')\nplt.scatter(X[y_pred == 4, 0], X[y_pred == 4, 1], s = 100, c = 'brown', label = 'Cluster 5')\n\nplt.title('Clusters of customers')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nplt.legend()\nplt.show()","cf71a35b":"# Within-cluster Sum of Squares (Inertia)\ninertia=[]\nk_range= range(1,11)\n\nfor i in k_range:\n    kmeans = KMeans(n_clusters= i, init='k-means++', random_state=0)\n    kmeans.fit(X)\n    inertia.append(kmeans.inertia_)\n\n#Visualizing the ELBOW method to get the optimal value of K \nfig, ax = plt.subplots(figsize=(12, 8))\nsns.lineplot(x=k_range, y=inertia, ax=ax)\nplt.title('The Elbow Method')\nplt.xlabel('No of clusters \"k\"')\nplt.ylabel('Inertia')\n\n# Annotate arrow\nax.annotate('Possible Elbow Point', xy=(3, 105000), xytext=(4, 150000), xycoords='data',          \n             arrowprops=dict(arrowstyle='->', connectionstyle='arc3', color='blue', lw=2))\n\nax.annotate('Possible Elbow Point', xy=(5, 46000), xytext=(5, 80000), xycoords='data',          \n             arrowprops=dict(arrowstyle='->', connectionstyle='arc3', color='blue', lw=2))\n\nplt.show()\n","01edb6f4":"from sklearn.cluster import KMeans\n\nkm3 = KMeans(n_clusters= 3, init='k-means++', random_state=0)\ny_pred = km3.fit_predict(X)","92cc93b6":"#Visualizing all the clusters \nplt.figure(figsize=(10,10))\nplt.scatter(X[y_pred == 0, 0], X[y_pred == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\nplt.scatter(X[y_pred == 1, 0], X[y_pred == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\nplt.scatter(X[y_pred == 2, 0], X[y_pred == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\n\nplt.title('Clusters of customers')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nplt.legend()\nplt.show()","c327ff42":"X= pd.get_dummies(df, drop_first= True)\nX","e337e8b6":"from sklearn.cluster import KMeans\n\nkm5 = KMeans(n_clusters= 5, init='k-means++', random_state=0)\ny_pred = km5.fit_predict(X.values)","ff745adb":"X","db1febbc":"X= X.values","cda02a30":"#Visualizing all the clusters \nplt.figure(figsize=(10,10))\nplt.scatter(X[y_pred == 0, 1], X[y_pred == 0, 2], s = 100, c = 'red', label = 'Cluster 1')\nplt.scatter(X[y_pred == 1, 1], X[y_pred == 1, 2], s = 100, c = 'blue', label = 'Cluster 2')\nplt.scatter(X[y_pred == 2, 1], X[y_pred == 2, 2], s = 100, c = 'green', label = 'Cluster 3')\nplt.scatter(X[y_pred == 3, 1], X[y_pred == 3, 2], s = 100, c = 'yellow', label = 'Cluster 4')\nplt.scatter(X[y_pred == 4, 1], X[y_pred == 4, 2], s = 100, c = 'brown', label = 'Cluster 5')\n\nplt.title('Clusters of customers')\nplt.xlabel('Income')\nplt.ylabel('Score')\nplt.legend()\nplt.show()","477cdd7a":"X= df[['Income', 'Score']].values","1019a1e9":"import scipy.cluster.hierarchy as sch\n\nplt.figure(figsize=(20,10))\ndendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\n\nplt.title('Dendrogram')\nplt.xlabel('Customers')\nplt.ylabel('Euclidean distances')\nplt.show()","edc73633":"from sklearn.cluster import AgglomerativeClustering\nhc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'ward')\ny_hc = hc.fit_predict(X)","9ee0aae3":"plt.figure(figsize=(10,10))\n\nplt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\nplt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\nplt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\nplt.scatter(X[y_hc == 3, 0], X[y_hc == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')\nplt.scatter(X[y_hc == 4, 0], X[y_hc == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')\nplt.title('Clusters of customers')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nplt.legend()\nplt.show()","3f988173":"def clustering(X= df[['Income', 'Score']].values):\n    #Kmeans\n    from sklearn.cluster import KMeans\n    km5 = KMeans(n_clusters= 5, init='k-means++', random_state=0)\n    y_pred = km5.fit_predict(X)\n    \n    #Visualizing all the clusters \n    fig= plt.figure(figsize=(20,15))\n    fig.add_subplot(221)\n    plt.scatter(X[y_pred == 0, 0], X[y_pred == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\n    plt.scatter(X[y_pred == 1, 0], X[y_pred == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\n    plt.scatter(X[y_pred == 2, 0], X[y_pred == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\n    plt.scatter(X[y_pred == 3, 0], X[y_pred == 3, 1], s = 100, c = 'yellow', label = 'Cluster 4')\n    plt.scatter(X[y_pred == 4, 0], X[y_pred == 4, 1], s = 100, c = 'brown', label = 'Cluster 5')\n\n    plt.title('Kmeans Clustering of customers')\n    plt.xlabel('Income')\n    plt.ylabel('Score')\n    plt.legend()\n    \n    #Agglomerative\n    from sklearn.cluster import AgglomerativeClustering\n    hc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'ward')\n    y_hc = hc.fit_predict(X)\n   \n    fig.add_subplot(222)\n    plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\n    plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\n    plt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\n    plt.scatter(X[y_hc == 3, 0], X[y_hc == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')\n    plt.scatter(X[y_hc == 4, 0], X[y_hc == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')\n    plt.title('Agglomerative Clusters of customers')\n    plt.xlabel('Income')\n    plt.ylabel('Score')\n    plt.legend()\n    plt.show()","c3ff1fb2":"clustering()","a8743f26":"# Wrap Up All in One Place\n","506b3135":"## Try all features","3e103c71":"## CustomerID","4b0ea1d6":"### Dendrogram also shows possible 3 or 5 clusters","4e0e2e6b":"**Considering only those two features, we can build our first**","ef5b90a5":"## Gender","34e0817b":"## Using the dendrogram to find the optimal number of clusters","5de13bcf":"## Annual Income (k$)","48e494d0":"**We can try different number of clusters to find the optimum number of clusters using Elbow Method**","3d070212":"### Try 3 Clusters","2c3181ab":"## Elbow Method","a893b604":"# Clustering using K-means","3791798e":"### As we deal with Unsupervised Learning, we have no hard metric to judge the performance and choosing the best no. of clusters, but we may  have two good options (3 or 5 clusters) and the business expert can choose the most suitable for this case study","dfff9c82":"## Spending Score (1-100)","1850827d":"# EDA","60abb879":"## Age","a6a59b16":"This is the most informative visualization till now, as we can observe about 5 clusters:\n1. Low Score, Low Income\n2. Low Score, High Income\n3. Mid Score, Mid Income\n4. High Score, Low Income\n5. High Score, High Income","ecf02d98":"# Clustering using Hierarchical Clustering"}}