{"cell_type":{"c52a797e":"code","e1d54eac":"code","a1d850a5":"code","69dce4dc":"code","f58db8b0":"code","9d14aec2":"code","7026e80d":"code","2a50223b":"code","4e56c8bb":"code","d62cee77":"code","fa38683f":"code","8b0cb086":"code","87c43be5":"code","8328d686":"code","0dbe2270":"code","36975f99":"code","faa6627b":"code","a8f176b6":"code","d283c036":"code","fad48c40":"markdown","36885dda":"markdown","771e60e2":"markdown","663475d6":"markdown","5caf3ef0":"markdown","77c22a4d":"markdown","e1956612":"markdown","f4e3ccd0":"markdown","380bc8bc":"markdown","814e4afd":"markdown","c7acf8d3":"markdown","aefa929f":"markdown","001dfea9":"markdown","eeec558e":"markdown","e06cb1c9":"markdown","6607fa6e":"markdown","81e85087":"markdown","73582c83":"markdown","69d7bd98":"markdown","b5f2d2bf":"markdown","6473e03f":"markdown","e454e4c4":"markdown","7e2568c8":"markdown","7143f7f9":"markdown","d1f88b6f":"markdown","b86405db":"markdown","c950c16c":"markdown","29e08525":"markdown","f981ea9e":"markdown","47089c59":"markdown","a9866388":"markdown","7fe141e1":"markdown","300381b3":"markdown","000f01a9":"markdown","4137c072":"markdown","f3b188c5":"markdown","d3a4fff5":"markdown","890f0778":"markdown","08d9e3f2":"markdown","d370d4c7":"markdown","25dafd3c":"markdown","d941e27e":"markdown","6f1dbbc0":"markdown","5d44d709":"markdown","67b129eb":"markdown","e95a8206":"markdown","bf66441e":"markdown","8ed6a0b0":"markdown","a44bdd0a":"markdown","5ffb227d":"markdown","29c66059":"markdown","5a9767cd":"markdown","402de4ad":"markdown"},"source":{"c52a797e":"# import python modules\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nimport time","e1d54eac":"def linear(X, b0, b1):\n    return [b0+b1*x for x in X]","a1d850a5":"# b0 - Intercept\ndef intercept(X, Y, b1): \n    x_ = np.mean(X)\n    y_ = np.mean(Y)\n    \n    return y_-b1*x_","69dce4dc":"# b1 - Slope\ndef slope(X, Y):\n    x_ = np.mean(X)\n    y_ = np.mean(Y)\n    \n    rise = sum([(x-x_) * (y-y_) for x,y in zip(X,Y)])\n    run = sum([(x-x_)**2 for x,y in zip(X,Y)])\n    \n    return rise \/ run","f58db8b0":"data = pd.read_csv(\"..\/input\/Automobile_data.csv\")\ndata.head()","9d14aec2":"print(\"Dataset size\")\nprint(\"Rows {} Columns {}\".format(data.shape[0], data.shape[1]))","7026e80d":"print(\"Columns and data types\")\npd.DataFrame(data.dtypes).rename(columns = {0:'dtype'})","2a50223b":"try:\n    data[['price']] = data[['price']].astype(int)\nexcept ValueError:\n    print(\"Trying out the line of code above will result to this error:\\n\")\n    print(\"Value Error: invalid literal for int() with base 10: '?'\")","4e56c8bb":"data['price'].value_counts()[:5]","d62cee77":"data = data.loc[data['price']!='?']\ndata[['price']] = data[['price']].astype(int)","fa38683f":"plt.figure(figsize=(10,8))\nsns.heatmap(data.corr(), annot=True)\nplt.show()","8b0cb086":"final_data = data[['engine-size', 'price']]\npredictor = data['engine-size']\ntarget = data['price']","87c43be5":"plt.figure(figsize=(8,5))\nplt.title(\"Price vs engine-size\")\nplt.scatter(predictor, target, color = \"#247ba0\")\nplt.xlabel('engine-size')\nplt.ylabel('price')\nplt.show()","8328d686":"b1 = slope(predictor, target)\nb0 = intercept(predictor, target, b1)\npredicted = linear(predictor, b0, b1)\n# print(predicted) - Uncomment to see predicted values","0dbe2270":"plt.figure(figsize = (8, 5))\nplt.plot(predictor, predicted, color = '#f25f5c')\nplt.scatter(predictor, predicted, color = '#f25f5c')\nplt.title('Predicted values by Linear Regression', fontsize = 15)\nplt.xlabel('engine-size')\nplt.ylabel('price')\nplt.scatter(predictor, target, color = \"#247ba0\")\nplt.show()","36975f99":"print(\"Coefficients:\\n=============\")\nprint(\"b0 : \", b0)\nprint(\"b1 : \", b1)","faa6627b":"def r_squared(Y, Y_HAT):\n    ssr, sse, r_sqr = [0]*3\n    y_ = np.mean(Y)\n#     ssr = sum([(y_hat - y_)**2 for y_hat in Y_HAT])\n    sse = sum([(y - y_hat)**2 for y,y_hat in zip(Y, Y_HAT)])\n    sst = sum([(y - y_)**2 for y in Y])\n    \n    r_sqr = 1 - (sse \/ sst)\n    \n    return r_sqr\n    ","a8f176b6":"r_squared(target, predicted)","d283c036":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nreg = LinearRegression()\npredictor = np.array(predictor).reshape((-1, 1))\nreg = reg.fit(predictor, target)\n\nY_pred = reg.predict(predictor)\nr2_score = reg.score(predictor, target)\nprint(r2_score)","fad48c40":"In this kernel, we've created the simplest Linear Regression model you could ever imagine, using Python. We've also understood the mathematics that is happening behind our model. Using one predictor variable and target variable we've implemented our Linear Regression model in a simple analytics use case, that is predicting the price of a car based on it's engine size. Evaluation shows that our linear regression performs exactly the same as sklearn's Linear Regression model, which is nice.","36885dda":"After prediction a data scientist should always show how was the performance of the model using different evaluation metrics. One of the most important evaluation metric for linear regression is the coefficient of determination also known as R-squared value ($ R^2 $). The $ R^2 $ is basically a statistical measure of how close the data are to the fitted regression line, by \"data\" it means our independent variable data, the **engine-size** data. Usually, the larger the $R^2$ value is, the better the model fits the data.","771e60e2":"Where:\n - $ SS_{regression} $ - \"regression sum of squares\"; quantifies how far the predicted value is from the mean\n - $ SS_{error} $ - \"error sum of squares\"; quantifies the variance of the target data points around the regression line\n - $ SS_{total} $ - \"total sum of squares\"; quantifies the variance of the target data points around the mean","663475d6":"To wrap things up let us look back to the objectives we've set at the beggining of this kernel and see if we've accomplished them. The first objective was to understand the theory behind Linear Regression. A couple of mathematical equations here and there, hopefully you were able to learn those mathematical and statistical mumbo-jumbo I have discussed in the section **Linear Regression demistyfied**. The second objective was to create a Simple Linear Regression model without using built-in ML libraries. Using python and some helper libraries (pandas and numpy) we were able to accomplish our second objective in the section **Linear Regression, up and running**. In the same section, we've also implemented our model to a simple analytics use case which answers our third objective.\n","5caf3ef0":"\\begin{align}\nSS_{total} = \\sum_{i=1}^m(y_i - \\bar y)^2\n\\end{align}","77c22a4d":"\\begin{align}\nSS_{error} = \\sum_{i=1}^m(y_i - \\hat y)^2\n\\end{align}","e1956612":"#### Evaluation","f4e3ccd0":"76% not bad for a LR model from scratch.","380bc8bc":"$ \\beta_{0} $ and $ \\beta_{1} $ are estimated from the training data. The $ \\beta_{1} $ is called a scale factor or coefficient and $ \\beta_{0} $ is called bias coefficient. They are tweaked to find the best fitting line between our variables. We will estimate these coefficients using a method called the<a src = \"https:\/\/www.wikiwand.com\/en\/Ordinary_least_squares\"> Ordinary Least Squares.<\/a><br>\nIts definition are as follows:","814e4afd":"**engine-size** has the highest correlation with **price** let us use that column.","c7acf8d3":"First things is to know how big of a data we're going to deal with.","aefa929f":"That's a lot of columns, since we are using Linear Regression which can only deal with TWO variables, as defined above, we will only select two columns: **price** and a column that we will know later on.","001dfea9":"#### Ordinary Least Squares","eeec558e":"\\begin{align}\n\\beta_{1} = \\frac{\\sum rise}{\\sum run}\n\\end{align}","e06cb1c9":"In python code:","6607fa6e":"\\begin{align}\nr^2 = 1 - \\frac{SS_{error}}{SS_{total}}\n\\end{align}","81e85087":"Where \n - Y is the dependent\/outcome\/response variable\n - $ \\beta_{0} $ is the Y intercept\n - $ \\beta_{1} $ is the slope intercept","73582c83":" - https:\/\/medium.com\/data-science-group-iitr\/linear-regression-back-to-basics-e4819829d78b\n - http:\/\/faculty.cas.usf.edu\/mbrannick\/regression\/Reg2IV.html\n - https:\/\/mubaris.com\/posts\/linear-regression\/","69d7bd98":"### Linear Regression, up and running","b5f2d2bf":"### Objectives\nThrougout this whole \"tutorial\" you\/we should be able to achieve the following:\n - To understand the theory behind Linear Regression (Mathematical part)\n - To be able to create a Simple Linear Regression model without using built-in Machine Learning libraries.\n - To implement our Linear Regression model for predicting the price of a car in our automobile dataset (Analytics use case)","6473e03f":"Eureka! the error was cause by a '?' value in the **price** column. To simplify things we will just remove those rows to prevent further inconvenience.","e454e4c4":"## Introduction : Linear Regression","7e2568c8":"To simplify the slope equation above:","7143f7f9":"\\begin{align}\nr^2 = \\frac{SS_{regression}}{SS_{total}}\n\\end{align}","d1f88b6f":"In python code:","b86405db":"### Prerequisites\nBefore going any further I am assuming that you have atleast the very basic knowledge regarding the following items, if not then you might want to read a little.\n1. Python programming\n2. Basic Algebra\n3. Basic Statistics (Statiscial terms, e.g., Independent variable, Dependent, etc.)\n\n","c950c16c":"Still there? You're doing great. We have now created our Linear Regression model the next section will go step-by-step on how to apply our newly created model into a real dataset.","29e08525":"Before we get our hands dirty on coding our Linear Regression let us first get to know the underlying operations in a Linear Regression. The following cells are aiming to explain the mathematics behind this legendary algorithm for us to accomplish our very first objective (stated above). So, how does it work? Basically, Linear regression is a method used to define, to measure a relationship between a dependent variable (Y) and independent variable (X). Linear Regression predicts a target variable by estimating a line that best fits the linear relationship between X and Y<br> **Note: A dependent variable is sometimes called outcome variable, sometimes response variable**<br>\nLinear equation is defined as below:","f981ea9e":"### Linear Regression demystified","47089c59":"## Summary and Conclusion","a9866388":"Do you notice something odd about the column and the datatypes of our data? Because I do. Any guesses?<br>\nThat's right the **price** column has a datatype of **object**. We dont want that, we need it to be either int or double\/float. This is the \"anomalies\" I was telling earlier. How to deal with this anomaly? that's easy let's just convert the **price** column into int and see what happens.","7fe141e1":"$ R^2 $ is defined as :","300381b3":"#### Prediction","000f01a9":"This linear regression implementation would probably be never used in production and it is unlikely that it will defeat sklearn's own LinearRegression module, however the goal of this kernel was to understand intrecately the structure of different algorithms, in this case, Linear Regression. This kernel helps me as much as it helps you. If you liked this kernel please leave an upvote, and please, do check-out my other \"from scratch\" kernels. thank you .\n - https:\/\/www.kaggle.com\/jeppbautista\/logistic-regression-from-scratch-python\n - https:\/\/www.kaggle.com\/jeppbautista\/naive-bayes-classifier-from-scratch","4137c072":"\\begin{align}\n\\beta_{1} = \\frac{\\sum_{i=1}^n(x_{i} - \\bar x)(y_{i} - \\bar y)}{\\sum_{i=1}^n(x_{i} - \\bar x)^2}\n\\end{align}\n<center>Co-efficient (Slope)<\/center>","f3b188c5":"![](https:\/\/www.biomedware.com\/files\/documentation\/spacestat\/Statistics\/Multivariate_Modeling\/Regression\/regression_line.png)","d3a4fff5":"Linear Regression is the oldest, most simple, and widely used supervised machine learning algorithm for analysis. This is basically the starting point of every aspiring data scientist (that includes yours truly). In technical terms, LR is a linear approach to modelling the relationship between a dependent variable and one independent variables. ","890f0778":"<center><h1>Linear Regression from Scratch using Python<\/h1><\/center><br>\n<center>by Jepp Bautista<\/center>","08d9e3f2":"Above is our data points. We will now fit our linear regression line in those data points","d370d4c7":"## References","25dafd3c":"Let's now decide which variable to use as our independent variable. One easy thing to do is by correlation. The most correlated variable to **price** will be our independent variable.","d941e27e":"For comparison I will implement linear regression using python's scikit-learn library.","6f1dbbc0":"\\begin{align}\nSS_{regression} = \\sum_{i=1}^m(\\hat y_i - \\bar y)^2\n\\end{align}","5d44d709":"#### Exploratory data analysis","67b129eb":"Where \n - $ \\bar y $ is the mean of the dependent\/outcome\/response variable\n - $ \\beta_{1} $ is the slope intercept (Defined below)\n - $ \\bar x $ is the mean of the independent variable","e95a8206":"We will use the <a src=\"https:\/\/www.kaggle.com\/toramky\/automobile-dataset\">Automobile dataset<\/a> to test out our Linear Regression model. For this particular use case we will predict the price column in our dataset. Then later on we will evaluate the performance of our linear regression. Hopefully it will not suck that much. First thing we have to do is explore the data that we have to find some patterns or some anomalies in our data so that we can apply appropriate steps to handle it but we will not be dwelling to much on data exploration, that is not our goal for today.","bf66441e":"In python code:","8ed6a0b0":"The red line above is our predicted values, it is the \"line that best fits the data\". Our linear regression prediction is done the next thing we're going to do is to evaluate how well does this regression line fits our data. ","a44bdd0a":"Same $r^2$","5ffb227d":"In this notebook I will try to implement Linear Regression for prediction without relying to Python's easy-to-use scikit-learn library. This notebook aims to create a Linear Regression model without the help of in-built Linear Regression libraries to help us fully understand how it works behind the scene. \n<br>**Beware: Mathematical mumbo-jumbos are present in this notebook**","29c66059":"$$\nY = \\beta_{0} + \\beta_{1}X \n$$","5a9767cd":"\\begin{align}\n\\beta_{0} = \\bar y - \\beta_{1}\\bar x\n\\end{align}\n<center>Intercept<\/center>\n<br>","402de4ad":"Look's like it's not that easy at all. Let's investigate even further"}}