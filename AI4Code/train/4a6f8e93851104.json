{"cell_type":{"07e433a5":"code","bfd0f40b":"code","c47d9ec1":"code","7c54ec9a":"code","01630ea1":"code","9e3e102a":"code","0b473483":"code","04054d96":"markdown","299cee8c":"markdown"},"source":{"07e433a5":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom matplotlib import pyplot as plt\nfrom PIL import Image","bfd0f40b":"dftrain = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ndftest = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\nX_train = dftrain.drop(columns='label').values.reshape((dftrain.shape[0], 28, 28, 1))\ny_train = keras.utils.to_categorical(dftrain['label'].values)\nX_test = dftest.values.reshape((dftest.shape[0], 28, 28, 1))","c47d9ec1":"x = tf.image.grayscale_to_rgb(tf.constant(X_train))\nx = tf.pad(x, ((0, 0), (2, 2), (2, 2), (0, 0)))\nx = keras.applications.resnet_v2.preprocess_input(tf.cast(x, tf.float32))","7c54ec9a":"lambda_ = 1e-3\nmodel = keras.Sequential()\ncore = keras.applications.ResNet152V2(\n    include_top=False,\n    weights='imagenet',\n    input_shape=x.shape[1:]\n)\nfor layer in core.layers[:-2]:\n    layer.trainable = False\nmodel.add(core)\nmodel.add(keras.layers.Flatten())\nmodel.add(keras.layers.Dense(10, activation='softmax', \n                             kernel_regularizer=keras.regularizers.l2(lambda_)))","01630ea1":"model.summary()","9e3e102a":"model.compile(\n    optimizer=keras.optimizers.Adam(),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)","0b473483":"history = model.fit(x, y_train, validation_split=0.2, batch_size=128, epochs=20)","04054d96":"I was expecting the results to be better, to be honest. A plain, small, fully connected NN performs better. This is probably because the dataset is too simple for a too complex NN. I could not unfreeze the last Conv2D layer, as it has 1M+ params - massive overfitting if I did. An option would be to try something simpler like AlexNet or LeNet5, but I haven't seen pre-trained models for these built in keras.","299cee8c":"# Transfer learning\n\nThis notebook shows how to do transfer learning using a pre-trained CNN for the MNIST dataset."}}