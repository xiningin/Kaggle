{"cell_type":{"a5af9ca3":"code","b42540c2":"code","8bfc0993":"code","033969d4":"code","3bec20d1":"code","725c80a2":"code","60522fd7":"markdown","a09d1cf3":"markdown","9e280bb5":"markdown","f56d96e5":"markdown","8c885988":"markdown"},"source":{"a5af9ca3":"# Imports\n\nimport bs4\nfrom bs4 import BeautifulSoup\nimport requests\nimport json\nimport urllib\nimport pandas as pd\nimport os\nimport time\nimport re\nimport numpy as np\n\nfrom tqdm.auto import tqdm\ntqdm.pandas()\n\npd.options.display.max_colwidth = None","b42540c2":"# Constants\n\nINPUT_DIR = '..\/input\/commonlitreadabilityprize'\n\n# Min and max lengths (in characters) of the excerpts in the training set.\nMIN_P_LENGTH = 669\nMAX_P_LENGTH = 1341\n\nLICENSE = 'CC BY-SA 3.0 and GFDL'     # License reference","8bfc0993":"data = pd.read_csv(os.path.join(INPUT_DIR, 'train.csv'))\ndata['url_legal'].fillna('', inplace=True)    # Nulls have to be converted to '' for the contains function to work\nsimple_wiki = data[data.url_legal.str.contains('simple.wiki')]\nprint(f'There are {len(simple_wiki)} urls from simple Wiki')\nsimple_wiki.head()","033969d4":"# Join consecutive paragraphs greedily so their length is between MIN_P_LENGTH and MAX_P_LENGTHS\ndef join_paragraphs(paragraphs):\n    \n    lengths = [len(p) for p in paragraphs]\n    \n    def candidate_length(i, j):\n        return np.sum(lengths[i:j])\n    \n    result = []\n    L = len(lengths)\n    i = 0\n    j = 1\n    while i < L and j < L:\n        l = candidate_length(i,j)\n        if l >= MIN_P_LENGTH and l <= MAX_P_LENGTH:\n            p = '\\r\\n'.join([p for p in paragraphs[i:j]])\n            result.append(p)\n            i = j\n            j = j+1\n        else:\n            j = j+1\n    return result\n\n# Get a list of the paragraphs and references to extra articles of an article (text retrieved with BeautifulSoup)\ndef get_paragraphs_and_links(article):\n    \n    soup = BeautifulSoup(article)\n    \n    # Find the article content div, and get all the paragraphs and links contained in it\n\n    content = soup.find_all('div', id='mw-content-text')[0].find('div', class_='mw-parser-output')\n    if not content is None:\n        paragraphs = content.find_all('p', recursive=False)\n\n        # Extract text and filter references\n        paragraphs = [re.sub(r'\\[\\d+\\]', '', p.text) for p in paragraphs]\n    \n        # Join paragraphs to be in the length range\n        paragraphs = join_paragraphs(paragraphs)\n    \n        # Find extra links to simple wiki articles\n        links = content.find_all('a', href=re.compile('^\/wiki\/[A-Za-z0-9_-]+$'))\n    \n        return paragraphs, links\n    else:\n        return [], []","3bec20d1":"# First run to grab extra excerpts from the same articles\nurl_legal = []\nexcerpts = []\nextra_articles = []\nfor link in tqdm(simple_wiki.url_legal):\n    article = requests.get(link).text\n    paragraphs, links = get_paragraphs_and_links(article)\n    url_legal.extend([link]*len(paragraphs))\n    excerpts.extend(paragraphs)\n    extra_articles.extend(links)\n\nprint(f'There are {len(excerpts)} excerpts and {len(extra_articles)} links to extra articles')\n\nextra = pd.DataFrame(data = {'url_legal': url_legal, 'license': LICENSE, 'excerpt': excerpts})\nextra.head()","725c80a2":"# Second run with extra articles\n\nextra_run = False\n\nif extra_run:\n    for article in tqdm(extra_articles):\n        link = f\"https:\/\/simple.wikipedia.org{article['href']}\"\n        article = requests.get(link).text\n        paragraphs, _ = get_paragraphs_and_links(article)\n        url_legal.extend([link]*len(paragraphs))\n        excerpts.extend(paragraphs)","60522fd7":"I won't do it here, though I show the code below, because it would take some time (more than an hour in my machine), but I second run could be done with the extra articles, in which case an additional <b>16000<\/b> extra excerpts could be retrieved in the same way.","a09d1cf3":"<h1>Generate extra data with web scraping<\/h1>\n\nThis is the first competition in which I've used transformers, and I've learned a lot from many kagglers and researchers. One of the most inspiring sources has been this article: <a href='https:\/\/arxiv.org\/abs\/1905.05583'>How to fine-tune BERT for text classification<\/a>. In it, the authors demonstrate that pretraining BERT models with task specific data or domain specific data can improve the results when fine-tuning a downstream task, especially when the dataset for the downstream task is small, as is the case in this competition, with only 2834 instances in the training set. \n\nIn this notebook, I'll show how to extract additional excerpts by webscraping <a href='https:\/\/simple.wikipedia.org\/wiki\/Main_Page'>simple.wikipedia.org<\/a>, a source already present in the training set, using two Python packages: <a href='https:\/\/pypi.org\/project\/requests\/'>requests<\/a> and <a href='https:\/\/pypi.org\/project\/beautifulsoup4\/'>bs4<\/a>.","9e280bb5":"There are already 196 excerpts in the training set from Simple Wikipedia. I'll filter those into a dataset for further processing.","f56d96e5":"<h2>Utilities<\/h2>\n\nThe next cell presents the two main utilities I'm using to web scrape the web.","8c885988":"<h2>Main section<\/h2>"}}