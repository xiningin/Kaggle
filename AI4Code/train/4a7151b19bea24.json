{"cell_type":{"949ff2dd":"code","a6fa98e2":"code","e6f97fde":"code","139b1d83":"code","a5d89e2d":"code","6cdf4098":"code","0360d949":"code","b8446a53":"code","dc720030":"code","507b3f36":"code","57d6b07f":"code","a644abd3":"code","a2eb8f23":"code","7c6a26bb":"code","486a4ef2":"code","6dac6a95":"code","b69614bc":"code","cd207378":"code","0e2984e5":"code","7338981e":"code","35a06bb3":"code","80b0343b":"code","db5c3d04":"code","12e54d1f":"code","d6c56dd5":"code","9da958ae":"markdown","b8b89f07":"markdown","a789f46c":"markdown","b7d77752":"markdown","fd0d5537":"markdown","28b01132":"markdown","8d9be1d9":"markdown","5d36924a":"markdown","79b6b5c6":"markdown","d2199ccd":"markdown","d451b9f3":"markdown"},"source":{"949ff2dd":"#standard\nimport os\nimport time\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom math import ceil\n\n#sklearn data_preprocessing\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder,OrdinalEncoder,LabelEncoder\nimport phik\n#sklearn categorical encoding\nimport category_encoders as ce\n\n#sklearn modelling\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve, train_test_split, KFold, RandomizedSearchCV\n\n\n#sklearn regressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import ElasticNetCV\n\n#LightGBM\nimport lightgbm as lgb\n\n#feature slection\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LassoCV, LinearRegression, RidgeCV\n\n\n#warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","a6fa98e2":"data = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2021\/test.csv\")\n# Set id as as index\ndata.set_index(\"id\",inplace=True)\ntest.set_index(\"id\",inplace=True)","e6f97fde":"data.describe(include = \"all\")","139b1d83":"data.dtypes","a5d89e2d":"# we have 1 id, 10 categorical variables, 14 continuous variables\ncat_feats = data.iloc[:,0:10].columns\nnumeric_feats = data.iloc[:,10:-1].columns\ntrain = data.iloc[:,:-1]\ntarget = data.iloc[:,-1]","6cdf4098":"train[numeric_feats]","0360d949":"def cor_heatmap(cor):\n    plt.figure(figsize=(12,10))\n    sns.heatmap(data = cor, annot = True, cmap = plt.cm.Reds, fmt='.1')\n    plt.show()\n    \n#DO IT\ncor_heatmap(train[numeric_feats].corr())    ","b8446a53":"def displot_all(df, columns):\n    #Prepare figure layout\n    rows = int(ceil(columns.shape[0]\/5))\n    sns.set()\n    fig, axes = plt.subplots(nrows = rows, ncols=5, figsize=(15,3*rows))\n\n    # Draw the boxplots\n    for i in zip(axes.flatten(), columns):\n        sns.distplot(x=df.loc[:,i[1]], ax=i[0])      \n        i[0].set_title(i[1])\n        i[0].set_ylabel(\"\")\n        for tick in i[0].get_xticklabels():\n            tick.set_rotation(-25)\n    # Finalize the plot\n    plt.subplots_adjust(wspace=0.5,hspace = 0.5)\n    fig.suptitle(\"Dist plots\", fontsize=25)\n    sns.despine(bottom=True)\n    plt.show()\n    \ndisplot_all(train,numeric_feats)    ","dc720030":"def scter_all(df, columns, target):\n    #Prepare figure layout\n    rows = int(ceil(columns.shape[0]\/5))\n    sns.set()\n    fig, axes = plt.subplots(nrows = rows, ncols=5, figsize=(15,3*rows))\n\n    # Draw the boxplots\n    for i in zip(axes.flatten(), columns):\n        sns.scatterplot(x= i[1], y = target,data=df, ax=i[0])      \n        #i[0].set_title(i[1])\n        i[0].set_ylabel(\"\")\n        for tick in i[0].get_xticklabels():\n            tick.set_rotation(-25)\n    # Finalize the plot\n    plt.subplots_adjust(wspace=0.5,hspace = 0.5)\n    fig.suptitle(\"Dist plots\", fontsize=25)\n    sns.despine(bottom=True)\n    plt.show()\n    \nscter_all(data ,numeric_feats, \"target\")    ","507b3f36":"def plot_importance(coef, name, ax):\n    imp_coef = coef.sort_values()\n    ax.bar(imp_coef)\n    ax.title(\"Feature importance using \" + name + \" Model\")\n    \nfig, axes = plt.subplots(nrows = 1, ncols = 2, sharex=\"all\", figsize=(10,6))\nmodels = [LassoCV(), RidgeCV()]\nnames = [\"Lasso\",\"Ridge\"]\nfor model, name, ax in zip(models, names, axes.flatten()):\n    reg = model\n    reg.fit(X = train[numeric_feats], y = target)\n    #DO IT\n    coef = pd.Series(reg.coef_, index  = train[numeric_feats].columns)\n    imp_coef = coef.sort_values()\n    g = sns.barplot(x = imp_coef.values, y = imp_coef.index, orient='h', ax = ax,color='Blue')\n    ax.set_title(\"Feature importance using \" + name + \" Model\")","57d6b07f":"#no of features\nnof_list=np.arange(1,13)            \nhigh_score=0\n#Variable to store the optimum features\nnof=0           \nscore_list =[]\nfor n in range(len(nof_list)):\n    # we are going to see in the next class this \"train_test_split()\"...\n    X_train, X_test, y_train, y_test = train_test_split(train[numeric_feats],target, test_size = 0.3, random_state = 0)\n    \n    model = LinearRegression()\n    rfe = RFE(model,nof_list[n])\n    X_train_rfe = rfe.fit_transform(X_train,y_train)\n    X_test_rfe = rfe.transform(X_test)\n    model.fit(X_train_rfe,y_train)\n    \n    score = model.score(X_test_rfe,y_test)\n    score_list.append(score)\n    \n    if(score>high_score):\n        high_score = score\n        nof = nof_list[n]\nprint(\"Optimum number of features: %d\" %nof)\nprint(\"Score with %d features: %f\" % (nof, high_score))","a644abd3":"phik = train[cat_feats].phik_matrix()\n\nmask = np.zeros_like(phik, dtype=np.bool)\nmask[np.triu_indices_from(mask)]= True\n\nf, ax = plt.subplots(figsize=(10, 15)) \nheatmap = sns.heatmap(phik, \n                      square = True,\n                      mask = mask,\n                      linewidths = .5,\n                      cmap = 'coolwarm',\n                      cbar_kws = {'shrink': .6, \n                                'ticks' : [-1, -.5, 0, 0.5, 1]},\n                      fmt='.2g',\n                      vmin = -1, \n                      vmax = 1,\n                      annot = True,\n                      annot_kws = {'size': 10})\n#add the column names as labels\nax.set_yticklabels(phik.columns, rotation = 0)\nax.set_xticklabels(phik.columns)\nsns.set_style({'xtick.bottom': True}, {'ytick.left': True})","a2eb8f23":"def countplot_all(df, columns):\n    #Prepare figure layout\n    rows = int(ceil(columns.shape[0]\/5))\n    sns.set()\n    fig, axes = plt.subplots(nrows = rows, ncols=5, figsize=(15,3*rows))\n\n    # Draw the boxplots\n    for i in zip(axes.flatten(), columns):\n        sns.countplot(x=df.loc[:,i[1]], ax=i[0])      \n        i[0].set_title(i[1])\n        i[0].set_ylabel(\"\")\n        for tick in i[0].get_xticklabels():\n            tick.set_rotation(-25)\n    # Finalize the plot\n    plt.subplots_adjust(wspace=0.5,hspace = 0.5)\n    fig.suptitle(\"Dist plots\", fontsize=25)\n    sns.despine(bottom=True)\n    plt.show()\n    \ncountplot_all(train,cat_feats)    ","7c6a26bb":"def displot_all_hue(df, columns):\n    #Prepare figure layout\n    rows = int(ceil(columns.shape[0]\/3))\n    sns.set()\n    fig, axes = plt.subplots(nrows = rows, ncols=3, figsize=(15,5*rows))\n\n    # Draw the boxplots\n    for i in zip(axes.flatten(), columns):\n        sns.histplot(x='target',hue = i[1] ,data = df, ax=i[0])      \n        i[0].set_title(i[1])\n        i[0].set_ylabel(\"\")\n        for tick in i[0].get_xticklabels():\n            tick.set_rotation(-25)\n    # Finalize the plot\n    plt.subplots_adjust(wspace=0.5,hspace = 0.5)\n    fig.suptitle(\"Dist plots\", fontsize=25)\n    sns.despine(bottom=True)\n    plt.show()\n    \ndisplot_all_hue(data, cat_feats)    \n","486a4ef2":"# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.3,  random_state=2)\n\n\ndef glmmEncode_Scaler_pipeline(X_train, y_train, X_test, scaler = MinMaxScaler()): \n    X_train_encoded = X_train.copy()\n    X_test_encoded= X_test.copy()\n    # Set up feature to encode\n    feature_to_encode = X_train.columns[X_train.dtypes == 'O'].tolist()\n    # Initia the encoder model\n    GLMMEncoder = ce.glmm.GLMMEncoder(binomial_target=False)\n    # fit the train data\n    GLMMEncoder.fit(X_train[feature_to_encode],y_train)\n\n    # transform training set\n    X_train_encoded[feature_to_encode] = GLMMEncoder.transform(X_train[feature_to_encode])\n    # transform test set\n    X_test_encoded[feature_to_encode] = GLMMEncoder.transform(X_test[feature_to_encode])\n\n    # setup MINMAXSCALER\n    scaler = StandardScaler()\n    # fit the scaler                    \n    scaler.fit(X_train_encoded)\n    # transform training set\n    X_train_scaled = pd.DataFrame(scaler.transform(X_train_encoded), columns=X_train_encoded.columns, index=X_train_encoded.index)\n    # transform test set\n    X_test_scaled = pd.DataFrame(scaler.transform(X_test_encoded), columns=X_test_encoded.columns, index=X_test_encoded.index)\n    # store back\n    return (X_train_scaled, X_test_scaled)\n\n    \nX_train, X_test =  glmmEncode_Scaler_pipeline(X_train, y_train, X_test)\ntraining_dataset, test_dataset= glmmEncode_Scaler_pipeline(train,target, test)","6dac6a95":"print('Train set:', X_train.shape)\nprint('Test set:', X_test.shape)\nprint('Whole training set set:', training_dataset.shape)\nprint('Whole test set:', test_dataset.shape)","b69614bc":"   \nfig, axes = plt.subplots(nrows = 1, ncols = 2, sharex=\"all\", figsize=(10,6))\nmodels = [LassoCV(), RidgeCV()]\nnames = [\"Lasso\",\"Ridge\"]\nfor model, name, ax in zip(models, names, axes.flatten()):\n    reg = model\n    reg.fit(X = training_dataset[cat_feats], y = target)\n    #DO IT\n    coef = pd.Series(reg.coef_, index  = training_dataset[cat_feats].columns)\n    imp_coef = coef.sort_values()\n    g = sns.barplot(x = imp_coef.values, y = imp_coef.index, orient='h', ax = ax,color='Blue')\n    ax.set_title(\"Feature importance using \" + name + \" Model\")","cd207378":"fig, axes = plt.subplots(nrows = 1, ncols = 2, sharex=\"all\", figsize=(10,6))\nmodels = [LassoCV(), RidgeCV()]\nnames = [\"Lasso\",\"Ridge\"]\nfor model, name, ax in zip(models, names, axes.flatten()):\n    reg = model\n    reg.fit(X = training_dataset[numeric_feats], y = target)\n    #DO IT\n    coef = pd.Series(reg.coef_, index  = training_dataset[numeric_feats].columns)\n    imp_coef = coef.sort_values()\n    g = sns.barplot(x = imp_coef.values, y = imp_coef.index, orient='h', ax = ax,color='Blue')\n    ax.set_title(\"Feature importance using \" + name + \" Model\")","0e2984e5":"np.concatenate((cat_feats.drop('cat4').values,numeric_feats.drop(['cont2','cont3']).values))","7338981e":"indx = y_train[y_train != 0].index\n\n#columns_select = np.concatenate((cat_feats.drop('cat4').values,numeric_feats.drop(['cont2','cont3']).values))\ncolumns_select = np.concatenate((cat_feats.values, numeric_feats.values))\nran_state = 2\n\nmodels = []\n#Logistic Regression\n\nparams = {'reg_alpha': 6.147694913504962,\n 'reg_lambda': 0.002457826062076097,\n 'colsample_bytree': 0.3,\n 'subsample': 0.8,\n 'learning_rate': 0.005,\n 'max_depth': 5,\n 'num_leaves': 30,\n 'min_child_samples': 285,\n 'random_state': 2,\n'verbose':-1,\n 'n_estimators': 15000,\n 'metric': 'rmse',\n 'cat_smooth': 39}\n\nmodels.append(('LightGBM', lgb.LGBMRegressor(boosting_type='gbdt',random_state = 2)))\nmodels.append(('LightGBM_tuned',lgb.LGBMRegressor(boosting_type='gbdt',**params)))\n# evalutate each model in turn\nresults = []\nnames = []\nscores = {}\nmodel = lgb.LGBMRegressor(boosting_type='gbdt',random_state = 2)\n\n#First, measure the base-line model with full features\nbegin = time.perf_counter()\nmodel.fit(X_train.loc[:,:], y_train[:])\n# Run cross-validation on training set\ntrain_score = np.sqrt(mean_squared_error(y_train, model.predict(X_train.loc[:,:])))\ntest_score = np.sqrt(mean_squared_error(y_test, model.predict(X_test.loc[:,:])))\n# finish counting time\nend = time.perf_counter()\nnames.append('baseline')\n# assign infomation\nscores['baseline'] = [train_score, test_score ,round(end-begin,3)]\n\n# Now, try on selected features\nfor name, model in models:\n    # start counting time\n    begin = time.perf_counter()\n    model.fit(X_train.loc[indx,:], y_train[indx])\n    # Run cross-validation on training set\n    train_score = np.sqrt(mean_squared_error(y_train, model.predict(X_train.loc[:,:])))\n    test_score = np.sqrt(mean_squared_error(y_test, model.predict(X_test.loc[:,:])))\n    # finish counting time\n    end = time.perf_counter()\n    names.append(name)\n    # assign infomation\n    scores[name] = [train_score, test_score ,round(end-begin,3)]\n\nfinal_score = pd.DataFrame.from_dict(scores, orient='index',columns=['Train','Test','Training time'])\nfinal_score","35a06bb3":"# Adapted from: https:\/\/scikit-optimize.github.io\/stable\/auto_examples\/hyperparameter-optimization.html\n\nfrom skopt.space import Real, Integer\nfrom skopt.utils import use_named_args\n\n\n# The list of hyper-parameters we want to optimize. For each one we define the\n# bounds, the corresponding scikit-learn parameter name, as well as how to\n# sample values from that dimension (`'log-uniform'` for the learning rate)\nspace  = [Integer(1, 10, name='max_depth'),          \n          Real(0.01, 0.5, name='learning_rate', prior='log-uniform'),\n          Integer(2, 100, name='num_leaves')\n         ]\nreg = lgb.LGBMRegressor(boosting_type='gbdt',                        \n                        n_estimators=100,\n                        random_state = 2\n                        )\n# this decorator allows your objective function to receive a the parameters as\n# keyword arguments. This is particularly convenient when you want to set\n# scikit-learn estimator parameters\n@use_named_args(space)\ndef objective(**params):\n    reg.set_params(**params)\n    return -np.mean(cross_val_score(reg, training_dataset, target, cv=6, n_jobs=-1,\n                                    scoring=\"neg_mean_squared_error\"))","80b0343b":"from skopt import forest_minimize\nres_gp = forest_minimize(objective, space, n_calls=50, random_state=0)\n\n\"Best score=%.4f\" % res_gp.fun","db5c3d04":"res_gp.x","12e54d1f":"params = {'reg_alpha': 6.147694913504962,\n 'reg_lambda': 0.002457826062076097,\n 'colsample_bytree': 0.3,\n 'subsample': 0.8,\n 'learning_rate': 0.0045,\n 'max_depth': 6,\n 'num_leaves': 35,\n 'min_child_samples': 285,\n 'random_state': 2,\n'verbose':-1,\n 'n_estimators': 15000,\n 'metric': 'rmse',\n 'cat_smooth': 39}","d6c56dd5":"model = lgb.LGBMRegressor(boosting_type='gbdt',**params)\nmodel.fit(training_dataset, target)\ny_predicted = model.predict(test_dataset)\n\nsubmission = pd.DataFrame({'id': test_dataset.index, 'target': y_predicted})\nfilename = 'predict_GB_GLMM_tunned.csv'\nsubmission.to_csv(filename, index=False)","9da958ae":"# 4. Model selection","b8b89f07":"# 2. EDA","a789f46c":"## Categorical features\n\nSame processes of EDA as numeric features\n\n**Apply Phi_K to measure correlation coeffictient of all features together:**\n\nPhi_K is a new and practical correlation coefficient based on several refinements to Pearson\u2019s hypothesis test of independence of two variables.\n","b7d77752":"### Apply feature selection using Lasso and Ridge","fd0d5537":"**Now, let's use the library skopt to tune the parameter of the estimator LightGBM**","28b01132":"# 1.Introduction\n\nThis kernel will provide some very fundamental EDA to help understand better both categorical and numeric features in the dataset. Interesting visualizations including multivariate correlation between categorical variables using PhiK library will be introduced\n\nThen, Recursive Feature Elimination (RFE) technique using Ridge and Lasso regression algorithm will be used to access the importance of each feature.\n\nThe categorical features will be encoded using General Linear Mixture Model (an encoder under the target encoder family)\n\nIn the Model selection, multiple LightGBM models with different hyperparameters will be compared\n\nKeywords: EDA | PhiK | LightGBM | GLMM encoder | RFE | Feature selection | Model selection","8d9be1d9":"### Perform RFE to find the optimum number of features","5d36924a":"First of all, let have a look at some basic statistical information such as multivariate correlation, distribution of the variables","79b6b5c6":"## Explore numeric features:","d2199ccd":"# 3. Data processing\n\nIn this step, we will create a pipeline to scale and encode the features using StandardScaler and GLMM encoder","d451b9f3":"## Submission"}}