{"cell_type":{"26f1e596":"code","63df0407":"code","f84408e3":"code","e4d7c66d":"code","134f483a":"code","66c936f0":"code","17e177d9":"code","102f9122":"code","278331a0":"code","5af5d709":"code","79876c1c":"code","0e066ed4":"code","4c2008ae":"code","2aef293e":"code","472153a7":"code","f0013778":"code","92704072":"code","7058cd36":"code","718f0e61":"code","62009eb8":"code","7830c9b1":"code","2c338b03":"code","8fc94e33":"code","07941c2d":"code","73462f3c":"code","ce57ed24":"code","7ce12763":"code","411b945a":"code","b23b90d7":"code","ba6f3516":"code","b2a3b2dc":"code","fac70789":"code","eec4ba28":"code","33e31589":"code","9fad7f22":"code","1cf0cb7f":"code","63e539fe":"code","80a73cd3":"code","e556e21f":"code","0f06c788":"code","d1da9685":"code","0e0b3ae4":"code","76da096f":"code","9e77354c":"code","cfa2b52e":"code","250dfec5":"code","32fc7926":"code","730e1bb0":"code","2f5a8986":"code","28dd8539":"code","a70e2a12":"code","f99db1fb":"code","c25348ac":"code","504a0cba":"code","ef3effae":"code","cc5d8fd1":"code","5dad3f1b":"code","d3a187b5":"code","a9848015":"code","995ebf3b":"code","a2298ae8":"code","d1bee9e2":"markdown"},"source":{"26f1e596":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","63df0407":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,mean_squared_error\nfrom tqdm import tqdm_notebook","f84408e3":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.colors\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,mean_squared_error\nfrom tqdm import tqdm_notebook\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.datasets import make_blobs #used to genereate non-linearly seperable data","e4d7c66d":"train_data=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\n# train_data_copy=read_csv()\ntest_data=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","134f483a":"train_data.head()","66c936f0":"train_data.shape","17e177d9":"train_data.isnull().sum()","102f9122":"train_data.drop(columns='Cabin',axis=1,inplace=True)","278331a0":"train_data.isnull().sum()","5af5d709":"train_data.Age.fillna(value=train_data.Age.mean(),inplace=True)","79876c1c":"train_data.isnull().sum()","0e066ed4":"train_data.Embarked.unique()","4c2008ae":"train_data.loc[train_data.Embarked=='S'].count()","2aef293e":"#maximum embarked have value S so lets replace null with S\ntrain_data.loc[train_data.Embarked=='S']","472153a7":"train_data.loc[train_data.Embarked.isna()].Embarked","f0013778":"train_data.loc[train_data.Embarked.isna()].Embarked","92704072":"train_data.Embarked.fillna(value='S',inplace=True)","7058cd36":"train_data.loc[train_data.Embarked.isna()].Embarked","718f0e61":"train_data.isna().sum()","62009eb8":"#now our data has no missing values\ntrain_data.head()","7830c9b1":"filtered_train_data=train_data[['PassengerId','Pclass','Sex','Age','SibSp','Parch','Embarked']]","2c338b03":"filtered_train_data.set_index('PassengerId',inplace=True)","8fc94e33":"filtered_train_data.head()","07941c2d":"Y_train=train_data['Survived']","73462f3c":"Y_train.head()","ce57ed24":"filtered_train_data.Pclass.unique()","7ce12763":"filtered_train_data.Sex.unique()","411b945a":"filtered_train_data.Sex.replace({'male':1,'female':0},inplace=True)","b23b90d7":"filtered_train_data.head()","ba6f3516":"filtered_train_data.Embarked.unique()","b2a3b2dc":"filtered_train_data.Embarked.replace({'S':0,'C':1,'Q':2},inplace=True)","fac70789":"filtered_train_data.head()","eec4ba28":"filtered_train_data.Parch.unique()","33e31589":"min=filtered_train_data.Age.min()\nmax=filtered_train_data.Age.max()\nprint(min,\" \",max)","9fad7f22":"filtered_train_data.Age=(filtered_train_data.Age-min)\/(max-min)","1cf0cb7f":"filtered_train_data.head()","63e539fe":"# model=DecisionTreeClassifier()\nmodel=RandomForestClassifier(n_estimators=10,max_leaf_nodes=10,random_state=0)","80a73cd3":"model.fit(filtered_train_data,Y_train)","e556e21f":"test_data.head()","0f06c788":"test_data.isnull().sum()","d1da9685":"filtered_test_data=test_data[['PassengerId','Pclass','Sex','Age','SibSp','Parch','Embarked']]","0e0b3ae4":"filtered_test_data.head()","76da096f":"filtered_test_data.set_index('PassengerId',inplace=True)","9e77354c":"filtered_test_data.Sex.replace({'male':1,'female':0},inplace=True)","cfa2b52e":"filtered_test_data.Age.fillna(value=filtered_test_data.Age.mean(),inplace=True)","250dfec5":"test_min=filtered_test_data.Age.min()\ntest_max=filtered_test_data.Age.max()\nprint(max,\" \",min)","32fc7926":"filtered_test_data.Age=(filtered_test_data.Age-test_min)\/(test_max-test_min)","730e1bb0":"filtered_test_data.head()","2f5a8986":"filtered_test_data.Embarked.replace({'S':0,'C':1,'Q':2},inplace=True)","28dd8539":"filtered_test_data.head()","a70e2a12":"Y_pred=model.predict(filtered_test_data)","f99db1fb":"# predicted_y=pd.DataFrame({'PassengerId':filtered_test_data.index,'Survived':Y_pred})","c25348ac":"# predicted_y.head()","504a0cba":"# predicted_y.to_csv('submission.csv',index=False)","ef3effae":"class FFSNNetwork:\n  \n  def __init__(self, n_inputs, hidden_sizes=[2]):\n    self.nx = n_inputs\n    self.ny = 1\n    self.nh = len(hidden_sizes)\n    self.sizes = [self.nx] + hidden_sizes + [self.ny]\n    \n    self.W = {}\n    self.B = {}\n    for i in range(self.nh+1):\n      self.W[i+1] = np.random.randn(self.sizes[i], self.sizes[i+1])\n      self.B[i+1] = np.zeros((1, self.sizes[i+1]))\n  \n  def sigmoid(self, x):\n    return 1.0\/(1.0 + np.exp(-x))\n  \n  def forward_pass(self, x):\n    self.A = {}\n    self.H = {}\n    self.H[0] = x.reshape(1, -1)\n    for i in range(self.nh+1):\n      self.A[i+1] = np.matmul(self.H[i], self.W[i+1]) + self.B[i+1]\n      self.H[i+1] = self.sigmoid(self.A[i+1])\n    return self.H[self.nh+1]\n  \n  def grad_sigmoid(self, x):\n    return x*(1-x) \n    \n  def grad(self, x, y):\n    self.forward_pass(x)\n    self.dW = {}\n    self.dB = {}\n    self.dH = {}\n    self.dA = {}\n    L = self.nh + 1\n    self.dA[L] = (self.H[L] - y)\n    for k in range(L, 0, -1):\n      self.dW[k] = np.matmul(self.H[k-1].T, self.dA[k])\n      self.dB[k] = self.dA[k]\n      self.dH[k-1] = np.matmul(self.dA[k], self.W[k].T)\n      self.dA[k-1] = np.multiply(self.dH[k-1], self.grad_sigmoid(self.H[k-1]))\n    \n  def fit(self, X, Y, epochs=1, learning_rate=1, initialise=True, display_loss=False):\n    \n    # initialise w, b\n    if initialise:\n      for i in range(self.nh+1):\n        self.W[i+1] = np.random.randn(self.sizes[i], self.sizes[i+1])\n        self.B[i+1] = np.zeros((1, self.sizes[i+1]))\n      \n    if display_loss:\n      loss = {}\n    \n    for e in tqdm_notebook(range(epochs), total=epochs, unit=\"epoch\"):\n      dW = {}\n      dB = {}\n      for i in range(self.nh+1):\n        dW[i+1] = np.zeros((self.sizes[i], self.sizes[i+1]))\n        dB[i+1] = np.zeros((1, self.sizes[i+1]))\n      for x, y in zip(X, Y):\n        self.grad(x, y)\n        for i in range(self.nh+1):\n          dW[i+1] += self.dW[i+1]\n          dB[i+1] += self.dB[i+1]\n        \n      m = X.shape[1]\n      for i in range(self.nh+1):\n        self.W[i+1] -= learning_rate * dW[i+1] \/ m\n        self.B[i+1] -= learning_rate * dB[i+1] \/ m\n      \n      if display_loss:\n        Y_pred = self.predict(X)\n        loss[e] = mean_squared_error(Y_pred, Y)\n    \n    if display_loss:\n      plt.plot(np.array(list(loss.values())).astype(float))\n      plt.xlabel('Epochs')\n      plt.ylabel('Mean Squared Error')\n      plt.show()\n      \n  def predict(self, X):\n    Y_pred = []\n    for x in X:\n      y_pred = self.forward_pass(x)\n      Y_pred.append(y_pred)\n    return np.array(Y_pred).squeeze()","cc5d8fd1":"converted_filtered_train_data=np.array(filtered_train_data)\nconverted_y_train=np.array(Y_train)\nprint(converted_filtered_data,converted_y_train)","5dad3f1b":"ffsnn = FFSNNetwork(6, [2, 3])\nffsnn.fit(converted_filtered_train_data, converted_y_train, epochs=1500, learning_rate=.01, display_loss=True)","d3a187b5":"converted_filtered_test_data=np.array(filtered_test_data)","a9848015":"Y_pred_val = ffsnn.predict(converted_filtered_test_data)\nY_pred_binarised_val = (Y_pred_val >= 0.5).astype(\"int\").ravel()\n","995ebf3b":"predicted_y=pd.DataFrame({'PassengerId':filtered_test_data.index,'Survived':Y_pred_binarised_val})","a2298ae8":"predicted_y.to_csv('submission.csv',index=False)","d1bee9e2":"# FeedForwordNetwork"}}