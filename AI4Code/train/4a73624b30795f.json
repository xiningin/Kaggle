{"cell_type":{"4dfd4fed":"code","9cc5d962":"code","d6ee125c":"code","2a35d1b7":"code","b733fcc6":"code","80fc6c48":"code","0aa61bef":"code","6aa647f8":"code","9df1d4c8":"code","7c3b255c":"code","292701c1":"code","70a67739":"code","4843d621":"code","a94c32c7":"markdown","fced0403":"markdown","b1a491e7":"markdown"},"source":{"4dfd4fed":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9cc5d962":"import pandas as pd\nimport numpy as np\nimport re\nimport os\n\nimport pandas as pd\nimport numpy as np\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.isri import ISRIStemmer\n\n!pip install Tashaphyne\nimport pyarabic.arabrepr\nfrom tashaphyne.stemming import ArabicLightStemmer\n\nimport random\nfrom sklearn.model_selection import train_test_split\n","d6ee125c":"stop_words =['\u0645\u0646',\n '\u0641\u064a',\n '\u0639\u0644\u0649',\n '\u0648',\n '\u0641\u0649',\n '\u064a\u0627',\n '\u0639\u0646',\n '\u0645\u0639',\n '\u0627\u0646',\n '\u0647\u0648',\n '\u0639\u0644\u064a',\n '\u0645\u0627',\n '\u0627\u0644\u0644\u064a',\n '\u0643\u0644',\n '\u0628\u0639\u062f',\n '\u062f\u0647',\n '\u0627\u0644\u064a\u0648\u0645',\n '\u0623\u0646',\n '\u064a\u0648\u0645',\n '\u0627\u0646\u0627',\n '\u0625\u0644\u0649',\n '\u0643\u0627\u0646',\n '\u0627\u064a\u0647',\n '\u0627\u0644\u0644\u0649',\n '\u0627\u0644\u0649',\n '\u062f\u064a',\n '\u0628\u064a\u0646',\n '\u0627\u0646\u062a',\n '\u0623\u0646\u0627',\n '\u062d\u062a\u0649',\n '\u0644\u0645\u0627',\n '\u0641\u064a\u0647',\n '\u0647\u0630\u0627',\n '\u0648\u0627\u062d\u062f',\n '\u0627\u062d\u0646\u0627',\n '\u0627\u064a',\n '\u0643\u062f\u0647',\n '\u0625\u0646',\n '\u0627\u0648',\n '\u0623\u0648',\n '\u0639\u0644\u064a\u0647',\n '\u0641',\n '\u062f\u0649',\n '\u0645\u064a\u0646',\n '\u0627\u0644\u064a',\n '\u0643\u0627\u0646\u062a',\n '\u0623\u0645\u0627\u0645',\n '\u0632\u064a',\n '\u064a\u0643\u0648\u0646',\n '\u062e\u0644\u0627\u0644',\n '\u0639',\n '\u0643\u0646\u062a',\n '\u0647\u064a',\n '\u0641\u064a\u0647\u0627',\n '\u0639\u0646\u062f',\n '\u0627\u0644\u062a\u064a',\n '\u0627\u0644\u0630\u064a',\n '\u0642\u0627\u0644',\n '\u0647\u0630\u0647',\n '\u0642\u062f',\n '\u0627\u0646\u0647',\n '\u0631\u064a\u062a\u0648\u064a\u062a',\n '\u0628\u0639\u0636',\n '\u0623\u0648\u0644',\n '\u0627\u064a\u0647',\n '\u0627\u0644\u0627\u0646',\n '\u0623\u064a',\n '\u0645\u0646\u0630',\n '\u0639\u0644\u064a\u0647\u0627',\n '\u0644\u0647',\n '\u0627\u0644',\n '\u062a\u0645',\n '\u0628',\n '\u062f\u0629',\n '\u0639\u0644\u064a\u0643',\n '\u0627\u0649',\n '\u0643\u0644\u0647\u0627',\n '\u0627\u0644\u0644\u062a\u0649',\n '\u0647\u0649',\n '\u062f\u0627',\n '\u0627\u0646\u0643',\n '\u0648\u0647\u0648',\n '\u0648\u0645\u0646',\n '\u0645\u0646\u0643',\n '\u0646\u062d\u0646',\n '\u0632\u0649',\n '\u0623\u0646\u062a',\n '\u0627\u0646\u0647\u0645',\n '\u0645\u0639\u0627\u0646\u0627',\n '\u062d\u062a\u064a',\n '\u0648\u0627\u0646\u0627',\n '\u0639\u0646\u0647',\n '\u0625\u0644\u064a',\n '\u0648\u0646\u062d\u0646',\n '\u0648\u0627\u0646\u062a',\n '\u0645\u0646\u0643\u0645',\n '\u0648\u0627\u0646',\n '\u0645\u0639\u0627\u0647\u0645',\n '\u0645\u0639\u0627\u064a\u0627',\n '\u0648\u0623\u0646\u0627',\n '\u0639\u0646\u0647\u0627',\n '\u0625\u0646\u0647',\n '\u0627\u0646\u064a',\n '\u0645\u0639\u0643',\n '\u0627\u0646\u0646\u0627',\n '\u0641\u064a\u0647\u0645',\n '\u062f',\n '\u0627\u0646\u062a\u0627',\n '\u0639\u0646\u0643',\n '\u0648\u0647\u0649',\n '\u0645\u0639\u0627',\n '\u0622\u0646',\n '\u0627\u0646\u062a\u064a',\n '\u0648\u0623\u0646\u062a',\n '\u0648\u0625\u0646',\n '\u0648\u0645\u0639',\n '\u0648\u0639\u0646',\n '\u0645\u0639\u0627\u0643\u0645',\n '\u0645\u0639\u0627\u0643\u0648',\n '\u0645\u0639\u0627\u0647\u0627',\n '\u0648\u0639\u0644\u064a\u0647',\n '\u0648\u0627\u0646\u062a\u0645',\n '\u0648\u0627\u0646\u062a\u064a',\n '\u00bf',\n '|']","2a35d1b7":"   \ndef normalize(sentence):\n    '''\n    Argument:\n        string of words\n    return:\n        string of words but standardize the words\n    '''\n    sentence = re.sub(\"[\u0625\u0623\u0622\u0627]\", \"\u0627\", sentence)\n    sentence = re.sub(\"\u0649\", \"\u064a\", sentence)\n    sentence = re.sub(\"\u0624\", \"\u0621\", sentence)\n    sentence = re.sub(\"\u0626\", \"\u0621\", sentence)\n    sentence = re.sub(\"\u0629\", \"\u0647\", sentence)\n    sentence = re.sub(\"\u06af\", \"\u0643\", sentence)\n    return sentence\n","b733fcc6":"def removing_ar_stopwords(text):\n    \"\"\"\n        Here we remove all Arabic stop words\n        \n    \"\"\"\n      # if read it from file\n#     ar_stopwords_list = open(\"arabic_stopwords.txt\", \"r\") \n#     stop_words = ar_stopwords_list.read().split(\"\\n\")\n#     stop_words = []\n    original_words = []\n    words = word_tokenize(text) # it works on one hadith not list\n    for word in words:\n        if word not in stop_words:\n            original_words.append(word)\n    filtered_sentence = \" \".join(original_words)\n    return filtered_sentence\n","80fc6c48":"def clearReg(text):\n    \"\"\"\n        This function for getting the normal values of out of lemmatization function\n        that takse a string of dict as a \n        takes  : '{\"result\":[\"\u0627\u0645\u0631\",\"\u0628\",\"\u0623\u062e\u0630\",\"\u0645\u0627\",\"\u0646\u0647\u0649\",\"\u0647\",\"\u0627\u0646\u062a\u0647\u0649\"]}'\n        return : ['\u0627\u0645\u0631 \u0623\u062e\u0630 \u0645\u0627 \u0646\u0647\u0649 \u0627\u0646\u062a\u0647\u0649']\n    \"\"\"\n    each_lemma_word = []\n    each_lemma_sentence = []\n    for hadith in text:\n        matches = re.findall(r'\\\"(.+?)\\\"',hadith)\n        for word in matches:\n            if len(word) >= 2 and word !='result':\n                each_lemma_word.append(word)\n        each_lemma_sentence.append(\" \".join(each_lemma_word))\n        each_lemma_word.clear()\n    return each_lemma_sentence\n","0aa61bef":"\ndef stemming_1(text):\n    \"\"\"\n        This is first functoin for stemming and it's looks not good accurac, NLTK by ISRIStemmer.\n    \"\"\"\n    st = ISRIStemmer()\n    stemmend_words = []\n    words = word_tokenize(text)\n    for word in words:\n        stemmend_words.append(st.stem(word))\n    stemmed_sentence = \" \".join(stemmend_words)\n    return stemmed_sentence\n        \n    \n    \ndef stemming_2(text):\n    \"\"\"\n        This is Second functoin for stemming and it's looks good results, with built in library called Tashaphyne.\n        The documentation here ==> https:\/\/pypi.org\/project\/Tashaphyne\/\n    \n    \"\"\"\n    import pyarabic.arabrepr\n    arepr = pyarabic.arabrepr.ArabicRepr()\n    repr = arepr.repr\n\n    from tashaphyne.stemming import ArabicLightStemmer\n    ArListem = ArabicLightStemmer()\n\n    hadiths_without_stop_words_and_with_normalization_and_with_stemming = []\n\n    for hadith in hadiths_without_stop_words_and_with_normalization:\n        words = word_tokenize(hadith)\n        new_list = []\n        for word in words:\n            stem = ArListem.light_stem(word)\n            stem = ArListem.get_stem()\n            new_list.append(stem)\n\n        hadith_sentence_with_stemming = \" \".join(new_list)\n        hadiths_without_stop_words_and_with_normalization_and_with_stemming.append(hadith_sentence_with_stemming)\n        \n    return hadiths_without_stop_words_and_with_normalization_and_with_stemming\n","6aa647f8":"def lemmatization(text):\n    \"\"\"\n        This function for lemma Arabic words by API, and it getting best result of the previous functions\n        return a string dictinary like exactly '{\"result\":[\"\u0627\u0645\u0631\",\"\u0628\",\"\u0623\u062e\u0630\",\"\u0645\u0627\",\"\u0646\u0647\u0649\",\"\u0647\",\"\u0627\u0646\u062a\u0647\u0649\"]}'\n    \"\"\"\n    import http.client\n    conn = http.client.HTTPSConnection(\"farasa-api.qcri.org\")\n    hadith_dict = {}\n    list_pyload_input = []\n    list_pyload_out = []\n    length = len(text)\n    for h in text[:length]:\n        q = '{\"text\":'+'\"{}\"'.format(h)+'}'\n        list_pyload_input.append(q)\n    headers = { 'content-type': \"application\/json\", 'cache-control': \"no-cache\", }\n    for h in list_pyload_input:\n        conn.request(\"POST\", \"\/msa\/webapi\/lemma\", h.encode('utf-8'), headers)\n        res = conn.getresponse()\n        data = res.read()\n        list_pyload_out.append(data.decode(\"utf-8\"))\n        final_result = clearReg(list_pyload_out)     # call clearReg for clean the text\n    return final_result","9df1d4c8":"\ndata_1 = pd.read_csv(dirname + '\/Maliks Muwatta Without_Tashkel.csv')\ndata_1.head()","7c3b255c":"all_hadiths_1 = []\n\nfor hadith in data_1['Maliks Muwatta Without_Tashkel']:\n    all_hadiths_1.append(hadith)\n    ","292701c1":"%%time\n\n# Maliks Muwatta\ncleared_Hadith_1 = []           # Removing stopwords\ncleared_Hadith_1_2 = []         # Normalization\ncleared_Hadith_1_2_3 = []       # Lematization\n\nfor hadith in all_hadiths_1:\n    cleared_Hadith_1.append(removing_ar_stopwords(hadith))         # Removing stopwords\nfor hadith in cleared_Hadith_1:\n    cleared_Hadith_1_2.append(normalize(hadith))                   # Normalization\ncleared_Hadith_1_2_3 = lemmatization(cleared_Hadith_1_2)           # Lematization\n\nprint('The size of data:')\nlen(cleared_Hadith_1), len(cleared_Hadith_1_2), len(cleared_Hadith_1_2_3)","70a67739":"cleared_Hadith_1_2_3[:1]","4843d621":"# make it as a DataFram \nMaliks_Muwatta_preprosessing_1 = pd.DataFrame(cleared_Hadith_1_2_3, columns=['Maliks_Muwatta_Preprosessing_Cleaned'])\nMaliks_Muwatta_preprosessing_1.head()","a94c32c7":"It takes ~3min.\n\nI make just one Hadith book to showing you the steps, so you can make the same for others books.","fced0403":"#### Upove some functions for like:\n* stemming_1 by `ISRIStemmer from NLTK`.\n* stemming_2 by `Tashaphyne` is an Arabic light stemmer(removing prefixes and suffixes) and give all possible segmentations. \n* lemmatization by [Farasa API](https:\/\/alt.qcri.org\/farasa\/)\n\n**By the experimental: lemmatization by Farasa have a good results.**","b1a491e7":"* **`Stemming`** : algorithms work by cutting off the end or the beginning of the word, taking into account a list of common prefixes and suffixes that can be found in an inflected word.\n* **`Lemmatization`** : takes into consideration the morphological analysis of the words. \n\n**Lemmatization is typically seen as much more informative than simple stemming, because stem may not be an actual word whereas lemma is an actual language word.**"}}