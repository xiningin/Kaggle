{"cell_type":{"2a1bc445":"code","b07238fb":"code","bdcd9b54":"code","311cf43f":"code","6a0bd641":"code","3f29a168":"code","bc1b7661":"code","8d63245d":"code","c5d05f44":"code","b0867fbf":"markdown","f2d0a733":"markdown","7f2eb821":"markdown","81e7223a":"markdown","74244187":"markdown","a167cca1":"markdown","0d72adf1":"markdown","dc40f8f5":"markdown","49f26358":"markdown","e31ae59b":"markdown"},"source":{"2a1bc445":"import pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nimport numpy as np # linear algebra\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score","b07238fb":"training = pd.read_csv('..\/input\/titanic\/train.csv')\ntest =  pd.read_csv('..\/input\/titanic\/test.csv')","bdcd9b54":"training['train_test'] = 1\ntest['train_test'] = 0\ntest['Survived'] = np.NaN\nall_data = pd.concat([training,test])\n\n%matplotlib inline\nall_data.columns","311cf43f":"print(pd.pivot_table(training, index = 'Survived', columns = 'Pclass', values = 'Ticket' ,aggfunc ='count'))\nprint()\nprint(pd.pivot_table(training, index = 'Survived', columns = 'Sex', values = 'Ticket' ,aggfunc ='count'))\nprint()\nprint(pd.pivot_table(training, index = 'Survived', columns = 'Embarked', values = 'Ticket' ,aggfunc ='count'))","6a0bd641":"all_data.dropna(subset=['Embarked'],inplace = True)\nall_data.Age = all_data.Age.fillna(training.Age.median())\nall_data.Fare = all_data.Fare.fillna(training.Fare.median())","3f29a168":"all_dummies = pd.get_dummies(all_data[['Pclass','Sex','Age','SibSp','Parch','Embarked', 'train_test']])\n\n#Split the train and test datasets back again\n\nX_train = all_dummies[all_dummies.train_test == 1].drop(['train_test'], axis =1)\nX_test = all_dummies[all_dummies.train_test == 0].drop(['train_test'], axis =1)\n\n\ny_train = all_data[all_data.train_test==1].Survived","bc1b7661":"scaler = StandardScaler()\n\ndummies_scaled = all_dummies.copy()\ndummies_scaled[['Age','SibSp','Parch']]= scaler.fit_transform(dummies_scaled[['Age','SibSp','Parch']])\ndummies_scaled\n\nX_train_scaled = dummies_scaled[dummies_scaled.train_test == 1].drop(['train_test'], axis =1)\nX_test_scaled = dummies_scaled[dummies_scaled.train_test == 0].drop(['train_test'], axis =1)\n\ny_train = all_data[all_data.train_test==1].Survived","8d63245d":"model_rf = RandomForestRegressor(random_state = 1)\ncv = cross_val_score(model_rf,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","c5d05f44":"model_rf.fit(X_train_scaled,y_train)\ny = model_rf.predict(X_test_scaled).astype(int)\nsubmission = {'PassengerId': test.PassengerId, 'Survived': y}\nbase_submission = pd.DataFrame(data=submission)\nbase_submission.to_csv('submission.csv', index=False)","b0867fbf":"We create dummy variables from the listed categories\n","f2d0a733":"Here we import the data from the Kaggle virtual directories","7f2eb821":"First of all we impoerted all the libraries we will need later on in this implementation","81e7223a":"Here we do some data explorration to see if there are prominent connections between certain features and the odds of survival in the entire dataset","74244187":"This approach resulted in a 0.74641 = 74.641% accurate submission which is in comparison with the exerted efforts, this could be seen as a relatively good outcome. ","a167cca1":"Data Scaling","0d72adf1":"Model Evaluation","dc40f8f5":"Here we concat the training and test datasets so we could work on them both more easily and we use this technique so we can split them up again once we are done with pre-processing and feature engineering. ","49f26358":"We here drop the null 'embarked' rows. Only 2 instances of this in training and 0 in test \nWe fill the null data in both the \"Age\" and \"Fare\" columns","e31ae59b":"Prediction and Submission Generation"}}