{"cell_type":{"ec92d99f":"code","bae29f80":"code","8bf16798":"code","32839eed":"code","f6779a86":"code","6049eb6e":"code","65cae993":"code","63cc5d7c":"code","34a708c8":"code","6267c9af":"code","23c9e29e":"code","f3202ac9":"code","5f8b368f":"code","53ead55c":"code","bf14c785":"code","0cbadfec":"code","cc1d9f53":"code","b5fffb18":"code","d4414303":"code","d40bbb33":"code","896fba2d":"code","e4cbb79c":"code","e56a1545":"code","b91c782f":"code","022a05db":"code","db0ca4bd":"code","b659e4a3":"code","0e5cef28":"code","e3d3cf20":"code","cf3643d9":"code","f9201654":"code","f70c1f8c":"code","7ea1e084":"code","a0fa67d2":"code","9821e2da":"code","7e8903ee":"code","11addb2e":"code","399ac7e7":"code","77fccff6":"code","cbe420a6":"code","4d93dd32":"code","7401b818":"code","edb11b23":"code","41c6a9f8":"code","ffd69b5e":"code","e84232c5":"code","5339dafa":"code","8cd2830a":"code","265d58fe":"code","98b7991f":"code","2274ce7f":"code","c24e344e":"code","b0ae68aa":"code","d0529683":"code","7aac0c9a":"code","7e8dc33d":"code","4c2e18f7":"code","d4cb3bce":"code","10f39baa":"code","1117273d":"code","9620f661":"code","6fb823bc":"code","9a3cf9f6":"code","8e1de22c":"code","c141f9a4":"code","29723b39":"code","89d496d3":"code","0a64b9a0":"code","3c942dcb":"code","5c9a33e6":"code","5b17038d":"code","8a70c3b8":"code","c88092b6":"code","538e2936":"code","accdc859":"code","628337da":"code","c4faad08":"code","63ec8027":"code","98bdaf78":"code","185163e0":"code","ce62e231":"code","a677209b":"code","1e81e499":"code","c56734c8":"code","31675995":"code","4dc47ca0":"code","4087e906":"code","2fb70508":"code","575b950e":"code","481d3551":"code","a899acdc":"code","395df8e7":"code","5b4c53ef":"code","6ff62480":"code","83d14a9b":"code","09b1c889":"code","eb0b664a":"code","8bd8453b":"code","db4ddce2":"markdown","9efac104":"markdown","4f7075d1":"markdown","5cf27aa6":"markdown","293252a4":"markdown","49245498":"markdown","4c38cc2a":"markdown"},"source":{"ec92d99f":"# initiating gpu using tensorflow.\nimport tensorflow as tf\n#from keras.backend.tensorflow_backend import set_session\n#config = tf.ConfigProto()\n#config.gpu_options.allow_growth = True\n#config.log_device_placement = True\n\n\n#sess = tf.Session(config=config)\n#set_session(sess)","bae29f80":"#!pip install albumentations > \/dev\/null","8bf16798":"#import albumentations","32839eed":"#importing libraries for the data processing and model.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nimport random\nimport tensorflow as tf\nimport datetime\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Flatten, Activation\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.datasets import cifar10\nfrom keras.utils import np_utils\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.callbacks import EarlyStopping\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import misc\nfrom keras.models import load_model\n%matplotlib inline","f6779a86":"# defining the path and classes.\ndirectory = '..\/input\/state-farm-distracted-driver-detection\/imgs\/train'\ntest_directory = '..\/input\/state-farm-distracted-driver-detection\/imgs\/test\/'\nrandom_test = '..\/input\/driver\/'\nclasses = ['c0','c1','c2','c3','c4','c5','c6','c7','c8','c9']","6049eb6e":"# defining a shape to be used for our models.\nimg_size1 = 64\nimg_size2 = 64","65cae993":"# Train class image for display.\nfor i in classes:\n    path = os.path.join(directory,i)\n    for img in os.listdir(path):\n        print(img)\n        img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n        plt.imshow(img_array, cmap='gray')\n        plt.show()\n        break\n    break","63cc5d7c":"os.listdir(path)","34a708c8":"# Test class image for display.\ntest_array = []\nfor img in os.listdir(test_directory):\n    img_array = cv2.imread(os.path.join(test_directory,img),cv2.IMREAD_GRAYSCALE)\n    test_array = img_array\n    plt.imshow(img_array, cmap='gray')\n    plt.show()\n    break","6267c9af":"img","23c9e29e":"# checkking image size using shape.\nprint(img_array.shape)","f3202ac9":"# trying out the resize image functionality\nnew_img = cv2.resize(test_array,(img_size2,img_size1))\nplt.imshow(new_img,cmap='gray')\nplt.show()","5f8b368f":"#flipping_horizontal (8\/30add)\nhflip_img = cv2.flip(new_img, 1)\nplt.imshow(hflip_img,cmap='gray')\nplt.show()","53ead55c":"#flipping_vertical (8\/30add)\nvflip_img = cv2.flip(new_img, 0)\nplt.imshow(vflip_img,cmap='gray')\nplt.show()","bf14c785":"#flipping_horizontal and vertical (8\/30add)\nhvflip_img = cv2.flip(new_img, -1)\nplt.imshow(hvflip_img,cmap='gray')\nplt.show()","0cbadfec":"#blur (8\/30add)\nblur_img = cv2.blur(new_img, (5,5))\n\nplt.imshow(blur_img,cmap='gray')\nplt.show()","cc1d9f53":"#Gaussian blur (8\/30add)\ngau_img = cv2.GaussianBlur(new_img, (5,5), 0)\n\nplt.imshow(gau_img,cmap='gray')\nplt.show()","b5fffb18":"#median blur (8\/30add)\nmed_img = cv2.medianBlur(new_img, 5)\n\nplt.imshow(med_img,cmap='gray')\nplt.show()","d4414303":"#Binarization (8\/30add)\nret, bin_img = cv2.threshold(new_img, 128, 255, cv2.THRESH_BINARY)\n\nplt.imshow(bin_img,cmap='gray')\nplt.show()","d40bbb33":"#Erosion (8\/30add)\nkernel = np.ones((10,10), np.uint8)\nimg_el = cv2.erode(new_img, kernel, iterations=1)\n\nplt.imshow(img_el,cmap='gray')\nplt.show()","896fba2d":"#Dilation (8\/30add)\nkernel = np.ones((5,5), np.uint8)\nimg_dl = cv2.dilate(new_img, kernel, iterations=1)\n\nplt.imshow(img_dl,cmap='gray')\nplt.show()","e4cbb79c":"#Opening (8\/30add)\nkernel = np.ones((5,5), np.uint8)\nimg_op = cv2.morphologyEx(new_img, cv2.MORPH_OPEN, kernel)\n\nplt.imshow(img_op,cmap='gray')\nplt.show()","e56a1545":"#Closing (8\/30add)\nkernel = np.ones((5,5), np.uint8)\nimg_cl = cv2.morphologyEx(new_img, cv2.MORPH_CLOSE, kernel)\n\nplt.imshow(img_cl,cmap='gray')\nplt.show()","b91c782f":"#GaussianNoise (8\/30add)\ndef addGaussianNoise(new_img):\n    row,col= new_img.shape\n    mean = 0\n    var = 0.1\n    sigma = 100\n    gauss = np.random.normal(mean,sigma,(row,col))\n    gauss = gauss.reshape(row,col)\n    noisy = new_img + gauss\n\n    return noisy\n\n\ngau_noi_img = addGaussianNoise(new_img)\nplt.imshow(gau_noi_img,cmap='gray')\nplt.show()","022a05db":"#Salt_Pepper_Noise (8\/30add)\ndef add_Salt_Pepper_Noise(new_img, s_vs_p = 0.5, amount = 0.05):\n    row,col = new_img.shape\n    s_and_p = np.copy(new_img)\n    # Salt mode\n    num_salt = np.ceil(amount * new_img.size * s_vs_p)\n    coords = [np.random.randint(0, i - 1, int(num_salt))\n              for i in new_img.shape]\n    s_and_p[coords] = 1\n\n    # Pepper mode\n    num_pepper = np.ceil(amount* new_img.size * (1. - s_vs_p))\n    coords = [np.random.randint(0, i - 1, int(num_pepper))\n              for i in new_img.shape]\n    s_and_p[coords] = 0\n    return s_and_p\n\nsap_noi_img = add_Salt_Pepper_Noise(new_img, s_vs_p = 0.5, amount = 0.05)\nplt.imshow(sap_noi_img,cmap='gray')\nplt.show()","db0ca4bd":"#NegaPosiDiverse (8\/30add)\nngp_img = cv2.bitwise_not(new_img)\n\nplt.imshow(ngp_img,cmap='gray')\nplt.show()","b659e4a3":"#Canny (8\/30add)\ncanny_img = cv2.Canny(new_img, 200, 200)\n\nplt.imshow(canny_img,cmap='gray')\nplt.show()","0e5cef28":"#Rotation\nheight,width = new_img.shape[:2]\ncenter = (int(width\/2), int(height\/2)) # \u4e2d\u5fc3\u70b9\nangle = 45 # \u5de6\u56de\u8ee2\nM = cv2.getRotationMatrix2D(center, angle, 1)\nrotated_img = cv2.warpAffine(new_img, M, (width, height))\nplt.imshow(rotated_img,cmap='gray')\nplt.show()","e3d3cf20":"#Shifted\nmoving_x = -10\nmoving_y = -10\nM = np.float32([[1, 0, moving_x], [0, 1, moving_y]])\nshifted_img = cv2.warpAffine(new_img, M, (width, height))\nplt.imshow(shifted_img,cmap='gray')\nplt.show()","cf3643d9":"os.listdir(test_directory)","f9201654":"img","f70c1f8c":"test_directory","7ea1e084":"try_img_size1 = 128\ntry_img_size2 = 128","a0fa67d2":"img = 'img_58997.jpg'\n\ntest_array = []\nimg_array = cv2.imread(os.path.join(test_directory,img),cv2.IMREAD_GRAYSCALE)\ntest_array = img_array\nplt.imshow(img_array, cmap='gray')\nplt.show()","9821e2da":"# trying out the resize image functionality\nnew_img = cv2.resize(test_array,(try_img_size2,try_img_size1))\nplt.imshow(new_img,cmap='gray')\nplt.show()","7e8903ee":"#Canny (8\/30add)\ncanny_img = cv2.Canny(new_img, 150, 150)\n\nplt.imshow(canny_img,cmap='gray')\nplt.show()","11addb2e":"#NegaPosiDiverse (8\/30add)\nngp_img = cv2.bitwise_not(new_img)\n\nplt.imshow(ngp_img,cmap='gray')\nplt.show()","399ac7e7":"#Binarization (8\/30add)\nret, bin_img = cv2.threshold(new_img, 128, 255, cv2.THRESH_BINARY)\n\nplt.imshow(bin_img,cmap='gray')\nplt.show()","77fccff6":"#NegaPosi -> GaussianNoise (8\/30add)\ngau_noi_img = addGaussianNoise(ngp_img)\nplt.imshow(gau_noi_img,cmap='gray')\nplt.show()","cbe420a6":"#NegaPosi -> Salt and Pepper (8\/30add)\nsap_noi_img = add_Salt_Pepper_Noise(ngp_img, s_vs_p = 0.5, amount = 0.15)\nplt.imshow(sap_noi_img,cmap='gray')\nplt.show()","4d93dd32":"#Binarization (8\/30add)\nret, bin_img = cv2.threshold(new_img, 128, 255, cv2.THRESH_BINARY)\n\nsap_noi_img = add_Salt_Pepper_Noise(bin_img, s_vs_p = 0.5, amount = 0.10)\nplt.imshow(sap_noi_img,cmap='gray')\nplt.show()","7401b818":"# creating a training dataset.\ntraining_data = []\ni = 0\ndef create_training_data():\n    for category in classes:\n        path = os.path.join(directory,category)\n        class_num = classes.index(category)\n        \n        for img in os.listdir(path):\n            img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n            new_img = cv2.resize(img_array,(img_size2,img_size1))\n            \n            #Additional preprocessing(8\/30)\n            #Binarization (8\/30add)\n            ret, bin_img = cv2.threshold(new_img, 128, 255, cv2.THRESH_BINARY)           \n            training_data.append([\n                bin_img,class_num])\n            #Binarization\u3000-> Salt and Pepper\n            sap_img = add_Salt_Pepper_Noise(bin_img, s_vs_p = 0.5, amount = 0.01)\n            training_data.append([\n                sap_img,class_num])\n            #Binarization\u3000-> Salt and Pepper\n            sap_img = add_Salt_Pepper_Noise(bin_img, s_vs_p = 0.5, amount = 0.03)\n            training_data.append([\n                sap_img,class_num])\n            #Binarization\u3000-> Salt and Pepper\n            sap_img = add_Salt_Pepper_Noise(bin_img, s_vs_p = 0.5, amount = 0.05)\n            training_data.append([\n                sap_img,class_num])\n            #Rotation\n            height,width = new_img.shape[:2]\n            center = (int(width\/2), int(height\/2)) # \u4e2d\u5fc3\u70b9\n            angle = 20 # \u5de6\u56de\u8ee2\n            M = cv2.getRotationMatrix2D(center, angle, 1)\n            rotated_img = cv2.warpAffine(bin_img, M, (width, height))\n            training_data.append([\n                rotated_img,class_num])\n            #Shifted\n            moving_x = -10\n            moving_y = -10\n            M = np.float32([[1, 0, moving_x], [0, 1, moving_y]])\n            shifted_img = cv2.warpAffine(bin_img, M, (width, height))\n            training_data.append([\n                shifted_img,class_num])","edb11b23":"for category in classes:\n    path = os.path.join(directory,category)\n    class_num = classes.index(category)\n        \n    for img in os.listdir(path):\n        img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n        new_img = cv2.resize(img_array,(img_size2,img_size1))\n        training_data.append([\n            new_img,class_num])\n        \n        print('path     :', path)\n        print('img      :', img)\n        print('img_array:', img_array)\n        print('img_array_shape:', img_array.shape)\n        print('new_img  :', new_img)\n        print('new_img_shape:', new_img.shape)\n        print('class_num:', class_num)\n        \n        break","41c6a9f8":"training_data","ffd69b5e":"training_data = []\ni = 0","e84232c5":"# Creating a test dataset.\ntesting_data = []\ni = 0\ndef create_testing_data():        \n    for img in os.listdir(test_directory):\n        img_array = cv2.imread(os.path.join(test_directory,img),cv2.IMREAD_GRAYSCALE)\n        new_img = cv2.resize(img_array,(img_size2,img_size1))\n        #Binarization (8\/30add)\n        ret, bin_img = cv2.threshold(new_img, 128, 255, cv2.THRESH_BINARY)          \n        testing_data.append([img,\n            bin_img])","5339dafa":"for img in os.listdir(test_directory):\n    img_array = cv2.imread(os.path.join(test_directory,img),cv2.IMREAD_GRAYSCALE)\n    new_img = cv2.resize(img_array,(img_size2,img_size1))\n    testing_data.append([img,\n                         new_img])\n    \n    print('test_directory     :', test_directory)\n    print('img      :', img)\n    print('img_array:', img_array)\n    print('img_array_shape:', img_array.shape)\n    print('new_img  :', new_img)\n    print('new_img_shape:', new_img.shape)\n        \n    break    ","8cd2830a":"testing_data","265d58fe":"testing_data = []\ni = 0","98b7991f":"import time\nstart = time.time()\ncreate_training_data()\nprint('Elapsed_time: ', time.time()-start, '[sec]')","2274ce7f":"start = time.time()\ncreate_testing_data()\nprint('Elapsed_time: ', time.time()-start, '[sec]')","c24e344e":"print('training_data.size:', len(training_data))\nprint('testing_data.size :', len(testing_data))","b0ae68aa":"random.shuffle(training_data)","d0529683":"x = []\ny = []","7aac0c9a":"for features, label in training_data:\n    x.append(features)\n    y.append(label)","7e8dc33d":"print('features: ', x[0])\nprint('label   : ', y[0])","4c2e18f7":"x[0].shape","d4cb3bce":"len(x)","10f39baa":"X = np.array(x).reshape(-1,img_size2,img_size1,1)\nX.shape,X[0].shape","1117273d":"np.array(x).shape","9620f661":"X[0]","6fb823bc":"x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=50)","9a3cf9f6":"Y_train = np_utils.to_categorical(y_train,num_classes=10)\nY_test = np_utils.to_categorical(y_test,num_classes=10)","8e1de22c":"model = Sequential()","c141f9a4":"model.add(Conv2D(32,kernel_size=(3,3),activation='relu',input_shape=(img_size1,img_size2,1)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32,kernel_size=(3,3),activation='relu',padding='same'))\nmodel.add(BatchNormalization(axis = 3))\nmodel.add(MaxPooling2D(pool_size=(2,2),padding='same'))\nmodel.add(Dropout(0.3))","29723b39":"model.add(Conv2D(64,kernel_size=(3,3),activation='relu',padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64,kernel_size=(3,3),activation='relu',padding='same'))\nmodel.add(BatchNormalization(axis = 3))\nmodel.add(MaxPooling2D(pool_size=(2,2),padding='same'))\nmodel.add(Dropout(0.3))","89d496d3":"model.add(Conv2D(128,kernel_size=(3,3),activation='relu',padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128,kernel_size=(3,3),activation='relu',padding='same'))\nmodel.add(BatchNormalization(axis = 3))\nmodel.add(MaxPooling2D(pool_size=(2,2),padding='same'))\nmodel.add(Dropout(0.5))","0a64b9a0":"model.add(Flatten())\nmodel.add(Dense(units = 512,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(units = 128,activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(10,activation='softmax'))","3c942dcb":"model.summary()","5c9a33e6":"model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')","5b17038d":"callbacks = [EarlyStopping(monitor='val_acc',patience=5)]","8a70c3b8":"batch_size = 50\nn_epochs = 20","c88092b6":"results = model.fit(x_train,Y_train,batch_size=batch_size,epochs=n_epochs,verbose=1,validation_data=(x_test,Y_test),callbacks=callbacks)","538e2936":"# Plot training & validation accuracy values\nplt.plot(results.history['accuracy'])\nplt.plot(results.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(results.history['loss'])\nplt.plot(results.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","accdc859":"preds = model.predict(np.array(testing_data[0][1]).reshape(-1,img_size2,img_size1,1))","628337da":"model.save_weights('.\/driverdistraction_lr_weights.h5', overwrite=True)","c4faad08":"model.save('.\/driverdistraction_lr_weights.h5')","63ec8027":"loaded_model = load_model('.\/driverdistraction_lr_weights.h5')","98bdaf78":"test_data = np.array(testing_data[1001][1]).reshape(-1,img_size2,img_size1,1)","185163e0":"test_data.shape","ce62e231":"preds = model.predict(test_data)\n#preds= np.argmax(preds)\npreds","a677209b":"preds= np.argmax(preds)\npreds","1e81e499":"classes = {0: \"safe driving\",\n1: \"texting - right\",\n2: \"talking on the phone - right\",\n3: \"texting - left\",\n4: \"talking on the phone - left\",\n5: \"operating the radio\",\n6: \"drinking\",\n7: \"reaching behind\",\n8: \"hair and makeup\",\n9: \"talking to passenger\",\n}\n\n\nfor key,value in classes.items():\n    if preds==key:\n        predicted = value\n\npredicted     ","c56734c8":"print(predicted)\nnew_img = cv2.resize(testing_data[1000][1],(img_size2,img_size1))\nplt.imshow(new_img,cmap='gray')\nplt.show()","31675995":"testing_data","4dc47ca0":"x_test=[]\ny_test=[]\n\nfor test_id, feature in testing_data:\n    x_test.append(feature)\n    y_test.append(test_id)","4087e906":"print('features: ', x_test[0])\nprint('test_id : ', y_test[0])","2fb70508":"X_test = np.array(x_test).reshape(-1,img_size2,img_size1,1)\nX_test.shape,X_test[0].shape","575b950e":"np.array(x).shape","481d3551":"X_test[0]","a899acdc":"preds = model.predict(X_test)","395df8e7":"preds","5b4c53ef":"preds.shape","6ff62480":"def create_submission(predictions, test_id, info):\n    result1 = pd.DataFrame(predictions, columns=['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'])\n    result1.loc[:, 'img'] = pd.Series(test_id, index=result1.index)\n    result1 = result1.sort_values(['img'])\n\n    result1.to_csv(index=False)\n    return result1","83d14a9b":"info = '200824'\nsubmission = create_submission(preds, y_test, info)","09b1c889":"submission.head()","eb0b664a":"now = datetime.datetime.now()\n\nif not os.path.isdir('subm'):\n    os.mkdir('subm')\nsuffix = info + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\"))\nsub_file = os.path.join('subm', 'submission_' + suffix + '.csv')","8bd8453b":"submission.to_csv('submission.csv', index=False)","db4ddce2":"## Investigate the best practice for this task\n### hlflip is not necessary because the direction of driver is the same\n### In my opinion, Canny , negaposi, binaryzation are better than others.","9efac104":"np.array.reshape(-1)\u3068\u306f\n\nhttps:\/\/qiita.com\/yosshi4486\/items\/deb49d5a433a2c8a8ed4","4f7075d1":"# Albamentation","5cf27aa6":"# Pre-Processing\n\n## Training data -> x4\n## a. Original, b. Negaposi, c. Original + SandP, d Negaposi + SandP\n## Test data -> no preprocessing","293252a4":"# Check preprocessing techniques","49245498":"# Investigation of Data preprocessing","4c38cc2a":"\n    c0: safe driving\n    c1: texting - right\n    c2: talking on the phone - right\n    c3: texting - left\n    c4: talking on the phone - left\n    c5: operating the radio\n    c6: drinking\n    c7: reaching behind\n    c8: hair and makeup\n    c9: talking to passenger\n"}}