{"cell_type":{"190441bd":"code","7016f2fa":"code","87676923":"code","ac98bc63":"code","80fa46fd":"code","7b221f38":"code","8f0bae76":"code","5226b207":"code","daaeac81":"code","ba40574d":"code","6c970f95":"code","ff5ecec0":"code","d1b5231b":"code","ad43d6d1":"code","02f590c6":"code","afa0506a":"code","52046ff8":"code","6e8ee2d8":"code","ef05f5f9":"code","cf4f0ce5":"code","2f15439c":"code","980c5d37":"code","11843827":"code","0763a8a9":"code","71ac092e":"code","1f9df4df":"code","03ab047c":"code","b15020f0":"code","4597ba97":"code","150db34e":"code","a66bea0f":"code","1947117e":"code","69b816e5":"code","1eb68f09":"code","1fbe496b":"code","5489e565":"code","f575e6f1":"code","d30c63d1":"code","a5ddd4bd":"code","2319cd80":"code","fab80dc0":"code","5ff27def":"markdown","4396d326":"markdown","b8b5cccb":"markdown","97a80dfa":"markdown","48bf7481":"markdown","43b6daa8":"markdown","c2aafa9d":"markdown","174e968c":"markdown"},"source":{"190441bd":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nfrom fastai.vision import *","7016f2fa":"bs = 64","87676923":"train=pd.read_csv('..\/input\/labels.csv')\ntrain.head()","ac98bc63":"train.id = train.id+'.jpg'","80fa46fd":"train.head()","7b221f38":"tfms = get_transforms(max_rotate = 20, max_zoom=1.3, max_lighting=0.4, max_warp=0.4, p_affine = 1., p_lighting=1.)","8f0bae76":"src = (ImageList.from_df(train, '..\/input\/', folder='train')\n      .split_by_rand_pct()\n      .label_from_df(cols = 'breed'))","5226b207":"def get_data(size, bs, padding_mode = 'reflection'):\n    return(src.transform(tfms, size = size, padding_mode=padding_mode)\n          .databunch(bs=bs, num_workers = 0).normalize(imagenet_stats))","daaeac81":"data = get_data(224, bs, 'zeros')","ba40574d":"def _plot(i,j,ax):\n    x,y = data.train_ds[4]\n    x.show(ax, y=y)\n    \nplot_multi(_plot, 3, 3, figsize=(8,8))","6c970f95":"gc.collect()\nlearn = cnn_learner(data, models.resnet34, \n                    metrics = error_rate, bn_final = True, \n                    model_dir = \"\/tmp\/model\")","ff5ecec0":"learn.fit_one_cycle(3, slice(1e-2), pct_start=0.8)","d1b5231b":"learn.unfreeze()\nlearn.fit_one_cycle(2, max_lr=slice(1e-6,1e-3), pct_start=0.8)","ad43d6d1":"data = get_data(352,bs)\nlearn.data = data","02f590c6":"learn.fit_one_cycle(2, max_lr=slice(1e-6,1e-4))","afa0506a":"learn.save('352')","52046ff8":"data = get_data(352, 16)","6e8ee2d8":"learn = cnn_learner(data, models.resnet34, metrics=error_rate, bn_final=True, model_dir = '\/tmp\/model').load('352')","ef05f5f9":"#Right Sobel kernel\nk = tensor ([\n    [-1., 0, 1],\n    [-2., 0, 2],\n    [-1., 0 , 1],\n]).expand(1,3,3,3)\/6","cf4f0ce5":"#Sharpen Kernel\nk = tensor ([\n    [0., -1, 0],\n    [-1, 5, -1],\n    [0, -1 , 0],\n]).expand(1,3,3,3)","2f15439c":"k","980c5d37":"k.shape","11843827":"idx = 2\nt = data.valid_ds[idx][0].data; t.shape #Pull out a single image sample, 0th is image and 1st is label","0763a8a9":"t[None].shape #t[None] is a trick to get a mini-batch of a tensor of size 1, this also works in numpy","71ac092e":"edge = F.conv2d(t[None], k)","1f9df4df":"show_image(edge[0], figsize = (5,5))","03ab047c":"x,y = data.valid_ds[idx]\nx.show()\ndata.valid_ds.y[idx]","b15020f0":"#Number of categories\ndata.c","4597ba97":"#Details of the resnet model \nlearn.model\n#theres a lot going on in the first Conv layer, but there are 64 chanels and a stride of 2 for the first layer\n#When you stride by 2 you can double the number of chanels (this preserves the complexity of model\/memory)","150db34e":"print(learn.summary())","a66bea0f":"m = learn.model.eval();","1947117e":"xb,_ = data.one_item(x) #takes all the settings from our previously created data object\nxb_im = Image(data.denorm(xb)[0])\nxb = xb.cuda()","69b816e5":"from fastai.callbacks.hooks import *","1eb68f09":"??hook_output","1fbe496b":"def hooked_backward(cat=y):\n    with hook_output(m[0]) as hook_a: #Get activations from the convolution layers\n        with hook_output(m[0], grad = True) as hook_g: #Get gradient from convolution layers\n            preds = m(xb) #DO foreward pass through model\n            preds[0,int(cat)].backward()\n    return hook_a, hook_g","5489e565":"hook_a, hook_g = hooked_backward()","f575e6f1":"hook_a.stored","d30c63d1":"acts = hook_a.stored[0].cpu()\nacts.shape #Now we see our 512 chanels over the 11x11 sections of the image","a5ddd4bd":"avg_acts = acts.mean(0)\navg_acts.shape","2319cd80":"def show_heatmap(hm):\n    _,ax = plt.subplots()\n    xb_im.show(ax) #fastai function to show the image\n    ax.imshow(hm, alpha = 0.6, extent = (0,352,352, 0), #extent expands the 11x11 image to 352,352\n             interpolation = 'bilinear', cmap = 'magma')","fab80dc0":"show_heatmap(avg_acts)","5ff27def":"## Convolution kernel","4396d326":"## Create Heatmap\n\nThe images get boiled down into 11 'sections' of the image through various stride-ings and 512 kernel based channels of for instance (how fluffy is it, how long are ears, etc)\n* So for each 11x11 image, there's an activation for each part of the image for each of the 512 features\n* The network determines itself what are the features based on the optimization of the model\n* If we take the average of all the 512 features we can determine how activated each of the 11x11 parts of the images are","b8b5cccb":"## Train the model","97a80dfa":"Here we're taking the output of the convolutional layers found in ResNet34, which in the model we've created is found with the m[0] part of the model\n\nhook_output is a fastai module that pulls the output of (in this case m[0]) out of the pytorch back end","48bf7481":"Before getting heatmap we need to:\n * Put data into pytorch databunch (in this case a single item mini batch)\n * Normalzie the image\n * Put it on the GPU","43b6daa8":"Hook allows you to hook into the pytorch machinary itself and run any python code you want","c2aafa9d":"This visualization of the convolutinoal layer shows how a convolution can identify an edge\/corner, etc","174e968c":"Create a convolutional layer by hand. Take a standard 3 x 3 matrix that is going to look for lower right hand cornrers and turn it into a 3d convolutional layer with the .expand command. \n\nCan play around with these convolutinoal layers to see how they interact with the images\n * Make sure to include decimal in first number to make the type of data in kernel floats\n \n [Good Resource for Convolution Visualization](http:\/\/setosa.io\/ev\/image-kernels\/)"}}