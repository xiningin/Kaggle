{"cell_type":{"9627423a":"code","5752eafd":"code","a22f1e0a":"code","ae1f9444":"code","781bb3bd":"code","800ace5f":"code","194611bf":"code","025d5d9d":"code","736961fc":"code","75b683f9":"code","5e91f442":"code","45d753d1":"code","c424555a":"code","736fa896":"code","8557b984":"code","78510c42":"code","91d0a077":"code","1e96c909":"code","0f059461":"code","18d97b5d":"code","15946da9":"code","65c917f5":"code","9c2b6ed8":"code","4a5a2ccc":"code","7792d138":"code","32a58ce8":"code","ce92857d":"code","dbb8c4d1":"code","a5a67f14":"code","11b415f5":"markdown","ccc13345":"markdown","87233895":"markdown","659a46b5":"markdown","a3d195ef":"markdown"},"source":{"9627423a":"import logging\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport random\nimport re\nimport nltk\nimport pprint\nfrom nltk.corpus import stopwords\nfrom IPython.core.display import display, HTML\nfrom sklearn.feature_extraction.text  import TfidfVectorizer \nfrom sklearn.feature_extraction.text  import CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom gensim.models import word2vec","5752eafd":"display(HTML(\"\"\"\n<button id='toggleInput'>Hide Input<\/button> \n<script>\n\tvar $toggleInput = document.querySelector('#toggleInput')\n\t$toggleInput.addEventListener('click', function() {\n\t\tif($toggleInput.innerText === 'Hide Input') {\n\t\t\t$toggleInput.innerText = 'Show Input';\n\t\t\tdocument.querySelectorAll('.input').forEach($input => $input.style.display = 'none');\n\t\t} else {\n\t\t\t$toggleInput.innerText = 'Hide Input';\n\t\t\tdocument.querySelectorAll('.input').forEach($input => $input.style.display = 'flex');\n\t\t}\n\t});\n<\/script>\n\"\"\"))","a22f1e0a":"display(HTML(\"\"\"<h2 id='inputs'>Inputs<\/h2>\"\"\"))\n","ae1f9444":"data_file_name = '..\/input\/heckyeh\/output.tsv'\npercent_training = 0.75\nsentiment_cutoff = 7\nvocab_size = 2000\nn_estimators = 100","781bb3bd":"display(HTML(\"\"\"<h2 id='dataCollection'>Data Collection<\/h2>\"\"\"))\n","800ace5f":"data = pd.read_csv(data_file_name, delimiter='\\t')\npd.options.display.max_colwidth = 10000000\ndata.head(10)","194611bf":"\nprint('this is the number of rows: %d , and this is the number of columns: %d' % data.shape)\n","025d5d9d":"display(HTML(\"\"\"<h2 id='featureExtraction'>Feature Extraction<\/h2>\"\"\"))\n","736961fc":"punktTokenizer = nltk.data.load('tokenizers\/punkt\/english.pickle')\n\ndef get_words(sentence):\n    clean_sentence = re.sub(\"[^a-z]\",\" \", sentence.lower())\n    return [word for word in clean_sentence.split(' ') if len(word) > 0]\n\ndef get_sentences(review):\n    clean_review = review.strip()\n    return [get_words(sentence) for sentence in punktTokenizer.tokenize(clean_review) if len(sentence) > 0 ]\n\nsentences = []\nfor review in data[\"review\"]:\n    sentences += get_sentences(review)\n\nprint(\"THERE ARE %s SENTENCES\" % len(sentences))","75b683f9":"logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\nlevel=logging.INFO)\n\nnum_features = 100# Word vector dimensionality                      \nmin_word_count = 2   # Minimum word count                    \nnum_workers = 4   # Number of threads to run in parallel\ncontext = 10      # Context window size                                                                                    \ndownsampling = 1e-3   # Downsample setting for frequent words\n\nprint(\"Training model...\")\nmodel = word2vec.Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count,window = context, sample = downsampling)\nmodel.init_sims(replace=True)\n\nmodel_name = \"imdb\"\nmodel.save(model_name)","5e91f442":"model.doesnt_match('good bad cool awesome stomach'.split(' '))\n","45d753d1":"data['rating'].replace('', np.nan, inplace=True)\ndata.dropna(subset=['rating'], inplace=True)\nprint('this is the number of rows: %d , and this is the number of columns: %d' % data.shape)","c424555a":"words_to_remove = stopwords.words(\"english\")\n\nextra_words = ['film','movie', 'one', 'story', 'see', 'ring', 'also']\nwords_to_remove += extra_words\n\nprint(words_to_remove)\n\n","736fa896":"\n\ninteresting_words = data['review'].map(lambda r: [re.sub(\"[^a-zA-Z]\", \"\", word) for word in r.lower().split() if not word in words_to_remove])\ninteresting_words = interesting_words.map(lambda words: [w for w in words if not w in words_to_remove])\ninteresting_words = interesting_words.map(lambda words: [w for w in words if re.sub('[aeiouy]', '', w) != w])\n\ndata['review'] = interesting_words.map(lambda words: [w for w in words if len(w)])\n\nprint(data['review'])","8557b984":"data = data.assign(sentiment=data['rating'].map(lambda r: 1 if r >= sentiment_cutoff else 0))\ndata[data['rating'] == 8].head(10)","78510c42":"display(HTML(\"\"\"<h2 id='trainingDataSelection'>Training Data Selection<\/h2>\"\"\"))\n","91d0a077":"row_count = int(percent_training*float(data.shape[0]))\ntraining_data = data.sample(n=row_count)\ntraining_data.info()","1e96c909":"display(HTML(\"\"\"<h2 id='testingDataSelection'>Testing Data Selection<\/h2>\"\"\"))\n","0f059461":"training_data_ids = [id for id in training_data.id]\ntesting_data = data[~data['id'].isin(training_data_ids)]\ntesting_data.info()","18d97b5d":"count_vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None,stop_words = None, max_features = vocab_size)\nall_words = []\nfor word_list in training_data['review']:\n    all_words.append(' '.join(word_list))\n\ncount_train_data_features = count_vectorizer.fit_transform(all_words).toarray()\nprint(count_train_data_features.shape)","15946da9":"tfidf_vectorizer = TfidfVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None,stop_words = None, max_features = vocab_size)\nall_words = []\nfor word_list in training_data['review']:\n    all_words.append(' '.join(word_list))\n\ntfidf_train_data_features = tfidf_vectorizer.fit_transform(all_words).toarray()\nprint(tfidf_train_data_features.shape)\n","65c917f5":"display(HTML(\"\"\"<h2 id='modelTraining'>Model Training<\/h2>\"\"\"))\n","9c2b6ed8":"count_vocab = count_vectorizer.get_feature_names()\nprint(vocab)\nprint('There are %d number of words in our vocab' % len(count_vocab))\n\ndist = np.sum(count_train_data_features, axis=0)\n\nhist = [];\nfor tag, count in zip(count_vocab, dist):\n    hist.append((count, tag))\n    \nprint(\"The top ten word list is \")\n\n# IMPORT pprint IN THE IMPORT BLOCK\npprint.pprint(sorted(hist, key= lambda x : x[0], reverse=True)[0:10])\n\nprint (\"Training the random forest...\")\n\ncount_forest = RandomForestClassifier(n_estimators=n_estimators) \n\ncount_forest = forest.fit(count_train_data_features, training_data[\"sentiment\"] )\nprint (\"Trained the random forest model\")","4a5a2ccc":"tfidf_vocab = tfidf_vectorizer.get_feature_names()\nprint(tfidf_vocab)\nprint('There are %d number of words in our vocab' % len(tfidf_vocab))\n\ndist = np.sum(tfidf_train_data_features, axis=0)\n\nhist = [];\nfor tag, count in zip(tfidf_vocab, dist):\n    hist.append((count, tag))\n    \nprint(\"The top ten word list is \")\n\n# IMPORT pprint IN THE IMPORT BLOCK\npprint.pprint(sorted(hist, key= lambda x : x[0], reverse=True)[0:10])\n\nprint (\"Training the random forest...\")\n\ntfidf_forest = RandomForestClassifier(n_estimators=n_estimators) \n\ntfidf_forest = tfidf_forest.fit(tfidf_train_data_features, training_data[\"sentiment\"] )\nprint (\"Trained the random forest model\")","7792d138":"display(HTML(\"\"\"<h2 id='predictionAnalysis'>Prediction Analysis<\/h2>\"\"\"))\n","32a58ce8":"test_words = []\nfor word_list in testing_data['review']:\n    test_words.append(' '.join(word_list))\n\ncount_test_data_features = count_vectorizer.transform(test_words).toarray()\ntfidf_test_data_features = count_vectorizer.transform(test_words).toarray()\n\nprint(count_test_data_features.shape)","ce92857d":"count_result = count_forest.predict(count_test_data_features)\nprint(count_result)\ntfidf_result = tfidf_forest.predict(tfidf_test_data_features)\nprint(tfidf_result)","dbb8c4d1":"count_output = pd.DataFrame(data ={\"actual\": testing_data['sentiment'], 'predicted':count_result})\n\ntrue_positives = 0\ntrue_negatives = 0\nfalse_positives = 0\nfalse_negatives = 0\n\n\nactual = count_output['actual'].tolist()\npredicted = count_output['predicted'].tolist()\neventCount = float(count_output.shape[0]);\n\nfor x in range(count_output.shape[0]):\n    a = actual[x]\n    p = predicted[x]\n    if a == 1 and p == 1:\n        true_positives += 1\n    elif a == 1 and p == 0:\n        false_negatives += 1\n    elif a == 0 and p == 1:\n        false_positives += 1\n    elif a == 0 and p == 0:\n        true_negatives += 1\n\ndisplay(HTML(\"\"\"\n    <h3>Confusion Matrix<\/h3>\n    <table>\n        <tr>\n            <th>Confusion Matrix Cell<\/th><th>Term<\/th><th>Value<\/th>\n        <\/tr>\n        <tr>\n            <th>True Positive<\/th><td>Sensitivity<\/td><td>%1.2f<\/td>\n        <\/tr>\n        <tr>\n            <th>False Positive<\/th><td>Fall-Out Rate<\/td><td>%1.2f<\/td>\n        <\/tr>\n        <tr>\n            <th>False Negative<\/th><td>Miss Rate<\/td><td>%1.2f<\/td>\n        <\/tr>\n        <tr>\n            <th>True Negative<\/th><td>Specificity<\/td><td>%1.2f<\/td>\n        <\/tr>\n    <\/table>\n\"\"\" % (float(true_positives) \/ eventCount, float(false_negatives) \/eventCount, float(false_positives)\/eventCount,float(true_negatives)\/eventCount)))","a5a67f14":"tfidf_output = pd.DataFrame(data ={\"actual\": testing_data['sentiment'], 'predicted':tfidf_result})\n\ntrue_positives = 0\ntrue_negatives = 0\nfalse_positives = 0\nfalse_negatives = 0\n\n\nactual = tfidf_output['actual'].tolist()\npredicted = tfidf_output['predicted'].tolist()\neventCount = float(tfidf_output.shape[0]);\n\nfor x in range(tfidf_output.shape[0]):\n    a = actual[x]\n    p = predicted[x]\n    if a == 1 and p == 1:\n        true_positives += 1\n    elif a == 1 and p == 0:\n        false_negatives += 1\n    elif a == 0 and p == 1:\n        false_positives += 1\n    elif a == 0 and p == 0:\n        true_negatives += 1\n\ndisplay(HTML(\"\"\"\n    <h3>Confusion Matrix<\/h3>\n    <table>\n        <tr>\n            <th>Confusion Matrix Cell<\/th><th>Term<\/th><th>Value<\/th>\n        <\/tr>\n        <tr>\n            <th>True Positive<\/th><td>Sensitivity<\/td><td>%1.2f<\/td>\n        <\/tr>\n        <tr>\n            <th>False Positive<\/th><td>Fall-Out Rate<\/td><td>%1.2f<\/td>\n        <\/tr>\n        <tr>\n            <th>False Negative<\/th><td>Miss Rate<\/td><td>%1.2f<\/td>\n        <\/tr>\n        <tr>\n            <th>True Negative<\/th><td>Specificity<\/td><td>%1.2f<\/td>\n        <\/tr>\n    <\/table>\n\"\"\" % (float(true_positives) \/ eventCount, float(false_negatives) \/eventCount, float(false_positives)\/eventCount,float(true_negatives)\/eventCount)))","11b415f5":"### Grabbing Sentences","ccc13345":"### Removing Stopwords","87233895":"homework:\nimprove things by removing more words in addition to the stop words","659a46b5":"### Removing all rows with no ratings","a3d195ef":"# HIYA THISTORY \n# Creating a notebook to create and analyze a given model to predict the sentiment of a given movie based on its reviews\n\n## Procedure\n1. <a href='#inputs'>Inputs <\/a>\n2. <a href='#dataCollection'>Data Collection <\/a>\n3. <a href='#featureExtraction'>Feature Extraction<\/a>\n4. <a href='#trainingDataSelection'>Training Data Selection<\/a>\n5. <a href='#testingDataSelection'>Testing Data Selection<\/a>\n6. <a href='#modelTraining'>Model Training<\/a>\n7. <a href='#predictionAnalysis'>Prediction Analysis<\/a>"}}