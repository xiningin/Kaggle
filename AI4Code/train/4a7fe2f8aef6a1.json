{"cell_type":{"9a61725f":"code","4d29bdc6":"code","bfe0ec89":"code","91a3ea2b":"code","59adcf4f":"code","ac99c8a2":"code","42a32bba":"code","94ea8056":"code","12e42ff3":"code","5741b429":"code","fc8b3fb6":"code","f502761d":"code","42ecffea":"code","17524876":"code","aa698f0c":"code","96b762b8":"code","607ae14e":"code","6d053020":"markdown"},"source":{"9a61725f":"import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport tensorflow as tf\nprint(\"Tensorflow version \" + tf.__version__)\nAUTO = tf.data.experimental.AUTOTUNE\n\nfrom kaggle_datasets import KaggleDatasets\nfrom matplotlib import image\nfrom sklearn.model_selection import train_test_split","4d29bdc6":"gcs_path = KaggleDatasets().get_gcs_path('cactus-tfrec')\n!gsutil ls $gcs_path","bfe0ec89":"img_size=(331,331)\nimg_shape=img_size+(3,)\nbatch_size=128\nclasses=['no_cactus','cactus'] #0,1","91a3ea2b":"from tensorflow.io import FixedLenFeature, VarLenFeature\n\ndef data_augment(image, label=None):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    \n    if label is None:\n        return image\n    else:\n        return image, label\n    \ndef read_tfrecord(example):\n    feature_description = {\n        'image': FixedLenFeature([], tf.string),\n        'label': FixedLenFeature([], tf.int64),\n    #     'class_name': FixedLenFeature([], tf.string),\n    }\n    example = tf.io.parse_single_example(example, feature_description)\n    image = tf.image.decode_jpeg(example['image'], channels=3) # pixel format uint8 [0,255] range\n    image = tf.image.resize(image, img_size)\n    image=tf.cast(image, tf.uint8)\n    image=tf.cast(image, tf.float32)\n    class_label = tf.cast(example['label'], tf.int32) # not used\n    return image, class_label\n\ndef load_dataset(filenames):\n    ignore_order = tf.data.Options()\n    ignore_order.experimental_deterministic = False\n\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO)\n    return dataset","59adcf4f":"paths={'train':gcs_path+'\/train\/*',\n    'val':gcs_path+'\/val\/*',\n      'test':gcs_path+'\/test\/*'}\ndef get_dataset(mode='train'): #train\/val\n    dataset = load_dataset(tf.io.gfile.glob(paths[mode]))\n    if 'train' in mode:\n        dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n        dataset = dataset.repeat()\n        dataset = dataset.shuffle(2020)\n    return dataset \\\n    .batch(batch_size) \\\n    .prefetch(AUTO)","ac99c8a2":"# rw=tf.data.TFRecordDataset(tf.io.gfile.glob(paths['train'])[0])\n# plt.imshow(read_tfrecord(raw_record)[0].numpy().astype('uint8'))","42a32bba":"# dt=get_dataset('train')\ndef dataset_to_numpy_util(dataset, N):\n    dataset = dataset.unbatch().batch(N)\n    for images, labels in dataset:\n        numpy_images = images.numpy()\n        numpy_labels = labels.numpy()\n        break;  \n    return numpy_images, numpy_labels\ndef display_one_flower(image, title, subplot, red=False):\n    plt.subplot(subplot)\n    plt.axis('off')\n    plt.imshow(image.astype(\"uint8\"))\n    plt.title(title, fontsize=16, color='red' if red else 'black')\n    return subplot+1\ndef display_9_images_from_dataset(dataset):\n    subplot=331\n    plt.figure(figsize=(13,13))\n    images, labels = dataset_to_numpy_util(dataset, 9)\n    for i, image in enumerate(images):\n        title = classes[np.argmax(labels[i], axis=-1)]\n        subplot = display_one_flower(image, title, subplot)\n        if i >= 8:\n            break;\n              \n    #plt.tight_layout()\n    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n    plt.show()  \n# display_9_images_from_dataset(dt)","94ea8056":"try: # detect TPUs\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError: # no TPU found, detect GPUs\n    strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n    #strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n    #strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() # for clusters of multi-GPU machines\n\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)","12e42ff3":"train_dataset=get_dataset('train')\nval_dataset=get_dataset('val')\n\ntest_dataset=get_dataset('test')","5741b429":"!pip install -q efficientnet","fc8b3fb6":"import efficientnet.tfkeras as efn\nbase_models={}\nbase_models['resnet50']=tf.keras.applications.ResNet50\n# base_models['resnet152']=tf.keras.applications.ResNet152\n# base_models['efficient3']=efn.EfficientNetB3\n# base_models['efficient7']=efn.EfficientNetB7\n# base_models['dense']=tf.keras.applications.DenseNet169","f502761d":"with strategy.scope():\n    preprocess={}\n    preprocess['efficient']=efn.preprocess_input\n    preprocess['dense']=tf.keras.applications.densenet.preprocess_input\n    preprocess['resnet']=tf.keras.applications.resnet.preprocess_input","42ecffea":"def return_pp(net):\n    for n in preprocess.keys():\n        if n in net:\n            return preprocess[n]\n    return None","17524876":"def clone_model(pre_model):\n    model=tf.keras.models.clone_model(pre_model)\n    model.set_weights(pre_model.get_weights())\n    return model\n\ndef model_builder(bm=base_models['resnet50'],img_pp=preprocess['resnet']\n                  ,lr=1e-3,opt='adam',top_units=(0,)): #base model,lr,optimizer,(num,units),ft_lr\n    hp=[(bm,img_pp),lr,opt,top_units]\n    print('builde config: '+str(hp))\n    \n    inputs=tf.keras.Input(shape=img_shape)\n    base_model=bm(input_shape=img_shape,\n                include_top=False,\n                weights='imagenet')\n    img_adjust=hp[0][1]\n    x=img_adjust(inputs)\n    base_model.trainable=False\n    x=base_model(x,training=False)\n    global_average_layer=tf.keras.layers.GlobalAveragePooling2D()\n    prediction_layer=tf.keras.layers.Dense(1)\n    x=global_average_layer(x)\n    x=tf.keras.layers.Dropout(0.2)(x)\n    for i in range(hp[3][0]):\n        hp_units=hp[3][1]\n        x=tf.keras.layers.Dense(units=hp_units,activation='relu')(x)\n    outputs=prediction_layer(x)   \n    model=tf.keras.Model(inputs,outputs)\n\n    hp_lr=hp[1]\n    hp_opt=hp[2]\n    opt={'sgd':tf.keras.optimizers.SGD,\n          'rms':tf.keras.optimizers.RMSprop,\n          'adam':tf.keras.optimizers.Adam}\n    model.compile(optimizer=opt[hp_opt](hp_lr),\n                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n                  metrics=['accuracy'])\n    return model","aa698f0c":"def get_base(model):\n        base_model=None\n        mx=0\n        for layer in model.layers:\n            if layer.count_params()>mx:\n                mx=layer.count_params()\n                base_model=layer\n        return base_model\ndef ft(model,top_layers=40,opt='adam',lr=1e-5,epochs=30):#\n    base_model=get_base(model)\n    base_model.trainable=True\n    hp=[top_layers,opt,lr,epochs]\n    print('ft config: '+str(hp))\n\n    for layer in base_model.layers[:-hp[0]]:\n        layer.trainable=False\n\n    hp_lr=hp[2]\n    hp_opt=hp[1]\n    opt={'sgd':tf.keras.optimizers.SGD,\n          'rms':tf.keras.optimizers.RMSprop,\n          'adam':tf.keras.optimizers.Adam}\n    model.compile(optimizer=opt[hp_opt](hp_lr),\n                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n                  metrics=['accuracy'])\n\n    f_epochs=epochs\n    es_callback=tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',verbose=1,patience=f_epochs\/2, restore_best_weights=True)\n\n    total=f_epochs\n    hist_f=model.fit(train_dataset,\n                    epochs=total,\n                    validation_data=val_dataset,\n                     steps_per_epoch=steps,\n                    callbacks=[es_callback])\n    print('ft_acc: %f'%max(hist_f.history['val_accuracy']))\n    return model\nsteps=14000\/batch_size","96b762b8":"import itertools\n# grid_search\nlayers=[(1,i) for i in [4,8,16,64]]\n\nhp=[base_models.keys(),\n    [1e-3],\n    ['adam',],\n    [(0,),*layers]\n   ]\nft_hp=[\n    [10,20,40],\n    [5e-5],\n    ['adam',],\n    [10],\n    ]\ncount=0\nfor hp in itertools.product(*hp,*ft_hp):\n    hp_n=([[\n    'type_model',\n    'lr',\n    'opt',\n    'top_units',\n    ],\n[\n    'top_layers',\n    'lr',\n    'opt',\n    'epochs',\n]])\n    hpp=[dict(zip(hp_n[0],hp[:4])),dict(zip(hp_n[1],hp[4:]))] \n    count+=1\n    print('trial %d'%count)\n    print(hpp)\n    ft_hp=hpp[1]\n    hp=hpp[0]\n    type_m=hp['type_model']\n    hp['bm']=base_models[type_m]\n    hp['img_pp']=return_pp(type_m)\n    hp.pop('type_model',None)\n    with strategy.scope():\n        model=model_builder(**hp)\n        \n    print('begin training')\n    hist=model.fit(train_dataset,validation_data=val_dataset,\n              epochs=80,\n              steps_per_epoch=steps,\n              verbose=1,  \n             callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',verbose=1, patience=9,restore_best_weights=True)])\n    print('best val: %f\\neval_on_test'%max(hist.history['val_accuracy']))\n    print('test_result: loss %f, acc %f' % tuple(model.evaluate(test_dataset)))\n\n    print('begin ft')\n    model=ft(model,**ft_hp)\n    print('test_result: loss %f, acc %f' % tuple(model.evaluate(test_dataset)))","607ae14e":"def run_trial(hp=[{\n    'type_model':'efficient7',\n    'lr':1e-3,\n    'opt':'adam',\n    'top_units':(0,),\n},\n{\n    'top_layers':40,\n    'opt':'adam',\n    'lr':1e-5,\n    'epochs':30\n}\n]): \n    print(hp)\n    ft_hp=hp[1]\n    hp=hp[0]\n    type_m=hp['type_model']\n    hp['bm']=base_models[type_m]\n    hp['img_pp']=return_pp(type_m)\n    hp.pop('type_model',None)\n    with strategy.scope():\n        model=model_builder(**hp)\n    print('begin training')\n    print(hp)\n    model.fit(train_dataset,validation_data=val_dataset,\n              epochs=80,\n              steps_per_epoch=steps,\n             callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',verbose=1, patience=15,restore_best_weights=True)])\n    print(model.evaluate(test_dataset))\n    print('begin ft')\n    model=ft_model(**ft_hp)\n    print(model.evaluate(test_dataset))","6d053020":"## Training"}}