{"cell_type":{"96b6547a":"code","5bb56055":"code","8b944c54":"code","54e3bf79":"code","7f893d95":"code","f15aaad1":"code","ce02387a":"code","0ca147fa":"markdown","50606ef1":"markdown","11fbdad4":"markdown","bf654f34":"markdown","fbb1c137":"markdown","d5218e6d":"markdown","ce6e0656":"markdown","bca677ad":"markdown","bddcacfa":"markdown","28c1c75f":"markdown","27dcd6b4":"markdown"},"source":{"96b6547a":"\ndef determine_winner(you, opponent):\n    winning_situations = [[0, 2], [2, 1], [1, 0]]\n    if [you, opponent] in winning_situations:\n        return 1\n    elif you == opponent:\n        return 0\n    else:\n        return -1\n\n\nSTATES = {(0, 0): 0,\n          (0, 1): 1,\n          (0, 2): 2,\n          (1, 0): 3,\n          (1, 1): 4,\n          (1, 2): 5,\n          (2, 0): 6,\n          (2, 1): 7,\n          (2, 2): 8}\n\ncurrent_state = 0\ncurrent_action = 0\n\n","5bb56055":"\n\ndef my_dqn_agent(observation, configuration):\n    global current_state\n    global current_action\n    global STATES\n    global dqn\n    dqn.track_reward(observation.reward)\n    if observation.step == 0:\n                                                    #first step, play random\n        current_action = int(np.random.randint(0, 3))\n        return current_action\n    elif observation.step == 1:\n                                                    #second step, keep track of last state ( current_state here)\n        current_state = STATES[(current_action, observation.lastOpponentAction)]\n        current_action = int(np.random.randint(0, 3))\n        return current_action\n    else:\n                                                    #other steps,\n                                                    #find reward\n        reward = determine_winner(current_action, observation.lastOpponentAction)\n                                                    #find next transition state(our last action and opponent last action)        \n        next_state = STATES[(current_action, observation.lastOpponentAction)]\n                                                    #Now we have experience tuple (last state, action, new state , reward)\n        experience = (current_state, current_action, next_state, reward)\n                                                    #our dqn will record and experience this.\n        dqn.experience(experience)\n                                                    #ask dqn to predict action\n        current_action = dqn.act(next_state, current_state)\n        current_state = next_state\n        return current_action\n","8b944c54":"class DQN():\n    def __init__(self):\n        self.Q = np.zeros((9, 3))\n        self.alpha = 0.7\n        self.alpha_decay = 1\n        self.discount = 0.31\n        self.epsilon = 0.80\n        self.epsilon_decay = 0.996\n        self.reward_history = deque(maxlen=10)\n        self.replay_memory_buffer = deque(maxlen=500000)\n        self.batch_size = 100\n        self.is_batch_applied = False\n        self.counter = 0\n        self.model = self.create_model()\n        self.random_action = 0\n        self.prediction_model = 0\n        pass\n\n    # Predict states or play random\n    \n    def act(self, current_state, state_before):\n        if self.epsilon > random.uniform(0, 1) or self.is_neg_reward_streak():\n            action = int(np.random.randint(0, 3))\n        else:\n            action_model = self.prediction_by_model(current_state, state_before)\n            action = action_model\n        return action\n    \n    # Record experience and train at intervals\n    \n    def experience(self, exp):\n        (c_state, c_action, n_state, n_reward) = exp\n        discounted_next_state = self.alpha * (n_reward +\n                                              self.discount * self.Q[\n                                                  n_state, self.prediction_by_model(n_state, c_state)] -\n                                              self.Q[c_state, c_action])\n\n        self.Q[c_state, c_action] = self.Q[c_state, c_action] + discounted_next_state\n        z_state_before_c_state = self.state_to_zeros(0)\n        if (len(self.replay_memory_buffer) > 0):\n            previous_exp = self.replay_memory_buffer[-1]\n            (z_state_before_c_state, _, _) = previous_exp\n        self.replay_memory_buffer.append(\n            (self.state_to_zeros(c_state), z_state_before_c_state, self.Q[c_state, :]))\n\n        self.alpha *= self.alpha_decay\n        self.epsilon *= self.epsilon_decay\n\n        self.counter = self.counter + 1\n        if (self.counter > self.batch_size \/ 2):\n            self.counter = 0\n            samples = random.sample(self.replay_memory_buffer, min(len(self.replay_memory_buffer), self.batch_size))\n            (states, q_values) = self.get_attribues_from_sample(samples)\n            self.model.fit(states, q_values, verbose=0, batch_size=self.batch_size)\n        pass\n\n    def state_to_zeros(self, c_state):\n        v = np.zeros(9)\n        v[c_state] = 1\n        return v\n\n    # Tensor flow dense model\n    \n    def create_model(self):\n        model = tf.keras.models.Sequential()\n        model.add(tf.keras.layers.Dense(1, activation=tf.nn.relu, input_dim=18))\n        model.add(tf.keras.layers.Dense(256, activation=tf.nn.relu))\n        model.add(tf.keras.layers.Dropout(0.2))\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))\n        model.add(tf.keras.layers.Dropout(0.2))\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.Dense(3))\n        model.compile(loss=tf.keras.losses.mean_squared_error, optimizer=\"adam\", metrics=['accuracy'])\n\n        # print(model1.summary())\n        return model\n\n    def get_attribues_from_sample(self, samples):\n        ff = []\n        for i in samples:\n            current_state = i[0]\n            state_before = i[1]\n            ff.append(np.append(current_state, state_before))\n        states = np.asarray(ff)\n        q_values = np.array([i[2] for i in samples])\n        return states, q_values\n\n    def prediction_by_model(self, current_state, state_before):\n        z_current_state = self.state_to_zeros(current_state)\n        z_state_before = self.state_to_zeros(state_before)\n        state = np.append(z_current_state, z_state_before).reshape(1, 18)\n        pred = self.model.predict(state)\n        return int(np.argmax(pred[0]))\n\n    def is_neg_reward_streak(self):\n        return all(i < 0 for i in list(self.reward_history))\n\n    def track_reward(self, reward):\n        self.reward_history.append(reward)\n        pass\n\n","54e3bf79":"from collections import deque\n\nimport numpy as np\nimport random\nimport tensorflow as tf\n\n\ndef determine_winner(you, opponent):\n    winning_situations = [[0, 2], [2, 1], [1, 0]]\n    if [you, opponent] in winning_situations:\n        return 1\n    elif you == opponent:\n        return 0\n    else:\n        return -1\n\n\nSTATES = {(0, 0): 0,\n          (0, 1): 1,\n          (0, 2): 2,\n          (1, 0): 3,\n          (1, 1): 4,\n          (1, 2): 5,\n          (2, 0): 6,\n          (2, 1): 7,\n          (2, 2): 8}\n\ncurrent_state = 0\ncurrent_action = 0\n\n\nclass DQN():\n    def __init__(self):\n        self.Q = np.zeros((9, 3))\n        self.alpha = 0.7\n        self.alpha_decay = 1\n        self.discount = 0.31\n        self.epsilon = 0.80\n        self.epsilon_decay = 0.996\n        self.reward_history = deque(maxlen=10)\n        self.replay_memory_buffer = deque(maxlen=500000)\n        self.batch_size = 100\n        self.is_batch_applied = False\n        self.counter = 0\n        self.model = self.create_model()\n        self.random_action = 0\n        self.prediction_model = 0\n        pass\n\n    def act(self, current_state, state_before):\n        if self.epsilon > random.uniform(0, 1) or self.is_neg_reward_streak():\n            action = int(np.random.randint(0, 3))\n        else:\n            action_model = self.prediction_by_model(current_state, state_before)\n            action = action_model\n        return action\n\n    def experience(self, exp):\n        (c_state, c_action, n_state, n_reward) = exp\n        discounted_next_state = self.alpha * (n_reward +\n                                              self.discount * self.Q[\n                                                  n_state, self.prediction_by_model(n_state, c_state)] -\n                                              self.Q[c_state, c_action])\n\n        self.Q[c_state, c_action] = self.Q[c_state, c_action] + discounted_next_state\n        z_state_before_c_state = self.state_to_zeros(0)\n        if (len(self.replay_memory_buffer) > 0):\n            previous_exp = self.replay_memory_buffer[-1]\n            (z_state_before_c_state, _, _) = previous_exp\n        self.replay_memory_buffer.append(\n            (self.state_to_zeros(c_state), z_state_before_c_state, self.Q[c_state, :]))\n\n        self.alpha *= self.alpha_decay\n        self.epsilon *= self.epsilon_decay\n\n        self.counter = self.counter + 1\n        if (self.counter > self.batch_size \/ 2):\n            self.counter = 0\n            samples = random.sample(self.replay_memory_buffer, min(len(self.replay_memory_buffer), self.batch_size))\n            (states, q_values) = self.get_attribues_from_sample(samples)\n            self.model.fit(states, q_values, verbose=0, batch_size=self.batch_size)\n        pass\n\n    def state_to_zeros(self, c_state):\n        v = np.zeros(9)\n        v[c_state] = 1\n        return v\n\n    def create_model(self):\n        model = tf.keras.models.Sequential()\n        model.add(tf.keras.layers.Dense(1, activation=tf.nn.relu, input_dim=18))\n        model.add(tf.keras.layers.Dense(256, activation=tf.nn.relu))\n        model.add(tf.keras.layers.Dropout(0.2))\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))\n        model.add(tf.keras.layers.Dropout(0.2))\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.Dense(3))\n        model.compile(loss=tf.keras.losses.mean_squared_error, optimizer=\"adam\", metrics=['accuracy'])\n\n        # print(model1.summary())\n        return model\n\n    def get_attribues_from_sample(self, samples):\n        ff = []\n        for i in samples:\n            current_state = i[0]\n            state_before = i[1]\n            ff.append(np.append(current_state, state_before))\n        states = np.asarray(ff)\n        q_values = np.array([i[2] for i in samples])\n        return states, q_values\n\n    def prediction_by_model(self, current_state, state_before):\n        z_current_state = self.state_to_zeros(current_state)\n        z_state_before = self.state_to_zeros(state_before)\n        state = np.append(z_current_state, z_state_before).reshape(1, 18)\n        pred = self.model.predict(state)\n        return int(np.argmax(pred[0]))\n\n    def is_neg_reward_streak(self):\n        return all(i < 0 for i in list(self.reward_history))\n\n    def track_reward(self, reward):\n        self.reward_history.append(reward)\n        pass\n\n\ndqn = DQN()\n\n\ndef dqn_agent(observation, configuration):\n    global current_state\n    global current_action\n    global STATES\n    global dqn\n    dqn.track_reward(observation.reward)\n    if observation.step == 0:\n        current_action = int(np.random.randint(0, 3))\n        return current_action\n    elif observation.step == 1:\n        current_state = STATES[(current_action, observation.lastOpponentAction)]\n        current_action = int(np.random.randint(0, 3))\n        return current_action\n    else:\n        reward = determine_winner(current_action, observation.lastOpponentAction)\n        next_state = STATES[(current_action, observation.lastOpponentAction)]\n        experience = (current_state, current_action, next_state, reward)\n        dqn.experience(experience)\n\n        current_action = dqn.act(next_state, current_state)\n        current_state = next_state\n        return current_action\n","7f893d95":"from kaggle_environments import evaluate, make, utils\n\nenv = make(\"rps\", configuration={\"episodeSteps\": 1000}, debug=True)","f15aaad1":"q_learning_demo=\"\/kaggle\/input\/rps-bot-files\/q_learning_demo.py\"\nrock=\"\/kaggle\/input\/rps-bot-files\/rock.py\"\n","ce02387a":"env.run([q_learning_demo, rock])\nprint(env.render(mode=\"ansi\"))","0ca147fa":"## Deep Q-Networks","50606ef1":"Lets create a dqn agent, according to the problem, our agent is called on each new game, with observation has history for lastOpponentAction and Reward.\n\nThus, we create an agent which does the following - \n1. Create a state map of agent action and opponent action. This will help in creating a single state for transition between states.\n2. We can figure out the reward by examining the our last action and last opponent action, \"determine_winner\" does the same.\n","11fbdad4":"## Code walkthrough","bf654f34":"# Deep Q Network (DQN)\nThis notebook is to create an agent that works using Deep Q Network (DQN) over the generic Q Learning approach. \n","fbb1c137":"Steps involved in reinforcement learning using deep Q-learning networks (DQNs):\n\n1. All the past experience is stored by the user in memory\n2. The next action is determined by the maximum output of the Q-network\n3. The loss function here is mean squared error of the predicted Q-value and the target Q-value \u2013 Q*. This is basically a regression problem. However, we do not know the target or actual value here as we are dealing with a reinforcement learning problem. Going back to the Q-value update equation derived fromthe Bellman equation. we have:\n\n![image.png](attachment:image.png)\n\nThe section in green represents the target. We can argue that it is predicting its own value, but since R is the unbiased true reward, the network is going to update its gradient using backpropagation to finally converge.","d5218e6d":"## Q Learning","ce6e0656":"## Complete code ","bca677ad":"We create my_dnq_agent which does the following (added comments)","bddcacfa":"\nQ-learning is an off policy reinforcement learning algorithm that seeks to find the best action to take given the current state. It\u2019s considered off-policy because the q-learning function learns from actions that are outside the current policy, like taking random actions, and therefore a policy isn\u2019t needed. More specifically, q-learning seeks to learn a policy that maximizes the total reward.\n\n![image.png](attachment:image.png)","28c1c75f":"\nIn deep Q-learning, we use a neural network to approximate the Q-value function. The state is given as the input and the Q-value of all possible actions is generated as the output. The comparison between Q-learning & deep Q-learning is wonderfully illustrated below:\n\n![image.png](attachment:image.png)\n","27dcd6b4":"A DQN ( Deep Q Network ) does the following --\n1. Create a Dense network that will learn and estimate q values\n2. Record experiences in a replay buffer, and use it for model training at intervals.\n3. Predict next action by model.predict() of Q values, selecting argmax\n4. Added a reward history queue to track if we are overfitting and loosing to opponent. Play random for a series of neg rewards."}}