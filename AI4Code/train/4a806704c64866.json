{"cell_type":{"dcde52f1":"code","d1c9d29e":"code","56497c41":"code","9a016471":"code","c66f4f15":"code","fa2ab1ee":"code","2274731c":"code","5f4c52a5":"code","9b7bcfbb":"code","1c5e8c2d":"code","83b90fa1":"code","5c8f1b0e":"code","247e79cc":"code","25504049":"code","f46e5bc0":"code","0d2d3b82":"code","2d0b0eee":"code","5db7021f":"code","38cdf4bc":"code","13eca4f8":"code","f1d77754":"code","c1941cf3":"code","5dd8e73d":"code","8bac33b1":"code","f36e47d5":"code","3ea37075":"code","914fe4e9":"code","34a053a4":"code","12f7eaa6":"code","e33e35dc":"code","955d87e7":"code","69a89da2":"code","fdefa559":"code","7d72bd08":"code","5964c909":"code","973897c0":"markdown","0904187b":"markdown","17bc167c":"markdown","53a70298":"markdown","08f57225":"markdown"},"source":{"dcde52f1":"# IMPORT MODULES\n# TURN ON the GPU !\n\nimport os\nfrom operator import itemgetter    \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nget_ipython().magic(u'matplotlib inline')\nplt.style.use('ggplot')\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import Imputer\nfrom pandas.tools.plotting import scatter_matrix\nfrom sklearn.preprocessing import RobustScaler, StandardScaler, LabelEncoder, MinMaxScaler, OneHotEncoder, LabelBinarizer\nfrom sklearn.metrics import mean_squared_error, accuracy_score, mean_absolute_error\nfrom sklearn.cross_validation import KFold, cross_val_score\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV, KFold, cross_val_predict, StratifiedKFold, train_test_split, learning_curve, ShuffleSplit\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor, BayesianRidge\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, BaggingRegressor\nfrom sklearn.svm import SVR, LinearSVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom xgboost import XGBRegressor\n\nimport tensorflow as tf\n\nfrom keras import models, regularizers, layers, optimizers, losses, metrics\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.utils import np_utils\n\nprint(os.getcwd())\nprint(\"Modules imported \\n\")\nimport os\nprint(os.listdir(\"..\/input\"))","d1c9d29e":"# Load MIMIC2 data \n\ndata = pd.read_csv('..\/input\/mimic3a.csv')\nprint(\"With id\", data.shape)\ndata_full = data.drop('hadm_id', 1)\nprint(\"No id\",data_full.shape)","56497c41":"print(data_full.shape)\ndata_full.info()\ndata_full.describe()","9a016471":"data_full.head(10)","c66f4f15":"# Label = LOS\ny = data_full['LOSdays']\nX = data_full.drop('LOSdays', 1)\nX = X.drop('ExpiredHospital', 1)\nX = X.drop('AdmitDiagnosis', 1)\nX = X.drop('AdmitProcedure', 1)\nX = X.drop('marital_status', 1)\nX = X.drop('ethnicity', 1)\nX = X.drop('religion', 1)\nX = X.drop('insurance', 1)\n\nprint(\"y - Labels\", y.shape)\nprint(\"X - No Label No id \", X.shape)\nprint(X.columns)","fa2ab1ee":"data_full.hist(bins=30, figsize=(20,15))\nplt.show()","2274731c":"age_histogram = data_full.hist(column='age', bins=20, range=[0, 100])\nfor ax in age_histogram.flatten():\n    ax.set_xlabel(\"Age\")\n    ax.set_ylabel(\"Num. of Patients\")\n    \nage_LOS = data_full.hist(column='LOSdays', bins=20, range=[0, 100])\nfor ax in age_LOS.flatten():\n    ax.set_xlabel(\"LOS\")\n    ax.set_ylabel(\"Num. of Patients\")","5f4c52a5":"# Pearson linear correlation\n\ncorr_matrix = data_full.corr()\ncorr_matrix[\"LOSdays\"].sort_values(ascending=False)","9b7bcfbb":"# Check that all X columns have no missing values\nX.info()\nX.describe()","1c5e8c2d":"#data_full.plot(kind=\"scatter\", x=\"sofa_max\", xlim=(0,25), y=\"LOS\", alpha=0.1, ylim=(0,50))\ndata_full.plot(kind=\"scatter\", x=\"age\", xlim=(0,100), y=\"LOSdays\", alpha=0.1, ylim=(0,50))\ndata_full.plot(kind=\"scatter\", x=\"TotalNumInteract\", xlim=(0,300), y=\"LOSdays\", alpha=0.1, ylim=(0,50))\n","83b90fa1":"# MAP Text to Numerical Data\n# Use one-hot-encoding to convert categorical features to numerical\n\nprint(X.shape)\ncategorical_columns = [\n                       'gender',                     \n                      'admit_type',\n                      'admit_location'                      \n                      ]\n\nfor col in categorical_columns:\n    #if the original column is present replace it with a one-hot\n    if col in X.columns:\n        one_hot_encoded = pd.get_dummies(X[col])\n        X = X.drop(col, axis=1)\n        X = X.join(one_hot_encoded, lsuffix='_left', rsuffix='_right')\n        \nprint(X.shape)","5c8f1b0e":"print(X.columns)\n","247e79cc":"\nprint(data_full.shape)\nprint(X.shape)\n#XnotNorm = np.array(X.copy())\nXnotNorm = X.copy()\nprint('XnotNorm ', XnotNorm.shape)\n\nyFI = data_full.LOSdays\nynotNorm = yFI.copy()\nprint('ynotNorm ', ynotNorm.shape)","25504049":"# Normalize X\n\nx = XnotNorm.values #returns a numpy array\nscaler = preprocessing.StandardScaler()\nx_scaled = scaler.fit_transform(x)\nXNorm = pd.DataFrame(x_scaled, columns=XnotNorm.columns)\nprint(XNorm)","f46e5bc0":"# Normalize y\n\ny = ynotNorm.values #returns a numpy array\ny = y.reshape(-1, 1)\ny_scaled = scaler.fit_transform(y)\nynorm=pd.DataFrame(y_scaled)\nprint(ynorm)","0d2d3b82":"# FEATURE IMPORTANCE Data NOT normalized using Lasso model - NOT the best one ...\n\ntrainFinalFI = XnotNorm\nyFinalFI = ynotNorm\n\nlasso=Lasso(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=1000,\n   normalize=False, positive=False, precompute=False, random_state=None,\n   selection='cyclic', tol=0.0001, warm_start=False)\nlasso.fit(trainFinalFI,yFinalFI)\n\nFI_lasso = pd.DataFrame({\"Feature Importance\":lasso.coef_}, index=trainFinalFI.columns)\n\n# Focus on those with 0 importance\n#print(FI_lasso.sort_values(\"Feature Importance\",ascending=False).to_string())\n#print(\"_\"*80)\nFI_lasso[FI_lasso[\"Feature Importance\"] >0.2].sort_values(\"Feature Importance\").plot(kind=\"barh\",figsize=(15,25))\nplt.xticks(rotation=90)\nFI_lasso[FI_lasso[\"Feature Importance\"] <-0.2].sort_values(\"Feature Importance\").plot(kind=\"barh\",figsize=(15,25))\nplt.xticks(rotation=90)\nplt.show()","2d0b0eee":"# CROSS VALIDATION\n\ndef rmse_cv(model,X,y):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=5))\n    return rmse","5db7021f":"# Lin reg ALL models HYPERPARAMS NOT optimized\n\nmodels = [RandomForestRegressor(), ExtraTreesRegressor(), GradientBoostingRegressor()]\nnames = [\"RandomForestRegressor\", \"ExtraTreesRegressor\", \"GradientBoostingRegressor\"]","38cdf4bc":"# Run the models and compare\n\nModScores = {}\n\nfor name, model in zip(names, models):\n    score = rmse_cv(model, XNorm, ynorm)\n    ModScores[name] = score.mean()\n    print(\"{}: {:.2f}\".format(name,score.mean()))\n\nprint(\"_\"*100)\nfor key, value in sorted(ModScores.items(), key = itemgetter(1), reverse = False):\n    print(key, round(value,3))","13eca4f8":"# Optimize hyper params for one model\n\nmodel = RandomForestRegressor()\n\nparam_grid = [{},]\n\ngrid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')\ngrid_search.fit(XNorm, ynorm)\n\nprint(grid_search.best_estimator_)","f1d77754":"model = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n           max_features='auto', max_leaf_nodes=None,\n           min_impurity_decrease=0.0, min_impurity_split=None,\n           min_samples_leaf=1, min_samples_split=2,\n           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n           oob_score=False, random_state=None, verbose=0, warm_start=False)","c1941cf3":"# FEATURE IMPORTANCE - NORMALIZED - last model\n\ntrainFinalFI = XNorm\nyFinalFI = ynorm\n\nmodel.fit(trainFinalFI,yFinalFI)\n\nFI_model = pd.DataFrame({\"Feature Importance\":model.feature_importances_,}, index=trainFinalFI.columns)\nFI_model[FI_model[\"Feature Importance\"] > 0.007].sort_values(\"Feature Importance\").plot(kind=\"barh\",figsize=(15,25))\nplt.xticks(rotation=90)\nplt.xticks(rotation=90)\nplt.show()","5dd8e73d":"# List of important features for model\nFI_model = pd.DataFrame({\"Feature Importance\":model.feature_importances_,}, index=trainFinalFI.columns)\nFI_model=FI_model.sort_values('Feature Importance', ascending = False)\nprint(FI_model[FI_model[\"Feature Importance\"] > 0.001])","8bac33b1":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Error\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = 1-np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = 1-np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","f36e47d5":"# LEARNING CURVES Train \/ Validation\n\ntitle = \"Learning Curves \"\ncv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\nplot_learning_curve(model, title, XNorm, ynorm, cv=cv, n_jobs=5)\n#plot_learning_curve(model, title, XNorm, y, ylim=(0.01, 0.99), cv=cv, n_jobs=4)","3ea37075":"# Split into Train & Test\n\nX_train, X_test, y_train, y_test = train_test_split(XNorm, ynorm, test_size=0.2, random_state=42)\nprint ('X_train: ', X_train.shape)\nprint ('X_test: ', X_test.shape)\nprint ('y_train: ', y_train.shape)\nprint ('y_test: ', y_test.shape)\n\n# Model FINAL fit and evaluation on test\n\nmodel.fit(X_train, y_train)\n\nfinal_predictions = model.predict(X_test)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse) \nprint(\"NORM rmse on test \", round(final_rmse, 4))\n\nfinal_mae = mean_absolute_error(y_test, final_predictions)\nprint(\"NORM mae on test \", round(final_mae, 4))","914fe4e9":"# Split into Train & Test\n\n#   NOTE - For ed purposes - MAE - ynotNorm was USED !!!\n\nX_train, X_test, y_train, y_test = train_test_split(XNorm, ynotNorm, test_size=0.2, random_state=42)\nprint ('X_train: ', X_train.shape)\nprint ('X_test: ', X_test.shape)\nprint ('y_train: ', y_train.shape)\nprint ('y_test: ', y_test.shape)","34a053a4":"# Model FINAL fit and evaluation on test\n\nmodel.fit(X_train, y_train)\n\nfinal_predictions = model.predict(X_test)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse) \nprint(\"rmse on test \", round(final_rmse, 4))\n\nfinal_mae = mean_absolute_error(y_test, final_predictions)\nprint(\"mae on test \", round(final_mae, 4))","12f7eaa6":"# PLOT True vs Predicted\n\nxChart = [np.array(y_test)]\nyChart = [np.array(final_predictions)]\n\nplt.scatter(xChart,yChart, alpha=0.2)\nplt.xlim(0,30)\nplt.ylim(0,30)\nplt.plot( [0,30],[0,30], 'b')\nplt.show()\n","e33e35dc":"# Split into Train & Test\n\nX_train, X_test, y_train, y_test = train_test_split(XNorm, ynorm, test_size=0.2, random_state=42)\nprint ('X_train: ', X_train.shape)\nprint ('X_test: ', X_test.shape)\nprint ('y_train: ', y_train.shape)\nprint ('y_test: ', y_test.shape)","955d87e7":"# Transfer data to NN format\n\nx_val = X_test\npartial_x_train = X_train\ny_val = y_test\npartial_y_train = y_train\n\nprint(\"partial_x_train \", partial_x_train.shape)\nprint(\"partial_y_train \", partial_y_train.shape)\n\nprint(\"x_val \", x_val.shape)\nprint(\"y_val \", y_val.shape)","69a89da2":"# NN MODEL\nfrom keras import models\nmodel = models.Sequential()\nmodel.add(layers.Dense(2048, activation='relu', kernel_regularizer=regularizers.l2(0.001),\n                       input_shape=(partial_x_train.shape[1],)))\n#model.add(layers.BatchNormalization())\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(2048, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(2048, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(2048, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(1))\nmodel.summary()\n\nmodel.compile(optimizer=optimizers.Adam(lr=1e-4), loss='mse', metrics=['mae'])\nprint(\"model compiled\")","fdefa559":"history = model.fit(partial_x_train, partial_y_train,\n                    validation_data=(x_val, y_val), \n                    verbose=1,\n                   epochs=100)","7d72bd08":"acc = history.history['mean_absolute_error']\nval_acc = history.history['val_mean_absolute_error']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training error')\nplt.plot(epochs, val_acc, 'r', label='Validation error')\nplt.title('Training and validation ERROR')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation LOSS')\nplt.legend()\nplt.show()","5964c909":"# Model evaluation on test\n\ntest_mse_score, test_mae_score = model.evaluate(x_val, y_val)\nfinal_rmse = np.sqrt(test_mse_score) \nprint(\"rmse on test \", round(final_rmse, 4))\n\nprint(\"mae on test \", round(test_mae_score, 4))\n","973897c0":"# Lin reg ALL models HYPERPARAMS NOT optimized\n\nmodels = [LinearRegression(),Ridge(),Lasso(),RandomForestRegressor(),GradientBoostingRegressor(),SVR(),LinearSVR(),\n          ElasticNet(),SGDRegressor(),BayesianRidge(),KernelRidge(),ExtraTreesRegressor()]\nnames = [\"LinearRegression\", \"Ridge\", \"Lasso\", \"RandomForestRegressor\", \"GradientBoostingRegressor\", \"SVR\", \"LinearSVR\", \n         \"ElasticNet\",\"SGDRegressor\",\"BayesianRidge\",\"KernelRidge\",\"ExtraTreesRegressor\"]\n         ","0904187b":"* The original data is from MIMIC2 - Multiparameter Intelligent Monitoring in Intensive Care (deidentified DB) available freely from \nhttps:\/\/mimic.physionet.org\/\n* Each instance in the mldata.csv attached is one admission\n* Testing a theory I have, that one can predict LOS just by the number of interactions betweeen patient and hospital per day, I've used the following features for the LOS prediction as a REGRESSION problem:\n* Age, Gender, Ethnicity, Insurance, Admission Type, Admission Source, etc.\n* Number of Diagnosis on Admission, Procedures on Admission\n* Daily average number of: Labs, Micro labs, IV meds, Non-IV meds, Imaging Reports, Notes, Orders, Caregivers, Careunits\n\nThe label is LOS in days\n\nI've compared initially 12 REGRESSOR models.  \n\n\nLet me know *your* results on this (overly simplified) dataset","17bc167c":"**NN model**  ","53a70298":"# IMPUTE missing values\n\nX.fillna(value='unknown', axis=1, inplace=True)","08f57225":"data_full.groupby('insurance').size().plot.bar()\nplt.show()\ndata_full.groupby('admit_type').size().plot.bar()\nplt.show()\ndata_full.groupby('admit_location').size().plot.bar()\nplt.show()"}}