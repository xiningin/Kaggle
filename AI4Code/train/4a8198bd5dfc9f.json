{"cell_type":{"0890784f":"code","b6f5045b":"code","e3636b54":"code","1eae7b1f":"code","1e1f88b4":"code","1be7e30a":"code","84566ce1":"code","5ec66217":"code","780fdf24":"code","fb8afaa1":"code","c2f16477":"code","d0a170b7":"code","37ba9327":"code","2dc14ab7":"code","0f144198":"code","eb7cc630":"code","29eecc8a":"code","f14f2b5d":"code","e262503d":"code","dab819bd":"code","71f69e48":"code","bb09acaf":"code","1cdca8ed":"code","e417b474":"markdown","c9b754f5":"markdown","e9821e67":"markdown","fc968753":"markdown","95ce01b3":"markdown","653de457":"markdown","723d6f4a":"markdown","38fd440b":"markdown","092eeb31":"markdown","97396359":"markdown","268b82af":"markdown","029e090b":"markdown","502073e9":"markdown","2a06427b":"markdown","390c0b15":"markdown","b8c1d929":"markdown"},"source":{"0890784f":"import sys, os, glob, time, imageio \nimport numpy as np, pandas as pd  \n\nimport matplotlib.pyplot as plt \nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n\nfrom PIL import Image \n\nimport torch \nimport torchvision.utils as vutils \nimport torchvision.transforms as transforms \n\nfrom keras import models, layers, optimizers \nfrom keras.models import Sequential \nfrom keras.preprocessing.image import array_to_img, img_to_array, load_img \n\nimport tensorflow as tf \n","b6f5045b":"# Python version\nprint('Python version: {}'.format(sys.version))\n\n# numpy \nprint('numpy version: {}'.format(np.__version__))\n\n# pandas \nprint('pandas version: {}'.format(pd.__version__))\n\n# matplotlib \nimport matplotlib; print('matplotlib version: {}'.format(matplotlib.__version__))\n\n# torch \nprint('torch version: {}'.format(torch.__version__))\n\n# scikit-learn \nimport sklearn; print('sklearn version: {}'.format(sklearn.__version__)) \n\n# tensorflow \nprint('tensorflow version: {}'.format(tf.__version__))","e3636b54":"# Root directory for dataset\npath_root = '..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/'\npath_train= '..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/train\/'\npath_test = '..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/test\/'\npath_val  = '..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/val\/'\n\n# Root paths for X-Ray images\nXRay_normal     = glob.glob(path_root+'*\/NORMAL\/*.jpeg', recursive=True)\nXRay_pneumonial = glob.glob(path_root+'*\/PNEUMONIA\/*.jpeg', recursive=True)\n\n# Root paths for X-Ray training images\ntrain_normal    = glob.glob(path_train+'NORMAL\/*.jpeg', recursive=True)\ntrain_pneumonial= glob.glob(path_train+'PNEUMONIA\/*.jpeg', recursive=True)\n\n# X-Ray testing images \ntest_normal     = os.listdir(path_test+'NORMAL\/')\ntest_pneumonial = os.listdir(path_test+'PNEUMONIA\/')\n\nprint('The \"Chest X-Ray Images\" dataset contains {:04d} NORMAL and {:04d} PNEUMONIA images ({:03d} in total)'\\\n      .format(len(XRay_normal), \n              len(XRay_pneumonial),\n              len(glob.glob(path_root+'*\/*\/*.jpeg')),))\nprint('   - {:04d} NORMAL and {:04d} PNEUMONIA ==> {:04d} images in the training sample'\\\n      .format(len(train_normal), \n              len(train_pneumonial), \n              len(glob.glob(path_train+'*\/*.jpeg'))))\nprint('   - {:04d} NORMAL and {:04d} PNEUMONIA ==> {:04d} images in the testing sample'\\\n      .format(len(test_normal), \n              len(test_pneumonial), \n              len(glob.glob(path_test+'*\/*.jpeg'))))\nprint('   - {:04d} NORMAL and {:04d} PNEUMONIA ==> {:04d} images in the validation sample'\\\n      .format(len(glob.glob(path_val+'NORMAL\/*.jpeg')), \n              len(glob.glob(path_val+'PNEUMONIA\/*.jpeg')), \n              len(glob.glob(path_val+'*\/*.jpeg'))))","1eae7b1f":"# Time \ndef _time(start, end): \n    # if in seconds \n    if (end-start)<60: \n        wall_time = f'{round((end-start),2)}sec'\n    # if in minute(s)  \n    elif (end-start)>=3600: \n        wall_time = f'{int((end-start)\/3600)}h {int(((end-start)%3600)\/60)}min {round((end-start)%60,2)}sec'\n    # if in houre(s)  \n    else: \n        wall_time = f'{int((end-start)\/60)}min {round((end-start)%60,2)}sec'\n    return wall_time ","1e1f88b4":"nrows, ncols = 4, 7\nplt.figure(figsize=(16,10))\nfor idx, name in enumerate(test_normal[:nrows*ncols]):\n    plt.subplot(nrows, ncols, idx+1)\n    img = Image.open(path_test+'NORMAL\/'+name) # or use plt.imread(path_test+'NORMAL\/'+name)\n    img = img.resize(size=(128, 128), resample=Image.ANTIALIAS, box=None)\n    plt.imshow(img)\n    plt.title(name[:-5], fontsize=9)\n    plt.axis('off')","1be7e30a":"def get_data(data_path, dim=(128, 128), rand_shuffle=True): \n    start = time.time() \n    imgs_data = []         \n    sample_size = len(data_path)\n    for idx, im_path in enumerate(data_path): \n        if idx%(sample_size\/\/10)==0:\n            print('Processing index {:05d} of {:05d} ==> {:03d}%'\\\n                  .format(idx, sample_size, round(100*idx\/sample_size))) \n        img = img_to_array(load_img(im_path, target_size = dim)) \n        imgs_data.append(img) \n        \n    # to float \n    imgs_data = np.array(imgs_data).astype('float32') \n    # scale to [0,1] (note the . after 255 - float)\n    imgs_data = imgs_data\/255. #for formalizing to [-1,1] ==> (imgs_data - 127.5)\/127.5 \n    \n    # shuffle the data \n    if rand_shuffle: \n        idx = np.arange(imgs_data.shape[0])\n        np.random.shuffle(idx) \n        imgs_data = imgs_data[idx,:,:,:] \n    \n    print(f\"Hey! the calculations are done in {_time(start, time.time())}\")\n    return imgs_data  \n","84566ce1":"print('Starting for NORMAL X-Ray images ...')\nX_normal = get_data(XRay_normal)\nprint()\nprint('Starting for PNEUMONIA X-Ray images ...')\nX_pneumonial = get_data(XRay_pneumonial) ","5ec66217":"def define_grid(data_images, nrows=4, ncols=5, plot_grid=True):\n    # save the started time \n    start = time.time() \n    # Number of GPUs available. Use 0 for CPU mode. \n    ngpu = 1 \n    # Decide which device we want to run on \n    device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n    # Rearange the shaphe of the data \n    data_transp = [np.transpose(data_images[i,:,:]) for i in range(data_images[:nrows*ncols].shape[0])]\n    # From to torch type for the grid \n    data_transp = torch.Tensor(data_transp)\n    print(f'The shape is reordered from {data_images.shape[1:]} to {data_transp.shape[1:]} in {_time(start, time.time())}')\n    \n    # Make the grid \n    grid_images = np.transpose(\n        vutils.make_grid(\n            data_transp.to(device)[:nrows*ncols], \n            nrow=nrows,\n            padding=2,\n            normalize=True,\n            scale_each=True,\n            pad_value=1,\n        ).cpu(), axes=(2,1,0))\n        \n    # Show the output grid \n    if plot_grid:\n        plt.figure(figsize=(12,12)) \n        plt.axis(\"off\") \n        plt.title(f'Grid of {nrows*ncols} real images', fontsize=27)\n        plt.imshow(grid_images)\n        \n    return grid_images\n\ngrid_X_normal = define_grid(X_normal, plot_grid=False)\ngrid_X_pneumonial = define_grid(X_pneumonial, plot_grid=False)","780fdf24":"fig, (ax1, ax2)= plt.subplots(nrows=1, ncols=2, figsize=(19, 8))\n\nax1.imshow(grid_X_normal); ax1.axis('off')\nax1.set_title(label = 'Grid of X-Ray NORMAL images', fontsize = 27)\n\nax2.imshow(grid_X_pneumonial); ax2.axis('off')\nax2.set_title(label = 'Grid of X-Ray PNEUMONIA images', fontsize = 27)\n\nplt.tight_layout(pad=1.08, h_pad=None, w_pad=None, rect=[0, 0.03, 1, 0.95])","fb8afaa1":"# Number of images to use (will be changed)\n#n_images = 12_000 \n\n# Number of training epochs\nn_epoch = 200 \n\n# Batch size during training \nbatch_size = 128 \n\n# Size of z latent vector (i.e. size of generator input) \nlatent_dim = 100 \n\n# Spatial size of training images. All images will be resized to this size \ncols, rows = 128, 128 \n\n# Number of channels in the training images. For RGB color images this is 3\nchannels = 3 \ndim = cols, rows # height, width \nin_shape = (cols, rows, channels) # height, width, color \n\n# Learning rate for optimizers\nlr = 0.0002\n\n# Beta1 hyperparam for Adam optimizers\nbeta1 = 0.5\n\n# Number of GPUs available. Use 0 for CPU mode.\nngpu = 1 \n\n# plot ncols images in row and nrows images in colomn\nnrows, ncols = 3, 4\n","c2f16477":"def define_discriminator(in_shape=(128,128,3)): \n    model = models.Sequential() \n    # normal \n    model.add(layers.Conv2D(128, (5,5), padding='same', input_shape=in_shape)) \n    model.add(layers.LeakyReLU(alpha=0.2)) \n    # downsample to 64x64 \n    model.add(layers.Conv2D(128, (5,5), strides=(2,2), padding='same')) \n    model.add(layers.LeakyReLU(alpha=0.2)) \n    # downsample to 32x32 \n    model.add(layers.Conv2D(128, (5,5), strides=(2,2), padding='same')) \n    model.add(layers.LeakyReLU(alpha=0.2)) \n    # downsample to 16x16 \n    model.add(layers.Conv2D(128, (5,5), strides=(2,2), padding='same')) \n    model.add(layers.LeakyReLU(alpha=0.2)) \n    # downsample to 8x8 \n    model.add(layers.Conv2D(128, (5,5), strides=(2,2), padding='same')) \n    model.add(layers.LeakyReLU(alpha=0.2)) \n    # classifier \n    model.add(layers.Flatten()) \n    model.add(layers.Dropout(0.4)) \n    model.add(layers.Dense(1, activation='sigmoid')) \n    # compile model \n    opt = optimizers.Adam(lr=0.0002, beta_1=0.5) \n    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy']) \n    return model","d0a170b7":"def define_generator(latent_dim):\n    model = models.Sequential()\n    # foundation for 8x8 feature maps\n    n_nodes = 128*8*8\n    model.add(layers.Dense(n_nodes, input_dim=latent_dim))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Reshape((8, 8, 128)))\n    # upsample to 16x16\n    model.add(layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    # upsample to 32x32\n    model.add(layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    # upsample to 64x64\n    model.add(layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    # upsample to 128x128\n    model.add(layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    # output layer 128x128x3\n    model.add(layers.Conv2D(3, (5,5), activation='tanh', padding='same'))\n    return model \n\n#input of G\ndef generate_latent_points(latent_dim, n_samples):\n    # generate points in the latent space\n    x_input = np.random.randn(latent_dim*n_samples)\n    # reshape into a batch of inputs for the network\n    x_input = x_input.reshape(n_samples, latent_dim)\n    return x_input \n\n# use the generator to generate n fake examples, with class labels\ndef generate_fake_samples(g_model, latent_dim, n_samples):\n    # generate points in latent space\n    x_input = generate_latent_points(latent_dim, n_samples)\n    # predict outputs\n    X = g_model.predict(x_input)\n    # create 'fake' class labels (0)\n    y = np.zeros((n_samples, 1))\n    return X, y","37ba9327":"def define_gan(g_model, d_model): \n    # make weights in the discriminator not trainable\n    d_model.trainable = False \n    # connect them\n    model = models.Sequential()\n    # add generator\n    model.add(g_model)\n    # add the discriminator\n    model.add(d_model)\n    # compile model\n    opt = optimizers.Adam(lr=0.0002, beta_1=0.5)\n    model.compile(loss='binary_crossentropy', optimizer=opt)\n    return model\n\n# retrive real samples\ndef get_real_samples(dataset, n_samples):\n    # choose random instances\n    ix = np.random.randint(0, dataset.shape[0], n_samples)\n    # retrieve selected images\n    X = dataset[ix]\n    # set 'real' class labels (1)\n    y = np.ones((n_samples, 1))\n    return X, y\n\n# create and save a plot of generated images \ndef show_generated(generated, epoch, nrows=4, ncols=5):\n    #[-1,1] -> [0,1] \n    #generated = (generated+1)\/2 \n    #generated = (generated[:ncols*nrows]*127.5)+127.5 \n    #generated = generated*255 \n    plt.figure(figsize=(10,10)) \n    for idx in range(nrows*ncols): \n        plt.subplot(nrows, ncols, idx+1) \n        plt.imshow(generated[idx]) \n        plt.axis('off') \n    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch+1)) \n    plt.show() \n\n# evaluate the discriminator and plot generated images \ndef summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=100):\n    # prepare real samples\n    X_real, y_real = get_real_samples(dataset, n_samples)\n    # evaluate discriminator on real examples \n    _, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n    # prepare fake examples \n    x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n    # evaluate discriminator on fake examples \n    _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n    # summarize discriminator performance \n    print('> Accuracy at epoch %d [real: %.0f%%, fake: %.0f%%]'%(epoch+1, acc_real*100, acc_fake*100))\n    # show plot \n    show_generated(x_fake, epoch)  \n    \ndef plot_loss(loss):\n    plt.figure(figsize=(10,5))\n    plt.title(\"Generator and Discriminator Loss During Training\", fontsize=20) \n    plt.plot(loss[0], label=\"D_real\") \n    plt.plot(loss[1], label=\"D_fake\") \n    plt.plot(loss[2], label=\"G\") \n    plt.xlabel(\"Iteration\", fontsize=20); plt.ylabel(\"Loss\", fontsize=20) \n    plt.legend(); plt.show() ","2dc14ab7":"def train(g_model, d_model, gan_model, dataset, latent_dim=100, n_epochs=100, n_batch=128):\n    \n    start = time.time() \n    bat_per_epo = int(dataset.shape[0]\/n_batch) \n    half_batch = int(n_batch\/2) \n    loss1, loss2, loss3 = [], [], [] \n    fake_liste = [] \n    \n    # manually enumerate epochs\n    print('Training Start...')\n    for i in range(n_epochs):\n        start1 = time.time()\n        # enumerate batches over the training set\n        for j in range(bat_per_epo):\n            # get randomly selected 'real' samples\n            X_real, y_real = get_real_samples(dataset, half_batch)\n            # update discriminator model weights\n            d_loss1, _ = d_model.train_on_batch(X_real, y_real)\n            # generate 'fake' examples\n            X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n            # update discriminator model weights\n            d_loss2, _ = d_model.train_on_batch(X_fake, y_fake)\n            # prepare points in latent space as input for the generator\n            X_gan = generate_latent_points(latent_dim, n_batch)\n            # create inverted labels for the fake samples\n            y_gan = np.ones((n_batch, 1))\n            # update the generator via the discriminator's error\n            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n            # summarize loss on this batch\n            loss1.append(d_loss1); loss2.append(d_loss2); loss3.append(g_loss) \n        \n        print('Epoch: {:03d}\/{:03d}, Loss: [D_real = {:2.3f}, D_fake = {:2.3f}, G = {:2.3f}], time: {:s}'\\\n              .format(i+1,n_epochs,d_loss1,d_loss2,g_loss, _time(start1,time.time())))\n        # evaluate the model performance \n        if (i+1)%(n_epochs\/\/10) == 0: \n            # Save and show generated images \n            summarize_performance(i, g_model, d_model, dataset, latent_dim) \n        \n    print('Total time for training {} epochs is {} sec'.format(n_epochs, _time(start, time.time())))\n    \n    # Show loss curves \n    loss = (loss1, loss2, loss3) \n    plot_loss(loss) ","0f144198":"discriminator = define_discriminator() \ngenerator = define_generator(latent_dim) \n\n# create the gan \ngan = define_gan(generator, discriminator)\n","eb7cc630":"# train model \ntrain(generator, discriminator, gan, X_normal, latent_dim, n_epochs=n_epoch, n_batch=batch_size)","29eecc8a":"fake_path = glob.glob('..\/working\/image_at_epoch*.png')\nfake_imgs = get_data(sorted(fake_path), rand_shuffle=False)","f14f2b5d":"fake_transp = [np.transpose(fake_imgs[i,:,:]) for i in range(fake_imgs.shape[0])]\n\nfig = plt.figure(figsize=(12,12)) \nplt.axis(\"off\") \nimgs = [[plt.imshow(np.transpose(i,(2,1,0)), animated=True)] for i in fake_transp] \nani = animation.ArtistAnimation(fig, imgs, interval=1000, repeat_delay=1000, blit=True) \n\nHTML(ani.to_jshtml())","e262503d":"files = []\nn_iter = int(n_epoch\/10)\nimgs_epochs = glob.glob('..\/working\/image_at_epoch_*.png') \nfor img_epoch in imgs_epochs: \n    files.append(imageio.imread(img_epoch)) \nimageio.mimsave('dcgan_celebA_generation_animation.gif', files, fps=5) ","dab819bd":"## TODO: Get better output images! ","71f69e48":"#!mkdir results\n#!ls","bb09acaf":"# use the generator to generate n fake\ndef XRayFakeGenerator(g_model=generator, latent_dim =100, n_samples=100, show_gen=False):\n    # generate points in latent space \n    x_input = generate_latent_points(latent_dim, n_samples)\n    # predict outputs \n    X = g_model.predict(x_input)  \n    \n    # Show the generated images\n    if show_gen and n_samples<=30: \n        ncols = 5\n        nrows = int(n_samples\/ncols)\n        plt.figure(figsize=(12,10)) \n        for idx in range(nrows*ncols): \n            plt.subplot(nrows, ncols, idx+1)\n            plt.imshow(X[idx,:,:]); plt.axis('off')\n        plt.show();\n    return X \n\nXRay_fake = XRayFakeGenerator(generator, n_samples=20)","1cdca8ed":"# SAVE TO ZIP FILE \nimport zipfile\noutput_path = zipfile.PyZipFile('..\/working\/XRayNormalFake.zip', mode='w')\n\nXRay_generated = XRayFakeGenerator(n_samples=1000)\nfor idx in range(XRay_generated.shape[0]):\n    img_XRayFake  = XRay_generated[idx,:,:]\n    name_XRayFake = 'XRay_generated {:04d}.png'.format(idx)\n    imageio.imwrite(name_XRayFake, img_XRayFake)\n    \n    output_path.write(name_XRayFake)\n    os.remove(name_XRayFake) \noutput_path.close()","e417b474":"## Paths and lengths ","c9b754f5":"## Set the parameters  ","e9821e67":"## Generator ","fc968753":"# Train the models ","95ce01b3":"## References \nThis challenge: https:\/\/www.kaggle.com\/paultimothymooney\/chest-xray-pneumonia  \nFor more data on Covid-19 (not only images), click [here](https:\/\/www.researchgate.net\/post\/updated_list_Last_updated_May_10th_2020_of_Coronavirus_Covid-19_dataset_and_other_Research_Resources) or [here](https:\/\/www.researchgate.net\/post\/Open_source_dataset_of_chest_CT_from_patients_with_COVID-19_infection)  \n\nGenerative Dog Images: Experiment with creating puppy pics [Kaggle Challenge](https:\/\/www.kaggle.com\/c\/generative-dog-images\/overview)  \nDog Memorizer GAN [Kaggle](https:\/\/www.kaggle.com\/cdeotte\/dog-memorizer-gan)  \nDCGAN (Deep convolutional generative adversarial networks) [Kaggle](https:\/\/www.kaggle.com\/jesucristo\/introducing-dcgan-dogs-images) ==> A regarder !  \n\nDeepfake Detection Challenge : [Kaggle video.mp4](https:\/\/www.kaggle.com\/c\/deepfake-detection-challenge\/notebooks?sortBy=voteCount&group=everyone&pageSize=20&competitionId=16880)  \n\nGANs vs. Autoencoders: Comparison of Deep Generative Models: [towards](https:\/\/towardsdatascience.com\/gans-vs-autoencoders-comparison-of-deep-generative-models-985cf15936ea)  \n\nAutoencoders - EXPLAINED [YouTube](https:\/\/www.youtube.com\/watch?v=7mRfwaGGAPg&t=146s)  \nFace editing with Generative Adversarial Networks: [YouTube](https:\/\/www.youtube.com\/watch?v=dCKbRCUyop8)  \nVariational Autoencoders for GANs: [YouTube](https:\/\/www.youtube.com\/watch?v=fcvYpzHmhvA)  \nGenerative Adversarial Networks : [YouTube](https:\/\/www.youtube.com\/watch?v=O8LAi6ksC80&list=PLTl9hO2Oobd-1jxZ01__NjibEY6h15Kha)  \n\nMIT 6.S191 Introduction to Deep Learning: [MIT 6.S191](http:\/\/introtodeeplearning.com)  \n\nKeras: [Convolution layers](https:\/\/keras.io\/api\/layers\/convolution_layers\/)  \nDCGAN with Keras: [kaggle](https:\/\/www.kaggle.com\/waltermaffy\/dcgan-with-keras)  \ngoogle: [colab](https:\/\/colab.research.google.com\/github\/tensorflow\/docs\/blob\/master\/site\/en\/tutorials\/generative\/dcgan.ipynb#scrollTo=WfO5wCdclHGL)  \n\n[Large-scale CelebFaces Attributes (CelebA) Dataset](http:\/\/mmlab.ie.cuhk.edu.hk\/projects\/CelebA.html)    \n[CelebFaces Attributes (CelebA) Dataset](https:\/\/www.kaggle.com\/jessicali9530\/celeba-dataset?select=img_align_celeba)  \n\n[Generate new Pokemon](https:\/\/github.com\/llSourcell\/Pokemon_GAN\/blob\/master\/Generative%20Adversarial%20Networks.ipynb)   \n[!!! Keras-GANs !!!](https:\/\/github.com\/eriklindernoren\/Keras-GAN)  \n\n\nhttps:\/\/www.kaggle.com\/djibybalde\/dcgan-keras-chest-x-ray-images?scriptVersionId=34191539\n","653de457":"## Animation graph\nThis annimation is inspired from https:\/\/www.kaggle.com\/bothmena\/dcgan-pytorch","723d6f4a":"## Take a quick look at of the images ","38fd440b":"## Define GAN model ","092eeb31":"## Cheik the versions ","97396359":"# Deep Convolutional Generative Adversarial Network(DCGAN)\n\nThis notebook is inspired from: https:\/\/www.kaggle.com\/waltermaffy\/dcgan-with-keras","268b82af":"## Get data ","029e090b":"## Generated and save the images in a zippez file ","502073e9":"## Create gif  \nYou can see the GIF at the end of the kernel! ","2a06427b":"# Discriminator ","390c0b15":"## Compute the time ","b8c1d929":"## Import the necessary libraries "}}