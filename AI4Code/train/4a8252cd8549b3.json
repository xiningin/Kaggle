{"cell_type":{"96e3d594":"code","0c419fe9":"code","c5279a97":"code","5206c207":"code","d97bb285":"code","22f4eee5":"code","16254f53":"code","77b0232b":"code","2a0586d7":"code","26d271c2":"code","25e6b350":"code","c7c02396":"code","8da8d298":"code","83a3cdc2":"code","1e001eb3":"code","94d0efae":"code","cf3d81aa":"code","ca6b92be":"code","3704f64b":"code","54358b9b":"code","46ff5989":"code","2f721146":"code","a5df3194":"code","ba6725a1":"code","6bd78685":"code","60238955":"code","ecfc8eb6":"code","98c4580e":"code","8350c8ee":"code","c8097870":"code","938f5d42":"code","a6fd3e0e":"code","d4726cc2":"code","f44da4e2":"code","69608026":"code","94276cb1":"code","bd104956":"code","b2607192":"code","92d1e587":"code","04904f37":"code","a1c9c6d3":"code","9faa51e0":"code","e83ed4d4":"code","56c50f2c":"code","f5402489":"code","406ac2d1":"code","c8c80c42":"code","e1c5e111":"code","3a7ca503":"code","7e036274":"code","0d463a20":"code","a00e58a7":"code","7863bc38":"code","f5ef25bf":"code","29caf032":"code","8ae1fde4":"code","859d8e0f":"code","19c6d2bd":"code","92b5ff34":"code","05891dab":"code","b316c564":"code","78334079":"code","79ecf433":"code","962b8046":"code","d06209b2":"code","d39b46fc":"code","04d005e0":"code","a5f52f11":"code","a78dcdf0":"code","abab2b58":"code","f2371aaa":"code","d8b7fb26":"code","c02e1694":"code","66431dc0":"code","d2f162a4":"code","1e7456cc":"code","059b2658":"code","f1a1e551":"code","5f9b76ab":"code","a72db879":"code","14c3aae5":"code","469e5bc4":"code","21d38bdf":"code","be3c7bcd":"code","221cfe92":"code","b716a647":"code","8f3a0eb7":"code","9be139ac":"code","bc766227":"code","d624177b":"code","4559e495":"code","4846d36e":"code","7fc01aa9":"code","a39a87e8":"code","be0f9c03":"code","ff698074":"code","19dd8e1b":"code","3ac5abf1":"code","3708c465":"code","f7981e04":"code","9f79e234":"code","dee08e1c":"code","36fc61a3":"code","17c4fe4a":"code","e9837293":"code","ffc64745":"code","7337ea91":"code","f4b1aae3":"code","35806587":"code","b3a08978":"code","acea0f7b":"code","80b0c15c":"code","8855b73f":"code","5d76a3f4":"code","e9505c69":"code","4367f643":"code","10ab3a7b":"code","4ca571b8":"code","1aa38121":"code","11c32aa3":"code","655a0190":"code","6a173f61":"code","9d9a1f89":"code","a0aade43":"code","afc88e34":"code","f40eb49f":"code","a3f8f8ff":"code","b4f26d1d":"code","891a98ad":"code","17ba6a3a":"code","28e2c1bc":"code","0cee9823":"markdown","29acbf37":"markdown","f9729fd2":"markdown","57055e45":"markdown","448f5ea4":"markdown","9d85ea5f":"markdown","021ea04c":"markdown","57ac7b26":"markdown","521ab914":"markdown","e0255aae":"markdown","17d70deb":"markdown","77fae834":"markdown","3e5f16ec":"markdown","71dfea69":"markdown","2289b78c":"markdown","b9e46ff0":"markdown","e422c513":"markdown","e9c6e437":"markdown","c006d975":"markdown","34181101":"markdown","2d7f9544":"markdown","5c587a31":"markdown","d86fd701":"markdown","5e098381":"markdown","14fc7332":"markdown","292f86bf":"markdown","2f9892ac":"markdown","363e1284":"markdown","a5b1152c":"markdown","7b860f6b":"markdown","fff8c169":"markdown","c148a0d2":"markdown","62eb3560":"markdown","bc04ddec":"markdown","3f66fd08":"markdown","b2aa981d":"markdown","7928cc9f":"markdown","ea667f3f":"markdown","d366acb0":"markdown","3c8061e6":"markdown","3cd11e16":"markdown","17ecd3dc":"markdown","52a94cc1":"markdown","aa5c42cf":"markdown","1d885c66":"markdown","27831dbd":"markdown","111f4c12":"markdown","316872f8":"markdown","449c297b":"markdown","3f6225a0":"markdown","f25e6b10":"markdown","c6e379ea":"markdown","bf74d18c":"markdown","3d60c92a":"markdown","7e3a3e44":"markdown"},"source":{"96e3d594":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","0c419fe9":"from sklearn.datasets import load_boston","c5279a97":"boston = load_boston()","5206c207":"type(boston)","d97bb285":"boston.keys()","22f4eee5":"print(boston.DESCR)","16254f53":"boston.feature_names","77b0232b":"boston.data","2a0586d7":"boston.filename","26d271c2":"data = boston.data","25e6b350":"data.shape","c7c02396":"df = pd.DataFrame(data=data, columns=boston.feature_names)","8da8d298":"df.head()","83a3cdc2":"df.shape","1e001eb3":"df['MEDV'] = boston.target","94d0efae":"df.shape","cf3d81aa":"df.head()","ca6b92be":"df.info()","3704f64b":"df.describe()  #gives descriptive statistics of all the numerical columns","54358b9b":"df.isnull().sum()","46ff5989":"sns.pairplot(df)","2f721146":"plt.figure(figsize=(10,5))\nplt.style.use('dark_background')\nsns.distplot(df['MEDV'])\nplt.title('price distribution')\nplt.xlabel('MEDV')","a5df3194":"fig, ax = plt.subplots(2, 7, figsize=(16,4))\nplt.style.use('ggplot')\nindex = 0\ncol = df.columns\n\nfor i in range(2):\n    \n    for j in range(7):\n        \n        sns.scatterplot(x=df['MEDV'], y=col[index], data=df, ax=ax[i][j])\n        \n        index=index+1\n\nplt.tight_layout()        \nplt.show()","ba6725a1":"corr = df.corr()","6bd78685":"corr.shape","60238955":"corr","ecfc8eb6":"fig, ax = plt.subplots(figsize = (12, 8))\nsns.heatmap(corr, annot = True, cmap='coolwarm', annot_kws = {'size': 9})\n\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom+0.5, top-.5)\nplt.show()","98c4580e":"cor_val = corr\n\ncor_val.loc['MEDV'].sort_values()","8350c8ee":"plt.figure(figsize=(10,4))\nplt.style.use('ggplot')\n\ncor_val.loc['MEDV'].plot.bar()","c8097870":"data = df[['LSTAT', 'PTRATIO', 'INDUS', 'RM']]","938f5d42":"data.head()","a6fd3e0e":"plt.figure(figsize=(15,8))\n\nsns.boxplot(data=data)","d4726cc2":"plt.figure(figsize=(10,5))\nsns.distplot(data['LSTAT'])","f44da4e2":"LSN = data['LSTAT'].to_numpy()","69608026":"LSN = np.log(LSN+1)","94276cb1":"df1 = pd.DataFrame(LSN, columns=['log_LSTAT'])","bd104956":"df1.head()","b2607192":"data['LSTAT'] = df1","92d1e587":"data.head()","04904f37":"sns.boxplot(data=data['LSTAT'])","a1c9c6d3":"sns.distplot(data['LSTAT'])","9faa51e0":"data['MEDV'] = boston.target","e83ed4d4":"data","56c50f2c":"plt.figure(figsize=(10,5))\nplt.style.use('ggplot')\nsns.lmplot(x='MEDV', y='RM', data=data, robust=True)  #robust=True will punish the outliers for consideration of best-fit line","f5402489":"sns.lmplot(x='MEDV', y='LSTAT', data=data, robust=True)  ","406ac2d1":"sns.lmplot(x='MEDV', y='PTRATIO', data=data, robust=True)  ","c8c80c42":"PTN = data['PTRATIO'].to_numpy()","e1c5e111":"PTN = np.log(PTN+1)","3a7ca503":"df2 = pd.DataFrame(PTN, columns=['log_PTRATIO'])","7e036274":"df2.head()","0d463a20":"data['PTRATIO'] = df2","a00e58a7":"data.head()","7863bc38":"sns.lmplot(x='MEDV', y='PTRATIO', data=data, robust=True)","f5ef25bf":"sns.lmplot(x='MEDV', y='INDUS', data=data, robust=True)","29caf032":"INN = data['INDUS'].to_numpy()","8ae1fde4":"INN = np.log(INN+1)","859d8e0f":"df3 = pd.DataFrame(INN, columns=['log_INDUS'])","19c6d2bd":"df3.head()","92b5ff34":"data['INDUS'] = df3","05891dab":"data.head()","b316c564":"sns.lmplot(x='MEDV', y='INDUS', data=data, robust=True)","78334079":"data = data.drop('INDUS', axis=1)","79ecf433":"data['INDUS'] = df['INDUS']","962b8046":"data.head()","d06209b2":"data['INDUS']=data['INDUS']**(1\/5)","d39b46fc":"data.head()","04d005e0":"sns.lmplot(x='MEDV', y='INDUS', data=data, robust=True)","a5f52f11":"X = df[['LSTAT', 'PTRATIO', 'INDUS', 'RM']]","a78dcdf0":"y = df['MEDV']","abab2b58":"X.shape","f2371aaa":"y.shape","d8b7fb26":"from sklearn.model_selection import train_test_split      # train_test_split is a built in function in sklearn\n\nX_train , X_test , y_train , y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)    # Tuple Unpacking","c02e1694":"X_train.shape","66431dc0":"X_test.shape","d2f162a4":"from sklearn.preprocessing import RobustScaler\n\nscaler = RobustScaler()","1e7456cc":"X_train = scaler.fit_transform(X_train)","059b2658":"X_test = scaler.transform(X_test)","f1a1e551":"X_train.shape","5f9b76ab":"X_train","a72db879":"from sklearn.linear_model import LinearRegression       # import the model from it's family in the sklearn library\n\nlinear =  LinearRegression()           # instantiate an instance of the model i.e., basically creating an Lin.Reg Object\n\nlinear.fit(X_train,y_train)      # fitting the model to train on train set","14c3aae5":"print(linear.intercept_)       # printing the intercept","469e5bc4":"linear.coef_","21d38bdf":"X.columns","be3c7bcd":"coeff_df = pd.DataFrame(data = linear.coef_ , index = X.columns , columns = ['Linear_Coeff'] )","221cfe92":"coeff_df","b716a647":"predictions = linear.predict(X_test)","8f3a0eb7":"predictions","9be139ac":"y_test","bc766227":"pred_train = linear.predict(X_train)","d624177b":"plt.figure(figsize=(12,6))\n\nplt.scatter(y_test,predictions)\n\nplt.plot(y_test.values,y_test.values,'r')   # best fit line that could be possible","4559e495":"plt.figure(figsize=(10,5))\nsns.distplot(y_test - predictions)","4846d36e":"from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score, r2_score","7fc01aa9":"print('MAE :' , mean_absolute_error(y_test , predictions))\n\nprint('MSE :' , mean_squared_error(y_test , predictions))\n\nprint('RMSE :' , np.sqrt(mean_squared_error(y_test , predictions)))\n\nprint('VAR :', explained_variance_score(y_test, predictions))","a39a87e8":"print('RMSE on train data:', np.sqrt(mean_squared_error(y_train, pred_train)))\nprint('\\n')\nprint('RMSE on test data:', np.sqrt(mean_squared_error(y_test, predictions)))\nprint('\\n')\nprint('R2_score on train data:', r2_score(y_train, pred_train))\nprint('\\n')\nprint('R2_Score on test data:', r2_score(y_test, predictions))","be0f9c03":"from sklearn.linear_model import Ridge","ff698074":"ridge = Ridge()","19dd8e1b":"ridge.fit(X_train, y_train)","3ac5abf1":"pred_rid = ridge.predict(X_test)","3708c465":"pred_rid_train = ridge.predict(X_train)","f7981e04":"ridge.intercept_","9f79e234":"coeff_df['Ridge_coeff'] = ridge.coef_","dee08e1c":"coeff_df","36fc61a3":"plt.figure(figsize=(12,6))\n\nplt.scatter(y_test,pred_rid)\n\nplt.plot(y_test.values,y_test.values,'r')   # best fit line that could be possible","17c4fe4a":"plt.figure(figsize=(10,5))\nsns.distplot(y_test - pred_rid)","e9837293":"print('RMSE on train data:', np.sqrt(mean_squared_error(y_train, pred_rid_train)))\nprint('\\n')\nprint('RMSE on test data:', np.sqrt(mean_squared_error(y_test, pred_rid)))\nprint('\\n')\nprint('R2_score on train data:', r2_score(y_train, pred_rid_train))\nprint('\\n')\nprint('R2_Score on test data:', r2_score(y_test, pred_rid))","ffc64745":"from sklearn.linear_model import Lasso","7337ea91":"lasso = Lasso()","f4b1aae3":"lasso.fit(X_train, y_train)","35806587":"pred_las = lasso.predict(X_test)","b3a08978":"pred_las_train = lasso.predict(X_train)","acea0f7b":"lasso.intercept_","80b0c15c":"coeff_df['lasso_coeff'] = lasso.coef_","8855b73f":"coeff_df","5d76a3f4":"plt.figure(figsize=(12,6))\n\nplt.scatter(y_test,pred_las)\n\nplt.plot(y_test.values,y_test.values,'r')   # best fit line that could be possible","e9505c69":"plt.figure(figsize=(10,5))\nsns.distplot(y_test - pred_las)","4367f643":"print('RMSE on train data:', np.sqrt(mean_squared_error(y_train, pred_las_train)))\nprint('\\n')\nprint('RMSE on test data:', np.sqrt(mean_squared_error(y_test, pred_las)))\nprint('\\n')\nprint('R2_score on train data:', r2_score(y_train, pred_las_train))\nprint('\\n')\nprint('R2_Score on test data:', r2_score(y_test, pred_las))","10ab3a7b":"from sklearn.ensemble import RandomForestRegressor\n\nrfg = RandomForestRegressor(n_estimators=500, random_state=101)\n\nrfg.fit(X_train, y_train)","4ca571b8":"rf_pred = rfg.predict(X_test)","1aa38121":"rf_pred_train = rfg.predict(X_train)","11c32aa3":"plt.figure(figsize=(10,5))\nsns.distplot(y_test - rf_pred)","655a0190":"print('RMSE on train data:', np.sqrt(mean_squared_error(y_train, rf_pred_train)))\nprint('\\n')\nprint('RMSE on test data:', np.sqrt(mean_squared_error(y_test, rf_pred)))\nprint('\\n')\nprint('R2_score on train data:', r2_score(y_train, rf_pred_train))\nprint('\\n')\nprint('R2_Score on test data:', r2_score(y_test, rf_pred))","6a173f61":"metric_frame = {'Model' :['Linear Regression','Ridge Regression','Lasso Regression', 'Random Forest Regression'],\n      'MAE': [mean_absolute_error(y_test, predictions),\n              mean_absolute_error(y_test, pred_rid),\n              mean_absolute_error(y_test, pred_las),\n              mean_absolute_error(y_test, rf_pred)], \n      'MSE':[mean_squared_error(y_test, predictions),\n             mean_squared_error(y_test, pred_rid),\n             mean_squared_error(y_test, pred_las),\n             mean_squared_error(y_test, rf_pred)],\n      'RMSE': [np.sqrt(mean_squared_error(y_test, predictions)),\n               np.sqrt(mean_squared_error(y_test, pred_rid)),\n               np.sqrt(mean_squared_error(y_test, pred_las)),\n               np.sqrt(mean_squared_error(y_test, rf_pred))],\n      'R2_score': [r2_score(y_test, predictions), r2_score(y_test, pred_rid), r2_score(y_test, pred_las), r2_score(y_test, rf_pred)]}","9d9a1f89":"metric_frame = pd.DataFrame(metric_frame)","a0aade43":"metric_frame","afc88e34":"plt.figure(figsize=(10,5))\nsns.set_style('white')\nplt.style.use('ggplot')\nmetric_frame.sort_values(by=['RMSE'], ascending=False, inplace=True)\nsns.barplot(x='Model', y='RMSE', data=metric_frame, hue='Model')","f40eb49f":"plt.figure(figsize=(10,8))\n\nmetric_frame.sort_values(by=['R2_score'], ascending=False, inplace=True)\nsns.barplot(x='Model', y='R2_score', data=metric_frame, hue='Model')","a3f8f8ff":"import random \nrandom.seed(101)\nrandom_ind = random.randint(0,len(df))\n\nnew_sample = X.iloc[random_ind]\n\nnew_sample","b4f26d1d":"new_sample.values.reshape(1,4)","891a98ad":"new_sample = scaler.transform(new_sample.values.reshape(1,4))","17ba6a3a":"rfg.predict(new_sample)","28e2c1bc":"y.iloc[random_ind]","0cee9823":"## Model Building:\n\n\n### Linear Regression:","29acbf37":"-----------\n## Random Forest Regressor:","f9729fd2":"----------\n## Train\/Test split:","57055e45":"###### It shows that there are some outliers in the output column because of the tail elongation at the end","448f5ea4":"### Random Forest model's prediction on this randomly selected data:","9d85ea5f":"### Import the Libraries and packages:","021ea04c":"### Model Evaluation :\n\n###### Let's evaluate the model by checking it's co-efficients and how we can interpret them :","57ac7b26":"### EDA","521ab914":"### As the values are highly deviated from the staright line, lets transform this INDUS variable to make it more linearly related with the MEDV:\n--------\n\n## Logarithmic transformation on INDUS:","e0255aae":"###### It is very clear that the values of LSTAT are not normally distributed and it has positive skewness. If this variable is not transformed to be a normally distributed variable, it can reduce the performance of the machine learning model. So lets transform this independent variable using logarithmic function:","17d70deb":"###### Here from the above plot we can say that Linear Regression is a good model for the data to predict the values appropiately\n\n\n###### Because there seems to be a linearity between Y_test and Predictions and hence predictions are almost correct\n\n\n## Linear Regression Assumption (Homoscedasticity) is satisfied here as the variance across our residuals is almost uniform except for a few ouliers \n----------------------","77fae834":"### Properties of the dataset:\n\n**1) Initially the dataset is stored in sklearn in the form of Bunch (which is a kind of dictionary) which has keys and corresponding values**\n\n**2) We have extracted the value or info of the respective keys like data, feature_names and target and stored in the form of a dataframe to perform analysis using pandas dataframe**\n\n**3) The memory usage of the dataset is 55.5 KB and it has 506 observations or rows with 14 columns where the last column is considered to be the o\/p column or dependent variable**\n\n**4) The data type of all the columns is numerical values (float values)**","3e5f16ec":"### Since Random Forest Regressor is giving very less MAE, MSE, RMSE compared to the remaining models and also having the highest R2 score of 0.8, we will consider Random Forest model for the deployment","71dfea69":"-----\n## Logarithmic transformation on PTRATIO variable:","2289b78c":"### Interpreting from the above co-efficients :\n\n###### Making all the other features fixed except LSTAT : Now one unit rise in LSTAT will lead to 5.97 units drops in the the MEDV ","b9e46ff0":"### Lets replace the LSTAT variable with this transformed LSTAT variable in the dataframe","e422c513":"## Actual value of that randomly selected data:","e9c6e437":"---\n\n## Linear Regression Assumption (No Multi-Collinearity):\n\n###### 1) Here, RAD and TAX have a strong correlation value of 0.91. So we should not train the model including both these features at the same time. \n\n###### 2) DIS and AGE have a strong correlation value of -0.747 so we should not include both these features together for the model training\n\n###### 3) INDUS is highly correlated with TAX with a value of 0.72 so we can ignore TAX and use INDUS for model training\n\n###### 4) INDUS is highly correlated with NOX with a value of 0.76 so we can ignore NOX and use INDUS for model training\n---------------\n\n#### So based on the correlation values and considering the effect of multi-collinearity we can consider LSTAT, PTRATIO, INDUS and RM to be the final independent variables for the model to be trained on ","c006d975":"### We can see that values of all the columns are on the similar scale now. Hence the model will learn accurately during training phase and performs better on the test data\n----------","34181101":"### We must make sure that new_sample is an numpy array and also to be in the same shape of the training data that the model was trained on ","2d7f9544":"-------\n## Predictions:","5c587a31":"### The model is doing pretty well on the unknown data as there is no big difference of RMSE on test data compared to RMSE on train data","d86fd701":"### R2 score tells how much variance of the data of the o\/p variable is explained by the input variables. If R2 score is high then model performed well on the data","5e098381":"###### These co-efficients relate to each feature in the data set :","14fc7332":"### Scatter plot with price vs remaining columns:","292f86bf":"--------------------\n\n## Ridge Regression:","2f9892ac":"### It is clear from the above plot that the most observations are far away from the line. So this PTRATIO feature if not transformed can decrease the performance of the machine learning model. So lets transform the PTRATIO variable using Logarithmic Function to make it linearly related with the dependent variable 'MEDV ","363e1284":"### It is clear that RM variable is linealry related with MEDV and the shaded region around the straight line tells that the predictions will be falling into that region with a 95% confidence interval as the default value of Confidence Interval is 95%","a5b1152c":"###### There is no target column in the above dataframe so lets add the target column:","7b860f6b":"### Now we can see there are no Outliers in the new transformed LSTAT variable. Lets check the distribution of LSTAT variable:","fff8c169":"## Testing the Model using Random Forest:\n\n-------\n-------\n\n##### So given a observation below, would you find the MEDV of that observation or sample?\n\n-------\n","c148a0d2":"----------\n## Data Cleaning:\n\n### Check for Null values:","62eb3560":"### It is very clear that there is a huge decrease in the skewness after transformation and it looks like LSTAT variable almost reached a perfect Gaussian or Normal distribution and it is because all the outliers are nullified after the logarithmic transformation\n-----------","bc04ddec":"![alt text](https:\/\/cdn10.bostonmagazine.com\/wp-content\/uploads\/sites\/2\/2019\/05\/boston-housing-social.jpg)\n\n###### Source: [Bouston_Housing](https:\/\/cdn10.bostonmagazine.com\/wp-content\/uploads\/sites\/2\/2019\/05\/boston-housing-social.jpg)","3f66fd08":"### Now lets compare y_test and predicted values:","b2aa981d":"### Since we have transformed the LSTAT variable, lets check whether we have any outliers in LSTAT as we had many outliers in this column before transformation:","7928cc9f":"-------\n### Check for Outliers:","ea667f3f":"-------\n## Lasso Regression:","d366acb0":"### Now we can see from the above plot that after applying logarithmic function on the INDUS variable, there is no much difference in the transformed variable comapred to the original INDUS variable. So lets try with an other transformation method:\n----------\n\n## Exponential Transformation on INDUS:","3c8061e6":"## Great!! Our model's prediction is perfect on the randomly selected data","3cd11e16":"## Linear Regression Assumption (Normality of Residuals):","17ecd3dc":"### Here we can see that coefficient of INDUS is put to zero and as LASSO is often used to select the right features to decrease the error on prediction by removing features that add noise to the model","52a94cc1":"---------\n## Linear Regression Assumption (Variables are Normally distributed):\n\n###### Since we have many Outliers in the LSTAT feature, the values of LSTAT variable may not be normally distributed. Lets check the distribution of LSTAT feature:","aa5c42cf":"-------------\n## Scaling:\n\n#### We will only do scaling post Split and then fit to train_set in that way we can stop the data leakage\n\n#### Since we have outliers in the data it is better to use RobustScaler method as it will punish the ouliers while calculating the scaling parameters as it uses quartile ranges for calculating the parameters ","1d885c66":"## Linea Regression Assumption (Linearity between independent variables and dependent variable):\n\n###### We can check the linear regression plot without building the linear regression model using lmplot from seaborn library:","27831dbd":"### So there is almost a perfect Normality of the residuals and the slight tail extension at the end is due to outliers\n--------------","111f4c12":"### Since the outliers were considered while training the model we can use the RMSE as our metric. RMSE can punish the outliers and also we have got a very minimum error rate of 5.8 units ","316872f8":"### So, from the above plot it is clear that LSTAT variable is linearly related with MEDV","449c297b":"### Since there is no much difference even after the exponential transformation we have to ignore this column but considering the high correlation value with the target variable its not good to drop this variable. So lets use Robust Scaler method instead of logarithmic transformations on all the selected original variables in order for thier values to be on the similar scale and obtain the normal distribution","3f6225a0":"### There is no big difference of RMSE on test data compared to RMSE on train data. So, the model is doing pretty well on test data","f25e6b10":"### Now we can see the difference between earlier plot and the transformed plot, after transforming the PTRATIO variable the values are very close to the straight line making it more linearly related with the dependent variable","c6e379ea":"---\n## Logarithmic transformation:","bf74d18c":"### There is no big difference of RMSE on test data compared to RMSE on train data. So, the model is doing pretty well","3d60c92a":"--------\n### So the Model is perfect except for the few outliers, if outliers were removed then we would have got the perfect best fit line and because of the noble outliers in our data which carries certain information they can't be ignored. \n\n---------\n### All the assumptions of Linear Regression are satisfied as well:\n\n#### 1) No multi-collinearity: As we have already removed inter correlated variables while developing the model\n\n#### 2) Linear Relationship: The four predictor variables were linearly related to the dependent variable \n\n#### 3) Homoscedasticity: satisfied as mentioned above with unifrom variance across the residuals\n\n#### 4) Normality of the residuals: The error terms of residuals are normally distributed as seen in the residual distribution plot above","7e3a3e44":"###### It is very clear that LSTAT, PTRATIO, INDUS, TAX, NOX, RM are highly correlated with MEDV variable"}}