{"cell_type":{"21699805":"code","f39144a8":"code","200e3793":"code","6d548568":"code","678502f7":"code","6f0bcc54":"code","852d1b36":"code","54dfb145":"code","f566d585":"code","6cdb1752":"code","93d4753f":"code","be543682":"code","86a99438":"code","3a24601e":"code","100906ab":"code","c8348c75":"code","aebe5233":"code","b8f2dbd9":"code","c5f5dccb":"code","34d6c115":"code","b91d2e52":"code","6ab43c4d":"code","07b34ffa":"code","1b1db4a6":"code","a6a8f544":"markdown","dd1f444a":"markdown","2eba733a":"markdown","f48efa26":"markdown","4bbe465f":"markdown","cc02c9e9":"markdown","e3c06482":"markdown","f21854bb":"markdown","dcfe5f43":"markdown","55c045ce":"markdown","9c5ca7a2":"markdown","3be210ee":"markdown","9640b9da":"markdown","93cd94fc":"markdown","1f6d8d4d":"markdown","306cceef":"markdown","13d9e10d":"markdown","8c13cfc5":"markdown","5cd10ff7":"markdown","59afc7fc":"markdown","babf4f1b":"markdown","ed2a5c5e":"markdown","acfa605e":"markdown","ade39b26":"markdown"},"source":{"21699805":"# Importing the required libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","f39144a8":"#reading csv file and creating a dataframe\ndf = pd.read_csv(\"..\/input\/Chickpea.data.csv\")\ndf.head()","200e3793":"#Na Handling\ndf.isnull().values.any()","6d548568":"df.shape","678502f7":"df.isnull()","6f0bcc54":"df1 = df.dropna()","852d1b36":"Y=df1['Predictor']\nY.describe()\nY=pd.DataFrame(Y)","54dfb145":"#collecting indepedent variables in X\nX = df1.iloc[:,1:332]\nX_col = X.columns\nX.head()","f566d585":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nY_encoded = Y.apply(le.fit_transform)\nY_encoded.head()","6cdb1752":"#Savitzky-Golay filter with second degree derivative.\nfrom scipy.signal import savgol_filter \n\nsg=savgol_filter(X,window_length=11, polyorder=3, deriv=2, delta=1.0)","93d4753f":"sg_x=pd.DataFrame(sg, columns=X_col)\nsg_x.head()","be543682":"# Splitting the data into train and test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test =train_test_split(sg_x, Y_encoded,\n                                                    test_size=0.2,\n                                                    random_state=123,stratify=Y)","86a99438":"from sklearn.tree import DecisionTreeClassifier\n\nDtree = DecisionTreeClassifier(random_state=52,criterion='entropy',min_samples_split=50)\nDtree_fit=Dtree.fit(X_train, y_train)","3a24601e":"y_pred = Dtree.predict(X_test)","100906ab":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nprint(\"Test Result:\\n\")        \nprint(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_test, y_pred)))\nprint(\"Classification Report: \\n {}\\n\".format(classification_report(y_test, y_pred)))\nprint(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_test,y_pred)))","c8348c75":"#Reduction of variables using Recursive Feature Elimination(RFE) techineque.\n\nfrom sklearn.feature_selection import RFE\n\n# RFE with 10 features\n\nrfe_10 = RFE(Dtree ,10)\n\nrfe_10.fit(X_train, y_train)\n\n# selected features\nfeatures_bool = np.array(rfe_10.support_)\nfeatures = np.array(X_col)\nresult = features[features_bool]\nprint('10 selected Features:')\nprint(result)","aebe5233":"y_pred = rfe_10.predict(X_test)\n\nprint(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_test, y_pred)))\nprint(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_test, y_pred)))","b8f2dbd9":"# RFE with 15 features\n\nrfe_15 = RFE(Dtree ,15)\n\n# fit with 15 features\nRFE15_fit=rfe_15.fit(X_train, y_train)\n\n# selected features\nfeatures_bool = np.array(rfe_15.support_)\nfeatures = np.array(X_col)\nresult = features[features_bool]\nprint('15 selected Features:')\nprint(result)","c5f5dccb":"y_pred = rfe_15.predict(X_test)\n\nprint(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_test, y_pred)))\nprint(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_test, y_pred)))","34d6c115":"from sklearn.ensemble import RandomForestClassifier\n\nRf = RandomForestClassifier(random_state=52)\nRf_fit=Rf.fit(X_train, y_train)","b91d2e52":"y_pred = Rf.predict(X_test)","6ab43c4d":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nprint(\"Test Result:\\n\")        \nprint(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_test, y_pred)))\nprint(\"Classification Report: \\n {}\\n\".format(classification_report(y_test, y_pred)))\nprint(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_test,y_pred)))","07b34ffa":"# RFE with 20 features\n\nrfe_20 = RFE(Rf,20)\n\n# fit with 20 features\nrfe_20.fit(X_train, y_train)","1b1db4a6":"y_pred = rfe_20.predict(X_test)\n\nprint(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_test, y_pred)))\nprint(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_test, y_pred)))","a6a8f544":"## **Let's see the accuracy with 10 features**","dd1f444a":"## **Applying Pre-Processing techniques**","2eba733a":"## **Handling NA values**","f48efa26":"## **We got a 69% accuracy in predicting the cultivars with 15 feature selected**","4bbe465f":"## **Putting the independent variable in another varaible**","cc02c9e9":"## **importing important library**","e3c06482":"## **Using classifier1 as decision tree**","f21854bb":"## **Selecting the 10 features with RFE** ","dcfe5f43":"## **Got an 65.79 accuracy with 10 features selected**","55c045ce":"## **Applying Savitzky-Golay filter** ","9c5ca7a2":"## **since the NA value is absent with the total features. I can drop the NA values** ","3be210ee":"## **Let's see with Another Classifier Random Forest**","9640b9da":"## **looking the total records and features**","93cd94fc":"## **Reading the csv file of chickpea**","1f6d8d4d":"## **Let's see the accuracy with 15 features selected**","306cceef":"# **I got an accuracy of 71.05% with all the features and the confusion matrix of 19 * 19**","13d9e10d":"# **I got 80 % result with 20 feature selected in random forest**","8c13cfc5":"# **Again if we increase the feature the accuracy decreases**","5cd10ff7":"## **checking the NA value is valid or invalid**","59afc7fc":"## **predicting the accuracy with all the features**","babf4f1b":"# **I got 84.74% result with all the feature selected in random forest with 19 * 19 confusion matrix**","ed2a5c5e":"## **putting the dependent varaible in one variable**","acfa605e":"## **15 feature selected with RFE let us see again the accuracy rate**","ade39b26":"## **Splitting the dataset into training and test**"}}