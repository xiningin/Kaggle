{"cell_type":{"aae02e09":"code","9f1f00fa":"code","b5995e40":"code","5c97d858":"code","5f8513df":"code","8cd6a11c":"code","abadf0ac":"code","745d5954":"code","1820bff2":"code","83c4d9dd":"code","5efe505c":"code","fe2b2680":"code","fab8e16a":"code","e581d5a0":"code","5bb7fbf6":"code","1c4de2d6":"code","009dca00":"code","7148178a":"code","4f2c7bd8":"code","d724a087":"code","d0c25074":"markdown","2652a8e4":"markdown","31328de6":"markdown","193cf013":"markdown","cbb9c7a1":"markdown","28aa1da9":"markdown","d1462c57":"markdown","a709a6b0":"markdown","821883ba":"markdown","f2f91eeb":"markdown","0134a1ef":"markdown","506a52b5":"markdown","4fc16470":"markdown","d447976b":"markdown","6db4cb4f":"markdown","93a01a07":"markdown","56b51d6c":"markdown","9b9a7033":"markdown","ca63ccd0":"markdown","9d7f8046":"markdown","5cac5821":"markdown","554e58df":"markdown","df0dd550":"markdown","211a4adc":"markdown","d5c64124":"markdown"},"source":{"aae02e09":"import numpy as np \nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom keras.preprocessing.image import ImageDataGenerator, load_img\nfrom keras.layers import Conv2D, BatchNormalization, Activation, MaxPooling2D, GlobalAveragePooling2D, Dense, Flatten, Dropout\nfrom keras.optimizers import RMSprop, Adam, SGD\nfrom keras import regularizers\nfrom keras.callbacks import CSVLogger, ModelCheckpoint, ReduceLROnPlateau","9f1f00fa":"train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\ntrain.head(5)","b5995e40":"print('Train set: ',train.shape, \"\\t Test Set\", test.shape)","5c97d858":"y = train['label']\n# droping label cloumn in training set\ntrain.drop('label', axis=1, inplace=True)","5f8513df":"g = sns.countplot(y)","8cd6a11c":"plt.figure(figsize=(12,5))\nfor i in range(40):\n    plt.subplot(4,10,i+1)\n    img = train.iloc[i,:].values.reshape(28,28)\n    plt.imshow(img)\n    plt.axis('off')\nplt.tight_layout()\nplt.show()","abadf0ac":"from keras.utils.np_utils import to_categorical \ny = to_categorical(y, num_classes = 10)\ny[0]","745d5954":"train = train.values.reshape(train.shape[0], 28, 28, 1)\ntest = test.values.reshape(test.shape[0], 28, 28, 1)\nprint('Reshaped Train set: ',train.shape, \" & Reshaped Test Set\", test.shape)","1820bff2":"train = train.astype(\"float32\")\/255.0\ntest = test.astype(\"float32\")\/255.0","83c4d9dd":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(train, y, test_size=0.25, random_state=0)\n\nprint(\"Number of samples in Training set :\", X_train.shape[0])\nprint(\"Number of samples in Validation set :\", X_val.shape[0])","5efe505c":"train_datagen = ImageDataGenerator(rotation_range=10,\n                                   zoom_range=0.1,\n                                   width_shift_range=0.1,\n                                   height_shift_range=0.1\n                                  )\n\ntraining_set = train_datagen.flow(X_train, y_train,\n                                  batch_size=64\n                                 )\n\nval_datagen = ImageDataGenerator()\nval_set = val_datagen.flow(X_val, y_val,\n                           batch_size=64\n                          )","fe2b2680":"model = tf.keras.models.Sequential()\n\nmodel.add(Conv2D(64, kernel_size=(5,5), padding='same', activation='relu', input_shape=(28,28,1)))\nmodel.add(Conv2D(64, kernel_size=(5,5), padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(128, kernel_size=(3,3), activation='relu', padding='same'))\nmodel.add(Conv2D(128, kernel_size=(3,3), activation='relu', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(256, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy']\n             )\nmodel.summary()","fab8e16a":"# If the model is not improving on validation, we need to reduce the learning rate, If val loss is not improved in 4 epoch then lr will be reduced \nreduce_lr = ReduceLROnPlateau(monitor='val_loss', \n                              factor=0.2, \n                              patience=4, \n                              verbose=1, \n                              min_delta=0.0001)","e581d5a0":"steps_per_epoch = training_set.n \/\/ training_set.batch_size\nvalidation_steps = val_set.n \/\/ val_set.batch_size\n\nhist = model.fit(x=training_set,\n                 validation_data=val_set,\n                 epochs=35,\n                 callbacks=[reduce_lr],\n                 steps_per_epoch=steps_per_epoch,\n                 validation_steps=validation_steps\n                )","5bb7fbf6":"plt.figure(figsize=(14,5))\nplt.subplot(1,2,2)\nplt.plot(hist.history['accuracy'])\nplt.plot(hist.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(['train', 'test'], loc='upper left')\n\nplt.subplot(1,2,1)\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title('model Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","1c4de2d6":"_, acc_val = model.evaluate(val_set)\n_, acc_tr = model.evaluate(val_set)\nprint(\"\\nFinal Accuracy on training set : {:.2f}% & accuracy on validation is set: {:.2f}%\".format(acc_tr*100, acc_val*100))","009dca00":"from keras.utils import plot_model\nplot_model(model, show_shapes=True, show_layer_names=True)","7148178a":"val_pred = model.predict(val_set)\nval_pred = np.argmax(val_pred, axis=1)\ny_val = np.argmax(y_val, axis=1)\n\nfrom sklearn.metrics import confusion_matrix, classification_report\nprint(\"Confusion Matrix\")\ncm = confusion_matrix(y_val, val_pred)\nprint(cm)\nprint(\"Classification Report\")\nprint(classification_report(y_val, val_pred))\n\n#g = sns.heatmap(cm, cmap='Blues')\nplt.figure(figsize=(8,8))\nplt.imshow(cm, interpolation='nearest')\nplt.colorbar()\ntarget_names = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\ntick_mark = np.arange(len(target_names))\n_ = plt.xticks(tick_mark, target_names)\n_ = plt.yticks(tick_mark, target_names)","4f2c7bd8":"pred = model.predict(test)\nres = np.argmax(pred, axis=1)\nsubmission = pd.DataFrame({\"ImageId\":[i+1 for i in range(len(test))],\n                           \"Label\": res})\nsubmission.head(10)","d724a087":"submission.to_csv(\"submission.csv\", index=False)","d0c25074":"MNIST images are gray scaled so it use only 1 channel, we would reshape the image to 28x28x1","2652a8e4":"# 4. CNN Model","31328de6":"# 5. Final Prediction","193cf013":"# 2. Importing Dataset","cbb9c7a1":"# 3 Data Preprocessing","28aa1da9":"So there are 42k images in training set and 28k images in testing set","d1462c57":"## 2.1 Plotting Figures","a709a6b0":"### 3.5 Data Argumentation","821883ba":"### 3.3 Data Normalization\n\nWe have to perform normalization on  images [0..1] data than on [0..255]. to reduce the effect of illumination's differences.\n\nThe Normailzed data learns faster as there is less gradient to cover","f2f91eeb":"### 4.2 Model Training ","0134a1ef":"### 4.4 Confusion Matrix and Classification Report","506a52b5":"Labels are 10 digits numbers from 0 to 9. We need to encode these lables to one hot vectors (ex : 9 -> [0,0,0,0,0,0,0,0,0,1]).","4fc16470":"* Randomly rotate some training images by 10 degrees\n* Randomly Zoom by 10% some training images\n* Randomly shift images horizontally by 10% of the width\n* Randomly shift images vertically by 10% of the height\n\nSince vertical_flip nor horizontal_flip could have lead to misclassify symetrical numbers such as 6 and 9, so i did not use it","d447976b":"now the final accuracy of model on test and training set satisfactiory","6db4cb4f":"### 4.1 Creating Model\nI tried several model and this has given me the best accuracy so far","93a01a07":"### 3.1 Converting Labels to Categorical","56b51d6c":"i might have run it more a bit longer i think 25 or 20 epochs will do fine","9b9a7033":"# 1. Importing Libraries","ca63ccd0":"* Your feedback in comments is much appreciated, Comment if you have any doubts or for inprovement\n* Please UPVOTE if you LIKE this notebook, it will keep me motivated","9d7f8046":"### 3.2 Reshaping ","5cac5821":"> Currently i am writing this notebook my rank is `300` in MNIST with score of `.99567`9\n16\/10\/20\n\n# MNIST - Digit Recognition using Tensorflow\n\n### Contents\n\n* 1. Importing Libraries\n* 2. Importing Dataset\n    * 2.1 Histogram\n    * 2.1 Plotting Figures\n* 3. Data Preprocessing\n    * 3.1 Converting Labels to Categorical\n    * 3.2 Reshaping\n    * 3.3 Data Normalization\n    * 3.4 Spliting Data into Training Set & Validation Set\n    * 3.5 Data Argumentation\n* 4. CNN Model\n    * 4.1 Creating Model\n    * 4.2 Model Training\n    * 4.3 Model Accuracy and Loss Plot\n    * 4.4 Confusion Matrix and Classification Report\n* 5. Final Prediction","554e58df":"### 3.4 Spliting Data into Training Set & Validation Set\nWe have 42k images in training set we can split it into training and validation sets, so that we can be sure that the model is not ovefitted on training data. I have splites 20 % of data to validation set","df0dd550":"### 2.1 Histogram","211a4adc":"### 4.3 Model Accuracy and Loss Plot","d5c64124":"The plot shows the frequency of Labels, the most fequent label is 1"}}