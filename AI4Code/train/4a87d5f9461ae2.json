{"cell_type":{"b2a47aa1":"code","260f744c":"code","73cdcfe0":"code","6dec36b7":"code","d0627c71":"code","e0dbba38":"code","b6084693":"code","4058dc37":"code","46fce01a":"code","fe6d430e":"code","70622cea":"code","66d5bb88":"code","20d5eab2":"code","42b52212":"code","2dd03876":"code","f8161065":"code","19d8edb9":"code","a0e99c36":"code","8a278cf1":"code","e6a164f7":"code","f5859905":"code","4f1f0581":"code","cb1f53f8":"code","843da0dc":"code","1ce26608":"code","52395281":"code","2cb3643b":"code","d0ea49b2":"code","f539d708":"markdown"},"source":{"b2a47aa1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","260f744c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.core.interactiveshell import InteractiveShell\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import RFE\nimport time\nfrom pprint import pprint\nfrom tabulate import tabulate\nfrom sklearn.tree import export_graphviz\nimport eli5\nfrom eli5.sklearn import PermutationImportance","73cdcfe0":"df = pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')","6dec36b7":"df.head()","d0627c71":"df['age'].value_counts()","e0dbba38":"df = df[df['thal'] != 0]\ndf = df[df['age'] != 29]\ndf.head()","b6084693":"df['thal'] = df['thal'].replace(1, 'fixed defect')\ndf['thal'] = df['thal'].replace(2, 'normal')\ndf['thal'] = df['thal'].replace(3, 'reversable defect')\ndf['cp'] = df['cp'].replace(0, 'asymptomatic')\ndf['cp'] = df['cp'].replace(1, 'atypical angina')\ndf['cp'] = df['cp'].replace(2, 'non-anginal pain')\ndf['cp'] = df['cp'].replace(3, 'typical angina')\ndf['restecg'] = df['restecg'].replace(0, 'ventricular hypertrophy')\ndf['restecg'] = df['restecg'].replace(1, 'normal')\ndf['restecg'] = df['restecg'].replace(2, 'ST-T wave abnormality')\ndf['slope'] = df['slope'].replace(0, 'downsloping')\ndf['slope'] = df['slope'].replace(1, 'flat')\ndf['slope'] = df['slope'].replace(2, 'upsloping')","4058dc37":"temp = pd.get_dummies(df[['cp', 'restecg', 'slope', 'thal']])\ndf = df.join(temp, how='left')\ndf = df.drop(columns = ['cp','restecg', 'slope', 'thal'], axis=1)\ndf.head()","46fce01a":"df = df.drop(columns = ['restecg_ventricular hypertrophy', 'slope_upsloping', 'thal_fixed defect', 'cp_typical angina'], axis=1)\ndf.head()","fe6d430e":"df = df.drop_duplicates()\ndf.shape","70622cea":"df.corr()","66d5bb88":"ax = sns.heatmap(df, annot=True)","20d5eab2":"sns.factorplot('age', kind='count', hue='target', data=df, palette='coolwarm', height=10, aspect=.8)","42b52212":"plt.figure(figsize=(8,8))\nsns.scatterplot(x=df['age'],y=df['thalach'],hue=df['target'])\nplt.xlabel('age')\nplt.ylabel('thalach')\nplt.show()","2dd03876":"sns.countplot(df.target)\ndf.target.value_counts()","f8161065":"features = ['age', 'ca', 'cp_asymptomatic', 'exang', 'oldpeak',\n  'slope_flat', 'thal_normal', 'thal_reversable defect',\n  'thalach', 'cp_non-anginal pain', 'trestbps',\n  'sex', 'chol', 'restecg_normal', 'cp_atypical angina',\n  'slope_downsloping', 'fbs','restecg_ST-T wave abnormality', 'target']\n\ndf = df[features]","19d8edb9":"X = df.drop(['target'], axis=1)\nY = df['target']\ntrain_features, test_features, train_labels, test_labels = train_test_split(X, Y, test_size=0.2, random_state=0)","a0e99c36":"rf = RandomForestClassifier()\ndt = DecisionTreeClassifier()\nlr = LogisticRegression()\nknn = KNeighborsClassifier()","8a278cf1":"# Create the parameter grid based on the results of random search \nparam_grid = {\n    'max_depth': [2],\n    'min_samples_leaf': [5],\n    'n_estimators': [200],\n    'oob_score': [True],\n    'random_state': [0],\n}\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                      cv = 3, n_jobs = -1, verbose = 2, return_train_score=True)\n\n# Fit the grid search to the data\ngrid_search.fit(train_features, train_labels);\n\ngrid_search.best_params_\nbest_grid = grid_search.best_estimator_\npprint(best_grid.get_params())\n\nselector = RFE(rf, step=1, verbose=3)\nselector = selector.fit(train_features, train_labels)\nprint(\"Features sorted by their rank:\")\npprint(sorted(zip(map(lambda x: round(x, 4), selector.ranking_), X)))","e6a164f7":"rf = RandomForestClassifier(**best_grid.get_params())\ndt = DecisionTreeClassifier()\nlr = LogisticRegression()\nknn = KNeighborsClassifier()","f5859905":"rf.fit(train_features, train_labels)\ndt.fit(train_features, train_labels)\nlr.fit(train_features, train_labels)\nknn.fit(train_features, train_labels)","4f1f0581":"rf_pred_train = rf.predict(train_features)\ndt_pred_train = dt.predict(train_features)\nlr_pred_train = lr.predict(train_features)\nknn_pred_train = knn.predict(train_features)\nrf_pred_test = rf.predict(test_features)\ndt_pred_test = dt.predict(test_features)\nlr_pred_test = lr.predict(test_features)\nknn_pred_test = knn.predict(test_features)","cb1f53f8":"rf_prob = rf.predict_proba(test_features)[:,1]\ndt_prob = dt.predict_proba(test_features)[:,1]\nlr_prob = lr.predict_proba(test_features)[:,1]\nknn_prob = knn.predict_proba(test_features)[:,1]","843da0dc":"rf_prob","1ce26608":"print(classification_report(test_labels,rf_pred_test))\nprint('Random Forest baseline: ' + str(roc_auc_score(train_labels, rf_pred_train)))\nprint('Random Forest: ' + str(roc_auc_score(test_labels, rf_pred_test)))\nprint(classification_report(test_labels,dt_pred_test))\nprint('Decision Tree baseline: ' + str(roc_auc_score(train_labels, dt_pred_train)))\nprint('Decision Tree: ' + str(roc_auc_score(test_labels, dt_pred_test)))\nprint(classification_report(test_labels,lr_pred_test))\nprint('Logistic Regression baseline: ' + str(roc_auc_score(train_labels, lr_pred_train)))\nprint('Logistic Regression: ' + str(roc_auc_score(test_labels, lr_pred_test)))\nprint(classification_report(test_labels,knn_pred_test))\nprint('KNN baseline: ' + str(roc_auc_score(train_labels, knn_pred_train)))\nprint('KNN: ' + str(roc_auc_score(test_labels, knn_pred_test)))","52395281":"ns_probs = [0 for _ in range(len(test_labels))]\nns_fpr, ns_tpr, _ = roc_curve(test_labels, ns_probs)\nlr_fpr, lr_tpr, _ = roc_curve(test_labels, rf_prob)\nplt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\nplt.plot(lr_fpr, lr_tpr, marker='.', label='Random Forest')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.show()","2cb3643b":"prec, rec, tre = precision_recall_curve(test_labels, rf_prob)\ndef plot_prec_recall_vs_tresh(precisions, recalls, thresholds):\n    fig, ax = plt.subplots(figsize=(10,6))\n    plt.plot(thresholds, precisions[:-1], 'r', label='Precisions')\n    plt.plot(thresholds, recalls[:-1], '#424242', label='Recalls')\n    plt.ylabel('Level of Precision and Recall', fontsize=12)\n    plt.title('Precision and Recall Scores as a function of the decision threshold', fontsize=12)\n    plt.xlabel('Thresholds', fontsize=12)\n    plt.legend(loc='best', fontsize=12)\n    plt.ylim([0,1])\n    plt.xlim([0,1])\n    plt.axvline(x=0.5, linewidth=3, color='#0B3861')\n\nplot_prec_recall_vs_tresh(prec, rec, tre)\nplt.show()","d0ea49b2":"perm = PermutationImportance(rf, random_state=1).fit(train_features, train_labels)\neli5.show_weights(perm, feature_names = X.columns.tolist())","f539d708":"***Exploratory data analysis***"}}