{"cell_type":{"63cfffe2":"code","085ce146":"code","52d3e486":"code","122f9459":"code","b0ecd3b9":"code","4fccf6b7":"code","362d1ba8":"code","cea06bc2":"code","260f5bcf":"code","ba7956e1":"code","921eb113":"code","f7713a84":"code","e9b645fe":"code","5a66f73b":"code","30d6fe71":"code","26f99a7f":"code","03e22ca5":"code","58a34d8e":"markdown","44da20bd":"markdown","0199fa4d":"markdown","13cc80b2":"markdown","c28575c5":"markdown","69c12c1d":"markdown","4474e4b2":"markdown","1790a3c3":"markdown","873c59a5":"markdown","90456588":"markdown","643154f5":"markdown","15057f31":"markdown","2955b0ff":"markdown","ee6b1bec":"markdown","df6c4814":"markdown","a455edbd":"markdown","aa2378c7":"markdown"},"source":{"63cfffe2":"import os\nimport sys\nimport cv2\nimport torch\nimport random\nimport warnings\nimport torchvision\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nimport seaborn as sns\nimport pydicom as pdcm\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as T\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import random_split\nfrom sklearn.model_selection import KFold\n\nwarnings.filterwarnings(\"ignore\")\nplt.rcParams['figure.figsize'] = 12, 6 ","085ce146":"main_path = \"\/kaggle\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\"\nfor files in os.listdir(main_path):\n    print(files)","52d3e486":"train_df = pd.read_csv(f\"{main_path}\/train_labels.csv\")\ntest_df = pd.read_csv(f\"{main_path}\/sample_submission.csv\")\n\nfor ids in [109, 123, 709]:\n    train_df.drop(train_df[train_df[\"BraTS21ID\"]==ids].index, inplace=True)    ","122f9459":"mp_mri_type = ['FLAIR', 'T1w', 'T1wCE', 'T2w']\n\ndef build_df(dataframe, train=True):\n    dir_type = (\"train\" if train else \"test\")\n    for mp_type in mp_mri_type:\n        count = [] \n        for ids in dataframe[\"BraTS21ID\"]:\n            fid = str(ids).zfill(5)\n            file = os.listdir(f\"{main_path}\/{dir_type}\/{fid}\/{mp_type}\")\n            count.append(len(file))\n        dataframe[mp_type] = count    \n    return dataframe\n\ntrain_df = build_df(train_df, train=True)\ntest_df = build_df(test_df, train=False)\n\ntrain_df[\"CountImages\"] = train_df[\"FLAIR\"] + train_df[\"T1w\"] + train_df[\"T2w\"] + train_df[\"T1wCE\"]\ntest_df[\"CountImages\"] = test_df[\"FLAIR\"] + test_df[\"T1w\"] + test_df[\"T2w\"] + test_df[\"T1wCE\"]","b0ecd3b9":"train_df.describe()","4fccf6b7":"test_df.describe()","362d1ba8":"sns.countplot(x='MGMT_value', data=train_df)\nplt.show()","cea06bc2":"IMG_SIZE = 64\nBATCH_SIZE = 64\nNUM_SAMPLES = 64\nEPOCHS = 9\nLRate = 0.005\nSEED = 42","260f5bcf":"def sort_paths(path):\n    return sorted(path, key=lambda x: int(x.split('-')[1].split('.')[0]))\n\ndef read_dicom_img(dataset_dir, file_id):\n    file_path = f\"{main_path}\/{dataset_dir}\/{file_id.zfill(5)}\"\n    final_img = []\n    for mri_type in mp_mri_type:\n        final_file_path = f\"{file_path}\/{mri_type}\"\n        sorted_files = sort_paths(os.listdir(final_file_path))\n        \n        for i in range(len(sorted_files)) :\n            data = pdcm.dcmread(f\"{final_file_path}\/{sorted_files[i]}\")\n            img_data = data.pixel_array\n            img_data = cv2.resize(img_data, (IMG_SIZE, IMG_SIZE))\n            img_data = (img_data - np.min(img_data))\/ (np.max(img_data)+1)\n            final_img.append(img_data)            \n    \n    final_img = sorted(final_img, key=lambda x: np.mean(x), reverse=True)[:NUM_SAMPLES]\n    return np.asarray(final_img)","ba7956e1":"def show_img(data, axis, row):\n    for i in range(6):\n        axis[row][i].imshow(data[i], cmap=\"gray\") \n        axis[row][i].set_axis_off()","921eb113":"dataset_dir = \"train\"\nfig, ax = plt.subplots(nrows=5, ncols=6, figsize=(10,15))\nfor index in range(5):\n    id = train_df['BraTS21ID'][index]\n    all_imgs = read_dicom_img(dataset_dir, str(id))\n    ax[index][2].set_title(f\"MGMT: {('Present' if train_df['MGMT_value'][index] else 'Absent')}\", \n                           fontsize=15, fontweight =\"bold\")\n    show_img(all_imgs, ax, index)\n\nplt.subplots_adjust(wspace=0, hspace=0)\nplt.show() \n    ","f7713a84":"class MRIdata(Dataset):\n    def __init__(self, path_to_data, train=True, transform=None):\n        self.data = pd.read_csv(path_to_data)\n        self.labels = self.data['MGMT_value']\n        self.transform = transform\n        self.train = train\n        self.train_dir = (\"train\" if train else \"test\")\n\n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        inp_data = read_dicom_img(self.train_dir, str(self.data['BraTS21ID'][index]))\n        inp_data = self.transform(inp_data).permute(1,0,2)\n    \n        if self.train:\n            labels = torch.tensor(self.labels[index], dtype=torch.float)\n            final_data = {\"features\": torch.tensor(inp_data).float(),\n                          \"labels\": labels}\n        else:\n            final_data = {\"features\": torch.tensor(inp_data).float(),\n                         \"IDs\": self.data['BraTS21ID'][index]}\n        \n        return final_data","e9b645fe":"transforms = T.Compose([T.ToTensor()])\n\ndata = MRIdata(f\"{main_path}\/train_labels.csv\", transform=transforms)\n\ntrain_loader = DataLoader(data, shuffle=True, batch_size=BATCH_SIZE)","5a66f73b":"class MRINet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Sequential(self.conv_layer(in_chan=64, out_chan=128),\n                                  self.conv_layer(in_chan=128, out_chan=128),\n                                  self.conv_layer(in_chan=128, out_chan=256))\n        \n        self.fc = nn.Sequential(nn.Linear(9216,512),\n                                nn.Dropout(p=0.15),\n                                nn.Linear(512, 1))\n        \n    \n    def conv_layer(self, in_chan, out_chan):\n        conv_layer = nn.Sequential(\n            nn.Conv2d(in_chan, out_chan, kernel_size=(3,3), padding=0),\n            nn.LeakyReLU(),\n            nn.MaxPool2d((2,2)),\n            nn.BatchNorm2d(out_chan))\n        \n        return conv_layer    \n           \n    def forward(self, x):\n        out = self.conv(x)\n        out = out.view(out.size(0), -1)\n        out = self.fc(out)    \n        return out","30d6fe71":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nnet = MRINet().to(device)\n\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(net.parameters(), lr=LRate)\n\nfor epoch in range(EPOCHS):\n    total_loss = 0.0\n    count = 0\n    for indx, data in enumerate(train_loader, 0):\n        inputs, labels = data[\"features\"].to(device), data[\"labels\"].to(device)\n        optimizer.zero_grad()\n        outputs = net(inputs).squeeze(1)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        total_loss += loss.detach().item()\n        optimizer.step()\n        \n        count += 1\n    \n    print(f\"Epoch:{epoch}\/{EPOCHS} - Loss:{total_loss\/count}\")\n\nprint(\"Training Complete\")    \n        ","26f99a7f":"test_data = MRIdata(f\"{main_path}\/sample_submission.csv\", train=False,\n                    transform=transforms)\n\ntest_loader = DataLoader(test_data, shuffle=False, batch_size=1)\n\npreds = []\n\nfor query in test_loader:\n    with torch.no_grad():\n        output = net(query[\"features\"].to(device))\n        output = torch.sigmoid(output).cpu().numpy().squeeze()\n        preds.append(output)\n    ","03e22ca5":"submission = pd.read_csv(f\"{main_path}\/sample_submission.csv\", index_col=\"BraTS21ID\")\nsubmission[\"MGMT_value\"] = preds\nsubmission[\"MGMT_value\"].to_csv(\"submission.csv\")","58a34d8e":"### **Analysis of the given MRI dataset**","44da20bd":"The above analysis show that the minimum total image count for a given subject taking into account all the variants is **80 for *train* and 92 for *test***. This is important as we can not exceed beyond these values while building the input data (for the given network architecture). For training for a single subject images from all the variants are taken to build a dataset of the form of (NUM_SAMPLES, IMG_SIZE, IMG_SIZE), where NUM_SAMPLES = 64.\n\n<span style=\"color:red\">**NOTE:** NUM_SAMPLES is not batch size, consider it as equivalent to frames in a video data.<\/span>\n\n\n*MGMT_value* is the label where 1 stands for presence and 0 for absence of MGMT in the given subject","0199fa4d":"The current dataset does not suffer from **data imbalance problem**, since there is a very small difference between the count of the samples of the two classes.","13cc80b2":"The train folder consists of sub folders which are the independent cases represented by a five digit number. For each case, four sub-folder exists which correspond to each of the **structural multi-parametric MRI** scans in **DICOM** format.  \n\n##### **Structural multi-parametric MRI(mpMRI)**\nMP-MRI, an important type of MRI which creates a more informative picture compared to a single MRI, since it combines different types of images. There can different variants but the current dataset includes:\n* Fluid Attenuated Inversion Recovery (FLAIR)\n* T1-weighted pre-contrast (T1w)\n* T1-weighted post-contrast (T1Gd)\n* T2-weighted (T2)\n\n![image.png](attachment:3e54fa72-c9a7-4e33-804e-69bdd7fb9227.png)\n![comparison-t2-clinical-t1-basics-original.jpeg](attachment:c3143f99-2611-4202-8b22-637afd7bf1cf.jpeg)\n\nThese different categories are mostly due to the ability of MR scanners to gather information depending on the pulse sequence. Each one differ by intensity which represents different tissues.\n\n##### **Digital Imaging and Communication in Medicine (DICOM)**\nA de-facto standard format with a set of well defined rules used for storing and transmitting medical images.","c28575c5":"### **Basics of Magnetic Resonance Imaging (MRI)**\nMRI is an important *diagnostic* tool for visulaizing the soft tissues of the body. The main objective is to map the internal body structures which are hidden by the skin and bones. This helps in diagnosis and proper treatment without a need of an external surgery for analysis.\n\n![image.png](attachment:6c5e796a-7a5b-446d-893c-a045c2ae7c99.png)\n\n#### **Working Methodology**\n* Our body is made up of water molecules.\n* MRI use a powerful magnetic field to align the protons of the water molecules of the tissues which are otherwise randomly orientated.\n* Further an external RF signal is used for the perturbation of this alignment.\n* As the nuclei is relieved an RF energy is emitted which is measured.\n* These frequency signals are converted using Fourier transform for further analysis.\n","69c12c1d":"### **Image Visualization**\nShowing some sample images and its label for each subject","4474e4b2":"### **Import Modules**","1790a3c3":"### **Adding Metadata**\nAdditional Informations like count of images of all the variants which are useful for stastical analysis is added to the previous dataframe. This also helps in preventing errors while building the input data for training.","873c59a5":"### **Training**","90456588":"### **RSNA-MICCAI Brain Tumor Radiogenomic Classification**\n* *Glioblastoma* is a malignant tumor in the brain which is the most common form of brain cancer in adults and with the worst prognosis.\n* A specific genetic sequence in the tumor known as MGMT promoter methylation is a favorable prognostic factor.\n* The current competition requires to predict the genetic subtype of glioblastoma using MRI scans to train and test model to detect the presence of MGMT.","643154f5":"### **Hyperparameters**","15057f31":"### **Submission**","2955b0ff":"### **Network Architecture**","ee6b1bec":"### **Filter Irrelevant Data**\nAs shared in different notebooks the dataset with ids (109, 123, 709) are redundant, hence they are removed using the following code.","df6c4814":"### **Feature Engineering**\n* When using a black box tool like Deep Learning, it is generally left to the model to extract the important features from the dataset and predict the relevant class. \n* But there are irrelevant noisy data which worsens the trainnig.\n* More data does not always mean better model.\n* Such data should be removed beforehand to improve performance.\n* In the current MRI classification, the initial and last time stamps consists of many **complete black frames** and **small blobs** which does not help in prediction.\n* For a given input data only the top 64 frames (all 4 variants) are used which has highest average pixel intensity.\n* This condition assumes images with the highest average has full slice of brain image leading to better model training.","a455edbd":"### **Inference**","aa2378c7":"### **Data Loader**"}}