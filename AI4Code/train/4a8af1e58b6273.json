{"cell_type":{"457adf80":"code","adf0fe19":"code","8fe37069":"code","b6533eb2":"code","2fcecfd8":"code","aff54431":"markdown","f07338b1":"markdown","f9f556d7":"markdown","2da5cf30":"markdown","3360dfb8":"markdown"},"source":{"457adf80":"#Get the twitter_samples database with 5000 positive tweets and 5000 negative tweets\nimport nltk\n# nltk.download(\"twitter_samples\")\nfrom nltk.corpus import twitter_samples\nprint (twitter_samples.fileids())#Should show three files","adf0fe19":"#Clean and Tokenize tweets\nimport string\nimport re\n \nfrom nltk.corpus import stopwords \nstopwords_english = stopwords.words('english')\n \nfrom nltk.stem import PorterStemmer\nstemmer = PorterStemmer() #This is used to reduce the words to their stem word ('likely' becomes 'like').\n\nfrom nltk.tokenize import TweetTokenizer\n\n#Use a variable for the positive, negative and all tweets using their respective file name:\npos_tweets = twitter_samples.strings('positive_tweets.json')\nneg_tweets = twitter_samples.strings('negative_tweets.json')\nall_tweets = twitter_samples.strings('tweets.20150430-223406.json')\n\n#Choose elements to remove:\n# Happy Emoticons\nemoticons_happy = set([\n    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n    '<3'\n    ])\n# Sad Emoticons\nemoticons_sad = set([\n    ':L', ':-\/', '>:\/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n    ':-[', ':-<', '=\\\\', '=\/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n    ':c', ':{', '>:\\\\', ';('\n    ])\n# all emoticons (happy + sad)\nemoticons = emoticons_happy.union(emoticons_sad)\n\ndef cleanAndTokenizeTweets(tweet):\n    # remove stock market tickers like $GE\n    tweet = re.sub(r'\\$\\w*', '', tweet)\n \n    # remove old style retweet text \"RT\"\n    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n \n    # remove hyperlinks\n    tweet = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', tweet)\n    \n    # remove hashtags\n    # only removing the hash # sign from the word\n    tweet = re.sub(r'#', '', tweet)\n \n    # tokenize tweets\n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n    tweet_tokens = tokenizer.tokenize(tweet)\n \n    tweets_clean = []    \n    for word in tweet_tokens:\n        if (word not in stopwords_english and # remove stopwords\n              word not in emoticons and # remove emoticons\n                word not in string.punctuation): # remove punctuation\n            #tweets_clean.append(word)\n            stem_word = stemmer.stem(word) # stemming word\n            tweets_clean.append(stem_word)\n \n    return tweets_clean\n\n#Create a bag of words function that cleans and tokenizes tweets, then returns the bag.\ndef bag_of_words(tweet):\n    words = cleanAndTokenizeTweets(tweet)\n    words_dictionary = dict([word, True] for word in words)    \n    return words_dictionary\n\n#Create a list of bags of words with all positive tweets\npos_tweets_set = []\nfor tweet in pos_tweets:\n    pos_tweets_set.append((bag_of_words(tweet), 'pos')) \n    \n#Create a list of bags of words with all negative tweets\nneg_tweets_set = []\nfor tweet in neg_tweets:\n    neg_tweets_set.append((bag_of_words(tweet), 'neg'))\n\n# Radomize pos_reviews_set and neg_reviews_set\n# doing so will output different accuracy result everytime we run the program\nfrom random import shuffle \nshuffle(pos_tweets_set)\nshuffle(neg_tweets_set)\n \ntest_set = pos_tweets_set[:1000] + neg_tweets_set[:1000]\ntrain_set = pos_tweets_set[1000:] + neg_tweets_set[1000:]\n \nprint('Test tweets:',len(test_set),'Train tweets:',len(train_set)) # Check the number of tweets in test and train\n\n#Training Classifier and Calculating Accuracy\nfrom nltk import classify\nfrom nltk import NaiveBayesClassifier\n \nclassifier = NaiveBayesClassifier.train(train_set)\n \naccuracy = classify.accuracy(classifier, test_set)\n\nfrom collections import defaultdict\nfrom nltk.metrics import precision, recall, f_measure, ConfusionMatrix\n\nactual_set = defaultdict(set)\npredicted_set = defaultdict(set)\n \nactual_set_cm = []\npredicted_set_cm = []\n \nfor index, (feature, actual_label) in enumerate(test_set):\n    actual_set[actual_label].add(index)\n    actual_set_cm.append(actual_label)\n \n    predicted_label = classifier.classify(feature)\n \n    predicted_set[predicted_label].add(index)\n    predicted_set_cm.append(predicted_label)","8fe37069":"import pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\nSpidermanDF = pd.read_csv('..\/input\/movie-tweets\/Spiderman Tweets')\n\n#Create bag of words for each of the SpiderVerse tweets\nSpider_text_set = []\nfor text in SpidermanDF['Text']:\n    Spider_text_set.append(bag_of_words(text))\n\n#Classify each bag of words\nSpider_result = []\nfor bag in Spider_text_set:\n    Spider_result.append(classifier.classify(bag))\n\nSpidermanDF['Sentiment'] = Spider_result\n\nplotSeries = SpidermanDF['Sentiment'].value_counts()\n\nAquamanDF = pd.read_csv('..\/input\/movie-tweets\/Aquaman Tweets')\n\nAquaman_text_set = []\nfor text in AquamanDF['Text']:\n    Aquaman_text_set.append(bag_of_words(text))\n\n#Classify each bag of words\nAquaman_result = []\nfor bag in Aquaman_text_set:\n    Aquaman_result.append(classifier.classify(bag))\n\nAquamanDF['Sentiment'] = Aquaman_result #Create a column with the category given\nplotSeries1 = AquamanDF['Sentiment'].value_counts()\n\n\nfig = plt.figure(facecolor=\"white\",figsize=(10,8))\nbar_width = 0.4\nax = fig.add_subplot(1, 1, 1)\nr = [0,0.5] #Space between bars\ntick_pos = [i + (bar_width\/40) for i in r]\n\nax1 = ax.bar(r, plotSeries.values, width=bar_width, label='Spiderman', color='#B11313',edgecolor='white' )\nax2 = ax.bar(r, plotSeries1.values, bottom=plotSeries.values, width=bar_width, label='Aquaman', \n             color='#006994',edgecolor='white')\nax.set_ylabel(\"Count\", fontsize=14, style='italic')\nax.set_xlabel(\"Sentiment\", fontsize=14, style='italic')\nax.legend(loc='best')\nplt.xticks(tick_pos, [\"Positive\", \"Negative\"], fontsize=14)\nplt.yticks(fontsize=13)\n\nfor r1, r2 in zip(ax1, ax2): #Code to configure text inside each plot\n    h1 = r1.get_height()\n    h2 = r2.get_height()\n    plt.text(r1.get_x() + r1.get_width() \/ 2., h1 \/ 2., \"%d\" % h1, ha=\"center\", va=\"center\", \n             color=\"white\", fontsize=13, fontweight=\"bold\")\n    plt.text(r2.get_x() + r2.get_width() \/ 2., h1 + h2 \/ 2., \"%d\" % h2, ha=\"center\", va=\"center\", \n             color=\"white\", fontsize=13, fontweight=\"bold\")\nplt.legend(prop={'size': 13})\nplt.title('Sentiment Analysis on Tweets about\\n Spiderman and Aquaman Movies')\nplt.show()\n","b6533eb2":"print('pos precision:', precision(actual_set['pos'], predicted_set['pos']))\nprint('pos recall:', recall(actual_set['pos'], predicted_set['pos'])) \nprint('pos F-measure:', f_measure(actual_set['pos'], predicted_set['pos']))\nprint('neg precision:', precision(actual_set['neg'], predicted_set['neg']))\nprint('neg recall:', recall(actual_set['neg'], predicted_set['neg'])) \nprint('neg F-measure:', f_measure(actual_set['neg'], predicted_set['neg'])) ","2fcecfd8":"cm = ConfusionMatrix(actual_set_cm, predicted_set_cm)\nprint (cm.pretty_format(sort_by_count=True, show_percents=True, truncate=9))","aff54431":"## Box Office Results (as of Jan 8 2019):\nHow popular are these movies in terms of ticket sales? The total US box office grosses for each movie are given below:\n\n__*<font color=#006994>Aquaman(2018) :  USD 266,460,074<\/font>*__\n\n__*<font color=#B11313>Spiderman: Into The Spider-Verse(2018) :  USD 136,847,588<\/font>*__","f07338b1":"## Classifier Acurracy on Labeling Sentiment\nThe following matrix shows that the classifier used in this case may have incorrectly labeled a percentage of tweets as negative\/positive:","f9f556d7":"*Acknowledgements: Mukesh Chapagain in his blog found [here.](http:\/\/blog.chapagain.com.np\/python-nltk-twitter-sentiment-analysis-natural-language-processing-nlp\/)*\n\n*Spiderman doll image courtesy of amazon.com and Aquaman figure image courtesy of entertainmentearth.com*","2da5cf30":"## Classifier Precision Stats\npos = positive\n\nneg = negative","3360dfb8":"# Twitterverse Sentiment Analysis for Spiderman: Into The Spider-Verse(2018)  and Aquaman(2018) Movies\n<img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/103095\/244640\/Characters.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1547339029&Signature=B0n%2FnbZldwE0mCj6uc28ylbg7Vn5zQuP5JMmEvWjY6xmp93dEePKPLenXUyEKBrrCuq0rgkuPrL9Q6VztEnePLmcLXwrhrEdnUcC1ONAXlHQWtWjHH8E5bw4MzvZ5qfBTo2I893uhZW2TcEZCukTiqy1hXQ0GyPwDOrWpC9gvuGAZuVrGNyJKdx1m%2FBQKAg2Ois1tksAdQ3%2Fl8S97rzbq6f7u%2BYONS5bhtZCDGB8kIVqDd2OK%2BJguctgTgqW2IbB3IBwsDsDb1ZamYj7cXYZCftU9vGKWLHX97kc03YeK%2FyNRw1Frx8k%2F03aTzRoQLniknZM0Z1aR3X%2BgbLNXj3yAA%3D%3D\" width=\"320\" height=\"320\" align=\"center\"\/>\nGathering 1600 tweets with the Spiderman hashtag, and 1600 tweets using #Aquaman, the following graph was produced with the results from a trained Naive Bayes Classifier, a model that classified the sentiments of text as positive or negative. Note that no neutral category was given, so every tweet had to fall in either one of the two categories. Help in getting a thorough categorization will be appreciated."}}