{"cell_type":{"d75d63bc":"code","31098975":"code","cfd27e6d":"code","66701a36":"markdown"},"source":{"d75d63bc":"from random import seed\nfrom random import randrange\nfrom csv import reader\nfrom math import sqrt\nfrom math import exp","31098975":"def load_csv(filename):\n\tdataset = list()\n\twith open(filename, 'r') as file:\n\t\tcsv_reader = reader(file)\n\t\tfor row in csv_reader:\n\t\t\tif not row:\n\t\t\t\tcontinue\n\t\t\tdataset.append(row)\n\treturn dataset\n\ndef str_column_to_float(dataset, column):\n\tfor row in dataset:\n\t\trow[column] = float(row[column].strip())\n\ndef str_column_to_int(dataset, column):\n\tclass_values = [row[column] for row in dataset]\n\tunique = set(class_values)\n\tlookup = dict()\n\tfor i, value in enumerate(unique):\n\t\tlookup[value] = i\n\tfor row in dataset:\n\t\trow[column] = lookup[row[column]]\n\treturn lookup\n\ndef cross_validation_split(dataset, n_folds):\n\tdataset_split = list()\n\tdataset_copy = list(dataset)\n\tfold_size = int(len(dataset) \/ n_folds)\n\tfor i in range(n_folds):\n\t\tfold = list()\n\t\twhile len(fold) < fold_size:\n\t\t\tindex = randrange(len(dataset_copy))\n\t\t\tfold.append(dataset_copy.pop(index))\n\t\tdataset_split.append(fold)\n\treturn dataset_split\n\n# Calculate accuracy percentage\ndef accuracy_metric(actual, predicted):\n\tcorrect = 0\n\tfor i in range(len(actual)):\n\t\tif actual[i] == predicted[i]:\n\t\t\tcorrect += 1\n\treturn correct \/ float(len(actual)) * 100.0\n\ndef evaluate_algorithm(dataset, algorithm, n_folds, *args):\n\tfolds = cross_validation_split(dataset, n_folds)\n\tscores = list()\n\tfor fold in folds:\n\t\ttrain_set = list(folds)\n\t\ttrain_set.remove(fold)\n\t\ttrain_set = sum(train_set, [])\n\t\ttest_set = list()\n\t\tfor row in fold:\n\t\t\trow_copy = list(row)\n\t\t\ttest_set.append(row_copy)\n\t\t\trow_copy[-1] = None\n\t\tpredicted = algorithm(train_set, test_set, *args)\n\t\tactual = [row[-1] for row in fold]\n\t\taccuracy = accuracy_metric(actual, predicted)\n\t\tscores.append(accuracy)\n\treturn scores\n\ndef euclidean_distance(row1, row2):\n\tdistance = 0.0\n\tfor i in range(len(row1)-1):\n\t\tdistance += (row1[i] - row2[i])**2\n\treturn sqrt(distance)\n\ndef get_neighbors(train, test_row, num_neighbors):\n\tdistances = list()\n\tfor train_row in train:\n\t\tdist = euclidean_distance(test_row, train_row)\n\t\tdistances.append((train_row, dist))\n\tdistances.sort(key=lambda tup: tup[1])\n\tneighbors = list()\n\tfor i in range(num_neighbors):\n\t\tneighbors.append(distances[i][0])\n\treturn neighbors\n\ndef knn_predict(model, test_row, num_neighbors=2):\n\tneighbors = get_neighbors(model, test_row, num_neighbors)\n\toutput_values = [row[-1] for row in neighbors]\n\tprediction = max(set(output_values), key=output_values.count)\n\treturn prediction\n\ndef knn_model(train):\n\treturn train\n\ndef perceptron_predict(model, row):\n\tactivation = model[0]\n\tfor i in range(len(row)-1):\n\t\tactivation += model[i + 1] * row[i]\n\treturn 1.0 if activation >= 0.0 else 0.0\n\ndef perceptron_model(train, l_rate=0.01, n_epoch=5000):\n\tweights = [0.0 for i in range(len(train[0]))]\n\tfor epoch in range(n_epoch):\n\t\tfor row in train:\n\t\t\tprediction = perceptron_predict(weights, row)\n\t\t\terror = row[-1] - prediction\n\t\t\tweights[0] = weights[0] + l_rate * error\n\t\t\tfor i in range(len(row)-1):\n\t\t\t\tweights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n\treturn weights\n\ndef logistic_regression_predict(model, row):\n\tyhat = model[0]\n\tfor i in range(len(row)-1):\n\t\tyhat += model[i + 1] * row[i]\n\treturn 1.0 \/ (1.0 + exp(-yhat))\n\ndef logistic_regression_model(train, l_rate=0.01, n_epoch=5000):\n\tcoef = [0.0 for i in range(len(train[0]))]\n\tfor epoch in range(n_epoch):\n\t\tfor row in train:\n\t\t\tyhat = logistic_regression_predict(coef, row)\n\t\t\terror = row[-1] - yhat\n\t\t\tcoef[0] = coef[0] + l_rate * error * yhat * (1.0 - yhat)\n\t\t\tfor i in range(len(row)-1):\n\t\t\t\tcoef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]\n\treturn coef\n\ndef to_stacked_row(models, predict_list, row):\n\tstacked_row = list()\n\tfor i in range(len(models)):\n\t\tprediction = predict_list[i](models[i], row)\n\t\tstacked_row.append(prediction)\n\tstacked_row.append(row[-1])\n\treturn row[0:len(row)-1] + stacked_row\n\ndef stacking(train, test):\n\tmodel_list = [knn_model, perceptron_model]\n\tpredict_list = [knn_predict, perceptron_predict]\n\tmodels = list()\n\tfor i in range(len(model_list)):\n\t\tmodel = model_list[i](train)\n\t\tmodels.append(model)\n\tstacked_dataset = list()\n\tfor row in train:\n\t\tstacked_row = to_stacked_row(models, predict_list, row)\n\t\tstacked_dataset.append(stacked_row)\n\tstacked_model = logistic_regression_model(stacked_dataset)\n\tpredictions = list()\n\tfor row in test:\n\t\tstacked_row = to_stacked_row(models, predict_list, row)\n\t\tstacked_dataset.append(stacked_row)\n\t\tprediction = logistic_regression_predict(stacked_model, stacked_row)\n\t\tprediction = round(prediction)\n\t\tpredictions.append(prediction)\n\treturn predictions","cfd27e6d":"seed(1)\n# load and prepare data\nfilename = '..\/input\/sonaralldata\/sonar.all-data.csv'\ndataset = load_csv(filename)\n# convert string attributes to integers\nfor i in range(len(dataset[0])-1):\n\tstr_column_to_float(dataset, i)\n# convert class column to integers\nstr_column_to_int(dataset, len(dataset[0])-1)\nn_folds = 3\nscores = evaluate_algorithm(dataset, stacking, n_folds)\nprint('Scores: %s' % scores)\nprint('Mean Accuracy: %.3f%%' % (sum(scores)\/float(len(scores))))","66701a36":"# Sonar All Stacking"}}