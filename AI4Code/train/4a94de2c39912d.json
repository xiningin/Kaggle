{"cell_type":{"a5ebb1e3":"code","e6d18475":"code","5d0a5a17":"code","69f36106":"code","81507406":"code","c665cec8":"code","901755a2":"code","23ab71de":"code","b0a0d754":"code","3173e8d2":"code","036f2d50":"code","1297e0e9":"code","b9108f73":"code","6e0e76b1":"code","0ae75afa":"code","141e6fce":"code","3e39422c":"code","720fe408":"code","5ce2b99c":"code","acbb3e41":"code","5f5ac6fb":"code","7551ca7a":"code","f50c1442":"code","3c708730":"code","d9f40021":"code","862290cf":"code","e0f605fb":"code","2995832a":"code","afd3c3bb":"code","3a3d07c7":"code","402b9da9":"code","8f2ef98a":"code","1241b0a9":"code","272e4c78":"code","6f9ac5b2":"code","333a5de7":"code","e5d9ad54":"code","eb5f1444":"code","122efc59":"code","e4251383":"code","4cc2ccea":"code","9b8a3813":"code","7a84c990":"code","a23b772d":"code","ae3c5bef":"code","2c0f7e85":"code","3ba0a1f3":"code","690b9527":"code","f3a4d511":"code","62d576e3":"code","ad4ef876":"code","e2cd8b8a":"code","82594c9c":"code","eb5cfe80":"code","51ebfd11":"code","c1d72e0c":"code","7525b99e":"code","cd82fc25":"code","e501a249":"code","f5568485":"code","461d70e9":"code","af23f5f5":"code","36fd3829":"code","8601cdbf":"code","6c98054e":"code","e6d16031":"code","24b5376e":"code","a7ab880c":"code","81dea977":"code","770f7998":"code","67ec3792":"code","525f16dd":"code","bacdbd8f":"code","bc6657b4":"code","11aa0908":"code","e4420c23":"code","ad55c563":"code","fc4e3c5b":"code","dafb688e":"code","a330692d":"code","c557f7e0":"code","973df8f1":"code","3a59a822":"code","2f00a234":"code","175acbea":"code","3b7a67de":"code","2e06c594":"code","551179e3":"code","54d05a85":"code","ac47c2ed":"code","70344a69":"code","9d53683e":"code","878ed79b":"code","cb067b55":"code","5eb0cdbd":"code","507a7636":"code","d5b3d0d3":"code","ac9bb495":"code","e91f43f3":"code","c2c359c0":"code","f5925b8e":"code","a2ff575e":"code","4fbcaa88":"code","a189ad20":"code","009280d0":"code","7e2e83d2":"code","909c76a4":"code","d8af0700":"code","5530fd5f":"code","e6cbc271":"code","35148e5c":"code","48d73dc6":"code","2a0616a9":"code","73927358":"code","581f2fa0":"code","3e31cd74":"code","3c5be870":"code","9db56467":"code","88e0bab6":"code","44671188":"code","c5ad925c":"code","418e11c5":"code","dc670843":"code","e47943f4":"code","61190ebe":"code","aca345c7":"code","3eaf34bb":"code","91968cee":"code","1434a233":"code","135ff9d3":"code","55f364eb":"code","7a908d36":"code","409e415d":"code","e03f9081":"code","34c46e13":"code","c6af38de":"code","61c495b8":"code","13385774":"code","6f83887a":"code","8b4f991e":"code","c10981e5":"code","bab3e86d":"code","b353fa58":"code","e6940c49":"code","9552d52a":"code","bebc43c1":"code","6ecf5594":"code","fd6c928a":"code","fa907f67":"code","5dbfb08f":"code","01c178b1":"code","db1ab257":"code","0b617936":"code","1c6adde6":"code","2b5ec704":"code","b9927245":"code","e4566a8a":"code","1f500177":"markdown","46236b9b":"markdown","a41aa14b":"markdown","1a4361ca":"markdown","82e4e7c6":"markdown","c9ba6d67":"markdown","f2313f6d":"markdown","f4fe0a67":"markdown","e2de1803":"markdown","75f6b030":"markdown","1de10f49":"markdown","a2218d73":"markdown","1bd48aff":"markdown","e398ed87":"markdown","2c07e18d":"markdown","5684cad8":"markdown","150392fe":"markdown","6d07d908":"markdown","94380ffa":"markdown","9670327a":"markdown","efc79d6d":"markdown","57f511b6":"markdown","68109236":"markdown","b3c1305f":"markdown","9f243e71":"markdown","5955061d":"markdown","6e07524d":"markdown","7627d2ae":"markdown","93d25d7f":"markdown","5ab342d2":"markdown","ef14124b":"markdown","8ab41294":"markdown","445944a3":"markdown","cabc4aac":"markdown","abdf0e9a":"markdown","0966f1f8":"markdown","6e7cc5c6":"markdown","c5d932a3":"markdown","9aa2559a":"markdown","e218d073":"markdown","1b5422de":"markdown","46c617d8":"markdown","203ccc32":"markdown","2afbc991":"markdown","73466007":"markdown","277164fb":"markdown","63bf2005":"markdown","2a4a76e8":"markdown","412cccf5":"markdown","b94d3541":"markdown","f0879bce":"markdown","a9450ecb":"markdown","8a57e70a":"markdown","f43c657d":"markdown","e29dd0cf":"markdown","8d3fe2ac":"markdown","5bd2a1ba":"markdown","3ea59009":"markdown","ef8cbd12":"markdown","cdda2953":"markdown","47a7765d":"markdown","331a5234":"markdown","982f2e93":"markdown","faef7e6f":"markdown","7a93d460":"markdown","0885c6f7":"markdown","90425d78":"markdown","032f985a":"markdown","76b02467":"markdown","0bad9209":"markdown","f6b2f3f3":"markdown","ffd1d3eb":"markdown","ab314309":"markdown","9cb90661":"markdown","2fb0ac36":"markdown","82b0c3b1":"markdown","3d39b165":"markdown","d580d706":"markdown","8f819cf9":"markdown","c9f9f402":"markdown","0b4cbf8c":"markdown","5bef4d18":"markdown","1037f1d9":"markdown","cd2225d3":"markdown","18a68e9d":"markdown","4c44943d":"markdown","f6c462ce":"markdown","c696177b":"markdown","2483516c":"markdown","bfc9aeb8":"markdown","d9edcab1":"markdown","82d9864d":"markdown","73f75192":"markdown","b28ad01f":"markdown","fffbd18c":"markdown","6be543b2":"markdown","cac8f002":"markdown"},"source":{"a5ebb1e3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, make_scorer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e6d18475":"# Data manipulation\nimport pandas as pd\nimport numpy as np\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import OrderedDict\n# Set a few plotting defaults\n%matplotlib inline\nplt.style.use('fivethirtyeight')\nplt.rcParams['font.size'] = 18\nplt.rcParams['patch.edgecolor'] = 'k'\nfrom scipy.stats import spearmanr","5d0a5a17":"pd.options.display.max_columns = 150\n\n# Read in data\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\ntrain.head()","69f36106":"train.info()","81507406":"test.info()\n","c665cec8":"train.select_dtypes(np.int64).nunique().value_counts().sort_index().plot.bar(color = 'blue', \n                                                                             figsize = (8, 6),\n                                                                            edgecolor = 'k', linewidth = 2);\nplt.xlabel('Number of Unique Values'); plt.ylabel('Count');\nplt.title('Count of Unique Values in Integer Columns');","901755a2":"plt.figure(figsize = (20, 16))\nplt.style.use('fivethirtyeight')\n# Color mapping\ncolors = OrderedDict({1: 'red', 2: 'orange', 3: 'blue', 4: 'green'})\npoverty_mapping = OrderedDict({1: 'extreme', 2: 'moderate', 3: 'vulnerable', 4: 'non vulnerable'})\nfor i, col in enumerate(train.select_dtypes('float')):\n    ax=plt.subplot(4,2,i+1)\n    for poverty_levels,color in colors.items():\n        \n        sns.kdeplot(train.loc[train['Target']==poverty_levels,col].dropna(),\n                   ax=ax,color=color,label=poverty_mapping[poverty_levels])\n        plt.title(f'{col.capitalize()} Distribution'); plt.xlabel(f'{col}'); plt.ylabel('Density')\nplt.subplots_adjust(top=2)\n    ","23ab71de":"train.select_dtypes('object').head()","b0a0d754":"maps={'yes':1,'no':0}\nfor df in [train,test]:\n    df['dependency'] = df['dependency'].replace(maps).astype(np.float64)\n    df['edjefa'] = df['edjefa'].replace(maps).astype(np.float64)\n    df['edjefe'] = df['edjefe'].replace(maps).astype(np.float64)","3173e8d2":"train.select_dtypes('object').head()","036f2d50":"train[['dependency', 'edjefa', 'edjefe']].describe()\n","1297e0e9":"plt.figure(figsize = (16, 12))\nfor i,col in enumerate(['dependency','edjefa','edjefe']):\n    ax=plt.subplot(3,1,i+1)\n    color_map={1:'Red',2:'orange',3:'blue',4:'green'}\n    poverty_map={1:'extreme',2:'moderate',3:'vulnerable',4:'non vulnerable'}\n    for level,color in color_map.items():\n        sns.kdeplot(train.loc[train['Target']==level,col].dropna(),\n                   ax=ax,color=color,label=poverty_map[level])\n        plt.title(f'{col.capitalize()} Distribution'); plt.xlabel(f'{col}'); plt.ylabel('Density')\nplt.subplots_adjust(top=2)\n        ","b9108f73":"test['Target'] = np.nan\ndata = train.append(test, ignore_index = True)","6e0e76b1":"#heads of household\nheads=data.loc[data['parentesco1']==1]\n\n#labels for training\ntrain_labels=data.loc[(data['Target'].notnull()) & (data['parentesco1']==1),['Target','idhogar']]\n\n#value_counts of target\nlabel_counts=train_labels['Target'].value_counts().sort_index()\n# bar plot\n\nlabel_counts.plot.bar(figsize=(8,6),color=colors.values(),edgecolor='k',linewidth=2)\n\n#formatting\nplt.xlabel(\"levels of poverty\")\nplt.ylabel(\"count\")\nplt.xticks([x-1 for x in poverty_map.keys()],\n          list(poverty_mapping.values()),rotation=60)\nplt.title(\"Poverty level\")\nprint(label_counts)","0ae75afa":"# Groupby the household and figure out the number of unique values\nall_equal = train.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\n\n# Households where targets are not all equal\nnot_equal = all_equal[all_equal != True]\nprint('There are {} households where the family members do not all have the same target.'.format(len(not_equal)))","141e6fce":"households_leader = train.groupby('idhogar')['parentesco1'].sum()\n\n# Find households without a head\nhouseholds_no_head = train.loc[train['idhogar'].isin(households_leader[households_leader == 0].index), :]\n\nprint('There are {} households without a head.'.format(households_no_head['idhogar'].nunique()))","3e39422c":"households_no_head_equal = households_no_head.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\nprint('{} Households with no head have different labels.'.format(sum(households_no_head_equal == False)))","720fe408":"#not_equal--all households with incorrect target than their heads\nfor household in not_equal.index:\n    true_target=int(train[(train['idhogar']==household) & (train['parentesco1']==1.0)][\"Target\"])\n    train.loc[train[\"idhogar\"]==household,\"Target\"]=true_target\n#now check again\nall_equal=train.groupby(\"idhogar\")[\"Target\"].apply(lambda x:x.nunique()==1)\nnot_equal=all_equal[all_equal !=True]\nprint('There are {} households where the family members do not all have the same target.'.format(len(not_equal)))","5ce2b99c":"missing=pd.DataFrame(data.isnull().sum())\nmissing=missing.rename(columns={0:'total'})\nmissing[\"percentage\"]=missing['total']\/len(data)\nmissing.sort_values('percentage',ascending=False).head(10).drop(\"Target\")","acbb3e41":"def plot_value_counts(df,col,heads_only=False):\n    if heads_only:\n        df=df.loc[df[\"parentesco1\"]==1].copy()\n    plt.figure(figsize=(10,7))\n    df[col].value_counts().sort_index().plot.bar(color=\"blue\",edgecolor='k',linewidth=2)\n    plt.xlabel(f'{col}')\n    plt.title(f'{col} value counts')\n    plt.ylabel('Count')\n    plt.show()","5f5ac6fb":"#print(heads)\nplot_value_counts(heads, 'v18q1')\n","7551ca7a":"heads.groupby('v18q')['v18q1'].apply(lambda x:x.isnull().sum())","f50c1442":"data['v18q1'] = data['v18q1'].fillna(0)\n","3c708730":"ownership_cols=[x for x in data if x.startswith('tipo')]\n\ndata.loc[data['v2a1'].isnull(),ownership_cols].sum().plot.bar(figsize=(10,8),color='blue',edgecolor='k',linewidth=2)\nplt.xticks([0,1,2,3,4],['Owns and Paid Off', 'Owns and Paying', 'Rented', 'Precarious', 'Other'],rotation=60)\nplt.title(\"Home ownership\")","d9f40021":"data.loc[data['tipovivi1']==1,'v2a1']=0\n\ndata[\"v2a1-missing\"]=data[\"v2a1\"].isnull()\ndata[\"v2a1-missing\"].value_counts()","862290cf":"data.loc[data['rez_esc'].notnull()]['age'].describe()","e0f605fb":"data.loc[data['rez_esc'].isnull()]['age'].describe()\n","2995832a":"data.loc[((data['age'] > 19) | (data['age'] < 7)) & (data['rez_esc'].isnull()), 'rez_esc'] = 0\n\n# Add a flag for those between 7 and 19 with a missing value\ndata['rez_esc-missing'] = data['rez_esc'].isnull()","afd3c3bb":"data.loc[data['rez_esc'] > 5, 'rez_esc'] = 5\n","3a3d07c7":"plot_value_counts(data[data['rez_esc-missing']==1],'Target')","402b9da9":"plot_value_counts(data[(data['v2a1-missing'] == 1)], \n                  'Target')","8f2ef98a":"import featuretools as ft\n\n# creating and entity set 'es'\nes = ft.EntitySet(id = 'Target')\n\n# adding a dataframe \nes.entity_from_dataframe(entity_id = 'costa rica', dataframe = data, index = 'Id')","1241b0a9":"es.normalize_entity(base_entity_id='costa rica', new_entity_id='households', index = 'idhogar',\n                    additional_variables = ['v2a1', 'hhsize'])\n","272e4c78":"print(es)","6f9ac5b2":"'''feature_matrix, feature_names = ft.dfs(entityset=es, \ntarget_entity = 'costa rica', \nmax_depth = 2, \nverbose = 1, \nn_jobs = 3)'''","333a5de7":"#feature_matrix.columns\n","e5d9ad54":"#feature_matrix.head()\n","eb5f1444":"id_ = ['Id', 'idhogar', 'Target']\n","122efc59":"ind_bool = ['v18q', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', \n            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', \n            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', \n            'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', \n            'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', \n            'instlevel9', 'mobilephone', 'rez_esc-missing']\nind_ordered = ['rez_esc', 'escolari', 'age']\n\n","e4251383":"hh_bool = ['hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', \n           'paredpreb','pisocemento', 'pareddes', 'paredmad',\n           'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', \n           'pisonatur', 'pisonotiene', 'pisomadera',\n           'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', \n           'abastaguadentro', 'abastaguafuera', 'abastaguano',\n            'public', 'planpri', 'noelec', 'coopele', 'sanitario1', \n           'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6',\n           'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4', \n           'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', \n           'elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3',\n           'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', \n           'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', \n           'computer', 'television', 'lugar1', 'lugar2', 'lugar3',\n           'lugar4', 'lugar5', 'lugar6', 'area1', 'area2', 'v2a1-missing']\n\nhh_ordered = [ 'rooms', 'r4h1', 'r4h2', 'r4h3', 'r4m1','r4m2','r4m3', 'r4t1',  'r4t2', \n              'r4t3', 'v18q1', 'tamhog','tamviv','hhsize','hogar_nin',\n              'hogar_adul','hogar_mayor','hogar_total',  'bedrooms', 'qmobilephone']\nhh_cont = ['v2a1', 'dependency', 'edjefe', 'edjefa', 'meaneduc', 'overcrowding']","4cc2ccea":"sqr_ = ['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', \n        'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq']","9b8a3813":"x = ind_bool + ind_ordered + id_ + hh_bool + hh_ordered + hh_cont + sqr_\n#print(x)\nfrom collections import Counter\n\n\nprint('There are no repeats: ', np.all(np.array(list(Counter(x).values())) == 1))\nprint('We covered every variable: ', len(x) == data.shape[1])","7a84c990":"sns.lmplot('age','SQBage',data=data,fit_reg=False)\nplt.title(\"Squared Age V\/S Age\")","a23b772d":"data = data.drop(columns = sqr_)\ndata.shape","ae3c5bef":"corr_matrix = heads.corr().abs()\nprint(corr_matrix)","2c0f7e85":"upper=corr_matrix.where(np.triu(np.ones(corr_matrix.shape),k=1).astype(np.bool))\nto_drop=[column for column in upper.columns if any(upper[column]>0.95)]\nto_drop","3ba0a1f3":"corr_matrix.loc[corr_matrix['tamhog'].abs() > 0.9, corr_matrix['tamhog'].abs() > 0.9]\n","690b9527":"sns.heatmap(corr_matrix.loc[corr_matrix['hhsize'].abs() > 0.9, corr_matrix['tamhog'].abs() > 0.9],\n            annot=True, cmap = plt.cm.autumn_r, fmt='.3f');","f3a4d511":"heads = heads.drop(columns = ['tamhog', 'hogar_total', 'r4t3'])","62d576e3":"sns.lmplot('tamviv','hhsize',data,fit_reg=False,size=8);\nplt.title('Household size vs num of persons in a household')","ad4ef876":"heads['hhsize-diff']=heads['tamviv']-heads['hhsize']","e2cd8b8a":"elec = []\n\n# Assign values\nfor i, row in heads.iterrows():\n    if row['noelec'] == 1:\n        elec.append(0)\n    elif row['coopele'] == 1:\n        elec.append(1)\n    elif row['public'] == 1:\n        elec.append(2)\n    elif row['planpri'] == 1:\n        elec.append(3)\n    else:\n        elec.append(np.nan)\n        \n# Record the new variable and missing flag\nheads['elec'] = elec\nheads['elec-missing'] = heads['elec'].isnull()","82594c9c":"heads = heads.drop(columns = ['noelec', 'coopele', 'public', 'planpri'])","eb5cfe80":"heads=heads.drop(columns=['area2'])\nheads.groupby('area1')['Target'].value_counts(normalize=True)","51ebfd11":"heads['walls']=np.argmax(np.array(heads[['epared1', 'epared2', 'epared3']]),axis=1)","c1d72e0c":"heads = heads.drop(columns = ['epared1', 'epared2', 'epared3'])","7525b99e":"# Roof ordinal variable\nheads['roof'] = np.argmax(np.array(heads[['etecho1', 'etecho2', 'etecho3']]),\n                           axis = 1)\nheads = heads.drop(columns = ['etecho1', 'etecho2', 'etecho3'])\n\n# Floor ordinal variable\nheads['floor'] = np.argmax(np.array(heads[['eviv1', 'eviv2', 'eviv3']]),\n                           axis = 1)\nheads = heads.drop(columns = ['eviv1', 'eviv2', 'eviv3'])","cd82fc25":"heads['walls+roof+floor'] = heads['walls'] + heads['roof'] + heads['floor']\n\n","e501a249":"series=heads.groupby(\"Target\")['walls+roof+floor'].apply(lambda x:np.sum(x))\nprint(series)\nseries.plot.bar(color = 'purple',figsize = (8, 6),edgecolor = 'k', linewidth = 2)\nplt.xlabel(\"Target\")\nplt.ylabel(\"Total walls+roof+floor\")\nplt.title(\"Target vs cost\")\n#plt.bar([1.0,2.0,3.0,4.0],[series[i] for i in range(1,5)],color = 'blue',figsize = (8, 6),edgecolor = 'k', linewidth = 2)","f5568485":"heads['warning'] = -1 * (heads['sanitario1'] + \n                         (heads['elec'] == 0) + \n                         heads['pisonotiene'] + \n                         heads['abastaguano'] + \n                         (heads['cielorazo'] == 0))","461d70e9":"plt.figure(figsize = (10, 6))\nsns.violinplot(x = 'warning', y = 'Target', data = heads);\nplt.title('Target vs Warning Variable');","af23f5f5":"heads['bonus'] = 1 * (heads['refrig'] + \n                      heads['computer'] + \n                      (heads['v18q1'] > 0) + \n                      heads['television'])\n\nsns.violinplot('bonus', 'Target', data = heads,\n                figsize = (10, 6));\nplt.title('Target vs Bonus Variable');","36fd3829":"heads['phones-per-capita'] = heads['qmobilephone'] \/ heads['tamviv']\nheads['tablets-per-capita'] = heads['v18q1'] \/ heads['tamviv']\nheads['rooms-per-capita'] = heads['rooms'] \/ heads['tamviv']\nheads['rent-per-capita'] = heads['v2a1'] \/ heads['tamviv']","8601cdbf":"def plot_corrs(x,y):\n    spr=spearmanr(x,y).correlation\n    pcr=np.corrcoef(x,y)[0,1]\n    \n    #scatterplot\n    data2=pd.DataFrame({'x':x,'y':y})\n    plt.figure(figsize=(6,4))\n    sns.regplot('x','y',data=data2,fit_reg=False)\n    plt.title(f'Spearman:{round(spr,2)}; Pearson {round(pcr,2)}')","6c98054e":"x = np.array(range(100))\ny = x ** 2\n\nplot_corrs(x, y)","e6d16031":"#Using only training data\n\ntrain_heads=heads.loc[heads.Target.notnull(),:].copy()\n\npcorrs = pd.DataFrame(train_heads.corr()['Target'].sort_values()).rename(columns = {'Target': 'pcorr'}).reset_index()\npcorrs = pcorrs.rename(columns = {'index': 'feature'})\n\nprint('Most negatively correlated variables:')\nprint(pcorrs.head())\n\nprint('\\nMost positively correlated variables:')\nprint(pcorrs.dropna().tail())","24b5376e":"'''import warnings\nwarnings.filterwarnings('ignore', category = RuntimeWarning)\n\nfeats = []\nscorr = []\npvalues = []\n\n# Iterate through each column\nfor c in heads:\n    #print(c)\n    # Only valid for numbers\n    if heads[c].dtype != 'object':\n        feats.append(c)\n        print(scorr)\n        # Calculate spearman correlation\n        scorr.append(spearmanr(train_heads[c], train_heads['Target']).correlation)\n        pvalues.append(spearmanr(train_heads[c], train_heads['Target']).pvalue)\n\nscorrs = pd.DataFrame({'feature': feats, 'scorr': scorr, 'pvalue': pvalues}).sort_values('scorr')\n\n\nprint('Most negative Spearman correlations:')\nprint(scorrs.head())\nprint('\\nMost positive Spearman correlations:')\nprint(scorrs.dropna().tail())\n\nFor the most part, the two methods of calculating correlations are in agreement. Just out of curiousity, we can look for the values that are furthest apart.\n\ncorrs = pcorrs.merge(scorrs, on = 'feature')\ncorrs['diff'] = corrs['pcorr'] - corrs['scorr']\n\ncorrs.sort_values('diff').head()\n\ncorrs.sort_values('diff').dropna().tail()\n\n\n\n'''","a7ab880c":"sns.lmplot('dependency', 'Target', fit_reg = True, data = train_heads, x_jitter=0.05, y_jitter=0.05);\nplt.title('Target vs Dependency');","81dea977":"sns.lmplot('rooms-per-capita', 'Target', fit_reg = True, data = train_heads, x_jitter=0.05, y_jitter=0.05);\nplt.title('Target vs Rooms Per Capita');","770f7998":"variables = ['Target', 'dependency', 'warning', 'walls+roof+floor', 'meaneduc',\n             'floor', 'r4m1', 'overcrowding']\n\n#Calculating correlations\ncorr_mat=train_heads[variables].corr().round(2)\n\n# Draw a correlation heatmap\nplt.rcParams['font.size'] = 18\nplt.figure(figsize = (12, 12))\n\nsns.heatmap(corr_mat, vmin = -0.5, vmax = 0.8, center = 0, \n            cmap = plt.cm.RdYlGn_r, annot = True);\n","67ec3792":"import warnings\nwarnings.filterwarnings('ignore')\n\n# Copy the data for plotting\nplot_data = train_heads[['Target', 'dependency', 'walls+roof+floor',\n                         'meaneduc', 'overcrowding']]\n\n# Create the pairgrid object\ngrid = sns.PairGrid(data = plot_data, size = 4, diag_sharey=False,\n                    hue = 'Target', hue_order = [4, 3, 2, 1], \n                    vars = [x for x in list(plot_data.columns) if x != 'Target'])\n\n# Upper is a scatter plot\ngrid.map_upper(plt.scatter, alpha = 0.8, s = 20)\n\n# Diagonal is a histogram\ngrid.map_diag(sns.kdeplot)\n\n# Bottom is density plot\ngrid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r);\ngrid = grid.add_legend()\nplt.suptitle('Feature Plots Colored By Target', size = 32, y = 1.05);\n","525f16dd":"household_feats = list(heads.columns)\n","bacdbd8f":"ind = data[id_ + ind_bool + ind_ordered]\nind.shape","bc6657b4":"corr_matrix=ind.corr()\n\nupper=corr_matrix.where(np.triu(np.ones(corr_matrix.shape).astype(np.bool)))\nto_drop=[column for column in upper.columns if any(abs(upper[column])>0.95)]\nto_drop","11aa0908":"ind = ind.drop(columns = 'male')#for male is just not female\n","e4420c23":"ind[[c for c in ind if c.startswith('instl')]].head()\n","ad55c563":"ind['inst'] = np.argmax(np.array(ind[[c for c in ind if c.startswith('instl')]]), axis = 1)\nls=[1,2,3,4]\ncolor_maps={1:'r',2:'y',3:'b',4:'g'}\nplt.figure(figsize = (10,10))\ns=25\nfor i in ls:\n    #print(ind.loc[ind['Target']==i]['inst'].value_counts())\n    sns.scatterplot(data=ind.loc[ind['Target']==i]['inst'].value_counts(),color=color_maps[i],label=str(i),s=s*(i+4))\nplt.xlabel(\"Years of education\")\nplt.ylabel(\"Count of people\")\nplt.title(\"years of education V\/S count of people\")","fc4e3c5b":"plt.figure(figsize = (10, 8))\nsns.violinplot(x = 'Target', y = 'inst', data = ind);\nplt.title('Education Distribution by Target');","dafb688e":"ind['escolari\/age'] = ind['escolari'] \/ ind['age']\n\nplt.figure(figsize = (10, 8))\nsns.violinplot('Target', 'escolari\/age', data = ind);","a330692d":"ind['inst\/age'] = ind['inst'] \/ ind['age']\nind['tech'] = ind['v18q'] + ind['mobilephone']\nind['tech'].describe()","c557f7e0":"# Define custom function\nrange_ = lambda x: x.max() - x.min()\nrange_.__name__ = 'range_'\n\n# Group and aggregate\nind_agg = ind.drop(columns = 'Target').groupby('idhogar').agg(['min', 'max', 'sum', 'count', 'std', range_])\nind_agg.head().describe()","973df8f1":"new_cols=[]\nfor c in ind_agg.columns.levels[0]:\n    for stat in ind_agg.columns.levels[1]:\n        new_cols.append(f'{c}-{stat}')\nind_agg.columns=new_cols\nind_agg.head().describe()","3a59a822":"ind_agg.iloc[:, [0, 1, 2, 3, 6, 7, 8, 9]].head()\n","2f00a234":"# Create correlation matrix\ncorr_matrix = ind_agg.corr()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n\nprint(f'There are {len(to_drop)} correlated columns to remove.')","175acbea":"to_drop\n","3b7a67de":"ind_agg = ind_agg.drop(columns = to_drop)\nind_feats = list(ind_agg.columns)\n\n\n# Merge on the household id\nfinal = heads.merge(ind_agg, on = 'idhogar', how = 'left')\n\nprint('Final features shape: ', final.shape)","2e06c594":"final=final.drop([\"v2a1\",\"rooms\",'v18q1','rez_esc','male',],axis=1)","551179e3":"final.head()","54d05a85":"corrs = final.corr()['Target']\ncorrs.sort_values().head()","ac47c2ed":"corrs.sort_values().dropna().tail()","70344a69":"plt.figure(figsize = (10, 6))\nsns.violinplot(x = 'Target', y = 'escolari-max', data = final);\nplt.title('Max Schooling by Target');","9d53683e":"plt.figure(figsize = (10, 6))\nsns.boxplot(x = 'Target', y = 'escolari-max', data = final);\nplt.title('Max Schooling by Target');","878ed79b":"plt.figure(figsize = (10, 6))\nsns.boxplot(x = 'Target', y = 'meaneduc', data = final);\nplt.xticks([0, 1, 2, 3], poverty_mapping.values())\nplt.title('Average Schooling by Target');","cb067b55":"plt.figure(figsize = (10, 6))\nsns.boxplot(x = 'Target', y = 'overcrowding', data = final);\nplt.xticks([0, 1, 2, 3], poverty_mapping.values())\nplt.title('Overcrowding by Target');","5eb0cdbd":"head_gender = ind.loc[ind['parentesco1'] == 1, ['idhogar', 'female']]\nfinal = final.merge(head_gender, on = 'idhogar', how = 'left').rename(columns = {'female': 'female-head'})","507a7636":"head_gender = ind.loc[ind['parentesco1'] == 1, ['idhogar', 'female']]\nfinal = final.merge(head_gender, on = 'idhogar', how = 'left').rename(columns = {'female': 'female-head'})","d5b3d0d3":"sns.violinplot(x = 'female-head', y = 'Target', data = final);\nplt.title('Target by Female Head of Household');","ac9bb495":"scorer=make_scorer(f1_score,greater_is_better=True,average='macro')","e91f43f3":"train_labels = np.array(list(final[final['Target'].notnull()]['Target'].astype(np.uint8)))\n\n# Extract the training data\ntrain_set = final[final['Target'].notnull()].drop(columns = ['Id', 'idhogar', 'Target'])\ntest_set = final[final['Target'].isnull()].drop(columns = ['Id', 'idhogar', 'Target'])\nprint(test_set.shape)\n\n\nsubmission_base = test[['Id', 'idhogar']].copy()\n","c2c359c0":"\n\npipeline=Pipeline([('imputer',Imputer(strategy='median')),\n                  ('scaler',MinMaxScaler())])\nfeatures=list(train_set.columns)\ntrain_set=pipeline.fit_transform(train_set)\ntest_set=pipeline.transform(test_set)","f5925b8e":"model=RandomForestClassifier(n_estimators=100,random_state=10,n_jobs=-1)\ncv_score=cross_val_score(model,train_set,train_labels,cv=10,scoring=scorer)\nprint(f'10 Fold Cross Validation F1 Score = {round(cv_score.mean(), 4)} with std = {round(cv_score.std(), 4)}')","a2ff575e":"model.fit(train_set,train_labels)\n\n#Feature importances into a dataframe\n\nfeature_importances=pd.DataFrame({'feature':features,'importance':model.feature_importances_})\nfeature_importances.head()","4fbcaa88":"\ndef plot_feature_importances(df, n = 10, threshold = None):\n    \"\"\"Plots n most important features. Also plots the cumulative importance if\n    threshold is specified and prints the number of features needed to reach threshold cumulative importance.\n    Intended for use with any tree-based feature importances. \n    \n    Args:\n        df (dataframe): Dataframe of feature importances. Columns must be \"feature\" and \"importance\".\n    \n        n (int): Number of most important features to plot. Default is 15.\n    \n        threshold (float): Threshold for cumulative importance plot. If not provided, no plot is made. Default is None.\n        \n    Returns:\n        df (dataframe): Dataframe ordered by feature importances with a normalized column (sums to 1) \n                        and a cumulative importance column\n    \n    Note:\n    \n        * Normalization in this case means sums to 1. \n        * Cumulative importance is calculated by summing features from most to least important\n        * A threshold of 0.9 will show the most important features needed to reach 90% of cumulative importance\n    \n    \"\"\"\n    plt.style.use('fivethirtyeight')\n    \n    # Sort features with most important at the head\n    df = df.sort_values('importance', ascending = False).reset_index(drop = True)\n    \n    # Normalize the feature importances to add up to one and calculate cumulative importance\n    df['importance_normalized'] = df['importance'] \/ df['importance'].sum()\n    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n    \n    plt.rcParams['font.size'] = 12\n    \n    # Bar plot of n most important features\n    df.loc[:n, :].plot.barh(y = 'importance_normalized', \n                            x = 'feature', color = 'darkgreen', \n                            edgecolor = 'k', figsize = (12, 8),\n                            legend = False, linewidth = 2)\n\n    plt.xlabel('Normalized Importance', size = 18); plt.ylabel(''); \n    plt.title(f'{n} Most Important Features', size = 18)\n    plt.gca().invert_yaxis()\n    \n    \n    if threshold:\n        # Cumulative importance plot\n        plt.figure(figsize = (8, 6))\n        plt.plot(list(range(len(df))), df['cumulative_importance'], 'b-')\n        plt.xlabel('Number of Features', size = 16); plt.ylabel('Cumulative Importance', size = 16); \n        plt.title('Cumulative Feature Importance', size = 18);\n        \n        # Number of features needed for threshold cumulative importance\n        # This is the index (will need to add 1 for the actual number)\n        importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n        \n        # Add vertical line to plot\n        plt.vlines(importance_index + 1, ymin = 0, ymax = 1.05, linestyles = '--', colors = 'red')\n        plt.show();\n        \n        print('{} features required for {:.0f}% of cumulative importance.'.format(importance_index + 1, \n                                                                                  100 * threshold))\n    \n    return df","a189ad20":"norm_fi = plot_feature_importances(feature_importances, threshold=0.95)\n","009280d0":"def kde_target(df, variable):\n    \"\"\"Plots the distribution of `variable` in `df` colored by the `Target` column\"\"\"\n    \n    colors = {1: 'red', 2: 'orange', 3: 'blue', 4: 'green'}\n\n    plt.figure(figsize = (12, 8))\n    \n    df = df[df['Target'].notnull()]\n    \n    for level in df['Target'].unique():\n        subset = df[df['Target'] == level].copy()\n        sns.kdeplot(subset[variable].dropna(), \n                    label = f'Poverty Level: {level}', \n                    color = colors[int(subset['Target'].unique())])\n\n    plt.xlabel(variable); plt.ylabel('Density');\n    plt.title('{} Distribution'.format(variable.capitalize()));","7e2e83d2":"kde_target(final, 'meaneduc')","909c76a4":"kde_target(final, 'escolari\/age-range_')","d8af0700":"from sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import LogisticRegressionCV, RidgeClassifierCV\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier","5530fd5f":"import warnings \nfrom sklearn.exceptions import ConvergenceWarning\n\n# Filter out warnings from models\nwarnings.filterwarnings('ignore', category = ConvergenceWarning)\nwarnings.filterwarnings('ignore', category = DeprecationWarning)\nwarnings.filterwarnings('ignore', category = UserWarning)\n\n# Dataframe to hold results\nmodel_results = pd.DataFrame(columns = ['model', 'cv_mean', 'cv_std'])\n\ndef cv_model(train, train_labels, model, name, model_results=None):\n    \"\"\"Perform 10 fold cross validation of a model\"\"\"\n    \n    cv_scores = cross_val_score(model, train, train_labels, cv = 10, scoring=scorer, n_jobs = -1)\n    print(f'10 Fold CV Score: {round(cv_scores.mean(), 5)} with std: {round(cv_scores.std(), 5)}')\n    \n    if model_results is not None:\n        model_results = model_results.append(pd.DataFrame({'model': name, \n                                                           'cv_mean': cv_scores.mean(), \n                                                            'cv_std': cv_scores.std()},\n                                                           index = [0]),\n                                             ignore_index = True)\n\n        return model_results","e6cbc271":"model_results = cv_model(train_set, train_labels, LinearSVC(), \n                         'LSVC', model_results)","35148e5c":"model_results = cv_model(train_set, train_labels, \n                         GaussianNB(), 'GNB', model_results)","48d73dc6":"model_results = cv_model(train_set, train_labels, \n                         MLPClassifier(hidden_layer_sizes=(32, 64, 128, 64, 32)),\n                         'MLP', model_results)","2a0616a9":"model_results = cv_model(train_set, train_labels, \n                          LinearDiscriminantAnalysis(), \n                          'LDA', model_results)","73927358":"model_results = cv_model(train_set, train_labels, \n                         RidgeClassifierCV(), 'RIDGE', model_results)","581f2fa0":"for n in [5, 10, 20]:\n    print(f'\\nKNN with {n} neighbors\\n')\n    model_results = cv_model(train_set, train_labels, \n                             KNeighborsClassifier(n_neighbors = n),\n                             f'knn-{n}', model_results)","3e31cd74":"from sklearn.ensemble import ExtraTreesClassifier\n\nmodel_results = cv_model(train_set, train_labels, \n                         ExtraTreesClassifier(n_estimators = 100, random_state = 10),\n                         'EXT', model_results)","3c5be870":"model_results","9db56467":"model_results = cv_model(train_set, train_labels,\n                          RandomForestClassifier(100, random_state=10),\n                              'RF', model_results)","88e0bab6":"model_results.index","44671188":"model_results.set_index('model',inplace=True)\nmodel_results['cv_mean'].plot.bar(color='purple',figsize=(8,6),\n                                 yerr=list(model_results['cv_std']),\n                                 edgecolor='k',linewidth=2)\nplt.xticks([0,1,2,3,4,5,6,7,8,9,10],[x for x in model_results.index ],rotation=60)\nplt.title(\"F1 scores models\")\nplt.ylabel('Mean F1 Score (with error bar)');\nmodel_results.reset_index(inplace = True)\n","c5ad925c":"test_ids = list(final.loc[final['Target'].isnull(), 'idhogar'])\n","418e11c5":"def submit(model, train, train_labels, test, test_ids):\n    \"\"\"Train and test a model on the dataset\"\"\"\n    \n    # Train on the data\n    model.fit(train, train_labels)\n    predictions = model.predict(test)\n    predictions = pd.DataFrame({'idhogar': test_ids,\n                               'Target': predictions})\n\n     # Make a submission dataframe\n    submission = submission_base.merge(predictions, \n                                       on = 'idhogar',\n                                       how = 'left').drop(columns = ['idhogar'])\n    \n    # Fill in households missing a head\n    submission['Target'] = submission['Target'].fillna(4).astype(np.int8)\n\n    return submission","dc670843":"rf_submission = submit(RandomForestClassifier(n_estimators = 100, \n                                              random_state=10, n_jobs = -1), \n                         train_set, train_labels, test_set, test_ids)\nprint(rf_submission)\n\nrf_submission.to_csv('rf_submission.csv', index = False)","e47943f4":"train_set=pd.DataFrame(train_set,columns=features)\ncorr_matrix=train_set.corr()\n\nupper=corr_matrix.where(np.triu(np.ones(corr_matrix.shape),k=1).astype(np.bool))\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n\nto_drop","61190ebe":"train_set.shape\ntrain_set = train_set.drop(columns = to_drop)\ntrain_set.shape","aca345c7":"train_set=pd.DataFrame(train_set,columns=features)\ncorr_matrix=train_set.corr()\n\nupper=corr_matrix.where(np.triu(np.ones(corr_matrix.shape),k=1).astype(np.bool))\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n\nto_drop","3eaf34bb":"test_set = pd.DataFrame(test_set, columns = features)\nprint(test_set.shape)\ntrain_set, test_set = train_set.align(test_set, axis = 1, join = 'inner')\nfeatures = list(train_set.columns)","91968cee":"from sklearn.feature_selection import RFECV\n\n# Create a model for feature selection\nestimator = RandomForestClassifier(random_state = 10, n_estimators = 100,  n_jobs = -1)\n\n# Create the object\nselector = RFECV(estimator, step = 1, cv = 3, scoring= scorer, n_jobs = -1)\n","1434a233":"new_drops=[cols for cols in train_set.columns if train_set[cols].isnull().sum()>2000]\ntrain_set=train_set.drop(columns=new_drops)\ntest_set=test_set.drop(columns=new_drops)\ntrain_set.isnull().sum().sum()","135ff9d3":"#test_set=test_set.drop(columns=new_drops)\ntest_set.shape\ntrain_set.shape","55f364eb":"selector.fit(train_set, train_labels)","7a908d36":"plt.plot(selector.grid_scores_);\n\nplt.xlabel('Number of Features'); plt.ylabel('Macro F1 Score'); plt.title('Feature Selection Scores');\nselector.n_features_","409e415d":"rankings = pd.DataFrame({'feature': list(train_set.columns), 'rank': list(selector.ranking_)}).sort_values('rank')\nrankings.head(10)","e03f9081":"train_selected = selector.transform(train_set)\ntest_selected = selector.transform(test_set)","34c46e13":"# Convert back to dataframe\nselected_features = train_set.columns[np.where(selector.ranking_==1)]\ntrain_selected = pd.DataFrame(train_selected, columns = selected_features)\ntest_selected = pd.DataFrame(test_selected, columns = selected_features)","c6af38de":"model_results = cv_model(train_selected, train_labels, model, 'RF-SEL', model_results)","61c495b8":"model_results.set_index('model', inplace = True)\nmodel_results['cv_mean'].plot.bar(color = 'orange', figsize = (8, 6),\n                                  yerr = list(model_results['cv_std']),\n                                 edgecolor = 'k', linewidth = 2)\nplt.title('Model F1 Score Results');\nplt.ylabel('Mean F1 Score (with error bar)');\nmodel_results.reset_index(inplace = True)","13385774":"def macro_f1_score(labels, predictions):\n    # Reshape the predictions as needed\n    predictions = predictions.reshape(len(np.unique(labels)), -1 ).argmax(axis = 0)\n    \n    metric_value = f1_score(labels, predictions, average = 'macro')\n    \n    # Return is name, value, is_higher_better\n    return 'macro_f1', metric_value, True","6f83887a":"print(test.shape)","8b4f991e":"from sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgb\nfrom IPython.display import display\n\ndef model_gbm(features, labels, test_features, test_ids, \n              nfolds = 5, return_preds = False, hyp = None):\n    \"\"\"Model using the GBM and cross validation.\n       Trains with early stopping on each fold.\n       Hyperparameters probably need to be tuned.\"\"\"\n    \n    feature_names = list(features.columns)\n    print(feature_names)\n\n    # Option for user specified hyperparameters\n    if hyp is not None:\n        # Using early stopping so do not need number of esimators\n        if 'n_estimators' in hyp:\n            del hyp['n_estimators']\n        params = hyp\n    \n    else:\n        # Model hyperparameters\n        params = {'boosting_type': 'dart', \n                  'colsample_bytree': 0.88, \n                  'learning_rate': 0.028, \n                   'min_child_samples': 10, \n                   'num_leaves': 36, 'reg_alpha': 0.76, \n                   'reg_lambda': 0.43, \n                   'subsample_for_bin': 40000, \n                   'subsample': 0.54, \n                   'class_weight': 'balanced'}\n    \n    # Build the model\n    model = lgb.LGBMClassifier(**params, objective = 'multiclass', \n                               n_jobs = -1, n_estimators = 10000,\n                               random_state = 10)\n    \n    # Using stratified kfold cross validation\n    strkfold = StratifiedKFold(n_splits = nfolds, shuffle = True)\n    \n    # Hold all the predictions from each fold\n    predictions = pd.DataFrame()\n    importances = np.zeros(len(feature_names))# to the size of features present\n    \n    # Convert to arrays for indexing\n    features = np.array(features)\n    print(features)\n    test_features = np.array(test_features)\n    labels = np.array(labels).reshape((-1 ))\n    \n    valid_scores = []\n    modeld=lgb.LGBMClassifier(**params, objective = 'multiclass', \n                               n_jobs = -1, n_estimators = 10000,\n                               random_state = 10)\n    # Iterate through the folds\n    for i, (train_indices, valid_indices) in enumerate(strkfold.split(features, labels)):\n        \n        # Dataframe for fold predictions\n        fold_predictions = pd.DataFrame()\n        \n        # Training and validation data\n        X_train = features[train_indices]\n        X_valid = features[valid_indices]\n        y_train = labels[train_indices]\n        y_valid = labels[valid_indices]\n        modeld=model\n        # Train with early stopping\n        model.fit(X_train, y_train, early_stopping_rounds = 100, \n                  eval_metric = macro_f1_score,\n                  eval_set = [(X_train, y_train), (X_valid, y_valid)],\n                  eval_names = ['train', 'valid'],#(X_train, y_train) >>>>>> train and (X_valid, y_valid)>>>>>>>>>>>\n                  verbose = 200)\n        display(model)\n        display(model.best_score_)\n        # Record the validation fold score\n        valid_scores.append(model.best_score_['valid']['macro_f1'])\n        \n        # Make predictions from the fold as probabilities\n        fold_probabilitites = model.predict_proba(test_features) #Returns prediction probabilities for each class of each output.\n        display(fold_probabilitites)\n        # Record each prediction for each class as a separate column\n        for j in range(4):\n            fold_predictions[(j + 1)] = fold_probabilitites[:, j]\n        display(fold_predictions)    \n        # Add needed information for predictions \n        fold_predictions['idhogar'] = test_ids\n        fold_predictions['fold'] = (i+1)\n        \n        # Add the predictions as new rows to the existing predictions\n        predictions = predictions.append(fold_predictions)\n        \n        # Feature importances\n        importances += model.feature_importances_ \/ nfolds   \n        display(model.feature_importances_)\n        display(importances)\n        # Display fold information\n        display(f'Fold {i + 1}, Validation Score: {round(valid_scores[i], 5)}, Estimators Trained: {model.best_iteration_}')\n\n    # Feature importances dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names,\n                                        'importance': importances})\n    display(\"feature_importances\")\n    display(feature_importances)\n    valid_scores = np.array(valid_scores)\n    display(f'{nfolds} cross validation score: {round(valid_scores.mean(), 5)} with std: {round(valid_scores.std(), 5)}.')\n    display(valid_scores)\n    # If we want to examine predictions don't average over folds\n    if return_preds:\n        predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n        predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n        return predictions, feature_importances\n    \n    # Average the predictions over folds\n    predictions = predictions.groupby('idhogar', as_index = False).mean()\n    \n    # Find the class and associated probability\n    predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n    predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n    predictions = predictions.drop(columns = ['fold'])\n    display(predictions)\n    # Merge with the base to have one prediction for each individual\n    submission = submission_base.merge(predictions[['idhogar', 'Target']], on = 'idhogar', how = 'left').drop(columns = ['idhogar'])\n        \n    # Fill in the individuals that do not have a head of household with 4 since these will not be scored\n    submission['Target'] = submission['Target'].fillna(4).astype(np.int8)\n    display(submission)\n    display(\"Model Name\")\n    display(model)\n    # return the submission and feature importances along with validation scores\n    return submission, feature_importances, valid_scores","c10981e5":"%%capture --no-display\n%%capture --no-display\npredictions, gbm_fi = model_gbm(train_set, train_labels, test_set, test_ids, return_preds=True)","bab3e86d":"print(type(predictions))\n'''predictions_2 = pd.DataFrame({'Id': predictions['idhogar'],\n                               'Target': predictions['Target']})'''","b353fa58":"predictions.to_csv('lgb1_submission.csv', index = False)","e6940c49":"predictions.head()","9552d52a":"plt.figure(figsize = (24, 12))\nsns.violinplot(x = 'Target', y = 'confidence', hue = 'fold', data = predictions);","bebc43c1":"predictions = predictions.groupby('idhogar', as_index = False).mean()\npredictions['Target']=predictions[[1,2,3,4]].idxmax(axis=1)\npredictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\npredictions = predictions.drop(columns = ['fold'])\n\n\nplt.figure(figsize=(10,6))\nsns.boxplot(x='Target',y='confidence',data=predictions)\nplt.title(\"Confidence vs Target\")\n\nplt.figure(figsize=(10,6))\nsns.violinplot(x='Target',y='confidence',data=predictions)\nplt.title(\"Confidence vs Target\")\n","6ecf5594":"%%capture\nsubmission, gbm_fi, valid_scores = model_gbm(train_set, train_labels, \n                                             test_set, test_ids, return_preds=False)\n\nsubmission.to_csv('gbm_baseline.csv')","fd6c928a":"_ = plot_feature_importances(gbm_fi, threshold=0.95)\n","fa907f67":"%%capture --no-display\nsubmission, gbm_fi_selected, valid_scores_selected = model_gbm(train_selected, train_labels, \n                                                               test_selected, test_ids)\n","5dbfb08f":"model_results = model_results.append(pd.DataFrame({'model': [\"GBM_10Fold\", \"GBM_10Fold_SEL\"], \n                                                   'cv_mean': [valid_scores.mean(), valid_scores_selected.mean()],\n                                                   'cv_std':  [valid_scores.std(), valid_scores_selected.std()]}),\n                                    sort = True)","01c178b1":"model_results.set_index('model', inplace = True)\nmodel_results['cv_mean'].plot.bar(color = 'orange', figsize = (8, 6),\n                                  yerr = list(model_results['cv_std']),\n                                 edgecolor = 'k', linewidth = 2)\nplt.title('Model F1 Score Results');\nplt.ylabel('Mean F1 Score (with error bar)');\nmodel_results.reset_index(inplace = True)\n","db1ab257":"%%capture\nsubmission, gbm_fi, valid_scores = model_gbm(train_set, train_labels, test_set, test_ids, \n                                             nfolds=10, return_preds=False)","0b617936":"submission.to_csv('gbm_10fold.csv', index = False)\n","1c6adde6":"%%capture\nsubmission, gbm_fi_selected, valid_scores_selected = model_gbm(train_selected, train_labels, test_selected, test_ids,\n                                                               nfolds=10)","2b5ec704":"submission.to_csv('gmb_10fold_selected.csv', index = False)\n","b9927245":"model_results = model_results.append(pd.DataFrame({'model': [\"GBM_10Fold\", \"GBM_10Fold_SEL\"], \n                                                   'cv_mean': [valid_scores.mean(), valid_scores_selected.mean()],\n                                                   'cv_std':  [valid_scores.std(), valid_scores_selected.std()]}),\n                                    sort = True)","e4566a8a":"model_results.set_index('model', inplace = True)\nmodel_results['cv_mean'].plot.bar(color = 'orange', figsize = (8, 6), \n                                  edgecolor = 'k', linewidth = 2,\n                                  yerr = list(model_results['cv_std']))\nplt.title('Model F1 Score Results');\nplt.ylabel('Mean F1 Score (with error bar)');\nmodel_results.reset_index(inplace = True)","1f500177":"We draw violinplot which shows the distribution of a variable on the y axis with the width of each plot showing the number of observations in that category.","46236b9b":"Start with imports","a41aa14b":"Thus number of cases where hhsize!=tamviv. This gives us a good idea for a new feature: the difference between these two measurements","1a4361ca":"A very low score again.","82e4e7c6":"Correlation HeatMap","c9ba6d67":"Thus we see households with a 4 ie non vulnerable have more spending on floor wall and roof 1 having least.","f2313f6d":"All 0.95 > correlations have been removed","f4fe0a67":"**Machine Learning modelling**","e2de1803":"Let us find umber of unique integer value","75f6b030":"If you run LinearDiscriminantAnalysis without filtering out the UserWarnings, you get many messages saying \"Variables are collinear.\" This might give us a hint that we want to remove some collinear features! We might want to try this model again after removing the collinear variables because the score is comparable to the random forest.","1de10f49":"v18q1 signifies whether or not a family contain tablet we can replace NAN with 1, however NAN may also mean that families dont own tablet **v18q**  signifies whether or not a family owns a tablet we combine values to see if our hypothesis is right","a2218d73":"Now we have 180  features from 30. Renaming the columns","1bd48aff":"Feature construction","e398ed87":"**Feature Importances**\n\n\nIf we want to view the feature importances, we'll have to train a model on the whole training set. Cross validation does not return the feature importances.","2c07e18d":"We now start to remove squared variables as we will be dealing with complex models rather than just linear as a result such variables add to redundancy\n\nWe plot SQBage vs age","5684cad8":"Thus we see everyone with no tablets has associated NAN value with it we simply fillna with 0 for v18q1","150392fe":"Thus there are no households with no heads and different targets","6d07d908":"Lesser value of warning should corrospond to lower target and higher value of warning corrospond to higher target as we see in 0 more concentration on 4.","94380ffa":"Function to plot feature importances","9670327a":"Measuring Relationships","efc79d6d":"Moving on to coopele (Redundant variable)","57f511b6":"The function below takes in a model, a training set, the training labels, and a testing set and performs the following operations:\n\nTrains the model on the training data using fit\nMakes predictions on the test data using predict\nCreates a submission dataframe that can be saved and uploaded to the competition","68109236":"We make a function to count values for different columns","b3c1305f":"This tells us oldest age with a missing values is 17 ie oldest one going to school is 17.\nWe next find max age when NAN. ","9f243e71":" For this variable, if the individual is over 19 and they have a missing value, or if they are younger than 7 and have a missing value we can set it to zero. For anyone else, we'll leave the value to be imputed and add a boolean flag.","5955061d":"Taking a look into items","6e07524d":"We draw a value count plot for where these values missing","7627d2ae":"For this it maybe possible that household has no childrens.We check max age of anyone going to school","93d25d7f":"**Plot Two Categoricals**","5ab342d2":"0: No electricity\n1: Electricity from cooperative\n2: Electricity from CNFL, ICA, ESPH\/JASEC\n3: Electricity from private plant\n","ef14124b":"**Upgrading Our Model: Gradient Boosting Machine\n**\n\n\nAfter using the Random Forest and getting decent scores, it's time to step up and use the gradient boosting machine. If you spend any time on Kaggle, you'll notice that the Gradient Boosting Machine (GBM) wins a high percentage of competitions where the data is structured (in tables) and the datasets are not that large (less than a million observations).\n\nWe will focus on the implementation. We'll use the GBM in LightGBM, although there are also options in Scikit-Learn, XGBOOST, and CatBoost. The first set of hyperparameters we'll use were based on those I've found have worked well for other problems.\n","8ab41294":"**Thus we see houses owned are the ones paying NAN as rent ie no rent we fill values with 1 in tipovivi1 and we can use imputer later to impute remaining values but we put a flag indicating these values**","445944a3":"We add null target to test data and append to train data too as we want to do feature engineering to both test and train we will later find test and train based on nan in targets","cabc4aac":"However we will manually adjust features as automated tools have created too many new fwatures and they may lead to overfit","abdf0e9a":"We see a low performance of scores so we dont use this","0966f1f8":"**Missing values**","6e7cc5c6":"The next step with the LightGBM is to try the features that were selected through recursive feature elimination.","c5d932a3":"model RandomForestClassifier","9aa2559a":"**Redundant Individual Variables\n**\n\nWe carry out same procedure as in household variables.\nWe'll focus on any variables that have an absolute magnitude of the correlation coefficient greater than 0.95.","e218d073":"Thus we see imbalanced class problem as far more non vulnerable households than extreme \nthis makes difficulty in predicting extreme as far less cases\n\nAlso from previous graphs we can determine there is case of wrong label as same household has different levels of poverty which is wrong.We are told to use head of household as true label.\n\n**Identification**","1b5422de":"Now we try to find households without a head but different labels","46c617d8":"However, feature importances don't tell us which direction of the feature is important (for example, we can't use these to tell whether more or less education leads to more severe poverty) they only tell us which features the model considered relevant.","203ccc32":"For each fold, the 1, 2, 3, 4 columns represent the probability for each Target. The Target is the maximum of these with the confidence the probability. We have the predictions for all 5 folds, so we can plot the confidence in each Target for the different folds.\n\n","2afbc991":"As a first round of selection we remove one out of every pair of variable with a correlation >0.95","73466007":"It looks like households where the head is female are slightly more likely to have a severe level of poverty.","277164fb":"**object columns**","63bf2005":"Checking that we have'nt done any repetition or we have'nt missed any variables.","2a4a76e8":"v2a1 having having NAN that there must be no rent associated this maybe a cause of ownership of house we have tipovivi1=1 for own and fully paid house.\n","412cccf5":"thus 4 columns with object dtypes dependency,edjefe,edjefa seem to be a mix of values","b94d3541":"Next variable will be warning about house no floor no water,no cieling -1 point for each.","f0879bce":"This shows scatterplots on the upper triangle, kernel density estimate (kde) plots on the diagonal, and 2D KDE plots on the lower triangle.","a9450ecb":"Coming to area2 redundant variable, we drop it cause we have area 1","8a57e70a":"The final household feature we can make for now is a bonus where a family gets a point for having a refrigerator, computer, tablet, or television.","f43c657d":"**Exploratory Data Analysis**","e29dd0cf":"**Correction**","8d3fe2ac":"tamviv is not necessarily the same as hhsize because there might be family members that are not living in the household. Let's visualize this difference in a scatterplot.","5bd2a1ba":"A high score with multi layer perceptron ,we can hyper tune its parameters ,limited amount of data may cause problem as thousands of examples needed with neural networks.","3ea59009":"These graphs give basic understanding like in sqbmeaned we see higher value with non vulnerable households similarly overcrowding we see non vulnerable not in higher overcrowding.","ef8cbd12":"Feature construction ","cdda2953":"1. Understand the problem (we're almost there already)\nExploratory Data Analysis\n2.Feature engineering to create a dataset for machine learning\n3.Compare several baseline machine learning models\n4.Try more complex machine learning models\n5.Optimize the selected model\n6.Investigate model predictions in context of problem\n7.Draw conclusions and lay out next steps","47a7765d":"One other feature that might be useful is the gender of the head of household. Since we aggregated the data, we'll have to go back to the individual level data and find the gender for the head of household.","331a5234":"**Exploring Label Distribution\n**\n\nWe use only subset columns where parentesco1==1 because him\/her being head of household","982f2e93":"\n1. **Pearson Correlation**-Measuring linear relationshipbetween 2 variables.\n\n\n2.**Spearman Correlation**:Measuring monotonic relationship between two variables.","faef7e6f":"For floats columns","7a93d460":"we replace 0s with no and 1 with yes","0885c6f7":"Feature scaling and imputing pipeline.","90425d78":"We'll drop the columns and then merge with the heads data to create a final dataframe.","032f985a":"Next feature is **v2a1** i.e. ***Monthly rent payments***  ","76b02467":"**Feature Selection**","0bad9209":"Similarly for roof and floors","f6b2f3f3":"We can also take our new variable, inst, and divide this by the age. The final variable we'll name tech: this represents the combination of tablet and mobile phones","ffd1d3eb":"We can see that the score improves as we add features up until 97 features. According to the selector, this is the optimal number of features.\n\nThe rankings of each feature can be found by inspecting the trained object. These represent essentially the importance of features averaged over the iterations. Features can share the same ranking, and only features with a rank of 1 are retained.","ab314309":"Then we fit the selector on the training data as with any other sklearn model. This will continue the feature selection until the cross validation scores no longer improve.","9cb90661":"**Feature Selection**\n\nWe try identify and keep only essential features.\n\n\nFor feature selection in this notebook, we'll first remove any columns with greater than 0.95 correlation (we already did some of this during feature engineering) and then we'll apply recursive feature elimination with the Scikit-Learn library.\n\nStarting with removing features with >0.95 correlation\n","2fb0ac36":"Then we fit the selector on the training data as with any other sklearn model. This will continue the feature selection until the cross validation scores no longer improve.","82b0c3b1":"In order to incorporate the individual data into the household data, we need to aggregate it for each household. The simplest way to do this is to groupby the family id idhogar and then agg the data. For the aggregations for ordered or continuous variables, we can use six, five of which are built in to pandas, and one of which we define ourselves range_. The boolean aggregations can be the same, but this will create many redundant columns which we will then need to drop. For this case, we'll use the same aggregations and then go back and drop the redundant columns.","3d39b165":"**Final Data Exploration***","d580d706":"Thus we see poverty level 4 are most educated ,1 being the least also there are nominal poverty level 1 people who are educated over 4 years of age. Also as education years increase poverty level tends to be more towards 4 i.e increase.","8f819cf9":"Finally, we select the features and then evaluate in cross validation.\n\n","c9f9f402":"any values above 5 should be set to 5.\n","0b4cbf8c":"**Light Gradient Boosting Machine Implementation**\n\n","5bef4d18":"We start with correcting households with target level different than household leader","1037f1d9":"**Recursive Feature Elimination with Random Forest**\n\n\nThe RFECV in Sklearn stands for Recursive Feature Elimination with Cross Validation. The selector operates using a model with feature importances in an iterative manner. At each iteration, it removes either a fraction of features or a set number of features. The iterations continue until the cross validation score no longer improves.\n\nTo create the selector object, we pass in the the model, the number of features to remove at each iteration, the cross validation folds, our custom scorer, and any other parameters to guide the selection.","cd2225d3":"The largest discrepancy in the correlations is dependency. We can make a scatterplot of the Target versus the dependency to visualize the relationship. We'll add a little jitter to the plot because these are both discrete variables.\n","18a68e9d":"These are highly correlated so we would drop square_Variables","4c44943d":"Features Plot","f6c462ce":"There are several different categories of variables:\n\n***1 Individual Variables: these are characteristics of each individual rather than the household\n**         \n          \n            Boolean: Yes or No (0 or 1)\n            \n            Ordered Discrete: Integers with an ordering\n            \n            \n***2.Household variables\n**            \n        \n            Boolean: Yes or No\n            \n            Ordered Discrete: Integers with an ordering\n            \n            Continuous numeric\n            \n            \n***3,Squared Variables: derived from squaring variables in the data\n**\n\n***4.Id variables: identifies the data and should not be used as features**","c696177b":"Now we go to next column rez_esc which signifies years behind school","2483516c":"More Features","bfc9aeb8":" There are also high correlations between some variables (such as floor and walls+roof+floor) which could pose an issue because of collinearity.","d9edcab1":"**Individual Level Variables\n**\n\nThere are two types of individual level variables: Boolean (1 or 0 for True or False) and ordinal (discrete values with a meaningful ordering).","82d9864d":"We try 10 fold with both sets and add them to plot.\n\n","73f75192":"Our data has two levels- Household level and personal level. We use featuretools on idhogar","b28ad01f":"Thus we see higher prevelence of 2 in more poverty","fffbd18c":"Families without heads of household","6be543b2":"Now we deal with redundant variables","cac8f002":"Now we will use Deep Feature Synthesis to create new features automatically"}}