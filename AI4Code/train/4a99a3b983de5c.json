{"cell_type":{"7771dc2b":"code","6975d6fb":"code","a5a53dcd":"code","630d43cf":"code","3336c8fb":"code","46f50fcc":"code","0fff3a14":"code","4245ec15":"code","c6cd75e2":"code","27fa1119":"code","4ebbbbfd":"code","04b871ea":"code","a01a2397":"code","e260986f":"code","40237844":"code","8c50af06":"code","ea1c7f31":"code","271de330":"code","3bd5635e":"code","f7dacc48":"code","d4c215f1":"code","d3c56823":"code","c706ebf8":"code","0a05bffd":"code","addaff04":"code","5aa2fc4f":"code","57353e0c":"code","697748ed":"code","739b2982":"code","ff6881bc":"code","914e711b":"code","a3b86ce2":"code","d4bb1827":"code","86e359e9":"code","31250712":"code","30266243":"code","a3aef0fe":"code","9ccd9652":"code","84499c8a":"code","410b26dd":"code","be8ca85b":"code","7c0d2fd1":"code","139583b1":"code","e0cc21ee":"code","952ecb58":"code","c2c37c3e":"code","cbd9dbab":"code","dae82b3b":"code","197e3ff3":"code","8c2243cb":"code","95b249b3":"code","291f0cb7":"code","e63c5fb3":"code","71d75377":"code","ee01b42d":"code","1edf8b0a":"code","097fc308":"markdown","3743ebe7":"markdown","51f3ace6":"markdown","95bc3fd4":"markdown","3ec3693c":"markdown","b6f105f9":"markdown","1e48cd25":"markdown","fb0c087f":"markdown","421acbcd":"markdown","3c188049":"markdown","cdeb68b9":"markdown","a35c2f9c":"markdown","abd6908d":"markdown","b992c0b0":"markdown","d2de5a7a":"markdown","f5ead0a1":"markdown","7d34784f":"markdown","1fc28f74":"markdown","c8de9966":"markdown","f4980c0a":"markdown","b7301692":"markdown","781a6003":"markdown","e5b7fd21":"markdown","c34aaf58":"markdown","50b85e48":"markdown","03f528f2":"markdown"},"source":{"7771dc2b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6975d6fb":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\n# machine learning\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n%matplotlib inline","a5a53dcd":"titanic_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntitanic_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","630d43cf":"titanic_test.head()","3336c8fb":"print(titanic_train.shape)\nprint(titanic_test.shape)","46f50fcc":"# First 5 rows of the training dataset\ntitanic_train.head()","0fff3a14":"# Shape of the training dataset\ntitanic_train.shape","4245ec15":"# Get some info about the training dataset\ntitanic_train.info()","c6cd75e2":"# view some statistical details of the training dataset\ntitanic_train.describe()","27fa1119":"titanic_train.describe(include=['O'])","4ebbbbfd":"# check how many missing values each column have in training dataset\ntitanic_train.isnull().sum()","04b871ea":"# Find out frequency of each value of Predictor variable - Survived  \ntitanic_train['Survived'].value_counts()","a01a2397":"# Plot the frequency of each value of Predictor variable\nsns.countplot(x='Survived', data=titanic_train)","e260986f":"# Plot of survival vs fare price\nsns.barplot(x='Survived', y='Fare', data=titanic_train)","40237844":"# Number male and female passengers\ntitanic_train['Sex'].value_counts()","8c50af06":"sns.countplot(x='Sex', data=titanic_train)","ea1c7f31":"# Survival based on sex\nsns.barplot(x='Sex', y='Survived', data=titanic_train)","271de330":"titanic_train[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","3bd5635e":"# Number of Passengers on various Pclass\ntitanic_train['Pclass'].value_counts()","f7dacc48":"# Survival on the basis of Pclass\nsns.barplot(x='Pclass', y='Survived', data=titanic_train)","d4c215f1":"# % of passengers survived in each class\ntitanic_train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","d3c56823":"# Histogram plot of age\nsns.histplot(data = titanic_train['Age'], kde = True)","c706ebf8":"g = sns.FacetGrid(titanic_train, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","0a05bffd":"# Survival on the basis of Embarked\nsns.barplot(x='Embarked', y='Survived', data=titanic_train)","addaff04":"titanic_train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","5aa2fc4f":"# Checking correlation among various numerical variables\ntitanic_train.corr()","57353e0c":"sns.heatmap(titanic_train.corr(), annot=True)","697748ed":"titanic_train.drop(['Name', 'Cabin', 'PassengerId', 'Ticket', 'SibSp', 'Parch'], axis=1, inplace=True)","739b2982":"# impute the missing age values with the mean \ntitanic_train['Age'].fillna(value=titanic_train['Age'].mean(), inplace=True)","ff6881bc":"# drop the  observations having missing Embarked (as only 2 of them are missing)\ntitanic_train.dropna(axis=0, inplace=True)","914e711b":"titanic_train.head()","a3b86ce2":"titanic_test.drop(['Name', 'Cabin', 'PassengerId', 'Ticket', 'SibSp', 'Parch'], axis=1, inplace=True)","d4bb1827":"# impute the missing age values with the mean \ntitanic_test['Age'].fillna(value=titanic_train['Age'].mean(), inplace=True)","86e359e9":"# drop the  observations having missing Fare (as only 1 of them are missing)\ntitanic_test['Fare'].fillna(value=titanic_train['Fare'].mean(), inplace=True)","31250712":"titanic_test.head()","30266243":"titanic_train.loc[ titanic_train['Age'] <= 16, 'Age'] = 0\ntitanic_train.loc[(titanic_train['Age'] > 16) & (titanic_train['Age'] <= 32), 'Age'] = 1\ntitanic_train.loc[(titanic_train['Age'] > 32) & (titanic_train['Age'] <= 48), 'Age'] = 2\ntitanic_train.loc[(titanic_train['Age'] > 48) & (titanic_train['Age'] <= 64), 'Age'] = 3\ntitanic_train.loc[ titanic_train['Age'] > 64, 'Age']","a3aef0fe":"titanic_train.head()","9ccd9652":"X_train = titanic_train.iloc[:, 1:].values\ny_train = titanic_train.iloc[:, :1].values.ravel()","84499c8a":"print(X_train.shape)\nprint(y_train.shape)","410b26dd":"# Encoding the Sex variable\ncolumnTransformer = ColumnTransformer([('encoder', OneHotEncoder(), [1])], remainder='passthrough')\nX_train = columnTransformer.fit_transform(X_train)","be8ca85b":"# Encoding the Emberked variable\ncolumnTransformer = ColumnTransformer([('encoder', OneHotEncoder(), [-1])], remainder='passthrough')\nX_train = columnTransformer.fit_transform(X_train)","7c0d2fd1":"X_train.shape","139583b1":"X_train","e0cc21ee":"titanic_test.loc[ titanic_test['Age'] <= 16, 'Age'] = 0\ntitanic_test.loc[(titanic_test['Age'] > 16) & (titanic_test['Age'] <= 32), 'Age'] = 1\ntitanic_test.loc[(titanic_test['Age'] > 32) & (titanic_test['Age'] <= 48), 'Age'] = 2\ntitanic_test.loc[(titanic_test['Age'] > 48) & (titanic_test['Age'] <= 64), 'Age'] = 3\ntitanic_test.loc[ titanic_test['Age'] > 64, 'Age']\n\ntitanic_test.head()","952ecb58":"titanic_test.head()","c2c37c3e":"X_test = titanic_test.iloc[:, :].values","cbd9dbab":"# Encoding the Sex variable\ncolumnTransformer = ColumnTransformer([('encoder', OneHotEncoder(), [1])], remainder='passthrough')\nX_test = columnTransformer.fit_transform(X_test)","dae82b3b":"# Encoding the Emberked variable\ncolumnTransformer = ColumnTransformer([('encoder', OneHotEncoder(), [-1])], remainder='passthrough')\nX_test = columnTransformer.fit_transform(X_test)","197e3ff3":"X_test.shape","8c2243cb":"X_test","95b249b3":"# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, y_train) * 100, 2)\nacc_svc","291f0cb7":"# K-Nearest Neighbors\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, y_train) * 100, 2)\nacc_knn","e63c5fb3":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\ny_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, y_train) * 100, 2)\nacc_gaussian","71d75377":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, y_train)\ny_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, y_train) * 100, 2)\nacc_decision_tree","ee01b42d":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, y_train)\ny_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, y_train)\nacc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\nacc_random_forest","1edf8b0a":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', \n              'Random Forest', 'Naive Bayes', 'Decision Tree'],\n    'Score': [acc_svc, acc_knn, \n              acc_random_forest, acc_gaussian, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)","097fc308":"---\nPassengers paying high fare had higher chances of survival","3743ebe7":"---\nMajority of the people did not survive ","51f3ace6":"## Analytical Approach\n\nOur target variable is categorical (survived \/ not survived), and hence we need classification models for this task.","95bc3fd4":"---\n## Feature Engineering","3ec3693c":"---\nThere are 891 observations in the training dataset with each having 12 columns. 11 of them are predictor variables and 1 being target variable.","b6f105f9":"## Data Cleaning","1e48cd25":"## Business Understanding\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nThe challenge here is - Given a passenger's information, how can we predict whether he\/she survived the Titanic disaster?\n","fb0c087f":"Age is a continous variable and I would like to continous numerical feature into an ordinal categorical feature","421acbcd":"## Import the Libraries","3c188049":"-----\nMost people were in the group 15-35 years of age","cdeb68b9":"## Modelling","a35c2f9c":"There are few different types of variables available.\n- Continous: Age, Fare\n- Discrete: SibSp, Parch\n- Categorical: Survived, Sex, and Embarked\n- Ordinal: Pclass\n- Mixed: Ticket\n- Alphanumeric: Cabin","abd6908d":"----\nObservations:\n- Infants (Age <=4) had high survival rate.\n- Oldest passengers (Age = 80) survived.\n- Large number of 15-25 year olds did not survive.\n","b992c0b0":"-----\nThere were more male passengers than female","d2de5a7a":"----\nAlthough Pclass = 3 had maximum number of passengers, very few survived","f5ead0a1":"## Import the Datasets","7d34784f":"----\nPclass = 3 had maximum passengers","1fc28f74":"### For the test set","c8de9966":"# Data requirements\n\nWe would require onboard passengers information which might include name, age, fare, gender, class.","f4980c0a":"----\n### Assumptions based on analysis so far:\n- Impute the missing Age values\n- Impute the missing Embarked values\n- drop Cabin [too many missing values]\n- drop Ticket [many duplicates]\n- drop PassengerID [not helpful]\n- drop Name [not helpful]\n- drop SibSp and Parch as well\n----","b7301692":"## Exploratory Data Analysis","781a6003":"----\nFemale passengers survived more than Male","e5b7fd21":"---\nFeatures with missing values\n- Cabin\n- Age\n- Embarked\n\nCabin has way too many missing values and hence it is better to drop","c34aaf58":"Test set","50b85e48":"\n## Data collection\n\nWe are given two datasets both of which are CSV files, one for training our model named as train.csv and the other test.csv to test if our model can determine survival based on observations, not having the survival info. ","03f528f2":"## Evaluation"}}