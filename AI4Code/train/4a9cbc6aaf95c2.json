{"cell_type":{"c3a0e8a3":"code","5c8195c3":"code","6e2dad7b":"code","6b00c998":"code","9e7718b3":"code","43db2246":"code","c05da5c1":"code","7e77920f":"code","b869646f":"code","c1785d38":"code","14fddf71":"code","9810c681":"code","aaab2375":"code","8e02c34b":"code","9576c0bf":"code","4a518a4f":"code","347c3f60":"code","cbc7203f":"code","f92004c3":"code","8115b7cf":"code","726bb3a0":"code","5eb377fe":"code","0b97e997":"code","6032eec5":"code","161a3809":"code","0b5ffda1":"code","ea1b9560":"code","69b03ae0":"code","b00756d1":"code","d11b1fdf":"code","9e695db5":"code","88671031":"code","943a31ef":"code","f83323d1":"code","0fd3e1cc":"code","3b2ac16b":"code","a2d33869":"code","5f625318":"code","2202bded":"markdown","ff16768b":"markdown","f3942ace":"markdown","7baeec57":"markdown","f44c60c6":"markdown","6ef4b094":"markdown","d6819aab":"markdown","9dfbd50c":"markdown","f855453d":"markdown","25df6e56":"markdown","6ba10635":"markdown","56bce95c":"markdown","310edca8":"markdown","6786aae4":"markdown","ff53910c":"markdown","a5c14fb2":"markdown","52c5903d":"markdown","94ec7db4":"markdown","c00eb7ab":"markdown","4856da17":"markdown","318a9b4f":"markdown","225efe39":"markdown","fd65f327":"markdown","2587ac0c":"markdown","23e9d0b0":"markdown","17b2455b":"markdown","0efc7cac":"markdown","b7878c71":"markdown","9154162e":"markdown","3707e3a4":"markdown","e7d5df6c":"markdown","1d32da16":"markdown","a8dc59b2":"markdown","57f95ada":"markdown","41eb0dc9":"markdown","85e4ad36":"markdown"},"source":{"c3a0e8a3":"import pandas as pd\nimport numpy as np\nimport tqdm.notebook as tq\nimport nltk\nfrom sklearn import preprocessing\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import pyplot\nimport ast\nfrom collections import Counter\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import distance\nimport copy\nfrom IPython.display import display\nimport warnings\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets\nimport re\nimport plotly\nfrom IPython.display import Image\nfrom sklearn.preprocessing import MinMaxScaler # for normalizing data\nfrom sklearn.cluster import KMeans \nwarnings.filterwarnings(\"ignore\")\n\nprint(\"Import successful\")","5c8195c3":"import os\nfor dirname, _, filenames in os.walk('..\/input\/spotifydata-19212020\/data.csv'):\n    for filename in filenames:\n        print(os.path.join(input\/spotifydata-19212020, data.csv))","6e2dad7b":"data = pd.read_csv('..\/input\/spotifydata-19212020\/data.csv')\ndata.head()","6b00c998":"data.shape","9e7718b3":"data.isna().sum()","43db2246":"data[data['name']=='Adore You']","c05da5c1":"data.drop_duplicates(subset=['artists','name'],inplace = True)","7e77920f":"data.shape","b869646f":"data.describe()","c1785d38":"#bucketing\nyear_list = sorted(list(data['year'].unique()))\nres = pd.cut(data['year'],10,labels=['1921-31','1931-41','1941-51','1951-61','1961-71','1971-81','1981-91','1991-01','2001-11','2011-2020'])\ndata['decade'] = res \ndata.head(2)","14fddf71":"data.to_csv('clean_data_spotify.csv',index=False)","9810c681":"print(data['decade'].value_counts())\ncount = dict(data['decade'].value_counts())\nfig = px.pie(values=list(count.values()), names=list(count.keys())) \nfig.show()","aaab2375":"print(data['mode'].value_counts())\ncount =dict(data['mode'].value_counts())\nfig = px.pie(values=list(count.values()), names=list(count.keys())) \nfig.show()","8e02c34b":"print(data['explicit'].value_counts())\ncount =dict(data['explicit'].value_counts())\nfig = px.pie(values=list(count.values()), names=list(count.keys())) \nfig.show()","9576c0bf":"import ipywidgets as widgets\nall = 'ALL'\ndef unique_sorted_values(array):\n  distinct = array.unique().tolist()\n  distinct=sorted(distinct)\n  distinct.insert(0,all)\n  return distinct\n\nyear = widgets.Dropdown(options= unique_sorted_values(data['year']))\n\ndef condition(change):\n    if (change.new == all):\n      df=data.sort_values('popularity',ascending = False)[['name','popularity']][0:5]\n      print(df)\n      plt.figure(figsize=(10,3))\n      plt.barh(df['name'], df['popularity'],color=('cyan','blue'))\n      plt.show()\n\n\n    else:\n      df=data[data['year'] == change.new]\n      df=df.sort_values('popularity',ascending = False)[['name','popularity']][0:5]\n      print(df)\n      plt.figure(figsize=(10,3))\n      plt.barh(df['name'], df['popularity'],color=('maroon','olive'))\n      plt.show()\n\n\n\nyear.observe(condition, names ='value')\ndisplay(year)","4a518a4f":"import ipywidgets as widgets\n\nlst = ['acousticness', 'danceability', 'valence', 'loudness', 'energy','duration_ms']\n\nfactors = widgets.Dropdown(options = lst)\n\ndef condition(change):\n  for i in range(len(lst)):\n    if (change.new == lst[i]):\n      df=data.sort_values(lst[i],ascending = False)[['name',lst[i]]][0:5]\n      print(df)\n      plt.figure(figsize=(10,3))\n      plt.barh(df['name'], df[lst[i]],color=('maroon','olive'))\n      plt.show()\n\nfactors.observe(condition, names ='value')\ndisplay(factors)","347c3f60":"plt.figure(figsize=(12,7))\nsns.boxplot(x='decade',y='popularity',hue = 'explicit',data=data)\nplt.title('year wise popularity and explicit content')\nplt.show()","cbc7203f":"sns.barplot(x='key', y = 'popularity', hue='mode', data=data,ci=None)\nplt.legend(loc='best', bbox_to_anchor=(0.8, 0., 0.5, 0.9))\nplt.show()","f92004c3":"# dataframe for clustering \ncluster = data[['valence','acousticness','danceability','duration_ms','energy','instrumentalness','liveness','loudness','speechiness','tempo']] \ncluster.head(5)","8115b7cf":"from sklearn.preprocessing import MinMaxScaler \nminmax = MinMaxScaler() \ncluster['duration_ms'] = minmax.fit_transform(np.array(cluster['duration_ms']).reshape(-1,1))\ncluster['loudness'] = minmax.fit_transform(np.array(cluster['loudness']).reshape(-1,1))\ncluster['tempo'] = minmax.fit_transform(np.array(cluster['tempo']).reshape(-1,1)) ","726bb3a0":"cluster.head()","5eb377fe":"from sklearn import decomposition \npca = decomposition.PCA()\npca.n_components = 2\npca_df = pca.fit_transform(cluster) \npca_df","0b97e997":"from sklearn.cluster import KMeans \nwcss_list = [] \nfor i in range(1,11):\n    kmeans = KMeans(n_clusters=i,init = 'k-means++',random_state=42) \n    kmeans.fit(cluster) \n    wcss_list.append(kmeans.inertia_) \nplt.plot(range(1,11),wcss_list) \nplt.title('elbow method') \nplt.xlabel('No of clusters(k)') \nplt.ylabel('wcss') \nplt.show()","6032eec5":"kmeans = KMeans(n_clusters=3,init = 'k-means++',random_state=42) \ny_pred = kmeans.fit_predict(cluster) ","161a3809":"y_pred","0b5ffda1":"plt.figure(figsize=(15,10))\nplt.scatter(pca_df[y_pred==0,0],pca_df[y_pred==0,1],s=100,c='blue',label = 'Cluster 1') \nplt.scatter(pca_df[y_pred==1,0],pca_df[y_pred==1,1],s=100,c='green',label = 'Cluster 2') \nplt.scatter(pca_df[y_pred==2,0],pca_df[y_pred==2,1],s=100,c='red',label = 'Cluster 3') \nplt.scatter(pca_df[y_pred==3,0],pca_df[y_pred==3,1],s=100,c='maroon',label = 'Cluster 4') \nplt.scatter(pca_df[y_pred==4,0],pca_df[y_pred==4,1],s=100,c='cyan',label = 'Cluster 5') \n \nplt.title('Clusters of songs') \nplt.xlabel('Principal_Comp1') \nplt.ylabel('Principal_Comp2') \nplt.show()","ea1b9560":"cluster['label'] = y_pred \ncluster.head()","69b03ae0":"from sklearn.metrics import silhouette_score\nsilhouette_score(cluster, cluster['label'], metric='euclidean')","b00756d1":"table = pd.pivot_table(cluster, index=['label'], aggfunc=np.mean) \ntable ","d11b1fdf":"cluster = cluster.drop(columns=['label'])\ncluster_sample = cluster.sample(n=10000) \ncluster_sample.head() ","9e695db5":"cluster_sample.shape","88671031":"from sklearn import decomposition \npca = decomposition.PCA()\npca.n_components = 2\npca_df = pca.fit_transform(cluster_sample) \npca_df","943a31ef":"pca_df.shape","f83323d1":"from sklearn.cluster import AgglomerativeClustering\nhc = AgglomerativeClustering(n_clusters=3, affinity= 'euclidean',linkage='ward') \ny_pred = hc.fit_predict(cluster_sample)","0fd3e1cc":"plt.figure(figsize=(15,10))\nplt.scatter(pca_df[y_pred==0,0],pca_df[y_pred==0,1],s=100,c='blue',label = 'Cluster 1') \nplt.scatter(pca_df[y_pred==1,0],pca_df[y_pred==1,1],s=100,c='green',label = 'Cluster 2') \nplt.scatter(pca_df[y_pred==2,0],pca_df[y_pred==2,1],s=100,c='red',label = 'Cluster 3') \nplt.scatter(pca_df[y_pred==3,0],pca_df[y_pred==3,1],s=100,c='maroon',label = 'Cluster 4') \nplt.scatter(pca_df[y_pred==4,0],pca_df[y_pred==4,1],s=100,c='cyan',label = 'Cluster 5') \n\nplt.title('Clusters of songs') \nplt.xlabel('Principal_Comp1') \nplt.ylabel('Principal_Comp2') \nplt.show()","3b2ac16b":"cluster_sample['Label']=y_pred","a2d33869":"from sklearn.metrics import silhouette_score\nsilhouette_score(cluster_sample, cluster_sample['Label'], metric='euclidean')","5f625318":"table = pd.pivot_table(cluster_sample, index=['Label'], aggfunc=np.mean) \ntable","2202bded":"### Cluster Analysis\n**Cluster 0**: These songs are less acoustic\/instrumental. This cluster consists of song which have highest loudness, energy and tempo. This is definitely the most dynamic cluster.They have the highest valence. These songs are the songs with happiest vibes.         \n**Cluster 1**:  This cluster has songs which are most likely to have been played live, since the liveness and speechiness index are the highest. This cluster also has high acousticnesss.         \n**Cluster 2**: These songs have the highest acousticness and instrumentalness. This cluster has the slowest music style.  \n","ff16768b":"Importing libraries","f3942ace":"Widgets(a dropdown) for getting top 5 most acoustic\/instrumental, happy,longest songs etc.","7baeec57":"Normalizing the data","f44c60c6":"Checking for Duplicate values","6ef4b094":"Most of the songs have mode : major ","d6819aab":"# Clustering","9dfbd50c":"#### Task at hand: \nOur objective her would be to clean the data, perform feature engineering, Explore the data and then get valuable insights from it. After this, we try to cluster the songs according to their numerical attributes so that we can Categorize them into different categories, for eg. Live songs, instrumental songs, danceable songs etc. This is esentially the genre prediction of songs. We perform dimensionality reduction for clustering. I have Used Principal Component Analysis(PCA) for the same","f855453d":"Popularity throughout the decades : is popularity affected by type of content(explicit\/implicit) ?","25df6e56":"To check if popularity is affected by mode","6ba10635":"Now, we have clean data and we start with data exploration","56bce95c":"The silhouette score is greater than 0.5. Its away from 0 which indicates that the clustering we have performed is fairly good and there are not significant overlaps between clusters. ","310edca8":"We can see that these duplicatre values might arise due to release of Album versions, etc. The ID is different for both the songs. We remove such duplicate values.","6786aae4":"There is a decrease of wcss at k = 3 . I Choose k as 3.","ff53910c":"### KMeans Clustering","a5c14fb2":"Mode: Major(1) \/ Minor(0)","52c5903d":"Explicit content(1) or 'no explicit content'(0)","94ec7db4":"A combination of mode 0 and key 1 is more likely to get ur song popular.","c00eb7ab":"### Heirarchichal CLustering","4856da17":"Choosing the optimum number of clusters","318a9b4f":"# **Spotify Analysis and Clustering**\n\n## About the data set:\n\nThe data file contains more than 170.000 songs collected from Spotify Web API, and also you can find data grouped by artist, year, or genre in the data section.\n\nPrimary:\n- id (Id of track generated by Spotify)\n\nNumerical:\n- acousticness (Ranges from 0 to 1)\n- danceability (Ranges from 0 to 1)\n- energy (Ranges from 0 to 1)\n- duration_ms (Integer typically ranging from 200k to 300k)\n- instrumentalness (Ranges from 0 to 1)\n- valence (Ranges from 0 to 1)- happiness index\n- popularity (Ranges from 0 to 100)- based on the number of times the song was played on spotify.\n- tempo (Float typically ranging from 50 to 150)\n- liveness (Ranges from 0 to 1)-indicates whether the song was played live or not\n- loudness (Float typically ranging from -60 to 0)\n- speechiness (Ranges from 0 to 1)\n- year (Ranges from 1921 to 2020)\n\nDummy:\n- mode (0 = Minor, 1 = Major)\n- explicit (0 = No explicit content, 1 = Explicit content)\n\nCategorical:\n- key (All keys on octave encoded as values ranging from 0 to 11, starting on C as 0, C# as 1 and so on\u2026)\n- artists (List of artists mentioned)\n- release_date (Date of release mostly in yyyy-mm-dd format, however precision of date may vary)\n- name (Name of the song)","225efe39":"#### I have taken a random sample of 10000 points from our dataset . Then, I have performed heirarchichal clustering","fd65f327":"#### Widgets(A dropdown) for popular songs of every year\nUsing these widgets, it is possible to choose a year from the list of years 1920-2020 and find the most popular song of that year","2587ac0c":"Choosing k=3 for Heirarchichal CLustering","23e9d0b0":"# EDA","17b2455b":"#### Dimensionality Reduction","0efc7cac":"Also, it is important to note that popularity increased over years because in this dataset, it is a metric which is measured on the basis of how many times the song is played. Due to this, older songs have less populariy.","b7878c71":"We can see that we have a fair distribution of data from alll decades except for the decades 1921-31 and 1931-41. we have less songs from theses decades in the data\n","9154162e":"There was very less explicit content before 1960s. Songs mostly started having explicit contents since 1970s. Songs having explicit content were in general, less popular than those that did not have explicit content. In the recent decade, we can see that songs having explicit content are more popular than those that do not have explicit content. This also shows the change of culture through the decade.","3707e3a4":"Creating a categorical decade column for the purpose of EDA","e7d5df6c":"The silhouette score is greater than 0.5. Its away from 0 which indicates that the clustering we have performed is fairly good and there are not significant overlaps between clusters. ","1d32da16":"Let us check distribution of songs throughout the year i.e how many songs come from various years","a8dc59b2":"#### Some Feature Engineering","57f95ada":"Using numerical columns for clustering","41eb0dc9":"#### Dataset Limitations:  \nThere are only nearly 2000 songs from each year. The popularity metric(range 0-100) is based on the number of times the song was played on spotify, Thus, it is naturally lower for songs in the older decades and higher for songs in the present decade","85e4ad36":"Above, we can see these are the top 5 longest songs in the dataset"}}