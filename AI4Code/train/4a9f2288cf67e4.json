{"cell_type":{"1fe4ed12":"code","9a8c19b8":"code","7ba1a2c3":"code","fd782984":"code","6a81db31":"code","8cf07111":"code","5b2b3d73":"code","7c855729":"code","a1c5e353":"code","b3b528fe":"code","bedde1f8":"code","0937c055":"code","e34e3caa":"code","a69688a8":"code","b0bf045f":"code","ba1a20fd":"code","ffd36ae8":"code","03848e49":"markdown","18b7dafa":"markdown","38aa8778":"markdown","b5fd8465":"markdown","b5a5e544":"markdown","7994aeb2":"markdown","57884ef0":"markdown","519c8f9d":"markdown","a6359a23":"markdown","3a20dbc9":"markdown","313264f9":"markdown"},"source":{"1fe4ed12":"#load the training dataset\nimport pandas as pd\n!wget https:\/\/raw.githubusercontent.com\/MicrosoftDocs\/mslearn-introduction-to-machine-learning\/main\/Data\/ml-basics\/diabetes.csv\ndiabetes = pd.read_csv('diabetes.csv')\ndiabetes.head()","9a8c19b8":"# Separate features and labels\nfeatures = ['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']\nlabel = 'Diabetic'\nX, y = diabetes[features].values, diabetes[label].values\n\nfor n in range(0,4):\n    print(\"Patient\",str(n+1),\"\\n Features: \", list(X[n]),\"\\n Label:\", y[n])\n","7ba1a2c3":"#Now let's compare the feature distributions for each label value.\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nfeatures = ['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']\n\nfor col in features:\n    diabetes.boxplot(column = col, by = 'Diabetic', figsize = (6,6))\n    plt.title(col)\nplt.show()","fd782984":"from sklearn.model_selection import train_test_split\n\n#Split the data into 70%-30% training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\nprint(\"Training cases: %d\\nTest cases: %d\" % (X_train.shape[0], X_test.shape[0]))","6a81db31":"#Import the model\nfrom sklearn.linear_model import LogisticRegression\n\n# Set regulation rate\nreg = 0.01\n\n#Train a Logistic regression model on the training set\nmodel = LogisticRegression(C = 1\/reg, solver = 'liblinear').fit(X_train, y_train)\nprint(model)","8cf07111":"#Predictions\npredictions = model.predict(X_test)\nprint('Predected labels: ', predictions)\nprint('Actual labels: ', y_test)","5b2b3d73":"from sklearn.metrics import accuracy_score\nprint('Accuracy: ', accuracy_score(y_test, predictions))","7c855729":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, predictions))","a1c5e353":"#Confusion matrix\nfrom sklearn.metrics import confusion_matrix\n#print the confusion matrix\ncm = confusion_matrix(y_test, predictions)\nprint(cm)","b3b528fe":"#Predictions with probabilities\ny_scores = model.predict_proba(X_test)\nprint(y_scores)","bedde1f8":"#Calculate and plot the ROC\nfrom sklearn.metrics import roc_curve\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#calculate the ROC\nfpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n\n#Plot the ROC curve\nfig = plt.figure(figsize = (6,6))\nplt.plot([0,1],[0,1],'k--') #Plot the diagonal 50% line\nplt.plot(fpr,tpr) #plot the FPR and TPR achieved by the model\nplt.xlabel('False Positive Rate')\nplt.ylabel('True positive Rate')\nplt.title('ROC Curve')\nplt.show()\n\n\n","0937c055":"# Calculate the area under the curve (AUC)\nfrom sklearn.metrics import roc_auc_score\n\nauc = roc_auc_score(y_test, y_scores[:,1])\n\nprint('AUC: '+ str(auc))","e34e3caa":"#Import the necessary libryries\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n\n# Define preprocessing for numeric columns\nnumeric_features = [0,1,2,3,4,5,6]\nnumeric_transformer = Pipeline(steps = [\n    ('scaler', StandardScaler())])\n\n# Define preprocessing for categorical features\ncategorical_features = [7]\ncategorical_transformer = Pipeline(steps = [\n    ('onehot', OneHotEncoder(handle_unknown = 'ignore'))])\n\n# Combine preprocessing steps\n\npreprocessor = ColumnTransformer(transformers = [\n    ('num',numeric_transformer, numeric_features),\n    ('cat', categorical_transformer, categorical_features)])\n\n# Create preprocessing and training pipeline\npipeline = Pipeline(steps = [\n    ('preprocessor', preprocessor),\n    ('logregressor', LogisticRegression(C=1\/reg, solver = 'liblinear'))])\n\n# Fit the pipeline to train the LogisticRegression on the training set\nmodel = pipeline.fit(X_train, y_train)\nprint(model)","a69688a8":"# get predictions from test data\npredictions = model.predict(X_test)\ny_scores = model.predict_proba(X_test)\n\n# get evalation metrics\nfrom sklearn.metrics import precision_score, recall_score\ncm =  confusion_matrix(y_test, predictions)\nprint('Confusion matrix:\\n', cm, '\\n')\nprint('Accuracy', accuracy_score(y_test, predictions))\nprint('Overall Precision:', precision_score(y_test, predictions))\nprint('Overall Recall:', recall_score(y_test, predictions))\n\nauc = roc_auc_score(y_test, y_scores[:,1])\nprint('AUC: '+ str(auc))\n\n# Calculate the ROC curve\n\nfpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n\n# plot ROC curve\nfig = plt.figure(figsize =(6,6))\n\n# plot the diagonal 50% line\nplt.plot([0,1], [0,1], 'k--')\n\n# plot the FPR and TPR achived by our model\n\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()","b0bf045f":"from sklearn.ensemble import RandomForestClassifier\n\n#create preprocesing and training pipeline\npipeline = Pipeline(steps = [\n    ('preprocessor', preprocessor),\n    ('logregressor', RandomForestClassifier(n_estimators = 100))])\n# fit the pipeline to train a Raldom Forest model on the training set\n\nmodel = pipeline.fit(X_train, y_train)\nprint(model)\n","ba1a20fd":"# Evaluate the performance of the new model\n\npredictions = model.predict(X_test)\ny_scores = model.predict_proba(X_test)\ncm =  confusion_matrix(y_test, predictions)\nprint(\"Confusion Matrix:\\n\", cm,\"\\n\")\nprint(\"Accuracy: \", accuracy_score(y_test, predictions))\nprint(\"Overall precision: \", precision_score(y_test, predictions))\nprint(\"Overall recall: \", recall_score(y_test, predictions))\nauc = roc_auc_score(y_test, y_scores[:,1])\nprint(\"\\nAUC: \"+ str(auc))\n\n# Calculate the ROC curve\n\nfpr, tpr, threshold = roc_curve(y_test, y_scores[:,1])\n\n# Plot ROC curve\nfig = plt.figure(figsize= (6,6))\nplt.plot([0,1],[0,1], 'k--') # plot the diagonal 50% line\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()","ffd36ae8":"import joblib\n\n# Save the model as pickle file\nfilename = '.\/diabetes_model.pkl'\n\njoblib.dump(model, filename)","03848e49":"## Import the data","18b7dafa":"## EDA","38aa8778":"## Use the model for Inferencing","b5fd8465":"<h1>Binary Classification<\/h1>\n\n<p>In this notebook, we will focus on an example of binary classification, where the model must predict a label that belongs to one of two classes. In this exercise, we'll train a binary classifier to predict whether or not a patient should be tested for diabetes based on some medical data.<\/p>","b5a5e544":"## Evaluate the trained model ","7994aeb2":"## Try an other algorithm","57884ef0":"## Split the data","519c8f9d":"In this case, the ROC curve and its AUC indicate that the model performs better than a random guess which is not bad considering we performed very little preprocessing of the data.\n\nIn practice, it's common to perform some preprocessing of the data to make it easier for the algorithm to fit a model to it. There's a huge range of preprocessing transformations you can perform to get your data ready for modeling, but we'll limit ourselves to a few common techniques:\n\n- Scaling numeric features so they're on the same scale. This prevents features with large values from producing coefficients that disproportionately affect the predictions.\n- Encoding categorical variables. For example, by using a *one hot encoding* technique you can create individual binary (true\/false) features for each possible category value.\n","a6359a23":"## Train a LogisticRegression Model","3a20dbc9":"The model looks better!","313264f9":"## Perform preprocessing in a pipeline"}}