{"cell_type":{"1a2c4fb2":"code","ea0c5e43":"code","cca05c61":"code","fc34c90b":"code","9da13d14":"code","e6e73db7":"code","97f45bd6":"code","9b970b24":"code","88c61db4":"code","398cafcd":"code","d96e0de1":"code","4707c801":"code","73a80f26":"markdown","df1bccc2":"markdown","b6f0123e":"markdown","9b4a296f":"markdown","c3057300":"markdown","90ed9519":"markdown","d2bd324a":"markdown"},"source":{"1a2c4fb2":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' #suppressing tensorflow warnings\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nimport matplotlib.pyplot as plt   #plotting learning curves for DNN\nplt.style.use('seaborn-whitegrid')\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='medium',\n       titleweight='bold', titlesize=14, titlepad=10)\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nSEED = 23","ea0c5e43":"%%time\ntrain = pd.read_csv('\/kaggle\/input\/tabular-playground-series-oct-2021\/train.csv', \n                    index_col='id')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-oct-2021\/test.csv', \n                   index_col='id')","cca05c61":"train.head(3)","fc34c90b":"print(train.info())\nprint(f'Train data dimensions: {train.shape}')\nprint(f'Test data dimensions: {test.shape}\\n')\nprint(f'Missing values: \\\n      Train data - {train.isna().sum().sum()}, \\\n      Test data - {test.isna().sum().sum()}')","9da13d14":"#separating out the target variable before modifying the training data\ntarget = train.pop('target')","e6e73db7":"target.value_counts()","97f45bd6":"%%time\n\nnumerical_features = list(train.select_dtypes(include=['float64']).columns)\ncategorical_features = list(train.select_dtypes(include=['int64']).columns)\n\nnumerical_transformer = make_pipeline(StandardScaler(),\n                                      MinMaxScaler())\n\ncategorical_transformer= make_pipeline(OneHotEncoder(handle_unknown='ignore'))\n\npreprocessor = make_column_transformer(\n    (numerical_transformer, numerical_features),\n    (categorical_transformer, categorical_features),\n)\n\nX_train, X_val, y_train, y_val = train_test_split(train, target,\n                                                 train_size=0.67, test_size=0.33,\n                                                 stratify=target, random_state=SEED)\n\nX_train = preprocessor.fit_transform(X_train)\nX_val = preprocessor.transform(X_val)\nX_test = preprocessor.transform(test)","9b970b24":"input_shape = [X_train.shape[1]]\nPATIENCE = 5\nMIN_DELTA = 0.0005\n\nmodel = keras.Sequential([\n    layers.BatchNormalization(input_shape=input_shape),\n    layers.Dense(units=128, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.5),\n    layers.Dense(units=128, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.5),\n    layers.Dense(units=1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', \n              loss='binary_crossentropy', \n              metrics=['binary_accuracy', 'AUC'])\n\nearly_stopping = keras.callbacks.EarlyStopping(\n    patience=PATIENCE,\n    min_delta=MIN_DELTA,\n    restore_best_weights=True,\n)","88c61db4":"model.summary()","398cafcd":"%%time\n\nBATCH_SIZE = 128\nEPOCHS = 200\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks=[early_stopping],\n)\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\nhistory_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot(title=\"Accuracy\")\nhistory_df.loc[:, ['auc', 'val_auc']].plot(title=\"AUC\")","d96e0de1":"preds_test = model.predict(X_test).ravel()","4707c801":"output = pd.DataFrame({'id': test.index, 'target': preds_test})\noutput.to_csv('submission.csv', index=False)\nprint('Time to submit!')","73a80f26":"**Training the model**  \n","df1bccc2":"The dataset is large (train - 1M rows,ntest - 0.5M rows). The first time I tried to load it, I panicked when it kept running for over 2 minutes.  \nAfter reading up on handling large datasets on Kaggle, I followed this approach:  \n1. Loaded 1% of the data for initial exploration and setting up data preparation pipeline.  \n2. Loaded 10% of the data to check if the model is actually working (useful for beginners like me)  \n3. Loaded the full dataset for the final run before submission.  \n\nThis can be done by using the 'nrows' parameter of the read_csv method: nrows = 10000 and 100000 for the first two steps. Not needed for loading the full dataset.\n\nThe full dataset also loads in a reasonable amount of time. For even larger datasets where it might not be possible, [this tutorial](https:\/\/www.kaggle.com\/rohanrao\/tutorial-on-reading-large-datasets) may help.","b6f0123e":"**Building the network architecture**  \nExperimented with number of hidden layers and number of units in layers. Increasing hidden layers upto 4 resulted in minor gains in performance.","9b4a296f":"I recently completed the [Intro to Deep Learning](https:\/\/www.kaggle.com\/learn\/intro-to-deep-learning) course on Kaggle and decided to use those concepts to participate in **my very first competition!** (the first non-tutorial one anyways)  \nCompared to my [first notebook](https:\/\/www.kaggle.com\/stiwar1\/a-disastrous-start-to-ml), I managed to make the code a bit cleaner. Of course, still a lot of best practices to be incorporated. **Advice is always welcome!**","c3057300":"* From the data description, we know there are only two types of features - continous and binary-categorical. **240** float64 continous columns and **45** int64 categorical columns.\n* No missing values. Will not need imputation in our preprocessing pipeline.","90ed9519":"**Data Preparation**  \n* Numerical (continous) data -> standardized and normalized\n* Categorical data -> one-hot encoded","d2bd324a":"Almost perfectly balanced. Will not need to do any resampling."}}