{"cell_type":{"caddbd47":"code","977f4a0a":"code","2f352734":"code","80da25d8":"code","90f6cfce":"code","88593fa5":"code","533cc98d":"code","d470ea3a":"code","a5b494f4":"code","14470902":"code","83eb27b0":"code","476fe087":"code","bc1cfb21":"code","fb78623e":"code","faff2aad":"code","ada2bb11":"code","33dfb971":"code","b0def5ae":"code","08914fb6":"code","6818cfbc":"code","1e793f55":"code","f7a937b5":"code","c9a1b187":"code","5763a482":"code","333bf412":"code","0e9bf34f":"code","38cff480":"code","09ae3189":"markdown","0a114b71":"markdown","9d6815f5":"markdown","b82d9edb":"markdown","2d7da59e":"markdown","3191163f":"markdown","d10803ab":"markdown","b7d7c778":"markdown","7aa3fadc":"markdown","fc1e1e56":"markdown","0570de94":"markdown","9425077f":"markdown","f30d1438":"markdown","373ba821":"markdown","9254023e":"markdown","eea6695d":"markdown","318feff4":"markdown","a386269f":"markdown","184ed85c":"markdown","52abb6d9":"markdown","90ea227c":"markdown","97908230":"markdown","eebecb5b":"markdown","acd84bb5":"markdown","4fcdb215":"markdown","23953dda":"markdown","f705ae57":"markdown","468f7acb":"markdown","1aa0db6c":"markdown","a0829921":"markdown"},"source":{"caddbd47":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nnp.random.seed(0)\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom numpy import mean\nfrom numpy import std\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom sklearn.neighbors import KNeighborsClassifier\nimport random as rn\nrn.seed(0)\n\n#colors for plot\ncolors = [\"red\", \"blue\", \"orange\", \"pink\"]\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","977f4a0a":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()\n\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","2f352734":"print(\"Train data missing value count for each feature\")\nprint(train_data.isnull().sum())\n\nprint(\"Test data missing value count for each feature\")\nprint(test_data.isnull().sum())","80da25d8":"missing_value_fill_method = \"2.3\"","90f6cfce":"if missing_value_fill_method == \"2.1\":\n    train_data.dropna(inplace=True)\n    test_data.dropna(inplace=True)\n    print(train_data.shape)\n    print(test_data.shape)","88593fa5":"if missing_value_fill_method == \"2.2\":\n    train_data.fillna(train_data.mean(), inplace=True)\n    print(train_data.isnull().sum())","533cc98d":"def plot_age_histogram_according_to_pclass(df, compared_feature):\n    unique_pclass_arr = df[compared_feature].unique()\n    unique_pclass_arr = np.sort(unique_pclass_arr)[::-1]\n\n    for i in range(len(unique_pclass_arr)):\n        plt.hist(df.loc[df[compared_feature] == unique_pclass_arr[i], \"Age\"], bins=10, color=colors[i],\n                 label=str(unique_pclass_arr[i]))\n        plt.legend(loc='upper right')\n        plt.show()\n\nplot_age_histogram_according_to_pclass(train_data, \"Pclass\");\nplot_age_histogram_according_to_pclass(train_data, \"Sex\");","d470ea3a":"def plot_plass_age_grouped_age_mean(df):\n    grouped_data = df.groupby([\"Pclass\", \"Sex\"])[\"Age\"].mean()\n    grouped_data.plot(kind=\"bar\")\n    plt.xlabel(\"Pclass grouped by Age Value\")\n    plt.ylabel(\"Average Age Value\")\n    plt.xticks(rotation=0)\n    plt.figure(figsize=(8, 5))\n    plt.show()\n    \nplot_plass_age_grouped_age_mean(train_data)","a5b494f4":"if missing_value_fill_method == \"2.3\":\n    \n    # Age - Replace missing value with the mean of each group\n    train_data[\"Age\"] = train_data.groupby([\"Pclass\", \"Sex\"])[\"Age\"].apply(lambda x: x.fillna(x.mean()))\n    test_data[\"Age\"] = test_data.groupby([\"Pclass\", \"Sex\"])[\"Age\"].apply(lambda x: x.fillna(x.mean()))\n\n    ","14470902":"print(train_data.Cabin.unique())\nprint(test_data.Cabin.unique())","83eb27b0":"if missing_value_fill_method == \"2.3\":\n    train_data[\"Cabin\"] = train_data[\"Cabin\"].apply(lambda s: s[0] if pd.notnull(s) and s[0]!= \"T\" else \"N\")\n    test_data[\"Cabin\"] = test_data[\"Cabin\"].apply(lambda s: s[0] if pd.notnull(s) and s[0]!= \"T\" else \"N\")","476fe087":"if missing_value_fill_method == \"2.3\":\n    print(train_data.groupby([\"Pclass\"])[\"Cabin\"].value_counts())","bc1cfb21":"if missing_value_fill_method == \"2.3\":\n    train_data.loc[ (train_data[\"Pclass\"] == 1) & (train_data[\"Cabin\"] == \"N\"), \"Cabin\"]= \"C\"\n    train_data.loc[ ((train_data[\"Pclass\"] == 2) | (train_data[\"Pclass\"] == 3)) & (train_data[\"Cabin\"] == \"N\"), \"Cabin\"]= \"F\"\n\n    test_data.loc[ (test_data[\"Pclass\"] == 1) & (test_data[\"Cabin\"] == \"N\"), \"Cabin\"]= \"C\"\n    test_data.loc[ ((test_data[\"Pclass\"] == 2) | (test_data[\"Pclass\"] == 3)) & (test_data[\"Cabin\"] == \"N\"), \"Cabin\"]= \"F\"","fb78623e":"if missing_value_fill_method == \"2.3\":\n    print(test_data.loc[test_data[\"Fare\"].isnull()])","faff2aad":"if missing_value_fill_method == \"2.3\":\n    sample_fare = test_data.loc[(test_data[\"Pclass\"] == 3) & (test_data[\"SibSp\"] == 0) & (test_data[\"Embarked\"] == \"S\")][\n        \"Fare\"].mean()\n    print(sample_fare)\n    test_data.loc[test_data[\"Fare\"].isnull(), \"Fare\"] = sample_fare","ada2bb11":"if missing_value_fill_method == \"2.3\":\n    print(train_data.loc[train_data[\"Embarked\"].isnull()])\n    print(test_data.loc[train_data[\"Embarked\"].isnull()])","33dfb971":"if missing_value_fill_method == \"2.3\":\n    # Embarked - filtered same data and found that S value has more.\n    print(train_data.loc[train_data[\"Embarked\"].isnull()].head())\n    print(train_data.loc[(train_data[\"Pclass\"] == 1) & (train_data[\"Fare\"] <= 80.0) & (train_data['Cabin'] == \"B\")][\n              \"Embarked\"].value_counts())\n\n    train_data.loc[train_data[\"Embarked\"].isnull(), \"Embarked\"] = \"S\"","b0def5ae":"print(train_data.isnull().sum())\nprint(test_data.isnull().sum())","08914fb6":"# BINNING\n\n# Age binning\nser_age, bin_age = pd.cut(train_data[\"Age\"].astype(int), 5, retbins=True, labels=False)\ntest_data[\"Age\"] = pd.cut(test_data[\"Age\"], bins=bin_age, labels=False, include_lowest=True)\ntrain_data[\"Age\"] = ser_age\ntrain_data[\"Age\"] = train_data[\"Age\"].astype(\"category\")\ntest_data[\"Age\"] = test_data[\"Age\"].astype(\"category\")\n\n# Fare Binning\nser, bins = pd.qcut(train_data[\"Fare\"], 5, retbins=True, labels=False)\ntrain_data[\"Fare\"] = ser\nser_tst = pd.cut(test_data[\"Fare\"], bins=bins, labels=False, include_lowest=True)\ntest_data[\"Fare\"] = ser_tst\ntrain_data[\"Fare\"] = train_data[\"Fare\"].astype(\"category\")\ntest_data[\"Fare\"] = test_data[\"Fare\"].astype(\"category\")","6818cfbc":"# ADD NEW FEATURES\n\n# Family size\ntrain_data[\"Family_Size\"] = train_data[\"SibSp\"] + train_data[\"Parch\"]\ntest_data[\"Family_Size\"] = test_data[\"SibSp\"] + test_data[\"Parch\"]\n\n# Survival rate according to family size\nfamily_size_surv_mean = train_data.groupby(by=\"Family_Size\")[\"Survived\"].mean()\nprint(family_size_surv_mean)","1e793f55":"# Ticket frequency, same ticket id passengers travelled together\ntrain_data[\"Travelled_Together\"] = train_data.groupby(by='Ticket')['Ticket'].transform('count')\ntest_data[\"Travelled_Together\"] = test_data.groupby(by='Ticket')['Ticket'].transform('count')","f7a937b5":"# Title\ntrain_data[\"Title\"] = train_data[\"Name\"].str.split(\",\", expand=True)[1].str.split(\".\", expand=True)[0]\ntest_data[\"Title\"] = test_data[\"Name\"].str.split(\",\", expand=True)[1].str.split(\".\", expand=True)[0]\n\ntitle_counts = train_data[\"Title\"].value_counts() < 10\ntrain_data[\"Title\"] = train_data[\"Title\"].apply(lambda x: \"Gen\" if title_counts.loc[x] else x)\n\ntitle_counts_tst = test_data[\"Title\"].value_counts() < 10\ntest_data[\"Title\"] = test_data[\"Title\"].apply(lambda x: \"Gen\" if title_counts_tst.loc[x] else x)","c9a1b187":"# Marry Status\ntrain_data[\"Marry_Status\"] = 0\ntrain_data[\"Marry_Status\"].loc[train_data[\"Title\"].str.strip() == \"Mrs\"] = 1\ntrain_data[\"Marry_Status\"] = train_data[\"Marry_Status\"].astype(\"category\")\n\ntest_data[\"Marry_Status\"] = 0\ntest_data[\"Marry_Status\"].loc[test_data[\"Title\"].str.strip() == \"Mrs\"] = 1\ntest_data[\"Marry_Status\"] = test_data[\"Marry_Status\"].astype(\"category\")","5763a482":"#change Pclass to categorical\ntrain_data[\"Pclass\"] = train_data[\"Pclass\"].astype(\"category\")\ntest_data[\"Pclass\"] = test_data[\"Pclass\"].astype(\"category\")                                                                         \n                                                                                                                                                                                                                                                                                                                          \ny = train_data[\"Survived\"]","333bf412":"features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Cabin\", \"Embarked\", \"Family_Size\",\n            \"Travelled_Together\", \"Title\", \"Marry_Status\"]\n\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])","0e9bf34f":"chosed_model = \"rf\"\n# baseline model\nif chosed_model == \"dp\":\n    model = Sequential()\n    model.add(Dense(10, input_dim=36, activation='tanh'))\n    model.add(Dense(1, activation='sigmoid'))\n    # Compile model\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    model.fit(X, y, epochs=100, batch_size=5, verbose=0)\n\n    predictions = model.predict_classes(X_test)\n    output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions.flatten()})\n    \nif (chosed_model==\"rf\") | (chosed_model == \"xboost\"):\n    model = RandomForestClassifier(n_estimators=200, max_depth=7, random_state=1)\n    model.fit(X, y)\n    predictions = model.predict(X_test)\n\n    output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n    output.to_csv('\/kaggle\/working\/my_submission_rf.csv', index=False)\n\nif (chosed_model == \"xboost\") | (chosed_model ==\"rf\"):\n    gbm = xgb.XGBClassifier(\n        n_estimators=2000,\n        max_depth=4,\n        min_child_weight=2,\n        gamma=0.9,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        objective='binary:logistic',\n        nthread=-1,\n        scale_pos_weight=1).fit(X, y)\n    xgb_predictions = gbm.predict(X_test)\n    output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n    output.to_csv('\/kaggle\/working\/my_submission_xboost.csv', index=False)\nif (chosed_model == 'knn'):\n    knn = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n           metric_params=None, n_jobs=1, n_neighbors=18, p=2,\n           weights='distance')\n    knn.fit(X, y)\n    knn_predictions = knn.predict(X_test)\n    output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': knn_predictions})\n    output.to_csv('my_submission.csv', index=False)\n    print(\"Your submission was successfully saved!\")\n\n","38cff480":"if (chosed_model==\"rf\") | (chosed_model == \"xboost\"):\n    #Ensemble Model:\n    df_xboost = pd.read_csv('\/kaggle\/working\/my_submission_xboost.csv')\n    df_rf = pd.read_csv('\/kaggle\/working\/my_submission_rf.csv')\n\n    df_ensemble = df_xboost.copy()\n    k = 'Survived'\n    df_ensemble[k] = 0.5 * df_xboost[k] + 0.7 * df_rf[k]\n    df_ensemble[k] = df_ensemble[k].apply(lambda f: 1 if f>=0.5 else 0)\n\n    df_ensemble.to_csv('my_submission.csv', index=False)\n    print(\"Your submission was successfully saved!\")","09ae3189":"* Marry Status\n\n> Whether people are married or single can affect the rate of survival. We can tell from the Mrs in the title whether the passenger is married or not.","0a114b71":"We try 3 different missing value filling technic, ***missing_value_fill_method*** variable is used for switch filling missing value method.\n\n* Delete rows containing missing data from the data set (missing_value_fill_method = \"2.1\")\n* Filling all numeric missing data with the same statistical method (missing_value_fill_method = \"2.2\")\n* Filling missing data with statistical method specific to sample data feature (missing_value_fill_method = \"2.3\")","9d6815f5":"Accordingly, it would be better to fill in the lost data in the age feature by grouping them according to passenger class and gender.","b82d9edb":"# **1. Data Preparation**","2d7da59e":"![titanic.jpg](attachment:titanic.jpg)","3191163f":"***Age Feature Missing Values***\n\nAge value interval can be change according to ticket class and sex of passenger. Let look at Age value according to Pclass feature value using histogram. We can see all of three Pclass value have different Age distribution.","d10803ab":"\nPclass feature turned category from int.\n\nClass feature Survived assigned to y data.\n","b7d7c778":"**CONTENTS**\n\n1. Data Preparation\n2. Check Data Set for Missing Values\n\n    2.1. Delete rows containing missing data from the data set\n    \n    2.2. Filling all numeric missing data with the same statistical method\n    \n    2.3. Filling missing data with statistical method specific to sample data feature\n    \n3. Binning\n4. Exploring New Features\n5. One Hot Encoding\n6. Prediction\n7. Ensemble Evaluation","7aa3fadc":"# Ensemble Modeling\n\nIn other studies, it was stated that evaluating more than one model together increased the success. I wanted to try :)\n\nBecause the accuracy of predicting xboost and random forest is almost the same, I couldn't get a better result with the ensemble.","fc1e1e56":"**Read Train and Test Data Set**","0570de94":"# 2.2. Filling all numeric missing data with the same statistical method\nAll numerical properties containing missing data are filled with the same statistical method. The mean method is given as an example. With this method, we do not cover categorical missing data like Cabin and Embarked.","9425077f":"# 4. Exploring New Features\n\n* Family Size\n\n> Family size is can be calculated by sum of siblings and parents. Large families may be more likely to survive","f30d1438":"* Travelled Together\n\n> As the number of people traveling together increases, the probability of survival may increase. We can find people traveling together from the ticket number.","373ba821":"# 5. One Hot Encoding\n\nConvert categorical variable into dummy\/indicator variables using pandas.get_dummies().","9254023e":" I filled in the missing Fare value by averaging the data with the same ticket class, the same SibSp count, and the same Embarked value.","eea6695d":"**Fare Feature Missing Values**\n\nThere is one missing Fare value in test dataset.\n","318feff4":"# **2. Check Data Set for Missing Values**","a386269f":"The average age of people in the same ticket class is different according to their gender. In 1911, the difference in age between married men and women may be large.\n\nLet see mean Age value according to Pclass distribution grouped by Sex. \n\nThe average age of 1st and 2nd class passengers seems to be higher than 3rd class passengers. Women are also generally younger than men.","184ed85c":"**Cabin Feature Missing Values**\n\nFirst letter of Cabin feature is can be A, B, C, D, E, F, G, T. I've updated the Cabin feature to use only the first letter. I assigned the letter N to the lost Cabin values.\n\nThere is one value which start with letter T. This T valued cabin belongs to Blackwell, Mr. Stephen Weart. T desk is in Boat Desk in Titanic. \n\nSince there is no T-valued data in the test dataset, I also considered this data as missing.","52abb6d9":"# 3. Binning\n\nData binning is performed for numeric data, outliers are evaluated in a certain group and the data set is can be more smooth.\n\n\nThere are two method for binning\n\n* Equal Frequency Binning : bins have equal frequency\n* Equal Width Binning : bins have equal width","90ea227c":"**Embarked Feature Missing Values**\n\nIn the Embarked feature, there are missing two missing data. In the same ticket class, in a similar price range, in similar cabin; the most common Embarked value S was used for filling missing values.","97908230":"Let see missing value count in train and test datasets.\n","eebecb5b":"# 6. Prediction\n\nPrediction will be made using Random Forest and basic Deep Learning method models.\n\nUsing chosed_model variable we can choose prediction method.\n\n* dp for deep learning\n* rf for random forest","acd84bb5":"According to value counts;\n* Pclass 1 mostly in Cabin C,\n* Pclass 2 and 3 mostly in Cabin F.\n\nI updated the missing data based on this information.","4fcdb215":"For the missing \"N\" values in the Cabin property, I used the most common Cabin letter found in passengers in similar ticket class.","23953dda":"# 2.3. Filling missing data with statistical method specific to sample data feature","f705ae57":"# Titanic Predict Survival Using Random Forest and Deep Learning","468f7acb":"# 2.1. Delete rows containing missing data from the data set\nDeleting rows containing lost data from the data set is not a recommended method, as it causes data loss. In addition, it is not suitable for the competition as the number of rows in the test data set will decrease. Still, let's try the implementation of the method with python.\n","1aa0db6c":"We fill all missing values :)","a0829921":"* Title\n\n> We can obtain the title of the persons from the name feature. \"Gen\" expression (as General) is used for titles with a frequency less than 10.\n\nTitle example: Fortune, **Miss**. Alice Elizabeth"}}