{"cell_type":{"7b09f5c5":"code","9a1aa5f6":"code","d2b1cbd1":"code","bf185134":"code","9b44aaa5":"code","182bc4ce":"code","2bc2b722":"code","859fb3ee":"code","213f80df":"code","5a57e51d":"code","740fd9de":"code","958ea5e0":"code","29e7b7bf":"code","d93cec6c":"code","20ae7311":"code","f2544dbf":"code","a77eb3b1":"code","da812481":"code","11c021c3":"code","d99b0723":"code","36226e66":"code","c695da1f":"code","7eeb5555":"code","0176d79c":"code","7b4f0adb":"code","ffce72dd":"code","680bbeaf":"code","0287da89":"code","ecdc19b9":"markdown","dc670cb6":"markdown","3cf754de":"markdown","11294954":"markdown","b0ebf06c":"markdown","95f94623":"markdown","8f9b7a06":"markdown","f770ee7e":"markdown","6999818b":"markdown","788ddef2":"markdown","97625cee":"markdown","26123848":"markdown","aba231da":"markdown","33380d5a":"markdown","34900177":"markdown","d88beea9":"markdown","ad69e128":"markdown","c8631e35":"markdown","2b7e4141":"markdown","22b43376":"markdown","91351a43":"markdown","b88059eb":"markdown","80e2517e":"markdown","d6568886":"markdown","26a7bc03":"markdown","33d085b2":"markdown","f687ea57":"markdown","805d5bcb":"markdown"},"source":{"7b09f5c5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn import preprocessing \nfrom collections import Counter\nfrom imblearn.under_sampling import RandomUnderSampler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc,recall_score,precision_score\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import cross_validate \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\npd.set_option(\"display.max_columns\", 111)\nsns.set_palette(\"Set2\")","9a1aa5f6":"df = pd.read_excel(\"\/kaggle\/input\/covid19\/dataset.xlsx\")\ndf.head()","d2b1cbd1":"df.shape","bf185134":"df.isna().sum()","9b44aaa5":"df.columns.tolist()","182bc4ce":"df['Patient ID'].nunique()","2bc2b722":"df.drop(columns='Patient ID', inplace=True)","859fb3ee":"print('Negative: {} ({}%)'.format(df['SARS-Cov-2 exam result'].value_counts()[0], round(df['SARS-Cov-2 exam result'].value_counts()[0]\/len(df)*100, 2)))\nprint('Positive: {} ({}%)'.format(df['SARS-Cov-2 exam result'].value_counts()[1], round(df['SARS-Cov-2 exam result'].value_counts()[1]\/len(df)*100, 2)))\nsns.countplot('SARS-Cov-2 exam result',data=df)","213f80df":"df[['Patient addmited to regular ward (1=yes, 0=no)', 'Patient addmited to semi-intensive unit (1=yes, 0=no)', 'Patient addmited to intensive care unit (1=yes, 0=no)']].isna().sum()","5a57e51d":"df['Patient age quantile'].describe()","740fd9de":"df['SARS-Cov-2 exam result'] = df['SARS-Cov-2 exam result'].map({'positive': 1, 'negative': 0})","958ea5e0":"drop_index = []\nfor i in range(df.shape[1]):\n    if df.iloc[:,i].isna().sum() == len(df):\n        drop_index.append(df.iloc[:,i].name)\n        \nfor j in drop_index:\n    df = df.drop([j],axis=1)","29e7b7bf":"df = df.dropna(thresh=0.20*len(df), axis=1)","d93cec6c":"df = df.dropna(axis=0)","20ae7311":"df.shape","f2544dbf":"df.dtypes.value_counts()","a77eb3b1":"df.select_dtypes(['float64','object','int64']).apply(pd.Series.nunique, axis = 0)","da812481":"categorical_variables = df.select_dtypes(['object'])\ncategorical_variables = categorical_variables.columns\ncategorical_variables.tolist()","11c021c3":"for i in categorical_variables:\n    le = preprocessing.LabelEncoder()\n    le.fit(df[i].values)\n    df[i] = le.transform(df[i].values)","d99b0723":"#Correlation between features\ncorr = df.corr('pearson')\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","36226e66":"X = df.loc[:, df.columns != 'SARS-Cov-2 exam result']\ny = df['SARS-Cov-2 exam result']\n\nbestfeatures = SelectKBest(score_func=chi2, k=10)\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n\nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  \nfeatureScores = featureScores.nlargest(10, 'Score')\n\nsns.barplot(x=\"Score\", y=\"Specs\", data=featureScores)\nfeatureScores","c695da1f":"print('Negative: {} ({}%)'.format(df['SARS-Cov-2 exam result'].value_counts()[0], round(df['SARS-Cov-2 exam result'].value_counts()[0]\/len(df)*100, 2)))\nprint('Positive: {} ({}%)'.format(df['SARS-Cov-2 exam result'].value_counts()[1], round(df['SARS-Cov-2 exam result'].value_counts()[1]\/len(df)*100, 2)))\nsns.countplot('SARS-Cov-2 exam result',data=df)","7eeb5555":"rus = RandomUnderSampler(random_state=42)\nX_res, y_res = rus.fit_resample(X, y)\nprint('Resampled dataset shape %s' % Counter(y_res))","0176d79c":"print('Negative: {} ({}%)'.format(y_res.value_counts()[0], round(y_res.value_counts()[0]\/len(y_res)*100, 2)))\nprint('Positive: {} ({}%)'.format(y_res.value_counts()[1], round(y_res.value_counts()[1]\/len(y_res)*100, 2)))\nsns.countplot(y_res)","7b4f0adb":"X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.1, random_state=42,)","ffce72dd":"lr_list = [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 0.8, 0.85, 1, 1.25, 1.5, 1.75, 2]\n\nfor learning_rate in lr_list:\n    gb_clf = GradientBoostingClassifier(n_estimators=20, learning_rate=learning_rate, max_features=2, max_depth=2, random_state=0)\n    gb_clf.fit(X_train, y_train)\n\n    print(\"Learning rate: \", learning_rate)\n    print(\"Accuracy score (training): {0:.3f}\".format(gb_clf.score(X_train, y_train)))\n    print(\"Accuracy score (validation): {0:.3f}\".format(gb_clf.score(X_test, y_test)))","680bbeaf":"def plot_confusion_matrix(cm,\n                          target_names,\n                          title='Confusion matrix',\n                          cmap=None,\n                          normalize=True):\n    \"\"\"\n    given a sklearn confusion matrix (cm), make a nice plot\n\n    Arguments\n    ---------\n    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n\n    target_names: given classification classes such as [0, 1, 2]\n                  the class names, for example: ['high', 'medium', 'low']\n\n    title:        the text to display at the top of the matrix\n\n    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n                  see http:\/\/matplotlib.org\/examples\/color\/colormaps_reference.html\n                  plt.get_cmap('jet') or plt.cm.Blues\n\n    normalize:    If False, plot the raw numbers\n                  If True, plot the proportions\n\n    Usage\n    -----\n    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n                                                              # sklearn.metrics.confusion_matrix\n                          normalize    = True,                # show proportions\n                          target_names = y_labels_vals,       # list of names of the classes\n                          title        = best_estimator_name) # title of graph\n\n    Citiation\n    ---------\n    http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import itertools\n\n    accuracy = np.trace(cm) \/ float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        \n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n\n    thresh = cm.max() \/ 1.5 if normalize else cm.max() \/ 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()","0287da89":"gb_clf2 = GradientBoostingClassifier(n_estimators=20, learning_rate=0.5, max_features=10, max_depth=2, random_state=0)\ngb_clf2.fit(X_train, y_train)\npredictions = gb_clf2.predict(X_test)\n\nplot_confusion_matrix(confusion_matrix(y_test, predictions), target_names=['0', '1'], normalize=False)\n\nprint(classification_report(y_test, predictions))\n\nscores = cross_validate(gb_clf2, X_train, y_train, cv=45, scoring=['precision','recall','roc_auc'])\n\nprint(\"Cross Validation Scores: \")\nprint('Precision: ', scores.get('test_precision').mean())\nprint('Recall: ', scores.get('test_recall').mean())\nprint('ROC_ACU: ', scores.get('test_roc_auc').mean())","ecdc19b9":"Analysing unique categories in each column","dc670cb6":"Encoding labels of categorical data.\n* detected = 1 \n* not_detected = 0","3cf754de":"Cheking metrics about patients age.","11294954":"Dataset shape after dropout","b0ebf06c":"# Treating NaN values\n\nIn this section we treat some NaN values. First, we remove columns with only NaNs since they don't have any value for us. We end up with 105 columns, but there are still a lot of NaNs on these columns. \n\nWe avoid to use common strategies for filling NaNs in this case, because any data inputation can easily lead to unrealistic values that do not correspond to those of a living human being. Instead, we choose to drop columns with more than 80% of NaNs. To get rid of the remaining NaNs, we drop rows that contains missing data. We end up with a dataset of shape (1352, 22).","95f94623":"# Dataset Balance\n\nA first glance shows that the dataset is quite imbalanced.","8f9b7a06":"Counting number of NaNs in each column.","f770ee7e":"Checking colum names","6999818b":"Checking missing data regard patient admission.","788ddef2":"Checking categorical variables","97625cee":"# Feature Importance\n\nIn this section we study the importance of each feature to the target variable. Using the chi-squared statistical test we try to identify the top 10 most important features among the remaining ones. We see that the patient age quartile and patient admittance to the ward points out as the most important features. This may be an indicative of the expertise of doctors identifying more dangerous COVID-19 cases early on, so that a high incidence of patients admited to the ward tests positive for the virus.\\\n\nReference: https:\/\/towardsdatascience.com\/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e\n","26123848":"# Correlation","aba231da":"Dropping columns with only NaN values","33380d5a":"Drop columns with more than 20% NaN values ","34900177":"Just mapping the target variable to 1s and 0s.","d88beea9":"Checking duplicates","ad69e128":"# Checking Dataset","c8631e35":"# Balancing the Dataset\n\nWe still have a imbalanced dataset, with abou 8% of positive cases for the virus against almost 92% of negative cases.\n\nAgain, in order to avoid unrealistic entries to the dataset, we choose a strategie of random undersampling. We end up with a 50\/50 balanced dataset with 224 entries.","2b7e4141":"Authors: [Renan Costalonga](https:\/\/www.kaggle.com\/rcmonteiro) and [Guilherme Rinaldo](https:\/\/www.kaggle.com\/grinaldo)\n\n","22b43376":"# Training Strategy and Modelling\n\nWe chose to undersample our dataset in order to garantee only realistc data for each patient. The downside for this approach is the small number of entires left, which can lead a model to overfit.\n\nTo prevent this behaviour, we use a cross-validation with a high number of folds strategy. Since our dataset is now smaller than the original, this approach becames computationally feseable and makes quite harder for our model to overfit.\n\nFor the model, we choose the GradientBoosting Classifier for its simplicity and robustness. Since we are dealing with medical data we try to maximize the recall metric so that every patient that is positive for the virus can be identified, even at the cost of some false positives.\n\nOthers machine learning algorithms are also tested, with similar or inferior behaviour.\n\nReference: https:\/\/stackabuse.com\/gradient-boosting-classifiers-in-python-with-scikit-learn\/","91351a43":"# Conclusions","b88059eb":"Comparing dataset after undersampling","80e2517e":"# Feature analysis","d6568886":"Our strategy led to a Precision of 74%, Recall of 88% and Roc_AuC of approximatelly 83%. Since our goal was to maximize recall, without loosing performance in Precision, we believe we have achieved a simple androbust classifier. \n\nThe high number of folds used for cross-validation helps the model to not overfit and possibly generalize well for other datasets. We hope this contribution, as small as it is, can be useful to our community in the search for means to stop COVID-19.\n\n\n# #StaySafe","26a7bc03":"Drop rows with any Nan value","33d085b2":"Since the patient ID is irrelevant in this context, we will drop it.","f687ea57":"Analisying type of data in each column","805d5bcb":"In this section we study the correlation between each features."}}