{"cell_type":{"b38639af":"code","28298bb7":"code","8beb982f":"code","ce631ebb":"code","7dcbe1b6":"code","d8364070":"code","b1fdc270":"code","9713a740":"code","25f60c0d":"code","31abb308":"code","f0abee71":"code","b10fe5f4":"code","8ae083b4":"code","17ad49b5":"code","3f6d2180":"code","695b0a25":"code","3b9c233b":"code","e1de1450":"code","fc905ba6":"code","5af6c87b":"code","2c1d1348":"code","7231949a":"code","0dccd2e9":"code","7556cda5":"code","ad889567":"code","512f1836":"code","52e042d6":"code","968a7680":"code","b9b04016":"code","f0294352":"code","89744693":"code","1ee5a60d":"code","faedbacd":"code","70540935":"code","8779152f":"code","bcc253b9":"code","7378fa30":"code","5c66b031":"code","fce1a4a0":"code","5252d4e2":"code","11b09ea6":"code","bb3aa8e8":"code","ea45dce7":"code","b2154dc4":"code","e450bbc7":"code","5664c848":"code","61e400ac":"code","7ea5f8ad":"code","61619414":"code","2ef00e3c":"code","3329e83b":"code","433843b0":"code","9fc67cdc":"code","6337f57e":"code","4a930a34":"code","5fa446dd":"code","4350e6b9":"code","48078577":"code","c3e176bf":"code","a3af4cde":"code","f287c0af":"markdown","7fe3ca07":"markdown","656df435":"markdown","51a5513c":"markdown","5ef8f1a4":"markdown","d0581f54":"markdown","faf96d1f":"markdown","36bf081c":"markdown","bb50d2f5":"markdown","b70762e2":"markdown","fc0f9599":"markdown","e42ba5b5":"markdown","10141f65":"markdown","4645978a":"markdown","c49b23fd":"markdown","b5a14452":"markdown","b8ac3b3c":"markdown"},"source":{"b38639af":"\nimport pandas as pd\nimport numpy as np\n\n\nimport seaborn as sns\nsns.set_style(\"whitegrid\", {'axes.grid' : False})\nimport os\nfrom tqdm.notebook import tqdm\nimport gc\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns; sns.set()\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import ParameterGrid\n\n#import shap\n#shap.initjs()","28298bb7":"TARGET_COL = \"diabetes_mellitus\"\ndf = pd.read_csv(\"..\/input\/widsdatathon2021\/TrainingWiDS2021.csv\")\nprint(df.shape)\ntest = pd.read_csv(\"..\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv\")\nprint(test.shape)\n","8beb982f":"df['label']='train'\ntest['label']='test'\nframes = [df,test]\njoin_df = pd.concat(frames, keys=['x', 'y'])\nassert len(join_df) == len(df) + len(test)","ce631ebb":"#join_df.describe().T","7dcbe1b6":"lst = join_df.isna().sum()\/len(join_df)\np = pd.DataFrame(lst)\np.reset_index(inplace=True)\np.columns = ['a','b']\nlow_count = p[p['b']>0.8]\ntodelete=low_count['a'].values","d8364070":"todelete","b1fdc270":"join_df.drop(todelete,axis=1,inplace=True)","9713a740":"join_df.shape","25f60c0d":"join_df.head()","31abb308":"#helps with reducing memory usage\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","f0abee71":"join_df.drop(['Unnamed: 0','encounter_id'],inplace=True,axis=1)","b10fe5f4":"## Print the categorical columns\nprint([c for c in df.columns if (1<df[c].nunique()) & (df[c].dtype != np.number)& (df[c].dtype != int) ])","8ae083b4":"## Print the categorical columns\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n\nnewdf = join_df.select_dtypes(include=numerics)\nnumeric_cols = newdf.columns","17ad49b5":"numeric_cols ","3f6d2180":"#join_df.describe()","695b0a25":"# Need to do column by column due to memory constraints\ncategorical_cols =  ['elective_surgery','hospital_id','icu_id',\n 'ethnicity', 'gender', 'hospital_admit_source', 'icu_admit_source', 'icu_stay_type', 'icu_type','aids','cirrhosis','hepatic_failure','immunosuppression',\n 'leukemia','lymphoma','solid_tumor_with_metastasis','elective_surgery','apache_post_operative','arf_apache','fio2_apache','gcs_unable_apache','gcs_eyes_apache',\n 'gcs_motor_apache','gcs_verbal_apache','intubated_apache','ventilated_apache','solid_tumor_with_metastasis']\nfor i, v in tqdm(enumerate(categorical_cols)):\n    join_df[v] = join_df[v].fillna(join_df[v].value_counts().index[0])\n","3b9c233b":"for i, v in tqdm(enumerate([numeric_cols])):\n    join_df[v] =join_df.groupby(['ethnicity','gender'], sort=False)[v].apply(lambda x: x.fillna(x.mean()))","e1de1450":"join_df[categorical_cols].isna().sum()","fc905ba6":"from sklearn.preprocessing import OrdinalEncoder\n\n# In loop to minimize memory use\nfor i, v in tqdm(enumerate(categorical_cols)):\n    join_df[v] = OrdinalEncoder(dtype=\"int\").fit_transform(join_df[[v]])\n    \n\ngc.collect()","5af6c87b":"train = join_df[join_df['label']==\"train\"]\npredict = join_df[join_df['label']=='test']","2c1d1348":"train.reset_index(inplace=True)\ntrain.drop(['level_0','level_1','label'],inplace=True,axis =1 )","7231949a":"train.shape","0dccd2e9":"predict.reset_index(inplace=True)\npredict.drop(['level_0','level_1','diabetes_mellitus','label'],inplace=True,axis=1)","7556cda5":"predict.shape","ad889567":"features = train.columns","512f1836":"num_feature = [col for col in features if col not in categorical_cols]","52e042d6":"num_feature = [col for col in features if col not in categorical_cols and train[col].dtype != 'object']\ndrop_columns=[]\ncorr = train[num_feature].corr()\n# Drop highly correlated features \ncolumns = np.full((corr.shape[0],), True, dtype=bool)\n\nfor i in range(corr.shape[0]):\n    for j in range(i+1, corr.shape[0]):\n        if corr.iloc[i,j] >=0.999 :\n            if columns[j]:\n                columns[j] = False\n                print('FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(train[num_feature].columns[i] , train[num_feature].columns[j], corr.iloc[i,j]))\n        elif corr.iloc[i,j] <= -0.995:\n            if columns[j]:\n                columns[j] = False\n","968a7680":"drop_columns = train[num_feature].columns[columns == False].values\nprint('drop_columns',len(drop_columns),drop_columns)","b9b04016":"train.drop(drop_columns,inplace=True,axis =1 )","f0294352":"predict.drop(drop_columns,inplace=True,axis =1 )","89744693":"train[TARGET_COL].value_counts()\/len(train)","1ee5a60d":"print(train.shape,predict.shape)","faedbacd":"# Separate majority and minority classes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import resample\ndf_majority = train[train['diabetes_mellitus']==0]\ndf_minority = train[train['diabetes_mellitus']==1]\n\ndf_minority_upsampled = resample(df_minority, \n                                 replace=True,     # sample with replacement\n                                 n_samples=83798,    # to match majority class\n                                 random_state= 303) # reproducible results\n \n# Combine majority class with upsampled minority class\ndf_upsampled = pd.concat([df_majority, df_minority_upsampled])\n \n# Display new class counts\ndf_upsampled.diabetes_mellitus.value_counts()","70540935":"train = df_upsampled","8779152f":"X_train, X_test, y_train, y_test = train_test_split(\n     train[[c for c in train if TARGET_COL != c]], train[TARGET_COL], test_size=0.01, random_state=42)\nprint(X_train.shape,X_test.shape)","bcc253b9":"X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.01, random_state=42)\nprint(X_train.shape,X_valid.shape)","7378fa30":"X_train.head()","5c66b031":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report","fce1a4a0":"from numpy import loadtxt\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import metrics","5252d4e2":"model = LGBMClassifier()\nmodel.fit(X_train, y_train)","11b09ea6":"print(f\"accuracy score is {accuracy_score(y_valid, model.predict(X_valid))}\")\nprint(metrics.classification_report(y_valid, model.predict(X_valid), labels=[0, 1]))","bb3aa8e8":"pred = model.predict_proba(predict)[:,1]\ntest[TARGET_COL] = pred\ntest[[\"encounter_id\",\"diabetes_mellitus\"]].to_csv(\".\/submission_baseline_2.csv\",index=False)","ea45dce7":"from lightgbm import LGBMClassifier","b2154dc4":"model = LGBMClassifier(\n                              random_state=33,\n                              early_stopping_rounds = 250,\n                              n_estimators=1000,\n                              boosting_type='gbdt', num_leaves=151, max_depth=- 1, learning_rate=0.02, subsample_for_bin=200, \n                              min_split_gain=0.5, min_child_weight=0.001, min_child_samples=20, subsample=1.0, subsample_freq=0, \n                              colsample_bytree=.75, reg_alpha=1.3, reg_lambda=0.1,  n_jobs=- 1,\n                              silent=True, importance_type='split')\n\nmodel.fit(\n    X_train,\n    y_train,\n    eval_set=[(X_valid, y_valid)],\n    eval_metric='auc',  \n    verbose=False,\n)\n#{'colsample_bytree': 0.7, 'learning_rate': 0.03, 'max_depth': 20, 'n_estimators': 1000}","e450bbc7":"print(f\"accuracy score is {accuracy_score(y_test, model.predict(X_test))}\") #0.02- 0.8423\nprint(metrics.classification_report(y_test, model.predict(X_test), labels=[0, 1]))","5664c848":"pred = model.predict_proba(predict)[:,1]\ntest[TARGET_COL] = pred\ntest[[\"encounter_id\",\"diabetes_mellitus\"]].to_csv(\".\/submission_tuned_lgbm.csv\",index=False)","61e400ac":"model= LGBMClassifier(\n                              random_state=33,\n                              early_stopping_rounds = 250,\n                              n_estimators=20000,min_data_per_group=5, # reduce overfitting when using categorical_features\n                              boosting_type='gbdt', num_leaves=170, max_depth=- 1, learning_rate=0.01, subsample_for_bin=200000, \n                              min_split_gain=0.0, min_child_weight=0.001, min_child_samples=20, subsample=1.0, subsample_freq=0, \n                              colsample_bytree=.75, reg_alpha=1.3, reg_lambda=0.1,  n_jobs=- 1,cat_smooth=1.0, \n                              silent=True, importance_type='split')\n\nmodel.fit(\n    X_train,\n    y_train,\n    eval_set=[(X_valid, y_valid)],\n    eval_metric = 'auc',\n    verbose=False\n)","7ea5f8ad":"print(f\"accuracy score is {accuracy_score(y_test, model.predict(X_test))}\") #0.02- 0.8423\nprint(metrics.classification_report(y_test, model.predict(X_test), labels=[0, 1]))","61619414":"pred = model.predict_proba(predict)[:,1]\ntest[TARGET_COL] = pred\ntest[[\"encounter_id\",\"diabetes_mellitus\"]].to_csv(\"submission_tuned_lgbm_2.csv\",index=False)","2ef00e3c":"def algorithm_pipeline(X_train_data, X_test_data, y_train_data, y_test_data, X_valid,y_valid,\n                       model, param_grid, cv=10, scoring_fit='neg_mean_squared_error',\n                       do_probabilities = False):\n    gs = GridSearchCV(\n        estimator=model,\n        param_grid=param_grid, \n        cv=cv, \n        n_jobs=-1, \n        scoring=scoring_fit,\n        verbose=2\n    )\n    fitted_model = gs.fit(X_train_data, y_train_data, eval_set=[(X_valid, y_valid)], eval_metric='l1')\n    \n    if do_probabilities:\n      pred = fitted_model.predict_proba(X_test_data)\n    else:\n      pred = fitted_model.predict(X_test_data)\n    \n    return fitted_model, pred","3329e83b":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV","433843b0":"# model = LGBMClassifier()\n# param_grid = {\n#     'learning_rate' : [0.01,0.02,0.03,0.04,0.05,0.06],\n#     'n_estimators': [1000],\n#      'colsample_bytree': [0.7],\n#      'max_depth': [50],\n#      'num_leaves': [50, 100, 200],\n#     # 'reg_alpha': [1.1, 1.2, 1.3],\n#     # 'reg_lambda': [1.1, 1.2, 1.3],\n#     # 'min_split_gain': [0.3, 0.4],\n#     # 'subsample': [0.7, 0.8, 0.9],\n#     # 'subsample_freq': [20]\n# }\n\n# model, pred = algorithm_pipeline(X_train, X_test, y_train, y_test,X_valid,y_valid, model, \n#                                  param_grid, cv=10, scoring_fit='accuracy')\n\n# print(model.best_score_)\n# print(model.best_params_)","9fc67cdc":"from sklearn.ensemble import GradientBoostingClassifier\nclf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.01, max_depth=10, random_state=0)\nclf.fit(X_train, y_train)","6337f57e":"print(f\"accuracy score is {accuracy_score(y_test, clf.predict(X_test))}\")\nprint(metrics.classification_report(y_test, clf.predict(X_test), labels=[0, 1]))\npred = clf.predict_proba(predict)[:,1]\ntest[TARGET_COL] = pred\ntest[[\"encounter_id\",\"diabetes_mellitus\"]].to_csv(\"submission_xgb.csv\",index=False)","4a930a34":"from sklearn.ensemble import RandomForestClassifier\nmodel_randomForest = RandomForestClassifier(n_estimators=1000, random_state=0,  n_jobs=-1)\n\n# Fit the Random Forest\nmodel_randomForest.fit(X_train, y_train)","5fa446dd":"print(f\"accuracy score is {accuracy_score(y_test, model_randomForest.predict(X_test))}\")\nprint(metrics.classification_report(y_test, model_randomForest.predict(X_test), labels=[0, 1]))\npred = model_randomForest.predict_proba(predict)[:,1]\ntest[TARGET_COL] = pred\ntest[[\"encounter_id\",\"diabetes_mellitus\"]].to_csv(\"submission_randomForest.csv\",index=False)","4350e6b9":"# n_estimators = [100, 300, 500, 800, 1200]\n# max_depth = [5, 8, 15, 25, 30]\n# min_samples_split = [2, 5, 10, 15, 100]\n# min_samples_leaf = [1, 2, 5, 10] \n\n# hyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  \n#               min_samples_split = min_samples_split, \n#              min_samples_leaf = min_samples_leaf)\n\n# gridF = GridSearchCV(model_randomForest, hyperF, cv = 3, verbose = 1 )\n# bestF = gridF.fit(X_train, y_train)","48078577":"# print(f\"accuracy score is {accuracy_score(y_test, bestF.predict(X_test))}\")\n# print(metrics.classification_report(y_test, bestF.predict(X_test), labels=[0, 1]))\n# pred = bestF.predict_proba(predict)[:,1]\n# test[TARGET_COL] = pred\n# test[[\"encounter_id\",\"diabetes_mellitus\"]].to_csv(\"submission_randomForest_ft.csv\",index=False)","c3e176bf":"model_randomForest = RandomForestClassifier(random_state = 1, max_depth = 15, n_estimators = 2000, min_samples_split = 2, \n                                            min_samples_leaf = 1,n_jobs=-1)\n                                   \n# Fit the Random Forest\nmodel_randomForest.fit(X_train, y_train)","a3af4cde":"print(f\"accuracy score is {accuracy_score(y_test, model_randomForest.predict(X_test))}\")\nprint(metrics.classification_report(y_test, model_randomForest.predict(X_test), labels=[0, 1]))\npred = model_randomForest.predict_proba(predict)[:,1]\ntest[TARGET_COL] = pred\ntest[[\"encounter_id\",\"diabetes_mellitus\"]].to_csv(\"submission_randomForest_ft2.csv\",index=False)","f287c0af":"# Prediction using lightgbm model with some hyperparameter tuning\n\nThe following code has gotten me to 0.85ish score. \n\nSome of the feature engineering I applied is:\n\n* Remove cols with higher than 80% nans\n* Fill na categorical variables witht the most common values\n* Fill na for numeric values with mean values after [groupby(ethnicity, gender)]\n* Remove highly correlated columns\n* Convert categorical variables to encodings\n\nThe model is a lightgbm model with some hyperparameter tuning, it helped with a little improvement\n\nNext thing to try: Embeddings with neural net\n\nP.S. Happy to learn what others are doing and open to joining teams\n\n","7fe3ca07":"### Convert categorical\/binary variables to OrdinalEncoders","656df435":"> ### Fine Tuning RF","51a5513c":"### Grid Search Approach\nThis will take some time and educated guess to set the parameters\n","5ef8f1a4":"### Seperating train and predict(test)","d0581f54":"### We can use the following code to delete certain colums that are empty more than 80%","faf96d1f":"### Fine Tuning RF 2","36bf081c":"### Baseline Model","bb50d2f5":"\n### Remove correlated columns\nCredit: https:\/\/www.kaggle.com\/ankitmalik\/simple-neural-net-0-84706","b70762e2":"### Joining train and test to ensure encodings done correctly","fc0f9599":"### Import the Libraries","e42ba5b5":"### Fill categorical columns","10141f65":"# XGBoost","4645978a":"### Create split","c49b23fd":"# Random Forest","b5a14452":"### Load the Data","b8ac3b3c":"### Model with Hyper Parameter Tuning"}}