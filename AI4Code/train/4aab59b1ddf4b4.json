{"cell_type":{"24b868a7":"code","e83ef35a":"code","ab0315b2":"code","04008a1f":"code","ed42d202":"code","73ba9f21":"code","d45932b3":"code","829c8637":"code","51b5050a":"code","a59c4fc1":"code","ba33fc0e":"code","d7fc1526":"code","a49ab763":"code","793199e2":"code","8c5794be":"code","a03066dd":"code","6686688d":"code","5fe3f10f":"code","3dac4add":"code","166fa451":"code","f6aea31e":"code","7c829223":"code","7e9c1ea0":"code","89ce978d":"code","bdcd43fc":"code","e0805934":"code","03168ae4":"code","99e4d2ab":"code","fc8ef9b7":"code","1fd7f25c":"code","0f021b4d":"code","bc92be24":"markdown","44ecbef1":"markdown","e67d533f":"markdown","a9186339":"markdown","fd80b096":"markdown","d3702494":"markdown","ee96de97":"markdown","cd170e6b":"markdown","c72862ee":"markdown","6336d9e4":"markdown","2a4cf516":"markdown"},"source":{"24b868a7":"\"\"\"\nimport os\nimport glob\nimport pandas as pd\n#os.chdir(\"\/mydir\")\n\n\nfiles = [i for i in glob.glob('\/kaggle\/input\/one-year-industrial-component-degradation\/*.{}'.format('csv'))]\nfiles\n\n\nextension = 'csv'\nall_filenames = [i for i in glob.glob('\/kaggle\/input\/one-year-industrial-component-degradation\/*[mode1].{}'.format(extension))] + \\\n                [i for i in glob.glob('\/kaggle\/input\/one-year-industrial-component-degradation\/oneyeardata\/*[mode1].{}'.format(extension))]\n#print(all_filenames)\n\n#combine all files in the list\ndf = pd.concat([pd.read_csv(f) for f in all_filenames ])\n#export to csv\ndf.to_csv( \"combined_csv.csv\", index=False, encoding='utf-8-sig')\n\n\"\"\"","e83ef35a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ab0315b2":"\"\"\"\nfilenames = os.listdir('\/kaggle\/input\/one-year-industrial-component-degradation\/')\nfilenames = [i.strip(\".csv\") for i in filenames]\nfilenames.sort()\nfilenames.remove('oneyeardata')\n\nparsed_filenames = []\nfor name in filenames:\n    temp = name.split(\"T\")\n    month, date = temp[0].split(\"-\")\n    rhs = temp[1].split(\"_\")\n    hours, minutes, seconds = rhs[0][:2], rhs[0][2:4], rhs[0][4:]\n    sample_no = rhs[1]\n    mode = rhs[2][-1]\n    # Now we have Month, Date, Hours, Minutes, Seconds, Sample Number, Mode \n    parsed_filenames.append([month, date, hours, minutes, seconds, sample_no, mode])\n    \nparsed_filenames = pd.DataFrame(parsed_filenames, columns=[\"Month\", \"Date\", \"Hours\", \"Minutes\", \"Seconds\", \"Sample Number\", \"Mode\"])\n\nfor i in parsed_filenames.columns:\n    parsed_filenames[i] = pd.to_numeric(parsed_filenames[i], errors='coerce')\n\n\n\npath = '\/kaggle\/input\/one-year-industrial-component-degradation\/'\ndf = pd.DataFrame()\n#f = pd.read_csv(path+filenames[0]+\".csv\")\n#f = f.join(parsed_filenames[0:1], how='left')\n#f = f.fillna(method='ffill')\n#f\nfor ind, file in enumerate(filenames):\n    file_content = pd.read_csv(path+file+\".csv\")\n    file_content = file_content.join(parsed_filenames[ind:ind+1], how='left')\n    file_content.fillna(method='ffill', inplace=True)\n    \n    if df.empty:\n        df = file_content\n        df.fillna(method='ffill', inplace=True)\n    else:\n        df = df.append(file_content, ignore_index=True)\n        df.fillna(method='ffill', inplace=True)\n\n        \nfor i in ['Mode', 'Sample Number', 'Seconds', 'Minutes', 'Hours', 'Date', 'Month']:\n    df[i] = pd.to_numeric(df[i], downcast='integer')\ndf.info()\n\n\"\"\"","04008a1f":"\"\"\"\nif not os.path.exists('\/kaggle\/working\/compiled_df'):\n    os.makedirs('\/kaggle\/working\/compiled_df')\n#Saves dataframe to a csv file, removes a index\ndf.to_csv('\/kaggle\/working\/compiled_df\/Combined.csv', index=False)\n\n\"\"\"","ed42d202":"df = pd.read_csv(\"..\/input\/combineddataset\/Combined.csv\")\nfor i in ['Mode', 'Sample Number', 'Seconds', 'Minutes', 'Hours', 'Date', 'Month']:\n    df[i] = pd.to_numeric(df[i], downcast='integer')\ndf.info()","73ba9f21":"df.head(10)","d45932b3":"data = df.copy()\ndata = data[:10000]\ndata=data.drop(['Sample Number', 'Seconds', 'Minutes', 'Hours', 'Date', 'Month'],axis=1)\n\n#df=pd.get_dummies(data)\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler() \nnum2 = scaler.fit_transform(data.drop(['timestamp'],axis=1))\nnum2 = pd.DataFrame(num2, columns = data.drop(['timestamp'],axis=1).columns)","829c8637":"from sklearn.cluster import DBSCAN\noutlier_detection = DBSCAN(\n eps = .2, \n metric='euclidean', \n min_samples = 5,\n n_jobs = -1)\nclusters = outlier_detection.fit_predict(num2)","51b5050a":"clusters.shape","a59c4fc1":"data['anomaly'] = pd.Series(clusters)\ndata.head()","ba33fc0e":"data['anomaly'].unique()","d7fc1526":"X_anomaly = data[data['anomaly'] == -1]\nX_normal = data[data['anomaly'] != -1]\nprint(X_anomaly.shape, X_normal.shape)\n","a49ab763":"#from matplotlib import cm\n#cmap = cm.get_cmap('Set1\u2019)\n#data.plot.scatter(x='Spend_Score',y='Income', c=clusters, cmap=cmap, colorbar = False)","793199e2":"from sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE","8c5794be":"data.columns","a03066dd":"\"\"\"\ncols = ['pCut::Motor_Torque',\n       'pCut::CTRL_Position_controller::Lag_error',\n       'pCut::CTRL_Position_controller::Actual_position',\n       'pCut::CTRL_Position_controller::Actual_speed',\n       'pSvolFilm::CTRL_Position_controller::Actual_position',\n       'pSvolFilm::CTRL_Position_controller::Actual_speed',\n       'pSvolFilm::CTRL_Position_controller::Lag_error', 'pSpintor::VAX_speed',\n       'Mode']\n\npca = PCA(n_components=2)\ndata_pca = pca.fit_transform(data[cols].values)\n\n#data['pca-one'] = data_pca[:,0]\n#data['pca-two'] = data_pca[:,1] \n\nprint('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n\"\"\"","6686688d":"anomaly_ind = data[data['anomaly']==-1].index\nnormal_ind = data[data['anomaly']!=-1].index","5fe3f10f":"anomaly_pca = pd.DataFrame(data_pca[anomaly_ind])\nnormal_pca = pd.DataFrame(data_pca[normal_ind])\nanomaly_pca","3dac4add":"data.columns","166fa451":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfeatures = ['pCut::Motor_Torque',\n       'pCut::CTRL_Position_controller::Lag_error',\n       'pCut::CTRL_Position_controller::Actual_position',\n       'pCut::CTRL_Position_controller::Actual_speed',\n       'pSvolFilm::CTRL_Position_controller::Actual_position',\n       'pSvolFilm::CTRL_Position_controller::Actual_speed',\n       'pSvolFilm::CTRL_Position_controller::Lag_error', 'pSpintor::VAX_speed',\n       'Mode']\n\nfor feature in features:\n    plt.figure(figsize=(15,7))\n    plt.plot(data[feature], color='blue', label = 'normal')\n    plt.scatter(x=data.iloc[anomaly_ind].index, y=data.iloc[anomaly_ind][feature], color='red', label = 'anomalous')\n    #plt.scatter(x=normal_pca[0], y=normal_pca[1], color='blue')\n    plt.title(feature)\n    plt.legend()","f6aea31e":"data = df.copy()\ndata = data[:10000]\ndata=data.drop(['timestamp', 'Sample Number', 'Seconds', 'Minutes', 'Hours', 'Date', 'Month'],axis=1)\n","7c829223":"from sklearn.ensemble import IsolationForest\nrs=np.random.RandomState(0)\nclf = IsolationForest(max_samples=100,random_state=rs, contamination=.1) \nclf.fit(data)\nif_scores = clf.decision_function(data)\nif_anomalies=clf.predict(data)\n#print(if_anomalies)\nif_anomalies=pd.Series(if_anomalies).replace([-1,1],[1,0])\n#print(if_anomalies)\n#if_anomalies=num[if_anomalies==1];","7e9c1ea0":"plt.figure(figsize=(12,8))\nplt.hist(if_scores);\nplt.title('Histogram of Avg Anomaly Scores: Lower => More Anomalous');","89ce978d":"anomaly_ind = if_anomalies[if_anomalies==1].index","bdcd43fc":"features = ['pCut::Motor_Torque',\n       'pCut::CTRL_Position_controller::Lag_error',\n       'pCut::CTRL_Position_controller::Actual_position',\n       'pCut::CTRL_Position_controller::Actual_speed',\n       'pSvolFilm::CTRL_Position_controller::Actual_position',\n       'pSvolFilm::CTRL_Position_controller::Actual_speed',\n       'pSvolFilm::CTRL_Position_controller::Lag_error', 'pSpintor::VAX_speed',\n       'Mode']\n\nfor feature in features:\n    plt.figure(figsize=(15,7))\n    #cmap=np.array(['white','red'])\n    plt.scatter(data.index,data[feature],c='green', label = 'normal')\n    plt.scatter(anomaly_ind,data.iloc[anomaly_ind][feature],c='red', label='anomaly')\n    plt.ylabel(feature)\n    plt.title(feature)\n    plt.legend()","e0805934":"data = df.copy()\ndata = data[:10000]\ndata=data.drop(['timestamp', 'Sample Number', 'Seconds', 'Minutes', 'Hours', 'Date', 'Month'],axis=1)","03168ae4":"from sklearn import svm\nclf=svm.OneClassSVM(nu=.1,kernel='rbf', gamma='auto')\nclf.fit(data)\ny_pred=clf.predict(data)","99e4d2ab":"y_pred = pd.Series(y_pred).replace([-1,1],[1,0])","fc8ef9b7":"anomaly_ind = y_pred[y_pred==1].index","1fd7f25c":"anomaly_ind","0f021b4d":"features = ['pCut::Motor_Torque',\n       'pCut::CTRL_Position_controller::Lag_error',\n       'pCut::CTRL_Position_controller::Actual_position',\n       'pCut::CTRL_Position_controller::Actual_speed',\n       'pSvolFilm::CTRL_Position_controller::Actual_position',\n       'pSvolFilm::CTRL_Position_controller::Actual_speed',\n       'pSvolFilm::CTRL_Position_controller::Lag_error', 'pSpintor::VAX_speed',\n       'Mode']\n\nfor feature in features:\n    plt.figure(figsize=(15,7))\n    #cmap=np.array(['white','red'])\n    plt.scatter(data.index,data[feature],c='green', label = 'normal')\n    plt.scatter(anomaly_ind,data.iloc[anomaly_ind][feature],c='red', label='anomaly')\n    plt.ylabel(feature)\n    plt.title(feature)\n    plt.legend()","bc92be24":"## Sklearn Implementation of Isolation Forests:","44ecbef1":"Below, I plot observations identified as anomalies. These observations have if_scores values below the clf.threshold_ value.","e67d533f":"### Sklearn Implementation of One-Class SVM:","a9186339":"Below, I plot observations identified as anomalies:","fd80b096":"DBSCAN will output an array of -1\u2019s and 0\u2019s, where -1 indicates an outlier. Below, I visualize outputted outliers in red by plotting two variables.","d3702494":"## Sklearn Implementation of DBSCAN:","ee96de97":"Below, I plot a histogram of if_scores values. Lower values indicate observations that are more anomalous.","cd170e6b":"### Data Preprocessing:","c72862ee":"# Isolation Forests\n\n### For each observation, do the following:\n1. Randomly select a feature and randomly select a value for that feature within its range.\n2. If the observation\u2019s feature value falls above (below) the selected value, then this value becomes the new min (max) of that feature\u2019s range.\n3. Check if at least one other observation has values in the range of each feature in the dataset, where some ranges were altered via step 2. If no, then the observation is isolated.\n4. Repeat steps 1\u20133 until the observation is isolated. The number of times you had to go through these steps is the isolation number. The lower the number, the more anomalous the observation is.","6336d9e4":"# One-Class Support Vector Machines\n\n#### The nu hyperparameter seems to be like the contamination hyperparameter in other methods. It sets the % of observations the algorithm will identify as outliers.","2a4cf516":"## DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n\nThis is a clustering algorithm (an alternative to K-Means) that clusters points together and identifies any points not belonging to a cluster as outliers. It\u2019s like K-means, except the number of clusters does not need to be specified in advance.\n\n### The method, step-by-step:\n1. Randomly select a point not already assigned to a cluster or designated as an outlier. Determine if it\u2019s a core point by seeing if there are at least min_samples points around it within epsilon distance.\n2. Create a cluster of this core point and all points within epsilon distance of it (all directly reachable points).\n3. Find all points that are within epsilon distance of each point in the cluster and add them to the cluster. Find all points that are within epsilon distance of all newly added points and add these to the cluster. Rinse and repeat. (i.e. perform \u201cneighborhood jumps\u201d to find all density-reachable points and add them to the cluster).\n\n### Lingo underlying the above:\n1. Any point that has at least min_samples points within epsilon distance of it will form a cluster. This point is called a core point. The core point will itself count towards the min_samples requirement.\n2. Any point within epsilon distance of a core point, but does not have min_samples points that are within epsilon distance of itself is called a borderline point and does not form its own cluster.\n3. A border point that is within epsilon distance of multiple core points (multiple epsilon balls) will arbitrarily end up in just one of these resultant clusters.\n4. Any point that is randomly selected that is not found to be a core point or a borderline point is called a noise point or outlier and is not assigned to any cluster. Thus, it does not contain at least min_samples points that are within epsilon distance from it or is not within epsilon distance of a core point.\n5. The epsilon-neighborhood of point p is all points within epsilon distance of p, which are said to be directly reachable from p.\n6. A point contained in the neighborhood of a point directly reachable from p is not necessarily directly reachable from p, but is density-reachable.\n7. Any point that can be reached by jumping from neighborhood to neighborhood from the original core point is density-reachable.\n\n### Implementation Considerations:\n1. You may need to standardize \/ scale \/ normalize your data first.\n2. Be mindful of data type and the distance measure. I\u2019ve read that the gower distance metric can be used for mixed data types. I\u2019ve implemented Euclidean, here, which needs continuous variables, so I removed gender.\n3. You will want to optimize epsilon and min_samples."}}