{"cell_type":{"a7650ee6":"code","8d65d154":"code","a7f7aac3":"code","1ed3df6e":"code","9aa6377b":"code","fc5a96c3":"code","b77530bb":"code","c19dfb7b":"code","19ebcf1a":"code","764cea3a":"code","ca58d115":"code","fe7e0086":"code","7faf6376":"code","7cff478f":"code","35a4d587":"code","b0933c13":"code","d8ebede7":"code","724cb4b3":"code","d8226076":"code","a8590d02":"code","3210ef3f":"code","3280e68d":"code","34f26b84":"code","af45e540":"code","0b1bd5df":"code","c8a11048":"code","9d7da550":"code","154f2cc3":"code","b314222b":"code","420a5178":"code","3e138ce6":"code","bf6bfe01":"code","d0c5982f":"code","ec92997d":"code","2134adfc":"code","336a6088":"code","26beae68":"code","4dc685e9":"code","d28f7493":"code","16d1189a":"code","7e8b8fef":"code","0afae524":"code","cdaca141":"code","18889606":"markdown","0e99ebc5":"markdown","40438146":"markdown","680804d7":"markdown","0a9a90ea":"markdown","e1404f37":"markdown","0f7a9e87":"markdown","fd5d2033":"markdown","81e3da91":"markdown","9d611c24":"markdown","fbb02df4":"markdown","835b06fa":"markdown","c3df6bbf":"markdown","78d91007":"markdown","2a624e82":"markdown","f8815a67":"markdown","cdd0dd5a":"markdown","21642a5c":"markdown","8f07d977":"markdown","7d3449f7":"markdown","11e1a7ce":"markdown","6be8ce2d":"markdown","efde090a":"markdown"},"source":{"a7650ee6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8d65d154":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n%matplotlib inline","a7f7aac3":"df=pd.read_csv('\/kaggle\/input\/health-insurance-cross-sell-prediction\/train.csv')\ndf.head()","1ed3df6e":"for i in df.columns:\n    rows=df.shape[0]\n    cols=df.shape[1]\nprint(f'total number of rows are {rows}')\nprint(f'total number of columns are {cols}')","9aa6377b":"#info\ndf.info()\n","fc5a96c3":"# drop not necessary columns\ndf.drop('id', axis=1, inplace=True)","b77530bb":"#details about data\ndf.describe()\n\n","c19dfb7b":"## cat and num columns\ncat_cols=[i for i in df.columns if df[i].dtype=='object']\nnum_cols=[j for j in df.columns if (df[j].dtype=='int64')|(df[j].dtypes=='float64')]\nprint(f'Categorical columns are {cat_cols}\\n')\nprint(f'Numerical columns are {num_cols}')","19ebcf1a":"#to find nan values\ndf.isna().sum()","764cea3a":"#correlation between the columns\ndf.corr()","ca58d115":"#unique values and number of unique values\nfor vars in df.columns:\n    print(f'unique values of {vars} are {df[vars].unique()}\\n')\n    print(f'number of unique values of {vars} are {df[vars].nunique()}\\n')","fe7e0086":"# categorical values\ncat_cols\nfig, axes=plt.subplots(2,2, figsize=(10,10))\nfor i, j in enumerate(cat_cols):\n    ax=axes[int(i\/2),i%2]\n    sns.countplot(df[j], ax=ax)\nfig.delaxes(axes[1,1])\n\n","7faf6376":"# driving_lisensce holders(1= with DL,0=without DL)\ndf['Driving_License'].value_counts().plot(kind='bar')","7cff478f":"#previously insured data(1=insured, 0=not insured)\ndf['Previously_Insured'].value_counts().plot(kind='bar')","35a4d587":"#kde plot of data\nkde_data=['Age','Annual_Premium','Vintage']\nfig, axes=plt.subplots(2,2, figsize=(10,10))\nfor i,j in enumerate(kde_data):\n    ax=axes[int(i\/2), i%2]\n    sns.kdeplot(df[j], ax=ax)\nfig.delaxes(axes[1,1])","b0933c13":"#to find outliers box plot\nnum_cols\nfig, axes=plt.subplots(4,2, figsize=(20,10))\nfor i,j in enumerate(num_cols):\n    ax=axes[int(i\/2), i%2]\n    sns.boxplot(df[j],ax=ax)","d8ebede7":"df.head()","724cb4b3":"## bivariate plots\nplt.figure(figsize=(20,6))\nsns.lineplot(x=df['Age'],y=df['Annual_Premium'])\n","d8226076":"plt.figure(figsize=(20,6))\nsns.lineplot(x=df['Age'],y=df['Vintage'])","a8590d02":"plt.figure(figsize=(10,10))\nsns.heatmap(df.corr(), annot=True)","3210ef3f":"wrt_res_data=['Gender','Driving_License','Previously_Insured','Vehicle_Age','Vehicle_Damage']\nfig, axes=plt.subplots(3,2, figsize=(20,10))\nfor i, j in enumerate(wrt_res_data):\n    ax=axes[int(i\/2),i%2]\n    sns.countplot(df[j], hue=df['Response'] ,ax=ax)\nfig.delaxes(axes[2,1])\n","3280e68d":"df.reset_index(drop='First')","34f26b84":"change={'> 2 Years':'grt2','1-2 Year':'1to2','< 1 Year':'less1'}\ndf['Vehicle_Age']=df['Vehicle_Age'].map(change)\ndf.head()","af45e540":"dummies=['Gender','Vehicle_Age','Vehicle_Damage']\ndf_encoding_1=pd.get_dummies(df['Gender'])\ndf_encoding_2=pd.get_dummies(df['Vehicle_Age'])\ndf_encoding_3=pd.get_dummies(df['Vehicle_Damage'])","0b1bd5df":"df_new=pd.concat([df,df_encoding_1,df_encoding_2,df_encoding_3], axis=1)","c8a11048":"df_new.head()","9d7da550":"df_new.drop(['Gender','Vehicle_Age','Vehicle_Damage'], axis=1, inplace=True)","154f2cc3":"df_new.head()","b314222b":"df_new.info()","420a5178":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier","3e138ce6":"y=df_new['Response']\nX=df_new.drop('Response', axis=1)\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2, random_state=42)","bf6bfe01":"lor=LogisticRegression()\nlor.fit(X_train,y_train)\ny_pred_lor=lor.predict(X_test)\n","d0c5982f":"lor.score(X_test,y_test)","ec92997d":"dtc=DecisionTreeClassifier()\ndtc.fit(X_train,y_train)\ny_pred_dtc=dtc.predict(X_test)","2134adfc":"dtc.score(X_test,y_test)","336a6088":"hdtc=DecisionTreeClassifier(ccp_alpha= 1,\n max_depth= 7.0,\n min_samples_leaf= 4,\n min_samples_split= 10)\nhdtc.fit(X_train,y_train)\ny_pred_hdtc=hdtc.predict(X_test)\nhdtc.score(X_test,y_test)","26beae68":"rfc=RandomForestClassifier()\nrfc.fit(X_train,y_train)\ny_pred_rfc=rfc.predict(X_test)\nrfc.score(X_test,y_test)","4dc685e9":"import re\nxgb=XGBClassifier()\n\nxgb.fit(X_train,y_train)\ny_pred_xgb=xgb.predict(X_test)\nxgb.score(X_test,y_test)","d28f7493":"nb=GaussianNB()\nnb.fit(X_train,y_train)\ny_pred_nb=nb.predict(X_test)\nnb.score(X_test,y_test)","16d1189a":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import auc\ncm=confusion_matrix(y_test,y_pred_lor)\nsns.heatmap(cm, annot=True)","7e8b8fef":"fpr,tpr, threshold=roc_curve(y_test,y_pred_dtc)\nroc_auc=auc(fpr,tpr)","0afae524":"roc_auc","cdaca141":"fpr, tpr, thresholds = roc_curve(y_test,y_pred_dtc)\nroc_auc = auc(fpr, tpr)\n\n# Plot ROC\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b',label = 'AUC = %0.2f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.0])\nplt.ylim([-0.1,1.01])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","18889606":"**Correlation between columns**","0e99ebc5":"**Navie Bayes classification**","40438146":"**Find out unique values in columns**","680804d7":"# **Exploratory Data Analysis**","0a9a90ea":"# Plotting columns with respect to response","e1404f37":"# **ROC_AUC Score**","0f7a9e87":"**Logistic Regression**","fd5d2033":"# **Work in Progress**","81e3da91":"**RUC = 60%**","9d611c24":"Hyperparameter tuned parameters","fbb02df4":"# **Model Building**","835b06fa":"**To find out outliers**","c3df6bbf":"# **ROC curve**","78d91007":"**Categorical and numerical columns**","2a624e82":"# Statistical details about data","f8815a67":"# **Importing data**","cdd0dd5a":"**Random Forest Classification**","21642a5c":"# Univariate Plotting","8f07d977":"**Numerical data**","7d3449f7":"**Categorical data**","11e1a7ce":"**Decision Tree Classification**","6be8ce2d":"**it looks like logistic regression has better score than others**","efde090a":"# Bivariate Plotting"}}