{"cell_type":{"8830d140":"code","a54e7243":"code","1e46ecac":"code","2ce7f706":"code","bf19a6e5":"code","6b66071d":"code","de192ade":"code","bbe2af64":"code","92fdb549":"code","b1c730c4":"code","7a331ab3":"code","72408404":"code","674accb4":"code","d396d134":"code","2d90d7d5":"code","4e3b06aa":"code","fea79693":"code","280935b5":"code","de040da4":"code","2a1b12d1":"code","9cbe395a":"code","37e48bcc":"markdown","7f68bfdb":"markdown","7c68c2e5":"markdown","b81dede4":"markdown","c049fe09":"markdown","90c78035":"markdown","3d99b636":"markdown","2282bd99":"markdown","ea467298":"markdown","499361ab":"markdown","5c55883e":"markdown","07b6e0a1":"markdown","d07d980a":"markdown","1c620d38":"markdown","cdf131ea":"markdown","807d3891":"markdown","b51fa1fd":"markdown","94c5d27c":"markdown"},"source":{"8830d140":"#to install category_encoders un comment next line\n#!pip install category_encoders","a54e7243":"from warnings import simplefilter\nsimplefilter('ignore')\n\nimport pandas as pd\nimport category_encoders as ce\nfrom sklearn import preprocessing\nimport numpy as np","1e46ecac":"Degrees =pd.DataFrame({'Degree':['Masters','Bachelors','Bachelors','Masters','Phd','High school']})\nmapper = {\"Masters\":3, \"Bachelors\":1, \"Phd\":4,\"High school\":0}\nDegrees['encoding']= Degrees['Degree'].map(mapper)\nDegrees","2ce7f706":"Degrees =pd.DataFrame({'Degree':['Masters','Bachelors','Bachelors','Masters','Phd','High school']})\noece = ce.OrdinalEncoder(cols=['Degree'],return_df=True,\n                           mapping=[{'col':'Degree','mapping':mapper}])\nDegrees['encoding']= oece.fit_transform(Degrees)\nDegrees","bf19a6e5":"oe = preprocessing.OrdinalEncoder()\nDegrees['encoding']=  oe.fit_transform(Degrees['Degree'].values.reshape(-1, 1))\nDegrees","6b66071d":"le = preprocessing.LabelEncoder()\nDegrees['encoding']=  le.fit_transform(Degrees['Degree'].values)\nDegrees","de192ade":"caps =pd.DataFrame({'Capital':[\"Moscow\",\"Cairo\",\"Cairo\",\"Cairo\", \"Cairo\", \"amsterdam\",'Moscow',\"Rome\",\n                               \"London\",\"London\",\"Cairo\",\"Rome\",\"Rome\",\"London\"]})\nohe = preprocessing.OneHotEncoder(sparse =False)\nohe.fit(caps['Capital'].values.reshape(-1, 1))\nnames = list('Captial_'+ohe.categories_[0])\npd.DataFrame( ohe.transform(\n                        caps['Capital'].values.reshape(-1, 1) \n            ) ,columns=names ).head()","bbe2af64":"encoder=ce.OneHotEncoder(cols='Capital',return_df=True,use_cat_names=True)\nencoder.fit_transform(caps).head()","92fdb549":"pd.get_dummies(caps,columns=['Capital'],drop_first=True).head()","b1c730c4":"caps =pd.DataFrame({'Capital':[\"Moscow\",\"Cairo\", \"amsterdam\",'Moscow',\"Rome\",\"London\",\"Cairo\",\"Cairo\", \"Cairo\",\n                               \"London\",\"London\",\"Cairo\",\"Rome\",\"Rome\"]})\ngr = caps.groupby(\"Capital\").size().div(caps.shape[0])\n\ndisplay(gr)\ncaps['enc']=caps[\"Capital\"].map(gr)\ncaps.head(6)","7a331ab3":"caps =pd.DataFrame({'Capital':[\"Moscow\",\"Cairo\", \"amsterdam\",'Moscow',\"Rome\",\"London\",\"Cairo\",\"Cairo\", \"Cairo\",\n                               \"London\",\"London\",\"Cairo\",\"Rome\",\"Rome\",np.nan,np.nan,np.nan]})\ncer= ce.count.CountEncoder(cols='Capital',return_df=True,min_group_size=1, combine_min_nan_groups =True)\ncaps['enc'] = cer.fit_transform(caps)\ncaps","72408404":"caps =pd.DataFrame({'Capital':[\"paris\",\"tokyo\",\"Cairo\",\"Cairo\", \"amsterdam\", \"amsterdam\",'Moscow',\"Rome\",\n                               \" London \",\"Lisbon\",\" Canberra \",\"Amman\",\" Alofi\",\"Algiers\"]})\nee= ce.sum_coding.SumEncoder(cols='Capital',return_df=True)\nee.fit_transform(caps)","674accb4":"caps =pd.DataFrame({'Capital':[\"paris\",\"tokyo\",\"Cairo\",\"Cairo\", \"amsterdam\", \"amsterdam\",'Moscow',\"Rome\",\n                               \"London\",\"Lisbon\",\"Canberra\",\"Amman\",\"Alofi\",\"Algiers\"]})\nhe= ce.HashingEncoder(cols='Capital',return_df=True,n_components=4)\nhe.fit_transform(caps)","d396d134":"caps =pd.DataFrame({'Capital':[\"paris\",\"tokyo\",\"Cairo\",\"Cairo\", \"amsterdam\", \"amsterdam\",'Moscow',\"Rome\",\n                               \" London \",\"Lisbon\",\" Canberra \",\"Amman\",\" Alofi\",\"Algiers\"]})\nbne = ce.BaseNEncoder(cols=['Capital'],return_df=True,base=4)\nbne.fit_transform(caps)","2d90d7d5":"be= ce.BinaryEncoder(cols='Capital',return_df=True)\nbe.fit_transform(caps)","4e3b06aa":"caps =pd.DataFrame({'Capital':[\"paris\",\"tokyo\",\"Cairo\",\"Cairo\", \"amsterdam\", \"amsterdam\",'Moscow',\"Rome\",\n                               \"London\",\"London\",\"tokyo\",\"Amman\",\"Rome\",\"Algiers\"],\n                    'y':[10,20,30,50, 10, 20,30,40,\n                               60,10,15,30,20,30]\n                   })\nte =ce.TargetEncoder(cols='Capital',smoothing = 0) #  smoothing effect to balance categorical average vs prior. Higher value means stronger regularization. \ncaps[\"enc\"]=te.fit_transform(caps['Capital'],caps['y'],return_df=True)\ncaps","fea79693":"caps =pd.DataFrame({'Capital':[\"A\",\"A\",\"A\",\"c\", \"c\", \"c\",'c',\n                               \"B\",\"B\",\"B\",\"B\",\"D\",\"D\"],\n                    'y':[1,1,1,1,0,1,1,\n                               1,0,1,0,0,0]\n                   })\nwoe =ce.woe.WOEEncoder(cols='Capital',randomized = False,regularization= 0.0001) \ncaps[\"enc\"]=woe.fit_transform(caps['Capital'],caps['y'],return_df=True)\ncaps","280935b5":"caps =pd.DataFrame({'Capital':[\"A\",\"A\",\"A\",\"A\",\"B\",\"B\"],\n                    'y':[1,0,0,0,0,1]})\n\ng1 =caps.groupby(['Capital']).size()\ndisplay(g1)\ng2 = caps.groupby(['Capital','y'],as_index=False).size()\ng2['size'] = g2['size'] \/np.repeat( g1.values,2)\ndisplay(g2)\n","de040da4":"woe =ce.woe.WOEEncoder(cols='Capital',randomized = False,regularization= 0)\ncaps[\"enc\"] = np.expm1(woe.fit_transform(caps['Capital'],caps['y'],return_df=True))\ncaps","2a1b12d1":"KBD = preprocessing.KBinsDiscretizer(n_bins=3)\n\nx = np.random.randint(0,10,(20,1))\nnp.hstack((x,KBD.fit_transform(x).toarray()))","9cbe395a":"caps =pd.DataFrame({'Capital':[\"paris\",\"tokyo\",\"Cairo\",\"Cairo\", \"amsterdam\", \"amsterdam\",'Moscow',\"Rome\",\n                               \"London\",\"London\",\"tokyo\",\"Amman\",\"Rome\",\"Algiers\"],\n                    'y':[10,20,30,50, 10, 20,30,40,\n                               60,10,15,30,20,30]\n                   })\ncbe =ce.cat_boost.CatBoostEncoder(cols='Capital',a=1) #  smoothing effect to balance categorical average vs prior. Higher value means stronger regularization. \ncaps[\"enc\"]=cbe.fit_transform(caps['Capital'],caps['y'],return_df=True)\ncaps\n","37e48bcc":"# Resources\n\n* [Category Encoders Documentation Page](https:\/\/contrib.scikit-learn.org\/category_encoders\/)\n* [module-sklearn.preprocessing Documentation Page](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.preprocessing)\n* [catboost.ai Documentation Page for categorical features to numerical features](https:\/\/catboost.ai\/docs\/concepts\/algorithm-main-stages_cat-to-numberic.html)\n\n* **analyticsvidhya.com** Article by **SHIPRA SAXENA** : [Here\u2019s All you Need to Know About Encoding Categorical Data (with Python code) ](https:\/\/www.analyticsvidhya.com\/blog\/2020\/08\/types-of-categorical-data-encoding\/)\n\n* **heartbeat.fritz.ai** Article by **Younes Charfaoui**: [Hands-on with Feature Engineering Techniques: Encoding Categorical Variables ](https:\/\/heartbeat.fritz.ai\/hands-on-with-feature-engineering-techniques-encoding-categorical-variables-be4bc0715394)\n\n* **.geeksforgeeks.org** Article by **hemavatisabu**: [Categorical Encoding with CatBoost Encoder](https:\/\/www.geeksforgeeks.org\/categorical-encoding-with-catboost-encoder\/)","7f68bfdb":"# Label Encoding or Ordinal Encoding\nuse this categorical data encoding technique when the categorical feature is ordinal. In this case, retaining the order is important. Hence encoding should reflect the sequence.\n\n* **sklearn LabelEncoder** : Encode target labels with value between 0 and n_classes-1\n* **sklearn LabelEncoder** : Encode target labels with value between 0 and n_classes-1\n* **pandas use map**: to assign value for each category \n\n**Advantages :**\n\n* Can work well enough with tree-based algorithms.\n* Does not expand the feature space.\n\n**Limitations:**\n\n* Creates an order relationship between the categories.","7c68c2e5":"## Used Libraries\n* sklearn preprocessing :The sklearn module for data preprocessing includes Encoding, scaling, centering, normalization, binarization methods \n[sklearn preprocessing docs](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.preprocessing).\n* Category Encoders:A set of scikit-learn-style transformers for encoding categorical variables into numeric with different techniques . based on sklearn and pandes and has some enhancement in encoding.\n[Category Encoders docs](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.preprocessing).\n* pandas \n","b81dede4":"# Data Preprocessing Feature Encoding \n","c049fe09":"# Base N Encoding\n- [Base N at wiki](https:\/\/en.wikipedia.org\/wiki\/Radix)\nEncode categorical variables with selected Base N system up to 8 as we cant use base 16 as it use alpha-numeric. ","90c78035":"# Continuous Data To Intervals\nBin continuous data into intervals using KBinsDiscretizer from sklearn      \n\n","3d99b636":"# Target  Encoding\n\nIn target encoding, calculate the mean(or any statistic eg. median ,mode ) of the target variable for each category and replace the category variable with the mean value. In the case of the categorical target variables, the posterior probability of the target replaces each category\n\n**Target Encoder in category encoders:** encode categorical variables with the mean value of the target.\n\n**Advantages :**\n* Creates a relationship between categories and the target (Add extra information).\n\n**Limitations:**\n* May lead to overfitting\n* May lead to a possible loss of value if two categories have the same statistic\n\n**Leave One Out Encoder** in category encoders same as target Encoder but excludes the current row target when calculating the mean target for a level to reduce the effect of outliers.","2282bd99":"# Binary Encoding\n\n**Binary encoding** or (Base 2 Encoder) for categorical variables, similar to **onehot**, but stores categories as binary bitstrings.\n\nBinary encoding is a memory-efficient encoding than one-hot encoding as it use Binary representation for n categories\nThen binary encoding results in the only log base 2 of n features, which is = ln(n) \/ ln(2).\n\n**Advantages:**\n* Does not expand the feature space too much.\n**Limitations:**\n* It exposes the loss of information during encoding.\n* It not the human readable.","ea467298":"# CatBoost Encoder\nCatBoost is a target coding for categorical features compute each encode value as follow:\n$$\\large e_i =\\frac{ count\\_in\\_class(i)+prior}{total\\_count+a} $$\n* **count in class:** Sum of the target for specific category.\n* **prior:** the sum of the Targets for specific category divided by number of all data. \n* **total count:** category values count.\n* **a:** default value 1 as in original , but in category_encoders set as parameter a for smoothing the values of encoding","499361ab":"# Frequency Encoding\neach category encoded with it's Frequency. \n","5c55883e":"# Weight of Evidence Encoding (WOE)\n\n**analyticsvidhya.com**  Article by**KRUTHIKA_K**: [Understand Weight of Evidence and Information Value!](https:\/\/www.analyticsvidhya.com\/blog\/2021\/06\/understand-weight-of-evidence-and-information-value\/)\n\nWeight of evidence (WOE) is a technique used to encode categorical variables for classification.\n\nWOE is the natural logarithm (ln) of the probability that the target equals 1 divided by the probability of the target equals 0 for each category.\n\n**Advantages:**\n* Creates a monotonic relationship between the target and the variables.\n* Orders the categories on a \u201clogistic\u201d scale, which is natural for logistic regression.\n* We can compare the transformed variables because they are on the same scale. Therefore, it\u2019s possible to determine which one is more predictive.\n\n**Limitations:**\n* May lead to **overfitting**.\n* Not defined when the denominator is 0 (category_encoders handle it using regularization parameter)","07b6e0a1":"# Hash Encoder\nHashing is the transformation of arbitrary size input in the form of a fixed-size value.\n\nHash encoder like one-hot encoding for n categories return n columns but can use n_components to adjust the number of columns.\nbut when reduce the data to lower dimensions we may loss important information.","d07d980a":"# Probability Ratio Encoding\nThis encoding is suitable for classification problems only, where the target is binary.\nIt\u2019s similar to WOE, but we don\u2019t apply the natural log (ln) just p(1)\/p(0) for each category.\n\n**Advantages** and **Limitations** same as woe","1c620d38":"# Effect Encoding\nThis encoding technique is also known as **Deviation Encoding** or **Sum Encoding**.\n\nEffect encoding is almost similar to **dummy encoding**, with a little difference: \n* dummy coding, we use 0 and 1 to represent the data.\n* effect encoding, use three values i.e. 1,0, and -1.","cdf131ea":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-Preprocessing-Feature-Encoding\" data-toc-modified-id=\"Data-Preprocessing-Feature-Encoding-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Data Preprocessing Feature Encoding<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Used-Libraries\" data-toc-modified-id=\"Used-Libraries-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;<\/span>Used Libraries<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Label-Encoding-or-Ordinal-Encoding\" data-toc-modified-id=\"Label-Encoding-or-Ordinal-Encoding-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Label Encoding or Ordinal Encoding<\/a><\/span><\/li><li><span><a href=\"#One-hot-Encoding-and-Dummy-Encoding\" data-toc-modified-id=\"One-hot-Encoding-and-Dummy-Encoding-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>One hot Encoding and Dummy Encoding<\/a><\/span><\/li><li><span><a href=\"#Frequency-Encoding\" data-toc-modified-id=\"Frequency-Encoding-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Frequency Encoding<\/a><\/span><\/li><li><span><a href=\"#Count-Encoding\" data-toc-modified-id=\"Count-Encoding-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Count Encoding<\/a><\/span><\/li><li><span><a href=\"#Effect-Encoding\" data-toc-modified-id=\"Effect-Encoding-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Effect Encoding<\/a><\/span><\/li><li><span><a href=\"#Hash-Encoder\" data-toc-modified-id=\"Hash-Encoder-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>Hash Encoder<\/a><\/span><\/li><li><span><a href=\"#Base-N-Encoding\" data-toc-modified-id=\"Base-N-Encoding-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;<\/span>Base N Encoding<\/a><\/span><\/li><li><span><a href=\"#Binary-Encoding\" data-toc-modified-id=\"Binary-Encoding-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;<\/span>Binary Encoding<\/a><\/span><\/li><li><span><a href=\"#Target--Encoding\" data-toc-modified-id=\"Target--Encoding-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;<\/span>Target  Encoding<\/a><\/span><\/li><li><span><a href=\"#Weight-of-Evidence-Encoding-(WOE)\" data-toc-modified-id=\"Weight-of-Evidence-Encoding-(WOE)-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;<\/span>Weight of Evidence Encoding (WOE)<\/a><\/span><\/li><li><span><a href=\"#Probability-Ratio-Encoding\" data-toc-modified-id=\"Probability-Ratio-Encoding-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;<\/span>Probability Ratio Encoding<\/a><\/span><\/li><li><span><a href=\"#Continuous-Data-To-Intervals\" data-toc-modified-id=\"Continuous-Data-To-Intervals-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;<\/span>Continuous Data To Intervals<\/a><\/span><\/li><li><span><a href=\"#CatBoost-Encoder\" data-toc-modified-id=\"CatBoost-Encoder-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;<\/span>CatBoost Encoder<\/a><\/span><\/li><li><span><a href=\"#Resources\" data-toc-modified-id=\"Resources-15\"><span class=\"toc-item-num\">15&nbsp;&nbsp;<\/span>Resources<\/a><\/span><\/li><\/ul><\/div>","807d3891":"instead of write code divide each p(1)\/p(0) we and map each category will use woe and inverse the log transformation. \nand also can handle dividing by zero","b51fa1fd":"# One hot Encoding and Dummy Encoding\nwhen the features are nominal(do not have any order). In one hot encoding, for each level of a categorical feature.\n\nDummy coding scheme is similar to one-hot encoding both transforms the categorical variable into a set of binary variables In the case of one-hot encoding, for N categories in a variable, it uses N binary variables. The dummy encoding is a small improvement over one-hot-encoding. Dummy encoding uses N-1 features to represent N labels\/categories.\n<img src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQqvOR5qXM_LK_liDezhs6xMkQ3wTZllQNcWw&usqp=CAU\" width=\"400\" height=\"400\" \/>\n\n* **pd get_dummies:** Encode categorical features as a one-hot numeric array and use `drop_first=True` to drop one category.\n* **sklearn OneHotEncoder:** Encode categorical features as a one-hot numeric array.\n\n\n**Limitations:**\n\n* A large number of levels are present in data. If there are multiple categories in a feature variable in such a case we need a similar number of dummy variables to encode the data. \n* Does not add extra information while encoding.\n* low variability of rare categories features.\n","94c5d27c":"# Count Encoding\nFor a given categorical feature, replace the names of the groups with the group counts.\n* **pandas:** use group by and same as freq Encode\n* **Count Encoder:** in category_encoders"}}