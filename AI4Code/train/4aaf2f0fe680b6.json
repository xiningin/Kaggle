{"cell_type":{"dc24ba18":"code","39a9c41f":"code","2935e7c9":"code","14d293d5":"code","761cfe9a":"code","37e019be":"code","b025e92e":"code","da8bf747":"code","fbe38c90":"code","3eaa04b2":"code","c9ebe548":"code","ecaacf6a":"code","3741c8c2":"code","7d1d2339":"code","ae3ca1d8":"code","cf52d1b1":"code","100c3ff2":"code","ffd19801":"code","23fa4deb":"code","c5551627":"code","af559372":"code","42a4cd52":"code","18574e9c":"code","43abfd28":"code","25f62a98":"code","f28d7b84":"code","44d87f65":"code","ec4edb7a":"code","f83c3924":"code","32f91d53":"code","1c2333f5":"code","c2189121":"code","28345587":"code","66cd9429":"code","49fa3b48":"code","ac44b5bc":"code","4a17508e":"code","3e6e2b16":"code","5c8d1732":"code","1477d15c":"code","1f1cc6fa":"code","14560fe3":"code","475b86f2":"code","c491df84":"code","ebeace5c":"code","efdcb82b":"code","32c31f05":"code","a0bad0c4":"code","05b8a5d5":"code","775ba5e3":"code","f7684c23":"code","a07c403e":"code","4d963275":"code","0db985f2":"code","163b1625":"code","52347501":"code","9758e0fc":"code","64ff2e5f":"code","cb4d8e78":"code","9b3b194d":"code","6174b359":"code","ea483d70":"code","882872fd":"code","aedb7fae":"code","8c375186":"markdown","f8891c06":"markdown","f3f9f6b2":"markdown","1ad21fca":"markdown","da21ecb6":"markdown","34bba3cc":"markdown","c7935f2f":"markdown","e0653887":"markdown","4d8b06f2":"markdown","3645c8b6":"markdown","38d18b35":"markdown","013ccb1f":"markdown","560f4567":"markdown","5197e83f":"markdown"},"source":{"dc24ba18":"import os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom wordcloud import WordCloud,STOPWORDS\nfrom bs4 import BeautifulSoup\nimport re,string,unicodedata\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix, plot_confusion_matrix, plot_roc_curve, plot_precision_recall_curve\nfrom xgboost.sklearn import XGBClassifier\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.layers import Dense,Input, Embedding,LSTM,Dropout,Conv1D, MaxPooling1D, GlobalMaxPooling1D,Dropout,Bidirectional,Flatten,BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import plot_model\nimport transformers\nimport tokenizers\n","39a9c41f":"data=pd.read_csv('..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')\ndata.head()","2935e7c9":"data.info()","14d293d5":"data.describe() #descriptive statistics","761cfe9a":"null_values = data.isnull().sum() #identifying missing values","37e019be":"null_values.index[0]","b025e92e":"print('There are {} missing values for {} and {} missing values for {}.'.format(null_values[0],null_values.index[0],null_values[1],null_values.index[1]))","da8bf747":"num_duplicates = data.duplicated().sum() #identify duplicates\nprint('There are {} duplicate reviews present in the dataset'.format(num_duplicates))","fbe38c90":"#view duplicate reviews\nreview = data['review']\nduplicated_review = data[review.isin(review[review.duplicated()])].sort_values(\"review\")\nduplicated_review.head()","3eaa04b2":"#drop duplicate reviews\ndata.drop_duplicates(inplace = True)","c9ebe548":"print('The dataset contains {} rows and {} columns after removing duplicates'.format(data.shape[0],data.shape[1]))","ecaacf6a":"stop = stopwords.words('english')\nwl = WordNetLemmatizer()","3741c8c2":"mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n           \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n           \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \n           \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \n           \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n           \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \n           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\n           \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n           \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n           \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n           \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n           \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n           \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n           \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n           \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n           \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\n           \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \n           \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \n           \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \n           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \n           \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n           \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n           \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n           \"what'll\": \"what will\", \"what'll've\": \"what will have\",\"what're\": \"what are\",  \n           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \n           \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n           \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n           \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n           \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n           \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \n           \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \n           \"you're\": \"you are\", \"you've\": \"you have\" }","7d1d2339":"#function to clean data\ndef clean_text(text,lemmatize = True):\n    soup = BeautifulSoup(text, \"html.parser\") #remove html tags\n    text = soup.get_text()\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")]) #expanding chatwords and contracts clearing contractions\n    emoji_clean= re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_clean.sub(r'',text)\n    text = re.sub(r'\\.(?=\\S)', '. ',text) #add space after full stop\n    text = re.sub(r'http\\S+', '', text) #remove urls\n    text = \"\".join([word.lower() for word in text if word not in string.punctuation]) #remove punctuation\n    #tokens = re.split('\\W+', text) #create tokens\n    if lemmatize:\n        text = \" \".join([wl.lemmatize(word) for word in text.split() if word not in stop and word.isalpha()]) #lemmatize\n    else:\n        text = \" \".join([word for word in text.split() if word not in stop and word.isalpha()]) \n    return text","ae3ca1d8":"data_copy = data.copy()","cf52d1b1":"data['review']=data['review'].apply(clean_text,lemmatize = True)","100c3ff2":"#converting target variable to numeric labels\ndata.sentiment = [ 1 if each == \"positive\" else 0 for each in data.sentiment]","ffd19801":"#after converting labels\ndata.head()","23fa4deb":"### Count Plot\nsns.set(style = \"whitegrid\" , font_scale = 1.2)\nsns.countplot(data.sentiment,palette = ['green','red'],order = [1,0])\nplt.xticks(ticks = np.arange(2),labels = ['positive','negative'])\nplt.title('Target count for IMBD reviews')\nplt.show()","c5551627":"print('Positive reviews are', (round(data['sentiment'].value_counts()[0])),'i.e.', round(data['sentiment'].value_counts()[0]\/len(data) * 100,2), '% of the dataset')\nprint('Negative reviews are', (round(data['sentiment'].value_counts()[1])),'i.e.',round(data['sentiment'].value_counts()[1]\/len(data) * 100,2), '% of the dataset')","af559372":"#word cloud for positive reviews\npositive_data = data[data.sentiment == 1]['review']\npositive_data_string = ' '.join(positive_data)\nplt.figure(figsize = (20,20))\nwc = WordCloud(max_words = 2000, width=1200, height=600,background_color=\"white\").generate(positive_data_string)\nplt.imshow(wc , interpolation = 'bilinear')\nplt.axis('off')\nplt.title('Word cloud for positive reviews',fontsize = 20)\nplt.show()","42a4cd52":"#word cloud for negative reviews\nnegative_data = data[data.sentiment == 0]['review']\nnegative_data_string = ' '.join(negative_data)\nplt.figure(figsize = (20,20))\nwc = WordCloud(max_words = 2000, width=1200, height=600,background_color=\"white\").generate(negative_data_string)\nplt.imshow(wc , interpolation = 'bilinear')\nplt.axis('off')\nplt.title('Word cloud for negative reviews',fontsize = 20)\nplt.show()","18574e9c":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,8))\ntext_len=positive_data.str.len()\nax1.hist(text_len,color='green')\nax1.set_title('Positive Reviews')\nax1.set_xlabel('Number of Characters')\nax1.set_ylabel('Count')\ntext_len=negative_data.str.len()\nax2.hist(text_len,color='red')\nax2.set_title('Negative Reviews')\nax2.set_xlabel('Number of Characters')\nax2.set_ylabel('Count')\nfig.suptitle('Number of characters in texts')\nplt.show()","43abfd28":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,8))\n\ntext_len=positive_data.str.split().map(lambda x: len(x))\nax1.hist(text_len,color='green')\nax1.set_title('Positive Reviews')\nax1.set_xlabel('Number of Words')\nax1.set_ylabel('Count')\ntext_len=negative_data.str.split().map(lambda x: len(x))\nax2.hist(text_len,color='red')\nax2.set_title('Negative Reviews')\nax2.set_xlabel('Number of Words')\nax2.set_ylabel('Count')\nfig.suptitle('Number of words in texts')\nplt.show()","25f62a98":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,10))\nword = positive_data.str.split().apply(lambda x : len(x) )\nsns.distplot(word, ax=ax1,color='green')\nax1.set_title('Positive Reviews')\nax1.set_xlabel('Number of words per review')\nword = negative_data.str.split().apply(lambda x :len(x) )\nsns.distplot(word,ax=ax2,color='red')\nax2.set_title('Negative Reviews')\nax2.set_xlabel('Number of words per review')\nfig.suptitle('Distribution of number of words per reviews')\nplt.show()","f28d7b84":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,10))\nword = positive_data.str.split().apply(lambda x : [len(i) for i in x] )\nsns.distplot(word.map(lambda x: np.mean(x)), ax=ax1,color='green')\nax1.set_title('Positive Reviews')\nax1.set_xlabel('Average word length per review')\nword = negative_data.str.split().apply(lambda x : [len(i) for i in x] )\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='red')\nax2.set_title('Negative Reviews')\nax2.set_xlabel('Average word length per review')\nfig.suptitle('Distribution of average word length in each review')\nplt.show()","44d87f65":"def get_corpus(text):\n    words = []\n    for i in text:\n        for j in i.split():\n            words.append(j.strip())\n    return words\ncorpus = get_corpus(data.review)\ncorpus[:5]","ec4edb7a":"from collections import Counter\ncounter = Counter(corpus)\nmost_common = counter.most_common(10)\nmost_common = pd.DataFrame(most_common,columns = ['corpus','countv'])\nmost_common","f83c3924":"most_common = most_common.sort_values('countv')","32f91d53":"plt.figure(figsize =(10,10))\nplt.yticks(range(len(most_common)), list(most_common.corpus))\nplt.barh(range(len(most_common)), list(most_common.countv),align='center',color = 'blue')\nplt.title('Most common words in the dataset')\nplt.show()\n","1c2333f5":"def get_ngrams(review, n, g):\n    vec = CountVectorizer(ngram_range=(g, g)).fit(review)\n    bag_of_words = vec.transform(review) #sparse matrix of count_vectorizer\n    sum_words = bag_of_words.sum(axis=0) #total number of words\n    sum_words = np.array(sum_words)[0].tolist() #convert to list\n    words_freq = [(word, sum_words[idx]) for word, idx in vec.vocabulary_.items()] #get word freqency for word location in count vec\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True) #key is used to perform sorting using word_freqency \n    return words_freq[:n]","c2189121":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(30,15))\nuni_positive = get_ngrams(positive_data,20,1)\nuni_positive = dict(uni_positive)\ntemp = pd.DataFrame(list(uni_positive.items()), columns = [\"Common_words\" , 'Count'])\nsns.barplot(data = temp, x=\"Count\", y=\"Common_words\", orient='h',ax = ax1)\nax1.set_title('Positive reviews')\nuni_negative = get_ngrams(negative_data,20,1)\nuni_negative = dict(uni_negative)\ntemp = pd.DataFrame(list(uni_negative.items()), columns = [\"Common_words\" , 'Count'])\nsns.barplot(data = temp, x=\"Count\", y=\"Common_words\", orient='h',ax = ax2)\nax2.set_title('Negative reviews')\nfig.suptitle('Unigram analysis for positive and negative reviews')\nplt.show()\n","28345587":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(30,15))\nbi_positive = get_ngrams(positive_data,20,2)\nbi_positive = dict(bi_positive)\ntemp = pd.DataFrame(list(bi_positive.items()), columns = [\"Common_words\" , 'Count'])\nsns.barplot(data = temp, x=\"Count\", y=\"Common_words\", orient='h',ax = ax1)\nax1.set_title('Positive reviews')\nbi_negative = get_ngrams(negative_data,20,2)\nbi_negative = dict(bi_negative)\ntemp = pd.DataFrame(list(bi_negative.items()), columns = [\"Common_words\" , 'Count'])\nsns.barplot(data = temp, x=\"Count\", y=\"Common_words\", orient='h',ax = ax2)\nax2.set_title('Negative reviews')\nfig.suptitle('Bigram analysis for positive and negative reviews')\nplt.show()","66cd9429":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(30,15))\ntri_positive = get_ngrams(positive_data,20,3)\ntri_positive = dict(tri_positive)\ntemp = pd.DataFrame(list(tri_positive.items()), columns = [\"Common_words\" , 'Count'])\nsns.barplot(data = temp, x=\"Count\", y=\"Common_words\", orient='h',ax = ax1)\nax1.set_title('Positive reviews')\ntri_negative = get_ngrams(negative_data,20,3)\ntri_negative = dict(tri_negative)\ntemp = pd.DataFrame(list(tri_negative.items()), columns = [\"Common_words\" , 'Count'])\nsns.barplot(data = temp, x=\"Count\", y=\"Common_words\", orient='h',ax = ax2)\nax2.set_title('Negative reviews')\nfig.suptitle('Trigram analysis for positive and negative reviews')\nplt.show()","49fa3b48":"#splitting into train and test\ntrain, test= train_test_split(data, test_size=0.2, random_state=42)\nXtrain, ytrain = train['review'], train['sentiment']\nXtest, ytest = test['review'], test['sentiment']","ac44b5bc":"#Vectorizing data\n\ntfidf_vect = TfidfVectorizer() #tfidfVectorizer\nXtrain_tfidf = tfidf_vect.fit_transform(Xtrain)\nXtest_tfidf = tfidf_vect.transform(Xtest)\n\n\ncount_vect = CountVectorizer() # CountVectorizer\nXtrain_count = count_vect.fit_transform(Xtrain)\nXtest_count = count_vect.transform(Xtest)","4a17508e":"lr = LogisticRegression()\nlr.fit(Xtrain_tfidf,ytrain)\np1=lr.predict(Xtest_tfidf)\ns1=accuracy_score(ytest,p1)\nprint(\"Logistic Regression Accuracy :\", \"{:.2f}%\".format(100*s1))\nplot_confusion_matrix(lr, Xtest_tfidf, ytest,cmap = 'Blues')\nplt.grid(False)","3e6e2b16":"mnb= MultinomialNB()\nmnb.fit(Xtrain_tfidf,ytrain)\np2=mnb.predict(Xtest_tfidf)\ns2=accuracy_score(ytest,p2)\nprint(\"Multinomial Naive Bayes Classifier Accuracy :\", \"{:.2f}%\".format(100*s2))\nplot_confusion_matrix(mnb, Xtest_tfidf, ytest,cmap = 'Blues')\nplt.grid(False)","5c8d1732":"linear_svc = LinearSVC(penalty='l2',loss = 'hinge')\nlinear_svc.fit(Xtrain_tfidf,ytrain)\np3=linear_svc.predict(Xtest_tfidf)\ns3=accuracy_score(ytest,p3)\nprint(\"Linear Support Vector Classifier Accuracy :\", \"{:.2f}%\".format(100*s3))\nplot_confusion_matrix(linear_svc, Xtest_tfidf, ytest,cmap = 'Blues')\nplt.grid(False)","1477d15c":"xgbo = XGBClassifier()\nxgbo.fit(Xtrain_tfidf,ytrain)\np4=xgbo.predict(Xtest_tfidf)\ns4=accuracy_score(ytest,p4)\nprint(\"XGBoost Accuracy :\", \"{:.2f}%\".format(100*s4))\nplot_confusion_matrix(xgbo, Xtest_tfidf, ytest, cmap = 'Blues')\nplt.grid(False)","1f1cc6fa":"def plotLearningCurve(history,epochs):\n  epochRange = range(1,epochs+1)\n  fig , ax = plt.subplots(1,2,figsize = (10,5))\n  \n  ax[0].plot(epochRange,history.history['accuracy'],label = 'Training Accuracy')\n  ax[0].plot(epochRange,history.history['val_accuracy'],label = 'Validation Accuracy')\n  ax[0].set_title('Training and Validation accuracy')\n  ax[0].set_xlabel('Epoch')\n  ax[0].set_ylabel('Accuracy')\n  ax[0].legend()\n  ax[1].plot(epochRange,history.history['loss'],label = 'Training Loss')\n  ax[1].plot(epochRange,history.history['val_loss'],label = 'Validation Loss')\n  ax[1].set_title('Training and Validation loss')\n  ax[1].set_xlabel('Epoch')\n  ax[1].set_ylabel('Loss')\n  ax[1].legend()\n  fig.tight_layout()\n  plt.show()","14560fe3":"#splitting into train and test\ndata_copy['review']=data_copy['review'].apply(clean_text,lemmatize = False)\n#converting target variable to numerical value\ndata_copy.sentiment = [ 1 if each == \"positive\" else 0 for each in data_copy.sentiment]\ntrain, test= train_test_split(data_copy, test_size=0.2, random_state=42)\nXtrain, ytrain = train['review'], train['sentiment']\nXtest, ytest = test['review'], test['sentiment']","475b86f2":"#set up the tokenizer\nMAX_VOCAB_SIZE = 10000\ntokenizer = Tokenizer(num_words = MAX_VOCAB_SIZE,oov_token=\"<oov>\")\ntokenizer.fit_on_texts(Xtrain)\nword_index = tokenizer.word_index\n#print(word_index)\nV = len(word_index)\nprint(\"Vocabulary of the dataset is : \",V)\n","c491df84":"##create sequences of reviews\nseq_train = tokenizer.texts_to_sequences(Xtrain)\nseq_test =  tokenizer.texts_to_sequences(Xtest)","ebeace5c":"#choice of maximum length of sequences\nseq_len_list = [len(i) for i in seq_train + seq_test]\n\n#if we take the direct maximum then\nmax_len=max(seq_len_list)\nprint('Maximum length of sequence in the list: {}'.format(max_len))","efdcb82b":"# when setting the maximum length of sequence, variability around the average is used.\nmax_seq_len = np.mean(seq_len_list) + 2 * np.std(seq_len_list)\nmax_seq_len = int(max_seq_len)\nprint('Maximum length of the sequence when considering data only two standard deviations from average: {}'.format(max_seq_len))","32c31f05":"perc_covered = np.sum(np.array(seq_len_list) < max_seq_len) \/ len(seq_len_list)*100\nprint('The above calculated number coveres approximately {} % of data'.format(np.round(perc_covered,2)))\n","a0bad0c4":"#create padded sequences\npad_train=pad_sequences(seq_train,truncating = 'post', padding = 'pre',maxlen=max_seq_len)\npad_test=pad_sequences(seq_test,truncating = 'post', padding = 'pre',maxlen=max_seq_len)","05b8a5d5":"#Splitting training set for validation purposes\nXtrain,Xval,ytrain,yval=train_test_split(pad_train,ytrain,\n                                             test_size=0.2,random_state=10)","775ba5e3":"def lstm_model(Xtrain,Xval,ytrain,yval,V,D,maxlen,epochs):\n\n    print(\"----Building the model----\")\n    i = Input(shape=(maxlen,))\n    x = Embedding(V + 1, D,input_length = maxlen)(i)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n    x = Conv1D(32,5,activation = 'relu')(x)\n    x = Dropout(0.3)(x)\n    x = MaxPooling1D(2)(x)\n    x = Bidirectional(LSTM(128,return_sequences=True))(x)\n    x = LSTM(64)(x)\n    x = Dropout(0.5)(x)\n    x = Dense(1, activation='sigmoid')(x)\n    model = Model(i, x)\n    model.summary()\n\n    #Training the LSTM\n    print(\"----Training the network----\")\n    model.compile(optimizer= Adam(0.0005),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n    \n#     #early_stop = EarlyStopping(monitor='val_accuracy', \n#                                mode='min', \n#                                patience = 2 )\n#     #checkpoints= ModelCheckpoint(filepath='.\/',\n#                             monitor=\"val_accuracy\",\n#                             verbose=0,\n#                             save_best_only=True\n#                            )\n  #  callbacks = [checkpoints,early_stop]\n    r = model.fit(Xtrain,ytrain, \n                  validation_data = (Xval,yval), \n                  epochs = epochs, \n                  verbose = 2,\n                  batch_size = 32)\n                  #callbacks = callbacks\n    print(\"Train score:\", model.evaluate(Xtrain,ytrain))\n    print(\"Validation score:\", model.evaluate(Xval,yval))\n    n_epochs = len(r.history['loss'])\n    \n    return r,model,n_epochs \n","f7684c23":"D = 64 #embedding dims\nepochs = 5\nr,model,n_epochs = lstm_model(Xtrain,Xval,ytrain,yval,V,D,max_seq_len,epochs)","a07c403e":"#Plot accuracy and loss\nplotLearningCurve(r,n_epochs)","4d963275":"print(\"Evaluate Model Performance on Test set\")\nresult = model.evaluate(pad_test,ytest)\nprint(dict(zip(model.metrics_names, result)))","0db985f2":"#Generate predictions for the test dataset\nypred = model.predict(pad_test)\nypred = ypred>0.5\n#Get the confusion matrix\ncf_matrix = confusion_matrix(ytest, ypred)\nsns.heatmap(cf_matrix,annot = True,fmt ='g', cmap='Blues')\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nplt.show()","163b1625":"train, test= train_test_split(data_copy, test_size=0.2, random_state=42)\nXtrain, ytrain = train['review'], train['sentiment']\nXtest, ytest = test['review'], test['sentiment']\n#splitting the train set into train and validation\nXtrain,Xval,ytrain,yval=train_test_split(Xtrain,ytrain,\n                                             test_size=0.2,random_state=10)","52347501":"#Perform tokenization\n# automatically download the vocab used during pretraining or fine-tuning a given model,use from_pretrained() method\ntokenizer = transformers.AutoTokenizer.from_pretrained('distilbert-base-uncased')\n","9758e0fc":"#pass our texts to the tokenizer. \nXtrain_enc = tokenizer(Xtrain.tolist(), max_length=max_seq_len, \n                         truncation=True, padding='max_length', \n                         add_special_tokens=True, return_tensors='np') #return numpy object\nXval_enc = tokenizer(Xval.tolist(), max_length=max_seq_len, \n                         truncation=True, padding='max_length', \n                         add_special_tokens=True, return_tensors='np') #return numpy object\nXtest_enc = tokenizer(Xtest.tolist(), max_length=max_seq_len, \n                         truncation=True, padding='max_length', \n                         add_special_tokens=True, return_tensors='np') #return numpy object\n\n\n","64ff2e5f":"#preparing our datasets\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(Xtrain_enc),\n    ytrain\n))\nval_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(Xval_enc),\n    yval\n))\ntest_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(Xtest_enc),\n    ytest\n))","cb4d8e78":"def bert_model(train_dataset,val_dataset,transformer,max_len,epochs):\n    print(\"----Building the model----\")\n    input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n    attention_mask = Input(shape=(max_len,),dtype=tf.int32,name = 'attention_mask') #attention mask\n    sequence_output = transformer(input_ids,attention_mask)[0]\n    cls_token = sequence_output[:, 0, :]\n    x = Dense(512, activation='relu')(cls_token)\n    x = Dropout(0.1)(x)\n    y = Dense(1, activation='sigmoid')(x)\n    model = Model(inputs=[input_ids,attention_mask], outputs=y)\n    model.summary()\n    model.compile(Adam(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    r = model.fit(train_dataset.batch(32),batch_size = 32,\n                  validation_data = val_dataset.batch(32),epochs = epochs)\n                  #callbacks = callbacks\n    print(\"Train score:\", model.evaluate(train_dataset.batch(32)))\n    print(\"Validation score:\", model.evaluate(val_dataset.batch(32)))\n    n_epochs = len(r.history['loss'])\n    \n    return r,model,n_epochs ","9b3b194d":"transformer = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased')","6174b359":"epochs = 2\nmax_len = max_seq_len\nr,model,n_epochs = bert_model(train_dataset,val_dataset,transformer,max_len,epochs)","ea483d70":"#Plot accuracy and loss\nplotLearningCurve(r,n_epochs)","882872fd":"print(\"Evaluate Model Performance on Test set\")\nresult = model.evaluate(test_dataset.batch(32))\nprint(dict(zip(model.metrics_names, result)))","aedb7fae":"#Generate predictions for the test dataset\nypred = model.predict(test_dataset.batch(32))\nypred = ypred>0.5\n#Get the confusion matrix\ncf_matrix = confusion_matrix(ytest, ypred)\nsns.heatmap(cf_matrix,annot = True,fmt ='g', cmap='Blues')\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nplt.show()","8c375186":"### ","f8891c06":"### LSTM","f3f9f6b2":"So we can use this number for our maxlen parameter.","1ad21fca":"### XGboost ","da21ecb6":"### Multinomial Naive Bayes","34bba3cc":"# **5. Predictive Modelling using Deep Learning** ","c7935f2f":"### Linear SVM","e0653887":"# **2. Data Extraction and Cleaning**","4d8b06f2":"# **4. Predictive Modelling using Machine Learning** ","3645c8b6":"# **3. Exploratory data analysis** ","38d18b35":"# **1. Importing Libraries**","013ccb1f":"### BERT\n","560f4567":"### Logistic Regression","5197e83f":"### Dataset Information\n\nIMDB dataset contains 50K movie reviews for natural language processing i.e.  for binary sentiment classification. The dataset contains two columns - review and sentiment to perform the sentimental analysis.\n\n### Problem Statement\nCorrectly classify the positive and negative sentiments for IMDB reviews.\n\n### Overview\nIn this notebook, I perform cleaning on the dataset, exploratory data analysis and predicitive modelling using machine learning and deep learning algorithms. For machine learning I use Logistic Regression, Multinomial Naive Bayes, Linear SVM and XGBoost.\nFor Deep Learning, I use a combination of CNN+LSTM and BERT. The maximum accuracy achieved using BERT is around 90%.\n\n### References\nThis kernel includes some ideas and parts of code from the following references:\n1. https:\/\/www.kaggle.com\/madz2000\/sentiment-analysis-cleaning-eda-bert-88-acc\/output\n2. https:\/\/www.kaggle.com\/aditya6040\/7-models-on-imdb-dataset-best-score-88-2\n3. https:\/\/www.kaggle.com\/faressayah\/sentiment-model-with-tensorflow-transformers\n4. https:\/\/www.kaggle.com\/sravyaysk\/imdb-movie-review-classification-using-tensorflow\n5. https:\/\/www.kaggle.com\/colearninglounge\/nlp-data-preprocessing-and-cleaning\n6. https:\/\/huggingface.co\/transformers\/master\/custom_datasets.html\n\nPlease upvote if you like my work.Thank you all of you !"}}