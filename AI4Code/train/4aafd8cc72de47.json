{"cell_type":{"db8552af":"code","ecbbf973":"code","602d427b":"code","175f7e9a":"code","24d36216":"code","e9163d64":"code","022510cd":"code","59b695b0":"code","cca6ccb6":"code","11b37777":"code","3fccdd12":"code","13127b98":"code","0ab20940":"code","4570cfd2":"code","b9efc719":"code","5ae2c8b5":"code","cb6b8412":"code","e0b70706":"code","d0f2475c":"code","01ad9ceb":"code","91ec5851":"code","348a46c7":"code","81b73679":"code","7365180b":"code","d5b9df20":"code","299dbf70":"code","40f3621e":"code","d889cf9e":"code","f2afd576":"code","1785f9f2":"code","4ec9c298":"code","1f029281":"code","f88259f9":"code","d7db5faa":"code","fc5cca1a":"code","a9950f3b":"code","072fd883":"code","626407e7":"code","e23dea70":"code","2a3dbf37":"code","de428e3f":"code","602b558a":"code","70a8eb16":"code","47bff025":"code","e454c705":"code","4d318dca":"code","88442697":"code","f192659c":"code","db5847bf":"code","bfb433e8":"code","80e40131":"code","f5457c91":"code","f2d4c9df":"code","42a72920":"code","ef3e3685":"code","eb56a3f7":"code","35de078b":"code","d1e3e3dc":"code","209fbb24":"code","9e16a3ff":"code","d230d0cc":"code","c775999c":"code","11bad9cc":"code","55905a76":"code","a3911c19":"code","5c3acbb9":"code","441c269d":"code","f0bfefbf":"code","75d7d307":"code","c8ca5aac":"code","d03a0cf5":"code","e0759cb7":"code","c4a07e41":"code","b4cbbb11":"code","25c78936":"code","45c2a666":"code","08c6217d":"code","6b3019af":"code","44adc89f":"code","148094b3":"code","e737eaff":"code","a757047f":"code","4d94319f":"code","377c1712":"code","0096fe6c":"code","c32c12e8":"code","3f69fead":"code","450120cd":"code","c7fc74e1":"code","fbc49b1a":"code","779e8654":"code","145abd2f":"code","e2998fac":"code","b50d2367":"markdown","3ebfc60e":"markdown","f904c446":"markdown","c1184b4d":"markdown","3cabcd9d":"markdown","a0eccc7f":"markdown","ff58bc02":"markdown","880d75ee":"markdown","f1b1ecbb":"markdown","1bbddff9":"markdown","908a847b":"markdown","790e3910":"markdown","595440de":"markdown","87f6de55":"markdown","0e50c665":"markdown","e85ddf27":"markdown","35ab82de":"markdown","6ebc5fb6":"markdown","300f3ebd":"markdown","4f0579e2":"markdown","8357dd41":"markdown","b1efd6ba":"markdown","41cde93a":"markdown","76d5eb4b":"markdown","38c609d2":"markdown","57391e12":"markdown","fe124f05":"markdown","678c0f18":"markdown","4e481602":"markdown","f8f24f81":"markdown","7acc94c4":"markdown","520cdfc8":"markdown","23c9ef5a":"markdown","4c46c296":"markdown","d5d277e0":"markdown","26b95950":"markdown","407e149d":"markdown","abe3abd0":"markdown","a8b3e867":"markdown","ff15174a":"markdown","17f0a704":"markdown","b58dfb39":"markdown","2586152c":"markdown","869f7785":"markdown","76ba0724":"markdown","021fc50f":"markdown","bd546c2c":"markdown","5da59b15":"markdown","65da0d7c":"markdown","93ab67f2":"markdown","bf549807":"markdown","3cfcec31":"markdown","e7258433":"markdown","65eb28fe":"markdown","778f9dd3":"markdown","d1056d80":"markdown","ed00a2d0":"markdown","c40d2223":"markdown","ebc5c684":"markdown","d6f4195e":"markdown","3490f1ff":"markdown","77b43001":"markdown","7d997434":"markdown","ae7fbed6":"markdown","038963fa":"markdown","439abcba":"markdown","4a7467f8":"markdown","a5563df4":"markdown","bfd91fc8":"markdown","91eef48c":"markdown","901b6ff6":"markdown","f8fa0f9c":"markdown","f1db03f8":"markdown"},"source":{"db8552af":"# Import the Libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB as gnb\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve, auc\n\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn import tree\nfrom os import system\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.preprocessing import scale\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve, auc\nfrom scipy.stats import zscore\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import model_selection\nfrom sklearn import preprocessing\nfrom sklearn import svm\nfrom sklearn.preprocessing import MinMaxScaler\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation, Dropout\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn import metrics\nfrom sklearn.neighbors import KNeighborsClassifier","ecbbf973":"# seed for reproducing same results\nseed = 5\nnp.random.seed(seed)","602d427b":"# loading the dataset\n\norg_data = pd.read_csv('..\/input\/Data - Parkinsons.csv')\n\nparkinsons_data = org_data.copy()","175f7e9a":"parkinsons_data.head()","24d36216":"parkinsons_data.tail()","e9163d64":"parkinsons_data.info()","022510cd":"parkinsons_data.shape","59b695b0":"def draw_missing_data_table(df):\n    total = df.isnull().sum().sort_values(ascending=False)\n    percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    return missing_data\n\ndraw_missing_data_table(parkinsons_data)","cca6ccb6":"# people affected with PD in the dataset \nparkinsons_data.status.value_counts()","11b37777":"# skewness along the index axis \nparkinsons_data.skew(axis = 0, skipna = True)","3fccdd12":"parkinsons_data.describe().T","13127b98":"parkinsons_data.hist(figsize=(28,28))","0ab20940":"parkinsons_data.hist(bins=50, figsize=(28,28))","4570cfd2":"fig, ax = plt.subplots(1,3,figsize=(16,8)) \nsns.distplot(parkinsons_data['MDVP:Flo(Hz)'],ax=ax[0]) \nsns.distplot(parkinsons_data['MDVP:Fo(Hz)'],ax=ax[1]) \nsns.distplot(parkinsons_data['MDVP:Fhi(Hz)'],ax=ax[2])","b9efc719":"fig, ax = plt.subplots(1,2,figsize=(16,8)) \nsns.distplot(parkinsons_data['NHR'],ax=ax[0]) \nsns.distplot(parkinsons_data['HNR'],ax=ax[1])","5ae2c8b5":"fig, ax = plt.subplots(2,3,figsize=(16,8)) \nsns.distplot(parkinsons_data['MDVP:Shimmer'],ax=ax[0,0]) \nsns.distplot(parkinsons_data['MDVP:Shimmer(dB)'],ax=ax[0,1]) \nsns.distplot(parkinsons_data['Shimmer:APQ3'],ax=ax[0,2]) \nsns.distplot(parkinsons_data['Shimmer:APQ5'],ax=ax[1,0]) \nsns.distplot(parkinsons_data['MDVP:APQ'],ax=ax[1,1]) \nsns.distplot(parkinsons_data['Shimmer:DDA'],ax=ax[1,2])","cb6b8412":"fig, ax = plt.subplots(2,3,figsize=(16,8)) \nsns.distplot(parkinsons_data['MDVP:Jitter(%)'],ax=ax[0,0]) \nsns.distplot(parkinsons_data['MDVP:Jitter(Abs)'],ax=ax[0,1]) \nsns.distplot(parkinsons_data['MDVP:RAP'],ax=ax[0,2]) \nsns.distplot(parkinsons_data['MDVP:PPQ'],ax=ax[1,0]) \nsns.distplot(parkinsons_data['Jitter:DDP'],ax=ax[1,1]) \nsns.distplot(parkinsons_data['RPDE'],ax=ax[1,2])","e0b70706":"fig, ax = plt.subplots(1,3,figsize=(16,8)) \nsns.distplot(parkinsons_data['DFA'],ax=ax[0]) \nsns.distplot(parkinsons_data['spread1'],ax=ax[1]) \nsns.distplot(parkinsons_data['spread2'],ax=ax[2])","d0f2475c":"parkinsons_data.plot(kind='box', subplots=True, layout=(5,5), sharex=False, sharey=False, figsize=(28, 28))\nplt.show()","01ad9ceb":"fig, ax = plt.subplots(1,3,figsize=(16,10)) \nsns.boxplot(x='spread1',data=parkinsons_data, ax=ax[0],orient='v') \nsns.boxplot(x='spread2',data=parkinsons_data, ax=ax[1],orient='v')\nsns.boxplot(x='PPE',data=parkinsons_data,ax=ax[2],orient='v')","91ec5851":"sns.pairplot(parkinsons_data, hue='status')","348a46c7":"fig, ax = plt.subplots(1,3,figsize=(16,8)) \nsns.swarmplot(x = 'status',y = 'MDVP:Flo(Hz)',data = parkinsons_data,ax=ax[0])\nsns.swarmplot(x = 'status',y = 'MDVP:Fo(Hz)',data = parkinsons_data,ax=ax[1])\nsns.swarmplot(x = 'status',y = 'MDVP:Fhi(Hz)',data = parkinsons_data,ax=ax[2])","81b73679":"fig, ax = plt.subplots(2,3,figsize=(20,8)) \nsns.swarmplot(x = 'status',y = 'MDVP:Shimmer',data = parkinsons_data,ax=ax[0,0])\nsns.swarmplot(x = 'status',y = 'MDVP:Shimmer(dB)',data = parkinsons_data,ax=ax[0,1])\nsns.swarmplot(x = 'status',y = 'Shimmer:APQ3',data = parkinsons_data,ax=ax[0,2])\nsns.swarmplot(x = 'status',y = 'Shimmer:APQ5',data = parkinsons_data,ax=ax[1,0])\nsns.swarmplot(x = 'status',y = 'MDVP:APQ',data = parkinsons_data,ax=ax[1,1])\nsns.swarmplot(x = 'status',y = 'Shimmer:DDA',data = parkinsons_data,ax=ax[1,2])","7365180b":"fig, ax = plt.subplots(2,3,figsize=(22,8)) \nsns.swarmplot(x = 'status',y = 'MDVP:Jitter(%)',data = parkinsons_data,ax=ax[0,0])\nsns.swarmplot(x = 'status',y = 'MDVP:Jitter(Abs)',data = parkinsons_data,ax=ax[0,1])\nsns.swarmplot(x = 'status',y = 'MDVP:RAP',data = parkinsons_data,ax=ax[0,2])\nsns.swarmplot(x = 'status',y = 'MDVP:PPQ',data = parkinsons_data,ax=ax[1,0])\nsns.swarmplot(x = 'status',y = 'Jitter:DDP',data = parkinsons_data,ax=ax[1,1])\nsns.swarmplot(x = 'status',y = 'RPDE',data = parkinsons_data,ax=ax[1,2])","d5b9df20":"fig, ax = plt.subplots(1,2,figsize=(16,8)) \nsns.swarmplot(x = 'status',y = 'NHR',data = parkinsons_data,ax=ax[0])\nsns.swarmplot(x = 'status',y = 'HNR',data = parkinsons_data,ax=ax[1])","299dbf70":"fig, ax = plt.subplots(1,3,figsize=(16,8)) \nsns.swarmplot(x = 'status',y = 'DFA',data = parkinsons_data,ax=ax[0])\nsns.swarmplot(x = 'status',y = 'spread1',data = parkinsons_data,ax=ax[1])\nsns.swarmplot(x = 'status',y = 'spread2',data = parkinsons_data,ax=ax[2])","40f3621e":"sns.distplot( parkinsons_data[parkinsons_data.status == 0]['spread1'], color = 'r')\nsns.distplot( parkinsons_data[parkinsons_data.status == 1]['spread1'], color = 'g')","d889cf9e":"sns.distplot( parkinsons_data[parkinsons_data.status == 0]['spread2'], color = 'r')\nsns.distplot( parkinsons_data[parkinsons_data.status == 1]['spread2'], color = 'g')","f2afd576":"fig, ax = plt.subplots(1,2,figsize=(16,8))\nsns.boxplot(x='status',y='NHR',data=parkinsons_data,ax=ax[0])\nsns.boxplot(x='status',y='HNR',data=parkinsons_data,ax=ax[1])","1785f9f2":"fig, ax = plt.subplots(1,2,figsize=(16,8))\nsns.boxplot(x='status',y='MDVP:Flo(Hz)',data=parkinsons_data,palette=\"Set1\",ax=ax[0])\nsns.boxplot(x='status',y='MDVP:Fo(Hz)',data=parkinsons_data,palette=\"Set1\",ax=ax[1])","4ec9c298":"# For categorical predictors\ncols = [\"MDVP:Jitter(%)\",\"MDVP:Jitter(%)\",\"MDVP:RAP\",\"MDVP:PPQ\",\"Jitter:DDP\"]\nfig, axs = plt.subplots(ncols = 5,figsize=(16,8))\nfig.tight_layout()\nfor i in range(0,len(cols)):\n    sns.boxplot(x='status',y=cols[i],data=parkinsons_data, ax = axs[i])","1f029281":"dataset = parkinsons_data\nall_cols = list(dataset.columns.values)\nall_cols.remove('name')\nall_cols","f88259f9":"bins = np.linspace(-10, 10, 30)\nplt.figure(figsize=(15,30))\nfor i in range(1, 22):\n    plt.subplot(14, 2, i)\n    col = all_cols[i]\n    plt.title(all_cols[i])\n    plt.hist(parkinsons_data[col][parkinsons_data.status == 1], alpha=0.5, label='x')\n    plt.hist(parkinsons_data[col][parkinsons_data.status == 0], alpha=0.5, label='y')\n    \n\nplt.legend(loc='upper right')\nplt.show()","d7db5faa":"corr = parkinsons_data.corr()\nsns.set_context(\"notebook\", font_scale=1.0, rc={\"lines.linewidth\": 3.5})\nplt.figure(figsize=(18,7))\n# create a mask so we only see the correlation values once\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask, 1)] = True\na = sns.heatmap(corr,mask=mask, annot=True, fmt='.2f')\nrotx = a.set_xticklabels(a.get_xticklabels(), rotation=90)\nroty = a.set_yticklabels(a.get_yticklabels(), rotation=30)","fc5cca1a":"# dropping name column as this column is not much significant\nX = parkinsons_data.drop(['name','status'],axis=1)\nY = parkinsons_data[\"status\"]","a9950f3b":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.30, random_state = 7)","072fd883":"scaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","626407e7":"model = KNeighborsClassifier()\nmodel.fit(X_train, Y_train)\ny_pred = model.predict(X_test)\n\nprint(\"KNeighborsClassifier: \")\nprint(metrics.accuracy_score(Y_test, y_pred)*100)","e23dea70":"#Count mis-classified one\ncount_misclassified = (Y_test != y_pred).sum()\nprint('Misclassified samples in KNeighborsClassifier : {}'.format(count_misclassified))","2a3dbf37":"model_lr = LogisticRegression()\nmodel_lr.fit(X_train, Y_train)\ny_lr_pred = model_lr.predict(X_test)\n\nprint(\"Logistic Regression: \")\nprint(metrics.accuracy_score(Y_test, y_lr_pred)*100)","de428e3f":"#Count mis-classified one\ncount_misclassified_lr = (Y_test != y_lr_pred).sum()\nprint('Misclassified samples in LogisticRegression : {}'.format(count_misclassified_lr))","602b558a":"model_nb = GaussianNB()\nmodel_nb.fit(X_train, Y_train)\ny_nb_pred = model_nb.predict(X_test)\n\nprint(\"Naive Bayes: \")\nprint(metrics.accuracy_score(Y_test, y_nb_pred)*100)","70a8eb16":"count_misclassified_nb = (Y_test != y_nb_pred).sum()\nprint('Misclassified samples in Naive Bayes : {}'.format(count_misclassified_nb))","47bff025":"model_clf = svm.SVC(C=3, kernel ='rbf', probability = True)\nmodel_clf.fit(X_train, Y_train)\ny_clf_pred = model_clf.predict(X_test)\n\nprint(\"SVM: \")\nprint(metrics.accuracy_score(Y_test, y_clf_pred)*100)","e454c705":"count_misclassified_svm = (Y_test != y_clf_pred).sum()\nprint('Misclassified samples in SVM : {}'.format(count_misclassified_svm))","4d318dca":"model_dt = DecisionTreeClassifier(criterion = 'entropy' )\nmodel_dt.fit(X_train, Y_train)\ny_dt_pred = model_dt.predict(X_test)\n\nprint(\"Decision Tree: \")\nprint(metrics.accuracy_score(Y_test, y_dt_pred)*100)","88442697":"count_misclassified_dt= (Y_test != y_dt_pred).sum()\nprint('Misclassified samples in DecisionTreeClassifier : {}'.format(count_misclassified_dt))","f192659c":"scaler5 = StandardScaler()\nscaler5.fit(X)\nX12 = scaler5.transform(X)\n","db5847bf":"from mlxtend.classifier import StackingClassifier\nclf1 = KNeighborsClassifier(n_neighbors=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\nlr = LogisticRegression()\nsclf = StackingClassifier(classifiers=[clf1, clf2, clf3], \n                          meta_classifier=lr)\n\nprint('3-fold cross validation:\\n')\n\nfor clf, label in zip([clf1, clf2, clf3, sclf], \n                      ['KNN', \n                       'Random Forest', \n                       'Naive Bayes',\n                       'StackingClassifier']):\n\n    scores = model_selection.cross_val_score(clf, X12, Y, \n                                              cv=3, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+\/- %0.2f) [%s]\" \n          % (scores.mean()*100, scores.std(), label))","bfb433e8":"clf1_1 = KNeighborsClassifier(n_neighbors=7)\nclf2_1 = RandomForestClassifier(n_estimators=10, max_depth=None,min_samples_split=2, random_state=0)\nclf3_1 = GaussianNB()\n\nlr_1 = LogisticRegression()\nsclf_1 = StackingClassifier(classifiers=[clf2_1, clf3_1], \n                          meta_classifier=lr_1)","80e40131":"import matplotlib.gridspec as gridspec\nimport itertools\nfrom itertools import product\nlabel_1 = ['KNN', 'Random Forest', 'Naive Bayes', 'Stacking Classifier']\nclf_list_1 = [clf1_1, clf2_1, clf3_1, sclf_1]\n    \nfig = plt.figure(figsize=(10,8))\ngs = gridspec.GridSpec(2, 2)\ngrid_1 = itertools.product([0,1],repeat=2)\n\nclf_cv_mean = []\nclf_cv_std = []\nfor clf, label, grd in zip(clf_list_1, label_1, grid_1):\n        \n    scores_1 = cross_val_score(clf, X12, Y, cv=3, scoring='accuracy')\n    print(\"Accuracy: %.2f (+\/- %.2f) [%s]\" %(scores_1.mean()*100, scores_1.std(), label))\n    clf_cv_mean.append(scores_1.mean())\n    clf_cv_std.append(scores_1.std())\n        \n    clf.fit(X12, Y)\n\n","f5457c91":"rfcl = RandomForestClassifier(criterion = 'entropy', class_weight={0:.5,1:.5}, max_depth = 5, min_samples_leaf=5)\nrfcl = rfcl.fit(X_train, Y_train)\ntest_pred = rfcl.predict(X_test)\n\nprint(\"Random forest: \")\nprint(metrics.accuracy_score(Y_test, test_pred)*100)\n","f2d4c9df":"feature_imp = pd.Series(rfcl.feature_importances_,index=X.columns).sort_values(ascending=False)\nfeature_imp\n# Creating a bar plot\nsns.barplot(x=feature_imp, y=feature_imp.index)\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.legend()\nplt.show()","42a72920":"rfcl_1 = RandomForestClassifier(n_estimators = 70)\nrfcl_1 = rfcl_1.fit(X_train, Y_train)\nrfcl_1_pred = rfcl_1.predict(X_test)\nprint(\"Random Forest Ensemble:  \")\nrfcl_1.score(X_test , Y_test)*100","ef3e3685":"from sklearn.ensemble import AdaBoostClassifier\n \nabcl = AdaBoostClassifier( n_estimators= 20)\nabcl = abcl.fit(X_train, Y_train)\n\nabcl_test_pred = abcl.predict(X_test)\n\nprint(\"Adaboost Ensemble Algorithm: \")\nprint(metrics.accuracy_score(Y_test, abcl_test_pred)*100)\n ","eb56a3f7":"from sklearn.ensemble import BaggingClassifier\n\nbgcl = BaggingClassifier(n_estimators=10, max_samples= .7, bootstrap=True)\nbgcl = bgcl.fit(X_train, Y_train)\n\nbgcl_test_pred = bgcl.predict(X_test)\n\nprint(\"Ensemble Bagging Classifier Algorithm : \")\nprint(metrics.accuracy_score(Y_test, bgcl_test_pred)*100) ","35de078b":"from sklearn.ensemble import GradientBoostingClassifier\ngbcl = GradientBoostingClassifier(n_estimators = 50, learning_rate = 0.05)\ngbcl = gbcl.fit(X_train, Y_train)\n\ngbcl_test_pred = gbcl.predict(X_test)\n\nprint(\"Ensemble GradientBoost Classifier Algorithm (learning rate = 0.05) : \")\nprint(metrics.accuracy_score(Y_test, gbcl_test_pred)*100) ","d1e3e3dc":"gbcl_1 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0).fit(X_train, Y_train)\nprint(\"Ensemble GradientBoost Classifier Algorithm  (learning rate = 1, max_depth = 1): \")\ngbcl_1.score(X_test, Y_test)*100","209fbb24":"# Voting Ensemble for Classification\nimport pandas\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\n\nseed = 7\nkfold = model_selection.KFold(n_splits=10, random_state=seed)\n# create the sub models\nestimators = []\nmodel1 = LogisticRegression()\nestimators.append(('logistic', model1))\nmodel2 = DecisionTreeClassifier()\nestimators.append(('cart', model2))\nmodel3 = SVC()\nestimators.append(('svm', model3))\n# create the ensemble model\nensemble = VotingClassifier(estimators)","9e16a3ff":"ensemble.fit(X_train,Y_train)\nensemble_y_pred = ensemble.predict(X_test)\nprint(\"Ensemble - Voting Classifier: \")\nensemble.score(X_test , Y_test)*100","d230d0cc":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom itertools import product\nfrom sklearn.ensemble import VotingClassifier\n\n\n# Training classifiers\nclf111 = DecisionTreeClassifier(max_depth=4)\nclf222 = KNeighborsClassifier(n_neighbors=7)\nclf333 = SVC(kernel='rbf', probability=True)\neclf222 = VotingClassifier(estimators=[('dt', clf111), ('knn', clf222), ('svc', clf333)],\n                        voting='soft', weights=[2,10, 3])\n\nclf111 = clf111.fit(X_train, Y_train)\nclf222 = clf222.fit(X_train, Y_train)\nclf333 = clf333.fit(X_train, Y_train)\neclf222 = eclf222.fit(X_train, Y_train)","c775999c":"print(\"Ensemble - Voting Classifier (Soft Voting): \")\neclf222.score(X_test, Y_test)*100","11bad9cc":"eclf2224 = VotingClassifier(estimators=[('dt', clf111), ('knn', clf222), ('svc', clf333)],\n                        voting='hard', weights=[2, 10, 3])\neclf2224 = eclf2224.fit(X_train, Y_train)\nprint(\"Ensemble - Voting Classifier (hard Voting): \")\neclf2224.score(X_test, Y_test)*100","55905a76":"from sklearn.neural_network import MLPClassifier\nmlp = MLPClassifier(hidden_layer_sizes=(22,18,12),max_iter=1500)\nmlp.fit(X_train,Y_train)\nmlp_y_pred = mlp.predict(X_test)\nprint(\"MLPClassifier: \")\nprint(metrics.accuracy_score(Y_test, mlp_y_pred)*100)","a3911c19":"from xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nxgbc_model = XGBClassifier()\nxgbc_model.fit(X_train, Y_train)\nxgbc_y_pred = xgbc_model.predict(X_test)\nprint(\"XGBClassifier: \")\nprint(metrics.accuracy_score(Y_test, xgbc_y_pred)*100)","5c3acbb9":"classifier = Sequential()\n#First Hidden Layer\nclassifier.add(Dense(16, activation='relu', kernel_initializer='random_normal', input_dim=22))\n#Second  Hidden Layer\nclassifier.add(Dense(8, activation='relu', kernel_initializer='random_normal'))\n#Output Layer\nclassifier.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n#Compiling the neural network\nclassifier.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])\n#Fitting the data to the training dataset\nclassifier.fit(X_train,Y_train, batch_size=10, epochs=100)","441c269d":"trscores=classifier.evaluate(X_train, Y_train)\nprint(\"Train Accuracy: %.2f%%\" %(trscores[1]*100))","f0bfefbf":"# evaluate the model\ntscores = classifier.evaluate(X_test, Y_test)\nprint(\"Test Accuracy: %.2f%%\" %(tscores[1]*100))","75d7d307":"print(len(X_train)),print(len(X_test))","c8ca5aac":"names_comp = X.columns\nnames_comp","d03a0cf5":"# prepare configuration for cross validation test harness\nseed = 7\n# prepare models\nmodels_comp = []\nmodels_comp.append(('LR', LogisticRegression()))\nmodels_comp.append(('KNN', KNeighborsClassifier()))\nmodels_comp.append(('DECTREE', DecisionTreeClassifier()))\nmodels_comp.append(('NB', GaussianNB()))\nmodels_comp.append(('SVM', SVC()))\nmodels_comp.append(('XGB', XGBClassifier()))\nmodels_comp.append(('MLP',MLPClassifier(hidden_layer_sizes=(22,18,12),max_iter=1500)))\nmodels_comp.append(('GB',GradientBoostingClassifier(n_estimators = 50, learning_rate = 0.05)))\nmodels_comp.append(('BAGG',BaggingClassifier(n_estimators=10, max_samples= .7, bootstrap=True)))\nmodels_comp.append(('RDMFC',RandomForestClassifier(n_estimators = 70)))\nmodels_comp.append(('ADAB',AdaBoostClassifier( n_estimators= 20)))\n# evaluate each model in turn\nnames_comp = X.columns\nresults_comp = []\nnames_comp = []\nscoring_comp = 'accuracy'\nfor name, model in models_comp:\n    kfold_comp = model_selection.KFold(n_splits=10, random_state=seed)\n    cv_results_comp = model_selection.cross_val_score(model, X12, Y, cv=kfold, scoring=scoring_comp)\n    results_comp.append(cv_results_comp)\n    names_comp.append(name)\n    msg_comp = \"%s: %f (%f)\" % (name, cv_results_comp.mean()*100, cv_results_comp.std())\n    print(msg_comp)\n# boxplot algorithm comparison\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results_comp)\nax.set_xticklabels(names_comp)\nplt.show()","e0759cb7":"# Import required libraries for performance metrics\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import cross_validate\n\n# Define dictionary with performance metrics\nscoring_alt_comp = {'accuracy':make_scorer(accuracy_score), \n           'precision':make_scorer(precision_score),\n           'recall':make_scorer(recall_score), \n           'f1_score':make_scorer(f1_score)}\n\n# Import required libraries for machine learning classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# Instantiate the machine learning classifiers\nlog_model_acomp = LogisticRegression(max_iter=10000)\nsvc_model_acomp = LinearSVC(dual=False)\ndtr_model_acomp = DecisionTreeClassifier()\nrfc_model_acomp = RandomForestClassifier()\ngnb_model_acomp = GaussianNB()\n\n# Define the models evaluation function\ndef models_evaluation_acmp(X, y, folds):\n    \n    '''\n    X : data set features (scaled using std scaler)\n    y : data set target\n    folds : number of cross-validation folds\n    \n    '''\n    \n    # Perform cross-validation to each machine learning classifier\n    log_alt = cross_validate(log_model_acomp, X, y, cv=folds, scoring=scoring_alt_comp)\n    svc_alt = cross_validate(svc_model_acomp, X, y, cv=folds, scoring=scoring_alt_comp)\n    dtr_alt = cross_validate(dtr_model_acomp, X, y, cv=folds, scoring=scoring_alt_comp)\n    rfc_alt = cross_validate(rfc_model_acomp, X, y, cv=folds, scoring=scoring_alt_comp)\n    gnb_alt = cross_validate(gnb_model_acomp, X, y, cv=folds, scoring=scoring_alt_comp)\n\n    # Create a data frame with the models perfoamnce metrics scores\n    models_scores_table = pd.DataFrame({'Logistic Regression':[log_alt['test_accuracy'].mean(),\n                                                               log_alt['test_precision'].mean(),\n                                                               log_alt['test_recall'].mean(),\n                                                               log_alt['test_f1_score'].mean()],\n                                       \n                                      'Support Vector Classifier':[svc_alt['test_accuracy'].mean(),\n                                                                   svc_alt['test_precision'].mean(),\n                                                                   svc_alt['test_recall'].mean(),\n                                                                   svc_alt['test_f1_score'].mean()],\n                                       \n                                      'Decision Tree':[dtr_alt['test_accuracy'].mean(),\n                                                       dtr_alt['test_precision'].mean(),\n                                                       dtr_alt['test_recall'].mean(),\n                                                       dtr_alt['test_f1_score'].mean()],\n                                       \n                                      'Random Forest':[rfc_alt['test_accuracy'].mean(),\n                                                       rfc_alt['test_precision'].mean(),\n                                                       rfc_alt['test_recall'].mean(),\n                                                       rfc_alt['test_f1_score'].mean()],\n                                       \n                                      'Gaussian Naive Bayes':[gnb_alt['test_accuracy'].mean(),\n                                                              gnb_alt['test_precision'].mean(),\n                                                              gnb_alt['test_recall'].mean(),\n                                                              gnb_alt['test_f1_score'].mean()]},\n                                      \n                                      index=['Accuracy', 'Precision', 'Recall', 'F1 Score'])\n    \n    # Add 'Best Score' column\n    models_scores_table['Best Score'] = models_scores_table.idxmax(axis=1)\n    \n    # Return models performance metrics scores data frame\n    return(models_scores_table)\n  \n# Run models_evaluation function\nmodels_evaluation_acmp(X12, Y, 5)","c4a07e41":"# Applying decision tree model\ndt_model = DecisionTreeClassifier(criterion='entropy',max_depth=6,random_state=100,min_samples_leaf=5)","b4cbbb11":"dt_model.fit(X_train, Y_train)","25c78936":"DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n            max_features=None, max_leaf_nodes=None,\n            min_impurity_split=1e-07, min_samples_leaf=5,\n            min_samples_split=2, min_weight_fraction_leaf=0.0,\n            presort=False, random_state=100, splitter='best')","45c2a666":"dt_model.score(X_test , Y_test) ","08c6217d":"dt1_y_pred = dt_model.predict(X_test)","6b3019af":"confusion_matrix(Y_test,dt1_y_pred)","44adc89f":"#Count mis-classified one\ncount_misclassified = (Y_test != dt1_y_pred).sum()\nprint('Misclassified samples: {}'.format(count_misclassified))","148094b3":"from IPython.display import Image\nfrom sklearn import tree\nfrom os import system\n\ntrain_char_label = ['No', 'Yes']\npd_tree_regularized = open('pd_tree_regularized.dot','w')\ndot_data = tree.export_graphviz(dt_model, out_file= pd_tree_regularized , feature_names = list(X), class_names = list(train_char_label))\n\npd_tree_regularized.close()\n\nprint (pd.DataFrame(dt_model.feature_importances_, columns = [\"Imp\"], index = X.columns))","e737eaff":"system(\"dot -Tpng pd_tree_regularized.dot -o pd_tree_regularized.png\")\nImage(\"pd_tree_regularized.png\")","a757047f":"    #Scale the features to between -1 and 1\n    X1 = parkinsons_data.drop(['name','status'],axis=1)\n    Y1 = parkinsons_data[\"status\"]\n    \n    from sklearn.model_selection import train_test_split\n    X1_train, X1_test, Y1_train, Y1_test = train_test_split(X1, Y1, test_size = 0.30, random_state = 7)\n    \n    from sklearn.preprocessing import MinMaxScaler  \n    \n    scaler3=MinMaxScaler((-1,1))\n     \n    \n    \n   ","4d94319f":"scaler3","377c1712":"X1_train = scaler3.fit_transform(X1_train)\nX1_test = scaler3.fit_transform(X1_test) ","0096fe6c":"model_XGB1=XGBClassifier()\nmodel_XGB1.fit(X1_train,Y1_train)","c32c12e8":"y_pred_XGB1=model_XGB1.predict(X1_test)\nprint(accuracy_score(Y1_test, y_pred_XGB1)*100)","3f69fead":"model_1 = KNeighborsClassifier()\nmodel_1.fit(X1_train, Y1_train)\ny_pred_knnc = model_1.predict(X1_test)\n\nprint(\"KNeighborsClassifier: \")\nprint(metrics.accuracy_score(Y1_test, y_pred_knnc)*100)","450120cd":"model_2 = LogisticRegression()\nmodel_2.fit(X1_train, Y1_train)\ny_pred_lr = model_2.predict(X1_test)\n\nprint(\"Logistic Regression: \")\nprint(metrics.accuracy_score(Y1_test, y_pred_lr)*100)","c7fc74e1":"model_3 = GaussianNB()\nmodel_3.fit(X1_train, Y1_train)\ny_pred_nb = model_3.predict(X1_test)\n\nprint(\"Naive Bayes: \")\nprint(metrics.accuracy_score(Y1_test, y_pred_nb)*100)","fbc49b1a":"model_4 = svm.SVC(C=3, kernel ='rbf', probability = True)\nmodel_4.fit(X1_train, Y1_train)\ny_pred_svm = model_4.predict(X1_test)\n\nprint(\"SVM: \")\nprint(metrics.accuracy_score(Y1_test, y_pred_svm)*100)","779e8654":"model_4 = DecisionTreeClassifier(criterion = 'entropy' )\nmodel_4.fit(X1_train, Y1_train)\ny_pred_dt = model_4.predict(X1_test)\n\nprint(\"Decision Tree: \")\nprint(metrics.accuracy_score(Y1_test, y_pred_dt)*100)","145abd2f":"model_5 = RandomForestClassifier(criterion = 'entropy', class_weight={0:.5,1:.5}, max_depth = 5, min_samples_leaf=5)\nmodel_5.fit(X1_train, Y1_train)\ny_pred_rndfor = model_5.predict(X1_test)\n\nprint(\"Random forest: \")\nprint(metrics.accuracy_score(Y1_test, y_pred_rndfor)*100)\n","e2998fac":"from sklearn.ensemble import GradientBoostingClassifier\nmodel_6 = GradientBoostingClassifier(n_estimators = 50, learning_rate = 0.05)\nmodel_6.fit(X1_train, Y1_train)\n\ny_pred_gbc = model_6.predict(X1_test)\n\nprint(\"GradientBoost Classifier Algorithm : \")\nprint(metrics.accuracy_score(Y1_test, y_pred_gbc)*100) ","b50d2367":"##### Observation\n\nThe following attributes has postive skew, which will also be notices in the distrubtion in below steps later in the notebook.\n   \n        - MDVP:Jitter(%)\n        - MDVP:RAP\n        - MDVP:PPQ\n        - NHR\n        - Jitter:DDP\n        - MDVP:APQ\n        - MDVP:Fhi(Hz)\n        - MDVP:Flo(Hz)\n        - MDVP:Jitter(Abs)\n        - MDVP:Shimmer\n        - MDVP:Shimmer(dB)\n        - Shimmer:APQ3\n        - Shimmer:APQ5\n        - Shimmer:DDA\n       \n        \n        \nThe following attributes has negative skew, which will also be notices in the distrubtion in below steps later in the notebook.\n\n        - status\n        - HNR\n        - RPDE\n        - DFA","3ebfc60e":"# Citation Request:\n\nIf you use this dataset, please cite the following paper: \n'Exploiting Nonlinear Recurrence and Fractal Scaling Properties for Voice Disorder Detection', \nLittle MA, McSharry PE, Roberts SJ, Costello DAE, Moroz IM. \nBioMedical Engineering OnLine 2007, 6:23 (26 June 2007)\n","f904c446":"## Model 13 : Ensemble - GradientBoost Classifier (learning rate = 1, max_depth = 1)","c1184b4d":"## Model 1 : k-NN (KNeighborsClassifier)","3cabcd9d":"##### ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","a0eccc7f":"Spread1 is normally distributed between person who have PD and who is normal. People who have spread1 between - 8.5 and -7.5 are more and they are normal. People whose spread1 is between -6.5 and -5 are having PD","ff58bc02":"# 1. Load the dataset","880d75ee":"# 5. Prepare the data for training - Scale the data if necessary, get rid of missing values (if any) etc","f1b1ecbb":"##### The data is scaled, there is no missing values. So we are ready to use the data for the model.","1bbddff9":"# used Min_Max Scaling Method","908a847b":"# Observation:\n\n\n# Variable types\n\nThere are 24 attributes in the datset.\nNUM  Type  :\t22\nBOOL Type  :\t1 , status - which will be our target label\nCAT  Type  :\t1 ,name - which is,  UNIQUE, can be dropped\n\n# Number of records\n\n195 records are there in this dataset.\n\n# Missing Values\n\nThere is no missing values in the dataset\n\n# Target Label\/Column\n\nstatus\n(one) - Parkinson's = 147\n(zero) - healthy    =  48\n\n\n# challenges with the dataset\n- number of reords is too less\n\n# What is Parkinson\u2019s Disease?\n\nParkinson\u2019s disease is a progressive disorder of the central nervous system affecting movement and inducing tremors and stiffness. It has 5 stages to it and affects more than 1 million individuals every year in India. This is chronic and has no cure yet. It is a neurodegenerative disorder affecting dopamine-producing neurons in the brain.","790e3910":"##### -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","595440de":"#  2. It is always a good practice to eye-ball raw data to get a feel of the data in terms of number of records, structure of the file, number of attributes, types of attributes and a general idea of likely challenges in the dataset. Mention a few comments in this regard","87f6de55":"# 3. Using univariate & bivariate analysis to check the individual attributes for their basic statistics such as central values, spread, tails, relationships between variables etc. mention your observations","0e50c665":"## Model 4 : SVM  ","e85ddf27":"## Model 2 : LogisticRegression  ","35ab82de":"###### ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","6ebc5fb6":"## Model 12 : Ensemble - GradientBoost Classifier (learning rate = 0.05)","300f3ebd":"##### This another additional way:","4f0579e2":"##### Based on the above results, in this apprach as per the logic, attributes, the best model is SVM with 83%","8357dd41":"##### --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","b1efd6ba":"## Model 6 : Stacking with meta classifier","41cde93a":"People who are suffering for PD tend to have higher jitter %. It seems if the values goes above 0.15 we can confirm the patient is having PD. The variation of fundamental frequency is in a low range for people who is normal","76d5eb4b":"## Model 19 : Neural Network(sequential)","38c609d2":"###### Alternate metthod comparision of model and performance , one more different way","57391e12":"## Model 14 : Ensemble - Voting Classifier ()","fe124f05":"##### Observation:\n\nThe above figure shows the box plot of the frequency variation. All the three variations have outliers. Generally speaking, decision trees are able to handle outliers. It is very unlikely that decision tree will create a leaf to isolate them","678c0f18":"# Objective:\n\nGoal is to classify the patients into the respective labels using the attributes from their voice recordings","4e481602":"# 9. Compare all the models (minimum 5) and pick the best one among them","f8f24f81":"## Model 11 : Ensemble - Bagging Classifier","7acc94c4":"##### ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","520cdfc8":"## Model 17 : Neural Network (MLPClassifier)","23c9ef5a":"##### Observation\n\nWe can see outlier in most of the attributes, but we not treat them, as we do not know the signifiance, and it may be geniue observations, as these are from the mediance filed, and each patieance may be unique.","4c46c296":"###### Based on this approach, the best model is Logistic Regression.","d5d277e0":"##### --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","26b95950":"## Model 9 : Ensemble - Random Forest (default and n-estimators)","407e149d":"# 6. Train at least 3 standard classification algorithms - Logistic Regression, Naive Bayes\u2019, SVM, k-NN etc, and note down their accuracies on the test data","abe3abd0":"## Model 7 : Stacking with meta classifier with GridSpec","a8b3e867":"# 4. Split the dataset into training and test set in the ratio of 70:30 (Training:Test)","ff15174a":"## Model 18 : XGB ","17f0a704":"## Univariate Analysis","b58dfb39":"##### -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","2586152c":"# Learning Outcomes:\n\n\u25cf Exploratory Data Analysis\n\u25cf Supervised Learning\n\u25cf Ensemble Learning","869f7785":"##### --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","76ba0724":"##### ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","021fc50f":"##### Observation\n\nThe data looks pretty reasonable, before building the model, we will scale the data, so all the variables are in same scale.","bd546c2c":"# 8. Train at least one standard Ensemble model - Random forest, Bagging, Boosting etc, and note the accuracy","5da59b15":"------------------------------------------------------","65da0d7c":"##### Additional ","93ab67f2":"## Model 3 : Naive Bayes\u2019  ","bf549807":"## Model 8 : Ensemble - Random Forest ('entropy')","3cfcec31":"##### Observation:\n\nFor the various attributes, we are able to see the distrubtion. We can see long tail in Shimmer - APQ5, APQ3, DDA, PPE, various MDVP attributes, Jitter-DDP.","e7258433":"##### ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","65eb28fe":"##### Observation\n\nspread1 and spread looks nornal distribution expect few spikes.","778f9dd3":"###### Based from the above the models , \n\n##### KNN gave 96.10 accuracy, Neural Network gave 98.31% accuracy, Gradient Boost 94.915 accuracy.\n\n### I will go with KNN model , 96.10% accuracy and 2 misclassification. I had not selected the Neural Netwrk, since we have still not study the same in course.","d1056d80":"# Bi -Variate Analysis","ed00a2d0":"## Model 5 :  Decision Tree  ","c40d2223":"## Model 16 : Ensemble - Voting Classifier ( hard voting)","ebc5c684":"## Model 10 : Ensemble - AdaBoost","d6f4195e":"# Context\n\nParkinson\u2019s Disease (PD) is a degenerative neurological disorder marked by decreased dopamine levels in the brain.It manifests itself through a deterioration of movement, including the presence of tremors and stiffness. \n\nThere is commonly a marked effect on speech, including dysarthria (difficulty articulating sounds), hypophonia (lowered volume), and monotone (reduced pitch range). Additionally, cognitive impairments and changes in mood can occur, and risk of dementia is increased.\n\nTraditional diagnosis of Parkinson\u2019s Disease involves a clinician taking a neurological history of the patient and observing motor skills in various situations. Since there is no definitive laboratory test to diagnose PD, diagnosis is often difficult, particularly in the early stages when motor effects are not yet severe. \n\nMonitoring progression of the disease over time requires repeated clinic visits by the patient. An effective screening process, particularly one that doesn\u2019t require a clinic visit, would be beneficial. Since PD patients exhibit characteristic vocal features, voice recordings are a useful and non-invasive tool for diagnosis. \n\nIf machine learning algorithms could be applied to a voice recording dataset to accurately diagnosis PD, this would be an effective screening step prior to an appointment with a clinician","3490f1ff":"# Data Description\n\nname - ASCII subject name and recording number\n\nMDVP:Fo(Hz) - Average vocal fundamental frequency\n\nMDVP:Fhi(Hz) - Maximum vocal fundamental frequency\n\nMDVP:Flo(Hz) - Minimum vocal fundamental frequency\n\nMDVP:Jitter(%),MDVP:Jitter(Abs),MDVP:RAP,MDVP:PPQ,Jitter:DDP - Several measures of variation in fundamental frequency\n\nMDVP:Shimmer,MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA - Several measures of variation in amplitude\n\nNHR,HNR - Two measures of ratio of noise to tonal components in the voice\n\nstatus - Health status of the subject (one) - Parkinson's, (zero) - healthy\n\nRPDE,D2 - Two nonlinear dynamical complexity measures\n\nDFA - Signal fractal scaling exponent\n\nspread1,spread2,PPE - Three nonlinear measures of fundamental frequency variation 9. car name: string (unique for each instance)","77b43001":"## Model 15 : Ensemble - Voting Classifier ( Soft voting)","7d997434":"When we look the relationship between status and MDVP:Fo(Hz) we can see the median value is around 199 Hz for people who are normal. For people who are affected with Parkinsons the median value comes around 145 Hz","ae7fbed6":"##### Observation\n\nFor all of the above graphs, we can observe that the measure of variation in amplitude is positively skewed","038963fa":"##### Observation\n\nThe measures of vocal fundamental frequency are shown above. There is a positive skewness for minimum vocal fundemental frequency with more high values between 75Hz and 125Hhz. The average vocal frequency is almost normally distributed with more values ranging 115Hz and 130Hz. The high vocal frequency does not have any skewness, but some range of values are at the right most tail","439abcba":"# Correlation comparision with heat map","4a7467f8":"##### Observation\n\nFor all of the above graphs, we can observe that the measure of variation in amplitude is positively skewed","a5563df4":"##### ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","bfd91fc8":"##### Observation\n\nMDVP:Jitter(%) has a very high correlation with MDVP:Jitter(Abs),MDVP:RAP,MDVP:PPQ,Jitter:DDP\nMDVP:Shimmer has a very correlation with MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA this may be because they are related to each other. This may be because multi-dimensinal voice programs analysis is closely related with these variables\nThe target variable status has a weak positive corelation with spread1","91eef48c":"##### Observation:\n\nThe measure of tonal component of frequency is shown above. The value NHR is right skewed for there are so many observations in the area, but they seem to be with very minimal values. The maximum number of observations is between 0 and 0.04. The value HNR looks like normally distributed, but in a first look there seems to be a slight negative skewness","901b6ff6":"###### But am also using the two different approach below by KFold and commparing the results.","f8fa0f9c":"# 7. Train a meta-classifier and note the accuracy on test data","f1db03f8":"People who have PD(status equal to one) have higher levels of Noise to Harmonic ratio. Also, looking into the HNR ratio people who have PD have lower levels in the same."}}