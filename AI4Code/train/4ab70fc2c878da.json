{"cell_type":{"46fdb59f":"code","bc0f66bc":"code","c1b434f2":"code","d73dd3d7":"code","7a8a94fb":"code","43e9ef55":"code","4789d184":"code","88fcb40a":"code","5ee30406":"code","3e1d5d1f":"code","f9fcb22c":"code","7c604ab9":"code","aa0e3fbe":"code","4bb8f687":"code","dd98e4ae":"code","a25f45bb":"code","2700167d":"code","66dec7b1":"code","92b54178":"code","40299e80":"code","f326dfc6":"code","0b387ea8":"code","3c2cd167":"code","ab8af8dd":"code","ca60bd72":"code","9141111b":"code","e075a97f":"code","a770a16c":"code","fe30edee":"code","427b0dce":"code","75dc1b43":"code","4835e823":"code","c74ec799":"code","8fd246e6":"code","b385a49e":"code","32c0c3a8":"code","1da1115d":"code","87ebc0d5":"code","8b9eb0bf":"markdown","46849122":"markdown","1252f40a":"markdown","5387d8f0":"markdown","873996ac":"markdown","d052dc80":"markdown","6c351534":"markdown","cca71d8f":"markdown","c8678d90":"markdown","bc95a65f":"markdown","8f056857":"markdown","512da696":"markdown","3dd9bd9a":"markdown","ad9f7257":"markdown","10bbd48e":"markdown","40d84d17":"markdown","d65947c8":"markdown","5ada0660":"markdown","fcbff6ef":"markdown","90e053a2":"markdown","5666b464":"markdown","63402538":"markdown","abec069f":"markdown","36d0070f":"markdown","d2de8769":"markdown","e02fd642":"markdown","9ea86ab3":"markdown","4f4a99ad":"markdown"},"source":{"46fdb59f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib\nfrom matplotlib import cm\nimport matplotlib.pyplot as plt\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nFPATH = '\/kaggle\/input\/housing\/'\nGPATH = '\/kaggle\/input\/caliboundaries\/'\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bc0f66bc":"df = pd.read_csv(FPATH+\"housing.csv\")\nl0 = -125\ndf['long_mercator'] = df['longitude']-l0\ndf['lat_mercator'] = np.arcsinh(np.tan(df['latitude']*np.pi\/180))\ndf['pop_per_household'] = df['population']\/df['households']\ndf['rooms_per_house'] = df['total_rooms']\/df['households']\ndf['bedrooms_per_house'] = df['total_bedrooms']\/df['households']\ndf['bedrooms_per_room'] = df['total_bedrooms']\/df['total_rooms']\ndf['rooms_per_person'] = df['total_rooms']\/df['population']\ndf['bedrooms_per_person'] = df['total_bedrooms']\/df['population']","c1b434f2":"f = open(GPATH+'pts.txt', 'r')\npts = []\nfor i in range(1500):\n    a = f.readline()\n    if a == '':\n        break\n    if ',' not in a:\n        pts.append([])\n    else:\n        tup = list(map(float,a[:-1].split(',')))\n        tup[0] -= l0\n        tup[1] = np.arcsinh(np.tan(tup[1]*np.pi\/180))\n        pts[-1].append(tup)","d73dd3d7":"for poly in pts:\n    x = [t[0] for t in poly]\n    y = [t[1] for t in poly]\n    plt.fill(x, y, facecolor='lightblue')","7a8a94fb":"f2 = open(GPATH+'densities.txt', 'r')\nf2.readline()\nds = []\nfor i in range(20640):\n    ds.append(float(f2.readline()[:-1]))\ndf['density'] = ds","43e9ef55":"critical = ['housing_median_age', 'median_income', 'median_house_value', 'pop_per_household', 'rooms_per_house',\n            'bedrooms_per_house', 'bedrooms_per_room', 'rooms_per_person', 'bedrooms_per_person', 'density']\nsumlist = ['longitude','latitude','housing_median_age','total_rooms','total_bedrooms','median_income','median_house_value',\n           'long_mercator','lat_mercator','pop_per_household','rooms_per_house','bedrooms_per_house','bedrooms_per_room',\n           'rooms_per_person','bedrooms_per_person','density']","4789d184":"# This calculates summary statistics for a df, weighted by # households\n\ndef cumulative(duf, cols):\n    df_summary = pd.DataFrame(index=['count', 'mean', 'std' , 'min' , '25%' , '50%' , '75%', 'max'])\n    dtf = duf.copy()\n    for s in cols:\n        df_summary[s] = pd.Series(float)\n        dtf.sort_values(s, inplace=True)\n        cumsum = dtf['households'].cumsum()\n        df_summary[s].loc['count'] = dtf[s].count()\n        df_summary[s].loc['min'] = dtf[s].min()\n        df_summary[s].loc['25%'] = dtf[s][cumsum >= dtf['households'].sum() \/ 4.0].iloc[0]\n        df_summary[s].loc['50%'] = dtf[s][cumsum >= dtf['households'].sum() \/ 2.0].iloc[0]\n        df_summary[s].loc['75%'] = dtf[s][cumsum >= 3*dtf['households'].sum() \/ 4.0].iloc[0]\n        df_summary[s].loc['max'] = dtf[s].max()\n\n        avg = (dtf[s]*dtf['households']).sum()\/dtf['households'].sum()\n        df_summary[s].loc['mean'] = avg\n        df_summary[s].loc['std'] = np.sqrt(((dtf[s]-avg)**2 * dtf['households']).sum()\/dtf['households'].sum())\n\n    return df_summary","88fcb40a":"cumulative(df,sumlist)","5ee30406":"df.describe()","3e1d5d1f":"for c in critical:\n    plt.scatter(df[c], df['median_house_value'], s=.1)\n    plt.title(c)\n#     if c == 'bedrooms_per_room':\n#         plt.xlim([0,0.5])\n    plt.show()","f9fcb22c":"for c in critical:\n    plt.scatter(df['density'], df[c], s=.1)\n    plt.title(c)\n    plt.ylim([0,df[c].quantile(.95)])\n#     if c == 'bedrooms_per_room':\n#         plt.xlim([0,0.5])\n    plt.show()","7c604ab9":"plt.scatter(df['density'], df['median_house_value'], s=.1)","aa0e3fbe":"cumulative(df, critical)","4bb8f687":"plt.scatter(df['median_income'], df['median_house_value'], s=.1)\nplt.plot([6, 14], [0, 400000])","dd98e4ae":"df_outlier1 = df.loc[(df['median_house_value']-0) < (400000-0)\/(14-6)*(df['median_income']-6)]","a25f45bb":"cumulative(df, critical)","2700167d":"df_outlier1","66dec7b1":"# 18501 is the most visible outlier (i.e. high income, low house value)\n    # State Park. \n# 19006 has an insanely high pop_per_household\n    # Rooms per house stats are all fine, but WTF with household size\n    # Checked coordinates on Google Maps. Turns out it's a state prison + med facility\n# Outliers are in much less dense areas\n# More rooms per house\n# But ppl per house roughly the same\ndf_outlier1[critical]","92b54178":"plt.scatter(df['rooms_per_house'], df['median_house_value'], s = .1)\n# plt.xlim([0,15])","40299e80":"# don't know why 1914 is so whack\n    # all really small looking houses\n    # maybe a 'house' is broken into many small structures?\n    # maybe clerical error?\n    \n# all seem pretty rural. SUPER LOW DENSITY\n    # a bunch seem clustered around Lake Tahoe (i created a map)\n    # less pop per house, med house value, income\ndf[df['rooms_per_house'] > 40]\n\n# df.loc[df['rooms_per_house'] > 40][critical].describe()","f326dfc6":"fig = plt.figure(figsize = (10,15))\naxs = fig.add_axes([0,0,1,1])\n\nfor poly in pts:\n    x = [t[0] for t in poly]\n    y = [t[1] for t in poly]\n    plt.fill(x, y,facecolor='whitesmoke')\n\ndef f(x):\n    if (x['median_house_value']-0) < (400000-0)\/(14-6)*(x['median_income']-6):\n        return 'red'\n    else:\n        return 'whitesmoke'\n\ndf['A'] = df[['median_income','median_house_value']].apply(f, axis=1)\n\n# .loc[(df['median_house_value']-0) < (400000-0)\/(14-6)*(df['median_income']-6)]\n\n\n# df.plot.scatter('long_mercator', 'lat_mercator', 20, ax = axs, c=df['A'], figsize = (10,15), cmap=cm.get_cmap('Spectral'), zorder = 2, alpha = 1)\ndf.loc[df['rooms_per_house'] > 40].plot.scatter('long_mercator', 'lat_mercator', 10, ax=axs, figsize = (10,15), zorder = 2, alpha=1)\nfig.savefig(\"densities.svg\")","0b387ea8":"for c in critical:\n    print (\"PLOTTING: \", c)\n    fig, ax1 = plt.subplots()\n    ax2 = ax1.twinx()\n    x = df[c]\n    ydf = df.loc[df['rooms_per_house'] > 40]\n    y = ydf[c]\n    y.hist(ax=ax2, color='orange')\n    x.hist(ax=ax1)\n    plt.tight_layout()\n    if (c == 'density'):\n        plt.xlim(0,5000)\n    plt.show()","3c2cd167":"plt.scatter(df['bedrooms_per_person'], df['median_house_value'], s = .1)\n# plt.xlim([0,4])","ab8af8dd":"# 1979 is a ski resort. A lot of overlap with rooms per house outliers. Again, very few houses\n\ndf.loc[df['bedrooms_per_person'] > 6]","ca60bd72":"cumulative(df,critical)","9141111b":"# rooms_per_house is centered about 1.5-2 (vs. 5-ish for total), which means these houses are generally quite small\n# Surprisingly, these are more expensive than on average. But that's also bc they're in denser regions (higher rent)\ncumulative(df.loc[df['bedrooms_per_room'] > .6], critical)","e075a97f":"l0 = -125\n# df['R'] = df['housing_median_age'].apply(lambda x: 'blue' if x > 30 else 'red')\ncol = ['darkred', 'tomato', 'darkorange','forestgreen','lightskyblue','thistle']\n# col = ['whitesmoke', 'darkorange','whitesmoke','whitesmoke','whitesmoke']\ndf['R'] = df['density'].apply(lambda x: col[0] if x >= 20000 else (col[1] if x >= 9200 else (col[2] if x >= 6500 else (col[3] if x >= 3700 else (col[4] if x >= 500 else col[5])))))\ndf.head()","a770a16c":"fig = plt.figure(figsize=(20,8))\naxs = fig.add_axes([0,0,1,1])\ndf.hist('density', ax=axs, bins=800)\nplt.title(\"Population Density Histogram\", fontsize = 30)\nplt.ylabel(\"Frequency\", fontsize = 20)\nplt.xlabel(\"Density Estimates\", fontsize = 20)\nplt.axvline(x=500, c='red', linestyle='--')\nplt.axvline(x=3700, c='red', linestyle='--')\nplt.axvline(x=6500, c='red', linestyle='--')\nplt.axvline(x=9200, c='red', linestyle='--')\nplt.axvline(x=20000, c='red', linestyle='--')\nplt.savefig('histo.svg')\nplt.ylim([0,600])","fe30edee":"fig = plt.figure(figsize = (10,15))\naxs = fig.add_axes([0,0,1,1])\n\nfor poly in pts:\n    x = [t[0] for t in poly]\n    y = [t[1] for t in poly]\n    plt.fill(x, y,facecolor='whitesmoke')\n\ndf.plot.scatter('long_mercator', 'lat_mercator', .4, ax = axs, c=df['R'], figsize = (13,15), zorder = 2, alpha = 1)\nfig.savefig(\"densities.svg\")","427b0dce":"df_RR = df.loc[df['density'] < 500]\ndf_RS = df.loc[(500 <= df['density']) & (df['density'] < 3700)]\ndf_SS = df.loc[(3700 <= df['density']) & (df['density'] < 6500)]\ndf_SU = df.loc[(6500 <= df['density']) & (df['density'] < 9200)]\ndf_UU = df.loc[(9200 <= df['density']) & (df['density'] < 20000)]\ndf_CC = df.loc[20000 <= df['density']]","75dc1b43":"df_list = [df_RR, df_RS, df_SS, df_SU, df_UU, df_CC]","4835e823":"for c in critical:\n    fig = plt.figure(figsize=(10,10))\n    axs = fig.add_axes([0,0,1,1])\n    plt.title(c)\n    for i, sub in enumerate(df_list):\n        cm = cumulative(sub, critical)\n        plt.plot([i,i],[cm[c].loc['25%'],cm[c].loc['75%']], color='lightblue')\n        plt.plot(i,cm[c].loc['50%'], 'ro', markersize=12)\n    plt.show()","c74ec799":"for c in critical:\n    plt.scatter(df_UU[c], df_UU['median_house_value'], s=.1)\n    plt.title(c)\n    plt.xlim([0,df_UU[c].quantile(.95)])\n    plt.show()","8fd246e6":"df.corr()['median_house_value']","b385a49e":"lizt = ['RR','RS','SS','SU','UU','CC']\nfor i, d in enumerate(df_list):\n    print('-'*20+lizt[i]+'-'*20)\n    cmat = d.corr()\n    print(cmat.loc[abs(cmat['median_house_value']) > .4]['median_house_value'])","32c0c3a8":"means = [154375.371074, 221558.781652, 209589.774185, 216382.263378, 209971.951837, 239581.831597]","1da1115d":"# Making the ocean_proximity variable quantitative\n\ndef makebin(x):\n    d = x['ocean_proximity']\n    mp = {'NEAR BAY':2, '<1H OCEAN':1, 'INLAND':0, 'NEAR OCEAN':3, 'ISLAND':4}\n    return mp[d]\n\ndf['ocean_proximity_bin'] = df[['ocean_proximity']].apply(makebin, axis=1)\ndf = df.drop(['R', 'A', 'long_mercator', 'lat_mercator', 'ocean_proximity'],axis=1)\ndf.head()","87ebc0d5":"from sklearn import linear_model\n\ndf_list_names = ['RR','RS','SS','SU','UU','CC']\n\nfor i, d in enumerate(df_list):\n    d['ocean_proximity_bin'] = d[['ocean_proximity']].apply(makebin, axis=1)\n    d = d.drop(['R', 'A', 'long_mercator', 'lat_mercator', 'ocean_proximity'],axis=1)\n#     print(d.columns)\n    variables = list(d.columns)\n    variables.remove('median_house_value')\n#     variables = ['median_income']\n    d = d.dropna()\n    X = d[variables]\n    y = d['median_house_value']\n    regr = linear_model.LinearRegression()\n    regr.fit(X, y)\n    \n    # evaluate\n    pred = regr.predict(np.array(d.drop('median_house_value', axis=1)))\n#     pred = regr.predict(np.array(pd.DataFrame(d['median_income'])))\n    actu = np.array(d['median_house_value'])\n    dumb = np.full((len(d.index),), means[i])\n    print(df_list_names[i]+' RMSE: ', (sum((pred-actu)**2)\/len(d.index))**.5)\n    \n# 1: 60363.56455980125","8b9eb0bf":"## Multiple Regression","46849122":"### In the next 2 cells, we examine the distribution of population density values for the dataset. We find several distinct clusters.","1252f40a":"## Reading population density proxy measure\n(I calculated this using population and long\/lat data via a separate C++ program)","5387d8f0":"# Preliminary Analysis","873996ac":"### Cool visualization below","d052dc80":"### Density proxy measure vs. each variable","6c351534":"## Function to easily display summary statistics for any sub-dataframe of df\nI decided not to use .describe() since each block group (geographical district) has a different # households. The \"cumulative\" function calculates summary stats, weighted by # households","cca71d8f":"## Correlation and Outlier Analysis\nI chose certain variables, examined their relationships with other variables, and looked at outliers","c8678d90":"## Main dataframe and feature engineering","bc95a65f":"# Processing dataframes","8f056857":"### Each variable vs. median_house_value","512da696":"SS: weaker pop per household,\nSU, UU, and (less so) CC: weak bedrooms\/rooms per person, pop per household, med income\nCC: {weak: }, {moderate: }","3dd9bd9a":"We find clear clustering on this histogram of density proxy values. Let's break the data points into 6 clusters, as seen above, and explore each. (NOTE: the cluster with the least density seems arbitrarily determined. I just used 500 ppl\/mi, as defined in https:\/\/www.ers.usda.gov\/topics\/rural-economy-population\/rural-classifications\/what-is-rural\/) ","ad9f7257":"#### Created overlapping histograms comparing high rooms-per-house districts (orange) vs. all districts (blue) on several important variables","10bbd48e":"#### Where are districts with abnormally high rooms_per_house?","40d84d17":"# Summary\n\n### Did exploratory analysis on 1990 CA housing price data\n\n### Computed a population density proxy measure (done on my computer using ad-hoc C++ algo; uploaded onto Kaggle)\n\n### Analyzed distribution of datapoints w\/ respect to the density measure. Found clear clustering\n\n### Ran a quick multiple regression model","d65947c8":"## Geographical Analysis","5ada0660":"### Price vs. bedrooms_per_room","fcbff6ef":"### Now, let's name the 6 groups\n* RR: rural\n* RS: rural suburban\n* SS: suburban\n* SU: suburban-urban\n* UU: urban\n* CC: city center","90e053a2":"### Multiple regression for each cluster, separately\n(Printed RMSE, but didn't do further analysis of model)","5666b464":"### For each of the critical variables, we plot its median (red dot) and IQR (blue line) for each of the 6 groups (labeled 0 to 5)\n#### There are some nice trends","63402538":"### We find below that breaking the dataset into the 6 groups improves the \"predictive power\" (as judged by Pearson correlation coefficient) of some variables","abec069f":"We unravel some clean relationships between population density clusters and other variables in df.\n\nNow let's analyze each cluster separately","36d0070f":"### Price vs. Median Income","d2de8769":"### Price vs. Bedrooms_per_person","e02fd642":"### Price vs. Rooms per House","9ea86ab3":"# Housing Value Prediction","4f4a99ad":"## Reading CA boundary points\n(for visualization purposes, mainly)"}}