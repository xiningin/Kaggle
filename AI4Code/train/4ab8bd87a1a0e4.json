{"cell_type":{"a06cce99":"code","ebeb7ad1":"code","9c83ee53":"code","9df82d15":"code","4c609525":"code","e08dee19":"code","7b0edbf8":"code","439d040b":"code","ece1a99c":"code","935c0476":"code","26484fee":"code","bebe97c1":"code","9e3f1104":"code","c1561dfa":"code","83e2818e":"code","900e6d62":"code","d8a98613":"code","977df119":"code","5face3ca":"code","ecc99717":"code","4ed80eb8":"code","b3b8f5c8":"code","530ac018":"code","9081bd82":"code","b71f63f3":"markdown"},"source":{"a06cce99":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ebeb7ad1":"train = pd.read_csv('\/kaggle\/input\/crowdflower-search-relevance\/train.csv.zip')\ntest = pd.read_csv('\/kaggle\/input\/crowdflower-search-relevance\/test.csv.zip')\ntrain.head()","9c83ee53":"train.info()","9df82d15":"test.info()","4c609525":"train.describe()","e08dee19":"train['query'].map(lambda x:len(x.split())).value_counts()","7b0edbf8":"train['product_title'].map(lambda x:len(x.split())).value_counts()","439d040b":"split = int(len(train)*0.8)\ntrain_0, dev = train[:split], train[split:]","ece1a99c":"clean_train_1 = train_0[train_0.relevance_variance <1].copy()\nclean_train_2 = train_0[train_0.relevance_variance <0.50].copy()\ndev.describe()","935c0476":"clean_train_1.describe()","26484fee":"clean_train_2.describe()","bebe97c1":"## Skipping product description as it's too lengthy and missing values\ntrain = train_0#clean_train_1\n# train_input = train.apply(lambda x: x['query']+' '+x['product_title']+' '+str(x['product_description']), axis=1)\n# dev_input =  dev.apply(lambda x: x['query']+' '+x['product_title']+' '+str(x['product_description']), axis=1)\ntrain_input = train.apply(lambda x: x['query']+' '+x['product_title'], axis=1)\ndev_input =  dev.apply(lambda x: x['query']+' '+x['product_title'], axis=1)","9e3f1104":"from sklearn.feature_extraction.text import TfidfVectorizer \ntfidf = TfidfVectorizer(ngram_range=(1, 5),stop_words = 'english', strip_accents='unicode', max_features=2*10**5)\ntrain_x = tfidf.fit_transform(train_input)\ndev_x = tfidf.transform(dev_input)\ntrain_x","c1561dfa":"train_y, dev_y = train.median_relevance.to_list(), dev.median_relevance.to_list()\ntrain_y = [(x-1)\/3 for x in train_y]\ndev_y = [(x-1)\/3 for x in dev_y]\nnp.mean(train_y), np.max(train_y), np.min(train_y)","83e2818e":"from sklearn.metrics import mean_squared_error, cohen_kappa_score, make_scorer\ndef reg_scorer(true, pred):\n    pred = [min(1, max(0,x)) for x in pred]\n    pred = [int(round((x*3)+1)) for x in pred]\n    true = [int(round((x*3)+1)) for x in true]\n    return cohen_kappa_score(true, pred)","900e6d62":"from sklearn.linear_model import LinearRegression, SGDRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVR\n#clf = LinearRegression().fit(train_x, train_y)\n#clf = SGDRegressor(verbose=1,n_iter_no_change=20).fit(train_x, train_y)\nparam_grid = {'C': [1,2,5,10], 'kernel': ('rbf','sigmoid')} #('linear','rbf', 'poly','sigmoid') #'epsilon':[0.1,0.2]\nsvr  = SVR()\nscorer = make_scorer(reg_scorer, greater_is_better=True)\nclf = GridSearchCV(svr, param_grid, verbose=True,scoring=scorer, n_jobs=16)\nclf.fit(train_x, train_y)\nclf.best_estimator_, clf.best_params_, clf.best_score_","d8a98613":"## 0.26 is the best score till now\n\npreds = clf.best_estimator_.predict(dev_x)\nmean_squared_error(dev_y, preds),  reg_scorer(dev_y, preds)","977df119":"test_input =  test.apply(lambda x: x['query']+' '+x['product_title'], axis=1)\ntest_x = tfidf.transform(test_input)\npred = clf.best_estimator_.predict(test_x)\npred = [min(1, max(0,x)) for x in pred]\npred = [int(round((x*3)+1)) for x in pred]\nout = pd.DataFrame({\"id\": test.id.to_list(), \"prediction\": pred})\nout.to_csv('submission.csv', index=False)","5face3ca":"train = train_0\ntrain_texts = train.apply(lambda x: x['query']+'<\\s><s>'+x['product_title'], axis=1).to_list()\nval_texts =  dev.apply(lambda x: x['query']+'<\\s><s>'+x['product_title'], axis=1).to_list()\ntrain_labels, val_labels = train.median_relevance.to_list(), dev.median_relevance.to_list()\ntest_texts = test.apply(lambda x: x['query']+'<\\s><s>'+x['product_title'], axis=1).to_list()","ecc99717":"from transformers import RobertaTokenizerFast\ntokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\ntrain_encodings = tokenizer.batch_encode_plus(train_texts, truncation=True, padding=True)\nval_encodings = tokenizer.batch_encode_plus(val_texts, truncation=True, padding=True)\ntest_encodings = tokenizer.batch_encode_plus(test_texts, truncation=True, padding=True)","4ed80eb8":"import torch\n\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = Dataset(train_encodings, train_labels)\nval_dataset = Dataset(val_encodings, val_labels)\n#test_dataset = Dataset(test_encodings, test_labels)","b3b8f5c8":"from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir='.\/results',         # output directory\n    num_train_epochs=3,              # total number of training epochs\n    per_device_train_batch_size=16,  # batch size per device during training\n    per_device_eval_batch_size=64,   # batch size for evaluation\n    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    logging_dir='.\/logs',            # directory for storing logs\n    logging_steps=50,\n    load_best_model_at_end=True,\n    overwrite_output_dir = True,\n    no_cuda=True\n)\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\")\n\ntrainer = Trainer(\n    model=model,                         # the instantiated \ud83e\udd17 Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=val_dataset             # evaluation dataset\n)\n\ntrainer.train()","530ac018":"sub = pd.read_csv('\/kaggle\/input\/crowdflower-search-relevance\/sampleSubmission.csv.zip')\nsub\n","9081bd82":"out","b71f63f3":"### Using Deeplearning based model for classification"}}