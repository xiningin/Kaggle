{"cell_type":{"f00583e0":"code","4628ea05":"code","c7b37387":"code","c961fec4":"code","31a00680":"code","03b44d92":"code","43713a28":"code","295a4178":"code","1005311f":"code","04447dbc":"code","d0e77445":"code","5de86538":"code","923cb984":"code","a7e7a6aa":"code","e2164b10":"code","d554b226":"code","f78ddd3e":"code","06b9f6ed":"code","342e5b2f":"code","ca937dc1":"code","7bb59f54":"code","6b646505":"code","998b2f46":"code","3d7f7820":"code","e60f6d74":"code","a68b0a86":"code","3082bc1b":"code","530ae785":"code","cbaf7671":"code","88a8de12":"code","60550103":"code","bd9ac81a":"code","0fcc3b45":"code","266dae80":"code","92f5a29a":"code","45f05b65":"code","08671934":"code","d3e0d50a":"code","0a76ea88":"markdown","56592c33":"markdown","d43d67b5":"markdown","e02daf04":"markdown","0f1be3bd":"markdown","24211838":"markdown","ea81d857":"markdown","2fe7b4a8":"markdown","a5dab993":"markdown","07c20e61":"markdown","20c71b3f":"markdown","639792a1":"markdown"},"source":{"f00583e0":"import pandas as pd\nimport numpy as np\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# surpress warnings\nimport warnings\nwarnings.filterwarnings('ignore') \nimport io\nimport requests\nimport re\nimport os\nprint(os.listdir(\"..\/input\"))\nimport sklearn\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\n%matplotlib inline\nfrom matplotlib.ticker import StrMethodFormatter\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.io as pio\npio.templates\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nplt.style.use('seaborn-notebook')\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport seaborn as sns\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelBinarizer\nfrom sklearn.svm import SVC\n\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4628ea05":"# Print train and test columns\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\nprint('Train columns:', train.columns.tolist())\nprint('Test columns:', test.columns.tolist())\ntest_id = test['PassengerId']","c7b37387":"display(train.head())\ndisplay(test.head())","c961fec4":"#This is test file from titanic and gender_submission combined\ntested = pd.read_csv(\"..\/input\/test-file\/tested.csv\")\ny_test = tested['Survived']","31a00680":"y_train = train['Survived'].ravel()\ntrain = train.drop(['Survived'], axis=1)","03b44d92":"train.describe() #only age is missing","43713a28":"def cleanup(listings_clean):\n    listings_clean = listings_clean.replace(' ', np.nan)\n    listings_clean = listings_clean.replace('', np.nan)\n    listings_clean = listings_clean.replace('#VALUE!', np.nan)\n    listings_clean = listings_clean.replace('nan', np.nan)\n    listings_clean = listings_clean.replace('NaN', np.nan)\n    return listings_clean","295a4178":"# cleanup, only get useful features, and replace with nans\ntrain = cleanup(train[[\"Pclass\",\"Age\", \"Sex\", \"SibSp\", \"Parch\"]])\ntest = cleanup(test[[\"Pclass\",\"Age\", \"Sex\", \"SibSp\", \"Parch\"]])","1005311f":"# Where are there missing values?\nmissing_vals = train.isna().sum()\nmissing_vals.index[missing_vals>0]","04447dbc":"# Where are there missing values?\nmissing_vals = test.isna().sum()\nmissing_vals.index[missing_vals>0]","d0e77445":"import missingno as ms\nmatrix = ms.matrix(train,figsize=(40, 10), color = (0.1, 0.4, 0.1), labels=True) ","5de86538":"train = pd.get_dummies(train)\ntest = pd.get_dummies(test)","923cb984":"from sklearn.impute import SimpleImputer\n\nimp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\nimp_mean.fit(train)\n\n# Get the imputed values and convert back into a dataframe (it will return a matrix)\ntrain = pd.DataFrame(imp_mean.transform(train), \n                        columns = train.columns)\ntest = pd.DataFrame(imp_mean.transform(test), \n                        columns = test.columns)","a7e7a6aa":"ms.matrix(train,figsize=(40, 10), color = (0.1, 0.4, 0.1), labels=True) ","e2164b10":"ms.matrix(test,figsize=(40, 10), color = (0.1, 0.4, 0.1), labels=True) ","d554b226":"X_train = train.values\nX_test = test.values ","f78ddd3e":"def classification_evaluation(model, X_train, y_train, X_test, y_test, t = 0.5):\n    train_pred = model.predict_proba(X_train)[:,1]\n    test_pred = model.predict_proba(X_test)[:,1]\n\n    print(\"Train AUC: %.3f\" % metrics.roc_auc_score(y_train, train_pred))\n    print(\"Test AUC: %.3f\" % metrics.roc_auc_score(y_test, test_pred))\n    \n    print(\"\\nTrain Accuracy: %.3f\" %  metrics.accuracy_score(y_train, train_pred > t))\n    print(\"Test Accuracy: %.3f\" %  metrics.accuracy_score(y_test, test_pred > t))\n\n    train_fpr, train_tpr, _ = metrics.roc_curve(y_train, train_pred)\n    test_fpr, test_tpr, _ = metrics.roc_curve(y_test, test_pred)\n\n    # plot the roc curve for the model\n    plt.plot(train_fpr, train_tpr, linestyle='--', label='Train')\n    plt.plot(test_fpr, test_tpr, marker='.', label='Test')\n    # axis labels\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    # show the legend\n    plt.legend()\n    # show the plot\n    plt.show()","06b9f6ed":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\n# Define the grid that we want to search over\nparam_grid = {'C': np.arange(0.001, 1, 0.05), \n              'penalty': ['l2','l1'], \n              'solver': ['liblinear']}\n\n# Define the parameters for the model \ngs = GridSearchCV(LogisticRegression(random_state=42, max_iter = 1000),\n                  return_train_score=True, \n                  param_grid=param_grid, \n                  scoring='accuracy',\n                  cv=5, verbose = 0)\n## Fit the model\nrandom.seed(42)\ngs.fit(X_train, y_train)\nm_lr = gs.best_estimator_\nprint(\"Best parameters: \", gs.best_params_)","342e5b2f":"classification_evaluation(m_lr, X_train, y_train, X_test, y_test)","ca937dc1":"from sklearn.tree import DecisionTreeClassifier\n\n# Define the grid that we want to search over\nparam_grid = {\"max_depth\": np.arange(3,10,1), \"criterion\": ['gini', 'entropy']}\n\n# Define the parameters for the model \ngs = GridSearchCV(DecisionTreeClassifier(random_state=42),\n                  return_train_score=True, \n                  param_grid=param_grid, \n                  scoring='roc_auc',\n                  cv=5, verbose = 0)\n\n## Fit the model\nrandom.seed(1)\ngs.fit(X_train, y_train)\nm_cart = gs.best_estimator_\nprint(\"Best parameters: \", gs.best_params_)","7bb59f54":"from sklearn.tree import plot_tree\n\nfig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (50,10), dpi=200)\nplot_tree(m_cart, feature_names = train.columns, filled = False, fontsize = 8);\nfig.savefig('cart_tree_clf.png')","6b646505":"classification_evaluation(m_cart, X_train, y_train, X_test, y_test)","998b2f46":"from sklearn.ensemble import RandomForestClassifier\nimport time\n\n# Define the parameter grid\nparam_grid = {\n    'n_estimators': [100,200,300],\n     'max_depth': np.arange(3,8,1),\n     'max_features': ['auto'],\n     'min_samples_leaf': [0.01,0.02],\n    'criterion' :['gini']\n}\n        \n# Define the parameters for the model \ngs = GridSearchCV(RandomForestClassifier(random_state=42),\n                  return_train_score=True, \n                  param_grid=param_grid, \n                  scoring='roc_auc',\n                  cv=5, verbose = 0)\n\n## Fit the model\nrandom.seed(1)\nstart_time = time.time()\ngs.fit(X_train, y_train)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\nm_rf = gs.best_estimator_\nprint(\"Best parameters: \", gs.best_params_)","3d7f7820":"# Let's evaluate the model performance\nclassification_evaluation(m_rf, X_train, y_train, X_test, y_test)","e60f6d74":"import xgboost as xgb\n\n# Define the parameter grid\nparam_grid = {\n    'n_estimators': [100,200],\n     'max_depth': np.arange(2,6,1),     \n}\n        \n# Define the parameters for the model \ngs = GridSearchCV(xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n                  return_train_score=True, \n                  param_grid=param_grid, \n                  scoring='roc_auc',\n                  cv=5, verbose = 0)\n\n## Fit the model\nrandom.seed(1)\nstart_time = time.time()\ngs.fit(X_train, y_train)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n\nm_xgb = gs.best_estimator_\nprint(\"Best parameters: \", gs.best_params_)","a68b0a86":"classification_evaluation(m_xgb, X_train, y_train, X_test, y_test)","3082bc1b":"import lightgbm as lgb\n\n# Define the parameter grid\nparam_grid = {\n    'n_estimators': [100,150],\n     'max_depth': np.arange(2,6,1),\n}\n        \n# Define the parameters for the model \ngs = GridSearchCV(lgb.LGBMClassifier(random_state=42),\n                  return_train_score=True, \n                  param_grid=param_grid, \n                  scoring='roc_auc',\n                  cv=5, verbose = 0)\n\n## Fit the model\nrandom.seed(1)\n\nstart_time = time.time()\ngs.fit(X_train, y_train)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n\nm_lgb = gs.best_estimator_\nprint(\"Best parameters: \", gs.best_params_)","530ae785":"classification_evaluation(m_lgb, X_train, y_train, X_test, y_test)","cbaf7671":"from sklearn.neural_network import MLPClassifier\n\n# Define the parameter grid\nparam_grid = {\n     'hidden_layer_sizes': [(128), (128,64)],\n     'alpha': [1e-3, 1e-4]\n}\n\n#It is very important to scale the data for neural networks.\nstd_scaler = StandardScaler()\nstd_scaler.fit(X_train)\nX_train_standardized2 = std_scaler.transform(X_train)\nX_test_standardized2 = std_scaler.transform(X_test)\n        \n# Define the parameters for the model \ngs = GridSearchCV(MLPClassifier(random_state=42),\n                  return_train_score=True, \n                  param_grid=param_grid, \n                  scoring='roc_auc',\n                  cv=5, verbose = 1, n_jobs = -1)\n\n## Fit the model\nrandom.seed(42)\ngs.fit(X_train_standardized2, y_train)\nm_mlp = gs.best_estimator_\nprint(\"Best parameters: \", gs.best_params_)","88a8de12":"classification_evaluation(m_mlp, X_train_standardized2, y_train, X_test_standardized2, y_test)","60550103":"from sklearn.model_selection import train_test_split\n#We keep our train\/test set from previously but we separate our train set into train and val.\nX_train_ensemble, X_val_ensemble, y_train_ensemble, y_val_ensemble = train_test_split(X_train, y_train, \n                                                    train_size = 0.75, random_state = 6,\n                                                   stratify = y_train)","bd9ac81a":"#We previously trained several models. For prototyping sake, we don't do our validation or cross-validation again, but you always should in practice!\n\n#Get predictions on unseen data\ndef create_prediction_data(model_list, X_train, y_train, X_val, y_val, X_test, y_test, verbose=False):\n    df_val = pd.DataFrame()\n    df_test = pd.DataFrame()\n    for key in model_list:\n        model_list[key].fit(X_train, y_train)\n        df_val[key] = model_list[key].predict_proba(X_val)[:,1]\n        df_test[key] = model_list[key].predict_proba(X_test)[:,1]\n        if verbose:\n            print(\"\\n#### \" + key +  \" ####\")\n            print(\"Test AUC: \", metrics.roc_auc_score(y_test, model_list[key].predict_proba(X_test)[:,1]))\n            print(\"Test Acc: \", metrics.accuracy_score(y_test, model_list[key].predict(X_test)))\n        #classification_evaluation(model_list[key], X_train, y_train, X_test, y_test)\n    return df_val, df_test","0fcc3b45":"#We get our ensemble data\nd_1 = {'logreg': m_lr, 'cart': m_cart, 'rf': m_rf, 'xgb': m_xgb, 'lgb': m_lgb}\nd_2 = {'logreg': m_lr, 'rf': m_rf, 'xgb': m_xgb, 'lgb': m_lgb}\nd_3 = {'logreg': m_lr, 'rf': m_rf, 'xgb': m_xgb, 'cart': m_cart}\n\ndf_val_1, df_test_1 = create_prediction_data(d_1, X_train_ensemble, y_train_ensemble, X_val_ensemble, y_val_ensemble, X_test, y_test, verbose = True)\ndf_val_2, df_test_2 = create_prediction_data(d_2, X_train_ensemble, y_train_ensemble, X_val_ensemble, y_val_ensemble, X_test, y_test)\ndf_val_3, df_test_3 = create_prediction_data(d_3, X_train_ensemble, y_train_ensemble, X_val_ensemble, y_val_ensemble, X_test, y_test)","266dae80":"# Weighted average\n# Define the grid that we want to search over\nparam_grid = {'C': np.arange(0.001, 1, 0.05), \n              'penalty': ['l2','l1'], \n              'solver': ['liblinear']}\n\n# Define the parameters for the model \ngs_ensemble = GridSearchCV(LogisticRegression(random_state=42, max_iter = 1000),\n                  return_train_score=True, \n                  param_grid=param_grid, \n                  scoring='roc_auc',\n                  cv=5, verbose = 0)\n## Fit the model\nrandom.seed(1)\ngs_ensemble.fit(df_val_1, y_val_ensemble)\nm_lr_ensemble = gs_ensemble.best_estimator_\nprint(\"Best parameters: \", gs_ensemble.best_params_)","92f5a29a":"print(\"Test AUC: \", metrics.roc_auc_score(y_test, m_lr_ensemble.predict_proba(df_test_1)[:,1]))\nprint(\"Test Acc: \", metrics.accuracy_score(y_test, m_lr_ensemble.predict(df_test_1)))","45f05b65":"# Let's look at the coefficients\ncoef_output = pd.DataFrame({'feature':df_test_1.columns,\n              'coefficient':m_lr_ensemble.coef_[0]})\n\nprint(\"Number of zeros: %d\" % (coef_output.query('coefficient == 0').shape[0]))\ncoef_output.sort_values('coefficient')","08671934":"print(\"LR Test Acc: \", metrics.accuracy_score(y_test, m_lr.predict(X_test)))\nprint(\"LR Ensemble Test Acc: \", metrics.accuracy_score(y_test, m_lr_ensemble.predict(df_test_1)))","d3e0d50a":"output = pd.DataFrame({'PassengerId': test_id, 'Survived': m_lr_ensemble.predict(df_test_1)})\noutput.to_csv('submission.csv', index=False)","0a76ea88":"### Impute Data","56592c33":"## Try Logistic Regression","d43d67b5":"### Clean Data","e02daf04":"## Try Random Forest","0f1be3bd":"## Try Submit Logistic Regression Ensemble","24211838":"## Try Decision Trees","ea81d857":"# Try Ensemble Modelling","2fe7b4a8":"## Try LightGBM","a5dab993":"## Try sklearn Neural Network","07c20e61":"## Setup Evalution","20c71b3f":"Code incorporated lessons from: https:\/\/github.com\/kscummings\/15.S60_2022\/blob\/main\/3_machine_learning_in_python\/Regression%20and%20Classification%20in%20Python.ipynb\n### Load Data","639792a1":"## Try XGBoost"}}