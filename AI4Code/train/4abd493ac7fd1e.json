{"cell_type":{"bbf73ed5":"code","61fca3c6":"code","9429b964":"code","7b2e68dd":"code","4990246d":"code","80eff35a":"code","24cc3725":"code","e81913aa":"code","4314ebd5":"code","73907c59":"code","7e51e9ad":"code","ceb89ce5":"code","57064232":"code","50683761":"code","d245be37":"code","1e270e08":"code","56d8c2e9":"code","99155556":"code","2ebe1965":"code","d04e6b31":"code","073c0bd5":"code","f99ebd12":"code","b7fa55c0":"code","a4c52c84":"code","d9856a96":"code","8701d1bc":"code","8c388733":"code","fb908746":"code","de741918":"markdown","df125e7c":"markdown","85ea2dfc":"markdown","f55362c6":"markdown","e3e8bc0e":"markdown","c895390b":"markdown","5592e4c1":"markdown","a34f1239":"markdown","96026bd3":"markdown","797295e4":"markdown","b83e7e8c":"markdown","7825f80f":"markdown","c821a37b":"markdown","0e6a9134":"markdown","7cda8468":"markdown","f13367f0":"markdown","05d5a7de":"markdown","7af1a5a0":"markdown","7d72eb81":"markdown","06f9d28d":"markdown","ac30a562":"markdown","ce9805e5":"markdown","78538614":"markdown","97072b96":"markdown","31502555":"markdown"},"source":{"bbf73ed5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","61fca3c6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\ncolors = ['royalblue','red','deeppink', 'maroon', 'mediumorchid', 'tan', 'forestgreen', 'olive', 'goldenrod', 'lightcyan', 'navy']\nvectorizer = np.vectorize(lambda x: colors[x % len(colors)])","9429b964":"iris = datasets.load_iris()\nX = iris.data\ny = iris.target\ntarget_names = iris.target_names","7b2e68dd":"lda = LinearDiscriminantAnalysis(n_components=2)\nX_r2 = lda.fit(X, y).transform(X)","4990246d":"lda.explained_variance_ratio_","80eff35a":"plt.scatter(X_r2[:,0],X_r2[:,1],c=vectorizer(y))","24cc3725":"pca = PCA(n_components=4)\nX_r = pca.fit(X).transform(X)","e81913aa":"from pylab import *\nsubplot(2,1,1)\ntitle(\"PCA\")\nplt.scatter(X_r[:,0],X_r[:,1],c=vectorizer(y))\nsubplot(2,1,2)\ntitle(\"LDA\")\nplt.scatter(X_r2[:,0],X_r2[:,1],c=vectorizer(y))","4314ebd5":"import seaborn as sns\ndf=pd.DataFrame(zip(X_r[:,0],X_r[:,1],X_r2[:,0],X_r2[:,1],y),columns=[\"pc1\",\"pc2\",\"ld1\",\"ld2\",\"class\"])","73907c59":"subplot(2,1,1)\nsns.boxplot(x='class', y='ld1', data=df)\nsubplot(2,1,2)\nsns.boxplot(x='class', y='ld2', data=df)","7e51e9ad":"subplot(2,1,1)\nsns.boxplot(x='class', y='ld1', data=df)\nsubplot(2,1,2)\nsns.boxplot(x='class', y='pc1', data=df)","ceb89ce5":"pc.columns","57064232":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y)\nlda = LinearDiscriminantAnalysis(n_components=2)\nlda.fit(X_train, y_train)\n#x_test_r2=lda.transform(X_test)","50683761":"from sklearn.metrics import accuracy_score\ny_pred = lda.predict(X_test)\nprint(accuracy_score(y_test, y_pred))","d245be37":"mnist_train=pd.read_csv(\"\/kaggle\/input\/mnist-in-csv\/mnist_train.csv\")\nmnist_test=pd.read_csv(\"\/kaggle\/input\/mnist-in-csv\/mnist_test.csv\")","1e270e08":"mnist_test.head(1)","56d8c2e9":"y_train=mnist_train.iloc[:,0]\nX_train=mnist_train.iloc[:,1:785]","99155556":"lda = LinearDiscriminantAnalysis(n_components=9)\nX_train_r2 = lda.fit(X_train, y_train).transform(X_train)","2ebe1965":"X_train_r2","d04e6b31":"lda.explained_variance_ratio_","073c0bd5":"subplot(1,2,1)\nscatter=plt.scatter(X_train.iloc[:,200],X_train.iloc[:,320],c=y_train,cmap=\"Spectral\")\nhandles, labels = scatter.legend_elements()\nplt.legend(handles, labels,loc=0)\n# Print out labels to see which appears first\nsubplot(1,2,2)\nplt.scatter(X_train_r2[:,0],X_train_r2[:,1],c=y_train,cmap=\"Spectral\")\nhandles, labels = scatter.legend_elements()\nplt.legend(handles, labels)","f99ebd12":"subplot(1,2,1)\nplt.scatter(X_train_r2[:,7],X_train_r2[:,8],c=y_train,cmap=\"Spectral\")\nhandles, labels = scatter.legend_elements()\nplt.legend(handles, labels)\n# Print out labels to see which appears first\nsubplot(1,2,2)\nplt.scatter(X_train_r2[:,0],X_train_r2[:,1],c=y_train,cmap=\"Spectral\")\nhandles, labels = scatter.legend_elements()\nplt.legend(handles, labels)","b7fa55c0":"y_test=mnist_test.iloc[:,0]\nX_test=mnist_test.iloc[:,1:785]","a4c52c84":"from sklearn.metrics import accuracy_score\ny_pred = lda.predict(X_test)\nprint(accuracy_score(y_test, y_pred))","d9856a96":"iris = datasets.load_iris()\nX = iris.data\ny = iris.target","8701d1bc":"lda = LinearDiscriminantAnalysis(n_components=2)\nX_r2 = lda.fit(X, y).transform(X)","8c388733":"pca = PCA(n_components=4)\nX_r = pca.fit(X).transform(X)","fb908746":"from pylab import *\nsubplot(1,3,1)\ntitle(\"PCA\")\nplt.scatter(X_r[:,0],X_r[:,1],c=vectorizer(y))\nsubplot(1,3,2)\ntitle(\"LDA\")\nplt.scatter(X_r2[:,0],X_r2[:,1],c=vectorizer(y))\nsubplot(1,3,3)\ntitle(\"LDA and PCA\")\nplt.scatter(X_r[:,0],X_r2[:,0],c=vectorizer(y))","de741918":"# Question 1: LDA on Iris data  ","df125e7c":"### Visualizing PCA and LDA","85ea2dfc":"# Question 5: Combining LDA and PCA","f55362c6":"* [<font size=4>Question 1: LDA on Iris data<\/font>](#1)   \n* [<font size=4>Question 2:LDA versus PCA Visualization <\/font>](#2)  \n* [<font size=4>Question 3:LDA as a classfier<\/font>](3#) \n* [<font size=4>Question 4: LDA on MNIST<\/font>](#4) \n* [<font size=4>Question 5: Combining LDA and PCA<\/font>](#4) ","e3e8bc0e":"### Fitting PCA","c895390b":"# Question 2: LDA versus PCA on iris","5592e4c1":"### Looking at the accuracy","a34f1239":"### Comparing with random features","96026bd3":"### Loading IRIS","797295e4":"### Transforming LDA","b83e7e8c":"# Quick Recap","7825f80f":"### Loading MNIST Data","c821a37b":"# Question 3: LDA as a classifier ","0e6a9134":"### Loading IRIS","7cda8468":"### Fitting LDA","f13367f0":"### Comparing across LDs","05d5a7de":"* Creates linear transformation of the original features\n* Number of such transformations are 1 less than the number of classes\n* Very Similar to PCA, LDA is supervised","7af1a5a0":"### Comparing across main LD and PC","7d72eb81":"### Accuracy Score","06f9d28d":"# Question 4: LDA on MNIST","ac30a562":"### Fitting LDA","ce9805e5":"x1,x2,x3 = 0.5 *x1 + 0.2 * x2 +0.3 * x3 ","78538614":"### Comparing through Visualization","97072b96":"### Fitting LDA on Iris","31502555":"### Fitting PCA"}}