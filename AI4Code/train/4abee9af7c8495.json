{"cell_type":{"09abe415":"code","08d1da65":"code","08da6039":"code","54d890bb":"code","4d19e961":"code","5f94f47b":"code","43ef09bf":"code","fa666cbe":"code","0d166969":"code","76f27c37":"code","9e75147a":"code","052ac0be":"code","34207e63":"code","a3ec703f":"code","bb722f74":"code","352ab978":"code","c433727c":"code","35a69908":"code","73fdc7c2":"code","0e2c67fa":"code","af17e2da":"code","d107da31":"code","42c4a3c1":"code","60025691":"code","9f3acbb1":"code","670a3b4b":"code","c80e864e":"code","83cf4c44":"code","5f457e1c":"code","bce0ed23":"code","812c9368":"code","a49e200b":"code","68dcddfd":"code","5fc858e5":"code","b73c0ea2":"code","35bdb15f":"code","78752a2d":"code","1817e6e8":"code","1d7d015d":"code","5fcdd20e":"code","77f8b7ab":"code","0eea9d2f":"code","f3fc73c5":"code","df28fd25":"code","4153be86":"code","f310d6a7":"code","5afc1471":"code","7233e8b5":"code","a9ff15ad":"code","dbe80ead":"code","18fd4fe3":"code","bc004571":"code","44e5779d":"code","6542ab9c":"code","78e5a02b":"code","d4b08598":"code","a2ec016f":"code","bdbfc593":"code","3a2fcc7f":"code","833b4e98":"code","6ec7937a":"code","1581c1e7":"code","c6c9fce5":"code","f589a158":"code","5bd786bf":"code","8d10f78a":"code","5b68427a":"markdown","0d3f1c4a":"markdown","a84c0950":"markdown","5680d1da":"markdown","57d342a9":"markdown","3fc6050f":"markdown","16fbc92f":"markdown","120edf3a":"markdown","452c1677":"markdown","592fd525":"markdown","1a83bebe":"markdown","188ac9b3":"markdown","e9435a0e":"markdown","530f7b80":"markdown","b2c7fab2":"markdown","17889d7c":"markdown","950b9ca7":"markdown","6a8d9c46":"markdown","1f2c745f":"markdown","18bd7339":"markdown","c3ac9c4a":"markdown","f3daf668":"markdown","00b5c08c":"markdown","e19545da":"markdown","0177b873":"markdown","2fb44381":"markdown","59a303f6":"markdown","f9b6b427":"markdown","ce494440":"markdown","20969e7c":"markdown","8cb8939e":"markdown","6ae8f013":"markdown","d440b42f":"markdown","2e267d1c":"markdown","d9ff0da5":"markdown","164385f4":"markdown","f661d4fc":"markdown"},"source":{"09abe415":"import time\nfrom datetime import datetime\n\n#measure notebook running time\nstart_time = time.time()\n\n%matplotlib inline\n\nimport os, warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np \nfrom numpy.random import seed\nimport pandas as pd \nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nimport re\nimport string \n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nimport scipy.sparse\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Input, Dropout, BatchNormalization, RNN, SimpleRNN, LSTM, GRU,Embedding, Bidirectional, GlobalMaxPool1D, Conv1D, MaxPooling1D, SpatialDropout1D, Concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import metrics, Model\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport tensorflow as tf\nfrom tensorflow.keras.initializers import Constant\n\n# import xgboost as xgb\n# import lightgbm as lgb\n# from sklearn.svm import SVC\n# from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\n# from sklearn.neighbors import KNeighborsClassifier\n# from sklearn.cluster import KMeans\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_val_score, train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,plot_confusion_matrix, precision_score,recall_score, f1_score, classification_report, accuracy_score\n\nsns.set(style='white', context='notebook', palette='deep', rc={'figure.figsize':(10,8)})\npd.set_option('max_colwidth', None)\nprint(\"loaded ...\")","08d1da65":"# Reproducibility\nRANDOM_SEED = 13\ndef set_seed(sd):\n    seed(sd)\n    np.random.seed(sd)\n    tf.random.set_seed(sd)\n    os.environ['PYTHONHASHSEED'] = str(sd)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\nset_seed(RANDOM_SEED)\nprint(\"random seed set as:\", RANDOM_SEED)","08da6039":"TRAIN = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\nTEST = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nTRAIN['Set'] = \"Train\"\nTEST['Set'] = \"Test\"\nTEST['target'] = -1\nTARGET = TRAIN['target']\nDATA = TRAIN.append(TEST)\nDATA['OriginalText'] = DATA.text\nDATA.reset_index(inplace=True)\n_train = DATA.Set == \"Train\"\n_test = DATA.Set == \"Test\"\n#DATA","54d890bb":"DATA.OriginalText","4d19e961":"DATA.keyword = DATA.keyword.fillna(\"\")\nDATA.keyword = DATA.keyword.str.replace(\"%20\",\" \", regex=True)\nDATA.keyword[DATA.keyword != \"\"].value_counts().head()","5f94f47b":"def add_keyword(text, key):\n    if key == \"\": return text\n    return text + \" \" + key\n\nDATA.text = DATA[['text','keyword']].apply(lambda row: add_keyword(*row), axis = 1)","43ef09bf":"DATA.location = DATA.location.fillna(\"\")\nDATA.location[DATA.location.notna()].value_counts().head()","fa666cbe":"#DATA.text = DATA[['text','location']].apply(lambda row: add_keyword(*row), axis = 1)","0d166969":"disaster = DATA['target'] == 1","76f27c37":"DATA['WC'] = DATA.text.apply(lambda row: len(row.split()))\nfig, ax = plt.subplots(figsize = (8, 5))\nsns.kdeplot(DATA.WC[_train & ~disaster], shade = True, label = 'Not disaster')\nsns.kdeplot(DATA.WC[_train & disaster], shade = True, label = 'Disaster')\nplt.title('Distribution of Word Count')\nplt.legend()\nplt.show()","9e75147a":"DATA['CC'] = DATA.text.apply(lambda row: len(row))\nfig, ax = plt.subplots(figsize = (8, 5))\nsns.kdeplot(DATA.CC[_train & ~disaster], shade = True, label = 'Not disaster')\nsns.kdeplot(DATA.CC[_train & disaster], shade = True, label = 'Disaster')\nplt.title('Distribution of Character Count')\nplt.legend()\nplt.show()","052ac0be":"DATA['AWL'] = DATA.text.apply(lambda row: np.mean([len(w) for w in row.split()]))\nfig, ax = plt.subplots(figsize = (8, 5))\nsns.kdeplot(DATA.AWL[_train & ~disaster], shade = True, label = 'Not disaster')\nsns.kdeplot(DATA.AWL[_train & disaster], shade = True, label = 'Disaster')\nplt.title('Average word length')\nplt.legend()\nplt.show()","34207e63":"DATA['PunctCount'] = DATA.text.apply(lambda row: len([i for i in str(row) if i in string.punctuation]))\nfig, ax = plt.subplots(figsize = (8, 5))\nsns.kdeplot(DATA.PunctCount[_train & ~disaster], shade = True, label = 'Not disaster')\nsns.kdeplot(DATA.PunctCount[_train & disaster], shade = True, label = 'Disaster')\nplt.title('Punctuation count')\nplt.legend()\nplt.show()","a3ec703f":"%%time\nDATA['StopwordCount'] = DATA.text.apply(lambda row: len([w for w in row.lower().split() if w in stopwords.words('english')]))\nDATA[\"SWR\"] = DATA['StopwordCount'] \/ DATA.WC\nfig, ax = plt.subplots(figsize = (8, 5))\nsns.kdeplot(DATA.SWR[_train & ~disaster], shade = True, label = 'Not disaster')\nsns.kdeplot(DATA.SWR[_train & disaster], shade = True, label = 'Disaster')\nplt.title('Stopword ratio (SWR)')\nplt.legend()\nplt.show()","bb722f74":"numeric = ['CC', \"AWL\", 'PunctCount',\"SWR\"]\n\nscaler = StandardScaler()\nDATA[numeric] = scaler.fit_transform(DATA[numeric])\nDATA[numeric].head()","352ab978":"fig, ax = plt.subplots(figsize=(6,6))     \ng = sns.heatmap(DATA[_train][[*numeric,'target']].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")","c433727c":"def get_tags(string):\n    pat = \"#\\w+\"\n    tags = re.findall(pat, string)\n    return [t.strip(\"#\") for t in tags]\n\ndef change_tags(string):\n    hashes = re.findall(r'#\\w+', string)\n    for h in hashes:\n        string = re.sub(h, ' TAG ' + h.split(\"#\")[1], string)\n    return string\n\ndef get_mentions(string):\n    pat = \"@\\w+\"\n    tags = re.findall(pat, string)\n    return [t.strip(\"@\") for t in tags]\n\ndef remove_stop_words(array):\n    return [a for a in array if a not in stopwords.words('english')]\n\nlem = WordNetLemmatizer()\ndef lemmatize(array):\n    lemmatized = [lem.lemmatize(t) for t in array]\n    return lemmatized\n\nstm = PorterStemmer()\ndef portStem(array):\n    stemmed = [stm.stem(t) for t in array]\n    return stemmed\n\ndef do_nothing(tokens):\n    return tokens\n\ndef remove_emoji(text):\n    #thanks, https:\/\/www.kaggle.com\/mariapushkareva\/nlp-disaster-tweets-with-glove-and-lstm\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef decontracted(phrase):\n    #it might be just start of single quotes\n    phrase = re.sub(r\"\\s\\'\", \" \", phrase)\n    \n    #thanks, https:\/\/stackoverflow.com\/a\/47091490\/4154250\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","35a69908":"%%time\n#split CamelCase worsens the score\n#DATA.text = DATA.text.apply(lambda row: \" \".join(re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', row)).split()))\n\nDATA.text = DATA.text.apply(remove_emoji)\nDATA.text = DATA.text.apply(decontracted)\nDATA.text = DATA.text.str.lower()\nDATA['mentions'] = DATA.text.apply(get_mentions)\nDATA.text = DATA.text.str.replace(r'((www\\.[\\s]+)|(https?:\/\/[^\\s]+))',' <URL> ',regex=True)\nDATA.text = DATA.text.apply(change_tags)\nDATA.text = DATA.text.str.replace(r'@\\w+',' USER ',regex=True)\nDATA.text = DATA.text.str.replace(r'\\\\n','',regex=True)\nDATA.text = DATA.text.str.replace(r'\\\\t','',regex=True)\nDATA.text = DATA.text.str.replace(r'rt\\b','retweet',regex=True)\nDATA.text = DATA.text.str.replace(r'&amp;',' and ',regex=True)\nDATA.text = DATA.text.str.replace(r'&lt','',regex=True)\nDATA.text = DATA.text.str.replace(r'&gt','',regex=True)\n\nremove = \"..;''()\u00fb\u00f2,&=!?-:|[]\u00e3\u00a2+\u00aa*<>%$\/+*\u00f3_#\u00ef\u00f7@\u00ec\u00f1~\/\u00e5\u00ea\u00db\u00e5\"\nfor char in remove:\n    DATA.text = DATA.text.str.replace(char,' ', regex=False)\n    \n    \nDATA.text = DATA.text.str.replace(r'\\d+',' NUMBER ', regex=True)\nDATA.text = DATA.text.str.replace(r'\\s+',' ', regex=True)\nDATA.text = DATA.text.str.strip()\n\nDATA.text.head(10)","73fdc7c2":"#_ = DATA[~disaster]['text'][0:50].apply(lambda row: print(row))\n#_ = DATA[disaster]['text'][0:50].apply(lambda row: print(row))\n#_ = DATA['text'][70:80].apply(lambda row: print(row))\n#DATA[disaster][['text','OriginalText']][0:50]","0e2c67fa":"# DATA['traffic'] = DATA.text.apply(lambda row: 1 if 'traffic accident' in row else 0)\n# DATA['traffic'].value_counts()","af17e2da":"# g = sns.catplot(x=\"traffic\",y=\"target\",data=DATA[_train], kind=\"bar\", height = 6, palette = \"muted\")\n# g = g.set_ylabels(\"disaster probability - traffic\")","d107da31":"DATA['nsfw'] = DATA.text.apply(lambda row: 1 if 'nsfw' in row else 0)\nDATA['dance'] = DATA.text.apply(lambda row: 1 if 'dance' in row else 0)\nDATA['aftershock'] = DATA.text.apply(lambda row: 1 if 'aftershock' in row else 0)\nDATA['zombie'] = DATA.text.apply(lambda row: 1 if 'zombie' in row else 0)\nDATA['fan army'] = DATA.text.apply(lambda row: 1 if 'fan army' in row else 0)\nDATA['ebay'] = DATA.text.apply(lambda row: 1 if 'ebay' in row else 0)\nDATA['armageddon'] = DATA.text.apply(lambda row: 1 if 'armageddon' in row else 0)\nDATA['battle'] = DATA.text.apply(lambda row: 1 if 'battle' in row else 0)\nDATA['game'] = DATA.text.apply(lambda row: 1 if 'game' in row else 0)\nDATA['police'] = DATA.text.apply(lambda row: 1 if 'police' in row else 0)\nDATA['girlfriend'] = DATA.text.apply(lambda row: 1 if 'girlfriend' in row else 0)\nDATA['earthquake'] = DATA.text.apply(lambda row: 1 if 'earthquake' in row else 0)\nDATA['flood'] = DATA.text.apply(lambda row: 1 if 'flood' in row else 0)\nDATA['wildfire'] = DATA.text.apply(lambda row: 1 if 'wildfire' in row else 0)","42c4a3c1":"#categorical = ['nsfw','dance','aftershock','zombie','fan army','ebay','armageddon','game','police','girlfriend','earthquake','flood','wildfire']\ncategorical = ['nsfw','dance','aftershock','fan army','ebay','girlfriend',]","60025691":"%%time\ndef boxplot(x,y,**kwargs):\n    sns.barplot(x=x,y=y)\n    _=plt.xticks(rotation=90)\n\nf = pd.melt(DATA[_train], id_vars=['target'], value_vars=categorical)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=6, sharex=False, sharey=True, height=4)\ng = g.map(boxplot, \"value\", \"target\")","9f3acbb1":"fig, ax = plt.subplots(figsize=(10,10))     \ng = sns.heatmap(DATA[_train][[*categorical,'target']].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")","670a3b4b":"DATA['tokens'] = DATA.text.apply(word_tokenize).apply(remove_stop_words)\nDATA['tokens'].head(10)","c80e864e":"DATA['tokens'] = DATA['tokens'].apply(lemmatize)\nDATA['tokens'] = DATA['tokens'].apply(portStem)\nDATA.head(10)","83cf4c44":"maxLenTokens = max([len(row) for row in DATA.tokens])\nDATA.tokens.head(10)","5f457e1c":"DATA['TokenText'] = DATA.tokens.apply(lambda row: \" \".join(row))\nDATA['TokenText'].head()","bce0ed23":"%%time\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(DATA.text)\ntexts_numeric = tokenizer.texts_to_sequences(DATA.text)\nmaxLen = max([len(row) for row in texts_numeric])\ntexts_pad = pad_sequences(texts_numeric, maxLen, padding='post')\nvocab_length = len(tokenizer.word_index) + 1\nDATA['texts_pad'] = list(texts_pad)\nDATA['texts_pad'].head()","812c9368":"# %%time\n# tokenizer = Tokenizer()\n# tokenizer.fit_on_texts(DATA.TokenText)\n# texts_numeric = tokenizer.texts_to_sequences(DATA.TokenText)\n# maxLen = max([len(row) for row in texts_numeric])\n# texts_pad = pad_sequences(texts_numeric, maxLen, padding='post')\n# vocab_length = len(tokenizer.word_index) + 1\n# DATA['texts_pad'] = list(texts_pad)\n# DATA['texts_pad'].head()","a49e200b":"# %%time\n# embeddings_dictionary = dict()\n# embedding_dim = 100\n# glove_file = open('..\/input\/glove6b\/glove.6B.100d.txt')\n# for line in glove_file:\n#     records = line.split()\n#     word = records[0]\n#     vector_dimensions = np.asarray(records[1:], dtype='float32')\n#     embeddings_dictionary[word] = vector_dimensions\n# glove_file.close()","68dcddfd":"%%time\nembeddings_dictionary = dict()\nembedding_dim = 200\nglove_file = open('..\/input\/glove6b\/glove.6B.200d.txt')\nfor line in glove_file:\n    records = line.split()\n    word = records[0]\n    vector_dimensions = np.asarray(records[1:], dtype='float32')\n    embeddings_dictionary[word] = vector_dimensions\nglove_file.close()","5fc858e5":"%%time\nembedding_matrix = np.zeros((vocab_length, embedding_dim))\nfor word, index in tokenizer.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector\nembedding_matrix.shape","b73c0ea2":"X = DATA[_train][\"tokens\"]\nX_PAD = np.stack(DATA[_train][\"texts_pad\"])\nX_PAD = X_PAD[:, :, None]\nTEST_PAD = np.stack(DATA[_test][\"texts_pad\"])\nTEST_PAD = TEST_PAD[:, :, None]","35bdb15f":"train_features = DATA[_train][[*numeric,*categorical]]\ntest_features = DATA[_test][[*numeric,*categorical]]","78752a2d":"count_vectorizer = CountVectorizer(tokenizer=do_nothing, preprocessor=do_nothing, ngram_range=(1,2)) #0.80324\nTRAIN_VECTORS = count_vectorizer.fit_transform(X)\nTEST_VECTORS = count_vectorizer.transform(DATA[_test][\"tokens\"])","1817e6e8":"tfidf_vectorizer = TfidfVectorizer(tokenizer=do_nothing, preprocessor=do_nothing, ngram_range=(1,2), min_df = 1) #min_df=1 default, test!\nTRAIN_VECTORS_TFIDF = tfidf_vectorizer.fit_transform(X)\nTEST_VECTORS_TFIDF = tfidf_vectorizer.transform(DATA[_test][\"tokens\"])","1d7d015d":"#TRAIN_VECTORS_TFIDF.todense().shape","5fcdd20e":"TRAIN_VECTORS = scipy.sparse.hstack([TRAIN_VECTORS, TRAIN_VECTORS_TFIDF])\nTEST_VECTORS = scipy.sparse.hstack([TEST_VECTORS, TEST_VECTORS_TFIDF])","77f8b7ab":"def plot_loss(loss,val_loss):\n    plt.figure()\n    plt.plot(loss)\n    plt.plot(val_loss)\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper right')\n    plt.show()\n\ndef plot_accuracy(acc,val_acc):\n    plt.figure()\n    plt.plot(acc)\n    plt.plot(val_acc)\n    plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show() ","0eea9d2f":"def create_LSTM_model():\n    nlp_input = Input(shape = (maxLen,), name = 'nlp_input')\n    feature_input = Input(shape = (len(numeric)+len(categorical),), name = \"feature_input\")\n    \n    emb = Embedding(input_dim=embedding_matrix.shape[0],\n                        output_dim = embedding_matrix.shape[1], \n                        input_length = maxLen, \n                        embeddings_initializer=Constant(embedding_matrix), \n                        name= \"Embedding\", \n                        trainable=False)(nlp_input)\n    SPD1 = SpatialDropout1D(0.2, seed = RANDOM_SEED, name = \"SP_drop_1\")(emb)\n    Conv1 = Conv1D(32, kernel_size= 3, padding=\"same\", name = \"Conv1D_1\")(SPD1)\n    BN1 = BatchNormalization(name = \"BN_1\")(Conv1)\n    MP1 = MaxPooling1D(2, name = 'MP_1')(BN1)\n    drop1 = Dropout(0.25, seed = RANDOM_SEED, name = \"drop1\")(MP1)\n    D1 = Dense(embedding_matrix.shape[1], activation='relu', name=\"Dense1\")(drop1)\n    drop2 = Dropout(0.25, seed = RANDOM_SEED, name = \"drop2\")(D1)\n    LSTM1 = Bidirectional(LSTM(64, return_sequences = True, dropout=0.1, recurrent_dropout=0.1, kernel_initializer = 'orthogonal', name = \"LSTM1\"))(drop2)\n    GRU1 = GRU(64, return_sequences = False, dropout=0.1, name = \"GRU1\")(LSTM1)\n    concatenate = Concatenate()([GRU1, feature_input])\n    BN2 = BatchNormalization(name = \"BN_2\")(concatenate)\n    D2 = Dense(64, activation='relu', kernel_initializer = 'he_normal', name=\"Dense2\")(BN2)\n    drop3 = Dropout(0.2, seed = RANDOM_SEED, name = \"drop4\")(D2)\n    D3 = Dense(16, activation='relu', kernel_initializer = 'he_normal', name=\"Dense3\")(drop3)\n    out = Dense(1, activation=\"sigmoid\", name= \"output\")(D3)\n\n    model = Model(inputs=[nlp_input, feature_input], outputs = out, name = \"RNN-LSTM\")\n    return model\n    \nRNN_MODEL = create_LSTM_model()\nRNN_MODEL.summary()","f3fc73c5":"tf.keras.utils.plot_model(RNN_MODEL, show_shapes=True)","df28fd25":"%%time\nRNN_MODEL.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001, epsilon=1e-03), metrics=['binary_accuracy'])\nearly_stopping_monitor = EarlyStopping(patience=25, monitor='val_binary_accuracy')\ncheckpoint = ModelCheckpoint(\"weights.hdf5\", monitor = 'val_binary_accuracy', save_best_only = True)\nRNN_MODEL.fit([X_PAD, train_features],TARGET, callbacks=[checkpoint, early_stopping_monitor], epochs=300, batch_size=64, verbose=0, validation_split=0.3);","4153be86":"RNN_MODEL.load_weights(\"weights.hdf5\")\nplot_loss(RNN_MODEL.history.history['loss'], RNN_MODEL.history.history['val_loss'])\nplot_accuracy(RNN_MODEL.history.history['binary_accuracy'], RNN_MODEL.history.history['val_binary_accuracy'])\n\n_, RNN_MODEL_SCORE = RNN_MODEL.evaluate([X_PAD, train_features],TARGET)\nprint('Train accuracy: {:.2f} %'.format(RNN_MODEL_SCORE*100))","f310d6a7":"class DNN_wrapper:\n    def __init__(self, model):\n        self.model = model\n    def predict(self, df):\n        pred = np.rint(self.model.predict(df))[:,0]\n        return pred.astype(np.int)\n    def predict_proba(self, df):\n        probs = self.model.predict(df)\n        probs2 = np.ones_like(probs) - probs\n        packed = np.concatenate((probs2, probs), axis=1)        \n        return packed\n    \nNN_MODEL = DNN_wrapper(RNN_MODEL)","5afc1471":"# %%time\n# clf = RidgeClassifier(max_iter=None, normalize=False, solver='auto', tol=0.001, random_state = RANDOM_SEED)\n# param_grid = {'alpha': np.logspace(-4, 4, 10)}\n# rc_grid = GridSearchCV(estimator = clf, param_grid=param_grid, cv=4, scoring= \"f1\", )\n# rc_grid.fit(TRAIN_VECTORS,TARGET)\n# print(rc_grid.best_params_)\n# print(rc_grid.best_estimator_)\n# RC_score = rc_grid.best_score_\n# print(RC_score)","7233e8b5":"#RIDGE_MODEL = RidgeClassifier(alpha=21.54434690031882, random_state=RANDOM_SEED)\nRIDGE_MODEL = RidgeClassifier(alpha=0.005994842503189409, random_state=13)\n\nRIDGE_MODEL.fit(TRAIN_VECTORS,TARGET)\nRIDGE_MODEL_SCORE = RIDGE_MODEL.score(TRAIN_VECTORS,TARGET)\nRIDGE_MODEL_SCORE","a9ff15ad":"# %%time\n# clf = MultinomialNB()\n# param_grid = {'alpha': np.logspace(-4, 1, 10)}\n# grid = GridSearchCV(estimator = clf, param_grid=param_grid, cv=4, scoring= \"f1\", )\n# grid.fit(TRAIN_VECTORS,TARGET)\n# print(grid.best_params_)\n# print(grid.best_estimator_)\n# score = grid.best_score_\n# print(RC_score)","dbe80ead":"#NB_MODEL = MultinomialNB(alpha=0.7742636826811278)\nNB_MODEL = MultinomialNB(alpha=2.782559402207126)\nNB_MODEL.fit(TRAIN_VECTORS,TARGET)\nNB_MODEL_SCORE = NB_MODEL.score(TRAIN_VECTORS,TARGET)\nNB_MODEL_SCORE","18fd4fe3":"# %%time\n# clf = RandomForestClassifier(random_state = RANDOM_SEED, n_jobs=-1)\n# #param_grid = {'n_estimators': [50,75,150,300, 500],'max_depth': [*range(3,13), None], 'max_features': [*np.arange(0.5,1.0,0.1),'auto','sqrt',\"log2\"],}\n# param_grid = {'n_estimators': [50, 100, 150],'max_depth': [4, 10, None], 'max_features': ['auto']}\n# param_dist = {'n_estimators': range(50,150),'max_depth': [*range(3,13), None], 'max_features': [*np.arange(0.5,1.0,0.1),'auto','sqrt',\"log2\"]}\n# #grid = GridSearchCV(clf, param_grid, cv=4, scoring= \"accuracy\")\n# #grid = GridSearchCV(clf, param_grid, cv=4, scoring= \"f1\")\n# grid = RandomizedSearchCV(clf, param_distributions = param_dist, cv=4, scoring= \"f1\")\n# grid.fit(TRAIN_VECTORS,TARGET)\n# print(grid.best_params_)\n# print(grid.best_estimator_)\n# score = grid.best_score_\n# print(score)","bc004571":"%%time\n#RF_MODEL = RandomForestClassifier(n_jobs=-1, random_state=RANDOM_SEED)\nRF_MODEL = RandomForestClassifier(n_estimators=131, n_jobs=-1, random_state=13)\n#RF_MODEL = RandomForestClassifier(max_depth=4, max_features=0.7999999999999999, n_estimators=65, n_jobs=-1, random_state=RANDOM_SEED)\n#RF_MODEL = RandomForestClassifier(n_estimators=50, n_jobs=-1, random_state=13)\nRF_MODEL.fit(TRAIN_VECTORS,TARGET)\nRF_MODEL_SCORE = RF_MODEL.score(TRAIN_VECTORS,TARGET)\nRF_MODEL_SCORE","44e5779d":"models = [RIDGE_MODEL,NB_MODEL, RF_MODEL]\nmodel_name = [\"Ridge\", \"NaiveBayes\", \"RF\"]\ntrain_scores = [RIDGE_MODEL_SCORE, NB_MODEL_SCORE, RF_MODEL_SCORE]","6542ab9c":"SCORES = pd.DataFrame(index = ['F1','Precision','Recall','Accuracy'])\n\ndef metrics(pred_tag, y_test, name):\n    w = 53\n    print(\"\\n\")\n    print(\"=\"*w)\n    print(name)\n    print(\"=\"*w)\n    print(\"F1-score: \", f1_score(pred_tag, y_test))\n    print(\"Precision: \", precision_score(pred_tag, y_test))\n    print(\"Recall: \", recall_score(pred_tag, y_test))\n    print(\"Acuracy: \", accuracy_score(pred_tag, y_test))\n    print(\"-\"*w)\n    \n    print(classification_report(pred_tag, y_test))\n    SCORES.loc['F1', name] = f1_score(pred_tag, y_test)\n    SCORES.loc['Precision', name] = precision_score(pred_tag, y_test)\n    SCORES.loc['Recall', name] = recall_score(pred_tag, y_test)\n    SCORES.loc['Accuracy', name] = accuracy_score(pred_tag, y_test)\n    ","78e5a02b":"%%time\nfor i,m in enumerate(models):\n    metrics(m.predict(TRAIN_VECTORS), TARGET, model_name[i])\n    \nmetrics(NN_MODEL.predict([X_PAD, train_features]), TARGET, \"RNN\")","d4b08598":"SCORES","a2ec016f":"%%time\nN_cols = 5\ncol_width = 6\nN_rows = round((len(models) +1) \/ N_cols + 0.49)\nfig, axs = plt.subplots(nrows = N_rows, ncols=N_cols, figsize=(col_width * N_cols, N_rows * col_width))\nfor i,(m,ax) in enumerate(zip(models, axs.flatten())):\n    cm = confusion_matrix(TARGET, m.predict(TRAIN_VECTORS), normalize = 'pred', labels = m.classes_)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=m.classes_)\n    disp.plot(ax=ax)\n    ax.set_title(model_name[i])\n\ndisp = ConfusionMatrixDisplay(confusion_matrix(TARGET, NN_MODEL.predict([X_PAD, train_features]), normalize = 'pred'));\ndisp.plot(ax=axs[-1]);\n_ =axs[-1].set_title(\"RNN\");","bdbfc593":"#selected_models = [\"Ridge\", 'NaiveBayes', \"RF\",\"NN\"]\nselected_models = [\"Ridge\", 'NaiveBayes', \"NN\"]\nTRESHOLD = 0.49","3a2fcc7f":"ALL_TRAIN = pd.DataFrame({\"id\":DATA[_train]['id'],\"target\":DATA[_train]['target']})\nfor i,m in enumerate(models):\n    ALL_TRAIN[model_name[i]] = m.predict(TRAIN_VECTORS)\n    \nALL_TRAIN['NN'] = NN_MODEL.predict([X_PAD, train_features])\nALL_TRAIN['Voting'] = ALL_TRAIN[selected_models].mean(axis=1)\nALL_TRAIN['VC'] = ALL_TRAIN['Voting'].apply(lambda row: 1 if row > TRESHOLD else 0)\nALL_TRAIN.head(15)","833b4e98":"#right = ALL_TRAIN.target == ALL_TRAIN.NN\nright = ALL_TRAIN.target == ALL_TRAIN.VC\nDATA[~right & _train][['target','text','tokens','OriginalText']].head(10)","6ec7937a":"ConfusionMatrixDisplay(confusion_matrix(TARGET, ALL_TRAIN.VC, normalize = 'pred')).plot();","1581c1e7":"ALL_TEST = pd.DataFrame({\"id\":DATA[_test]['id']})\nfor i,m in enumerate(models):\n    ALL_TEST[model_name[i]] = m.predict(TEST_VECTORS)\n    \nALL_TEST['NN'] = NN_MODEL.predict([TEST_PAD, test_features])\nALL_TEST['Voting'] = ALL_TEST[selected_models].mean(axis=1)\nALL_TEST['VC'] = ALL_TEST['Voting'].apply(lambda row: 1 if row > TRESHOLD else 0)\nALL_TEST.head(10)","c6c9fce5":"_id = 7618\nprint(DATA[_test]['text'][_id])\nprint(DATA[_test]['tokens'][_id])\nprint(DATA[_test]['OriginalText'][_id])","f589a158":"#output = pd.DataFrame({\"id\": ALL_TEST.id, \"target\": ALL_TEST.Ridge}) #0.80539\n#output = pd.DataFrame({\"id\": ALL_TEST.id, \"target\": ALL_TEST.NaiveBayes}) #0.80539\n#output = pd.DataFrame({\"id\": ALL_TEST.id, \"target\": ALL_TEST.RF}) #0.79190\n#output = pd.DataFrame({\"id\": ALL_TEST.id, \"target\": ALL_TEST.NN}) #0.81152\noutput = pd.DataFrame({\"id\": ALL_TEST.id, \"target\": ALL_TEST.VC}) #0.81428\noutput.head(10)","5bd786bf":"output.to_csv('submission.csv', index=False)\nprint(\"Submission was successfully saved!\")","8d10f78a":"end_time = time.time()\nprint(\"Notebook run time: {:.1f} seconds. Finished at {}\".format(end_time - start_time, datetime.now()) )","5b68427a":"### Classification report","0d3f1c4a":"### Feature Engineering - Numerical","a84c0950":"### Random Forest","5680d1da":"## Preprocessing for Keras","57d342a9":"## Locations (not improving score)","3fc6050f":"## RidgeClassifier","16fbc92f":"---","120edf3a":"## Text preprocessing","452c1677":"---","592fd525":"---","1a83bebe":"## TokenText (experiment)","188ac9b3":"---","e9435a0e":"## Tokenization","530f7b80":"## Test predictions","b2c7fab2":"# MODELS","17889d7c":"# Prediction review","950b9ca7":"# Preprocessing and feature engineering","6a8d9c46":"## Naive Bayes","1f2c745f":"links:\n\n* https:\/\/www.kaggle.com\/tuckerarrants\/disaster-tweets-eda-glove-rnns-bert\n* https:\/\/www.kaggle.com\/mariapushkareva\/nlp-disaster-tweets-with-glove-and-lstm\n* https:\/\/keras.io\/guides\/functional_api\/\n    ","18bd7339":"## Setup","c3ac9c4a":"## VC selection","f3daf668":"# Data","00b5c08c":"---","e19545da":"### Embedings","0177b873":"---","2fb44381":"## Tfidf","59a303f6":"---","f9b6b427":"### Feature Engineering - Categorical","ce494440":"### Confusion matrices","20969e7c":"---","8cb8939e":"## Train predictions","6ae8f013":"## Stack","d440b42f":"### RNN","2e267d1c":"### CountVectorizer","d9ff0da5":"# Submission","164385f4":"## Keywords","f661d4fc":"## Confusion Matrix of voting classifier"}}