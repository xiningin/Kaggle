{"cell_type":{"9a0fccce":"code","3ec4edc1":"code","bb654422":"code","f9ef8e8f":"code","f033e870":"code","10021ddd":"code","eb2d4367":"code","e6c4a781":"code","5316f15c":"code","3a7b1d4c":"code","5138504b":"code","703c0cce":"code","c9d76845":"code","8153e993":"code","ec5577f5":"code","8d0b2b1a":"code","bc853b5a":"code","513be937":"code","31fe9da7":"code","17ecd180":"code","997acc55":"code","4b55c869":"code","03a713e4":"markdown","f2171456":"markdown","32d631dc":"markdown","898e8de6":"markdown","86bcfb1c":"markdown","db05260d":"markdown","8e5c48b1":"markdown","555f5f0c":"markdown"},"source":{"9a0fccce":"!pip install segmentation-models-pytorch\n!pip install albumentations --upgrade","3ec4edc1":"import os\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport re\nimport sys","bb654422":"DATA_DIR = \"..\/input\/lane-detection-for-carla-driving-simulator\"\nx_train_dir = os.path.join(DATA_DIR, 'train')\ny_train_dir = os.path.join(DATA_DIR, 'train_label')\n\nx_valid_dir = os.path.join(DATA_DIR, 'val')\ny_valid_dir = os.path.join(DATA_DIR, 'val_label')","f9ef8e8f":"# helper function for data visualization\ndef visualize(**images):\n    \"\"\"Plot images in one row.\"\"\"\n    n = len(images)\n    plt.figure(figsize=(16, 5))\n    for i, (name, image) in enumerate(images.items()):\n        plt.subplot(1, n, i + 1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(' '.join(name.split('_')).title())\n        plt.imshow(image)\n    plt.show()","f033e870":"from torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom torch import LongTensor","10021ddd":"class CarlaLanesDataset(Dataset):\n    \"\"\" Read images, apply augmentation and preprocessing transformations.\n    \n    Args:\n        images_dir (str): path to images folder\n        masks_dir (str): path to segmentation masks folder\n        class_values (list): values of classes to extract from segmentation mask\n        augmentation (albumentations.Compose): data transfromation pipeline \n            (e.g. flip, scale, etc.)\n        preprocessing (albumentations.Compose): data preprocessing \n            (e.g. noralization, shape manipulation, etc.)\n    \n    \"\"\"\n    \n    CLASSES = ['background', 'left_marker', 'right_marker']\n    \n    def __init__(\n            self, \n            images_dir, \n            masks_dir, \n            classes=None, \n            augmentation=None, \n            preprocessing=None,\n    ):\n        self.ids = os.listdir(images_dir)\n        #random.shuffle(self.ids)\n        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n        get_label_name = lambda fn: re.sub(\".png\", \"_label.png\", fn)\n        self.masks_fps = [os.path.join(masks_dir, get_label_name(image_id)) for image_id in self.ids]\n        \n        # convert str names to class values on masks\n        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n        \n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n    \n    def __getitem__(self, i):\n        \n        # read data\n        image = cv2.imread(self.images_fps[i])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(self.masks_fps[i], 0)\n        \n        # apply augmentations\n        if self.augmentation:\n            sample = self.augmentation(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n        \n        # apply preprocessing\n        if self.preprocessing:\n            sample = self.preprocessing(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n            \n        return image, LongTensor(mask)\n        \n    def __len__(self):\n        return len(self.ids)","eb2d4367":"dataset = CarlaLanesDataset(x_train_dir, y_train_dir, classes=CarlaLanesDataset.CLASSES)\n\nimage, mask = dataset[4] # get some sample\nvisualize(\n    image=image, \n    label = mask\n)","e6c4a781":"import albumentations as albu","5316f15c":"def get_training_augmentation():\n    train_transform = [\n        albu.ShiftScaleRotate(scale_limit=0.1, rotate_limit=0., shift_limit=0.1, p=1, border_mode=0),\n\n        albu.IAAAdditiveGaussianNoise(p=0.2),\n\n        albu.OneOf(\n            [\n                albu.CLAHE(p=1),\n                albu.RandomBrightness(p=1),\n                albu.RandomGamma(p=1),\n            ],\n            p=0.6,\n        ),\n\n        albu.OneOf(\n            [\n                albu.IAASharpen(p=1),\n                albu.Blur(blur_limit=3, p=1),\n                albu.MotionBlur(blur_limit=3, p=1),\n            ],\n            p=0.6,\n        ),\n\n        albu.OneOf(\n            [\n                albu.RandomContrast(p=1),\n                albu.HueSaturationValue(p=1),\n            ],\n            p=0.6,\n        ),\n    ]\n    return albu.Compose(train_transform)\n\n\ndef get_validation_augmentation():\n    return None\n\ndef to_tensor(x, **kwargs):\n    return x.transpose(2, 0, 1).astype('float32')\n\ndef get_preprocessing(preprocessing_fn):   \n    _transform = [\n        albu.Lambda(image=preprocessing_fn),\n        albu.Lambda(image=to_tensor),\n    ]\n    return albu.Compose(_transform)","3a7b1d4c":"#### Visualize resulted augmented images and masks\n\naugmented_dataset = CarlaLanesDataset(\n    x_train_dir, \n    y_train_dir, \n    augmentation=get_training_augmentation(), \n    classes=CarlaLanesDataset.CLASSES,\n)\n\n# same image with different random transforms\nfor i in range(3):\n    image, mask = augmented_dataset[1]\n    visualize(image=image, label=mask)","5138504b":"import torch\nimport numpy as np\nimport segmentation_models_pytorch as smp","703c0cce":"\nloss_string = 'multi_dice_loss'\n\nENCODER = 'efficientnet-b0'\nENCODER_WEIGHTS = 'imagenet'\nACTIVATION = 'softmax2d'\nDEVICE = 'cuda'\n\n# create segmentation model with pretrained encoder\nmodel = smp.FPN(\n    encoder_name=ENCODER, \n    encoder_weights=ENCODER_WEIGHTS, \n    classes=len(CarlaLanesDataset.CLASSES), \n    activation=ACTIVATION,\n    #encoder_depth = 4\n)\n\npreprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)","c9d76845":"train_dataset = CarlaLanesDataset(\n    x_train_dir, \n    y_train_dir, \n    augmentation=get_training_augmentation(), \n    preprocessing=get_preprocessing(preprocessing_fn),\n    classes=CarlaLanesDataset.CLASSES,\n)\n\nvalid_dataset = CarlaLanesDataset(\n    x_valid_dir, \n    y_valid_dir, \n    augmentation=get_validation_augmentation(), \n    preprocessing=get_preprocessing(preprocessing_fn),\n    classes=CarlaLanesDataset.CLASSES,\n)","8153e993":"bs_train = 8 \nbs_valid = 8 \ntrain_loader = DataLoader(train_dataset, batch_size=bs_train, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=bs_valid, shuffle=False)","ec5577f5":"from segmentation_models_pytorch.utils import base\nfrom segmentation_models_pytorch.utils.losses import DiceLoss\nfrom segmentation_models_pytorch.utils.metrics import Accuracy\n\nlabel_left = CarlaLanesDataset.CLASSES.index('left_marker')\nlabel_right = CarlaLanesDataset.CLASSES.index('right_marker')\n\nclass MultiDiceLoss(base.Loss):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.BinaryDiceLossLeft = DiceLoss()\n        self.BinaryDiceLossRight = DiceLoss()\n        \n    def forward(self, y_pr, y_gt):\n        #print(\"shape y_pr:\", y_pr.shape)\n        #print(\"shape y_gt:\", y_gt.shape)\n        # ypr.shape=bs,3,512,1024, ygt.shape=bs,512,1024\n        left_gt = (y_gt == label_left)\n        right_gt = (y_gt == label_right)\n        loss_left = self.BinaryDiceLossLeft.forward(y_pr[:,label_left,:,:] , left_gt)\n        loss_right = self.BinaryDiceLossRight.forward(y_pr[:,label_right,:,:] , right_gt)\n        return (loss_left + loss_right)*0.5","8d0b2b1a":"metrics = []\n\nloss = MultiDiceLoss()\n\noptimizer = torch.optim.Adam([ \n    dict(params=model.parameters(), lr=1e-4),\n    #dict(params=model.parameters(), lr=1e-3)\n])","bc853b5a":"# create epoch runners \n# it is a simple loop of iterating over dataloader`s samples\ntrain_epoch = smp.utils.train.TrainEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    optimizer=optimizer,\n    device=DEVICE,\n    verbose=True,\n)\n\nvalid_epoch = smp.utils.train.ValidEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    device=DEVICE,\n    verbose=True,\n)","513be937":"# train model\nbest_loss = 1e10\n\nfor i in range(0, 5):\n    \n    print('\\nEpoch: {}'.format(i))\n    train_logs = train_epoch.run(train_loader)\n    valid_logs = valid_epoch.run(valid_loader)\n    \n    # do something (save model, change lr, etc.)\n    if best_loss > valid_logs[loss_string]:\n        best_loss = valid_logs[loss_string]\n        torch.save(model, '.\/best_model_{}.pth'.format(loss_string))\n        print('Model saved!')\n        \n    if i == 3:\n        optimizer.param_groups[0]['lr'] = 1e-5\n        print('Decrease decoder learning rate to 1e-5!')","31fe9da7":"# load best saved checkpoint\nbest_model = torch.load('.\/best_model_multi_dice_loss.pth')","17ecd180":"test_best_model = True\nif test_best_model:\n    # create test dataset\n    test_dataset = CarlaLanesDataset(\n        x_valid_dir, \n        y_valid_dir, \n        augmentation=get_validation_augmentation(), \n        preprocessing=get_preprocessing(preprocessing_fn),\n        classes=CarlaLanesDataset.CLASSES,\n    )\n\n    test_dataloader = DataLoader(test_dataset)\n\n    # evaluate model on test set\n    test_epoch = smp.utils.train.ValidEpoch(\n        model=best_model,\n        loss=loss,\n        metrics=metrics,\n        device=DEVICE,\n    )\n\n    logs = test_epoch.run(test_dataloader)","997acc55":"# test dataset without transformations for image visualization\ntest_dataset_vis = CarlaLanesDataset(\n    x_valid_dir, y_valid_dir, \n    classes=CarlaLanesDataset.CLASSES,\n    preprocessing=get_preprocessing(preprocessing_fn)\n)","4b55c869":"for i in range(3):\n    n = np.random.choice(len(test_dataset_vis))\n    \n    image_vis = test_dataset_vis[n][0].astype('uint8')\n    image, gt_mask = test_dataset_vis[n]\n    \n    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n    pr_mask_left = best_model.predict(x_tensor)[0,1,:,:] \n    pr_mask_left = (pr_mask_left.cpu().numpy())\n\n    pr_mask_right = best_model.predict(x_tensor)[0,2,:,:] \n    pr_mask_right = (pr_mask_right.cpu().numpy())\n        \n    visualize( \n        ground_truth_mask=gt_mask, \n        predicted_mask_left=pr_mask_left,\n        predicted_mask_right=pr_mask_right\n    )","03a713e4":"## Create model and train","f2171456":"This notebook is based on the [example notebook in the smp repo](https:\/\/github.com\/qubvel\/segmentation_models.pytorch\/blob\/master\/examples\/cars%20segmentation%20(camvid).ipynb). It is modified to work with this dataset.","32d631dc":"### DataLoader","898e8de6":"## Visualize predictions","86bcfb1c":"## Test best saved model","db05260d":"# Sample Solution for \"Lane Detection for Carla Driving Simulator\"","8e5c48b1":"### Augmentations\nData augmentation is a powerful technique to increase the amount of your data and prevent model overfitting.\n\nNote that we do not apply horizontal flip, since this would also flip our labels (right boundary would be labeled left boundary).\n\nAll transforms can be easily applied with Albumentations - fast augmentation library.","555f5f0c":"## Loading Data"}}