{"cell_type":{"36ed3ffb":"code","b6260398":"code","53e3ee2a":"code","26251239":"code","103c43bf":"code","e93b0ad7":"code","1a1763f5":"code","d8b1cb77":"code","4fe789c0":"code","b03e3517":"code","f9cadab7":"code","9c341176":"code","7e41e191":"code","8031d786":"code","2ea7abcc":"code","ee119194":"code","87d68f1c":"code","d0e085e0":"code","032dac5c":"code","2a3df566":"code","986c920c":"code","25affe60":"code","c7d6878d":"code","d78d139f":"code","54435449":"code","9da8686e":"code","0faad2a6":"code","4f47cd2c":"code","bbc51059":"code","35da5057":"code","78add650":"code","041f66e1":"code","a4ba3d34":"code","3dea480c":"code","411348aa":"code","a2393a4d":"code","9cd798f9":"code","d51b603f":"code","93f2441f":"code","c39e4dd5":"code","681092be":"code","27fb8a65":"code","c7e07eec":"code","afd88aa0":"code","f3b8f2dd":"code","879edd8a":"code","a66ee678":"code","8b0922f6":"code","4756f20e":"code","580ddfc5":"code","afea291e":"code","d0d2f9c6":"code","6a48f00e":"code","732a2518":"code","9bcbf493":"code","88a0f7e2":"code","a39f8e2d":"code","77c2706a":"code","7e22e181":"code","8d6108b1":"code","8e6348ff":"code","8b6ebfd6":"code","07c22649":"code","6d731395":"code","e5e67c98":"code","b580b52f":"code","aa90d99a":"code","895a2aa9":"code","4f654d21":"code","a0f3c10e":"code","bbd62344":"code","6362f330":"code","42d46caa":"code","26868fc8":"code","a5c9d795":"code","aab1084f":"code","cb96cbc3":"code","b42868be":"code","336dfd1f":"code","cc682d89":"code","7d2ed84f":"code","78ba5420":"code","bddab849":"code","da64ebba":"code","4907124c":"code","f82bfc48":"code","eb358f8b":"code","7bb5a1bf":"code","ecedd0e0":"code","9e578824":"code","27d37829":"code","7fe7d209":"code","4529abca":"code","fe1d2054":"code","6d807a64":"code","28b1649b":"code","65447fcf":"code","9d772064":"code","12ec9a9f":"code","fbe69197":"code","7368f8bd":"code","40d2a3af":"code","ffea6c7e":"code","3ceea0e3":"code","768228ae":"code","4583af96":"code","0e909052":"code","9a0ace97":"code","7a4fd60a":"code","27a4a611":"code","5cf31f42":"code","2b8b3625":"code","8985a76d":"code","664028a5":"code","3b12bf2e":"code","687cb95f":"code","5c2a2348":"code","e06e82cc":"code","44ed27dd":"code","70e6e8e3":"code","493a75cd":"code","14020e50":"code","47708d5d":"code","a2f05080":"code","0a7fa1d9":"code","e04968d1":"code","56091ab4":"code","e983e387":"code","f2903757":"code","fdb7971f":"code","8f7e38c4":"code","a0fef18e":"code","fb2180b4":"code","bd8ffe93":"code","c64f883c":"code","0da0d2b0":"code","95bb43a8":"code","8688d1ce":"code","8ffeb450":"code","dc1b68c2":"code","bf900c7b":"code","dd7582c3":"code","a73fc745":"code","8a687f05":"code","0313b95b":"code","0595d89a":"code","338a436b":"code","9585266f":"code","ae1204fc":"code","728ae1a7":"code","639b5690":"markdown","aeee1001":"markdown","b92ea90d":"markdown","fef15ccf":"markdown","841a6c5c":"markdown","20f33590":"markdown","453d4c9c":"markdown","c2ce83b9":"markdown","cf8fd5b3":"markdown","0ce14386":"markdown","752da98f":"markdown","b43e28da":"markdown","662325e4":"markdown","15781750":"markdown"},"source":{"36ed3ffb":"import numpy as np # linear algebra\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b6260398":"train = pd.read_csv('..\/input\/amexpert-2019\/train.csv')\nsubmission = pd.read_csv('..\/input\/amexpert-2019\/sample_submission_Byiv0dS.csv')\ncoupon_item_mapping = pd.read_csv('..\/input\/amexpert-2019\/coupon_item_mapping.csv')\ntest = pd.read_csv('..\/input\/amexpert-2019\/test_QyjYwdj.csv')\ncampaign_data = pd.read_csv('..\/input\/amexpert-2019\/campaign_data.csv')\nitem_data = pd.read_csv('..\/input\/amexpert-2019\/item_data.csv')\ncustomer_transaction_data = pd.read_csv('..\/input\/amexpert-2019\/customer_transaction_data.csv')\ncustomer_demographics = pd.read_csv('..\/input\/amexpert-2019\/customer_demographics.csv')","53e3ee2a":"print('train :',train.shape)\nprint('submission :',submission.shape)\nprint('coupon_item_mapping :',coupon_item_mapping.shape)\nprint('test :',test.shape)\nprint('campaign_data :',campaign_data.shape)\nprint('item_data :',item_data.shape)\nprint('customer_transaction_data :',customer_transaction_data.shape)\nprint('customer_demographics :',customer_demographics.shape)","26251239":"train.head()","103c43bf":"train['redemption_status'].value_counts()\/train.shape[0]*100","e93b0ad7":"plt.figure(figsize=(10,10))\nsns.countplot(train['campaign_id'])\nplt.show()","1a1763f5":"train.groupby(['campaign_id']).count()","d8b1cb77":"print(' col1 ',train[train.columns[0]].value_counts().count())\nprint(' col2 ',train[train.columns[1]].value_counts().count())\nprint(' col3 ',train[train.columns[2]].value_counts().count())\nprint(' col4 ',train[train.columns[3]].value_counts().count())\nprint(' col5 ',train[train.columns[4]].value_counts().count())\nprint('shape ', train.shape[0])","4fe789c0":"campaign_data.head()","b03e3517":"print(campaign_data.columns[0],':', campaign_data[campaign_data.columns[0]].value_counts().count())\nprint(campaign_data.columns[1],':', campaign_data[campaign_data.columns[1]].value_counts().count())\nprint(campaign_data.columns[2],':', campaign_data[campaign_data.columns[2]].value_counts().count())\nprint(campaign_data.columns[3],':', campaign_data[campaign_data.columns[3]].value_counts().count())\nprint('shape : ',campaign_data.shape[0])","f9cadab7":"kl, axes = plt.subplots(2,2,figsize=(10,10))\nk=0\nfor i in range(2):\n    for j in range(2):\n        sns.countplot(campaign_data[campaign_data.columns[k]], ax=axes[i,j])\n        k+=1","9c341176":"campaign_data.start_date[0][1:4]","7e41e191":"coupon_item_mapping.head()","8031d786":"print('coupon id:',coupon_item_mapping['coupon_id'].value_counts().count())\nprint('coupon id:',coupon_item_mapping['item_id'].value_counts().count())\nprint('coupon shape:',coupon_item_mapping.shape[0])","2ea7abcc":"coupon_item_mapping['coupon_id'].value_counts()","ee119194":"item_data.head()","87d68f1c":"item_data.shape","d0e085e0":"item_data.category.value_counts()","032dac5c":"customer_transaction_data.shape","2a3df566":"customer_transaction_data.head()","986c920c":"customer_transaction_data.coupon_discount.value_counts()","25affe60":"customer_transaction_data.coupon_discount.value_counts().index.to_frame().plot.hist(bins=200)","c7d6878d":"customer_transaction_data.quantity.value_counts().index.to_frame().plot.hist(bins=500)","d78d139f":"customer_transaction_data.other_discount.value_counts()","54435449":"customer_transaction_data.other_discount.value_counts().index.to_frame().plot.hist(bins=500)","9da8686e":"customer_transaction_data.selling_price.value_counts()","0faad2a6":"customer_transaction_data.selling_price.value_counts().index.to_frame().plot.hist(bins=500)","4f47cd2c":"customer_transaction_data.date.value_counts()","bbc51059":"customer_transaction_data.customer_id.value_counts()","35da5057":"customer_demographics.shape","78add650":"customer_demographics.head()","041f66e1":"customer_demographics.isnull().sum()","a4ba3d34":"customer_demographics.age_range.value_counts()","3dea480c":"customer_demographics.marital_status.value_counts()","411348aa":"customer_demographics.rented.value_counts()","a2393a4d":"customer_demographics.family_size.value_counts()","9cd798f9":"customer_demographics.no_of_children.value_counts()","d51b603f":"customer_demographics['no_of_children'].fillna(0,inplace=True)","93f2441f":"customer_demographics[['marital_status','family_size','no_of_children']].head(10)","c39e4dd5":"customer_demographics['family_size'].loc[customer_demographics['family_size'] == '5+'] = '5'\ncustomer_demographics['no_of_children'].loc[customer_demographics['no_of_children'] == '3+'] = '3'","681092be":"customer_demographics['marital_status'].loc[(customer_demographics['marital_status'].isna()) & \n                      ((customer_demographics['family_size'].astype(int)-customer_demographics['no_of_children'].astype(int)) > 1 )] = 'Married'","27fb8a65":"customer_demographics['marital_status'].fillna('Single', inplace=True)","c7e07eec":"customer_demographics.isnull().sum()","afd88aa0":"test.head()","f3b8f2dd":"test.shape","879edd8a":"plt.figure(figsize=(20,8))\nsns.distplot(test['campaign_id'], bins=30)\nsns.distplot(train['campaign_id'],bins=30)\nplt.show()","a66ee678":"plt.figure(figsize=(20,8))\nsns.distplot(test['coupon_id'], bins=200)\nsns.distplot(train['coupon_id'], bins=200)\nplt.legend(['train','test'])\n# sns.distplot(coupon_item_mapping['coupon_id'], bins=200)\nplt.show()","8b0922f6":"plt.figure(figsize=(20,8))\nsns.distplot(test['customer_id'], bins=200)\nsns.distplot(train['customer_id'], bins=200)\nplt.show()","4756f20e":"plt.figure(figsize=(20,8))\nsns.distplot(test['id'], bins=200)\nsns.distplot(train['id'], bins=200)\nplt.show()","580ddfc5":"print(train.shape)\ntrain.head()","afea291e":"campaign_data.head() ","d0d2f9c6":"\ndef month_year(row):\n    return row[3:]\n\nfrom datetime import date\n\ndef date_diff(row):\n    d0 = date(int('20'+str(row.start_date[6:])), int(row.start_date[3:5]), int(row.start_date[:2]))\n    d1 = date(int('20'+str(row.end_date[6:])), int(row.end_date[3:5]), int(row.end_date[:2]))\n    delta = d1 - d0\n    return int(delta.days)\n\ncampaign_data['date_diff'] = campaign_data.apply(date_diff,axis=1)\ncampaign_data['start_month_year'] = campaign_data.start_date.map(month_year)\ncampaign_data['end_month_year'] = campaign_data.end_date.map(month_year)\ncampaign_data.head()","6a48f00e":"campaign_data = campaign_data.drop(['start_date','end_date'],axis=1)\ncampaign_data['campaign_type'] = campaign_data['campaign_type'].map({'Y':1, 'X':0})\ncampaign_data= pd.concat((campaign_data.drop(['start_month_year','end_month_year'],axis=1),pd.get_dummies(campaign_data[['start_month_year','end_month_year']])),axis=1)\ncampaign_data.head()","732a2518":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ncampaign_data['date_diff'] = scaler.fit_transform(campaign_data[['date_diff']])\ncampaign_data.head()","9bcbf493":"train = train.merge(campaign_data,on = 'campaign_id', how='left')\ntrain.head()","88a0f7e2":"train.shape","a39f8e2d":"print(coupon_item_mapping.shape)\ncoupon_item_mapping.head()","77c2706a":"coupon_item_mapping.item_id.value_counts()","7e22e181":"print(item_data.shape)\nitem_data.head()","8d6108b1":"coupon_item_mapping = coupon_item_mapping.merge(item_data,on='item_id', how='left')\nprint(coupon_item_mapping.shape)\ncoupon_item_mapping.head()","8e6348ff":"coupon_item_mapping.item_id.value_counts()","8b6ebfd6":"df = coupon_item_mapping.groupby('coupon_id', as_index=False)['item_id'].count()\ndf.head()","07c22649":"item_data.head()","6d731395":"print(item_data.shape)\nitem_data.item_id.value_counts().sum()","e5e67c98":"item_data = pd.concat((item_data, pd.get_dummies(item_data.brand_type)), axis=1)\nitem_data.head()","b580b52f":"item_data = pd.concat((item_data, pd.get_dummies(item_data.category)), axis=1)\nitem_data.head()","aa90d99a":"print(item_data.shape)\nitem_data.head()","895a2aa9":"item_data = item_data.drop(['brand_type', 'category'], axis=1)\nprint(item_data.shape)\nitem_data.head()","4f654d21":"coupon_item_mapping = coupon_item_mapping.merge(item_data, on='item_id', how=\"left\")\ncoupon_item_mapping.shape","a0f3c10e":"coupon_item_mapping.columns","bbd62344":"cols = [ 'brand_x', 'brand_type', 'category', 'brand_y']","6362f330":"coupon_item_mapping.head()","42d46caa":"coupon_item_mapping = coupon_item_mapping.drop(cols,axis=1).groupby('coupon_id', as_index = False).agg(['count','sum']).reset_index()\ncoupon_item_mapping.shape","26868fc8":"# List of column names\ncolumns = ['coupon_id']\n\n# Iterate through the variables names\nfor var in coupon_item_mapping.columns.levels[0]:\n    # Skip the id name\n    if var != 'coupon_id':\n        \n        # Iterate through the stat names\n        for stat in coupon_item_mapping.columns.levels[1][:-1]:\n            # Make a new column name for the variable and stat\n            columns.append('item_%s_%s' % (var, stat))\n\nprint(len(columns))","a5c9d795":"coupon_item_mapping.columns = columns\ncoupon_item_mapping.head()","aab1084f":"drop_cols = ['coupon_id',\n    'item_item_id_count',\n 'item_Established_sum',\n 'item_Local_sum',\n 'item_Alcohol_sum',\n 'item_Bakery_sum',\n 'item_Dairy, Juices & Snacks_sum',\n 'item_Flowers & Plants_sum',\n 'item_Fuel_sum',\n 'item_Garden_sum',\n 'item_Grocery_sum',\n 'item_Meat_sum',\n 'item_Miscellaneous_sum',\n 'item_Natural Products_sum',\n 'item_Packaged Meat_sum',\n 'item_Pharmaceutical_sum',\n 'item_Prepared Food_sum',\n 'item_Restauarant_sum',\n 'item_Salads_sum',\n 'item_Seafood_sum',\n 'item_Skin & Hair Care_sum',\n 'item_Travel_sum',\n 'item_Vegetables (cut)_sum']\ncoupon_item_mapping = coupon_item_mapping[drop_cols]\ncoupon_item_mapping","cb96cbc3":"from sklearn.preprocessing import MinMaxScaler\nminmax = MinMaxScaler()\nx = minmax.fit_transform(coupon_item_mapping.drop('coupon_id',axis=1))\nx = pd.DataFrame(x,columns = drop_cols[1:])\nx['coupon_id'] = coupon_item_mapping.coupon_id\nx.head()","b42868be":"train.head()","336dfd1f":"# train = train.merge(coupon_item_mapping, on='coupon_id', how='left')\ntrain = train.merge(x, on='coupon_id', how='left')\ntrain.shape","cc682d89":"customer_transaction_data.head()","7d2ed84f":"cols_ex = ['date']","78ba5420":"customer_transaction_data =customer_transaction_data.drop(cols_ex,axis=1).groupby('customer_id', as_index = False).agg(['count','sum','mean','median']).reset_index()","bddab849":"# List of column names\ncolumns = ['customer_id']\n\n# Iterate through the variables names\nfor var in customer_transaction_data.columns.levels[0]:\n    # Skip the id name\n    if var != 'customer_id':\n        \n        # Iterate through the stat names\n        for stat in customer_transaction_data.columns.levels[1][:-1]:\n            # Make a new column name for the variable and stat\n            columns.append('customer_%s_%s' % (var, stat))\n\nprint(len(columns))\ncolumns","da64ebba":"customer_transaction_data.columns = columns","4907124c":"useful_cols = ['customer_id',\n 'customer_item_id_count',\n     'customer_quantity_count',\n 'customer_quantity_sum',\n 'customer_quantity_mean',\n 'customer_quantity_median',\n 'customer_selling_price_count',\n 'customer_selling_price_sum',\n 'customer_selling_price_mean',\n 'customer_selling_price_median',\n 'customer_other_discount_count',\n 'customer_other_discount_sum',\n 'customer_other_discount_mean',\n 'customer_other_discount_median',\n 'customer_coupon_discount_count',\n 'customer_coupon_discount_sum',\n 'customer_coupon_discount_mean',\n 'customer_coupon_discount_median'\n]\ncustomer_transaction_data = customer_transaction_data[useful_cols]\ncustomer_transaction_data","f82bfc48":"minmax2 = MinMaxScaler()\nx2 = minmax2.fit_transform(customer_transaction_data.drop('customer_id',axis=1))\nx2 = pd.DataFrame(x2,columns = useful_cols[1:])\nx2['customer_id'] = customer_transaction_data.customer_id\nx2.head()","eb358f8b":"train = train.merge(x2, on='customer_id', how='left')\ntrain.shape","7bb5a1bf":"customer_demographics.head()","ecedd0e0":"customer_demographics['no_of_children'].fillna(0,inplace=True)\ncustomer_demographics['family_size'].loc[customer_demographics['family_size'] == '5+'] = '5'\ncustomer_demographics['no_of_children'].loc[customer_demographics['no_of_children'] == '3+'] = '3'","9e578824":"customer_demographics['marital_status'].loc[(customer_demographics['marital_status'].isna()) & \n                      ((customer_demographics['family_size'].astype(int)-customer_demographics['no_of_children'].astype(int)) > 1 )] = 'Married'","27d37829":"customer_demographics['marital_status'].fillna('Single', inplace=True)","7fe7d209":"customer_demographics.head()","4529abca":"customer_demographics =pd.concat((customer_demographics.drop('age_range',axis=1), pd.get_dummies(customer_demographics['age_range'])),axis=1)","fe1d2054":"customer_demographics['marital_status'] = customer_demographics['marital_status'].map({'Married':0, 'Single':1})\ncustomer_demographics.astype(int)","6d807a64":"customer_demographics =customer_demographics.astype(int)\ncustomer_demographics.head()","28b1649b":"scaler2 =MinMaxScaler()\ncustomer_demographics[['family_size','no_of_children','income_bracket']] = scaler2.fit_transform(customer_demographics[['family_size','no_of_children','income_bracket']])\ncustomer_demographics","65447fcf":"train = train.merge(customer_demographics, on='customer_id',how='left')\ntrain.shape","9d772064":"test = pd.read_csv('..\/input\/amexpert-2019\/test_QyjYwdj.csv')\ntest = test.merge(campaign_data,on = 'campaign_id', how='left')\ntest = test.merge(x, on='coupon_id', how='left')\ntest = test.merge(x2, on='customer_id', how='left')\ntest = test.merge(customer_demographics, on='customer_id',how='left')\ntest.shape","12ec9a9f":"train.head()","fbe69197":"train.isnull().sum().sort_values(ascending=False)[:20].index","7368f8bd":"X = train.drop(['id','campaign_id','coupon_id','customer_id','redemption_status'], axis=1)\ny = pd.DataFrame(train[['redemption_status']], columns=['redemption_status'])\nX.shape, y.shape","40d2a3af":"X.head()","ffea6c7e":"corrmat = X.corrwith(y['redemption_status'])\nc1 = corrmat.sort_values()[0:20].index.tolist()\nc1.extend(corrmat.sort_values()[-20:].index.tolist())\nlen(c1)","3ceea0e3":"test_set = test.drop(['id','campaign_id','coupon_id','customer_id'], axis=1)\ntest_set.shape","768228ae":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X[c1], y,test_size=0.25, random_state=78)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","4583af96":"import lightgbm as lgb\nmodel = lgb.LGBMClassifier(random_state=50)\n\n# Training set\ntrain_sets = lgb.Dataset(X_train, label = y_train)\ntest_sets = lgb.Dataset(X_test, label = y_test)","0e909052":"# Default hyperparamters\nhyperparameters = model.get_params()\n\n# Using early stopping to determine number of estimators.\ndel hyperparameters['n_estimators']\n\n# Perform cross validation with early stopping\ncv_results = lgb.cv(hyperparameters, train_sets, num_boost_round = 10000, nfold = 5, metrics = 'auc', \n           early_stopping_rounds = 100, verbose_eval = False, seed = 42)\n\n# Highest score\nbest = cv_results['auc-mean'][-1]\n\n# Standard deviation of best score\nbest_std = cv_results['auc-stdv'][-1]\n\nprint('The maximium ROC AUC in cross validation was {:.5f} with std of {:.5f}.'.format(best, best_std))\nprint('The ideal number of iterations was {}.'.format(len(cv_results['auc-mean'])))","9a0ace97":"from sklearn.metrics import f1_score, classification_report, roc_auc_score\n\n# Optimal number of esimators found in cv\nmodel.n_estimators = len(cv_results['auc-mean'])\n\n# Train and make predicions with model\nmodel.fit(X_train, y_train)\npreds = model.predict_proba(X_test)[:, 1]\nbaseline_auc = roc_auc_score(y_test, preds)\n\nprint('The baseline model scores {:.5f} ROC AUC on the test set.'.format(baseline_auc))","7a4fd60a":"\nimport csv\nfrom hyperopt import STATUS_OK\nfrom timeit import default_timer as timer\n\ndef objective(hyperparameters):\n    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Optimization.\n       Writes a new line to `outfile` on every iteration\"\"\"\n    \n    # Keep track of evals\n    global ITERATION\n    \n    ITERATION += 1\n    \n    # Using early stopping to find number of trees trained\n    if 'n_estimators' in hyperparameters:\n        del hyperparameters['n_estimators']\n    \n    # Retrieve the subsample\n    subsample = hyperparameters['boosting_type'].get('subsample', 1.0)\n    \n    # Extract the boosting type and subsample to top level keys\n    hyperparameters['boosting_type'] = hyperparameters['boosting_type']['boosting_type']\n    hyperparameters['subsample'] = subsample\n    \n    # Make sure parameters that need to be integers are integers\n    for parameter_name in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n        hyperparameters[parameter_name] = int(hyperparameters[parameter_name])\n\n    start = timer()\n    \n    # Perform n_folds cross validation\n    cv_results = lgb.cv(hyperparameters, train_sets, num_boost_round = 10000, nfold = 5, \n                        early_stopping_rounds = 100, metrics = 'auc', seed = 50)\n\n    run_time = timer() - start\n    \n    # Extract the best score\n    best_score = cv_results['auc-mean'][-1]\n    \n    # Loss must be minimized\n    loss = 1 - best_score\n    \n    # Boosting rounds that returned the highest cv score\n    n_estimators = len(cv_results['auc-mean'])\n    \n    # Add the number of estimators to the hyperparameters\n    hyperparameters['n_estimators'] = n_estimators\n\n    # Write to the csv file ('a' means append)\n    of_connection = open(OUT_FILE, 'a')\n    writer = csv.writer(of_connection)\n    writer.writerow([loss, hyperparameters, ITERATION, run_time, best_score])\n    of_connection.close()\n\n    # Dictionary with information for evaluation\n    return {'loss': loss, 'hyperparameters': hyperparameters, 'iteration': ITERATION,\n            'train_time': run_time, 'status': STATUS_OK}","27a4a611":"from hyperopt import hp\nfrom hyperopt.pyll.stochastic import sample","5cf31f42":"# Create the learning rate\nlearning_rate = {'learning_rate': hp.loguniform('learning_rate', np.log(0.005), np.log(0.2))}\nlearning_rate","2b8b3625":"learning_rate_dist = []\n\n# Draw 10000 samples from the learning rate domain\nfor _ in range(10000):\n    learning_rate_dist.append(sample(learning_rate)['learning_rate'])\n    \nplt.figure(figsize = (8, 6))\nsns.kdeplot(learning_rate_dist, color = 'red', linewidth = 2, shade = True);\nplt.title('Learning Rate Distribution', size = 18); plt.xlabel('Learning Rate', size = 16); plt.ylabel('Density', size = 16);","8985a76d":"# Discrete uniform distribution\nnum_leaves = {'num_leaves': hp.quniform('num_leaves', 30, 150, 1)}\nnum_leaves_dist = []\n\n# Sample 10000 times from the number of leaves distribution\nfor _ in range(10000):\n    num_leaves_dist.append(sample(num_leaves)['num_leaves'])\n    \n# kdeplot\nplt.figure(figsize = (8, 6))\nsns.kdeplot(num_leaves_dist, linewidth = 2, shade = True);\nplt.title('Number of Leaves Distribution', size = 18); plt.xlabel('Number of Leaves', size = 16); plt.ylabel('Density', size = 16);","664028a5":"# boosting type domain \nboosting_type = {'boosting_type': hp.choice('boosting_type', \n                                            [{'boosting_type': 'gbdt', 'subsample': hp.uniform('subsample', 0.5, 1)}, \n                                             {'boosting_type': 'dart', 'subsample': hp.uniform('subsample', 0.5, 1)},\n                                             {'boosting_type': 'goss', 'subsample': 1.0}])}\n\n# Draw a sample\nhyperparams = sample(boosting_type)\nhyperparams","3b12bf2e":"# Retrieve the subsample if present otherwise set to 1.0\nsubsample = hyperparams['boosting_type'].get('subsample', 1.0)\n\n# Extract the boosting type\nhyperparams['boosting_type'] = hyperparams['boosting_type']['boosting_type']\nhyperparams['subsample'] = subsample\n\nhyperparams","687cb95f":"# Define the search space\nspace = {\n    'boosting_type': hp.choice('boosting_type', \n                                            [{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}, \n                                             {'boosting_type': 'dart', 'subsample': hp.uniform('dart_subsample', 0.5, 1)},\n                                             {'boosting_type': 'goss', 'subsample': 1.0}]),\n    'num_leaves': hp.quniform('num_leaves', 20, 150, 1),\n    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.5)),\n    'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),\n    'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0),\n    'is_unbalance': hp.choice('is_unbalance', [True, False]),\n}","5c2a2348":"# Sample from the full space\nx = sample(space)\n\n# Conditional logic to assign top-level keys\nsubsample = x['boosting_type'].get('subsample', 1.0)\nx['boosting_type'] = x['boosting_type']['boosting_type']\nx['subsample'] = subsample\n\nx","e06e82cc":"x = sample(space)\nsubsample = x['boosting_type'].get('subsample', 1.0)\nx['boosting_type'] = x['boosting_type']['boosting_type']\nx['subsample'] = subsample\nx","44ed27dd":"# Create a new file and open a connection\nOUT_FILE = 'bayes_test.csv'\nof_connection = open(OUT_FILE, 'w')\nwriter = csv.writer(of_connection)\n\nITERATION = 0\n\n# Write column names\nheaders = ['loss', 'hyperparameters', 'iteration', 'runtime', 'score']\nwriter.writerow(headers)\nof_connection.close()\n\n# Test the objective function\nresults = objective(sample(space))\nprint('The cross validation loss = {:.5f}.'.format(results['loss']))\nprint('The optimal number of estimators was {}.'.format(results['hyperparameters']['n_estimators']))","70e6e8e3":"from hyperopt import tpe\n\n# Create the algorithm\ntpe_algorithm = tpe.suggest","493a75cd":"from hyperopt import Trials\n\n# Record results\ntrials = Trials()","14020e50":"# Create a file and open a connection\nOUT_FILE = 'bayes_test.csv'\nof_connection = open(OUT_FILE, 'w')\nwriter = csv.writer(of_connection)\n\nITERATION = 0\n\n# Write column names\nheaders = ['loss', 'hyperparameters', 'iteration', 'runtime', 'score']\nwriter.writerow(headers)\nof_connection.close()","47708d5d":"from hyperopt import fmin","a2f05080":"# Global variable\nglobal  ITERATION\n%time\nITERATION = 0\nMAX_EVALS = 5\n# Run optimization\nbest = fmin(fn = objective, space = space, algo = tpe.suggest, trials = trials,\n            max_evals = MAX_EVALS)\n\nbest","0a7fa1d9":"# Sort the trials with lowest loss (highest AUC) first\ntrials_dict = sorted(trials.results, key = lambda x: x['loss'])\ntrials_dict[:1]","e04968d1":"results = pd.read_csv(OUT_FILE)\n","56091ab4":"import ast\n\ndef evaluate(results, name):\n    \"\"\"Evaluate model on test data using hyperparameters in results\n       Return dataframe of hyperparameters\"\"\"\n    \n    new_results = results.copy()\n    # String to dictionary\n    new_results['hyperparameters'] = new_results['hyperparameters'].map(ast.literal_eval)\n    \n    # Sort with best values on top\n    new_results = new_results.sort_values('score', ascending = False).reset_index(drop = True)\n    \n    # Print out cross validation high score\n    print('The highest cross validation score from {} was {:.5f} found on iteration {}.'.format(name, new_results.loc[0, 'score'], new_results.loc[0, 'iteration']))\n    \n    # Use best hyperparameters to create a model\n    hyperparameters = new_results.loc[0, 'hyperparameters']\n    model = lgb.LGBMClassifier(**hyperparameters)\n    \n    # Train and make predictions\n    model.fit(X_train, y_train)\n    preds = model.predict_proba(X_test)[:, 1]\n    \n    print('ROC AUC from {} on test data = {:.5f}.'.format(name, roc_auc_score(y_test, preds)))\n    \n    # Create dataframe of hyperparameters\n    hyp_df = pd.DataFrame(columns = list(new_results.loc[0, 'hyperparameters'].keys()))\n\n    # Iterate through each set of hyperparameters that were evaluated\n    for i, hyp in enumerate(new_results['hyperparameters']):\n        hyp_df = hyp_df.append(pd.DataFrame(hyp, index = [0]), \n                               ignore_index = True)\n        \n    # Put the iteration and score in the hyperparameter dataframe\n    hyp_df['iteration'] = new_results['iteration']\n    hyp_df['score'] = new_results['score']\n    \n    return hyp_df","e983e387":"%time\nbayes_results = evaluate(results, name = 'Bayesian')\nbayes_results\n","f2903757":"MAX_EVALS = 10\n\n# Continue training\nbest = fmin(fn = objective, space = space, algo = tpe.suggest, trials = trials,\n            max_evals = MAX_EVALS)","fdb7971f":"import json\n\n# Save the trial results\nwith open('trials.json', 'w') as f:\n    f.write(json.dumps(trials_dict))","8f7e38c4":"bayes_results = pd.read_csv('..\/input\/home-credit-model-tuning\/bayesian_trials_1000.csv').sort_values('score', ascending = False).reset_index()\nrandom_results = pd.read_csv('..\/input\/home-credit-model-tuning\/random_search_trials_1000.csv').sort_values('score', ascending = False).reset_index()\nrandom_results['loss'] = 1 - random_results['score']\n\nbayes_params = evaluate(bayes_results, name = 'Bayesian')\nrandom_params = evaluate(random_results, name = 'random')","a0fef18e":"# Dataframe of just scores\nscores = pd.DataFrame({'ROC AUC': random_params['score'], 'iteration': random_params['iteration'], 'search': 'Random'})\nscores = scores.append(pd.DataFrame({'ROC AUC': bayes_params['score'], 'iteration': bayes_params['iteration'], 'search': 'Bayesian'}))\n\nscores['ROC AUC'] = scores['ROC AUC'].astype(np.float32)\nscores['iteration'] = scores['iteration'].astype(np.int32)\n\nscores.head()","fb2180b4":"best_random_params = random_params.iloc[random_params['score'].idxmax(), :].copy()\nbest_bayes_params = bayes_params.iloc[bayes_params['score'].idxmax(), :].copy()","bd8ffe93":"# Plot of scores over the course of searching\nsns.lmplot('iteration', 'ROC AUC', hue = 'search', data = scores, size = 8);\nplt.scatter(best_bayes_params['iteration'], best_bayes_params['score'], marker = '*', s = 400, c = 'orange', edgecolor = 'k')\nplt.scatter(best_random_params['iteration'], best_random_params['score'], marker = '*', s = 400, c = 'blue', edgecolor = 'k')\nplt.xlabel('Iteration'); plt.ylabel('ROC AUC'); plt.title(\"Validation ROC AUC versus Iteration\");","c64f883c":"import altair as alt\n\nalt.renderers.enable('notebook')\n\nc = alt.Chart(scores).mark_circle().encode(x = 'iteration', y = alt.Y('ROC AUC', \n                                                                  scale = alt.Scale(domain = [0.64, 0.74])),\n                                       color = 'search')\nc.title = 'Validation ROC AUC vs Iteration'\nc","0da0d2b0":"plt.figure(figsize = (20, 8))\nplt.rcParams['font.size'] = 18\n\n# Density plots of the learning rate distributions \nsns.kdeplot(learning_rate_dist, label = 'Sampling Distribution', linewidth = 4)\nsns.kdeplot(random_params['learning_rate'], label = 'Random Search', linewidth = 4)\nsns.kdeplot(bayes_params['learning_rate'], label = 'Bayes Optimization', linewidth = 4)\nplt.vlines([best_random_params['learning_rate'], best_bayes_params['learning_rate']],\n           ymin = 0.0, ymax = 50.0, linestyles = '--', linewidth = 4, colors = ['orange', 'green'])\nplt.legend()\nplt.xlabel('Learning Rate'); plt.ylabel('Density'); plt.title('Learning Rate Distribution');","95bb43a8":"# Iterate through each hyperparameter\nfor i, hyper in enumerate(random_params.columns):\n    if hyper not in ['class_weight', 'n_estimators', 'score', 'is_unbalance',\n                    'boosting_type', 'iteration', 'subsample', 'metric', 'verbose', 'loss', 'learning_rate']:\n        plt.figure(figsize = (14, 6))\n        # Plot the random search distribution and the bayes search distribution\n        if hyper != 'loss':\n            sns.kdeplot([sample(space[hyper]) for _ in range(1000)], label = 'Sampling Distribution', linewidth = 4)\n        sns.kdeplot(random_params[hyper], label = 'Random Search', linewidth = 4)\n        sns.kdeplot(bayes_params[hyper], label = 'Bayes Optimization', linewidth = 4)\n        plt.vlines([best_random_params[hyper], best_bayes_params[hyper]],\n                     ymin = 0.0, ymax = 10.0, linestyles = '--', linewidth = 4, colors = ['orange', 'green'])\n        plt.legend(loc = 1)\n        plt.title('{} Distribution'.format(hyper))\n        plt.xlabel('{}'.format(hyper)); plt.ylabel('Density');\n        plt.show();","8688d1ce":"fig, axs = plt.subplots(1, 4, figsize = (24, 6))\ni = 0\n\n# Plot of four hyperparameters\nfor i, hyper in enumerate(['colsample_bytree', 'learning_rate', 'min_child_samples', 'num_leaves']):\n    \n        # Scatterplot\n        sns.regplot('iteration', hyper, data = bayes_params, ax = axs[i])\n        axs[i].scatter(best_bayes_params['iteration'], best_bayes_params[hyper], marker = '*', s = 200, c = 'k')\n        axs[i].set(xlabel = 'Iteration', ylabel = '{}'.format(hyper), title = '{} over Search'.format(hyper));\n\nplt.tight_layout()","8ffeb450":"fig, axs = plt.subplots(1, 4, figsize = (24, 6))\ni = 0\n\n# Scatterplot of next three hyperparameters\nfor i, hyper in enumerate(['reg_alpha', 'reg_lambda', 'subsample_for_bin', 'subsample']):\n        sns.regplot('iteration', hyper, data = bayes_params, ax = axs[i])\n        axs[i].scatter(best_bayes_params['iteration'], best_bayes_params[hyper], marker = '*', s = 200, c = 'k')\n        axs[i].set(xlabel = 'Iteration', ylabel = '{}'.format(hyper), title = '{} over Search'.format(hyper));\n\nplt.tight_layout()","dc1b68c2":"fig, axs = plt.subplots(1, 2, sharey = True, sharex = True)\n\n# Bar plots of boosting type\nrandom_params['boosting_type'].value_counts().plot.bar(ax = axs[0], figsize = (14, 6), color = 'orange', title = 'Random Search Boosting Type')\nbayes_params['boosting_type'].value_counts().plot.bar(ax = axs[1], figsize = (14, 6), color = 'green', title = 'Bayes Optimization Boosting Type');","bf900c7b":"bars = alt.Chart(random_params, width = 500).mark_bar(color = 'orange').encode(x = 'boosting_type', y = alt.Y('count()', scale = alt.Scale(domain = [0, 400])))\ntext = bars.mark_text(size = 20, align = 'center', baseline = 'bottom').encode(text = 'count()')\n\nbars + text","dd7582c3":"bars = alt.Chart(bayes_params, width = 500).mark_bar(color = 'green').encode(x = 'boosting_type', y = alt.Y('count()', scale = alt.Scale(domain = [0, 800])))\ntext = bars.mark_text(size = 20, align = 'center', baseline = 'bottom').encode(text = 'count()')\n\nbars + text","a73fc745":"random_results['hyperparameters'] = random_results['hyperparameters'].map(ast.literal_eval)\nbayes_results['hyperparameters'] = bayes_results['hyperparameters'].map(ast.literal_eval)","8a687f05":"train_set2 = lgb.Dataset(X, label = y)\n\nhyperparameters = dict(**random_results.loc[0, 'hyperparameters'])\ndel hyperparameters['n_estimators']\n\n# Cross validation with n_folds and early stopping\ncv_results = lgb.cv(hyperparameters, train_set2,\n                    num_boost_round = 10000, early_stopping_rounds = 100, \n                    metrics = 'auc', nfold = 5)\n\nprint('The cross validation score on the full dataset  for Random Search= {:.5f} with std: {:.5f}.'.format(\n    cv_results['auc-mean'][-1], cv_results['auc-stdv'][-1]))\nprint('Number of estimators = {}.'.format(len(cv_results['auc-mean'])))","0313b95b":"model = lgb.LGBMClassifier(n_estimators = len(cv_results['auc-mean']), **hyperparameters)\nmodel.fit(X, y)\n\nfinal_preds = model.predict_proba(test_set)[:, 1]\n\n# submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': preds})\n# submission.to_csv('submission_random_search.csv', index = False)","0595d89a":"preds = model.predict(X_test)\nprint(classification_report(preds, y_test))\nprint(f1_score(preds, y_test))\nprint(roc_auc_score(preds, y_test))","338a436b":"prediction = model.predict(test_set[c1])\nsubmission['redemption_status'] = prediction\nprint(submission.shape)\nsubmission.head()","9585266f":"y['redemption_status'].value_counts()","ae1204fc":"submission['redemption_status'].value_counts()","728ae1a7":"from IPython.display import HTML\nimport base64\n\ndef create_download_link(df, title = \"Download CSV file\", filename = \"output.csv\"):  \n    csv = df.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\ncreate_download_link(submission)","639b5690":"# modeling","aeee1001":"## train to customer_id","b92ea90d":"## train to campaign_data","fef15ccf":"## train to customer_demographic","841a6c5c":"# coupon_item_mapping","20f33590":"# feature engineering","453d4c9c":"# campaign_data","c2ce83b9":"# customer_demographics","cf8fd5b3":" ## train to coupon_item_mapping","0ce14386":"# test","752da98f":"# customer_transaction_data","b43e28da":"# train","662325e4":"# item_data","15781750":"## coupon_item_mapping to item_data"}}