{"cell_type":{"cfe7dacd":"code","0f573541":"code","0514c093":"code","96c35088":"code","32b46ff7":"code","3391f07a":"code","091fd5a0":"code","199f0b43":"code","e706ad22":"code","6e8ddf67":"code","0601bca0":"code","ac753f12":"code","79e82ce9":"code","b00b16ad":"code","3e8669ab":"code","d087f7a7":"code","2d88686c":"code","f4b2b4fe":"code","cda63356":"markdown","ff091965":"markdown","6e713a5e":"markdown","59ee60e8":"markdown","45da6056":"markdown","f2b0e028":"markdown","781d4894":"markdown","d7b8c772":"markdown","64fd2f2f":"markdown","59b2b131":"markdown","a13b9dec":"markdown","324e00de":"markdown"},"source":{"cfe7dacd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings(\"ignore\") # ignore the warnings about file size\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\n%matplotlib inline\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0f573541":"# import des donn\u00e9es\nraw_data = (pd.read_csv('..\/input\/openfoodfacts\/en.openfoodfacts.org.products.csv',sep='\\t', error_bad_lines=False, low_memory=False))\nsample10 = raw_data.sample(frac=0.1) \n# cr\u00e9ation d'un sample de 10% du dataset pour all\u00e9ger la m\u00e9moire du notebook lors du nettoyage\nsample10.head() # 5 premi\u00e8res colonnes et header","0514c093":"sample10.describe()","96c35088":"print('number of columns:',len(sample10.columns)) # return the number of columns \nprint('number of rows:',len(sample10)) # return the number of rows\nprint('number of cells:',len(sample10)*len(sample10.columns))# return the number of data \nprint('number of missing values:',sample10.isna().sum().sum()) # return the number of total missing values\nprint('number of missing values in % : {:.2%}'.format((sample10.isna().sum().sum())\/(len(sample10)*len(sample10.columns)))) # return the percentage of missing values\n\n\n","32b46ff7":"for i in sample10.columns:\n    print(i,'||',sample10[i].dtypes,'||', sample10[i].isna().sum(),'||','{:.2%}'.format((sample10.isna().sum()\/len(sample10))[i])) \n","3391f07a":"missing_data = pd.DataFrame()\nmissing_data['Column variable'] = sample10.columns\nmissing_data['Data type'] = list(sample10.dtypes)\nmissing_data['Missing values count'] = list(sample10.isna().sum())\nmissing_data['% Missing values'] = list((sample10.isna().sum()\/len(sample10))) \nprint(missing_data)","091fd5a0":"# data visualization for missing values of our dataframe called missing_data\n\nto_plot = missing_data.sort_values(by=['% Missing values'])\nfig, ax = plt.subplots(figsize=(18, 9))\nax.set_facecolor(\"#2E2E2E\")\nplt.title('Representation of the % Missing values per column', fontsize=20, y=1.03)\nplt.xlabel('Columns Name', fontsize=14)\nplt.ylabel('% Missing values', fontsize=14)\nplt.grid(True, color=\"#93a1a1\", alpha=0.05)\nplt.xticks(rotation=90) # this line allow us to rotate the x_labels so they can be readable\nplt.plot(to_plot['Column variable'],to_plot['% Missing values'],color=\"#E8FF41\"); \n","199f0b43":"data = sample10.dropna(axis=1, thresh= len(sample10)*0.75) \n\n# axis = 1 will drop columns with missing values, if 0, then it drops the rows by default\n\nfor i in data.columns:\n    print(i,'||',data[i].dtypes,'||', data[i].isna().sum(),'||','{:.2%}'.format((sample10.isna().sum()\/len(sample10))[i])) \n    \n# Let's have a look at our new dataframe below\n","e706ad22":"missing_data2 = pd.DataFrame()\nmissing_data2['Column variable'] = data.columns\nmissing_data2['Data type'] = list(data.dtypes)\nmissing_data2['Missing values count'] = list(data.isna().sum())\nmissing_data2['% Missing values'] = list(data.isna().sum()\/len(data)) \nprint(missing_data2)","6e8ddf67":"# data visualization for missing values of our dataframe called missing_data\n\nto_plot = missing_data2.sort_values(by=['% Missing values'])\nfig, ax = plt.subplots(figsize=(18, 9))\nax.set_facecolor(\"#2E2E2E\")\nplt.title('Representation of the % Missing values per column after cleaning', fontsize=20, y=1.03)\nplt.xlabel('Column names', fontsize=14)\nplt.ylabel('% Missing values', fontsize=14)\nplt.grid(True, color=\"#93a1a1\", alpha=0.05)\nplt.xticks(rotation=90) # this line allow us to rotate the x_labels so they can be readable\nplt.plot(to_plot['Column variable'],to_plot['% Missing values'],color=\"#E8FF41\"); ","0601bca0":"miss_rows=data[data.isnull().all(axis=1)]\n\nprint('Number of rows with entire NaN values is:', len(miss_rows))\n\n","ac753f12":"drop_data = data.dropna(axis=0, how='any', subset=['product_name'], inplace=False)\n\nmissing_data3 = pd.DataFrame()\nmissing_data3['Column variable'] = drop_data.columns\nmissing_data3['Data type'] = list(drop_data.dtypes)\nmissing_data3['Missing values count'] = list(drop_data.isna().sum())\nmissing_data3['% Missing values'] = list(drop_data.isna().sum()\/(len(drop_data))) \nprint(missing_data3)","79e82ce9":"check = pd.DataFrame(drop_data['product_name'].duplicated()) # return a dataframe of boolean containing the column product_name and a second column True or False\ncheck = check.set_index(drop_data['product_name']).set_axis(['Duplicated count'], axis='columns') # change the index to the product name to know what products we are talking about\n# returns True which means that there are product_name that are duplicated\ncheck.head(10)","b00b16ad":"dups = check[check['Duplicated count'] == True]\ncount_dups = dups.groupby(['product_name'], sort=True).count().sort_values(by=['Duplicated count'], axis=0, ascending=False)\nprint(count_dups)","3e8669ab":"mask = drop_data[drop_data.loc[:,['energy-kcal_100g','energy_100g','fat_100g','saturated-fat_100g','carbohydrates_100g','sugars_100g','proteins_100g']].isnull().sum(axis=1) == 9]\n# mask contains all the rows with NaN values on the quantitative variables","d087f7a7":"drop_data_nan = drop_data.append(mask).drop_duplicates(keep=False)\ndrop_data_nan.info()","2d88686c":"import os\nos.chdir(r'.\/')\ndrop_data_nan.to_csv('clean_p2.csv',sep = '\\t',index = True)","f4b2b4fe":"from IPython.display import FileLink\nFileLink(r'clean_p2.csv')","cda63356":"Now we want to dive in deeper in the understanding of the missing values. \nTherefore, we're creating a loop that is going to display for each column, the label, the dtype of the variables and the number of missing values.\n\nTo do so, we've used:\n* .columns() operation that returns the labels of the columns\n* .dtypes argument that is returning the type of the variables \n* isna() operation is returning a boolean of the missing value (either True or False) that we combine with .sum() operation to have to total number of each missing value (NaN) for each columns","ff091965":"# Context\n\nWe're going to work on the database of Open Food Facts that you can find here: https:\/\/world.openfoodfacts.org\/\n\nOpen Food Facts is a non-profit project developed by thousands of volunteers from around the world made by everyone, for everyone.\n\nThe goal of this notebook is to have a better understanding of the data, clean our dataframe and propose an application based on our observations and exploratory manipulations.\n\n# Notebook preparation\n\nThe current notebook has been written on Kaggle due to our low machine performance on Jupyter or Google Collab, explained by the file size which is over 4 Gb. Some of the basic operations such as reading the dataframe were taking a lot of time and was slowing us down in the process. \n\nWe're starting by defining the various librairies that we're going to use in the notebook such as **pandas** to work on the dataframes, **numpy** for linear algebra operations, **matplotlib** for graphics on the missing values and univariable analysis \n\nNote that we've imported the **warnings librairy** in oder to ignore the warning message about the file size (currently over 4 GB) when reading the file.\n","6e713a5e":"# Entire NaN Rows for Quantitative Variables\n\nA step further in our row cleaning, we want to delete the rows that contain only NaN values on the quantitative variables because we won't make much of an application with no quantitative values. \nTo do so, we proceed with two dataframes again:\n* First, we create a dataframe called **mask** and using the operations *.loc* to locate the columns that interest us, *.isnull()* to target only the missing values on those columns and *.sum()* with the condition that NaN values appear on respectively all our columns, we gather all the rows with missing values on the quantitative variables into it. \n* Then, we create our last clean dataframe called **drop_data_nan** that is a combination of **drop_data** - our original dataframe with the clean columns - and **mask** using the *.append()* operation. Using *.drop_duplicates()* and keeping the option *keep=False* we delete all the duplicates values, which allows us to delete all the rows with NaN that we previously gathered in our **mask** dataframe because they obviously are contained in **drop_data** as well.  ","59ee60e8":"# Treating Missing 'product_name' Values \n\nIn our *missing_data2*, we notice that an important number of values is missing in the column named **product_name** and it seems complicated to define our application without the name of the product we're working with. \nWe decide to drop the rows that have a missing value on the **product_name** column.  ","45da6056":"Now, we're iterating the methodology we used previously for displaying our missing values.\nWe create a new dataframe called *missing_data2* that is containing the columns with 40% or less missing values in their columns, then we plot the result to get a new graph of the dataframe. ","f2b0e028":"# Check for entire rows with NaN values\n\nWe notice that we don't have any rows in our dataframe that have missing values for each column.","781d4894":"# Decisions for Treating with Missing Values\n\nIt seems obvious that we don't need to work with columns that have 100% missing values.\nIt seems also smart to drop the columns that have missing values above a certain threshold. \nWe define this threshold based on the variables we want to keep for our application later, which means: fat_per_100g, proteins_per_100g, carbohydrates_per_100g, countries, product_name.\nWe notice that the variables don't exceed 21% missing values but we're working on a sample so this number can have small fluctuations so we'll define 25% in case of variations in future samples.\n\nTo do so, we use:\n* .dropna() in pandas library using the arguments **axis** to work on the columns and **thresh** to drop the columns that have more than 25% of missing values\n\n","d7b8c772":"Now, we want to insert the values above in a new dataframe so it will be easier for us to manipulate the data and use it for data visualization. \n\nIn order to do so, we're using:\n* .DataFrame() to create an empty dataframe in which we're going to add the columns we're interested in such as Product name, Data type, Missing values count, % Missing values\n\n","64fd2f2f":"# Observations\n\nWe're starting by importing the file and put it into a dataframe using pandas library with the pd.read_csv() operation. \n\n**Note:**\n* We're using **sep** to split the data by tabulation \n* **error_bad_lines** will drop the rows that are invalid (due to csv written mistakes in the file)\n* The argument **low_memory** is used for better allocation of the RAM\n\n\nWe're then creating a new dataframe that is going to include a random 10% of the initial dataframe we previously imported that we call *sample10*.\nTo do so, we've used .sample() operation from pandas library using frac=0.1 argument to get 10% of the initial *raw_data*.\n\nNext, to have an idea of the columns and rows we're treating here, we display the five first rows of the dataframe we just created using .head() operation.  \n","59b2b131":"In order to have a better idea of the quality of the data we're treating, we calculate the number of columns, rows, cells, missing values and percentage of missing values out of our dataframe.\n\nTo do so, we use basics arithmetics operations on our dataframe using pandas library such as:\n* len() that is returning the size of the data\n* .isna().sum().sum() returns the total missing valeus of the dataframe\n* .format() that allows us to return a certain number of digit for the % of missing values\n\nWe can already notice that we have an overall **80% of missing values** that we will have to deal with.","a13b9dec":"# Data Visualization for Missing Values\n\nWe're using the **matplotlib** library to display our graph of missing values.\n\nIn order to do so, we:\n* Sort our Missing values using .sort_values() operation on our *missing_data* dataframe\n* Display a figure using .figure()\n* Give a comprehensive label to our axis and title\n\nIn the following graph, the **x axis** represents our Column variable axis and the **y axis** represents the % Missing values axis.\nWe've used various operations found in **matplotlib** library to design our graph.\n","324e00de":"# Duplicated 'product_name' Values Observations\n\nFor us, this step is an observation to know how many duplicated values we can find in the column **product_name** so we know we can keep this information for later in case we need it. \n\nTo do so, we're going to create two dataframes:\n* Our dataframe **check** is going to return boolean values for each *product_name* to locate if the value got a duplicate in the same column. **True** if a duplicate value exist and **False** if not.\n* The next step is about determining the number of duplicate values across the *product_name* column for each cell. In two steps, we firstly create a dataframe called **dups** that contain all the duplicated value from *product_name* in our **check** dataframe. Secondly, we create another dataframe named **count_dups** that we sort by **product_name** and by descending order, that is going to contain the count of duplicated value for each **product_name**."}}