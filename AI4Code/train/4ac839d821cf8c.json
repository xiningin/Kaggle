{"cell_type":{"1d837cae":"code","c3a97371":"code","b2bf1b5f":"code","a679a698":"code","e9ef8d6b":"code","9f67e958":"code","04651df8":"code","898706ea":"code","44991ae8":"code","52bea7d4":"code","e66fcabe":"code","d15c42ad":"code","42e55156":"code","d02f73d5":"code","2a4a8073":"code","49eecd56":"code","f10cdd02":"code","993a9a7a":"code","fb408791":"code","4219aca7":"code","e9a34f06":"code","5790d204":"code","3310831f":"code","0eb4a77b":"markdown","fe031259":"markdown","4d8347e3":"markdown","0ac44124":"markdown","d0086751":"markdown","82d602f9":"markdown"},"source":{"1d837cae":"from itertools import product\nimport random\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport sklearn.datasets as datasets\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm","c3a97371":"plt.rc(\"font\", size=20) #controls default text size\nplt.rc(\"axes\", titlesize=20) #fontsize of the title\nplt.rc(\"axes\", labelsize=20) #fontsize of the x and y labels\nplt.rc(\"xtick\", labelsize=20) #fontsize of the x tick labels\nplt.rc(\"ytick\", labelsize=20) #fontsize of the y tick labels\nplt.rc(\"legend\", fontsize=20) #fontsize of the legend","b2bf1b5f":"random_seed = 16\ntest_size = 0.2","a679a698":"random.seed(random_seed)\nnp.random.seed(random_seed)","e9ef8d6b":"X, y = datasets.make_classification(\n    n_samples=500,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_repeated=0,\n    class_sep=0.5,\n)","9f67e958":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_seed)","04651df8":"x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))","898706ea":"k_values = np.arange(1, int(0.9*len(X_train)), 1)","44991ae8":"train_accuracies = []\ntest_accuracies = []\nclassifiers = []\n\nfor k in tqdm(k_values):\n    clf = KNeighborsClassifier(n_neighbors=k)\n    clf.fit(X_train, y_train)\n    classifiers += [clf]\n    train_accuracies += [clf.score(X_train, y_train)]\n    test_accuracies += [clf.score(X_test, y_test)]","52bea7d4":"fig, ax = plt.subplots(figsize=(15, 10))\nax.plot(k_values, train_accuracies, label=\"Training Accuracy\")\nax.plot(k_values, test_accuracies, label=\"Testing Accuracy\")\nax.set_xlabel(\"K\")\nax.set_ylabel(\"Accuracy\")\nax.legend()\nplt.show()","e66fcabe":"fig, axarr = plt.subplots(2, 2, sharex='col', sharey='row', figsize=(15, 15))\n\nindices = [0, 10, 50, -1]\n\nfor idx, clf, tt in zip(\n    product([0, 1], [0, 1]),\n    [classifiers[i] for i in indices],\n    [f\"k-Nearest Neighbours (k={k_values[i]})\" for i in indices]\n):\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.4)\n    axarr[idx[0], idx[1]].scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=20, edgecolor='k')\n    axarr[idx[0], idx[1]].set_title(tt)\n\nplt.show()","d15c42ad":"depth_values = np.arange(1, 20, 1)","42e55156":"train_accuracies = []\ntest_accuracies = []\nclassifiers = []\n\nfor depth in tqdm(depth_values):\n    clf = DecisionTreeClassifier(max_depth=depth, random_state=random_seed)\n    clf.fit(X_train, y_train)\n    classifiers += [clf]\n    train_accuracies += [clf.score(X_train, y_train)]\n    test_accuracies += [clf.score(X_test, y_test)]","d02f73d5":"fig, ax = plt.subplots(figsize=(15, 10))\nax.plot(depth_values, train_accuracies, label=\"Training Accuracy\")\nax.plot(depth_values, test_accuracies, label=\"Testing Accuracy\")\nax.set_xlabel(\"Maximum Depth\")\nax.set_ylabel(\"Accuracy\")\nax.legend()\nplt.show()","2a4a8073":"fig, axarr = plt.subplots(2, 2, sharex='col', sharey='row', figsize=(15, 15))\n\nindices = [0, 1, 3, -1]\n\nfor idx, clf, tt in zip(\n    product([0, 1], [0, 1]),\n    [classifiers[i] for i in indices],\n    [f\"Decision Tree (max_depth={depth_values[i]})\" for i in indices]\n):\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.4)\n    axarr[idx[0], idx[1]].scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=20, edgecolor='k')\n    axarr[idx[0], idx[1]].set_title(tt)\n\nplt.show()","49eecd56":"n_estimator_values = np.arange(1, 100, 1)\nmax_depth = 5","f10cdd02":"train_accuracies = []\ntest_accuracies = []\nclassifiers = []\n\nfor n_estimators in tqdm(n_estimator_values):\n    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=random_seed)\n    clf.fit(X_train, y_train)\n    classifiers += [clf]\n    train_accuracies += [clf.score(X_train, y_train)]\n    test_accuracies += [clf.score(X_test, y_test)]","993a9a7a":"fig, ax = plt.subplots(figsize=(15, 10))\nax.plot(n_estimator_values, train_accuracies, label=\"Training Accuracy\")\nax.plot(n_estimator_values, test_accuracies, label=\"Testing Accuracy\")\nax.set_xlabel(\"Number of Estimators\")\nax.set_ylabel(\"Accuracy\")\nax.legend()\nplt.show()","fb408791":"fig, axarr = plt.subplots(2, 2, sharex='col', sharey='row', figsize=(15, 15))\n\nindices = [0, 3, 10, -1]\n\nfor idx, clf, tt in zip(\n    product([0, 1], [0, 1]),\n    [classifiers[i] for i in indices],\n    [f\"Random Forest (n_estimators={n_estimator_values[i]})\" for i in indices]\n):\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.4)\n    axarr[idx[0], idx[1]].scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=20, edgecolor='k')\n    axarr[idx[0], idx[1]].set_title(tt)\n\nplt.show()","4219aca7":"n_estimator_values = np.arange(1, 100, 1)","e9a34f06":"train_accuracies = []\ntest_accuracies = []\nclassifiers = []\n\nfor n_estimators in tqdm(n_estimator_values):\n    clf = AdaBoostClassifier(n_estimators=n_estimators, random_state=random_seed)\n    clf.fit(X_train, y_train)\n    classifiers += [clf]\n    train_accuracies += [clf.score(X_train, y_train)]\n    test_accuracies += [clf.score(X_test, y_test)]","5790d204":"fig, ax = plt.subplots(figsize=(15, 10))\nax.plot(n_estimator_values, train_accuracies, label=\"Training Accuracy\")\nax.plot(n_estimator_values, test_accuracies, label=\"Testing Accuracy\")\nax.set_xlabel(\"Number of Estimators\")\nax.set_ylabel(\"Accuracy\")\nax.legend()\nplt.show()","3310831f":"fig, axarr = plt.subplots(2, 2, sharex='col', sharey='row', figsize=(15, 15))\n\nindices = [0, 10, 50, -1]\n\nfor idx, clf, tt in zip(\n    product([0, 1], [0, 1]),\n    [classifiers[i] for i in indices],\n    [f\"AdaBoost (n_estimators={n_estimator_values[i]})\" for i in indices]\n):\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.4)\n    axarr[idx[0], idx[1]].scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=20, edgecolor='k')\n    axarr[idx[0], idx[1]].set_title(tt)\n\nplt.show()","0eb4a77b":"# Data","fe031259":"# k-Nearest Neighbours","4d8347e3":"# Random Forest","0ac44124":"# Decision Tree","d0086751":"# Libraries","82d602f9":"# AdaBoost"}}