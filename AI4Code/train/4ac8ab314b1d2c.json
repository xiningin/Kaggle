{"cell_type":{"4935c38e":"code","105525ce":"code","df18e168":"code","9fe18e29":"code","9b42df96":"code","12913ce1":"code","a03918a0":"code","74c98a4a":"code","b15a3ba2":"code","9a81e652":"code","fe70fb1e":"code","7e302daa":"code","a4d4eda5":"code","c5a8bdbd":"code","04ec3cff":"code","889fef02":"code","387c1244":"code","b0f9bf14":"code","f0485cc4":"code","976f0183":"code","cc34856c":"code","fd8eb648":"code","160011ce":"code","4bb4648e":"code","fac9ecd6":"code","bc117b04":"code","570a2bad":"code","be16ef2b":"code","0d9de022":"code","9b55f9a7":"code","b82b1106":"code","6e440b9b":"code","44d9b38d":"code","9c4ba6cb":"code","8103d4b3":"code","2ee8988f":"code","5170150d":"code","97b1bed5":"code","5cb0e41d":"code","da4ccda6":"code","7670ff3f":"code","de8163de":"code","3947806e":"code","90683f95":"code","5c238e27":"code","0cf02abc":"code","11b938a6":"code","42dd3f11":"code","823af9c3":"code","85973bed":"code","8b43cabc":"code","4d5c9826":"code","2be58c1c":"code","814ee5d5":"code","760165f7":"code","01f1bfd2":"code","6cd4d453":"code","1667c00f":"code","a9d80884":"code","bbd4dbe3":"code","b188e7fb":"code","1e954b02":"code","31418a94":"code","704088b9":"code","ed5cbb0b":"code","63b9289b":"code","4530a6bc":"code","9ff6fd09":"code","911b03fd":"code","4a048c34":"code","630d4a06":"code","5965718e":"code","c95a292f":"code","2bd21f9c":"code","5dd2b22d":"code","ef3ca679":"code","28cbfabc":"code","528c3359":"code","42aee00a":"code","9583350d":"markdown","ba9b6a36":"markdown","df4fbcee":"markdown","abc56bd4":"markdown","49428562":"markdown","8f45cc2d":"markdown","1e110c60":"markdown","b7b2f56b":"markdown","f325ff05":"markdown","7a587840":"markdown"},"source":{"4935c38e":"import numpy as np\nimport math\nimport re\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nimport seaborn as sns\nimport spacy as sp\nimport string\nimport random\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nimport tensorflow_datasets as tfds\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode,iplot\nimport plotly.express as px\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split","105525ce":"data = pd.read_csv('..\/input\/brazilian-ecommerce\/olist_order_reviews_dataset.csv')\ndata","df18e168":"data.drop(['review_id', 'order_id', 'review_creation_date', 'review_answer_timestamp', 'review_comment_title'], axis = 1, inplace=True)","9fe18e29":"data","9b42df96":"data.info()","12913ce1":"data.isnull().sum(axis=0)","a03918a0":"data.dropna(axis=0, inplace=True)","74c98a4a":"sns.heatmap(data.isnull());","b15a3ba2":"data.info()","9a81e652":"data","fe70fb1e":"data.describe()","7e302daa":"g1 = [go.Box(y=data.review_score,name=\"review_score\",marker=dict(color=\"rgba(0,102,102,0.9)\"),hoverinfo=\"name+y\")]\nlayout1 = go.Layout(title=\"Evaluation Notes\",yaxis=dict(range=[0,13])) \nfig1 = go.Figure(data=g1,layout=layout1)\niplot(fig1)","a4d4eda5":"fig2 = px.histogram(data,x='review_score',color='review_score',template='plotly_dark')\nfig2.show()","c5a8bdbd":"data['review_score'] = data['review_score'].apply(lambda x: 1 if x >= 2 else 0) ","04ec3cff":"posite = data[data['review_score'] == 1 ]\nnegative = data[data['review_score'] == 0]","889fef02":"plt.rcParams['figure.figsize'] = (10, 10)\nplt.style.use('fast')\n\nwc = WordCloud(background_color = 'orange', width = 1500, height = 1500).generate(str(posite['review_comment_message']))\nplt.title('Description Positive', fontsize = 15)\n\nplt.imshow(wc)\nplt.axis('off')\nplt.show()","387c1244":"plt.rcParams['figure.figsize'] = (10, 10)\nplt.style.use('fast')\n\nwc = WordCloud(background_color = 'orange', width = 1500, height = 1500).generate(str(negative['review_comment_message']))\nplt.title('Description Negative', fontsize = 15)\n\nplt.imshow(wc)\nplt.axis('off')\nplt.show()","b0f9bf14":"data.head()","f0485cc4":"X = data.iloc[:, 1].values\nX","976f0183":"X.shape","cc34856c":"type(X)","fd8eb648":"y = data.iloc[:, 0].values\ny","160011ce":"X, _, y, _ = train_test_split(X, y, stratify = y)","4bb4648e":"print(X.shape, y.shape )","fac9ecd6":"unique, counts = np.unique(y, return_counts=True)\nunique, counts","bc117b04":"def clean_t(t):\n  t = BeautifulSoup(t, 'lxml').get_text()\n  t = re.sub(r\"@[A-Za-z0-9]+\", ' ', t)\n  t = re.sub(r\"https?:\/\/[A-Za-z0-9.\/]+\", ' ', t)\n  t = re.sub(r\"[^a-zA-Z.!?]\", ' ', t)\n  t = re.sub(r\" +\", ' ', t)\n  return t","570a2bad":"text = 'n\u00e3o gostei dessas roupas'","be16ef2b":"text = clean_t(text)\ntext","0d9de022":"import spacy","9b55f9a7":"nlp = spacy.blank(\"pt\")","b82b1106":"nlp","6e440b9b":"stop_words = sp.lang.pt.STOP_WORDS","44d9b38d":"print(stop_words)","9c4ba6cb":"len(stop_words)","8103d4b3":"string.punctuation","2ee8988f":"def clean_t2(tt):\n  tt = tt.lower()\n  document = nlp(tt)\n\n  words = []\n  for token in document:\n    words.append(token.text)\n\n  words = [word for word in words if word not in stop_words and word not in string.punctuation]\n  words = ' '.join([str(element) for element in words])\n\n  return words","5170150d":"text2 = clean_t2(text)\ntext2","97b1bed5":"data_clean = [clean_t2(clean_t(t)) for t in X]","5cb0e41d":"for _ in range(10):\n  print(data_clean[random.randint(0, len(data_clean) - 1)])","da4ccda6":"data_labels = y","7670ff3f":"data_labels","de8163de":"np.unique(data_labels)","3947806e":"tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(data_clean, target_vocab_size=2**16)","90683f95":"tokenizer.vocab_size","5c238e27":"print(tokenizer.subwords)","0cf02abc":"ids = tokenizer.encode('eu gostei')\nids","11b938a6":"data_inputs = [tokenizer.encode(sentence) for sentence in data_clean]","42dd3f11":"for _ in range(10):\n  print(data_inputs[random.randint(0, len(data_inputs) - 1)])","823af9c3":"max_len = max([len(sentence) for sentence in data_inputs])\nmax_len","85973bed":"data_inputs = tf.keras.preprocessing.sequence.pad_sequences(data_inputs,\n                                                            value = 0,\n                                                            padding = 'post',\n                                                            maxlen=max_len)","8b43cabc":"for _ in range(10):\n  print(data_inputs[random.randint(0, len(data_inputs) - 1)])","4d5c9826":"train_inputs, test_inputs, train_labels, test_labels = train_test_split(data_inputs,\n                                                                        data_labels,\n                                                                        test_size=0.3,\n                                                                        stratify = data_labels)","2be58c1c":"train_inputs[0]","814ee5d5":"train_inputs.shape","760165f7":"train_labels.shape","01f1bfd2":"test_inputs.shape","6cd4d453":"test_labels.shape","1667c00f":"class DCNN(tf.keras.Model):\n\n  def __init__(self,\n               vocab_size,\n               emb_dim=128,\n               nb_filters=50,\n               ffn_units=512,\n               nb_classes=2,\n               dropout_rate=0.1,\n               training=True,\n               name=\"dcnn\"):\n    super(DCNN, self).__init__(name=name)\n    self.embedding = layers.Embedding(vocab_size, emb_dim)\n    self.bigram = layers.Conv1D(filters=nb_filters, kernel_size=2, padding='same', activation='relu')\n    self.trigram = layers.Conv1D(filters=nb_filters, kernel_size=3, padding='same', activation='relu')\n    self.fourgram = layers.Conv1D(filters=nb_filters, kernel_size=4, padding='same', activation='relu')\n    self.pool = layers.GlobalMaxPool1D()\n    \n#estrutura da rede neural\n    self.dense_1 = layers.Dense(units = ffn_units, activation = 'relu')\n    self.dropout = layers.Dropout(rate = dropout_rate)\n    if nb_classes == 2:\n      self.last_dense = layers.Dense(units = 1, activation = 'sigmoid')\n    else:\n      self.last_dense = layers.Dense(units = nb_classes, activation = 'softmax')\n\n  def call(self, inputs, training):\n    x = self.embedding(inputs)\n    x_1 = self.bigram(x)\n    x_1 = self.pool(x_1)\n    x_2 = self.trigram(x)\n    x_2 = self.pool(x_2)\n    x_3 = self.fourgram(x)\n    x_3 = self.pool(x_3)\n\n    merged = tf.concat([x_1, x_2, x_3], axis = -1)\n    merged = self.dense_1(merged)\n    merged = self.dropout(merged, training)\n    output = self.last_dense(merged)\n\n    return output","a9d80884":"vocab_size = tokenizer.vocab_size\nvocab_size","bbd4dbe3":"emb_dim = 200\nnb_filters = 100\nffn_units = 256\nbatch_size = 64\nnb_classes = len(set(train_labels))\nnb_classes","b188e7fb":"dropout_rate = 0.2\nnb_epochs = 5  ","1e954b02":"Dcnn = DCNN(vocab_size=vocab_size, emb_dim=emb_dim, nb_filters=nb_filters,\n            ffn_units=ffn_units, nb_classes=nb_classes, dropout_rate=dropout_rate)","31418a94":"if nb_classes == 2:\n  Dcnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nelse:\n  Dcnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","704088b9":"history = Dcnn.fit(train_inputs, train_labels,\n                   batch_size = batch_size,\n                   epochs = nb_epochs,\n                   verbose = 1,\n                   validation_split = 0.10)","ed5cbb0b":"results = Dcnn.evaluate(test_inputs, test_labels, batch_size=batch_size)\nprint(results)","63b9289b":"y_pred_test = Dcnn.predict(test_inputs)","4530a6bc":"y_pred_test","9ff6fd09":"y_pred_test = (y_pred_test > 0.5)\ny_pred_test","911b03fd":"test_labels","4a048c34":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(test_labels, y_pred_test)\ncm","630d4a06":"sns.heatmap(cm, annot=True)","5965718e":"history.history.keys()","c95a292f":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss progress during training and validation')\nplt.xlabel('Epoch')\nplt.ylabel('Losses')\nplt.legend(['Training loss', 'Validation loss'])","2bd21f9c":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy progress during training and validation')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(['Training accuracy', 'Validation accuracy'])","5dd2b22d":"text = 'ruim demais'\ntext = tokenizer.encode(text)\nDcnn(np.array([text]), training=False).numpy()","ef3ca679":"text = 'gostei muito'\ntext = tokenizer.encode(text)\nDcnn(np.array([text]), training=False).numpy()","28cbfabc":"text = 'quero devolver'\ntext = tokenizer.encode(text)\nDcnn(np.array([text]), training=False).numpy()","528c3359":"text = 'produto fraco'\ntext = tokenizer.encode(text)\nDcnn(np.array([text]), training=False).numpy()","42aee00a":"text = 'chegou antes'\ntext = tokenizer.encode(text)\nDcnn(np.array([text]), training=False).numpy()","9583350d":"# Model building","ba9b6a36":"# Forecasts","df4fbcee":"# Tokenization","abc56bd4":"# Model Evaluation","49428562":"# Training","8f45cc2d":"Very satisfactory, the value the closer to 1, positive comment, the closer to negative 0.\n\nVery satisfactory, the value the closer to 1, positive comment, the closer to negative 0.\n\nIn developing this algorithm I ended up by default removing the accents, it certainly influenced the result of the algorithm that could be better, the algorithms are in Portuguese (Brazilian), that there are many accents and can define the context of a word or phrase., mainly negative.","1e110c60":"# Division of database into training and testing","b7b2f56b":"# **Conclusion**","f325ff05":"# **Brazilian E-Commerce Public Dataset by Olist**\nWelcome! This is a Brazilian ecommerce public dataset of orders made at Olist Store. The dataset has information of 100k orders from 2016 to 2018 made at multiple marketplaces in Brazil. Its features allows viewing an order from multiple dimensions: from order status, price, payment and freight performance to customer location, product attributes and finally reviews written by customers. We also released a geolocation dataset that relates Brazilian zip codes to lat\/lng coordinates.\n\nThis is real commercial data, it has been anonymised, and references to the companies and partners in the review text have been replaced with the names of Game of Thrones great houses.\n\nJoin it With the Marketing Funnel by Olist\nWe have also released a Marketing Funnel Dataset. You may join both datasets and see an order from Marketing perspective now!\n\nInstructions on joining are available on this Kernel.\n\nContext\nThis dataset was generously provided by Olist, the largest department store in Brazilian marketplaces. Olist connects small businesses from all over Brazil to channels without hassle and with a single contract. Those merchants are able to sell their products through the Olist Store and ship them directly to the customers using Olist logistics partners. See more on our website: www.olist.com\n\nAfter a customer purchases the product from Olist Store a seller gets notified to fulfill that order. Once the customer receives the product, or the estimated delivery date is due, the customer gets a satisfaction survey by email where he can give a note for the purchase experience and write down some comments.\n\nhttps:\/\/www.kaggle.com\/olistbr\/brazilian-ecommerce?select=olist_order_reviews_dataset.csv\n","7a587840":"# Padding"}}