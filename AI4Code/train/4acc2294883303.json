{"cell_type":{"2ca8f40a":"code","bc9eb647":"code","da9bd5bc":"code","cd8d89b4":"code","71f52e8b":"code","4c1ea1f2":"code","b6182c15":"code","b25757c4":"code","ed264522":"code","8b77ae9d":"code","1219aa87":"code","e79463eb":"code","c5684fba":"code","9c23152b":"code","ed423253":"code","24ffb118":"code","e9ea54b2":"code","7e9ca948":"code","40ab6ae7":"code","8e57fd8c":"code","e1c7646c":"code","60a18231":"code","2a82c4de":"code","5341cdf4":"code","6a250c27":"code","cdc1d682":"code","04c268f0":"code","f2b61d1a":"code","617b693d":"code","1e5f3c62":"code","da7952ab":"code","a77f17f2":"code","b91f7ba7":"code","a811de4c":"code","9bc0d9da":"markdown","dadbbca9":"markdown","8c0959cb":"markdown","4a678d0b":"markdown","c1fbd5f9":"markdown","5ede3f31":"markdown","beb9435f":"markdown","28cbfea0":"markdown"},"source":{"2ca8f40a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bc9eb647":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.rcParams['font.sans-serif']=['SimHei']\nplt.rcParams['axes.unicode_minus']=False\npd.set_option('display.float_format', '{:.3f}'.format)\n%matplotlib inline\nRANDOM_STATE = 0\n\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor \nfrom sklearn.metrics import classification_report","da9bd5bc":"def RanForest(data, fill_data, target, drop):\n    feature = list(data)\n    feature.pop(list(data).index(target))\n    feature.pop(list(data).index(drop))\n    x = data[feature]\n    y = data[target]\n    x_train, x_test, y_train, y_test = train_test_split(x, y)\n    # \u968f\u673a\u68ee\u6797\u8fdb\u884c\u9884\u6d4b\uff08\u8d85\u53c2\u6570\u8c03\u4f18\uff09\n    rf = RandomForestRegressor()\n    param = {\"n_estimators\": [300, 500, 700, 900, 1100], \"max_depth\":[3, 5, 7, 9, 10,  12]}\n    # \u7f51\u683c\u641c\u7d22\u4e0e\u4ea4\u53c9\u9a8c\u8bc1\n    gc = GridSearchCV(rf, param_grid=param, cv=3)\n    gc.fit(x_train, y_train)\n    print(\"\u51c6\u786e\u7387\uff1a\", gc.score(x_test, y_test))\n    print(\"\u67e5\u770b\u9009\u62e9\u6700\u597d\u7684\u53c2\u6570\u6a21\u578b\uff1a\", gc.best_params_)\n    y_predict = gc.predict(fill_data)\n    return y_predict","cd8d89b4":"def label_missing_values(s):\n    \"\"\" Label missing values (=0) with None \"\"\"\n    if s == 0:\n        return None\n    else:\n        return s","71f52e8b":"def summary(df):\n    \"\"\"\n    \u4f5c\u7528\uff1a\u5145\u5f53\u4e00\u4e2adf.info()\u7684\u6548\u679c\n    index\tdtypes\tMissing\tUniques\n    \u7279\u5f81-->\u7c7b\u578b-->\u7f3a\u5931\u503c-->\u4e0d\u540c\u503c\u4e2a\u6570\n    return pandas.core.frame.DataFrame\u7c7b\u578b\n    \"\"\"\n    summary = pd.DataFrame(df.dtypes, columns=['dtypes'])\n    # \u91cd\u7f6e\u7d22\u5f15\uff0c\u8ba9\u7279\u5f81\u540d\u5b57\u4ece\u7d22\u5f15\u53d8\u6210index\u7684\u4e00\u5217\n    summary = summary.reset_index()\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    return summary","4c1ea1f2":"def num_vs_ctr(df, var1, var2):\n    \"\"\"\n    df: DataFrame --> pandas.core.frame.DataFrame\n    var1: feature --> str\n    var2: target --> str\n    return: \u8fd4\u56de\u4e00\u4e2apandas.core.frame.DataFrame\u7c7b\u578b\u3002\u8868\u8fbe\u51fa\u4e86\u8be5feature\u4e0eOutcome\u4e4b\u95f4\u7684\u6743\u91cd\u5173\u7cfb\n    Tips\uff1a\u6b64\u51fd\u6570\u5e76\u6ca1\u6709\u666e\u9002\u6027\uff0c \u53ea\u9002\u7528\u4e8e\u4e8c\u5206\u7c7b\u95ee\u9898\u3002\n    \"\"\"\n    # \u5bf9var1\u8fdb\u884c\u5206\u7ec4\u7136\u540e\u6c42\u5176\u5e73\u5747\u503c\uff0c \u5bf9\u5e73\u5747\u503c\u8fdb\u884c\u6392\u5e8f\n    ctr = df[[var1, var2]].groupby(var1, as_index=False).mean().sort_values(var2, ascending=False)\n#     print(ctr)\n    # \u5bf9var1\u8fdb\u884c\u5206\u7ec4\u7136\u540e\u6c42\u5176var2\u7684\u4e2a\u6570\uff0c \u5bf9\u4e2a\u6570\u8fdb\u884c\u6392\u5e8f\n    count = df[[var1, var2]].groupby(var1, as_index=False).count().sort_values(var2, ascending=False)\n#     print(count)\n    # \u4ee5var1\u8fdb\u884c\u8fde\u63a5\uff0ccount\u5de6\uff0cctr\u53f3\uff0c\u8fdb\u884c\u5de6\u8fde\u63a5\u3002\u548c\u6570\u636e\u5e93\u7684\u64cd\u4f5c\u4e00\u6837\u3002\n    merge = count.merge(ctr, on=var1, how='left')\n#     print(merge)\n    merge.columns=[var1, '\u603b\u4eba\u6570', '\u60a3\u75c5\u6743\u91cd']\n    merge['\u60a3\u75c5\u4eba\u6570'] = merge['\u603b\u4eba\u6570']*merge['\u60a3\u75c5\u6743\u91cd']\n    merge['\u60a3\u75c5\u4eba\u6570'] = merge['\u60a3\u75c5\u4eba\u6570'].astype('int64')\n    cols = list(merge)\n    cols.insert(2, cols.pop(cols.index('\u60a3\u75c5\u4eba\u6570')))\n    merge = merge.loc[:,cols]\n    return merge","b6182c15":"def crosstab(df, features, target, label_cutoff = 'none'):\n    for feature in features:    \n        # plot the crosstab\n        # pd.crosstab()\u8fd9\u4e2a\u51fd\u6570\u5f88\u5f3a\u5927\uff0c\u4ea4\u53c9\u8868\u3002\u4ea4\u53c9\u8868\u662f\u7528\u4e8e\u7edf\u8ba1\u5206\u7ec4\u9891\u7387\u7684\u7279\u6b8a\u900f\u89c6\u8868, \u8fd4\u56de\u6570\u636e\u5e27(pandas.core.frame.DataFrame)\n        # \u7b2c\u4e00\u4e2a\u53c2\u6570\u4e3aindex\uff0c \u7b2c\u4e8c\u4e2a\u53c2\u6570\u4e3acolumns\n        # \u59cb\u7ec8\u5fc5\u987b\u540c\u65f6\u4f7f\u7528values\u548caggfunc\uff0c values\u53c2\u6570\u6765\u5f15\u5165\u7b2c\u4e09\u4e2a\u8981\u805a\u5408\u7684\u6570\u503c\u53d8\u91cf\uff0c aggfunc\u8bf4\u660e\u9700\u8981\u4f7f\u7528\u7684\u51fd\u6570\u4f8b\u5982\uff1aagggunc=np.mean\n        # margins = True \u5728\u8fb9\u8ddd\u8fdb\u884c\u6c42\u548c\n        # shift+tab \u53ef\u4ee5\u67e5\u770b\u51fd\u6570\u7684\u7528\u6cd5\n        # kind \u4e2d\u6587\uff1a\u7c7b\n        pd.crosstab(df[feature],df[target]).plot(kind='bar', figsize=(20,5), stacked=True)\n        plt.title(feature+' \/ '+target)\n        plt.xlabel(feature)\n        plt.ylabel(feature+' \/ '+target)\n            \n        # display the table obove each chart \n        return num_vs_ctr(df, feature, target) ","b25757c4":"data = pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")\nprint(data.info())\nsummary(data)","ed264522":"fig, ax = plt.subplots(4,2, figsize=(16,16))\nsns.distplot(data.Age, bins = 20, ax=ax[0,0]) \nsns.distplot(data.Pregnancies, bins = 20, ax=ax[0,1]) \nsns.distplot(data.Glucose, bins = 20, ax=ax[1,0]) \nsns.distplot(data.BloodPressure, bins = 20, ax=ax[1,1]) \nsns.distplot(data.SkinThickness, bins = 20, ax=ax[2,0])\nsns.distplot(data.Insulin, bins = 20, ax=ax[2,1])\nsns.distplot(data.DiabetesPedigreeFunction, bins = 20, ax=ax[3,0]) \nsns.distplot(data.BMI, bins = 20, ax=ax[3,1]) \nplt.show()\nfor feature in list(data):\n    try:\n        cnt = data[feature].value_counts()[0]\n    except:\n        cnt = 0\n    print(f\"\u7279\u5f81{feature}\u4e3a\u96f6\u7684\u4e2a\u6570\u4e3a\uff1a\", cnt)","8b77ae9d":"# Glucose(\u8840\u7cd6\u6d53\u5ea6)\u3001BloodPressure(\u8212\u5f20\u538b)\u3001BMI(\u8eab\u4f53\u8d28\u91cf\u6307\u6570)\u7b49\u7279\u5f81\u4e0d\u80fd\u542b\u6709\u96f6\u503c\n# \u5bf9\u96f6\u503c\u8f83\u5c11\u7684\u7279\u5f81\u7528\u5e73\u5747\u503c\u586b\u5145\uff0c\u5bf9\u542b\u6709\u8f83\u591a\u7684\u96f6\u503c\u7279\u5f81\u4f7f\u7528\u968f\u673a\u68ee\u6797\u586b\u5145\ndata.loc[data['Glucose']==0, 'Glucose'] = int(data['Glucose'].mean())\ndata.loc[data['BMI']==0, 'BMI'] = float(data['BMI'].mean())\ndata.loc[data['BloodPressure']==0, 'BloodPressure'] = int(data['BloodPressure'].mean())","1219aa87":"# \u83b7\u53d6\u5f85\u586b\u5145\u7684\u6570\u636e\u96c6\nfill_SkinThickness_data = data.loc[data.SkinThickness==0, :]\nfill_SkinThickness_data = fill_SkinThickness_data.drop(['SkinThickness', 'Insulin'], axis=1)\nfill_Insulin_data = data.loc[data.Insulin==0, :]\nfill_Insulin_data = fill_Insulin_data.drop(['SkinThickness', 'Insulin'], axis=1)","e79463eb":"# \u4f7f\u7528\u968f\u673a\u68ee\u6797\u586b\u5145SkinThickness(\u76ae\u80a4\u539a\u5ea6)\u548cInsulin(\u8840\u6e05\u80f0\u5c9b\u7d20)\u6765\u586b\u5145\u96f6\u503c\nfeatures_with_missing_values = ['SkinThickness', 'Insulin']\nfor feature in features_with_missing_values:\n    # \u5c06\u96f6\u503c\u8f6c\u6362\u6210np.nan\n    data[feature] = data[feature].apply(label_missing_values)","c5684fba":"new_data = data.dropna()","9c23152b":"# SkinThickness\u7684\u968f\u673a\u68ee\u6797\ndata.loc[data.loc[:, 'SkinThickness'].isna(),'SkinThickness'] = RanForest(new_data, fill_SkinThickness_data, 'SkinThickness', 'Insulin')\n# Insulin\u7684\u968f\u673a\u68ee\u6797\ndata.loc[data.loc[:, 'Insulin'].isna(),'Insulin'] = RanForest(new_data, fill_Insulin_data, 'Insulin', 'SkinThickness')","ed423253":"fig, ax = plt.subplots(4,2, figsize=(16,16))\nsns.distplot(data.Age, bins = 20, ax=ax[0,0]) \nsns.distplot(data.Pregnancies, bins = 20, ax=ax[0,1]) \nsns.distplot(data.Glucose, bins = 20, ax=ax[1,0]) \nsns.distplot(data.BloodPressure, bins = 20, ax=ax[1,1]) \nsns.distplot(data.SkinThickness, bins = 20, ax=ax[2,0])\nsns.distplot(data.Insulin, bins = 20, ax=ax[2,1])\nsns.distplot(data.DiabetesPedigreeFunction, bins = 20, ax=ax[3,0]) \nsns.distplot(data.BMI, bins = 20, ax=ax[3,1]) \nplt.show()\nfor feature in list(data):\n    try:\n        cnt = data[feature].value_counts()[0]\n    except:\n        cnt = 0\n    print(f\"\u7279\u5f81{feature}\u4e3a\u96f6\u7684\u4e2a\u6570\u4e3a\uff1a\", cnt)","24ffb118":"summary(data)","e9ea54b2":"fig, ax = plt.subplots(figsize=(8,8))\nfig.suptitle('Analysis of correlation between outcome and other features',fontsize=20)\nax=sns.heatmap(data.corr()[[\"Outcome\"]].sort_values(\"Outcome\"),vmax=1,\n               vmin=-1, cmap=\"YlGnBu\", annot=True, ax=ax, annot_kws={\"size\":15})\nax.invert_yaxis()","7e9ca948":"NewGlucose = pd.Series([\"Normal\", \"Overweight\", \"Secret\"], dtype = \"category\")\ndata[\"NewGlucose\"] = NewGlucose\ndata.loc[data[\"Glucose\"] <= 99, \"NewGlucose\"] = NewGlucose[0]\ndata.loc[(data[\"Glucose\"] > 99) & (data[\"Glucose\"] <= 126), \"NewGlucose\"] = NewGlucose[1]\ndata.loc[data[\"Glucose\"] > 126 ,\"NewGlucose\"] = NewGlucose[2]\ncrosstab(data, ['NewGlucose'], 'Outcome')","40ab6ae7":"Q1, Q3 = data.Insulin.quantile(0.25), data.Insulin.quantile(0.75)\nIQR = Q3-Q1\nlower, upper = Q1 - 1.5*IQR, Q3 + 1.5*IQR\ndata.loc[data[\"Insulin\"] > upper,\"Insulin\"] = upper\ndef set_insulin(row):\n    return \"Normal\" if (row[\"Insulin\"] >= 16 and row[\"Insulin\"] <= 166) else \"Abnormal\"\ndata = data.assign(NewInsulin=data.apply(set_insulin, axis=1))\ncrosstab(data, ['NewInsulin'], 'Outcome')","8e57fd8c":"NewBMI = pd.Series([\"Normal\", \"Overweight\", \"Obesity 1\", \"Obesity 2\", \"Obesity 3\"], dtype = \"category\")\ndata[\"NewBMI\"] = NewBMI\ndata.loc[data[\"BMI\"] <= 24.9, \"NewBMI\"] = NewBMI[0]\ndata.loc[(data[\"BMI\"] > 24.9) & (data[\"BMI\"] <= 29.9), \"NewBMI\"] = NewBMI[1]\ndata.loc[(data[\"BMI\"] > 29.9) & (data[\"BMI\"] <= 34.9), \"NewBMI\"] = NewBMI[2]\ndata.loc[(data[\"BMI\"] > 34.9) & (data[\"BMI\"] <= 39.9), \"NewBMI\"] = NewBMI[3]\ndata.loc[data[\"BMI\"] > 39.9 ,\"NewBMI\"] = NewBMI[4]\ncrosstab(data, ['NewBMI'], 'Outcome')","e1c7646c":"df = pd.get_dummies(data, columns =[\"NewBMI\",\"NewInsulin\", \"NewGlucose\"], drop_first = True)\ndf.head()","60a18231":"categorical_df = df[['NewBMI_Obesity 1','NewBMI_Obesity 2', 'NewBMI_Obesity 3', 'NewBMI_Overweight',\n                     'NewInsulin_Normal', 'NewGlucose_Overweight', 'NewGlucose_Secret']]\n\ncategorical_df.head()","2a82c4de":"y = df[\"Outcome\"]\nX = data.drop([\"Outcome\", \"NewBMI\",\"NewInsulin\", \"NewGlucose\"], axis = 1)\ncols = X.columns\nindex = X.index\nX.head()","5341cdf4":"from sklearn.preprocessing import RobustScaler\ntransformer = RobustScaler().fit(X)\nX = transformer.transform(X)\nX = pd.DataFrame(X, columns = cols, index = index)\nX.head()","6a250c27":"X = pd.concat([X,categorical_df], axis = 1)\nX.head()","cdc1d682":"data = pd.concat([X,y], axis = 1)\ndata.head()","04c268f0":"summary(data)","f2b61d1a":"fig, ax = plt.subplots(figsize=(8,8))\nfig.suptitle('Analysis of correlation between outcome and other features',fontsize=20)\nax=sns.heatmap(data.corr()[[\"Outcome\"]].sort_values(\"Outcome\"),vmax=1,\n               vmin=-1, cmap=\"YlGnBu\", annot=True, ax=ax, annot_kws={\"size\":15})\nax.invert_yaxis()","617b693d":"# \u5220\u9664NewBMI_Obesity 2\u7279\u5f81\ndata.drop(['NewBMI_Obesity 2'], axis=1, inplace=True)","1e5f3c62":"summary(data)","da7952ab":"import pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error, r2_score, roc_auc_score, roc_curve, classification_report\nplt.rcParams['font.sans-serif']=['SimHei']\nplt.rcParams['axes.unicode_minus']=False\npd.set_option('display.float_format', '{:.3f}'.format)\n%matplotlib inline\n\n# \u591a\u5c42\u611f\u77e5\u5668\uff08Multi-layer Perceptron\uff09\n# \u5b83\u53ef\u4ee5\u7528\u4e8e\u5206\u7c7b\u6216\u56de\u5f52\u7684\u975e\u7ebf\u6027\u51fd\u6570\u3002\u7528\u4e8e\u5206\u7c7b\u7684\u6a21\u5757\u662f MLPClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom seaborn import heatmap\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport copy","a77f17f2":"def MLP(data, layers):\n    x = data[list(data)[:-1]]\n    y = data[list(data)[-1]]\n    x_train, x_test, y_train, y_test = train_test_split(x, y)\n    accuracy = []\n    acc = 0\n    for layer_size in layers:\n        clf = MLPClassifier(solver='adam', alpha=1e-5,hidden_layer_sizes=(layer_size, layer_size), \n                            random_state=1,max_iter=10000)\n        clf.fit(x, y)\n        MLPClassifier(activation='relu')\n        score = clf.score(x_test, y_test)\n        accuracy.append(score)\n        if score > acc:\n            model = copy.deepcopy(clf)\n            size = layer_size\n            acc = score\n    print(f\"layer_sizes=({size}, {size})\u65f6\uff0cscore\u6700\u9ad8\uff1a{acc}\")\n    print(\"\u5728\u8bad\u7ec3\u96c6\u4e0a\u51c6\u786e\u7387\uff1a\",model.score(x_train,y_train)) #\u6a21\u578b\u8bc4\u5206\n    print(\"\u5728\u6d4b\u8bd5\u96c6\u4e0a\u51c6\u786e\u7387\uff1a\",model.score(x_test,y_test)) #\u6a21\u578b\u8bc4\u5206\n    y_predict = model.predict(x_test)\n    print(classification_report(y_test, y_predict, labels=[1, 0], \n                                target_names=['\u60a3\u75c5', '\u672a\u60a3\u75c5']))\n    print(\"\u6df7\u6dc6\u77e9\u9635\uff1a\\n\", confusion_matrix(y_test, y_predict))\n    heatmap(np.array(confusion_matrix(y_test, y_predict)), annot=True)\n    return accuracy","b91f7ba7":"layers = [i for i in range(1, 20)]\naccuracy = MLP(data, layers)","a811de4c":"plt.figure(figsize=(8, 5))\nplt.plot(np.array(layers), np.array(accuracy))\nplt.xlim(1, 20)\nplt.xlabel(\"hidden_layer_sizes\")\nplt.ylabel(\"Accuracy of test set\")\nplt.grid(True)\nprint(\"Maximum accuracy: \", max(accuracy))\nplt.show()","9bc0d9da":"##  Data preprocessing\n","dadbbca9":"## CONTEXT\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\nThis dataset consists several variables :\n\nPregnancies: Number of times pregnant\n\nGlucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n\nBloodPressure: Diastolic blood pressure (mm Hg)\n\nSkinThickness: Triceps skin fold thickness (mm)\n\nInsulin: 2-Hour serum insulin (mu U\/ml)\n\nBMI: Body mass index (weight in kg\/(height in m)^2)\n\nDiabetesPedigreeFunction: Diabetes pedigree function\n\nAge: Age (years)\n\nOutcome: Class variable (0 or 1)\n","8c0959cb":"## Load data","4a678d0b":"##  Algorithm and model","c1fbd5f9":"## Import package\n## Import function\n","5ede3f31":"## Feature Engineering\n\n> \u5de5\u4e1a\u754c\u6d41\u4f20\u8005\u8fd9\u4e48\u4e00\u53e5\u8bdd\uff1a\u6570\u636e\u548c\u7279\u5f81\u51b3\u5b9a\u4e86\u673a\u5668\u5b66\u4e60\u7684\u4e0a\u9650\uff0c\u800c\u6a21\u578b\u548c\u7b97\u6cd5\u53ea\u662f\u903c\u8fd1\u8fd9\u4e2a\u4e0a\u9650\u800c\u5df2\u3002\n\n> A popular saying in industry: data and features determine the upper limit of machine learning, and models and algorithms only approach this upper limit.","beb9435f":"> Because NewBMI_Obesity 2 has a low correlation with outcome, it can be removed\n\n> \u56e0\u4e3aNewBMI_Obesity 2\u4e0eOutcome\u7684\u76f8\u5173\u6027\u8f83\u4f4e\uff0c\u6240\u4ee5\u53ef\u4ee5\u5c06\u5b83\u5220\u9664","28cbfea0":"## \u5bf9\u6bd4\u672a\u5904\u7406\u7684\u6570\u636e\u548c\u7528\u968f\u673a\u68ee\u6797\u586b\u5145\u7684\u6570\u636e\uff08\u53ea\u6bd4\u8f83SkinThickness\u548cInsulin\u8fd9\u4e24\u4e2a\u7279\u5f81\uff09\n\u8fd9\u662f\u672a\u586b\u5145\u7684\u6570\u636e\u5206\u5e03\u56fe\n![%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202020-12-21%20112832.png](attachment:%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202020-12-21%20112832.png)\n\u8fd9\u662f\u7528\u968f\u673a\u68ee\u6797\u586b\u5145\u7684\u6570\u636e\u5206\u5e03\u56fe\n![%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202020-12-21%20112915.png](attachment:%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202020-12-21%20112915.png)\n"}}