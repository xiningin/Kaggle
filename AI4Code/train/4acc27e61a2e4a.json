{"cell_type":{"9b78c83d":"code","6341ae02":"code","c1aa04fc":"code","451bc0c2":"code","d05e6db0":"code","94cdff47":"code","5dbe5ffd":"code","3068829f":"code","34b35fd2":"code","d79ef82e":"code","5b1e0391":"markdown","3728af69":"markdown","bfd3c47d":"markdown","4085ea04":"markdown","f8ee381e":"markdown","545a5d85":"markdown","860f227e":"markdown","89f5aaed":"markdown","473afeb2":"markdown"},"source":{"9b78c83d":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score","6341ae02":"df = pd.read_csv('\/kaggle\/input\/heart-disease\/heart.csv')","c1aa04fc":"df.head()","451bc0c2":"df.info()","d05e6db0":"sns.heatmap(df.corr())","94cdff47":"X = df.loc[:, df.columns != 'target']\ny = df.loc[:, 'target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n\nmodel1 = RandomForestRegressor()\nbags = 10\nseed = 1\nbagged_prediction = np.zeros(X_val.shape[0])\nfor n in range(0, bags):\n    model1.set_params(random_state = seed + n) \n    model1.fit(X_train, y_train)\n    preds1 = model1.predict(X_val)\n    bagged_prediction += preds1\nbagged_prediction \/= bags\ntest_preds1 = model1.predict(X_test)","5dbe5ffd":"df['log_cp'] = np.log(df['cp'] + 1)\ndf['log_ca'] = np.log(df['ca'] + 1)\ndf['log_oldpeak'] = np.log(df['oldpeak'] + 1)\ndf['log_trestbps'] = np.log(df['trestbps'] + 1)\ndf['log_thalach'] = np.log(df['thalach'] + 1)\ndf.drop('cp', axis=1, inplace=True)\ndf.drop('ca', axis=1, inplace=True)\ndf.drop('oldpeak', axis=1, inplace=True)\ndf.drop('trestbps', axis=1, inplace=True)\ndf.drop('thalach', axis=1, inplace=True)","3068829f":"X = df.loc[:, df.columns != 'target']\ny = df.loc[:, 'target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n\nmodel2 = LinearRegression().fit(X_train, y_train)\npreds2 = model2.predict(X_val)\ntest_preds2 = model2.predict(X_test)\n\nstacked_predictions = np.column_stack((preds1, preds2))\nstacked_test_predictions = np.column_stack((test_preds1, test_preds2))\nmeta_model = LinearRegression()\nmeta_model.fit(stacked_predictions, y_val)\nfinal_predictions = meta_model.predict(stacked_test_predictions)","34b35fd2":"final_predictions = (final_predictions >= 0.5).astype(int)\nfinal_predictions","d79ef82e":"f1_score(final_predictions, y_test)","5b1e0391":"Our second model uses Linear Regression with the scaled values. Then we stack our predictions from the two models and take their average to create a meta model which we will use to generate our final predictions","3728af69":"# Exploratory Data Analysis","bfd3c47d":"We can see using .head() that all features are numeric","4085ea04":"Now we take replace some features that showed skewness in the pairplot above with its natural log","f8ee381e":"Before we apply scaling to some of the numeric features, we create a RandomForest model using the bagging technique. We split the data we have into 80% train and 20% test, and further split train into 80% train and 20% validation.","545a5d85":"Using .info() we see that none of the columns contain NaN values","860f227e":"Since the final predictions are averaged, it will fall on a continuous scale. The target variables are either 0 or 1 so we apply a threshold of 0.5 to determine its value","89f5aaed":"# Feature Engineering and Model Building","473afeb2":"We can create a heatmap comparing the correlation bewteen all pairwise features since they are all numeric"}}