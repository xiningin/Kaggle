{"cell_type":{"af3fe354":"code","4a36431a":"code","91161d43":"code","508be3ad":"code","52ecf588":"code","cb29b5a9":"code","62b419d7":"code","f21da53d":"code","173c9f2b":"code","265bb494":"code","01be8a26":"code","5288f83c":"code","5a71e3dd":"code","b9077381":"code","32b49ee3":"code","ad20d02d":"code","fb61beb3":"code","c3d22bc0":"code","948a58b2":"code","a1c3aa29":"code","bdf54a77":"code","2a69c98f":"code","1aefe2ab":"code","5b0cc65c":"code","82532cc7":"code","ef10aa75":"code","33b470f7":"markdown","12ae4294":"markdown","9e8af4fb":"markdown","e58e6833":"markdown","35c9c86d":"markdown","d5ba8dd8":"markdown","a8869726":"markdown","7cf4efa4":"markdown","4ee4dbe5":"markdown","dc30ae35":"markdown","0a41f8a9":"markdown","ae197112":"markdown","3b9b1587":"markdown","950074c4":"markdown"},"source":{"af3fe354":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns","4a36431a":"# Global meta data \nDATA_DIR = '..\/input\/lish-moa\/'\nWORK_DIR = '..\/working\/'","91161d43":"sample_sub = pd.read_csv(DATA_DIR + 'sample_submission.csv')\n# shape of submission \nprint(sample_sub.shape)\nsample_sub.head()","508be3ad":"train_features = pd.read_csv(DATA_DIR + 'train_features.csv')\n# shape of train_features\nprint(train_features.shape)\n# check head of data\ntrain_features.head()","52ecf588":"train_features.isnull().sum().sum()","cb29b5a9":"# check duplicates\ntrain_features['sig_id'].nunique()","62b419d7":"# categorical unique values \nprint('Unique values in cp_type: ', train_features['cp_type'].unique())\nprint('Unique values in cp_dose: ', train_features['cp_dose'].unique())\n# we can treat cp_time as categorical valriable from description\nprint('Unique values in cp_time: ', train_features['cp_time'].unique())","f21da53d":"cat_cols = ['cp_type', 'cp_dose', 'cp_time']\n\n# Count plot for all categorical variables\nplt.figure(figsize=(5, 20))\nfor i, col in enumerate(cat_cols):\n    plt.subplot(3, 1, i+1)\n    plt.title(col)\n    sns.countplot(train_features[col], palette='Blues')","173c9f2b":"# how many c and g columns?\ng_cols = [col for col in train_features.columns if 'g-' in col]\nc_cols = [col for col in train_features.columns if 'c-' in col]\n\n# print first 10 column names and total length of c- and g- columns \nprint(f'First 10 columns for g-: {g_cols[:10]}')\nprint(f'Length of columns with g-: {len(g_cols)}')\nprint(f'First 10 columns for c-: {c_cols[:10]}')\nprint(f'Length of columns with c-: {len(c_cols)}')","265bb494":"train_target = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\nprint(train_target.shape)\ntrain_target.head()","01be8a26":"list(sample_sub.columns) == list(train_target.columns)","5288f83c":"test_features = pd.read_csv(DATA_DIR + 'test_features.csv')\nprint(test_features.shape)\ntest_features.head()","5a71e3dd":"test_features.isnull().sum().sum()","b9077381":"test_features['sig_id'].nunique()","32b49ee3":"# Count plot for all categorical variables\nplt.figure(figsize=(5, 20))\nfor i, col in enumerate(cat_cols):\n    plt.subplot(3, 1, i+1)\n    plt.title(col)\n    sns.countplot(test_features[col], palette='Blues')","ad20d02d":"from collections import OrderedDict\nfrom pathlib import Path \nimport os\nimport sys\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, Dataset\n\nsys.path.append('..\/input\/iter-strat\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","fb61beb3":"# Config data\nEPOCHS = 30\nDROPOUT = 0.3\nSTEP_SIZE = 10\nFOLDS = 5\nHIDDEN_SIZE = 1024\nBATCH_SIZE = 1024\nLR = 3e-3\nSPLIT_COL = 'cp_type'","c3d22bc0":"# Data preprocessing\ntrain_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsub = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\ntargets = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\n\n# Label encode categorical variables \ncp_type = {'trt_cp': 0, 'ctl_vehicle': 1}\ncp_time = {24: 0, 48: 1, 72: 2}\ncp_dose = {'D1': 0, 'D2': 1}\n\nfor data in [train_features, test_features]:\n    data['cp_type'] = data['cp_type'].map(cp_type)\n    data['cp_time'] = data['cp_time'].map(cp_time)\n    data['cp_dose'] = data['cp_dose'].map(cp_dose)\n    \n# get columns for features and targets\nfeature_cols = list(train_features.drop('sig_id', axis=1).columns)\ntarget_cols = list(targets.drop('sig_id', axis=1).columns)\n\n# configure input and output size \nINPUT_SIZE = len(feature_cols)\nOUTPUT_SIZE = len(target_cols)\n\n# set fold number for data\ntrain_features.loc[:, 'fold'] = -1\n\nskf = MultilabelStratifiedKFold(n_splits=FOLDS)\nfor fold, (train_ind, val_ind) in enumerate(skf.split(train_features, targets.drop('sig_id', axis=1))):\n    train_features.loc[val_ind, 'fold'] = fold\n\n# merge train features and targets\ntrain_data = train_features.merge(targets, on='sig_id').reset_index()","948a58b2":"# define dataset for both train and test\nclass TrainData(Dataset):\n    def __init__(self, table_data, features, targets):\n        super().__init__()\n        self.features = table_data[features].values\n        self.targets = table_data[targets].values\n\n    def __len__(self):\n        return len(self.features)\n\n    def __getitem__(self, ind):\n        X = torch.Tensor(self.features[ind])\n        y = torch.Tensor(self.targets[ind])\n        return X, y\n\nclass TestData(Dataset):\n    def __init__(self, table_data):\n        super().__init__()\n        self.test = table_data.drop('sig_id', axis=1).values\n\n    def __len__(self):\n        return len(self.test)\n\n    def __getitem__(self, ind):\n        X = torch.Tensor(self.test[ind])\n        return X\n","a1c3aa29":"# Define model (NN)\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(INPUT_SIZE, HIDDEN_SIZE), \n            nn.BatchNorm1d(HIDDEN_SIZE),\n            nn.Dropout(DROPOUT), \n            nn.ReLU(), \n            nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE), \n            nn.BatchNorm1d(HIDDEN_SIZE), \n            nn.Dropout(DROPOUT), \n            nn.ReLU(), \n            nn.Linear(HIDDEN_SIZE, OUTPUT_SIZE)\n        )\n\n    def forward(self, inputs):\n        inputs = self.net(inputs)\n        return inputs","bdf54a77":"# set everything needed for training and validation\nclass Train:\n    def __init__(self, model, optim, device):\n        self.model = model \n        self.optim = optim \n        self.device = device \n    \n    @staticmethod\n    def loss_fn(output, target):\n        return nn.BCEWithLogitsLoss()(output, target)\n    \n    # train method \n    def train(self, train_loader):\n        # seet model to training mode\n        self.model.train()\n        single_loss = 0\n        \n        for x, y in train_loader:\n            # initialize optimizer \n            self.optim.zero_grad()\n            # move x and y to device \n            x, y = x.to(self.device), y.to(self.device)\n            # output from model \n            out = self.model(x)\n            # compute loss based on output and target values\n            loss = self.loss_fn(out, y)\n            # back propagation \n            loss.backward()\n            # update optimizer \n            self.optim.step()\n            # add loss to calculate loss \n            single_loss += loss.item()\n            \n        # return average loss \n        return single_loss \/ len(train_loader)\n\n    def validation(self, val_loader):\n        # set model to evaluation mode \n        self.model.eval()\n        single_loss = 0\n        \n        for x, y in val_loader:\n            x, y = x.to(self.device), y.to(self.device)\n            # without gradients \n            with torch.no_grad():\n                out = self.model(x)\n                loss = self.loss_fn(out, y)\n                single_loss += loss.item()\n                \n        return single_loss \/ len(val_loader)","2a69c98f":"def train_step(train_data, test_data, fold, features, targets, device):\n    ### Training and validation for one fold ###\n    # get train and val data \n    train = train_data[train_data['fold']!=fold].reset_index(drop=True)\n    val = train_data[train_data['fold']==fold].reset_index(drop=True)\n    \n    # convert into tensor \n    train_set = TrainData(train, features, targets)\n    val_set = TrainData(val, features, targets)\n    \n    # define data loader for both train and test data \n    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n    \n    # define model, optimizer and scheduler\n    model = Model()\n    model.to(device)\n    optim = torch.optim.Adam(model.parameters(), lr=LR)\n    # due to the epoch number, I did not define scheduler but if you want, try out\n    \n    # define Train class \n    trainer = Train(model, optim, device)\n    \n    # configs \n    best_weight = ''\n    best_loss = np.inf\n    train_losses = []\n    val_losses = []\n    \n    # epoch loop\n    for epoch in range(EPOCHS):\n        # compute train and val loss \n        train_loss = trainer.train(train_loader)\n        val_loss = trainer.validation(val_loader)\n\n        # append all values and use for computing mean \n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        \n        # update (or keep track on) model which results with better val loss\n        if val_loss < best_loss:\n            print(f'Updating best model weights in epoch {epoch+1}: {val_loss}')\n            # save best model weight (so far)\n            torch.save(model.state_dict(), f'{fold}_{epoch}.pth')\n            # track file for best weight (so far)\n            best_weight = f'{fold}_{epoch}.pth'\n            # update best loss \n            best_loss = val_loss\n    \n    print(f'FOLD #{fold+1}: Average train loss: {np.mean(np.array(train_losses))}, Average val loss: {np.mean(np.array(val_losses))}')\n    \n    # get test dataset and loader \n    test_set = TestData(test_data)\n    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n    \n    # define model for testing (prediction)\n    model = Model()\n    model.load_state_dict(torch.load(best_weight))\n    model.to(device)\n    \n    # compute prediction\n    preds = test_step(model, test_loader, device)\n\n    return preds","1aefe2ab":"def test_step(model, test_loader, device):\n    # evaluation mode \n    model.eval()\n    preds = []\n    \n    # prediction loop\n    for x in test_loader:\n        x = x.to(device)\n        \n        with torch.no_grad():\n            pred = model(x)\n        \n        # append predicted values with sigmoid (since we are predicting probability)\n        preds.append(pred.sigmoid().detach().cpu().numpy())\n    \n    # reshape preds list  \n    preds = np.concatenate(preds)\n    return preds\n\ndef k_fold_trainer(train_data, test_data, features, targets):\n    # set variables for final result \n    preds = np.zeros((len(test_data), OUTPUT_SIZE))\n    device = torch.device('cuda:0')\n    \n    # fold loop\n    for fold in range(FOLDS):\n        print(f'------------Fold #{fold+1}------------')\n        \n        # add predictions per folds \n        preds += train_step(train_data, test_data, fold, features, targets, device) \/ FOLDS\n\n    return preds","5b0cc65c":"# get predicted value \npred = k_fold_trainer(train_data, test_features, feature_cols, target_cols)","82532cc7":"# convert into submission format\nsub.iloc[:, 1:] = pred \nsub.to_csv('submission.csv', index=False)","ef10aa75":"sub.head()","33b470f7":"Thanksfully, distribution of train and test dataset are similar (almost identical). Thus, we will take in the distibution of train data as a consideration. ","12ae4294":"<div id='section'\/>\n\n## 2. Quick data look-in\n\n### Explanation of csv data\nSee [official explanation](https:\/\/www.kaggle.com\/c\/lish-moa\/data) since it's most detailed one.","9e8af4fb":"We now know that columns with \"g-\" is from g-0 to g-771, and c-0 to c-99 for \"c-\". From now on, we will look at train_targets_scored, which contains target variables. ","e58e6833":"Check train data","35c9c86d":"<div id='section1'\/>\n\n## 3. Baseline\n\nThis baseline is actually a 'baseline', so I did not implement fancy techniques or sorts. I will write down what to do with this baseline in the end of this notebook.","d5ba8dd8":"# <center>MoA - EDA and simple PyTorch Baseline -<\/center> \n\n![headache-pain-pills-medication-159211.jpeg](attachment:headache-pain-pills-medication-159211.jpeg)\n\n1. [Introduction](#section1)\n2. [EDA](#section2) \n3. [Baseline](#section3)","a8869726":"Check from submission file since it's a final form of prediction.","7cf4efa4":"Qualified that train_target and sample_sub columns are identical. This fact gives an intuition that **we just need to create model which takes variables (and potential features to be created) in train_features, and train based on train_target values**. As instruction says, target is binary representation, so semi-OHE. Now, we will take a look at test_features.csv. We will repeat the look over process that we used in train_features. ","4ee4dbe5":"## So far what we did and what can we do next.\nSo far we did: \n* Minimal data look-in \n* Minimal data preprocessing (no feature engineering or normalizations)\n* Minimal codes to run pytorch model \n\nWhat we can do for next: \n* Detailed EDA and feature engineering\n* Implement scheduler \/ callbacks and increase epochs \n* Hyperparameter tuning\n* Model modification (making deeper? wider? or vice versa for both?)","dc30ae35":"We can label encode these (OHE). Let's check the distribution of categorical values.","0a41f8a9":"Hooray! We don't have any missing\/duplicated values for now! What about the unique values in categorical values??","ae197112":"Seems cp_type has skewed distribution. Otherwise, we cannot observe significant features. **Maybe we can split data w.r.t cp_type, since multiple target variables exists.** \n\nI will laave detailed exploration for numerical data, since it is too much. Just touch with basic infos.","3b9b1587":"<div id='section1'\/>\n\n## 1. Introduction\n\n### 1.1 What's MoA (Mechanisms of Action) and how? \nAccording to [official description of this competition](https:\/\/www.kaggle.com\/c\/lish-moa\/overview\/description), MoA is a numerical measurement for biological activity of chemical molecules (i.e. drug) which targets to modulate disease they are focusing. For how part, they give the test drug to the test human cell and record cellular response, which can be derived by algorithm that retrieves similarities from know patterns in geometric databases.\n\n### 1.2 Goal of this competition\nWe are going to construct an algorithm that automaticaly labels each case in test set based on train set. Note that this competition does not specify the target varible to one, but also can be classified into multiple cases. \n\n### 1.3 Evaluation metric (score)\nScore is derived from modified version of log loss: \n\n$$score = -\\frac{1}{M}\\sum\\limits_{m=1}{M}\\frac{1}{N}\\sum\\limits_{i=1}{N}[y_{i, m}log(\\hat{y}_{i,m})+(1-y_{i,m})(1-\\hat{y}_{i,m})]$$\n\nWhere: \n* $N$ is the number of sig_id observations in the test data (i=1,\u2026,N)\n* $M$ is the number of scored MoA targets (m=1,\u2026,M)\n* $\\hat{y}_{i,m}$ is the predicted probability of a positive MoA response for a sig_id\n* $y_{i,m}$ is the ground truth, 1 for a positive response, 0 otherwise\n* $log()$ is the natural (base e) logarithm","950074c4":"train_target and sample submission has identical column numbers. Check are columns same. "}}