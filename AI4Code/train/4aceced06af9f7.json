{"cell_type":{"112af168":"code","fb69866c":"code","a1e16e2f":"code","60edebf2":"code","ce7e16f3":"code","d0c4b348":"code","d6962d25":"code","fb6eab5a":"code","da804e2b":"code","f2383b5a":"code","44ebade4":"code","08f3e674":"code","b79e7fe6":"code","9bb58a99":"code","30c651ad":"markdown","c44ed84e":"markdown","6411bfe1":"markdown","3c7e2a1d":"markdown","ea6a9212":"markdown","b81ea421":"markdown","2d89a87a":"markdown","8a9f89cc":"markdown","51c521cd":"markdown","f4070e23":"markdown","bd152a77":"markdown"},"source":{"112af168":"#import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nmatplotlib.rcParams[\"figure.figsize\"] = (12,10)\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import mean_squared_error\n\n#Model library\nfrom sklearn.model_selection import train_test_split,cross_val_score,KFold\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\n","fb69866c":"train = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\")\nsample= pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")\ntrain.shape,test.shape","a1e16e2f":"train.isnull().sum()","60edebf2":"train.describe(include='all')","ce7e16f3":"#insert the kfold columns\ntrain['kfold'] = -1\n#distributing the data\nkfold = KFold(n_splits = 5,shuffle=True,random_state = 42)\nfor fold, (tr_i,va_i) in enumerate(kfold.split(X=train)):\n    train.loc[va_i,'kfold'] = fold\n    \nprint(train.kfold.value_counts())\ntrain.to_csv(\"folds_5.csv\",index=False)\nprint(\"successfully folds\")\n","d0c4b348":"train=train.dropna(axis=0)\ntrain.isnull().sum()","d6962d25":"train.describe(include='all')","fb6eab5a":"#Folds data\ntrain = pd.read_csv(\".\/folds_5.csv\")\n\n#features taken to train\nfeatures = [f for f in train.columns if f not in(\"id\",\"kfold\",\"claim\")]\nnum_cols = [cols for cols in features if 'f' in cols]\n\ntest= test[features]\nprediction = []\nscore = []\n\nfor fold in range (5):\n    xtrain = train[train.kfold != fold].reset_index(drop=True)\n    xvalid = train[train.kfold == fold].reset_index(drop=True)\n    xtest = test.copy()\n    \n    ytrain = xtrain.claim\n    yvalid = xvalid.claim\n    \n    xtrain = xtrain[features]\n    xvalid = xvalid[features]\n    \n    lE = StandardScaler()\n    xtrain[num_cols] = lE.fit_transform(xtrain[num_cols])\n    xvalid[num_cols] = lE.transform(xvalid[num_cols])\n    xtest[num_cols] = lE.transform(xtest[num_cols])\n    \n    \n    \n    #xgboost model\n    xgb_m = XGBRegressor(learning_rate=0.021537077920105466,\n                         n_estimators=10606,\n                         random_state=228,\n                         gpu_id=0,\n                         tree_method='gpu_hist',\n                         predictor='gpu_predictor'\n                        )\n    xgb_m.fit(xtrain,ytrain,early_stopping_rounds=100,eval_set=[(xvalid,yvalid)],verbose=1000)\n    predict_valid = xgb_m.predict(xvalid)\n    test_predict = xgb_m.predict(xtest)\n    prediction.append(test_predict)\n    \n    #Root_mean_square\n    rms = mean_squared_error(yvalid,predict_valid,squared=False)\n    \n    #Score \n    score.append(rms)\n    print(f\"fold|split:{fold},rmse:{rms}\")\n    \nprint(np.mean(score),np.std(score))\n\n\n","da804e2b":"#reconfigure of split data\nfinal_predict = np.mean(np.column_stack(prediction),axis=1)\nprint(final_predict)\nsample.claim = final_predict\nsample.to_csv(\"First_submission_xgb.csv\",index=False)\nprint(\"Final achieve to send xgboost output data\")","f2383b5a":"#Folds data\ntrain = pd.read_csv(\".\/folds_5.csv\")\n\n#features taken to train\nfeatures = [f for f in train.columns if f not in(\"id\",\"kfold\",\"claim\")]\nnum_cols = [cols for cols in features if 'f' in cols]\n\ntest= test[features]\n\nprediction0 = []\nscore = []\n\nfor fold in range (5):\n    xtrain = train[train.kfold != fold].reset_index(drop=True)\n    xvalid = train[train.kfold == fold].reset_index(drop=True)\n    xtest = test.copy()\n    \n    ytrain = xtrain.claim\n    yvalid = xvalid.claim\n    \n    xtrain = xtrain[features]\n    xvalid = xvalid[features]\n    \n    lE = StandardScaler()\n    xtrain[num_cols] = lE.fit_transform(xtrain[num_cols])\n    xvalid[num_cols] = lE.transform(xvalid[num_cols])\n    xtest[num_cols] = lE.transform(xtest[num_cols])\n    \n    #catboost model\n    catpara={\n        'subsample': 0.95312,\n        'learning_rate': 0.014589235,\n        \"max_depth\": 6,\n        \"min_data_in_leaf\":77,\n        'random_state':228,\n        'n_estimators':10000,\n        'rsm':0.5,\n        'l2_leaf_reg': 0.02247766515106271\n    }\n    \n    model=CatBoostRegressor(**catpara)\n    model.fit(xtrain,ytrain,early_stopping_rounds=100,eval_set=[(xvalid,yvalid)],verbose=1000)\n    \n    predict_valid = model.predict(xvalid)\n    test_predict1 = model.predict(xtest)\n    prediction0.append(test_predict1)\n    \n    #Root_mean_square\n    rms = mean_squared_error(yvalid,predict_valid,squared=False)\n    \n    #Score \n    score.append(rms)\n    print(f\"fold|split:{fold},rmse:{rms}\")\n    \nprint(np.mean(score),np.std(score))","44ebade4":"#reconfigure of split data\npred0 = np.mean(np.column_stack(prediction0),axis=1)\nprint(pred0)\nsample.claim = pred0\nsample.to_csv(\"mysubmission_cat.csv\",index=False)\nprint(\"Finally send the output data\")","08f3e674":"#Folds data\ntrain = pd.read_csv(\".\/folds_5.csv\")\n\n#features taken to train\nfeatures = [f for f in train.columns if f not in(\"id\",\"kfold\",\"claim\")]\nnum_cols = [cols for cols in features if 'f' in cols]\n\ntest= test[features]\n\nprediction1 = []\nscore = []\n\nfor fold in range (5):\n    xtrain = train[train.kfold != fold].reset_index(drop=True)\n    xvalid = train[train.kfold == fold].reset_index(drop=True)\n    xtest = test.copy()\n    \n    ytrain = xtrain.claim\n    yvalid = xvalid.claim\n    \n    xtrain = xtrain[features]\n    xvalid = xvalid[features]\n    \n    lE = StandardScaler()\n    xtrain[num_cols] = lE.fit_transform(xtrain[num_cols])\n    xvalid[num_cols] = lE.transform(xvalid[num_cols])\n    xtest[num_cols] = lE.transform(xtest[num_cols])\n    \n    #lgb parameters\n    params_lgb = {\n    \"task\": \"train\",\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"regression\",\n    'subsample': 0.95312,\n    \"metric\": \"rmse\",\n    'learning_rate': 0.01635,\n    \"max_depth\": 3,\n    \"feature_fraction\": 0.2256038826485174,\n    \"bagging_fraction\": 0.7705303688019942,\n    \"min_child_samples\": 290,\n    \"reg_alpha\": 14.68267919457715,\n    \"reg_lambda\": 66.156,\n    \"max_bin\": 772,\n    \"min_data_per_group\": 177,\n    \"bagging_freq\": 1,\n    \"cat_smooth\": 96,\n    \"cat_l2\": 17,\n    \"verbosity\": -1,\n    'random_state':288,\n    'n_estimators':11990,\n    'colsample_bytree':0.1107,\n    'njobs':4\n    }\n    \n    lgb_train = lgb.Dataset(xtrain, ytrain)\n    lgb_val = lgb.Dataset(xvalid, yvalid)\n    \n    model = lgb.train(params=params_lgb,\n                      train_set=lgb_train,\n                      valid_sets=lgb_val,\n                      early_stopping_rounds=300,\n                      verbose_eval=1000)\n    \n   \n    preds_valid = model.predict(xvalid,num_iteration=model.best_iteration)\n    test_predict = model.predict(xtest,num_iteration=model.best_iteration)\n    \n    prediction1.append(test_predict)\n    \n    #Root_mean_square\n    rms = mean_squared_error(yvalid,preds_valid,squared=False)\n    \n    #Score \n    score.append(rms)\n    print(f\"fold|split:{fold},rmse:{rms}\")\n    \nprint(np.mean(score),np.std(score))","b79e7fe6":"#reconfigure of split data\npred1 = np.mean(np.column_stack(prediction1),axis=1)\nprint(pred1)\nsample.claim = pred1\nsample.to_csv(\"mysubmission_lgb.csv\",index=False)\nprint(\"Finally send the output data\")","9bb58a99":"print(\"Finally achieve to send  output data\")","30c651ad":"# CATBOOSTREGRESSOR \u2728 ","c44ed84e":"## Submission File\n","6411bfe1":"# Tabular playground Series 2021 \u2728 \n\n#### **Task: Regression problem** \ud83d\ude0a \n\nPreviously try : [https:\/\/www.kaggle.com\/venkatkumar001\/tps-sep-21-day2-eda-xgb](http:\/\/)\n\n**Version 4 Newly try voting method of regression problem**\n\n#### **Process** \u270c\ufe0f \n\n1. Read the data\n2. Identify the null data and apply simple imputer(simple imputer identify the missing data and fill mean values)\n3. Preprocessing the train and test data\n4. Apply Standardscaler and xgb,catboost,Lgbm\n5. Submitting the output files","3c7e2a1d":"#  Identify the null data and drop","ea6a9212":"# Read the data","b81ea421":"# XGBOOSTREGRESSOR","2d89a87a":"# LIGHTGBM \u2728 ","8a9f89cc":"# KFold","51c521cd":"# Import Necessary Library \u2728 \u270c\ufe0f ","f4070e23":"# Preprocessing the data (StandardScaler)","bd152a77":"## **Thankyou** \u2728 \u270c\ufe0f \ud83d\udc4d \ud83d\ude0a "}}