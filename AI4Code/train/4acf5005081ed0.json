{"cell_type":{"293aab65":"code","ff2fd7cf":"code","a7d137a0":"code","cd2f97eb":"code","d04cda34":"code","f07de382":"code","65ba14bd":"code","c0d5d00f":"code","52db4df1":"code","ca57485e":"code","e93e0df5":"markdown","5c59fdcf":"markdown","8c2cfe3f":"markdown","0a29ad5b":"markdown","60f88efd":"markdown","14701f33":"markdown"},"source":{"293aab65":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ff2fd7cf":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas_profiling import ProfileReport\n%matplotlib inline","a7d137a0":"df = pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\ndf.head()","cd2f97eb":"report = ProfileReport(df)","d04cda34":"report","f07de382":"X = df.drop('quality' , 1).values # drop target variable\ny1 = df['quality'].values\ny = y1 <= 5","65ba14bd":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC","c0d5d00f":"from sklearn.preprocessing import scale \n\nX_scaled = scale(X)\n\nprint(\"Mean of Unscaled Features: {}\".format(np.mean(X))) \nprint(\"Standard Deviation of Unscaled Features: {}\".format(np.std(X)))\nprint(\"Mean of Scaled Features: {}\".format(np.mean(X_scaled))) \nprint(\"Standard Deviation of Scaled Features: {}\".format(np.std(X_scaled)))","52db4df1":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nsteps = [('scaler', StandardScaler()),\n        ('knn', KNeighborsClassifier())]\n        \npipeline = Pipeline(steps)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nknn_scaled = pipeline.fit(X_train, y_train)\nknn_unscaled = KNeighborsClassifier().fit(X_train, y_train)\n\nprint('Accuracy with Scaling: {}'.format(knn_scaled.score(X_test, y_test)))\nprint('Accuracy without Scaling: {}'.format(knn_unscaled.score(X_test, y_test)))","ca57485e":"steps = [('scaler', StandardScaler()),\n         ('SVM', SVC())]\n\npipeline = Pipeline(steps)\n\nparameters = {'SVM__C':[1, 10, 100],\n              'SVM__gamma':[0.1, 0.01]}\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)\n\ncv = GridSearchCV(pipeline, parameters, cv=3)\n\ncv.fit(X_train, y_train)\n\ny_pred = cv.predict(X_test)\n\nprint(\"Accuracy: {}\".format(cv.score(X_test, y_test)))\nprint(classification_report(y_test, y_pred))\nprint(\"Tuned Model Parameters: {}\".format(cv.best_params_))","e93e0df5":"# Centering and scaling the data","5c59fdcf":"# Bringing it all together: Pipeline for classification","8c2cfe3f":"# Centering and scaling in a pipeline","0a29ad5b":"1. Import the following modules:\n2. StandardScaler from sklearn.preprocessing.\n3. Pipeline from sklearn.pipeline.\n4. Complete the steps of the pipeline with StandardScaler() for 'scaler' and KNeighborsClassifier() for 'knn'.\n5. Create the pipeline using Pipeline() and steps.\n6. Create training and test sets, with 30% used for testing. Use a random state of 42.\n7. Fit the pipeline to the training set.\n8. Compute the accuracy scores of the scaled and unscaled models by using the .score() method inside the provided print() functions.","60f88efd":"1. Import scale from sklearn.preprocessing.\n2. Scale the features X using scale().\n3. Print the mean and standard deviation of the unscaled features X, and then the scaled features X_scaled. Use the numpy functions np.mean() and np.std() to compute the mean and standard deviations.\n4. Compute the classification report.","14701f33":"1. Setup the pipeline with the following steps:\n2. Scaling, called 'scaler' with StandardScaler().\n3. Classification, called 'SVM' with SVC().\n4. Specify the hyperparameter space using the following notation: 'step_name__parameter_name'. Here, the step_name is SVM, and the parameter_names are C and gamma.\n5. Create training and test sets, with 20% of the data used for the test set. Use a random state of 21.\n6. Instantiate GridSearchCV with the pipeline and hyperparameter space and fit it to the training set. Use 3-fold cross-validation (This is the default, so you don't have to specify it).\n7. Predict the labels of the test set and compute the metrics. The metrics have been computed for you."}}