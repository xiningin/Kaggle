{"cell_type":{"d336f627":"code","c4000ec3":"code","47649ba5":"code","bc0143bd":"code","e41eb694":"code","3ad7b9cd":"code","7dea9e4e":"code","7ee8e510":"code","2163cc20":"code","9bd7ea47":"code","7e3c1de2":"code","fea36cf3":"markdown","7641f67a":"markdown","9962af93":"markdown","b0a6ed7a":"markdown","f289460a":"markdown","3f841ab0":"markdown","ed0979c4":"markdown","c8d7bdfb":"markdown","35c2cfc3":"markdown","7512bdaa":"markdown","eb3fef6a":"markdown"},"source":{"d336f627":"from IPython.display import Image\nImage(\"\/kaggle\/input\/project-figures\/figure1.PNG\")","c4000ec3":"Image(\"\/kaggle\/input\/project-figures\/figure2.PNG\")","47649ba5":"Image(\"\/kaggle\/input\/project-figures\/figure3.PNG\")","bc0143bd":"Image(\"\/kaggle\/input\/project-figures\/table1.PNG\")","e41eb694":"Image(\"\/kaggle\/input\/project-figures\/table2.PNG\")","3ad7b9cd":"Image(\"\/kaggle\/input\/project-figures\/figure6.PNG\")","7dea9e4e":"Image(\"\/kaggle\/input\/project-figures\/figure7.PNG\")","7ee8e510":"Image(\"\/kaggle\/input\/project-figures\/figure8.PNG\")","2163cc20":"Image(\"\/kaggle\/input\/project-figures\/figure9.PNG\")","9bd7ea47":"#This script is to read the excel files and create a ndarray of length n video and each cell in this array contains\n# multiple feature vectors and each of them represents one day while trending\n\n#Features: how many days the video was trending (including same day recording), title sentiment (polarity), title sentiment (subjectivity)\n# ,title average tf-idf, tags sentiment (polarity), tags sentiment (subjectivity), tags average tf-idf,\n#  description sentiment (polarity), description sentiment (subjectivity), description average tf-idf, category_id,\n# The time since the video was uploaded, views, likes, dislikes, comments count, like rate, dislike rate, comment rate, change in views since the trending day\n#, change in likes since the trending day, change in dislikes since the trending day, change in comments count since the trending day\n#, comments disabled (0 or 1), ratings disabled (0 or 1).\n# 'title_tfidf', 'tags_tfidf' and 'description_tfidf' are checking if rare and significant words been used.\n# Rate features are calculated to find if the video is interesting in each day\n# change features are calculated to check if the number of likes, dislikes and comments are increasing or decreasing in each day.\n#TODO add more features\n\nimport pandas as pd\nimport numpy as np\nfrom textblob import TextBlob\nimport string,math,pickle\n\ndef remove_punctuation (doc):\n    return \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in doc]).split())\n\ndef tfidf_fun(word, blob, bloblist):\n    tf=blob.words.count(word) \/ len(blob.words)\n    n_containing=sum(1 for blob2 in bloblist if word in blob2.words)\n    idf=math.log(len(bloblist) \/ (1 + n_containing))\n    return tf * idf\n\ndef avg_tfidf_fun(blob, bloblist):\n    avg_tfidf=0\n    for wordI in blob.words:\n        avg_tfidf=avg_tfidf+tfidf_fun(wordI, blob, bloblist)\n    if blob.words.__len__() != 0:\n        avg_tfidf=avg_tfidf\/blob.words.__len__()\n        return avg_tfidf\n    else:\n        return 0\n\n################################## Feature Extraction ###############################################\nnumFeatures=25\ndf_yout = pd.read_csv(\"\/kaggle\/input\/youtube-new\/USvideos.csv\")\n\n#create the features and labels dataframe\ndf_features = pd.DataFrame(np.zeros([df_yout.shape[0],numFeatures]),columns=['days_was_trending', 'title_polarity', 'title_subjectivity','title_tfidf' ,\n                                                                             'tags_polarity', 'tags_subjectivity' , 'tags_tfidf','description_polarity',\n                                                                             'description_subjectivity', 'description_tfidf',\n                                                                             'category_id','days_since_uploaded','views',\n                                                                             'likes','dislikes','comment_count','like_rate','dislike_rate',\n                                                                             'comment_rate','view_change','like_change','dislike_change','comment_change',\n                                                                             'comments_disabled','ratings_disabled'])\n                                    # 'title_tfidf', 'tags_tfidf' and 'description_tfidf' take too much time to be calculated. Checking if rare and significant words been used.\n\ndf_label = pd.DataFrame(np.zeros([df_yout.shape[0],2]),columns=['video_ind','days_will_be_trending'])\n\n#construct bloblist by combining all the titles, tags and descriptions into multiple documents\nunique_ids=df_yout['video_id'].unique()\nbloblist=[]\nfor docI in range(unique_ids.shape[0]):\n    vedI_data = df_yout[df_yout['video_id'] == unique_ids[docI]]\n    vedI_data=vedI_data.reset_index(drop=True)\n    document = vedI_data.title[0]+ \" \" +vedI_data.tags[0]+\" \" +str(vedI_data.description[0])\n    bloblist.append(TextBlob(remove_punctuation(document)))\n\n#Calculating the features for each vedio for each day\ncount1=0\nfor vidI in range(100):#range(unique_ids.shape[0]): #TODO remove the comment to use all the videos and not just 10\n    print('Extracting the features of video #:' + str(vidI))\n    vedI_data=df_yout[df_yout['video_id']==unique_ids[vidI]]\n    vedI_data=vedI_data.reset_index(drop=True)\n    total_num_trending=vedI_data.shape[0]\n\n    titleI = TextBlob(remove_punctuation(vedI_data.title[0])) #removing the punctuation results in changing sentiment results slightly\n    tagsI = TextBlob(remove_punctuation(vedI_data.tags[0]))\n    descriptionI = TextBlob(remove_punctuation(str(vedI_data.description[0])))\n    title_tfidf=avg_tfidf_fun(titleI,bloblist)\n    tags_tfidf = avg_tfidf_fun(tagsI, bloblist)\n    description_tfidf = avg_tfidf_fun(descriptionI, bloblist)\n\n    trending_date=pd.to_datetime(vedI_data.trending_date[0], format='%y.%d.%m')\n    published_date=pd.to_datetime(vedI_data.publish_time[0], format='%Y-%m-%dT%H:%M:%S.%fZ')\n    date_published_trending=trending_date -published_date\n    days_published_trending=float(date_published_trending._d)+(float(date_published_trending._h)\/24)\n\n    for dayI in range(total_num_trending):\n        #print('day number :' + str(dayI))\n        df_label.video_ind[count1] = vidI\n        df_label.days_will_be_trending[count1]=total_num_trending-dayI-1\n\n        df_features.days_was_trending[count1]=dayI+1\n\n        df_features.title_polarity[count1] =titleI.sentiment.polarity\n        df_features.title_subjectivity[count1] = titleI.sentiment.subjectivity\n        df_features.title_tfidf[count1]=title_tfidf\n\n        df_features.tags_polarity[count1] =tagsI.sentiment.polarity\n        df_features.tags_subjectivity[count1] = tagsI.sentiment.subjectivity\n        df_features.tags_tfidf[count1]=tags_tfidf\n\n        df_features.description_polarity[count1] =descriptionI.sentiment.polarity #average for all the sentences\n        df_features.description_subjectivity[count1] = descriptionI.sentiment.subjectivity\n        df_features.description_tfidf[count1]=description_tfidf\n\n        df_features.category_id[count1]=vedI_data.category_id[dayI]\n        df_features.days_since_uploaded[count1]=days_published_trending+dayI+1\n        df_features.views[count1]=vedI_data.views[dayI]\n        df_features.likes[count1]=vedI_data.likes[dayI]\n        df_features.dislikes[count1]=vedI_data.dislikes[dayI]\n        df_features.comment_count[count1]=vedI_data.comment_count[dayI]\n\n        df_features.like_rate[count1]=vedI_data.likes[dayI]\/vedI_data.views[dayI]\n        df_features.dislike_rate[count1]=vedI_data.dislikes[dayI]\/vedI_data.views[dayI]\n        df_features.comment_rate[count1]=vedI_data.comment_count[dayI]\/vedI_data.views[dayI]\n\n        if dayI==0: #no change for day 1 but give it 1 for each feature ;)\n            df_features.view_change[count1]=1\n            df_features.like_change[count1]=1\n            df_features.dislike_change[count1]=1\n            df_features.comment_change[count1]=1\n        else:\n            df_features.view_change[count1]=vedI_data.views[dayI]-vedI_data.views[dayI-1]\n            df_features.like_change[count1]=vedI_data.likes[dayI]-vedI_data.likes[dayI-1]\n            df_features.dislike_change[count1]=vedI_data.dislikes[dayI]-vedI_data.dislikes[dayI-1]\n            df_features.comment_change[count1]=vedI_data.comment_count[dayI]-vedI_data.comment_count[dayI-1]\n\n        df_features.comments_disabled[count1] = np.double(vedI_data.comments_disabled[dayI])\n        df_features.ratings_disabled[count1] = np.double(vedI_data.ratings_disabled[dayI])\n        count1+=1\n\nwith open('\/kaggle\/working\/NLP_data.pkl', 'wb') as Data:\n    pickle.dump([df_features, df_label, numFeatures, bloblist, unique_ids], Data)","7e3c1de2":"from __future__ import print_function, division\nimport numpy as np, pickle, os\nimport matplotlib.pyplot as plt\nimport matplotlib.mlab as mlab\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nfrom imblearn.over_sampling import SMOTE\nfrom xgboost import plot_importance\n\n########################### Plotting functions and others #############################################################\ndef plot_fun(loss_train, loss_val, accuracy_train_All, accuracy_val_All):\n    plt.cla()\n    plt.subplot(1, 2, 1)\n    plt.plot(np.arange(1, loss_train.__len__() + 1), loss_train, 'b', np.arange(1, loss_val.__len__() + 1),\n             loss_val, 'g')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch number')\n    plt.legend(['Training Loss', 'Validation Loss'])\n    # plt.ylim([0, 3])\n\n    plt.subplot(1, 2, 2)\n    plt.plot(np.arange(1, accuracy_train_All.__len__() + 1), accuracy_train_All, 'b-s',\n             np.arange(1, accuracy_train_All.__len__() + 1),\n             accuracy_val_All, 'g-^')\n    plt.ylabel('corrcoef')\n    plt.xlabel('Epoch number')\n    plt.legend(['Training corrcoef', 'Validation corrcoef'])\n    # plt.ylim([0,100])\n    plt.draw()\n    plt.pause(0.000000001)\n\ndef plot_dis(x):\n    num_bins = range(int(min(x)),int(max(x)+2))\n    # the histogram of the data\n    n, bins, patches = plt.hist(x, num_bins, normed=1, facecolor='blue', alpha=0.5)\n\n    # add a 'best fit' line\n    #y = mlab.normpdf(bins, np.mean(x), np.std(x))\n    #plt.plot(bins, y, 'r--')\n    plt.xlabel('Days')\n    plt.ylabel('Probability')\n    plt.title(r'Training Data Distribution')\n\n    # Tweak spacing to prevent clipping of ylabel\n    plt.subplots_adjust(left=0.15)\n    plt.show()\n\ndef plot_feature_imp(model):\n    plot_importance(model)\n################################### XGBoost hyperparameters #################################################\n\nSaveDirParent = '\/kaggle\/working\/'\nnum_classes = 1\nlearning_rateI=0.1  #step size shrinkage used to prevent overfitting. Range is [0,1]\nn_estimatorsRange=[30]#range(10,100,20) #[10,30,50,..]  number of trees you want to build.\nmax_depthRange=[5]#range(3,10,2) # determines how deeply each tree is allowed to grow during any boosting round.\ncolsample_byteeRange=[0.3]#[i\/10.0 for i in range(1,5)] #percentage of features used per tree. High value can lead to overfitting.\n#subsampleI=0.8 #percentage of samples used per tree. Low value can lead to underfitting.\ngamaI=[0.3]#[i\/10.0 for i in range(0,6)] #controls whether a given node will split based on the expected reduction\n                                        # in loss after the split. A higher value leads to fewer splits. Supported only for tree-based learners.\n#reg_alphaRange=[1e-5, 1e-2, 0.1, 1, 100] #alpha: L1 regularization on leaf weights. A large value leads to more regularization.\n\n#Example https:\/\/www.datacamp.com\/community\/tutorials\/xgboost-in-python#what\n#Tunning https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/\n\n\nnum_classes = 1\n# truncated_backprop_length = ? # not used\nRandomStart = 1 \/ 4  # pick a random start for each signal between 0 and (RandomStart*signal length)\n\n################################ Import the dataset #########################################\nexists = os.path.isfile('\/kaggle\/working\/NLP_data_reformed.pkl')\nif exists:\n    with open('\/kaggle\/working\/NLP_data_reformed.pkl', 'rb') as Data:\n        [array_features_train, array_features_val, array_features_test, array_label_train,\n         array_label_val,\n         array_label_test, num_samples_train, num_samples_val, num_samples_test, features_mean, features_std,\n         numFeatures, bloblist, unique_ids]= pickle.load(Data)\nelse: # Load and reformat the split data data\n    exists = os.path.isfile('\/kaggle\/working\/NLP_data_splits.pkl')\n    if exists:\n        with open('\/kaggle\/working\/NLP_data_splits.pkl', 'rb') as Data:\n            [list_features_train, list_features_val, list_features_test, list_features_train, list_label_train,\n             list_label_val,\n             list_label_test, num_videos_train, num_videos_val, num_videos_test, features_mean, features_std,\n             numFeatures,\n             bloblist, unique_ids] = pickle.load(Data)\n    else:  # Load and split the original data\n        with open('\/kaggle\/working\/NLP_data.pkl', 'rb') as Data:\n            [df_features, df_label, numFeatures, bloblist, unique_ids] = pickle.load(Data)\n\n        # reformat the data\n        list_videos = np.array(df_label['video_ind'])  # contains indeces for all the videos with repetition.\n        num_videos = np.int32(list_videos[-1])\n        list_features = np.empty((num_videos,), dtype=object)\n        list_label = np.empty((num_videos,), dtype=object)\n        for i in range(num_videos):\n            list_features[i] = df_features[df_label['video_ind'] == i]\n            list_label[i] = df_label.loc[df_label['video_ind'] == i, 'days_will_be_trending']\n\n        # Splitting the data into (60% training + 20% validation), 20%testing\n        list_features_train, list_features_test, list_label_train, list_label_test = train_test_split(list_features,\n                                                                                                      list_label,\n                                                                                                      test_size=0.2,\n                                                                                                      random_state=1)\n\n        list_features_train, list_features_val, list_label_train, list_label_val = train_test_split(list_features_train,\n                                                                                                    list_label_train,\n                                                                                                    test_size=0.2,\n                                                                                                    random_state=1)\n\n        num_videos_train = list_features_train.shape[0]  # number of videos for training\n        num_videos_val = list_features_val.shape[0]  # number of videos for validation\n        num_videos_test = list_features_val.shape[0]  # number of videos for testing\n\n        # normalizing the features based on mean and std of training data\n        num_samples = 0\n        features_mean = np.zeros(numFeatures)\n        features_std = np.zeros(numFeatures)\n        for video_i in range(num_videos_train):\n            features_mean = np.sum(list_features_train[video_i], axis=0) + features_mean\n            features_std = np.std(list_features_train[video_i], axis=0) + features_std\n            num_samples = num_samples + list_features_train[video_i].shape[0]\n\n        features_mean = features_mean \/ num_samples\n        features_std = features_std \/ num_samples\n\n        # normalize training data\n        for video_i in range(num_videos_train):\n            list_features_train[video_i] = (list_features_train[video_i] - features_mean) \/ features_std\n\n        # normalize validation data\n        for video_i in range(num_videos_val):\n            list_features_val[video_i] = (list_features_val[video_i] - features_mean) \/ features_std\n        # normalize testing data\n        for video_i in range(num_videos_test):\n            list_features_test[video_i] = (list_features_test[video_i] - features_mean) \/ features_std\n\n        with open('\/kaggle\/working\/NLP_data_splits.pkl', 'wb') as Data:\n            pickle.dump(\n                [list_features_train, list_features_val, list_features_test, list_features_train, list_label_train,\n                 list_label_val,\n                 list_label_test, num_videos_train, num_videos_val, num_videos_test, features_mean, features_std,\n                 numFeatures,\n                 bloblist, unique_ids], Data)\n\n    # reformat the data to one array for training , validation testing\n    num_samples_train = 0\n    for video_i in range(num_videos_train):\n        num_samples_train = num_samples_train + list_features_train[video_i].shape[0]\n    num_samples_val = 0\n    for video_i in range(num_videos_val):\n        num_samples_val = num_samples_val + list_features_val[video_i].shape[0]\n    num_samples_test = 0\n    for video_i in range(num_videos_test):\n        num_samples_test = num_samples_test + list_features_test[video_i].shape[0]\n\n    array_features_train = np.zeros([num_samples_train, numFeatures])\n    array_label_train = np.zeros([num_samples_train])\n    countI = 0\n    for video_i in range(num_videos_train):\n        indFrom = countI\n        indTo = indFrom + list_features_train[video_i].shape[0]\n        array_features_train[indFrom:indTo, :] = np.array(list_features_train[video_i])\n        array_label_train[indFrom:indTo] = np.array(list_label_train[video_i])\n        countI = indTo\n\n    array_features_val = np.zeros([num_samples_val, numFeatures])\n    array_label_val = np.zeros([num_samples_val])\n    countI = 0\n    for video_i in range(num_videos_val):\n        indFrom = countI\n        indTo = indFrom + list_features_val[video_i].shape[0]\n        array_features_val[indFrom:indTo, :] = np.array(list_features_val[video_i])\n        array_label_val[indFrom:indTo] = np.array(list_label_val[video_i])\n        countI = indTo\n\n    array_features_test = np.zeros([num_samples_test, numFeatures])\n    array_label_test = np.zeros([num_samples_test])\n    countI = 0\n    for video_i in range(num_videos_test):\n        indFrom = countI\n        indTo = indFrom + list_features_test[video_i].shape[0]\n        array_features_test[indFrom:indTo, :] = np.array(list_features_test[video_i])\n        array_label_test[indFrom:indTo] = np.array(list_label_test[video_i])\n        countI = indTo\n\n    with open('\/kaggle\/working\/NLP_data_reformed.pkl', 'wb') as Data:\n        pickle.dump(\n            [array_features_train, array_features_val, array_features_test, array_label_train,\n             array_label_val,\n             array_label_test, num_samples_train, num_samples_val, num_samples_test, features_mean, features_std,\n             numFeatures, bloblist, unique_ids], Data)\n\n#Random oversampling\nros = SMOTE(random_state=42,k_neighbors=3)\narray_features_train, array_label_train = ros.fit_resample(array_features_train, array_label_train)\n\n\n# Saving results parameters\nsavingTime = 1  # save after N epochs\nResults_train_All = np.zeros((len(n_estimatorsRange) * len(max_depthRange) * len(colsample_byteeRange),\n                              5))  # saving the results for each fold in CV, for 160 trials and for 5 metrics\nResults_val_All = np.zeros((len(n_estimatorsRange) * len(max_depthRange) * len(colsample_byteeRange), 5))  #\nCVpredictionsMaxCorr = np.zeros((num_samples_test))\nCVpredictionsMinRMSE = np.zeros((num_samples_test))\ncountSave = 0\nhighestInnCorr = -1  # initial values\nlowestInnRMSE = 100\n\nfor n_estimatorsI in  n_estimatorsRange:\n    for max_depthI in max_depthRange:\n        for colsample_byteeI in colsample_byteeRange:\n            xg_reg = xgb.XGBRegressor(objective='reg:linear', colsample_bytree=colsample_byteeI,\n                                      learning_rate=learning_rateI,\n                                      max_depth=max_depthI, gama=gamaI, n_estimators=n_estimatorsI)\n            xg_reg.fit(array_features_train, array_label_train)\n\n            # Results on training data\n            predictionsPerDay = xg_reg.predict(array_features_train)\n\n            Results_train_All[countSave, 0] = np.sqrt(mean_squared_error(array_label_train, predictionsPerDay))\n            Results_train_All[countSave, 1] = mean_absolute_error(array_label_train, predictionsPerDay)\n            Results_train_All[countSave, 2] = r2_score(array_label_train, predictionsPerDay)\n            corrI = stats.pearsonr(array_label_train, predictionsPerDay)\n            Results_train_All[countSave, 3] = corrI[0]\n            Results_train_All[countSave, 4] = corrI[1]\n\n            # Results on validation data\n            predictionsPerDay = xg_reg.predict(array_features_val)\n\n            Results_val_All[countSave, 0] = np.sqrt(mean_squared_error(array_label_val, predictionsPerDay))\n            Results_val_All[countSave, 1] = mean_absolute_error(array_label_val, predictionsPerDay)\n            Results_val_All[countSave, 2] = r2_score(array_label_val, predictionsPerDay)\n            corrI = stats.pearsonr(array_label_val, predictionsPerDay)\n            Results_val_All[countSave, 3] = corrI[0]\n            Results_val_All[countSave, 4] = corrI[1]\n            print(\"XGboost with estimators # %d, max depth # %d, training RMSE %.2f, validation RMSE %.2f\" % (\n            n_estimatorsI, max_depthI,\n            Results_train_All[\n                countSave, 0],\n            Results_val_All[\n                countSave, 0]))\n\n            # Saving the model with the highest validation correlation\n\n            if (Results_val_All[countSave, 3] > highestInnCorr and Results_val_All[countSave, 3] ==\n                    Results_val_All[countSave, 3]):\n                save_path = SaveDirParent + 'xgbMode' + '_CorrCV.dat'\n                pickle.dump(xg_reg, open(save_path, \"wb\"))\n                highestInnCorr = Results_val_All[countSave, 3]\n                print(\"Model saved in file: %s\" % save_path)\n            if (Results_val_All[countSave, 0] < lowestInnRMSE and Results_val_All[countSave, 0] ==\n                    Results_val_All[countSave, 0]):\n                save_path = SaveDirParent + 'xgbMode' + '_RMSECV.dat'\n                pickle.dump(xg_reg, open(save_path, \"wb\"))\n                lowestInnRMSE = Results_val_All[countSave, 0]\n                print(\"Model saved in file: %s\" % save_path)\n\n            countSave = countSave + 1\n\n\n\n\n# Results on Testing data\n# Using saved best model with highest correlation to find testing predictions and save it\nsave_path = SaveDirParent +'xgbMode'+ '_CorrCV.dat'\nxg_reg = pickle.load(open(save_path, \"rb\"))\npredictions_MaxCorr= xg_reg.predict(array_features_test)\n\n# Using saved best model with lowest RMSE to find testing predictions and save it\nsave_path = SaveDirParent + 'xgbMode' + '_RMSECV.dat'\nxg_reg = pickle.load(open(save_path, \"rb\"))\npredictions_MinRMSE = xg_reg.predict(array_features_test)\n\n# Finding Final Testing Results\ncorrI = stats.pearsonr(array_label_test, predictions_MaxCorr)\nclass_reportMaxCorr = [np.sqrt(mean_squared_error(array_label_test, predictions_MaxCorr)),\n                       mean_absolute_error(array_label_test, predictions_MaxCorr),\n                       r2_score(array_label_test, predictions_MaxCorr), corrI[0], corrI[1]]\nprint(\n    \"Final testing using Max validation Corr: RMSE %.2f, MAE %.2f, R2 score %.2f, Correlation coefficient %.2f (p=%.4f).\" % (\n        np.sqrt(mean_squared_error(array_label_test, predictions_MaxCorr)),\n        mean_absolute_error(array_label_test, predictions_MaxCorr),\n        r2_score(array_label_test, predictions_MaxCorr),\n        corrI[0], corrI[1]))\ncorrI = stats.pearsonr(array_label_test, predictions_MinRMSE)\nclass_reportMinRMSE = [np.sqrt(mean_squared_error(array_label_test, predictions_MinRMSE)),\n                       mean_absolute_error(array_label_test, predictions_MinRMSE),\n                       r2_score(array_label_test, predictions_MinRMSE), corrI[0], corrI[1]]\n\nprint(\n    \"Final testing using Min validation RMSE: RMSE %.2f, MAE %.2f, R2 score %.2f, Correlation coefficient %.2f (p=%.4f).\" % (\n        np.sqrt(mean_squared_error(array_label_test, predictions_MinRMSE)),\n        mean_absolute_error(array_label_test, predictions_MinRMSE),\n        r2_score(array_label_test, predictions_MinRMSE),\n        corrI[0], corrI[1]))\n\n###############Saving the variables#################\n# Saving the objects:\nwith open(SaveDirParent + 'XGboostResults.pkl', 'wb') as Results:  # Python 3: open(..., 'wb')\n    pickle.dump(\n        [Results_train_All, Results_val_All, class_reportMaxCorr, class_reportMinRMSE,\n         predictions_MinRMSE, array_label_test, predictions_MaxCorr], Results)","fea36cf3":"After feature extraction, Data distribution was plotted as shown in Figure 5 which shows that the number of videos stay trending longer decreases exponentially. This trend resulted in unbalance dataset, thus to solve this issue and to balance training dataset, Synthetic Minority Over-sampling Technique (SMOTE) was applied on the training dataset.","7641f67a":"Figure 9 shows the feature importance for Gradient Tree Boosting model. The first three important features were number of days a video was trending and a video was uploaded and number of views. After that, polarity features were important. Tags polarity was important as important as the number of views, whereas tags subjectivity was unrelated to the prediction problem. Title tf-idf had more importance than tags and description tf-idf. Lastly, change features were not important.","9962af93":"# Conclusion\nA machine learning model was developed in this project to predict the number of days a video will be trending. The dataset used in this project was a daily record of the top trending YouTube videos in the USA. Many features related to nature language processing were extracted and feed to two models which were LSTM and Gradient Tree Boosting. The latter was able to predict the number of trending days with 4-day margin of error and 0.34 (p<1e-03) correlation. The polarities of titles, tags and descriptions have a relation to the number of days a video will continue being trending and they were good features. Future work is performing text analysis of YouTube comments to extract more features to enhance the model performance.\n# References\n[1]\tTrending on YouTube, https:\/\/support.google.com\/youtube\/answer\/7239739?hl=en, April 10, 2019.\n\n[2]\tPinto, H., Almeida, J. M., & Gon\u00e7alves, M. A. (2013, February). Using early view patterns to predict the popularity of youtube videos. In Proceedings of the sixth ACM international conference on Web search and data mining (pp. 365-374). ACM.\n\n[3]\tTrending YouTube Video Statistics, https:\/\/www.kaggle.com\/datasnaek\/youtube-new, April 1, 2019.\n\n[4]\tExtensive US Youtube [EDA], https:\/\/www.kaggle.com\/kabure\/extensive-us-youtube-eda\/log, April 1, 2019.\n\n[5]\tW. Zaremba, I. Sutskever, and O. Vinyals, \u201cRecurrent neural networkregularization,\u201d arXiv preprint arXiv:1409.2329, 2014.\n\n[6]\tT. Chen and C. Guestrin, \u201cXgboost: A scalable tree boosting system,\u201d in Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pp. 785\u2013794, ACM, 2016.","b0a6ed7a":"After reformatting the data to find the labels, 25 features were extracted as shown in Table 2. Many of these features were new and involved natural language processing. Sentiment analysis was performed using TextBlob library. Term frequency\u2013inverse document frequency (tf-idf) of the titles, tags and descriptions are checking if rare and significant words been used. It is calculated by first forming a total list of the titles, tags and descriptions of all the videos after removing the stop words and punctuation. Second, a function was developed to calculate the average tf-idf between the titles, tags and descriptions with the total list using TextBlob library.\nRate features (features 17-19) are calculated to find if the video is still interesting by gaining more views with more interactions such as likes, comments and dislikes in each day.\nChange features (features 20-23) are calculated to check if the number of likes, dislikes and comments are increasing or decreasing in each day.","f289460a":"# Regression Methods\nOne method was developed in this project to predict the number of days a video stays trending.\nThe method was Gradient Tree Boosting [6]. Tree Boosting is one of the machine learning methods that used in practice in solving regression problems in many applications. It is based on ensemble of n weak regression trees to estimate the output. It was implemented using XGBoost library in python.","3f841ab0":"# Results and Discussions\nThe dataset was split into 60% for training (3.6K videos), 20% for validation (1.2K videos) and 20% for testing (1.2K videos). Performance Metrics used in the project were mean absolute error (MAE) and correlation. The model with the best performance on the validation data was selected and evaluated on the testing data. \nUnfortunately, the LSTM model was not able to learn the pattern in the training data and training and validation loss were high. \nOn the other hand, Gradient Tree Boosting was able to fit on the training data. Grid search was performed to optimize the model hyperparameters. The best model was using 30 regression trees with maximum depth of 5 with validation MAE of 4.5 and correlation of 0.31 (p<1e-03). Evaluating this model on the testing data resulted in 4.5 MAE and 0.34 (p<1e-03) correlation which means the model was able to predict the trending days with 4-day margin of error and low correlation.\nThe correlation between the predicted trending days and the actual days in which the videos stay trending is shown in Figure 6. The model was not able correctly to predict the trending days for videos stayed few days, but the correlation was better for videos stayed longer time trending. This trend can be confirmed using Figure 7 and 8 that show predicted trending days for 10 videos that actually stayed trending for 15, 20 and 25 days. The model learnt a pattern that decreases the predicted number of days over time which correlate with the number of days a video will be trending.","ed0979c4":"# CODE","c8d7bdfb":"# Intoduction\nTrending videos are YouTube videos that are appreciated by a wide range of viewers and reflects what\u2019s happening in the world [1]. Some videos are predictable: songs, movie trailer while others are surprising viral videos. Selecting trending video is not personalized for each YouTube subscriber which means same list of trending videos in each country. This feature makes it a perfect place for ads, gaining more views and having new subscribers. Every 15 minutes, the list is updated and new videos may be added. Selecting the videos that will stay and adding new video to the list are based on users\u2019 interactions such as number of views, shares, comments and likes. Therefore, some videos may stay less than a day and other may stay for many days. \nPinto et. al. developed a model to predict if a video will be popular or not based on statistical historical information of the video, but they did not target trending videos and how long a video will be popular. Therefore, the objective of this project is to find if it is possible to predict how long a video will be trending using the video\u2019s title, tags, description, number of likes per day, number of dislikes per day, number of comments per day and the actual comments. One advantage of this prediction is to preschedule and manage ads on videos in trending list.\nThe rest of this report is describing the dataset used and how it is reformatted for the purpose of this project, the feature extraction methods including sentiment analysis and statistical methods, the regression methods used to perform the prediction and lastly the project results and discussions.","35c2cfc3":"# Feature Extraction\nThe dataset does not have labels for how long a video stays in the trending list. Therefore, the data is reformatted for the purpose of this project before preforming data extraction. The data is reformatted using Python to find the number of days a video stays trending (an example of a video stays trending for 3 days is shown in Table 1).","7512bdaa":"# Abstract\nA machine learning model was developed in this project to predict the number of days a video will be trending on YouTube. The dataset used in this project was a daily record of the top trending YouTube videos in the USA. Prior to use the data, it was reformatted to find the number of days a video stays trending. Many features related to nature language processing were extracted and feed to a machine learning model which was Gradient Tree Boosting (XGBoost). The model was able to predict the number of trending days with 4-day margin of error and 0.34 (p<1e-03) correlation. The model is able of predicting the number of days the videos stay trending for long duration of time more precisely than predicting the number of trending days of the videos that stay for short duration. The polarities of titles, tags and descriptions have a relation to the number of days a video will continue being trending and they were good features.","eb3fef6a":"# Dataset\nThe dataset used in this project is a daily record of the top trending YouTube videos in the USA and other countries which was uploaded on Kaggle website [3]. The recording was on daily basis between November 14, 2017 and March 5, 2018. Each data record includes the video title, channel title, publish time, tags, views, likes and dislikes, description, and comment count. Maximum number of YouTube videos listed on trending each day is 200 and they stay between 1 to 30 days. \nOnly the USA trending list was used in this project. An example of the dataset is shown in Figure 1. The distribution of videos for different categories is shown in Figure 2 [4]. The highest frequency of videos was for entertainment category then music, whereas the lowest frequency was for cars and vehicles. The like, dislike and comment rate over time for trending videos is shown in Figure 3 [4]. The comment rate is the highest for the first two days then like rate was the highest after that. Word clouds of titles and descriptions which shown in Figure 4 show the words with highest count such as official, talk, video, vs, check and now.\n\n![](http:\/\/)"}}