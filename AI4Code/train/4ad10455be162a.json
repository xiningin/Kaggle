{"cell_type":{"2416c31e":"code","8dfe4b83":"code","311101d2":"code","749d1f2a":"code","25da10e2":"code","0b0bee01":"code","954e9f09":"code","d344937c":"markdown"},"source":{"2416c31e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport torch\nimport numpy as np\nfrom datetime import datetime\n#from util import AverageMeter\n#from model import SwapNoiseMasker, TransformerAutoEncoder\n#from data import get_data, SingleDataset\nfrom torch.utils.data import DataLoader\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport pandas as pd\nimport gc\nfrom sklearn.preprocessing import OneHotEncoder\nfrom torch.utils.data import Dataset","8dfe4b83":"class CFG:\n    \n    batch_size = 384\n    init_lr = 3e-4\n    lr_decay = .998\n    max_epochs = 5\n    save_freq=50","311101d2":"PATH = '\/kaggle\/input\/tabular-playground-series-mar-2021\/'\nfts_categorical = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9', 'cat10', \n                   'cat11', 'cat12', 'cat13', 'cat14', 'cat15', 'cat16', 'cat17', 'cat18']\n\nfts_continuous = ['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10']\n\n#unique counts should be the count of train PLUS test\nunique_counts=[  2,  15,  19,  13,  20,  84,  16,  51,  61,  19, 307,   2,   2,\n         2,   2,   4,   4,   4,   4]\n\nprint('Categorical Features', fts_categorical)\nprint('Continuous Features', fts_continuous)\n\nprint('Categorical Feature Count', len(fts_categorical))\nprint('Continuous Feature Count', len(fts_continuous))","749d1f2a":"def get_data():\n    train_data = pd.read_csv(PATH+'train.csv')\n    test_data = pd.read_csv(PATH+'test.csv')\n    \n    #combine train and test data vertically\n    X_nums = np.vstack([\n        train_data.iloc[:, 20:-1].to_numpy(),\n        test_data.iloc[:, 20:].to_numpy()\n    ])\n    X_nums = (X_nums - X_nums.mean(0)) \/ X_nums.std(0) #normalize\n    \n    #stack the categorical data\n    X_cat = np.vstack([\n        train_data.iloc[:, 1:20].to_numpy(),\n        test_data.iloc[:, 1:20].to_numpy()\n    ])\n    #encode the categoricals\n    encoder = OneHotEncoder(sparse=False)\n    X_cat = encoder.fit_transform(X_cat)\n    \n    #join the categorical and continuous data horizontally\n    X = np.hstack([X_cat, X_nums])\n    y = train_data['target'].to_numpy().reshape(-1, 1)\n    return X, y, X_cat.shape[1], X_nums.shape[1] #this lets us know how many categorical and continuous features there are\n\nclass SingleDataset(Dataset):\n    def __init__(self, x, is_sparse=False):\n        self.x = x.astype('float32')\n        self.is_sparse = is_sparse\n\n    def __len__(self):\n        return self.x.shape[0]\n\n    def __getitem__(self, index):\n        x = self.x[index]\n        if self.is_sparse: x = x.toarray().squeeze()\n        return x    ","25da10e2":"bce_logits = torch.nn.functional.binary_cross_entropy_with_logits\nmse = torch.nn.functional.mse_loss\n\n#torch docs\n\n#embed_dim \u2013 total dimension of the model.\n#num_heads \u2013 parallel attention heads.\n#dropout \u2013 a Dropout layer on attn_output_weights. Default: 0.0.\n#bias \u2013 add bias as module parameter. Default: True.\n#add_bias_kv \u2013 add bias to the key and value sequences at dim=0.\n#add_zero_attn \u2013 add a new batch of zeros to the key and value sequences at dim=1.\n#kdim \u2013 total number of features in key. Default: None.\n#vdim \u2013 total number of features in value. Default: None.\n\nclass TransformerEncoder(torch.nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout, feedforward_dim):\n        super().__init__()\n        self.attn = torch.nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n        self.linear_1 = torch.nn.Linear(embed_dim, feedforward_dim)\n        self.linear_2 = torch.nn.Linear(feedforward_dim, embed_dim)\n        self.layernorm_1 = torch.nn.LayerNorm(embed_dim)\n        self.layernorm_2 = torch.nn.LayerNorm(embed_dim)\n    \n    def forward(self, x_in):\n        attn_out, _ = self.attn(x_in, x_in, x_in)\n        x = self.layernorm_1(x_in + attn_out)\n        ff_out = self.linear_2(torch.nn.functional.relu(self.linear_1(x)))\n        x = self.layernorm_2(x + ff_out)\n        return x\n\n\nclass TransformerAutoEncoder(torch.nn.Module):\n    def __init__(\n            self, \n            num_inputs, \n            n_cats, \n            n_nums, \n            hidden_size=1024, \n            num_subspaces=8,\n            embed_dim=128, \n            num_heads=8, \n            dropout=0, \n            feedforward_dim=512, \n            emphasis=.75, \n            task_weights=[len(fts_categorical), len(fts_continuous)],\n            mask_loss_weight=2,\n        ):\n        super().__init__()\n        assert hidden_size == embed_dim * num_subspaces\n        self.n_cats = n_cats\n        self.n_nums = n_nums\n        self.num_subspaces = num_subspaces\n        self.num_heads = num_heads\n        self.embed_dim = embed_dim\n        self.emphasis = emphasis\n        self.task_weights = np.array(task_weights) \/ sum(task_weights)\n        self.mask_loss_weight = mask_loss_weight\n\n        self.excite = torch.nn.Linear(in_features=num_inputs, out_features=hidden_size)\n        self.encoder_1 = TransformerEncoder(embed_dim, num_heads, dropout, feedforward_dim)\n        self.encoder_2 = TransformerEncoder(embed_dim, num_heads, dropout, feedforward_dim)\n        self.encoder_3 = TransformerEncoder(embed_dim, num_heads, dropout, feedforward_dim)\n        \n        \n        self.mask_predictor = torch.nn.Linear(in_features=hidden_size, out_features=num_inputs)\n        self.reconstructor = torch.nn.Linear(in_features=hidden_size + num_inputs, out_features=num_inputs)\n\n    def divide(self, x):\n        batch_size = x.shape[0]\n        x = x.reshape((batch_size, self.num_subspaces, self.embed_dim)).permute((1, 0, 2))\n        return x\n\n    def combine(self, x):\n        batch_size = x.shape[1]\n        x = x.permute((1, 0, 2)).reshape((batch_size, -1))\n        return x\n\n    def forward(self, x):\n        x = torch.nn.functional.relu(self.excite(x))\n        \n        x = self.divide(x)\n        x1 = self.encoder_1(x)\n        x2 = self.encoder_2(x1)\n        x3 = self.encoder_3(x2)\n        x = self.combine(x3)\n        \n        predicted_mask = self.mask_predictor(x)\n        reconstruction = self.reconstructor(torch.cat([x, predicted_mask], dim=1))\n        return (x1, x2, x3), (reconstruction, predicted_mask)\n\n    def split(self, t):\n        return torch.split(t, [self.n_cats, self.n_nums], dim=1)\n\n    def feature(self, x):\n        attn_outs, _ = self.forward(x)\n        return torch.cat([self.combine(x) for x in attn_outs], dim=1)\n\n    def loss(self, x, y, mask, reduction='mean'):   \n        _, (reconstruction, predicted_mask) = self.forward(x)\n        \n        x_cats, x_nums = self.split(reconstruction)\n        y_cats, y_nums = self.split(y)\n        w_cats, w_nums = self.split(mask * self.emphasis + (1 - mask) * (1 - self.emphasis))\n        cat_loss = self.task_weights[0] * torch.mul(w_cats, bce_logits(x_cats, y_cats, reduction='none'))\n        num_loss = self.task_weights[1] * torch.mul(w_nums, mse(x_nums, y_nums, reduction='none'))\n        reconstruction_loss = torch.cat([cat_loss, num_loss], dim=1) if reduction == 'none' else cat_loss.mean() + num_loss.mean()\n        mask_loss = self.mask_loss_weight * bce_logits(predicted_mask, mask, reduction=reduction)\n        return reconstruction_loss + mask_loss if reduction == 'mean' else [reconstruction_loss, mask_loss]\n\n\nclass SwapNoiseMasker(object):\n    def __init__(self, probas):\n        self.probas = torch.from_numpy(np.array(probas))\n\n    def apply(self, X):\n        should_swap = torch.bernoulli(self.probas.to(X.device) * torch.ones((X.shape)).to(X.device))\n        corrupted_X = torch.where(should_swap == 1, X[torch.randperm(X.shape[0])], X)\n        mask = (corrupted_X != X).float()\n        return corrupted_X, mask\n\n\ndef test_tf_encoder():\n    m = TransformerEncoder(4, 2, .1, 16)\n    x = torch.rand((32, 8))\n    x = x.reshape((32, 2, 4)).permute((1, 0, 2))\n    o = m(x)\n    assert o.shape == torch.Size([2, 32, 4])\n\n\ndef test_dae_model():\n    m = TransformerAutoEncoder(5, 2, 3, 16, 4, 4, 2, .1, 4, .75)\n    x = torch.cat([torch.randint(0, 2, (5, 2)), torch.rand((5, 3))], dim=1)\n    f = m.feature(x)\n    assert f.shape == torch.Size([5, 16 * 3])\n    loss = m.loss(x, x, (x > .2).float())\n\n\ndef test_swap_noise():\n    probas = [.2, .5, .8]\n    m = SwapNoiseMasker(probas)\n    diffs = []\n    for i in range(1000):\n        x = torch.rand((32, 3))\n        noisy_x, _ = m.apply(x)\n        diffs.append((x != noisy_x).float().mean(0).unsqueeze(0)) \n\n    print('specified : ', probas, ' - actual : ', torch.cat(diffs, 0).mean(0))\n\n\nif __name__ == '__main__':\n    test_tf_encoder()\n    test_dae_model()\n    test_swap_noise()","0b0bee01":"class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count","954e9f09":"# Hyper-params\nmodel_params = dict(\n    hidden_size=1024,\n    num_subspaces=8,\n    embed_dim=128,\n    num_heads=8,\n    dropout=0,\n    feedforward_dim=512,\n    emphasis=.75,\n    mask_loss_weight=2\n)\n\n#repeats should correspond to categorical counts for categorical columns\nrepeats = [x for x in unique_counts] + [1 for x in range(len(fts_continuous))]\n\n#probabilities are just set to 0.5 for now\nprobas = [0.5 for x in range(len(fts_categorical))] + [0.5 for x in range(len(fts_continuous))]\n\nprint(len(repeats), len(probas))\nswap_probas = sum([[p] * r for p, r in zip(probas, repeats)], [])\n\n#  get data\nX, Y, n_cats, n_nums = get_data()\n\ntrain_dl = DataLoader(\n    dataset=SingleDataset(X),\n    batch_size=CFG.batch_size,\n    shuffle=True,\n    pin_memory=True,\n    drop_last=True\n)\n\n# setup model\nmodel = TransformerAutoEncoder(\n    num_inputs=X.shape[1],\n    n_cats=n_cats,\n    n_nums=n_nums,\n    **model_params\n).cuda()\nmodel_checkpoint = 'model_checkpoint.pth'\n\nprint(model)\n\nnoise_maker = SwapNoiseMasker(swap_probas)\noptimizer = torch.optim.Adam(model.parameters(), lr=CFG.init_lr)\nscheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=CFG.lr_decay)\n\n# train model\nfor epoch in range(CFG.max_epochs):\n    t0 = datetime.now()\n    model.train()\n    meter = AverageMeter()\n    for i, x in enumerate(train_dl):\n        x = x.cuda()\n        x_corrputed, mask = noise_maker.apply(x)\n        optimizer.zero_grad()\n        loss = model.loss(x_corrputed, x, mask)\n        loss.backward()\n        optimizer.step()\n\n        meter.update(loss.detach().cpu().numpy())\n\n    delta = (datetime.now() - t0).seconds\n    scheduler.step()\n    \n    print('epoch {:5d} - loss {:.6f} - {:4.6f} sec per epoch'.format(epoch, meter.avg, delta))  \n    \n    model_checkpoint = f'model_checkpoint_{epoch}.pth'\n    \n    if epoch%CFG.save_freq==0:\n        print('Saving to checkpoint')\n        model_checkpoint = f'model_checkpoint_{epoch}.pth'\n        torch.save({\n                \"optimizer\": optimizer.state_dict(),\n                \"scheduler\": scheduler.state_dict(),\n                \"model\": model.state_dict()\n            }, model_checkpoint\n        )\n        \n\nmodel_checkpoint = f'model_checkpoint_final.pth'\ntorch.save({\n        \"optimizer\": optimizer.state_dict(),\n        \"scheduler\": scheduler.state_dict(),\n        \"model\": model.state_dict()\n    }, model_checkpoint\n)\nmodel_state = torch.load(model_checkpoint)\nmodel.load_state_dict(model_state['model'])","d344937c":"If you find this notebook useful, please visit original discussion post (#1 Solution in Feb21 Comp, which is where I have taken this code from) and upvote.\n    \nhttps:\/\/www.kaggle.com\/c\/tabular-playground-series-feb-2021\/discussion\/222745\n\nAnd winning solution from Jan\n\nhttps:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2021\/discussion\/216037"}}