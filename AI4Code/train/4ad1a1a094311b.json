{"cell_type":{"4bdb10b2":"code","410fdad2":"code","bd8d1fe5":"code","bc821d9f":"code","501132ba":"code","d2165ec3":"code","5eb76bbc":"code","f946aca4":"code","0097be79":"code","f61f4852":"code","84ef4b8f":"code","b8b6b590":"code","8e56af1a":"code","13da70db":"code","28068e38":"code","d7c53c5c":"code","b99d8484":"code","2e4e70c6":"code","4b21ec73":"code","e872a6f5":"code","86b1a760":"code","32190bc0":"code","b0cee96a":"code","ec745615":"code","250ca87a":"code","4170ef76":"code","ebf9d879":"code","99052829":"code","b5e1cf54":"code","1c5258c2":"code","e7780410":"code","133990b6":"code","30c7dad3":"code","e77df054":"code","d3696223":"code","9a44a31f":"code","7a54363f":"code","cd61a82a":"code","b780d961":"code","2293f510":"code","db695a58":"code","74450e52":"code","ac1a0d41":"code","35892bb2":"code","a7d89d36":"code","03f534d8":"code","d65c7ade":"code","85b4401b":"code","b903cbb2":"code","6bd1c84d":"code","75148c72":"code","c6caaba1":"code","486a9411":"code","2045c032":"code","c9f4a2ba":"code","9b6c4e4c":"code","05de72a7":"code","51a3a1c4":"code","d1de40aa":"code","c4ab7ca0":"code","f2c97375":"code","3c1e4614":"markdown","7a5b0d93":"markdown","0492ce0a":"markdown","75a5e523":"markdown","ae5a4bf4":"markdown","4024dd02":"markdown","3d23864a":"markdown","1da1e4d5":"markdown","b25f6ac0":"markdown","245ae1e8":"markdown","19dac051":"markdown","51325100":"markdown","4d34d671":"markdown","3931b291":"markdown","d27f6d84":"markdown","77c55085":"markdown","5c4957d6":"markdown","5fa3d25e":"markdown","eb2bba80":"markdown","84af1ea4":"markdown","beea5adc":"markdown","b9abb941":"markdown","06f49528":"markdown","e70cc04c":"markdown","e4a866b9":"markdown","592f2a22":"markdown","9da329e0":"markdown","d8ad544c":"markdown","93362ea3":"markdown","c3f18e1e":"markdown","b5a0156b":"markdown","10eb1133":"markdown","0c4e8c10":"markdown","638f698e":"markdown","0b7a2ce9":"markdown","be737511":"markdown","dec06ff8":"markdown","899a5b65":"markdown"},"source":{"4bdb10b2":"import nltk\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('stopwords')\nnltk.download('averaged_perceptron_tagger')","410fdad2":"text = 'The quick brown fox jumped over The Big Dog'\ntext","bd8d1fe5":"text.lower()","bc821d9f":"text.upper()","501132ba":"text.title()","d2165ec3":"sample_text = (\"US unveils world's most powerful supercomputer, beats China. \" \n               \"The US has unveiled the world's most powerful supercomputer called 'Summit', \" \n               \"beating the previous record-holder China's Sunway TaihuLight. With a peak performance \"\n               \"of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, \"\n               \"which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, \"\n               \"which reportedly take up the size of two tennis courts.\")\nsample_text","5eb76bbc":"nltk.sent_tokenize(sample_text)","f946aca4":"# It doen\u2019t seperates the punctuation from the words.\n# from nltk.tokenize import PunktWordTokenizer\n# PunktWordTokenizer().tokenize(sample_text)","0097be79":"print(nltk.word_tokenize(sample_text))","f61f4852":"# It seperates the punctuation from the words.\nfrom nltk.tokenize import WordPunctTokenizer\nWordPunctTokenizer().tokenize(sample_text)","84ef4b8f":"from nltk.tokenize import TreebankWordTokenizer\n  \nTreebankWordTokenizer().tokenize(sample_text)","b8b6b590":"import spacy\n\n# spacy.load('en') won't work so change it to spacy.load('en_core_web_sm')\nnlp = spacy.load('en_core_web_sm') \n\ntext_spacy = nlp(sample_text)","8e56af1a":"[obj.text for obj in text_spacy.sents]","13da70db":"print([obj.text for obj in text_spacy])","28068e38":"import requests\n\ndata = requests.get('http:\/\/www.gutenberg.org\/cache\/epub\/8001\/pg8001.html')\ncontent = data.text\nprint(content[2745:3948])","d7c53c5c":"import re\nfrom bs4 import BeautifulSoup\n\ndef strip_html_tags(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    [s.extract() for s in soup(['iframe', 'script'])]\n    stripped_text = soup.get_text()\n    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n    return stripped_text\n\nclean_content = strip_html_tags(content)\nprint(clean_content[1163:1957])","b99d8484":"import unicodedata\n\ndef remove_accented_chars(text):\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return text","2e4e70c6":"s = 'S\u00f3m\u011b \u00c1cc\u011bnt\u011bd t\u011bxt'\ns","4b21ec73":"remove_accented_chars(s)","e872a6f5":"import re\n\ndef remove_special_characters(text, remove_digits=False):\n    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n    text = re.sub(pattern, '', text)\n    return text\n","86b1a760":"# Sentence\ns = \"Well this was fun! See you at 7:30, What do you think!!? #$@@9318@ \ud83d\ude42\ud83d\ude42\ud83d\ude42\"\ns","32190bc0":"# Removing special caracters and digits too\nremove_special_characters(s, remove_digits=True)","b0cee96a":"# Removing only special caracters\nremove_special_characters(s)","ec745615":"!pip install contractions\n!pip install textsearch\nprint(\"installation done\")","250ca87a":"s = \"Y'all can't expand contractions I'd think! You wouldn't be able to. How'd you do it?\"\ns","4170ef76":"import contractions\n\n# list all the contractions\nlist(contractions.contractions_dict.items())[:10]","ebf9d879":"# fix the sentence containing contractions.\ncontractions.fix(s)","99052829":"# contracted text\ntext = '''I'll be there within 5 min. Shouldn't you be there too? \n          I'd love to see u there my dear. It's awesome to meet new friends.\n          We've been waiting for this day for so long.'''\n  \n# creating an empty list\nexpanded_words = []    \nfor word in text.split():\n  # using contractions.fix to expand the shotened words\n  expanded_words.append(contractions.fix(word))   \n    \nexpanded_text = ' '.join(expanded_words)\nprint('Original text: ' + text)\nprint(\"--\"*80+\"\\n\")\nprint('Expanded_text: ' + expanded_text)","b5e1cf54":"# Porter Stemmer\nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()\n\nps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped')","1c5258c2":"ps.stem('lying')","e7780410":"ps.stem('strange')","133990b6":"from nltk.stem import WordNetLemmatizer\nwnl = WordNetLemmatizer()","30c7dad3":"help(wnl.lemmatize)","e77df054":"# lemmatize nouns\nprint(wnl.lemmatize('cars', 'n'))\nprint(wnl.lemmatize('boxes', 'n'))","d3696223":"# lemmatize verbs\nprint(wnl.lemmatize('running', 'v'))\nprint(wnl.lemmatize('ate', 'v'))","9a44a31f":"# lemmatize adjectives\nprint(wnl.lemmatize('saddest', 'a'))\nprint(wnl.lemmatize('fancier', 'a'))","7a54363f":"# ineffective lemmatization\nprint(wnl.lemmatize('ate', 'n'))\nprint(wnl.lemmatize('fancier', 'v'))\nprint(wnl.lemmatize('fancier'))","cd61a82a":"s = 'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'","b780d961":"tokens = nltk.word_tokenize(s)\nprint(tokens)","2293f510":"lemmatized_text = ' '.join(wnl.lemmatize(token) for token in tokens)\nlemmatized_text","db695a58":"tagged_tokens = nltk.pos_tag(tokens)\nprint(tagged_tokens)","74450e52":"from nltk.corpus import wordnet\n\ndef pos_tag_wordnet(tagged_tokens):\n    tag_map = {'j': wordnet.ADJ, 'v': wordnet.VERB, 'n': wordnet.NOUN, 'r': wordnet.ADV}\n    new_tagged_tokens = [(word, tag_map.get(tag[0].lower(), wordnet.NOUN))\n                            for word, tag in tagged_tokens]\n    return new_tagged_tokens","ac1a0d41":"wordnet_tokens = pos_tag_wordnet(tagged_tokens)\nprint(wordnet_tokens)","35892bb2":"sentence = \"the little yellow dog barked at the cat\"","a7d89d36":"#Define your grammar using regular expressions\ngrammar = ('''\n    NP: {<DT>?<JJ>*<NN>} # NP\n    ''')","03f534d8":"chunkParser = nltk.RegexpParser(grammar)\ntagged = nltk.pos_tag(nltk.word_tokenize(sentence))\ntagged","d65c7ade":"tree = chunkParser.parse(tagged)\nfor subtree in tree.subtrees():\n    print(subtree)","85b4401b":"# tree.draw()\n# .draw() have some issues to fix","b903cbb2":"lemmatized_text = ' '.join(wnl.lemmatize(word, tag) for word, tag in wordnet_tokens)\nlemmatized_text","6bd1c84d":"wnl = WordNetLemmatizer()\n\ndef wordnet_lemmatize_text(text):\n    tagged_tokens = nltk.pos_tag(nltk.word_tokenize(text))\n    wordnet_tokens = pos_tag_wordnet(tagged_tokens)\n    lemmatized_text = ' '.join(wnl.lemmatize(word, tag) for word, tag in wordnet_tokens)\n    return lemmatized_text","75148c72":"s","c6caaba1":"wordnet_lemmatize_text(s)","486a9411":"import spacy\nnlp = spacy.load('en_core_web_sm', parse=False, tag=False, entity=False)\n\ndef spacy_lemmatize_text(text):\n    text = nlp(text)\n    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n    return text","2045c032":"# the sentence\ns","c9f4a2ba":"spacy_lemmatize_text(s)","9b6c4e4c":"def remove_stopwords(text, is_lower_case=False, stopwords=None):\n    if not stopwords:\n        stopwords = nltk.corpus.stopwords.words('english')\n    tokens = nltk.word_tokenize(text)\n    tokens = [token.strip() for token in tokens]\n    \n    if is_lower_case:\n        filtered_tokens = [token for token in tokens if token not in stopwords]\n    else:\n        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n    \n    filtered_text = ' '.join(filtered_tokens)    \n    return filtered_text","05de72a7":"stop_words = nltk.corpus.stopwords.words('english')\nprint(stop_words[:10])","51a3a1c4":"s","d1de40aa":"remove_stopwords(s, is_lower_case=False)","c4ab7ca0":"stop_words.remove('the')\nstop_words.append('brown')","f2c97375":"remove_stopwords(s, is_lower_case=False, stopwords=stop_words)","3c1e4614":"# Case Conversion","7a5b0d93":"**N.B**: `PunktWordTokenizer` .There isn't one called PunktWordTokenizer anymore. It was internal and was not intended to be public. Which is why you can't import that name.","0492ce0a":"# Removing HTML tags & noise","75a5e523":"# Stemming","ae5a4bf4":" <div class=\"alert alert-block alert-info\">  \n    <h1 id=\"heading\" align=\"center\"> \ud83d\udcccNatural Language Processing - NLP 101 \ud83d\udccc<\/h1>\n    <img src=\"https:\/\/litslink.com\/wp-content\/uploads\/2020\/07\/nlp-illustration.png\"\/>\n <\/div>","4024dd02":"### Let's call the function on the below sentence and test it","3d23864a":"**What is Part of Speech?**\n\nThe **part of speech** explains how a word is used in a sentence. There are eight main parts of speech - **nouns, pronouns, adjectives, verbs, adverbs, prepositions, conjunctions and interjections.**\n\n![Part of Speech](https:\/\/miro.medium.com\/max\/345\/0*V635bzjWK2n1jBsd.png)\n\n* Noun (N)- Daniel, London, table, dog, teacher, pen, city, happiness, hope\n* Verb (V)- go, speak, run, eat, play, live, walk, have, like, are, is\n* Adjective(ADJ)- big, happy, green, young, fun, crazy, three\n* Adverb(ADV)- slowly, quietly, very, always, never, too, well, tomorrow\n* Preposition (P)- at, on, in, from, with, near, between, about, under\n* Conjunction (CON)- and, or, but, because, so, yet, unless, since, if\n* Pronoun(PRO)- I, you, we, they, he, she, it, me, us, them, him, her, this\n* Interjection (INT)- Ouch! Wow! Great! Help! Oh! Hey! Hi!\n\nMost **POS** are divided into sub-classes. **POS Tagging** simply means labeling words with their appropriate Part-Of-Speech.\n\n[reference](https:\/\/medium.com\/greyatom\/learning-pos-tagging-chunking-in-nlp-85f7f811a8cb)\n","1da1e4d5":"### Let's remove the words 'the' and 'brown' from the stop_words list and call the function with this new list","b25f6ac0":"# Removing Special Characters, Numbers and Symbols","245ae1e8":"# Expanding Contractions","19dac051":"# Removing Accented Characters","51325100":"# Spacy Tokenization\n\n## Sentence\n## Word","4d34d671":"**Lemmatization**, on the other hand, takes into consideration the morphological analysis of the words. To do so, it is necessary to have detailed dictionaries which the algorithm can look through to link the form back to its lemma. ","3931b291":"**What are contractions?**\n\nContractions are words or combinations of words that are shortened by dropping letters and replacing them by an apostrophe.\n\nNowadays, where everything is shifting online, we communicate with others more through text messages or posts on different social media like Facebook, Instagram, Whatsapp, Twitter, LinkedIn, etc. in the form of texts. With so many people to talk, we rely on abbreviations and shortened form of words for texting people.\n\n**For example** I\u2019ll be there within 5 min. Are u not gng there? Am I mssng out on smthng? I\u2019d like to see u near d park.\n\nIn English **contractions**, we often drop the vowels from a word to form the contractions. Removing contractions contributes to text standardization and is useful when we are working on Twitter data, on reviews of a product as the words play an important role in sentiment analysis.\n\nHow to expand contractions?\n\n* Using contractions library installed above.","d27f6d84":"### POS Tagging","77c55085":"---","5c4957d6":"## WordPunkt Tokenizer","5fa3d25e":"**Tokenization** is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph\n\n![Tokenization](https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/tokenizer.jpg)","eb2bba80":"## Sentence tokenizer","84af1ea4":"![Chunking](https:\/\/miro.medium.com\/max\/548\/1*ZNcznQAcvThZwjF0mPjTqw.png)\n\n* Tree diagram from the above code","beea5adc":"**Stemming** algorithms work by cutting off the end or the beginning of the word, taking into account a list of common prefixes and suffixes that can be found in an inflected word. This indiscriminate cutting can be successful in some occasions, but not always.","b9abb941":"# Natural Language Processing (NLP)\n\n**NLP** is a subfield of computer science, artificial intelligence, information engineering, and human-computer interaction. This field focuses on how to program computers to process and analyze large amounts of natural language data. It is difficult to perform as the process of reading and understanding languages is far more complex than it seems at first glance.","06f49528":"# Lemmatization","e70cc04c":"**Chunking** is a process of extracting phrases from unstructured text. Instead of just simple tokens which may not represent the actual meaning of the text, its advisable to use phrases such as `\u201cSouth Africa\u201d` as a single word instead of `\u2018South\u2019` and `\u2018Africa\u2019` separate words.","e4a866b9":"# Install Dependencies","592f2a22":"## PunktWord Tokenizer","9da329e0":"## TreebanWord Tokenizer","d8ad544c":"### Tokenize","93362ea3":"### Effective Lemmatization","c3f18e1e":"# Stopword Removal","b5a0156b":"# This is all fot this Notebook. \ud83d\udcd4\n\n\nI am starting to use NLP so, you remarks, suggestion are welcome and appreciated.\ud83d\udca1\ud83d\udca1\ud83d\udca1","10eb1133":"### Tag conversion to WordNet Tags","0c4e8c10":"### Chunking","638f698e":"## Word tokenizer","0b7a2ce9":"## Lemmatization with Spacy","be737511":"### Let's define a function such that you put all the above steps together so that it does the following\n\n- Function name is __`wordnet_lemmatize_text(...)`__\n- Input is a variable __`text`__ which should take in a document (bunch of words)\n- Call the earlier defined functions and utilize them\n- Return lemmatized text as the output (as a string)","dec06ff8":"# Tokenization","899a5b65":"**How sent_tokenize works ?**\n\nThe `sent_tokenize` function uses an instance of `PunktSentenceTokenizer` from the `nltk.tokenize.punkt module`, which is already been trained and thus very well knows to mark the end and beginning of sentence at what characters and punctuation."}}