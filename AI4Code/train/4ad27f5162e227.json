{"cell_type":{"cad552c6":"code","074ec6fd":"code","cde4a684":"code","99b121b3":"code","fc9bcff6":"code","1610420c":"code","f4999ce7":"code","987ecb9f":"code","78e52a04":"code","d20ad14d":"code","728275c7":"code","8dc3b0ff":"code","1fad3dab":"code","b1eea651":"code","dabc84b8":"code","d3ffcafb":"code","b98320ba":"code","ce6ff20b":"code","b7ee8906":"code","c1953a53":"code","33a723a6":"code","45bbb067":"code","696ed7d8":"code","aa817b82":"code","c68dec63":"code","365db686":"code","b019b2b5":"code","dfe594bb":"code","6091db27":"code","95701fae":"code","e3862854":"code","b0ed47a3":"code","76c0d80b":"code","70ad246f":"code","f8ea4da8":"code","788cb49d":"code","b6f899c5":"code","fc2d0f51":"code","aed67fad":"code","cc69172d":"code","f20e1038":"code","e5294a51":"code","427b7231":"code","850fc457":"code","d68b44aa":"code","0aff7998":"code","f893e3d7":"code","1e79b86d":"code","a22d6bc1":"code","46021e24":"code","9bba7ed2":"code","36b5ab31":"code","0b5464c2":"code","2cf0b5fc":"code","8b0ccf81":"code","d39ab6af":"code","0b490bff":"code","5e353c74":"code","663e8442":"code","6a0925de":"code","6bd3255c":"markdown","58a38bdb":"markdown","22b6966d":"markdown","17047c66":"markdown","1ef9893c":"markdown","49101eab":"markdown","4581370d":"markdown","0be03a0f":"markdown","58584a59":"markdown","1d7a7557":"markdown","9b8fcd3d":"markdown","3f59adb1":"markdown","3d2f89c3":"markdown","ce55ecfb":"markdown","db3ae5cd":"markdown","96d1e254":"markdown","3411b5fa":"markdown","5ee740fe":"markdown","ffc6012d":"markdown","8db5a7bc":"markdown","e57bb062":"markdown","ec002947":"markdown","3b9cf199":"markdown"},"source":{"cad552c6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","074ec6fd":"import numpy as np # linear algebra\nimport pandas as pd # data processing\npd.set_option('max_rows',10)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","cde4a684":"# save the test_data for easier reference\ntest_data_path = \"..\/input\/titanic\/test.csv\"\n# save the test_data into a Dataframe\ntest_data = pd.read_csv(test_data_path)\n# save the train_data for easier reference\ntrain_data_path =\"..\/input\/titanic\/train.csv\"\n# save the train_data into a Dataframe\ntrain_data = pd.read_csv(train_data_path)\n","99b121b3":"train_data.columns","fc9bcff6":"train_data.shape","1610420c":"train_data.head()","f4999ce7":"train_data.describe()","987ecb9f":"train_data.info()","78e52a04":"sns.pairplot(train_data)\n","d20ad14d":"sns.scatterplot(x ='Age', y ='Survived', data = train_data, hue ='Sex')\nplt.figure(figsize = (10,10))","728275c7":"sns.barplot(x ='Pclass', y ='Survived', data = train_data, hue ='Sex')","8dc3b0ff":"\nsns.barplot(x='SibSp', y='Survived', hue ='Sex', data = train_data)","1fad3dab":"sns.barplot(x ='Parch', y = 'Survived', data = train_data, hue ='Sex')","b1eea651":"sns.barplot(x ='Embarked', y ='Survived', data = train_data, hue ='Sex')","dabc84b8":"sns.countplot(x ='Embarked', data = train_data, hue = 'Sex')","d3ffcafb":"sns.distplot(train_data['Age'], kde = False, bins = 50)","b98320ba":"sns.distplot(test_data['Age'], kde = False)","ce6ff20b":"sns.distplot(train_data.Fare, kde = False)","b7ee8906":"'''''\n\nfeatures_dropped = ['Name','Ticket','Cabin']\n\ndef dropped_features(df):\n    return df.drop(features_dropped, axis = 1, inplace = True)\n\ndropped_features(train_data)\ndropped_features(test_data)\n\ntrain_data\n'''''\n","c1953a53":"'''''\n# Drop the PassengerId from the training set\n\ntrain_data.drop('PassengerId', axis = 1, inplace = True)\n'''''","33a723a6":"train_data","45bbb067":"from sklearn.model_selection import train_test_split\n\ntrain_test, test_set = train_test_split(train_data, test_size = 0.2, random_state = 42)","696ed7d8":"train_test","aa817b82":"X = train_data.drop('Survived', axis = 1) # predictors\ny = train_data['Survived']\n","c68dec63":"X.shape","365db686":"y.shape","b019b2b5":"from sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder,StandardScaler\nfrom sklearn.pipeline import Pipeline","dfe594bb":"from sklearn.compose import make_column_selector as selector\n# Scale numeric values\nnum_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler()),\n    ])\n\n# One-hot encode categorical values\ncat_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore')),\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', num_transformer, selector(dtype_include='float64')),\n        ('cat', cat_transformer, selector(dtype_include='object')),\n    ])\n\nX_train = preprocessor.fit_transform(X)","6091db27":"from sklearn.model_selection import  cross_val_predict\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score,f1_score","95701fae":"from sklearn.linear_model import SGDClassifier\nfrom scipy.stats import randint\n\nsgd_clf = SGDClassifier(random_state = 42)\n\nparams = {\n    'loss': ['hinge','log','modified_huber','squared_loss','squared_hinge'],\n    'penalty': ['l1','l2','elasticnet'],\n    'alpha': [0.001,0.01,0.1,0.03,0.3],\n    'max_iter': randint(low=1, high=10000),\n    \n          \n}\nrnd_sgd = RandomizedSearchCV(sgd_clf,param_distributions=params, random_state =42, cv = 3,\n                            scoring ='precision')\n\nrnd_sgd.fit(X_train,y)","e3862854":"rnd_sgd.best_estimator_","b0ed47a3":"from sklearn.model_selection import cross_val_score\n\n\nsgd_clf = SGDClassifier(alpha=0.1, loss='modified_huber', max_iter=5579, penalty='l1',\n              random_state=42)\ncross_val_score(sgd_clf, X_train,y, cv = 3, scoring ='precision')\n\n","76c0d80b":"import sklearn.metrics\nsorted(sklearn.metrics.SCORERS.keys())","70ad246f":"# measure accuracy score of my model\nfrom sklearn.model_selection import cross_val_score\ncross_val_score(sgd_clf, X_train,y, cv = 3, scoring ='precision')","f8ea4da8":"sgd_clf?","788cb49d":"params = {\n    'loss': ['hinge','log','modified_huber','squared_hinge','perceptron'],\n    'penalty': ['l2', 'l1', 'elasticnet'],\n    'fit_intercept': [True,False],\n    'l1_ratio': np.linspace(0,1,15),\n    'max_iter': randint(low = 1, high = 10000),\n    'learning_rate': ['optimal','invscaling','adaptive','constant'],\n    'shuffle': [True, False],\n    'early_stopping': [True,False],\n    'alpha': randint(low = 0.01, high = 9),\n    'eta0': [0.1,0.15,0.2,0.25,0.5,0.75,1],\n    \n    \n}\n\nsgd_rnd = RandomizedSearchCV( sgd_clf, param_distributions = params,\n                            cv = 3, random_state = 42, scoring = 'precision')\nsgd_rnd.fit(X_train,y)","b6f899c5":"sgd_rnd.best_estimator_","fc2d0f51":"from sklearn.model_selection import  cross_val_predict\n\nsgd_clf = SGDClassifier(alpha=0, eta0=0.15, fit_intercept=False,\n              l1_ratio=0.21428571428571427, learning_rate='invscaling',\n              loss='log', max_iter=3844, penalty='l1', random_state=42)\n\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score,f1_score\n\ny_train_pred = cross_val_predict(sgd_clf, X_train,y, cv = 3)\nconfusion_matrix(y,y_train_pred)","aed67fad":"print('Precsion Score: ',precision_score(y,y_train_pred))\nprint('Recall Score:' ,recall_score(y,y_train_pred))\nprint(\"f1_score : \", f1_score(y,y_train_pred))","cc69172d":"from sklearn.ensemble import RandomForestClassifier\n\nforest_clf = RandomForestClassifier(random_state = 42)\n\n\nforest_scores = cross_val_score(forest_clf,X_train,y,scoring ='precision',\n                               cv = 3)\nforest_scores","f20e1038":"params = {\n    'n_estimators': randint(low =1, high = 500),\n    'criterion': [\"gini\",\"entropy\"],\n    'max_depth': randint(low = 1, high = 50),\n    'min_samples_leaf': randint( low = 1, high = 25),\n    'max_features': [None, 'auto','sqrt','log2'],\n    'max_leaf_nodes': randint(low =1, high = 15),\n     \n    \n}\n\nforest_rnd = RandomizedSearchCV(forest_clf,param_distributions=params, random_state =42, cv = 3,\n                            scoring ='precision')\nforest_rnd.fit(X_train,y)","e5294a51":"forest_rnd.best_estimator_","427b7231":"forest_clf2 = RandomForestClassifier(max_depth=22, max_features=None, max_leaf_nodes=2,\n                       min_samples_leaf=24, n_estimators=492, random_state=42)\nforest_score = cross_val_score(forest_clf2, X_train, y,\n                              scoring ='precision', cv = 3)\n\nforest_score","850fc457":"forest_clf2.fit(X_train,y)","d68b44aa":"from sklearn.model_selection import  cross_val_predict\n\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score,f1_score\n\ny_train_pred1 = cross_val_predict(forest_clf2, X_train,y, cv = 3)\nconfusion_matrix(y,y_train_pred1)","0aff7998":"print('Precision Score: ',precision_score(y,y_train_pred1))\nprint('Recall_score: ', recall_score(y,y_train_pred1))\nprint('F1 Score: ',f1_score(y,y_train_pred1) )\n","f893e3d7":"from sklearn.linear_model import LogisticRegression\n\nlog_clf = LogisticRegression()","1e79b86d":"params = {\n    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n    'dual': [True,False],\n    'C': randint(low = 0.01, high = 2),\n    'fit_intercept': [True,False],\n    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n    'max_iter': randint(low =1, high = 10000),\n       \n}\n\nrnd_log = RandomizedSearchCV(log_clf, param_distributions = params,\n                            cv = 5, scoring ='precision')\nrnd_log.fit(X_train,y)","a22d6bc1":"rnd_log.best_estimator_","46021e24":"log_clf2 = LogisticRegression(C=1, max_iter=6100, penalty='none')\ny_train_pred2 = cross_val_predict(log_clf2, X_train,y, cv = 3)\nconfusion_matrix(y,y_train_pred2)\n\n","9bba7ed2":"print('Precision score :', precision_score(y,y_train_pred2))\nprecision_score(y,y_train_pred)\nprint(\"Recall_score \", recall_score(y,y_train_pred2))\nrecall_score(y,y_train_pred)\nprint('F1_score: ', f1_score(y,y_train_pred2))","36b5ab31":"test_data.head()","0b5464c2":"test_data.info()","2cf0b5fc":"test_data.info()","8b0ccf81":"test_data.info()","d39ab6af":"# transform the test data\nnew_prediction_pr = preprocessor.transform(test_data)\n","0b490bff":"log_clf2.fit(X_train,y)","5e353c74":"new_prediction_class = log_clf2.predict(new_prediction_pr)","663e8442":"new_prediction_class","6a0925de":"\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': new_prediction_class})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n\n","6bd3255c":"From the above graph, most people use S port to board the titanic. This Value will be use the fill missing values in that Embarked columns","58a38bdb":"Using the describe() methods to have few statistics of the numerical features of the data","22b6966d":"### Train a few model\nI am going to try a few machine learning models to try to solve the problems","17047c66":"## Import important Library","1ef9893c":"Drop the name feature from the dataset","49101eab":"### Logistic Regressor","4581370d":"### Feature Selection\nX : list of predictors\ny : labels","0be03a0f":"### Start working with test data","58584a59":"### Delete unecessary features\nI am going to select a few features that I unncessary so as to not make the model to complexe\n\n1. Name\n2. Ticket\n3. Cabin\n\nThese features will be dropped. Furthermore passengerID will be drop from the training data set\n","1d7a7557":"### Data Visualization steps\nTo better understand the scope of the problems, we are going to plot a few a collumn in the dataset to determine which passengers are more likely to survived","9b8fcd3d":"#### Use a RamdomForestClassifier\nSince my previous model did not work too well with the train_data, I am going to use a new_model","3f59adb1":"* ### Data Tranformation Steps\n\nThe data will be stratified based on the age category","3d2f89c3":"<b>Columbs with missing values: <b> \"Age\",\"Cabin\",\"Embarked\"","ce55ecfb":"### Load the data and make sense of the data","db3ae5cd":"### Confusion matrix\nbelow is the confusion matrix","96d1e254":"#### Stochastic Gradient descent\nNoww let\u2019s pick a classifier and train it. A good place to start is with a Stochastic\nGradient Descent (SGD) classifier, using Scikit-Learn\u2019s SGDClassifier class. This clas\u2010\nsifier has the advantage of being capable of handling very large datasets efficiently.","3411b5fa":"### Data Preprocessing\nwe will deals with missing values in the dataset","5ee740fe":"Let's take a quick look at the dataset","ffc6012d":"By using the df.info() methods we can determine the types attributes of each features and determine missing values","8db5a7bc":"#### Below is the accuracy score for my model","e57bb062":"### Model Validation\nI am going to use different matrics to validate my models","ec002947":"From the above correlation data, Pclass,  Age, SibSp, Parch play have a negative correlation with Survival.","3b9cf199":"# Titanic Disaster Competition\n\n\n## Descritption\nThe RMS Titanic, a luxury steamship, sank in the early hours of April 15, 1912, off the coast of Newfoundland in the North Atlantic after sideswiping an iceberg during its maiden voyage.Of the 2,240 passengers and crew on board, more than 1,500 lost their lives in the disaster.[https:\/\/www.history.com\/topics\/early-20th-century-us\/titanic#:~:text=The%20RMS%20Titanic%2C%20a%20luxury,their%20lives%20in%20the%20disaster.](http:\/\/)\n\n## Problem\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).\n\n"}}