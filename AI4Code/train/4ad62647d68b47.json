{"cell_type":{"e363ccab":"code","efc64ca1":"code","0dc96adc":"code","0d5214af":"code","e8aafff5":"code","74ed8675":"code","0dea3a71":"code","1e98ea2a":"code","cb784497":"code","f2b6f9ea":"code","ef2936d9":"code","e0db065c":"code","4b6a11d0":"code","06e80d05":"code","73a93402":"code","d109d8ab":"code","d700773a":"code","fa492b57":"markdown","18d6287d":"markdown"},"source":{"e363ccab":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","efc64ca1":"#I will use deep learning method and for scaling will use minmaxscaler\nimport tensorflow as tf\nfrom sklearn.preprocessing import MinMaxScaler\n#firt of all i will make the dataset ready to model.\ncolumn_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'N', 'LSTAT', 'MEDV']\nhousing = pd.read_csv(\"\/kaggle\/input\/boston-house-prices\/housing.csv\", header=None, delimiter=r\"\\s+\", names=column_names)\nhousing.head()","0dc96adc":"#dimension of the dataset\nprint(np.shape(housing))","0d5214af":"# summarized statistics of data\nprint(housing.describe())","e8aafff5":"#while prediction of MEDV , the columns ZN and CHAS is not necessary \n#and in the features above 50.00 in MEDV columns are not necessary \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nfig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor k,v in housing.items():\n    sns.boxplot(y=v, ax=axs[index])\n    index += 1\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)","74ed8675":"    for k, v in housing.items():\n        q1 = v.quantile(0.25)\n        q3 = v.quantile(0.75)\n        irq = q3 - q1\n        v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]\n        perc = np.shape(v_col)[0] * 100.0 \/ np.shape(housing)[0]\n        print(\"Column %s outliers = %.2f%%\" % (k, perc))\n    \n#there are outliers in the columns CRIM,ZN,RM and B seemed in the graphs above. \n#lets see the percentages of them","0dea3a71":"#lets remove features of MEDV columns below 50\nhousing = housing[~(housing['MEDV'] >= 50.0)]\nprint(np.shape(housing))","1e98ea2a":"#lets see the new graphs as plot\nfig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor k,v in housing.items():\n    sns.distplot(v, ax=axs[index])\n    index += 1\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)","cb784497":"#lets see the correlation\nplt.figure(figsize=(20, 10))\nsns.heatmap(housing.corr().abs(),  annot=True)","f2b6f9ea":"#in the matrix above it is seemed that TAX and RAD coloumns are highly correlated but LSTAT,INDUS,RM,TAX,NOX,PTRAIO columns have low correlation.\nfrom sklearn import preprocessing\n# Let's scale the columns before plotting them against MEDV\nmin_max_scaler = preprocessing.MinMaxScaler()\ncolumn_sels = ['LSTAT', 'INDUS', 'NOX', 'PTRATIO', 'RM', 'TAX', 'DIS', 'AGE']\nx = housing.loc[:,column_sels]\ny = housing['MEDV']\nx = pd.DataFrame(data=min_max_scaler.fit_transform(x), columns=column_sels)\nfig, axs = plt.subplots(ncols=4, nrows=2, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor i, k in enumerate(column_sels):\n    sns.regplot(y=y, x=x[k], ax=axs[i])\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)","ef2936d9":"#lets remove the skewness of the data\ny =  np.log1p(y)\nfor col in x.columns:\n    if np.abs(x[col].skew()) > 0.3:\n        x[col] = np.log1p(x[col])","e0db065c":"#starting the model. split the train and test datas. after that i will make fit transfer at the same time\nfrom sklearn.model_selection import train_test_split\n\ntrain_test_split(x, y)\nx_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.8, random_state = 20)\nscaler = MinMaxScaler((-1,1))\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)","4b6a11d0":"from sklearn.linear_model import ElasticNet\n\n# we use elastic net. alpha is the total penalty parameter and l1_ratio is the proportion of alpha to be given to l1 norm\nelastic = ElasticNet(alpha = 0.05 , l1_ratio= 0.5, max_iter = 1000) \n\n#fit the model with train data. \nmodel = elastic.fit(x_train,y_train)\n\n#R2 is the default scoring method for linear regression\nr2_train = model.score(x_train,y_train)\nr2_test = model.score(x_test,y_test)\n\nprint(\"R2 Score for train data is \", r2_train)\nprint(\"R2 Score for test data is \", r2_test)\nmodel.coef_ ","06e80d05":"# import ridge from linear model\nfrom sklearn.linear_model import Ridge\n\n# we use ridge instead of linear regression. \nridge = Ridge(alpha = 0.3) \n\n\n#column_sels = ['LSTAT', 'INDUS', 'NOX', 'PTRATIO', 'RM', 'TAX', 'DIS', 'AGE']\n#x = housing.loc[:,column_sels]\n#y = housing['MEDV']\n\n\n#fit the model with train data. \nmodel = ridge.fit(x_train,y_train)\n\n#R2 is the default scoring method for linear regression\nr2_train = model.score(x_train,y_train)\nr2_test = model.score(x_test,y_test)\n\nprint(\"R2 Score for train data is \", r2_train)\nprint(\"R2 Score for test data is \", r2_test)\nmodel.coef_","73a93402":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression() \n#fit the model with train data. \nmodel = lr.fit(x_train,y_train)\n\n#R2 is the default scoring method for linear regression\nr2_train = model.score(x_train,y_train)\nr2_test = model.score(x_test,y_test)\n\nprint(\"R2 Score for train data is \", r2_train)\nprint(\"R2 Score for test data is \", r2_test)\nlinear_coef = model.coef_\nlinear_coef","d109d8ab":"from sklearn.preprocessing import StandardScaler\n\nscaler=StandardScaler()\n\nx_train_transformed = scaler.fit_transform(x_train)","d700773a":"from sklearn.linear_model import SGDRegressor\n#lets first train a linear regression model\n#try: alpha=0, max_iter = 10000\n#predictor = SGDRegressor(alpha=0) \n\npredictor = SGDRegressor(alpha=0, max_iter = 10000) \n\n\n#fit the model with train data. \nmodel = predictor.fit(x_train_transformed,y_train)\n\nx_test_transformed = scaler.transform(x_test)\n\n#R2 is the default scoring method for linear regression\nr2_train = model.score(x_train_transformed,y_train)\nr2_test = model.score(x_test_transformed,y_test)\n\nprint(\"R2 Score for train data is \", r2_train)\nprint(\"R2 Score for test data is \", r2_test)\nmodel.coef_","fa492b57":"The Boston Housing Dataset\n\nThe Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of Boston MA. The following describes the dataset columns:\n\nCRIM - per capita crime rate by town\n\nZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n\nINDUS - proportion of non-retail business acres per town.\n\nCHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n\nNOX - nitric oxides concentration (parts per 10 million)\n\nRM - average number of rooms per dwelling\n\nAGE - proportion of owner-occupied units built prior to 1940\n\nDIS - weighted distances to five Boston employment centres\n\nRAD - index of accessibility to radial highways\n\nTAX - full-value property-tax rate per $10,000\n\nPTRATIO - pupil-teacher ratio by town\n\nN - 1000(N - 0.63)^2 where N is the proportion of Non-Americans by town\n\nLSTAT - % lower status of the population\n\nMEDV - Median value of owner-occupied homes in $1000's\n","18d6287d":"The target is MEDV (Median value of owner-occupied homes)"}}