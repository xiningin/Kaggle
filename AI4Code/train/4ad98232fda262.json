{"cell_type":{"dd74dfd0":"code","f514949c":"code","889b9e4a":"code","c91cd6a4":"code","5c057f0b":"code","7f5ac5f6":"code","366f0750":"code","3b3e8188":"code","a5ea4b6e":"code","a4678b36":"code","6995d095":"code","79aa8377":"code","56c1afb0":"code","38e2f5fd":"code","3264a442":"code","6079be51":"code","7488d438":"code","a0d75b4c":"code","9788caef":"code","7e63a454":"code","cfe178c5":"code","0dd95b1d":"code","87952f76":"code","d25d5dee":"code","692055d8":"code","97201a9d":"code","ba1f23ef":"code","b49312b8":"code","6b37a9a3":"code","fc9cc569":"code","b4a500c0":"code","1ce229ed":"code","71254812":"code","ea6ea587":"code","08fc2678":"code","c9099d1b":"code","c363615f":"code","f893c5e0":"code","aab1b31a":"code","ba5af191":"code","c9cefbd9":"code","ee12e05a":"code","d05c0df3":"code","1301911a":"code","af402cb0":"code","b88459cb":"code","1a35b366":"code","fd47aa75":"code","9b5cbecd":"code","7838e377":"code","1eedeb22":"code","b0a619e9":"code","3c9b631b":"code","7656c6aa":"code","a0980dec":"markdown","0d6452ed":"markdown","29afc603":"markdown","c29bcee3":"markdown","620aab1a":"markdown","f612f5ea":"markdown","146ace3e":"markdown","30048f4a":"markdown","b62aabf8":"markdown","ae1734f1":"markdown","38e45a34":"markdown","18ac331d":"markdown","db7711ab":"markdown","341a0c82":"markdown","020d1e37":"markdown","12d243be":"markdown","616aeb09":"markdown","4ef8ca70":"markdown","22d82ac6":"markdown","0c4b3b96":"markdown","9bf9d277":"markdown","de915a88":"markdown","58512d68":"markdown","956fbf97":"markdown","82c98704":"markdown","70f5ca0b":"markdown","012e6229":"markdown","e3f2bcfb":"markdown","5bb2d8d0":"markdown","def09077":"markdown","2e3b67c2":"markdown","813d5697":"markdown","49029136":"markdown","84a852b2":"markdown","235533aa":"markdown","69fb139e":"markdown","9e3d5e56":"markdown","e808957d":"markdown","19633f87":"markdown","de62c9de":"markdown","54dcef3c":"markdown","8bc4b7d7":"markdown","b50fe4fc":"markdown","60acde01":"markdown","440d20ba":"markdown","ad7fdb06":"markdown","5ad72620":"markdown","6f4169e9":"markdown","f39cc7a2":"markdown","5b617e9d":"markdown","e60e258d":"markdown","f2e12a0e":"markdown","8bc7f23e":"markdown","b84c32c4":"markdown","3bd40a12":"markdown","823e443e":"markdown","a3967626":"markdown","4013a2dc":"markdown","37660f9c":"markdown"},"source":{"dd74dfd0":"import numpy as np\nimport pandas as pd \n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport matplotlib.pylab as plt","f514949c":"from sklearn import metrics\nmetrics.homogeneity_score([0, 0, 1, 1], [1, 1, 0, 0]) \n#en argument : (label_True, label_Pred). Si on \u00e9change label_True et label_Pred, on a la completeness ! \nprint(\"%.3f\" % metrics.homogeneity_score([0, 0, 1, 1], [0, 0, 1, 2]))","889b9e4a":"print(\"%.3f\" % metrics.homogeneity_score([0, 0, 1, 1], [0, 1, 2, 3]))","c91cd6a4":"print(\"%.3f\" % metrics.homogeneity_score([0, 0, 1, 1], [0, 1, 0, 1]))\nprint(\"%.3f\" % metrics.homogeneity_score([0, 0, 1, 1], [0, 0, 0, 0]))\nprint (metrics.completeness_score([0, 0, 1, 1], [1, 1, 0, 0]))\nprint(metrics.completeness_score([0, 0, 1, 1], [0, 0, 0, 0]))\nprint(metrics.completeness_score([0, 1, 2, 3], [0, 0, 1, 1]))\nprint(metrics.completeness_score([0, 0, 1, 1], [0, 1, 0, 1]))\nprint(metrics.completeness_score([0, 0, 0, 0], [0, 1, 2, 3]))","5c057f0b":"print (metrics.v_measure_score([0, 0, 1, 1], [0, 0, 1, 1]))\nprint (metrics.v_measure_score([0, 0, 1, 1], [1, 1, 0, 0]))","7f5ac5f6":"print(\"%.3f\" % metrics.completeness_score([0, 1, 2, 3], [0, 0, 0, 0]))\nprint(\"%.3f\" % metrics.homogeneity_score([0, 1, 2, 3], [0, 0, 0, 0]))\nprint(\"%.3f\" % metrics.v_measure_score([0, 1, 2, 3], [0, 0, 0, 0]))\nprint(\"%.3f\" % metrics.v_measure_score([0, 0, 1, 2], [0, 0, 1, 1]))\nprint(\"%.3f\" % metrics.v_measure_score([0, 1, 2, 3], [0, 0, 1, 1]))","366f0750":"print(\"%.3f\" % metrics.v_measure_score([0, 0, 1, 1], [0, 0, 1, 2]))\nprint(\"%.3f\" % metrics.v_measure_score([0, 0, 1, 1], [0, 1, 2, 3]))","3b3e8188":"print(\"%.3f\" % metrics.v_measure_score([0, 0, 0, 0], [0, 1, 2, 3]))","a5ea4b6e":"print(\"%.3f\" % metrics.v_measure_score([0, 0, 1, 1], [0, 0, 0, 0]))","a4678b36":"import numpy as np\n\n#Create some data\nMAXN=40\nX = np.concatenate([1.25*np.random.randn(MAXN,2), 5+1.5*np.random.randn(MAXN,2)])\nX = np.concatenate([X,[8,3]+1.2*np.random.randn(MAXN,2)]) # np.concatenate joint une s\u00e9quence de tableaux le long d'un axe existant.\nX.shape","6995d095":"#Just for visualization purposes, create the labels of the 3 distributions\ny = np.concatenate([np.ones((MAXN,1)),2*np.ones((MAXN,1))])\ny = np.concatenate([y,3*np.ones((MAXN,1))])\n\nplt.subplot(1,2,1)\nplt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\nplt.title('Data as were generated')","79aa8377":"plt.subplot(1,2,2)\nplt.scatter(X[:,0],X[:,1],color='r')\nplt.title('Data as the algorithm sees them')","56c1afb0":"plt.savefig('\/kaggle\/working.png',dpi=300, bbox_inches='tight')","38e2f5fd":"from sklearn import cluster\n\nK=3 # Assuming to be 3 clusters!\n\nclf = cluster.KMeans(init='random', n_clusters=K)\nclf.fit(X)","3264a442":"print (clf.labels_) # or\nprint (clf.predict(X)) # equivalent","6079be51":"print (X[(y==1).ravel(),0]) #numpy.ravel() retourne un array du meme type que celui en input\nprint (X[(y==1).ravel(),1])\n# On affiche ci-dessous les coordonn\u00e9es des donn\u00e9es (X[y]) pour y=1 (groupe 1).","7488d438":"plt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\n#On affiche une colonne en fonction de l'autre (la premi\u00e8re colonne contient les ordonn\u00e9es et la deuxieme les abcisses) pour les 3 clusters d\u00e9finis\n\nfig = plt.gcf()\nfig.set_size_inches((6,5))\n#On a 3 clusters qui apparaissent en trois couleurs diff\u00e9rentes. ","a0d75b4c":"x = np.linspace(-5,15,200)\nXX,YY = np.meshgrid(x,x) #Cr\u00e9e une matrice en assemblant les vecteurs en argument\nsz=XX.shape\ndata=np.c_[XX.ravel(),YY.ravel()]\n# c_ translates slice objects to concatenation along the second axis.","9788caef":"Z=clf.predict(data) # predict : pr\u00e9voit la classe la plus proche \u00e0 laquelle appartient chaque \u00e9chantillon dans X\nprint (Z)","7e63a454":"# Visualize space partition\nplt.imshow(Z.reshape(sz), interpolation='bilinear', origin='lower',\nextent=(-5,15,-5,15),alpha=0.3, vmin=0, vmax=K-1)\nplt.title('Space partitions', size=14)\nplt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\n\nfig = plt.gcf()\nfig.set_size_inches((6,5))","cfe178c5":"plt.savefig(\"\/kaggle\/working.png\",dpi=300, bbox_inches='tight')","0dd95b1d":"clf = cluster.KMeans(n_clusters=K, random_state=0)\n#initialize the k-means clustering\nclf.fit(X) #run the k-means clustering\n\ndata=np.c_[XX.ravel(),YY.ravel()]\nZ=clf.predict(data) # returns the clustering labels of the data","87952f76":"plt.title('Final result of K-means', size=14)\n\nplt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\n\nplt.imshow(Z.reshape(sz), interpolation='bilinear', origin='lower',\nextent=(-5,15,-5,15),alpha=0.3, vmin=0, vmax=K-1)","d25d5dee":"x = np.linspace(-5,15,200) #cr\u00e9e 200 \u00e9l\u00e9ments r\u00e9guli\u00e8rement \u00e9spac\u00e9s, compris dans l'intervalle [-5;15]\nXX,YY = np.meshgrid(x,x)\nfig = plt.gcf()\nfig.set_size_inches((6,5))","692055d8":"plt.savefig(\"\/kaggle\/working.png\",dpi=300, bbox_inches='tight')","97201a9d":"clf = cluster.KMeans(init='random', n_clusters=K, random_state=0) # les centroids sont choisis au hazard pour la premi\u00e8re initialisation\n#initialize the k-means clustering\nclf.fit(X) #run the k-means clustering\nZx=clf.predict(X)\n\nplt.subplot(1,3,1)\nplt.title('Original labels', size=14)\nplt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b') # b\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g') # g\nfig = plt.gcf()\nfig.set_size_inches((12,3))","ba1f23ef":"plt.subplot(1,3,2)\nplt.title('Data without labels', size=14)\nplt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='r') # b\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='r') # g\nfig = plt.gcf()\nfig.set_size_inches((12,3))","b49312b8":"plt.subplot(1,3,3)\nplt.title('Clustering labels', size=14)\nplt.scatter(X[(Zx==1).ravel(),0],X[(Zx==1).ravel(),1],color='r')\nplt.scatter(X[(Zx==2).ravel(),0],X[(Zx==2).ravel(),1],color='b')\nplt.scatter(X[(Zx==0).ravel(),0],X[(Zx==0).ravel(),1],color='g')\nfig = plt.gcf()\nfig.set_size_inches((12,3))","6b37a9a3":"from sklearn import metrics\n\nclf = cluster.KMeans(n_clusters=K, init='k-means++', random_state=0, # Les centroids sont initialement choisi distants les un des autres ce qui permet de minimiser le crit\u00e8re d'inertie (mesure la coh\u00e9rence interne des clusters)\nmax_iter=300, n_init=10)\n#initialize the k-means clustering\nclf.fit(X) #run the k-means clustering\n\nprint ('Final evaluation of the clustering:')\n\nprint('Inertia: %.2f' % clf.inertia_)\n\nprint('Adjusted_rand_score %.2f' % metrics.adjusted_rand_score(y.ravel(),\nclf.labels_))\n\nprint('Homogeneity %.2f' % metrics.homogeneity_score(y.ravel(),\nclf.labels_))\n\nprint('Completeness %.2f' % metrics.completeness_score(y.ravel(),\nclf.labels_))\n\nprint('V_measure %.2f' % metrics.v_measure_score(y.ravel(), clf.labels_))\n\nprint('Silhouette %.2f' % metrics.silhouette_score(X, clf.labels_,\nmetric='euclidean'))","fc9cc569":"clf1 = cluster.KMeans(n_clusters=K, init='random', random_state=0,\nmax_iter=2, n_init=2)\n#initialize the k-means clustering\nclf1.fit(X) #run the k-means clustering\n\nprint ('Final evaluation of the clustering:')\n\nprint ('Inertia: %.2f' % clf1.inertia_)\n\nprint ('Adjusted_rand_score %.2f' % metrics.adjusted_rand_score(y.ravel(),\nclf1.labels_))\n\nprint ('Homogeneity %.2f' % metrics.homogeneity_score(y.ravel(),\nclf1.labels_))\n\nprint ('Completeness %.2f' % metrics.completeness_score(y.ravel(),\nclf1.labels_))\n\nprint ('V_measure %.2f' % metrics.v_measure_score(y.ravel(),\nclf1.labels_))\n\nprint ('Silhouette %.2f' % metrics.silhouette_score(X, clf1.labels_,\nmetric='euclidean'))","b4a500c0":"#Read and check the dataset downloaded from the EuroStat\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import cluster\n\nedu=pd.read_csv('\/kaggle\/input\/datalab3\/files\/ch07\/educ_figdp_1_Data.csv',na_values=':')\nedu.head()","1ce229ed":"edu.tail()","71254812":"#Pivot table in order to get a nice feature vector representation with dual indexing by TIME and GEO\npivedu=pd.pivot_table(edu, values='Value', index=['TIME', 'GEO'], columns=['INDIC_ED'])\npivedu.head()","ea6ea587":"print ('Let us check the two indices:\\n')\nprint ('\\nPrimary index (TIME): \\n' + str(pivedu.index.levels[0].tolist()))\nprint ('\\nSecondary index (GEO): \\n' + str(pivedu.index.levels[1].tolist()))","08fc2678":"edu2010=pivedu.loc[2010]\nedu2010.head()","c9099d1b":"#Store column names and clear them for better handling. Do the same with countries\nedu2010 = edu2010.rename(index={'Euro area (13 countries)': 'EU13',\n'Euro area (15 countries)': 'EU15',\n'European Union (25 countries)': 'EU25',\n'European Union (27 countries)': 'EU27',\n'Former Yugoslav Republic of Macedonia, the': 'Macedonia',\n'Germany (until 1990 former territory of the FRG)': 'Germany'\n})\nfeatures = edu2010.columns.tolist()\n\ncountries = edu2010.index.tolist()\n\nedu2010.columns=range(12)\nedu2010.head()","c363615f":"#Check what is going on in the NaN data\nnan_countries=np.sum(np.where(edu2010.isnull(),1,0),axis=1) #fait la somme des NaN pour chaque pays\nplt.bar(np.arange(nan_countries.shape[0]),nan_countries)\nplt.xticks(np.arange(nan_countries.shape[0]),countries,rotation=90,horizontalalignment='left',\nfontsize=12)\nfig = plt.gcf()\nfig.set_size_inches((12,5))","f893c5e0":"#Remove non info countries\nwrk_countries = nan_countries<4 #Work_country : quand la somme des NaN est strictement inf\u00e9rieure \u00e0 4\n\neduclean=edu2010.loc[wrk_countries] #educlean est une matrice des donn\u00e9es des informations de chaque \"work_country\" pour l'ann\u00e9e 2010.\n#.loc - Construct an open mesh from multiple sequences.\n\n#Let us check the features we have\nna_features = np.sum(np.where(educlean.isnull(),1,0),axis=0)\nprint (na_features) #Affiche les caract\u00e9ristiques qui ne sont pas renseign\u00e9es.\n\nplt.bar(np.arange(na_features.shape[0]),na_features)\nplt.xticks(fontsize=12)\nfig = plt.gcf()\nfig.set_size_inches((8,4))","aab1b31a":"#Option A fills those features with some value, at risk of extracting wrong information\n#Constant filling : edufill0=educlean.fillna(0)\nedufill=educlean.fillna(educlean.mean())\nprint ('Filled in data shape: ' + str(edufill.shape))\n\n#Option B drops those features\nedudrop=educlean.dropna(axis=1)\n#dropna: Return object with labels on given axis omitted where alternately any or\n# all of the data are missing\nprint ('Drop data shape: ' + str(edudrop.shape))","ba5af191":"scaler = StandardScaler() #Standardize features by removing the mean and scaling to unit variance\n\nX_train_fill = edufill.values\nX_train_fill = scaler.fit_transform(X_train_fill)\n\nclf = cluster.KMeans(init='k-means++', n_clusters=3, random_state=42)\n\nclf.fit(X_train_fill) #Compute k-means clustering.\n\ny_pred_fill = clf.predict(X_train_fill)\n#Predict the closest cluster each sample in X belongs to.\n\nidx=y_pred_fill.argsort()","c9cefbd9":"plt.plot(np.arange(35),y_pred_fill[idx],'ro')\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\n\nplt.xticks(np.arange(len(wrk_countries_names)),[wrk_countries_names[i] for i in idx],\nrotation=90,horizontalalignment='left',fontsize=12)\nplt.title('Using filled in data', size=15)\nplt.yticks([0,1,2])\nfig = plt.gcf()\n\nfig.set_size_inches((12,5))","ee12e05a":"X_train_drop = edudrop.values\nX_train_drop = scaler.fit_transform(X_train_drop)\n\nclf.fit(X_train_drop) #Compute k-means clustering.\ny_pred_drop = clf.predict(X_train_drop) #Predict the closest cluster of each sample in X.","d05c0df3":"idx=y_pred_drop.argsort()\nplt.plot(np.arange(35),y_pred_drop[idx],'ro')\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\n\nplt.xticks(np.arange(len(wrk_countries_names)),[wrk_countries_names[i] for i in idx],\nrotation=90,horizontalalignment='left',fontsize=12)\nplt.title('Using dropped missing values data',size=15)\nfig = plt.gcf()\nplt.yticks([0,1,2])\nfig.set_size_inches((12,5))","1301911a":"# On affiche les clusters pr\u00e9dits d'une magni\u00e8re plus visuelle (m\u00e9thode dropped missing values) :\nplt.plot(y_pred_drop+0.2*np.random.rand(35),y_pred_fill+0.2*np.random.rand(35),'bo')\nplt.xlabel('Predicted clusters for the filled in dataset.')\nplt.ylabel('Predicted clusters for the dropped missing values dataset.')\nplt.title('Correlations')\nplt.xticks([0,1,2])\nplt.yticks([0,1,2])\nplt.savefig(\"\/kaggle\/working.png\",dpi=300, bbox_inches='tight')","af402cb0":"print ('Cluster 0: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_fill)\nif item==0]))\nprint ('Cluster 0: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_drop)\nif item==0]))\nprint ('\\n')\nprint ('Cluster 1: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_fill)\nif item==1]))\nprint ('Cluster 1: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_drop)\nif item==1]))\nprint ('\\n')\nprint ('Cluster 2: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_fill)\nif item==2]))\nprint ('Cluster 2: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_drop)\nif item==2]))\nprint ('\\n')","b88459cb":"width=0.3\np1 = plt.bar(np.arange(8),scaler.inverse_transform(clf.cluster_centers_[1]),width,color='b')\n# Scale back the data to the original representation\np2 = plt.bar(np.arange(8)+width,scaler.inverse_transform(clf.cluster_centers_[2]),\nwidth,color='yellow')\np0 = plt.bar(np.arange(8)+2*width,scaler.inverse_transform(clf.cluster_centers_[0]),\nwidth,color='r')\n\nplt.legend( (p0[0], p1[0], p2[0]), ('Cluster 0', 'Cluster 1', 'Cluster 2') ,loc=9)\nplt.xticks(np.arange(8) + 0.5, np.arange(8),size=12)\nplt.yticks(size=12)\nplt.xlabel('Economical indicators')\nplt.ylabel('Average expanditure')\nfig = plt.gcf()\n\nplt.savefig(\"\/kaggle\/working.png\",dpi=300, bbox_inches='tight')","1a35b366":"from scipy.spatial import distance\np = distance.cdist(X_train_drop[y_pred_drop==0,:],[clf.cluster_centers_[1]],'euclidean')\n#the distance of the elements of cluster 0 to the center of cluster 1\n\nfx = np.vectorize(np.int)\n\nplt.plot(np.arange(p.shape[0]),\nfx(p)\n)\n\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\nzero_countries_names = [wrk_countries_names[i] for i,item in enumerate(y_pred_drop)\nif item==0]\nplt.xticks(np.arange(len(zero_countries_names)),zero_countries_names,rotation=90,\nhorizontalalignment='left',fontsize=12)","fd47aa75":"from scipy.spatial import distance\np = distance.cdist(X_train_drop[y_pred_drop==0,:],[clf.cluster_centers_[1]],'euclidean')\npown = distance.cdist(X_train_drop[y_pred_drop==0,:],[clf.cluster_centers_[0]],'euclidean')\n\nwidth=0.45\np0=plt.plot(np.arange(p.shape[0]),fx(p),width)\np1=plt.plot(np.arange(p.shape[0])+width,fx(pown),width,color = 'red')\n\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\nzero_countries_names = [wrk_countries_names[i] for i,item in enumerate(y_pred_drop)\nif item==0]\nplt.xticks(np.arange(len(zero_countries_names)),zero_countries_names,rotation=90,\nhorizontalalignment='left',fontsize=12)\nplt.legend( (p0[0], p1[0]), ('d -> 1', 'd -> 0') ,loc=1) #On trace la distance p0 des pays du cluster 0 jusqu'au centroid du cluster 0 et la distance p1 des pays du cluster 0 jusqu'au centroid du cluster 1.\nplt.savefig(\"\/kaggle\/working.png\",dpi=300, bbox_inches='tight')","9b5cbecd":"X_train = edudrop.values\nclf = cluster.KMeans(init='k-means++', n_clusters=4, random_state=0) #4 clusters cette fois-ci\nclf.fit(X_train)\ny_pred = clf.predict(X_train)\n\nidx=y_pred.argsort()\nplt.plot(np.arange(35),y_pred[idx],'ro')\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\n\nplt.xticks(np.arange(len(wrk_countries_names)),[wrk_countries_names[i] for i in idx],rotation=90,\nhorizontalalignment='left',fontsize=12)\nplt.title('Using drop features',size=15)\nplt.yticks([0,1,2,3])\nfig = plt.gcf()\nfig.set_size_inches((12,5))","7838e377":"width=0.2\np0 = plt.bar(np.arange(8)+1*width,clf.cluster_centers_[0],width,color='r')\np1 = plt.bar(np.arange(8),clf.cluster_centers_[1],width,color='b')\np2 = plt.bar(np.arange(8)+3*width,clf.cluster_centers_[2],width,color='yellow')\np3 = plt.bar(np.arange(8)+2*width,clf.cluster_centers_[3],width,color='pink')\n\nplt.legend( (p0[0], p1[0], p2[0], p3[0]), ('Cluster 0', 'Cluster 1', 'Cluster 2',\n'Cluster 3') ,loc=9)\nplt.xticks(np.arange(8) + 0.5, np.arange(8),size=12)\nplt.yticks(size=12)\nplt.xlabel('Economical indicator')\nplt.ylabel('Average expenditure')\nfig = plt.gcf()\nfig.set_size_inches((12,5))\nplt.savefig(\"\/kaggle\/working.png\",dpi=300, bbox_inches='tight')","1eedeb22":"print ('Cluster 0: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred) if item==0]))\n\nprint ('Cluster 1: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred) if item==1]))\n\nprint ('Cluster 2: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred) if item==2]))\n\nprint ('Cluster 3: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred) if item==3]))\n\n#Save data for future use.\nimport pickle\nofname = open('edu2010.pkl', 'wb')\ns = pickle.dump([edu2010, wrk_countries_names,y_pred ],ofname)\nofname.close()","b0a619e9":"# M\u00e9thode du spectral clustering :\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nfrom scipy.spatial.distance import pdist\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.metrics import euclidean_distances\n\nX = StandardScaler().fit_transform(edudrop.values)\n\ndistances = euclidean_distances(edudrop.values)\n\nspectral = cluster.SpectralClustering(n_clusters=4, affinity=\"nearest_neighbors\")\nspectral.fit(edudrop.values)\n\ny_pred = spectral.labels_.astype(np.int)","3c9b631b":"idx=y_pred.argsort()\n\nplt.plot(np.arange(35),y_pred[idx],'ro')\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\n\nplt.xticks(np.arange(len(wrk_countries_names)),[wrk_countries_names[i]\nfor i in idx],rotation=90,horizontalalignment='left',fontsize=12)\n\nplt.yticks([0,1,2,3])\n\nplt.title('Applying Spectral Clustering on the drop features',size=15)\nfig = plt.gcf()\nfig.set_size_inches((12,5))","7656c6aa":"# Agglom\u00e9rative clustering :\nX_train = edudrop.values\ndist = pdist(X_train,'euclidean')\nlinkage_matrix = linkage(dist,method = 'complete');\nplt.figure() # we need a tall figure\nfig = plt.gcf()\nfig.set_size_inches((12,12))\ndendrogram(linkage_matrix, orientation=\"right\", color_threshold = 4,labels = wrk_countries_names, leaf_font_size=20);\n\nplt.savefig(\"\/kaggle\/working.png\",dpi=300, bbox_inches='tight')\nplt.show()\n\n#plt.tight_layout() # fixes margins","a0980dec":"### **It looks like cluster 0 (en rouge) ('Cyprus', 'Denmark', 'Iceland', 'Malta', 'Norway', 'Sweden', 'United Kingdom) spends more on education while cluster 1 (Bulgaria', 'Croatia', 'Czech Republic', 'Hungary', 'Italy', 'Japan', 'Latvia', 'Romania', 'Slovakia', 'Spain') is the one with less resources on education. Spain is in the cluster that spend less on education.**","0d6452ed":"**3. CASE STUDY: EUROSTAT data analysis**","29afc603":"### **Question 4: How many \u201cmisclusterings\u201d do we have? Il y a 12 misclustering car dans Z on voit que des \u00e9l\u00e9ments de data sont class\u00e9s dans 3 groupes avec 12 erreurs de points-donn\u00e9es qui ont \u00e9t\u00e9 mits dans un groupe mais qui auraient du \u00eatre mits dans un autre pour coller aux vraies classes**\n","c29bcee3":"There are four features with missing data. At this point we can proceed in two ways:\n\nFill in the features with some non-informative, non-biasing data.\nDrop the features with missing values.\nIf we have many features and only a few have missing values then it is not much harmful to drop them. However, if missing values are spread across the features, we have to eventually deal with them. In our case, both options seem reasonable, so we will proceed with both at the same time.","620aab1a":"Observe that we have ten years information on these indicators, and as expected we have all members of the European Union with some aggregates and control\/reference countries. For the sake of simplicity, let us focus on values on year 2010.","f612f5ea":"### **Commentaire : On a utilis\u00e9 l'algorithme Kmeans pour seulement 2 it\u00e9rations (donc peu) et avec des centroids initials choisis al\u00e9atoirement donc on est dans une configuration non optimale pour pr\u00e9dire les clusters. On remarque cependant que les valeurs ci-dessus sonttr\u00e8s proches de celles de la cellule de code pr\u00e9c\u00e9dante mais traduisent une l\u00e9g\u00e8re moins bonne d\u00e9finition des clusters par l'algorithme. On en conclu que l'efficacit\u00e9 de la m\u00e9thode Kmeans est peu d\u00e9pendante du nombre d'it\u00e9rations et du choix initial des centroids. ","146ace3e":"Un r\u00e9sultat de regroupement satisfait \u00e0 l'exhaustivit\u00e9 si tous les points de donn\u00e9es qui font partie d'une classe donn\u00e9e sont des \u00e9l\u00e9ments du m\u00eame regroupement automatique. Il a alors une valeur proche de 1. Dans le cas inferse, il est rpoche de 0. ","30048f4a":"Let us visualise the result of the K-means clustering:","b62aabf8":"Ci-dessus, on a r\u00e9partit les datas en 3 clusters d\u00e9sign\u00e9s respectivement par les nombres 0,1 et 2. Chaque donn\u00e9e, est associ\u00e9e \u00e0 l'un de ces cluster","ae1734f1":"Le Silhouette Coefficient est calcul\u00e9 en utilisant la distance moyenne intra-groupe (a) et la distance moyenne au plus proche groupe (b) pour chaque \u00e9chantillon.","38e45a34":"Un regroupement parfaitement homog\u00e8ne est un regroupement o\u00f9 chaque cluster a des points de donn\u00e9es appartenant au m\u00eame label de classe. L'homog\u00e9n\u00e9it\u00e9 d\u00e9crit la proximit\u00e9 de l'algorithme de mise en cluster \u00e0 cette perfection.","18ac331d":"As we can observe, this is not a clean data set, there are missing values. Some countries may not collect or have access to some indicators and there are countries without any indicators. Let us display this effect.","db7711ab":"### **Commentaire : ci-dessus, ce sont les clusters pr\u00e9dits par l'algorithme Kmeans. On remarque que quelques \u00e9l\u00e9ments ne sont pas bien class\u00e9s (pour les clusters 2 et 3 uniquement).**","341a0c82":"### **Question 2: Labelings that have pure clusters with members coming from the same classes are homogeneous but un-necessary splits harm to the completeness and thus penalise V-measure as well**","020d1e37":"### **Question 1 : Labelings that assign all classes members to the same clusters are: complete, but not homogeneous**","12d243be":"Let us clean and store the names of the features and the countries.","616aeb09":"### **Commentaire : La caract\u00e9ristique 0 n'est pas renseign\u00e9e pour 1 pays, la caract\u00e9ristique 2 n'est pas renseign\u00e9e pour 3 pays, etc...**","4ef8ca70":"Let us reshape the table into a feature vector style data set.\n\nTo the process of reshaping stacked data into a table is sometimes called pivoting.","22d82ac6":"The K-means algorithm aims to choose centroids minimising a criterion known as the inertia or within-cluster sum-of-squares.\nthe within-cluster sum of squares criterion, can be recognised as a measure of how internally coherent clusters are.\nThe computation is often done several times, with different initializations of the centroids. One method to help address this issue is the k-means++ initialization scheme (use the init=\u2019kmeans++\u2019 parameter). This initialises the centroids to be (generally) distant from each other, leading to provably better results than random initialisation. Some seeds can result in poor convergence rate, or convergence to sub-optimal clusterings\n\nInertia is not a normalized metric: we just know that lower values are better and zero is optimal.","0c4b3b96":"Let us check the profile of the clusters by looking at the centroids:","9bf9d277":"### **On conclue que cet agglomerative clustering am\u00e8ne \u00e0 la meme d\u00e9finition des clusters qu'avec la m\u00e9thode K-means. Pour ce cas-ci, les deux m\u00e9thodes ont des performances \u00e9quivalentes. **","de915a88":"Let us apply the clustering on the dataset with dropped missing values:","58512d68":"### **Commentaire : On a utilis\u00e9 l'algorithme Kmeans pour 300 it\u00e9rations (donc beaucoup) et avec des centroids choisis initialement \u00e9loign\u00e9s les une des autres donc on est dans la configuration optimale pour pr\u00e9dire au mieux les clusters. La valeur d'intertie est relativement \u00e9lev\u00e9e (elle n'est pas normalis\u00e9e donc on ne peut pas en dire plus) donc les \u00e9l\u00e9ments de chaque clusters sont relativement \u00e9loign\u00e9s les uns des autres. Le adjusted_rand_score est proche de 1 (sans y etre \u00e9gal) donc les regroupements pr\u00e9dits sont proches des regroupements r\u00e9els. Les clusters pr\u00e9dits sont relativement mais pas totalement homog\u00e8nes et complets d'ou un v_mesure proche de 0,84 (plus proche de 1 que de 0). Le coefficient de silhouette est de 0,58 donc cela veut dire que des points de X sont proches de la limite des clusters d\u00e9finis. **","956fbf97":"Let us refine a little bit more cluster \u201c0\u201c and check how close are members from this cluster to cluster \u201c1\u201c. This may give us a hint on a possible ordering.","82c98704":"### **On peut d\u00e9ja remarquer que des \u00e9l\u00e9ments du nuage de point bleu sont \u00e9loign\u00e9s du centroid du \"groupe bleu\" (qui prend la moyenne des points) et sont plus proche du centroid vert. On se doute donc que l'algorithme va faire des erreurs pour ces points la.\n### Idem pour les quelques points verts qui sont plus proche du centre du nuage de point bleu que celui du nuage de point vert. **","70f5ca0b":"### **On voit ici encore que selon la m\u00e9thode (\"Fill in the features with some non-informative, non-biasing data\" or\n### \"Drop the features with missing values\", certains pays ne sont pas class\u00e9s dans les m\u00eame clusters.**","012e6229":"### **Commentaire sur les r\u00e9sultats : Dans ces deux exemples, v_mesure_score vaut un car les groupes de label_pred reprenne exactement les \u00e9l\u00e9ments des m\u00eame classes de label_True. Dans chaque groupe de label_pred, on a donc que des \u00e9l\u00e9ments de la m\u00eame classe (homog\u00e9n\u00e9it\u00e9 parfaite) et tous les \u00e9l\u00e9ments d'un classe de label_true sont tous dans une classe de label_pred (completensess totale)**","e3f2bcfb":"Si la Silhouette s(i) est proche de 0, cela signifie que l'\u00e9chantillon se trouve \u00e0 la limite de son cluster et est le plus proche du reste des clusters de l'ensemble de donn\u00e9es.\nUne valeur n\u00e9gative signifie que l'\u00e9chantillon est plus proche du cluster voisin.\nLa moyenne des coefficients de silhouette de tous les \u00e9chantillons d'un cluster donn\u00e9 d\u00e9finit la \"qualit\u00e9\" du cluster.\nLa moyenne des coefficients de silhouette de tous les groupes donne une id\u00e9e de la qualit\u00e9 du r\u00e9sultat de la mise en clusters.","5bb2d8d0":"### **the distance of the elements of cluster 0 to the center of cluster 1 is high for Danemark and Iceland. Those 2 countires are the ones that spend the more on education in their cluster. **","def09077":"Un r\u00e9sultat de clustering satisfait \u00e0 l'homog\u00e9n\u00e9it\u00e9 si tous ses clusters ne contiennent que des points de donn\u00e9es qui sont membres de la m\u00eame classe originale (une seule).\nIl est proche de 1 s'il y a une forte homog\u00e9n\u00e9it\u00e9, proche de 0 dans le cas \u00e9ch\u00e9ant. ","2e3b67c2":"Le rand index calcule une mesure de similarit\u00e9 entre deux groupements en consid\u00e9rant toutes les paires d'\u00e9chantillons et en comptant les paires qui sont assign\u00e9es dans le m\u00eame groupement ou dans des groupements diff\u00e9rents dans les groupements pr\u00e9vus et r\u00e9els.\n\nLe rand index ajust\u00e9 est ainsi assur\u00e9 d'avoir une valeur proche de 0,0 pour l'\u00e9tiquetage al\u00e9atoire ind\u00e9pendamment du nombre de clusters et d'\u00e9chantillons et exactement 1,0 lorsque les clusters sont identiques .\n","813d5697":"Well, looking at both methods, both may yield the same results, but not necessarily always. This is mainly due to two aspects: the random initialisation of the k-means clustering and the fact that each method works in a different space (dropped data vs. filled-in data).\n\nLet us check the list of countries in both methods. Note that we should not consider the cluster value, since it is irrelevant.","49029136":"On a trac\u00e9 la distance p0 des pays du cluster 0 jusqu'au centroid du cluster 0 et la distance p1 des pays du cluster 0 jusqu'au centroid du cluster 1. La distance p0 est logiquement inf\u00e9rieur, le cluster 0 a bien \u00e9t\u00e9 d\u00e9fini. Norway et Sweden sont les pays les plus proches du cluster 1 donc ceux qui d\u00e9pensent le moins pour l'\u00e9ducation du cluster 0, contrairement au Danemark et \u00e0 l'Iceland qui sont ceux qui d\u00e9pensent le plus.","84a852b2":"### **Commentaire : v_mesure_score vaut z\u00e9ro car en r\u00e9alit\u00e9, tous les \u00e9l\u00e9ments viennt de la m\u00eame classe or label_pred montre que l'algorithme de clustering les a tous classer dans des groupes diff\u00e9rents donc tous ces groupes sont totalement homog\u00e8nes mais aucun n'est complet. **","235533aa":"### **Commentaire sur les r\u00e9sultats : on remarque qu'en \u00e9changeant les deux arguments de completeness_score(label-true,label_pred) on obtient le homogeneity_score.\n### **v_mesure_score vaut 0,8 dans le 4eme exemple de ce code car on a homogeneit\u00e9 et completeness de la classe O de label_pred, on a completeness de la class 1 mais on n'a pas homog\u00e9n\u00e9it\u00e9 de la classe 1 car il y a des \u00e9l\u00e9ments de deux classes diff\u00e9rentes de label_true (les classes 2 et 3).**","69fb139e":"### **Question 5: Shall the centroids belong to the original set of points? Pas necessairement car un centroid est un point qui sont calcul\u00e9 en faisant la moyenne de tous les \u00e9chantillons (points) d'un m\u00eame cluster. Cette moyenne ne correspond pas forc\u00e9ment \u00e0 un point du set of samples.   **","9e3d5e56":"Let us redo the clustering with K=4 and see what we can conclude.","e808957d":"### **Commentaire : On affiche le Z contenant les clusters pr\u00e9dits et les points de donn\u00e9es r\u00e9partis dans les 3 cluster r\u00e9els de la m\u00eame facon que d\u00e9crite pr\u00e9cisement. On a utilis\u00e9 cette fois-ci l'algorithme de Kmeans qui ne semble pour le moment pas plus efficiente. **","19633f87":"Well, it seems that Spain belongs to cluster \u201c0\u201c, it is the closest to change to a policy in the lines of the other clusters.\n\nAdditionally, we can also check the distance to the centroid of cluster \u201c0\u201c.","de62c9de":"In scikit-learn, the parameter color_threshold colors all the descendent links below a cluster node k the same color if k is the first node below the color threshold. All links connecting nodes with distances greater than or equal to the threshold are colored blue. Thus, if we use color threshold = 3, the obtained clusters are as follows:\n\nCluster 0: [\u2018Cyprus\u2019, \u2018Denmark\u2019, \u2018Iceland\u2019]\nCluster 1: [\u2018Bulgaria\u2019, \u2018Croatia\u2019, \u2018Czech Republic\u2019, \u2018Italy\u2019,\n\u2018Japan\u2019, \u2018Romania\u2019, \u2018Slovakia\u2019]\nCluster 2: [\u2018Belgium\u2019, \u2018Finland\u2019, \u2018Ireland\u2019, \u2018Malta\u2019, \u2018Norway\u2019,\n\u2018Sweden\u2019]\nCluster 3: [\u2018Austria\u2019, \u2018Estonia\u2019, \u2018EU13\u2019, \u2018EU15\u2019, \u2018EU25\u2019, \u2018EU27\u2019,\n\u2018France\u2019, \u2018Germany\u2019, \u2018Hungary\u2019, \u2018Latvia\u2019, \u2018Lithuania\u2019, \u2018Netherlands\u2019,\n\u2018Poland\u2019, \u2018Portugal\u2019, \u2018Slovenia\u2019, \u2018Spain\u2019, \u2018Switzerland\u2019, \u2018United\nKingdom\u2019, \u2018United States\u2019]\n\nNote that they correspond in high degree to the clusters obtained by the K-means (except permutation of clusters labels that is irrelevant). The figure shows the construction of the clusters using the complete linkage agglomerative clustering. Different cuts at different levels of the dendrogram allow to obtain different number of clusters. As a summary, let us compare the results of the three approaches of clustering. We cannot expect that the results coincide since different approaches are based on different criteria to construct the clusters. Still, we can observe that in this case K-means and the agglomerative approaches gave the same results (up to a permutation of the number of cluster that is irrelevant), meanwhile the spectral clustering gave more evenly distributed clusters. It fused cluster 0 and 2 of the agglomerative clustering in cluster 1, and split cluster 3 of agglomerative clustering in clusters 0 and 3 of it. Note that these results can change when using different distance between data.","54dcef3c":"### **Commentaire par rapport aux r\u00e9sultats : ([0, 0, 0, 0], [0, 1, 2, 3]) est un bon exemple pour comprendre. L'homog\u00e9n\u00e9it\u00e9 est satisfaite totalement (homogeneity_score=1) car chaque cluster pr\u00e9dit contient uniquement des points de donn\u00e9es appartement \u00e0 la m\u00eame classe. En revache, la completeness est nulle (completeness_score=0)car les points de donn\u00e9es sont tous dans la m\u00eame classe (O) (voir label_True) hors dans label_Pred, on les met chaqun dans un cluster diff\u00e9rent donc chaque cluster est incomplet, c'est \u00e0 dire qu'il ne contient pas tous les \u00e9l\u00e9ments de m\u00eame classe qu'il existe.**","8bc4b7d7":"We have sorted the data for better visualization. At a simple glance we can see that both partitions can be different. We can better check this effect plotting the clusters values of one technique against the other.","b50fe4fc":"Note that in general, the spectral clustering intends to obtain more balanced clusters. In this way, the predicted cluster 1 merges the cluster 2 and 3 of the K-means clustering, cluster 2 corresponds to the cluster 1 of the K-means clustering, cluster 0 mainly goes to cluster 2, and clusters 3 corresponds to cluster 0 of the K-means.\n\nApplying the agglomerative clustering, we obtain not only the different clusters, but also we can see how different clusters are obtained. This, in some way it is giving us information on which are the pairs of countries and clusters that are most similar. The corresponding code that applies the agglomerative clustering is:","60acde01":"### **Avec la m\u00e9thode K-means, et la m\u00e9thode \"filled in data\" pour enlever eventuellement les Nan, on a pr\u00e9dit 3 groupes de pays. Ils sont group\u00e9s en fonction de leurs investissements dans l'\u00e9ducation. **","440d20ba":"Let us now apply a K-means clustering technique on this data in order to partition the countries according to their investment in education and check their profiles.","ad7fdb06":"### **En accord avec ce qui a \u00e9t\u00e9 d\u00e9duit pr\u00e9c\u00e9dement, le Danemark, l'Island et Chypre sont les 3 pays qui d\u00e9pensent le plus en \u00e9ducation.**","5ad72620":"We do not have info on Albania, Macedonia and Greece. And very limited info from Liechtenstein, Luxembourg and Turkey. So let us work without them. Now let us check the features.","6f4169e9":"Chaque algorithme de regroupement se d\u00e9cline en deux variantes :\n\nune classe, qui met en \u0153uvre la m\u00e9thode d'ajustement pour connaitre les donn\u00e9es,\n\nune fonction de pr\u00e9diction qui, \u00e0 partir de donn\u00e9es de test, renvoie un tableau de labels entiers correspondant aux diff\u00e9rents clusters.","f39cc7a2":"### **Le clustering en enlevant les caract\u00e9ristiques non renseign\u00e9es (m\u00e9thode \"dropped missing values data\") donne lieu \u00e0 3 groupes mais les pays de chaque groupe et le nombre de pays dans chaque groupe n'est pas exactement similaire au clustering obtenu pr\u00e9c\u00e9dement, sans avoir enlev\u00e9 ces missing values.**","5b617e9d":"### **Commentaire : Ici c'est le cas inverse de la ligne pr\u00e9c\u00e9dante : v_mesure_score vaut z\u00e9ro car l'algorithme a assur\u00e9 une completeness totale mais une homog\u00e9n\u00e9it\u00e9 nulle pour le cluster de label_Pred **","e60e258d":"We can repeat the process using the alternative clustering techniques and compare their results. Let us first apply the spectral clustering. The corresponding code will be:","f2e12a0e":"### **Question 3: Clusters that include samples from totally different classes totally destroy the  harmonic mean between homogeneity and completeness (v-measure) of the labelling, hence:**","8bc7f23e":"Spain is still in cluster \u201c0\u201c. But as we observed in our previous clustering it was very close to changing cluster. This time cluster \u201c0\u201c includes the averages values for the EU members. Just for the sake of completeness, let us write down the name of the countries in the clusters.","b84c32c4":"### **Avec la 1ere methode pour g\u00e9rer les Nan features, on a enlev\u00e9 les 4 features pour lesquelles on avait au moins un Nan. \n### Avec la 2eme m\u00e9thode, on n'a enlev\u00e9 aucune caract\u00e9ristique, en jugeant que toutes les caract\u00e9ristiques etaient renseign\u00e9es pour suffisament de pays. **","3bd40a12":"V-measure :  est la moyenne harmonique entre l'homog\u00e9n\u00e9it\u00e9 et l'exhaustivit\u00e9\nSi les membres de la classe sont compl\u00e8tement r\u00e9partis entre diff\u00e9rents groupes, l'attribution est totalement incompl\u00e8te, d'o\u00f9 la mesure V nulle.\n\nLes labellisations parfaites sont \u00e0 la fois homog\u00e8nes et compl\u00e8tes, d'o\u00f9 un score de 1,0","823e443e":"The K-means algorithm clusters data by trying to separate samples in n groups of equal variance. In other words, the K-means algorithm divides a set of N samples X into K disjoint clusters C, each described by the mean of the samples in the cluster. The means are commonly called the cluster \u201ccentroids\u201d.","a3967626":"Il existe deux grandes familles de techniques de regroupement :\n\nLes algorithmes partiels : Commencez par une partition al\u00e9atoire et affinez-la de fa\u00e7on it\u00e9rative.\nLes algorithmes hi\u00e9rarchiques : Agglom\u00e9ratifs (ascendants), descendants.\n\nAlgorithmes partiels. Ils peuvent \u00eatre divis\u00e9s en deux branches :\nLes algorithmes de partition dure, tels que les K-means, attribuent une valeur de regroupement unique \u00e0 chaque \u00e9l\u00e9ment dans l'espace de caract\u00e9ristiques.\nLes algorithmes de partitionnement doux, tels que le m\u00e9lange de gaussiens,assignent une confiance ou une probabilit\u00e9 \u00e0 chaque point de l'espace.\n\nAlgorithme de K-means :\n1. Initialise la valeur K des clusters souhait\u00e9s.\n2. Initialise les K clusters centres de facon al\u00e9atoire.\n3. D\u00e9cide de l'appartenance \u00e0 une classe des N \u00e9chantillons de donn\u00e9es en les assignant au plus proche cluster centroids (par exemple, le centre de gravit\u00e9 ou la moyenne).\n4. Re-estime les K cluster centres, en supposant que les adh\u00e9sions trouv\u00e9es ci-dessus sont correctes.\n5. Si aucun des N objets n'a chang\u00e9 d'appartenance lors de la derni\u00e8re it\u00e9ration, exit. Sinon, retourner \u00e0 3.\n","4013a2dc":"L'apprentissage non supervis\u00e9, dans lequel les donn\u00e9es de formation consistent en un ensemble de vecteurs d'entr\u00e9e x sans aucune valeur cible correspondante. Le but de ces probl\u00e8mes peut \u00eatre de d\u00e9couvrir des groupes dans les donn\u00e9es, ce que l'on appelle le clustering, ou de d\u00e9terminer la distribution des donn\u00e9es dans l'espace d'entr\u00e9e, ce que l'on appelle l'estimation de la densit\u00e9, ou de projeter les donn\u00e9es d'un espace \u00e0 haute dimension vers deux ou trois dimensions \u00e0 des fins de visualisation.","37660f9c":"### **Commentaire : Le fond color\u00e9 en trois couleurs correspond au 3 clusters cr\u00e9\u00e9s dans Z qui contient les affectations des \u00e9l\u00e9ments dans 3 classes pr\u00e9dites par l'algorithme, les points rouges, bleu et verts correspondent aux vrais clusters qu'on cherche a approximer. On remarque donc que dans Z, certains points de donn\u00e9es r\u00e9els se retrouve dans une mauvaise classe pr\u00e9dite.**"}}