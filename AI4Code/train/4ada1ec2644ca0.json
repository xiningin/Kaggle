{"cell_type":{"55929d69":"code","c516af5d":"code","587adfe2":"code","3e557cf6":"code","b61fbd54":"code","2899608b":"code","931f26f2":"code","1e998da6":"code","119f5272":"markdown","6cbd0f17":"markdown","d6cd558f":"markdown","cd6bc61e":"markdown","faf6608a":"markdown","adc525a6":"markdown","c2a1c901":"markdown"},"source":{"55929d69":"##### PACKAGES\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torchvision\nfrom torch.utils.data import Dataset, DataLoader\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport cv2\n\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","c516af5d":"##### PARAMS\n\ndevice      = torch.device('cpu') \nnum_workers = 4\nbatch_size  = 64\nimage_size  = 380\ndata_path   = '..\/input\/petfinder-pawpularity-score\/'","587adfe2":"##### DATA IMPORT\n\ndf         = pd.read_csv(data_path + 'train.csv')\ndf['path'] = df['Id'].apply(lambda x: '{}train\/{}.jpg'.format(data_path, x))\ndf.head()","3e557cf6":"##### DATASET\n\nclass ImageData(Dataset):\n       \n    def __init__(self, \n                 df, \n                 transform = None):\n        self.df        = df\n        self.transform = transform\n\n        \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        \n        # import image\n        path  = self.df.loc[idx, 'path']\n        image = cv2.imread(path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        # scale to [0, 1]\n        image = np.array(image) \/ 255.0\n        \n        # augmentations\n        if self.transform is not None:\n            image = self.transform(image = image)['image']\n                                \n        # output\n        return image    ","b61fbd54":"##### AUGMENTATIONS\n\naugs = A.Compose([A.Resize(height  = image_size, \n                           width   = image_size),\n                  ToTensorV2()])","2899608b":"##### CHECK SAMPLE BATCH\n\n# data loader\nimage_dataset = ImageData(df = df, transform = augs)\nimage_loader = DataLoader(image_dataset, \n                          batch_size  = batch_size, \n                          shuffle     = False, \n                          num_workers = num_workers)\n\n# display images\nfor batch_idx, inputs in enumerate(image_loader):\n    fig = plt.figure(figsize = (16, 8))\n    for i in range(5):\n        ax = fig.add_subplot(2, 5, i + 1, xticks = [], yticks = [])     \n        plt.imshow(inputs[i].numpy().transpose(1, 2, 0))\n    break","931f26f2":"##### COMPUTE PIXEL SUM AND SQUARED SUM\n\n# placeholders\npsum    = torch.tensor([0.0, 0.0, 0.0])\npsum_sq = torch.tensor([0.0, 0.0, 0.0])\n\n# loop through images\nfor inputs in tqdm(image_loader):\n    psum    += inputs.sum(axis        = [0, 2, 3])\n    psum_sq += (inputs ** 2).sum(axis = [0, 2, 3])","1e998da6":"##### FINAL CALCULATIONS\n\n# pixel count\ncount = len(df) * image_size * image_size\n\n# mean and STD\ntotal_mean = psum \/ count\ntotal_var  = (psum_sq \/ count) - (total_mean ** 2)\ntotal_std  = torch.sqrt(total_var)\n\n# output\nprint('Training data stats:')\nprint('- mean:', total_mean.numpy())\nprint('- std: ', total_std.numpy())","119f5272":"# PREPARATIONS\n\nFirst, we import some libraries and specify a few parameters. No need to use GPU because there is no modeling involved.","6cbd0f17":"Our augmentation pipeline only uses `A.Resize()` to resize the images.","d6cd558f":"# DATA PREP\n\nLet's set up a Dataset and a DataLoader.","cd6bc61e":"# CALCULATIONS\n\nThe computation is done in three steps:\n\n1. Define placeholders to store two batch-level stats: sum and squared sum of pixel values. The first will be used to compute means, and the latter will be needed for standard deviation calculations.\n2. Loop through the batches and add up channel-specific sum and squared sum values.\n3. Perform final calculations to obtain data-level mean and standard deviation.","faf6608a":"Finally, we make some further calculations:\n\n- mean: simply divide the sum of pixel values by the total count - number of pixels in the dataset computed as `len(df) * image_size * image_size`\n- standard deviation: use the following equation: `total_std = sqrt(psum_sq \/ count - total_mean ** 2)`\n\nThe formula for STD uses the sum of squares to perform calculations. [Click here](https:\/\/www.thoughtco.com\/sum-of-squares-formula-shortcut-3126266) if you want to see some details.\n\n![variance equation](https:\/\/kozodoi.me\/images\/copied_from_nb\/images\/fig_variance.jpg)","adc525a6":"Note that we divide the original pixel values in `[0, 255]` by 255 to scale them to `[0, 1]`. This will affect the mean and STD calculations.","c2a1c901":"# SUMMARY\n\nThis notebook caluclates mean and standard deviation of training images. Knowing mean and STD may be helpful for normalizing images within the augmentation pipeline. While computing mean is easy (we can simply average means over batches), standard deviation is a bit more tricky: averaging STDs across batches is not the same as calculating the overall STD. Let's do it properly!\n\nNote: the pipeline comes from [this notebook](https:\/\/www.kaggle.com\/kozodoi\/computing-dataset-mean-and-std).\n\n\n### TL;DR\n\n- mean: `[0.5183  0.4835 0.4457]`\n- std: `[0.2681 0.2638 0.2658]`"}}