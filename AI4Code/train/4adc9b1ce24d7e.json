{"cell_type":{"d894016b":"code","8e915094":"code","8fda37cd":"code","e6a1e03d":"code","91229046":"code","4e83e354":"code","99af6756":"code","25f1b3ff":"code","c474a922":"code","c00498c3":"code","31d6750e":"code","62c7893b":"code","a34c1eee":"code","b6b0321c":"code","1fd297b5":"code","16e29892":"code","087e80fd":"code","07f691b9":"code","af971ab1":"code","bcf4a15d":"code","782e7c1a":"code","4fc5a7cc":"code","a287161a":"code","299dc44b":"code","d716432a":"code","b0b62e3f":"code","3b92bc8b":"code","69cd9289":"code","569490f5":"code","269303ea":"code","3dbe4d38":"code","779642d1":"code","939958cf":"code","46145aa6":"markdown","2057a125":"markdown","a36369f3":"markdown","91973060":"markdown","8ba96e11":"markdown","769c3971":"markdown","f318b03c":"markdown","0edb390b":"markdown","6d1b68d4":"markdown","43a17bb1":"markdown","48d465e7":"markdown","7dce20e1":"markdown","5012032c":"markdown","4c0f9396":"markdown","73fc61ff":"markdown","4027e17a":"markdown","da3c8719":"markdown","d08ca7fb":"markdown","58b0547e":"markdown","afd69022":"markdown","31e48641":"markdown","13c57bdb":"markdown","cbc9a734":"markdown","797a38e1":"markdown","341f1850":"markdown","0890e3ec":"markdown","4bf8bb13":"markdown","97c5589c":"markdown","5a881b1e":"markdown","285e82fd":"markdown","f82ebe00":"markdown","c8c555e5":"markdown","3cc96eed":"markdown","61a03be4":"markdown","c3c23c6f":"markdown","d575b204":"markdown","49f86c6e":"markdown","e9c31f3e":"markdown","f246836d":"markdown","9bc89d7d":"markdown"},"source":{"d894016b":"# General libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\nimport seaborn as sns\n\n# Libraries for data cleaning\nimport unidecode\nimport re\nimport string\n!pip install pycontractions\nfrom pycontractions import Contractions\nimport gensim.downloader as api\n# Choose model accordingly for contractions function\nmodel = api.load(\"glove-twitter-25\")\n# model = api.load(\"glove-twitter-100\")\n# model = api.load(\"word2vec-google-news-300\")\ncont = Contractions(kv_model=model)\ncont.load_models()\nimport operator\n\n# NLP libraries\nimport spacy\nfrom spacy.lang.en import English\n!pip install pyspellchecker\nfrom spellchecker import SpellChecker\nimport nltk\nfrom nltk.corpus import stopwords\n\n# ML libraries\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\n# NN libraries\nfrom keras.models import Sequential\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.models import Model\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint","8e915094":"# List of files (including output files)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8fda37cd":"sample_submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\n\nprint(\"Train dataset size: \", len(train))\nprint(\"Test dataset size: \", len(test))\n\n# A brief look on the train dataset\ntrain[90:100]","e6a1e03d":"# Join  train and test datasets to analyze all data in single commands\nall_data = pd.concat([train,test], axis = 0, sort=False)\n\n# How many locations\/keywords?\nprint(\"Number of unique locations: \", all_data.location.nunique())\nprint(\"Number of unique keywords: \",all_data.keyword.nunique())\n\n# Check data balance; frequency of real disaster tweets (target=1)\nprint(\"Percentage of real disaster tweets: \", all_data.target.sum()\/len(train)*100)\n\n# Find missing values in a single code line,\nprint(\"Missing values:\")\ndisplay(all_data.isna().sum())\n\nax = sns.barplot(train['target'].value_counts().index,train['target'].value_counts()\/len(train))\nax.set_title(\"Real vs fake tweets\")\nax.set_ylabel(\"Percentage\")\nax.set_xlabel(\"Target\")","91229046":"f1, axes = plt.subplots(1, 2, figsize=(15,5))\nf1.subplots_adjust(hspace=0.4, wspace=0.3)\nax1 = sns.barplot(y=train['keyword'].value_counts()[:15].index,x=train['keyword'].value_counts()[:15],\n            orient='h', ax=axes[0])\nax1.set_title(\"Top 15 keywords\")\nax2 = sns.barplot(y=train['location'].value_counts()[:15].index,x=train['location'].value_counts()[:15],\n            orient='h', ax=axes[1])\nax2.set_title(\"Top 15 locations\")","4e83e354":"# Fill missings for train and test datasets\nall_data.location.fillna(\"None\", inplace=True)\nall_data.keyword.fillna(\"None\", inplace=True)\n\n# Fill the target column for the test rows. This will help us with future calls to model predictions\nall_data.target.fillna(0, inplace=True)","99af6756":"# Define the vectorizer counter that will compute the BOW\ndef count_vector(data):\n    count_vectorizer = CountVectorizer()\n    emb = count_vectorizer.fit_transform(data)\n    return emb, count_vectorizer\n\n\n# Define a function for the logistic regression algorithm\ndef logreg_bow(data, valid_fraction):\n    \n    # Transform data to list\n    list_corpus = data[\"text\"].tolist()\n    list_labels = data[\"target\"].tolist()\n\n    # Split train-validation data\n    X_train, X_valid, y_train, y_valid = train_test_split(list_corpus, list_labels, \n                                                          test_size=valid_fraction, random_state=21)\n\n    # Generate the bag of words through the count_vectorizer function\n    X_train_counts, count_vectorizer = count_vector(X_train)\n    X_valid_counts = count_vectorizer.transform(X_valid)\n\n    # Run LogisticRegression model\n    clf = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', \n                             multi_class='multinomial', n_jobs=-1, random_state=21)\n    clf.fit(X_train_counts, y_train)\n    y_predicted_counts = clf.predict(X_valid_counts)\n    \n    # Cross validation score over 10 folds\n    scores = cross_val_score(clf, X_train_counts, y_train, cv=10)\n    print(\"Cross validation over 10 folds: \", scores, \" --- \", sum(scores)\/10.)\n\n    return y_predicted_counts, y_valid\n\n\n# Define a metrics function named get_metrics to evaluate the model's performance\ndef get_metrics(y_test, y_predicted):  \n    # true positives \/ (true positives+false positives)\n    precision = precision_score(y_test, y_predicted, pos_label=None,\n                                    average='weighted')             \n    # true positives \/ (true positives + false negatives)\n    recall = recall_score(y_test, y_predicted, pos_label=None,\n                              average='weighted')\n    \n    # harmonic mean of precision and recall\n    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n    \n    # true positives + true negatives\/ total\n    accuracy = accuracy_score(y_test, y_predicted)\n    return accuracy, precision, recall, f1\n\n\n# Final call of the logistic regression model and metrics\ny_predicted_logistic, y_val = logreg_bow(all_data[:len(train)], 0.2)\naccuracy, precision, recall, f1 = get_metrics(y_val, y_predicted_logistic)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","25f1b3ff":"def submit_logistic(outfile_name, data, test, valid_frac):\n    \n    # Run the LogisticRegression model\n    y_pred, y_valid = logreg_bow(data, valid_frac)\n    \n    # Submit results\n    submission = pd.DataFrame({\n            \"id\": test.id, \n            \"target\": y_pred\n        })\n    submission.to_csv(outfile_name, index=False)\n    \n    return y_pred, y_valid\n    \ny_logistic_pred, y_logistic_test = submit_logistic(\"submission_logistic_basic.csv\", all_data, test, len(test)\/len(all_data))","c474a922":"# Convert text column to lowercase\nall_data2 = all_data.copy()\nall_data2['text'] = all_data['text'].apply(lambda x: x.lower())\nall_data2.head(5)\nall_data2.to_csv(\"tweets_lowercased\", index=False)","c00498c3":"y_logistic_lower, y_val = logreg_bow(all_data2[:len(train)], 0.2)\naccuracy, precision, recall, f1 = get_metrics(y_val, y_logistic_lower)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","31d6750e":"def remove_accented_chars(text):\n    text = unidecode.unidecode(text)\n    return text\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_url(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\ndef expand_contractions(text):\n    text = list(cont.expand_texts([text], precise=True))[0]\n    return text\n\nspell = SpellChecker()\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n\n# Now compact all the normalization function calls into a single function\ndef normalization(text):\n    text = remove_accented_chars(text)\n    text = remove_html(text)\n    text = remove_url(text)\n    text = remove_emoji(text)\n    text = remove_punct(text)\n    text = expand_contractions(text)\n    text = correct_spellings(text)\n    return text","62c7893b":"ts = time.time()\n\nall_data2 = all_data.copy()\nall_data2['text'] = all_data['text'].apply(lambda x: normalization(x))\nall_data2.head(5)\nall_data2.to_csv(\"tweets_normalized\", index=False)\n\nprint(\"Time spent: \", time.time() - ts)","a34c1eee":"y_logistic_norm, y_val = logreg_bow(pd.read_csv(\"\/kaggle\/input\/disaster-tweets-comp-introduction-to-nlp\/tweets_normalized\")[:len(train)], 0.2)\naccuracy, precision, recall, f1 = get_metrics(y_val, y_logistic_norm)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","b6b0321c":"# Load the spacy model to get sentence vectors\nnlp = spacy.load('en_core_web_lg')","1fd297b5":"def remove_stopwords(text):\n    tokens = [token.text for token in text if not token.is_stop]\n    return ' '.join([token for token in tokens])\n\nts = time.time()\nall_data2 = all_data.copy()\nall_data2['text'] = all_data['text'].apply(lambda x: remove_stopwords(nlp(x)))\nall_data2.head(5)\nall_data2.to_csv(\"tweets_no_stopwords\", index=False)\nprint(\"Time spent: \", time.time() - ts)","16e29892":"y_logistic_stop, y_val = logreg_bow(pd.read_csv(\"\/kaggle\/input\/disaster-tweets-comp-introduction-to-nlp\/tweets_no_stopwords\")[:len(train)], 0.2)\naccuracy, precision, recall, f1 = get_metrics(y_val, y_logistic_stop)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","087e80fd":"def lemmatizer(text):\n    tokens = [token.lemma_ for token in text]\n    return ' '.join([token for token in tokens])\n\nts = time.time()\nall_data2 = all_data.copy()\nall_data2['text'] = all_data['text'].apply(lambda x: lemmatizer(nlp(x)))\nall_data2.head(5)\nall_data2.to_csv(\"tweets_lemmatized\", index=False)\nprint(\"Time spent: \", time.time() - ts)","07f691b9":"y_logistic_lemma, y_val = logreg_bow(pd.read_csv(\"\/kaggle\/input\/disaster-tweets-comp-introduction-to-nlp\/tweets_lemmatized\")[:len(train)], 0.2)\naccuracy, precision, recall, f1 = get_metrics(y_val, y_logistic_lemma)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","af971ab1":"def tfidf(data):\n    tfidf_vectorizer = TfidfVectorizer()\n    train = tfidf_vectorizer.fit_transform(data)\n    return train, tfidf_vectorizer\n\ndef logreg_tfidf(data, valid_fraction):\n    \n    # Transform data to list\n    list_corpus = data[\"text\"].tolist()\n    list_labels = data[\"target\"].tolist()\n    \n    # Split train-validation data\n    X_train, X_valid, y_train, y_valid = train_test_split(list_corpus, list_labels, \n                                                          test_size=valid_fraction, random_state=21)\n\n    # Compute the tfidf vectors\n    X_train_tfidf, tfidf_vectorizer = tfidf(X_train)\n    X_valid_tfidf = tfidf_vectorizer.transform(X_valid)\n\n    # Run the logistic regression model\n    clf_tfidf = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', \n                             multi_class='multinomial', n_jobs=-1, random_state=21)\n    clf_tfidf.fit(X_train_tfidf, y_train)\n\n    y_predicted_tfidf = clf_tfidf.predict(X_valid_tfidf)\n    \n    # Cross validation score over 10 folds\n    scores = cross_val_score(clf_tfidf, X_train_tfidf, y_train, cv=10)\n    print(\"Cross validation over 10 folds: \", sum(scores)\/10.)\n    \n    return y_predicted_tfidf, y_valid","bcf4a15d":"print(\"Basic model\")\ny_predicted_tfidf, y_valid = logreg_tfidf(all_data[:len(train)], 0.2)\n\nprint(\"Lowercased model\")\ny_predicted_tfidf, y_valid = logreg_tfidf(pd.read_csv(\"\/kaggle\/input\/disaster-tweets-comp-introduction-to-nlp\/tweets_lowercased\")[:len(train)], 0.2)\n\nprint(\"Normalized model\")\ny_predicted_tfidf, y_valid = logreg_tfidf(pd.read_csv(\"\/kaggle\/input\/disaster-tweets-comp-introduction-to-nlp\/tweets_normalized\")[:len(train)], 0.2)\n\nprint(\"No stopwords model\")\ny_predicted_tfidf, y_valid = logreg_tfidf(pd.read_csv(\"\/kaggle\/input\/disaster-tweets-comp-introduction-to-nlp\/tweets_no_stopwords\")[:len(train)], 0.2)\n\nprint(\"Lemmatized model\")\ny_predicted_tfidf, y_valid = logreg_tfidf(pd.read_csv(\"\/kaggle\/input\/disaster-tweets-comp-introduction-to-nlp\/tweets_lemmatized\")[:len(train)], 0.2)","782e7c1a":"ts = time.time()\n\n# Read the file with no stop words\nclean_data = pd.read_csv(\"\/kaggle\/input\/disaster-tweets-comp-introduction-to-nlp\/tweets_no_stopwords\")\n\n# Lemmatize\nclean_data['text'] = clean_data['text'].apply(lambda x: lemmatizer(nlp(x)))\n\n# Lowercase\nclean_data['text'] = clean_data['text'].apply(lambda x: x.lower())\n\nclean_data.to_csv(\"tweets_clean\", index=False)\n\nprint(\"Time spent: \", time.time() - ts)","4fc5a7cc":"# Define a tfidf and split function\ndef tfidf_split(data, valid_fraction):\n    \n    # Transform data to list\n    list_corpus = data[\"text\"].tolist()\n    list_labels = data[\"target\"].tolist()\n    \n    # Split train-validation data\n    X_train, X_valid, y_train, y_valid = train_test_split(list_corpus, list_labels, \n                                                          test_size=valid_fraction, random_state=21)\n\n    # Compute the tfidf vectors\n    X_train_tfidf, tfidf_vectorizer = tfidf(X_train)\n    X_valid_tfidf = tfidf_vectorizer.transform(X_valid)\n    \n    return X_train_tfidf, X_valid_tfidf, y_train, y_valid\n\nX_train, X_valid, y_train, y_valid = tfidf_split(clean_data[:len(train)], 0.2)\n\n\n# Define a general call for the different models\ndef get_cross_val(model, X_train, X_valid, y_train, y_valid):\n    \n    # Fit on train, predict on validation\n    clf = model\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_valid)\n    \n    # Cross validation score over 10 folds\n    scores = cross_val_score(clf, X_train, y_train, cv=10)\n    print(\"Cross validation over 10 folds: \", sum(scores)\/10.)\n    \n    return y_pred","a287161a":"ts = time.time()\n\nprint(\"Logistic regression: \")\ny_LR = get_cross_val(LogisticRegression(C=30.0, class_weight='balanced', \n                                        solver='newton-cg', multi_class='multinomial', \n                                        n_jobs=-1, random_state=21), \n                                        X_train.toarray(), X_valid.toarray(), y_train, y_valid)\n\nprint(\"\\nNaive Bayes:\")\ny_bayes = get_cross_val(GaussianNB(), X_train.toarray(), X_valid.toarray(), y_train, y_valid)\n\nprint(\"\\nSVC:\")\ny_SVC = get_cross_val(LinearSVC(random_state=21, dual=False), X_train, X_valid, y_train, y_valid)\n\nprint(\"\\nKNN:\")\ny_KNN = get_cross_val(KNeighborsClassifier(n_neighbors=15), X_train, X_valid, y_train, y_valid)\n\nprint(\"\\nRandom forest:\")\ny_RF = get_cross_val(RandomForestClassifier(random_state=21), X_train, X_valid, y_train, y_valid)\n\nprint(\"\\nXGBoost:\")\ny_XGB = get_cross_val(XGBClassifier(n_estimators=1000, random_state=21), X_train, X_valid, y_train, y_valid)\n\nprint(\"Time spent: \", time.time() - ts)","299dc44b":"ts = time.time()\nclean_data = pd.read_csv(\"\/kaggle\/input\/disaster-tweets-comp-introduction-to-nlp\/tweets_clean\")\nvectors = np.array([nlp(tweet.text).vector for idx, tweet in clean_data.iterrows()])\nvectors.shape\nprint(\"Time spent: \", time.time() - ts)","d716432a":"# Center the vectors\nvec_mean = vectors.mean(axis=0)\ncentered = pd.DataFrame([vec - vec_mean for vec in vectors])\nprint(\"Center shape: \", centered.shape)","b0b62e3f":"ts = time.time()\n\ndef svc_model(vectors, train):\n    # Split train-validation data\n    X_train, X_valid, y_train, y_valid = train_test_split(vectors[:len(train)], train.target, \n                                                          test_size=0.2, random_state=21)\n\n    # Create the LinearSVC model\n    model = LinearSVC(random_state=21, dual=False)\n    \n    # Fit the model\n    model.fit(X_train, y_train)\n    \n    # Cross validation score over 10 folds\n    scores = cross_val_score(model, X_train, y_train, cv=10)\n    print(\"Cross validation over 10 folds: \", scores, \" --- \", sum(scores)\/10.)\n    \n    # Uncomment to see model accuracy\n    #print(f'Model test accuracy: {model.score(X_valid, y_valid)*100:.3f}%')\n    \n    return model\n\nmodel_svc_basic = svc_model(centered, train)\nprint(\"Time spent: \", time.time() - ts)","3b92bc8b":"# Submit results\n\ny_test = model_svc_basic.predict(centered[-len(test):])\nsubmission = pd.DataFrame({\n    \"id\": test.id, \n    \"target\": y_test\n})\nsubmission.to_csv('submission_svc_basic.csv', index=False)","69cd9289":"ts = time.time()\nclean_data = all_data\nvectors = np.array([nlp(tweet.text).vector for idx, tweet in clean_data.iterrows()])\nvec_mean = vectors.mean(axis=0)\ncentered = pd.DataFrame([vec - vec_mean for vec in vectors])\nmodel_svc_basic = svc_model(centered, train)\nprint(\"Time spent: \", time.time() - ts)","569490f5":"# Read the file with no stop words\nclean_data = pd.read_csv(\"\/kaggle\/input\/disaster-tweets-comp-introduction-to-nlp\/tweets_clean\")","269303ea":"# Example https:\/\/www.kaggle.com\/reiinakano\/basic-nlp-bag-of-words-tf-idf-word2vec-lstm\n\ntext_lengths = [len(x.split()) for x in (clean_data['text'])]\nnum_words = max(text_lengths)\n\ndef model_RNN(num_words, embed_dim, lstm_out, clean_data, batch_size, eta, dropout, n_epochs): \n    \n    # Use the Keras tokenizer\n    tokenizer = Tokenizer(num_words=num_words)\n    tokenizer.fit_on_texts(clean_data['text'].values)\n\n    # Pad the data \n    X = tokenizer.texts_to_sequences(clean_data['text'].values)\n    X = pad_sequences(X, maxlen=num_words + 1)\n\n    # Split data\n    Y = pd.get_dummies(clean_data['target']).values\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 21, stratify=Y)\n    \n    model = Sequential()\n    model.add(Embedding(num_words, embed_dim, input_length = X.shape[1]))\n    model.add(LSTM(lstm_out, recurrent_dropout=dropout, dropout=dropout))\n    model.add(Dense(2,activation='sigmoid'))\n    model.compile(loss = 'binary_crossentropy', optimizer=Adam(lr=eta), metrics = ['accuracy'])\n    model_history = model.fit(X_train, Y_train, epochs=n_epochs, batch_size=batch_size, validation_split=0.2, verbose=0)\n    \n    return model, model_history\n\n# Define RNN parameters\nembed_dim = 64\nlstm_out = 64 \nbatch_size = 16\neta = 0.001\ndropout = 0.5\nn_epochs = 20\nn_reps = 10\n\n#model_RNN_1, model_RNN_1_hist_rep = model_RNN(num_words, embed_dim, lstm_out, clean_data, batch_size, eta, dropout, n_epochs)","3dbe4d38":"model_RNN_1_hist = [0]*n_epochs\n\nfor rep in range(n_reps):\n    print(\"Repetition \", rep)\n    model_RNN_1, model_RNN_1_hist_rep = model_RNN(num_words, embed_dim, lstm_out, clean_data, batch_size, eta, 0.2, n_epochs)\n    model_RNN_1_hist = tuple(map(operator.add, model_RNN_1_hist, model_RNN_1_hist_rep.history['val_accuracy']))\n    \nmodel_RNN_1_hist = [x\/n_reps for x in list(model_RNN_1_hist)] ","779642d1":"plt.plot(model_RNN_1_hist)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Val_accuracy\")\nplt.xlim(0,20)","939958cf":"Xtest = tokenizer.texts_to_sequences(clean_data[-len(train):]['text'].values)\nXtest = pad_sequences(Xtest, maxlen=num_words + 1)\n\nRNN = model.predict(Xtest)\n\ny_test = model.predict(Xtest)\ny_test = np.argmax(y_test,axis = 1)\n\nsubmission = pd.DataFrame({\n    \"id\": test.id, \n    \"target\": y_test\n})\nsubmission.to_csv('submission_lstm.csv', index=False)","46145aa6":"**Conclusion**: lemmatization has proved to slightly increase the tweets classification process. We will apply lemmatization in future models. From all the techniques, normalization is the only one that has provided a lower accuracy. Hence, we conclude that we should avoid normalization for this dataset.\n\nPerformance of the models:\n* **Basic logistic**:  cross_val_accuracy = 0.7754, accuracy = 0.797, precision = 0.797, recall = 0.797, f1 = 0.797\n* **Lowercase logistic**:  cross_val_accuracy = 0.7754, accuracy = 0.797, precision = 0.797, recall = 0.797, f1 = 0.797\n* **Normalized logistic**:  cross_val_accuracy = 0.7598, accuracy = 0.788, precision = 0.788, recall = 0.788, f1 = 0.788\n* **No stopwords logistic**:   cross_val_accuracy = 0.7772, accuracy = 0.787, precision = 0.786, recall = 0.787, f1 = 0.786\n* **Lemmatized logistic**:   cross_val_accuracy = 0.7767, accuracy = 0.776, precision = 0.776, recall = 0.776, f1 = 0.776","2057a125":"# 4. Comparison of classification models <a id=\"section4\"><\/a>\n\nAt this point, we are in disposition to conclude that the best results are obtained by a combination of TF-IDF and a certain data cleaning (no normalization). At least, that's what we observed from a Logistic regression model. But, what about other models? In this section we will analyze different classification algorithms to see which performs better in the tweets dataset.\n\nModels to study:\n* Naive Bayes\n* Linear SVC\n* KNN\n* Random Forest\n* XGBoost\n\nBefore starting with the predictions, we need to generate the clean tweet dataset to analyze (lowercasing + stop removal + lemmatization):","a36369f3":"Stopwords are succesfully removed from the tweets text. Let's see if this improves the performance:","91973060":"# Disaster tweets comp: introduction to NLP\n\nThe aim of this notebook is to classify tweets potentially related to catastrophes into real or misleading tweets. It may also serve as an introduction to Natural Language Processing (NLP), since it covers several data cleaning techniques and classification algorithms in order to conclude which combinations are the most succesfull.The spirit of this notebook is not to obtain a top LB score, but to learn how to deal with text data, decide which techniques are more suited for this project and why others are not recommended from a beginner perspective. \n\nHope you enjoy it and, if you like this work, I'll be glad if you upvote it. Let's begin!\n\n\n**TABLE OF CONTENTS**\n\n1. [Bag of words (BOW)](#section1)\n\n    * [Load data](#section11)\n    \n    * [Brief EDA](#section12)\n    \n    * [BOW and logistic prediction regression](#section13)\n\n\n2. [Text data cleaning](#section2)\n\n    * [Lowercasing](#section21)\n    \n    * [Normalization](#section22)\n  \n    * [Stop-word removal](#section23)\n    \n    * [Lemmatization](#section24)\n\n\n3. [Term frequency - Inverse document frequency (TFIDF)](#section3)\n\n\n4. [Comparison of classification models](#section4)\n\n\n5. [Word embeddings](#section5)\n\n    * [Word embeddings](#section51)\n    \n    * [Linear SVC model](#section52)\n\n\n6. [Recurrent neural networks (RNN)](#section6)\n\n\n\n**Disclaimer**: I myself am a beginner with NLP, and this notebook reflects my first attempt through one of these projects. I found really useful to develop each of the sections of the kernel in an ordered way, but surely there will be things that could be improved by someone with more experience in NLP. Any advice, question or comment is always welcome!","8ba96e11":"# 5. Word embeddings <a id=\"section5\"><\/a>\n\nWord embeddings (or word vectors) are a **transformation of words into vectors** based on large language models. Two vectors will be similar when their respective words share a common background; for example, queen and king have similar components, while queen and table don't. I strongly recommend [Mat's Kaggle course](https:\/\/www.kaggle.com\/matleonard\/word-vectors) for more details.\n\nOn the other hand, we will classify these vectors with  a **Support vector machine** (SVM). SVMs are machine learning algorithms that aim to find an hyperplane in a N-dimensional space, where N is the number of features, in order to classifiy data points into classes. SVM are both precise and fast when dealing with high dimensional datasets.","769c3971":"Performance of the models for 10 cv folds:\n* **Logistic regression (TFIDF)**:  0.7763546798029556\n* **Naive Bayes (TFIDF)**:  0.6021346469622332\n* **SVC (TFIDF)**: 0.789655172413793\n* **KNN (TFIDF)**:  0.777175697865353\n* **Random forest (TFIDF)**: 0.7821018062397372\n* **XGBoost (TFIDF)**: 0.7819376026272578\n* **SVC (word embeddings)**: 0.7886699507389163\n* **SVC (word embeddings, no clean data)**: 0.7960591133004927","f318b03c":"## 1.1. Load data <a id=\"section11\"><\/a>\n\nFirst of all, let's read the data and take a look on its structure.","0edb390b":"## 5.1. Word embeddings <a id=\"section51\"><\/a>\n\nTo transform text into vectors we are going to use the spaCy library, that we previously used for the removal of stop-words. Notice that the word vector transformation is at the level of words, but in this project we are dealing with full sentences (tweets). To transform a whole text into a single vector, spaCy just computes the **average of all vectors (words) in a sentence to create the text vector**.","6d1b68d4":"**Conclusion**: looks like normalization has worsen the model's accuracy, as well as the rest of the metrics. Maybe some of the removed characters contained some useful information, so that we conclude that no normalization is required for the dataset.\n\nPerformance of the models:\n* **Basic logistic**:  cross_val_accuracy = 0.7754, accuracy = 0.797, precision = 0.797, recall = 0.797, f1 = 0.797\n* **Lowercase logistic**:  cross_val_accuracy = 0.7754, accuracy = 0.797, precision = 0.797, recall = 0.797, f1 = 0.797\n* **Normalized logistic**:  cross_val_accuracy = 0.7599, accuracy = 0.788, precision = 0.788, recall = 0.788, f1 = 0.788","43a17bb1":"# 6. Recurrent neural networks (RNN) <a id=\"section6\"><\/a>\n\nAll our previous attempts to classify tweets have been based on different machine learning algorithms, but none of them considered the potential of deep learning. Both section 6 and 7 will cover this approach to analyze if it can obtain a better classification performance.\n\nOur first model is a recurrent neural network (RNN), in particular the so called **long-short term memory (LSTM)** model, a type of neural network that permits a bidirectional flow of information through layers. At some level, the idea is to **replicate the idea of a memory**, since previous segments of data may affect the following ones, hence helping the model to recognise patterns in the sentence. You cna find a clear and detailed exaplanation in [Colah's blog](https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/) (maybe a bit outdated). ","48d465e7":"Import the required libraries:","7dce20e1":"## 2.2. Normalization <a id=\"section22\"><\/a>\n\n**Definition**: The jargon used in coloquial language, specially in social media, leads to either content not useful for NLP or to different versions of the same sentence but written in alternative ways. For example: punctuation marks, emojis, contractions, URLs, grammar errors... \n\nTo normalize data and reduce noise, we will apply several transformations:\n* **Remove accents**. Instead of receiving accented characters (latt\u00e9) we just write plain letters.\n* **Remove URL**. Most tweets may include links and such, it may be good to clean them.\n* **Remove html**. Scratched data usually contains headers and marks (<br\/), this should be removed.\n* **Remove emojis**. Despite emojis are related to sentiments, they are abused and used in any type of tweet.\n* **Remove punctuation**. All punctuation marks are deleted.\n* **Expand contraction**s. Contractions like isn't are expanded to is not.\n* **Spell checking**. Substitute wrong sentences (Am gona ned you) to correct ones (I am going to need you).\n\n**Disclaimer**: the origin of several of the functions used in this subsection come from the awesome kernel of Shahules786; https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove. Please check it out for reckon.","5012032c":"Another detail to consider is that **language texts from a certain topic frequently share a lot of common words**. Hence, the different sentence vectors may be very similar to each other. To fix this, it is useful to compute the mean of all vectors (i.e. the whole dataset vector) and substract it from them:","4c0f9396":"Performance of the models for 10 cv folds:\n* **Logistic regression (TFIDF)**:  0.7763546798029556\n* **Naive Bayes (TFIDF)**:  0.6021346469622332\n* **SVC (TFIDF)**: 0.789655172413793\n* **KNN (TFIDF)**:  0.777175697865353\n* **Random forest (TFIDF)**: 0.7821018062397372\n* **XGBoost (TFIDF)**: 0.7819376026272578\n* **SVC (word embeddings)**: 0.7886699507389163","73fc61ff":"## 1.2. Brief EDA <a id=\"section12\"><\/a>\n\nThe dataset contains only 3 relevant features, and most of the useful information is contained in the text column. Hence, let's keep things simple and explore only the surface of this data. A few question to answer:\n* How many different locations?\n* How many different keywords?\n* Is the target class balanced? Number of real tweets vs fake tweets\n* Has the dataset missing values?","4027e17a":"# 2. Text data cleaning <a id=\"section2\"><\/a>\n\nDespite being very simple, our first model has reached a reasonably good accuracy. Let's see if we can improve it by performing specific cleaning techniques for language data. \n\nSteps in this section:\n* 2.1. Lowercasing\n* 2.2. Normalization\n* 2.3. Stop word removal\n* 2.4. Lemmatization\n\n**Note 1**: this techniques should not be considered as a mandatory procedure to apply in all NLP projects. Some of them could lead to lower performances depending on the specific nature of the problem, and that's why we are going to add them to the model independently in order to verify if the accuracy increases.\n\n**Note 2**: to achieve a more readable code, we will compute everything based on generalized functions that include all the previous steps. Moreover, some of the cleaning techniques take some time to process the data, so that we will save one file per transformation.","da3c8719":"Looks like there are some duplicates (i.e. USA\/United States), and some USA countries are also specified (i.e. Chicago, IL). However, since locations have proven to contain lots of missing values, it's not worth working these duplicates. \n\nA more interesting task is to **replace missing values** in locations and keywords by the string \"None\".","d08ca7fb":"## 2.3. Stop word removal <a id=\"section23\"><\/a>\n\n**Definition**: Analogous to normalization, some words are merely argon based and do not add much value to sentences. For example: the, to, and,...\n\nIn order to remove stop words, we will use a powerful NLP library named spaCy. **SpaCy is able to recognise different languages**, split full texts into words (tokens) and extract additional features that we will review in the following sections. For now on, let's just use it for transformation purposes.\n\nThe first step to work with spaCy is to load the language model to use (see https:\/\/spacy.io\/usage\/models and https:\/\/spacy.io\/usage\/spacy-101 for detailed info), in our case English. This sets up the specific rules of the language in order to perform the **tokenization**.","58b0547e":"**Conclusions**:\n* There's almost half the number of locations than the number of data rows\n* Not many different keywords\n* Target variable is balanced (43% are real tweets)\n* Some keywords are missing\n* Almost 30% of the locations are missing\n\nThe most common keywords and locations may also give us some insight:","afd69022":"The tokenizer works fine, transforming all characters into lowercase. Does this transformation improves the model's performance?","31e48641":"In order to properly analyze the results of the TF-IDF transformation, we will run the logistic regression model for all the data cleaning cases:","13c57bdb":"## 5.2. Linear SVC model <a id=\"section52\"><\/a>\n\nFinally, data is relatively clean and has been transformed into vectors. The only task remaining is to feed these data into the SVM model, train it and finally submit the results from the test dataset.\n\nWe will use 80% of data for training and the remaining 20% to validate.","cbc9a734":"Performance of the models:\n* **Basic logistic**:  cross_val_accuracy = 0.7754, accuracy = 0.797, precision = 0.797, recall = 0.797, f1 = 0.797","797a38e1":"**Conclusions**: All models have performed well except for Naive Bayes. The best scores are obtained by SVC, Random Forest and XGB. Notice that we performed no fine tunning of the parameters, which may affect the results (in particular for complex algorithms like XGB).","341f1850":"Once all the sub-processes have been defined, we just need to apply the normalization function:","0890e3ec":"## 2.4. Lemmatization <a id=\"section24\"><\/a>\n\n**Definition**: Several words share a common root, but their slight differences are not very important. Hence, lemmatization translates all these words into their root form, so that models are not confused by different versions. For example: bad and worse, or flying, flew and flown.\n\nAgain, we will use Spacy, since it automatically computes the lemma of each word. Analogously to stop words, obtaining the lemma is very easy:","4bf8bb13":"## 2.1. Lowercasing <a id=\"section21\"><\/a>\n\n**Definition**: In order to allow the program to generalize words, transform all of them to lower case.","97c5589c":"Tweets have been correctly transformed according to our normalization rules. Do normalization increases the logistic regression performance?","5a881b1e":"Now, to obtain the cross validation score, we simply call the function **get_cross_val** for each of the models:","285e82fd":"# 1. Bag of words (BOW) <a id=\"section1\"><\/a>\n\nA simple procedure when working with language data is to compute a **bag of words (BOW)**, that consists on creating one column for each word and keeping track of when they appear in the tweets. This will be our first approach given that it's a common technique in NLP projects, significantly fast and works fine without many data cleaning. \n\nOnce the  BOW has been created, we will train a **logistic regression** model and evaluate the performance of the predictions.\n\nSteps in this section:\n* 1.1. Load data\n* 1.2. Brief EDA\n* 1.3. BOW and logistic regression prediction","f82ebe00":"Lemmatization has tranformed all words into their root form. Once a model is run, it will recognize the meaning of the words but ignoring specific conjugations and alternative forms. Our next objective is to verify if the accuracy is improved.","c8c555e5":"**Conclusion**: lowercased text do not improve the logistic regression performance. However, it does not make it worse either.\n\nPerformance of the models:\n* **Basic logistic**:  cross_val_accuracy = 0.7754, accuracy = 0.797, precision = 0.797, recall = 0.797, f1 = 0.797\n* **Lowercase logistic**:  cross_val_accuracy = 0.7754, accuracy = 0.797, precision = 0.797, recall = 0.797, f1 = 0.797","3cc96eed":"A validation accuracy of 0.797, quite good for such a simple model. Let's compute the prediction over the test dataset and submit the output file to obtain a LB score. ","61a03be4":"## 1.3. BOW and logistic regression prediction <a id=\"section13\"><\/a>\n\nData has been analysed and there are no missing values in the dataset. We could now proceed to analyze all columns in order to classify tweets into real or misleading disaster tweets, and even extract some additional features (number of words, number of punctuation marks, etc). However, since we are interested in the language treatment of this dataset, **for the sake of simplicity I decided to focus only on the text column** and ignore the other ones. Notice that ignoring keywords, locations and engineered features may prevent our algorithms from obtaining better scores.\n\nWe are in disposition to **compute the bag of words** (BOW), split the dataset into a train\/validation subsets and finally compute a **logistic regression model**. ","c3c23c6f":"Define a couple quality of life functions to split train\/test data and get the cross validation score of any classification model:","d575b204":"**Conclusion**: results are slightly better than in the BOW case, but they point into the same direction with respect to data cleaning. All  cleaning techniques have improved the model's accuracy, except for normalization. From now on, we will use a dataset with the **full cleaning but for normalization**.","49f86c6e":"**Conclusion**: removing stopwords has slightly increased the cross validation accuracy. Other metrics are also improved, so that we conclude that stop word removal helps the model to predict real disaster tweets. \n\nPerformance of the models:\n* **Basic logistic**:  cross_val_accuracy = 0.7754, accuracy = 0.797, precision = 0.797, recall = 0.797, f1 = 0.797\n* **Lowercase logistic**:  cross_val_accuracy = 0.7754, accuracy = 0.797, precision = 0.797, recall = 0.797, f1 = 0.797\n* **Normalized logistic**:  cross_val_accuracy = 0.7599, accuracy = 0.788, precision = 0.788, recall = 0.788, f1 = 0.788\n* **No stopwords logistic**:   cross_val_accuracy = 0.7755, accuracy = 0.787, precision = 0.786, recall = 0.787, f1 = 0.786","e9c31f3e":"The SpaCy library contains an in-built stop word function, so that removing these words is very simple:","f246836d":"# 3. Term frequency - Inverse document frequency (TFIDF) <a id=\"section3\"><\/a>\n\nData cleaning has demonstrated its utility, but also that it's important to explore which techniques are suitable for our problem. In order to keep getting better results, we'll try a subtle modification of the BOW named **term frequency, inverse document frequency (TF-IDF)**. \n\nAnalogous to BOW, TF-IDF creates one column for each word, but this time the weight is averaged by both the frequency of the word and the number of cases in which it appears. Hence, if there are multiple occurrences of a certain word, but it's common in most of the texts, then the weight is lowered. ","9bc89d7d":"A reasonably good accuracy (around 79%) spending only 14 seconds for 10 cross validations folds and using cleaned data, not bad at all. Let's submit the prediction over test dataset in order to see how well performs our first model on the leaderboard, before we move to another section."}}