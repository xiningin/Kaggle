{"cell_type":{"9341217f":"code","a27549de":"code","207f4a0c":"code","7ea2644d":"code","139341a6":"code","94a32ef9":"code","41982aec":"code","9c07836d":"code","62ea6dd5":"code","1bc1d10c":"code","96fdbf03":"code","d69797f4":"code","f58f2f68":"code","7c7cf9b9":"code","1d0485ed":"code","061db20e":"code","591babb2":"markdown","7290ec3d":"markdown","20839eb5":"markdown","79f722df":"markdown","3a3b6045":"markdown","21523175":"markdown"},"source":{"9341217f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a27549de":"# data exploring and basic libraries\nimport random\nimport re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nfrom collections import deque as dq\n\n# NLP preprocessing\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk import word_tokenize as TK\nfrom nltk import pos_tag\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder as LE\n\n# binary class classification model\nfrom sklearn.svm import SVC\n\n# One vs All wrapper\nfrom sklearn.multiclass import OneVsRestClassifier as OVRC\n\n# Pretty display for notebooks\nfrom IPython.display import display # Allows the use of display() for DataFrames\n%matplotlib inline","207f4a0c":"# load the data - test data\nrawdf_te = pd.read_json(path_or_buf='..\/input\/test.json')\nrawdf_te.head(n=3)\n# load the data - train data\nrawdf_tr = pd.read_json(path_or_buf='..\/input\/train.json')\nrawdf_tr.head(n=3)","7ea2644d":"# cuisine distribution\nsns.countplot(y='cuisine', data=rawdf_tr, palette ='Set3')\n\n# number of recipes for each cuisines\nprint('Weight\\t Recipe\\t Cuisine\\n')\nfor _ in (Counter(rawdf_tr['cuisine']).most_common()):print(round(_[1]\/rawdf_tr.cuisine.count()*100, 2),'%\\t',_[1],'\\t', _[0])","139341a6":"# change id column type to string\nrawdf_tr = rawdf_tr.set_index('id')\nrawdf_te = rawdf_te.set_index('id')\n\n# Total number of recipes\nprint('Total of %d recipes\\n'% len(rawdf_tr))\n\n# total number of UNIQUE cuisines\nprint('Total of %d types of cuisines including %s\\n' % \\\n      (len(rawdf_tr['cuisine'].unique()), rawdf_tr['cuisine'].unique().tolist()))\n                                          \n# UNIQUE # ingredients set - ingredients_set()\n# training ingredient list\ningredients_list_tr = []\nfor _ in rawdf_tr['ingredients']:\n    ingredients_list_tr.append(_)\n# ingredients set - ingredients_set()\ningredients_set_tr = set()\nfor a in range(len(ingredients_list_tr)):\n    for _ in range(len(ingredients_list_tr[a])):\n        ingredients_set_tr.add(ingredients_list_tr[a][_])\nprint(\"Total of %d unique ingredients\\n\" % len(ingredients_set_tr))\n\n# total ingredients list (with repition) occurred in the train data\ntotal_ingredients_list_tr = []\nfor i in range(len(ingredients_list_tr)):\n    for j in range(len(ingredients_list_tr[i])):\n        total_ingredients_list_tr.append(ingredients_list_tr[i][j])\nprint(\"Most common ingredients used:\\n\")\nfor _ in range(len(Counter(total_ingredients_list_tr).most_common(11))):\n    print(Counter(total_ingredients_list_tr).most_common(11)[_])","94a32ef9":"print(rawdf_tr['ingredients'].loc[41935])\nprint(rawdf_tr['ingredients'].loc[27566])\nprint(rawdf_tr['ingredients'].loc[32596])\nprint(rawdf_tr['ingredients'].loc[8476])","41982aec":"# copy the series from the dataframe\ningredients_tr = rawdf_tr['ingredients']\n# do the test.json while at it\ningredients_te = rawdf_te['ingredients']","9c07836d":"# substitute the matched pattern\ndef sub_match(pattern, sub_pattern, ingredients):\n    for i in ingredients.index.values:\n        for j in range(len(ingredients[i])):\n            ingredients[i][j] = re.sub(pattern, sub_pattern, ingredients[i][j].strip())\n            ingredients[i][j] = ingredients[i][j].strip()\n    re.purge()\n    return ingredients\n\ndef regex_sub_match(series):\n    # remove all units\n    p0 = re.compile(r'\\s*(oz|ounc|ounce|pound|lb|inch|inches|kg|to)\\s*[^a-z]')\n    series = sub_match(p0, ' ', series)\n    # remove all digits\n    p1 = re.compile(r'\\d+')\n    series = sub_match(p1, ' ', series)\n    # remove all the non-letter characters\n    p2 = re.compile('[^\\w]')\n    series = sub_match(p2, ' ', series)\n    return series","62ea6dd5":"# regex train data\ningredients_tr = regex_sub_match(ingredients_tr)\n# regex test.json data\ningredients_te = regex_sub_match(ingredients_te)","1bc1d10c":"# declare instance from WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n\n# remove all the words that are not nouns -- keep the essential ingredients\ndef lemma(series):\n    for i in series.index.values:\n        for j in range(len(series[i])):\n            # get rid of all extra spaces\n            series[i][j] = series[i][j].strip()\n            # Tokenize a string to split off punctuation other than periods\n            token = TK(series[i][j])\n            # set all the plural nouns into singular nouns\n            for k in range(len(token)):\n                token[k] = lemmatizer.lemmatize(token[k])\n            token = ' '.join(token)\n            # write them back\n            series[i][j] = token\n    return series","96fdbf03":"# lemmatize the train data\ningredients_tr = lemma(ingredients_tr)\n# lemmatize test.json\ningredients_te = lemma(ingredients_te)","d69797f4":"print(ingredients_tr[41935])\nprint(ingredients_tr[27566])\nprint(ingredients_tr[32596])\nprint(ingredients_tr[8476])","f58f2f68":"# copy back to the dataframe\nrawdf_tr['ingredients_lemma'] = ingredients_tr\nrawdf_tr['ingredients_lemma_string'] = [' '.join(_).strip() for _ in rawdf_tr['ingredients_lemma']]\n# do the same for the test.json dataset\nrawdf_te['ingredients_lemma'] = ingredients_te\nrawdf_te['ingredients_lemma_string'] = [' '.join(_).strip() for _ in rawdf_te['ingredients_lemma']]","7c7cf9b9":"# DataFrame for training and validation\ntraindf = rawdf_tr[['cuisine', 'ingredients_lemma_string']].reset_index(drop=True)\n# same for the test set\ntestdf = rawdf_te[['ingredients_lemma_string']]","1d0485ed":"# training ===================\n# X_train\nX_train = traindf['ingredients_lemma_string']\nvectorizertr = TfidfVectorizer(stop_words='english', analyzer=\"word\", max_df=0.65, min_df=2, binary=True)\nX_train = vectorizertr.fit_transform(X_train)\n\n# y_train\ny_train = traindf['cuisine']\n# for xgboost the labels need to be labeled with encoder\nle = LE()\ny_train_ec = le.fit_transform(y_train)\n\n# predicting =================\n# X_pred\nX_pred = testdf['ingredients_lemma_string']\nvectorizerts = TfidfVectorizer(stop_words='english')\nX_pred = vectorizertr.transform(X_pred)\n\n# y_true","061db20e":"# Best parameters after running the grid search for One-Versus-All SVM\nclf_ovrc_svm = SVC(C=3.25, cache_size=500, class_weight=None, coef0=0.0,\\\n  decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf',\\\n  max_iter=-1, probability=False, random_state=0, shrinking=True,\\\n  tol=0.001, verbose=False)\n\nclf_ovrc_svm = clf_ovrc_svm.fit(X_train, y_train)\n\ny_pred_ovrc_svm = clf_ovrc_svm.predict(X_pred)\n\ntestdf['cuisine'] = y_pred_ovrc_svm\nd = pd.DataFrame(data=testdf['cuisine'], index=testdf.index).sort_index().reset_index().to_csv('submission_ovr_svm.csv', index=False)","591babb2":"Preprocess the datasets","7290ec3d":"> SVM model wiht 'ovr'","20839eb5":"What does the ingredients column look like?","79f722df":"Lemmatize!","3a3b6045":"What do they look like now?","21523175":"TF-IDF vectorizing"}}