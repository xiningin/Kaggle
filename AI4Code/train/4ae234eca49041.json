{"cell_type":{"c522a034":"code","6133e847":"code","f31a45b7":"code","7ccef9b4":"code","215a96a4":"code","ece96561":"code","45a1a2c5":"code","5ab10dac":"code","4199415a":"markdown","754b3890":"markdown"},"source":{"c522a034":"import numpy as np # linear algebra\nfrom numpy import newaxis\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport os\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.models import Sequential\nfrom keras import optimizers\n\nprint(os.listdir(\"..\/input\"))","6133e847":"def normalise_windows(window_data):\n    # A support function to normalize a dataset\n    normalised_data = []\n    for window in window_data:\n        normalised_window = [((float(p) \/ float(window[0])) - 1) for p in window]\n        normalised_data.append(normalised_window)\n    return normalised_data\n\ndef load_data(datasetname, column, seq_len, normalise_window):\n    # A support function to help prepare datasets for an RNN\/LSTM\/GRU\n    data = datasetname.loc[:,column]\n    sequence_length = seq_len + 1\n    result = []\n    \n    for index in range(len(data) - sequence_length):\n        result.append(data[index: index + sequence_length])\n    \n    if normalise_window:\n        #result = sc.fit_transform(result)\n        result = normalise_windows(result)\n    result = np.array(result)\n\n    #Last 10% is used for validation test, first 90% for training\n    row = round(0.9 * result.shape[0])\n    train = result[:int(row), :]\n    np.random.shuffle(train)\n    x_train = train[:, :-1]\n    y_train = train[:, -1]\n    x_test = result[int(row):, :-1]\n    y_test = result[int(row):, -1]\n    \n    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))  \n\n    return [x_train, y_train, x_test, y_test]\n\ndef predict_sequence_full(model, data, window_size):\n    #Shift the window by 1 new prediction each time, re-run predictions on new window\n    curr_frame = data[0]\n    predicted = []\n    for i in range(len(data)):\n        predicted.append(model.predict(curr_frame[newaxis,:,:])[0,0])\n        curr_frame = curr_frame[1:]\n        curr_frame = np.insert(curr_frame, [window_size-1], predicted[-1], axis=0)\n    return predicted\n\ndef plot_results(predicted_data, true_data): \n    fig = plt.figure(facecolor='white') \n    ax = fig.add_subplot(111) \n    ax.plot(true_data, label='True Data') \n    plt.plot(predicted_data, label='Prediction') \n    plt.legend() \n    plt.show() ","f31a45b7":"# Let's get the stock data\ndataset = pd.read_csv('..\/input\/IBM_2006-01-01_to_2018-01-01.csv', index_col='Date', parse_dates=['Date'])\ndataset.head()","7ccef9b4":"# Prepare the dataset, note that the stock price data will be normalized between 0 and 1\n# A label is the thing we're predicting\n# A feature is an input variable, in this case a stock price\n# Selected 'Close' (stock pric at closing) attribute for prices. Let's see what it looks like\nEnrol_window = 100\nfeature_train, label_train, feature_test, label_test = load_data(dataset, 'Close', Enrol_window, True)\n\ndataset[\"Close\"][:'2016'].plot(figsize=(16,4),legend=True)\ndataset[\"Close\"]['2017':].plot(figsize=(16,4),legend=True) # 10% is used for thraining data which is approx 2017 data\nplt.legend(['Training set (First 90%, approx before 2017)','Test set (Last 10%, approax 2017 and beyond)'])\nplt.title('IBM stock price')\nplt.show()","215a96a4":"# The same LSTM model I would like to test, lets see if the sinus prediction results can be matched\n# Note: replace LSTM with GRU or RNN if you want to try those\n\nmodel = Sequential()\nmodel.add(LSTM(50, return_sequences=True, input_shape=(feature_train.shape[1],1)))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(100, return_sequences=False))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation = \"linear\"))\n\nmodel.compile(loss='mse', optimizer='adam')\n\nprint ('model compiled')","ece96561":"#Train the model\nmodel.fit(feature_train, label_train, batch_size=512, epochs=5, validation_data = (feature_test, label_test))","45a1a2c5":"#Let's use the model and predict the stock\npredicted_stock_price = model.predict(feature_test)\nplot_results(predicted_stock_price,label_test)","5ab10dac":"def predict_sequences_multiple(model, data, window_size, prediction_len):\n    #Predict sequence of <prediction_len> steps before shifting prediction run forward by <prediction_len> steps\n    prediction_seqs = []\n    for i in range(int(len(data)\/prediction_len)):\n        curr_frame = data[i*prediction_len]\n        predicted = []\n        for j in range(prediction_len):\n            predicted.append(model.predict(curr_frame[newaxis,:,:])[0,0])\n            curr_frame = curr_frame[1:]\n            curr_frame = np.insert(curr_frame, [window_size-1], predicted[-1], axis=0)\n        prediction_seqs.append(predicted)\n    return prediction_seqs\n\ndef plot_results_multiple(predicted_data, true_data, prediction_len):\n    fig = plt.figure(facecolor='white')\n    ax = fig.add_subplot(111)\n    ax.plot(true_data, label='True Data')\n    #Pad the list of predictions to shift it in the graph to it's correct start\n    for i, data in enumerate(predicted_data):\n        padding = [None for p in range(i * prediction_len)]\n        plt.plot(padding + data, label='Prediction')\n        plt.legend()\n    plt.show()\n\npredictions = predict_sequences_multiple(model, feature_test, Enrol_window, 50)\nplot_results_multiple(predictions, label_test, 50)  ","4199415a":"Everybody on Kaggle rich! This looks incredible correct, but \"if it is too good to be true, it is probably not true\".\nLet's step back and actually see what we did. We created a testset of 100 (dependend on how you set the enrol_window) actual datapoints and ask to predict nr 101 (which is probably an how close to nr 100). And we did so for each point in this graph. Hence the fantastic result, it wasn't that hard (Remember that you are looking at normalised data)\n\nLike the sinewave example we need to predict a new point based on the actual last 100 points, the next point on 99 actual points and 1 prediction, the next point on 98 actuals and 2 predictions, and so forth.\nLets make some 50 predictions ahead in the future and do this every 50 times to get a bearing how the model predicts","754b3890":"Ouch, can't use that to put some real money on the stock market. We basically knew already that you cant predict future stock prices on historic data. Pick for example the grey or purple line, it probably learned the stock went down last 100 sequence so it predicts it will go down, what would be a recognizable pattern to predict the trend will break and would go back up again after point 200. So it is nog recognisable in the historic data, else the algorithem would have found it. Maybe with a richer data set with correleated stocks? Other (News?) items? Etc Anyhow, still a nice learning example which helped me to practice with LSTM (but could also picked GRU or RNN, fw simple code changes in the model)"}}