{"cell_type":{"29aacafe":"code","3f19d833":"code","646a2236":"code","31bd32c4":"code","855a03dd":"code","836acc9f":"code","0a22a246":"code","7b014df5":"code","e291bc2a":"code","407b5a27":"code","3d179c1c":"code","ca934564":"code","65d8bc4a":"code","03b6faae":"code","acb9a09a":"code","105ab4c0":"code","e0e6dbc9":"code","cc5916fc":"code","7348d9b5":"code","c3e9dc24":"code","d031c5d8":"code","5ab7c72a":"code","a6ddf707":"code","4614ae53":"code","1f7da48e":"code","9987270e":"code","22405470":"code","bea79c92":"code","ea202784":"code","75a1bc97":"code","24f4444c":"code","b24aca70":"code","8238bdb2":"code","d2df510d":"code","bf6039df":"code","cdcd1ffb":"code","f47f8758":"code","ba2fbbf4":"code","437a63c3":"code","96aa6fca":"code","9d73b462":"code","2d7ac6b4":"code","fe22c273":"code","e1d23df2":"code","e2c23468":"markdown","7e7b7b3b":"markdown","d81619ea":"markdown","d92c3b50":"markdown","07251771":"markdown","cfe086e6":"markdown","143f2589":"markdown","59dba6ee":"markdown","bdae25fb":"markdown","7cf88c88":"markdown","561bbe29":"markdown","9c3b6c80":"markdown","53585ce2":"markdown","ab86f244":"markdown","322f63ab":"markdown","82861136":"markdown","bb3253c6":"markdown","7e32f1a1":"markdown","270a5bca":"markdown","1875f5d2":"markdown","9416b930":"markdown","17aeba9c":"markdown","a9e94339":"markdown","2761e37b":"markdown","762d8e24":"markdown","68be5814":"markdown","a676baca":"markdown","3f5009e6":"markdown","7324527b":"markdown"},"source":{"29aacafe":"import string\nimport re\nfrom os import listdir\nfrom numpy import array\nfrom nltk.corpus import stopwords\nfrom collections import Counter\n# Scikit Learn\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom subprocess import check_output\nfrom wordcloud import WordCloud, STOPWORDS\n\nfrom sklearn.linear_model import LogisticRegression\npd.set_option('display.max_colwidth', -1)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score,roc_auc_score,roc_curve\nimport numpy as np\nfrom sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\nfrom keras.preprocessing.sequence import pad_sequences\n\nimport os\nimport time\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, Dense, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D, Flatten\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras.models import Sequential\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nprint(\"Libraries loaded\")","3f19d833":"path = '..\/input\/nlp-getting-started\/'\ntrain = pd.read_csv(path + \"train.csv\")\ntest = pd.read_csv(path + \"test.csv\")\n","646a2236":"train['actual_text'] = train['text']","31bd32c4":"test.isnull().sum()","855a03dd":"train[train.target==0]['text']","836acc9f":"train[train.target==1]['text']","0a22a246":"# train = train.head(10)?\ntest_df = test","7b014df5":"train[\"target\"].value_counts() \/ len(train) *100","e291bc2a":"import seaborn as sns\nax = sns.countplot(x='target', data=train)\nax.set_xlabel('target')\nax.set_ylabel(\"count\")  ","407b5a27":"stopwords = set(STOPWORDS)\n\n\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stopwords,\n                          max_words=200,\n                          max_font_size=40, \n                          random_state=42\n                         ).generate(str(train[train.target==0]['text']))\n\nprint(wordcloud)\nfig = plt.figure(1)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()\nfig.savefig(\"word1.png\", dpi=900)","3d179c1c":"stopwords = set(STOPWORDS)\n\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stopwords,\n                          max_words=200,\n                          max_font_size=40, \n                          random_state=42\n                         ).generate(str(train[train.target==1]['text']))\n\nprint(wordcloud)\nfig = plt.figure(1)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()\nfig.savefig(\"word1.png\", dpi=300)","ca934564":"from collections import defaultdict\ntrain1_df = train[train[\"target\"]==1]\ntrain0_df = train[train[\"target\"]==0]\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n## Get the bar chart from sincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train0_df[\"text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n## Get the bar chart from insincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words of Non Diaster Tweets\", \n                                          \"Frequent words of real Diaster Tweets\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\npy.iplot(fig, filename='word-plots')\n\n#plt.figure(figsize=(10,16))\n#sns.barplot(x=\"ngram_count\", y=\"ngram\", data=fd_sorted.loc[:50,:], color=\"b\")\n#plt.title(\"Frequent words for Insincere Questions\", fontsize=16)\n#plt.show()\n","65d8bc4a":"freq_dict = defaultdict(int)\nfor sent in train0_df[\"text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=[\"Frequent bigrams of Non Diaster Tweets\", \n                                          \"Frequent bigrams of real Diaster Tweets\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Count Plots\")\npy.iplot(fig, filename='word-plots')\n","03b6faae":"## Number of words in the text ##\ntrain[\"num_words\"] = train[\"text\"].apply(lambda x: len(str(x).split()))\n\n## Number of unique words in the text ##\ntrain[\"num_unique_words\"] = train[\"text\"].apply(lambda x: len(set(str(x).split())))\n\n## Number of characters in the text ##\ntrain[\"num_chars\"] = train[\"text\"].apply(lambda x: len(str(x)))\n\n## Number of stopwords in the text ##\ntrain[\"num_stopwords\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\n## Number of punctuations in the text ##\ntrain[\"num_punctuations\"] =train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n## Number of title case words in the text ##\ntrain[\"num_words_upper\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n## Number of title case words in the text ##\ntrain[\"num_words_title\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n## Average length of the words in the text ##\ntrain[\"mean_word_len\"] = train[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n","acb9a09a":"## Truncate some extreme values for better visuals ##\ntrain['num_words'].loc[train['num_words']>60] = 60 #truncation for better visuals\ntrain['num_punctuations'].loc[train['num_punctuations']>10] = 10 #truncation for better visuals\ntrain['num_chars'].loc[train['num_chars']>350] = 350 #truncation for better visuals\n\nf, axes = plt.subplots(3, 1, figsize=(10,20))\nsns.boxplot(x='target', y='num_words', data=train, ax=axes[0])\naxes[0].set_xlabel('Target', fontsize=12)\naxes[0].set_title(\"Number of words in each class\", fontsize=15)\n\nsns.boxplot(x='target', y='num_chars', data=train, ax=axes[1])\naxes[1].set_xlabel('Target', fontsize=12)\naxes[1].set_title(\"Number of characters in each class\", fontsize=15)\n\nsns.boxplot(x='target', y='num_punctuations', data=train, ax=axes[2])\naxes[2].set_xlabel('Target', fontsize=12)\n#plt.ylabel('Number of punctuations in text', fontsize=12)\naxes[2].set_title(\"Number of punctuations in each class\", fontsize=15)\nplt.show()\n","105ab4c0":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\n\ntrain['text']=train['text'].apply(lambda x : remove_URL(x))\ntrain['text']=train['text'].apply(lambda x : remove_html(x))\ntrain['text']=train['text'].apply(lambda x : remove_punct(x))\n\n\ntest_df['text']=test_df['text'].apply(lambda x : remove_URL(x))\ntest_df['text']=test_df['text'].apply(lambda x : remove_html(x))\ntest_df['text']=test_df['text'].apply(lambda x : remove_punct(x))\n\ntrain['text']=train['text'].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\ntest_df['text']=test_df['text'].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\n\n\nimport warnings; warnings.simplefilter('ignore')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nstop = stopwords.words('english')\n\ntrain['text'] = train['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n# train['text'] = train['text'].str.replace('\\d+', '')\ntest_df['text'] = test_df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n# test_df['text'] = test_df['text'].str.replace('\\d+', '')\n\n\n\nstemmer = SnowballStemmer('english')\ntrain['text'].apply(lambda x: \" \".join([stemmer.stem(word) for word in x.split()]))\ntest_df['text'] = test_df['text'].apply(lambda x: \" \".join([stemmer.stem(word) for word in x.split()]))","e0e6dbc9":"## split to train and val\ntrain_df, val_df = train_test_split(train, test_size=0.1, random_state=2018)\n","cc5916fc":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score,roc_auc_score,roc_curve, auc,  f1_score\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import f1_score\n# Making the Confusion Matrix\ndef get_metrics(y_test, y_pred):\n    cm = confusion_matrix(y_test, y_pred)\n\n    class_names=[0,1] # name  of classes\n    fig, ax = plt.subplots()\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names)\n    plt.yticks(tick_marks, class_names)\n    # create heatmap\n    sns.heatmap(pd.DataFrame(cm), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n    ax.xaxis.set_label_position(\"top\")\n    plt.tight_layout()\n    plt.title('Confusion matrix', y=1.1)\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n\n\n    print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n\n    # Model Precision: what percentage of positive tuples are labeled as such?\n    print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n\n    # Model Recall: what percentage of positive tuples are labelled as such?\n    print(\" True positive rate or (Recall or Sensitivity) :\",metrics.recall_score(y_test, y_pred))\n\n    tn, fp, fn, tp = metrics.confusion_matrix(y_test, y_pred).ravel()\n    specificity = tn \/ (tn+fp)\n\n    #Specitivity. or True negative rate\n    print(\" True Negative rate or Specitivity :\",specificity)\n\n    false_negative = fn \/ (fn+tp)\n\n    #False negative rate\n    print(\" False Negative rate :\",false_negative)\n\n    #False positive rate\n    print(\" False positive rate (Type 1 error) :\",1 - specificity)\n    \n    print('F Score', f1_score(y_test, y_pred))\n    print(cm)\n","7348d9b5":"# ## some config values \n# Get the tfidf vectors #\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.svm import LinearSVC\n\nfrom sklearn.model_selection import GridSearchCV\n\n\ntrain_X = train_df[\"text\"].fillna(\"_na_\")\nval_X = val_df[\"text\"].fillna(\"_na_\")\ntest_X = test_df[\"text\"].fillna(\"_na_\")\n\n## Get the target values\ntrain_y = train_df['target'].values\nval_y = val_df['target'].values\n\ntfidf_vec = TfidfVectorizer(stop_words='english')\ntfidf_vec.fit_transform(train_X.values.tolist()+val_X.values.tolist())\ntrain_tfidf = tfidf_vec.transform(train_X.values.tolist())\nval_tfidf = tfidf_vec.transform(val_X.values.tolist())\ntest_tfidf = tfidf_vec.transform(test_X.values.tolist())\nprint(\"tfidf done\")\n\n\n","c3e9dc24":"import numpy as np\nfrom sklearn.dummy import DummyClassifier\n\ndummy_clf = DummyClassifier(strategy=\"most_frequent\")\nprint(dummy_clf)\ndummy_clf.fit(train_tfidf, train_y)\n\ny_dummy_clf = dummy_clf.predict(val_tfidf)\n\nget_metrics(val_y, y_dummy_clf)","d031c5d8":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier( n_estimators=100, max_depth=5,    \n                            min_samples_leaf=10, min_samples_split=20,\n                            random_state=10)\ngrid_param = {\n    'max_depth': [4,5],\n    'min_samples_leaf': [10,20]\n}\n\ngd_sr = GridSearchCV(estimator=rf,\n                     param_grid=grid_param,\n                     scoring='accuracy',\n                     cv=3,\n                     n_jobs=-1)\n\ngd_sr.fit(train_tfidf, train_y)\n\ny_pred_rf = gd_sr.predict(val_tfidf)\n\n\nget_metrics(val_y, y_pred_rf)\n","5ab7c72a":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression()\n\n\nclassifier.fit(train_tfidf, train_y)\n\ny_pred_val_lr = classifier.predict(val_tfidf)\n\nget_metrics(val_y, y_pred_val_lr)\n\n","a6ddf707":"import eli5\neli5.show_weights(classifier, vec=tfidf_vec, top=100, feature_filter=lambda x: x != '<BIAS>')","4614ae53":"# from sklearn import svm\n\n# model = svm.SVC(kernel='linear', probability=True)\n# grid_param = {\n#     'C': [1],\n#     'gamma': [0.001]\n# }\n\n# gd_sr = GridSearchCV(estimator=model,\n#                      param_grid=grid_param,\n#                      scoring='accuracy',\n#                      cv=3,\n#                      n_jobs=-1)\n\n# gd_sr.fit(train_tfidf, train_y)\n\n# y_pred_val_svm = gd_sr.predict(val_tfidf)\n\n# get_metrics(val_y, y_pred_val_svm)\n# print(gd_sr.best_params_)","1f7da48e":"temp = val_df\ntemp.reset_index(drop=True, inplace=True)\nforReview =pd.concat([temp, pd.DataFrame(y_pred_val_lr)],axis=1,ignore_index=True)\nforReview= forReview.iloc[:,[3,4,5,14]]\nforReview.columns = ['text','target','actual_text','predicted']\nforReview","9987270e":"forReview[(forReview['target']==0) & (forReview['predicted']==1)]","22405470":"forReview[(forReview['target']==1) & (forReview['predicted']==0)]","bea79c92":"# # define the model\n# model = Sequential()\n# model.add(Dense(1024, input_dim=train_tfidf.shape[1]))\n# model.add(Activation('relu'))\n\n# model.add(Dense(1024))\n# model.add(Activation('relu'))\n\n# model.add(Dense(1))\n# model.add(Activation('sigmoid'))\n# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n# print(model.summary())\n# es = EarlyStopping(monitor='val_loss', patience=5)\n# model.fit(train_tfidf,train_y, batch_size=512, epochs=1, validation_data=(val_tfidf, val_y), callbacks=[es])\n\n# val_pred_y = model.predict_classes([val_tfidf], batch_size=1024, verbose=1)\n# # val_pred_y = (val_pred_y>0.5).astype(int)\n# get_metrics(val_y,val_pred_y)\n","ea202784":"\n# ## some config values \n# embed_size = 300 # how big is each word vector\n# max_features = 5000 # how many unique words to use (i.e num rows in embedding vector)\n# maxlen = 100 # max number of words in a question to use\n\n# ## fill up the missing values\n# train_X = train_df[\"text\"].fillna(\"_na_\").values\n# val_X = val_df[\"text\"].fillna(\"_na_\").values\n# test_X = test_df[\"text\"].fillna(\"_na_\").values\n\n# ## Tokenize the sentences\n# tokenizer = Tokenizer(num_words=max_features)\n# tokenizer.fit_on_texts(list(train_X))\n# train_X = tokenizer.texts_to_sequences(train_X)\n# val_X = tokenizer.texts_to_sequences(val_X)\n# test_X = tokenizer.texts_to_sequences(test_X)\n\n# ## Pad the sentences \n# train_X = pad_sequences(train_X, maxlen=maxlen)\n# val_X = pad_sequences(val_X, maxlen=maxlen)\n# test_X = pad_sequences(test_X, maxlen=maxlen)\n\n# ## Get the target values\n# train_y = train_df['target'].values\n# val_y = val_df['target'].values\n","75a1bc97":"# embedding_vecor_length = 32\n# model = Sequential()\n# model.add(Embedding(max_features, embedding_vecor_length, input_length=maxlen))\n# model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n\n# model.add(Dense(1024))\n# model.add(Activation('relu'))\n# model.add(Dropout(.2))\n\n# model.add(Dense(1, activation='sigmoid'))\n# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# print(model.summary())\n# es = EarlyStopping(monitor='val_loss', patience=10)\n# model.fit(train_X, train_y, batch_size=512, epochs=1, validation_data=(val_X, val_y), callbacks=[es])\n\n# val_pred_y = model.predict_classes([val_X], batch_size=1024, verbose=1)\n# get_metrics(val_y,val_pred_y)\n","24f4444c":"# import numpy as np\n# EMBEDDING_FILE = '..\/input\/glove840b300dtxt\/glove.840B.300d.txt'\n# def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n# embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n\n# all_embs = np.stack(embeddings_index.values())\n# emb_mean,emb_std = all_embs.mean(), all_embs.std()\n# embed_size = all_embs.shape[1]\n\n# word_index = tokenizer.word_index\n# nb_words = min(max_features, len(word_index)) + 1\n# embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n# for word, i in word_index.items():\n#     if i >= max_features: continue\n#     embedding_vector = embeddings_index.get(word)\n#     if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n        ","b24aca70":"\n# # define the model\n# model = Sequential()\n\n# e = Embedding(len(embedding_matrix), embed_size, weights=[embedding_matrix], input_length=maxlen, trainable=False)\n# model.add(e)\n# model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n# model.add(Dense(1, activation='sigmoid'))\n# # compile the model\n# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n# # summarize the model\n# print(model.summary())\n# es = EarlyStopping(monitor='val_loss', patience=10)\n# model.fit(train_X, train_y, batch_size=512, epochs=10, validation_data=(val_X, val_y), callbacks=[es])\n\n# val_pred_y = model.predict_classes([val_X], batch_size=1024, verbose=1)\n# get_metrics(val_y,val_pred_y)\n\n# # pred_glove_embed_y = model.predict([test_X], batch_size=1024, verbose=1)","8238bdb2":"# !pip install tokenizers","d2df510d":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nimport torch\nimport transformers as ppb\nimport warnings\nwarnings.filterwarnings('ignore')","bf6039df":"path = '..\/input\/nlp-getting-started\/'\ntrain = pd.read_csv(path + \"train.csv\")\ntest = pd.read_csv(path + \"test.csv\")\n\ntrain = train[:2000]\n## split to train and val\n# train_df, val_df = train_test_split(train, test_size=0.2, random_state=2018)","cdcd1ffb":"# For DistilBERT:\nmodel_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n\n## Want BERT instead of distilBERT? Uncomment the following line:\n#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n\n# Load pretrained model\/tokenizer\ntokenizer = tokenizer_class.from_pretrained(pretrained_weights)\nmodel = model_class.from_pretrained(pretrained_weights)","f47f8758":"tokenized = train['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))","ba2fbbf4":"max_len = 0\nfor i in tokenized.values:\n    if len(i) > max_len:\n        max_len = len(i)\n\npadded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])","437a63c3":"np.array(padded).shape","96aa6fca":"attention_mask = np.where(padded != 0, 1, 0)\nattention_mask.shape","9d73b462":"input_ids = torch.tensor(padded)  \nattention_mask = torch.tensor(attention_mask)\n\nwith torch.no_grad():\n    last_hidden_states = model(input_ids, attention_mask=attention_mask)","2d7ac6b4":"last_hidden_states","fe22c273":"features = last_hidden_states[0][:,0,:].numpy()\nlabels = train.target\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels)","e1d23df2":"lr_clf = LogisticRegression()\nlr_clf.fit(train_features, train_labels)\nlr_clf.score(test_features, test_labels)","e2c23468":"**Word Cloud:**\n\nNow let us look at the frequently occuring words in the data by creating a word cloud on the 'text' column.\n\n**1) Word cloud of Not disaster tweet**","7e7b7b3b":"Masking","d81619ea":"\nObjective of the notebook is to explore the data, to build and compare baseline models.\n\n**Objective of the competition:**\n\nThe objective is to predict which Tweets are about real disasters and which ones are not\nPredict whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.\n\n","d92c3b50":"**Now try with Glove embedding Embedding**","07251771":"**Observations:**\n* Some of the top words are common across both the classes like 'people', emergency, building\n\nNow let us also create bigram frequency plots for both the classes separately to get more idea.","cfe086e6":"** Code for Confusion matrix**","143f2589":"**Meta Features:**\n\nNow let us create some meta features and then look at how they are distributed between the classes. The ones that we will create are\n1. Number of words in the text\n2. Number of unique words in the text\n3. Number of characters in the text\n4. Number of stopwords\n5. Number of punctuations\n6. Number of upper case words\n7. Number of title case words\n8. Average length of the words","59dba6ee":"# NLP:\n* EDA (with WordCloud) \n* Bag of Words \n* TF IDF\n* SVM, RF, using TFIDF\n* Neural Network Using TFIDF\n* Recurrent Neural Network\n* Recurrent Network With Glove Embedding\n","bdae25fb":"** Recurrent Neural Network **","7cf88c88":"\nTo start with, let us just build a baseline model (Logistic Regression) with TFIDF vectors.\n\n** TF IDF **","561bbe29":"** Build a DummyClassifier which works on basic rules **","9c3b6c80":"**Not Disaster tweets**","53585ce2":"About 42 % of the data is about real disasters","ab86f244":"Lets check some tweets that are misclassified","322f63ab":"**Target Distribution:**\n\nFirst let us look at the distribution of the target variable to understand more about the imbalance and so on.","82861136":"**Inference:**\n* We can see that the tweets of both classes has more or less same words\n\n**Text Preprocessing:**\n* Lower casing\n* Removal of Punctuations\n* Removal of Stopwords\n* Stemming\n* Removal of URLs\n* Removal of HTML tags","bb3253c6":"Padding","7e32f1a1":"**Without Pretrained Embeddings:**\n","270a5bca":"Now let us look at the important words used for classifying the real vs not real disaster tweets\n","1875f5d2":"Tweets that got misclassified as Disaster tweets","9416b930":"**Read the data**","17aeba9c":"\n** Now, Lets try out a Random Forest Classifier**","a9e94339":"**2) Word cloud of Real Disaster Tweet**","2761e37b":"** SVM **","762d8e24":"We can see that the Accuracy is not good with the Random Forest.\n\n** Lets try out Logistic Regression **","68be5814":"**Disaster tweets**","a676baca":"**Neural Network**","3f5009e6":"BERT","7324527b":"Disaster Tweets that got misclassified as Not Disaster tweets"}}