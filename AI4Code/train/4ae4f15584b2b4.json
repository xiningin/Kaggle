{"cell_type":{"377883f4":"code","cfa89ad2":"code","52fd6aa7":"code","6b0158ff":"code","43fe4335":"code","65cb616f":"code","a3cc9915":"code","2636b2e9":"code","bc4125dc":"markdown","11421df5":"markdown","37c8b594":"markdown","89f398c2":"markdown","ab05ada6":"markdown","d92be9a6":"markdown","e6704e66":"markdown","32579bdd":"markdown"},"source":{"377883f4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cfa89ad2":"data_dec_18 = pd.read_csv(\"..\/input\/form-477-1218\/tract_map_dec_2018.csv\", converters={'tractcode': lambda x: str(x)})\ndata_jun_18 = pd.read_csv(\"..\/input\/form-4770618\/tract_map_jun_2018.csv\", converters={'tractcode': lambda x: str(x)})\ndata_dec_17 = pd.read_csv(\"..\/input\/form477-16-17\/tract_map_dec_2017.csv\", converters={'tractcode': lambda x: str(x)})\ndata_jun_17 = pd.read_csv(\"..\/input\/form477-16-17\/tract_map_jun_2017.csv\", converters={'tractcode': lambda x: str(x)})\ndata_dec_16 = pd.read_csv(\"..\/input\/form477-16-17\/tract_map_dec_2016.csv\", converters={'tractcode': lambda x: str(x)})\ndata_jun_16 = pd.read_csv(\"..\/input\/form477-16-17\/tract_map_jun_2016.csv\", converters={'tractcode': lambda x: str(x)})\ndata_dec_15 = pd.read_csv(\"..\/input\/form-477-2015\/tract_map_dec_2015.csv\", converters={'tractcode': lambda x: str(x)})\ndata_jun_15 = pd.read_csv(\"..\/input\/form-477-2015\/tract_map_jun_2015.csv\", converters={'tractcode': lambda x: str(x)})\n\nprint([data_jun_18.count(), data_dec_17.count(), data_jun_17.count(), data_dec_16.count(), data_jun_16.count(), data_dec_15.count(), data_jun_15.count()])\n\n    ","52fd6aa7":"data_dec_18 = data_dec_18.rename(columns={'pcat_10x1': 'Dec_2018'})\ndata_jun_18 = data_jun_18.rename(columns={'pcat_10x1': 'Jun_2018'})\ndata_dec_17 = data_dec_17.rename(columns={'pcat_10x1': 'Dec_2017'})\ndata_jun_17 = data_jun_17.rename(columns={'pcat_10x1': 'Jun_2017'})\ndata_dec_16 = data_dec_16.rename(columns={'pcat_10x1': 'Dec_2016'})\ndata_jun_16 = data_jun_16.rename(columns={'pcat_10x1': 'Jun_2016'})\ndata_dec_15 = data_dec_15.rename(columns={'pcat_10x1': 'Dec_2015'})\ndata_jun_15 = data_jun_15.rename(columns={'pcat_10x1': 'Jun_2015'})\n\ndata_dec_18 = data_dec_18.drop(columns=['pcat_all'])\ndata_jun_18 = data_jun_18.drop(columns=['pcat_all'])\ndata_dec_17 = data_dec_17.drop(columns=['pcat_all'])\ndata_jun_17 = data_jun_17.drop(columns=['pcat_all'])\ndata_dec_16 = data_dec_16.drop(columns=['pcat_all'])\ndata_jun_16 = data_jun_16.drop(columns=['pcat_all'])\ndata_dec_15 = data_dec_15.drop(columns=['pcat_all'])\ndata_jun_15 = data_jun_15.drop(columns=['pcat_all'])\n    \nchange_connections = pd.merge(data_jun_15, data_dec_15)\ndatasets = [data_jun_16, data_dec_16, data_jun_17, data_dec_17, data_jun_18, data_dec_18]\n\nfor data in datasets:\n    change_connections = pd.merge(change_connections, data)\n\nchange_connections.head()","6b0158ff":"change_connections.isnull().values.any()","43fe4335":"census_tracts_exclude = ['02','15','72','60','66','78','69']\n\nfor census_tract in census_tracts_exclude:\n    change_connections = change_connections[~change_connections['tractcode'].astype(str).str.startswith(census_tract)]\nchange_connections.info()","65cb616f":"change_connections.isnull().values.any()","a3cc9915":"change_connections","2636b2e9":"change_connections.to_csv('\/kaggle\/working\/fcc_form477_residential_fixed_connections_10x1_2015_2018_updated.csv', index = False, header=True)","bc4125dc":"Now, we have a combined dataset with which we can view changes in Residential Fixed High-Speed Connections (at least 10 Mbps downstream and at least 1 Mbps upstream per 1000 Households) in a certain census tract in the continental United States (CONUS).  \n\nHere are the following codes:  \n0 == 0 connections per 1000 households  \n1 == 0< x <=200 connections per 1000 households  \n2 == 200 < x <=400 connections per 1000 households  \n3 == 400 < x <=600 connections per 1000 households  \n4 == 600 < x <=800 connections per 1000 households  \n5 == 800 < x connections per 1000 households  ","11421df5":"All datasets have 73767 rows.  Thus they all seem to be compatible with one another for merging. ","37c8b594":"Census tract codes are set up so that the first 2 digits in the census tract represent the state.  As such, any census tract that starts in \"01\" means it's an Alabama census tract, \"02\" is an Alaska census tract, etc.  \n\nWe are dropping the census tracts starting with the following:  \n02 - Alaska  \n15 - Hawaii  \n72 - Puerto Rico  \n60 - American Samoa  \n66 - Guam  \n78 - US Virgin Islands  \n69 - Northern Marinara Islands  ","89f398c2":"Merged all dataframes into one dataframe. Here, we also made sure the \"tractcode\" column in the dataframe are strings because 1) we want to preserve the leading zeros in the tractcodes and 2) we want to make the tractcodes easier to search through.  ","ab05ada6":"Check for missing values again just for good measure","d92be9a6":"save as csv","e6704e66":"Tasks:  \n1. Combine data from 8 different FCC Form 477 Datasets that measure census tract data on internet access services from the period of June 2015 - December 2018 (most recent data from the FCC: https:\/\/www.fcc.gov\/form-477-census-tract-data-internet-access-services).\n2. Check if all the datasets are compatible with each other (i.e. check for same number of rows) & check for missing values. \n3. Delete extraneous data -- we've decided to exclude Alaska, Hawaii, and US territores in this investigation due to unique population distributions and unique challenges when it comes to building infrastructure in those areas not present in the continential US (CONUS).  Thus we will focus only on CONUS.\n\nThe goal is to have a suitable dataset where we can analyze changes over time (June 2015 - December 2018) in residential fixed high-speed connections (at least 10 Mbps downstream and at least 1 Mbps upstream per 1000 Households) for census tracts in CONUS. ","32579bdd":"We have no missing values in this new combined dataset."}}