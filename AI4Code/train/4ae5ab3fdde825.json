{"cell_type":{"117d1ddf":"code","717a1a1c":"code","9a224012":"code","815f96d9":"code","d7d3e679":"code","67bbff91":"code","ef12c1b3":"code","6f8c650d":"code","ff9ada33":"code","0cb366e9":"code","baa16872":"code","81fb4a4e":"code","7b8b9e27":"code","26d7f61c":"code","fe410555":"code","b9b67c84":"code","e961a5a3":"code","2ae752b5":"code","fdda7b45":"code","4f83e1f2":"code","cd9bb9f5":"code","1c9d6a49":"code","1325d2ff":"code","d7dc4d0b":"code","a6c4cfde":"code","65e3ce32":"code","339931a2":"code","d5aba123":"code","c2aefdc2":"markdown","6228de35":"markdown","16f812aa":"markdown","ffde312f":"markdown","8a09891b":"markdown","66c6d9b6":"markdown","66c2018c":"markdown","0cf01d68":"markdown","f7ca2803":"markdown","552c985c":"markdown","ab2e0a53":"markdown","15e47d53":"markdown","d9a39c10":"markdown","fa3c494a":"markdown","9666fe39":"markdown","e14c275d":"markdown","dfb8d782":"markdown","354ba500":"markdown","b9048927":"markdown"},"source":{"117d1ddf":"!pip install langdetect","717a1a1c":"# Import Libraries\nimport os\nimport time\nimport missingno as msno\nimport pandas as pd\nimport geopandas as gpd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk, re, string, collections, unicodedata\n\n%matplotlib inline\nfrom matplotlib import cm, dates\nfrom matplotlib.ticker import ScalarFormatter\nfrom matplotlib.ticker import FuncFormatter\nfrom datetime import datetime, timedelta\nfrom textblob import TextBlob\nfrom wordcloud import WordCloud, STOPWORDS\nfrom langdetect import detect\nfrom nltk.util import ngrams\nfrom nltk.tokenize import word_tokenize\nfrom nltk.classify import NaiveBayesClassifier\nfrom nltk.corpus import subjectivity\nfrom nltk.sentiment import SentimentAnalyzer\nfrom nltk.sentiment.util import *\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9a224012":"# Reading both the csv files\ntweets_biden = pd.read_csv('\/kaggle\/input\/us-election-2020-tweets\/hashtag_joebiden.csv', lineterminator='\\n', parse_dates=True)\ntweets_trump = pd.read_csv('\/kaggle\/input\/us-election-2020-tweets\/hashtag_donaldtrump.csv', lineterminator='\\n', parse_dates=True)\n\n# Clean data\ntweets_biden['country'].replace({'United States':'United States of America'}, inplace=True)\ntweets_trump['country'].replace({'United States':'United States of America'}, inplace=True)\n\n\n# Add Features\ndef normalise(x,y):\n    x = np.array(x)\n    y = np.array(y)\n    return np.where(x == 0, 0, x \/ y)\n\ndef sentiment(data):\n    temp=[]\n    for row in data:\n        tmp=sid.polarity_scores(row)\n        temp.append(tmp)\n    return temp\n\n# convert to datetime object\ntweets_biden['user_join_date']=pd.to_datetime(tweets_biden['user_join_date'])\ntweets_trump['user_join_date']=pd.to_datetime(tweets_trump['user_join_date'])\ntweets_biden['collected_at']=pd.to_datetime(tweets_biden['collected_at'])\ntweets_trump['collected_at']=pd.to_datetime(tweets_trump['collected_at'])\ntweets_biden['created_at']=pd.to_datetime(tweets_biden['created_at'])\ntweets_trump['created_at']=pd.to_datetime(tweets_trump['created_at'])\n\n# create additional date time columns\ntweets_biden['created_at_r']=tweets_biden['created_at'].dt.strftime('%Y-%m-%d %H')\ntweets_trump['created_at_r']=tweets_trump['created_at'].dt.strftime('%Y-%m-%d %H')\ntweets_biden['created_at_r2']=tweets_biden['created_at'].dt.strftime('%m-%d')\ntweets_trump['created_at_r2']=tweets_trump['created_at'].dt.strftime('%m-%d')\n\n# normalise likes and retweets to allow fair analysis\nb_tdiff=(tweets_biden['collected_at'] - tweets_biden['created_at'])\nt_tdiff=(tweets_trump['collected_at'] - tweets_trump['created_at'])\nb_tdiff=(b_tdiff.dt.days * 24 + b_tdiff.dt.seconds \/ 3600)\nt_tdiff=(t_tdiff.dt.days * 24 + t_tdiff.dt.seconds \/ 3600)\n\n# Use numpy vectorisation to create new columns for normalised likes and retweets\ntweets_biden['likes_norm'] = normalise(tweets_biden['likes'],b_tdiff)\ntweets_biden['retweet_norm'] = normalise(tweets_biden['retweet_count'],b_tdiff)\ntweets_trump['likes_norm'] = normalise(tweets_trump['likes'],t_tdiff)\ntweets_trump['retweet_norm'] = normalise(tweets_trump['retweet_count'],t_tdiff)","815f96d9":"# Visualisation args\ncmap = sns.diverging_palette(0, 230, 90, 60, as_cmap=True)\nbarcolors = ['#87B88C','#9ED2A1','#E7E8CB','#48A0C9','#2A58A1','#2E8B55','#DF3659','Grey']\nbarstyle = {\"edgecolor\":\"black\", \"linewidth\":1}\nheatmap1_args = dict(annot=True, fmt='.0f', square=False, cmap=cm.get_cmap(\"RdGy\", 10), center = 90, vmin=0, vmax=10000, lw=4, cbar=False)\nheatmap2_args = dict(annot=True, fmt='.3f', square=False, cmap=\"Greens\", center = 0.5, lw=4, cbar=False)\nheatmap3_args = dict(annot=True, fmt='.0f', square=False, cmap=cmap, center = 9200, lw=4, cbar=False)\n\ndef hide_axes(this_ax):\n    this_ax.set_frame_on(False)\n    this_ax.set_xticks([])\n    this_ax.set_yticks([])\n    return this_ax\n\ndef draw_heatmap1(df,this_ax):\n    hm = sns.heatmap(df, ax = this_ax, **heatmap1_args)\n    this_ax.set_yticklabels(this_ax.get_yticklabels(), rotation=0)\n    this_ax.yaxis.tick_right()\n    this_ax.yaxis.set_label_position(\"right\")\n    for axis in ['top','bottom','left','right']:\n        this_ax.spines[axis].set_visible(True)\n        this_ax.spines[axis].set_color('black')\n    return hm \n\ndef draw_heatmap2(df,this_ax):\n    hm = sns.heatmap(df, ax = this_ax, **heatmap2_args)\n    this_ax.set_yticklabels(this_ax.get_yticklabels(), rotation=0)\n    this_ax.yaxis.tick_right()\n    this_ax.yaxis.set_label_position(\"right\")\n    for axis in ['top','bottom','left','right']:\n        this_ax.spines[axis].set_visible(True)\n        this_ax.spines[axis].set_color('black')\n    return hm \n\ndef draw_heatmap3(df,this_ax):\n    hm = sns.heatmap(df, ax = this_ax, **heatmap3_args)\n    this_ax.set_yticklabels(this_ax.get_yticklabels(), rotation=0)\n    this_ax.yaxis.tick_right()\n    this_ax.yaxis.set_label_position(\"right\")\n    for axis in ['top','bottom','left','right']:\n        this_ax.spines[axis].set_visible(True)\n        this_ax.spines[axis].set_color('black')\n    return hm \n\ndef thousands1(x, pos):\n    'The two args are the value and tick position'\n    return '%1.0fK' % (x * 1e-3)\n\nformatterK1 = FuncFormatter(thousands1)\n\ndef thousands2(x, pos):\n    'The two args are the value and tick position'\n    return '%1.1fK' % (x * 1e-3)\n\nformatterK2 = FuncFormatter(thousands2)","d7d3e679":"na_vals_b=pd.DataFrame({'Null Values':tweets_biden.isna().sum()})\nna_vals_b=na_vals_b.loc[na_vals_b['Null Values'] > 0]\nna_vals_t=pd.DataFrame({'Null Values':tweets_trump.isna().sum()})\nna_vals_t=na_vals_t.loc[na_vals_t['Null Values'] > 0]\n\n# Null values visualisation for tweets about Joe Biden and Donald Trump\nfig, ax=plt.subplots(2,1, figsize=(8,8), gridspec_kw={'hspace':0.7})\n\nna_vals_b.plot.bar(color=barcolors[3], **barstyle, ax=ax[0])\nax[0].set_title('Joe Biden Dataset')\nax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=90)\n\nna_vals_t.plot.bar(color=barcolors[6], **barstyle, ax=ax[1])\nax[1].set_title('Donald Trump Dataset')\nax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=90)\nplt.show()","67bbff91":"source_df=pd.concat([tweets_biden[['source','tweet','country']].copy(),tweets_trump[['source','tweet','country']].copy()])\nsource_df['country'].replace({'United States of America':'United States'}, inplace=True)\nsource_df=source_df.fillna('Geo Data NA')\nsource_df=source_df.drop_duplicates()\n\nsources=pd.DataFrame(source_df.groupby(['source'])['tweet'].count().sort_values(ascending=False)[:6])\nsources=sources.reset_index()\nsourceslst=sources.source.to_list()\n\ncountry=pd.DataFrame(source_df.groupby(['country'])['tweet'].count().sort_values(ascending=False)[:6])\ncountry=country.reset_index()\ncountrylst=country.country.to_list()\n\nplatXtab=pd.DataFrame(source_df.groupby(['source','country'])['tweet'].count().unstack().fillna(0))\n\nfig, ax=plt.subplots(2,2, figsize=(9,9), \n                     gridspec_kw={'height_ratios':[2,5], 'width_ratios':[2,5], 'wspace':0.1, 'hspace':0.1})\n\nhide_ax = ax[0,0]\nhide_axes(hide_ax)\n\nhm_ax = ax[1,1]\ndraw_heatmap1(platXtab.loc[sourceslst,countrylst], hm_ax)\nhm_ax.set_xlabel('tweets country of origin')\nhm_ax.set_ylabel('platform used for tweets')\nhm_ax.set_yticklabels(('Twitter Web','iPhone','Android','iPad','TweetDeck','Hootsuite'), rotation=0) \n\nbar_ax = ax[0,1]\nplatXtab.loc[sourceslst,countrylst].sum().plot.bar(ax=bar_ax, color=barcolors[1],**barstyle)\nbar_ax.set_xlabel(bar_ax.get_xlabel())\nbar_ax.xaxis.tick_top()\nbar_ax.xaxis.set_label_position(\"top\")\nbar_ax.yaxis.set_major_formatter(formatterK1)\nbar_ax.set_xticklabels(('NA', 'US', 'UK', 'CAN', 'GE','FRA'), rotation=0) \nbar_ax.set_xlabel('')\nbar_ax.set_ylabel('# tweets')\n\nbarh_ax = ax[1,0]\nplatXtab.loc[sourceslst,countrylst].sum(axis=1)[::-1].plot.barh(ax=barh_ax, color=barcolors[2],**barstyle)\nbarh_ax.yaxis.set_label_position(\"left\")\nbarh_ax.xaxis.tick_top()\nbarh_ax.xaxis.set_label_position(\"top\")\nbarh_ax.xaxis.set_major_formatter(formatterK1)\nbarh_ax.set_xlim(barh_ax.get_xlim()[::-1])\nbarh_ax.set_yticklabels(('Hootsuite','TweetDeck','iPad','Android','iPhone','Twitter Web'), rotation=0) \nbarh_ax.set_xlabel('# tweets')\nbarh_ax.set_ylabel('')\nplt.show()","ef12c1b3":"world=gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\nfrom shapely.geometry import Point, Polygon\ncrs = {'init': 'EPSG:4326'}\n\ntmp=pd.concat([tweets_biden[['lat','long']].copy(),tweets_trump[['lat','long']].copy()])\ntmp=tmp.dropna()\ngeometry = [Point(xy) for xy in zip(tmp['long'],tmp['lat'])]\ngeo_df=gpd.GeoDataFrame(tmp, crs=crs, geometry = geometry)","6f8c650d":"fig, ax = plt.subplots(1,figsize=(16,8), facecolor='lightblue')\nworld = world[world.name != \"Antarctica\"]\nworld.plot(ax=ax, cmap='Greens', edgecolors='black')\ngeo_df.plot(ax=ax, markersize=1, color='m', marker='o')\nax.axis('off')\nplt.show()","ff9ada33":"%%time\ndef detect_tweetlang(tweet):\n    try:\n        return detect(tweet)\n    except:\n        return 'unknown'\n\n# Combine two data files and drop duplicates\nlang_df=pd.concat([tweets_biden[['tweet','country']].copy(),tweets_trump[['tweet','country']].copy()])\nlang_df['country'].replace({'United States of America':'United States'}, inplace=True)\nlang_df=lang_df.fillna('Geo Data NA')\nlang_df=lang_df.drop_duplicates()\n\n# Randomly sample data for langauge analysis\nlang_smdf=lang_df.sample(n=4000).copy()\nlang_smdf['lang'] = lang_smdf['tweet'].apply(detect_tweetlang)\n\n# Select top five languages and five countries for heatmap\nlangs=pd.DataFrame(lang_smdf.groupby(['lang'])['tweet'].count().sort_values(ascending=False)[:5])\nlangs=langs.reset_index()\nlangslst=langs.lang.to_list()\n\ncountry=pd.DataFrame(lang_smdf.groupby(['country'])['tweet'].count().sort_values(ascending=False)[:5])\ncountry=country.reset_index()\ncountrylst=country.country.to_list()\n\n# Create a crosstab to feed data to heatmap\nlangXtab=pd.crosstab(lang_smdf.lang, lang_smdf.country, normalize=True)","0cb366e9":"fig, ax=plt.subplots(2,2, figsize=(8,8), \n                     gridspec_kw={'height_ratios':[2,5], 'width_ratios':[2,5], 'wspace':0.1, 'hspace':0.1})\n\nhide_ax = ax[0,0]\nhide_axes(hide_ax)\n\nhm_ax = ax[1,1]\ndraw_heatmap2(langXtab.loc[langslst,countrylst], hm_ax)\nhm_ax.set_yticklabels(('English', 'Dutch', 'German','Spanish','French'), rotation=0) \nhm_ax.set_xlabel('tweets country of origin')\nhm_ax.set_ylabel('language used for tweets')\n\nbar_ax = ax[0,1]\nlangXtab.loc[langslst,countrylst].sum().plot.bar(ax=bar_ax, color=barcolors[1],**barstyle)\nbar_ax.set_xlabel(bar_ax.get_xlabel())\nbar_ax.xaxis.tick_top()\nbar_ax.xaxis.set_label_position(\"top\")\nbar_ax.set_xticklabels(('NA','US', 'GB', 'CAN', 'GE', 'FRA'), rotation=0) \nbar_ax.set_ylim([0, 0.5])\nbar_ax.set_xlabel('')\nbar_ax.set_ylabel('# proportion tweets')\n\nbarh_ax = ax[1,0]\nlangXtab.loc[langslst,countrylst].sum(axis=1)[::-1].plot.barh(ax=barh_ax, color=barcolors[5],**barstyle)\nbarh_ax.yaxis.set_label_position(\"left\")\nbarh_ax.xaxis.tick_top()\nbarh_ax.xaxis.set_label_position(\"top\")\nbarh_ax.set_xlim(barh_ax.get_xlim()[::-1])\nbarh_ax.set_xlim([0.8, 0])\nbarh_ax.set_xlabel('# proportion tweets')\nbarh_ax.set_ylabel('')\nbarh_ax.set_yticklabels(langslst[::-1], rotation=0) \n\nplt.show()","baa16872":"# Identify the common UserId's in both datasets and create tables for feed visualisation\ncommon_ids=np.intersect1d(tweets_biden.user_id, tweets_trump.user_id)\nunique_b=tweets_biden[~tweets_biden.user_id.isin(common_ids)].copy()\ncommon_b=tweets_biden[tweets_biden.user_id.isin(common_ids)].copy()\nunique_t=tweets_trump[~tweets_trump.user_id.isin(common_ids)].copy()\ncommon_t=tweets_trump[tweets_trump.user_id.isin(common_ids)].copy()\n\ncommon_df=pd.concat([common_b,common_t])\ncommon_df=common_df.drop_duplicates()\n\n# Create columns for visualiation\nunique_b['usertype'] = 'Biden'\nunique_t['usertype'] = 'Trump'\ncommon_df['usertype'] = 'Both'\n\n# Narrow down data\ncont_df=pd.concat([unique_b[['tweet','continent','usertype']].copy(),\n                   unique_t[['tweet','continent','usertype']].copy(),\n                   common_df[['tweet','continent','usertype']].copy()])\n\n# Label NA Geo Data\ncont_df=cont_df.fillna('Geo Data NA')\n\n# Calculate tweet counts for each usertype and continuent\nusertype=pd.DataFrame(cont_df.groupby(['usertype'])['tweet'].count().sort_values(ascending=False))\nusertype=usertype.reset_index()\nuserlst=usertype.usertype.tolist()\n\ncontinent=pd.DataFrame(cont_df.groupby(['continent'])['tweet'].count().sort_values(ascending=False)[:6])\ncontinent=continent.reset_index()\ncontlst=continent.continent.to_list()\n\n# Create crosstab to feed heatmap\ncontXtab=pd.crosstab(cont_df.continent, cont_df.usertype)","81fb4a4e":"fig, ax=plt.subplots(2,2, figsize=(5.5,9), \n                     gridspec_kw={'height_ratios':[2,5], 'width_ratios':[2,3], 'wspace':0.15, 'hspace':0.1})\n\nhide_ax = ax[0,0]\nhide_axes(hide_ax)\n\nhm_ax = ax[1,1]\ndraw_heatmap3(contXtab.loc[contlst,userlst], hm_ax)\nhm_ax.set_xlabel('UserId Membership')\nhm_ax.set_ylabel('tweets continent of origin')\n\nbar_ax = ax[0,1]\ncontXtab.loc[contlst,userlst].sum().plot.bar(ax=bar_ax, color=barcolors[7],**barstyle)\nbar_ax.set_xlabel(bar_ax.get_xlabel())\nbar_ax.xaxis.tick_top()\nbar_ax.xaxis.set_label_position(\"top\")\nbar_ax.yaxis.set_major_formatter(formatterK1)\nbar_ax.set_ylabel('# tweets')\nbar_ax.set_xlabel('')\n\nbarh_ax = ax[1,0]\ncontXtab.loc[contlst,userlst].sum(axis=1)[::-1].plot.barh(ax=barh_ax, color=barcolors[4],**barstyle)\nbarh_ax.yaxis.set_label_position(\"left\")\nbarh_ax.xaxis.tick_top()\nbarh_ax.xaxis.set_label_position(\"top\")\nbarh_ax.xaxis.set_major_formatter(formatterK1)\nbarh_ax.set_xlim(barh_ax.get_xlim()[::-1])\nbarh_ax.set_xlabel('# tweets')\nbarh_ax.set_ylabel('')\n\nplt.show()","7b8b9e27":"# Identify common tweet creation dates\ncommon_creat=np.intersect1d(tweets_biden.created_at_r, tweets_trump.created_at_r)\n\n# Mask out data to ensure common lenth arrays to feed visualisation\ncnt_tbiden=tweets_biden[tweets_biden.created_at_r.isin(common_creat)]['created_at_r'].value_counts().sort_index()\ncnt_ttrump=tweets_trump[tweets_trump.created_at_r.isin(common_creat)]['created_at_r'].value_counts().sort_index()","26d7f61c":"plt.figure(figsize=(12,5))\np6=sns.lineplot(cnt_tbiden.index, cnt_tbiden.values, color=barcolors[3], label='Biden Dataset')\np6.set_title('Count of tweets per hour')\np6=sns.lineplot(cnt_ttrump.index, cnt_ttrump.values, color=barcolors[6], label='Trump Dataset')\np6.set_xticks(range(0, len(cnt_tbiden.index), 24))\np6.set_xticklabels(common_df['created_at'].dt.strftime('%m-%d').unique().tolist())\np6.set_yscale('log')\nplt.show()","fe410555":"fig, ax=plt.subplots(1,1, figsize=(12,5))\n\nline_ax = ax\nline_ax.set_title('Kernel Distribution of Number of tweets posted by users')\nsns.kdeplot(tweets_biden.groupby(['user_id'])['tweet'].count(), shade=True, color='b', label='Biden Dataset', ax=line_ax)\nsns.kdeplot(tweets_trump.groupby(['user_id'])['tweet'].count(), shade=True, color='r', label='Trump Dataset', ax=line_ax)\nplt.show()","b9b67c84":"fig, ax=plt.subplots(1,1, figsize=(12,5))\n\nline_ax = ax\nline_ax.set_title('Kernel Distribution of Normalised Likes (Per Hour)')\nsns.kdeplot(tweets_biden['likes_norm'],bw=0.1, shade=True, color='b', label='Biden Dataset', ax=line_ax)\nsns.kdeplot(tweets_trump['likes_norm'],bw=0.1, shade=True, color='r', label='Trump Dataset', ax=line_ax)\nline_ax.set_yscale('log')\nplt.show()","e961a5a3":"fig, ax=plt.subplots(1,1, figsize=(12,5))\n\nline_ax = ax\nline_ax.set_title('Kernel Distribution of Normalised Retweets (Per Hour)')\nsns.kdeplot(tweets_biden['retweet_norm'],bw=0.1, shade=True, color='b', label='Biden Dataset', ax=line_ax)\nsns.kdeplot(tweets_trump['retweet_norm'],bw=0.1, shade=True, color='r', label='Trump Dataset', ax=line_ax)\nline_ax.set_yscale('log')\nplt.show()","2ae752b5":"fig, ax=plt.subplots(1,1, figsize=(12,5))\ntweets_biden['user_join_length'] = max(tweets_biden['created_at']) - tweets_biden['user_join_date']\ntweets_trump['user_join_length'] = max(tweets_trump['created_at']) - tweets_trump['user_join_date']\n\nline_ax = ax\nplt.xlim(-2, 16)\nline_ax.set_title('Kernel Distribution of Years Since Joining')\nsns.kdeplot(tweets_biden['user_join_length'].dt.days\/365.25, shade=True, color='b', label='Biden Dataset', ax=line_ax)\nsns.kdeplot(tweets_trump['user_join_length'].dt.days\/365.25, shade=True, color='r', label='Trump Dataset', ax=line_ax)\nplt.show()","fdda7b45":"# Obtain tweets only from data that has Geo Data from the US\ntext1=tweets_biden.loc[tweets_biden['country'] == 'United States of America']['tweet']\ntext2=tweets_trump.loc[tweets_trump['country'] == 'United States of America']['tweet']","4f83e1f2":"def clean1(sent):\n    filtered_sent=\"\"\n    stopwords = nltk.corpus.stopwords.words('english')\n    sent = (unicodedata.normalize('NFKD', sent)\n            .encode('ascii', 'ignore')\n            .decode('utf-8', 'ignore')\n            .lower())\n    sent = re.sub(r'#.+|https.+|[^(a-zA-Z)\\s]','',sent)\n    words=sent.split()\n    for word in words:\n        if word not in stopwords:\n            filtered_sent=filtered_sent+' '+word\n    return filtered_sent\n\ndef clean2(text):\n    wnl = nltk.stem.WordNetLemmatizer()\n    stopwords = nltk.corpus.stopwords.words('english')\n    text = (unicodedata.normalize('NFKD', text)\n            .encode('ascii', 'ignore')\n            .decode('utf-8', 'ignore')\n            .lower())\n    words = re.sub(r'[^\\w\\s]', '', text).split()\n    return [wnl.lemmatize(word) for word in words if word not in stopwords]\n\nwords1 = clean2(''.join(str(text1.apply(clean1).tolist())))\nwords2 = clean2(''.join(str(text2.apply(clean1).tolist())))\nwords1[:10]","cd9bb9f5":"# Obtain top 10 Bi and Tri Ngrams from cleaned data\nbiden_2ngrams=(pd.Series(nltk.ngrams(words1, 2)).value_counts())[:10]\ntrump_2ngrams=(pd.Series(nltk.ngrams(words2, 2)).value_counts())[:10]\nbiden_3ngrams=(pd.Series(nltk.ngrams(words1, 3)).value_counts())[:10]\ntrump_3ngrams=(pd.Series(nltk.ngrams(words2, 3)).value_counts())[:10]\n\n# Input Bi and Tri Ngrams into dataframes for plotting\nbiden_ngrams=pd.concat([biden_2ngrams,biden_3ngrams])\ntrump_ngrams=pd.concat([trump_2ngrams,trump_3ngrams])","1c9d6a49":"fig, ax=plt.subplots(1,2, figsize=(8,16), \n                     gridspec_kw={'width_ratios':[1,1], 'wspace':0.1, 'hspace':0.1})\n\nbarh_ax = ax[0]\nbiden_ngrams[::-1].plot.barh(ax=barh_ax, color=barcolors[3],**barstyle)\nbarh_ax.yaxis.set_label_position(\"left\")\nbarh_ax.xaxis.tick_top()\nbarh_ax.xaxis.set_label_position(\"top\")\nbarh_ax.xaxis.set_major_formatter(formatterK2)\nbarh_ax.set_xlim([0, 3500])\nbarh_ax.set_xlim(barh_ax.get_xlim()[::-1])\nbarh_ax.set_xlabel('Bi & Tri N-gram Count - Biden Dataset')\nbarh_ax.set_ylabel('')\n\nbarh_ax = ax[1]\ntrump_ngrams[::-1].plot.barh(ax=barh_ax, color=barcolors[6],**barstyle)\nbarh_ax.xaxis.tick_top()\nbarh_ax.xaxis.set_label_position(\"top\")\nbarh_ax.xaxis.set_major_formatter(formatterK2)\nbarh_ax.set_xlim([0, 3500])\nbarh_ax.set_xlim(barh_ax.get_xlim())\nbarh_ax.yaxis.tick_right()\nbarh_ax.set_xlabel('Bi & Tri N-gram Count - Trump Dataset')\nbarh_ax.set_ylabel('')\nplt.show()","1325d2ff":"plt.subplots(1,1, figsize=(9,9))\nwc_b = WordCloud(stopwords=STOPWORDS, \n                 background_color=\"white\", max_words=2000,\n                 max_font_size=256, random_state=42,\n                 width=1600, height=1600)\nwc_b.generate(str(text1.dropna()))\nplt.imshow(wc_b, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","d7dc4d0b":"plt.subplots(1,1, figsize=(9,9))\nwc_t = WordCloud(stopwords=STOPWORDS, \n                 background_color=\"black\", max_words=2000,\n                 max_font_size=256, random_state=42,\n                 width=1600, height=1600)\nwc_t.generate(str(text2.dropna()))\nplt.imshow(wc_t, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","a6c4cfde":"# Obtain sentiment scores for both datasets\nsid = SentimentIntensityAnalyzer()\ntweets_biden['VADAR']=sentiment(tweets_biden['tweet'])\ntweets_trump['VADAR']=sentiment(tweets_trump['tweet'])\ntweets_biden['compound']  = tweets_biden['VADAR'].apply(lambda score_dict: score_dict['compound'])\ntweets_trump['compound']  = tweets_trump['VADAR'].apply(lambda score_dict: score_dict['compound'])\ntweets_trump['sentiment']  = tweets_trump['compound'].apply(lambda x: 'pos' if x > 0.05 else ('neg' if x < -0.05 else 'neu'))\ntweets_biden['sentiment']  = tweets_biden['compound'].apply(lambda x: 'pos' if x > 0.05 else ('neg' if x < -0.05 else 'neu'))\n\n# Create 52 state set\nstates=set(tweets_biden.loc[tweets_biden['country'] == 'United States of America']['state'].dropna())\nstates.remove('District of Columbia')\nstates.remove('Northern Mariana Islands')\n\n# Create feature to allow masking of data and then mask data for votable states\ntweets_biden['voting_rights']=tweets_biden['state'].apply(lambda x: 'Yes' if x in states else 'No')\ntweets_trump['voting_rights']=tweets_trump['state'].apply(lambda x: 'Yes' if x in states else 'No')\nsent_t=tweets_trump.loc[tweets_trump['voting_rights'] == 'Yes']\nsent_b=tweets_biden.loc[tweets_biden['voting_rights'] == 'Yes']\n\n# Further mask data for only the last 14 days\nstate_b=sent_b.loc[sent_b['created_at'] > max(sent_b['created_at']) - timedelta(14)]\nstate_t=sent_t.loc[sent_t['created_at'] > max(sent_t['created_at']) - timedelta(14)]\nstate_b_mean=state_b.groupby('state')['compound'].mean().reset_index()\nstate_t_mean=state_t.groupby('state')['compound'].mean().reset_index()\n\n# Further mask data for only the last 14 days\nstate_bp=sent_b.loc[sent_b['created_at'] < min(sent_b['created_at']) + timedelta(14)]\nstate_tp=sent_t.loc[sent_t['created_at'] < min(sent_t['created_at']) + timedelta(14)]\nstate_bp_mean=state_bp.groupby('state')['compound'].mean().reset_index()\nstate_tp_mean=state_tp.groupby('state')['compound'].mean().reset_index()\n\n# Create dataframe for visualisation\nstates_sent=pd.DataFrame({'state':state_b_mean['state'],\n                          'biden1':state_b_mean['compound'],\n                          'trump1':state_t_mean['compound'],\n                          'biden2':state_bp_mean['compound'],\n                          'trump2':state_tp_mean['compound'],})","65e3ce32":"fig, ax=plt.subplots(2,1, figsize=(12,10), gridspec_kw={'hspace':0.05})\nlineax=ax[0]\nsns.lineplot(x='state', y='trump1', color=barcolors[6], data=states_sent, ax=lineax, label='Trump Dataset (L14D)')\nsns.scatterplot(x='state', y='trump1', color=barcolors[6], data=states_sent, ax=lineax)\nsns.lineplot(x='state', y='trump2', color='lightgrey', data=states_sent, ax=lineax, label='Trump Dataset (F14D)')\nsns.scatterplot(x='state', y='trump2', color='lightgrey', data=states_sent, ax=lineax)\nlineax.set_ylim([-0.2, 0.2])\nlineax.set_ylabel('mean sentiment score (Last 14D Data)')\nlineax.set_xlabel('')\nplt.xticks(rotation=90)\nlineax.axhline(y=0, color='k', linestyle='-')\nlineax.axhline(y=0.05, color='lightgrey', linestyle='-')\nlineax.axhline(y=-0.05, color='lightgrey', linestyle='-')\nlineax.axes.get_xaxis().set_ticks([])\nlineax.spines['right'].set_visible(False)\nlineax.spines['top'].set_visible(False)\nlineax.spines['bottom'].set_visible(False)\n\nlineax=ax[1]\nsns.lineplot(x='state', y='biden1', color=barcolors[3], data=states_sent, ax=lineax, label='Biden Dataset (L14D)')\nsns.scatterplot(x='state', y='biden1', color=barcolors[3], data=states_sent, ax=lineax)\nsns.lineplot(x='state', y='biden2', color='lightgrey', data=states_sent, ax=lineax, label='Biden Dataset (F14D)')\nsns.scatterplot(x='state', y='biden2', color='lightgrey', data=states_sent, ax=lineax)\nlineax.set_ylim([-0.2, 0.2])\nlineax.set_ylabel('mean sentiment score')\nlineax.set_xlabel('')\nplt.xticks(rotation=90)\nlineax.axhline(y=0, color='k', linestyle='-')\nlineax.axhline(y=0.05, color='lightgrey', linestyle='-')\nlineax.axhline(y=-0.05, color='lightgrey', linestyle='-')\nlineax.spines['right'].set_visible(False)\nlineax.spines['top'].set_visible(False)\nplt.show()","339931a2":"# Calculate counts of sentiments\nstack_t=sent_t.groupby(['created_at_r','sentiment'])['tweet'].count().reset_index()\nstack_b=sent_b.groupby(['created_at_r','sentiment'])['tweet'].count().reset_index()\n\n# Setup np.arrays to allow quick calculations of the proportions of tweet sentiments\na1=np.array(stack_b.loc[stack_b.sentiment == 'pos']['tweet'].tolist())\nb1=np.array(stack_b.loc[stack_b.sentiment == 'neu']['tweet'].tolist())\nc1=np.array(stack_b.loc[stack_b.sentiment == 'neg']['tweet'].tolist())\nd1=np.array(stack_b.groupby('created_at_r')['tweet'].sum().tolist())\n\na2=np.array(stack_t.loc[stack_t.sentiment == 'pos']['tweet'].tolist())\nb2=np.array(stack_t.loc[stack_t.sentiment == 'neu']['tweet'].tolist())\nc2=np.array(stack_t.loc[stack_t.sentiment == 'neg']['tweet'].tolist())\nd2=np.array(stack_t.groupby('created_at_r')['tweet'].sum().tolist())\n\n# Calculate sentiment proportions and feed into dataframes for visualisation\nSentiDat_b=pd.DataFrame({'date':pd.to_datetime(stack_b.created_at_r.unique()),\n                         'datenum':dates.datestr2num(stack_b.created_at_r.unique()),\n                         'pos':a1\/d1,'neu':b1\/d1,'neg':c1\/d1})\n\nSentiDat_t=pd.DataFrame({'date':pd.to_datetime(stack_t.created_at_r.unique()),\n                         'datenum':dates.datestr2num(stack_t.created_at_r.unique()),\n                         'pos':a2\/d2,'neu':b2\/d2,'neg':c2\/d2})","d5aba123":"@plt.FuncFormatter\ndef fake_dates(x, pos):\n    \"\"\" Custom formater to turn floats into e.g., 05-08\"\"\"\n    return dates.num2date(x).strftime('%m-%d')\n\nfig, ax=plt.subplots(3,1, figsize=(12,12), gridspec_kw={'hspace':0.05})\n \n# Plot\nlineax=ax[0]\nlineax.set_title('Sentiment Analysis per Hour')\nsns.regplot(x='datenum',y='pos', data=SentiDat_b, ax=lineax, color=barcolors[3], scatter_kws={'s':5}, logistic=True, ci=95)\nsns.lineplot(x='datenum',y='pos', data=SentiDat_b, ax=lineax, color=barcolors[3], alpha=0.5, label='Biden Dataset')\nsns.regplot(x='datenum',y='pos', data=SentiDat_t, ax=lineax, color=barcolors[6], scatter_kws={'s':5}, logistic=True, ci=95)\nsns.lineplot(x='datenum',y='pos', data=SentiDat_t, ax=lineax, color=barcolors[6], alpha=0.5, label='Trump Dataset')\nlineax.xaxis.set_major_formatter(fake_dates)\nlineax.set_ylim([0, 0.7])\nlineax.set_xlabel('')\nlineax.set_ylabel('positive (proportion)')\nlineax.axes.get_xaxis().set_ticks([])\nlineax.spines['right'].set_visible(False)\nlineax.spines['top'].set_visible(False)\nlineax.spines['bottom'].set_visible(False)\n\nlineax1=ax[1]\nsns.regplot(x='datenum',y='neu', data=SentiDat_b, ax=lineax1, color=barcolors[3], scatter_kws={'s':5}, logistic=True, ci=95)\nsns.lineplot(x='datenum',y='neu', data=SentiDat_b, ax=lineax1, color=barcolors[3], alpha=0.5)\nsns.regplot(x='datenum',y='neu', data=SentiDat_t, ax=lineax1, color=barcolors[6], scatter_kws={'s':5}, logistic=True, ci=95)\nsns.lineplot(x='datenum',y='neu', data=SentiDat_t, ax=lineax1, color=barcolors[6], alpha=0.5)\nlineax1.xaxis.set_major_formatter(fake_dates)\nlineax1.set_ylim([0, 0.7])\nlineax1.set_xlabel('')\nlineax1.set_ylabel('neutral (proportion)')\nlineax1.axes.get_xaxis().set_ticks([])\nlineax1.spines['right'].set_visible(False)\nlineax1.spines['top'].set_visible(False)\nlineax1.spines['bottom'].set_visible(False)\n\nlineax2=ax[2]\nsns.regplot(x='datenum',y='neg', data=SentiDat_b, ax=lineax2, color=barcolors[3], scatter_kws={'s':5}, logistic=True, ci=95)\nsns.lineplot(x='datenum',y='neg', data=SentiDat_b, ax=lineax2, color=barcolors[3], alpha=0.5)\nsns.regplot(x='datenum',y='neg', data=SentiDat_t, ax=lineax2, color=barcolors[6], scatter_kws={'s':5}, logistic=True, ci=95)\nsns.lineplot(x='datenum',y='neg', data=SentiDat_t, ax=lineax2, color=barcolors[6], alpha=0.5)\nlineax2.xaxis.set_major_formatter(fake_dates)\nlineax2.set_ylim([0, 0.7])\nlineax2.set_ylabel('negative (proportion)')\nlineax2.set_xlabel('date')\nlineax2.spines['right'].set_visible(False)\nlineax2.spines['top'].set_visible(False)\n\nplt.show()","c2aefdc2":"#### Updated 4th Nov - Version 26\nThe VADAR analysis produces a compound sentiment score, to create the visualisation I took the mean compound score for the most recent 14 days (Labelled L14D on the charts) and the first 14 days (Labelled F14D and light grey on the charts ) for each state.\n\nIt should be noted that any sentiment score between 0.05 and -0.05 is considered \"Neutral\".\n\nThe results seems to show a large number of states are trending to a \u201cPositive\u201d  sentiment score for the democratic candidate from the previous more \u201cNeutral\u201d sentiment. Whereas most states are still largely \"Neutral\" for the republican candidate.","6228de35":"## Conclusion\n\n#### Updated 5th Nov - Version 28\nIn this kernel, we performed EDA and sentiment analysis on the [US Election 2020 Tweets dataset](https:\/\/www.kaggle.com\/manchunhui\/us-election-2020-tweets). \n\nThere is interest in the US elections from many different countries in the world with tweets from 40 different languages, however with a large proportion of the tweets in English and originating from the US.\n\nThe sentiment analysis was performed only on data that had geo-data originating from the \"United States of America\" to try to ascertain the sentiment in each respective dataset and therefore each presidential candidate. When reviewing sentiment at the state level as we approached the election date a large number of states were trending to a \u201cPositive\u201d sentiment score for the democratic candidate from the previously more \u201cNeutral\u201d sentiment. Whereas most states are still largely \u201cNeutral\u201d for the republican candidate. This trend is correlatable when viewing the sentiment analysis from a date perspective. \n\nI hope you enjoyed reading this kernel, please do not hesitate to comment below if you have any feedback!","16f812aa":"The above bar chart shows the most common Bi and Tri N-gram's in each of the respective datasets, filtered from words just from the \"United States of America\". The N-gram's show a clear relation to the upcoming election and each respective dataset seems to be related to each of the presidential candidates.","ffde312f":"Wordcloud created just from the \"Trump\" dataset, similarly to the above wordcloud I cannot ascertain much information from this.","8a09891b":"In terms of years since joiner for users, both dataset have a similar distribution with a peak at 12 years and another peak showing users joining twitter to tweet about each of the respective presidential candidates.","66c6d9b6":"Before the N-gram analysis we first must clean the tweets to remove stopwords, strings with \"http\" etc and then lemmatize the words.","66c2018c":"#### Updated 5th Nov - Version 28\nTweets about both presidential candidates have been fairly stable in terms of tweets per hour and pattern regularity, with notable exceptions on the date of the last presidential debate and as we come up to and on the election date. On the election date itself there is large notable changes in the pattern regularity on both datasets, with an steady increasing trend in tweet volume for the both of the presidential candidates.","0cf01d68":"The kernel distribution shows that users in both datasets have similar posting frequencies, with a small number of users in the \"Biden\" dataset posting more than a 1000 tweets.","f7ca2803":"Geo spatial features ***```lat, long, city, country, continent, state state code```*** were all derived from ***```user_location```***, using ***sciSpacy NER*** and ***OpenCage API***. It should be noted due a number of factors (such as erroneous ***```user_location```*** inputs, my methods of deriving useful data from subject erroneous inputs and its impact on ***OpenCage API*** returned results), the geo spatial features should be used with caution! ","552c985c":"Plotting all the available geo data shows that many countries around the world are tweeting about the two presidential candidates.","ab2e0a53":"The \"Biden\" dataset, similar to the previous visualisation, shows small but persistant number of tweets with higher normalised \"retweets\" than seen in the \"Trump\" dataset..","15e47d53":"## Sentiment analysis on twitter 2020 election data\n\nThe 2020 US election is happening on the 3rd November 2020, today as of writing this kernel, and the resulting impact to the world will no doubt be large, irrespective of which candidate is elected! After reading the two papers, [here](https:\/\/arxiv.org\/abs\/1706.02271) and [here](https:\/\/dl.acm.org\/doi\/fullHtml\/10.1145\/3339909), I was inspired to attempt a similar sentiment analysis myself!\n\nHowever before I could do this I had to create a [dataset](https:\/\/www.kaggle.com\/manchunhui\/us-election-2020-tweets) first! Please feel free to download and use at your pleasure, the intention is to obtain data upto the end of the 4th of Nov. With regards to the dataset, ot contains two .csv files containing tweets obtained using snsscrape, keywords #JoeBiden, #DonaldTrump, ,#Biden #Trump with the Twitter API.\n\n\n## Chapter 1: Exploratory Data Analysis\nBefore we jump into the sentiment analysis, lets carry some exploration of the dataset! You can view preliminary EDA at this [link](https:\/\/www.kaggle.com\/manchunhui\/starter-kernel-us-election-2020-tweets-prelim-eda), I will not be running over the same details in here. I've decided to hide some of the visualisation code as it distracts from the flow of the analysis, please click on the buttons to view the code if you wish.","d9a39c10":"UserId's that post about both presidential candidates post the most, accounting for greater than 60% of the tweets. Furthermore as with the heatmaps above most tweets are from the America, at least the tweets that have geo data.","fa3c494a":"## Chapter 2: N-grams and Sentiment Analysis (VADAR)\n\nIn second and last part of this kernel I will perform Sentiment Analysis on each of the datasets respectively to ascertain the overall sentiment. To perform the Sentiment Analysis I will be using VADER (Valence Aware Dictionary and sEntiment Reasoner) package, which is a lexicon and rule-based sentiment analysis tool that is specifically tuned to sentiments expressed in social media! Perfect for my use case.","9666fe39":"First of all I must acknowledge that I unashamedly borrowed this heatmap design from @tkubacka, shown in her excellent [kernel](https:\/\/www.kaggle.com\/tkubacka\/a-story-told-through-a-heatmap), as it great at displaying a lot of information in compact space!\n\nFrom the above heatmap we can see that most tweets, in the two .csv files, are published from ***```Twitter Web, Twitter iPhone, Twitter Android or Twitter iPad```*** with most of these from the US or an unidentified location.","e14c275d":"The y-axis is using a log scale to extract more detail from both the tweet and retweet visualisations.\n\nAdditionally both the ***```likes```*** and ***```retweets```*** have been normalised to ***```likes```*** and ***```retweets```*** per hour to try to account for the difference in time from the creation of each tweet and the time of collecting the data. \n\nThe \"Biden\" dataset shows small but persistant number of tweets with higher normalised \"Likes\" than in the \"Trump\" dataset.","dfb8d782":"#### Updated 5th Nov - Version 28\nThe above visualisation was generated by first assigning each tweet a \"Positive\", \"Neutral\" or \"Negative\" sentiment then summing those for each day and calculating the proportions for each sentiment group. Then using logistic regression to find the best fit line to better show the sentiment trend overtime. When viewing the results:\n\nThe trend over the entire timeframe of the dataset for both presidential candidates is an increasing \"Positive\" and \"Neutral\" sentiment, with reducing negative sentiment. \n\nHowever interesting developments occured as we approached and subsequent to election day, where we see \"Positive\" sentiment increase quicker for the democratic candidate over the republican where we see a noticeable gap develop in the logistic regression lines. \n\nMoving onto the \"Neutral\" sentiment, the differences between two candidates remain largely steady until there is a noticable blip on election day where the gap briefly disappears before then returning to the previous steady difference.\n\nPost-election day we see a shape increase in \"Negative\" sentiment for the republican candidate. ","354ba500":"Wordcloud created just from the \"Biden\" dataset, if I am honest I cannot gain much information from this.","b9048927":"Wanting to understand the langauges used to tweet, I used the ***```langdetect```*** function to sample 4000 tweets (which should be sufficient to provide a 99% confidence level with 1-2% margin of error for population of upto 900k tweets). The heatmap above only illustrates the top 5 languages used and the top 5 countries that they were tweeted from, in all 40 languages were detected with English accounting for almost 80% of the tweets."}}