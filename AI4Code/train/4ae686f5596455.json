{"cell_type":{"60de7875":"code","9345d6e8":"code","c304e787":"code","0b49cad5":"code","1b65ee09":"code","9fb84bfd":"code","5b5d60b6":"code","7f5159f0":"code","061a20a7":"code","a2c88785":"code","ccac8443":"code","a9fba216":"code","e0185904":"code","f62d7411":"code","95b36534":"code","50c0e704":"code","4fcd43c6":"code","55166e06":"code","ed2050f0":"code","2af0ca28":"code","c714fab2":"code","ac905856":"code","6cd1cc5f":"code","266d9ec0":"code","59badb72":"code","93839570":"code","e66cee22":"code","cbb38653":"code","efdb5b5f":"code","2fd5b6b7":"code","0bd252c2":"code","9a9aea5a":"code","c62f68de":"code","4db3e558":"code","9c0c494c":"code","877a4f33":"code","36678254":"code","78576349":"code","c7cefd1d":"code","f4be2da1":"code","0afd48a9":"code","32ad544c":"code","24cc43f4":"code","0c781715":"markdown","1dc0927f":"markdown","e37abe38":"markdown","843141ae":"markdown","533a8155":"markdown","ce4cab64":"markdown","5246f263":"markdown","4a5e8e47":"markdown","3faba27f":"markdown","2e443596":"markdown","2c5390ad":"markdown","98e6ea9e":"markdown","c97743d2":"markdown","911a94e1":"markdown","e1eebea5":"markdown","8cb96a63":"markdown","6a145905":"markdown","048ddb84":"markdown","a1fb8e40":"markdown","8a8d464c":"markdown","4bc840a3":"markdown","5a3c8d82":"markdown","74e75f36":"markdown","6c1928fc":"markdown","fabbe194":"markdown","d5ca7da0":"markdown","26cb12fb":"markdown","86f96d54":"markdown","0d829ac8":"markdown","4ed4af48":"markdown","a4b6de9a":"markdown"},"source":{"60de7875":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9345d6e8":"import glob\nimport numpy as np\nimport pandas as pd \nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array","c304e787":"import os\nimport cv2                 \nfrom random import shuffle\nfrom tqdm import tqdm  \nimport tensorflow as tf \nfrom tensorflow.keras import Model\nfrom tensorflow.keras.utils import plot_model\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau , ModelCheckpoint\nfrom collections import Counter\nimport cv2\nfrom tqdm import tqdm\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Input, merge, UpSampling2D, Cropping2D, ZeroPadding2D, Reshape, core, Convolution2D\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom keras import optimizers\nfrom keras import backend as K\nfrom keras.optimizers import SGD\nfrom keras.layers.merge import concatenate\nfrom sklearn.metrics import fbeta_score\n","0b49cad5":"glob.glob('..\/input\/intel-image-classification\/seg_train\/seg_train\/*')","1b65ee09":"TrainImage=\"..\/input\/intel-image-classification\/seg_train\/seg_train\"\nTestImage=\"..\/input\/intel-image-classification\/seg_train\/seg_test\"\nSeaimages = os.listdir(TrainImage + \"\/sea\")\nForestimages = os.listdir(TrainImage + \"\/forest\")\nMountimages = os.listdir(TrainImage + \"\/mountain\")\nGlacierimages = os.listdir(TrainImage + \"\/glacier\")\nBuildingsimages = os.listdir(TrainImage + \"\/buildings\")\nStreetimages = os.listdir(TrainImage + \"\/street\")","9fb84bfd":"print(len(Seaimages), len(Forestimages), len(Mountimages), len(Glacierimages), len(Buildingsimages), len(Streetimages))\nNUM_TRAINING_IMAGES = len(Seaimages)+ len(Forestimages)+ len(Mountimages)+ len(Glacierimages)+ len(Buildingsimages)+ len(Streetimages)\nprint(NUM_TRAINING_IMAGES)","5b5d60b6":"image_size = 128 \nBATCH_SIZE = 16 \nSTEPS_PER_EPOCH = NUM_TRAINING_IMAGES \/\/ BATCH_SIZE\n\ndata_path = '..\/input\/intel-image-classification'\n\ntrain_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                   zoom_range = 0.2,\n                                   rotation_range=15,\n                                   horizontal_flip = True)\n\ntest_datagen = ImageDataGenerator(rescale = 1.\/255)\n\ntraining_set = train_datagen.flow_from_directory(data_path + '\/seg_train\/seg_train',\n                                                 target_size = (image_size, image_size),\n                                                 batch_size = BATCH_SIZE,\n                                                 class_mode = 'categorical',\n                                                 shuffle=True)\n\ntesting_set = test_datagen.flow_from_directory(data_path + '\/seg_test\/seg_test',\n                                            target_size = (image_size, image_size),\n                                            batch_size = BATCH_SIZE,\n                                            class_mode = 'categorical',\n                                            shuffle = True)","7f5159f0":"print(\"train batch \", training_set.__getitem__(0)[0].shape)\nprint(\"test batch \", testing_set.__getitem__(0)[0].shape)","061a20a7":"training_set.class_indices","a2c88785":"labels = ['buildings', 'forest','glacier','mountain','sea','street']","ccac8443":"sample_data = testing_set.__getitem__(1)[0] \nsample_label = testing_set.__getitem__(1)[1] ","a9fba216":"plt.figure(figsize=(10,8))\nfor i in range(12):\n    plt.subplot(3, 6, i + 1)\n    plt.axis('off')\n    plt.imshow(sample_data[i])\n    plt.title(labels[np.argmax(sample_label[i])])","e0185904":"def display_training_curves(training, validation, title, subplot):\n    if subplot%10==1: # set up the subplots on the first call\n        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n        plt.tight_layout()\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(training)\n    ax.plot(validation)\n    ax.set_title('model '+ title)\n    ax.set_ylabel(title)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.'])","f62d7411":"# https:\/\/keras.io\/examples\/vision\/grad_cam\/\nfrom tensorflow import keras\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, classifier_layer_names):\n    # First, we create a model that maps the input image to the activations\n    # of the last conv layer\n    last_conv_layer = model.get_layer(last_conv_layer_name)\n    last_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output)\n\n    # Second, we create a model that maps the activations of the last conv\n    # layer to the final class predictions\n    classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\n    x = classifier_input\n    for layer_name in classifier_layer_names:\n        x = model.get_layer(layer_name)(x)\n    classifier_model = keras.Model(classifier_input, x)\n\n    # Then, we compute the gradient of the top predicted class for our input image\n    # with respect to the activations of the last conv layer\n    with tf.GradientTape() as tape:\n        # Compute activations of the last conv layer and make the tape watch it\n        last_conv_layer_output = last_conv_layer_model(img_array)\n        tape.watch(last_conv_layer_output)\n        # Compute class predictions\n        preds = classifier_model(last_conv_layer_output)\n        top_pred_index = tf.argmax(preds[0])\n        top_class_channel = preds[:, top_pred_index]\n\n    # This is the gradient of the top predicted class with regard to\n    # the output feature map of the last conv layer\n    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n    pooled_grads = pooled_grads.numpy()\n    for i in range(pooled_grads.shape[-1]):\n        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n\n    # The channel-wise mean of the resulting feature map\n    # is our heatmap of class activation\n    heatmap = np.mean(last_conv_layer_output, axis=-1)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = np.maximum(heatmap, 0) \/ np.max(heatmap)\n    return heatmap, top_pred_index.numpy()","95b36534":"# https:\/\/keras.io\/examples\/vision\/grad_cam\/\ndef superimposed_img(image, heatmap):\n    # We rescale heatmap to a range 0-255\n    heatmap = np.uint8(255 * heatmap)\n\n    # We use jet colormap to colorize heatmap\n    jet = cm.get_cmap(\"jet\")\n\n    # We use RGB values of the colormap\n    jet_colors = jet(np.arange(256))[:, :3]\n    jet_heatmap = jet_colors[heatmap]\n\n    # We create an image with RGB colorized heatmap\n    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n    jet_heatmap = jet_heatmap.resize((image_size, image_size))\n    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n\n    # Superimpose the heatmap on original image\n    superimposed_img = jet_heatmap * 0.4 + image\n    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n    return superimposed_img","50c0e704":"#label smoothing https:\/\/www.linkedin.com\/pulse\/label-smoothing-solving-overfitting-overconfidence-code-sobh-phd\/\ndef categorical_smooth_loss(y_true, y_pred, label_smoothing=0.1):\n    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred, label_smoothing=label_smoothing)\n    return loss","4fcd43c6":"!pip install efficientnet\nimport efficientnet.tfkeras as efn","55166e06":"# training call backs \nlr_reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, epsilon=0.0001, patience=3, verbose=1)\nes_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1)","ed2050f0":"# https:\/\/stackoverflow.com\/questions\/42586475\/is-it-possible-to-automatically-infer-the-class-weight-from-flow-from-directory\ncounter = Counter(training_set.classes)                          \nmax_val = float(max(counter.values()))       \nclass_weights = {class_id : max_val\/num_images for class_id, num_images in counter.items()}\nclass_weights","2af0ca28":"# default input shapes \nprint(tf.keras.applications.DenseNet201(weights='imagenet').input_shape)\nprint(efn.EfficientNetB7(weights='imagenet').input_shape) \nprint(tf.keras.applications.VGG16(weights='imagenet').input_shape)","c714fab2":"# EXTRA In case you want to save the h5 file \n# please dont forget to change the model names.\ncheckpoint = ModelCheckpoint(\n    'Densenet201_model.h5', \n    monitor='val_acc', \n    verbose=1, \n    save_best_only=True, \n    save_weights_only=False,\n    mode='auto'\n)","ac905856":"from keras import Input\nfrom keras.layers import Conv2D, MaxPooling2D, Embedding, Reshape, Concatenate, SeparableConv2D\nimport tensorflow as tf\nfrom keras.layers.normalization import BatchNormalization\n\ninputs = Input(shape=(128, 128, 3))\n\n# First conv block\nx = Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\nx = Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\n# Second conv block\nx = SeparableConv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = SeparableConv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\n# Third conv block\nx = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\n# Fourth conv block\nx = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\nx = Dropout(rate=0.2)(x)\n\n# Fifth conv block\nx = SeparableConv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = SeparableConv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\nx = Dropout(rate=0.2)(x)\n\n# FC layer\nx = Flatten()(x)\nx = Dense(units=512, activation='relu')(x)\nx = Dropout(rate=0.7)(x)\nx = Dense(units=128, activation='relu')(x)\nx = Dropout(rate=0.5)(x)\nx = Dense(units=64, activation='relu')(x)\nx = Dropout(rate=0.3)(x)\n\n# Output layer\noutput = Dense(units=6, activation='softmax')(x)\n\n# Creating model and compiling\nmodel = tf.keras.Model(inputs=inputs, outputs=output)\nmodel.compile(optimizer='RMSprop', loss=categorical_smooth_loss, metrics=['accuracy'])\n\n# Callbacks\ncheckpoint = ModelCheckpoint(filepath='best_weights.hdf5', save_best_only=True, save_weights_only=True)\nlr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, verbose=2, mode='max')\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1, patience=1, mode='min')","6cd1cc5f":"history = model.fit_generator(training_set, validation_data=testing_set, callbacks=[lr_reduce, es_callback], epochs=30)","266d9ec0":"display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 211)\ndisplay_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 212)","59badb72":"from tensorflow.keras import applications\nfrom tensorflow.keras import models, layers\nfrom keras import Sequential \n\npretrained_model = applications.VGG19(\n    weights=\"imagenet\", \n    include_top=False, \n    input_shape=(128, 128, 3)\n)\n\nfor layer in pretrained_model.layers[:-1]:\n  layer.trainable = False\n\npretrained_model.summary()\n\n#pretrained_model.trainable = False\n\nmodel = models.Sequential()\n\nmodel.add(pretrained_model)\n\nmodel.add(layers.Flatten())\n\nmodel.add(layers.Dropout(0.25))\nmodel.add(layers.Dense(1024, activation=\"relu\"))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(6, activation=\"softmax\"))","93839570":"# VGG19\nmodel.compile(optimizer='RMSprop',loss='categorical_crossentropy',metrics=['accuracy'])\nhistory_densenet = model.fit_generator(training_set, validation_data=testing_set, callbacks=[checkpoint,lr_reduce, es_callback], epochs=20)","e66cee22":"# plot for VGG19\ndisplay_training_curves(history_densenet.history['loss'], history_densenet.history['val_loss'], 'loss', 211)\ndisplay_training_curves(history_densenet.history['accuracy'], history_densenet.history['val_accuracy'], 'accuracy', 212)","cbb38653":"pretrained_efnet = efn.EfficientNetB7(input_shape=(image_size, image_size, 3), weights='noisy-student', include_top=False)\n\nfor layer in pretrained_efnet.layers:\n  layer.trainable = False\n\nx2 = pretrained_efnet.output\nx2 = tf.keras.layers.AveragePooling2D(name=\"averagepooling2d_head\")(x2)\nx2 = tf.keras.layers.Flatten(name=\"flatten_head\")(x2)\nx2 = tf.keras.layers.Dense(64, activation=\"relu\", name=\"dense_head\")(x2)\nx2 = tf.keras.layers.Dropout(0.5, name=\"dropout_head\")(x2)\nmodel_out = tf.keras.layers.Dense(6, activation='softmax', name=\"predictions_head\")(x2)\n\nmodel_efnet = Model(inputs=pretrained_efnet.input, outputs=model_out)\nmodel_efnet.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),loss=categorical_smooth_loss,metrics=['accuracy'])\nmodel_efnet.summary()","efdb5b5f":"history_efnet = model_efnet.fit_generator(training_set, validation_data=testing_set, callbacks=[lr_reduce, es_callback], epochs=15)","2fd5b6b7":"#try3\npretrained_densenet = tf.keras.applications.DenseNet121(input_shape=(image_size, image_size, 3), weights='imagenet', include_top=False)\n\nfor layer in pretrained_densenet.layers[:-3]:\n  layer.trainable = False\n\nx1 = pretrained_densenet.output\nx1 = tf.keras.layers.AveragePooling2D(name=\"averagepooling2d_head\")(x1)\nx1 = tf.keras.layers.Flatten(name=\"flatten_head\")(x1)\nx1 = tf.keras.layers.Dense(64, activation=\"relu\", name=\"dense_head\")(x1)\nx1 = tf.keras.layers.Dropout(0.5, name=\"dropout_head\")(x1)\nmodel_out = tf.keras.layers.Dense(6, activation='softmax', name=\"predictions_head\")(x1)\n\nmodel_densenet = Model(inputs=pretrained_densenet.input, outputs=model_out)\nmodel_densenet.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),loss=categorical_smooth_loss,metrics=['accuracy'])\n\nmodel_densenet.summary()","0bd252c2":"history_densenet = model_densenet.fit_generator(training_set, validation_data=testing_set, callbacks=[checkpoint,lr_reduce, es_callback], epochs=20)","9a9aea5a":"pretrained_densenet = tf.keras.applications.DenseNet201(input_shape=(image_size, image_size, 3), weights='imagenet', include_top=False)\n\nfor layer in pretrained_densenet.layers:\n  layer.trainable = False\n\nx1 = pretrained_densenet.output\nx1 = tf.keras.layers.AveragePooling2D(name=\"averagepooling2d_head\")(x1)\nx1 = tf.keras.layers.Flatten(name=\"flatten_head\")(x1)\nx1 = tf.keras.layers.Dense(64, activation=\"relu\", name=\"dense_head\")(x1)\nx1 = tf.keras.layers.Dropout(0.5, name=\"dropout_head\")(x1)\nmodel_out = tf.keras.layers.Dense(6, activation='softmax', name=\"predictions_head\")(x1)\n\nmodel_densenet = Model(inputs=pretrained_densenet.input, outputs=model_out)\nmodel_densenet.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001),loss=categorical_smooth_loss,metrics=['accuracy'])\n\nmodel_densenet.summary()","c62f68de":"#try2 -> with RMSprop\nhistory_densenet = model_densenet.fit_generator(training_set, validation_data=testing_set, callbacks=[lr_reduce, es_callback], epochs=15)","4db3e558":"history_densenet = model_densenet.fit_generator(training_set, validation_data=testing_set, callbacks=[lr_reduce, es_callback], epochs=5)","9c0c494c":"pretrained_densenet = tf.keras.applications.DenseNet201(input_shape=(image_size, image_size, 3), weights='imagenet', include_top=False)\n\nfor layer in pretrained_densenet.layers:\n  layer.trainable = False\n\nx1 = pretrained_densenet.output\nx1 = tf.keras.layers.AveragePooling2D(name=\"averagepooling2d_head\")(x1)\nx1 = tf.keras.layers.Flatten(name=\"flatten_head\")(x1)\nx1 = tf.keras.layers.Dense(64, activation=\"relu\", name=\"dense_head\")(x1)\nx1 = tf.keras.layers.Dropout(0.5, name=\"dropout_head\")(x1)\nmodel_out = tf.keras.layers.Dense(6, activation='softmax', name=\"predictions_head\")(x1)\n\nmodel_densenet = Model(inputs=pretrained_densenet.input, outputs=model_out)\nmodel_densenet.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),loss=categorical_smooth_loss,metrics=['accuracy'])\n\nmodel_densenet.summary()","877a4f33":"plot_model(model_densenet, show_shapes=True, to_file='model_densenet.png')","36678254":"history_densenet = model_densenet.fit_generator(training_set, validation_data=testing_set, callbacks=[lr_reduce, es_callback], epochs=15)","78576349":"display_training_curves(history_densenet.history['loss'], history_densenet.history['val_loss'], 'loss', 211)\ndisplay_training_curves(history_densenet.history['accuracy'], history_densenet.history['val_accuracy'], 'accuracy', 212)","c7cefd1d":"last_conv_layer_name = \"conv5_block32_concat\"\nclassifier_layer_names = [\n    \"bn\",\n    \"relu\",\n    \"averagepooling2d_head\",\n    \"flatten_head\",\n    \"dense_head\",\n    \"dropout_head\",\n    \"predictions_head\"\n]","f4be2da1":"# test image\nfile_path =  '..\/input\/intel-image-classification\/seg_test\/seg_test\/buildings\/20064.jpg'\ntest_image = cv2.imread(file_path)\ntest_image = cv2.resize(test_image, (128,128),interpolation=cv2.INTER_NEAREST)\nplt.imshow(test_image)\ntest_image = np.expand_dims(test_image,axis=0)","0afd48a9":"heatmap, top_index = make_gradcam_heatmap(test_image, model_densenet, last_conv_layer_name, classifier_layer_names)\nprint(\"predicted as\", labels[top_index])","32ad544c":"plt.matshow(heatmap)\nplt.show()","24cc43f4":"plt.figure(figsize=(10,8))\nfor i in range(12):\n    plt.subplot(3, 4, i + 1)\n    plt.axis('off')\n    heatmap, top_index = make_gradcam_heatmap(np.expand_dims(sample_data[i], axis=0), model_densenet, last_conv_layer_name, classifier_layer_names)\n    img = np.uint8(255 * sample_data[i])\n    s_img = superimposed_img(img, heatmap)\n    plt.imshow(s_img)\n    plt.title(labels[np.argmax(sample_label[i])] + \" pred as: \" + labels[top_index], fontsize=8)","0c781715":"### We can take any test image","1dc0927f":"# DenseNet121\n\noptimizer: RMSprop\nloss: categorical_smooth_loss","e37abe38":"# EfficientNetB7","843141ae":"This function can be used to plot the loss and accuracy of any model, I am not plotting this for all of the models but feel free to plot them to visualize the loss and accuracy.","533a8155":"optimizer: RmsProp\n\nloss: categorical_smooth_loss","ce4cab64":"# BEST MODEL\n\n# DenseNet201\n\noptimizer: Adam\n\nloss: categorical_smooth_loss","5246f263":"The EfficientNetB7 did not work well for us.","4a5e8e47":"### This notebook is beginner friendly, I have not explained much theory or architecture. I have tried my best to make the code reproducable, please feel free to use them.","3faba27f":"### It can be seen that DenseNet201 has done a pretty good job in classifying them\n","2e443596":"# Lets get started on the modelling part","2c5390ad":"## Let us start with a few Conv2D layers","98e6ea9e":"# Lets try another variant of DenseNet\n\n# DenseNet201","c97743d2":"optimizer: Adam\n\nloss: categorical_smooth_loss (refer the function def)","911a94e1":"### We observe that the Training and validation accuracy is around 0.80\n\n### We will try to improve the model","e1eebea5":"## There are a few helper functions defined above which are useful. \n\n## The models are organized by their performance, I have just used the same variable names for easier use. \n\n## Feel free to skip to the end of the notebook to DenseNet201, which gives us nearly 90% accuracy on Training and 90% accuracy on validation\n\n### I did not set the seed, so you may get a different result \n\n### The Final model DenseNet201 at the end of the notebook alone has some good visualizations and I try to explain how to interpret them based on my understanding, please feel free to use them on any of the models in the notebook such as VGG19, DenseNet121, EfficientNetB7.\n\n","8cb96a63":"It seems like 15 epoch is not sufficient and we can get better results with a few more, if we want to increase the number of epochs, we just fit the same model again with the number of epochs, it will continue from the last epoch. \n\nabove we trained till 15, now we execute a copy of the cell, with 5 epochs, observe that the model is still using what it learned and resumes the training.\n\nHowever the best alternative way is to dowload the h5 file, but we will get to it later.","6a145905":"optimizer: RMSprop\n\nloss: categorical_crossentropy\n\nNote: We are training the last layer of the VGG19, and then adding a few layers. Finally we use 6 neurons, since we have 6 classes that we wish to classify. The last layer uses softmax optimizer.","048ddb84":"okay, so we have 14034 Training images and 3000 Test images","a1fb8e40":"## To improve a model, if we use a pre-trained model which is trained on a data which is similar to the problem we are trying to solve, the process is called Transfer Learning.\n\n### There are various models available, here We have implemented a few of them, there are different ways to fine tune the model which will be discussed below.\n\n# Note: The accuracy can be improved further depending on the architecture and a number of parameters, but here we are limiting it to focus on the fine-tuning part hence we have run it for a small number of epochs","8a8d464c":"Lets visualize some of the images","4bc840a3":"From the output of glob, it is clear that there are 6 folders, each containing images of their folder name.\n\nThere are 6 classes in total.","5a3c8d82":"This is a good improvement, although it trained only for a few epoch it reached an validation accuracy of 0.86","74e75f36":"Great! let us see how many images are there for the 6 classes.\n\nThe dataset seems to pretty balanced as there almost similar number of images in all the classes.\n","6c1928fc":"Defining the Learning Rate Annealer and Early Stopping callbacks","fabbe194":"### see what the prediction is","d5ca7da0":"## In case you find anything useful, or learnt something kindly upvote the notebook","26cb12fb":"### These links and notebooks have helped me a lot, consider upvoting them too.\n\nauthor: ImrahimSobh\n\nLink: https:\/\/github.com\/IbrahimSobh\/kaggle-COVID19-Classification\/blob\/master\/chest%20x-ray%20covid19%20efnet%20densenet%20vgg%20Grad-CAM.ipynb\n\nauthor: janvichokshi\n\nLink: https:\/\/www.kaggle.com\/janvichokshi\/transfer-learning-cnn-resnet-vgg16-iceptionv3\/?\n\n","86f96d54":"Heatmaps are useful in some cases to identify what the classifier has learnt, but it is a bit difficult to interpret exactly.","0d829ac8":"VGG19 is a pre trained model, but however to use it for our problem, we have to modify a few layers.","4ed4af48":"Densenet has many variants, let us see if Densenet121 works better than Densenet201, here we are training the last 3 layers too, this totally depends on the problem that is to be solved. ","a4b6de9a":"# *VGG19*"}}