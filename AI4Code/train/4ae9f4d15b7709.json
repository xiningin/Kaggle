{"cell_type":{"472cab6c":"code","51845616":"code","8f0a0e8e":"code","f6f39969":"code","11377ea8":"code","2eb1f38d":"code","6c84547b":"code","2bc84c8a":"code","aca6f598":"code","bfbbb17b":"code","57e05292":"code","1d3d9519":"code","864a54b2":"code","65edeb96":"code","1c844284":"code","9a97a4b7":"code","fa06a492":"code","e84ed15d":"code","eb5a7284":"markdown","dd1ba96d":"markdown","cbdefa85":"markdown","00fdfbd3":"markdown","5c04c2e2":"markdown","64ee407d":"markdown","54775c41":"markdown","b5e05324":"markdown","85b502a2":"markdown","f1e28202":"markdown","d1b96939":"markdown","97ad05b6":"markdown","8d87fc10":"markdown","a2fd1f3a":"markdown","26445c5f":"markdown","90a3591c":"markdown","be7f0bf3":"markdown"},"source":{"472cab6c":"import os\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tqdm import tqdm\nfrom tokenizers import BertWordPieceTokenizer\nimport datetime\nfrom matplotlib import pyplot as plt\nimport sklearn\n\nAUTO = tf.data.experimental.AUTOTUNE\n\nprint(\"Tensorflow version \" + tf.__version__)","51845616":"# Set `DETERMINISTIC` to `True`\n# if you want to have more stable results among different trainings.\nDETERMINISTIC = True\n\nSEED = 0\n\nGLOBAL_SEED = None\nOP_SEED = None\n\nif DETERMINISTIC:\n\n    GLOBAL_SEED = SEED\n    OP_SEED = SEED\n\n    tf.random.set_seed(seed=GLOBAL_SEED)\n    np.random.seed(GLOBAL_SEED)\n\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'","8f0a0e8e":"# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","f6f39969":"# Easier to do experiments.\nCONFIG_DICT = {\n    \"distilbert-base-multilingual-cased\": {\n        'fast_encode': True,\n        'fast_tokenizer_class': BertWordPieceTokenizer,\n        'padding_token': 0\n    },\n    'bert-base-multilingual-cased': {\n        'fast_encode': True,\n        'fast_tokenizer_class': BertWordPieceTokenizer,\n        'padding_token': 0\n    },\n    'jplu\/tf-xlm-roberta-base': {\n        'fast_encode': False,\n        'padding_token': 1\n    },\n    'jplu\/tf-xlm-roberta-large': {\n        'fast_encode': False,\n        'padding_token': 1\n    }\n}\n\n# TRANSFORMER_TYPE = 'distilbert-base-multilingual-cased'\n# TRANSFORMER_TYPE = 'bert-base-multilingual-cased'\n# TRANSFORMER_TYPE = 'jplu\/tf-xlm-roberta-base'\nTRANSFORMER_TYPE = 'jplu\/tf-xlm-roberta-large'\n\nCONFIG = CONFIG_DICT[TRANSFORMER_TYPE]\n\nFAST_ENCODE = CONFIG['fast_encode']\nif FAST_ENCODE:\n    FAST_TOKENIZER_CLASS = CONFIG['fast_tokenizer_class']\nPADDING_TOKEN = CONFIG['padding_token']\n\nMAX_LEN = 192  # 512\n\nEPOCHS = 3  # 8\nWARMUP_EPOCHS = 1  # 4\n\n# The number of examples for which the training procedure running on a single replica will compute the gradients once in order to accumulate them.\nBATCH_SIZE_PER_REPLICA = 8  # 2\n\n# Accumulate `BATCHES_PER_UPDATE` of gradients before updating the model's parameters.\nBATCHES_PER_UPDATE = 2  # 8\n\n# The batch size for prediction procedure running on a single replica.\nPREDICTION_BATCH_SIZE_PER_REPLICA = 32\n\nLEARNING_RATE_SCALING = 2  # 1\nSTART_LR = 5e-6\nMAX_LR = 1e-5  # * strategy.num_replicas_in_sync\nENDING_LR = 5e-6\n\nSHUFFLE_BUFFER_SIZE = 4096\n\n# If to convert labels to 0 and 1.\nROUND_LABELS = True\n\n# The number of positive examples to sample from each training .csv files.\n# Set to `None` to use all positive examples.\nN_POSITIVE_EXAMPLES_TO_SAMPLE = None\n\n# The number of negative examples to sample from each training .csv files\n# will be `N_NEGATIVE_EXAMPLES_FACTOR * N_POSITIVE_EXAMPLES_TO_SAMPLE`.\n# Set to `None` to use all negative examples.\nN_NEGATIVE_EXAMPLES_FACTOR = 1  # 2\n\n# If to use optimized training \/ validation loops.\nOPTIMIZED_LOOP = True\n\n# If to use gradient accumulation.\nUSE_GRADIENT_ACCUMULATION = True\nif BATCHES_PER_UPDATE > 1:\n    USE_GRADIENT_ACCUMULATION = True","11377ea8":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n    \"\"\"\n    https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras\n    \"\"\"\n\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n\n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n\n    return np.array(all_ids)\n\n\ndef regular_encode(texts, tokenizer, maxlen=512):\n    \"\"\"\n    https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-xlm-roberta\n    \"\"\"\n    enc_di = tokenizer.batch_encode_plus(\n        texts,\n        return_attention_masks=False,\n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen\n    )\n\n    return np.array(enc_di['input_ids'])\n\n\ndef get_training_dataset(batch_size):\n\n    dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n    dataset = dataset.repeat()\n\n    dataset = dataset.shuffle(SHUFFLE_BUFFER_SIZE, seed=OP_SEED)\n\n    dataset = dataset.batch(batch_size, drop_remainder=True)\n    dataset = dataset.prefetch(AUTO)\n\n    return dataset\n\n\ndef get_validation_dataset(batch_size, repeated=False):\n\n    dataset = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n\n    if repeated:\n        dataset = dataset.repeat()\n\n        # If no repetition, don't shuffle validation dataset\n        if not DETERMINISTIC:\n            dataset = dataset.shuffle(SHUFFLE_BUFFER_SIZE, seed=OP_SEED)\n\n    dataset = dataset.batch(batch_size, drop_remainder=True)\n    dataset = dataset.prefetch(AUTO)\n\n    return dataset\n\n\ndef get_test_dataset(batch_size):\n\n    dataset = tf.data.Dataset.from_tensor_slices(x_test)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(AUTO)\n\n    return dataset","2eb1f38d":"# From https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras\n\n# First load the real tokenizer\ntokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_TYPE)\n\n# Save the loaded tokenizer locally\ntokenizer.save_pretrained('.')\n\n# Reload it with the huggingface tokenizers library\nif FAST_ENCODE:\n    fast_tokenizer = FAST_TOKENIZER_CLASS('vocab.txt', lowercase=False)\n\n# CSV\ntrain_1 = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv\")\ntrain_2 = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-unintended-bias-train.csv\")\nvalid = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/validation.csv')\ntest = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/test.csv')\n\nif ROUND_LABELS:\n    train_2.toxic = train_2.toxic.round().astype(int)\n\nPOSITIVE_EXAMPLES_1 = train_1[['comment_text', 'toxic']].query('toxic > 0.5')\nPOSITIVE_EXAMPLES_2 = train_2[['comment_text', 'toxic']].query('toxic > 0.5')\nNEGATIVE_EXAMPLES_1 = train_1[['comment_text', 'toxic']].query('toxic <= 0.5')\nNEGATIVE_EXAMPLES_2 = train_2[['comment_text', 'toxic']].query('toxic <= 0.5')\n\nPOSITIVE_EXAMPLES_USED_1 = POSITIVE_EXAMPLES_1\nPOSITIVE_EXAMPLES_USED_2 = POSITIVE_EXAMPLES_2\nNEGATIVE_EXAMPLES_USED_1 = NEGATIVE_EXAMPLES_1\nNEGATIVE_EXAMPLES_USED_2 = NEGATIVE_EXAMPLES_2\n\nif N_POSITIVE_EXAMPLES_TO_SAMPLE is not None:\n    POSITIVE_EXAMPLES_USED_1 = POSITIVE_EXAMPLES_USED_1.sample(n=N_POSITIVE_EXAMPLES_TO_SAMPLE, random_state=OP_SEED)\n    POSITIVE_EXAMPLES_USED_2 = POSITIVE_EXAMPLES_USED_2.sample(n=N_POSITIVE_EXAMPLES_TO_SAMPLE, random_state=OP_SEED)\n\nif N_NEGATIVE_EXAMPLES_FACTOR is not None:\n    NEGATIVE_EXAMPLES_USED_1 = NEGATIVE_EXAMPLES_1.sample(n=int(len(POSITIVE_EXAMPLES_USED_1) * N_NEGATIVE_EXAMPLES_FACTOR), random_state=OP_SEED)    \n    NEGATIVE_EXAMPLES_USED_2 = NEGATIVE_EXAMPLES_2.sample(n=int(len(POSITIVE_EXAMPLES_USED_2) * N_NEGATIVE_EXAMPLES_FACTOR), random_state=OP_SEED)    \n\n# Combine train_1 with a subset of train_2\ntrain = pd.concat([\n    POSITIVE_EXAMPLES_USED_1,\n    NEGATIVE_EXAMPLES_USED_1,\n    POSITIVE_EXAMPLES_USED_2,\n    NEGATIVE_EXAMPLES_USED_2\n])\n# Shuffle\ntrain = sklearn.utils.shuffle(train, random_state=OP_SEED)\n\nvalid = valid[['comment_text', 'toxic']]\n\nN_TRAINING_EXAMPLES = len(train)\nN_VALIDATION_EXAMPLES = len(valid)\nN_TEST_EXAMPLES = len(test)\n\nprint(f'N_TRAINING_EXAMPLES = {N_TRAINING_EXAMPLES}')\nprint(f'N_POSITIVE_EXAMPLES = {len(POSITIVE_EXAMPLES_USED_1) + len(POSITIVE_EXAMPLES_USED_2)}')\nprint(f'N_NEGATIVE_EXAMPLES = {len(NEGATIVE_EXAMPLES_USED_1) + len(NEGATIVE_EXAMPLES_USED_2)}')\nprint(f'N_VALIDATION_EXAMPLES = {N_VALIDATION_EXAMPLES}')\nprint(f'N_TEST_EXAMPLES = {N_TEST_EXAMPLES}')","6c84547b":"if FAST_ENCODE:\n    x_train = fast_encode(train.comment_text.values, fast_tokenizer, maxlen=MAX_LEN)\n    x_valid = fast_encode(valid.comment_text.values, fast_tokenizer, maxlen=MAX_LEN)\n    x_test = fast_encode(test.content.values, fast_tokenizer, maxlen=MAX_LEN)\nelse:\n    x_train = regular_encode(train.comment_text.values, tokenizer, maxlen=MAX_LEN)\n    x_valid = regular_encode(valid.comment_text.values, tokenizer, maxlen=MAX_LEN)\n    x_test = regular_encode(test.content.values, tokenizer, maxlen=MAX_LEN)\n\n#  Labels needs to be reshaped\ny_train = train['toxic'].values.reshape(len(x_train), 1)\ny_valid = valid['toxic'].values.reshape(len(x_valid), 1)","2bc84c8a":"training_dataset = get_training_dataset(batch_size=2)\nvalidation_dataset = get_validation_dataset(batch_size=2)\ntest_dataset = get_test_dataset(batch_size=2)\n\nfor batch in training_dataset.take(1):\n    print(batch)\n\nfor batch in validation_dataset.take(1):\n    print(batch)\n\nfor batch in test_dataset.take(1):\n    print(batch)","aca6f598":"# https:\/\/github.com\/OpenNMT\/OpenNMT-tf\/blob\/f14c05a7cb8b1b8f3a692d6fea3c12067bc3eb2c\/opennmt\/optimizers\/utils.py#L64\n\nclass GradientAccumulator(object):\n    \"\"\"Gradient accumulation utility.\n    When used with a distribution strategy, the accumulator should be called in a\n    replica context. Gradients will be accumulated locally on each replica and\n    without synchronization. Users should then call ``.gradients``, scale the\n    gradients if required, and pass the result to ``apply_gradients``.\n    \"\"\"\n\n    # We use the ON_READ synchronization policy so that no synchronization is\n    # performed on assignment. To get the value, we call .value() which returns the\n    # value on the current replica without synchronization.\n\n    def __init__(self):\n        \"\"\"Initializes the accumulator.\"\"\"\n\n        self._gradients = []\n\n    def gradients(self):\n        \"\"\"The accumulated gradients on the current replica.\"\"\"\n\n        if not self._gradients:\n            raise ValueError(\"The accumulator should be called first to initialize the gradients\")\n\n        # return list(gradient.value() for gradient in self._gradients)\n        return self._gradients\n\n    def __call__(self, gradients):\n        \"\"\"Accumulates :obj:`gradients` on the current replica.\"\"\"\n\n        if not self._gradients:\n\n            self._gradients.extend(\n                [\n                    tf.Variable(\n                        tf.zeros_like(gradient),\n                        trainable=False,\n                        synchronization=tf.VariableSynchronization.ON_READ\n                    ) if gradient is not None else None for gradient in gradients\n                ]\n            )\n\n        if len(gradients) != len(self._gradients):\n            raise ValueError(\"Expected %s gradients, but got %d\" % (\n                    len(self._gradients), len(gradients)))\n\n        for accum_gradient, gradient in zip(self._gradients, gradients):\n            if gradient is not None:\n                accum_gradient.assign_add(gradient)\n\n    def reset(self):\n        \"\"\"Resets the accumulated gradients on the current replica.\"\"\"\n\n        if not self._gradients:\n            return\n\n        for gradient in self._gradients:\n            if gradient is not None:\n                gradient.assign(tf.zeros_like(gradient))","bfbbb17b":"# The total number of examples for which the training procedure will compute the gradients once in order to accumulate them.\n# This is also used for validation step.\nBATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n\n# The number of examples for which the training procedure will update the model's parameters once.\n# This is the `effective` batch size, which will be used in tf.data.Dataset.\nUPDATE_SIZE = BATCH_SIZE * BATCHES_PER_UPDATE\n\n# The number of parameter updates in 1 epoch\nUPDATES_PER_EPOCH = N_TRAINING_EXAMPLES \/\/ UPDATE_SIZE\n\nPREDICTION_BATCH_SIZE = PREDICTION_BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n\n# The number of batches for a validation step.\nVALID_BATCHES_PER_EPOCH = N_VALIDATION_EXAMPLES \/\/ PREDICTION_BATCH_SIZE","57e05292":"class Toxic_Classifier(tf.keras.models.Model):\n\n    def __init__(self, transformer):\n\n        super(Toxic_Classifier, self).__init__()\n\n        self.transformer = transformer\n        self.dropout = tf.keras.layers.Dropout(rate=0.25, name='dropout')\n        self.average_pooling_layer = tf.keras.layers.GlobalAveragePooling1D(name='average_pooling_layer')\n        self.max_pooling_layer = tf.keras.layers.GlobalMaxPool1D(name='max_pooling_layer')\n        self.dense_layer = tf.keras.layers.Dense(\n            1, name='probabilities', activation='sigmoid',\n            kernel_initializer=tf.keras.initializers.GlorotUniform(seed=OP_SEED),\n            bias_initializer='zeros'\n        )\n\n    def call(self, inputs, **kwargs):\n\n        comments = inputs\n        attention_mask = tf.math.not_equal(comments, PADDING_TOKEN)\n\n        # sequence_outpu: shape = [batch_size, seq_len, hidden_dim]\n        # pooled_output: shape = [batch_size, hidden_dim]\n        sequence_output = self.transformer([comments, attention_mask], **kwargs)[0]\n\n        # shape = [batch_size, hidden_dim]\n        average_pooling = self.average_pooling_layer(sequence_output, mask=attention_mask)\n        \n        # Avoid the padding timestamps to contribute to `max_pooling`\n        sequence_output_masked = sequence_output * tf.cast(attention_mask, tf.float32)[:, :, tf.newaxis] - (1e9) * tf.cast(tf.math.logical_not(attention_mask), tf.float32)[:, :, tf.newaxis]\n        max_pooling = self.max_pooling_layer(sequence_output_masked)\n\n        pooling = tf.concat([average_pooling, max_pooling], axis=1)\n\n        # shape = [batch_size, seq_len, hidden_dim]\n        x = self.dropout(pooling, training=kwargs.get('training', False))\n\n        # shape = [batch_size, 1]\n        probabilities = self.dense_layer(x)\n\n        return probabilities\n\n\nclass CustomExponentialDecaySchedule(tf.keras.optimizers.schedules.ExponentialDecay):\n    \"\"\"\n    Learning rate with exponential decay and linear warmup.\n    \"\"\"\n\n    def __init__(\n            self,\n            start_lr,\n            max_lr,\n            ending_lr,\n            num_training_steps,\n            num_warmup_steps,\n            scaling=1,\n            cycle=False,\n            name=None,\n      ):\n\n        self.start_lr = tf.cast(start_lr, tf.float32)\n        self.max_lr = tf.cast(max_lr, tf.float32)\n        self.ending_lr = tf.cast(ending_lr, tf.float32)\n\n        self.decay_rate = self.ending_lr \/ self.max_lr\n\n        self.num_training_steps = tf.cast(num_training_steps, tf.float32)\n        self.num_warmup_steps = max(num_warmup_steps, 0)\n\n        self.decay_steps = self.num_training_steps - self.num_warmup_steps\n\n        self.decay_lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n            self.max_lr,\n            self.decay_steps,\n            self.decay_rate,\n            staircase=False,\n            name=name\n        )\n\n        self.scaling = scaling\n\n        self.cycle = tf.constant(cycle, dtype=tf.bool)\n\n    def __call__(self, step):\n\n        step = tf.cond(self.cycle and step >= self.num_training_steps, lambda: step % self.num_training_steps, lambda: tf.cast(step, tf.float32))\n        lr = self.decay_lr_schedule(step)\n        lr = tf.math.maximum(lr, self.ending_lr)\n\n        if self.num_warmup_steps > 0:\n\n            num_warmup_steps = tf.cast(self.num_warmup_steps, tf.float32)\n\n            is_warmup = tf.cast(step < num_warmup_steps, tf.float32)\n\n            warmup_lr = (self.max_lr - self.start_lr) \/ num_warmup_steps * step + self.start_lr\n\n            decay_lr = self.decay_lr_schedule(step - num_warmup_steps)\n            decay_lr = tf.math.maximum(decay_lr, self.ending_lr)\n\n            lr = (1.0 - is_warmup) * decay_lr + is_warmup * warmup_lr\n\n        return lr * self.scaling","1d3d9519":"def set_model(learning_rate_scaling=1):\n\n    with strategy.scope():\n\n        transformer = TFAutoModel.from_pretrained(TRANSFORMER_TYPE)\n        model = Toxic_Classifier(transformer)\n\n        # number of training steps\n        num_training_steps = UPDATES_PER_EPOCH * EPOCHS\n\n        # warmup epochs\n        num_warmup_steps = UPDATES_PER_EPOCH * WARMUP_EPOCHS\n\n        lr_schedule = CustomExponentialDecaySchedule(\n            start_lr=START_LR,\n            max_lr=MAX_LR,\n            ending_lr=ENDING_LR,\n            num_training_steps=num_training_steps,\n            num_warmup_steps=num_warmup_steps,\n            scaling=learning_rate_scaling,\n            cycle=False,\n            name=None\n        )\n\n        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n\n        gradient_accumulator = None\n        if USE_GRADIENT_ACCUMULATION:\n            gradient_accumulator = GradientAccumulator()\n            gradient_accumulator.reset()\n\n        # Instantiate metrics\n        train_accuracy = tf.keras.metrics.BinaryAccuracy()\n        valid_accuracy = tf.keras.metrics.BinaryAccuracy()\n        train_loss = tf.keras.metrics.Sum()\n        valid_loss = tf.keras.metrics.Sum()\n\n        loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.SUM)\n\n        return model, loss_fn, optimizer, gradient_accumulator, train_loss, train_accuracy, valid_loss, valid_accuracy","864a54b2":"def set_routines():\n\n    with strategy.scope():\n\n        def train_step_1_forward_backward(comments, labels):\n\n            with tf.GradientTape() as tape:\n\n                probabilities = model(comments, training=True)\n                loss = loss_fn(labels, probabilities)\n\n                # Take into account the fact that 1 parameter update for `UPDATE_SIZE` training examples.\n                scaled_loss = loss \/ UPDATE_SIZE\n\n            grads = tape.gradient(scaled_loss, model.trainable_variables)\n\n            # Accumulated already scaled gradients\n            if USE_GRADIENT_ACCUMULATION:\n                gradient_accumulator(grads)\n\n            # update metrics\n            train_accuracy.update_state(labels, probabilities)\n            train_loss.update_state(loss)\n\n            if not USE_GRADIENT_ACCUMULATION:\n                return grads\n\n        def train_step_1_update(comments, labels):\n\n            if not USE_GRADIENT_ACCUMULATION:\n\n                grads = train_step_1_forward_backward(comments, labels)\n                optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))\n\n            else:\n\n                for _ in tf.range(BATCHES_PER_UPDATE):\n\n                    # Take the 1st `BATCH_SIZE_PER_REPLICA` examples.\n                    small_comments = comments[:BATCH_SIZE_PER_REPLICA]\n                    small_labels = labels[:BATCH_SIZE_PER_REPLICA]\n\n                    train_step_1_forward_backward(small_comments, small_labels)\n\n                    # Move the leading part to the end, so the shape is not changed.\n                    comments = tf.concat([comments[BATCH_SIZE_PER_REPLICA:], small_comments], axis=0)\n                    labels = tf.concat([labels[BATCH_SIZE_PER_REPLICA:], small_labels], axis=0)\n\n                # Update the model's parameters\n                gradients = gradient_accumulator.gradients()\n                optimizer.apply_gradients(list(zip(gradients, model.trainable_variables)))\n                gradient_accumulator.reset()\n\n        @tf.function\n        def dist_train_step(batch):\n            strategy.experimental_run_v2(train_step_1_update, batch)\n\n        @tf.function\n        def dist_train_1_epoch(data_iter):\n\n            for _ in tf.range(UPDATES_PER_EPOCH):\n                dist_train_step(next(data_iter))\n\n        def valid_step(comments, labels):\n\n            probabilities = model(comments, training=False)\n            loss = loss_fn(labels, probabilities)\n\n            # update metrics\n            valid_accuracy.update_state(labels, probabilities)\n            valid_loss.update_state(loss)\n\n        @tf.function\n        def dist_valid_step(batch):\n            strategy.experimental_run_v2(valid_step, batch)\n\n        @tf.function\n        def dist_valid(data_iter):\n\n            for _ in tf.range(VALID_BATCHES_PER_EPOCH):\n                dist_valid_step(next(data_iter))\n\n        return dist_train_1_epoch, dist_train_step, dist_valid, dist_valid_step","65edeb96":"print(\"NUM_TRAINING_EXAMPLES: {}\".format(N_TRAINING_EXAMPLES))\nprint(\"BATCH_SIZE_PER_REPLICA: {}\".format(BATCH_SIZE_PER_REPLICA))\nprint(\"BATCH_SIZE: {}\".format(BATCH_SIZE))\nprint(\"BATCHES_PER_UPDATE: {}\".format(BATCHES_PER_UPDATE))\nprint(\"UPDATE_SIZE: {}\".format(UPDATE_SIZE))\nprint(\"UPDATES_PER_EPOCH: {}\".format(UPDATES_PER_EPOCH))\nprint(\"NUM_VALIDATION_EXAMPLES: {}\".format(N_VALIDATION_EXAMPLES))\nprint(\"PREDICTION_BATCH_SIZE: {}\".format(PREDICTION_BATCH_SIZE))\nprint(\"VALID_BATCHES_PER_EPOCH: {}\".format(VALID_BATCHES_PER_EPOCH))\n\ntrain_ds = get_training_dataset(batch_size=UPDATE_SIZE)\ntrain_dist_ds = strategy.experimental_distribute_dataset(train_ds)\n\nvalid_ds = get_validation_dataset(batch_size=PREDICTION_BATCH_SIZE, repeated=True)\nvalid_dist_ds = strategy.experimental_distribute_dataset(valid_ds)","1c844284":"if not OPTIMIZED_LOOP:\n\n    with strategy.scope():\n    \n        model, loss_fn, optimizer, gradient_accumulator, train_loss, train_accuracy, valid_loss, valid_accuracy = set_model(learning_rate_scaling=LEARNING_RATE_SCALING)\n        dist_train_1_epoch, dist_train_step, dist_valid, dist_valid_step = set_routines()\n\n        for epoch_idx in range(EPOCHS):\n\n            s = datetime.datetime.now()\n\n            for batch_idx, batch in enumerate(train_dist_ds):\n\n                if batch_idx >= UPDATES_PER_EPOCH:\n                    break\n\n                dist_train_step(batch)\n\n                loss = train_loss.result() \/ ((batch_idx + 1) * UPDATE_SIZE)\n                acc = train_accuracy.result()\n\n                if (batch_idx + 1) == UPDATES_PER_EPOCH or (batch_idx + 1) % (UPDATES_PER_EPOCH \/\/ 10) == 0:\n                    print(\"epoch: {} | batch: {}\".format(epoch_idx + 1, batch_idx + 1))\n                    print(\"train loss: {}\".format(loss))\n                    print(\"train accuracy: {}\".format(acc))\n\n            train_loss.reset_states()\n            train_accuracy.reset_states()\n\n            e = datetime.datetime.now()\n            print(\"training time: {}\".format((e-s).total_seconds()))\n\n            s = datetime.datetime.now()\n\n            for batch_idx, batch in enumerate(valid_dist_ds):\n\n                if batch_idx >= VALID_BATCHES_PER_EPOCH:\n                    break\n\n                dist_valid_step(batch)\n\n                val_loss = valid_loss.result() \/ (VALID_BATCHES_PER_EPOCH * PREDICTION_BATCH_SIZE)\n                val_acc = valid_accuracy.result()\n\n            print(\"valid loss: {}\".format(val_loss))\n            print(\"valid accuracy: {}\".format(val_acc))\n\n            valid_loss.reset_states()\n            valid_accuracy.reset_states()\n\n            e = datetime.datetime.now()\n            print(\"validation time: {}\".format((e-s).total_seconds()))\n\n            print(\"-\" * 80)","9a97a4b7":"if OPTIMIZED_LOOP:\n\n    with strategy.scope():\n\n        model, loss_fn, optimizer, gradient_accumulator, train_loss, train_accuracy, valid_loss, valid_accuracy = set_model(learning_rate_scaling=LEARNING_RATE_SCALING)\n        dist_train_1_epoch, dist_train_step, dist_valid, dist_valid_step = set_routines()\n\n        train_data_iter = iter(train_dist_ds)\n        valid_data_iter = iter(valid_dist_ds)\n\n        for epoch_idx in range(EPOCHS):\n\n            s = datetime.datetime.now()\n\n            dist_train_1_epoch(train_data_iter)\n\n            loss = train_loss.result() \/ (UPDATES_PER_EPOCH * UPDATE_SIZE)\n            acc = train_accuracy.result()\n\n            print(\"epoch: {}\".format(epoch_idx + 1))\n            print(\"train loss: {}\".format(loss))\n            print(\"train accuracy: {}\".format(acc))\n\n            train_loss.reset_states()\n            train_accuracy.reset_states()\n\n            e = datetime.datetime.now()\n            print(\"training time: {}\".format((e-s).total_seconds()))\n\n            s = datetime.datetime.now()\n\n            dist_valid(valid_data_iter)\n\n            val_loss = valid_loss.result() \/ (VALID_BATCHES_PER_EPOCH * PREDICTION_BATCH_SIZE)\n            val_acc = valid_accuracy.result()\n\n            print(\"valid loss: {}\".format(val_loss))\n            print(\"valid accuracy: {}\".format(val_acc))\n\n            valid_loss.reset_states()\n            valid_accuracy.reset_states()\n\n            e = datetime.datetime.now()\n            print(\"validation time: {}\".format((e-s).total_seconds()))\n\n            print(\"-\" * 80)","fa06a492":"with strategy.scope():\n\n    def predict_step(comments):\n\n        probabilities = model(comments, training=False)\n\n        return probabilities\n\n    @tf.function\n    def dist_predict_step(batch):\n\n        probabilities = strategy.experimental_run_v2(predict_step, args=[batch])\n\n        return probabilities\n\ntest_ds = get_test_dataset(batch_size=PREDICTION_BATCH_SIZE)\ndist_test_ds = strategy.experimental_distribute_dataset(test_ds)\nprint(\"PREDICTION_BATCH_SIZE: {}\".format(PREDICTION_BATCH_SIZE))\nPREDICTION_BATCHES = N_TEST_EXAMPLES \/\/ PREDICTION_BATCH_SIZE + int(N_TEST_EXAMPLES % PREDICTION_BATCH_SIZE > 0)\nprint(\"PREDICTION_BATCHES: {}\".format(PREDICTION_BATCHES))\n\nall_probabilities = []\nfor batch in tqdm(dist_test_ds):\n\n    # PerReplica object\n    probabilities = dist_predict_step(batch)\n\n    # Tuple of tensors\n    probabilities = strategy.experimental_local_results(probabilities)\n\n    # tf.Tensor\n    probabilities = tf.concat(probabilities, axis=0)\n\n    all_probabilities.append(probabilities)\n\n# tf.Tensor\npredictions = tf.concat(all_probabilities, axis=0)\n# numpy\npredictions = predictions.numpy()\n\nsubmission = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/sample_submission.csv')\nsubmission['toxic'] = predictions\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head(10)","e84ed15d":"model.save_weights(\"model.h5\")","eb5a7284":"## Predict and submit","dd1ba96d":"## About this notebook\n\nThis notebook focus on: Gradient Accumulation with Tensorflow & TPU.\n\nFrom [Distributions of no. of tokens](#Distributions-of-no.-of-tokens), we can see that there are non-negligible number of comments which have more than 256 tokens. It is probably worth training with a higher value for `MAX_LEN`. However, with a large model like `XLM Roberta Large`, we can't have the same batch size for `MAX_LEN = 512` as for `MAX_LEN = 192`. So I looked how [OpenNMT\/OpenNMT-tf](https:\/\/github.com\/OpenNMT\/OpenNMT-tf\/) implements gradient accumulation and copy the code with modification to this kernel.\n\nThe following picture shows the results of training on 65536 examples with different batch configurations (use the optimized training loop).\n\n![batch.PNG](attachment:batch.PNG)\n\nFor `MAX_LEN = 192`, we can train with `batch_size_per_replica = 16` (without grandient accumulatioin). However, for `MAX_LEN = 256` or higher, `batch_size_per_replica = 16` no longer works. In order to keep the same effective batch size, it is necessary to use a smaller value for `batch_size_per_replica` with gradient accumulation.\n\nThe training configuation (fewer epochs \/ training examples, larger learning rate and `MAX_LEN = 192`) in this version is only for demonstrating the gradient accumulation works. Currently, my best LB score is given by:\n\n    * MAX_LEN = 512\n    * EPOCHS = 8\n    * WARMUP_EPOCHS = 4\n    * LEARNING_RATE_SCALING = 1\n    * N_NEGATIVE_EXAMPLES_FACTOR = 2\n    \nHowever, it takes too long to train, and it can't be trained on Kaggle. I am still looking for a faster training configuation that can achieve the same result.","cbdefa85":"## Configuration","00fdfbd3":"## Model","5c04c2e2":"## Get datasets","64ee407d":"## Peek the datasets","54775c41":"## Training routines","b5e05324":"## Datasets","85b502a2":"## Usual custom training loop\nSlower, but training information could be shown more often.","f1e28202":"## Dataset methods","d1b96939":"## Seed","97ad05b6":"## TPU or GPU detection","8d87fc10":"## Distributions of no. of tokens\n\nWe only look the comments with no. of tokens in the range (256, 512].\n\nThe tokens are obtained from the `bert-base-multilingual-cased` tokenizer. However, the distributions should be almost the same even other tokenizers are used.\n\n| ![ccc](attachment:train_long.png) | ![valid_long.png](attachment:valid_long.png) | ![test_long.png](attachment:test_long.png) |\n|:-|:-|:-|  \n|training dataset|validation dataset|test dataset| \n","a2fd1f3a":"## batch configurations","26445c5f":"## Gradient Accumulator\n\nCopy from the [OpenNMT\/OpenNMT-tf](https:\/\/github.com\/OpenNMT\/OpenNMT-tf\/) with modification.\n","90a3591c":"## Optimized custom training loop\n\nOptimized by calling the TPU less often and performing more steps per call.\nTraining information is shown after each epoch.\n","be7f0bf3":"## Save model weights"}}