{"cell_type":{"32f46927":"code","007a902f":"code","9efa2aa1":"code","20becfeb":"code","2e39fa63":"code","a39cd904":"code","6e3a0ead":"code","1ca19027":"code","f03be2dd":"code","c1e454eb":"code","8c455b12":"code","118354ca":"code","3a0a4c3e":"code","240b225d":"code","fb453b7a":"code","48b982e5":"code","3ab073ed":"code","38f1a49f":"code","d81383eb":"code","4be40978":"code","088b2695":"code","9bd042d8":"code","bc6e43ae":"code","2dd686a4":"code","c9bc780e":"code","b95f2b35":"code","f56f2437":"code","84597142":"code","11ae8f60":"code","d9bf88d3":"code","af2c9b75":"code","564dc9d0":"code","68156330":"code","39bd8c6f":"code","b94ed542":"code","e97ccab5":"code","ede2603b":"code","bd28d82e":"code","a3620368":"code","0605965e":"code","b515cf9a":"markdown","73483cb6":"markdown","29f3a934":"markdown","5ed7043e":"markdown","43143074":"markdown","a3324c70":"markdown","111922d0":"markdown","214a16ea":"markdown","673d016f":"markdown","ce3868e8":"markdown","a811ebdf":"markdown","524a2f55":"markdown","7313f25f":"markdown","57b5db55":"markdown","8f7f7571":"markdown","4e3e06fe":"markdown","b695fae2":"markdown","db75c3f5":"markdown","0e4bc401":"markdown","5eb0a2b0":"markdown","ad6422cf":"markdown","65fa86af":"markdown","e2098198":"markdown","ac87d0e5":"markdown","317e35a4":"markdown","aaa13d3a":"markdown","805c6ae3":"markdown","961fec43":"markdown","03083e2c":"markdown","806f68a8":"markdown","7b9d5615":"markdown","e35e243c":"markdown","7e0923c9":"markdown","5c2a2622":"markdown","1eff6f52":"markdown","662d9ec3":"markdown","622f90e4":"markdown","f3087ffc":"markdown","75b69a87":"markdown","64f32879":"markdown","b6f0772f":"markdown","ee124547":"markdown","2473e15e":"markdown","c379e8ef":"markdown","6f54e795":"markdown"},"source":{"32f46927":"# Load Libraries:\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport plotly.offline as pyo \nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly.offline import init_notebook_mode,iplot\ninit_notebook_mode(connected = True)\nfrom matplotlib.colors import ListedColormap\nfrom statsmodels.graphics.gofplots import qqplot\nfrom scipy.stats import shapiro\nfrom scipy.stats import stats\nfrom scipy.stats import norm\nfrom scipy.stats import probplot\nimport scipy as stat\nfrom scipy.stats import skew\nfrom scipy import stats\nfrom sklearn.preprocessing import PowerTransformer\n#\nfrom sklearn.metrics import classification_report \nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.metrics import accuracy_score \n#\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler,RobustScaler\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.model_selection import KFold \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\n#\nfrom sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis, LocalOutlierFactor\nfrom sklearn.decomposition import PCA\n#\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","007a902f":"df = pd.read_csv(\"\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv\")\ndf.head(10)","9efa2aa1":"# Drop Unnecessary columns\ndf.drop([\"Unnamed: 32\",\"id\"],axis=1,inplace=True)\ndf.head(10)","20becfeb":"describe = df.describe().T\nprint(describe)","2e39fa63":"\n\ndef check_skewness(dataframe):\n\n    global describe\n    \n    for i in range(describe.shape[1]):\n        mean = round(describe.iloc[1,i],3)\n        median = round(describe.iloc[5,i],3)\n    \n        if mean > median:\n            print(\"Positive Skewness\\t:\\t{}\\t\".format(describe.columns[i]))\n        elif mean == median:\n            print(\"No Skew          \\t:\\t{}\\t\".format(describe.columns[i]))\n        else:\n            print(\"Negative Skewness\\t:\\t{}\\t\".format(describe.columns[i]))\n            \ncheck_skewness(df)","a39cd904":"#df.info()\ndef Missing_Values_Table(df):\n    missing_value = df.isnull().sum()\n    missing_value_percent = 100*df.isnull().sum()\/len(df)\n    missing_value_table = pd.concat([missing_value,missing_value_percent],axis = 1)\n    missing_value_table__ = missing_value_table.rename(columns = {0 : 'Missing Values', 1 : '% Percent'})\n    return missing_value_table__\n\nMissing_Values_Table(df)","6e3a0ead":"def descriptive_statistics(dataframe, target_column, column_name):\n\n    df_B, df_M = [i for _, i in df.groupby(dataframe[target_column])]\n\n    # normality test (hypothesis testing)\n\n    stat_M, p_M = shapiro(df_M[column_name])\n    stat_B, p_B = shapiro(df_B[column_name])\n\n    alpha = 0.05\n\n    if p_M > alpha:\n        print(\"Malignant {} P value {:.3f} --> Gaussian Distribution\".format(column_name, p_M))\n        a = \"P Value:\"+\" \"+str(round(p_M, 3))+\"-->\"+\"Gaussian Distribution\"\n    else:\n        print(\"Malignant {} P value: {:.3f} --> Not Gaussian Distribution\".format(column_name, p_M))\n        a = \"P Value:\"+\" \"+str(round(p_M, 3))+\"-->\"+\"Not Gaussian Distribution\"\n    if p_B > alpha:\n        print(\"Benign {} P value {:.3f} --> Gaussian Distribution\".format(column_name, p_B))\n        b = \"P Value:\"+\" \"+str(round(p_B, 3))+\"-->\"+\"Gaussian Distribution\"\n    else:\n        print(\"Benign {} P value: {:.3f} --> Not Gaussian Distribution\".format(column_name, p_B))\n        b = \"P Value:\"+\" \"+str(round(p_B, 3))+\"-->\"+\"Not Gaussian Distribution\"\n\n    # control chart\n\n    mean_M = df_M[column_name].mean()\n    mean_B = df_B[column_name].mean()\n    three_std_M = np.std(df_M[column_name]) * 3\n    three_std_B = np.std(df_B[column_name]) * 3\n    UCL_M = mean_M + three_std_M\n    UCL_B = mean_B + three_std_B\n    LCL_M = mean_M - three_std_M\n    LCL_B = mean_B - three_std_B\n\n    df_M_control_chart = pd.DataFrame()\n    df_M_control_chart[column_name] = df_M[column_name]\n    df_M_control_chart[\"mean_M\"] = mean_M\n    df_M_control_chart[\"UCL_M\"] = UCL_M\n    df_M_control_chart[\"LCL_M\"] = LCL_M\n\n    df_B_control_chart = pd.DataFrame()\n    df_B_control_chart[column_name] = df_B[column_name]\n    df_B_control_chart[\"mean_B\"] = mean_B\n    df_B_control_chart[\"UCL_B\"] = UCL_B\n    df_B_control_chart[\"LCL_B\"] = LCL_B\n\n    # Visualization\n \n    fig = plt.figure(figsize=(30, 13))\n    fig.suptitle(\"Malignant\"+\"-->\"+column_name+\" \"+\"(\"+a+\")\", fontsize=30, color=\"r\")\n    ax1 = fig.add_subplot(131)\n    qqplot(df_M[column_name], line = \"s\", ax=ax1)\n    ax1.set_title(\"Probability Plot\")\n\n    ax2 = fig.add_subplot(132)\n    sns.distplot(df_M[column_name], fit=norm, ax=ax2)\n    ax2.set_title(\"Distplot\")\n\n    ax3 = fig.add_subplot(133)\n    df_M_control_chart[column_name].plot(marker=\"o\", ax=ax3)\n    df_M_control_chart[\"mean_M\"].plot(color=\"r\", ax=ax3)\n    df_M_control_chart[\"UCL_M\"].plot(color=\"b\", ax=ax3)\n    df_M_control_chart[\"LCL_M\"].plot(color=\"g\", ax=ax3)\n    ax3.set_title(\"Control Chart\")\n\n    plt.show()\n\n    fig = plt.figure(figsize=(30, 13))\n    fig.suptitle(\"Benign\" + \"-->\" + column_name + \" \" + \"(\" + b + \")\", fontsize=30, color=\"r\")\n    ax1 = fig.add_subplot(131)\n    qqplot(df_B[column_name], line=\"s\", ax=ax1)\n    ax1.set_title(\"Probability Plot\")\n\n    ax2 = fig.add_subplot(132)\n    sns.distplot(df_B[column_name], fit=norm, ax=ax2)\n    ax2.set_title(\"Distplot\")\n\n    ax3 = fig.add_subplot(133)\n    df_B_control_chart[column_name].plot(marker=\"o\", ax=ax3)\n    df_B_control_chart[\"mean_B\"].plot(color=\"r\", ax=ax3)\n    df_B_control_chart[\"UCL_B\"].plot(color=\"b\", ax=ax3)\n    df_B_control_chart[\"LCL_B\"].plot(color=\"g\", ax=ax3)\n    ax3.set_title(\"Control Chart\")\n\n    plt.show()","1ca19027":"#df.columns\n'''\n['diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst'],\n'''\n\n\ndescriptive_statistics(df, \"diagnosis\", \"area_se\")\n","f03be2dd":"df = df.rename(columns = {\"diagnosis\":\"Target\"})\n\ndf[\"Target\"] = [ 1 if i.strip() == \"M\" else 0 for i in df[\"Target\"]]","c1e454eb":"# correlation:\ncorr_matrix = df.corr()\nsns.clustermap(corr_matrix,annot=True,fmt=\".2f\",figsize=(20,15))\nplt.title(\"Correlation Features\")","8c455b12":"thrs = 0.7\nfilt = np.abs(corr_matrix[\"Target\"])>=thrs\ncorrelation_features = corr_matrix.columns[filt].tolist()\nsns.clustermap(df[correlation_features].corr(),annot = True, fmt = \".2f\")\nplt.title(\"Highly Correlated Features With Target\")\nplt.show()\n\n","118354ca":"def ThreeD_visualization(x,y,target):\n\n    trace1 = go.Scatter3d(\n        x=df[x],\n        y=df[y],\n        z=df[target],\n        mode='markers',\n        marker=dict(\n            size=10,\n            color=\"red\",\n        )\n    )\n\n    dff = [trace1]\n    layout = go.Layout(\n        margin=dict(\n            l=0,\n            r=0,\n            b=0,\n            t=0  \n        )\n\n    )\n    fig = go.Figure(data=dff, layout=layout)\n    iplot(fig)","3a0a4c3e":"#df.columns\n'''\n['Target', 'radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst'],\n'''\n\n\nThreeD_visualization(\"area_se\",\"concavity_se\",\"Target\")","240b225d":"# pair plot\nthrs = 0.6\nfilt1 = np.abs(corr_matrix[\"Target\"])<=thrs\ncorrelation_features = corr_matrix.columns[filt1].tolist()\nsns.pairplot(df[correlation_features], diag_kind=\"kde\")\nplt.show()","fb453b7a":"# We only used StandardScaler for visualization.\n\nY_visualization = df[\"Target\"]\nX_visualization = df.drop(\"Target\",axis = 1)\n\nscaler_visualization = StandardScaler()\nX_visualization = scaler_visualization.fit_transform(X_visualization)\n\ndf_visualization = pd.DataFrame(X_visualization)\ndf_visualization[\"Target\"] = Y_visualization\n\n\ndf_melted = pd.melt(df_visualization,id_vars=\"Target\",var_name=\"features\",value_name=\"value\")\nplt.figure(figsize=(25,15))\nsns.boxplot(x=\"features\",y=\"value\",hue=\"Target\",data=df_melted)\nplt.show()","48b982e5":"sns.countplot(df[\"Target\"])\nplt.show()","3ab073ed":"y = df[\"Target\"]\nX = df.drop([\"Target\"],axis=1)\ncolumns = X.columns.tolist()","38f1a49f":"clf = LocalOutlierFactor()\ny_pred = clf.fit_predict(X)\noutlier = len(y_pred)-(y_pred.sum())\ninlier = len(y_pred)-outlier\n\nprint(\"We have {} Outlier and {} Inlier Values\".format(outlier,inlier))\nsns.countplot(y_pred,palette=\"Set2\")\nplt.title(\"Outlier & Inlier Values\")\nplt.show()","d81383eb":"X_score = clf.negative_outlier_factor_\noutlier_score = pd.DataFrame()\noutlier_score[\"score\"] = X_score","4be40978":"np.abs(outlier_score[\"score\"]).sort_values(ascending=False).head(10)","088b2695":"thresh = -2.5\nfilt = outlier_score[\"score\"] < thresh\noutlier_index = outlier_score[filt].index.tolist()\n\n# Radius for our outliers\nradius = (X_score.max()-X_score)\/(X_score.max()-X_score.min())\noutlier_score[\"radius\"] = radius\n","9bd042d8":"plt.figure(figsize=(25,15))\nplt.scatter(X.iloc[outlier_index,0],X.iloc[outlier_index,1],color = \"blue\",s=50, label = \"Outlier\")\nplt.scatter(X.iloc[:,0],X.iloc[:,1],color = \"k\",s=3, label = \"Data Points\")\nplt.scatter(X.iloc[:,0],X.iloc[:,1],color = \"k\",s=1000*radius,edgecolors=\"r\",facecolors=\"none\", label = \"Outlier Scores\")\nplt.legend()\nplt.show()","bc6e43ae":"y = df[\"Target\"]\nX = df.drop([\"Target\"],axis=1)\ncolumns = X.columns.tolist()","2dd686a4":"skewed = X.apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame(skewed, columns=[\"skewness\"])\nprint(\"Before:\",skewness)\n\npt = PowerTransformer(standardize=False, copy=False)\npt.fit_transform(X[\"area_se\"].values.reshape(-1, 1))\n\nskewed = X.apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame(skewed, columns=[\"skewness\"])\nprint(\"After:\",skewness)","c9bc780e":"x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","b95f2b35":"scaler = RobustScaler()\nx_train = scaler.fit_transform(x_train)\nx_test  = scaler.transform(x_test) ","f56f2437":"# logistic regression\n\nfrom sklearn.linear_model import LogisticRegression\nlogr = LogisticRegression(random_state=42,max_iter=10000)\nlogr.fit(x_train,y_train)\ny_pred_logr = logr.predict(x_test)\n\nlogr_cm = confusion_matrix(y_test,y_pred_logr)\nlogr_acc = metrics.accuracy_score(y_test, y_pred_logr)\nprint(logr_cm)\nprint(logr_acc)\n\n","84597142":"scaler2 = RobustScaler()\nx_scaled = scaler2.fit_transform(X)\n","11ae8f60":"pca = PCA(n_components=2,random_state=42)\npca.fit(x_scaled)\nx_pca = pca.transform(x_scaled)\npca_df = pd.DataFrame(x_pca,columns=[\"p1\", \"p2\"])\npca_df[\"target\"] = y\n\nplt.figure(figsize=(15,10))\nsns.scatterplot(x=\"p1\", y=\"p2\", hue=\"target\",data=pca_df)\nplt.title(\"PCA Components\")\nplt.show()","d9bf88d3":"x_train_pca,x_test_pca,y_train_pca,y_test_pca = train_test_split(x_pca,y,test_size=0.33,random_state=42)","af2c9b75":"def Logistic_Regression_Best_params(x_train,x_test,y_train,y_test):\n\n    C = np.logspace(-4, 4, 50)\n    penalty = ['l1', 'l2']\n    max_iter = list(range(1000, 10000, 1000))\n    random_state = 42\n    print(\"*************************************************************\")\n    param_grid = {\"C\": C, \"penalty\": penalty, \"max_iter\": max_iter}\n\n    logr = LogisticRegression(random_state=random_state)\n    grid = GridSearchCV(logr, param_grid, cv=10, scoring=\"accuracy\")\n    grid.fit(x_train, y_train)\n\n    print(\"Best Training Score: {} with parameters: {} \".format(grid.best_score_,grid.best_params_))\n    print(\"*************************************************************\")\n\n    logr = LogisticRegression(**grid.best_params_)\n    logr.fit(x_train, y_train)\n\n    y_pred_test = logr.predict(x_test)\n    y_pred_train = logr.predict(x_train)\n\n    cm_test = confusion_matrix(y_test, y_pred_test)\n    cm_train = confusion_matrix(y_train, y_pred_train)\n\n    acc_test = accuracy_score(y_test, y_pred_test)\n    acc_train = accuracy_score(y_train, y_pred_train)\n\n    print(\"Test Score : {}, Train Score : {}\".format(acc_test, acc_train))\n    print()\n    print(\"Conf Matrix Test\\n\", cm_test)\n    print(\"Conf Matrix Train\\n\", cm_train)\n\n    return grid\n\n\ngrid_logr_pca = Logistic_Regression_Best_params(x_train_pca,x_test_pca,y_train_pca,y_test_pca)","564dc9d0":"cmap_light = ListedColormap(['orange',  'cornflowerblue'])\ncmap_bold = ListedColormap(['darkorange', 'darkblue'])\n\nh = .3 # step size in the mesh\nX = pca_df\nx_min, x_max = (X.iloc[:, 0].min() - 1), (X.iloc[:, 0].max() + 1)\ny_min, y_max = X.iloc[:, 1].min() - 1, X.iloc[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\nZ = grid_logr_pca.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure(figsize=(20, 10), dpi=80)\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n# Plot also the training points\nplt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, cmap=cmap_bold,\n            edgecolor='k', s=20)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.title(\"Logistic Regression & PCA\")\nplt.show()","68156330":"def KNN_Best_params(x_train,x_test,y_train,y_test):\n\n    k_range = list(range(1,31))\n    weight_potions = [\"uniform\",\"distance\"]\n    print()\n    param_grid = {'n_neighbors': k_range, 'weights': weight_potions}\n\n    knn = KNeighborsClassifier()\n    grid = GridSearchCV(knn, param_grid, cv=10, scoring=\"accuracy\")\n    grid.fit(x_train, y_train)\n\n    print(\"Best Training Score: {} with parameters: {} \".format(grid.best_score_,grid.best_params_))\n    print()\n\n    knn = KNeighborsClassifier(**grid.best_params_)\n    knn.fit(x_train,y_train)\n\n    y_pred_test = knn.predict(x_test)\n    y_pred_train = knn.predict(x_train)\n\n    cm_test = confusion_matrix(y_test, y_pred_test)\n    cm_train = confusion_matrix(y_train, y_pred_train)\n\n    acc_test = accuracy_score(y_test, y_pred_test)\n    acc_train = accuracy_score(y_train, y_pred_train)\n\n    print(\"Test Score : {}, Train Score : {}\".format(acc_test, acc_train))\n    print()\n    print(\"Conf Matrix Test\\n\", cm_test)\n    print(\"Conf Matrix Train\\n\", cm_train)\n\n    return grid","39bd8c6f":"grid_knn_pca = KNN_Best_params(x_train_pca,x_test_pca,y_train_pca,y_test_pca)","b94ed542":"cmap_light = ListedColormap(['orange',  'cornflowerblue'])\ncmap_bold = ListedColormap(['darkorange', 'darkblue'])\n\nh = .3 # step size in the mesh\nX = pca_df\nx_min, x_max = (X.iloc[:, 0].min() - 1), (X.iloc[:, 0].max() + 1)\ny_min, y_max = X.iloc[:, 1].min() - 1, X.iloc[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\nZ = grid_knn_pca.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure(figsize=(20, 10), dpi=80)\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n# Plot also the training points\nplt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, cmap=cmap_bold,\n            edgecolor='k', s=20)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.title(\"KNN & PCA\")\nplt.show()","e97ccab5":"nca = NeighborhoodComponentsAnalysis(n_components=2,random_state=42)\nnca.fit(x_scaled,y)\nx_reduced_nca = nca.transform(x_scaled)\nnca_data = pd.DataFrame(x_reduced_nca,columns=[\"p1\", \"p2\"])\nnca_data[\"target\"] = y\n\nplt.figure(figsize=(15,10))\nsns.scatterplot(x=\"p1\", y=\"p2\", hue=\"target\",data=nca_data)\nplt.title(\"NCA Components\")\nplt.show()","ede2603b":"x_train_nca,x_test_nca,y_train_nca,y_test_nca = train_test_split(x_reduced_nca,y,test_size=0.33,random_state=42)","bd28d82e":"def KNN_Best_params(x_train,x_test,y_train,y_test):\n\n    k_range = list(range(1,31))\n    weight_potions = [\"uniform\",\"distance\"]\n    print()\n    param_grid = {'n_neighbors': k_range, 'weights': weight_potions}\n\n    knn = KNeighborsClassifier()\n    grid = GridSearchCV(knn, param_grid, cv=10, scoring=\"accuracy\")\n    grid.fit(x_train, y_train)\n\n    print(\"Best Training Score: {} with parameters: {} \".format(grid.best_score_,grid.best_params_))\n    print()\n\n    knn = KNeighborsClassifier(**grid.best_params_)\n    knn.fit(x_train,y_train)\n\n    y_pred_test = knn.predict(x_test)\n    y_pred_train = knn.predict(x_train)\n\n    cm_test = confusion_matrix(y_test, y_pred_test)\n    cm_train = confusion_matrix(y_train, y_pred_train)\n\n    acc_test = accuracy_score(y_test, y_pred_test)\n    acc_train = accuracy_score(y_train, y_pred_train)\n\n    print(\"Test Score : {}, Train Score : {}\".format(acc_test, acc_train))\n    print()\n    print(\"Conf Matrix Test\\n\", cm_test)\n    print(\"Conf Matrix Train\\n\", cm_train)\n\n    return grid\n","a3620368":"grid_nca = KNN_Best_params(x_train_nca, x_test_nca, y_train_nca, y_test_nca)","0605965e":"cmap_light = ListedColormap(['orange',  'cornflowerblue'])\ncmap_bold = ListedColormap(['darkorange', 'darkblue'])\n\nh = .3 # step size in the mesh\nX = nca_data\nx_min, x_max = (X.iloc[:, 0].min() - 1), (X.iloc[:, 0].max() + 1)\ny_min, y_max = X.iloc[:, 1].min() - 1, X.iloc[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\nZ = grid_nca.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure(figsize=(20, 10), dpi=80)\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n# Plot also the training points\nplt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, cmap=cmap_bold,\n            edgecolor='k', s=20)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.title(\"NCA & KNN\")\nplt.show()","b515cf9a":"<a id = \"6\"><\/a><br>\n## 2.3.1 Checking Skewness Basically\n","73483cb6":"<a id = \"30\"><\/a><br>\n## 4.2.2 Accuracy,Recall,F_Score ...","29f3a934":"<a id = \"18\"><\/a><br>\n## 3.3 Create Train and Test Dataset","5ed7043e":"<a id = \"4\"><\/a><br>\n## 2.2 import Dataset","43143074":"<a id = \"23\"><\/a><br>\n## 4.1.2 PCA","a3324c70":"We will use RobustScaler() because we have a lot of outliers\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.RobustScaler.html","111922d0":"<a id = \"7\"><\/a><br>\n## 2.3.2 Missing Values","214a16ea":"<a id = \"5\"><\/a><br>\n## 2.3 Descriptive Analysis","673d016f":"In the deployment stage you'll take your evaluation results and determine a strategy for their deployment. If a general procedure has been identified to create the relevant model(s), this procedure is documented here for later deployment. It makes sense to consider the ways and means of deployment during the business understanding phase as well, because deployment is absolutely crucial to the success of the project. This is where predictive analytics really helps to improve the operational side of your business.\n\nhttps:\/\/www.sv-europe.com\/crisp-dm-methodology\/","ce3868e8":"<a id = \"14\"><\/a><br>\n## 2.3.4.5 Count Plot","a811ebdf":"<font color = 'red'>\n<h1>Breast Cancer Wisconsin<h1>","524a2f55":"**NCA & KNN Visualization**","7313f25f":"Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\nn the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\n\nThis database is also available through the UW CS ftp server:\nftp ftp.cs.wisc.edu\ncd math-prog\/cpo-dataset\/machine-learn\/WDBC\/\n\nAlso can be found on UCI Machine Learning Repository: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+%28Diagnostic%29","57b5db55":"<a id = \"1\"><\/a><br>\n## 1-) Business Understanding","8f7f7571":"<a id = \"32\"><\/a><br>\n## 6-) DEPLOYMENT","4e3e06fe":"<a id = \"3\"><\/a><br>\n## 2.1 import Libraries","b695fae2":"<a id = \"22\"><\/a><br>\n## 4.1.1 LOGISTIC REGRESSION MODEL","db75c3f5":"<a id = \"29\"><\/a><br>\n## 4.2.1 Confusion Matrix","0e4bc401":"<a id = \"31\"><\/a><br>\n## 5-) EVALUATION","5eb0a2b0":"<a id = \"24\"><\/a><br>\n## 4.1.3 PCA & LOGISTIC REGRESSION","ad6422cf":"<a id = \"8\"><\/a><br>\n## 2.3.3 Descriptive Statistic","65fa86af":"<a id = \"12\"><\/a><br>\n## 2.3.4.3 Pair Plot","e2098198":"<a id = \"15\"><\/a><br>\n## 3-) Data Preparation","ac87d0e5":"# Introduction\n\n\n\n\n1. [BUSINESS UNDERSTANDING](#1)\n1. [DATA UNDERSTANDING](#2)\n    * [Import Libraries](#3)\n    * [Import Dataset](#4)\n    * [Descriptive Analysis](#5)\n        * [Checking Skewness Basically](#6)\n        * [Missing Values](#7)\n        * [Descriptive Statistic](#8)\n        * [Exploratary Data Analysis (EDA)](#9)\n            * [Correlation Matrix](#10)\n            * [3D Visualization](#11)\n            * [Pair Plot](#12)\n            * [Box Plot](#13)   \n            * [Count Plot](#14)\n1. [DATA PREPARATION](#15)\n    * [Outlier Detection (LOF)](#16)\n    * [Skewness & Data Transform & PowerTransformer](#17)\n    * [Split The Dataset](#18)\n    * [Standardization](#19)   \n1. [MODELLING](#20)\n    * [Select Modelling Technique](#21)\n        * [Logistic Regression](#22)\n        * [PCA](#23)\n        * [PCA & Logistic Regression](#24)\n        * [PCA & KNN](#25)\n        * [NCA](#26)\n        * [NCA & KNN](#27)\n    * [Generate Test Design](#28)\n        * [Confusion Matrix](#29)\n        * [Accuracy](#30)      \n1. [EVALUATION](#31)\n1. [DEPLOYMENT](#32)","317e35a4":"<a id = \"28\"><\/a><br>\n## 4.2 Generate Test Design","aaa13d3a":"<a id = \"2\"><\/a><br>\n## 2-) Data Understanding","805c6ae3":"![image.png](attachment:image.png)\n\nBasic measures derived from the confusion matrix;\n\n\nError Rate = (FP+FN)\/(P+N)\n\nAccuracy = (TP+TN)\/(P+N)\n\nSensitivity(Recall or True positive rate) = TP\/P\n\nSpecificity(True negative rate) = TN\/N\n\nPrecision(Positive predicted value) = TP\/(TP+FP)\n\nF-Score(Harmonic mean of precision and recall) = (1+b)(PREC.REC)\/(b\u00b2PREC+REC) where b is commonly 0.5, 1, 2.\n\nhttps:\/\/www.edureka.co\/blog\/interview-questions\/data-science-interview-questions\/#machine-learning","961fec43":"Before you actually build a model you need to generate a procedure or mechanism to test the model\u2019s quality and validity. For example, in supervised data mining tasks such as classification, it is common to use error rates as quality measures for data mining models. Therefore, you typically separate the dataset into train and test sets, build the model on the train set, and estimate its quality on the separate test set.\n\nhttps:\/\/www.sv-europe.com\/crisp-dm-methodology\/","03083e2c":"<a id = \"13\"><\/a><br>\n## 2.3.4.4 Box Plot (we can see outliers)","806f68a8":"<a id = \"27\"><\/a><br>\n## 4.1.5 NCA & KNN","7b9d5615":"<a id = \"21\"><\/a><br>\n## 4.1 Select Modelling Technique","e35e243c":"Evaluate your results\n\nPrevious evaluation steps dealt with factors such as the accuracy and generality of the model. During this step you'll assesses the degree to which the model meets your business objectives and seek to determine if there is some business reason why this model is deficient. Another option is to test the model(s) on test applications in the real application, if time and budget constraints permit. The evaluation phase also involves assessing any other data mining results you've generated. Data mining results involve models that are necessarily related to the original business objectives and all other findings that are not necessarily related to the original business objectives, but might also unveil additional challenges, information, or hints for future directions.\n\nhttps:\/\/www.sv-europe.com\/crisp-dm-methodology\/","7e0923c9":"<a id = \"19\"><\/a><br>\n## 3.4 Standardization","5c2a2622":"1) ID number\n2) Diagnosis (M = malignant, B = benign)\n3-32)\n\nTen real-valued features are computed for each cell nucleus:\n\na) radius (mean of distances from center to points on the perimeter)\nb) texture (standard deviation of gray-scale values)\nc) perimeter\nd) area\ne) smoothness (local variation in radius lengths)\nf) compactness (perimeter^2 \/ area - 1.0)\ng) concavity (severity of concave portions of the contour)\nh) concave points (number of concave portions of the contour)\ni) symmetry\nj) fractal dimension (\"coastline approximation\" - 1)\n\nThe mean, standard error and \"worst\" or largest (mean of the three\nlargest values) of these features were computed for each image,\nresulting in 30 features. For instance, field 3 is Mean Radius, field\n13 is Radius SE, field 23 is Worst Radius.\n\nAll feature values are recoded with four significant digits.\n\nMissing attribute values: none\n\nClass distribution: 357 benign, 212 malignant","1eff6f52":"<a id = \"9\"><\/a><br>\n## 2.3.4 EDA (Exploratory Data Analysis)","662d9ec3":"<a id = \"16\"><\/a><br>\n## 3.1 Outlier Detection (LOF)","622f90e4":"<a id = \"10\"><\/a><br>\n## 2.3.4.1 Correlation Matrix","f3087ffc":"The confusion matrix is a 2X2 table that contains 4 outputs provided by the binary classifier. Various measures, such as error-rate, accuracy, specificity, sensitivity, precision and recall are derived from it. Confusion Matrix;\n\n![image.png](attachment:image.png)\n\nA data set used for performance evaluation is called a test data set. It should contain the correct labels and predicted labels.\n\n\n\n\nThe predicted labels will exactly the same if the performance of a binary classifier is perfect.\n\n\n\nThe predicted labels usually match with part of the observed labels in real-world scenarios.\n\n\n\nA binary classifier predicts all data instances of a test data set as either positive or negative. This produces four outcomes-\n\nTrue-positive(TP) \u2014 Correct positive prediction\n\nFalse-positive(FP) \u2014 Incorrect positive prediction\n\nTrue-negative(TN) \u2014 Correct negative prediction\n\nFalse-negative(FN) \u2014 Incorrect negative prediction\n\nhttps:\/\/www.edureka.co\/blog\/interview-questions\/data-science-interview-questions\/#machine-learning","75b69a87":"**Logistic Regression & PCA Visualization**","64f32879":"<a id = \"17\"><\/a><br>\n## 3.2 Skewness & Data Transformation & PowerTransformer","b6f0772f":"<a id = \"26\"><\/a><br>\n## 4.1.4 NCA","ee124547":"**KNN & PCA Visualization**","2473e15e":"<a id = \"11\"><\/a><br>\n## 2.3.4.2 3D Visualization\n","c379e8ef":"<a id = \"25\"><\/a><br>\n## 4.1.3 PCA & KNN","6f54e795":"<a id = \"20\"><\/a><br>\n## 4-) MODELLING"}}