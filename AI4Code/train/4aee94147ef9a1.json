{"cell_type":{"c43a9559":"code","d40e5296":"code","25d25972":"code","f66ca023":"code","d61c3cda":"code","75ddd59d":"code","003e8521":"code","2e2b03eb":"code","633d4916":"code","5e8e2a1c":"code","f46ea095":"code","496290c3":"code","b00eeb33":"code","9b5d1188":"code","dd9c8d15":"code","52e009dc":"code","a978d3e0":"code","6ee0902a":"code","d1e6ba3f":"code","4bbd7dfc":"code","2270b3b3":"code","46e0e182":"code","30dfb387":"code","eb249c6f":"code","27e30335":"code","2b5552f7":"code","13936fd4":"code","2674aa43":"code","d8c8beef":"code","f41d99ca":"code","4314fef4":"code","e8d81e15":"code","259ac4a5":"code","df46370c":"code","826f2c19":"code","26eccf8c":"code","a88ff106":"code","f1d51c83":"code","1f33b873":"code","741aa557":"code","e4f9890c":"code","18b1a0e8":"code","32730f55":"code","e1ef25e2":"code","8a2d2f5b":"code","a662440f":"code","93bb198c":"code","1c31aad3":"code","7333e621":"markdown","e41ca827":"markdown","ea0a00ec":"markdown","c3939a2d":"markdown","61a4bf71":"markdown","fcd08dcc":"markdown","7f7001e6":"markdown"},"source":{"c43a9559":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d40e5296":"import pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nen = OneHotEncoder(drop = 'first')\n","25d25972":"df_train = pd.read_csv(\"..\/input\/titanic\/train.csv\")","f66ca023":"\ndf_train[\"Fam_size\"] = df_train['SibSp'] + df_train[\"Parch\"] + 1\ndf_train = df_train.drop(columns=[\"Ticket\", \"Embarked\", \"PassengerId\", \"Fare\"])","d61c3cda":"df_test = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\nb = df_test[\"PassengerId\"]\n\ndf_test[\"Fam_size\"] = df_test['SibSp'] + df_test[\"Parch\"] + 1\n\ndf_test = df_test.drop(columns=[\"Ticket\", \"Embarked\", \"PassengerId\", \"Fare\"])","75ddd59d":"df_test['Title'] = df_test.Name.str.extract('([A-Za-z]+)\\.', expand = False)\ndf_train['Title'] = df_train.Name.str.extract('([A-Za-z]+)\\.', expand = False)","003e8521":"rare = [  'Rev', 'Dr', 'Mme', 'Ms',\n       'Major', 'Lady', \"Don\", 'Sir', 'Mlle', 'Col', 'Capt', 'Countess','Dona',\n       'Jonkheer']\n\ndf_test.Title = df_test.Title.replace(rare, 'U')\ndf_train.Title = df_train.Title.replace(rare, 'U')","2e2b03eb":"df_test = df_test.drop(columns=[\"Name\"])\ndf_train = df_train.drop(columns=[\"Name\"])\ndf_test.head()\n","633d4916":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\nfor i, obj in df_test.iterrows():\n    if pd.isnull(obj['Title']):\n        if obj['Sex'] == 'female':\n            obj['Title'] = 'Miss'\n        else:\n            obj['Title'] = 'Mr'\n            \nfor i, obj in df_train.iterrows():\n    if pd.isnull(obj['Title']):\n        if obj['Sex'] == 'female':\n            obj['Title'] = \"Miss\"\n        else:\n            obj['Title'] = 'Mr'\n\n\n\n\ndf_train['Title'] = df_train['Title'].map(title_mapping)\ndf_test['Title'] = df_test['Title'].map(title_mapping)","5e8e2a1c":"def cabin_letter(cabin):\n    if type(cabin) == str:\n        return cabin[0]\n    else: return \"U\"\n","f46ea095":"# cleaning the cabin column\ndf_train[\"Cabin\"] = df_train[\"Cabin\"].apply(lambda x: cabin_letter(x))\ndf_test[\"Cabin\"] = df_test[\"Cabin\"].apply(lambda x: cabin_letter(x))","496290c3":"#categorical encoding\ndf_train[\"Sex\"] = df_train[\"Sex\"].astype('category')\ndf_train[\"Sex\"] = df_train[\"Sex\"].cat.codes\n\ndf_train[\"Cabin\"] = df_train[\"Cabin\"].astype('category')\ndf_train[\"Cabin\"] = df_train[\"Cabin\"].cat.codes\n\ndf_test[\"Sex\"] = df_test[\"Sex\"].astype('category')\ndf_test[\"Sex\"] = df_test[\"Sex\"].cat.codes\n\ndf_test[\"Cabin\"] = df_test[\"Cabin\"].astype('category')\ndf_test[\"Cabin\"] = df_test[\"Cabin\"].cat.codes","b00eeb33":"\ndf_test[\"Age\"] = df_test[\"Age\"].fillna(df_test[\"Age\"].median())\ndf_train[\"Age\"] = df_train[\"Age\"].fillna(df_train[\"Age\"].median()) ","9b5d1188":"df_train.head()","dd9c8d15":"min_max_scaler = preprocessing.MinMaxScaler()\nX_train = min_max_scaler.fit_transform(df_train)\ndf_train.loc[:,:] = X_train\nX_test = min_max_scaler.fit_transform(df_test)\ndf_test.loc[:,:] = X_test\n\n\ndf_train.head()","52e009dc":"training_set = df_train.sample(frac = 0.8) # splitting into train and validation\n\ntest_set = df_train.drop(training_set.index) \ntest_y = test_set[\"Survived\"]","a978d3e0":"_train_y = training_set[\"Survived\"]\n\ntrain_y = df_train[\"Survived\"]\ndf_train = df_train.drop(columns=[\"Survived\"])\ntraining_set = training_set.drop(columns=[\"Survived\"])\ntest_set = test_set.drop(columns=[\"Survived\"])\n","6ee0902a":"#last minute cleaning\ntraining_set = training_set.fillna(0)\ntest_set = test_set.fillna(0)","d1e6ba3f":"rf_clf = RandomForestClassifier(n_estimators=130, random_state=0, max_depth=4)","4bbd7dfc":"rf_clf.fit(training_set, _train_y)\npredicted_survival = rf_clf.predict(test_set)\n","2270b3b3":"from sklearn.metrics import classification_report\nprint(classification_report(test_y, predicted_survival))","46e0e182":"pl = pd.plotting.scatter_matrix(training_set, figsize  = [10, 10],\n    marker   = \".\",\n    diagonal = \"kde\")\n","30dfb387":"def rf_feat_imp(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)","eb249c6f":"fi = rf_feat_imp(rf_clf, training_set)","27e30335":"def plot_fi(fi):\n    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\nplot_fi(fi)","2b5552f7":"training_set = training_set.drop(['Parch'], axis=1)\ntest_set = test_set.drop(['Parch'], axis=1)\n","13936fd4":"rf_clf.fit(training_set, _train_y)\npredicted_survival = rf_clf.predict(test_set)","2674aa43":"print(classification_report(test_y, predicted_survival))","d8c8beef":"training_set = training_set.drop(['SibSp'], axis=1)\ntest_set = test_set.drop(['SibSp'], axis=1)","f41d99ca":"rf_clf.fit(training_set, _train_y)\npredicted_survival = rf_clf.predict(test_set)","4314fef4":"print(classification_report(test_y, predicted_survival))","e8d81e15":"from fastai.tabular.all import *\n\ndf_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndf_train.head()\n\ndf_test = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ndf_test.head()\n\nb = df_test[\"PassengerId\"]\ndf_test.drop(['PassengerId', 'Ticket'], axis=1)","259ac4a5":"# feature engineering again\ndf_train[\"Cabin\"] = df_train[\"Cabin\"].apply(lambda x: cabin_letter(x))\ndf_test[\"Cabin\"] = df_test[\"Cabin\"].apply(lambda x: cabin_letter(x))\ndf_test['Fare'] = df_test['Fare'].fillna(df_test['Fare'].median())\ndf_train['Fare'] = df_train['Fare'].fillna(df_train['Fare'].median())\ndf_train[\"Fam_size\"] = df_train['SibSp'] + df_train[\"Parch\"] + 1\ndf_train = df_train.drop(columns=[\"Ticket\", \"PassengerId\", \"Fare\"])","df46370c":"\n\nb = df_test[\"PassengerId\"]\n\ndf_test[\"Fam_size\"] = df_test['SibSp'] + df_test[\"Parch\"] + 1\n\ndf_test = df_test.drop(columns=[\"Ticket\", \"PassengerId\", \"Fare\"])","826f2c19":"df_test['Title'] = df_test.Name.str.extract('([A-Za-z]+)\\.', expand = False)\ndf_train['Title'] = df_train.Name.str.extract('([A-Za-z]+)\\.', expand = False)","26eccf8c":"uncommon = [  'Rev', 'Dr', 'Mme', 'Ms',\n       'Major', 'Lady', \"Don\", 'Sir', 'Mlle', 'Col', 'Capt', 'Countess','Dona',\n       'Jonkheer']\n\ndf_test.Title = df_test.Title.replace(uncommon, 'U')\ndf_train.Title = df_train.Title.replace(uncommon, 'U')","a88ff106":"df_test = df_test.drop(columns=[\"Name\"])\ndf_train = df_train.drop(columns=[\"Name\"])","f1d51c83":"df_train[\"Age\"] = df_train[\"Age\"].fillna(df_train['Age'].mean())\ndf_test[\"Age\"] = df_test[\"Age\"].fillna(df_test['Age'].mean())","1f33b873":"for i, obj in df_test.iterrows():\n    if pd.isnull(obj['Title']):\n        if obj['Sex'] == 'female':\n            obj['Title'] = 'Miss'\n        else:\n            obj['Title'] = 'Mr'\n            \nfor i, obj in df_train.iterrows():\n    if pd.isnull(obj['Title']):\n        if obj['Sex'] == 'female':\n            obj['Title'] = \"Miss\"\n        else:\n            obj['Title'] = 'Mr'","741aa557":"df_train = df_train.drop(columns=['SibSp', 'Parch', 'Embarked'])\ndf_test = df_test.drop(columns=['SibSp', 'Parch', 'Embarked'])","e4f9890c":"\ndls = TabularDataLoaders.from_df(df_train, y_names=\"Survived\", y_block=CategoryBlock, \n    cat_names = ['Cabin', 'Title', 'Sex'],\n    cont_names = ['Age', 'Pclass', 'Fam_size'],\n    procs = [Categorify, FillMissing, Normalize])","18b1a0e8":"learn = tabular_learner(dls, metrics=accuracy, layers=[32,64,128,256,128,64,32,16,8])\n\nlearn.lr_find()","32730f55":"learn.fit_one_cycle(10, 2e-2, cbs=EarlyStoppingCallback(monitor='accuracy', min_delta=0.05, patience=3))","e1ef25e2":"predictions = []\nfor i, obj in df_test.iterrows():\n    predictions.append(learn.predict(obj)[1])","8a2d2f5b":"pred = pd.Series(predictions).astype(int)","a662440f":"data = {\"PassengerId\": b, \n        \"Survived\": pred}\npredictions = pd.concat(data, \n               axis = 1)","93bb198c":"predictions = predictions.astype(int)\npredictions.head()","1c31aad3":"predictions.to_csv(\"tit_preds_nn.csv\", index=False)","7333e621":"Again, the feature was basically redundant.\n\nLets see if we can get better results using deep learning with fastai","e41ca827":"plotting the feature importance","ea0a00ec":"Feature engineering:\n\n\nthanks to this user for some feature engineering ideas: https:\/\/www.kaggle.com\/theblackmamba31\/titanic-using-neural-network","c3939a2d":"Cool. Similar results. Now lets remove SibSp","61a4bf71":"Lets drop the Parch feature and look at the results. The feature importance graph and the scatter matrix indicate that Parch is both unimportant and correlated with fam_size","fcd08dcc":"Interestingly, the fastai neural net got better results against the actual test set for this competition. Which suggests that the random forest may have been overfitting.","7f7001e6":"Normalizing the data"}}