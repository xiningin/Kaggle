{"cell_type":{"03d024a1":"code","9038ea61":"code","c4b0bf29":"code","6faed8d4":"code","e5262332":"code","0d087f61":"code","74225c64":"code","2efeb208":"code","e456b757":"code","d9657a66":"code","9fee654d":"code","18035e4d":"code","f21d982b":"code","1257bd99":"code","17939d1b":"code","b7f9e612":"code","a7dc8d16":"code","c0eddd4a":"code","dfa93faa":"code","40eb4b19":"code","bfa9dd3a":"code","449dbcb2":"code","01523e69":"code","3eccc360":"code","4083c615":"code","3de2a262":"code","fe9b6075":"code","ca341cd0":"code","53f3a6e5":"code","b068aad3":"code","9a78d8c1":"code","3a728c1b":"code","56393b1f":"code","99a29b55":"code","9f20f477":"code","ae23a63c":"code","52207d9a":"code","45ee9978":"code","7a063781":"code","3506ac55":"code","25c6b1dd":"code","bc3d855c":"code","c4fe40ec":"code","ac62c9cf":"code","c94f6b0a":"code","1d856af0":"code","26d5ac72":"code","c06890e6":"code","265147b0":"code","308ef44b":"code","e58eebe8":"code","d62aa909":"code","ce131b1a":"code","aeb3a1ae":"code","24927df3":"code","64a75b7e":"markdown","f588593e":"markdown","fbd3b99f":"markdown","0bba5180":"markdown","2e584503":"markdown","eecb5898":"markdown","2f87ba44":"markdown","d2be2055":"markdown","45d138a0":"markdown","478aab83":"markdown"},"source":{"03d024a1":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt # library for visualization\nimport seaborn as sns\nsns.set() # this command sets the seaborn chart style as the default\nfrom matplotlib.ticker import PercentFormatter #converts values into percentage format\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom collections import Counter\nimport plotly.graph_objects as go\nimport plotly.express as xp\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.manifold import TSNE\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, StackingClassifier, ExtraTreesClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.metrics import accuracy_score\n\n\npd.options.display.max_columns = None #sometimes you might not be able to see all your columns\n# use this to overcome this \n#none means NO LIMIT, so you can see everything\npd.options.display.max_rows = None\n\ndf_train= pd.read_csv(\"..\/input\/mymusicalprefrences\/train.csv\")\ndf_test=pd.read_csv(\"..\/input\/mymusicalprefrences\/test.csv\")\ndf_sample_submission=pd.read_csv(\"..\/input\/mymusicalprefrences\/sample_submition.csv\")\n\n#df_all = df_train.append(df_test)\ndf_all=pd.concat([df_train,df_test], axis = 0, ignore_index = True)\ndf_all.describe()\n\ndef split_to_onehot(df, col):\n    \"\"\"\n    This method converts features separated by '|' into one-hot vectors.\n    Additionally it drops unnecessary values, which are only present in \n    test set \/ train set or have only one value.\n    \"\"\"\n    # Getting all unique genre values.\n    unique = []\n    for i in df.index:\n        #print(i,df.loc[i,col])\n        unique.extend(df.loc[i,col].split(\"|\"))\n    if \"\" in unique:\n        unique.remove(\"\")\n    unique = list(set(unique))\n    \n    # Putting values into binary form \n    onehot = df.loc[:,[\"Category\"]]\n    onehot[unique] = np.zeros((len(unique),), dtype = np.int8)\n    for i in df.index:\n        \n        g = set(df.loc[i,col].split(\"|\"))\n        for j in g:\n            if j!=\"\":\n                onehot.loc[i,j] = 1\n                \n    # Dropping unnecessary values            \n    _a = onehot.groupby(\"Category\").sum()\n    only_one = list(_a.sum()[_a.sum()==1].index)\n    only_train = list(_a.loc[\"none\"][_a.loc[\"none\"]==0].index)\n    only_test = list(_a.loc[[\"like\",'dislike']].sum()[_a.loc[[\"like\",'dislike']].sum()==0].index)\n    _a = set(only_one + only_train + only_test)\n    onehot = onehot.drop(_a, axis=1)\n    \n    return onehot\n\ndef onehot_to_tsne2(df, title):\n    \"\"\"\n    This method converts one-hot representation into two tsne values.\n    Such operation is needed to shrink the dimensionality of the dataset.\n    \"\"\"\n    onehot = df.drop(\"Category\",axis=1)\n    \n    embedding = TSNE(n_components=2, init=\"pca\")\n    embedded = embedding.fit_transform(onehot)\n    embedded = pd.DataFrame(embedded,columns=[f\"{title}_tsne1\",f\"{title}_tsne2\"])\n    \n    return embedded\n\ndef plot_cumulative_onehot(onehot):\n    \"\"\"\n    Method of plotting commulative values of the one hot feature representation.\n    \"\"\"\n    _df = onehot.groupby(\"Category\").sum()\n    fig = go.Figure()\n    for i in range(len(_df.index)):\n        k = _df.index[i]\n        x,y=[],[]\n        for g in _df.columns:\n            if _df.loc[k,g]!=0:\n                x.append(g)\n                y.append(_df.loc[k,g])\n        fig.add_trace(go.Bar(x=x, y=y,name=k))\n    fig.show()\n","9038ea61":"df_all.info()","c4b0bf29":"display(df_all.head(100))","6faed8d4":"df_new = df_all.drop('Id',axis=1)\ntrain_mask = ~df_new.Category.isna()\ndf_new[\"Category\"] = df_new[\"Category\"].fillna(\"none\").replace({0:\"dislike\",1:\"like\"})\n\n\nnumerical = []\ncategorical = []\nfor col in df_new.columns:\n    col_dtype = df_new.dtypes[col]\n    if col_dtype == 'float64' or col_dtype == 'int64':\n        numerical.append(col)\n    else:\n        categorical.append(col)\n        \nprint('Numerical:',numerical)\n\nfor category in categorical:\n    print(category,':',len(set(df_new[category])))\n","e5262332":"df_new.loc[df_new['Artists'].isna()==True]\ndf_new = df_new.drop([661])","0d087f61":"display(df_new)","74225c64":"# How many artists are there\nall_artists = []\nfor i in df_new.index:\n    all_artists.extend(df_new.loc[i, \"Artists\"].split(\"|\"))\n","2efeb208":"# We will put some threshold, not to put some rare artists into one-hot vector.\nthreshold = 3\nrare_artists = Counter(all_artists)\nrare_artists = [k for k in rare_artists if rare_artists[k]<=threshold]\n","e456b757":"# Drop all artists who are only in the test-set or the train-set\nin_train, in_test = [], []\nfor i in df_new.loc[train_mask].index:\n    in_train.extend(df_new.loc[i, \"Artists\"].split(\"|\"))\n\n\nfor i in df_new.loc[~train_mask].index:\n    print(i)\n    in_test.extend(df_new.loc[i, \"Artists\"].split(\"|\"))\n\n    \nonly_test = set(in_test) - set(in_train)\nonly_train = set(in_train) - set(in_test)\ndisplay(len(only_test))\ndisplay(len(only_train))","d9657a66":"all_artists = list(set(all_artists) - set(rare_artists) - only_test - only_train)\n\nrare_artists = set(rare_artists) | only_test | only_train\n","9fee654d":"# Create onehot vector for artists\nresult = []\ndef prune(x):\n    vector = np.zeros(len(all_artists)+1) # for rare artists\n    x = [i for i in x.split(\"|\")]\n    for i in range(len(all_artists)):\n        vector[i]=1 if all_artists[i] in x else 0\n    if len(x)>sum(vector):\n        vector[-1]=1\n    result.append(vector)\n\ndf_new[\"Artists\"].apply(prune)\n\n\nonehot_artists = pd.DataFrame(result, columns = all_artists + [\"Others\"], index=df_new.index)\n\nonehot_artists","18035e4d":"# We drop the rare artists (Others) column, it's not really relevant.\nonehot_artists = onehot_artists.drop(\"Others\", axis=1)\n\n# Let's plot the artists\nonehot_artists[\"Category\"] = df_new[\"Category\"]\n","f21d982b":"display(df_new)","1257bd99":"df_new.info()","17939d1b":"# Since there are too many features in the onehot vector, we will apply tsne (dimensionality reduction)\nartists_embedded = onehot_to_tsne2(onehot_artists, \"Artists\")\n\nartists_embedded=artists_embedded.set_index(df_new.index)\ndisplay(artists_embedded)","b7f9e612":"#df_new = df_new.drop([661])\ndf_new = pd.concat([df_new,artists_embedded[[\"Artists_tsne1\",\"Artists_tsne2\"]]], axis=1)","a7dc8d16":"display(df_new)\n","c0eddd4a":"df_new.info()","dfa93faa":"artists_embedded[[\"Artists_tsne1\",\"Artists_tsne2\"]].info()","40eb4b19":"import re \n\n#Check if either Artist or Track Name are in cyrillic. \n#order as follows:\n#both unknown -  unknown\n#one or both in cyrillic - cyrillic\n#both in latin - latin\n#one unknown, one cyrillc - cyrillic\n\ndef has_cyrillic(text):\n    return bool(re.search('[\u0430-\u044f\u0410-\u042f]', text))\n\nfor i in df_new.index:\n    artist = df_new.at[i,'Artists']\n    track = df_new.at[i,'Track']\n    combined = str(artist)*(not pd.isna(artist)) +str(track)*(not pd.isna(track))\n    \n    if len(combined) ==0:\n        df_new.at[i,'Artists'] = 0\n        df_new.at[i,'Track'] = 0\n    \n    else: \n        df_new.at[i,'Artists']  = int(has_cyrillic(combined)) +1\n        df_new.at[i,'Track']  = int(has_cyrillic(combined)) +1\n       \n\n","bfa9dd3a":"df_new = pd.get_dummies(df_new, prefix=['Artists'], columns=['Artists'])\ndf_new = df_new.rename(columns={'Artists_1': 'Latin','Artists_2':'Cyrillic'})\ndf_new=df_new.drop('Track',axis=1)\n\ndisplay(df_new)","449dbcb2":"df_new[['Version']] = df_new[['Version']].fillna('Version Unknown')\nversions = set(df_new['Version'])\ndf_new = pd.get_dummies(df_new, prefix=[''],prefix_sep='', columns=['Version'])\n\n\n","01523e69":"display(df_new.head(20))","3eccc360":"set(df_new['Artists_Genres'])\n","4083c615":"df_new.loc[df_new['Artists_Genres'].isna()==True]\n#df_new =df_new.drop([661])\n","3de2a262":"display(df_new)","fe9b6075":"set([type(d) for d in df_new['Artists_Genres']])","ca341cd0":"df_new.columns","53f3a6e5":"\n# To onehot vector\ngenres_onehot = split_to_onehot(df_new, \"Artists_Genres\")\n#plot_cumulative_onehot(genres_onehot)\n# We have too much values, so we reduce the dimensionality\ngenres_embedded = onehot_to_tsne2(genres_onehot, \"Genres\")\ngenres_embedded=genres_embedded.set_index(df_new.index)\n# Replace the old column\ndf_new = pd.concat([df_new,genres_embedded], axis=1)\ndf_new = df_new.drop(\"Artists_Genres\", axis=1)\ndisplay(df_new)","b068aad3":"df_new[['Album']] = df_new[['Album']].fillna('none')\n# To onehot vector\nalbum_onehot = split_to_onehot(df_new, \"Album\")\nplot_cumulative_onehot(album_onehot)\n\n# Again, too much values, so reduce to 2 tsne values\nalbum_embedded = onehot_to_tsne2(onehot_artists, \"Album\")\nalbum_embedded=album_embedded.set_index(df_new.index)\n_df = album_embedded.copy(deep=True)\n_df[[\"Category\",\"Album\"]] = df_new[[\"Category\",\"Album\"]]\n\n# Scatterplot\nxp.scatter(_df,x=\"Album_tsne1\",y=\"Album_tsne2\",color=\"Category\", hover_data=[\"Album\"], height=500)\n\n# Replace column\ndf_new = pd.concat([df_new,album_embedded[[\"Album_tsne1\",\"Album_tsne2\"]]], axis=1)\ndf_new = df_new.drop(\"Album\", axis=1)\ndf_new.head()","9a78d8c1":"# album_scores = {}\n# for i in df_train.index:\n#     album = df_train.at[i,'Album']\n    \n#     if not (album in album_scores.keys()):\n#         album_scores[album] = 0\n#     like =df_train.at[i,'Category'] #=1 if liked\n#     add = (like*1)-((1-like)*1)\n#     album_scores[album] = album_scores[album] + add\n\n# for j in df_new.index:\n#     album = df_new.at[j,'Album']\n#     if not (album in album_scores.keys()):\n#         df_new.at[j,'Album'] = float(0)\n#     else:\n#         df_new.at[j,'Album'] = float(album_scores[album])\n    \n# df_new = df_new.rename(columns={'Album': 'Album Favorability'})","3a728c1b":"# label_scores = {}\n# for i in df_train.index:\n#     label = df_train.at[i,'Labels']\n    \n#     if not (label in label_scores.keys()):\n#         label_scores[label] = 0\n#     like =df_train.at[i,'Category'] #=1 if liked\n#     add = (like*1)-((1-like)*1)\n#     label_scores[label] = label_scores[label] + add\n\n# for j in df_new.index:\n#     label = df_new.at[j,'Labels']\n#     if not (label in label_scores.keys()):\n#         df_new.at[j,'Labels'] = 0\n#     else:\n#         df_new.at[j,'Labels'] = label_scores[label]\n    \n# df_new = df_new.rename(columns={'Labels': 'Label Favorability'})","56393b1f":"df_new[\"Labels\"].replace(np.nan, \"none\", inplace=True)\nlabels_onehot = split_to_onehot(df_new, \"Labels\")\nplot_cumulative_onehot(labels_onehot)\n# Use tsne function to reduce dimensionality\nlabels_embedded = onehot_to_tsne2(labels_onehot, \"Labels\")\nlabels_embedded=labels_embedded.set_index(df_new.index)\n_df = labels_embedded.copy(deep=True)\n_df[[\"Category\",\"Labels\"]] = df_new[[\"Category\",\"Labels\"]]\n\n# Create scatterplot\nxp.scatter(_df,x=\"Labels_tsne1\",y=\"Labels_tsne2\",color=\"Category\", hover_data=[\"Labels\"], height=500)\n\n# Replace old column\ndf_new = pd.concat([df_new,labels_embedded[[\"Labels_tsne1\",\"Labels_tsne2\"]]], axis=1)\ndf_new = df_new.drop(\"Labels\", axis=1)","99a29b55":"df_new.head(20)","9f20f477":"display(df_new)\n","ae23a63c":"# We correct some keys and replace them with the same value (C# = D\u266d, etc)\ndf_new[\"Major\"], df_new[\"Key\"] = df_new[\"Key\"].apply(lambda x: x.split(\" \")[1]), df_new[\"Key\"].apply(lambda x: x.split(\" \")[0])\ndf_new.loc[:,\"Key\"] = df_new[\"Key\"].replace({\"D\u266d\": \"C#\", \"E\u266d\": \"D#\", \"G\u266d\": \"F#\", \"A\u266d\": \"G#\",\"B\u266d\":\"A#\"})\n\n# We put the Major\/Minor part into new feature, to make it more easy for our model in the fitting process\ndf_new.loc[:,\"Major\"] = (df_new[\"Major\"]==\"Major\").astype(int)\n\n# Replace the old column with our onehot vector\ndf_new[list(set(df_new[\"Key\"].values))] = OneHotEncoder().fit_transform(df_new[[\"Key\"]]).toarray()\ndf_new = df_new.drop(\"Key\", axis=1)","52207d9a":"# key_scores = {}\n# for i in df_train.index:\n#     key = df_train.at[i,'Key']\n    \n#     if not (key in key_scores.keys()):\n#         key_scores[key] = 0\n#     like =df_train.at[i,'Category'] #=1 if liked\n#     add = (like*1)-((1-like)*1)\n#     key_scores[key] = key_scores[key] + add\n\n# for j in df_new.index:\n#     key = df_new.at[j,'Key']\n#     if not (key in key_scores.keys()):\n#         df_new.at[j,'Key'] = 0\n#     else:\n#         df_new.at[j,'Key'] = key_scores[key]\n    \n# df_new = df_new.rename(columns={'Key': 'Key Favorability'})","45ee9978":"df_new.head(20)","7a063781":"df_new[['Album_type']] = df_new[['Album_type']].fillna('Unknown type')\ndf_new = pd.get_dummies(df_new, prefix=[''],prefix_sep='', columns=['Album_type'])","3506ac55":"df_new = df_new.rename(columns={'Vocal ': 'Vocal','Dancebility':'Danceability'})\ndf_new = pd.get_dummies(df_new, prefix=[''],prefix_sep='', columns=['Vocal'])","25c6b1dd":"df_new.head(20)","bc3d855c":"df_new['Country'].value_counts(dropna=False)\n","c4fe40ec":"# mycountries = ['USA','GB','RUS','UA','SWE','KZ','AU','FIN','DK','DE','Unknown Country','other']\n# countryrows=[]\n\n# for index in df_new.index:\n#     countryrow = [0 for country in mycountries]\n#     given_country = df_new['Country'][index]\n#     other=1\n    \n#     if pd.isna(given_country):\n#         countryrow[-2]=1\n#         other=0\n    \n#     else:\n#         for j in range(len(mycountries))[:-2]:\n#             mycountry = mycountries[j]\n#             if mycountry in given_country:\n#                 countryrow[j]=1\n#                 other=0\n    \n#     if other ==1:\n#         countryrow[-1]=1\n#     countryrows.append(countryrow.copy())\n\n    \n    \n# df_countries = pd.DataFrame(countryrows, columns=mycountries,index=df_new.index)\n\n# df_new = df_new.join(df_countries)\n# df_new = df_new.drop('Country',axis=1)\n# display(df_new.head(20))","ac62c9cf":"df_new[['Country']] = df_new[['Country']].fillna('none')\n#display(df_new['Country'])\n# We split to onehot vector and plot the countries\ncountry_onehot = split_to_onehot(df_new, \"Country\")\nplot_cumulative_onehot(country_onehot)\n\n# We drop the old column and replace it by the new onehot vector\ncountry_onehot = country_onehot.drop(\"Category\", axis=1)\ndf_new = pd.concat([df_new,country_onehot], axis=1)\ndf_new = df_new.drop([\"Country\", \"none\"], axis=1)\ndf_new.head()","c94f6b0a":"df_new.columns","1d856af0":"display(df_new)","26d5ac72":"df_new=df_new[df_new['Energy'].notna()]\n\ndf_new[\"Category\"] = df_new[\"Category\"].replace({\"dislike\":0,\"like\":1,\"none\":np.nan})\ndf_new = df_new.apply(pd.to_numeric)\n\nscaler = MinMaxScaler()\n\nscaler.fit(df_new)\ndf_new_scaled = pd.DataFrame(scaler.fit_transform(df_new), columns=df_new.columns, index =df_new.index)\n\ndf_new_train = df_new_scaled[df_new_scaled['Category'].notna()]\nX = df_new_train.drop('Category',axis=1)\ny= df_new_train['Category']\n\ndf_new_test = df_new_scaled[df_new_scaled['Category'].isna()]\ny_real_test = df_new_test.drop('Category',axis=1)\n\nplotlist = ['Category','Duration', 'Album Favorability', 'Release_year',\n       'Label Favorability', 'Key Favorability', 'BPM', 'Energy',\n       'Danceability', 'Happiness', 'Cyrillic', 'Major']\n#sns.pairplot(df_new_train[plotlist])","c06890e6":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\nimport numpy as np\n\n\nLRgrid = {'C': np.linspace(0.0001,10,100),'solver': ['newton-cg', 'lbfgs','sag']}\nLRdict = {\"clf\":LogisticRegression(), \"param_grid\":LRgrid}\n\nSVCgrid = {'C': np.linspace(0.0001,10,100),'kernel': ['linear', 'poly', 'rbf','sigmoid'],'gamma':['scale','auto']}\nSVCdict= {\"clf\":SVC(), \"param_grid\":SVCgrid}\n\nDTgrid = {'min_samples_split': np.arange(20)+2,'splitter': ['best', 'random'],'min_impurity_decrease': np.linspace(0.0001,10,100)}\nDTdict = {\"clf\":DecisionTreeClassifier(), \"param_grid\": DTgrid}\n\nRFgrid = {'n_estimators':[10+10*i for i  in range(5)],'min_samples_split':np.arange(20)+2}#,'min_impurity_decrease': np.linspace(0.0001,10,100)}\nRFDict = {\"clf\":RandomForestClassifier(), \"param_grid\":RFgrid}\n\nKNgrid = {'n_neighbors':np.arange(15)+5, 'weights':['uniform','distance']}\nKNDict = {\"clf\": KNeighborsClassifier(),\"param_grid\":KNgrid}\n\nclassifierDict ={\"SVC\":SVCdict,\"DecisionTree\":DTdict}\nclassifierDict2={\"RandomForest\":RFDict,\"Kneighbours\":KNDict}\n\nX, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ndef test_classifiers(classifierDict):\n    for clf_name, clf_dict in classifierDict.items():        \n        print(\"\\n\",clf_name+':',\"\\n\")\n        clf = clf_dict[\"clf\"]\n        param_grid = clf_dict[\"param_grid\"]\n\n        grid = GridSearchCV(clf, param_grid, cv=10)\n        grid.fit(X, y)\n        print(grid.best_params_)\n\n        model = grid.best_estimator_\n        prediction_grid = model.fit(X, y).predict(X_test)\n        print(metrics.accuracy_score(y_test, prediction_grid))\n        print(metrics.classification_report(y_test, prediction_grid))\n        print(\"Not predicted:\",set(y_test)-set(prediction_grid))\n\ntest_classifiers(classifierDict2)","265147b0":"test_classifiers(classifierDict)","308ef44b":"from sklearn.ensemble import AdaBoostClassifier\n\nABgrid = {'n_estimators':[10+100*i for i  in range(5)]}\nABDict = {\"clf\":AdaBoostClassifier(), \"param_grid\":ABgrid}\nclassifierDict3={\"AdaBoost\":ABDict}\n\ntest_classifiers(classifierDict3)","e58eebe8":"from sklearn.ensemble import AdaBoostClassifier\nRFgrid2 = {'n_estimators':[10+3000*i for i  in range(2)],'max_depth' : [20]}#,'min_impurity_decrease': np.linspace(0.0001,10,100)}\nRFDict2 = {\"clf\":RandomForestClassifier(), \"param_grid\":RFgrid2}\n\nclassifierDict3={\"Random Forest 2\":RFDict2}\n\ntest_classifiers(classifierDict3)","d62aa909":"y_real_test","ce131b1a":"best_RF_old ={'min_samples_split': 7, 'n_estimators': 40}\nbest_RF_new={'min_samples_split': 17, 'n_estimators': 30}\nbest_RF_new_refined={'min_samples_split': 14, 'n_estimators': 310}\nbest_SVC_old = {'C': 1.5152363636363637, 'gamma': 'scale', 'kernel': 'linear'} #####\nbest_SVC ={'C': 3.0303727272727277, 'gamma': 'scale', 'kernel': 'linear'}\nbest_DT = {'min_impurity_decrease': 0.10110909090909091, 'min_samples_split': 2, 'splitter': 'best'}\n\n#####SVC\nclf = SVC(C= 3.0303727272727277, gamma= 'scale', kernel= 'linear')\nclf.fit(X,y)\npred_Svc_new =clf.predict(y_real_test)\n\nclf = SVC(C= 1.5152363636363637, gamma= 'scale', kernel= 'linear')\nclf.fit(X,y)\npred_Svc_old =clf.predict(y_real_test)\nsns.heatmap([pred_Svc_old])\nplt.figure()\nsns.heatmap([pred_Svc_new])","aeb3a1ae":"#####RF\nclf2 = RandomForestClassifier(min_samples_split= 20, n_estimators= 3010,max_depth = 10, random_state = 42)\nclf2.fit(X,y)\npred_RF=clf2.predict(y_real_test)\nsns.heatmap([pred_RF])\n\nclf3 = RandomForestClassifier(min_samples_split= 18, n_estimators= 5000,max_depth = 5, random_state = 42)\nclf3.fit(X,y)\npred_RF1=clf3.predict(y_real_test)\nplt.figure()\nsns.heatmap([pred_RF1])\n\nclf2 = RandomForestClassifier(n_estimators = 1000, max_depth = 10, random_state = 42)\nclf2.fit(X,y)\npred_RF_refined=clf2.predict(y_real_test)\nplt.figure()\nsns.heatmap([pred_RF_refined])\n\nplt.figure()\nsns.heatmap([pred_Svc_new])\n\n","24927df3":"save_pred=pred_RF1.astype(int)\n\ndf_pred = pd.DataFrame(save_pred,index = y_real_test.index,columns=['Category'])\ndf_pred.index.names = ['Id']\ndisplay(df_pred)\ndf_pred.to_csv('submission.csv')","64a75b7e":"## Album","f588593e":"## Album_type: one hot encoding","fbd3b99f":"## Key: extract Major\/Minor, replace enharmonics, one hot encoding\n","0bba5180":"## Version: one hot encoding","2e584503":"## Labels","eecb5898":"## Country","2f87ba44":"## **Artists & Track Names**","d2be2055":"# Finishing touches, scaling, split again","45d138a0":"## Artists_Genres","478aab83":"Introduce new column to display if the song is major."}}