{"cell_type":{"36cef15b":"code","f40b3bab":"code","55cb0c25":"code","6394dd1e":"code","f7e1b822":"code","78d19033":"code","be51b7fe":"code","fb6cf996":"code","47d2b89d":"code","d3db584d":"code","b3154aa8":"code","203f2dc2":"code","34a74a1b":"code","8e22e814":"code","3f3315af":"code","21122646":"code","526a75e6":"code","25107481":"code","122dee8e":"code","f0ece272":"code","b388c311":"code","959dbc1f":"code","88dd2b40":"code","34724d8c":"code","38b94793":"code","6da3c0d9":"code","34cc2dc6":"code","7daa2038":"code","82121229":"code","b98a7692":"code","3a1a49dd":"code","279221e5":"code","731d712b":"code","8992a781":"markdown","661de062":"markdown","247bd82a":"markdown","8fbebc3f":"markdown","4e288d6b":"markdown","d59d4dc0":"markdown","d27d535a":"markdown","6b61123c":"markdown","2f0cd15f":"markdown","d38f7d6a":"markdown","031c29fe":"markdown"},"source":{"36cef15b":"!pip install ktrain","f40b3bab":"!pip install contractions","55cb0c25":"import pandas as pd\nimport numpy as np\nimport sys  \nimport re\nimport string\nimport contractions\nfrom sklearn.model_selection import train_test_split\nimport ktrain\nimport tensorflow as tf\nfrom ktrain import text","6394dd1e":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f7e1b822":"df_train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ndf_train","78d19033":"df_train.dtypes","be51b7fe":"df_val = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ndf_val","fb6cf996":"df_train['target'].value_counts(normalize=True)","47d2b89d":"sum(df_train.keyword.isna())","d3db584d":"sum(df_train.location.isna())","b3154aa8":"df_train.drop(columns=['keyword', 'location' ,'id'], inplace=True)\ndf_train","203f2dc2":"def pre_process(tweet):\n    tweet = ' '.join(re.sub(\"(@[A-Za-z0-9_]+)|(#[A-Za-z0-9]+)\", \" \", tweet).split())  # remove #tags and @usernames\n    tweet = ' '.join(re.sub(\"(\\w+:\\\/\\\/\\S+)\", \" \", tweet).split()) # remove urls\n    return(tweet)","34a74a1b":"def pre_process1(tweet):\n    tweet = ' '.join(re.sub(\"(\\w+:\\\/\\\/\\S+)\", \" \", tweet).split()) # remove urls\n    return(tweet)","8e22e814":"def fn_contractions(tweet):\n    expanded_words = []\n    for word in tweet.split():\n        expanded_words.append(contractions.fix(word))\n    return(' '.join(expanded_words))","3f3315af":"df_train['text'] = df_train['text'].apply(lambda x:pre_process(x))\ndf_train","21122646":"df_train['text'] = df_train['text'].apply(lambda x:fn_contractions(x))\ndf_train","526a75e6":"df_val['text'] = df_val['text'].apply(lambda x:pre_process(x))\ndf_val['text'] = df_val['text'].apply(lambda x:fn_contractions(x))\ndf_val","25107481":"train, test = train_test_split(df_train, test_size=0.2)\nX_train = train.text.tolist()\nX_test = test.text.tolist()\ny_train = train.target.tolist()\ny_test = test.target.tolist()","122dee8e":"X_train","f0ece272":"y_train","b388c311":"print(len(X_train),len(X_test),len(y_train),len(y_test))","959dbc1f":"model_arch ='bert-base-uncased'\nfactors = [0,1] # We have two factors to predict.\nMAXLEN = 512\ntrans = text.Transformer(model_arch, maxlen=MAXLEN, class_names= factors)","88dd2b40":"train_data = trans.preprocess_train(X_train,y_train)\ntest_data = trans.preprocess_test(X_test,y_test)","34724d8c":"model = trans.get_classifier()","38b94793":"learner = ktrain.get_learner(model, train_data=train_data, val_data=test_data, batch_size=10)","6da3c0d9":"#learner.lr_find(show_plot=True, max_epochs=10) #finding optimal learning rate","34cc2dc6":"learner.fit_onecycle(3e-5, 4)","7daa2038":"learner.validate(val_data=test_data, class_names=factors)","82121229":"predictor = ktrain.get_predictor(learner.model, preproc=trans)","b98a7692":"df_val['target'] = predictor.predict(df_val.text.tolist())\ndf_val","3a1a49dd":"df_val.to_csv('\/kaggle\/working\/test_result_final.csv', index=False)","279221e5":"df_submission = df_val[['id','target']]","731d712b":"df_submission.to_csv('\/kaggle\/working\/submission5.csv', index=False)","8992a781":"# Model building using BERT","661de062":"We are using bert-base-uncased model. You can choose any other model. I am selecting maxlen of tokenization as 512 (it's max for BERT).","247bd82a":"# Installing Required Packages","8fbebc3f":"# **Predict which Tweets are about real disasters**\n","4e288d6b":"# Spliting Data for Test and Train","d59d4dc0":"# **Initial Text Pre-Processing**\n**We'll remove hashtags(#example), @username and links(starting with http:\/\/ or https:\/\/) only. As we are going to use BERT, we are not removing emoticons as it will help BERT in prediction. We will again do text pre-processing later using BERT.**","d27d535a":"**Handling constractions**:  Below funnction will replace constactions (e.g. wouldn't to would not).","6b61123c":"# Data Preparation","2f0cd15f":"**Droping keyword and location columns**","d38f7d6a":"# Prediction","031c29fe":"#  Importing Required Packages"}}