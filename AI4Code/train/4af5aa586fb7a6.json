{"cell_type":{"da75e033":"code","d84b0c05":"code","a651afdf":"code","323ab665":"code","98ad564e":"code","fc80ebb4":"code","6b304bed":"code","4f676eb3":"code","7a3e4dc4":"code","6e16d94d":"code","543c4d91":"code","b6ae540d":"code","46e48c63":"code","a6649726":"markdown","6cbd19b2":"markdown","26746015":"markdown","6f06a491":"markdown","cbaf53c5":"markdown","f115bae0":"markdown"},"source":{"da75e033":"pip3 install Sastrawi\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\nimport pandas as pd\nfrom Sastrawi.Stemmer.StemmerFactory import StemmerFactory\nimport joblib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport re\nimport string\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.base import TransformerMixin\n%matplotlib inline\n","d84b0c05":"df = joblib.load(\"..\/input\/twitter-data-cleaned\/data_cleaned\")\n# df = df.drop([\"Unnamed: 2\",\"Unnamed: 3\"],axis=1)","a651afdf":"df.head()","323ab665":"df.describe()","98ad564e":"len_data = [len(x.split()) for x in df['CLEAN_DATA']]\nprint(\"Mean\", np.mean(len_data))\nprint(\"Median\", np.median(len_data))\nprint(\"Std\", np.std(len_data))\nprint(\"Min\", np.min(len_data))\nprint(\"Max\", np.max(len_data))","fc80ebb4":"temp = df.groupby('SENTIMEN').count()['TWEET'].reset_index().sort_values(by='TWEET',ascending=False)\ntemp.style.background_gradient(cmap='Reds')\n","6b304bed":"fig = go.Figure(go.Funnelarea(\n    text =temp.SENTIMEN,\n    values = temp.TWEET,\n    title = {\"position\": \"top center\", \"text\": \"Funnel-Chart Sentimen Distribution\"}\n    ))\nfig.show()","4f676eb3":"factory = StemmerFactory()\nstemmer = factory.create_stemmer()\ndef preprocessing(text):\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = stemmer.stem(text)\n    return text","7a3e4dc4":"df[\"CLEAN_DATA\"] = df[\"TWEET\"].apply(lambda x: preprocessing(x))","6e16d94d":"df = joblib.load(\"..\/input\/twitter-data-cleaned\/data_cleaned\") # load existing dataset ","543c4d91":"X_train, X_test, y_train, y_test = train_test_split(df[\"CLEAN_DATA\"], df[\"SENTIMEN\"],train_size=0.8,shuffle=True)","b6ae540d":"class DenseTransformer(TransformerMixin):\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def transform(self, X, y=None, **fit_params):\n        return X.todense()\n    \npipe = Pipeline([('vc', TfidfVectorizer()), ('vb',DenseTransformer()) , ('nb', GaussianNB())])\nparameter = {\n    \"vc__ngram_range\": [(1,1),(1,2),(2,2)],\n    \"vc__max_features\": [27,None],\n    \"vc__norm\": [\"l1\",\"l2\"],\n    \"vc__use_idf\": [True,False,None],\n    \"vc__smooth_idf\" :[True,False,None],\n    \n}\n\ngrid = GridSearchCV(pipe,\n                    param_grid=parameter,\n                    verbose=10,\n                    \n                   )\nhist = grid.fit(X_train,y_train)\n","46e48c63":"hist.best_score_","a6649726":"**Split Dataset Training**\n\nTraining **80%**\nTest **20%**","6cbd19b2":"**Data Exploration**","26746015":"Data Loading","6f06a491":"**Prepare**","cbaf53c5":"Build Model Naives Bayes","f115bae0":"**Data Preprocesing**"}}