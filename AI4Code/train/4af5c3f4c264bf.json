{"cell_type":{"08720ba0":"code","f61e6da7":"code","05305537":"code","a336b565":"code","9f1cfcd8":"code","b6a69bb5":"code","61b69916":"code","5728c75b":"code","619aa80d":"code","4a8add78":"code","ed1b3fa0":"code","751e65e2":"code","c757e52e":"code","0180d152":"code","6c2a0b41":"code","a8d32800":"code","eae7b4cf":"code","82eee500":"code","9bf18447":"code","1cc38540":"code","c69f25c8":"markdown","e7ef47c9":"markdown","2b15726e":"markdown","311a31eb":"markdown","d4f384b9":"markdown","9b4970c1":"markdown","a203bedb":"markdown","815e1a66":"markdown"},"source":{"08720ba0":"# Load csv into data frame\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\ndf = pd.read_csv('..\/input\/Seed_Data.csv')\ndf.head() # first 5 rows","f61e6da7":"df.shape #(rows, columns)\n# 210 data points","05305537":"df.describe()","a336b565":"#  Are the features strongly related ? \n# To know this, take each column as dependent variable and try to predict this column\n# from other columns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\ncols = df.columns\n\nfor col in cols:\n    X = df.drop([col], axis=1)\n    y = pd.DataFrame(df.loc[:, col])\n    X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42)\n    reg = LinearRegression()\n    reg.fit(X_train,y_train)\n    score = reg.score(X_test,y_test)\n    print('Score for {} as dependent variable is {}'.format(col,score))","9f1cfcd8":"# Visualize how each feature is related to another feature\npd.scatter_matrix(df, diagonal='kde', figsize=(16,9))","b6a69bb5":"df_A = df[['A','A_Coef']]\ndf_A.head()","61b69916":"import matplotlib.pyplot as plt\ndf_A.plot('A','A_Coef',kind='scatter',figsize=(7,5))","5728c75b":"from sklearn.cluster import KMeans\n\nno_of_clusters = range(1, 10)\nkmeans = [KMeans(n_clusters=i) for i in no_of_clusters]\nscore = [kmeans[i].fit(df_A).score(df_A) for i in range(len(kmeans))]\nplt.plot(no_of_clusters,score)\nplt.xlabel('Number of Clusters')\nplt.ylabel('Score')\nplt.title('Elbow Curve')\nplt.show()","619aa80d":"kmeans = KMeans(n_clusters=3, random_state=42)\nkmeans.fit(df_A)\ncluster_labels = kmeans.predict(df_A)\nkmeans.cluster_centers_\n# 3 cluster centres","4a8add78":"kmeans.labels_ \n# 0, 1, 2","ed1b3fa0":"plt.figure(figsize=(7,5))\nplt.scatter(df_A['A'],df_A['A_Coef'],c=kmeans.labels_)\nplt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1], color='red')\nplt.title('3 Means Clustering')\nplt.xlabel('A')\nplt.ylabel('A_Coef')\nplt.show()","751e65e2":"from sklearn.cluster import AgglomerativeClustering\nhac_clustering = AgglomerativeClustering(n_clusters=3).fit(df_A)\nhac_clustering","c757e52e":"plt.figure(figsize=(7,5))\nplt.scatter(df_A['A'],df_A['A_Coef'],c=hac_clustering.labels_)\nplt.title('3 Means Clustering')\nplt.xlabel('A')\nplt.ylabel('A_Coef')\nplt.show()","0180d152":"df.plot.scatter('WK','LKG')","6c2a0b41":"df_LKG_WK = df[['LKG','WK']]\nno_of_clusters=range(1,10)\nkmeans = [KMeans(n_clusters=i) for i in no_of_clusters]\nscore = [kmeans[i].fit(df_LKG_WK).score(df_LKG_WK) for i in range(len(kmeans))]\nscore\nplt.plot(no_of_clusters, score)","a8d32800":"kmeans = KMeans(n_clusters=2,random_state=42)\nkmeans.fit(df_LKG_WK)\nkmeans.predict(df_LKG_WK)\nkmeans.cluster_centers_","eae7b4cf":"plt.scatter(df_LKG_WK.iloc[:,0],df_LKG_WK.iloc[:,1],c=kmeans.labels_)\nplt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1], color='red')\nplt.title('2 Means Clustering')\nplt.xlabel('WK')\nplt.ylabel('LKG')\nplt.show()","82eee500":"from sklearn.mixture import GaussianMixture\ngmm = GaussianMixture(n_components=2).fit(df_LKG_WK)\ngmm_labels = gmm.predict(df_LKG_WK)\nplt.scatter(df_LKG_WK.iloc[:,0],df_LKG_WK.iloc[:,1],c=gmm_labels)\nplt.title('2 Means Clustering')\nplt.xlabel('WK')\nplt.ylabel('LKG')\nplt.show()","9bf18447":"# As we know that this soft clustering, we can find the probability\n# with which each data point belongs to the two clusters\ny_pred = gmm.predict_proba(df_LKG_WK)\ny_pred[50] # Probability that that data point at row 50 belongs to cluster 1 is 0.99674903 and cluster 2 is 0.00325097","1cc38540":"y_pred[100] #cluster 1 - 0.01608574, cluster 2 - 0.98391426","c69f25c8":"Now, I am going to pick two columns whose graph gives us some cluster like distribution i.e. I am not going to choose graphs with linear distribution(P and LK). For example, A and A_Coef have a non-linear distribution. Let's work on that!","e7ef47c9":"**How to find the number of clusters we need?**\n\nI am going to use elbow method and draw a graph that can give us the rough idea on the number of clusters that best suits for our data set.","2b15726e":"**Agglomerative Clustering**","311a31eb":"**GaussianMixture(Soft Clustering) on LKG and WK**","d4f384b9":"In this kernel, I am going to build few clustering models. Clustering is used to group data points together into a group which we call as a cluster. Broadly, there are two types of clustering :\n1. Hard Clustering(each point belongs to only one cluster)\n2. Soft Clustering(each data point is associated with probabilistic values for each cluster)\n\nWe are going to use some common clustering algorithms like K-Means Clustering , Agglomerative Clustering and Gaussian Mixture.\nK-Means and Agglomerative are hard clustering methods whereas Gaussian Mixture is a soft clustering method.","9b4970c1":"**K- Means Clustering on WK and LKG**","a203bedb":"The above graph plots number of clusters against score. We can see that after 3, there isn't much increase in score.","815e1a66":"**K-Means Clustering on A and A_Coef**"}}