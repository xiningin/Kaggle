{"cell_type":{"03e57fb3":"code","276c9511":"code","04c31d34":"code","482a5179":"code","4a58733e":"code","576f3096":"code","8bcbcece":"code","4262a7e6":"code","d1956847":"code","9529a9cd":"code","8e0d5607":"code","7e170d17":"code","067690a0":"code","6d7fb345":"code","c02be721":"code","21ca57da":"code","a96b8f9a":"code","29fe07d2":"code","b5e5d8fc":"code","0e26fba6":"code","a7f23441":"code","2df66971":"markdown","237b87ef":"markdown","0b145524":"markdown","54f89c8d":"markdown","44504b96":"markdown","94c018e4":"markdown","c6f2a379":"markdown","4f0110a2":"markdown","36c55793":"markdown"},"source":{"03e57fb3":"# !curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n# !python pytorch-xla-env-setup.py --version nightly  --apt-packages libomp5 libopenblas-dev","276c9511":"# import warnings\n# import torch_xla\n# import torch_xla.debug.metrics as met\n# import torch_xla.distributed.data_parallel as dp\n# import torch_xla.distributed.parallel_loader as tlp\n# import torch_xla.utils.utils as xu\n# import torch_xla.core.xla_model as xm\n# import torch_xla.distributed.xla_multiprocessing as xmp\n# import torch_xla.test.test_utils as test_utils\n# import warnings","04c31d34":"# !pip install --upgrade pip\n# !pip install pymap3d==2.1.0\n# !pip install -U l5kit","482a5179":"# from typing import Dict\n\n# from tempfile import gettempdir\n# import matplotlib.pyplot as plt\n# import numpy as np\n# import torch\n# from torch import nn, optim\n# from torch.utils.data import DataLoader\n# from tqdm import tqdm\n\n# from l5kit.configs import load_config_data\n# from l5kit.data import LocalDataManager, ChunkedDataset\n# from l5kit.dataset import AgentDataset, EgoDataset\n# from l5kit.rasterization import build_rasterizer\n# from l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, create_chopped_dataset\n# from l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\n# from l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace\n# from l5kit.geometry import transform_points\n# from l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\n# from prettytable import PrettyTable\n# from pathlib import Path\n\n# import os","4a58733e":"# # set env variable for data\n# os.environ[\"L5KIT_DATA_FOLDER\"] = \"..\/input\/lyft-motion-prediction-autonomous-vehicles\"\n# dm = LocalDataManager(None)\n# # get config\n# cfg = {\n#     'format_version': 4,\n#     'model_params': {\n#         'history_num_frames': 15,\n#         'history_step_size': 1,\n#         'history_delta_time': 0.1,\n#         'future_num_frames': 50,\n#         'future_step_size': 1,\n#         'future_delta_time': 0.1\n#     },\n    \n#     'raster_params': {\n#         'raster_size': [331, 331],\n#         'pixel_size': [0.5, 0.5],\n#         'ego_center': [0.25, 0.5],\n#         'map_type': 'py_semantic',\n#         'satellite_map_key': 'aerial_map\/aerial_map.png',\n#         'semantic_map_key': 'semantic_map\/semantic_map.pb',\n#         'dataset_meta_key': 'meta.json',\n#         'filter_agents_threshold': 0.5\n#     },\n#     'train_data_loader': {\n#         'key': 'scenes\/train.zarr',\n#         'batch_size': 8,\n#         'shuffle': True,\n#         'num_workers': 1\n#     },\n    \n#     'test_data_loader': {\n#         'key': 'scenes\/test.zarr',\n#         'batch_size': 12,\n#         'shuffle': False,\n#         'num_workers': 1\n#     }\n\n# }\n\n# TRAIN_MODE = False","576f3096":"# # ===== INIT DATASET\n# if TRAIN_MODE:\n#     train_cfg = cfg[\"train_data_loader\"]\n#     rasterizer = build_rasterizer(cfg, dm)\n#     train_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()\n#     train_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n#     train_dataloader = DataLoader(train_dataset, shuffle=train_cfg[\"shuffle\"], \n#                                   batch_size=train_cfg[\"batch_size\"], \n#                                  num_workers=train_cfg[\"num_workers\"])\n#     print(train_dataset)","8bcbcece":"# def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n#     \"\"\"3x3 convolution with padding\"\"\"\n#     return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n#                      padding=dilation, groups=groups, bias=False, dilation=dilation)\n\n\n# def conv1x1(in_planes, out_planes, stride=1):\n#     \"\"\"1x1 convolution\"\"\"\n#     return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\n# class BasicBlock(nn.Module):\n#     expansion = 1\n\n#     def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n#                  base_width=64, dilation=1, norm_layer=None):\n#         super(BasicBlock, self).__init__()\n#         if norm_layer is None:\n#             norm_layer = nn.BatchNorm2d\n#         if groups != 1 or base_width != 64:\n#             raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n#         if dilation > 1:\n#             raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n#         # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n#         self.conv1 = conv3x3(inplanes, planes, stride)\n#         self.bn1 = norm_layer(planes)\n#         self.relu = nn.ReLU(inplace=True)\n#         self.conv2 = conv3x3(planes, planes)\n#         self.bn2 = norm_layer(planes)\n#         self.downsample = downsample\n#         self.stride = stride\n\n#     def forward(self, x):\n#         identity = x\n\n#         out = self.conv1(x)\n#         out = self.bn1(out)\n#         out = self.relu(out)\n\n#         out = self.conv2(out)\n#         out = self.bn2(out)\n\n#         if self.downsample is not None:\n#             identity = self.downsample(x)\n\n#         out += identity\n#         out = self.relu(out)\n\n#         return out\n\n\n# class Bottleneck(nn.Module):\n#     # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n#     # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n#     # according to \"Deep residual learning for image recognition\"https:\/\/arxiv.org\/abs\/1512.03385.\n#     # This variant is also known as ResNet V1.5 and improves accuracy according to\n#     # https:\/\/ngc.nvidia.com\/catalog\/model-scripts\/nvidia:resnet_50_v1_5_for_pytorch.\n\n#     expansion = 4\n\n#     def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n#                  base_width=64, dilation=1, norm_layer=None):\n#         super(Bottleneck, self).__init__()\n#         if norm_layer is None:\n#             norm_layer = nn.BatchNorm2d\n#         width = int(planes * (base_width \/ 64.)) * groups\n#         # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n#         self.conv1 = conv1x1(inplanes, width)\n#         self.bn1 = norm_layer(width)\n#         self.conv2 = conv3x3(width, width, stride, groups, dilation)\n#         self.bn2 = norm_layer(width)\n#         self.conv3 = conv1x1(width, planes * self.expansion)\n#         self.bn3 = norm_layer(planes * self.expansion)\n#         self.relu = nn.ReLU(inplace=True)\n#         self.downsample = downsample\n#         self.stride = stride\n\n#     def forward(self, x):\n#         identity = x\n\n#         out = self.conv1(x)\n#         out = self.bn1(out)\n#         out = self.relu(out)\n\n#         out = self.conv2(out)\n#         out = self.bn2(out)\n#         out = self.relu(out)\n\n#         out = self.conv3(out)\n#         out = self.bn3(out)\n\n#         if self.downsample is not None:\n#             identity = self.downsample(x)\n\n#         out += identity\n#         out = self.relu(out)\n\n#         return out\n\n\n# class ResNet(nn.Module):\n\n#     def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n#                  groups=1, width_per_group=64, replace_stride_with_dilation=None,\n#                  norm_layer=None):\n#         super(ResNet, self).__init__()\n#         if norm_layer is None:\n#             norm_layer = nn.BatchNorm2d\n#         self._norm_layer = norm_layer\n\n#         self.inplanes = 64\n#         self.dilation = 1\n#         if replace_stride_with_dilation is None:\n#             # each element in the tuple indicates if we should replace\n#             # the 2x2 stride with a dilated convolution instead\n#             replace_stride_with_dilation = [False, False, False]\n#         if len(replace_stride_with_dilation) != 3:\n#             raise ValueError(\"replace_stride_with_dilation should be None \"\n#                              \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n#         self.groups = groups\n#         self.base_width = width_per_group\n#         self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n#                                bias=False)\n#         self.bn1 = norm_layer(self.inplanes)\n#         self.relu = nn.ReLU(inplace=True)\n#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n#         self.layer1 = self._make_layer(block, 64, layers[0])\n#         self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n#                                        dilate=replace_stride_with_dilation[0])\n#         self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n#                                        dilate=replace_stride_with_dilation[1])\n#         self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n#                                        dilate=replace_stride_with_dilation[2])\n#         self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n#         self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n#         for m in self.modules():\n#             if isinstance(m, nn.Conv2d):\n#                 nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n#             elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n#                 nn.init.constant_(m.weight, 1)\n#                 nn.init.constant_(m.bias, 0)\n\n#         # Zero-initialize the last BN in each residual branch,\n#         # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n#         # This improves the model by 0.2~0.3% according to https:\/\/arxiv.org\/abs\/1706.02677\n#         if zero_init_residual:\n#             for m in self.modules():\n#                 if isinstance(m, Bottleneck):\n#                     nn.init.constant_(m.bn3.weight, 0)\n#                 elif isinstance(m, BasicBlock):\n#                     nn.init.constant_(m.bn2.weight, 0)\n\n#     def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n#         norm_layer = self._norm_layer\n#         downsample = None\n#         previous_dilation = self.dilation\n#         if dilate:\n#             self.dilation *= stride\n#             stride = 1\n#         if stride != 1 or self.inplanes != planes * block.expansion:\n#             downsample = nn.Sequential(\n#                 conv1x1(self.inplanes, planes * block.expansion, stride),\n#                 norm_layer(planes * block.expansion),\n#             )\n\n#         layers = []\n#         layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n#                             self.base_width, previous_dilation, norm_layer))\n#         self.inplanes = planes * block.expansion\n#         for _ in range(1, blocks):\n#             layers.append(block(self.inplanes, planes, groups=self.groups,\n#                                 base_width=self.base_width, dilation=self.dilation,\n#                                 norm_layer=norm_layer))\n\n#         return nn.Sequential(*layers)\n\n#     def _forward_impl(self, x):\n#         # See note [TorchScript super()]\n#         x = self.conv1(x)\n#         x = self.bn1(x)\n#         x = self.relu(x)\n#         x = self.maxpool(x)\n\n#         x = self.layer1(x)\n#         x = self.layer2(x)\n#         x = self.layer3(x)\n#         x = self.layer4(x)\n\n#         x = self.avgpool(x)\n#         x = torch.flatten(x, 1)\n#         x = self.fc(x)\n\n#         return x\n\n#     def forward(self, x):\n#         return self._forward_impl(x)\n\n\n# def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n#     model = ResNet(block, layers, **kwargs)\n#     if pretrained:\n#         state_dict = load_state_dict_from_url(model_urls[arch],\n#                                               progress=progress)\n#         model.load_state_dict(state_dict)\n#     return model\n\n\n# def resnet18(pretrained=False, progress=True, **kwargs):\n#     r\"\"\"ResNet-18 model from\n#     `\"Deep Residual Learning for Image Recognition\" <https:\/\/arxiv.org\/pdf\/1512.03385.pdf>`_\n#     Args:\n#         pretrained (bool): If True, returns a model pre-trained on ImageNet\n#         progress (bool): If True, displays a progress bar of the download to stderr\n#     \"\"\"\n#     return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,\n#                    **kwargs)\n","4262a7e6":"# import torch\n# import torch.nn as nn\n\n# class LyftModel(nn.Module):\n#     def __init__(self, cfg):\n#         super(LyftModel, self).__init__()\n#         # load pre-trained Conv2D model\n#         self.backbone = resnet18(pretrained=False, progress=False)\n#         self.backbone.load_state_dict(\n#             torch.load(\n#                 '..\/input\/resnet18\/resnet18.pth'\n#             )\n#         )\n#         # change input channels number to match the rasterizer's output\n#         num_history_channels = (cfg['model_params']['history_num_frames']+1) * 2\n#         num_in_channels = 3 + num_history_channels\n#         self.backbone.conv1 = nn.Conv2d(\n#             num_in_channels,\n#             self.backbone.conv1.out_channels,\n#             kernel_size=self.backbone.conv1.kernel_size,\n#             stride=self.backbone.conv1.stride,\n#             padding=self.backbone.conv1.padding,\n#             bias=False\n#         )\n        \n#         # change output size to (X, Y) * number of future states\n#         num_targets = 2 * cfg[\"model_params\"][\"future_num_frames\"]\n#         self.backbone.fc = nn.Linear(in_features=512, out_features=num_targets)        \n    \n#     def forward(self, x):\n#         # Forward pass\n#         return self.backbone(x)","d1956847":"# def build_model(cfg: Dict) -> torch.nn.Module:\n#     # load pre-trained Conv2D model\n#     model = LyftModel(cfg)\n#     ckpt = torch.load('..\/input\/lyftmodelall\/resnet18_15hist_331px.pth', map_location=torch.device('cpu'))\n#     model.load_state_dict(ckpt)\n#     return model","9529a9cd":"# build_model(cfg)","8e0d5607":"# class AverageMeter(object):\n#     def __init__(self, name, fmt=':f'):\n#         self.name = name\n#         self.fmt = fmt\n#         self.reset()\n\n#     def reset(self):\n#         self.val = 0\n#         self.avg = 0\n#         self.sum = 0\n#         self.count = 0\n\n#     def update(self, val, n=1):\n#         self.val = val\n#         self.sum += val * n\n#         self.count += n\n#         self.avg = self.sum \/ self.count\n\n#     def __str__(self):\n#         fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n#         return fmtstr.format(**self.__dict__)\n\n# class ProgressMeter(object):\n#     def __init__(self, num_batches, meters, prefix=\"\"):\n#         self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n#         self.meters = meters\n#         self.prefix = prefix\n\n#     def display(self, batch):\n#         entries = [self.prefix + self.batch_fmtstr.format(batch)]\n#         entries += [str(meter) for meter in self.meters]\n#         print('\\t'.join(entries))\n\n#     def _get_batch_fmtstr(self, num_batches):\n#         num_digits = len(str(num_batches \/\/ 1))\n#         fmt = '{:' + str(num_digits) + 'd}'\n#         return '[' + fmt + '\/' + fmt.format(num_batches) + ']'","7e170d17":"# def train_loop_fn(train_loader, model, optimizer, device, scheduler, epoch=None):\n#     # Train\n#     batch_time = AverageMeter('Time', ':6.1f')\n#     data_time = AverageMeter('Data', ':6.1f')\n#     losses = AverageMeter('Loss', ':6.1e')\n#     progress = ProgressMeter(\n#         len(train_loader),\n#         [batch_time, data_time, losses],\n#         prefix=\"[xla:{}]Train:  Epoch: [{}]\".format(xm.get_ordinal(), epoch)\n#     )\n#     criterion = nn.MSELoss(reduction=\"none\")\n#     model.train()\n#     end = time.time()\n#     best = 1000\n#     for i, data in enumerate(train_loader):\n#         data_time.update(time.time()-end)\n#         inputs = data[\"image\"].to(device)\n#         target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n#         targets = data[\"target_positions\"].to(device)\n#         optimizer.zero_grad()\n#         # Forward pass\n#         outputs = model(inputs).reshape(targets.shape)\n#         loss = criterion(outputs, targets)\n#         # not all the output steps are valid, but we can filter them out from the loss using availabilities\n#         loss = loss * target_availabilities\n#         loss = loss.mean()\n#         loss.backward()\n#         xm.optimizer_step(optimizer)\n#         losses.update(loss.item(), inputs.size(0))\n#         scheduler.step(metrics=loss)\n#         batch_time.update(time.time() - end)\n#         end = time.time()\n        \n#         if i % 50 == 0:\n#             progress.display(i)\n#             if losses.avg< best:\n#                 best = losses.avg \n#                 xm.save(model.state_dict(), \"model.bin\")\n#                 xm.master_print(f'Model Saved.{best}')\n        \n#     del loss\n#     del outputs\n#     gc.collect()\n    \n    ","067690a0":"# from torch.utils.data.distributed import DistributedSampler\n# import torch_xla.debug.metrics as met\n# WRAPPED_MODEL = xmp.MpModelWrapper(build_model(cfg))\n\n# def _run():\n#     TRAIN_BATCH_SIZE = 8\n#     EPOCHS = 1\n#     xm.master_print('Starting Run ...')\n#     train_sampler = DistributedSampler(\n#         train_dataset,\n#         num_replicas=xm.xrt_world_size(),\n#         rank=xm.get_ordinal(),\n#         shuffle=False\n#     )\n    \n#     train_data_loader = DataLoader(\n#         train_dataset,\n#         batch_size=TRAIN_BATCH_SIZE,\n#         sampler=train_sampler,\n#         drop_last=True,\n#         num_workers=0\n#     )\n#     xm.master_print('Train Loader Created.')\n    \n#     num_train_steps = int(len(train_dataset) \/ TRAIN_BATCH_SIZE \/ xm.xrt_world_size())\n#     device = xm.xla_device()\n#     model = WRAPPED_MODEL.to(device)\n#     xm.master_print('Done Model Loading.')\n#     optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n#                                                            factor=0.8, patience=3, \n#                                                            verbose=False, \n#                                                            threshold=0.0001, min_lr=0.00001)\n#     xm.master_print(f'Num Train Steps= {num_train_steps}, XRT World Size= {xm.xrt_world_size()}.')\n    \n#     for epoch in range(EPOCHS):\n#         para_loader = tlp.ParallelLoader(train_data_loader, [device])\n#         xm.master_print('Parallel Loader Created. Training ...')\n#         train_loop_fn(para_loader.per_device_loader(device),\n#                       model,  \n#                       optimizer, \n#                       device, \n#                       scheduler, \n#                       epoch\n#                      )\n        \n#         xm.master_print(\"Finished training epoch {}\".format(epoch))\n#         if epoch == EPOCHS-1:\n#             xm.master_print('Saving Model ..')\n#             xm.save(model.state_dict(), \"model.bin\")\n#             xm.master_print('Model Saved.')\n            \n#     if METRICS_DEBUG:\n#       xm.master_print(met.metrics_report(), flush=True)","6d7fb345":"# import time\n# from torch.nn import functional as F\n# def _mp_fn(rank, flags):\n#     torch.set_default_tensor_type('torch.FloatTensor')\n#     _run()\n\n# FLAGS={}\n# if TRAIN_MODE:\n#     xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=1, start_method='fork')","c02be721":"# import numpy as np\n# import os\n# import torch\n\n# from torch import nn, optim\n# from torch.utils.data import DataLoader\n# from tqdm import tqdm\n# from typing import Dict\n\n# from l5kit.data import LocalDataManager, ChunkedDataset\n# from l5kit.dataset import AgentDataset, EgoDataset\n# from l5kit.evaluation import write_pred_csv\n# from l5kit.rasterization import build_rasterizer","21ca57da":"# DIR_INPUT = \"\/kaggle\/input\/lyft-motion-prediction-autonomous-vehicles\"\n\n# SINGLE_MODE_SUBMISSION = f\"{DIR_INPUT}\/single_mode_sample_submission.csv\"\n# MULTI_MODE_SUBMISSION = f\"{DIR_INPUT}\/multi_mode_sample_submission.csv\"\n\n# # Training notebook's output.","a96b8f9a":"# model = LyftModel(cfg)\n# ckpt = torch.load('..\/input\/lyftmodelall\/resnet18_15_331px.bin')\n# model.load_state_dict(ckpt)","29fe07d2":"# # ===== INIT DATASET\n# test_cfg = cfg[\"test_data_loader\"]\n\n# # Rasterizer\n# rasterizer = build_rasterizer(cfg, dm)\n\n# # Test dataset\/dataloader\n# test_zarr = ChunkedDataset(dm.require(test_cfg[\"key\"])).open()\n# test_mask = np.load(f\"{DIR_INPUT}\/scenes\/mask.npz\")[\"arr_0\"]\n# test_dataset = AgentDataset(cfg, test_zarr, rasterizer, agents_mask=test_mask)\n# test_dataloader = DataLoader(test_dataset,\n#                              shuffle=test_cfg[\"shuffle\"],\n#                              batch_size=test_cfg[\"batch_size\"],\n#                              num_workers=test_cfg[\"num_workers\"])\n\n\n# print(test_dataloader)","b5e5d8fc":"# def _test():\n    \n\n#     future_coords_offsets_pd = []\n#     timestamps = []\n#     agent_ids = []\n#     device = 'xla:0'\n#     print(f\"device: {device} ready!\")\n#     model = LyftModel(cfg)\n#     ckpt = torch.load('..\/input\/lyftmodelall\/resnet18_15_331px.bin')\n#     model.load_state_dict(ckpt)\n#     model = model.to(device)\n#     model.eval()\n#     with torch.no_grad():\n#         dataiter = tqdm(test_dataloader)\n\n#         for data in dataiter:\n#             inputs = data[\"image\"].to(device)\n#             target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n#             targets = data[\"target_positions\"].to(device)\n\n#             outputs = model(inputs).reshape(targets.shape)\n\n#             future_coords_offsets_pd.append(outputs.cpu().numpy().copy())\n#             timestamps.append(data[\"timestamp\"].numpy().copy())\n#             agent_ids.append(data[\"track_id\"].numpy().copy())\n#     write_pred_csv('submission.csv',\n#                timestamps=np.concatenate(timestamps),\n#                track_ids=np.concatenate(agent_ids),\n#                coords=np.concatenate(future_coords_offsets_pd))","0e26fba6":"# def _mp_fn(rank, flags):\n# #     torch.set_default_tensor_type('torch.FloatTensor')\n#     _test()\n\n# FLAGS={}\n# if not TRAIN_MODE:\n#     xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=1, start_method='fork')","a7f23441":"import pandas as pd\n\n# Change these to your dataset\/submission.csv\nSUBMISSION_FOLDER = '..\/input\/resnet4days'\nSUBMISSION_FILE = 'resnet50-10000000.csv' #keras resnet50\n\n\nsubmissions = pd.read_csv(f\"{SUBMISSION_FOLDER}\/{SUBMISSION_FILE}\")\nsubmissions.to_csv(\"submission.csv\", index=False)","2df66971":"### Prepare data path and config file","237b87ef":"### TEST MODE","0b145524":"Due to the fact that the following steps take way too long, they are commented out.","54f89c8d":"### Loading the data","44504b96":"### Installing l5kit","94c018e4":"Config is where you can make your changes to have different `model_architecture`, `history_step_size`, `history_num_frames`, `batch_size`, etc. Inspect `cfg` for more details.","c6f2a379":"### Baseline Model\n\nTPU Trainging. With TPU i can train more sample faster","4f0110a2":"## Model Define: Efficientnet-b0 + L2norm + Binary Head","36c55793":"### Importing PyTorch and l5kit"}}