{"cell_type":{"6dae2147":"code","002ff908":"code","01814a93":"code","ac15aaa2":"code","89f2f1a6":"code","d30cec5c":"code","90d9d5f2":"code","a1cb0df5":"code","3f37301f":"code","08c326b7":"code","c4fc4965":"code","a8ac4cdf":"code","b249ff3b":"code","dde76435":"code","6a408612":"code","457cc184":"code","e1397a08":"code","a34c95e5":"code","48aa6608":"code","bda20206":"code","a4cfcb0b":"code","88bb91ef":"code","0a894f5a":"code","b8b90205":"code","1b4901e7":"code","bb9d723c":"code","2442422b":"code","e847e798":"code","52e54f41":"code","933bf1d7":"code","80415c35":"code","3bee92ff":"code","6d66ea24":"code","e304b588":"code","434294a5":"code","c4fa4de2":"code","cc0d3816":"code","bcce302e":"code","fd38353e":"code","6cadf95c":"code","a15da719":"code","d21eba51":"code","17e10fd0":"code","27d18cde":"code","ec47db86":"code","65987fb9":"markdown","c696f630":"markdown"},"source":{"6dae2147":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","002ff908":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set()\n\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import LinearRegression\nnp.set_printoptions(precision = 2, suppress = True)\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n#import raw data\ndf_train_raw = pd.read_csv(\"\/kaggle\/input\/home-data-for-ml-course\/train.csv\")\ndf_test_raw = pd.read_csv(\"\/kaggle\/input\/home-data-for-ml-course\/test.csv\")\ndf_train = df_train_raw.copy()\ndf_test = df_test_raw.copy()\n","01814a93":"#check shape of datasets\ndf_train.shape, df_test.shape","ac15aaa2":"df_train.head()","89f2f1a6":"df_train.describe()","d30cec5c":"df_test.isnull().sum()[df_test.isnull().sum()>0].sort_values(ascending=False)","90d9d5f2":"#check for NaN values\n\ndf_train.isnull().sum()[df_train.isnull().sum()>0].sort_values(ascending=False)","a1cb0df5":"df_test.isnull().sum()[df_test.isnull().sum()>0].sort_values(ascending=False)\/df_test.shape[0]","3f37301f":"#check for percentage that are null vs the total number in dataset\n\ndf_train.isnull().sum()[df_train.isnull().sum()>0].sort_values(ascending=False)\/df_train.shape[0]","08c326b7":"Checkpoint1_train = df_train.copy()\nCheckpoint1_test = df_test.copy()\n\n# drop all columns with Nan Values above 65 Percent as they will \n# not assist much in our model output\nlist_to_drop_1 = ['PoolQC', 'MiscFeature','Alley', 'Fence']\ndf_train_dropped = df_train.drop(list_to_drop_1, axis=1,inplace=False)\ndf_test_dropped = df_test.drop(list_to_drop_1, axis=1,inplace=False)\n","c4fc4965":"#Check data types for each column\ndf_train_dropped.info()","a8ac4cdf":"Checkpoint2_train = df_train_dropped.copy()\nCheckpoint2_test = df_test_dropped.copy()\n\n#Separate training data into numeric and categorical columns for analysis\n\nnumeric_features = df_train_dropped.select_dtypes(exclude=['object']).copy()\ncategorical_features = df_train_dropped.select_dtypes(include=['object']).copy()","b249ff3b":"numeric_features.columns, categorical_features.columns","dde76435":"# fill all null ordinal features with 'NA'\n\ncat = ['GarageType','GarageFinish','BsmtFinType2','BsmtExposure','BsmtFinType1', \n       'GarageCond','GarageQual','BsmtCond','BsmtQual','FireplaceQu',\"KitchenQual\",\n       \"HeatingQC\",'ExterQual','ExterCond']\n\ndf_train_dropped[cat] = df_train_dropped[cat].fillna(\"NA\")\ndf_test_dropped[cat] = df_train_dropped[cat].fillna(\"NA\")","6a408612":"df_test_dropped.isnull().sum()[df_test_dropped.isnull().sum()>0].sort_values(ascending=False)","457cc184":"# fill in missing categorical features with their mode\n\ncolscat = [\"MasVnrType\", \"MSZoning\", \"Exterior1st\", \"Exterior2nd\", \"SaleType\", \"Electrical\", \"Functional\"]\n\ndf_train_dropped[colscat] = df_train_dropped.groupby(\"Neighborhood\")[colscat].transform(lambda x: x.fillna(x.mode()[0]))\ndf_test_dropped[colscat] = df_test_dropped.groupby(\"Neighborhood\")[colscat].transform(lambda x: x.fillna(x.mode()[0]))\n\n","e1397a08":"#put all discrete numerical variables into a list for plotting distributions\nnumeric_discrete_features = ['OverallQual','OverallCond','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath',\n                'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'MoSold', 'YrSold']\n\n#put all continuous numerical variables into a list for plotting distributions\nnumeric_continuous_features = []\nfor i in numeric_features.columns:\n    if i not in numeric_discrete_features:\n        numeric_continuous_features.append(i)","a34c95e5":"# draw distributions of continuous numeric valuses to look at distributions\nfig = plt.figure(figsize=(18,16))\nfor index,col in enumerate(numeric_continuous_features):\n    plt.subplot(10,4,index+1)\n    sns.distplot(numeric_features.loc[:,col].dropna(), kde=False)\nfig.tight_layout(pad=1.0)\n\n\n","48aa6608":"\nCheckpoint3_train = df_train_dropped.copy()\nCheckpoint3_test = df_test_dropped.copy()\n\n#drop the features that one have single value as they will not influence the model much\n\nlist_to_drop_2 = ['BsmtFinSF2', 'LowQualFinSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal' ]\ndf_train_dropped = df_train_dropped.drop(list_to_drop_2, axis=1,inplace=False)\ndf_test_dropped = df_test_dropped.drop(list_to_drop_2, axis=1,inplace=False)","bda20206":"# take a look at numerical discrete features\nfig = plt.figure(figsize=(20,15))\nfor index,col in enumerate(numeric_discrete_features):\n    plt.subplot(10,3,index+1)\n    sns.countplot(x=col, data=numeric_features.dropna())\nfig.tight_layout(pad=1.0)","a4cfcb0b":"#now check categorical variables (skip null values in rows)\n\nfig = plt.figure(figsize=(18,20))\nfor index in range(len(categorical_features.columns)):\n    plt.subplot(9,5,index+1)\n    sns.countplot(x=categorical_features.iloc[:,index], data=categorical_features.dropna())\n    plt.xticks(rotation=90)\nfig.tight_layout(pad=1.0)","88bb91ef":"#now see how the features correlate amongst each other to check for multicollinearity with a correlation matrix and for possible linear relationships, we will mask anthing that is under 0.75 \n\nplt.figure(figsize=(14,12))\ncorrelation = numeric_features.corr()\nsns.heatmap(correlation, mask = correlation < 0.75, linewidth=0.5, cmap='Blues')","0a894f5a":"Checkpoint4_train = df_train_dropped.copy()\nCheckpoint4_test = df_test_dropped.copy()\n\ndf_train_dropped.drop(['GarageYrBlt','TotRmsAbvGrd','1stFlrSF','GarageCars'], axis=1, inplace=True)\ndf_test_dropped.drop(['GarageYrBlt','TotRmsAbvGrd','1stFlrSF','GarageCars'], axis=1, inplace=True)\n","b8b90205":"Checkpoint5_train = df_train_dropped.copy()\nCheckpoint5_test = df_test_dropped.copy()\n\n\n#check for features where more than 95% of values the same and drop them as they will not assist the model much\noverfit_num = []\nfor i in df_train_dropped.columns:\n    counts = df_train_dropped[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(df_train_dropped) * 100 > 95:\n        overfit_num.append(i)\n\noverfit_num = list(overfit_num)\n\ndf_train_dropped.drop(overfit_num, axis=1, inplace=True)\ndf_test_dropped.drop(overfit_num, axis=1, inplace=True)\n","1b4901e7":"# create boxplots to take a look at outliers\nnum_feat_lst = df_train_dropped.select_dtypes(exclude=['object']).copy()\nfig = plt.figure(figsize=(20,20))\nfor index,col in enumerate(num_feat_lst.columns):\n    plt.subplot(6,5,index+1)\n    sns.boxplot(y=col, data=num_feat_lst)\nfig.tight_layout(pad=1.5)","bb9d723c":"Checkpoint6_train = df_train_dropped.copy()\nCheckpoint6_test = df_test_dropped.copy()\n\n#Remove outliers abover obvious thresholds\n\ndf_train_dropped = df_train_dropped.drop(df_train_dropped[df_train_dropped['LotFrontage'] > 200].index)\ndf_train_dropped = df_train_dropped.drop(df_train_dropped[df_train_dropped['LotArea'] > 10000].index)\ndf_train_dropped = df_train_dropped.drop(df_train_dropped[df_train_dropped['MasVnrArea'] > 1200].index)\ndf_train_dropped = df_train_dropped.drop(df_train_dropped[df_train_dropped['BsmtFinSF1'] > 3000].index)\ndf_train_dropped = df_train_dropped.drop(df_train_dropped[df_train_dropped['TotalBsmtSF'] > 5000].index)\ndf_train_dropped = df_train_dropped.drop(df_train_dropped[df_train_dropped['GrLivArea'] > 4000].index)\n\n\n\ndf_train_dropped.shape,df_test_dropped.shape","2442422b":"#now replace all NaN values that should just mean that that feature is not available in the house with 'NA'\n\nNa_list = ['FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType', ]\nfor i in Na_list:\n    df_train_dropped[i].fillna(value='NA', inplace=True)\n    df_test_dropped[i].fillna(value='NA', inplace=True)\n    \ndf_train_dropped.shape,df_test_dropped.shape\n\n","e847e798":"# fill the electrical system missing values with the mode:\nmd = df_train_dropped['Electrical'].mode()[0]\nmdt = df_test_dropped['Electrical'].mode()[0]\ndf_train_dropped['Electrical'].fillna(value=md, inplace=True)\ndf_test_dropped['Electrical'].fillna(value=mdt, inplace=True)\n\ndf_train_dropped.shape,df_test_dropped.shape","52e54f41":"# fill the MasVnrArea  missing values with the mean:\ndf_train_dropped['MasVnrArea'].fillna(value=df_train_dropped['MasVnrArea'].mean(), inplace=True)\ndf_test_dropped['MasVnrArea'].fillna(value=df_test_dropped['MasVnrArea'].mean(), inplace=True)\n\ndf_train_dropped.shape,df_test_dropped.shape","933bf1d7":"#fill in left over numerical NaN features with the mean\n\nnum_left = ['BsmtFullBath', 'BsmtHalfBath', 'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF', 'GarageArea' ]\n\nfor i in num_left:\n    df_train_dropped[i].fillna(value=df_train_dropped[i].mean(), inplace=True)\n    df_test_dropped[i].fillna(value=df_test_dropped[i].mean(), inplace=True)","80415c35":"Chkpoint3 = df_train_dropped.copy()\ndf_train_dropped['LotFrontage'].fillna(value=df_train_dropped['LotFrontage'].mean(), inplace=True)\ndf_test_dropped['LotFrontage'].fillna(value=df_test_dropped['LotFrontage'].mean(), inplace=True)\ndf_train_dropped.shape,df_test_dropped.shape","3bee92ff":"#change MSsubClass to a string variable as each number is representative of a class\n\ndf_train_dropped['MSSubClass'] = df_train_dropped['MSSubClass'].astype(str)\ndf_test_dropped['MSSubClass'] = df_test_dropped['MSSubClass'].astype(str)\ndf_train_dropped.shape,df_test_dropped.shape","6d66ea24":"#map ordinal features to groups that are similar according to the text file given\n\nordinal_map = {'Ex': 5,'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA':0}\nfintype_map = {'GLQ': 6,'ALQ': 5,'BLQ': 4,'Rec': 3,'LwQ': 2,'Unf': 1, 'NA': 0}\nexpose_map = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NA': 0}\n\nord_col = ['ExterQual','ExterCond','BsmtQual', 'BsmtCond','HeatingQC','KitchenQual','GarageQual','GarageCond', 'FireplaceQu']\n\nfor col in ord_col:\n    df_train_dropped[col] = df_train_dropped[col].map(ordinal_map)\n    df_test_dropped[col] = df_test_dropped[col].map(ordinal_map)\n    \nfin_col = ['BsmtFinType1','BsmtFinType2']\nfor col in fin_col:\n    df_train_dropped[col] = df_train_dropped[col].map(fintype_map)\n    df_test_dropped[col] = df_test_dropped[col].map(fintype_map)\n\ndf_train_dropped['BsmtExposure'] = df_train_dropped['BsmtExposure'].map(expose_map)\ndf_test_dropped['BsmtExposure'] = df_test_dropped['BsmtExposure'].map(expose_map)\n                                                    \n","e304b588":"# create columns for minor features to indicate presence or no with 0 or 1\ncolum = ['MasVnrArea','TotalBsmtSF','2ndFlrSF','WoodDeckSF']\n\nfor col in colum:\n    col_name = col+'_bin'\n    df_test_dropped[col_name] = df_test_dropped[col].apply(lambda x: 1 if x > 0 else 0)\n    df_train_dropped[col_name] = df_train_dropped[col].apply(lambda x: 1 if x > 0 else 0)","434294a5":"#use one hot encoding to convert categorical variables to numeric\n\ndf_train_dropped = pd.get_dummies(df_train_dropped)\ndf_test_dropped = pd.get_dummies(df_test_dropped)\n\n#add any missing columns that come from the one hot encoding on test\n\n# Get missing columns in the training test\nmissing_cols = set(df_train_dropped.columns) - set(df_test_dropped.columns)\n# Add a missing column in test set with default value equal to 0\nfor c in missing_cols:\n    df_test_dropped[c] = 0\n# Ensure the order of column in the test set is in the same order than in train set\ndf_test_dropped = df_test_dropped[df_train_dropped.columns]\n# Fill the extra columns with 0\ndf_train_dropped[missing_cols].fillna(value=0, inplace=True)\n\ndf_train_dropped.shape, df_test_dropped.shape\n","c4fa4de2":"Checkpoint7_train = df_train_dropped.copy()\nCheckpoint7_test = df_test_dropped.copy()\n\ny = df_train_dropped[\"SalePrice\"] #separate 'SalePrice' for scaling\nx = df_train_dropped.drop('SalePrice', axis=1) #features without 'Sale price'\ntest = df_test_dropped.drop('SalePrice', axis=1) # test feature set without 'Sale price' - which is all zeros anyway\n\n#Scale features\n\ncols = x.select_dtypes(np.number).columns\n\ntransformer = RobustScaler().fit(x[cols])# fit using training data\nx[cols] = transformer.transform(x[cols]) #transform features\ntest[cols] = transformer.transform(test[cols]) #transform test data using same fit as training data\n\n","cc0d3816":"#create training and validaton sets\n\nX_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=2020)\n\n","bcce302e":"#linear regression\n\n#fit the training data\n\nLinear_model  = LinearRegression()\nLinear_model.fit(X_train,y_train)\nLinear_score = Linear_model.score(X_train,y_train)\nLinear_score","fd38353e":"\nval_predictions = Linear_model.predict(X_val)\nprint(mean_absolute_error(y_val, val_predictions))","6cadf95c":"# this is as far as ive gotten but am new to model fitting ..... can anyone help me understand why the mean absolute error is so high? \n# Is it because i didnt standardie the dependent variable?","a15da719":"# Decision tree regrerssior model\n\nDt_model = DecisionTreeRegressor()\nDt_model.fit(X_train,y_train)\nDt_score = Dt_model.score(X_train,y_train)\nDt_score","d21eba51":"val_tree_predictions = Dt_model.predict(X_val)\nprint(mean_absolute_error(y_val, val_tree_predictions))","17e10fd0":"#random forest\n\nforest_model = RandomForestRegressor(random_state=1)\nforest_model.fit(X_train,y_train)\n#forest_score = forest_model.score(X_train,y_train)\nforest_predictions = forest_model.predict(X_val)\nprint(mean_absolute_error(y_val, forest_predictions))\n","27d18cde":"# use test data\n\noutput = forest_model.predict(test)\n","ec47db86":"submission = pd.DataFrame({'Id': test.index,\n                           'SalePrice': output})\n\nsubmission.to_csv(\"..\/..\/kaggle\/working\/submission.csv\", index=False)","65987fb9":"### Model","c696f630":"GarageArea and GarageCarsm, GrLivArea and TotRmsAbvGrd and GarageYrBlt and YearBuilt show high correlation therefore we remove them\n"}}