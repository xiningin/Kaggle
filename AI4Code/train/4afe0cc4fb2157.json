{"cell_type":{"58180331":"code","6034297c":"code","5ef47fd4":"code","1c62b411":"code","75342489":"code","63f25b5b":"code","5b7f898b":"code","467d79a7":"code","e83c8f65":"code","83f5a66a":"code","67cce0d9":"code","ff2ba01c":"code","ba028fa2":"code","bc3325ae":"code","9ec8856d":"code","51a674ce":"code","519856b2":"code","9f747085":"code","d11bae05":"code","7709e28f":"code","3375dcd1":"code","dce09121":"code","45d5685c":"markdown","5a0d98d2":"markdown","7e77a7a4":"markdown","4867ad80":"markdown","7dc5b297":"markdown","448966b4":"markdown","35bec16d":"markdown","9cf35ed0":"markdown","12788050":"markdown","203a63c6":"markdown","13c50ccc":"markdown","660aca7e":"markdown","401d6e28":"markdown","ffcf7ed1":"markdown","ceea4c62":"markdown","ef565804":"markdown","55a8556d":"markdown","7ed9c988":"markdown","6efd6da8":"markdown","df6e48da":"markdown","9b0bc14c":"markdown","f9224531":"markdown","751718a3":"markdown"},"source":{"58180331":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport random\n\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import roc_auc_score\n\n# Pandas setting to display more dataset rows and columns\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=UserWarning)","6034297c":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df","5ef47fd4":"def min_max_scaler(train, test):\n    x_Mm_scaler = MinMaxScaler()\n    X = pd.DataFrame(x_Mm_scaler.fit_transform(train.drop(\"claim\", axis=1)),\n                     columns=train.drop(\"claim\", axis=1).columns, index = train.index)\n    X_test = pd.DataFrame(x_Mm_scaler.transform(test), columns=test.columns, index = test.index)\n    return X, train.claim, X_test","1c62b411":"# xgb parameters obtained by optuna studies: \nxgb_params = {'n_estimators': 10000, \n              'learning_rate': 0.05378242228966539, \n              'subsample': 0.7502075656719964, \n              'colsample_bytree': 0.8665945134281674, \n              'max_depth': 6, 'booster': 'gbtree', \n              'tree_method': 'gpu_hist', \n              'reg_lambda': 99.32136303076183,\n              'reg_alpha': 32.78057640555784,\n              'random_state': 42,\n              'n_jobs': 4}\n\ndef xgb_train(name, X, y ,X_test,splits = 6, xgb_params = xgb_params):\n  \n    \n    predictions = pd.DataFrame()\n    predictions[\"id\"] = X_test.index\n    \n    skf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros((X.shape[0],))\n    preds = 0\n    model_fi = 0\n    total_mean_rmse = 0\n    fold_roc_auc_score = 0\n    total_mean_roc_auc_score = 0\n\n    for fold, (train_indicies, valid_indicies) in enumerate(skf.split(X,y)):\n\n        X_train, X_valid = X.loc[train_indicies], X.loc[valid_indicies]\n        y_train, y_valid = y.loc[train_indicies], y.loc[valid_indicies]\n        print(f\"Training fold num. {fold+1} of {splits}\")\n        \n        model = XGBClassifier(**xgb_params)\n        model.fit(X_train, y_train,\n                  eval_set=[(X_train, y_train), (X_valid, y_valid)],\n                  eval_metric=\"rmse\",\n                  early_stopping_rounds=100,\n                  verbose=False)\n#         print(\"fitted\")\n        preds += (model.predict_proba(X_test))[:,1] \/ splits\n#         print(preds.shape)\n#         print(\"preds ok\")\n#         model_fi += model.feature_importances_\n#         print(\"model_fi ok\")\n        oof_preds[valid_indicies] =  model.predict_proba(X_valid)[:,1]\n        # print(oof_preds)\n        oof_preds[oof_preds < 0] = 0\n#         fold_rmse = np.sqrt(mean_squared_error(y_scaler.inverse_transform(np.array(y_valid).reshape(-1,1)), y_scaler.inverse_transform(np.array(oof_preds[valid_idx]).reshape(-1,1))))\n#         fold_rmse = np.sqrt(mean_squared_error(y_valid, oof_preds[valid_indicies]))\n        fold_roc_auc_score = roc_auc_score(y_valid, oof_preds[valid_indicies])\n\n        print(f\"\\nScorinf fold num. {fold+1} of {splits}: ROC AUC Score = {fold_roc_auc_score}\")\n\n#         print(f\"Fold {fold} RMSE: {fold_rmse}\")\n#         total_mean_rmse += fold_rmse \/ splits\n        total_mean_roc_auc_score += fold_roc_auc_score \/ splits\n    print(f\"\\nOverall mean Roc AUC Score: {total_mean_roc_auc_score}\")\n     \n    predictions[\"claim\"] = preds\n    csv_name = \"subm_\"+name+\".csv\"\n    predictions.to_csv(csv_name, index=False, header=predictions.columns)\n    \n    \n    return total_mean_roc_auc_score","75342489":"import datatable as dt  # pip install datatable\n# Read the data\ntrain = dt.fread(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\").to_pandas().set_index(\"id\")\ntest = dt.fread(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\").to_pandas().set_index(\"id\")\ntrain = reduce_memory_usage(train, verbose=True)\ntest = reduce_memory_usage(test, verbose=True)","63f25b5b":"print(\"(train, test) na --> \",(train.isna().sum().sum(), test.isna().sum().sum()))","5b7f898b":"print(train.shape, test.shape)\ntrain[\"nNA\"] = train.isna().sum(axis = 1)\ntest[\"nNA\"] = test.isna().sum(axis = 1)\nprint(train.shape, test.shape)","467d79a7":"# I create a dictionary to store training results\nresults = {}","e83c8f65":"%%time\n\nX, y, X_test = min_max_scaler(train, test)\nprint(\"(X, X_test) na --> \",(X.isna().sum().sum(), X_test.isna().sum().sum()))\n\nresults[\"no_NA_handling\"] = xgb_train(\"no_NA_handling\", X, y, X_test)","83f5a66a":"from sklearn.impute import SimpleImputer","67cce0d9":"test.head()","ff2ba01c":"%%time\nX, y, X_test = min_max_scaler(train, test)\nprint(\"(train, test) na --> \",(X.isna().sum().sum(), X_test.isna().sum().sum()))\n\nimputer = SimpleImputer(strategy=\"constant\", fill_value = 0)\nX = pd.DataFrame(imputer.fit_transform(X),\n                 columns=X.columns,\n                index = X.index)\nX_test = pd.DataFrame(imputer.transform(test), \n                      columns=X_test.columns, \n                      index = X_test.index)\n\nprint(\"(train, test) na --> \",(X.isna().sum().sum(), X_test.isna().sum().sum()))\nresults[\"NA_to_0\"] = xgb_train(\"NA_to_0\", X, y, X_test)","ba028fa2":"%%time\nX, y, X_test = min_max_scaler(train, test)\nprint(\"(train, test) na --> \",(X.isna().sum().sum(), X_test.isna().sum().sum()))\n\nimputer = SimpleImputer(strategy=\"mean\")\nX = pd.DataFrame(imputer.fit_transform(X),\n                 columns=X.columns,\n                index = X.index)\nX_test = pd.DataFrame(imputer.transform(test), \n                      columns=X_test.columns, \n                      index = X_test.index)\n\nprint(\"(train, test) na --> \",(X.isna().sum().sum(), X_test.isna().sum().sum()))\nresults[\"NA_to_mean\"] = xgb_train(\"NA_to_mean\", X, y, X_test)\n","bc3325ae":"%%time\nX, y, X_test = min_max_scaler(train, test)\nprint(\"(train, test) na --> \",(X.isna().sum().sum(), X_test.isna().sum().sum()))\n\nimputer = SimpleImputer(strategy=\"median\")\nX = pd.DataFrame(imputer.fit_transform(X),\n                 columns=X.columns, \n                 index = X.index)\n\nX_test = pd.DataFrame(imputer.transform(test), \n                      columns=X_test.columns, \n                      index = X_test.index)\n\nprint(\"(train, test) na --> \",(X.isna().sum().sum(), X_test.isna().sum().sum()))\nresults[\"NA_to_median\"] = xgb_train(\"NA_to_median\", X, y, X_test)\n","9ec8856d":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n#train_mice = train.copy(deep=True)","51a674ce":"%%time\n\nX, y, X_test = min_max_scaler(train, test)\nmice_imputer = IterativeImputer()\nmice_train_set = X.dropna()\n\nprint(\"The mice_train_set shape is: \", mice_train_set.shape)\nprint(\"mice_train_set na values =\", mice_train_set.isna().sum().sum())\n\nmice_imputer.fit(mice_train_set)\nX = pd.DataFrame(mice_imputer.transform(X),\n                 columns=X.columns,\n                 index = X.index)\nX_test = pd.DataFrame(mice_imputer.transform(X_test),\n                 columns=X_test.columns,\n                     index = X_test.index)\n\nprint(\"(train, test) na --> \",(X.isna().sum().sum(), X_test.isna().sum().sum()))\n\nresults[\"NA_to_MICE_naDrop\"] = xgb_train(\"NA_to_MICE_naDrop\", X, y, X_test)","519856b2":"%%time\n\nX, y, X_test = min_max_scaler(train, test)\n\nmice_imputer = IterativeImputer()\n\nmice_train_set = X.sample(X.dropna().shape[0])\nprint(\"The mice_train_set shape is: \", mice_train_set.shape)\nprint(\"mice_train_set na values =\", mice_train_set.isna().sum().sum())\n\nmice_imputer.fit(mice_train_set)\nX = pd.DataFrame(mice_imputer.transform(X),\n                 columns=X.columns,\n                index = X.index)\nX_test = pd.DataFrame(mice_imputer.transform(X_test),\n                 columns=X_test.columns,\n                     index = X_test.index)\n\n\n\nprint(\"(train, test) na --> \",(X.isna().sum().sum(), X_test.isna().sum().sum()))\nresults[\"NA_to_MICE_sampled\"] = xgb_train(\"NA_to_MICE_sampled\", X, y, X_test)","9f747085":"print(\"(train, test) na --> \",(X.isna().sum().sum(), X_test.isna().sum().sum()))","d11bae05":"%%time\n\nX, y, X_test = min_max_scaler(train, test)\n\nmice_imputer = IterativeImputer()\n\nmice_train_set = X.copy(deep = True)\nprint(\"The mice_train_set shape is: \", mice_train_set.shape)\nprint(\"mice_train_set na values =\", mice_train_set.isna().sum().sum())\n\nmice_imputer.fit(mice_train_set)\nX = pd.DataFrame(mice_imputer.transform(X),\n                 columns=X.columns,\n                index = X.index)\nX_test = pd.DataFrame(mice_imputer.transform(X_test),\n                 columns=X_test.columns\n                     ,index = X_test.index)\n\nprint(\"(train, test) na --> \",(X.isna().sum().sum(), X_test.isna().sum().sum()))\nresults[\"NA_to_MICE\"] = xgb_train(\"NA_to_MICE\", X, y, X_test)","7709e28f":"%%time\n\nX, y, X_test = min_max_scaler(train, test)\nfrom tqdm import tqdm\nfeatures = [x for x in X.columns.values if x[0]==\"f\"]\n\nfill_value_dict = {\n    'f1': 'Mean', \n    'f2': 'Median', \n    'f3': 'Median', \n    'f4': 'Median', \n    'f5': 'Mode', \n    'f6': 'Mean', \n    'f7': 'Median', \n    'f8': 'Median', \n    'f9': 'Median', \n    'f10': 'Median', \n    'f11': 'Mean', \n    'f12': 'Median', \n    'f13': 'Mean', \n    'f14': 'Median', \n    'f15': 'Mean', \n    'f16': 'Median', \n    'f17': 'Median', \n    'f18': 'Median', \n    'f19': 'Median', \n    'f20': 'Median', \n    'f21': 'Median', \n    'f22': 'Mean', \n    'f23': 'Mode', \n    'f24': 'Median', \n    'f25': 'Median', \n    'f26': 'Median', \n    'f27': 'Median', \n    'f28': 'Median', \n    'f29': 'Mode', \n    'f30': 'Median', \n    'f31': 'Median', \n    'f32': 'Median', \n    'f33': 'Median', \n    'f34': 'Mean', \n    'f35': 'Median', \n    'f36': 'Mean', \n    'f37': 'Median', \n    'f38': 'Median', \n    'f39': 'Median', \n    'f40': 'Mode', \n    'f41': 'Median', \n    'f42': 'Mode', \n    'f43': 'Mean', \n    'f44': 'Median', \n    'f45': 'Median', \n    'f46': 'Mean', \n    'f47': 'Mode', \n    'f48': 'Mean', \n    'f49': 'Mode', \n    'f50': 'Mode', \n    'f51': 'Median', \n    'f52': 'Median', \n    'f53': 'Median', \n    'f54': 'Mean', \n    'f55': 'Mean', \n    'f56': 'Mode', \n    'f57': 'Mean', \n    'f58': 'Median', \n    'f59': 'Median', \n    'f60': 'Median', \n    'f61': 'Median', \n    'f62': 'Median', \n    'f63': 'Median', \n    'f64': 'Median', \n    'f65': 'Mode', \n    'f66': 'Median', \n    'f67': 'Median', \n    'f68': 'Median', \n    'f69': 'Mean', \n    'f70': 'Mode', \n    'f71': 'Median', \n    'f72': 'Median', \n    'f73': 'Median', \n    'f74': 'Mode', \n    'f75': 'Mode', \n    'f76': 'Mean', \n    'f77': 'Mode', \n    'f78': 'Median', \n    'f79': 'Mean', \n    'f80': 'Median', \n    'f81': 'Mode', \n    'f82': 'Median', \n    'f83': 'Mode', \n    'f84': 'Median', \n    'f85': 'Median', \n    'f86': 'Median', \n    'f87': 'Median', \n    'f88': 'Median', \n    'f89': 'Median', \n    'f90': 'Mean', \n    'f91': 'Mode', \n    'f92': 'Median', \n    'f93': 'Median', \n    'f94': 'Median', \n    'f95': 'Median', \n    'f96': 'Median', \n    'f97': 'Mean', \n    'f98': 'Median', \n    'f99': 'Median', \n    'f100': 'Mode', \n    'f101': 'Median', \n    'f102': 'Median', \n    'f103': 'Median', \n    'f104': 'Median', \n    'f105': 'Median', \n    'f106': 'Median', \n    'f107': 'Median', \n    'f108': 'Median', \n    'f109': 'Mode', \n    'f110': 'Median', \n    'f111': 'Median', \n    'f112': 'Median', \n    'f113': 'Mean', \n    'f114': 'Median', \n    'f115': 'Median', \n    'f116': 'Mode', \n    'f117': 'Median', \n    'f118': 'Mean'\n}\n\n\nprint(\"(train, test) na --> \",(X.isna().sum().sum(), X_test.isna().sum().sum()))\n\nfor col in tqdm(features):\n    if fill_value_dict.get(col)=='Mean':\n        fill_value = X[col].mean()\n    elif fill_value_dict.get(col)=='Median':\n        fill_value = X[col].median()\n    elif fill_value_dict.get(col)=='Mode':\n        fill_value = X[col].mode().iloc[0]\n    \n    X[col].fillna(fill_value, inplace=True)\n    X_test[col].fillna(fill_value, inplace=True)\n\nprint(\"(train, test) na --> \",(X.isna().sum().sum(), X_test.isna().sum().sum()))\nresults[\"NA_to_MMM\"] = xgb_train(\"NA_to_MMM\", X, y, X_test)","3375dcd1":"train.head()\n","dce09121":"results_df = pd.DataFrame(list(results.items()),columns = ['Strategy','ROC AUC'], index = [(51+x)\/10 for x in (range(len(results))) ] )\nresults_df[\"Wall Time\"] = [\"5min 5s\", \"4min 35s\", \"4min 43s\", \"4min 51s\", \"14min 46s\", \"35min 20s\", \"1h 24min 28s\", \"4m 10s\" ]\nresults_df[\"Pubblic Score\"] = [0.81722, 0.78181, 0.78671, 0.78640, 0.81743, 0.81747, 0.81727, 0.81730]\nprint(results_df.sort_values(by = ['ROC AUC'], ascending=False))","45d5685c":"[back to top](#table-of-contents)\n<a id=\"5.7\"><\/a>\n## 5.7 - MICE with the whole training set\nIn this test I'll use the whole training set to calculate the MICE Na Value","5a0d98d2":"We will use sklearn \"Simple Imputer\".\nWe will fit the SimpleImputer only in the Train Set. We will aplly it to both Train and Test set to avoid \nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.SimpleImputer.html","7e77a7a4":"5.8 - ","4867ad80":"# Table of Contents\n<a id=\"table-of-contents\"><\/a>\n- [1 - Introduction](#1)\n- [2 - Libraries and Data import](#2)\n    - [2.1 - Memory Reduction Funtionality](#2.1)\n    - [2.2 - Min Max Scaler FUntion](#2.2)\n    - [2.3 - XGB Trainer Function](#2.3)\n    - [2.4 - Data Import](#2.4)\n- [3 - NA values in train and test](#3)\n- [4 - Feature Engeneer with NA Values](#4)\n- [5 - Filling NA value strategies](#5)\n    - [5.1 - No NA handling](#5.1)\n    - [5.2 - NA values to Zeros](#5.2)\n    - [5.3 - NA Values Imputed to mean](#5.3)\n    - [5.4 - NA Values Imputed to median](#5.4)\n    - [5.5 - MICE with a dropNa training set](#5.5)\n    - [5.6 - MICE with a sampled training set](#5.6)\n    - [5.7 - MICE with the whole training set](#5.7)\n    - [5.8 - Mean, Median, Mode](#5.8)\n- [6 - Results](#6)\n- [7 - Comments](#6)\n   ","7dc5b297":"[back to top](#table-of-contents)\n<a id=\"2.2\"><\/a>\n## 2.2 - Min Max Scaler function","448966b4":"[back to top](#table-of-contents)\n<a id=\"1\"><\/a>\n# 1 - Introduction\nHello everybody.\nIn this notebook I tried to calculate the differences in performance between several strategies of NA handling.\nI applied a simple, fast, barely tuned Stratified XGB Classifier in a 6 branches CrossValidation to understand which strategy could aim to better result.\nHope you like this notebook.\nFeel free to comment and please upvote if you like.\n","35bec16d":"[back to top](#table-of-contents)\n<a id=\"2.4\"><\/a>\n## 2.4 - Data Import\n\nDue to the size of the DataSet I choose DataTable to create the DataFrame\n","9cf35ed0":"[back to top](#table-of-contents)\n<a id=\"5.5\"><\/a>\n## 5.5 - MICE with a dropNa training set\n\nApplying the multivariate feature imputation by chained equations (MICE) with a drop_NA training set","12788050":"[back to top](#table-of-contents)\n<a id=\"5\"><\/a>\n\n# 5 - Filling NA value strategies\nThere are some ML alghorithms that doesn't support the presence of NA values in dataframe. \nI'd like to verify the efficiency of several methods applying a barely tuned XGBC:\n\n* [No NA handling (XGBC can handle a DF with NA)](#5.1)\n* [Filling all NA with zeros](#5.2)\n* [Filling all NA wth the mean value ](#5.3)\n* [Filling all NA with the median value for each features](#5.4)\n* [CINI ML training algorithm to search an appropriate feature value on a sample of datas without NA](#5.5)\n* [CINI ML training algorithm to search an appropriate feature value on a sample of datas ](#5.6)\n* [CINI ML training algorithm to search an appropriate feature value on the whole dataset](#5.7)\n* [Mean, Median, Mode](#5.8)","203a63c6":"[back to top](#table-of-contents)\n<a id=\"6\"><\/a>\n# 6 - Results and Considerations:\nTime and pubblic results are based on version 10 of this notebook: \nThis are the results of the simulation :<br>","13c50ccc":"Quite a lot Na Values to handle. We need to handle all the NA. \nWe have studied them in the EDA session: https:\/\/www.kaggle.com\/sgiuri\/sep21tp-eda-na-handle-xgbc\n","660aca7e":"[back to top](#table-of-contents)\n<a id=\"4\"><\/a>\n\n## 4 - Feature Engeneer with NA Values\nAfter we have imputed the NA we will miss some information: We no longer know where the NA-values were and how many for each row. The following code will avoid at least the count.","401d6e28":"[back to top](#table-of-contents)\n<a id=\"2.3\"><\/a>\n## 2.3- XGB Trainer function\n","ffcf7ed1":"[back to top](#table-of-contents)\n<a id=\"5.3\"><\/a>\n## 5.3 - NA Values Imputed to mean","ceea4c62":"[back to top](#table-of-contents)\n<a id=\"3\"><\/a>\n# 3 - NA values in train and test\nLet's check how many NA values there are in train and test dataframe","ef565804":"[back to top](#table-of-contents)\n<a id=\"7\"><\/a>\n# 7 - Comments\nTHe best strategy looks like to be the CINI - MICE trained on a sample dataset. \nThis Imputer is really slower, but aim to better results.<br>\nI can't understand the very slow results of Pubblic Score of Simple Imputer strategies.. More studies soon.\n\n\n","55a8556d":"[back to top](#table-of-contents)\n<a id=\"5.2\"><\/a>\n## 5.2 - NA Values Imputed to zeros","7ed9c988":"[back to top](#table-of-contents)\n<a id=\"5.4\"><\/a>\n## 5.4 - NA Values Imputed to median","6efd6da8":"[back to top](#table-of-contents)\n<a id=\"5.8\"><\/a>\n## 5.8 - Mean, Median, Mode\n\nIdea taken from www.kaggle.com\/dlaststark\/tps-sep-single-xgboost-model, copied from: https:\/\/www.kaggle.com\/realtimshady\/single-simple-lightgbm\n\nMean: normal distribution\nMedian: unimodal and skewed\nMode: all other cases","df6e48da":"[back to top](#table-of-contents)\n<a id=\"2.1\"><\/a>\n## 2.1 - Memory Reduction Funtionality\n\nDue to the size of the DataSet I had to reduce memory usage to avoid NB crash.\n","9b0bc14c":"[back to top](#table-of-contents)\n<a id=\"5.6\"><\/a>\n## 5.6 - MICE with a sampled training set\nIn this test I use a sample training set o same shape of previous one.","f9224531":"[back to top](#table-of-contents)\n<a id=\"1\"><\/a>\n# 2 - Libraries and Data import","751718a3":"[back to top](#table-of-contents)\n<a id=\"5.1\"><\/a>\n## 5.1 - No NA handling"}}