{"cell_type":{"019d388e":"code","c4da2c05":"code","35def47e":"code","ab950344":"code","4d773ca8":"code","ebef8266":"code","e9250015":"code","7225ac98":"code","5cd936bc":"code","55c06153":"code","df890225":"code","64523c44":"code","1c5f00cc":"code","c420dd9c":"code","099c122c":"code","bb54397f":"code","545e214d":"markdown","386695d2":"markdown","3c21ac74":"markdown","5a81c0db":"markdown","8dae8be5":"markdown","ff34a763":"markdown","5118ed67":"markdown","497ff464":"markdown","876eab28":"markdown"},"source":{"019d388e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c4da2c05":"import pandas as pd\nimport numpy as np\nimport random\nimport os\n\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.metrics import log_loss, confusion_matrix, classification_report\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.options.display.max_rows = 100\npd.options.display.max_columns = 100\n\nimport warnings\nwarnings.simplefilter('ignore')","35def47e":"CFG = {\n    'seed': 2021,\n    'n_splits': 5,\n    'verbose': 0,\n    'target': 'target'\n}","ab950344":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    \nseed_everything(CFG['seed'])","4d773ca8":"def description(df):\n    summary = pd.DataFrame(df.dtypes, columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['Mean'] = np.nanmean(df, axis=0).astype(df.dtypes)\n    summary['Std'] = np.nanstd(df, axis=0).astype(df.dtypes)\n    summary['Minimum'] = np.nanmin(df, axis=0).astype(df.dtypes)\n    summary['Maximum'] = np.nanmax(df, axis=0).astype(df.dtypes)\n    summary['First Value'] = df.iloc[0].values\n    summary['Second Value'] = df.iloc[1].values\n    summary['Third Value'] = df.iloc[2].values\n    summary['dimension'] = str(df.shape)\n    return summary","ebef8266":"train = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/test.csv')\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv')\n\nfeatures = [col for col in train.columns if 'feature_' in col]","e9250015":"# train = train.drop_duplicates(subset=features+['target'])\ntrain = train.drop_duplicates(subset=features, keep=False).reset_index(drop=True)\n","7225ac98":"train[features] = np.log1p(train[features])\ntest[features] = np.log1p(test[features])","5cd936bc":"target = train[CFG['target']].apply(lambda x: int(x.split(\"_\")[-1])-1)\ntrain_df = train[features]\ntest_df = test[features].reset_index(drop=True)","55c06153":"description(train_df).T","df890225":"description(test_df).T","64523c44":"kf = StratifiedKFold(n_splits=CFG['n_splits'], shuffle=True, random_state=CFG['seed'])\n\novr_oof = np.zeros((train_df.shape[0], 9))\novr_pred = 0\n\nscore_list = []\n\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(X=train_df, y=target)):\n    print(f\"===== FOLD {fold} =====\")\n    \n    X_train, y_train = train_df.iloc[trn_idx], target.iloc[trn_idx]\n    X_valid, y_valid = train_df.iloc[val_idx], target.iloc[val_idx]\n    X_test = test_df\n    \n    # OVRClassifier\n    estimator = HistGradientBoostingClassifier(\n        max_iter=250,\n        validation_fraction=None, \n        learning_rate=0.01, \n        max_depth=10, \n        min_samples_leaf=24, \n        max_leaf_nodes=60,\n        random_state=2021\n    )\n    clf = OneVsRestClassifier(estimator, n_jobs=-1)\n    clf.fit(X_train, y_train)\n    \n    ovr_oof[val_idx] = clf.predict_proba(X_valid)\n    ovr_pred += clf.predict_proba(X_test) \/ CFG['n_splits']\n    m_logloss = log_loss(y_valid, ovr_oof[val_idx])\n    \n    score_list.append([CFG['seed'], fold, m_logloss])\n    print(\"+-\" * 40)\n    print(f\"fold {fold} ovr multi_logloss: {m_logloss}\\n\")\n    \nm_logloss = log_loss(target, ovr_oof)\nscore_list.append(['avg', 'oof', m_logloss])\nscore_df = pd.DataFrame(score_list, columns=['seed', 'fold', 'logloss_score'])\nscore_df.to_csv(\"score.csv\", index=False)\n\nprint(\"+-\" * 40)\nprint(f\"multi_logloss: {m_logloss}\")","1c5f00cc":"cm = confusion_matrix(target, ovr_oof.argmax(axis=1))\n\nplt.figure(figsize=((16,8)))\nsns.heatmap(cm, annot=True, fmt='5d', cmap='Blues')\nplt.savefig(\"confusion_matrix.png\")","c420dd9c":"print(classification_report(target, ovr_oof.argmax(axis=1), digits=4))\n\nreport = pd.DataFrame(classification_report(target, ovr_oof.argmax(axis=1), digits=4, output_dict=True)).T\nreport.to_csv(\"report.csv\")","099c122c":"submission.iloc[:, 1:] = ovr_pred  \nsubmission.to_csv(\"submission_01.csv\", index=False)","bb54397f":"plt.figure(figsize=(16, 4), tight_layout=True)\nfor i in range(9):\n    #plt.subplot(3, 3, i+1)\n    #plt.title(f\"Class_{i+1}\")\n    plt.hist(submission[f'Class_{i+1}'], label=f'Class_{i+1}', bins=20, alpha=0.7)\nplt.legend()","545e214d":"### Confusion matrix","386695d2":"### Preparing for submission","3c21ac74":"### Loading the dataset","5a81c0db":"### Creating One-VS-Rest Classifier","8dae8be5":"### Importing Important Libraries","ff34a763":"#### Removing the duplicates in traain data ","5118ed67":"### classification Report","497ff464":"### Preparing the dataset for modelling","876eab28":"#### Tansforming the skewed data by using log normal transformation"}}