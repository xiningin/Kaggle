{"cell_type":{"d7d6515e":"code","ef1241c5":"code","0031f5fb":"code","3092d5c8":"code","aea539e8":"code","1393dbdf":"code","1164924c":"code","6a543f4f":"code","e7c8b9a7":"code","3a440f94":"code","73932672":"code","1c482398":"code","2f99aca6":"code","0d125e22":"code","17bce2cb":"code","7da4806f":"code","c74e9b43":"code","a7378c18":"code","c2356356":"code","1722e70d":"code","5519143d":"code","87f7c5f4":"code","81b07105":"code","21ceeb95":"code","3fd4cb42":"code","5126287e":"code","418f0a1d":"code","9519d91d":"code","2e5d6466":"markdown","1d226f78":"markdown","4ee70718":"markdown","e42a1f5b":"markdown","7f25570b":"markdown","dd1279f3":"markdown","307ae128":"markdown","49c25fb3":"markdown","3f3dead9":"markdown","9b1ea8e5":"markdown","5cbfebff":"markdown"},"source":{"d7d6515e":"!git clone --depth 1 -b v2.3.0 https:\/\/github.com\/tensorflow\/models.git","ef1241c5":"!pip install -r models\/official\/requirements.txt","0031f5fb":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow_hub as hub\nimport sys\nimport re\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nsys.path.append('models')\nfrom official.nlp.data import classifier_data_lib\nfrom official.nlp.bert import tokenization\nfrom official.nlp import optimization","3092d5c8":"print(\"TF Version: \", tf.__version__)\nprint(\"Eager mode: \", tf.executing_eagerly())\nprint(\"Hub version: \", hub.__version__)\nprint(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")","aea539e8":"df = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ndf.head()","1393dbdf":"fig = plt.figure(figsize=(5,5))\nall_disaster_tweets = df[df[\"target\"] == 1][\"text\"].to_list()\nall_non_disaster_tweets = df[df[\"target\"] == 0][\"text\"].to_list()\nlabels = \"Disaster\", \"non-disaster\"\n\nsizes = [len(all_disaster_tweets), len(all_non_disaster_tweets)]\nplt.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True, \n        startangle=90)\n\nplt.axis('equal')\nplt.show()","1164924c":"train_df, valid_df = train_test_split(df, random_state=42, train_size=0.8, stratify=df.target.values)\ntrain_df.shape, valid_df.shape","6a543f4f":"train_data = tf.data.Dataset.from_tensor_slices((train_df.text.values, train_df.target.values))\nvalid_data = tf.data.Dataset.from_tensor_slices((valid_df.text.values, valid_df.target.values))\n\nfor text, label in train_data.take(1):\n    print('\\033[92m' + text.numpy().decode('utf-8'))\n    print(label.numpy())","e7c8b9a7":"label_list = [0, 1]\nmax_seq_length = 128\n\nbert_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/2\", trainable=True)\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","3a440f94":"tokenizer.convert_tokens_to_ids(tokenizer.wordpiece_tokenizer.tokenize('hi, how are you doing?'))","73932672":"def process_tweet(tweet):\n    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n    tweet = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', tweet)\n    tweet = re.sub(r'#', '', tweet)\n    return tweet  ","1c482398":"def to_feature(text, label, label_list=label_list, max_seq_length=max_seq_length, tokenizer=tokenizer):\n\n    example = classifier_data_lib.InputExample(guid=None, \n                                             text_a = process_tweet(text.numpy().decode(\"utf-8\")),\n                                             text_b = None,\n                                             label = label.numpy())\n  \n    feature = classifier_data_lib.convert_single_example(0, example, label_list,\n                                                       max_seq_length, tokenizer)\n  \n    return feature.input_ids, feature.input_mask, feature.segment_ids, feature.label_id","2f99aca6":"def to_feature_map(text, label):\n    input_ids, input_mask, segment_ids, label_id = tf.py_function(to_feature,\n                                                                inp=[text, label],\n                                                                Tout=[tf.int32, tf.int32, tf.int32, tf.int32])\n\n    input_ids.set_shape([max_seq_length])  \n    input_mask.set_shape([max_seq_length])  \n    segment_ids.set_shape([max_seq_length])  \n    label_id.set_shape([])\n\n    X = {\n        'input_word_ids': input_ids,\n        'input_mask': input_mask,\n        'input_type_ids': segment_ids\n    }  \n\n    return X, label_id","0d125e22":"train_data = (train_data.map(to_feature_map,\n                               num_parallel_calls=tf.data.experimental.AUTOTUNE)\n.shuffle(1000)\n.batch(32, drop_remainder=True)\n.prefetch(tf.data.experimental.AUTOTUNE))\n\nvalid_data = (valid_data.map(to_feature_map,\n                               num_parallel_calls=tf.data.experimental.AUTOTUNE)\n.batch(32, drop_remainder=True)\n.prefetch(tf.data.experimental.AUTOTUNE))","17bce2cb":"train_data.element_spec","7da4806f":"def create_model():\n    input_word_ids = keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n                                         name=\"input_word_ids\")\n    input_mask = keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n                                         name=\"input_mask\")\n    input_type_ids = keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n                                         name=\"input_type_ids\")\n    \n    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])\n    drop = tf.keras.layers.Dropout(0.5)(pooled_output)\n    output = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"output\")(drop)\n\n    model = tf.keras.Model(\n        inputs={\n            \"input_word_ids\": input_word_ids,\n            \"input_mask\": input_mask,\n            \"input_type_ids\": input_type_ids\n        },\n        outputs = output)\n    \n    return model","c74e9b43":"model = create_model()\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=[tf.keras.metrics.BinaryAccuracy()])\nmodel.summary()","a7378c18":"tf.keras.utils.plot_model(model=model, show_shapes=True, dpi=76)","c2356356":"with tf.device('\/gpu:0'):\n    epochs = 10\n    history = model.fit(train_data,\n                    validation_data=valid_data,\n                    epochs=epochs,\n                    verbose = 1)","1722e70d":"def plot_graphs(history, metric):\n    plt.plot(history.history[metric])\n    plt.plot(history.history['val_'+metric], '')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(metric)\n    plt.grid(True)\n    plt.legend([metric, 'val_'+metric])\n    plt.show()","5519143d":"plot_graphs(history, 'loss')","87f7c5f4":"plot_graphs(history, 'binary_accuracy')","81b07105":"test_df = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\ntest_df.head()","21ceeb95":"test_data = tf.data.Dataset.from_tensor_slices((test_df.text.values, np.zeros(test_df.shape[0])))\ntest_data = (test_data.map(to_feature_map,\n                               num_parallel_calls=tf.data.experimental.AUTOTUNE)\n.batch(32, drop_remainder=False)\n.prefetch(tf.data.experimental.AUTOTUNE))","3fd4cb42":"y_pred = model.predict(test_data)","5126287e":"target_pred = [1 if y[0] >= 0.5 else 0 for y in y_pred.tolist()]","418f0a1d":"submission_df = test_df.drop([\"keyword\", \"location\", \"text\"], axis=1)\nsubmission_df[\"target\"] = target_pred\nsubmission_df.head()","9519d91d":"submission_df.to_csv(\"submission.csv\", index=False)","2e5d6466":"# Create Model","1d226f78":"# Test predict","4ee70718":"# Wrap a Python Function into a TensorFlow op for Eager Execution","e42a1f5b":"# Create a TensorFlow input Pipeline","7f25570b":"# Download a Pre-trained BERT Model from TensorFlow Hub","dd1279f3":"# Train Model","307ae128":"# Import the dataset","49c25fb3":"# Split Train\/test","3f3dead9":"# Tokenize and Preprocess Text for BERT","9b1ea8e5":"# Evaluate visually the model","5cbfebff":"# Setup"}}