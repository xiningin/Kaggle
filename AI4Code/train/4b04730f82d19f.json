{"cell_type":{"2e6fd96c":"code","2d8f112c":"code","8fe76383":"code","36bba822":"code","14bcdb24":"code","c496f16b":"code","ae924779":"code","8556ecee":"code","8759f9ba":"code","ea778a98":"code","381c3fd4":"code","2bee61ce":"code","47648589":"code","fdc5de82":"code","bfef11ee":"code","76244611":"code","5f7f2e8b":"code","c193914c":"code","26b8fa9b":"code","a0ab93a5":"code","8c9b0bc4":"code","ae434dd8":"code","25afb40b":"code","4bfc61c6":"code","48d6d040":"code","08194c50":"code","fcf63752":"code","11f13669":"code","d774aa39":"code","9c3efc12":"code","6ae8d85f":"code","17a5a619":"code","7e274e65":"code","24c5f7bd":"code","e3ad88df":"code","8951d43f":"code","6ed5b6fc":"code","adfe4ef9":"code","b16325f7":"code","50f8af2e":"code","1564ec1b":"code","bbf42a43":"code","2978e7b1":"code","2e60cc0c":"code","9eb4a33b":"code","28d43e94":"code","619e7959":"code","e5606082":"code","1c72cdc9":"code","d7c2fd6d":"code","e64be6cf":"code","b9b77a5f":"code","0ce0e479":"code","77de60d3":"code","2d847cb3":"code","ec97c0ed":"code","3bbc252d":"code","84eeae0c":"code","fea0ea5e":"code","6f99cd55":"code","53003f70":"code","80a14b6d":"code","eca93659":"code","04f68b89":"code","2f892d17":"code","b948c968":"code","6d81b76a":"code","07b99cf3":"code","0f775bab":"code","9f8dd270":"code","cbda1308":"code","3b393c05":"code","fb4b473e":"markdown","ffcb37a4":"markdown","4b0aec1f":"markdown","f534845a":"markdown","0441bc67":"markdown","57c407a2":"markdown","ce052858":"markdown","8116fc91":"markdown","c9cf97b1":"markdown","397e099a":"markdown","6f24ebbf":"markdown","2f2a7fd5":"markdown","8b46ea2e":"markdown","0f0c4474":"markdown","a680c3e3":"markdown","0e181b1a":"markdown","c22e13df":"markdown","8f0ad219":"markdown","e5607b8d":"markdown","f8374b9b":"markdown","fae85323":"markdown","4bed1885":"markdown","6869ed46":"markdown","1c99f315":"markdown","a56a68b4":"markdown","0d9de4b8":"markdown","f84aa96e":"markdown","4a805893":"markdown","4757fd97":"markdown","4b593082":"markdown","1e4f8312":"markdown","bc85ab76":"markdown"},"source":{"2e6fd96c":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","2d8f112c":"bike_df = pd.read_csv('..\/input\/boombikedata\/day.csv')\nbike_df.shape","8fe76383":"bike_df.head()","36bba822":"bike_df.info()","14bcdb24":"bike_df.isnull().sum()","c496f16b":"## instant column is not of use so we can drop stright forward\n\nbike_df = bike_df.drop(['instant','casual','registered','dteday'],axis=1)\nbike_df.head()","ae924779":"print(bike_df.mnth.value_counts())\nprint(bike_df.weathersit.value_counts())\nprint(bike_df.weekday.value_counts())\nprint(bike_df.season.value_counts())\nprint(bike_df.yr.value_counts())\nprint(bike_df.holiday.value_counts())\nprint(bike_df.workingday.value_counts())","8556ecee":"bike_df.season = bike_df.season.astype('str')\nbike_df.mnth = bike_df.mnth.astype('str')\nbike_df.weekday = bike_df.weekday.astype('str')\nbike_df.weathersit = bike_df.weathersit.astype('str')\nbike_df.yr = bike_df.yr.astype('str')\n#bike_df.holiday = bike_df.holiday.astype('str')\n#bike_df.workingday = bike_df.workingday.astype('str')","8759f9ba":"def convert_weekday(day):\n   # ynm = ynm.lower()\n    if day =='0':\n        return 'tue'\n    elif day =='1':\n        return 'wed'\n    elif day =='2':\n        return 'thu'\n    if day =='3':\n        return 'fri'\n    elif day =='4':\n        return 'sat'\n    elif day =='5':\n        return 'sun'\n    elif day =='6':\n        return 'mon'","ea778a98":"def convert_weathersit(weathersit):\n   # ynm = ynm.lower()\n    if weathersit =='1':\n        return 'clear'\n    elif weathersit =='2':\n        return 'mist'\n    elif weathersit =='3':\n        return 'light'\n    elif weathersit =='4':\n        return 'heavy'\n\n\n","381c3fd4":"def convert_mon(mon):\n   # ynm = ynm.lower()\n    if mon =='1':\n        return 'jan'\n    elif mon =='2':\n        return 'feb'\n    elif mon =='3':\n        return'mar'\n    if mon =='4':\n        return 'apr'\n    elif mon =='5':\n        return 'may'\n    elif mon =='6':\n        return'jun'\n    if mon =='7':\n        return 'jul'\n    elif mon =='8':\n        return 'aug'\n    elif mon =='9':\n        return'sep'\n    if mon =='10':\n        return 'oct'\n    elif mon =='11':\n        return 'nov'\n    else:\n        return 'dec'","2bee61ce":"def convert_season(season):\n   # ynm = ynm.lower()\n    if season =='1':\n        return 'spring'\n    elif season =='2':\n        return 'summer'\n    elif season =='3':\n        return'fall'\n    else:\n        return 'winter'","47648589":"def convert_yr(yr):\n   # ynm = ynm.lower()\n    if yr =='1':\n        return '2019'\n    elif yr =='0':\n        return '2018'","fdc5de82":"bike_df[\"season\"] = bike_df['season'].apply(lambda x: convert_season(x))\n\nbike_df[\"mnth\"] = bike_df['mnth'].apply(lambda x: convert_mon(x))\n\nbike_df[\"weekday\"] = bike_df['weekday'].apply(lambda x: convert_weekday(x))\n\nbike_df[\"weathersit\"] = bike_df['weathersit'].apply(lambda x: convert_weathersit(x))\n\nbike_df[\"yr\"] = bike_df['yr'].apply(lambda x: convert_yr(x))","bfef11ee":"print(bike_df.mnth.value_counts())\nprint(bike_df.weekday.value_counts())\nprint(bike_df.season.value_counts())\nprint(bike_df.weathersit.value_counts())\nprint(bike_df.yr.value_counts())\nprint(bike_df.holiday.value_counts())\nprint(bike_df.workingday.value_counts())","76244611":"bike_df.head()\n","5f7f2e8b":"sns.pairplot(bike_df[['temp','atemp','hum','windspeed','cnt']])\nplt.show()","c193914c":"# since temp and atemp are correlated and apply logic we can remove one of them stright forward\nbike_df = bike_df.drop(['atemp'],axis=1)","26b8fa9b":"plt.figure(figsize=(20, 14))\nplt.subplot(2,3,1)\nsns.boxplot(x='holiday',y='cnt',data=bike_df)\nplt.subplot(2,3,2)\nsns.boxplot(x='season',y='cnt',data=bike_df)\nplt.subplot(2,3,3)\nsns.boxplot(x='yr',y='cnt',data=bike_df)\nplt.subplot(2,3,4)\nsns.boxplot(x='workingday',y='cnt',data=bike_df)\nplt.subplot(2,3,5)\nsns.boxplot(x='weathersit',y='cnt',data=bike_df)\nplt.show()","a0ab93a5":"plt.figure(figsize=(24, 12))\nplt.subplot(2,3,1)\nsns.boxplot(x='weekday',y='cnt',data=bike_df)\nplt.subplot(2,3,2)\nsns.boxplot(x='mnth',y='cnt',data=bike_df)\nplt.subplot(2,3,3)\nsns.boxplot(x='yr',y='cnt',data=bike_df)\nplt.show()","8c9b0bc4":"season_dummy = pd.get_dummies(bike_df['season'], drop_first = True)\nweathersit_dummy = pd.get_dummies(bike_df['weathersit'], drop_first = True)\nweekday_dummy = pd.get_dummies(bike_df['weekday'], drop_first = True)\nmnth_dummy = pd.get_dummies(bike_df['mnth'], drop_first = True)\nyr_dummy = pd.get_dummies(bike_df['yr'], drop_first = True)\n\n","ae434dd8":"season_dummy.spring.value_counts()","25afb40b":"print(season_dummy.head())\nprint(weathersit_dummy.head())\nprint(weekday_dummy.head())\nprint(mnth_dummy.head())\nprint(yr_dummy.head())","4bfc61c6":"bike_df = pd.concat([bike_df,season_dummy,mnth_dummy,weathersit_dummy,weekday_dummy,yr_dummy],axis=1)\n","48d6d040":"bike_df.drop(['mnth','weekday','weathersit','season','yr'],axis=1,inplace = True)","08194c50":"bike_df.head()","fcf63752":"bike_df.shape","11f13669":"bike_df.columns","d774aa39":"np.random.seed(0)\ndf_train,df_test = train_test_split(bike_df,train_size=0.7,random_state=50)\nprint(df_test.shape)\nprint(df_train.shape)","9c3efc12":"scaler = MinMaxScaler()\n","6ae8d85f":"# Apply scaler to numeric variable except dummy or binary categorical variable\n\nnum_vars_scale = ['temp','hum','windspeed','cnt']\n\ndf_train[num_vars_scale] = scaler.fit_transform(df_train[num_vars_scale])","17a5a619":"df_train.describe()","7e274e65":"plt.figure(figsize = (20, 10))\nsns.heatmap(df_train.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","24c5f7bd":"y_train = df_train.pop('cnt')\nX_train = df_train","e3ad88df":"# Running RFE with the output number of the variable equal to 15\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, 15)             # running RFE\nrfe = rfe.fit(X_train, y_train)","8951d43f":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","6ed5b6fc":"selected_col = X_train.columns[rfe.support_]\nselected_col","adfe4ef9":"# lets first get the training data for rfe columns only\n\nX_train_rfe = X_train[selected_col]\nX_train_sm = sm.add_constant(X_train_rfe)\nX_train_sm","b16325f7":"lm = sm.OLS(y_train,X_train_sm).fit()","50f8af2e":"print(lm.summary())","1564ec1b":"### $ VIF_i = \\frac{1}{1 - {R_i}^2} $","bbf42a43":"VIF = pd.DataFrame()\nVIF['features'] = X_train_sm.columns\nVIF['VIF'] = [variance_inflation_factor(X_train_sm.values,i) for i in range(X_train_sm.shape[1])]\nVIF['VIF'] = round(VIF['VIF'], 2)\nVIF = VIF.sort_values(by = \"VIF\", ascending = False)\nVIF","2978e7b1":"X = X_train_rfe.drop('jan',axis=1)","2e60cc0c":"## creating new modal after removing droping 1st column\n\n# Build a third fitted model\nX_train_lm = sm.add_constant(X)\n\nlr_2 = sm.OLS(y_train, X_train_lm).fit()","9eb4a33b":"# Print the summary of the model\nprint(lr_2.summary())","28d43e94":"vif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","619e7959":"X = X.drop('dec',axis=1)","e5606082":"## creating new modal after removing droping 2nd column\n\n# Build a third fitted model\nX_train_2m = sm.add_constant(X)\n\nlr_3 = sm.OLS(y_train, X_train_2m).fit()","1c72cdc9":"# Print the summary of the model\nprint(lr_3.summary())","d7c2fd6d":"X = X.drop('nov',axis=1)","e64be6cf":"## creating new modal after removing droping 2nd column\n\n# Build a third fitted model\nX_train_3m = sm.add_constant(X)\n\nlr_4 = sm.OLS(y_train, X_train_3m).fit()","b9b77a5f":"# Print the summary of the model\nprint(lr_4.summary())","0ce0e479":"X = X.drop('spring',axis=1)","77de60d3":"## creating new modal after removing droping 2nd column\n\n# Build a third fitted model\nX_train_4m = sm.add_constant(X)\n\nlr_5 = sm.OLS(y_train, X_train_4m).fit()","2d847cb3":"# Print the summary of the model\nprint(lr_5.summary())","ec97c0ed":"vif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","3bbc252d":"X = X.drop('hum',axis=1)","84eeae0c":"## creating new modal after removing droping 2nd column\n\n# Build a third fitted model\nX_train_5m = sm.add_constant(X)\n\nlr_6 = sm.OLS(y_train, X_train_5m).fit()","fea0ea5e":"# Print the summary of the model\nprint(lr_6.summary())","6f99cd55":"vif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","53003f70":"y_train_pred = lr_6.predict(X_train_5m)","80a14b6d":"# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((y_train - y_train_pred), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18) ","eca93659":"# Apply scaler to numeric variable except dummy or binary categorical variable\n\nnum_vars_scale = ['temp','hum','windspeed','cnt']\n\ndf_test[num_vars_scale] = scaler.fit_transform(df_test[num_vars_scale])","04f68b89":"df_test.head()","2f892d17":"y_test = df_test.pop('cnt')\nX_test = df_test\n","b948c968":"X_test_new = X_test[X.columns]\nX_test_new.head()","6d81b76a":"X_test_sm = sm.add_constant(X_test_new)","07b99cf3":"# Making predictions\ny_pred = lr_6.predict(X_test_sm)","0f775bab":"r_sqre = r2_score(y_test,y_pred)\nr_sqre","9f8dd270":"diff_r_sq_train_vs_test = .835 - .796\nprint(round(diff_r_sq_train_vs_test,2))","cbda1308":"fig = plt.figure()\nsns.distplot((y_test - y_pred),bins=20)\nfig.suptitle('Test Dataset Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18) ","3b393c05":"#residual_sum = y_test - y_pred\n#residual_sum","fb4b473e":"X_test_sm.head()","ffcb37a4":"Looking at the p-values, it looks like some of the variables aren't really significant (in the presence of other variables).\n\nMaybe we could drop some?\n\nWe could simply drop the variable with the highest, non-significant p value. A better way would be to supplement this with the VIF information. ","4b0aec1f":"### Feature Selection using RFE\n\n    from sklearn.feature_selection import RFE\n    from sklearn.linear_model import LinearRegression","f534845a":"### it is less than 4% which is a good check","0441bc67":"## Step 6 Feature Selection Dropping the variable and updating the model","57c407a2":"#### Applying the scaling on the test sets","ce052858":"### Calculating ViF","8116fc91":"### Rescaling the Features  -- df_train\n\nAs you saw in the demonstration for Simple Linear Regression, scaling doesn't impact your model. Here we can see that except for `temp`, `hum`,`windspeed`,all the columns have small integer values. So it is extremely important to rescale the variables so that they have a comparable scale. If we don't have comparable scales, then some of the coefficients as obtained by fitting the regression model might be very large or very small as compared to the other coefficients. This might become very annoying at the time of model evaluation. So it is advised to use standardization or normalization so that the units of the coefficients obtained are all on the same scale. As you know, there are two common ways of rescaling:\n\n1. Min-Max scaling from sklearn.preprocessing import MinMaxScaler\n2. Standardisation (mean-0, sigma-1) \n\nThis time, we will use MinMax scaling.","c9cf97b1":"### Building model using statmodel\n\n#### with all variables","397e099a":"# Bike Sharing Assignment - ML Linear Regression #UpGrad_AIML_IIITB","6f24ebbf":"#### lets drop jan as it has high p value .039","2f2a7fd5":"#### lets drop dec as it has high p value .019","8b46ea2e":"#### lets drop jan as it has high p value .021","0f0c4474":"## Step 4: Splitting the Data into Training and Testing Sets\n\nAs you know, the first basic step for regression is performing a train-test split.","a680c3e3":"#### Lets devide the data into X and y","0e181b1a":"## Step 3. Data Preparation\n\n    \u25cf Create dummy variables for all the categorical features.\n    \u25cf Divide the data to train and test.\n    \u25cf Perform scaling.\n    \u25cf Divide the data into X and y.","c22e13df":"## Step 7: Residual Analysis of the train data\n\nSo, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of the error terms and see what it looks like.","8f0ad219":"## Step 8: Making Predictions Using the Final Model\n\nNow that we have fitted the model and checked the normality of error terms, it's time to go ahead and make predictions using the final, i.e. fourth model.","e5607b8d":"#### lets drop jan as it has high p value .046","f8374b9b":"### Calculating VIF","fae85323":"### Calculating VIF","4bed1885":"#### Conclusion:\n\n    \u25cf 'season''mnth','weekday','wathersit' as categorical data.\n    \u25cf Binday Numeric data is 'holiday','yr', 'workingday' and rest all continuous     features\n    \u25cf Drop the unnecessary variables: \u2018instant\u2019, \u2018dteday\u2019, \u2018casual\u2019 and \u2018registered\u2019. \n      as they are not adding any value\n    \u25cf Then we have to check the data-type of all the columns and make necessary changes if required.\n    \u25cf Converting data type to Categorical data type of these columns ","6869ed46":"## Business Goal:\nYou are required to model the demand for shared bikes with the available independent variables. It will be used by the management to understand how exactly the demands vary with different features. They can accordingly manipulate the business strategy to meet the demand levels and meet the customer's expectations. Further, the model will be a good way for management to understand the demand dynamics of a new market. \n\n#### The company wants to know:\n\n    \u25cf Which variables are significant in predicting the demand for shared bikes.\n    \u25cf How well those variables describe the bike demands\n\n#### What you need to do?\n\n    \u25cf Create a linear model that describe the effect of various features on demand.\n    \u25cf The model should be interpretable so that the management can understand it.","1c99f315":"### Error terms are normally distributed with mean zero(not X, Y)","a56a68b4":"## Step 1: Reading and Understanding the Data\n\nSteps:\n\n    \u25cf Identify the categorical and continuous features.\n    \u25cf Drop the unnecessary variables: \u2018instant\u2019, \u2018dteday\u2019, \u2018casual\u2019 and \u2018registered\u2019. as they are not adding any value\n    \u25cf Then we have to check the data-type of all the columns and make necessary changes if required.","0d9de4b8":"#### Visualising Numeric Variables\n\nLet's make a pairplot of all the numeric variables","f84aa96e":"### Checking VIF\n\nVariance Inflation Factor or VIF, gives a basic quantitative idea about how much the feature variables are correlated with each other. It is an extremely important parameter to test our linear model. The formula for calculating `VIF` is:\n\n### $ VIF_i = \\frac{1}{1 - {R_i}^2} $","4a805893":"## Step 5. Building The Model\n\nCreate Linear Regression model using mixed approach(RFE and VIF\/p-value).\n\n    \u25cf Check the various assumptions for Feature Selection process.\n    \u25cf Check the Adjusted R-Square for both test and train data.\n    \u25cf Report the final model\n\n\nFit a regression line through the training data using `statsmodels`. Remember that in `statsmodels`, you need to explicitly fit a constant using `sm.add_constant(X)` because if we don't perform this step, `statsmodels` fits a regression line passing through the origin, by default.\n\nimport statsmodels.api as sm","4757fd97":"## Step 9. Model Evaluation\n\n#### from sklearn.metrics import mean_squared_error\n#### from sklearn.metrics import r2_score\n\n\nCheck Assumptions for Residual analysis\n    1. Error terms are normally distributed with mean zero(not X, Y)\n    2. Error terms are independent of each other\n    3. Error terms have constant variance (homoscedasticity)","4b593082":"## Step 2. Data Visualisation\n\n    \u25cf Perform EDA to understand various variables.\n    \u25cf Check the correlation between the variables.","1e4f8312":"#### Visualising Categorical Variables\n\nLet's make a box of all the numeric variables","bc85ab76":"#### lets drop jan as it has high VIF value 14.58"}}