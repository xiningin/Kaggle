{"cell_type":{"330f7c2c":"code","75b70159":"code","12afe065":"code","37dd18e9":"code","fd5cb135":"code","2a6703f5":"code","a58e1e2a":"code","1171a381":"code","c81c8e4e":"code","f0c2e3e4":"code","a8e7f704":"code","e1739fa4":"code","5991961c":"code","14dff169":"code","07fbeb7b":"code","90fbcab2":"code","87fa798f":"code","1bbc6039":"code","15008203":"code","d795b88e":"code","81f1f9da":"code","31e49ae4":"markdown","9f99a2ff":"markdown","a130fea6":"markdown"},"source":{"330f7c2c":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import KNNImputer, SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import KFold, GridSearchCV\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.metrics import classification_report,confusion_matrix, accuracy_score, precision_recall_curve\nfrom catboost import Pool, CatBoostClassifier, cv, CatBoostRegressor\nimport sklearn.metrics as metrics\n","75b70159":"train_df = pd.read_csv('..\/input\/iba-ml1-mid-project\/train.csv')","12afe065":"test_df=pd.read_csv('..\/input\/iba-ml1-mid-project\/test.csv')","37dd18e9":"#As I mentioned in EDA file, credit_line_utilization column needs to be converted from object to float\ntest_df['credit_line_utilization'].replace(',','.',regex=True, inplace=True)\ntest_df[\"credit_line_utilization\"] = pd.to_numeric(test_df[\"credit_line_utilization\"])\ntrain_df['credit_line_utilization'].replace(',','.',regex=True, inplace=True)\ntrain_df[\"credit_line_utilization\"] = pd.to_numeric(train_df[\"credit_line_utilization\"])","fd5cb135":"train_df.head()","2a6703f5":"train_df.describe()","a58e1e2a":"train_df.info()","1171a381":"#droppping id column\ntrain_df.drop(labels='Id', axis=1, inplace=True)","c81c8e4e":"features = train_df.columns.difference(['defaulted_on_loan']).values.tolist()\nfeatures","f0c2e3e4":"X = train_df[features]\ny = train_df['defaulted_on_loan']\nnumeric_features = features\n#imputation using Simple Imputer\nX[numeric_features]=SimpleImputer(strategy='mean').fit_transform(X[numeric_features])","a8e7f704":"X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=1234)","e1739fa4":"#Making model with CatBoostClassifier\ncate_features_index = np.where(X.dtypes != float)[0]\nmodel=CatBoostClassifier(random_seed=1234, iterations=1600, learning_rate=0.0097, l2_leaf_reg=3,eval_metric='AUC',use_best_model=True)","5991961c":"# Using GridSearch for finding best parameters for CatBoostClassification\n# model_params = {  \n#     'depth': [8, 9, 10, 12],\n#     'learning_rate': [0.01, 0.02, 0.03],\n#     'iterations':[1000, 1200, 1400, 1600 ],\n#     'l2_leaf_reg':[1, 2, 3, 4, 5]\n#          }\n# model_grid = GridSearchCV(estimator=model, param_grid=model_params)\n\n# It takes long time so I used the best params I got\n# model_grid.fit(X_train, y_train)\n# model_grid.best_params_","14dff169":"#After getting best values we need to remake our model with best parameter values\nmodel=CatBoostClassifier(random_seed=1234,depth=9, iterations=1600, learning_rate=0.01, l2_leaf_reg=3,use_best_model=True)","07fbeb7b":"#fitting\nmodel.fit(X_train, y_train, cat_features=cate_features_index,eval_set=(X_test,y_test))","90fbcab2":"y_pre=model.predict(X_test)\ny_prob=model.predict_proba(X_test)","87fa798f":"#classification report\nprint(classification_report(y_test, y_pre))","1bbc6039":"#confusion matrix\nprint(confusion_matrix(y_test, y_pre))","15008203":"#Plotting recall and precision curve\nprecisions, recalls, thresholds = precision_recall_curve(y_test,y_prob[:, 1])\nplt.plot(precisions, recalls)\nplt.xlabel('Precision')\nplt.ylabel('Recall')\nplt.show()","d795b88e":"# calculate the fpr and tpr for all thresholds of the classification\nprobs = model.predict_proba(X_test)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\n#plotting ROC and getting AUC value\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","81f1f9da":"#Fitting our test data to the model and making submission\nX = test_df[features]\n#numerical values imputation\nX[numeric_features]=SimpleImputer(strategy='mean').fit_transform(X[numeric_features])\n#trial check\ntrial = model.predict_proba(X)[:,1:]\ndata_frame=pd.DataFrame(trial)\ndata_frame.index = data_frame.index+1 \ndata_frame.columns=['Predicted']\ndata_frame.to_csv('mypredictionFinal.csv', index_label='Id')","31e49ae4":"Although, I tried different techniques to get rid of outliers(Isolation Forest and manually), balancing for 'defaulted_on_loan' column(SMOTE, undersampling, oversampling) I got lower score on kaggle. So I only used CatBoost Classification model with best parameters from grid search cv. ","9f99a2ff":"After I tried different classification methods including KNN Classification, Logistic Regression, Decision Tree Classification, Boosting methods, I got best score with CatBoostClassification. I applied grid search cv to get best parameter for this Classification method. I got lower marks with other methods so I did not add them into my notebook.","a130fea6":"With this model we get our AUC score 0.86 which is quite good."}}