{"cell_type":{"0a2d7520":"code","56fe6dab":"code","770f2e49":"code","c56b001b":"code","cd181a1d":"code","a47ff697":"code","93ad352a":"code","45fa1c19":"code","dfb8d004":"code","b129131d":"code","6abd3d00":"code","11070778":"code","7ea11f66":"code","478bad2c":"code","c1566bef":"code","3dba6e7c":"code","791afb03":"code","68cd1642":"code","607dc61b":"code","0c4985b5":"code","5405b81b":"code","0a2fa325":"code","22864042":"code","4057581f":"code","2ab4aea5":"code","19b647b3":"code","f74cf400":"code","906c4a3b":"code","debdadce":"code","2fdff6a3":"code","e0356a6b":"code","a84a4b39":"code","a497e490":"code","5f599f95":"code","76d11c65":"code","57c69c3e":"code","14ae52ab":"code","99211fad":"code","3db77e62":"code","cbd3eaf3":"code","b99148a9":"code","1bf2cb1f":"code","93c7cc38":"code","dbf11585":"code","2c264bcb":"code","2dc68d00":"code","543a18eb":"code","c25bcbdd":"code","8f3962e7":"code","c35b2ee0":"code","50a5c05f":"code","15dbd7b2":"code","639ac3c1":"code","e7ac6529":"code","f1d42623":"code","a6e5ef17":"code","c1081be1":"code","3e3e049c":"code","fa2a43f5":"code","cb38a681":"code","907a30ca":"markdown","be6795e1":"markdown","e1c14a0c":"markdown","d5b2c546":"markdown","d1e07c88":"markdown","4332647c":"markdown","bd54eea4":"markdown","5faa5706":"markdown","d361e74e":"markdown","079ac2dc":"markdown","e78e35ee":"markdown","1f8cf6ee":"markdown","c78c6806":"markdown","76c7e86a":"markdown","0b57e69d":"markdown","5693bee6":"markdown","b104a93c":"markdown","6c9de611":"markdown","155b9530":"markdown","38660c02":"markdown","a5aa6a14":"markdown","56f121fe":"markdown","73213fa7":"markdown","e4a89781":"markdown","840d3039":"markdown","850841ab":"markdown","63eea045":"markdown","30b4118f":"markdown","ebfad20f":"markdown","884b1bde":"markdown","905c4197":"markdown","35f78961":"markdown","2ce0b3af":"markdown","edd37f43":"markdown","bfd19543":"markdown","dbf370a6":"markdown","d9337a9c":"markdown","bb73f1b2":"markdown","5f756461":"markdown","6789c48d":"markdown","71109b8c":"markdown","219b92c8":"markdown"},"source":{"0a2d7520":"import pandas as pd\nimport numpy as np\n","56fe6dab":"df=pd.read_csv('..\/input\/diabetes.csv')","770f2e49":"missing_data=df.isnull()\nmissing_data.head(5)","c56b001b":"missing_data.sum()","cd181a1d":"df.head(10)","a47ff697":"df.shape","93ad352a":"df.info()","45fa1c19":"#On classification problems you need to know how balanced the class values are.( This is an example)\ndf.groupby('Outcome').size() ","dfb8d004":"# We can analyze all the data set \ndf.describe()","b129131d":"#We can analyze any colums separate\n#df['MODELYEAR'].describe()","6abd3d00":"# When we have categorical values in the data set, we can create a table and sumarize it\n#df.describe(include=['O'])","11070778":"df.corr()","7ea11f66":"corr_matrix= df.corr()","478bad2c":"#To check a correlation with our target\n\ncorr_matrix['Outcome'].sort_values(ascending=False)\n","c1566bef":"import seaborn as sns","3dba6e7c":"sns.heatmap(df.corr(), vmin=-1, vmax=1.0, annot=True)","791afb03":"df.skew()","68cd1642":"from matplotlib import pyplot as plt\ndf.hist(bins=10, figsize=(20,15))\nplt.show()","607dc61b":"df.plot(kind='box', subplots=True, layout=(3,3), sharex=False, sharey=False, figsize=(10,15))\nplt.show()","0c4985b5":"from pandas.tools.plotting import scatter_matrix\n","5405b81b":"\nscatter_matrix(df,figsize=(20,20))\nplt.show()","0a2fa325":"missing_data= df.isnull()","22864042":"missing_data.head(5)","4057581f":"missing_data.sum()","2ab4aea5":"for column in missing_data.columns.values.tolist():\n    print(column)\n    print(missing_data[column].value_counts())\n    print(\"--------------------------------\")","19b647b3":"#Finding the porcentage of  missing data\nround(((missing_data.sum()\/len(missing_data))*100), 4)","f74cf400":"# q = df.quantile(0.99)\n#df [df > q]","906c4a3b":"#Lets check the types \ndf.dtypes\n","debdadce":"#df = pd.get_dummies(df, prefix_sep='_', drop_first=True)","2fdff6a3":"df.shape","e0356a6b":"X=df.drop('Outcome',axis=1)\ny=df['Outcome']","a84a4b39":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n#apply SelectKBest class to extract top 10 best features\nbestfeatures = SelectKBest(score_func=chi2, k='all')\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(10,'Score'))  #print 10 best features","a497e490":"import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesClassifier()\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()","5f599f95":"df.columns","76d11c65":"#X_train=\n#y_train=\n#X_test=\n\nX=df.drop(['Outcome'], axis=1).values\ny=df['Outcome'].values\n","57c69c3e":"#from sklearn import preprocessing\n#X= preprocessing.StandardScaler().fit(X).transform(X.astype(float))","14ae52ab":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0, 1))\nrescaledX = scaler.fit_transform(X)\n","99211fad":"#X_test= preprocessing.StandardScaler().fit(X_test).transform(X_test.astype(float))","3db77e62":"\n#X_train.shape, y_train.shape, X_test.shape","cbd3eaf3":"#Load Libraries\nfrom sklearn.linear_model import LinearRegression \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost.sklearn import XGBClassifier  \nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn import preprocessing\nfrom scipy.stats import uniform\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import RandomizedSearchCV","b99148a9":"#Linear Regression \nkfold = KFold(n_splits=10, random_state=42,)\nlin_reg= LinearRegression()\nresults_linreg= cross_val_score(lin_reg, X, y, cv=kfold)\nprint('Estimate accuracy',results_linreg.mean())","1bf2cb1f":"\n# Logistic Regression\nkfold = KFold(n_splits=10, random_state=42)\nlogreg = LogisticRegression(solver='lbfgs',max_iter=10000)\nresults_logreg = cross_val_score(logreg, X, y, cv=kfold,scoring='accuracy')\nprint('Estimate accuracy',results_logreg.mean())","93c7cc38":"# Support Vector Machines\nkfold = KFold(n_splits=10, random_state=42)\nsvc = SVC(C=0.01, cache_size=200, class_weight=None, coef0=0.0,\n   decision_function_shape='ovr', degree=3, gamma=1, kernel='linear',\n   max_iter=-1, probability=False, random_state=None, shrinking=True,\n   tol=0.001, verbose=False)\nresults_svc = cross_val_score(svc, X, y, cv=kfold,scoring='accuracy')\nprint('Estimate accuracy',results_svc.mean())","dbf11585":"kfold = KFold(n_splits=10, random_state=42)\nknn = KNeighborsClassifier(n_neighbors = 3)\nresults_knn = cross_val_score(knn, X, y, cv=kfold)\nprint('Estimate accuracy',results_knn.mean())","2c264bcb":"# Gaussian Naive Bayes\nkfold = KFold(n_splits=10, random_state=42)\ngaussian = GaussianNB()\nresults_gaussian = cross_val_score(gaussian, X, y, cv=kfold)\nprint('Estimate accuracy',results_gaussian.mean())","2dc68d00":"# Perceptron\nkfold = KFold(n_splits=10, random_state=42)\nperceptron = Perceptron(max_iter=1000,tol=1e-3)\nresults_perceptron = cross_val_score(perceptron, X, y, cv=kfold,scoring='accuracy')\nprint('Estimate accuracy',results_perceptron.mean())","543a18eb":"# Linear SVC\nkfold = KFold(n_splits=10, random_state=42)\nlinear_svc = LinearSVC(max_iter=1000)\nresults_linearsvc= cross_val_score(linear_svc, X, y, cv=kfold, scoring='accuracy')\nprint('Estimate accuracy',results_linearsvc.mean())","c25bcbdd":"# Stochastic Gradient Descent\nkfold = KFold(n_splits=10, random_state=42)\nsgd = SGDClassifier(max_iter=1000,tol=1e-3)\nresults_sgd = cross_val_score(sgd, X, y, cv=kfold, scoring='accuracy')\nprint('Estimate accuracy',results_sgd.mean())","8f3962e7":"# Decision Tree\nkfold = KFold(n_splits=10, random_state=42)\ndecision_tree = DecisionTreeClassifier()\nresults_decisiontree = cross_val_score(decision_tree, X, y, cv=kfold,scoring='accuracy')\nprint('Estimate accuracy',results_decisiontree.mean())","c35b2ee0":"# Random Forest\nkfold = KFold(n_splits=10, random_state=42)\nrandom_forest = RandomForestClassifier(n_estimators=100)\nresults_randomforest = cross_val_score(decision_tree, X, y, cv=kfold,scoring='accuracy')\nprint('Estimate accuracy',results_randomforest.mean())","50a5c05f":"#Linear Discriminant Analysis\nkfold = KFold(n_splits=10, random_state=42)\nclf = LinearDiscriminantAnalysis()\nresults_clf = cross_val_score(clf, X, y, cv=kfold, scoring='accuracy')\nprint('Estimate accuracy',results_clf.mean())","15dbd7b2":"# Ada Boost Classifier\nkfold = KFold(n_splits=10, random_state=42)\nAB = AdaBoostClassifier()\nresults_AB = cross_val_score(AB, X, y, cv=kfold, scoring='accuracy')\nprint('Estimate accuracy',results_AB.mean())\n\n#AdaBoostClassifier(base_estimator=None, n_estimators=50, learning_rate=1.0, algorithm=\u2019SAMME.R\u2019, random_state=None)\n","639ac3c1":"# Gradient Boosting Classifier\nkfold = KFold(n_splits=10, random_state=42)\nGBC = GradientBoostingClassifier()\nresults_GBC = cross_val_score(GBC, X, y, cv=kfold, scoring='accuracy')\nprint('Estimate accuracy',results_GBC.mean())\n#GradientBoostingClassifier(loss=\u2019deviance\u2019, learning_rate=0.1, n_estimators=100, subsample=1.0, criterion=\u2019friedman_mse\u2019, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, presort=\u2019auto\u2019, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001)","e7ac6529":"#ExtraTreesClassifier\nkfold = KFold(n_splits=10, random_state=42)\nETC=ExtraTreesClassifier(n_estimators=100)\nresults_ETC = cross_val_score(ETC, X, y, cv=kfold, scoring='accuracy')\nprint('Estimate accuracy',results_ETC.mean())\n#ExtraTreesClassifier(n_estimators=\u2019warn\u2019, criterion=\u2019gini\u2019, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=\u2019auto\u2019, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=False, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None)","f1d42623":"#XGBClassifier(objective\nxgbs = XGBClassifier(objective=\"binary:logistic\", random_state=42)\nresults_xgbs = cross_val_score(xgbs, X, y, cv=kfold, scoring='accuracy')\nprint('Estimate accuracy',results_xgbs.mean())","a6e5ef17":"models = pd.DataFrame({\n    'Model': ['Linear Regression','Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree','Linear Discriminant Analysis','Ada Boost Classifier','Gradient Boosting Classifier','Extra TreesClassifier','XGB Classifier'],\n    'Score': [results_linreg.mean(),results_logreg.mean(),results_svc.mean(),results_knn.mean(),results_gaussian.mean(),results_perceptron.mean(),results_linearsvc.mean(),results_sgd.mean(),results_decisiontree.mean(),results_randomforest.mean(),results_clf.mean(),results_AB.mean(),results_GBC.mean(),results_ETC.mean(),results_xgbs.mean()]})\nmodels.sort_values(by='Score', ascending=False)","c1081be1":"def svc_param_selection(X, y, nfolds):\n    Cs = [0.001, 0.01, 0.1, 1, 10]\n    gammas = [0.001, 0.01, 0.1, 1]\n    param_grid = {'C': Cs, 'gamma' : gammas}\n    grid_search = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=nfolds)\n    grid_search.fit(X, y)\n    grid_search.best_params_\n    return grid_search.best_score_, grid_search.best_params_ ,grid_search.best_estimator_","3e3e049c":"svc_param_selection(X, y, 20)","fa2a43f5":"def svc_param_selection2(X,y,nfolds):\n    Cs = [0.001, 0.01, 0.1, 1, 10]\n    gammas = [0.001, 0.01, 0.1, 1]\n    kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n    param_dist = {'C': Cs, 'gamma' : gammas, 'kernel': kernels}\n    rand = RandomizedSearchCV(SVC(), param_dist, cv=nfolds, scoring='accuracy', n_iter=10, random_state=42)\n    rand.fit(X,y)\n    rand.best_score_\n    rand.best_params_\n    rand.best_estimator_\n    return  rand.best_score_, rand.best_params_ ,rand.best_estimator_","cb38a681":"svc_param_selection2(X,y,20)","907a30ca":"It is better to use some data visualization to get a better idea.","be6795e1":"----------------------------------------------------------------------------","e1c14a0c":"Understand Your Data With Descriptive Statistics and Visualizations","d5b2c546":"---------------------------------------------------------","d1e07c88":"---------------------------------","4332647c":"-----------------------------------------","bd54eea4":"Most Machine Learning algorithms cannot work with missing features, so let\u2019s create\na few functions to take care of them. You noticed earlier that the total_bedrooms\nattribute has some missing values, so let\u2019s fix this. You have three options:\n\n\u2022 Get rid of the corresponding districts.\n\n\u2022 Get rid of the whole attribute.\n\n\u2022 Set the values to some value (zero, the mean, the median, etc.).\n\nYou can accomplish these easily using DataFrame\u2019s dropna(), drop(), and fillna()\nmethods:\n\nhousing.dropna(subset=[\"total_bedrooms\"]) # option 1\n\nhousing.drop(\"total_bedrooms\", axis=1) # option 2\n\nmedian = housing[\"total_bedrooms\"].median()\n\nhousing[\"total_bedrooms\"].fillna(median) # option 3\n\nIf you choose option 3, you should compute the median value on the training set, and\nuse it to fill the missing values in the training set, but also don\u2019t forget to save the\nmedian value that you have computed. You will need it later to replace missing values\nin the test set when you want to evaluate your system, and also once the system goes\nlive to replace missing values in new data.","5faa5706":"## Model, predict and solve\n\nNow we are ready to train a model and predict the required solution. There are 60+ predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning plus Classification and Regression, we can narrow down our choice of models to a few. These include:\n\n- Logistic Regression\n- KNN or k-Nearest Neighbors\n- Support Vector Machines\n- Naive Bayes classifier\n- Decision Tree\n- Random Forrest\n- Perceptron\n- Artificial neural network\n- RVM or Relevance Vector Machine\n- LinearDiscriminantAnalysis\n- AdaBoostClassifier\n- GradientBoostingClassifier\nExtraTreesClassifier","d361e74e":"This is useful to know, because some machine learning algorithms like linear and logistic regression can have poor performance if there are highly correlated input variables in your data.","079ac2dc":"-------------------------------------------","e78e35ee":"Firts Import the lybraries and data sets","1f8cf6ee":"----------------------------------------------------","c78c6806":"Lest check how the missing data behave in the data set","76c7e86a":"It is better to use some data visualization to get a better idea.","0b57e69d":"The skew result can show us a positive(right) or negative (left) skew. Values closer to zero show less skew","5693bee6":"1- Take a peek at your row data","b104a93c":"2- Data Cleaning ","6c9de611":"---------------------","155b9530":"2-Feature Importance","38660c02":"One Hot Encoding ","a5aa6a14":"3-Handling Text and Categorical Attributes ","56f121fe":"---------------------------------------------------------------------------------------------------","73213fa7":"1-Univariete Selection","e4a89781":"Load the test,train data.","840d3039":"Scatter Plot Matrix","850841ab":"Summarize the insights  from the results.","63eea045":"Class Distribution (Classification Only)","30b4118f":"Optional - Lets see our outliers in the data set ","ebfad20f":"### Model evaluation\n\n","884b1bde":"----------------------------------------------------------------------","905c4197":"5- Descriptive Statistics","35f78961":"Data Cleaning and Preparation for Machine Learning ","2ce0b3af":"Select the atribites and save the data frame that is ready for use with the machine learning model.","edd37f43":"----------------------------------------------------","bfd19543":"You must to undesrtand your data in orden to get the best results.\nSeven recipes that we can use to get this done.\n\n1-Take a peek at your row data\n\n2-Review the dimensions of your data\n\n3-Review the data type of attributes in your data\n\n4-Summarize the distribution of instances across classes in your dataset.\n\n5-Summarize your data using descriptive statistics.\n\n6-Understand the relationships in your data using correlations.\n\n7-Review the skew of the distributions of each attribute.\n","dbf370a6":"Feature Selection For Machine Learning ","d9337a9c":"It Needs to be done only with numerical values, in case that we have categorical values we can create dummies values or transform the data frame.\n","bb73f1b2":"(2,3) -Dimmension of Your Data and Data Types","5f756461":"6-Correlation Between Attributes","6789c48d":"----------------------------------------------------------------","71109b8c":"1- Evaluation for Missing Data","219b92c8":"7-Skew of Univariate Distributions"}}