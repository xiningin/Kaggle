{"cell_type":{"9478914d":"code","5c3dc56c":"code","6ec6e321":"code","05da1e9a":"code","ee8a1e5a":"code","0042333d":"code","c4452964":"code","df93bd31":"code","4adeb5a4":"code","2eaad02f":"code","191df044":"code","271d4bb2":"code","caf2ac90":"code","11f6ff82":"code","e498643a":"code","ed1aaa18":"code","31dd13f0":"code","18a6855f":"code","dbe64e6f":"code","de9c1fef":"code","dfc9dd2a":"code","a68d0afa":"code","fad6226b":"code","7887807b":"code","493faad4":"code","6c54e024":"code","35566bc0":"code","ecfc2b8b":"code","8d3876cf":"code","7d581fdc":"code","d885964e":"code","a21b0e28":"code","45c4bfcb":"code","06a668a7":"code","539fa0ff":"code","5f703fc0":"code","8c0d01b4":"code","20ce6f2c":"code","dca52d26":"code","bda63ca7":"code","c72ebc63":"code","6f8ca9ff":"code","a6fa961a":"code","9610df7f":"code","8074fec8":"code","3123534f":"code","d2d0a4d5":"code","6d8ba8e6":"code","ea21cd85":"code","65349837":"code","6d1757f2":"code","994437f7":"code","665b405c":"code","9593a6f2":"code","d3548ce1":"code","aba2c901":"code","2f6a3512":"code","b8126f7d":"code","3b3e509b":"code","4ddbc49c":"code","3609bb97":"code","60ed5af9":"code","f80dc8d3":"code","fa43f8b1":"code","667e89c8":"code","ddd7fc75":"markdown","63b7ed81":"markdown","bf273967":"markdown","36cde03b":"markdown","0c82d196":"markdown","4300d667":"markdown","3a509ea7":"markdown","25127756":"markdown","1c1c8dcb":"markdown","f5391094":"markdown","1f24ac71":"markdown","8dbdd410":"markdown","ea9fa5db":"markdown","97562fc4":"markdown"},"source":{"9478914d":"import os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.image import imread\n%matplotlib inline","5c3dc56c":"my_data_dir = '..\/input\/files1\/Malaria Cells'","6ec6e321":"# CONFIRM THAT THIS REPORTS BACK 'test', and 'train'\nos.listdir(my_data_dir) ","05da1e9a":"test_path = my_data_dir+'\/testing_set'\ntrain_path = my_data_dir+'\/training_set'","ee8a1e5a":"os.listdir(test_path)","0042333d":"os.listdir(train_path)","c4452964":"os.listdir(train_path+'\/Parasitized')[0]","df93bd31":"para_cell = train_path+'\/Parasitized'+'\/C100P61ThinF_IMG_20150918_144104_cell_162.png'","4adeb5a4":"para_img= imread(para_cell)","2eaad02f":"plt.imshow(para_img)","191df044":"para_img.shape","271d4bb2":"unifected_cell_path = train_path+'\/Uninfected\/'+os.listdir(train_path+'\/\/Uninfected')[0]\nunifected_cell = imread(unifected_cell_path)\nplt.imshow(unifected_cell)","caf2ac90":"len(os.listdir(train_path+'\/Parasitized'))","11f6ff82":"len(os.listdir(train_path+'\/Uninfected'))","e498643a":"unifected_cell.shape","ed1aaa18":"para_img.shape","31dd13f0":"dim1 = []\ndim2 = []\nfor image_filename in os.listdir(test_path+'\/Uninfected'):\n    \n    img = imread(test_path+'\/Uninfected'+'\/'+image_filename)\n    d1,d2,colors = img.shape\n    dim1.append(d1)\n    dim2.append(d2)","18a6855f":"sns.jointplot(dim1,dim2)","dbe64e6f":"np.mean(dim1)","de9c1fef":"np.mean(dim2)","dfc9dd2a":"image_shape = (130,130,3)","a68d0afa":"from tensorflow.keras.preprocessing.image import ImageDataGenerator","fad6226b":"help(ImageDataGenerator)","7887807b":"image_gen = ImageDataGenerator(rotation_range=20, # rotate the image 20 degrees\n                               width_shift_range=0.10, # Shift the pic width by a max of 5%\n                               height_shift_range=0.10, # Shift the pic height by a max of 5%\n                               rescale=1\/255, # Rescale the image by normalzing it.\n                               shear_range=0.1, # Shear means cutting away part of the image (max 10%)\n                               zoom_range=0.1, # Zoom in by 10% max\n                               horizontal_flip=True, # Allo horizontal flipping\n                               fill_mode='nearest' # Fill in missing pixels with the nearest filled value\n                              )","493faad4":"plt.imshow(para_img)","6c54e024":"plt.imshow(image_gen.random_transform(para_img))","35566bc0":"plt.imshow(image_gen.random_transform(para_img))","ecfc2b8b":"image_gen.flow_from_directory(train_path)","8d3876cf":"image_gen.flow_from_directory(test_path)","7d581fdc":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D\nimport warnings\nwarnings.filterwarnings(\"ignore\")","d885964e":"# Initialising the CNN\nclassifier = Sequential()\n\n#1 - Convolution\nclassifier.add(Conv2D(filters=32, kernel_size=(3,3),input_shape=image_shape, activation='relu',))\nclassifier.add(MaxPooling2D(pool_size=(2, 2)))\n\n#2 - Pooling\n# Hidden Layer 1\nclassifier.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=image_shape, activation='relu',))\nclassifier.add(MaxPooling2D(pool_size=(2, 2)))\n\n# Adding a second convolution layer\n# relu turns negative values in images to 0\nclassifier.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=image_shape, activation='relu',))\nclassifier.add(MaxPooling2D(pool_size=(2, 2)))\n\n#3 - Flattening\n# converts the matrix in a singe array\nclassifier.add(Flatten())\n\n\nclassifier.add(Dense(128))\nclassifier.add(Activation('relu'))\n\n\n# Dropouts help reduce overfitting by randomly turning neurons off during training.\n# Here we say randomly turn off 50% of neurons.\nclassifier.add(Dropout(0.5))\n\n#4 - Full Connection\n# 128 is the final layer of outputs & from that 1 will be considered.\n# Last layer, remember its binary so we use sigmoid\nclassifier.add(Dense(1))\nclassifier.add(Activation('sigmoid'))\n\nclassifier.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","a21b0e28":"classifier.summary()","45c4bfcb":"from tensorflow.keras.callbacks import EarlyStopping","06a668a7":"early_stop = EarlyStopping(monitor='val_loss',patience=2)","539fa0ff":"batch_size = 16","5f703fc0":"train_image_gen = image_gen.flow_from_directory(train_path,\n                                               target_size=image_shape[:2],\n                                                color_mode='rgb',\n                                               batch_size=batch_size,\n                                               class_mode='binary')","8c0d01b4":"test_image_gen = image_gen.flow_from_directory(test_path,\n                                               target_size=image_shape[:2],\n                                               color_mode='rgb',\n                                               batch_size=batch_size,\n                                               class_mode='binary',shuffle=False)","20ce6f2c":"train_image_gen.class_indices","dca52d26":"results = classifier.fit_generator(train_image_gen,epochs=20,\n                              validation_data=test_image_gen,\n                             callbacks=[early_stop])","bda63ca7":"from tensorflow.keras.models import load_model\nclassifier.save('malaria_detector.h5')","c72ebc63":"losses = pd.DataFrame(classifier.history.history)","6f8ca9ff":"losses[['loss','val_loss']].plot()","a6fa961a":"classifier.metrics_names","9610df7f":"classifier.evaluate_generator(test_image_gen)","8074fec8":"from tensorflow.keras.preprocessing import image","3123534f":"pred_probabilities = classifier.predict_generator(test_image_gen)","d2d0a4d5":"pred_probabilities","6d8ba8e6":"test_image_gen.classes","ea21cd85":"predictions = pred_probabilities > 0.5","65349837":"# Numpy can treat this as True\/False for us\npredictions","6d1757f2":"from sklearn.metrics import classification_report,confusion_matrix","994437f7":"print(classification_report(test_image_gen.classes,predictions))","665b405c":"confusion_matrix(test_image_gen.classes,predictions)","9593a6f2":"# Your file path will be different!\npara_cell","d3548ce1":"my_image = image.load_img(para_cell,target_size=image_shape)","aba2c901":"my_image","2f6a3512":"type(my_image)","b8126f7d":"my_image = image.img_to_array(my_image)","3b3e509b":"type(my_image)","4ddbc49c":"my_image.shape","3609bb97":"my_image = np.expand_dims(my_image, axis=0)","60ed5af9":"my_image.shape","f80dc8d3":"classifier.predict(my_image)","fa43f8b1":"train_image_gen.class_indices","667e89c8":"test_image_gen.class_indices","ddd7fc75":"![](https:\/\/www.mayoclinic.org\/-\/media\/kcms\/gbs\/patient-consumer\/images\/2013\/08\/26\/10\/13\/ds00475_im00175_mcdc7_malaria_transmitthu_jpg.jpg)","63b7ed81":"**Let's find out the average dimensions of these images.**","bf273967":"### ORIGINAL DATA SOURCE:\n\n>The dataset contains 2 folders - Infected - Uninfected\nAnd a total of 27,558 images.\n\n>Acknowledgements\nThis Dataset is taken from the official NIH Website: https:\/\/ceb.nlm.nih.gov\/repositories\/malaria-datasets\/ \n\n >**Note: We will be dealing with real image files, NOT numpy arrays. Which means a large part of this process will be learning how to work with and deal with large groups of image files. This is too much data to fit in memory as a numpy array, so we'll need to feed it into our model in batches. **","36cde03b":"# Creating the Model","0c82d196":"# Evaluating the Model","4300d667":"> I used 0.5 or 50% dropping of neurons while training to avoid overfitting problem (dropout)","3a509ea7":">Thank You","25127756":"## Early Stopping","1c1c8dcb":"# Predicting on an Image","f5391094":"## Training the Model","1f24ac71":"### Generating many manipulated images from a directory\n\n\nIn order to use .flow_from_directory, you must organize the images in sub-directories. This is an absolute requirement, otherwise the method won't work. The directories should only contain images of one class, so one folder per class of images.\n\nStructure Needed:\n\n* Image Data Folder\n    * Class 1\n        * 0.jpg\n        * 1.jpg\n        * ...\n    * Class 2\n        * 0.jpg\n        * 1.jpg\n        * ...\n    * ...\n    * Class n","8dbdd410":"# **Detection of Malaria cells using Convolution Neural Network**","ea9fa5db":"**Let's check how many images there are.**","97562fc4":"## Preparing the Data for the model\n\n>There is too much data for us to read all at once in memory. We can use some built in functions in Keras to automatically process the data, generate a flow of batches from a directory, and also manipulate the images.\n\n### Image Manipulation\n\n>Its usually a good idea to manipulate the images with rotation, resizing, and scaling so the model becomes more robust to different images that our data set doesn't have. We can use the **ImageDataGenerator** to do this automatically for us. Check out the documentation for a full list of all the parameters you can use here!"}}