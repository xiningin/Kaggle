{"cell_type":{"4096bf47":"code","f9fca093":"code","e3adf98e":"code","77ee9135":"code","21bfd8c0":"code","fe99a9f2":"code","6bf96e26":"code","86d38ad8":"code","97669fb9":"code","c4077a98":"code","fed72d5f":"code","9e230c5c":"code","27a9c59b":"code","9fddf017":"code","5272de56":"code","2bd19900":"code","fe7f9d3f":"code","6c36ccfc":"code","51d82b5d":"code","5c026be8":"code","191a6957":"code","d9faf452":"code","1d4a4069":"code","b45587b4":"code","dd8a181e":"code","8c2589c0":"code","36bd0ae9":"code","265d0c80":"code","34632f02":"code","b426e49f":"code","302da807":"code","4780eeb1":"code","f17de647":"code","ee655694":"code","26c7e638":"code","1a543550":"code","b20999c1":"code","2d0bb5f3":"code","20ff4ee4":"code","e190ce42":"code","c0b927e6":"code","e69d819f":"code","bcd8fc04":"code","493c74ab":"code","92dab93e":"code","8b78579e":"code","059f2953":"code","c81fad8b":"code","09db1c8b":"code","7501f40e":"code","7df3395c":"code","75a33767":"code","83ae1d0d":"code","bc795cc0":"code","004064a6":"code","b4e3fda7":"code","ac629dee":"code","78f76e56":"code","d0289882":"code","64e7d811":"code","8e35271e":"code","6994b68c":"code","afce44f5":"code","af91da8e":"code","5c86f539":"code","fdac2434":"code","86d74d58":"code","fcc396d5":"code","b98a5893":"code","2590b573":"code","8f994687":"code","046d8694":"code","5dfbc913":"code","4c5034cf":"code","203f392c":"code","87a06f64":"code","e55d9f26":"code","10eb680d":"code","703cb019":"code","67a91408":"code","c92d0da6":"code","b8a65a87":"code","3c46c624":"markdown","7ed37527":"markdown","0c92e8f5":"markdown","1ea7603e":"markdown","4a4738df":"markdown","607ebce2":"markdown","688da658":"markdown","e6222fc3":"markdown","78346701":"markdown","29c05802":"markdown","0527da6b":"markdown","8aaac3fc":"markdown","4d6094fc":"markdown","52281e07":"markdown","6a167254":"markdown","ecee55d8":"markdown","279a08ef":"markdown","7bf71486":"markdown","33cad81f":"markdown","bd62b7f7":"markdown","19ee7c9e":"markdown","13d96a9b":"markdown","0d006766":"markdown","eaada47b":"markdown","c06f3496":"markdown","a7b6dd05":"markdown","d2ff5dfc":"markdown","93896f3b":"markdown","1abfe6db":"markdown","ece10412":"markdown","3181dc65":"markdown","ab50d5ee":"markdown","afb2bee1":"markdown","bb8de445":"markdown","b0a54f54":"markdown","bb82911b":"markdown","3aa5f81c":"markdown","a55a0090":"markdown","73d1dafb":"markdown","a36c22db":"markdown","c5b667b7":"markdown","460db406":"markdown","1ff59168":"markdown","3e2e81fa":"markdown","ba40da89":"markdown","0eba3d68":"markdown","61bdde1b":"markdown","0f000366":"markdown","b5bb5f78":"markdown","b7835996":"markdown","72d92333":"markdown","d5e9f52c":"markdown","b968c6d9":"markdown","0a98ced9":"markdown","3215754e":"markdown","f5719e9c":"markdown","1f69421b":"markdown","6339db74":"markdown","ea4f9aca":"markdown","6c04be75":"markdown","662f3d98":"markdown","f4a6f427":"markdown","2317efe3":"markdown","59d57e64":"markdown","7ccc6901":"markdown"},"source":{"4096bf47":"%matplotlib inline\n\nimport pandas as pd\nimport missingno as mno\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression","f9fca093":"df = pd.read_csv(\"..\/input\/weather-dataset-rattle-package\/weatherAUS.csv\")","e3adf98e":"df.head()","77ee9135":"df.shape","21bfd8c0":"df.info()","fe99a9f2":"df.isnull().sum()","6bf96e26":"mno.matrix(df, figsize = (15, 6))","86d38ad8":"numerical_df_cols = df.columns[df.dtypes != object]","97669fb9":"numerical_df_cols","c4077a98":"categorical_df_cols = df.columns[df.dtypes == object]","fed72d5f":"categorical_df_cols","9e230c5c":"null_cols_df =  df[df.columns[df.isnull().any()]].copy()","27a9c59b":"null_cols_df","9fddf017":"def draw_histplot_between_num_features():\n    fig, axes = plt.subplots(4, len(numerical_df_cols) \/\/ 4, figsize=(50, 50))\n    row_idx = 0\n    col_idx = 0\n    for col in numerical_df_cols:\n        if col_idx > 1 and col_idx % 3 == 1:\n            row_idx += 1\n            col_idx = 0\n        sns.histplot(x=col, data=df, ax = axes[row_idx, col_idx])\n        col_idx += 1\n    plt.show()","5272de56":"draw_histplot_between_num_features()","2bd19900":"def draw_boxplot_for_num_features():\n    fig, axes = plt.subplots(4, len(numerical_df_cols) \/\/ 4, figsize=(50, 50))\n    row_idx = 0\n    col_idx = 0\n    for col in numerical_df_cols:\n        if col_idx > 1 and col_idx % 3 == 1:\n            row_idx += 1\n            col_idx = 0\n        sns.boxplot(x=col, data=df, ax = axes[row_idx, col_idx])\n        col_idx += 1\n    plt.show()","fe7f9d3f":"draw_boxplot_for_num_features()","6c36ccfc":"sns.pairplot(df[numerical_df_cols])","51d82b5d":"numberical_null_value_cols = null_cols_df.columns[null_cols_df.dtypes != object]","5c026be8":"numberical_null_value_cols","191a6957":"def simple_random_imputation(df):\n    dataset = df.copy()\n    for feature in numberical_null_value_cols:\n        dataset[feature] = dataset[feature]\n        number_missing = dataset[feature].isnull().sum()\n        observed_values = dataset.loc[dataset[feature].notnull(), feature]\n        dataset.loc[df[feature].isnull(), feature] = np.random.choice(observed_values, number_missing, replace = True)\n            \n    return dataset\n       ","d9faf452":"def deterministic_regression_imputation(df, feature, correlated_cols):\n    dataset = df.copy()\n    dataset[\"det_\" + feature] = df_imp[feature]\n    parameters = list(correlated_cols) \n\n    #linear Regression model to estimate the missing data\n    model = LinearRegression()\n    model.fit(X = df_imp[parameters], y = df_imp[feature])\n    dataset.loc[dataset[feature].isnull(), \"det_\" + feature] = model.predict(df_imp[parameters])[dataset[feature].isnull()]\n    \n    dataset[feature] = dataset[\"det_\" + feature]\n    dataset.drop(\"det_\" + feature, axis = 1, inplace = True)\n    return dataset\n\n        ","1d4a4069":"df_imp = simple_random_imputation(df)","b45587b4":"df_imp.columns","dd8a181e":"df_imp.isnull().sum()","8c2589c0":"def find_correlated_cols(df):\n    dataset = df.copy()\n    corr = dataset.corr()\n    corr_col_arr = []\n    for col in numberical_null_value_cols:\n        correlated_cols = []\n\n        # find correlated columns\n        for rel_col, rel_col_corr in corr[col].iteritems():\n            if abs(rel_col_corr) >= 0.2 and abs(rel_col_corr) <= 0.9 and \"_imp\" not in rel_col  and \"det_\" not in rel_col:\n                correlated_cols.append(rel_col)\n        corr_col_arr.append({'col':col, 'correlated_cols': correlated_cols})\n    return corr_col_arr\n    ","36bd0ae9":"corr_col_arr = find_correlated_cols(df_imp)","265d0c80":"for v in corr_col_arr:\n    print(\"Column:\", v['col'])\n    print(\"Correlated Column:\", v['correlated_cols'])\n    print(\"\\n\")\n","34632f02":"def draw_scatter_plot(x, correlated_cols, df):\n    fig, axes = plt.subplots(1, len(correlated_cols), figsize=(50, 8))\n    print(\"\\n\") \n    fig.suptitle(x + ' vs Correlated Columns:' + ', '.join(correlated_cols))\n    for idx, rel_col in enumerate(correlated_cols):\n        sns.scatterplot(x = x, y = rel_col, data= df, ax = axes[idx])\n    plt.show()","b426e49f":"def draw_scatter_plot_between_correlated_feature(df, corr_col_arr):\n    dataset = df.copy()\n    for v in corr_col_arr:\n        draw_scatter_plot(v['col'], v['correlated_cols'], dataset)\n        ","302da807":"draw_scatter_plot_between_correlated_feature(df_imp, corr_col_arr)","4780eeb1":"def appy_deterministic_imp(df):\n    dataset = df.copy()\n    for v in corr_col_arr:\n        print(\"Column:\", v['col'])\n        print(\"Correlated Column:\",v['correlated_cols'])\n        if len(v['correlated_cols']) > 0: \n            dataset = deterministic_regression_imputation(dataset, v['col'], v['correlated_cols'])\n            print(dataset[v['col']].isnull().sum())\n        else:\n            print(\"No Correlated Column for\", v['col'])\n        print(\"\\n\")\n    return dataset","f17de647":"df_det = appy_deterministic_imp(df)","ee655694":"df_det.columns","26c7e638":"categorical_null_value_cols = null_cols_df.columns[null_cols_df.dtypes == object]","1a543550":"categorical_null_value_cols","b20999c1":"for col in categorical_null_value_cols:\n    print(\"Unique value of column:\", col)\n    print(df[col].unique())\n    print(\"\\n\")","2d0bb5f3":"def impute_categorical_variable(df, cat_features):\n    for cat_feature in cat_features:\n        print(\"Imputing Column:\", cat_feature)\n        df[cat_feature] = df[cat_feature].fillna((df.groupby('Location')[cat_feature].transform(lambda x:  next(iter(x.mode()), np.nan))))\n        print(df[[cat_feature]].isnull().sum())\n        print(\"\\n\")","20ff4ee4":"impute_categorical_variable(df_det, categorical_null_value_cols)","e190ce42":"df_det['WindGustDir']=df_det['WindGustDir'].fillna(df_det['WindGustDir'].mode().max())","c0b927e6":"df_det['WindGustDir'].isnull().sum()","e69d819f":"df_det['RainToday'] = df_det['RainToday'].apply(lambda x: 1 if x == 'Yes' else 0)","bcd8fc04":"df_det['RainToday'].value_counts()","493c74ab":"df_det['RainTomorrow'] = df_det['RainTomorrow'].apply(lambda x: 1 if x == 'Yes' else 0)","92dab93e":"df_det['RainTomorrow'].value_counts()","8b78579e":"df_det = pd.get_dummies(data=df_det, columns=['WindGustDir','WindDir9am','WindDir3pm'])","059f2953":"df_det.columns","c81fad8b":"df_det.isnull().sum()","09db1c8b":"\nfrom sklearn.preprocessing import StandardScaler\n\ndef scale_dataset(df):\n    dataset = df.copy()\n    dataset.drop(['Date', 'Location', 'RainToday', 'RainTomorrow'], axis = 1, inplace = True)\n    scaler = StandardScaler()\n    return pd.DataFrame(scaler.fit_transform(dataset),columns = dataset.columns)","7501f40e":"df_det_scaled = pd.merge(scale_dataset(df_det), df_det[['Date', 'Location', 'RainToday', 'RainTomorrow']],left_index=True, right_index=True )","7df3395c":"df_det_scaled.columns","75a33767":"df_det_scaled.head()","83ae1d0d":"gp_min_temp = df_det.groupby('Location')['MinTemp'].agg(['min', 'max']).reset_index()","bc795cc0":"gp_min_temp.sort_values('min').head(3)","004064a6":"gp_min_temp.sort_values('max', ascending=False).head(3)","b4e3fda7":"gp_max_temp = df_det.groupby('Location')['MaxTemp'].agg(['min', 'max']).reset_index()","ac629dee":"gp_max_temp.sort_values('min').head(3)","78f76e56":"gp_max_temp.sort_values('max', ascending=False).head(3)","d0289882":"corr = df_det_scaled.corr().round(2)","64e7d811":"corr","8e35271e":"plt.figure(figsize=(40,30))\nheatmap = sns.heatmap(corr, vmin=-1, vmax=1, annot=True)","6994b68c":"def drop_column_to_avoid_overfitting(df):\n    dataset = df.copy()\n    dataset.drop('Temp9am',axis=1,inplace=True)\n    dataset.drop('Temp3pm',axis=1,inplace=True)\n    return dataset","afce44f5":"df_det_scaled_new = drop_column_to_avoid_overfitting(df_det_scaled)","af91da8e":"zreo_corr_col =  corr[corr['RainTomorrow'] == 0].index","5c86f539":"zreo_corr_col","fdac2434":"df_det_scaled_new.drop(zreo_corr_col, axis = 1, inplace = True)\n","86d74d58":"df_det_scaled_new.columns","fcc396d5":"total_dp = len(df['RainTomorrow'])\ndf_det_scaled_new.groupby('RainTomorrow')['RainTomorrow'].count().apply( lambda x: (x\/total_dp) * 100  )","b98a5893":"from sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel","2590b573":"dataset = df_det_scaled_new.copy()\ndataset.drop(['Date', 'Location'], axis = 1, inplace =True)\nY_det = dataset['RainTomorrow']\nX_det = dataset.drop('RainTomorrow', axis = 1)","8f994687":"feature_sel_model = SelectFromModel(Lasso(alpha=0.005, random_state = 0))\nfeature_sel_model.fit(X_det, Y_det)","046d8694":"feature_sel_model.get_support()","5dfbc913":"selected_feat = X_det.columns[(feature_sel_model.get_support())]","4c5034cf":"selected_feat","203f392c":"from sklearn.metrics import confusion_matrix, classification_report,accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)","87a06f64":"def apply_stratified_k_fold_validation( model, x = df_det_scaled_new[selected_feat], y = Y_det ):\n    stratified_acc = []\n    for train_index, test_index in skf.split(x , y):\n        x_train_fold, x_test_fold = x.iloc[train_index.tolist()], x.iloc[test_index.tolist()]\n        y_train_fold, y_test_fold = y.iloc[train_index.tolist()], y.iloc[test_index.tolist()]\n        model.fit(x_train_fold, y_train_fold)\n        stratified_acc.append(model.score(x_test_fold, y_test_fold))\n    \n    print(\"\\n\")\n    print('List of possible accuracy:', stratified_acc)\n    \n    print(\"\\n\")\n    print('Maximum Accuracy That can be obtained from this model is:', max(stratified_acc)*100, '%')\n    \n    print(\"\\n\")\n    print('Minimum Accuracy:', min(stratified_acc)*100, '%')\n    \n    print(\"\\n\")\n    print('Overall Accuracy:', np.mean(stratified_acc)*100, '%')\n    \n    print(\"\\n\")\n    print('Standard Deviation is:', np.std(stratified_acc)*100, '%')","e55d9f26":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\napply_stratified_k_fold_validation(model)","10eb680d":"from sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier(n_neighbors=5)    \napply_stratified_k_fold_validation(model)","703cb019":"from sklearn import tree\n\nmodel = tree.DecisionTreeClassifier()\napply_stratified_k_fold_validation(model)","67a91408":"from xgboost import XGBClassifier\n\nmodel = XGBClassifier(max_depth=12,random_state = 42, use_label_encoder =False)\napply_stratified_k_fold_validation(model)","c92d0da6":"from sklearn.naive_bayes import GaussianNB\n\nmodel = GaussianNB()\napply_stratified_k_fold_validation(model)","b8a65a87":"from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(random_state=0)\napply_stratified_k_fold_validation(model)","3c46c624":"Our dataset has lot of missing value, even dependent variable will also have missing values, So we can't use them directly. \n* First we have to create a copy of all numberical missing column.\n* Then we can fill some random values using Simple Random Imputation. *(Simple Random Imputation)* Check [this](#Filling-NaN-with-Simple-Random-Imputation) section.\n* Then we will use the regression to fill the value in actual columns, one by one. *(Deterministic Regression Imputation)*. Check [this](#Applying-Deterministic-Regression-Imputation) section.\n\n**Before using Regression, we have to find the higly correlated column for each feature. So that we use only higly correlated column to impute the values.**","7ed37527":"### Scatter plot between numerical features","0c92e8f5":"- **det_Temp9am and det_Temp3pm has high correlation with det_MinTemp and det_MaxTemp. So we will drop those column. To avoid overfitting**","1ea7603e":"Adelaide has the highest MinTemp ","4a4738df":"### Boxplot to visualize outlier","607ebce2":"Woomera has the highest MaxTemp.","688da658":"#### Numerical Null value columns","e6222fc3":"## **Best Models are Logistic Regression, XGBoost and Random forest with Maximum Accuracy of 85%.**","78346701":"## Checking shape of Dataframe","29c05802":"### Imputing Categotical value","0527da6b":"There are lot of null values in each column, except for date and location.","8aaac3fc":"# Exploratory Data Analysis","4d6094fc":"#### Encoding Column: RainToday ","52281e07":"It can be seen that there are still some missing values in WindGustDir. It means some location have no value at all. We wll using mode of complete dataset.","6a167254":"#### Checking newly added Columns","ecee55d8":"### Checking if Central tendencies can be used for imputing missing values.","279a08ef":"It can be seen that mostly all features all follow linear relationship with other features. So we can try using Linear Regression to impute missing value.","7bf71486":"## k-Nearest Neighbors","33cad81f":"## Checking Dataframe Info","bd62b7f7":"It can be seen that most outliers are in **Rainfall** and **Evaporation**.","19ee7c9e":"## Checking null value count for each column","13d96a9b":"# Variable Identification","0d006766":"### Encode Categorical Value","eaada47b":"MonutGinni has the lowest MinTemp.","c06f3496":"## Applying Deterministic Regression Imputation","a7b6dd05":"## Data Preprocessing for Categorical Columns","d2ff5dfc":"It is used to replace the missing value with exact regression output without considering the error term. It may result in overfitting because error term is not considered. **We will prefix the column with 'det_' imputed with this regression**. Later in this notebook we will implement stochastic regression imputation that will overcome the issue of Deterministic Regression Imputation.","93896f3b":"#### Encoding Column: RainTomorrow","1abfe6db":"### Deterministic Regression Imputation","ece10412":"It is used to fill missing values in dependent variable,so that we can use regression models. We will suffix the column with *'_imp'*","3181dc65":"MountGinini has the lowest MaxTemp","ab50d5ee":"## Naive Bayes Classifier","afb2bee1":"## XBoost Classifier","bb8de445":"## Highest and Lowest MinTemp by Location","b0a54f54":"## Logistics Regression","bb82911b":"### Filtering Categorical Null value columns","3aa5f81c":"## Regression to impute missing value","a55a0090":"We are using mode of a particular location to impute missing values.","73d1dafb":"### Highest and Lowest MaxTemp by Location","a36c22db":"## Filtering numerical and categorical columns","c5b667b7":"## Visualizing Null values using missingno","460db406":"First we will encode RainToday and RainTomorrow Column. \"Yes\" => 1 and \"No\" => 0","1ff59168":"### Checking first 5 rows","3e2e81fa":"### Simple Random Imputation","ba40da89":"### Encoding using get_dummies","0eba3d68":"#### Column with Zero Correlation","61bdde1b":"# Model Selection and Cross Validation","0f000366":"### Scaling using Standard scaler","b5bb5f78":"# Importing Packages","b7835996":"#### Dropping column with Zero correlation","72d92333":"# Feature Selection","d5e9f52c":"## Decision Trees","b968c6d9":"## Data Preprocessing for Numberical Columns","0a98ced9":"# Dataset analysis","3215754e":"Though most of the features seems to follow normal distribution, we cannot use mean to impute missing value because outliers. We can verify outliers using Boxplot","f5719e9c":"It can be visually seen that Evaporation, Sunshine, Cloud9am and Cloud3pm has lot of missing values.","1f69421b":"There are 16 *float64* column (numerical) and 7 *object* column (categorical).\n\n","6339db74":"### Reading CSV Dataset","ea4f9aca":"We will use lasso regression for feature selection.","6c04be75":"# Data Preprocessing","662f3d98":"Dataset is higly imbalance. 78% contains No, and 22% contains Yes.","f4a6f427":"* First we will find out higly positively and negatively correlated columns.\n* Use those copy of correlated columns that we created in [simple random imputation](#Filling-NaN-with-Simple-Random-Imputation) for Regression Imputation.","2317efe3":"## Filetring Null value column dataframe","59d57e64":"## Random Forest Classifier","7ccc6901":"#### Filling NaN with Simple Random Imputation"}}