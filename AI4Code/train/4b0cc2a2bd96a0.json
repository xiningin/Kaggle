{"cell_type":{"9d4f4a76":"code","9fd9fa0f":"code","ef41a38c":"code","d63c453f":"code","feb36e35":"code","cf9280be":"code","c66a2314":"code","673f7a7a":"code","52e2028f":"code","9b1324a3":"markdown","7a693028":"markdown","9c92a85f":"markdown","d460b9d1":"markdown","823adefb":"markdown"},"source":{"9d4f4a76":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9fd9fa0f":"train = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv',nrows=30000)","ef41a38c":"features = [col for col in list(train.columns) if 'feature' in col]\ntrain = train[train['weight'] != 0]\ntrain['action'] = (train['resp'].values > 0).astype(int)\nf_mean = train.mean()\ntrain = train.fillna(f_mean)\nX = train.loc[:, features]\ny = train.loc[:, 'action']\ndel train\nX = np.array(X)\ny = np.array(y)","d63c453f":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)","feb36e35":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report","cf9280be":"'''code taken from https:\/\/www.kaggle.com\/nilanml\/telecom-customer-churn-voting-80-1-accuracy'''\n\nclassifiers = [['Neural Network :', MLPClassifier(max_iter = 1000)],\n               ['LogisticRegression :', LogisticRegression(max_iter = 1000)],\n               ['ExtraTreesClassifier :', ExtraTreesClassifier()],\n               ['DecisionTree :',DecisionTreeClassifier()],\n               ['RandomForest :',RandomForestClassifier()], \n               ['Naive Bayes :', GaussianNB()],\n               ['KNeighbours :', KNeighborsClassifier()],\n               ['SVM :', SVC()],\n               ['AdaBoostClassifier :', AdaBoostClassifier()],\n               ['GradientBoostingClassifier: ', GradientBoostingClassifier()],\n               ['XGB :', XGBClassifier()],\n               ['CatBoost :', CatBoostClassifier(logging_level='Silent')]]\n\npredictions_df = pd.DataFrame()\npredictions_df['action'] = y_test\n\nfor name,classifier in classifiers:\n    classifier = classifier\n    classifier.fit(X_train, y_train.ravel())\n    predictions = classifier.predict(X_test)\n    predictions_df[name.strip(\" :\")] = predictions\n    print(name, accuracy_score(y_test, predictions))","c66a2314":"from sklearn.ensemble import VotingClassifier\nclf1 = ExtraTreesClassifier()\nclf2 = CatBoostClassifier(logging_level='Silent')\nclf3 = RandomForestClassifier()\neclf1 = VotingClassifier(estimators=[('ExTrees', clf1), ('CatBoost', clf2), ('RF', clf3)], voting='soft')\neclf1.fit(X_train, y_train)\npredictions = eclf1.predict(X_test)\nprint(classification_report(y_test, predictions))\n","673f7a7a":"eclf2 = VotingClassifier(estimators=[('ExTrees', clf1), ('CatBoost', clf2), ('RF', clf3)], voting='hard')\neclf2.fit(X_train, y_train)\npredictions = eclf2.predict(X_test)\nprint(classification_report(y_test, predictions))","52e2028f":"from sklearn.model_selection import cross_val_score\nc = []\nc.append(cross_val_score(clf1,X_train,y_train,scoring='accuracy',cv=10).mean())\nc.append(cross_val_score(clf2,X_train,y_train,scoring='accuracy',cv=10).mean())\nc.append(cross_val_score(clf3,X_train,y_train,scoring='accuracy',cv=10).mean())\nprint(c)","9b1324a3":"![hard](https:\/\/miro.medium.com\/max\/535\/1*XnZwlg7Th3nga25sSlanJQ.jpeg)\nImage Source - https:\/\/towardsdatascience.com\/ensemble-learning-stacking-blending-voting-b37737c4f483","7a693028":"## What is voting classifier?\nA Voting Classifier is a machine learning model that trains on an ensemble of numerous models and predicts an output (class) based on their highest probability of chosen class as the output.\nIt simply aggregates the findings of each classifier passed into Voting Classifier and predicts the output class based on the highest majority of voting. The idea is instead of creating separate dedicated models and finding the accuracy for each them, we create a single model which trains by these models and predicts output based on their combined majority of voting for each output class.","9c92a85f":"### As we can see that ExtraTreesClassifier, Catboost and RandomForest are showing the higher accuracy score than the rest, we can use these models in voting classifier.","d460b9d1":"![soft](https:\/\/miro.medium.com\/max\/504\/1*bliKQZGPccS7ho9Zo6uC7A.jpeg)\n\nImage Source - https:\/\/towardsdatascience.com\/ensemble-learning-stacking-blending-voting-b37737c4f483","823adefb":"### In short voting classifier instead allows the mixing of different classifiers adopting a majority vote to decide which class must be considered as the winning one during a prediction."}}