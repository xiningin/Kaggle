{"cell_type":{"5b953773":"code","4791718a":"code","ef70fbe4":"code","7121582d":"code","3f13a708":"code","721c8aac":"code","86bb1082":"code","5abe7099":"code","9c6e9e16":"code","59d00b70":"code","d9aaef53":"code","2fcc71a1":"code","1b3b342b":"code","178768e2":"code","6276aa4b":"code","ae890929":"markdown","b129d039":"markdown","4a6fbb17":"markdown","322f04f1":"markdown","ebb34107":"markdown","3e704e24":"markdown","0a9fc639":"markdown"},"source":{"5b953773":"# Bibliotecas necess\u00e1rias\n# Manipula\u00e7\u00e3o de dados\nimport pandas as pd\n# Redes Neurais\nfrom tensorflow import keras\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\nfrom tensorflow.keras.optimizers import RMSprop\n# Plot\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Avalia\u00e7\u00e3o\nfrom sklearn.metrics import classification_report, confusion_matrix\n","4791718a":"# Lendo o dataset Kaggle\ntrain = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\nY = train[\"label\"]\nX = train.drop(labels = [\"label\"],axis = 1)\nprint(X.head())","ef70fbe4":"# Transformando a imagem 2d em um numpy array (imagem 28*28)\nx = X.values.reshape(42000, 28, 28, 1)\n\n#Normalizando para valores entre 0 e 1\nx = x.astype('float32')\nx \/= 255\n\n#print(x[0])","7121582d":"# Vamos ajustar o formato da saida\nnum_classes = 10\n\n# Convertendo para um vetor de saida com 10 dimensoes\n# ex. 8 => [0,0,0,0,0,0,0,0,1,0]\ny = keras.utils.to_categorical(Y, num_classes)\nprint(y[0])","3f13a708":"# Separando uma parte para treino (90%) e outra para valida\u00e7\u00e3o (10%)\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size = 0.1, random_state=5)\nprint('Qtde de treino: {}'.format(len(x_train)))\nprint('Qtde de valida\u00e7\u00e3o: {}'.format(len(x_val)))","721c8aac":"model = Sequential()\nmodel.add(Conv2D(30, kernel_size=(5, 5),\n                 activation='relu',\n                 input_shape=(28,28,1)))\nmodel.add(Conv2D(30, kernel_size=(5, 5),\n                 activation='relu',\n                 input_shape=(28,28,1)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(60, kernel_size=(3,3), activation='relu'))\nmodel.add(Conv2D(60, kernel_size=(3,3), activation='relu'))\nmodel.add(Flatten())\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()","86bb1082":"# Compila o modelo\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=RMSprop(),\n              metrics=['accuracy'])","5abe7099":"# Treina com os parte dos dados\nbatch_size = 32\nepochs = 15\n\n#Salvar o melhor modelo\ncallbacks_list = [\n    keras.callbacks.ModelCheckpoint(\n        filepath='model.h5',\n        monitor='val_loss', save_best_only=True, verbose=1),\n    keras.callbacks.EarlyStopping(monitor='val_loss', patience=10,verbose=1)\n]\n\nhistory = model.fit(x_train, y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    callbacks = callbacks_list,\n                    verbose=1,\n                    validation_data=(x_val, y_val))","9c6e9e16":"fig, ax = plt.subplots(1,2, figsize=(16,8))\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","59d00b70":"from tensorflow.keras.models import load_model\n# Load the best saved model\nmodel = load_model('model.h5')","d9aaef53":"# Testa\nscore = model.evaluate(x_val, y_val, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","2fcc71a1":"# Testando uma entrada qualquer\nprint(y_train[10])\nprint(model.predict(x_train[10].reshape((1,28,28,1))))\nprint(model.predict_classes(x_train[10].reshape((1,28,28,1))))","1b3b342b":"import itertools\n\n#Plot the confusion matrix. Set Normalize = True\/False\ndef plot_confusion_matrix(cm, classes, normalize=True, title='Confusion matrix', cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.figure(figsize=(10,10))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        cm = np.around(cm, decimals=2)\n        cm[np.isnan(cm)] = 0.0\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","178768e2":"# Vendo alguns reports# Vendo alguns reports\n# Usando sklearn\nimport numpy as np\n\n# Classificando toda base de teste\ny_pred = model.predict_classes(x_val)\n# voltando pro formato de classes\ny_test_c = np.argmax(y_val, axis=1)\ntarget_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n\n#Confution Matrix\ncm = confusion_matrix(y_test_c, y_pred)\nplot_confusion_matrix(cm, target_names, normalize=False, title='Confusion Matrix')\n\nprint('Classification Report')\nprint(classification_report(y_test_c, y_pred, target_names=target_names))","6276aa4b":"# Gerando sa\u00edda para dataset de teste\n\n#Carrega dataset de teste\ntest = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\nprint(\"Qtde de testes: {}\".format(len(test)))\n# Bota no formato numpy e normaliza\nx_test = test.values.reshape(len(test),28,28,1)\nx_test = x_test.astype('float32')\nx_test \/= 255\n\n# Faz classifica\u00e7\u00e3o para dataset de teste\ny_pred = model.predict_classes(x_test)\n\n# Verficando algum exemplo\ni = 0\nplt.imshow(test.values[i].reshape(28,28), cmap=plt.cm.binary)\nplt.show()\nprint('Previsto: {}'.format(y_pred[i]))\n\n# Botando no formato de sa\u00edda (competi\u00e7\u00e3o Kaggle)\nresults = pd.Series(y_pred,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,len(y_pred)+1),name = \"ImageId\"),results],axis = 1)\nprint(submission.head(10))\n#Salvando Arquivo\nsubmission.to_csv(\"mlp_mnist_v1.csv\",index=False)","ae890929":"# Justificativa","b129d039":"# Avaliando o Modelo","4a6fbb17":"Aqui ser\u00e1 feito a apresenta\u00e7\u00e3o e justifica\u00e7\u00e3o dos par\u00e2metros que foram alterados, bem como a implementa\u00e7\u00e3o da cria\u00e7\u00e3o do modelo modificado.","322f04f1":"# Bibliotecas e Dados","ebb34107":"# Criando e treinando o Modelo","3e704e24":"# Gerando Sa\u00edda","0a9fc639":"Conv2D: Em pesquisas por modelos que tratem de reconhecimento de escritas, foi verificado que camadas convolucionais de 32 filtros eram as mais usadas para resolu\u00e7\u00e3o deste tipo de problemas, al\u00e9m disso se pode colocar uma camada extra de 64 filtros para refinar os resultados obtidos pela camada anterior. Por\u00e9m ao realizar alguns testes diminuindo o n\u00famero de filtros, verificou-se que uma camada de 30, e uma camada extra de refinamento para 60, alcan\u00e7ou uma acur\u00e1cia maior ao custo de um leve aumento no Loss.\n\nEpochs: Durante testes pr\u00e1ticos rodando o modelo, foi verificado que geralmente os modelos paravam seu treinamento prematuramente por volta da 15\u00aa \u00e9poca, ent\u00e3o mesmo que tenha sido considerado o aumento do n\u00famero de \u00e9pocas, ela foi descartada posteriormente. Dessa forma, foram reduzidas o n\u00famero de \u00e9pocas para bater com o real n\u00famero de \u00e9pocas que o algoritmo alcan\u00e7a.\n\nCamadas extras: Durante os testes, foi verificado que ao duplicar a primeira camada de 30 filtros, o ganho de acur\u00e1cia era algo a se considerar, por\u00e9m o Loss do modelo era muito alto, portanto foi duplicado tamb\u00e9m a camada de 60 filtros e se obt\u00e9ve um resultado melhor, tanto na acur\u00e1cia quanto no Loss."}}