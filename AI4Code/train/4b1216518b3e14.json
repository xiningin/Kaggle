{"cell_type":{"0a09f57a":"code","a16c7e56":"code","9f4bb216":"code","cc09a0e9":"code","46b87815":"code","e12e4f76":"code","3e90c120":"code","61f3c480":"code","8f500027":"code","9d5d439c":"code","bca33ae3":"code","2c278d28":"code","e9b2e486":"code","17001cea":"code","877410c7":"code","bc372d65":"code","50271b2e":"code","cec8a59f":"code","55e3b994":"code","c8a34376":"code","9a053c80":"code","ad2974b1":"code","5be00a64":"code","f7590ad9":"code","5fb82740":"code","ea1361c5":"code","189dcf4e":"code","cca44ea9":"code","4dd3b5e8":"code","67d8794a":"code","d38d7fc4":"code","16376f00":"markdown","040ad68c":"markdown","9a9b7d35":"markdown","0dc89956":"markdown","06618479":"markdown","65a989b1":"markdown","2338a192":"markdown","56dbf4c4":"markdown","9812959e":"markdown","05f3e3f9":"markdown","fb4fbe52":"markdown","75797e3c":"markdown","dcfc5ca4":"markdown","910220eb":"markdown","c0618432":"markdown","48216102":"markdown","7ac0a28f":"markdown","38b23a46":"markdown","3381701a":"markdown","3f670de4":"markdown","73e8a8f5":"markdown","0f8aa2e4":"markdown","a25723ff":"markdown","5adfe6ef":"markdown","fb17552b":"markdown","fe6e08da":"markdown","d7d30485":"markdown","e36dfcf3":"markdown","a40d3503":"markdown","9a1148af":"markdown","04d4829e":"markdown","9a2cb06e":"markdown","83937f40":"markdown","3a3c712b":"markdown","3c12861d":"markdown","1b737dfa":"markdown","9e4218ba":"markdown","4943e934":"markdown","f494da02":"markdown","e334cfaa":"markdown","e5c850f2":"markdown","20eabad1":"markdown","6139b660":"markdown","9dc587d9":"markdown"},"source":{"0a09f57a":"import warnings\nimport numpy as np\nimport pandas as pd\nimport sklearn as sk\nimport matplotlib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport collections\nfrom collections import Counter\nfrom sklearn.decomposition import PCA\n%matplotlib inline\nwarnings.filterwarnings('ignore')\n\nimport xgboost as xgb\nfrom scipy import stats\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import precision_score\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import classification_report\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndata_folder = \"\/kaggle\/input\/dunnhumby-the-complete-journey\/\"","a16c7e56":"df = dict()\ndf[\"hh_demographic\"] = pd.read_csv(data_folder + \"hh_demographic.csv\")\ndemographic=df[\"hh_demographic\"]\ndemographic[\"MARITAL_STATUS_CODE\"].replace(['A', 'B', 'U'],['Married','Unknown','Single'],inplace=True)\n\ndemographic","9f4bb216":"df[\"campaign_desc\"] = pd.read_csv(data_folder+\"campaign_desc.csv\")\ncampaign_desc=df[\"campaign_desc\"]\n#Sort campaign by start date\ncampaign_desc=campaign_desc.sort_values(by=['START_DAY','CAMPAIGN'],ascending=True)\ncampaign_desc","cc09a0e9":"#We exclude the last five campaigns filtering on campaigns starting before days 615. we don't consider campaign 20\ncampaign_desc = campaign_desc[campaign_desc['START_DAY']<615]","46b87815":"df[\"campaign_table\"] = pd.read_csv(data_folder+\"campaign_table.csv\")\ncampaign_table=df[\"campaign_table\"]\ncampaign_table.head(10)","e12e4f76":"#We call campaign the new dataframe merging the dataset\ncampaign = pd.merge(campaign_desc[['CAMPAIGN','START_DAY']],campaign_table[['household_key','CAMPAIGN']],on=\"CAMPAIGN\",how=\"left\")\n#Count number of campaign per household\ncampaign['#campaign']=campaign.groupby(by='household_key')['CAMPAIGN'].transform('count')\n#Delete useless column\ncampaign=campaign.drop(columns=['CAMPAIGN','START_DAY'])\n#Delete duplicates\ncampaign.drop_duplicates(subset=['household_key', '#campaign'], keep=\"first\", inplace=True)\ncampaign","3e90c120":"#Read the coupon_redempt table\ndf[\"coupon_redempt\"] = pd.read_csv(data_folder+\"coupon_redempt.csv\")\ncoupon_redempt=df[\"coupon_redempt\"]\n#Keep only coupon redeemed before DAY 615\ncoupon_redempt=coupon_redempt[coupon_redempt['DAY']<615]\n#Drop useless columns\ncoupon_redempt=coupon_redempt.drop(columns=['DAY','COUPON_UPC'])\n#Keep only one occurence of coupon redeemed by campaign\ncoupon_redempt.drop_duplicates(subset=['household_key', 'CAMPAIGN'], keep=\"first\", inplace=True)\n#Count number of campaign the customer redeemed at least one coupon\nredemption_per_household=coupon_redempt.groupby(['household_key'], as_index=False)['CAMPAIGN'].agg({'redeemed': pd.Series.nunique})\nredemption_per_household","61f3c480":"#Merging of campaign and coupon redemption tables\ntemp = pd.merge(campaign, redemption_per_household, on=['household_key'],how=\"left\")\n#Creation of our output variable\ntemp[\"Sensitivity\"]= np.where(temp[\"redeemed\"]>0, 'Sensible', 'Not sensible')\n#Creation of our aggregated dataset. We use the inner join to keep only customers for which we have the demographics data and thoose who were part of at least one campaign\ndataset= pd.merge(demographic, temp[['household_key','Sensitivity']], on=['household_key'],how=\"inner\")","8f500027":"# load the dataset\ndf[\"transaction_data\"] = pd.read_csv(data_folder+\"transaction_data.csv\")\ntransaction=df[\"transaction_data\"]\n#Keep transaction before day 615\ntransaction=transaction[transaction['DAY']<615]\n#Exclude transactions related to returns\ntransaction=transaction[transaction['SALES_VALUE']>0]\ntransaction=transaction[transaction['QUANTITY']>0]\ntransaction.head(20)","9d5d439c":"#Calculate total sales per customer\ntotal_sales=transaction.groupby(by='household_key', as_index=False)['SALES_VALUE'].sum().rename(columns={'SALES_VALUE': 'Total_sales'})\n#Calculate total number of visits per customer\ntotal_visits=transaction.groupby(['household_key'], as_index=False)['BASKET_ID'].agg({'total_visits': pd.Series.nunique})\n#Calculate median basket amount per customer\ntemp_basket=transaction.groupby(['household_key','BASKET_ID'], as_index=False)['SALES_VALUE'].sum()\ntemp_median_basket=temp_basket.groupby(['household_key'], as_index=False)['SALES_VALUE'].median().rename(columns={'SALES_VALUE': 'median_basket'})\n#Calculate average product price bought per customer\ntemp_product=transaction.groupby(['household_key'], as_index=False)['SALES_VALUE'].mean().rename(columns={'SALES_VALUE': 'avg_price'})\ndataset=dataset.merge(total_sales,on='household_key').merge(total_visits,on='household_key').merge(temp_median_basket,on='household_key').merge(temp_product,on='household_key')\ndataset=dataset.drop(columns=['household_key'])\ndataset","bca33ae3":"print(dataset.shape)","2c278d28":"print(dataset.info())","e9b2e486":"pd.options.display.float_format = \"{:.2f}\".format\ndataset.describe()","17001cea":"categorical_vars = ['AGE_DESC','MARITAL_STATUS_CODE','INCOME_DESC','HOMEOWNER_DESC','HH_COMP_DESC','KID_CATEGORY_DESC']\nnum_plots = len(categorical_vars)\ntotal_cols = 2\ntotal_rows = num_plots\/\/total_cols\nfig, axs = plt.subplots(nrows=total_rows, ncols=total_cols,\n                        figsize=(7*total_cols, 7*total_rows), constrained_layout=True)\nfor i, var in enumerate(categorical_vars):\n    row = i\/\/total_cols\n    pos = i % total_cols    \n    plot = sns.countplot(x=var, data=dataset, ax=axs[row][pos],hue='Sensitivity',palette=\"Set1\")","877410c7":"categorical_vars = ['AGE_DESC','MARITAL_STATUS_CODE','INCOME_DESC','HOMEOWNER_DESC','HH_COMP_DESC','KID_CATEGORY_DESC']\nnum_plots = len(categorical_vars)\ntotal_cols = 2\ntotal_rows = num_plots\/\/total_cols\nfig, axs = plt.subplots(nrows=total_rows, ncols=total_cols,\n                        figsize=(7*total_cols, 7*total_rows), constrained_layout=True)\nfor i, var in enumerate(categorical_vars):\n    row = i\/\/total_cols\n    pos = i % total_cols\n    plot = sns.barplot(x=var, y='median_basket',data=dataset, ax=axs[row][pos],palette=\"Set1\",estimator=np.median)","bc372d65":"target = dataset['Sensitivity']\ncounter = Counter(target)\nfor k,v in counter.items():\n    per = v \/ len(target) * 100\n    print('Class=%s, Count=%d, Percentage=%.2f%%' % (k, v, per))","50271b2e":"#1. Split data into X and Y\nX=dataset.drop(columns=['Sensitivity'])\nY=dataset['Sensitivity']\n\n#2.A. Encode string class values as integers\nlabel_encoder = preprocessing.LabelEncoder()\nlabel_encoder = label_encoder.fit(dataset['Sensitivity'])\nlabel_encoded_y = label_encoder.transform(dataset['Sensitivity'])\n\n#2.A. Encode Income values as integers\nX['INCOME_DESC'].replace(['Under 15K', '15-24K', '25-34K', '35-49K', '50-74K', '75-99K', '100-124K', '125-149K', '150-174K', '175-199K', '200-249K', '250K+'],[0,1,2,3,4,5,6,7,8,9,10,11],inplace=True)\n\n#2.A. Encode Income values as integers\nX['AGE_DESC'].replace(['19-24', '25-34', '35-44', '45-54', '55-64', '65+'],[0,1,2,3,4,5],inplace=True)\n\n#2.A. Label encoding the other categorical data\nlabelencoder_X_1 = LabelEncoder()\nX['MARITAL_STATUS_CODE'] = labelencoder_X_1.fit_transform(X['MARITAL_STATUS_CODE'])\nlabelencoder_X_2 = LabelEncoder()\nX['HOMEOWNER_DESC'] = labelencoder_X_2.fit_transform(X['HOMEOWNER_DESC'])\nlabelencoder_X_3 = LabelEncoder()\nX['HH_COMP_DESC'] = labelencoder_X_3.fit_transform(X['HH_COMP_DESC'])\nlabelencoder_X_4 = LabelEncoder()\nX['HOUSEHOLD_SIZE_DESC'] = labelencoder_X_4.fit_transform(X['HOUSEHOLD_SIZE_DESC'])\nX[\"KID_CATEGORY_DESC\"].replace(['None\/Unknown','3+'],[0,3],inplace=True)\nX['HOUSEHOLD_SIZE_DESC'] = X.HOUSEHOLD_SIZE_DESC.astype(float)\nX['KID_CATEGORY_DESC'] = X.KID_CATEGORY_DESC.astype(float)","cec8a59f":"#Let's plot the Skewness by decreasing order\nnum_feats=X.dtypes[X.dtypes!='object'].index\nskew_feats=X[num_feats].skew().sort_values(ascending=False)\nskewness=pd.DataFrame({'Skew':skew_feats})\nprint(skewness)","55e3b994":"df = pd.DataFrame(data=X, columns=['AGE_DESC','MARITAL_STATUS_CODE','INCOME_DESC','HOMEOWNER_DESC','HH_COMP_DESC','KID_CATEGORY_DESC','Total_sales','total_visits','median_basket','avg_price'])\n#Permet de tracer les courbes de distribution de toutes les variables\nnd = pd.melt(df, value_vars =df )\nn1 = sns.FacetGrid (nd, col='variable', col_wrap=5, sharex=False, sharey = False)\nn1 = n1.map(sns.distplot, 'value')\nn1","c8a34376":"# Finding the relations between the variables\nplt.figure(figsize=(20,10))\nc= X.corr(method='spearman')\nsns.heatmap(c,annot=True)\nc","9a053c80":"#remove attribute\nX=X.drop(columns=['HOUSEHOLD_SIZE_DESC'])","ad2974b1":"X_train, X_test, y_train, y_test = train_test_split(X,label_encoded_y ,\ntest_size=0.3, random_state=7,shuffle=True)","5be00a64":"#instantiate \npt = PowerTransformer(method='yeo-johnson', standardize=True) \n\n#Fit the data to the powertransformer\nrescaler = pt.fit(X_train)\n\n#Lets get the Lambdas that were found\nprint (rescaler.lambdas_)\n\ncalc_lambdas = rescaler.lambdas_\n\n#Transform the data \nX_train_resc = rescaler.transform(X_train)\nX_test_resc=rescaler.transform(X_test)\n\n#Pass the transformed data into a new dataframe \ndf_xt = pd.DataFrame(data=X_train_resc, columns=['AGE_DESC','MARITAL_STATUS_CODE','INCOME_DESC','HOMEOWNER_DESC','HH_COMP_DESC','KID_CATEGORY_DESC','Total_sales','total_visits','median_basket','avg_price'])\n\ndf_xt.describe()","f7590ad9":"pca = PCA().fit(X_train_resc)\n\nplt.rcParams[\"figure.figsize\"] = (12,11)\n\nfig, ax = plt.subplots()\nxi = np.arange(1, 11, step=1)\ny = np.cumsum(pca.explained_variance_ratio_)\n\nplt.ylim(0.0,1.1)\nplt.plot(xi, y, marker='o', linestyle='--', color='b')\n\nplt.xlabel('Number of Components')\nplt.xticks(np.arange(0, 11, step=1))\nplt.ylabel('Cumulative variance (%)')\nplt.title('The number of components needed to explain variance')\n\nplt.axhline(y=0.90, color='r', linestyle='-')\nplt.text(0.5, 0.90, '90% cut-off threshold', color = 'red', fontsize=16)\n\nax.grid(axis='x')\nplt.show()","5fb82740":"# on standardized data\npca_std = PCA(n_components=7).fit(X_train_resc)\nX_train_PCA = pca_std.transform(X_train_resc)\nX_test_PCA = pca_std.transform(X_test_resc)\npca_std.explained_variance_ratio_","ea1361c5":"from sklearn.model_selection import cross_val_score\n\n#check the performance of the XGBoost model without tune parameters\n# fit model on training data\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nmodel = xgb.XGBClassifier()\nkfold = StratifiedKFold(n_splits=5, random_state=7,shuffle=True)\n\n\nAccuracy = cross_val_score(model, X_train_PCA, y_train, cv=kfold,scoring='accuracy')\nPrecision = cross_val_score(model, X_train_PCA, y_train, cv=kfold,scoring='precision')\n\nprint(\"Accuracy: %.1f%% (%.1f%%)\" % (Accuracy.mean()*100, Accuracy.std()*100))\nprint(\"Precision: %.1f%% (%.1f%%)\" % (Precision.mean()*100, Precision.std()*100))","189dcf4e":"# grid search to tune algorithm\nmodel = xgb.XGBClassifier()\nn_estimators = [50,100, 200, 300, 400,450, 500,600,1000]\nlearning_rate = [0.01,0.05,0.1,0.2]\nmax_depth= range(2,8)\ngamma=[0, 0.25, 0.5, 0.7, 0.9, 1.0]\n\nparam_grid = dict(gamma=gamma,learning_rate=learning_rate, n_estimators=n_estimators,max_depth=max_depth)\neval_set=[(X_train_PCA, y_train), (X_test_PCA, y_test)]\nkfold = StratifiedKFold(n_splits=5, random_state=7,shuffle=True)\ngrid_search = GridSearchCV(model, param_grid,scoring='precision', n_jobs=-1, cv=kfold)\ngrid_result = grid_search.fit(X_train_PCA, y_train,early_stopping_rounds= 20,eval_metric= [\"logloss\"],eval_set=eval_set,verbose=20)                       \n\n# summarize result\nprint(\"Best: %.1f%% using %s\" % (grid_result.best_score_*100, grid_result.best_params_))","cca44ea9":"# fit model on training data\nmodel = xgb.XGBClassifier(learning_rate = 0.05,\\\n                          max_depth=2,\\\n                          n_estimators=200,\\\n                          gamma=0.7,\\\n                          objective = 'binary:logistic',\\\n                         )\nfit_params={'early_stopping_rounds': 20, \n            'eval_metric': 'logloss',\n            'verbose': False,\n            'eval_set': [(X_train_PCA, y_train), (X_test_PCA, y_test)]}\n                         \nkfold =  StratifiedKFold(n_splits=5, random_state=7,shuffle=True)\n\nAccuracy = cross_val_score(model, X_train_PCA, y_train, cv=kfold,scoring='accuracy',fit_params = fit_params)\nPrecision = cross_val_score(model, X_train_PCA, y_train, cv=kfold,scoring='precision',fit_params = fit_params)\n\nprint(\"Accuracy: %.1f%%\" % (Accuracy.mean()*100))\nprint(\"Precision: %.1f%%\" % (Precision.mean()*100))","4dd3b5e8":"model = xgb.XGBClassifier(learning_rate = 0.05,\\\n                          max_depth=2,\\\n                          n_estimators=200,\\\n                          gamma=0.7,\\\n                          objective = 'binary:logistic',\\\n                          )\n\neval_set = [(X_train_PCA, y_train), (X_test_PCA, y_test)]\nmodel.fit(X_train_PCA, y_train, early_stopping_rounds=20, eval_metric=[\"error\",\"logloss\"], eval_set=eval_set, verbose=False)\n# make predictions for test data\npredictions = model.predict(X_test_PCA)\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\nprecision = precision_score(y_test, predictions)\n\nprint(\"accuracy: %.2f%%\" % (accuracy * 100.0))\nprint(\"Precision: %.2f%%\" % (precision * 100.0))\nprint(classification_report(y_test, predictions,   labels=[1,0]))","67d8794a":"# retrieve performance metrics\nresults = model.evals_result()\nepochs = len(results['validation_0']['error'])\nx_axis = range(0, epochs)\n# plot log loss\nfig, ax = plt.subplots()\nax.plot(x_axis, results['validation_0']['logloss'], label='Train')\nax.plot(x_axis, results['validation_1']['logloss'], label='Test')\nax.legend()\nplt.ylabel('Log Loss')\nplt.title('XGBoost Log Loss')\nplt.show()\n# plot classification error\nfig, ax = plt.subplots()\nax.plot(x_axis, results['validation_0']['error'], label='Train')\nax.plot(x_axis, results['validation_1']['error'], label='Test')\nax.legend()\nplt.ylabel('Classification Error')\nplt.title('XGBoost Classification Error')\nplt.show()","d38d7fc4":"# confusion marix for the test data\ncm = sk.metrics.confusion_matrix(y_test, predictions,  labels=[1,0])\n\nfig, ax= plt.subplots(figsize=(10,10))\nsns.heatmap(cm, annot=True, fmt='g', ax = ax); \n\n# labels, title and ticks\nax.set_xlabel('Predicted labels');\nax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(['Sensible','Not sensible']); \nax.yaxis.set_ticklabels(['Sensible','Not sensible']);","16376f00":"### Campaign data <a class=\"anchor\" id=\"section_1_2\"><\/a>\nThe tables campaign_table and campaign_desc contains all the campaign information. As we want to predict the customers that won't redeem their coupons for the __next 5 planned campaigns__, we will create our model based on all the previous campaigns. \nWe can see that the last five campaigns are campaigns number __21, 22, 23, 24 and 25__.","040ad68c":"# Coupon redemption classification model\n**We know from the dataset that 70% of the customers never use the coupons they receive and this would lead to a waste of money and time for the company.\nMy objective here is to create a classification model to predict if a customer will redeem its coupons or not for the last 5 campaigns of the year. Beyond knowing which customers will redeem their coupons, it can be more interesting for a company to identify which customers won't redeem them in order to either decide on different marketing and communication actions for reaching them or to not send them coupon at all and save money.**","9a9b7d35":"We can see from the graph that __overfitting has been avoided__ thanks to early stopping. The best iteration has been found earlier at __round 105__ and this is confirmed graphically : we can see the test curve start slowly increasing again at this point ","0dc89956":"To assess the performance of our model, we will concentrate on __two evaluation metrics :__  \n- Accuracy\n- Precision  \n\nPrecision is very interesting here as we would like to minimize as much as possible __Type 1 error__ also known as False positive. Indeed, we would like to avoid the case where a customer is predicted to be sensitive to coupons while in fact he is not. This would lead to money loss as we would print and send him\/her coupons for nothing.  \nOur model report an __accuracy of 66.7%__ and a __precision of 55.8%__. This means that when our model predict that a customer is sensitive to coupon, it's correct 55.8% of the time.\nWe will try to improve our model performance with some tuning.","06618479":"# 5. Model results <a class=\"anchor\" id=\"section_5\"><\/a>","65a989b1":"Performance on test set reports an __accuracy of 67%__ and a __precision of 70%__","2338a192":"Value close to 0 show __less skew__.  \nPlotting the distribution is the fastest way to know if an attribute is Gaussian or skewed.","56dbf4c4":"### Demographics data <a class=\"anchor\" id=\"section_1_1\"><\/a>\nThis table contains several demographics information concerning __802 regular customers__. let's load the data and start an exploratory data analysis ","9812959e":"### C. Feature selection <a class=\"anchor\" id=\"section_3_3\"><\/a>\nWe will use the PCA transformation. Our first step will be to choose the number of components","05f3e3f9":"We have __8 categorical variables__ and __4 numerical values__.  \nWe don't have any missing value among our 751 observations.","fb4fbe52":"We can see the median basket is the __highest__ for :\n- Married households\n- Households with 3 kids and more\n- Households with an income superior than 250k+","75797e3c":"# 1. Dataset Creation <a class=\"anchor\" id=\"section_1\"><\/a>","dcfc5ca4":"We will choose the number of components that __explains 90% of the variance__. From the graph we can see that we need __7 components__.","910220eb":"### G. Correlation between attributes <a class=\"anchor\" id=\"section_2_7\"><\/a>\nNow we want to be sure we don't have variables with similar information. For that we will plot the correlation matrix.\nCorrelation refers to the relationship between two variables and how they move together.\nEven if XGBoost manages very well correlated features, it's always a good step to check for multicollinearity","c0618432":"# 4. Model creation <a class=\"anchor\" id=\"section_4\"><\/a>","48216102":"From this point we can merge the two campaign tables","7ac0a28f":"In conclusion we can use this model to predict if a customer is __sensitive__ to coupons __or not__. Compared to before, when we were sending coupons to customers, only 38% of them used to redeem their coupons meaning 62% of them were not interested. Now before each campaign we can use our prediction model to identify customers who won't use their coupons and either remove them from the mailing list or decide on different marketing actions to address them","38b23a46":"### C. Statistical Summary <a class=\"anchor\" id=\"section_2_3\"><\/a>\nLet's use the __*describe()*__ function to have a basic description of our dataset. It will enable us to have various summary statistics :","3381701a":"__We now fit model on training data with the optimized parameters found__","3f670de4":"The table campaign_table tells us which customers received a specific campaign.","73e8a8f5":"### B. Model hyperparameter tuning <a class=\"anchor\" id=\"section_4_2\"><\/a>","0f8aa2e4":"__62%__ of our customers are not sensitive to coupons. Hence our dataset is sligthly imbalanced with more customers not sensible to coupons.<br> We will keep that in mind when choosing our cross validation technique.","a25723ff":"### D. Class distribution <a class=\"anchor\" id=\"section_2_4\"><\/a>\nOn classification problems, analyzing the class distribution is always an important step as highly imbalanced data are common and need special treatment. Let's check if data are imbalanced :","5adfe6ef":"### A. Shape of our data <a class=\"anchor\" id=\"section_2_1\"><\/a>\nLet's use the shape() method to know the dimensions of our dataset :","fb17552b":"### B. Learning curves <a class=\"anchor\" id=\"section_5_2\"><\/a>","fe6e08da":"### F. Skew of univariate distribution <a class=\"anchor\" id=\"section_2_6\"><\/a>\nThis steps is one of the most important as many algorithm performance would depend on the normal distribution of the variables.\nXGboost is a non-parametric model and non-parametric models are rarely affected by skewed features. However normalizing features will not have a negative effect on our models\u2019 performance. Let's check if our data are skewed :","d7d30485":"__Feature creation and table merging__","e36dfcf3":"We have __12 variables__ and __751 observations__ in our dataset","a40d3503":"### A. Model baseline <a class=\"anchor\" id=\"section_4_1\"><\/a>","9a1148af":"### Table of Contents\n\n* [1. Dataset Creation](#section_1)\n    * [A. Demographic data](#section_1_1)\n    * [B. Campaign data](#section_1_2)\n    * [C. Coupon redemption data](#section_1_3)\n    * [D. Transaction data](#section_1_4)  \n    ___\n* [2. Data Exploration](#section_2)\n    * [A. Shape of our data](#section_2_1)\n    * [B. Data types and data completeness](#section_2_2)\n    * [C. Statistical summary](#section_2_3)\n    * [D. Class distribution](#section_2_4)\n    * [E. Variable encoding and split](#section_2_5)\n    * [F. Skew of univariate distribution](#section_2_6)\n    * [G. Correlation between attributes](#section_2_7)\n    ---\n* [3. Data Preprocessing](#section_3)\n    * [A. Split into train test](#section_3_1)\n    * [B. Data transformation](#section_3_2)\n    * [C. Feature selection](#section_3_3)\n   ___\n* [4. Model Creation](#section_4)\n    * [A. Baseline model](#section_4_1)\n    * [B. Model hyperparameter tuning](#section_4_2)\n    ___\n* [5. Model Results](#section_5)\n    * [A. Accuracy and Precision scores](#section_5_1)\n    * [B. Learning curve](#section_5_2)\n    * [C. Confusion matrix](#section_5_3)\n    ---","04d4829e":"# 2. Data exploration <a class=\"anchor\" id=\"section_2\"><\/a>\nNow our dataset is ready, let's discover it with descriptive statistics","9a2cb06e":"### A. Split data into train test <a class=\"anchor\" id=\"section_3_1\"><\/a>","83937f40":"### E. Variable encoding and split <a class=\"anchor\" id=\"section_2_5\"><\/a>\nThese two intermediate steps are needed in order to :\n1. Separate the columns of our dataset into input patterns (X) and output patterns (Y)\n2. Transform our eight categorical variables in numeric variables","3a3c712b":"### Coupon redemption data <a class=\"anchor\" id=\"section_1_3\"><\/a>\nThis table contains all the coupons that have been redeemed by each customer. We will use it to count how many coupons a customer redeemed for each campaign.<br>\nWe will follow the below steps :\n- Loading the dataset\n- Retrieving coupons redeemed before Day 615\n- Count the number of campaign for which at least one coupon has been redeemed\n- Merge our campaign table, coupon redemption table and demographic table\n- Define if a customer is sensitive to coupon or not","3c12861d":"# 3. Data preprocessing <a class=\"anchor\" id=\"section_3\"><\/a>","1b737dfa":"We can see we have variables with Multinomial distribution and variables with Gaussian-like distribution with a long right tail. This is confirmed with the positive values of the skewness","9e4218ba":"After hyperparameter tuning, our accuracy __improved to 69%__ and our __precision to 62%__. Let's now discover the performance of our model on the test set.","4943e934":"We can see that Households_SIZE_DESC,KID_CATEGORY_DESC and HH_COMP_DESC are correlated. Even if as said, XGboost handle correlated features, we will remove HOUSEHOLD_SIZE_DESC","f494da02":"Now our variables are scaled to a 0 mean and unit variance thanks to Standardization and they have a more Gaussian distribution thanks to Power Transform transformation. We can perform a PCA for feature selection  \n(PCA doesn't necessarily assume the dataset to be Gaussian distributed)","e334cfaa":"### A. Accuracy and Precision scores <a class=\"anchor\" id=\"section_5_1\"><\/a>","e5c850f2":"### Transactions data <a class=\"anchor\" id=\"section_1_4\"><\/a>\nThis table contains all the transactions made by customers during 2 years. it contains 12 columns and 2 595 732 rows.\nWe will use the transactional dataset to create some features for our model :\n- Total sales between day 1 and day 615\n- Total number of visits in the shops\n- Median basket spend by customer\n- Average product price purchased by customer\n\nTo clean the dataset, we will follow the below steps :\n- Load the data\n- Keep sales before day 615\n- Exclude transaction with sales value and quantity inferior or equal to 0","20eabad1":"### C. Confusion matrix <a class=\"anchor\" id=\"section_5_3\"><\/a>","6139b660":"### B. Data transform <a class=\"anchor\" id=\"section_3_2\"><\/a>\nTo give our data a more Gaussian distribution, we will use the power transform method","9dc587d9":"### B. Data types and data completeness <a class=\"anchor\" id=\"section_2_2\"><\/a>\nLet's use the __*info()*__ function to get more information about our dataframe as variables types and data completness :"}}