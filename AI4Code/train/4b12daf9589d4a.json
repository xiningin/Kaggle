{"cell_type":{"94e4e20b":"code","36dd8c14":"code","67d87c8c":"code","abb33dac":"code","7c8ba5a3":"code","df2d4d93":"code","6b723222":"code","6ead93e5":"code","a4efef3f":"code","239a176d":"code","ebb49152":"code","7c7ea729":"code","e28e4311":"code","0b95e0c8":"code","48936008":"code","105ba60a":"code","c7890134":"code","88af2c53":"markdown","b352e371":"markdown","95e7a93d":"markdown","3080aa30":"markdown","c099a560":"markdown","b43fabdc":"markdown","2de34165":"markdown","b75fd477":"markdown"},"source":{"94e4e20b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport math\n\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing, metrics\n\nfrom time import time\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","36dd8c14":"#Import datasets used\nfund=pd.read_parquet('\/kaggle\/input\/eq-inv-ml\/fund.parquet.gzip')\nfund['date']=pd.to_datetime(fund['date'])\nfund['InfoPublDate']=pd.to_datetime(fund['InfoPublDate'])\nfund['EndDate']=pd.to_datetime(fund['EndDate'])\n\nmsf=pd.read_parquet('\/kaggle\/input\/eq-inv-ml\/msf.parquet.gzip')\nmsf['date']=pd.to_datetime(msf['date'])\n\nfactors = pd.read_parquet('\/kaggle\/input\/eq-inv-ml\/factors.parquet.gzip')\nfactors['date']=pd.to_datetime(factors['date'])","67d87c8c":"# Merge fund, msf and factors\nfund=fund.merge(msf[['SecuCode','date','totcap','prc','pret20d','pret250d']],on=['SecuCode','date'],how='left')\nfund=fund.merge(factors[['SecuCode','date','sue']],on=['SecuCode','date'],how='left')\n# Drop NaN in sue\nfund=fund.dropna(subset=['sue'])","abb33dac":"#extract and define factors used\nfund['size']=np.log(fund['totcap']) #size factor\nfund['value']=fund['opprofit_0']\/fund['prc'] #value factor\n#SUE factor already exist in datasets","7c8ba5a3":"# This function is used to winsorize the variables at (1%, 99%) so that they don't have outliers.\nWIN_LIMIT = 1\ndef preprocess(a):\n    a = a.astype(np.float64)\n    a[np.isinf(a)] = np.nan\n    a=np.clip(a,np.nanpercentile(a,WIN_LIMIT),np.nanpercentile(a,100-WIN_LIMIT))\n    return a","df2d4d93":"# Winsorization\nfor x in ['size','value','sue']:\n    fund[x]=preprocess(fund[x])\n    \nfund['simple_average'] = (fund['size']+fund['value']+fund['sue'])\/3","6b723222":"# filter the highest 70% of stock data by size factor\nfund.sort_values('size', ascending=False, inplace=True)\nfund = fund[fund['size'].cumsum() \/ fund['size'].sum() < 0.7]","6ead93e5":"#Z-score transformation\nzscore = lambda x: (x - x.mean()) \/ x.std()\nresults = fund[['SecuCode','date','size','value','sue','simple_average']].dropna(subset=['simple_average'])\n\nfor x in ['size','value','sue','simple_average']:    \n    results.loc[:,[x]]=results.groupby(['date'])[x].transform(zscore)\n    results.loc[:,[x]]=results[x].clip(lower = -3, upper = 3)","a4efef3f":"results = results.merge(msf[['SecuCode','date','ret1']].dropna(subset=['ret1']),on=['SecuCode','date'],how='inner')","239a176d":"#build training\/test datasets (60% training, 40% testing)\nfrdatax = results.copy()\n\nfrdatax['ret1'] = frdatax['ret1'].clip(lower = -1, upper = 1)\n\nfrdatax['returns_rank'] = frdatax.groupby(['date'])['ret1'].transform(lambda x: pd.qcut(x.rank(method='first'), 10, labels=range(1,11)))\n\nfrdatax.pop('ret1')\n\nfrdatax.sort_values(by=['date'],inplace=True)\n\ndates = frdatax['date'].unique()\nn_time = len(dates)\n\ntrain_size_perc = 0.6\ntrain_size = np.int16(np.round(train_size_perc * n_time))\n\ntrain_dates = dates[:train_size]\ntest_dates = dates[train_size:]\n\nX_train = frdatax.loc[frdatax['date'].isin(train_dates)]\nX_test = frdatax.loc[frdatax['date'].isin(test_dates)]\n\nfactors_train = X_train[['SecuCode','date','simple_average']]\nfactors_test = X_test[['SecuCode','date','simple_average']]\n\nX_train = X_train.drop(['SecuCode','date','simple_average'], axis=1)\nX_test = X_test.drop(['SecuCode','date','simple_average'], axis=1)\n\npreds = pd.DataFrame(factors_test)\n\nX_train = X_train[(X_train.returns_rank<4) | (X_train.returns_rank>7)]\n\nX_train['Y'] = (X_train['returns_rank']>5).astype(int)\nX_train.pop('returns_rank')\nY_train = X_train.pop('Y')\n\n\nX_test['Y'] = (X_test['returns_rank']>5).astype(int)\nX_test.pop('returns_rank')\nY_test = X_test.pop('Y')\n\nX_train_shift = X_train.values\nY_train_shift = Y_train.values\n\nX_test_shift = X_test.values\nY_test_shift = Y_test.values","ebb49152":"#standardize datasets\nscaler = preprocessing.MinMaxScaler()\n\nX_train_trans = scaler.fit_transform(X_train_shift)\n\n# Transform test data\nX_test_trans = scaler.transform(X_test_shift)","7c7ea729":"#Portfolio analysis function\ndef PortAnalysis(dfin,sig,ngroup):\n\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    import numpy as np\n    from IPython.display import display, HTML\n    import math\n\n    sdf=dfin[['SecuCode','date',sig]].dropna(subset=[sig])\n    sdf=sdf.merge(msf[['SecuCode','date','ret1']].dropna(subset=['ret1']),on=['SecuCode','date'],how='inner')\n\n    sdf['sig_rank'] = sdf.groupby(['date'])[sig].transform(lambda x: pd.qcut(x.rank(method='first'), ngroup, labels=range(1,ngroup+1)))\n\n    avgret=pd.DataFrame(sdf.groupby(['date'])['ret1'].mean()) # calculate average future returns in general\n    sigret=pd.DataFrame(sdf.groupby(['date', 'sig_rank'])['ret1'].mean()) # calculate average future returns in different subgroups sorted by the factor\n    avgret.reset_index(inplace=True)\n    sigret.reset_index(inplace=True)   \n    avgret.rename(columns={'ret1': 'avg_ret'}, inplace=True)\n    sigret=pd.merge(sigret, avgret, on='date', how='inner')\n    sigret['act_ret']=sigret['ret1']-sigret['avg_ret']\n\n    retvar='ret1'\n\n    sigret=sigret.sort_values(axis=0,ascending=True,by=['sig_rank','date'])   \n    sigret.sig_rank = sigret.sig_rank.astype(int)\n    sigret['sig_rank']=sigret['sig_rank']+100\n    sigret['sig_rank']=sigret['sig_rank'].astype(str)\n    sigret['sig_rank']=\"G\"+sigret.sig_rank.str[-2:]\n    sigret['cumactret']=sigret.groupby(['sig_rank'])['act_ret'].cumsum()\n\n    sigret['cumrawret']=sigret.groupby(['sig_rank'])['ret1'].cumsum()\n\n    cumret=sigret.pivot(index='date', columns='sig_rank', values='cumactret')\n    cumret['GH']=cumret['G'+str(100+ngroup)[-2:]]-cumret['G01']\n\n\n    cumrawret=sigret.pivot(index='date', columns='sig_rank', values='cumrawret')\n    cumrawret['GH']=cumrawret['G'+str(100+ngroup)[-2:]]-cumrawret['G01']\n\n\n    sigactret=sigret.pivot(index='date', columns='sig_rank', values='act_ret')\n    sigactret['GH']=sigactret['G'+str(100+ngroup)[-2:]]-sigactret['G01']\n    meanact=12*pd.DataFrame(sigactret.mean())\n    stdact=math.sqrt(12)*pd.DataFrame(sigactret.std())\n    meanact.rename(columns={0: 'Annualized Active Return'}, inplace=True)\n    stdact.rename(columns={0: 'Annualized Active Risk'}, inplace=True)\n\n\n\n    sigrawret=sigret.pivot(index='date', columns='sig_rank', values='ret1')\n    sigrawret['GH']=sigrawret['G'+str(100+ngroup)[-2:]]-sigrawret['G01']\n    meanraw=12*pd.DataFrame(sigrawret.mean())\n    stdraw=math.sqrt(12)*pd.DataFrame(sigrawret.std())\n    meanraw.rename(columns={0: 'Annualized Return'}, inplace=True)\n    stdraw.rename(columns={0: 'Annualized Risk'}, inplace=True)\n\n\n    Perfact=meanact.join(stdact,how='inner')\n    Perfact['Information Ratio']=Perfact['Annualized Active Return']\/Perfact['Annualized Active Risk']\n    Perfact=Perfact.round({\"Information Ratio\":3,\"Annualized Active Return\":4,\"Annualized Active Risk\":4})\n\n\n    Perfraw=meanraw.join(stdraw,how='inner')\n    Perfraw['Sharpe Ratio']=Perfraw['Annualized Return']\/Perfraw['Annualized Risk']\n    Perfraw=Perfraw.round({\"Sharpe Ratio\":3,\"Annualized Return\":4,\"Annualized Risk\":4})\n\n    display(Perfraw)\n\n    display(Perfact)\n\n    Perfact.reset_index(inplace=True)\n    aa=Perfact.plot(x=\"sig_rank\", y=['Annualized Active Return','Annualized Active Risk'], kind=\"bar\")\n    aa.set_ylabel(\"Annualized Active Return and Risk\")\n    ax=Perfact['Information Ratio'].plot(secondary_y=True, color='k', marker='o')\n    ax.set_ylabel('Information Ratio')\n\n    Perfraw.reset_index(inplace=True)\n    ba=Perfraw.plot(x=\"sig_rank\", y=['Annualized Return','Annualized Risk'], kind=\"bar\")\n    ba.set_ylabel(\"Annualized Return and Risk\")\n    bx=Perfraw['Sharpe Ratio'].plot(secondary_y=True, color='k', marker='o')\n    bx.set_ylabel('Sharpe Ratio')\n\n\n    cumret.reset_index(inplace=True)\n    bb=cumret.plot(x=\"date\", y=['G01','G02','G03','G04','G05','GH'], kind=\"line\")   \n    bb.set_ylabel('Cumulative Active Returns')\n\n\n    cumrawret.reset_index(inplace=True)\n    cc=cumrawret.plot(x=\"date\", y=['G01','G02','G03','G04','G05','GH'], kind=\"line\")   \n    cc.set_ylabel('Cumulative Raw Returns')\n    \n    return None","e28e4311":"PortAnalysis(preds,'simple_average',5)","0b95e0c8":"#RandomForest\nfrom sklearn.ensemble import RandomForestClassifier\n\nstart_timer = time()\nclf0 = RandomForestClassifier(max_depth=5,n_estimators=200,random_state=0)\nclf0.fit(X_train_trans, Y_train_shift)\nend_timer = time()\n\n# Predict!\nY_pred = clf0.predict(X_test_trans)\nY_pred_prob = clf0.predict_proba(X_test_trans)\npreds['rf']=Y_pred_prob[:, 1] \n\nprint(\"Time to train full ML pipline: %0.2f secs\" % (end_timer - start_timer))\nprint('Accuracy on test set = {:.2f}%'.format(metrics.accuracy_score(Y_test_shift, Y_pred) * 100))\nprint('Log-loss = {:.5f}'.format(metrics.log_loss(Y_test_shift, Y_pred_prob)))\nPortAnalysis(preds,'rf',5)","48936008":"#AdaBoost\nfrom sklearn.ensemble import AdaBoostClassifier\n\nstart_timer = time()\n\n\nclf1 = AdaBoostClassifier(random_state=0)\nclf1.fit(X_train_trans, Y_train_shift)\n\nend_timer = time()\n\n# Predict!\nY_predAB = clf1.predict(X_test_trans)\nY_predAB_prob = clf1.predict_proba(X_test_trans)\npreds['AB']=Y_predAB_prob[:, 1] \n\nprint(\"Time to train full ML pipline: %0.2f secs\" % (end_timer - start_timer))\nprint('Accuracy on test set = {:.2f}%'.format(metrics.accuracy_score(Y_test_shift, Y_predAB) * 100))\nprint('Log-loss = {:.5f}'.format(metrics.log_loss(Y_test_shift, Y_predAB_prob)))\nPortAnalysis(preds,'AB',5)","105ba60a":"from sklearn.svm import SVC\n\nstart_timer = time()\nclf3 = SVC(kernel='poly',gamma='auto',random_state=0,probability = True)\nclf3.fit(X_train_trans, Y_train_shift)\nend_timer = time()\n\n# Predict!\nY_pred_svc = clf3.predict(X_test_trans)\nY_pred_prob_svc = clf3.predict_proba(X_test_trans)\npreds['svc']=Y_pred_prob_svc[:, 1] \n\nprint(\"Time to train full ML pipline: %0.2f secs\" % (end_timer - start_timer))\nprint('Accuracy on test set = {:.2f}%'.format(metrics.accuracy_score(Y_test_shift, Y_pred_svc) * 100))\nprint('Log-loss = {:.5f}'.format(metrics.log_loss(Y_test_shift, Y_pred_prob_svc)))\nPortAnalysis(preds,'svc',5)","c7890134":"from sklearn.ensemble import BaggingClassifier\n\nstart_timer = time()\nclf5 = BaggingClassifier(base_estimator=SVC(kernel='poly',gamma='auto',random_state=0,probability = True), n_estimators=3, n_jobs=-1, random_state=0)\nclf5.fit(X_train_trans, Y_train_shift)\nend_timer = time()\n\n# Predict!\nY_pred_bsvc2 = clf5.predict(X_test_trans)\nY_pred_prob_bsvc2 = clf5.predict_proba(X_test_trans)\npreds['bsvc2']=Y_pred_prob_bsvc2[:, 1] \n\nprint(\"Time to train full ML pipline: %0.2f secs\" % (end_timer - start_timer))\nprint('Accuracy on test set = {:.2f}%'.format(metrics.accuracy_score(Y_test_shift, Y_pred_bsvc2) * 100))\nprint('Log-loss = {:.5f}'.format(metrics.log_loss(Y_test_shift, Y_pred_prob_bsvc2)))\nPortAnalysis(preds,'bsvc2',5)","88af2c53":"#### Part 2 Analysis of Benchmark - Simple average of factors","b352e371":"##### AdaBoost","95e7a93d":"##### Bagging classifier with a SVM kernel","3080aa30":"#### Part 1 Data prepration","c099a560":"### Conclusion of test result\n1. All Machine learning algorithms out-perform benchmark - simple average.\n2. Among all types of algorithms, Adaboost shows the highest return.\n3. Rank by return from High to low: Adaboost > Bagging classifier with a SVM kernel > SVM > Random Forest > Simple average","b43fabdc":"#### Part 3 Analysis of different Machine Learning Algorithms\n##### Random Forest","2de34165":"##### SVM","b75fd477":"# Final Project test report - An investment strategy\n## \u2014\u2014 Utilizing modified Fama-french 3 factor model, Standardized unexpected earning and machine learning method\n###  Group 3"}}