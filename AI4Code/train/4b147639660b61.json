{"cell_type":{"102d357a":"code","6e4a2fb8":"code","c3e57887":"code","c2e7f8b7":"code","40887e34":"code","9d59615f":"code","7dff6e9c":"code","06a97234":"code","29e2546f":"code","fc5ca864":"code","920a251c":"code","beb0381f":"code","2918d03a":"markdown","5e7476b8":"markdown","622e13d3":"markdown","703ceb91":"markdown","97a3c978":"markdown","2aabf2ae":"markdown","871f7733":"markdown","d2ad3ba0":"markdown","2e20d0dd":"markdown","b248df8b":"markdown"},"source":{"102d357a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6e4a2fb8":"import pandas as pd\nsubmission=pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\ntrain_data=pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\nmain_test_data=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntrain_data.head()","c3e57887":"import seaborn as sns\nsns.countplot(train_data['target'])","c2e7f8b7":"sentences = train_data['text'].tolist()\nlabels = train_data['target'].tolist()\ntest_sentences=main_test_data['text'].tolist()\n# Separate out the sentences and labels into training and test sets\ntraining_size = int(len(sentences) * 0.8)\n\ntraining_sentences = sentences[0:training_size]\ntesting_sentences = sentences[training_size:]\ntraining_labels = labels[0:training_size]\ntesting_labels = labels[training_size:]\n\n# Make labels into numpy arrays for use with the network later\ntraining_labels_final = np.array(training_labels)\ntesting_labels_final = np.array(testing_labels)","40887e34":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","9d59615f":"tokenizer = Tokenizer(num_words = 500, oov_token='<OOV>')\ntokenizer.fit_on_texts(training_sentences)\nword_index = tokenizer.word_index\ntraining_sequences = tokenizer.texts_to_sequences(training_sentences)\ntraining_padded = pad_sequences(training_sequences, maxlen=40, padding='post', truncating='post')\n\ntesting_sequences = tokenizer.texts_to_sequences(testing_sentences)\ntesting_padded = pad_sequences(testing_sequences, maxlen=40, padding='post', truncating='post')\nmain_test_sequence=tokenizer.texts_to_sequences(test_sentences)\nmain_test_padded=pad_sequences(main_test_sequence,maxlen=40,padding='post',truncating='post')","7dff6e9c":"from tensorflow import keras\nfrom tensorflow.keras import layers\nmodel= keras.Sequential([\n    layers.Embedding(500,16,input_length=40),\n    layers.Bidirectional(tf.keras.layers.LSTM(16,return_sequences=True)),\n    layers.Bidirectional(tf.keras.layers.LSTM(16)),\n    layers.Dense(18,activation='relu'),\n    layers.Dropout(0.2),\n    layers.BatchNormalization(),\n    layers.Dense(9,activation='relu'),\n    layers.Dropout(0.2),\n    layers.BatchNormalization(),\n    layers.Dense(1,activation='sigmoid')     \n                        \n])\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","06a97234":"from tensorflow.keras.callbacks import EarlyStopping\n\nearly_stopping=EarlyStopping(min_delta=0.001,patience=10)","29e2546f":"history=model.fit(training_padded,\n                  training_labels_final,\n                  epochs=15,\n                  validation_data=(testing_padded,testing_labels_final),\n                  callbacks=[early_stopping]\n                 )","fc5ca864":"test_pred=model.predict_classes(main_test_padded)\nsubmission['target']=test_pred\nsubmission.to_csv('submission',index=False)\nsubmission.head(5)","920a251c":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot();\nprint(\"Minimum validation loss: {}\".format(history_df['val_loss'].min()))","beb0381f":"fake_tweets = ['forest fire occured in northen hilly region  ',\n                'I hate pizza but I love spaghetti', \n                'Everything was great at football match',\n                'heavy flood occured in south texas', \n                'a major earthquake hitted the western beach ',\n                'Our planetary system is located in an outer spiral arm of the Milky Way galaxy ', \n                ]\n\nsample_sequences = tokenizer.texts_to_sequences(fake_tweets)\nfake_tweets_padded = pad_sequences(sample_sequences, padding='post', maxlen=10)           \n              \n\nclasses = model.predict(fake_tweets_padded)\n\n\nfor x in range(len(fake_tweets)):\n  print(fake_tweets[x])\n  print(int(classes[x].round()))\n  print('\\n')","2918d03a":"<H2>This is the Notebook which I created for \"Real or Not? NLP with Disaster Tweets\" competion of kaggle the aim of this notebook is to predict that Whether the tweet is about any disaster or not.this is the example of sentiment analysis. here I used keras as main deep learning framework and used tensorflow in backend. for faster computation I used the Nvidia Tesla K80 GPU provided in the kaggle kernel.\n\n","5e7476b8":"<H3>lets create some fake tweets to analyse the performance of the model.","622e13d3":"<h3>this is the output which i submitted to the competition.","703ceb91":"<H3>using early stopping to prevent overfitting of the model.","97a3c978":"<h3>the model correctly predict the sentiment of  all the fake tweets this show that model have performed really well.","2aabf2ae":"<H3>Start our work by importing required test data and train data.the train data have 5 attributes out of which will be only use \"text\" and \"target\" to train our module as the \"location\" and \"keyword\" attribute contain most of null values and that may affect model training in negative way which we dont want so will ignore them completely.","871f7733":"<H3>the train data contains almost equal amount of postive(1) and negative(0) sentiment so now it is not required to trim the train data to equal the number of positive and negative sentiments. it is important have euqal amout of both sentiments otherwise model may overfit to one of the sentiment","d2ad3ba0":"<h3>this shows that model did little over fitting after 10 epoch but the overall performance of the model is good.","2e20d0dd":"<H3>the neural network contians embedding layers which is used to encode the text data into dense vector of fixed size.the LSTM layer is a recurrent neural network (RNN) architecture that processes the entire sequence of data not only a single point as in the case of standard feed-forward neural networks. the last layer in neural network have sigmoid activation to give probibility of prediction as the final output.   ","b248df8b":"<H3>before trainig the model will do some little data pre-processing so that the input data for layers will be compatible.will do tokanization and padding to the text attribute of train data."}}