{"cell_type":{"6859d4af":"code","7fc69f41":"code","87bb9b56":"code","45cafd17":"code","9c54c966":"code","5bb3c600":"code","660e0c65":"code","3e4aee91":"code","f398ca5c":"code","770880f9":"code","804261d8":"code","b9482999":"code","7ecd03cc":"code","4b73ec28":"code","3772fb1d":"code","2291e5e2":"code","50ded289":"code","d13e2b55":"code","d51d3891":"code","65ea1368":"code","8362536b":"code","889899cd":"code","fc5ad8cf":"code","495382a4":"code","f8bb4f6a":"code","bac5ce07":"code","98a7004d":"markdown","0ea9e165":"markdown","e0a9364e":"markdown","e36a0583":"markdown","c64784ab":"markdown","30becca0":"markdown","b9e85515":"markdown","a48849dc":"markdown","ea70bf87":"markdown","9004814c":"markdown","64269e51":"markdown"},"source":{"6859d4af":"from gensim.matutils import sparse2full \nfrom gensim.corpora import Dictionary\nfrom gensim.models import TfidfModel\nfrom multiprocessing import Pool\nfrom tqdm import tqdm\nimport sqlite3 as sql\nimport pandas as pd\nimport numpy as np\nimport logging\nimport time\nimport re\n\ndb = '..\/input\/english-wikipedia-articles-20170820-sqlite\/enwiki-20170820.db'\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)","7fc69f41":"def get_query(select, db=db):\n    '''\n    1. Connects to SQLite database (db)\n    2. Executes select statement\n    3. Return results and column names\n    \n    Input: 'select * from analytics limit 2'\n    Output: ([(1, 2, 3)], ['col_1', 'col_2', 'col_3'])\n    '''\n    with sql.connect(db) as conn:\n        c = conn.cursor()\n        c.execute(select)\n        col_names = [str(name[0]).lower() for name in c.description]\n    return c.fetchall(), col_names","87bb9b56":"select = '''select * from articles limit 5'''\ndata, cols = get_query(select)\ndf = pd.DataFrame(data, columns=cols)\ndf","45cafd17":"def get_article_text(article_id):\n    '''\n    1. Construct select statement\n    2. Retrieve all section_texts associated with article_id\n    3. Join section_texts into a single string (article_text)\n    4. Return article_text\n    \n    Input: 100\n    Output: ['the','austroasiatic','languages','in',...]\n    '''\n    select = '''select section_text from articles where article_id=%d''' % article_id\n    docs, _ = get_query(select)\n    docs = [doc[0] for doc in docs]\n    doc = '\\n'.join(docs)\n    return doc","9c54c966":"article_text = get_article_text(0)\nprint(article_text)","5bb3c600":"def tokenize(text, lower=True):\n    '''\n    1. Strips apostrophes\n    2. Searches for all alpha tokens (exception for underscore)\n    3. Return list of tokens\n\n    Input: 'The 3 dogs jumped over Scott's tent!'\n    Output: ['the', 'dogs', 'jumped', 'over', 'scotts', 'tent']\n    '''\n    text = re.sub(\"'\", \"\", text)\n    if lower:\n        tokens = re.findall('''[a-z_]+''', text.lower())\n    else:\n        tokens = re.findall('''[A-Za-z_]''', text)\n    return tokens","660e0c65":"tokens = tokenize(article_text)\nprint(tokens[:5])","3e4aee91":"len(tokens)","f398ca5c":"dictionary = Dictionary([tokens])","770880f9":"def get_article_tokens(article_id):\n    '''\n    1. Construct select statement\n    2. Retrieve all section_texts associated with article_id\n    3. Join section_texts into a single string (article_text)\n    4. Tokenize article_text\n    5. Return list of tokens\n    \n    Input: 100\n    Output: ['the','austroasiatic','languages','in',...]\n    '''\n    select = '''select section_text from articles where article_id=%d''' % article_id\n    docs, _ = get_query(select)\n    docs = [doc[0] for doc in docs]\n    doc = '\\n'.join(docs)\n    tokens = tokenize(doc)\n    return tokens","804261d8":"# First, we need to grab all article_ids from the database\nselect = '''select distinct article_id from articles'''\narticle_ids, _ = get_query(select)\narticle_ids = [article_id[0] for article_id in article_ids]","b9482999":"len(article_ids)","7ecd03cc":"start = time.time()\n# Grab a random sample of 10K articles and read into memory\nsample_ids = np.random.choice(article_ids, size=10000, replace=False)\ndocs = []\nfor sample_id in tqdm(sample_ids):\n    docs.append(get_article_tokens(sample_id))\n# Train dictionary\ndictionary = Dictionary(docs)\nend = time.time()\nprint('Time to train dictionary from in-memory sample: %0.2fs' % (end - start))","4b73ec28":"start = time.time()\n# Grab a random sample of 10K articles and set up a generator\nsample_ids = np.random.choice(article_ids, size=10000, replace=False)\ndocs = (get_article_tokens(sample_id) for sample_id in sample_ids)\n# Train dictionary\ndictionary = Dictionary(docs)\nend = time.time()\nprint('Time to train dictionary from generator: %0.2fs' % (end - start))","3772fb1d":"start = time.time()\n# Grab a random sample of 10K articles and set up a pooled-process generator\nsample_ids = np.random.choice(article_ids, size=10000, replace=False)\nwith Pool(processes=4, maxtasksperchild=2048) as pool:\n    docs = pool.imap_unordered(get_article_tokens, sample_ids)\n    dictionary = Dictionary(docs)\nend = time.time()\nprint('Time to train dictionary from pooled-process generator: %0.2fs' % (end - start))","2291e5e2":"class Corpus():\n    def __init__(self, article_ids):\n        self.article_ids = article_ids\n        self.len = len(article_ids)\n\n    def __iter__(self):\n        article_ids = np.random.choice(self.article_ids, self.len, replace=False)\n        with Pool(processes=4, maxtasksperchild=2048) as pool:\n            docs = pool.imap_unordered(get_article_tokens, article_ids)\n            for doc in docs:\n                yield doc\n\n    def __len__(self):\n        return self.len","50ded289":"dictionary = Dictionary.load('..\/input\/english-wikipedia-articles-20170820-models\/enwiki_2017_08_20.dict')","d13e2b55":"len(dictionary.dfs)","d51d3891":"pd.value_counts(list(dictionary.dfs.values()), normalize=True)","65ea1368":"dictionary.filter_extremes(no_below=200, no_above=0.5, keep_n=10000000000)","8362536b":"dictionary[0]","889899cd":"tokens = get_article_tokens(0)\ndictionary.doc2bow(tokens)","fc5ad8cf":"tfidf = TfidfModel(dictionary=dictionary)","495382a4":"x = tfidf[dictionary.doc2bow(tokens)]","f8bb4f6a":"from gensim.matutils import sparse2full\n\nsparse2full(x, len(dictionary.dfs))","bac5ce07":"sparse2full(x, len(dictionary.dfs)).shape","98a7004d":"As we can see, we have four fields: article_id, title, section_title, and section_text. Each wikipedia article is broken into sections, and to reconstitute them, we'll need to grab all section_text associated with a single article_id and combine them. Let's create a function to do just that.","0ea9e165":"For various reasons, it's actually better to define an *iterable* when working with Gensim. Here is a simple template.","e0a9364e":"Now let's try it *from a generator*.","e36a0583":"Let's start by loading all documents (from a random sample) into memory, then building a dictionary.","c64784ab":"# Tutorial: Dictionary\nThis is a basic guide to efficiently training a Dictionary on the English Wikipedia dump using Gensim.","30becca0":"Now let's create a *processing pool* to retrieve and preprocess documents.","b9e85515":"Now that we can get retrieve data from the database, let's start building a dictionary. Dictionaries in Gensim are built on top of the high-performance [containers](https:\/\/docs.python.org\/3.6\/library\/collections.html) module found in base python. Essentially, we want to assign an integer ID to each unique word, and keep track of how many times that word comes up across a collection of documents (i.e., corpus). Let's create a function to split our article text into tokens.","a48849dc":"Now that we have a way to get tokens, let's create a dictionary. Once imported, all you have to do to create a dictionary is to instantiate an object and feed it an interator\/iterable. Gensim assuming a nested list of lists structure, where each item in the sublist is a string token.","ea70bf87":"So if we want to train a dictionary on the entire dataset, we can simple create an iterable of all article_ids and feed it into a Dictionary.\n\n`dictionary = Dictionary(Corpus(article_ids))`\n\nFeel free to try it, but I've already pre-trained a Dictionary on this dataset, so we'll just load it into memory and explore how to work with it.","9004814c":"That's all well and good, but we want more than just one document in our corpus! Let's create a wrapper function to combine the retrieval and tokenization step, then try a few ways of creating a dictionary.","64269e51":"Let's start by creating a helper function to pull data from the database and examine some output."}}