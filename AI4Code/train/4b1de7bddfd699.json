{"cell_type":{"d33705d2":"code","7e65df55":"code","b5560723":"code","06ef4e3d":"code","498fc1b3":"code","fbe904bc":"code","ab4d5a52":"code","957bfc30":"code","cbf79c87":"code","4c0f044f":"code","4adc0627":"code","2cc33ac3":"code","c0989309":"code","4c0c86ba":"code","8d2d3258":"code","8c9d3907":"code","fae83aef":"code","5f64b79d":"code","e2641926":"code","5167002f":"code","f92133a9":"code","f593c07b":"code","65ff65da":"code","e78639d3":"code","49819e7f":"code","54461372":"code","6a2b3646":"code","dd7a173d":"code","051d53b4":"code","bdf028e4":"code","bf611b99":"code","c57894af":"code","17f6293f":"code","cddbc9f9":"code","e40df41d":"code","7bed448f":"code","6bd441c0":"code","aa8178f7":"code","8039b5a4":"code","60293900":"code","f133b1a3":"code","b0fbf30d":"code","41bebdf2":"code","add3ac24":"code","98c2895c":"code","0a9e6823":"code","ae07e398":"code","1d975d06":"code","e3a0f71f":"code","d23e8958":"code","77dc1a01":"code","e9235ae0":"code","3326c293":"code","2970588f":"code","c4a28c3a":"code","416d00a5":"code","2fd598d1":"code","6de58fa5":"code","348386e1":"code","6e45f3dd":"code","fd29fac4":"code","9f16ef62":"code","aafe192b":"code","bd67300c":"code","e0ab4235":"code","a0fb4e32":"code","aa3c7409":"code","12c6c9e9":"code","1db23115":"code","c6453668":"code","13a6e7bc":"code","0d0d5dd8":"code","400aa1b1":"code","9266f243":"code","a5f23f66":"code","82f0691c":"code","ad860f84":"code","2058c415":"code","7045c0d2":"code","a82aec02":"code","e23d4a94":"code","18701b98":"code","55bf8de8":"code","d4bdd7d6":"code","1ee065ab":"code","01664468":"code","2a2634e6":"code","635a95a0":"code","11d17f77":"code","61a7052a":"code","b88d22a7":"code","f896afa9":"code","764e3fb9":"code","89f43865":"code","a7d45162":"code","bad0c077":"code","fa8bd64c":"code","705e7703":"code","52798a7a":"code","6e1258be":"code","d519d18e":"code","6ca68eec":"code","5b45bff7":"code","81576afc":"code","05865a1f":"code","bb3c467b":"code","29d51887":"code","ce2a2e5e":"code","ffb4e695":"code","dc82f938":"code","943a2223":"code","04319792":"code","4bb26fdb":"code","b21d1b1f":"code","ef93b077":"code","a523958b":"code","21b3d070":"code","8188f87b":"code","fe1f4a84":"code","9748a3aa":"code","95f79ec1":"code","53d4f410":"code","a34700f3":"code","04414901":"code","ecb336b8":"code","480d7b97":"code","7f98d573":"code","23fb9da7":"code","0547d7dd":"code","d7e5e2be":"code","74020881":"code","26df33a4":"code","ac9b9dec":"code","0bb4229f":"code","f2141924":"code","f7e32b0e":"code","8ad6f2a6":"code","10b518b3":"code","645bf858":"code","32709d81":"code","c2ecc350":"code","90a2c35e":"code","05c27726":"code","97a8612a":"code","260e0784":"code","dd78f350":"code","0b6c036a":"code","881c5588":"code","bbb523d5":"code","1bef3614":"code","6a2d2d43":"code","63eb38da":"code","8e976f8e":"code","637975fe":"code","3970db98":"code","ebccc35c":"code","7aaf8060":"code","326c0e5d":"code","d69f9f25":"code","ae86ce99":"code","fd7ff6ab":"code","78cc70eb":"code","4e941eaf":"code","20d61857":"code","3b9b60a2":"code","d370e900":"code","07f7b075":"code","12bc4439":"code","fd21047e":"code","e5c07311":"code","7dbce9a1":"code","8f8915c8":"code","b7bff0f5":"code","873965fb":"code","7ca1a820":"code","ceb13dd9":"code","f417d5b7":"code","31e1b503":"code","11030055":"code","9dffdaf4":"code","9b554112":"code","85a9fc68":"markdown","a6ca3e45":"markdown","b6ab2d09":"markdown","efe68ac0":"markdown","f226b47d":"markdown","bf1fcd10":"markdown","635c3918":"markdown","28e3e64a":"markdown","70f6bc3e":"markdown","1d3fcea8":"markdown","33f74903":"markdown","a04ccc6f":"markdown","a386c18f":"markdown","58775483":"markdown","88410d00":"markdown","602ec213":"markdown","0d4bed62":"markdown","ca51d714":"markdown","03e2dbf6":"markdown","51129951":"markdown","18d11209":"markdown","4d0dd8d0":"markdown","d9bbbbfd":"markdown","ef98b125":"markdown","f1f4e2bf":"markdown","ec17f48b":"markdown","34beb10d":"markdown","2e14aa35":"markdown","6cc1dfbe":"markdown","8609ec0e":"markdown","cfa6c39d":"markdown","bb6e3e1a":"markdown","dd38bd20":"markdown","d64a03f2":"markdown","545b91ce":"markdown","3d7bc06e":"markdown","3f626727":"markdown","7bb7187d":"markdown","0bb9a3ec":"markdown","60d352e5":"markdown","0851180d":"markdown","e71ecd81":"markdown","6353565d":"markdown","3c5609b3":"markdown","80e20105":"markdown","5ebf8a37":"markdown","6bbe4420":"markdown","f39b90b3":"markdown","539d136c":"markdown","4bc13c6b":"markdown","a7ed3ab8":"markdown","4406b7ee":"markdown","441fd41a":"markdown","c372eab1":"markdown","4ef54c53":"markdown","3b681483":"markdown","a6ba47f9":"markdown","fe40e9b0":"markdown","59b5a82b":"markdown","937f7427":"markdown","2ec1fa15":"markdown","6b92b18b":"markdown","5cc0ebdd":"markdown","f5c4bf17":"markdown","f0ff826f":"markdown","3f2a278f":"markdown","ab4ba89a":"markdown","e96b96d8":"markdown","622ba650":"markdown","c802fddb":"markdown","01bbc11b":"markdown","6dcd2c3f":"markdown","8d54c915":"markdown","2cc5f54b":"markdown","d3309bca":"markdown","18baa3f5":"markdown","a5e920ee":"markdown","b8bd9292":"markdown","4b3c8be0":"markdown","f96aa45f":"markdown","fb7fd260":"markdown","23e68061":"markdown","a517ea93":"markdown","1482a1e6":"markdown","66b7ad28":"markdown","5f9c0143":"markdown","291dfd57":"markdown","a15eca95":"markdown","0bf143b7":"markdown","e885ffc6":"markdown","de6d887d":"markdown","d08a0883":"markdown","f97c2e7d":"markdown","ea827a11":"markdown","e2de4a10":"markdown","2fe2bd55":"markdown","8c3482c7":"markdown","89d1604d":"markdown","21b25237":"markdown","1786dd01":"markdown","654df169":"markdown","8aef91d1":"markdown","ac725633":"markdown","8817790b":"markdown","dd4667d5":"markdown","4632e015":"markdown","b4d4df02":"markdown","5836386a":"markdown","30670a67":"markdown","b8b611a7":"markdown","24e2c226":"markdown","14dbe3bb":"markdown","77cd8b29":"markdown","3ce36fcb":"markdown","f6378586":"markdown","bad9a310":"markdown","38902d19":"markdown","6fb6b686":"markdown","28db8a56":"markdown","9dababf3":"markdown","49c93983":"markdown","f01c2b10":"markdown","ad287228":"markdown","6698507b":"markdown","918ca50b":"markdown","97095351":"markdown","105f1866":"markdown","01f22885":"markdown","bc31e23e":"markdown","fadcf20e":"markdown","eee93513":"markdown","6f61db8b":"markdown","f6e297a2":"markdown","c270e9b3":"markdown","2990c361":"markdown","13673549":"markdown","74888ec9":"markdown","9167c39e":"markdown"},"source":{"d33705d2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nimport time \n\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","7e65df55":"df = pd.read_csv('..\/input\/data.csv')\nprint(df.shape)","b5560723":"df.head()","06ef4e3d":"df.isnull().sum()","498fc1b3":"df.describe()","fbe904bc":"df.info()","ab4d5a52":"def weight_correction(df):\n    try:\n        value = float(df[:-3])\n    except:\n        value = 0\n    return value\ndf['Weight'] = df.Weight.apply(weight_correction)","957bfc30":"df.Weight = pd.to_numeric(df.Weight)","cbf79c87":"df.Weight = df.Weight.replace(0, np.nan)","4c0f044f":"def value_to_int(df_value):\n    try:\n        value = float(df_value[1:-1])\n        suffix = df_value[-1:]\n        if suffix == 'M':\n            value = value * 1000000\n        elif suffix == 'K':\n            value = value * 1000\n    except ValueError:\n        value = 0\n    return value\n\ndf['Value'] = df['Value'].apply(value_to_int)\ndf['Wage'] = df['Wage'].apply(value_to_int)\n\ndf.Value = df.Value.replace(0, np.nan)\ndf.Wage = df.Wage.replace(0, np.nan)","4adc0627":"df.Weight.isna().sum()","2cc33ac3":"df.Weight.mean()","c0989309":"df['Weight'].fillna(df.Weight.mean(), inplace = True)","4c0c86ba":"df.Height.isna().sum()","8d2d3258":"plt.figure(figsize = (20, 10))\nsns.countplot(x = 'Height', data = df)\nplt.show()","8c9d3907":"df['Height'].fillna(\"5'11\", inplace = True)","fae83aef":"wf_missing = df['Weak Foot'].isna()\nwf_missing.sum()","5f64b79d":"weak_foot_prob = df['Weak Foot'].value_counts(normalize = True)\nweak_foot_prob","e2641926":"df.loc[wf_missing, 'Weak Foot'] = np.random.choice(weak_foot_prob.index, size = wf_missing.sum(), p = weak_foot_prob.values)","5167002f":"pf_missing = df['Preferred Foot'].isna()\npf_missing.sum()","f92133a9":"foot_distribution = df['Preferred Foot'].value_counts(normalize = True)\nfoot_distribution","f593c07b":"df.loc[pf_missing, 'Preferred Foot'] = np.random.choice(foot_distribution.index, size = pf_missing.sum(), p = foot_distribution.values)","65ff65da":"fp_missing = df.Position.isna()\nfp_missing.sum()","e78639d3":"position_prob = df.Position.value_counts(normalize = True)\nposition_prob ","49819e7f":"df.loc[fp_missing, 'Position'] = np.random.choice(position_prob.index, p = position_prob.values, size = fp_missing.sum())","54461372":"fs_missing = df['Skill Moves'].isna()\nfs_missing.sum()","6a2b3646":"skill_moves_prob = df['Skill Moves'].value_counts(normalize=True)\nskill_moves_prob","dd7a173d":"df.loc[fs_missing, 'Skill Moves'] = np.random.choice(skill_moves_prob.index, p = skill_moves_prob.values, size = fs_missing.sum())","051d53b4":"bt_missing = df['Body Type'].isna()\nbt_missing.sum()","bdf028e4":"bt_prob = df['Body Type'].value_counts(normalize = True)\nbt_prob","bf611b99":"df.loc[bt_missing, 'Body Type'] = np.random.choice(['Normal', 'Lean'], p = [.63,.37], size = bt_missing.sum())","c57894af":"wage_missing = df['Wage'].isna()\nwage_missing.sum()","17f6293f":"wage_prob = df.Wage.value_counts(normalize = True)\nwage_prob","cddbc9f9":"df.loc[wage_missing, 'Wage'] = np.random.choice(wage_prob.index, p = wage_prob.values, size = wage_missing.sum())","e40df41d":"wr_missing = df['Work Rate'].isna()\nwr_missing.sum()","7bed448f":"wr_prob = df['Work Rate'].value_counts(normalize=True)\nwr_prob","6bd441c0":"df.loc[wr_missing, 'Work Rate'] = np.random.choice(wr_prob.index, p = wr_prob.values, size = wr_missing.sum())","aa8178f7":"ir_missing = df['International Reputation'].isna()\nir_missing.sum()","8039b5a4":"ir_prob = df['International Reputation'].value_counts(normalize = True)\nir_prob","60293900":"df.loc[ir_missing, 'International Reputation'] = np.random.choice(ir_prob.index, p = ir_prob.values, size = ir_missing.sum())","f133b1a3":"# filling the missing value for the continous variables for proper data visualization\n\ndf['ShortPassing'].fillna(df['ShortPassing'].mean(), inplace = True)\ndf['Volleys'].fillna(df['Volleys'].mean(), inplace = True)\ndf['Dribbling'].fillna(df['Dribbling'].mean(), inplace = True)\ndf['Curve'].fillna(df['Curve'].mean(), inplace = True)\ndf['FKAccuracy'].fillna(df['FKAccuracy'].mean(), inplace = True)\ndf['LongPassing'].fillna(df['LongPassing'].mean(), inplace = True)\ndf['BallControl'].fillna(df['BallControl'].mean(), inplace = True)\ndf['HeadingAccuracy'].fillna(df['HeadingAccuracy'].mean(), inplace = True)\ndf['Finishing'].fillna(df['Finishing'].mean(), inplace = True)\ndf['Crossing'].fillna(df['Crossing'].mean(), inplace = True)\ndf['Acceleration'].fillna(df['Acceleration'].mean(), inplace = True)\ndf['SprintSpeed'].fillna(df['SprintSpeed'].mean(), inplace = True)\ndf['Agility'].fillna(df['Agility'].mean(), inplace = True)\ndf['Reactions'].fillna(df['Reactions'].mean(), inplace = True)\ndf['Balance'].fillna(df['Balance'].mean(), inplace = True)\ndf['ShotPower'].fillna(df['ShotPower'].mean(), inplace = True)\ndf['Jumping'].fillna(df['Jumping'].mean(), inplace = True)\ndf['Stamina'].fillna(df['Stamina'].mean(), inplace = True)\ndf['Strength'].fillna(df['Strength'].mean(), inplace = True)\ndf['LongShots'].fillna(df['LongShots'].mean(), inplace = True)\ndf['Aggression'].fillna(df['Aggression'].mean(), inplace = True)\ndf['Interceptions'].fillna(df['Interceptions'].mean(), inplace = True)\ndf['Positioning'].fillna(df['Positioning'].mean(), inplace = True)\ndf['Vision'].fillna(df['Vision'].mean(), inplace = True)\ndf['Penalties'].fillna(df['Penalties'].mean(), inplace = True)\ndf['Composure'].fillna(df['Composure'].mean(), inplace = True)\ndf['Marking'].fillna(df['Marking'].mean(), inplace = True)\ndf['StandingTackle'].fillna(df['StandingTackle'].mean(), inplace = True)\ndf['SlidingTackle'].fillna(df['SlidingTackle'].mean(), inplace = True)","b0fbf30d":"df['Loaned From'].fillna('None', inplace = True)\ndf['Club'].fillna('No Club', inplace = True)","41bebdf2":"df.fillna(0, inplace = True)","add3ac24":"def defending(df):\n    return int(round((df[['Marking', 'StandingTackle', \n                               'SlidingTackle']].mean()).mean()))\n\ndef general(df):\n    return int(round((df[['HeadingAccuracy', 'Dribbling', 'Curve', \n                               'BallControl']].mean()).mean()))\n\ndef mental(df):\n    return int(round((df[['Aggression', 'Interceptions', 'Positioning', \n                               'Vision','Composure']].mean()).mean()))\n\ndef passing(df):\n    return int(round((df[['Crossing', 'ShortPassing', \n                               'LongPassing']].mean()).mean()))\n\ndef mobility(df):\n    return int(round((df[['Acceleration', 'SprintSpeed', \n                               'Agility','Reactions']].mean()).mean()))\ndef power(df):\n    return int(round((df[['Balance', 'Jumping', 'Stamina', \n                               'Strength']].mean()).mean()))\n\ndef rating(df):\n    return int(round((df[['Potential', 'Overall']].mean()).mean()))\n\ndef shooting(df):\n    return int(round((df[['Finishing', 'Volleys', 'FKAccuracy', \n                               'ShotPower','LongShots', 'Penalties']].mean()).mean()))","98c2895c":"df['Defending'] = df.apply(defending, axis = 1)\ndf['General'] = df.apply(general, axis = 1)\ndf['Mental'] = df.apply(mental, axis = 1)\ndf['Passing'] = df.apply(passing, axis = 1)\ndf['Mobility'] = df.apply(mobility, axis = 1)\ndf['Power'] = df.apply(power, axis = 1)\ndf['Rating'] = df.apply(rating, axis = 1)\ndf['Shooting'] = df.apply(shooting, axis = 1)","0a9e6823":"players = df[['Name', 'Defending', 'General', 'Mental', 'Passing',\n                'Mobility', 'Power', 'Rating', 'Shooting', 'Age',\n                'Nationality', 'Club']]","ae07e398":"plt.figure(figsize = (10, 10))\nax = sns.countplot(x = 'Skill Moves', data = df, palette = 'bright')\nax.set_title(label = 'Count of players on the basis of their skill moves', fontsize = 20)\nax.set_xlabel(xlabel = 'Rating of skill moves', fontsize = 16)\nax.set_ylabel(ylabel = 'Count', fontsize = 16)\nplt.show()","1d975d06":"labels = ['3', '2', '4', '5', '1']\nsizes = df['Weak Foot'].value_counts()\nplt.rcParams['figure.figsize'] = (10, 10)\nplt.pie(sizes, labels = labels)\nplt.title('Distribution of players on the basis of their weak foot rating', fontsize = 20)\nplt.legend()\nplt.show()","e3a0f71f":"plt.figure(figsize = (10, 10))\nax = sns.countplot(x = 'Preferred Foot', data = df, palette = 'deep')\nax.set_title(label = 'Count of players on the basis of their preferred foot', fontsize = 20)\nax.set_xlabel(xlabel = 'Preferred foot', fontsize = 16)\nax.set_ylabel(ylabel = 'Count', fontsize = 16)\nplt.show()","d23e8958":"labels = ['1', '2', '3', '4', '5']\nsizes = df['International Reputation'].value_counts()\nexplode = [0.1, 0.2, 0.3, 0.7, 0.9]\n\nplt.rcParams['figure.figsize'] = (10, 10)\nplt.pie(sizes, labels = labels, explode = explode)\nplt.title('Distribution of the international reputation of the players', fontsize = 20)\nplt.legend()\nplt.show()","77dc1a01":"plt.rcParams['figure.figsize'] = (10, 10)\nsns.distplot(df['Wage'], color = 'blue')\nplt.xlabel('Wage Range for Players', fontsize = 16)\nplt.ylabel('Count of the Players', fontsize = 16)\nplt.title('Distribution of Wages of Players', fontsize = 20)\nplt.show()","e9235ae0":"plt.figure(figsize = (20, 10))\nplt.style.use('fivethirtyeight')\nax = sns.countplot('Position', data = df, palette = 'Reds_r')\nax.set_xlabel(xlabel = 'Different positions in football', fontsize = 16)\nax.set_ylabel(ylabel = 'Count', fontsize = 16)\nax.set_title(label = 'Count of players on the basis of position', fontsize = 20)\nplt.show()","3326c293":"plt.figure(figsize = (20, 10))\nplt.style.use('fast')\nsns.countplot(x = 'Work Rate', data = df, palette = 'hls')\nplt.title('Count of players on the basis of work rate', fontsize = 20)\nplt.xlabel('Work rates', fontsize = 16)\nplt.ylabel('Count', fontsize = 16)\nplt.show()","2970588f":"x = df.Special\nplt.figure(figsize = (20, 10))\nplt.style.use('tableau-colorblind10')\nax = sns.distplot(x, bins = 50, kde = True, color = 'g')\nax.set_xlabel(xlabel = 'Special score range', fontsize = 16)\nax.set_ylabel(ylabel = 'Count',fontsize = 16)\nax.set_title(label = 'Count of players on the basis of their speciality score', fontsize = 20)\nplt.show()","c4a28c3a":"x = df.Potential\nplt.figure(figsize = (20, 10))\nplt.style.use('seaborn-paper')\nax = sns.distplot(x, bins = 50, color = 'y')\nax.set_xlabel(xlabel = \"Player\\'s potential scores\", fontsize = 16)\nax.set_ylabel(ylabel = 'Count', fontsize = 16)\nax.set_title(label = 'Count of players on the basis of potential scores', fontsize = 20)\nplt.show()","416d00a5":"sns.set(style = \"dark\", palette = \"deep\", color_codes = True)\nx = df.Overall\nplt.figure(figsize = (20, 10))\nplt.style.use('ggplot')\nax = sns.distplot(x, bins = 50, color = 'r')\nax.set_xlabel(xlabel = \"Player\\'s Scores\", fontsize = 16)\nax.set_ylabel(ylabel = 'Count', fontsize = 16)\nax.set_title(label = 'Count of players on the basis of their overall scores', fontsize = 20)\nplt.show()","2fd598d1":"plt.style.use('fast')\nsns.jointplot(x = 'Age', y = 'Potential', data = df)\nplt.show()","6de58fa5":"sns.jointplot(x = 'Special', y = 'Overall', data = df, joint_kws={'color':'orange'}, marginal_kws={'color':'blue'})\nplt.show()","348386e1":"plt.style.use('dark_background')\ndf['Nationality'].value_counts().plot.bar(color = 'orange', figsize = (30, 15))\nplt.title('Different Nations Participating in FIFA 2019', fontsize = 20)\nplt.xlabel('Name of The Country', fontsize = 16)\nplt.ylabel('Count', fontsize = 16)\nplt.xticks(fontsize = 12)\nplt.yticks(fontsize = 15)\nplt.show()","6e45f3dd":"player_features = (\n    'Acceleration', 'Aggression', 'Agility', \n    'Balance', 'BallControl', 'Composure', \n    'Crossing', 'Dribbling', 'FKAccuracy', \n    'Finishing', 'GKDiving', 'GKHandling', \n    'GKKicking', 'GKPositioning', 'GKReflexes', \n    'HeadingAccuracy', 'Interceptions', 'Jumping', \n    'LongPassing', 'LongShots', 'Marking', 'Penalties', \n    'ShortPassing', 'Volleys', 'Dribbling', 'Curve',\n    'Finishing', 'Crossing', 'SprintSpeed', 'Reactions',\n    'ShotPower', 'Stamina', 'Strength',\n    'Positioning', 'StandingTackle', 'SlidingTackle'\n)\n\nfrom math import pi\nidx = 1\nplt.style.use('seaborn-bright')\nplt.figure(figsize = (25,45))\nfor position_name, features in df.groupby(df['Position'])[player_features].mean().iterrows():\n    top_features = dict(features.nlargest(4))\n    \n    # number of variable\n    categories = top_features.keys()\n    N = len(categories)\n\n    # We are going to plot the first line of the data frame.\n    # But we need to repeat the first value to close the circular graph:\n    values = list(top_features.values())\n    values += values[:1]\n\n    # What will be the angle of each axis in the plot? (we divide the plot \/ number of variable)\n    angles = [n \/ float(N) * 2 * pi for n in range(N)]\n    angles += angles[:1]\n\n    # Initialise the spider plot\n    ax = plt.subplot(9, 3, idx, polar = True)\n\n    # Draw one axe per variable + add labels labels yet\n    plt.xticks(angles[:-1], categories, color = 'white', size = 10)\n    \n    # Draw ylabels\n    ax.set_rlabel_position(0)\n    plt.yticks([25,50,75], [\"25\",\"50\",\"75\"], color = \"white\", size = 9)\n    plt.ylim(0,100)\n    \n    plt.subplots_adjust(hspace = 0.5)\n    \n    # Plot data\n    ax.plot(angles, values, linewidth = 1, linestyle = 'solid')\n\n    # Fill area\n    ax.fill(angles, values, 'b', alpha = 0.1)\n    \n    plt.title(position_name, size = 10, y = 1.1)\n    \n    idx += 1","fd29fac4":"labels = np.array(['Acceleration', 'Strength', 'Finishing',  \n    'LongPassing', 'Penalties', \n    'ShortPassing', 'Volleys',\n    'Finishing', 'Crossing', 'SprintSpeed',\n    'ShotPower'])\n\nstats = df.loc[0, labels].values\nstats_1 = df.loc[1, labels].values\nstats_2 = df.loc[2, labels].values","9f16ef62":"angles = np.linspace(0, 2 * np.pi, len(labels), endpoint = False)\n\n# close the plot\nstats = np.concatenate((stats, [stats[0]]))\nstats_1 = np.concatenate((stats_1, [stats_1[0]]))\nstats_2 = np.concatenate((stats_2, [stats_2[0]]))\n\nangles = np.concatenate((angles, [angles[0]]))","aafe192b":"fig = plt.figure()\nax = fig.add_subplot(111, polar = True)\nax.plot(angles, stats, 'o-', linewidth = 2)\nax.fill(angles, stats, alpha = 0.25)\nax.set_thetagrids(angles * 180\/np.pi, labels)\nax.set_title([df.loc[0, 'Name']], position = (0, 1))# Changing the 1st argument in df.loc will give the radar charts for different players\nax.grid(True)","bd67300c":"fig = plt.figure()\nax_1 = fig.add_subplot(111, polar = True)\nax_1.plot(angles, stats_1, 'o-', linewidth = 2)\nax_1.fill(angles, stats_1, alpha = 0.25)\nax_1.set_thetagrids(angles * 180\/np.pi, labels)\nax_1.set_title([df.loc[1, 'Name']], position = (0, 1))\nax_1.grid(True)","e0ab4235":"fig = plt.figure()\nax_2 = fig.add_subplot(111, polar = True)\nax_2.plot(angles, stats_2, 'o-', linewidth = 2)\nax_2.fill(angles, stats_2, alpha = 0.25)\nax_2.set_thetagrids(angles * 180\/np.pi, labels)\nax_2.set_title([df.loc[2, 'Name']], position = (0, 1))\nax_2.grid(True)","a0fb4e32":"plt.figure(figsize = (10, 10))\nplt.style.use('seaborn-darkgrid')\nplt.scatter(df['Overall'], df['International Reputation'])\nplt.xlabel('Overall Ratings', fontsize = 16)\nplt.ylabel('International Reputation', fontsize = 16)\nplt.title('Ratings vs Reputation', fontsize = 20)\nplt.show()","aa3c7409":"df.loc[df.groupby(df['Position'])['Potential'].idxmax()][['Name', 'Position', 'Overall', 'Potential', 'Age', 'Nationality', 'Club']]","12c6c9e9":"df.loc[df.groupby(df['Position'])['Overall'].idxmax()][['Name', 'Position', 'Overall', 'Age', 'Nationality', 'Club']]","1db23115":"df.groupby('Age')['Overall'].mean().plot(figsize = (20, 10))\nplt.xlabel('Age', fontsize = 16)\nplt.ylabel('Mean', fontsize = 16)\nplt.title('Mean of overall, age-wise', fontsize = 20)\nplt.show()","c6453668":"df['Age'].value_counts()","13a6e7bc":"plt.style.use('seaborn-deep')\nplt.figure(figsize = (20, 10))\nsns.countplot(x = 'Age', data = df)\nplt.xlabel('Age', fontsize = 16)\nplt.ylabel('Count', fontsize = 16)\nplt.title('Count of players, age-wise', fontsize = 20)\nplt.show()","0d0d5dd8":"df[df['Age'] > 40][['Name', 'Overall', 'Age', 'Nationality']]","400aa1b1":"df.sort_values('Age', ascending = True)[['Name', 'Age', 'Club', 'Nationality']].head(15)","9266f243":"df.sort_values('Age', ascending = False)[['Name', 'Age', 'Club', 'Nationality']].head(15)","a5f23f66":"df[df['Preferred Foot'] == 'Left'][['Name', 'Age', 'Club', 'Nationality']].head(10)","82f0691c":"df[df['Preferred Foot'] == 'Right'][['Name', 'Age', 'Club', 'Nationality']].head(10)","ad860f84":"d = {'Overall': 'Average_Rating'}\nbest_overall_club_df = df.groupby('Club').agg({'Overall' : 'mean'}).rename(columns = d)\nclubs = best_overall_club_df.Average_Rating.nlargest(5).index\nprint(clubs)","2058c415":"attck_list = ['Shooting', 'Power', 'Passing']\n\nbest_attack_df = players.groupby('Club')[attck_list].sum().sum(axis = 1)\nclubs = best_attack_df.nlargest(6).index\nprint(clubs)","7045c0d2":"best_defense_df = players.groupby('Club')['Defending'].sum()\nclubs = best_defense_df.nlargest(6).index\nprint(clubs)","a82aec02":"df['Club'].value_counts().head(15)","e23d4a94":"some_clubs = ('Manchester United', 'Arsenal', 'Juventus', 'Paris Saint-Germain', 'Napoli', 'Manchester City',\n             'Tottenham Hotspur', 'FC Barcelona', 'Inter', 'Chelsea', 'Real Madrid', 'Borussia Dortmund', 'Liverpool', 'Roma', 'Ajax')","18701b98":"df_clubs = df.loc[df['Club'].isin(some_clubs) & df['Overall']]\nplt.rcParams['figure.figsize'] = (20, 10)\nax = sns.boxenplot(x = df_clubs['Club'], y = df_clubs['Overall'], palette = 'rocket')\nax.set_xlabel(xlabel = 'Some popular clubs', fontsize = 16)\nax.set_ylabel(ylabel = 'Overall score', fontsize = 16)\nax.set_title(label = 'Distribution of overall score in different popular clubs', fontsize = 20)\nplt.xticks(rotation = 90)\nplt.show()","55bf8de8":"df_club = df.loc[df['Club'].isin(some_clubs) & df['Age']]\nplt.rcParams['figure.figsize'] = (20, 10)\nax = sns.boxenplot(x = 'Club', y = 'Age', data = df_club, palette = 'magma')\nax.set_xlabel(xlabel = 'Some popular clubs', fontsize = 16)\nax.set_ylabel(ylabel = 'Distribution', fontsize = 16)\nax.set_title(label = 'Distribution of ages in some popular clubs', fontsize = 20)\nplt.xticks(rotation = 90)\nplt.show()","d4bdd7d6":"df_club = df.loc[df['Club'].isin(some_clubs) & df['Wage']]\nplt.rcParams['figure.figsize'] = (20, 10)\nax = sns.boxenplot(x = 'Club', y = 'Wage', data = df_club, palette = 'Reds')\nax.set_xlabel(xlabel = 'Some popular clubs', fontsize = 16)\nax.set_ylabel(ylabel = 'Distribution', fontsize = 16)\nax.set_title(label = 'Disstribution of wages in some popular clubs', fontsize = 20)\nplt.xticks(rotation = 90)\nplt.show()","1ee065ab":"df_club = df.loc[df['Club'].isin(some_clubs) & df['International Reputation']]\nplt.rcParams['figure.figsize'] = (20, 10)\nax = sns.boxenplot(x = 'Club', y = 'International Reputation', data = df_club, palette = 'bright')\nax.set_xlabel(xlabel = 'Some popular clubs', fontsize = 16)\nax.set_ylabel(ylabel = 'International reputation', fontsize = 16)\nax.set_title(label = 'Distribution of international reputation in some popular clubs', fontsize = 20)\nplt.xticks(rotation = 90)\nplt.show()","01664468":"df_clubs = df.loc[df['Club'].isin(some_clubs) & df['Weight']]\nplt.rcParams['figure.figsize'] = (20, 10)\nax = sns.boxenplot(x = 'Club', y = 'Weight', data = df_clubs, palette = 'rainbow')\nax.set_xlabel(xlabel = 'Some popular clubs', fontsize = 16)\nax.set_ylabel(ylabel = 'Weight in lbs', fontsize = 16)\nax.set_title(label = 'Distribution of weight in different popular clubs', fontsize = 20)\nplt.xticks(rotation = 90)\nplt.show()","2a2634e6":"df.groupby(df['Club'])['Nationality'].nunique().sort_values(ascending = False).head(11)","635a95a0":"df.groupby(df['Club'])['Nationality'].nunique().sort_values(ascending = True).head(10)","11d17f77":"d = {'Overall': 'Average_Rating'}\nbest_overall_country_df = df.groupby('Nationality').agg({'Overall':'mean'}).rename(columns = d)\nnations = best_overall_country_df.Average_Rating.nlargest(5).index\nprint(nations)","61a7052a":"best_attack_nation_df = players.groupby('Nationality')[attck_list].sum().mean(axis = 1)\nnations = best_attack_nation_df.nlargest(5).index\nprint(nations)","b88d22a7":"best_defense_nation_df = players.groupby('Nationality')['Defending'].sum()\nnations = best_defense_nation_df.nlargest(5).index\nprint(nations)","f896afa9":"df['Nationality'].value_counts().head(15)","764e3fb9":"some_countries = ('England', 'Germany', 'Spain', 'Argentina', 'France', 'Brazil', 'Italy', 'Colombia', 'Japan', 'Netherlands')","89f43865":"df_countries = df.loc[df['Nationality'].isin(some_countries) & df['Weight']]\nplt.rcParams['figure.figsize'] = (20, 10)\nax = sns.boxenplot(x = df_countries['Nationality'], y = df_countries['Weight'], palette = 'cubehelix')\nax.set_xlabel(xlabel = 'Countries', fontsize = 16)\nax.set_ylabel(ylabel = 'Weight in lbs', fontsize = 16)\nax.set_title(label = 'Distribution of weight of players from different countries', fontsize = 20)\nplt.show()","a7d45162":"df_countries = df.loc[df['Nationality'].isin(some_countries) & df['Overall']]\nplt.rcParams['figure.figsize'] = (20, 10)\nax = sns.boxenplot(x = df_countries['Nationality'], y = df_countries['Overall'], palette = 'spring')\nax.set_xlabel(xlabel = 'Countries', fontsize = 16)\nax.set_ylabel(ylabel = 'Overall scores', fontsize = 16)\nax.set_title(label = 'Distribution of overall scores of players from different countries', fontsize = 20)\nplt.show()","bad0c077":"df_countries = df.loc[df['Nationality'].isin(some_countries) & df['Wage']]\nplt.rcParams['figure.figsize'] = (20, 10)\nax = sns.boxenplot(x = df_countries['Nationality'], y = df_countries['Wage'], palette = 'hot')\nax.set_xlabel(xlabel = 'Countries', fontsize = 16)\nax.set_ylabel(ylabel = 'Wage', fontsize = 16)\nax.set_title(label = 'Distribution of wages of players from different countries', fontsize = 20)\nplt.show()","fa8bd64c":"df_countries = df.loc[df['Nationality'].isin(some_countries) & df['International Reputation']]\nplt.rcParams['figure.figsize'] = (20, 10)\nax = sns.boxenplot(x = df_countries['Nationality'], y = df_countries['International Reputation'], palette = 'autumn')\nax.set_xlabel(xlabel = 'Countries', fontsize = 16)\nax.set_ylabel(ylabel = 'International reputation', fontsize = 16)\nax.set_title(label = 'Distribution of international repuatation of players from different countries', fontsize = 20)\nplt.show()","705e7703":"selected_columns = ['Name', 'Age', 'Nationality', 'Overall', 'Potential', 'Club', 'Value',\n                    'Wage', 'Special', 'Preferred Foot', 'International Reputation', 'Weak Foot',\n                    'Skill Moves', 'Work Rate', 'Body Type', 'Position', 'Height', 'Weight',\n                    'Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys', 'Dribbling',\n                    'Curve', 'FKAccuracy', 'LongPassing', 'BallControl', 'Acceleration',\n                    'SprintSpeed', 'Agility', 'Reactions', 'Balance', 'ShotPower',\n                    'Jumping', 'Stamina', 'Strength', 'LongShots', 'Aggression',\n                    'Interceptions', 'Positioning', 'Vision', 'Penalties', 'Composure',\n                    'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving', 'GKHandling',\n                    'GKKicking', 'GKPositioning', 'GKReflexes', 'Release Clause']\n\ndf_selected = pd.DataFrame(df, columns = selected_columns)\ndf_selected.columns","52798a7a":"df_selected.sample(5)","6e1258be":"plt.rcParams['figure.figsize'] = (30, 30)\nsns.heatmap(df_selected[['Age', 'Nationality', 'Overall', 'Potential', 'Club', 'Value',\n                    'Wage', 'Special', 'Preferred Foot', 'International Reputation', 'Weak Foot',\n                    'Skill Moves', 'Work Rate', 'Body Type', 'Position', 'Height', 'Weight',\n                    'Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys', 'Dribbling',\n                    'Curve', 'FKAccuracy', 'LongPassing', 'BallControl', 'Acceleration',\n                    'SprintSpeed', 'Agility', 'Reactions', 'Balance', 'ShotPower',\n                    'Jumping', 'Stamina', 'Strength', 'LongShots', 'Aggression',\n                    'Interceptions', 'Positioning', 'Vision', 'Penalties', 'Composure',\n                    'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving', 'GKHandling',\n                    'GKKicking', 'GKPositioning', 'GKReflexes', 'Release Clause']].corr(), annot = True)\n\nplt.title('Heatmap of the dataset', fontsize = 30)\nplt.show()","d519d18e":"GK_attributes = df[['GKPositioning','GKKicking','GKHandling','GKReflexes','GKDiving']]\n\nplt.rcParams['figure.figsize'] = (10, 10)\nsns.heatmap(GK_attributes.corr(), annot = True)\n\nplt.title('correlations between attributes of goalkeeper', fontsize = 30)\nplt.show()","6ca68eec":"dummy_df = df.copy()","5b45bff7":"print(dummy_df.keys())\nprint(dummy_df.shape)\nprint(dummy_df.select_dtypes(['O']).shape)        \nprint(dummy_df.select_dtypes([np.number]).shape)","81576afc":"dummy_df.drop(['GKPositioning', 'GKKicking', 'GKHandling', 'GKReflexes'], inplace = True, axis = 1)","05865a1f":"dummy_df.drop(['Rating'], inplace = True, axis = 1)","bb3c467b":"dummy_df.drop(['StandingTackle', 'SlidingTackle'], inplace = True, axis = 1)","29d51887":"dummy_df.drop(['Interceptions'], inplace = True, axis = 1)","ce2a2e5e":"dummy_df.drop(['BallControl'], inplace = True, axis = 1)","ffb4e695":"dummy_df.drop(['LongShots'], inplace = True, axis = 1)","dc82f938":"string_columns = dummy_df.select_dtypes(['O']).columns           \nstring_columns","943a2223":"dummy_df.drop(['Photo', 'Flag', 'Club Logo', 'Real Face', 'Joined', 'Loaned From', 'Contract Valid Until', 'Height', 'Release Clause'],inplace = True,axis = 1)","04319792":"dummy_df.drop(['Preferred Foot', 'Nationality'], inplace = True, axis = 1)","4bb26fdb":"dummy_df[['w_r_attack','w_r_defence']] = dummy_df['Work Rate'].str.split('\/',expand=True)","b21d1b1f":"dummy_df.w_r_attack = dummy_df.w_r_attack.str.strip()\ndummy_df.w_r_defence = dummy_df.w_r_defence.str.strip()","ef93b077":"dummy_df.w_r_defence = dummy_df.w_r_defence.map({'High':3,'Medium':2,'Low':1})\ndummy_df.w_r_attack = dummy_df.w_r_attack.map({'High':3,'Medium':2,'Low':1})","a523958b":"dummy_df.drop(['Work Rate'],inplace = True,axis = 1)","21b3d070":"dummy_df.drop(['LS', 'ST', 'RS', 'LW', 'LF', 'CF', 'RF', 'RW',\n       'LAM', 'CAM', 'RAM', 'LM', 'LCM', 'CM', 'RCM', 'RM', 'LWB', 'LDM',\n       'CDM', 'RDM', 'RWB', 'LB', 'LCB', 'CB', 'RCB', 'RB'], inplace = True, axis = 1)","8188f87b":"dummy_df.drop(['Body Type'], inplace = True, axis = 1)","fe1f4a84":"string_columns = dummy_df.select_dtypes(['O']).columns           \nstring_columns","9748a3aa":"le = LabelEncoder()\nfor column in string_columns[1:]:                                    \n    dummy_df[column] = le.fit_transform(dummy_df[column])","95f79ec1":"string_columns = dummy_df.select_dtypes(['O']).columns      \nstring_columns","53d4f410":"number_columns = dummy_df.select_dtypes([np.number]).columns           \nnumber_columns","a34700f3":"dummy_df.drop(['Unnamed: 0', 'Jersey Number', 'ID'], inplace = True, axis = 1)","04414901":"dummy_df","ecb336b8":"from sklearn.preprocessing import MinMaxScaler","480d7b97":"dummy_df_scaled = dummy_df.copy()","7f98d573":"output_2 = dummy_df['Overall']\ndummy_df.drop(['Overall'], inplace = True, axis = 1)","23fb9da7":"scaling = MinMaxScaler(copy = False).fit(dummy_df_scaled.iloc[:, 1:])\ndummy_df_scaled.iloc[:, 1:] = scaling.transform(dummy_df_scaled.iloc[:, 1:])","0547d7dd":"dummy_df_scaled","d7e5e2be":"from sklearn.model_selection import train_test_split,cross_val_score,KFold\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn.metrics import mean_squared_error\nfrom IPython.display import display","74020881":"def model_performance(model, data, output_df):\n    \n    # Splitting the data into train and test set\n    x_train, x_test, y_train, y_test = train_test_split(data, output_df)\n    \n    train_names = x_train.iloc[:,0]\n    x_train = x_train.iloc[:,1:]\n    test_names = x_test.iloc[:,0]\n    x_test = x_test.iloc[:,1:]\n    \n    start = time.time()\n    model.fit(x_train, y_train)\n    print(\"fitting time : {}\".format(time.time()-start))\n\n    start = time.time()\n    y_pred = model.predict(x_test)\n    print(\"\\nModel's score is :\", model.score(x_test, y_test))          # Returns the coefficient of determination R^2 of the prediction.\n    print(\"testing time : {}\".format(time.time() - start))\n    \n    print('\\nRMSE : '+str(np.sqrt(mean_squared_error(y_test, y_pred))))\n    \n    #Cross-validation score\n    crossScore = cross_val_score(model, X = data.iloc[:,1:], y = output_df, cv = KFold(n_splits = 5, shuffle = True)).mean()\n    print(\"\\nThe cross-validation score is:\", crossScore) \n    \n    comparisionRDF = pd.DataFrame(y_test)\n    comparisionRDF['predicted'] = y_pred\n    comparisionRDF['player Name'] = test_names\n    comparisionRDF['error in Prediction'] = np.abs(comparisionRDF['predicted'] - comparisionRDF['Overall'])\n    print(\"\\nThe error in the prediction for each player is:\")\n    print(comparisionRDF)\n    \n    #Limits of error\n    print(\"\\nLimit of error of our model is ({},{})\".format(comparisionRDF['error in Prediction'].min(),comparisionRDF['error in Prediction'].max()))\n    \n    model.get_params()              \n    weights = model.coef_           # Coefficients of all features\/columns\n    bias = model.intercept_         # Constant term in a linear line equation\n    print(\"\\nweights are:\",weights)\n    print(\"Constant is :\",bias)\n    \n    print(\"\\nThe importance of each feature for the model is:\")\n    perm = PermutationImportance(model).fit(x_test, y_test)\n    display(eli5.show_weights(perm, feature_names = x_test.columns.tolist()))\n    \n    # Visualising the results\n    plt.figure(figsize=(20, 10))\n    sns.regplot(y_pred, y_test, scatter_kws = {'color':'lime'}, line_kws = {'color':'red'})\n    plt.xlabel('Predictions')\n    plt.ylabel('Overall')\n    plt.title(\"Prediction of overall rating\")\n    plt.show()\n    \n    # Visualising the residual plot\n    plt.figure(figsize=(20, 10))\n    sns.scatterplot(y_pred, y_test - y_pred)\n    plt.xlabel('Predictions')\n    plt.ylabel('Residual')\n    plt.title(\"Residual plot\")\n    plt.show()","26df33a4":"output = dummy_df_scaled['Overall']\ndummy_df_scaled.drop(['Overall'], inplace = True, axis = 1)","ac9b9dec":"from sklearn.linear_model import LinearRegression","0bb4229f":"model = LinearRegression() \nmodel_performance(model, dummy_df_scaled, output)\nmodel_performance(model, dummy_df, output_2)","f2141924":"from sklearn.linear_model import Ridge","f7e32b0e":"model = Ridge()\nmodel_performance(model, dummy_df_scaled, output)\nmodel_performance(model, dummy_df, output_2)","8ad6f2a6":"model = Ridge(alpha = 0.05)\nmodel_performance(model, dummy_df_scaled, output)\nmodel_performance(model, dummy_df, output_2)","10b518b3":"model = Ridge(alpha = 0.5)\nmodel_performance(model, dummy_df_scaled, output)\nmodel_performance(model, dummy_df, output_2)","645bf858":"model = Ridge(alpha = 5)\nmodel_performance(model, dummy_df_scaled, output)\nmodel_performance(model, dummy_df, output_2)","32709d81":"from sklearn.linear_model import Lasso","c2ecc350":"model = Lasso(alpha = 0.05)\nmodel_performance(model, dummy_df_scaled, output)\nmodel_performance(model, dummy_df, output_2)","90a2c35e":"model = Lasso(alpha = 0.05, max_iter = 10000)\nmodel_performance(model, dummy_df_scaled, output)\nmodel_performance(model, dummy_df, output_2)","05c27726":"model = Lasso(alpha = 0.5, max_iter = 10000)\nmodel_performance(model, dummy_df_scaled, output)\nmodel_performance(model, dummy_df, output_2)","97a8612a":"model = Lasso(alpha = 5, max_iter = 10000)\nmodel_performance(model, dummy_df_scaled, output)\nmodel_performance(model, dummy_df, output_2)","260e0784":"from sklearn.linear_model import ElasticNet","dd78f350":"model = ElasticNet(alpha = 1, l1_ratio = 0.5)\nmodel_performance(model, dummy_df_scaled, output)\nmodel_performance(model, dummy_df, output_2)","0b6c036a":"model = ElasticNet(alpha = 1, l1_ratio = 0.3)\nmodel_performance(model, dummy_df_scaled, output)\nmodel_performance(model, dummy_df, output_2)","881c5588":"model = ElasticNet(alpha = 1, l1_ratio = 0.7)\nmodel_performance(model, dummy_df_scaled, output)\nmodel_performance(model, dummy_df, output_2)","bbb523d5":"from sklearn.svm import LinearSVR","1bef3614":"model = LinearSVR()\nmodel_performance(model, dummy_df_scaled, output)\nmodel_performance(model, dummy_df, output_2)","6a2d2d43":"from sklearn.model_selection import RandomizedSearchCV","63eb38da":"x_train, x_test, y_train, y_test = train_test_split(dummy_df_scaled, output)\n    \ntrain_names = x_train.iloc[:,0]\nx_train = x_train.iloc[:,1:]\ntest_names = x_test.iloc[:,0]\nx_test = x_test.iloc[:,1:]","8e976f8e":"from sklearn.ensemble import RandomForestRegressor","637975fe":"model = RandomForestRegressor()\nprint('Parameters currently in use:\\n')\nprint(model.get_params())","3970db98":"# Create the random grid\nrandom_grid = {'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],# Number of trees in random forest\n               'max_features': ['auto', 'sqrt'],# Number of features to consider at every split\n               'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],# Maximum number of levels in tree\n               'min_samples_split': [2, 5, 10, 20],# Minimum number of samples required to split a node\n               'min_samples_leaf': [1, 2, 4, 10, 25],# Minimum number of samples required at each leaf node \n               'bootstrap': [True, False]}# Method of selecting samples for training each tree\nprint(random_grid)\n        \n# Use the random grid to search for best hyperparameters\n# Random search of parameters, using 3 fold cross validation, \n# search across 5 different combinations\nmodel_random = RandomizedSearchCV(estimator = model, param_distributions = random_grid, n_iter = 2, cv = 3, verbose = 1, n_jobs = 1)\n        \n# Fit the random search model\nmodel_random.fit(x_train, y_train)\n\nprint(\"\\nThe best parameters for the model:\", model_random.best_params_)\n        \nmodel = model_random.best_estimator_\n        \nstart = time.time()\nmodel.fit(x_train, y_train)\nprint(\"fitting time : {}\".format(time.time()-start))\n        \nstart = time.time()\ny_pred = model.predict(x_test)\nprint(\"testing time : {}\".format(time.time() - start))\n\nprint(\"\\nThe importance of each feature for the model:\", model.feature_importances_)","ebccc35c":"print(\"\\nModel's score is :\", model.score(x_test, y_test))\nprint('\\nRMSE : '+str(np.sqrt(mean_squared_error(y_test, y_pred))))\n        \ncrossScore = cross_val_score(model, X = dummy_df_scaled.iloc[:,1:], y = output, cv = KFold(n_splits = 5, shuffle = True)).mean()\nprint(\"\\nThe cross-validation score is:\", crossScore)\n        \ncomparisionRDF = pd.DataFrame(y_test)\ncomparisionRDF['predicted'] = y_pred\ncomparisionRDF['player Name'] = test_names\ncomparisionRDF['error in Prediction'] = np.abs(comparisionRDF['predicted'] - comparisionRDF['Overall'])\nprint(\"\\nThe error in the prediction for each player is:\")\nprint(comparisionRDF)\n    \n#Limits of error\nprint(\"\\nLimit of error of our model is ({},{})\".format(comparisionRDF['error in Prediction'].min(),comparisionRDF['error in Prediction'].max()))\n\nprint('\\nParameters currently in use:\\n')\nprint(model.get_params())\n    \n#Visualising the results\nplt.figure(figsize=(20, 10))\nsns.regplot(y_pred, y_test, scatter_kws = {'color':'lime'}, line_kws = {'color':'red'})\nplt.xlabel('Predictions')\nplt.ylabel('Overall')\nplt.title(\"Prediction of overall rating\")\nplt.show()\n\n#Visualising the residual plot\nplt.figure(figsize=(20, 10))\nsns.scatterplot(y_pred, y_test - y_pred)\nplt.xlabel('Predictions')\nplt.ylabel('Residual')\nplt.title(\"Residual plot\")\nplt.show()","7aaf8060":"from sklearn.ensemble import ExtraTreesRegressor","326c0e5d":"model = ExtraTreesRegressor()\nprint('Parameters currently in use:\\n')\nprint(model.get_params())","d69f9f25":"# Create the random grid\nrandom_grid = {'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],# Number of trees in random forest\n               'max_features': ['auto', 'sqrt'],# Number of features to consider at every split\n               'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],# Maximum number of levels in tree\n               'min_samples_split': [2, 5, 10, 20],# Minimum number of samples required to split a node\n               'min_samples_leaf': [1, 2, 4, 10, 25],# Minimum number of samples required at each leaf node \n               'bootstrap': [True, False]}# Method of selecting samples for training each tree\nprint(random_grid)\n        \n# Use the random grid to search for best hyperparameters\n# Random search of parameters, using 3 fold cross validation, \n# search across 10 different combinations\nmodel_random = RandomizedSearchCV(estimator = model, param_distributions = random_grid, n_iter = 2, cv = 3, verbose = 1, n_jobs = 1)\n        \n# Fit the random search model\nmodel_random.fit(x_train, y_train)\n\n\nprint(\"\\nThe best parameters for the model:\", model_random.best_params_)\n        \nmodel = model_random.best_estimator_\n        \nstart = time.time()\nmodel.fit(x_train, y_train)\nprint(\"fitting time : {}\".format(time.time()-start))\n        \nstart = time.time()\ny_pred = model.predict(x_test)\nprint(\"testing time : {}\".format(time.time() - start))\n\nprint(\"\\nThe importance of each feature for the model:\", model.feature_importances_)","ae86ce99":"print(\"\\nModel's score is :\", model.score(x_test, y_test))\nprint('\\nRMSE : '+str(np.sqrt(mean_squared_error(y_test, y_pred))))\n        \ncrossScore = cross_val_score(model, X = dummy_df_scaled.iloc[:,1:], y = output, cv = KFold(n_splits = 5, shuffle = True)).mean()\nprint(\"\\nThe cross-validation score is:\", crossScore)\n        \ncomparisionRDF = pd.DataFrame(y_test)\ncomparisionRDF['predicted'] = y_pred\ncomparisionRDF['player Name'] = test_names\ncomparisionRDF['error in Prediction'] = np.abs(comparisionRDF['predicted'] - comparisionRDF['Overall'])\nprint(\"\\nThe error in the prediction for each player is:\")\nprint(comparisionRDF)\n    \n#Limits of error\nprint(\"\\nLimit of error of our model is ({},{})\".format(comparisionRDF['error in Prediction'].min(),comparisionRDF['error in Prediction'].max()))\n\nprint('\\nParameters currently in use:\\n')\nprint(model.get_params())\n    \n#Visualising the results\nplt.figure(figsize=(20, 10))\nsns.regplot(y_pred, y_test, scatter_kws = {'color':'lime'}, line_kws = {'color':'red'})\nplt.xlabel('Predictions')\nplt.ylabel('Overall')\nplt.title(\"Prediction of overall rating\")\nplt.show()\n\n#Visualising the residual plot\nplt.figure(figsize=(20, 10))\nsns.scatterplot(y_pred, y_test - y_pred)\nplt.xlabel('Predictions')\nplt.ylabel('Residual')\nplt.title(\"Residual plot\")\nplt.show()","fd7ff6ab":"from sklearn.ensemble import AdaBoostRegressor","78cc70eb":"model = AdaBoostRegressor()\nprint('Parameters currently in use:\\n')\nprint(model.get_params())","4e941eaf":"# Create the random grid\nrandom_grid = {'n_estimators': [int(x) for x in np.linspace(start = 20, stop = 200, num = 10)],# Maximum number of trees at which boosting is terminated\n               'learning_rate': [0.01, 0.05, 0.1, 0.3, 1],# Learning rate of the algorithm\n               'loss': ['linear', 'square', 'exponential']}# Loss function to be used for updating the weights\nprint(random_grid)\n        \n# Use the random grid to search for best hyperparameters\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nmodel_random = RandomizedSearchCV(estimator = model, param_distributions = random_grid, n_iter = 5, cv = 3, verbose = 1, n_jobs = 1)\n\n# Fit the random search model\nmodel_random.fit(x_train, y_train)\n        \nprint(\"\\nThe best parameters for the model:\", model_random.best_params_)\n        \nmodel = model_random.best_estimator_\n        \nstart = time.time()\nmodel.fit(x_train, y_train)\nprint(\"fitting time : {}\".format(time.time()-start))\n        \nstart = time.time()\ny_pred = model.predict(x_test)\nprint(\"testing time : {}\".format(time.time() - start))        \n        \nprint(\"\\nThe weights given to different estimators:\", model.estimator_weights_)\n        \nprint(\"\\nThe errors of different estimators:\", model.estimator_errors_)","20d61857":"print(\"\\nModel's score is :\", model.score(x_test, y_test))\nprint('\\nRMSE : '+str(np.sqrt(mean_squared_error(y_test, y_pred))))\n        \ncrossScore = cross_val_score(model, X = dummy_df_scaled.iloc[:,1:], y = output, cv = KFold(n_splits = 5, shuffle = True)).mean()\nprint(\"\\nThe cross-validation score is:\", crossScore)\n        \ncomparisionRDF = pd.DataFrame(y_test)\ncomparisionRDF['predicted'] = y_pred\ncomparisionRDF['player Name'] = test_names\ncomparisionRDF['error in Prediction'] = np.abs(comparisionRDF['predicted'] - comparisionRDF['Overall'])\nprint(\"\\nThe error in the prediction for each player is:\")\nprint(comparisionRDF)\n    \n#Limits of error\nprint(\"\\nLimit of error of our model is ({},{})\".format(comparisionRDF['error in Prediction'].min(),comparisionRDF['error in Prediction'].max()))\n\nprint('\\nParameters currently in use:\\n')\nprint(model.get_params())\n    \n#Visualising the results\nplt.figure(figsize=(20, 10))\nsns.regplot(y_pred, y_test, scatter_kws = {'color':'lime'}, line_kws = {'color':'red'})\nplt.xlabel('Predictions')\nplt.ylabel('Overall')\nplt.title(\"Prediction of overall rating\")\nplt.show()\n\n#Visualising the residual plot\nplt.figure(figsize=(20, 10))\nsns.scatterplot(y_pred, y_test - y_pred)\nplt.xlabel('Predictions')\nplt.ylabel('Residual')\nplt.title(\"Residual plot\")\nplt.show()","3b9b60a2":"from sklearn.ensemble import GradientBoostingRegressor","d370e900":"model = GradientBoostingRegressor()\nprint('Parameters currently in use:\\n')\nprint(model.get_params())","07f7b075":"# Create the random grid\nrandom_grid = {'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],# Number of trees \n               'max_features': ['auto', 'sqrt'],# Number of features to consider at every split\n               'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],# Maximum number of levels in tree\n               'min_samples_split': [2, 5, 10, 20],# Minimum number of samples required to split a node\n               'min_samples_leaf': [1, 2, 4, 10, 25]}# Minimum number of samples required at each leaf node\nprint(random_grid)\n        \n# Use the random grid to search for best hyperparameters\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nmodel_random = RandomizedSearchCV(estimator = model, param_distributions = random_grid, n_iter = 5, cv = 3, verbose = 1, n_jobs = 1)\n\n# Fit the random search model\nmodel_random.fit(x_train, y_train)\n        \nprint(\"\\nThe best parameters for the model:\", model_random.best_params_)\n        \nmodel = model_random.best_estimator_\n        \nstart = time.time()\nmodel.fit(x_train, y_train)\nprint(\"fitting time : {}\".format(time.time()-start))\n        \nstart = time.time()\ny_pred = model.predict(x_test)\nprint(\"testing time : {}\".format(time.time() - start))","12bc4439":"print(\"\\nModel's score is :\", model.score(x_test, y_test))\nprint('\\nRMSE : '+str(np.sqrt(mean_squared_error(y_test, y_pred))))\n        \ncrossScore = cross_val_score(model, X = dummy_df_scaled.iloc[:,1:], y = output, cv = KFold(n_splits = 5, shuffle = True)).mean()\nprint(\"\\nThe cross-validation score is:\", crossScore)\n        \ncomparisionRDF = pd.DataFrame(y_test)\ncomparisionRDF['predicted'] = y_pred\ncomparisionRDF['player Name'] = test_names\ncomparisionRDF['error in Prediction'] = np.abs(comparisionRDF['predicted'] - comparisionRDF['Overall'])\nprint(\"\\nThe error in the prediction for each player is:\")\nprint(comparisionRDF)\n    \n#Limits of error\nprint(\"\\nLimit of error of our model is ({},{})\".format(comparisionRDF['error in Prediction'].min(),comparisionRDF['error in Prediction'].max()))\n\nprint('\\nParameters currently in use:\\n')\nprint(model.get_params())\n    \n#Visualising the results\nplt.figure(figsize=(20, 10))\nsns.regplot(y_pred, y_test, scatter_kws = {'color':'lime'}, line_kws = {'color':'red'})\nplt.xlabel('Predictions')\nplt.ylabel('Overall')\nplt.title(\"Prediction of overall rating\")\nplt.show()\n\n#Visualising the residual plot\nplt.figure(figsize=(20, 10))\nsns.scatterplot(y_pred, y_test - y_pred)\nplt.xlabel('Predictions')\nplt.ylabel('Residual')\nplt.title(\"Residual plot\")\nplt.show()","fd21047e":"import xgboost as xgb","e5c07311":"model = xgb.XGBRegressor()\nprint('Parameters currently in use:\\n')\nprint(model.get_params())","7dbce9a1":"# Create the random grid\nrandom_grid = {'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],# Number of trees\n               'learning_rate': [0.01, 0.05, 0.1, 0.3, 1],# Learning rate of the algorithm\n               'min_child_weight': [1, 3, 5, 7, 9],# Minimum sum of instance weight needed in a child\n               'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],# Maximum number of levels in a tree\n               'colsample_bytree': [0.5, 0.8, 1],# Subsample ratio of columns when constructing each tree,\n               'scale_pos_weight': [1, 2, 3, 4, 5]}# Balancing of positive and negative weights\nprint(random_grid)\n        \n# Use the random grid to search for best hyperparameters\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nmodel_random = RandomizedSearchCV(estimator = model, param_distributions = random_grid, n_iter = 5, cv = 3, verbose = 1, n_jobs = 1)\n\n# Fit the random search model\nmodel_random.fit(x_train, y_train)\n        \nprint(\"\\nThe best parameters for the model:\", model_random.best_params_)\n        \nmodel = model_random.best_estimator_\n        \nstart = time.time()\nmodel.fit(x_train, y_train)\nprint(\"fitting time : {}\".format(time.time()-start))\n        \nstart = time.time()\ny_pred = model.predict(x_test)\nprint(\"testing time : {}\".format(time.time() - start))\n        \nprint(\"\\nThe underlying xgboost booster of this model:\", model.get_booster())\n        \nprint(\"\\nThe number of xgboost boosting rounds:\", model.get_num_boosting_rounds())\n    \nprint(\"\\nXgboost type parameters:\", model.get_xgb_params())        ","8f8915c8":"print(\"\\nModel's score is :\", model.score(x_test, y_test))\nprint('\\nRMSE : '+str(np.sqrt(mean_squared_error(y_test, y_pred))))\n        \ncrossScore = cross_val_score(model, X = dummy_df_scaled.iloc[:,1:], y = output, cv = KFold(n_splits = 5, shuffle = True)).mean()\nprint(\"\\nThe cross-validation score is:\", crossScore)\n        \ncomparisionRDF = pd.DataFrame(y_test)\ncomparisionRDF['predicted'] = y_pred\ncomparisionRDF['player Name'] = test_names\ncomparisionRDF['error in Prediction'] = np.abs(comparisionRDF['predicted'] - comparisionRDF['Overall'])\nprint(\"\\nThe error in the prediction for each player is:\")\nprint(comparisionRDF)\n    \n#Limits of error\nprint(\"\\nLimit of error of our model is ({},{})\".format(comparisionRDF['error in Prediction'].min(),comparisionRDF['error in Prediction'].max()))\n\nprint('\\nParameters currently in use:\\n')\nprint(model.get_params())\n    \n#Visualising the results\nplt.figure(figsize=(20, 10))\nsns.regplot(y_pred, y_test, scatter_kws = {'color':'lime'}, line_kws = {'color':'red'})\nplt.xlabel('Predictions')\nplt.ylabel('Overall')\nplt.title(\"Prediction of overall rating\")\nplt.show()\n\n#Visualising the residual plot\nplt.figure(figsize=(20, 10))\nsns.scatterplot(y_pred, y_test - y_pred)\nplt.xlabel('Predictions')\nplt.ylabel('Residual')\nplt.title(\"Residual plot\")\nplt.show()","b7bff0f5":"from sklearn.neighbors import KNeighborsRegressor","873965fb":"# Create the random grid\nrandom_grid = {'n_neighbors': [5, 10, 15, 20],# Number of neighbors\n               'weights': ['uniform', 'distance'],# Whether to weigh each point in the neighborhood equally or by the inverse of their distance\n               'leaf_size': [20, 30, 40, 50]}# To be passed to the algorithm which will be used to compute the nearest neighbors\nprint(random_grid)\n\n# Use the random grid to search for best hyperparameters\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nmodel_random = RandomizedSearchCV(estimator = model, param_distributions = random_grid, n_iter = 5, cv = 3, verbose = 1, n_jobs = 1)\n\n# Fit the random search model\nmodel_random.fit(x_train, y_train)\n        \nprint(\"\\nThe best parameters for the model:\", model_random.best_params_)\n        \nmodel = model_random.best_estimator_\n        \nstart = time.time()\nmodel.fit(x_train, y_train)\nprint(\"fitting time : {}\".format(time.time()-start))\n        \nstart = time.time()\ny_pred = model.predict(x_test)\nprint(\"testing time : {}\".format(time.time() - start))","7ca1a820":"print(\"\\nModel's score is :\", model.score(x_test, y_test))\nprint('\\nRMSE : '+str(np.sqrt(mean_squared_error(y_test, y_pred))))\n        \ncrossScore = cross_val_score(model, X = dummy_df_scaled.iloc[:,1:], y = output, cv = KFold(n_splits = 5, shuffle = True)).mean()\nprint(\"\\nThe cross-validation score is:\", crossScore)\n        \ncomparisionRDF = pd.DataFrame(y_test)\ncomparisionRDF['predicted'] = y_pred\ncomparisionRDF['player Name'] = test_names\ncomparisionRDF['error in Prediction'] = np.abs(comparisionRDF['predicted'] - comparisionRDF['Overall'])\nprint(\"\\nThe error in the prediction for each player is:\")\nprint(comparisionRDF)\n    \n#Limits of error\nprint(\"\\nLimit of error of our model is ({},{})\".format(comparisionRDF['error in Prediction'].min(),comparisionRDF['error in Prediction'].max()))\n\nprint('\\nParameters currently in use:\\n')\nprint(model.get_params())\n    \n#Visualising the results\nplt.figure(figsize=(20, 10))\nsns.regplot(y_pred, y_test, scatter_kws = {'color':'lime'}, line_kws = {'color':'red'})\nplt.xlabel('Predictions')\nplt.ylabel('Overall')\nplt.title(\"Prediction of overall rating\")\nplt.show()\n\n#Visualising the residual plot\nplt.figure(figsize=(20, 10))\nsns.scatterplot(y_pred, y_test - y_pred)\nplt.xlabel('Predictions')\nplt.ylabel('Residual')\nplt.title(\"Residual plot\")\nplt.show()","ceb13dd9":"from keras.layers import Input,Dense\nfrom keras.models import Model\nfrom keras.optimizers import Adam","f417d5b7":"x_train_nn, x_test_nn, y_train_nn, y_test_nn = train_test_split(dummy_df_scaled, output)\n    \ntrain_names_nn = x_train_nn.iloc[:,0]\nx_train_nn = x_train_nn.iloc[:,1:]\ntest_names_nn = x_test_nn.iloc[:,0]\nx_test_nn = x_test_nn.iloc[:,1:]","31e1b503":"input_layer = Input((dummy_df_scaled.shape[1] - 1,))\ny = Dense(64, kernel_initializer = 'he_normal', activation = 'relu')(input_layer)\ny = Dense(32, kernel_initializer = 'he_normal', activation = 'relu')(y)\ny = Dense(8, kernel_initializer = 'he_normal', activation = 'relu')(y)\ny = Dense(1, kernel_initializer = 'he_normal', activation = 'sigmoid')(y)\n\nmodel = Model(inputs = input_layer, outputs = y)\nmodel.compile(optimizer = Adam(lr = 0.001), loss = 'mse', metrics = ['mean_squared_error'])\nmodel.summary()","11030055":"history = model.fit(x_train_nn, y_train_nn, epochs = 1000, batch_size = 512)","9dffdaf4":"plt.plot(history.history['loss'], label = 'train')\nplt.ylabel('loss')\nplt.xlabel('epochs')\nplt.legend()\nplt.show()","9b554112":"scores = model.evaluate(x_test,y_test)\nprint(\"Test Set MSE(loss, metric):\",scores)","85a9fc68":"We will fill the null values with the same probability distribution","a6ca3e45":"Inference-As expected, the potential of players is greatest during their 20s and decrease with increase in age.","b6ab2d09":"The 15 oldest players of the dataset","efe68ac0":"Since the loss and the metric are the same, both the outputs from model.evaluate are the same. As observed from the mean squared error, neural network outperforms all the methods tested so far in this kernel and so far, we have not even tried to optimise the neural network. This goes on to show why neural networks have become so popular nowadays, with one of the major reasons being the unprecedented accuracy which they can achieve.","f226b47d":"Representing distribution of wages in top clubs","bf1fcd10":"Function for removing 'lbs' from the 'weight' column","635c3918":"1. **Information about the data**","28e3e64a":"Here, we are grouping the skills together and generalizing them to 8 categories.\nWe do this so that we could analyze the players better and positon them accordingly.","70f6bc3e":"Radar chart for Cristiano Ronaldo","1d3fcea8":"The top clubs on the basis of defence","33f74903":"Representing share of preferred foot of players using countplot","a04ccc6f":"The top nations on the basis of attack","a386c18f":"Let us try the XGBoost algorithm. For a tutorial on hyperparameter tuning of the XGBoost algorithm, refer the following article:\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/\n\nFor understanding the effectiveness of this algorithm, take a look at the following link:\n\nhttps:\/\/towardsdatascience.com\/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d","58775483":"Representing distribution of wages in top nations","88410d00":"5. **Club level analysis**","602ec213":"Even k-nearest neighbours performs significantly better than linear models and is at par with ensemble models.","0d4bed62":"Representing share of international reputation of players using pie chart","ca51d714":"Now, let us try Extra Trees regression.","03e2dbf6":"The top clubs on the basis of attack","51129951":"Plotting mean of overall, age-wise","18d11209":"As we can observe, there are very few players above the age of 40 and this might be the reason for the anomaly observed in the overall mean vs. age plot.","4d0dd8d0":"Representing distribution of age in top clubs","d9bbbbfd":"Searching for columns of string type which can be eliminated.","ef98b125":"You must have observed that Lasso regression gave an extremely low accuracy for scaled data since according to this algorithm, if the values of the features are small, it will have bigger coefficients which will result in bigger Lasso penalty and therefore the algorithm will assign weights close to 0 to almost all the variables which will result in extremely low accuracy. Also, the accuracy decreased as we increases the value of alpha, the reason being that the penalty for the features will increase with increase in the value of alpha which will result in a higher number of features having weights 0.","f1f4e2bf":"Players with maximum overall grouped by position","ec17f48b":"Creating a dataframe which will be used later to compare players across different clubs and countries.","34beb10d":"As we can observe, there is an extremely high correlation between the different attributes of a goalkeeper. We will plot a heatmap between these attributes.","2e14aa35":"Inference-Most of the players have extremely low wages and only a few of the top players have wages above $100000","6cc1dfbe":"The clubs with players from highest number of different nationalities","8609ec0e":"We will fill the other null values in the remaining important columns containing float values using the mean.","cfa6c39d":"We will fill the null values with the same probability distribution","bb6e3e1a":"Let us try out ElasticNet regression, which is a combination of Lasso and Ridge regression.","dd38bd20":"The same reason as above for 'No Club' being at the top.","d64a03f2":"Although we do not need the 'name' column for our model, we will keep it for referencing players later on and as far as the other 2 columns are concerned, we will encode them using scikit-learn's 'LabelEncoder' function.","545b91ce":"Inference-As expected, the overall is highest during the 20s and starts decreasing with age. However, we see that suddenly it starts increasing around the age of 45, this might be due to outliers and so we will investigate further.","3d7bc06e":"Representing player's abilities on a few important parameters","3f626727":"We will fill the null values with the same probability distribution","7bb7187d":"Radar chart for Lionel Messi","0bb9a3ec":"According to https:\/\/www.sportsrec.com\/5464445\/the-ideal-weight-for-a-soccer-player,\n\nThe average height of players is 5 feet, 11 and 1\/2 inches. Also, from the data, we find that most of the players have their height between 5 feet, 9 inches and 6 feet, 1 inch. So, we can fill the null values in the 'height' column with 5 feet, 11 inches.","60d352e5":"As you might have observed for the case of LinearSVR, the scaled data gives a much better performance than unscaled data, the reason for which is explained in the given link:\n\nhttps:\/\/stackoverflow.com\/questions\/15436367\/svm-scaling-input-values\n\nAlthough the answer is about SVM, the same concept applies to LinearSVR as well.","0851180d":"Function for converting values in 'value' and 'wage' columns to numeric type","e71ecd81":"Finally, let us try to fit a neural network on the scaled data. We will build the neural network using Keras.","6353565d":"Representing distribution of international reputation in top clubs","3c5609b3":"8. **Modelling**","80e20105":"Let us first try Random Forest regression. ","5ebf8a37":"Representing number of players from different countries using barplot","6bbe4420":"Representing speciality scores of players using distplot","f39b90b3":"'No Club' is on top as more than 200 players have not been assigned clubs which is much higher than the number of players in any club and hence the sum of attack of those players would be higher than that of any club.","539d136c":"Radar chart for Neymar Jr.","4bc13c6b":"Splitting the data into train and test set.","a7ed3ab8":"'long shots' has a high correlation with 'shot power' since having a high or low rating in either of them automatically implies the same for the other as well in most of the cases.","4406b7ee":"The residual plot for Ridge regression with the value of alpha as 1 looks better compared to Linear regression for both scaled and unscaled data, probably due to the penalty term which is added in Ridge regression.","441fd41a":"Although I have only used 2 iterations for the random search for this algorithm and Extra Trees regressor, you can try out more iterations which might increase the acccuracy of the models. The reason being that even with 5 iterations, it took about 45 minutes to find the optimal set of features for these 2 algorithms.","c372eab1":"3. **Group similar skills together**","4ef54c53":"According to https:\/\/www.sportsrec.com\/5464445\/the-ideal-weight-for-a-soccer-player,\n\nThe normal weight range for a player with a height of 5 feet 9 inches is 136 to 169 pounds.\nSince the mean weight of the player is 165 pounds in our data and it gels with the global data, we could set the mean weight to fill the null values.","3b681483":"The top 10 left-footed footballers according to overall rating","a6ba47f9":"Plotting a correlation heatmap","fe40e9b0":"6. **National level analysis**","59b5a82b":"Finding the clubs with the most number of players","937f7427":"The 15 youngest players of the dataset","2ec1fa15":"Plotting overall rating against international reputation","6b92b18b":"Searching for columns of numeric type which can be eliminated.","5cc0ebdd":"First of all, we will create a function which outputs a model's performance on various metrics and also prints details about the model's parameters.\n\nThe parameters to be passed to the function are:\n\n1. model = Model to be used for prediction \n\n2. data = Data to be used for prediction(does not contain the column which is to be predicted)\n\n3. output_df = The variable's column which is to be predicted\n\nThe function outputs the following:\n\n1. R-squared score \n\n2. Root-mean-square error \n\n3. Cross-validation score\n\n4. The error in the predicted value for each player\n\n5. Limits of the error in prediction\n\n6. Weights and constant of the linear model\n\n7. Importance of each feature for the model\n\n8. Prediction and residual plots\n\nFor understanding the different linear models such as Linear, Ridge, Lasso and ElasticNet regression, the importance of R-squared score and residual plot, take a look at the following links:\n\n1. https:\/\/www.analyticsvidhya.com\/blog\/2017\/06\/a-comprehensive-guide-for-linear-ridge-and-lasso-regression\/\n\n2. https:\/\/blog.minitab.com\/blog\/adventures-in-statistics-2\/why-you-need-to-check-your-residual-plots-for-regression-analysis\n\n3. https:\/\/blog.minitab.com\/blog\/adventures-in-statistics-2\/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit\n\nFor more information about the 'PermutationImportance' function:\n\n1. https:\/\/eli5.readthedocs.io\/en\/latest\/blackbox\/permutation_importance.html","f5c4bf17":"Removing the 'Overall' column as it is the variable which we want to predict.","f0ff826f":"We will create a new dataframe named 'dummy_df_scaled' which will contain the values of 'Value', 'Wage' and 'Special' between 0 and 1, since they are having a much higher range of values than the other columns, so, our model might give more importance to these columns which may result in a lower accuracy.","3f2a278f":"Splitting the data into train and test set.","ab4ba89a":"'standing tackle', 'sliding tackle', 'marking' and 'interceptions' have extremely high correlation which is understandable since all of them are the attributes of a defender and thus we will only keep 'marking' for our model.","e96b96d8":"The loss and the metric, both, are mean squared error since we will compare the performance of the neural network with the other algorithms using this metric.","622ba650":"Information about the types of columns in the dataframe.","c802fddb":"The accuracy of the algorithm for both, unscaled and scaled data is similar, the reason for which is explained in the following link:\nhttps:\/\/www.quora.com\/Do-I-need-to-do-feature-scaling-for-simple-linear-regression","01bbc11b":"Creating a new dataframe which will be used for prediction","6dcd2c3f":"The optimal value of alpha for Ridge regression is 1 for this problem, since the residual plot is almost the best which we can achieve, while there is not much difference in the accuracy of the models with different values of alpha.","8d54c915":"Plotting special against overall","2cc5f54b":"We will fill the null values with the same probability distribution","d3309bca":"'ball control' has a high correlation with 'dribbling' which makes sense since having a high or low rating in either one of them mostly implies the same for the other, implying that both the skills go hand in hand.","18baa3f5":"This notebook contains the analysis and visualisations for the FIFA 19 dataset.\nAlso, the overall score of a player is predicted using various regression algorithms, ensemble algorithms and neural network.\n\nThe kernels from which references have been taken are:\n* https:\/\/www.kaggle.com\/shriramganesh\/awards-section-how-to-fill-nan-values\/data\n* https:\/\/www.kaggle.com\/nitindatta\/fifa-in-depth-analysis-with-linear-regression\n* https:\/\/www.kaggle.com\/rupavj\/fifa-17-detailed-analysis\/notebook\n\n**Table of contents**\n1. Information about the data\n2. Data cleaning \n3. Grouping similar skills together\n4. Data analysis and visualisation\n5. Club level analysis\n6. National level analysis\n7. Preparing data for modelling\n8. Modelling","a5e920ee":"Representing number of players in different positions using countplot","b8bd9292":"Now, let us try k-nearest neighbours regression.","4b3c8be0":"We will fill the null values in 'loaned from' as 'none' and in 'club' as 'no club'","f96aa45f":"We will try to predict the overall rating of a player using various regression algorithms. For determining the important attributes for prediction, we will try to find correlation between them and then decide which attributes to use.","fb7fd260":"Representing distribution of overall rating in top nations","23e68061":"Representing distribution of work rate of players using countplot","a517ea93":"Comparing the top clubs on the basis of a few important parameters","1482a1e6":"We will fill the null values in the other remaining columns as 0","66b7ad28":"The top nations on the basis of average overall rating","5f9c0143":"Filling the NaN values","291dfd57":"Representing share of weak foot of players using pie chart","a15eca95":"The top 10 right-footed footballers according to overall rating","0bf143b7":"Representing distribution of international reputation in top nations","e885ffc6":"'Neymar', 'Messi', 'Shaqiri', 'Akinfenwa', 'Courtois' are definitely not body types but the names of football players. So, we will fill the null values with 'normal' and 'lean'.","de6d887d":"Let us now try Gradient Boosting regression.","d08a0883":"Clearly, we don't want columns such as 'Unnamed: 0', 'Jersey Number', 'ID'.","f97c2e7d":"Now, let us try out Lasso regression, which applies the L1 penalty.","ea827a11":"Clearly, we do not want columns such as 'Photo', 'Flag', 'Club Logo', 'Real Face', 'Joined', 'Loaned From', 'Contract Valid Until', 'Height', 'Release Clause', 'Preferred Foot', 'Nationality','LS', 'ST', 'RS', 'LW', 'LF', 'CF', 'RF', 'RW','LAM', 'CAM', 'RAM', 'LM', 'LCM', 'CM', 'RCM', 'RM', 'LWB', 'LDM','CDM', 'RDM', 'RWB', 'LB', 'LCB', 'CB', 'RCB', 'RB', 'Body Type'.","e2de4a10":"Clearly, we also don't want to have 'rating' as one of our attributes as it is the mean of 'overall' and 'potential' and 'overall' is the variable which we want to predict.","2fe2bd55":"Now, we will apply ensemble algorithms over the scaled data which will hopefully give a much better performance than the linear algorithms.\nFor understanding the reason due to which ensemble methods mostly work better than individual models, refer the given link:\n\nhttps:\/\/www.quora.com\/How-do-ensemble-methods-work-and-why-are-they-superior-to-individual-models\n\nTo understand the working of the ensemble algorithms, the scikit-learn documentation is an excellent source:\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/ensemble.html","8c3482c7":"Plotting the graph of epochs vs. loss","89d1604d":"We will also train the linear regression algorithms on the unscaled data so as to compare the difference in performance for both, the unscaled and scaled dataset.","21b25237":"We can see a huge jump in the score and a decrease in RMSE compared to the linear models even though we had only used 2 iterations for the random search of the hyperparameters, this demonstrates the power of ensembling algorithms. Also, the residual plot has drastically improved compared to the linear models.","1786dd01":"We will fill the null values with the same probability distribution","654df169":"Changing the range of the values in all the numeric type columns of the data to (0, 1). ","8aef91d1":"Because of the high correlation between the attributes of a goalkeeper, we will only keep only 1 attribute out of the 5 as we do not want redundant features for our model.","ac725633":"We will fill the null values with the same probability distribution","8817790b":"Let us use AdaBoost regression","dd4667d5":"We will build a five layered neural network with 3 hidden layers, however, this is not based on any theory, rather, it is based on trial and error and since this model gave a great performance, I went ahead with this, you can try building your own architecture and experiment with them. The learning rate was also selected after doing a few experiments, however, there are techniques which can assist in finding an optimal learning rate, one such technique is described here:\n\nhttps:\/\/arxiv.org\/abs\/1506.01186","4632e015":"There is definitely a positive correlation between international reputation and overall rating but surprisingly there are many players rated above 75 who have an international reputation of only 1.","b4d4df02":"Representing potential score of players using distplot","5836386a":"Representing distribution of overall rating in top clubs","30670a67":"4. **Data analysis and visualisation**","b8b611a7":"The top nations on the basis of defence","24e2c226":"7. **Preparing data for modelling**","14dbe3bb":"Adding these categories to the data","77cd8b29":"Representing wage range of players using distplot","3ce36fcb":"The clubs with players from least number of different nationalities","f6378586":"2. **Data cleaning**","bad9a310":"Representing overall score of players using distplot","38902d19":"'Work rate' is an extremely important parameter for our model, but, we need to convert it to numeric type from string type since we can only use numeric values for the parameters in our model.","6fb6b686":"Representing distribution of weight in top clubs","28db8a56":"We will keep just 1 attribute out of these 5 as the other 4 attributes would be redundant for prediction.","9dababf3":"We will do a random grid search over the hyperparameters rather than a normal grid search. For understanding the concepts of hyperparameter tuning, refer the following article:\n\nhttps:\/\/www.oreilly.com\/ideas\/evaluating-machine-learning-models\/page\/5\/hyperparameter-tuning\n\nThe following paper shows that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a regular grid.\n\nhttp:\/\/jmlr.csail.mit.edu\/papers\/volume13\/bergstra12a\/bergstra12a.pdf","49c93983":"As we can see, the outliers are the reason for the high mean of players aged around 45.","f01c2b10":"Representing skill moves of players using countplot","ad287228":"The top clubs on the basis of average overall rating","6698507b":"Inference-Overall rating has a high correlation with special rating.","918ca50b":"Inference-The most number of players are from top European and South American footballing nations like England, Germany, Spain, Argentina, France, Brazil etc.","97095351":"Representing distribution of weight in top nations","105f1866":"We will fill the null values with the same probability distribution","01f22885":"This was my first kernel, wherein I learned a whole lot of new techniques and concepts. It was a great learning experience for me and I would like to thank the Kaggle community for creating such a wonderful platform where people who are new to data science, like me, can leverage their skills so easily. Any suggestions or comments are welcome. Please consider upvoting this kernel if it was helpful in any way to you, it would be much appreciated.\n\nThank you","bc31e23e":"Finding the nations with the most number of players","fadcf20e":"Plotting age against potential","eee93513":"Players with maximum potential grouped by position","6f61db8b":"There is not much difference in the performance of the algorithm for different values of l1 ratio, but as was the case with Lasso regression, the weights assigned to the variables are close to 0 for scaled data since ElasticNet regression has some amount of L1 penalty(Lasso regression).","f6e297a2":"Let us try Ridge regression, which applies the L2 penalty.","c270e9b3":"As observed above, all the ensemble algorithms performed much better than the linear algorithms, hence, proving their supremacy.","2990c361":"Radar charts for a few of the top players\n\nCredit:https:\/\/www.kaggle.com\/typewind\/draw-a-radar-chart-with-python-in-a-simple-way","13673549":"The top 4 features of players according to different positions","74888ec9":"Number of null values in each column","9167c39e":"Comparing the top nations on the basis of a few important parameters"}}