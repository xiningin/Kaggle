{"cell_type":{"fad20852":"code","4c333fc5":"code","ebda19bb":"code","81afb201":"code","0efd6703":"code","3a97ee33":"code","c45d00dd":"code","17f34b00":"code","ab6083d3":"code","5d306c2c":"code","8636d422":"code","3d336bf0":"code","85c27a13":"code","b6e80f8b":"code","ca7eeb24":"code","e7e5639d":"code","1d94a646":"code","df265f2b":"code","8eae7718":"code","b85b407b":"code","5e1d9cfe":"code","d2d15abc":"code","9155841a":"code","52184580":"code","910cbc8b":"code","94ec9754":"code","37ce8302":"code","f74d0399":"code","8a305412":"code","d7c694c8":"code","4a43818a":"code","b8ee346d":"code","57dca695":"code","dddf5043":"code","7afbc09c":"code","acc48f87":"code","1e86843a":"code","3ea45caa":"code","45519aaa":"code","2ae9fd79":"code","90d9ae5d":"code","1ce3d0c3":"code","24044220":"code","7bfefe7c":"code","ed566864":"code","d0481e4d":"code","526735aa":"code","b0346b04":"code","4e0e9f79":"code","e0405942":"code","fdad399a":"code","3b7fc25d":"code","a9db9473":"code","82d5f8c0":"code","81d4aac6":"code","7ce178a3":"code","23158150":"code","766c36f6":"code","bdaff26c":"code","8d30bb96":"code","554c5a10":"code","043a052d":"code","2d90e142":"code","ee8cee40":"code","813e1f0a":"code","e1a90990":"markdown","64d7159e":"markdown","32c13d56":"markdown","0f3a6c3b":"markdown","99631630":"markdown","919aca82":"markdown","f78c339a":"markdown","9b15940d":"markdown","a1fa15cb":"markdown","dad2808b":"markdown","02e062dc":"markdown","45dde456":"markdown","be6cec58":"markdown","0f867a6d":"markdown"},"source":{"fad20852":"# saving all required libraries under requirements.txt\n# ! pip freeze > requirements.txt","4c333fc5":"# import necessary libraries\nimport warnings\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom keras.models import Sequential\nfrom keras.layers import Dense,LSTM, TimeDistributed, Flatten, MaxPooling1D,Conv1D,Dropout\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso,ElasticNet,HuberRegressor,PassiveAggressiveRegressor,SGDRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.tree import DecisionTreeRegressor,ExtraTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import AdaBoostRegressor,BaggingRegressor,RandomForestRegressor,ExtraTreesRegressor,GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split,GridSearchCV,RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom statsmodels.tsa.arima.model import ARIMA\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","ebda19bb":"# with open('requirements.txt') as f:\n#     print(f.read())","81afb201":"# Load all data\ntrain = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\ntest = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv')\nitems = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv')\nitems_cat = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nshops = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv')","0efd6703":"# first 5 rows of train data\ntrain.head()","3a97ee33":"# first 5 rows of test data\ntest.head()","c45d00dd":"# first 5 rows of shop name and id\nshops.head()","17f34b00":"# item name, id and category-id\nitems.head()","ab6083d3":"# item category and category id, to mape to item names in items\nitems_cat.head()","5d306c2c":"# statistical summary of each feature, assuming negative means more items are returned than sold\ntrain.describe()","8636d422":"# checking for missing values\ntrain.isnull().sum()","3d336bf0":"# check for duplicates\ntrain[train.duplicated(keep = False)]","85c27a13":"# drop duplicates, keep one copy\ntrain.drop_duplicates(keep = 'first',inplace = True)","b6e80f8b":"# Changing the date column to datetime format and date as index\ntrain.date = pd.to_datetime(train.date, format = '%d.%m.%Y' )\n# train = train.set_index('date')\n\ntrain.head()","ca7eeb24":"# Looking at the correlation between features\nmask = np.triu(np.ones_like(train.corr(),dtype = bool))\nf,ax = plt.subplots()\nsns.heatmap(train.corr(),mask = mask,annot = True);","e7e5639d":"# setting date as index\ntrain_1 = train.copy()\ntrain_1 = train_1.set_index('date').sort_index()\ntrain_1","1d94a646":"# looking at total number of products sold throughout the months\n# sales is the highest in Jan of every month\nfig,ax = plt.subplots(figsize = (20,10))\nax.plot(train_1.resample('M').agg({'item_price':'mean','item_cnt_day':'sum'})['item_cnt_day'])\nax.set_title('Total number of products sold per month for all shops',fontsize = 14)\nax.set_ylabel('Total monthly products sold',fontsize = 14)\nax.set_xlabel('Year - Month',fontsize = 14)","df265f2b":"# Looking at the distribution of target variable\nsns.boxplot(train.item_cnt_day)","8eae7718":"train[train.item_cnt_day > 2000]","b85b407b":"# based on the distribution of past item_cnt_day of the same item, seems like its an anomaly \nsns.boxplot(train[train.item_id == 11373][\"item_cnt_day\"])","5e1d9cfe":"# Drop the anomaly\ntrain.drop(2909818,inplace = True)","d2d15abc":"fig,ax = plt.subplots(figsize = (20,10))\nsns.violinplot(train.item_price)","9155841a":"# There is an extreme outlier with price > 300000\ntrain[train.item_price > 250000]","52184580":"# Only 1 item with that price, might be an outlier\ntrain[train.item_id == 6066]","910cbc8b":"# Drop the outlier\ntrain.drop(1163158,inplace = True)","94ec9754":"# extracting year and month\ntrain_1['year_month'] = train_1.index.strftime('%Y-%m')","37ce8302":"# group according to shop_id and item_id\ndf = pd.pivot_table(train_1, index = ['shop_id','item_id'], \n                              columns = 'year_month',\n                              values = 'item_cnt_day',\n                              aggfunc = np.sum,\n                              fill_value = 0)\ndf = df.reset_index().rename_axis(None,axis = 1)\ndf","f74d0399":"# to filter out those item counts that we want to predict afterwards\ndf_test = test.merge(df, on = ['shop_id','item_id'],how = 'left').fillna(0).drop(columns = 'ID')\n\n# drop redundant columns for df_test\ndf_test = df_test.drop(columns = ['shop_id','item_id'])\ndf_test.head()","8a305412":"# df_test_train: Training set where we use all dates except the latest one to train model\n# df_test_val: Test set where we use latest date to validate model\ndf_test_train = df_test.iloc[:,:-1]\ndf_test_val = df_test.iloc[:,-1]","d7c694c8":"print(f'validation set: {df_test_val.shape}')\nprint(f'train set: {df_test_train.shape}')","4a43818a":"# train test split df: X = all columns except last one, y = last column\nX = df_test_train\ny = df_test_val\nX_train,X_test,y_train,y_test = train_test_split(X,y,shuffle = False,test_size = 0.2, random_state = 42)","b8ee346d":"# looking at rows, columns for train and validation set\nprint(f'train: {X.shape}')\nprint(f'test: {y.shape}')\nprint(f'val train: {X_test.shape}')\nprint(f'val test: {y_test.shape}')","57dca695":"# scale data\nss = StandardScaler()\nss.fit(X_train)\nX_train_ss = ss.transform(X_train)\nX_test_ss = ss.transform(X_test)","dddf5043":"def get_models(models=dict()):\n# linear models\n    models['lr'] = LinearRegression()\n    models['lasso'] = Lasso()\n    models['ridge'] = Ridge()\n    models['en'] = ElasticNet()\n    models['huber'] = HuberRegressor()\n    models['pa'] = PassiveAggressiveRegressor(max_iter=1000, tol=1e-3)\n   \n    return models\n\ndef get_models_nl(models=dict()):\n# non-linear models\n    models['svr'] = SVR()\n# ensemble models\n    n_trees = 100\n    models['ada'] = AdaBoostRegressor(n_estimators=n_trees)\n    models['bag'] = BaggingRegressor(n_estimators=n_trees)\n    models['rf'] = RandomForestRegressor(n_estimators=n_trees)\n    models['et'] = ExtraTreesRegressor(n_estimators=n_trees)\n    models['gbm'] = GradientBoostingRegressor(n_estimators=n_trees)\n    return models\n\ndef pipeline(model):\n    pipe = Pipeline([(model, model_dict[model])])\n    return pipe\n\ndef params(model):\n    \n\n    if model == 'lasso':\n        return {\"alpha\":[0.01,0.1,1,2,5,10],\n               }\n    \n    \n    elif model == 'ridge':\n        return {\n            \"alpha\":[0.01,0.1,1,2,5,10],\n            }\n    \n    elif model == 'en':\n        return {\n            'alpha':[0.01,0.1,1,10],\n            'l1_ratio':[0.2,0.3,0.4,0.5,0.6]\n            }\n    elif model == 'knn':\n        return {\n            'n_neighbors':[4,5,6,7]}\n\n    elif model == 'dt':\n        return {\n            'max_depth':[3,4,5],\n            'min_samples_split':[2,3,4],\n            'min_samples_leaf':[2,3,4]\n        }\n    elif model == 'bag':\n        return {\n            'max_features':[100, 150]\n        }\n        \n    elif model == 'rf':\n        return {\n            'n_estimators':[100,150],\n            'max_depth':[4],\n            'min_samples_leaf':[2,3,4]\n        }\n    elif model == 'et':\n        return {\n            'n_estimators':[50,100,150],\n            'max_depth':[5],\n            'min_samples_leaf':[2,3],\n        }\n    elif model == 'abc':\n        return {\n            'n_estimators':[50, 100,150],\n            'learning_rate':[0.3,0.6,1]\n        }\n    elif model == 'gbc':\n        return {\n            'learning_rate':[0.2],\n            'max_depth':[5],\n            'min_samples_split':[2,5]\n            \n        }\n    elif model == 'xgb':\n        return {\n            'eval_metric' : ['auc'],\n            'subsample' : [0.8], \n            'colsample_bytree' : [0.5], \n            'learning_rate' : [0.1],\n            'max_depth' : [5], \n            'scale_pos_weight': [5], \n            'n_estimators' : [100,200],\n            'reg_alpha' : [0, 0.05],\n            'reg_lambda' : [2,3],\n            'gamma' : [0.01]\n                             \n        }\n    elif model == 'svr':\n        return {\n            'kernel': ['rbf', 'linear','poly'], \n            'C': [1,20,50,100],\n            'gamma':['scale','auto'],\n            'epsilon':[0.1,1,10]\n        }\n    elif model == 'ada':\n        return {\n            'n_estimators':[50,100,150],\n            'learning_rate':[0.01,0.1,1],\n            \n        }\n    elif model == 'bag':\n        return {\n            'n_estimators':[20,50,100,150],\n            'max_features':[5,10,20],\n            'max_samples':[0.1,0.2,0.3,0.5,0.7],\n            'bootstrap':[True]\n            \n        }\n    elif model == 'rf':\n        return {\n             'bootstrap': [True],\n             'max_depth': [5,10,20],\n             'max_features': [\"auto\", \"sqrt\", \"log2\"],\n             'min_samples_leaf': [2,4,6,8,10],\n             'min_samples_split': [2,5,8,10],\n             'n_estimators': [50,200,300,400],\n             'random_state': 42,\n             }\n    elif model == 'et':\n        return {\n             'bootstrap': [True],\n             'max_depth': [5,10,20],\n             'max_features': [\"auto\", \"sqrt\", \"log2\"],\n             'min_samples_leaf': [2,4,6,8,10],\n             'min_samples_split': [2,5,8,10],\n             'n_estimators': [50,200,300,400],\n             'random_state': 42,\n        }\n            \n    elif model == 'gbm':\n        return {\n            'learning_rate' : [0.1,0.3,0.6,1], \n            'min_samples_split':[500,1000,2000,3000,5000],\n            'min_samples_leaf': [50,200,400,1000],\n            'max_depth' : [8,10,20,30]\n        }","7afbc09c":"def evaluate_models(models, X_train_ss,y_train,X_test_ss,y_test):\n    for name, model in models.items():\n    # fit models\n        model_fit = model.fit(X_train_ss,y_train)\n        # make predictions\n        train_preds = model_fit.predict(X_train_ss)\n        test_preds = model_fit.predict(X_test_ss)\n        # evaluate forecast\n        train_mse = mean_squared_error(y_train,train_preds)\n        test_mse = mean_squared_error(y_test,test_preds)\n        print(f'{name}:')\n        print(f'----')\n        print(f'Train MAE: {round(train_mse,2)}')\n        print(f'Test MAE: {round(test_mse,2)}')\n        print(f'\\n')\n    \n\n","acc48f87":"# grid search with gridsearchcv\ndef grid_search(model,models,X_train = X_train_ss,y_train = y_train,X_test = X_test_ss,y_test=y_test):\n    pipe_params = params(model)\n    model = models[model]\n    gs = GridSearchCV(model,param_grid = pipe_params,cv = 5,scoring = 'neg_mean_squared_error', verbose=True, n_jobs=8)\n    gs.fit(X_train_ss,y_train)\n    train_score = gs.score(X_train_ss,y_train)\n    test_score = gs.score(X_test_ss,y_test)\n    \n    print(f'Results from: {model}')\n    print(f'-----------------------------------')\n    print(f'Best Hyperparameters: {gs.best_params_}')\n    print(f'Mean MSE: {round(gs.best_score_,4)}')\n    print(f'Train Score: {round(train_score,4)}')\n    print(f'Test Score: {round(test_score,4)}')\n    print(' ')","1e86843a":"# grid search with randomizedsearchcv\ndef grid_search_rs(model,models,X_train = X_train_ss,y_train = y_train,X_test = X_test_ss,y_test=y_test):\n    pipe_params = params(model)\n    model = models[model]\n    gs = RandomizedSearchCV(model,param_distributions = pipe_params,cv = 5,scoring = 'neg_mean_squared_error', verbose=True, n_jobs=8)\n    gs.fit(X_train_ss,y_train)\n    train_score = gs.score(X_train_ss,y_train)\n    test_score = gs.score(X_test_ss,y_test)\n    \n    print(f'Results from: {model}')\n    print(f'-----------------------------------')\n    print(f'Best Hyperparameters: {gs.best_params_}')\n    print(f'Mean MSE: {round(gs.best_score_,4)}')\n    print(f'Train Score: {round(train_score,4)}')\n    print(f'Test Score: {round(test_score,4)}')\n    print(' ')","3ea45caa":"models = get_models()\nevaluate_models(models,X_train_ss,y_train,X_test_ss,y_test)","45519aaa":"# best params of lasso\n%time grid_search('lasso',models)","2ae9fd79":"%%time\ngrid_search_rs('lasso',models)","90d9ae5d":"# best params of en:\n%time grid_search_rs('en',models)","1ce3d0c3":"# best params of en:\n%time grid_search('en',models)","24044220":"X_train_ss.shape","7bfefe7c":"# reshape [samples,timesteps] into expected shape [samples,timesteps,n_features]\n# samples = number of records ,timestep = how far back are we looking? , n_features = no. of variables, 1 as we are only using cnt to predict next month's cnt\nX_train_ss_rs = X_train_ss.reshape((X_train_ss.shape[0],X_train_ss.shape[1],1))\nX_test_ss_rs = X_test_ss.reshape((X_test_ss.shape[0],X_test_ss.shape[1],1))\nss1 = StandardScaler()\nss1.fit(y_train.values.reshape(y_train.shape[0],1))\ny_train_ss = ss1.transform(y_train.values.reshape(y_train.shape[0],1))","ed566864":"%%time\nmodel = Sequential()\nmodel.add(LSTM(50, input_shape = (X_train_ss_rs.shape[1],X_train_ss_rs.shape[2]),activation = 'relu'))\nmodel.add(Dense(16,activation = 'relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(1))\nmodel.compile(loss = 'mae',optimizer = 'adam')\nhistory = model.fit(X_train_ss_rs,y_train_ss,epochs = 30, batch_size = 50000, verbose = 2,shuffle = False)\nplt.plot(history.history['loss'],label = 'loss')","d0481e4d":"%%time\nmodel_1 = Sequential()\n# [samples,timesteps,features]\nmodel_1.add(LSTM(50, input_shape = (X_train_ss_rs.shape[1],X_train_ss_rs.shape[2]),activation = 'relu'))\nmodel_1.add(Dense(16,activation = 'relu'))\nmodel_1.add(Dropout(0.4))\nmodel_1.add(Dense(1))\nmodel_1.compile(loss = 'mae',optimizer = 'adam')\nhistory_1 = model_1.fit(X_train_ss_rs,y_train_ss,epochs = 50, batch_size = 100000, verbose = 2,shuffle = False)\nplt.plot(history_1.history['loss'],label = 'loss')","526735aa":"fig,ax = plt.subplots(figsize = (20,10))\nax.plot(history.history['loss'],label = 'Smaller batch size')\nax.plot(history_1.history['loss'],label = 'Larger batch size, more epochs')\n# ax.plot(history_2.history['loss'],label = 'Larger batch size, more epochs, no relu')\nplt.legend()","b0346b04":"# function to evaluate performance of models\ndef mae_train_test(model,model_name, X_train,y_train,X_test,y_test,ss1):\n    trainpreds = ss1.inverse_transform(model.predict(X_train)).reshape((y_train.shape[0],))\n    testpreds = ss1.inverse_transform(model.predict(X_test)).reshape((y_test.shape[0],))\n    print(f'Model: {model_name}')\n    print('----')\n    print(f'Train MAE: {mean_squared_error(y_train,trainpreds)}')\n    print(f'Test MAE: {mean_squared_error(y_test,testpreds)}')\n    print('\\n')\n    ","4e0e9f79":"mae_train_test(model,\"LSTM - Smaller Batch Size\", X_train_ss_rs,y_train,X_test_ss_rs,y_test,ss1)\nmae_train_test(model_1,\"LSTM - Larger Batch Size, more epochs\", X_train_ss_rs,y_train,X_test_ss_rs,y_test,ss1)\n# mae_train_test(model_2,\"LSTM -Larger Batch Size, more epochs, no relu\", X_train_ss_rs,y_train,X_test_ss_rs,y_test,ss1)","e0405942":"print(f'Shape of X_train: {X_train.shape}')\nprint(f'Shape of X_test: {X_test.shape}')","fdad399a":"# required input shape: [samples, subsequences, timesteps, features]\n# number of samples: \nsubseq = 1\nts = 33\nss_cnn = StandardScaler()\nss_cnn.fit(X_train)\nX_train_sub = ss_cnn.transform(X_train).reshape((X_train.shape[0],subseq,ts,1))\nX_test_sub = ss_cnn.transform(X_test).reshape((X_test.shape[0],subseq,ts,1))\nprint(f'Shape of X_train_sub: {X_train_sub.shape}')\nprint(f'Shape of X_test_sub: {X_test_sub.shape}')","3b7fc25d":"%%time \ncnn_lstm = Sequential()\ncnn_lstm.add(TimeDistributed(Conv1D(filters = 64, kernel_size = 2, input_shape = (None,X_train_sub.shape[2],X_train_sub.shape[3]),activation = 'relu')))\ncnn_lstm.add(TimeDistributed(Dropout(0.4)))\ncnn_lstm.add(TimeDistributed(MaxPooling1D(pool_size = 1)))\ncnn_lstm.add(TimeDistributed(Flatten()))\ncnn_lstm.add(LSTM(50, activation = 'relu',return_sequences = True))\ncnn_lstm.add(Dense(16,activation = 'relu'))\ncnn_lstm.add(Dropout(0.4))\ncnn_lstm.add(Dense(1))\ncnn_lstm.compile(loss= 'mae',optimizer = 'adam')\ncnn_lstm_history = cnn_lstm.fit(X_train_sub,y_train_ss, epochs = 50, batch_size = 100000, verbose = 2,shuffle = False)\n","a9db9473":"# cnn_lstm.reset_states()","82d5f8c0":"%%time\ncnn_lstm_1 = Sequential()\ncnn_lstm_1.add(TimeDistributed(Conv1D(filters = 64, kernel_size = 11, input_shape = (None,1,X_train_sub.shape[2],X_train_sub.shape[3]),activation = 'relu')))\ncnn_lstm_1.add(TimeDistributed(Dropout(0.4)))\ncnn_lstm_1.add(TimeDistributed(MaxPooling1D(pool_size = 1)))\ncnn_lstm_1.add(TimeDistributed(Flatten()))\ncnn_lstm_1.add(LSTM(50, activation = 'relu'))\ncnn_lstm_1.add(Dense(16))\ncnn_lstm_1.add((Dropout(0.4)))\ncnn_lstm_1.add(Dense(1))\ncnn_lstm_1.compile(loss= 'mae',optimizer = 'adam')\ncnn_lstm_1_history = cnn_lstm_1.fit(X_train_sub,y_train_ss, epochs = 30, batch_size = 100000, verbose = 2,shuffle = False)\n","81d4aac6":"fig,ax = plt.subplots(figsize = (20,10))\nax.plot(cnn_lstm_history.history['loss'],label = 'more epochs, return state')\nax.plot(cnn_lstm_1_history.history['loss'],label = 'fewer epochs, without relu')\nax.legend()","7ce178a3":"mae_train_test(cnn_lstm,\"CNN-LSTM\", X_train_sub,y_train,X_test_sub,y_test,ss1)","23158150":"mae_train_test(cnn_lstm_1,\"CNN-LSTM - No relu\", X_train_sub,y_train,X_test_sub,y_test,ss1)","766c36f6":"print(f'test: {df_test.shape}')\nprint(f'train: {df.shape}')","bdaff26c":"## Using only 33 columns to make our next forecasts\ndf_test_1 = df_test.iloc[:,1:]\nss_test = StandardScaler()\nss_test.fit(df_test_1)\ndf_test_1_sub = ss_test.transform(df_test_1).reshape((df_test_1.shape[0],subseq,ts,1))","8d30bb96":"df_test_1.shape","554c5a10":"print(f'Shape of test: {df_test_1_sub.shape}')","043a052d":"# generating forecasts, input = 33 months of data, output = 34th month\npreds = cnn_lstm_1.predict(df_test_1_sub)","2d90e142":"# since we scaled 33 variables, the standardscaler expects 33 variables so we'll just leave them as 0s\nforecasts = np.zeros(shape=(len(preds), 33) )\n# replace the first column with the actual preds\nforecasts[:,0] = preds[:,0]\n# inverse transform and then select the same preds column\nforecasts = ss_test.inverse_transform(forecasts)[:,0]","ee8cee40":"# saving as a new dataframe\nsubmissions = pd.DataFrame({\"ID\":test.ID, \"item_cnt_month\":forecasts})\nsubmissions.head()","813e1f0a":"# ready for submissions!\nsubmissions.to_csv('submissions.csv',index = False)","e1a90990":"## Exploratory Data Analysis\nQuick EDA and data cleaning, mainly to remove anomalies that might skew our data and to establish a few assumptions! ","64d7159e":"### Problem Statement\nWith daily historical data, our main goal is to create and optimize a model that makes a forecast on the total number of items for **each** item id sold in **every** shop in the test set (month of November 2015) as their prices fluctuate across time. ","32c13d56":"### StandardScaler\nNormalize scales of features to improve accuracy of predictions especially if our variables are on different scales\/magnitudes. This is because this would affect the performances of models that specifically rely on distance metrics(k-NN, PCA) as well as to speed up gradient descent convergence for deep neural networks during backpropagation. Mainly to ensure that every feature contributes equally to the models! ","0f3a6c3b":"### LSTM\nAn extension of RNN that overcomes the vanishing gradient problem and to learn long term dependencies in sequence prediction problems using the memory cells present in the hidden states!","99631630":"### Pre-target variable: `item_cnt_day`\ntarget variable is the monthly value: `item_cnt_month`","919aca82":"### CNN-LSTM\nCNN: For additional feature engineering, LSTM: to overcome vanishing gradient problem and to learn long-term dependencies. ","f78c339a":"## Predict Future Sales\n","9b15940d":"### `item_cnt_day`","a1fa15cb":"### Data Modeling \nUsing **2015-10** as our target feature and earlier time periods as our predictor features","dad2808b":"### `item_price`","02e062dc":"### Data visualiztion","45dde456":"### Linear models","be6cec58":"### Reading data","0f867a6d":"### Generating forecasts\nSince our model is trained on the past 33 months of data, we will feed the same duration of historical data to make our forecasts! "}}