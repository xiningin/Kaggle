{"cell_type":{"e040bccd":"code","9b7d841a":"code","c552fbfb":"code","049fa5d9":"code","ac01b7dd":"code","d837c80d":"code","c6338810":"code","5585b75b":"code","d8d7f741":"code","0804ee7f":"code","3203f0f6":"code","9c5d0155":"code","cc22ca8b":"code","f21f0545":"code","40632393":"code","15d04f8b":"code","4296eece":"code","675fbbd6":"code","e0a5b174":"code","8ed1c533":"code","be27f8c1":"code","9b3bc2ff":"code","d0802005":"code","c89df3cb":"code","5d9d47e9":"code","f668b5a2":"code","dec7d871":"code","587f2cef":"code","ec3059ee":"code","a503e6e3":"code","ea6125e2":"code","f15052ab":"code","2c86a5a7":"code","7bfdafd8":"code","60576905":"markdown","f4640961":"markdown","fa6616f0":"markdown","92bcd12f":"markdown","ab96d394":"markdown","d45dcc55":"markdown","1cf4015f":"markdown","b3418cbf":"markdown","bedfe6c6":"markdown","c1824bfc":"markdown","8694a1c2":"markdown","b2b298e4":"markdown","7fb91ec0":"markdown","5367b68f":"markdown"},"source":{"e040bccd":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings \nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns',None)\npd.set_option('display.max_rows',None)\n\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nimport sklearn.metrics\nfrom sklearn.model_selection import train_test_split,KFold,cross_val_score,RandomizedSearchCV, GridSearchCV\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import QuantileTransformer","9b7d841a":"def plot_model_comparison(models,results,title):\n    \"\"\" \n        Compares the results of different models and plots box plots for the algorithms.\n        models: list of names of models\n        results: training results\n        title: title for the graph\n        \n    \"\"\"\n    fig = plt.figure()\n    fig.suptitle(title)\n    ax = fig.add_subplot(111)\n    plt.boxplot(results)\n    ax.set_xticklabels(models)\n    plt.show()\n\ndef timer(start_time=None):\n    \"\"\" \n        Helps  to keep track of time elapsed while training.\n        start time: if none then start time tracking\n                    if not none tracks time from start time         \n    \"\"\"\n    from datetime import datetime\n    if not start_time:\n        print(datetime.now())\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print(\"Time taken: %i hours %i minutes and %s seconds.\" % (thour, tmin, round(tsec, 2)))\n\ndef getbounds(col):\n    ''' \n    This function returns the upper bound and the lower bound using the IQR for the column \"col\".\n    '''\n    sorted(col)\n    q1,q3 = np.percentile(col,[25,75]) # quartailes\n    iqr = q3-q1 # inter quartile range\n    lb = q1 -(1.5*iqr) # lower bound\n    ub = q3 +(1.5*iqr) # upper bound\n    return lb,ub\n\ndef plothists(df):\n    '''\n    we'll use this function to iteratively plot the histplot for all columns of the df\n    '''\n    nrows = 30\n    ncols = 4\n    i = 0\n    fig, ax = plt.subplots(nrows, ncols, figsize = (40,120))\n    for row in range(nrows):\n        for col in range(ncols):\n            if i==118:\n                break\n            else:\n                sns.histplot(data = df.iloc[:, i], bins = 30, ax = ax[row, col]).set(ylabel = '')\n                i += 1","c552fbfb":"train=pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\")","049fa5d9":"features = train.columns[1:-1]\ndf=train[features].copy()","ac01b7dd":"df.shape","d837c80d":"df.describe().T","c6338810":"df.isnull().sum()","5585b75b":"si=SimpleImputer(strategy='median',copy=False)\nsi.fit_transform(df)\nidf=pd.DataFrame(data=df,columns=features)","d8d7f741":"idf.describe().T","0804ee7f":"idf.isnull().sum().sum()","3203f0f6":"#before transformation\nplothists(idf)","9c5d0155":"qt=QuantileTransformer(\n    n_quantiles=1000, \n    random_state=21,\n    output_distribution= 'normal',\n    copy=False)\nqt.fit_transform(idf)","cc22ca8b":"qtidf=pd.DataFrame(data=idf,columns=features)","f21f0545":"#after transformation\nplothists(qtidf)","40632393":"X=qtidf.values\nY=train[['claim']].values","15d04f8b":"scorer = \"roc_auc\"\nsplits = 5\nseed = 21","4296eece":"x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size=0.20,  random_state=seed)","675fbbd6":"print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)","e0a5b174":"models = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('SGD', SGDClassifier(random_state=21)))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append((\"QDA\", QuadraticDiscriminantAnalysis()))\nmodels.append(('CART', DecisionTreeClassifier(max_depth=10,max_features = 10)))\nmodels.append(('NB', GaussianNB()))\nmodels.append((\"Neural Net\", MLPClassifier(alpha=1, max_iter=1000)))\nmodels","8ed1c533":"results = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=splits, shuffle=True,  random_state=seed)\n    start_time=timer(None)\n    cv_results = cross_val_score (model, x_train, y_train, cv=kfold,  scoring=scorer)\n    timer(start_time)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %5.2f (%5.2f)\" % (name, cv_results.mean()*100, \\\n                           cv_results.std()*100)\n    print(msg)\nresults_df = pd.DataFrame(results, index=names, \\\n                          columns='CV1 CV2 CV3 CV4 CV5 '.split())\nresults_df['CV Mean'] = results_df.iloc[:,0:splits].mean(axis=1)\nresults_df['CV Std Dev'] = results_df.iloc[:,0:splits].std(axis=1)\nresults_df.sort_values(by='CV Mean', ascending=False)*100","be27f8c1":"title=\"Algorithms Comparison\"\nplot_model_comparison(names,results,title)","9b3bc2ff":"param_grid_nb = {\n    'var_smoothing': np.logspace(0,-9, num=100)\n}","d0802005":"start_time=timer(None)\nnb_grid = GridSearchCV(estimator=GaussianNB(), param_grid=param_grid_nb, verbose=1, cv=10, n_jobs=-1)\nnb_grid.fit(x_train, y_train)\ntimer(start_time)\nprint(nb_grid.best_estimator_)","c89df3cb":"y_pred = nb_grid.predict(x_test)\ny_pred","5d9d47e9":"print(y_pred.shape,y_test.shape)","f668b5a2":"from sklearn.metrics import accuracy_score\nprint(accuracy_score(y_test, y_pred), \": is the accuracy score\")\nfrom sklearn.metrics import precision_score\nprint(precision_score(y_test, y_pred), \": is the precision score\")\nfrom sklearn.metrics import recall_score\nprint(recall_score(y_test, y_pred), \": is the recall score\")\nfrom sklearn.metrics import f1_score\nprint(f1_score(y_test, y_pred), \": is the f1 score\")","dec7d871":"testdf=pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\")\nsub_df=pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")","587f2cef":"print(testdf.shape,sub_df.shape)","ec3059ee":"testdf.drop(columns=\"id\",inplace=True)\ntestdf.head(2)","a503e6e3":"si.transform(testdf)\nqt.transform(testdf)","ea6125e2":"test=testdf.values","f15052ab":"preds = nb_grid.predict_proba(test)\nprint(preds.shape)\npreds","2c86a5a7":"sub_df['claim'] = preds[:,0]\nsub_df.head()","7bfdafd8":"sub_df.to_csv(\"submission_gnb.csv\",index=False)","60576905":"#### In the earlier notebook we did the [EDA](http:\/\/www.kaggle.com\/saileshnair\/tps202109-normal-and-quick-eda)","f4640961":"## Hyperparameter Tuning the Gaussian Naive Bayes Classifier","fa6616f0":"### The best model seems to be Gaussian Naive Bayes. Lets do a submission and check","92bcd12f":"### Kindly let me know if the approach , transformers, imputers, Algorithm Comparison were helpful for you with an upvote or comment to improve my understanding","ab96d394":"## 3. Prepare Data\n## a) Data Cleaning\n## b) Data Transforms","d45dcc55":"#### We've taken care of the null values. \n#### Now to normalize we could use transformers like RobustScaler, PowerTransformer,  QuantileTransformer. In this Notebook I use QuantileTransformer.","1cf4015f":"## 4. Evaluate Algorithms\n## a) Split-out validation dataset\n## b) Test options and evaluation metric\n## c) Compare Algorithms","b3418cbf":"**QuantileTransformer** This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme.","bedfe6c6":"#### In this notebook I'm trying out how to transform the data into a normal distribution\n","c1824bfc":"### First let's take a look at imputation of the null values","8694a1c2":"#### Now that the data is normalized, we can fit a model and check predictions. ","b2b298e4":"#### The Simple Imputer is the quickest imputer. Other I have tried are\n+ KNNImputer() -  this uses the KNN to impute missing values\n+ IterativeImputer(random_state=21) - it is an experimental implementation of imputer in scikit learn - time consuming","7fb91ec0":"#### Simple Imputer","5367b68f":"+ **RobustScaler** This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile).\n+ **Power Transformer + yeo-johnson method**:  Yeo-Johnson supports both positive or negative data.\n+ **Power Transformer + Box Cox method**:  Box-Cox requires input data to be strictly positive. First we need to treat oultliers. I capped the ouliers at the bounds. To remove the negative values I had to explicitely square the values for features containing negative values before applying this transformation."}}