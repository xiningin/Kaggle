{"cell_type":{"e1ddc737":"code","d22ccc32":"code","d6f9a90b":"code","336e2be5":"code","ac524456":"code","b4abb4f7":"code","f054728e":"code","06e75080":"code","2beeece4":"code","9c47e646":"markdown","729beedf":"markdown","6b890464":"markdown","cec47a7a":"markdown"},"source":{"e1ddc737":"#Import Statements\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom keras import layers\nfrom keras.callbacks import EarlyStopping\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\nfrom tensorflow.keras.applications.xception import Xception\nfrom tensorflow.keras.applications.vgg16 import VGG16\n\nfrom tensorflow.keras.optimizers import RMSprop\n\nimport os","d22ccc32":"#Define Paths\ntrain_dir = '..\/input\/horses-or-humans-dataset\/horse-or-human\/train\/'\ntrain_dir_horses = '..\/input\/horses-or-humans-dataset\/horse-or-human\/train\/horses\/'\ntrain_dir_humans = '..\/input\/horses-or-humans-dataset\/horse-or-human\/train\/humans\/'\n\nval_dir = '..\/input\/horses-or-humans-dataset\/horse-or-human\/validation\/'\nval_dir_horses = '..\/input\/horses-or-humans-dataset\/horse-or-human\/validation\/horses\/'\nval_dir_humans = '..\/input\/horses-or-humans-dataset\/horse-or-human\/validation\/humans\/'\n\n#Get total number of training and validation images\nprint('Total number of training horse images: ',len(os.listdir(train_dir_horses)))\nprint('Total number of training human images: ',len(os.listdir(train_dir_humans)))\nprint('Total number of training images: ',len(os.listdir(train_dir_humans)) + len(os.listdir(train_dir_horses)))\nprint('\\n')\nprint('Total number of validation horse images: ',len(os.listdir(val_dir_horses)))\nprint('Total number of validation human images: ',len(os.listdir(val_dir_humans)))\nprint('Total number of validation images: ',len(os.listdir(val_dir_humans)) + len(os.listdir(val_dir_horses)))","d6f9a90b":"#Creating Traing and validation generators along with standardization and augmentation\ntrain_generator = ImageDataGenerator(1.\/255, \n                                     rotation_range=40,\n                                     width_shift_range=0.2,\n                                     height_shift_range=0.2,\n                                     zoom_range=0.2,\n                                     horizontal_flip=True) #We augment the training data in various ways. \n\nval_generator = ImageDataGenerator(1.\/255) #We dont want to augment the validation data. We leave the data as is. \n\n#Batch Generators\ntrain_batch_gen = train_generator.flow_from_directory(train_dir,\n                                                target_size = (150,150),\n                                                batch_size = 20,\n                                                class_mode = 'binary')\n\nval_batch_gen = val_generator.flow_from_directory(val_dir,\n                                                 target_size = (150,150),\n                                                 batch_size = 20,\n                                                 class_mode = 'binary')","336e2be5":"#Learning Rate Reduction callback\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.000001)","ac524456":"#Loading base\nxception_base = Xception(include_top = False,\n                         weights=\"imagenet\",\n                         input_shape=(150,150,3)\n                        )\nxception_base.trainable = True #Make base trainable.\n\nvgg16_base = VGG16(include_top = False,\n                   weights=\"imagenet\",\n                   input_shape=(150,150,3)\n                  )\nvgg16_base.trainable = False #Base is not trainable in this case. ","b4abb4f7":"#Creating Model\nmodel = keras.Sequential([\n    xception_base,\n    layers.Flatten(),\n    layers.Dropout(0.25),\n    layers.Dense(256, activation = 'relu'),\n    layers.Dense(1, activation = 'sigmoid')\n])\n\n#Compiling model\nmodel.compile(optimizer = RMSprop(lr = 2e-5),\n             loss = 'binary_crossentropy',\n             metrics = ['accuracy'])\n\n#Fit model\nhistory = model.fit_generator(train_batch_gen,\n                    epochs = 30,\n                    steps_per_epoch = 50,\n                    validation_data = val_batch_gen,\n                    validation_steps = 12,\n                    callbacks = [learning_rate_reduction]\n         \n)\n#Consider adding an early stopping callback as well in order to prevent overfitting and save on those precious GPU usage minutes.","f054728e":"#Plotting the results\nplt.figure(figsize = (15,5))\n\nplt.subplot(2, 1, 1)\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(loc='lower right')\n# plt.yticks(np.linspace(0.96,0.995))\n\nplt.subplot(2, 1, 2)\nplt.plot(history.history['loss'], label='loss')\nplt.plot(history.history['val_loss'], label='val_loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend(loc='lower left')\n\n\nplt.tight_layout()\nplt.show()","06e75080":"#Second model - VGG16\n#Creating Model\nmodel2 = keras.Sequential([\n    vgg16_base,\n    layers.Flatten(),\n    layers.Dropout(0.25),\n    layers.Dense(256, activation = 'relu'),\n    layers.Dense(1, activation = 'sigmoid')\n])\n\n#Compiling model\nmodel2.compile(optimizer = RMSprop(lr = 2e-5),\n             loss = 'binary_crossentropy',\n             metrics = ['binary_accuracy'])\n\n#Fit model\nhistory2 = model.fit_generator(train_batch_gen,\n                    epochs = 30,\n                    steps_per_epoch = 50,\n                    validation_data = val_batch_gen,\n                    validation_steps = 12,\n                    callbacks = [learning_rate_reduction]\n         \n)","2beeece4":"#Plotting the results\nplt.figure(figsize = (15,5))\n\nplt.subplot(2, 1, 1)\nplt.plot(history2.history['accuracy'], label='accuracy')\nplt.plot(history2.history['val_accuracy'], label='val_accuracy')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(loc='lower right')\n# plt.yticks(np.linspace(0.96,0.995))\n\nplt.subplot(2, 1, 2)\nplt.plot(history2.history['loss'], label='loss')\nplt.plot(history2.history['val_loss'], label='val_loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend(loc='lower left')\n\n\nplt.tight_layout()\nplt.show()","9c47e646":"# Horse or Human?\n---\nThe goal of this project is to use augmented data to build a classifier to detect horse or humans. \n\nOften, when working with image data to build models, we find the same images over and over again. Collecting good quality images of a particular subject requires a lot of work. Instead, this experiment aims to find weather we can create a model that can classify images based on rendered images. This would enable us to create a near infinite amount of training data on virtually any subject in a relatively streamlined and easy process. \n\nThe model uses a pretrained base under a simple head as well as over 1000 rendered images of horses and humans in an attempy to successfully classify them. \n\nMore informaiton about the dataset can be found here: https:\/\/www.kaggle.com\/sanikamal\/horses-or-humans-dataset\n\n---","729beedf":"## Model Creation\n\n---\nWe use two models here: Xception and VGG16. Most solutions to this dataset use VGG16, but i was curious to know weather Xception could outperform it. This would be the first time I am persoanlly using an Xception base as well, which helps further scratch my curiosity itch. \n\nWe use a learning rate reduction callback in order to make sure our model performs optimally. \n\n---","6b890464":"## Data Prep\n\n---\nHere, we use a generator to convert all the images into an array format. We do this to enable our Neural Netowrk to process the data. We also use augmentation during this step to artifically increase the number of images we have to train our model. Furthermore, we also divide all the pixel values of our images by 255 in order to standardize our data between 0 and 1.\n\nIn an image, each pixel is represented by a number between 0 and 255 corresponding to its intensity. By dividing by this number, we ensure that all the data lies between 0 and 1 to make it easier for our Neural Network to work with the data. Smaller numbers will ensure a faster overall process.\n\n\n\n---","cec47a7a":"---\nBoth Xception and VGG16 perform exceptionally well, with Xception doing just slightly better. With slightly better tuning and a more an early stopping callback function we should might be able to squeeze out slightly better performace as well. The extra perfomace might also be attributed to the fact that we allowed the base to be trained. Ordinarily, you dont want to train the base without first also training the head because the random, unoptized weights of the head would cause the pretrained base to completely change its weights, however, in this scenario, I believe it is okay becuase we are using an optimizer with an extremely small learning rate to being with (RMSprop @ 2e-5)\n\nThank you for reading my notebook!    \nIf you have any questions about the notebook or any methods used within, please let me know! \n\nReferenced notebooks:    \nhttps:\/\/www.kaggle.com\/luukhang\/horses-vs-humans-pretrained-vgg16    \nhttps:\/\/www.kaggle.com\/shravanraikar\/computer-vision-using-tensorflow-vgg16    \n\n---"}}