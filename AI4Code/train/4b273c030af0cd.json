{"cell_type":{"7787132b":"code","9b97819f":"code","65d47c7f":"code","2942e3c1":"code","230ad4ee":"code","2f0eb70c":"code","a110f3e7":"code","3234e4b1":"code","3ebaeffb":"code","fba131d3":"code","30ef85f6":"code","e4539ef1":"code","af9ce4e0":"code","047959ea":"code","6b5a92f8":"code","395a36eb":"code","bd7ea1d9":"code","908ef5f1":"code","e3782e6d":"code","ffa050cc":"code","686c2fa7":"code","d9678e78":"code","57be84b9":"code","c89b39c3":"code","0fb95ae6":"code","1808ae95":"code","3a8c5827":"code","c5c577cb":"code","e7ba09dd":"code","c1e145a9":"code","e66d1810":"code","98d0c856":"code","cf1b9152":"code","65513ffd":"code","3ada0dec":"code","bc730d5a":"markdown","a898d2db":"markdown","96db7d09":"markdown","f3ee9b54":"markdown","32485af9":"markdown","37b8bdfd":"markdown","d0ca60f7":"markdown","255c1bd4":"markdown","8e83516e":"markdown","5bf97de3":"markdown","a662b13f":"markdown","d9f2b26a":"markdown","37322d0a":"markdown","4cd1aa5f":"markdown","6b356492":"markdown","30f86a61":"markdown","d1e480b4":"markdown","7cf16c4c":"markdown","a588ba2b":"markdown","a85db3af":"markdown","c74b339d":"markdown","60457f14":"markdown"},"source":{"7787132b":"from pprint import pprint # pretty print sometimes helps printing the data more distinguished\nimport time # to measure execution time\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# For reading and writing datasets\nimport csv\nimport json\n\n# For clustering\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import KMeans\n\n# For plotting\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport seaborn as sns","9b97819f":"# Read the json file\ndf_business = pd.read_json(\"\/kaggle\/input\/yelp-dataset\/yelp_academic_dataset_business.json\", lines=True)\n\n# Get the values that are in British Columbia state\ndf_business = df_business[df_business['state'] == 'BC']\n\n# Get the values that are in Vancouver city (Not used for now)\n# df_business[df_business['city'] == 'Vancouver']\n\ndf_business","65d47c7f":"# Check to see if any of there is any business without business_id\nprint(\"There are\", df_business['business_id'].isnull().values.sum(), \"businesses without a business_id.\")","2942e3c1":"# Check to see if any of there is any business without category\nprint(\"There are\", df_business['categories'].isnull().values.sum(), \"businesses without categories.\")","230ad4ee":"# Look at the businesses without category\ndf_business[df_business['categories'].isnull()]","2f0eb70c":"# Deleting the business with NAN in the categories column\ndf_business = df_business.dropna(subset=['categories'])\n# To address this change in all other datasets, we may eliminate the rows that don't have a matching busness_id with this one.","a110f3e7":"# A business can have multiple categories separated by comma.\n# Make a list of strings for each business (instead of the one-string situation).\ndf_categories = pd.DataFrame(df_business['categories'].apply(lambda x: x.split(', ') if x is not None else []))\n\n# Explode the categories to have individual rows of one business_id and one category related to it.\n# (We'll get redundant business_ids, and that's fine.)\ndf_categories_exploded = df_categories.explode('categories').groupby('categories')","3234e4b1":"# Count the similar categories.\ncat_count = pd.DataFrame(df_categories_exploded.categories.value_counts())\n\n# In the previous process the dataframe had gotten two duplicated indexes (MultiIndex). So, we drop one of them.\ncat_count = cat_count.droplevel(1)\n\n# Since the indexes start with \"categories\",\n# the column name is changed to \"counts\" to prevent any mistake in the future.\ncat_count = cat_count.rename(columns={'categories': 'counts'})\n\n# Now we can sort by \"counts\" confidently, knowing that it's not taking the other name in consideration.\ncat_count = cat_count.sort_values(by=\"counts\", ascending=False)\n\ncat_count","3ebaeffb":"# This is a dictionary that maps the main categories to the number of businesses in the sub-categories of that main category.\nmain_categories = {\"Active Life\": 0, \"Arts and Entertainment\": 0, \"Automotive\": 0,\n                   \"Beauty and Spas\": 0, \"Education\": 0, \"Event Planning and Services\": 0,\n                   \"Financial Services\": 0, \"Food\": 0, \"Health and Medical\": 0,\n                   \"Home Services\": 0, \"Hotels and Travel\": 0, \"Local Flavor\": 0,\n                   \"Local Services\": 0, \"Mass Media\": 0, \"Nightlife\": 0,\n                   \"Pets\": 0, \"Professional Services\": 0, \"Public Services and Government\": 0,\n                   \"Real Estate\": 0, \"Religious Organizations\": 0, \"Restaurants\": 0, \"Shopping\": 0}","fba131d3":"# Counter function\n'''Parameters\n        df: a dataframe of categories as indexes and their counts as the only column\n        category_group: an string array of main categories\n        \n   Returns the number of businesses in that category group\n'''           \ndef sub_category_counter(df, category_group):\n    counter = 0\n    for index, row in df.iterrows():\n        if index in category_group:\n            counter = counter + row['counts']\n    return counter","30ef85f6":"# For each main category do the following.\nfor cat in main_categories:\n    \n    # Read the sub-categories of cat category.\n    path = \"..\/input\/yelpcategorytitles\/\" + cat + \".txt\"\n    sub_categories = open(path, \"r\")\n    \n    sub_categories = [x.strip() for x in sub_categories]\n    \n    # Count the sub-categories using the function above.\n    main_categories[cat] = sub_category_counter(cat_count, sub_categories)","e4539ef1":"# Finally, sort the main categories counts for a better visualization.\nmain_categories_sorted = {k: v for k, v in sorted(main_categories.items(),\n                                                  key=lambda item: item[1],\n                                                  reverse=True)}","af9ce4e0":"fig = plt.figure(figsize=(18,6))\n\nax = fig.add_axes([0,0,1,1])\nax = sns.set_theme(style=\"whitegrid\")\nax = sns.barplot(list(main_categories_sorted.keys()), list(main_categories_sorted.values()))\n\nplt.xticks(rotation=80)\nplt.ylabel('# of Businesses', fontsize=14)\nplt.xlabel('Category', fontsize=14)\n\nplt.show()","047959ea":"# Statistical descriptions\nprint(df_business.describe())\n# box and whisker plots for 10 critical features of df_business\n#----------------------\n# latitude\tLongitude\tstars\treview_count\tis_open\t\nfrom matplotlib import pyplot\ndf_business.latitude.plot(kind='box', subplots=True, sharex=False, sharey=False)\npyplot.show()\ndf_business.longitude.plot(kind='box', subplots=True, sharex=False, sharey=False)\npyplot.show()\ndf_business.stars.plot(kind='box', subplots=True, sharex=False, sharey=False)\npyplot.show()\ndf_business.review_count.plot(kind='box', subplots=True, sharex=False, sharey=False)\npyplot.show()\ndf_business.is_open.plot(kind='box', subplots=True, sharex=False, sharey=False)\npyplot.show()","6b5a92f8":"#Histograms for numerical columns of the database\ndf_business.latitude.hist()\npyplot.show()\ndf_business.longitude.hist()\npyplot.show()\ndf_business.stars.hist()\npyplot.show()\ndf_business.review_count.hist()\npyplot.show()\ndf_business.is_open.hist()\npyplot.show()","395a36eb":"traffic_header = ['business_id', 'weighted_stars', 'date']","bd7ea1d9":"''' Tip dataset '''\n\nstart_t = time.time()\nwith open(\"\/kaggle\/input\/yelp-dataset\/yelp_academic_dataset_tip.json\") as f:\n    with open(\"\/kaggle\/working\/yelp_academic_dataset_traffic.csv\", 'w') as out:\n        \n        traffic_writer = csv.writer(out)\n        traffic_writer.writerow(traffic_header)\n        \n        for line in f:\n            line_dict = json.loads(line)\n            traffic_writer.writerow([line_dict['business_id'],\n                                     None,\n                                     line_dict['date']])\n\nprint(time.time() - start_t)","908ef5f1":"''' Review dataset '''\n\nstart_t = time.time()\nwith open(\"\/kaggle\/input\/yelp-dataset\/yelp_academic_dataset_review.json\") as f:\n    with open(\"\/kaggle\/working\/yelp_academic_dataset_traffic.csv\", 'a') as out:\n        \n        traffic_writer = csv.writer(out)\n        \n        for line in f:\n            line_dict = json.loads(line)\n            \n            # The more engagement a review has, the more reliable it is. However, since it can be zero, we add 1 to balance it.\n            # If the engagement is zero, then star_reliability equals 1, and its multiplication by star wouldn't be effective.\n            star_reliability = line_dict['useful'] + line_dict['funny'] + line_dict['cool'] + 1\n\n            traffic_writer.writerow([line_dict['business_id'],\n                                     line_dict['stars'] * star_reliability,\n                                     line_dict['date']])\n\nprint(time.time() - start_t)","e3782e6d":"''' Check-in dataset '''\n\nstart_t = time.time()\n\nwith open(\"\/kaggle\/input\/yelp-dataset\/yelp_academic_dataset_checkin.json\") as f:\n    with open(\"\/kaggle\/working\/yelp_academic_dataset_traffic.csv\", 'a') as out:\n        \n        traffic_writer = csv.writer(out)\n        \n        for line in f:\n            line_dict = json.loads(line)\n            \n            checkin_dates = line_dict['date'].split(', ')\n\n            if len(checkin_dates) == 0:\n                checkin_dates = [line_dict['date']]\n            # If there's one or more than one date, store them one by one.\n            for date in checkin_dates:\n                traffic_writer.writerow([line_dict['business_id'],\n                                         None,\n                                         date])\n\nprint(time.time() - start_t)","ffa050cc":"import dask.dataframe as dd\n\ndd_traffic = dd.read_csv(\".\/yelp_academic_dataset_traffic.csv\", parse_dates=['date'])","686c2fa7":"# It might take a little while, because it's reading in chunks and computing on all.\ndf_traffic = dd_traffic[dd_traffic.business_id.isin(df_business.business_id)].compute()\ndf_traffic","d9678e78":"grouped_traffic = df_traffic.groupby(by='business_id')\n\n# Lifetime = Last date - first date\nfirst_date = grouped_traffic['date'].min()\nlast_date = grouped_traffic['date'].max()\n# We only keep the days and eliminate the overhead hours\/minutes\/seconds\nlifetime = (last_date.subtract(first_date)).dt.days\n\n# Traffic = Number of tips, reviews, and check-ins\ntraffic = grouped_traffic.size()\n\n# weighted_stars = The average of all weighted stars (that are not Null)\nweighted_stars = df_traffic.dropna(subset=['weighted_stars']).groupby(by='business_id')['weighted_stars'].mean()\n\ndf_traffic_results =  pd.concat([traffic, weighted_stars, lifetime], axis=1, keys=['traffic', 'weighted_stars', 'lifetime'])\ndf_traffic_results","57be84b9":"# Merge based on business_id: df_traffic_results + df_business.\ndf_merged_business = df_business.join(df_traffic_results, on='business_id')\n\n# We collect the columns that might have an influence in a business' lifetime\ndf_influential_vars = df_merged_business[['business_id', 'categories', 'latitude','longitude',\n                                          'traffic', 'stars', 'weighted_stars', 'lifetime']]\n\ndf_influential_vars = df_influential_vars.reset_index()\ndf_influential_vars.pop('index')\ndf_influential_vars","c89b39c3":"# Is there any business with no traffic count?\nprint(\"There are\", (df_merged_business['traffic'] == 0).sum(),\n      \"businesses without any traffic registered.\")\n\n# Is there any busniess with zero lifetime?\nprint(\"There are\", (df_merged_business['lifetime'] == 0).sum(),\n      \"businesses with zero lifetime.\")","0fb95ae6":"# Counter function\n'''Parameters\n        category_series: it expects a column of strings of categories as input.\n        \n   Returns vectors of the input categories. So that each business has a vector that shows its categories.\n   For example if we have 7 categories overall, instead of [chicken, Food] it now has [1, 0, 0, 0, 1, 0, 0].\n'''    \n\ndef get_business_cat_vector(category_series):\n    global cat_count\n\n    category_df = pd.DataFrame(category_series.apply(lambda x: x.split(', ') if x is not None else []),\n                               columns=['categories']).reset_index(drop=True)\n    \n    neighbourhood_cats = category_df.explode('categories').drop_duplicates()\n    category_df[list(neighbourhood_cats['categories'])] = 0\n\n    def set_count(row):\n        for cat in row['categories']:\n            row[cat] = 1\n        return(row)\n    \n    counts_df = category_df.apply(set_count, axis=1).drop('categories', axis=1).transpose()\n    all_cats_df = pd.DataFrame(cat_count.index).join(counts_df, on=['categories'])\n    all_counts_df = all_cats_df.fillna(0).transpose().drop('categories')\n    vectors_df = all_counts_df.apply(lambda x: x.astype('int32').to_numpy(), axis=1)\n    return vectors_df","1808ae95":"# Convert categories to vectors\ncategory_vectors = get_business_cat_vector(df_influential_vars['categories'])\ncategory_vectors_np = np.stack(category_vectors.to_numpy())","3a8c5827":"from sklearn.decomposition import PCA\npca = PCA(n_components=100)\npca.fit(category_vectors_np)\ncategory_vectors_pca = pca.transform(category_vectors_np)\n\n# category_vectors_pca = category_vectors_np\ndf_cat_vec = pd.DataFrame(category_vectors_pca,\n                          columns=['cat_'+ str(i) for i in range(category_vectors_pca.shape[1])])\ndf_cat_vec","c5c577cb":"# Add the vectors to the train dataframe\ndf_train = pd.concat([df_influential_vars, df_cat_vec], axis=1)\n\n# Convert days to years\ndf_train['lifetime'] = (df_train['lifetime'] \/ 365).astype('int')\ndf_train","e7ba09dd":"# Models\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.metrics import r2_score\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.metrics import accuracy_score\nimport sklearn\n\n# We don't want to change the actual df_train, so we get a copy of that.\ndf_temp = df_train.copy()\n# df_temp = df_temp[df_temp['categories'].str.contains('Shopping')]\n\ndf_temp.pop('business_id')\ndf_temp.pop('categories')\ny = df_temp.pop('lifetime').to_numpy().astype('float')\nX = df_temp.to_numpy().astype('float')\n\n# Sometimes scaling helps the modeling to classify better.\n# X = StandardScaler().fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.01, random_state=10, shuffle=True)\n\nLR = LinearRegression().fit(X_train, y_train).predict(X_test)\nLGR = LogisticRegression(random_state=0).fit(X_train, y_train).predict(X_test)\nSVM = SVC(kernel='rbf').fit(X_train, y_train).predict(X_test)\nGNB = GaussianNB().fit(X_train, y_train).predict(X_test)\nDT = DecisionTreeClassifier().fit(X_train, y_train).predict(X_test)\n\ny_pred = np.mean(np.array((np.transpose(LR), np.transpose(LGR), np.transpose(SVM), np.transpose(GNB), np.transpose(DT))), axis=0)\n\n# print('PRED: ', y_pred)\n# print('ACTUAL: ', y_test)\n\nprint('LR:', sklearn.metrics.mean_squared_error(y_test, LR, squared=False))\nprint('LGR:', sklearn.metrics.mean_squared_error(y_test, LGR, squared=False))\nprint('SVM:', sklearn.metrics.mean_squared_error(y_test, SVM, squared=False))\nprint('GNB:', sklearn.metrics.mean_squared_error(y_test, GNB, squared=False))\nprint('DT:', sklearn.metrics.mean_squared_error(y_test, DT, squared=False))\nprint('Total:', sklearn.metrics.mean_squared_error(y_test, y_pred, squared=False))","c1e145a9":"coords = df_business[['longitude', 'latitude']].to_numpy()\n\ncutoff = 0.0045\n\nclustering = AgglomerativeClustering(None, linkage='complete', distance_threshold=cutoff)\ncluster_ids = clustering.fit_predict(coords)\n\ndf_business['neighbourhood'] = cluster_ids # ignore warning\n\nprint(\"The number of neighbourhoods found in BC is:\", max(cluster_ids))","e66d1810":"import matplotlib.pyplot as plt\nfrom matplotlib import cm\n\n# Plot neighborhoods\n\n# filter out some weird businesses in NE Ontario for plotting\nplot_clusters = cluster_ids[coords[:,0] < -74.5]\nplot_coords = coords[coords[:,0] < -74.5]\n\ndef get_color(value):\n    cycle = 20\n    scale = cm.get_cmap('tab20', 12)\n    color_value = (value % cycle) \/ (cycle - 1)\n    return scale(color_value)\n\n# large figure\nplt.figure(figsize=(12,8), dpi= 400)\n\ncolors = []\nfor i, label in enumerate(plot_clusters):\n    colors.append(get_color(int(label)))\n\nplt.scatter(plot_coords[:,0], plot_coords[:,1], c=colors, s=1)\n\nplt.show()","98d0c856":"import folium\nfrom folium.plugins import FastMarkerCluster\n\nlats = df_business['latitude'].tolist()\nlons = df_business['longitude'].tolist()\nlocations = list(zip(lats, lons))\n\nmap1 = folium.Map(location=[49.2827, -123.1207],\n                        tiles = \"Stamen Terrain\",\n                        zoom_start = 12)\nFastMarkerCluster(data=locations).add_to(map1)\nmap1","cf1b9152":"x = df_business.iloc[:, [6 ,7]].values\ncluster_range = range(1225, 1235)\n\n\nwcss = []\nfor i in cluster_range:\n    print(\"looking at cluster: \", i)\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n    kmeans.fit(x)\n    wcss.append(kmeans.inertia_)\n    \n#Plotting the results onto a line graph, allowing us to observe 'The elbow'\nplt.plot(cluster_range, wcss)\nplt.title('The elbow method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS') #within cluster sum of squares\nplt.show()","65513ffd":"# Small K\n\nfrom yellowbrick.cluster import KElbowVisualizer\n    \n# Instantiate the clustering model and visualizer\nmodel = KMeans()\nvisualizer = KElbowVisualizer(model, k=(4,12))\n\nvisualizer.fit(x)        # Fit the data to the visualizer\nvisualizer.show()        # Finalize and render the figure","3ada0dec":"# Large K\n\n#Clustering based on elbow results\nkmeans = KMeans(n_clusters = 1231, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\ny_kmeans = kmeans.fit_predict(x)\n\nfig, ax = plt.subplots(figsize=(12,8), dpi= 400)\nax.scatter(x[:,1], x[:,0], c=y_kmeans, alpha=0.9, s = 1, cmap='jet') # c is colour\n\n\n#Plotting the centroids of the clusters\nax.scatter(kmeans.cluster_centers_[:, 1], kmeans.cluster_centers_[:,0], s = 50, c = 'pink', label = 'Centroids', alpha=0.3)\n\n# plt.legend()\nplt.show()","bc730d5a":"# Clustering Visualization","a898d2db":"# Importing Essential Libraries","96db7d09":"# Identifying the influential variables of each category\n### Step 1: Calculating the lifetime of businesses (the _classes_ or _y_ of our final training dataset).\ndf_traffic now contains all kinds of interactions documented by people, whether it's a review someone's written, a tip someone's given, or a check-in date that has been registered. The first interaction would be the date that store\/business started their work. Similarly, the last date registered displays the closed date. However, there's one additional information in the df_business that comes in handy! To be more accurate, if the _is_open_ is 1, we set the closed date to *None* since it's no longer the closed date but the last date.\n\nNote! Fortunately, there was at least one review for each business (17294 businesses were found).","f3ee9b54":"We've found four businesses with categories as None. Since the data is large enough, four instances are easily neglectable. Yet, we want to see if we can update them because the \"is_open\" value is 1 for all of them. Searching them by name and address, it seems like all, but not the third, don't exist at the claimed location anymore.\n\nThe [third one](https:\/\/www.google.com\/maps\/place\/Printplus\/@49.2879393,-123.1245266,16.26z\/data=!4m5!3m4!1s0x54867183e662c37d:0x136bc9b30dcb2d98!8m2!3d49.2877835!4d-123.1208748)... {we can fix it, is it worth the trouble? :D We straight up delete all the four for now}","32485af9":"### Using Kmeans to cluster the businesses into different neighbourhoods:","37b8bdfd":"# Supervised Learning","d0ca60f7":"# Analyzing the data to extract the traffic of each business:\nTraffic shows how popular that specific business is; not neccessarily how qualified its service is.\n\nStoring all the information in dataframes allocates too much memory, plus, we have to only accept the part of other datasets that has a business_id in business datasets (which is limited to British Columbia now). For these two reasons, we cannot upload the whole datasets in Pandas DataFrames like the business dataset. What's the alternative? We need to read in _chunks_. You can refer to this [blog post](https:\/\/towardsdatascience.com\/why-and-how-to-use-pandas-with-large-data-9594dda2ea4c#:~:text=Here%20comes%20the%20good%20news,fit%20into%20the%20local%20memory.https:\/\/towardsdatascience.com\/why-and-how-to-use-pandas-with-large-data-9594dda2ea4c#:~:text=Here%20comes%20the%20good%20news,fit%20into%20the%20local%20memory.) for more details about reading large data.\n\n### Step 1: Converting the Traffic datasets from JSON to CSV:","255c1bd4":"### Using Elbow Method to find the best K:","8e83516e":"### Step 2: Merging the data together\nNote that we kept the business_id all over different datasets to be able to glue the dataset back together later. Once we do this, we don't care about it anymore. We merge the df_dates which we just gathered to df_business which we had from before.","5bf97de3":"# Reading the Business dataset:","a662b13f":"# Data Cleaning for the Business dataset\nCorrect or replace any issue of:\n* Misspellings {Montgromery street}\n* Outliers {1,2,4,2,4,123,3,4}\n* Incorrect values {invalid zip, neg number}\n* Missing values {6,7,4,3,,4,5,6}\n* Incorrect values {94025, -345,96066,\u2026}\n* Misspellings {Montgromery street}\n* Outliers {1,2,4,2,4,123,3,4}\n* Incorrect values {invalid zip, neg number}\n* Missing values {6,7,4,3,,4,5,6}\n* Incorrect values {94025, -345,96066,\u2026}","d9f2b26a":"### Step 3: Embedding categories (convert them to vectors)","37322d0a":"### Step 3: Prune the Traffic datasets (delete the ones that are not in business dataset)","4cd1aa5f":"# Unsupervised Learning\n### Clustering using Agglomerative clustering algorithm:","6b356492":"### Another visualization for the neighbourhoods that also clusters them realtime:","30f86a61":"### Step 3: Plotting the category distribution:","d1e480b4":"# Other vidualizations\nIn order to evaluate a business's success, we need to define our own customized metric. Let us call it the success metric for now. The variables we aim to use to extract the success metric are the number of reviews, the number of tips, the stars, and the number of check-ins for each business. We want to look closely into the interactions between the mentioned variables using the multivariate and univariate plots.","7cf16c4c":"### A couple of observations!\nBefore we proceed to calculate the influnetial metrics, let's be sure of some values. To make a metric we have to come up with a combination of arithmetic operations. So, we have to know if there are any zeros in the values we want to work with.","a588ba2b":"#  Category distribution visualization\n### Step 1: Pre-processing for the sub-categories:\nThe goal of this step is to extract the number of businesses associated with each individual business. (In other words we want to build a dataframe that has all the categories as index and a column that shows an integer, which is the number of businesses that have that specific category in their business description)","a85db3af":"### Step 2: Reading the Traffic datasets (new csv file) using [Dask](https:\/\/docs.dask.org\/en\/latest\/dataframe.html):\n\nAnother alternative is using [chunksize](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.read_csv.html).","c74b339d":"### Step 2: Pre-processing for the main categories:\n\nCategories in the Yelp dataset are grouped into main categories and defined on the [Yelp's official blog](https:\/\/blog.yelp.com\/2018\/01\/yelp_category_list). The main categories are listed in the \"category_group_titles\" array, and the sub-categories are collected as .txt file formats in a dataset called [yelpcategorytitles](https:\/\/www.kaggle.com\/yaldayazdanpanah\/yelpcategorytitles). Note that there is no redundancy in sub-categories. The goal of this step is to calculate the number of businesses in each main category.","60457f14":"# Run PCA\nAccording to the documentation, PCA is a tool that helps us shrink the data without losing any information. We run the PCA on the category vectors. They used to have 915 columns as it was the number of categories in the dataset. And now, it has 100."}}