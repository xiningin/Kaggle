{"cell_type":{"97fd3e23":"code","4366c779":"code","f7d9e321":"code","cc3abd0d":"code","e51d1ca6":"code","74417203":"code","fa4673be":"code","665abfda":"code","7846e3b1":"code","e83b868e":"code","cfdfe75a":"code","f78172e4":"code","3daa9929":"code","28e228f5":"code","20bd2491":"code","dbd8ed56":"code","477ae2eb":"code","0e8069fc":"code","5708fa4b":"code","24b0da3a":"markdown","34fe25b5":"markdown","e993a31f":"markdown"},"source":{"97fd3e23":"import cudf\nimport cuml\nimport cupy as cp\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nimport os\nfrom scipy.interpolate import interp1d\nimport gc\nfrom cuml.linear_model import LogisticRegression\nfrom cuml.neighbors import KNeighborsClassifier\nfrom cuml.svm import SVC\nfrom cuml.ensemble import RandomForestClassifier\nfrom cuml.preprocessing.TargetEncoder import TargetEncoder\nfrom sklearn.model_selection import GroupKFold, KFold\nfrom cuml.metrics import mean_squared_error\n\nimport soundfile as sf\n# Librosa Libraries\nimport librosa\nimport librosa.display\nimport IPython.display as ipd\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n\nfrom sklearn.metrics import roc_auc_score, label_ranking_average_precision_score","4366c779":"train = cudf.read_csv(\"\/kaggle\/input\/tabular-playground-series-mar-2021\/train.csv\")\ntest = cudf.read_csv(\"\/kaggle\/input\/tabular-playground-series-mar-2021\/test.csv\")\nsample_submission = cudf.read_csv('..\/input\/tabular-playground-series-mar-2021\/sample_submission.csv')","f7d9e321":"target = train['target'].values\ncolumns = test.columns[1:]\ncat_features = columns[:19]\ncat_features","cc3abd0d":"train.head()","e51d1ca6":"test.head()","74417203":"lr_train_oof = cp.zeros((300000,))\nlr_test_preds = 0\nlr_train_oof.shape","fa4673be":"%%time\nNUM_FOLDS = 10\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=137)\n\nfor f, (train_ind, val_ind) in tqdm(enumerate(kf.split(train, target))):\n        #print(f'Fold {f}')\n        train_df, val_df = train.iloc[train_ind][columns], train.iloc[val_ind][columns]\n        train_target, val_target = target[train_ind], target[val_ind]\n        test_df = test.copy()\n        \n        for cat_col in cat_features:\n            te = TargetEncoder()\n            train_df[cat_col] = te.fit_transform(train_df[cat_col], train_target)\n    \n            val_df[cat_col] = te.transform(val_df[cat_col])\n            test_df[cat_col] = te.transform(test_df[cat_col])\n            \n        model = LogisticRegression()\n        model.fit(train_df, train_target)\n        temp_oof = model.predict_proba(val_df)[[1]].values.flatten()\n        temp_test = model.predict_proba(test_df[columns])[[1]].values.flatten()\n\n        lr_train_oof[val_ind] = temp_oof\n        lr_test_preds += temp_test\/NUM_FOLDS\n        \n        print(roc_auc_score(val_target.get(), temp_oof.get()))","665abfda":"print(roc_auc_score(target.get(), lr_train_oof.get()))","7846e3b1":"cp.save('lr_train_oof', lr_train_oof)\ncp.save('lr_test_preds', lr_test_preds)","e83b868e":"%%time\n\nknn_train_oof = cp.zeros((300000,))\nknn_test_preds = 0\nknn_train_oof.shape\n\nNUM_FOLDS = 10\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=137)\n\nfor f, (train_ind, val_ind) in tqdm(enumerate(kf.split(train, target))):\n        #print(f'Fold {f}')\n        train_df, val_df = train.iloc[train_ind][columns], train.iloc[val_ind][columns]\n        train_target, val_target = target[train_ind], target[val_ind]\n        test_df = test.copy()\n        \n        for cat_col in cat_features:\n            te = TargetEncoder()\n            train_df[cat_col] = te.fit_transform(train_df[cat_col], train_target)\n    \n            val_df[cat_col] = te.transform(val_df[cat_col])\n            test_df[cat_col] = te.transform(test_df[cat_col])\n            \n        model = KNeighborsClassifier(n_neighbors=150)\n        model.fit(train_df, train_target)\n        temp_oof = model.predict_proba(val_df)[[1]].values.flatten()\n        temp_test = model.predict_proba(test_df[columns])[[1]].values.flatten()\n\n        knn_train_oof[val_ind] = temp_oof\n        knn_test_preds += temp_test\/NUM_FOLDS\n        \n        print(roc_auc_score(val_target.get(), temp_oof.get()))\n        \nprint('\\nOverall score:', roc_auc_score(target.get(), knn_train_oof.get()))\n\ncp.save('knn_train_oof', knn_train_oof)\ncp.save('knn_test_preds', knn_test_preds)","cfdfe75a":"0.880144183149357","f78172e4":"%%time\n\nrf_train_oof = cp.zeros((300000,))\nrf_test_preds = 0\nrf_train_oof.shape\n\ncu_rf_params = {'n_estimators': 2000,\n    'max_depth': 12,\n    'n_bins': 15,\n    'n_streams': 8\n}\n\nNUM_FOLDS = 10\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=137)\n\nfor f, (train_ind, val_ind) in tqdm(enumerate(kf.split(train, target))):\n        #print(f'Fold {f}')\n        train_df, val_df = train.iloc[train_ind][columns], train.iloc[val_ind][columns]\n        train_target, val_target = target[train_ind], target[val_ind]\n        test_df = test.copy()\n        \n        for cat_col in cat_features:\n            te = TargetEncoder()\n            train_df[cat_col] = te.fit_transform(train_df[cat_col], train_target)\n    \n            val_df[cat_col] = te.transform(val_df[cat_col])\n            test_df[cat_col] = te.transform(test_df[cat_col])\n            \n        model = RandomForestClassifier(**cu_rf_params)\n        model.fit(train_df.astype(np.float32), train_target.astype(np.float32))\n        temp_oof = model.predict_proba(val_df.astype(np.float32))[[1]].values.flatten()\n        temp_test = model.predict_proba(test_df.astype(np.float32)[columns])[[1]].values.flatten()\n\n        rf_train_oof[val_ind] = temp_oof\n        rf_test_preds += temp_test\/NUM_FOLDS\n        \n        print(roc_auc_score(val_target.get(), temp_oof.get()))\n        \nprint('\\nOverall score:', roc_auc_score(target.get(), rf_train_oof.get()))\n\ncp.save('rf_train_oof', rf_train_oof)\ncp.save('rf_test_preds', rf_test_preds)","3daa9929":"print(roc_auc_score(target.get(), 0.6*knn_train_oof.get()+0.4*lr_train_oof.get()))","28e228f5":"print(roc_auc_score(target.get(), 0.5*knn_train_oof.get()+0.25*lr_train_oof.get()+0.25*rf_train_oof.get()))","20bd2491":"sample_submission['target'] = lr_test_preds\nsample_submission.to_csv('submission_lr.csv', index=False)","dbd8ed56":"sample_submission['target'] = knn_test_preds\nsample_submission.to_csv('submission_knn.csv', index=False)","477ae2eb":"sample_submission['target'] = rf_test_preds\nsample_submission.to_csv('submission_rf.csv', index=False)","0e8069fc":"sample_submission['target'] = 0.6*knn_test_preds+0.4*lr_test_preds\nsample_submission.to_csv('submission_blend_0.csv', index=False)","5708fa4b":"sample_submission['target'] = 0.5*knn_test_preds+0.25*lr_test_preds+0.25*rf_test_preds\nsample_submission.to_csv('submission_blend_1.csv', index=False)","24b0da3a":"In this notebook we'll deal with categorical features using Target Encoding. For the sake of consistency, target encoding needs to be applied withing the cross-validation loop; otherwise, we'll be easily leakign targt information to the out-of-fold rows, which can lead to serious overfitting.\n\nWe'll also start with a simple Ridge regression. This is the simplest ML algo, and in general can give us a good idea of what the baseline score would be for our problem.","34fe25b5":"<img src=\"https:\/\/developer.nvidia.com\/sites\/default\/files\/pictures\/2018\/rapids\/rapids-logo.png\"\/>","e993a31f":"[Rapids](https:\/\/rapids.ai) is an open-source GPU accelerated Data Sceince and Machine Learning library, developed and mainatained by [Nvidia](https:\/\/www.nvidia.com). It is designed to be compatible with many existing CPU tools, such as Pandas, scikit-learn, numpy, etc. It enables **massive** acceleration of many data-science and machine learning tasks, oftentimes by a factor fo 100X, or even more. If you are interested in installing and running Rapids locally on your own machine, then you should [refer to the followong instructions](https:\/\/rapids.ai\/start.html)."}}