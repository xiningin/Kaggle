{"cell_type":{"e2c8c94c":"code","2da992e7":"code","d7ba0a50":"code","b92f8497":"code","65e4f2a8":"code","f5002ea6":"code","956c8cba":"code","f3f5a1d0":"code","14a762b9":"code","af3a479a":"code","ed3bf4bf":"code","6a2bc0c8":"code","7d047fd9":"code","159ba91f":"code","71000701":"code","5425fba6":"code","e0eab4c2":"code","9d3317de":"code","2571a1dc":"code","dcce33ed":"code","692bb77d":"code","f6798296":"code","63d08be2":"code","a03cc7b5":"code","49782679":"code","853e3d69":"code","d9761290":"code","6409ee19":"code","85139342":"code","01a609d0":"code","cf794419":"code","503c5f78":"code","a1c130d0":"code","8ec4159e":"code","0a8a19f4":"code","68ccae15":"code","f986c1ae":"code","9470c4bb":"code","bde73715":"code","9ba654d1":"code","ad891666":"code","32d033d6":"code","ab37dfe9":"code","4f1b2d8c":"code","a7a4bcda":"code","9805bc3b":"code","e27c0a4b":"code","b2b2f10f":"code","3cd20aa4":"code","9862a6a8":"code","b8982c93":"code","cf9d174e":"code","ea47ebae":"code","3eaff853":"code","5f4ffe3e":"code","8058e6e6":"code","6436efce":"code","e9d81965":"code","bdb14d81":"code","38961351":"code","60a2a0e2":"code","6e904478":"code","bb1798d9":"code","8130aa71":"code","9852158d":"code","c2db9a55":"code","e1ebfa3f":"code","a609322b":"code","67439c37":"code","e7a839b4":"markdown","73de63fd":"markdown","9ec7444a":"markdown","ac1dc294":"markdown","09a93dc3":"markdown","b9c9232d":"markdown","cb76c6e1":"markdown"},"source":{"e2c8c94c":"import numpy as np\nimport pandas as pd\nimport gc\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# plt.figure(figsize=(16, 6))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","2da992e7":"# from google.colab import drive\n# drive.mount('\/content\/drive')\n","d7ba0a50":"import pandas as pd\ndata_dir = \"\/kaggle\/input\/shopee-sentiment-analysis\/\"\ndftest = pd.read_csv(data_dir+\"test.csv\")\ndftrain = pd.read_csv(data_dir+\"train.csv\")","b92f8497":"print(dftrain.shape)\ndftrain.head()","65e4f2a8":"print(dftrain.groupby(\"rating\")[\"rating\"].value_counts())\ndftrain.groupby(\"rating\")[\"rating\"].hist();\n\n# recommend only 2\u20134 epochs of training for fine-tuning BERT","f5002ea6":"train_df = dftrain\ntest_df = dftest","956c8cba":"import emoji  # https:\/\/pypi.org\/project\/emoji\/\n\nhave_emoji_train_idx = []\nhave_emoji_test_idx = []\n\nfor idx, review in enumerate(dftrain['review']):\n    if any(char in emoji.UNICODE_EMOJI for char in review):\n        have_emoji_train_idx.append(idx)\n        \nfor idx, review in enumerate(dftest['review']):\n    if any(char in emoji.UNICODE_EMOJI for char in review):\n        have_emoji_test_idx.append(idx)","f3f5a1d0":"train_emoji_percentage = round(len(have_emoji_train_idx) \/ train_df.shape[0] * 100, 2)\nprint(f'Train data has {len(have_emoji_train_idx)} rows that used emoji, that means {train_emoji_percentage} percent of the total')\n\ntest_emoji_percentage = round(len(have_emoji_test_idx) \/ test_df.shape[0] * 100, 2)\nprint(f'Test data has {len(have_emoji_test_idx)} rows that used emoji, that means {test_emoji_percentage} percent of the total')","14a762b9":"def emoji_cleaning(text):\n    \n    # Change emoji to text\n    text = emoji.demojize(text).replace(\":\", \" \")\n    \n#     # Delete repeated emoji\n#     tokenizer = text.split()\n#     repeated_list = []\n    \n#     for word in tokenizer:\n#         if word not in repeated_list:\n#             repeated_list.append(word)\n    \n#     text = ' '.join(text for text in repeated_list)\n    text = text.replace(\"_\", \" \").replace(\"-\", \" \")\n    return text","af3a479a":"train_df['demojize_review'] = train_df['review'].apply(emoji_cleaning)\ntest_df['demojize_review'] = test_df['review'].apply(emoji_cleaning)","ed3bf4bf":"import re\ndef review_cleaning(text):\n    \n    # delete lowercase and newline\n    text = text.lower()\n    text = re.sub(r'\\n', '', text)\n    \n    # change emoticon to text\n    text = re.sub(r':\\(', 'dislike', text)\n    text = re.sub(r': \\(\\(', 'dislike', text)\n    text = re.sub(r':, \\(', 'dislike', text)\n    text = re.sub(r':\\)', 'smile', text)\n    text = re.sub(r';\\)', 'smile', text)\n    text = re.sub(r':\\)\\)\\)', 'smile', text)\n    text = re.sub(r':\\)\\)\\)\\)\\)\\)', 'smile', text)\n    text = re.sub(r'=\\)\\)\\)\\)', 'smile', text)\n    text = re.sub(r'=\\)\\)\\)\\)', 'smile', text)\n    \n#     # delete punctuation\n    text = re.sub('[^a-z0-9 ]', '', text)\n    \n    tokenizer = text.split()\n    \n    return ' '.join([text for text in tokenizer]).strip()\n\ntrain_df['clean_review'] = train_df['demojize_review'].apply(review_cleaning)\ntest_df['clean_review'] = test_df['demojize_review'].apply(review_cleaning)","6a2bc0c8":"def delete_repeated_char(text):\n    text = re.sub(r'(\\w)\\1{2,}', r'\\1', text)\n    return text\n\ntrain_df['clean_review'] = train_df['clean_review'].apply(delete_repeated_char)\ntest_df['clean_review'] = test_df['clean_review'].apply(delete_repeated_char)","7d047fd9":"print('Before: ', train_df.loc[92129, 'review'])\nprint('After: ', train_df.loc[92129, 'clean_review'])\n\nprint('\\nBefore: ', train_df.loc[56938, 'review'])\nprint('After: ', train_df.loc[56938, 'clean_review'])\n\nprint('\\nBefore: ', train_df.loc[72677, 'review'])\nprint('After: ', train_df.loc[72677, 'clean_review'])\n\nprint('\\nBefore: ', train_df.loc[36558, 'review'])\nprint('After: ', train_df.loc[36558, 'clean_review'])","159ba91f":"# how many too short sentences\ntrain_df[train_df['clean_review'].str.len() < 2].shape","71000701":"# fill empty review with \"good\" << average word in rating 3\ntest_df.loc[test_df['clean_review'].str.len() <=2, 'clean_review'] = \"good\"\ntrain_df = train_df[train_df['clean_review'].str.len() >= 2]","5425fba6":"# !pip install pyenchant\n# !apt-get install python-enchant --yes","e0eab4c2":"train_tokens = train_df['clean_review']\ntest_tokens = test_df['clean_review']\n\ntokens = ' '.join([text for text in train_tokens] + [text for text in test_tokens]).split(\" \")\ntokens = set(tokens)\n","9d3317de":"import enchant\nd = enchant.Dict(\"en_US\")\n\nunknown_words = []\nfor idx, t in enumerate(tokens):\n    if len(t)<=1:\n        continue\n        \n#     if idx >= 10:\n#         break\n        \n    if not d.check(t):\n#         print(t, d.suggest(t))\n        unknown_words.append(t)\n    \nprint(len(unknown_words), len(tokens), len(unknown_words)*100.0\/len(tokens))","2571a1dc":"# !pip install googletrans","dcce33ed":"# freq_unknown_words = []\n# for idx, t in enumerate(unknown_words):\n#     if len(t)<=1:\n#         continue\n        \n#     if idx%100==0:\n#         print(f\"{idx}\/{len(unknown_words)} => \", len(freq_unknown_words))\n        \n#     n1 = len(train_df[train_df['clean_review'].str.contains(t)])\n#     n2 = len(test_df[test_df['clean_review'].str.contains(t)])\n    \n#     n = n1+n2\n    \n#     if n>=10:\n#         freq_unknown_words.append(t)\n\n\n# print(len(freq_unknown_words), len(unknown_words), len(tokens))","692bb77d":"# import googletrans\n# from googletrans import Translator\n# translator = Translator()\n\n# translated_words = {}\n\n\n# for idx, t in enumerate(freq_unknown_words):\n#     if len(t)<=1:\n#         continue\n    \n#     if idx%100==0:\n#         print(f\"{idx}\/{len(freq_unknown_words)}\")\n        \n#     translated_words[t] = []\n#     for scr_lang in ['id', \"ms\", \"tl\", \"th\"]:\n#         # vi: Vietnam\n        \n#         if scr_lang==\"th\":\n#             th_char = re.sub('[\u0e01-\u0e2e]', '', t)\n#             if th_char==t:\n#                 continue\n        \n#         w = translator.translate(t, src=scr_lang, dest='en').text\n#         if w!=t:\n#             translated_words[t].append((scr_lang, w))\n#             break","f6798296":"# translated_words\n# import json\n# with open('translated_words.json', 'w') as outfile:\n#     json.dump(translated_words, outfile)","63d08be2":"# with open('translated_words.json', 'r') as outfile:\n#     translated_words = json.load(outfile)","a03cc7b5":"def recover_shortened_words(text):\n    \n    slang_words = [\n        (r'\\bbgus\\b', 'awesome'),\n\n\n        (r'\\btq\\b', 'thanks'),\n        (r'\\btks\\b', 'thanks'),\n        \n        (r'\\bsis\\b', 'sister'),\n        (r'\\bsuka\\b', 'love'),\n        (r'\\bssuka\\b', 'love'),\n    ]\n    \n    for w in slang_words:\n        text = re.sub(w[0], w[1], text)\n    \n    for w in translated_words:\n        if len(translated_words[w])==0:\n            continue\n        _, s = translated_words[w][0]\n        text = re.sub(r'\\b'+w+r'\\b', s, text)\n        \n    return text\n\nprint(\"suka\", recover_shortened_words(\"suka\"))\nprint(\"sukaaxasxa\", recover_shortened_words(\"sukaaxasxa\"))","49782679":"rows = []\nfor idx, row in train_df.iterrows():\n    row[\"translated_clean_review\"] = recover_shortened_words(row[\"clean_review\"])\n    rows.append(row.values)\n    \n    if idx%100==0:\n        print(f\"{idx}\/{len(train_df)}\")\n\n\ncolumns = list(train_df.columns)+[\"translated_clean_review\"]\nnew_train = pd.DataFrame(rows, columns=columns)\n\nrows = []\nfor idx, row in test_df.iterrows():\n    row[\"translated_clean_review\"] = recover_shortened_words(row[\"clean_review\"])\n    rows.append(row.values)\n    \n    if idx%100==0:\n        print(f\"{idx}\/{len(test_df)}\")\n\n\ncolumns = list(test_df.columns)+[\"translated_clean_review\"]\nnew_test = pd.DataFrame(rows, columns=columns)\n\n\n\nnew_train.head()","853e3d69":"# train_df['translated_clean_review'] = train_df['clean_review'].apply(recover_shortened_words)\n# test_df['translated_clean_review'] = test_df['clean_review'].apply(recover_shortened_words)","d9761290":"new_test.to_csv('preprocess_test.csv', index=False)\nnew_train.to_csv('preprocess_train.csv', index=False)","6409ee19":"new_train[\"n\"] = new_train['translated_clean_review'].str.len()","85139342":"new_train[new_train['n']<100][\"n\"].hist(bins=100)","01a609d0":"new_train.head()","cf794419":"import pandas as pd\ndata_dir = \"\/kaggle\/input\/shopee-sentiment-analysis\/\"\ndftest = pd.read_csv(data_dir+\"test.csv\")\ndftrain = pd.read_csv(data_dir+\"train.csv\")","503c5f78":"import tensorflow as tf","a1c130d0":"import torch\n# If there's a GPU available...\nif torch.cuda.is_available():    \n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n    print(\"Device: CUDA\")\nelse: \n    device = torch.device(\"cpu\")","8ec4159e":"!pip install transformers","0a8a19f4":"# MODEL_CLASSES = {\n#     'bert': (BertConfig, BertForSequenceClassification, BertTokenizer),\n#     'xlnet': (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer),\n#     'xlm': (XLMConfig, XLMForSequenceClassification, XLMTokenizer),\n#     'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer),\n#     'distilbert': (DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer),\n#     'albert': (AlbertConfig, AlbertForSequenceClassification, AlbertTokenizer)\n# }\n","68ccae15":"dftrain.shape","f986c1ae":"sentences = dftrain.review.values\nlabels = dftrain.rating.values\n\n# 146811\n\n# import random\n# idx = random.sample(range(len(sentences)), 1000)\n# sentences = dftrain.review.values[idx]\n# labels = dftrain.rating.values[idx]\n","9470c4bb":"labels = [l-1 for l in labels]","bde73715":"sentences[0:10], labels[0:10]","9ba654d1":"from transformers import BertTokenizer\n# Load the BERT tokenizer.\nprint('Loading BERT tokenizer...')\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","ad891666":"# Tokenize all of the sentences and map the tokens to thier word IDs.\ndef sentencesToIds(sentences):\n  input_ids = []\n  for sent in sentences:\n      # `encode` will:\n      #   (1) Tokenize the sentence.\n      #   (2) Prepend the `[CLS]` token to the start.\n      #   (3) Append the `[SEP]` token to the end.\n      #   (4) Map tokens to their IDs.\n\n      # This function also supports truncation and conversion\n      encoded_sent = tokenizer.encode(\n                          sent,                      # Sentence to encode.\n                          add_special_tokens = True) # Add '[CLS]' and '[SEP]'\n                          \n      # Add the encoded sentence to the list.\n      input_ids.append(encoded_sent)\n  return input_ids\n\n# input_ids = sentencesToIds(sentences)\n# print('Original: ', sentences[0])\n# print('Token IDs:', input_ids[0])\n","32d033d6":"from keras.preprocessing.sequence import pad_sequences\n# Set the maximum sequence length.\nMAX_LEN = 256\n\nprint('\\nPadding\/truncating all sentences to %d values...' % MAX_LEN)\nprint('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n\n# input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")","ab37dfe9":"def getAttentionMask(input_ids):\n  # Create attention masks\n  attention_masks = []\n  # For each sentence...\n  nUnkToken = 0\n  for sent in input_ids:\n      \n      # Create the attention mask.\n      #   - If a token ID is 0, then it's padding, set the mask to 0.\n      #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n      att_mask = [int(token_id > 0) for token_id in sent]\n      \n      n = sum([int(token_id==tokenizer.unk_token_id) for token_id in sent])\n      nUnkToken += n\n      # Store the attention mask for this sentence.\n      attention_masks.append(att_mask)\n\n  print(\"#UnknownTokens:\", nUnkToken)\n  return attention_masks\n\n# attention_masks = getAttentionMask(input_ids)","4f1b2d8c":"input_ids = sentencesToIds(sentences)\nprint('Max sentence length: ', max([len(sen) for sen in input_ids]))\n\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\nattention_masks = getAttentionMask(input_ids)","a7a4bcda":"import pickle\n\n# Save to file in the current working directory\npkl_filename = \"input_ids.pkl\"\nwith open(pkl_filename, 'wb') as file:\n    pickle.dump(input_ids, file)\n    \npkl_filename = \"attention_masks.pkl\"\nwith open(pkl_filename, 'wb') as file:\n    pickle.dump(attention_masks, file)\n\n# # Load from file\n# with open(pkl_filename, 'rb') as file:\n#     pickle_model = pickle.load(file)\n ","9805bc3b":"print(\"DONE\")","e27c0a4b":"from sklearn.model_selection import train_test_split\n\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n# The DataLoader needs to know our batch size for training, so we specify it  here.\n# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n\nbatch_size = 45\n\ndef preprocess(sentences, labels, test=False):\n  # input_ids = sentencesToIds(sentences)\n  # print('Max sentence length: ', max([len(sen) for sen in input_ids]))\n\n  # input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n  # attention_masks = getAttentionMask(input_ids)\n\n  if not test:\n    # Use 90% for training and 10% for validation.\n    train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=2018, test_size=0.1)\n    # Do the same for the masks.\n    train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=2018, test_size=0.1)\n  else:\n    train_inputs = input_ids\n    train_labels = labels\n    train_masks = attention_masks\n\n    validation_inputs = []\n    validation_labels = []\n    validation_masks = []\n\n  # Convert all inputs and labels into torch tensors\n  train_inputs = torch.tensor(train_inputs)\n  validation_inputs = torch.tensor(validation_inputs)\n\n  train_labels = torch.tensor(train_labels)\n  validation_labels = torch.tensor(validation_labels)\n\n  train_masks = torch.tensor(train_masks)\n  validation_masks = torch.tensor(validation_masks)\n\n\n  \n  # Create the DataLoader for our training set.\n  train_data = TensorDataset(train_inputs, train_masks, train_labels)\n  train_sampler = RandomSampler(train_data)\n  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n  # Create the DataLoader for our validation set.\n  validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n  validation_sampler = SequentialSampler(validation_data)\n  validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n\n  return train_dataloader, validation_dataloader","b2b2f10f":"train_dataloader, validation_dataloader = preprocess(sentences, labels)","3cd20aa4":"import os\nCUDA_LAUNCH_BLOCKING=1\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"","9862a6a8":"import gc\ngc.collect()","b8982c93":"from transformers import BertForSequenceClassification, AdamW, BertConfig\n# Load BertForSequenceClassification, the pretrained BERT model with a single \n# linear classification layer on top. \nmodel = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n    num_labels = 5,\n    output_attentions = False, # Whether the model returns attentions weights.\n    output_hidden_states = False, # Whether the model returns all hidden-states.\n)\n# Tell pytorch to run this model on the GPU.\nmodel.cuda();\n","cf9d174e":"optimizer = AdamW(model.parameters(), lr = 2e-5,eps = 1e-8)\nfrom transformers import get_linear_schedule_with_warmup\n\n# Number of training epochs (authors recommend between 2 and 4)\nepochs = 2\n# Total number of training steps is number of batches * number of epochs.\ntotal_steps = len(train_dataloader) * epochs\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(optimizer,  num_warmup_steps = 0, num_training_steps = total_steps)\n","ea47ebae":"import numpy as np\n# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) \/ len(labels_flat)","3eaff853":"import time\nimport datetime\ndef format_time(elapsed):\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","5f4ffe3e":"import random\n\nseed_val = 42\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\n# Store the average loss after each epoch so we can plot them.\nloss_values = []\n# For each epoch...\nfor epoch_i in range(0, epochs):\n    \n    # ========================================\n    #               Training\n    # ========================================\n    \n    # Perform one full pass over the training set.\n    print(\"\")\n    print('======== Epoch {:} \/ {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n    # Measure how long the training epoch takes.\n    t0 = time.time()\n    # Reset the total loss for this epoch.\n    total_loss = 0\n    \n    model.train()\n    # For each batch of training data...\n    for step, batch in enumerate(train_dataloader):\n        # Progress update every 40 batches.\n        if step % 40 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            \n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n        \n        # print(device)\n        # print(batch[0].shape, batch[1].shape, batch[2].shape, )\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n      \n        # Always clear any previously calculated gradients before performing a\n        model.zero_grad()        \n        \n        # Perform a forward pass (evaluate the model on this training batch).\n        # This will return the loss (rather than the model output) because we have provided the `labels`.\n        \n        # The documentation for this `model` function is here: \n        # https:\/\/huggingface.co\/transformers\/v2.2.0\/model_doc\/bert.html#transformers.BertForSequenceClassification\n        outputs = model(b_input_ids, \n                    token_type_ids=None, \n                    attention_mask=b_input_mask, \n                    labels=b_labels)\n        \n        # The call to `model` always returns a tuple, so we need to pull the \n        # loss value out of the tuple.\n        loss = outputs[0]\n        # Accumulate the training loss over all of the batches so that we can\n        # calculate the average loss at the end. \n        total_loss += loss.item()\n\n        # Perform a backward pass to calculate the gradients.\n        loss.backward()\n        # Clip the norm of the gradients to 1.0.\n        # This is to help prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        # Update parameters and take a step using the computed gradient.\n        optimizer.step()\n        # Update the learning rate.\n        scheduler.step()\n\n    # Calculate the average loss over the training data.\n    avg_train_loss = total_loss \/ len(train_dataloader)            \n    \n    # Store the loss value for plotting the learning curve.\n    loss_values.append(avg_train_loss)\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n        \n    # ========================================\n    #               Validation\n    # ========================================\n    # After the completion of each training epoch, measure our performance on\n    # our validation set.\n    print(\"\")\n    print(\"Running Validation...\")\n    t0 = time.time()\n    # Put the model in evaluation mode--the dropout layers behave differently\n    # during evaluation.\n    model.eval()\n    # Tracking variables \n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_steps, nb_eval_examples = 0, 0\n    # Evaluate data for one epoch\n    for batch in validation_dataloader:\n        \n        # Add batch to GPU\n        batch = tuple(t.to(device) for t in batch)\n        \n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels = batch\n        \n        # Telling the model not to compute or store gradients, saving memory and\n        # speeding up validation\n        with torch.no_grad():        \n            # Forward pass, calculate logit predictions.\n            \n            outputs = model(b_input_ids, \n                            token_type_ids=None, \n                            attention_mask=b_input_mask)\n        \n        # Get the \"logits\" output by the model. The \"logits\" are the output\n        # values prior to applying an activation function like the softmax.\n        logits = outputs[0]\n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        \n        # Calculate the accuracy for this batch of test sentences.\n        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n        \n        # Accumulate the total accuracy.\n        eval_accuracy += tmp_eval_accuracy\n        # Track the number of batches\n        nb_eval_steps += 1\n    # Report the final accuracy for this validation run.\n    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy\/nb_eval_steps))\n    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n\nprint(\"\")\nprint(\"Training complete!\")\n","8058e6e6":"print(\"DONE\")","6436efce":"import plotly.express as px\nf = pd.DataFrame(loss_values)\nf.columns=['Loss']\nfig = px.line(f, x=f.index, y=f.Loss)\nfig.update_layout(title='Training loss of the Model',\n                   xaxis_title='Epoch',\n                   yaxis_title='Loss')\nfig.show()\n","e9d81965":"pkl_filename = \"v1BERT.pkl\"\nwith open(pkl_filename, 'wb') as file:\n    pickle.dump(model, file)\n","bdb14d81":"# model.save_pretrained(data_dir+\"\/v1_BERT\/\")\n\n","38961351":"# model = BertForSequenceClassification.from_pretrained(data_dir+\"\/v1_BERT\/\")  ","60a2a0e2":"# dftest = dftest.head(100)\nprint(dftest.shape)\ndftest.head()","6e904478":"# dftest = dftest.head()","bb1798d9":"print('Predicting labels for {:,} test sentences...'.format(len(dftest.review)))\nfake_labels = [1 for i in range(len(dftest))]\n\nsentences = dftest.review.values\ninput_ids = sentencesToIds(sentences)\nprint('Max sentence length: ', max([len(sen) for sen in input_ids]))\n\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\nattention_masks = getAttentionMask(input_ids)\n\ntest_dataloader, _ = preprocess(dftest.review.values, fake_labels, test=True)","8130aa71":"# Put model in evaluation mode\nmodel.eval()\n# Tracking variables \npredictions , true_labels = [], []\n# Predict \nfor batch in test_dataloader:\n  # Add batch to GPU\n  batch = tuple(t.to(device) for t in batch)\n  \n  # Unpack the inputs from our dataloader\n  b_input_ids, b_input_mask, b_labels = batch\n  \n  # Telling the model not to compute or store gradients, saving memory and \n  # speeding up prediction\n  with torch.no_grad():\n      # Forward pass, calculate logit predictions\n      outputs = model(b_input_ids, token_type_ids=None, \n                      attention_mask=b_input_mask)\n  logits = outputs[0]\n  # Move logits and labels to CPU\n  logits = logits.detach().cpu().numpy()\n  label_ids = b_labels.to('cpu').numpy()\n  \n  # Store predictions and true labels\n  predictions.append(logits)\n  true_labels.append(label_ids)\nprint('DONE.')\n","9852158d":"# predictions[0].shape","c2db9a55":"def get_pred_class(preds):\n  pred_classes = []\n  for p in preds:\n    # print(p.shape)\n    b_pred_class = np.argmax(p, axis=1).flatten() + 1\n    pred_classes = pred_classes + list(b_pred_class)\n  return pred_classes","e1ebfa3f":"dftest[\"rating\"] = get_pred_class(predictions)","a609322b":"dftest[[\"review_id\", \"rating\"]].to_csv(\"v1_BERT.csv\", index=False)","67439c37":"print(\"DONE\")","e7a839b4":"# Set up BERT","73de63fd":"# Preprocess","9ec7444a":"# Check words with dictionary","ac1dc294":"# Load Tokenizer","09a93dc3":"# Getting Prediction","b9c9232d":"### BERT has two constraints:\n1. All sentences must be padded or truncated to a single, fixed length.\n2. The maximum sentence length is 512 tokens.\n\nPadding is done with a special [PAD] token, which is at index 0 in the BERT vocabulary.","cb76c6e1":"# Loading BERT"}}