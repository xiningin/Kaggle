{"cell_type":{"0b18f3e0":"code","9ab4e27f":"code","c5230e52":"code","a8e2ac10":"code","0a41b1e6":"code","fe915f03":"code","a467a44b":"code","ed83bca3":"code","5c34d81f":"code","71a8b267":"code","30ce2fbc":"code","437ad0a1":"code","3fc11c4e":"code","fbe6cc5c":"code","87ed8dad":"code","b07f3e14":"code","ae2738ce":"code","839f8e8b":"code","6ac0e279":"code","935c292f":"code","24bfb9b3":"code","2bc49db8":"code","4d6369ce":"code","07ca0305":"code","092966e8":"code","cfe9557e":"code","5f08194e":"code","b42a54f8":"code","f9ac1db2":"code","0f03ab8c":"code","b3cefa51":"code","a9d49095":"code","f4f9dc9a":"code","d57811c2":"code","ba3c0c1e":"code","9caca1e8":"code","f16d209f":"code","744b340f":"code","348a4633":"code","e584000d":"code","6811dcf0":"code","f2e4cfdc":"code","08ddd850":"code","e48f81df":"code","53e0bc76":"markdown","5fda8aa1":"markdown","c355c649":"markdown","4b17efb0":"markdown"},"source":{"0b18f3e0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9ab4e27f":"import sys\nsys.path.append('..\/input\/sentence-transformers\/sentence-transformers-master')\nsys.path.append('..\/input\/huggingface-bert-variants')\nsys.path.append('..\/input\/huggingface-roberta-variants')","c5230e52":"import random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom tqdm import tqdm\ntqdm.pandas()\nfrom collections import Counter\nimport statsmodels.api as sm\nfrom scipy.stats import skew, iqr, kurtosis\n\n#notebook formatting\n#from rich.jupyter import print\n#from rich.console import Console\n#from rich.theme import Theme\n#from rich import pretty\n\n#visualization imports \nimport seaborn as sns\nimport matplotlib.pyplot as plt \n%matplotlib inline\n\n#sklearn imports\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression, Ridge, BayesianRidge\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n#nlp imports \nimport re\nimport string\nfrom nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.snowball import SnowballStemmer\n\nimport spacy\nfrom textblob import TextBlob","a8e2ac10":"import transformers\nimport sentence_transformers\nfrom sentence_transformers import SentenceTransformer, models","0a41b1e6":"nlp_eng_emb = spacy.load(\"en_core_web_lg\")","fe915f03":"##seed everything\ndef seed_everything(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n\nseed = 42\nseed_everything(seed)","a467a44b":"#define your own rmse and set greater_is_better=False\ndef rmse_custom(y_true, y_pred):\n    return np.sqrt(((y_true - y_pred) ** 2).mean())\n\nrmse = make_scorer(rmse_custom, greater_is_better=False)","ed83bca3":"df_train = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/train.csv')\nprint(f\"Shape df train: {df_train.shape}\")\ndf_test = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/test.csv')\ntest_id = df_test[['id']]\nprint(f\"Shape df test: {df_test.shape}\")","5c34d81f":"#drop columns that we are not going to use in training:\ncolumns_drop = ['url_legal', 'license', 'id', 'standard_error']\nfor df in [df_train, df_test]:\n    for col in columns_drop:\n        try: df = df.drop(col, axis=1)\n        except: pass","71a8b267":"def get_pos(excerpt):\n    '''Returns number of nouns, adj and verbs in the text'''\n    nouns = ['NN', 'NNS', 'NNP', 'NNPS']\n    adjectives = ['JJ', 'JJR', 'JJS']\n    verbs = ['VB','VBD','VBG','VBN','VBP','VBZ']\n    #first tokenize in words\n    text_tokens = word_tokenize(excerpt)\n    pos_lst = pos_tag(text_tokens)\n    num_nouns = len([noun[0] for noun in pos_lst if noun[1] in nouns])\n    num_adj = len([adj[0] for adj in pos_lst if adj[1] in adjectives])\n    num_verbs = len([verb[0] for verb in pos_lst if verb[1] in verbs])\n    \n    return [num_nouns, num_adj, num_verbs]","30ce2fbc":"#get polarity and subjecivity \n\ntext_blob_obj = TextBlob('I hate football')\n","437ad0a1":"text_blob_obj.sentiment","3fc11c4e":"text_blob_obj.sentiment.polarity","fbe6cc5c":"text_blob_obj.sentiment.subjectivity","87ed8dad":"#First let\u00b4s generate new features in the train and test set dataset\n\nfor df in [df_train, df_test]:\n    print(f\" ====== Generating Counting features..... =======\")\n    df['excerpt_length'] = df['excerpt'].apply(lambda x: len(x))\n    df['excerpt_num_words'] = df['excerpt'].apply(lambda x: len(word_tokenize(x)))\n    df['excerpt_num_sentences'] = df['excerpt'].apply(lambda x: len(sent_tokenize(x)))\n    df['count_exclamation_mark'] = df['excerpt'].apply(lambda x: x.count('!'))\n    df['count_question_mark'] = df['excerpt'].apply(lambda x: x.count('?'))\n    df['count_punctuation'] =  df['excerpt'].apply(lambda x: sum([x.count(punct) for punct in '.,;:']))\n\n    #POS features\n    print(f\" ====== Generating POS features..... =======\")\n    df['num_nouns'], df['num_adj'], df['num_verbs'] = zip(*df['excerpt'].apply(lambda x: get_pos(x)))\n    # proportion of nouns, adj and verbs with respect to the total number of words\n    df['nouns_proportion'] = df['num_nouns'] \/ df['excerpt_num_words']\n    df['adj_proportion'] = df['num_adj'] \/ df['excerpt_num_words']\n    df['verbs_proportion'] = df['num_verbs'] \/ df['excerpt_num_words']\n    \n    #Text Blob features: polarity and subjectivity\n    print(f\" ====== Generating Text Blob features..... =======\")\n    df['polarity'] = df['excerpt'].apply(lambda x: TextBlob(x).sentiment.polarity)\n    df['subjectivity'] = df['excerpt'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n\n    #More additional features\n    print(f\" ====== Generating Additional Features.... =======\")\n    df['num_words_capital'] = df['excerpt'].apply(lambda x: len([word for word in x.split() if word.istitle()]))\n    #average lenght of tokens (words) and sentences in each excerpt\n    df['avg_len_words'] = df['excerpt'].apply(lambda x: np.mean([len(word) for word in x.split()]))\n    df['avg_len_sentences'] = df['excerpt'].apply(lambda x: np.mean([len(sent) for sent in sent_tokenize(x)]))","b07f3e14":"def clean_text(excerpt, remove_stopwords=False, lemmatizer=False):\n    #remove punctuation '!\"#$%&\\'()*+,-.\/:;<=>?@[\\\\]^_`{|}~'\n    excerpt = re.sub(f\"[{string.punctuation}]\", '', excerpt)\n    weird_characters = ['\\n', '\u2014', '\u201d', '\u201c', '\\xad', '\u2026', '\u00bd', '\u00bc', \n                            '\u00e6', '\u00b0', '\u00b1', '\u00b7', '\u2018', '\u00b4', '\u2013', '\u00f7']\n    for character in weird_characters:\n        excerpt = excerpt.replace(character, \"\")\n    #remove numbers\n    excerpt = re.sub('[0-9]', '', excerpt)\n    text_token = word_tokenize(excerpt)\n    \n    clean_words_list = [w for w in text_token if len(w) > 2]\n    \n    #remove stopwords\n    if remove_stopwords:\n        stop_words = [w for w in stopwords.words('english')]\n        clean_words_list = [w for w in clean_words_list if w not in set(stop_words) and len(w) > 2]\n    \n    ##TODO: add lemmatizer##\n    if lemmatizer:\n        lemmatizer = WordNetLemmatizer()\n        clean_words_list = [lemmatizer.lemmatize(word) for word in clean_words_list]\n        lemmatizer_snow_ball = SnowballStemmer(\"english\")\n        clean_words_list = [lemmatizer_snow_ball.stem(word) for word in clean_words_list]\n        \n    clean_text = ' '.join(clean_words_list)\n\n    return clean_text.lower()","ae2738ce":"def clean_text_sbert(excerpt, remove_stopwords=False, lemmatizer=False):\n    #remove punctuation '!\"#$%&\\'()*+,-.\/:;<=>?@[\\\\]^_`{|}~'\n    remove_punctuation = '\"#$%&\\'()*+,-\/<=>?@[\\\\]^_`{|}~'\n    excerpt = re.sub(f\"[{remove_punctuation}]\", '', excerpt)\n    excerpt = excerpt.lower()\n    weird_characters = ['\\n', '\u2014', '\u201d', '\u201c', '\\xad', '\u2026', '\u00bd', '\u00bc', \n                            '\u00e6', '\u00b0', '\u00b1', '\u00b7', '\u2018', '\u00b4', '\u2013', '\u00f7']\n    for character in weird_characters:\n        excerpt = excerpt.replace(character, \"\")\n    #remove numbers\n    excerpt = re.sub('[0-9]', '', excerpt)\n    \n    #clean_words_list = [w for w in text_token if len(w) > 2]\n    \n    #clean_text = ' '.join(clean_words_list)\n\n    return excerpt","839f8e8b":"# df_train['excerpt'] = df_train['excerpt'].apply(lambda x: clean_text_sbert(x, remove_stopwords=False, lemmatizer=False))\n# df_test['excerpt'] = df_test['excerpt'].apply(lambda x: clean_text_sbert(x, remove_stopwords=False, lemmatizer=False))","6ac0e279":"ner_entities_lst = ['PERSON', 'NORP', 'FAC', 'ORG', 'GPE', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART',\n                    'LAW', 'LANGUAGE', 'DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL']\n\nfor ent in ner_entities_lst:\n    df_train['count_' + ent] = 0.0\n    df_test['count_' + ent] = 0.0","935c292f":"def get_ner_tags(excerpt):\n    entities = []\n    doc = nlp_eng_emb(excerpt)\n    for ent in doc.ents:\n        entities.append(ent.label_)\n    return dict(Counter(entities))","24bfb9b3":"df_train['ner'] = df_train['excerpt'].apply(get_ner_tags)\ndf_test['ner'] = df_test['excerpt'].apply(get_ner_tags)","2bc49db8":"for df in [df_train, df_test]:\n    for index, row in tqdm(df.iterrows()):\n        for key, value in row['ner'].items():\n            df.loc[index, 'count_' + key] = value\n    del df['ner']    ","4d6369ce":"##SELECT FETAURES \n\nfeatures_training = []\n\ncol_vec_dim = [i for i in range(256)] #dimensions of the vector\nfeatures_training.extend(col_vec_dim)\n\nfeatures_eda = ['excerpt_length', 'excerpt_num_words', 'excerpt_num_sentences','num_nouns', 'num_adj', \n                   'num_verbs', 'nouns_proportion','adj_proportion', 'verbs_proportion', 'num_words_capital',\n                   'count_exclamation_mark', 'count_question_mark', 'count_punctuation', 'avg_len_words', 'avg_len_sentences']\nfeatures_training.extend(features_eda)\n\nfeatures_training.extend(['polarity', 'subjectivity'])\n\nfeatures_ner = ['count_' + ent for ent in ner_entities_lst] #features NER\nfeatures_training.extend(features_ner)","07ca0305":"from torch import nn","092966e8":"#BASE_MODEL = '\/kaggle\/input\/huggingface-bert-variants\/bert-base-uncased\/bert-base-uncased'\nBASE_MODEL = '\/kaggle\/input\/huggingface-roberta-variants\/roberta-base\/roberta-base'\nword_embedding_model = models.Transformer(BASE_MODEL)\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\ndense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), \n                           out_features=256, \n                           activation_function=nn.Tanh())\nmodel_sbert = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])","cfe9557e":"df = pd.DataFrame(columns=[i for i in range(256)])\nfor excerpt in tqdm(list(df_train['excerpt'])):\n#     vec = nlp_eng_emb(excerpt).vector\n    vec =  model_sbert.encode(excerpt)\n    vec_series = pd.Series(list(vec), index=df.columns)\n    df = df.append(vec_series, ignore_index=True)\n    \n    \ntrain = pd.concat([df_train, df], axis=1)\ntrain.head()","5f08194e":"test = pd.DataFrame(columns=[i for i in range(256)])\nfor excerpt in tqdm(list(df_test['excerpt'])):\n#     vec = nlp_eng_emb(excerpt).vector\n    vec =  model_sbert.encode(excerpt)\n    vec_series = pd.Series(list(vec), index=df.columns)\n    test = test.append(vec_series, ignore_index=True)\n\ndf_pred = pd.concat([df_test, test], axis=1)\ncolumns_drop = ['url_legal', 'license', 'id', 'excerpt']\nfor col in columns_drop:\n    df_pred = df_pred.drop(col, axis=1)\ndf_pred.head()","b42a54f8":"\nX_train, X_test, y_train, y_test = train_test_split(train, train['target'], test_size=0.01,\n                                                   random_state=seed)\n\nprint(f\"Shape X_train: {X_train.shape}\")\nprint(f\"Shape X_test: {X_test.shape}\")\n\nX_train = train[features_training]\ny_train = train['target']\n#filter X_train by columns training","f9ac1db2":"pipe_spacy = Pipeline([('scaler', MinMaxScaler()),\n                        ('bayesian_ridge', BayesianRidge())]\n                        )\n\nparam_grid_spacy = {\n\n}\n\nmodel_spacy = GridSearchCV(estimator=pipe_spacy,\n                                param_grid=param_grid_spacy,\n                                scoring=rmse,\n                                cv=10,\n                                verbose=3)\n\n\nmodel_spacy.fit(X_train, y_train)\nprint(f'Best params are : {model_spacy.best_params_}')\nprint(f'Best training score: {round(model_spacy.best_score_, 5)}')\n\n#y_pred = model_spacy.predict(X_test[features_training])\n#print(f\"RMSE baseline with testing set: {round(rmse_custom(y_test, y_pred), 5)}\")","0f03ab8c":"import xgboost as xgb","b3cefa51":"xgb_regressor = xgb.XGBRegressor(booster='gbtree', \n                                reg_lambda=10,\n                                max_depth=4)","a9d49095":"xgb_regressor.fit(X_train, y_train)","f4f9dc9a":"y_train","d57811c2":"mean_squared_error(np.array(y_train), xgb_regressor.predict(X_train), squared=False)","ba3c0c1e":"# pipe_spacy = Pipeline([('scaler', MinMaxScaler()),\n#                         ('svr', SVR())]\n#                         )\n\n# param_grid_spacy = {\n#     'svr__C': [1, 2.2, 4, 8],\n#     'svr__gamma': [0.1, 0.08, 0.01],\n#     'svr__kernel': ['rbf'], \n#     'svr__epsilon': [1, 0.1, 0.01]\n\n# }\n\n# model_spacy = GridSearchCV(estimator=pipe_spacy,\n#                                 param_grid=param_grid_spacy,\n#                                 scoring=rmse,\n#                                 cv=10,\n#                                 verbose=3)\n\n\n# model_spacy.fit(X_train[features_training], y_train)\n# print(f'Best params are : {model_spacy.best_params_}')\n# print(f'Best training score: {round(model_spacy.best_score_, 5)}')\n\n#y_pred = model_spacy.predict(X_test[features_training])\n#print(f\"RMSE baseline with testing set: {round(rmse_custom(y_test, y_pred), 5)}\")","9caca1e8":"from sklearn.model_selection import StratifiedKFold","f16d209f":"#create groups\/bins in the target \ntrain['target_bin'] = pd.cut(train['target'], bins=10, labels=[i + 1 for i in range(10)])","744b340f":"skfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)","348a4633":"X_train = train[features_training]\ny_train = train['target']","e584000d":"from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import MaxAbsScaler","6811dcf0":"predictions = []\ntraining_scores = []\n\ndf_out_of_fold = train.copy()\ndf_out_of_fold['out_of_fold'] = 0\n\nfor fold, (train_idx, val_idx) in enumerate(list(skfold.split(X=X_train, y=train['target_bin']))):\n    print(f\"\\n Training FOLD: {fold + 1} \/ {10}\")\n    \n    #bayesian_ridge = BayesianRidge()\n    pipe_br = make_pipeline(MinMaxScaler(), BayesianRidge())\n    pipe_br.fit(X_train.loc[train_idx], y_train.loc[train_idx])\n    train_rmse = mean_squared_error(y_train.loc[train_idx], pipe_br.predict(X_train.loc[train_idx]), squared=False)\n    training_scores.append(train_rmse)\n    print(f'Fold {fold + 1}: Training score: {round(train_rmse, 4)}')\n    \n    #precit traget for each fold >> submission values\n    #print(pipe_br.predict(df_pred))\n    predictions.append(pipe_br.predict(df_pred[features_training]))\n    #now let\u00b4s predict the results with the validation (not used for training) set of each fold\n    pred_oof = pipe_br.predict(X_train.loc[val_idx])\n    df_out_of_fold['out_of_fold'].iloc[val_idx] += pred_oof\n\nprint(f'Training score: {round(np.mean(training_scores), 4)}, Training STD: {round(np.std(training_scores), 4)}')\nprint(f'Oout of fold score across folds: {round(mean_squared_error(df_out_of_fold.target, df_out_of_fold.out_of_fold, squared=False), 5)}')","f2e4cfdc":"predictions = xgb_regressor.predict(df_pred)","08ddd850":"test_id['target'] = np.mean(predictions, axis=0)\ndf_submission = test_id[['id', 'target']]\ndf_submission.to_csv('submission.csv', index=False)","e48f81df":"df_submission","53e0bc76":"### XGBOOST REGRESSOR\n\n- gamma: The default is 0. Values of less than 10 are standard. Increasing the value prevents overfitting.\n- reg_alpha: L1 regularization on leaf weights. Larger values mean more regularization and prevent overfitting. The default is 0.\n- reg_lambda: L2 regularization on leaf weights. Increasing the value prevents overfitting. The default is 1\n- booster: gbtree","5fda8aa1":"### INFERIENCE","c355c649":"### CROSS VALIDATION STRATEGY\n\n- out of fold (oof) score based on predictions made by data not used to train a model, using the validations folds.\n- Traget follows a normal distribution, we are going to stratify the training dataset using that column","4b17efb0":"## 2. Modeling"}}