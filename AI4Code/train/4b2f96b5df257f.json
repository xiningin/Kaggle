{"cell_type":{"24f0ba39":"code","dd46e421":"code","fcda93a8":"code","f7a51119":"code","6be8a96b":"code","0466f4b7":"code","c9ad3ba6":"code","260d3220":"code","4a177b85":"code","03896ff7":"code","a8ea608b":"code","65546ff5":"code","3a56c6b2":"code","274f1e6e":"code","7bd67fd9":"code","9b1c2127":"code","04ca46cf":"code","3b0ed879":"code","97944457":"code","871f65d7":"code","9d8e4c0e":"code","c68609a3":"code","364cf2d8":"code","9b06ad8f":"code","d65e2367":"code","a6fd4dae":"code","32dee4e9":"code","27e04468":"code","9cdf3e6e":"code","60143bec":"code","cf7fdd6c":"code","d911f3cc":"code","e42ca921":"code","2f836870":"code","3e50f631":"code","0447b8a0":"code","5c5ae1fb":"code","37cb308c":"markdown","704c1380":"markdown","c6a34233":"markdown","7cd4706b":"markdown","8f7fdc07":"markdown","1603dc58":"markdown","8a5c37a6":"markdown","7e9398dc":"markdown","4b447cf5":"markdown","89bc14b2":"markdown","107e2ca9":"markdown","d7a11c0e":"markdown","47284304":"markdown","ccd27a63":"markdown","d16375ac":"markdown","8a2cedb9":"markdown","36f1ad3a":"markdown","ff4cbad9":"markdown","d3b8cbc5":"markdown","05bf20b4":"markdown","f23735a3":"markdown","c953be0f":"markdown","96dcded3":"markdown","229f8496":"markdown","e4bd7ce0":"markdown","a20e7ccd":"markdown","4528488a":"markdown","305e91e8":"markdown","d8364912":"markdown","ebba8f80":"markdown","2d6d1128":"markdown","f3f1c83a":"markdown","cea8f78c":"markdown","0e1f3a7b":"markdown","67db4c90":"markdown","fafe9513":"markdown","4193c9ed":"markdown","6002cec0":"markdown","fb45da46":"markdown"},"source":{"24f0ba39":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy.stats\nimport seaborn as sns\n\npd.set_option('display.float_format', lambda x: '%.3f' % x)\ndataset = pd.read_csv('..\/input\/mushroom-classification\/mushrooms.csv', header=0, delimiter=',',na_values='?' )","dd46e421":"dataset.info()","fcda93a8":"dataset['class'].value_counts()\/len(dataset)","f7a51119":"caps = dataset['cap-shape'].value_counts()\nprint(caps)\ncap_labels = caps.axes[0].tolist() \npoisonous = []\nedible = [] \nind = np.arange(6) \nfor cap in cap_labels:\n    size = len(dataset[dataset['cap-shape'] == cap].index)\n    edibles = len(dataset[(dataset['cap-shape'] == cap) & (dataset['class'] == 'e')].index)\n    edible.append(edibles)\n    poisonous.append(size-edibles)\n\nwidth = 0.40\nfig, ax = plt.subplots(figsize=(12,7))\nedible_bars = ax.bar(ind, edible , width, color='#93c244')\npoison_bars = ax.bar(ind+width, poisonous , width, color='#f44336')\n\nax.set_title('Edible and Poisonous based on cap shape',fontsize=22)\nax.set_xticks(ind + width \/ 2)\nax.set_xticklabels(('convex', 'flat', 'knobbed', 'bell', 'sunken', 'conical'),fontsize = 12)\nax.legend((edible_bars,poison_bars),('edible','poisonous'),fontsize=17)\n\nplt.show()\n","6be8a96b":"popul = dataset['population'].value_counts()\nprint(popul)\npopul_labels = popul.axes[0].tolist() \npoisonous = []\nedible = []\nind = np.arange(6) \nfor pop in popul_labels:\n    size = len(dataset[dataset['population'] == pop].index)\n    edibles = len(dataset[(dataset['population'] == pop) & (dataset['class'] == 'e')].index)\n    edible.append(edibles)\n    poisonous.append(size-edibles)\n\nwidth = 0.40\nfig, ax = plt.subplots(figsize=(12,7))\nedible_bars = ax.bar(ind, edible , width, color='#93c244')\npoison_bars = ax.bar(ind+width, poisonous , width, color='#f44336')\n\nax.set_title('Edible and Poisonous based on population',fontsize=22)\nax.set_xticks(ind + width \/ 2)\nax.set_xticklabels(('several','solitary', 'scattered', 'numerous','abundant', 'clustered'),fontsize = 12)\nax.legend((edible_bars,poison_bars),('edible','poisonous'),fontsize=17)\n\nplt.show()","0466f4b7":"ringt = dataset['ring-type'].value_counts()\nprint(ringt)\nringt_labels = ringt.axes[0].tolist() \npoisonous = []\nedible = []\nind = np.arange(5) \nfor rt in ringt_labels:\n    size = len(dataset[dataset['ring-type'] == rt].index)\n    edibles = len(dataset[(dataset['ring-type'] == rt) & (dataset['class'] == 'e')].index)\n    edible.append(edibles)\n    poisonous.append(size-edibles)\n\nwidth = 0.40\nfig, ax = plt.subplots(figsize=(12,7))\nedible_bars = ax.bar(ind, edible , width, color='#93c244')\npoison_bars = ax.bar(ind+width, poisonous , width, color='#f44336')\n\nax.set_title('Edible and Poisonous based on ring type',fontsize=22)\nax.set_xticks(ind + width \/ 2)\nax.set_xticklabels(('pendant','evanescent', 'large', 'flaring','none'),fontsize = 12)\nax.legend((edible_bars,poison_bars),('edible','poisonous'),fontsize=17)\nplt.show()","c9ad3ba6":"dataset.info()","260d3220":"dataset.nunique()","4a177b85":"dataset.drop(columns=['veil-type'], axis=1, inplace=True)","03896ff7":"dataset.isnull().sum()","a8ea608b":"dataset['stalk-root'] = dataset['stalk-root'].fillna(dataset['stalk-root'].mode()[0])\ndataset['stalk-root'].isnull().sum()","65546ff5":"from sklearn.preprocessing import LabelEncoder\n\nlab = LabelEncoder()\nfor col in dataset.columns:\n\n    dataset[col] = lab.fit_transform(dataset[col])\n\ndataset.head()","3a56c6b2":"dataset.describe()","274f1e6e":"from sklearn.preprocessing import StandardScaler\n\nX = dataset.loc[:, dataset.columns != 'class']\nY = dataset.loc[:, dataset.columns == 'class']\nY = Y.to_numpy().ravel()\n\nscaler = StandardScaler()\ncols = X.columns\nX = scaler.fit_transform(X)\nX = pd.DataFrame(X, columns=[cols])\nX.describe()\nX = X.to_numpy()\n","7bd67fd9":"plt.figure(figsize=(16,12))\nax = sns.heatmap(dataset.corr(), annot=True, fmt='.2f')\nax.set_title('Correlations mushrooms')","9b1c2127":"from sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import neighbors\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score,  precision_score, recall_score, roc_curve,roc_auc_score, auc\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix, precision_recall_curve\nfrom sklearn.model_selection import cross_val_score","04ca46cf":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33)\n\ndecisionTree = DecisionTreeClassifier()\nlogreg = LogisticRegression(max_iter=100000)\nknn = neighbors.KNeighborsClassifier()\nRandomForest = RandomForestClassifier()\nsvc = svm.SVC()\nNB = GaussianNB()\nMLP = MLPClassifier()\n\ndecisionTree.fit(X_train,Y_train)\nlogreg.fit(X_train,Y_train)\nknn.fit(X_train,Y_train)\nRandomForest.fit(X_train,Y_train)\nsvc.fit(X_train,Y_train)\nNB.fit(X_train,Y_train)\nMLP.fit(X_train,Y_train)\n\nY_decisionTree = decisionTree.predict(X_test)\nY_logreg = logreg.predict(X_test)\nY_knn = knn.predict(X_test)\nY_RandomForest = RandomForest.predict(X_test)\nY_svc = svc.predict(X_test)\nY_NB = NB.predict(X_test)\nY_MLP = MLP.predict(X_test)","3b0ed879":"accuracy = []\n\naccuracy.append(metrics.accuracy_score(Y_test, Y_decisionTree))\naccuracy.append(metrics.accuracy_score(Y_test, Y_logreg))\naccuracy.append(metrics.accuracy_score(Y_test, Y_knn))\naccuracy.append(metrics.accuracy_score(Y_test, Y_RandomForest))\naccuracy.append(metrics.accuracy_score(Y_test, Y_svc))\naccuracy.append(metrics.accuracy_score(Y_test, Y_NB))\naccuracy.append(metrics.accuracy_score(Y_test, Y_MLP))\n\nplt.plot([1,2,3,4,5,6,7], accuracy,  'ro')\nplt.xticks([1,2,3,4,5,6,7],['Tree','LogRegression','KNN','Forest', 'SVM','NB','MLP'])\nplt.ylabel('Accuracy')\nplt.show()","97944457":"fscore = []\n\nfscore.append(metrics.f1_score(Y_test, Y_decisionTree))\nfscore.append(metrics.f1_score(Y_test, Y_logreg))\nfscore.append(metrics.f1_score(Y_test, Y_knn))\nfscore.append(metrics.f1_score(Y_test, Y_RandomForest))\nfscore.append(metrics.f1_score(Y_test, Y_svc))\nfscore.append(metrics.f1_score(Y_test, Y_NB))\nfscore.append(metrics.f1_score(Y_test, Y_MLP))\n\nplt.plot([1,2,3,4,5,6,7], fscore,  'ro')\nplt.xticks([1,2,3,4,5,6,7],['Tree','LogRegression','KNN','Forest', 'SVM','NB','MLP'])\nplt.ylabel('F-Score')\nplt.show()","871f65d7":"scomean=[]\nlogreg = LogisticRegression(max_iter=100000)\nfor k in range(2, 15):\n    scomean.append(cross_val_score(logreg, X, Y, cv = k, scoring='f1').mean())\n\nplt.plot(range(2,15), scomean, label='CrosVal')\nplt.legend()\nplt.xlabel('K-fold')\nplt.ylabel('Score')","9d8e4c0e":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33)\n\nlogreg = LogisticRegression(max_iter=100000)\nlogreg.fit(X_train, Y_train)\nlr_probs = logreg.predict_proba(X_test)\nlr_probs = lr_probs[:, 1]\n\nlr_auc = roc_auc_score(Y_test, lr_probs)\nprint('ROC AUC =', lr_auc)\nlr_fpr, lr_tpr, _ = roc_curve(Y_test, lr_probs)\n\nplt.plot(lr_fpr, lr_tpr, marker='.')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","c68609a3":"lr_precision, lr_recall, _ = precision_recall_curve(Y_test, lr_probs)\nlr_auc = auc(lr_recall, lr_precision)\n\nprint('AUC =', lr_auc)\nplt.plot(lr_recall, lr_precision, marker='.')\n# axis labels\nplt.xlabel('Recall')\nplt.ylabel('Precision')\n# show the plot\nplt.ylim((0,1.1))\nplt.show()","364cf2d8":"from sklearn.metrics import classification_report\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33)\n\nlogreg = LogisticRegression(max_iter=100000)\nlogreg.fit(X_train, Y_train)\nY_logreg = logreg.predict(X_test)\nprint(classification_report(Y_test, Y_logreg))","9b06ad8f":"from sklearn.model_selection import RandomizedSearchCV\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33)\ntuned_parameters = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] ,'penalty':['l1','l2'], 'max_iter':[10,50,100,1000,10000,100000]}\nlogRegression = LogisticRegression()\nLR = RandomizedSearchCV(logRegression, tuned_parameters, random_state=0, cv=8, scoring='accuracy', n_iter = 30, n_jobs=-1)\n","d65e2367":"LR.fit(X_train,Y_train)","a6fd4dae":"y_LR = LR.predict(X_test)\nprint(LR.best_score_)\nprint(LR.best_params_)\nprint('Accuracy =', metrics.accuracy_score(Y_test, y_LR))\nprint('F-score =', metrics.f1_score(Y_test, y_LR))","32dee4e9":"scomean=[]\nNB = GaussianNB()\nfor k in range(2, 15):\n    scomean.append(cross_val_score(NB, X, Y, cv = k, scoring='f1').mean())\n\nplt.plot(range(2,15), scomean, label='CrosVal')\nplt.legend()\nplt.xlabel('K-fold')\nplt.ylabel('Score')","27e04468":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33)\n\nNB = GaussianNB()\nNB.fit(X_train, Y_train)\nlr_probs = NB.predict_proba(X_test)\nlr_probs = lr_probs[:, 1]\n\nlr_auc = roc_auc_score(Y_test, lr_probs)\nprint('ROC AUC =', lr_auc)\nlr_fpr, lr_tpr, _ = roc_curve(Y_test, lr_probs)\n\nplt.plot(lr_fpr, lr_tpr, marker='.')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","9cdf3e6e":"lr_precision, lr_recall, _ = precision_recall_curve(Y_test, lr_probs)\nlr_auc = auc(lr_recall, lr_precision)\n\nprint('AUC =', lr_auc)\nplt.plot(lr_recall, lr_precision, marker='.')\n# axis labels\nplt.xlabel('Recall')\nplt.ylabel('Precision')\n# show the plot\nplt.ylim((0,1.1))\nplt.show()","60143bec":"from sklearn.metrics import classification_report\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33)\n\nNB = GaussianNB()\nNB.fit(X_train, Y_train)\nY_NB = NB.predict(X_test)\nprint(classification_report(Y_test, Y_NB))","cf7fdd6c":"scomean=[]\nRandomForest = RandomForestClassifier()\nfor k in range(2, 15):\n    scomean.append(cross_val_score(NB, X, Y, cv = k, scoring='f1').mean())\n\nplt.plot(range(2,15), scomean, label='CrosVal')\nplt.legend()\nplt.xlabel('K-fold')\nplt.ylabel('Score')","d911f3cc":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33)\n\nRandomForest = RandomForestClassifier()\nRandomForest.fit(X_train, Y_train)\nlr_probs = RandomForest.predict_proba(X_test)\nlr_probs = lr_probs[:, 1]\n\nlr_auc = roc_auc_score(Y_test, lr_probs)\nprint('ROC AUC =', lr_auc)\nlr_fpr, lr_tpr, _ = roc_curve(Y_test, lr_probs)\nplt.plot(lr_fpr, lr_tpr, marker='.')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","e42ca921":"lr_precision, lr_recall, _ = precision_recall_curve(Y_test, lr_probs)\nlr_auc = auc(lr_recall, lr_precision)\n\nprint('AUC =', lr_auc)\nplt.plot(lr_recall, lr_precision, marker='.')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim((0,1.1))\nplt.show()","2f836870":"from sklearn.metrics import classification_report\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33)\n\nRandomForest = RandomForestClassifier()\nRandomForest.fit(X_train, Y_train)\nY_RandomForest = RandomForest.predict(X_test)\nprint(classification_report(Y_test, Y_RandomForest))","3e50f631":"tuned_parameters = {'min_samples_leaf': range(10,200,10), 'n_estimators' : range(10,200,10),'max_features':['auto','sqrt','log2']}\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33)\n\nRandomForest = RandomForestClassifier()\nRF = RandomizedSearchCV(RandomForest, tuned_parameters, random_state=0, cv=8, scoring='accuracy', n_iter = 30, n_jobs=-1)\n","0447b8a0":"RF.fit(X_train,Y_train)","5c5ae1fb":"y_RF = RF.predict(X_test)\nprint(RF.best_score_)\nprint(RF.best_params_)\nprint('Accuracy =', metrics.accuracy_score(Y_test, y_RF))\nprint('F-score =', metrics.f1_score(Y_test, y_RF))","37cb308c":"### Results\nAfter trying all the diferent models the one that works better is the decisionTree or the ensemble RandomForest. This models are fast and work perfectly in these dataset. Some attributes like population or ring-type are very helpful to classify the mushrooms for its class.","704c1380":"### Model Selection\n\nWe are going to try diferent models to find the one that has the best results in our datset, the models that we will be testing are the following ones:\n\n- Decision tree\n- Logistic regression\n- Naive bByes\n- Random Forest\n- KNN\n- SVM \n- MLP","c6a34233":"### Conclusions\nAs we have seen, all the models are almost perfect in these dataset, the reason of this might be because as we saw in the data analysis, some of the attributes are specific for poisonous mushrooms and some of them for edible mushrooms, with these features, models like decisionTrees can be very powerful.\nThis dataset is very helpful to learn how to apply machine learning models as well as to analyze it. This data is clear and there are a lot of thing that can be done.\n","7cd4706b":"The first step of is reading our data and understand it.","8f7fdc07":"Our dataset only contains one attribute with NaN values. We are going to fill the NaN values with the most freq\u00fcent one, i decided this technique because the attribute is categorical and the most frequent attribute has about the 50% of the data.","1603dc58":"The atribute veil_type can be deleted because it only takes one value, the rest of attributes don't show any problem.","8a5c37a6":"We are going to use the same train and test data to try our different models, we are going to compare its accuracy and f1-score","7e9398dc":"Lets use the function nunique() to visualize the amount of values that the features can take.","4b447cf5":"The target attribute is going to be the mushroom class. This attribute is binary: edible or poisonous.\nLets see the distribution of the target variable.","89bc14b2":"We are going to plot the distribution of some attributes to explore and understand better our data.\nLets start with the diferent shapes that the cap can have.\n","107e2ca9":"### Gaussian Naive Bayes\n#### Crossvalidation","d7a11c0e":"In these nootebook we will be testing diferent machine learning models on the following dataset: https:\/\/www.kaggle.com\/uciml\/mushroom-classification \nWe are going to choose some models and tune its parameters so we can get the best score possible, we are also going to plot the ROC curve and PR curve to compare the models.","47284304":"### Preprocessing","ccd27a63":"#### Hyperparameter Search\nLets get the best parameters for Random Forest.\nThere are thre features that can be tuned to improve the performance of the model:\n- max_features = {auto, sqrt,log2}\n- n_estimators = range(10,200,10)\n- min_sample_leaf = range(10,200,10)","d16375ac":"Lets see if there are any outliers that can afect the models.","8a2cedb9":"### Logistic Regression\n#### Crossvalidation\nTo assure that our model isn't overfitted we are going to validate our results using crossvalidation.","36f1ad3a":"We are also going to observe the attribute ring-type","ff4cbad9":"#### Hyperparameter Search\nLets get the best parameters for Logistic Regression.\nThere are two features that can be tuned to improve the performance of the model:\n- C = { 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000 ] -> Inverse of regularization strength\n- penalty = { l1, l2 } -> regularitzation parameters\n","d3b8cbc5":"Lets look if there are any NaN values in our dataset.","05bf20b4":"As we can see in the plot all the models get a pretty high accuracy. The ones that doesn't work perfectly are the Logistic Regression and Gaussian Naive Bayes","f23735a3":"#### ROC Curve and PR Curve","c953be0f":"In the dataset there are 22 features and 8124 different samples (rows).\nThe info() function let us see all the attributtes as well as how many non-null values have and their type.","96dcded3":"As expected, the values of f1-score and accuracy are similar.\n","229f8496":"![image.png](attachment:image.png)\nAs well as the previous plot, here we can see that there are some type of rings that only poisonous mushrooms has. there are more types of rings but there aren't any samples with those values so we do not include them in the plot.","e4bd7ce0":"Looking at the table above, there aren't any variables that contain outliers.\n\nThe data is going to be standarized to get more accurated results","a20e7ccd":"This dataset gives descriptions of mushrooms samples corresponding to 23 species of grilled mushrooms in the Agaricus and Lepiota family mushrooms.\nThis descriptions can be divided in the diferent parts of the mushroom (cap, gill, stalk, veil(annulus)), for each part it gives information like its shape, size, color,... In the dataset there is also some attributes that gives details from their habitat or their odor.\n\n\n![image.png](attachment:image.png)","4528488a":"Lets see the distribution of the population attribute.","305e91e8":"### Exploratory Data Analysis (EDA)","d8364912":"### RandomForest\n#### Crossvalidation","ebba8f80":"#### ROC Curve and PR Curve","2d6d1128":"The data is balanced so both the accuracy and f1-score can be used to evaluate the models.","f3f1c83a":"Our values have to be converted to integers to be able to work with them. To do so we'll be using Ordinal encoding.","cea8f78c":"## Mushroom Classification","0e1f3a7b":"In hte heat map we can se that there are some attributes very similar, for example, stalk-color-below-ring and stalk-color-above-ring or stalk-surface-above-ring and stalk-surface-below-ring we could group them.","67db4c90":"#### ROC Curve and PR Curve\nLets plot the two different curves to show our results from another point of view","fafe9513":"ROC and PR curve gives us similar results and we can rely on both of them because our dataset is balanced.","4193c9ed":"### Introduction","6002cec0":"![image.png](attachment:image.png)\n\nAs we can see in the plot almost all mushrooms cap are convex-shaped or flat-shaped and there aren't almost any samples of mushrooms with sunken-shaped or conical-shaped cap.","fb45da46":"As we can see in the plot there are some types of populations that only edible mushrooms have (numerous and abundant). These attributes can be very helpful when implenting the decision tree classifier."}}