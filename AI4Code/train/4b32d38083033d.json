{"cell_type":{"dd3ac310":"code","69a10eec":"code","d1a07586":"code","58d226b6":"code","d596508f":"code","dd953029":"code","5ac25314":"code","8bc90025":"code","eb4563b5":"code","830f6e76":"code","96ba6b1c":"code","f2453edc":"code","4ec81408":"code","1ff91261":"code","126ac7e0":"code","5ef91e35":"code","0ed1a329":"code","76a91237":"code","36fc7bab":"code","bacc107d":"code","d52a061d":"code","a835bccc":"code","2853e547":"code","e0914010":"code","5126e377":"code","9078ea0a":"code","45dc3158":"code","1528500c":"code","09d87575":"markdown","45df1e0d":"markdown","773b223e":"markdown","06ea056d":"markdown","b190dedc":"markdown","2d5d24e4":"markdown"},"source":{"dd3ac310":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport dask.dataframe as dd\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nimport cuml\nimport cudf  # looks and feels like Pandas, but runs on the GPU\nfrom cuml.preprocessing.TargetEncoder import TargetEncoder\nfrom cuml.preprocessing.model_selection import train_test_split\nfrom cuml.metrics import accuracy_score,roc_auc_score\n\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nimport matplotlib.pyplot as plt","69a10eec":"train_dir = '\/kaggle\/input\/tabular-playground-series-mar-2021\/train.csv'\ntest_dir = '\/kaggle\/input\/tabular-playground-series-mar-2021\/test.csv'","d1a07586":"%%time\ndf_dd = dd.read_csv(train_dir).compute()","58d226b6":"%%time\ndf_cudf = cudf.read_csv(train_dir)","d596508f":"train_df = pd.read_csv(train_dir)\ntrain_df.drop(columns = 'id', axis = 1, inplace = True)\ntest_df = pd.read_csv(test_dir)\nids = test_df['id']\ntest_df.drop(columns = 'id', axis = 1, inplace = True)","dd953029":"train_df.head()","5ac25314":"train_df.describe()","8bc90025":"dct = {}\nfor trgt in train_df['target']:\n    if(trgt not in dct):\n        dct[trgt] = 1\n    else:\n        dct[trgt] += 1","eb4563b5":"plt.figure(figsize = (7,7))\nplt.pie(dct.values(), labels = [f'0 : {dct[0]\/3000} %', f'1 : {dct[1]\/3000} %'])\n\nmy_circle = plt.Circle( (0,0), 0.7, color='white')\nfig = plt.gcf()\nfig.gca().add_artist(my_circle)\nplt.show()","830f6e76":"categorical_columns = []\nfor i in range(19):\n    categorical_columns.append('cat'+str(i))\nnumerical_columns = []\nfor i in range(11):\n    numerical_columns.append('cont'+str(i))\n","96ba6b1c":"num_rows, num_cols = 6,2\nf, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(16, 24))\nf.suptitle('Distribution of Features', fontsize=26)\n\nfor index, column in enumerate(train_df[numerical_columns].columns):\n    i,j = (index \/\/ num_cols, index % num_cols)\n    sns.kdeplot(train_df.loc[train_df['target'] == 0, column], color = \"m\", shade = True, ax = axes[i,j])\n    sns.kdeplot(train_df.loc[train_df['target'] == 1, column], color = \"b\", shade = True, ax = axes[i,j])\n\nf.delaxes(axes[5, 1])\nplt.tight_layout()\nplt.show()","f2453edc":"plt.figure(figsize = (30,35))\ncorr = train_df.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask = mask, cmap = 'spring', vmax = .3, center = 0,\n            square = True, linewidths = .5, annot=True, annot_kws={\"fontsize\":18})\nplt.show()","4ec81408":" for index, column in enumerate(train_df[categorical_columns].columns):\n    index_0 = train_df.loc[train_df['target'] == 0, column].value_counts().reset_index()[\"index\"].values\n    values_0 = train_df.loc[train_df['target'] == 0, column].value_counts().reset_index()['cat'+str(index)].values\n    index_1 = train_df.loc[train_df['target'] == 1, column].value_counts().reset_index()[\"index\"].values\n    values_1 = train_df.loc[train_df['target'] == 1, column].value_counts().reset_index()['cat'+str(index)].values\n    if len(values_0)>10:\n        index_0 = index_0[:50]\n        values_0 = values_0[:50]\n    if len(values_1)>10:\n        index_1 = index_1[:50]\n        values_1 = values_1[:50]\n        \n    plt.figure(figsize = (18, 8))\n    \n    sns.barplot(x = index_0, y = values_0, palette = 'spring')\n    sns.barplot(x = index_1, y = values_1, palette = 'spring')\n    \n    plt.title(categorical_columns[index], fontsize=15)\nplt.show()    ","1ff91261":"from copy import deepcopy\n\ndf_0 = train_df.loc[train_df['target'] == 0]\ndf_1 = train_df.loc[train_df['target'] == 1]\n\nfor index, column in enumerate(train_df[categorical_columns].columns):\n    data = df_0.groupby(column)[column].count().sort_values(ascending=False)\n    if len(data) < 10:\n        continue\n    # data = data if len(data) < 25 else data[:25]\n    \n    target_0_values = set(deepcopy(data.index))\n    \n    data = df_1.groupby(column)[column].count().sort_values(ascending=False)\n    # data = data if len(data) < 25 else data[:25]\n    \n    target_1_values = set(deepcopy(data.index))\n    \n    print('-------------------   {}   ---------------------'.format(column))\n    print('Unique values for class 0: {}'.format(target_0_values - target_1_values))\n    print('Unique values for class 1: {}'.format(target_1_values - target_0_values))\n","126ac7e0":"# Fix cat5 variable\ntrain_df['cat5'] = train_df['cat5'].apply(lambda x: x if x not in \n                                          ['AG', 'CB', 'BP', 'ZZ', 'BM', 'BX', 'AK', 'B'] \n                                          else 'B')\ntest_df['cat5'] = test_df['cat5'].apply(lambda x: x if x not in \n                                        ['AG', 'CB', 'BP', 'ZZ', 'BM', 'BX', 'AK', 'B'] \n                                        else 'B')\n\n# Fix cat8 variable\ntrain_df['cat8'] = train_df['cat8'].apply(lambda x: x if x not in ['AC', 'P'] else 'P')\ntest_df['cat8'] = test_df['cat8'].apply(lambda x: x if x not in ['AC', 'P'] else 'P')\n\n# Fix cat10 variable\ntrain_df['cat10'] = train_df['cat10'].apply(lambda x: x if x not in \n                                            ['HF', 'KK', 'GD', 'JE', 'KD', 'MA', 'BA', 'DT', \n                                             'LK', 'GR', 'KU', 'MW', 'LR', 'ME', 'CN', 'JF', \n                                             'DA', 'JC', 'IU', 'GV', 'ED', 'EB', 'IL', 'EF', \n                                             'BD', 'GG', 'CM', 'CH', 'EG', 'FA', 'KN', 'IM', \n                                             'DU', 'IN', 'HI', 'DX', 'IP', 'DM', 'CF', 'MO', \n                                             'DL', 'KI', 'FW', 'GH', 'MP', 'MR', 'BO', 'IY', \n                                             'CQ', 'GF', 'AF', 'CX', 'MQ', 'GJ', 'FF', 'LT', \n                                             'AJ', 'IQ', 'HY', 'LH', 'DN', 'MK', 'GY', 'BS', \n                                             'DK', 'AW', 'JU', 'BX', 'CT', 'EH', 'ML', 'EN', \n                                             'MU', 'MI'] else 'MI')\ntest_df['cat10'] = test_df['cat10'].apply(lambda x: x if x not in \n                                        ['HF', 'KK', 'GD', 'JE', 'KD', 'MA', 'BA', 'DT', \n                                         'LK', 'GR', 'KU', 'MW', 'LR', 'ME', 'CN', 'JF', \n                                         'DA', 'JC', 'IU', 'GV', 'ED', 'EB', 'IL', 'EF', \n                                         'BD', 'GG', 'CM', 'CH', 'EG', 'FA', 'KN', 'IM', \n                                         'DU', 'IN', 'HI', 'DX', 'IP', 'DM', 'CF', 'MO', \n                                         'DL', 'KI', 'FW', 'GH', 'MP', 'MR', 'BO', 'IY', \n                                         'CQ', 'GF', 'AF', 'CX', 'MQ', 'GJ', 'FF', 'LT', \n                                         'AJ', 'IQ', 'HY', 'LH', 'DN', 'MK', 'GY', 'BS', \n                                         'DK', 'AW', 'JU', 'BX', 'CT', 'EH', 'ML', 'EN', \n                                         'MU', 'MI'] else 'MI')","5ef91e35":"encoder = TargetEncoder()\n\nX = cudf.DataFrame(train_df.drop([\"target\"],axis=1))\ny = cudf.Series(train_df[\"target\"])\ntest_df = cudf.DataFrame(test_df)\n\nfor col in train_df.columns:\n    if train_df[col].dtype=='object':\n        print(col)\n        X[col] = encoder.fit_transform(X[col],y)\n        test_df[col] = encoder.transform(test_df[col])","0ed1a329":"from lightgbm import LGBMClassifier\nimport xgboost\nimport lightgbm\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier","76a91237":"k_fold = 8\nskf = StratifiedKFold(n_splits=k_fold)","36fc7bab":"params = {'n_estimators': 10000,\n 'learning_rate': 0.05,\n 'metric': 'auc',\n 'num_leaves': 708,\n 'max_depth': 31,\n 'reg_alpha': 11.308,\n 'reg_lambda': 15.091,\n 'colsample_bytree': 0.233,\n 'force_col_wise': True,\n 'cat_smooth': 39.657}\n\nfor i, (train_index, test_index) in enumerate(skf.split(X, y.to_array())):\n    print('[Fold %d\/%d]' % (i + 1, k_fold))\n    X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_valid = y[train_index], y[test_index]\n    # Convert our data into LGBM format\n    d_train = lightgbm.Dataset(X_train.to_pandas(), label = y_train.to_pandas())\n    d_valid = lightgbm.Dataset(X_valid.to_pandas(), label = y_valid.to_pandas())\n    d_test = lightgbm.Dataset(test_df)\n\n    mdl = lightgbm.train(params, d_train, 1000, valid_sets = [d_train, d_valid], \n                         early_stopping_rounds=300, verbose_eval=500)\n\n    # Predicting...\n    p_test = mdl.predict(d_test)\n    sub['target_'+str(k)] += p_test\/k_fold","bacc107d":"lgbm = LGBMClassifier()\n\nlgbm.fit(X_train.as_matrix(), \n         y_train.to_array(), \n         eval_set = (X_valid.as_matrix(), y_valid.to_array()), \n         verbose = True)\npredictions = lgbm.predict_proba(X_valid.as_matrix())[:,1]\n\nauc = roc_auc_score(y_valid, predictions)\n\nprint(f'LGBM Score: {auc}')","d52a061d":"preds_lgbm = lgbm.predict_proba(test_df.as_matrix())[:,1]","a835bccc":"preds_lgbm","2853e547":"df_sub = {'id': ids, 'target': preds_lgbm}\ndf_predictions = cudf.DataFrame(df_sub).set_index(['id'])\n\ndf_predictions.to_csv('\/kaggle\/working\/predictions_lgbm.csv')","e0914010":"def gini(actual, pred, cmpcol = 0, sortcol = 1):\n    assert( len(actual) == len(pred) )\n    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n    totalLosses = all[:,0].sum()\n    giniSum = all[:,0].cumsum().sum() \/ totalLosses\n    \n    giniSum -= (len(actual) + 1) \/ 2.\n    return giniSum \/ len(actual)\n \ndef gini_normalized(actual, pred):\n    return gini(actual, pred) \/ gini(actual, actual)\n\ndef gini_xgb(preds, dtrain):\n    labels = dtrain.get_label()\n    gini_score = gini_normalized(labels, preds)\n    return 'gini', gini_score","5126e377":"k_fold = 8\nskf = StratifiedKFold(n_splits=k_fold)","9078ea0a":"sub = pd.DataFrame()\nsub['id'] = ids\nsub['target'] = np.zeros_like(ids)","45dc3158":"params =   {'objective': 'binary:logistic',\n        'booster': 'gbtree',\n        'tree_method': 'gpu_hist',\n        'eval_metric': 'auc',\n        'random_state': 1,\n        'max_depth': 12,\n        'learning_rate': 0.03,\n        'min_child_weight': 20,\n        'gamma': 0.1,\n        'alpha': 0.2,\n        'lambda': 9,\n        'colsample_bytree': 0.2,\n        'subsample': 0.8}\nfor i, (train_index, test_index) in enumerate(skf.split(X, y.to_array())):\n    print('[Fold %d\/%d]' % (i + 1, k_fold))\n    X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_valid = y[train_index], y[test_index]\n    # Convert our data into XGBoost format\n    d_train = xgboost.DMatrix(X_train, y_train)\n    d_valid = xgboost.DMatrix(X_valid, y_valid)\n    d_test = xgboost.DMatrix(test_df)\n    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n\n    # Train the model! We pass in a max of 1,600 rounds (with early stopping after 70)\n    # and the custom metric (maximize=True tells xgb that higher metric is better)\n    mdl = xgboost.train(params, d_train, 1600, watchlist, early_stopping_rounds=70, feval=gini_xgb, maximize=True, verbose_eval=500)\n\n    # Predicting...\n    p_test = mdl.predict(d_test, ntree_limit=mdl.best_ntree_limit)\n    sub['target'] += p_test\/k_fold","1528500c":"target_ = sub['target']\ndf_sub = {'id': ids, 'target': target_}\ndf_predictions = cudf.DataFrame(df_sub).set_index(['id'])\n\ndf_predictions.to_csv('\/kaggle\/working\/predictions_xgb_.csv')","09d87575":"# LGBM","45df1e0d":"I used **Target encoding** which works by averaging the target value by category.\n\n* Target encoding is a fast way to get the most out of your categorical variables with little effort. The idea is quite simple. Say you have a categorical variable X and a target y \u2013 y can be binary or continuous, it doesn\u2019t matter. For each distinct element in X you\u2019re going to compute the average of the corresponding values in y. Then you\u2019re going to replace each Xi with the according mean.\n\nHere **stratify** parameter makes a split so that the proportion of values in the sample produced will be the same as the proportion of values provided to parameter **stratify**.\n\nFor example, if variable y is a binary categorical variable with values 0 and 1 and there are 25% of zeros and 75% of ones, stratify=y will make sure that your random split has 25% of 0's and 75% of 1's","773b223e":"# XGBoost","06ea056d":"# Feature Engineering","b190dedc":"# EDA","2d5d24e4":"# Modelling"}}