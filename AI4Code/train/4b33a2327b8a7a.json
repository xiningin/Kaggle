{"cell_type":{"020ae5c0":"code","0a345917":"code","929b8ac7":"code","18119d06":"code","63b31566":"code","d6a13521":"code","d7f4dfce":"code","275a3a25":"code","cd799166":"code","7d628374":"code","3facbf99":"code","541bcd55":"code","b5b0a01f":"code","5fd291f3":"code","cb9e0ae0":"code","4286ace0":"code","e669a0ce":"code","d4b33f11":"code","8e1147dd":"code","357a7703":"code","33913b45":"code","3bbad25e":"code","af5b5224":"code","91b3575b":"code","a49deb6e":"code","536f4d08":"code","0bf65144":"markdown","e6d91b00":"markdown","f837992b":"markdown","58d265cf":"markdown","9527165d":"markdown","f1241058":"markdown","f906ae20":"markdown","979ce5f2":"markdown","f654d174":"markdown","42d5e7c6":"markdown","fe4f5a5b":"markdown"},"source":{"020ae5c0":"!apt-get install -y graphviz libgraphviz-dev libcgraph6","0a345917":"!pip install -qq tensorflow-gpu==1.15.0\n!pip install -qq git+https:\/\/github.com\/danielegrattarola\/spektral","929b8ac7":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom skimage.util import montage\nfrom IPython.display import Image, display, SVG, clear_output, HTML\nplt.rcParams[\"figure.figsize\"] = (6, 6)\nplt.rcParams[\"figure.dpi\"] = 125\nplt.rcParams[\"font.size\"] = 14\nplt.rcParams['font.family'] = ['sans-serif']\nplt.rcParams['font.sans-serif'] = ['DejaVu Sans']\nplt.style.use('ggplot')\nsns.set_style(\"whitegrid\", {'axes.grid': False})\nplt.rcParams['image.cmap'] = 'gray' # grayscale looks better","18119d06":"from keras import Input, Model\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import Dense, Flatten, Dropout\nfrom keras.optimizers import Adam\nfrom keras.regularizers import l2\n\nfrom spektral.datasets import mnist\nfrom spektral.layers import GraphConv, GraphConvSkip, GraphAttention, GlobalAttentionPool, TopKPool, MinCutPool, DiffPool\nfrom spektral.layers.ops import sp_matrix_to_sp_tensor\nfrom spektral.utils import normalized_laplacian, normalized_adjacency","63b31566":"import networkx as nx\ndef draw_graph_mpl(g, pos=None, ax=None, layout_func=nx.drawing.layout.kamada_kawai_layout):\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=(25, 25))\n    else:\n        fig = None\n    if pos is None:\n        pos = layout_func(g)\n    node_color = []\n    node_labels = {}\n    shift_pos = {}\n    for k in g:\n        node_color.append(g.nodes[k].get('color', 'green'))\n        node_labels[k] = g.nodes[k].get('label', k)\n        shift_pos[k] = [pos[k][0], pos[k][1]]\n    \n    nx.draw_networkx_edges(g, pos, font_weight='bold', ax=ax)\n    nx.draw_networkx_nodes(g, pos, node_color=node_color, node_shape='p', node_size=300, alpha=0.75)\n    nx.draw_networkx_labels(g, shift_pos, labels=node_labels, ax=ax, arrows=True)\n    ax.autoscale()\n    return fig, ax, pos","d6a13521":"# Parameters\nl2_reg = 5e-4         # Regularization rate for l2\nlearning_rate = 1e-3  # Learning rate for SGD\nbatch_size = 32       # Batch size\nepochs = 10000        # Number of training epochs\nes_patience = 15     # Patience fot early stopping","d7f4dfce":"# Load data\nX_train, y_train, X_val, y_val, X_test, y_test, adj = mnist.load_data()\nX_train, X_val, X_test = X_train[..., None], X_val[..., None], X_test[..., None]\nN = X_train.shape[-2]      # Number of nodes in the graphs\nF = X_train.shape[-1]      # Node features dimensionality\nn_out = 10  # Dimension of the target\nfltr = normalized_laplacian(adj)\nnorm_adj = normalized_adjacency(adj)\nprint(X_train.shape, 'model input')","275a3a25":"print(adj.shape, 'adjacency matrix')\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\nplot_adj_args = dict(cmap='RdBu', vmin=-0.5, vmax=0.5)\nax1.matshow(adj.todense(), **plot_adj_args)\nax1.set_title('Adjacency')\nax2.matshow(norm_adj.todense(), **plot_adj_args)\nax2.set_title('Normalized')\nax3.matshow(fltr.todense(),**plot_adj_args)\nax3.set_title('Laplacian Adjacency')","cd799166":"xx, yy = np.meshgrid(np.arange(28), np.arange(28))\nnode_id = ['X:{:02d}_Y:{:02d}'.format(x, y) for x, y in zip(xx.ravel(), yy.ravel())]","7d628374":"print(node_id[300], 'is connected to')\nfor row, col in zip(*adj[300].nonzero()):\n    print(col, '->', node_id[col])","3facbf99":"G = nx.from_scipy_sparse_matrix(adj[:10, :10])\nfor k in G.nodes:\n    G.nodes[k]['label'] = node_id[k]\ndraw_graph_mpl(G);","541bcd55":"MAX_NODE = 28*5\nG = nx.from_scipy_sparse_matrix(adj[:MAX_NODE, :MAX_NODE])\nfor k in G.nodes:\n    G.nodes[k]['label'] = node_id[k]\ndraw_graph_mpl(G);","b5b0a01f":"G = nx.from_scipy_sparse_matrix(adj)\ndraw_graph_mpl(G, pos=np.stack([xx.ravel(), yy.ravel()], -1));","5fd291f3":"from keras import backend as K\nK.clear_session()","cb9e0ae0":"# Model definition\n\nX_in = Input(shape=(N, F))\n# Pass A as a fixed tensor, otherwise Keras will complain about inputs of\n# different rank.\nA_in = Input(tensor=sp_matrix_to_sp_tensor(fltr), name='LaplacianAdjacencyMatrix')\n\ngraph_conv_1 = GraphConv(32,\n                       activation='elu',\n                       kernel_regularizer=l2(l2_reg),\n                       use_bias=True)([X_in, A_in])\n\ngraph_conv_2 = GraphConv(64,\n                       activation='elu',\n                       kernel_regularizer=l2(l2_reg),\n                       use_bias=True)([graph_conv_1, A_in])\n\nmc_1, A_mincut = graph_conv_2, A_in\n# doesn't work yet\n#diffpool_1, A_mincut, _ = DiffPool(k=64)([graph_conv_22, A_mincut])\n\ngraph_conv_21 = GraphConv(64,\n                       activation='elu',\n                       kernel_regularizer=l2(l2_reg),\n                       use_bias=True)([mc_1, A_mincut])\n\ngraph_conv_22 = GraphConv(128,\n                       activation='elu',\n                       kernel_regularizer=l2(l2_reg),\n                       use_bias=True)([graph_conv_21, A_mincut])\n\n\ngap_1 = GlobalAttentionPool(32)(graph_conv_22)\ngap_dr = Dropout(0.5)(gap_1)\nfc_1 = Dense(16, activation='elu')(gap_dr)\noutput = Dense(n_out, activation='softmax')(fc_1)\n\n# Build model\nmodel = Model(inputs=[X_in, A_in], outputs=output)\noptimizer = Adam(lr=learning_rate)\nmodel.compile(optimizer=optimizer,\n              loss='sparse_categorical_crossentropy',\n              metrics=['acc'])\nmodel.summary()","4286ace0":"from keras.utils.vis_utils import model_to_dot\nImage(model_to_dot(model, show_shapes=True).create_png())","e669a0ce":"# Train model\nvalidation_data = (X_val, y_val)\nmodel.fit(X_train,\n          y_train,\n          batch_size=batch_size,\n          validation_data=validation_data,\n          epochs=epochs,\n          callbacks=[\n              EarlyStopping(patience=es_patience, restore_best_weights=True)\n          ])","d4b33f11":"# Evaluate model\nprint('Evaluating model.')\neval_results = model.evaluate(X_test,\n                              y_test,\n                              batch_size=batch_size)\nprint('Test loss: {}\\n'\n      'Test acc: {}'.format(*eval_results))","8e1147dd":"W, b = model.layers[2].get_weights()\nprint(W.shape, b.shape)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\nax1.bar(np.arange(W.shape[1]), W[0])\nax2.bar(np.arange(W.shape[1]), b)","357a7703":"W, b = model.layers[3].get_weights()\nprint(W.shape, b.shape)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\nax1.imshow(W, vmin=-1, vmax=1, cmap='RdBu')\nax2.bar(np.arange(W.shape[1]), b)","33913b45":"W, b = model.layers[4].get_weights()\nprint(W.shape, b.shape)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\nax1.imshow(W, vmin=-1, vmax=1, cmap='RdBu')\nax2.bar(np.arange(W.shape[1]), b)","3bbad25e":"W, b = model.layers[5].get_weights()\nprint(W.shape, b.shape)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\nax1.imshow(W, vmin=-1, vmax=1, cmap='RdBu')\nax2.bar(np.arange(W.shape[1]), b)","af5b5224":"Ws = model.layers[6].get_weights()\nprint([W.shape for W in Ws])\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(16, 4))\nax1.imshow(Ws[0], vmin=-1, vmax=1, cmap='RdBu')\nax2.bar(np.arange(Ws[1].shape[0]), Ws[1])\nax3.imshow(Ws[2], vmin=-1, vmax=1, cmap='RdBu')\nax4.bar(np.arange(Ws[3].shape[0]), Ws[3])","91b3575b":"i_model = Model(inputs=[X_in, A_in], outputs=[graph_conv_1, graph_conv_2, graph_conv_21, graph_conv_22])","a49deb6e":"output_list = i_model.predict(X_test[:32])","536f4d08":"fig, m_axs = plt.subplots(4, 5, figsize=(20, 15))\nstack_it = lambda x: x.reshape((28, 28, -1)).swapaxes(0, 2).swapaxes(1, 2)\nfor n_axs, c_data in zip(m_axs.T, [X_test]+output_list):\n    for i, c_ax in enumerate(n_axs):\n        c_img = stack_it(c_data[i]).squeeze()\n        if c_img.ndim==2:\n            c_ax.imshow(c_img)\n        else:\n            c_ax.imshow(montage(c_img), vmin=-0.5, vmax=0.5, cmap='RdBu')\n        c_ax.axis('off')","0bf65144":"# Weights\nNot sure exactly how to interpret these but we can show them easily enough","e6d91b00":"## Goal\nThe goal of the problem is to correctly classify the digits using the intensity values as the nodes and the neighborhood relationships as the edges. When we visualize the adjacency matrix we can see the effect of a simply unraveled 2D array","f837992b":"### Label Nodes and Show Connections\nHere we can visualize the topology a bit better and see what the graph actually looks like.","58d265cf":"- Show 5 rows of the network","9527165d":"- Show network (using the X, Y coordinates)","f1241058":"# Overview\nThe notebook is mainly done for my own benefit to better understand what graph convolutional networks do on a very basic and visual task (MNIST). \n\nThe notebook is just a slightly more visual version of the MNIST example provided at https:\/\/github.com\/danielegrattarola\/spektral\/blob\/master\/examples\/graph_signal_classification_mnist.py as part of the [Spektral](https:\/\/github.com\/danielegrattarola\/spektral) package. ","f906ae20":"## Libraries\nHere are the libraries and imports to make the model","979ce5f2":"## Install Dependencies","f654d174":"# Model Building\nNow we can build the model which uses the graph topology shown above as the basis. We feed the topology in as a constant tensor ($A_{in}$) and the convolutions occur across this topology. ","42d5e7c6":"## Show intermediate output values\nHere we can rearrange the output of the graph convolutions to see if the model is learning similar sorts of features to the standard convolutional neural networks","fe4f5a5b":"# What did the model actually learn?\nWe can now try and reassemble what the model actually learnt by exporting the intermediate layers"}}