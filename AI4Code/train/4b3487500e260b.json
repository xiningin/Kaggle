{"cell_type":{"78b03f56":"code","0664ac0d":"code","8b4f3670":"code","a36da827":"code","6c5182b3":"code","fbdf7821":"code","da0de6ab":"code","4e4e4fbf":"code","c9627af6":"code","a33d23ce":"markdown","58cbbfab":"markdown","bda424d9":"markdown","6c7066f8":"markdown","39441e03":"markdown","456919ff":"markdown"},"source":{"78b03f56":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nfrom sklearn.model_selection import train_test_split","0664ac0d":"data = pd.read_csv(\"..\/input\/credit-card-customers\/BankChurners.csv\")","8b4f3670":"data = data.drop('Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1', axis=1)\ndata = data.drop('Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2', axis=1)\ndata = data.drop('CLIENTNUM', axis = 1)\ndata.head()","a36da827":"# Attrition_Flag is going to be our target\ndata['Attrition_Flag'].unique()\n\n\n# Remove rows with missing target, separate target from predictors\ndata.dropna(axis=0, subset=['Attrition_Flag'], inplace=True)\ny = data['Attrition_Flag']\ndata.drop(['Attrition_Flag'], axis=1, inplace=True)\n\ny.head()","6c5182b3":"# select low cardinality categorical columns with low cardinality\ncategorical_cols = [cname for cname in data.columns if \n                   data[cname].nunique() < 10 and\n                   data[cname].dtype == 'object']\n# probably redundant with this dataset since its pretty clean\n  # but double checking is good practice","fbdf7821":"# select numerical columns\nnum_cols = [cname for cname in data.columns if \n           data[cname].dtype in ['int64', 'float64']]","da0de6ab":"# Only keep these specific colums\nmy_cols = categorical_cols + num_cols\nX = data[my_cols].copy()\n\n# One hot encode via pandas for speed\nX= pd.get_dummies(X)\nX.head()","4e4e4fbf":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.svm import NuSVC\nfrom sklearn.svm import LinearSVC\nfrom xgboost import XGBClassifier\n\n\nfrom sklearn.model_selection import cross_val_score\n\n\ndef evaluate_model(model):\n    my_model = model()\n    scores = cross_val_score(my_model, X, y,\n                              cv=5,\n                              scoring='accuracy')\n    return scores.mean()\n\nalgoritms = [GaussianNB,\n            DecisionTreeClassifier,\n            RandomForestClassifier,\n            KNeighborsClassifier,\n            SVC,\n            LinearSVC,\n            XGBClassifier]\n\nstr_algoritms = ['GaussianNB',\n                 'DecisionTreeClassifier',\n                 'RandomForestClassifier',\n                 'KNeighborsClassifier',\n                 'SVC',\n                 'LinearSVC',\n                 'XGBClassifier']\n\nresults = {}\nfor i in range(0, len(algoritms)):\n    results[str_algoritms[i]] = evaluate_model(algoritms[i])\n\nresults","c9627af6":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(20, 10))\n\n\nsns.barplot(x=list(results.keys()), y=list(results.values()))\n\nplt.ylabel('Accuracy')\nplt.title('Algorithm Performance')\n","a33d23ce":"The data is largely clean and complete and as such not much preprocessing occurs. There are some categorical columns which have \"Unknown\" values. These values could possibly be filled\/dropped however since they don't significantly raise the cardinality of the data as well as the primary goal of this investagation is to try several models, they will be treated as their own outright feature. The data will not be split into a test and training set since we our evaluating our model using cross validation. The only significant prepocessing is to one hot encode all the categorical columns.","58cbbfab":"# Predict Churing Customers\n\nA bank is having a problem of having a high number of customers leaving their credit card services. The bank would like to identify these customers early in order to maliciously trap them in a usurous cycle. The goal of this brief investagation will be to try to develope a model which identifes customers who are about to leave their credit card services. While this data can be used by banks in order to retain customers. This data can be utilized by a regulating body such as the SEC in order to integrate into a larger model in order to identify preditory lending techniques and identify at risk consumers. \n\nThis investagation will explore serveral different models and compare the resultant accuracy of these model via a cross validation metric along with a brief discussion. ","bda424d9":"The goal of this script is to spray the problem with a bunch of different algoritms to evaluate each algorithm's performance. This is going to take a while to run and is probably not the most efficent method of modelling, however this is primarially an educational experience for myself. Additionally, some of the algorithms such as the k-NN and SVMs are supposedly sensitive to feature transformations. Features should be scaled when using these algoritms and thus I hypothesize before running them that they will perform poorly. In fact, I would honestly be surprised if they converge to a global minimum. I am going to be lazy and use the vanilla arguements, however, as a learning exercise I think it would be interesting to see if this is actually reflected in the results.\n\nSince only 16% of customers have churned, I am implementing cross validation in order to score these models. \n\n_*this will probably have to run overnight_","6c7066f8":"# Build Models - Try a bunch out","39441e03":"# Conclusion\nAs was predicted the worst performing algoritms were the ones which are sensitive to feature transformation (k-NN and SVCs). Additionally, due to some of the warnings yielded, it seems the support vector machines may not have even converged on every itteration. This validated my initial hypothesis as I didn't scale\/normalize any of my features. A nonconvergance is an additional demonstration for why you want to normalize utilizing these algorithms. Additionally, the vanilla arguements (learning parameters, ext ...) should be adjusted for less marginal results \n\n\nThe next worst performing algoithm was the GaussianNB which is a Naive Bayes variant, which is not a huge surprise. Naive Bayes relies on an assumption that features are independant of one another, which may not be the case with our dataset. For example there is a possible correlation between Education_Level and Income_Category. The next best performing algorithm is the decission tree classifier. Descission trees benefit from the fact that they are relatively simple models and require relatively little data preparation. Finally, our top perfoming algorithms are the two ensemble algorithms - the random forest classifier and the XGBClassifier. The XGBClassifier was able to predict whether or not a customer was going to churn with an accuracy of 93%. ","456919ff":"# Preprocessing"}}