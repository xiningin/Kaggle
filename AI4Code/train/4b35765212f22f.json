{"cell_type":{"b0da225d":"code","156e7ffa":"code","b5171edc":"code","7ec47a4d":"code","8aecca4b":"code","81d82dc4":"code","33d764ea":"code","07fbd30e":"code","a1ca5e87":"code","51378ab3":"code","7772c693":"code","7a96bd06":"code","2351ca35":"markdown","38548ed3":"markdown","b6770ea8":"markdown","3156ac38":"markdown","9a9fa417":"markdown","154a8651":"markdown","000183a8":"markdown","f27cea27":"markdown"},"source":{"b0da225d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","156e7ffa":"from sklearn.datasets import load_iris\n\niris = load_iris()\n\nx = iris.data\ny = iris.target","b5171edc":"x = (x-np.min(x))\/(np.max(x)-np.min(x))","7ec47a4d":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3)","8aecca4b":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)# k = n_neighbors","81d82dc4":"#K Fold CV K=10\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = knn, X=x_train, y=y_train, cv=10)\naccuracies","33d764ea":"print(\"average accuracy: \",np.mean(accuracies))\nprint(\"average std: \",np.std(accuracies))","07fbd30e":"knn.fit(x_train,y_train)\nprint(\"test accuracy: \",knn.score(x_test,y_test))","a1ca5e87":"#grid search cross validation for knn\nfrom sklearn.model_selection import GridSearchCV\n\ngrid = {\"n_neighbors\":np.arange(1,50)}\nknn = KNeighborsClassifier()\n\nknn_cv = GridSearchCV(knn, grid, cv=10)#GridSearchCV\nknn_cv.fit(x_train,y_train)\n\n#%% print hyperparameter KNN algoritmasindaki K degeri\nprint(\"tuned hyperparameter K: \",knn_cv.best_params_)\nprint(\"tuned parametreye g\u00f6re en iyi accuracy (best score): \",knn_cv.best_score_)","51378ab3":"x = x[:100,:]\ny = y[:100]","7772c693":"from sklearn.linear_model import LogisticRegression\n#\"C\":Logistic Regression regularization parameter\n#if it is small it will be underfit.if it is big it will be overfit.\n#we need to choose good value for C.\n#loss functions\ngrid = {\"C\":np.logspace(-3,3,7),\"penalty\":[\"l1\",\"l2\"]} # l1 = lasso ve l2 = ridge\n\nlogreg = LogisticRegression()\nlogreg_cv = GridSearchCV(logreg, grid, cv=10)\nlogreg_cv.fit(x_train,y_train)\n\nprint(\"tuned hyperparameters: (best parameters): \",logreg_cv.best_params_)\nprint(\"accuracy: \",logreg_cv.best_score_)","7a96bd06":"logreg2 = LogisticRegression(C=100,penalty=\"l1\")\nlogreg2.fit(x_train,y_train)\nprint(\"score: \",logreg2.score(x_test,y_test))","2351ca35":"### Grid Search CV with K-Nearest Neighbors Classification","38548ed3":"# INTRODUCTION<br><br>\n* **In this kernel,we will see K-Fold Cross Validation and Grid Search Cross Validation with Machine Learning Classification Algorithms.**","b6770ea8":"### K-Fold Cross Validation with Machine Learning Classification Algorithms","3156ac38":"### Grid Search CV with Logistic Regression Classification","9a9fa417":"### Train and Test Split","154a8651":"### Normalization Data","000183a8":"# Conclusion\n* **If you like it, Please upvote my kernel.**\n* **If you have any question, I will happy to hear it.**","f27cea27":"### Import Data"}}