{"cell_type":{"06c77f9f":"code","8dbc1a6c":"code","479d2e9c":"code","d9c8ed7e":"code","54b97356":"code","170cec24":"code","75cc4889":"code","e4a2b670":"code","67aff2fa":"code","7244675b":"code","0163be48":"code","f3bd8c9d":"code","029c11a7":"code","592af801":"code","ccea1829":"code","fd1991c8":"code","5d3bf703":"code","8d47e071":"code","d87d35b3":"code","09286cbb":"code","56b1994a":"code","b5ac7b5c":"code","112eaf0d":"code","17d0f692":"code","452199cd":"code","74b2b09d":"code","0c98ef45":"code","6572fe8c":"code","21e9a355":"code","acd773b3":"code","c6a880b1":"code","b2283d36":"code","fb383561":"code","ef7f1c07":"code","29a4a4a4":"code","9560db24":"code","b2f70e83":"code","c75a8540":"code","6301c8df":"code","9fb8ef6d":"code","ba5d24c2":"code","a37adab0":"code","91feabaa":"code","2d440c78":"code","5af14ec6":"code","de7dadb9":"code","876af3c1":"code","b3dbf02e":"code","b3ac05ba":"code","520a20f7":"code","983a6b12":"code","cf248c93":"code","d5b5a0d6":"code","557e2cb9":"code","c5cca6fb":"code","e001fcea":"code","bbf5b917":"code","8db81cde":"code","23283472":"code","69c71a4b":"code","dcc27d92":"code","5091655f":"code","42492d3d":"code","f68b0831":"code","659746cd":"code","fdcb72b3":"code","fedebe3b":"code","356dcf4b":"code","70423ae1":"markdown","e9cbf0ef":"markdown","028c4fcf":"markdown","7ca242a9":"markdown","a8b03d23":"markdown","a57b808d":"markdown","d4f62848":"markdown","60746bc0":"markdown","c58cc6ec":"markdown","e7ce05bd":"markdown","11d109e1":"markdown","0deba65a":"markdown","a42dc96d":"markdown","4b62f264":"markdown","5d69506a":"markdown","ffa4e177":"markdown","5ab728d7":"markdown","1e3c9f37":"markdown","5d11331b":"markdown","3c39b0b1":"markdown","d7d1213f":"markdown","ed0c828a":"markdown","afa382a9":"markdown","ffbf23f0":"markdown","3eeede39":"markdown","788da689":"markdown","1a9809c1":"markdown","4029914e":"markdown","98b766d2":"markdown","f860a0b6":"markdown","2d84b0a9":"markdown","1582383e":"markdown","a4ac02ab":"markdown","09315bd5":"markdown","121a94e6":"markdown","50ab7f09":"markdown","9958ba28":"markdown","91aa06fb":"markdown","0f8d06fa":"markdown","6b1eda93":"markdown","68b7ee61":"markdown","cceb4b5b":"markdown","c87abb47":"markdown","0c3a3298":"markdown","d8d7c517":"markdown","f32a17f6":"markdown","f5693f87":"markdown","11c60903":"markdown","80759a82":"markdown","9d7a5213":"markdown","a09490f3":"markdown","e4582898":"markdown","cef12865":"markdown","112c8448":"markdown","0f9d4eed":"markdown","3b3623ff":"markdown","ec099d58":"markdown","b9af1753":"markdown","acddb78f":"markdown","99777f2a":"markdown","93e662c4":"markdown","f08f87b1":"markdown","a4032afb":"markdown","471a7222":"markdown","232d5f81":"markdown","3fcbecc3":"markdown","ea327f02":"markdown","281b9219":"markdown","d2b4ceeb":"markdown","beb24f3a":"markdown","0517f724":"markdown","b895431b":"markdown","737a33cc":"markdown","3bafd81a":"markdown","1d661a92":"markdown","647721d9":"markdown","6d549266":"markdown","af68be1b":"markdown","bf9660b5":"markdown","a19114ab":"markdown","a7395932":"markdown","1a10ee26":"markdown","e5d2444b":"markdown","1f3336a6":"markdown","e4919934":"markdown"},"source":{"06c77f9f":"# Linear algebra\nimport numpy as np \n\n# Data processing\nimport pandas as pd \n\n# Data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\n# Algorithms\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Evaluating algorithms\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import roc_auc_score\n\n# Files location\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8dbc1a6c":"# Test dataset\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\n# Train dataset\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")","479d2e9c":"# Look at a sample of the train dataset\ntrain.head()","d9c8ed7e":"# Get the size of the train dataset\ntrain.shape","54b97356":"# Get the data types of the columns and check for missing values\ntrain.info()","170cec24":"# Use describe() function to get a summary of the data\ntrain.describe(include = 'all')","75cc4889":"# Sum all missing values for each column \ntotal = train.isnull().sum().sort_values(ascending=False)\n\n# Calculate the percentage of missing values for each column\npercent = round(train.isnull().sum()\/train.isnull().count()*100, 1).sort_values(ascending=False)\n\n# Create a data frame containing the total number of missing values and the % out of the total number of values\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', '%'])\n\n# Check the first 5 rows of the missing_data data frame\nmissing_data.head(5)","e4a2b670":"# Create a copy of the original train dataset to work with further on without PassengerId, Name, Ticket & Cabin \ntrain_ml = train.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n\n# Replace the 2 missing values for Embarked with the most common value \"S\" in the dataset\ntrain_ml[\"Embarked\"] = train_ml[\"Embarked\"].fillna(\"S\")\n\n# Replace missing values for Age with random numbers between mean-std and mean+std\n# Calculate mean value for Age\nmean = train_ml[\"Age\"].mean()\n# Calculate standard deviation for Age\nstd = train_ml[\"Age\"].std()\n# Get the size for the randint() function\nis_null = train_ml[\"Age\"].isnull().sum()\n# Compute random numbers between mean-std and mean+std\nrand_age = np.random.randint(mean - std, mean + std, size = is_null)\n# Fill NaN values in Age column with random values generated\nage_slice = train_ml[\"Age\"].copy()\nage_slice[np.isnan(age_slice)] = rand_age\ntrain_ml[\"Age\"] = age_slice\n\n# Check that everything worked as expected using info()\ntrain_ml.info()","67aff2fa":"# Convert Sex column to integer form\ntrain_ml.loc[train_ml[\"Sex\"] == \"male\", \"Sex\"] = 0\ntrain_ml.loc[train_ml[\"Sex\"] == \"female\", \"Sex\"] = 1\ntrain_ml[\"Sex\"] = train_ml[\"Sex\"].astype(int)\n\n# Convert Embarked column to integer form\ntrain_ml.loc[train_ml[\"Embarked\"] == \"S\", \"Embarked\"] = 0\ntrain_ml.loc[train_ml[\"Embarked\"] == \"C\", \"Embarked\"] = 1\ntrain_ml.loc[train_ml[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\ntrain_ml[\"Embarked\"] = train_ml[\"Embarked\"].astype(int)\n\n# Check that everything work as expected\ntrain_ml.head()","7244675b":"# CREATE A FUNCTION TO PLOT THE COUNT % PER CATEGORICAL COLUMN\n\ndef plot_count_percentage(x, x_axis_label, title, h=5, a=1, y_range=100):\n    # Create a new data frame to plot the percentage of passengers per chosen categorical column\n    df1 = train_ml[x].value_counts(normalize=True)*100\n\n    # Rename the columns using an appropriate x_axis_label\n    df1 = df1.reset_index().rename(columns={\"index\": x_axis_label, x: \"Percentage\"})\n    \n    # If chosen columns are Sex or Embarked convert to text\n    if x == \"Sex\":\n        df1.loc[df1[x_axis_label] == 0, x_axis_label] = \"male\"\n        df1.loc[df1[x_axis_label] == 1, x_axis_label] = \"female\"\n    elif x == 'Survived':\n        df1.loc[df1[x_axis_label] == 0, x_axis_label] = \"No\"\n        df1.loc[df1[x_axis_label] == 1, x_axis_label] = \"Yes\"\n    elif x == \"Embarked\":\n        df1.loc[df1[x_axis_label] == 1, x_axis_label] = \"Cherbourg\"\n        df1.loc[df1[x_axis_label] == 2, x_axis_label] = \"Queenstown\"\n        df1.loc[df1[x_axis_label] == 0, x_axis_label] = \"Southampton\"\n\n    # Plot a barplot showing the percentage of passengers per chosen categorical column\n    g = sns.catplot(x=x_axis_label,y='Percentage',kind='bar',data=df1, height=h, aspect=a)\n    g.ax.set_ylim(0,y_range)\n    \n    # Set an appropriate title to the bar plot\n    g.set(title = title + \"\\n\")\n\n    # Add the bar values to the plot\n    for p in g.ax.patches:\n        txt = str(p.get_height().round(0)) + '%'\n        txt_x = p.get_x() \n        txt_y = p.get_height()\n        g.ax.text(txt_x,txt_y,txt)","0163be48":"# CREATE A FUNCTION TO PLOT THE SURVIVED % PER CATEGORICAL COLUMN\n\ndef plot_survived_percentage(x, x_axis_label, title, h=5, a=1, y_range=100):\n    # Create a new data frame to plot the % of survived\/dead passengers vs. chosen categorical column\n    df2 = train_ml.groupby(x)['Survived'].value_counts(normalize=True)*100\n    df2 = df2.to_frame().rename(columns={'Survived': \"Percentage\"}).reset_index().rename(columns={x: x_axis_label})\n\n    # Convert the values for survived to text for plotting\n    df2.loc[df2[\"Survived\"] == 0, \"Survived\"] = \"dead\"\n    df2.loc[df2[\"Survived\"] == 1, \"Survived\"] = \"survived\"\n    \n    # If chosen columns are Sex or Embarked convert to text\n    if x == \"Sex\":\n        df2.loc[df2[x_axis_label] == 0, x_axis_label] = \"male\"\n        df2.loc[df2[x_axis_label] == 1, x_axis_label] = \"female\"\n    elif x == \"Embarked\":\n        df2.loc[df2[x_axis_label] == 1, x_axis_label] = \"Cherbourg\"\n        df2.loc[df2[x_axis_label] == 2, x_axis_label] = \"Queenstown\"\n        df2.loc[df2[x_axis_label] == 0, x_axis_label] = \"Southampton\"\n\n    # Plot a barplot showing the % of survived\/dead passengers vs. chosen categorical column\n    g = sns.catplot(x=x_axis_label,y='Percentage', hue = 'Survived', kind='bar',data=df2, height=h, aspect=a)\n    g.ax.set_ylim(0,y_range)\n    \n    # Set an approriate title\n    g.set(title = title+ \"\\n\")\n    g._legend.remove()\n    g.ax.legend().set_title('')\n\n    # Add the bar values to the plot\n    for p in g.ax.patches:\n        txt = str(p.get_height().round(0)) + '%'\n        txt_x = p.get_x() \n        txt_y = p.get_height()\n        g.ax.text(txt_x,txt_y,txt)","f3bd8c9d":"# Use the function plot_count_percentage to plot the % of total passengers that survived\/died\nplot_count_percentage('Survived', \"Survived Yes\/No\", 'Percentage of total passengers that survived\/died')","029c11a7":"# Use the function plot_count_percentage to plot the % of total passengers per passenger class\nplot_count_percentage('Pclass', \"Passenger Class\", 'Percentage of total passengers per passenger class')","592af801":"# Use the function plot_survived_percentage to plot the % of survived\/dead passengers per passenger class\nplot_survived_percentage('Pclass', 'Passenger Class', 'Percentage of survived\/dead passengers vs. passenger class')","ccea1829":"# Use the function plot_count_percentage to plot the % of total passengers per gender\nplot_count_percentage('Sex', \"Gender\", 'Percentage of total passengers per gender')","fd1991c8":"# Use the function plot_survived_percentage to plot the % of survived\/dead passengers per gender\nplot_survived_percentage('Sex', 'Gender', 'Percentage of survived\/dead passengers vs. gender')","5d3bf703":"# Use the function plot_count_percentage to plot the % of total passengers per number of siblings\/spouses\nplot_count_percentage('SibSp', \"Number of siblings\/spouses\", 'Percentage of total passengers per number of siblings\/spouses')","8d47e071":"# Use the function plot_survived_percentage to plot the % of survived\/dead passengers per number of siblings\/spouses\nplot_survived_percentage('SibSp', \"Number of siblings\/spouses\", 'Percentage of survived\/dead passengers vs. number of siblings\/spouses', 8, 1)","d87d35b3":"# Use the function plot_count_percentage to plot the % of total passengers per number of parents\/children\nplot_count_percentage('Parch', \"Number of parents\/children\", 'Percentage of total passengers per number of parents\/children')","09286cbb":"# Use the function plot_survived_percentage to plot the % of survived\/dead passengers per number of parents\/children\"\nplot_survived_percentage('Parch', \"Number of parents\/children\", 'Percentage of survived\/dead passengers vs. number of parents\/children\"', 8, 1)","56b1994a":"# Use the function plot_count_percentage to plot the % of total passengers per embarkation point\nplot_count_percentage('Embarked', \"Embarkation point\", 'Percentage of total passengers per embarkation point')","b5ac7b5c":"# Use the function plot_survived_percentage to plot the % of survived\/dead passengers per embarkation point\"\nplot_survived_percentage('Embarked', \"Embarkation point\", 'Percentage of survived\/dead passengers vs. embarkation point')","112eaf0d":"# As \"Age\" is a continous numerical variable is best to first have a look at its histogram\nsns.distplot(train_ml.Age, kde=False, bins = 20)","17d0f692":"# Create the column age_categories '0-10', '11-17','18-22', '23-26', '27-32', '33-39', '40-49','>=50'\ntrain_ml['age_categories'] = pd.cut(train_ml.Age, [0, 11, 18, 23, 27, 33, 40, 50, np.inf], \n                                    labels=[0, 1, 2, 3, 4, 5, 6,7], include_lowest=True, right=False).astype(int)\n\n# Create age_categories_text for plotting\ntrain_ml['age_categories_text'] = pd.cut(train_ml.Age, [0, 11, 18, 23, 27, 33, 40, 50, np.inf],right=False,\n                                    labels=['0-10', '11-17','18-22', '23-26', '27-32', '33-39', '40-49','>=50'], include_lowest=True)","452199cd":"# As \"Fare\" is a continous numerical variable is best to first have a look at its histogram\nsns.distplot(train_ml.Fare, kde=False, bins = 50)","74b2b09d":"# Create the column fare_categories '0-7.99', '8-13.99','14-30.99', '31-98.99', '99-249.99', '>=250'\ntrain_ml['fare_categories'] = pd.cut(train_ml.Fare, [0, 8, 14, 31, 99, 250, np.inf], \n                                    labels=[0, 1, 2, 3, 4, 5], include_lowest=True, right=False).astype(int)\n\n# Create age_categories_text for plotting\ntrain_ml['fare_categories_text'] = pd.cut(train_ml.Fare, [0, 8, 14, 31, 99, 250, np.inf], right=False,\n                                    labels=['0-7.99', '8-13.99','14-30.99', '31-98.99', '99-249.99', '>=250'], include_lowest=True)","0c98ef45":"# Use the function plot_count_percentage to plot the % of total passengers per age_category\nplot_count_percentage('age_categories_text', \"Age Categories\", 'Percentage of total passengers per age category', h =6, y_range=25)","6572fe8c":"# Use the function plot_survived_percentage to plot the % of survived\/dead passengers per age category\"\nplot_survived_percentage('age_categories_text', \"Age Category\", 'Percentage of survived\/dead passengers vs. age category', h=8)","21e9a355":"# Use the function plot_count_percentage to plot the % of total passengers per fare_category\nplot_count_percentage('fare_categories_text', \"Fare Categories\", 'Percentage of total passengers per fare category', h =6, y_range=35)","acd773b3":"# Use the function plot_survived_percentage to plot the % of survived\/dead passengers per fare category\"\nplot_survived_percentage('fare_categories_text', \"Fare Category\", 'Percentage of survived\/dead passengers vs. fare category', h=8)","c6a880b1":"# Plot the heat map for the correlation matrix calculated using Pearson method\nfig, ax = plt.subplots(figsize=(7,7)) \nsns.heatmap(train_ml[['Survived', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked', 'age_categories', 'fare_categories']].corr(method='pearson'), \n            annot = True, square=True, fmt='.2g', vmin=-1, vmax=1, center= 0, cmap= 'coolwarm', ax=ax, linewidths=.5, cbar=False)","b2283d36":"# Plot fare categories by passenger class vs. survived\ng = sns.catplot(x=\"fare_categories_text\", y=\"Survived\", hue=\"Pclass\", kind=\"point\", data=train_ml, height = 6,\n                markers=[\"^\", \"o\", \"*\"], linestyles=[\"solid\", \"dotted\", \"dashed\"])\ng.set(title = \"Fare categories by Passenger Class vs. Survived\"+ \"\\n\")\ng._legend.remove()\ng.ax.legend().set_title('Passenger Class')\ng.set_axis_labels(\"Fare Categories\", \"Survival Rate\")","fb383561":"# Create a new string column for gender for plotting\ntrain_ml['sex_text'] = train_ml['Sex'] \ntrain_ml.loc[train_ml['sex_text'] == 0, 'sex_text'] = \"male\"\ntrain_ml.loc[train_ml['sex_text'] == 1, 'sex_text'] = \"female\"\n\n# Plot gender by passenger class vs. survived\ng = sns.catplot(x=\"sex_text\", y=\"Survived\", hue=\"Pclass\", kind=\"point\", data=train_ml, height = 6,\n                markers=[\"^\", \"o\", \"*\"], linestyles=[\"solid\", \"dotted\", \"dashed\"])\ng.set(title = \"Gender by Passenger Class vs. Survived\"+ \"\\n\")\ng._legend.remove()\ng.ax.legend().set_title('Passenger Class')\ng.set_axis_labels(\"Gender\", \"Survival Rate\")","ef7f1c07":"# Plot gender by fare categories vs. survived\ng = sns.catplot(x=\"fare_categories_text\", y=\"Survived\", hue=\"sex_text\", kind=\"point\", data=train_ml, height = 6,\n                markers=[\"^\", \"o\"], linestyles=[\"solid\", \"dotted\"])\ng.set(title = \"Gender by Fare Categories vs. Survived\"+ \"\\n\")\ng._legend.remove()\ng.ax.legend().set_title('Gender')\ng.set_axis_labels(\"Fare Categories\", \"Survival Rate\")","29a4a4a4":"# Create a new string column for gender for plotting\ntrain_ml['embarked_text'] = train_ml['Embarked'] \ntrain_ml.loc[train_ml['embarked_text'] == 1, 'embarked_text'] = \"Cherbourg\"\ntrain_ml.loc[train_ml['embarked_text'] == 2, 'embarked_text'] = \"Queenstown\"\ntrain_ml.loc[train_ml['embarked_text'] == 0, 'embarked_text'] = \"Southampton\"\n\n# Plot passenger class, gender, embarkation point\ng = sns.FacetGrid(train_ml, row='embarked_text', height=4.5, aspect=1.6)\ng.map(sns.pointplot, 'Pclass', 'Survived', 'sex_text', palette=None,  order=None, hue_order=None )\ng.add_legend()\ng.set_axis_labels(\"Passenger Class\", \"Survival Rate\")\ng.set_titles(row_template = 'Embarkation point: {row_name}')","9560db24":"# As we've seen from the correlation matrix there is a relatively high corr between # of siblings\/spouces & # of parents\/children: 0.41\n# We will create a new feature denoted family_members\ntrain_ml['family_members'] = train_ml.SibSp + train_ml.Parch","b2f70e83":"# Plot Family Members by Gender vs. Survived\ng = sns.catplot(x=\"family_members\", y=\"Survived\", hue=\"sex_text\", kind=\"point\", data=train_ml, height = 6,\n                markers=[\"^\", \"o\"], linestyles=[\"solid\", \"dotted\"])\ng.set(title = \"Family Members by Gender vs. Survived\"+ \"\\n\")\ng._legend.remove()\ng.ax.legend().set_title('Gender')\ng.set_axis_labels(\"Family Members\", \"Survival Rate\")","c75a8540":"# Until now we have only worked with the train dataset\n# Lets have a look at a sample of the test dataset\ntest.head()","6301c8df":"# Get the size of the test dataset\ntest.shape","9fb8ef6d":"# Use describe() function to get a summary of the test dataset\ntest.describe(include = 'all')","ba5d24c2":"# Check for missing values\n\n# Sum all missing values for each column \ntotal = test.isnull().sum().sort_values(ascending=False)\n\n# Calculate the percentage of missing values for each column\npercent = round(test.isnull().sum()\/test.isnull().count()*100, 1).sort_values(ascending=False)\n\n# Create a data frame containing the total number of missing values and the % out of the total number of values\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', '%'])\n\n# Check the first 5 rows of the missing_data data frame\nmissing_data.head(5)","a37adab0":"# Create a copy of the original test dataset without Name, Ticket & Cabin\n# In this case we keep the PassengerId column as it will be needed in the submission file\ntest_ml = test.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n\n# Replace missing values for Age with random numbers between mean-std and mean+std\n# Calculate mean value for Age\nmean = test_ml[\"Age\"].mean()\n# Calculate standard deviation for Age\nstd = test_ml[\"Age\"].std()\n# Get the size for the randint() function\nis_null = test_ml[\"Age\"].isnull().sum()\n# Compute random numbers between mean-std and mean+std\nrand_age = np.random.randint(mean - std, mean + std, size = is_null)\n# Fill NaN values in Age column with random values generated\nage_slice = test_ml[\"Age\"].copy()\nage_slice[np.isnan(age_slice)] = rand_age\ntest_ml[\"Age\"] = age_slice","91feabaa":"# Check the passenger class for the 1 missing value of Fare\ntest_ml[test_ml.isnull().any(axis=1)]\n\n# Replace missing values for Fare with mean value for the Passenger Class 3\ntest_ml[\"Fare\"] = test_ml[\"Fare\"].fillna(test_ml.groupby('Pclass').mean()['Fare'][3])\n\n# Check that everything worked as expected using info()\ntest_ml.info()","2d440c78":"# Convert Sex column to integer form\ntest_ml.loc[test_ml[\"Sex\"] == \"male\", \"Sex\"] = 0\ntest_ml.loc[test_ml[\"Sex\"] == \"female\", \"Sex\"] = 1\ntest_ml[\"Sex\"] = test_ml[\"Sex\"].astype(int)\n\n# Convert Embarked column to integer form\ntest_ml.loc[test_ml[\"Embarked\"] == \"S\", \"Embarked\"] = 0\ntest_ml.loc[test_ml[\"Embarked\"] == \"C\", \"Embarked\"] = 1\ntest_ml.loc[test_ml[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\ntest_ml[\"Embarked\"] = test_ml[\"Embarked\"].astype(int)","5af14ec6":"# Create the column age_categories '0-10', '11-17','18-22', '23-26', '27-32', '33-39', '40-49','>=50'\ntest_ml['age_categories'] = pd.cut(test_ml.Age, [0, 11, 18, 23, 27, 33, 40, 50, np.inf], \n                                    labels=[0, 1, 2, 3, 4, 5, 6,7], include_lowest=True, right=False).astype(int)\n\n# Create the column fare_categories '0-7.99', '8-13.99','14-30.99', '31-98.99', '99-249.99', '>=250'\ntest_ml['fare_categories'] = pd.cut(test_ml.Fare, [0, 8, 14, 31, 99, 250, np.inf], \n                                    labels=[0, 1, 2, 3, 4, 5], include_lowest=True, right=False).astype(int)","de7dadb9":"# We will create a new feature denoted family_members\ntest_ml['family_members'] = test_ml.SibSp + test_ml.Parch","876af3c1":"# Now the test dataset is ready to use\n# Let's select only the relevant columns\ntest_final = test_ml[['PassengerId', 'Pclass', 'Sex', 'Embarked', 'age_categories', 'fare_categories', 'family_members']]\ntest_final.head()","b3dbf02e":"# Test dataset features to use in the ML algorithms\nX_test = test_final.drop('PassengerId', axis=1)","b3ac05ba":"# Train dataset features to use in the ML algorithms\nX_train = train_ml[['Pclass', 'Sex', 'Embarked', 'age_categories', 'fare_categories', 'family_members']]\n\n# Target variable\nY_train = train_ml.Survived","520a20f7":"# Apply logistic regression model to the train dataset\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train, Y_train)\n\n# Perform K-Fold Cross Validation to get an accuracy score\nlog_reg_acc = round(cross_val_score(log_reg, X_train, Y_train, cv=10, scoring = \"accuracy\").mean()*100, 4)\n\n# Predictions\npredictions = log_reg.predict(X_test)","983a6b12":"# Apply K-nearest Neighbors model to the train dataset\nknn = KNeighborsClassifier()\nknn.fit(X_train, Y_train)\n\n# Perform K-Fold Cross Validation to get an accuracy score\nknn_acc = round(cross_val_score(knn, X_train, Y_train, cv=10, scoring = \"accuracy\").mean()*100, 4)\n\n# Predictions\npredictions = knn.predict(X_test)","cf248c93":"# Apply Na\u00efve Bayes model to the train dataset\ngaussian_nb = GaussianNB()\ngaussian_nb.fit(X_train, Y_train)\n\n# Perform K-Fold Cross Validation to get an accuracy score\ngaussian_nb_acc = round(cross_val_score(gaussian_nb, X_train, Y_train, cv=10, scoring = \"accuracy\").mean()*100, 4)\n\n# Predictions\npredictions = gaussian_nb.predict(X_test)","d5b5a0d6":"# Apply Decision Tree Classification model to the train dataset\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\n\n# Perform K-Fold Cross Validation to get an accuracy score\ndecision_tree_acc = round(cross_val_score(decision_tree, X_train, Y_train, cv=10, scoring = \"accuracy\").mean()*100, 4)\n\n# Predictions\npredictions = decision_tree.predict(X_test)","557e2cb9":"# Apply Random Forest model to the train dataset\nrandom_forest = RandomForestClassifier()\nrandom_forest.fit(X_train, Y_train)\n\n# Perform K-Fold Cross Validation to get an accuracy score\nrandom_forest_acc = round(cross_val_score(random_forest, X_train, Y_train, cv=10, scoring = \"accuracy\").mean()*100, 4)\n\n# Predictions\npredictions = random_forest.predict(X_test)","c5cca6fb":"# Create a data frame with the name of the model used and the score\nmodel_score = pd.DataFrame({'Model': ['Logistic regression', 'K-nearest Neighbors', 'Gaussian Na\u00efve Bayes', 'Decision Tree', 'Random Forest'],\n                            'Score': [log_reg_acc, knn_acc, gaussian_nb_acc, decision_tree_acc, random_forest_acc]})\n\n# Sort by Score\nmodel_score.sort_values(by='Score', ascending=False).reset_index().drop('index', axis=1)","e001fcea":"# Create a data frame showing the feature importance as given by the random forest model\nfeature_importance = pd.DataFrame({'Feature':X_train.columns,'Importance':np.round(random_forest.feature_importances_,3)})\nfeature_importance.sort_values('Importance',ascending=False).reset_index().drop('index', axis=1)","bbf5b917":"# Lets have a look at the parameters that the random forest model is using\nrandom_forest.get_params()","8db81cde":"# We will use RandomizedSearchCV for hyperparameter tuning so first we need to create a parameter grid\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt', 'log2']\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nrandom_grid","23283472":"# Random search of parameters using 3 fold cross validation and 100 different combinations\nrandom_forest_random = RandomizedSearchCV(estimator = random_forest, param_distributions = random_grid, n_iter = 100, verbose=2, cv = 3, random_state=42, n_jobs = -1)\n\n# Fit the random search model\nrandom_forest_random.fit(X_train, Y_train)","69c71a4b":"# Let's have a look at the best parameters from fitting the random search\nrandom_forest_random.best_params_","dcc27d92":"# Apply Random Forest model to the train dataset using the best parameters\nrandom_forest_2 = random_forest_random.best_estimator_\nrandom_forest_2.fit(X_train, Y_train)\n\n# Perform K-Fold Cross Validation to get an accuracy score\nrandom_forest_acc_2 = round(cross_val_score(random_forest_2, X_train, Y_train, cv=10, scoring = \"accuracy\").mean()*100, 4)\n\n# Let's compare the 2 Random Forest models\n# Add the accuracy score to the model_score data frame\nmodel_score = model_score.append(pd.DataFrame({'Model': ['Random Forest 2'], 'Score': [random_forest_acc_2]})) \n\n# Sort by Score\nmodel_score.sort_values(by='Score', ascending=False).reset_index().drop('index', axis=1)","5091655f":"# We are gonna use GridSearchCV to evaluate all combinations we define\n\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [20, 40, 60, 80, 100],\n    'max_features': ['log2'],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [4, 5, 6],\n    'n_estimators': [100, 200, 300, 700, 1000]\n}\n\n# Define the grid search model\nrandom_forest_grid_search = GridSearchCV(estimator = random_forest, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)\n\n# Fit the grid search model\nrandom_forest_grid_search.fit(X_train, Y_train)","42492d3d":"# Let's at the best parameters from teh grid search\nrandom_forest_grid_search.best_params_","f68b0831":"# Apply Random Forest model to the train dataset using the best parameters\nrandom_forest_3 = random_forest_grid_search.best_estimator_\nrandom_forest_3.fit(X_train, Y_train)\n\n# Perform K-Fold Cross Validation to get an accuracy score\nrandom_forest_acc_3 = round(cross_val_score(random_forest_3, X_train, Y_train, cv=10, scoring = \"accuracy\").mean()*100, 4)\n\n# Let's compare the 2 Random Forest models\n# Add the accuracy score to the model_score data frame\nmodel_score = model_score.append(pd.DataFrame({'Model': ['Random Forest 3'], 'Score': [random_forest_acc_3]})) \n\n# Sort by Score\nmodel_score.sort_values(by='Score', ascending=False).reset_index().drop('index', axis=1)","659746cd":"## Use confusion matrix to get a matrix of true negatives & false positives predictions\npredictions = cross_val_predict(random_forest_3, X_train, Y_train, cv=3)\nconfusion_matrix(Y_train, predictions)","fdcb72b3":"# Let's now look at precision and recall\nprint(\"Precision:\", precision_score(Y_train, predictions))\nprint(\"Recall:\",recall_score(Y_train, predictions))","fedebe3b":"# Getting the probabilities of our predictions\ny_scores = random_forest_3.predict_proba(X_train)\ny_scores = y_scores[:,1]\n\n# Computing the roc_auc_score\nr_a_score = roc_auc_score(Y_train, y_scores)\nprint(\"ROC-AUC-Score:\", r_a_score)","356dcf4b":"# Use the final random_forest_3 model to make predictions on the test dataset\nfinal_predictions = random_forest_3.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_final.PassengerId, 'Survived': final_predictions})\noutput.to_csv('random_forest_model.csv', index=False)","70423ae1":"## Random Forest","e9cbf0ef":"* **We can see that Random Forest is the highest ranked model based on the K-Fold Cross Validation accuracy score**\n* **Further on we will have a look at how to improve the model**","028c4fcf":"## Gender by Fare Categories vs Survived","7ca242a9":"## Compare the chosen machine learning models","a8b03d23":"* **Having 1 or 2 siblings\/spouses on board increases the chances of survival: 54% & 46%**\n* **Traveling alone the passenger had a 35% chance of surviving**\n* **Having 3 or 4 siblings\/spouses on board you had 25% & 17% chance of surviving**\n* **All passengers with 5 & 8 siblings\/spouses on board died**","a57b808d":"## Precision and Recall","d4f62848":"* **For embarkation points Southhampron & Queenstown women have a higher survival rate than men accross all passenger classes**\n* **For embarkation point Cherbough we observe the opposite men have higher survival rate than women accross all passenger classes**","60746bc0":"* **It looks similar to the train dataset and includes all same columns except \"Survived\" which we need to predict**\n* **As for the train dataset we will need to convert the categorical columns and created categories for Age and Fare**","c58cc6ec":"# Getting the Data","e7ce05bd":"* **34% of passengers that embarked at Southhampton survived**\n* **55% of passengers that embarked at Cherbourg survived**\n* **39% of passengers that embarked at Queenstown survived**","11d109e1":"# Preparing the Datasets for Machine Learning","0deba65a":"## K-nearest Neighbors (KNN)","a42dc96d":"* **We can start with \"Embarked\" which has 2 missing values. We can safely fill in the missing values with the most common value which is \"S\"**\n* **For \"Age\" we have 20% missing values. Here we can fill the missing values random numbers between mean-std and mean+std**\n* **For \"Cabin\" we have 77% missing values so we can drop this column**","4b62f264":"* **We have 891 rows and 12 columns**","5d69506a":"* **We can see that we have 2 categorical columns in our dataset Sex & Embarked**\n* **In the next step we will convert the categorical columns to integer columns**\n* **This will help further on when appling different ML algorithms & for plotting**","ffa4e177":"## Percentage of total survived\/dead passengers","5ab728d7":"# Creating new features","1e3c9f37":"## Create two functions to plot the % of total counts and % of survived\/dead passengers","5d11331b":"## Decision Tree Classification","3c39b0b1":"# Data visualization","d7d1213f":"## Confusion Matrix","ed0c828a":"## Random Search Training","afa382a9":"## Family Members by Gender vs. Survived","ffbf23f0":"* **We can clearly see that the passenger class affects the survival rate**\n* **63% of passengers in 1st Class survived**\n* **47% of passengers in 2nd Class survived**\n* **24% of passengers in 3rd Class survived**\n* **It seems like better passenger class means higher survival rate**","3eeede39":"* **The higher the fare the higher chance of survival for both women & men**","788da689":"* **We have created 8 age categories that follow the original distribution of Age values**\n* **Most passengers have ages between 18-49**\n* **14% of passengers are children below 18 years old**\n* **Only 8% of passengers are over 50 years old**","1a9809c1":"* **We obtain a very small increase of 0.2% so we will stop with hyperparameter tuning here**","4029914e":"* **Most passengers were traveling alone: 68%**\n* **23% of passengers were traveling with 1 sibling or spouse**\n* **The % of passengers that were traveling with more than or 2 siblings\/spouse is < 10%**","98b766d2":"# Correlation matrix","f860a0b6":"* **Lower # of family members means higher survival rate for women**\n* **Highest survival rate for both genders is for # of family members = 3**","2d84b0a9":"## Number of parents\/children aboard the Titanic vs. Survived","1582383e":"* **Most passengers embarked in Southhampton: 73%**\n* **19% of passengers embarked in Cherbourg**\n* **9% of passengers embarked in Queenstown**","a4ac02ab":"## Logistic regression","09315bd5":"* **This is a lot of parameters but we will try adjusting just a couple of them:**\n* **n_estimators = number of trees in the forest**\n* **max_features = max number of features considered for splitting a node**\n* **max_depth = max number of levels in each decision tree**\n* **min_samples_split = min number of data points placed in a node before the node is split**\n* **min_samples_leaf = min number of data points allowed in a leaf node**\n* **bootstrap = method for sampling data points (with or without replacement)**","121a94e6":"## Passenger class vs. Survived","50ab7f09":"* **Most fare tickets are between 0-98.99**\n* **Few values above 99: 6% from total tickets**","9958ba28":"* **We can see that more than half of all passengers were in 3rd Class: 55%**\n* **In the 1st Class we have 24% while in 2nd class we have 21%**","91aa06fb":"## Gender, Passenger Class, Embarcation point vs. Survived","0f8d06fa":"* **We can clearly see that the gender has an influence on the survival rate**\n* **Female passengers are more likely to survive: 74%**\n* **Male passengers are less likely to survive: 19%**","6b1eda93":"* **From this plot we can see that most passengers are male: 65%**\n* **Female passengers account for only: 35%**","68b7ee61":"* **Having 1,2 or 3 parents\/children on board increases the chances of survival: 50-60%**\n* **Having no parents\/children on board meant a 34% chance of surviving**\n* **Having 5 parents\/children on board you had 20% chance of surviving**\n* **All passengers with 4 & 6 parents\/children on board died**","cceb4b5b":"## Grid Search with Cross Validation","c87abb47":"* **Similar to train dataset we will drop the Cabin column as it has too many missing values**\n* **For Age column we will use the same method as for the train dataset and replace missing values for Age with random numbers between mean-std and mean+std**\n* **For Fare column we will check the passenger class for the row with missing value and replace it with the mean values for that passenger class as we have seen that there is a high correlation between fare categories and passenger class**","0c3a3298":"## Train dataset","d8d7c517":"## Create one feature for Family Members on board of Titanic","f32a17f6":"## Hyperparameter Tuning","f5693f87":"# Data Exploration and Analysis","11c60903":"* **We can see that more than half of all passengers died: 62%**\n* **Only 38% of all passengers survived**","80759a82":"* Age, Cabin & Embarked have missing values as the non-null count is lower than 891 which is the total no of entries\n* 1 target variable: Survived (integer)\n* 11 features: 5 are integers, 5 are objects and 2 are floats\n<pre> \n**PassengerId:** Unique Id of a passenger <br \/>\n**Survived:**    0 = No, 1 = Yes <br \/>\n**Name:**        The name of the passenger <br \/>\n**Pclass:**      Ticket class (1st, 2nd, 3rd) <br \/>\n**Sex:**         Gender (male, female) <br \/>\n**Age:**         Age in years <br \/>\n**SibSp:**       # of siblings \/ spouses aboard the Titanic <br \/>\n**Parch:**       # of parents \/ children aboard the Titanic <br \/>\n**Ticket:**      Ticket number <br \/>\n**Fare:**        Passenger fare <br \/>\n**Cabin:**       Cabin number <br \/>\n**Embarked:**    Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton) <br \/>","9d7a5213":"* **Most passengers were traveling without children\/parents: 76%**\n* **13% of passengers had 1 parent\/child on board**\n* **9% of passengers had 2 parents\/children on board**\n* **Only 2% of passengers had 3, 4, 5 or 6 parents\/children on board**","a09490f3":"# Evaluating our Model","e4582898":"## Filling missing values","cef12865":"* **This means that our model predict 80% of time correctly**\n* **Recall tells us that the model predicted the survival of 69% of the people who actually survived**","112c8448":"* **38% of passengers from the train dataset survived**\n* **577 passengers were male (64.7%**)\n* **Age ranges from 0.4 - 80.0**\n* **SibSp ranges from 0 - 8**\n* **Parch ranges from 0 - 6**\n* **Fare ranges from 0.0 - 512.3**\n* **644 embarkation ports were of type S (72.2%)**","0f9d4eed":"* **A higher fare category significantly increases the chances of survival**\n* **Highest fare category gives the higher chance of survival: 78%**\n* **Lowest fare category gives the lowest chance of survival: 22%**","3b3623ff":"## Convert categorical columns to integer","ec099d58":"* **First row shows the that 490 passengers were correctly classified as not survived (called true negatives) and 59 where wrongly classified as not survived (false positives)**\n* **Second row shows that 104 passengers where wrongly classified as survived (false negatives) and 238 where correctly classified as survived (true positives)**","b9af1753":"* **266 passengers were male (63.6%**)\n* **Age ranges from 0.17 - 76.0**\n* **SibSp ranges from 0 - 8**\n* **Parch ranges from 0 - 9**\n* **Fare ranges from 0.0 - 512.3**\n* **270 embarkation ports were of type S (64.5%)**\n* **Similar observations as for the train dataset which is a good indication that we can use the train dataset to train our model and use it to predict the \"Survived\" variable for the test dataset**","acddb78f":"## Create categories for Age & Fare","99777f2a":"## Gaussian Na\u00efve Bayes","93e662c4":"# Importing Libraries","f08f87b1":"* **We can see that the accuracy score was improved by 3%**\n* **Now let's try to narrow down our hyperparameter tuning based on the random search results and see if we can improve the accuracy even more**","a4032afb":"# Create the Output File for Submission","471a7222":"1. We are dealing with a classification problem as the output or target variable is discrete\n1. Our target variable \"Survived\" can only take 2 values: 0 = No & 1 = Yes\n1. Classification belongs to the category of supervised learning where the labels or targets are provided in the train dataset","232d5f81":"* **The histogram tells us that most fare values are between 0-100**\n* **Some outliers values are present over 200**\n* **In the next step we will transform the Fare variable into a categorical variable with 6 groups**","3fcbecc3":"## Checking missing values in more detail","ea327f02":"## Age vs. Survived","281b9219":"* **Passenger class 1 is associated with high fare categories resulting in high survival rates**\n* **Low fare categories associated with 1st & 2nd passenger classes result in very low survival rates**\n* **Survival rate for 3rd passenger class is around 20% accross all its fare categories**","d2b4ceeb":"## Embarkation point vs. Survived","beb24f3a":"## Random Forest - Feature Importance","0517f724":"# Testing Several Machine Learning Models","b895431b":"## Fare vs. Survived","737a33cc":"* **We can see that the age category 18-22 has a slight higher chance of survival: 69%**\n* **All age categories have a survival rate between 41-69%**\n* **Lowest survival rate is for children below 11 years old: 41%**","3bafd81a":"## Evaluate the Random Search for Hyperparameter Tuning","1d661a92":"## Gender per Passenger Class vs. Survived","647721d9":"* **Looking at the dataset we can assume that except PassengerId, Name & Ticket all other features could contribute to a high survival rate**\n* **So a detailed data analysis is necessary to understand the relationship between the features & target variable (Survived)**","6d549266":"* **The histogram tells us that most passengers have ages between 16-44**\n* **Around 60 passengers are children bellow 16 years old**\n* **Around 50 passengers are adults over 50 years old**\n* **In the next step we will transform the Age variable into a categorical variable with 8 groups**","af68be1b":"* **Highest correlation with the target variable Survived we observe for gender: positive corr 0.54**\n* **A negative corr of -0.34 is observed for passenger class & the target variable Survived**\n* **A positive corr of 0.3 is observed for fare categories & the target variable Survived**\n* **A positive corr of 0.11 is observed for the point of embarkation & the target variable Survived**\n* **Age, # of siblings\/spouces & # of parents\/children & the target variable Survived have corr < 0.1**\n* **High negative corr -0.66 is observed between passenger class & fare categories**\n* **Relative high corr between # of siblings\/spouces & # of parents\/children: 0.41**","bf9660b5":"## Fare Categories per Passenger Class vs. Survived","a19114ab":"* **Women have higher chance of survival accross all passenger classes with high advantage against males for 1st & 2nd class**","a7395932":"## Number of siblings\/spouses aboard the Titanic vs Survived","1a10ee26":"## Gender vs. Survived","e5d2444b":"## ROC AUC Score","1f3336a6":"## Test dataset","e4919934":"* **The test dataset is approximately half the size of the train dataset: 418 rows vs 891 rows for train dataset**"}}