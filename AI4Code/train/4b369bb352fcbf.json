{"cell_type":{"38f92249":"code","d816c916":"code","9bab04ab":"code","728e2fd0":"code","a749bc90":"code","7bb158c1":"code","992da151":"code","9d0090ae":"code","d2a1e139":"code","59c372a0":"code","82eb1f9d":"code","071eba2a":"code","7fcce2a4":"code","3b347b9d":"code","ae1b52b9":"code","cd801a9b":"code","01cae0ce":"code","662e6519":"code","17d1030f":"code","e3395456":"code","a9190e12":"code","b1801d42":"code","3824af84":"code","1e7f7569":"code","ab5af4da":"code","10a63b6c":"code","db9ab5e3":"code","73b720b8":"markdown","76daa3c7":"markdown","42c90a7a":"markdown","a4a484fd":"markdown","f265ab56":"markdown","636424c8":"markdown","4db61d10":"markdown","27378d0e":"markdown","abd29a73":"markdown","913a4887":"markdown","b32c1c4f":"markdown","692f2f9b":"markdown","144a483f":"markdown","ae8d34f4":"markdown","2b4999db":"markdown","413f23f9":"markdown","8c9520ac":"markdown","b6eee856":"markdown","e39b5c9f":"markdown","7476c23b":"markdown","6f3ef2d0":"markdown","b7303863":"markdown","e654251d":"markdown","cdffa3c5":"markdown","0567e163":"markdown","05b196ad":"markdown","63f993f3":"markdown","da0ba3c9":"markdown"},"source":{"38f92249":"from sklearn.preprocessing import RobustScaler #robust normlization for outliers\nimport sklearn.metrics as metrics #metrics librry\nimport seaborn as sns # for intractve graphs\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nimport matplotlib.pyplot as plt #for visualization\nfrom xgboost.sklearn import XGBClassifier #XGBoost\nfrom sklearn.metrics import classification_report\nfrom xgboost import plot_importance #feature rimportance\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom keras.models import Model, load_model\nfrom keras.layers import Input,Dense, BatchNormalization #layers of autoencoder\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping #callbacks\nfrom keras import regularizers #regularization\nfrom sklearn.ensemble import IsolationForest #Isolation Forest\nfrom sklearn.mixture import GaussianMixture #Gaussian Mixture\nfrom imblearn.over_sampling import SMOTE\n# This Python 3 environment comes with many helpful analytics libraries installed\n\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Any results you write to the current directory are saved as output.\n# Any results you write to the current directory are saved as output.\ndataframe = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")\ndataframe.info()\n","d816c916":"RS=RobustScaler()\ndataframe['Amount'] = RS.fit_transform(dataframe['Amount'].values.reshape(-1, 1))\ndataframe['Time'] = RS.fit_transform(dataframe['Time'].values.reshape(-1, 1))\ndf = dataframe.sample(frac=1, random_state = 42)\ndf.head()","9bab04ab":"print('Normal', round(\n        df['Class'].value_counts()[0]\/len(df)*100, 2), '% of the dataset')\nprint('Fraud', round(\n        df['Class'].value_counts()[1]\/len(df)*100, 2), '% of the dataset')\nsns.countplot(\"Class\",data=df)\n","728e2fd0":"fraud_df_train = df.loc[df['Class'] == 1][:int(492*0.8)]\nfraud_df_test = df.loc[df['Class'] == 1][int(492*0.8):]\n\n\n#undersampling of the data. Fraude represent 10% of base now\nnormal_df_train_sup= df.loc[df['Class'] == 0][:int(492*0.8*9*3)]\nnormal_df_test= df.loc[df['Class'] == 0][int(492*0.8)*9*3:int(492*0.8*9*3)+int(284800*0.2)]\nnew_df_train = pd.concat([pd.DataFrame(normal_df_train_sup), fraud_df_train])\n\n#oversampling of the data. The number of Fraud was twiced\nsm = SMOTE(k_neighbors=5, random_state=0, n_jobs=8, ratio={1:int(492*0.8*3), 0:int(492*0.8*9*3)})\n\nnormal_df_train_sup, fraud_df_train = sm.fit_resample(new_df_train.drop('Class', axis=1), new_df_train['Class'])\nfraud_df_train = pd.DataFrame(fraud_df_train.transpose()).rename(columns={0:\"Class\"})\nnew_df_train = pd.concat([pd.DataFrame(normal_df_train_sup), fraud_df_train ], axis=1)\nnew_df_train.columns=['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\\\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19',\\\n       'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28',\\\n       'Amount','Class']\nnew_df_test = pd.concat([pd.DataFrame(normal_df_test), fraud_df_test])\n\n\n","a749bc90":"new_df_train","7bb158c1":"print('Normal', round(\n        new_df_test['Class'].value_counts()[0]\/len(new_df_test)*100, 2), '% of the test dataset')\nprint('Fraud', round(\n        new_df_test['Class'].value_counts()[1]\/len(new_df_test)*100, 2), '% of the test dataset')\nsns.countplot(\"Class\",data=new_df_test).set_title('Class Count - Test Data')","992da151":"\nX_train_sup = new_df_train.drop('Class', axis=1)\ny_train = new_df_train['Class']\n\nX_test=new_df_test.drop('Class', axis=1)\ny_test=new_df_test['Class']\nnew_df_train.head()\n\nprint('Normal', round(\n        pd.Series(y_train).value_counts()[0]\/len(X_train_sup)*100, 2), '% of the train dataset')\nprint('Fraude', round(\n        pd.Series(y_train).value_counts()[1]\/len(X_train_sup)*100, 2), '% of the train dataset')\nsns.countplot(\"Class\",data=new_df_train).set_title('Class Count - Train Data')","9d0090ae":"#grid_search\n\"\"\"\nfrom sklearn.model_selection import RandomizedSearchCV\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nbootstrap = [True, False]\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)\n\n\nrf_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, \n                               n_iter = 100, verbose=2, n_jobs = -1)\nrf_random.fit(X_res,y_res)\nrf_random.best_params_\n\"\"\"\n\nrfc = RandomForestClassifier(n_estimators = 1600,\n min_samples_split = 2,\n min_samples_leaf = 1,\n max_features = 'sqrt',\n max_depth = 100,\n bootstrap = False)\n;\n\nxgb = XGBClassifier(min_child_weight = 5,\n max_depth=12,\n learning_rate= 0.1,\n gamma= 0.2,\n colsample_bytree= 0.7)","d2a1e139":"rfc.fit(X_train_sup,y_train)\npredi\u00e7tion_rfc = rfc.predict_proba(X_test.values)\ntresholds = np.linspace(0 , 1 , 200)\nscores_rfc=[]\nfor treshold in tresholds:\n    y_hat_rfc = (predi\u00e7tion_rfc[:,0] < treshold).astype(int)\n    scores_rfc.append([metrics.recall_score(y_pred=y_hat_rfc, y_true=y_test),\n                 metrics.precision_score(y_pred=y_hat_rfc, y_true=y_test),\n                 metrics.fbeta_score(y_pred=y_hat_rfc, y_true=y_test, beta=2),\n                 metrics.cohen_kappa_score(y1=y_hat_rfc, y2=y_test)])\nscores_rfc = np.array(scores_rfc)\nfinal_tresh = tresholds[scores_rfc[:, 2].argmax()]\ny_hat_rfc = (predi\u00e7tion_rfc < final_tresh).astype(int)\ncm_rfc = metrics.confusion_matrix(y_test,y_hat_rfc[:,0])\nbest_score = scores_rfc[scores_rfc[:, 2].argmax(),:]\nrecall_score = best_score[0]\nprecision_score = best_score[1]\nfbeta_score = best_score[2]\ncohen_kappa_score = best_score[3]\n\nprint('The recall score is: %.3f' % recall_score)\nprint('The precision score is: %.3f' % precision_score)\nprint('The f2 score is: %.3f' % fbeta_score)\nprint('The Kappa score is: %.3f' % cohen_kappa_score)","59c372a0":"predictors = ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\\\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19',\\\n       'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28',\\\n       'Amount']\ntmp = pd.DataFrame({'Feature': predictors, 'Feature importance': rfc.feature_importances_})\ntmp = tmp.sort_values(by='Feature importance',ascending=False)\nplt.figure(figsize = (7,4))\nplt.title('Features importance - Random Forest',fontsize=14)\ns = sns.barplot(x='Feature',y='Feature importance',data=tmp)\ns.set_xticklabels(s.get_xticklabels(),rotation=90)\nplt.show()  \n","82eb1f9d":"\ny_true = y_test\ny_pred_rfc = y_hat_rfc[:,0]\ntarget_names = ['class 0 (Normal)', 'class 1 (Fraud)']\nprint(classification_report(y_true, y_pred_rfc, target_names=target_names))","071eba2a":"cm = pd.crosstab(y_test, y_pred_rfc, rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\nsns.heatmap(cm, \n            xticklabels=['Not Fraud', 'Fraud'],\n            yticklabels=['Not Fraud', 'Fraud'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\nplt.title('Confusion Matrix for RandomForest', fontsize=14)\nplt.show()","7fcce2a4":"#precision_recall\nplt.plot(tresholds, scores_rfc[:, 0], label='$Recall$')\nplt.plot(tresholds, scores_rfc[:, 1], label='$Precision$')\nplt.plot(tresholds, scores_rfc[:, 2], label='$F_2$')\nplt.ylabel('Score')\n# plt.xticks(np.logspace(-10, -200, 3))\nplt.xlabel('Threshold')\nplt.legend(loc='best')\nplt.title('trade off Precision - Recall for Random Forrest')\nplt.show()","3b347b9d":"xgb = XGBClassifier(min_child_weight = 5,\n max_depth=12,\n learning_rate= 0.1,\n gamma= 0.2,\n colsample_bytree= 0.7)\n\nxgb.fit(X_train_sup, y_train)\n\nprediction_xgb = xgb.predict_proba(X_test)\ntresholds = np.linspace(0 , 1 , 200)\nscores_xgb=[]\nfor treshold in tresholds:\n    y_hat_xgb = (prediction_xgb[:,0] < treshold).astype(int)\n    scores_xgb.append([metrics.recall_score(y_pred=y_hat_xgb, y_true=y_test),\n                 metrics.precision_score(y_pred=y_hat_xgb, y_true=y_test),\n                 metrics.fbeta_score(y_pred=y_hat_xgb, y_true=y_test, beta=2),\n                 metrics.cohen_kappa_score(y1=y_hat_xgb, y2=y_test)])  \nscores_xgb = np.array(scores_xgb)\nfinal_tresh = tresholds[scores_xgb[:, 2].argmax()]\ny_hat_xgb = (prediction_xgb < final_tresh).astype(int)\nbest_score_xgb = scores_xgb[scores_xgb[:, 2].argmax(),:]\nrecall_score_xgb = best_score_xgb[0]\nprecision_score_xgb = best_score_xgb[1]\nfbeta_score_xgb = best_score_xgb[2]\ncohen_kappa_score_xgb = best_score_xgb[3]\n\nprint('The recall score is\": %.3f' % recall_score_xgb)\nprint('The precision score is\": %.3f' % precision_score_xgb)\nprint('The f2 score is\": %.3f' % fbeta_score_xgb)\nprint('The Kappa score is\": %.3f' % cohen_kappa_score_xgb)","ae1b52b9":"\ny_true = y_test\ny_pred_xgb = y_hat_xgb[:,0]\ntarget_names = ['class 0 (Normal)', 'class 1 (Fraud)']\nprint(classification_report(y_true, y_pred_xgb, target_names=target_names))\ncm = pd.crosstab(y_test, y_pred_xgb, rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\nsns.heatmap(cm, \n            xticklabels=['Not Fraud', 'Fraud'],\n            yticklabels=['Not Fraud', 'Fraud'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\nplt.title('Confusion Matrix for XGBoost', fontsize=14)\nplt.show()\nfig, (ax) = plt.subplots(ncols=1, figsize=(8,5))\nplot_importance(xgb, height=0.8, title=\"Features importance - XGBoost\", ax=ax, color=\"blue\") \nplt.show()","cd801a9b":"#precision_recall\nplt.plot(tresholds, scores_xgb[:, 0], label='$Recall$')\nplt.plot(tresholds, scores_xgb[:, 1], label='$Precision$')\nplt.plot(tresholds, scores_xgb[:, 2], label='$F_2$')\nplt.ylabel('Score')\n# plt.xticks(np.logspace(-10, -200, 3))\nplt.xlabel('Threshold')\nplt.legend(loc='best')\nplt.title('trade off Precision - Recall for XGBoost')\nplt.show()","01cae0ce":"scores=[]\nfor i in  list(range(1,11)):\n    df = dataframe.sample(frac=1, random_state = i)\n    #divid frauds in train (80%) and test (20%)\n    fraud_df_train = df.loc[df['Class'] == 1][:int(492*0.8)]\n    fraud_df_test = df.loc[df['Class'] == 1][int(492*0.8):]\n\n    # undersmpling of norml data: the normal data represent 90% of the new base\n    normal_df_train_sup= df.loc[df['Class'] == 0][:int(492*0.8*9)]\n    normal_df_test= df.loc[df['Class'] == 0][int(492*0.8)*9:int(492*0.8*9)+int(284807*0.2)]\n    new_df_train = pd.concat([normal_df_train_sup, fraud_df_train])\n    new_df_test = pd.concat([normal_df_test, fraud_df_test])\n\n    X_train_sup = new_df_train.drop('Class', axis=1)\n    y_train = new_df_train['Class']\n    X_test=new_df_test.drop('Class', axis=1)\n    y_test=new_df_test['Class']\n\n    rfc.fit(X_train_sup,y_train)\n    prediction_rfc = rfc.predict_proba(X_test.values)\n    tresholds = np.linspace(0 , 1 , 200)\n    scores_rfc=[]\n    for treshold in tresholds:\n        y_hat_rfc = (prediction_rfc[:,0] < treshold).astype(int)\n        scores_rfc.append([metrics.recall_score(y_pred=y_hat_rfc, y_true=y_test),\n                 metrics.precision_score(y_pred=y_hat_rfc, y_true=y_test),\n                 metrics.fbeta_score(y_pred=y_hat_rfc, y_true=y_test, beta=2),\n                 metrics.cohen_kappa_score(y1=y_hat_rfc, y2=y_test)])\n    scores_rfc = np.array(scores_rfc)\n    #choice the model with best f2 score\n    best_scores = scores_rfc[scores_rfc[:, 2].argmax(),:]\n    scores.append(best_scores)\n    print('recall, precision, f2, kappa in shuffle %1d' %i)\n    print(best_scores)\n\n    recall_score_rfc = np.mean(scores, axis=0)[0]\n    precision_score_rfc = np.mean(scores,axis=0)[1]\n    fbeta_score_rfc = np.mean(scores, axis=0)[2]\n    cohen_kappa_score_rfc = np.mean(scores, axis=0)[3]\nprint('--------------------------------------------------')\nprint(\"for the random forest algorithm:\")\nprint('The recall score is: %.3f' % recall_score_rfc)\nprint('The precision score is: %.3f' % precision_score_rfc)\nprint('The f2 score is: %.3f' % fbeta_score_rfc)\nprint('The Kappa score is: %.3f' % cohen_kappa_score_rfc)","662e6519":"scores=[]\n\nfor i in  list(range(1,11)):\n    df = dataframe.sample(frac=1, random_state = i)\n    #divid frauds in train (80%) and test (20%)\n    fraud_df_train = df.loc[df['Class'] == 1][:int(492*0.8)]\n    fraud_df_test = df.loc[df['Class'] == 1][int(492*0.8):]\n\n    # undersmpling of norml data: the normal data represent 90% of the new base\n    normal_df_train_sup= df.loc[df['Class'] == 0][:int(492*0.8*9)]\n    normal_df_test= df.loc[df['Class'] == 0][int(492*0.8)*9:int(492*0.8*9)+int(284807*0.2)]\n    new_df_train = pd.concat([normal_df_train_sup, fraud_df_train])\n    new_df_test = pd.concat([normal_df_test, fraud_df_test])\n    \n    X_train_sup = new_df_train.drop('Class', axis=1)\n    y_train = new_df_train['Class']\n    X_test=new_df_test.drop('Class', axis=1)\n    y_test=new_df_test['Class']\n\n    xgb.fit(X_train_sup.values,y_train)\n    prediction_xgb = xgb.predict_proba(X_test.values)\n    tresholds = np.linspace(0 , 1 , 200)\n    scores_xgb=[]\n    for treshold in tresholds:\n        y_hat_xgb = (prediction_xgb[:,0] < treshold).astype(int)\n        scores_xgb.append([metrics.recall_score(y_pred=y_hat_xgb, y_true=y_test),\n                 metrics.precision_score(y_pred=y_hat_xgb, y_true=y_test),\n                 metrics.fbeta_score(y_pred=y_hat_xgb, y_true=y_test, beta=2),\n                 metrics.cohen_kappa_score(y1=y_hat_xgb, y2=y_test)])\n    scores_xgb = np.array(scores_xgb)\n    #choice the model with best f2 score\n    best_scores = scores_xgb[scores_xgb[:, 2].argmax(),:]\n    \n    print('recall, precision, f2, kappa in shuffle %1d' %i)\n    print(best_scores)\n    scores.append(best_scores)\n    \nrecall_score_xgb = np.mean(scores, axis=0)[0]\nprecision_score_xgb = np.mean(scores,axis=0)[1]\nfbeta_score_xgb = np.mean(scores, axis=0)[2]\ncohen_kappa_score_xgb = np.mean(scores, axis=0)[3]\n\nprint('---------------------------------------------')\nprint(\"for the XGBoost algorithm classifier:\")\nprint('The recall score is: %.3f' % recall_score_xgb)\nprint('The precision score is: %.3f' % precision_score_xgb)\nprint('The f2 score is: %.3f' % fbeta_score_xgb)\nprint('The Kappa score is: %.3f' % cohen_kappa_score_xgb)","17d1030f":"i = 0\nt0 = dataframe.loc[df['Class'] == 0]\nt1 = dataframe.loc[df['Class'] == 1]\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(8,4,figsize=(16,28))\n\nfor feature in predictors:\n    i += 1\n    plt.subplot(8,4,i)\n    sns.kdeplot(t0[feature], bw=0.5,label=\"Class = 0\")\n    sns.kdeplot(t1[feature], bw=0.5,label=\"Class = 1\")\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","e3395456":"df = dataframe.sample(frac=1, random_state=42)\nfraude_df_train = df.loc[df['Class'] == 1][:int(492*0.8)]\nfraude_df_test = df.loc[df['Class'] == 1][int(492*0.8):]\n\nnormal_df_test= df.loc[df['Class'] == 0][int(492*0.8)*9:int(492*0.8*9)+int(284807*0.2)]\n\nnew_df_train_semisup= df.loc[df['Class'] == 0][:int(284807*.8)]\nnew_df_test = pd.concat([normal_df_test, fraude_df_test])\n\nX_train_semisup = new_df_train_semisup.drop('Class', axis=1)\n#X_train_semisup=X_train_semisup[['V4','V5', 'V7','V9','V10','V11', 'V12', 'V14', 'V16', 'V17', 'V18', 'Amount']]\nX_test = new_df_test.drop('Class', axis=1)\n#X_test=X_test[['V4','V5', 'V7','V9','V10','V11', 'V12', 'V14', 'V16', 'V17', 'V18', 'Amount']]\ny_test = new_df_test['Class']\n","a9190e12":"X_train_semisup.head()","b1801d42":"from sklearn.mixture import GaussianMixture\n\ngmm = GaussianMixture(n_components=3, n_init=5)\ngmm.fit(X_train_semisup)\n\nprediction_MG = gmm.score_samples(X_test)\n\nscores_MG = []\ntresholds = np.linspace(-1000 , 100 , 200)\n\nfor treshold in tresholds:\n    y_hat_MG = (prediction_MG < treshold).astype(int)\n    scores_MG.append([metrics.recall_score(y_pred=y_hat_MG, y_true=y_test),\n                 metrics.precision_score(y_pred=y_hat_MG, y_true=y_test),\n                 metrics.fbeta_score(y_pred=y_hat_MG, y_true=y_test, beta=2),\n                 metrics.cohen_kappa_score(y1=y_hat_MG, y2=y_test)])\n    \nscores_MG=np.array(scores_MG)\nfinal_tresh = tresholds[scores_MG[:, 2].argmax()]\ny_hat_MG = (prediction_MG < final_tresh).astype(int)\nbest_score = scores_MG[scores_MG[:, 2].argmax(),:]\nrecall_score = best_score[0]\nprecision_score = best_score[1]\nfbeta_score = best_score[2]\ncohen_kappa_score = best_score[3]\n\nprint('The recall score is\": %.3f' % recall_score)\nprint('The precision score is\": %.3f' % precision_score)\nprint('The f2 score is\": %.3f' % fbeta_score)\nprint('The Kappa score is\": %.3f' % cohen_kappa_score)\ntarget_names = ['class 0 (Normal)', 'class 1 (Fraud)']\nprint(classification_report(y_test, y_hat_MG, target_names=target_names))\n\ncm = pd.crosstab(y_test, y_hat_MG, rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\nsns.heatmap(cm, \n            xticklabels=['Not Fraud', 'Fraud'],\n            yticklabels=['Not Fraud', 'Fraud'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\nplt.title('Confusion Matrix for Gaussian Mix', fontsize=14)\nplt.show()\n","3824af84":"model_IF = IsolationForest(random_state=42, n_jobs=4, max_samples=X_train_semisup.shape[0], bootstrap=False, n_estimators=100)\nmodel_IF.fit(X_train_semisup)\ntresholds = np.linspace(-.2, .2, 200)\nprediction_IF = model_IF.decision_function(X_test)\nscores_IF = []\nfor treshold in tresholds:\n    y_hat_IF = (prediction_IF < treshold).astype(int)\n    scores_IF.append([metrics.recall_score(y_pred=y_hat_IF, y_true=y_test),\n                 metrics.precision_score(y_pred=y_hat_IF, y_true=y_test),\n                 metrics.fbeta_score(y_pred=y_hat_IF, y_true=y_test, beta=2),\n                 metrics.cohen_kappa_score(y1=y_hat_IF, y2=y_test)])\nscores_IF=np.array(scores_IF)\na=scores_IF[:,2].argmax()\nopt_trh=tresholds[a]\ny_hat_IF = (prediction_IF < opt_trh).astype(int)\nbest_score = scores_IF[scores_IF[:, 2].argmax(),:]\nrecall_score = best_score[0]\nprecision_score = best_score[1]\nfbeta_score = best_score[2]\ncohen_kappa_score = best_score[3]\n\nprint('The recall score is\": %.3f' % recall_score)\nprint('The precision score is\": %.3f' % precision_score)\nprint('The f2 score is\": %.3f' % fbeta_score)\nprint('The Kappa score is\": %.3f' % cohen_kappa_score)\ntarget_names = ['class 0 (Normal)', 'class 1 (Fraud)']\nprint(classification_report(y_test, y_hat_IF, target_names=target_names))\n\ncm = pd.crosstab(y_test, y_hat_IF, rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\nsns.heatmap(cm, \n            xticklabels=['Not Fraud', 'Fraud'],\n            yticklabels=['Not Fraud', 'Fraud'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\nplt.title('Confusion Matrix for Gaussian Mix', fontsize=14)\nplt.show()\n","1e7f7569":"from numpy.random import seed\nseed(4)\ninput_dim = X_train_semisup.shape[1]\n\nInput_layer=Input(shape=(input_dim,))\n#encoder\nencoding_dim = 100\nfrom keras import initializers\n#init = initializers.VarianceScaling(scale=1.0, mode='fan_in', distribution='normal', seed=42)\n#encoder\nencoder=Dense(units=encoding_dim,activation='tanh', use_bias=False, \n              activity_regularizer=regularizers.l1(10e-5))(Input_layer)\nencoder=BatchNormalization()(encoder)\nencoder=Dense(units=int(encoding_dim\/2), activation='relu')(encoder)\nencoder=Dense(units=int(encoding_dim\/4), activation='relu')(encoder)\n\n#decoder\ndecoder=Dense(units=int(encoding_dim\/2), activation='relu')(encoder)\ndecoder=Dense(units=int(encoding_dim), activation='relu')(decoder)\ndecoder=Dense(units=input_dim,activation='relu')(decoder)\n\n\n#modelo\nautoencoder=Model(inputs=Input_layer,outputs=decoder)\nencodermodel=Model(inputs=Input_layer,outputs=encoder)\n\nepochs= 400\nbatch_size=128\n\n#TensorBoard=TensorBoard(log_dir='.\/logs',histogram_freq=0,write_grads=True,write_images=True)\nbest_weights_filepath = '.\/best_weights.hdf5'\n\nearlyStopping=EarlyStopping(monitor='val_loss', patience=30, verbose=1, mode='auto')\n\nsaveBestModel = ModelCheckpoint(best_weights_filepath, monitor='val_loss', \n                                verbose=1, save_best_only=True, mode='auto')\n#we use 90% of data for train and 10% for validation\nautoencoder.compile(optimizer='adam',loss='mean_squared_error',metrics=['accuracy'])\n\nhistory=autoencoder.fit(X_train_semisup[5000*0:5000*1],X_train_semisup[5000*0:5000*1],\n                    epochs=epochs,batch_size=batch_size,shuffle=False,\n                    validation_split=0.1,verbose=0, \n                    callbacks=[saveBestModel,earlyStopping]).history\nautoencoder.load_weights(best_weights_filepath)\npredictions_AE=autoencoder.predict(X_test)\n#mse is the mean squared error between the original data points and the reconstruction data points\nmse=np.mean(np.power(X_test - predictions_AE, 2), axis=1)\n","ab5af4da":"#plotting the history of the model\\qz\nplt.figure()\nplt.plot(history['loss'],label='loss')\nplt.legend()\nplt.plot(history['val_loss'],label='validation loss')\nplt.legend()\nplt.title('loss in train and validation split')","10a63b6c":"fpr, tpr, tresholds = metrics.roc_curve(y_test, mse)\nscores_AE=[]\nfor treshold in tresholds:\n    y_hat_AE = (mse > treshold).astype(int)\n    scores_AE.append([metrics.recall_score(y_pred=y_hat_AE, y_true=y_test),\n                 metrics.precision_score(y_pred=y_hat_AE, y_true=y_test),\n                 metrics.fbeta_score(y_pred=y_hat_AE, y_true=y_test, beta=2),\n                 metrics.cohen_kappa_score(y1=y_hat_AE, y2=y_test)])\n\nscores_AE = np.array(scores_AE)\nfinal_tresh = tresholds[scores_AE[:, 2].argmax()]\ny_hat_AE = (mse > final_tresh).astype(int)\nbest_score_AE = scores_AE[scores_AE[:, 2].argmax(),:]\nrecall_score_AE = best_score_AE[0]\nprecision_score_AE = best_score_AE[1]\nfbeta_score_AE = best_score_AE[2]\ncohen_kappa_score_AE = best_score_AE[3]\n\nprint('The recall score is\": %.3f' % recall_score_AE)\nprint('The precision score is\": %.3f' % precision_score_AE)\nprint('The f2 score is\": %.3f' % fbeta_score_AE)\nprint('The Kappa score is\": %.3f' % cohen_kappa_score_AE)\ntarget_names = ['class 0 (Normal)', 'class 1 (Fraud)']\nprint(classification_report(y_test, y_hat_AE, target_names=target_names))\n\ncm = pd.crosstab(y_test, y_hat_AE, rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\nsns.heatmap(cm, \n            xticklabels=['Not Fraud', 'Fraud'],\n            yticklabels=['Not Fraud', 'Fraud'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\nplt.title('Confusion Matrix for Autoencoder', fontsize=14)\nplt.show()","db9ab5e3":"\ny_test=np.array(y_test)\nmse=np.array(mse)\nerror_df=pd.DataFrame({'reconstruction_error':mse,'true_class':y_test})\nt0 = error_df.loc[error_df['true_class'] == 0]\nt1 = error_df.loc[error_df['true_class'] == 1]\nt2=final_tresh*np.ones((60000,), dtype=int)\nsns.scatterplot(data = t0['reconstruction_error'] ,label=\"Normal\")\nsns.lineplot(data=t2, color='black', label='treshold')\nsns.scatterplot(data = t1['reconstruction_error'],label=\"Fraud\").set_title('Reconstruction error')","73b720b8":"<h2>Isolation Forest <\/h2>\n\nThe main idea is that Isolation Forest explicitly identifies anomalies instead of profiling normal data points. Isolation Forest, like any tree ensemble method, is built on the basis of decision trees. \n\nThe isolation forest model is another universal approximation model of distributions. Thus, we do not need to put any restrictive assumptions on how data is distributed. This model is based on tree structures, a class of machine learning methods that has become very popular in recent years due to its intuitive simplicity and rapid training. The isolation forest model fits multiple isolation trees to the data. To build each isolation tree, we first randomly select one of the variables in the data. Then we select a random value between the maximum and minimum of this variable, which will be used to separate the data. We keep doing these random segmentations until all observations are isolated, that is, separate from the others.\n\nLet's to build the isolation forest model,similarly to what we did with gaussian mixtures.","76daa3c7":"In the graph below, we can see the difference in data point reconstruction between normal and fraud classes.","42c90a7a":"\nFor the test base, we will use the same proportion as the original base.","a4a484fd":"Let's to do a model of gaussian mix with 3 gaussians. Using a best threshold to evaluate the f2 score, we can differentiate the normal data points of fraud data points.","f265ab56":"1. <h2> Conclusion <\/h2>\n\nThe Autoencoder and Gaussian Mixture supervised learning models performed very well according to the f2 metric. Although not outperfomed the Random forest and XGBoost algorithms, the difference between the two methods was not significant.","636424c8":"<h3> Training Random Forest and XGBoost <\/h3>\n","4db61d10":"<center><h1>Supervised vs Semisupervised Learning<\/h1><\/center>\n\nSupervised and unsupervised learning techniques are the most popular types among machine learning. There is a different kind of learning, semi-supervised learning, which seeks to be a combination of both supervised and semi-supervised learning models.\n\nSemi-supervised learning is very useful in cases where the dataset is very unbalanced. The idea is to apply a model that fits very well with labels with the majority class. This model will not fit so well with labels that are fraud and by applying a threshold we can establish a condition of how adjusted a model needs to be to be or not to be fraud.\n\n<h3> Metrics <\/h3>\n\nHow this data frame is very umbalanced, the accuracy can't be useful for evaluation of the model. Here, the metrics used are:\n<ol>\n<li>,<b>Precision<\/b>: Measures the percentage of hits between observations rated positive<\/li>\n<li><b>Recall<\/b>: \nMeasures the percentage of positive observations that were correctly classified<\/li>\n<li><b>f2<\/b>: This measurement strikes a balance between sensitivity and accuracy using the harmonic mean of these metrics, givin weight 2 to the Recall\n    $$(1+\\beta^2)\\times\\frac{precision\\times recall}{\\beta^2\\times precision+recall}$$<\/li>\n    \n    <li><b>Kappa<\/b>: It is a statistical method to assess the level of agreement between two data sets.\n    $$kappa=\\frac{Acc-Acc_{e}}{1-Acc_{e}},$$ \nwhere $Acc$ is the accuracy and $Acc_e$ the observed accuracy in a random classifier<\/li>\n<\/ol>\n\n","27378d0e":"<h3>Shuffle 10 times the dataset<\/h3> \nBoth the algorithms, random forest andt XGBoost algorithm, perfomed very close, according to the f2 metric. To avoid overfitting, we will shuffle our data points another 10 times and average the metrics on each shuffle. ","abd29a73":"<h3> Robust Normalization <\/h3>","913a4887":"\nWe can see from the graph above that the distribution of fraud data points follows a very different distribution than normal in some features like V12, V14 and V17 for example. Using a gaussine mix to adjust normal labels can be a good idea indeed.","b32c1c4f":"<h2> Autoencoders <\/h2>\n\n\nLet's use a neural model called autoencoder, in which we ask a neural network to reconstruct the signal passed to it. We will ask the model to reconstruct normal data points and if we pass a fraud data point, it will have a big error which we will use to detect the fraud. In our case. The coded latent layer will have  25 neurons and since our data has 30 variables, this means that the neural network will have to learn an latent representation that condenses 30 dimensions in 25 dimens\u00e3o going through representation with 100 and 50 neurons (30-100-50-25-50-100 layers).\n\nIt's not good use all the normal datapoints of the base, because the model need learn only how to detect frauds, not create a good generalization of normal data points. \nHave little points that represent normal data points is better. \n","692f2f9b":"<h3> Fit with Random Forest <\/h3>\nTo get the confusion matrix with the best metric f2, let's predict the probability that the data points are in a given class and apply a threshold that will find the adjustment we need.","144a483f":"\nWith the confusion matrix, we can get a better view of how the model classified the data points:","ae8d34f4":"<h3> Feature Importance <\/h3>","2b4999db":"<h2> Fit with XGBoost Classifier <\/h2>\nLet's set the parameters for the model and initialize the model, \nanalogously to what we did before:","413f23f9":"<h1> Semisupervised Learning <\/h1>\n<h2> Gaussian Mixture <\/h2>\nAnalyzing the distribution of data points with respect to the normal class, we can see that the data roughly follows a normal distribution. For the sake of improved evaluation, let's assume that data points follow a mix of Gaussians. This hypothesis is not at all restrictive. Quite complex distributions can be represented by Gaussian mixtures. In fact, as long as we have enough Gaussians, any distribution can be approximated by a mixture of Gaussians.\n","8c9520ac":"<h3> Trade off Between Precision and Recall <\/h3>\n\nThere is a trade-off between precision and recall as we change the threshold. At a threshold of zero, all values are 'fraud', in which case the precision would be 1 and the recall would be the lowest possible, because in the denominator of this metric the number of false negative would be maximum. The opposite would be true for a threshold of 1: all values rated 'normal' would have recall 1, as it has no false negative, but precision would be as low as possible, as it would have the maximum number of false positive.\n\nWe will choose the threshold where the f2-score is maximum.","b6eee856":"<h3> Undersampling <\/h3>\n\nSince the dataset is extremely unbalanced (492 frudes and 284315 normal data points), we need to undersample the normal data points. With the procedure below, we will get a 9:1 ratio dataset between normal data points and fraud.","e39b5c9f":"<h2> Supervised Learning with Random Forest  and XGBoost <\/h2>\n\nRandom forest and XGBoost are great algorithms for unbalanced datasets validation. Firstly, \nlet's do a data preparation on this base.\n\n","7476c23b":"<h3> DataPrep <\/h3>\nFirstly, we will load all the required libraries.","6f3ef2d0":"<h3>","b7303863":"Setting the training and test bases:","e654251d":"The Robust normalization is a scale features using statistics that are robust to outliers and in our model perform better than Standar Scaler. Let's apply Robust normalization and shuffle the data points.\n","cdffa3c5":"Here we can see which variables influenced the model performance the most.","0567e163":"Both models performed very well, with random forest outperfomed XGBoost. We will use the result of as a benchmark \nto evaluate semi-supervised learning algorithms.","05b196ad":"Counting the data points, we have:","63f993f3":"<h3> Preparation of Data <\/h3>\n\nFor semisupervised Learning, the train dataset is 80% of the all normal data points, while the test dataset is the same than supervised learning.","da0ba3c9":"\nWe will now train or dataset, use random forest and XGBoost models, with gridsearch optimized hyperparameters."}}