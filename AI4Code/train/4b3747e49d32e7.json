{"cell_type":{"a8b53324":"code","53106508":"code","412cebcb":"code","aa7eca37":"code","2f0570bc":"code","1ed5de1b":"code","ecc2cb4a":"code","667c55e9":"code","14dbb50f":"code","073fcdff":"code","db5113db":"code","bd43c2a6":"code","66ad8438":"code","e5505455":"code","e0b6a2f0":"code","7a7920de":"code","39f89446":"code","945612e6":"markdown","4fdfa7da":"markdown","bb3c55c5":"markdown","97b6f253":"markdown","09004a7f":"markdown","2c823a6c":"markdown","21ae8240":"markdown","8df19ac5":"markdown","8e36075d":"markdown"},"source":{"a8b53324":"!pip install python-box timm pytorch-lightning==1.4.0 grad-cam ttach","53106508":"import os\nimport warnings\nfrom pprint import pprint\nfrom glob import glob\nfrom tqdm import tqdm\n\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as T\nfrom box import Box\nfrom timm import create_model\nfrom sklearn.model_selection import StratifiedKFold\nfrom torchvision.io import read_image\nfrom torch.utils.data import DataLoader, Dataset\nfrom pytorch_grad_cam import GradCAMPlusPlus\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\n\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.utilities.seed import seed_everything\nfrom pytorch_lightning import callbacks\nfrom pytorch_lightning.callbacks.progress import ProgressBarBase\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom pytorch_lightning import LightningDataModule, LightningModule\n\n\nwarnings.filterwarnings(\"ignore\")","412cebcb":"config = {'seed': 2021,\n          'root': '\/kaggle\/input\/petfinder-pawpularity-score\/', \n          'n_splits': 5,\n          'epoch': 20,\n          'trainer': {\n              'gpus': 1,\n              'accumulate_grad_batches': 1,\n              'progress_bar_refresh_rate': 1,\n              'fast_dev_run': False,\n              'num_sanity_val_steps': 0,\n              'resume_from_checkpoint': None,\n          },\n          'transform':{\n              'name': 'get_default_transforms',\n              'image_size': 224\n          },\n          'train_loader':{\n              'batch_size': 64,\n              'shuffle': True,\n              'num_workers': 4,\n              'pin_memory': False,\n              'drop_last': True,\n          },\n          'val_loader': {\n              'batch_size': 64,\n              'shuffle': False,\n              'num_workers': 4,\n              'pin_memory': False,\n              'drop_last': False\n         },\n          'model':{\n              'name': 'swin_tiny_patch4_window7_224',\n              'output_dim': 1\n          },\n          'optimizer':{\n              'name': 'optim.AdamW',\n              'params':{\n                  'lr': 1e-5\n              },\n          },\n          'scheduler':{\n              'name': 'optim.lr_scheduler.CosineAnnealingWarmRestarts',\n              'params':{\n                  'T_0': 20,\n                  'eta_min': 1e-4,\n              }\n          },\n          'loss': 'nn.BCEWithLogitsLoss',\n}\n\nconfig = Box(config)","aa7eca37":"FOLDS = [0]\ndense_features = [\n    'Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n    'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur'\n]","2f0570bc":"pprint(config)","1ed5de1b":"class PetfinderDataset(Dataset):\n    def __init__(self, df, image_size=224):\n        self._X = df[\"Id\"].values\n        self._tab = df[dense_features].values\n        self._y = None\n        if \"Pawpularity\" in df.keys():\n            self._y = df[\"Pawpularity\"].values\n        self._transform = T.Resize([image_size, image_size])\n\n    def __len__(self):\n        return len(self._X)\n\n    def __getitem__(self, idx):\n        image_path = self._X[idx]\n        image = read_image(image_path)\n        tab = self._tab[idx]\n        image = self._transform(image)\n        if self._y is not None:\n            label = self._y[idx]\n            return image, tab, label\n        return image, tab\n\nclass PetfinderDataModule(LightningDataModule):\n    def __init__(\n        self,\n        train_df,\n        val_df,\n        cfg,\n    ):\n        super().__init__()\n        self._train_df = train_df\n        self._val_df = val_df\n        self._cfg = cfg\n\n    def __create_dataset(self, train=True):\n        return (\n            PetfinderDataset(self._train_df, self._cfg.transform.image_size)\n            if train\n            else PetfinderDataset(self._val_df, self._cfg.transform.image_size)\n        )\n\n    def train_dataloader(self):\n        dataset = self.__create_dataset(True)\n        return DataLoader(dataset, **self._cfg.train_loader)\n\n    def val_dataloader(self):\n        dataset = self.__create_dataset(False)\n        return DataLoader(dataset, **self._cfg.val_loader)","ecc2cb4a":"torch.autograd.set_detect_anomaly(True)\nseed_everything(config.seed)\n\ndf = pd.read_csv(os.path.join(config.root, \"train.csv\"))\ndf[\"Id\"] = df[\"Id\"].apply(lambda x: os.path.join(config.root, \"train\", x + \".jpg\"))","667c55e9":"sample_dataloader = PetfinderDataModule(df, df, config).val_dataloader()\nimages, _, labels = iter(sample_dataloader).next()\n\nplt.figure(figsize=(12, 12))\nfor it, (image, label) in enumerate(zip(images[:16], labels[:16])):\n    plt.subplot(4, 4, it+1)\n    plt.imshow(image.permute(1, 2, 0))\n    plt.axis('off')\n    plt.title(f'Pawpularity: {int(label)}')","14dbb50f":"IMAGENET_MEAN = [0.485, 0.456, 0.406]  # RGB\nIMAGENET_STD = [0.229, 0.224, 0.225]  # RGB\n\n\ndef get_default_transforms():\n    transform = {\n        \"train\": T.Compose(\n            [\n                T.RandomHorizontalFlip(),\n                T.RandomVerticalFlip(),\n                T.RandomAffine(15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n                T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n                T.ConvertImageDtype(torch.float),\n                T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n            ]\n        ),\n        \"val\": T.Compose(\n            [\n                T.ConvertImageDtype(torch.float),\n                T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n            ]\n        ),\n    }\n    return transform\n","073fcdff":"def mixup(x: torch.Tensor, y: torch.Tensor, alpha: float = 1.0):\n    assert alpha > 0, \"alpha should be larger than 0\"\n    assert x.size(0) > 1, \"Mixup cannot be applied to a single instance.\"\n\n    lam = np.random.beta(alpha, alpha)\n    rand_index = torch.randperm(x.size()[0])\n    mixed_x = lam * x + (1 - lam) * x[rand_index, :]\n    target_a, target_b = y, y[rand_index]\n    return mixed_x, target_a, target_b, lam\n\nclass Model(pl.LightningModule):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.__build_model()\n        self._criterion = eval(self.cfg.loss)()\n        self.transform = get_default_transforms()\n        self.save_hyperparameters(cfg)\n\n    def __build_model(self):\n        self.backbone = create_model(\n            self.cfg.model.name, pretrained=True, num_classes=0, in_chans=3\n        )\n        num_features = self.backbone.num_features\n        self.fc = nn.Sequential(\n            nn.Dropout(0.5), nn.Linear(num_features, self.cfg.model.output_dim)\n        )\n        self.nn_head = nn.Sequential(\n            nn.Linear(num_features+12, (num_features+12)\/\/2),\n            nn.LayerNorm((num_features+12)\/\/2),\n            nn.ReLU(),\n            nn.Linear((num_features+12)\/\/2, (num_features+12)\/\/2),\n            nn.LayerNorm((num_features+12)\/\/2),\n            nn.ReLU(),\n            nn.Linear((num_features+12)\/\/2, self.cfg.model.output_dim)\n        )\n\n    def forward(self, x, tab_fea):\n        f = self.backbone(x)\n        cnn_out = self.fc(f)\n        f = torch.cat((f, tab_fea),dim=1)  # dim\u8fd8\u6ca1\u6539\n        head_out = self.nn_head(f)\n        return cnn_out, head_out\n\n    def training_step(self, batch, batch_idx):\n        loss, pred, labels = self.__share_step(batch, 'train')\n        return {'loss': loss, 'pred': pred, 'labels': labels}\n        \n    def validation_step(self, batch, batch_idx):\n        loss, pred, labels = self.__share_step(batch, 'val')\n        return {'pred': pred, 'labels': labels}\n    \n    def __share_step(self, batch, mode):\n        images, tab, labels = batch\n        labels = labels.float() \/ 100.0\n        images = self.transform[mode](images)\n        \n        if torch.rand(1)[0] < 0.5 and mode == 'train':\n            mix_images, target_a, target_b, lam = mixup(images, labels, alpha=0.5)\n            logits_cnn,logits_head = self.forward(images, tab)\n            logits_cnn,logits_head = logits_cnn.squeeze(1), logits_head.squeeze(1)\n            loss_cnn = self._criterion(logits_cnn, target_a) * lam + \\\n                (1 - lam) * self._criterion(logits_cnn, target_b)\n            loss_head = self._criterion(logits_head, target_a) * lam + \\\n                (1 - lam) * self._criterion(logits_head, target_b)\n            loss = (loss_cnn+loss_head)\/2 # \u53ef\u4ee5\u8c03\u8fd9\u91cc\u7684\u6bd4\u4f8b\n            \n        else:\n            logits_cnn,logits_head = self.forward(images, tab)\n            logits_cnn,logits_head = logits_cnn.squeeze(1), logits_head.squeeze(1)\n            loss_cnn = self._criterion(logits_cnn, labels)\n            loss_head = self._criterion(logits_head, labels)\n            loss = (loss_cnn+loss_head)\/2  # \u8fd9\u91cc\u4e5f\u8981\u4e00\u8d77\u8c03\n        pred = logits_cnn.sigmoid().detach().cpu() * 100.\n        labels = labels.detach().cpu() * 100.\n        return loss, pred, labels\n        \n    def training_epoch_end(self, outputs):\n        self.__share_epoch_end(outputs, 'train')\n\n    def validation_epoch_end(self, outputs):\n        self.__share_epoch_end(outputs, 'val')    \n        \n    def __share_epoch_end(self, outputs, mode):\n        preds = []\n        labels = []\n        for out in outputs:\n            pred, label = out['pred'], out['labels']\n            preds.append(pred)\n            labels.append(label)\n        preds = torch.cat(preds)\n        labels = torch.cat(labels)\n        metrics = torch.sqrt(((labels - preds) ** 2).mean())\n        self.log(f'{mode}_loss', metrics)\n    \n    def check_gradcam(self, dataloader, target_layer, target_category, reshape_transform=None):\n        cam = GradCAMPlusPlus(\n            model=self,\n            target_layer=target_layer, \n            use_cuda=self.cfg.trainer.gpus, \n            reshape_transform=reshape_transform)\n        \n        org_images, tab, labels = iter(dataloader).next()\n        cam.batch_size = len(org_images)\n        images = self.transform['val'](org_images)\n        images = images.to(self.device)\n        logits_cnn,logits_head = self.forward(images, tab)\n        logits_cnn,logits_head = logits_cnn.squeeze(1),logits_head.squeeze(1)\n        logits = (logits_cnn+logits_head)\/2\n        pred = logits.sigmoid().detach().cpu().numpy() * 100\n        labels = labels.cpu().numpy()\n        \n        grayscale_cam = cam(input_tensor=images, target_category=target_category, eigen_smooth=True)\n        org_images = org_images.detach().cpu().numpy().transpose(0, 2, 3, 1) \/ 255.\n        return org_images, grayscale_cam, pred, labels\n\n    def configure_optimizers(self):\n        optimizer = eval(self.cfg.optimizer.name)(\n            self.parameters(), **self.cfg.optimizer.params\n        )\n        scheduler = eval(self.cfg.scheduler.name)(\n            optimizer,\n            **self.cfg.scheduler.params\n        )\n        return [optimizer], [scheduler]","db5113db":"skf = StratifiedKFold(\n    n_splits=config.n_splits, shuffle=True, random_state=config.seed\n)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(df[\"Id\"], df[\"Pawpularity\"])):\n    if fold not in FOLDS: continue\n    train_df = df.loc[train_idx].reset_index(drop=True)\n    val_df = df.loc[val_idx].reset_index(drop=True)\n    datamodule = PetfinderDataModule(train_df, val_df, config)\n    model = Model(config)\n    earystopping = EarlyStopping(monitor=\"val_loss\")\n    lr_monitor = callbacks.LearningRateMonitor()\n    loss_checkpoint = callbacks.ModelCheckpoint(\n        filename=\"best_loss\",\n        monitor=\"val_loss\",\n        save_top_k=1,\n        mode=\"min\",\n        save_last=False,\n    )\n    logger = TensorBoardLogger(config.model.name)\n    \n    trainer = pl.Trainer(\n        logger=logger,\n        max_epochs=config.epoch,\n        callbacks=[lr_monitor, loss_checkpoint, earystopping],\n        **config.trainer,\n    )\n    trainer.fit(model, datamodule=datamodule)","bd43c2a6":"# gradcam reshape_transform for vit\ndef reshape_transform(tensor, height=7, width=7):\n    result = tensor.reshape(tensor.size(0),\n                            height, width, tensor.size(2))\n\n    # like in CNNs.\n    result = result.permute(0, 3, 1, 2)\n    return result","66ad8438":"# model = Model(config) \n# model.load_state_dict(torch.load(f'{config.model.name}\/default\/version_0\/checkpoints\/best_loss.ckpt')['state_dict'])\n# model = model.cuda().eval()\n# config.val_loader.batch_size = 16\n# datamodule = PetfinderDataModule(train_df, val_df, config)\n# images, grayscale_cams, preds, labels = model.check_gradcam(\n#                                             datamodule.val_dataloader(), \n#                                             target_layer=model.backbone.layers[-1].blocks[-1].norm1,\n#                                             target_category=None,\n#                                             reshape_transform=reshape_transform)","e5505455":"# plt.figure(figsize=(12, 12))\n# for it, (image, grayscale_cam, pred, label) in enumerate(zip(images, grayscale_cams, preds, labels)):\n#     plt.subplot(4, 4, it + 1)\n#     visualization = show_cam_on_image(image, grayscale_cam)\n#     plt.imshow(visualization)\n#     plt.title(f'pred: {pred:.1f} label: {label}')\n#     plt.axis('off')","e0b6a2f0":"from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n\npath = glob(f'.\/{config.model.name}\/default\/version_0\/events*')[0]\nevent_acc = EventAccumulator(path, size_guidance={'scalars': 0})\nevent_acc.Reload()\n\nscalars = {}\nfor tag in event_acc.Tags()['scalars']:\n    events = event_acc.Scalars(tag)\n    scalars[tag] = [event.value for event in events]","7a7920de":"import seaborn as sns\nsns.set()\n\nplt.figure(figsize=(16, 6))\nplt.subplot(1, 2, 1)\nplt.plot(range(len(scalars['lr-AdamW'])), scalars['lr-AdamW'])\nplt.xlabel('epoch')\nplt.ylabel('lr')\nplt.title('adamw lr')\n\nplt.subplot(1, 2, 2)\nplt.plot(range(len(scalars['train_loss'])), scalars['train_loss'], label='train_loss')\nplt.plot(range(len(scalars['val_loss'])), scalars['val_loss'], label='val_loss')\nplt.legend()\nplt.ylabel('rmse')\nplt.xlabel('epoch')\nplt.title('train\/val rmse')\nplt.show()","39f89446":"print('best_val_loss', min(scalars['val_loss']))","945612e6":"## model","4fdfa7da":"## dataset","bb3c55c5":"## visualize data","97b6f253":"# visualize result","09004a7f":"#### This notebook is copied from @phalanx notebook [https:\/\/www.kaggle.com\/phalanx\/train-swin-t-pytorch-lightning\/notebook](https:\/\/www.kaggle.com\/phalanx\/train-swin-t-pytorch-lightning\/notebook)\n\n#### Inspired by @cdeotte notebook, [https:\/\/www.kaggle.com\/cdeotte\/rapids-svr-boost-17-8](https:\/\/www.kaggle.com\/cdeotte\/rapids-svr-boost-17-8) , I tried to train a multi-task end-to-end network.\n\n#### Task1 loss based on the output of SwinT, Task2 loss based on stack of output feature map of SwinT and 12 feature in the tab.\n\nTo save my poor GPU, I only run 5 epochs of 1fold. It has not beed tested yet so I don't know how I work on LB.","2c823a6c":"# class activation map","21ae8240":"## augmentation","8df19ac5":"## train","8e36075d":"## config"}}