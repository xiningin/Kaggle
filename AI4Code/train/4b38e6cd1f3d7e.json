{"cell_type":{"efd4aaa6":"code","5b6c82e3":"code","73dcac4f":"code","93c6eb2f":"code","278de476":"code","f883a166":"code","dcaa9566":"code","6ec80082":"code","191d4aba":"code","101b5e8c":"code","2c9a9b2d":"code","b2dbc380":"code","dd66080e":"code","5c44af9a":"code","298da015":"code","ae56410c":"code","83238575":"code","884d9c6e":"code","bd071e2a":"code","150f16dd":"code","7530924a":"code","b81afefc":"code","8ea571a2":"code","3b110807":"code","03875694":"code","873378ff":"code","a26932de":"code","62dac8a3":"code","e8952de0":"code","c247e308":"code","ee1aa8cd":"code","557da26b":"code","0bc972f2":"code","fc757989":"code","d41288c6":"code","db01c378":"code","7f13de20":"code","50f289d9":"code","54706528":"code","e42dc7b8":"code","8533866a":"code","73047484":"code","1dcdbdfb":"code","8da530f1":"code","bbc2292e":"code","00c898ae":"code","060ee157":"code","f953277f":"code","e12bb9be":"code","76dd24a8":"code","2b8d7e95":"code","f50325f2":"code","0b8df778":"code","58506437":"code","913c2292":"code","490d481b":"code","6454560d":"code","7835ee51":"markdown","4c629c74":"markdown","68d6ddeb":"markdown","5d505f5e":"markdown","f6fe55c6":"markdown","a42a96c7":"markdown","b4f2d13d":"markdown","b8c6901f":"markdown","2c82d822":"markdown","f276ef4a":"markdown","b5d24175":"markdown","c3db830b":"markdown","b19c4e9b":"markdown","7e344140":"markdown","d5fc0db5":"markdown","74553b31":"markdown","026e3a2f":"markdown","62658b5e":"markdown","c20488fe":"markdown","1f08f7f7":"markdown","1180ca7d":"markdown","c384f355":"markdown","20096e28":"markdown","78c30b9b":"markdown","c806d965":"markdown","0b9259c5":"markdown","35de0c44":"markdown","40dc1906":"markdown","b7dc5bab":"markdown","76737f8f":"markdown"},"source":{"efd4aaa6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"..\/input\"))","5b6c82e3":"!unzip ..\/input\/nlp-data-corpus\/1d2261e2276cbb0257a2ed6e2f1f4320464c7c07\n!ls","73dcac4f":"!mv ad1d9c58d338e20d09ff26bcc06c4235-1d2261e2276cbb0257a2ed6e2f1f4320464c7c07 data","93c6eb2f":"!ls -alh\n!ls -alh data\/*","278de476":"data = open('data\/corpus').read()\n\nlabels, texts = [], []\n\nfor i, line in enumerate(data.split(\"\\n\")):\n  content = line.split()\n  labels.append(content[0])\n  texts.append(' '.join(content[1:]))\n\n# create a dataFrame using texts and labels\ntrainDF = pd.DataFrame()\ntrainDF['text'] = texts\ntrainDF['label'] = labels\n\ntrainDF.head()","f883a166":"from sklearn import model_selection\n\ntrain_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\nprint(train_x.shape, train_y.shape)\nprint(valid_x.shape, valid_y.shape)\nprint(train_y[:5])","dcaa9566":"# Series\u5bf9\u8c61\uff0cindex -> value\n# \u83b7\u53d6 Series \u7684 index\ntrain_x_index = train_x.index.tolist()\nvalid_x_index = valid_x.index.tolist()\n\nprint(max(train_x_index), len(train_x_index))\nprint(max(valid_x_index), len(valid_x_index))\nprint('train_x index:', sorted(train_x_index)[:15], sorted(train_x_index)[-5:])\nprint('valid_x index:', sorted(valid_x_index)[:15], sorted(valid_x_index)[-5:])","6ec80082":"print(type(train_x))\ntrain_x.head()","191d4aba":"# \u53d6 Series \u524d 2 \u4e2a\u5143\u7d20\nprint(train_x[:2], '\\n')\n\n# \u901a\u8fc7 index \u83b7\u53d6\u5143\u7d20\uff0c\u7b2c1\u4e2a\u5143\u7d20\u7684 index \u4e3a 9040\nprint(9040, train_x[9040], '\\n')\n\n# \u53d6 index \u4e3a 0 \u7684\u5143\u7d20\nprint(train_x[0], '\\n')","101b5e8c":"from sklearn import preprocessing\n\nencoder = preprocessing.LabelEncoder()\ntrain_y = encoder.fit_transform(train_y)\nvalid_y = encoder.fit_transform(valid_y)\nprint(train_y.shape)\nprint(valid_y.shape)\nprint(train_y[:10])","2c9a9b2d":"from sklearn.feature_extraction import text","b2dbc380":"count_vect = text.CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n# Learn a vocabulary dictionary of all tokens in the raw documents.\n# \u5b66\u4e60 \u539f\u59cb\u6587\u6863\u4e2d\u6240\u6709\u6807\u8bb0 \u7684\u8bcd\u6c47\u8868\ncount_vect.fit(trainDF['text'])","dd66080e":"vocab = count_vect.vocabulary_\nreverse_vocab = { idx: word for word, idx in vocab.items() }\n\n# \u9a8c\u8bc1\u8f6c\u6362\u7d22\u5f15\u8bcd\u8868\nprint(reverse_vocab[1743])\nprint(vocab[reverse_vocab[1743]])\nprint('vocab length:', len(vocab))","5c44af9a":"xtrain_count = count_vect.transform(train_x)\nxvalid_count = count_vect.transform(valid_x)\nxvalid_count","298da015":"train_x_0_1 = train_x[:10].tolist()\ndoc_id = 5\n\nprint('raw text document:\\n', train_x_0_1[doc_id])\nprint()\nprint(xtrain_count[doc_id])","ae56410c":"for word_idx, word_freq in enumerate(xtrain_count.A[doc_id]):\n    if word_freq > 0:\n        word = reverse_vocab[word_idx]  # \u4f9d\u636e\u8bcdID\u4ece\u9006\u5411\u8bcd\u5178\u4e2d\u53d6\u51fa\u8bcd\n        origin_word_freq = sum(map(lambda x: x.lower() == word, train_x_0_1[doc_id].split()))\n        if word_freq == origin_word_freq:\n            print(word, word_freq, origin_word_freq)\n        else:\n            print(word, word_freq, origin_word_freq, \n                  [w for w in train_x_0_1[doc_id].split() if w.lower().startswith(word)])","83238575":"from sklearn.feature_extraction import text","884d9c6e":"tfidf_vect = text.TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\ntfidf_vect.fit(trainDF['text'])","bd071e2a":"xtrain_tfidf = tfidf_vect.transform(train_x)\nxtrain_tfidf","150f16dd":"tfidf_vect_ngram = text.TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n                                        ngram_range=(2,3), \n                                        max_features=5000)\ntfidf_vect_ngram.fit(trainDF['text'])\nxtrain_tfidf_ngram = tfidf_vect_ngram.transform(train_x)\nxtrain_tfidf_ngram","7530924a":"tfidf_vect_ngram_chars = text.TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}',\n                                              ngram_range=(2,3), \n                                              max_features=5000)\ntfidf_vect_ngram_chars.fit(trainDF['text'])\nxtrain_tfidf_ngram_chars = tfidf_vect_ngram_chars.transform(train_x)\nxtrain_tfidf_ngram_chars","b81afefc":"wiki_news_vec_file = open('..\/input\/wikinews300d1mvec\/wiki-news-300d-1M.vec')\n\n# \u52a0\u8f7d\u9884\u8bad\u7ec3\u7684\u8bcd\u5411\u91cf\nembeddings_index = {}\nfor i, line in enumerate(wiki_news_vec_file):\n    values = line.split()\n    if i == 0:\n        print('total words:', values[0], ', embeddings length:', values[1])\n    else:\n        embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n\nprint(len(embeddings_index))","8ea571a2":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# \u5206\u8bcd\ntoken = Tokenizer()\ntoken.fit_on_texts(trainDF['text'])\n\n# \u83b7\u53d6\u8bcd\u8868\uff0c word -> word_index\nword_index = token.word_index\n# \u9006\u5411\u8bcd\u8868\nreverse_word_index ={ i: w for w, i in word_index.items()}\nprint(len(word_index))\n\n# \u6587\u6863\u5206\u8bcd\uff0c\u8bcdID\u5e8f\u5217\n# maxlen=70\uff0c\u622a\u53d6\u6587\u672c\u540e70\u4e2a\u8bcd\uff0c\u4e0d\u8db3\u7684\u5de6\u8865\u96f6\n# \u5373\u6587\u6863\u90fd\u88ab\u5bf9\u9f50\u523070\u4e2a\u8bcd\u957f\u5ea6\uff0c\u6bcf\u4e2a\u8bcd\u7528300\u7ef4\u7684\u8bcd\u5d4c\u5165\u8868\u793a\ntrain_seq_x = pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\nvalid_seq_x = pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\ntrain_seq_x.shape","3b110807":"# \u521b\u5efa token-embedding \u4e4b\u95f4\u7684\u6620\u5c04\nembedding_matrix = np.zeros(shape=(len(word_index) + 1, 300))\nfor word, index in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector","03875694":"print(embedding_matrix.shape)\nprint(len(word_index))\nword_index # word_index \u7684 0 \u53f7\u7d22\u5f15\u6ca1\u6709\u4f7f\u7528 ","873378ff":"doc_id = 1\n\nprint(train_seq_x[doc_id])\nprint([reverse_word_index[i] for i in train_seq_x[doc_id] if i > 0])\nprint(train_x.tolist()[doc_id])","a26932de":"text_seq_demo = pad_sequences(\n    token.texts_to_sequences(\n        [\n            'HP workhorse: Great Printer, but does get a bit cranky at times.',\n            'I feel I made a good choice for my photos.'\n        ]\n    ),\n    maxlen=70)\nprint(text_seq_demo)\nprint([reverse_word_index[i] for i in text_seq_demo[0] if i > 0])\nprint([reverse_word_index[i] for i in text_seq_demo[1] if i > 0])","62dac8a3":"import string\n\ntrainDF['char_count'] = trainDF['text'].apply(len)\ntrainDF['word_count'] = trainDF['text'].apply(lambda x: len(x.split()))\ntrainDF['word_density'] = trainDF['char_count'] \/ (trainDF['word_count'] + 1)\ntrainDF['punctuation_count'] = trainDF['text'].apply(\n    lambda x: len([_ for _ in x if _ in string.punctuation])\n)\ntrainDF['title_word_count'] = trainDF['text'].apply(\n    lambda x: len([wrd for wrd in x.split() if wrd.istitle()])\n)\ntrainDF['upper_case_word_count'] = trainDF['text'].apply(\n    lambda x: len([wrd for wrd in x.split() if wrd.isupper()])\n)\ntrainDF.head()","e8952de0":"len(trainDF['text'][1])","c247e308":"list(map(lambda x: len(x.split()), trainDF['text'][:2]))","ee1aa8cd":"doc_demo = trainDF['text'][4]\nprint(doc_demo)\nprint([wrd for wrd in doc_demo.split() if wrd.istitle()])\nprint([wrd for wrd in doc_demo.split() if wrd.isupper()])","557da26b":"pos_family = {\n    'noun' : ['NN','NNS','NNP','NNPS'],\n    'pron' : ['PRP','PRP$','WP','WP$'],\n    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n    'adj' :  ['JJ','JJR','JJS'],\n    'adv' : ['RB','RBR','RBS','WRB']\n}\n\nimport textblob\n\ndef check_pos_tag(x, flag):\n    cnt = 0\n    try:\n        wiki = textblob.TextBlob(x)\n        for tup in wiki.tags:\n            ppo = list(tup)[1]\n            if ppo in pos_family[flag]:\n                cnt += 1\n    except:\n        pass\n    return cnt","0bc972f2":"trainDF['noun_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'noun'))\ntrainDF['verb_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'verb'))\ntrainDF['adj_count']  = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adj'))\ntrainDF['adv_count']  = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adv'))\ntrainDF['pron_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'pron'))\ntrainDF.head()","fc757989":"from sklearn.feature_extraction import text\n\ncount_vect = text.CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n# Learn a vocabulary dictionary of all tokens in the raw documents.\n# \u5b66\u4e60 \u539f\u59cb\u6587\u6863\u4e2d\u6240\u6709\u6807\u8bb0 \u7684\u8bcd\u6c47\u8868\ncount_vect.fit(trainDF['text'])\n\nvocab = count_vect.vocabulary_\nreverse_vocab = { idx: word for word, idx in vocab.items() }\n\nxtrain_count = count_vect.transform(train_x)","d41288c6":"from sklearn import decomposition\n\n# \u8bad\u7ec3\u4e00\u4e2a LDA \u4e3b\u9898\u6a21\u578b\nlda_model = decomposition.LatentDirichletAllocation(n_components=20, \n                                                    learning_method='online', \n                                                    max_iter=20)\nlda_model","db01c378":"X_topics = lda_model.fit_transform(xtrain_count)","7f13de20":"topic_word = lda_model.components_ \nprint('topic word matrix:', topic_word.shape)  # (20, 31666)\nvocab = count_vect.get_feature_names()\nprint('vocab length:', len(vocab))\n\n\"\"\"\n\u5171\u670920\u4e2a\u4e3b\u9898\uff0c\u53d6\u6bcf\u4e2a\u4e3b\u9898\u6700\u6709\u4ee3\u8868\u6027\u7684 10 \u4e2a\u8bcd\n\"\"\"\nn_top_words = 10\ntopic_summaries = []\nfor i, topic_dist in enumerate(topic_word):\n    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n    topic_summaries.append(' '.join(topic_words))\n\nprint(len(topic_summaries))\ntopic_summaries","50f289d9":"from sklearn import metrics \n\ndef train_model(classifier, \n                feature_vector_train, label, \n                feature_vector_valid, \n                is_neural_net=False):\n    \"\"\" \u901a\u7528\u7684\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\n    \"\"\"\n    classifier.fit(feature_vector_train, label)\n    predictions = classifier.predict(feature_vector_valid)\n    if is_neural_net:\n        predictions = predictions.argmax(axis=-1)\n    return metrics.accuracy_score(predictions, valid_y)\n    ","54706528":"from sklearn import model_selection\nfrom sklearn import preprocessing\n\ntrain_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], \n                                                                      trainDF['label'])\nprint(train_x.shape, train_y.shape)\nprint(valid_x.shape, valid_y.shape)\nprint(train_y[:5])\n\n# Series labels \u8f6c\u6362\u4e3a \u6570\u503c\nencoder = preprocessing.LabelEncoder()\ntrain_y = encoder.fit_transform(train_y)\nvalid_y = encoder.fit_transform(valid_y)\nprint(train_y.shape)\nprint(valid_y.shape)\nprint(train_y[:10])","e42dc7b8":"from sklearn.feature_extraction import text\n\n# Word Count\ncount_vect = text.CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\ncount_vect.fit(trainDF['text'])\nxtrain_count = count_vect.transform(train_x)\nxvalid_count = count_vect.transform(valid_x)\n\n# TF-IDF (word level)\ntfidf_vect = text.TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\ntfidf_vect.fit(trainDF['text'])\nxtrain_tfidf = tfidf_vect.transform(train_x)\nxvalid_tfidf = tfidf_vect.transform(valid_x)\n\n# TF-IDF (word n-gram)\ntfidf_vect_ngram = text.TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n                                        ngram_range=(2,3), \n                                        max_features=5000)\ntfidf_vect_ngram.fit(trainDF['text'])\nxtrain_tfidf_ngram = tfidf_vect_ngram.transform(train_x)\nxvalid_tfidf_ngram = tfidf_vect_ngram.transform(valid_x)\n\n# TF-IDF (char n-gram)\ntfidf_vect_ngram_chars = text.TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}',\n                                              ngram_range=(2,3), \n                                              max_features=5000)\ntfidf_vect_ngram_chars.fit(trainDF['text'])\nxtrain_tfidf_ngram_chars = tfidf_vect_ngram_chars.transform(train_x)\nxvalid_tfidf_ngram_chars = tfidf_vect_ngram_chars.transform(valid_x)","8533866a":"from sklearn.naive_bayes import MultinomialNB\n\nprint('NB, Count Vectors:', \n      train_model(MultinomialNB(), xtrain_count, train_y, xvalid_count))\n\nprint('NB, Word Level TF-IDF:', \n      train_model(MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf))\n\nprint('NB, Word N-gram Level TF-IDF:', \n      train_model(MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram))\n\nprint('NB, Char N-gram Level TF-IDF:', \n      train_model(MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, \n                  xvalid_tfidf_ngram_chars))","73047484":"from sklearn.linear_model import LogisticRegression","1dcdbdfb":"print('LR, Count Vectors:', \n      train_model(LogisticRegression(), xtrain_count, train_y, xvalid_count))\n\nprint('LR, Word Level TF-IDF:', \n      train_model(LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf))\n\nprint('LR, Word N-gram Level TF-IDF:', \n      train_model(LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram))\n\nprint('LR, Char N-gram Level TF-IDF:', \n      train_model(LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, \n                  xvalid_tfidf_ngram_chars))","8da530f1":"from sklearn.svm import SVC\n\nprint('SVM, Count Vectors:', \n      train_model(SVC(), xtrain_count, train_y, xvalid_count))\n\nprint('SVM, Word Level TF-IDF:', \n      train_model(SVC(gamma=auto), xtrain_tfidf, train_y, xvalid_tfidf))\n\nprint('SVM, Word N-gram Level TF-IDF:', \n      train_model(SVC(gamma=auto), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram))\n\nprint('SVM, Char N-gram Level TF-IDF:', \n      train_model(SVC(gamma=auto), xtrain_tfidf_ngram_chars, train_y, \n                  xvalid_tfidf_ngram_chars))","bbc2292e":"from sklearn.ensemble import RandomForestClassifier\n\nprint('RF, Count Vectors:', \n      train_model(RandomForestClassifier(n_estimators=10), xtrain_count, train_y, \n                  xvalid_count))\n\nprint('RF, Word Level TF-IDF:', \n      train_model(RandomForestClassifier(n_estimators=10), xtrain_tfidf, train_y, \n                  xvalid_tfidf))\n\nprint('RF, Word N-gram Level TF-IDF:', \n      train_model(RandomForestClassifier(n_estimators=10), xtrain_tfidf_ngram, train_y, \n                  xvalid_tfidf_ngram))","00c898ae":"RandomForestClassifier(n_estimators=10)","060ee157":"from xgboost import XGBClassifier\n\nprint('Xgb, Count Vectors:', \n      train_model(XGBClassifier(), xtrain_count, train_y, \n                  xvalid_count))\n\nprint('Xgb, Word Level TF-IDF:', \n      train_model(XGBClassifier(), xtrain_tfidf, train_y, \n                  xvalid_tfidf))\n\nprint('Xgb, Word N-gram Level TF-IDF:', \n      train_model(XGBClassifier(), xtrain_tfidf_ngram, train_y, \n                  xvalid_tfidf_ngram))","f953277f":"from keras.layers import Input, Dense\nfrom keras.models import Model\nfrom keras.optimizers import Adam\n","e12bb9be":"input_size = xtrain_tfidf_ngram.shape[1]\n\ninput_layer = Input(shape=(input_size,), sparse=True)\nhidden_layer = Dense(units=100, activation='relu')(input_layer)\noutput_layer = Dense(units=1, activation='sigmoid')(hidden_layer)\n\nclassifier = Model(inputs=input_layer, outputs=output_layer)\nclassifier.summary()","76dd24a8":"classifier.compile(Adam(), loss='binary_crossentropy')\naccuracy = train_model(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, is_neural_net=True)\nprint('NN, N-gram Level IF-IDF Vectors:', accuracy)","2b8d7e95":"wiki_news_vec_file = open('..\/input\/wikinews300d1mvec\/wiki-news-300d-1M.vec')\n\n# \u52a0\u8f7d\u9884\u8bad\u7ec3\u7684\u8bcd\u5411\u91cf\nembeddings_index = {}\nfor i, line in enumerate(wiki_news_vec_file):\n    values = line.split()\n    if i == 0:\n        print('total words:', values[0], ', embeddings length:', values[1])\n    else:\n        embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n\nprint(len(embeddings_index))\n\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# \u5206\u8bcd\ntoken = Tokenizer()\ntoken.fit_on_texts(trainDF['text'])\n\n# \u83b7\u53d6\u8bcd\u8868\uff0c word -> word_index\nword_index = token.word_index\nreverse_word_index ={ i: w for w, i in word_index.items()}\n\n# \u6587\u6863\u5206\u8bcd\uff0c\u8bcdID\u5e8f\u5217\n# maxlen=70\uff0c\u622a\u53d6\u6587\u672c\u540e70\u4e2a\u8bcd\uff0c\u4e0d\u8db3\u7684\u5de6\u8865\u96f6\n# \u5373\u6587\u6863\u90fd\u88ab\u5bf9\u9f50\u523070\u4e2a\u8bcd\u957f\u5ea6\uff0c\u6bcf\u4e2a\u8bcd\u7528300\u7ef4\u7684\u8bcd\u5d4c\u5165\u8868\u793a\ntrain_seq_x = pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\nvalid_seq_x = pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n\n# \u521b\u5efa token-embedding \u4e4b\u95f4\u7684\u6620\u5c04\nembedding_matrix = np.zeros(shape=(len(word_index) + 1, 300))\nfor word, index in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector","f50325f2":"def network_train_estimate(classifier):\n    # train\n    classifier.fit(x=train_seq_x, y=train_y, batch_size=128, epochs=5, validation_data=(valid_seq_x, valid_y))\n    # predict\n    predictions = classifier.predict(valid_seq_x)\n    # score\n    score = metrics.accuracy_score(valid_y, [1 if i >= 0.5 else 0 for i in predictions])\n    return score","0b8df778":"from keras import layers, models, optimizers","58506437":"input_layer = layers.Input(shape=(70,))\nembedding_layer = layers.Embedding(input_dim=len(word_index) + 1, output_dim=300, \n                                   weights=[embedding_matrix], \n                                   trainable=False)(input_layer)\nembedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n\n# \u5377\u79ef\u5c42\u53ca\u6c60\u5316\u5c42\nconv_layer = layers.Convolution1D(filters=100, kernel_size=3, activation=\"relu\")(embedding_layer)\npooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n\n\n# Add the output Layers\noutput_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\noutput_layer1 = layers.Dropout(0.25)(output_layer1)\noutput_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n\nmodel = models.Model(inputs=input_layer, outputs=output_layer2)\nmodel.summary()\n\n# Compile the model\nmodel.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n\n\nprint('CNN, Word Embeddings:', network_train_estimate(model))","913c2292":"def create_rnn_lstm():\n    input_layer = layers.Input(shape=(70,))\n    embedding_layer = layers.Embedding(input_dim=len(word_index) + 1, output_dim=300, \n                                       weights=[embedding_matrix], \n                                       trainable=False)(input_layer)\n    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n\n    # lstm\n    lstm_layer = layers.LSTM(100)(embedding_layer)\n\n\n    # Add the output Layers\n    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n    output_layer1 = layers.Dropout(0.25)(output_layer1)\n    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n\n    model = models.Model(inputs=input_layer, outputs=output_layer2)\n    model.summary()\n    # Compile the model\n    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n    return model\n\n\nclassifier = create_rnn_lstm()\nprint('RNN-LSTM, Word Embeddings:', network_train_estimate(classifier))","490d481b":"def create_rnn_gru():\n    # Add an Input Layer\n    input_layer = layers.Input((70, ))\n\n    # Add the word embedding Layer\n    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n\n    # Add the GRU Layer\n    lstm_layer = layers.GRU(100)(embedding_layer)\n\n    # Add the output Layers\n    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n    output_layer1 = layers.Dropout(0.25)(output_layer1)\n    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n\n    # construct the model\n    model = models.Model(inputs=input_layer, outputs=output_layer2)\n    model.summary()\n    \n    # Compile the model\n    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n    \n    return model\n\nclassifier = create_rnn_gru()\nprint('RNN-GRU, Word Embeddings:', network_train_estimate(classifier))","6454560d":"def create_bidirectional_rnn():\n    # Add an Input Layer\n    input_layer = layers.Input((70, ))\n\n    # Add the word embedding Layer\n    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n\n    # Add the LSTM Layer\n    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n\n    # Add the output Layers\n    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n    output_layer1 = layers.Dropout(0.25)(output_layer1)\n    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n\n    # Construct the model\n    model = models.Model(inputs=input_layer, outputs=output_layer2)\n    model.summary()\n    # Compile the model\n    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n    \n    return model\n\nclassifier = create_bidirectional_rnn()\nprint('RNN-Bidirectional, Word Embeddings:', network_train_estimate(classifier))","7835ee51":"## \u5207\u5206\u51fa\u8bad\u7ec3\u96c6\u548c\u68c0\u9a8c\u96c6","4c629c74":"# 2 \u7279\u5f81\u5de5\u7a0b","68d6ddeb":"\u9a8c\u8bc1\u7531 texts \u751f\u6210\u7684 sequences \u80fd\u591f\u4e0e\u539ftexts\u5bf9\u5e94","5d505f5e":"## 2.5 \u4e3b\u9898\u6a21\u578b\n","f6fe55c6":"### \u9a8c\u8bc1\u8f6c\u6362\u540e\u7684\u201c\u6587\u6863-\u8bcd\u201d\u77e9\u9635\uff0c\u80fd\u591f\u4e0e\u539f\u6587\u672c\u6587\u6863\u5bf9\u5e94","a42a96c7":"\u672c\u6587\u52a8\u624b\u8fd0\u884c\u4e86\u8be5\u535a\u5ba2\u4e2d\u7684\u4ee3\u7801  \nhttps:\/\/www.analyticsvidhya.com\/blog\/2018\/04\/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python\/  \n\u8be5\u7bc7\u535a\u5ba2\u4f7f\u7528:  \n1. pandas\u5e93\u52a0\u8f7d\u6570\u636e  \n\n2. \u4f7f\u7528 sklearn \u628a\u6587\u672c\u8f6c\u6362\u6210\u5411\u91cf\n    ```python\n    sklearn.feature_extraction.text.CountVectorizer  \n        CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')  \n\n    sklearn.feature_extraction.text.TfidfVectorizer  \n        TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)  \n        TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)  \n        TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)  \n\n    vectorizer.fit(trainDF['text'])  \n    xtrain_vector = vectorizer.transform(train_x)\n    ```\nCountVectorizer \u548c TfidfVectorizer \u7684 fit \u65b9\u6cd5\u4f1a\u81ea\u52a8\u5b8c\u6210\u5bf9\u6587\u672c\u7684\u5206\u8bcd\uff0c\u5e76\u5efa\u7acb\u8bcd\u8868\u3002 trainsform \u65b9\u6cd5\uff0c\u57fa\u4e8e\u5efa\u7acb\u597d\u7684\u8bcd\u8868\uff0c\u628a\u6587\u672c\u8f6c\u6362\u4e3a\u5411\u91cf\u3002\u5411\u91cf\u7684\u957f\u5ea6\u4e3a\u8bcd\u8868\u7684\u5927\u5c0f\uff0c[_, v1, v2, v3, ..., v|V|], 0\u53f7\u7d22\u5f15\u672a\u4f7f\u7528\u3002 \n\u5411\u91cf\u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u503c\uff0c\u8868\u793a\u5176\u5bf9\u5e94\u7684\u5355\u8bcd\u5728\u8be5\u6587\u6863\u4e2d\u7684\u91cd\u8981\u7a0b\u5ea6  \n\n\n3. \u8bcd\u5d4c\u5165\u90e8\u5206\uff0c\u52a0\u8f7d\u9884\u8bad\u7ec3\u597d\u7684\u8bcd\u5411\u91cf\u6587\u4ef6\uff0c\u5206\u8bcd\u548c\u6587\u6863\u957f\u5ea6\u622a\u53d6\u4f7f\u7528 keras \u5e93\n    ```\n    keras.preprocessing.text.Tokenizer  \n    keras.preprocessing.sequence.pad_sequences  \n    ```\n    \n\n4. \u6a21\u578b\u90e8\u5206\uff0c\u4f7f\u7528 sklearn \u5e93\u4e2d\u7684\u6a21\u578b  \n    * \u6734\u7d20\u8d1d\u53f6\u65af\n    * \u903b\u8f91\u56de\u5f52\n    * \u968f\u673a\u68ee\u6797\n    * xgboost\n    * SVM, \u7b49\n    ","b4f2d13d":"## \u52a0\u8f7d\u6570\u636e\u96c6","b8c6901f":"## 2.1 CountVectorizer \u628a\u6587\u6863\u7528\u8bcd\u9891\u8868\u793a\uff0c\u5411\u91cf\u957f\u5ea6\u4e3a\u6b64\u8868\u5927\u5c0f","2c82d822":"## 3.5 Boosting Model\nxgboost","f276ef4a":"## 3.1 \u6734\u7d20\u8d1d\u53f6\u65af","b5d24175":"## 3.4 \u96c6\u6210\u6a21\u578b - \u5305\u6a21\u578b - \u968f\u673a\u68ee\u6797","c3db830b":"### \u628a\u6587\u672c\u6587\u6863\u8f6c\u6362\u4e3a\u201c\u6587\u6863-\u8bcd\u201d\u77e9\u9635","b19c4e9b":"# 3 \u6784\u5efa\u5206\u7c7b\u6a21\u578b","7e344140":"2) \u4ee5 N-gram \u4e3a\u5355\u4f4d\u63d0\u53d6 token\uff0c\u5e76\u8ba1\u7b97\u5176 tf-idf\u5f97\u5206","d5fc0db5":"## 3.6 \u6d45\u5c42\u795e\u7ecf\u7f51\u7edc\uff08\u5355\u9690\u5c42\uff09","74553b31":"## 3.2 \u7ebf\u6027\u5206\u7c7b\u5668 - \u903b\u8f91\u56de\u5f52","026e3a2f":"**CNN**","62658b5e":"Topic Modelling is a technique  \n    to identify the groups of words (called a topic) from a collection of documents  \n    that contains best information in the collection.  \n \n\nI have used Latent Dirichlet Allocation for generating Topic Modelling Features.  \n\u4f7f\u7528 LDA \u751f\u6210\u4e3b\u9898\u6a21\u578b\u7279\u5f81  \nLDA is an iterative model which starts from a fixed number of topics.  \nLDA\u662f\u4e00\u4e2a\u53ef\u8fed\u4ee3\u6a21\u578b\uff0c\u4ece\u56fa\u5b9a\u6570\u91cf\u7684\u4e3b\u9898\u5f00\u59cb  \nEach topic is represented as a distribution over words,  \nand each document is then represented as a distribution over topics.  \n\u6bcf\u4e2a\u4e3b\u9898\u88ab\u8868\u793a\u4e3a \u8bcd\u4e0a\u7684\u5206\u5e03\uff0c\u6bcf\u4e2a\u6587\u6863\u88ab\u8868\u793a\u4e3a \u4e3b\u9898\u4e0a\u7684\u5206\u5e03  \n\nAlthough the tokens themselves are meaningless,  \nthe probability distributions over words provided by the topics provide a sense of the different ideas contained in the documents.  \n\u4e3b\u9898\u63d0\u4f9b\u7684\u8bcd\u4e0a\u7684\u6982\u7387\u5206\u5e03\uff0c\u63d0\u4f9b\u4e86\u6587\u7ae0\u4e2d\u4e0d\u540c\u89c2\u70b9\u7684\u610f\u601d  \nOne can read more about topic modelling here","c20488fe":"## 2.4 \n1. Word Count of the documents \u2013 total number of words in the documents  \n\u6587\u6863\u4e2d\u7684\u5355\u8bcd\u603b\u6570\n2. Character Count of the documents \u2013 total number of characters in the documents  \n\u6587\u6863\u4e2d\u7684\u5b57\u7b26\u603b\u6570\n3. Average Word Density of the documents \u2013 average length of the words used in the documents  \n\u6587\u6863\u4e2d\u4f7f\u7528\u7684\u5355\u8bcd\u7684\u5e73\u5747\u957f\u5ea6\n4. Puncutation Count in the Complete Essay \u2013 total number of punctuation marks in the documents  \n\u6807\u70b9\u7b26\u53f7\u603b\u6570\n5. Upper Case Count in the Complete Essay \u2013 total number of upper count words in the documents  \n\u5927\u5199\u5355\u8bcd\u603b\u6570\n6. Title Word Count in the Complete Essay \u2013 total number of proper case (title) words in the documents  \n\u6807\u9898\u8bcd\u603b\u6570\n\n\n7. Frequency distribution of Part of Speech Tags:\n    * Noun Count \u540d\u8bcd\u8ba1\u6570\n    * Verb Count \u52a8\u8bcd\u8ba1\u6570\n    * Adjective Count \u5f62\u5bb9\u8bcd\u8ba1\u6570\n    * Adverb Count  \u526f\u8bcd\u8ba1\u6570\n    * Pronoun Count \u4ee3\u8bcd\u8ba1\u6570","1f08f7f7":"## \u628a\u6807\u7b7e\u8f6c\u6362\u4e3a\u6570\u5b57\u8868\u793a\u7684\u7c7b\u522b\n\u7c7b\u522b\u5e8f\u53f7\u4ece0\u5f00\u59cb\uff0c\uff080,1,..)","1180ca7d":"## 2.2 TF-IDF \u7279\u5f81\n\nTF-IDF\u5f97\u5206\uff0c\u4ee3\u8868\u6587\u6863\u4e2d\u8bcd\u7684\u76f8\u5bf9\u91cd\u8981\u6027  \n\nTF-IDF\u5411\u91cf\u53ef\u4ee5\u5728\u4e0d\u540c\u7684\u7ea7\u522b\u751f\u6210\uff1a \n1. \u8bcd\u7ea7\u522b  \n    \u77e9\u9635\u4e2d\u6bcf\u4e2a\u503c\uff0c\u4ee3\u8868\u4e86\u4e0d\u540c\u6587\u6863\u4e2d\u6bcf\u4e2a\u8bcd\u7684\u5f97\u5206\n2. N-gram \u7ea7\u522b  \n    N\u4e2a\u8bcd\u7684\u7ec4\u5408\uff0c\u4ee3\u8868 N-grams \u7684 tf-idf \u5f97\u5206\n3. \u5b57\u7b26\u7ea7\u522b  \n    \u4ee3\u8868\u5b57\u7b26\u7ea7\u522b N-grams \u7684\u5f97\u5206","c384f355":"## 2.3 \u8bcd\u5d4c\u5165 Word Embeddings","20096e28":"\u6a21\u578b\u7684\u51c6\u786e\u7387\u4e0e\u8bad\u7ec3\u96c6\u3001\u6821\u9a8c\u96c6\u7684\u5207\u5206\u6709\u5173  \ntrain_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], \n                                                                      trainDF['label'])","78c30b9b":"### \u5b66\u4e60 \u539f\u59cb\u6587\u6863\u4e2d\u6240\u6709\u6807\u8bb0 \u7684\u8bcd\u6c47\u8868","c806d965":"## 3.7 \u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc","0b9259c5":"\u8bcd\u5d4c\u5165\u662f\u8868\u793a\u8bcd\u548c\u6587\u6863\u7684\u4e00\u79cd\u5f62\u5f0f\uff0c\u4f7f\u7528\u7a20\u5bc6\u5411\u91cf\u8868\u793a\u3002  \n\u5411\u91cf\u7a7a\u95f4\u4e2d\u4e00\u4e2a\u8bcd\u7684\u4f4d\u7f6e\u662f\u4ece\u6587\u672c\u4e2d\u5b66\u6765\u7684\uff0c\u5e76\u8be5\u4f4d\u7f6e\u662f\u57fa\u4e8e\u5b83\u7684\u5468\u56f4\u8bcd\u7684  \nWord embeddings can be trained using the input corpus itself   \n\u4f7f\u7528\u8f93\u5165\u8bed\u6599\u672c\u8eab\u8fdb\u884c\u8bad\u7ec3  \nor can be generated using pre-trained word embeddings such as Glove, FastText, and Word2Vec.   \n\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u8bcd\u5d4c\u5165\u751f\u6210  \nAny one of them can be downloaded and used as transfer learning.  \n\u7528\u4f5c\u8fc1\u79fb\u5b66\u4e60","35de0c44":"## 3.3 SVM","40dc1906":"\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u8bcd\u5411\u91cf\uff1a\n1. \u52a0\u8f7d\u9884\u8bad\u7ec3\u8bcd\u5411\u91cf\n2. \u521b\u5efa\u4e00\u4e2a tokenizer \u5bf9\u8c61\uff0c\u7528\u4e8e\u5206\u8bcd\n3. \u628a\u6587\u672c\u8f6c\u6362\u4e3a tokens \u5e8f\u5217\uff0c\u5e76\u8865\u5168\u957f\u5ea6\n4. \u521b\u5efa token \u4e0e \u8bcd\u5411\u91cf \u95f4\u7684\u6620\u5c04","b7dc5bab":" 3) \u4ee5 char \u4e3a\u5355\u4f4d\u63d0\u53d6 token\uff0c\u5e76\u8ba1\u7b97\u5176 tf-idf\u5f97\u5206","76737f8f":"**RNN-LSTM**"}}