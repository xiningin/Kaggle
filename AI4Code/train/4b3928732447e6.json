{"cell_type":{"34fc406c":"code","c7b23dae":"code","593dcb67":"code","81f55127":"code","43a7ca45":"code","b464dc7c":"code","b94ea2dc":"code","41dc6957":"code","ddf4c8b2":"code","6ec95709":"code","21fa7f77":"code","74e9482e":"code","dd3580f1":"code","8b9cacac":"code","bf76d42d":"code","69291a5b":"code","0af2eb31":"code","cf6a62a9":"code","e4a6ab11":"code","e6e79538":"code","d004ea89":"code","bdccc253":"code","293ac787":"code","f9737ae4":"code","69fa091e":"code","ebaeeeaa":"code","749297ae":"code","6cac8f21":"code","93eeaff8":"code","1cc50253":"code","e16feb12":"markdown","79aa8bb9":"markdown","3b3bc3c1":"markdown","1c72bc26":"markdown","5407858b":"markdown","bc95ee6f":"markdown","c4e32b2c":"markdown","7895a2eb":"markdown","410a262f":"markdown","4080a29a":"markdown","971e4128":"markdown","a9576aca":"markdown","67b153c0":"markdown","d30adbc5":"markdown","e57356aa":"markdown","fed27250":"markdown","f59f18e8":"markdown","2606507a":"markdown","2184e700":"markdown","6650a581":"markdown","880cdb14":"markdown","4450d817":"markdown","b933128d":"markdown","d6b6bd51":"markdown","77e95c15":"markdown","51d89c36":"markdown","30f2882a":"markdown","5e367aab":"markdown","d21afe86":"markdown","d62a2e78":"markdown","03a2a841":"markdown"},"source":{"34fc406c":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\nimport nltk\nimport time\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c7b23dae":"data1 = pd.read_csv('\/kaggle\/input\/email-spam-dataset\/lingSpam.csv')\ndata2 = pd.read_csv('\/kaggle\/input\/email-spam-dataset\/enronSpamSubset.csv')\ndata3 = pd.read_csv('\/kaggle\/input\/email-spam-dataset\/completeSpamAssassin.csv')\n\ndata1.head()\n","593dcb67":"data2.head()\n","81f55127":"data3.head()","43a7ca45":"data1.drop(\"Unnamed: 0\",inplace=True,axis=1)\ndata2.drop([\"Unnamed: 0\",\"Unnamed: 0.1\"],inplace=True,axis=1)\ndata3.drop(\"Unnamed: 0\",inplace=True,axis=1)\n\ndata2.head()","b464dc7c":"data = pd.concat([data1,data2,data3],axis=0)\ndata.head()","b94ea2dc":"data.info()","41dc6957":"data.dropna(inplace=True)\ndata.info()","ddf4c8b2":"x = data[\"Body\"]\nx_clnd_link = [re.sub(r\"http\\S+\", \"\", text) for text in x]\n\nprint(x_clnd_link[0])","6ec95709":"pattern = \"[^a-zA-Z0-9]\"\n","21fa7f77":"# This means replace all the chars that follow the pattern \nx_cleaned = [re.sub(pattern,\" \",text) for text in x_clnd_link]\n","74e9482e":"x_lowered = [text.lower() for text in x_cleaned]\nprint(x_lowered[0])","dd3580f1":"x_tokenized = [nltk.word_tokenize(text) for text in x_lowered]\n","8b9cacac":"print(x_tokenized[0])","bf76d42d":"nltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\nlemma = WordNetLemmatizer()","69291a5b":"words = [\"bats\",\"removed\",\"cheers\",\"good\",\"stopped\",\"went\",\"fired\",\"cleaner\",\"beers\"]\nfor word in words:\n    print(lemma.lemmatize(word),end=\" \")","0af2eb31":"x_lemmatized = [[lemma.lemmatize(word) for word in text] for text in x_tokenized]\n","cf6a62a9":"print(x_lemmatized[0])","e4a6ab11":"stopwords = nltk.corpus.stopwords.words(\"english\")\nx_prepared = [[word for word in text if word not in stopwords] for text in x_lemmatized]","e6e79538":"print(x_prepared[0])","d004ea89":"len(np.unique([word for text in x_prepared for word in text]))\n","bdccc253":"vectorizer = CountVectorizer(max_features=20000)\nx = vectorizer.fit_transform([\" \".join(text) for text in x_prepared]).toarray()\n\n","293ac787":"x.shape","f9737ae4":"x_train,x_test,y_train,y_test = train_test_split(x,np.asarray(data[\"Label\"]),random_state=42,test_size=0.2)\nx_train.shape","69fa091e":"start_time = time.time()\nNB = GaussianNB()\nNB.fit(x_train,y_train)\nend_time = time.time()\n\nprint(round(end_time-start_time,2))","ebaeeeaa":"NB.score(x_test,y_test)","749297ae":"from sklearn.metrics import confusion_matrix\ny_pred = NB.predict(x_test)\n\nconf = confusion_matrix(y_pred=y_pred,y_true=y_test)\nimport seaborn\nseaborn.heatmap(conf,annot=True,fmt=\".1f\",linewidths=1.5)\nimport matplotlib.pyplot as plt\nplt.show()","6cac8f21":"import pickle\n# We will save count vectorizer and model\nwith open(\"model.pckl\",mode=\"wb\") as F:\n    pickle.dump(NB,F)\n    \nwith open(\"vectorizer.pckl\",mode=\"wb\") as F:\n    pickle.dump(vectorizer,F)","93eeaff8":"def predict_mail(mail):\n    \n    model = pickle.load(open(\"model.pckl\",mode=\"rb\"))\n    vectorizer = pickle.load(open(\"vectorizer.pckl\",mode=\"rb\"))\n    \n    lemma = WordNetLemmatizer()\n    \n    stopwords = nltk.corpus.stopwords.words('english')\n    \n    mail = re.sub(r\"http\\S+\", \"\", mail)\n    mail = re.sub(\"[^a-zA-Z0-9]\",\" \",mail)\n    mail = mail.lower()\n    mail = nltk.word_tokenize(mail)\n    mail = [lemma.lemmatize(word) for word in mail]\n    mail = [word for word in mail if word not in stopwords]\n    mail = \" \".join(mail)\n    \n    vector = vectorizer.transform([mail])\n    decision = model.predict(vector.toarray())\n    \n    return decision[0]\n    \n    ","1cc50253":"predict_mail(\"Flash sale: you can buy a laptop just with one dollar\")","e16feb12":"If we apply it to our work, it will be:\n* **Posterior Probability ( P(A|B) ) = Probability of data being spam and probability of data being real.**\n* **Likelihood ( P(B|A) ) = Probability of spam mails being data.**\n* **Class Prior Probability ( P(A) ) = Probability of class being spam**\n\nAnd naive bayes algorithm computes probabilities using bayes theorem. Naive Bayes algorithm is a lazy algorithm. You don't train them because they don't have something you can train (like a line or a neuron) so preparing it takes very short time.\n\nNaive Bayes computes probabilities and chooses the best probability. Although Naive Bayes is a good and fast classifier, it has a real problem: **Zero Frequency**\n\nLet's take a look at the formula again: Especially at the Likelihood. We said **probability of spam mails being data**. Data includes feature (we'll use bag of words technique, so each feature will be a word) and if a feature has a value that not occured in the data, its probability will be **zero** (impossible) There are techniques to prevent this problem but we won't need them.\n\nAnd now we can start to implement, we'll use SKlearn and in the end of this kernel I will show you how to save a sklearn model to disk.\n","79aa8bb9":"# Cleaning and Preparing Text\n\nNow let's clean and prepare the text in order to use in Naive Bayes. We will follow these stpes:\n\n* Cleaning links\n* Cleaning digits except alphabetical and numerical characters.\n* Lowering\n* Tokenizing\n* Lemmatizing and Removing Stopwords\n* Bag of Words\n","3b3bc3c1":"* Now we can lemmatize our texts.","1c72bc26":"# Conclusion\nIn this kernel I showed how to develop a spam filter model using Naive Bayes lazy ml algorithm. If you have questions in your mind, please ask in comment section. And if you liked this kernel please do upvote.\n\nHave a great day!","5407858b":"# About Bayes Theorem and Naive Bayes\nBayes Theorem is a probability theorem that explained first by **Thomas Bayes** It has a simple formula, you can understand if you know basic probability.\n\n![image.png](attachment:image.png)\n\n\n","bc95ee6f":"* Now everything is ready, we can fit our model!","c4e32b2c":"* And in order to deploy model we can define a function","7895a2eb":"* Now we can use this lemmatizer easily, let's take a look at that.","410a262f":"(This looks a bit wrong, if you can see please help me in comment section)","4080a29a":"### Removing Stopwords\nIn natural languages there are words that not have a special meaning such as **will**, it is always a tense and such as **and,or**\n\nIn order to win from time and improve the model we should remove them. There are several ways to remove them but in this kernel we'll use stopwords corpora of NLTK. There are stopwords of 11 natural language in there.","971e4128":"# Naive Bayes Model\nIn this section we'll fit naive bayes model and prepare our spam filter.","a9576aca":"# EXTRA: How to Save and Deploy The Model\nIn this extra section I will show you how to save and deploy your sklearn model using pickle.\n\nPickle is the easiest way of saving a python object","67b153c0":"* This pattern means remove everything except alphabetical and numerical digits.","d30adbc5":"* As you can see there is no link.","e57356aa":"* Each sentence turned into a list that contains words.","fed27250":"**Now let's lower the texts, I won't add a section for it because it is a familiar process from the vanilla python.**","f59f18e8":"## Lemmatizing and Removing Stopwords\nIn natural languages, words can get additional so each word can have a lot of versions, sometimes these additionals may give tips to us but in filtering spams, we don't need them\n\nThere are two ways to remove additionals: **Stemmers and Lemmatizers**\n\n### Stemmers\nStemmers are rule based weak tools, they remove additionals using rules but in natural languages everything does not follow the rules. Also It cant change tenses, for instance lemmatizers convert **learnt** into learn, stemmers don't touch them. Although stemmers are weak they are fast and although so many natural language do not have lemmatizers most of them have stemmers.\n\n### Lemmatizers\nLemmatizers uses dictionaries to remove additionals and change tenses. They work good but developing a lemmatizer is hard and needs a lot of resource, so they are rare. Also lemmatizers use dictionaries, and that causes lemmatizers being slow.\n\nIn this kernel we'll use NLTK's WordNet Lemmatizer. WordNet is a big dictionary.\n","2606507a":"* There is a missing value in the body texts, we should remove it.","2184e700":"## Cleaning Digits Except Alphabetical and Numerical Characters\nAs you can see from the text above, there are a lot of digits such as <span style=\"font-weight:bold; font-size:14px;\">* and : <\/span> They don't have a meaning, so we should remove them from the texts.\n\nIn order to clean unrelevant digits we'll use regex again.","6650a581":"* Now we can concatenate them.","880cdb14":"* We have 18000 entries with 5000 feature data and it just took 3 seconds!","4450d817":"* In these datasets spam's label is 1 and ham's label is 0","b933128d":"## Tokenizing \nIn order to create a feature that shows whether the text includes the word or not, we need to split words into lists, we can do this using **pythonString.split()** but there is a better function to do this in NLTK. \n\nLet's tokenize the texts.","d6b6bd51":"## Bag of Words\nAnd we came to the final process of this section: Bag of Words. Bag of Words is an easy approach to make sense of texts.\nIn order to explain it I'll give an example\n\n\n        =======TEXTS======           WE HAVE SOME WORD HELLO WORLD FROM PYTHON I APPLE LOVE                \n        We have some words           1   1    1    1     0    0     0    0     0   0    0\n        Hello world from Python      0   0    0    0     1    1     1    1     0   0    0\n        Hello I have some apples     0   1    1    0     1    0     0    0     1   1    0\n        I love the world             0   0    0    0     0    1     0    0     1   0    1\n   \n \nIn bag of words approach, each feature shows whether the text contains the word or not.\nNow let's do it.","77e95c15":"* We have tree datasets that contains spam and ham mails. Let's clear and concatenate them.","51d89c36":"* We've used a really goood nested list comprehension","30f2882a":"* Now let's take a look at how many unique words we have in our dataset","5e367aab":"# Introduction: Filtering Spam E-Mails\nHello community! Nowadays, I am interested in Bayes Theorem and today I decided to make a spam filter using the power of Naive Bayes. You might say <span style=\"font-size:15px;\">*power?* naive bayes is a week meachine learning algorithm. <\/span> You're wrong, if you use it true and in true field, it can be a monster.\n\nI will explain everything (implementation and basics of theorem included) So, let's start!","d21afe86":"## Cleaning Links\nAs you can predict, there are links in a mails such as: **https:\/\/google.com.tr** If we don't remove them they can cause problems (problems might be small but we don't want problems in here :D )\n\nIn order to clean links we will use regex (regular expressions) I will explain it.","d62a2e78":"* Now let's take a look at the accuracy and confusion matrix","03a2a841":"* As you can see, we trained a model in just 3 seconds and its accuracy is %82."}}