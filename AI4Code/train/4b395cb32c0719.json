{"cell_type":{"b0008546":"code","7bccc048":"code","05d379e6":"code","90f79e4b":"code","0812579d":"code","85ea1e2b":"code","838aae19":"code","b229f468":"code","3aeb1a95":"code","8490008b":"code","27aa22cf":"code","3ebf8de4":"code","06512302":"code","96313e27":"code","5794ab4f":"code","217f45be":"code","d6d07f7c":"code","403e66f1":"code","75add847":"code","8d9f1102":"code","95be3384":"code","b5b4a77c":"code","2cc094e7":"code","75437db7":"code","8029f1b8":"code","cfd38ab2":"code","46ca5690":"code","c6cbc486":"code","1ed86293":"code","975d0d40":"code","10931501":"code","e55c4840":"code","2bb22b63":"code","ed95ec9c":"code","f1aff33c":"code","bb873930":"code","982cfaa1":"code","4cba2ec6":"markdown","59d03e98":"markdown","f4349981":"markdown","bf5eb646":"markdown","4654224e":"markdown","287cf2d9":"markdown","b70e0e9d":"markdown","e75f5ab4":"markdown","c12a596d":"markdown","8d05579e":"markdown","3300bf04":"markdown","c801f3f0":"markdown","349c77ae":"markdown","410f3d38":"markdown","c246c5ff":"markdown","9b01bee8":"markdown"},"source":{"b0008546":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd, numpy as np, seaborn as sns\nimport math, json, os, random\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport keras.backend as K\n\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\nfrom sklearn.cluster import KMeans","7bccc048":"def seed_everything(seed = 34):\n    os.environ['PYTHONHASHSEED']=str(seed)\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    \nseed_everything()","05d379e6":"#get comp data\ntrain = pd.read_json('..\/input\/stanford-covid-vaccine\/train.json', lines=True)\ntest = pd.read_json('..\/input\/stanford-covid-vaccine\/test.json', lines=True)\nsample_sub = pd.read_csv('..\/input\/stanford-covid-vaccine\/sample_submission.csv')","90f79e4b":"print(train.columns)","0812579d":"#sneak peak\nprint(train.shape)\nif ~train.isnull().values.any(): print('No missing values')\ntrain.head()","85ea1e2b":"#sneak peak\nprint(test.shape)\nif ~test.isnull().values.any(): print('No missing values')\ntest.head()","838aae19":"#sneak peak\nprint(sample_sub.shape)\nif ~sample_sub.isnull().values.any(): print('No missing values')\nsample_sub.head()","b229f468":"fig, ax = plt.subplots(1, 2, figsize=(15, 5))\nsns.kdeplot(train['signal_to_noise'], shade=True, ax=ax[0])\nsns.countplot(train['SN_filter'], ax=ax[1])\n\nax[0].set_title('Signal\/Noise Distribution')\nax[1].set_title('Signal\/Noise Filter Distribution');","3aeb1a95":"print(f\"Samples with signal_to_noise greater than 1: {len(train.loc[(train['signal_to_noise'] > 1 )])}\")\nprint(f\"Samples with SN_filter = 1: {len(train.loc[(train['SN_filter'] == 1 )])}\")\nprint(f\"Samples with signal_to_noise greater than 1, but SN_filter == 0: {len(train.loc[(train['signal_to_noise'] > 1) & (train['SN_filter'] == 0)])}\")","8490008b":"def read_bpps_sum(df):\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps_arr.append(np.load(f\"..\/input\/stanford-covid-vaccine\/bpps\/{mol_id}.npy\").sum(axis=1))\n    return bpps_arr\n\ndef read_bpps_max(df):\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps_arr.append(np.load(f\"..\/input\/stanford-covid-vaccine\/bpps\/{mol_id}.npy\").max(axis=1))\n    return bpps_arr\n\ndef read_bpps_nb(df):\n    #mean and std from https:\/\/www.kaggle.com\/symyksr\/openvaccine-deepergcn \n    bpps_nb_mean = 0.077522\n    bpps_nb_std = 0.08914\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps = np.load(f\"..\/input\/stanford-covid-vaccine\/bpps\/{mol_id}.npy\")\n        bpps_nb = (bpps > 0).sum(axis=0) \/ bpps.shape[0]\n        bpps_nb = (bpps_nb - bpps_nb_mean) \/ bpps_nb_std\n        bpps_arr.append(bpps_nb)\n    return bpps_arr \n\ntrain['bpps_sum'] = read_bpps_sum(train)\ntest['bpps_sum'] = read_bpps_sum(test)\ntrain['bpps_max'] = read_bpps_max(train)\ntest['bpps_max'] = read_bpps_max(test)\ntrain['bpps_nb'] = read_bpps_nb(train)\ntest['bpps_nb'] = read_bpps_nb(test)\n\n#sanity check\ntrain.head()","27aa22cf":"fig, ax = plt.subplots(3, figsize=(15, 10))\nsns.kdeplot(np.array(train['bpps_max'].to_list()).reshape(-1),\n            color=\"Blue\", ax=ax[0], label='Train')\nsns.kdeplot(np.array(test[test['seq_length'] == 107]['bpps_max'].to_list()).reshape(-1),\n            color=\"Red\", ax=ax[0], label='Public test')\nsns.kdeplot(np.array(test[test['seq_length'] == 130]['bpps_max'].to_list()).reshape(-1),\n            color=\"Green\", ax=ax[0], label='Private test')\nsns.kdeplot(np.array(train['bpps_sum'].to_list()).reshape(-1),\n            color=\"Blue\", ax=ax[1], label='Train')\nsns.kdeplot(np.array(test[test['seq_length'] == 107]['bpps_sum'].to_list()).reshape(-1),\n            color=\"Red\", ax=ax[1], label='Public test')\nsns.kdeplot(np.array(test[test['seq_length'] == 130]['bpps_sum'].to_list()).reshape(-1),\n            color=\"Green\", ax=ax[1], label='Private test')\nsns.kdeplot(np.array(train['bpps_nb'].to_list()).reshape(-1),\n            color=\"Blue\", ax=ax[2], label='Train')\nsns.kdeplot(np.array(test[test['seq_length'] == 107]['bpps_nb'].to_list()).reshape(-1),\n            color=\"Red\", ax=ax[2], label='Public test')\nsns.kdeplot(np.array(test[test['seq_length'] == 130]['bpps_nb'].to_list()).reshape(-1),\n            color=\"Green\", ax=ax[2], label='Private test')\n\nax[0].set_title('Distribution of bpps_max')\nax[1].set_title('Distribution of bpps_sum')\nax[2].set_title('Distribution of bpps_nb')\nplt.tight_layout();","3ebf8de4":"AUGMENT=True","06512302":"aug_df = pd.read_csv('..\/input\/openvaccineaugmented\/aug_data_n2.csv')\nprint(aug_df.shape)\naug_df.head()","96313e27":"def aug_data(df):\n    target_df = df.copy()\n    new_df = aug_df[aug_df['id'].isin(target_df['id'])]\n                         \n    del target_df['structure']\n    del target_df['predicted_loop_type']\n    new_df = new_df.merge(target_df, on=['id','sequence'], how='left')\n\n    df['cnt'] = df['id'].map(new_df[['id','cnt']].set_index('id').to_dict()['cnt'])\n    df['log_gamma'] = 100\n    df['score'] = 1.0\n    df = df.append(new_df[df.columns])\n    return df","5794ab4f":"print(f\"Samples in train before augmentation: {len(train)}\")\nprint(f\"Samples in test before augmentation: {len(test)}\")\n\nif AUGMENT:\n    train = aug_data(train)\n    test = aug_data(test)\n\nprint(f\"Samples in train after augmentation: {len(train)}\")\nprint(f\"Samples in test after augmentation: {len(test)}\")\n\nprint(f\"Unique sequences in train: {len(train['sequence'].unique())}\")\nprint(f\"Unique sequences in test: {len(test['sequence'].unique())}\")","217f45be":"DENOISE = False","d6d07f7c":"target_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']","403e66f1":"token2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}","75add847":"def preprocess_inputs(df, cols=['sequence', 'structure', 'predicted_loop_type']):\n    base_fea = np.transpose(\n        np.array(\n            df[cols]\n            .applymap(lambda seq: [token2int[x] for x in seq])\n            .values\n            .tolist()\n        ),\n        (0, 2, 1)\n    )\n    bpps_sum_fea = np.array(df['bpps_sum'].to_list())[:,:,np.newaxis]\n    bpps_max_fea = np.array(df['bpps_max'].to_list())[:,:,np.newaxis]\n    return np.concatenate([base_fea,bpps_sum_fea,bpps_max_fea], 2)","8d9f1102":"if DENOISE:\n    train = train[train['signal_to_noise'] > .25]","95be3384":"len(token2int)","b5b4a77c":"# https:\/\/www.kaggle.com\/c\/stanford-covid-vaccine\/discussion\/183211\ndef rmse(y_actual, y_pred):\n    mse = tf.keras.losses.mean_squared_error(y_actual, y_pred)\n    return K.sqrt(mse)\n\ndef mcrmse(y_actual, y_pred, num_scored=len(target_cols)):\n    score = 0\n    for i in range(num_scored):\n        score += rmse(y_actual[:, :, i], y_pred[:, :, i]) \/ num_scored\n    return score","2cc094e7":"def gru_layer(hidden_dim, dropout):\n    return tf.keras.layers.Bidirectional(\n                                tf.keras.layers.GRU(hidden_dim,\n                                dropout=dropout,\n                                return_sequences=True,\n                                kernel_initializer='orthogonal'))\n\ndef lstm_layer(hidden_dim, dropout):\n    return tf.keras.layers.Bidirectional(\n                                tf.keras.layers.LSTM(hidden_dim,\n                                dropout=dropout,\n                                return_sequences=True,\n                                kernel_initializer='orthogonal'))\n\ndef build_model(rnn='gru', convolve=False, conv_dim=512, \n                dropout=.4, sp_dropout=.2, embed_dim=200,\n                hidden_dim=256, layers=3,\n                seq_len=107, pred_len=68):\n    \n###############################################\n#### Inputs\n###############################################\n\n    inputs = tf.keras.layers.Input(shape=(seq_len, 5))\n    categorical_feats = inputs[:, :, :3]\n    numerical_feats = inputs[:, :, 3:]\n\n    embed = tf.keras.layers.Embedding(input_dim=len(token2int),\n                                      output_dim=embed_dim)(categorical_feats)\n    reshaped = tf.reshape(\n        embed, shape=(-1, embed.shape[1],  embed.shape[2] * embed.shape[3]))\n    \n    reshaped = tf.keras.layers.concatenate([reshaped, numerical_feats], axis=2)\n    hidden = tf.keras.layers.SpatialDropout1D(sp_dropout)(reshaped)\n    \n    if convolve:\n        hidden = tf.keras.layers.Conv1D(conv_dim, 5, padding='same', activation=tf.keras.activations.swish)(hidden)\n\n###############################################\n#### RNN Layers\n###############################################\n\n    if rnn is 'gru':\n        for _ in range(layers):\n            hidden = gru_layer(hidden_dim, dropout)(hidden)\n        \n    elif rnn is 'lstm':\n        for _ in range(layers):\n            hidden = lstm_layer(hidden_dim, dropout)(hidden)\n\n###############################################\n#### Output\n###############################################\n\n    out = hidden[:, :pred_len]\n    out = tf.keras.layers.Dense(5, activation='linear')(out)\n    \n    model = tf.keras.Model(inputs=inputs, outputs=out)\n    adam = tf.optimizers.Adam()\n    model.compile(optimizer=adam, loss=mcrmse)\n\n    return model","75437db7":"test_model = build_model(rnn='gru')\ntest_model.summary()","8029f1b8":"def train_and_infer(rnn, STRATIFY=True, FOLDS=4, EPOCHS=50, BATCH_SIZE=64,\n                    REPEATS=3, SEED=34, VERBOSE=2):\n\n    #get test now for OOF \n    public_df = test.query(\"seq_length == 107\").copy()\n    private_df = test.query(\"seq_length == 130\").copy()\n    private_preds = np.zeros((private_df.shape[0], 130, 5))\n    public_preds = np.zeros((public_df.shape[0], 107, 5))\n    public_inputs = preprocess_inputs(public_df)\n    private_inputs = preprocess_inputs(private_df)\n\n    #to evaluate TTA effects\/post processing\n    holdouts = []\n    holdout_preds = []\n    \n    #to view learning curves\n    histories = []\n    \n    #put similar RNA in the same fold\n    gkf = GroupKFold(n_splits=FOLDS)\n    kf=KFold(n_splits=FOLDS, random_state=SEED)\n    kmeans_model = KMeans(n_clusters=200, random_state=SEED).fit(preprocess_inputs(train)[:,:,0])\n    train['cluster_id'] = kmeans_model.labels_\n\n    for _ in range(REPEATS):\n        \n        for f, (train_index, val_index) in enumerate((gkf if STRATIFY else kf).split(train,\n                train['reactivity'], train['cluster_id'] if STRATIFY else None)):\n\n            #define training callbacks\n            lr_callback = tf.keras.callbacks.ReduceLROnPlateau(patience=8, \n                                                               factor=.1,\n                                                               #min_lr=1e-5,\n                                                               verbose=VERBOSE)\n            save = tf.keras.callbacks.ModelCheckpoint(f'model-{f}.h5')\n\n            #define sample weight function\n            epsilon = .1\n            sample_weighting = np.log1p(train.iloc[train_index]['signal_to_noise'] + epsilon) \/ 2\n\n            #get train data\n            trn = train.iloc[train_index]\n            trn_ = preprocess_inputs(trn)\n            trn_labs = np.array(trn[target_cols].values.tolist()).transpose((0, 2, 1))\n\n            #get validation data\n            val = train.iloc[val_index]\n            val_all = preprocess_inputs(val)\n            val = val[val.SN_filter == 1]\n            val_ = preprocess_inputs(val)\n            val_labs = np.array(val[target_cols].values.tolist()).transpose((0, 2, 1))\n\n            #pre-build models for different sequence lengths\n            model = build_model(rnn=rnn)\n            model_short = build_model(rnn=rnn,seq_len=107, pred_len=107)\n            model_long = build_model(rnn=rnn,seq_len=130, pred_len=130)\n\n            #train model\n            history = model.fit(\n                trn_, trn_labs,\n                validation_data = (val_, val_labs),\n                batch_size=BATCH_SIZE,\n                epochs=EPOCHS,\n                sample_weight=sample_weighting,\n                callbacks=[save, lr_callback],\n                verbose=VERBOSE\n            )\n\n            histories.append(history)\n\n            #load best models\n            model.load_weights(f'model-{f}.h5')\n            model_short.load_weights(f'model-{f}.h5')\n            model_long.load_weights(f'model-{f}.h5')\n\n            holdouts.append(train.iloc[val_index])\n            holdout_preds.append(model.predict(val_all))\n\n            public_preds += model_short.predict(public_inputs) \/ (FOLDS * REPEATS)\n            private_preds += model_long.predict(private_inputs) \/ (FOLDS * REPEATS)\n        \n        del model, model_short, model_long\n        \n    return holdouts, holdout_preds, public_df, public_preds, private_df, private_preds, histories","cfd38ab2":"gru_holdouts, gru_holdout_preds, public_df, gru_public_preds, private_df, gru_private_preds, gru_histories = train_and_infer(rnn='gru')","46ca5690":"lstm_holdouts, lstm_holdout_preds, public_df, lstm_public_preds, private_df, lstm_private_preds, lstm_histories = train_and_infer(rnn='lstm')","c6cbc486":"def plot_learning_curves(results):\n\n    fig, ax = plt.subplots(1, len(results['histories']), figsize = (20, 10))\n    \n    for i, result in enumerate(results['histories']):\n        for history in result:\n            ax[i].plot(history.history['loss'], color='C0')\n            ax[i].plot(history.history['val_loss'], color='C1')\n            ax[i].set_title(f\"{results['models'][i]}\")\n            ax[i].set_ylabel('MCRMSE')\n            ax[i].set_xlabel('Epoch')\n            ax[i].legend(['train', 'validation'], loc = 'upper right')\n            \nresults = {\n            \"models\" : ['GRU', 'LSTM'],    \n            \"histories\" : [gru_histories, lstm_histories],\n            }","1ed86293":"#https:\/\/www.kaggle.com\/xhlulu\/openvaccine-simple-gru-model\ndef format_predictions(test_df, test_preds, val=False):\n    preds = []\n    \n    for df, preds_ in zip(test_df, test_preds):\n        for i, uid in enumerate(df['id']):\n            single_pred = preds_[i]\n\n            single_df = pd.DataFrame(single_pred, columns=target_cols)\n            single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n            if val: single_df['SN_filter'] = df[df['id'] == uid].SN_filter.values[0]\n\n            preds.append(single_df)\n    return pd.concat(preds).groupby('id_seqpos').mean().reset_index() if AUGMENT else pd.concat(preds)","975d0d40":"def get_error(preds):\n    val = pd.read_json('..\/input\/stanford-covid-vaccine\/train.json', lines=True)\n\n    val_data = []\n    for mol_id in val['id'].unique():\n        sample_data = val.loc[val['id'] == mol_id]\n        sample_seq_length = sample_data.seq_length.values[0]\n        for i in range(68):\n            sample_dict = {\n                           'id_seqpos' : sample_data['id'].values[0] + '_' + str(i),\n                           'reactivity_gt' : sample_data['reactivity'].values[0][i],\n                           'deg_Mg_pH10_gt' : sample_data['deg_Mg_pH10'].values[0][i],\n                           'deg_Mg_50C_gt' : sample_data['deg_Mg_50C'].values[0][i],\n                           }\n            \n            val_data.append(sample_dict)\n            \n    val_data = pd.DataFrame(val_data)\n    val_data = val_data.merge(preds, on='id_seqpos')\n\n    rmses = []\n    mses = []\n    \n    for col in ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C']:\n        rmse = ((val_data[col] - val_data[col+'_gt']) ** 2).mean() ** .5\n        mse = ((val_data[col] - val_data[col+'_gt']) ** 2).mean()\n        rmses.append(rmse)\n        mses.append(mse)\n        print(col, rmse, mse)\n    print(np.mean(rmses), np.mean(mses))\n    print('')","10931501":"plot_learning_curves(results)","e55c4840":"gru_val_preds = format_predictions(gru_holdouts, gru_holdout_preds, val=True)\nlstm_val_preds = format_predictions(lstm_holdouts, lstm_holdout_preds, val=True)\n\nprint('-'*25); print('Unfiltered training results'); print('-'*25)\nprint('GRU training results'); print('')\nget_error(gru_val_preds)\nprint('LSTM training results'); print('')\nget_error(lstm_val_preds)\nprint('-'*25); print('SN_filter == 1 training results'); print('-'*25)\nprint('GRU training results'); print('')\nget_error(gru_val_preds[gru_val_preds['SN_filter'] == 1])\nprint('LSTM training results'); print('')\nget_error(lstm_val_preds[lstm_val_preds['SN_filter'] == 1])","2bb22b63":"gru_preds = [gru_public_preds, gru_private_preds]\nlstm_preds = [gru_public_preds, gru_private_preds]\ntest_df = [public_df, private_df]\ngru_preds = format_predictions(test_df, gru_preds)\nlstm_preds = format_predictions(test_df, lstm_preds)","ed95ec9c":"gru_weight = .5\nlstm_weight = .5","f1aff33c":"blended_preds = pd.DataFrame()\nblended_preds['id_seqpos'] = gru_preds['id_seqpos']\nblended_preds['reactivity'] = gru_weight*gru_preds['reactivity'] + lstm_weight*lstm_preds['reactivity']\nblended_preds['deg_Mg_pH10'] = gru_weight*gru_preds['deg_Mg_pH10'] + lstm_weight*lstm_preds['deg_Mg_pH10']\nblended_preds['deg_pH10'] = gru_weight*gru_preds['deg_pH10'] + lstm_weight*lstm_preds['deg_pH10']\nblended_preds['deg_Mg_50C'] = gru_weight*gru_preds['deg_Mg_50C'] + lstm_weight*lstm_preds['deg_Mg_50C']\nblended_preds['deg_50C'] = gru_weight*gru_preds['deg_50C'] + lstm_weight*lstm_preds['deg_50C']","bb873930":"submission = sample_sub[['id_seqpos']].merge(blended_preds, on=['id_seqpos'])\nsubmission.head()","982cfaa1":"submission.to_csv(f'submission_new.csv', index=False)\nprint('Submission saved')","4cba2ec6":"**Let's explore these newly engineered features to see if they can be trusted (i.e., are their distributions similar across the training set and the two testing sets?)**","59d03e98":"**Now we explore `signal_to_noise` and `SN_filter` distributions. As per the data tab of this competition the samples in `test.json` have been filtered in the following way:**\n\n1. Minimum value across all 5 conditions must be greater than -0.5.\n2. Mean signal\/noise across all 5 conditions must be greater than 1.0. [Signal\/noise is defined as mean( measurement value over 68 nts )\/mean( statistical error in measurement value over 68 nts)]\n3. To help ensure sequence diversity, the resulting sequences were clustered into clusters with less than 50% sequence similarity, and the 629 test set sequences were chosen from clusters with 3 or fewer members. That is, any sequence in the test set should be sequence similar to at most 2 other sequences.","f4349981":"### Learning Curves and Evaluation","bf5eb646":"**Looks like `bpps_max` and `bpps_sum` are okay to use, but there is a large difference in the distribution of `bpps_nb` in public vs. private test sets. So even if it improves our LB (or local CV scores), we do not know if it will help with the private test score. For this reason, I will not include it in training.**","4654224e":"# Model\n\n**The below RNN architecture is adapted from the one and only [Xhlulu](https:\/\/www.kaggle.com\/xhlulu)'s notebook [here](https:\/\/www.kaggle.com\/xhlulu\/openvaccine-simple-gru-model). For his explanation of the model\/procedure, see his discussion post [here](https:\/\/www.kaggle.com\/c\/stanford-covid-vaccine\/discussion\/182303). I have made minor tweaks to some parameters and added an LSTM to experiment with blending.**\n\n**Note that for submission, the output must be the same length as the input, which is 107 for `train.json` and `test.json` and 130 for the private test set. However, this is not true for training, so training prediction sequences only need to be 68 long**\n\n**So we actually build 3 different models: one for training, one for predicting public test, and one for predicting private test set, each with different sequence lengths and prediction lengths. Luckily, we only need to train one model, save its weights, and load these weights into the other models.**\n\n**The last thing to set is the size of the embedding layer. In the context of NLP, the input dimension size of an embedding layer is the size of the vocabulary, which in our case is `len(token2int)`. The output dimension is typically the length of the pre-trained vectors you are using, like the GloVe vectors or Word2Vec vectors, which we don't have in this case, so we are free to experiment with different sizes.**","287cf2d9":"### GRU & LSTM","b70e0e9d":"# Brief EDA\n\n**From the data [description tab](https:\/\/www.kaggle.com\/c\/stanford-covid-vaccine\/data), we must predict multiple ground truths in this competition, 5 to be exact. While the submission requires all 5, only 3 are scored: `reactivity`, `deg_Mg_pH10` and `deg_Mg_50C`. It might be interesting to see how performance differs when training for all 5 predictors vs. just the 3 that are scored.**\n\n**The training features we are given are as follows:**\n\n* **id** - An arbitrary identifier for each sample.\n* **seq_scored** - (68 in Train and Public Test, 91 in Private Test) Integer value denoting the number of positions used in scoring with predicted values. This should match the length of `reactivity`, `deg_*` and `*_error_*` columns. Note that molecules used for the Private Test will be longer than those in the Train and Public Test data, so the size of this vector will be different.\n* **seq_length** - (107 in Train and Public Test, 130 in Private Test) Integer values, denotes the length of `sequence`. Note that molecules used for the Private Test will be longer than those in the Train and Public Test data, so the size of this vector will be different.\n* **sequence** - (1x107 string in Train and Public Test, 130 in Private Test) Describes the RNA sequence, a combination of `A`, `G`, `U`, and `C` for each sample. Should be 107 characters long, and the first 68 bases should correspond to the 68 positions specified in `seq_scored` (note: indexed starting at 0).\n* **structure** - (1x107 string in Train and Public Test, 130 in Private Test) An array of `(`, `)`, and `.` characters that describe whether a base is estimated to be paired or unpaired. Paired bases are denoted by opening and closing parentheses e.g. (....) means that base 0 is paired to base 5, and bases 1-4 are unpaired.\n* **reactivity** - (1x68 vector in Train and Public Test, 1x91 in Private Test) An array of floating point numbers, should have the same length as `seq_scored`. These numbers are reactivity values for the first 68 bases as denoted in `sequence`, and used to determine the likely secondary structure of the RNA sample.\n* **deg_pH10** - (1x68 vector in Train and Public Test, 1x91 in Private Test) An array of floating point numbers, should have the same length as `seq_scored`. These numbers are reactivity values for the first 68 bases as denoted in `sequence`, and used to determine the likelihood of degradation at the base\/linkage after incubating without magnesium at high pH (pH 10).\n* **deg_Mg_pH10** - (1x68 vector in Train and Public Test, 1x91 in Private Test) An array of floating point numbers, should have the same length as `seq_scored`. These numbers are reactivity values for the first 68 bases as denoted in `sequence`, and used to determine the likelihood of degradation at the base\/linkage after incubating with magnesium in high pH (pH 10).\n* **deg_50C** - (1x68 vector in Train and Public Test, 1x91 in Private Test) An array of floating point numbers, should have the same length as `seq_scored`. These numbers are reactivity values for the first 68 bases as denoted in `sequence`, and used to determine the likelihood of degradation at the base\/linkage after incubating without magnesium at high temperature (50 degrees Celsius).\n* **deg_Mg_50C** - (1x68 vector in Train and Public Test, 1x91 in Private Test) An array of floating point numbers, should have the same length as `seq_scored`. These numbers are reactivity values for the first 68 bases as denoted in `sequence`, and used to determine the likelihood of degradation at the base\/linkage after incubating with magnesium at high temperature (50 degrees Celsius).\n* **`*_error_*`** - An array of floating point numbers, should have the same length as the corresponding `reactivity` or `deg_*` columns, calculated errors in experimental values obtained in `reactivity` and `deg_*` columns.\n* **predicted_loop_type** - (1x107 string) Describes the structural context (also referred to as 'loop type')of each character in `sequence`. Loop types assigned by bpRNA from Vienna RNAfold 2 structure. From the bpRNA_documentation: S: paired \"Stem\" M: Multiloop I: Internal loop B: Bulge H: Hairpin loop E: dangling End X: eXternal loop","e75f5ab4":"# Augmentation\n\n**Augmentation code can be found in [Tito](https:\/\/www.kaggle.com\/its7171)'s notebook [here](https:\/\/www.kaggle.com\/its7171\/how-to-generate-augmentation-data). It can be used to generate augmented samples that you can use for training augmentation and test time augmentation (TTA). We are essentially generating new `structures` and `predicted_loop_types` for each `sequence` using the software that was originally used to create them (ARNIE, ViennaRNA, and bpRNA).**","c12a596d":"# Competition Overview\n\n**In this [new competition](https:\/\/www.kaggle.com\/c\/stanford-covid-vaccine\/overview) we are helping to fight against the worldwide pandemic COVID-19. mRNA vaccines are the fastest vaccine candidates to treat COVID-19 but they currently facing several limitations. In particular, it is a challenge to design stable messenger RNA molecules. Typical vaccines are packaged in syringes and shipped under refrigeration around the world, but that is not possible for mRNA vaccines (currently).**\n\n**Researches have noticed that RNA molecules tend to spontaneously degrade, which is highly problematic because a single cut can render mRNA vaccines useless. Not much is known about which part of the backbone of a particular RNA is most susceptible to being damaged.**\n\n**Without this knowledge, the current mRNA vaccines are shopped under intense refrigeration and are unlikely to reach enough humans unless they can be stabilized. This is our task as Kagglers: we must create a model to predict the most likely degradation rates at each base of an RNA molecule.**\n\n**We are given a subset of an Eterna dataset comprised of over 3000 RNA molecules and their degradation rates at each position. Our models are then tested on the new generation of RNA sequences that were just created by Eterna players for COVID-19 mRNA vaccines**\n\n**Before we get started, please check out [Xhlulu](https:\/\/www.kaggle.com\/xhlulu)'s notebook [here](https:\/\/www.kaggle.com\/xhlulu\/openvaccine-simple-gru-model) as this one is based on it: I just added comments, made minor code changes, an LSTM, and fold training:**","8d05579e":"# Version Changes\n\n**Version 10:**\n\n* added competition metric, as inspired by [Xhlulu](https:\/\/www.kaggle.com\/xhlulu)'s discussion post [here](https:\/\/www.kaggle.com\/c\/stanford-covid-vaccine\/discussion\/183211)\n* removed filtering (no `SN_filter == 1` constraint)\n* added kfold stratification by `SN_filter`\n\n**Version 11 (and V12; V11 failed to commit):**\n\n* changed repeats from 1 to 3\n* dropped all samples where `signal_to_noise < 1` as per [this discussion post](https:\/\/www.kaggle.com\/c\/stanford-covid-vaccine\/discussion\/183992)\n* cleaned up some code\n\n**Version 13:**\n\n* made models larger - `embed_dim = 200`, `hidden_dim = 256` and consequently lowered training epochs to 75\n\n**Version 14:**\n* added feature engineering and augmentation from [Tito](https:\/\/www.kaggle.com\/its7171)'s incredible kernel [here](https:\/\/www.kaggle.com\/its7171\/gru-lstm-with-feature-engineering-and-augmentation) (check it out, it is fantastic work!)\n* included all samples in training, but added sample weighting by `signal_to_noise`, as inspired (again) by Tito's notebook above\n* only validated against samples with `SN_filter == 1`\n\n**Version 15\/16\/17\/18:**\n* removed `bpps_nb` feature from training\n* added GroupKFold to put similar RNA into the same fold (another of Tito's ideas)\n* cleaned up some more code, updated some comments\n* accidentally trained two GRUs, thanks for spotting that @junkoda\n\n\n**Update 9\/28\/2020:**\n\nAs this competition is entering its last week, this will be the final version of this notebook. I wanted to clean up some more code and add some last minute improvements for those that perhaps reference this notebook during the next week. This notebook received far more attention than it deserved. It is nothing without [Xhlulu](https:\/\/www.kaggle.com\/xhlulu)'s kernel [here](https:\/\/www.kaggle.com\/xhlulu\/openvaccine-simple-gru-model) and his contributions to the dicussion forums over the past few weeks. If you give this notebook an upvote, please give Xhlulu's one as well (and Tito's). Good luck to everyone over the next week.","3300bf04":"**It seems we also have a `signal_to_noise` and a `SN_filter` column. These columns control the 'quality' of samples, and as such are important training hyperparameters. We will explore them shortly:**","c801f3f0":"# Feature Engineering\n\n**Check out [Tito](https:\/\/www.kaggle.com\/its7171)'s kernel [here](https:\/\/www.kaggle.com\/its7171\/gru-lstm-with-feature-engineering-and-augmentation) for the feature engineering code below. The `bpps` folder contains Base Pairing Probabilities matrices for each sequence. These matrices give the probability that each pair of nucleotides in the RNA forms a base pair. Each matrix is a symmetric square matrix the same length as the sequence. For a complete EDA of the `bpps` folder, see this notebook [here](https:\/\/www.kaggle.com\/hidehisaarai1213\/openvaccine-checkout-bpps?scriptVersionId=42460013).**","349c77ae":"# Processing","410f3d38":"# Submission","c246c5ff":"# KFold Training and Inference\n\n**In previous commits, I either filtered by `SN_filter == 1` or with `signal_to_noise > 1`. But it seems that these RNN models generalize better when exposed to the noisier samples in the dataset. If you review the `tf.keras` [documentation](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/Model), you can see that you can pass a Numpy array of weights during training used to weight the loss function. So we can weight samples with higher `signal_to_noise` values more during training than the noisier samples. As inspired by [Tito](https:\/\/www.kaggle.com\/its7171), we will pass the following array to `sample_weight`: `np.log1p(train.signal_to_noise + epsilon)\/2` where epsilon is a small number to ensure we don't get `log(1)` for any weights.**\n\n**But since the competition hosts have said [here](https:\/\/www.kaggle.com\/c\/stanford-covid-vaccine\/discussion\/183992), the public and private test sets only contain samples where `SN_filter == 1`, so we ought to validate against the such samples as well:**","9b01bee8":"**Update: as per [this discussion post](https:\/\/www.kaggle.com\/c\/stanford-covid-vaccine\/discussion\/183992), both public *and* private test datasets are now filtered with the same 3 above conditions.**"}}