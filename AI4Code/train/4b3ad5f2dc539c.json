{"cell_type":{"31178940":"code","ccd9d5b0":"code","ef27a852":"code","03714403":"code","6086508b":"code","c59a87cb":"code","a2f6a8ca":"code","000d3afd":"code","d2fa75f0":"code","865a3063":"code","3ef77ac9":"code","0b1db2a0":"code","aeb5d148":"code","369577f1":"code","2a96f27f":"code","e62ef6d2":"code","842bfcd9":"code","7fbd642a":"code","ce940b04":"code","566ed03f":"code","81586678":"code","fa0258c0":"code","04c0ef72":"code","d0590da0":"code","9b7c5935":"code","9d4d846c":"code","002d835e":"code","f9c0aea5":"code","1bc97cde":"code","fe120734":"code","e15c77d2":"code","e97795e5":"code","d6439804":"code","0aaa8fc7":"code","4c57474a":"code","ad2314b7":"markdown","94c283e4":"markdown","6a21518f":"markdown","49057023":"markdown","bf644bf3":"markdown","e3f8b09b":"markdown","81e317a8":"markdown","c449debc":"markdown","e9dc873a":"markdown","8aaf3eb2":"markdown","32649eed":"markdown","04e20c45":"markdown","f069d766":"markdown","49e118cd":"markdown","555e0e62":"markdown","8ace5151":"markdown","88481ec7":"markdown","62b972b1":"markdown","3d458d65":"markdown","2764e67e":"markdown","9b80edbc":"markdown","4f3a86c9":"markdown","a3036b14":"markdown","6f33578b":"markdown"},"source":{"31178940":"### install package \n!pip install ..\/input\/pytorchtabnet\/pytorch_tabnet-3.1.1-py3-none-any.whl","ccd9d5b0":"#importing libraries \nimport os \nimport torch\nfrom math import sqrt\nimport pandas as pd  \nimport numpy  as np  \nimport matplotlib.pyplot as plt # ploting\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nimport torch.optim as optim\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')","ef27a852":"## prepare the data  \ntrain_data = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv')\ntest_data = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/test.csv')\nsample     = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')","03714403":"#quick look at the train data\ntrain_data.head()\ntrain_data.tail()","6086508b":"print(f'Number of rows: {train_data.shape[0]};  Number of columns: {train_data.shape[1]}; No of missing values: {sum(train_data.isna().sum())}')","c59a87cb":"print('Info about train data: ')\ntrain_data.info()","a2f6a8ca":"train_data.describe().T.style.bar().background_gradient(cmap='coolwarm')","000d3afd":"# Correlationmatrix\ncorrMatrix =train_data.corr(method='pearson', min_periods=1)\ncorrMatrix \n","d2fa75f0":"cor_targ = train_data.corrwith(train_data[\"loss\"])\n","865a3063":"# variables variaition \ntrain_data.var()\n#Standard  deviation  \ntrain_data.std()","3ef77ac9":"print('loss column basic statistics:')\ntrain_data['loss'].describe()","0b1db2a0":"print('percentage of each loss value:')\npercent_value = pd.DataFrame(train_data['loss'].value_counts()\/len(train_data))\npercent_value","aeb5d148":"# visualization \ncountplt, ax = plt.subplots(figsize = (10,7))\nax =sns.countplot(train_data['loss'])","369577f1":"test_data.head() # head \ntest_data.tail() # tail","2a96f27f":"print(f'Number of rows: {test_data.shape[0]};  Number of columns: {test_data.shape[1]}; No of missing values: {sum(test_data.isna().sum())}')","e62ef6d2":"print('Info about test data: ')\ntest_data.info()","842bfcd9":"test_data.describe().T.style.bar().background_gradient(cmap='coolwarm')","7fbd642a":"features = train_data.iloc[:,1:101]\ni = 1\nplt.figure()\nfig, ax = plt.subplots(8,6,figsize=(24, 24))\nfor feature in features:\n    plt.subplot(17, 6,i)\n    sns.distplot(train_data[feature],color=\"blue\", kde=True,bins=120, label='train')\n    sns.distplot(test_data[feature],color=\"green\", kde=True,bins=120, label='test')\n    plt.xlabel(feature, fontsize=9); plt.legend()\n    i += 1\nplt.show()","ce940b04":"feat = train_data.iloc[:,1:101] \ntarg=train_data[\"loss\"]","566ed03f":"var = VarianceThreshold(threshold=0.8) #threshold of 0.8 you can try other values \nvar = var.fit(feat,targ)","81586678":"cols = var.get_support(indices=True)\ncols.shape # from 100 to 63 features , you can start from here  ...","fa0258c0":"# correlations only with the target variable\ncor_target = abs(corrMatrix['loss'])\n\n#Select correlations with a correlation above a threshold\nfeat_cor = cor_target[cor_target>0.01]\nfeat_cor.shape # 36 feat cool ! ","04c0ef72":"# data normalization \nX_tr =  train_data.iloc[:,1:101]\nX_ts  = test_data.drop (['id'], axis=1)\nnorm = MinMaxScaler().fit(X_tr)\n\n# transform training data\nX_train_norm = norm.transform(X_tr)\n\n# transform testing data\nX_test_norm = norm.transform(X_ts)","d0590da0":"# converting to df \nX_train_normal = pd.DataFrame(X_train_norm)\nX_train_normal.describe()\nX_test_normal = pd.DataFrame(X_test_norm)\n### adding loss column back \nX_train_normal[\"loss\"] = train_data[\"loss\"]\nX_train_normal.shape","9b7c5935":"# copy of datasets\nX_train_stand = X_tr.copy()\nX_test_stand = X_ts.copy()\nnum_cols = X_tr.columns\nfor i in num_cols:\n    scale = StandardScaler().fit(X_train_stand[[i]])\n    X_train_stand[i] = scale.transform(X_train_stand[[i]])\n    X_test_stand[i] = scale.transform(X_test_stand[[i]])\n    \n# adding loss column back \nX_train_stand[\"loss\"] = train_data[\"loss\"]","9d4d846c":"target = 'loss'\nignore_id = 'id'\nfeatures = [ col for col in train_data.columns if col not in [ignore_id]+[target]]","002d835e":"X = train_data[[*features]]\ny= train_data[\"loss\"]\nX_test = test_data[[*features]]\nprint(X.shape, X_test.shape)","f9c0aea5":"X  = X.to_numpy()  \ny = y.to_numpy().reshape(-1, 1) \nX_test = X_test.to_numpy()","1bc97cde":"\n#y = np.log1p(train_data[\"loss\"])\n","fe120734":"# model definition \nmax_epochs = 1000\nBs = 2048 \nclf = TabNetRegressor(  verbose = 0,\n                       optimizer_fn=torch.optim.Adam,\n                       optimizer_params=dict(lr=2e-2),\n                       scheduler_params={\"step_size\":10, # how to use learning rate scheduler\n                                         \"gamma\":0.9},\n                       scheduler_fn=torch.optim.lr_scheduler.StepLR,\n                       mask_type='sparsemax'\n                           )","e15c77d2":"# training ,validating and predicting \nkf = KFold(n_splits=7, random_state=42, shuffle=True)\npredictions_array =[]\nCV_score_array    =[]\nfor train_index, test_index in kf.split(X):\n    X_train, X_valid = X[train_index], X[test_index]\n    y_train, y_valid = y[train_index], y[test_index]\n    clf.fit( \n        X_train=X_train, y_train=y_train,\n        eval_set=[(X_valid, y_valid)],\n        eval_metric=['rmse'],\n        max_epochs=max_epochs,\n        patience=50,\n        batch_size=Bs, virtual_batch_size=128,\n        num_workers=0,\n        drop_last=False\n           )\n    CV_score_array.append(clf.best_cost)\n    predictions_array.append(clf.predict(X_test))\npredictions = np.mean(predictions_array,axis=0)","e97795e5":"print(f\"BEST VALID SCORE  : {clf.best_cost}\") # validation score \n","d6439804":"feat_importances = clf.feature_importances_\nindices = np.argsort(feat_importances)","0aaa8fc7":"plt.figure(figsize=(22,24))\nplt.title(\"Feature importances\")\nplt.barh(range(len(feat_importances)), feat_importances[indices],\n       color=\"darkblue\", align=\"center\")\nplt.yticks(range(len(feat_importances)), [features[idx] for idx in indices])\nplt.ylim([-1, len(feat_importances)])\nplt.show()","4c57474a":"# creat submission\n#predictions = pd.DataFrame(predictions) # from array to dataframe\nsample['loss'] = predictions\nsample.to_csv('tabnet_submission.csv',index=False)\nsample\n\n\n","ad2314b7":"## Tabnet Implimentation  \n","94c283e4":"**Data preparation**","6a21518f":"**References**\n* [Pytorch_tabnet](https:\/\/github.com\/dreamquark-ai\/tabnet)\n* [TabNet on AI Platform](https:\/\/cloud.google.com\/blog\/products\/ai-machine-learning\/ml-model-tabnet-is-easy-to-use-on-cloud-ai-platform)\n* [tabnet baseline ](https:\/\/www.kaggle.com\/optimo\/tabnet-baseline)","49057023":"### Basic Feature Selection \nWe will try to reduce the number of features based on their variation and their correlation with the target.","bf644bf3":"Features from train and test data have almost identical distribution...","e3f8b09b":"#### Basic summary statistic for test data \nbasic statistics for each variables in the test data  which contain information on count, mean, standard deviation, minimum, median ,1st quartile and  3rd quartile and maximum.\n","81e317a8":"***Creat submission***","c449debc":"#### Target Column\nThe target [\"loss\"] for this competition  which we are predicting is is an integer value.","e9dc873a":"**Data Normalization** ","8aaf3eb2":"#### Basic summary statistic\nbasic statistics for each variables in the train data  which contain information on count, mean, standard deviation, minimum, median ,1st quartile and  3rd quartile and maximum.\n","32649eed":"**Quick look at the Test dataset**","04e20c45":"## About the Notebook  \nThis notebook is for the implimentation of TabNet for TPS Aug , it uses the  pyTorch implementation of Tabnet which can be found  in this [GitHub repository](http:\/\/https:\/\/github.com\/dreamquark-ai\/tabnet).\nYou can also check the origin paper [TabNet: Attentive Interpretable Tabular Learning](https:\/\/arxiv.org\/pdf\/1908.07442.pdf) . \n![tabnet architecture .PNG](attachment:3313cd97-eb53-4024-bbcb-0a72dcc5f924.PNG)\n","f069d766":"**Model Definition and Fitting**","49e118cd":"95 columns are in a float4 type while 7 are in int64 type. ","555e0e62":"**Data standardization**","8ace5151":"The test dataset has 150000 of rows with 101 of columns and there is 0 missing values. we can continue.. ","88481ec7":"you can use filtred features or Standarized \/ normalized data and see if it can improve your score ","62b972b1":"**Global explainability**\n","3d458d65":"<div class=\"alert alert-secondary\" role=\"alert\">\n  <center> <h1> TabNet implimentation for Tabular-playground-serie <\/h1> <\/center>  \n<\/div>\n","2764e67e":"### Basic EDA to better know the data\n","9b80edbc":"**Remove features which are not correlated with the response variable**","4f3a86c9":"The train dataset has 250000 of rows with 102 of columns and there is 0 missing values. we can continue.. ","a3036b14":"**Remove features with low variance** \n\n","6f33578b":"## Observations \n* This is just a baseline submission over which a lot of improvement can be made including model parameters tuning.\n* There is always need for Feature Engineering and Selection so you should concentrate on that \n* Even if tabnet doesn't achive Achieve a SOTA Result in this problem , it's output is a great condidate for stacking and ensembling techniques.\n* I would really appreaciate any Feedback  from you. \n"}}