{"cell_type":{"19141a66":"code","05add148":"code","1015ac19":"code","86a41f07":"code","64f04cd2":"code","622c32a7":"code","34d19860":"code","f46e4fb3":"code","34495f23":"code","ac4dfc96":"code","70bc771a":"code","6d67eb0a":"code","af56225a":"code","1430e1e5":"code","3fd3fa9c":"code","e2fcc28f":"code","fef86c0e":"code","af37046c":"code","fe3dbe43":"code","8023f062":"code","31e5b7ce":"code","284478ad":"code","b3fd408e":"code","40cb13f9":"code","5682cc70":"code","742b68bd":"code","7c21f72c":"code","07ecf3ea":"code","8826c1c2":"code","4b8c42ac":"code","fe2b565e":"code","52370a72":"code","04f81076":"code","0c06a4b2":"markdown","698cfc93":"markdown","bb9f212a":"markdown","45dee202":"markdown","07596db9":"markdown","fa0face9":"markdown","db746172":"markdown","ee14cd91":"markdown","b9c940bb":"markdown","d34dbec3":"markdown","87b7c9c5":"markdown","6f1408c1":"markdown","5ecfbe98":"markdown","b20d503e":"markdown","4d44c3d6":"markdown","26e8cb4c":"markdown","af0968b8":"markdown","dfe8664b":"markdown","82adbcec":"markdown","d85abd11":"markdown","bb6cc13b":"markdown"},"source":{"19141a66":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","05add148":"df_train = pd.read_csv('\/kaggle\/input\/30-days-kfolds\/training_folds.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv')","1015ac19":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","86a41f07":"df_train.head()","64f04cd2":"df_test.head()","622c32a7":"print(df_train.shape)\nprint(df_test.shape)","34d19860":"df_train.info()","f46e4fb3":"df_test.info()","34495f23":"df_train.isnull().sum()","ac4dfc96":"df_test.isnull().sum()","70bc771a":"df_train.describe(include='all')","6d67eb0a":"df_test.describe(include='all')","af56225a":"numerical_features = [feature for feature in df_train.columns if df_train[feature].dtypes=='float64']\nprint('Number of Numerical Features:',len(numerical_features))\nprint('Following are the Numerical Features:')\nprint(numerical_features)","1430e1e5":"df_train[numerical_features].head()","3fd3fa9c":"#First let's observe our target variable\nsns.distplot(df_train['target'])\nplt.title('Target Distribution')\nplt.show()","e2fcc28f":"sns.boxplot(y=df_train['target'])\nplt.title('Target Spread')\nplt.show()","fef86c0e":"print(df_train['target'].describe(percentiles = [0.25,0.5,0.75,0.85,0.9,1]))","af37046c":"#Let's see the distribution of these features and scatterplots with respect to target variable.\nfor feature in numerical_features:\n    plt.figure(figsize=(15,10))\n    plt.subplot(1,2,1)\n    sns.distplot(df_train[feature])\n    plt.xlabel(feature)\n    plt.subplot(1,2,2)\n    plt.scatter(df_train[feature],df_train['target'])\n    plt.xlabel(feature)\n    plt.ylabel('Target')\n    plt.show()","fe3dbe43":"cat_features = [feature for feature in df_train.columns if df_train[feature].dtypes=='object']\nprint('Number of categorical features:',len(cat_features))\nprint('Following are our categorical features:')\nprint(cat_features)","8023f062":"df_train[cat_features].head()","31e5b7ce":"#Let's see how many categories are present in each feature.\nfor feature in cat_features:\n    print(feature,'has:',len(df_train[feature].unique()),'categories')","284478ad":"#Let's look at each category in the features.\nfor feature in cat_features:\n    print(df_train[feature].value_counts())\n    print('--------------------------')","b3fd408e":"#Let's use barplots for a better visualization.\nfor feature in cat_features:\n    plt.figure(figsize=(15,10))\n    df_train[feature].value_counts().sort_values(ascending=False).plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('Count')\n    plt.show()","40cb13f9":"for feature in numerical_features:\n    plt.figure(figsize=(15,10))\n    sns.boxplot(y=df_train[feature])\n    plt.xlabel(feature)\n    plt.show()\n","5682cc70":"features = ['cont0','cont6','cont8','target']\nfor feature in features:\n    IQR = df_train[feature].quantile(0.75) - df_train[feature].quantile(0.25)\n    Lower_Bound = df_train[feature].quantile(0.25) - (3 * IQR)\n    Upper_Bound = df_train[feature].quantile(0.75) + (3 * IQR)\n    print(feature,'has outliers when',feature,'is less than {} and more than {}'.format(Lower_Bound,Upper_Bound))","742b68bd":"features = ['cont0','cont6','cont8']\nfor feature in features:\n    IQR = df_test[feature].quantile(0.75) - df_test[feature].quantile(0.25)\n    Lower_Bound = df_test[feature].quantile(0.25) - (3 * IQR)\n    Upper_Bound = df_test[feature].quantile(0.75) + (3 * IQR)\n    print(feature,'has outliers when',feature,'is less than {} and more than {}'.format(Lower_Bound,Upper_Bound))","7c21f72c":"#Let's see the correlation among the features using heatmap.\nplt.figure(figsize=(20,20))\ncorr_matrix = df_train.drop(['id','kfold'],axis=1).corr()\nsns.heatmap(corr_matrix,annot=True,cmap='RdYlGn')\nplt.show()","07ecf3ea":"#To take care of outliers we'll implement top-coding method and cap the maximum values to neglect the affect of outliers\n#I took this peice of code from this kaggle notebook: https:\/\/www.kaggle.com\/prashant111\/extensive-analysis-eda-fe-modelling\ndef maximum_value(df,feature,max_value): #We've discovered max_value to be considered as an outlier in the above outlier analysis\n    return np.where(df[feature]>max_value,max_value,df[feature]) #Wherever the datapoint is greater than max_value replace it with max_value else leave it be\nfor dataframe in [df_train]:\n    dataframe['cont0'] = maximum_value(dataframe,'cont0',1.45)\n    dataframe['cont6'] = maximum_value(dataframe,'cont6',1.26)\n    dataframe['cont8'] = maximum_value(dataframe,'cont8',1.42)\n    dataframe['target'] = maximum_value(dataframe,'target',11.68)\n\ndef minimum_value(df,feature,min_value): #We've discovered min_value to be considered as an outlier in the above outlier analysis\n    return np.where(df[feature]<min_value,min_value,df[feature]) #Wherever the datapoint is less than min_value replace it with min_value else leave it be\nfor dataframe in [df_train]:\n    dataframe['cont0'] = minimum_value(dataframe,'cont0',-0.38)\n    dataframe['cont6'] = minimum_value(dataframe,'cont6',-0.34)\n    dataframe['cont8'] = minimum_value(dataframe,'cont8',-0.48)\n    dataframe['target'] = minimum_value(dataframe,'target',4.78)","8826c1c2":"#Similarly for the test dataset.\ndef maximum_value(df,feature,max_value): #We've discovered max_value to be considered as an outlier in the above outlier analysis\n    return np.where(df[feature]>max_value,max_value,df[feature]) #Wherever the datapoint is greater than max_value replace it with max_value else leave it be\nfor dataframe in [df_test]:\n    dataframe['cont0'] = maximum_value(dataframe,'cont0',1.45)\n    dataframe['cont6'] = maximum_value(dataframe,'cont6',1.26)\n    dataframe['cont8'] = maximum_value(dataframe,'cont8',1.42)\n\ndef minimum_value(df,feature,min_value): #We've discovered min_value to be considered as an outlier in the above outlier analysis\n    return np.where(df[feature]<min_value,min_value,df[feature]) #Wherever the datapoint is less than min_value replace it with min_value else leave it be\nfor dataframe in [df_test]:\n    dataframe['cont0'] = minimum_value(dataframe,'cont0',-0.38)\n    dataframe['cont6'] = minimum_value(dataframe,'cont6',-0.35)\n    dataframe['cont8'] = minimum_value(dataframe,'cont8',-0.48)","4b8c42ac":"#print(xgb_random.best_score_)","fe2b565e":"#print(xgb_random.best_params_)","52370a72":"from sklearn.preprocessing import OrdinalEncoder\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nnumerical_cols = ['cont0','cont1','cont2','cont3','cont4','cont5','cont6','cont7','cont8','cont9','cont10','cont11','cont12','cont13']\ncategorical_cols = ['cat0','cat1','cat2','cat3','cat4','cat5','cat6','cat7','cat8','cat9']\n\npredictions = list()\nscores = list()\n\nfor fold in range(5):\n    X_train = df_train[df_train['kfold']!=fold].reset_index(drop=True)\n    X_val = df_train[df_train['kfold']==fold].reset_index(drop=True)\n    \n    X_test = df_test.copy()\n    y_train = X_train['target']\n    y_val = X_val['target']\n    X_train = X_train[categorical_cols+numerical_cols]\n    X_val = X_val[categorical_cols+numerical_cols]\n    \n    encoder = OrdinalEncoder()\n    X_train[categorical_cols] = encoder.fit_transform(X_train[categorical_cols])\n    X_val[categorical_cols] = encoder.transform(X_val[categorical_cols])\n    X_test[categorical_cols] = encoder.transform(X_test[categorical_cols])\n    \n    model = XGBRegressor(random_state=fold,\n                         tree_method = 'gpu_hist',\n                         gpu_id = 0,\n                         predictor = 'gpu_predictor',\n                         n_estimators = 1500,\n                         learning_rate = 0.15,\n                         max_depth = 2,\n                         min_child_weight = 3,\n                         reg_alpha = 30,\n                         colsample_bytree = 0.7,\n                         subsample = 0.6,\n                         eval_metric = 'rmse'\n                        )\n    model.fit(X_train,y_train)\n    pred_val = model.predict(X_val)\n    pred_test = model.predict(X_test.drop('id',axis=1))\n    predictions.append(pred_test)\n    rmse = mean_squared_error(y_val,pred_val,squared=False) #squared=False would calculate rmse. It is True by default which calculates mse.\n    \n    print(fold,rmse)\n    scores.append(rmse)\n    \nprint('Mean RMSE:',np.mean(scores))\nprint('STD:',np.std(scores))","04f81076":"#Mean of all predictions\nfinal_predictions = np.mean(np.column_stack(predictions),axis=1)\nmy_submission = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/sample_submission.csv\")\nmy_submission['target'] = final_predictions\nmy_submission.to_csv('Submission.csv',index=False)\nprint('Your Submission was successfully saved!')","0c06a4b2":"### 2.1 Numerical Features","698cfc93":"**Observations:** We can see quite a bit of outliers are present in some of the features in particular, cont0, cont6, cont8 and target.Let's see them using IQR.","bb9f212a":"### 3.1 Handling Outliers","45dee202":"**Observations:** Some of the categorical features have significant imbalance with respect to each category present in them.","07596db9":"### 2.2 Categorical Features","fa0face9":"### 2.3 Outlier Analysis","db746172":"Credits: Thanks [Abhishek Thakur](https:\/\/www.kaggle.com\/abhishek) for the tutorial on creating Kfolds. Learnt a lot from you.","ee14cd91":"**Observations:** 85% of the target values are less than 9 and the remaining 15% are between 9 to 10.5","b9c940bb":"**It is good that we have no missing values in both training and testing datasets!**","d34dbec3":"### 3.2 Base Model","87b7c9c5":"**Observations:** The plot is skewed, indicating most of the targt values range between 6-10. ","6f1408c1":"xgb_random = RandomizedSearchCV(estimator=xgb_model,\n                               param_distributions=hyperparameter_grid,\n                               scoring = 'neg_mean_squared_error',\n                               n_iter = 10,\n                               cv = 5,\n                               verbose = False,\n                               random_state = 0,\n                               n_jobs = -1\n                               )\nxgb_random.fit(X_train,y_train)","5ecfbe98":"## 2. Exploratory Data Analysis","b20d503e":"### 3.3 Hyperparameter Optimization - RandomizedSearchCV","4d44c3d6":"### 3.4 Final Model","26e8cb4c":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import cross_val_score\n\nxgb_model = XGBRegressor(random_state=0,tree_method='gpu_hist',gpu_id=0,predictor='gpu_predictor')\nxgb_model.fit(X_train,y_train)\nhyperparameter_grid = {'n_estimators':[10,50,300,750,1200,1300,1500],\n                       'max_depth':[2,3,5,10,15],\n                       'learning_rate':[0.05,0.1,0.15,0.2],\n                       'min_child_weight':[1,2,3,4],\n                       'colsample_bytree':[0.5,0.6,0.7,0.8,0.9,1],\n                       'reg_alpha':[5,15,30,40,50],\n                       'subsample':[0.5,0.6,0.7,0.8,0.9,1]   \n                      }\nprint(hyperparameter_grid)","af0968b8":"## 1. Import Basic Libraries","dfe8664b":"from sklearn.preprocessing import OrdinalEncoder\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nnumerical_cols = ['cont0','cont1','cont2','cont3','cont4','cont5','cont6','cont7','cont8','cont9','cont10','cont11','cont12','cont13']\ncategorical_cols = ['cat0','cat1','cat2','cat3','cat4','cat5','cat6','cat7','cat8','cat9']\n\npredictions = list()\nscores = list()\n\nfor fold in range(5):\n    X_train = df_train[df_train['kfold']!=fold].reset_index(drop=True)\n    X_val = df_train[df_train['kfold']==fold].reset_index(drop=True)\n    \n    X_test = df_test.copy()\n    y_train = X_train['target']\n    y_val = X_val['target']\n    X_train = X_train[categorical_cols+numerical_cols]\n    X_val = X_val[categorical_cols+numerical_cols]\n    \n    encoder = OrdinalEncoder()\n    X_train[categorical_cols] = encoder.fit_transform(X_train[categorical_cols])\n    X_val[categorical_cols] = encoder.transform(X_val[categorical_cols])\n    X_test[categorical_cols] = encoder.transform(X_test[categorical_cols])\n    \n    model = XGBRegressor(random_state=fold,\n                         tree_method = 'gpu_hist',\n                         gpu_id = 0,\n                         predictor = 'gpu_predictor'\n                        )\n    model.fit(X_train,y_train)\n    pred_val = model.predict(X_val)\n    pred_test = model.predict(X_test.drop('id',axis=1))\n    predictions.append(pred_test)\n    rmse = mean_squared_error(y_val,pred_val,squared=False) #squared=False would calculate rmse. It is True by default which calculates mse.\n    \n    print(fold,rmse)\n    scores.append(rmse)\n    \nprint('Mean RMSE:',np.mean(scores))\nprint('STD:',np.std(scores))","82adbcec":"**Observations:** Features have a very similar distribution. We cannot use a linear model here.","d85abd11":"### 2.4 Correlation Heatmap","bb6cc13b":"## 3. Feature Engineering + Model Building"}}