{"cell_type":{"44366cae":"code","4b7bbc08":"code","9a19a1f1":"code","3d2b2743":"code","a9cfbb64":"code","1bb16b8c":"code","3f853c3a":"code","7eb06f56":"code","2a5eef6f":"code","9073034a":"code","b4cc9318":"code","a750dce4":"code","943030c8":"code","f0274e6a":"code","8f9e2c7d":"code","bf56d978":"code","0e7b2bfa":"code","a4943366":"code","c91f444f":"code","2ec6665a":"code","9ffb7a16":"code","6b6911d7":"code","bfc54673":"code","f55215b1":"code","6e62a77f":"code","b0e36a30":"code","13ad564e":"code","5b838f6d":"code","dd5aa872":"code","7d210a0b":"code","2c0074e9":"code","0eb4d508":"code","d06f22f8":"code","f1ac4d47":"code","6d62b647":"code","f24d2c32":"code","17a3e8d5":"code","5e92d82e":"code","2787324c":"code","5a84918c":"code","14d5963c":"code","5519b2a7":"code","f5c3644c":"code","d9057f4d":"code","8ccfd035":"code","2e08dc19":"code","f6a53033":"code","ee43db30":"code","f3e88de6":"code","95620135":"code","547d015d":"code","848b0e61":"code","7a8466c3":"code","e02a797a":"code","8aebadbd":"code","611dc865":"code","387bb2a4":"markdown","14e52003":"markdown","3da941ea":"markdown","64ff6813":"markdown","0d115fa9":"markdown","51f24bd1":"markdown","876e018f":"markdown","1c2c599b":"markdown","884fdb29":"markdown","c7a1f83b":"markdown","45a745d8":"markdown","22413e4d":"markdown","638ecb8a":"markdown","863d58fb":"markdown","d94edba1":"markdown","0dd560fd":"markdown","aa3369a6":"markdown","e15bc259":"markdown","dab4a164":"markdown","f5cbb261":"markdown","bf7f3c65":"markdown","c46eddcc":"markdown","6a7b1cfb":"markdown","00983680":"markdown","24594dd1":"markdown","c4582cf4":"markdown","0f2da3fc":"markdown","dd5b62b3":"markdown","192f5764":"markdown","4683f1b1":"markdown","2c1aca2a":"markdown","948248d7":"markdown","abfb966d":"markdown","a31f601a":"markdown"},"source":{"44366cae":"# Ignoring unnecessory warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")  \n# Specialized container datatypes\nimport collections\n# For data vizualization \nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n# For large and multi-dimensional arrays\nimport numpy as np\n# For data manipulation and analysis\nimport pandas as pd\n# Natural language processing library\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.stem import SnowballStemmer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.util import ngrams\n# For basic cleaning and data preprocessing \nimport re\nimport string \n# Machine learning libary\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\n# For wordcloud generating \nfrom wordcloud import WordCloud","4b7bbc08":"DATAPATH = '..\/input\/sms-spam-collection-dataset\/spam.csv'\ndf = pd.read_csv(DATAPATH, encoding='latin')\ndf.info()","9a19a1f1":"# Shape of our data\ndf.columns","3d2b2743":"df.head()","a9cfbb64":"df.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'], axis=1, inplace=True)\ndf.rename(columns = {'v1':'label','v2':'message'}, inplace=True)\ndf.dropna(inplace=True)","1bb16b8c":"df.head()","3f853c3a":"df['label'].value_counts().plot(kind = 'barh', color = ['blue','red'], figsize = (8, 6))\nplt.title('Horizontal Bar Chart for Data Distribution', fontsize = 20)\nplt.ylabel('Spam vs Ham')\nplt.xlabel('Number of messages')\nplt.show()","7eb06f56":"df['label'].value_counts().plot(kind = 'pie', colors = ['blue','red'], explode = [0, 0.1], figsize = (6, 6), autopct = '%1.1f%%', shadow = True)\nplt.title('Pie Chart for Data Distribution', fontsize = 20)\nplt.ylabel('Spam vs Ham')\nplt.show()","2a5eef6f":"df.groupby('label').describe()","9073034a":"# Pick at random a ham sms \ndf.loc[df['label'] == 'ham'].sample()","b4cc9318":"# Pick at random a spam sms \ndf.loc[df['label'] == 'spam'].sample()","a750dce4":"cv = CountVectorizer()\nwords = cv.fit_transform(df.message)\n\nsum_words = words.sum(axis=0)\n\nwords_freq = [(word, sum_words[0, i]) for word, i in cv.vocabulary_.items()]\nwords_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n\nfrequency = pd.DataFrame(words_freq, columns=['word', 'freq'])\n\nfrequency.head(30).plot(x='word', y='freq', kind='bar', figsize=(15, 7), color = 'orange')\nplt.title(\"Most Frequently Occuring Words - Top 30\")","943030c8":"# First define a function to find all words (excluding numbers and stopwords) of each category\ndef getWords(label):\n    temp_words = ' '.join(list(df.loc[df['label'] == label]['message'])) \n    lst_words = []\n    words = [word.lower() for word in word_tokenize(temp_words) \n             if word.lower() not in stopwords.words(\"english\") and word.lower().isalpha()]\n    lst_words = lst_words + words\n    return lst_words","f0274e6a":"# Get both spam and ham words\nspam_words = getWords('spam')\nham_words = getWords('ham')","8f9e2c7d":"def generate_wordcloud(words):\n    # exclude stop words \n    wordcloud = WordCloud(max_words=1000,width=840, height=540).generate(words)\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()","bf56d978":"# generate wordcloud for 'spam' sms\ngenerate_wordcloud(' '.join(spam_words))","0e7b2bfa":"# generate wordcloud for 'ham' sms\ngenerate_wordcloud(' '.join(ham_words))","a4943366":"# 20 most frequent spam words\nCounter = collections.Counter(spam_words)\nmost_occur_spam_words = Counter.most_common(20)\ndf_most_occur_spam_words = pd.DataFrame(most_occur_spam_words, columns=['word','frequency'])\ndf_most_occur_spam_words","c91f444f":"df_most_occur_spam_words.plot(x='word', y='frequency', kind='bar', figsize=(15, 7), color = 'red')","2ec6665a":"# 20 most frequent ham words\nCounter = collections.Counter(ham_words)\nmost_occur_ham_words = Counter.most_common(20)\ndf_most_occur_ham_words = pd.DataFrame(most_occur_ham_words, columns=['word','frequency'])\ndf_most_occur_ham_words","9ffb7a16":"df_most_occur_ham_words.plot(x='word', y='frequency', kind='bar', figsize=(15, 7), color = 'blue')","6b6911d7":"# top 20 bigrams in spam_words\nspam_bigrams = ngrams(spam_words, 2)\nspam_bigrams_freq = collections.Counter(spam_bigrams)\nmost_freq_spam_bigrams = spam_bigrams_freq.most_common(20)\ndf_most_freq_spam_bigrams = pd.DataFrame(most_freq_spam_bigrams, columns=['bigram','frequency'])\ndf_most_freq_spam_bigrams","bfc54673":"df_most_freq_spam_bigrams.plot(x='bigram', y='frequency', kind='bar', figsize=(15, 7), color = 'red')","f55215b1":"# top 20 bigrams in ham_words\nham_bigrams = ngrams(ham_words, 2)\nham_bigrams_freq = collections.Counter(ham_bigrams)\nmost_freq_ham_bigrams = ham_bigrams_freq.most_common(20)\ndf_most_freq_ham_bigrams = pd.DataFrame(most_freq_ham_bigrams, columns=['bigram','frequency'])\ndf_most_freq_ham_bigrams","6e62a77f":"df_most_freq_ham_bigrams.plot(x='bigram', y='frequency', kind='bar', figsize=(15, 7), color = 'blue')","b0e36a30":"def preprocess_text(text):\n    # remove all punctuation\n    text = re.sub(r'[^\\w\\d\\s]', ' ', text)\n    # collapse all white spaces\n    text = re.sub(r'\\s+', ' ', text)\n    # convert to lower case\n    text = re.sub(r'^\\s+|\\s+?$', '', text.lower())\n    # remove stop words and perform stemming\n    stop_words = nltk.corpus.stopwords.words('english')\n    lemmatizer = WordNetLemmatizer() \n    return ' '.join(\n        lemmatizer.lemmatize(term) \n        for term in text.split()\n        if term not in set(stop_words)\n    )\n    ","13ad564e":"df['processed_text'] = df.message.apply(lambda row : preprocess_text(row))\ndf.head()","5b838f6d":"tfidf_vec = TfidfVectorizer(ngram_range=(1, 2)).fit_transform(df.processed_text)\ntfidf_data = pd.DataFrame(tfidf_vec.toarray())\ntfidf_data.head()","dd5aa872":"df['message_length'] = df.message.apply(lambda row : len(row))\ndf.head()","7d210a0b":"sns.violinplot(df['message_length'], df['label'])\nplt.title('Distribution of message length')\nplt.show()","2c0074e9":"df.hist(column='message_length', by='label', bins=50, figsize=(12,4))","0eb4d508":"def count_punct(text):\n    count = sum([1 for char in text if char in string.punctuation])\n    return round(count\/(len(text)), 3)*100","d06f22f8":"df['punct%'] = df.message.apply(lambda row : count_punct(row))\ndf.head()","f1ac4d47":"sns.violinplot(df['punct%'], df['label'])\nplt.title('Distribution of punctuation')\nplt.show()","6d62b647":"df.hist(column='punct%', by='label', bins=50, figsize=(12,4))","f24d2c32":"def count_https(text):\n    http_regex = re.compile(r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)')\n    count = sum([1 for word in text.split() if http_regex.match(word)])\n    return round(count\/(len(text)), 3)*100","17a3e8d5":"df['httpaddr%'] = df.message.apply(lambda row : count_https(row))\ndf.head()","5e92d82e":"sns.violinplot(df['httpaddr%'], df['label'])\nplt.title('Distribution of https')\nplt.show()","2787324c":"df.hist(column='httpaddr%', by='label', bins=50, figsize=(12,4))","5a84918c":"def count_money_symb(text):\n    money_symb = ['\u00a3','$','\u20ac']\n    count = sum([1 for char in text if char in money_symb])\n    return round(count\/(len(text)), 3)*100","14d5963c":"df['money_symb%'] = df.message.apply(lambda row : count_money_symb(row))\ndf.head()","5519b2a7":"sns.violinplot(df['money_symb%'], df['label'])\nplt.title('Distribution of money symbols')\nplt.show()","f5c3644c":"df.hist(column='money_symb%', by='label', bins=50, figsize=(12,4))","d9057f4d":"def count_phone_numbers(text):\n    phone_regex = re.compile(r'\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b')\n    count = sum([1 for word in text.split() if phone_regex.match(word)])\n    return round(count\/(len(text)), 3)*100","8ccfd035":"df['phone_numb%'] = df.message.apply(lambda row : count_phone_numbers(row))\ndf.head()","2e08dc19":"sns.violinplot(df['phone_numb%'], df['label'])\nplt.title('Distribution of phone numbers')\nplt.show()","f6a53033":"df.hist(column='phone_numb%', by='label', bins=50, figsize=(12,4))","ee43db30":"def count_numbers(text):\n    count = sum([1 for word in text.split() if word.isdigit()])\n    return round(count\/(len(text)), 3)*100","f3e88de6":"df['numbers%'] = df.message.apply(lambda row : count_numbers(row))\ndf.head()","95620135":"sns.violinplot(df['numbers%'], df['label'])\nplt.title('Distribution of numbers')\nplt.show()","547d015d":"df.hist(column='numbers%', by='label', bins=50, figsize=(12,4))","848b0e61":"final_data = pd.concat([df['money_symb%'], df['phone_numb%'], tfidf_data], axis=1)\nfinal_data.head()","7a8466c3":"X_train, X_test, y_train, y_test = train_test_split(final_data, df['label'], test_size=.2)","e02a797a":"clf = MultinomialNB(alpha=0.2)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint(classification_report(y_test,y_pred))","8aebadbd":"print(\"Accuracy score: {}\".format(round(accuracy_score(y_test,y_pred),3)))","611dc865":"pd.DataFrame(\n    confusion_matrix(y_test, y_pred),\n    index=[['actual', 'actual'], ['spam', 'ham']],\n    columns=[['predicted', 'predicted'], ['spam', 'ham']]\n)","387bb2a4":"Have a look on the first 5 rows of the dataset\n","14e52003":"#### Http address","3da941ea":"As we see, only phone number and money symbol are trustworthy indicators of a spam sms. So our final data will contain money_symb%, phone_numb% and tfidf_data","64ff6813":"Let's use a confusion matrix to take a peek at what types of mistakes the classifier is making.","0d115fa9":"Target count for data\n","51f24bd1":"It's crystal clear that 'ham' sms are 6.5 times more than the 'spam' ones. \n- We have to be very careful when we gonna split our data set into train and test set or when we gonna use cross-validation. Otherwise we have a chance of our training model being skewed towards normal messages. That's will happen if the sample we choose to train our model consists majorly of 'ham' sms. In this case, it's very propable to end up predicting everything as 'ham'.\n- Also precision is very important as we don't want to predict any 'ham' sms as 'spam'. Actually, we don't mind if we miss any odd 'spam' sms.","876e018f":"#### Numbers ","1c2c599b":"Read the data using pandas' read_csv method and let's look at the dataset info to see if everything is alright","884fdb29":"We have now a data with 38084 columns where each column represent a different 1\/2-gram contained in the current message","c7a1f83b":"Now define our text precessing function. It will remove any punctuation and stopwords. Also it will convert all letters to lowercase and perform stemming aswell.","45a745d8":"In many cases bigrams is also an important tool in data analysis. So let's see if there some words which occur more frequently than the other ones for each category","22413e4d":"#### Phone number","638ecb8a":"#### Money symbol","863d58fb":"As we have seen from the EDA step, both 1-gram and 2-gram  play a significant role when it comes to classify a message as spam or ham. So our role is to transform these 1-gram and 2-gram into a feature vector. To do that, we gonna use TfidfVectorizer","d94edba1":"Vizualize data distribution using pie chart\n","0dd560fd":"Let's have a look into the data grouped by into labels 'ham' or 'spam'\n","aa3369a6":"Split data set into train and test set ","e15bc259":"## Exploratory Data Analysis (EDA)\n\n","dab4a164":"As we see words like 'free', 'text', 'call', 'reply', 'mobile' and 'now' appear very often in 'spam' sms. Respectively the most frequent words in 'ham' sms are 'ok', 'will', 'now', 'got', 'gt', 'lt' and etc","f5cbb261":"Now drop \"unnamed\" columns and rename v1 and v2 to \"label\" and \"message\" respectively.  Also drop any row which has any NaN value","bf7f3c65":"## Import Libraries\n","c46eddcc":"## Training and evaluating the model","6a7b1cfb":"Fit classifier and make predictions on the test set","00983680":"Let's see the first 5 rows of the dataset again","24594dd1":"It's time to generate wordclouds for both 'spam' and 'ham' sms to have a rough estimate of the words that has the highest frequency in the data","c4582cf4":"Let's get more precise and find out what are the 20 most frequent words in each category","0f2da3fc":"# SMS Spam Detection using Naive Bayes classifier along with sophisticated features (0.99% accuracy achieved)","dd5b62b3":"#### Message length","192f5764":"Let's see how a ham and a spam sms looks like","4683f1b1":"## Preprocessing ","2c1aca2a":"## Feature Engineering","948248d7":"Goal of this notebook is to test Multinomial Naive-Bayes classifier in combination with a plethora of sophisticated features and see how it perform on the given dataset","abfb966d":"#### Punctuation ","a31f601a":"Now let's see 30 most frequent occuring words "}}