{"cell_type":{"f8291b2b":"code","16fe019c":"code","19677453":"code","6aa4e388":"code","3a5df068":"code","04aa3e30":"markdown","dd895ad3":"markdown","1db50918":"markdown","3028b00b":"markdown","d6b076ce":"markdown","4e188eff":"markdown"},"source":{"f8291b2b":"# Import data\nimport matplotlib.pyplot as plt\nimport pandas as pd \nimport re\n\ndata = pd.read_csv('\/kaggle\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')","16fe019c":"# Preprocessing\nfrom nltk.stem import PorterStemmer\n\n# Cleansing\ndef clean_text(string):\n    string = re.sub(pattern= '<[^>]*>', repl='', string=string)   # remove tags\n    string = re.sub(pattern= '[^\\w\\s]',  repl='', string=string)  # remove symbols\n    return string\n\n# Stemming\ndef stemming_text(string):\n    s = PorterStemmer()\n    string = string.split()\n    string = \" \".join([s.stem(w) for w in string])\n    return string\n\ndef category(v):\n    if v =='positive': \n        return 1\n    else:\n        return 0\n\n# Preprocessing\nfor i,d in enumerate(data['review']):\n    string = d.lower()\n    string = clean_text(string)\n    #string = stemming_text(string)   \/\/Removing stemming shows better performance\n    data.iloc[i, 0] =  string\n","19677453":"import re\n\nword_dict = {}\n\nnumber = 0 \nfor d in data['review']:\n    sent = d.split(' ')\n    for w in sent:\n        if w in word_dict:\n            word_dict[w][1] +=1\n        else:\n            word_dict[w] = [number, 1]\n            number+=1\n        \n        \n\n###################################################################\n# New Hyper parameters\nS = len(data)\nlow  = 0.05*S\nhigh = 1.0*S\n\n# Make Smaller Dictionary\n###################################################################\nword_dict2 = {}\nnumber = 0\nfor w in word_dict:\n    if low < word_dict[w][1] < high:\n        word_dict2[w] = word_dict[w]\n        word_dict2[w][0] = number\n        number+=1\n\nprint(\"The number of total words : \", len(word_dict))\nprint(\"The number of new   words : \", len(word_dict2))","6aa4e388":"import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef category(v):\n    if v =='positive': \n        return 1\n    else:\n        return 0\n\ny = np.array(list(map(category, data['sentiment'])))\nx = np.zeros((len(data), len(word_dict2)))\n\n# Make X\nfor i, d in enumerate(data['review']):\n    sent = d.split(' ')\n    for w in sent:\n        try:\n            index  = word_dict2[w][0]\n            x[i, index] +=1   \n        except:\n            pass\n\n        \n# Normalize\nscaler = MinMaxScaler()\nscaler.fit(x)\nx = scaler.transform(x)\nprint(f\"Shape X:{x.shape}\")\nprint(f\"Shape y:{y.shape}\")","3a5df068":"# Models \nfrom sklearn.svm import LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=.3,random_state=0)\n\n\nsvc = LinearSVC(random_state=0, tol=1e-5, verbose=0)\ngaunb = GaussianNB()\ndt = tree.DecisionTreeClassifier()\nrf = RandomForestClassifier()\n\n# Training \n\n\nsvc.fit(X_train, y_train).decision_function(X_test)\nprint(f\"SVC Accuracy: {accuracy_score(svc.predict(X_test),y_test)}\")\ngaunb.fit(X_train, y_train)\nprint(f\"GNB Accuracy: {accuracy_score(gaunb.predict(X_test),y_test)}\")\ndt.fit(X_train, y_train)\nprint(f\"DT Accuracy: {accuracy_score(dt.predict(X_test),y_test)}\")\nrf.fit(X_train, y_train)\nprint(f\"RF Accuracy: {accuracy_score(rf.predict(X_test),y_test)}\")","04aa3e30":"# Word count and accuracy\n\nEvery day people write something with mobile phones or laptops and most of the content people produce is not numbers, but words.\n\nRecent studies have been conducted on the text and there are many resources such as blogs, twitter or even reviews. In general, \n\nit is not easy to create data from the text for machine learning training. A single sentence is composed of many embedded words for the sentence. \n\nHowever, the size of training data grows with the embedded words. This paper discusses how to define the number of words for natural language data.","dd895ad3":"I tried many kinds of smaller word frequency and post the result graph","1db50918":"The **SVM\u2019s performance was the best**, and other machine learning models predicted label correctly in the order of RF, GNB, and DT. \n\nIt should be noted that, when the number of words is 10K or 1K, the difference in accuracy was not so large despite 10 times more words (0.87 with 10K words and 0.85 with 1K words). \n\nThis means that removing words with low frequency does not significantly affect the performance. Furthermore, in the case of Gaussian Na\u00efve Bayes, the more words you use the worse the performance is. The 87.4 percent performance of SVM which reduced the word count to 4K, based on the total word count of 223K, is the best. \n\nThis analysis suggests that it is possible to obtain meaningful performance even if you generate data based on frequently occurring words\n\n\nThanks \ud83d\ude0b","3028b00b":"Natural language data has wide variety of grammar issues and requires many preprocessing tasks. \n\nEach record had already a sentiment label, positive or negative. \n\nMost of the sentences have more than 100 words. \n\nThis data has not only words but also punctuation and symbols. Before processing, it is necessary to clean the data. ","d6b076ce":"# Results","4e188eff":"![image.png](attachment:image.png)"}}