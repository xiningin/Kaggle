{"cell_type":{"1ec99add":"code","7b21403f":"code","3de7c199":"code","375faf30":"code","8ee927f6":"code","8f6ab151":"code","fddfdf1a":"code","43848fb2":"code","76421cdf":"code","05ef4593":"code","313a80b5":"code","ea3c1cfe":"code","806cdb4a":"code","9c6d0728":"code","86f0f6d1":"code","c2245090":"code","de3b09cc":"code","22331296":"code","4d35ff14":"code","d043556a":"code","c9752f42":"code","231f5d6f":"code","2261c596":"code","f527d1d7":"code","8cfcc955":"code","adbe6a86":"code","a60c2b73":"code","945d921c":"code","849b476c":"code","44199c3a":"code","d51fe729":"code","70e28c7a":"code","47c6da0a":"code","3d1f4d5c":"code","c41fe7d0":"code","9e1f402c":"code","a676d0b9":"code","332512d0":"code","29a280e4":"code","223c8096":"markdown","a564692b":"markdown","db74b15d":"markdown","44b55adf":"markdown","9fa10f02":"markdown","d60ea0bd":"markdown","11ae16f1":"markdown","b5d4492f":"markdown","4f11764f":"markdown","e9c9a88c":"markdown","6015dbb4":"markdown","3343d109":"markdown","540ef5c3":"markdown","8a7bbf8a":"markdown","719fdf2a":"markdown","23b25842":"markdown","17488dbb":"markdown","1d3d01c2":"markdown","b36577a3":"markdown","3ea6dc05":"markdown","07067cf2":"markdown","eba3bc26":"markdown","cc5033b8":"markdown","9f95b372":"markdown","a557d3a2":"markdown","8d2cb1c7":"markdown","1e3a72ae":"markdown","506bae0b":"markdown","c3808f90":"markdown","e451d7c9":"markdown"},"source":{"1ec99add":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n%matplotlib inline","7b21403f":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","3de7c199":"wine_data=pd.read_csv(\"\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")","375faf30":"wine_data.columns","8ee927f6":"print(\"Rows,columns:\" + str(wine_data.shape))","8f6ab151":"wine_data.info()","fddfdf1a":"wine_data.describe().transpose()","43848fb2":"wine_data.head(5)","76421cdf":"wine_data.isnull().sum()\n#Same as wine_data.isna().sum()","05ef4593":"#From this plot, we see that most wine types are in 5 and 6 category, which means most are considered bad. \nplt.figure(figsize = (11,6))\nsns.countplot(data=wine_data, x='quality')\n","313a80b5":"# Let us see the relationship between the label \"quality\" and other variables.\nplt.figure(figsize = (11,6))\nsns.heatmap(wine_data.corr(), \n            xticklabels=wine_data.corr().columns, \n            yticklabels=wine_data.corr().columns, \n            annot=True, \n            cmap=sns.diverging_palette(220,20,\n            as_cmap=True))","ea3c1cfe":"#PH has a correlation of -0.058. Now plotting it is obvious that wine quality doesn't depend on pH. \n#PH is Power of Hydrogen which is a scale used to specify how acidic or basic a water-based solution is\nplt.figure(figsize = (11,6))\nsns.barplot(data=wine_data, x='quality',y='pH')","806cdb4a":"#Alcohol is the most correlated feature with the quality of the wine, hence a reason why here good wine has high alcohol\nplt.figure(figsize = (11,6))\nsns.barplot(data=wine_data, x='quality',y='alcohol')","9c6d0728":"#Citric acid also correlates with the quality of the wine, hence a reason why here good wine has high citric acid\nplt.figure(figsize = (11,6))\nsns.barplot(data=wine_data, x='quality',y='citric acid')","86f0f6d1":"#Another feature to explore is sulphates. \nplt.figure(figsize = (11,6))\nsns.barplot(data=wine_data, x='quality',y='sulphates')","c2245090":"#Also, we can see that the quality of the wine does not depend on the residual sugar\n#Alcohol is the most correlated feature with the quality of the wine, hence a reason why here good wine has high alcohol\nplt.figure(figsize = (11,6))\nsns.barplot(data=wine_data, x='quality',y='residual sugar')","de3b09cc":"wine_data['good quality']=[1 if x>=7 else 0 for x in wine_data['quality']]","22331296":"#plt.figure(figsize = (11,6))\nsns.countplot(data=wine_data, x='good quality')","4d35ff14":"#The exact values of wine quality. 0 stands for bad wine, whereas 1 is for good wine.\nwine_data['good quality'].value_counts()","d043556a":"#Let us drop quality off the dataset\n\nwine_data=wine_data.drop('quality',axis=1)","c9752f42":"#Now let us seperate the dataset as response variable and label or target variable\nX = wine_data.drop('good quality', axis = 1)\ny = wine_data['good quality']","231f5d6f":"#Splitting data into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","2261c596":"#Applying Standard scaling to get optimized result\nscaler = StandardScaler()","f527d1d7":"X_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","8cfcc955":"model1=SVC()","adbe6a86":"model1.fit(X_train, y_train)","a60c2b73":"pred_svc=model1.predict(X_test)","945d921c":"print(classification_report(y_test, pred_svc))","849b476c":"model2 = RandomForestClassifier(n_estimators=200)\nmodel2.fit(X_train, y_train)\npred_rfc = model2.predict(X_test)","44199c3a":"print(classification_report(y_test, pred_rfc))","d51fe729":"#Confusion matrix\nprint(confusion_matrix(y_test, pred_rfc))","70e28c7a":"lrmodel=LogisticRegression()\nlrmodel.fit(X_train,y_train)\nlogpred=lrmodel.predict(X_test)\nprint(classification_report(y_test, logpred))","47c6da0a":"#Finding best parameters for our SVC model\nparam = {\n    'C': [0.1,0.8,0.9,1,1.1,1.2,1.3,1.4],\n    'kernel':['linear', 'rbf'],\n    'gamma' :[0.1,0.8,0.9,1,1.1,1.2,1.3,1.4]\n}\ngrid_svc = GridSearchCV(model1, param_grid=param, scoring='accuracy', cv=10)","3d1f4d5c":"grid_svc.fit(X_train, y_train)","c41fe7d0":"grid_svc.best_params_","9e1f402c":"#Let's run SVC again with the best parameters.\nmodel_svc2 = SVC(C = 1.2, gamma =  0.9, kernel= 'rbf')\nmodel_svc2.fit(X_train, y_train)\npred_svc2 = model_svc2.predict(X_test)\nprint(classification_report(y_test, pred_svc2))","a676d0b9":"rand_forest_val = cross_val_score(estimator = model2, X = X_train, y = y_train, cv = 10)\nrand_forest_val.mean()","332512d0":"svc_val = cross_val_score(estimator = model1, X = X_train, y = y_train, cv = 10)\nsvc_val.mean()","29a280e4":"log_val = cross_val_score(estimator = lrmodel, X = X_train, y = y_train, cv = 10)\nlog_val.mean()","223c8096":"# Introduction\n\nIf you thought to work on exploring more on Red Wine Quality, it is probably that you don't have any problem having a wine(Same for me, I may not be right on this saying). Having that in mind, let us start to see what makes good wine. Each wine is rated as good or bad on scale of 0 to 10. Good wine start from 7 to 10 whereas bad wine is less than 7 based on label of quality. \n\nLet us start by importing relevant libraries. I will import all libraries to use all over the notebook. ","a564692b":"# Thank you for reading. If you made it to this point, help me know your observations(can be a comment, correcting mistake, or additional input) so that anyone reading this can get the full understanding of algorithms used in this notebook. \n\n# * Happy modeling & predicting!*","db74b15d":"Now we see that the most 3 features which are very linked to the quality of the wine are alcohol, citric acid, and sulphates.","44b55adf":"The Random Forest Classifier Model achieved 88% accuracy. ","9fa10f02":"As we now see, the model accuracy achieved with cross validating the model is 91% from 88%","d60ea0bd":"# Predicting Good Wine With ML Algorithms","11ae16f1":"# Conclusion","b5d4492f":"# Credits & Inspirations\n\n\n[1] Blog, Terence Shin, TDS. Available [Here](https:\/\/towardsdatascience.com\/predicting-wine-quality-with-several-classification-techniques-179038ea6434)\n\n[2] Kaggle Notebook, Prediction of quality of Wine, Vishar Kumar. Available [Here](https:\/\/www.kaggle.com\/vishalyo990\/prediction-of-quality-of-wine)","4f11764f":"Now, let us attempt to improve the model performance by using cross validation.\n\n> Cross Validation is used to assess the predictive performance of the models and and to judge how they perform outside the sample to a new data set also known as test data. The motivation to use cross validation techniques is that when we fit a model, we are fitting it to a training dataset.  (Full definition by Research Gate)","e9c9a88c":"Also, let us import the data that we are going to be working with. ","6015dbb4":"I wiss compare three Machine learning algotithms namely SVC(Support Vector Classifier), Random Forest Classifier,and Logistic Regression. \n\n# With Support Vector Classifier **(SVC)","3343d109":"By doing the same thing on the Logistic model, the accuracy increase 2%, which is good improvement. ","540ef5c3":"# Data Preprocessing","8a7bbf8a":"The SVC Model achieved 86% accuracy. ","719fdf2a":"# With Random Forest Classifier","23b25842":"Now that we are pretty sure our data is okay, we can start understanding the data by using the power of visualizations.","17488dbb":"Looking on the classification report, the model achieved 86% of accuracy.","1d3d01c2":"We've see that the wine quality is rated from 0 to 10. I will make a column of good quality, where wine from 0 to 6 or less than 7 will be labelled 0, and else 1. [1]","b36577a3":"In this section, we will make the use two powerful options for improving model performance, which are Grid search and Cross Validation.","3ea6dc05":"Jumping on the start of our work, the goal was to predict the good quality of the wine given the data of what makes the good wine.\nWe performed analysis to have insights on various quality measures such as alcohol, citric acid, etc... The data set was orginally clean, so we didn't have to spend much time doing cleaning. \n\nWe also compared three different Machine Learning Algorith which are Support Vector Classifier (SVC), Random Forest Classifier, and Logistic Regression. Random Forest outlined other algorithms in making good predictions. \nThere are many other algorithms that can be applied here, our focus was to use some of them.  \n\n\nFinally, we opted to use two techniques (which are GridSearch for finding best parameters, and Cross Validation) for improving accuracy on each of the 3 Machine learning algorithms. The model accuracy for each one was improved a number of percentages(Example is for Random Forest Classifer, accuracy went from 88% to 91%). ","07067cf2":"# Cross Validation Score for random forest and SVC","eba3bc26":"# With Logistic Regression","cc5033b8":"# Improving Model Performance[2]","9f95b372":"Let us see the variables that we are going to be working with. ","a557d3a2":"As seen on above report, the model with the best parameters achieved 90% accuracy from 86%.","8d2cb1c7":"Welcome to this notebook of predicting the best wine quality using provided dataset. In this notebook, we will have the following parts:\n\n* Introduction\n* Exploratory Data Analysis\n* Data Pre-processing\n* Creating a Models\n* Improving Model Performance\n* Conclusion\n* Credits & Aspirations\n\nLet us get started.","1e3a72ae":"The data has 1599 rows and 12 columns. Let us quickly use 3 methods to explore our data, which are info, describe, and head. ","506bae0b":"Also, as seen above, by assessing the model to unseen data or test set (Cross validation), The SVC model accuracy increased from 86% to 89% ","c3808f90":"# Exploratory Data Analysis\n\nIn this section, we will check if our data is ready for further processing, perform visualization with seaborn, and clean it (where necessary). \n\nLet us start by checking the missing values","e451d7c9":"# Creating a Model"}}