{"cell_type":{"786d7ce3":"code","562e736f":"code","0ffea6a1":"code","38ccbb98":"code","22c33855":"code","2569ed04":"code","045df2c0":"code","8958bc95":"code","3d6bba20":"code","7b2f5e28":"code","958c8588":"markdown","e0d3512b":"markdown","44e4924d":"markdown","f70929fc":"markdown","396da91c":"markdown","9803090a":"markdown","126b8c34":"markdown","e557289a":"markdown"},"source":{"786d7ce3":"from sklearn.datasets import load_iris\ndata = load_iris(as_frame=True)\ndata.data.head()","562e736f":"data.target_names","0ffea6a1":"from sklearn.model_selection import train_test_split\nX = data.data.iloc[:, 2:]  # w\u00e4hle nur `petal length` und `petal width`\ny = data.target == 2  # Bin\u00e4rer Vektor, 1: Iris Virginica, 0: nicht Iris Virginica\nX.head()","38ccbb98":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_validate\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","22c33855":"from sklearn.model_selection import GridSearchCV","2569ed04":"import numpy as np\nimport matplotlib.pyplot as plt\n\nmodel.fit(X_train, y_train)\n\ndef decision_boundary(x):\n    ...   # Ersetze `...` mit dem richtigen Code.\n    return ...   # Ersetze `...` mit dem richtigen Code.\n\nplt.scatter(X_train.iloc[:, 0], X_train.iloc[:, 1], c=y_train)\nx = np.arange(2,7,0.1)\ny = decision_boundary(x)\nplt.plot(x, y)","045df2c0":"X = data.data.iloc[:, 2:]  # w\u00e4hle nur `petal length` und `petal width`\ny = data.target == 1  # Bin\u00e4rer Vektor 1: Iris Versicolor, 0: nicht Iris Versicolor\nX_train, X_test, y_train, y_test = train_test_split(X, y == 1, test_size=0.2, random_state=42)","8958bc95":"X = data.data.iloc[:, 2:]  # w\u00e4hle nur `petal length` und `petal width`\ny = data.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100)\n\nmodel = LogisticRegression(penalty='none', multi_class='multinomial')\nmodel.fit(X_train, y_train)","3d6bba20":"import numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_decision_boundary(model, X, y):\n    plt.figure()\n    petal_length = np.arange(1, 7, 0.1)\n    petal_width = np.arange(0, 3, 0.1)\n\n    X_grid, Y_grid = np.meshgrid(petal_length, petal_width)\n\n    def pred(x, y, label):\n        return model.predict_proba(np.c_[x, y])[0, label]\n\n    pred = np.frompyfunc(pred, 3, 1, )\n\n    plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y)\n\n    Z = pred(X_grid, Y_grid, 0)\n    plt.contour(X_grid, Y_grid, Z, [0.5])\n    Z = pred(X_grid, Y_grid, 1)\n    plt.contour(X_grid, Y_grid, Z, [0.5])\n    Z = pred(X_grid, Y_grid, 2)\n    plt.contour(X_grid, Y_grid, Z, [0.5])\n    \nplot_decision_boundary(model, X_train, y_train)","7b2f5e28":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","958c8588":"# Klassifikation\n\nIn diesem Beispiel versuchen wir ein tieferes Verst\u00e4ndnis f\u00fcr Logistische Regression und Softmax Regression zu bekommen. Dazu plotten wir einige Entscheidungsgrenzen unserer Klassifikatoren.\n\nZuerst laden wir das Iris Datenset. Dieses Datenset beinhaltet 150 Beobachtungen von jeweils vier Attributen von Schwertlilien. Gemessen wurden dabei jeweils die Breite und die L\u00e4nge des Kelchblatts (Sepalum) sowie des Kronblatts (Petalum) in Zentimeter. Des weiteren ist f\u00fcr jeden Datensatz die Art der Schwertlilie (Iris setosa, Iris virginica oder Iris versicolor) angegeben.","e0d3512b":"## Bin\u00e4rer Klassifikator\n\nZu Visualisierungszwecken benutzen wir nur die Features Petal-L\u00e4nge (Index 2) und Petal-Breite (Index 3). Weiters wollen wir zuerst einen Klassifikator trainieren, welcher Iris Virginica (Index 2) von den restlichen Iris Arten unterscheiden kann.","44e4924d":"## 3a. Bin\u00e4re Logistische Regression\n- Trainiere eine bin\u00e4re Logistische Regression (`sklearn.linear_model.LogisticRegression`) um Iris Virginica zu erkennen.\n- Benutze eine l2 Regularisierung `penalty='l2'` und `GridSearchCV` f\u00fcr den Parameter `C`.\n- Welchen Einfluss hat der Hyperparameter C?","f70929fc":"## 3c. Softmax Regression\nUm einen Multioutput Klassifikator zu trainieren, k\u00f6nnen wir einfach den urspr\u00fcnglichen Targetvektor verwenden. `sklearn` trainiert dann eine Softmax Regression.\n","396da91c":"Um die Entschdeidungsgrenzen zu plotten verwenden wir einen Konturplot.","9803090a":"## 3b. Entscheidungsgrenze von Iris Versicolor\n- Trainiere eine Logistische Regression f\u00fcr Iris Versicolor.\n- Plotte wie im vorherigen Beispiel die Entscheidungsgrenze.","126b8c34":"- Plotte die Entscheidungsgrenzen f\u00fcr verschiedene Werte des Hyperparameters `C`. Verwende dazu `penalty='l2'`.\n- Berechne die Wahrheitsmatrix (Confusion Matrix)\n- Berechne folgenden Werte: `accuracy_score, precision_score, recall_score, f1_score`","e557289a":"Im n\u00e4chsten Schritt wollen wir die Entscheidungsgrenze plotten. Forme dazu die Klassenzugeh\u00f6rigkeitswahrscheinlichkeit in Form der Sigmoid Funktion auf $x_2$ (Petal-Breite) um und setze $p=0.5$.\n$$ p = \\sigma(x) = {1 \\over 1 + \\exp(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2)}$$\n\n- Schreibe eine Python Funktion (benutze die Parameter `intercept_` und `coef_`).\n- Erstelle einen Plot mit dem Datensatz (nur Petal L\u00e4nge und Petal Breite) und zeichne die Entscheidungsgrenze ein.\n"}}