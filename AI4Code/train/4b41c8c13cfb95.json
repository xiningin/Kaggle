{"cell_type":{"8ae73847":"code","bdec4adf":"code","7c732dca":"code","501c8f54":"code","15f63cf0":"code","9b9926d6":"code","f8b9e5a4":"code","9d1d41d2":"code","fb88108f":"code","ffcd65c1":"code","9e520bd1":"code","9a4e97bc":"code","0a2f29c4":"code","9087f9b2":"code","948d5168":"code","d33ea835":"code","25a6ba31":"code","0da7ca85":"code","6e3bcae9":"code","dd02c67f":"code","d6554e94":"code","100869ac":"code","b4ae8b9c":"code","9340c5ab":"code","1871d533":"code","a3e30954":"code","52420ae5":"markdown","a297e0f4":"markdown","de7d4c04":"markdown","b3e32bab":"markdown","f16cb31d":"markdown","361be02f":"markdown","4cc9b48a":"markdown","d18e040c":"markdown","99915ddd":"markdown","e208b9c2":"markdown","023dc862":"markdown","6086552c":"markdown","530d5e1a":"markdown","e7549b79":"markdown","8f5253ce":"markdown","5f89fc52":"markdown","0d90e261":"markdown","f68d9d9f":"markdown","368779e5":"markdown","aaedf412":"markdown","d76cc6b3":"markdown"},"source":{"8ae73847":"import pandas as pd\n\ndata = pd.read_csv('..\/input\/WA_Fn-UseC_-HR-Employee-Attrition.csv')","bdec4adf":"# first few rows\ndata.head()","7c732dca":"# columns\ndata.info()","501c8f54":"catDf = pd.DataFrame(columns=['Categorical Features','Distinct Values','Distinct Count'])","15f63cf0":"data['Attrition'].nunique()","9b9926d6":"i=0\nfor col in data.select_dtypes(['object']).columns.tolist():\n    catDf.loc[i] = [col,data[col].unique(),data[col].nunique()]\n    i=i+1","f8b9e5a4":"catDf","9d1d41d2":"%matplotlib inline\nimport matplotlib.pyplot as plt\ndata.hist(bins=50,figsize=(20,15))\nplt.show()","fb88108f":"# stata\ndata.describe()","ffcd65c1":"plt.figure(figsize=(80,80))\ni=1\n\nfor col in data.select_dtypes('int64').columns.tolist():\n    plt.subplot(7,4,i)\n    x1 = list(data[data['Attrition'] == 'Yes'][col])\n    x2 = list(data[data['Attrition'] == 'No'][col])\n    colors = ['#E69F00', '#56B4E9']\n    names = ['Attrition-Yes(1)','Attrition-No(0)']\n    plt.hist([x1,x2],bins=50,label=names,color=colors)\n    plt.legend(fontsize=28)\n    plt.xlabel(col,fontsize=28)\n    plt.ylabel('# of attrition',fontsize=28)\n    plt.tick_params(labelsize=26)\n    i = i+1\n    ","9e520bd1":"# mapping the target variable to numeric \nimport numpy as np\n\ndata['Attrition'] = np.where(data['Attrition'] == 'Yes',1,0)\ndata['Attrition'].head(3)","9a4e97bc":"from sklearn.model_selection import train_test_split\n\ntrain,test = train_test_split(data,test_size=0.2,random_state=7)","0a2f29c4":"X_train = train.drop(['Attrition'],axis=1)\ny_train = train['Attrition']","9087f9b2":"# Ref: https:\/\/stackoverflow.com\/questions\/17778394\/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas\n\ndef get_redundant_pairs(df):\n    '''Get diagonal and lower triangular pairs of correlation matrix'''\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\ndef get_top_abs_correlations(df, n=5):\n    au_corr = df.corr().abs().unstack()\n    labels_to_drop = get_redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    return au_corr[0:n]","948d5168":"X_train.head()","d33ea835":"print(\"Top 10 Correlations pairs\")\nprint(get_top_abs_correlations(X_train.select_dtypes(exclude=['object']), 10))","25a6ba31":"# We will use the below statement, lets wrap it under feature engineering class\n#data = data.drop(['DailyRate','EmployeeCount','StandardHours'],axis=1)\n\n#include Object\ncat_attributes = X_train.select_dtypes(include=['object']).columns.tolist()\n#exclude Object\nnum_attributes = X_train.select_dtypes(exclude=['object']).columns.tolist()\n\n#We also observed we can drop columns such as Daily rate, Employee Count and standard hours\nnum_attributes.remove('DailyRate')\nnum_attributes.remove('EmployeeCount')\nnum_attributes.remove('StandardHours')\nnum_attributes.remove('MonthlyIncome')\n#num_attributes.append('monthlyIncomePerUnitAge')\n\nprint('cat_attributes: ',cat_attributes)\nprint('num_attributes: ',num_attributes)","0da7ca85":"len(num_attributes)","6e3bcae9":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import MinMaxScaler\n\nmergeCols_pipeline = ColumnTransformer([\n         #('imputer' ,Imputer(strategy=\"median\")), we don't need in this case there is no NA\n        #('attrAdder',CombinedAttributesAdder(),num_attributes),\n       ('scaler',MinMaxScaler(),num_attributes),\n       (\"cat\", OneHotEncoder(), cat_attributes)\n    ])\n","dd02c67f":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.feature_selection import SelectKBest, chi2\n\nclassifiers = {\n    'Nearest Neighbors' : KNeighborsClassifier(3),\n    'Linear SVM'        :SVC(kernel=\"linear\", C=0.025),\n    'RBF SVM'           :SVC(gamma=2, C=1),\n    'Gaussian Process'  :GaussianProcessClassifier(1.0 * RBF(1.0)),\n    'Decision Tree'     :DecisionTreeClassifier(max_depth=5),\n    'Random Forest'     :RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n    'Neural Net'        :MLPClassifier(alpha=1),\n    'AdaBoost'          :AdaBoostClassifier(),\n    'Naive Bayes'       :GaussianNB(),\n    'QDA'               :QuadraticDiscriminantAnalysis()\n}\n\n\nmergeColWithSelectKbest_pipeline = Pipeline([('prep',mergeCols_pipeline),\n                          ('featSelection',SelectKBest(chi2,k=10))\n                        ])\n\nX_train_prep = mergeColWithSelectKbest_pipeline.named_steps['prep'].fit_transform(X_train)\n\nX_train_featureSel = mergeColWithSelectKbest_pipeline.named_steps['featSelection'].fit_transform(X_train_prep,y_train)\n\n#ref: https:\/\/chrisalbon.com\/machine_learning\/model_evaluation\/plot_the_learning_curve\/\n\nplt.figure(figsize=(40,30))\ni=1\n\n\nfor key in classifiers:\n    \n    train_sizes, train_scores, test_scores = learning_curve(classifiers[key], \n                                                        X_train_featureSel, \n                                                        y_train,\n                                                        # Number of folds in cross-validation\n                                                        cv=7,\n                                                        # Evaluation metric\n                                                        scoring='accuracy',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.10, 1.0, 50))\n\n    # Create means and standard deviations of training set scores\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n\n    # Create means and standard deviations of test set scores\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n\n    plt.subplot(4,3,i)\n    \n    \n    # Draw lines\n    plt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\n    plt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n    # Draw bands\n    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\n    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n    # Create plot\n    plt.title(\"Learning Curve \" + key)\n    plt.xlabel(\"Training Set Size\",fontsize=30), plt.ylabel(\"Accuracy Score\",fontsize=30), plt.legend(loc=\"best\",fontsize=24)\n    plt.tight_layout()\n    \n    plt.tick_params(axis='both', which='major', labelsize=25)\n    plt.tick_params(axis='both', which='minor', labelsize=25)\n    \n    i=i+1\n    \n    #plt.show()","d6554e94":"# How many features we have \nnp.shape(X_train_prep)[1]","100869ac":"from sklearn.metrics import accuracy_score\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndf_k_acc = pd.DataFrame(columns=['K','Accuracy'])\ni=1\nfor i in range(1,np.shape(X_train_prep)[1]+1):\n    \n    mergeColWithSelectKbest_pipeline = Pipeline([('prep',mergeCols_pipeline),\n                          ('featSelection',SelectKBest(chi2,k=i))\n                        ])\n\n    X_train_prep = mergeColWithSelectKbest_pipeline.named_steps['prep'].fit_transform(X_train)\n\n    X_train_featureSel = mergeColWithSelectKbest_pipeline.named_steps['featSelection'].fit_transform(X_train_prep,y_train)\n    \n    regressor = AdaBoostClassifier()\n    # fit\n    regressor.fit(X_train_featureSel,y_train)\n    # predict \n    train_predictions = regressor.predict(X_train_featureSel)\n    \n    \n    df_k_acc.loc[i] = [i,accuracy_score(y_train, train_predictions)]\n\n","b4ae8b9c":"#Plot the accuracy v\/s K - number of features \nplt.plot(df_k_acc['K'],df_k_acc['Accuracy'])\nplt.xlabel('K[Number of features]')\nplt.ylabel('Accuracy')","9340c5ab":"from sklearn.model_selection import GridSearchCV\n\n#list of dictionary \nparam_grid = [ {'n_estimators'  : [10,100,500,1000],\n                'learning_rate' : [0.2,0.4,0.6,0.8,1,1.2,1.4],\n                'algorithm'     : ['SAMME', 'SAMME.R']\n               }\n             ]\n\nclassifier = AdaBoostClassifier()\n\ngrid_search = GridSearchCV(classifier, param_grid, cv=5,\n                           scoring='accuracy', return_train_score=True)\n\ngrid_search.fit(X_train_prep, y_train)","1871d533":"grid_search.best_params_","a3e30954":"X_test = test.drop(['Attrition'],axis=1)\ny_test = test['Attrition']\n\nX_test_prep = mergeCols_pipeline.transform(X_test)\n\ntest_predictions = grid_search.best_estimator_.predict(X_test_prep)\naccuracy_score(y_test, test_predictions)\n","52420ae5":"# Feature Engineering","a297e0f4":"# Build numerical and categorical feature pipeline ","de7d4c04":"__Interesting observations:__\n\n1. Iteration rate is higher in the early age: twenties  \n2. Iteration rate is higher among those who lives farther from the office. \n3. Iteration rate is higher in their second year\n4. Lower jobs levels tend to have higher iteration rate.\n\n__and many more, We also observed we can drop columns such as Daily rate, Employee Conunt and standard hours for obvious reasons__","b3e32bab":"__AdaBoost appears to perform best and consistant with this data set__","f16cb31d":"# Split training and test data","361be02f":"__Goal here is to have as much as less correlated features in the model. Dropping MonthlyIncome seams to be wise choice since it will drop first and third pair__","4cc9b48a":"# Download Data","d18e040c":"# Grid Search - Feature Tuning on the best model","99915ddd":"__No Null values and we have 26 int types and 9 object types  mainly string. These 9 objects are categorical attributes .In total we have 35 features__","e208b9c2":"# Evaluating number of features on the best Model [AdaBoost] - Tuning SelectKBest","023dc862":"# Exploratory data analysis","6086552c":"# Segregating X and y ","530d5e1a":"__Looks like all features are important and we will skip the feature selection step__","e7549b79":"# Data visualization and EDA","8f5253ce":"# Check for Outliers ","5f89fc52":"__Performing grid search on AdaBoost__","0d90e261":"__Since we don't see small outliers, Min-Max scaling technique would be ideal. Lets build a pipeline__","f68d9d9f":"# Model Selection - Learning Curve","368779e5":"__Visualization via histograms__","aaedf412":"# Evaluating accuracy on the true test data","d76cc6b3":"__Data looks clean with no potential outliers. We can drop following features since they are constant and does not contribute to the model:__\n\n1. __Employee Count__\n2. __StanardHours__"}}