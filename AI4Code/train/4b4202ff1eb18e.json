{"cell_type":{"89d734fa":"code","957614e9":"code","a7858d68":"code","240d03ee":"code","f191806b":"code","76709a99":"code","120f4036":"code","09faa22d":"code","b41aab8f":"code","83ed1e5a":"code","6c9123ad":"code","de16b26d":"code","37bcdf1b":"code","3a6a1dd0":"code","adbf726c":"code","1a3cd8e8":"code","aa886799":"code","58d4a1d4":"code","7d7affa4":"code","248bb0fc":"code","5507414a":"code","d9ede365":"code","7ab6b3e8":"code","f51eeb51":"code","d2bcd698":"code","de488a9d":"code","28bf6094":"code","18db8131":"code","98a4de3b":"code","c7432d61":"code","63066a72":"code","3a9ecc97":"code","4e44b50c":"code","f198613c":"code","30e68274":"code","f7c1e5c6":"code","0d86032e":"code","09d654dc":"code","2c31a82b":"code","2121604f":"code","b8af5833":"code","93a54121":"code","401f90a2":"code","f8c47b5c":"code","037cdf52":"code","54aea376":"code","dbc2bea5":"code","ee7a0216":"code","868e2a3f":"code","091d3ced":"code","ad07215c":"code","0eb01fd2":"code","398d039d":"code","94ce7784":"code","add842b2":"code","82646df6":"code","4bebbde5":"code","4a69b590":"code","aa5a3d80":"code","c1d77e5c":"code","d78ed428":"code","5ed35c2f":"code","24343913":"code","bf488228":"code","b43e89bd":"code","c3b748ee":"code","f72cb194":"code","ca14dfd9":"code","9079970e":"code","a5b07c45":"code","8ddb4671":"code","17ff8bca":"code","8f3001ba":"code","db1c688b":"code","c3f80fc6":"code","b9175c72":"code","da2fce75":"code","41c36944":"code","46ee67c7":"code","8628eff2":"code","8b0eda4b":"code","6cf85f81":"code","7efdf93b":"code","b8fd1fc1":"code","75d53547":"code","0bed48c4":"code","8b72cf3c":"code","e2582ce6":"code","64946ee2":"code","7ad05584":"code","2bf3a6f4":"code","edceaf74":"code","910594fb":"code","6c20ceaf":"code","e291fb81":"code","e4eb59db":"code","68a9921b":"code","af471606":"code","ebcd20b0":"code","fa3b394d":"code","f235120a":"code","cc774b44":"code","5c742de2":"code","bd1446fd":"code","fd6c9bf4":"code","675b5ee7":"code","2935d086":"code","ee4ae80b":"code","70332bdc":"code","12c9bfa9":"code","299fc1bd":"code","6da9a3cb":"code","ddaeaee3":"code","187eedfd":"code","d3c8b8bc":"code","0e9c9b8c":"code","dc7556b0":"code","7a278ade":"code","2f99d4db":"code","b95b7094":"code","220a2dae":"code","d3cec076":"code","11114b90":"code","0f03656f":"code","e969721f":"code","6a390d7a":"code","957f1d16":"code","0a4aa146":"code","f02e8703":"code","92070438":"code","ee4f8b52":"code","c2177b8a":"code","c754f435":"code","b3c1f851":"code","22acb7ec":"code","12dbfb68":"code","6e7e223c":"code","3f549118":"code","e110c8c3":"markdown","043e66ec":"markdown","535490e0":"markdown","7fc30841":"markdown","470d37a2":"markdown","4a832cec":"markdown","d62a5417":"markdown","889624ac":"markdown","823b55fd":"markdown","bd73d969":"markdown","87e4557d":"markdown","397c6ddc":"markdown","97e395c3":"markdown","555d8bae":"markdown","98f90831":"markdown","9c186241":"markdown","e4cb1c2b":"markdown","71730330":"markdown","e952000e":"markdown","cc835b1e":"markdown","baf28a80":"markdown","5ee86d77":"markdown","3048f41c":"markdown","29ac0b9b":"markdown","08faa5f8":"markdown","ee7e17b8":"markdown","20489ea6":"markdown","aa610650":"markdown","ece1b5fb":"markdown"},"source":{"89d734fa":"# conventional way to import pandas\nimport pandas as pd","957614e9":"#read file.\ndf = pd.read_csv('..\/input\/anonymized_full_release_competition_dataset20181128.csv')\n#replace spaces with underscores for all columns \ndf.columns = df.columns.str.replace(' ', '_')\ndf.head()","a7858d68":"df.shape","240d03ee":"#locate a value in a column as Nan https:\/\/stackoverflow.com\/questions\/45416684\/python-pandas-replace-multiple-columns-zero-to-nan?rq=1\nimport numpy as np\ndf.loc[df['MCAS'] == -999.0,'MCAS'] = np.nan","f191806b":"# create the 'genderFemale' dummy variable using the 'map' method\ndf['genderFemale'] = df.InferredGender.map({'Female':1, 'Male':0})\n# Removing unused columns\nlist_drop = ['InferredGender']\ndf.drop(list_drop, axis=1, inplace=True)","76709a99":"df.shape","120f4036":"df.head()","09faa22d":"# create dummy variables for multiple categories; this drops nominal columns and creates dummy variables\ndfDummy=pd.get_dummies(df, columns=['MiddleSchoolId'], drop_first=True)\ndfDummy.shape","b41aab8f":"#use observations only with no missing in isSTEM\ndfStem=dfDummy.dropna(subset=['isSTEM'], how='any')\ndfStem.shape","83ed1e5a":"#locate columns with the data type of object\ndfStem.loc[:, dfStem.dtypes == 'object'].head()","6c9123ad":"# use means to transform df to student level data\nstud = dfStem.groupby('studentId').mean()\nstud.shape # from 84 to 78 columns due to delete object columns and 'isSTEM'","de16b26d":"stud.head()","37bcdf1b":"#Find column names with missing data because sklearn does not allow missing data\nstud.columns[stud.isnull().any()]","3a6a1dd0":"# impute MCAS missing values with median\nMCAS_median = np.nanmedian(stud['MCAS'])\nnew_MCAS = np.where(stud['MCAS'].isnull(), MCAS_median, stud['MCAS'])\nstud['MCAS'] = new_MCAS\nstud.head()","adbf726c":"feature_colsCor= [\n  'isSTEM',\n 'AveResBored',\n 'AveResEngcon',\n 'AveResConf',\n 'AveResFrust',\n 'AveResOfftask',\n 'AveResGaming']\ncorrAll = stud[feature_colsCor]","1a3cd8e8":"from scipy.stats import pearsonr\nimport pandas as pd\n\ndef calculate_pvalues(df):\n    df = df.dropna()._get_numeric_data()\n    dfcols = pd.DataFrame(columns=df.columns)\n    pvalues = dfcols.transpose().join(dfcols, how='outer')\n    for r in df.columns:\n        for c in df.columns:\n            pvalues[r][c] = round(pearsonr(df[r], df[c])[1], 4)\n    return pvalues\n\nrho = corrAll.corr()\nrho = rho.round(4)\npval = calculate_pvalues(corrAll) \n# create three masks\nr1 = rho.applymap(lambda x: '{}*'.format(x))\nr2 = rho.applymap(lambda x: '{}**'.format(x))\n# apply them where appropriate\nrho = rho.mask(pval< 0.05,r1)\nrho = rho.mask(pval< 0.01,r2)\nrho","aa886799":"#reference: http:\/\/seaborn.pydata.org\/tutorial\/distributions.html\nimport seaborn as sns\nsns.pairplot(corrAll); #corrAll = stud[feature_colsCor]","58d4a1d4":"corrAll.describe()","7d7affa4":"corrAll.skew()","248bb0fc":"corrAll.kurt()","5507414a":"# list(stud) to copy column names\nfeature_cols = [\n 'AveResBored',\n 'AveResEngcon',\n 'AveResConf',\n 'AveResFrust',\n 'AveResOfftask',\n 'AveResGaming']\nX = stud[feature_cols]\ny = stud.isSTEM","d9ede365":"#Find column names with missing data because sklearn does not allow missing data\nX.columns[X.isnull().any()]","7ab6b3e8":"#Compute linear regression standardized coefficient (beta) with Python\n#https:\/\/stackoverflow.com\/questions\/33913868\/compute-linear-regression-standardized-coefficient-beta-with-python\nimport statsmodels.api as sm\nfrom scipy.stats.mstats import zscore","f51eeb51":"#logistic regression result is ok.\nlogit = sm.Logit(y, X).fit()\nlogit.summary()","d2bcd698":"#odds ratio with conf. intervals; odds ratio as effect sizes; results hard to explain with relative importance\nparams = logit.params\nconf = logit.conf_int()\nconf['OR'] = params\nconf.columns = ['2.5%', '97.5%', 'OR']\nnp.exp(conf)","de488a9d":"#negative pseudo R-square, cannot do y2 = zscore(y)\nXz = zscore(X)\nlogitXz = sm.Logit(y, Xz).fit()\nlogitXz.summary()","28bf6094":"#odds ratio with conf. intervals; odds ratio as effect sizes; results hard to explain with relative importance\nparams = logitXz.params\nconf = logitXz.conf_int()\nconf['OR'] = params\nconf.columns = ['2.5%', '97.5%', 'OR']\nnp.exp(conf)","18db8131":"logit1 = sm.Logit(y, stud.AveResBored).fit()\nlogit1.summary()","98a4de3b":"#odds ratio with conf. intervals; OR hard to explain\nparams = logit1.params\nconf = logit1.conf_int()\nconf['OR'] = params\nconf.columns = ['2.5%', '97.5%', 'OR']\nnp.exp(conf)","c7432d61":"logit2 = sm.Logit(y, stud.AveResEngcon).fit() # significant but negative, not good because Engcon is positive in meaning.\nlogit2.summary()","63066a72":"#odds ratio with conf. intervals; OR hard to explain\nparams = logit2.params\nconf = logit2.conf_int()\nconf['OR'] = params\nconf.columns = ['2.5%', '97.5%', 'OR']\nnp.exp(conf)","3a9ecc97":"logit3 = sm.Logit(y, stud.AveResConf).fit()\nlogit3.summary()","4e44b50c":"#odds ratio with conf. intervals\nparams = logit3.params\nconf = logit3.conf_int()\nconf['OR'] = params\nconf.columns = ['2.5%', '97.5%', 'OR']\nnp.exp(conf)","f198613c":"logit4 = sm.Logit(y, stud.AveResFrust).fit()\nlogit4.summary()","30e68274":"#odds ratio with conf. intervals\nparams = logit4.params\nconf = logit4.conf_int()\nconf['OR'] = params\nconf.columns = ['2.5%', '97.5%', 'OR']\nnp.exp(conf)","f7c1e5c6":"logit5 = sm.Logit(y, stud.AveResOfftask).fit()\nlogit5.summary()","0d86032e":"#odds ratio with conf. intervals\nparams = logit5.params\nconf = logit5.conf_int()\nconf['OR'] = params\nconf.columns = ['2.5%', '97.5%', 'OR']\nnp.exp(conf)","09d654dc":"logit6 = sm.Logit(y, stud.AveResGaming).fit()\nlogit6.summary()","2c31a82b":"#odds ratio with conf. intervals\nparams = logit6.params\nconf = logit6.conf_int()\nconf['OR'] = params\nconf.columns = ['2.5%', '97.5%', 'OR']\nnp.exp(conf)","2121604f":"logit1z = sm.Logit(y, zscore(stud.AveResBored)).fit()\nlogit1z.summary()","b8af5833":"#odds ratio with conf. intervals\nparams = logit1z.params\nconf = logit1z.conf_int()\nconf['OR'] = params\nconf.columns = ['2.5%', '97.5%', 'OR']\nnp.exp(conf)","93a54121":"logit2z = sm.Logit(y, zscore(stud.AveResEngcon)).fit()\nlogit2z.summary()","401f90a2":"#odds ratio with conf. intervals\nparams = logit2z.params\nconf = logit2z.conf_int()\nconf['OR'] = params\nconf.columns = ['2.5%', '97.5%', 'OR']\nnp.exp(conf)","f8c47b5c":"logit3z = sm.Logit(y, zscore(stud.AveResConf)).fit()\nlogit3z.summary()","037cdf52":"#odds ratio with conf. intervals\nparams = logit3z.params\nconf = logit3z.conf_int()\nconf['OR'] = params\nconf.columns = ['2.5%', '97.5%', 'OR']\nnp.exp(conf)","54aea376":"logit4z = sm.Logit(y, zscore(stud.AveResFrust)).fit()\nlogit4z.summary()","dbc2bea5":"#odds ratio with conf. intervals\nparams = logit4z.params\nconf = logit4z.conf_int()\nconf['OR'] = params\nconf.columns = ['2.5%', '97.5%', 'OR']\nnp.exp(conf)","ee7a0216":"logit5z = sm.Logit(y, zscore(stud.AveResOfftask)).fit()\nlogit5z.summary()","868e2a3f":"#odds ratio with conf. intervals\nparams = logit5z.params\nconf = logit5z.conf_int()\nconf['OR'] = params\nconf.columns = ['2.5%', '97.5%', 'OR']\nnp.exp(conf)","091d3ced":"logit6z = sm.Logit(y, zscore(stud.AveResGaming)).fit()\nlogit6z.summary()","ad07215c":"#odds ratio with conf. intervals\nparams = logit6z.params\nconf = logit6z.conf_int()\nconf['OR'] = params\nconf.columns = ['2.5%', '97.5%', 'OR']\nnp.exp(conf)","0eb01fd2":"sm.OLS(y, X).fit().summary() # large R-squared, go for this because also good result for individual predictor OLS result ","398d039d":"# as effect size measures\nXz = zscore(X)\nyz = zscore(y)\norzxy = sm.OLS(yz, Xz).fit()\norzxy.summary()","94ce7784":"orzx = sm.OLS(y, Xz).fit() #lower Adj. R-squared than both y and x zscore-->no use\norzx.summary()","add842b2":"sm.OLS(y, stud.AveResBored).fit().summary()","82646df6":"sm.OLS(y, stud.AveResEngcon).fit().summary()","4bebbde5":"sm.OLS(y, stud.AveResConf).fit().summary()","4a69b590":"sm.OLS(y, stud.AveResFrust).fit().summary()","aa5a3d80":"sm.OLS(y, stud.AveResOfftask).fit().summary()","c1d77e5c":"sm.OLS(y, stud.AveResGaming).fit().summary()","d78ed428":"sm.OLS(y, zscore(stud.AveResBored)).fit().summary() # zscore x1 with negative adj. r-squared","5ed35c2f":"sm.OLS(zscore(y), zscore(stud.AveResBored)).fit().summary() \n# zscore y and x1 with negative adj. r-squared; same value as correlation between y and x1","24343913":"sm.OLS(zscore(y), zscore(stud.AveResBored)).fit().summary()","bf488228":"sm.OLS(zscore(y), zscore(stud.AveResEngcon)).fit().summary()","b43e89bd":"sm.OLS(zscore(y), zscore(stud.AveResConf)).fit().summary()","c3b748ee":"sm.OLS(zscore(y), zscore(stud.AveResFrust)).fit().summary()","f72cb194":"sm.OLS(zscore(y), zscore(stud.AveResOfftask)).fit().summary()","ca14dfd9":"sm.OLS(zscore(y), zscore(stud.AveResGaming)).fit().summary()","9079970e":"!pip install eli5","a5b07c45":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\nmy_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)\n#https:\/\/www.kaggle.com\/dansbecker\/permutation-importance?utm_medium=email&utm_source=mailchimp&utm_campaign=ml4insights\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)\neli5.show_weights(perm, feature_names = val_X.columns.tolist())\n","8ddb4671":"import shap  # package used to calculate Shap values\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(my_model)\n\n# calculate shap values. This is what we will plot.\n# Calculate shap_values for all of val_X rather than a single row, to have more data for plot.\nshap_values = explainer.shap_values(val_X)\n\n# Make plot. Index of [1] is explained in text below.\nshap.summary_plot(shap_values[1], val_X)","17ff8bca":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\ny_predict = model.predict(X_test)\n\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\ny_predict_probabilities = model.predict_proba(X_test)[:,1]\n\nfpr, tpr, _ = roc_curve(y_test, y_predict_probabilities)\nroc_auc = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange',\n         lw=2, label='ROC curve (area = %0.3f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n","8f3001ba":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\nmy_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)\nval_y = my_model.predict(train_X)\n\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\ny_predict_probabilities2 = model.predict_proba(train_X)[:,1]\n\nfpr2, tpr2, _ = roc_curve(val_y, y_predict_probabilities2)\nroc_auc2 = auc(fpr2, tpr2)\n\nplt.figure()\nplt.plot(fpr2, tpr2, color='darkorange',\n         lw=2, label='ROC curve (area = %0.3f)' % roc_auc2)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n","db1c688b":"#random forest: too high accuracy and R-squared\n#https:\/\/www.kaggle.com\/shrutimechlearn\/step-by-step-assumptions-linear-regression?utm_medium=email&utm_source=intercom&utm_campaign=datanotes-2019\nfrom sklearn.metrics import r2_score\n\nrf_tree = RandomForestClassifier(random_state=0)\nrf_tree.fit(X,y)\nrf_tree_y_pred = rf_tree.predict(X)\nprint(\"Accuracy: {}\".format(rf_tree.score(X,y)))\nprint(\"R squared: {}\".format(r2_score(y_true=y,y_pred=rf_tree_y_pred)))","c3f80fc6":"import numpy as np","b9175c72":"female=stud[stud.genderFemale == 1]\nmale=stud[stud.genderFemale == 0]","da2fce75":"female.shape","41c36944":"male.shape","46ee67c7":"corrF = female[feature_colsCor]\nrho = corrF.corr()\nrho = rho.round(4)\npval = calculate_pvalues(corrF) \n# create three masks\nr1 = rho.applymap(lambda x: '{}*'.format(x))\nr2 = rho.applymap(lambda x: '{}**'.format(x))\n# apply them where appropriate\nrho = rho.mask(pval< 0.05,r1)\nrho = rho.mask(pval< 0.01,r2)\nrho","8628eff2":"feature_colsF = [\n 'AveResBored',\n 'AveResEngcon',\n 'AveResConf',\n 'AveResFrust',\n 'AveResOfftask',\n 'AveResGaming']\nXF = female[feature_colsF]\nyF = female.isSTEM","8b0eda4b":"#Compute linear regression standardized coefficient (beta) with Python\n#https:\/\/stackoverflow.com\/questions\/33913868\/compute-linear-regression-standardized-coefficient-beta-with-python\nimport statsmodels.api as sm\nfrom scipy.stats.mstats import zscore\n\n#logistic regression result is ok.\nlogitF = sm.Logit(yF, XF).fit()\nlogitF.summary()","6cf85f81":"#odds ratio with conf. intervals; results hard to explain with relative importance\nparams = logitF.params\nconf = logitF.conf_int()\nconf['OR'] = params\nconf.columns = ['2.5%', '97.5%', 'OR']\nnp.exp(conf)","7efdf93b":"logitF1 = sm.Logit(yF, female.AveResBored).fit()\nlogitF1.summary()","b8fd1fc1":"#odds ratio with conf. intervals\nparams = logitF1.params\nconf = logitF1.conf_int()\nconf['OR'] = params\nconf.columns = ['2.5%', '97.5%', 'OR']\nnp.exp(conf)","75d53547":"logitF2 = sm.Logit(yF, female.AveResEngcon).fit()\nlogitF2.summary()","0bed48c4":"#odds ratio with conf. intervals\nparams = logitF2.params\nconf = logitF2.conf_int()\nconf['OR'] = params\nconf.columns = ['2.5%', '97.5%', 'OR']\nnp.exp(conf)","8b72cf3c":"logitF3 = sm.Logit(yF, female.AveResConf).fit()\nlogitF3.summary()","e2582ce6":"#odds ratio with conf. intervals\nparams = logitF3.params\nconf = logitF3.conf_int()\nconf['OR'] = params\nconf.columns = ['2.5%', '97.5%', 'OR']\nnp.exp(conf)","64946ee2":"logitF4 = sm.Logit(yF, female.AveResFrust).fit()\nlogitF4.summary()","7ad05584":"#odds ratio with conf. intervals\nparams = logitF4.params\nconf = logitF4.conf_int()\nconf['OR'] = params\nconf.columns = ['2.5%', '97.5%', 'OR']\nnp.exp(conf)","2bf3a6f4":"logitF5 = sm.Logit(yF, female.AveResOfftask).fit()\nlogitF5.summary()","edceaf74":"#odds ratio with conf. intervals\nparams = logitF5.params\nconf = logitF5.conf_int()\nconf['OR'] = params\nconf.columns = ['2.5%', '97.5%', 'OR']\nnp.exp(conf)","910594fb":"logitF6 = sm.Logit(yF, female.AveResGaming).fit()\nlogitF6.summary()","6c20ceaf":"#odds ratio with conf. intervals\nparams = logitF6.params\nconf = logitF6.conf_int()\nconf['OR'] = params\nconf.columns = ['2.5%', '97.5%', 'OR']\nnp.exp(conf)","e291fb81":"sm.OLS(yF, XF).fit().summary()#sig: concentrate positive; game negative ","e4eb59db":"sm.OLS(zscore(yF), zscore(XF)).fit().summary() # as effect sizes --> x2 become negative and non-significant-->hard to explain","68a9921b":"sm.OLS(zscore(yF), zscore(female.AveResBored)).fit().summary()","af471606":"sm.OLS(zscore(yF), zscore(female.AveResEngcon)).fit().summary()","ebcd20b0":"sm.OLS(zscore(yF), zscore(female.AveResConf)).fit().summary()","fa3b394d":"sm.OLS(zscore(yF), zscore(female.AveResFrust)).fit().summary()","f235120a":"sm.OLS(zscore(yF), zscore(female.AveResOfftask)).fit().summary()","cc774b44":"sm.OLS(zscore(yF), zscore(female.AveResGaming)).fit().summary()","5c742de2":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\ntrain_XF, val_XF, train_yF, val_yF = train_test_split(XF, yF, random_state=1)\nmy_modelF = RandomForestClassifier(random_state=0).fit(train_XF, train_yF)\npermF = PermutationImportance(my_modelF, random_state=1).fit(val_XF, val_yF)\neli5.show_weights(permF, feature_names = val_XF.columns.tolist())","bd1446fd":"import shap  # package used to calculate Shap values\n# Create object that can calculate shap values\nexplainerF = shap.TreeExplainer(my_modelF)\n\n# calculate shap values. This is what we will plot.\n# Calculate shap_values for all of val_X rather than a single row, to have more data for plot.\nshap_valuesF = explainer.shap_values(val_XF)\n\n# Make plot. Index of [1] is explained in text below.\nshap.summary_plot(shap_valuesF[1], val_XF)","fd6c9bf4":"# Female_ROC and AUC: Logit regression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nXF_train, XF_test, yF_train, yF_test = train_test_split(XF, yF, random_state=1)\n\nmodelF = LogisticRegression()\nmodelF.fit(XF_train, yF_train)\n\ny_predictF = model.predict(XF_test)\n\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\ny_predict_probabilitiesF = modelF.predict_proba(XF_test)[:,1]\n\nfprF, tprF, _ = roc_curve(yF_test, y_predict_probabilitiesF)\nroc_aucF = auc(fprF, tprF)\n\nplt.figure()\nplt.plot(fprF, tprF, color='darkorange',\n         lw=2, label='ROC curve (area = %0.3f)' % roc_aucF)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n","675b5ee7":"#Female_ROC and AUC: Random Forest\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\ntrain_XF, val_XF, train_yF, val_yF = train_test_split(XF, yF, random_state=1)\nmy_modelF = RandomForestClassifier(random_state=0).fit(train_XF, train_yF)\nval_yF = my_modelF.predict(train_XF)\n\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\ny_predict_probabilities2F = model.predict_proba(train_XF)[:,1]\n\nfpr2F, tpr2F, _ = roc_curve(val_yF, y_predict_probabilities2F)\nroc_auc2F = auc(fpr2F, tpr2F)\n\nplt.figure()\nplt.plot(fpr2F, tpr2F, color='darkorange',\n         lw=2, label='ROC curve (area = %0.3f)' % roc_auc2F)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n","2935d086":"corrM = male[feature_colsCor]\nrho = corrM.corr()\nrho = rho.round(4)\npval = calculate_pvalues(corrM) \n# create three masks\nr1 = rho.applymap(lambda x: '{}*'.format(x))\nr2 = rho.applymap(lambda x: '{}**'.format(x))\n# apply them where appropriate\nrho = rho.mask(pval< 0.05,r1)\nrho = rho.mask(pval< 0.01,r2)\nrho","ee4ae80b":"feature_colsM = [\n 'AveResBored',\n 'AveResEngcon',\n 'AveResConf',\n 'AveResFrust',\n 'AveResOfftask',\n 'AveResGaming']\nXM = male[feature_colsM]\nyM = male.isSTEM","70332bdc":"#Compute linear regression standardized coefficient (beta) with Python\n#https:\/\/stackoverflow.com\/questions\/33913868\/compute-linear-regression-standardized-coefficient-beta-with-python\nimport statsmodels.api as sm\nfrom scipy.stats.mstats import zscore\n\n#logistic regression result: no significant predictors\nlogitM = sm.Logit(yM, XM).fit()\nlogitM.summary()","12c9bfa9":"#odds ratio with conf. intervals; results hard to explain with relative importance\nparams = logitM.params\nconf = logitM.conf_int()\nconf['OR'] = params\nconf.columns = ['2.5%', '97.5%', 'OR']\nnp.exp(conf)","299fc1bd":"logitM1 = sm.Logit(yM, male.AveResBored).fit()\nlogitM1.summary()","6da9a3cb":"#odds ratio with conf. intervals\nparams = logitM1.params\nconf = logitM1.conf_int()\nconf['OR'] = params\nconf.columns = ['2.5%', '97.5%', 'OR']\nnp.exp(conf)","ddaeaee3":"logitM2 = sm.Logit(yM, male.AveResEngcon).fit()\nlogitM2.summary()","187eedfd":"#odds ratio with conf. intervals\nparams = logitM2.params\nconf = logitM2.conf_int()\nconf['OR'] = params\nconf.columns = ['2.5%', '97.5%', 'OR']\nnp.exp(conf)","d3c8b8bc":"logitM3 = sm.Logit(yM, male.AveResConf).fit()\nlogitM3.summary()","0e9c9b8c":"#odds ratio with conf. intervals\nparams = logitM3.params\nconf = logitM3.conf_int()\nconf['OR'] = params\nconf.columns = ['2.5%', '97.5%', 'OR']\nnp.exp(conf)","dc7556b0":"logitM4 = sm.Logit(yM, male.AveResFrust).fit()\nlogitM4.summary()","7a278ade":"#odds ratio with conf. intervals\nparams = logitM4.params\nconf = logitM4.conf_int()\nconf['OR'] = params\nconf.columns = ['2.5%', '97.5%', 'OR']\nnp.exp(conf)","2f99d4db":"logitM5 = sm.Logit(yM, male.AveResOfftask).fit()\nlogitM5.summary()","b95b7094":"#odds ratio with conf. intervals\nparams = logitM5.params\nconf = logitM5.conf_int()\nconf['OR'] = params\nconf.columns = ['2.5%', '97.5%', 'OR']\nnp.exp(conf)","220a2dae":"logitM6 = sm.Logit(yM, male.AveResGaming).fit()\nlogitM6.summary()","d3cec076":"#odds ratio with conf. intervals\nparams = logitM6.params\nconf = logitM6.conf_int()\nconf['OR'] = params\nconf.columns = ['2.5%', '97.5%', 'OR']\nnp.exp(conf)","11114b90":"sm.OLS(yM, XM).fit().summary() #all non-significant","0f03656f":"sm.OLS(zscore(yM), zscore(XM)).fit().summary() # as effect sizes, all non-significant","e969721f":"sm.OLS(zscore(yM), zscore(male.AveResBored)).fit().summary()","6a390d7a":"sm.OLS(zscore(yM), zscore(male.AveResEngcon)).fit().summary()","957f1d16":"sm.OLS(zscore(yM), zscore(male.AveResConf)).fit().summary()","0a4aa146":"sm.OLS(zscore(yM), zscore(male.AveResFrust)).fit().summary()","f02e8703":"sm.OLS(zscore(yM), zscore(male.AveResOfftask)).fit().summary()","92070438":"sm.OLS(zscore(yM), zscore(male.AveResGaming)).fit().summary()","ee4f8b52":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\ntrain_XM, val_XM, train_yM, val_yM = train_test_split(XM, yM, random_state=1)\nmy_modelM = RandomForestClassifier(random_state=0).fit(train_XM, train_yM)\npermM = PermutationImportance(my_modelM, random_state=1).fit(val_XM, val_yM)\neli5.show_weights(permM, feature_names = val_XM.columns.tolist())","c2177b8a":"import shap \n# Create object that can calculate shap values\nexplainerM = shap.TreeExplainer(my_modelM)\n\n# calculate shap values. This is what we will plot.\n# Calculate shap_values for all of val_X rather than a single row, to have more data for plot.\nshap_valuesM = explainer.shap_values(val_XM)\n\n# Make plot. Index of [1] is explained in text below.\nshap.summary_plot(shap_valuesM[1], val_XM)","c754f435":"# Male_ROC and AUC: Logit regression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nXM_train, XM_test, yM_train, yM_test = train_test_split(XM, yM, random_state=1)\n\nmodelM = LogisticRegression()\nmodelM.fit(XM_train, yM_train)\n\ny_predictM = model.predict(XM_test)\n\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\ny_predict_probabilitiesM = modelM.predict_proba(XM_test)[:,1]\n\nfprM, tprM, _ = roc_curve(yM_test, y_predict_probabilitiesM)\nroc_aucM = auc(fprM, tprM)\n\nplt.figure()\nplt.plot(fprM, tprM, color='darkorange',\n         lw=2, label='ROC curve (area = %0.3f)' % roc_aucM)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n","b3c1f851":"#Male_ROC and AUC: Random Forest\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\ntrain_XM, val_XM, train_yM, val_yM = train_test_split(XM, yM, random_state=1)\nmy_modelM = RandomForestClassifier(random_state=0).fit(train_XM, train_yM)\nval_yM = my_modelM.predict(train_XM)\n\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\ny_predict_probabilities2M = model.predict_proba(train_XM)[:,1]\n\nfpr2M, tpr2M, _ = roc_curve(val_yM, y_predict_probabilities2M)\nroc_auc2M = auc(fpr2M, tpr2M)\n\nplt.figure()\nplt.plot(fpr2M, tpr2M, color='darkorange',\n         lw=2, label='ROC curve (area = %0.3f)' % roc_auc2M)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n","22acb7ec":"#Writing a function to calculate the VIF values; https:\/\/statinfer.com\/204-1-9-issue-of-multicollinearity-in-python\/\nimport statsmodels.formula.api as sm\ndef vif_cal(input_data, dependent_col):\n    x_vars=input_data.drop([dependent_col], axis=1)\n    xvar_names=x_vars.columns\n    for i in range(0,xvar_names.shape[0]):\n        y=x_vars[xvar_names[i]] \n        x=x_vars[xvar_names.drop(xvar_names[i])]\n        rsq=sm.ols(formula=\"y~x\", data=x_vars).fit().rsquared  \n        vif=round(1\/(1-rsq),3)\n        print (xvar_names[i], \" VIF = \" , vif)#Calculating VIF values using that function","12dbfb68":"vif_cal(input_data=corrAll, dependent_col=\"isSTEM\") #all student data","6e7e223c":"vif_cal(input_data=corrF, dependent_col=\"isSTEM\") #female data","3f549118":"vif_cal(input_data=corrM, dependent_col=\"isSTEM\") #male data","e110c8c3":"# OLS regression for individual predictor_raw score","043e66ec":"## male_OLS regression for individual predictor_zscore","535490e0":"# correlation","7fc30841":"# male regression","470d37a2":"# female feature seleciton","4a832cec":"## female_Logit regression for individual predictor_raw score","d62a5417":"# Logit regression for individual predictor_raw score\n","889624ac":"# male correlation","823b55fd":"# male_OLS regression for all predictors together","bd73d969":"# group females vs. males","87e4557d":"# Logit regression for individual predictor_zscore(predictor)\n### negative pseudo r-squared; no significant regression coefficients","397c6ddc":"## ROC and AUC: Random Forest","97e395c3":"# random forest: feature selection","555d8bae":"# male feature seleciton","98f90831":"# female regression","9c186241":"# female correlation","e4cb1c2b":"# female_OLS regression for all predictors together","71730330":"# OLS regression for all predictors together\n### raw score of y and X: positive concentrating; negative gaming\n### zscore of both y and X as effect sizes; negative gaming: same results as logit regression","e952000e":"# Regression","cc835b1e":"# ROC and AUC: Logit regression","baf28a80":"# logit regression for all predictors","5ee86d77":"# multicollinearity checking VIF","3048f41c":"# data preparation","29ac0b9b":"# OLS regression for individual predictor_zscore\n","08faa5f8":"## female_OLS regression for individual predictor_zscore","ee7e17b8":"## male_Logit regression for individual predictor_raw score","20489ea6":"# Regression: seclect needed features","aa610650":"# Predicting STEM Choice by Affect and Behavior in Online Mathematical Problem-Solving: Gender and Methodology Differences\n### This kernel presents the code and results of data analysis, preparation, and exploration or binning on the data set provided [ASSISTments](http:\/\/www.assistments.org) project and its \" [ASSISTments Data Mining Competition 2017](http:\/\/sites.google.com\/view\/assistmentsdatamining\/data-mining-competition-2017)\" in the aspect of affect traits (or student-level affect and behavior).  \n### The kernels on affective states (or action-level affect and behaivor) are presented on the kernels of \n\"[AssistmentsAffectState_action-level data](https:\/\/www.kaggle.com\/meishiuchiu1\/assistmentsaffectstate-action-level-data)\", \"[AssistmentsAffectState_randomForestFeatureAll](http:\/\/www.kaggle.com\/meishiuchiu1\/assistmentsaffectstate-randomforestfeatureall)\", \"[AssistmentsAffectState_randomForestFeatureFemale](http:\/\/www.kaggle.com\/meishiuchiu1\/assistmentsaffectstate-randomforestfeaturefemale)\", and \"[AssistmentsAffectState_randomForestFeatureMale](http:\/\/www.kaggle.com\/meishiuchiu1\/assistmentsaffectstate-randomforestfeaturemale)\".\n","ece1b5fb":"# Regression: data preparation\nreferences\nhttps:\/\/github.com\/justmarkham\/scikit-learn-videos\/blob\/master\/09_classification_metrics.ipynb\nhttps:\/\/stackoverflow.com\/questions\/29763620\/how-to-select-all-columns-except-one-column-in-pandas\/29763653\n##missing data (no missing data needed in logistic regression)\nhttps:\/\/www.analyticsindiamag.com\/5-ways-handle-missing-values-machine-learning-datasets\/"}}