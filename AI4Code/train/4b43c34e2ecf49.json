{"cell_type":{"2b5316b3":"code","9ec613af":"code","cf530511":"code","64eae246":"code","dfb8a030":"code","e04eeb8a":"code","78cb24f5":"code","ed9ec3c5":"code","98b41ca0":"code","8cfb7171":"code","8d237ca6":"code","e6fa9083":"code","b46eca7d":"code","58799362":"code","5c526b7d":"code","e4f492f7":"code","9103a003":"code","e953ed54":"code","98300091":"code","ed823241":"code","8fbee8ef":"code","18effd30":"code","e04403dd":"code","a14baecf":"code","2445fd93":"code","5d8f10b1":"code","3b5f6729":"code","22ff656b":"code","b4c8379c":"code","b9fc2d0e":"code","67400bcc":"code","b651bc53":"code","37318913":"code","3cbb609d":"code","a0782c19":"code","4fd08941":"code","d5f56349":"code","71a59b01":"code","e911bfdd":"markdown","5f5e18f8":"markdown","77a9b606":"markdown","596e3e0e":"markdown","7ec21905":"markdown","b9b80e2f":"markdown","03167b0f":"markdown","e5e33ea6":"markdown","3177a89d":"markdown","6c631da9":"markdown","12867638":"markdown","3e0ed03c":"markdown","faa82c79":"markdown","ad16dbaf":"markdown","3d8b33bc":"markdown","c320c83f":"markdown","ba59f1f2":"markdown","81bf645b":"markdown","98716e36":"markdown","24ba08cd":"markdown","ae49ac39":"markdown","d52a09cf":"markdown","7e173797":"markdown","7fced3e5":"markdown","191e8e41":"markdown","0529f92a":"markdown","4f09c105":"markdown","d021f6ba":"markdown","28039ad0":"markdown","33f9a401":"markdown","af6f5eeb":"markdown","a44613f7":"markdown","794382f9":"markdown","62a43e13":"markdown","18ece4fd":"markdown","ed7262be":"markdown","f8e29a48":"markdown","bbe7a074":"markdown"},"source":{"2b5316b3":"import tensorflow as tf\nimport transformers\nfrom transformers import AutoTokenizer, TFAutoModel\n\nimport tensorflow_datasets as tfds\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import losses\n\nimport pandas as pd\npd.set_option('display.max_colwidth', 150)\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.colors import LogNorm, CenteredNorm\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils import class_weight\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nimport json\nimport shutil\nimport os\nimport zipfile\n\n# For nicer prints\nfrom IPython.display import display\n\ntf_version = tf.__version__\nprint(tf_version)","9ec613af":"!pip install tf-models-official==2.4.0","cf530511":"from official.nlp import optimization","64eae246":"try:\n    # Sync the TPU to the tensorflow version used in this notebook\n    from cloud_tpu_client import Client\n    c = Client()\n    c.configure_tpu_version(tf.__version__, restart_type='ifNeeded')\n    \n    \n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. If a GPU is available, it will be utilized with this strategy.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"Number of replicas: \", strategy.num_replicas_in_sync)","dfb8a030":"def load_data(file_path):\n    df = pd.read_json(file_path, lines = True)\n    df['category'] = pd.Categorical(df['category'])\n    return df[[\"headline\", \"category\"]]\n\ndataframe = load_data('\/kaggle\/input\/news-category-dataset\/News_Category_Dataset_v2.json')\ndataframe.head()","e04eeb8a":"def explore_data(df, number_of_bins=30, text_only=False):\n    stats = get_key_statistics_about_data(df)\n    print(\"Number of samples:\", stats[\"n_samples\"])\n    print(\"Number of classes:\", stats[\"n_classes\"])\n    print(\"Number of empty headlines:\", stats[\"n_empty_headlines\"])\n    print(\"Number of missing headlines:\", stats[\"n_missing_headlines\"])\n    print(\"Number of missing classes:\", stats[\"n_missing_classes\"])\n    print(\"Samples per class: Min:\", stats[\"min_n_samples_per_class\"], \"Median:\", stats[\"median_n_samples_per_class\"],\n         \"Max:\", stats[\"max_n_samples_per_class\"])\n    print(\"Words per sample: Min:\", stats[\"min_n_words_per_sample\"], \"Median:\", stats[\"median_n_words_per_sample\"],\n         \"Max:\", stats[\"max_n_words_per_sample\"])\n    print(\"Samples \/ Median number of words per sample:\", stats[\"n_samples\"] \/ stats[\"median_n_words_per_sample\"])\n    print()\n    display(df[\"category\"].value_counts().to_frame())\n    \n    if not text_only:\n        plot_class_distribution(df[\"category\"])\n\n        plot_word_count_distribution(df[\"headline\"], bins=number_of_bins)\n        plot_frequency_distribution_of_ngrams(df[\"headline\"])\n    \n    \ndef get_key_statistics_about_data(df):\n    ret = dict()\n    ret[\"n_samples\"] = len(df)\n    ret[\"n_classes\"] = len(df[\"category\"].unique())\n    ret[\"n_missing_headlines\"] = df[\"headline\"].isnull().sum()\n    ret[\"n_empty_headlines\"] = len(df[df[\"headline\"] == \"\"])\n    ret[\"n_missing_classes\"] = df[\"category\"].isnull().sum()\n    number_of_words_per_sample = df[\"headline\"].apply(lambda x: len(x.split()))\n    ret[\"n_words_per_sample\"] = number_of_words_per_sample\n    ret[\"min_n_words_per_sample\"] = number_of_words_per_sample.min()\n    ret[\"median_n_words_per_sample\"] = number_of_words_per_sample.median()\n    ret[\"max_n_words_per_sample\"] = number_of_words_per_sample.max()\n    number_of_samples_per_class = df[\"category\"].value_counts()\n    ret[\"min_n_samples_per_class\"] = number_of_samples_per_class.min()\n    ret[\"median_n_samples_per_class\"] = number_of_samples_per_class.median()\n    ret[\"max_n_samples_per_class\"] = number_of_samples_per_class.max()\n    \n    return ret\n\n\n# inspired by https:\/\/github.com\/google\/eng-edu\/blob\/main\/ml\/guides\/text_classification\/explore_data.py\ndef plot_class_distribution(labels):\n    counts = labels.value_counts()\n    plt.figure(figsize=(16, 8))\n    plt.bar(counts.index, counts, width=0.8, color='b')\n    plt.xlabel('Class')\n    plt.ylabel('Number of samples')\n    plt.title('Class distribution')\n    plt.xticks(counts.index, counts.index, rotation=90)\n    plt.show()\n    \n\ndef plot_word_count_distribution(sample_texts, bins=30):\n    plt.figure(figsize=(16, 8))\n    plt.hist([len(s.split()) for s in sample_texts], bins)\n    plt.xlabel('Word count')\n    plt.ylabel('Number of samples')\n    plt.title('Word count distribution')\n    plt.show()\n\n\ndef plot_frequency_distribution_of_ngrams(sample_texts,\n                                          ngram_range=(1, 2),\n                                          num_ngrams=50):\n    # Create args required for vectorizing.\n    kwargs = {\n            'ngram_range': (1, 1),\n            'dtype': 'int32',\n            'strip_accents': 'unicode',\n            'decode_error': 'replace',\n            'analyzer': 'word',  # Split text into word tokens.\n    }\n    vectorizer = CountVectorizer(**kwargs)\n\n    # This creates a vocabulary (dict, where keys are n-grams and values are\n    # idxices). This also converts every text to an array the length of\n    # vocabulary, where every element idxicates the count of the n-gram\n    # corresponding at that idxex in vocabulary.\n    vectorized_texts = vectorizer.fit_transform(sample_texts)\n\n    # This is the list of all n-grams in the index order from the vocabulary.\n    all_ngrams = list(vectorizer.get_feature_names())\n    num_ngrams = min(num_ngrams, len(all_ngrams))\n    # ngrams = all_ngrams[:num_ngrams]\n\n    # Add up the counts per n-gram ie. column-wise\n    all_counts = vectorized_texts.sum(axis=0).tolist()[0]\n\n    # Sort n-grams and counts by frequency and get top `num_ngrams` ngrams.\n    all_counts, all_ngrams = zip(*[(c, n) for c, n in sorted(\n        zip(all_counts, all_ngrams), reverse=True)])\n    ngrams = list(all_ngrams)[:num_ngrams]\n    counts = list(all_counts)[:num_ngrams]\n\n    idx = np.arange(num_ngrams)\n    plt.figure(figsize=(16, 8))\n    plt.bar(idx, counts, width=0.8, color='b')\n    plt.xlabel('N-grams')\n    plt.ylabel('Frequencies')\n    plt.title('Frequency distribution of n-grams')\n    plt.xticks(idx, ngrams, rotation=45)\n    plt.show()","78cb24f5":"explore_data(dataframe)","ed9ec3c5":"dataframe = dataframe[dataframe[\"headline\"] != \"\"]\n\ndef combine_categories(category):\n    # THE WORLDPOST becomes WORLD NEWS\n    if category == \"THE WORLDPOST\":\n        return \"WORLD NEWS\"\n    # WORLDPOST becomes WORLD NEWS\n    if category == \"WORLDPOST\":\n        return \"WORLD NEWS\"\n    # ARTS becomes ARTS & CULTURE\n    if category == \"ARTS\":\n        return \"ARTS & CULTURE\"\n    # CULTURE & ARTS becomes ARTS & CULTURE\n    if category == \"CULTURE & ARTS\":\n        return \"ARTS & CULTURE\"\n    # HEALTHY LIVING becomes WELLNESS\n    if category == \"HEALTHY LIVING\":\n        return \"WELLNESS\"\n    # PARENTING becomes PARENTS\n    if category == \"PARENTING\":\n        return \"PARENTS\"\n    # STYLE becomes STYLE & BEAUTY\n    if category == \"STYLE\":\n        return \"STYLE & BEAUTY\"\n    # GREEN becomes ENVIRONMENT\n    if category == \"GREEN\":\n        return \"ENVIRONMENT\"\n    # TASTE becomes FOOD & DRINK\n    if category == \"TASTE\":\n        return \"FOOD & DRINK\"\n    # MONEY becomes BUSINESS\n    if category == \"MONEY\":\n        return \"BUSINESS\"\n    return category\n\ndataframe[\"category\"] = dataframe[\"category\"].map(combine_categories)\ndataframe['category'] = pd.Categorical(dataframe['category'])","98b41ca0":"explore_data(dataframe, text_only=False)","8cfb7171":"display(dataframe[dataframe[\"category\"] == \"FIFTY\"])","8d237ca6":"display(dataframe[dataframe[\"category\"] == \"IMPACT\"])","e6fa9083":"display(dataframe[dataframe[\"category\"] == \"WOMEN\"])","b46eca7d":"display(dataframe[dataframe[\"category\"] == \"WELLNESS\"])","58799362":"def codify_labels(df):\n    label = dict( zip( df['category'].cat.codes, df['category'] ) )\n    category_code = dict( zip( df['category'], df['category'].cat.codes ) )\n    df['category'] = df.category.cat.codes\n    return label, category_code\n\nlabel, category_code = codify_labels(dataframe)\ndisplay(label)","5c526b7d":"categories = dataframe[\"category\"].unique()\ndisplay(categories)","e4f492f7":"def split_dataset(df, seed=42, percentage_train=0.8, percentage_validation=0.15, percentage_test=0.05):\n    assert percentage_train + percentage_validation + percentage_test == 1\n\n    dataset_size = len(df)\n    \n    headlines_train, headlines_rest, categories_train, categories_rest = train_test_split(df[\"headline\"], df[\"category\"], random_state=seed, test_size=0.2)\n    headlines_validation, headlines_test, categories_validation, categories_test = train_test_split(headlines_rest, categories_rest, random_state=seed, test_size=0.25)\n\n    train_df = pd.concat([categories_train, headlines_train], axis=1)\n    train_df.columns = [\"category\", \"headline\"]\n    validation_df = pd.concat([categories_validation, headlines_validation], axis=1)\n    validation_df.columns = [\"category\", \"headline\"]\n    test_df = pd.concat([categories_test, headlines_test], axis=1)\n    test_df.columns = [\"category\", \"headline\"]\n    \n    return train_df, validation_df, test_df\n\ntrain_df, validation_df, test_df = split_dataset(dataframe)\n\ndisplay(train_df.head())","9103a003":"class_weights = dict(zip(train_df[\"category\"].unique(), class_weight.compute_class_weight('balanced', train_df[\"category\"].unique(), train_df[\"category\"])))\n# class_weights = {category: 1.0 for category in train_df[\"category\"]}\n# class_weights[category_code[\"WELLNESS\"]] = 0.2","e953ed54":"display({label[category_code]: weight for category_code, weight in class_weights.items()})","98300091":"pretrained_model = \"roberta-large\"","ed823241":"class TextEncoder:\n    def __init__(self, max_length=70):\n        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n        self.max_length = max_length\n    \n    def encode(self, texts):\n        texts = texts.apply(lambda s: s.lower())\n        encoded_texts = self.tokenizer.batch_encode_plus(list(texts), padding='max_length', \n                                                     truncation=True, max_length=self.max_length, \n                                                     add_special_tokens=True,\n                                                     return_token_type_ids=False,\n                                                     return_attention_mask=True)\n        return np.array(encoded_texts[\"input_ids\"]), np.array(encoded_texts[\"attention_mask\"])\n        \nmax_length = 70\nencoder = TextEncoder(max_length=max_length)","8fbee8ef":"train_ids, train_attention_mask = encoder.encode(train_df[\"headline\"])\nvalidation_ids, validation_attention_mask = encoder.encode(validation_df[\"headline\"])\ntest_ids, test_attention_mask = encoder.encode(test_df[\"headline\"])","18effd30":"def compose_dataset(word_ids, attention_masks, categories):\n    ids_ds = tf.data.Dataset.from_tensor_slices(word_ids)\n    attention_mask_ds = tf.data.Dataset.from_tensor_slices(attention_masks)\n    y_ds = tf.data.Dataset.from_tensor_slices(categories)\n    x_ds = tf.data.Dataset.zip((ids_ds, attention_mask_ds))\n    return tf.data.Dataset.zip((x_ds, y_ds))\n\ntrain_ds = compose_dataset(train_ids, train_attention_mask, train_df[\"category\"].values)\nvalidation_ds = compose_dataset(validation_ids, validation_attention_mask, validation_df[\"category\"].values)\ntest_ds = compose_dataset(test_ids, test_attention_mask, test_df[\"category\"].values)","e04403dd":"if tpu:\n    batch_size = 32 * strategy.num_replicas_in_sync\nelse:\n    batch_size = 32\n\nbatch_train_ds = train_ds.batch(batch_size)\nbatch_validation_ds = validation_ds.batch(batch_size)\nbatch_test_ds = test_ds.batch(batch_size)\n    \nAUTOTUNE = tf.data.AUTOTUNE\n\nbatch_train_ds = batch_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\nbatch_validation_ds = batch_validation_ds.cache().prefetch(buffer_size=AUTOTUNE)\nbatch_test_ds = batch_test_ds.cache().prefetch(buffer_size=AUTOTUNE)","a14baecf":"class ModelBuilder:\n    def __init__(self, init_lr, num_train_steps):\n        self.metric = [tf.metrics.SparseCategoricalAccuracy(), tf.metrics.SparseTopKCategoricalAccuracy(3)]\n        num_warmup_steps = int(0.1*num_train_steps)\n        self.optimizer = optimization.create_optimizer(init_lr=init_lr,\n                                          num_train_steps=num_train_steps,\n                                          num_warmup_steps=num_warmup_steps,\n                                          optimizer_type='adamw')\n        \n    def build_model(self):\n        auto_model = transformers.TFAutoModel.from_pretrained(pretrained_model, return_dict=True)\n        \n        word_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='word_ids')\n        attention_masks = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_masks')\n        pooler_output = auto_model(input_ids=word_ids, attention_mask=attention_masks)[\"pooler_output\"]\n        drop_out = tf.keras.layers.Dropout(0.1)(pooler_output)\n        output = tf.keras.layers.Dense(len(categories), activation='softmax')(drop_out)\n        model = tf.keras.Model(inputs=[word_ids, attention_masks], outputs=output)\n        \n        model.compile(self.optimizer, metrics=self.metric, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False))\n        return model\n    \n    def build_sequence_classifier_model(self):\n        auto_model = transformers.TFAutoModelForSequenceClassification.from_pretrained(pretrained_model, return_dict=True, num_labels=len(categories))\n        \n        word_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='word_ids')\n        attention_masks = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_masks')\n        output = auto_model(input_ids=word_ids, attention_mask=attention_masks)[\"logits\"]\n        model = tf.keras.Model(inputs=[word_ids, attention_masks], outputs=output)\n        \n        model.compile(self.optimizer, metrics=self.metric, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n        return model","2445fd93":"model_name = \"roberta_large_e5_b32_seq\"","5d8f10b1":"epochs = 5\n\nwith strategy.scope():\n    steps_per_epoch = tf.data.experimental.cardinality(batch_train_ds).numpy()\n    num_train_steps = steps_per_epoch * epochs\n    model_builder = ModelBuilder(3e-5, num_train_steps)\n    model = model_builder.build_sequence_classifier_model()\nmodel.summary()","3b5f6729":"# Set this flag if you do not want to load the model weights\nload_weights = True\n\nmodel_loaded = False\nif load_weights:\n    if os.path.isdir(\"\/kaggle\/input\/\" + model_name.replace(\"_\", \"-\")): \n        with strategy.scope():\n            load_locally = tf.saved_model.LoadOptions(experimental_io_device='\/job:localhost')\n            model.load_weights(\"\/kaggle\/input\/\" + model_name.replace(\"_\", \"-\") + \"\/\" + model_name, options=load_locally)\n            model_loaded = True\n    elif os.path.isdir(\"\/kaggle\/working\/models\/\" + model_name): \n        with strategy.scope():\n            load_locally = tf.saved_model.LoadOptions(experimental_io_device='\/job:localhost')\n            model.load_weights(\"\/kaggle\/working\/models\/\" + model_name + \"\/\" + model_name, options=load_locally)\n            model_loaded = True\n        \nif model_loaded:\n    print(\"Successfully loaded model!\")","22ff656b":"if not model_loaded:\n    steps_per_epoch = tf.data.experimental.cardinality(batch_train_ds).numpy()\n    num_train_steps = steps_per_epoch * epochs\n    num_warmup_steps = int(0.1*num_train_steps)\n\n    history = model.fit(x=batch_train_ds,\n                        validation_data=batch_validation_ds,\n                        epochs=epochs)\n                        # class_weight=class_weights)","b4c8379c":"# Flag to control if the model should be saved\nsave_model = False\n\nif not model_loaded and save_model:\n    if not os.path.isdir(\"\/kaggle\/working\/models\"):\n        os.mkdir(\"\/kaggle\/working\/models\")\n    if not os.path.isdir(\"\/kaggle\/working\/models\/\" + model_name):\n        os.mkdir(\"\/kaggle\/working\/models\/\" + model_name)\n    save_locally = tf.saved_model.SaveOptions(experimental_io_device='\/job:localhost')\n    # Just saving the weights since loading a saved model with TFBert in it throws an error\n    model.save_weights('.\/models\/' + model_name + \"\/\" + model_name, options=save_locally)\n    shutil.make_archive(model_name, 'zip', '\/kaggle\/working\/models\/' + model_name)","b9fc2d0e":"def plot_graphs(history, metric):\n    plt.plot(history.history[metric])\n    plt.plot(history.history['val_' + metric], '')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(metric)\n    plt.legend([metric, 'val_' + metric])\n\nif not model_loaded:\n    plt.figure(figsize=(16, 8))\n    plt.subplot(1, 2, 1)\n    plot_graphs(history, 'sparse_categorical_accuracy')\n    plt.ylim(None, 1)\n    plt.subplot(1, 2, 2)\n    plot_graphs(history, 'loss')\n    plt.ylim(0, None)","67400bcc":"loss, accuracy, top_k_accuracy = model.evaluate(batch_test_ds)\n\nprint(\"Loss: \", loss)\nprint(\"Accuracy: \", accuracy)\nprint(\"Top-k accuracy: \", top_k_accuracy)","b651bc53":"predictions = model.predict(batch_test_ds,verbose = 1)\npredicted_categories = [label[np.argmax(prediction)] for prediction in predictions]\ntrue_categories = []\nfor _, categories in batch_test_ds:\n    for category in categories:\n        true_categories.append(label[category.numpy()])","37318913":"result_df = pd.DataFrame({'description':test_df[\"headline\"],'true_category':true_categories, 'predicted_category':predicted_categories})\ndisplay(result_df)","3cbb609d":"wrong_predictions = result_df[result_df['true_category'] != result_df['predicted_category']]\nwrong_predictions.to_csv(model_name + \"_wrongs.csv\", index=False)\ndisplay(wrong_predictions)","a0782c19":"confusion_mat = confusion_matrix(y_true = true_categories, y_pred = predicted_categories, labels=list(label.values()))","4fd08941":"df_cm = pd.DataFrame(confusion_mat, index = list(label.values()), columns = list(label.values()))\n# plt.rcParams['figure.figsize'] = (20,20)\ncm = sns.color_palette(\"rocket\", as_cmap=True)\ncm.set_bad(\"black\")\nheatmap = sns.heatmap(df_cm, norm=LogNorm(), cmap=cm, xticklabels=True, yticklabels=True)\nfig = plt.gcf()\nfig.set_size_inches((15, 11))\nfig.savefig(model_name + \"_confusion_heatmap.pdf\")\nplt.show()","d5f56349":"wrong_predictions[(wrong_predictions[\"true_category\"] == \"POLITICS\") & (wrong_predictions[\"predicted_category\"] == \"WOMEN\")]","71a59b01":"wrong_predictions[(wrong_predictions[\"true_category\"] == \"PARENTS\") & (wrong_predictions[\"predicted_category\"] == \"WELLNESS\")]","e911bfdd":"# Modeling\n\n## Building the model\n\nWe are going to have two approaches. The first one uses a `TFAutoModel` which returns a pooled output of size `(batch_size, hidden_layer_size)`. We then append a dropout layer and a dense layer with the size equal to the number of caetgories. For the second approach we directly utilize `TFAutoModelForSequenceClassification` which does not require any trailing layers to do a classification. \n\nAs we will later see, some categories can be easily confused because some headlines reasonably match multiple categories. That is the reason why we also add the top-3 accuracy metric. The metric measures if the true class is one of the 3 most probable predicted classes. Clearly, we expect this metric to have much higher accuracy than the standard accuracy metric. We use it to get more insight into the behavior of the trained classifier.","5f5e18f8":"## Creating tensorflow datasets\n\nUntil now we only have worked on the pandas dataframe. For using the data with tensorflow we have to create tensorflow datasets. The datasets have three components: the input ids, the corresponding attention masks, and the class labels belonging to the respective headlines.","77a9b606":"Let's read some headline of certain categories to better understand what kind of articles the categories comprise.\n\n### FIFTY category","596e3e0e":"## Confusion matrix\n\nWe want to better understand where mistakes are being made. For that reason, we generate a confusion matrix and plot the matrix in a heatmap.","7ec21905":"We are mainly interested in the mistakes the classifier does. As such, we collect the mistakes, display them, and save them in a CSV-file.","b9b80e2f":"# Evaluation\n## Training history","03167b0f":"Most of the wrongly predicted headlines in this sample are concerned with the health and life balance of parents and children. Hence, it's not too bad to predict wellness in those cases. \n\nO. Fuks makes the same observation in the report [Classification of News Dataset](http:\/\/cs229.stanford.edu\/proj2018\/report\/183.pdf). She reports that \n\n> Often there is some combination of categories present in one news, though it has just\none \"true\" label in the dataset. Example 1: \u201cAustralian Senator Becomes First To Breastfeed On\nParliament Floor \"We need more women and parents in Parliament,\" said Larissa Waters.\u201d - here\nthe true category \"Parents\" was confused by the models with \"World news\", probably as it mentions\nAustralia and senator but the news is also about parenthood. Example 2: \"Most U.S. Troops Kicked\nOut For Misconduct Had Mental Illness. The new report will likely add to scrutiny over whether\nthe military is doing enough to care for troops with mental health issues\" - this news belongs to\nthe \"Healthy Living\" category whereas the models identify it as \"Politics\" likely because the news\nmention US troops and military, however it is mainly about health issues.\n\nand \n\n> Overlap of different categories - we believe this may be due to the subjective assignment of\nthe category upon news publication. Example: \"How Do Scientists Study Dreams? Dreams are a\ncompelling area of research for scientists, in part because there\u2019s still so much to learn about how,\nand why, we dream. \" - this news belongs for some reason to \"Healthy Living\" category, though it\nmentions a lot about scientific research, so there is no surprise that all models identify it as category\n\"Science\".","e5e33ea6":"The category `FIFTY` mostly includes headlines aiming for people of age 50.\n\n### IMPACT category","3177a89d":"The category contains a lot of headlines concerning sexual harrassment or headlines concerning extraordinary women.\n\n### WELLNESS category","6c631da9":"# Conclusion\n\nWe can see that we have problems with the two largest categories `POLITICS` and `WELLNESS`. Additionally, the two categories blur with other categories or are outright super-categories of other categories. For example there are headlines which are clearly concerned with women rights, however they have the true label `POLITICS`. There are multiple of such examples. In particular, we can observe that many headlines allow for multiple reasonable classifications. This presumption is supported by the fact that the top-3 accuracy metric shows a significantly higher value. It means that the classifier recognizes that multiple categories can fit a headline and that it can identify them. However, it loses accuracy when having to choose from the candidate categories. ","12867638":"We now store the unique categories in a variable.","3e0ed03c":"# News classification with pre-trained transformers based on BERT\n\nIn this notebook we are going to train a BERT model (and its derivatives) for classifying news headlines. Even though the dataset has more information, we restrict ourselves to use the headlines as inputs.\n\nIn order to get access to the BERT models we are going to use the [huggingface transformer library](https:\/\/huggingface.co\/transformers\/).\n\nCredits to Aayush Jain [https:\/\/www.kaggle.com\/foolofatook\/news-classification-using-bert](https:\/\/www.kaggle.com\/foolofatook\/news-classification-using-bert) for his excellent work. Some of his code and approaches were incorporated into this notebook.\n\n# Setup\n## Imports\nFirst let us import all the necessary modules.","faa82c79":"## Inspecting wrong predictions\n\nWe first collect the predictions of the classifier and the corresponding true labels.","ad16dbaf":"## Computing class weights\n\nAs explained in the beginning, the distribution of the classes is imbalanced. Class weights can mitigate this effect. However, it showed that accuracy decreased using class weights and that no major changes in the confusion matrix could be detected. The issue of the imbalanced classes should be explored more thoroughly. ","3d8b33bc":"For the heatmap we use a log-scale to better see where our classifier makes the most mistakes.","c320c83f":"It's difficult to get a feeling for this category. Some headlines are political, some are strongly addressing the reader and others are focusing on extraordinary people. \n\n### WOMEN category","ba59f1f2":"The `WELLNESS` category contains health advices but could potentially be confused with `FOOD & DRINK`.","81bf645b":"## Dividing the dataset into batches\n\nAn important hyperparameter for our network is the batch size. In case of a stochastic gradient method we have that for small batch sizes the approximation of the gradient can be quickly computed. However, the approximation can be inaccurate and can have the effect that more gradient steps are required to reach a local minimum. On the other hand, having large batches the computation of the approximation of the gradient is computational intense, but we require less steps to reach a local minimum. Those trade-offs do not take into accounts the effects on the predictive quality. It turns out that large batch sizes can also reduce the predictive quality of the model. \n\nFor more information read the discussion [here](https:\/\/stats.stackexchange.com\/questions\/164876\/what-is-the-trade-off-between-batch-size-and-number-of-iterations-to-train-a-neu).\n\nBeside choosing the batch size, we also tune I\/O for the dataset.","98716e36":"## Saving the model weights","24ba08cd":"# Training\nAs the computed class weights were not helpful we will not use them.","ae49ac39":"Next, we name our model...","d52a09cf":"Hence, it might be impossible for a classifier to learn the difference. Using class weights to overcome the imbalancement did not help with this issue. Since we are dealing with a hierachy of categories, the class weights would push the classifier to rather predict for example `WOMEN` than `POLITICS` when in doubt. Since we have way more `POLITICS` labels, in such a case it would be wiser to predict `POLITICS` if the prediction is unclear. In our sample above we see cases where the classifier's prediction reasonably matches the `WOMEN` category even though `POLITICS` would be the correct label.  \n\nAnother major confusion is between `WELLNESS` and `PARENTS`:","7e173797":"## Assigning numeric values to the categories\n\nInstead of using the human-readable categories we translate them to numeric labels.","7fced3e5":"The confusion matrix is nearly symmetric. Only the `WELLNESS` category tends to be slightly overpredicted. The near symmetry of the confusion matrix shows that the imbalanced dataset is not a major problem. This is in line with the observation that adding class weights to the model did not have a major impact. ","191e8e41":"## Splitting the data into training, validation, and test data\n\nWe split the dataset into training, validation, and test data. We use a 80\/15\/5 split. For the splitting we apply the `sklearn` method `train_test_split` two times.","0529f92a":"## Load the data into a pandas dataframe\nWe next load the data from the provided JSON into a pandas dataframe. Note, that loading the file with the module `json` raises an error (see the [discussion](https:\/\/www.kaggle.com\/rmisra\/news-category-dataset\/discussion\/191283)).","4f09c105":"### Conclusion of our data inspection\nThe take aways are:\n- We have empty headlines. There are two ways of dealing with missing data.\n\n    1. We could impute missing data\n    2. We could delete missing data\n\n  Imputation can be a difficult problem in itself and usually we would choose imputation if we had to deal with a lot of missing data. Since we only have 6 missing headlines we can simply delete the 6 units.\n- Many categories are synonymous or at least equivalent. We do the following mapping\n\n| From | To |\n| --- | --- |\n| THE WORLDPOST | WORLD NEWS |\n| WORLDPOST | WORLD NEWS |\n| ARTS | ARTS & CULTURE |\n| CULTURE & ARTS | ARTS & CULTURE |\n| HEALTHY LIVING | WELLNESS |\n| PARENTING | PARENTS |\n| STYLE | STYLE & BEATUY |\n| GREEN | ENVIRONMENT |\n| TASTE | FOOD & DRINK |\n| MONEY | BUSINESS |\n    \n- We have imbalanced data. I approached this issue by computing class weights, i.e., penalizing miss-classifications for small categories more severely than large categories. The problem with that was that we do not have disjunct categories. In fact, many categories like `WOMEN`, `BLACK VOICES`, `QUEER VOICES`, etc. can be considered sub-categories of`POLITICS`. Furthermore, many headlines, which I would categorize as one of the sub-categories, are classified as `POLITICS`. From a human point-of-view both classifications would be correct. However, for the standard accuracy measurement it is considered an error. Now imagine there are a lot of such cases and we would nudge our classifier to prefer the sub-category if in doubt. Then we would get a worse accuracy as the `POLITICS` category is more prevelant and more often the correct class. ","d021f6ba":"# Data pre-processing\n## Exploring the data\nInspecting and exploring our data is majorly important. We have to look out for the following issues:\n- Missing data\n- Synonymous categories, e.g., `ARTS & CULTURE` and `CULTURE & ARTS`\n- Imbalanced distribution of categories\n\n[Google](https:\/\/developers.google.com\/machine-learning\/guides\/text-classification\/step-2) provides a helpful guide for exploring text data.","28039ad0":"Plot the model history.","33f9a401":"## Encoding the texts with AutoTokenizer corresponding to the chosen pre-trained model\n\nThe tokenizer transforms the raw text into lists of integers, which can be understood by the transformer. The function `batch_encode_plus` is used to encode all headlines at once such that we can get rid of some Python overhead. We pass the following parameters to the encoder:\n\n- Our headlines as a list of strings\n- padding: In our model the transformer must get tensors of same dimensions. Hence, we have to pad the tokenized sentenced with zeros. We choose a default maximal length of 70 tokens. From the data exploration we know that the longest sentence has 44 words. Since the tokenizer also adds special tokens and since we might use longer headlines when the model is deployed, we choose the maximal length of 70 tokens per sentence.\n- truncation: In case we pass a sentence which is too long, we cut it off.\n- add_special_tokens: We like the tokenizer to add special tokens like a token for the start or the end of a sentence.\n- return_token_type_ids: We are not using token_type_ids, thus we do not need to return them. They are typically used to mark questions and answers, which we are not doing here.\n- return_attention_mask: We have different sized sentences and hence we want to draw the attention away from the 0 tokens, which are used for padding. That's why we want to have the attention mask.\n\nLook [here](https:\/\/huggingface.co\/transformers\/glossary.html) for glossary of the different terms like attention mask, token type id, etc.","af6f5eeb":"Then we create a pandas dataframe comparing prediction and true response.","a44613f7":"## TPU\/GPU Settings\nBefore working with the data we setup the TPU or GPU. It is highly recommended to activate a TPU for this notebook.","794382f9":"... and build it.","62a43e13":"Again we inspect the data.","18ece4fd":"As specified [here](https:\/\/www.tensorflow.org\/text\/tutorials\/classify_text_with_bert#optimizer) the AdamW solver is recommended for BERT. For that we need the `tf-models-official` library. Hence, we need to install it and afterwards import it. Note that we need to install a version matching the installed tensorflow version.","ed7262be":"## Test accuracy\nCompute and display the loss, accuracy and top-3 accuracy of the out-of-sample test set.","f8e29a48":"# Model building\n## Choosing a pretrained model\n\nWe choose a BERT model from [here](https:\/\/huggingface.co\/transformers\/pretrained_models.html). Some results for different models are listed below. \n\n- roberta-large:\n    - (BATCH_SIZE: 16 * 8 TPU cores) yields 73.36% test accuracy\n    - (BATCH_SIZE: 32 * 8 TPU cores) yields 74.26% test accuracy\n    - (BATCH_SIZE: 64 * 8 TPU cores) yields 73.83% test accuracy\n- distilbert-base-uncased:\n    - (BATCH_SIZE: 16 * 8 TPU cores) yields 71.09% test accuracy","bbe7a074":"## Loading available fine-tuned models\n\nI fine-tuned some models for this notebook, which are available as an input. If a model with the same name is saved in the working directory or in the input files, it is loaded.\n\nUnfortunately, straight-up saving the model produces an error when loading again, probably due to a bug with the huggingface transformer. Since I was unable to fix the issue we just save and load the weights. "}}