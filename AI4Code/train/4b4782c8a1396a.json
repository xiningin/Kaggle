{"cell_type":{"65557d9b":"code","205f27ff":"code","1475594f":"code","ade163f7":"code","41ce7f52":"code","e4214fc1":"code","168bc0c5":"code","0ae2cf94":"code","59a502ff":"code","4cf63b6c":"code","7b8a4c9a":"code","9fe0baf0":"code","a1ea9420":"code","117946b9":"code","8ac924df":"code","728b0c12":"code","06cab43e":"code","92ce3470":"code","59821e90":"code","60ae7aeb":"code","5908c525":"code","1eb5d1b9":"code","0b567360":"code","566e7267":"code","d5b08892":"code","fa9f0d59":"code","02675b8d":"markdown","81b954a8":"markdown","e4e29bdb":"markdown","67d25fbf":"markdown","0f6917d7":"markdown","3eb651cf":"markdown","e1fa2ff2":"markdown","01e259d3":"markdown","eee6ee22":"markdown"},"source":{"65557d9b":"import pandas as pd\nimport numpy as np\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, matthews_corrcoef, f1_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\n\n%matplotlib inline","205f27ff":"# Assign current path to c_path\nc_path = '..\/input'","1475594f":"my_df = pd.read_csv(f'{c_path}\/adult.csv')\n\nprint(my_df.shape)\nmy_df.head()","ade163f7":"def mydf_splitter(my_df, num_rows):\n    return my_df[:num_rows].copy(), my_df[num_rows:].copy()\n\nmydf_train_valid, mydf_test = mydf_splitter(my_df, 30000)\n\nprint(mydf_train_valid.shape, mydf_test.shape)","41ce7f52":"# Get a general picture of mydf_train_valid\nmydf_train_valid.info()","e4214fc1":"'''Convert data in columns from 'object' type to 'category' type'''\n\ndef str_to_cat(my_df):\n    for p, q in my_df.items(): \n        if is_string_dtype(q): \n            my_df[p] = q.astype('category').cat.as_ordered()\n    return my_df\n\nmydf_train_valid = str_to_cat(mydf_train_valid)\n\nmydf_train_valid.info()","168bc0c5":"# Check information of some 'category' type columns\nprint(mydf_train_valid[\"workclass\"].cat.categories)\nprint(mydf_train_valid[\"education\"].cat.categories)\nmydf_train_valid.head(3)","0ae2cf94":"'''Convert data in these 'category' type culumns to their corresponding numerical values'''\n\ndef cat_to_num(my_df):\n    for p, q in my_df.items():\n        if not is_numeric_dtype(q):\n            my_df[p] = q.cat.codes\n    return my_df\n            \nmydf_train_valid = cat_to_num(mydf_train_valid)\n\nmydf_train_valid.head()","59a502ff":"\"\"\"Seperate data into the X and Y varibles\"\"\"\n\nY_train_valid = mydf_train_valid[\"income\"]\nX_train_valid = mydf_train_valid.drop([\"income\"], axis=1)\n\nprint(X_train_valid.shape, Y_train_valid.shape)\n\nY_train_valid.unique()","4cf63b6c":"\"\"\"X variable columns might be a continuous variable column or a categorical \nvariable column. Seperate into continuous variables and categorical variables\"\"\"\n\nX_train_valid_cat = X_train_valid[[\"workclass\", \"education\", \"marital.status\", \"occupation\", \n                                   \"relationship\", \"race\", \"sex\", \"native.country\"]]\nX_train_valid_con = X_train_valid.drop(X_train_valid_cat, axis=1)\n\nprint(X_train_valid_cat.shape, X_train_valid_con.shape)","7b8a4c9a":"'''Scale the continuous variables. To standardize (includes scaling), \nwe subtract mean of that column from every value, then divide the results \nby the variable's standard deviation'''\n\nscaler = preprocessing.StandardScaler().fit(X_train_valid_con)\nX_train_valid_con_sc = pd.DataFrame(scaler.transform(X_train_valid_con))\nX_train_valid_con_sc.columns = [\"age\",\"fnlwgt\", \"education.num\", \"capital.gain\", \"capital.loss\", \"hours.per.week\"]\n\nprint(X_train_valid_con_sc.shape)\nX_train_valid_con_sc.head()","9fe0baf0":"df_list = [X_train_valid_cat, X_train_valid_con_sc]\nX_full = pd.concat(df_list, axis = 1)\n\nprint(X_full.shape)\nX_full.head()","a1ea9420":"X_train, X_valid = mydf_splitter(X_full, 27500)\nY_train, Y_valid = mydf_splitter(Y_train_valid, 27500)\n\nprint(X_train.shape, X_valid.shape, Y_train.shape, Y_valid.shape)","117946b9":"\"\"\"Use multiple for-loops to search for the best combination of parameters for kNN\"\"\"\n\nparams = {'n_neighbors': [k for k in range(1, 20, 2)],\n        'weights': ['uniform', 'distance'],\n        'metric': ['manhattan', 'euclidean']}\n\nnum_neighs = list()\nval_weights = list()\nval_metric = list()\naccuracy_list = list()\n\nfor n in params[\"n_neighbors\"]:\n    for w in params[\"weights\"]:\n        for m in params[\"metric\"]:\n                my_knn_model = KNeighborsClassifier(n_neighbors=n, weights=w, metric=m)\n                my_knn_model.fit(X_train, Y_train)\n                Y_pred = my_knn_model.predict(X_valid)\n                accuracy = accuracy_score(Y_valid, Y_pred)\n                num_neighs.append(n)\n                val_weights.append(w)\n                val_metric.append(m)\n                accuracy_list.append(accuracy)\n            ","8ac924df":"eval_df =  pd.DataFrame({\"n_neighbors\": num_neighs, \"weights\": val_weights, \n                         \"metric\": val_metric, \"accuracy score\": accuracy_list})\neval_df.index = eval_df.index + 1\neval_df.index.name = \"No.\"\n\neval_df","728b0c12":"#Plot accuracy Vs No.\nplt.figure(figsize=(8, 5), dpi=80)\nplt.xticks(np.arange(1,  41,  1))\nplt.scatter(eval_df.index, eval_df[\"accuracy score\"], marker='+')","06cab43e":"\"\"\"We then use the best combination of parameters- \n{n_neighbors=19, weights='uniform', metric='euclidean'} to train our kNN model\"\"\" \n\nmy_knn_model_final = KNeighborsClassifier(n_neighbors=19, weights='uniform', metric='euclidean')\nmy_knn_model_final.fit(X_full, Y_train_valid)","92ce3470":"'''Before we can apply our kNN model on the test set, we\nneed to preprocess the test set in exactly the same way we did the\ntrain-valid set'''\n\nprint(mydf_test.shape)\nmydf_test.head()","59821e90":"mydf_test = str_to_cat(mydf_test)\n\nmydf_test.info()","60ae7aeb":"mydf_test = cat_to_num(mydf_test)\n\nmydf_test.head()","5908c525":"Y_test = mydf_test[\"income\"]\nX_test = mydf_test.drop([\"income\"], axis=1)\n\nprint(X_test.shape, Y_test.shape)","1eb5d1b9":"X_test_cat = X_test[[\"workclass\", \"education\", \"marital.status\", \"occupation\", \n                                   \"relationship\", \"race\", \"sex\", \"native.country\"]]\nX_test_con = X_test.drop(X_test_cat, axis=1)\n\nprint(X_test_cat.shape, X_test_con.shape)","0b567360":"scaler = preprocessing.StandardScaler().fit(X_test_con)\nX_test_con_sc = pd.DataFrame(scaler.transform(X_test_con))\nX_test_con_sc.columns = [\"age\",\"fnlwgt\", \"education.num\", \"capital.gain\", \"capital.loss\", \"hours.per.week\"]\n\nprint(X_test_con_sc.shape)\nX_test_con_sc.head()","566e7267":"X_test_cat.index = [i for i in range(len(X_test_cat))]\ndf_list = [X_test_cat, X_test_con_sc]\nX_test = pd.concat(df_list, axis = 1)\nX_test.index = range(30000, len(X_test)+30000) \n\nprint(X_test.shape)\nX_test.head()","d5b08892":"# Testing...\nY_test_pred = my_knn_model_final.predict(X_test)\n\nprint(accuracy_score(Y_test, Y_test_pred),\n      matthews_corrcoef(Y_test,Y_test_pred), f1_score(Y_test,Y_test_pred))","fa9f0d59":"my_knn_cmatrix = confusion_matrix(Y_test, Y_test_pred)\n\nmy_knn_df = pd.DataFrame(my_knn_cmatrix)\nplt.figure(figsize = (8, 8))\nsns.heatmap(my_knn_df, xticklabels = [\"<=50K\",\">50K\"],\n            yticklabels = [\"<=50K\",\">50K\"], annot = True)","02675b8d":"### &#8549;. Evaluate Test Set Accuracy with the Trained Model","81b954a8":"### &#8545;. Read Data in","e4e29bdb":"## Contents\n<ul>\n    <li>Import Modules<\/li>\n    <li>Read Data in<\/li>\n    <li>Split Date into Train-Valid-Test Sets<\/li>\n    <li>Preprocess Data<\/li>\n    <li>Tune Hyperparameters for kNN<\/li>\n    <li>Evaluate Test Set Accuracy with the Trained Model<\/li>\n<\/ul>","67d25fbf":"<i>From the output above, we can read that every column in mydf_train_valid has exactly 30000 non-null values, which means we have no missing value to deal with.<\/i>","0f6917d7":"### &#8547;. Preprocess Data","3eb651cf":"<i>From the plot above, we can read that No.38 combination of parameters, which is {n_neighbors=19, weights='uniform', metric='euclidean'}, got the best accuracy score.<\/i>","e1fa2ff2":"### &#8544;. Import Modules ","01e259d3":"### &#8546;.  Split Date into Train-Valid-Test Sets","eee6ee22":"### &#8548;. Tune Hyperparameters for kNN"}}