{"cell_type":{"64172dbf":"code","7a48f8b0":"code","bc55379d":"code","1b61da00":"code","1d71b163":"code","16b0f915":"code","652b4b78":"code","fb557bd9":"code","0d699b37":"code","2b92d712":"code","2f6cea0d":"markdown","ae8bf3fb":"markdown"},"source":{"64172dbf":"# Install SentenceBert Library\n!pip install -U sentence-transformers","7a48f8b0":"# Import kaggle dataset\nimport pandas as pd\npd.set_option('display.max_colwidth', 500)\n\ndf = pd.read_csv(\"..\/input\/netflix-shows\/netflix_titles.csv\")\ndf.head()","bc55379d":"# Filtering df\ndf = df[['title', 'rating', 'description']]","1b61da00":"# Quick check of description lens (for BERT token len)\nimport matplotlib.pyplot as plt\n\nlens = [len(x.split()) for x in df['description']]\n\nplt.hist(lens, bins=30)\nplt.show()","1d71b163":"# Import library, utilities \nfrom sentence_transformers import SentenceTransformer, util\nimport torch\n\n# Set embedding model and max_seq_len and push to GPU\nembedder = SentenceTransformer('bert-base-uncased')\nembedder.to('cuda')\n# going a little longer for user inputed synopsis\nembedder.max_seq_len = 128","16b0f915":"# Set feature lists for concatonation to sematic asearch results\ntitles = df['title'].tolist()\nratings = df['rating'].tolist()\nstories = df['description'].tolist()\n\n# Fit model to corpus qnd push to GPU\nstory_embeddings = embedder.encode(stories, convert_to_tensor=True)\nstory_embeddings = story_embeddings.to('cuda')","652b4b78":"# Define Semantic Search Function\ndef semantic_search(input_data):\n  # set lists to capture results\n  title_list = []\n  rating_list = []\n  story_list = []\n  score_list = []\n  # empty dataframe to display results \n  results = pd.DataFrame()\n  # Find the closest 5 stories of the corpus for each query sentence based on cosine similarity\n  top_k = min(10, len(story_embeddings))\n  \n  # If the input is too short to be a story or its not in the dataset  \n  if len(input_data) < 20 and input_data not in titles:\n    print('Title Not Found')\n\n  # If input is in the dataset\n  elif input_data in titles:\n    # Load and encode the description for the title match    \n    query_embeddings = embedder.encode(str(df[df['title'] == input_data]['description'])[5:-33], convert_to_tensor=True)\n    query_embeddings = query_embeddings.to('cuda')\n\n    # Use cosine-similarity and torch.topk to find the highest 5 scores\n    cos_scores = util.pytorch_cos_sim(query_embeddings, story_embeddings)[0]\n    top_results = torch.topk(cos_scores, k=top_k)\n    # Format the astory not to run off the cell\n    input_data_2 = input_data.replace('.', '.\\n')\n    print(\"\\n\\n======================\")\n    print(\"\\tSTORY\")\n    print(\"======================\\n\")\n    print('',str(df[df['title'] == input_data]['description'])[5:-33])\n    print(\"\\n\\n======================\")\n    print(\"    TOP RESULTS\")\n    print(\"======================\\n\")\n    \n    # For score, index in torch.topk(cos_scores, k=top_k) use index  locator for feature lists\n    # push score to cpu and convert to 1D array\n    for score, idx in zip(top_results[0], top_results[1]):\n      title_list.append(titles[idx])\n      rating_list.append(ratings[idx])\n      story_list.append(stories[idx])\n      score_list.append(score.cpu().numpy().flatten())\n\n    # Push results to dictionary columns \n    results['Title'] = title_list\n    results['Rating'] = rating_list\n    results['Story'] = story_list\n    results['Score'] = score_list\n    # return dictionary\n    return results.iloc[1:, :]\n\n  # If the input is long enough to be a story which is in the dataset\n  elif len(input_data) > 20 and input_data not in titles:\n    # Find the closest 5 stories of the corpus for each query sentence based on cosine similarity\n    query_embeddings = embedder.encode(input_data, convert_to_tensor=True)\n    query_embeddings = query_embeddings.to('cuda')\n\n    # Use cosine-similarity and torch.topk to find the highest 5 scores\n    cos_scores = util.pytorch_cos_sim(query_embeddings, story_embeddings)[0]\n    top_results = torch.topk(cos_scores, k=top_k)\n\n    input_data = input_data.replace('.', '.\\n')\n    print(\"\\n\\n======================\")\n    print(\"\\tSTORY\")\n    print(\"======================\\n\")\n    print(input_data)\n    print(\"\\n\\n======================\")\n    print(\"    TOP RESULTS\")\n    print(\"======================\\n\")\n    \n    # For score, index in torch.topk(cos_scores, k=top_k) use index  locator for feature lists\n    # push score to cpu and convert to 1D array\n    for score, idx in zip(top_results[0], top_results[1]):\n      title_list.append(titles[idx])\n      rating_list.append(ratings[idx])\n      story_list.append(stories[idx])\n      score_list.append(score.cpu().numpy().flatten())\n\n    # Push results to dictionary columns \n    results['Title'] = title_list\n    results['Rating'] = rating_list\n    results['Story'] = story_list\n    results['Score'] = score_list\n    # return dictionary\n    return results\n    ","fb557bd9":"# Push user input to Semantic Search function\n# Example of user created synopsis\nsemantic_search(\"When CIA analyst Jack Ryan stumbles upon a suspicious series of bank transfers his search for answers pulls him from the safety of his desk job and catapults him into a deadly game of cat and mouse throughout Europe and the Middle East, with a rising terrorist figurehead preparing for a massive attack against the US\")","0d699b37":"# Example of existing title\nsemantic_search('Chappie')","2b92d712":"# Example of incorrect entry\nsemantic_search(\"chappie\")","2f6cea0d":"# Just run the cell below and enter a title or a short synopsis which you would like to find similar results for... ","ae8bf3fb":"# A short and sweet modification of the SBERT tutorials \n\nSentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings. The initial work is described in the paper Sentence-BERT: [Sentence Embeddings using Siamese BERT-Networks](https:\/\/arxiv.org\/abs\/1908.10084).\n\n\n"}}