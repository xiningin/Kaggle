{"cell_type":{"923ad2a1":"code","e3664580":"code","9a6f5fac":"code","d1ea23f2":"code","111ca08f":"code","2e58df0a":"code","66e7cf55":"code","737be6a0":"code","7bd7f713":"code","771c5198":"code","733ad1f8":"code","1ef27326":"code","e9e8335b":"code","6576c47a":"code","ade11fd2":"code","a751d4e4":"code","def6a751":"code","0df1b473":"code","6012535d":"code","1c5a9819":"code","f1d0e50a":"code","e265b396":"code","bb053e80":"code","78539b42":"code","3909411c":"code","4bcbfa33":"markdown","0d696114":"markdown","56eaf3a6":"markdown","136edf9c":"markdown","ff092376":"markdown","20589a15":"markdown","313c0601":"markdown","2a37dedb":"markdown","52fec304":"markdown","b0b4f76e":"markdown","333044cd":"markdown","9bdbe391":"markdown"},"source":{"923ad2a1":"import os\nfrom time import time\nimport datetime as dt\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn import ensemble\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble.partial_dependence import plot_partial_dependence\nfrom sklearn.ensemble.partial_dependence import partial_dependence","e3664580":"input_dir = \"..\/input\"\nexamples_dir = os.path.join(input_dir,'cv-data-augmentation-text-scores')\nexamples = pd.read_parquet(os.path.join(examples_dir,'positive_negative_examples.parquet.gzip'))","9a6f5fac":"examples.sample(10)","d1ea23f2":"examples.shape","111ca08f":"examples.dropna().shape","2e58df0a":"test_period_start = dt.datetime(2018, 7, 1)\ntest_period_end = dt.datetime(2019, 1, 31)\n\n# the response or target column\ny_col = 'matched' \n\n# 'questions_id' is only used in the Conditional Logistic Regression model\n# 'answer_user_id' and 'emails_date_sent' are only used for comparison statistics\nfeature_cols = ['questions_id', 'answer_user_id', 'emails_date_sent',\n                'days_from_joined_dates', 'days_from_last_activities',\n                'professional_activities_sum_100000', 'professional_activities_sum_365', 'professional_activities_sum_30', \n                'questioner_answerer_shared_schools', 'questioner_answerer_shared_tags', 'questioner_answerer_shared_groups', \n                'questioners_answerers_paths', 'commenters_questioners_paths', 'commenters_answerers_paths',\n                'LSI_Score']","66e7cf55":"def split_train_test(examples, feature_cols, y_col,\n                     test_period_start, test_period_end):\n    \n    train_data = examples[examples['questions_date_added'] < test_period_start]\n    train_x = train_data[feature_cols]\n    train_y = train_data[y_col]\n    print('Train Data Set Size: {}'.format(train_x.shape))\n    \n    test_data = examples[(examples['questions_date_added'] >= test_period_start) & (examples['questions_date_added'] <= test_period_end)]\n    test_x = test_data[feature_cols]\n    test_y = test_data[y_col]\n    print('Test Data Set Size: {}'.format(test_x.shape))\n    \n    return train_x, train_y, test_x, test_y","737be6a0":"# Slipt the original data set \ntrain_x, train_y, test_x, test_y = split_train_test(examples, feature_cols, y_col, test_period_start, test_period_end)","7bd7f713":"# Save train and test data sets in CSV format\ntrain_x.to_csv('train_x.gz', compression='gzip')\ntrain_y.to_csv('train_y.gz', compression='gzip')\ntest_x.to_csv('test_x.gz', compression='gzip')\ntest_y.to_csv('test_y.gz', compression='gzip')","771c5198":"# Drop 'questions_id' column which is only used in the Conditional Logistic Regression model\n# Drop 'answer_user_id' and 'emails_date_sent' columns which are only used for comparison statistics\nfor dropped_col in ['questions_id', 'answer_user_id', 'emails_date_sent']:    \n    train_x = train_x.drop(dropped_col, axis=1)\n    test_x = test_x.drop(dropped_col, axis=1)\n    feature_cols.remove(dropped_col)\nprint(feature_cols)","733ad1f8":"train_sample_size = None\nif train_sample_size is not None:    \n    train_sample_index = np.random.choice(train_x.index, size=train_sample_size, replace=False)\n    train_x = train_x.iloc[train_sample_index]\n    train_y = train_y.iloc[train_sample_index]\nprint('Label distribution: ')\nprint(train_y.value_counts())","1ef27326":"def report(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                  results['mean_test_score'][candidate],\n                  results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")","e9e8335b":"# This hyper parameter set is selected by the below randomized grid search in the previous runs of this notebook.\n# Therefore, we use this set of hyper parameters from now on to reduce the running time\n# In production, this step needs to be fine tuned further\nbest_params = {\"learning_rate\": 0.1,\n               \"n_estimators\": 100,\n               \"max_depth\": 3}\n\n# if we would like to search for best hyper parameters, uncomment the below line to set best_params to None #\n# best_params = None         \nif best_params is None:    \n    # a sample grid of hyper parameters\n    param_dist = {\"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n                  \"n_estimators\": [50, 100, 200],\n                  \"max_depth\": [3]}\n    # run randomized search\n    n_iter_search = 5\n    random_search = RandomizedSearchCV(ensemble.GradientBoostingClassifier(), param_distributions=param_dist,\n                                       n_iter=n_iter_search, cv=3)\n    start = time()\n    random_search.fit(train_x, train_y)\n    print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n          \" parameter settings.\" % ((time() - start), n_iter_search))\n    report(random_search.cv_results_)\n    best_params = random_search.best_params_","6576c47a":"%%time\ngbc_model = ensemble.GradientBoostingClassifier(**best_params)\nprint(gbc_model)\ngbc_model.fit(train_x, train_y)","ade11fd2":"feature_importance = gbc_model.feature_importances_\n# make importances relative to max importance\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.subplot(1, 2, 2)\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, np.asarray(feature_cols)[np.asarray(sorted_idx)])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance')\nplt.savefig('feature_importance.jpg')\nplt.show()","a751d4e4":"feature_names = pd.DataFrame({'Name':feature_cols, \n                              'ID': range(0, len(feature_cols))}).set_index('Name')","def6a751":"plot_partial_dependence(gbc_model, train_x, [feature_names.loc['days_from_joined_dates']],\n                        feature_names=feature_cols,\n                        n_jobs=3, grid_resolution=50)","0df1b473":"plot_partial_dependence(gbc_model, train_x, [feature_names.loc['professional_activities_sum_30']],\n                        feature_names=feature_cols,\n                        n_jobs=3, grid_resolution=50)","6012535d":"plot_partial_dependence(gbc_model, train_x, [feature_names.loc['questioners_answerers_paths']],\n                        feature_names=feature_cols,\n                        n_jobs=3, grid_resolution=50)","1c5a9819":"plot_partial_dependence(gbc_model, train_x, [feature_names.loc['LSI_Score']],\n                        feature_names=feature_cols,\n                        n_jobs=3, grid_resolution=50)","f1d0e50a":"for feature_name in feature_names.index:\n    plot_partial_dependence(gbc_model, train_x, [feature_names.loc[feature_name]],\n                            feature_names=feature_cols,\n                            n_jobs=3, grid_resolution=50)\n    plt.savefig('{}.jpg'.format(feature_name))\nos.listdir()","e265b396":"predicted_test_y = pd.DataFrame(gbc_model.predict_proba(test_x)[:,1], index=test_y.index, columns=['Predicted_Match_Prob'])","bb053e80":"predicted_test_y.sample(10)","78539b42":"predicted_test_y.to_csv('predicted_test_y.gz', compression='gzip')","3909411c":"os.listdir()","4bcbfa33":"### II.2.1. Feature Importance ###","0d696114":"## II.2 Model Interpretation: Feature Importance and Interpretation ##","56eaf3a6":"# I Train and Test Data Preparation #","136edf9c":"### II.1.1. Search for Optimal Hyper Parameters ###","ff092376":"### II.2.2. Feature Intepretation with Partial Dependence Plots ###","20589a15":"### The test period is from July 2018 to January 2019 ###","313c0601":"## II.1 Model Training ##","2a37dedb":"## II.3 Model Prediction on the Test Data Set ##","52fec304":"### II.1.2 Model Training with the Optimal Hyper Parameters ###","b0b4f76e":"# II. Classification with Gradient Boosting Decision Trees #","333044cd":"## I.1. Loading the Supervised Machine Learning Data Set ##","9bdbe391":"## I.2. Splitting Data into Train and Test Data Sets ##"}}