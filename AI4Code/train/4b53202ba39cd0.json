{"cell_type":{"91b2ca5d":"code","9786dfa7":"code","000cf1af":"code","a2113d25":"code","71b7a5eb":"code","c4709a5b":"code","390b57cf":"code","a65e3b02":"code","12bcd814":"code","c72eff28":"code","e387dc7a":"code","992694df":"code","5e423de9":"code","058e8682":"code","159e911b":"code","21d263fb":"code","05c857bc":"code","d0d3eca1":"code","474f03da":"code","f0c3fa65":"code","c1061c07":"code","237c8f5d":"code","cd201ffb":"code","efe5def7":"code","dacb4e9f":"code","9889f5a5":"code","ac09a31d":"code","a498b866":"code","f4117edb":"code","bf47e26f":"code","b05042a8":"code","27e7ee29":"code","b6940472":"code","10a77c4a":"code","80ac5887":"code","53d54f13":"markdown","68123d41":"markdown"},"source":{"91b2ca5d":"# import library\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy import stats\nfrom scipy.stats import norm\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","9786dfa7":"\n#Import datasets\ndf_train_B = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')","000cf1af":"df_train.head()","a2113d25":"df_train['SalePrice'].describe()","71b7a5eb":"def correlation(data): # Plot the heatmap to show the relationships between the output and the related variables\n    data\n    corrmat = df_train.corr()\n    plt.subplots(figsize = (12,9))\n    sns.heatmap(corrmat , vmax = .8, square = True );\n    \n    k = 10\n    cols = corrmat.nlargest(k , 'SalePrice')['SalePrice'].index\n    cm  = np.corrcoef(df_train[cols].values.T)\n    plt.subplots(figsize = (12,9))\n    sns.set(font_scale = 1.25)\n    hm = sns.heatmap(cm , cbar = True , annot = True , square = True , fmt = '.2f', annot_kws={'size':10} , yticklabels = cols.values , xticklabels = cols.values)\n    plt.show()\n    \ndef futureselection (data): # Based on our the correlation map we will select the best features \n    data = data    \n    cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt' , 'Id']\n    data[cols]\n    return data[cols]\n\n#missing data\ndef missingdata (data): # print the number of missing data and percent of total  \n    data = data \n    total = data.isnull().sum().sort_values(ascending=False)\n    percent = (data.isnull().sum()\/data.isnull().count()).sort_values(ascending=False)\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    print (missing_data.head(20))\n\n    \n'''    \n\nThe point here is to test numerical variables in a very lean way. We'll do this paying attention to:\n\nHistogram - Kurtosis and skewness.\nNormal probability plot - Data distribution should closely follow the diagonal that represents the normal distribution.\n\n\n'''\ndef transformation (data , title):\n    title = title\n    data = data\n    data_b = data    \n        \n    data = np.log(data)   \n    \n    fig , axes = plt.subplots(2, 2, figsize=(16, 14))\n    fig.suptitle('{0} histogram and normal probability plot '.format(title))\n    \n    sns.distplot(  data_b, fit=norm  , ax = axes[0][0] )\n    axes[0][0].set_title('{0} histogram befor transformation'.format(title) )\n    \n    sns.distplot(  data  , fit=norm , ax = axes[0][1])\n    axes[0][1].set_title('{0} histogram after transformation'.format(title))\n    \n    stats.probplot(data_b, plot=axes[1][0])\n    axes[1][0].set_title('{0} normal probability plot befor transformation'.format(title))\n       \n    stats.probplot(data, plot=axes[1][1])\n    axes[1][1].set_title('{0} normal probability plot after transformation'.format(title))        \n    plt.show()\n    return data\n\ndef categorical_variables_plot (x , y , title_x , title_y , backup_data ):\n    \n    x = x\n    \n    y = y\n    title_x = title_x\n    title_y = title_y\n    backup_data = backup_data\n    \n    fig , axes = plt.subplots(1, 2, figsize=(16, 6))\n    fig.suptitle('Box plot {0} & {1} '.format(title_x , title_y))\n    \n    data_A = pd.concat([ backup_data[title_y], backup_data[title_x] ], axis=1)\n    \n    sns.boxplot(x = title_x , y= title_y , data =  data_A  , ax = axes[0])\n      \n    axes[0].set_title('Box plot {0} & {1} befor transformation'.format(title_x , title_y))    \n    \n    data_B = pd.concat([y, x], axis=1)\n    \n    sns.boxplot(x= title_x, y = title_y, data = data_B , ax = axes[1])\n    \n    axes[1].set_title('Box plot {0} & {1} after transformation'.format(title_x , title_y))\n    \n    plt.show()\n    \n    \ndef numerical_variables_plot (x , y , title_x , title_y , backup_data ):    \n\n    x = x\n    y = y\n    title_x = title_x\n    title_y = title_y\n    \n    backup_data = backup_data\n    \n    fig , axes = plt.subplots(1, 2, figsize=(16, 6))\n    fig.suptitle('bivariate analysis {0} & {1}'.format(title_x , title_y))\n    \n    \n    data_A = pd.concat( [ backup_data[title_y], backup_data[title_x] ] , axis=1)\n    \n    data_A.plot.scatter(x = title_x , y = title_y, ax = axes[0]);\n    \n    axes[0].set_title('bivariate analysis {0} & {1} befor transformation'.format(title_x , title_y))\n    \n    data_B = pd.concat([y, x], axis=1)\n    \n    data_B.plot.scatter( x = title_x  , y = title_y , ax = axes[1]);\n    \n    axes[1].set_title('bivariate analysis {0} & {1} after transformation'.format(title_x , title_y))\n    ","c4709a5b":"correlation(df_train)\n","390b57cf":"df_train = futureselection (df_train)","a65e3b02":"missingdata (df_train)","12bcd814":"df_train['SalePrice'] = transformation ( df_train['SalePrice']  ,  'SalePrice')","c72eff28":"df_train['GrLivArea'] = transformation ( df_train['GrLivArea'], 'GrLivArea')","e387dc7a":"df_train = df_train.drop (df_train.loc[df_train['TotalBsmtSF'] == 0].index) # we delate the zero values\ndf_train['TotalBsmtSF'] = transformation (  df_train['TotalBsmtSF'] ,   'TotalBsmtSF' )    ","992694df":"categorical_variables_plot (df_train['GarageCars'] , df_train['SalePrice'] ,'GarageCars' , 'SalePrice' , df_train_B)","5e423de9":"categorical_variables_plot (df_train['OverallQual'] , df_train['SalePrice'] ,'OverallQual' , 'SalePrice' , df_train_B)","058e8682":"categorical_variables_plot (df_train['FullBath'] , df_train['SalePrice'] ,'FullBath' , 'SalePrice' , df_train_B)","159e911b":"numerical_variables_plot (df_train['GrLivArea'] , df_train['SalePrice'] ,'GrLivArea' , 'SalePrice' , df_train_B)","21d263fb":"numerical_variables_plot (df_train['TotalBsmtSF'] , df_train['SalePrice'] ,'TotalBsmtSF' , 'SalePrice' , df_train_B)","05c857bc":"train = df_train.drop('Id'  , axis=1)","d0d3eca1":"train.head (5)","474f03da":"missingdata (df_train)","f0c3fa65":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train.drop('SalePrice', axis=1), train['SalePrice'], test_size=0.3, random_state=101)","c1061c07":"# import regression model\nfrom sklearn import ensemble\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import mean_squared_error, r2_score","237c8f5d":"# #############################################################################\n\nparams = {'n_estimators': 300, 'max_depth': 4, 'min_samples_split': 2,\n          'learning_rate': 0.01, 'loss': 'ls'}\nclf = ensemble.GradientBoostingRegressor(**params)\n\nclf.fit(X_train, y_train)\nmse = mean_squared_error(y_test, clf.predict(X_test))\nprint(\"MSE: %.4f\" % mse)\n\n# #############################################################################\n# Plot training deviance\n\n# compute test set deviance\ntest_score = np.zeros((params['n_estimators'],), dtype=np.float64)\n\nfor i, y_pred in enumerate(clf.staged_predict(X_test)):\n    test_score[i] = clf.loss_(y_test, y_pred)\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title('Deviance')\nplt.plot(np.arange(params['n_estimators']) + 1, clf.train_score_, 'b-',label='Training Set Deviance')\nplt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-', label='Test Set Deviance')\n\nplt.legend(loc='upper right')\nplt.xlabel('Boosting Iterations')\nplt.ylabel('Deviance')\n\n","cd201ffb":"from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(y_test, y_pred))\nprint('MSE:', metrics.mean_squared_error(y_test, y_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","efe5def7":"print('Train Score is : ' , clf.score(X_train, y_train))\nprint('Test Score is : ' , clf.score(X_test,y_pred))","dacb4e9f":"df_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest_id = df_test['Id']","9889f5a5":"cols = ['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\ndf_test = df_test [cols]\n","ac09a31d":"df_test.isnull().sum().sort_values(ascending=False).head(20)","a498b866":"df_test['GarageCars'] =df_test['GarageCars'].fillna(df_test['GarageCars'].mean())\ndf_test['TotalBsmtSF'] =df_test['TotalBsmtSF'].fillna(df_test['TotalBsmtSF'].mean())\n","f4117edb":"c = clf.predict(df_test)","bf47e26f":"xx = np.exp(c)","b05042a8":"a = pd.DataFrame(test_id, columns=['Id'])","27e7ee29":"output  = pd.DataFrame(xx, columns=['SalePrice'])","b6940472":"result = pd.concat([a,output], axis=1)","10a77c4a":"result.head(5)","80ac5887":"result.to_csv('submission.csv',index=False)","53d54f13":"# House Prices prediction","68123d41":"This is the first code I'm writing in\nI would to mention some resources I used to build my own \nThe Hesham Asem first one who taught me ML\nI would thank a lot PEDRO MARCELINO for his notebook and he explains(Some code I copies from his) "}}