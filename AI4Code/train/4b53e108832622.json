{"cell_type":{"c0b06a4b":"code","9663e7b2":"code","02a7a3e5":"code","70d49475":"code","b49f8809":"code","a26d1cd3":"code","178f4cb0":"code","477f9069":"code","87a19596":"code","9f8c8a43":"code","40d8ca55":"markdown","de9870ae":"markdown","2224c53e":"markdown","0e36cf28":"markdown","d19cb0f2":"markdown","e7495d45":"markdown"},"source":{"c0b06a4b":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.callbacks import ModelCheckpoint","9663e7b2":"# Measure run time\nimport time\nstart_time = time.time() ","02a7a3e5":"# Load the data\ntrain = pd.read_csv(\"..\/input\/fashion-mnist-dataset\/fashion-mnist_train.csv\")\ntest = pd.read_csv(\"..\/input\/fashion-mnist-dataset\/fashion-mnist_test.csv\")\n\n# X for Features, Y for Labels\nX_train = train.drop([\"label\"], axis=1) \nY_train = train[\"label\"]\nX_test = test.drop([\"label\"], axis=1)\n\n# Normalize the data\nX_train = X_train \/ 255.0\nX_test = X_test \/ 255.0\n\n# Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1)\nX_train = X_train.values.reshape(-1,28,28,1)\nX_test = X_test.values.reshape(-1,28,28,1)\n\n# Split the train and the validation set for the fitting\nrandom_seed = 0\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=random_seed)\n\n# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\nY_train = pd.get_dummies(Y_train)\nY_val_dummies = pd.get_dummies(Y_val)","70d49475":"# n*models\ndef make_models(n, X_train, Y_train, X_val, Y_val, batch_size, epochs):\n    for i in range(n):\n        \n        # Create Model\n        model = Sequential()\n        \n        # Add Layers\n        model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n        model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Dropout(0.5))\n        \n        model.add(Conv2D(32, (3, 3), activation='relu'))\n        model.add(Conv2D(64, (3, 3), activation='relu'))\n        model.add(Dropout(0.666))\n        \n        model.add(Flatten())\n        model.add(Dense(128, activation='relu'))\n        model.add(Dropout(0.5))\n        model.add(Dense(10, activation='softmax'))\n        \n        # Compile\n        model.compile(loss='categorical_crossentropy',\n                      optimizer=keras.optimizers.Adam(),\n                      metrics=['accuracy'])\n        \n        # checkpoint\n        filepath=\"weights.best\"+str(i)+\".hdf5\"\n        checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n        callbacks_list = [checkpoint]   \n        \n        # Fit the model to the train data\n        model.fit(X_train, Y_train,\n        batch_size=batch_size,\n        verbose=1,\n        epochs=epochs,\n        validation_data=(X_val, Y_val),\n        callbacks=callbacks_list)    ","b49f8809":"# What we want in this cell is \"Majority rule\"\n# By using n*models, we can predict n*answers\n# So take \"Majority rule\" like RandomForest\n# And then we will have more accuracy\n\n# Y_test n*answers for X_test\ndef answers(n, X_test, X_val, Y_val):\n    \n    # answer array\n    answer = [[] for p in range(n)]\n    \n    # Create Model\n    model = Sequential()\n        \n    # Add Layers\n    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.5))\n        \n    model.add(Conv2D(32, (3, 3), activation='relu'))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(Dropout(0.666))\n        \n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(10, activation='softmax'))\n        \n    # Compile\n    model.compile(loss='categorical_crossentropy',\n                optimizer=keras.optimizers.Adam(),\n                metrics=['accuracy'])\n    # load weights and insert predict results for each\n    for q in range(n):\n        print(\"----------Loading weights.best\" + str(q) + \".hdf5----------\")\n        model.load_weights(\"weights.best\"+str(q)+\".hdf5\")\n        print(\"[\"+\"val_loss, val_acc\"+\"]\"+\" ---> \"+str(model.evaluate(X_val, Y_val, verbose=0)))\n        results = model.predict(X_test)\n        results = np.argmax(results, axis=1)\n        results = pd.Series(results, name=\"label\")\n        answer[q] = results\n        \n    # Concat results\n    results_concat = answer[0]\n    for r in range(n-1):\n        results_concat = pd.concat([results_concat, answer[r+1]], axis=1)\n        \n    # Mode of concat results\n    results_concat_mode = results_concat.mode(axis=1).iloc[ : , 0]\n    results_concat_mode = pd.Series(results_concat_mode, name=\"label\")\n    \n    # Concat id as you can submit\n    submission = pd.concat([pd.Series(range(1,10001), name='id'), results_concat_mode], axis=1)\n    \n    # float ---> int\n    submission = submission.astype(np.int64)\n    \n    # to_csv\n    submission.to_csv('submission.csv', index=False)\n    print(\"----------complete----------\")","a26d1cd3":"# Y_val n*answers for X_val\ndef val_answers(n, X_val, Y_val):\n    \n    # answer array\n    answer = [[] for p in range(n)]\n    \n    # Create Model\n    model = Sequential()\n        \n    # Add Layers\n    model.add(Conv2D(32, kernel_size=(3, 3), activation=\"relu\", input_shape=(28, 28, 1)))\n    model.add(Conv2D(64, kernel_size=(3, 3), activation=\"relu\"))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.5))\n        \n    model.add(Conv2D(32, (3, 3), activation=\"relu\"))\n    model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n    model.add(Dropout(0.666))\n        \n    model.add(Flatten())\n    model.add(Dense(128, activation=\"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(10, activation=\"softmax\"))\n        \n    # Compile\n    model.compile(loss=\"categorical_crossentropy\",\n                optimizer=keras.optimizers.Adam(),\n                metrics=[\"accuracy\"])\n    # load weights and insert predict results for each\n    for q in range(n):\n        print(\"----------Loading weights.best\" + str(q) + \".hdf5----------\")\n        model.load_weights(\"weights.best\"+str(q)+\".hdf5\")\n        print(\"[\"+\"val_loss, val_acc\"+\"]\"+\" ---> \"+str(model.evaluate(X_val, Y_val, verbose=0)))\n        results = model.predict(X_val)\n        results = np.argmax(results, axis=1)\n        results = pd.Series(results, name=\"label\")\n        answer[q] = results\n        \n    # Concat results\n    results_concat = answer[0]\n    for r in range(n-1):\n        results_concat = pd.concat([results_concat, answer[r+1]], axis=1)\n        \n    # Mode of concat results\n    results_concat_mode = results_concat.mode(axis=1).iloc[ : , 0]\n    results_concat_mode = pd.Series(results_concat_mode, name=\"label\")\n    \n    # Concat id as you can submit\n    results_concat_mode = pd.concat([pd.Series(range(1,6001), name='id'), results_concat_mode], axis=1)\n    \n    # float ---> int\n    results_concat_mode = results_concat_mode.astype(np.int64)\n    \n    # to_csv\n    results_concat_mode.to_csv(\"val_results_concat_mode.csv\", index=True)\n    print(\"----------complete----------\")","178f4cb0":"# set parameters\nn = 5\nbatch_size = 128\nepochs = 350\n\n# make models\nmake_models(n, X_train, Y_train, X_val, Y_val_dummies, batch_size, epochs)","477f9069":"# Majority rule for val\nval_answers(n, X_val, Y_val_dummies)\n# check val accuracy after Majority rule\nval_results_concat_mode = pd.read_csv(\"val_results_concat_mode.csv\")\naccuracy_score(Y_val, val_results_concat_mode[\"label\"])","87a19596":"# Majority rule for test\nanswers(n, X_test, X_val, Y_val_dummies)","9f8c8a43":"# Measure run time\nend_time = time.time()\nelapsed_time = end_time - start_time\nprint(f\"elapsed_time\uff1a{elapsed_time}\")","40d8ca55":"# 2. Data","de9870ae":"# 4. Use functions","2224c53e":"Maybe, test Accuracy will be around 0.95000\n\n6 hours run time is allowed on kaggle, so if you use more models and better models, you will have better accuracy","0e36cf28":"# 3. Define functions","d19cb0f2":"Thanks, val Accuracy is UP!","e7495d45":"# 1. Preparation"}}