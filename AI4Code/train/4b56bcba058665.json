{"cell_type":{"7745c03a":"code","81623f58":"code","cdccb757":"code","dea5c095":"code","7b2366f8":"code","d0c88d7e":"code","86a79d92":"code","b593e71c":"code","28a4fa22":"code","322d37ef":"code","421a658d":"code","bee50fc3":"code","7e0f099f":"code","09cadba5":"code","f4d3ec7e":"code","6dc7134c":"code","ed781e2a":"code","98c0c360":"code","cf174bc7":"code","f250bf7f":"code","4c3198f7":"code","c810d820":"code","33a7a207":"code","71895b12":"code","823a0788":"code","5fd9527e":"code","3f2044f6":"code","56496079":"code","329e0a96":"code","bdbb166a":"code","4398ac0c":"code","918599f9":"code","1acaf88d":"code","821316c3":"markdown","abea6470":"markdown","01851fe1":"markdown","f8fcb262":"markdown","acd34baf":"markdown","69901df2":"markdown","b46c3841":"markdown","f338029c":"markdown","4abae1e9":"markdown","5f5d6efa":"markdown","0bb1e101":"markdown","3253457c":"markdown","7bae9807":"markdown","efa03983":"markdown","5da776dd":"markdown","e61bda49":"markdown","a6ec97ad":"markdown","6fb3a05a":"markdown","4a600f4f":"markdown","e4e557b4":"markdown","5f57a0bf":"markdown","e3a6b88d":"markdown","97e4e6e8":"markdown","810ebe47":"markdown","c1978c2a":"markdown","1932d2ab":"markdown"},"source":{"7745c03a":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\n% matplotlib inline\nfrom datetime import datetime\nfrom scipy import stats\npd.options.mode.chained_assignment = None\nfrom scipy.stats import norm, skew\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\nimport warnings","81623f58":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","cdccb757":"train.head()","dea5c095":"test.head()","7b2366f8":"train.describe()","d0c88d7e":"print('Train Dataset Shape : {0}'.format(train.shape))\nprint('Test Dataset Shape : {0}'.format(test.shape))","86a79d92":"train.dtypes","b593e71c":"sns.boxplot(train['count'])","28a4fa22":"train = train[np.abs(train[\"count\"]-train[\"count\"].mean())<=(3*train[\"count\"].std())] ","322d37ef":"fig,ax = plt.subplots(2,1,figsize = (10,10))\nsns.distplot(train['count'],ax=ax[0])\nstats.probplot(train[\"count\"], dist='norm', fit=True, plot=ax[1])\nprint('Skewness : {0}'.format(train['count'].skew()))\nprint('Kurt : {0}'.format(train['count'].kurt()))","421a658d":"fig,ax = plt.subplots(2,1,figsize = (10,10))\n#logcount = np.log1p(train['count']).kurt()\n#rootcount = np.sqrt(train['count']).kurt()\n#cubiccount = np.power(train['count'],2).kurt()\n#minVal = min([logcount, rootcount, cubiccount])\n#if logcount == minVal:\nbest = 'log'\ntrain['count_log'] = np.log1p(train['count'])\nsns.distplot(train['count_log'],ax=ax[0])\nstats.probplot(train[\"count_log\"], dist='norm', fit=True, plot=ax[1])\n#elif rootcount == minVal:\n    #best = 'root'\n    #train['count_root'] = np.sqrt(train['count'])\n    #sns.distplot(train['count_root'],ax=ax[0])\n    #stats.probplot(train[\"count_root\"], dist='norm', fit=True, plot=ax[1])\n#elif cubiccount == minVal:\n    #best = 'cubic'\n    #train['count_cubic'] = np.power(train['count'],2)\n    #sns.distplot(train['count_cubic'],ax=ax[0])\n    #stats.probplot(train[\"count_cubic\"], dist='norm', fit=True, plot=ax[1])\n#print('For count, the Best TF is ' + best)","bee50fc3":"train['date']  = train.datetime.apply(lambda x: x.split()[0])\ntrain['hour'] = train.datetime.apply(lambda x: x.split()[1].split(':')[0])\ntrain['weekday'] = train.date.apply(lambda dateString : datetime.strptime(dateString, '%Y-%m-%d').weekday())\ntrain['month'] = train.date.apply(lambda dateString : datetime.strptime(dateString, '%Y-%m-%d').month)\ntrain = train.drop('datetime',axis=1)","7e0f099f":"train.shape","09cadba5":"train.dtypes","f4d3ec7e":"categorical = ['date','weekday','month','hour','season','holiday','workingday','weather']\nnumeric = [\"temp\",\"atemp\",\"casual\",\"registered\",\"humidity\",\"windspeed\",\"count\",\"count_log\"]","6dc7134c":"for idx in categorical:\n    train[idx].astype('category')","ed781e2a":"fig,axes = plt.subplots(ncols=2 ,nrows=2)\nfig.set_size_inches(15,10)\nsns.boxplot(data=train,x='season',y='count',ax=axes[0][0])\nsns.boxplot(data=train,x='holiday',y='count',ax=axes[0][1])\nsns.boxplot(data=train,x='workingday',y='count',ax=axes[1][0])\nsns.boxplot(data=train,x='weather',y='count',ax=axes[1][1])\n\nfig1,axes1 = plt.subplots()\nfig1.set_size_inches(15,10)\nsns.boxplot(data=train,x='hour',y='count')","98c0c360":"plt.subplots(figsize=(15,8))\nsns.heatmap(train[numeric].corr(),annot=True)","cf174bc7":"corr = train[numeric].drop('count', axis=1).corr()\ncorr =corr.drop('count_log', axis=1).corr() # We already examined SalePrice correlations\nplt.figure(figsize=(12, 10))\nsns.heatmap(corr[(corr >= 0.5) | (corr <= -0.4)], \n            cmap='viridis', vmax=1.0, vmin=-1.0, linewidths=0.1,\n            annot=True, annot_kws={\"size\": 8}, square=True);","f250bf7f":"### count,month\nplt.figure(figsize=(15,8))\nmonthagg = pd.DataFrame(train.groupby('month')['count'].mean()).reset_index()\nsns.barplot(data=monthagg, x='month',y='count').set(title = 'Month Vs Count')","4c3198f7":"### count,season,hour\nplt.figure(figsize=(15,8))\nhouragg = pd.DataFrame(train.groupby(['hour','season'])['count'].mean()).reset_index()\nsns.pointplot(data=houragg,x=houragg['hour'],y=houragg['count'],hue=houragg['season']).set(title='Hour,Season Vs Count')","c810d820":"### count,hour,weekday\nplt.figure(figsize=(15,8))\nhourweekagg = pd.DataFrame(train.groupby(['hour','weekday'])['count'].mean()).reset_index()\nsns.pointplot(data=hourweekagg,x=hourweekagg['hour'],y=hourweekagg['count'],hue=hourweekagg['weekday']).set(title='Hour,Week Vs Count')","33a7a207":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","71895b12":"target = train['count']\ntarget_log=train['count_log']\ntrain = train.drop('count_log',axis=1)\ntrain = train.drop('count',axis=1)\ntrain = train.drop('atemp',axis=1)\ntrain = train.drop('date',axis=1)\ntrain = train.drop('casual',axis=1)\ntrain = train.drop('registered',axis=1)\nm_dum = pd.get_dummies(train['month'],prefix='m')\nho_dum = pd.get_dummies(train['hour'],prefix='ho')\ns_dum = pd.get_dummies(train['season'],prefix='s')\nwe_dum = pd.get_dummies(train['weather'],prefix='we')\ntrain = pd.concat([train,s_dum,we_dum,m_dum,ho_dum],axis=1)\n\ntestid = test['datetime']\ntest['date']  = test.datetime.apply(lambda x: x.split()[0])\ntest['hour'] = test.datetime.apply(lambda x: x.split()[1].split(':')[0])\ntest['weekday'] = test.date.apply(lambda dateString : datetime.strptime(dateString, '%Y-%m-%d').weekday())\ntest['month'] = test.date.apply(lambda dateString : datetime.strptime(dateString, '%Y-%m-%d').month)\ntest = test.drop('datetime',axis=1)\ntest = test.drop('atemp',axis=1)\ntest = test.drop('date',axis=1)\ns_dum = pd.get_dummies(test['season'],prefix='s')\nwe_dum = pd.get_dummies(test['weather'],prefix='we')\nm_dum = pd.get_dummies(test['month'],prefix='m')\nho_dum = pd.get_dummies(test['hour'],prefix='ho')\ntest= pd.concat([test,s_dum,we_dum,m_dum,ho_dum],axis=1)","823a0788":"train.shape","5fd9527e":"test.shape","3f2044f6":"gbr = GradientBoostingRegressor(n_estimators=2000, learning_rate=0.01, max_depth=4).fit(train.values, target_log)","56496079":"def loss_func(truth, prediction):\n    y = np.expm1(truth)\n    y_ = np.expm1(prediction)\n    log1 = np.array([np.log(x + 1) for x in truth])\n    log2 = np.array([np.log(x + 1) for x in prediction])\n    return np.sqrt(np.mean((log1 - log2)**2))","329e0a96":"#from sklearn.model_selection import GridSearchCV\n#from sklearn.ensemble import GradientBoostingRegressor\n#from sklearn.metrics import make_scorer\n#param_grid = {\n#    'learning_rate': [0.1, 0.01, 0.001],\n#    'n_estimators': [100, 1000, 1500, 2000, 4000],\n#    'max_depth': [1, 2, 3, 4, 5]\n#}\n#scorer = make_scorer(loss_func, greater_is_better=False)\n#model = GradientBoostingRegressor(random_state=42)\n#result = GridSearchCV(model, param_grid, cv=4, scoring=scorer, n_jobs=3).fit(train.values, target_log)\n#print('\\tParams:', result.best_params_)\n#print('\\tScore:', result.best_score_)","bdbb166a":"##\tParams: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1500}\n#\tScore: -0.12669018059776296","4398ac0c":"model_gbr = GradientBoostingRegressor(n_estimators=1500,max_depth=5,learning_rate=0.01).fit(train.values,target_log)","918599f9":"prediction = model_gbr.predict(test.values)\nprediction = np.expm1(prediction)","1acaf88d":"output = pd.DataFrame()\noutput['datetime'] = testid\noutput['count'] = prediction\noutput.to_csv('output.csv',index=False)","821316c3":"# Visualization","abea6470":"* > Season - (spring, 2 : summer, 3 : fall, 4 : winter)\n* > Holiday considerations\n* > Workingday\n* > Weather - (1 : Sunny, 2 : Mist, 3 : light snow, rain, 4 : heavy snow, rain)\n* > Temp-temperature\n* > Atemp - weather (impression index)\n* > Windspeed\n* > Casual - the number of times an unregistered user lease has started\n* > Register - starts with the number of registered user rentals\n* > Count - Total lease count (dependent variables)","01851fe1":"You can judge there is an extlier by count.\nIt is good to remove outlier because it has a detrimental effect on the model. So, first, you have to remove it before you analyze it.","f8fcb262":"# Introduction","acd34baf":"##  Data Summary","69901df2":"* Load Data\n    * Data summary\n* Target\n    * outlier\n* Feature engineering\n    * Datetime\n    * Categorical&Numeric\n* Visualization\n    * Categorical\n    * Numerical\n    * All Feature\n* Modeling","b46c3841":"Fortunately, the Missing value of the entire data does not seem to exist.","f338029c":"# Target","4abae1e9":"## Outlier","5f5d6efa":"In general, you can see that most variables have a lot to do with Count. So let's take a look at the relationship between the different variables.","0bb1e101":"# Categorical & Numeric","3253457c":"You can see a few things from the graph above.\n* > Season can see that spring has a lower count than summer, fall, and winter.\n* > You can see there are quite a few outliers for Count. 3. When looking at Workingday, outlier is higher when working than when not working.(Use more when working.)\n* > The hour shows the most distribution in the 08 AM and 17 PM.","7bae9807":"Temp and atemp appear to be too closely associated with each other. \nIf this occurs, then remove the atemp later because creating a model that predicts dependent variables may prevent the correct results.","efa03983":"As we saw above, the usage in spring is significantly lower than in summer, fall, and winter, but it is most commonly used between 8 and 9 a.m. during rush hour and between 17 and 19 during rush hour.","5da776dd":"# Load Data","e61bda49":"# Modelling","a6ec97ad":"## All Feature","6fb3a05a":"# Numercial ","4a600f4f":"2018-04-05","e4e557b4":"## Datetime","5f57a0bf":"# GBR","e3a6b88d":"You can find out interesting facts. On the weekends, they are relatively used in the afternoon, but from Monday to Friday, they have the most hours to leave work.\nThe results came out as we had expected.","97e4e6e8":"# Categorical ","810ebe47":"The graph above shows the distribution chart of Count. As a graph before the log is processed on the left, you can see that the concentration is very high between 0 and 200. This means that it follows the normal distribution, as shown in the graph on the right, because it has a shifted power distribution.","c1978c2a":"Well, before we make a quick analysis, let's make some new features.\nAt first glance at the data, the datetime is likely to be used in some significant way.\nI have to check the number of bicycles I borrow regularly.\nLet's touch the datetime first.","1932d2ab":"# Feature engineering "}}