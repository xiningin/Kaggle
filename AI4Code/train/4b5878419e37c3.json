{"cell_type":{"38602864":"code","02451f42":"code","4770f0f9":"code","a485051e":"code","a6ef5e55":"code","b731f164":"code","639b71d6":"code","55c373bd":"code","f6e60e9e":"code","7188f315":"code","dcd46fea":"code","8d59f1f8":"code","4ca9ce27":"code","28855bfa":"code","078fdce8":"code","0c861d41":"code","cdcfb939":"code","c2dc286a":"code","f9372581":"code","e4e7d5d7":"code","7dcd5be7":"code","7119f51a":"code","208f0ac1":"code","10245058":"code","1f847c30":"code","30572ab5":"code","5f8157f1":"code","31a9587b":"code","6983fbc0":"code","33d877a0":"code","85bfd0ea":"code","bb37bb71":"code","74ebbc98":"code","a50180c4":"code","3376a822":"code","f233a19b":"code","9c83f547":"code","16ce4c2c":"code","7beadd01":"code","6b703b15":"code","646465d8":"code","426df22f":"code","cbb6a936":"code","70d140fe":"code","53a7a0dc":"code","e944a28b":"code","c7ff2357":"code","75c2c9a1":"code","3286a460":"code","8a15f60e":"code","d3e607f3":"code","e835ce36":"code","99567309":"code","e6870085":"code","18c45c4c":"code","e60562a0":"code","f8c3c17d":"code","916d7574":"code","f4f2a8ab":"code","06531863":"code","aad7781e":"code","77586545":"code","1c01faae":"code","4e92f10e":"code","163001c8":"code","19f1cdb4":"code","82b63850":"code","7c37cadb":"code","f40d1922":"code","6208345d":"code","cf448900":"code","f0884a37":"code","28bde395":"code","68f43d55":"code","34bdb89c":"code","e4c0f68e":"code","30b61f46":"code","99949723":"code","0ed29607":"markdown","90c1811c":"markdown","a3f66a31":"markdown","cf6abe1f":"markdown","061315ec":"markdown","90eef1fc":"markdown","432b877e":"markdown","96cc7b05":"markdown","8511f685":"markdown","d0b977f0":"markdown","81e8567f":"markdown","49e48aee":"markdown","4ce3384c":"markdown","321e889f":"markdown","184f337b":"markdown","48efea62":"markdown","b0d44321":"markdown","aa398bb0":"markdown","f7e389ec":"markdown","b53fa227":"markdown","65b7d4c6":"markdown","3e52ef48":"markdown","b9b1800c":"markdown","d49020e6":"markdown","5bc842d2":"markdown","f18c5c55":"markdown","4e86ff4c":"markdown","5756e883":"markdown","0d655dfa":"markdown","d4d0428b":"markdown","4a743c23":"markdown","44258b1e":"markdown","29d27be9":"markdown","e49e15ef":"markdown","ca4a95a9":"markdown","124b3594":"markdown","ac60e1ef":"markdown","68ec0dd5":"markdown","11ca92ec":"markdown","3cab14f9":"markdown","189dad71":"markdown","16ad4016":"markdown","bcc4fd20":"markdown","fd7a76dc":"markdown","a34b8d80":"markdown","5d91fe4d":"markdown","cb4a52af":"markdown","586aa711":"markdown","25d90c8f":"markdown","e790db77":"markdown","59285e84":"markdown","2a643b7d":"markdown","9967be9c":"markdown","d1d2a13a":"markdown"},"source":{"38602864":"#Load the librarys\nimport pandas as pd #To work with dataset\nimport numpy as np #Math library\nimport seaborn as sns #Graph library that use matplot in background\nimport matplotlib.pyplot as plt #to plot some parameters in seaborn\n\n#Importing the data\ndf_credit = pd.read_csv(\"..\/input\/german-credit-data-with-risk\/german_credit_data.csv\",index_col=0)","02451f42":"#Searching for Missings,type of data and also known the shape of data\nprint(df_credit.info())","4770f0f9":"#Looking unique values\nprint(df_credit.nunique())\n#Looking the data\nprint(df_credit.head())","a485051e":"# it's a library that we work with plotly\nimport plotly.offline as py \npy.init_notebook_mode(connected=True) # this code, allow us to work with offline plotly version\nimport plotly.graph_objs as go # it's like \"plt\" of matplot\nimport plotly.tools as tls # It's useful to we get some tools of plotly\nimport warnings # This library will be used to ignore some warnings\nfrom collections import Counter # To do counter of some features\n\ntrace0 = go.Bar(\n            x = df_credit[df_credit[\"Risk\"]== 'good'][\"Risk\"].value_counts().index.values,\n            y = df_credit[df_credit[\"Risk\"]== 'good'][\"Risk\"].value_counts().values,\n            name='Good credit'\n    )\n\ntrace1 = go.Bar(\n            x = df_credit[df_credit[\"Risk\"]== 'bad'][\"Risk\"].value_counts().index.values,\n            y = df_credit[df_credit[\"Risk\"]== 'bad'][\"Risk\"].value_counts().values,\n            name='Bad credit'\n    )\n\ndata = [trace0, trace1]\n\nlayout = go.Layout(\n    \n)\n\nlayout = go.Layout(\n    yaxis=dict(\n        title='Count'\n    ),\n    xaxis=dict(\n        title='Risk Variable'\n    ),\n    title='Target variable distribution'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='grouped-bar')","a6ef5e55":"df_good = df_credit.loc[df_credit[\"Risk\"] == 'good']['Age'].values.tolist()\ndf_bad = df_credit.loc[df_credit[\"Risk\"] == 'bad']['Age'].values.tolist()\ndf_age = df_credit['Age'].values.tolist()\n\n#First plot\ntrace0 = go.Histogram(\n    x=df_good,\n    histnorm='probability',\n    name=\"Good Credit\"\n)\n#Second plot\ntrace1 = go.Histogram(\n    x=df_bad,\n    histnorm='probability',\n    name=\"Bad Credit\"\n)\n#Third plot\ntrace2 = go.Histogram(\n    x=df_age,\n    histnorm='probability',\n    name=\"Overall Age\"\n)\n\n#Creating the grid\nfig = tls.make_subplots(rows=2, cols=2, specs=[[{}, {}], [{'colspan': 2}, None]],\n                          subplot_titles=('Good','Bad', 'General Distribuition'))\n\n#setting the figs\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig.append_trace(trace2, 2, 1)\n\nfig['layout'].update(showlegend=True, title='Age Distribuition', bargap=0.05)\npy.iplot(fig, filename='custom-sized-subplot-with-subplot-titles')","b731f164":"df_good = df_credit[df_credit[\"Risk\"] == 'good']\ndf_bad = df_credit[df_credit[\"Risk\"] == 'bad']\n\nfig, ax = plt.subplots(nrows=2, figsize=(12,8))\nplt.subplots_adjust(hspace = 0.4, top = 0.8)\n\ng1 = sns.distplot(df_good[\"Age\"], ax=ax[0], \n             color=\"g\")\ng1 = sns.distplot(df_bad[\"Age\"], ax=ax[0], \n             color='r')\ng1.set_title(\"Age Distribuition\", fontsize=15)\ng1.set_xlabel(\"Age\")\ng1.set_xlabel(\"Frequency\")\n\ng2 = sns.countplot(x=\"Age\",data=df_credit, \n              palette=\"hls\", ax=ax[1], \n              hue = \"Risk\")\ng2.set_title(\"Age Counting by Risk\", fontsize=15)\ng2.set_xlabel(\"Age\")\ng2.set_xlabel(\"Count\")\nplt.show()","639b71d6":"#Let's look the Credit Amount column\ninterval = (18, 25, 35, 60, 120)\n\ncats = ['Student', 'Young', 'Adult', 'Senior']\ndf_credit[\"Age_cat\"] = pd.cut(df_credit.Age, interval, labels=cats)\n\n\ndf_good = df_credit[df_credit[\"Risk\"] == 'good']\ndf_bad = df_credit[df_credit[\"Risk\"] == 'bad']","55c373bd":"trace0 = go.Box(\n    y=df_good[\"Credit amount\"],\n    x=df_good[\"Age_cat\"],\n    name='Good credit',\n    marker=dict(\n        color='#3D9970'\n    )\n)\n\ntrace1 = go.Box(\n    y=df_bad['Credit amount'],\n    x=df_bad['Age_cat'],\n    name='Bad credit',\n    marker=dict(\n        color='#FF4136'\n    )\n)\n    \ndata = [trace0, trace1]\n\nlayout = go.Layout(\n    yaxis=dict(\n        title='Credit Amount (US Dollar)',\n        zeroline=False\n    ),\n    xaxis=dict(\n        title='Age Categorical'\n    ),\n    boxmode='group'\n)\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='box-age-cat')","f6e60e9e":"#First plot\ntrace0 = go.Bar(\n    x = df_credit[df_credit[\"Risk\"]== 'good'][\"Housing\"].value_counts().index.values,\n    y = df_credit[df_credit[\"Risk\"]== 'good'][\"Housing\"].value_counts().values,\n    name='Good credit'\n)\n\n#Second plot\ntrace1 = go.Bar(\n    x = df_credit[df_credit[\"Risk\"]== 'bad'][\"Housing\"].value_counts().index.values,\n    y = df_credit[df_credit[\"Risk\"]== 'bad'][\"Housing\"].value_counts().values,\n    name=\"Bad Credit\"\n)\n\ndata = [trace0, trace1]\n\nlayout = go.Layout(\n    title='Housing Distribuition'\n)\n\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='Housing-Grouped')","7188f315":"fig = {\n    \"data\": [\n        {\n            \"type\": 'violin',\n            \"x\": df_good['Housing'],\n            \"y\": df_good['Credit amount'],\n            \"legendgroup\": 'Good Credit',\n            \"scalegroup\": 'No',\n            \"name\": 'Good Credit',\n            \"side\": 'negative',\n            \"box\": {\n                \"visible\": True\n            },\n            \"meanline\": {\n                \"visible\": True\n            },\n            \"line\": {\n                \"color\": 'blue'\n            }\n        },\n        {\n            \"type\": 'violin',\n            \"x\": df_bad['Housing'],\n            \"y\": df_bad['Credit amount'],\n            \"legendgroup\": 'Bad Credit',\n            \"scalegroup\": 'No',\n            \"name\": 'Bad Credit',\n            \"side\": 'positive',\n            \"box\": {\n                \"visible\": True\n            },\n            \"meanline\": {\n                \"visible\": True\n            },\n            \"line\": {\n                \"color\": 'green'\n            }\n        }\n    ],\n    \"layout\" : {\n        \"yaxis\": {\n            \"zeroline\": False,\n        },\n        \"violingap\": 0,\n        \"violinmode\": \"overlay\"\n    }\n}\n\n\npy.iplot(fig, filename = 'violin\/split', validate = False)","dcd46fea":"#First plot\ntrace0 = go.Bar(\n    x = df_credit[df_credit[\"Risk\"]== 'good'][\"Sex\"].value_counts().index.values,\n    y = df_credit[df_credit[\"Risk\"]== 'good'][\"Sex\"].value_counts().values,\n    name='Good credit'\n)\n\n#First plot 2\ntrace1 = go.Bar(\n    x = df_credit[df_credit[\"Risk\"]== 'bad'][\"Sex\"].value_counts().index.values,\n    y = df_credit[df_credit[\"Risk\"]== 'bad'][\"Sex\"].value_counts().values,\n    name=\"Bad Credit\"\n)\n\n#Second plot\ntrace2 = go.Box(\n    x = df_credit[df_credit[\"Risk\"]== 'good'][\"Sex\"],\n    y = df_credit[df_credit[\"Risk\"]== 'good'][\"Credit amount\"],\n    name=trace0.name\n)\n\n#Second plot 2\ntrace3 = go.Box(\n    x = df_credit[df_credit[\"Risk\"]== 'bad'][\"Sex\"],\n    y = df_credit[df_credit[\"Risk\"]== 'bad'][\"Credit amount\"],\n    name=trace1.name\n)\n\ndata = [trace0, trace1, trace2,trace3]\n\n\nfig = tls.make_subplots(rows=1, cols=2, \n                        subplot_titles=('Sex Count', 'Credit Amount by Sex'))\n\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace3, 1, 2)\n\nfig['layout'].update(height=400, width=800, title='Sex Distribuition', boxmode='group')\npy.iplot(fig, filename='sex-subplot')","8d59f1f8":"#First plot\ntrace0 = go.Bar(\n    x = df_credit[df_credit[\"Risk\"]== 'good'][\"Job\"].value_counts().index.values,\n    y = df_credit[df_credit[\"Risk\"]== 'good'][\"Job\"].value_counts().values,\n    name='Good credit Distribuition'\n)\n\n#Second plot\ntrace1 = go.Bar(\n    x = df_credit[df_credit[\"Risk\"]== 'bad'][\"Job\"].value_counts().index.values,\n    y = df_credit[df_credit[\"Risk\"]== 'bad'][\"Job\"].value_counts().values,\n    name=\"Bad Credit Distribuition\"\n)\n\ndata = [trace0, trace1]\n\nlayout = go.Layout(\n    title='Job Distribuition'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='grouped-bar')","4ca9ce27":"trace0 = go.Box(\n    x=df_good[\"Job\"],\n    y=df_good[\"Credit amount\"],\n    name='Good credit'\n)\n\ntrace1 = go.Box(\n    x=df_bad['Job'],\n    y=df_bad['Credit amount'],\n    name='Bad credit'\n)\n    \ndata = [trace0, trace1]\n\nlayout = go.Layout(\n    yaxis=dict(\n        title='Credit Amount distribuition by Job'\n    ),\n    boxmode='group'\n)\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='box-age-cat')","28855bfa":"\nfig = {\n    \"data\": [\n        {\n            \"type\": 'violin',\n            \"x\": df_good['Job'],\n            \"y\": df_good['Age'],\n            \"legendgroup\": 'Good Credit',\n            \"scalegroup\": 'No',\n            \"name\": 'Good Credit',\n            \"side\": 'negative',\n            \"box\": {\n                \"visible\": True\n            },\n            \"meanline\": {\n                \"visible\": True\n            },\n            \"line\": {\n                \"color\": 'blue'\n            }\n        },\n        {\n            \"type\": 'violin',\n            \"x\": df_bad['Job'],\n            \"y\": df_bad['Age'],\n            \"legendgroup\": 'Bad Credit',\n            \"scalegroup\": 'No',\n            \"name\": 'Bad Credit',\n            \"side\": 'positive',\n            \"box\": {\n                \"visible\": True\n            },\n            \"meanline\": {\n                \"visible\": True\n            },\n            \"line\": {\n                \"color\": 'green'\n            }\n        }\n    ],\n    \"layout\" : {\n        \"yaxis\": {\n            \"zeroline\": False,\n        },\n        \"violingap\": 0,\n        \"violinmode\": \"overlay\"\n    }\n}\n\n\npy.iplot(fig, filename = 'Age-Housing', validate = False)","078fdce8":"fig, ax = plt.subplots(figsize=(12,12), nrows=2)\n\ng1 = sns.boxplot(x=\"Job\", y=\"Credit amount\", data=df_credit, \n            palette=\"hls\", ax=ax[0], hue=\"Risk\")\ng1.set_title(\"Credit Amount by Job\", fontsize=15)\ng1.set_xlabel(\"Job Reference\", fontsize=12)\ng1.set_ylabel(\"Credit Amount\", fontsize=12)\n\ng2 = sns.violinplot(x=\"Job\", y=\"Age\", data=df_credit, ax=ax[1],  \n               hue=\"Risk\", split=True, palette=\"hls\")\ng2.set_title(\"Job Type reference x Age\", fontsize=15)\ng2.set_xlabel(\"Job Reference\", fontsize=12)\ng2.set_ylabel(\"Age\", fontsize=12)\n\nplt.subplots_adjust(hspace = 0.4,top = 0.9)\n\nplt.show()\n","0c861d41":"import plotly.figure_factory as ff\n\nimport numpy as np\n\n# Add histogram data\nx1 = np.log(df_good['Credit amount']) \nx2 = np.log(df_bad[\"Credit amount\"])\n\n# Group data together\nhist_data = [x1, x2]\n\ngroup_labels = ['Good Credit', 'Bad Credit']\n\n# Create distplot with custom bin_size\nfig = ff.create_distplot(hist_data, group_labels, bin_size=.2)\n\n# Plot!\npy.iplot(fig, filename='Distplot with Multiple Datasets')","cdcfb939":"#Ploting the good and bad dataframes in distplot\nplt.figure(figsize = (8,5))\n\ng= sns.distplot(df_good['Credit amount'], color='r')\ng = sns.distplot(df_bad[\"Credit amount\"], color='g')\ng.set_title(\"Credit Amount Frequency distribuition\", fontsize=15)\nplt.show()","c2dc286a":"from plotly import tools\nimport numpy as np\nimport plotly.graph_objs as go\n\ncount_good = go.Bar(\n    x = df_good[\"Saving accounts\"].value_counts().index.values,\n    y = df_good[\"Saving accounts\"].value_counts().values,\n    name='Good credit'\n)\ncount_bad = go.Bar(\n    x = df_bad[\"Saving accounts\"].value_counts().index.values,\n    y = df_bad[\"Saving accounts\"].value_counts().values,\n    name='Bad credit'\n)\n\n\nbox_1 = go.Box(\n    x=df_good[\"Saving accounts\"],\n    y=df_good[\"Credit amount\"],\n    name='Good credit'\n)\nbox_2 = go.Box(\n    x=df_bad[\"Saving accounts\"],\n    y=df_bad[\"Credit amount\"],\n    name='Bad credit'\n)\n\nscat_1 = go.Box(\n    x=df_good[\"Saving accounts\"],\n    y=df_good[\"Age\"],\n    name='Good credit'\n)\nscat_2 = go.Box(\n    x=df_bad[\"Saving accounts\"],\n    y=df_bad[\"Age\"],\n    name='Bad credit'\n)\n\ndata = [scat_1, scat_2, box_1, box_2, count_good, count_bad]\n\nfig = tools.make_subplots(rows=2, cols=2, specs=[[{}, {}], [{'colspan': 2}, None]],\n                          subplot_titles=('Count Saving Accounts','Credit Amount by Savings Acc', \n                                          'Age by Saving accounts'))\n\nfig.append_trace(count_good, 1, 1)\nfig.append_trace(count_bad, 1, 1)\n\nfig.append_trace(box_2, 1, 2)\nfig.append_trace(box_1, 1, 2)\n\nfig.append_trace(scat_1, 2, 1)\nfig.append_trace(scat_2, 2, 1)\n\n\n\nfig['layout'].update(height=700, width=800, title='Saving Accounts Exploration', boxmode='group')\n\npy.iplot(fig, filename='combined-savings')\n","f9372581":"print(\"Description of Distribuition Saving accounts by Risk:  \")\nprint(pd.crosstab(df_credit[\"Saving accounts\"],df_credit.Risk))\n\nfig, ax = plt.subplots(3,1, figsize=(12,12))\ng = sns.countplot(x=\"Saving accounts\", data=df_credit, palette=\"hls\", \n              ax=ax[0],hue=\"Risk\")\ng.set_title(\"Saving Accounts Count\", fontsize=15)\ng.set_xlabel(\"Saving Accounts type\", fontsize=12)\ng.set_ylabel(\"Count\", fontsize=12)\n\ng1 = sns.violinplot(x=\"Saving accounts\", y=\"Job\", data=df_credit, palette=\"hls\", \n               hue = \"Risk\", ax=ax[1],split=True)\ng1.set_title(\"Saving Accounts by Job\", fontsize=15)\ng1.set_xlabel(\"Savings Accounts type\", fontsize=12)\ng1.set_ylabel(\"Job\", fontsize=12)\n\ng = sns.boxplot(x=\"Saving accounts\", y=\"Credit amount\", data=df_credit, ax=ax[2],\n            hue = \"Risk\",palette=\"hls\")\ng2.set_title(\"Saving Accounts by Credit Amount\", fontsize=15)\ng2.set_xlabel(\"Savings Accounts type\", fontsize=12)\ng2.set_ylabel(\"Credit Amount(US)\", fontsize=12)\n\nplt.subplots_adjust(hspace = 0.4,top = 0.9)\n\nplt.show()\n","e4e7d5d7":"print(\"Values describe: \")\nprint(pd.crosstab(df_credit.Purpose, df_credit.Risk))\n\nplt.figure(figsize = (14,12))\n\nplt.subplot(221)\ng = sns.countplot(x=\"Purpose\", data=df_credit, \n              palette=\"hls\", hue = \"Risk\")\ng.set_xticklabels(g.get_xticklabels(),rotation=45)\ng.set_xlabel(\"\", fontsize=12)\ng.set_ylabel(\"Count\", fontsize=12)\ng.set_title(\"Purposes Count\", fontsize=20)\n\nplt.subplot(222)\ng1 = sns.violinplot(x=\"Purpose\", y=\"Age\", data=df_credit, \n                    palette=\"hls\", hue = \"Risk\",split=True)\ng1.set_xticklabels(g1.get_xticklabels(),rotation=45)\ng1.set_xlabel(\"\", fontsize=12)\ng1.set_ylabel(\"Count\", fontsize=12)\ng1.set_title(\"Purposes by Age\", fontsize=20)\n\nplt.subplot(212)\ng2 = sns.boxplot(x=\"Purpose\", y=\"Credit amount\", data=df_credit, \n               palette=\"hls\", hue = \"Risk\")\ng2.set_xlabel(\"Purposes\", fontsize=12)\ng2.set_ylabel(\"Credit Amount\", fontsize=12)\ng2.set_title(\"Credit Amount distribuition by Purposes\", fontsize=20)\n\nplt.subplots_adjust(hspace = 0.6, top = 0.8)\n\nplt.show()","7dcd5be7":"plt.figure(figsize = (12,14))\n\ng= plt.subplot(311)\ng = sns.countplot(x=\"Duration\", data=df_credit, \n              palette=\"hls\",  hue = \"Risk\")\ng.set_xlabel(\"Duration Distribuition\", fontsize=12)\ng.set_ylabel(\"Count\", fontsize=12)\ng.set_title(\"Duration Count\", fontsize=20)\n\ng1 = plt.subplot(312)\ng1 = sns.pointplot(x=\"Duration\", y =\"Credit amount\",data=df_credit,\n                   hue=\"Risk\", palette=\"hls\")\ng1.set_xlabel(\"Duration\", fontsize=12)\ng1.set_ylabel(\"Credit Amount(US)\", fontsize=12)\ng1.set_title(\"Credit Amount distribuition by Duration\", fontsize=20)\n\ng2 = plt.subplot(313)\ng2 = sns.distplot(df_good[\"Duration\"], color='g')\ng2 = sns.distplot(df_bad[\"Duration\"], color='r')\ng2.set_xlabel(\"Duration (month)\", fontsize=12)\ng2.set_ylabel(\"Frequency\", fontsize=12)\ng2.set_title(\"Duration Frequency x good and bad Credit\", fontsize=20)\n\nplt.subplots_adjust(wspace = 0.4, hspace = 0.4,top = 0.9)\n\nplt.show()","7119f51a":"#First plot\ntrace0 = go.Bar(\n    x = df_credit[df_credit[\"Risk\"]== 'good'][\"Checking account\"].value_counts().index.values,\n    y = df_credit[df_credit[\"Risk\"]== 'good'][\"Checking account\"].value_counts().values,\n    name='Good credit Distribuition' \n    \n)\n\n#Second plot\ntrace1 = go.Bar(\n    x = df_credit[df_credit[\"Risk\"]== 'bad'][\"Checking account\"].value_counts().index.values,\n    y = df_credit[df_credit[\"Risk\"]== 'bad'][\"Checking account\"].value_counts().values,\n    name=\"Bad Credit Distribuition\"\n)\n\ndata = [trace0, trace1]\n\nlayout = go.Layout(\n    title='Checking accounts Distribuition',\n    xaxis=dict(title='Checking accounts name'),\n    yaxis=dict(title='Count'),\n    barmode='group'\n)\n\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename = 'Age-ba', validate = False)","208f0ac1":"df_good = df_credit[df_credit[\"Risk\"] == 'good']\ndf_bad = df_credit[df_credit[\"Risk\"] == 'bad']\n\ntrace0 = go.Box(\n    y=df_good[\"Credit amount\"],\n    x=df_good[\"Checking account\"],\n    name='Good credit',\n    marker=dict(\n        color='#3D9970'\n    )\n)\n\ntrace1 = go.Box(\n    y=df_bad['Credit amount'],\n    x=df_bad['Checking account'],\n    name='Bad credit',\n    marker=dict(\n        color='#FF4136'\n    )\n)\n    \ndata = [trace0, trace1]\n\nlayout = go.Layout(\n    yaxis=dict(\n        title='Cheking distribuition'\n    ),\n    boxmode='group'\n)\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='box-age-cat')","10245058":"print(\"Total values of the most missing variable: \")\nprint(df_credit.groupby(\"Checking account\")[\"Checking account\"].count())\n\nplt.figure(figsize = (12,10))\n\ng = plt.subplot(221)\ng = sns.countplot(x=\"Checking account\", data=df_credit, \n              palette=\"hls\", hue=\"Risk\")\ng.set_xlabel(\"Checking Account\", fontsize=12)\ng.set_ylabel(\"Count\", fontsize=12)\ng.set_title(\"Checking Account Counting by Risk\", fontsize=20)\n\ng1 = plt.subplot(222)\ng1 = sns.violinplot(x=\"Checking account\", y=\"Age\", data=df_credit, palette=\"hls\", hue = \"Risk\",split=True)\ng1.set_xlabel(\"Checking Account\", fontsize=12)\ng1.set_ylabel(\"Age\", fontsize=12)\ng1.set_title(\"Age by Checking Account\", fontsize=20)\n\ng2 = plt.subplot(212)\ng2 = sns.boxplot(x=\"Checking account\",y=\"Credit amount\", data=df_credit,hue='Risk',palette=\"hls\")\ng2.set_xlabel(\"Checking Account\", fontsize=12)\ng2.set_ylabel(\"Credit Amount(US)\", fontsize=12)\ng2.set_title(\"Credit Amount by Cheking Account\", fontsize=20)\n\nplt.subplots_adjust(wspace = 0.2, hspace = 0.3, top = 0.9)\n\nplt.show()\nplt.show()","1f847c30":"print(pd.crosstab(df_credit.Sex, df_credit.Job))","30572ab5":"plt.figure(figsize = (10,6))\n\ng = sns.violinplot(x=\"Housing\",y=\"Job\",data=df_credit,\n                   hue=\"Risk\", palette=\"hls\",split=True)\ng.set_xlabel(\"Housing\", fontsize=12)\ng.set_ylabel(\"Job\", fontsize=12)\ng.set_title(\"Housing x Job - Dist\", fontsize=20)\n\nplt.show()","5f8157f1":"print(pd.crosstab(df_credit[\"Checking account\"],df_credit.Sex))","31a9587b":"date_int = [\"Purpose\", 'Sex']\ncm = sns.light_palette(\"green\", as_cmap=True)\npd.crosstab(df_credit[date_int[0]], df_credit[date_int[1]]).style.background_gradient(cmap = cm)","6983fbc0":"date_int = [\"Purpose\", 'Sex']\ncm = sns.light_palette(\"green\", as_cmap=True)\npd.crosstab(df_credit[date_int[0]], df_credit[date_int[1]]).style.background_gradient(cmap = cm)","33d877a0":"print(\"Purpose : \",df_credit.Purpose.unique())\nprint(\"Sex : \",df_credit.Sex.unique())\nprint(\"Housing : \",df_credit.Housing.unique())\nprint(\"Saving accounts : \",df_credit['Saving accounts'].unique())\nprint(\"Risk : \",df_credit['Risk'].unique())\nprint(\"Checking account : \",df_credit['Checking account'].unique())\nprint(\"Aget_cat : \",df_credit['Age_cat'].unique())","85bfd0ea":"def one_hot_encoder(df, nan_as_category = False):\n    original_columns = list(df.columns)\n    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category, drop_first=True)\n    new_columns = [c for c in df.columns if c not in original_columns]\n    return df, new_columns","bb37bb71":"df_credit['Saving accounts'] = df_credit['Saving accounts'].fillna('no_inf')\ndf_credit['Checking account'] = df_credit['Checking account'].fillna('no_inf')\n\n#Purpose to Dummies Variable\ndf_credit = df_credit.merge(pd.get_dummies(df_credit.Purpose, drop_first=True, prefix='Purpose'), left_index=True, right_index=True)\n#Sex feature in dummies\ndf_credit = df_credit.merge(pd.get_dummies(df_credit.Sex, drop_first=True, prefix='Sex'), left_index=True, right_index=True)\n# Housing get dummies\ndf_credit = df_credit.merge(pd.get_dummies(df_credit.Housing, drop_first=True, prefix='Housing'), left_index=True, right_index=True)\n# Housing get Saving Accounts\ndf_credit = df_credit.merge(pd.get_dummies(df_credit[\"Saving accounts\"], drop_first=True, prefix='Savings'), left_index=True, right_index=True)\n# Housing get Risk\ndf_credit = df_credit.merge(pd.get_dummies(df_credit.Risk, prefix='Risk'), left_index=True, right_index=True)\n# Housing get Checking Account\ndf_credit = df_credit.merge(pd.get_dummies(df_credit[\"Checking account\"], drop_first=True, prefix='Check'), left_index=True, right_index=True)\n# Housing get Age categorical\ndf_credit = df_credit.merge(pd.get_dummies(df_credit[\"Age_cat\"], drop_first=True, prefix='Age_cat'), left_index=True, right_index=True)","74ebbc98":"#Excluding the missing columns\ndel df_credit[\"Saving accounts\"]\ndel df_credit[\"Checking account\"]\ndel df_credit[\"Purpose\"]\ndel df_credit[\"Sex\"]\ndel df_credit[\"Housing\"]\ndel df_credit[\"Age_cat\"]\ndel df_credit[\"Risk\"]\ndel df_credit['Risk_good']","a50180c4":"plt.figure(figsize=(14,12))\nsns.heatmap(df_credit.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True,  linecolor='white', annot=True)\nplt.show()","3376a822":"from sklearn.model_selection import train_test_split, KFold, cross_val_score # to split the data\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, fbeta_score #To evaluate our model\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Algorithmns models to be compared\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\n","f233a19b":"df_credit['Credit amount'] = np.log(df_credit['Credit amount'])","9c83f547":"#Creating the X and y variables\nX = df_credit.drop('Risk_bad', 1).values\ny = df_credit[\"Risk_bad\"].values\n\n# Spliting X and y into train and test version\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=42)","16ce4c2c":"# # to feed the random state\n# seed = 7\n\n# # prepare models\n# models = []\n# models.append(('LR', LogisticRegression()))\n# models.append(('LDA', LinearDiscriminantAnalysis()))\n# models.append(('KNN', KNeighborsClassifier()))\n# models.append(('CART', DecisionTreeClassifier()))\n# models.append(('NB', GaussianNB()))\n# models.append(('RF', RandomForestClassifier()))\n# models.append(('SVM', SVC(gamma='auto')))\n# models.append(('XGB', XGBClassifier()))\n\n# # evaluate each model in turn\n# results = []\n# names = []\n# scoring = 'recall'\n\n# for name, model in models:\n#         kfold = KFold(n_splits=10, random_state=seed)\n#         cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n#         results.append(cv_results)\n#         names.append(name)\n#         msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n#         print(msg)\n        \n# # boxplot algorithm comparison\n# fig = plt.figure(figsize=(11,6))\n# fig.suptitle('Algorithm Comparison')\n# ax = fig.add_subplot(111)\n# plt.boxplot(results)\n# ax.set_xticklabels(names)\n# plt.show()","7beadd01":"# #Seting the Hyper Parameters\n# param_grid = {\"max_depth\": [3,5, 7, 10,None],\n#               \"n_estimators\":[3,5,10,25,50,150],\n#               \"max_features\": [4,7,15,20]}\n\n# #Creating the classifier\n# model = RandomForestClassifier(random_state=2)\n\n# grid_search = GridSearchCV(model, param_grid=param_grid, cv=5, scoring='recall', verbose=4)\n# grid_search.fit(X_train, y_train)","6b703b15":"# print(grid_search.best_score_)\n# print(grid_search.best_params_)","646465d8":"# rf = RandomForestClassifier(max_depth=None, max_features=10, n_estimators=15, random_state=2)\n\n# #trainning with the best params\n# rf.fit(X_train, y_train)","426df22f":"# #Testing the model \n# #Predicting using our  model\n# y_pred = rf.predict(X_test)\n\n# # Verificaar os resultados obtidos\n# print(accuracy_score(y_test,y_pred))\n# print(\"\\n\")\n# print(confusion_matrix(y_test, y_pred))\n# print(\"\\n\")\n# print(fbeta_score(y_test, y_pred, beta=2))","cbb6a936":"# from sklearn.utils import resample\n# from sklearn.metrics import roc_curve","70d140fe":"# # Criando o classificador logreg\n# GNB = GaussianNB()\n\n# # Fitting with train data\n# model = GNB.fit(X_train, y_train)","53a7a0dc":"# # Printing the Training Score\n# print(\"Training score data: \")\n# print(model.score(X_train, y_train))","e944a28b":"# y_pred = model.predict(X_test)\n\n# print(accuracy_score(y_test,y_pred))\n# print(\"\\n\")\n# print(confusion_matrix(y_test, y_pred))\n# print(\"\\n\")\n# print(classification_report(y_test, y_pred))","c7ff2357":"# #Predicting proba\n# y_pred_prob = model.predict_proba(X_test)[:,1]\n\n# # Generate ROC curve values: fpr, tpr, thresholds\n# fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\n# # Plot ROC curve\n# plt.plot([0, 1], [0, 1], 'k--')\n# plt.plot(fpr, tpr)\n# plt.xlabel('False Positive Rate')\n# plt.ylabel('True Positive Rate')\n# plt.title('ROC Curve')\n# plt.show()","75c2c9a1":"# from sklearn.model_selection import KFold\n# from sklearn.model_selection import cross_val_score\n# from sklearn.pipeline import Pipeline\n# from sklearn.pipeline import FeatureUnion\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.decomposition import PCA\n# from sklearn.feature_selection import SelectKBest","3286a460":"# features = []\n# features.append(('pca', PCA(n_components=2)))\n# features.append(('select_best', SelectKBest(k=6)))\n# feature_union = FeatureUnion(features)\n# # create pipeline\n# estimators = []\n# estimators.append(('feature_union', feature_union))\n# estimators.append(('logistic', GaussianNB()))\n# model = Pipeline(estimators)\n# # evaluate pipeline\n# seed = 7\n# kfold = KFold(n_splits=10, random_state=seed)\n# results = cross_val_score(model, X_train, y_train, cv=kfold)\n# print(results.mean())","8a15f60e":"# model.fit(X_train, y_train)\n# y_pred = model.predict(X_test)\n\n# print(accuracy_score(y_test,y_pred))\n# print(\"\\n\")\n# print(confusion_matrix(y_test, y_pred))\n# print(\"\\n\")\n# print(fbeta_score(y_test, y_pred, beta=2))","d3e607f3":"#Creating the X and y variables\n# Defined earlier\n# X = df_credit.drop('Risk_bad', 1).values\n# y = df_credit[\"Risk_bad\"].values","e835ce36":"X.shape","99567309":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","e6870085":"from imblearn.over_sampling import SMOTE","18c45c4c":"y_train.shape","e60562a0":"np.unique(y_train, return_counts=True)","f8c3c17d":"oversample = SMOTE()\nos_X,os_y =oversample.fit_resample(X_train, y_train)","916d7574":"np.unique(os_y, return_counts=True)","f4f2a8ab":"import optuna.integration.lightgbm as lgb\nimport optuna\nfrom sklearn.model_selection import RepeatedKFold","06531863":"rkf = RepeatedKFold(n_splits=10, n_repeats=10, random_state=42)\n\n# select another metric?\n# binary error is inversely proportional to accuracy\nparams = {\n        \"objective\": \"binary\",\n        \"metric\": \"binary_logloss\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",                \n        \"seed\": 42\n    }\n\n# minimize the error - maximise the accuracy\nstudy_tuner = optuna.create_study(direction='minimize')\n# dtrain = lgb.Dataset(X_train, label=y_train)\n# Passing SMOTE data\ndtrain = lgb.Dataset(os_X, label=os_y)\n\n\n# Suppress information only outputs - otherwise optuna is \n# quite verbose, which can be nice, but takes up a lot of space\noptuna.logging.set_verbosity(optuna.logging.WARNING) \n\n# Run optuna LightGBMTunerCV tuning of LightGBM with cross-validation\n\n# learning rate callback was initially implemented - why is it hardcoded so?\n# training time reduced considerably when using callback (Took more than 2 hrs to train only feature fraction and num leaves)- the session failed after that\n# so tried to comment out the callback. Now, it took few mins to run the first two hyperparameters that it trains - validation score more or less similar. \n# using callback - is it important?\n\n# Possibly it is not important - https:\/\/github.com\/Microsoft\/LightGBM\/issues\/1841\n# However, keep in mind that shrinking the learning rate over increasing boosting rounds could lead to tremendously high overfitting; \n# so I guess that you will rarely want to decrease the learning rate over time \n# (as opposed to in deep learning, where the lower bias in the model across epochs helps mitigating the overfit caused by the shrinkage) quoting \"julioasotodv\"\ntuner = lgb.LightGBMTunerCV(params, \n                            dtrain, \n                            study=study_tuner,\n                            verbose_eval=False,                            \n                            early_stopping_rounds=250,\n                            time_budget=19800, # Time budget of 5 hours, we will not really need it\n                            seed = 42,\n                            folds=rkf,\n                            num_boost_round=10000,\n#                             callbacks=[lgb.reset_parameter(learning_rate = [0.005]*200 + [0.001]*9800) ] #[0.1]*5 + [0.05]*15 + [0.01]*45 + \n                           )\n\ntuner.run()","aad7781e":"print(tuner.best_params)\n# Classification error\nprint(tuner.best_score)\n# Or expressed as accuracy\nprint(1.0-tuner.best_score)","77586545":"# Set-up a temporary set of best parameters that we will use as a starting point below.\n# Note that optuna will complain about values on the edge of the search space, so we move \n# such values a tiny little bit inside the search space.\ntmp_best_params = tuner.best_params\nif tmp_best_params['feature_fraction']==1:\n    tmp_best_params['feature_fraction']=1.0-1e-9\nif tmp_best_params['feature_fraction']==0:\n    tmp_best_params['feature_fraction']=1e-9\nif tmp_best_params['bagging_fraction']==1:\n    tmp_best_params['bagging_fraction']=1.0-1e-9\nif tmp_best_params['bagging_fraction']==0:\n    tmp_best_params['bagging_fraction']=1e-9  ","1c01faae":"# import lightgbm as lgb\n# dtrain = lgb.Dataset(X, label=y)\n\n# # We will track how many training rounds we needed for our best score.\n# # We will use that number of rounds later.\n# best_score = 999\n# training_rounds = 10000\n\n# # Declare how we evaluate how good a set of hyperparameters are, i.e.\n# # declare an objective function.\n# def objective(trial):\n#     # Specify a search space using distributions across plausible values of hyperparameters.\n#     param = {\n#         \"objective\": \"binary\",\n#         \"metric\": \"binary_error\",\n#         \"verbosity\": -1,\n#         \"boosting_type\": \"gbdt\",                \n#         \"seed\": 42,\n#         'learning_rate':0.1,\n#         'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n#         'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n#         'num_leaves': trial.suggest_int('num_leaves', 2, 512),\n#         'feature_fraction': trial.suggest_uniform('feature_fraction', 0.1, 1.0),\n#         'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.1, 1.0),\n#         'bagging_freq': trial.suggest_int('bagging_freq', 0, 15),\n#         'min_child_samples': trial.suggest_int('min_child_samples', 1, 100),\n#         'seed': 1979,\n#         'feature_pre_filter':False\n#     }\n    \n#     # Run LightGBM for the hyperparameter values\n#     lgbcv = lgb.cv(param,\n#                    dtrain,\n# #                    categorical_feature=ids_of_categorical,\n#                    folds=rkf,\n#                    verbose_eval=False,                   \n#                    early_stopping_rounds=250,                   \n#                    num_boost_round=10000  \n# #                    callbacks=[lgb.reset_parameter(learning_rate = [0.005]*200 + [0.001]*9800) ]\n#                   )\n    \n#     cv_score = lgbcv['binary_error-mean'][-1] + lgbcv['binary_error-stdv'][-1]\n#     if cv_score<best_score:\n#         training_rounds = len( list(lgbcv.values())[0] )\n    \n#     # Return metric of interest\n#     return cv_score\n\n# # Suppress information only outputs - otherwise optuna is \n# # quite verbose, which can be nice, but takes up a lot of space\n# optuna.logging.set_verbosity(optuna.logging.WARNING) \n\n# # We search for another 4 hours (3600 s are an hours, so timeout=14400).\n# # We could instead do e.g. n_trials=1000, to try 1000 hyperparameters chosen \n# # by optuna or set neither timeout or n_trials so that we keep going until \n# # the user interrupts (\"Cancel run\").\n# study = optuna.create_study(direction='minimize')  \n# study.enqueue_trial(tmp_best_params)\n# study.optimize(objective, timeout=14400) ","4e92f10e":"# optuna.visualization.plot_optimization_history(study)","163001c8":"# optuna.visualization.plot_parallel_coordinate(study)","19f1cdb4":"# optuna.visualization.plot_param_importances(study)","82b63850":"# optuna.visualization.plot_slice(study)\n","7c37cadb":"# print(study.best_params)","f40d1922":"# # Classification error\n# print(study.best_value)\n# # Or expressed as accuracy\n# print(1.0-study.best_value)","6208345d":"%%time \nimport lightgbm as lgb\n# dtrain = lgb.Dataset(X_train, label=y_train)\ndtrain = lgb.Dataset(os_X, label=os_y)\n\nrkf = RepeatedKFold(n_splits=10, n_repeats=10, random_state=42)\n\n# We will track how many training rounds we needed for our best score.\n# We will use that number of rounds later.\nbest_score = 99999\ntraining_rounds = 10000\n\n# Declare how we evaluate how good a set of hyperparameters are, i.e.\n# declare an objective function.\n\ndef objective(trial):\n    # Specify a search space using distributions across plausible values of hyperparameters.\n    param = {\n        \"objective\": \"binary\",\n        \"metric\": \"binary_logloss\",#using similar to baseline\n        \"verbosity\": -1,\n        'learning_rate':0.01,\n        \"boosting_type\": \"gbdt\",   \n        'num_boost_round': trial.suggest_int('num_boost_round', 1, 10000),\n        'max_depth': trial.suggest_int('max_depth', 1, 200),\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 100.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 100.0),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.1, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.1, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 0, 15),\n        'min_child_samples': trial.suggest_int('min_child_samples', 1, 100),\n        'seed': 1979,\n        'feature_pre_filter':False\n    }\n    \n    # Run LightGBM for the hyperparameter values\n    lgbcv = lgb.cv(param,\n                   dtrain,\n#                    categorical_feature=ids_of_categorical,\n                   folds=rkf,\n                   verbose_eval=False,    \n                   early_stopping_rounds=250,                   \n                  )\n    \n#     cv_score = lgbcv['binary_error-mean'][-1] + lgbcv['binary_error-stdv'][-1]\n#     if cv_score<best_score:\n#         training_rounds = len( list(lgbcv.values())[0] )\n\n# Using similar optimisation\n      # Return the minimum log-loss found from cv\n    cv_score = pd.Series(lgbcv['binary_logloss-mean']).min()\n    \n#     # The index of the minimum log-loss value is the number of trees\n    optimal_trees = pd.Series(lgbcv['binary_logloss-mean']).idxmin()\n\n#     cv_score = lgbcv['binary_logloss-mean'][-1] + lgbcv['binary_logloss-stdv'][-1]\n    if cv_score<best_score:\n        training_rounds = len( list(lgbcv.values())[0] )\n    \n#     # Add the number of trees to the Trial stage\n#     trial.set_user_attr(\"optimal_trees\", optimal_trees)\n    \n    # Return metric of interest\n    return cv_score\n","cf448900":"# Suppress information only outputs - otherwise optuna is \n# quite verbose, which can be nice, but takes up a lot of space\n# optuna.logging.set_verbosity(optuna.logging.WARNING) \n\n# We search for another 4 hours (3600 s are an hours, so timeout=14400).\n# We could instead do e.g. n_trials=1000, to try 1000 hyperparameters chosen \n# by optuna or set neither timeout or n_trials so that we keep going until \n# the user interrupts (\"Cancel run\").\nstudy1 = optuna.create_study(direction='minimize')  \nstudy1.enqueue_trial(tmp_best_params)\n\n# We search for another 3 hours (3600 s are an hours, so timeout=10800).\n\nstudy1.optimize(objective, timeout=10800)\n# study1.optimize(objective, n_trials=300)","f0884a37":"# Print optimal trees\n# print('optimal number of trees: ', study1.best_trial.user_attrs['optimal_trees'])\n\nprint('best logloss: ', study1.best_trial.values[0])\n\n# Print best params\nstudy1.best_trial.params","28bde395":"optuna.visualization.plot_param_importances(study1)","68f43d55":"optuna.visualization.plot_parallel_coordinate(study1)","34bdb89c":"# Retrain model on all of X_train with optimal parameters to return a model object\n\ndtrain = lgb.Dataset(os_X, label=os_y)\n\nparam = {\n    \"objective\": \"binary\",\n    \"metric\": \"binary_error\", #binary_logloss\n    \"verbosity\": -1,\n    \"boosting_type\": \"gbdt\",                \n    \"seed\": 42,\n    'lambda_l1': study1.best_trial.params['lambda_l1'],\n    'lambda_l2': study1.best_trial.params['lambda_l2'],\n    'num_leaves': study1.best_trial.params['num_leaves'],\n    'num_boost_round': study1.best_trial.params['num_boost_round'],\n    'max_depth': study1.best_trial.params['max_depth'],\n    'feature_fraction': study1.best_trial.params['feature_fraction'],\n    'bagging_fraction': study1.best_trial.params['bagging_fraction'],\n    'bagging_freq': study1.best_trial.params['bagging_freq'],\n    'min_child_samples': study1.best_trial.params['min_child_samples'],\n    'learning_rate':0.1,\n    'feature_pre_filter':'false'\n}\n    \nclf = lgb.train(train_set=dtrain,\n          params=param,\n#           num_boost_round=study1.best_trial.user_attrs['optimal_trees']\n         )","e4c0f68e":"y_pred = clf.predict(X_test)\n","30b61f46":"from sklearn.metrics import log_loss, accuracy_score\n\n\ntest_metric = log_loss(y_test, y_pred) #log_loss\ncross_val_metric = study1.best_trial.values[0]\n\nprint('Log-loss from cross validation on training data: ', cross_val_metric)\nprint('Log-loss from unseen test data: ', test_metric)","99949723":"# SMOTE Results in major overfitting!!!!!","0ed29607":"# **1. Introduction:** \n<h2>Context<\/h2>\nThe original dataset contains 1000 entries with 20 categorial\/symbolic attributes prepared by Prof. Hofmann. In this dataset, each entry represents a person who takes a credit by a bank. Each person is classified as good or bad credit risks according to the set of attributes. The link to the original dataset can be found below.\n\n<h2>Content<\/h2>\nIt is almost impossible to understand the original dataset due to its complicated system of categories and symbols. Thus, I wrote a small Python script to convert it into a readable CSV file. Several columns are simply ignored, because in my opinion either they are not important or their descriptions are obscure. The selected attributes are:\n\n<b>Age <\/b>(numeric)<br>\n<b>Sex <\/b>(text: male, female)<br>\n<b>Job <\/b>(numeric: 0 - unskilled and non-resident, 1 - unskilled and resident, 2 - skilled, 3 - highly skilled)<br>\n<b>Housing<\/b> (text: own, rent, or free)<br>\n<b>Saving accounts<\/b> (text - little, moderate, quite rich, rich)<br>\n<b>Checking account <\/b>(numeric, in DM - Deutsch Mark)<br>\n<b>Credit amount<\/b> (numeric, in DM)<br>\n<b>Duration<\/b> (numeric, in month)<br>\n<b>Purpose<\/b>(text: car, furniture\/equipment, radio\/TV, domestic appliances, repairs, education, business, vacation\/others<br>\n<b>Risk <\/b> (Value target - Good or Bad Risk)<br>","90c1811c":"<h3>Distribuition of Credit Amount by Housing<\/h3>","a3f66a31":"## Deleting the old features","cf6abe1f":"\nInteresting, we can see that the highest duration have the high amounts. <br>\nThe highest density is between [12 ~ 18 ~ 24] months<br>\nIt all make sense.\n","061315ec":"Very interesting. Almost all models shows a low value to recall. \n\nWe can observe that our best results was with CART, NB and XGBoost. <br>\nI will implement some models and try to do a simple Tunning on them","90eef1fc":"<b> How can I set the boxplots in different places? how can I use the same legend to both graphs?<\/b>","432b877e":"**W\/O SMOTE**\n\ntuner.best_params = {'objective': 'binary', 'metric': 'binary_logloss', 'verbosity': -1, 'boosting_type': 'gbdt', 'seed': 42, 'feature_pre_filter': False, 'lambda_l1': 4.311120696175597e-06, 'lambda_l2': 0.052582244425462714, 'num_leaves': 13, 'feature_fraction': 0.45199999999999996, 'bagging_fraction': 1.0, 'bagging_freq': 0, 'min_child_samples': 20}\n\nLogloss error: tuner.best_score = 0.5280193431880829","96cc7b05":"First, let's look the distribuition ","8511f685":"## Transforming the data into Dummy variables","d0b977f0":"# **7.2 Model 2:** <a id=\"Modelling 2\"><\/a> <br>","81e8567f":"<h2>I will now Look the distribuition of Housing own and rent by Risk<\/h2>\n","49e48aee":"WITH SMOTE: \n\n{'num_boost_round': 5385,\n 'max_depth': 95,\n 'lambda_l1': 0.0016864295177581943,\n 'lambda_l2': 0.01064782356317584,\n 'num_leaves': 192,\n 'feature_fraction': 0.489464930792113,\n 'bagging_fraction': 0.5723988870610315,\n 'bagging_freq': 8,\n 'min_child_samples': 1}\n \n \n best logloss:  0.38668516640141903\n","4ce3384c":"Looking the distribuition of Credit Amont","321e889f":"optimal number of trees:  32\nbest logloss:  0.5821837794857733\n\n\n\n{'lambda_l1': 0.027557048144281192,\n 'lambda_l2': 6.733793502288289,\n 'num_leaves': 300,\n 'feature_fraction': 0.9995093343010514,\n 'bagging_fraction': 0.6997723787350977,\n 'bagging_freq': 1,\n 'min_child_samples': 3}","184f337b":"## SMOTE","48efea62":"I will do some explorations through the Job\n- Distribuition\n- Crossed by Credit amount\n- Crossed by Age","b0d44321":"<h2> Checking Account variable <\/h2>","aa398bb0":"Crosstab session and anothers to explore our data by another metrics a little deep","f7e389ec":"Duration of the loans distribuition and density","b53fa227":"we can see that the own and good risk have a high correlation","65b7d4c6":"Copied from https:\/\/www.kaggle.com\/kabure\/<br>","3e52ef48":"'num_boost_round': 5385,\n 'max_depth': 95,\n 'lambda_l1': 0.0016864295177581943,\n 'lambda_l2': 0.01064782356317584,\n 'num_leaves': 192,\n 'feature_fraction': 0.489464930792113,\n 'bagging_fraction': 0.5723988870610315,\n 'bagging_freq': 8,\n 'min_child_samples': 1","b9b1800c":"# **7.1 Model 1 :** <a id=\"Modelling 1\"><\/a> <br>\n- Using Random Forest to predictict the credit score \n- Some of Validation Parameters","d49020e6":"{'lambda_l1': 0.3216393371762933, 'lambda_l2': 0.020172055755509347, 'num_leaves': 501, 'feature_fraction': 0.7259695210423447, 'bagging_fraction': 0.9563525530698169, 'bagging_freq': 1, 'min_child_samples': 15}\n","5bc842d2":"<h2>Looking the diference by Sex<\/h2>","f18c5c55":"<h2>Let's start looking through target variable and their distribuition<\/h2>","4e86ff4c":"Using optuna to perform initial hyperparamter optimisation\n\nThe following hyperparamters are tuned using the implementation\n\n* feature_fraction\n* num_leaves\n* bagging_fraction and bagging_freq\n* feature_fraction (again)\n* regularization factors (i.e. 'lambda_l1' and 'lambda_l2')\n* min_child_samples","5756e883":"# **4. Some explorations:** <a id=\"Explorations\"><\/a> <br>\n\n- Starting by distribuition of column Age.\n- Some Seaborn graphical\n- Columns crossing\n\n","0d655dfa":"# Hyperparameter optimization with cross-validation\n","d4d0428b":"Interesting moviments! Highest values come from category \"free\" and we have a different distribuition by Risk","4a743c23":"**With SMOTE**\n\ntune.best_params = {'objective': 'binary', 'metric': 'binary_logloss', 'verbosity': -1, 'boosting_type': 'gbdt', 'seed': 42, 'feature_pre_filter': False, 'lambda_l1': 2.8205516919422897e-07, 'lambda_l2': 5.737854219621876e-07, 'num_leaves': 34, 'feature_fraction': 0.6, 'bagging_fraction': 1.0, 'bagging_freq': 0, 'min_child_samples': 20}\n\nLogloss error: tuner.best_score = 0.3925921826901682\n","44258b1e":"<a id=\"Introduction\"><\/a> <br>\n","29d27be9":"## Trying to improve the score","e49e15ef":"How can I better configure the legends?  I am trying to substitute the graph below, so how can I use the violinplot on subplots of plotly?","ca4a95a9":"Pretty and interesting distribution...","124b3594":"## Training and optimizing after getting a best parameters using optuna implementation","ac60e1ef":"# **5. Correlation:** <a id=\"Correlation\"><\/a> <br>\n- Looking at the data correlation\n<h1>Looking at the correlation of the data","68ec0dd5":"# **6. Preprocessing:** <a id=\"Preprocessing\"><\/a> <br>\n- Importing ML librarys\n- Setting X and y variables to the prediction\n- Splitting Data\n","11ca92ec":"I will create categories of Age and look the distribuition of Credit Amount by Risk...\n","3cab14f9":"## Let's do some feature engineering on this values and create variable Dummies of the values","189dad71":"## Looking the total of values in each categorical feature","16ad4016":"With the Gaussian Model we got a best recall. ","bcc4fd20":"<a id=\"Known\"><\/a> <br>\n# **3. First Look at the data:** \n- Looking the Type of Data\n- Null Numbers\n- Unique values\n- The first rows of our dataset","fd7a76dc":"<a id=\"Librarys\"><\/a> <br>\n# **2. Librarys:** \n- Importing Librarys\n- Importing Dataset","a34b8d80":"# Hyperparameter Visualisation","5d91fe4d":"# Tables of Content:\n\n**1. [Introduction](#Introduction)** <br>\n    - Info's about datasets\n**2. [Librarys](#Librarys)** <br>\n    - Importing Librarys\n    - Importing Dataset\n**3. [Knowning the data](#Known)** <br>\n    - 3.1 Looking the Type of Data\n    - 3.2 Shape of data\n    - 3.3 Null Numbers\n    - 3.4 Unique values\n    - 3.5 The first rows of our dataset\n**4. [Exploring some Variables](#Explorations)** <br>\n    - 4.1 Ploting some graphical and descriptive informations\n**5. [Correlation of data](#Correlation)** <br>\n\t- 5.1 Correlation Data\n**6. [Preprocess](#Preprocessing)** <br>\n\t- 6.1 Importing Librarys\n\t- 6.2 Setting X and Y\n    - 6.3 Spliting the X and Y in train and test \n**7. 1 [Model 1](#Modelling 1)** <br>\n    - 7.1.1 Random Forest \n    - 7.1.2 Score values\n    - 7.1.3 Cross Validation \n**7. 2 [Model 2](#Modelling 2)** <br>\n    - 7.2.1 Logistic Regression \n    - 7.2.2 Score values\n    - 7.2.3 Cross Validation \n    - 7.2.4 ROC Curve","cb4a52af":"Interesting distribuition","586aa711":"Now, we will verify the values through Checking Accounts","25d90c8f":"## Let's verify the ROC curve","e790db77":"Distruibution of Saving accounts by Risk","59285e84":"<h2>Creating an categorical variable to handle with the Age variable <\/h2>","2a643b7d":"<h1>Welcome to my Kernel ! <\/h1>\n\n<h2>I have used the inital code which does data exploration and feature engineering on the German Credit Risk to understand their distribuitions and patterns. Following this I have performed my own LightGBM training using Optuna for learning purpose<\/h2>\n","9967be9c":"I will try implement some interactive visuals in my Kernels, this will be the first, inspired in Alexader's Kernel and I will also continue implementing plotly and bokeh in my Kerne","d1d2a13a":"The old plot that I am trying to substitute with interactive plots"}}