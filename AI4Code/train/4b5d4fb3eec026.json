{"cell_type":{"dbe95d4c":"code","33310333":"code","4c63b6f7":"code","b4b15df1":"code","aef9caa8":"code","89bafe1f":"code","ef0bbe3d":"code","2c484235":"code","6db15f6b":"code","fc709547":"code","203c20af":"code","73b41336":"code","014da182":"markdown","8c801996":"markdown","727bb93b":"markdown","b0dacb6f":"markdown","d84b153d":"markdown","36ddc7a3":"markdown","5bd7a0e6":"markdown","34e33681":"markdown","fd3a015d":"markdown","6f4aaa20":"markdown","d972145a":"markdown","ddac91f8":"markdown","95e42adc":"markdown","4c9ab1a8":"markdown","0c91a8c3":"markdown","dbd2c7f1":"markdown","5d907a4a":"markdown"},"source":{"dbe95d4c":"\nimport os\nimport sys\nimport re\nimport collections\n\nimport csv\n\n# Generate regex patterns for filter words, for the moment we use a simple list of terms, this ought to be more sophisticated                                                                                                                                                                                     \nfilterwords=[\"2019-nCoV\",\"COVID-19\",\"novel coronavirus\",\"SARS-CoV-2\",\"Wuhan coronavirus\"]\nword_patterns=collections.OrderedDict()\nfor w in filterwords:\n    wrdlc=w.lower()\n    wrdptrn=re.compile(r\"\\b\"+re.escape(wrdlc))\n    word_patterns[wrdlc]=wrdptrn\n\n    \noutput=[]\nprocessed={}\nmetadata=csv.DictReader('kaggle\/input\/metadata.csv', dialect='excel')#delimiter='\\t',  quoting=csv.QUOTE_NONE) #.drop_duplicates()                                                                                                  \n\nof=open(out_file,\"w\", encoding='utf-8')\nfieldnames=metadata.fieldnames\nfieldnames.append('keywords_found')\nsys.stderr.write(\"fields: {}\\n\".format(fieldnames))\n\nwr=csv.DictWriter(of,fieldnames=fieldnames, dialect='excel')\nwr.writeheader()\n\nskipped=0\ndocs_found=0\nfile_problems=0\nfor row in metadata:\n    proces_count+=1\n        #if proces_count > 10:                                                                                                                                                                                     \n        #    sys.exit(100)                                                                                                                                                                                         \n        sys.stderr.write(\"\\r {a:8d} documents processed\".format(a=proces_count))\n        #sys.stderr.write(\"\\n document sha {} and pmcid {} --> \\n row {}\\n\".format(row[\"sha\"],row[\"pmcid\"],row))                                                                                                   \n        #we give preference to sha over pmc                                                                                                                                                                        \n        file_id=row[\"sha\"]\n        file_type=\"pdf_json\"\n        if row[\"sha\"] == None or row[\"sha\"] == '':\n            file_id=row[\"pmcid\"]\n            file_type=\"pmc_json\"\n            #sys.stderr.write(\"WARN: document {} has no sha {}\\n\".format(row[\"cord_uid\"],row[\"sha\"]))                                                                                                              \n\n            if row[\"pmcid\"] == None or row[\"pmcid\"] == '':\n                skipped+=1\n                sys.stderr.write(\"WARN: document {} has neither sha nor pmcid, skipping ({})\\n\".format(row[\"cord_uid\"],skipped))\n                continue\n\n        if file_id in processed:\n            sys.stderr.write(\"WARN: document with file_id {} (sha or pmcid) already processed, skipping\\n\".format(file_id))\n        else:\n            processed[file_id]=1\n            if row[\"sha\"] != None and row[\"pmcid\"] != None:\n                processed[row[\"pmcid\"]]=1\n\n        extension=\".json\"\n        if file_type == \"pmc_json\":\n            extension=\".xml.json\"\n            \n        # check if the document has more than one files asssociated, and if so parse all of them for reconstructing the full text.    \n        file_ids=file_id.split(\"; \")\n        full_text = \"\"\n        paragraphs=[]\n        sentences=[]\n        for i in file_ids:\n            in_file=os.path.join(inFolder,row[\"full_text_file\"],row[\"full_text_file\"],file_type,i+extension)\n            article={}\n            try: \n                with open(in_file,\"r\") as infile: \n                    article = json.load(infile)\n                    #sys.stderr.write(\"current document: {}\\n\".format(artikulu))\n            except:\n                file_problems+=1\n                sys.stderr.write(\"WARN: document with file_id {} (sha or pmcid) has problems with accessing the files, skipping\\n\".format(file_id))\n                continue\n        \n            if \"metadata\" in article and \"title\" in  article[\"metadata\"]:\n                if article[\"metadata\"][\"title\"] != \"\":\n                    full_text = full_text + \"\\n\" + article[\"metadata\"][\"title\"]\n                    \n            if \"abstract\" in article and len(article[\"abstract\"]) > 0:\n                for abstract_node in article[\"abstract\"]:\n                    if abstract_node[\"text\"] == None or abstract_node[\"text\"] == \"\":\n                        continue\n                \n                    full_text = full_text + \"\\n\" + abstract_node[\"text\"]\n            for paragraph in article[\"body_text\"]:\n                if paragraph[\"text\"] == None or paragraph[\"text\"] == \"\":\n                    continue\n                \n                full_text=full_text + \"\\n\" + paragraph[\"text\"]\n           \n        ## finally, once  full text is reconstructed, look for keywords\n        kwords_found= []\n        full_textlc = full_text.lower()\n        for w,ptrn in words.items():\n            if re.search(ptrn,full_textlc):\n                kwords_found.append(w)\n                #break # for the moment all keywords are look for, \n                             \n        # if keywords are found (at the moment as single keyword is required) write the tuple including the keywords in a new column.\n        if (len(kwords_found)>0):\n            docs_found+=1\n            row[\"keywords_elh\"]=\";\".join(kwords_found)\n            wr.writerow(row)\n            \n    of.close()","33310333":"from sklearn.manifold import TSNE\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nimport csv\nfrom nltk.tokenize import RegexpTokenizer\nimport joblib\nfrom nltk.corpus import stopwords\nimport string\n\nimport sys\n\ntokenizer = RegexpTokenizer(r'\\w+') \n\n\n#print(stopwords_english)\npassages=True\n\nin_file='metadata.csv_covid-19.kwrds.csv'\nif passages:\n    in_file='metadata.csv_covid-19.kwrds.paragraphs.csv'\n\nwith open(in_file) as tsvfile:    \n    reader = csv.DictReader(tsvfile, dialect='excel')\n    # output file\n    out_file = in_file+\".tfidf-coords.csv\"\n    of=open(out_file,\"w\", encoding='utf-8')\n    fieldnames=reader.fieldnames\n    fieldnames.append('tfidf_coord_x')\n    fieldnames.append('tfidf_coord_y')\n    sys.stderr.write(\"fields: {}\\n\".format(fieldnames))\n    wr=csv.DictWriter(of,fieldnames=fieldnames, dialect='excel')\n    wr.writeheader()\n    \n    texts=[]\n    ids={}\n    count=0\n    output=[]\n    for row in reader:\n        sys.stderr.write(\"\\r {} rows processed\".format(count))\n        dokid=\"\"\n        text=\"\"\n        if passages :\n            dokid=row['paragraph_id']\n            text=row['text']\n        else:\n            dokid=row['cord_uid']    \n            text=row['title']+\" \"+row['abstract']\n      \n        # tokenization\n        tokens = tokenizer.tokenize(text)\n        tokenak=[]\n        for tok in tokens:\n            #filter non alphanumeric tokens and stop words\n            if not (tok in string.punctuation) and not(tok.isnumeric()) and not(tok.lower() in stopwords.words('english')) and (len(tok) > 1) :\n                tokenak.append(tok)\n        tokenized_text=\" \".join(tokenak)\n        #add tokenized document\n        tokenized_text=tokenized_text.lower()\n        #texts list\n        texts.append(tokenized_text)\n        #Dokids list\n        ids[dokid]=count\n        count+=1\n        output.append(row)\n\n    #Generate tfidf model\n    vectorizer = TfidfVectorizer(min_df=2,max_df=0.6,norm=\"l2\",max_features=10000)\n    vecfit = vectorizer.fit(texts)\n    vec_trans = vecfit.transform(texts)\n\n    \n    #Dimension reduction\n    tsne_model = TSNE(n_components=2, verbose=1, random_state=0)\n    svd = TruncatedSVD(n_components=50, random_state=0)\n    svd_tfidf = svd.fit_transform(vec_trans)\n    tsne_tfidf = tsne_model.fit_transform(svd_tfidf)\n    #for i in vec_trans.toarray():\n    #    print(i)\n    for row in output:\n        dokid='cord_uid'\n        if passages:\n            dokid='paragraph_id'\n \n        row['tfidf_coord_x']=tsne_tfidf[ids[row[dokid]]][0]\n        row['tfidf_coord_y']=tsne_tfidf[ids[row[dokid]]][1]\n        \n        wr.writerow(row)\n\n    of.close()","4c63b6f7":"\n# Flair library is used to generate document embeddings\nfrom flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentPoolEmbeddings, Sentence, ELMoEmbeddings,BertEmbeddings,DocumentRNNEmbeddings\nimport numpy as np\n \n    #... Everything is the same as above up to the tokenization of the texts.\n    \n    ## Tokenization part is changed to create Flair Sentence objects\n    \n        #Remove stopwords?\n        if stopwords_flag:\n            tokens = tokenizer.tokenize(text)\n            tokenak=[]\n            for tok in tokens:\n                #filter non alphanumeric tokens and stop words\n                if not (tok in string.punctuation) and not(tok.isnumeric()) and not(tok.lower() in stopwords.words('english')) and (len(tok) > 1) :\n                    tokenak.append(tok)\n            tokenized_text=\" \".join(tokenak)\n            text=tokenized_text.lower()\n\n        sentence = Sentence(text,use_tokenizer=True)\n        #sentence.tokens = sentence.tokens[:100]        \n        if len(sentence)>0:\n            document_embeddings.embed(sentence)\n            new_row=sentence.get_embedding().detach().numpy()\n            #sys.stderr.write(\"row shape: {}\\n\".format(new_row.shape))\n        else:\n             new_row=np.zeros(300)\n        if len(A)==0:\n            A=new_row\n        else:\n            A = np.vstack((A, new_row))   \n\n       ## ...   \n            \n    #Dimension reduction\n    tsne_model = TSNE(n_components=2, verbose=1, random_state=0)\n    svd = TruncatedSVD(n_components=50, random_state=0)\n    svd_A = svd.fit_transform(A)\n    tsne_A = tsne_model.fit_transform(svd_A)\n\n    for row in output:\n        dokid='cord_uid'\n        if passages:\n            dokid='paragraph_id'\n \n        row['fasttext_coord_x']=tsne_A[ids[row[dokid]]][0]\n        row['fasttext_coord_y']=tsne_A[ids[row[dokid]]][1]\n        \n        wr.writerow(row)","b4b15df1":"import csv\n\n#cord_uid,sha,source_x,title,doi,pmcid,pubmed_id,license,abstract,publish_time,authors,journal,Microsoft Academic Paper ID,WHO #Covidence,has_pdf_parse,has_pmc_xml_parse,full_text_file,url,keywords_elh\nwith open('\/media\/nfs\/multilingual\/kaggle-covid19\/metadata.csv_covid-19.kwrds.csv') as tsvfile:    \n    reader = csv.reader(tsvfile, dialect='excel')\n    #Dont print header\n    next(reader)\n    for row in reader:\n        dokid=row[0]\n        title=row[3]\n        abstract=row[8]\n        print(\"<DOC>\")\n        print(\"<DOCID>\"+dokid+\"<\/DOCID>\")\n        print(\"<DOCNO>\"+dokid+\"<\/DOCNO>\")\n        print(\"<TEXT>\\n\"+title+\"\\n\"+abstract+\"\\n<\/TEXT>\")\n        print(\"<\/DOC>\")","aef9caa8":"import csv\n\n#cord_uid,paper_id,paragraph_id,paragraph_type,text\nwith open('\/media\/nfs\/multilingual\/kaggle-covid19\/metadata.csv_covid-19.kwrds.paragraphs.csv') as tsvfile:    \n    reader = csv.reader(tsvfile, dialect='excel')\n    #Dont print header\n    next(reader)\n    for row in reader:\n        par_id=row[2]\n        text_type=row[3]\n        par_text=row[4]\n        if text_type == \"body\":\n            print(\"<DOC>\")\n            print(\"<DOCID>\"+par_id+\"<\/DOCID>\")\n            print(\"<DOCNO>\"+par_id+\"<\/DOCNO>\")\n            print(\"<TEXT>\\n\"+par_text+\"\\n<\/TEXT>\")\n            print(\"<\/DOC>\")","89bafe1f":"buildindex\/IndriBuildIndex parameter.file\nbuildindex\/IndriBuildIndex parameterPar.file","ef0bbe3d":"import csv\nimport random\n\n\ntitles={}\nabsts={}\n\n\nwith open('\/data\/input\/metadata.csv') as tsvfile:    \n    reader = csv.reader(tsvfile, delimiter=',',quotechar='\"')\n    #Don't print header\n    next(reader)\n    for row in reader:\n        dokid=row[0]\n        title=row[3]\n        abstract=row[8]\n        if abstract.strip() != \"\" and title.strip() != \"\": \n            titles[dokid]=title\n            absts[dokid]=abstract\n\n\nfor dokid in titles:\n    # positive examples\n    print(titles[dokid]+\"\\t\"+dokid+\"\\t\"+absts[dokid]+\"\\t\"+dokid+\"\\t1\")\n    #negative examples (1:10 positve:negative ratio)\n    for i in range(10):\n        dokid2=random.choice(list(absts.keys()))\n        print(titles[dokid]+\"\\t\"+dokid+\"\\t\"+absts[dokid2]+\"\\t\"+dokid2+\"\\t0\")\n","2c484235":"class CovidKaggleProcessor(DataProcessor):\n  \"\"\"...\"\"\"\n\n  def get_labels(self):\n    \"\"\"See base class.\"\"\"\n    return [\"0\", \"1\"]\n\n  def _create_examples(self, lines, set_type):\n    \"\"\"Creates examples for the training and dev sets.\"\"\"\n    examples = []\n    for (i, line) in enumerate(lines):\n      guid = \"%s-%s\" % (set_type, i)\n      text_a = tokenization.convert_to_unicode(line[0])\n      text_b = tokenization.convert_to_unicode(line[2])\n      if set_type == \"test\":\n        label = tokenization.convert_to_unicode(line[4])\n    else:\n        label = tokenization.convert_to_unicode(line[4])\n      examples.append(\n          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n","6db15f6b":"BERT_BASE_DIR=\"clinicalBert\/pretrained_bert_tf\/biobert_pretrain_output_all_notes_150000\"\n\npython -u run_classifier.py  --task_name=covid \\\n       --do_train=true \\\n       --do_eval=true \\\n       --data_dir=$GLUE_DIR \\\n       --vocab_file=$BERT_BASE_DIR\/vocab.txt \\\n       --bert_config_file=$BERT_BASE_DIR\/bert_config.json \\\n       --init_checkpoint=$BERT_BASE_DIR\/model.ckpt-150000 \\\n       --max_seq_length=128 \\\n       --train_batch_size=16 \\\n       --learning_rate=2e-5 \\\n       --num_train_epochs=4.0 \\\n       --output_dir=$GLUE_DIR\/output-4e-1 \\\n       --do_lower_case=False \\","fc709547":"import pyndri\nfrom nltk.tokenize import RegexpTokenizer\n\nimport pandas as pd\nimport json\nimport sys\nimport os\nimport random\nimport argparse\n\nimport csv\n\nfrom math import exp\n\n## 1.- First of all various metadata files containing the information enrichment done in the previous steps are loaded.\n\nmetadata=\"metadata.csv_covid-19.kwrds.csv\"\npassage_metadata=\"metadata.csv_covid-19.kwrds.paragraphs.csv\"\n    \n# metadata for documents\nmetadata_doc=pd.read_csv(os.path.join(metadata_path,metadata))\n\n# metadata for passages.\nmetadata_pas=pd.read_csv(os.path.join(metadata_path,passage_metadata))\n\n# reranking scores are computed previously at this point. In a future version the system will work with open queries an compute this scores on the fly\nreranking_scores_df=pd.DataFrame(columns=[\"query_candidate_id\",\"label\",\"neg_score\",\"pos_score\"])\n# if exists, reranking-scores file\nif os.path.isfile(reranking_scores):\n    reranking_scores_df=pd.read_csv(reranking_scores,dialect='excel-tab')\n\n\n## 2.- next step is to load indri indexes\n\n# indri\nindex_doc_path=os.path.join(index_root,'Indri_doc_Index')\nindex_pas_path=os.path.join(index_root,'Indri_passage_Index')\n\nindex_doc = pyndri.Index(index_doc_path)\nindex_pas = pyndri.Index(index_pas_path)\n\n#query tokenizer\ntokenizer = RegexpTokenizer(r'\\w+')\n\n##  3.- Now comes the retrieval part. For the moment we use a close set of queries including all the questions proprosed in the various challenge tasks. Queries are stored in a tsv file.\nqueries_df = pd.read_csv(queries,dialect='excel-tab')\nfor index, row in queries_df.iterrows():\n\n    # lowercasing and tokenization\n    querylc = row['query'].lower()\n    tokens = tokenizer.tokenize(querylc)        \n    tokenized_query=\" \".join(tokens)\n\n    # document level results\n    results = index_doc.query(tokenized_query, results_requested=maxdocs)\n    docs = process_results(results,index_doc,metadata_doc, metadata_pas, reranking_scores_df, row[\"id\"])\n\n    # pasage level results\n    results = index_pas.query(tokenized_query, results_requested=maxdocs)\n    pas = process_results(results,index_pas,metadata_doc, metadata_pas, reranking_scores_df, row[\"id\"], passages=True)\n\n    query_json={\"query_id\":row['id'], \"task\": row['task'], \"query\":row['query'], \"docs\":docs,\"pas\":pas}\n    output.append(query_json)\n\n## final output is a json ready for visualization with bokeh as we will see in section 6\nprint(json.dumps(output, indent=4, sort_keys=True))\n\n\n\n","203c20af":"def process_results(indri_results,index,metadata_df, metadata_pas_df, reranking_scores_df, query_id, passages=False):\n    output=[]\n    count=0\n\n    # Minmax score normalization for indri_scores\n    min=1\n    max=0\n    for int_document_id, score in indri_results:\n        if exp(score) < min:\n            min=exp(score)\n        if exp(score) > max:\n            max=exp(score)\n\n    #loop throgout result and prepare output\n    for int_document_id, score in indri_results:\n        count+=1\n        ext_document_id, _ = index.document(int_document_id)\n\n        doc_id = ext_document_id\n        sys.stderr.write(\"\\r processed {} documents {} \".format(count, ext_document_id))\n        snippet=\"\"\n        if passages == True:\n            passage_metadata_row = metadata_pas_df[metadata_pas_df[\"paragraph_id\"]==int(ext_document_id)]\n            if passage_metadata_row.empty:\n                sys.stderr.write(\"\\r no passage metadata for document {} \\n \".format(ext_document_id))\n                continue\n\n            doc_id=passage_metadata_row.iloc[0][\"cord_uid\"]\n            snippet=passage_metadata_row.iloc[0][\"text\"]\n\n        # common fields for documents and passages\n        doc_metadata_row = metadata_df[metadata_df[\"cord_uid\"]==doc_id]\n        if doc_metadata_row.empty:\n            sys.stderr.write(\"\\r no document metadata for document {} \\n \".format(ext_document_id))\n            continue\n        url=doc_metadata_row.iloc[0][\"url\"]\n        title=doc_metadata_row.iloc[0][\"title\"]\n        author=doc_metadata_row.iloc[0][\"authors\"]\n        journal=doc_metadata_row.iloc[0][\"journal\"]\n        publish_date=doc_metadata_row.iloc[0][\"publish_time\"]\n        \n        # coordinates of the document\/passage. Here tfidf is mentioned, but embedding-based coordinate are also used\n        coords = {\"coord_x\":doc_metadata_row.iloc[0][\"tfidf_coord_x\"],\"coord_y\":doc_metadata_row.iloc[0][\"tfidf_coord_y\"],}\n        if passages == True:\n            coords = {\"coord_x\":passage_metadata_row.iloc[0][\"tfidf_coord_x\"],\"coord_y\":passage_metadata_row.iloc[0][\"tfidf_coord_y\"],}\n\n        #reranking starts now\n        q_candidate_id=\"q-\"+str(query_id)+\"-\"+str(doc_id)\n        if passages == True:\n            q_candidate_id = q_candidate_id+\"_\"+ext_document_id\n            \n        bert_score=None #bert score, if not found do not take it into account\n        reranking_score_row=reranking_scores_df[reranking_scores_df[\"query_candidate_id\"]==q_candidate_id]\n        if not reranking_score_row.empty:\n            bert_score=reranking_score_row.iloc[0][\"pos_score\"]\n            #sys.stderr.write(\"bert score for candidate {}: {} \\n\".format(q_candidate_id,bert_score))\n            \n        indri_score=(exp(score)-min)\/(max-min)  # normalized indri score\n\n        ## reranking is only applied to passages, not documents\n        if passages == True and bert_score != None:\n            ranking_score=0.8*indri_score+0.2*bert_score\n        else:\n            ranking_score=indri_score\n                \n        if passages == False:\n            snippet=doc_metadata_row.iloc[0][\"abstract\"]\n        \n        ## end of reranking\n\n        #generate uniq doc_ids for both pas and docs\n        if passages == True:\n            doc_id= doc_id+\"_\"+ext_document_id\n        \n        doc ={\"doc_id\":doc_id, \"title\":title, \"journal\":journal,\"author\":author,\"publish_date\":publish_date, \"url\":url,\"text\":snippet,\"ranking_score\":ranking_score, \"indri_score\":indri_score, \"coordinates\": coords}\n        output.append(doc)\n        #print(ext_document_id, score)\n\n    return output","73b41336":"import json\nimport os\nimport math\nfrom bokeh.palettes import RdBu3\nfrom bokeh.models import ColumnDataSource, LabelSet, DataTable, TableColumn, Div\nfrom bokeh.models.callbacks import CustomJS\nfrom bokeh.models.selections import Selection\nfrom bokeh.io import output_notebook, push_notebook, show\nfrom bokeh.plotting import figure, output_file, save, curdoc\nfrom bokeh.layouts import row, gridplot, layout\n\n\nBOX_WIDTH = 400\nBOX_HEIGHT = 340\nTEXT_MAX_LEN = 1000\n\n\ndef parse_json(fpath):\n    queries = []\n    with open(fpath, 'r') as f:\n        queries_j = json.load(f)\n        for query_j in queries_j:\n            query = {\n                'id': query_j['query_id'],\n                'title': query_j['query'],\n                'task': query_j['task'],\n                'docs': parse_entries(query_j['docs']),\n                'pas': parse_entries(query_j['pas'])\n            }\n            queries.append(query)\n    return queries\n    \n\ndef parse_entries(entries):\n    _ents = []\n    for entry in entries:\n        _ent = {\n            'doc_id': parse_doc_id(entry['doc_id']),\n            'score': entry['ranking_score'],\n            'title': entry['title'],\n            'coord_x': entry['coordinates']['coord_x'],\n            'coord_y': entry['coordinates']['coord_y'],\n            'text': entry['text'] if type(entry['text'])==str else \"\",\n            'authors': entry['author'] if type(entry['author'])==str else \"\",\n            'journal': entry['journal'] if type(entry['journal'])==str else \"\",\n            'url': entry['url'] if type(entry['url'])==str else \"\",\n            'date': entry['publish_date'],\n        }\n        _ents.append(_ent)\n        _ents.sort(key=lambda p: p['score'], reverse=True)\n    return _ents\n\n\ndef parse_doc_id(doc_id):\n    _ind = doc_id.find('_')\n    if _ind < 0:\n        return doc_id\n    else:\n        return doc_id[:_ind]\n\n\n\n    \ndef create_plots(query, path):\n    output_file('{}\/{}_{}.html'.format(path, query['task'], query['id']))\n    rankd = query['docs']\n    rankp = query['pas']   \n    \n    # document ranking data\n    dscores = [ p['score'] for p in rankd ]\n    source_doc = ColumnDataSource(dict(\n        doc = [ p['doc_id'] for p in rankd ],\n        x = [ p['coord_x'] for p in rankd ],\n        y = [ p['coord_y'] for p in rankd ],\n        scr = [ round(p['score'], 3) for p in rankd ],\n        rad = [ get_circle_size(p['score'], dscores) for p in rankd ],\n        color = [ RdBu3[0] for p in rankd ],\n        title = [ p['title'] for p in rankd ],\n        text = [ p['text'][:TEXT_MAX_LEN]+'...' if len(p['text'])>TEXT_MAX_LEN else p['text'] for p in rankd ],\n        authors = [ p['authors'] for p in rankd ],\n        journal = [ p['journal'] for p in rankd ],\n        url = [ p['url'] for p in rankd ],\n        date = [ p['date'] for p in rankd ],\n    ))\n\n    # paragraph ranking data\n    pscores = [ p['score'] for p in rankp ]\n    source_par = ColumnDataSource(dict(\n        doc = [ p['doc_id'] for p in rankp ],\n        x = [ p['coord_x'] for p in rankp ],\n        y = [ p['coord_y'] for p in rankp ],\n        scr = [ round(p['score'], 3) for p in rankp ],\n        rad = [ get_circle_size(p['score'], pscores) for p in rankp ],\n        color = [ RdBu3[2] for p in rankp ],\n        title = [ p['title'] for p in rankp ],\n        text = [ p['text'] for p in rankp ],\n        authors = [ p['authors'] for p in rankp ],\n        journal = [ p['journal'] for p in rankp ],\n        url = [ p['url'] for p in rankp ],\n        date = [ p['date'] for p in rankp ],\n    ))\n\n    # plots\n    doc_sp = create_scatter_plot(source_doc, True)\n    doc_tab = create_table(source_doc, True)\n    par_sp = create_scatter_plot(source_par, False)\n    par_tab = create_table(source_par, False)\n\n    # selection callbacks\n    source_doc.selected.js_on_change('indices', CustomJS(args=dict(sd=source_doc, sp=source_par), code=\"\"\"\n    if (sd.selected.indices.length == 0) {\n      return;\n    }\n    var sel = sd.selected.indices[0];\n    var new_selected = [];\n    for (var i = 0; i < sp.data['doc'].length; i++) {\n      if (sp.data['doc'][i] == sd.data['doc'][sel]) {\n        new_selected.push(i);\n      }\n    }\n    if (JSON.stringify(new_selected) != JSON.stringify(sp.selected.indices) &&\n    !(sp.selected.indices.length == 1 && new_selected.includes(sp.selected.indices[0]))) {\n      sp.selected.indices = new_selected;\n      sp.change.emit();\n    }\n    \"\"\"))\n    source_par.selected.js_on_change('indices', CustomJS(args=dict(sd=source_doc, sp=source_par), code=\"\"\"\n    if (sp.selected.indices.length == 0) {\n      return;\n    }\n    var sel = sp.selected.indices[0];\n    var new_selected = [];\n    for (var i = 0; i < sd.data['doc'].length; i++) {\n      if (sd.data['doc'][i] == sp.data['doc'][sel]) {\n        new_selected.push(i);\n      }\n    }\n    if (JSON.stringify(new_selected) != JSON.stringify(sd.selected.indices) &&\n    !(sd.selected.indices.length == 1 && new_selected.includes(sd.selected.indices[0]))) {\n      sd.selected.indices = new_selected;\n      sd.change.emit();\n    }\n    \"\"\"))\n\n    interface_layout=layout([[Div(text=\"<h2>{}<\/h2>\".format(query['title']), sizing_mode=\"stretch_width\")],\n                   [Div(text=\"<h3>Document ranking<\/h3>\")], [doc_tab, doc_sp],\n                   [Div(text=\"<h3>Paragraph ranking<\/h3>\")], [par_tab, par_sp]])\n    \n    # save layout\n    #save(interface_layout)\n    output_notebook()\n    show(interface_layout)#, notebook_handle=True)\n    \n    \ndef create_scatter_plot(source, isdoc=True):\n    tooltips = [\n        (\"Title\", \"@title\"),\n        (\"Abstract\" if isdoc else \"Passage\", \"@text\"),\n        (\"Published\", \"@date\"),\n        (\"Authors\", \"@authors\"),\n        (\"Journal\", \"@journal\"),\n        (\"URL\", \"@url\")\n    ]\n    p = figure(x_range=(0, 1), y_range=(0, 1), plot_width=BOX_WIDTH, plot_height=BOX_HEIGHT, tools='tap,reset', tooltips=tooltips, toolbar_location=\"below\")\n    p.circle(x='x', y='y', radius='rad', color='color', source=source)\n    \n    labels = LabelSet(x='x', y='y', text='title', level='glyph', text_font_size=\"10pt\",\n                      x_offset=-10, y_offset=5, source=source, render_mode='canvas')\n    #p.add_layout(labels)\n\n    return p\n\n\ndef create_table(source, isdoc=True):\n    columns = [\n        TableColumn(field=\"scr\", title=\"Score\", width=30),\n        TableColumn(field=(\"title\" if isdoc else \"text\"), title=(\"Document title\" if isdoc else \"Passage\"), width=770),\n    ]\n    table = DataTable(source=source, columns=columns, width=BOX_WIDTH, height=BOX_HEIGHT)\n\n    return table\n\n\ndef get_circle_size(score, all_scores):\n    min_a = 0.0\n    max_a = 1.0\n    min_b = 0.003\n    max_b = 0.02\n    return (((score - min_a) \/ (max_a - min_a)) * (max_b - min_b)) + min_b\n\n\n\nos.makedirs(\"visualization\/html\", exist_ok=True)\nqueries = parse_json(\"\/kaggle\/input\/covid19-challenge-all-task-queriesir-rankings\/all_queries_processed_fasttext_coords.json\")\n\n#example query visualization\ncreate_plots(queries[2], \"visualization\/html\")\n\n# Code for processing all the queries. For the purpose of this notebook a single query is processed in the line above.\n#for query in queries:\n#    create_plots(query, \"visualization\/html\")","014da182":"### Reranking by means of Fine-tuned BERT for sentence pair classification\n\nClinical BERT (Bio+ClinicalBERT) (Alsentzer et al., 2019) finetuned for sentence pair classification, over two datasets:\n- Titles as queries + abstracts as passages, from CORD-19 kaggle dataset. \n- MedQuAD question answering dataset (Ben Abacha & Demner-Fushman, 2019). Medline Plus collections were discarded.\n\nAll datasets in this section were randomly split 80\/20 for train\/test (no development set was used as we train for a fixed number of steps (4 epochs).\n\n\n","8c801996":"Training results for reranking finetuning over the COVID-19 title-abstract collection.\n\n\n------------ Result of Bert fine-tuned model ----------\n\n              precision    recall  f1-score   support\n\n           0     0.9962    0.9976    0.9969     77791\n           1     0.9761    0.9620    0.9690      7782\n\n    accuracy                         0.9944     85573\n    macro avg    0.9862    0.9798    0.9830     85573\n    weighted avg 0.9944    0.9944    0.9944     85573\n\n--------------------------------------------------------\n\nResults are very good. We repeated the experiment with the MedQuAD dataset, but results weren't nearly as good. Specifically the following experiments were further carried out:\n* MedQuAd Dataset.\n* MedQuAd Dataset prunned, leaving out question\/answer pairs whose answers are longer than 150 tokens.\n* Combination of MedQuAD and COVID-19 title-abstract dataset.\n\nMedQuAD dataset had a very low recall (0.1 - 0.20) for positive examples, leading to a very low performance. Prunning out long answer improved the results on MedQuAD by 10 points, but still far from the results of the COVID-19 title-abstract dataset. Thus we used the BERT model finetuned over that dataset for reranking.\n\nReranking is done by means of linear combination between the rank score returned by INDRI and the probability to be a correct answer returned by BERT.\n\n\n\n","727bb93b":"<a id=\"section-future\"><\/a>\n# 7 Ongoing and Future Work\n\nFuture work will focus on two main fronts, research on the one hand, and functionality on the other.\n\nAs for research, the following issues will be addressed:\n\n* Study of contextual embeddings adapted to the medical field for the visual representation of documents and passages.\n* Study of new approaches to generate silverdata for tuning the BERT model of passage reranking.\n\nRegarding the functionality of the system, we are already working on the following improvements:\n* Improvement of the user interface:\n* * Improve the formatting of the ranking results: better layout, pagination, and inclusion of metadata (authors, date...).\n* * Open query interface.\n* Update collection dinamically as new dataset versions are published\n","b0dacb6f":"<a id=\"section-retrieval\"><\/a>\n# 5 Document and passage Retrieval \n\nAs explained in step 3, we use the Indri model to retrieve the documents and passages relevant to the queries submitted by the user. To implement this step we used Pyndri (Van Gysel et al., 2017), an Indri interface for Python.\n\nIn the case of document queries, we retrieve the ranking of the 50 most relevant documents and normalize their scores using the Minmax strategy.","d84b153d":"<a id=\"section-vect-repr\"><\/a>\n# 2 Vectorial representation of abstracts and passages for visualization\n\nThe system offers a visual representation, in addition to the textual one, of the document and passage rankings relevant to the queries submitted by the user. The visualization of documents and passages will be done in a two-dimensional plane according to their vectorial representations. We have studied bag-of-words representations as well as embedding based ones.\n\nFor the construction of vectorial representations of bags of words we tokenize the text and remove stopwords, tokens from one character and punctuation marks. We also filter out tokens that appear in less than two documents and in more than 60% of the documents in the collection. We assign tf-idf weights to the dimensions represented by the tokens, and reduce the dimensions from 10,000 to 2 in two steps: First we apply SVD to make a reduction to 50 dimensions, and then TSNE to reduce to 2 dimensions.","36ddc7a3":"<a id=\"section-title-intro\"><\/a>\n# Wide perspective query system focused on COVID-19\n\n\nSystem that offers integral consultation of scientific papers on COVID-19 through search at document and passage level and with auxiliary visualization of the results.\n\nFull code is available at https:\/\/github.com\/Elhuyar\/covid-19-IR.\n\nAlso, a minimum interactive working demo is available at http:\/\/covid19kaggle.elhuyar.eus , providing exploration and result navigation for all the queries in the challenge. \n\n**The system has the following features:**\n* Simultaneous and complementary retrieval of documents (coarse grain) and passages (fine grain) relevant to queries.\n* Visual representation of relevant documents and paragraphs according to their semantic content.\n\n**Techniques:**\n* Retrieval of documents through language models (Indri).\n* Retrieval of passages by combining language models (Indri) and re-ranking based on BERT fine-tuned for sentence pair classification task.\n* Visualization by means of embeddings and reduction of dimensions.\n\n**Contributions:**\n* Results and visualization according to different techniques that offer an enriched and wide perspective consultation.\n* Fine-tuning of BERT by trainset built from (titles,abstracts) pairs extracted from the COVID-19 Open Research dataset.\n\n\nWe describe below in different steps how we have implemented the system. The steps to be explained are the following:\n\n1. Preprocess of the collection.\n2. Vectorial representation of abstracts and passages for visualization.\n3. Indexing of the collection for document and passage retrieval.\n4. BERT model fine-tuning for classification of related sentences of COVID-19 domain.\n5. Document and passage retrieval.\n6. Visualization of results.\n","5bd7a0e6":"<a id=\"section-index\"><\/a>\n# 3 Indexing of the collection for document and paragraph retrieval\n\nIn order to obtain the relevant documents and passages of the collection corresponding to the queries, we use a language modeling based information retrieval approach (Ponte & Croft, 1998). For that purpose, we used the Indri search engine (Strohman, 2005), which combines Bayesian networks with language models.\n\nWe indexed two collections: one of documents and one of paragraphs. For each document we index its title and abstract, and for each paragraph its corresponding paragraph.\nWe convert the initial collections to TREC format.","34e33681":"The function \"process_results\" we see above takes care of gathering metadata from the different resources loaded above and generate a json object for each candidate in the ranking. We can see the function below.\n\nIn the case of passages, after obtaining the ranking of the 50 most relevant passages and normalizing their scores, we reorder the ranking taking into account the probabilities returned by the BERT model tuned to the classification task of similar sentence pairs (see step 4). It is a similar strategy to the one proposed by Nogueira and Cho (2019).\n\nIn our case, we linearly combine the normalized Indri score and the probability of the tuned BERT model according to the following formula: final_score(candidate_i) = k*i_score + (1-k)*bert_score\n\nFor this demo we have set an initial value k=0.8 which will be tuned in the next phase.","fd3a015d":"Finetuning is done over Bio+ClinicalBERT for 4 epochs, with a bastch size of 16 in order to fit into our GPU (GeForce 2080 RTX Ti). Following the exact command used:","6f4aaa20":"<a id=\"section-vis\"><\/a>\n# 6 Visualization \/ User Interface\n\nThe system offers the relevant information to the query by means of two rankings. The first ranking corresponds to the relevant documents, and the second to the relevant passages. The rankings are offered in two formats, one textual and one graphic.\n\nThe document ranking allows the user to search for articles relevant to the query. The passage ranking, on the other hand, allows for finer searches and makes it possible to find the precise relevant information required.\n\nThe system presents both rankings at the same time because we believe they are complementary, and one can help to understand the results of the other. For the same reason they are interconnected, and if you select a result from one ranking, its related result is highlighted in the other.\n\nOn the other hand, the graphic visualization of the rankings helps to understand the degree of convergence or semantic relation, at content level, between the different results.","d972145a":"### Interaction\n\nBoth rankings and visualizations are connected. When a document is selected, all the passages that belong to that document and appear in the passage ranking are higlighted. We can also hover over the graph circles and explore the details of each document. A minimum interface is available at github to explore all the queries and their corresponding results.\n\nAs mentioned in the introduction, a minimum interactive working demo is available at http:\/\/covid19kaggle.elhuyar.eus, providing exploration and result navigation for all the queries in the challenge.\n\n![irudia.png](attachment:irudia.png)\n","ddac91f8":"The first step is preparing the dataset for finetuning BERT models. The title-abstract collection is straightforward, we just need to extract title and abstract pairs in BERT readable format. ","95e42adc":"<a id=\"section-preprocess\"><\/a>\n# 1 Preprocess of collections\n\nThe first step is to find the documents talking about COVID-19 in the dataset, but not about other Corona viruses. For the first version of the system we approach this task by using a keyword list. \nThe list was taken from the [Kaggle COVID-19 Medical Dictionary](https:\/\/docs.google.com\/spreadsheets\/d\/1t2e3CHGxHJBiFgHeW0dfwtvCG4x0CDCzcTFX7yz9Z2E\/edit?pli=1#gid=389064679).\n\nBelow you can find the code for filtering the dataset. It reads metadata.csv file and reconstructs full text papers, looking for keywords in the whole papers.","4c9ab1a8":"Then, we create indexing configuration files parameter.file and parameterPar.file to create the two indexes using the IndriBuildIndex command. The indexes are created in the BildumaIndex and BildumaParIndex folders.","0c91a8c3":"The construction of dense vector representations by means of embeddings is similar, but in this case we use the average vector of FastText embeddings of all the words in the text. In future versions we will include representations according to contextual embeddings based on neural language models.\n\nThe code is very similar to that of tdidf based coordinate generation, following we show the changes over the aforementioned code.\n\n\n","dbd2c7f1":"<a id=\"section-reranking\"><\/a>\n# 4 BERT model fine-tuning for classification of related sentences of COVID-19 domain\n\nBERT models have been successfully used to reorder rankings obtained by classic IR algorithms based on word bags. As they are heavy models they are only applied to the top of the rankings.\n\nIn our case we propose to make a re-ranking based on BERT on the paragraph ranking following a strategy similar to the one proposed by Nogueira and Cho (2019).\nAs we do not have a collection of query pairs and relevant paragraphs, we simulate a training collection composed of titles and their corresponding abstracts from the COVID-19 Open Research dataset. Through this training collection we tuned the BERT model to the task of identifying relevant queries and paragraphs.","5d907a4a":"Now for the finetuning part we use original BERT distribution [run_classifier.py](https:\/\/github.com\/google-research\/bert\/blob\/master\/run_classifier.py) finetuning script, with a custom data processor very similar to the one used for MRPC dataset (only minimal changes done to the _create_examples function to adapt it to our needs.). "}}