{"cell_type":{"e1c6b59c":"code","b60fd457":"code","828f568a":"code","c6b1acd6":"code","c4b2bafc":"code","97661749":"code","90e76cf2":"code","612a0a90":"code","47287937":"code","2fd73756":"code","9894c336":"code","1a91c5ed":"code","0b363d83":"code","c60f4afd":"code","171374fd":"code","b14e2b33":"code","db77cfcb":"code","c1d91cf5":"code","f4b505e4":"code","52ed1e7b":"code","8465187b":"code","468bc3b8":"code","13cd0320":"code","bd8f82e7":"code","63876bed":"code","e19a5bf2":"code","af6e4dbd":"code","66fe512c":"code","c5c65791":"markdown","55632427":"markdown","f870e6dc":"markdown","8e3319cd":"markdown","73f6cd09":"markdown","bf285f66":"markdown","df330406":"markdown","3b9082ea":"markdown","d87a4ac5":"markdown","bf2ddce2":"markdown","ccfdbfaa":"markdown","4fe44b23":"markdown","0444533d":"markdown","2cca32bc":"markdown","9916b356":"markdown","99792e38":"markdown","3dbd18c3":"markdown","1cc765bf":"markdown","818c4fa1":"markdown","33c7f36c":"markdown","b8e89188":"markdown","363eaa95":"markdown","844749c1":"markdown","b327fc27":"markdown","9e166cd8":"markdown","054cc2b2":"markdown","b241ebc6":"markdown"},"source":{"e1c6b59c":"#importing packages\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","b60fd457":"#import train and test datasets and put in pandas dataframe\ndf_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","828f568a":"#display the first five rows of train dataset\ndf_train.head(5)","c6b1acd6":"#display all the columns\ndf_train.columns\n\n#for col in df_train.columns: \n#    print(col) ","c4b2bafc":"#check the summary\ndf_train['SalePrice'].describe()","97661749":"#histogram\nsns.distplot(df_train['SalePrice']);","90e76cf2":"#scatter plot grlivarea\/saleprice\nvar = 'GrLivArea'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","612a0a90":"#box plot overallqual\/saleprice\nvar = 'OverallQual'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","47287937":"var = 'YearBuilt'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis =1)\nfig, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nplt.xticks(rotation=90)","2fd73756":"#correlation matrix\ncorrmat = df_train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=0.8, square=True);","9894c336":"#saleprice correlation matrix\nk = 10 \ncols= corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","1a91c5ed":"total = df_train.isnull().sum().sort_values(ascending = False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending = False)\nmissing_data = pd.concat([total, percent], axis = 1, keys = ['Total', 'Percent'])\nmissing_data.head(19)","0b363d83":"df_train = df_train.drop((missing_data[missing_data['Total'] > 1]).index,1)\ndf_train = df_train.drop(df_train.loc[df_train['Electrical'].isnull()].index)\ndf_train.isnull().sum().max()","c60f4afd":"#scatter plot grlivarea\/saleprice\nvar = 'GrLivArea'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","171374fd":"#deleting points\ndf_train.sort_values(by = 'GrLivArea', ascending = False)[:2]\ndf_train = df_train.drop(df_train[df_train['Id'] == 1299].index)\ndf_train = df_train.drop(df_train[df_train['Id'] == 524].index)","b14e2b33":"#scatter plot grlivarea\/saleprice\nvar = 'GrLivArea'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","db77cfcb":"#histogram and normal probability plot\nsns.distplot(df_train['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)","c1d91cf5":"#applying log transformation\ndf_train['SalePrice'] = np.log(df_train['SalePrice'])","f4b505e4":"#transformed histogram and normal probability plot\nsns.distplot(df_train['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)","52ed1e7b":"#data transformation\ndf_train['GrLivArea'] = np.log(df_train['GrLivArea'])\n\n#transformed histogram and normal probability plot\nsns.distplot(df_train['GrLivArea'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['GrLivArea'], plot=plt)","8465187b":"#create column for new variable (one is enough because it's a binary categorical feature)\n#if area>0 it gets 1, for area==0 it gets 0\ndf_train['HasBsmt'] = pd.Series(len(df_train['TotalBsmtSF']), index=df_train.index)\ndf_train['HasBsmt'] = 0 \ndf_train.loc[df_train['TotalBsmtSF']>0,'HasBsmt'] = 1","468bc3b8":"#transform data\ndf_train.loc[df_train['HasBsmt']==1,'TotalBsmtSF'] = np.log(df_train['TotalBsmtSF'])\n\n#histogram and normal probability plot\nsns.distplot(df_train[df_train['TotalBsmtSF']>0]['TotalBsmtSF'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train[df_train['TotalBsmtSF']>0]['TotalBsmtSF'], plot=plt)","13cd0320":"y = np.log(df_train.SalePrice)\nX = data.drop(['SalePrice'], axis=1)","bd8f82e7":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n                          X, y, random_state=42, test_size=.33)","63876bed":"#linear regression model\nfrom sklearn import linear_model\nlr = linear_model.LinearRegression()","e19a5bf2":"model = lr.fit(X_train, y_train)","af6e4dbd":"print (\"R^2 is: \\n\", model.score(X_test, y_test))","66fe512c":"predictions = model.predict(X_test)\nplt.scatter(predictions, y_test, alpha=.7,\n            color='b') \nplt.xlabel('Predicted Price')\nplt.ylabel('Actual Price')\nplt.title('Linear Regression Model')\nplt.show()","c5c65791":"Part of the code is taking note of the outliers because it can affect the model's performance. Let's take a look at 'SalePrice'. To implement this, we use a **Bivariate Analysis** through the scatter plot.","55632427":"Same thing happens with 'GrLivArea.","f870e6dc":"Now it's deleted. Yay!\n\n# Test Assumptions\n\nThis part is mainly for checking if the data meets the 'assumptions' of multivariate techniques. Let's get to know more about 'SalePrice'.","8e3319cd":"The data makes sense. Better quality, higher price!","73f6cd09":"As observed, there are two light boxes. The first one refers to 'TotalBsmtSF' and '1stFlrSF' variables. The other one refers to 'GarageX' variables. These boxes are an indication of the relationship's significance. If you think about it, these boxes state the obvious. Bigger garage areas, more garage cars can fit in... A big basement area gives almost the same measurement aboveground... Therefore, we can conclude *multicollinearity* from these variables. This detection of multicollinearity is one of the many advantages of using a heatmap. ","bf285f66":"# Multivariate study\n\nThis method focuses on understanding the relationship betweeen the dependent and independent variables. \n\nThe first two methods were a bit subjective and bias. As scientists, we want to generate data-driven conclusions. Let's proceed with making a more objective analysis. The best way to show relations between sets of variables is a heatmap style of the correlation matrix. This allows you to see relationships with the highest correlation.","df330406":"Let's talk about the normality of the data. We expect (and hope) a normal distribution, theoretically. Transforming the data into a normal distribution is essential as several statistic tests rely on it. ","3b9082ea":"There are 19 columns that contain missing data. I will follow Marcelino's rule of deleting the corresponding variable when more than 15% of the data is missing. Therefore, I will delete six columns. Judging from the input of these columns, the variables seem to be not important. These aspects are strong candidates for being an outlier and definitely not the factors we tend to think about when buying a house.\n\nAnalyzing the 'GarageX', 'MasVnrX', and 'BsmtX' variables, the same percentages might imply the same set of observations. Deleting these variables will not matter. For 'Electrical', I can simply delete that one entry with missing data but keep the variable. \n\nIn a nutshell, I will delete everything except for 'Electrical'.","d87a4ac5":"# Conclusion\n\nThere are various strategies when doing data exploration. This is my first time to try EDA. I personally followed Marcelino's comprehensive data exploration. I liked how his writing was concise and structured perfectly. The strategies mentioned were based on [Hair et al. (2013).](https:\/\/www.amazon.com\/Multivariate-Data-Analysis-Joseph-Hair\/dp\/9332536503\/ref=as_sl_pc_tf_til?tag=pmarcelino-20&linkCode=w00&linkId=5e9109fa2213fef911dae80731a07a17&creativeASIN=9332536503) if you wanna know more about it.\n\nThat's it! The next step is to use the data to train a model for house prediction.","bf2ddce2":"Great! There is an actual minimum. This data point won't ruin the training process. Now let's try to check the histogram to get an overview. As clich\u00e9 as it may sound, a picture tells a thousand words. *;)*  ","ccfdbfaa":"# Time To Build the Model\n\nThe next step is to build a linear model. We try to separate the target variables (y) and features (X). We are applying np.log to the y variable to transform it for the model. ","4fe44b23":"Hello! This notebook is my first attempt at writing a Kaggle notebook. As I begin my Kaggle journey, I would like to start with the basics. I've read a few notebooks to direct me on the right path and I've come across Perdro Marcelino's [Comprehensive data exploration with Python](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python). His work motivated me to learn more about Exploratory Data Analysis (EDA). I've worked before with medical images and cancer-gene datasets. I understand the weight of analyzing datasets before feeding them to your model.\n\nAs a good starting point, I will attempt to extract information from the dataset using Marcelino's 5 steps and his written code! Big thanks to him!\n\n","0444533d":"We evaluate the performance and try to visualize it using a scatter plot as well. ","2cca32bc":"Let's try to understand its other components (columns that affect its existence).","9916b356":"Now let's focus to 'SalePrice' correlations! The best way to do this is to zoom the heatmap. ","99792e38":"There is two large 'GrLivArea' that do not follow the crowd. We can simply delete them. ","3dbd18c3":"I can't really conclude something here since the property price index fluctuates from time to time. But it's not surprising to see the behavior of 'SalePrice' increase. \n\nThere are *many variables* to compare and analyze. The chosen variables here are just tome good indicators of 'SalePrice'. Based on the three variables, I observed direct proportionality.","1cc765bf":"I see that there is a **linear relationship** between 'SalePrice' and 'GrLivingArea' which means that 'SalePrice' is dependent on the living area. \n\nNow let's proceed to check the overall quality. Since it's categorical from 1 to 10, I use a box plot to see the relationship clearly. ","818c4fa1":"Time for modelling. Let's start with Linear Regression. ","33c7f36c":"Obviously, 'SalePrice' did not show a normal distribution and did not follow the diagonal line for the probability plot. The way to solve the problem is to tweak the data. Let's try to apply a log transformation.\n","b8e89188":"Now it's time to fit the model using X_train and y_train. We evaluate the performance with X_test and y_test. ","363eaa95":"# Understand the problem \n\nIt may sound *tedious* but part of the process is to look at each variable and do some analysis about their contribution and role for this problem. For some cancer-gene-drug datasets, this step is very crucial. You must be careful not to interchange the columns to avoid incorrect diagnosis to a cancer patient. \n\nI created a [Google sheet](https:\/\/docs.google.com\/spreadsheets\/d\/1rFONid3SfsqlPlhWX6nfGzKpQRgnw6S0roZXVfJw4U8\/edit?usp=sharing) to establish some discipline in my analysis. The columns are: \n\n* **Variable** - variable names\n* **Type** - either numerical or categorical\n* **Segment** - either building, space, or location\n* **Expectation** - its influence in 'SalePrice'. either high, medium, or low\n* **Conclusion** - over-all importance of the variable.\n* **Comments** - general comments\n\n'Expectation' is mainly for developing 'sixth sense' when dealing with data. To answer each column, Marcelino presented three questions to ask ourselves: \n\n* Do we think about this variable when we are buying a house?\n* If so, how important would this variable be?\n* Is this information already described in any other variable?\n\nTake note that answering this question might depend on your market. If for example, your target market is the Philippines, a fireplace will be categorized as low priority. It's a different case when you're dealing with colder places. After that *painstaking* process, I now evaluate the 'Conclusion' which is basically the correction of our 'Expectations'. \n\nAfter filling all the columns, I conclude that there are certain variables that play a critical role:\n\n* Lot Area\n* Overall Quality\n* Overall Condition\n* Year Built\n\nThese variables make sense. If it were me, these elements matter the most. But of course, there is bias. Surprisingly, these variables go against the real estate conformity which location is the best price indicator. \n\nThe main point of the exercise is to understand our data and set our expectations. Now, it's time to proceed with more action! \n\n# Univariable study\n\nThis step focuses on 'SalePrice', the main goal of this quest. This method is my attempt to get to know it better. \n","844749c1":"# Basic Cleaning\n\nIt's time to clean the data. This part involves handling the missing data, outliers, and categorical variables.\n\nWhen handling the incompleteness,  we wanna know if the missing data is random or it does have a pattern. This step is also essential because of its capability to reduce the sample size. Reducing the sample size can help in preserving memory as well. But more importantly, we want to ensure that the incomplete data is not biased and hiding essential information.","b327fc27":"For 'TotalBsmtSF', I will use a different approach as suggested by Marcelino's *high-risk engineering*. The problem here lies with the value zero that does not allow log transformations. It's important to create a variable that can get the effect of a binary variable (with:1 or with no basement:0). We will only do a log transformation to the non-zero observations.","9e166cd8":"nawppppp. far from 1, damn. ","054cc2b2":"The next steps will be partitioning of the data to create a training set and a hold-out set. This method allows the model to perform on data it has never seen before. ","b241ebc6":"Here I have a few points:\n\n* there is strong relationship between 'GarageCars' and 'GarageArea', 'TotRmsAbvGrd' and 'GrLivArea'\n* there is strong relationship between '1stFlrSF' and 'TotalBsmtSF', i mean... what can we expect?\n* 'SalePrice' has the strongest relationship to the 'OverallQual', next is the 'GrLiveArea'. We can conclude these two variables are great indicators. \n* 'FullBath', of course... \n\n"}}