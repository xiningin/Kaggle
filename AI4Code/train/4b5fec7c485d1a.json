{"cell_type":{"bec14cc9":"code","25ecc894":"code","00f5acfa":"code","015d3b71":"code","d64f661f":"code","c6147cd4":"code","ad87ec74":"code","1e2e6bbe":"code","a0bed386":"code","c3db46db":"code","d305baeb":"code","69202b1b":"code","f79c3850":"code","d59d3571":"code","7e9b9f7f":"code","2a915beb":"code","77901a73":"code","a22f94a2":"code","def8948d":"code","0e6ae252":"code","800988bd":"code","70af3136":"code","abf4c38e":"code","85862c94":"code","d8eb98b6":"code","30cfb9a4":"code","9879497a":"code","92bab37c":"code","65d8f299":"code","8c282e87":"code","b52ef094":"code","c4a756f7":"code","c05da833":"code","263d800c":"code","6388dcdd":"code","69be0707":"code","dbc053f2":"code","b82edbee":"code","7082695e":"code","1a8421d9":"markdown","5c56f97f":"markdown","aee61494":"markdown","b6857b0c":"markdown","a2d4ab25":"markdown","5e1d867e":"markdown","323b082f":"markdown","b4ce3888":"markdown","5b5a37aa":"markdown","5d31fa23":"markdown","0c1c4df7":"markdown","74c298cb":"markdown","49645c9a":"markdown"},"source":{"bec14cc9":"!pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https:\/\/download.pytorch.org\/whl\/torch_stable.html -q\n!pip install fastai==2.0.13 -q","25ecc894":"from fastai.text.all import *\nimport pandas as pd\nimport numpy as np\nfrom tqdm.autonotebook import tqdm\nfrom torch import nn\nfrom sklearn.model_selection import KFold","00f5acfa":"path = '\/kaggle\/input\/stanford-covid-vaccine'\ntrain = pd.read_json(f'{path}\/train.json',lines=True)\ntest = pd.read_json(f'{path}\/test.json', lines=True)\nsub = pd.read_csv(f'{path}\/sample_submission.csv')","015d3b71":"train.shape, train['id'].nunique(), test.shape, sub.shape","d64f661f":"# BPPS features from: https:\/\/www.kaggle.com\/its7171\/gru-lstm-with-feature-engineering-and-augmentation\n\ndef read_bpps_sum(df):\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps_arr.append(np.load(f\"..\/input\/stanford-covid-vaccine\/bpps\/{mol_id}.npy\").sum(axis=1))\n    return bpps_arr\n\ndef read_bpps_max(df):\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps_arr.append(np.load(f\"..\/input\/stanford-covid-vaccine\/bpps\/{mol_id}.npy\").max(axis=1))\n    return bpps_arr\n\ndef read_bpps_nb(df):\n    # normalized non-zero number\n    # from https:\/\/www.kaggle.com\/symyksr\/openvaccine-deepergcn \n    bpps_nb_mean = 0.077522 # mean of bpps_nb across all training data\n    bpps_nb_std = 0.08914   # std of bpps_nb across all training data\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps = np.load(f\"..\/input\/stanford-covid-vaccine\/bpps\/{mol_id}.npy\")\n        bpps_nb = (bpps > 0).sum(axis=0) \/ bpps.shape[0]\n        bpps_nb = (bpps_nb - bpps_nb_mean) \/ bpps_nb_std\n        bpps_arr.append(bpps_nb)\n    return bpps_arr \n\ntrain['bpps_sum'] = read_bpps_sum(train)\ntest['bpps_sum'] = read_bpps_sum(test)\ntrain['bpps_max'] = read_bpps_max(train)\ntest['bpps_max'] = read_bpps_max(test)\ntrain['bpps_nb'] = read_bpps_nb(train)\ntest['bpps_nb'] = read_bpps_nb(test)","c6147cd4":"train = train.sample(frac=1, random_state=42)","ad87ec74":"all1 = []\nall2 = []\nall3 = []\nfor i in range(len(train)):\n    all1.extend(train['sequence'].loc[i])\n    all2.extend(train['structure'].loc[i])\n    all3.extend(train['predicted_loop_type'].loc[i])","1e2e6bbe":"all1 = L(all1)\nall2 = L(all2)\nall3 = L(all3)","a0bed386":"vocab1 = all1.unique()\nvocab2 = all2.unique()\nvocab3 = all3.unique()","c3db46db":"word2idx1 = {w:i for i,w in enumerate(vocab1)}\nword2idx2 = {w:i for i,w in enumerate(vocab2)}\nword2idx3 = {w:i for i,w in enumerate(vocab3)}","d305baeb":"def joiner(row):\n    l1 =  list(row[0])\n    l2 =  list(row[1])\n    l3 =  list(row[2])\n    l4 =  list(row[3])\n    l5 =  list(row[4])\n    l6 =  list(row[5])\n    out = [[word2idx1[l1[i]], word2idx2[l2[i]], word2idx3[l3[i]], l4[i], l5[i], l6[i]] for i in range(len(l1))]\n    return out","69202b1b":"train['seqs'] = train[['sequence', 'structure', 'predicted_loop_type', 'bpps_sum', 'bpps_max', 'bpps_nb']].apply(joiner, axis=1)","f79c3850":"train = train[train['SN_filter'] == 1]","d59d3571":"txts = L([x for x in train['seqs'].values])\ntgts1 = L([x for x in train['reactivity'].values])\ntgts2 = L([x for x in train['deg_Mg_pH10'].values])\ntgts3 = L([x for x in train['deg_pH10'].values])\ntgts4 = L([x for x in train['deg_Mg_50C'].values])\ntgts5 = L([x for x in train['deg_50C'].values])","7e9b9f7f":"seqs = L((tensor(txts[i]), tensor([tgts1[i], tgts2[i], tgts3[i], tgts4[i], tgts5[i]])) for i in range(len(txts)))","2a915beb":"test['seqs'] = test[['sequence', 'structure', 'predicted_loop_type', 'bpps_sum', 'bpps_max', 'bpps_nb']].apply(joiner, axis=1)\ntest107 = test[test['seq_length'] == 107].reset_index(drop=True)\ntest130 = test[test['seq_length'] == 130].reset_index(drop=True)\nlen(test107), len(test130)","77901a73":"test107_ids = pd.DataFrame()\ntest107_ids['id'] = test107['id']\nfor i in range(11): # fill up the batch for prediction\n    test107_ids.loc[len(test107_ids)] = 'id_dummy'    \ntest107_ids['seqnum'] = ''    \ntest107_ids['seqnum'] = test107_ids['seqnum'].astype(object)\nsn = np.array(list(range(107)))\nfor i in range(len(test107_ids)):\n    test107_ids['seqnum'].loc[i] = sn\ntest107_ids = test107_ids.explode('seqnum').reset_index(drop=True)\ntest107_ids['id_seqpos'] = test107_ids.apply(lambda r: str(r[0]) + '_' + str(r[1]), axis=1)","a22f94a2":"test107_seqs = [(tensor(x), torch.zeros(5,68)) for x in test107['seqs'].values]\nlen(test107_seqs)\n#11 empty seqs to fill up the batch :\/\ntest107_seqs_empty = [(torch.zeros((107,6), dtype=torch.long), torch.zeros(5,68)) for i in range(11)]\ntest107_seqs += test107_seqs_empty\ntest107_seqs = L(test107_seqs)\nlen(test107_seqs), len(test107_seqs) % 32","def8948d":"test107_seqs = [(a.to('cuda'), b.to('cuda')) for (a,b) in test107_seqs]","0e6ae252":"test130_ids = pd.DataFrame()\ntest130_ids['id'] = test130['id']\nfor i in range(3): # fill up the batch for prediction\n    test130_ids.loc[len(test130_ids)] = 'id_dummy'    \ntest130_ids['seqnum'] = ''    \ntest130_ids['seqnum'] = test130_ids['seqnum'].astype(object)\nsn = np.array(list(range(130)))\nfor i in range(len(test130_ids)):\n    test130_ids['seqnum'].loc[i] = sn\ntest130_ids = test130_ids.explode('seqnum').reset_index(drop=True)\ntest130_ids['id_seqpos'] = test130_ids.apply(lambda r: str(r[0]) + '_' + str(r[1]), axis=1)","800988bd":"test130_seqs = [(tensor(x), torch.zeros(5,68)) for x in test130['seqs'].values]\nlen(test130_seqs)\n#3 empty seqs to fill up the batch :\/\ntest130_seqs_empty = [(torch.zeros((130,6), dtype=torch.long), torch.zeros(5,68)) for i in range(3)]\ntest130_seqs += test130_seqs_empty\ntest130_seqs = L(test130_seqs)\nlen(test130_seqs), len(test130_seqs) % 32","70af3136":"test130_seqs = [(a.to('cuda'), b.to('cuda')) for (a,b) in test130_seqs]","abf4c38e":"BS = 32 # batch size \nES = 32 # embedding size\nNH = 512 # number hidden units\nNL = 3 # number layers\nDO = 0.3 # dropout\nEP = 20 # epochs\nLR = 0.009281670019785143 # learning rate\nWD = 0.0 # weight decay","85862c94":"sl = 107\n\nclass OVModel(Module):\n    def __init__(self, vocab1_sz, vocab2_sz, vocab3_sz, emb_sz, n_hidden, n_layers, p, y_range=None):\n        self.y_range = y_range\n        self.i_h1 = nn.Embedding(vocab1_sz, emb_sz)\n        self.i_h2 = nn.Embedding(vocab2_sz, emb_sz)\n        self.i_h3 = nn.Embedding(vocab3_sz, emb_sz)\n        self.rnn = nn.LSTM(emb_sz*3+3, n_hidden, n_layers, batch_first=True, bidirectional=True)\n        self.drop = nn.Dropout(p)\n        self.h_o = nn.Linear(n_hidden*2, 5)\n        self.h = [torch.zeros(n_layers*2, BS, n_hidden).to('cuda') for _ in range(2)]\n        \n    def forward(self, x):\n        e1 = self.i_h1(x[:,:,0].long())\n        e2 = self.i_h2(x[:,:,1].long())\n        e3 = self.i_h3(x[:,:,2].long())\n        bp = x[:,:,3:]\n        e = torch.cat((e1, e2, e3, bp), dim=2)\n        raw,h = self.rnn(e, self.h)\n        do = self.drop(raw)\n        out = self.h_o(do)\n        if self.y_range is None: \n            self.h = [h_.detach() for h_ in h]\n            return out, raw, do        \n        out = torch.sigmoid(out) * (self.y_range[1]-self.y_range[0]) + self.y_range[0]\n        self.h = [h_.detach() for h_ in h]\n        return out, raw, do\n    \n    def reset(self): \n        for h in self.h: h.zero_()","d8eb98b6":"def loss_func(inp, targ):\n    inp = inp[0]\n    inp = inp[:,:68,:]\n    l1 = F.mse_loss(inp[:,:,0], targ[:,0,:])\n    l2 = F.mse_loss(inp[:,:,1], targ[:,1,:])\n    l3 = F.mse_loss(inp[:,:,2], targ[:,2,:])\n    l4 = F.mse_loss(inp[:,:,3], targ[:,3,:])\n    l5 = F.mse_loss(inp[:,:,4], targ[:,4,:])\n    return torch.sqrt((l1 + l2 + l3 + l4 +l5)\/5)","30cfb9a4":"test_dl107 = DataLoader(dataset=test107_seqs, bs=BS, shuffle=False, drop_last=True)\ntest_dl130 = DataLoader(dataset=test130_seqs, bs=BS, shuffle=False, drop_last=True)","9879497a":"spltidx = np.array(range(len(seqs)))\nkf = KFold(n_splits=5)\nsplts = list(kf.split(spltidx))","92bab37c":"all_preds107 = []\nall_preds130 = []\n\nfor i in range(5):\n    dls = DataLoaders.from_dsets(seqs[splts[i][0]], seqs[splts[i][1]], bs=BS, drop_last=True, shuffle=True).cuda()\n    net = OVModel(len(vocab1), len(vocab2), len(vocab3), ES, NH, NL, DO, y_range=None)\n    learn = Learner(dls, net, loss_func=loss_func, cbs=ModelResetter)\n    learn.fit_one_cycle(EP, LR, wd=WD)\n    preds107 = learn.get_preds(dl=test_dl107, reorder=False)\n    all_preds107.append(preds107[0][0])\n    preds130 = learn.get_preds(dl=test_dl130, reorder=False)\n    all_preds130.append(preds130[0][0])","65d8f299":"predictions107 = sum(all_preds107) \/ len(all_preds107)\npredictions107.shape\n\npredictions130 = sum(all_preds130) \/ len(all_preds130)\npredictions130.shape","8c282e87":"s107 = pd.DataFrame()\ns107['id_seqpos'] = test107_ids['id_seqpos']\ns107['reactivity'] = predictions107[:,:,0].flatten().numpy().tolist()\ns107['deg_Mg_pH10'] = predictions107[:,:,1].flatten().numpy().tolist()\ns107['deg_pH10'] = predictions107[:,:,2].flatten().numpy().tolist()\ns107['deg_Mg_50C'] = predictions107[:,:,3].flatten().numpy().tolist()\ns107['deg_50C'] = predictions107[:,:,4].flatten().numpy().tolist()\ns107 = s107.iloc[:-11*107]","b52ef094":"s130 = pd.DataFrame()\ns130['id_seqpos'] = test130_ids['id_seqpos']\ns130['reactivity'] = predictions130[:,:,0].flatten().numpy().tolist()\ns130['deg_Mg_pH10'] = predictions130[:,:,1].flatten().numpy().tolist()\ns130['deg_pH10'] = predictions130[:,:,2].flatten().numpy().tolist()\ns130['deg_Mg_50C'] = predictions130[:,:,3].flatten().numpy().tolist()\ns130['deg_50C'] = predictions130[:,:,4].flatten().numpy().tolist()\ns130 = s130.iloc[:-3*130]","c4a756f7":"s = pd.concat([s107, s130], axis=0)","c05da833":"s.to_csv('submission.csv', index=False)","263d800c":"s.head()","6388dcdd":"import matplotlib.pyplot as plt\n%matplotlib inline","69be0707":"viz = pd.DataFrame()\nviz['reactivity'] = predictions130[:,:,0].numpy().tolist()\nviz['deg_Mg_pH10'] = predictions130[:,:,1].numpy().tolist()\nviz['deg_Mg_50C'] = predictions130[:,:,3].numpy().tolist()","dbc053f2":"fig, axes = plt.subplots(nrows=4, ncols=5, figsize=(21,9), sharex=True, sharey=True)\nfig.suptitle('Reactivity', fontsize=24, color='blue')\nfor i, ax in enumerate(axes.flatten()):\n    ax.plot(viz['reactivity'].loc[i])\n    ax.axvline(x=68, color='red')\n    ax.axvline(x=91, color='blue')\n    ax.axvline(x=107, color='green')\nplt.show()","b82edbee":"fig, axes = plt.subplots(nrows=4, ncols=5, figsize=(21,9), sharex=True, sharey=True)\nfig.suptitle('deg_Mg_pH10', fontsize=24, color='blue')\nfor i, ax in enumerate(axes.flatten()):\n    ax.plot(viz['deg_Mg_pH10'].loc[i])\n    ax.axvline(x=68, color='red')\n    ax.axvline(x=91, color='blue')\n    ax.axvline(x=107, color='green')\nplt.show()","7082695e":"fig, axes = plt.subplots(nrows=4, ncols=5, figsize=(21,9), sharex=True, sharey=True)\nfig.suptitle('deg_Mg_50C', fontsize=24, color='blue')\nfor i, ax in enumerate(axes.flatten()):\n    ax.plot(viz['deg_Mg_50C'].loc[i])\n    ax.axvline(x=68, color='red')\n    ax.axvline(x=91, color='blue')\n    ax.axvline(x=107, color='green')\nplt.show()","1a8421d9":"# Model and loss function","5c56f97f":"## 107-length","aee61494":"# Preparing the data for RNN","b6857b0c":"# Submission","a2d4ab25":"# Data Loaders, Training, Inference","5e1d867e":"# Imports, installs, reading the data","323b082f":"## 130-length","b4ce3888":"# Open Vaccine Fastai RNN","5b5a37aa":"# Test Data Preparation","5d31fa23":"I will use this notebook to experiment with various RNN approaches to Open Vaccine competition using fastai library. To read more about RNN with fastai, read this: https:\/\/github.com\/fastai\/fastbook\/blob\/master\/12_nlp_dive.ipynb","0c1c4df7":"# Visualize Predictions","74c298cb":"# Config","49645c9a":"## All updates:\n- FIX: predict for 130-long sequences in test\n- Visualize predictions\n- loss function (from xhlulu)\n- FIX: kernel now running on GPU! (thanks to fast.ai forums, especially Satyabrata Pal and Zach Mueller!)\n- some hyperparameter tuning...\n- improved inference time with pandas explode\n- add bpps feature (from tito)\n- k-fold validation and ensemble\n- hyperparam tuning\n    - epochs count\n    - batch size\n    - learning rates\n    - embedding \/ hidden sizes \/ n-layers\n    - dropout"}}