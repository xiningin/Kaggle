{"cell_type":{"f579ebfe":"code","dcadad99":"code","a7543b37":"code","7beb9335":"code","dddec743":"code","0433dd8f":"code","8a120658":"code","13c4a65e":"code","ffc207e9":"code","0f15e767":"code","fa85e769":"code","61aa7723":"code","feaa45c7":"code","07fe3ee8":"code","fdf31396":"code","b253d8cb":"code","1cc458ab":"markdown","0ed6a2cd":"markdown","3b7416d2":"markdown","53f388d7":"markdown","811fd04e":"markdown","9baf3e67":"markdown","65595fee":"markdown","379c2ea3":"markdown","39220e6c":"markdown","f53a7485":"markdown","8f06c444":"markdown","0e2b7749":"markdown","d36f5218":"markdown","2e7f36eb":"markdown","432f23ab":"markdown","946d3d7e":"markdown","f0ac3f71":"markdown"},"source":{"f579ebfe":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport re\nimport string\nfrom string import digits\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer","dcadad99":"print(os.listdir(\"..\/input\"))\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nprint(\"\\nTrain data: \\n\",train.head())\nprint(\"\\nTest data: \\n\",test.head())","a7543b37":"train_data=train.drop(train.columns[0], axis=1) \ntest_data=test\nprint(train_data.head())\nprint(test_data.head())","7beb9335":"train_comments=train_data.iloc[:,0]\ntest_comments=test_data.iloc[:,1]\n\n#saving index to separate them later\ntrain_comments_index=train_comments.index\ntest_comments_index=test_comments.index\n\nframes = [train_comments, test_comments]\ncomments = pd.concat(frames, ignore_index=True)\n\n\nlabels=train_data.iloc[:,1:]\n\nprint(\"Train Comments Shape: \",train_comments.shape)\nprint(\"Test Comments Shape: \",test_comments.shape)\nprint(\"Comments Shape after Merge: \",comments.shape)\nprint(\"Comments are: \\n\",comments.head())\nprint(\"\\nLabels are: \\n\", labels.head())","dddec743":"c=comments.str.translate(str.maketrans(' ', ' ', string.punctuation))\nc.head()","0433dd8f":"c=c.str.translate(str.maketrans(' ', ' ', '\\n'))\nc=c.str.translate(str.maketrans(' ', ' ', digits))\nc.head()","8a120658":"c=c.apply(lambda tweet: re.sub(r'([a-z])([A-Z])',r'\\1 \\2',tweet))\nc.head()","13c4a65e":"c=c.str.lower()\nc.head()","ffc207e9":"c=c.str.split()\nc.head()","0f15e767":"stop = set(stopwords.words('english'))\nc=c.apply(lambda x: [item for item in x if item not in stop])\nc.head()    ","fa85e769":"from tqdm import tqdm\nlemmatizer = WordNetLemmatizer()\ncom=[]\nfor y in tqdm(c):\n    new=[]\n    for x in y:\n        z=lemmatizer.lemmatize(x)\n        z=lemmatizer.lemmatize(z,'v')\n        new.append(z)\n    y=new\n    com.append(y)","61aa7723":"clean_data=pd.DataFrame(np.array(com), index=comments.index,columns={'comment_text'})\nclean_data['comment_text']=clean_data['comment_text'].str.join(\" \")\nprint(clean_data.head())","feaa45c7":"train_clean_data=clean_data.loc[train_comments_index]\ntest_clean_data=clean_data.drop(train_comments_index,axis=0).reset_index(drop=True)","07fe3ee8":"print(\"PreProcessed Train Data : \",train_clean_data.head(5))\nprint(\"PreProcessed Test Data : \",test_clean_data.head(5))","fdf31396":"frames=[train_clean_data,labels]\ntrain_result = pd.concat(frames,axis=1)\nframes=[test.iloc[:,0],test_clean_data]\ntest_result = pd.concat(frames,axis=1)\nprint(train_result.head())\nprint(test_result.head())","b253d8cb":"train_result.to_csv('train_data.csv', index = False)\ntest_result.to_csv('test_data.csv', index = False)","1cc458ab":"Saving data in csv format to use it in different notebook, or you can continue working in the same notebook.","0ed6a2cd":"<a id=\"4\"> Step 4 - Convert to lowercase <\/a>\n","3b7416d2":"Data obtained after Lemmatization is in array form, and is converted to Dataframe in the next step.","53f388d7":"<a id=\"2\">Step 2 - Remove Digits <\/a>\n\nRemoving \\n and digits","811fd04e":"<br>\n###   <a id=\"jump\">Basic Text Preprocessing Steps - Cleaning <\/a>\n<br>\nNow that we have comments, its time to process them to convert them into a form that can be fed to classifier.\n\nTo do so following basic steps are performed and to get a better idea of what these steps do, an example is added as well. \n\n**\u201cYou are annoying!!! goJumpOff4Cliff pleaseeeeeeee\u201d**\n* Step 1 - [Remove punctuation](#1) \u2192** You are annoying goJumpOff4Cliff pleaseeeeeeee**\n* Step 2 - [Remove digits](#2)\u2192 ** You are annoying goJumpOffCliff please**\n* Step 3 - [Split combined words](#3) \u2192 **You are annoying go Jump Off Cliff please**\n* Step 4 - [Convert to lowercase](#4) \u2192   ** your are annoying go jump off cliff please**\n* Step 5 - [Split each sentence using delimiter](#5) \u2192   ** your, are, annoying, go, jump, off, cliff, please**\n* Step 6 - [Remove stop words](#6) \u2192       **annoying, jump, cliff **\n* Step 7 - [Convert Word to Base Form](#7) \u2192                      **annoy, jump, cliff** \n\nPlease note that order of steps matter here, if step number 4 is performed before Step 3, we wont be able to split the Combined words like **goJumpOffCliff**.","9baf3e67":"<a id=\"7\"> Step 7 - Convert Word to Base Form or Lematize <\/a> \n\nConverting each word to its base form e.g. trying to try, or tried to try for simplification; using **WordNetLemmatizer** function from **NLTK** library.","65595fee":"Starting by importing required libraries.","379c2ea3":"<a id=\"3\"> Step 3 - Split combined words <\/a>\n\nFor instance, converting **whyAreYou** to **why Are You **","39220e6c":"<a id=\"5\"> Step 5 - Split each sentence using delimiter <\/a>\n\nConverting each sentence to list of words. We are doing it to keep necessary words in the upcoming steps and descarding the rest.","f53a7485":"Reading training and test data from CSV file and saving as Pandas' Dataframe","8f06c444":"Merging comments and labels for training data set and ids for test data set.","0e2b7749":"<a id=\"1\">Step 1 - Remove Punctuation<\/a>","d36f5218":"Data here comprises of ids, comments, and labels. \n\nRemoving IDs from Train data, keeping Test data IDs for submission. ","2e7f36eb":"<a id=\"6\"> Step 6 - Remove Stop Words <\/a>\n\nStop words are the most common words in a language and mostly filtered in NLP problems.","432f23ab":"## [Workbook 1](https:\/\/www.kaggle.com\/sabasiddiqi\/workbook-1-text-pre-processing-for-beginners) - Text Preprocessing for Beginners - Data Cleaning\n<br>\n**Level** : Beginner\n\nThis notebook discusses **Text Data Preprocessing** for **NLP Problems** using Toxic Comment Classification Dataset. Data comprises of large number of Wikipedia comments which have been labeled by human raters for toxic behavior\n\nData is available via following link.\n[Toxic Comment Classification](https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge\/data)\n\nNext Workbook : [Workbook 2 - Text Preprocessing for Beginners - Feature Extraction](https:\/\/www.kaggle.com\/sabasiddiqi\/workbook-2-text-preprocessing-feature-extraction) \n\nTo skip the initial steps (reading data, text extraction from data), Jump to [Text Pre-Processing Steps](#jump).","946d3d7e":"Now extracting comments from train and test data, and storing their index for later use.\nMerging comments for both train and test, so that Preprocessing Steps can be performed on both at same time.","f0ac3f71":"Separating Train and Test Comments using the index stored earlier."}}