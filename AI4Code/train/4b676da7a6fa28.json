{"cell_type":{"77460b39":"code","dfe9fd18":"code","e9d23f75":"code","4e970a0e":"code","3b0551ab":"code","bfae0bfa":"code","33875334":"code","cf9c80b6":"code","47e9af78":"code","6dca2bdd":"code","3b940504":"code","1369520d":"code","70ee2a14":"code","1ab806e1":"code","08d7ae8c":"code","cb841c44":"code","5dc5cada":"code","887ccb13":"code","70eac159":"code","bdca721c":"code","ff4f1905":"code","1d27a15f":"code","481a58cf":"code","c55f8822":"code","7916c726":"code","8f33ba70":"code","ca78f5e0":"code","d2c01d71":"code","fa1ce3a6":"code","3e6de97e":"code","916be36c":"code","41de4c33":"code","4be17ffa":"code","079d2ece":"code","79bf3fc7":"code","6c7f29a8":"code","652ae455":"code","4f5ba710":"code","a48ec718":"code","19e201da":"code","4f0c0de0":"code","6413cd5c":"code","21621be3":"code","de70d150":"code","b4e238a0":"code","6db76bed":"code","072279e6":"code","f3d45722":"code","3080b892":"code","e4442e07":"code","e275db18":"code","cd61e4e0":"code","d97c71c5":"code","4f55f4ea":"code","2a35e126":"code","f221cc5b":"code","4d0139a6":"markdown","7cf27dfd":"markdown","7c26a9b1":"markdown","ec3783cc":"markdown","c5252147":"markdown","3b592363":"markdown","bf1424ed":"markdown","9f269c03":"markdown","6b87a9b9":"markdown","c39208d1":"markdown","649e6934":"markdown","37a54c53":"markdown","ab5826d0":"markdown","53f5bd69":"markdown","ca01cc90":"markdown","ff0df472":"markdown","7476040b":"markdown","dd61b104":"markdown","1ae6deea":"markdown","095651a8":"markdown"},"source":{"77460b39":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dfe9fd18":"df_train = pd.read_csv(\"\/kaggle\/input\/av-jobathon-sep2021\/train.csv\")\n","e9d23f75":"df_train.shape","4e970a0e":"df_train.describe()","3b0551ab":"df_train.drop([\"ID\"],axis=1,inplace=True)","bfae0bfa":"df_train.head()","33875334":"encoded_data = pd.get_dummies(df_train,columns=[\"Store_Type\",\"Location_Type\",\"Region_Code\",\"Discount\"])\nencoded_data.head()","cf9c80b6":"df_test = pd.read_csv(\"\/kaggle\/input\/av-jobathon-sep2021\/test.csv\")\ndf_test.head()","47e9af78":"encoded_test_data = pd.get_dummies(df_test,columns=[\"Store_Type\",\"Location_Type\",\"Region_Code\",\"Discount\"])\nencoded_test_data.head(10)","6dca2bdd":"#training_set\nencoded_data['Date'] = pd.to_datetime(encoded_data['Date'])\nencoded_data['Day']=encoded_data['Date'].dt.day_name()\nencoded_data","3b940504":"#test_set\nencoded_test_data['Date'] = pd.to_datetime(encoded_test_data['Date'])\nencoded_test_data['Day']=encoded_test_data['Date'].dt.day_name()\nencoded_test_data.head(10)","1369520d":"#training_data encoding\nencoded_data = pd.get_dummies(encoded_data,columns=[\"Day\"]) \nencoded_data.head(10)","70ee2a14":"#test_data encoding \nencoded_test_data = pd.get_dummies(encoded_test_data,columns=[\"Day\"])\nencoded_test_data.head(10)","1ab806e1":"#training_set\nencoded_data[\"month\"] = pd.DatetimeIndex(encoded_data[\"Date\"]).month\nencoded_data","08d7ae8c":"#test_set\nencoded_test_data[\"month\"] = pd.DatetimeIndex(encoded_test_data[\"Date\"]).month\nencoded_data","cb841c44":"#training_data\nencoded_data = pd.get_dummies(encoded_data,columns=[\"month\"]) \nencoded_data.head()","5dc5cada":"#test_data\nencoded_test_data = pd.get_dummies(encoded_test_data,columns=[\"month\"]) \nencoded_test_data","887ccb13":"encoded_data.columns","70eac159":"encoded_test_data.columns","bdca721c":"encoded_test_data.drop([\"ID\"],axis=1,inplace=True)","ff4f1905":"encoded_data.drop([\"#Order\"],axis=1,inplace=True)","1d27a15f":"encoded_data.columns","481a58cf":"encoded_test_data.columns","c55f8822":"encoded_data.head()","7916c726":"encoded_test_data.head()","8f33ba70":"encoded_data.drop([\"Date\"],axis=1,inplace=True)\nencoded_test_data.drop([\"Date\"],axis=1,inplace=True)","ca78f5e0":"encoded_data.head()","d2c01d71":"encoded_test_data.head()","fa1ce3a6":"from sklearn.model_selection import train_test_split","3e6de97e":"X = encoded_data.drop([\"Sales\"],axis=1)\nX.head(5)","916be36c":"y = encoded_data[\"Sales\"]\ny.head(10)","41de4c33":"#splitting the dataset for training the model\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25)","4be17ffa":"# from sklearn.linear_model import LinearRegression","079d2ece":"# LR = LinearRegression()","79bf3fc7":"# LR.fit(X_train,y_train)","6c7f29a8":"# y_prediction =  LR.predict(X_test)\n# y_prediction","652ae455":"# from sklearn.metrics import mean_squared_log_error as msle\n# lr_msle = msle(y_test,y_prediction)\n# print(\"mean_squared_log_error is :- \",lr_msle)\n","4f5ba710":"import xgboost as xg","a48ec718":"xgb_r = xg.XGBRegressor(objective ='reg:linear',\n                  n_estimators = 50, seed = 123)","19e201da":"xgb_r.fit(X_train, y_train)\n","4f0c0de0":"pred = xgb_r.predict(X_test)","6413cd5c":"pred","21621be3":"from sklearn.metrics import mean_squared_error as MSE\nxgb_r_msle = MSE(y_test,pred)\nprint(\"mean_squared_log_error :\", xgb_r_msle)","de70d150":"encoded_test_data","b4e238a0":"encoded_test_data[\"month_1\"] = encoded_test_data[\"month_6\"]\nencoded_test_data[\"month_2\"] = encoded_test_data[\"month_6\"]\nencoded_test_data[\"month_3\"] = encoded_test_data[\"month_6\"]\nencoded_test_data[\"month_4\"] = encoded_test_data[\"month_6\"]\nencoded_test_data[\"month_5\"] = encoded_test_data[\"month_6\"]\nencoded_test_data[\"month_6_new\"] = encoded_test_data[\"month_6\"]\nencoded_test_data[\"month_7_new\"] = encoded_test_data[\"month_7\"]\nencoded_test_data[\"month_8\"] = encoded_test_data[\"month_6\"]\nencoded_test_data[\"month_9\"] = encoded_test_data[\"month_6\"]\nencoded_test_data[\"month_10\"] = encoded_test_data[\"month_6\"]\nencoded_test_data[\"month_11\"] = encoded_test_data[\"month_6\"]\nencoded_test_data[\"month_12\"] = encoded_test_data[\"month_6\"]\n\n\nencoded_test_data.head(5)","6db76bed":"encoded_test_data.drop([\"month_6\",\"month_7\"],axis=1,inplace=True)","072279e6":"encoded_test_data.rename(columns={\"month_6_new\":\"month_6\",\"month_7_new\":\"month_7\"},inplace=True)","f3d45722":"cols=[\"month_1\",\"month_2\",\"month_3\",\"month_4\",\"month_5\",\"month_8\",\"month_9\",\"month_10\",\"month_11\",\"month_12\"]\ncols","3080b892":"for i in cols:\n    encoded_test_data[cols]=0","e4442e07":"encoded_test_data[cols]","e275db18":"encoded_test_data","cd61e4e0":"result = xgb_r.predict(encoded_test_data)\nresult","d97c71c5":"# result2 =  LR.predict(encoded_test_data)\n# result2","4f55f4ea":"final_sub = pd.concat([df_test['ID'], pd.DataFrame(result)], axis=1)\n# rename columns\nfinal_sub.columns = ['ID', 'Sales']\nfinal_sub","2a35e126":"# final_sub2 = pd.concat([df_test['ID'], pd.DataFrame(result2)], axis=1)\n# # rename columns\n# final_sub2.columns = ['ID', 'Sales']\n# final_sub2","f221cc5b":"final_sub.to_csv(\"Submission for n 25.csv\",index=False)\n# final_sub2.to_csv(\"SubmissionLR.csv\",index=False)","4d0139a6":"Here X represent our feature vector and y is our target values i.e. \"Sales\" so except \"Sales\" everything will be in X feature vector.","7cf27dfd":"### Linear Regression","7c26a9b1":"After droping ID column our data frame is as follows","ec3783cc":"## Feature Engineering\n### Extracting the Days of the week using date","c5252147":"### XGBOOSTER","3b592363":"Now Date column is not required as we have done categorial encoding on them, so we will drop the columns in both the dataframe","bf1424ed":"## Pre-processing ","9f269c03":"### Viewing the final datasets","6b87a9b9":"## Exploratory Data Analysis","c39208d1":"## Preparing the data for training the model ","649e6934":"### There is and extra column in test data i.e. encoded_test_data -->Id and in encode_data --> #order \nwe can drop them from their respective dataframe","37a54c53":"### Checking wheather all columns are same in training and test data","ab5826d0":"There are few columns missing in the encoded_test_data, month_1 to month_12 excluding month_6 and month_7 is missing","53f5bd69":"### Encoding the Day Column","ca01cc90":"## Reading the datasets","ff0df472":" From describe we can say that :-\n*  so there is **no missing** values in any cell in data\n* **\"ID\"** is unique so it is not required for our analysis","7476040b":"After removing **ID** we have 9 columns remaining out of which\n\n* There are 5 categorical columns viz  \"Store_Type\",\"Location_Type\",\"Region_Code\",\"Discount\" , \"Holiday\"\n* and 1 numerical column viz \"Date\"  in the data.\nFor each of the categorical columns we need to convert it into numerical data, so we perform one hot encoding for the categorical features.","dd61b104":"### Performing encoding on months","1ae6deea":"#### Importing the test data and performing the transformation on it","095651a8":"### Extracting the month from date column"}}