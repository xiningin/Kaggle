{"cell_type":{"e52847e9":"code","19ed5dae":"code","e795e546":"code","eebce4ec":"code","5d35d64c":"code","6bf53995":"code","77744768":"code","1e484c7b":"code","7b7897cd":"code","e333f6b7":"code","be24eb57":"code","2fa31a57":"code","ad71cf68":"code","c88057b3":"code","f904daf9":"code","5e0932bd":"code","cc67bddd":"code","72ed635d":"code","344879a6":"code","fe55c15e":"code","616e7b4c":"code","1400bf4f":"code","42e8f0dd":"code","e0b61be5":"code","03d2b5a5":"code","1dfe8fa2":"code","ae5d2391":"code","2164496d":"code","3b328b4b":"code","fb088c6a":"code","afd08e6d":"code","7db2cb29":"code","65249edc":"code","697b9db5":"code","278a88d0":"code","273ec416":"markdown","83361553":"markdown","5c3962a3":"markdown","e3fd0c2b":"markdown","8739db46":"markdown","61ae930b":"markdown","079f6f69":"markdown"},"source":{"e52847e9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\")) #check the files available in the directory\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","19ed5dae":"train_data = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntrain_data.head()","e795e546":"test_data = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntest_data.head()","eebce4ec":"train_data.info()\nprint('_'*40)\ntest_data.info()","5d35d64c":"train_data.describe()","6bf53995":"fig, ax = plt.subplots()\nax.scatter(x = train_data['GrLivArea'], y = train_data['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","77744768":"#Deleting outliers\ntrain_data = train_data.drop(train_data[(train_data['GrLivArea']>4000) & (train_data['SalePrice']<300000)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(train_data['GrLivArea'], train_data['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","1e484c7b":"sns.distplot(train_data['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train_data['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train_data['SalePrice'], plot=plt)\nplt.show()","7b7897cd":"\ntrain_data[\"SalePrice\"] = np.log1p(train_data[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train_data['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train_data['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train_data['SalePrice'], plot=plt)\nplt.show()","e333f6b7":"#Save the 'Id' column\ntrain_ID = train_data['Id']\ntest_ID = test_data['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain_data.drop(\"Id\", axis = 1, inplace = True)\ntest_data.drop(\"Id\", axis = 1, inplace = True)\ntarget = train_data.SalePrice","be24eb57":"ntrain = train_data.shape[0]\nntest = test_data.shape[0]\ny_train = target.values\nall_data = pd.concat((train_data, test_data)).reset_index(drop=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","2fa31a57":"all_data.tail()","ad71cf68":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","c88057b3":"all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\nall_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\n\ntrain_data = train_data.drop(['PoolQC','MiscFeature', 'Alley', 'Fence'] ,axis=1)\n\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\nall_data[\"GarageFinish\"] = all_data[\"GarageFinish\"].fillna(\"None\")\n\n#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)\n\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')\n    \nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\n\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')\n    \nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n\nall_data = all_data.drop(['Utilities'], axis=1)\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)","f904daf9":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)\n\nfrom sklearn.preprocessing import LabelEncoder\n\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))","5e0932bd":"all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","cc67bddd":"#Correlation map to see how features are correlated with SalePrice\ncorrmat = train_data.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","72ed635d":"corrmat['SalePrice'].sort_values(ascending=False)","344879a6":"train_data.plot(kind='scatter', x='OverallQual', y='SalePrice')","fe55c15e":"train_data.plot(kind='scatter', y='SalePrice', x='GrLivArea')","616e7b4c":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=True)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","1400bf4f":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam)","42e8f0dd":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","e0b61be5":"all_data.drop('SalePrice', axis=1, inplace=True)","03d2b5a5":"train_data = all_data[:ntrain]\ntest_data = all_data[ntrain:]","1dfe8fa2":"from sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nfrom lightgbm import LGBMRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nimport optuna\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ae5d2391":"from sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\n\n# ------------------------------------------------------------------------------\n# Parameters\n# ------------------------------------------------------------------------------\nN_FOLDS = 10\nN_ESTIMATORS = 100000\nSEED = 2021\nBAGGING_SEED = 48\n\nN_TRIALS = 50","2164496d":"\"\"\"\n\ncv = KFold(n_splits=4, random_state=42)\n\ndef objective(trial):\n    _C = trial.suggest_float(\"C\", 0.1, 0.5)\n    _epsilon = trial.suggest_float(\"epsilon\", 0.01, 0.1)\n    _coef = trial.suggest_float(\"coef0\", 0.5, 1)\n\n    svr = SVR(cache_size=5000, kernel=\"poly\", C=_C, epsilon=_epsilon, coef0=_coef)\n\n    score = cross_val_score(\n     svr, train_data, target, cv=cv, scoring=\"neg_root_mean_squared_error\"\n    ).mean()\n    return score\n\n\noptuna.logging.set_verbosity(0)\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=50)\n\nsvr_params = study.best_params\nsvr_best_score = study.best_value\nprint(f\"Best score:{svr_best_score} \\nOptimized parameters: {svr_params}\")\n\"\"\"","3b328b4b":"\"\"\"\n\n\ndef objective(trial):\n    _n_estimators = trial.suggest_int(\"n_estimators\", 50, 200)\n    _max_depth = trial.suggest_int(\"max_depth\", 5, 12)\n    _min_samp_split = trial.suggest_int(\"min_samples_split\", 2, 8)\n    _min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 3, 6)\n    _max_features = trial.suggest_int(\"max_features\", 10, 50)\n\n    rf = RandomForestRegressor(\n     max_depth=_max_depth,\n     min_samples_split=_min_samp_split,\n     min_samples_leaf=_min_samples_leaf,\n     max_features=_max_features,\n     n_estimators=_n_estimators,\n     n_jobs=-1,\n     random_state=42,\n    )\n\n    score = cross_val_score(\n     rf, train_data, target, cv=cv, scoring=\"neg_root_mean_squared_error\"\n    ).mean()\n    return score\n\n\noptuna.logging.set_verbosity(0)\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=50)\n\nrf_params = study.best_params\nrf_best_score = study.best_value\nprint(f\"Best score:{rf_best_score} \\nOptimized parameters: {rf_params}\")\n\"\"\"","fb088c6a":"\"\"\"\ndef objective(trial):\n    _num_leaves = trial.suggest_int(\"num_leaves\", 5, 20)\n    _max_depth = trial.suggest_int(\"max_depth\", 3, 8)\n    _learning_rate = trial.suggest_float(\"learning_rate\", 0.1, 0.4)\n    _n_estimators = trial.suggest_int(\"n_estimators\", 50, 150)\n    _min_child_weight = trial.suggest_float(\"min_child_weight\", 0.2, 0.6)\n\n    lgbm = LGBMRegressor(\n     num_leaves=_num_leaves,\n     max_depth=_max_depth,\n     learning_rate=_learning_rate,\n     n_estimators=_n_estimators,\n     min_child_weight=_min_child_weight,\n    )\n\n    score = cross_val_score(\n     lgbm, train_data, target, cv=cv, scoring=\"neg_root_mean_squared_error\"\n    ).mean()\n    \n    return score\n\n\noptuna.logging.set_verbosity(0)\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=100)\n\nlgbm_params = study.best_params\nlgbm_best_score = study.best_value\nprint(f\"Best score:{lgbm_best_score} \\nOptimized parameters: {lgbm_params}\")\n\"\"\"","afd08e6d":"rf_params = {\n    'max_depth': 12, \n    'min_samples_split': 2,\n    'min_samples_leaf': 3,\n    'max_features': 48,\n    \"n_estimators\": 145\n}\nsvr_params = {\n    \"kernel\": \"poly\",\n    \"C\": 0.3146360056513227,\n    \"epsilon\": 0.010198286623541677,\n    \"coef0\": 0.9829844203739042,\n}\nlgbm_params = {\n    'num_leaves': 10,\n    'max_depth': 8,\n    'learning_rate': 0.10036766735481062,\n    'n_estimators': 150,\n    'min_child_weight': 0.42212224732027986\n}\n","7db2cb29":"from mlxtend.regressor import StackingCVRegressor\nfrom sklearn.linear_model import LinearRegression\n\ncv = KFold(n_splits=4, random_state=42)\n\nsvr = SVR(**svr_params)\nlgbm = LGBMRegressor(**lgbm_params, random_state=42)\nrf = RandomForestRegressor(**rf_params, random_state=42)\nstack = StackingCVRegressor(\n    regressors=[svr, lgbm, rf],\n    meta_regressor=LinearRegression(n_jobs=-1),\n    random_state=42,\n    cv=cv,\n    n_jobs=-1,\n)\n\nstack_scores = cross_val_score(\n    stack, train_data, target, cv=cv, n_jobs=-1, error_score=\"neg_root_mean_squared_error\"\n)\nstack.fit(train_data.values, target.values)\n","65249edc":"predictions = stack.predict(test_data)\npredictions = np.exp(predictions)","697b9db5":"submission = pd.DataFrame({\"Id\": test_ID.values, \"SalePrice\": predictions})\nsubmission.to_csv('submission.csv', index=False)","278a88d0":"predictions","273ec416":"### Missing Values","83361553":"## Modelling\n","5c3962a3":"## Data Preprocessing","e3fd0c2b":"### Target Variable","8739db46":"### Categorical Variables","61ae930b":"## Read Data ","079f6f69":"### Data Correlation"}}