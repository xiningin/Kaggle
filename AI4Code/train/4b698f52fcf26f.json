{"cell_type":{"2c7e49a6":"code","51dd089d":"code","dabf7367":"code","eedaa543":"code","be4e61d1":"code","9f2238c7":"code","c4f032bf":"code","263c2382":"code","8536070f":"code","07620e93":"code","64728891":"code","ed3ec738":"code","59d49727":"code","609d379d":"code","93168840":"code","a59aebdd":"code","b088ad13":"code","cfc73199":"code","2f4a97e3":"code","e079428d":"code","e151092c":"code","33bd7e02":"code","43cc2b8a":"code","dd0db21e":"code","94c889a6":"code","01a9de44":"code","8eaf9d64":"code","1c3f1e7b":"code","73797722":"code","ea7fe6a9":"code","ed6180cd":"code","a9234692":"code","3d0a35ae":"code","924b2b28":"code","5a389675":"code","94b86b65":"code","64cb8cdb":"code","88dc107b":"code","b9851a33":"code","385afbc1":"markdown","86fef4e9":"markdown","0208cd8c":"markdown","15d6243b":"markdown","555db1b2":"markdown","c9ffc581":"markdown","01d112b8":"markdown","40f73f39":"markdown","6ee8f867":"markdown","91df6bd6":"markdown","ed011c95":"markdown","3be1890c":"markdown","471e5a75":"markdown","2dc1888f":"markdown","edfb6505":"markdown","8c5ed99d":"markdown","8346e3e1":"markdown","d7640429":"markdown","b5111ee6":"markdown","1550d9b5":"markdown","77567507":"markdown","a95a0da7":"markdown","ad587ccc":"markdown","84f01777":"markdown","e054688a":"markdown","a99e6b26":"markdown","7faa6b7f":"markdown","f9c70586":"markdown"},"source":{"2c7e49a6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","51dd089d":"# DataFrame\nimport pandas as pd\n\n# Matplot\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Scikit-learn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.manifold import TSNE\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# Summary\nfrom sklearn import datasets\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\n\n# from sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.pipeline import Pipeline\n\nfrom scipy.stats import randint\n\n# tensor-Keras\nimport tensorflow as tf \nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\nfrom tensorflow.keras import utils\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\n# nltk\nimport nltk\nfrom nltk.corpus import stopwords\nfrom  nltk.stem import SnowballStemmer\n\n# Word2vec\nimport gensim\n\n# Utility\nimport re\nimport numpy as np\nimport os\nfrom collections import Counter\nimport logging\nimport time\nimport pickle\nimport itertools\nfrom sklearn.impute import SimpleImputer\n\n\nimport warnings\nimport scipy.io\nprint(\"TensorFlow version: \", tf.__version__)\n\nwarnings.filterwarnings('ignore')","dabf7367":"# Import the hashing vectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\n# Import functional utilities\nfrom sklearn.preprocessing import FunctionTransformer, MaxAbsScaler\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom scipy import sparse\nfrom itertools import combinations\nimport string ","eedaa543":"print(tf.test.gpu_device_name())\n# See https:\/\/www.tensorflow.org\/tutorials\/using_gpu#allowing_gpu_memory_growth","be4e61d1":"# Import the datasets\ntrain_raw= pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_raw = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')","9f2238c7":"import spacy\nnlp = spacy.load('en_core_web_sm')\nclass CleanText(BaseEstimator, TransformerMixin):\n   \n    def remove_mentions(self, input_text):\n        return re.sub(r'@\\w+', '', input_text)\n    \n    def remove_urls(self, input_text):\n        return re.sub(r'http.?:\/\/[^\\s]+[\\s]?', '', input_text)\n    \n    def emoji_oneword(self, input_text):\n        # By compressing the underscore, the emoji is kept as one word\n        return input_text.replace('_','')\n    \n    def remove_punctuation(self, input_text):\n        # Make translation table\n        punct = string.punctuation\n        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n        return input_text.translate(trantab)\n\n    def remove_digits(self, input_text):\n        return re.sub('\\d+', '', input_text)\n    \n    def to_lower(self, input_text):\n        return input_text.lower()\n    \n    def remove_stopwords(self, input_text):\n        stopwords_list = stopwords.words('english')\n        # Some words which might indicate a certain sentiment are kept via a whitelist\n        whitelist = [\"n't\", \"not\", \"no\"]\n        words = input_text.split() \n        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n        return \" \".join(clean_words) \n    \n    def Lemmatizing(self, input_text):\n        # Create our list of stopwords\n        # Lemmatizing each token and converting each token into lowercase\n       \n        # use word_tokenize to tokenize the sentences\n        mytokens = nlp(input_text)\n        mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n        return \" \".join(mytokens)\n    \n    def fit(self, X, y=None, **fit_params):\n        return self\n    \n    def transform(self, X, **transform_params):\n        clean_X = X.apply(self.remove_mentions).apply(self.remove_urls).apply(self.emoji_oneword).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.to_lower).apply(self.remove_stopwords).apply(self.Lemmatizing)\n        return clean_X","c4f032bf":"#ct = CleanText()\n#train_clean = ct.fit_transform(train_raw['excerpt'])","263c2382":"#empty_clean = train_clean == ''\n#print('{} records have no words left after text cleaning'.format(train_clean[empty_clean].count()))\n#train_clean.loc[empty_clean] = '[no_text]'","8536070f":"#df_model = train_raw.copy()\n#df_model['clean_text'] = train_clean\n#df_model.columns.tolist()","07620e93":"# Transform the list of sentences into a list of words\n#all_words = ' '.join(df_model['clean_text']).split(' ')\n#all_words=[w for w in DOC ]\n# Get number of unique words\n#vocab_size = len(set(all_words))\n#print(vocab_size)","64728891":"from sklearn.preprocessing import OneHotEncoder,LabelEncoder\nimport tensorflow as tf \nfeatures =train_raw['excerpt']\ntarget = train_raw['target'].values","ed3ec738":"import pandas as pd\nfrom transformers import TFBertModel\nfrom transformers import AutoTokenizer\n\nSEQ_LEN = 128  # we will cut\/pad our sequences to a length of 128 tokens\n\ntokenizer = AutoTokenizer.from_pretrained('..\/input\/huggingface-bert\/bert-base-uncased')\n\ndef tokenize(sentence):\n    tokens = tokenizer.encode_plus(sentence, max_length=SEQ_LEN,\n                                   truncation=True, padding='max_length',\n                                   add_special_tokens=True, return_attention_mask=True,\n                                   return_token_type_ids=False, return_tensors='tf')\n    return tokens['input_ids'], tokens['attention_mask']\n\n# initialize two arrays for input tensors\nXids = np.zeros((len(train_raw), SEQ_LEN))\nXmask = np.zeros((len(train_raw), SEQ_LEN))\n\nfor i, sentence in enumerate(train_raw['excerpt']):\n    Xids[i, :], Xmask[i, :] = tokenize(sentence)\n    if i % 10000 == 0:\n        print(i)  # do this so we can see some progress","59d49727":"import tensorflow as tf\nfrom transformers import BertTokenizer, TFBertModel\n\ntokenizer = BertTokenizer.from_pretrained('..\/input\/huggingface-bert\/bert-base-uncased')\nbert_model = TFBertModel.from_pretrained('..\/input\/huggingface-bert\/bert-base-uncased')\ninput_ids = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')\ninput_masks_ids = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32')\nbert_output=bert_model([input_ids,input_masks_ids])[0][:,0,:]\n\nmodel= tf.keras.Model(inputs=[input_ids,input_masks_ids],outputs=[bert_output])\nmodel.summary()","609d379d":"Xids.shape","93168840":"cls_emb = model.predict([Xids, Xmask])\ncls_emb.shape","a59aebdd":"cls_emb.shape","b088ad13":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom scipy import sparse\nfrom itertools import combinations\n\n\nclass SparseInteractions(BaseEstimator, TransformerMixin):\n    def __init__(self, degree=2, feature_name_separator=\"_\"):\n        self.degree = degree\n        self.feature_name_separator = feature_name_separator\n    \n    def fit(self, X, y=None):\n        return self\n        \n    def transform(self, X):\n        if not sparse.isspmatrix_csc(X):\n            X = sparse.csc_matrix(X)\n            \n        if hasattr(X, \"columns\"):\n            self.orig_col_names = X.columns\n        else:\n            self.orig_col_names = np.array([str(i) for i in range(X.shape[1])])\n            \n        spi = self._create_sparse_interactions(X)\n        return spi\n    \n    \n    def get_feature_names(self):\n        return self.feature_names\n    \n    def _create_sparse_interactions(self, X):\n        out_mat = []\n        self.feature_names = self.orig_col_names.tolist()\n        \n        for sub_degree in range(2, self.degree + 1):\n            for col_ixs in combinations(range(X.shape[1]), sub_degree):\n                # add name for new column\n                name = self.feature_name_separator.join(self.orig_col_names[list(col_ixs)])\n                self.feature_names.append(name)\n                \n                # get column multiplications value\n                out = X[:, col_ixs[0]]    \n                for j in col_ixs[1:]:\n                    out = out.multiply(X[:, j])\n\n                out_mat.append(out)\n\n        return sparse.hstack([X] + out_mat)\n# Outlier Handle \nclass OutlierReplace(BaseEstimator,TransformerMixin):\n    def __init__(self,factor=1.5):\n        self.factor = factor\n\n    def outlier_removal(self,X,y=None):\n        X = pd.Series(X).copy()\n        qmin=X.quantile(0.05)\n        qmax=X.quantile(0.95)\n        q1 = X.quantile(0.25)\n        q3 = X.quantile(0.75)\n        iqr = q3 - q1\n        lower_bound = q1 - (self.factor * iqr)\n        upper_bound = q3 + (self.factor * iqr)\n        #X.loc[((X < lower_bound) | (X > upper_bound))] = np.nan \n        X.loc[X < lower_bound] = qmin\n        X.loc[X > upper_bound] = qmax\n        return pd.Series(X)\n\n    def fit(self,X,y=None):\n        return self\n\n    def transform(self,X,y=None):\n        return X.apply(self.outlier_removal)  ","cfc73199":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\ncross_validation_design = KFold(n_splits=3,\n                                shuffle=True,\n                                random_state=77)\n\ncross_validation_design","2f4a97e3":"from xgboost import XGBRegressor\nfrom numpy import absolute\nfrom numpy import mean\nXGB_pipe=Pipeline([('Scaler',StandardScaler()),\n                   #('dim_red', SelectKBest(f_regression, k=50)),\n                   #('int', SparseInteractions(degree=2)),\n                   ('XGB',  XGBRegressor(verbosity=0,n_estimators=120, n_jobs=6))])","e079428d":"target.shape","e151092c":"#scores = cross_val_score(XGB_pipe, cls_emb, target, scoring='neg_mean_squared_error', cv=cross_validation_design, n_jobs=-1)\n# convert scores to positive\n#scores = absolute(scores)\n# summarize the result\n#s_mean = mean(scores)\n#print('Mean mean_squared_error: %.3f' % (s_mean))","33bd7e02":"#XGB_pipe.fit(cls_emb,target)\n#XGB_pipe.score(cls_emb,target)","43cc2b8a":"from tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n","dd0db21e":"# Build and compile the model\ndef create_model():\n    optimizer = tf.keras.optimizers.Adam(0.01)\n    #loss = tf.keras.losses.CategoricalCrossentropy()  # categorical = one-hot\n\n    best_weights_file = \".\/weights.h5\"\n    batch_size = 8\n    max_epochs= 1500\n    es = EarlyStopping(monitor='val_loss',min_delta=0.00000000000000000001, patience=10)\n    rmse = RootMeanSquaredError()\n    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n    model_RNN = tf.keras.Sequential(name='model_RNN')\n    model_RNN.add(tf.keras.layers.LSTM(64,input_shape=(1,768),  return_sequences=True  ))\n    model_RNN.add(tf.keras.layers.LSTM(32, return_sequences=False))\n    model_RNN.add(tf.keras.layers.Dense(16))\n    model_RNN.add(tf.keras.layers.Dense(1, name='outputs'))\n    model_RNN.compile(optimizer=optimizer, loss='mse', metrics=[rmse])\n    return model_RNN","94c889a6":"# Create a basic model instance\nmodel_RNN = create_model()\n# Display the model's architecture\nmodel_RNN.summary()","01a9de44":"# configure early stopping\nbest_weights_file = \".\/weights.h5\"\nes = EarlyStopping(monitor='val_loss',min_delta=0.000000000000000000001, patience=20)\nm_ckpt = ModelCheckpoint(best_weights_file, monitor='val_loss', mode='max', verbose=2,\n                             save_weights_only=True, save_best_only=True)\n# fit model using our gpu\nwith tf.device('\/gpu:0'):\n    history_model_RNN =model_RNN.fit(cls_emb.reshape(-1, 1,768),\n                                     target.reshape(-1, 1, 1), \n                                     batch_size=4,\n                                     epochs=1000, \n                                     verbose=0,\n                                     callbacks=[es ,m_ckpt],\n                                     validation_split=0.1)","8eaf9d64":"import matplotlib.pyplot as plt\nplt.style.use('ggplot')\ndef plot_history(history):\n    acc = history.history['root_mean_squared_error']\n    val_acc = history.history['val_root_mean_squared_error']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training root_mean_squared_error')\n    plt.plot(x, val_acc, 'r', label='Validation root_mean_squared_error')\n    plt.title('Training and validation root_mean_squared_error')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n\nplot_history(history_model_RNN)\n","1c3f1e7b":"loss_RNN, root_mean_squared_error_RNN = model_RNN.evaluate( cls_emb.reshape(-1, 1,768),\n                                     target.reshape(-1, 1, 1), verbose=0)\nprint('root_mean_squared_error_model: %f' % (root_mean_squared_error_RNN ))\nprint('loss_model: %f' % (loss_RNN ))","73797722":"def load_model():\n    bestsavedRnn = create_model()\n    best_weights_file = \".\/weights.h5\"\n    batch_size = 8\n    max_epochs= 1500\n    rmse = RootMeanSquaredError()\n    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n    bestsavedRnn.compile(optimizer=optimizer, loss='mse', metrics=[rmse])\n    return bestsavedRnn\nbestsavedRnn = load_model()\nloss_savedRnn, root_mean_squared_error_savedRnn = bestsavedRnn.evaluate( cls_emb.reshape(-1, 1,768),\n                                     target.reshape(-1, 1, 1), verbose=0)\nprint('root_mean_squared_error_model: %f' % (root_mean_squared_error_savedRnn *100))\nprint('loss_model: %f' % (loss_savedRnn*100))","ea7fe6a9":"#tf.keras.utils.plot_model(model=model_CNN_LSTM, show_shapes=True, dpi=76, )","ed6180cd":"from tensorflow.keras.layers import Dense, Lambda, Dot, Activation, Concatenate\nfrom tensorflow.keras.layers import Layer\n\n\nclass Attention(Layer):\n\n    def __init__(self, units=128, **kwargs):\n        self.units = units\n        super().__init__(**kwargs)\n\n    def __call__(self, inputs):\n        \"\"\"\n        Many-to-one attention mechanism for Keras.\n        @param inputs: 3D tensor with shape (batch_size, time_steps, input_dim).\n        @return: 2D tensor with shape (batch_size, 128)\n        @author: felixhao28, philipperemy.\n        \"\"\"\n        hidden_states = inputs\n        hidden_size = int(hidden_states.shape[2])\n        # Inside dense layer\n        #              hidden_states            dot               W            =>           score_first_part\n        # (batch_size, time_steps, hidden_size) dot (hidden_size, hidden_size) => (batch_size, time_steps, hidden_size)\n        # W is the trainable weight matrix of attention Luong's multiplicative style score\n        score_first_part = Dense(hidden_size, use_bias=False, name='attention_score_vec')(hidden_states)\n        #            score_first_part           dot        last_hidden_state     => attention_weights\n        # (batch_size, time_steps, hidden_size) dot   (batch_size, hidden_size)  => (batch_size, time_steps)\n        h_t = Lambda(lambda x: x[:, -1, :], output_shape=(hidden_size,), name='last_hidden_state')(hidden_states)\n        score = Dot(axes=[1, 2], name='attention_score')([h_t, score_first_part])\n        attention_weights = Activation('softmax', name='attention_weight')(score)\n        # (batch_size, time_steps, hidden_size) dot (batch_size, time_steps) => (batch_size, hidden_size)\n        context_vector = Dot(axes=[1, 1], name='context_vector')([hidden_states, attention_weights])\n        pre_activation = Concatenate(name='attention_output')([context_vector, h_t])\n        attention_vector = Dense(self.units, use_bias=False, activation='tanh', name='attention_vector')(pre_activation)\n        return attention_vector\n\n    def get_config(self):\n        return {'units': self.units}\n\n    @classmethod\n    def from_config(cls, config):\n        return cls(**config)","a9234692":"# summarize layers\n#print(ModelBilstm.summary())","3d0a35ae":"#tf.keras.utils.plot_model(model=ModelBilstm, show_shapes=True, dpi=76, )","924b2b28":"#es = EarlyStopping(monitor='loss',min_delta=0.0000000000000000001, patience=10)\n#rmse = RootMeanSquaredError()\n#optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n#ModelBilstm.compile(optimizer=optimizer, loss='mse', metrics=[rmse])\n","5a389675":"# initialize two arrays for input tensors\nXids_test = np.zeros((len(test_raw), SEQ_LEN))\nXmask_test = np.zeros((len(test_raw), SEQ_LEN))\n\nfor i, sentence in enumerate(test_raw['excerpt']):\n    Xids_test[i, :], Xmask_test[i, :] = tokenize(sentence)\n    if i % 10000 == 0:\n        print(i)  # do this so we can see some progress","94b86b65":"cls_emb_test = model.predict([Xids_test, Xmask_test])","64cb8cdb":"cls_emb_test.shape","88dc107b":"preds = model_RNN.predict(cls_emb_test.reshape(-1, 1,768))   \n# use the method pipeline.predict on X_test data to predict the labels\n\nmy_submission = pd.DataFrame({'id': test_raw.id, 'target': preds.ravel()})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)\n","b9851a33":"my_submission","385afbc1":"# fit model using our gpu\nwith tf.device('\/gpu:0'):\n    history_ModelBilstm = ModelBilstm.fit(cls_emb.reshape(-1, 1,768),\n                                     target.reshape(-1, 1, 1), \n                                     batch_size=8,\n                                     epochs=1000, \n                                     verbose=0,\n                                     callbacks=[es ],validation_split=0.1)","86fef4e9":"#  Deep Learning Approch\n# Simple Stack LSTM","0208cd8c":"import matplotlib.pyplot as plt\nplt.style.use('ggplot')\ndef plot_history(history):\n    acc = history.history['root_mean_squared_error']\n    val_acc = history.history['val_root_mean_squared_error']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training root_mean_squared_error')\n    plt.plot(x, val_acc, 'r', label='Validation root_mean_squared_error')\n    plt.title('Training and validation root_mean_squared_error')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n\nplot_history(history_model_CNN_LSTM)\n","15d6243b":"# Vocab Size ","555db1b2":"# Evaluate ","c9ffc581":"We compute these attention weights simply by building a small fully connected neural network on top of each encoded state. This network will have a single unit final layer which will correspond to the attention weight we will assign.\n![image.png](attachment:804f1d62-2bd9-46dc-a4c7-ec016d8a98b1.png)\n## Bi-directional RNN\nWe will use a bi-directional RNN. This is simply the concatentation of two RNNs, one which processes the sequence from left to right (the \u201cforward\u201d RNN) and one which process from right to left (the \u201cbackward\u201d RNN). By using both directions, we get a stronger encoding as each word can be encoded using the context of its neighbors on boths sides rather than just a single side. \n![image.png](attachment:c60864c0-417c-4c19-b48d-e609c10f834e.png)\n\nhttps:\/\/stackoverflow.com\/questions\/62948332\/how-to-add-attention-layer-to-a-bi-lstm\nhttps:\/\/github.com\/philipperemy\/keras-attention-mechanism\/blob\/master\/examples\/example-attention.py\nhttps:\/\/matthewmcateer.me\/blog\/getting-started-with-attention-for-classification\/","01d112b8":"# Text Preprocessing\nData preprocessing is the phase of preparing raw data to make it suitable for a machine learning model. \n\nFor NLP, that includes text cleaning, stopwords removal, stemming and lemmatization.\n\nText cleaning steps vary according to the type of data and the required task. Generally, the string is converted to lowercase and punctuation is removed before text gets tokenized. Tokenization is the process of splitting a string into a list of strings (or \u201ctokens\u201d).\nI will put all those preprocessing steps into a single function and apply it to the whole dataset\n\n## Cleaning\nBefore we start using the tweets' text we clean it. We'll do the this in the class CleanText:\n- remove the **mentions**, as we want to make the model generalisable.\n- remove the **hash tag sign** (#) but not the actual tag as this may contain information\n- set all words to **lowercase**\n- remove all **punctuations**, including the question and exclamation marks\n- remove the **urls** as they do not contain useful information and we did not notice a distinction in the number of urls used between the sentiment classes\n- make sure the converted **emojis** are kept as one word. \n- remove **digits**\n- remove **stopwords**\n- apply the **Lem** to keep the lem  of the words\n\nhttps:\/\/srinivas-yeeda.medium.com\/preprocessing-for-natural-language-processing-498df071ab6e","40f73f39":"# Transfer learning + XGBR\n## Bert Embedding \n###  Preparation and Feature Extraction for bert model :\n#### Tokenization\n\nWe have our text data in the textcolumn, which we now need to tokenize. We will use the BERT tokenizer, because we will use a BERT transformer later.\nTrain Data\nfeature Extraction X :\nhttps:\/\/www.kaggle.com\/colearninglounge\/vectorization-embeddings-elmo-bert-gpt","6ee8f867":"# Train","91df6bd6":"loss_RNN, root_mean_squared_error_RNN = model_RNN.evaluate( cls_emb.reshape(-1, 1,768),\n                                     target.reshape(-1, 1, 1), verbose=0)\nprint('root_mean_squared_error_model: %f' % (root_mean_squared_error_RNN *100))\nprint('loss_model: %f' % (loss_RNN *100))# summarize layers\nprint(BilstmAttentiondel.summary())https:\/\/medium.com\/dsaid-govtech\/building-text-classifiers-to-handle-municipal-issues-experiments-with-tf-idf-glove-bilstm-cnn-f175afafd5ea","ed011c95":"# CNN+LSTM \n![image.png](attachment:a5730cd7-c052-4128-b21a-2a40b1ab9b40.png)\n\n![image.png](attachment:6e23647b-74c9-4e1c-bc82-03ffd61babe0.png)","3be1890c":"# configure early stopping\nes = EarlyStopping(monitor='loss',min_delta=0.00000000000000000001, patience=10)\n# fit model using our gpu\nwith tf.device('\/gpu:0'):\n    history_model_CNN_LSTM =model_CNN_LSTM.fit(cls_emb.reshape(-1, 1,768),\n                                     target.reshape(-1, 1, 1), \n                                     batch_size=16,\n                                     epochs=1000, \n                                     verbose=0,\n                                     callbacks=[es ],\n                                     validation_split=0.1)","471e5a75":"# Avoid Overfitting :\n\nIn order to prevent overfitting, EarlyStopping should monitor a validation metric. Because your loss function is the mse, by monitoring val_loss you are essentially monitoring the validation Mean Squared Error. If you think that mae is a better metric for your task, you should monitor val_mae instead.\n\nWhy monitor a validation metric when performing early stopping?\n\nEarly stopping, is mostly intended to combat overfitting in your model. Overfitting is a phenomenon, commonly occurring in Machine Learning, where a model performs worse on a validation\/test set than the training set.","2dc1888f":"#Convolutional neural networks have been found to work well with text data ,the CNN Model for feature extraction \n#LSTM Model for interpreting the features across time steps.\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Activation, Multiply, Add, LSTM, LeakyReLU\nfrom tensorflow.keras.layers import Input, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.layers import Concatenate\n\n","edfb6505":"## compile models: ","8c5ed99d":"## Complete Pipe ","8346e3e1":"# X and y ","d7640429":"# Bilstm +CNN+Attention\n![image.png](attachment:e799a0b4-f4c5-40f6-b799-74aebd947ae7.png)\n\n## Create Attention Layer\nYou can use the final encoded state of a recurrent neural network for prediction. This could lose some useful information encoded in the previous steps of the sequence. In order to keep that information, you can use an average of the encoded states outputted by the RNN. But all of the encoded states of the RNN are equally valuable. Thus, we are using a weighted sum of these encoded states to make our prediction.","b5111ee6":"![image.png](attachment:ef77e825-7287-45f4-a6e1-4d778ac5c4ab.png)!","1550d9b5":"loss_CNN_LSTM, root_mean_squared_error_CNN_LSTM = model_CNN_LSTM.evaluate( cls_emb.reshape(-1, 1,768),\n                                     target.reshape(-1, 1, 1), verbose=0)\nprint('root_mean_squared_error_model: %f' % (root_mean_squared_error_CNN_LSTM *100))\nprint('loss_model: %f' % (loss_CNN_LSTM *100))","77567507":"from tensorflow.keras import Input, Model\nfrom tensorflow.keras.layers import Embedding, Dense, Dropout, Bidirectional, LSTM\n# Define\/compile the model.\ntime_steps, input_dim =  1, 768\nmodel_input = Input(shape=(time_steps, input_dim))\nbilstm1 = Bidirectional(LSTM(64, return_sequences=True))(model_input)\nbilstm2 = Bidirectional(LSTM(32, return_sequences=True))(bilstm1)\nx = Attention(32)(bilstm2)\ny = Dense(1)(x)\nModelBilstm= Model(inputs=model_input, outputs=y)","a95a0da7":"# NLP PIPELINE\n\nThere are mainly 4 stages of an NLP pipeline :  \n\n## Exploratory Data Analysis\n## Text Processing\n    Cleaning\n    Normalization\n    Tokenize\n    Stop word removal\n    Stemming and Lemmatization\n    POS and NER\n    \n## Feature Extraction\n    Bag of Words\n    TF-IDF\n    word2vec\n    Glove\n\n##  Modeling\n    Model\n    Train\n    Predict ","ad587ccc":"import matplotlib.pyplot as plt\nplt.style.use('ggplot')\ndef plot_history(history):\n    acc = history.history['root_mean_squared_error']\n    val_acc = history.history['val_root_mean_squared_error']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training root_mean_squared_error')\n    plt.plot(x, val_acc, 'r', label='Validation root_mean_squared_error')\n    plt.title('Training and validation root_mean_squared_error')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n\nplot_history(history_ModelBilstm)\n","84f01777":"model_CNN_LSTM = tf.keras.Sequential(name='model_CNN_LSTM')\nmodel_CNN_LSTM.add(tf.keras.layers.Conv1D(200, 7, input_shape=(1,768),activation='relu',padding='same'))\n#model_CNN_LSTM.add(tf.keras.layers.MaxPooling1D(pool_size=3,padding=\"VALID\"))\nmodel_CNN_LSTM.add(tf.keras.layers.Dropout(rate=0.2))\nmodel_CNN_LSTM.add(tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))\nmodel_CNN_LSTM.add(tf.keras.layers.LSTM(128, return_sequences=False, dropout=0.1, recurrent_dropout=0.1))\nmodel_CNN_LSTM.add(tf.keras.layers.Dense(16))\nmodel_CNN_LSTM.add(tf.keras.layers.Dense(1, name='outputs'))\nmodel_CNN_LSTM.compile(optimizer=optimizer, loss='mse', metrics=[rmse])\nmodel_CNN_LSTM.summary()","e054688a":"loss_Bi_LSTM, root_mean_squared_error_Bi_LSTM = ModelBilstm.evaluate( cls_emb.reshape(-1, 1,768),\n                                     target.reshape(-1, 1, 1), verbose=0)\nprint('root_mean_squared_error_model: %f' % (root_mean_squared_error_Bi_LSTM *100))\nprint('loss_model: %f' % (loss_Bi_LSTM *100))","a99e6b26":"# Sumbmission: ","7faa6b7f":"# Best saved wieghts score ","f9c70586":"# XGBR + Bert Embedding "}}