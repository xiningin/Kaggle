{"cell_type":{"4cadb453":"code","469e8cc2":"code","862d9523":"code","9269e22e":"code","d728c787":"code","d19a9710":"code","3fe3e281":"code","335fc58c":"code","3dc14f7d":"code","910b98b5":"code","9f80aca7":"code","f05a9846":"code","a6263d9e":"code","a61c50cb":"code","e2310f09":"code","d7bf3c0a":"code","1dc8c0da":"code","21982a8d":"code","5b6b55ab":"code","c2a88815":"code","b93a42e0":"code","317797e9":"code","e9edf728":"code","0fdc9e55":"code","a400ca52":"code","e8405a59":"code","bd46b2fa":"code","889b7b1d":"code","d7e541da":"code","1504a7b2":"code","cec59093":"code","13f16b14":"code","268de2f4":"markdown","34c4978e":"markdown","d8cefa28":"markdown","2a78d230":"markdown","dc151f7f":"markdown","0fba0542":"markdown","2a9479b3":"markdown","252abad8":"markdown","f1c44774":"markdown","d19b62d0":"markdown","44c2761c":"markdown","c6ac2139":"markdown","d3b917ef":"markdown","739854b0":"markdown","7c586d02":"markdown","abe5dda3":"markdown","e88da9ac":"markdown","5a2d844a":"markdown","af66ef7a":"markdown","1f73859a":"markdown","5f35b251":"markdown","fb0373c6":"markdown","0230f2a1":"markdown","b0c1d663":"markdown","19192f0b":"markdown","0947f10e":"markdown","8457cf17":"markdown","32b2f6a7":"markdown","12e44382":"markdown","a5d5b67a":"markdown","f48b6387":"markdown","904e32ab":"markdown","c835916b":"markdown","2b6fa67c":"markdown","3c8af631":"markdown","3e83e481":"markdown","3675c7ab":"markdown","e04be7a9":"markdown","77c350c6":"markdown","6862afb7":"markdown","61fb3f61":"markdown","91c98c00":"markdown","e96d03ed":"markdown","1e3141dd":"markdown","1421dc32":"markdown","3a55e78b":"markdown","b2cfbfee":"markdown","88232b59":"markdown"},"source":{"4cadb453":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom glob import glob\nfrom PIL import Image\n\nfrom warnings import filterwarnings\n\nnp.random.seed(101)\nfilterwarnings('ignore')\nsns.set_style('darkgrid')","469e8cc2":"base_loc = '..\/input'\nimage_paths = {os.path.splitext(os.path.basename(x))[0]: x for x in glob(os.path.join(base_loc, '*', '*.jpg'))}","862d9523":"df_skin = pd.read_csv(os.path.join(base_loc, 'HAM10000_metadata.csv'))\ndf_skin.head()","9269e22e":"df_skin['image_path'] = df_skin['image_id'].map(image_paths.get)","d728c787":"lesion_types = {\n    'nv': 'Melanocytic nevi',\n    'mel': 'Melanoma',\n    'bkl': 'Benign keratosis-like lesions ',\n    'bcc': 'Basal cell carcinoma',\n    'akiec': 'Actinic keratoses',\n    'vasc': 'Vascular lesions',\n    'df': 'Dermatofibroma'\n}\n\ndf_skin['type'] = df_skin['dx'].map(lesion_types.get)\n\n# Converting the type to the categorical values\ndf_skin['type_id'] = pd.Categorical(df_skin['type']).codes\n\ndf_skin.head()","d19a9710":"sns.heatmap(df_skin.isna(), cbar = False, cmap= 'plasma')\nprint(df_skin.isna().sum())","3fe3e281":"print('The % of missing values is only {:.2f} so we will consider filling them with mean valueof the entire column'.format(df_skin['age'].isna().sum()\/len(df_skin)*100))\ndf_skin['age'] = df_skin['age'].fillna(df_skin['age'].mean())\ndf_skin.head()","335fc58c":"df_skin.info()","3dc14f7d":"plt.figure(figsize = (20,14))\n\nplt.subplot(2,2,1)\nfig = sns.countplot(y = df_skin['type'], order = df_skin['type'].value_counts().index, palette= 'viridis')\nplt.xticks(fig.get_xticks())\nplt.title('Most Frequent Type of Cells')\n\nplt.subplot(2,2,2)\nfig = sns.countplot(x = df_skin['dx_type'], order = df_skin['dx_type'].value_counts().index, palette= 'autumn')\nplt.xticks(fig.get_xticks())\nplt.title('The Technical Validation')\n\nplt.subplot(2,2,3)\nfig = sns.countplot(x = df_skin['localization'], order = df_skin['localization'].value_counts().index, palette= 'inferno')\nplt.xticks(fig.get_xticks(),rotation = 90)\nplt.title('Most Frequent Localizations')\n\nplt.subplot(2,2,4)\nfig = sns.countplot(x = df_skin['sex'], order = df_skin['sex'].value_counts().index, palette= 'summer')\nplt.xticks(fig.get_xticks(),rotation = 90)\nplt.title('Sex')\n\n# plt.tight_layout()\nplt.show()","910b98b5":"plt.figure(figsize=(18, 12))\nplt.suptitle('Distribution of Age')\n\nplt.subplot(2,2,1)\nsns.distplot(df_skin['age'], color= 'green')\nplt.title('Overall Distribution')\nplt.xticks(list(range(0,100,10)))\n\nplt.subplot(2,2,2)\nsns.kdeplot(df_skin['age'], shade = True, color = 'green')\nplt.title('Overall Distribution')\nplt.xticks(list(range(0,100,10)))\n\nplt.subplot(2,2,2)\nsns.kdeplot(df_skin['age'], shade = True, color = 'green')\nplt.xticks(list(range(0,100,10)))\n\nplt.subplot(2,2,3)\nsns.kdeplot(df_skin[df_skin['sex'] == 'male']['age'],label = 'Male', shade = True, color = 'blue')\nplt.xticks(list(range(0,100,10)))\nplt.title('Distribution Among Males')\n\nplt.subplot(2,2,4)\nsns.kdeplot(df_skin[df_skin['sex'] == 'female']['age'],label = 'Female', shade = True, color = 'red')\nplt.title('Distribution Among Females')\nplt.xticks(list(range(0,100,10)))\n\n\nplt.show()","9f80aca7":"plt.figure(figsize=(9, 7))\nsns.scatterplot(df_skin['age'], df_skin['type_id'])\nplt.title('Types vs Age')\nplt.show()","f05a9846":"Image.open(df_skin['image_path'][1])","a6263d9e":"Image.open(df_skin['image_path'][1]).resize((100,75))","a61c50cb":"print(np.asarray(Image.open(df_skin['image_path'][0]).resize((100,75)))[:4])","e2310f09":"df_skin['image'] = df_skin['image_path'].map(lambda x: np.asarray(Image.open(x).resize((100,75))))","d7bf3c0a":"df_skin.head()","1dc8c0da":"plt.figure(figsize= (20,10))\nfor i,img in enumerate(np.random.random_integers(0, 10000, 25)):\n    plt.subplot(5,5,i+1)\n    plt.imshow(df_skin['image'][img])\n    plt.title(df_skin['type'][img])\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid()\nplt.tight_layout()\nplt.show()","21982a8d":"df_skin.head()","5b6b55ab":"features = df_skin.drop(columns=['type_id'],axis=1)\ntarget = df_skin['type_id']","c2a88815":"from sklearn.model_selection import train_test_split\nX_train_, X_test_, y_train_, y_test_ = train_test_split(features, target, test_size = 0.20, random_state = 101)\nprint('The length of training Set is {}\\nThe length of the test set is {}\\nThe ratio is {}'.format(len(X_train), len(X_test), '80\/20'))","b93a42e0":"X_train = np.asarray(X_train_['image'].tolist())\nX_test = np.asarray(X_test_['image'].tolist())\n\nX_train = (X_train - X_train.mean())\/X_train.std()\nX_test = (X_test - X_test.mean())\/X_test.std()","317797e9":"from keras.utils.np_utils import to_categorical","e9edf728":"y_train = to_categorical(y_train_, num_classes= 7)\ny_test = to_categorical(y_test_, num_classes= 7)","0fdc9e55":"y_test[:10]","a400ca52":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ReduceLROnPlateau","e8405a59":"model = Sequential()\nmodel.add(Conv2D(32, kernel_size= (3,3), activation= 'relu', padding= 'Same', input_shape = (75, 100, 3)))\nmodel.add(Conv2D(32, kernel_size= (3,3), activation= 'relu', padding= 'Same'))\nmodel.add(MaxPool2D(pool_size= (2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, kernel_size= (3,3), activation= 'relu', padding= 'Same'))\nmodel.add(Conv2D(64, kernel_size= (3,3), activation= 'relu', padding= 'Same'))\nmodel.add(MaxPool2D(pool_size= (2, 2)))\nmodel.add(Dropout(0.4))\n\nmodel.add(Flatten())\nmodel.add(Dense(128, activation= 'relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(7, activation= 'softmax'))\nmodel.summary()\n","bd46b2fa":"model.compile(Adam(), loss = 'categorical_crossentropy', metrics = ['mae', 'acc'])","889b7b1d":"annealer = ReduceLROnPlateau(monitor= 'val_acc')","d7e541da":"mod = model.fit(x = X_train, y = y_train, epochs= 50, callbacks= [annealer])","1504a7b2":"import pickle\npickle.dump(model,open('model.pkl','wb'))","cec59093":"loss, mae, acc = model.evaluate(X_test, y_test)\nprint(\"The accuracy of the model is {:.3f}\\nThe Loss in the model is {:.3f}\".format(acc,loss))","13f16b14":"plt.figure(figsize=(18,6))\nplt.subplot(1,2,1)\nplt.plot(mod.history['acc'], color = 'green')\nplt.title('The Training Accuracy')\nplt.subplot(1,2,2)\nplt.plot(mod.history['loss'], color = 'red')\nplt.title('The Training Loss')\nplt.show()","268de2f4":"**Learning rate annealing**, recommends starting with a relatively high learning rate and then gradually lowering the learning rate during training. The intuition behind this approach is that we'd like to traverse quickly from the initial parameters to a range of \"good\" parameter values but then we'd like a learning rate small enough that we can explore the \"deeper, but narrower parts of the loss function\".  \n\nFor more on Leaning Rate Annealing read [this](https:\/\/www.jeremyjordan.me\/nn-learning-rate\/)","34c4978e":"Let us observe the age for the lesions.","d8cefa28":"## De nada!","2a78d230":"## EDA","dc151f7f":"___","0fba0542":"We will use the standard normalization formula\n> ![normalization](https:\/\/cdn-images-1.medium.com\/max\/1600\/1*w5nOX2X-62jGQ6_52nqmFA@2x.png)","2a9479b3":"The array of image will be like","252abad8":"Fitting the model","f1c44774":"Now we see how good our model performs on the Test Set","d19b62d0":"Checking the data for missing values.","44c2761c":"Mapping the image paths to the respective **image_id**s","c6ac2139":"## The Imports","d3b917ef":"We can observe that there are 57 missing values in the age column which seems to be in the final observations of the data.","739854b0":"So we have obtained an accuracy of 75% on the test set","7c586d02":"### Normalizing the data","abe5dda3":"### Encoding the Target Data","e88da9ac":"We use the Keras API for the implementation of the CNN.  \n\n* The first two layers use 32 filters and the last two contains 64 filters.  \n* We downsample the images twice by Maxpooling.\n* To prevent from overfitting we use DropOut thrice in the network\n* After the convolution layers we flatten the output.\n* The output is ten fed to a Dense neural network.\n* The intermediate layers use 'relu' for activation\n* The final layer uses 'softmax' as activation","5a2d844a":"We will use *Adam* as our optimizer","af66ef7a":"Setting the features and target variables","1f73859a":"### Splitting the data","5f35b251":"Let's check on the types of the features","fb0373c6":"## Building up to the model","0230f2a1":"## Loading the Data Properly","b0c1d663":"Since our target column contains the respective type ids we will convert the column into proper *One Hot Encoding*","19192f0b":"Resized Image","0947f10e":"# Skin Lesion Identification","8457cf17":"**Observations**:\n\n**Plot 1**: It can be seen that *Melanocytic Nevi* is around twice more than all of the other types combined.  \n**Plot 2**: We can observe that *histo* and *follow up* are the most occuring ground truths.  \n**Plot 3**: *back* and *lower extremity* are most frequent localizations on the body.  \n**Plot 4**: There are slightly more males in the data. And few unknown values as well which could just be missings or other sex categories.","32b2f6a7":"Plotting some basic features","12e44382":"The meta-data file contains all the information regarding to the lesions and the patients of the lesions.","a5d5b67a":"**Observations**:  \n\nIt can be observed that most patients are in the age group of 35 to 65 with most in the range of 40-50.  \nThe *males* are mostly in the range 40-70.  \nThe *females* are in the range 30-60.","f48b6387":"We split the data in order to check the performance of the model","904e32ab":"For more information on the data, follow [this](https:\/\/www.nature.com\/articles\/sdata2018161) link","c835916b":"Firstly, we form a dictionary with the format {image_name: image_path} so we could easily manipulate it later.","2b6fa67c":"![cnn](https:\/\/www.machinecurve.com\/wp-content\/uploads\/2018\/12\/convnet_fig-710x210.png)","3c8af631":"**Observation**:  \n\nIt seems that categories 0, 1, 3 and 5 are less common in age group less than 20.  \n\n0 - Melanocytic nevi  \n1 - Dermatofibroma  \n3 - Basal cell carcinoma  \n5 - Vascular lesions ","3e83e481":"Full Image","3675c7ab":"## Image Loading","e04be7a9":"Now we convert all the images in this way and add it to a new column **image**","77c350c6":"The image dimensions are *450 x 600 x 3* which are too large to handle so we size it down to *100 x 75* and load the image matrix in the **image** column.","6862afb7":"#### Using the Convolution Neural Network","61fb3f61":"[Prashant Brahmbhatt](https:\/\/www.github.com\/hashbanger)","91c98c00":"___","e96d03ed":"## Loading the Meta Data","1e3141dd":"Dermatoscopy is a widely used diagnostic technique that improves the diagnosis of benign and malignant pigmented skin lesions in comparison to examination with the unaided eye1. Dermatoscopic images are also a suitable source to train artificial neural networks to diagnose pigmented skin lesions automatically.  \nMachine learning techniques set new benchmarks with regard to the complexity of neural networks and raised expectations that automated diagnostic systems will diagnose all kinds of pigmented skin lesions without the need of human expertise.","1421dc32":"The output dctionary is like in the format   \n**{'ISIC_0027269'  :  '..\/input\/ham10000_images_part_1\/ISIC_0027269.jpg',  \n    'ISIC_0025716'  :  '..\/input\/ham10000_images_part_1\/ISIC_0025716.jpg',  \n    ...}**","3a55e78b":"### Testing on test Set","b2cfbfee":"We can form a dictionary of the types of lesions to map the names to the **dx** column given in the metadata and also convert it corresponding categorical values in the a new column.","88232b59":"### Model"}}