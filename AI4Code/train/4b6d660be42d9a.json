{"cell_type":{"8068166e":"code","fa332a9e":"code","312fdab9":"code","f07e3a2a":"code","83b612dd":"code","22912880":"code","5cb44606":"code","ac0fea42":"code","55bbe0ff":"code","720f181f":"code","ebc8e2f9":"code","3ea6f423":"code","b379fe4d":"code","d59ddc0f":"code","66b99e0f":"code","e83564ef":"code","599dfc21":"code","fd167b1a":"code","a6766c50":"code","5f7f797d":"code","10e98329":"code","6cdfcd0f":"code","f495fca3":"code","6eb9f12b":"code","30f2c999":"code","7b9a8620":"code","cccc2393":"markdown","5fe6ed61":"markdown","9ff04f45":"markdown","ee70afa3":"markdown","183a6060":"markdown","6241d434":"markdown","c624cac9":"markdown","71cd2d8f":"markdown","8c40cfb8":"markdown","56360a6e":"markdown","f479c341":"markdown","47daf335":"markdown","544d499f":"markdown","5809900f":"markdown","f9d1a4c3":"markdown","fb31684a":"markdown","a2b30649":"markdown","d00792af":"markdown","da331db9":"markdown","c08fb58d":"markdown","75f017bc":"markdown"},"source":{"8068166e":"#Import important libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","fa332a9e":"# Load the wisconsin dataset\ndata = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\n\ndata.head(10)","312fdab9":"# Printing all the columns\ndata.columns[1:]","f07e3a2a":"data.drop('Unnamed: 32', axis=1, inplace=True)\n\ndata.columns[1:]","83b612dd":"#Using Label Encoders to replace values in a single column\nfrom sklearn import preprocessing\n\nlabelEncoder = preprocessing.LabelEncoder()","22912880":"data['diagnosis'] = labelEncoder.fit_transform(data['diagnosis'])\n\nprint(data['diagnosis'].head())","5cb44606":"# import plotting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns","ac0fea42":"# Pie chart representation\npie_labels = ['Benign', 'Malignant']\n\n#Number of benign and malignant cases\npie_y = data['diagnosis'].value_counts()\n\npie_explode = [0, 0.1]\n\nplt.figure(figsize=(10, 8))\nplt.pie(pie_y, labels=pie_labels, shadow=True,  autopct='%1.1f%%', explode=pie_explode, textprops={'fontsize': 14})\nplt.legend()\nplt.title(\"Percent of Cases in the Data\")\nplt.show()","55bbe0ff":"# Settings all columns on y axis and plotting them\n%matplotlib inline\nsns.set_theme(style=\"whitegrid\")\n\nx = data['diagnosis']\n\ny = ['radius_mean', 'texture_mean', 'perimeter_mean',\n     'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n     'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']\n\nfig, axes = plt.subplots(len(y)\/\/2, 2, figsize=(20, 35))\naxes_flat = axes.flatten()\n\nindex = 0\nfor para in y:\n    axis = axes_flat[index]\n    axis.xaxis.label.set_size(20)\n    axis.yaxis.label.set_size(20)\n    axis.tick_params(labelsize=18)\n    \n    sns.boxplot(x=x, y=data[para], data=data, ax=axis)\n    index += 1\n    \nplt.show()","720f181f":"#Separating result from input dataset\ny = data['diagnosis']\ndata.drop(labels=['diagnosis'], axis=1, inplace=True)\n\ndata.head()\nprint(y.head())","ebc8e2f9":"#Splitting the dataset \nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.3, random_state=0)\n\nprint(X_train.shape)","3ea6f423":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","b379fe4d":"#Getting a Scaled Dataframe\nall_cols = data.columns\nindex = data.index\n\nscaled_data = pd.DataFrame(X_train, columns=all_cols)\n\nscaled_data.head()","d59ddc0f":"from sklearn.metrics import confusion_matrix\n\n#Using GridSearchCV for hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV\n\n#Helps to plot confusion matrix of different Models\ndef plot_confusion(prediction):\n    confusion_labels = np.array([['True Neg.', 'False Pos.'], ['False Neg.', 'True Pos.']])\n    \n    cm = confusion_matrix(y_test, prediction)\n\n    df_cm = pd.DataFrame(cm, range(2), range(2))\n\n    plt.figure(figsize=(8,8))\n\n    sns.set(font_scale=1.4) # for label size\n\n    labels = (np.asarray([\"{0}\\n\\n{1}\".format(string, value)\n                          for string, value in zip(confusion_labels.flatten(),\n                                                   cm.flatten())])\n             ).reshape(2,2)\n\n    ax = sns.heatmap(df_cm, annot=labels, annot_kws={\"size\": 16}, fmt='', cbar=False) # font size\n    \n    \n    # Confusion Matrix Labels\n    ax.set_xticklabels(['Negative', 'Positive'])\n    ax.set_yticklabels(['Negative', 'Positive'])\n\n    plt.show()\n    \n    #Print Important Medical terms\n    Specificity = (cm[0][0]) \/ (cm[0][0] + cm[0][1])\n    print(\"\\nSpecificity is: {0:.2f}%\".format(Specificity*100))\n\n    Sensitivity = (cm[1][1]) \/ (cm[1][1] + cm[1][0])\n    print(\"\\nSensitivity is: {0:.2f}%\".format(Sensitivity*100))","66b99e0f":"# Use Logistic Selection Model for classification\nfrom sklearn.linear_model import LogisticRegression\n\ngrid_values = {'penalty': ['l1','l2'], 'C': [0.001,0.01,0.1,1,10,100,1000]}\nclassifier_lr = GridSearchCV(\n                cv=None,\n                estimator=LogisticRegression(random_state = 0, penalty='l1', solver='liblinear'), \n                param_grid=grid_values)\n\nclassifier_lr.fit(X_train, y_train)\npred_lr = classifier_lr.predict(X_test)\n\n#Printing the best hyperparameters\nprint('Best paramters for Logistic Regression: ', classifier_lr.best_params_)","e83564ef":"#Prediction of Logistic Regression\nplot_confusion(pred_lr)","599dfc21":"# Use KNeighbors Model for classification\nfrom sklearn.neighbors import KNeighborsClassifier\n\ngrid_values = {'n_neighbors': [3, 5, 9, 11],\n               'weights': ['uniform', 'distance'],\n               'metric': ['euclidean', 'manhattan']\n              }\n\nclassifier_knn = GridSearchCV(\n                 KNeighborsClassifier(),\n                 param_grid=grid_values\n                )\n\nclassifier_knn.fit(X_train, y_train)\npred_knn = classifier_knn.predict(X_test)\n\n#Printing the best hyperparameters\nprint('Best paramters for KNN: ', classifier_knn.best_params_)","fd167b1a":"#Prediction of K nearest Neighbours\nplot_confusion(pred_knn)","a6766c50":"#Using GaussianNB method to use Naive Bayes Algorithm\nfrom sklearn.naive_bayes import GaussianNB\n\n#No need for GridSearch as of now since GaussianNB does not accept parameters\nclassifier_nb = GaussianNB()\n\nclassifier_nb.fit(X_train, y_train)\npred_nb = classifier_nb.predict(X_test)","5f7f797d":"#Prediction of Naive Bayes\nplot_confusion(pred_nb)","10e98329":"# Using Decision Tree Model for Classification\nfrom sklearn.tree import DecisionTreeClassifier\n\ngrid_values = {'criterion':['gini','entropy'],'max_depth':[4,5,6,7,8,9,10,11,12,15]}\n\nclassifier_dt = GridSearchCV(\n                 DecisionTreeClassifier(),\n                 grid_values)\n\nclassifier_dt.fit(X_train, y_train)\npred_dt = classifier_dt.predict(X_test)\n\nprint('Best paramters for Decision Tree: ', classifier_dt.best_params_)","6cdfcd0f":"#Prediction of Decision Tree\nplot_confusion(pred_dt)","f495fca3":"#Using Support Vector Machine for Classification\nfrom sklearn.svm import SVC\n\ngrid_values = {'kernel': ['rbf', 'linear', 'poly'], \n               'C': [0.1, 1, 10, 100, 1000], \n               'gamma': [1, 0.1, 0.01, 0.001, 0.0001]}\n\nclassifier_svc = GridSearchCV(\n                 SVC(),\n                 grid_values)\n\nclassifier_svc.fit(X_train, y_train)\npred_svc = classifier_svc.predict(X_test)\n\nprint('Best paramters for SVC: ', classifier_svc.best_params_)","6eb9f12b":"#Prediction of Support Vector\nplot_confusion(pred_svc)","30f2c999":"#Using Random Forest Model for Classification\nfrom sklearn.ensemble import RandomForestClassifier\n\ngrid_values = {'bootstrap': [True], \n               'max_depth': [5, 10, None], \n               'max_features': ['auto', 'log2'], \n               'n_estimators': [5, 6, 7, 8, 9, 10, 11, 12, 13, 15]}\n\nclassifier_rf = GridSearchCV(\n                 RandomForestClassifier(),\n                 grid_values)\n\nclassifier_rf.fit(X_train, y_train)\npred_rf = classifier_rf.predict(X_test)\n\nprint('Best paramters for Random Forest: ', classifier_rf.best_params_)","7b9a8620":"#Prediction of Random Forest\nplot_confusion(pred_rf)","cccc2393":"![](https:\/\/miro.medium.com\/max\/4000\/0*0XRrnsr7h5hebu8r.png)\n\n\n### As obvious from the image above, malignant cells are larger in size, thus have a bigger radius, perimeter and area.  Due to an arbitrary shape, the malignant cells are also expected to be more concave, with a large number of concave, finger-like projections.","5fe6ed61":"## Model Selection","9ff04f45":"## Decision Tree","ee70afa3":"![](https:\/\/myvetzone.com\/wp-content\/uploads\/2017\/11\/SeSp-Sen-and-Spec-VetZone.jpg)\n\n### Thus we can define the two parameters in detection as:\n### Sensitivity: The ability of a test to correctly identify people with a disease.\n### Specificity: The ability of a test to correctly identify people without the disease.","183a6060":"It is clear and obvious that the Logistic Regression model is the most accurate for prediction of Breast Cancer w.r.t the dataset collected by Dr. William H. Wolberg, physician at the University Of Wisconsin Hospital at Madison, Wisconsin, USA.","6241d434":"### Observations from Box plots","c624cac9":"## Encoding Categorical Values","71cd2d8f":"## Naive Bayes","8c40cfb8":"## Cleaning the data","56360a6e":"## K Nearest Neighbours","f479c341":"The classification of cancer depending on variation in each individual parameter has been shown with the help of boxplots. The following inferrences can be drawn by observing these graphs:-\n\n- For each parameter (eg. radius_mean), more separated the two boxes are, the more significant role the parameter would play in deciding whether the cancer is benign or malignant. This is because, more the separation, more clear would be the signs of an abnormal behaviour by the cells.\n- This implies that the \"Fractal dimension\" of the cell will have little to no impact in determining the outcome.\n- Similarly, \"Symmetry\" of the cell is also not that influential for the result.\n- The gap between the boxes (and in turn distribution of data) in \"Smoothness\" of the cell is not that significant. The upper limit of smoothness in benign cases almost overlaps with the median of the malignant ones. Thus this property of the cell should not be given a lot of weight (but cannot be neglected) in the detection, since there is a probability that it can classify average and below average smoothness cases as benign instead of malignant.\n- Almost all the other properties of the cells show a clearer distinction in benign and malignant cases, indicating that they will probably have a stronger say in determination of the result.\n\nImportant: It must be noted that the above observations have be made by considering only the middle 50 percentile (i.e the box part) as it is just an human observation rather than a calculated judgement (Which would be too complicated for an initial observation and cannot be done with graphical observation).","47daf335":"## SVC","544d499f":"## Random Forest ","5809900f":"# Breast Cancer Detection","f9d1a4c3":"## Helper Funtion For Plotting","fb31684a":"## Visual Analysis","a2b30649":"- The final model has a commendable Specificity of 99.07%, implying that the model can correctly identify 99% of the people without Breast Cancer.\n- The model also has a Sensitivity of 93.65%, implying that the model can correctly identify approximately 94% of the people with Breast Cancer.","d00792af":"# Conclusion","da331db9":"## Plotting the initial data","c08fb58d":"## More Preprocessing and Scaling","75f017bc":"## Logistic Regression"}}