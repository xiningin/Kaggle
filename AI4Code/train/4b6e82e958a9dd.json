{"cell_type":{"cee60d88":"code","a684f08f":"code","8677f31d":"code","8cfff5a9":"code","de632bd0":"code","29c68776":"markdown","8ef56785":"markdown","76b4343e":"markdown","896a18a8":"markdown","b8a35820":"markdown","77e334e3":"markdown","59ba1842":"markdown","2c03d2fe":"markdown"},"source":{"cee60d88":"# basic ML library import\n%matplotlib inline\nimport numpy as np \nimport pandas as pd\nimport regex as re\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import *\nfrom sklearn.preprocessing import LabelEncoder\nimport gc\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\n\nimport os","a684f08f":"df_sales_train = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\ndf_sales_train.head()","8677f31d":"df_sales_train.columns","8cfff5a9":"df_train = pd.read_pickle('..\/input\/end-to-end-data-wrangling-simple-random-forest\/df_train_pickle.pkl')\ndf_train.head()","de632bd0":"df_train.columns","29c68776":"# M5 compeition Summary\nhttps:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy\nThis contest was to predict the amount of sales for each individual item that would be sold in an indidual Walmart. There was a companion competition to predict a range of sales, we will save an analysis of that contest for another notebook.\n\nSolutions thread: https:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy\/discussion\/163414\n\n# Brief overview of time series and grouped time series\n### The M5 contest is a grouped time series problem. \n\n**In a time series problem the history of something is available, and the task is to predict the next interval.**\n\n * Basic examples: predict how many people will ride the subway in a city on a given day, what price a stock will close at tomorrow, how many inches of rain will fall next month, etc etc... \n * In technical terms, the main twist of time series from other ML problems are how test and training data are split. If the data is split randomly (using something like sk-learn train test split), then the model will be allowed to \"cheat\" during training. Training must be done on the earlier dated data, and testing must be done on later data.\n  * For example if you were trying to predictthe price that a stock cloed at on Tuesday it would be much easier to do this if you know the price on Monday and Wednesday.\n\n**A grouped time series problem adds more complexity to time series in that there are many different things that often have similiarties, but are distinct from each other.**\n\n * In this contest the goal is to predict sales of individual items in individual stores. Many items sell more during holiday seasons, others are more affected by days of the week, etc.\n * [Grandmaster Filho](https:\/\/www.kaggle.com\/mariofilho) has a good [explaner article on the basics of grouped time series](https:\/\/www.mariofilho.com\/how-to-predict-multiple-time-series-with-scikit-learn-with-sales-forecasting-example\/)\n  * This article covers many of the ideas in this notebook.\n  * Also searching grouped time series on the Towards Data Science blog will [return many good articles](https:\/\/www.google.com\/search?q=grouped+time+series+towards+data+science&rlz=1C1GCEA_enUS879US879&oq=grouped+time+series+towards+data+science&aqs=chrome..69i57.4977j0j1&sourceid=chrome&ie=UTF-8)\n\n![time_series.png](attachment:2c0dba99-feab-4087-b4d2-6f0563224090.png)","8ef56785":"### After manipulations and merging other data to it, ends like this:","76b4343e":"# Top solutions\n* [1st place](https:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy\/discussion\/163684) used lightGBM many features\n* [2nd place](https:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy\/discussion\/164599) used a huge ensemble of LightGBM with different paramaters, tweedie loss\n* [3rd place](https:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy\/discussion\/164374) used DeepAR tweedie loss\n\nAll used Tweedie loss...\n[Good article explaining improving a lightGBM model by using tweedie loss.](https:\/\/medium.com\/@royravid\/have-you-heard-about-tweedie-loss-bb94551dd82f) Essentially tweedie is a distribution with a large number of zeros, and then something that looks like a normal curve after. e.g. the amount of money individuals spend who go to Amazon.com on a given day.\n\n![tweedie loss.png](attachment:1de8e7ee-e35a-4345-a710-f6ba2c464289.png)","896a18a8":"# Baseline (non-ML) model\n[Extremel simple grouped by weekday](https:\/\/www.kaggle.com\/chrisrichardmiles\/simple-model-avg-last-28-days-grouped-by-weekday) is a great example of a simple baseline that makes very good predictions, here it only had a 0.75 error, which puts it at about 850 out of 5,550 entries into the contest.\n\n#### If you remember nothing else... remember to always compare any model to a simple baseline...","b8a35820":"# Lag features\n\nUsually, the first step in improving a time series problem is creating features, usually those are lag features.\n\n### What are lag features?\n* \"A lag features is a fancy name for a variable which contains data from prior time steps. If we have time-series data, we can convert it into rows. Every row contains data about one observation and includes all previous occurrences of that observation.\"\n * https:\/\/www.mikulskibartosz.name\/forecasting-time-series-using-lag-features\/\n* [This article](https:\/\/www.mariofilho.com\/how-to-predict-multiple-time-series-with-scikit-learn-with-sales-forecasting-example\/) has a number of examples of how to create simple lag features. \n * Some exaplles of lag features are just simply looking at yesterday\/2 days ago\/last week. Moving summed stats like mean, median and standard deviation.\n\n\n### [Grandmaster Konstantin](https:\/\/www.kaggle.com\/kyakovlev\/notebooks?userId=2405813&sortBy=voteCount&tab=profile) \nThis grandmaster created and shared a series of notebooks the one that we'll talk about now is this one:\n### [Konstantin's lag features notebook](https:\/\/www.kaggle.com\/kyakovlev\/m5-lags-features)\nThis notebook goes through how to create lags for this competition with the given data. \n\nSimilar to Filho's article the example code uses lines like these:\n\n![Screen Shot 2021-06-13 at 11.07.04 PM.png](attachment:dc153b6a-2d1f-44b9-8414-31b7a5b9b240.png)\n\n![Screen Shot 2021-06-13 at 11.18.05 PM.png](attachment:266a35fc-1390-4652-965c-a2095a6fac84.png)\n\nThe key things to note here are:\n* Grouping by id, because it's a grouped time series\n* Use of shift to make the lag\n* Use of .mean and .std to make aggregated lags\n* This particular snippet code is using a range and loop to make simple lags for 1-8 days, and aggregated lags for 14, 30, and 60 days","77e334e3":"### Notes on the resulting cleaned data\nAfter doing some processing in [this notebook that I wrote](https:\/\/www.kaggle.com\/beezus666\/end-to-end-data-wrangling-simple-random-forest) the data winds up in a format that can be used by an ML model. This notebook is a good example of fundamental data cleaning and preperation techniques taught in these courses:\n* https:\/\/www.kaggle.com\/learn\/pandas\n* https:\/\/www.kaggle.com\/learn\/data-cleaning\n* https:\/\/course18.fast.ai\/ml.html\n * This Fast.ai course is my personal favorite... it is long but from just the first few lessons it will give a mastery of data cleaning and tree-based models.\n\n### Notes on the resulting submission\/model\nThe submission from the notebook ranked in the top 50%... not great, but not bad for just doing the absolute basics and minimum.\n* I did no feature engineering, simply taking the data as it was as and formatting it so it could be used in a model\n* The notebook used a very basic out of the box random forest, where it has been shown in the competition that lightGBM was the best choice.\n","59ba1842":"### The data starts like this:","2c03d2fe":"# Look at and clean the data\nThe sales training data was given in what looks like a form that is more suited to reporting rather than to useful for ML or to be stored in a database. This reporting format required that the data needed to be manipulated using melt and pivot functionalities found in Pandas. It also required other manipulaitons to convert non-numeric data to numeric data so that it can be used by ML models."}}