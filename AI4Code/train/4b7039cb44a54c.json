{"cell_type":{"a3890f8e":"code","c5779245":"code","42b33d98":"code","31793b50":"code","7d874d6e":"code","2abbad81":"code","fe813a76":"code","1ea8a583":"code","a6dfcf7a":"code","c68c9117":"code","32a3be42":"code","b94b00ed":"code","8d7576e3":"code","549a49e0":"code","85170a3c":"code","83e33590":"code","40713522":"code","29f1a95d":"code","dfbce4e5":"code","08fc5330":"markdown","e3fc9235":"markdown","60f84052":"markdown","8f23e550":"markdown","f096d832":"markdown","3afb6d41":"markdown","81a920e3":"markdown","74d28b10":"markdown","2698e232":"markdown","fbcdf0bf":"markdown"},"source":{"a3890f8e":"!nvidia-smi","c5779245":"import sys\nsys.path.append('..\/input\/pytorch-image-models\/pytorch-image-models-master')","42b33d98":"# Asthetics\nimport warnings\nimport sklearn.exceptions\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n\n# General\nfrom tqdm.auto import tqdm\nimport pandas as pd\nimport numpy as np\nimport os\nimport glob\nimport random\nimport cv2\nimport pickle\npd.set_option('display.max_columns', None)\n\n# Image Aug\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\n\n# Machine Learning\nfrom xgboost import XGBRegressor\n\n# Deep Learning\nimport torch\nimport torchvision\nimport timm\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# Random Seed Initialize\nRANDOM_SEED = 42\n\ndef seed_everything(seed=RANDOM_SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything()\n\n# Device Optimization\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n    \nprint(f'Using device: {device}')","31793b50":"csv_dir = '..\/input\/petfinder-pawpularity-score'\ntest_dir = '..\/input\/petfinder-pawpularity-score\/test'\nmodels_dir = '..\/input\/pawpularity-contest-models\/swin_large_patch4_window12_384_2'\n\ntest_file_path = os.path.join(csv_dir, 'test.csv')\nsample_sub_file_path = os.path.join(csv_dir, 'sample_submission.csv')\nprint(f'Test file: {test_file_path}')\nprint(f'Models path: {models_dir}')","7d874d6e":"test_df = pd.read_csv(test_file_path)\nsample_df = pd.read_csv(sample_sub_file_path)","2abbad81":"def return_filpath(name, folder):\n    path = os.path.join(folder, f'{name}.jpg')\n    return path","fe813a76":"test_df['image_path'] = test_df['Id'].apply(lambda x: return_filpath(x, folder=test_dir))","1ea8a583":"test_df.head()","a6dfcf7a":"params = {\n    'model': 'swin_large_patch4_window12_384',\n    'dense_features': ['Subject Focus', 'Eyes', 'Face', 'Near',\n                       'Action', 'Accessory', 'Group', 'Collage',\n                       'Human', 'Occlusion', 'Info', 'Blur'],\n    'pretrained': False,\n    'inp_channels': 3,\n    'im_size': 384,\n    'device': device,\n    'batch_size': 16,\n    'num_workers' : 2,\n    'out_features': 1,\n    'debug': False\n}","c68c9117":"if params['debug']:\n    test_df = test_df.sample(frac=0.1)","32a3be42":"def get_test_transforms(DIM = params['im_size']):\n    return albumentations.Compose(\n        [\n          albumentations.Resize(DIM,DIM),\n          albumentations.Normalize(\n              mean=[0.485, 0.456, 0.406],\n              std=[0.229, 0.224, 0.225],\n          ),\n          ToTensorV2(p=1.0)\n        ]\n    )","b94b00ed":"class CuteDataset(Dataset):\n    def __init__(self, images_filepaths, dense_features, targets, transform=None):\n        self.images_filepaths = images_filepaths\n        self.dense_features = dense_features\n        self.targets = targets\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images_filepaths)\n\n    def __getitem__(self, idx):\n        image_filepath = self.images_filepaths[idx]\n        image = cv2.imread(image_filepath)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        if self.transform is not None:\n            image = self.transform(image=image)['image']\n        \n        dense = self.dense_features[idx, :]\n        label = torch.tensor(self.targets[idx]).float()\n        return image, dense, label","8d7576e3":"class PetNet(nn.Module):\n    def __init__(self, model_name=params['model'], out_features=params['out_features'], inp_channels=params['inp_channels'],\n                 pretrained=params['pretrained'], num_dense=len(params['dense_features'])):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained, in_chans=inp_channels)\n        n_features = self.model.head.in_features\n        self.model.head = nn.Linear(n_features, 128)\n        self.fc = nn.Sequential(\n            nn.Linear(128 + num_dense, 64),\n            nn.ReLU(),\n            nn.Linear(64, out_features)\n        )\n        self.dropout = nn.Dropout(0.2)\n    \n    def forward(self, image, dense):\n        embeddings = self.model(image)\n        x = self.dropout(embeddings)\n        x = torch.cat([x, dense], dim=1)\n        output = self.fc(x)\n        return output","549a49e0":"for model_name in glob.glob(models_dir + '\/*.pth'):\n    model = PetNet()\n    model.load_state_dict(torch.load(model_name))\n    model = model.to(params['device'])\n    model.eval()\n\n    test_dataset = CuteDataset(\n        images_filepaths = test_df['image_path'].values,\n        dense_features = test_df[params['dense_features']].values,\n        targets = sample_df['Pawpularity'].values,\n        transform = get_test_transforms()\n    )\n    test_loader = DataLoader(\n        test_dataset, batch_size=params['batch_size'],\n        shuffle=False, num_workers=params['num_workers'],\n        pin_memory=True\n    )\n\n    temp_preds = None\n    with torch.no_grad():\n        for (images, dense, target) in tqdm(test_loader, desc=f'Predicting. '):\n            images = images.to(params['device'], non_blocking=True)\n            dense = dense.to(params['device'], non_blocking=True)\n            predictions = torch.sigmoid(model(images, dense)).to('cpu').numpy()*100\n            \n            if temp_preds is None:\n                temp_preds = predictions\n            else:\n                temp_preds = np.vstack((temp_preds, predictions))\n\n    test_df[model_name.split('\/')[-1].split('_')[-3]] = temp_preds","85170a3c":"dense_features = []\nfor model_name in glob.glob(models_dir + '\/*.pth'):\n    dense_features.append(model_name.split('\/')[-1].split('_')[-3])","83e33590":"test_pred_all = None\nfor regressor_name in glob.glob(models_dir + '\/*.bin'):\n    model = XGBRegressor()\n    model.load_model(regressor_name)\n    test_pred = model.predict(test_df[dense_features].values)\n    \n    if test_pred_all is None:\n        test_pred_all = test_pred\n    else:\n        test_pred_all += test_pred\n        \ntest_pred_all \/= (len(glob.glob(models_dir + '\/*.bin')))","40713522":"sub_df = pd.DataFrame()\nsub_df['Id'] = test_df['Id']\nsub_df['Pawpularity'] = test_pred_all","29f1a95d":"sub_df.head()","dfbce4e5":"sub_df.to_csv('submission.csv', index=False)","08fc5330":"# CNN Model","e3fc9235":"# CFG","60f84052":"# XGBoost","8f23e550":"# Get GPU Info","f096d832":"# About This Notebook\n\nThis implementation is based on a vanilla **swin_large_patch4_window12_384** in Pytorch for the Pawpularity Competition.  \nThis model uses **both images and dense features** for score prediction.  \nThis scores around 18.847 LB and 18.02 CV.\n\nTraining Params: -\n1. **Dataset**: - 3-channel RGB Images (384x384) with separate dense features\n2. **Augmentations**: - Resize, Normalize, HorizontalFlip, VerticalFlip, RandomBrightness, RandomResizedCrop, HueSaturationValue, RandomBrightnessContrast\n3. **Optimizer**: - AdamW\n4. **Scheduler**: - CosineAnnealingLR\n5. **Model**: - swin_large_patch4_window12_384\n6. **Initial Weights**: - Imagenet\n5. **Max Epochs**: - 8 (~22 min per epoch on P100 PCIE GPU)\n6. **Saved Weights**: - 10-fold ensemble. Weights having highest OOF score on RMSE metric were saved.\n\nThis notebook only contains the inference for the model as described above. If you are looking for the training notebook please follow the link below.\n\nBaseline Model Notebook:- https:\/\/www.kaggle.com\/manabendrarout\/pawpularity-score-starter-image-dense-train\n\n![SETI](https:\/\/www.petfinder.my\/images\/cuteness_meter.jpg)  \n\n**If you find this notebook useful and use parts of it in your work, please don't forget to show your appreciation by upvoting this kernel. That keeps me motivated and inspires me to write and share these public kernels. \ud83d\ude0a**","3afb6d41":"# Dataset","81a920e3":"# Augmentations","74d28b10":"**If you find this notebook useful and use parts of it in your work, please don't forget to show your appreciation by upvoting this kernel. That keeps me motivated and inspires me to write and share these public kernels. \ud83d\ude0a**","2698e232":"# Import","fbcdf0bf":"# CNN Prediction"}}