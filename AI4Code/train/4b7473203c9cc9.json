{"cell_type":{"60d9b492":"code","371041c0":"code","95c33d79":"code","30441bd6":"code","3d165e14":"code","08fc28e4":"code","1277a66b":"code","fbaa1ca0":"code","30953a22":"code","49ea264a":"code","5935034b":"code","3ef1659c":"code","bc51db3b":"code","c5a2a0de":"code","e6ab50b5":"code","b0b7d900":"code","1bac7b5d":"code","dfb4057d":"code","14224a2e":"code","2b6f6f90":"code","ba5126ac":"code","ad035609":"code","187d5cac":"code","92bb805a":"code","35fff9b9":"code","519de678":"code","bdd286a5":"code","fc048567":"code","f833a0e1":"code","893353d3":"code","7beae9aa":"code","5f5350cc":"code","6c4d7ca6":"code","e6643efb":"code","0e8b0d43":"code","73ba7292":"code","260fe2bb":"code","9351b613":"code","c0bdac47":"code","383b4d31":"code","cf9b50da":"code","10dc4365":"code","1fd45ee0":"code","9c6a5c59":"code","4ca68cf7":"code","e4c7d49f":"code","86d26e99":"code","17d35c7c":"code","cc266d65":"code","29f7792c":"code","9b0796d9":"code","15bedbdd":"code","b0e45026":"code","efecc666":"code","4b8d7c07":"code","5d46b3f7":"code","f7024ffc":"code","ecd94f1f":"code","0966c5b2":"code","4520045d":"code","18f1014d":"code","a26d2db3":"code","973a4d9d":"code","a1ca882b":"code","a61a6bd9":"code","ead1fc54":"code","7b458c61":"code","c9f222ce":"code","81adadea":"code","d9a0cde0":"markdown","67dff966":"markdown","bdadf821":"markdown","1df0aa87":"markdown","fd9624df":"markdown","5b6eb24e":"markdown","66e01819":"markdown","c76805e9":"markdown","3512d636":"markdown","711549e9":"markdown","132cf50b":"markdown","fe3e13b9":"markdown","1278d2e4":"markdown","f816ac1c":"markdown","08205280":"markdown","d4918a48":"markdown","30eb5850":"markdown","0c772db3":"markdown","bf922113":"markdown","552c6f5b":"markdown","b734f572":"markdown","2a590861":"markdown","daf0b4e5":"markdown","9cf3fe90":"markdown","421eeff7":"markdown","f569b723":"markdown","3af3f2db":"markdown","74ce6e20":"markdown","93045762":"markdown","d1311796":"markdown","995e061d":"markdown","6dc55020":"markdown"},"source":{"60d9b492":"import pandas as pd","371041c0":"data_path = \"..\/input\/ted_main.csv\"\ndata = pd.read_csv(data_path)","95c33d79":"data.head()","30441bd6":"data = data[['description', 'main_speaker', 'name']]","3d165e14":"data.head()","08fc28e4":"from nltk import WordPunctTokenizer","1277a66b":"tokenizer = WordPunctTokenizer()","fbaa1ca0":"descriptions = [tokenizer.tokenize(description.lower()) for description in data[\"description\"]]","30953a22":"print(descriptions[0])","49ea264a":"from gensim import corpora","5935034b":"corpora_dict = corpora.Dictionary(descriptions)","3ef1659c":"print(corpora_dict.token2id)","bc51db3b":"for token, token_id in corpora_dict.token2id.items():\n    corpora_dict.id2token[token_id] = token","c5a2a0de":"print(corpora_dict.id2token)","e6ab50b5":"len(corpora_dict)","b0b7d900":"new_doc = \"Save trees in sake of ecology!\"\nnew_vec = corpora_dict.doc2bow(tokenizer.tokenize(new_doc.lower()))\nprint(new_vec)\n\nfor word_id, _ in new_vec:\n    print(corpora_dict.id2token[word_id], end=' ')","1bac7b5d":"corpus = [corpora_dict.doc2bow(text) for text in descriptions]","dfb4057d":"print(corpus[0])","14224a2e":"from gensim import similarities","2b6f6f90":"index_bow = similarities.SparseMatrixSimilarity(corpus, num_features=len(corpora_dict))","ba5126ac":"def search(index, query, top_n=5, prints=False):\n    \"\"\"\n    This function searches the most similar texts to the query.\n        :param index: gensim.similarities object\n        :param query: a string\n        :param top_n: how many variants it returns\n        :param prints: if True returns the results, otherwise prints the results\n        :returns: a list of tuples (matched_document_index, similarity_value)\n    \"\"\"\n    # getting a BoW vector\n    bow_vec = corpora_dict.doc2bow(query.lower().split())\n    similarities = index[bow_vec]  # get similarities between the query and all index documents\n    similarities = [(x, i) for i, x in enumerate(similarities)]\n    similarities.sort(key=lambda elem: -elem[0])  # sorting by similarity_value in decreasing order\n    res = []\n    if prints:\n        print(f\"{query}\\n\")\n    for result in similarities[:top_n]:\n        if prints:\n            print(f\"{data['description'][result[1]]} \\t {result[0]}\\n\")\n        else:\n            res.append((result[1], result[0]))\n    if not prints:\n        return res","ad035609":"search(index_bow, \"education system\", prints=True)","187d5cac":"search(index_bow, \"healthy food\", prints=True)","92bb805a":"search(index_bow, \"In an emotionally charged talk\", prints=True)","35fff9b9":"search(index_bow, \"Majora Carter: Greening the ghetto\", prints=True)","519de678":"from gensim.models import TfidfModel","bdd286a5":"model_tfidf = TfidfModel(corpus)","fc048567":"vector = model_tfidf[corpus[0]]","f833a0e1":"print(vector)","893353d3":"corpus_tfidf = model_tfidf[corpus]","7beae9aa":"index_tfidf = similarities.SparseMatrixSimilarity(corpus_tfidf, num_features=len(corpora_dict))","5f5350cc":"search(index_tfidf, \"Majora Carter: Greening the ghetto\", prints=True)","6c4d7ca6":"data[data[\"main_speaker\"] == \"Majora Carter\"]","e6643efb":"from scipy.sparse import coo_matrix","0e8b0d43":"i_inds = []\nj_inds = []\ndata_ij_values = []\n\nfor i_ind, sparse_doc in enumerate(corpus):\n    for j_ind, data_ij in sparse_doc:\n        i_inds.append(i_ind)\n        j_inds.append(j_ind)\n        data_ij_values.append(data_ij)\nsparse_corpus = coo_matrix((data_ij_values, (i_inds, j_inds)))\nfull_corpus = sparse_corpus.toarray()","73ba7292":"sparse_corpus","260fe2bb":"full_corpus","9351b613":"import numpy as np\nimport scipy.linalg as la","c0bdac47":"full_corpus = full_corpus.T","383b4d31":"U, s, Vt = la.svd(full_corpus)","cf9b50da":"print(U.shape, s.shape, Vt.shape)","10dc4365":"import matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline","1fd45ee0":"plt.figure(figsize=(16,10))\nplt.plot(np.arange(1, s.shape[0] + 1), s, label=\"singular values\")","9c6a5c59":"rank_svd = 250\n\nU_trunced = U[:, :rank_svd]\ns_trunced = s[:rank_svd]\nVt_trunced = Vt[:rank_svd, :]","4ca68cf7":"print(U_trunced.shape, s_trunced.shape, Vt_trunced.shape)","e4c7d49f":"corpus_lsa = U_trunced.dot(np.diag(s_trunced)).dot(Vt_trunced)","86d26e99":"corpus_lsa.shape","17d35c7c":"corpus_lsa[0]","cc266d65":"corpus_lsa = corpus_lsa.T","29f7792c":"index_lsa_bow = similarities.MatrixSimilarity(corpus_lsa, num_features=len(corpora_dict))","9b0796d9":"search(index_lsa_bow, \"healthy food\", prints=True)","15bedbdd":"from gensim.models import LsiModel","b0e45026":"model_lsi = LsiModel(corpus, id2word=corpora_dict.id2token, num_topics=rank_svd)","efecc666":"model_lsi.print_topics(5)","4b8d7c07":"for i in range(rank_svd):\n    print(i, model_lsi.projection.s[i], s_trunced[i], np.allclose(model_lsi.projection.s[i], s_trunced[i]))","5d46b3f7":"corpus_lsi = model_lsi[corpus]","f7024ffc":"len(corpus_lsi), len(corpus_lsi[0])","ecd94f1f":"index_lsi_bow = similarities.MatrixSimilarity(corpus_lsi, num_features=len(corpora_dict))","0966c5b2":"search(index_lsi_bow, \"education system\", prints=True)","4520045d":"search(index_lsi_bow, \"healthy food\", prints=True)","18f1014d":"class MyDictionary():\n    def __init__(tokenized_texts):\n        self.token2id = dict()\n        self.id2token = dict()\n        # YOUR CODE HERE    \n    def doc2bow(tokenized_text):\n        # YOUR CODE HERE\n        return # YOUR CODE HERE","a26d2db3":"test_corpus = [['hello', 'world'], ['hello']]\nmy_dictionary = MyDictionary(test_corpus)\nfor word in {'hello', 'world'}:\n    assert word in my_dictionary.token2id\n    assert my_dictionary.token2id[word] = my_dictionary.id2token[my_dictionary.token2id[word]]\nmy_test_corpus_bow = [my_dictionary.doc2bow(text) for text in test_corpus] \ntest_corpus_bow = [[(0, 1), (1, 1)], [(0, 1)]]\nassert my_test_corpus_bow == test_corpus_bow","973a4d9d":"# You may need regular expressions to check tokens on being real 'words'\nimport re\n\n# YOUR CODE HERE\n\nclean_corpus = [] # YOUR CODE HERE","a1ca882b":"import bokeh.models as bm, bokeh.plotting as pl\nfrom bokeh.io import output_notebook\noutput_notebook()\n\ndef draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n                 width=600, height=400, show=True, **kwargs):\n    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n    if isinstance(color, str): color = [color] * len(x)\n    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n\n    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n\n    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n    if show: pl.show(fig)\n    return fig","a61a6bd9":"from sklearn.decomposition import PCA\nfrom sklearn import preprocessing\n\nword_vectors_pca = PCA(n_components=2, random_state=4117).fit_transform(full_corpus)  # insert TF-IDF vectors here\nword_vectors_pca = preprocessing.scale(word_vectors_pca)","ead1fc54":"period = 50  # you can use 10 or 25 if it's ok for your computer\n\nwords = [corpora_dict.id2token[i] for i in range(len(corpora_dict))][::period]\ndraw_vectors(word_vectors_pca[:, 0][::period], word_vectors_pca[:, 1][::period], token=words)","7b458c61":"from sklearn.manifold import TSNE","c9f222ce":"word_tsne = TSNE(n_components=2, verbose=100).fit_transform(full_corpus[::period])","81adadea":"draw_vectors(word_tsne[:, 0], word_tsne[:, 1], color='green', token=words)","d9a0cde0":"# Homework (10 points)","67dff966":"To make a tokenization, we can simply use tokenizers from nltk.","bdadf821":"The other way of projecting high-dimentional data on a 2D plain is t-SNE.","1df0aa87":"## BoW search machine","fd9624df":"## Deleting stopwords (4 points)\n\nIn this task, you will clear our text corpur from stopwords and non-words like ',', '!)' etc. After that, build a new BoW and TF-IDF models. Make several queries to old and new systems and compare tre results. Did deleting stopwords really increased a quality of the search?","5b6eb24e":"Now, it's time to use dense vectors instead of sparse ones.","66e01819":"Now let's do it with all texts.","c76805e9":"Now let's look at the BoW representation of an arbitrary sentense.","3512d636":"Here you can run experiments on word similarity measurement.","711549e9":"Back to documents.","132cf50b":"By default, id2token is empty. Let's fill this dictionary.","fe3e13b9":"sparse_corpus and full_corpus are matrices with sizes $N_{documents} \\times V$ where $V = len(vocabulary)$","1278d2e4":"Much better! Now that we have Majora Carter talks in the top of the results. How many talks did she have?","f816ac1c":"In this seminar, we will learn how to build the simples count-based models for information retrieval tasks. We will ide a TED Talks dataset from Kaggle (https:\/\/www.kaggle.com\/rounakbanik\/ted-talks).","08205280":"## TF-IDF model","d4918a48":"We want to work with words as rows, so we have to transpose the matrix.","30eb5850":"Great! But what about searching by an annotation?","0c772db3":"## Doing SVD \/ LSA with your own hands\n\nThis approach has a lot of names but it's main idea is quite simple: we try to approximate out source matrix by matrix of a lower rank. In this task, we will use original BoW matrix.","bf922113":"Seems like it works. But can our system search texts by citations?","552c6f5b":"Now we can choose how many singular values (s) we will take to approximate an original matrix.","b734f572":"## LSI\nIt is almost the same that we did in the previous section but this time we will used a built-in function.","2a590861":"## Reading data\nAt this step, we will open a dataset and select two text columns from it.","daf0b4e5":"# NSU Distributional Semantics 2019 Course. Seminar 2","9cf3fe90":"Here we're gonna use the default Dictionary function (but you can implement your own converter if you wish).","421eeff7":"## Visualizing word embeddings (4 points)\n\nGiven the example of visualizing BoW vectors on a 2D-plain, plot the same graphs for TF-IDF model without stopwords. Does distributional hipothesis work here? Explain your answer.","f569b723":"Can you explain why do we have zeros here?","3af3f2db":"Seems like our tagret document is not in top-5 results.\n\nOn the next step, we will make more 'smart' model, TF-IDF model. ","74ce6e20":"## Preparing data for processing\nAt this step, we are going to prepare our text data. Preparation includes tokenization and stop-words filtering. Today we will omit the second step. We will also be working only with descriptions.","93045762":"# Conclusion\n\nTell what have you learned from this seminar.","d1311796":"## Converting texts to a Bag-of-Words format","995e061d":"## Your own Dictionary (2 points)\n\nImplement a class analogous to corpora.Dictionary.","6dc55020":"Now it's time to make a simple search machine."}}