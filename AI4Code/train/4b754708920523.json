{"cell_type":{"e7a047ff":"code","b59001bd":"code","f53d06b4":"code","d43b9c54":"code","5c816c37":"code","e0e873aa":"code","08acf233":"code","5989b6e0":"code","d36639bb":"code","d7a24dff":"code","1eec1dfb":"code","7b83f95d":"code","22e5cd46":"code","3ecff688":"code","4ddd1246":"code","5e922c4c":"code","680b373c":"code","b1f63645":"code","5a4311a5":"code","7199029e":"code","cf4e66df":"code","f16b765e":"code","e6981731":"code","1f4d8d6c":"code","5c279f8f":"code","404dbea3":"code","3a45747e":"code","30522721":"code","86e41a8b":"code","4ed89e1e":"code","06594761":"code","25d36801":"code","293e5b62":"code","900f6287":"code","80ead767":"code","406a0e9d":"code","ad8a639e":"code","952227f3":"code","dd625c26":"code","9243f450":"code","cd4f549d":"code","1a0cc77c":"code","93709295":"code","cc7f0784":"markdown","4f98a2c2":"markdown","cdd75d56":"markdown","fd08518c":"markdown","07f24c93":"markdown","340fb9ff":"markdown","8852e3be":"markdown","c10a073e":"markdown","9cfae0e9":"markdown","e9b1c01e":"markdown","eaa91441":"markdown","93201772":"markdown","365747a6":"markdown","b4bcbd68":"markdown","58fd742e":"markdown","4d481438":"markdown","4eca5b26":"markdown","d2ffa810":"markdown","d2575cf3":"markdown","edc51479":"markdown","aaad1699":"markdown","dad1cc71":"markdown","eead5693":"markdown","91fab661":"markdown","7ea0f63f":"markdown","462bd37d":"markdown","48c45ab2":"markdown","821e0230":"markdown"},"source":{"e7a047ff":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","b59001bd":"train = pd.read_csv('..\/input\/mobile-price-classification\/train.csv')\ntest = pd.read_csv('..\/input\/mobile-price-classification\/test.csv',index_col='id')\ntrain.head()","f53d06b4":"from load_kaggle import load_kaggle","d43b9c54":"list_of_dfs = load_kaggle()","5c816c37":"train, test = list_of_dfs","e0e873aa":"test.info()","08acf233":"!pip install autoviml","5989b6e0":"train.info()","d36639bb":"train.loc[train['px_height'] == 0,'px_height'] = np.nan\ntrain.loc[train['sc_w'] == 0,'sc_w'] = np.nan\n\ntest.loc[test['px_height'] == 0,'px_height'] = np.nan\ntest.loc[test['sc_w'] == 0,'sc_w'] = np.nan","d7a24dff":"value_counts = train['price_range'].value_counts()\nplt.pie(value_counts.values, labels = value_counts.index, autopct='%1.1f%%', startangle=90)\nplt.show()","1eec1dfb":"fig, ax = plt.subplots(5, 4,figsize=(19,19))\nfor i, col in enumerate(train.iloc[:,:-1]):\n    sns.histplot(x=col, hue = 'price_range', data=train,ax =ax[i\/\/4][i%4], multiple = 'stack')","7b83f95d":"plt.figure(figsize=(16,16))\nmat = train.corr()\nsns.heatmap(mat, annot=True)","22e5cd46":"train['px_height'].fillna(train['px_height'].median(), inplace=True)\ntest['px_height'].fillna(test['px_height'].median(), inplace=True)\n\ntrain['sc_w'].fillna(train['sc_w'].median(), inplace=True)\ntest['sc_w'].fillna(test['sc_w'].median(), inplace=True)","3ecff688":"main_cols = ['battery_power', 'px_height','px_width', 'ram']\ntrain_main = train[main_cols+['price_range']]\ntest_main = test[main_cols]","4ddd1246":"train['has_pc'] = (train['pc'] != 0).astype(np.int8)\ntest['has_pc'] = (test['pc'] != 0).astype(np.int8)\n\ntrain['has_fc'] = (train['fc'] != 0).astype(np.int8)\ntest['has_fc'] = (test['fc'] != 0).astype(np.int8)","5e922c4c":"train['px_count'] = train['px_width']*train['px_height']\ntest['px_count'] = test['px_width']*test['px_height']\n\ntrain['sc_area'] = train['sc_w']*train['sc_h']\ntest['sc_area'] = test['sc_w']*test['sc_h']\n\ntrain['dp'] = train['px_count']\/train['sc_area'] \ntest['dp'] = test['px_count']\/test['sc_area']\n\ntrain['sc_ratio'] = train['sc_h']\/train['sc_w']\ntest['sc_ratio'] = test['sc_h']\/test['sc_w']\n\ntrain['px_ratio'] = train['px_height']\/train['px_width']\ntest['px_ratio'] = test['px_height']\/test['px_width']","680b373c":"plt.figure(figsize=(16,16))\nmat = train[['px_ratio','sc_area','sc_ratio', 'px_count','has_pc','has_fc','dp','price_range']].corr()\nsns.heatmap(mat, annot=True)","b1f63645":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n","5a4311a5":"X_full = train.drop(['price_range'], axis = 1)\ny_full = train['price_range']","7199029e":"cat_cols = ['blue', 'dual_sim', 'four_g', 'three_g', 'touch_screen', 'wifi','has_pc', 'has_fc']\nnum_cols =  list(X_full.drop(cat_cols, axis = 1).columns)\n\n\nscaler = StandardScaler()\nX_full[num_cols] = scaler.fit_transform(X_full[num_cols])\ntest[num_cols] = scaler.transform(test[num_cols])\n","cf4e66df":"X_train, X_val, y_train, y_val = train_test_split(X_full, y_full,stratify=y_full, random_state=42,test_size=0.25)","f16b765e":"from autoviml.Auto_ViML import Auto_ViML","e6981731":"train_1 = X_train.join(y_train)\nvalid_1 = X_val.join(y_val)","1f4d8d6c":"target = 'price_range'","5c279f8f":"m, feats, trainm, testm = Auto_ViML(train_1, target, valid_1,\n                            sample_submission='',\n                            scoring_parameter='', KMeans_Featurizer=False,\n                            hyper_param='RS',feature_reduction=True,\n                             Boosting_Flag=\"CatBoost\", Binning_Flag=False,\n                            Add_Poly=0, Stacking_Flag=False,Imbalanced_Flag=False,\n                            verbose=1)","404dbea3":"models = [m, LogisticRegression(),RandomForestClassifier(),DecisionTreeClassifier(), KNeighborsClassifier()]\nscores = []\nmodel_names = []\nfor model in models:\n    scores.append(np.mean(cross_val_score(model, X_train, y_train, n_jobs=3, verbose=2, cv=5)))\n    model_names.append(model.__class__.__name__)","3a45747e":"plt.figure(figsize=(10,8))\ng=sns.barplot(model_names, scores)\nax=g\nfor p in ax.patches:\n             ax.annotate(\"%.2f\" % p.get_height(), (p.get_x() + p.get_width() \/ 2., p.get_height()),\n                 ha='center', va='center', fontsize=11,  xytext=(0, 10),\n                 textcoords='offset points')","30522721":"print(classification_report(valid_1[target].values, testm[target+'_predictions'].values))","86e41a8b":"params = [{'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']    ,\n            'penalty':['none'],\n            },\n          { 'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n           'penalty':['l1'],\n              'C':[0.001,0.003,0.01,0.03,0.1,0.3,1,3,10,30,100]},\n          {'solver':['liblinear'],\n           'penalty':['l1','l2'],\n            'C':[0.001,0.003,0.01,0.03,0.1,0.3,1,3,10,30,100]}\n         ]\nlog_reg_cv = GridSearchCV(LogisticRegression(max_iter=10000), param_grid=params, cv = 5, verbose = 2, n_jobs = -1).fit(X_train, y_train)","4ed89e1e":"log_reg_model = log_reg_cv.best_estimator_\nprint(classification_report(y_val, log_reg_model.predict(X_val)))","06594761":"params = {'n_estimators':np.arange(100,2001,100),\n         'max_depth':np.arange(3,31,2)}\nrfc_cv = GridSearchCV(RandomForestClassifier(), param_grid = params, cv =5, verbose = 2, n_jobs = -1).fit(X_train, y_train)\nrfc_cv.best_score_","25d36801":"log_reg_model = log_reg_cv.best_estimator_\ny_pred = log_reg_model.predict(X_val)\nmat = confusion_matrix(y_val, y_pred)\nsns.heatmap(mat, annot=True,fmt='1')","293e5b62":"log_reg_model.fit(X_full, y_full)\ntest_pred = log_reg_model.predict(test)","900f6287":"submission = pd.DataFrame({'id':test.index,\n                          'class':test_pred})\nsubmission.to_csv('submission.csv', index=False)","80ead767":"train_main.shape, test_main.shape","406a0e9d":"train_main['px_count'] = train_main['px_width']*train_main['px_height']\ntest_main['px_count'] = test_main['px_width']*test_main['px_height']\ntrain_main['px_ratio'] = train_main['px_height']\/train_main['px_width']\ntest_main['px_ratio'] = test_main['px_height']\/test_main['px_width']","ad8a639e":"X_main_full = train_main.drop(['price_range'],axis = 1)\ny_main_full = train_main['price_range']\n\nscaler = StandardScaler()\nX_main_full = scaler.fit_transform(X_main_full)\ntest_main = scaler.fit_transform(test_main)\n\nX_main_train, X_main_val, y_main_train, y_main_val = train_test_split(X_main_full, y_main_full,stratify=y_main_full, random_state=42,test_size=0.25)","952227f3":"models = [LogisticRegression(),RandomForestClassifier(),DecisionTreeClassifier(), KNeighborsClassifier()]\nscores = []\nmodel_names = []\nfor model in models:\n    scores.append(np.mean(cross_val_score(model, X_main_train, y_main_train, n_jobs=3, verbose=2, cv=5)))\n    model_names.append(model.__class__.__name__)\n    ","dd625c26":"plt.figure(figsize=(10,8))\ng=sns.barplot(model_names, scores)\nax=g\nfor p in ax.patches:\n             ax.annotate(\"%.2f\" % p.get_height(), (p.get_x() + p.get_width() \/ 2., p.get_height()),\n                 ha='center', va='center', fontsize=11,  xytext=(0, 10),\n                 textcoords='offset points')","9243f450":"params = {'n_neighbors':range(1,20)}\nknn_cv = GridSearchCV(KNeighborsClassifier(),param_grid = params, cv =5, verbose = 2, n_jobs = -1).fit(X_main_train, y_main_train)\nprint(\"Best score:\",knn_cv.best_score_)","cd4f549d":"params = [{'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']    ,\n            'penalty':['none'],\n            },\n          { 'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n           'penalty':['l1'],\n              'C':[0.001,0.003,0.01,0.03,0.1,0.3,1,3,10,30,100]},\n          {'solver':['liblinear'],\n           'penalty':['l1','l2'],\n            'C':[0.001,0.003,0.01,0.03,0.1,0.3,1,3,10,30,100]}\n         ]\nlog_reg_cv = GridSearchCV(LogisticRegression(max_iter=10000), param_grid=params, cv = 5, verbose = 2, n_jobs = -1).fit(X_main_train, y_main_train)","1a0cc77c":"log_reg_main_model = log_reg_cv.best_estimator_\nlog_reg_main_model.score(X_main_val, y_main_val)","93709295":"params = {'n_estimators':np.arange(100,301,100),\n         'max_depth':np.arange(7,31,2),\n          \"criterion\": [\"gini\", \"entropy\"]}\nrfc_cv = GridSearchCV(RandomForestClassifier(), param_grid = params, cv =5, verbose = 2, n_jobs = -1).fit(X_main_train, y_main_train)\nrfc_cv.best_score_","cc7f0784":"# Holy cow! the Logistic Regression Model beats CatBoost!","4f98a2c2":"Screen area and pixel count (`px_width * px_height`) can also be useful","cdd75d56":"Extracting \"main\" columns for further use","fd08518c":"Looks like there are no null values in any of the sets but there can be 0s in some columns which can actually be missing data.","07f24c93":"I wrote that I will check the main (columns with high correlation with target) features. Let's do that","340fb9ff":"Since it's classification task, we should split data evenly, so we pass y_full as stratify","8852e3be":"# This notebook is derived from the notebook below.\nPlease upvote this notebook if you liked this one. I am grateful to the author!\nhttps:\/\/www.kaggle.com\/tuqayabdullazade\/97-8-accuracy-on-validation-set","c10a073e":"# Comparatively Speaking, A highly tuned Logistic Regression Does Slightly better than CatBoost on this dataset","9cfae0e9":" + Correlation between ram and price_range is too high as expected. \n + Screen Height and Screen Width correlation is understandable as phones are created at similar screen ratios. Same thing goes for pixel height and pixel width.\n + There is a little correlation between price range and battery power, pixel height, pixel width.\n + Contrary to my assumptions clock speed and internal memory has no correlation with price range\n + Correlation between 3G and 4G is also high\n + One last thing, primary camera and front camera has similar correlation. \n + The other columns have too little or no correlation between them. I will train model with and without them and compare results.\n \n Let's fill missing values with median","e9b1c01e":"Dataset is balanced, which makes things easier, let's look at distribution of features with regard to class","eaa91441":"Looks like other the main characteristics of a mobile phone is its ram, distribution of price range says so. Other key features which makes visible difference are battery_power, px_width, px_height. Let's look at correlation matrix","93201772":"There has been great improvement in KNeighborsClassifier. Let's fine-tune it. It would be better if I used knife method, but now I just want to decide if KNeighborsClassifier can come near to LogisticRegression.","365747a6":"# Modelling","b4bcbd68":"px_height, sc_w has minimum value of 0, which is of course missing values, let's mark them as nan to make things easier","58fd742e":"# Let's predict on the Validation Dataset and see how Auto_ViML performs compared to others - It does about 95% Macro Accuracy","4d481438":"Let's search for best parameters for Logistic Regression and Random Forest Classifier","4eca5b26":"Looks like new features won't do anything too important, but let's keep them where they are","d2ffa810":"Logistic Regression outperformed Random Forest Classifier again. With only 6 features we were able to get 96.8% which is only 1% less than accuracy of model trained with all features. Even though accuracy is less than previous model, we can train our models much faster now and if we had more samples in our train set, we could see the difference really easy. I will stop here and consider Logistic Regression best model for this task. If you think this notebook is useful, please upvote. If you have any suggestions, please do write them in comments. so that I can improve myself ","d2575cf3":"There are no categorical variable's in this \"new\" dataset, so we can scale dataset easily","edc51479":"Random Forest Classifier didn't show any significant improvement, but Logistic Regression gets 97.8% accuracy on the validation set which I think is pretty good. Let's look at confusion matrix","aaad1699":"# Let's now compare the resulting Auto_ViML model to other models","dad1cc71":"Front camera and primary camera having 0 megapixels means phone doesn't have camera at that side, we can create two extra features from that.","eead5693":"# Main columns","91fab661":"It misclassified only 13 mobile phones wrong, I think it is good enough to predict test dataset ","7ea0f63f":"# Loading data","462bd37d":"We will repeat some steps from before","48c45ab2":"Looks like not","821e0230":"There are categorical variables in data but they can have only 2 values (True, False), so there is no need for one-hot encoding. Linear models work better with scaled values, so we will scale them. But we have to be careful not to scale categorical columns"}}