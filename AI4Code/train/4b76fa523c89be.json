{"cell_type":{"cc598c01":"code","592ca8f8":"code","7aa98dd6":"code","08336b0b":"code","82c892ad":"code","d6aee6e1":"code","3eee761d":"code","04e96cfd":"code","4c81d497":"code","5f9d4228":"code","a575ac42":"code","53f65b35":"code","3428ebe6":"code","b5f061e3":"code","7f15d80e":"code","d24ac1a3":"code","98e46114":"code","7054f4f0":"code","7861fff2":"code","6abe1cf1":"markdown","36ffc711":"markdown","d3bf2c37":"markdown","01e2f182":"markdown","72d52b36":"markdown","0db1441f":"markdown","ee215e5b":"markdown","9b66d9f0":"markdown","a9aeff95":"markdown","2a0df6e8":"markdown","c4098eee":"markdown","9b21d571":"markdown","b81cad73":"markdown","83a3d281":"markdown"},"source":{"cc598c01":"import os\nimport cv2\nimport time\nimport random\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ntf.__version__, np.__version__","592ca8f8":"# Setting random seeds to enable consistency while testing.\nrandom.seed(5)\nnp.random.seed(5)\ntf.random.set_seed(5)\n\nROOT = \"..\/input\/face-recognition-dataset\/Extracted Faces\/Extracted Faces\"\n\ndef read_image(index):\n    path = os.path.join(ROOT, index[0], index[1])\n    image = cv2.imread(path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    return image","7aa98dd6":"def split_dataset(directory, split=0.9):\n    folders = os.listdir(directory)\n    num_train = int(len(folders)*split)\n    \n    random.shuffle(folders)\n    \n    train_list, test_list = {}, {}\n    \n    # Creating Train-list\n    for folder in folders[:num_train]:\n        num_files = len(os.listdir(os.path.join(directory, folder)))\n        train_list[folder] = num_files\n    \n    # Creating Test-list\n    for folder in folders[num_train:]:\n        num_files = len(os.listdir(os.path.join(directory, folder)))\n        test_list[folder] = num_files  \n    \n    return train_list, test_list\n\ntrain_list, test_list = split_dataset(ROOT, split=0.9)\nprint(\"Length of training list:\", len(train_list))\nprint(\"Length of testing list :\", len(test_list))\n\n# train_list, test list contains the folder names along with the number of files in the folder.\nprint(\"\\nTest List:\", test_list)","08336b0b":"def create_triplets(directory, folder_list, max_files=20):\n    triplets = []\n    folders = list(folder_list.keys())\n    \n    for folder in folders:\n        path = os.path.join(directory, folder)\n        files = list(os.listdir(path))[:max_files]\n        num_files = len(files)\n        \n        for i in range(num_files-1):\n            for j in range(i+1, num_files):\n                anchor = (folder, f\"{i}.jpg\")\n                positive = (folder, f\"{j}.jpg\")\n\n                neg_folder = folder\n                while neg_folder == folder:\n                    neg_folder = random.choice(folders)\n                neg_file = random.randint(0, folder_list[neg_folder]-1)\n                negative = (neg_folder, f\"{neg_file}.jpg\")\n\n                triplets.append((anchor, positive, negative))\n            \n    random.shuffle(triplets)\n    return triplets","82c892ad":"train_triplet = create_triplets(ROOT, train_list)\ntest_triplet  = create_triplets(ROOT, test_list)\n\nprint(\"Number of training triplets:\", len(train_triplet))\nprint(\"Number of testing triplets :\", len(test_triplet))\n\nprint(\"\\nExamples of triplets:\")\nfor i in range(5):\n    print(train_triplet[i])","d6aee6e1":"def get_batch(triplet_list, batch_size=256, preprocess=True):\n    batch_steps = len(triplet_list)\/\/batch_size\n    \n    for i in range(batch_steps+1):\n        anchor   = []\n        positive = []\n        negative = []\n        \n        j = i*batch_size\n        while j<(i+1)*batch_size and j<len(triplet_list):\n            a, p, n = triplet_list[j]\n            anchor.append(read_image(a))\n            positive.append(read_image(p))\n            negative.append(read_image(n))\n            j+=1\n            \n        anchor = np.array(anchor)\n        positive = np.array(positive)\n        negative = np.array(negative)\n        \n        if preprocess:\n            anchor = preprocess_input(anchor)\n            positive = preprocess_input(positive)\n            negative = preprocess_input(negative)\n        \n        yield ([anchor, positive, negative])","3eee761d":"num_plots = 6\n\nf, axes = plt.subplots(num_plots, 3, figsize=(15, 20))\n\nfor x in get_batch(train_triplet, batch_size=num_plots, preprocess=False):\n    a,p,n = x\n    for i in range(num_plots):\n        axes[i, 0].imshow(a[i])\n        axes[i, 1].imshow(p[i])\n        axes[i, 2].imshow(n[i])\n        i+=1\n    break","04e96cfd":"from tensorflow.keras import backend, layers, metrics\n\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import Xception\nfrom tensorflow.keras.models import Model, Sequential\n\nfrom tensorflow.keras.utils import plot_model\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report","4c81d497":"def get_encoder(input_shape):\n    \"\"\" Returns the image encoding model \"\"\"\n\n    pretrained_model = Xception(\n        input_shape=input_shape,\n        weights='imagenet',\n        include_top=False,\n        pooling='avg',\n    )\n    \n    for i in range(len(pretrained_model.layers)-27):\n        pretrained_model.layers[i].trainable = False\n\n    encode_model = Sequential([\n        pretrained_model,\n        layers.Flatten(),\n        layers.Dense(512, activation='relu'),\n        layers.BatchNormalization(),\n        layers.Dense(256, activation=\"relu\"),\n        layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))\n    ], name=\"Encode_Model\")\n    return encode_model","5f9d4228":"class DistanceLayer(layers.Layer):\n    # A layer to compute \u2016f(A) - f(P)\u2016\u00b2 and \u2016f(A) - f(N)\u2016\u00b2\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def call(self, anchor, positive, negative):\n        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n        return (ap_distance, an_distance)\n    \n\ndef get_siamese_network(input_shape = (128, 128, 3)):\n    encoder = get_encoder(input_shape)\n    \n    # Input Layers for the images\n    anchor_input   = layers.Input(input_shape, name=\"Anchor_Input\")\n    positive_input = layers.Input(input_shape, name=\"Positive_Input\")\n    negative_input = layers.Input(input_shape, name=\"Negative_Input\")\n    \n    ## Generate the encodings (feature vectors) for the images\n    encoded_a = encoder(anchor_input)\n    encoded_p = encoder(positive_input)\n    encoded_n = encoder(negative_input)\n    \n    # A layer to compute \u2016f(A) - f(P)\u2016\u00b2 and \u2016f(A) - f(N)\u2016\u00b2\n    distances = DistanceLayer()(\n        encoder(anchor_input),\n        encoder(positive_input),\n        encoder(negative_input)\n    )\n    \n    # Creating the Model\n    siamese_network = Model(\n        inputs  = [anchor_input, positive_input, negative_input],\n        outputs = distances,\n        name = \"Siamese_Network\"\n    )\n    return siamese_network\n\nsiamese_network = get_siamese_network()\nsiamese_network.summary()","a575ac42":"plot_model(siamese_network, show_shapes=True, show_layer_names=True)","53f65b35":"class SiameseModel(Model):\n    # Builds a Siamese model based on a base-model\n    def __init__(self, siamese_network, margin=1.0):\n        super(SiameseModel, self).__init__()\n        \n        self.margin = margin\n        self.siamese_network = siamese_network\n        self.loss_tracker = metrics.Mean(name=\"loss\")\n\n    def call(self, inputs):\n        return self.siamese_network(inputs)\n\n    def train_step(self, data):\n        # GradientTape get the gradients when we compute loss, and uses them to update the weights\n        with tf.GradientTape() as tape:\n            loss = self._compute_loss(data)\n            \n        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n        self.optimizer.apply_gradients(zip(gradients, self.siamese_network.trainable_weights))\n        \n        self.loss_tracker.update_state(loss)\n        return {\"loss\": self.loss_tracker.result()}\n\n    def test_step(self, data):\n        loss = self._compute_loss(data)\n        \n        self.loss_tracker.update_state(loss)\n        return {\"loss\": self.loss_tracker.result()}\n\n    def _compute_loss(self, data):\n        # Get the two distances from the network, then compute the triplet loss\n        ap_distance, an_distance = self.siamese_network(data)\n        loss = tf.maximum(ap_distance - an_distance + self.margin, 0.0)\n        return loss\n\n    @property\n    def metrics(self):\n        # We need to list our metrics so the reset_states() can be called automatically.\n        return [self.loss_tracker]","3428ebe6":"siamese_model = SiameseModel(siamese_network)\n\noptimizer = Adam(learning_rate=1e-3, epsilon=1e-01)\nsiamese_model.compile(optimizer=optimizer)","b5f061e3":"def test_on_triplets(batch_size = 256):\n    pos_scores, neg_scores = [], []\n\n    for data in get_batch(test_triplet, batch_size=batch_size):\n        prediction = siamese_model.predict(data)\n        pos_scores += list(prediction[0])\n        neg_scores += list(prediction[1])\n    \n    accuracy = np.sum(np.array(pos_scores) < np.array(neg_scores)) \/ len(pos_scores)\n    ap_mean = np.mean(pos_scores)\n    an_mean = np.mean(neg_scores)\n    ap_stds = np.std(pos_scores)\n    an_stds = np.std(neg_scores)\n    \n    print(f\"Accuracy on test = {accuracy:.5f}\")\n    return (accuracy, ap_mean, an_mean, ap_stds, an_stds)","7f15d80e":"save_all = False\nepochs = 128\nbatch_size = 128\n\nmax_acc = 0\ntrain_loss = []\ntest_metrics = []\n\nfor epoch in range(1, epochs+1):\n    t = time.time()\n    \n    # Training the model on train data\n    epoch_loss = []\n    for data in get_batch(train_triplet, batch_size=batch_size):\n        loss = siamese_model.train_on_batch(data)\n        epoch_loss.append(loss)\n    epoch_loss = sum(epoch_loss)\/len(epoch_loss)\n    train_loss.append(epoch_loss)\n\n    print(f\"\\nEPOCH: {epoch} \\t (Epoch done in {int(time.time()-t)} sec)\")\n    print(f\"Loss on train    = {epoch_loss:.5f}\")\n    \n    # Testing the model on test data\n    metric = test_on_triplets(batch_size=batch_size)\n    test_metrics.append(metric)\n    accuracy = metric[0]\n    \n    # Saving the model weights\n    if save_all or accuracy>=max_acc:\n        siamese_model.save_weights(\"siamese_model\")\n        max_acc = accuracy\n\n# Saving the model after all epochs run\nsiamese_model.save_weights(\"siamese_model-final\")","d24ac1a3":"def plot_metrics(loss, metrics):\n    # Extracting individual metrics from metrics\n    accuracy = metrics[:, 0]\n    ap_mean  = metrics[:, 1]\n    an_mean  = metrics[:, 2]\n    ap_stds  = metrics[:, 3]\n    an_stds  = metrics[:, 4]\n    \n    plt.figure(figsize=(15,5))\n    \n    # Plotting the loss over epochs\n    plt.subplot(121)\n    plt.plot(loss, 'b', label='Loss')\n    plt.title('Training loss')\n    plt.legend()\n    \n    # Plotting the accuracy over epochs\n    plt.subplot(122)\n    plt.plot(accuracy, 'r', label='Accuracy')\n    plt.title('Testing Accuracy')\n    plt.legend()\n    \n    plt.figure(figsize=(15,5))\n    \n    # Comparing the Means over epochs\n    plt.subplot(121)\n    plt.plot(ap_mean, 'b', label='AP Mean')\n    plt.plot(an_mean, 'g', label='AN Mean')\n    plt.title('Means Comparision')\n    plt.legend()\n    \n    # Plotting the accuracy\n    ap_75quartile = (ap_mean+ap_stds)\n    an_75quartile = (an_mean-an_stds)\n    plt.subplot(122)\n    plt.plot(ap_75quartile, 'b', label='AP (Mean+SD)')\n    plt.plot(an_75quartile, 'g', label='AN (Mean-SD)')\n    plt.title('75th Quartile Comparision')\n    plt.legend()\n\ntest_metrics = np.array(test_metrics)\nplot_metrics(train_loss, test_metrics)","98e46114":"def extract_encoder(model):\n    encoder = get_encoder((128, 128, 3))\n    i=0\n    for e_layer in model.layers[0].layers[3].layers:\n        layer_weight = e_layer.get_weights()\n        encoder.layers[i].set_weights(layer_weight)\n        i+=1\n    return encoder\n\nencoder = extract_encoder(siamese_model)\nencoder.save_weights(\"encoder\")\nencoder.summary()","7054f4f0":"def classify_images(face_list1, face_list2, threshold=1.3):\n    # Getting the encodings for the passed faces\n    tensor1 = encoder.predict(face_list1)\n    tensor2 = encoder.predict(face_list2)\n    \n    distance = np.sum(np.square(tensor1-tensor2), axis=-1)\n    prediction = np.where(distance<=threshold, 0, 1)\n    return prediction","7861fff2":"def ModelMetrics(pos_list, neg_list):\n    true = np.array([0]*len(pos_list)+[1]*len(neg_list))\n    pred = np.append(pos_list, neg_list)\n    \n    # Compute and print the accuracy\n    print(f\"\\nAccuracy of model: {accuracy_score(true, pred)}\\n\")\n    \n    # Compute and plot the Confusion matrix\n    cf_matrix = confusion_matrix(true, pred)\n\n    categories  = ['Similar','Different']\n    names = ['True Similar','False Similar', 'False Different','True Different']\n    percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() \/ np.sum(cf_matrix)]\n\n    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(names, percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n\n    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n                xticklabels = categories, yticklabels = categories)\n\n    plt.xlabel(\"Predicted\", fontdict = {'size':14}, labelpad = 10)\n    plt.ylabel(\"Actual\"   , fontdict = {'size':14}, labelpad = 10)\n    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)\n\n\npos_list = np.array([])\nneg_list = np.array([])\n\nfor data in get_batch(test_triplet, batch_size=256):\n    a, p, n = data\n    pos_list = np.append(pos_list, classify_images(a, p))\n    neg_list = np.append(neg_list, classify_images(a, n))\n    break\n\nModelMetrics(pos_list, neg_list)","6abe1cf1":"### Encoder\n\nThe **Encoder** is responsible for converting the passed images into their feature vectors. We're using a pretrained model, **Xception model** which is based on **Inception_V3 model.** By using transfer learning, we can significantly reduce the training time and size of the dataset.\n\nThe Model is connected to **Fully Connected (Dense)** layers and the last layer normalises the data using **L2 Normalisation**. *(L2 Normalisation is a technique that modifies the dataset values in a way that in each row the sum of the squares will always be up to 1)*","36ffc711":"### Siamese Network\n\nWe're creating a **Siamese Network** that takes 3 input images, (anchor, postive, negative) and uses the encoder above to encode the images to their feature vectors. Those features are passed to a distance layer which computes the distance between **(anchor, positive)** and **(anchor, negative)** pairs.\n\nWe'll be defining a custom layer to compute the distance.\n\n**Distance Formula**:\n\n![image.png](attachment:5e83389c-697d-4b02-8b39-81f5eba0ba9e.png)","d3bf2c37":"## **Reading the Dataset**\n\nWe're reading the folders and splitting them into **train and test set** for training purposes.","01e2f182":"## **Using the Model**\n\nNow that we've finished training our model, we need to **extract the encoder** so that we can use it to encode images and then get use the feature vectors to compute the distance between those images.\n\nWe'll also be saving the encoder for latter use.","72d52b36":"### Creating Triplets\n\nWe use the train and test list to create triplets of **(anchor, postive, negative)** face data, where positive is the same person and negative is a different person than anchor.","0db1441f":"## References\n\n- FaceNet: A Unified Embedding for Face Recognition and Clustering: https:\/\/arxiv.org\/abs\/1503.03832\n- Image similarity estimation using a Siamese Network with a triplet loss: https:\/\/keras.io\/examples\/vision\/siamese_network\/\n- Celebrity Face Recognition: https:\/\/www.kaggle.com\/ravehgillmore\/celebrity-face-recognition\/\n- Face Recognition using Siamese Networks: https:\/\/medium.com\/wicds\/face-recognition-using-siamese-networks-84d6f2e54ea4\n","ee215e5b":"## **Training the Model**\n\nWe'll now be training the siamese_model on batches of triplets. We'll print the training loss, along with additional metrics from testing every epoch. The model weights will also be saved whenever it outperforms the previous max_accuracy.\n\nWe're hoping to collect more metrics about the model to evaluate how to increase the accuracy of the model. The epochs have been set to avoid going over Kaggle's time constraint.\n\n### Test Function\n\n**test_on_triplets()** function will be responsible for testing the model on test_triplets. It'll collect metrics **(accuracy, means, stds)** by predicting on the train data. We'll also be printing the Accuracy of the model after testing.","9b66d9f0":"### Classify Images\n\nTo compute the distance between the encodings of the images, we'll be using distance formula. Distance over a certain threshold to be \"different\" and below the threshold as \"same\".","a9aeff95":"## **Evaluating the Model**\n","2a0df6e8":"### Putting everything together\n\nWe now need to implement a model with custom training loop and loss function so we can compute the **triplet loss** using the three embeddings produced by the Siamese network.\n\nWe'll create a **Mean metric** instance to track the loss of the training process.\n\n**Triplet Loss Function:**\n\n<img src=\"https:\/\/miro.medium.com\/max\/1838\/0*AX2TSZNk19_gDgTN.png\" alt=\"Loss Formula\" width=\"400\"\/>","c4098eee":"## **Face Recognition**\n\n**Facial Recognition System** is a technology capable of matching a human face from a digital image or a video frame against a database of faces, typically employed to authenticate users through ID verification services, works by pinpointing and measuring facial features from a given image. \n\nWe'll be building a face recognition model that uses **Siamese Networks** to give us a distance value that indicates whether 2 images are same or different. \n\n#### **The Dataset**\nWe'll be using the **Extracted Faces** from **face-recognition-dataset**, which is derived from the **LFW Dataset**.\nThe Extracted Faces contains faces extracted from the base images using **Haar-Cascade Face-Detection** (CV2).\n- The dataset contains 1324 different individuals, with 2-50 images per person.\n- The images are of size (128,128,3) and are encoded in RGB.\n- Each folder and image is named with a number, i.e 0.jpg, 1.jpg","9b21d571":"## **Creating the Model**\n\nUnlike a conventional CNN, the **Siamese Network** does not classify the images into certain categories or labels, rather it only finds out the distance between any two given images. If the images have the same label, then the network should learn the parameters, i.e. the weights and the biases in such a way that it should produce a smaller distance between the two images, and if they belong to different labels, then the distance should be larger\n\n![Siamese Network Image](https:\/\/miro.medium.com\/max\/2000\/1*05hUCDHhnl4hdjqvdVTHtw.png)","b81cad73":"### Plotting the Data\n\nPlotting the data generated from **get_batch()** to see the results","83a3d281":"### Creating Batch-Generator\n\nCreating a **Batch-Generator** that converts the triplets passed into batches of face-data and **preproccesses** it before returning the data into seperate lists.\n\n**Parameters:**\n- Batch_size: Batch_size of the data to return\n- Preprocess: Whether to preprocess the data or not"}}