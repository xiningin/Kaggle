{"cell_type":{"f6005994":"code","10fe2a5f":"code","a154e2a5":"code","3a113d21":"code","9c3134d4":"code","6f2af61a":"code","5216a63b":"code","f91073d4":"code","9044975a":"code","e6924161":"code","6131257b":"code","e2137e1b":"code","c5de2dea":"code","31d6a63d":"code","9425a220":"code","5a310fd4":"code","2cc4414e":"code","bf6161fe":"code","b68fd2dd":"code","81790d34":"code","2e0bf33c":"code","95aced97":"code","b000a558":"code","519231bb":"code","760e79fe":"code","08adbbe6":"code","52ed5468":"code","05c0fcb4":"markdown","264665ba":"markdown","166eb728":"markdown","913001f2":"markdown","1605ef9d":"markdown","1f422a98":"markdown","1c369d31":"markdown","3807d7e1":"markdown","7a0a4b1c":"markdown","e77e11c0":"markdown","13f98260":"markdown","34b59c08":"markdown","a13f472a":"markdown","c7ba8b30":"markdown","8a02e44e":"markdown","465bb6a0":"markdown","a04e6be9":"markdown","3bcdfaa0":"markdown","1185f84b":"markdown","efa8f854":"markdown","2d956577":"markdown","84fde006":"markdown","6654a9ee":"markdown","6688da1e":"markdown","1f2dbd3e":"markdown","45534fa3":"markdown","12e5a86a":"markdown","866e43fd":"markdown","8ad658aa":"markdown"},"source":{"f6005994":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom collections import Counter\nfrom xgboost import XGBClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler as ss, MinMaxScaler as mms\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score, train_test_split\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","10fe2a5f":"df = pd.read_csv('..\/input\/credit-card-customers\/BankChurners.csv')","a154e2a5":"df = df.drop(['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1'], axis=1)\ndf = df.drop(['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'], axis=1)\ndf = df.drop('CLIENTNUM', axis=1)","3a113d21":"df","9c3134d4":"X = df.drop('Attrition_Flag', axis=1)\ny = df['Attrition_Flag']","6f2af61a":"fig, ax = plt.subplots(figsize=(7, 7))\ncount = Counter(y)\nax.pie(count.values(), labels=count.keys(), autopct=lambda p:f'{p:.2f}%')\nax.set_title('Percentage of existing and attrited customers')\nplt.show()","5216a63b":"fig, ax = plt.subplots(figsize=(15, 6))\ncount = Counter(X['Education_Level'])\ncount = pd.Series(count).sort_values(ascending=False)\nlabels = []\n\nfor i in count.keys():\n    labels.append(i + ' (' + str(count[i]\/len(X['Education_Level'])*100)[:5] + '%)')\n\nplt.bar(labels, count, color='blue')\nplt.title('Education level of the customers')\nplt.xlabel('Education level')\nplt.ylabel('Number of customers')\nplt.show()","f91073d4":"fig, ax = plt.subplots(figsize=(7, 7))\ncount = Counter(X['Gender'])\n\nax.pie(count.values(), labels=count.keys(), autopct=lambda p:f'{p:.2f}%')\nax.set_title('Gender of customers')\nplt.show()","9044975a":"fig, ax = plt.subplots(figsize=(8, 7))\ncount = Counter(X['Marital_Status'])\nlabels = []\n\nfor i in count:\n    labels.append(i + ' (' + str(count[i]\/len(X['Marital_Status'])*100)[:5] + '%)')\n    \nplt.bar(labels, count.values(), color='green')\nplt.title('Marital status for customers')\nplt.ylabel('Number of customers')\nplt.xlabel('Marital status')\nplt.show()","e6924161":"fig, ax = plt.subplots(figsize=(7, 7))\ncount = Counter(X['Card_Category'])\n\nax.pie(count.values(), labels=count.keys(), autopct=lambda p:f'{p:.2f}%')\nax.set_title('Card category of customers')\nplt.show()","6131257b":"fig, ax = plt.subplots(figsize=(7, 6))\ncount = Counter(X['Total_Relationship_Count'])\ncount = pd.Series(count).sort_values(ascending=False)\nlabels = []\n\nfor i in count.keys():\n    labels.append(str(i) + ' (' + str(count[i]\/len(X['Total_Relationship_Count'])*100)[:5] + '%)')\n    \nplt.bar(labels, count, color='purple')\nplt.title('Number of relationships for customers')\nplt.ylabel('Customers')\nplt.xlabel('Number of relationships')\nplt.show()","e2137e1b":"fig, ax = plt.subplots(figsize=(8, 6))\ncount = Counter(X['Income_Category'])\n\nax.pie(count.values(), labels=count.keys(), autopct=lambda p:f'{p:.2f}%')\nax.set_title('Income per customer')\nplt.show()","c5de2dea":"fig, ax = plt.subplots(figsize=(10, 5))\nsns.heatmap(X.corr(), annot=True)\nplt.show()","31d6a63d":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 5))\n\nax1.scatter(X['Total_Trans_Amt'], df['Total_Trans_Ct'])\nax2.scatter(X['Total_Revolving_Bal'], df['Avg_Utilization_Ratio'])\nax3.scatter(X['Months_on_book'], df['Customer_Age'])\n\nax1.set_xlabel('Total_Trans_Amt', fontsize=20)\nax1.set_ylabel('Total_Trans_Ct', fontsize=20)\n\nax2.set_xlabel('Total_Revolving_Bal', fontsize=20)\nax2.set_ylabel('Avg_Utilization_Ratio', fontsize=20)\n\nax3.set_xlabel('Months_on_book', fontsize=20)\nax3.set_ylabel('Customer_Age', fontsize=20)\n\nax2.set_title('Correlation of features', fontsize=40, pad=40)\n\nplt.show()","9425a220":"cols =['Credit_Limit','Avg_Open_To_Buy','Total_Amt_Chng_Q4_Q1','Total_Trans_Amt','Avg_Utilization_Ratio']\n\nfor col in cols:\n    i = 0\n    \n    fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n    \n    f1 = df[col]\n    f2 = (df[col]+1).transform(np.log)\n    f3 = pd.DataFrame(stats.boxcox(df[col]+1)[0])\n    f4 = pd.DataFrame(ss().fit_transform(np.array(df[col]).reshape(-1, 1)))\n    f5 = pd.DataFrame(mms().fit_transform(np.array(df[col]).reshape(-1, 1)))\n    \n    for column in [[f1, 'cyan', 'Normal'], [f2, 'pink', 'Log'], [f3, 'lightgreen', 'Box Cox'], \n                   [f4, 'skyblue', 'Standard'], [f5, 'yellow', 'MinMax']]:\n        feature = column[0]\n        colour = column[1]\n        name = column[2]\n        \n        feature.hist(ax=axes[i], color=colour)\n        deciles = feature.quantile([.1, .2, .3, .4, .5, .6, .7, .8, .9])\n        \n        for pos in np.array(deciles).reshape(1, -1)[0]:\n            handle = axes[i].axvline(pos, color='darkblue', linewidth=1)\n\n        axes[i].legend([handle], ['decile'])\n        axes[i].set_xlabel(name)\n        \n        i += 1 \n    \n    axes[2].set_title(col, fontsize=15, pad=15)\n    axes[3].set_title('')\n    axes[4].set_title('')\n                    \n    plt.show()\n\nplt.show()","5a310fd4":"X['Credit_Limit'] = X['Credit_Limit']\nX['Avg_Open_To_Buy'] = stats.boxcox(X['Avg_Open_To_Buy']+1)[0]\nX['Total_Amt_Chng_Q4_Q1'] = stats.boxcox(X['Total_Amt_Chng_Q4_Q1']+1)[0]\nX['Total_Trans_Amt'] = (X['Total_Trans_Amt']+1).transform(np.log)\nX['Avg_Utilization_Ratio'] = X['Avg_Utilization_Ratio']","2cc4414e":"for i in ['Credit_Limit', 'Total_Revolving_Bal','Avg_Open_To_Buy', 'Total_Amt_Chng_Q4_Q1', \n            'Total_Trans_Amt']:\n    col = X[i]\n    diff = col.max() - col.min()\n    bins = np.digitize(col, np.arange(col.min(), col.max(), (diff\/100)).tolist())\n    X[i+'_bin'] = bins","bf6161fe":"i = 0\ncols = ['Credit_Limit_bin', 'Total_Revolving_Bal_bin', 'Avg_Open_To_Buy_bin', 'Total_Amt_Chng_Q4_Q1_bin',\n        'Total_Trans_Amt_bin']\ncolours = ['pink', 'lightblue', 'lightgreen', 'skyblue', 'yellow']\n\nfig1, axes1 = plt.subplots(1, 2, figsize=(8, 3))\nfig2, axes2 = plt.subplots(1, 3, figsize=(15, 3))\n\nfor ax in axes1:\n    col = X[cols[i]]\n    pd.DataFrame(col).hist(ax=ax, color=colours[i])\n    deciles = col.quantile([.1, .2, .3, .4, .5, .6, .7, .8, .9])\n    \n    for pos in deciles:\n        handle = ax.axvline(pos, color='darkblue', linewidth=1.15)\n    \n    ax.legend([handle], ['decile'])\n    i += 1\n    \nfor ax in axes2:\n    col = X[cols[i]]\n    pd.DataFrame(col).hist(ax=ax, color=colours[i])\n    deciles = col.quantile([.1, .2, .3, .4, .5, .6, .7, .8, .9])\n    \n    for pos in deciles:\n        handle = ax.axvline(pos, color='darkblue', linewidth=1.15)\n    \n    ax.legend([handle], ['decile'])\n    i += 1\n    \nplt.show()","b68fd2dd":"cat_cols = ['Gender','Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category']\nfor col in cat_cols:\n    le = LabelEncoder()\n    X[col] = le.fit_transform(X[col])\n    \nle = LabelEncoder()\ny = le.fit_transform(y)\nX = X.drop(['Credit_Limit', 'Total_Revolving_Bal_bin', 'Avg_Open_To_Buy_bin', 'Total_Amt_Chng_Q4_Q1_bin',\n            'Total_Trans_Amt_bin'], axis=1)","81790d34":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","2e0bf33c":"count = Counter(y_train)\nprint('Distribution of target 1 & 2:', count[1], '&', count[0])","95aced97":"smote = SMOTE()\nX_train, y_train = smote.fit_resample(X_train, y_train)","b000a558":"count = Counter(y_train)\nprint('Distribution of target 1 & 2:', count[1], '&', count[0])","519231bb":"classifiers = [[XGBClassifier(),'XGB Classifier'], [RandomForestClassifier(),'Random Forest'], \n    [KNeighborsClassifier(), 'K-Nearest Neighbours'], [SGDClassifier(),'SGD Classifier'], [SVC(),'SVC']]","760e79fe":"score_list = []\ncross_val_list = []\nroc_auc_list = []\n\nfor classifier in classifiers:\n    model = classifier[0]\n    model.fit(X_train, y_train)\n    model_name = classifier[1]\n    \n    pred = model.predict(X_test)\n\n    score = model.score(X_test, y_test)\n    cross_val = cross_val_score(model, X_test, y_test).mean()\n    roc_auc = roc_auc_score(y_test, pred)\n    \n    score_list.append(score)\n    cross_val_list.append(cross_val)\n    roc_auc_list.append(roc_auc)\n    \n    print(model_name, 'model score:     ' + str(round(score*100, 2)) + '%')\n    print(model_name, 'cross val score: ' +str(round(cross_val*100, 2)) + '%')\n    print(model_name, 'roc auc score:   ' + str(round(roc_auc*100, 2)) + '%')\n    \n    if model_name != classifiers[-1][1]:\n        print('')","08adbbe6":"labels = ['XGBoost', 'Random Forest', 'KNN', 'SGD Classifier', 'SVC']\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 5))\n\nax1.bar(labels, score_list, color='blue')\nax2.bar(labels, cross_val_list, color='red')\nax3.bar(labels, roc_auc_list, color='green')\n\nax1.set_title('Model score')\nax2.set_title('Cross validation score')\nax3.set_title('ROC AUC score')\n\nplt.show()","52ed5468":"model = XGBClassifier()\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\n\nscore = model.score(X_test, y_test)\ncross_val = cross_val_score(model, X_test, y_test).mean()\nroc_auc = roc_auc_score(y_test, pred)\n\nprint('model score:     ' + str(round(score*100, 2)) + '%')\nprint('cross val score: ' +str(round(cross_val*100, 2)) + '%')\nprint('roc auc score:   ' + str(round(roc_auc*100, 2)) + '%')","05c0fcb4":"## Categorical visualisation\nThe first step in this dataset is the visualisation of the different features.","264665ba":"## Numerical visualisation","166eb728":"The distribution of gender for this bank is relatively equal, with there being only around 3% more women than men.","913001f2":"### Thank you for reading my notebook.","1605ef9d":"We will firstly use a LabelEncoder to convert the 'Gender', 'Education_Level', 'Marital_Status', 'Income_Category', and 'Card_Category' columns from categorical into numerical. Afterwards, we split the X and y into train and test datasets. The train will have 80% of X and the test will have 20%.","1f422a98":"The vast majority (93%) of customers use blue cards, followed by Silver (5%), Gold (1%) and Platinum (0.2%).","1c369d31":"To visualise that correlation, we will use a scattergraph on the six most correlatable features: '**Total_Trans_Amt**' and '**Total_Trans_Ct**', '**Total_Revolving_Bal**' and '**Avg_Utilization_Ratio**', '**Months_on_book**' and '**Customer_Age**'.","3807d7e1":"Afterwards, we check the distribution of the five least evenly-distributed features and see how they change with log transform, box cox, standard scaler and min max scaler.","7a0a4b1c":"However, the '0' target has 5 times less samples than the '1' target. Therefore, we will need to use SMOTE to resample it so that both can be even.","e77e11c0":"These techniques are applied below:","13f98260":"## Predicting Customer Churn\nNow is the time to use our dataset to predict whether a customer will end their use of the credit card company.","34b59c08":"The graphs below show us that 'Credit_Limit' and 'Avg_Utilization_Ratio' work best without a transformation, 'Avg_Open_To_Buy' and 'Total_Amt_Chng_Q4_Q1' are best with box cox and 'Total_Trans_amt' needs the log transform on it.","a13f472a":"The below plot is a pie chart which shows that roughly 84% of customers in our data are staying with the same firm, while 16% left.","c7ba8b30":"Next is the K-Nearest Neighbours achieving **81%, 88% and 79%**.","8a02e44e":"Next, we will create a selection of classifiers and use the best one for our final output. The predictors that will be used are XGBoost, Random Forest, K Nearest Neighbours, SGD Classifier and SVC.","465bb6a0":"### If you enjoyed this notebook and found it helpful, please upvote it so that I can make more of these.","a04e6be9":"The features in our dataset have some correlation, for example, Months_on_book and Customer_age, Avg_Utilization_Ratio and Total_Revolving_Bal.","3bcdfaa0":"As seen from the bar chart below, the XGBoost and the Random Forest consistently crush the rest of the competition. However, through a small margin, the winning model is the XGBoost Classifier. This predictor manages to achieve very high accuracies of **96%, 95%, and 94%**.","1185f84b":"The marital status of the customers in our bank shows us that around half are married, roughly 40% are single, and 7% are unknown and 7% divorced.","efa8f854":"The final piece of feature visualisation that we will do is displaying the distribution of the binned variables, where we can see that they are roughly centered, except from 'Credit_Limit_bin'.","2d956577":"The next graph is a bar chart which tells us the education level of the customers. Most of the people using the bank have some form of education, with only around 1,500 not being educated.","84fde006":"More than a third of the people make less than $40K, and the next most common category is $40-60K, which is almost half as frequent. After that is $80-120K (15%), $60-80K (14%), Unknown (11%) and then $120K+ (7%).","6654a9ee":"The Random Forest achieves slightly lower than this, with **95%, 93% and 92%**.","6688da1e":"# Predicting & Visualising Customer Churning\nThis notebook aims to visualise different features in this data, and use them to predict whether a customer will leave the credit card company.","1f2dbd3e":"To evaluate the results of our models, we will loop over them, fit them with the train sets and display the results with the score, cross_val and roc_auc metrics.","45534fa3":"We have chosen the XGBoost Classifier as the one to make our final prediction with, and the results are shown below.","12e5a86a":"Now, we will do some binning on the five features which have the widest range of values. We will reduce the amount of unique categories per feature from thousands to one hundred.","866e43fd":"The most common number of relationships people have had is three (22%), followed by four, five and six, which have roughly 18% each. Followed by that is 2 relationships (12%) and 1 relationship (9%).","8ad658aa":"The SGD and SVC classifiers manage to get accuracies ranging from **40% to 84%**"}}