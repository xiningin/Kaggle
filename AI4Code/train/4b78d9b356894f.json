{"cell_type":{"7ead39b4":"code","efdc088e":"code","8512cea1":"code","c24f6004":"code","450e410a":"code","eadb7733":"code","d29ba57c":"code","159cc02e":"code","0d0cb02e":"code","19955afe":"code","817074c4":"code","f527c585":"code","19b7d5a1":"code","ffe909c5":"code","df98c371":"code","6629b324":"code","29f0d093":"code","2743dd02":"code","5cbd1d00":"code","721a2003":"code","00bf5f73":"code","b099efa9":"code","8dfcc636":"code","5b2b85f0":"code","0f19f9f6":"code","966c14b2":"code","e15667f3":"code","97d793f9":"code","2beb5708":"code","e84df72a":"code","2cb3c4da":"code","11e36dbc":"code","e3bbc407":"code","f246bac1":"code","91d917c4":"markdown","f6209ec1":"markdown","133fd271":"markdown","61dd417b":"markdown","8ac82bbf":"markdown","2b9ddfa1":"markdown","7482ea70":"markdown","cadd76b4":"markdown","219a746c":"markdown","7917db86":"markdown","c0919b36":"markdown","4581c2d8":"markdown","5a540845":"markdown","09930ffa":"markdown","03c6d588":"markdown","7a6e71f6":"markdown","6d6f5e6c":"markdown","b910ab46":"markdown","567af3c3":"markdown","56f71c42":"markdown","ae8719d3":"markdown","7d10cdb3":"markdown","42583720":"markdown","e1ff276a":"markdown","6505d807":"markdown"},"source":{"7ead39b4":"from __future__ import division\nimport pandas as pd\nimport numpy as np\nimport requests\nimport nltk\nimport string\nimport re\nimport os\nfrom os import path\nfrom PIL import Image\nfrom bs4 import BeautifulSoup\nfrom time import sleep\nfrom collections import Counter\nfrom nltk.classify import NaiveBayesClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom textblob import TextBlob \nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n%matplotlib inline","efdc088e":"raw_data = pd.read_csv('\/kaggle\/input\/first-gop-debate-twitter-sentiment\/Sentiment.csv', encoding='utf-8')\ntweets = raw_data['text']\nlabels = raw_data['sentiment'] \nprint(tweets.head(2))\nprint(len(tweets),len(labels))","8512cea1":"#Remove all newlines from inside a string\n#ref.: https:\/\/stackoverflow.com\/questions\/13298907\/remove-all-newlines-from-inside-a-string\nclean_tweets = [tweet.replace('\\n','').strip() for tweet in tweets]\n#To remove all newline characters and then all leading\/tailing whitespaces from the string\n#Note: strip() only removes the specified characters from the VERY beginning or end of a string. You want to use replace:\n\n#remove the unicodes for the single left and right quote characters - see https:\/\/stackoverflow.com\/questions\/24358361\/removing-u2018-and-u2019-character\nclean_tweets[:] = [tweet.replace(u'\\u2018',\"'\").replace(u'\\u2019',\"'\") for tweet in clean_tweets] \n\n#convert abbrevations \nclean_tweets[:] = [tweet.replace('n\\'t',' not') for tweet in clean_tweets] #convert n't to  not \n\n#remove any sub-string containing 'http'\nclean_tweets[:] = [re.sub(r\"^.*http.*$\", '', tweet) for tweet in clean_tweets] \n\n#remove non-ASCII characters\n#see https:\/\/stackoverflow.com\/questions\/20078816\/replace-non-ascii-characters-with-a-single-space \nclean_tweets[:] = [re.sub(r'[^\\x00-\\x7F]+','', tweet) for tweet in clean_tweets] \n\n#remove tweeter's RT' tags\nclean_tweets[:] = [tweet.replace('RT','') for tweet in clean_tweets] \n\n#make all words lower case\nclean_tweets[:] = [tweet.lower() for tweet in clean_tweets] \n\nclean_tweets[0]","c24f6004":"#remove comment mark if you'd like to download for the first time\n#nltk.download(\"stopwords\")","450e410a":"string.punctuation","eadb7733":"#here I customised the useless words by adding these: 'gop', 'debate', 'gopdebate', 'news' as they are 'neutral' and exist in both positive and negative tweets, won't give any insights; instead, they are likely 'noise' to ML algo.\nuseless_ones = nltk.corpus.stopwords.words(\"english\") + list(string.punctuation) + ['``', \"''\",'gop','debate','gopdeb','gopdebate','gopdebates','fox','news','foxnew','foxnews', 'amp']\n#useless_ones = nltk.corpus.stopwords.words(\"english\") + list(string.punctuation) ","d29ba57c":"#remove comment mark if you'd like to download for the first time\n#nltk.download(\"punkt\")","159cc02e":"#tokenize and clean up the whole set of tweet texts (tokenized and cleaned tweets: tc_tweets)\ntc_tweets = []\nfor tweet in clean_tweets:\n    wordlist = [word for word in nltk.word_tokenize(tweet) if word not in useless_ones] #a list of words per tweet\n    tc_tweets.append(wordlist)\ntc_tweets[0] ","0d0cb02e":"#apply stemming - you can use other stemming algo. \nsno = nltk.stem.SnowballStemmer('english')\ntc_tweets_stemmed = []\nfor words in tc_tweets:\n    stemmed_words = [sno.stem(word) for word in words]\n    tc_tweets_stemmed.append(stemmed_words)\n\ntc_tweets[:] = tc_tweets_stemmed","19955afe":"#Making a flat list out of list of lists in Python\n#ref.: https:\/\/stackoverflow.com\/questions\/952914\/making-a-flat-list-out-of-list-of-lists-in-python\nall_words = [item\n              for sublist in tc_tweets\n              for item in sublist]","817074c4":"len(all_words)","f527c585":"word_counter = Counter(all_words)\nmost_common_words = word_counter.most_common(10)\nmost_common_words","19b7d5a1":"sorted_word_counts = sorted(word_counter.values(),reverse=True)\n#sorted_word_counts = sorted(list(word_counter.values()), reverse=True)\nsorted_word_counts[:10]","ffe909c5":"plt.loglog(sorted_word_counts)\nplt.ylabel(\"Freq\")\nplt.xlabel(\"Word Rank\")\nplt.title('Word Rank for GOP Twitters')","df98c371":"plt.hist(sorted_word_counts, bins=50, log=True);","6629b324":"label_count=pd.Series(labels).value_counts()\nlabel_count","29f0d093":"review_ratio = [opinion\/sum(label_count)*100 for opinion in label_count]\nprint('Sentiment Ratio: ', review_ratio)\nPos_ratio = label_count['Positive']\/sum(label_count)*100\nprint('Positive comments ratio: {0}%'.format(Pos_ratio))","2743dd02":"y_pos = range(len(label_count))\n#plt.bar(y_pos,label_count,align='center', alpha=.5)\nplt.bar(y_pos,review_ratio,align='center', alpha=.5)\nplt.xticks(y_pos,label_count.index)\nplt.ylabel('Percentage (100%)')\nplt.title('Sentiment Ratio for GOP Twitters')","5cbd1d00":"text_label_pair_list = list(zip(tc_tweets,labels))\ntext_label_pair_list[0]","721a2003":"#remove those neutral tweets as I am only interested in neg \/ pos ones\ntext_label_pair_list[:] = [tuple for tuple in text_label_pair_list if tuple[1]!='Neutral']","00bf5f73":"#split into train and test set, 90% for training set, 10% reserved for testing and evaluation\ntrain, test = train_test_split(text_label_pair_list, test_size = .1, random_state=7)","b099efa9":"train_pos = [tuple for tuple in text_label_pair_list if tuple[1]=='Positive']\ntrain_neg = [tuple for tuple in text_label_pair_list if tuple[1]=='Negative']","8dfcc636":"#unzip texts\ntrain_pos_texts, _ = list(zip(*train_pos))\ntrain_neg_texts, _ = list(zip(*train_neg))","5b2b85f0":"train_pos_texts_str = ' '.join([word for sublist in train_pos_texts\n                                        for word in sublist])\ntrain_neg_texts_str = ' '.join([word for sublist in train_neg_texts\n                                        for word in sublist])","0f19f9f6":"def create_wordcloud_with_mask (data):\n    # read the mask image\n    usa_mask = np.array(Image.open(path.join(\"\/kaggle\/input\/usamap\/usa_map.jpg\")))\n    wcloud = WordCloud(max_words=1000,\n                       mask=usa_mask,\n                       stopwords=set(STOPWORDS),\n                       background_color='white',\n                       contour_width=3,\n                       contour_color='steelblue')\n    #create word cloud\n    wcloud.generate(data)\n    #display\n    plt.figure(1,figsize=(12, 12))\n    plt.imshow(wcloud, cmap=plt.cm.gray, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()\n    \nprint(\"Positive comments in training set\")\ncreate_wordcloud_with_mask(train_pos_texts_str)\nprint(\"Negative comments in training set\")\ncreate_wordcloud_with_mask(train_neg_texts_str)","966c14b2":"def build_bow_features(words):\n    return {word:True for word in words}","e15667f3":"#build a list of tuples (BOW_dict, label) for all tweets\ntrain_bow = [(build_bow_features(tuple[0]), tuple[1]) for tuple in train]\ntest_bow = [(build_bow_features(tuple[0]), tuple[1]) for tuple in test]","97d793f9":"print(len(train_bow),len(test_bow))","2beb5708":"sentiment_classifier = NaiveBayesClassifier.train(train_bow)","e84df72a":"nltk.classify.util.accuracy(sentiment_classifier, train_bow)*100","2cb3c4da":"nltk.classify.util.accuracy(sentiment_classifier, test_bow)*100","11e36dbc":"test_comment_dicts, test_labels = list(zip(*test_bow))\npreds = [sentiment_classifier.classify(comment_dict) for comment_dict in test_comment_dicts]\npred_vs_observ = pd.DataFrame(np.array([test_labels,preds]).T,columns=['observation','prediction'])\npred_vs_observ.transpose()","e3bbc407":"#print confusion matrix\nprint(confusion_matrix(test_labels, preds))","f246bac1":"sentiment_classifier.show_most_informative_features(100)","91d917c4":"# GOP Twitter Sentiment Analysis in Python\n\n#### Created by Kevin 2018 \n\n#### Sentiment Analysis: \n\nSentiment analysis (a.k.a. Opinion Ming) refers to the use of natural language processing (NLP), text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine.\n\n#### NLTK\n`nltk` is the most popular Python package for Natural Language processing, it provides algorithms for importing, cleaning, pre-processing text data in human language and then apply computational linguistics algorithms like sentiment analysis.\n\nIt also includes many easy-to-use datasets in the `nltk.corpus` package, we can download for example the `movie_reviews` package using the `nltk.download` function:   \nnltk.download(\"movie_reviews\")  \nnltk.download(\"stopwords\"): nltk English stopwords\nnltk.download(\"punkt\"): nltk tokenizer trained on English  ","f6209ec1":"#### Evaluate model accuracy on the test set","133fd271":"### Build WordCloud for GoP Twitter","61dd417b":"We can check after training what is the accuracy on the training set, i.e. the same data used for training, we expect this to be a very high number because the algorithm already \"saw\" those data. Accuracy is the fraction of the data that is classified correctly, we can turn it into percent:","8ac82bbf":"## Data Acquisition \n###  Get raw data\nIn the original dataset, I extracted tweets and sentiment only as they are relevant to my semtiment analysis.   \nI could have built pandas data frame, but it has no obvious advantage as most of time I will process text and sentiment separately for this specific purpose.","2b9ddfa1":"## Summary\n1. NLTK is a handy and powerful tool to do NLP in Python\n2. Accuracy is reasonably good using Naive Bayes classifier \n3. If we want to further improve accuracy, a few options to consider: \n\t- The model has to become smarter to recognize text context and noise. For example, for this negative tweeter: \"new game! 1 person can\\'t win. players close eyes; try to find them by saying \"marco\", waiting for the loser to reply\", the separate 'positive' words like 'win', 'new' and 'marco' might made the model to mistakenly predict it as positive.\n\t- Apply NN Deep Learning like RNN and LSTM techniques to better model the texts \n    \n    ","7482ea70":"Using the Python string.punctuation list and the English stopwords we can build better features by filtering out those words that would not help in the classification:","cadd76b4":"This is what we wanted, but we notice that also punctuation like \"?\" and words useless for classification purposes like \"the\" or \"did\" are also included.\nThose words are named \"stopwords\" and `nltk` has a convenient corpus we can download:","219a746c":"The `collection` package of the standard library contains a `Counter` class that is handy for counting frequencies of words in our list.  \nIt also has a most_common() method to access the words with the higher count:  ","7917db86":"#### Naive Bayes\nOne of the simplest supervised machine learning classifiers is the Naive Bayes Classifier, it can be trained on 90% of the data to learn what words are generally associated with positive or with negative comments.","c0919b36":"## Train a Classifier for Sentiment Analysis","4581c2d8":"## Build BOW (Bag-of-Words) Model\n\nThe simplest model for analyzing text is just to think about text as an unordered collection of words (bag-of-words). This can generally allow to infer from the text the category, the topic or the sentiment.\n\nFrom the bag-of-words model we can build features to be used by a classifier, here we assume that each word is a feature that can either be `True` or `False`.\nWe implement this in Python as a dictionary where for each word in a sentence we associate `True`, if a word is missing, that would be the same as assigning `False`.","5a540845":"We can sort the word counts and plot their values on Logarithmic axes to check the shape of the distribution. This visualization is particularly useful if comparing 2 or more datasets, a flatter distribution indicates a large vocabulary while a peaked distribution a restricted vocabulary often due to a focused topic or specialized language.","09930ffa":"The accuracy above is mostly a check that nothing went very wrong in the training, the real measure of accuracy is on the remaining 10% of the data that wasn't used in training, the test data:","03c6d588":"Ok, there are 13871 tweets in the raw dataset. ","7a6e71f6":"### Text Data Cleaning ","6d6f5e6c":"Another related plot is the histogram of sorted_word_counts, which displays how many words have a count in a specific range.\n\nOf course the distribution is highly peaked at low counts, i.e. most of the words appear which a low count, so we better display it on semilogarithmic axes to inspect the tail of the distribution.","b910ab46":"`nltk` has a sophisticated word tokenizer trained on English named `punkt`, we first have to download its parameters: ","567af3c3":"### Text Tokenizing and Removing Stopwords & Punctuations ","56f71c42":"Accuracy here is around 81% which is pretty good for such a simple model if we consider that the estimated accuracy for a person is about 80%. We can finally print the most informative features, i.e. the words that mostly identify a positive or a negative review:","ae8719d3":"## Data Preprocessing (cleaning, wranggling, transforming, ETL)","7d10cdb3":"### Text Stemming","42583720":"### Plotting Frequencies of Words\n\nIt is common to explore a dataset before starting the analysis, in this section we will find the most common words and plot their frequency.\n\nWe can extract the words from the entire set of reviews.","e1ff276a":"### Visualise Positives vs. Negatives Ratio","6505d807":"## Exploratory Study & Data Visualisation"}}