{"cell_type":{"f29042ad":"code","3ea8d8ec":"code","c7835208":"code","6c1cc037":"code","986f6d1c":"code","cc3c4019":"code","e0a78f3c":"code","d5d6b1c4":"code","e5b14a96":"code","b1b18e44":"code","4a890a95":"code","ca5af9aa":"code","4377183e":"code","234ba361":"code","299e62d3":"code","08193e55":"code","1bd33155":"code","8544efb5":"code","3c95ccac":"code","59dd4b26":"code","3321a88b":"code","cafc8948":"code","73d7015a":"code","e170ffbb":"code","308e0f2e":"code","8cfa93ef":"code","d6ed135a":"code","a215fba1":"code","ad3aeb63":"code","60fbe59c":"code","17480c5e":"code","fa4eae67":"code","0e52f894":"code","cc4311b1":"code","144e04bc":"markdown","a76ecf98":"markdown","5f6c778a":"markdown","2ad58e6a":"markdown","93df9afe":"markdown","6fea9e23":"markdown","4d811bad":"markdown","f021b99d":"markdown","24f8c31e":"markdown","9d035897":"markdown","045e508a":"markdown","b2d59cf3":"markdown","4380d268":"markdown","e4773ed5":"markdown","0109157f":"markdown","a5947b37":"markdown","12134dff":"markdown","a146821a":"markdown","4f1547b7":"markdown","95a0cf03":"markdown","4abe564d":"markdown","969681ab":"markdown"},"source":{"f29042ad":"from joblib import Parallel, delayed\nimport pandas as pd\nimport re\nfrom collections import defaultdict\nimport json\nimport itertools\nimport datetime\nfrom gensim.corpora import Dictionary\nfrom unidecode import unidecode\nfrom tqdm.notebook import tqdm","3ea8d8ec":"!pip install langdetect","c7835208":"class config():\n    INPUT_DIR=\"\/kaggle\/input\/CORD-19-research-challenge\/\"\n    \n    META_FN='\/kaggle\/input\/cord-19-step1-meta\/meta_df.pkl'\n    CORPUS_FN='\/kaggle\/working\/corpus.pkl'","6c1cc037":"\"\"\"\nHelper functions\n\"\"\"\n\nimport pickle\n\ndef save(obj, fname):\n    with open(fname, 'wb') as file:\n        pickle.dump(obj, file, protocol=pickle.HIGHEST_PROTOCOL)\n\ndef load(fname):\n    with open(fname, 'rb') as file:\n        return pickle.load(file)\n\n#https:\/\/stackoverflow.com\/questions\/312443\/how-do-you-split-a-list-into-evenly-sized-chunks\ndef chunks(lst, n):\n    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n    for i in range(0, len(lst), n):\n        yield lst[i:i + n]\n","986f6d1c":"# Update, we only use sars-cov-2, covid-19 keywords\ncov_tokens = ['sars-cov-2',\n              'hcov-19',\n              'covid-19',\n              '2019-ncov']","cc3c4019":"class Cleaner():\n    def __init__(self, is_recent=False):\n        self.is_recent = is_recent\n        \n    re_flags = re.ASCII\n    \n    # https:\/\/github.com\/jfilter\/clean-text\/blob\/master\/cleantext\/constants.py\n    URL_REGEX = re.compile(\n        r\"(?:^|(?<![\\w\\\/\\.]))\"\n        # protocol identifier\n        # r\"(?:(?:https?|ftp):\/\/)\"  <-- alt?\n        r\"(?:(?:https?:\\\/\\\/|ftp:\\\/\\\/|www\\d{0,3}\\.))\"\n        # user:pass authentication\n        r\"(?:\\S+(?::\\S*)?@)?\" r\"(?:\"\n        # IP address exclusion\n        # private & local networks\n        r\"(?!(?:10|127)(?:\\.\\d{1,3}){3})\"\n        r\"(?!(?:169\\.254|192\\.168)(?:\\.\\d{1,3}){2})\"\n        r\"(?!172\\.(?:1[6-9]|2\\d|3[0-1])(?:\\.\\d{1,3}){2})\"\n        # IP address dotted notation octets\n        # excludes loopback network 0.0.0.0\n        # excludes reserved space >= 224.0.0.0\n        # excludes network & broadcast addresses\n        # (first & last IP address of each class)\n        r\"(?:[1-9]\\d?|1\\d\\d|2[01]\\d|22[0-3])\"\n        r\"(?:\\.(?:1?\\d{1,2}|2[0-4]\\d|25[0-5])){2}\"\n        r\"(?:\\.(?:[1-9]\\d?|1\\d\\d|2[0-4]\\d|25[0-4]))\"\n        r\"|\"\n        # host name\n        r\"(?:(?:[a-z\\\\u00a1-\\\\uffff0-9]-?)*[a-z\\\\u00a1-\\\\uffff0-9]+)\"\n        # domain name\n        r\"(?:\\.(?:[a-z\\\\u00a1-\\\\uffff0-9]-?)*[a-z\\\\u00a1-\\\\uffff0-9]+)*\"\n        # TLD identifier\n        r\"(?:\\.(?:[a-z\\\\u00a1-\\\\uffff]{2,}))\" r\")\"\n        # port number\n        r\"(?::\\d{2,5})?\"\n        # resource path\n        r\"(?:\\\/[^\\)\\]\\}\\s]*)?\",\n        # r\"(?:$|(?![\\w?!+&\\\/\\)]))\",\n        # @jfilter: I removed the line above from the regex because I don't understand what it is used for, maybe it was useful?\n        # But I made sure that it does not include ), ] and } in the URL.\n        flags=re_flags | re.IGNORECASE,\n    )\n\n    \"\"\"\n    Drop license information\n    \"\"\"\n    LICENSE1_REGEX = re.compile(r'\\(\\W*which\\W*was\\W*(?:[^)])+reviewed\\)\\W+', flags=re_flags)\n    LICENSE2_REGEX = re.compile(r'CC-(?:CC0-?|BY-?|SA-?|NC-?|ND-?)+\\s+\\d?\\s?\\.?\\d?\\s+', flags=re_flags)\n    LICENSE3_REGEX = re.compile(r'International license (:?It )is made available under a\\s+', flags=re_flags)\n    LICENSE4_REGEX = re.compile(r'author\/funder\\s*|(:?URL )?doi: \\w+ preprint', flags=re_flags)\n    LICENSE5_REGEX = re.compile(r'The copyright holder for this preprint(:? is the)?\\s*\\.?\\s*(?:URL\\s*)?', flags=re_flags)\n    LICENSE6_REGEX = re.compile(r'who has granted \\w+ a license to display the preprint in perpetuity\\.?', flags=re_flags)\n    LICENSE7_REGEX = re.compile(r'BY-(?:SA-?|NC-?|ND-?)+\\s+\\d?\\s?\\.?\\d?\\s+', flags=re_flags)\n    LICENSE8_REGEX = re.compile(r'No reuse allowed without permission\\.?', flags=re_flags)\n    LICENSE9_REGEX = re.compile(r'All rights reserved\\.?', flags=re_flags)\n\n    \"\"\"\n    Virus, disease names\n    https:\/\/en.wikipedia.org\/wiki\/Novel_coronavirus\n    \"\"\"\n    vn1 = r'sars\\W{,2}cov\\W{,2}2' # SARS-CoV-2\n    vn2 = r'hcov\\W{,2}(?:20)?19'  # HCoV-2019\n    vn3 = r'(?:20)?19\\W{,2}ncov'  # 2019-nCoV\n    # vn4= r'Novel coronavirus' # a bit risky\n    VIRUS_REGEX = re.compile(fr'{vn1}|{vn2}|{vn3}', flags=re.IGNORECASE|re_flags)\n    DISEASE_REGEX = re.compile(r'covid\\W{,2}(?:20)?19', flags=re.IGNORECASE|re_flags)\n    \n    \"\"\"\n    Warning we drop usefull information\n    TODO: Explain why\n    \"\"\"\n    LONG_REGEX = re.compile(r'[^\\s]{64,}', flags=re_flags)\n    # Should became recursive\n    IN_BRACKETS_REGEX = re.compile(r'\\[[^\\[\\]]+\\]', flags=re_flags)\n    IN_PARENTHESES_REGEX = re.compile(r'\\([^()]+\\)', flags=re_flags)\n    \n    LATEX_BEGIN = re.compile(r'\\\\begin', flags=re_flags | re.IGNORECASE)\n    LATEX_END = re.compile(r'\\\\end', flags=re_flags | re.IGNORECASE)\n\n    MULTI_SPACE = re.compile(r' +', flags=re_flags)\n    \n    regex_pre = [\n        (URL_REGEX, ' URL '),\n\n        (LICENSE1_REGEX, ' '),\n        (LICENSE2_REGEX, ' '),\n        (LICENSE3_REGEX, ' '),\n        (LICENSE4_REGEX, ' '),\n        (LICENSE5_REGEX, ' '),\n        (LICENSE6_REGEX, ' '),\n        (LICENSE7_REGEX, ' '),\n        (LICENSE8_REGEX, ' '),\n        (LICENSE9_REGEX, ' '),\n\n        (LATEX_BEGIN, ' {'),\n        (LATEX_END, '} '),\n\n        (LONG_REGEX, ' '),\n        (IN_BRACKETS_REGEX, ' '),\n        (IN_PARENTHESES_REGEX, ' '),\n        \n        (MULTI_SPACE, ' ')\n    ]\n    \n    \"\"\"\n    Recursively drop all what is inside {}\n    \"\"\"\n    regex_rec = re.compile(r'\\{[^\\{\\}]*\\}', flags=re_flags)\n    \n    \"\"\"\n    Post processing\n    Drop Latex commands like: \\\\begin, \\\\end...\n    Add space around \/ and =\n        This break some chemical names\n        TODO: Show examples and\/or improve\n    \"\"\"\n    LATEX_CMD = re.compile(r'\\\\[^\\s]+', flags=re_flags)\n    AND_REGEX = re.compile(r'\/', flags=re_flags)\n    EQUAL_REGEX = re.compile(r'=', flags=re_flags)\n    regex_post = [\n        (LATEX_CMD, ' '),\n        (AND_REGEX, ' \/ '),\n        (EQUAL_REGEX, ' = ')\n    ]\n    \n    def clean(self, txt, is_bib=False):\n        \"\"\"\n        \n        \"\"\"\n        \n        # ASCII transliterations of Unicode text\n        txt = unidecode(txt)\n        \n        # TODO: make apply regex function\n        \n        # Change virus, diease name to official names only if recent publication and not bibliography title\n        if self.is_recent and not is_bib:\n            txt = self.VIRUS_REGEX.sub(' SARS-CoV-2 ', txt)\n            txt = self.DISEASE_REGEX.sub(' COVID-19 ', txt)\n        \n        for regex, replace_with in self.regex_pre:\n            txt = regex.sub(replace_with, txt)\n        \n        max_iter = 20\n        while max_iter>0:\n            o=txt\n            txt = self.regex_rec.sub(' ', txt)\n            if o==txt:\n                break\n            max_iter -= 1\n                \n        for regex, replace_with in self.regex_post:\n            txt = regex.sub(replace_with, txt)\n        \n        return txt.strip()","e0a78f3c":"from nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\ndef lemmatize(word):\n    \"\"\"\n    WordNet lemmatizer, try verb, noun, adjective andd return the smalest\n    Always begin with verb.\n    WARN: Slow code, we can for ex remove adjective from the list\n    \"\"\"\n    lemmatizer = WordNetLemmatizer()\n    \n    word_v = lemmatizer.lemmatize(word, pos='v')\n    word_n = lemmatizer.lemmatize(word, pos='n')\n    word_a = lemmatizer.lemmatize(word, pos='a')\n    \n    return min([word_v, word_n, word_a], key=len)\n\n\"\"\"\nCommun useless words\n\"\"\"\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english')) | ENGLISH_STOP_WORDS\n\n# what is: ed\nstop_words.update([\"'s\", \"n't\",\n                   \"e.g.\", \"e.g\", \"i.e.\", \"i.e\", \"etc.\", \"etc\",\n                   \"co.\", \"ltd.\",\n                   \"et\", \"al\", \"al.\", \"dr.\",\n                   \"table\", \"figure\", \"fig\", \"fig.\",\n                   \"url\",\n                   \"http\", \"https\"\n                  ])\n\n\n# BAD code rewrite\nnumber = r'[\\d\\Wex]+' # fake number, but do the job\nnumber_re = re.compile(f'^{number}s?$')\nnot_letters_digits_re = re.compile(r'^[^a-zA-Z0-9]+$')\nsingle_re = re.compile(r'^[a-zA-Z]\\.?$')\n\ndef valid_token(token):\n    not_stop_words = not token in stop_words\n    not_number = not number_re.match(token)\n    not_punctuation = not not_letters_digits_re.match(token)\n    not_single = not single_re.match(token)\n    \n    return (not_number and \n            not_stop_words and \n            not_punctuation and \n            not_single and \n            len(token)>1 and len(token)<70)\n\n\n# Rewrite proprely, at the right place (tokenizer class)\na_reg = re.compile(fr\"^[\\W]+(.+)$\")\nreg = r'^(\\d+[.e\\d\\W]+)((?!.*ncov$).+)'\nb_reg = re.compile(reg)\nc_reg = re.compile(r'^(.*)[-*]+$')\n\ndef post(token):\n    # Drop non words characters at begining: \"-+,=sometext\" -> \"sometext\"\n    token = a_reg.sub(r'\\1',token)\n    # \"10km\" -> \"km\", \"10-years-old\" -> \"years-old\" but keep \"2019-ncov\"\n    token = b_reg.sub(r'\\2',token)\n    # \"sometext-\" -> \"sometext\", \"word*\" -> \"word\"\n    token = c_reg.sub(r'\\1',token)\n    \n    return token\n","d5d6b1c4":"class Sentence():\n    ws_reg = re.compile(r'\\s+')\n    \n    def __init__(self, text):\n        # ~ Original text\n        self.original_text = self.ws_reg.sub(' ', text)\n        \n        # List of str\n        self._text = []\n        \n        # Lazy init\n        self._bow = None\n        # Sentence text tokens id\n        self._tokensid = None\n        # Unique tokens id\n        self._tokensid_set = None\n        \n    def tokenize(self):\n        # Rewrite proprely, at the right place (tokenizer class)\n        self.text[:] = word_tokenize(self.original_text, preserve_line=True)\n        self.text[:] = [token.lower() for token in self.text]\n        self.text[:] = [post(token) for token in self.text]\n        self.text[:] = [lemmatize(token) for token in self.text]\n        self.text[:] = [token for token in self.text if valid_token(token)]\n    \n    @property\n    def tokensid(self):\n        if self._tokensid is None:\n            self._tokensid = [tokenid for tokenid, _ in self.bow]\n        return self._tokensid\n    \n    @property\n    def tokensid_set(self):\n        if self._tokensid_set is None:\n            self._tokensid_set = set(self.tokensid)\n        return self._tokensid_set\n    \n    @property\n    def bow(self):\n        if self._bow is None:\n            self._bow = dictionary.doc2bow(self.text)\n        return self._bow\n    \n    @property\n    def text(self):\n        return self._text\n\n    @text.setter\n    def text(self, text):\n        self._text = text\n    \n    def __hash__(self):\n        return hash(self.original_text)\n\n    def __eq__(self, other):\n        return self.original_text == other.original_text\n    \n    def __repr__(self):\n        return self.original_text\n","e5b14a96":"class Document():\n    def __init__(self, txt_blocks):\n        txt_blocks = txt_blocks if isinstance(txt_blocks,list) else [txt_blocks]\n        \n        # Lazy init\n        # Document BoW\n        self._bow = None\n        # Document text tokens id\n        self._tokensid = None\n        # Unique tokens id\n        self._tokensid_set = None\n        \n        self.sentences = list(set([Sentence(sentence) for txt in txt_blocks for sentence in sent_tokenize(txt)]))\n    \n    def tokenize(self):\n        for sent in self.sentences:\n            sent.tokenize()\n    \n    \"\"\"\n    Commun with Sentence class, merge them.\n    \"\"\"\n    @property\n    def bow(self):\n        if self._bow is None:\n            self._bow = dictionary.doc2bow(self.text)\n        return self._bow\n    \n    @property\n    def tokensid(self):\n        if self._tokensid is None:\n            self._tokensid = [tokenid for tokenid, _ in self.bow]\n        return self._tokensid\n    \n    @property\n    def tokensid_set(self):\n        if self._tokensid_set is None:\n            self._tokensid_set = set(self.tokensid)\n        return self._tokensid_set\n    \"\"\"\n    End commun\n    \"\"\"\n    \n    @property\n    def text(self):\n        return list(itertools.chain(*[sentence.text for sentence in self]))\n    \n    def __iter__(self):\n        return iter(self.sentences)\n            \n    def __len__(self):\n        return len(self.sentences)\n","b1b18e44":"# Paper ex FileReader class, they are so different now but it helps!\n# https:\/\/www.kaggle.com\/maksimeren\/covid-19-literature-clustering\nclass Paper(Document):\n    def __init__(self, txt_blocks=None):\n        pass\n    \n    def get_content(self):\n        with open(f'{config.INPUT_DIR}\/{self.file_path}') as file:\n            content = json.load(file)\n            return content\n    \n    def add_entry(self, container, text, cleaner=None, is_bib=False, min_len=24, max_len=2e4):\n        text = text.strip()\n        \n        n_chars = len(text)\n        if n_chars>min_len and n_chars<max_len:\n            if cleaner is not None:\n                text = cleaner.clean(text, is_bib)\n            container.add(text)\n            \n            return text\n        return None\n    \n    def from_json(self, row):\n        \"\"\"\n        Parse JSON paper\n        Better to put this function away from this class\n        \n        Parameters\n            row : Pandas Series (title, doi, abstract, publish_time, pmc_json_files)\n        returns\n            Paper\n        \"\"\"\n        \n        # Paper text blocks\n        txt_blocks = set()\n        \n        self.file_path = row.pmc_json_files\n        self.doi = row.doi\n        \n        # Epoch time\n        self.publish_time = (row.publish_time - pd.Timestamp(\"1970-01-01\")) \/\/ pd.Timedelta('1s')\n        \n        \n        # Old code, now all papers are recent\n        pub_date = datetime.datetime.fromtimestamp(self.publish_time).date()\n        is_recent = pub_date > datetime.date(2019, 12, 1)\n        cleaner = Cleaner(is_recent)\n        \n        self.title = self.add_entry(txt_blocks, row.title, cleaner)\n        self.abstract = self.add_entry(txt_blocks, row.abstract, cleaner)\n        \n        content = self.get_content()\n\n        for entry in content['body_text']:\n            self.add_entry(txt_blocks, entry['section'], cleaner) #sub title\n            self.add_entry(txt_blocks, entry['text'], cleaner)\n\n        for entry in content['ref_entries']:\n            self.add_entry(txt_blocks, content['ref_entries'][entry]['text'], cleaner)\n\n        \"\"\"\n        Not used now\n        We keep bib entries to build a graph, and set PageRank\n            We need prior paper relevance, can be combination of authority and newest\n        Make it outside this class\n        Remember to not change papers title use 'cleaner.clean(text, is_bib=True)'\n        \"\"\"\n        bib_entries = set()\n        for entry in content['bib_entries']:\n            self.add_entry(bib_entries, content['bib_entries'][entry]['title'], cleaner, is_bib=True)\n        self.bib_entries = list(bib_entries)\n        \n        # When debuging keep paper text...\n        #self.txt_blocks = list(txt_blocks)\n        \n        super(Paper, self).__init__(list(txt_blocks))\n        self.tokenize()\n        \n        return self\n    \n    def __repr__(self):\n        return f'{self.title}:\\n{self.abstract[:200]}...\\n'\n","4a890a95":"\"\"\"\nCorpus class\n\"\"\"\nclass Corpus:\n    def __init__(self, docs):\n        # List of Document\n        self.docs = docs\n        \n        \"\"\"\n        Lazy init\n        Corpus Term frequency & Term relative frequency\n        Shape [n, m]\n          n = #documents\n          m = #terms\n        \"\"\"\n        self._TF = None\n        self._TRF = None\n    \n    @property\n    def TF(self):\n        if self._TF is None:\n            # Like in gensim:docsim:SparseMatrixSimilarity\n            # But transform to dense array... explain!\n            self._TF = matutils.corpus2csc([doc.bow for doc in self], num_terms=len(dictionary)).T\n            self._TF = np.asarray(self._TF.todense()) # currently no-op, CSC.T is already CSR\n        \n        return self._TF\n    \n    @property\n    def TRF(self):\n        if self._TRF is None:\n            self._TRF = self.TF\/self.TF.sum(axis=1).reshape(-1,1)\n        \n        return self._TRF\n    \n    @property\n    def bow(self):\n        return [doc.bow for doc in self.docs]\n    \n    @property\n    def text(self):\n        return [doc.text for doc in self.docs]\n    \n    # https:\/\/gaopinghuang0.github.io\/2018\/11\/17\/python-slicing\n    def __getitem__(self, key):\n        if isinstance(key, slice):\n            start, stop, step = key.indices(len(self))\n            return Corpus([self.docs[i] for i in range(start, stop, step)])\n        elif isinstance(key, list):\n            assert len(key)>0, f'Empty list provided'\n            assert type(key[0]) == int, f'Only list of integers supported'\n            \n            return Corpus([self.docs[i] for i in key])\n        elif isinstance(key, int):\n            return self.docs[key]\n        else:\n            raise Exception(f'Invalid argument type: {type(key)}')\n\n    def __iter__(self):\n        return iter(self.docs)\n            \n    def __len__(self):\n        return len(self.docs)","ca5af9aa":"meta_df  = pd.read_pickle(config.META_FN)","4377183e":"meta_df.head()","234ba361":"len(meta_df)","299e62d3":"meta_df = meta_df.query(\"publish_time > '2019\/12\/01'\")\n\nlen(meta_df)","08193e55":"%%time\n\nfrom langdetect import detect\nfrom langdetect import detect_langs\nfrom langdetect import DetectorFactory\nDetectorFactory.seed = 0\n\ndef get_langid(txt):\n    return detect(txt)\n\nmeta_df['abstract_langid'] = meta_df['abstract'].apply(get_langid)\nmeta_df['title_langid'] = meta_df['title'].apply(get_langid)\n\nmeta_df['abstract_langid'].value_counts()","1bd33155":"meta_df = meta_df.query(\"(title_langid=='en') and (abstract_langid=='en')\")\n\nlen(meta_df)","8544efb5":"def get_paper(row):\n    paper = Paper().from_json(row)\n    \n    return paper","3c95ccac":"def get_papers():\n    with Parallel(n_jobs=-1, backend='multiprocessing') as parallel:\n        dfunc = delayed(get_paper)\n        res = parallel(dfunc(row) for i, row in tqdm(meta_df.iterrows(),total = len(meta_df)))\n        \n        return res","59dd4b26":"%%time\n\npapers = get_papers()","3321a88b":"len(papers)","cafc8948":"papers[:] = [paper for paper in papers if len(paper.bib_entries) > 2 and \n             (paper.title is not None) and \n             (paper.abstract is not None) and \n             len(paper.sentences)>2]","73d7015a":"len(papers)","e170ffbb":"def get_covid_papers(covid):\n    for paper in tqdm(papers):\n        is_covid = False\n        for token in cov_tokens:\n            if token in paper.text:\n                is_covid = True\n                break\n        if covid and is_covid:\n            yield paper\n        elif not covid and not is_covid:\n            yield paper","308e0f2e":"covid_papers = [paper for paper in get_covid_papers(True)]","8cfa93ef":"len(covid_papers)","d6ed135a":"from nltk.corpus import wordnet as wn","a215fba1":"\"\"\"\nClean clean then clean\n\"\"\"\ndef merge_words(paper):\n    for si, sent in enumerate(paper):\n        sent = sent.text\n\n        new_sent = []\n        n = len(sent)\n        i = 0\n        look_ahead = 4\n        while i<n:\n            j = min(look_ahead, max(0, n-i-1))\n            while j>0:\n                ngram = '_'.join(sent[i:i+j+1])\n                if len(wn.synsets(ngram)):\n                    # Found a match\n                    break\n                j-=1\n\n            assert j>=0\n\n            if j==0:\n                new_sent.append(sent[i])\n            else:\n                new_sent.append(ngram)\n\n            i += 1+j\n\n        paper.sentences[si].text = new_sent","ad3aeb63":"%%time\nfor paper in covid_papers:\n    merge_words(paper)","60fbe59c":"dictionary = Dictionary([paper.text for paper in covid_papers])\n\nlen(dictionary)","17480c5e":"dictionary.filter_extremes(no_below=20, no_above=0.8)\n\nlen(dictionary)","fa4eae67":"corpus = Corpus(covid_papers)\ncorpus.dictionary = dictionary","0e52f894":"save(corpus, config.CORPUS_FN)","cc4311b1":"%%javascript\n$.getScript('https:\/\/kmahelona.github.io\/ipython_notebook_goodies\/ipython_notebook_toc.js')","144e04bc":"## Paper\n\nPaper class, Document subclass plus some paper information like (title, doi, abstract)","a76ecf98":"Get papers, please note the multiprocessing backend, is one of the ways to serialize the output...","5f6c778a":"# COVID-19 papers","2ad58e6a":"# Clean","93df9afe":"<h1 id=\"tocheading\">Table of Contents<\/h1>\n<div id=\"toc\"><\/div>","6fea9e23":"## Tokenizer\n\nSplit text to sentences than words...","4d811bad":"# Parse papers","f021b99d":"## Keep recent publications","24f8c31e":"## Cleaner\n\n\"NLP is 80% preprocessing.\" -Lev Konstantinovskiy ([Citation of citation](https:\/\/markroxor.github.io\/gensim\/static\/notebooks\/gensim_news_classification.html#topic=0&lambda=1&term=))\n\nClean papers text, like removing URLs, licenses...","9d035897":"## Sentence\n\nSentence class, words container...","045e508a":"Drop papers with low content","b2d59cf3":"## Document\n\nDocument class, sentences container...","4380d268":"## Corpus\n\nCorpus class, documents container.","e4773ed5":"**<center style=\"color:#FBB03B; font-size: 24pt;\">Work in progress...<\/center>**","0109157f":"# Merge known words","a5947b37":"# Classes","12134dff":"- Build dictionary\n- Filter dictionary\n- Build corpus","a146821a":"# Save","4f1547b7":"# Meta","95a0cf03":"**Build corpus**","4abe564d":"## Keep english papers\n\nThis is only probabilistic, so we will drop some english papers and keep some non-english ones!\n","969681ab":"Use Wordnet to merge consecutive words (until 4) into one token.  \nEx:  \n- world health organization -> world_health_organization  \n- severe acute respiratory syndrome -> severe_acute_respiratory_syndrome  \n- health care -> health_care  \n- but not this one -> but not this one  "}}