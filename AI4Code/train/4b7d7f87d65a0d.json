{"cell_type":{"35d71b93":"code","3479102e":"code","d8f511e0":"code","8d3da2c1":"code","e33cb391":"code","d0603dfa":"code","2e69ea57":"code","42f823be":"code","083dec0f":"code","46f21005":"code","0769343c":"code","dd39e315":"code","bb512fe1":"code","6def417c":"code","0f6930e6":"code","1bbb5eb0":"code","adfac620":"code","e86847fd":"code","35651815":"code","5516378f":"markdown","e4250075":"markdown","f34f2136":"markdown","1779ddf1":"markdown","2509ce48":"markdown","5ceb6e1a":"markdown","4f614632":"markdown","68bbe410":"markdown","c28ac6f9":"markdown","dd34a4ae":"markdown","0696b7b3":"markdown","bdf86854":"markdown","dd74f901":"markdown","aee92a58":"markdown","a9e54836":"markdown","6affba71":"markdown","96e57518":"markdown","01290fe7":"markdown","3098ffd9":"markdown"},"source":{"35d71b93":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, cross_val_score\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import metrics, model_selection\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n","3479102e":"df = pd.read_csv(\"\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\")\n                 \ndf.columns = [\"ID\", \"Gender\", \"Age\", \"Hypertension\", \"Heart Disease\", \"Ever Married\", \"Work Type\", \"Residence Type\", \"Avg. Glucose Level\", \"BMI\", \"Smoking Status\", \"Stroke\"]\n","d8f511e0":"#drop ID column\ndf = df.drop('ID', axis=1)","8d3da2c1":"#plots other feature sets\ndf.hist(figsize = (15, 15))\nplt.show()","e33cb391":"#plots Stroke feature\ndf['Stroke'].value_counts(dropna = False).plot.bar(color = 'blue')\nplt.title('Imbalanced Stroke Feature')\nplt.xlabel('zero & one')\nplt.ylabel('count')\nplt.show()","d0603dfa":"print(df.isnull().sum())\nprint(df.count())\n\n#removing null values in BMI column\ndf.dropna(axis=0, inplace=True)\ndf.reset_index(drop=True, inplace=True)\n","2e69ea57":"print(df.isnull().sum())\nprint(df.count())","42f823be":"\n#transforming dataset with dummies variables to replace characters with binary integers\ndf[\"Hypertension\"].replace([0,1], [\"No\",\"Yes\"], inplace=True)\ndf[\"Heart Disease\"].replace([0,1], [\"No\",\"Yes\"], inplace=True)\n\ndf2 = df[[\"Gender\",\"Age\",\"Hypertension\",\"Heart Disease\",\"Ever Married\",\"Work Type\",\"Residence Type\",\"Avg. Glucose Level\",\"BMI\", \"Smoking Status\",\"Stroke\"]]\n\ngender = pd.get_dummies(df2[\"Gender\"], drop_first=True)\nhypertension = pd.get_dummies(df2[\"Hypertension\"], drop_first=True, prefix=\"HT\")\nheartdisease = pd.get_dummies(df2[\"Heart Disease\"], drop_first=True, prefix=\"HD\")\nevermarried = pd.get_dummies(df2[\"Ever Married\"], drop_first=True, prefix=\"EM\")\nworktype = pd.get_dummies(df2[\"Work Type\"], drop_first=True)\nresidence = pd.get_dummies(df2[\"Residence Type\"],drop_first=True)\nsmoking = pd.get_dummies(df2[\"Smoking Status\"], drop_first=True)\n\ndf3 = pd.concat([df2,gender,hypertension,heartdisease,evermarried,worktype,residence,smoking], axis=1, join='outer', ignore_index=False)\nprint(df3.head(15))\n","083dec0f":"df3.drop([\"Gender\",\"Hypertension\",\"Heart Disease\",\"Ever Married\",\"Work Type\", \"Residence Type\",\"Smoking Status\"], axis=1, inplace=True)\n\n#relabeling dataset with proper headers\ndf4 = df3.reindex(labels=[\"Age\",\"Male\",\"HT_Yes\",\"HD_Yes\",\"EM_Yes\",\"Never_worked\",\"Private\",\"Self-employed\",\"children\",\"BMI\",\"Urban\",\"Avg. Glucose Level\",\"formerly smoked\", \"never smoked\", \"smokes\",\"Stroke\"], axis=1)\nprint(df4.head(15))\n","46f21005":"\n#feature set\nX = df4[[\"Age\",\"Male\",\"HT_Yes\",\"HD_Yes\",\"EM_Yes\",\"Never_worked\",\"Private\",\"Self-employed\",\"children\",\"BMI\",\"Avg. Glucose Level\",\"formerly smoked\", \"never smoked\", \"smokes\"]]\n\n#label set\ny = df4[\"Stroke\"]\n\nprint(X.count())\nprint(y.count())","0769343c":"#train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nprint(\"Number transactions X_train dataset: \", X_train.shape)\nprint(\"Number transactions y_train dataset: \", y_train.shape)\nprint(\"Number transactions X_test dataset: \", X_test.shape)\nprint(\"Number transactions y_test dataset: \", y_test.shape)","dd39e315":"y_train.value_counts(dropna = False).plot.bar(color = 'blue')\nplt.title('Stroke Feature Training Set')\nplt.xlabel('zero & one')\nplt.ylabel('count')\nplt.show()","bb512fe1":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n","6def417c":"print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))","0f6930e6":"sm = SMOTE()\nX_train, y_train = sm.fit_resample(X_train, y_train)\n\nprint('After OverSampling, the shape of train_X: {}'.format(X_train.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train.shape))\n\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train==0)))\n","1bbb5eb0":"#Gaussian naive bayes model\nclf = GaussianNB()\nclf = clf.fit(X_train,y_train)\ny_pred = clf.predict(X_test)\nprint(\"Accuracy Score Gaussian = \", accuracy_score(y_test, y_pred))\n","adfac620":"seed = 42\nkfold = model_selection.KFold(n_splits = 3,random_state = seed,shuffle=True)\n  \n# initialize the base classifier\nbase_cls = DecisionTreeClassifier()\n  \n# no. of base classifier\n# #Total Number of decision trees that will be used to train an ensemble\nnum_trees = 100","e86847fd":"# bagging classifier\nmodel = BaggingClassifier(base_estimator = base_cls,            # base estimator to fit on random subsets of the datraset\n                            n_estimators = num_trees,           # number of base estimators in the ensemble\n                            max_samples=50,                     # the number of features to draw from X to train each base estimator\n                            bootstrap = True,                   # Bootstrap = True means use bagging method\n                            random_state = seed)","35651815":"results = model_selection.cross_val_score(model, X_train, y_train, cv = kfold)\n\nprint(\"Bagging Accuracy Score:\\t\", results.mean())","5516378f":"Fitting our dataset to a Gaussian Naive Bayes model.","e4250075":"The objective of this project is to create models (i.e. Naive Bayes and Bagging Ensemble) that could help predict a future stroke based on certain lifestyle features such as Gender, Age, Hypertension, Heart Disease, Ever Married, Work Type, Residence Type, Avg. Glucose Level, BMI, Smoking Status.","f34f2136":"Now we are fixing the unbalanced dataset. Here we can understand the total count.","1779ddf1":"Ploting the label set after the train test split to visually see the train\/test size. train size 70% and test size 30%.","2509ce48":"Here we want to visualize our dataset and understand the distribution of each feature and label.","5ceb6e1a":"Transforming categorical data into binary integers. Method used is get_dummies. Label_encoders was not used since it is more fit for ranking hierarchy. Get_dummies allows us to transform features with multiple categories into separate feature. This increases our feature set size. ","4f614632":"Dropping original feature set after the split using get_dummies. Relabeling all new feature sets.","68bbe410":"Oversampling dataset via SMOTE. This oversamples the dataset. We did not want to downsample since we would lose important parts of the dataset.","c28ac6f9":"Verifying null values were deleted.","dd34a4ae":"Obtaining the accuracy score for bagging method via cross_val_score","0696b7b3":"Identifying null values in our dataset. BMI column had missing values therefore deleting the entire row was the decision. We have enough data that deleting will not affect our classification significantly.","bdf86854":"Train test split our dataset","dd74f901":"Feature set and Label set. Verifying the number of rows present.","aee92a58":"Dropping ID column. This feature does not correspond with the data analysis being conducted.","a9e54836":"Importing healthcare dataset","6affba71":"Feature scaling to standardize the independent features\nRescale feature with distribution value of 0 mean and variance equal to 1","96e57518":"We have identified that stroke label set is very unbalanced which could affect how we process our data. We need to fix this issue.","01290fe7":"Setting up the bagging classifier for ensemble method.","3098ffd9":"These are the libraries used to help achieve this project"}}