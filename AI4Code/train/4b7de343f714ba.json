{"cell_type":{"87d08f31":"code","e887af10":"code","7d884f8c":"code","c54d3f3d":"code","af2bb3b8":"code","fe6add89":"code","241e8e98":"code","38488e60":"code","9f139ef4":"code","6c1b552a":"code","88ce6559":"code","95cd0924":"code","c69b6f2d":"code","32623f3f":"code","c5e1c889":"code","c9c88389":"code","7e144237":"code","bc3674e7":"code","30ea5f23":"code","d810d8ed":"code","a55e9242":"code","2175887f":"code","fc62043a":"code","c28fbe5f":"markdown","5803fd0a":"markdown","b4077ff6":"markdown","00d23975":"markdown","ee0c1cb7":"markdown","37547411":"markdown"},"source":{"87d08f31":"# check the target data is exist\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        fpath = os.path.join(dirname, filename)\n        print(fpath)","e887af10":"# import packages\nimport os\nimport pandas as pd\nimport numpy\nimport matplotlib.pyplot as plt\n%matplotlib inline","7d884f8c":"# load data\nfpath = '\/kaggle\/input\/data-structured\/bank.csv'\ndf = pd.read_csv(fpath,sep=',')\n\n# check index & data types\nprint('[ data shape ]\\n  {}\\n'.format(df.shape))\nprint('[ data type ]\\n%s' % df.dtypes)\ndf.head()","c54d3f3d":"# summary about dataset\ndf.describe()","af2bb3b8":"# check null values by col\nA = df.isnull().any(axis=0)\nA = pd.DataFrame(A,columns=['exist_null'])\n# check number of null values by col\nB = df.isnull().sum(axis=0)\nB = pd.DataFrame(B,columns=['num_of_null'])\n# merge data\nmerge = pd.concat([A,B],axis=1)\nmerge","fe6add89":"# check null values by rows\nA = df.isnull().any(axis=1)\nA = pd.DataFrame(A,columns=['exist_null'])\n# check number of null values by rows\nB = df.isnull().sum(axis=1)\nB = pd.DataFrame(B,columns=['num_of_null'])\n# merge data\nmerge = pd.concat([A,B],axis=1)\nmerge","241e8e98":"# draw histogram graph to check number of data distribution\nplt.hist(df['age'])\nplt.xlabel('age')\nplt.ylabel('number of data')\nplt.show()","38488e60":"# draw scatter graph & check corelation\nplt.scatter(df['age'],df['balance'])\nplt.xlabel('age')\nplt.ylabel('balance')\nplt.show()\n\ndf[['age','balance']].corr()","9f139ef4":"# string data ratio (decending, normalize)\nprint(df['education'].value_counts(ascending=False, normalize=True))\n\n# draw pie diagram\nedu_labels = df['education'].value_counts(ascending=False,normalize=True).index\nedu_vals = df['education'].value_counts(ascending=False,normalize=True).values\nplt.pie(edu_vals,labels=edu_labels)\nplt.axis('equal')\nplt.show()","6c1b552a":"job_labels = df['job'].value_counts(ascending=False,normalize=True).index\njob_vals = df['job'].value_counts(ascending=False,normalize=True).values\n\nplt.pie(job_vals, labels=job_labels)\nplt.axis('equal')\nplt.show()","88ce6559":"# y parameter ratio\ny_labels = df['y'].value_counts(ascending=False,normalize=True).index\ny_vals = df['y'].value_counts(ascending=False,normalize=True).values\n\nplt.pie(y_vals,labels=y_labels)\nplt.axis('equal')\nplt.show()","95cd0924":"# box & whisker plot\ny_yes = df[df['y']=='yes']\ny_no = df[df['y'] =='no']\n\ny_age = [y_yes['age'],y_no['age']]\ny_age[:5]\n\nplt.boxplot(y_age)\nplt.xlabel('y')\nplt.ylabel('age')\nax = plt.gca()\nplt.setp(ax,xticklabels =['yes','no'])\nplt.show()","c69b6f2d":"# drop null values\ndf = df.dropna(subset=['job','education'])\n\n# replace values of 'contact' NaN -> unknown\ndf = df.fillna({'contact':'unknown'})\n\n# delete too many null value column\ndf = df.dropna(thresh=2400,axis=1)\n\n# delete abnormal values\ndf = df[df['age'] >= 18]\ndf = df[df['age']<100]\n\n# replace string to binary\ndf = df.replace('yes','1')\ndf = df.replace('no','0')\n\n# string values to one-hot encoding\ndf_job = pd.get_dummies(df['job'])\ndf_marital = pd.get_dummies(df['marital'])\ndf_education = pd.get_dummies(df['education'])\ndf_contact = pd.get_dummies(df['contact'])\ndf_month = pd.get_dummies(df['month'])\n# df_month.head()\n\nprint(df.shape)\ndf.head()\n\n## create new dataset\ntmp1 = df[['age','default','balance','housing','loan','day','duration',\n                'campaign','pdays','previous','y']]\n\ntmp2 = pd.concat([tmp1,df_marital],axis=1)\ntmp3 = pd.concat([tmp2,df_education],axis=1)\ntmp4 = pd.concat([tmp3,df_contact],axis=1)\ndf_new = pd.concat([tmp4,df_month],axis=1)\ndf_new.head()\n\n# save preprocessed data\ndf_new.to_csv('bank-prep.csv',index=False)","32623f3f":"# install 'imbalanced-learn' package\n!pip install imbalanced-learn","c5e1c889":"# load pre-processed data\nimport os\nrdir = '\/kaggle\/input\/data-structured'\nfname = 'bank-prep.csv'\nfdir = os.path.join(rdir,fname)\ndf = pd.read_csv(fdir,sep=',')\ndf_new = pd.read_csv(fdir,sep=',')","c9c88389":"# under sampling\nimport numpy as np\nfrom imblearn.under_sampling import RandomUnderSampler\n\nX = np.array(df_new.drop('y',axis=1))\nY = np.array(df_new[['y',]])\n\nprint(np.sum(Y==1),np.sum(Y==0))\n\nsampler = RandomUnderSampler(random_state=42)\nX,Y = sampler.fit_resample(X,Y)\nprint(np.sum(Y==1),np.sum(Y==0))","7e144237":"# over sampling\nimport numpy as np\nfrom imblearn.over_sampling import RandomOverSampler\n\nX = np.array(df_new.drop('y',axis=1))\nY = np.array(df_new[['y',]])\n\nprint(np.sum(Y==1),np.sum(Y==0))\n\nsampler = RandomOverSampler(random_state=42)\nX,Y = sampler.fit_resample(X,Y)\nprint(np.sum(Y==1),np.sum(Y==0))","bc3674e7":"# Decision Tree\n\nfrom sklearn.model_selection import KFold\nfrom sklearn import tree\nfrom sklearn.metrics import accuracy_score\n\nkf = KFold(n_splits=10,shuffle=True)\nscores =[]\n\nfor train_id,test_id in kf.split(X):\n    x = X[train_id]\n    y = Y[train_id]\n    clf = tree.DecisionTreeClassifier()\n    clf.fit(x,y)\n    print(clf.get_params)\n    pred_y = clf.predict(X[test_id])\n    score = accuracy_score(Y[test_id],pred_y)\n    scores.append(score)\n\nscores = np.array(scores)\nprint(scores)\nprint(scores.mean(),scores.std())","30ea5f23":"from sklearn.metrics import recall_score, precision_score\n\nprint('Recall Score    : ',recall_score(Y[test_id],pred_y))\nprint('Precision Score : ',precision_score(Y[test_id],pred_y))","d810d8ed":"# Grid Search to find best params\n\nfrom sklearn.model_selection import GridSearchCV\n\nparams = {\n    'criterion':['entropy'],\n    'max_depth':[2,4,6,8,10],\n    'min_samples_leaf':[10,20,30,40,50],\n}\n\nclf_gs = GridSearchCV(tree.DecisionTreeClassifier(),params,\n                     cv=KFold(n_splits=10,shuffle=True),scoring='accuracy')\n\nclf_gs.fit(X,Y)","a55e9242":"print(clf_gs.best_score_)\nprint(clf_gs.best_params_)","2175887f":"clf_best = tree.DecisionTreeClassifier(\n                    criterion = 'entropy',max_depth=10,min_samples_leaf=20)\nclf_best.fit(X,Y)\nfi = clf_best.feature_importances_\nprint(len(fi))","fc62043a":"features = df_new.keys() \nimportances = clf_best.feature_importances_\nindices = np.argsort(importances)\n\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","c28fbe5f":"# 5. Decision Tree Classification","5803fd0a":"# Contents\n\n1. about dataset\n2. Used Packages\n3. Check & Pre-Processing Dataset\n4. Balancing Dataset\n5. Dicision Tree Classification","b4077ff6":"# 1. About Dataset\n    > bank.csv\n        - purpose : data for practice machine learning\n        - format : csv\n        - keys : age,job,marital,edu\n        - about : '16 x parameters' for '1 y parameter'\n    ","00d23975":"# 2. Used Packages ","ee0c1cb7":"# 4. Balancing Dataset","37547411":"# 3. Check & Pre-Processing Dataset"}}