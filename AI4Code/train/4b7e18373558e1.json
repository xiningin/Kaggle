{"cell_type":{"3b896c7d":"code","24210bb5":"code","7a9eaf27":"code","f509099d":"code","5f0be9f7":"code","f64c456b":"code","eb770880":"code","c79473fc":"code","3e494cc1":"code","9340e5f0":"code","808eab51":"code","1001cb66":"code","967b1f2b":"code","65dc7ca7":"code","2fa07165":"code","ec61f865":"code","f7996301":"code","c90db165":"code","bc3e24d8":"code","7f7ecdc7":"code","9050acfe":"code","b059c482":"code","f422dcc2":"code","df9d77af":"code","71ca54af":"code","193e508a":"code","a6ba62b1":"code","dafaa6f4":"code","655d449d":"code","0fbbd76b":"code","8553c026":"code","be2417e3":"code","d96aa23c":"code","8fa14356":"code","05bec346":"code","f4df14aa":"code","687cf312":"code","14bef7bc":"code","874eeb8e":"code","dcaaa3d9":"code","eca3039a":"code","fe11c05b":"code","c5c8f909":"code","6f579e75":"code","33b7cccb":"code","6207004e":"code","fa9cfce8":"code","be30010a":"code","8da6bf4c":"code","dc062200":"code","9dd96244":"code","9335680e":"code","db00de86":"code","550e81d0":"code","8319da03":"code","cb521182":"code","4cb5190e":"code","9326fdf9":"code","c68ff612":"code","5082629e":"code","71fdfba2":"code","f89526d7":"code","01b52e28":"code","bbd03cba":"code","fb5a42e1":"code","a876ab9d":"code","4466413f":"code","8173c577":"code","7d634724":"code","b2fc027f":"code","e37af406":"code","d39745ca":"code","7f8ebc37":"code","42625ae6":"code","ef93bdf1":"code","af7bb362":"code","a382a0e8":"code","a36e8952":"code","5d9be5a5":"code","755c277d":"code","52b51e49":"code","18dce973":"code","9718e5a8":"code","b4c94a4d":"code","1f487986":"code","ca3699af":"code","3fd13621":"code","9124bbfd":"code","6a4e889c":"code","1dfaabf7":"code","8a4aab31":"code","4d66dd54":"code","30429031":"code","98dc914a":"code","a627689d":"code","b9070518":"code","93a18873":"code","b2544660":"code","5c2ac38b":"code","7d22db65":"code","5b62e521":"code","5c96a1a4":"code","5fbd472b":"code","1e61c68a":"code","309e3edc":"code","cd5a276d":"code","7d9bd0bc":"code","666fda97":"code","d30ee422":"code","65a29189":"code","0d95a8b0":"code","727a13b7":"code","90ee1433":"code","f113976c":"code","e067efc1":"code","22472a5a":"code","8cc4a71e":"code","174e267c":"code","8f980e45":"code","8ea9ae8b":"code","47ed3fbb":"code","e3618646":"code","9e1e07a4":"code","5315d17f":"code","64345653":"code","974cecd0":"code","073c87a6":"code","6010f152":"code","1c8ffd80":"code","c7e3dce9":"code","806d571d":"code","a05f16af":"code","88442d2a":"code","577fd09e":"code","fdabb063":"code","dcfad5b5":"code","9a04d788":"code","cce53eef":"code","0447480d":"code","4604793d":"code","7ca60a27":"code","ee0eb936":"code","5a85e97c":"code","77da5aa4":"code","5d2bcb87":"code","61d50c8d":"code","d5262a7d":"code","4274de40":"code","c9120a3a":"code","7bc7fe87":"code","11e9d00d":"code","cbb00e04":"code","2f906f97":"code","b7d9a2c9":"code","f49f3a9e":"code","00264a61":"code","278f5b22":"code","e35c1f97":"code","978cbdc8":"code","ee588fac":"code","bbe95f49":"code","4a72b474":"code","a297ddd5":"code","a407cd99":"code","056430a2":"code","b5ab7c91":"code","f6025816":"code","bccc19e7":"code","1916d41c":"code","9035d8b9":"code","9d59818b":"code","606d38c6":"code","9724bc44":"code","e3c408f9":"code","6ae7e8c7":"code","f8a6c908":"code","3a698b03":"code","c1d88704":"code","bc3ae810":"code","677371b5":"code","be4de70a":"code","b39c2893":"code","4e89abc9":"code","c4e8bc18":"code","ebcb7968":"markdown","7babe897":"markdown","b359cd83":"markdown","b2168586":"markdown","964be7fe":"markdown","9f59dd2e":"markdown","0a1a3d6b":"markdown","3a95d458":"markdown","7534188f":"markdown","805655d8":"markdown","c77efbdd":"markdown","27e670c3":"markdown","35c7f37b":"markdown","eff0e51f":"markdown","5362d1e5":"markdown","43d1fbcb":"markdown","8af634e4":"markdown","57d04e17":"markdown","b94f5b1d":"markdown","54b52cc2":"markdown","e49f35c5":"markdown","86a30362":"markdown","55486dbf":"markdown","9c81c012":"markdown","09616a88":"markdown","15c2910c":"markdown","f813b2a9":"markdown","c564298f":"markdown","363250f9":"markdown","f469bd79":"markdown","34de1ff9":"markdown","294c4c92":"markdown","d0a1bbf9":"markdown","950af6f9":"markdown","64729b3a":"markdown","9c235e3a":"markdown","223dcad5":"markdown","53219859":"markdown","436c4ae2":"markdown","e1afe75e":"markdown","5ab5fedb":"markdown","e2190652":"markdown","597cd3ca":"markdown","3984bfc5":"markdown","5260d51c":"markdown","54a4cc3a":"markdown","b91f2995":"markdown","202aad80":"markdown","d51d868d":"markdown","313afa21":"markdown","39616d40":"markdown","f737f92c":"markdown","6a53ab46":"markdown","810651b7":"markdown","8bf9bc1a":"markdown","39423162":"markdown","7c39cca0":"markdown","96e5e7d3":"markdown","56d649d7":"markdown","ff0280fe":"markdown","2dcfd7aa":"markdown","f43307b4":"markdown","c6177c85":"markdown","6e7dec44":"markdown","443641c1":"markdown","721dab70":"markdown","36106c05":"markdown","98a42cb8":"markdown","743ac4fe":"markdown","bd295b1c":"markdown","ee07f112":"markdown","2dfdc78e":"markdown","70d9ce3f":"markdown","00ffa723":"markdown","bb5dc6b5":"markdown","a6f8d7da":"markdown","ba6de25a":"markdown","54525b0e":"markdown","9737f549":"markdown","0e4a9feb":"markdown","5107220f":"markdown","04b70f0d":"markdown","dbf0b012":"markdown","3786750f":"markdown","22099236":"markdown","c8739cdb":"markdown","822bb690":"markdown","b66cf550":"markdown","da85c747":"markdown","f6e93224":"markdown","361bacc2":"markdown","6524c4c7":"markdown","78d13d80":"markdown","9ee36207":"markdown","4bf1aab2":"markdown","2668a9a9":"markdown","581a95c7":"markdown","03db9b15":"markdown","5f87b058":"markdown","e0648472":"markdown","92728fbc":"markdown","06944617":"markdown","20005e87":"markdown","e83dc408":"markdown","e20f4fc7":"markdown","85ba3ad5":"markdown","ef8251d4":"markdown","031088cc":"markdown","b12d817a":"markdown","d3ff703a":"markdown","b8220298":"markdown","c5235c55":"markdown","777699aa":"markdown"},"source":{"3b896c7d":"!pip install unidecode\n!pip install autocorrect\n!pip install google_trans_new\n!pip install ann_visualizer\n!pip install googletrans","24210bb5":"# Importing all the necessary libraries\nimport warnings # to ignore warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import tokenize, stem\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\nimport re\nimport string\nfrom nltk.probability import FreqDist\nfrom nltk.util import ngrams\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\nimport unicodedata\nimport unidecode\nfrom autocorrect import Speller\nfrom string import punctuation\nfrom wordcloud import WordCloud, STOPWORDS\nfrom textblob import TextBlob\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom gensim.models import Word2Vec\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Flatten, Bidirectional, GlobalMaxPool1D\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.utils import to_categorical\n\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\nfrom google_trans_new import google_translator  \n\nfrom ann_visualizer.visualize import ann_viz;\nfrom keras.utils.vis_utils import plot_model\n","7a9eaf27":"nltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger');","f509099d":"#the dataset is on github\npath = '..\/input\/industrial-safety-and-health-analytics-database\/IHMStefanini_industrial_safety_and_health_database_with_accidents_description.csv'\ndf = pd.read_csv(path)\n\n# A quick look at the data\ndf.head(10)","5f0be9f7":"# Drop irrelevant column Unnamed:0\ndf.drop('Unnamed: 0', axis=1, inplace=True)","f64c456b":"# Rename the columns in a correct fashion\ndf.rename(columns = { 'Data' : 'Date',\n                      'Industry Sector' : 'Industry_Sector', \n                      'Accident Level': 'Accident_Level',\n                      'Countries' : 'Country',\n                      'Genre' : 'Gender',\n                      'Potential Accident Level' : 'Potential_Accident_Level',\n                      'Employee or Third Party' : 'Employee_Type', \n                      'Critical Risk' : 'Critical_Risk'}, inplace = True)","eb770880":"# Defining a function which gives the dataframe with all the analysis.\ndef indetailtable(df):\n    print('Dataset Shape: {}'.format(df.shape))\n    print('Total Number of rows in dataset: {}'.format(df.shape[0]))\n    print('Total Number of columns in dataset: {}'.format(df.shape[1]))\n    print('Various datatypes present in the dataset are: {}'.format(df.dtypes.value_counts()))\n    \n    summary = pd.DataFrame(df.dtypes, columns = ['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary['Name'] = summary[['Name', 'dtypes']]\n    summary['Misssing_values'] = df.isnull().sum().values\n    summary['Unique_values'] = df.nunique().values\n    summary['Duplicate_values'] = df.duplicated().sum()\n    summary['1st value'] = df.loc[0].values\n    summary['2nd Value'] = df.loc[1].values\n    summary['423th Value'] = df.iloc[-2].values\n    summary['424th Value'] = df.iloc[-1].values\n    \n    return summary","c79473fc":"# Calling the above defined function by giving the input as a dataframe\nindetailtable(df)","3e494cc1":"# Checking those the duplicates we found in above dataframe\nprint('There are {} duplicates in the dataset as below'.format(df.duplicated().sum()))\ndf[df.duplicated()]","9340e5f0":"# Drop the duplicate values from the dataframe using drop_duplicates function.\ndf.drop_duplicates(inplace=True)\nprint('After removing duplicates the shape of the dataset is:',df.shape)","808eab51":"# Calling the function again to get some more insights\nindetailtable(df)","1001cb66":"# Printing all the duplicate values (keep=False)\nprint('There are still {} duplicates in the dataset as below'.format(df.duplicated(subset=['Description'],keep=False).sum()))\ndf[df.duplicated(subset=['Description'],keep=False)].sort_values(by='Description')","967b1f2b":"# Dropping the duplicates we detected above.\ndf.drop_duplicates(subset=['Description'], keep='first', inplace=True)\nprint('After removing duplicates the shape of the dataset is:', df.shape)","65dc7ca7":"# Calling the function again to verify everything is correct.\nindetailtable(df)","2fa07165":"# Just checking shape of the dataframe\ndata_shape = df.shape\nprint('Data set contains {} number of rows and {} number of columns' .format(data_shape[0], data_shape[1]))","ec61f865":"# Check the info to print a concise summary of a DataFrame\ndf.info()","f7996301":"# Looping through all the columns in the dataframe and checking counts of unique values.\nfor col in df.columns:\n    if (col!='Description') and (col!='Date'):\n        print(df[col].value_counts())\n        print('*'*50)","c90db165":"# Replace Level 6 value to Level 5\ndf['Potential_Accident_Level'] = df['Potential_Accident_Level'].replace('VI', 'V')","bc3e24d8":"# Creating a function to make univariate plots for analysis\ndef plot(col, title, palette, edgecolor):\n\n    value = df[col].value_counts()[1]\n    plt.figure(figsize = (20,15))\n    plt.subplot(2,2,1)\n    sns.countplot(df[col], palette = palette, edgecolor = edgecolor, order=df[col].value_counts().index, alpha = 1.0, saturation=1);\n    sns.lineplot(df[col].value_counts().index, df[col].value_counts().values, palette='seismic')\n    plt.title(title);\n\n    plt.subplot(2,2,2)\n    # colors = ['yellowgreen', 'violet', 'orange', 'grey', 'cyan']\n\n    plt.pie(df[col].value_counts(), autopct = \"%.2f\",\n       labels = df[col].value_counts().index , shadow = True, explode = [0.1]*len(df[col].value_counts().index), startangle = -135);\n    plt.title(title);\n    plt.show()","7f7ecdc7":"plot('Industry_Sector', 'Industry Sector where Accident happened', 'RdYlBu', 'orange')","9050acfe":"plot('Gender', 'Which Gender has been affected more by accidents', 'Pastel2', 'red')","b059c482":"plot('Employee_Type', 'What catagory of employee has been affected more by accidents', 'magma_r', 'green')","f422dcc2":"plot('Country', 'What catagory of country has been affected more by accidents', 'coolwarm', 'yellow')","df9d77af":"# Locals Count\nplt.figure(figsize = (16,7))\nax = sns.countplot(x = df['Local'], order=df['Local'].value_counts().index, palette = 'autumn_r', edgecolor='.4', saturation=1);\nsns.lineplot(df['Local'].value_counts().index, df['Local'].value_counts().values)\n\nplt.title('Frequency of accidents in various Localities');\nplt.xticks(rotation = 'vertical');\n\ntotal = sum(df['Local'].value_counts())\nfor p in ax.patches:\n    ax.annotate('{:.1f}%'.format(100*p.get_height()\/total), \n                (p.get_x(), p.get_height()),\n                 size=12,\n                 xytext = (10, 10), \n                 textcoords = 'offset points')","71ca54af":"# Critical Risk Count\nplt.figure(figsize = (16,7))\nax = sns.countplot(x = df['Critical_Risk'], order=df['Critical_Risk'].value_counts().sort_values(ascending = True).index, palette = 'gist_heat_r');\nplt.title('What is the critical risk of accidents and its distribution ?');\nplt.xticks(rotation = 'vertical');\n\ntotal = sum(df['Critical_Risk'].value_counts())\nfor p in ax.patches:\n    ax.annotate('{}'.format(p.get_height()), \n                (p.get_x(), p.get_height()),\n                 size=12,\n                 xytext = (4, 8), \n                 textcoords = 'offset points')","193e508a":"plot('Accident_Level', 'Level of Accident', 'cool', 'green')","a6ba62b1":"plot('Potential_Accident_Level', 'Severite of Accident that could have been occured', 'afmhot', 'red')","dafaa6f4":"plt.figure(figsize=(12,5))\nax = sns.countplot(df['Country'], hue=df['Potential_Accident_Level'], hue_order=df['Potential_Accident_Level'].value_counts().sort_index().index, palette='pastel', edgecolor='.4', saturation=1)\n\ntotal = sum(df['Country'].value_counts())\nfor p in ax.patches:\n    ax.annotate('{}'.format(p.get_height()), \n                (p.get_x(), p.get_height()),\n                 size=12,\n                 xytext = (0, 3), \n                 textcoords = 'offset points')\n\nplt.title('Potential Accident level counts by Country');\nplt.ylabel('Count');\nplt.legend(loc='upper right')","655d449d":"plt.figure(figsize=(28,8))\nax = sns.countplot(df['Local'], hue=df['Potential_Accident_Level'], hue_order=df['Potential_Accident_Level'].value_counts().sort_index().index, palette='tab20c_r', edgecolor='.4', saturation=1)\n\ntotal = sum(df['Local'].value_counts())\nfor p in ax.patches:\n    ax.annotate('{:.0f}'.format(p.get_height()), \n                (p.get_x(), p.get_height()),\n                 size=12,\n                 xytext = (0, 3), \n                 textcoords = 'offset points')\n\nplt.title('Potential Accident level counts by Local');\nplt.ylabel('Count');\nplt.legend(loc='upper right')","0fbbd76b":"plt.figure(figsize=(12,5))\nax = sns.countplot(df['Industry_Sector'], hue=df['Potential_Accident_Level'], hue_order=df['Potential_Accident_Level'].value_counts().sort_index().index, palette='cool', edgecolor='1.0', saturation=1)\n\ntotal = sum(df['Industry_Sector'].value_counts())\nfor p in ax.patches:\n    ax.annotate('{}'.format(p.get_height()), \n                (p.get_x(), p.get_height()),\n                 size=12,\n                 xytext = (0, 3), \n                 textcoords = 'offset points')\n\nplt.title('Potential Accident level counts by Industry Sector');\nplt.ylabel('Count');\nplt.legend(loc='upper right')","8553c026":"plt.figure(figsize=(12,5))\nax = sns.countplot(df['Employee_Type'], hue=df['Potential_Accident_Level'], hue_order=df['Potential_Accident_Level'].value_counts().sort_index().index, palette='Greys_r', edgecolor='.5', saturation=1);\n\ntotal = sum(df['Employee_Type'].value_counts())\nfor p in ax.patches:\n    ax.annotate('{}'.format(p.get_height()), \n                (p.get_x(), p.get_height()),\n                 size=12,\n                 xytext = (0, 3), \n                 textcoords = 'offset points')\n\nplt.title('Potential Accident level counts by Employee Type');\nplt.ylabel('Count');\nplt.legend(loc='upper right')","be2417e3":"plt.figure(figsize=(10,22))\nax = sns.countplot(y = df['Critical_Risk'], hue=df['Potential_Accident_Level'], hue_order=df['Potential_Accident_Level'].value_counts().sort_index().index, palette='pastel', edgecolor='.4', saturation=1)\n\n# total = sum(df['Critical_Risk'].value_counts())\n# for p in ax.patches:\n#     ax.annotate('{}'.format(p.get_height()), \n#                 (p.get_x(), p.get_height()),\n#                  size=12,\n#                  xytext = (0, 3), \n#                  textcoords = 'offset points')\n\nplt.title('Potential Accident level counts by Country');\nplt.ylabel('Count');\nplt.legend(loc='upper right')","d96aa23c":"# Filtering the dataframe by Gender and Employee type and taking the count.\ndf1 = df.groupby(['Gender','Potential_Accident_Level'])['Potential_Accident_Level'].count()\n\n# 1.unstack() Function in dataframe unstacks the row to columns.\n# 2.applying the row total and taking the ratio of male and female employees.\n# 3.resetting the index\n\ndf1 = df1.unstack().apply(lambda x: round(x \/ x.sum()*100), axis=1).reset_index() \ndf1 = pd.melt(df1, ['Gender']).fillna(0) # melt() function unpivots a DataFrame from wide format to long format\ndf1","8fa14356":"plt.figure(figsize=(12,5))\nsns.barplot(df1['Gender'], df1['value'], hue=df1['Potential_Accident_Level'], palette='pastel', edgecolor='.4', saturation=1);\nplt.title('Potential Accident level by Gender Count');\nplt.ylabel('Percentage');","05bec346":"# Getting the value counts of each of the columns\naccident = df['Accident_Level'].value_counts()\npotential = df['Potential_Accident_Level'].value_counts()\n\n# Merging both values of the dataframe\nacc_pot = pd.concat([accident, potential], axis=1,sort=False).fillna(0).reset_index()\n\n# Need to melt down columns so that they can be treated as hue in plotting\nacc_pot = pd.melt(acc_pot, ['index'], var_name='Accident type', value_name='count')\nacc_pot","f4df14aa":"#plotting\nplt.figure(figsize=(10,5))\nsns.barplot(acc_pot['index'], acc_pot['count'], hue=acc_pot['Accident type'], palette='Paired', edgecolor='.1', saturation=1)\nsns.lineplot(acc_pot['index'], acc_pot['count'], hue=acc_pot['Accident type'], palette='seismic')\n\nplt.xlabel('Severity\/Levels');\nplt.ylabel('Frequency');","687cf312":"# Country by Local count\nplt.figure(figsize=(12,5))\nsns.catplot(data=df, kind='count', x='Country', hue='Local', size=4, aspect=3, edgecolor='.4', saturation=1);\nplt.title('Country by Local count');\nplt.ylabel('Frequency');","14bef7bc":"plt.figure(figsize=(12,5))\nsns.countplot(df['Industry_Sector'], hue=df['Local'], edgecolor='.4', saturation=1, palette='seismic');\nplt.title('Industry_Sector by Local count');\nplt.ylabel('Frequency')\nplt.legend(loc='best', bbox_to_anchor=(1.15, 1))","874eeb8e":"# Filtering the dataframe by Gender and Employee type and taking the count.\ndf1 = df.groupby(['Gender','Employee_Type'])['Employee_Type'].count()\n\n# 1.unstack() Function in dataframe unstacks the row to columns.\n# 2.applying the row total and taking the ratio of male and female employees.\n# 3.resetting the index\n\ndf1 = df1.unstack().apply(lambda x: round(x \/ x.sum()*100), axis=1).reset_index() \ndf1 = pd.melt(df1, ['Gender']) # melt() function unpivots a DataFrame from wide format to long format\ndf1","dcaaa3d9":"plt.figure(figsize=(12,5))\nsns.barplot(df1['Gender'], df1['value'], hue=df1['Employee_Type'], palette='pastel', edgecolor='.4', saturation=1);\nplt.title('Employeee type by Gender Count');\nplt.ylabel('Percentage');","eca3039a":"# Filtering the dataframe by Gender and Employee type and taking the count.\ndf1 = pd.melt(df.groupby(['Gender','Industry_Sector'])['Industry_Sector'].count().unstack().apply(lambda x: round(x \/ x.sum()*100), axis=1).reset_index(),['Gender'])\ndf1","fe11c05b":"plt.figure(figsize=(12,5))\nsns.barplot(df1['Gender'], df1['value'], hue=df1['Industry_Sector'], palette='tab20c_r', edgecolor='.4', saturation=1);\nplt.title('Industry Sector by Gender Count');\nplt.ylabel('Percentage');","c5c8f909":"plt.figure(figsize=(10,10))\nsns.countplot(y = df['Critical_Risk'], hue=df['Gender'], palette='gist_rainbow_r')\nplt.title('Critical Risk counts by Gender');","6f579e75":"# Correlation\nle = LabelEncoder()\ndf_enc = df.apply(le.fit_transform)\n\nplt.figure(figsize=(12,12))\nplt.title('Correlation_Matrix', fontsize=20)\nsns.heatmap(df_enc.corr(), square=True, cmap='bone', annot=True, linewidth=0.2);","33b7cccb":"# Count of Accidents grouped by Accident Level and Potential accident level\nplt.figure(figsize = (16,7))\nsns.heatmap(pd.crosstab(df.Accident_Level, df.Potential_Accident_Level), square=True, cmap='cool', annot=True, linewidth=0.1);","6207004e":"# Count of Critical Risks grouped by Industry sectors\ndf.pivot_table(index='Critical_Risk', columns='Industry_Sector', aggfunc='count').fillna(0)['Accident_Level'].T","fa9cfce8":"fig, ax = plt.subplots(figsize=(12,10))\nsns.scatterplot(y=\"Potential_Accident_Level\", x=\"Critical_Risk\", hue=\"Industry_Sector\",style='Industry_Sector',\n                size = 'Accident_Level',  data=df, ax=ax, sizes=(50, 300), palette = 'tab10')\nax.set_title(\"Accident_Level vs (potential accident level, industry sector, Critical_Risk )\")\nax.legend(loc=\"upper left\", bbox_to_anchor=(1,1))\nplt.xticks(rotation = 'vertical');\nplt.show()","be30010a":"df1 = df.copy()\ndf1['Potential_Accident_Level'] = LabelEncoder().fit_transform(df1['Potential_Accident_Level']).astype(np.int8) # Just converting one of the column to numerical data type\n\nplt.figure(figsize=(12,10))\nsns.violinplot(x=df1[\"Potential_Accident_Level\"], y=df1[\"Industry_Sector\"], hue = df1['Employee_Type'] ,palette='rainbow');\nplt.title(\"Presence of Male and Female employees in various location with respect to countries\")\nplt.legend(loc='lower right')","8da6bf4c":"# Convert string Date time into Python Date time object. \ndf['Date'] = pd.to_datetime(df.Date)","dc062200":"# Accidents may increase or decrease throughout the year or month, so we are adding datetime features such as year and month\ndf['Month'] = df.Date.dt.month\ndf['Year'] = df.Date.dt.year\ndf['Day'] = df.Date.dt.day\ndf['Weekday'] = df.Date.dt.weekday\n\n# df.drop(columns =['Date'], inplace = True)","9dd96244":"# Basic trend plot accumulating accidents of all the countries day wise\ndf.set_index('Date', inplace=True)\ndf1= df.Country.resample('24H').count()\n                        \nplt.figure(figsize=(18,6))\nsns.lineplot(x = df1.index, y = df1, color='blue')\nplt.title('Number of Accidents\/Day (all countries)')","9335680e":"k=1\nplt.figure(figsize=(17,10))\n\n# using for loop to iterate over defined columns in the dataframe and create the plot\n\nfor col in df[['Year', 'Month', 'Day', 'Weekday']]:\n    plt.subplot(2,2,k)\n    sns.countplot(df[col], palette='magma',edgecolor = '.6', alpha = 1.0)\n    # sns.lineplot(df[col].value_counts().index, df[col].value_counts().values, palette='seismic')\n    plt.title(col+' count')\n    k=k+1\n\nprint('Months in 2016:',df[df['Year']==2016]['Month'].unique())\nprint('Months in 2017:',df[df['Year']==2017]['Month'].unique())\n\n# pd.DataFrame(df[df['Year']==2016]['Month']).value_counts().sort_index()\n# pd.DataFrame(df[df['Year']==2017]['Month']).value_counts().sort_index()","db00de86":"ct = pd.crosstab(columns=df[\"Potential_Accident_Level\"],index=df[\"Month\"])\nax = ct.plot(kind=\"bar\",stacked=True,figsize=(13,6))\nax.set_ylabel('No. of accidents')\nax.set_xlabel('Month')","550e81d0":"# Removing the columns which are not required for modelling\ndf = df.reset_index()\ndf.drop(columns=['Date', 'Day', 'Weekday', 'Year', 'Month'], inplace= True)","8319da03":"df","cb521182":"df['Description'][0]","4cb5190e":"stopwords = set(stopwords.words(\"english\"))","9326fdf9":"# Defining a function for NLP preprocessing\ndef des_cleaning(text):\n\n    # Initialize the object for Lemmatizer class\n    lemmatizer = nltk.stem.WordNetLemmatizer()\n\n    # Set the stopwords to English\n    stopwords = nltk.corpus.stopwords.words('english')\n\n    # Normalize the text in order deal with accented words and unicodes\n    text = (unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore').lower())\n\n    # Consider only alphabets and numbers from the text\n    words = re.sub(r'[^a-zA-Z.,!?\/:;\\\"\\'\\s]', '', text).split()\n\n    # Consider the words which are not in stopwords of english and lemmatize them\n    lemmatizer = nltk.stem.WordNetLemmatizer()\n    lems = [lemmatizer.lemmatize(i) for i in words if i not in stopwords]\n\n    # #remove non-alphabetical characters like '(', '.' or '!'\n    # alphas = [i for i in lems if (i.isalpha() or i.isnumeric()) and (i not in stopwords)]\n\n    words = [w for w in lems if len(w)>2]\n\n    return words","c68ff612":"# Apply all the preprocessing techniques we have defined on Description columns\ntokens = des_cleaning(' '.join(df['Description'].sum().split()))\n# words = des_cleaning(''.join(str(df['Description'].tolist())))","5082629e":"# Verifying by printing first 20 words\nprint('Length of all the words:', len(tokens),'\\n')\nprint('Length of unique tokens in the dataset:', len(np.unique(tokens)),'\\n')\nnp.array(tokens[0:20])","71fdfba2":"df['NewDescription'] = df.apply(lambda x: \" \".join(des_cleaning(x.Description)), axis=1)","f89526d7":"df['NewDescription'][0]","01b52e28":"# Function to calculate ngrams\ndef extract_ngrams(data, num):\n  # Taking ngrams on Description column text and taking the value counts of each of the tokens\n  words_with_count  = nltk.FreqDist(nltk.ngrams(data, num)).most_common(30) # taking top 30 most common words\n\n  # Creating the dataframe the words and thier counts\n  words_with_count = pd.DataFrame(words_with_count, columns=['Words', 'Count'])\n\n  # Removing the brackets and commans\n  words_with_count.Words = [' '.join(i) for i in words_with_count.Words]\n\n  # words_with_count.index = [' '.join(i) for i in words_with_count.Words]\n  words_with_count.set_index('Words', inplace=True) # setting the Words as index\n\n  # Returns the dataframe which contains unique tokens ordered by their counts \n  return words_with_count","bbd03cba":"# Uni-Grams\nuni_grams = extract_ngrams(tokens, 1)\n\n# Printing top words with their counts\nuni_grams[0:10]","fb5a42e1":"# Visualising the ngrams\nuni_grams.sort_values(by='Count').plot.barh(color = 'orange', width = 0.8, figsize = (12,8));","a876ab9d":"# Bi-Grams\nbi_grams = extract_ngrams(tokens, 2)\n\n# Printing the words with their counts\nbi_grams[0:10]","4466413f":"bi_grams.sort_values(by='Count').plot.barh(color = 'green', width = 0.8, figsize = (12,8));","8173c577":"# tri-Grams\ntri_grams = extract_ngrams(tokens, 3)\n\n# Printing the words with their counts\ntri_grams[0:10]","7d634724":"tri_grams.sort_values(by='Count').plot.barh(color = 'blue', width = 0.8, figsize = (12,8));","b2fc027f":"# Dividing the tokens with respect to Industry Sector from the description text\ntokens_metals = des_cleaning(' '.join(df[df.Industry_Sector=='Metals']['Description'].sum().split()))\ntokens_mining = des_cleaning(' '.join(df[df.Industry_Sector=='Mining']['Description'].sum().split()))","e37af406":"print('Total number of words in Metals category:', len(tokens_metals))\nprint('Total number of words in Mining category:',len(tokens_mining))","d39745ca":"# Extracting unigrams on metals category\nunigrams_metals = extract_ngrams(tokens_metals, 1).reset_index()\n\n# Extracting unigrams on mining category\nunigrams_mining = extract_ngrams(tokens_mining, 1).reset_index()\n\nunigrams_metals.join(unigrams_mining, lsuffix='_Metals', rsuffix='_Mining')","7f8ebc37":"# Dividing the tokens of male and female category from the description text\ntokens_male = des_cleaning(' '.join(df[df.Gender=='Male']['Description'].sum().split()))\ntokens_female = des_cleaning(' '.join(df[df.Gender=='Female']['Description'].sum().split()))","42625ae6":"print('Total number of words in Male category:', len(tokens_male))\nprint('Total number of words in Female category:',len(tokens_female))","ef93bdf1":"# Extracting unigrams on male category\nunigrams_male = extract_ngrams(tokens_male, 1).reset_index()\n\n# Extracting unigrams on female category\nunigrams_female = extract_ngrams(tokens_female, 1).reset_index()\n\n# Joining both the dataframes\nuni_male_female = unigrams_male.join(unigrams_female, lsuffix='_Male', rsuffix='_Female')\n\n#------------------------------------------------------------------------------------------\n\n# Extracting bigrams on male category\nbigrams_male = extract_ngrams(tokens_male, 2).reset_index()\n\n# Extracting unigrams on female category\nbigrams_female = extract_ngrams(tokens_female, 2).reset_index()\n\n# Joining both the dataframes\nbi_male_female = bigrams_male.join(bigrams_female, lsuffix='_Male', rsuffix='_Female')\nprint(bi_male_female)","af7bb362":"fig, axes = plt.subplots(1,4, figsize=(30, 10))\n\nsns.barplot(y = uni_male_female['Words_Male'], x = uni_male_female['Count_Male'], ax=axes[0], color='red');\naxes[0].set_title('Unigram with Male');\n\nsns.barplot(y = uni_male_female['Words_Female'], x = uni_male_female['Count_Female'], ax=axes[1], color='red');\naxes[1].set_title('Unigram with Female');\n\nsns.barplot(y = bi_male_female['Words_Male'], x = bi_male_female['Count_Male'], ax=axes[2], color='skyblue');\naxes[2].set_title('Bigram with Male');\n\nsns.barplot(y = bi_male_female['Words_Female'], x = bi_male_female['Count_Female'], ax=axes[3], color='skyblue');\naxes[3].set_title('Bigram with Male');","a382a0e8":"wordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords,\n                min_font_size = 10).generate(' '.join(des_cleaning(' '.join(df['Description'].sum().split()))))\n\nplt.figure(figsize = (10, 15), facecolor = 'white', edgecolor='blue') \nplt.imshow(wordcloud) \nplt.axis(\"off\") \n  \nplt.show()","a36e8952":"blob = TextBlob(str(df['NewDescription']))\npos_df = pd.DataFrame(blob.tags, columns = ['word', 'pos'])\npos_df = pos_df.pos.value_counts()[:20]\npos_df.head()","5d9be5a5":"pos_df.sort_values().plot.barh(color = 'blue', width = 0.8, figsize = (12,8))","755c277d":"X_train, X_test, y_train, y_test = train_test_split(df['NewDescription'], df['Potential_Accident_Level'].values, test_size=0.2, random_state=42)\n\nprint('Training utterances: {}'.format(X_train.shape[0]))\nprint('Validation utterances: {}'.format(X_test.shape[0]))","52b51e49":"# Defining a function which quickly test the fit of 6 different models on the dataset\ndef ml_models(X_train , y_train, X_test, y_test):\n\n    # creating a dictionary with different ML models\n    models = {\n        'LogReg': LogisticRegression(), \n        'Naive Bayes': GaussianNB(),        \n        'KNN': KNeighborsClassifier(),\n        'SVM': SVC(), \n        'Decision Tree': DecisionTreeClassifier(criterion='entropy',max_depth=6,random_state=100,min_samples_leaf=5),          \n        'RandomForest': RandomForestClassifier(n_estimators=100, max_depth=7),\n        'Bagging': BaggingClassifier(n_estimators=50, max_samples=.7),\n        'AdaBoost': AdaBoostClassifier(n_estimators= 50),\n        'Gradient Boost': GradientBoostingClassifier(n_estimators = 50, learning_rate = 0.05),\n        'XGBoost': XGBClassifier()\n    }\n    \n    names = []\n    scores = []\n\n    for name, model in models.items(): # Looping through each and every model\n        clf = model.fit(X_train, y_train) # Fit the models one by one\n        result = clf.score(X_test,y_test) \n\n        names.append(name)\n        scores.append(result) # Appending the test scores to the list\n\n        result_df =  pd.DataFrame({'model': names, 'accuracy': scores}) # Creating the dataframe using the model scores\n      \n    return result_df # Returns the dataframe\n","18dce973":"vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))\nX_train_bow = vectorizer.fit_transform(X_train)\nX_test_bow = vectorizer.transform(X_test)","9718e5a8":"ml_models(X_train_bow.toarray(), y_train, X_test_bow.toarray(), y_test)","b4c94a4d":"# Converting the categorical values to one-hot encoding\ny_train_new = pd.get_dummies(y_train)\ny_test_new = pd.get_dummies(y_test)","1f487986":"epochs = 5\nbatch_size = 12\nloss = \"categorical_crossentropy\"\noptimizer = \"adam\"\nmetrics = [\"accuracy\"]\n\nearly_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=3)\n\n# Build neural network\nbow_model = Sequential()\nbow_model.add(Dense(512, activation='relu'))\nbow_model.add(Dense(256, activation='relu'))\nbow_model.add(Dense(5, activation='softmax'))\nbow_model.compile(loss=loss, optimizer=optimizer, metrics= metrics)","ca3699af":"bow_model.fit(X_train_bow.toarray(), y_train_new, validation_split = 0.2, epochs=epochs, batch_size=batch_size, callbacks=[early_stopping])","3fd13621":"# Validating the model on test set\nloss, accuracy = bow_model.evaluate(X_test_bow.toarray(), y_test_new, verbose = 0)\nloss, accuracy","9124bbfd":"# ann_viz(bow_model, view=True, filename=\"bow_model\")\nplot_model(bow_model, to_file='bow_model.png', show_shapes=True, show_layer_names=True)","6a4e889c":"# Initializing TfidfVectorizer object\ntfIdfVectorizer = TfidfVectorizer()\nX_train_tf = tfIdfVectorizer.fit_transform(X_train)\nX_test_tf = tfIdfVectorizer.transform(X_test)","1dfaabf7":"ml_models(X_train_tf.toarray(), y_train, X_test_tf.toarray(), y_test)","8a4aab31":"epochs = 5\nbatch_size = 12\nloss = \"categorical_crossentropy\"\noptimizer = \"adam\"\nmetrics = [\"accuracy\"]\n\nearly_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=3)\n\n# Build neural network\ntfidf_model = Sequential()\ntfidf_model.add(Dense(512, activation='relu'))\ntfidf_model.add(Dense(256, activation='relu'))\ntfidf_model.add(Dense(5, activation='softmax'))\ntfidf_model.compile(loss=loss, optimizer=optimizer, metrics= metrics)","4d66dd54":"tfidf_model.fit(X_train_tf.toarray(), y_train_new, validation_split = 0.2, epochs=epochs, batch_size=batch_size, callbacks=[early_stopping])","30429031":"# Validating the model on test set\nloss, accuracy = tfidf_model.evaluate(X_test_tf.toarray(), y_test_new, verbose = 0)\nloss, accuracy","98dc914a":"# ann_viz(tfidf_model, view=True, filename=\"tfidf_model\")\nplot_model(tfidf_model, to_file='tfidf_model.png', show_shapes=True, show_layer_names=True)","a627689d":"# Converting the words back to the sentence form for modelling\ndef word_vector(tokens, size):\n    vec = np.zeros(size).reshape((1, size))\n    count = 0.\n    for word in tokens:\n        try:\n            vec += model.wv[word].reshape((1, size))\n            count += 1.\n        except KeyError: # handling the case where the token is not in vocabulary\n                         \n            continue\n    if count != 0:\n        vec \/= count\n    return vec","b9070518":"training = df['NewDescription'].tolist() # Covnerting the text to list\ntraining = [sentence.split(' ')  for sentence in training] # Splitting on each sentence which gives the multi dimensional list \nnp.array(training[0]) # Priting the first sentence","93a18873":"# Initiation of Word2Vec model\n# Every word represented by 100 dimensions, ignore words which appears less than 2 times, and use skip gram model\n\nmodel = Word2Vec(sentences=training,\n                 min_count =2,\n                 sg=0, \n                 vector_size=100)\nprint(model)","b2544660":"# The trained word vectors are stored in a KeyedVectors instance, as model.wv\nprint(model.wv['removing'].reshape(1,100)) # printing the 100 dimensional vector for first word\nvectors = model.wv.vectors # Storing the vectors of words which is trained on word2vec model","5c2ac38b":"# df.reset_index(inplace=True)\ntokenized_words = [i.split() for i in df['NewDescription']]\n\n# wordvec_arrays = np.zeros((len(tokenized_words), 100))\n\n# for i in tokenized_words:\n#     wordvec_arrays[i,:] = model.wv[tokenized_words[i]].reshape(1,100)","7d22db65":"wordvec_arrays = np.zeros((len(tokenized_words), 100))\n\nfor i in range(len(tokenized_words)):\n    wordvec_arrays[i,:] = word_vector(tokenized_words[i], 100)\n    \nwordvec_df = pd.DataFrame(wordvec_arrays)\nwordvec_df.shape","5b62e521":"# Splitting the data on word2vec embeddings\nxtrain_w2v, xtest_w2v = train_test_split(wordvec_df, random_state=42, test_size=0.2)","5c96a1a4":"ml_models(xtrain_w2v, y_train, xtest_w2v, y_test)","5fbd472b":"epochs = 10\nbatch_size = 10\nloss = \"categorical_crossentropy\"\noptimizer = \"adam\"\nmetrics = [\"accuracy\"]\n\ncallbacks = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=3)\n\n# Build neural network\ncbow_model = Sequential()\ncbow_model.add(Dense(512, activation='relu'))\ncbow_model.add(Dense(256, activation='relu'))\ncbow_model.add(Dense(5, activation='softmax'))\ncbow_model.compile(loss=loss, optimizer=optimizer, metrics= metrics)","1e61c68a":"cbow_model.fit(xtrain_w2v, y_train_new, validation_data=(xtest_w2v, y_test_new), epochs=epochs, batch_size=batch_size, callbacks=callbacks)","309e3edc":"# Validating the model on test set\nloss, accuracy = cbow_model.evaluate(xtest_w2v, y_test_new, verbose = 0)\nloss, accuracy","cd5a276d":"# ann_viz(cbow_model, view=True, filename=\"cbow_model\")\nplot_model(cbow_model, to_file='cbow_model.png', show_shapes=True, show_layer_names=True)","7d9bd0bc":"training = df['NewDescription'].tolist() # Covnerting the text to list\ntraining = [sentence.split(' ')  for sentence in training] # Splitting on each sentence which gives the multi dimensional list \nnp.array(training[0]) # Priting the first sentence","666fda97":"# Initiation of Word2Vec model\n# Every word represented by 100 dimensions, ignore words which appears less than 2 times, and use skip gram model\n\nmodel = Word2Vec(sentences=training,\n                 min_count =2,\n                 sg=1, \n                 vector_size=100)\nprint(model)","d30ee422":"# The trained word vectors are stored in a KeyedVectors instance, as model.wv\nmodel.wv['removing'] # printing the 100 dimensional vector for first word","65a29189":"# df.reset_index(inplace=True)\ntokenized_words = [i.split() for i in df['NewDescription']]","0d95a8b0":"wordvec_arrays = np.zeros((len(tokenized_words), 100))\n\nfor i in range(len(tokenized_words)):\n    wordvec_arrays[i,:] = word_vector(tokenized_words[i], 100)\n    \nwordvec_df = pd.DataFrame(wordvec_arrays)\nwordvec_df.shape","727a13b7":"# Splitting the data on word2vec embeddings\nxtrain_w2v, xtest_w2v = train_test_split(wordvec_df, random_state=42, test_size=0.2)","90ee1433":"ml_models(xtrain_w2v, y_train, xtest_w2v, y_test)","f113976c":"epochs = 10\nbatch_size = 10\nloss = \"categorical_crossentropy\"\noptimizer = \"adam\"\nmetrics = [\"accuracy\"]\n\ncallbacks = EarlyStopping(monitor='val_loss', verbose=0, patience=3)\n\n# Build neural network\nskipgram_model = Sequential()\nskipgram_model.add(Dense(512, activation='relu'))\nskipgram_model.add(Dense(256, activation='relu'))\nskipgram_model.add(Dense(5, activation='softmax'))\nskipgram_model.compile(loss=loss, optimizer=optimizer, metrics= metrics)","e067efc1":"skipgram_model.fit(xtrain_w2v, y_train_new, validation_data=(xtest_w2v, y_test_new), epochs=epochs, batch_size=batch_size, callbacks=callbacks)","22472a5a":"# Validating the model on test set\nloss, accuracy = skipgram_model.evaluate(xtest_w2v, y_test_new, verbose = 0)\nloss, accuracy","8cc4a71e":"# ann_viz(skipgram_model, view=True, filename=\"skipgram model\")\nplot_model(skipgram_model, to_file='skipgram_model.png', show_shapes=True, show_layer_names=True)","174e267c":"# Getting the length of the each description\ndf['sen_length'] = [len(i.split()) for i in df.NewDescription] \n\nprint('min length:',df.sen_length.min(), '\\t' ,'max length:', df.sen_length.max())","8f980e45":"sns.distplot(df.sen_length)","8ea9ae8b":"# #Set your project path \n# # project_path = \"\/content\/drive\/MyDrive\/AIML\/Projects\/glove\/\"\n# project_path =  \"\/content\/drive\/MyDrive\/AIML\/Capstone\/\"\n# # project_path =  \"\/content\/drive\/MyDrive\/\/Capstone Project\/\"\n#------------------------------------------------------------\n\nmax_features = 10000\nmaxlen = df.sen_length.max()      # Add your max length here\nembedding_size = 100              # Size of the embeddings\n#------------------------------------------------------------\n\n# Tokeninzing the text\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df['NewDescription'])","47ed3fbb":"# Transforming each text to a sequence of integers.\nX = tokenizer.texts_to_sequences(df['NewDescription'])\n#------------------------------------------------------------\n\n# Get the Vocabulary size\nvocab_size = len(tokenizer.word_index)\n#-------------------------------------------------------------\n\n# Padding the sequences to ensure that all sequences in a list have the same length\nX = pad_sequences(X, maxlen = maxlen)\ny = np.asarray(df['Potential_Accident_Level'])\n#-------------------------------------------------------------\n\nprint(\"Number of Samples:\", len(X))","e3618646":"# # Get the Word Embeddings\n# EMBEDDING_FILE = project_path + 'glove.6B.100d.txt'\n\n# embeddings = {}\n# for o in open(EMBEDDING_FILE, encoding=\"utf8\"):\n#     word = o.split(\" \")[0]\n#     # print(word)\n#     embd = o.split(\" \")[1:]\n#     embd = np.asarray(embd, dtype='float32')\n#     # print(embd)\n#     embeddings[word] = embd","9e1e07a4":"# embedding_matrix = np.zeros((vocab_size+1, 100))\n\n# for word, i in tokenizer.word_index.items():\n#     embedding_vector = embeddings.get(word)\n#     if embedding_vector is not None:\n#         embedding_matrix[i-1] = embedding_vector\n\n# len(embeddings.values())","5315d17f":"# # Input - Layer\n# glove_model = Sequential()\n\n# # Hidden - Layers (Embedding and LSTM layers)\n# glove_model.add(Embedding(vocab_size+1, embedding_size, weights = [embedding_matrix], input_length=maxlen))\n# glove_model.add(LSTM(100))\n# glove_model.add(Dropout(0.5))\n# # model.add(Flatten())\n# glove_model.add(Dense(250, activation='relu'))\n\n# # Output- Layer\n# glove_model.add(Dense(5,activation='softmax'))\n\n# # Adding callbacks\n\n# early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)","64345653":"# batch_size = 12\n# epochs = 12\n\n# glove_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')\n# print(glove_model.summary())","974cecd0":"# y = pd.get_dummies(df['Potential_Accident_Level']) # One-hot encoding\n# x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=100, test_size=0.2)\n\n# # Fit the model\n# history = glove_model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2, callbacks=[early_stopping])","073c87a6":"# # Validating the model on test set\n# loss, accuracy = glove_model.evaluate(x_test, y_test, verbose = 0)\n# loss, accuracy","6010f152":"# plot_model(glove_model, to_file='glove_model.png', show_shapes=True, show_layer_names=True)","1c8ffd80":"# # Making predictions on test data\n# y_pred = (glove_model.predict(x_test)>0.5).astype(int)\n# print('\\t***********Classification Report*********** \\n', classification_report(y_pred, y_test))\n\n# # Creation of confusion matrix betweeen predictions and actuals\n# cm = confusion_matrix(y_test.values.argmax(axis=1),y_pred.argmax(axis=1))\n# cm = pd.DataFrame(cm, index = ['I', 'II', 'III', 'IV', 'V'] , columns = ['I', 'II', 'III', 'IV', 'V'])\n\n# # Plotting confusion matrix\n# plt.title('Confusion Matrix between predictions and actuals')\n# sns.heatmap(cm, annot=True, fmt = '', cbar=False, cmap='RdPu', linecolor='black', linewidths=0.1);","c7e3dce9":"# import matplotlib.pyplot as plt\n\n# f, (ax1, ax2) = plt.subplots(1, 2, figsize = (15, 7.5))\n# f.suptitle('Monitoring the performance of the model')\n\n# ax1.plot(history.history['loss'], label = 'Train')\n# ax1.plot(history.history['val_loss'], label = 'Test')\n# ax1.set_title('Model Loss')\n# ax1.legend(['Train', 'Test'])\n\n# ax2.plot(history.history['accuracy'], label = 'Train')\n# ax2.plot(history.history['val_accuracy'], label = 'Test')\n# ax2.set_title('Model Accuracy')\n# ax2.legend(['Train', 'Test'])\n\n# plt.show()","806d571d":"# ann_viz(glove_model, view=True, filename=\"glove_model\")","a05f16af":"from imblearn.over_sampling import SMOTE\nfrom collections import Counter\n\ncounter = Counter(df['Potential_Accident_Level'].to_list())\nprint('Before',counter)\n\n# oversampling the train dataset using SMOTE\nsmt = SMOTE()\nlabels = df['Potential_Accident_Level'].tolist()\nX_smote, y_smote = smt.fit_resample(X, labels)\n\nX_train_smote, X_test_smote, y_train_smote, y_test_smote = train_test_split(X_smote, y_smote, stratify=y_smote, random_state=1)   \n\ncounter = Counter(y_train_smote)\nprint('After',counter)","88442d2a":"X_smote","577fd09e":"# Checking with ML models after using SMOTE technique\nml_models(X_smote, y_smote, X_test_smote, y_test_smote)","fdabb063":"# Implementing Neural Networks Bag-of-words model after balancing the class. \n\nepochs = 8\nbatch_size = 12\nloss = \"categorical_crossentropy\"\noptimizer = \"adam\"\nmetrics = [\"accuracy\"]\n\n# early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=3)\n\n# Build neural network\nsmote_model = Sequential()\nsmote_model.add(Dense(512, activation='relu'))\nsmote_model.add(Dense(256, activation='relu'))\nsmote_model.add(Dense(5, activation='softmax'))\nsmote_model.compile(loss=loss, optimizer=optimizer, metrics= metrics)","dcfad5b5":"y_train_smote = pd.get_dummies(y_train_smote) # One-hot encoding\ny_test_smote = pd.get_dummies(y_test_smote) # One-hot encoding\n\nsmote_model.fit(X_train_smote, y_train_smote, epochs=epochs, validation_data=(X_test_smote, y_test_smote), batch_size=batch_size)","9a04d788":"# Validating the model on test set\ny_test_smote = pd.get_dummies(y_test_smote)\nloss, accuracy = smote_model.evaluate(X_test_smote, y_test_smote, verbose = 0)\nloss, accuracy","cce53eef":"# ann_viz(smote_model, view=True, filename=\"smote_model\")\nplot_model(smote_model, to_file='smote_model.png', show_shapes=True, show_layer_names=True)","0447480d":"# df_text = df[df['Potential_Accident_Level'].isin(['I','V'])].reset_index()\ndf_text = df.copy()  #tried augmenting all \ndf_text.head()","4604793d":"# Generating more data for minority classes in dependent variable Using Back Translation\n\ntranslator = google_translator()  \ndef backTranslate(text):\n    translate_text = translator.translate(text, lang_tgt='pt-br')  \n    translate_text = translator.translate(translate_text, lang_tgt='en') \n    return translate_text \n\ndf_text.Description = df_text.Description.apply(lambda x: backTranslate(x) )","7ca60a27":"# Concatenating the new datapoints extracted from the backtranslation technique with the original dataset \nnew_df_text = pd.concat([df_text, df.drop_duplicates()]).reset_index(drop = True)\nnew_df_text = new_df_text.drop_duplicates()\nnew_df_text.count()","ee0eb936":"new_df_text.head(2)","5a85e97c":"# Nlp preprocessing on the augumented text data\nnew_df_text['Augmented'] = new_df_text.apply(lambda x: \" \".join(des_cleaning(x.Description)), axis=1)","77da5aa4":"new_df_text.Potential_Accident_Level.value_counts(), df.Potential_Accident_Level.value_counts()","5d2bcb87":"X_train_aug, X_test_aug, y_train_aug, y_test_aug = train_test_split(new_df_text['Augmented'], new_df_text['Potential_Accident_Level'], test_size=0.3, random_state=42)\n\nprint('Training utterances: {}'.format(X_train_aug.shape[0]))\nprint('Validation utterances: {}'.format(X_test_aug.shape[0]))","61d50c8d":"# Import libraries\ntry:\n  import textaugment, gensim\nexcept ModuleNotFoundError:\n  !pip -q install textaugment gensim\n  import textaugment, gensim\n\nfrom textaugment import Wordnet\n\npf = pd.concat([pd.DataFrame(X_train_aug, columns =[\"Augmented\"]).reset_index(drop = True), \n                pd.DataFrame(y_train_aug, columns =[\"Potential_Accident_Level\"]).reset_index(drop = True)], axis =1)\npf.head()","d5262a7d":"y_train_aug = le.fit_transform(y_train_aug).astype(np.int8)\npf.Potential_Accident_Level = le.fit_transform(pf.Potential_Accident_Level).astype(np.int8)\ny_train_aug[0:3]","4274de40":"text = np.array(pf[(pf['Potential_Accident_Level'] == 0) | (pf['Potential_Accident_Level'] == 4) | (pf['Potential_Accident_Level'] == 1)]['Augmented'], dtype = str)\nytext = np.array(pf[(pf['Potential_Accident_Level'] == 0) | (pf['Potential_Accident_Level'] == 4) | (pf['Potential_Accident_Level'] == 1)]['Potential_Accident_Level'], dtype = np.int8)\nytext.dtype","c9120a3a":"from collections import Counter\ncounter = Counter(y_train_aug)\nprint('Before',counter) # Potential Accident Level Distribution for training Data","7bc7fe87":"# This functions takes in text to be augmented, its target as well as the augmentation model used and adds augmented rows to Training data\naugmentedtxt = pd.DataFrame()\n\n# AS OF NOW WE ARE NOT USING THIS TRAINING SET FOR OUR MODEL \ndef augment(txtarray, ytxtarray, model):\n    #create an empty dataframe\n    pf_text = pd.DataFrame()\n\n    # loop through all rows and augment data, also append it to the df created\n    for i,x in enumerate(txtarray):\n        taug = model.augment(str(x))\n        if taug == str(x):\n            continue\n            # do not add duplicate data if augmentation doesnt modify the text\n        else:\n            pf_text = pf_text.append(pd.Series([taug,ytxtarray[i].astype(np.int8)], index = ['Augmented', 'Potential_Accident_Level']), ignore_index=True)\n    #concatenating augmented data with the existing training dataframe\n    final = pd.concat([pf_text, pf])\n    augmentedtxt.append(pf_text)\n    #updating the training data set\n    X_train_aug = np.array(final.Augmented)\n    y_train_aug = np.array(final.Potential_Accident_Level.astype(np.int8))\n    #See the counts of target variale within the training set\n    counter = Counter(y_train_aug)\n    print('After',counter)\n    return final.reset_index(drop = True)","11e9d00d":"t = Wordnet(n=True, p=0.7)\npf = augment(text, ytext,t)\npf['Potential_Accident_Level'] = pf['Potential_Accident_Level'].astype(np.int8)","cbb00e04":"ftext = np.array(pf[pf['Potential_Accident_Level'] == 4], dtype = str)\nfytext = np.array(pf[pf['Potential_Accident_Level'] == 4]['Potential_Accident_Level'], dtype = np.int8)","2f906f97":"from textaugment import Translate\n#t = Translate(src=\"en\", to=\"pt\")\nt = Wordnet( p=0.6, runs =5)\npf =augment(ftext, fytext,t)","b7d9a2c9":"#Adding augmented data from minority classes into main data set\nnew_df_text = pd.concat([new_df_text, augmentedtxt], axis=1)\n","f49f3a9e":"# Getting the length of the each description\npf['sen_length'] = [len(i.split()) for i in pf['Augmented']] \n\nprint('min length:', pf.sen_length.min(), '\\t' ,'max length:', pf.sen_length.max())","00264a61":"# Tokeninzing the text\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(new_df_text['Augmented'])","278f5b22":"# Transforming each text to a sequence of integers.\nX = tokenizer.texts_to_sequences(new_df_text['Augmented'])\n#------------------------------------------------------------\n\n# Padding the sequences to ensure that all sequences in a list have the same length\nX = pad_sequences(X, maxlen = pf.sen_length.max())\n#-------------------------------------------------------------\n\nprint(\"Number of Samples:\", len(X))","e35c1f97":"counter = Counter(new_df_text['Potential_Accident_Level'].to_list())\nprint('Before',counter)\n\n# oversampling the train dataset using SMOTE\nsmt = SMOTE()\nlabels = new_df_text['Potential_Accident_Level'].tolist()\nX_smote, y_smote = smt.fit_resample(X, labels)\n\nX_train_smote, X_test_smote, y_train_smote, y_test_smote = train_test_split(X_smote, y_smote, stratify=y_smote, random_state=1)   \n\ncounter = Counter(y_train_smote)\nprint('After',counter)","978cbdc8":"ml_models(X_train_smote, y_train_smote, X_test_smote, y_test_smote)","ee588fac":"# Here implementing cross validation using bagging classifer as it is giving the bettwe accuracy compared to other ML models\nBag_clf = BaggingClassifier(base_estimator = RandomForestClassifier(n_estimators=150, max_features='sqrt'), \n                            n_estimators=200,\n                            random_state=52)\n\nscores =[]\ndef model(algo, x, y):  # defining a function\n    cross_val = cross_val_score(algo, x, y, cv=10) # 10 splits\n    print ('************************')\n    scores.append(cross_val)\n\n    # Report Performance\n    print (\"cv-mean score :\",'{:.2f}%'.format(cross_val.mean()*100)) # printing the average of all the 10 scores\n    print (\"cv-std  :\",'{:.2f}%'.format(cross_val.std()*100))   # printing the standard deviation to check the spread among all the scores","bbe95f49":"print('Bagging classifier cv score')\nmodel(Bag_clf, X_smote, y_smote)","4a72b474":"# plot scores\nsns.distplot(scores)\nplt.show()\n# confidence intervals\nalpha = 95       # for 95% confidence level\nlower =  np.mean(scores)-(np.std(scores)*2) # minus two standard deviation\nupper = np.mean(scores)+(np.std(scores)*2)  # plus two standard deviation\nprint('{}% confidence interval {:.1f}% and {:.1f}%'.format(alpha, lower*100, upper*100))","a297ddd5":"# Implementing Neural Networks Bag-of-words model after using text augmentaion and balancing the class. \n\nepochs = 8\nbatch_size = 12\nloss = \"categorical_crossentropy\"\noptimizer = \"adam\"\nmetrics = [\"accuracy\"]\n\n# early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=3)\n\n# Build neural network\nsmote_model = Sequential()\nsmote_model.add(Dense(512, activation='relu'))\nsmote_model.add(Dense(256, activation='relu'))\nsmote_model.add(Dense(128, activation='relu'))\nsmote_model.add(Dense(5, activation='softmax'))\nsmote_model.compile(loss=loss, optimizer=optimizer, metrics= metrics)","a407cd99":"smote_model.fit(X_train_smote, pd.get_dummies(y_train_smote), epochs=epochs, batch_size=batch_size)","056430a2":"smote_model.evaluate(X_test_smote, pd.get_dummies(y_test_smote))","b5ab7c91":"Bag_clf = BaggingClassifier(base_estimator = RandomForestClassifier(n_estimators=100, max_features='sqrt'), \n                            n_estimators=200,\n                            random_state=52)\n                            \nBag_clf.fit(X_train_smote, y_train_smote) # fitting the model \nprint('\\nTest score:', '{:.2f}%'.format(Bag_clf.score(X_test_smote, y_test_smote))) # evaluating it on test set","f6025816":"# Making predictions on test data\ny_pred = Bag_clf.predict(X_test_smote)\nprint('\\t***********Classification Report*********** \\n', classification_report(y_pred, y_test_smote))\n \n# Creation of confusion matrix\ncm = confusion_matrix(y_test_smote, y_pred)\ncm = pd.DataFrame(cm, index = ['I', 'II', 'III', 'IV', 'V'] , columns = ['I', 'II', 'III', 'IV', 'V'])\n \n# Plotting the confusion matrix\nplt.title('Confusion Matrix between predictions and actuals')\nsns.heatmap(cm, annot=True, fmt = '', cbar=False, cmap='RdPu', linecolor='black', linewidths=0.1);","bccc19e7":"from sklearn.metrics import roc_curve\n# roc_curve() which computes the ROC for classifier. It returns the FPR, TPR, and threshold values\n\npred_prob = Bag_clf.predict_proba(X_test_smote)\n\n# roc curve for classes\nfpr = {}\ntpr = {}\nthresh ={}\n\nn_class = 5\n\nfor i in range(n_class): # looping on number of classes and computing three metrics for each class\n    # Compute Receiver operating characteristic (ROC), the function returns\n    fpr[i], tpr[i], thresh[i] = roc_curve(y_true = le.fit_transform(y_test_smote), y_score = pred_prob[:,i], pos_label=i)","1916d41c":"# plotting\nplt.figure(figsize=(10,7))\nplt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Level I vs Rest')\nplt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Level II vs Rest')\nplt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Level III vs Rest')\nplt.plot(fpr[3], tpr[3], linestyle='--',color='red', label='Level IV vs Rest')\nplt.plot(fpr[4], tpr[4], linestyle='--',color='black', label='Level V vs Rest')\n\nplt.title('ROC curve comparision against each of the classes')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive rate')\nplt.legend(loc='best')","9035d8b9":"# Taking the categorical columns along with text\ndf_new = df[['Country', 'Local', 'Industry_Sector', 'Accident_Level',\n       'Potential_Accident_Level', 'Gender', 'Employee_Type', 'Critical_Risk', 'NewDescription']]","9d59818b":"# Dividing independent and dependent variable\nx = df_new.drop('Potential_Accident_Level', 1)\ny = df_new.pop('Potential_Accident_Level')","606d38c6":"# LabelBinarizer encodes the data into dummy variables indicating the presence of a particular label or not.\n\nfrom sklearn.preprocessing import LabelBinarizer\n\nlb = LabelBinarizer(sparse_output=True) # Returns an Numpy array but here we are setting sparse o\/p to true in order to stack all the variable values.\n\nX_train_country = lb.fit_transform(x['Country'])\n# X_test_country = lb.transform(y['Country'])\n\nX_train_local = lb.fit_transform(x['Local'])\n# X_test_local = lb.transform(y['Local'])\n\nX_train_industry_sector = lb.fit_transform(x['Industry_Sector'])\n# X_test_industry_sector = lb.transform(y['Industry_Sector'])\n\nX_train_accident_level = lb.fit_transform(x['Accident_Level'])\n# X_test_accident_level = lb.transform(y['Accident_Level'])\n\nX_train_gender = lb.fit_transform(x['Gender'])\n# X_test_gender = lb.transform(y['Gender'])\n\nX_train_employee_type = lb.fit_transform(x['Employee_Type'])\n# X_test_employee_type = lb.transform(y['Employee_Type'])\n\nX_train_critical_risk = lb.fit_transform(x['Critical_Risk'])\n# X_test_critical_risk = lb.transform(y['Critical_Risk'])\n\n# Here for text data, \nX_train_Description = vectorizer.fit_transform(x['NewDescription'])","9724bc44":"sparse_matrix_list = (X_train_country, X_train_local, X_train_industry_sector, \n                      X_train_accident_level, X_train_gender, \n                      X_train_employee_type, X_train_critical_risk, X_train_Description)","e3c408f9":"# Convert this matrix to Compressed Sparse Row format\nfrom scipy.sparse import hstack\nX_train = hstack(sparse_matrix_list)   # Stack sparse matrices horizontally\nprint(type(X_train), X_train.shape)","6ae7e8c7":"counter = Counter(y)\nprint('Before',counter)\n\n# oversampling the train dataset using SMOTE\nsmt = SMOTE()\n# labels = df['Potential_Accident_Level'].tolist()\nX_smote, y_smote = smt.fit_resample(X_train, y)\n\nX_train_smote, X_test_smote, y_train_smote, y_test_smote = train_test_split(X_smote, y_smote, stratify = y_smote, random_state=1)   \n\ncounter = Counter(y_train_smote)\nprint('After',counter)","f8a6c908":"ml_models(X_train_smote.toarray(), y_train_smote, X_test_smote.toarray(), y_test_smote)","3a698b03":"# Implementing Neural Networks Bag-of-words model after balancing the class. \n\nepochs = 8\nbatch_size = 24\nloss = \"categorical_crossentropy\"\noptimizer = \"adam\"\nmetrics = [\"accuracy\"]\n\n# early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=3)\n\n# Build neural network\nsmote_model = Sequential()\n# smote_model.add(Dense(512, activation='relu'))\nsmote_model.add(Dense(256, activation='relu'))\nsmote_model.add(Dense(5, activation='softmax'))\nsmote_model.compile(loss=loss, optimizer=optimizer, metrics= metrics)","c1d88704":"y_train_smote = pd.get_dummies(y_train_smote) # One-hot encoding\ny_test_smote = pd.get_dummies(y_test_smote) # One-hot encoding\n\nsmote_model.fit(X_train_smote.toarray(), y_train_smote, epochs=epochs, validation_data=(X_test_smote.toarray(), y_test_smote), batch_size=batch_size)","bc3ae810":"smote_model.evaluate(X_test_smote.toarray(), y_test_smote)","677371b5":"plot_model(smote_model, to_file='smote_model.png', show_shapes=True, show_layer_names=True)","be4de70a":"#### THE-END ####","b39c2893":"#Pickle file used to store the model\n\nimport pickle\nBag_clf.fit(X_smote, y_smote)\npickle.dump(Bag_clf, open('nlp_chatbot.sav','wb'))","4e89abc9":"# Function used for UI integration -- takes description as input vectorizes and predicts it via our bagging classifier\ndef predict(message):\n    data = \" \".join(des_cleaning(message))\n    x = tokenizer.texts_to_sequences([data])\n    # Padding the sequences to ensure that all sequences in a list have the same length\n    message = pad_sequences(x, maxlen = 93)\n    my_prediction = Bag_clf.predict(message)\n    return my_prediction","c4e8bc18":"# predict('observing pulp overflow overflow reception drawer thickener filter operator approach verify operation pump making sure stopped press keypad start pump getting start proceeds remove guard manipulates motor pump transmission strip left hand imprisoned pulley motor transmission belt')\n# # sample output","ebcb7968":"* Division of employee_type in men and women is almost same\n* As seen in the above plots third party empoloyees are slightly higher than the employee count in both of the sectors(men&women)\n* Proportion of female third party remote employee is moderately higher than that of the males","7babe897":"To combat the class imbalance in the dependent variable i.e. Potential_accident_level\n\nSMOTE is an oversampling technique where the synthetic samples are generated for the minority class. These synthetic instances are then added to the original dataset.","b359cd83":"## Bivariate Analysis\n* On target column","b2168586":"Some Examples:\n\nNN - noun\n\nNNP - proper noun\n\nNNS - noun plural\n\nCD - cardinal digit\n\nDT - determiner\n\nVB - verb\n\nJJ - adjective\n\nRB - adverb\n\nVB -\tverb\n\nVBD\t- verb past tense...etc","964be7fe":"#### Accidents Count by Industry_Sector and Local","9f59dd2e":"Most injuries involved hands as seen earlier","0a1a3d6b":"#### Ngram with Industry column","3a95d458":"## Train Test split","7534188f":"* There is a major diferrence in mining and metals industries within males and females\n* We can observe the distinct safety levels by the industry sectors towards male and female","805655d8":"* Total number of Direct Employees and Third party employees are almost same, However, Third party remote employees are less in number.\n\n","c77efbdd":"#### Accidents Count by Country and Local","27e670c3":"* Most of the Critical Risks are classified as 'Others' that is almost 50% of the dataset, hence there are too many risks need to be classified precisely.\n* This is followed by Pressed, Manual tools, Chemical substances, Cut etc..\n","35c7f37b":"* There are total of Three countries and Twelve plants\/cities overall, \n* Three type of Industry sectors exist namely\n      namely: 'Mining'\u2692\ufe0f, 'Metals'\u2699\ufe0f and Others\ud83e\udd14, \n* Five type of accident levels (1 to 5) are present.\n* Six type of Potential Accident Levels (1 to 6) and we see that there is only one value registered under 'Accident level 6' and   it can be replaced to level 5 to reduce the complexity\n* Three type of employee types \n      namely: 'Third Party', 'Employee', 'Third Party(Remote)'\n* There are about 33 critical risks in which more than half of the incidents fall under the 'Others' category, these entries have been changed to 'Others' as part of the anonymization process.\n* Level VI accident is having only one incident hence it can be replaced with Level V as it is more closer one\n","eff0e51f":"* Males are more involved in Severe Accidents whereas Females are suffering with less Servere ones (Level II specifically)","5362d1e5":"## Word Embeddings","43d1fbcb":"* Our model has a vocabulary of 1644 unique words and their vectors of size 100 each.","8af634e4":"* There are some amount of females who involved in Critical Risks such as Chemical Substances, Cut, Fall other than Others category\n* As dataset has more number of male data points, the plot bars tends to increase on the same class.","57d04e17":"Description about accidents is important to understand the cause of accidents, so we need to discover characteristical words or phrases indicating situation.","b94f5b1d":"# EDA","54b52cc2":"### Local","e49f35c5":"## Time Series Analysis\n\n#### Here we have a data in a series of particular time periods or intervals as it has a time component. Hence let's do some analysis","86a30362":"### NN","55486dbf":"## 3. CBOW","9c81c012":"#### Potential Accident level counts by Critical Risk","09616a88":"## Multivariate Analysis","15c2910c":"## Word2Vec\nWord2vec is a combination of models used to represent distributed representations of words in a corpus C. Word2Vec (W2V) is an algorithm that accepts text corpus as an input and outputs a vector representation for each word of 32 or more dimension","f813b2a9":"* For metals industry, the distribution of potential accident level for third party employee is right skewed, that means most of the accidents have moderate risk.\n\n* For Direct employee, there is an equal distribution between low and moderate risk.\nSame for Third party remote employees, but it is long tailed.\n\n* For other industries, most of the third party employees are affected by very low risk which is a good sign.\n\n* Similarly, for Direct employees the potential accident level is distributed well and the mode of the distribution is located at a low risk level (with a very extended inter quartile range).\n* No data is available for Third party remote employees who belong to industries.","c564298f":"* It can be observed that more number of accidents occured in 2016 compared to 2017, in year 2016 we have all 12 months of data whereas year 2017 has only 7 months of data.\n* It seems that the number of accidents decreased in latter of the year \/ month.\n* The number of accidents increased during the middle of the week and declined since the middle of the week.","363250f9":"#### Industry_Sector by Gender Count","f469bd79":"## Text Augmentation","34de1ff9":"* There are many phrases which is related to hands. For example hand causing, left hand, right hand, finger left, finger right, middle finger and ring finger.\n* There are also some phrases which is related to other body parts. For example left foot and right side.","294c4c92":"* Potential accident level indicates how severe the accident would been due to other factors involved in the accidents.\n* Potential accident level IV has the highest count and signifies the moderate severity of accidents.\n* We have to check the correlation of this Potential Accident level and Accident level along with the industry sector.","d0a1bbf9":"### Bivariate Analysis between the independent variables","950af6f9":"## Univariate Analysis","64729b3a":"It can be observed that Industry Sector depends on the Local area too. \n\n*   Local Area 1,2,3,4 and 7 belong to Mining Sector.\n*   Local Area 5,6,8 and 9 belong to Metal Sector.\n*   Local Area 10, 11 and 12 belong to Other Sectors.\n\n\n\n\n\n\n\n\n\n\n\n\n","9c235e3a":"### Employee type","223dcad5":"* Third Party Employees are more involved in Accidents\n* We can observe that apart from Accident_Level_1 the people are also facing severe accidents (Accident_Level_IV) in the industry.","53219859":"**Insights:**\n\n* Dataset contains total of 10 columns in which all are object data type.\n* There are no null \/ missing values present in the dataset\ud83d\ude07, hence we can safely move ahead.\n* There are 7 duplicates for which all the columns have same value, These duplicate records refer to different persons involved in the same accident.There are only 7 records hence we will be dropping them down the line. As capturing the number of people will not be helpful in our analysis.\n","436c4ae2":"* Local_03 (which also belongs to Country_01) is where most of the accidents happen","e1afe75e":"#### DOMAIN: Industrial safety. NLP based Chatbot.\n\n##Objective:\nThe goal of this case study is to design a ML\/DL based chatbot utility which can help the professionals to highlight the safety risk as per the incident description and other input variables.","5ab5fedb":"## Cleaning the text","e2190652":"## POS Tagging","597cd3ca":"* Using all the independent columns along with the text data and building the models is also not giving much accuracy.","3984bfc5":"#### Potential Accident level counts by Country","5260d51c":"* Country_01 has more number of severe accidents especially Level IV\n* Country_02 has moderate accidents across all the levels\n* Country_03 'level I' accidents counts is more compared to country_01 and country_02 but less severe accidents.","54a4cc3a":"### NN","b91f2995":"It is evident from the plot that, the curves are higher than the threshold. More than 95% of the 'Level V' and 'Level I' accidents are classifed correclty. Some of the data points in other classes are miss-classified, hence the curve of those classes are slightly bend towards False positive rate. Therefore, we can say that bagging classifier did a better job of classifying the most of the classes precisly in the dataset.","202aad80":"#### Employeee type by Gender Count\n###### Since Gender is more biased towards male, we are taking ratio count with respect to other variables","d51d868d":"* Level I \u261d\ufe0f signifies not severe and V \ud83d\udd90\ufe0f signifies very severe.\n* Accidents with the level I are most common. These are due to small misses, like people forgot their PPE, or they dropped a tool, etc.","313afa21":"## Bagging Classifier\n#### As we can see that the ensemble models are giving the highest accuracy, hence Considering the bagging classifier as the final model\n","39616d40":"## 4. Skip Gram","f737f92c":"# Taking all colums","6a53ab46":"### Accident Level","810651b7":"* Notice there is a significant difference between the severity of an incident, and the potential severity of the incident.\n* If the number of accidents increases, the potential accident level decreases\n* There are high number of the Level 1 accidents.\n* If the Accident level increases, the potential accident level also increases","8bf9bc1a":"# Model Building","39423162":"* It can be noticed that there are multiple peaks of accidents every year. There is a high peak in February of 2017\n","7c39cca0":"## SMOTE ","96e5e7d3":"* As a characteristic of the industry, the proportion of Men employees is overwhelming.\n* The dataset is biased towards Male employees.","56d649d7":"#### Critical Risk counts by Gender","ff0280fe":"* Out of all industries, Mining Industry has had most severe accidents and their corresponding potential Accident level is also high.\n* Most of the accidents have been reported from mining industry and almost all of them are of moderate level, followed by Metal industry and Others.\n* The Potential accident level and Accident level are equivalent to each other.\nSome of the critical risks are less severe and have reported maximum number of accidents.","2dcfd7aa":"ROC curve is important, since it is ideal to maximize the true positive rate while minimizing the false positive rate.\n","f43307b4":"* It clearly indicates that mining industries witness more accidents.\n* We can say that the number of accidents in Mining Industry is considerably more than that in the Metal Industry, therefore mining job is more risky than the latter.","c6177c85":"#### Text Augmentation","6e7dec44":"* We can see that classical ML approaches giving very low accuracy since they cannot handle the text data very well.","443641c1":"# Data pre-processing","721dab70":"### Potential Accident level counts by Month\n\n","36106c05":"### ML Classifiers","98a42cb8":"## Word Cloud","743ac4fe":"* Maximum number of accidents are contributed by Country_01 and the plants\/locals belongs to Country_01 as we saw in the previous plots where Local_01 and Local_05 (belongs to Country_01) have the highest number of incidents","bd295b1c":"#### Potential Accident level counts by Employee Type","ee07f112":"* We can see that classical ML approaches giving very low accuracy since they cannot handle the text data very well.","2dfdc78e":"### Cross Validaton","70d9ce3f":"The disadvantages of bag of words are there is no semantic meaning as it just contains zeros and ones and Bag of words leads to a high dimensional feature vector due to large size of Vocabulary","00ffa723":"For TF-IDF vectorization, the accuracy is quite less. Though NaiveBayes performs better than the rest, The accuracy is not upto the mark.","bb5dc6b5":"## N-grams","a6f8d7da":"## 1. Bag of words\nA bag-of-words is a representation of text that describes the occurrence of words within a document.","ba6de25a":"* We can cleary observe that the above dataframe contains 7 duplicates in which only one or two column values are dissimilar among the datapoints where in the Description is matching, which is logically unsound.\n* It can also be noticed that the incidents which are having duplicate values happened on the time stamp(Date Column).\n* Hence we will be dropping these hidden duplicates which doesn't seem right logically.","54525b0e":"# NLP Pre-Processing","9737f549":"#### Potential Accident level counts by Local\n","0e4a9feb":"## 5. GLOVE EMBEDDINGS","5107220f":"### Accident Level counts by Year","04b70f0d":"#### Potential Accident level counts by Industry Sector\n","dbf0b012":"## Correlation plot","3786750f":"* One can observe in the above dataframe that there are only 411 unique values in the Description column but we have 418 number of records in total, we can assume that there are still 7 number of duplicate values on description column.\nWe will check for those duplicates on column level and drop if required for further analysis.","22099236":"* There is a moderate correleation among the values of both variables.\n<!-- * Accident_Level_I is highly positively correlated with most of the Potential_Accident_Levels which indicates minor incidents have the potential to cause severe accident levels in an industry.\ud83d\ude25 -->\n\n","c8739cdb":"# Improving model performance","822bb690":"* Accident level have the tendency that non-severe levels(I,II) decreased throughout the year but severe(III, IV) levels did not change much.\n* Initial months see more accidents which seem to decrease by year end\n* High level accidents(V) occur in the initial months","b66cf550":"### Industry Sector","da85c747":"### NN","f6e93224":"* Out of all industries, Mining Industry has seen some accidents whose level is \nthe most severe and the corresponding potential Accident level is also highest.\n* This is followed by Metal industry and other.\n* Severity levels of the incidents are more in Mining sector (rate of level 4 is slightly higher than the level 2 & 3)\n","361bacc2":"### ML Classifiers","6524c4c7":"* As same as the Ngram analysis above, there are many hand-related and movement-related words.\n* Hand-related: left, right, hand, finger, and glove\n* Movement-related: fall, hit, carry, lift and slip","78d13d80":"### Gender","9ee36207":"* Like Unigram and Bigram, there are also many phrases which is related to hands or other body parts, but increasing the grams makes sense\n* For example left\/right hand finger, left hand causing, hit back right and wearing safety glove.. etc.","4bf1aab2":"* Our model has a vocabulary of 1644 unique words and their vectors of size 100 each.","2668a9a9":"#### Potential Accident level counts with respect to Gender\n###### Since Gender is more biased towards male, we are taking ratio count with respect to other variables","581a95c7":"### NN","03db9b15":"## 2. TF-IDF\nTF-IDF is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents.","5f87b058":"* Local_03 has recorded maximum number of accidents which is approx 21% of all the plants in the country.\n* This is followed by Local-05, Local-01 and so on.","e0648472":"### Critical Risk","92728fbc":"### Potential Accident Level","06944617":"### ML Classifiers","20005e87":"* Now we can see that after removing the duplicates the count of unique values in the description column and the total number of datapoints in the dataset is matching.","e83dc408":"# Visualization\n","e20f4fc7":"* By looking at the above confusion matrix it can be derived that, the model is able to classify most of the test datapoints to the actuals correctly","85ba3ad5":"The cross validation mean score of Bagging Classifier is better than all the ML based models and we can give the range estimate of performance of the model.\nWe can say that our model would perform in the range between 73% and 93% with 95% of the confidence interval","ef8251d4":"* Causing is the most frequent word. \n* There are several nouns like pipe, collaborator, time etc. \n* Most accidents involved the hands of the persons involved.\n* Moreover there are other words which depict some sort of action (verbs). For example hit, remove, fall move...etc","031088cc":"### Country","b12d817a":"Accuracies have been increased with Machine learning models using SMOTE technique","d3ff703a":"#### Ngram with Gender column","b8220298":"**Performing augmentation on training data  -- Can be ignored for now**","c5235c55":"### ML Classifiers","777699aa":"### NN"}}