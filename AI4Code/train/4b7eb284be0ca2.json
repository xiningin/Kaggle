{"cell_type":{"f88c48be":"code","8a202eb3":"code","c092c723":"code","01276295":"code","c2aca83e":"code","cad56f89":"code","d83779ca":"code","130d3441":"code","5418f390":"code","fd6385fd":"code","684fdbba":"code","bb09cb99":"code","5ef88bfb":"code","aabfc805":"code","4061bc46":"code","c4462756":"code","752fe4f6":"code","aea4b767":"code","8130c567":"code","b7cbe4c2":"code","9544a4de":"code","a7dfe5bd":"code","3a24b4e4":"code","c758e37b":"markdown","631f6341":"markdown","b590825f":"markdown","8630de99":"markdown","53f369e0":"markdown","9d739bb6":"markdown","3d4fcb6e":"markdown","b729f908":"markdown","47a7dc04":"markdown","6a1dc752":"markdown","0a6dff5a":"markdown","1dca60fc":"markdown","333e0e8c":"markdown","707e53b6":"markdown","74d0966c":"markdown","0f5d1752":"markdown"},"source":{"f88c48be":"# Importing the Dataset\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Importing the Packages\nimport collections\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\npy.init_notebook_mode(connected=True)\n\nfrom tqdm import tqdm\nfrom datetime import datetime\nfrom wordcloud import WordCloud\nfrom IPython.display import Image\nfrom sklearn.manifold import TSNE\nfrom sklearn.impute import SimpleImputer","8a202eb3":"df_train = pd.read_csv(\"..\/input\/music-albums-popularity-prediction\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/music-albums-popularity-prediction\/test.csv\")\nprint(df_train.shape, df_test.shape)","c092c723":"# Separating the Target variable from the training set\nY = df_train['popularity']\ndf_train.drop(['popularity'], axis=1, inplace=True)\nprint(df_train.shape, Y.shape)","01276295":"# Concatenating the df_train and df_test into one Dataframe\ndf = pd.concat([df_train, df_test], axis=0)\nprint(df.shape)","c2aca83e":"# Setting the ID as index\ndf = df.set_index('id')\n\n# Summarizing the Dataset \ndf.info()","cad56f89":"# Describing the Dataset\ndf.describe()","d83779ca":"# Having a quick glance at the Dataset\ndf.head()","130d3441":"# Visualizing the Distribution of Data-types\ndtypes = df.dtypes\ntypes = ['int64', 'float64', 'object']\nlist_d = [0, 0, 0]\n\nfor dt in dtypes:\n    if(str(dt) == 'int64'): list_d[0] += 1\n    elif(str(dt) == 'float64'): list_d[1] += 1\n    elif(str(dt) == 'object'): list_d[2] += 1\n\nprint(list_d)\nplt.figure(figsize=(8, 6))\nplt.bar(types, list_d)\nplt.title(\"Distribution of Data-Types\")\nplt.xlabel(\"Data-Type\")\nplt.ylabel(\"Frequency\")\nplt.grid()\nplt.show()","5418f390":"# Visualizing the Distribution of Albums according to Release Years\nyears = []\n\n# Getting the Release Years from Release Dates\nfor rd in tqdm(df['release_date']):\n    years.append(int(rd.split('-')[0]))\n\nyears = pd.Series(years)\nyears = pd.Series([y for y in years if y != 0])\n\nplt.figure(figsize=(14, 10))\nplt.hist(years, bins = 24)\nplt.locator_params(axis = \"x\", nbins = 24)\nplt.locator_params(axis = \"y\", nbins = 24)\nplt.title(\"Distribution of Albums according to Release Years\")\nplt.xlabel(\"Release Years\")\nplt.ylabel(\"Frequency\")\nplt.grid()\nplt.show()","fd6385fd":"# Visualizing the Distribution of Albums according to the Number of Tracks\nnum_tracks = pd.Series(df['total_tracks'])\n\nplt.figure(figsize=(14, 10))\nplt.hist(num_tracks, bins = 24)\nplt.locator_params(axis = \"x\", nbins = 24)\nplt.locator_params(axis = \"y\", nbins = 12)\nplt.title(\"Distribution of Albums according to the Number of Tracks\")\nplt.xlabel(\"Number of Tracks\")\nplt.ylabel(\"Frequency\")\nplt.grid()\nplt.show()","684fdbba":"# Visualizing the Distribution of Albums accoridng to Avg Duration of it's Songs\ndur = []\n\nfor i in tqdm(range(df.shape[0])):\n    sam = df.loc[i, : ]\n    sam_dur = 0\n    num_tracks = 0\n    if(sam['t_dur0'] is not None): \n        sam_dur += sam['t_dur0']\n        num_tracks += 1\n    if(sam['t_dur1'] is not None): \n        sam_dur += sam['t_dur1']\n        num_tracks += 1\n    if(sam['t_dur2'] is not None): \n        sam_dur += sam['t_dur2']\n        num_tracks += 1\n    \n    # Taking the Average\n    if(num_tracks != 0): sam_dur \/= num_tracks\n        \n    # Removing all the data-points which doesn't have the information regarding Tracks' duration\n    if(pd.isna(sam_dur) == False): dur.append(sam_dur)\n\n# Plotting\nplt.figure(figsize=(12, 9))\nplt.hist(dur, bins = 24)\nplt.locator_params(axis = \"x\", nbins = 24)\nplt.locator_params(axis = \"y\", nbins = 12)\nplt.title(\"Distribution of Albums according to the Average Duration of the Songs\")\nplt.xlabel(\"Average Duration of the Songs\")\nplt.ylabel(\"Frequency\")\nplt.grid()\nplt.show()","bb09cb99":"# Visualizing a WordCloud of the most frequent artists\nartists = []\n\nfor i in tqdm(range(df.shape[0])):\n    names = df.loc[i, 'artists']\n    names = names.split(',')[ :-1]\n    artists.extend(names)\n    \n# Obtaining the Frequency Distribution of artists\nctr = collections.Counter(artists)\n\n# Selecting the 150 most common artists\ndis_art = dict(ctr.most_common(150))\n\n# Plotting the WordCloud\nwordcloud = WordCloud(background_color = 'black', width = 1600, height = 800).generate_from_frequencies(dis_art)\nfig = plt.figure(figsize=(30,20))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","5ef88bfb":"# Visualizing the distribution of Albums according to the Target Variable (Popularity)\npopularity = Y\n\n# Plotting\nplt.figure(figsize = (12, 9))\nplt.hist(popularity, bins = 20)\nplt.locator_params(axis = \"x\", nbins = 20)\nplt.locator_params(axis = \"y\", nbins = 20)\nplt.title(\"Distribution of Albums according to the Target Variable (Popularity)\")\nplt.xlabel(\"Popularity\")\nplt.ylabel(\"Frequency\")\nplt.grid()\nplt.show()","aabfc805":"# Visualizing the distribution of the rest of the features (11 features)\n# For visualizing these distributions, we are taking the average of each attribute across the Tracks\n\n# Creating an object for storing the average values for all the attributes\navg = {\n    'dance': [], 'energy': [], 'key': [], 'mode': [], 'speech': [],\n    'acous': [], 'ins': [], 'live': [], 'val': [], 'tempo': [], 'sig': [],\n}\n\n# Creating an object for storing the initial values for all the attributes\n# Has to be re-initialized after every iteration\ninit = {\n    'dance': 0, 'energy': 0, 'key': 0, 'mode': 0, 'speech': 0,\n    'acous': 0, 'ins': 0, 'live': 0, 'val': 0, 'tempo': 0, 'sig': 0,\n}\n\nattr = list(avg.keys())\nprint(\"Attributes Covered:\", attr)\n\nfor i in tqdm(range(df.shape[0])):\n    sam = df.loc[i, : ]\n    num_tracks = 0\n    \n    for atr in attr:\n        init[atr] = 0\n        for j in range(3):\n            col_name = 't_' + atr + str(j)\n            if(sam[col_name] is not None): \n                init[atr] += sam[col_name]\n                num_tracks += 1\n\n        # Taking the Average\n        if(num_tracks != 0): init[atr] \/= num_tracks\n\n        # Removing all the data-points which doesn't have the information regarding Tracks' duration\n        if(pd.isna(init[atr]) == False): avg[atr].append(init[atr])","4061bc46":"# Plotting\nfig, axs = plt.subplots(6, 2, figsize = (18, 30))\nfor ind, atr in enumerate(attr):\n    sub = axs[ind \/\/ 2][ind % 2].boxplot(avg[atr], vert = False)\n    axs[ind \/\/ 2][ind % 2].set_ylabel(atr.capitalize(), fontsize=18)\n    axs[ind \/\/ 2][ind % 2].set_xlabel('Quartile', fontsize=18)\n    axs[ind \/\/ 2][ind % 2].grid()","c4462756":"# Source: Matplotlib Official Docs\nImage(\"https:\/\/matplotlib.org\/stable\/_images\/boxplot_explanation.png\", width = 800)","752fe4f6":"# An overview of distributions of all the features in our dataset\n# Added the ';' in order to ignore the textual information provided by Matplotlib\ndf.hist(figsize = (20, 20), bins = 50, xlabelsize = 8, ylabelsize = 8);","aea4b767":"# Dropping all the non-numerical columns\ndf.drop(columns = ['name', 'release_date', 'artists', 't_name0', 't_name1', 't_name2'], \n    inplace = True, axis = 1)\n\n# Sampling the first 5000 rows from the DataFrame, in order to reduce the time required\ndf_sub = df.loc[ :4999, : ]\nprint(df_sub.shape)","8130c567":"# Performing Median-based Imputation\n# Replacing all the NULL values with the median of the respective features\nimp_med = SimpleImputer(missing_values = np.nan, strategy = 'median')\ndf_sub = imp_med.fit_transform(df_sub)\ndf_sub = pd.DataFrame(df_sub)","b7cbe4c2":"# Performing T-SNE on the 5000 Sampled Rows\ntsne3d = TSNE(\n    n_components = 3, perplexity = 30.0, n_iter = 1000, verbose = 2,\n    init = 'random', random_state = 49, method = 'barnes_hut', angle = 0.5, \n).fit_transform(df_sub)","9544a4de":"trace = go.Scatter3d(\n    x = tsne3d[ : , 0], y = tsne3d[ : , 1], z = tsne3d[ : , 2],\n    mode='markers',\n    marker = dict(\n        sizemode = 'diameter', color = Y,\n        colorscale = 'Portland', colorbar = dict(title = 'duplicate'),\n        line = dict(color = 'rgb(255, 255, 255)'), opacity = 0.75\n    )\n)\n\ndata = [trace]\nlayout = dict(height = 800, width = 800, title='3D-Embedding with Numerical Features')\nfig = dict(data = data, layout = layout)\npy.iplot(fig, filename = '3DBubble')","a7dfe5bd":"# LET THE PRE-PROCESSING BEGIN!","3a24b4e4":"# LET THE MODELING BEGIN!","c758e37b":"## Conclusion\n- From the above [Histogram](https:\/\/matplotlib.org\/stable\/api\/_as_gen\/matplotlib.pyplot.hist.html?highlight=hist#matplotlib.pyplot.hist), we can clearly see that most of the albums in the dataset are from the period 2000 - 2020, i.e., the 21st century.\n- Also, we can claim that our dataset is **highly skewed**, if we use the `release_date` of the albums as the plotting feature.\n- Apart from this, our dataset also contains a bunch of outliers, which we can remove on the basis of  `release_date`, for instance, we can remove all the albums that were released before 1965.","631f6341":"# Visualizing the Dataset\n- In this section, we are going to perform some interesting visualizations on our dataset, based on which, we will obtain some **conclusions** for each of our visualizations.\n- You are more than welcome to do some more visualizations, which will only help you understand the dataset even better.","b590825f":"# Modeling the Dataset\n- Once you have completed the pre-processing of your dataset in the previous section, it's time to train some models on it. So, the rest is upto your shoulders, **Brave Kaggler**!\n- After you believe that you have trained a decent model which can beat your previous score, make a submission file, and pay a visit to the leaderboard!\n- This will give you a hint as to in which direction you should move, to further climb the leaderboard.","8630de99":"# Getting Started: Visualization\n- Hola amigos, welcome to the **EDA and Visualization Guide** of [Music Albums Popularity Prediction](https:\/\/www.kaggle.com\/elemento\/music-albums-popularity-prediction) dataset. We hope that you have a lot of fun working with this dataset!","53f369e0":"## Conclusion\n- From the above [Histogram](https:\/\/matplotlib.org\/stable\/api\/_as_gen\/matplotlib.pyplot.hist.html?highlight=hist#matplotlib.pyplot.hist), we can clearly see that most of the albums have an average track duration of **0 - 50,000 (ms)**. \n- Also, we can claim that our dataset is **highly skewed**, if we use the `t_dur` (Average) of the albums as the plotting feature.\n- Apart from this, our dataset also contains a bunch of outliers, which we can remove on the basis of  `t_dur` (Average), for instance, we can remove all the albums having more than an average track duration of **100,000 ms**.","9d739bb6":"## Conclusion\n- From the above [Boxplots](https:\/\/matplotlib.org\/stable\/api\/_as_gen\/matplotlib.pyplot.boxplot.html), we can have an overview of the outlier and non-outlier range of the rest of the features (*11 features*).\n- **Box plots** provide insight into distribution properties of the data. However, they can be challenging to interpret for the unfamiliar reader. The figure below illustrates the different visual features of a box plot.\n- The whiskers mark the range of the non-outlier data. The most common definition of non-outlier is [Q1 - 1.5xIQR, Q3 + 1.5xIQR], which is also the default in this function. Other whisker meanings can be applied via the `whis` parameter.\n- For further information, refer to the [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Box_plot) page of Boxplot. ","3d4fcb6e":"## Conclusion\n- From the above [Histogram](https:\/\/matplotlib.org\/stable\/api\/_as_gen\/matplotlib.pyplot.hist.html?highlight=hist#matplotlib.pyplot.hist), we can clearly see that most of the albums in the dataset have less than 50 tracks.\n- Also, we can claim that our dataset is **highly skewed**, if we use the `num_tracks` of the albums as the plotting feature.\n- Apart from this, our dataset also contains a bunch of outliers, which we can remove on the basis of  `num_tracks`, for instance, we can remove all the albums having more than 100 tracks","b729f908":"# Conclusion\n- From the above [WordCloud](https:\/\/pypi.org\/project\/wordcloud\/), we can see some of the prominent artists as per our 180K+ data-points.\n- Some of my favorite ones among these include *Taylor Swift, Nicki Minaj, Drake, Eminem and Ariana Grande*. What about yours?\n- As of now, we haven't given much thought as to how can we exploit this feature, but we will be waiting eagerly to see what all you come up with!","47a7dc04":"# Importing the Dataset","6a1dc752":"## Conclusion\n- The above plot is perhaps one of my most favorite visualizations. Not just because it is **interactive** and **beautiful**, which it is off-course, but because it gives us a really useful insight into our features.\n- Before finding out the significance, let's understand some basic things. **1st thing**, since this visualization is calculated on the basis of **inter-point distance(s)**, hence, it can only work with numerical features.\n- For other types of features, such as categorical features, text-based features, etc, we need to use the various featurizing techniques. These include techniques like one-hot encoding, BoW, TF-IDF, etc. \n- For simplicity, in order to visualize, we have simply dropped all the non-numerical features.\n- **2nd thing**, in order for us to use [T-SNE (T-Distributed Stochastic Neighbor Embedding)](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.manifold.TSNE.html), we need to make sure that the dataset doesn't contain any `NULL` values. \n- And in order to ensure that, we performed **Median-based Imputation**, which we can easily perform using the [SimpleImputer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.SimpleImputer.html) function from scikit-learn.\n- Now, let's discuss the significance of this plot. In our sampled dataset, we have 37 numerical features, but the important question that arises here is, can we somehow visualize the distribution of `popularity` across these 37 numerical features in a single plot?\n- This is where T-SNE steps in, whose main aim is to perform **Dimensionality Reduction** for Visualization purposes. So, we have used T-SNE, and we have transformed our data-points from having 37 to having 3 numerical features (3D-Embeddings).\n- And after that, we can easily visualize the distribution of `popularity` across these 3 embedded features, with the help of [3D-Scatter Plot](https:\/\/www.kaggle.com\/elemento\/getting-started-visualization\/edit\/run\/75908866).","0a6dff5a":"# Pre-Processing the Dataset\n- In the previous section, we have performed a plethora of visualization(s). In this section, you have to perform pre-processing on your dataset, so that your dataset can be ready for modeling purposes.\n- So, this is where you can outshine the other Kagglers. **Are you ready for it?**\n- But before that, I would like to highlight one **very important point**. In the previous section, we have performed all the visualization(s) without any pre-processing.\n- This means that the above visualizations and conclusions inherently contain the biases that the dataset itself may contain. \n- So, our suggestion to you is to eliminate all the biases from the dataset first, and then perform all these visualizations afterwards, to make sure you obtain **accurate visualizations**, based on which, you can take decisions for further pre-processing and modeling.","1dca60fc":"# Importing the Packages","333e0e8c":"### Conclusion\n- From the above [Bar Plot](https:\/\/matplotlib.org\/stable\/api\/_as_gen\/matplotlib.pyplot.bar.html#matplotlib.pyplot.bar), we can clearly see that the dataset consists majorly of numerical features.\n- In total, we have 37 numerical features (excluding `id` & `popularity`) and we have 6 non-numerical features.","707e53b6":"## Conclusion\n- From the above plots, we can have a quick overview of the *Distribution* of all the features in our dataset, with just a single line of code.\n- One might ask why didn't we did this in the first place. This is because it is pretty difficult to make any kind of decisions based on these subplots. It is just for getting a *Bird-Eye's View* of the dataset.","74d0966c":"# Interpreting the Dataset","0f5d1752":"## Conclusion\n- From the above [Histogram](https:\/\/matplotlib.org\/stable\/api\/_as_gen\/matplotlib.pyplot.hist.html?highlight=hist#matplotlib.pyplot.hist), we can clearly see that most of the albums in the dataset have a popularity lesser than 80.\n- Also, we can claim that our dataset is **well-distributed**, if we use the `popularity` of the albums as the plotting feature.\n- This is perhaps the most important visualization as it tells us that there is a decent balance in our dataset. Watch out for it while modelling!"}}