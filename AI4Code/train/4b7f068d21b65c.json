{"cell_type":{"a2931d07":"code","ff745e2d":"code","a57d12ea":"code","cf10893c":"code","0b660f53":"code","3956d357":"code","9ea4fd39":"code","92ef7284":"code","49b15f1f":"code","cc627f79":"code","4258ced9":"code","78818102":"markdown","f7d84a27":"markdown","858a399a":"markdown","79e23d5d":"markdown","d26b4c5e":"markdown","9dddf60b":"markdown"},"source":{"a2931d07":"import numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt","ff745e2d":"data = load_iris().data\ndata","a57d12ea":"km = KMeans(n_clusters = 3)\nkm.fit(data[:,:2])\ny_pred = km.predict(data[:,:2])\ny_pred","cf10893c":"centroids = km.cluster_centers_\ncentroids","0b660f53":"plt.scatter(data[:, 0], data[:, 1], c = y_pred)\nplt.scatter(5.7, 2.69, marker = '*', s = 500, color = 'red')\nplt.scatter(6.81276596,3.07446809, marker = '*', s = 500, color = 'red')\nplt.scatter(5.006,3.428, marker = '*', s = 500, color = 'red')\n","3956d357":"km.inertia_","9ea4fd39":"inertia_list = []\nfor k in np.arange(1, 6):\n    kn = KMeans(n_clusters = k)\n    kn.fit(data[:, :2])\n    inertia_list.append(kn.inertia_)\n    \ninertia_list","92ef7284":"plt.plot(np.arange(1, 6), inertia_list, 'o-')\nplt.xlabel('number of cluster', size = 20)\nplt.ylabel('Inertia index', size = 20)\nplt.grid()","49b15f1f":"from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\nimport matplotlib.pyplot as plt\n","cc627f79":"hierachical = linkage(data, method = 'complete')\ndendrogram(hierachical);\nlabels = fcluster(hierachical, 6, criterion = 'distance')\nlabels","4258ced9":"from sklearn.cluster import MeanShift\nms = MeanShift()\nms.fit(data)\nlabels = ms.predict(data)","78818102":"# DBSCAN","f7d84a27":"# How to evaluate our model?\nwe use inertia index which indicates that distance from each sample to centroids of its cluster or how spread out the clusters.\n(Lower is better)","858a399a":"# MeanShift","79e23d5d":"### Lets try different k:","d26b4c5e":"# KMeans method","9dddf60b":"# Hierarchical Clustering"}}