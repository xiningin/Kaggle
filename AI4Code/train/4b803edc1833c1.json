{"cell_type":{"160493b2":"code","87c344d5":"code","87aafc0b":"code","86d91cb8":"code","bcd36979":"code","4d2ff9aa":"code","d14c9ed2":"code","35902de3":"code","8755eecf":"code","4726784d":"code","4a27f136":"code","5ca5837d":"code","4cd05dfa":"code","de7a2768":"code","e0d29bf8":"code","2f97771c":"code","45a13d59":"code","ca8a18ed":"code","201c6e28":"code","ccf946d1":"code","989a10b3":"code","666abc16":"code","0bf870fe":"code","4499f2c8":"code","784c8e9b":"code","502aca1a":"code","7006fff9":"code","b79386be":"code","f49ee76b":"code","6b9ef117":"code","b870dbe1":"code","9697514e":"code","d1ca6d05":"code","135dd00c":"code","275b6698":"code","b54c6f64":"code","3a5f939f":"code","a60e1033":"code","525fa763":"code","adef9045":"code","43a80be5":"code","9a12a774":"code","6992962a":"code","380f58a3":"code","b5b6924e":"markdown","b5d46866":"markdown","1c71893b":"markdown","cc3af10d":"markdown","c7a5247d":"markdown","d983e9a4":"markdown","9fb2f62a":"markdown","7e180069":"markdown","3ea37f08":"markdown","2abecf97":"markdown","2194e13e":"markdown","df05efdb":"markdown","df4ce1a4":"markdown","b47ed186":"markdown","37cf2fd0":"markdown","d756e576":"markdown","7b05558a":"markdown","d11ecebe":"markdown","e101905e":"markdown","3e23e74a":"markdown","65bee9d4":"markdown","bd648a58":"markdown","c8404a2a":"markdown"},"source":{"160493b2":"%%javascript\n$.getScript('https:\/\/kmahelona.github.io\/ipython_notebook_goodies\/ipython_notebook_toc.js')","87c344d5":"dsnm='BSM14 DATASET' # 'BSM100 DATASET'   #this param also sets which training data is used - if 100 then by yclass else by label\n\n## Model Parameter Inputs\n\nif dsnm=='BSM100 DATASET':\n    c1_batsz= 28\n    c2_batsz= 35\n    al_batsz= 50\n    y= 'class'  #or 'class' 'labels'\n    nrm=  15 # 8 (new 4) for classes ; 14 (new 10) for emotions number of randome augmentation duplicates in addition to the 6 \nelse:\n    c1_batsz= 25\n    c2_batsz= 70\n    al_batsz= 50  #optimized 100\n    y='labels'\n    nrm=20  #number of random modifications #was 94; then added 4 more \n\nif dsnm=='BSM100 DATASET' and y=='class':\n    trainby='class'\nelse:\n    trainby='label'\n    \nepo=50  #50","87aafc0b":"# timer.py\n\nimport time\n\nclass TimerError(Exception):\n    \"\"\"A custom exception used to report errors in use of Timer class\"\"\"\n\nclass Timer:\n    def __init__(self):\n        self._start_time = None\n\n    def start(self):\n        \"\"\"Start a new timer\"\"\"\n        if self._start_time is not None:\n            raise TimerError(f\"Timer is running. Use .stop() to stop it\")\n\n        self._start_time = time.perf_counter()\n\n    def stop(self):\n        \"\"\"Stop the timer, and report the elapsed time\"\"\"\n        if self._start_time is None:\n            raise TimerError(f\"Timer is not running. Use .start() to start it\")\n\n        elapsed_time = time.perf_counter() - self._start_time\n        self._start_time = None\n        print(f\"Elapsed time: {elapsed_time:0.4f} seconds\")\n","86d91cb8":"!cp -r ..\/input\/imgaugp\/imgaug-master\/* .\/\nfrom imgaug import augmenters as iaa\nimport imgaug as ia\nprint(ia.__version__)","bcd36979":"#initialization\nimport pandas as pd\nimport numpy as np\nimport scipy.io as spio\nfrom scipy import ndimage\nfrom PIL import Image  #pillow package for image manupliation\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n%matplotlib inline\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom random import shuffle\nfrom tqdm import tqdm\nimport cv2\nimport pickle\nimport glob #used to clear out folder contents\nimport random #used to select random sample to move to test folder\nimport shutil #used to move test items to test folder\nimport skimage\nfrom sklearn.model_selection import train_test_split #used to split training set into train & test\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import *\nfrom keras.optimizers import *\nfrom keras.constraints import maxnorm\n\nfrom keras import backend\n# force channels-first ordering\nbackend.set_image_data_format('channels_last')  #our images have channels last which is not the keras default\nprint(backend.image_data_format())","4d2ff9aa":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfor dirname, _, filenames in os.walk('\/kaggle\/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nprint(os.getcwd())\n#os.mkdir('Eouts')\n#os.mkdir('modelcheckpoints')\n# remove the file\n#os.remove(\"myemtdata.pickle\")","d14c9ed2":"def showImg(imgarr_num):\n    pix = np.array(imgarr_num)\n    plt.imshow(pix)\n    plt.axis('off')\n    plt.show()\n    \ndef split_img(imgnm,rows, cols,sx,sy,xd,yd,parrynm,imgarnm):\n\n    xd = xd\n    yd = yd\n    x = sx\n    y = sy\n    n = 0\n    for rw in range (rows): \n        x=sx\n        #print('ROW:',rw, '@: ',x)\n        for cl in range(cols):\n            wkgimg=imgnm.copy()\n            #print('Col:',cl, '@: ',y)\n            fnm='em_'+str(n)\n            nm = wkgimg.crop((x, y, x+xd, y+yd)) #left, upper, right, lower\n            nm = nm.resize((246, 760), Image.ANTIALIAS)  #rize to x, y for feedinginto network\n            #print(nm.size)\n            #nm.save('train\/'+fnm+'.png')\n            imgarnm.append(fnm)\n            parrynm.append(nm)\n            x = x + xd\n            n +=1\n        y = y + yd-8 # move onto next row","35902de3":"if dsnm=='BSM100 DATASET':\n    #get 1 big image file of emotions\n    #not ideal but lets go with it .... MVP!\n    #os.chdir('\/Users\/jennifer_home\/Documents\/01_Masters\/Northwestern\/DL590_Thesis\/01_ClassificationModel\/models')\n    print(os.getcwd())\n    #os.chdir('..\/data\/raw\/')\n    #emtIM = Image.open('sensations_body.png') # no labels\n    bsm100IMG = Image.open('..\/input\/sensationimages\/sensations_body.png')\n    #emtIM = Image.open('AllDataImages.png') #with Labels\n    print(bsm100IMG)\n    bsm100IMG.size\n    print('Img size: ',bsm100IMG.size)\n\n\n    pilArray=[]\n    imgnms=[]\n    bsm100imgs=[]\n    bsm100nms=[]\n    #Split image into individual person image arrays\n    split_img(bsm100IMG,5,20,820,480.5,246,760,bsm100imgs,bsm100nms)# version with no labels on images These numbers split a (6336, 4896) pixel image into 5 rows & 20 columns\n\n    #split_img(emtIM,5,20,sx=14.3,sy=7,xd=59,yd=169) #this is for version with words\n\n    print('len(bsm100IMG): ',len(bsm100imgs), 'TYPE',type(bsm100imgs[1]), 'size', bsm100imgs[1].size)\n    print('len(bsm100nms): ',len(bsm100nms))\n\n    #list comprehension to make every image an array in the imglist\n    bsm100 = [np.array(i) for i in bsm100imgs] \n\n    print('Shape of the first image array', bsm100[0].shape)\n    bsm100nms[3]\n\n    # Now create labels and class arrays\n    # these are hard coded since the source had an image with these shown on the graphic\n\n    bsm100labels=['Anger','Fear','Failing','Guilt','Anxiety','Stress','Exhaustion','Tiredness','Mannia','Defecation','Heartbeat','Movement','Pride','Relaxation','Succeeding','Wanting','Orgasm','Craving','Feeling touch','Social Longing',\n            'Hositity','Losing','Shame','Despair','Nervousness','Shriving','Itching','Numbness','Acceleration','Eating','Breathing','Smelling','Winning','Laughing','Happiness','Pleasure','Love','Sympathy','Sexual Arousal','Lounging for',\n           'Despise','Disgust','Disappointment','Sadness','Lonliness','Aching','Flu','Coughing','Hotness','Thirst','Satiation','Tasting','Sleeping','Dazzled','Daydreaming','Gratefulness','Self-regulation','Surprise','Closeness','Togetherness',\n           'Ostractism','Panic','Social exclusion','Feeling pain','Depression','Toothache','Fever','Having Cold','Coldness','Hunger','Urination','Hearing','Vitality','Imagining','Thinking','Being Conscious','Attending','Memorizing','Remembering','Forgetting',\n           'Drunkenness','Hangover','Suffocation','Headache','Nauseous','Stomach flu','Heartburn','Dizziness','Sneezing','Sweating','Drinking','Seeing','Speaking','Reading','Reasoning','Inferring','Estimating','Recognition','Recollection','Waiting']\n\n    bsm100_yclass=['NE','NE','NE','NE','NE','NE','NE','NE','US','US','US','US','PE','PE','PE','PE','PE','PE','PE','US',\n            'NE','NE','NE','NE','NE','IL','IL','US','US','HS','HS','HS','PE','PE','PE','PE','PE','PE','PE','US',\n            'NE','NE','NE','NE','NE','IL','IL','IL','US','HS','HS','HS','US','US','CG','PE','US','US','PE','PE',\n            'NE','NE','NE','IL','NE','IL','IL','IL','US','HS','US','HS','US','CG','CG','CG','CG','CG','CG','US',\n            'IL','IL','US','IL','IL','IL','IL','IL','IL','US','HS','HS','HS','CG','CG','CG','CG','CG','CG','US']\n\n    #labels.to_csv('emotlabels.csv',index=False)\n\n    bsm100IMG\nelse:print('BSM100 DATASET not selected')","8755eecf":"if dsnm=='BSM14 DATASET':\n    print(os.getcwd())\n    #os.chdir('..\/data\/raw\/')\n    import sys\n    import PIL\n    from PIL import Image\n\n    #join basic & complex images\n\n    list_im = ['..\/input\/bodymaps14\/bodilymaps_basic.jpeg', '..\/input\/bodymaps14\/bodilymaps_complex.jpeg']\n    imgs    = [ PIL.Image.open(i) for i in list_im ]\n    # pick the image which is the smallest, and resize the others to match it (can be arbitrary image shape here)\n    min_shape = sorted( [(np.sum(i.size), i.size ) for i in imgs])[0][1]\n    print(imgs)\n    bsm14 = np.vstack( (np.asarray(i.resize(min_shape) ) for i in imgs ) ) #changes PIL to image to join\n    bsm14 = PIL.Image.fromarray(bsm14) #converts it back to a PIL for splitting\n    print(bsm14)\n    #bsm14=[]\n    #for image_path in list_im: \n    #    with Image.open(image_path) as image:    \n    #        im_arr = np.fromstring(image.tobytes(), dtype=np.uint8)\n    #        print(im_arr.shape)\n    #        #im_arr = im_arr.reshape((image.size[1], image.size[0], 3))\n    #    bsm14.append(im_arr)\n\n    #bsm14.save( 'bsm14.png' )\n\n    #bsm14 = Image.open('..\/input\/bodilymaps-14\/bodilymaps_basic.png')\n    #emtIM = Image.open('AllDataImages.png') #with Labels\n\n    bsm14.size\n    print('Img size: ',bsm14.size)\n\n    showImg(bsm14)\n\n    bsm14imgs=[]  #PIL type Object\n    bsm14nms=[]\n    #Split image into individual person image arrays\n    #(imgnam, rows, cols,sx (start X),sy(start Y),xd(X Delta),yd- Y DELTA) \n\n    split_img(bsm14,2,7,25,5,130,353,bsm14imgs,bsm14nms) # version with no labels on images These numbers split a (953, 700) pixel image into 14 images - 2 rows & 7 columns\n\n    print('len(bsm14img): ',len(bsm14imgs),'TYPE',type(bsm14imgs[1]))\n    print('len(bsm14nm): ',len(bsm14nms))\n    print(bsm14imgs[2].size)\n\n    #list comprehension to make every image an array in the imglist\n    bsm14 = [np.array(i) for i in bsm14imgs]   #ARRAY TYPE\n\n    imshp=bsm14[0].shape  #save the image shape for random augmentation modifications\n    print('BSM image shape',imshp)\n    showImg(bsm14[0])\n    bsm14labels=['Anger','Fear','Disgust','Happiness','Sadness','Surprise','Neutral','Anxiety','Love','Depression','Contempt','Pride','Shame','Envy']\n    bsm14_yclass=['NE','NE','NE','PE','NE','US','US','NE','PE','NE','NE','PE','NE','NE']\nelse: print('BSM14 DATASET Not Selected')","4726784d":"if dsnm=='BSM14 DATASET': \n    imgarry=bsm14\n    imgnms=bsm14nms\n    labels=bsm14labels\n    y_class=bsm14_yclass\n\nelif dsnm=='BSM100 DATASET':\n    imgarry=bsm100\n    imgnms=bsm100nms\n    labels=bsm100labels\n    y_class=bsm100_yclass\n\n\n\nprint('names: ',len(imgnms),'\\nlabels: ',len(labels),'\\nyclass:',len(y_class))","4a27f136":"# determine the dominant color\n# https:\/\/stackoverflow.com\/questions\/3241929\/python-find-dominant-most-common-color-in-an-image\n\nfrom PIL import Image\nimport cv2\n\ndef find_dominant_color(img):\n    #Resizing parameters\n    width, height = 246,760\n    #image = Image.open(filename) commented out since we are using an array instead of saved image files\n    #image = filename.resize((width, height), )\n    #image = cv2.resize(img, dsize=(width, height), interpolation=cv2.INTER_CUBIC)\n    #Get colors from image object\n    pixels = img.getcolors(width * height)\n    #Sort them by count number(first element of tuple)\n    sorted_pixels = sorted(pixels, key=lambda t: t[0])\n    #Get the most frequent color\n    dominant_color = sorted_pixels[-1][1]\n    return dominant_color\n\n#colorme = find_dominant_color(imgarry[0])\n\n\n### OR\n#from colorthief import ColorThief\n#palette = color_thief.get_palette(color_count=6)","5ca5837d":"## Image Duplication via filtering\nfrom scipy import misc\nprint(dsnm)\nprint(\"Original Image\")\nshowImg(imgarry[2])\n\nprint('-------- A sample image filtering -----------')\npix = np.array(imgarry[0])\npix=pix-30\nplt.imshow(pix)\nplt.axis('off')\nplt.show()\n\n","4cd05dfa":"# ia.seed(1)\n\n# Example batch of images.\n# The array has shape (32, 64, 64, 3) and dtype uint8.\n\nsometimes = lambda aug: iaa.Sometimes(0.5, aug)\n\n# Define our sequence of augmentation steps that will be applied to every image.\nseq = iaa.Sequential(\n    [\n        #\n        # Apply the following augmenters to most images.\n        #\n        iaa.Fliplr(0.2), # horizontally flip 50% of all images\n        #iaa.Flipud(0.2), # vertically flip 20% of all images\n\n        # crop some of the images by 0-10% of their height\/width\n        sometimes(iaa.Crop(percent=(0, 0.1))),\n\n        # Apply affine transformations to some of the images\n        # - scale to 80-120% of image height\/width (each axis independently)\n        # - translate by -20 to +20 relative to height\/width (per axis)\n        # - rotate by -15 to +15 degrees\n        # - shear by -16 to +16 degrees\n        # - order: use nearest neighbour or bilinear interpolation (fast)\n        # - mode: use any available mode to fill newly created pixels\n        #         see API or scikit-image for which modes are available\n        # - cval: if the mode is constant, then use a random brightness\n        #         for the newly created pixels (e.g. sometimes black,\n        #         sometimes white)\n        sometimes(iaa.Affine(\n            scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n            translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n            rotate=(-15, 15),\n            shear=(-16, 16),\n            order=[0, 1],\n            cval=(0, 255),\n            mode=ia.ALL\n        )),\n\n        #\n        # Execute 0 to 3 of the following (less important) augmenters per\n        # image. Don't execute all of them, as that would often be way too\n        # strong.\n        #\n        iaa.SomeOf((0, 3),\n            [\n                # Convert some images into their superpixel representation,\n                # sample between 20 and 200 superpixels per image, but do\n                # not replace all superpixels with their average, only\n                # some of them (p_replace).\n                sometimes(\n                    iaa.Superpixels(\n                        p_replace=(0, 1.0),\n                        n_segments=(20, 200)\n                    )\n                ),\n\n                # Blur each image with varying strength using\n                # gaussian blur (sigma between 0 and 3.0),\n                # average\/uniform blur (kernel size between 2x2 and 7x7)\n                # median blur (kernel size between 3x3 and 11x11).\n                iaa.OneOf([\n                    iaa.GaussianBlur((0, 3.0)),\n                    iaa.AverageBlur(k=(2, 7)),\n                    iaa.MedianBlur(k=(3, 11)),\n                ]),\n\n                # Sharpen each image, overlay the result with the original\n                # image using an alpha between 0 (no sharpening) and 1\n                # (full sharpening effect).\n                iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)),\n\n                # Same as sharpen, but for an embossing effect.\n                iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)),\n\n                # Search in some images either for all edges or for\n                # directed edges. These edges are then marked in a black\n                # and white image and overlayed with the original image\n                # using an alpha of 0 to 0.7.\n                sometimes(iaa.OneOf([\n                    iaa.EdgeDetect(alpha=(0, 0.7)),\n                    iaa.DirectedEdgeDetect(\n                        alpha=(0, 0.7), direction=(0.0, 1.0)\n                    ),\n                ])),\n\n                # Add gaussian noise to some images.\n                # In 50% of these cases, the noise is randomly sampled per\n                # channel and pixel.\n                # In the other 50% of all cases it is sampled once per\n                # pixel (i.e. brightness change).\n                iaa.AdditiveGaussianNoise(\n                    loc=0, scale=(0.0, 0.05*255), per_channel=0.5\n                ),\n\n                # Either drop randomly 1 to 10% of all pixels (i.e. set\n                # them to black) or drop them on an image with 2-5% percent\n                # of the original size, leading to large dropped\n                # rectangles.\n                iaa.OneOf([\n                    iaa.Dropout((0.01, 0.1), per_channel=0.5),\n                    iaa.CoarseDropout(\n                        (0.03, 0.15), size_percent=(0.02, 0.05),\n                        per_channel=0.2\n                    ),\n                ]),\n\n                # Invert each image's channel with 5% probability.\n                # This sets each pixel value v to 255-v.\n                iaa.Invert(0.05, per_channel=True), # invert color channels\n\n                # Add a value of -10 to 10 to each pixel.\n                iaa.Add((-10, 10), per_channel=0.5),\n\n                # Change brightness of images (10-50% of original value).\n                iaa.Multiply((0.1, 0.3), per_channel=0.5),\n\n                # Improve or worsen the contrast of images.\n                iaa.LinearContrast((0.5, 2.0), per_channel=0.5),\n\n                # Convert each image to grayscale and then overlay the\n                # result with the original with random alpha. I.e. remove\n                # colors with varying strengths.\n                iaa.Grayscale(alpha=(0.0, 1.0)),\n\n                # In some images move pixels locally around (with random\n                # strengths).\n                sometimes(\n                    iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25)\n                ),\n\n                # In some images distort local areas with varying strength.\n                #sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.05)))\n            ],\n            # do all of the above augmentations in random order\n            random_order=True\n        )\n    ],\n    # do all of the above augmentations in random order\n    random_order=True\n)\n\n\nimages_aug = seq(images=imgarry[9:11])\nimages_aug\n","de7a2768":"\nmodimgs=[]  # complete list of modified images\nimgnm=[]    # complete list of modified imgage names\nmodcls=[]   # complete list of modified imgage classes\nmodlabs=[]  # complete list of modified imgage emotions\nmodid=[]    # complete list of modified identification identifiers\nmodimid=[]  #the related image index from the raw\n\ndef duplc(aug,nm,lab,cls,imid,mid):  #this appends each image and its metadata to the various lists\n    modimgs.append(aug)\n    imgnm.append(nm)\n    modlabs.append(lab)\n    modcls.append(cls)\n    modid.append(mid)\n    modimid.append(imid)\n    \ndef modallimgs(img,nm,lab,cls,imid,nrm=9):  #do this to keep all labels together for verification of code writing\n    \n    #rescale colors from 0-255 to 0-1\n    #img=cv2.resize(image, (128, 128))\n    \n    ##save original in new array\n    #modifier #1\n    mid=1 #mod id\n    aug=img\n    nm==nm\n    onm=nm\n    imid==imid  #orig image id\n    duplc(aug,nm,lab,cls,imid,mid)\n    \n    images_aug = seq(images=imgarry[9:11])\n    for i in range(nrm):#Modifier #2\n        mid=2\n        rdeg=random.randint(-20,20)\n        aug=datagen.apply_transform(x=img, transform_parameters={'theta':rdeg})\n        nm=(nm+'_rot_'+str(rdeg)) #append the name to the name array\n        duplc(aug,nm,lab,cls,imid,mid)\n\nimages_aug = seq(images=imgarry[9:11])\nimages_aug\nfor i in range(3):#Modifier #2\n    modimgs.append(seq(images=imgarry[9:11]))\n    modlabs.append(labels[9:11])\n    modid.append(i)\nprint('LENGTHs of LISTS:',len(modimgs),len(modlabs),len(modid))\n\n\ndef display_multiple_img(images, rows = 1, cols=1):\n    figure, ax = plt.subplots(nrows=rows,ncols=cols )\n    for ind,title in enumerate(images):\n        try:\n                if ind<rows*cols:\n                imgarray=images[title][0][ind]\n                #print('INDEX:',ind,imgarray)\n                ax.ravel()[ind].imshow(imgarray)\n                ax.ravel()[ind].set_title(title)\n                ax.ravel()[ind].set_axis_off()\n            plt.tight_layout()\n            plt.show()\n\ntotal_images = 10\nasarryimages= [np.asarray(modimgs)]\nprint(len(asarryimages))\n\nimages = {'Image'+str(i): modimgs for i in range(total_images)}\n\ndisplay_multiple_img(images, 2, 5)","e0d29bf8":"from keras_preprocessing.image import ImageDataGenerator\nimport cv2\nimport imgaug.augmenters as iaa\n\ndef blur(img):\n    return (cv2.blur(img,(5,5)))\n\n#oneof = iaa.OneOf([\n#    iaa.cval=(0, 255),\n#    iaa.Add((-10, 10), per_channel=0.5)\n#])\n\n\nseq = iaa.Sequential([\n    iaa.Crop(px=(1, 16), keep_size=False),\n    iaa.Fliplr(0.5),\n    iaa.GaussianBlur(sigma=(0, 3.0))\n])\n \n#this is used for generating more training data and validation data\ndatagen = ImageDataGenerator(\n    #featurewise_center=True,\n    #featurewise_std_normalization=False,\n    rotation_range=15,\n    width_shift_range=.2,\n    height_shift_range=.1,\n    #rescale=1.\/255,\n    zoom_range=[.8,1.2] , #lower upper zoom\n    vertical_flip=False,\n    horizontal_flip=True,\n    #brightness_range=[.95,1.05],\n    #preprocessing_function=blur,\n    fill_mode='reflect',\n    )\n\n\n# USE THIS TO IMPROVE IMAGE MODS\n## http:\/\/machinelearninguru.com\/computer_vision\/basics\/convolution\/image_convolution_1.html\n## or ImageDataGenerator module\n\nmodimgs=[]  # complete list of modified images\nimgnm=[]    # complete list of modified imgage names\nmodcls=[]   # complete list of modified imgage classes\nmodlabs=[]  # complete list of modified imgage emotions\nmodid=[]    # complete list of modified identification identifiers\nmodimid=[]  #the related image index from the raw\n\ndef duplc(aug,nm,lab,cls,imid,mid):  #this appends each image and its metadata to the various lists\n    modimgs.append(aug)\n    imgnm.append(nm)\n    modlabs.append(lab)\n    modcls.append(cls)\n    modid.append(mid)\n    modimid.append(imid)\n    \n    \ndef modmyimg(img,nm,lab,cls,imid,nrm=9):  #do this to keep all labels together for verification of code writing\n    \n    #rescale colors from 0-255 to 0-1\n    #img=cv2.resize(image, (128, 128))\n    \n    ##save original in new array\n    #modifier #1\n    mid=1 #mod id\n    aug=img\n    nm==nm\n    onm=nm\n    imid==imid  #orig image id\n    duplc(aug,nm,lab,cls,imid,mid)\n    \n    \n    for i in range(nrm):#Modifier #2\n        mid=2\n        rdeg=random.randint(-20,20)\n        aug=datagen.apply_transform(x=img, transform_parameters={'theta':rdeg})\n        nm=(nm+'_rot_'+str(rdeg)) #append the name to the name array\n        duplc(aug,nm,lab,cls,imid,mid)\n\n        #Modifier #3 #sharpens\n        mid=mid+1\n       #blurred_f = ndimage.gaussian_filter(img, 1)\n       # filter_blurred_f = ndimage.gaussian_filter(blurred_f, 1)\n        alpha = random.randint(-5,5)\n        #aug=blurred_f + alpha * (blurred_f - filter_blurred_f)\n        image=img\n        #aug=sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.05)))\n        iaa.Add((-3, 3), per_channel=0.2)\n        nm=(nm +'_recolor'+str(mid))\n        #nm=(nm +'_shpn'+str(alpha))\n        duplc(aug,nm,lab,cls,imid,mid)\n\n        #Modifier #4\n        zx=random.uniform(-0.8,1.3)\n        ty=random.uniform(-5,5)\n        aug=datagen.apply_transform(x=img, transform_parameters={'zx':zx, 'ty':ty})\n        nm=(nm +'_shift')\n        mid=mid+1\n        duplc(aug,nm,lab,cls,imid,mid)\n\n        #Modifier #5\n        mid=mid+1\n        sigma=random.uniform(0,1)\n        aug = ndimage.gaussian_filter(img, sigma)\n        nm=(nm +'blur'+str(sigma))\n        duplc(aug,nm,lab,cls,imid,mid)\n\n        #Modifier #6\n        mid=mid+1\n        blurred_f = ndimage.gaussian_filter(img, 1)\n        filter_blurred_f = ndimage.gaussian_filter(blurred_f, 1)\n        alpha = random.randint(-10,10)\n        aug=blurred_f + alpha * (blurred_f - filter_blurred_f)\n        duplc(aug,nm,lab,cls,imid,mid)\n\n        #Modifier #7\n        zx=random.uniform(-0.8,1.3)\n        ty=random.uniform(-5,5)\n        aug=datagen.apply_transform(x=img, transform_parameters={'zx':zx, 'ty':ty})\n        nm=(nm +'_shift')\n        mid=mid+1\n        duplc(aug,nm,lab,cls,imid,mid)\n\n       \n        #Modifier #8\n        mid=mid+1\n        aug=seq(image=img)  #use imageS if entire array of images\n        nm=(nm +'_shpn'+str(alpha)+'_'+str(mid))\n        duplc(aug,nm,lab,cls,imid,mid)\n\n        #Modifier #9\n        mid=mid+1\n        #aug=recolor(image=img)  #use imageS if entire array of images\n        image=img\n        iaa.Add((-10, 10), per_channel=0.5)\n        nm=(nm +'_recolor'+str(mid))\n        duplc(aug,nm,lab,cls,imid,mid)\n        \n        \n        #Modifier #10\n        # txfm=get_random_transform()\n        img=img\n        rs=random.randint(0,5)\n        #aug=modimgs.append(datagen.random_transform(img, seed=rs))\n        aug=datagen.random_transform(img, seed=None)\n        mid=mid+1\n        nm=(onm +'_random'+str(mid))\n        duplc(aug,nm,lab,cls,imid,mid)\n   \n    \n# For each image in imgarry add it to the modimgs array, with its label, then rotate it a few times, then do more agumentation \nfor cnt, data in enumerate(imgarry):   \n    nm=imgnms[cnt]\n    lab=labels[cnt]\n    cls=y_class[cnt]\n    idc=cnt #count of images\n    #nrm  number of random mods is defined in the data selection block above\n    modmyimg(data,nm,lab,cls,idc,nrm) \n    \nprint('Image Modifications Complete')\nprint(len(modimgs),'Total Images')","2f97771c":"#create thumbnails for image class & emotion comparision\ntnimgs_PILs=[] #Thumbnail images\n#images_aug or modimgs\nfor cnt, i in enumerate(modimgs):\n    try:\n        #print(type(i),i.size)\n        im=Image.fromarray(i)   \n        #print(type(im),im.width, im.height)\n        (width, height) = (im.width \/\/ 3, im.height \/\/ 3) # Provide the target width and height of the image\n        im_resized = im.resize((width, height), 3)\n        #print(\"I'm resized\",im_resized.size)\n        tnimgs_PILs.append(im_resized)\n    except:\n        try:  # this catches modifications where color has been removed\n            im=Image.fromarray((test * 255).astype(np.uint8))\n            (width, height) = (im.width \/\/ 3, im.height \/\/ 3) # Provide the target width and height of the image\n            im_resized = im.resize((width, height), 3)\n            #print(\"I'm resized\",im_resized.size)\n            tnimgs_PILs.append(im_resized)\n        except:    \n            print('**************************I SKIPPED ONE',cnt)\n            next\nprint('total number of images:',len(tnimgs_PILs))\n\n# Create a dataframe to \"see the mods\"\nimgdf = pd.DataFrame(list(zip(modimid, modlabs,modcls,modid,imgnm,modimgs,tnimgs_PILs)), \n               columns =['imID', 'Label','Class','ModID','Name','ImgArray','TnImgs']) \n## ONLY USED FOR VISUALS\nprint(len(imgdf))\nimgdf","45a13d59":"# Lets create a dataframe of all our modifications - create a dictionary of our labels then turn into df\n\n#d = {'imID':modimid,'Label':modlabs,'Class':modcls,'ModID':modid,'Name':imgnm,'Image Array':modimgs}\n\n\n","ca8a18ed":"print('Now I have',len(modimgs),'images in my dataset \\nHere are some examples of the augmentation of the Anger silloutte')\nprint('     Lets look at all the images for mod1: \\n')\n\nmod1=imgdf.loc[imgdf['Label'] == 'Anger','ImgArray']\nprint(type(mod1))\n\nmod1_tn=[]\nfor i in mod1:\n    tn_array = np.asarray(i)\n    mod1_tn.append(tn_array)\n    \nprint(type(mod1_tn))\n\nfig=plt.figure(figsize=(12, 6),linewidth=5, edgecolor=\"#04253a\")\n\ncolumns = 10\nrows = 4\nfor cnt, img in enumerate(mod1_tn):  \n    try:\n        i=cnt+1\n        fig.add_subplot(rows, columns, i)\n        plt.imshow(img)\n        plt.axis('off') \n    except:\n        next\nplt.show()\n\nfig.savefig('modsimages.jpeg')\n#unique(modimgs)\n#modimgs[1].shape\n#modimgs[0][1].shape\n#imshow(mod1_tn[3])","201c6e28":"x=random.randint(0,len(modimgs)-1)\n\ndef getaugs(x):\n    label=imgdf.iloc[x]['Label']\n    print(label)\n    augdf=imgdf.loc[imgdf['Label'] == label]\n    #print(augdf)\n\nmyaugdf=getaugs(x,)\n#imgar=myaugdf['ImgArray'].to_list()\n\n\n#for i in imgar:\n#    plt.imshow(i)","ccf946d1":"testv=83  #change this number to test image consistency\nprint('Example of agumented data')\nprint('# of images: ',len(modimgs))\nprint('Img Name: ',imgnm[testv])\nprint('Label: ',modlabs[testv])\nprint('Class: ', modcls[testv])\n\nshowImg(modimgs[testv])\n#Image.fromarray(modimgs[testv])\nprint('***** DATA IS AUGMENTED ***** READY TO MODEL *****')\n","989a10b3":"#Use Skikit to split arrays\n#imgarray, y_class = np.arange(10).reshape((5, 2)), range(5)\nsplitsize=.15\n\nif dsnm=='BSM100 DATASET' and y=='class':\n    print('Training by y_class')\n    X_train, X_test, y_train, y_test = train_test_split(modimgs, modcls, test_size=splitsize, random_state=42)\nelse:\n    print('Training by label')\n    X_train, X_test, y_train, y_test = train_test_split(modimgs, modlabs, test_size=splitsize, random_state=42)\nprint('***** DATA SPLIT ******', splitsize)","666abc16":"# lets create a df to translate between acryn, int, string \n\n# initialize list of lists \nclassdata = [['NE','Negative Emotion',0], ['PE', 'Positive Emotion',1], ['IL','Illness', 2],\n       ['HS', 'Homeostasis' ,3],['CG',  'Cognition',  4], ['US', 'Unspecific', 5]] \n\n\n# BSM14 clsdf table\n# initialize list of lists \nbsm14data = [['Anger','Anger',0], ['Fear', 'Fear',1], ['Disgust','Disgust', 2],\n       ['Happiness', 'Happiness' ,3],['Sadness',  'Sadness',  4], ['Surprise', 'Surprise', 5],\n       ['Neutral', 'Neutral' ,6],['Anxiety',  'Anxiety',  7], ['Love', 'Love', 8],\n       ['Depression', 'Depression' ,9],['Contempt',  'Contempt',  10], ['Pride', 'Pride', 11], \n       ['Shame',  'Shame',  12], ['Envy', 'Envy', 13]]\n\nlabeldata=[]\nfor cnt,i in enumerate(labels):\n    labeldata.append([i,i,cnt])\nprint(labeldata[:10])   \n    \n# Create the pandas DataFrame \nif trainby=='class':\n    data=classdata\nelse:\n    data=labeldata\n\nclsdf = pd.DataFrame(data, columns = ['Acr', 'ECstr','ECint'])\n\nclsdf.set_index('Acr' ,inplace=True, drop=False)\n# print dataframe. \nprint(clsdf)\n\n#lookups are done like this:\n#To select rows whose column value equals a scalar, some_value, use ==:\n","0bf870fe":"#X_train, X_test, y_train, y_test\n\n# [1,0,0,0,0,0] - Negative Emotion NE\n# [0,1,0,0,0,0] - Positive Emotion PE\n# [0,0,1,0,0,0] - Illnesses\n# [0,0,0,1,0,0] - Homeostasis\n# [0,0,0,0,1,0] - Cognition\n# [0,0,0,0,0,1] - Unspecifc\n# encode the labels (which are currently strings) as integers and then\n# one-hot encode them\n#le = LabelEncoder()\n#labels = le.fit_transform(labels)\n#labels = to_categorical(labels, 2)\n\ndef one_hot_label(labels):\n    ohl=[]\n    n_classes=len(clsdf)\n    a=np.empty((n_classes),int)\n    for i, y in enumerate(labels):\n        a=clsdf.loc[y, 'ECint']\n        ohl.append(a)\n    #print('OHL',ohl)\n    return ohl\n\n   \n# run labels through function to turn string ('NE') values into numerical\nohl_trn=one_hot_label(y_train)\nohl_tst=one_hot_label(y_test)\nn_classes=len(clsdf)\n    \n# change numerical into categorical\n# for example: a = 0 is => #a=np.array([1,0,0,0,0,0]) #a = 5 => np.array([0,0,0,0,0,1])\ny_trainC = keras.utils.to_categorical(ohl_trn, n_classes)\ny_testC = keras.utils.to_categorical(ohl_tst, n_classes)","4499f2c8":"#Lets see what our images look like:\ntestv=65\nprint('Emotional Class',y_train[testv])\nprint('Binary:',y_trainC[testv])\nshowImg(X_train[testv])\n","784c8e9b":"## these reshape the data to feed into the neural net\n\n#run images through function\n\n#(-1 - ?, # of pixels, # of pixels, 3- color)\n#[ expression for item in list \nprint('Shape of incoming images: ',modimgs[testv].shape,' \/Data type:',modimgs[testv].dtype, '\/Image size: ',modimgs[testv].size)\n\ntr_img_data = np.array(np.array([i for i in X_train]).reshape(-1,760,246,3))\ntst_img_data = np.array(np.array([i for i in X_test]).reshape(-1,760,246,3))\n\n\n\ntr_lbl_data = np.array(np.array([i for i in y_trainC])) #use this with the categorical function\ntst_lbl_data = np.array(np.array([i for i in y_testC]))\n\nprint('Train img data: ',len(tr_img_data), tr_img_data.shape, type(tr_img_data))\nprint('Train label data: ',len(tr_lbl_data), tr_lbl_data.shape, type(tr_lbl_data))\nprint('Validation image: ',len(tst_img_data) , tst_img_data.shape, type(tst_img_data))\nprint('Validation data: ',len(tst_lbl_data))\nprint(tr_lbl_data[0])\n","502aca1a":"import math\n\ndef divisorGenerator(n):\n    large_divisors = []\n    for i in range(1, int(math.sqrt(n) + 1)):\n        if n % i == 0:\n            yield i\n            if i*i != n:\n                large_divisors.append(n \/ i)\n    for divisor in reversed(large_divisors):\n        yield divisor\n\n\ntraningsize = len(tr_img_data)\ndivisors=(list(divisorGenerator(traningsize)))\ni=(int(len(divisors)*.6))\nbatchsize = divisors[i]\nbatches=int(traningsize\/batchsize)\nprint('Batchsize',batchsize,'\\nBatches',batches)","7006fff9":"def modelplots(mdname, modelhis,plttitle,dsnm,batchs):\n    # evaluate the model\n    train_acc = mdname.evaluate(tr_img_data, tr_lbl_data, verbose=0)\n    test_acc = mdname.evaluate(tst_img_data, tst_lbl_data, verbose=0)\n    print('Train Loss: %.3f, Test Loss: %.3f' % (train_acc[0], test_acc[0]))\n    print('Train accuracy: %.3f, Test accuracy: %.3f' % (train_acc[1], test_acc[1]))\n    acc='Train accuracy: %.3f, Test accuracy: %.3f' % (train_acc[1], test_acc[1])\n\n    accuracy = modelhis.history['accuracy']\n    val_accuracy = modelhis.history['val_accuracy']\n    loss = modelhis.history['loss']\n    val_loss = modelhis.history['val_loss']\n    epochs = range(len(accuracy))\n    vepochs = range(len(val_accuracy))\n    \n    print('Lengths: acc',len(accuracy),'loss',len(loss),'val accur',len(val_accuracy),'val loss',len(val_loss),'epochs',len(epochs))\n\n    # Define plot space\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 8))\n\n    # Define x and y axes\n    # Plot training & validation accuracy values\n    ax1.plot(epochs, accuracy, 'bo', label='Training accuracy')\n    ax1.plot(vepochs, val_accuracy, 'b', label='Test accuracy')\n    tit=plttitle+' predicting '+y+' with '+dsnm+' & Batch Size: '+batchs+'\\n '+acc\n    ax1.set(title=tit,\n            ylabel='Accuracy',\n            xlabel='Epoch')\n    ax1.legend()\n   \n\n    # Plot training & validation loss values\n    los='Train Loss: %.3f, Test Loss: %.3f' % (train_acc[0], test_acc[0])\n    ax2.plot(epochs, loss, 'bo', label='Training Loss')\n    ax2.plot(vepochs, val_loss, 'b', label='Validation Loss')\n    tit2=plttitle+' predicting '+y+' with '+dsnm+' & Batch Size: '+batchs+'\\n '+ los\n    ax2.set(title=tit2,\n            ylabel='Loss',\n            xlabel='Epoch')\n    ax2.legend()\n    \n    plt.tight_layout()\n    fname=dsnm+'_'+plttitle+'_'+y+'.png'\n    plt.savefig(fname=fname,format='png')","b79386be":"print('******* Start Simple CNN Model ********')\n#with tpu_strategy.scope():  #comment out if not using TPU\ncnn1 = Sequential()\ncnn1.add(Conv2D(32, kernel_size=(8, 6), activation='relu', input_shape=(760,246,3)))\nprint('Model Shape post 1st Conv2D',cnn1.output_shape)\ncnn1.add(MaxPooling2D(pool_size=(4, 4)))\n#cnn1.add(Dropout(0.1))\ncnn1.add(BatchNormalization())\n\ncnn1.add(Flatten())  #Transforms 3D activation map into 1D array to feed into dense layer\nprint('Model Shape post Flattening',cnn1.output_shape)\n\ncnn1.add(Dense(128, activation='relu'))\ncnn1.add(BatchNormalization())\n#cnn1.add(Dropout(0.3))\ncnn1.add(Dense(n_classes, activation='softmax')) # n_classes - # of categories we want to stuff into 6 or 14\nprint('Model Shape post Dense Softmax Layer',cnn1.output_shape)\n\ncnn1.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adam(),\n              metrics=['accuracy'])\ncnn1.summary()\nprint()\n# code to train the model\n\nverbosity=1 #output logging detail information (0 min - 2 max)\n\nprint('MODELING PARAMETERS: \\nBatch Size: ',c1_batsz,'\\nEpoches: ',epo,'\\nVerbosity: ',verbosity)\nprint('=================================================================')\nhiscnn1=cnn1.fit(tr_img_data, tr_lbl_data, batch_size=c1_batsz, epochs=epo, verbose=verbosity, \n          validation_data=(tst_img_data, tst_lbl_data), validation_freq=1)  #val_freq - how many training epochs to run before a new validation keep at 1 for plotting to work\n\n\n","f49ee76b":"cnn1score, cnn1acc = cnn1.evaluate(tst_img_data, tst_lbl_data,\n                            batch_size=c1_batsz)","6b9ef117":"split= ',{:.3f}'.format(tst_lbl_data.shape[0]\/tr_lbl_data.shape[0])\nprint('TR Img:',tr_img_data.shape, 'TR lbl:',tr_lbl_data.shape,'\\nVal Img:',tst_img_data.shape, 'Var lbl:',tst_lbl_data.shape,'\\nTraining\/test Split:',split ,'Batch Size: ',c1_batsz, 'Epochs',epo)\n\nmodelplots(cnn1,hiscnn1,\"CNN1\",dsnm, str(c1_batsz))","b870dbe1":"print('******* Start Moderate CNN Model ********')\nfrom keras.optimizers import SGD\n# First Convoultion layer with pooling and Batch Normalization\ncnn2 = Sequential()\ncnn2.add(Conv2D(32, kernel_size=(8, 6), activation='relu', input_shape=(760,246,3)))\ncnn2.add(MaxPooling2D(pool_size=(4, 4)))\n#cnn2.add(Dropout(0.1))\ncnn2.add(BatchNormalization()) \n\n# # Second conv layer with pooling & batch Normalization\ncnn2.add(Conv2D(filters=256,kernel_size=(3,3), activation='relu'))\ncnn2.add(MaxPooling2D(pool_size=(4,4)))\n#cnn2.add(Dropout(0.1))\ncnn2.add(BatchNormalization())  #reduces overfitting and resists vanishing gradient descent\n    \ncnn2.add(Flatten())  #Transforms 3D activation map into 1D array to feed into dense layer\nprint('Model Shape post 1st Conv2D',cnn2.output_shape)\ncnn2.add(Dense(128, activation='relu'))\n#cnn2.add(Dropout(0.05))   #at risk of overfitting since we don't have enough data\ncnn2.add(Dense(n_classes, activation='softmax')) #6 bc i have 6 classes\nprint('Model Shape post 1st Conv2D',cnn2.output_shape)\n\ncnn2.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adam(),\n              metrics=['accuracy'])\ncnn2.summary()\nprint()\n# code to train the model\n\nverbosity=0 #output logging detail information (0 min - 2 max)\nsteps_per_epoch=300\n#epo=100\nprint('MODELING PARAMETERS: \\nBatch Size: ',c2_batsz,'\\nEpoches: ',epo,'\\nVerbosity: ',verbosity)\nprint('=================================================================')\nhiscnn2=cnn2.fit(tr_img_data, tr_lbl_data, validation_data=(tst_img_data, tst_lbl_data), \n                 batch_size=c2_batsz, epochs=epo, verbose=verbosity, validation_freq=1)\nlearning_rate = 0.1\ndecay_rate = learning_rate \/ epo\nmomentum = 0.8\n#sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n#model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n#model.fit(tr_img_data, tr_lbl_data, validation_split=0.33, epochs=epochs, batch_size=28, verbose=2)","9697514e":"split= ',{:.3f}'.format(tst_lbl_data.shape[0]\/tr_lbl_data.shape[0])\nprint('TR Img:',tr_img_data.shape, 'TR lbl:',tr_lbl_data.shape,'\\nVal Img:',tst_img_data.shape, 'Var lbl:',tst_lbl_data.shape,'\\nTraining\/test Split:',split ,'Batch size: ',c2_batsz, 'Epochs',epo)\nmodelplots(cnn2,hiscnn2,\"CNN2\",dsnm,str(c2_batsz))","d1ca6d05":"print('******* Start ALEXNET Modeling ********')\nprint('ALEXNET TAKES LOTS MUCH MEMORY TO RUN... USE KAGGLE')\n\ndef create_model():\n    #with tpu_strategy.scope(): #comment out if not using TPUs\n    model = Sequential()  #Instantiate an empty model\n\n    # # First Convoultion layer with pooling\n    model.add(Conv2D(64, kernel_size=(8,6),strides=(2,2), activation='relu', input_shape=[760,246,3]))\n    print('Model Shape post 1st Conv2D',model.output_shape)\n    model.add(MaxPooling2D(pool_size=(3,3), strides=(3,2)))\n    model.add(BatchNormalization())\n\n    # # Second conv layer with pooling\n    model.add(Conv2D(filters=256,kernel_size=(3,3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(6,6), strides=(2,2)))\n    model.add(BatchNormalization())\n\n    # # Third Conv layer with pooling & dropout\n    model.add(Conv2D(filters=384, kernel_size=(3,3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(6,6), strides=(2,2))) #Reduces computational complexity\n    model.add(Dropout(rate=0.2)) #Reduces amount of overfitting\n    model.add(BatchNormalization())\n\n\n    model.add(Flatten())  # transforms 3D activation map into 1D array to feed into dense layer\n    print('Model Flattend Shape',model.output_shape)\n    # # Dense layer with dropout\n    #model.add(Dense(32, activation='relu'))\n    model.add(Dropout(rate=0.5)) #Reduces amount of overfitting Input_shape required just on first dense layer\n    # model.add(Dense(4096, activation='tanh'))\n    # model.add(Dropout(rate=0.5))\n\n    # # Output layer\n    model.add(Dense(n_classes, activation='softmax'))\n\n    model.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adam(),\n              metrics=['accuracy'])\n    return model\n\n\n# Create a basic model instance\nalex = create_model()\n\n# Display the model's architecture\nalex.summary()\n\n# code to train the model\n\nverbosity=0 #output logging detail information (0 min - 2 max)\n\nprint('MODELING PARAMETERS: \\nBatch Size: ',al_batsz,'\\nEpoches: ',epo,'\\nVerbosity: ',verbosity)\nprint('=================================================================')\n\nalexhis=alex.fit(tr_img_data, tr_lbl_data, validation_data=(tst_img_data, tst_lbl_data), batch_size=al_batsz, epochs=epo, verbose=verbosity, validation_freq=1)\n\n\n# Save the entire model as a SavedModel in HDF5 format.\nalex.save('Alex_adam.h5')\n","135dd00c":"split= ',{:.3f}'.format(tst_lbl_data.shape[0]\/tr_lbl_data.shape[0])\nprint('TR Img:',tr_img_data.shape, 'TR lbl:',tr_lbl_data.shape,'\\nVal Img:',tst_img_data.shape, 'Var lbl:',tst_lbl_data.shape,'\\nTraining\/test Split:',split ,'Batches: ',al_batsz, 'Epochs',epo)\nmodelplots(alex,alexhis,\"ModAlex\",dsnm,str(al_batsz))","275b6698":"cnn1_acc,cnn1_loss = cnn1.evaluate(tr_img_data, tr_lbl_data, verbose=0)\ncnn2_acc,cnn2_loss= cnn2.evaluate(tr_img_data, tr_lbl_data, verbose=0)\nalex_acc,alex_loss = alex.evaluate(tr_img_data, tr_lbl_data, verbose=0)\nprint(type(cnn1_acc),cnn1_acc)","b54c6f64":"#Comparision Plot\n\n# evaluate the model to get acc & loss for test data\n\n#use this to show training acc & loss  accuracy & loss\ncnn1_acc = hiscnn1.history['val_accuracy']\ncnn1_loss = hiscnn1.history['val_loss']\ncnn2_acc = hiscnn2.history['val_accuracy']\ncnn2_loss = hiscnn2.history['val_loss']\nalex_acc = alexhis.history['val_accuracy']\nalex_loss = alexhis.history['val_loss']\n\nc1_epo = range(len(cnn1_acc))\nc2_epo = range(len(cnn2_acc))\na_epo = range(len(alex_acc))\n\n\n# Define plot space\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 3))\n\n# Plot training & validation accuracy values\nax1.plot(c1_epo, cnn1_acc, linestyle='dashed', color='blue',label='CNN1 accuracy')\nax1.plot(c2_epo, cnn2_acc, linestyle='dotted', color='red', label='CNN2 accuracy')\nax1.plot(a_epo, alex_acc, linestyle='solid', color='black',label='Alex accuracy')\n \nif dsnm=='BSM100 DATASET':\n    ax1.axis([0,102,.22,1.02]) \nelif dsnm=='BSM14 DATASET':\n    ax1.axis([0,102,.92,1.02]) \ntit='Validation Accuracy Model Comparisons with '+dsnm\nax1.set(title=tit,\n        ylabel='Accuracy',\n        xlabel='Epoch')\nax1.legend()\n\n\n# Plot training & validation loss values\nax2.plot(c1_epo, cnn1_loss, linestyle='dashed',color='blue', label='CNN1 Loss')\nax2.plot(c2_epo, cnn2_loss, linestyle='dotted', color='orange',label='CNN2 Loss')\nax2.plot(a_epo, alex_loss, linestyle='solid', color='black',label='Alex Loss')\nif dsnm=='BSM100 DATASET':\n    ax2.axis([0,102,-.2,1.8]) \nelif dsnm=='BSM14 DATASET':\n    ax2.axis([0,102,-.25,1.5])\ntit2='Validation Loss Model Comparisions with '+dsnm+' by'+y\nax2.set(title=tit2,\n        ylabel='Loss',\n        xlabel='Epoch')\nax2.legend()\n\nplt.tight_layout()\nfname=dsnm+'_'+y+'_comparison.png'\nplt.savefig(fname=fname,format='png')","3a5f939f":"# Load the Model back from file\nprint(\"loading model...\")\nmodel = tf.keras.models.load_model('Alex_adam.h5')\nprint('complete')\n\n# Check its architecture\nmodel.summary()\n\n# Evaluate the restored model\nloss, acc = model.evaluate(tst_img_data,  tst_lbl_data, verbose=2)\nprint('Restored model, accuracy: {:5.2f}%'.format(100*acc))\n\nprint(model.predict(tst_img_data).shape)","a60e1033":"imgInd=random.randint(1,len(imgnms))\nprint('Random Image number',imgInd)\n## E=getmyemotion(imgInd)  # the parameters here are the start and end index of images to predict. \nnImg=imgarry[imgInd]\nprint('Emotion: ',labels[imgInd], '\\nEmotion Class:', y_class[imgInd])\n#showImg(imgarry[imgInd])\nprint ('now lets make this into a new image by randomly augmenting it')\n\n#pick one of these:\npredarray=[]\n\npredarray.append(datagen.random_transform(nImg, seed=None))\n#showImg(predarray[0])\n\n\nf = plt.figure()\nf.add_subplot(1,2, 1)\nplt.imshow(imgarry[imgInd])\nf.add_subplot(1,2, 2)\nplt.imshow(predarray[0])\nplt.show(block=True)\n\nplot.save('before_after_prediction.jpg')\n\n\nprint('Lets use our model to predict its emotional class')","525fa763":"# decode decodes the labels into a number 0-6 instead of the binary array\ndef decode(datum):\n    return np.argmax(datum)\n\ndef predmood(i,prints=False):\n    #Lets reshape our data\n    pred_data = np.array(predarray[i].reshape(-1,760,246,3))\n    predicted_class = model.predict_classes(pred_data)\n    #print('Predicted Emotion Number',predicted_label[0])\n    predstr=clsdf.loc[predicted_class[0], 'ECstr']\n    #print('Predicted Class:',predstr)\n\n    if dsnm=='BSM100 DATASET':\n        y_true = y_class[imgInd]  # Using the image indicy get the truth label\n    else:\n        y_true = labels[imgInd]\n\n    predictionvalidity.append([y_true,predstr])\n    \n    if prints:\n        print('Truth: ',y_true)\n        print ('Prediction' , predstr)\n    \n        return y_true, predstr","adef9045":"clsdf['Acr'] = clsdf.index  #SAVE INDEX AS COLUMN\nclsdf.set_index('ECint' ,inplace=True, drop=False)  #RESET INDEX TO BE #\n#print(clsdf)\n\npredictionvalidity=[]\npredarray=[]\nfor i in range(10):\n    imgInd=random.randint(0,len(imgnms)-1)\n    #print('\\nRandom Image number',imgInd)\n    nImg=imgarry[imgInd]\n    #print('Emotion: ',labels[imgInd], '\\nEmotion Class:', y_class[imgInd])\n    #print ('now lets make this into a new image by randomly augmenting it')\n    predarray.append(datagen.random_transform(nImg, seed=None))  #randomly augment the image\n    #showImg(predarray[0])\n    #print('Lets use our model to predict its emotional class')\n    predmood(i)\n    \npredictionvalidity  #Truth, Prediction","43a80be5":"from sklearn.metrics import classification_report, confusion_matrix\n# code to split it into 2 lists \ny_truth, y_pred = map(list, zip(*predictionvalidity)) \n\n#Confution Matrix and Classification Report\n\n#Y_pred = model.predict_generator(validation_generator, num_of_test_samples \/\/ batch_size+1)\n#y_pred = np.argmax(Y_pred, axis=1)\nprint('Confusion Matrix')\nprint(confusion_matrix(y_truth, y_pred))\nprint('Classification Report')\ntarget_names = ['Cats', 'Dogs', 'Horse']\nprint(classification_report(validation_generator.classes, y_pred, target_names=target_names))\n","9a12a774":"# set plot figure size\nfig, c_ax = plt.subplots(1,1, figsize = (12, 8))\n\ndef multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n    lb = LabelBinarizer()\n    lb.fit(y_test)\n    y_test = lb.transform(y_test)\n    y_pred = lb.transform(y_pred)\n\n    for (idx, c_label) in enumerate(all_labels): # all_labels: no of the labels, for ex. ['cat', 'dog', 'rat']\n        fpr, tpr, thresholds = roc_curve(y_test[:,idx].astype(int), y_pred[:,idx])\n        c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\n    c_ax.plot(fpr, fpr, 'b-', label = 'Random Guessing')\n    return roc_auc_score(y_test, y_pred, average=average)\n\n# calling\nvalid_generator.reset() # resetting generator\ny_pred = model.predict_generator(valid_generator, verbose = True)\ny_pred = np.argmax(y_pred, axis=1)\nmulticlass_roc_auc_score(valid_generator.classes, y_pred)","6992962a":"#pick a random one to send to generate sentences from\nx=random.randint(0,len(predictionvalidity)-1)\n\n\nif trainby=='class':\n    emotdict = {labels[i]: y_class[i] for i in range(len(labels))} \n    print('The Emotion dictionary looks like this:\\n',list(emotdict.items())[:10])\n    #E= 'Positive Emotion'\n    E=predictionvalidity[x][1]\n    print(\"Predicted emotional class is: \",E)\n    clsdf.set_index('ECstr' ,inplace=True, drop=False)\n    EAcr=clsdf.loc[E,'Acr']\n    print(EAcr)\n    truth=predictionvalidity[x][0]\n    print('Truth: ',truth)\n    \n    # Randommly select an emotion of the predicted class\n    listOfemots = [key  for (key, value) in emotdict.items() if value == EAcr]\n    print(\"Emotion List:\", listOfemots)\n    classemot= random.choice(listOfemots)\n    #classemot\n    \n    print(\"\\nrandom item from list is: \", classemot)\n    \nelif trainby=='label':\n    classemot=predictionvalidity[x][1]\n    truth=predictionvalidity[x][0]\n    print('Truth: ',classemot,'\\nPrediction: ',truth)\n    \nshowImg(predarray[x])","380f58a3":"#send my predicted output to the Affect Similarity Model via csv file\n#Edf = pd.DataFrame([[E], [classemot]],\n#                   columns=['EC', 'Emotion'])\n#Edf.to_csv('classifiedemotion.csv',index=False)\n\nimport _pickle as cPickle\ndat = {\"myemot\": classemot}\nprint(\"myemot: \", classemot)\n\n#with open(r\"Eouts\/myemtdata.pickle\", \"wb\") as output_file:\nwith open(r\"myemtdata.pickle\", \"wb\") as output_file:\n     cPickle.dump(dat, output_file)\n# pickle_file will be closed at this point, preventing your from accessing it any further\n\nimport csv\nwith open('myemot.csv', 'w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerow([classemot])","b5b6924e":"### Now lets try tensorflow for classification\nThe steps to do this are:\n- Encode our classes into a binary array\n- Reshape the data to feed into a neural network\n- Run through single layer CNN\n- Run through complex CNN\n- ","b5d46866":"## lets do a prediction of several new images & create a confusion matrix","1c71893b":"As expected the more complex model provides a better accuracy than the simple 1 CNN.  We'll use the Alex based model for our predictions. ","cc3af10d":"## Conclusion\nThis concludes the image classification model.  We've read in training data, generated more data using image modifications, trained a classifaction model using the Alexnet framework, generated new images by modifying the original and have tested predicting these as a validation set.  Now we will send one classifation to the NLP model to generate a sentence communicating the emomtion classifaction.  \n\n### Additional work to be done on this model\n1. Test the predictions\n2. Improve the output images\n3. Improve the model results\n4. Inrease data volume \n","c7a5247d":"### Data Augmentation \nWe've read in data, split, labled and augment the data multiples times with minor image modifications to increase our data set volume.  These modifications will look like unique images for the model since the pixels on each image are different. ","d983e9a4":"## USER SELECTIONS","9fb2f62a":"## PREDICTION \nNow I have a well trained model that's relatively acceptable given my input data size and fidelity.  \n\nTo use the model, I'll create a new unseen image by applying an image modification from the original data set, use my model to predict the emotional class.  \n\nSteps:\n    1. Load the model\n    2. Pick a random # between 1-100  #len(imgnms)\n    3. Apply image augmentation to generate a new image\n    4. predict the class from the model\n \n\nConfirm this is working properly\n1. put in a known image\n2. take data and modify it again for new \"unseen\" data","7e180069":"# Physiological Sensing Emotional Classifier\n\n## Model Parameters","3ea37f08":"### Model pre-processinge\nNow we've read in our main temperature graph, split each \"person\" into its own .jpg file.  Now I'm going to turn each image into an array format and append it to an array. \n  \nNow I have 100 images of (246, 760) pixels (total of  pixels)\nEach image has a unique label and belongs to one of 6 classes.  \nThe classes are Negative emotion, positive emotion, illness, homeostatis, cognition and unspecific.  \n\nFirst we'll train to identify the classes.  \n\nSPlit the image array into a test and validation set","2abecf97":"### Class to Emotion for BSM100\nLets get back to an actual emotion or feeling now.... \ngiven the predicted class, we'll randomly select one of the emotional words\nNormally, we'd want to actually predict the emotional words.  Since the focus of this word is to show an end to end model and there is a significant amount of work going on using much more intelligent detection methods, I'll  say the class prediction is close enough, and we'll make a random guess on which exact emotion it is. ","2194e13e":"## Get a new image and create a never seen image for prediction","df05efdb":"#### Add image modification work here operating on each image in the 'Train\/ folder\nFor each image in the \/train folder perform a random modification  \nOptions are:\n- re-coloring, flip, twist, \n- Blurring\/smoothing\n- Sharpening\n- Denoising\n- Mathematical morphology","df4ce1a4":"### CNN1 with 1 Convolutional Layer\nref: https:\/\/github.com\/khanhnamle1994\/fashion-mnist\/blob\/master\/CNN-1Conv.ipynb\n* \"The 1st layer is a Conv2D layer for the convolution operation that extracts features from the input images by sliding a convolution filter over the input to produce a feature map. Here I choose feature map with size 8 x 6.\n* The 2nd layer is a MaxPooling2D layer for the max-pooling operation that reduces the dimensionality of each feature, which helps shorten training time and reduce number of parameters. Here I choose the pooling window with size 2 x 2.\n* To combat overfititng, I add a Dropout layer as the 3rd layer, a powerful regularization technique. Dropout is the method used to reduce overfitting. It forces the model to learn multiple independent representations of the same data by randomly disabling neurons in the learning phase. In this model, dropout will randomnly disable 5% of the neurons (normally would be ~20% but since our dataset is small, lets use 5%)\n* The next step is to feed the last output tensor into a stack of Dense layers, otherwise known as fully-connected layers. These densely connected classifiers process vectors, which are 1D, whereas the current output is a 3D tensor. Thus, I need to flatten the 3D outputs to 1D, and then add 2 Dense layers on top.\n* I do a 6-way classification (as there are 6 classes of fashion images), using a final layer with 6 outputs and a softmax activation. Softmax activation enables me to calculate the output based on the probabilities. Each class is assigned a probability and the class with the maximum probability is the model\u2019s output for the input.\"","b47ed186":"### model Parameters\nNext, I'll use an Alexnet framework.  This framework is a deep net made up of multiple conv layers, max pooling, dropout and dense layers.  \n\nThis is an interesting resource for creating model functions, may want to implement at later date\nhttps:\/\/github.com\/microsoft\/CNTK\/blob\/master\/Examples\/Image\/Classification\/AlexNet\/Python\/AlexNet_ImageNet_Distributed.py\n\nThis alexnet is modeled after this:\nhttps:\/\/engmrk.com\/alexnet-implementation-using-keras\/\n\nThe typical Alexnet classifier architecture consists of five convolutional layers, some of which are followed by maximum pooling layers and then three fully-connected layers and finally a softmax classifier. (ref: https:\/\/engmrk.com\/alexnet-implementation-using-keras\/)\n\nOur loss function is cross entropy as defined by:   \n\\begin{equation*}\nC = -1\/n \\sum_{n=1}^{n} y_i * ln (y_i) +(1-y_i)ln(1-y_i)\\\n\\end{equation*}\n\nLets look at a few model parameters:\n$$\\text{number of batches = size of training \/ batch size}$$ \n$$\\text{number of batches = 400\/50}$$\n$$\\text{number of batchs = 8}$$\n\n\n Which says there are 8 batches of gradient descent per epoch.  \n \n Each round of training has:\n 1. sample of mini-batch of x values\n 2. forward prop x through network to estimate y and $\\hat{h}$\n 3. Calculate cost by compying y & $\\hat{h}$\n 4. Descent gradient of cost to adjust weight and bias, enabling x to better predict y.\n (Ref: J. Krohn)\n \n https:\/\/machinelearningmastery.com\/how-to-choose-loss-functions-when-training-deep-learning-neural-networks\/\n \n #### Optimizer: ADAM\nhttps:\/\/towardsdatascience.com\/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f\n\nFor sparse data sets one should use one of the adaptive learning-rate methods. An additional benefit is that we won\u2019t need to adjust the learning rate but likely achieve the best results with the default value.\n\nAdam works well in practice and compares favorably to other adaptive learning-method algorithms as it converges very fast and the learning speed of the Model is quiet Fast and efficient and also it rectifies every problem that is faced in other optimization techniques such as vanishing Learning rate , slow convergence or High variance in the parameter updates which leads to fluctuating Loss function\n\nAdam stands for Adaptive Moment Estimation. Adaptive Moment Estimation (Adam) is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients like AdaDelta ,Adam also keeps an exponentially decaying average of past gradients M(t), similar to momentum:\n\n\n#tf.keras.optimizers.Adam(\n    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n    name='Adam', **kwargs\n)\n ","37cf2fd0":"## Get Data 14 Bodily Sensation Maps  (BSM14)\nsource: \nLauri Nummenmaa, Enrico Glerean, et al. \u201cBodily Maps of Emotions\u201d. In:Pro-ceedings of the National Academy of Sciences of the United States of America111.2 (2014).doi:10.1073\/pnas.1321664111.\n\n","d756e576":"### Create a confusion Matrix of top model","7b05558a":"### One Hot Encoding ","d11ecebe":"This notebook reads in the .mat file from https:\/\/www.pnas.org\/content\/115\/37\/9198#ref-11\nfigure 2.0.   \n\nReferences: \nbase code:  \nhttps:\/\/blog.francium.tech\/build-your-own-image-classifier-with-tensorflow-and-keras-dc147a15e38e\ntuning and details  \nhttps:\/\/blog.keras.io\/building-powerful-image-classification-models-using-very-little-data.html\ntuning\nhttps:\/\/medium.com\/@jsflo.dev\/training-a-tensorflow-model-to-recognize-emotions-a20c3bcd6468\n\n\nimage modification:\nhttps:\/\/medium.com\/neuronio\/how-to-deal-with-image-resizing-in-deep-learning-e5177fad7d89\n\nAlexnet sizes:\nhttps:\/\/www.learnopencv.com\/number-of-parameters-and-tensor-sizes-in-convolutional-neural-network\/\n  \nRequired TODO: \n- further refine NN for improved output\n- Expand the prediction part\n- refine markup language descriptions throughtout to better explain\n- add in see https:\/\/123machinelearn.wordpress.com\/2017\/12\/25\/image-enhancement-using-high-frequency-emphasis-filtering-and-histogram-equalization\/\n    to do emphasis better and see if results are better\nOptional TODO: \n- increase data modifications\n- attempt to predict emotion - unsure if this is worth it... \n- change coloring on predictive map to reds instead of blues\n- Add intensity level based on co-centricity of emotion and strength <-research\n- tune the learning parameters\n- show the loss function curve of the learning rate - C vs P of a parameter\n- show graph of learning speed vs epoch\n","e101905e":"note: all y-class labels are same from \"Maps of subjective feelings\" except contempt and neutral which don't exist in the bsm100 data","3e23e74a":"### CNN2 Moderate Size NN\nWell that's okay, lets see what we can get with a more complex model.  I'll use an architecture similar to the alexnet architecture with multiple convolutional layers and maxpooling layers.","65bee9d4":"## Get Data 100 Bodily Sensation Maps  (BSM100)\nsource: auri Nummenmaa, Riitta Hari, et al. \u201cMaps of subjective feelings\u201d. In:Proceed-ings of the National Academy of Sciences of the United States of America115.37(Sept. 2018), pp. 9198\u20139203.issn: 10916490.doi:10.1073\/pnas.1807390115.\n","bd648a58":"Alternative method:\nfrom keras.preprocessing.image import ImageDataGenerator\ngen = ImageDataGenerator(rotation_range=8, width_shift_range=0.08, shear_range=0.3,\n                               height_shift_range=0.08, zoom_range=0.08)\nbatches = gen.flow(X_train, y_train, batch_size=256)\nval_batches = gen.flow(X_val, y_val, batch_size=256)","c8404a2a":"<h1 id=\"tocheading\">Table of Contents<\/h1>\n<div id=\"toc\"><\/div>"}}