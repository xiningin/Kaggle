{"cell_type":{"c98361fe":"code","bf058caf":"code","6bf7a9ad":"code","141c9f3f":"code","c51789ca":"code","91e052aa":"code","61dff743":"code","79e05044":"code","9b2e29dc":"code","91489c16":"code","cab35fb7":"code","1e63df53":"code","91e9b0e8":"code","60cc2b8a":"code","f6a84421":"code","2aa1788c":"code","587ddfe4":"code","fcea63f2":"markdown","8c0b3d73":"markdown","beb36b73":"markdown","6f1b8741":"markdown","f345ed04":"markdown","6f017501":"markdown","59823f63":"markdown","47cfbea0":"markdown","cf903fa1":"markdown","1c57927d":"markdown","ce1f1b74":"markdown"},"source":{"c98361fe":"!pip install openpyxl","bf058caf":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas_profiling\nimport warnings\nimport shap\n\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 40)\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6bf7a9ad":"data=pd.read_excel('..\/input\/delivery-truck-trips-data\/Delivery truck trip data.xlsx')","141c9f3f":"data.info()","c51789ca":"!pip install facets-overview","91e052aa":"### Create the feature stats for the datasets and stringify it.\nimport base64\nfrom facets_overview.generic_feature_statistics_generator import GenericFeatureStatisticsGenerator\n\ngfsg = GenericFeatureStatisticsGenerator()\nproto = gfsg.ProtoFromDataFrames([{'name': 'train', 'table': data}])\nprotostr = base64.b64encode(proto.SerializeToString()).decode(\"utf-8\")\n\n### Display the facets overview visualization for this data\nfrom IPython.core.display import display, HTML\n\nHTML_TEMPLATE = \"\"\"\n        <script src=\"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/webcomponentsjs\/1.3.3\/webcomponents-lite.js\"><\/script>\n        <link rel=\"import\" href=\"https:\/\/raw.githubusercontent.com\/PAIR-code\/facets\/1.0.0\/facets-dist\/facets-jupyter.html\" >\n        <facets-overview id=\"elem\"><\/facets-overview>\n        <script>\n          document.querySelector(\"#elem\").protoInput = \"{protostr}\";\n        <\/script>\"\"\"\nhtml = HTML_TEMPLATE.format(protostr=protostr)\ndisplay(HTML(html))","61dff743":"# 8. Change ontime colunm to 1 for ontime and 0 for delay\ndata.ontime.replace({'G':1, np.NaN:0}, inplace=True)\n\n# 3. Replace NaN with unknown for vechicle type\ndata.vehicleType.replace(np.NaN, 'unknown', inplace=True)","79e05044":"# 10. Filling NaN in Transportation distance with difference in lon\/lot of origin and destination\nfrom geopy import distance\n\ngeodistance_km = []\nfor row in data.itertuples(index=False):\n   geodistance_km.append(distance.distance(row.Org_lat_lon, row.Des_lat_lon).km)\n\ndata['geodistaince_km']=geodistance_km\n\n# Replace NaN row in 'TRANSPORTATION_DISTANCE_IN_KM' with geodisatnce values\ndata.TRANSPORTATION_DISTANCE_IN_KM.fillna(data.geodistaince_km, inplace=True)","9b2e29dc":"#11. Create the expected travel time in hours\ndata['expected_travelhours']=(data.Planned_ETA-data.trip_start_date).astype('timedelta64[h]')\n\n# There are negative travel hours. I replace them with 0 hour.\ndata.expected_travelhours[data.expected_travelhours<0]=0\ndata.expected_travelhours.sort_values()\n\n# 6784 row can be dropped, because it looks like an outlier.\ndata.drop(index=6784,axis=0,inplace=True)","91489c16":"# Based on the consideration above, I use the following the first 9 columes as input features. The last one is the target.\ndata_use = data[['Market\/Regular ','OriginLocation_Code','DestinationLocation_Code',\n                 'TRANSPORTATION_DISTANCE_IN_KM','expected_travelhours','vehicleType',\n                 'customerID','supplierID','Material Shipped','ontime']]\n\n# There are still some NaNs. \ndata_use.info()","cab35fb7":"# They are not many som just trop them.\ndata_use.dropna(axis=0,inplace=True)\n\n# I also found that three columes have mixed type. Integers blended into string columns. \ndata_use.applymap(type).nunique()","1e63df53":"# Fix these columns\ndata_use['OriginLocation_Code'] = data_use['OriginLocation_Code'].apply(str)\ndata_use['DestinationLocation_Code'] = data_use['DestinationLocation_Code'].apply(str)\ndata_use['supplierID'] = data_use['supplierID'].apply(str)\n\n# Now data is cleaned, and feature extraction is done!","91e9b0e8":"# Take a copy for preproessing\ndf = data_use.copy()\n\n# Make X and y\nX = df.drop(columns='ontime', axis=1)\ny = df['ontime']\n\n# Since applying OrdinalEncoder after split cause unknown-category error in testset. The version of sklearn in Kaggle (0.23) cannot handle this. \n# There are two options. 1) Not optimal but I apply encoder now, or 2) Apply OneHotEncoder after the split. I go for 1).   \nfrom sklearn.preprocessing import OrdinalEncoder\nencoder = OrdinalEncoder()\ncat_columns = ['Market\/Regular ','OriginLocation_Code','DestinationLocation_Code','vehicleType','customerID','supplierID','Material Shipped']\nX[cat_columns] = encoder.fit_transform(X[cat_columns])\n\n# split X y to train and test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","60cc2b8a":"from xgboost import XGBClassifier\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\ny_proba = model.predict_proba(X_test)\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\ncm =confusion_matrix(y_test, y_pred)\nprint(cm)\nprint('accuracy:', accuracy_score(y_test, y_pred))\nprint('precision:', precision_score(y_test, y_pred))\nprint('recall:', recall_score(y_test, y_pred))","f6a84421":"ax = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n\nax.set_title('Confusion Matrix with labels\\n\\n');\nax.set_xlabel('\\nPredicted Values')\nax.set_ylabel('Actual Values ');\n\n## Ticket labels \nax.xaxis.set_ticklabels(['Delay','Ontime'])\nax.yaxis.set_ticklabels(['Delay','Ontime'])\n\n## Display the visualization of the Confusion Matrix.\nax","2aa1788c":"# Let's try with some other parameters than default parameters if the result becomes better.\n\nmodel_2 = XGBClassifier(n_estimators=1500, learning_rate=0.05, n_jobs=4)\nmodel_2.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_test, y_test)], \n             verbose=False)\n\ny_pred_2 = model.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\ncm_2= confusion_matrix(y_test, y_pred_2)\ncm_2\n\n# Ok the same result. Maybe parameter tuning does not go further.","587ddfe4":"explainer = shap.explainers.Tree(model)\nshap_values = explainer(X_test)\nshap.summary_plot(shap_values, X_test, show=False)\n\nfig = plt.gcf()\nfig.set_figheight(12)\nfig.set_figwidth(10)\nplt.show()","fcea63f2":"![image.png](attachment:8a69c2dc-5e52-40c2-b5a3-de545627bd46.png)","8c0b3d73":"## 1. Data exploration\n#### I also use MITO to explore the dataset which cannot be run in this notebook. This was very helpful in exploring data and getting initial insights into the data!!","beb36b73":"I attach some of the further analysis with dependancy plots, using ExplainerDash.\n\n![image.png](attachment:930aab60-2c32-474d-bce6-1529bfecf28c.png)\n\n\n![image.png](attachment:d05e7789-8156-4836-aaa0-de339a38d849.png)\n\n\n![image.png](attachment:e80a13f1-1e57-49ab-aaec-e051dc613e16.png)\n","6f1b8741":"Here is a screenshot when I explore the data with MITO.","f345ed04":"### Consideration after intial data exploration\n1. Features related to current location and current lon and lat are difficult to understand but it seems they are inforamtion when the GPS ping is received. So there can be two alternative to build a model:\n    a) one model is that when I input current time and location and other relevant input features, the model can predict ontime or not.\n    b) another model is that it predict ontime or not before the trip. In this alternative, the \"current\" info is irrelevant.\n    => Let's go for b)!\n2. \"Minimum_kms_to_be_covered_in_a_day\", \"Driver Mobil No\", \"Drive name\" have more than 45% NaN -> Drop them.\n3. \"veichleType\" is more informative than \"vechile_no\"(registration number?) which has high cardinality (2325 unique).But the former has high NaN(>800). Still I use the former feature and fill the NaNs with 'unknown'.\n3. \"BookingID\" is an uniform distribution 6880 count vs 6875 unique. -> Drop this.\n4. \"BookingID_date\" means little for the truck tranportation ontime question. -> Drop them.\n5. \"DestinationLocation\" is duplication of \"Destination_Location\" -> Drop it.\n6. I use code and ID instead of actual name. I use origincode, destinationcode, customerID, and supplierID\n7. Ontime and delay columns are redundant. Change Onetime coloume to 1 or 0.\n8. Since I go for b), 'trip_end_date' and 'atual_eta' cannot be input features. -> Drop them.\n9. I wonder certain month and hours affect the target feature. One way is to separate them. -> I do not do this so far. \n10. 'Transpotation distance in km' can be an important featue but 712 NaN. For those, I can replace them with the direct distance between origin and destination lon\/lat.\n11. What can I do with two remaining columns of 'trip_start_date' and 'Planned_ETA'? Maybe I can create a new colum with expected travel time (hours) from these two.","6f017501":"## 3. Preprocessing and training the model","59823f63":"### Anslysis and insights\n* expected_travelhours: Generally longer expected travel hours positively affect the ontime prediction. Shoter time is the opposite.\n* supplier: Not exactly why it has a high influence to the prediction. OrdinalEncoder econdes in an alphabetical order (acc. to the doc), thus the distriubtion in x does not mean anything. From the dependancy plot, I can see that supplies with longer travel distace generally negatively affect the ontime prediction.\n* transportation distace: Shorter distance positively affect on the ontime prediction. The opposite goes for the longer distance.\n* Origin and destination: Difficult to interpret this categirical features.\n* Other features only limitedly influence on the prediction.\n\n### Can this model be applied in a real life?\n* The metrics of the model may be improved with some extra effort but I do not do this now. \n* Accuracy and precision are rather ok (around 90%). Recall is a bit lower. The model may be used for a non-critical application, for instance, a model integrated app informs logistic planners to ontime\/delpay prediction with probablity. With ontime prediction with higher probablity, the planner does not need to do any specific. When the prediction is delay or low probability, the planner can ask or change the planning system so that truck companies\/drives can depart some time earlier (how early depends on the traveling distance). \n","47cfbea0":"## 2. Data clearning","cf903fa1":"# Truck deliverly service analysis\n### Goals\n* Builld a model that predict ontime\/delay\n* Identify feasures that influece the predication\n\n\nI use the dataset provided by RAM THAIAGU. Thanks!","1c57927d":"### Analysis with SHAP \nI also used ExplianerDash. This is also very helping to getting insights about the mode quckly without much coding.","ce1f1b74":"## 4. Model evaluation and explanation"}}