{"cell_type":{"3784c018":"code","295d7f45":"code","bbd8b6b8":"code","ef844bfb":"code","0856062f":"code","ec01b416":"code","15902ad8":"code","1eae3963":"code","10df4642":"code","5013488d":"code","71871775":"code","e333a145":"code","be733737":"code","e37ebd9e":"code","d8072e70":"code","d28cd338":"code","6e5342a2":"code","852b14aa":"code","11733c47":"code","2049e93a":"code","c3460fae":"code","b93ce098":"code","49ca01b4":"code","9ff3ea2a":"code","a3af9436":"code","da115456":"code","ba5ea03d":"code","45dab8b7":"code","c1300683":"code","82e99d61":"code","ed3502c1":"code","aa4d425e":"code","bab1bcaf":"code","a2d2846e":"code","577ae93d":"code","78c7f2b1":"code","eb4b9567":"code","2539f390":"code","4cf88720":"code","9e3530c4":"code","9fb7e524":"code","72a826ea":"code","480cef02":"code","6642b1c1":"markdown","ebe1caed":"markdown","317bfc02":"markdown","798ceac4":"markdown","c41ecb1a":"markdown","7282b50a":"markdown","87293454":"markdown","0e5bce1f":"markdown","cad3772b":"markdown","ead30e32":"markdown","b457b12b":"markdown"},"source":{"3784c018":"# import the required libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pickle","295d7f45":"# read the csv files\n\ndf = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndf","bbd8b6b8":"df.shape","ef844bfb":"df.info()","0856062f":"df.isna().sum()","ec01b416":"import seaborn as sns\n#checking the distrubition of missing columns\nsns.pairplot(df)","15902ad8":"# handling missing data \n# we will drop the cabin colum as it is mostly not present\ndf_tmp = df.drop(['Cabin','Ticket'],axis = 1)","1eae3963":"# split data into training and test \nfrom sklearn.model_selection import train_test_split\nx = df_tmp.drop(['Survived'] , axis = 1)\ny = df_tmp['Survived']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)","10df4642":"df_tmp['Age']","5013488d":"# for age lets see its distribution first\n\nsns.distplot(df['Age'].dropna())","71871775":"# we can see its more a right skewed graph so lets take mode median of the age to fill the missing values.\nx_train['Age'].fillna(value = x_train['Age'].median(), inplace = True)\nx_test['Age'].fillna(value = x_test['Age'].median(), inplace = True)\nx_train.isna().sum()\n","e333a145":"# now lets convert the object type variable to categorical type\n# object_cols1 = list(x_train.select_dtypes(include='object').columns)\n# x_train[object_cols1] = x_train[object_cols1].astype('category')\n\n# object_cols2 = list(x_test.select_dtypes(include='object').columns)\n# x_test[object_cols2] = x_test[object_cols2].astype('category')\n\n\nx_train['Embarked'].fillna(x_train['Embarked'].mode()[0], inplace=True)\nx_test['Embarked'].fillna(x_test['Embarked'].mode()[0], inplace=True)","be733737":"# as we can see it throws a error for the name column lets then extract the Mr Mrs etc from the name because the full name\n# is not required for us to predict and it is of no use. may be there is a probability that if the passenger is male or female\n# chances of survival would increase. so lets refrom the name column.\n\n\ndf['Name'].head(20)","e37ebd9e":"name= []\nfor i in x_train.Name:\n    name.append(i)\nname","d8072e70":"salutation = []\n\nfor i in x_train.Name:\n    \n    a = i.split(',')\n    b = a[1].split('.')\n    c = b[0]\n    salutation.append(c)\n    \nsalutation\n    \n\n\nx_train['salutation'] = salutation\nx_train.drop(['Name'], axis = True, inplace = True)\n\n","d28cd338":"salutation = []\n\nfor i in x_test.Name:\n    \n    a = i.split(',')\n    b = a[1].split('.')\n    c = b[0]\n    salutation.append(c)\n    \nsalutation\n    \n\n\nx_test['salutation'] = salutation\nx_test.drop(['Name'], axis = True, inplace = True)","6e5342a2":"x_test.info()","852b14aa":"# outlier detection\n\nx_train.columns\n# 'PassengerId', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare','Embarked', 'salutation'\nplt.figure(figsize=(12,10))\nsns.boxplot(data = x_train )","11733c47":"# dbscan clustering\nfrom sklearn.cluster import DBSCAN\n\ndb = DBSCAN(min_samples = 2, eps = 3)\ncluster = db.fit_predict(x_train[['Age','Fare']])\n\nlist(cluster).count(-1)\n\n","2049e93a":"# isolation forest\nfrom sklearn.ensemble import IsolationForest\nclf = IsolationForest( behaviour = 'new', max_samples=100, random_state = 1, contamination= 'auto')\npreds = clf.fit_predict(x_train[['Age','Fare']])\npreds","c3460fae":"# now lets onehotencode the categorical column\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nobject_cols = [col for col in list(x_train.select_dtypes(include = 'object'))]\nonehot = OneHotEncoder(sparse= False, handle_unknown='ignore')\nct = ColumnTransformer([('onehot', onehot, object_cols)])\n\ntransformed_x_train = ct.fit_transform(x_train)\ntransformed_x_test = ct.transform(x_test)","b93ce098":"# lets train a SGD classifier as the training set contai less than 100k samples\n\nfrom sklearn.linear_model import SGDClassifier\n\nclf = SGDClassifier(random_state=0)\n\nclf.fit(transformed_x_train, y_train)\nclf.score(transformed_x_test,y_test)","49ca01b4":"# now lest check with randomforest regressor though it is not recommended to use randomforest for less than 100k samples\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(random_state = 1)\n\nmodel.fit(transformed_x_train, y_train)\nmodel.score(transformed_x_test,y_test)","9ff3ea2a":"# as we can see the accuracy score is worst in randomforest we will stick to the SGD estimator\n# now we will try to randomizedsearchcv and gridsearch for tuning hyper parameter\n\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n\n# now check the hyperparameters of the estimator\n\nclf.get_params()","a3af9436":"grid = {\n 'loss': ['hinge','log', 'modified_huber','squared_hinge', 'perceptron',  'squared_loss','huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n 'penalty': ['none', 'l2', 'l1','elasticnet']\n}\n\ngrid_model = GridSearchCV(estimator= SGDClassifier(),\n                     param_grid= grid,\n                         cv=10,\n                         verbose=2,\n                         n_jobs=-1)\n\ngrid_model.fit(transformed_x_train, y_train)\ngrid_model.score(transformed_x_test, y_test)","da115456":"# lets try with xgboost to check wether we can improve the score or not\n\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier()\nxgb.fit(transformed_x_train, y_train)\nxgb.score(transformed_x_test, y_test)","ba5ea03d":"# as the score improves lets try with tuning the hyperparmeters\n\nxgb.get_xgb_params()","45dab8b7":"# now lets check roc curve and confusion matrix \nfrom sklearn.metrics import roc_curve\n\npredicted_proba = xgb.predict_proba(transformed_x_test)\npositive_proba = predicted_proba[:,1]\nroc_curve(y_test,positive_proba)\n","c1300683":"from sklearn.metrics import plot_roc_curve,confusion_matrix,plot_confusion_matrix\n\nplot_roc_curve(xgb,transformed_x_test,y_test)","82e99d61":"y_predict = xgb.predict(transformed_x_test)\ncm = confusion_matrix(y_test, y_predict)\n\nplot_confusion_matrix(xgb,transformed_x_test,y_test)","ed3502c1":"# save model\nimport joblib\njoblib.dump(xgb, 'xgb.joblib')","aa4d425e":"load_model= joblib.load('xgb.joblib') # load saved model","bab1bcaf":"# now load the test dataset\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ntest","a2d2846e":"# preprocessing test data for prediction\ntest.isna().sum()\n\n# as we can see most of the cabin data are missing we are gonna remove the cabin column\n# and also Ticket column is irrelevant\ntest.drop(['Ticket','Cabin'], axis = 1, inplace = True)","577ae93d":"# filling missing value of age column\ntest.Age.fillna(value= test.Age.median(), inplace = True)\ntest.info()","78c7f2b1":"# filling missing value of Fare column\n\nsns.distplot(test.Fare.dropna())","eb4b9567":"# as it is a left skew data we can use median for  the missing value\n\ntest.Fare.fillna(value=test.Fare.mode()[0], inplace = True)","2539f390":"test.isna().sum()","4cf88720":"# now remove the Name column and add salutation column as we did with training and validation data\nsalutation = []\n\nfor i in test.Name:\n    \n    a = i.split(',')\n    b = a[1].split('.')\n    c = b[0]\n    salutation.append(c)\n    \nsalutation\n    \n\n\ntest['salutation'] = salutation\ntest.drop(['Name'], axis = True, inplace = True)","9e3530c4":"# now hotencode the categorical variable\n\ntransformed_test = ct.transform(test)","9fb7e524":"predicted_values = load_model.predict(transformed_test)\nlen(predicted_values) # matched","72a826ea":"# create the dataframe\nsubmit_df = pd.DataFrame()\nsubmit_df['PassengerID'] = test.PassengerId\nsubmit_df['Survived'] = predicted_values\nsubmit_df","480cef02":"#save the dataframe into csv\nsubmit_df.to_csv(\"Titanic_Survived.csv\" , index = False)","6642b1c1":"# onehotencode to change categorical values","ebe1caed":"# XGBoost traning","317bfc02":"# RandomForestClassifier training","798ceac4":"# hyperparameter tuning","c41ecb1a":"saving and loading model using joblib","7282b50a":"# missing data handling and preprocessing training data","87293454":"# outlier checking","0e5bce1f":"# Importing Libraries","cad3772b":"# SGD classfier training","ead30e32":"# preprocessing and missing data handling for test dataset","b457b12b":"# XGBoost Model Evaluation"}}