{"cell_type":{"ecfe26c7":"code","fa5c4a65":"code","aeaa61a3":"code","1a548506":"code","64fb48bc":"code","9843c4b9":"code","22f59756":"code","735d447f":"code","4e0f3b19":"code","082adf30":"markdown","19d1d025":"markdown","1440bd66":"markdown","5f1269c3":"markdown","8ed98b1d":"markdown"},"source":{"ecfe26c7":"# Importing Libraries\r\nimport pandas as pd # Intermediate DS\r\nimport numpy as np # Scientific Operations\r\n\r\nimport optuna # For Hyper Parameter Tuning\r\nfrom functools import partial\r\nimport multiprocessing\r\n\r\nfrom sklearn import preprocessing # Preprocesing library for Encoding, etc.\r\nfrom sklearn.model_selection import train_test_split # For splitting train and test data\r\nfrom sklearn.metrics import mean_squared_error # For Calculating Mean Squared Error\r\nimport lightgbm as lgb # Light GBM Regressor\r\n\r\nimport warnings\r\nwarnings.filterwarnings('ignore')","fa5c4a65":"# Load the training data\r\ntrain = pd.read_csv(\"input\/train.csv\", index_col=0)\r\ntest = pd.read_csv(\"input\/test.csv\", index_col=0)\r\n\r\n# Preview the data\r\ntrain.head()\r\ntest.head()","aeaa61a3":"y_train = train.target\r\nX_train = train.drop(['target'], axis=1)\r\nX_test = test.copy()\r\n\r\n# Preview features\r\nX_train.head()","1a548506":"# Extract the Categorical Columns\r\ncat_cols = [feature for feature in train.columns if 'cat' in feature]\r\nprint(cat_cols)\r\n\r\n# Copy of original data to prevent overwwritting them\r\nlabel_X_train = X_train.copy()\r\nlabel_X_test = X_test.copy()\r\n\r\n# Apply ordinal encoder to each column with categorical data\r\nordinal_encoder = preprocessing.OrdinalEncoder()\r\nlabel_X_train[cat_cols] = ordinal_encoder.fit_transform(label_X_train[cat_cols])\r\nlabel_X_test[cat_cols] = ordinal_encoder.transform(label_X_test[cat_cols])","64fb48bc":"label_X_train","9843c4b9":"label_X_test","22f59756":"def objective(trial, X, y):\r\n    train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.2)\r\n    dtrain = lgb.Dataset(train_x, label=train_y, free_raw_data=False)\r\n    dvalid = lgb.Dataset(valid_x, label=valid_y, free_raw_data=False)\r\n\r\n    param = {\r\n        \"metric\": \"rmse\",\r\n        \"verbosity\": -1,\r\n        'max_depth':trial.suggest_int('max_depth', 5, 50),\r\n        'n_estimators':trial.suggest_int(\"n_estimators\", 1000, 50000, step=100),\r\n        'subsample': trial.suggest_uniform('subsample', 0.2, 1.0),\r\n        'colsample_bytree':trial.suggest_uniform('colsample_bytree', 0.2, 1.0),\r\n        'learning_rate':trial.suggest_uniform('learning_rate', 0.001, 0.01),\r\n        'reg_lambda':trial.suggest_uniform('reg_lambda', 0.01, 50),\r\n        'reg_alpha':trial.suggest_uniform('reg_alpha', 0.01, 50),\r\n        'min_child_samples':trial.suggest_int('min_child_samples', 5, 100),\r\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 10, 200),\r\n        'max_bin':trial.suggest_int('max_bin', 30, 1000),\r\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\r\n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\r\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\r\n        'learning_rate':trial.suggest_uniform('learning_rate', 0.001, 0.01),\r\n        'cat_smooth':trial.suggest_int('cat_smooth', 5, 100),\r\n        'cat_l2':trial.suggest_loguniform('cat_l2', 1e-3, 100),\r\n        'num_threads': multiprocessing.cpu_count()-2,\r\n        #'device': 'gpu',\r\n        #'gpu_platform_id': 0,\r\n        #'gpu_device_id': 0\r\n    }\r\n\r\n    lightgbm_model = lgb.train(param, dtrain, valid_sets=(dtrain, dvalid), early_stopping_rounds=250)\r\n    \r\n    train_score = np.round(mean_squared_error(train_y, lightgbm_model.predict(train_x), squared=False), 5)\r\n    test_score = np.round(mean_squared_error(valid_y, lightgbm_model.predict(valid_x), squared=False), 5)\r\n    \r\n    print(f'TRAIN RMSE : {train_score} || TEST RMSE : {test_score}')\r\n    \r\n    return test_score","735d447f":"print(multiprocessing.cpu_count())","4e0f3b19":"optimize = partial(objective, X=label_X_train, y=y_train)\r\nstudy_lgbm = optuna.create_study(direction='minimize')\r\nstudy_lgbm.optimize(optimize, n_trials=100)\r\n\r\n\r\nprint(\"Number of finished trials: {}\".format(len(study_lgbm.trials)))\r\n\r\nprint(\"Best trial:\")\r\ntrial = study_lgbm.best_trial\r\n\r\nprint(\"  Value: {}\".format(trial.value))\r\n\r\nprint(\"  Params: \")\r\nfor key, value in trial.params.items():\r\n    print(\"    {}: {}\".format(key, value))","082adf30":"# Step 1: Import helpful libraries","19d1d025":"The next section removes target from train data and creates the target variable","1440bd66":"Use Optuna to perform hyper parameter tuning","5f1269c3":"# Step 2: Load the data\n\nNext, we'll load the training and test data.  ","8ed98b1d":"The next section helps to identify the categorical columns and treat those specific columns by using Ordinal Encoder"}}