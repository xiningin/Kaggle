{"cell_type":{"d1296ddc":"code","f2d91ea7":"code","1ed0a60d":"code","274bb6ab":"code","3bdd34a0":"code","2772c608":"code","58045151":"code","ec944db9":"code","3c299a93":"code","bb45d11a":"code","3724bdda":"code","ddba009a":"code","0f7cea71":"code","e610dcc3":"code","b6f7b2a7":"code","5aaeb028":"code","0e9f03e6":"code","0264faf2":"code","cec6b565":"code","116a6d83":"code","152c1eda":"code","b5ac9156":"code","f226f913":"code","3421b79f":"code","497d9b6d":"code","7bdadc9c":"code","e4c79b9d":"code","ac18d180":"code","9597940f":"code","ab77938d":"code","250c1a23":"code","81213281":"code","c30d9191":"code","2e2e8261":"code","3781bba5":"code","e895dc4d":"code","29e7d182":"code","61973811":"code","9cb91ef7":"code","9dfdd9f6":"code","ffa36643":"code","9f52fe2c":"code","7fdf1c3d":"code","29e4d5ff":"code","d424c345":"code","ad762759":"code","74411802":"code","4c23b4b8":"code","31c312ca":"code","c4a9072f":"code","a52db4ec":"code","cee44eb2":"code","e5aeeff7":"code","0438b154":"code","9728b111":"code","cf7c28c6":"code","88578d26":"code","ab958090":"code","b5683600":"code","6ad5aa3d":"code","0441d117":"code","6e66d51f":"code","b8324f68":"markdown","eb2c6677":"markdown","816f6001":"markdown","bbe3ea5a":"markdown","b3a47501":"markdown","d994f495":"markdown","39b1a684":"markdown","af05558d":"markdown","8f68d63c":"markdown","7e31823a":"markdown","e4f6e4dc":"markdown","2c80ab90":"markdown","f2f1060c":"markdown","d2721862":"markdown","29fe0ed1":"markdown","e0e5f823":"markdown","52613d59":"markdown","f6a34a36":"markdown","5d23a5eb":"markdown","134defb0":"markdown","26654e86":"markdown","fc7e6fee":"markdown","95cd07f3":"markdown","8e80eae4":"markdown","b43f53c1":"markdown","752bdee5":"markdown","e4870252":"markdown","5d80badf":"markdown","884bc7ae":"markdown","eef18e8c":"markdown","c9485f3b":"markdown","601ae420":"markdown","2292328c":"markdown","7c1d43b1":"markdown","3657a128":"markdown","467be01b":"markdown","487e58d4":"markdown","a7004c95":"markdown","00a2153d":"markdown","c49724e8":"markdown","7414fee8":"markdown","5bbce633":"markdown","aed7155c":"markdown","02270d07":"markdown","dd45e627":"markdown","a1878729":"markdown","323fccb7":"markdown","5b69a61c":"markdown","e4c2143d":"markdown","76e51ea5":"markdown","536a8c36":"markdown","4d76ace8":"markdown","64d864d4":"markdown","d64003d0":"markdown","4e533ada":"markdown","d2a80c05":"markdown","87bd29b9":"markdown","36fcfacb":"markdown"},"source":{"d1296ddc":"#dataframe\nimport pandas as pd\nimport numpy as np\n\n#visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#preprocessing\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Binarizer\nfrom imblearn. over_sampling import SMOTE\n\n\n#Classification Algorithms\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve, auc\n\n# suppressing warnings\nimport warnings\nwarnings.filterwarnings('ignore')","f2d91ea7":"df = pd.read_csv('..\/input\/income-classification\/income_evaluation.csv')","1ed0a60d":"#checking the imported datafile\ndf.head()","274bb6ab":"#Determining dimensionality of dataframe\ndf.shape","3bdd34a0":"#Listing Dataframe columns\ndf.columns","2772c608":"#Removing spaces infront of the column names\nstripped_columns = []\nfor item in list(df.columns):\n    item = item.strip()\n    stripped_columns.append(item)\n#Replacing column names with stripped_columns\ndf.columns = stripped_columns\n#Checking new column names\ndf.columns","58045151":"#Using '_' to replace the column names\ncolumns= list(df.columns)\nadder = \"\"\nnew_columns =[]\nfor each_column in columns:\n    for letter in each_column:\n        if letter==\"-\":\n            adder = adder + \"_\"\n        else:\n            adder = adder + letter\n    new_columns.append(adder)\n    adder=\"\"\nnew_columns\n","ec944db9":"#replacing old columns with new column names\ndf.columns=new_columns","3c299a93":"#Determining dataframe columns characteristics\ndf.info()","bb45d11a":"#Listing datatypes of the columns\ndf.dtypes","3724bdda":"#Descriptive Analytics of quantitative variables in the dataframe\ndf.describe().T","ddba009a":"#calculating mode for categorical variables:\ndef mode(list_of_nums):\n    average = list_of_nums.mode()\n    return average\n\ncategorical_var = list(df.select_dtypes(include=object).columns)\n\n#printing mode of categorical variables\nfor item in categorical_var:\n    print(item + \" mode: \"+ mode(df[item]))\n    ","0f7cea71":"#Checking if any cells has missing values\ndf.isnull().sum()","e610dcc3":"#Determining different attributes for categorical variables\ncategorical_var = list(df.select_dtypes(include=object).columns)\nfor item in categorical_var:\n    print(item + \":\")\n    print(df[item].unique())","b6f7b2a7":"#replacing '?' with Unknown:\ndf['workclass'].replace(' ?', ' Unknown',inplace=True)\ndf['occupation'].replace(' ?', ' Unknown',inplace=True)\ndf['native_country'].replace(' ?', ' Unknown',inplace=True)","5aaeb028":"#list of numberical and categorical variables\ncategorical_var = list(df.select_dtypes(include=object).columns)\nnumerical_var = list(df.select_dtypes(exclude=object).columns)","0e9f03e6":"#importing matplotlib and seaborn packages\nimport matplotlib.pyplot as plt\nimport seaborn as sns","0264faf2":"#Creating bargraph for workclass\nplt.figure(figsize=(15,5))\nsns.set(style = 'white')\nsns.countplot(x=\"workclass\", data=df)\nplt.title(\"Population with Occupation Type\\n\",size =14)\nplt.ylabel(\"No of people\")\nplt.xlabel(\"\\nWorkclass Types\")","cec6b565":"#Bargraph representing number of people with different education level\nplt.figure(figsize=(18,5))\nsns.set(style = 'whitegrid')\nsns.countplot(x=\"education\", data=df,color='blue')\nplt.title(\"Population based upon their education level\\n\",size=14)\nplt.ylabel(\"Number of people\")\nplt.xlabel(\"\\n Education Level\")","116a6d83":"#Graph representing number of people based upon eduacation and sex\nplt.figure(figsize=(18,5))\nsns.countplot(\"education\",data=df,hue=\"sex\",color=\"blue\")","152c1eda":"#Graph representing marital_status of people in the dataset:\nplt.figure(figsize=(15,5))\nsns.countplot(\"marital_status\",data=df,hue=\"sex\",color=\"red\")","b5ac9156":"#Graph representing number of people based upon income:\nplt.figure(figsize=(10,5))\nsns.countplot(\"income\",data=df,hue=\"sex\",color=\"green\")","f226f913":"#Piechart showing representation of race in dataset\nfig1, ax1 = plt.subplots()\nX = list(df['race'].value_counts().index)\nY = list(df['race'].value_counts().values)\nax1.pie(Y,labels=X)\nax1.axis('equal') \nplt.show()","3421b79f":"#Histogram representing age\nsns.distplot(df['age'],bins=8)\nplt.title(\"Age Histogram\")","497d9b6d":"#Variation between age and income level\nsns.boxplot(x=df['income'],y=df['age'],color='r')\nplt.title(\"Box plot representing age and income \\n\", size = 15)","7bdadc9c":"#Variation between race and age\nplt.figure(figsize=(10,5))\nsns.boxplot(x=df['race'],y=df['age'],color='b')\nplt.title(\"Box plot representing age and income \\n\", size = 15)","e4c79b9d":"#Variation between marital status and age\nplt.figure(figsize=(14,5))\nsns.boxplot(x=df['marital_status'],y=df['age'],color='g')\nplt.title(\"Box plot representing age and income \\n\", size = 15)","ac18d180":"#Scatterplot representing relationship between age and hours-per-week\nplt.figure(figsize=(10,5))\nsns.scatterplot(x=\"age\",y=\"hours_per_week\",hue=\"sex\",data=df)\nplt.title('Relationship between hours worked and age',size=15)","9597940f":"#pairplot to determine relationship between all the quantitative variables\nsns.pairplot(df,height =1.5, hue=\"sex\", diag_kind=\"hist\")\n","ab77938d":"#Dropping Numerical Variables with Zero Variance\ndf.std()","250c1a23":"#Dropping Categorical Variables with zero variation\ncategorical_var = list(df.select_dtypes(include=object).columns)\nzero_cardinality = []\nfor column in categorical_var:\n    if len(df[column].value_counts().index)==1:\n        zero_cardinality.append(column)\nzero_cardinality","81213281":"#Dropping Categorical Variables with Multiple Levels:\nhigh_cardinality = []\nfor column in categorical_var:\n    if len(df[column].value_counts().index)>100:\n        high_cardinality.append(column)\nhigh_cardinality","c30d9191":"#Dropping relationship column\ndf=df.drop('relationship',axis=1)","2e2e8261":"df.head()","3781bba5":"\n#Standarizing numerical variables\nnumerical_var = list(df.select_dtypes(exclude=object).columns)\narray = df[numerical_var].values\nscaler = StandardScaler().fit(array)\ndf[numerical_var] = pd.DataFrame(scaler.transform(array))\n\n","e895dc4d":"#checking in our numerical variables are not standarized\ndf[numerical_var].head()","29e7d182":"#Checking if any column in our dataset has missing values\ndf.isnull().sum()","61973811":"#Selecting all categorical variables except for dependent variable\ncategorical_var = list(set(df.dtypes[df.dtypes==object].index) - set(['income']))\n\n#Creating dummy variables adn assigning dummy variabels to their original columns\ndummy_cat_df = pd.get_dummies(df[categorical_var],drop_first=True)\ndf=df.drop(categorical_var,axis=1)\ndf=pd.concat([df,dummy_cat_df],axis = 1)\n\n","9cb91ef7":"#Binarizing (encoding) dependent variable 'income'\ndummy_cat_df1 = pd.get_dummies(df['income'],drop_first=True)\ndf=df.drop('income',axis=1)\ndf=pd.concat([df,dummy_cat_df1],axis = 1)","9dfdd9f6":"df.head()","ffa36643":"df[' >50K'].value_counts()","9f52fe2c":"#balancing dependent variable\nfrom imblearn. over_sampling import SMOTE\nos= SMOTE(random_state=0)\ninput_var = list(set(df.columns)-set([' >50K']))\n\n\n\nX,Y = os.fit_sample(df[input_var],df[' >50K'])\n\n\nX=pd.DataFrame(X,columns=input_var)\nY = pd.DataFrame(Y,columns = [' >50K'])\n\n\n\ndf = pd.concat([X,Y],axis = 1)","7fdf1c3d":"df[' >50K'].value_counts()","29e4d5ff":"#renaming dependent variable to income\ndf.rename(columns={' >50K':'income'},inplace=True)","d424c345":"\ninput_var  = set(df.columns) - set(['income'])\nX = df[input_var]\nY = df['income']\n\nmodels = ['LogisticReg', 'DecisionTree', 'RandomForest', 'KNN','Support Vector Machine','Neural Networks']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n","ad762759":"# Logistic Regression\nlr = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr') # creates a lR instance\nlr.fit(X_train, Y_train)","74411802":"# Decision Trees\ndt = DecisionTreeClassifier(criterion = 'gini', splitter='best', max_depth=15)\ndt.fit(X_train, Y_train)","4c23b4b8":"# Random Forests\nrf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)\nrf.fit(X_train, Y_train)","31c312ca":"# K-NN\nknn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(X_train, Y_train)","c4a9072f":"#Support Vector Machines\nSVM = svm.LinearSVC()\nSVM.fit(X_train, Y_train)","a52db4ec":"#Neural Networks\nNN = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\nNN.fit(X_train, Y_train)","cee44eb2":"# computes the confusion matrix and performence indicators\ndef get_performances(actual_Y, pred_Y):\n    cm = confusion_matrix(actual_Y, pred_Y.round())\n    total = sum(sum(cm))\n    accuracy = (cm[0,0]+cm[1,1])\/total\n    sensitivity = cm[0,0]\/(cm[0,0]+cm[0,1])\n    specificity = cm[1,1]\/(cm[1,0]+cm[1,1])\n    return accuracy, sensitivity, specificity","e5aeeff7":"\npred_Y_lr = lr.predict(X_test) # predicts the Y given the fitted model lr\n\npred_Y_dt = dt.predict(X_test) # predicts the Y given the fitted model dt\n\npred_Y_rf = rf.predict(X_test) # predicts the Y given the fitted model rf\n\npred_Y_knn = knn.predict(X_test) # predicts the Y given the fitted model knn\n\npred_Y_svm = SVM.predict(X_test) # predicts the Y given the fitted model Support Vector Machines\n\npred_Y_nn = NN.predict(X_test) # predicts the Y given the fitted model Neural Networks","0438b154":"\n# accuracy, sensitivity, and specificity for model lr\naccuracy_lr, sensitivity_lr, specificity_lr = get_performances(Y_test, pred_Y_lr)\n\n\n# accuracy, sensitivity, and specificity for model dt\naccuracy_dt, sensitivity_dt, specificity_dt = get_performances(Y_test, pred_Y_dt)\n\n\n# accuracy, sensitivity, and specificity for model rf\naccuracy_rf, sensitivity_rf, specificity_rf = get_performances(Y_test, pred_Y_rf)\n\n# accuracy, sensitivity, and specificity for model knn\naccuracy_knn, sensitivity_knn, specificity_knn = get_performances(Y_test, pred_Y_knn)\n\n# accuracy, sensitivity, and specificity for model Support Vector Machines\naccuracy_svm, sensitivity_svm, specificity_svm = get_performances(Y_test, pred_Y_svm)\n\n# accuracy, sensitivity, and specificity for model Neural Networks\naccuracy_nn, sensitivity_nn, specificity_nn = get_performances(Y_test, pred_Y_nn)\n","9728b111":"# Builds a dataframe using the performance indicators so that we can compare the models easily\n\nperf = pd.DataFrame([accuracy_lr, accuracy_dt,accuracy_rf,accuracy_knn,accuracy_svm,accuracy_nn], \n                    columns = ['accuracy'], \n                    index = ['Logistic Regression',\n                             'Decision Trees', 'Random Forest','K-NN','Support Vector Machine','Neural Networks'])\n\nperf['sensitivity'] = np.asarray([sensitivity_lr,\n                                  sensitivity_dt, \n                                  sensitivity_rf,\n                                  sensitivity_knn,sensitivity_svm,sensitivity_nn])\n\nperf['specificity'] = np.asarray([specificity_lr,\n                                  specificity_dt,\n                                  specificity_rf,\n                                  specificity_knn,specificity_svm,specificity_nn])\nperf","cf7c28c6":"from sklearn.metrics import roc_curve, auc","88578d26":"score_Y_nn = NN.predict_proba(X_test)\nscore_Y_nn ","ab958090":"score_Y_nn = NN.predict_proba(X_test)\nfpr, tpr, _ = roc_curve(Y_test, score_Y_nn[:,1])\nroc_auc = auc(fpr, tpr)","b5683600":"score_Y_nn = NN.predict_proba(X_test)\nfpr, tpr, _ = roc_curve(Y_test, score_Y_nn[:,1])\nroc_auc = auc(fpr, tpr)\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkred',lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='pink', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()","6ad5aa3d":"clf = RandomForestClassifier(n_estimators=100, random_state=0)\n\nclf.fit(X_train, Y_train)","0441d117":"\nft_scores = pd.Series(clf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n\nft_scores","6e66d51f":"#plotting ft_scores to visualize significance of individual column\nplt.figure(figsize = (17,10))\nplt.xlabel('Columns')\nplt.ylabel('feature_scores \\n')\nplt.tight_layout()\n\nft_scores.plot.bar()\n\n","b8324f68":"From above histogram, we can see that most of the people belong to the age group of 20-40 years and age group above 80 has the least number of people.","eb2c6677":"The original dataset was obtained from the 1994 U.S. Census database.   The census is performed every decade within the United States.  It gathers information on the population living within the country.  The information captured from the census consists of data on age, gender, country of origin, marital status, housing conditions, marriage, education, employment, etc. The sneakpeak of the dataset is as follows: <br>\n![image.png](attachment:image.png)\n","816f6001":"The above bargraph shows that most of the people are engaged in private occupaction followed by self-employed, local-gov, and state-gov.","bbe3ea5a":"Above result shows the mode of each categorical columns in the dataset.","b3a47501":"Predictive analytics can be divided into three parts:<br>\na) Supervised Learning<br>\nb) Unsupervised Learning<br>\nc) Reinforcement Learning\n\nOur project falls under supervised learning category as have a target variable (income) generated by feeding other input variables. Specifically, it falls under classification as we are predicting binary outcomes (i.e. whether income is above 50K or not). The classification algorithms used to test our model are:\n- Logistic Regression\n- Decision Trees\n- Random Forest\n- K-NN\n- Vector Machine\n- Neural Network\n","d994f495":"Above code checks whether there are any empty\/blank cells in the dataset. As we can see, there are no empty cell in any column in our dataset.","39b1a684":"Majority of younger people are single. The median age of single people is 25 and that of married and divorced are 42 and 42 respectively. We can also see that most of the widowed female belonged to the age group between 50 to 70 with 60 being the median age.","af05558d":"###  e. Balancing Dataset","8f68d63c":"As we can see, the value count of income less than or equal to 50K (encoded by 0) is 24720 whereas income above 50K (encoded by 1) is 7841. However, before runny any classification algorithm, we need to balance the dataset.  ","7e31823a":"Now, the income greater than 50K is encoded by 1, and income less than or equal to 50K is encoded by 0.","e4f6e4dc":"### d. Feature Importance using Random Forest ","2c80ab90":"Above are the list of column names in our dataset.","f2f1060c":"\nHaving salary data is important in the business world because it provides businesses an insight that they need. It gives them a better idea regarding what should be consistent, what went well, what didn't go well, and what would have been done differently in the future. Having the right set of data, the business will have a clear vision and an informed decision. Salary data would help people understand as well as improving the business process which might help people reduce the wasted money or time and every company can fill in the gap of this. This would help in depleting the resources or impact the bottom line or anything else which is related. Salary data could be a problem if it is not accounted for as this would not give the business visibility of how much has been paid and how much profit is remaining with the organization for other activity or growth.","d2721862":"As we can see, Neural Networks has high accuracy and sensitivity, and K-NN has high specificity. Depending upon the purpose of study, we can chose either Neural Networks or K-NN. Since, we are trying to predict income based upon accuracy, we will chose Neural Network for the purpose of our project.","29fe0ed1":"Boxplot can be used to accuratly measure the variation between the variables. As we can see, the median age of people earning less than 50,000 is around 34 years and that of people earling more than 50,000 is around 45 years. ","e0e5f823":"All our initial column names included \" \" (space) before their names. Therefore, to make it easier to call columns, we removed the spaces before the column names.","52613d59":"## 7. Data Preprocessing","f6a34a36":"The above piechart shows that most of the records in our dataset belong to white population followed by black, pacific islanders, American-Indian eskimo, and others.","5d23a5eb":"## 2. Why problem is important?\n","134defb0":"Dataset gathers income information about people living in America in 1994.  The data tracks age, education, marital status, hours worked and shares how much money an individual brings home based on those characteristics.  The implications of this project may help society to determine what a person living in America needs to do to acquire a better income, while also bringing to light possible discrimination.  It shows how working more hours has a better chance of improving your earning potential.  It also shows a correlation between being married and income.  Generally, it seems married couples on average have a better chance of making more than 50,000 than a single person. The project also shows the racial inequality in the 1994\u2019s.  Whites had a greater percentage of people with incomes greater than 50,000.  Another socioeconomic issue brought forth is how men were three times more likely to have incomes greater than 50,000 as compared to women.  \n\nThis model can be used to analyze data of any year. We can update the datasource and use the same code to derive our socioeconomic conclusions. By adding more recent observations of individuals to the dataset, we can also build a predictive model which takes present salary data into consideration.\n","26654e86":"The above bar graph shows that most people in the dataset are highschool graduates followed by some college, and Bachelors degree.","fc7e6fee":"## 4. Importing Dataset ","95cd07f3":"1. age: Represents an individual\u2019s years on earth. <br>\n2. workclass: Represents an individual\u2019s employment status\n3. fnlwgt: Represents the final weight.  This is the number of people the census believes the entry represents.\n4. education: Represents an individual\u2019s highest level of education achieved in object form.\n5. education-num: Represents an individual\u2019s highest level of education achieved in numerical form.\n6. marital-status: Represents an individual\u2019s civil status in relation to a significant other.\n7. occupation: Represents an individual\u2019s general occupation\n8. relationship: Represents an individual\u2019s relation to others in a family unit.  The options aren\u2019t consistent and seem to overlap with marital-status column.\n9. race: Represents an individual\u2019s shared physical or general region of origin\n10. sex: Represents an individual\u2019s gender.  This dataset only had two options for gender.\n11. capital-gain: Represents an individual\u2019s profit from the sale of property or an investment\n12. capital-loss: Represents an individual\u2019s loss from the sale of property or an investment\n13. hours-per-week: Represents an individual\u2019s hours worked per week.\n14. native-country: Represents an individual\u2019s country of origin\n15. Income: The amount of money an individual generates within a year.\n\n_Source: Kaggle_\n","8e80eae4":"Since we do not have any numerical variable with zero standard deviation and categorical variable with zero or high cardinality, we do not need to drop any variables. However, both marital_status and relationship represent similar information. So, we will drop the relationship column.","b43f53c1":"Now, our output variable is balanced as shown above. <br> <br> For our simpliticy to call the dependent column, we will rename dependent variable ' >50K' to 'income'. ","752bdee5":"The above table shows descriptive statistics for numerical variables in our dataset.","e4870252":"This graph shows the education level of people along with their sex. We can clearly make a distinction that the dataset contains more male recods than female.","5d80badf":"   Salary data is important for many businesses as it can play a significant role in distribution of their resources. For example, a high end product needs to market their goods only to people with high income. A charitable organization may be focused to invest in areas where people make less money. Likewise, it will also help businesses to determine prices of its products and forecast sales. Governments may also be interested to know the status-quo of their citizens to carry out effective economic and development plans. \n    \n    \nSince the project determines whether a person makes more than 50K or not, we are determining the dependent categorical variable. Here, we will first collect the relevant data set. Once we collect the data, we will prepare the data using pandas, numpy and visualize the data to determine the relationship between the variables. After studying the data set, we will preprocess the data using python where we will eliminate the outliers  and manage missing values. Once we successfully preprocess the data, we will make use of python\u2019s supervised learning packages and algorithms to build our model. Once we prepare the model, we will access the model based upon its accuracy, sensitivity, and specificity. If the model meets our requirements, we will deploy the model to make predictions on income level of individuals.\n","884bc7ae":"## 9. Implications of the project for future studies\n","eef18e8c":"## 6. Data Visualization","c9485f3b":"For the purpose of this project, we will standardize the numerical variables. While there is no any specific way to determine whether to normalize or standardize, we chose standardization as our model uses multiple input variables. We also tested both normalization and standardization, and standardization gave better output for our model.","601ae420":"Our column names also include '-' which is not recognised by pandas and gives error while calling it. Therefore, we will replaced '-' with '_' as shown above.","2292328c":"## 1. Problem Definition","7c1d43b1":"### b. Model Assessment and Making Predictions","3657a128":"## 2. Understanding Dataset","467be01b":"### c. ROC curve","487e58d4":"There is not much difference in age of working population among different races. ","a7004c95":"Majority of people have income less than or equal to $50,000 as shown by the graph above ","00a2153d":"Since we do not have any missing cells in our dataset, we do not need to imputate missing values. If we had any missing quantitative variables, we would replace missing values with median, and with mode in case of categorical variables. If we had too many missing values (>20%), we would drop the entire column to maintain accuracy of our model. ","c49724e8":"## 3. Importing Libraries","7414fee8":"Roc curve shows performance of the classification model. It plots false positive rate (X-axis) and true positive rate (Y-axis). The area under the ROC curve is 0.93 which represents that our model is very useful and it able to provide output 93% of the times.","5bbce633":"# <p style=\"text-align: center;\">Income Prediction <\/p>","aed7155c":"## 5. Exploratory Data Analysis","02270d07":"Most of the working people are married followed by single people. But in case of female, most of the working females are single followed by divorced and married.","dd45e627":"### b. Standardizing Numerical Variables","a1878729":"As we can see, there are 32561 rows (records) and 15 columns in our dataset.","323fccb7":"We can see that age, fnlwgt has major significance in our model. Whereas, workclass_never-worked, native_country_ Holand-Netherlands has very less significance. Therefore, we can also drop those columns and re-run our prediction model to improve accuracy.","5b69a61c":"From above scatterplot, we can see that there is no definite relationship between age and hours they work, both in case of male and female. However, we can see that male tends to work more hours per week than that of female.","e4c2143d":"### d. Encoding Categorical variables","76e51ea5":"### a. Fitting Model","536a8c36":"Data Preprocessing is an important step in predictive analytics. It helps us to eliminate outliers, manage missing values, and handle categorical variables. Our data preprocessing steps includes:\n- Dropping Variables\n- Data Scaling(Normalization\/ Standardization)\n- Missing Value Imputation\n- Encoding Catergorical variables\n- Balancing dataset","4d76ace8":"### a. Dropping Variables","64d864d4":"### c. Data Imputation","d64003d0":"## 8. Predictive analytics","4e533ada":"### a. Categorical Variables","d2a80c05":"The above graph shows the relationship between the quantitative variables. From above scatterplots, we can see that there is not definite correlatio between the numrical variables.","87bd29b9":"This shows different categories for each column. We can see that occupation column, workclass and native_country column consists of ?, which is a missing\/unknown value. So, we will replace them with another separate attribute known as Unknown.","36fcfacb":"### b. Quantitative Variables"}}