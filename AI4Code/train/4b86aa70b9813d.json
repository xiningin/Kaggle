{"cell_type":{"04edb79c":"code","4e4e948f":"code","5fc603b2":"code","d3038a0b":"code","3ee589a8":"code","91aaabb5":"code","a358eb2e":"code","9a6491f5":"code","0fab0a82":"code","56fb3d09":"code","cf957c56":"code","d435c434":"code","03428d1c":"code","14255617":"code","baadb9f7":"code","b03df95f":"code","d5f42fda":"code","7a8a4478":"code","20d543e7":"code","dc2797a7":"code","225415b8":"code","915ccb2b":"code","db240392":"code","2122ec0b":"code","d4a6473d":"code","ac8fe29e":"code","312dc35f":"code","298a49fa":"code","a1b07980":"code","2e185d08":"code","2e17649f":"code","dc54dc2f":"code","79002519":"code","b2e8dbfa":"code","41b96776":"code","fc647d88":"markdown","f0e1801f":"markdown"},"source":{"04edb79c":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score , average_precision_score \nfrom sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve ,auc , log_loss ,  classification_report \nfrom sklearn.preprocessing import StandardScaler , Binarizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nimport time\nimport os, sys, gc, warnings, random, datetime\nimport math\nimport shap\nimport joblib\nwarnings.filterwarnings('ignore')\n\nimport xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold , cross_val_score\nfrom sklearn.metrics import roc_auc_score","4e4e948f":"df = pd.read_pickle('..\/input\/searching-for-bad-loan-data-preprocessing\/df_pp.pkl')","5fc603b2":"df['Loan_status'].value_counts()","d3038a0b":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndf['Loan_status'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Loan_status')\nax[0].set_ylabel('')\nsns.countplot('Loan_status',data=df,ax=ax[1])\nax[1].set_title('Loan_status')\nplt.show()","3ee589a8":"!pip install fastcluster","91aaabb5":"###Libraries\n\n\nimport numpy as np\nimport pandas as pd\nimport os, time, re\nimport pickle, gzip\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nimport matplotlib as mpl\n\n%matplotlib inline\n\n\nfrom sklearn import preprocessing as pp\n#from sklearn import impute.SimpleImputer as pp\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import precision_recall_curve, average_precision_score\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\n\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nimport fastcluster\nfrom scipy.cluster.hierarchy import dendrogram, cophenet, fcluster\nfrom scipy.spatial.distance import pdist\nimport os, sys, gc, warnings, random, datetime\nwarnings.filterwarnings('ignore')\n\n","a358eb2e":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","9a6491f5":"# # for min_max scaling\n# from mlxtend.preprocessing import minmax_scaling\n# df = minmax_scaling(df ,columns =df.columns)","0fab0a82":"from sklearn import preprocessing as pp","56fb3d09":"X = df.drop('Loan_status', axis=1)\n# x_dd = X.copy()\n# from sklearn.preprocessing import StandardScaler\n# from sklearn.preprocessing import MinMaxScaler\n# minMaxScaler = MinMaxScaler()\n# minMaxScaler.fit(X)\n# X_scaled = minMaxScaler.transform(X)\n\n# X= pd.DataFrame(X_scaled, columns=x_dd.columns)\n# X = reduce_mem_usage(X)\ny = df['Loan_status']\n\nfrom sklearn import preprocessing as pp\nfeaturesToScale = X.columns\nsX = pp.MinMaxScaler(copy=True)\nX.loc[:,featuresToScale] = sX.fit_transform(X[featuresToScale])\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2 , random_state = 2020, stratify = y)","cf957c56":"def anomalyScores(originalDF, reducedDF):\n    loss = np.sum((np.array(originalDF)-np.array(reducedDF))**2, axis=1)\n    loss = pd.Series(data=loss,index=originalDF.index)\n    loss = (loss-np.min(loss))\/(np.max(loss)-np.min(loss))\n    return loss","d435c434":"def plotResults(trueLabels, anomalyScores, returnPreds = False):\n    preds = pd.concat([trueLabels, anomalyScores], axis=1)\n    preds.columns = ['trueLabel', 'anomalyScore']\n    precision, recall, thresholds = \\\n        precision_recall_curve(preds['trueLabel'],preds['anomalyScore'])\n    average_precision = \\\n        average_precision_score(preds['trueLabel'],preds['anomalyScore'])\n    \n    plt.step(recall, precision, color='k', alpha=0.7, where='post')\n    plt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.ylim([0.0, 1.05])\n    plt.xlim([0.0, 1.0])\n    \n    plt.title('Precision-Recall curve: Average Precision = \\\n    {0:0.2f}'.format(average_precision))\n\n    fpr, tpr, thresholds = roc_curve(preds['trueLabel'], \\\n                                     preds['anomalyScore'])\n    areaUnderROC = auc(fpr, tpr)\n\n    plt.figure()\n    plt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\n    plt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic: \\\n    Area under the curve = {0:0.2f}'.format(areaUnderROC))\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    \n    if returnPreds==True:\n        return preds","03428d1c":"def scatterPlot(xDF, yDF, algoName):\n    tempDF = pd.DataFrame(data=xDF.loc[:,0:1], index=xDF.index)\n    tempDF = pd.concat((tempDF,yDF), axis=1, join=\"inner\")\n    tempDF.columns = [\"First Vector\", \"Second Vector\", \"Label\"]\n    sns.lmplot(x=\"First Vector\", y=\"Second Vector\", hue=\"Label\", \\\n               data=tempDF, fit_reg=False)\n    ax = plt.gca()\n    ax.set_title(\"Separation of Observations using \"+algoName)","14255617":"len(df.columns)","baadb9f7":"\nfrom sklearn.decomposition import PCA\n\n\nn_components = range(20,32)\n\nfor i in n_components:\n    \n    print('Number of N_components : ', i)\n    whiten = False\n    random_state = 2020\n\n    pca = PCA(n_components=i, whiten=whiten, \\\n              random_state=random_state)\n\n    X_train_PCA = pca.fit_transform(X_train)\n    X_train_PCA = pd.DataFrame(data=X_train_PCA, index=X_train.index)\n\n    X_train_PCA_inverse = pca.inverse_transform(X_train_PCA)\n    X_train_PCA_inverse = pd.DataFrame(data=X_train_PCA_inverse, \\\n                                       index=X_train.index)\n    \n    anomalyScoresPCA = anomalyScores(X_train, X_train_PCA_inverse)\n    preds = plotResults(y_train, anomalyScoresPCA, True)\n\n","b03df95f":"whiten = False\nrandom_state = 2020\n\npca = PCA(n_components=26, whiten=whiten, \\\n              random_state=random_state)\n\nX_train_PCA = pca.fit_transform(X_train)\nX_train_PCA = pd.DataFrame(data=X_train_PCA, index=X_train.index)\n\nX_train_PCA_inverse = pca.inverse_transform(X_train_PCA)\nX_train_PCA_inverse = pd.DataFrame(data=X_train_PCA_inverse, \\\n                                       index=X_train.index)\nscatterPlot(X_train_PCA, y_train, \"PCA\")    ","d5f42fda":"anomalyScoresPCA = anomalyScores(X_train, X_train_PCA_inverse)\npreds = plotResults(y_train, anomalyScoresPCA, True)","7a8a4478":"preds.sort_values(by=\"anomalyScore\",ascending=False,inplace=True)\ncutoff = 46467\npredsTop = preds[:cutoff]\nprint(\"Precision: \",np.round(predsTop. \\\n            anomalyScore[predsTop.trueLabel==1].count()\/cutoff,2))\nprint(\"Recall: \",np.round(predsTop. \\\n            anomalyScore[predsTop.trueLabel==1].count()\/y_train.sum(),2))\nprint(\"Bad Loan Caught out of 46467 cases:\", predsTop.trueLabel.sum())","20d543e7":"from sklearn.decomposition import SparsePCA\n\nn_components = 26\nalpha = 0.0001\nrandom_state = 2020\nn_jobs = -1\n\n# sparsePCA = SparsePCA(n_components=n_components, \\\n#                alpha=alpha, random_state=random_state, n_jobs=n_jobs)\n\nsparsePCA = SparsePCA(n_components=n_components, \\\n                alpha=alpha, random_state=random_state, n_jobs=n_jobs,normalize_components='deprecated')\n\nsparsePCA.fit(X_train.loc[:,:])\nX_train_sparsePCA = sparsePCA.transform(X_train)\nX_train_sparsePCA = pd.DataFrame(data=X_train_sparsePCA, index=X_train.index)\n\nscatterPlot(X_train_sparsePCA, y_train, \"Sparse PCA\")","dc2797a7":"X_train_sparsePCA_inverse = np.array(X_train_sparsePCA). \\\n    dot(sparsePCA.components_) + np.array(X_train.mean(axis=0))\nX_train_sparsePCA_inverse = \\\n    pd.DataFrame(data=X_train_sparsePCA_inverse, index=X_train.index)\n\nanomalyScoresSparsePCA = anomalyScores(X_train, X_train_sparsePCA_inverse)\npreds = plotResults(y_train, anomalyScoresSparsePCA, True)","225415b8":"# \ucee4\ub110 PCA\nfrom sklearn.decomposition import KernelPCA\n\nn_components = 26\nkernel = 'rbf'\ngamma = None\nfit_inverse_transform = True\nrandom_state = 2020\nn_jobs = 1\n\nkernelPCA = KernelPCA(n_components=n_components, kernel=kernel, \\\n                gamma=gamma, fit_inverse_transform= \\\n                fit_inverse_transform, n_jobs=n_jobs, \\\n                random_state=random_state)\n\nkernelPCA.fit(X_train.iloc[:2000])\nX_train_kernelPCA = kernelPCA.transform(X_train)\nX_train_kernelPCA = pd.DataFrame(data=X_train_kernelPCA, \\\n                                 index=X_train.index)\n\nX_train_kernelPCA_inverse = kernelPCA.inverse_transform(X_train_kernelPCA)\nX_train_kernelPCA_inverse = pd.DataFrame(data=X_train_kernelPCA_inverse, \\\n                                         index=X_train.index)\n\nscatterPlot(X_train_kernelPCA, y_train, \"Kernel PCA\")","915ccb2b":"anomalyScoresKernelPCA = anomalyScores(X_train, X_train_kernelPCA_inverse)\npreds = plotResults(y_train, anomalyScoresKernelPCA, True)","db240392":"# \uac00\uc6b0\uc2dc\uc548 \ub79c\ub364 \ud22c\uc601\nfrom sklearn.random_projection import GaussianRandomProjection\n\nn_components = 26\neps = None\nrandom_state = 2020\n\nGRP = GaussianRandomProjection(n_components=n_components, \\\n                               eps=eps, random_state=random_state)\n\nX_train_GRP = GRP.fit_transform(X_train)\nX_train_GRP = pd.DataFrame(data=X_train_GRP, index=X_train.index)\n\nscatterPlot(X_train_GRP, y_train, \"Gaussian Random Projection\")","2122ec0b":"X_train_GRP_inverse = np.array(X_train_GRP).dot(GRP.components_)\nX_train_GRP_inverse = pd.DataFrame(data=X_train_GRP_inverse, \\\n                                   index=X_train.index)\n\nanomalyScoresGRP = anomalyScores(X_train, X_train_GRP_inverse)\npreds = plotResults(y_train, anomalyScoresGRP, True)","d4a6473d":"# \ud76c\uc18c \ub79c\ub364 \ud22c\uc601\n\nfrom sklearn.random_projection import SparseRandomProjection\n\nn_components = 26\ndensity = 'auto'\neps = .01\ndense_output = True\nrandom_state = 2018\n\nSRP = SparseRandomProjection(n_components=n_components, \\\n        density=density, eps=eps, dense_output=dense_output, \\\n                                random_state=random_state)\n\nX_train_SRP = SRP.fit_transform(X_train)\nX_train_SRP = pd.DataFrame(data=X_train_SRP, index=X_train.index)\n\nscatterPlot(X_train_SRP, y_train, \"Sparse Random Projection\")","ac8fe29e":"X_train_SRP_inverse = np.array(X_train_SRP).dot(SRP.components_.todense())\nX_train_SRP_inverse = pd.DataFrame(data=X_train_SRP_inverse, index=X_train.index)\n\nanomalyScoresSRP = anomalyScores(X_train, X_train_SRP_inverse)\nplotResults(y_train, anomalyScoresSRP)","312dc35f":"# \ubbf8\ub2c8-\ubc30\uce58 \uc0ac\uc804 \ud559\uc2b5\nfrom sklearn.decomposition import MiniBatchDictionaryLearning\n\nn_components = 7\nalpha = 1\nbatch_size = 200\nn_iter = 10\nrandom_state = 2020\n\nminiBatchDictLearning = MiniBatchDictionaryLearning( \\\n    n_components=n_components, alpha=alpha, batch_size=batch_size, \\\n    n_iter=n_iter, random_state=random_state)\n\nminiBatchDictLearning.fit(X_train)\nX_train_miniBatchDictLearning = \\\n    miniBatchDictLearning.fit_transform(X_train)\nX_train_miniBatchDictLearning = \\\n    pd.DataFrame(data=X_train_miniBatchDictLearning, index=X_train.index)\n\nscatterPlot(X_train_miniBatchDictLearning, y_train, \\\n            \"Mini-batch Dictionary Learning\")","298a49fa":"X_train_miniBatchDictLearning_inverse = \\\n    np.array(X_train_miniBatchDictLearning). \\\n    dot(miniBatchDictLearning.components_)\n\nX_train_miniBatchDictLearning_inverse = \\\n    pd.DataFrame(data=X_train_miniBatchDictLearning_inverse, \\\n                 index=X_train.index)\n\nanomalyScoresMiniBatchDictLearning = anomalyScores(X_train, \\\n    X_train_miniBatchDictLearning_inverse)\npreds = plotResults(y_train, anomalyScoresMiniBatchDictLearning, True)","a1b07980":"# \ub3c5\ub9bd \uc131\ubd84 \ubd84\uc11d\n\nfrom sklearn.decomposition import FastICA\n\nn_components = 26\nalgorithm = 'parallel'\nwhiten = True\nmax_iter = 200\nrandom_state = 2020\n\nfastICA = FastICA(n_components=n_components, \\\n    algorithm=algorithm, whiten=whiten, max_iter=max_iter, \\\n    random_state=random_state)\n\nX_train_fastICA = fastICA.fit_transform(X_train)\nX_train_fastICA = pd.DataFrame(data=X_train_fastICA, index=X_train.index)\n\nX_train_fastICA_inverse = fastICA.inverse_transform(X_train_fastICA)\nX_train_fastICA_inverse = pd.DataFrame(data=X_train_fastICA_inverse, \\\n                                       index=X_train.index)\n\nscatterPlot(X_train_fastICA, y_train, \"Independent Component Analysis\")","2e185d08":"anomalyScoresFastICA = anomalyScores(X_train, X_train_fastICA_inverse)\nplotResults(y_train, anomalyScoresFastICA)","2e17649f":"# \ud14c\uc2a4\ud2b8 \uc14b\uc5d0 PCA \uc801\uc6a9\nX_test_PCA = pca.transform(X_test)\nX_test_PCA = pd.DataFrame(data=X_test_PCA, index=X_test.index)\n\nX_test_PCA_inverse = pca.inverse_transform(X_test_PCA)\nX_test_PCA_inverse = pd.DataFrame(data=X_test_PCA_inverse, \\\n                                  index=X_test.index)\n\nscatterPlot(X_test_PCA, y_test, \"PCA\")","dc54dc2f":"anomalyScoresPCA = anomalyScores(X_test, X_test_PCA_inverse)\npreds = plotResults(y_test, anomalyScoresPCA, True)","79002519":"# \ud14c\uc2a4\ud2b8 \uc14b\uc5d0 \ub3c5\ub9bd \uc131\ubd84 \ubd84\uc11d \uc801\uc6a9\nX_test_fastICA = fastICA.transform(X_test)\nX_test_fastICA = pd.DataFrame(data=X_test_fastICA, index=X_test.index)\n\nX_test_fastICA_inverse = fastICA.inverse_transform(X_test_fastICA)\nX_test_fastICA_inverse = pd.DataFrame(data=X_test_fastICA_inverse, \\\n                                      index=X_test.index)\n\nscatterPlot(X_test_fastICA, y_test, \"Independent Component Analysis\")","b2e8dbfa":"anomalyScoresFastICA = anomalyScores(X_test, X_test_fastICA_inverse)\nplotResults(y_test, anomalyScoresFastICA)","41b96776":"from sklearn.cluster import KMeans\n#K-Means Clustering with two clusters\nkmeans = KMeans(n_clusters=5)\nkmeans.fit(X_train)\n\nclusters = kmeans.predict(X_train)\n\n#'cluster_df' will be used as a DataFrame\n#to assist in the visualization\ncluster_df = pd.DataFrame()\n\ncluster_df['cluster'] = clusters\ncluster_df['class'] = y\n\nsns.factorplot(col='cluster', y=None, x='class', data=cluster_df, kind='count', order=[1,0], palette=([\"#7d069b\",\"#069b15\"]))","fc647d88":"### Sparse PCA","f0e1801f":"### Normal PCA"}}