{"cell_type":{"23b79c5b":"code","11b74f9f":"code","fcd42c41":"code","286c31b9":"code","954091b2":"code","1bf69e4c":"code","025e2c1c":"code","8af271f3":"code","99fbb237":"code","a464115c":"code","845bb819":"code","4a449f04":"code","d6741ac9":"code","62c9148c":"code","b32ad998":"code","bfcfc692":"code","0164b781":"code","e9bc3f8f":"code","b04714cd":"code","c1ea8c6f":"code","a0cf8f9f":"code","c558e561":"code","d6c3d1dd":"code","a619ccc3":"code","ad09efe0":"code","fd07033d":"code","3fcdfd03":"code","8f6319f8":"code","b9c696e5":"code","c675787b":"code","00c09075":"code","c86a3dd7":"code","cdf82bc8":"code","f05a2969":"code","10c6a4f0":"code","e2ad94d0":"code","590ae574":"code","822b1ccc":"code","93642a42":"code","2fb505d2":"code","a959d68c":"code","41e9d412":"code","73885fcb":"code","e5fa89f6":"code","39598d9c":"code","37358f04":"code","2360f7dd":"code","f8038d49":"markdown","231eed5b":"markdown","4f72ddfd":"markdown","3ab6ee98":"markdown","19a18afb":"markdown","5359e5bb":"markdown","80ec50f1":"markdown","2e47996c":"markdown","2a97ecdf":"markdown","4e554ed6":"markdown","da093e2c":"markdown","545ea644":"markdown","a852ccbd":"markdown","070b0aa3":"markdown","790f8307":"markdown","54cd8ea4":"markdown"},"source":{"23b79c5b":"import warnings\nwarnings.filterwarnings(\"ignore\");","11b74f9f":"import pandas as pd","fcd42c41":"PS_reviews_data = pd.read_csv('..\/input\/google-play-store-reviews\/reviews.csv')\nPS_reviews_data.head()","286c31b9":"PS_reviews_data = PS_reviews_data[['content','score']]\nPS_reviews_data.head()","954091b2":"PS_reviews_data.isnull().sum()","1bf69e4c":"import seaborn as sns","025e2c1c":"sns.barplot(x=PS_reviews_data.score.value_counts().index,y=PS_reviews_data.score.value_counts())","8af271f3":"import nltk\nnltk.download('vader_lexicon')","99fbb237":"from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA","a464115c":"def analyze_sentiment(data):\n    analysis = SIA().polarity_scores(data)\n    if analysis['compound'] > 0:    # Converting rating 3 with positive sentiment as group 1\n        return 'Positive'\n    elif analysis['compound'] == 0: # Converting rating 3 with neutral sentiment as group to drop\n        return 'Neutral'\n    else:                           # Converting rating 3 with negative sentiment as group 0\n        return 'Negative'","845bb819":"PS_reviews_data['sentiment_neutral_ratings'] = PS_reviews_data[PS_reviews_data.score == 3].content.apply(analyze_sentiment)","4a449f04":"PS_reviews_data[PS_reviews_data.score == 3].head()","d6741ac9":"PS_reviews_data[PS_reviews_data.sentiment_neutral_ratings == 'Neutral'].head() # Drop these records","62c9148c":"PS_reviews_data = PS_reviews_data[PS_reviews_data.sentiment_neutral_ratings != 'Neutral'] # Discarding neutral records","b32ad998":"def rating_seg(df):\n  score_ = 0\n  if df['score'] <= 2:\n    score_ = 0\n  elif df['score'] == 3:\n    if df['sentiment_neutral_ratings'] == 'Positive':\n      score_ = 0\n    else:\n      score_ = 1\n  else:\n    score_ = 1\n  return score_\n","bfcfc692":"PS_reviews_data['new_score'] = PS_reviews_data.apply(rating_seg, axis=1)\nPS_reviews_data[['score','new_score']].sample(frac=1.0).head() # verifying data","0164b781":"PS_reviews_data.drop(['sentiment_neutral_ratings'],inplace=True,axis=1)","e9bc3f8f":"PS_reviews_data.isnull().sum()\n# Since our data has no null values will be skip this step","b04714cd":"PS_reviews_data['content'].head()\n# from below data we can verify that our data has uppercase letters, we will convert our data in lower case","c1ea8c6f":"PS_reviews_data['new_content'] = PS_reviews_data['content'].apply(lambda x: str(x).lower())\nPS_reviews_data[['content','new_content']].head()","a0cf8f9f":"PS_reviews_data.loc[PS_reviews_data['new_content'].str.contains(\"[^a-z\\s]\"),'new_content'].head()\n\n# From below data we can notice that there are many rows that has non-aplhabet data","c558e561":"PS_reviews_data['new_content'] = PS_reviews_data['new_content'].str.replace(r'[^a-zA-Z\\s]', ' ') # replacing junk data with space\nPS_reviews_data[['content','new_content']].head()","d6c3d1dd":"PS_reviews_data.loc[PS_reviews_data['new_content'].str.contains(\"[^a-z\\s]\"),'new_content']\n# Our data now contains only alphabets","a619ccc3":"PS_reviews_data[['content','new_content']].head().style.set_properties( **{'width': '400px'})","ad09efe0":"PS_reviews_data.loc[PS_reviews_data['new_content'].str.contains(\"\\s{2,}\"),'new_content'].head()\n# From below data we can notice after removing non-alphabet data we have extra spaces(2 or more) in between","fd07033d":"PS_reviews_data['new_content'] = PS_reviews_data['new_content'].str.replace(r'\\s{2,}', ' ')","3fcdfd03":"from nltk.tokenize import word_tokenize\nnltk.download('punkt')","8f6319f8":"PS_reviews_data['new_content'] = PS_reviews_data['new_content'].apply(lambda x: word_tokenize(x))","b9c696e5":"PS_reviews_data[['content','new_content']].head()","c675787b":"!pip install stopwords","00c09075":"from nltk.corpus import stopwords\nnltk.download('stopwords')","c86a3dd7":"stopwords.words(\"english\")[:20]","cdf82bc8":"PS_reviews_data['new_content'] = PS_reviews_data.new_content.apply(lambda x: [word for word in x if word not in stopwords.words(\"english\")])","f05a2969":"PS_reviews_data[['content','new_content']].head() # We have removed the stop words","10c6a4f0":"PS_reviews_data[PS_reviews_data['new_content'].map(lambda x: len(x)) < 2].head()","e2ad94d0":"PS_reviews_data = PS_reviews_data[PS_reviews_data['new_content'].map(lambda x: len(x)) > 1]","590ae574":"PS_reviews_data['new_content'] = PS_reviews_data['new_content'].apply(lambda x: [word for word in x if word.isalpha()])","822b1ccc":"PS_reviews_data = PS_reviews_data[PS_reviews_data['new_content'].map(lambda x: len(x)) > 1] # Repeating STEP 7","93642a42":"from nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')","2fb505d2":"PS_reviews_data['new_content'] = PS_reviews_data['new_content'].apply(lambda x: [WordNetLemmatizer().lemmatize(word) for word in x])","a959d68c":"from sklearn.feature_extraction.text import TfidfVectorizer","41e9d412":"Tfidf_vect = TfidfVectorizer()\nTrain_content_tfidf = Tfidf_vect.fit_transform(PS_reviews_data['new_content'].astype(str))","73885fcb":"# Tfidf_vect.vocabulary_ # To check vocabulary","e5fa89f6":"print(Train_content_tfidf[:3])","39598d9c":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.model_selection import StratifiedKFold,cross_val_score","37358f04":"SKF = StratifiedKFold(n_splits=5, shuffle=True)","2360f7dd":"for model in [LogisticRegression(),DecisionTreeClassifier(),RandomForestClassifier(),KNeighborsClassifier(),SVC()]:\n  model_name = str(model).split('(',1)[0]\n  print(f' Accuracy of {model_name}  : {round(cross_val_score(model,Train_content_tfidf,PS_reviews_data.new_score,cv=SKF,scoring=\"accuracy\").mean()*100,2)}%')","f8038d49":"# Data Pre-Processing\n\nML models require data to be in numeric format, since we are dealinng with textual data, we will have to clean textual data and then convert it into numeric format.\n\nBelow are the steps to clean any textual data,","231eed5b":"## 2. Converting to LOWER case","4f72ddfd":"Before data pre-processing we will drop columns that are not required.","3ab6ee98":"We will segregate the data into 2 groups for better analysis,\n```\n1. Rating from 1-2 will be negative(0)\n2. Rating from 4-5 will be positive(1)\n```\n\n","19a18afb":"## 5. WORD TOKENIZATION","5359e5bb":"## Training ML Models","80ec50f1":"## 1. Replacing\/Dropping NULL values","2e47996c":"## 6. REMOVE STOP WORDS","2a97ecdf":"## 3. REMOVE NON-ALPHA DATA(DIGITS,PUNCTUATIONS,DIACRITICS)","4e554ed6":"## 7. REMOVE RECORDS THAT HAS 0 OR 1 WORD","da093e2c":"## 8. REMOVE NON-ALPHABETS WORDS","545ea644":"## 10. TF-IDF","a852ccbd":"<img src='https:\/\/drive.google.com\/uc?export=view&id=1NJZ0j5Rj9S8GpPRs5LNnO6refNxumJHQ' height=300 >\n\nIn this notebook we will work on data preprocessing steps to supply textual data to ML model.\n\n","070b0aa3":"There is no null values in data.","790f8307":"## 9. LEMMATIZATION","54cd8ea4":"## 4. REMOVING WHITE SPACE"}}