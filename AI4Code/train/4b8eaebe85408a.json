{"cell_type":{"29f5b885":"code","5b0a0c61":"code","842c908b":"code","fd92cf0d":"code","3d9672f4":"code","1b1ec44f":"code","be43064f":"code","f7c3c705":"code","d75b0aec":"markdown","0d3c8d82":"markdown","ebe8f82d":"markdown","72979100":"markdown","3a295870":"markdown","1b265dec":"markdown","32272603":"markdown","2a83f986":"markdown","4c9579a5":"markdown"},"source":{"29f5b885":"%%capture\n!pip install transformers\n!pip install sentencepiece","5b0a0c61":"class Input():\n    \n    def __init__(self, text: str, genre: str=None,):\n      self.text = text\n      self.genre = genre    \n\nclass Target(Input):\n\n    @property\n    def summary(self):\n      return self._summary\n\n    @summary.setter\n    def summary(self, x: str):\n      self._summary = x\n      _wc = len(x.split())\n      self._wc = _wc  \n      \n    @property\n    def wc(self):\n      return self._wc\n\n    @wc.setter\n    def wc(self, x: int):\n      self._wc = x\n\nclass AbSum():\n   def __init__(self, transformer: object, version: str, tokenizer: object):     \n     self.model = transformer.from_pretrained(version)\n     self.tokenizer = tokenizer.from_pretrained(version)\n     self.version = version\n     print(\"Model & Tokenizer Downloaded\")\n\n   def generate_tokens(self, target: Target,prefix: str=''):\n      self.input_ids = self.tokenizer(prefix + target.text, return_tensors=\"tf\").input_ids\n\n   def generate_summary(self):\n      outputs = self.model.generate(self.input_ids)\n      return self.tokenizer.decode(outputs[0],skip_special_tokens=True)    \n\nclass Experiment(AbSum):\n\n   def run(self, target: Target, tokenizer_prefix: str='')->Target:\n       self.generate_tokens(target=target, prefix=tokenizer_prefix)\n       target.summary = self.generate_summary()\n       target.version = self.version\n       return target","842c908b":"import pandas as pd\nfrom pandas import DataFrame\ntry:\n    df = pd.read_csv(\"inputs.csv\")\nexcept:\n    df = pd.read_csv(\"..\/input\/exampleinputfiletransformersummarization\/inputs.csv\")\n#print(df)\ndef populate_target(x: DataFrame)->Target:\n  #print(x)\n  target = Target(text=x[\"text\"], genre=x[\"genre\"])\n  return target\n\ndef summarise(x: DataFrame, experiment: Experiment, prefix: str='')->DataFrame:\n   target = populate_target(x)\n   final = experiment.run(target = target, tokenizer_prefix=prefix)\n   x[experiment.version] = final.summary\n   x[experiment.version+\"-word-count\"] = final.wc\n   return x\n\n","fd92cf0d":"from transformers import T5Tokenizer, TFT5ForConditionalGeneration\nexperiment = Experiment(transformer=TFT5ForConditionalGeneration, version=\"t5-large\",tokenizer=T5Tokenizer)\ndf = df.apply(lambda x: summarise(x,experiment, \"summarize: \"),axis=1)","3d9672f4":"from transformers import PegasusTokenizer, TFPegasusForConditionalGeneration\nexperiment = Experiment(transformer=TFPegasusForConditionalGeneration, version=\"google\/pegasus-large\",tokenizer=PegasusTokenizer)\ndf = df.apply(lambda x: summarise(x,experiment),axis=1)","1b1ec44f":"from transformers import PegasusTokenizer, TFPegasusForConditionalGeneration\nexperiment = Experiment(transformer=TFPegasusForConditionalGeneration, version=\"human-centered-summarization\/financial-summarization-pegasus\",tokenizer=PegasusTokenizer)\ndf = df.apply(lambda x: summarise(x,experiment),axis=1)","be43064f":"from transformers import BartTokenizer, TFBartForConditionalGeneration\nexperiment = Experiment(transformer=TFBartForConditionalGeneration, version=\"facebook\/bart-large-cnn\",tokenizer=BartTokenizer)\ndf = df.apply(lambda x: summarise(x,experiment),axis=1)","f7c3c705":"print(df)\ndf.to_csv(\"outputs.csv\")","d75b0aec":"<h4> Candidate: T5<\/h4>\n\nNote: Restart the kernal is tokenizer is None","0d3c8d82":"<h2> <mark>End of Experiments<\/mark> <\/h4>","ebe8f82d":"<h2> Space for <mark>Running Experiment<\/mark> <\/h2>\n\n<h5><b>How to Run Experiments with different transformers?<\/b><\/h5>\n\n<code>from transformers import X_Tokenizers, TF_X_model <br>\nexperiment = Experiment(transformer=TF_X_model, version=\"what-ever-checkpoint\",tokenizer=X_Tokenizers)<br>\ndf = df.apply(lambda x: summarise(x,experiment, prefix_for_tokenizer|''),axis=1)<\/code>","72979100":"<h1> Helper Classes Setup <\/h1>","3a295870":"<h4> Candidate: Pegasus<\/h4>","1b265dec":"<h2> What is this script? <\/h2>\n\n<p> There are plenty of transformers that do summarization, and to evaluate how well does <br><code>transfomer(s): X(s)<\/code> perform on <code>custom dataset Y<\/code> - the following script helps.  \n\n<h2> Structure of Code <\/h2>\n\n<ul><li> class <code>Inputs<\/code> to capture various attributes of input text. If you plan to use a <code>.csv<\/code> file as an input file, then the input columns must match with the <code>Input<\/code> class attributes<\/li>\n<li> Class <code>Target<\/code> inherits class <code>Input<\/code> to save post-process level data i.e summary, word_count<\/li>\n<li> All candidate transformers are required to be instantiated via class <code>AbSum<\/code> which is inherited by class <code>Experiment<\/code>\n","32272603":"<h4> Define Input Texts <\/h4>","2a83f986":"<h1>Now let us save the dataframe<h1>","4c9579a5":"<h4>Candidate: BART<\/h4>"}}