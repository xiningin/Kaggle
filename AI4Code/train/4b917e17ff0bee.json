{"cell_type":{"277e45d8":"code","92c7b86b":"code","d2c15574":"code","2dad26e9":"code","20595953":"code","651ccd16":"code","8968a16e":"code","e4592d81":"code","852f8aa8":"code","a57f74b1":"code","b2f64748":"code","81c0d0f4":"code","c870f7a8":"code","217336ab":"code","af23a4bb":"code","a44add89":"code","d7674e27":"code","a26dc6ac":"code","15819496":"code","f5eddbb8":"code","ef01692d":"code","df1d4d0b":"code","eb9ed6cd":"code","cc9f6209":"code","6313d609":"code","f5ba7b75":"code","1f431dfa":"code","0fe5bbd1":"code","d291abbb":"code","165f1f31":"code","5543c29a":"code","d5a4de47":"code","68506265":"code","1666b411":"code","539e0167":"code","8d8d09ca":"code","6a5afdc3":"code","d64b5352":"code","34e80cbf":"code","04f70dac":"code","501a4712":"code","4c14174d":"code","62f768c4":"code","47acc7ea":"code","9d2cd990":"code","5b4a8b4f":"code","72ab2f5b":"code","790f661d":"code","07864faf":"code","d8c5cc5f":"code","e7e50cf8":"code","e55706c8":"code","31728a2b":"code","77d0f010":"code","1ff59bd4":"code","9274aff3":"code","104a2381":"code","b837975b":"markdown","1c751e8d":"markdown","73b48bfd":"markdown","a38f8b75":"markdown","c5aa7d43":"markdown","2ea9f98f":"markdown","f4cf10ec":"markdown","b9e2c09b":"markdown","f86d5130":"markdown","305ea375":"markdown","fb50d15d":"markdown","605b7e30":"markdown","032cf0df":"markdown","83d2a8f3":"markdown","77fdfa67":"markdown","41f322c2":"markdown","5a0a6abc":"markdown","a06fb6e7":"markdown","6b23f5c5":"markdown","3e696c21":"markdown","b796590d":"markdown","f109e1ea":"markdown","da91f506":"markdown","7bf20ef9":"markdown","548a9f30":"markdown","85faaff8":"markdown","f79c852f":"markdown","e752518d":"markdown","8f6cc5a3":"markdown","7ef74b3f":"markdown","7d638400":"markdown","b1848b28":"markdown","eea1601d":"markdown","fa7a2f96":"markdown","6f1ce0ad":"markdown","30d02ee5":"markdown","84f0588c":"markdown","85b8af16":"markdown","6f7a04db":"markdown","25d8a802":"markdown"},"source":{"277e45d8":"#import needed libraries\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n\nfrom nltk.corpus import stopwords\nimport string\nfrom collections import Counter \nfrom wordcloud import WordCloud,STOPWORDS\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom mlxtend.plotting import plot_confusion_matrix\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\n\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import LSTM","92c7b86b":"#define work directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d2c15574":"#read data \ndata= pd.read_csv(\"\/kaggle\/input\/twitter-airline-sentiment\/Tweets.csv\")\n","2dad26e9":"#let's see samples of the data \ndata.head()","20595953":"#data size\ndata.shape","651ccd16":"data.info()","8968a16e":"data['airline_sentiment'].value_counts()","e4592d81":"negativereason_values_count= data['negativereason'].value_counts()\nprint(negativereason_values_count)\nprint(\"all negative resons :\" ,(negativereason_values_count).sum())\n","852f8aa8":"#plot negative reasons distribution\nx =(data['negativereason']).value_counts().keys()\nx_pos = np.arange(len(x))\ny = (data['negativereason'].value_counts()).values\n\n\nbarlist = plt.bar(x_pos, y, align='center')\nplt.xticks(x_pos, x,rotation=90)\nplt.xlabel('Negative Reason')\nplt.ylabel('Count')\nplt.xlim(-1,len(x) )\nplt.ylim(0,3000)\n\nplt.show()","a57f74b1":"airline_values= data['airline'].value_counts()\nprint(airline_values)\n","b2f64748":"#plot airlines with tweets distribution\nx =(data['airline']).value_counts().keys()\nx_pos = np.arange(len(x))\ny = (data['airline'].value_counts()).values\n\n\nbarlist = plt.bar(x_pos, y, align='center')\nplt.xticks(x_pos, x,rotation=90)\nplt.xlabel('Airline')\nplt.ylabel('Count')\nplt.xlim(-1,len(x) )\nplt.ylim(0,4000)\n\nplt.show()","81c0d0f4":"#plot airlines with tweets distribution\ndata_neg = data[data['airline_sentiment']=='negative']\n\nx =(data_neg['airline']).value_counts().keys()\nx_pos = np.arange(len(x))\ny = (data_neg['airline'].value_counts()).values\n\n\nbarlist = plt.bar(x_pos, y, align='center')\nplt.xticks(x_pos, x,rotation=45)\nplt.xlabel('Airline')\nplt.ylabel('Negative Count')\nplt.xlim(-1,len(x) )\nplt.ylim(0,3000)\n\nplt.show()","c870f7a8":"#plot airlines with tweets distribution\ndata_pos = data[data['airline_sentiment']=='positive']\n\nx =(data_pos['airline']).value_counts().keys()\nx_pos = np.arange(len(x))\ny = (data_pos['airline'].value_counts()).values\n\n\n\nbarlist = plt.bar(x_pos, y, align='center')\nplt.xticks(x_pos, x,rotation=45)\nplt.xlabel('Airline')\nplt.ylabel('Positive Count')\nplt.xlim(-1,len(x) )\nplt.ylim(0,700)\n\nplt.show()","217336ab":"#plot airlines with tweets distribution\ndata_neut= data[data['airline_sentiment']=='neutral']\n\nx =(data_neut['airline']).value_counts().keys()\nx_pos = np.arange(len(x))\ny = (data_neut['airline'].value_counts()).values\n\n\n\nbarlist = plt.bar(x_pos, y, align='center')\nplt.xticks(x_pos, x,rotation=45)\nplt.xlabel('Airline')\nplt.ylabel('Neutral Count')\nplt.xlim(-1,len(x) )\nplt.ylim(0,800)\n\nplt.show()","af23a4bb":"def plotAirlineSentiment(airline):\n    data_air= data[data['airline']==airline]\n\n    x =(data_air['airline_sentiment']).value_counts().keys()\n    x_pos = np.arange(len(x))\n    y = (data_air['airline_sentiment'].value_counts()).values\n\n\n\n    barlist = plt.bar(x_pos, y, align='center')\n    plt.xticks(x_pos, x)\n    plt.xlabel('Sentiment')\n    plt.ylabel('Sentiment Count')\n    plt.xlim(-1,len(x) )\n    plt.ylim(0, 3000)\n    plt.title(airline)","a44add89":"plt.figure(1,figsize=(15, 15))\nplt.subplot(231)\nplotAirlineSentiment('United')\nplt.subplot(232)\nplotAirlineSentiment('US Airways')\nplt.subplot(233)\nplotAirlineSentiment('American')\nplt.subplot(234)\nplotAirlineSentiment('Southwest')\nplt.subplot(235)\nplotAirlineSentiment('Delta')\nplt.subplot(236)\nplotAirlineSentiment('Virgin America')","d7674e27":"def plotAirlineNegativeReason(airline):\n    data_air= data[data['airline']==airline]\n\n    x =(data_air['negativereason']).value_counts().keys()\n    x_pos = np.arange(len(x))\n    y = (data_air['negativereason'].value_counts()).values\n\n\n\n    barlist = plt.bar(x_pos, y, align='center')\n    plt.xticks(x_pos, x,rotation=90)\n    plt.xlabel('Negative Reason')\n    plt.ylabel('Tweets Count')\n    plt.xlim(-1,len(x) )\n    plt.ylim(0, 1000)\n    plt.title(airline)","a26dc6ac":"plotAirlineNegativeReason('United')\n","15819496":"#seprate each sentiment tweets \ntweets = data[\"text\"]\nneg_tweets = data[data[\"airline_sentiment\"] ==\"negative\"][\"text\"]\nneut_tweets = data[data[\"airline_sentiment\"] ==\"neutral\"][\"text\"]\npos_tweets = data[data[\"airline_sentiment\"] ==\"positive\"][\"text\"]","f5eddbb8":"stopwordslist = set(stopwords.words('english'))\nairline_names =[\"united\", \"usairways\", \"americanair\" ,\"southwestair\", \"deltaair\", \"virginamericair\" ,\"flight\"]\nallunWantedwords= list(stopwordslist) + airline_names\n","ef01692d":"# function to remove stop words, remove airline names, words starts with @ \ndef pre_processData(data):\n  \n    data =list(data)\n    new_data =[]\n    \n    \n    for l in range(len(data)):\n        line = data[l]\n        line = line.lower()\n        line=' '.join(word for word in line.split() if not word.startswith('@'))\n        querywords = line.split()  \n        resultwords  = [word for word in querywords if word not in stopwordslist]\n        resultwords2  = [word for word in resultwords if word not in airline_names]\n\n        newline = ' '.join(resultwords2)       \n        newline = newline.translate(str.maketrans('', '', string.punctuation))\n        new_data.append(newline)\n        \n    return new_data","df1d4d0b":"#pre_process tweets from the three categories\nneg_tweets= pre_processData(neg_tweets)\nneut_tweets= pre_processData(neut_tweets)\npos_tweets= pre_processData(pos_tweets)","eb9ed6cd":"#function to find some statistics about tweets texts\ndef tweetStat(tweetsList):\n    min_len= min(len(x) for x in tweetsList) \n    max_len= max(len(x) for x in tweetsList) \n    avg_len= sum(len(x) for x in tweetsList) \/ len(tweetsList)\n\n    return min_len, max_len, avg_len","cc9f6209":"print(\"Negative tweets stats (min, max, avg): \", tweetStat(neg_tweets))\nprint(\"Neutral tweets stats (min, max, avg): \", tweetStat(neut_tweets))\nprint(\"Positive tweets stats (min, max, avg): \", tweetStat(pos_tweets))","6313d609":"#function to get most k frquent words in a set of tweets\ndef getFrequentWords(textList, k):  #k frequent words\n    texts = ' '.join(textList)\n    split_it = texts.split() \n    count = Counter(split_it) \n    most_occur = count.most_common(k) \n\n    print (most_occur)\n    return most_occur","f5ba7b75":"print(\"Most frquent 10 words in negative tweets are:\" )\nnegwords = getFrequentWords(neg_tweets,10)","1f431dfa":"print(\"Most frquent 10 words in neutral tweets are:\" )\nneut_words =getFrequentWords(neut_tweets,10)","0fe5bbd1":"print(\"Most frquent 10 words in positive tweets are:\" )\npos_words= getFrequentWords(pos_tweets,10)","d291abbb":"#function to get wordcloud \ndef getWordCloud(texts):\n    texts =\" \".join(texts)\n    wordcloud = WordCloud(stopwords=allunWantedwords,\n                          background_color='white',\n                          width=2000,\n                          height=2000\n                         ).generate(texts)\n    \n    return wordcloud","165f1f31":"#negtaive tweets word cloud \nneg_wordcloud = getWordCloud(neg_tweets)\nplt.figure(1,figsize=(7, 7))\nplt.imshow(neg_wordcloud)\nplt.axis('off')\nplt.show()\n","5543c29a":"#neutral tweets word cloud \nneut_wordcloud = getWordCloud(neut_tweets)\nplt.figure(2,figsize=(7, 7))\nplt.imshow(neut_wordcloud)\nplt.axis('off')\nplt.show()","d5a4de47":"#positive tweets word cloud \npos_wordcloud = getWordCloud(pos_tweets)\nplt.figure(2,figsize=(7, 7))\nplt.imshow(pos_wordcloud)\nplt.axis('off')\nplt.show()","68506265":"#store the required two columns in a new data frame \ncleanedTweets= pre_processData(data[\"text\"])  # apply preprocessing step \nfinalData= pd.DataFrame()\nfinalData['text']= cleanedTweets\nfinalData['airline_sentiment']= data['airline_sentiment']\nfinalData.head()","1666b411":"#convert airline_sentiment values (negative, neutral, and positive) to numerical values (0,1,and 2) and map the data to it\nsentiments = data['airline_sentiment'].astype('category').cat.categories.tolist()\nreplace_map_comp = {'airline_sentiment' : {k: v for k,v in zip(sentiments,list(range(0,len(sentiments))))}}\nprint(replace_map_comp)\n\nfinalData.replace(replace_map_comp, inplace=True)\nfinalData.head()","539e0167":"# store tweets text in x & the target label in y\nx=  finalData['text'].values\ny= finalData['airline_sentiment'].values","8d8d09ca":"#split data into training & test with 15%\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.15, random_state=42)\n\nprint(\"x_train:\" ,x_train.shape, \", y_train: \", len(y_train))\nprint(\"x_test: \",x_test.shape, \", y_test: \", len(y_test))\n","6a5afdc3":"v = CountVectorizer(analyzer = \"word\")\ntrain_features= v.fit_transform(x_train)\ntest_features=v.transform(x_test)","d64b5352":"#convert from sparse (contain a lot of zeros) to dense\nfinal_train_features=train_features.toarray()\nfinal_test_features= test_features.toarray()\nprint(final_train_features.shape)\nprint(final_test_features.shape)","34e80cbf":"print('training model (this could take sometime)...')\nclf = LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=200)\nclf.fit(final_train_features, y_train)\n\nprint('calculating results...')\npredictions_train = clf.predict(final_train_features)\npredictions_test = clf.predict(final_test_features)\n\naccuracy = accuracy_score(predictions_train,y_train)\nprint(\" Logistic Regression Train accuracy is: {:.4f}\".format(accuracy))\n\naccuracy = accuracy_score(predictions_test,y_test)\nprint(\" Logistic Regression Test accuracy is: {:.4f}\".format(accuracy))","04f70dac":"#print other performance measures, espically the data is unbalanced\nprint(classification_report(predictions_test , y_test))\n","501a4712":"#calculate the confusion matrix and plot it\n\ncm=confusion_matrix(predictions_test , y_test)\nclass_names =  ['Negative', 'Neutral', 'Positive']\nfig, ax = plot_confusion_matrix(conf_mat=cm,\n                                colorbar=True,\n                                show_absolute=True,\n                                show_normed=True,\n                                class_names=class_names)\nplt.show()","4c14174d":"print('training model (this could take sometime)...')\nclf =     RandomForestClassifier(n_estimators=10)\nclf.fit(final_train_features, y_train)\n\nprint('calculating results...')\npredictions_train = clf.predict(final_train_features)\npredictions_test = clf.predict(final_test_features)\n\naccuracy = accuracy_score(predictions_train,y_train)\nprint(\"Random Forest Train accuracy is: {:.4f}\".format(accuracy))\n\naccuracy = accuracy_score(predictions_test,y_test)\nprint(\"Random Forest Test accuracy is: {:.4f}\".format(accuracy))\n","62f768c4":"#print other performance measures, espically the data is unbalanced\nprint(classification_report(predictions_test , y_test))","47acc7ea":"#calculate the confusion matrix and plot it\n\ncm=confusion_matrix(predictions_test , y_test)\nclass_names =  ['Negative', 'Neutral', 'Positive']\nfig, ax = plot_confusion_matrix(conf_mat=cm,\n                                colorbar=True,\n                                show_absolute=True,\n                                show_normed=True,\n                                class_names=class_names)\nplt.show()","9d2cd990":"tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\ntfidf_vect.fit(x_train)\ntrain_features =  tfidf_vect.transform(x_train)\ntest_features =  tfidf_vect.transform(x_test)\n","5b4a8b4f":"#convert from sparse (contain a lot of zeros) to dense\nfinal_train_features=train_features.toarray()\nfinal_test_features= test_features.toarray()","72ab2f5b":"print('training model (this could take sometime)...')\nclf = LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=200)\nclf.fit(final_train_features, y_train)\n\nprint('calculating results...')\n\npredictions_train = clf.predict(final_train_features)\npredictions_test = clf.predict(final_test_features)\n\naccuracy = accuracy_score(predictions_train,y_train)\nprint(\"Logisitc regression Train accuracy is: {:.4f}\".format(accuracy))\n\naccuracy = accuracy_score(predictions_test,y_test)\nprint(\"Logisitc regression Test accuracy is: {:.4f}\".format(accuracy))\n\n","790f661d":"from sklearn.ensemble import RandomForestClassifier\n\nprint('training model (this could take sometime)...')\nclf = RandomForestClassifier(n_estimators=10)\nclf.fit(final_train_features, y_train)\n\nprint('calculating results...')\npredictions_train = clf.predict(final_train_features)\npredictions_test = clf.predict(final_test_features)\n\naccuracy = accuracy_score(predictions_train,y_train)\nprint(\"Random forest Train accuracy is: {:.4f}\".format(accuracy))\n\naccuracy = accuracy_score(predictions_test,y_test)\nprint(\"Random forest Test accuracy is: {:.4f}\".format(accuracy))","07864faf":"embeddings_index = {}\nf = open('..\/input\/glove840b300dtxt\/glove.840B.300d.txt')\n\nfor line in f:\n    values = line.split(' ')\n    word = values[0] ## The first entry is the word\n    coefs = np.asarray(values[1:], dtype='float32') \n    embeddings_index[word] = coefs\nf.close()\n\nprint('GloVe data loaded')\nprint('Loaded %s word vectors.' % len(embeddings_index))","d8c5cc5f":"#encode train texts and test texts using the a tokenizer\nMAX_NUM_WORDS = 1000\nMAX_SEQUENCE_LENGTH = 135 #from the stats we found previously\ntokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\ntokenizer.fit_on_texts(x)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\nsequences_train = tokenizer.texts_to_sequences(x_train)\nx_train_seq = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n\nsequences_test = tokenizer.texts_to_sequences(x_test)\nx_test_seq = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n\n#convert labels to one hot vectors\nlabels_train = to_categorical(np.asarray(y_train))\nlabels_test = to_categorical(np.asarray(y_test))\n\nprint(\"train data :\")\nprint(x_train_seq.shape)\nprint(labels_train.shape)\n\nprint(\"test data :\")\nprint(x_test_seq.shape)\nprint(labels_test.shape)","e7e50cf8":"# Find number of unique words in our tweets\nvocab_size = len(word_index) + 1 # +1 is for UNKNOWN words","e55706c8":"# Define size of embedding matrix: number of unique words x embedding dim (300)\nembedding_matrix = np.zeros((vocab_size, 300))\n\n# fill in matrix\nfor word, i in word_index.items():  # dictionary\n    embedding_vector = embeddings_index.get(word) # gets embedded vector of word from GloVe\n    if embedding_vector is not None:\n        # add to matrix\n        embedding_matrix[i] = embedding_vector # each row of matrix","31728a2b":"#DL model: pass the encoded data to an embedding layer and use the Glove pre_trained weights, then pass the \n# output to an LSTM layer follwed by 2 dense layers.\n# the optimizer used is Adam, since it achivied higher accurcies usually.\n\ncell_size= 256\ndeepLModel1 = Sequential()\nembedding_layer = Embedding(input_dim=vocab_size, output_dim=300, weights=[embedding_matrix],\n                           input_length = MAX_SEQUENCE_LENGTH, trainable=False)\ndeepLModel1.add(embedding_layer)\ndeepLModel1.add(LSTM(cell_size, dropout = 0.2))\ndeepLModel1.add(Dense(64,activation='relu'))\ndeepLModel1.add(Flatten())\ndeepLModel1.add(Dense(3, activation='softmax'))\ndeepLModel1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\ndeepLModel1.summary()","77d0f010":"#train the model\ndeepLModel1_history = deepLModel1.fit(x_train_seq, labels_train, validation_split = 0.15,\n                    epochs=100, batch_size=256)","1ff59bd4":"# Find train and test accuracy\nloss, accuracy = deepLModel1.evaluate(x_train_seq, labels_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\n\nloss, accuracy = deepLModel1.evaluate(x_test_seq, labels_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))","9274aff3":"predictions_test = deepLModel1.predict_classes(x_test_seq)\n#print other performance measures, espically the data is unbalanced\nprint(classification_report(predictions_test , y_test))","104a2381":"#calculate the confusion matrix and plot it\n\ncm=confusion_matrix(predictions_test , y_test)\nclass_names =  ['Negative', 'Neutral', 'Positive']\nfig, ax = plot_confusion_matrix(conf_mat=cm,\n                                colorbar=True,\n                                show_absolute=True,\n                                show_normed=True,\n                                class_names=class_names)\nplt.show()","b837975b":"If we want to visualize the negative reasons for each airline, the following function helps us. For example for 'United' airline, the most mentioned reason for the negative tweets is customer service!","1c751e8d":"**Notice that it's possible to achieve a higher performance with more parameter-tuning.**","73b48bfd":"Let's visualize it to be more clear","a38f8b75":"To sum up, let's see the distribution of the three sentiments among each airline separately as the following figures show. That in general, each airline has most of the tweets negative, then natural and the least are the positive tweets.","c5aa7d43":"## B. Deep Learning Model","2ea9f98f":"What about positive tweets for each airline?! The figure below shows that it's not necessary the airline with the most negative tweets to have the lowest positive tweets.","f4cf10ec":"To understand tweets texts,pre_processData function is created, which removes stop words, airline names, and any word starting with the symbol @\nThe reason for removing airline names is, regardless of the sentiment, the user who wrote the tweets will tag or mention the airline official profile, in addition, to remove any words starts with @ which include any tag to other twitter accounts.","b9e2c09b":"From the following cells, it's clear that on average the negative tweets are longer than positive and neutral tweets","f86d5130":"There are different deep learning (DL) techniques that can be applied to this data such as Recurrent neural network (RNN) and convolutional neural networks (CNN). \n\nIn this kernel I will use Long Short Term Memory networks (LSTM), which is a special kind of RNN; since RNN are mostly used with texts (as a sequence data); later will add more DL techniques.","305ea375":"First step is to choose the method of presenting texts: so I will use Glove embedding which is an unsupervised machine learning algorithm for getting vector representations for words. Each word is mapped to a 300-dimension vector to learn the semantics of words.","fb50d15d":"Using CountVectorizer for encoding tweets texts and the Random Forest algorithm as a prediction model for tweet sentiment achieved 76% accuracy. As the confusion matrix shows, most errors are in predicting 'neutral' as 'negative', 'neutral' as 'negative' and 'negative'  as 'positive'. Most errors in classifying 'negative' tweets, but don't forget that the number of tweets with negative sentiment is the largest in the actual data.","605b7e30":"So, using CountVectorizer for encoding tweets texts and the Logistic regression algorithm as a prediction model for tweet sentiment achieved 79% accuracy. As the confusion matrix shows, most errors are in predicting 'neutral' as 'negative' followed by predicting 'neutral' as 'positive' and vice-versa. This is logical since some times neutral words combine both words of negativity and positivity.","032cf0df":"To discover the frequent words in each tweets category in a more interesting way,\nthe following cell shows the cloud word for each category, where the most frequent word appears in large size.","83d2a8f3":"\nTwitter US Airline Sentiment scraped from February of 2015 from Crowdflower\u2019s Data for Everyone library. Tweets are classified as either \u201cpositive\u201d, \u201cneutral\u201d, or \u201cnegative\u201d. Other information are provided such as the reason for a negative classification and to which airline.\n\u200b\nThis notebook consists of three parts as following:\n1. Explore data part\n2. Feature engineering part\n3. Models Part\n","77fdfa67":"The aim of the previous part content is to understand the data and take an insight into each feature. But to build a sentiment prediction model, we will use the tweet text to determine that. So, all the features are dropped except the tweet text. So, our focus on tweet text and the target label (sentiment).\n","41f322c2":"### Overall Summary","5a0a6abc":"* Best achieved test accuracy among the used techniques is 80% with logistic regression mode either with CountVectorizer & TfidfVectorizer. \n* ML models, especially logistic regression, outperform the DL model used. This is may due to the DL model needs more training or more data.\n* In all models, the overfitting issue clearly appeared, where training accuracy reaches 95%-98% while test accuracy is 77%-80%.\n* In all models, the number of tweets classified correctly is larger for negative tweets, this is logical since more half the data is for negative tweets. While the 'neutral' category is the least classified correctly, this might goes to neutral tweets use words between negativity and positivity so cause confusion.\n\n**Whats next?**\n* Study false predicted tweets, and reprocess data based on that.\n* Try other ML models: Decision tree, SVM, Naive Bayes, and KNN.\n* Try grid search CV technique to search for the best parameter for each ML model.\n* Try ensemble methods: voting, adaboost, and gradient boost.\n* Try another pre-trained embedding.\n* Try more DL techniques such as CNN.\n\n","a06fb6e7":"We know that the sentiment is from three options (positive, neutral, and negative). In the next cell, we explore the distribution of them among the data. It's clear that the \"negative\" sentiment tweets are the dominant of the data. But why?! This may due to the nature of the human to complain when there is an issue, but keeps calm when everything going fine.","6b23f5c5":"Using info command we can explore each feature data type, and if there is any missing data ","3e696c21":"## Feature Engineering ","b796590d":"To build a machine learning (ML) model, we can't deal with words directly, so it's necessary to encode text words (i.e. convert them to numbers).\nThere are different methods for that, from them: CountVectorizer & TfidfVectorizer.\nWe will use the two methods with different ML models.\n\nThere are different ML algorithms, in this kernel, we use Logistic regression, Random forest, and SVM. \nLater will add Naive Bayes, decision tree, KNN, and ensemble classifiers, such as Adaboost and voting classifier.","f109e1ea":"From the cell below, it shows the most frequent words in negative tweets.\nWords such as canceled, time, service, help, etc. are clearly connected with complaints.","da91f506":"#### 1.1 - CountVectorizer with Logistic regression model","7bf20ef9":"### Explore Data","548a9f30":"There is a feature called \"negative reason\" when the sentiment is negative, and NAN for other sentiments. Let's discover the reasons which led them to give negative reasons. From the cell below, there are 10 reasons, most of them complained from the customer service, later flight, and can't tell! The distribution of the different reasons are shown in the figure below; sorted descending.\n\nNote: summation of all negative reasons equals the # of negative sentiments, so each negative sentiment has a reason; no missing data.","85faaff8":"Results of using logistic regression model with CountVectorizer & TfidfVectorizer are nearly the same . The same applies when using Random forest algorithm with the two encoding methods.\n\n**Notice that it's possible to achieve a higher performance with more parameter-tuning.\n**\n","f79c852f":"#### 2- Using TfidfVectorizer\n Using TF-IDF method which assigns scores to each word based on its frequency, that the more frequent word got higher weights.\n","e752518d":"#### 1.2 - CountVectorizer with Random Forest model","8f6cc5a3":"\n#### 2.1 - TfidfVectorizer with Logistic regression model","7ef74b3f":"# Prediction Models","7d638400":"## A. Machine Learning Models","b1848b28":"#### 1- using CountVectorizer\n The idea of CountVectorizer is to tokenize texts and build a vocabulary of known words to encode texts with these vocabularies; An integer count for the number of times each word appeared in the document.\n","eea1601d":"#### 2.2 - TfidfVectorizer with Random Forest model\n","fa7a2f96":"All the tweets are for US airlines, let's explore these airlines firstly. \nThe following cell shows that there are 6 airlines, and the number of tweets for each.","6f1ce0ad":"\nThere are 14,640 rows and 15 columns. Features included are: tweet id, airline_sentiment, sentiment_confidence score, negativereason, negativereason_confidence, airline, airline_sentiment_gold, name, negativereason_gold, retweet_count, tweet_text, tweet_coord, tweet_created, tweet_location, and tweet_timezone.\n","30d02ee5":"From the cell below, it shows the most frequent words in neutral tweets.\nWords such as please, need, thanks are kind of neutral compared with words used in positive tweets as will be shown next cell.","84f0588c":"Now for the most important part of the data which is tweets texts, let's discover if there is any something interesting about it.","85b8af16":"From the cell below, it shows the most frequent words in positive tweets.\nWords such as thanks, great, love, etc. are clearly connected with satisfaction.","6f7a04db":"For neutral tweets, the same applies here as positive tweets, there is no direct relationship between the airline and number of neural tweets with the number of negative and positive tweets.","25d8a802":"If we want to know which airline got a negative sentiment. From the figure below, it's clear that the airline with most tweets is the same airline with the most negative tweets."}}