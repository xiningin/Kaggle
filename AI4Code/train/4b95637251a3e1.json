{"cell_type":{"4296bee0":"code","6ea9ad29":"code","5100b34b":"code","e247c0f3":"code","64123a17":"code","f0384108":"code","b374ced6":"code","31f8ac69":"code","6e78a570":"code","057a0ec6":"code","339b522c":"code","f3428ca7":"code","208486f8":"code","42920741":"code","02303d75":"markdown","5bec5cc8":"markdown","85aff525":"markdown","d8d54bcd":"markdown","d9ac61ff":"markdown","9455b839":"markdown","8e20cf95":"markdown","2601189e":"markdown","230a56bf":"markdown","33367eec":"markdown"},"source":{"4296bee0":"# This is where I will keep all Inported sources. As in all notebooks it is easier to keep all calls and globals in one location.\nimport pandas as pd\nimport numpy as np\nimport keras\nimport matplotlib.pyplot as plt\nimport itertools\n\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MaxAbsScaler, MinMaxScaler\nfrom sklearn.metrics import confusion_matrix, accuracy_score","6ea9ad29":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nHeart_Failure_Data = pd.read_csv(\"\/kaggle\/input\/heart-failure-prediction\/heart.csv\")","5100b34b":"Heart_Failure_Data.head()","e247c0f3":"Heart_Failure_Data.count()","64123a17":"Heart_Failure_Data.nunique()","f0384108":"Heart_Failure_Data.shape, Heart_Failure_Data.dtypes","b374ced6":"Heart_Failure_Data.select_dtypes('object').nunique()","31f8ac69":"Heart_Failure_Data_Numbers = pd.get_dummies(data=Heart_Failure_Data, columns = [\"Sex\", \"ChestPainType\", \"RestingECG\", \"ExerciseAngina\", \"ST_Slope\"])","6e78a570":"Heart_Failure_Data_Numbers.head()","057a0ec6":"Heart_Failure_Data_Numbers.shape, Heart_Failure_Data_Numbers.dtypes","339b522c":"scaler = MinMaxScaler()\nscaler.fit(Heart_Failure_Data_Numbers)\nscaled = scaler.transform(Heart_Failure_Data_Numbers)\nscaled_Heart_Failure_Data_Numbers = pd.DataFrame(scaled, columns=Heart_Failure_Data_Numbers.columns)\n\nscaled_Heart_Failure_Data_Numbers.head()\n\nX = scaled_Heart_Failure_Data_Numbers.drop(\"HeartDisease\", axis = 1)\ny = scaled_Heart_Failure_Data_Numbers[\"HeartDisease\"]\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=100)","f3428ca7":"model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n    filepath='weights.{epoch:02d}-{val_loss:.2f}.h5',\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True)","208486f8":"tf.random.set_seed(42)\n\nmodel = Sequential([\n    Flatten(input_shape=(20,)),\n    Dense(700, activation = 'relu'),   \n    Dense(700, activation = 'relu'),\n    Dense(200, activation = 'relu'),\n    Dense(100, activation = 'relu'),\n    Dense(1, activation = 'sigmoid')\n])\n\nmodel.compile(loss='binary_crossentropy',\n                optimizer='sgd',\n                metrics=['accuracy'])\n\n\n#model.save_weights('model.h5')\n\nhistory = model.fit(X_train, y_train,\n                    validation_data=(X_test, y_test),\n                    batch_size=32, \n                    epochs=10,\n                    callbacks=[model_checkpoint_callback],\n                    verbose=2)\n\nmodel.evaluate(X_test, y_test)\n\n#model.load_weights('model.h5')\nhistory\n\nplt.plot(range(len(history.history['loss'])), history.history['loss'], label='training_loss')\nplt.plot(range(len(history.history['loss'])), history.history['val_loss'], label='val_loss')\nplt.title('Loss')\nplt.xlabel('Epochs')\nplt.legend()\n\nplt.figure()\nplt.plot(range(len(history.history['loss'])), history.history['accuracy'], label='training_accuracy')\nplt.plot(range(len(history.history['loss'])), history.history['val_accuracy'], label='val_accuracy')\nplt.title('Accuracy')\nplt.xlabel('Epochs')\nplt.legend();","42920741":"y_preds = model.predict(X_test)\n\ncm = confusion_matrix(y_test, tf.round(y_preds))\nprint(cm)\naccuracy_score(y_test, tf.round(y_preds))","02303d75":"This is a binary classification. Meaning that our model is predicting one out of two values for the dependent variable. We are going to use a keras sequential model made of only densely connected layers. The output layer of this model is a single node that uses a sigmoid curve to compress all values to between 0 and 1. These values are not the integer form that we will need for analysis later. We will round these percentages to be either 0 or 1 after. The model it self will be made of dense layers. This means that every output is connected to every input of the next layer. The layers use relu activation. This is rectified linear units. This is a half retified function. Meaning that any negative inputs will be set to 0 while any positive values will be left alone. \n\nThe model uses the sgd optimizer for compiling. This is a momentum-based optimizer. The role of the optimizer is to implement a gradient descent learning routine. In this case is uses stochastic gradient decent. This supports different loss functions and penalties for classification. This optimizer fit the shape of our data better. We used an array X (num_samples, num_features) and array y (num_samples)","5bec5cc8":"This shows us that the columns that are objects and not numbers only have a few unique values. We can change these in a few ways. We can give convert each column into numerical values for those number of objects. Such as for ChestPainType we can use the number 0,1,2,3 for each of the 4 values. This does work well, though sometimes it is more accurate to seperate and create new columns for each unique object. Use an encoder to do this. The best thing to do is keep things simple. pandas get_dummies will do just fine.","85aff525":"From above it is shown that the data cantains 918 entries for each column. Meaning we have no missing data. The number of unique values per category is large for some. Cholesterol and MaxHR have a number of values. This can be handled in different ways. We can group them into range based groups. I will leave the dataset mostly untouched to see what the outcome will be. ","d8d54bcd":"## Attribute Information in the provided dataset","d9ac61ff":"# Heart Failure Predictive Model Using Deep Learning  (89.13% accuracy)","9455b839":"Heart failure is a chronic progressive disease. It refers to the hearts decreased ability to to pump blood as well as it should. To live the higest quality of life that an individual can one needs to take care of themself. Heart failure is the leading cause of hospitalization in people over the age of 65.\n\nThere are some tests that can be used to mesure if and how severe an individuals heart failure might be. We are provided with a dataset that gives a list of 11 attributes of an individual and then a binary classification of if they suffer from heart failure related issues. We are unable to rate the level of severity, but we can try afterwards and see what we can find out.","8e20cf95":"The test accuracy is a little above 89%. This was achieved will minimal data preprocessing and a deep learning modele that had 6 layers. With moderate effort we can increase that by a few percentages. I will look into this at a later time. For now we can see that the accuracy of the model is sufficent for the complexity of it.","2601189e":"These are the attributes listed in the column order of the data set.\n\n\n1 Age: age of the patient [years]\n\n2 Sex: sex of the patient [M: Male, F: Female]\n\n3 ChestPainType: chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY:  ymptomatic]\n\n4 RestingBP: resting blood pressure [mm Hg]\n\n5 Cholesterol: serum cholesterol [mm\/dl]\n\n6 FastingBS: fasting blood sugar [1: if FastingBS > 120 mg\/dl, 0: otherwise]\n\n7 RestingECG: resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n\n8 MaxHR: maximum heart rate achieved [Numeric value between 60 and 202]\n\n9 ExerciseAngina: exercise-induced angina [Y: Yes, N: No]\n\n10 Oldpeak: oldpeak = ST [Numeric value measured in depression]\n\n11 ST_Slope: the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n\n12 HeartDisease: output class [1: heart disease, 0: Normal]","230a56bf":"This encoder expanded our dataset to 21 columns. We now have 20 independent variables. All are now in numerical formate. The data can be cleaned further but this is to keep it simple. No more changes will be made outside of normalization. This is to scale the values but the max value in each respective column. We can scale, seperate our dependent variable to its own and split the data into training an testing values. ","33367eec":"The attributes listed at the start of this book are shown here. We have 11 independent variables and 1 dependent. The goal is to be able to predict the value of the HeartDisease column for new candidates. Some variables are strings and will need to be coverted. We can check for missing values, number of unique values and get a better sense of the grouping of our data. "}}