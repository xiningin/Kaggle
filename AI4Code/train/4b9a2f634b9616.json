{"cell_type":{"26b3fe23":"code","ac86792c":"code","94efc204":"code","7362a95e":"code","57347ee2":"code","65c63510":"code","7f610ff8":"code","b7db0b76":"code","4c3a75dd":"code","3d929193":"code","0e958ace":"code","16440734":"code","72b4b625":"code","838f9c9d":"code","2e1d1158":"code","ab210c44":"code","c16bb470":"code","3526f159":"code","1f6b7554":"code","688bf1fd":"code","9e3ab12a":"code","4ead5ba3":"markdown","53c49097":"markdown","2485dd99":"markdown","aa4948ea":"markdown","a8d6ba36":"markdown","7b279852":"markdown","3c3def3c":"markdown","18c2a512":"markdown","d223190b":"markdown","1e7939c3":"markdown","3350b013":"markdown","bcf7cd86":"markdown","bf523276":"markdown","1e3a6587":"markdown","15df00d1":"markdown"},"source":{"26b3fe23":"from tensorflow.compat.v1 import ConfigProto\nfrom tensorflow.compat.v1 import InteractiveSession\n\nconfig = ConfigProto()\nconfig.gpu_options.allow_growth = True \nsession = InteractiveSession(config=config)","ac86792c":"import os\nimport cv2 \nimport sys \nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd  \nfrom time import time \nimport matplotlib.pyplot as plt \nfrom tqdm import tqdm \nfrom itertools import chain \nfrom skimage.io import imread, imshow, imread_collection, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.morphology import label \n\nimport tensorflow as tf \nfrom tensorflow.keras.layers import Input, Dropout, Lambda, Conv2D, Conv2DTranspose, MaxPooling2D, Concatenate, Activation, Add, multiply, add, concatenate, LeakyReLU, ZeroPadding2D, UpSampling2D, BatchNormalization\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom tensorflow.keras import backend as K \n\nfrom sklearn.metrics import classification_report, confusion_matrix\n%matplotlib inline \n\nIMG_HEIGHT = 128\nIMG_WIDTH = 128\nIMG_CHANNEL = 3\nIMG_PATH = '..\/input\/pneumothorax-chest-xray-images-and-masks\/siim-acr-pneumothorax\/png_images'\nMASK_PATH = '..\/input\/pneumothorax-chest-xray-images-and-masks\/siim-acr-pneumothorax\/png_masks'\n\nwarnings.filterwarnings('ignore', category=UserWarning, module='skimage')\nseed = 42 \nrandom.seed = seed\nnp.random.seed = seed","94efc204":"train_df = pd.read_csv('..\/input\/pneumothorax-chest-xray-images-and-masks\/siim-acr-pneumothorax\/stage_1_train_images.csv')\ntest_df = pd.read_csv('..\/input\/pneumothorax-chest-xray-images-and-masks\/siim-acr-pneumothorax\/stage_1_test_images.csv')","7362a95e":"train_df.head()","57347ee2":"train_df_pneumo = train_df[train_df['has_pneumo'] == 1]","65c63510":"print(\"Getting and Resizing Train Images, Train Mask, and Adding Label ...\\n\\n\")\n\n# Create X_train, Y_train, and Label\nX_train_seg = np.zeros((len(train_df_pneumo), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNEL), dtype=np.uint8)\nY_train_seg = np.zeros((len(train_df_pneumo), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\nLabel_seg = np.zeros(len(train_df_pneumo), dtype=np.uint8)\n\nimg_data = list(train_df_pneumo.T.to_dict().values())\n\nfor i, data_row in tqdm(enumerate(img_data), total=len(img_data)):\n    \n    patientImage = data_row['new_filename']\n    imageLabel  = data_row['has_pneumo']\n\n    imagePath = os.path.join(IMG_PATH, patientImage)\n    lungImage = imread(imagePath)\n    lungImage = np.expand_dims(resize(lungImage, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True), axis=-1)\n    \n    X_train_seg[i] = lungImage\n\n    Label_seg[i] = imageLabel\n\n    maskPath = os.path.join(MASK_PATH, patientImage)\n    maskImage = imread(maskPath)\n    maskImage = np.expand_dims(resize(maskImage, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True), axis=-1)\n\n    Y_train_seg[i] = maskImage\n\nprint('\\n\\nProcess ... C O M P L E T E')","7f610ff8":"# Illustrate the train images and masks\nplt.figure(figsize=(20, 16))\nx, y = 12, 4\nfor i in range(y):\n    for j in range(x):\n        plt.subplot(y*2, x, i*2*x+j+1)\n        pos = i*120 + j*10\n        plt.imshow(X_train_seg[pos], cmap=plt.cm.bone)\n        plt.title('Image #{}'.format(pos))\n        plt.axis('off')\n        plt.subplot(y*2, x, (i*2+1)*x+j+1)\n\n        plt.imshow(np.squeeze(Y_train_seg[pos]), cmap='gray_r')\n        plt.title('Mask #{}\\nLabel: {}'.format(pos, Label_seg[pos]))\n        plt.axis('off')\n\nplt.tight_layout()\nplt.show()","b7db0b76":"def iou_score(y_pred, y_true, smooth=1):\n    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n    union = K.sum(y_true, -1) + K.sum(y_pred, -1) - intersection\n    iou = (intersection + smooth)\/(union + smooth)\n    return iou\n\ndef dice_coef(y_true, y_pred, smooth=1):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) \/ (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\n\ndef dice_coef_loss(y_true, y_pred):\n    return 1 - dice_coef(y_true, y_pred)","4c3a75dd":"def AttnBlock2D(x, g, inter_channel, data_format='channels_first'):\n\n    theta_x = Conv2D(inter_channel, [1, 1], strides=[1, 1], data_format=data_format)(x)\n\n    phi_g = Conv2D(inter_channel, [1, 1], strides=[1, 1], data_format=data_format)(g)\n\n    f = Activation('relu')(add([theta_x, phi_g]))\n\n    psi_f = Conv2D(1, [1, 1], strides=[1, 1], data_format=data_format)(f)\n\n    rate = Activation('sigmoid')(psi_f)\n\n    att_x = multiply([x, rate])\n\n    return att_x\n\n\ndef attention_up_and_concate(down_layer, layer, data_format='channels_first'):\n    \n    if data_format == 'channels_first':\n        in_channel = down_layer.get_shape().as_list()[1]\n    else:\n        in_channel = down_layer.get_shape().as_list()[3]\n    \n    up = UpSampling2D(size=(2, 2), data_format=data_format)(down_layer)\n    layer = AttnBlock2D(x=layer, g=up, inter_channel=in_channel \/\/ 4, data_format=data_format)\n\n    if data_format == 'channels_first':\n        my_concat = Lambda(lambda x: K.concatenate([x[0], x[1]], axis=1))\n    else:\n        my_concat = Lambda(lambda x: K.concatenate([x[0], x[3]], axis=3))\n    \n    concate = my_concat([up, layer])\n    return concate","3d929193":"# Attention U-Net \ndef att_unet(img_w, img_h, n_label, data_format='channels_first'):\n    inputs = Input((IMG_CHANNEL, img_w, img_h))\n    x = inputs\n    depth = 4\n    features = 32\n    skips = []\n    for i in range(depth):\n\n        # ENCODER\n        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n        x = Dropout(0.2)(x)\n        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n        skips.append(x)\n        x = MaxPooling2D((2, 2), data_format='channels_first')(x)\n        features = features * 2\n\n    # BOTTLENECK\n    x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n    x = Dropout(0.2)(x)\n    x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n\n    # DECODER\n    for i in reversed(range(depth)):\n        features = features \/\/ 2\n        x = attention_up_and_concate(x, skips[i], data_format=data_format)\n        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n        x = Dropout(0.2)(x)\n        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n    \n    conv6 = Conv2D(n_label, (1, 1), padding='same', data_format=data_format)(x)\n    conv7 = Activation('sigmoid')(conv6)\n    \n    model = Model(inputs=inputs, outputs=conv7)\n\n    return model","0e958ace":"model = att_unet(IMG_WIDTH, IMG_HEIGHT, n_label=1)","16440734":"model.summary()","72b4b625":"model.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=[dice_coef_loss, iou_score]\n)","838f9c9d":"X_train_seg = np.rollaxis(X_train_seg, 3, 1)\nY_train_seg = np.rollaxis(Y_train_seg, 3, 1)\nprint(X_train_seg.shape)\nprint(Y_train_seg.shape)","2e1d1158":"early_stop = EarlyStopping(\n    monitor='val_loss',\n    min_delta=0,\n    patience=5,\n    restore_best_weights=True,\n    verbose=1\n)\n\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.25, min_delta=1e-10 ,patience=3, \n                                   verbose=1, mode='auto')\n\ncallbacks = [reduceLROnPlat]","ab210c44":"epochs = 80\nmodel.fit(X_train_seg, Y_train_seg, validation_split=0.1, batch_size=16, epochs=epochs, callbacks=callbacks)","c16bb470":"X_train_seg = np.rollaxis(X_train_seg, 3, 1)\nY_train_seg = np.rollaxis(Y_train_seg, 3, 1)\nprint(X_train_seg.shape)\nprint(Y_train_seg.shape)","3526f159":"# The first 90% used for training\npred_train = model.predict(X_train_seg[:int(X_train_seg.shape[0]*0.9)].astype(np.float16), verbose=1)\n# The last 10% used for validation\npred_val = model.predict(X_train_seg[int(X_train_seg.shape[0]*0.9):].astype(np.float16), verbose=1)\n\n# pred_test = model.predict(X_test, verbose=1)\n\n# Thresholds prediction\npred_train_threshold = (pred_train > 0.40).astype(np.float16)\npred_val_threshold = (pred_val > 0.40).astype(np.float16)","1f6b7554":"# CHANGE TO CHANNEL LAST \nX_train_seg = np.rollaxis(X_train_seg, 3, 1)\nX_train_seg = np.rollaxis(X_train_seg, 3, 2)\n\nY_train_seg  = np.rollaxis(Y_train_seg, 3, 1)\nY_train_seg = np.rollaxis(Y_train_seg, 3, 2)\n\npred_train_threshold = np.rollaxis(pred_train_threshold, 3, 1)\npred_train_threshold = np.rollaxis(pred_train_threshold, 3, 2)\n\npred_val_threshold = np.rollaxis(pred_val_threshold, 3, 1)\npred_val_threshold = np.rollaxis(pred_val_threshold, 3, 2)","688bf1fd":"## Showing our predicted masks on our training data\nix = random.randint(0, 682)\nplt.figure(figsize=(20, 28))\n\n# Our original training image\nplt.subplot(131)\nimshow(X_train_seg[ix])\nplt.title('Image')\n\n# Our original combined mask\nplt.subplot(132)\nimshow(np.squeeze(Y_train_seg[ix]), cmap='gray_r')\nplt.title('Mask')\n\n# The mask of our model U-Net prediction\nplt.subplot(133)\nimshow(np.squeeze(pred_train_threshold[ix] > 0.40), cmap='gray_r')\nplt.title('Prediction')\nplt.show()","9e3ab12a":"## Showing our predicted masks on our training data\nix = random.randint(602, 668)\nplt.figure(figsize=(20, 28))\n\n# Our original training image\nplt.subplot(121)\nimshow(X_train_seg[ix])\nplt.title('Image')\n\n# The mask of our model U-Net prediction\nplt.subplot(122)\nix = ix - 603\nimshow(np.squeeze(pred_val_threshold[ix] > 0.40), cmap='gray_r')\nplt.title('Prediction')\nplt.show()","4ead5ba3":"Mask only images that have label of 1 (pneumothorax detected). We use this data for train segmentation model later.","53c49097":"## Preprocessing\nload tabular data","2485dd99":"Resizing images and its mask to 128 x 128, also add label to ensure we only put data that have label of  1 (it has pneumothorax). Resizing images to smaller dimention will help reduce background that contains pretty large number of 0 values","aa4948ea":"## Train the Model","a8d6ba36":"Visualize the data and its mask. The picture show to us that all the images only contains label of 1","7b279852":"Last, maybe you can tuning the model so it will perform better than i do \ud83d\ude0a. Good Luck!!!","3c3def3c":"## Create IOU score, Dice Coef, and Dice Coef Loss","18c2a512":"We must change dimension to first channel format to train","d223190b":"## **Keras-Implementation of Attention-UNet for Pneumothorax Segmentation**","1e7939c3":"## Attention-UNet","3350b013":"## Build Attention Block","bcf7cd86":"## Predict \npredict using some images in train data to ensure how well our model draw a segmentation compare to ground truth mask","bf523276":"### Load Packages","1e3a6587":"## About the Model\nhere is a link about attention-unet paper : https:\/\/arxiv.org\/abs\/1804.03999 <br>\nmedium : https:\/\/medium.com\/@sh.tsang\/review-attention-u-net-learning-where-to-look-for-the-pancreas-biomedical-image-segmentation-e5f4699daf9f <br>\ngithub : https:\/\/github.com\/lixiaolei1982\/Keras-Implementation-of-U-Net-R2U-Net-Attention-U-Net-Attention-R2U-Net.-","15df00d1":"To visualize we change to last channel format "}}