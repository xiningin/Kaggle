{"cell_type":{"5e687244":"code","8a263ae3":"code","4d36b877":"code","c9f2073c":"code","b03f623c":"code","e57fae0a":"code","b94b7f44":"code","da42c502":"code","44b433ba":"code","b41a4c51":"code","1a6514a4":"code","7b16d365":"code","ea46c335":"code","54341989":"code","5f362a88":"code","228d8fb4":"code","fa0257b6":"code","dc6ddeee":"code","abde1dad":"code","32d796dc":"code","8014c516":"code","d48dc88b":"code","270420b3":"code","8eb54c78":"code","793f8491":"code","d4f83b78":"code","030bdd5f":"code","3de35a20":"code","d74545da":"code","0d4f6a55":"code","6c1c178f":"code","659a072e":"code","1e743e33":"markdown"},"source":{"5e687244":"!wget https:\/\/zenodo.org\/record\/841984\/files\/wili-2018.zip","8a263ae3":"!unzip wili-2018.zip","4d36b877":"import numpy as np\nimport pandas as pd\nimport re\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing import sequence\nfrom keras.callbacks import LearningRateScheduler, ModelCheckpoint\nfrom sklearn.preprocessing import OneHotEncoder\nimport matplotlib.pyplot as plt","c9f2073c":"with open('x_train.txt') as f:\n  data = f.read()\n\ndata_x_train = data.split('\\n')\nprint(len(data_x_train))\n\nwith open('x_test.txt') as f:\n  data = f.read()\n\ndata_x_test = data.split('\\n')\nprint(len(data_x_test))\n\nwith open('y_train.txt') as f:\n  data = f.read()\n\ndata_y_train = data.split('\\n')\nprint(len(data_y_train))\n\nwith open('y_test.txt') as f:\n  data = f.read()\n\ndata_y_test = data.split('\\n')\nprint(len(data_y_test))","b03f623c":"data_x_train.pop(-1)\ndata_x_test.pop(-1)\ndata_y_train.pop(-1)\ndata_y_test.pop(-1)","e57fae0a":"print(len(data_x_train))\nprint(len(data_x_test))\nprint(len(data_y_train))\nprint(len(data_y_test))","b94b7f44":"data_x_train  = pd.DataFrame(data_x_train,columns=['sentence'])\ndata_y_train  = pd.DataFrame(data_y_train,columns=['language'])\n\ndata_x_test   = pd.DataFrame(data_x_test,columns=['sentence'])\ndata_y_test   = pd.DataFrame(data_y_test,columns=['language'])\n","da42c502":"def process_sentence(sentence):\n    '''Removes all special characters from sentence. It will also strip out\n    extra whitespace and makes the string lowercase.\n    '''\n    \n    return re.sub(r'[\\\\\\\\\/:*\u00ab`\\'?\u00bf\";!<>,.|]', '', sentence.lower().strip())","44b433ba":"x_train = data_x_train['sentence'].apply(process_sentence)\ny_train = data_y_train['language']\n\nx_test = data_x_test['sentence'].apply(process_sentence)\ny_test = data_y_test['language']","b41a4c51":"print(len(x_train))\nprint(len(y_train))","1a6514a4":"elements =  set((''.join([element for element in x_train])).split())","7b16d365":"languages = set(y_train)","ea46c335":"print(\"Total Words       :    \" + str(len(elements)))\nprint(\"Total Languages   :    \" + str(len(languages)) )","54341989":"def create_lookup_table(text):\n    \"\"\"Create lookup tables for vocabulary\n    \"\"\"\n    \n    text_to_int = { word : i for i, word in enumerate(text)}\n    int_to_text = {   v  : k for k, v in text_to_int.items()}\n    \n    return text_to_int, int_to_text","5f362a88":"elements.add(\"<UNK>\")","228d8fb4":"elements_to_int, int_to_elements = create_lookup_table(elements)\nlanguages_to_int, int_to_languages = create_lookup_table(languages)","fa0257b6":"def encode_to_int(data, data_to_int):\n    \"\"\"Converts all our text to integers\n    \"\"\"\n    encoded_items = []\n    for sentence in data: \n        encoded_items.append([data_to_int[word] if word in data_to_int else data_to_int[\"<UNK>\"] for word in sentence.split()])\n    \n    return encoded_items","dc6ddeee":"x_train_encoded = encode_to_int(x_train, elements_to_int)\nx_test_encoded = encode_to_int(x_test, elements_to_int)","abde1dad":"print(elements_to_int['<UNK>'])","32d796dc":"print(x_test_encoded[1111])","8014c516":"y_train_encoded = OneHotEncoder().fit_transform(encode_to_int(y_train, languages_to_int)).toarray()\ny_test_encoded = OneHotEncoder().fit_transform(encode_to_int(y_test, languages_to_int)).toarray()","d48dc88b":"max_sentence_length = 200\nembedding_vector_length = 300\ndropout = 0.5\n","270420b3":"x_train_pad = sequence.pad_sequences(x_train_encoded, maxlen=max_sentence_length)\nx_test_pad = sequence.pad_sequences(x_test_encoded, maxlen=max_sentence_length)","8eb54c78":"print(x_test_pad[0])","793f8491":"# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","d4f83b78":"with tpu_strategy.scope():\n    model = Sequential()\n\n    model.add(Embedding(len(elements_to_int), embedding_vector_length, input_length=max_sentence_length))\n    model.add(LSTM(256, return_sequences=True, dropout=dropout, recurrent_dropout=dropout))\n    model.add(LSTM(256, dropout=dropout, recurrent_dropout=dropout))\n    model.add(Dense(len(languages), activation='softmax'))\n\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n    #checkpointer = ModelCheckpoint(filepath='language_model.hdf5', monitor = \"val_loss\", verbose=1, save_best_only=True, mode = 'auto')\n\n    print(model.summary())","030bdd5f":"print(len(x_train_encoded))\nprint(len(y_train_encoded))","3de35a20":"scores = []\n\n\nfor i in range(0,15):\n  print(\"\\tEpoch  :  \"+str(i))\n  score = model.fit(x_train_pad, y_train_encoded, epochs=1, batch_size=256, validation_data=(x_test_pad, y_test_encoded ))\n  scores.append(score)\n  \n\ntrain_loss = []\nvalidation_loss = []\n\nfor history in scores:\n  train_loss.append(history.history['loss'])\n  validation_loss.append(history.history['val_loss'])\n\n\nplt.plot(train_loss)\nplt.plot(validation_loss)\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\ndel train_loss\ndel validation_loss","d74545da":"model","0d4f6a55":"def predict_sentence(sentence):\n    \"\"\"Converts the text and sends it to the model for classification\n    \"\"\"\n    \n    # Clean the sentence\n    sentence = process_sentence(sentence)\n    # Transform and pad it before using the model to predict\n    x = np.array(encode_to_int([sentence], elements_to_int))\n    #print(x)\n    x = sequence.pad_sequences(x, maxlen=max_sentence_length)\n    \n    #print(x.shape)\n    \n    prediction = model.predict(x)\n    #print(prediction)\n    \n    # Get the highest prediction\n    lang_index = np.argmax(prediction)\n    #print(lang_index)\n    #print(prediction[0][lang_index])\n    #print(prediction[0][93])\n    return int_to_languages[lang_index]","6c1c178f":"predict_sentence('\u092c\u093e\u0902\u0917\u094d\u0932\u093e\u0926\u0947\u0936 \u0915\u0947 \u092e\u0941\u0916\u094d\u092f \u0928\u094d\u092f\u093e\u092f\u093e\u0927\u0940\u0936 \u0915\u093e \u092a\u0926, \u092c\u093e\u0902\u0917\u094d\u0932\u093e\u0926\u0947\u0936 \u0938\u0930\u094d\u0935\u094b\u091a\u094d\u091a \u0928\u094d\u092f\u093e\u092f\u093f\u0915 \u092a\u0926 \u0939\u0948\u0964 \u0907\u0938 \u092a\u0926 \u092a\u0930 \u0935\u093f\u0930\u093e\u091c\u092e\u093e\u0928 \u0939\u094b\u0928\u0947 \u0935\u093e\u0932\u0947 \u092a\u0939\u0932\u0947 \u092a\u0926\u093e\u0927\u093f\u0915\u093e\u0930\u0940 \u0928\u094d\u092f\u093e\u092f\u092e\u0942\u0930\u094d\u0924\u093f \u0905\u092c \u0938\u093e\u0926\u093e\u0924 \u092e\u094b\u0939\u092e\u094d\u092e\u0939 \u0916\u093e\u0928 \u0938\u092f\u092e \u0925\u0947, \u091c\u094b\u0915\u093f \u0967\u096c \u0926\u093f\u0938\u0902\u092c\u0930 \u0967\u096f\u096d\u0968 \u0938\u0947 \u0928\u0935\u0902\u092c\u0930 \u0967\u096f\u096d\u096b \u0924\u0915 \u0907\u0938 \u092a\u0926 \u092a\u0930 \u0930\u0939\u0947 \u0925\u0947\u0964 \u0924\u0924\u094d\u092a\u0936\u094d\u091a\u093e\u0924, \u091c\u0928\u0935\u0930\u0940 \u0968\u0966\u0967\u096b \u0924\u0915 \u0907\u0938 \u092a\u0926 \u092a\u0930 \u0915\u0941\u0932 \u0968\u0967 \u0932\u094b\u0917 \u0935\u093f\u0930\u093e\u091c\u092e\u093e\u0928 \u0939\u094b \u091a\u0941\u0915\u0947 \u0939\u0948\u0902\u0964 \u0935\u0930\u094d\u0924\u092e\u093e\u0928 \u092e\u0941\u0916\u094d\u092f \u0928\u094d\u092f\u093e\u092f\u093e\u0927\u0940\u0936 \u0938\u0941\u0930\u0947\u0928\u094d\u0926\u094d\u0930 \u0915\u0941\u092e\u093e\u0930 \u0938\u093f\u0928\u094d\u0939\u093e \u0907\u0938 \u092a\u0926 \u092a\u0930 \u0967\u096d \u091c\u0928\u0935\u0930\u0940 \u0968\u0966\u0967\u096b \u0935\u093f\u0930\u093e\u091c\u092e\u093e\u0928 \u0939\u0948\u0902\u0964 \u0935\u0947 \u0939\u093f\u0928\u094d\u0926\u0942 \u0927\u0930\u094d\u092e \u0915\u0947 \u0905\u0928\u0941\u092f\u093e\u092f\u0940 \u0939\u0948\u0902, \u0924\u0925\u093e \u092c\u093f\u0937\u094d\u0923\u0941\u092a\u094d\u0930\u093f\u092f \u092e\u0923\u093f\u092a\u0941\u0930\u0940 \u0938\u092e\u0941\u0926\u093e\u092f \u0938\u0947 \u0906\u0924\u0947 \u0939\u0948\u0902, \u0924\u0925\u093e \u0935\u0947 \u092c\u093e\u0902\u0917\u094d\u0932\u093e\u0926\u0947\u0936 \u092e\u0947\u0902 \u0915\u093f\u0938\u0940 \u092d\u0940 \u0905\u0932\u094d\u092a\u0938\u0902\u0916\u094d\u092f\u0915 \u091c\u093e\u0924\u0940\u092f \u0938\u092e\u0942\u0939\u094b\u0902 \u0938\u0947 \u0928\u093f\u092f\u0941\u0915\u094d\u0924 \u092a\u0939\u0932\u0940 \u092e\u0941\u0916\u094d\u092f \u0928\u094d\u092f\u093e\u092f\u093e\u0927\u0940\u0936 \u0939\u0948\u0964 \u0928\u094d\u092f\u093e\u092f\u092e\u0942\u0930\u094d\u0924\u093f \u092d\u093e\u0935\u0928\u0940 \u092a\u094d\u0930\u0938\u093e\u0926 \u0938\u093f\u0928\u094d\u0939\u093e \u092d\u0940 \u090f\u0915 \u0939\u0940 \u0938\u092e\u0941\u0926\u093e\u092f \u0938\u0947 \u0939\u0948\u0902\u0964 \u0928\u094d\u092f\u093e\u092f\u092e\u0942\u0930\u094d\u0924\u093f \u092e\u0939\u094c\u0926\u092f \u0928\u093e\u091c\u093c\u092e\u0928 \u0906\u0930\u093e \u0938\u0941\u0932\u094d\u0924\u093e\u0928\u093e \u0907\u0938 \u092a\u0926 \u0915\u094b \u0938\u0941\u0936\u094b\u092d\u093f\u0924 \u0915\u0930\u0928\u0947 \u0935\u093e\u0932\u0940 \u092a\u0939\u0932\u0940 \u092e\u0939\u093f\u0932\u093e \u0928\u094d\u092f\u093e\u092f\u093e\u0927\u0940\u0936 \u0925\u0940\u0902, \u0914\u0930 \u092e\u0948\u0921\u092e \u091c\u0938\u094d\u091f\u093f\u0938 \u0915\u0943\u0937\u094d\u0923\u093e \u0926\u0947\u092c\u0928\u093e\u0925 \u092c\u093e\u0902\u0917\u094d\u0932\u093e\u0926\u0947\u0936 \u0915\u0940 \u092a\u0939\u0932\u0940 \u092e\u0939\u093f\u0932\u093e \u0939\u093f\u0902\u0926\u0942 \u0928\u094d\u092f\u093e\u092f\u093e\u0927\u0940\u0936 \u0939\u0948\u0964 \u0935\u0930\u094d\u0924\u092e\u093e\u0928 \u092e\u0947\u0902 \u0938\u0941\u092a\u094d\u0930\u0940\u092e \u0915\u094b\u0930\u094d\u091f \u092e\u0947\u0902 \u091b\u0939 \u092e\u0939\u093f\u0932\u093e \u0928\u094d\u092f\u093e\u092f\u093e\u0927\u0940\u0936\u094b\u0902 \u0930\u0939\u0947 \u0939\u0948\u0902\u0964')","659a072e":"predict_sentence('Ne l fin de l seclo XIX l Japon era inda \u00e7conhecido i s\u00f3tico pa l mundo oucidental. Cula antrodu\u00e7on de la st\u00e9tica japonesa, particularmente na Sposi\u00e7on Ounibersal de 1900, an Paris, l Oucidente adquiriu un apetite ansaciable pul Japon i Heiarn se tornou mundialmente coincido pula perfundidade, ouriginalidade i sinceridade de ls sous cuntos. An sous radadeiros anhos, alguns cr\u00edticos, cumo George Orwell, acus\u00f3run Heiarn de trasferir sou nacionalismo i fazer l Japon parecer mais s\u00f3tico, mas, cumo loufereciu al Oucidente alguns de sous purmeiros lampeijos de l Japon pr\u00e9-andustrial i de l Per\u00edodo Meiji, sou trabalho inda ye balioso at\u00e9 hoije.')","1e743e33":"**BASED ON WILI-2018 PAPER**"}}