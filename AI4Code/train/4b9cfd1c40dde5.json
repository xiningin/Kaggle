{"cell_type":{"3ca33b2d":"code","0ab56b7b":"code","32098bec":"code","bc542390":"code","f7b7c7e1":"code","44d83bfc":"code","397047d6":"code","3c50214b":"code","47f884c3":"code","c99cdc42":"code","e04ff71c":"code","1373f700":"code","0799fd76":"code","dd308f6e":"code","78f237eb":"code","471ce327":"code","eb22ebb5":"code","c905691d":"code","05f5376e":"code","0b0bb482":"code","ed345049":"code","4dc77880":"code","d634e0e4":"code","99b7cff1":"code","1a2ec3e9":"code","f174f588":"code","4bf24460":"code","0cb22aa7":"code","c72df99f":"code","c5844da7":"code","e426fe41":"code","dcdf8b68":"code","6b8b353a":"code","64a655d0":"code","619967a0":"code","33820027":"code","09bdd6a5":"code","87c79067":"code","46895428":"code","40f0a9bf":"code","5f90d40e":"code","394db7fc":"code","10220e77":"code","3dfdeeba":"code","4962828f":"code","3094b43c":"code","9bd3c854":"code","dd7660db":"code","661417db":"code","26371180":"code","1206270a":"code","9b0bd1b3":"code","25b7181d":"code","4362aa87":"code","b58c40a7":"code","80e07c71":"code","05bb1c82":"code","e13c0b4c":"code","f42e68ee":"code","d68c078a":"markdown","81c254fc":"markdown","26ede873":"markdown","ab503ba4":"markdown","707dd460":"markdown","2da1813a":"markdown","42d0e485":"markdown","1d634f2f":"markdown","9386920b":"markdown","a752f33a":"markdown","53fb584b":"markdown","69c9fad2":"markdown","b2c18032":"markdown","f6a2177b":"markdown","e76597a6":"markdown","b2aa95bb":"markdown","d350d566":"markdown"},"source":{"3ca33b2d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# numpy and pandas for data manipulation\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# File system manangement\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0ab56b7b":"# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns","32098bec":"# List files available\nprint(os.listdir(\"..\/input\/home-credit-default-risk\"))","bc542390":"dir_name = \"..\/input\/home-credit-default-risk\"","f7b7c7e1":"app_train = pd.read_csv(os.path.join(dir_name,'application_train.csv'))\nprint(\"Training data shape: \", app_train.shape)\napp_train.head()","44d83bfc":"print(\"Application train column names: \")\nprint(\"------------------------------\")\nfor _ in app_train.columns.values:\n    print(_, end=' , ')","397047d6":"app_test = pd.read_csv(os.path.join(dir_name,'application_test.csv'))\nprint(\"Test data shape: \", app_test.shape)\napp_test.head()","3c50214b":"print(\"Application test column names: \")\nprint(\"------------------------------\")\nfor _ in app_test.columns.values:\n    print(_, end=' , ')","47f884c3":"app_train['TARGET'].value_counts()","c99cdc42":"app_train['TARGET'].astype(int).plot.hist()","e04ff71c":"app_train.isnull().sum()","1373f700":"def missing_values_table(df):\n    # Total missing values\n    missing_values = df.isnull().sum()\n#     print(missing_values)\n    # Percentage of missing values\n    missing_values_percentage = 100 * missing_values \/ len(df)\n#     print(missing_values_percentage)\n    missing_values_table = pd.concat([missing_values,missing_values_percentage],axis=1)\n    print(\"Missing values and percentage shape: \",missing_values_table.shape)\n#     print(missing_values_table)\n    # Renaming the column names\n    missing_values_rename_columns = missing_values_table.rename(columns = {0: 'Missing Values', 1: 'Missing % of total values'})\n#     print(missing_values_rename_columns)\n    # Sorting the table by percentage of missing descendents\n    missing_values_rename_columns = missing_values_rename_columns[missing_values_rename_columns.iloc[:,1] != 0].sort_values('Missing % of total values', ascending = False).round(1)\n#     print(missing_values_rename_columns)\n    print(f'This dataframe has {df.shape[1]} columns. There are {missing_values_rename_columns.shape[0]} columns that have missing values.')\n    \n    return missing_values_rename_columns\n    ","0799fd76":"app_train_missing_values = missing_values_table(app_train)\napp_train_missing_values.head(10)","dd308f6e":"app_train.dtypes.value_counts()","78f237eb":"app_train.select_dtypes('object').apply(pd.Series.nunique,axis = 0) # here axis = 1 means row in the dataframe and 0 means column","471ce327":"#Create a label encoder object\ndef label_encoder_train_test(train, test):\n\n    le = LabelEncoder()\n    le_count = 0\n\n    # Iterate through the columns\n    for col in train:\n        if train[col].dtype == 'object':\n            if len(list(train[col].unique())) <= 2:\n                le.fit(train[col])\n                train[col] = le.transform(train[col])\n                test[col] = le.transform(test[col])\n                le_count += 1\n\n    print(f'{le_count} columns were encoded.')","eb22ebb5":"label_encoder_train_test(app_train,app_test)","c905691d":"# One hot encoding for more than two categorical values\ndef one_hot_encoding_train_test(train, test):\n    train = pd.get_dummies(train)\n    test = pd.get_dummies(test)\n    \n    print(f\"Training feature shape: {train.shape}\")\n    print(f\"Testing feature shape: {test.shape}\")","05f5376e":"one_hot_encoding_train_test(app_train,app_test)","0b0bb482":"train_labels = app_train['TARGET']\n# Align the training and testing data, keep only columns present in both dataframes\napp_train, app_test = app_train.align(app_test, join='inner', axis=1) # axis = 1 for column based alignment\napp_train['TARGET'] = train_labels\n\nprint(f'Training feature shape: {app_train.shape}')\nprint(f'Testing feature shape: {app_test.shape}')\n","ed345049":"# The numbers in the DAYS_BIRTH column are negative because they are recorded relative to the current loan application\nage = (app_train['DAYS_BIRTH']\/ -365).describe()\nage","4dc77880":"app_train['DAYS_EMPLOYED'].describe()","d634e0e4":"app_train.DAYS_EMPLOYED.plot.hist(title='Days employment histogram');\nplt.xlabel('Days employment');","99b7cff1":"anomalous = app_train[app_train.DAYS_EMPLOYED == 365243]\nnon_anomalous = app_train[app_train.DAYS_EMPLOYED != 365243]\n\nprint(f'The non-anomalous default on {non_anomalous.TARGET.mean() * 100}% of loans')\nprint(f'The anomalous default on {anomalous.TARGET.mean() * 100}% of loans')\nprint(f'There are {len(anomalous)} anomalous days of employment')","1a2ec3e9":"app_train.DAYS_EMPLOYED_ANOM = app_train.DAYS_EMPLOYED == 365243\napp_train.DAYS_EMPLOYED.replace({365243: np.nan}, inplace = True)\napp_train.DAYS_EMPLOYED.plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","f174f588":"app_test.DAYS_EMPLOYED_ANOM = app_test.DAYS_EMPLOYED == 365243\napp_test.DAYS_EMPLOYED.replace({365243: np.nan}, inplace = True)\nprint(f'There are {app_test.DAYS_EMPLOYED_ANOM.sum()} anomalies in the test data out of {len(app_test)} entries.')","4bf24460":"correlations = app_train.corr()['TARGET'].sort_values()\n\nprint('Most positive correlations:\\n', correlations.tail(15))\nprint('\\nMost negative correlations:\\n', correlations.head(15))","0cb22aa7":"app_train.DAYS_BIRTH = abs(app_train.DAYS_BIRTH)\napp_train.DAYS_BIRTH.corr(app_train.TARGET)","c72df99f":"plt.style.use('fivethirtyeight')\nplt.hist(app_train['DAYS_BIRTH']\/365, edgecolor='k', bins=25)\nplt.title('Age of client');plt.xlabel('Age (years)');plt.ylabel('Count')","c5844da7":"plt.figure(figsize = (10, 8))\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'] \/ 365, label = 'target == 0')\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] \/ 365, label = 'target == 1')\nplt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages');","e426fe41":"# Age information into a separate dataframe\nage_data = app_train[['TARGET', 'DAYS_BIRTH']]\nage_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] \/ 365\n\n# Bin the age data\nage_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\nage_data.head(10)\n# ( = \"excluding\", ] = \"including\"","dcdf8b68":"type(age_data['YEARS_BINNED']) #special case of excluding and including","6b8b353a":"# Group by the bin and calculate averages\n\nage_groups  = age_data.groupby('YEARS_BINNED').mean()\nage_groups","64a655d0":"plt.figure(figsize = (8, 8))\n\n# Graph the age bins and the average of the target as a bar plot\nplt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\n\n# Plot labeling\nplt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\nplt.title('Failure to Repay by Age Group');","619967a0":"# Extract the EXT_SOURCE variables and show correlations\next_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\next_data_corrs = ext_data.corr()\next_data_corrs\n","33820027":"plt.figure(figsize = (8, 6))\n\n# Heatmap of correlations\nsns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap');","09bdd6a5":"plt.figure(figsize = (10, 12))\n\n# iterate through the sources\nfor i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n    \n    # create a new subplot for each source\n    plt.subplot(3, 1, i + 1)\n    # plot repaid loans\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, source], label = 'target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, source], label = 'target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % source)\n    plt.xlabel('%s' % source); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)","87c79067":"# Copy the data for plotting\nplot_data = ext_data.drop(columns = ['DAYS_BIRTH']).copy()\n\n# Add in the age of the client in years\nplot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n\n# Drop na values and limit to first 100000 rows\nplot_data = plot_data.dropna().loc[:100000, :]\n\n# Function to calculate correlation coefficient between two columns\ndef corr_func(x, y, **kwargs):\n    r = np.corrcoef(x, y)[0][1]\n    ax = plt.gca()\n    ax.annotate(\"r = {:.2f}\".format(r),\n                xy=(.2, .8), xycoords=ax.transAxes,\n                size = 20)\n\n# Create the pairgrid object\ngrid = sns.PairGrid(data = plot_data, size = 3, diag_sharey=False,\n                    hue = 'TARGET', \n                    vars = [x for x in list(plot_data.columns) if x != 'TARGET'])\n\n# Upper is a scatter plot\ngrid.map_upper(plt.scatter, alpha = 0.2)\n\n# Diagonal is a histogram\ngrid.map_diag(sns.kdeplot)\n\n# Bottom is density plot\ngrid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r);\n\nplt.suptitle('Ext Source and Age Features Pairs Plot', size = 32, y = 1.05);","46895428":"# Make a new dataframe for polynomial features\npoly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\npoly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n\n# imputer for handling missing values\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(strategy = 'median')\n\npoly_target = poly_features['TARGET']\n\npoly_features = poly_features.drop(columns = ['TARGET'])\n\n# Need to impute missing values\npoly_features = imputer.fit_transform(poly_features)\npoly_features_test = imputer.transform(poly_features_test)\n\nfrom sklearn.preprocessing import PolynomialFeatures\n                                  \n# Create the polynomial object with specified degree\npoly_transformer = PolynomialFeatures(degree = 3)","40f0a9bf":"# Train the polynomial features\npoly_transformer.fit(poly_features)\n\n# Transform the features\npoly_features = poly_transformer.transform(poly_features)\npoly_features_test = poly_transformer.transform(poly_features_test)\nprint('Polynomial Features shape: ', poly_features.shape)","5f90d40e":"poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:15]","394db7fc":"# Create a dataframe of the features \npoly_features = pd.DataFrame(poly_features, \n                             columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                           'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n# Add in the target\npoly_features['TARGET'] = poly_target\n\n# Find the correlations with the target\npoly_corrs = poly_features.corr()['TARGET'].sort_values()\n\n# Display most negative and most positive\nprint(poly_corrs.head(10))\nprint(poly_corrs.tail(5))","10220e77":"# Put test features into dataframe\npoly_features_test = pd.DataFrame(poly_features_test, \n                                  columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                                'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n# Merge polynomial features into training dataframe\npoly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\napp_train_poly = app_train.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\n\n# Merge polnomial features into testing dataframe\npoly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\napp_test_poly = app_test.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left')\n\n# Align the dataframes\napp_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n\n# Print out the new shapes\nprint('Training data with polynomial features shape: ', app_train_poly.shape)\nprint('Testing data with polynomial features shape:  ', app_test_poly.shape)","3dfdeeba":"app_train_domain = app_train.copy()\napp_test_domain = app_test.copy()\n\napp_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] \/ app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] \/ app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] \/ app_train_domain['AMT_CREDIT']\napp_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] \/ app_train_domain['DAYS_BIRTH']","4962828f":"app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] \/ app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] \/ app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] \/ app_test_domain['AMT_CREDIT']\napp_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] \/ app_test_domain['DAYS_BIRTH']","3094b43c":"plt.figure(figsize = (12, 20))\n# iterate through the new features\nfor i, feature in enumerate(['CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT', 'CREDIT_TERM', 'DAYS_EMPLOYED_PERCENT']):\n    \n    # create a new subplot for each source\n    plt.subplot(4, 1, i + 1)\n    # plot repaid loans\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 0, feature], label = 'target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 1, feature], label = 'target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % feature)\n    plt.xlabel('%s' % feature); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)","9bd3c854":"from sklearn.preprocessing import MinMaxScaler, Imputer\n\n# Drop the target from the training data\nif 'TARGET' in app_train:\n    train = app_train.drop(columns = ['TARGET'])\nelse:\n    train = app_train.copy()\n    \n# Feature names\nfeatures = list(train.columns)\n\n# Copy of the testing data\ntest = app_test.copy()\n\n# Median imputation of missing values\nimputer = Imputer(strategy = 'median')\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0, 1))\n\n# Fit on the training data\nimputer.fit(train)\n\n# Transform both training and testing data\ntrain = imputer.transform(train)\ntest = imputer.transform(app_test)\n\n# Repeat with the scaler\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)","dd7660db":"from sklearn.linear_model import LogisticRegression\n\n# Make the model with the specified regularization parameter\nlog_reg = LogisticRegression(C = 0.0001)\n\n# Train on the training data\nlog_reg.fit(train, train_labels)","661417db":"log_reg_pred = log_reg.predict_proba(test)[:, 1]","26371180":"submit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = log_reg_pred\n\nsubmit.head()","1206270a":"submit.to_csv('log_reg_baseline.csv', index = False)","9b0bd1b3":"from sklearn.ensemble import RandomForestClassifier\n\n# Make the random forest classifier\nrandom_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)","25b7181d":"# Train on the training data\nrandom_forest.fit(train, train_labels)\n\n# Extract feature importances\nfeature_importance_values = random_forest.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n\n# Make predictions on the test data\npredictions = random_forest.predict_proba(test)[:, 1]","4362aa87":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline.csv', index = False)","b58c40a7":"poly_features_names = list(app_train_poly.columns)\n\n# Impute the polynomial features\nimputer = Imputer(strategy = 'median')\n\npoly_features = imputer.fit_transform(app_train_poly)\npoly_features_test = imputer.transform(app_test_poly)\n\n# Scale the polynomial features\nscaler = MinMaxScaler(feature_range = (0, 1))\n\npoly_features = scaler.fit_transform(poly_features)\npoly_features_test = scaler.transform(poly_features_test)\n\nrandom_forest_poly = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)","80e07c71":"# Train on the training data\nrandom_forest_poly.fit(poly_features, train_labels)\n\n# Make predictions on the test data\npredictions = random_forest_poly.predict_proba(poly_features_test)[:, 1]\n","05bb1c82":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline_engineered.csv', index = False)","e13c0b4c":"app_train_domain = app_train_domain.drop(columns = 'TARGET')\n\ndomain_features_names = list(app_train_domain.columns)\n\n# Impute the domainnomial features\nimputer = Imputer(strategy = 'median')\n\ndomain_features = imputer.fit_transform(app_train_domain)\ndomain_features_test = imputer.transform(app_test_domain)\n\n# Scale the domainnomial features\nscaler = MinMaxScaler(feature_range = (0, 1))\n\ndomain_features = scaler.fit_transform(domain_features)\ndomain_features_test = scaler.transform(domain_features_test)\n\nrandom_forest_domain = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\n\n# Train on the training data\nrandom_forest_domain.fit(domain_features, train_labels)\n\n# Extract feature importances\nfeature_importance_values_domain = random_forest_domain.feature_importances_\nfeature_importances_domain = pd.DataFrame({'feature': domain_features_names, 'importance': feature_importance_values_domain})\n\n# Make predictions on the test data\npredictions = random_forest_domain.predict_proba(domain_features_test)[:, 1]","f42e68ee":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline_domain.csv', index = False)","d68c078a":"**Baseline**\n* logistic regression implementation\n* \n","81c254fc":"## **Distributions of repaid and non-repaid loan record**","26ede873":"## Back to Exploratory Data Analysis\n> **Anomalies**\n* One way to support anomalies quantitatively is by looking at the statistics of a column using the describe method\n* These may be due to **mis-typed numbers**, **errors in measuring equipment**, or **they could be valid but extreme measurements**","ab503ba4":"## kernel density estimation plot (KDE)","707dd460":"## **The training data has 307511 observations (each one a separate loan) **","2da1813a":"## **Checking column types**","42d0e485":"## **Encoding Categorical Variables**\n\n> * Label encoding\n> * One hot encoding","1d634f2f":"## Aligning Train and Test data","9386920b":"## **Missing values**","a752f33a":"## **Exploratory Data Analysis**\n> Exploratory Data Analysis (EDA) is an open-ended process where we calculate\n* statistics\n* make figures to find trends\n* anomalies\n* patterns\n* relationships\nwithin the data. ","53fb584b":"**Effect of age on repayment**","69c9fad2":"**Feature Engineering**\n* Polynomial features\n* Domain knowledge features","b2c18032":"**There is a clear trend: younger applicants are more likely to not repay the loan! The rate of failure to repay is above 10% for the youngest three age groups and beolow 5% for the oldest age group.**","f6a2177b":"**Emining the distributions of the target columns**\n> Here we will determine the distributions of loan repayment or not related data","e76597a6":"## Anomalous value eradication  ","b2aa95bb":"**Improved Model: Random Forest**","d350d566":"## Correlations\n>  Pearson correlation coefficient\n\n* .00-.19 \u201cvery weak\u201d\n* .20-.39 \u201cweak\u201d\n* .40-.59 \u201cmoderate\u201d\n* .60-.79 \u201cstrong\u201d\n* .80-1.0 \u201cvery strong\u201d"}}