{"cell_type":{"7dfe5621":"code","3b90d638":"code","be7aa06d":"code","107720a0":"code","f02ec170":"code","5354cb28":"code","83d5d08a":"code","d0817f41":"code","e2f895b5":"code","e778d145":"code","e64680b9":"code","ec43bcc4":"code","26672303":"code","c13b7501":"code","212a3904":"code","e207dd32":"code","9cd21705":"code","bfda1115":"code","356e64e2":"code","83edce9e":"code","5077e659":"markdown","e548e8d2":"markdown","d3cd9b63":"markdown","a2b704ef":"markdown","3bb289b6":"markdown","b24f424f":"markdown"},"source":{"7dfe5621":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3b90d638":"input_path = '\/kaggle\/input\/nlp-getting-started\/'\ntrain = pd.read_csv(os.path.join(input_path, 'train.csv'))\ntest = pd.read_csv(os.path.join(input_path, 'test.csv'))","be7aa06d":"train.head()","107720a0":"print('Train: ', train.shape)\nprint('Test: ', test.shape)","f02ec170":"# check the missing values for keyword and location\nlen(train['keyword'].isnull()), len(train['location'].isnull())","5354cb28":"# non disaster tweet\ntrain[train['target'] == 0]['text'].values[0]","83d5d08a":"# disaster tweet\ntrain[train['target'] == 1]['text'].values[0]","d0817f41":"import re\nimport unicodedata\nimport spacy\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC","e2f895b5":"# reference:~ https:\/\/github.com\/dipanjanS\/practical-machine-learning-with-python\/blob\/master\/bonus%20content\/nlp%20proven%20approach\/contractions.py\n\nCONTRACTION_MAP = {\n    \"ain't\": \"is not\",\n    \"aren't\": \"are not\",\n    \"can't\": \"cannot\",\n    \"can't've\": \"cannot have\",\n    \"'cause\": \"because\",\n    \"could've\": \"could have\",\n    \"couldn't\": \"could not\",\n    \"couldn't've\": \"could not have\",\n    \"didn't\": \"did not\",\n    \"doesn't\": \"does not\",\n    \"don't\": \"do not\",\n    \"hadn't\": \"had not\",\n    \"hadn't've\": \"had not have\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"he'd\": \"he would\",\n    \"he'd've\": \"he would have\",\n    \"he'll\": \"he will\",\n    \"he'll've\": \"he he will have\",\n    \"he's\": \"he is\",\n    \"how'd\": \"how did\",\n    \"how'd'y\": \"how do you\",\n    \"how'll\": \"how will\",\n    \"how's\": \"how is\",\n    \"I'd\": \"I would\",\n    \"I'd've\": \"I would have\",\n    \"I'll\": \"I will\",\n    \"I'll've\": \"I will have\",\n    \"I'm\": \"I am\",\n    \"I've\": \"I have\",\n    \"i'd\": \"i would\",\n    \"i'd've\": \"i would have\",\n    \"i'll\": \"i will\",\n    \"i'll've\": \"i will have\",\n    \"i'm\": \"i am\",\n    \"i've\": \"i have\",\n    \"isn't\": \"is not\",\n    \"it'd\": \"it would\",\n    \"it'd've\": \"it would have\",\n    \"it'll\": \"it will\",\n    \"it'll've\": \"it will have\",\n    \"it's\": \"it is\",\n    \"let's\": \"let us\",\n    \"ma'am\": \"madam\",\n    \"mayn't\": \"may not\",\n    \"might've\": \"might have\",\n    \"mightn't\": \"might not\",\n    \"mightn't've\": \"might not have\",\n    \"must've\": \"must have\",\n    \"mustn't\": \"must not\",\n    \"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\",\n    \"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\",\n    \"oughtn't\": \"ought not\",\n    \"oughtn't've\": \"ought not have\",\n    \"shan't\": \"shall not\",\n    \"sha'n't\": \"shall not\",\n    \"shan't've\": \"shall not have\",\n    \"she'd\": \"she would\",\n    \"she'd've\": \"she would have\",\n    \"she'll\": \"she will\",\n    \"she'll've\": \"she will have\",\n    \"she's\": \"she is\",\n    \"should've\": \"should have\",\n    \"shouldn't\": \"should not\",\n    \"shouldn't've\": \"should not have\",\n    \"so've\": \"so have\",\n    \"so's\": \"so as\",\n    \"that'd\": \"that would\",\n    \"that'd've\": \"that would have\",\n    \"that's\": \"that is\",\n    \"there'd\": \"there would\",\n    \"there'd've\": \"there would have\",\n    \"there's\": \"there is\",\n    \"they'd\": \"they would\",\n    \"they'd've\": \"they would have\",\n    \"they'll\": \"they will\",\n    \"they'll've\": \"they will have\",\n    \"they're\": \"they are\",\n    \"they've\": \"they have\",\n    \"to've\": \"to have\",\n    \"wasn't\": \"was not\",\n    \"we'd\": \"we would\",\n    \"we'd've\": \"we would have\",\n    \"we'll\": \"we will\",\n    \"we'll've\": \"we will have\",\n    \"we're\": \"we are\",\n    \"we've\": \"we have\",\n    \"weren't\": \"were not\",\n    \"what'll\": \"what will\",\n    \"what'll've\": \"what will have\",\n    \"what're\": \"what are\",\n    \"what's\": \"what is\",\n    \"what've\": \"what have\",\n    \"when's\": \"when is\",\n    \"when've\": \"when have\",\n    \"where'd\": \"where did\",\n    \"where's\": \"where is\",\n    \"where've\": \"where have\",\n    \"who'll\": \"who will\",\n    \"who'll've\": \"who will have\",\n    \"who's\": \"who is\",\n    \"who've\": \"who have\",\n    \"why's\": \"why is\",\n    \"why've\": \"why have\",\n    \"will've\": \"will have\",\n    \"won't\": \"will not\",\n    \"won't've\": \"will not have\",\n    \"would've\": \"would have\",\n    \"wouldn't\": \"would not\",\n    \"wouldn't've\": \"would not have\",\n    \"y'all\": \"you all\",\n    \"y'all'd\": \"you all would\",\n    \"y'all'd've\": \"you all would have\",\n    \"y'all're\": \"you all are\",\n    \"y'all've\": \"you all have\",\n    \"you'd\": \"you would\",\n    \"you'd've\": \"you would have\",\n    \"you'll\": \"you will\",\n    \"you'll've\": \"you will have\",\n    \"you're\": \"you are\",\n    \"you've\": \"you have\"\n}","e778d145":"# loading the spacy's en_core_web_sm\nnlp = spacy.load('en_core_web_sm')\nnlp.pipe_names","e64680b9":"# create and add sentencizer to the pipeline\nsent = nlp.create_pipe('sentencizer')\nnlp.add_pipe(sent, before='parser')\nnlp.pipe_names","ec43bcc4":"def text_cleaning(text):\n    \"\"\"\n    Returns cleaned text (Accented Characters, Expand Contractions, Special Characters)\n    Parameters\n    ----------\n    text -> String\n    \"\"\"\n    # remove accented characters\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    \n    # expand contractions\n    for word in text.split():\n        if word.lower() in CONTRACTION_MAP:\n            text = text.replace(word[1:], CONTRACTION_MAP[word.lower()][1:])\n    \n    # remove special characters\n    pattern = r'[^a-zA-Z0-9\\s,:)(!]'\n    text = re.sub(pattern, '', text)\n    \n    doc = nlp(text)\n    tokens = []\n    \n    for token in doc:\n        if token.lemma_ != '-PRON-':\n            tokens.append(token.lemma_.lower().strip())\n        else:\n            tokens.append(token.lower_)\n\n    return tokens","26672303":"text_cleaning(\"I don't like this movie :)\")","c13b7501":"# split the data into inputs and outputs\nX_train = train['text']\ny_train = train['target']\nX_test = test['text']","212a3904":"# build a model pipeline\n# stage 1: preprocessing, stage 2: linear SVC\n\ntext_clf = Pipeline([\n    ('tfidf', TfidfVectorizer(tokenizer=text_cleaning)),\n    ('clf', LinearSVC())\n])","e207dd32":"# using F1 metric in cross validation\nscores = cross_val_score(text_clf, X_train, y_train, cv=3, scoring='f1')\nscores","9cd21705":"# fit the model on train data\ntext_clf.fit(X_train, y_train)","bfda1115":"# load the sample submission csv file\nsample_submission = pd.read_csv(os.path.join(input_path, 'sample_submission.csv'))","356e64e2":"# predict on test data\nsample_submission['target'] = text_clf.predict(X_test)","83edce9e":"# save the sample submission csv file\nsample_submission.to_csv('submission.csv', index=False)","5077e659":"### Test Submission","e548e8d2":"### Quick Look at Data\n","d3cd9b63":"#### Load the spacy en_core library and add a sentencizer to pipeline","a2b704ef":"### Build a Pipeline\n","3bb289b6":"#### Text Preprocessing\n\n- Remove accented Characters\n- Expand the contractions\n- Remove special characters\n- Remove stop words","b24f424f":"# NLP with Disaster Tweets - Sentiment Analysis\n"}}