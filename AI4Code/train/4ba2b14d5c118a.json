{"cell_type":{"2c95ae7a":"code","98367697":"code","44efa7f1":"code","d100f628":"code","f799a9ed":"code","28e80113":"code","0ce7eddb":"code","7a654c88":"code","fe23af4b":"code","1b4a71ea":"code","795406f2":"code","2d19716d":"code","0d3bd636":"code","a5b5a3e5":"code","d9ab2589":"code","d9e713ed":"code","eaa13a9b":"code","611c3392":"code","de69ff03":"code","c16cfbae":"code","98af0733":"code","5f1dc8d1":"markdown","73098eb4":"markdown","b14f0ce5":"markdown","8e17c34d":"markdown","af81e8d2":"markdown","daa03af2":"markdown","a1e0bb93":"markdown","92e7b181":"markdown","d602dc02":"markdown","58083c91":"markdown","98caf055":"markdown","73ab7ed8":"markdown","6677dfe0":"markdown","411c5ca1":"markdown","33b1bc74":"markdown","38d4416e":"markdown","1d7b79d0":"markdown"},"source":{"2c95ae7a":"import nltk\nimport numpy as np\nfrom nltk.tokenize import sent_tokenize\n\nhamlet_raw = nltk.corpus.gutenberg.raw('shakespeare-hamlet.txt')\n\nsents = sent_tokenize(hamlet_raw)\n\nhamlet_np = np.array(sents)\n\nprint(hamlet_np.shape)\n","98367697":"from nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nimport string\nfrom nltk.corpus import wordnet\n\nstopwords_list = stopwords.words('english')\n\nlemmatizer = WordNetLemmatizer()\n\ndef my_tokenizer(doc):\n    words = word_tokenize(doc)\n    \n    pos_tags = pos_tag(words)\n    \n    non_stopwords = [w for w in pos_tags if not w[0].lower() in stopwords_list]\n    \n    non_punctuation = [w for w in non_stopwords if not w[0] in string.punctuation]\n    \n    lemmas = []\n    for w in non_punctuation:\n        if w[1].startswith('J'):\n            pos = wordnet.ADJ\n        elif w[1].startswith('V'):\n            pos = wordnet.VERB\n        elif w[1].startswith('N'):\n            pos = wordnet.NOUN\n        elif w[1].startswith('R'):\n            pos = wordnet.ADV\n        else:\n            pos = wordnet.NOUN\n        \n        lemmas.append(lemmatizer.lemmatize(w[0], pos))\n\n    return lemmas\n    \n    ","44efa7f1":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nhamlet_raw = nltk.corpus.gutenberg.raw('shakespeare-hamlet.txt')\n\nsents = sent_tokenize(hamlet_raw)\n\nhamlet_np = np.array(sents)\n\ntfidf_vectorizer = TfidfVectorizer(tokenizer=my_tokenizer)\n\ntfs = tfidf_vectorizer.fit_transform(hamlet_np)\n\nprint(tfs.shape)","d100f628":"print([k for k in tfidf_vectorizer.vocabulary_.keys()][:20])","f799a9ed":"print(tfs[:50,:50])","28e80113":"import pprint\nprint(tfs[0,:60].toarray())\nprint(tfs[0].shape)","0ce7eddb":"for item in tfidf_vectorizer.vocabulary_.items():\n    print(item)\n    if item[1] == 17:\n        print(item[0])","7a654c88":"print(tfs[0])","fe23af4b":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\ncount_vect = CountVectorizer(ngram_range=(3,3))\n\nn_gram_counts = count_vect.fit_transform(hamlet_np)\n\ntfidf_transformer = TfidfTransformer()\n\ntfs_ngrams = tfidf_transformer.fit_transform(n_gram_counts)\n\nprint(tfs_ngrams.shape)","1b4a71ea":"for item in count_vect.vocabulary_.items():\n    print(item)","795406f2":"print(tfs_ngrams[0,:17].toarray())","2d19716d":"from sklearn.decomposition import TruncatedSVD\n\nsvd_transformer = TruncatedSVD(n_components=1000)\n\nsvd_transformer.fit(tfs)\n\nprint(sorted(svd_transformer.explained_variance_ratio_)[::-1][:30])","0d3bd636":"cummulative_variance = 0.0\nk = 0\nfor var in sorted(svd_transformer.explained_variance_ratio_)[::-1]:\n    cummulative_variance += var\n    if cummulative_variance >= 0.70:\n        break\n    else:\n        k += 1\n        \nprint(k)","a5b5a3e5":"svd_transformer = TruncatedSVD(n_components=k)\nsvd_data = svd_transformer.fit_transform(tfs)\nprint(sorted(svd_transformer.explained_variance_ratio_)[::-1])","d9ab2589":"print(svd_data.shape)\nprint(svd_data)","d9e713ed":"    documents = (\n    \"The sky is blue\",\n    \"The sun is bright\",\n    \"The sun in the sky is bright\",\n    \"We can see the shining sun, the bright sun\"\n    )","eaa13a9b":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer()\ntfidf_matrix = tfidf_vectorizer.fit_transform(documents)\nprint(tfidf_matrix.shape)","611c3392":"from sklearn.metrics.pairwise import cosine_similarity\n\nquery_vect = tfidf_vectorizer.transform([\"The sun is red in the sky\"])\n#query_vect = tfidf_vectorizer.transform([\"The sun in the sky is bright\"])\n\ncosine_similarity(query_vect, tfidf_matrix)","de69ff03":"!ls ..\/input","c16cfbae":"qa_file = open('..\/input\/S08_question_answer_pairs.txt', 'r')","98af0733":"qa_file.read()","5f1dc8d1":"<b>1. TF-IDF (Term Frequency - Inverse Document Frequency)<\/b>","73098eb4":"<p>Uma vez que o texto j\u00e1 foi devidamente tratado, removendo stopwords e pontua\u00e7\u00f5es, e aplicando stemming ou lemmatization, agora precisamos contar a frequ\u00eancia das palavras (ou n-grams) que utilizaremos em seguida como atributos para as t\u00e9cnicas de aprendizado de m\u00e1quina.<\/p>","b14f0ce5":"<p>Uma vez que dois documentos s\u00e3o representados como vetores num\u00e9ricos, \u00e9 poss\u00edvel comparar a similaridade entre os documentos ao calcular o cosseno do \u00e2ngulo entre esses documentos - uma medida de dist\u00e2ncia, n\u00e3o de amplitude. Para isso, basta resolver a equa\u00e7\u00e3o do produto escalar entre os vetores, para encontrar o cosseno.<\/p>\n\n<img src=\"http:\/\/s0.wp.com\/latex.php?latex=++%5Cdisplaystyle++%5Cvec%7Ba%7D+%5Ccdot+%5Cvec%7Bb%7D+%3D+%5C%7C%5Cvec%7Ba%7D%5C%7C%5C%7C%5Cvec%7Bb%7D%5C%7C%5Ccos%7B%5Ctheta%7D+%5C%5C+%5C%5C++%5Ccos%7B%5Ctheta%7D+%3D+%5Cfrac%7B%5Cvec%7Ba%7D+%5Ccdot+%5Cvec%7Bb%7D%7D%7B%5C%7C%5Cvec%7Ba%7D%5C%7C%5C%7C%5Cvec%7Bb%7D%5C%7C%7D++&amp;bg=ffffff&amp;fg=000000&amp;s=0\" alt=\"  \\displaystyle  \\vec{a} \\cdot \\vec{b} = \\|\\vec{a}\\|\\|\\vec{b}\\|\\cos{\\theta} \\\\ \\\\  \\cos{\\theta} = \\frac{\\vec{a} \\cdot \\vec{b}}{\\|\\vec{a}\\|\\|\\vec{b}\\|}  \" title=\"  \\displaystyle  \\vec{a} \\cdot \\vec{b} = \\|\\vec{a}\\|\\|\\vec{b}\\|\\cos{\\theta} \\\\ \\\\  \\cos{\\theta} = \\frac{\\vec{a} \\cdot \\vec{b}}{\\|\\vec{a}\\|\\|\\vec{b}\\|}  \" class=\"latex\">","8e17c34d":"<b>3. Redu\u00e7\u00e3o de Dimensionalidade<\/b>","af81e8d2":"<b>2. TF-IDF de N-gramas<\/b>","daa03af2":"Opcionalmente, podemos obter os atributos tf-idf de n-grams, combinando as classes CountVectorizer e TfidfTransformer. Em nosso exemplo, vamos utilizar apenas trigramas:","a1e0bb93":"<h2> T\u00e9cnicas para Pr\u00e9-Processamento - Parte 2<\/h2>","92e7b181":"<h2> Similaridade de Cosseno<\/h2>","d602dc02":"<img class=\" wp-image-2582 \" title=\"vector_space\" src=\"http:\/\/blog.christianperone.com\/wp-content\/uploads\/2013\/09\/vector_space.png\" alt=\"\" width=\"504\" height=\"378\">\n<p fontsize=8>Fonte: http:\/\/blog.christianperone.com<\/p>","58083c91":"<h1 align=\"center\"> Aplica\u00e7\u00f5es em Processamento de Linguagem Natural <\/h1>\n<h2 align=\"center\"> Aula 03 - T\u00e9cnicas de Pr\u00e9-Processamento de Texto e Similaridade<\/h2>\n<h3 align=\"center\"> Prof. Fernando Vieira da Silva MSc.<\/h3>","98caf055":"<p>Agora vamos definir uma fun\u00e7\u00e3o para tokeniza\u00e7\u00e3o pelo scikit-learn.<\/p>","73ab7ed8":"<p>Agora vamos manter as dimens\u00f5es at\u00e9 que a vari\u00e2ncia acumulada seja maior ou igual a 0.50.<\/p>","6677dfe0":"<p>A transforma\u00e7\u00e3o do corpus em atributos contendo as frequ\u00eancias TF-IDF em geral resultar\u00e1 numa ndarray bastante esparsa, ou seja, com muitas dimens\u00f5es. Por\u00e9m, al\u00e9m de isso tornar o treinamento de algoritmos mais demorado e custoso (computacionalmente falando), muitas dessas dimens\u00f5es provavelmente s\u00e3o pouco representativas ou mesmo podem causar ru\u00eddo durante o treinamento. Para resolver esse problema, podemos aplicar uma t\u00e9cnica de redu\u00e7\u00e3o de dimensionalidade simples chamada <b>Singular Value Decomposition (SVD)<\/b>. \n\n<p>Essa t\u00e9cnica transformar\u00e1 os vetores da matriz original, rotacionando e escalando-os, resultando em novas representa\u00e7\u00f5es. A redu\u00e7\u00e3o de dimensionalidade \u00e9 feita ao manter apenas as <i>k<\/i> dimens\u00f5es mais representativas que escolhermos. Outra vantagem dessa t\u00e9cnica \u00e9 que as dimens\u00f5es originais s\u00e3o, de certa forma, \"combinadas\", o que resulta em uma nova forma de representar a combina\u00e7\u00e3o de termos. No contexto de PLN, essa t\u00e9cnica \u00e9 conhecida como <b>Latent Semantic Analysis (LSA)<\/b><\/p>","411c5ca1":"<p>Transformarmos novamente, mas desta vez com o n\u00famero de k componentes que obtemos anteriormente.<\/p>","33b1bc74":"<p><b>Term Frequency:<\/b> um termo que aparece muito em um documento, tende a ser um termo importante. Em resumo, divide-se o n\u00famero de vezes em que um termo apareceu pelo maior n\u00famero de vezes em que algum outro termo apareceu no documento.<\/p>\n\ntf<sub>wd<\/sub> = f<sub>wd<\/sub> \/ m<sub>wd<\/sub>\n\nonde:<br>\nf<sub>wd<\/sub> \u00e9 o n\u00famero de vezes em que o termo <i>w<\/i> aparece no documento <i>d<\/i>.<br>\nm<sub>wd<\/sub> \u00e9 o maior valor de f<sub>wd<\/sub> obtido para algum termo do documento <i>d<\/i><br>\n\n<p><b>Inverse Document Frequency:<\/b> um termo que aparece em poucos documentos pode ser um bom descriminante. Obtem-se dividindo o n\u00famero de documentos pelo n\u00famero de documentos em que o termo aparece.<\/p>\n\nidf<sub>w<\/sub> = log<sub>2<\/sub>(n \/ n<sub>w<\/sub>)\n\nonde:<br>\nn \u00e9 o n\u00famero de documentos no corpus\nn<sub>w<\/sub> \u00e9 o n\u00famero de documentos em que o termo <i>w<\/i> aparece.\n\nNa pr\u00e1tica, usa-se:\n\ntf-idf = tf<sub>wd<\/sub> * idf<sub>w<\/sub>\n\nPodemos calcular o TF-IDF de um corpus usando o pacote <b>scikit-learn<\/b>. Primeiramente, vamos abrir novamente o texto de Hamlet e armazenar as senten\u00e7as em uma ndarray do numpy (como se cada senten\u00e7a fosse um documento do corpus):","38d4416e":"<p><b>Exerc\u00edcio 3:<\/b>Escreva um chatbot que, dado uma pergunta em Ingl\u00eas, encontre uma pergunta mais parecida no corpus de perguntas e respostas dispon\u00edvel no Kaggle (https:\/\/www.kaggle.com\/rtatman\/questionanswer-dataset#S08_question_answer_pairs.txt) e exiba a pergunta original e a resposta.","1d7b79d0":"E essa fun\u00e7\u00e3o ser\u00e1 chamada pelo objeto TfidfVectorizer"}}