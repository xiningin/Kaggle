{"cell_type":{"efd4f72c":"code","9d567a16":"code","e5e8bd4e":"code","4cd228f2":"code","898b843d":"code","3095d723":"code","d8f1d4d4":"code","a4c7588d":"code","04bcbc01":"code","7cccec37":"code","04d4661f":"code","d7ed40e7":"code","ee7fda96":"code","842364fd":"code","206d8397":"code","8107548b":"code","f88b20cf":"code","cf0ce1cf":"code","70d24384":"code","c9299141":"code","048b5fdb":"code","7effc841":"code","94d1710f":"code","a82ffe58":"code","6731c938":"code","d4db6f44":"code","429c60c9":"code","8a463e5a":"code","a6ef8d99":"code","15728bb0":"code","3200a16e":"code","aed54355":"code","9b5121e3":"code","ffeecfc1":"code","523b04aa":"code","9a29d774":"code","f2d439ed":"code","0c00397e":"code","a2d96206":"markdown","a2b73c3e":"markdown","afd4d8de":"markdown","de087a97":"markdown","7735e630":"markdown","0f5e10bc":"markdown","6c33b4f5":"markdown","c8eb8850":"markdown","494fe373":"markdown","9e39092c":"markdown","67acb8ad":"markdown","6e829e07":"markdown","68486448":"markdown","6706a8ee":"markdown","e7a62249":"markdown","32e4ec00":"markdown","a86312c2":"markdown","79fb06d8":"markdown","21b3c11a":"markdown","47f0bd7a":"markdown","2f2ff362":"markdown","f91a03c8":"markdown","0cfcc200":"markdown","5a2c381d":"markdown"},"source":{"efd4f72c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9d567a16":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\nfrom wordcloud import STOPWORDS\nimport seaborn as sns\n# from nltk import WordNetLemmatizer\nimport re\nimport string\nimport os\nimport seaborn as sb\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer\nimport matplotlib.pyplot as plt\nfrom scipy.sparse import coo_matrix, hstack\nfrom sklearn import preprocessing\nfrom tqdm import tqdm","e5e8bd4e":"train = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/test.csv')","4cd228f2":"train.head()","898b843d":"print(\"Train shape :\", train.shape)\nprint(\"Test shape :\", test.shape)","3095d723":"train.info()","d8f1d4d4":"print(\"T\u1ed5ng s\u1ed1 d\u1eef li\u1ec7u trong t\u1eadp train: \",train.shape[0])\nprint(\"S\u1ed1 c\u00e2u h\u1ecfi b\u00ecnh th\u01b0\u1eddng: \", len(train[train.target == 0]))\nprint(\"S\u1ed1 c\u00e2u h\u1ecfi toxic: \",len(train[train.target == 1]))\nprint(\"T\u1ec9 l\u1ec7 gi\u1eefa 2 l\u1edbp: \",len(train[train.target == 1])\/len(train[train.target == 0]))\nprint('\\n')\n\nsb.countplot(train['target'])","a4c7588d":"#s\u1eed d\u1ee5ng tf-idf t\u1eeb th\u01b0 vi\u1ec7n sklearn\ntfidf = TfidfVectorizer(ngram_range=(1, 3))","04bcbc01":"def test_funtion_split():\n    X = train.question_text\n    y = train.target\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n    print(\"T\u1ec9 l\u1ec7 gi\u1eefa 2 l\u1edbp trong t\u1eadp train\",len(y_train[y_train == 0])\/len(y_train[y_train == 1]))\n    print(\"T\u1ec9 l\u1ec7 gi\u1eefa 2 l\u1edbp trong t\u1eadp test\",len(y_val[y_val == 0])\/len(y_val[y_val == 1]))\n\ntest_funtion_split()","7cccec37":"#H\u00e0m d\u1ef1 \u0111o\u00e1n s\u1eed d\u1ee5ng linear\ndef predict_linearSVC(X_train,y_train,X_test):\n    tfidf.fit(X_train)\n    X_train = tfidf.transform(X_train)\n    X_test = tfidf.transform(X_test)\n    svm = LinearSVC()\n    svm.fit(X_train,y_train)\n    return svm.predict(X_test)\n \n#D\u1ef1 \u0111o\u00e1n tr\u00ean t\u1eadp validation\ndef validate_base_model():\n    X = train.question_text\n    y = train.target\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    predict = predict_linearSVC(X_train,y_train,X_test)\n    return f1_score(predict,y_test)","04d4661f":"print('F1-Score c\u1ee7a base-model tr\u00ean t\u1eadp validation: ',validate_base_model())","d7ed40e7":"def create_file_submission(predict):\n    submission = pd.DataFrame(test['qid'])\n    submission['prediction'] = predict\n    submission.to_csv('submission.csv', index=False)\n    \ndef submit_base_model():\n    X_train = train['question_text']\n    y_train = train.target\n    X_test = test['question_text']\n    predict = predict_linearSVC(X_train,y_train,X_test)\n    create_file_submission(predict)","ee7fda96":"# submit_base_model()","842364fd":"train.question_text.iloc[0]","206d8397":"print(\"Math:\")\na = 0\nfor i in range(0,len(train)):\n    if '[math]' in train.question_text.iloc[i]:\n        print(train.question_text.iloc[i])\n        a = a + 1\n        if a == 5:\n            break\n            \nprint()\nprint(\"URL:\")\na = 0\nfor i in range(0,len(train)):\n    if 'https' in train.question_text.iloc[i]:\n        print(train.question_text.iloc[i])\n        a = a + 1\n        if a == 5:\n            break","8107548b":"# X\u1ee9 l\u00fd k\u00ed t\u1ef1 to\u00e1n h\u1ecdc v\u00e0 link URL\n\ndef clean_tag(x):\n  if '[math]' in x:\n    x = re.sub('\\[math\\].*?math\\]', 'MATH EQUATION', x) #replacing with [MATH EQUATION]    \n  if 'http' in x or 'www' in x:\n    x = re.sub('(?:(?:https?|ftp):\\\/\\\/)?[\\w\/\\-?=%.]+\\.[\\w\/\\-?=%.]+', 'URL', x) #replacing with [url]\n  return x","f88b20cf":"# Lo\u1ea1i b\u1ecf k\u00ed t\u1ef1 \u0111\u1eb7c bi\u1ec7t\n# Ngu\u1ed3n: https:\/\/www.kaggle.com\/canming\/ensemble-mean-iii-64-36\n\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', \n        '\u2022', '~', '@', '\u00a3', '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`', '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a', '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', \n        '\u2588', '\u2026', '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u25ba', '\u2212', '\u00a2', '\u00ac', '\u2591', '\u00a1', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', \n        '\u2014', '\u2039', '\u2500', '\u2592', '\uff1a', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00b8', '\u22c5', '\u2018', '\u221e', \n        '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u2264', '\u2021', '\u221a', '\u25c4', '\u2501', \n        '\u21d2', '\u25b6', '\u2265', '\u255d', '\u2661', '\u25ca', '\u3002', '\u2708', '\u2261', '\u263a', '\u2714', '\u21b5', '\u2248', '\u2713', '\u2663', '\u260e', '\u2103', '\u25e6', '\u2514', '\u201f', '\uff5e', '\uff01', '\u25cb', \n        '\u25c6', '\u2116', '\u2660', '\u258c', '\u273f', '\u25b8', '\u2044', '\u25a1', '\u2756', '\u2726', '\uff0e', '\u00f7', '\uff5c', '\u2503', '\uff0f', '\uffe5', '\u2560', '\u21a9', '\u272d', '\u2590', '\u263c', '\u263b', '\u2510', \n        '\u251c', '\u00ab', '\u223c', '\u250c', '\u2109', '\u262e', '\u0e3f', '\u2266', '\u266c', '\u2727', '\u232a', '\uff0d', '\u2302', '\u2716', '\uff65', '\u25d5', '\u203b', '\u2016', '\u25c0', '\u2030', '\\x97', '\u21ba', \n        '\u2206', '\u2518', '\u252c', '\u256c', '\u060c', '\u2318', '\u2282', '\uff1e', '\u2329', '\u2399', '\uff1f', '\u2620', '\u21d0', '\u25ab', '\u2217', '\u2208', '\u2260', '\u2640', '\u2654', '\u02da', '\u2117', '\u2517', '\uff0a', \n        '\u253c', '\u2740', '\uff06', '\u2229', '\u2642', '\u203f', '\u2211', '\u2023', '\u279c', '\u251b', '\u21d3', '\u262f', '\u2296', '\u2600', '\u2533', '\uff1b', '\u2207', '\u21d1', '\u2730', '\u25c7', '\u266f', '\u261e', '\u00b4', \n        '\u2194', '\u250f', '\uff61', '\u25d8', '\u2202', '\u270c', '\u266d', '\u2523', '\u2534', '\u2513', '\u2728', '\\xa0', '\u02dc', '\u2765', '\u252b', '\u2120', '\u2712', '\uff3b', '\u222b', '\\x93', '\u2267', '\uff3d', \n        '\\x94', '\u2200', '\u265b', '\\x96', '\u2228', '\u25ce', '\u21bb', '\u21e9', '\uff1c', '\u226b', '\u2729', '\u272a', '\u2655', '\u061f', '\u20a4', '\u261b', '\u256e', '\u240a', '\uff0b', '\u2508', '\uff05', \n        '\u254b', '\u25bd', '\u21e8', '\u253b', '\u2297', '\uffe1', '\u0964', '\u2582', '\u272f', '\u2587', '\uff3f', '\u27a4', '\u271e', '\uff1d', '\u25b7', '\u25b3', '\u25d9', '\u2585', '\u271d', '\u2227', '\u2409', '\u262d', \n        '\u250a', '\u256f', '\u263e', '\u2794', '\u2234', '\\x92', '\u2583', '\u21b3', '\uff3e', '\u05f3', '\u27a2', '\u256d', '\u27a1', '\uff20', '\u2299', '\u2622', '\u02dd', '\u220f', '\u201e', '\u2225', '\u275d', '\u2610', \n        '\u2586', '\u2571', '\u22d9', '\u0e4f', '\u2601', '\u21d4', '\u2594', '\\x91', '\u279a', '\u25e1', '\u2570', '\\x85', '\u2662', '\u02d9', '\u06de', '\u2718', '\u272e', '\u2611', '\u22c6', '\u24d8', '\u2752', \n        '\u2623', '\u2709', '\u230a', '\u27a0', '\u2223', '\u2751', '\u25e2', '\u24d2', '\\x80', '\u3012', '\u2215', '\u25ae', '\u29bf', '\u272b', '\u271a', '\u22ef', '\u2669', '\u2602', '\u275e', '\u2017', '\u0702', '\u261c', \n        '\u203e', '\u271c', '\u2572', '\u2218', '\u27e9', '\uff3c', '\u27e8', '\u0387', '\u2717', '\u265a', '\u2205', '\u24d4', '\u25e3', '\u0361', '\u201b', '\u2766', '\u25e0', '\u2704', '\u2744', '\u2203', '\u2423', '\u226a', '\uff62', \n        '\u2245', '\u25ef', '\u263d', '\u220e', '\uff63', '\u2767', '\u0305', '\u24d0', '\u2198', '\u2693', '\u25a3', '\u02d8', '\u222a', '\u21e2', '\u270d', '\u22a5', '\uff03', '\u23af', '\u21a0', '\u06e9', '\u2630', '\u25e5', \n        '\u2286', '\u273d', '\u26a1', '\u21aa', '\u2741', '\u2639', '\u25fc', '\u2603', '\u25e4', '\u274f', '\u24e2', '\u22b1', '\u279d', '\u0323', '\u2721', '\u2220', '\uff40', '\u25b4', '\u2524', '\u221d', '\u264f', '\u24d0', \n        '\u270e', '\u037e', '\u2424', '\uff07', '\u2763', '\u2702', '\u2724', '\u24de', '\u262a', '\u2734', '\u2312', '\u02db', '\u2652', '\uff04', '\u2736', '\u25bb', '\u24d4', '\u25cc', '\u25c8', '\u275a', '\u2742', '\uffe6', \n        '\u25c9', '\u255c', '\u0303', '\u2731', '\u2556', '\u2749', '\u24e1', '\u2197', '\u24e3', '\u267b', '\u27bd', '\u05c0', '\u2732', '\u272c', '\u2609', '\u2589', '\u2252', '\u2625', '\u2310', '\u2668', '\u2715', '\u24dd', \n        '\u22b0', '\u2758', '\uff02', '\u21e7', '\u0335', '\u27aa', '\u2581', '\u258f', '\u2283', '\u24db', '\u201a', '\u2670', '\u0301', '\u270f', '\u23d1', '\u0336', '\u24e2', '\u2a7e', '\uffe0', '\u274d', '\u2243', '\u22f0', '\u264b', \n        '\uff64', '\u0302', '\u274b', '\u2733', '\u24e4', '\u2564', '\u2595', '\u2323', '\u2738', '\u212e', '\u207a', '\u25a8', '\u2568', '\u24e5', '\u2648', '\u2743', '\u261d', '\u273b', '\u2287', '\u227b', '\u2658', '\u265e', \n        '\u25c2', '\u271f', '\u2320', '\u2720', '\u261a', '\u2725', '\u274a', '\u24d2', '\u2308', '\u2745', '\u24e1', '\u2667', '\u24de', '\u25ad', '\u2771', '\u24e3', '\u221f', '\u2615', '\u267a', '\u2235', '\u235d', '\u24d1', \n        '\u2735', '\u2723', '\u066d', '\u2646', '\u24d8', '\u2236', '\u269c', '\u25de', '\u0bcd', '\u2739', '\u27a5', '\u2195', '\u0333', '\u2237', '\u270b', '\u27a7', '\u220b', '\u033f', '\u0367', '\u2505', '\u2964', '\u2b06', '\u22f1', \n        '\u2604', '\u2196', '\u22ee', '\u06d4', '\u264c', '\u24db', '\u2555', '\u2653', '\u276f', '\u264d', '\u258b', '\u273a', '\u2b50', '\u273e', '\u264a', '\u27a3', '\u25bf', '\u24d1', '\u2649', '\u23e0', '\u25fe', '\u25b9', \n        '\u2a7d', '\u21a6', '\u2565', '\u2375', '\u230b', '\u0589', '\u27a8', '\u222e', '\u21e5', '\u24d7', '\u24d3', '\u207b', '\u239d', '\u2325', '\u2309', '\u25d4', '\u25d1', '\u273c', '\u264e', '\u2650', '\u256a', '\u229a', \n        '\u2612', '\u21e4', '\u24dc', '\u23a0', '\u25d0', '\u26a0', '\u255e', '\u25d7', '\u2395', '\u24e8', '\u261f', '\u24df', '\u265f', '\u2748', '\u21ac', '\u24d3', '\u25fb', '\u266e', '\u2759', '\u2664', '\u2209', '\u061b', \n        '\u2042', '\u24dd', '\u05be', '\u2651', '\u256b', '\u2553', '\u2573', '\u2b05', '\u2614', '\u2638', '\u2504', '\u2567', '\u05c3', '\u23a2', '\u2746', '\u22c4', '\u26ab', '\u030f', '\u260f', '\u279e', '\u0342', '\u2419', \n        '\u24e4', '\u25df', '\u030a', '\u2690', '\u2719', '\u2199', '\u033e', '\u2118', '\u2737', '\u237a', '\u274c', '\u22a2', '\u25b5', '\u2705', '\u24d6', '\u2628', '\u25b0', '\u2561', '\u24dc', '\u2624', '\u223d', '\u2558', \n        '\u02f9', '\u21a8', '\u2659', '\u2b07', '\u2671', '\u2321', '\u2800', '\u255b', '\u2755', '\u2509', '\u24df', '\u0300', '\u2656', '\u24da', '\u2506', '\u239c', '\u25dc', '\u26be', '\u2934', '\u2707', '\u255f', '\u239b', \n        '\u2629', '\u27b2', '\u279f', '\u24e5', '\u24d7', '\u23dd', '\u25c3', '\u2562', '\u21af', '\u2706', '\u02c3', '\u2374', '\u2747', '\u26bd', '\u2552', '\u0338', '\u265c', '\u2613', '\u27b3', '\u21c4', '\u262c', '\u2691', \n        '\u2710', '\u2303', '\u25c5', '\u25a2', '\u2750', '\u220a', '\u2608', '\u0965', '\u23ae', '\u25a9', '\u0bc1', '\u22b9', '\u2035', '\u2414', '\u260a', '\u27b8', '\u030c', '\u263f', '\u21c9', '\u22b3', '\u2559', '\u24e6', \n        '\u21e3', '\uff5b', '\u0304', '\u219d', '\u239f', '\u258d', '\u2757', '\u05f4', '\u0384', '\u259e', '\u25c1', '\u26c4', '\u21dd', '\u23aa', '\u2641', '\u21e0', '\u2607', '\u270a', '\u0bbf', '\uff5d', '\u2b55', '\u2798', \n        '\u2040', '\u2619', '\u275b', '\u2753', '\u27f2', '\u21c0', '\u2272', '\u24d5', '\u23a5', '\\u06dd', '\u0364', '\u208b', '\u0331', '\u030e', '\u265d', '\u2273', '\u2599', '\u27ad', '\u0700', '\u24d6', '\u21db', '\u258a', \n        '\u21d7', '\u0337', '\u21f1', '\u2105', '\u24e7', '\u269b', '\u0310', '\u0315', '\u21cc', '\u2400', '\u224c', '\u24e6', '\u22a4', '\u0313', '\u2626', '\u24d5', '\u259c', '\u2799', '\u24e8', '\u2328', '\u25ee', '\u2637', \n        '\u25cd', '\u24da', '\u2254', '\u23e9', '\u2373', '\u211e', '\u250b', '\u02fb', '\u259a', '\u227a', '\u0652', '\u259f', '\u27bb', '\u032a', '\u23ea', '\u0309', '\u239e', '\u2507', '\u235f', '\u21ea', '\u258e', '\u21e6', '\u241d', \n        '\u2937', '\u2256', '\u27f6', '\u2657', '\u0334', '\u2644', '\u0368', '\u0308', '\u275c', '\u0321', '\u259b', '\u2701', '\u27a9', '\u0bbe', '\u02c2', '\u21a5', '\u23ce', '\u23b7', '\u0332', '\u2796', '\u21b2', '\u2a75', '\u0317', '\u2762', \n        '\u224e', '\u2694', '\u21c7', '\u0311', '\u22bf', '\u0316', '\u260d', '\u27b9', '\u294a', '\u2041', '\u2722']\n\ndef clean_punct(x):\n  x = str(x)\n  for punct in puncts:\n    if punct in x:\n      x = x.replace(punct, ' ')\n  return x","cf0ce1cf":"def find_unique_words():\n    unique_words = {'a'}\n    non_unique_words = {'a'}\n    for i in tqdm(range(0,len(train))):\n        for word in train.question_text.iloc[i].split():\n            if word in unique_words:\n                non_unique_words.add(word)\n            else:\n                unique_words.add(word)\n    a = pd.DataFrame(unique_words - non_unique_words)\n    return a\n\nfind_unique_words().head(30)","70d24384":"# S\u1eeda t\u1eeb vi\u1ebft sai v\u00e0 \u0111\u01b0a m\u1ed9t s\u1ed1 t\u1eeb v\u1ec1 d\u1ea1ng th\u00f4ng d\u1ee5ng.\n# Ngu\u1ed3n: https:\/\/www.kaggle.com\/oysiyl\/107-place-solution-using-public-kernel\n\nmispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'bitcoin', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization', \n                'electroneum':'bitcoin','nanodegree':'degree','hotstar':'star','dream11':'dream','ftre':'fire','tensorflow':'framework','unocoin':'bitcoin',\n                'lnmiit':'limit','unacademy':'academy','altcoin':'bitcoin','altcoins':'bitcoin','litecoin':'bitcoin','coinbase':'bitcoin','cryptocurency':'cryptocurrency',\n                'simpliv':'simple','quoras':'quora','schizoids':'psychopath','remainers':'remainder','twinflame':'soulmate','quorans':'quora','brexit':'demonetized',\n                'iiest':'institute','dceu':'comics','pessat':'exam','uceed':'college','bhakts':'devotee','boruto':'anime',\n                'cryptocoin':'bitcoin','blockchains':'blockchain','fiancee':'fiance','redmi':'smartphone','oneplus':'smartphone','qoura':'quora','deepmind':'framework','ryzen':'cpu','whattsapp':'whatsapp',\n                'undertale':'adventure','zenfone':'smartphone','cryptocurencies':'cryptocurrencies','koinex':'bitcoin','zebpay':'bitcoin','binance':'bitcoin','whtsapp':'whatsapp',\n                'reactjs':'framework','bittrex':'bitcoin','bitconnect':'bitcoin','bitfinex':'bitcoin','yourquote':'your quote','whyis':'why is','jiophone':'smartphone',\n                'dogecoin':'bitcoin','onecoin':'bitcoin','poloniex':'bitcoin','7700k':'cpu','angular2':'framework','segwit2x':'bitcoin','hashflare':'bitcoin','940mx':'gpu',\n                'openai':'framework','hashflare':'bitcoin','1050ti':'gpu','nearbuy':'near buy','freebitco':'bitcoin','antminer':'bitcoin','filecoin':'bitcoin','whatapp':'whatsapp',\n                'empowr':'empower','1080ti':'gpu','crytocurrency':'cryptocurrency','8700k':'cpu','whatsaap':'whatsapp','g4560':'cpu','payymoney':'pay money',\n                'fuckboys':'fuck boys','intenship':'internship','zcash':'bitcoin','demonatisation':'demonetization','narcicist':'narcissist','mastuburation':'masturbation',\n                'trignometric':'trigonometric','cryptocurreny':'cryptocurrency','howdid':'how did','crytocurrencies':'cryptocurrencies','phycopath':'psychopath',\n                'bytecoin':'bitcoin','possesiveness':'possessiveness','scollege':'college','humanties':'humanities','altacoin':'bitcoin','demonitised':'demonetized',\n                'bras\u00edlia':'brazilia','accolite':'accolyte','econimics':'economics','varrier':'warrier','quroa':'quora','statergy':'strategy','langague':'language',\n                'splatoon':'game','7600k':'cpu','gate2018':'gate 2018','in2018':'in 2018','narcassist':'narcissist','jiocoin':'bitcoin','hnlu':'hulu','7300hq':'cpu',\n                'weatern':'western','interledger':'blockchain','deplation':'deflation', 'cryptocurrencies':'cryptocurrency', 'bitcoin':'blockchain cryptocurrency',}\n\ndef correct_mispell(x):\n  words = x.split()\n  for i in range(0, len(words)):\n    if mispell_dict.get(words[i]) is not None:\n      words[i] = mispell_dict.get(words[i])\n    elif mispell_dict.get(words[i].lower()) is not None:\n      words[i] = mispell_dict.get(words[i].lower())\n        \n  words = \" \".join(words)\n  return words","c9299141":"# B\u1ecf vi\u1ebft t\u1eaft\n# Ngu\u1ed3n: https:\/\/www.kaggle.com\/theoviel\/improve-your-score-with-text-preprocessing-v2 \n\ncontraction_mapping = {\"We'd\": \"We had\", \"That'd\": \"That had\", \"AREN'T\": \"Are not\", \"HADN'T\": \"Had not\", \"Could've\": \"Could have\", \"LeT's\": \"Let us\", \"How'll\": \"How will\", \"They'll\": \"They will\", \"DOESN'T\": \"Does not\", \"HE'S\": \"He has\", \"O'Clock\": \"Of the clock\", \"Who'll\": \"Who will\", \"What'S\": \"What is\", \"Ain't\": \"Am not\", \"WEREN'T\": \"Were not\", \"Y'all\": \"You all\", \"Y'ALL\": \"You all\", \"Here's\": \"Here is\", \"It'd\": \"It had\", \"Should've\": \"Should have\", \"I'M\": \"I am\", \"ISN'T\": \"Is not\", \"Would've\": \"Would have\", \"He'll\": \"He will\", \"DON'T\": \"Do not\", \"She'd\": \"She had\", \"WOULDN'T\": \"Would not\", \"She'll\": \"She will\", \"IT's\": \"It is\", \"There'd\": \"There had\", \"It'll\": \"It will\", \"You'll\": \"You will\", \"He'd\": \"He had\", \"What'll\": \"What will\", \"Ma'am\": \"Madam\", \"CAN'T\": \"Can not\", \"THAT'S\": \"That is\", \"You've\": \"You have\", \"She's\": \"She is\", \"Weren't\": \"Were not\", \"They've\": \"They have\", \"Couldn't\": \"Could not\", \"When's\": \"When is\", \"Haven't\": \"Have not\", \"We'll\": \"We will\", \"That's\": \"That is\", \"We're\": \"We are\", \"They're\": \"They' are\", \"You'd\": \"You would\", \"How'd\": \"How did\", \"What're\": \"What are\", \"Hasn't\": \"Has not\", \"Wasn't\": \"Was not\", \"Won't\": \"Will not\", \"There's\": \"There is\", \"Didn't\": \"Did not\", \"Doesn't\": \"Does not\", \"You're\": \"You are\", \"He's\": \"He is\", \"SO's\": \"So is\", \"We've\": \"We have\", \"Who's\": \"Who is\", \"Wouldn't\": \"Would not\", \"Why's\": \"Why is\", \"WHO's\": \"Who is\", \"Let's\": \"Let us\", \"How's\": \"How is\", \"Can't\": \"Can not\", \"Where's\": \"Where is\", \"They'd\": \"They had\", \"Don't\": \"Do not\", \"Shouldn't\":\"Should not\", \"Aren't\":\"Are not\", \"ain't\": \"is not\", \"What's\": \"What is\", \"It's\": \"It is\", \"Isn't\":\"Is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n\ndef clean_contractions(text):\n    specials = [\"\u2019\", \"\u2018\", \"\u00b4\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    \n    text = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in text.split(\" \")])\n    return text","048b5fdb":"# \u0110\u01b0a t\u1eeb \u0111\u01b0\u1ee3c chia v\u1ec1 d\u1ea1ng b\u00ecnh th\u01b0\u1eddng\n\n# lemmatizer = WordNetLemmatizer()\n# def lemma_text(x):\n#   x = x.split()\n#   x = [lemmatizer.lemmatize(word) for word in x]\n#   x = ' '.join(x)\n#   return x","7effc841":"# X\u00f3a stopword\nstopwords = STOPWORDS  - {'ought', 'whom',\"wouldn't\", \"you'll\", \"you've\"}# b\u1ecf \u0111i m\u1ed9t s\u1ed1 t\u1eeb khi\u1ebfn hi\u1ec7u qu\u1ea3 d\u1ef1 \u0111o\u00e1n gi\u1ea3m\n\n\ndef remove_stopwords(x):\n  x = [word for word in x.split() if word not in stopwords]\n  x = ' '.join(x)\n  return x","94d1710f":"# L\u1ea5y t\u1eeb stopword \u0111\u00e3 b\u1ecf trong c\u00e2u\ndef UncommonWords(A, B):\n  \n    count = {'a'}\n      \n    # insert words of string A to hash\n    for word in A.split():\n        if word not in B:\n            count.add(word)\n    # return required list of words\n    \n    return count","a82ffe58":"# T\u1ea1o b\u1ea3ng ph\u00e2n t\u00edch \u0111\u00e1nh gi\u00e1 \u1ea3nh h\u01b0\u1edfng stopword\ndef create_check_stopword_affection():\n    train['question_text_cleaned'] = train['question_text'].apply(lambda x: remove_stopwords(x))\n    test['question_text_cleaned'] = test['question_text'].apply(lambda x: remove_stopwords(x))\n    \n    X = train\n    y = train.target\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    result_predict_to_check_stopword = pd.DataFrame()\n    result_predict_to_check_stopword ['predict_with_stopword'] = predict_linearSVC(X_train.question_text,y_train,X_test.question_text)\n    result_predict_to_check_stopword ['predict_without_stopword'] = predict_linearSVC(X_train.question_text_cleaned,y_train,X_test.question_text_cleaned)\n    \n    a = pd.DataFrame(y_test).reset_index(drop=True, inplace=False)\n    result_predict_to_check_stopword = pd.concat([result_predict_to_check_stopword,a],axis = 1)\n    a = pd.DataFrame(X_test.question_text).reset_index(drop=True, inplace=False)\n    result_predict_to_check_stopword = pd.concat([result_predict_to_check_stopword,a],axis = 1)\n    a = pd.DataFrame(X_test.question_text_cleaned).reset_index(drop=True, inplace=False)\n    result_predict_to_check_stopword = pd.concat([result_predict_to_check_stopword,a],axis = 1)\n    \n    result_predict_to_check_stopword['check_with_stopword'] = result_predict_to_check_stopword.predict_with_stopword == result_predict_to_check_stopword.target\n    result_predict_to_check_stopword['check_without_stopword'] = result_predict_to_check_stopword.predict_without_stopword == result_predict_to_check_stopword.target\n    \n    result_predict_to_check_stopword.to_csv(\"check_stopword_affection.csv\",index = False)\n    \n    \ncreate_check_stopword_affection()","6731c938":"# file check_stopword_affection l\u01b0u trong out\/kaggle\/working s\u1eed d\u1ee5ng \u0111\u1ec3 \u0111\u00e1nh gi\u00e1 s\u1ef1 \u1ea3nh h\u01b0\u1edfng c\u1ee7a stopword\ncheck_stopword_affection = pd.read_csv(\".\/check_stopword_affection.csv\")\n\n\n# T\u1eadp h\u1ee3p c\u00e2u l\u00fac \u0111\u1ea7u \u0111o\u00e1n sai, sau khi b\u1ecf stopword \u0111o\u00e1n \u0111\u00fang\neff = check_stopword_affection[(check_stopword_affection.check_with_stopword == False) & (check_stopword_affection.check_without_stopword == True)]\n# T\u1eadp h\u1ee3p c\u00e2u l\u00fac \u0111\u1ea7u \u0111o\u00e1n \u0111\u00fang, sau khi b\u1ecf stopword \u0111o\u00e1n sai\nineff = check_stopword_affection[(check_stopword_affection.check_with_stopword == True) & (check_stopword_affection.check_without_stopword == False)]\n\n# T\u1eadp stopword \u0111\u00e3 b\u1ecf trong nh\u1eefng c\u00e2u \u0111o\u00e1n sai th\u00e0nh \u0111\u00fang (hi\u1ec7u qu\u1ea3)\nword_effective = {'a'}\nfor i in range(0,len(eff)):\n    word_effective.update(UncommonWords(eff.question_text.iloc[i],eff.question_text_cleaned.iloc[i]))\n\n# T\u1eadp stopword \u0111\u00e3 b\u1ecf trong nh\u1eefng c\u00e2u \u0111o\u00e1n \u0111\u00fang th\u00e0nh sai (kh\u00f4ng hi\u1ec7u qu\u1ea3)    \nword_not_effective = {'a'}\nfor i in range(0,len(ineff)):\n    word_not_effective.update(UncommonWords(ineff.question_text.iloc[i],ineff.question_text_cleaned.iloc[i]))\n    \n\nprint(\"Sau khi b\u1ecf stopword, s\u1ed1 l\u01b0\u1ee3ng c\u00e2u h\u1ecfi d\u1ef1 \u0111o\u00e1n \u0111\u00fang gi\u1ea3m:\",len(ineff) - len(eff))\nprint(\"S\u1ed1 c\u00e2u h\u1ecfi b\u1ecb \u1ea3nh h\u01b0\u1edfng (thay \u0111\u1ed5i k\u1ebft qu\u1ea3) b\u1edbi stopword: \",len(ineff) + len(eff))\nprint(\"T\u1ed5ng s\u1ed1 c\u00e2u h\u1ecfi: \",len(check_stopword_affection))\nprint(\"T\u1ed5ng s\u1ed1 c\u00e2u h\u1ecfi l\u1edbp 0: \",len(check_stopword_affection[check_stopword_affection.target == 0]))\nprint(\"T\u1ed5ng s\u1ed1 c\u00e2u h\u1ecfi l\u1edbp 1: \",len(check_stopword_affection[check_stopword_affection.target == 1]))\nprint(\"Trong \u0111\u00f3:\")\nprint(\"    - V\u1edbi c\u00e2u h\u1ecfi c\u00f3 target = 0 : \u0110\u00fang th\u00eam \",len(eff[eff.target == 0]),\", l\u00e0m sai m\u1ea5t: \",len(ineff[ineff.target == 0]))\nprint(\"    - V\u1edbi c\u00e2u h\u1ecfi c\u00f3 target = 1 : \u0110\u00fang th\u00eam \",len(eff[eff.target == 1]),\", l\u00e0m sai m\u1ea5t: \",len(ineff[ineff.target == 1]))\nprint()\nprint(\"Nh\u1eefng t\u1eeb stopword kh\u00f4ng hi\u1ec7u qu\u1ea3 (khi b\u1ecf \u0111i bi\u1ebfn c\u00e2u \u0111\u00fang th\u00e0nh c\u00e2u sai): \",word_not_effective - word_effective)","d4db6f44":"# G\u1ecdi t\u1ea5t c\u1ea3 c\u00e1c h\u00e0m ti\u1ec1n x\u1eed l\u00fd d\u1eef li\u1ec7u tr\u00ean\ndef data_cleaning(x):\n  x = clean_tag(x)\n  x = clean_punct(x)\n  x = correct_mispell(x)\n  x = clean_contractions(x)\n  x = remove_stopwords(x)\n#   x = lemma_text(x)\n  return x","429c60c9":"#T\u1ea1o ra 1 t\u1eadp d\u1eef li\u1ec7u \u0111\u00e3 \u0111\u01b0\u1ee3c cleaning\ntrain['question_text_cleaned'] = train['question_text'].apply(lambda x: data_cleaning(x))\ntest['question_text_cleaned'] = test['question_text'].apply(lambda x: data_cleaning(x))\ndisplay(train.head(),test.head())","8a463e5a":"def validate_baseModel_dataCleaded():\n    X = train.question_text_cleaned\n    y = train.target\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    predict = predict_linearSVC(X_train,y_train,X_test)\n    return f1_score(predict,y_test)\n\nprint(\"F1-Score tr\u00ean t\u1eadp d\u1eef li\u1ec7u \u0111\u00e3 x\u1eed l\u00fd: \",validate_baseModel_dataCleaded())","a6ef8d99":"def submit():\n    X_train = train['question_text_cleaned']\n    y_train = train.target\n    X_test = test['question_text_cleaned']\n    predict = predict_linearSVC(X_train,y_train,X_test)\n    create_file_submission(predict)\n    \n# submit()","15728bb0":"def validate_undersampling():\n    X_target0 = train[train.target == 0].sample(frac = 0.26) #l\u1ea5y random t\u1eadp data con t\u1eeb l\u1edbp 0 c\u00f3 k\u00edch th\u01b0\u1edbc b\u1eb1ng 0.26 l\u1edbp 0 ban \u0111\u1ea7u\n    data = X_target0.append(train[train.target == 1])  # gh\u00e9p v\u1edbi t\u1eadp data l\u1edbp 1\n    X = data.question_text_cleaned\n    y = data.target\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    predict = predict_linearSVC(X_train,y_train,X_test)\n    return f1_score(predict,y_test)\n    \nprint(\"F1-Score v\u1edbi Under Sampling: \",validate_undersampling())","3200a16e":"def submit_undersampling():\n    X_target0 = train[train.target == 0].sample(frac = 0.26) #l\u1ea5y random t\u1eadp data con t\u1eeb l\u1edbp 0\n    data = X_target0.append(train[train.target == 1])  # gh\u00e9p v\u1edbi t\u1eadp data l\u1edbp 1\n    X_train = data.question_text_cleaned\n    y_train = data.target\n    X_test = test.question_text_cleaned\n    predict = predict_linearSVC(X_train,y_train,X_test)\n    create_file_submission(predict)\n\n# submit_undersampling()","aed54355":"def remove_data():\n    \n    train_0 = train[train.target == 0] #l\u1ea5y ra t\u1eadp data target 0\n    tfidf.fit(train_0.question_text_cleaned) #chuy\u1ec3n text sang vector v\u1edbi tfidf\n    encode = tfidf.transform(train_0.question_text_cleaned)\n    kmeans = KMeans(n_clusters=14,verbose=1,max_iter=5,n_init=3) # s\u1ed1 cluster = 14,kh\u1edfi t\u1ea1o l\u1ea5y t\u00e2m ng\u1eabu nhi\u00ean 3 l\u1ea7n, m\u1ed7i l\u1ea7n t\u00ecm t\u00e2m t\u1ed1i \u01b0u 5 l\u1ea7n\n    kmeans.fit(encode)\n    train_0['cluster'] = kmeans.labels_ #t\u1ea1o c\u1ed9t \u0111\u00e1nh nh\u00e3n t\u00ean c\u1ee5m\n\n    #m\u1ed7i c\u1ee5m l\u1ea5y ra 1\/4 data \u0111\u1ec3\n    a = train_0[train_0.cluster==0] \n    data = a.sample(frac = 0.25)\n    for i in range(1,14):\n        a = train_0[train_0.cluster==i]\n        b = a.sample(frac = 0.25)\n        data = data.append(b)\n\n    data.drop('cluster',axis='columns',inplace=True)\n\n    data = data.append(train[train.target == 1])#g\u1ed9p t\u1eadp l\u1edbp 0 sau khi gi\u1ea3m b\u1edbt d\u1eef li\u1ec7u v\u1edbi t\u1eadp d\u1eef li\u1ec7u l\u1edbp 1\n    return data\n\ndata_removed =  remove_data()","9b5121e3":"def validation_undersampling_kmean():\n    X = data_removed.question_text_cleaned\n    y = data_removed.target\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    predict = predict_linearSVC(X_train,y_train,X_test)\n    return f1_score(predict,y_test)\n\nprint(\"F1-Score v\u1edbi Under Sampling v\u00e0 K-mean: \",validation_undersampling_kmean())","ffeecfc1":"def submit_undersampling_kmean():\n    X_train = data_removed.question_text_cleaned\n    y_train = data_removed.target\n    X_test = test.question_text_cleaned\n    predict = predict_linearSVC(X_train,y_train,X_test)\n    create_file_submission(predict)\n\n# submit_undersampling_kmean()","523b04aa":"def generate_feature():\n    train['qlen'] = train['question_text'].str.len() \n    train['n_words'] = train['question_text'].apply(lambda row: len(row.split(\" \")))\n    train['numeric_words'] = train['question_text'].apply(lambda row: sum(c.isdigit() for c in row))\n    train['sp_char_words'] = train['question_text'].str.findall(r'[^a-zA-Z0-9 ]').str.len()\n    train['char_words'] = train['question_text'].apply(lambda row: len(str(row)))\n    train['unique_words'] = train['question_text'].apply(lambda row: len(set(str(row).split())))\n    train['stopwords'] = train['question_text'].apply(lambda x: len([c for c in str(x).lower().split() if c in STOPWORDS]))\n    test['qlen'] = test['question_text'].str.len() \n    test['n_words'] = test['question_text'].apply(lambda row: len(row.split(\" \")))\n    test['numeric_words'] = test['question_text'].apply(lambda row: sum(c.isdigit() for c in row))\n    test['sp_char_words'] = test['question_text'].str.findall(r'[^a-zA-Z0-9 ]').str.len()\n    test['char_words'] = test['question_text'].apply(lambda row: len(str(row)))\n    test['unique_words'] = test['question_text'].apply(lambda row: len(set(str(row).split())))\n    test['stopwords'] = test['question_text'].apply(lambda x: len([c for c in str(x).lower().split() if c in STOPWORDS])-1)\n    \ngenerate_feature()\ndisplay(train.head(),test.head())","9a29d774":"f, ax = plt.subplots(figsize=(11, 9))\n \ncorr = train.drop(['qid','question_text','question_text_cleaned'],axis='columns').corr() # t\u00ednh \u0111\u1ed9 t\u01b0\u01a1ng quan gi\u1eefa 2 c\u00e1c c\u1eb7p d\u1eef li\u1ec7u, c\u00e0ng g\u1ea7n 0 \u0111\u1ed9 t\u01b0\u01a1ng quan c\u00e0ng th\u1ea5p\n\n# Draw the heatmap\nsns.heatmap(corr, ax=ax)\n\nplt.title(\"Correlation matrix\")\nplt.show()","f2d439ed":"def validate_with_new_feature():\n    X = train[['question_text_cleaned','qlen','n_words','numeric_words','sp_char_words','char_words','unique_words','stopwords']]\n    y = train.target\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    tfidf.fit(X_train.question_text_cleaned)\n\n    A_train = tfidf.transform(X_train.question_text_cleaned)\n    A_test = tfidf.transform(X_test.question_text_cleaned)\n\n    scaler = preprocessing.MinMaxScaler()\n    print(scaler)\n    scaled_train = scaler.fit_transform(X_train[['qlen','n_words','numeric_words','sp_char_words','unique_words','stopwords']])\n    scaled_test = scaler.fit_transform(X_test[['qlen','n_words','numeric_words','sp_char_words','unique_words','stopwords']])\n\n    X_train = hstack([A_train,coo_matrix(scaled_train)])\n    X_test = hstack([A_test,coo_matrix(scaled_test)])\n    svm = LinearSVC()\n    svm.fit(X_train,y_train)\n    return f1_score(svm.predict(X_test),y_test)\n\nprint(\"F1-Score v\u1edbi new Feature: \",validate_with_new_feature())","0c00397e":"def submit_with_new_feature():\n\n    tfidf.fit(train.question_text_cleaned)\n\n    A_train = tfidf.transform(train.question_text_cleaned)\n    A_test = tfidf.transform(test.question_text_cleaned)\n\n    scaler = preprocessing.MinMaxScaler()\n    print(scaler)\n    scaled_train = scaler.fit_transform(train[['qlen','n_words','numeric_words','sp_char_words','unique_words','stopwords']])\n    scaled_test = scaler.fit_transform(test[['qlen','n_words','numeric_words','sp_char_words','unique_words','stopwords']])\n\n    X_train = hstack([A_train,coo_matrix(scaled_train)])\n    X_test = hstack([A_test,coo_matrix(scaled_test)])\n    svm = LinearSVC()\n    svm.fit(X_train,train.target)\n    predict = svm.predict(X_test)\n\n    create_file_submission(predict)\n    \nsubmit_with_new_feature()","a2d96206":"Th\u1eed chia t\u1eadp validation v\u1edbi h\u00e0m train_test_split ta c\u00f3 th\u1ec3 th\u1ea5y r\u00f5 t\u1ec9 l\u1ec7 gi\u1eefa 2 l\u1edbp \u1edf t\u1eadp train v\u00e0 validation l\u00e0 kh\u00e1 t\u01b0\u01a1ng \u0111\u1ed3ng v\u1edbi t\u1ec9 l\u1ec7 c\u1ee7a t\u1eadp train l\u00fac ban \u0111\u1ea7u.\n\n**3.4. X\u00e2y d\u1ef1ng m\u00f4 h\u00ecnh**","a2b73c3e":"**Nh\u1eadn x\u00e9t:**\nS\u1ed1 l\u01b0\u1ee3ng c\u00e2u h\u1ecfi sincerce:insincere c\u00f3 s\u1ef1 ch\u00eanh l\u1ec7ch r\u1ea5t l\u1edbn. T\u1ec9 l\u1ec7 d\u1eef li\u1ec7u gi\u1eefa 2 nh\u00f3m l\u00e0 1:15 .S\u1ed1 l\u01b0\u1ee3ng c\u00e2u h\u1ecfi ch\u00e2n th\u00e0nh chi\u1ebfm \u0111\u1ebfn g\u1ea7n 94% t\u1eadp d\u1eef li\u1ec7u trong khi s\u1ed1 l\u01b0\u1ee3ng c\u00e2u h\u1ecfi kh\u00f4ng ch\u00e2n th\u00e0nh ch\u1ec9 chi\u1ebfm 6.2%.S\u1ef1 m\u1ea5t c\u00e2n b\u1eb1ng nghi\u00eam tr\u1ecdng n\u00e0y s\u1ebd d\u1eabn t\u1edbi 2 v\u1ea5n \u0111\u1ec1 l\u1edbn:\n\n* \u0110\u00e1nh gi\u00e1 sai ch\u1ea5t l\u01b0\u1ee3ng m\u00f4 h\u00ecnh: s\u1ef1 m\u1ea5t c\u00e2n b\u1eb1ng c\u00f3 th\u1ec3 khi\u1ebfn th\u01b0\u1edbc \u0111o \u0111\u00e1nh gi\u00e1 m\u00f4 h\u00ecnh l\u00e0 \u0111\u1ed9 ch\u00ednh x\u00e1c (accuracy) c\u00f3 th\u1ec3 \u0111\u1ea1t \u0111\u01b0\u1ee3c r\u1ea5t cao m\u00e0 kh\u00f4ng c\u1ea7n t\u1edbi m\u00f4 h\u00ecnh. Minh ch\u1ee9ng \u0111\u01a1n gi\u1ea3n v\u1edbi b\u1ed9 d\u1eef li\u1ec7u tr\u00ean th\u00ec n\u1ebfu k\u1ebft qu\u1ea3 d\u1ef1 \u0111o\u00e1n \u0111\u01b0a ra t\u1ea5t c\u1ea3 \u0111\u1ec1u c\u00f3 target = 0 th\u00ec \u0111\u1ed9 ch\u00ednh x\u00e1c \u0111\u00e3 \u0111\u1ea1t 94%.\n* M\u00f4 h\u00ecnh d\u1ef1 \u0111o\u00e1n k\u00e9m ch\u00ednh x\u00e1c: s\u1ef1 m\u1ea5t c\u00e2n b\u1eb1ng c\u00f3 th\u1ec3 g\u00e2y hi\u1ec7n t\u01b0\u1ee3ng Overfitting, khi\u1ebfn k\u1ebft qu\u1ea3 d\u1ef1 \u0111o\u00e1n ra th\u01b0\u1eddng nghi\u00eang v\u1ec1 nh\u00f3m \u0111a s\u1ed1 v\u00e0 k\u00e9m tr\u00ean nh\u00f3m thi\u1ec3u s\u1ed1 trong khi t\u1ea7m quan tr\u1ecdng c\u1ee7a vi\u1ec7c d\u1ef1 b\u00e1o \u0111\u01b0\u1ee3c ch\u00ednh x\u00e1c m\u1ed9t m\u1eabu thu\u1ed9c nh\u00f3m thi\u1ec3u s\u1ed1 l\u1edbn h\u01a1n nhi\u1ec1u so v\u1edbi d\u1ef1 b\u00e1o m\u1eabu thu\u1ed9c nh\u00f3m \u0111a s\u1ed1. ( VD \u1edf \u0111\u00e2y vi\u1ec7c quan tr\u1ecdng nh\u1ea5t l\u00e0 x\u00e1c \u0111\u1ecbnh \u0111\u01b0\u1ee3c nh\u00f3m c\u00e2u h\u1ecfi toxic)\n\n-> D\u1eef li\u1ec7u n\u00e0y m\u1ea5t c\u00e2n b\u1eb1ng r\u1ea5t l\u1edbn\n\n-> C\u1ea7n \u00e1p d\u1ee5ng c\u00e1c ph\u01b0\u01a1ng ph\u00e1p c\u00e2n b\u1eb1ng d\u1eef li\u1ec7u\n\n-> F1-score s\u1ebd \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 \u0111\u00e1nh gi\u00e1 hi\u1ec7u n\u0103ng c\u1ee7a m\u00f4 h\u00ecnh. F1 score l\u00e0 \u0111\u1ed9 c\u00e2n b\u1eb1ng \u0111\u1ed3ng \u0111\u1ec1u gi\u1eefa precision v\u00e0 recall","afd4d8de":"# **3. CHU\u1ea8N B\u1eca M\u00d4 H\u00ccNH**\n\nTr\u01b0\u1edbc khi th\u1ef1c hi\u1ec7n b\u1ea5t c\u1ee9 qu\u00e1 tr\u00ecnh ti\u1ec1n x\u1eed l\u00fd d\u1eef li\u1ec7u hay \u00e1p d\u1ee5ng c\u00e1c model kh\u00e1c nhau cho b\u00e0i to\u00e1n th\u00ec ta ngh\u0129 t\u1edbi vi\u1ec7c \u0111\u01b0a ra m\u1ed9t gi\u1ea3i ph\u00e1p n\u1ec1n t\u1ea3ng \u0111\u1ec3 t\u1eeb \u0111\u00f3 ph\u00e1t tri\u1ec3n v\u00e0 c\u00f3 th\u1ec3 d\u1ec5 d\u00e0ng \u0111\u00e1nh gi\u00e1 \u0111\u1ed9 hi\u1ec7u qu\u1ea3 c\u1ee7a c\u00e1c thay \u0111\u1ed5i sau n\u00e0y. V\u00ec th\u1ebf, tr\u01b0\u1edbc ti\u00ean, ta s\u1ebd x\u00e2y d\u1ef1ng m\u1ed9t m\u00f4 h\u00ecnh \u0111\u01a1n gi\u1ea3n \u0111\u1ec3 d\u1ef1 \u0111o\u00e1n b\u00e0i to\u00e0n tr\u00ean.\n\n**3.1. TF-IDF** \n\nVi\u1ec7c \u0111\u1ea7u ti\u00ean c\u1ea7n l\u00e0m l\u00e0 x\u1eed l\u00fd d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o. V\u1edbi d\u1eef li\u1ec7u d\u1ea1ng text, ch\u00fang ta c\u1ea7n ti\u1ebfn h\u00e0nh encoding \u0111\u1ec3 \u0111\u01b0a d\u1eef li\u1ec7u v\u1ec1 d\u1ea1ng kh\u00f4ng gian vector. C\u00e1ch \u0111\u01a1n gi\u1ea3n nh\u1ea5t l\u00e0 s\u1eed d\u1ee5ng one-hot encoding. Tuy nhi\u00ean, one-hot encoding mang l\u1ea1i \u00edt gi\u00e1 tr\u1ecb th\u00f4ng tin v\u00e0 c\u00f3 \u0111\u1ed9 kh\u00e1i qu\u00e1t y\u1ebfu. Ta \u0111\u01b0a ra m\u1ed9t gi\u1ea3i ph\u00e1p t\u1ed1t h\u01a1n 1 ch\u00fat \u0111\u00f3 l\u00e0 s\u1eed d\u1ee5ng TF-IDF.\n\nnh\u1eefng t\u1eeb c\u00f3 gi\u00e1 tr\u1ecb TF-IDF cao l\u00e0 nh\u1eefng t\u1eeb xu\u1ea5t hi\u1ec7n nhi\u1ec1u trong v\u0103n b\u1ea3n n\u00e0y, v\u00e0 xu\u1ea5t hi\u1ec7n \u00edt trong c\u00e1c v\u0103n b\u1ea3n kh\u00e1c. Vi\u1ec7c n\u00e0y gi\u00fap l\u1ecdc ra nh\u1eefng t\u1eeb ph\u1ed5 bi\u1ebfn v\u00e0 gi\u1eef l\u1ea1i nh\u1eefng t\u1eeb c\u00f3 gi\u00e1 tr\u1ecb cao (t\u1eeb kho\u00e1 c\u1ee7a v\u0103n b\u1ea3n \u0111\u00f3).","de087a97":"![download.png](attachment:24719a62-0017-41cd-a1a1-daa8f62efd7b.png)\n\n**Nh\u1eadn x\u00e9t:**\n\u0110i\u1ec3m s\u1ed1 \u0111\u1ea1t \u0111\u01b0\u1ee3c tr\u00ean t\u1eadp test kh\u00e1 t\u01b0\u01a1ng \u0111\u1ed3ng v\u1edbi \u0111i\u1ec3m s\u1ed1 tr\u00ean t\u1eadp validation cho th\u1ea5y t\u1eadp validation \u0111\u00e3 \u0111\u00e1nh gi\u00e1 ch\u00ednh x\u00e1c hi\u1ec7u n\u0103ng c\u1ee7a model \u0111\u1ed3ng th\u1eddi cho th\u1ea5y m\u00f4 h\u00ecnh kh\u00f4ng x\u1ea3y ra c\u00e1c hi\u1ec7n t\u01b0\u1ee3ng underfiting, overfiting.\n\n\n# **4.X\u1eec L\u00dd D\u1eee LI\u1ec6U**\n","7735e630":"Lo\u1ea1i b\u1ecf stopword kh\u00f4ng g\u00e2y qu\u00e1 nhi\u1ec1u \u1ea3nh h\u01b0\u1edfng t\u1edbi d\u1ef1 \u0111o\u00e1n c\u1ee7a m\u00f4 h\u00ecnh (kho\u1ea3ng tr\u00ean 2%). Sau khi b\u1ecf stopword, s\u1ed1 c\u00e2u \u0111o\u00e1n \u0111\u00fang gi\u1ea3m 750 c\u00e2u, tuy nhi\u00ean s\u1ed1 li\u1ec7u \u1edf tr\u00ean gi\u00fap ta ph\u00e2n t\u00edch k\u0129 h\u01a1n v\u1edbi t\u1eebng l\u1edbp:\n\n* V\u1edbi c\u00e2u h\u1ecfi target = 1 (l\u1edbp thi\u1ec3u s\u1ed1), s\u1ed1 c\u00e2u h\u1ecfi d\u1ef1 \u0111o\u00e1n \u0111\u00fang t\u0103ng kh\u00e1 \u0111\u00e1ng k\u1ec3.\n* S\u1ed1 c\u00e2u h\u1ecfi \u0111\u01b0\u1ee3c \u0111o\u00e1n thu\u1ed9c l\u1edbp 1 c\u00f3 xu h\u01b0\u01a1ng t\u0103ng ( h\u01a1n 1800 c\u00e2u so v\u1edbi tr\u01b0\u1edbc khi b\u1ecf stop word).\n* V\u1edbi l\u1edbp 0, l\u01b0\u1ee3ng \u0111o\u00e1n \u0111\u00fang t\u0103ng 0.48% v\u00e0 l\u01b0\u1ee3ng \u0111o\u00e1n sai t\u0103ng 1%.\n* V\u1edbi l\u1edbp 1, l\u01b0\u1ee3ng \u0111o\u00e1n \u0111\u00fang t\u0103ng 8.16% v\u00e0 l\u01b0\u1ee3ng \u0111o\u00e1n sai t\u0103ng 4,7%.\nV\u1edbi d\u1eef li\u1ec7u \u0111\u1ed9 l\u1ec7ch l\u1edbn nh\u01b0 hi\u1ec7n t\u1ea1i, d\u1ef1 \u0111o\u00e1n s\u1ebd c\u00f3 xu h\u01b0\u1edbng l\u1ec7ch v\u1ec1 l\u1edbp \u0111a s\u1ed1 (l\u1edbp 0). S\u1ed1 li\u1ec7u tr\u00ean cho th\u1ea5y, l\u1edbp 1 c\u00f3 xu h\u01b0\u1edbng v\u00e0 t\u1ec9 l\u1ec7 d\u1ef1 \u0111o\u00e1n \u0111\u00fang t\u0103ng , \u0111i\u1ec1u \u0111\u00f3 ch\u1ee9ng t\u1ecf hi\u1ec7u qu\u1ea3 d\u1ef1 \u0111o\u00e1n l\u1edbp thi\u1ec3u s\u1ed1 \u0111ang \u0111\u01b0\u1ee3c t\u0103ng l\u00ean.\n\nS\u1ed1 l\u01b0\u1ee3ng c\u00e2u \u0111o\u00e1n \u0111\u00fang gi\u1ea3m nh\u01b0ng \u0111\u00f3 l\u00e0 tr\u00ean b\u1ed9 d\u1eef li\u1ec7u thi\u1ebfu c\u00e2n b\u1eb1ng. V\u1edbi s\u1ed1 li\u1ec7u \u1edf d\u1ea1ng % ta c\u00f3 th\u1ec3 th\u1ea5y n\u1ebfu b\u1ed9 d\u1eef li\u1ec7u c\u00f3 t\u1ec9 l\u1ec7 ngang nhau gi\u1eefa 2 l\u1edbp, lo\u1ea1i b\u1ecf stopword c\u00f3 th\u1ec3 gi\u00fap t\u0103ng g\u1ea7n 3% s\u1ed1 c\u00e2u tr\u1ea3 l\u1eddi \u0111\u00fang. \u0110\u00e2y l\u00e0 con s\u1ed1 kh\u1ea3 quan ch\u1ee9ng t\u1ecf vi\u1ec7c lo\u1ea1i b\u1ecf stopword l\u00e0 c\u00f3 hi\u1ec7u qu\u1ea3.\n\nM\u1ed9t s\u1ed1 stopword kh\u00f4ng hi\u1ec7u qu\u1ea3 \u0111\u01b0\u1ee3c l\u1ecdc ra ph\u00eda tr\u00ean \u0111\u1ec3 lo\u1ea1i b\u1ecf kh\u1ecfi b\u1ed9 stopword.\n\n# * D\u1eef li\u1ec7u \u0111\u01b0\u1ee3c x\u1eed l\u00fd","0f5e10bc":"Ta t\u00ecm ra nh\u1eefng t\u1eeb ch\u1ec9 xu\u1ea5t hi\u1ec7n 1 l\u1ea7n tr\u00ean to\u00e0n t\u1eadp train v\u00e0 th\u1ea5y r\u1eb1ng trong \u0111\u00f3 bao g\u1ed3m:\n\n* C\u00e1c t\u1eeb b\u1ecb vi\u1ebft sai ch\u00ednh t\u1ea3.\n* T\u00ean g\u1ecdi c\u1ee7a ng\u01b0\u1eddi.\n* T\u00ean g\u1ecdi trong m\u1ed9t s\u1ed1 l\u0129nh v\u1ef1c kh\u00e1c nhau nh\u01b0 t\u00ean con ch\u00edp, t\u00ean c\u00e1c lo\u1ea1i ti\u1ec1n \u1ea3o, t\u00ean app,....\n* C\u00e1c t\u1eeb b\u1ecb d\u00ednh b\u1edfi d\u1ea5u c\u00e2u, k\u00ed t\u1ef1 \u0111\u1eb7c bi\u1ec7t (vd: shop?) - v\u1ea5n \u0111\u1ec1 n\u00e0y s\u1ebd \u0111\u01b0\u1ee3c lo\u1ea1i b\u1ecf v\u1edbi h\u00e0m clean punct.\n\nTa ti\u1ebfn h\u00e0nh s\u1eeda c\u00e1c t\u1eeb vi\u1ebft sai ch\u00ednh t\u1ea3. \u0110\u1ed1i v\u1edbi c\u00e1c t\u00ean g\u1ecdi trong c\u00e1c l\u0129nh v\u1ef1c kh\u00e1c nhau, ta \u0111\u01b0a chung ch\u00fang v\u1ec1 m\u1ed9t d\u1ea1ng \u0111\u1ec3 khi m\u00e3 h\u00f3a ch\u00fang s\u1ebd th\u1ec3 hi\u1ec7n c\u00f9ng m\u1ed9t ph\u00e2n b\u1ed1 (v\u00ed d\u1ee5 nh\u01b0 t\u00ean gpu \"1080ti\" ta chuy\u1ec3n v\u1ec1 \"GPU\" hay t\u00ean m\u1ed9t s\u1ed1 lo\u1ea1i ti\u1ec1n \u1ea3o ta chuy\u1ec3n chung v\u1ec1 \"bitcoin\") trong kh\u00f4ng gian vector (m\u00f4 h\u00ecnh s\u1ebd hi\u1ec3u c\u00e1c t\u1eeb \u0111\u00f3 mang c\u00f9ng \u00fd ngh\u0129a, t\u00ednh ch\u1ea5t).","6c33b4f5":"Ta th\u1ea5y r\u1eb1ng nhi\u1ec1u c\u00e2u h\u1ecfi c\u00f3 ch\u1ee9a c\u00e1c bi\u1ec3u th\u1ee9c to\u00e1n h\u1ecdc v\u00e0 \u0111\u01b0\u1eddng d\u1eabn URL. C\u00e1c bi\u1ec3u th\u1ee9c v\u00e0 \u0111\u01b0\u1eddng d\u1eabn g\u1ea7n nh\u01b0 kh\u00f4ng c\u00f3 \u00fd ngh\u0129a trong vi\u1ec7c ph\u00e2n lo\u1ea1i c\u00e2u h\u1ecfi v\u00e0 vi\u1ec7c \u0111\u1ec3 nguy\u00ean c\u00e1c \u0111\u01b0\u1eddng d\u1eabn v\u00e0 bi\u1ec3u th\u1ee9c to\u00e1n n\u00e0y trong qu\u00e1 tr\u00ecnh encoding v\u1edbi Tfidf s\u1ebd bi\u1ebfn th\u00e0nh c\u00e1c tr\u01b0\u1eddng kh\u00e1c nhau d\u1ec5 g\u00e2y ra nhi\u1ec5u \u1ea3nh h\u01b0\u1edfng t\u1edbi qu\u00e1 tr\u00ecnh hu\u1ea5n luy\u1ec7n v\u00e0 d\u1ef1 \u0111o\u00e1n.\n\nTa ti\u1ebfn h\u00e0nh chuy\u1ec3n c\u00e1c bi\u1ec3u th\u1ee9c to\u00e1n v\u1ec1 \"MATH EQUATION\" v\u00e0 \u0111\u01b0\u1eddng d\u1eabn v\u1ec1 \"URL\". \u0110i\u1ec1u n\u00e0y s\u1ebd khi\u1ebfn ch\u00fang \u0111\u01b0\u1ee3c m\u00e3 h\u00f3a gi\u1ed1ng nhau v\u00e0 th\u1ec3 hi\u1ec7n c\u00f9ng m\u1ed9t ph\u00e2n b\u1ed1 trong qu\u00e1 tr\u00ecnh hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh (m\u00f4 h\u00ecnh s\u1ebd hi\u1ec3u c\u00e1c t\u1eeb \u0111\u00f3 mang c\u00f9ng \u00fd ngh\u0129a, t\u00ednh ch\u1ea5t)","c8eb8850":"# * Lemmatizer\n\n\u0110\u01b0a t\u1eeb \u0111\u01b0\u1ee3c chia v\u1ec1 d\u1ea1ng b\u00ecnh th\u01b0\u1eddng\n\nC\u00f9ng m\u1ed9t t\u1eeb trong nh\u1eefng c\u00e2u kh\u00e1c nhau s\u1ebd chia \u1edf c\u00e1c d\u1ea1ng kh\u00e1c nhau (v\u00ed d\u1ee5 go, goes). Ta \u0111\u01b0a c\u00e1c t\u1eeb \u0111\u00f3 v\u1ec1 d\u1ea1ng g\u1ed1c \u0111\u1ec3 qu\u00e1 tr\u00ecnh encoding kh\u00f4ng hi\u1ec3u sai ch\u00fang th\u00e0nh 2 t\u1eeb kh\u00e1c nhau","494fe373":"# * Clean punct\nLo\u1ea1i b\u1ecf d\u1ea5u v\u00e0 c\u00e1c k\u00ed t\u1ef1 \u0111\u1eb7c bi\u1ec7t.\n\nC\u00e1c d\u1ea5u v\u00e0 k\u00ed t\u1ef1 \u0111\u1eb7c bi\u1ec7t th\u01b0\u1eddng c\u00f3 t\u00e1c d\u1ee5ng ph\u00e2n lo\u1ea1i v\u00ec th\u1ebf ta \u0111\u01a1n gi\u1ea3n x\u00f3a b\u1ecf ch\u00fang kh\u1ecfi c\u00e2u.","9e39092c":"# 1. M\u00d4 T\u1ea2 B\u00c0I TO\u00c1N\n* Quora l\u00e0 m\u1ed9t n\u1ec1n t\u1ea3ng \u0111\u1ec3 m\u1ecdi ng\u01b0\u1eddi h\u1ecdc h\u1ecfi v\u1edbi nhau. M\u1ecdi ng\u01b0\u1eddi c\u00f3 th\u1ec3 \u0111\u0103ng nh\u1eefng c\u00e2u h\u1ecfi v\u00e0 ng\u01b0\u1eddi kh\u00e1c v\u00e0o chia s\u1ebb, tr\u1ea3 l\u1eddi nh\u1eefng th\u1eafc m\u1eafc \u0111\u00f3. M\u1ee5c \u0111\u00edch c\u1ee7a b\u00e0i to\u00e1n n\u00e0y l\u00e0 \u0111\u1ec3 ch\u1ec9 ra c\u00e2u h\u1ecfi c\u00f3 ch\u00e2n th\u00e0nh hay kh\u00f4ng. Nh\u1eefng c\u00e2u h\u1ecfi kh\u00f4ng ch\u00e2n th\u00e0nh th\u01b0\u1eddng l\u00e0 nh\u1eefng c\u00e2u \u0111\u01b0a ra tuy\u00ean b\u1ed1 quan \u0111i\u1ec3m c\u1ee7a m\u00ecnh h\u01a1n l\u00e0 \u0111\u1ec3 t\u00ecm nh\u1eefng c\u00e2u tr\u1ea3 l\u1eddi c\u00f3 \u00edch: kh\u00f4ng trung l\u1eadp, khi\u00eau kh\u00edch ho\u1eb7c ch\u00ea bai, kh\u00f4ng c\u00f3 c\u0103n c\u1ee9 th\u1ef1c t\u1ebf v\u00e0 n\u1ed9i dung khi\u00eau d\u00e2m.\n* M\u1ed9t th\u00e1ch th\u1ee9c l\u1edbn \u0111\u00f3 l\u00e0 lo\u1ea1i b\u1ecf \u0111i nh\u1eefng c\u00e2u h\u1ecfi toxic - nh\u1eefng c\u00e2u h\u1ecfi \u0111\u01b0\u1ee3c \u0111\u1eb7t ra d\u1ef1a tr\u00ean nh\u1eefng ti\u1ec1n \u0111\u1ec1 sai l\u1ea7m ho\u1eb7c c\u00f3 \u00fd \u0111\u1ecbnh \u0111\u01b0a ra m\u1ed9t tuy\u00ean b\u1ed1 h\u01a1n l\u00e0 t\u00ecm ki\u1ebfm c\u00e2u tr\u1ea3 l\u1eddi h\u1eefu \u00edch.\n* V\u1eady m\u1ee5c ti\u00eau, th\u00e1ch th\u1ee9c c\u1ee7a d\u1ef1 \u00e1n n\u00e0y l\u00e0 lo\u1ea1i b\u1ecf nh\u1eefng c\u00e2u h\u1ecfi \"insincere\" - nh\u1eefng c\u00e2u h\u1ecfi kh\u00f4ng mang t\u00ednh ch\u1ea5t \u0111\u00f3ng g\u00f3p, thi\u1ebfu ch\u00e2n th\u00e0nh, th\u1eadm ch\u00ed \u0111\u1ec3 \u0111\u1ea3 k\u00edch m\u1ed9t c\u00e1 nh\u00e2n, t\u1eadp th\u1ec3 hay t\u1ed5 ch\u1ee9c n\u00e0o \u0111\u00f3. Ngo\u00e0i ra, nh\u1eefng c\u00e2u h\u1ecfi c\u00f3 \u00fd \u0111\u1ecbnh \u0111\u01b0a ra m\u1ed9t tuy\u00ean b\u1ed1 h\u01a1n l\u00e0 t\u00ecm ki\u1ebfm nh\u1eefng c\u00e2u tr\u1ea3 l\u1eddi h\u1eefu \u00edch c\u0169ng c\u1ea7n b\u1ecb lo\u1ea1i b\u1ecf.\n* Input: C\u00e2u h\u1ecfi d\u01b0\u1edbi d\u1ea1ng v\u0103n b\u1ea3n\n* Output: 0\/1 (Yes\/No)","67acb8ad":"![download.png](attachment:3ef3fbe9-6006-4967-a6b0-8e5298cfafa2.png)\n\n**Nh\u1eadn x\u00e9t:**\nC\u00f3 th\u1ec3 th\u1ea5y r\u1eb1ng m\u00f4 h\u00ecnh v\u1eabn cho ra k\u1ebft qu\u1ea3 r\u1ea5t cao tr\u00ean t\u1eadp validation nh\u01b0ng l\u1ea1i th\u1ea5p tr\u00ean t\u1eadp test. Tuy nhi\u00ean, vi\u1ec7c s\u1eed d\u1ee5ng K-mean \u0111\u00e3 t\u0103ng F1-Score l\u00ean h\u01a1n 3% so v\u1edbi l\u00fac tr\u01b0\u1edbc ch\u1ee9ng t\u1ecf vi\u1ec7c x\u00f3a ph\u1ea7n t\u1eed \u1edf t\u1eebng c\u1ee5m l\u00e0 c\u00f3 hi\u1ec7u qu\u1ea3 v\u00e0 \u0111\u00e3 c\u1ea3i thi\u1ec7n \u0111\u00e1ng k\u1ec3 ph\u01b0\u01a1ng ph\u00e1p Under Sampling. N\u1ebfu c\u00f3 th\u1ec3 t\u0103ng s\u1ed1 c\u1ee5m l\u00ean th\u00ec c\u00f3 kh\u1ea3 n\u0103ng hi\u1ec7u qu\u1ea3 c\u1ee7a m\u00f4 h\u00ecnh c\u0169ng s\u1ebd \u0111\u01b0\u1ee3c c\u1ea3i thi\u1ec7n.\n\n\n\n# 6. C\u1ea2I THI\u1ec6N\n\nSinh th\u00eam c\u00e1c tr\u01b0\u1eddng d\u1eef li\u1ec7u c\u00f3 th\u1ec3 t\u0103ng th\u00f4ng tin t\u1eeb \u0111\u00f3 \u1ea3nh h\u01b0\u1edfng t\u1edbi kh\u1ea3 n\u0103ng d\u1ef1 \u0111o\u00e1n m\u00f4 h\u00ecnh. \u0110\u1ed1i v\u1edbi b\u00e0i to\u00e1n hi\u1ec7n t\u1ea1i, m\u1ed9t v\u00e0i \u0111\u1eb7c \u0111i\u1ec3m v\u1ec1 s\u1ed1 l\u01b0\u1ee3ng k\u00ed t\u1ef1 \u0111\u1eb7c bi\u1ec7t, ch\u1eef s\u1ed1, s\u1ed1 l\u01b0\u1ee3ng stopword,... trong c\u00e2u c\u00f3 th\u1ec3 l\u00e0 nh\u1eefng th\u00f4ng tin quan tr\u1ecdng gi\u00fap ph\u00e2n lo\u1ea1i c\u00e2u h\u1ecfi.\n\nTa th\u00eam 1 s\u1ed1 tr\u01b0\u1eddng th\u00f4ng tin cho c\u00e2u h\u1ecfi:\n\n* freq_id : t\u1ea7n su\u1ea5t xu\u1ea5t hi\u1ec7n c\u1ee7a id\n* q_len : \u0111\u1ed9 d\u00e0i c\u1ee7a c\u00e1c c\u00e2u h\u1ecfi\n* n_words : s\u1ed1 t\u1eeb trong c\u00e2u h\u1ecfi\n* numeric_words : s\u1ed1 l\u01b0\u1ee3ng c\u00e1c ch\u1eef s\u1ed1 trong c\u00e2u\n* sp_char_words : s\u1ed1 l\u01b0\u1ee3ng c\u00e1c k\u00ed t\u1ef1 \u0111\u1eb7c bi\u1ec7t trong c\u00e2u\n* char_words : s\u1ed1 l\u01b0\u1ee3ng c\u00e1c k\u00ed t\u1ef1 trong c\u00e2u\n* unique_words : s\u1ed1 l\u01b0\u1ee3ng c\u00e1c t\u1eeb duy nh\u1ea5t trong c\u00e2u","6e829e07":"![download.png](attachment:4e19873b-d169-4b0f-89d2-bc8da56805f0.png)\n\n**Nh\u1eadn x\u00e9t:**\n\u0110i\u1ec3m s\u1ed1 \u0111\u00e3 \u0111\u01b0\u1ee3c c\u1ea3i thi\u1ec7n h\u01a1n so v\u1edbi tr\u01b0\u1edbc. T\u1eadp feature m\u1edbi \u0111\u01b0\u1ee3c th\u00eam v\u00e0o \u0111\u00e3 c\u00f3 hi\u1ec7u qu\u1ea3.\n\n\n\n# 7. K\u1ebeT QU\u1ea2\n\n\u01afu \u0111i\u1ec3m:\n* \u0110\u00e3 c\u1ea3i thi\u1ec7n \u0111\u01b0\u1ee3c F1-score m\u00f4 h\u00ecnh c\u01a1 s\u1edf t\u1eeb 0.6305 l\u00ean 0.6375\n* V\u1edbi b\u00e0i to\u00e1n hi\u1ec7n t\u1ea1i, khi so s\u00e1nh v\u1edbi nh\u1eefng m\u00f4 h\u00ecnh deep learning, Linear SVC c\u0169ng cho th\u1ea5y kh\u1ea3 n\u0103ng ph\u00e2n l\u1edbp t\u01b0\u01a1ng \u0111\u1ed1i hi\u1ec7u qu\u1ea3.\n* M\u00f4 h\u00ecnh \u0111\u01b0\u1ee3c c\u1ea3i thi\u1ec7n nh\u1eb9 v\u1edbi t\u1eadp d\u1eef li\u1ec7u sau khi \u0111\u01b0\u1ee3c l\u1ecdc b\u1edbt ph\u1ea7n th\u1eeba v\u00e0 t\u0103ng th\u00eam tr\u01b0\u1eddng d\u1eef li\u1ec7u.\n* K-mean c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c \u00e1p d\u1ee5ng \u0111\u1ec3 c\u1ea3i thi\u1ec7n cho ph\u01b0\u01a1ng ph\u00e1p Under Sampling, tr\u00e1nh hi\u1ec7n t\u01b0\u1ee3ng Overfitting.\n\nNh\u01b0\u1ee3c \u0111i\u1ec3m:\n* M\u00e3 h\u00f3a text b\u1eb1ng Tfidf l\u00e0 t\u01b0\u01a1ng \u0111\u1ed1i \u0111\u01a1n gi\u1ea3n, vector \u0111\u01b0\u1ee3c t\u1ea1o ra mang \u00edt gi\u00e1 tr\u1ecb trong hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh.\n* K-mean \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng hi\u1ec7u qu\u1ea3 nh\u01b0ng s\u1ed1 l\u01b0\u1ee3ng cluster kh\u00e1 th\u1ea5p, kh\u00f4ng \u0111\u1ee7 \u0111\u1ec3 c\u1ea3i thi\u1ec7n m\u00f4 h\u00ecnh v\u1edbi Under Sampling.\n\nH\u01b0\u1edbng ti\u1ebfp c\u1eadn c\u1ea3i thi\u1ec7n m\u00f4 h\u00ecnh:\n* S\u1eed d\u1ee5ng c\u00e1c b\u1ed9 vector bi\u1ec3u di\u1ec5n t\u1eeb \u0111\u01b0\u1ee3c pretrain nh\u01b0 glove thay th\u1ebf Tfidf, c\u00e1c vector n\u00e0y t\u1eadn d\u1ee5ng th\u00f4ng tin v\u1ec1 m\u1ed1i quan h\u1ec7 ng\u1eef ngh\u0129a gi\u1eefa c\u00e1c t\u1eeb t\u1ed1t h\u01a1n, mang l\u1ea1i nhi\u1ec1u th\u00f4ng tin h\u01a1n trong qu\u00e1 tr\u00ecnh hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh.\n* S\u1eed d\u1ee5ng ph\u01b0\u01a1ng ph\u00e1p Over Sampling t\u0103ng th\u00eam d\u1eef li\u1ec7u cho t\u1eadp thi\u1ec3u s\u1ed1.\n* \u00c1p d\u1ee5ng m\u1ed9t s\u1ed1 m\u00f4 h\u00ecnh deep learning nh\u01b0 LSTM, RNN.","68486448":"T\u1eeb bi\u1ec3u \u0111\u1ed3 m\u1ee9c \u0111\u1ed9 t\u01b0\u01a1ng quan gi\u1eefa c\u00e1c tr\u01b0\u1eddng cho ta th\u1ea5y r\u1eb1ng n\u1ebfu x\u00e9t \u0111\u1ed9c l\u1eadp th\u00ec c\u00e1c tr\u01b0\u1eddng c\u00f3 \u0111\u1ed9 t\u01b0\u01a1ng quan kh\u00e1 th\u1ea5p v\u1edbi target (kho\u1ea3ng 0.1). Tuy nhi\u00ean c\u0169ng c\u00f3 th\u1ec3 target b\u1ecb \u1ea3nh h\u01b0\u1edfng b\u1edfi nhi\u1ec1u tr\u01b0\u1eddng. Ta ti\u1ebfn h\u00e0nh th\u1eed nghi\u1ec7m v\u1edbi c\u00e1c tr\u01b0\u1eddng.","6706a8ee":"Khi ti\u1ec1n h\u00e0nh validate tr\u00ean b\u1ed9 d\u1eef li\u1ec7u \u0111\u01b0\u1ee3c lo\u1ea1i b\u1ecf stopword, x\u1ea3y ra hi\u1ec7n t\u01b0\u1ee3ng c\u00f3 s\u1ef1 gi\u1ea3m nh\u1eb9 c\u1ee7a F1-score.","e7a62249":"\u1ede \u0111\u00e2y ta ch\u1ec9nh tham s\u1ed1 ngram_range=(1,3) \u0111\u1ec3 t\u1ea1o \u0111\u01b0\u1ee3c c\u00e1c c\u1ee5m t\u1eeb g\u1ed3m 1-3 t\u1eeb trong b\u1ed9 d\u1eef li\u1ec7u. V\u00ed d\u1ee5 v\u1edbi c\u00e2u \"Python is cool\" s\u1ebd sinh ra \u0111\u01b0\u1ee3c c\u00e1c t\u1eeb v\u00e0 c\u1ee5m t\u1eeb \"Python\", \"is\", \"cool\", \"Python is\", \"Python is cool\" , \"is cool\". \u0110i\u1ec1u n\u00e0y s\u1ebd gi\u00fap ta t\u1ea1o \u0111\u01b0\u1ee3c m\u1ed1i quan h\u1ec7 gi\u1eefa c\u00e1c c\u1ee5m t\u1eeb xu\u1ea5t hi\u1ec7n trong t\u1eadp d\u1eef li\u1ec7u.\n\n\n**3.2. LinearSVC**\n\nTh\u1eed nghi\u1ec7m v\u1edbi nhi\u1ec1u t\u1eadp m\u1eabu cho th\u1ea5y k\u1ebft qu\u1ea3 thu \u0111\u01b0\u1ee3c t\u1eeb LinearSVC l\u00e0 cao v\u00e0 \u1ed5n \u0111\u1ecbnh nh\u1ea5t. V\u00ec v\u1eady, ta ch\u1ecdn LinearSVC l\u00e0m m\u00f4 h\u00ecnh c\u01a1 s\u1edf.\n![download.png](attachment:9830a083-7cd1-4bf0-978b-cd48b5f7e20e.png)\n\nLinearSVC gi\u00fap ta t\u00ecm \u0111\u01b0\u1ee3c \u0111\u01b0\u1eddng ph\u00e2n chia gi\u1eefa 2 l\u1edbp sao cho margin l\u00e0 l\u1edbn nh\u1ea5t. Vi\u1ec7c margin r\u1ed9ng h\u01a1n s\u1ebd mang l\u1ea1i hi\u1ec7u \u1ee9ng ph\u00e2n l\u1edbp t\u1ed1t h\u01a1n v\u00ec s\u1ef1 ph\u00e2n chia gi\u1eefa hai classes l\u00e0 r\u1ea1ch r\u00f2i h\u01a1n.\n![download.png](attachment:762a4da7-b03c-468b-8669-50211ecfa8ab.png)\n\n**3.3. Split data to train and validation**\n\nTa c\u1ea7n chia t\u1eadp train th\u00e0nh 2 ph\u1ea7n \u0111\u1ec3 hu\u1ea5n luy\u1ec7n v\u00e0 \u0111\u00e1nh gi\u00e1 m\u00f4 h\u00ecnh.\n\nC\u00e1ch \u0111\u01a1n gi\u1ea3n nh\u1ea5t l\u00e0 ta c\u00f3 th\u1ec3 l\u1ea5y ng\u1eabu nhi\u00ean m\u1ed9t t\u1eadp d\u1eef li\u1ec7u con t\u1eeb t\u1eadp train \u0111\u1ec3 l\u00e0m t\u1eadp validation. Tuy nhi\u00ean ta \u0111\u1ec3 \u00fd t\u1edbi tr\u01b0\u1eddng h\u1ee3p vi\u1ec7c l\u1ea5y ng\u1eabu nhi\u00ean t\u1eeb to\u00e0n b\u1ed9 t\u1eadp train c\u00f3 th\u1ec3 x\u1ea3y ra tr\u01b0\u1eddng h\u1ee3p ta l\u1ea5y ra t\u1eadp validation ch\u1ee9a to\u00e0n d\u1eef li\u1ec7u thu\u1ed9c c\u00f9ng m\u1ed9t l\u1edbp. V\u1ea5n \u0111\u1ec1 n\u00e0y \u0111\u1eb7c bi\u1ec7t d\u1ec5 x\u1ea3y ra v\u1edbi t\u1eadp d\u1eef li\u1ec7u hi\u1ec7n t\u1ea1i do \u0111\u1ed9 l\u1ec7ch l\u1edbn gi\u1eefa l\u01b0\u1ee3ng d\u1eef li\u1ec7u c\u1ee7a 2 l\u1edbp (t\u1ec9 l\u1ec7 15:1).\n\nGi\u1ea3i ph\u00e1p: l\u1ea5y ng\u1eabu nhi\u00ean d\u1eef li\u1ec7u t\u1eeb t\u1eebng l\u1edbp. V\u00ed d\u1ee5 v\u1edbi b\u1ed9 d\u1eef li\u1ec7u hi\u1ec7n t\u1ea1i, ta mu\u1ed1n t\u1ea1o ra t\u1eadp validation c\u00f3 t\u1ec9 l\u1ec7 1:10 t\u1eeb t\u1eadp train th\u00ec ta l\u1ea5y ta 1\/10 d\u1eef li\u1ec7u t\u1eeb l\u1edbp c\u00f3 target = 0 v\u00e0 1\/10 d\u1eef li\u1ec7u t\u1eeb t\u1eadp target = 1.\n\nTa s\u1eed d\u1ee5ng h\u00e0m train_test_split() c\u1ee7a sklearn \u0111\u1ec3 chia t\u1eadp theo ph\u01b0\u01a1ng ph\u00e1p tr\u00ean.\n","32e4ec00":"![download.png](attachment:40fada2c-1002-4c67-8c52-6a5ff7d261e6.png)\n\nTa c\u00f3 th\u1ec3 th\u1ea5y \u00e1p d\u1ee5ng under sampling c\u1ea3i thi\u1ec7n t\u1ed1t kh\u1ea3 n\u0103ng d\u1ef1 \u0111o\u00e1n tr\u00ean t\u1eadp validation nh\u01b0ng l\u1ea1i l\u00e0m th\u1ee5t gi\u1ea3m nghi\u00eam tr\u1ecdng kh\u1ea3 n\u0103ng d\u1ef1 \u0111o\u00e1n tr\u00ean t\u1eadp test.\n\n\u0110i\u1ec3m s\u1ed1 F1_score tr\u00ean t\u1eadp validation t\u0103ng ch\u1ee7 y\u1ebfu do so l\u01b0\u1ee3ng m\u1eabu d\u00f9ng \u0111\u1ec3 \u0111\u00e1nh gi\u00e1 m\u00f4 h\u00ecnh tr\u00ean t\u1eadp n\u00e0y \u0111\u00e3 gi\u1ea3m r\u1ea5t nhi\u1ec1u so v\u1edbi ban \u0111\u1ea7u.\n\nS\u1ed1 l\u01b0\u1ee3ng m\u1eabu \u1edf t\u1eadp training b\u1ecb gi\u1ea3m m\u1ea1nh (ch\u1ec9 c\u00f2n kho\u1ea3ng 1\/3 so v\u1edbi l\u00fac \u0111\u1ea7u) d\u1eabn t\u1edbi t\u1eadp d\u1eef li\u1ec7u trainning kh\u00f4ng \u0111\u1ea1i di\u1ec7n \u0111\u01b0\u1ee3c cho ph\u00e2n ph\u1ed1i c\u1ee7a to\u00e0n b\u1ed9 t\u1eadp d\u1eef li\u1ec7u d\u1eabn t\u1edbi hi\u1ec7n t\u01b0\u1ee3ng overfitting\n\n\n**5.2. Under Sampling combined K-means**\n\nK-means thu\u1eadt to\u00e1n \u0111\u01a1n gi\u1ea3n nh\u1ea5t c\u1ee7a d\u1ea1ng b\u00e0i to\u00e1n h\u1ecdc c\u00f3 gi\u00e1m s\u00e1t. K-means d\u1ef1a v\u00e0o kho\u1ea3ng c\u00e1ch gi\u1eefa c\u00e1c vector trong kh\u00f4ng gian, chia d\u1eef li\u1ec7u th\u00e0nh c\u00e1c c\u1ee5m v\u00e0 t\u00ecm ra t\u00e2m c\u1ee5m th\u00edch h\u1ee3p.\n![download.png](attachment:fec42f6e-1a43-4f5e-8160-ab00a2f28daf.png)\n\nProblem: V\u1ea5n \u0111\u1ec1 ta g\u1eb7p ph\u1ea3i khi s\u1eed d\u1ee5ng Under Sampling ch\u00ednh l\u00e0 vi\u1ec7c c\u1eaft gi\u1ea3m s\u1ed1 l\u01b0\u1ee3ng l\u1edbn m\u1eabu t\u1eeb t\u1eadp 0 khi\u1ebfn t\u1eadp d\u1eef li\u1ec7u m\u1edbi c\u00f3 kh\u1ea3 n\u0103ng kh\u00f4ng th\u1ec3 hi\u1ec7n \u0111\u01b0\u1ee3c h\u1ebft ph\u00e2n b\u1ed1 t\u1eadp d\u1eef li\u1ec7u ban \u0111\u1ea7u. Ta bi\u1ec3u di\u1ec5n \u0111\u01a1n gi\u1ea3n v\u1ea5n \u0111\u1ec1 tr\u00ean trong kh\u00f4ng gian 2 chi\u1ec1u\n\n\n![download.png](attachment:fcd1af33-ce07-45c5-87a6-544ce1cb4024.png)\n\nTa th\u1ea5y r\u1eb1ng trong tr\u01b0\u1eddng h\u1ee3p ta lo\u1ea1i b\u1ecf nh\u1eefng \u0111i\u1ec3m d\u1eef li\u1ec7u nh\u01b0 tr\u00ean h\u00ecnh, ph\u00e2n b\u1ed1 d\u1eef li\u1ec7u t\u1ea1i khu v\u1ef1c \u0111\u00f3 ho\u00e0n to\u00e0n b\u1ecb thay \u0111\u1ed5i khi\u1ebfn ph\u01b0\u01a1ng tr\u00ecnh \u0111\u01b0\u1eddng ph\u00e2n l\u1edbp ti\u1ebfn s\u00e2u v\u00e0o v\u00f9ng d\u1eef li\u1ec7u target 0 v\u00e0 khi\u1ebfn d\u1ef1 \u0111o\u00e1n b\u1ecb sai.\n\nSolution: ta s\u1ebd s\u1ef1 d\u1ee5ng K-mean nh\u1eb1m chia t\u1eadp c\u00e2u h\u1ecfi \u0111\u1ea7u v\u00e0o th\u00e0nh c\u00e1c c\u1ee5m ch\u1ee9a c\u00e1c c\u00e2u h\u1ecfi c\u00f3 \u0111\u1eb7c tr\u01b0ng t\u01b0\u01a1ng \u0111\u1ed3ng (ph\u00e2n b\u1ed1 g\u1ea7n nhau tr\u00ean t\u1eadp kh\u00f4ng gian vector). Sau \u0111\u00f3 ta ti\u1ebfn h\u00e1nh lo\u1ea1i b\u1edbt m\u1eabu \u1edf m\u1ed7i c\u1ee5m. Vi\u1ec7c n\u00e0y gi\u00fap ta c\u00f3 \u0111\u01b0\u1ee3c t\u1eadp c\u00e2u h\u1ecfi m\u1edbi c\u00f3 \u0111\u1ed9 bao qu\u00e1t cao, c\u00f3 th\u1ec3 \u0111\u1ea1i di\u1ec7n cho ph\u00e2n ph\u1ed1i to\u00e0n t\u1eadp d\u1eef li\u1ec7u b\u1ea1n \u0111\u1ea7u\n\n![download.png](attachment:abca6ca6-8577-4097-a927-97d93a9c5a0d.png)\n\nTa c\u00f3 th\u1ec3 th\u1ea5y khi ph\u00e2n c\u1ee5m, nh\u1eefng \u0111i\u1ec3m b\u1ecb x\u00f3a \u0111i \u1edf t\u1eebng c\u1ee5m s\u1ebd \u00edt g\u00e2y ra s\u1ef1 thay \u0111\u1ed5i ph\u00e2n b\u1ed1 tr\u00ean to\u00e0n t\u1eadp d\u1eef li\u1ec7u.\nTh\u00f4ng th\u01b0\u1eddng, khi s\u1eed d\u1ee5ng K-mean, ta s\u1ebd s\u1eed d\u1ee5ng ph\u01b0\u01a1ng ph\u00e1p Elbow ch\u1ecdn ra s\u1ed1 cluster th\u00edch h\u1ee3p nh\u1ea5t cho t\u1eadp d\u1eef li\u1ec7u. Tuy nhi\u00ean v\u1edbi b\u00e0i to\u00e1n hi\u1ec7n t\u1ea1i, vi\u1ec7c t\u00ecm ra s\u1ed1 c\u1ee5m th\u00edch h\u1ee3p l\u00e0 kh\u00f4ng c\u1ea7n thi\u1ebft. Ta d\u1ec5 th\u1ea5y r\u1eb1ng n\u1ebfu chia ra c\u00e0ng nhi\u1ec1u c\u1ee5m (t\u1ee9c l\u00e0 chia d\u1eef li\u1ec7u th\u00e0nh nhi\u1ec1u t\u1eadp ch\u1ee9a c\u00e1c \u0111i\u1ec3m g\u1ea7n nhau) th\u00ec vi\u1ec7c x\u00f3a b\u1ecf ph\u1ea7n t\u1eed c\u1ee7a m\u1ed7i c\u1ee5m s\u1ebd c\u00e0ng g\u00e2y \u00edt \u1ea3nh h\u01b0\u1edfng t\u1edbi ph\u00e2n b\u1ed1 d\u1eef li\u1ec7u. V\u00ec v\u1eady m\u1ee5c ti\u00eau c\u1ee7a ta s\u1ebd l\u00e0 chia ra v\u1edbi s\u1ed1 c\u1ee5m t\u1ed1i \u0111a.\n\nV\u00ed d\u1ee5 \u1edf b\u00e0i to\u00e1n n\u00e0y, t\u1ec9 l\u1ec7 d\u1eef li\u1ec7u 2 l\u1edbp \u0111ang l\u00e0 1:15. Ta mu\u1ed1n gi\u1ea3m d\u1eef li\u1ec7u l\u1edbp 0 xu\u1ed1ng sao cho t\u1ec9 l\u1ec7 2 l\u1edbp c\u00f2n kho\u1ea3ng 1:5, v\u1eady ta s\u1ebd gi\u1ea3m \u0111i 2\/3 l\u01b0\u1ee3ng d\u1eef li\u1ec7u \u1edf l\u1edbp 0. L\u1edbp 0 c\u00f3 1200000 d\u1eef li\u1ec7u, ta chia l\u1edbp 0 th\u00e0nh 400000 l\u1edbp, m\u1ed7i l\u1edbp s\u1ebd c\u00f3 3 ph\u1ea7n t\u1eed v\u00e0 \u1edf m\u1ed7i l\u1edbp ta s\u1ebd ti\u1ebfn h\u00e0nh x\u00f3a 2 ph\u1ea7n t\u1eed trong \u0111\u00f3.\n\nProblem: V\u1ea5n \u0111\u1ec1 x\u1ea3y ra khi s\u1eed d\u1ee5ng Kmeans t\u1eeb th\u01b0 vi\u1ec7n sklearn. D\u00f9 \u0111\u00e3 t\u1ed1i \u01b0u 2 th\u00f4ng s\u1ed1 max_iter(s\u1ed1 l\u1ea7n l\u1eb7p t\u1ed1i \u0111a v\u1edbi m\u1ed7i l\u1ea7n ch\u1ea1y) v\u00e0 n_init(s\u1ed1 l\u1ea7n ch\u1ea1y v\u1edbi m\u1ed7i t\u00e2m) th\u00ec khi t\u0103ng s\u1ed1 l\u01b0\u1ee3ng cluster l\u00ean \u0111\u1ebfn qu\u00e1 15 cluster th\u00ec xu\u1ea5t hi\u1ec7n tr\u00e0n ram tr\u00ean kaggle. V\u00ec v\u1eady \u1edf \u0111\u00e2y, ta ch\u1ec9 c\u00f3 th\u1ec3 th\u1eed nghi\u1ec7m v\u1edbi s\u1ed1 cluster l\u1edbn nh\u1ea5t l\u00e0 14.","a86312c2":"\u0110\u1ed1i v\u1edbi d\u1eef li\u1ec7u d\u1ea1ng text, chi ti\u1ebft h\u01a1n l\u00e0 d\u1ea1ng c\u00e2u h\u1ecfi trong b\u00e0i to\u00e1n n\u00e0y, ta th\u1ea5y r\u1eb1ng c\u00f3 th\u1ec3 c\u00f3 nh\u1eefng ph\u1ea7n d\u1eef li\u1ec7u d\u01b0 th\u1eeba - nh\u1eefng ph\u1ea7n d\u1eef li\u1ec7u kh\u00f4ng mang l\u1ea1i gi\u00e1 tr\u1ecb th\u1ec3 hi\u1ec7n n\u1ed9i dung hay ph\u00e2n lo\u1ea1i c\u00e2u nh\u01b0 d\u1ea5u, ch\u1eef c\u00e1i in hoa, ch\u1eef s\u1ed1, c\u00e1c k\u00ed t\u1ef1 \u0111\u1eb7c bi\u1ec7t, stopword (nh\u1eefng ch\u1eef kh\u00f4ng mang l\u1ea1i gi\u00e1 tr\u1ecb v\u1ec1 n\u1ed9i dung nh\u01b0 is, are, am,the,..) .\n\nSau khi \u0111\u00e1nh gi\u00e1 ta th\u1ea5y d\u1eef li\u1ec7u c\u00f2n kh\u00e1 ph\u1ee9c t\u1ea1p v\u00e0 nhi\u1ec1u nhi\u1ec5u. \u0110\u1ec3 \u0111\u01a1n gi\u1ea3n ho\u00e1 d\u1eef li\u1ec7u ta c\u00f3 th\u1ec3 th\u1ef1c hi\u1ec7n m\u1ed9t s\u1ed1 b\u01b0\u1edbc sau:\n\n# * **Clean tag**\n\n\u0110\u01b0a c\u00e1c bi\u1ec3u th\u1ee9c to\u00e1n h\u1ecdc v\u1ec1 \"MATH EQUATION\" v\u00e0 \u0111\u01b0a \u0111\u01b0\u1eddng d\u1eabn web v\u1ec1 \"URL\".\n\n","79fb06d8":"# * Mispell Dict\n\nS\u1eeda t\u1eeb vi\u1ebft sai v\u00e0 \u0111\u01b0a m\u1ed9t s\u1ed1 t\u1eeb v\u1ec1 d\u1ea1ng th\u00f4ng d\u1ee5ng.","21b3c11a":"![download.png](attachment:97c08c05-645a-462a-952b-9e335e86080a.png)\n\n**Nh\u1eadn x\u00e9t:**\n\u0110i\u1ec3m s\u1ed1 sau khi x\u1eed l\u00fd d\u1eef li\u1ec7u \u0111\u00e3 t\u0103ng th\u00eam kho\u1ea3ng 0.4%. Ngo\u00e0i ra, vi\u1ec7c lo\u1ea1i b\u1ecf \u0111i stopword v\u00e0 c\u00e1c k\u00ed t\u1ef1 th\u1eeba c\u0169ng khi\u1ebfn th\u1eddi gian m\u00e3 h\u00f3a d\u1eef li\u1ec7u t\u1eeb text sang vector v\u00e0 th\u1eddi gian hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh gi\u1ea3m \u0111i \u0111\u00e1ng k\u1ec3.\n\n\n# 5. THU\u1eacT TO\u00c1N\n\nUnder sampling l\u00e0 vi\u1ec7c ta gi\u1ea3m s\u1ed1 l\u01b0\u1ee3ng c\u00e1c quan s\u00e1t c\u1ee7a nh\u00f3m \u0111a s\u1ed1 \u0111\u1ec3 n\u00f3 tr\u1edf n\u00ean c\u00e2n b\u1eb1ng v\u1edbi s\u1ed1 quan s\u00e1t c\u1ee7a nh\u00f3m thi\u1ec3u s\u1ed1. \u01afu \u0111i\u1ec3m c\u1ee7a under sampling l\u00e0 l\u00e0m c\u00e2n b\u1eb1ng m\u1eabu m\u1ed9t c\u00e1ch nhanh ch\u00f3ng, d\u1ec5 d\u00e0ng ti\u1ebfn h\u00e0nh th\u1ef1c hi\u1ec7n m\u00e0 kh\u00f4ng c\u1ea7n \u0111\u1ebfn thu\u1eadt to\u00e1n gi\u1ea3 l\u1eadp m\u1eabu.\n\n**5.1. Simple Under Sampling**\n\n\u00c1p d\u1ee5ng v\u1edbi b\u1ed9 d\u1eef li\u1ec7u hi\u1ec7n t\u1ea1i, ta gi\u1ea3m s\u1ed1 l\u01b0\u1ee3ng m\u1eabu c\u1ee7a l\u1edbp 0 (target = 0) sao cho t\u1ec9 l\u1ec7 gi\u1eefa 2 l\u1edbp l\u00e0 4:1 b\u1eb1ng c\u00e1ch l\u1ea5y ng\u1eabu nhi\u00ean m\u1ed9t s\u1ed1 l\u01b0\u1ee3ng ph\u1ea7n t\u1eed t\u1eeb l\u1edbp n\u00e0y.\n","47f0bd7a":"# * Contraction Mapping\n\n\u0110\u01b0a c\u00e1c t\u1eeb vi\u1ebft t\u1eaft v\u1ec1 d\u1ea1ng b\u00ecnh th\u01b0\u1eddng","2f2ff362":"**Nh\u1eadn x\u00e9t:**\nT\u1eadp d\u1eef li\u1ec7u train c\u00f3 1306122 c\u00e2u h\u1ecfi v\u00e0 t\u1eadp test ch\u1ee9a 375806 c\u00e2u h\u1ecfi \u0111\u1ec3 \u0111\u00e1nh gi\u00e1 hi\u1ec7u n\u0103ng m\u00f4 h\u00ecnh. Trong \u0111\u00f3, kh\u00f4ng c\u00f3 gi\u00e1 tr\u1ecb null n\u00e0o v\u00e0 c\u00f3 3 tr\u01b0\u1eddng d\u1eef li\u1ec7u: qid, question_text, target.\n\nqid : id c\u1ee7a c\u00e2u h\u1ecfi, m\u1ed7i c\u00e2u h\u1ecfi c\u00f3 1 id ri\u00eang ph\u00e2n bi\u1ec7t\n\nquestion_text : n\u1ed9i dung c\u00e2u h\u1ecfi \u1edf d\u1ea1ng text\n\ntarget: g\u1ed3m 2 nh\u00e3n 0 v\u00e0 1. Nh\u00e3n 0 l\u00e0 sincere l\u00e0 c\u00e2u h\u1ecfi ch\u00e2n th\u00e0nh, nh\u00e3n 1 l\u00e0 insincere l\u00e0 c\u00e2u h\u1ecfi kh\u00f4ng ch\u00e2n th\u00e0nh (toxic)","f91a03c8":"# **2. KH\u1ea2O S\u00c1T D\u1eee LI\u1ec6U**\nD\u1eef li\u1ec7u \u0111\u01b0\u1ee3c cung c\u1ea5p g\u1ed3m 2 file ch\u00ednh l\u00e0 train.csv v\u00e0 test.csv.","0cfcc200":"Ta submit th\u1eed v\u1edbi base model \u0111\u1ec3 ki\u1ec3m tra \u0111i\u1ec3m s\u1ed1 \u0111\u1ea1t \u0111\u01b0\u1ee3c tr\u00ean t\u1eadp test so v\u1edbi t\u1eadp validation","5a2c381d":"# * Stopword\n\nX\u00f3a stopword\n\nTrong c\u00e1c c\u00e2u ch\u1ee9a r\u1ea5t nhi\u1ec1u nh\u1eefng t\u1eeb xu\u1ea5t hi\u1ec7n nhi\u1ec1u nh\u01b0ng kh\u00f4ng c\u00f3 gi\u00e1 tr\u1ecb trong ph\u00e2n lo\u1ea1i c\u00e2u, ch\u00fang \u0111\u01b0\u1ee3c g\u1ecdi l\u00e0 stopword. X\u00f3a b\u1ecf stopword gi\u00fap c\u1ea3i thi\u1ec7n hi\u1ec7u n\u0103ng, gi\u1eef l\u1ea1i nh\u1eefng t\u1eeb quan tr\u1ecdng gi\u00fap c\u1ea3i thi\u1ec7n hi\u1ec7u qu\u1ea3 ph\u00e2n l\u1edbp c\u1ee7a m\u00f4 h\u00ecnh"}}