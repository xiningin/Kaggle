{"cell_type":{"540b3a72":"code","7262f075":"code","c08c7780":"code","a1411554":"code","a78f7cee":"code","948ec846":"code","582f9659":"code","0c7844b2":"code","6e1fd0f7":"code","2b8d7572":"code","ae19e541":"code","f8bf7173":"code","5d0c19db":"code","23ec4dd4":"code","6f90dd69":"code","d3ef2a4a":"code","8ce536d3":"code","1763af3a":"code","d11874da":"code","2b3be3df":"code","df11d39f":"code","9076a7e3":"code","c2941755":"code","c4040016":"code","82edb6f8":"code","08827e34":"code","97005c62":"code","a8fae6c3":"code","2d36e570":"code","750d0ec5":"code","84bbe216":"code","f5eb9903":"code","8b1d9df0":"code","edd7bcff":"code","9a9b7a14":"code","62b12c9e":"markdown","cebc63a5":"markdown","6b982b29":"markdown","0be0856e":"markdown","1b277321":"markdown","75f1bddb":"markdown","4985a1fd":"markdown","cea54cee":"markdown","b3909183":"markdown","595ac5ad":"markdown","82bba3f0":"markdown","5ea19e01":"markdown","94133bf9":"markdown","a88ce6ec":"markdown","5623c7c5":"markdown","aa916e7d":"markdown","48ab2d45":"markdown","269def48":"markdown","dccd252f":"markdown","7efca5d5":"markdown","ff72be39":"markdown","62a99868":"markdown","0c6751f1":"markdown","08d8a32c":"markdown"},"source":{"540b3a72":"from collections import defaultdict, Counter\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import make_scorer, accuracy_score, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nparameters = {'n_estimators': [4, 6, 9],\n              'max_features': ['log2', 'sqrt', 'auto'],\n              'criterion': ['entropy', 'gini'],\n              'max_depth': [2, 3, 5, 10],\n              'min_samples_split': [2, 3, 5],\n              'min_samples_leaf': [1, 5, 8]\n              }\n\ndef Checker(x):\n    if type(x) is pd.DataFrame:\n        return 0\n    elif type(x) is pd.Series:\n        return 1\n    else:\n        return -1\n\n\ndef dtype(data):\n    what = Checker(data)\n    if what == 0:\n        dtypes = data.dtypes.astype('str')\n        dtypes = dtypes.str.split(r'\\d').str[0]\n    else:\n        dtypes = str(data.dtypes)\n        dtypes = re.split(r'\\d', dtypes)[0]\n    return dtypes\n\n\ndef split(x, pattern):\n    '''Regex pattern finds in data and returns match. Then, it is splitted accordingly.\n        \\d = digits\n        \\l = lowercase alphabet\n        \\p = uppercase alphabet\n        \\a = all alphabet\n        \\s = symbols and punctuation\n        \\e = end of sentence\n        '''\n    pattern2 = pattern.replace('\\d', '[0-9]').replace('\\l', '[a-z]').replace('\\p', '[A-Z]').replace('\\a', '[a-zA-Z]')        .replace('\\s', '[^0-9a-zA-Z]').replace('\\e', '(?:\\s|$)')\n\n    if dtype(x) != 'object':\n        print('Data is not string. Convert first')\n        return False\n\n    regex = re.compile(r'{}'.format(pattern))\n    if pattern == pattern2:\n        return x.str.split(pattern)\n    else:\n        return x.apply(lambda i: re.split(regex, i))\n\n\ndef replace(x, pattern, with_=None):\n    '''Regex pattern finds in data and returns match. Then, it is replaced accordingly.\n        \\d = digits\n        \\l = lowercase alphabet\n        \\p = uppercase alphabet\n        \\a = all alphabet\n        \\s = symbols and punctuation\n        \\e = end of sentence\n        '''\n    if type(pattern) is list:\n        d = {}\n        for l in pattern:\n            d[l[0]] = l[1]\n        try:\n            return x.replace(d)\n        except:\n            return x.astype('str').replace(d)\n\n    pattern2 = pattern.replace('\\d', '[0-9]').replace('\\l', '[a-z]').replace('\\p', '[A-Z]').replace('\\a', '[a-zA-Z]')        .replace('\\s', '[^0-9a-zA-Z]').replace('\\e', '(?:\\s|$)')\n\n    if dtype(x) != 'object':\n        print('Data is not string. Convert first')\n        return False\n\n    regex = re.compile(r'{}'.format(pattern))\n    if pattern == pattern2:\n        return x.str.replace(pattern, with_)\n    else:\n        return x.apply(lambda i: re.sub(regex, with_, i))\n\n\ndef hcat(*columns):\n    cols = []\n    for c in columns:\n        if c is None:\n            continue\n        if type(c) in (list, tuple):\n            for i in c:\n                if type(i) not in (pd.DataFrame, pd.Series):\n                    cols.append(pd.Series(i))\n                else:\n                    cols.append(i)\n        elif type(c) not in (pd.DataFrame, pd.Series):\n            cols.append(pd.Series(c))\n        else:\n            cols.append(c)\n    return pd.concat(cols, 1)\n\n\ndef parse_col_json(df, column, key, nested):\n    \"\"\"\n    Args:\n        column: string\n            name of the column to be processed.\n        key: string\n            name of the dictionary key which needs to be extracted\n    \"\"\"\n    import json\n    for index, i in zip(df.index, df[column].apply(json.loads)):\n        list1 = []\n        males = []\n        females = []\n\n        for j in range(len(i)):\n            if nested:\n                if not(((i[j][\"department\"] == \"Directing\") and (i[j][\"job\"] == \"Director\"))):\n                    continue\n            name = i[j][key]\n            if \",\" in name:\n                name = name.replace(\",\", \" \")\n            if \" \" in name:\n                name = name.replace(\" \", \"_\")\n            list1.append(name)\n            if column==\"cast\":\n                if i[j][\"gender\"] == 1:\n                    females.append(name)\n                elif i[j][\"gender\"] == 2:\n                    males.append(name)\n        df.loc[index, column] = str(list1)\n        if column==\"cast\":\n            df.loc[index, \"actors\"] = str(males)\n            df.loc[index, \"actress\"] = str(females)\n\n\ndef counts_elements(df, columns):\n    d = defaultdict(Counter)\n    for column in columns:\n        for el in df[column]:\n            l = eval(str(el))\n            for x in l:\n                d[column][x] += 1\n    return d\n\n\ndef counts_vectorized(df, col, min=1, vocabulary=None):\n    from sklearn.feature_extraction.text import CountVectorizer\n    vectorizer = CountVectorizer(tokenizer=lambda x: x.split(\n        \",\"), min_df=min, vocabulary=vocabulary)\n    #analyze = vectorizer.build_analyzer()\n    #f = analyze(succ_movies.cast.iloc[0].strip(\"[]\"))\n    data = [x.strip(\"[]\") for x in df[col]]\n    #analyze([\"ciao, mamma, come, stai_oggi\", \"ehi, mamma, stai_oggi, cane\"])\n    vectorizer.fit(data)\n    counts = pd.DataFrame(vectorizer.transform(data).toarray())\n    counts.columns = [x.replace(\"'\", \"\")\n                      for x in vectorizer.get_feature_names()]\n    return counts\n\n\ndef simplify(df, col, bins, group_names):\n    df[col] = df[col].fillna(-0.5)\n    categories = pd.cut(df[col], bins, labels=group_names)\n    df[col] = categories\n\n\ndef encode_features(df):\n    features = ['year', 'runtime']\n\n    for feature in features:\n        le = preprocessing.LabelEncoder()\n        le = le.fit(df[feature])\n        df[feature] = le.transform(df[feature])\n    return df\n\n\ndef testClassifier(clf, name, dict):\n    y_pred = []\n    if name == \"Gradient Boosting\":\n        y_pred = clf.fit(X_train, y_train, early_stopping_rounds=5,\n             eval_set=[(X_test, y_test)], verbose=False).predict(X_test)\n        y_pred = [round(value) for value in y_pred]\n    else:\n        clf = clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        \n    # Compute confusion matrix\n    CM = confusion_matrix(y_test, y_pred)\n\n    TN = CM[0][0]\n    FN = CM[1][0]\n    TP = CM[1][1]\n    FP = CM[0][1]\n\n    # Sensitivity, hit rate, recall, or true positive rate\n    TPR = TP\/(TP+FN)\n    # Specificity or true negative rate\n    TNR = TN\/(TN+FP) \n    # Precision or positive predictive value\n    PPV = TP\/(TP+FP)\n    # Negative predictive value\n    NPV = TN\/(TN+FN)\n    # Fall out or false positive rate\n    FPR = FP\/(FP+TN)\n    # False negative rate\n    FNR = FN\/(TP+FN)\n    # False discovery rate\n    FDR = FP\/(TP+FP)\n    # Overall accuracy\n    ACC = (TP+TN)\/(TP+FP+FN+TN)\n    print(\"{} Scores:\\n\".format(name))\n    print(\"Accuracy: {0:.2f} %\\nPrecision: {1:.2f} %\\nRecall: {2:.2f} %\\nFall out: {3:.2f} %\\nFalse Negative Rate: {4:.2f} %\\n\\n\"\n        .format(ACC.round(4)*100.0,PPV.round(4)*100.0,TPR.round(4)*100.0,FDR.round(4)*100.0,FNR.round(4)*100.0))\n\n    dict[\"classifier\"].append(name)\n    dict[\"accuracy\"].append(ACC.round(4)*100.0)\n    dict[\"fallout\"].append(FDR.round(4)*100.0)\n    dict[\"fnr\"].append(FNR.round(4)*100.0)\n    return clf","7262f075":"movies = pd.read_csv(\n    '..\/input\/tmdb_5000_movies.csv', index_col=3)\ncredits = pd.read_csv(\n    '..\/input\/tmdb_5000_credits.csv', index_col=0)\n\nmovies.drop_duplicates(keep=\"first\", inplace=True)\nuseless_col = ['homepage', 'original_title', 'original_language', 'overview',\n               'spoken_languages', 'keywords', 'status', 'tagline']\nmovies.drop(useless_col, axis=1, inplace=True)\n\ncredits.drop([\"title\"], axis=1, inplace=True)\n\n# Split the year from the date\nmovies.release_date = pd.to_datetime(movies['release_date'])\nmovies[\"year\"] = movies.release_date.dt.year\n\n# Changing the data type of the below mentioned columns and\nchange_cols = ['budget', 'revenue',\"popularity\", \"runtime\", \"vote_average\", \"year\"]\n\n# replacing all the zeros from revenue and budget cols.\nmovies[change_cols] = movies[change_cols].replace(0, np.nan)\n# dropping all the rows with na in the columns mentioned above in the list.\nmovies.dropna(subset=change_cols, inplace=True)\n# filter useless records\nbudget_filter = movies['budget'] > 1e6\nrevenue_filter = movies['revenue'] > 1e6\nmovies = movies[budget_filter & revenue_filter]\n\nmovies = movies.join(credits)\n\nparse_col_json(movies, \"crew\", \"name\", True)\nmovies.rename(columns={'crew': 'directors'}, inplace=True)\nparse_col_json(movies, 'genres', 'name', False)\nparse_col_json(movies, 'production_companies', 'name', False)\nparse_col_json(movies, 'cast', 'name', False)\n\n\n\n# changing data type\nmovies[change_cols] = movies[change_cols].applymap(np.int64)\n\n#movies['profit'] = movies['revenue'] - movies['budget']\n\n# columns list usefull for later manipulation\nlist_columns = [\"directors\", \"genres\",\"production_companies\",\n                \"cast\", \"actors\", \"actress\"]\n\nmovies = movies[['title', 'genres', 'directors', 'production_companies',\n                 'year', 'cast', 'actors','actress', 'runtime', 'budget',\n                 'revenue', 'popularity','vote_average']]\n\n","c08c7780":"movies = movies.set_index(\"title\")\nmovies = movies.reset_index()\nmovies.head(3)\n\n","a1411554":"summary = counts_elements(movies, list_columns)\ndirectors = pd.Series(summary[\"directors\"], name=\"movies\")\ngenres = pd.Series(summary[\"genres\"], name=\"movies\")\np_companies = pd.Series(summary[\"production_companies\"], name=\"movies\")\ncast = pd.Series(summary[\"cast\"], name=\"movies\")\n\nactors = pd.Series(summary[\"actors\"], name=\"movies\")\nactress = pd.Series(summary[\"actress\"], name=\"movies\")\n","a78f7cee":"df1 = pd.DataFrame({ 'gender' : [\"male\" for _ in range(actors.size)]})\ndf1[\"name\"] = actors.index\ndf1[\"movies\"] = actors.values\ndf1.drop(df1.tail(1).index,inplace=True) # drop last n rows\n\ndf2 = pd.DataFrame({ 'gender' : [\"female\" for _ in range(actress.size)]})\ndf2[\"name\"] = actress.index\ndf2[\"movies\"] = actress.values\ndf2.drop(df2.tail(1).index,inplace=True)\n\ncastDf = pd.concat([df1, df2], ignore_index=True)\ncastDf = castDf.set_index(\"name\")\ncastDf[\"movies\"] = castDf[\"movies\"].map(np.int64)","948ec846":"# create new df based on a definition of successful movie:\n# \"it's successful when it earns the double it costs\"\nsucc_movies = movies[movies[\"revenue\"] > movies[\"budget\"].apply(lambda x: 2*x)]\nsucc_summary = counts_elements(succ_movies, list_columns)\n\nsucc_directors = pd.Series(succ_summary[\"directors\"], name=\"succ_movies\")\nsucc_genres = pd.Series(succ_summary[\"genres\"], name=\"succ_movies\")\nsucc_p_companies = pd.Series(\n    succ_summary[\"production_companies\"], name=\"succ_movies\")\nsucc_cast = pd.Series(succ_summary[\"cast\"], name=\"succ_movies\")\n\nsucc_actors = pd.Series(succ_summary[\"actors\"], name=\"succ_movies\")\nsucc_actress = pd.Series(succ_summary[\"actress\"], name=\"succ_movies\")\n","582f9659":"df1 = pd.DataFrame(index = succ_actors.index)\ndf1[\"succ_movies\"] = succ_actors.values\ndf1.drop(df1.tail(1).index,inplace=True)\n\ndf2 = pd.DataFrame(index = succ_actress.index)\ndf2[\"succ_movies\"] = succ_actress.values\ndf2.drop(df2.tail(1).index,inplace=True)\n\nsucc_castDf = pd.concat([df1, df2], ignore_index=False)\nsucc_castDf[\"succ_movies\"] = succ_castDf[\"succ_movies\"].map(np.int64)\ncastDf = castDf.join(succ_castDf)","0c7844b2":"# defining target variable\nmovies[\"success\"] = movies.title.isin(succ_movies.title)\n","6e1fd0f7":"sns.pairplot(movies,hue=\"success\",vars=[\"year\",\"runtime\",\"budget\",\"revenue\",\"popularity\",\"vote_average\"])\nplt.show()\n\n","2b8d7572":"# let's see some correlations\nsns.set(rc={'figure.figsize': (12, 10)})\nsns.heatmap(movies.corr(), annot=True)\nplt.show()\n\n","ae19e541":"sns.heatmap(succ_movies.corr(), annot=True)\n# older movies had lower runtime\n# budget slightly increased across the years\n# longer movies could have higher votes\n# higher budget often means higher revenue (and perhaps popularity)\nplt.show()\n\n","f8bf7173":"fig, axes = plt.subplots(nrows=2, figsize=(5, 5))\ndirectors.nlargest(5).plot.barh(color=\"#99e699\", legend=True, ax=axes[0])\nsucc_directors.nlargest(5).plot.barh(color=\"#1a8cff\", legend=True, ax=axes[1])\nplt.show()\n\n","5d0c19db":"genres.plot.barh(color=\"#b3ecff\", legend=True)\nsucc_genres.plot.barh(color=\"#1a8cff\", legend=True)\nplt.show()\n\n","23ec4dd4":"p_companies.nlargest(5).plot.barh(color=\"#b3ecff\", legend=True)\nsucc_p_companies.nlargest(5).plot.barh(color=\"#1a8cff\", legend=True)\nplt.show()\n","6f90dd69":"fig, axes = plt.subplots(ncols=2, figsize=(10, 5),constrained_layout=True)\ncastDf.movies.nlargest(10).plot.barh(color=\"#99e699\", legend=True, ax=axes[0])\ncastDf.succ_movies.nlargest(10).plot.barh(color=\"#1a8cff\", legend=True, ax=axes[1])\naxes[0].set_xlim([20, 55])\naxes[1].set_xlim([20, 55])\nplt.show()","d3ef2a4a":"fig, axes = plt.subplots(ncols=2, figsize=(10, 5),constrained_layout=True)\ncastDf[castDf.gender==\"female\"].movies.nlargest(10).plot.barh(color=\"tomato\", legend=True, ax=axes[0])\ncastDf[castDf.gender==\"female\"].succ_movies.nlargest(10).plot.barh(color=\"purple\", legend=True, ax=axes[1])\naxes[0].set_xlim([10, 35])\naxes[1].set_xlim([10, 35])\nplt.show()","8ce536d3":"cast_counts = counts_vectorized(movies, \"cast\", 20)\ndir_counts = counts_vectorized(movies, \"directors\", 10)\ngen_counts = counts_vectorized(movies, \"genres\", 100)\ncompanies_counts = counts_vectorized(movies, \"production_companies\", 50)\n\n","1763af3a":"X=[]\n# concatenate vectors count\nX = hcat(movies, cast_counts, dir_counts,\n         gen_counts, companies_counts)\n\n","d11874da":"X = X.set_index(\"title\")\nX = X.drop(columns=[\"cast\", \"directors\", \"genres\", \"budget\",\"revenue\",\"popularity\",\"vote_average\",\n                    \"production_companies\", \"actors\",\"actress\"])\nX.head()","2b3be3df":"simplify(X, \"year\", (1, 1998, 2005, 2016), [\"vecchio\", \"contemporaneo\", \"moderno\"])\nsimplify(X, \"runtime\", (1, 72, 89, 120, 180, 250), [\"XS\", \"S\", \"M\", \"L\", \"XL\"])\n#simplify(X, \"popularity\", (-1, 11, 21, 876), [\"Low\", \"Medium\", \"High\"])\n#simplify(X, \"vote_average\", (0, 4, 6, 10), [\"basso\", \"normale\", \"alto\"])\n\ny = X.success\nX = X.drop(columns=\"success\")\n# for one hot encoding\nX_h = pd.get_dummies(X)\n# label encoder\nX_l = encode_features(X)\n","df11d39f":"X_train, X_test, y_train, y_test = train_test_split(\n    X_l, y, test_size=0.2, random_state=1)\nmc_l = defaultdict(list)\n","9076a7e3":"# generate lists of parameters for GridSearch\nC_list = [C for C in (0.0001*(10**p) for p in range(1, 8))]\ngamma_l = [C for C in [0.1**n for n in range(1, 8)]]\nparam_grid = {'C': C_list, 'gamma': gamma_l, 'kernel': ['rbf']}\n# Make grid search classifier\nclf = GridSearchCV(SVC(), param_grid, verbose=0, cv=5).fit(X_train, y_train)\nC = clf.best_params_['C']\ngamma = clf.best_params_['gamma']\ntestClassifier(SVC(kernel='rbf', C=C, gamma=gamma), \"Optimized RBF SVM\",mc_l)\n","c2941755":"testClassifier(KNeighborsClassifier(), \"K-Nearest Kneighbors\",mc_l)\n","c4040016":"testClassifier(LogisticRegression(), \"Logistic Regression\",mc_l)\n","82edb6f8":"testClassifier(DecisionTreeClassifier(), \"Decision Tree\",mc_l)\n","08827e34":"clf = RandomForestClassifier()\n# Type of scoring used to compare parameter combinations\nacc_scorer = make_scorer(accuracy_score)\n# Run the grid search\ngrid_obj = GridSearchCV(\n    clf, parameters, scoring=acc_scorer).fit(X_train, y_train)\n# Set the clf to the best combination of parameters\nclf = testClassifier(grid_obj.best_estimator_, \"Random Forest\",mc_l)\n\nfeature_importances = pd.DataFrame(clf.feature_importances_,\n                                   index = X_train.columns,\n                                    columns=['importance']).sort_values('importance',ascending=False)\n","97005c62":"feature_importances.importance.nlargest(40).plot.barh()","a8fae6c3":"testClassifier(XGBRegressor(n_estimators=1000, learning_rate=0.05), \"Gradient Boosting\",mc_l)\n\n","2d36e570":"comparison_l = pd.DataFrame(mc_l, columns=['classifier','accuracy','fallout'])\n","750d0ec5":"from sklearn.decomposition import PCA\npca = PCA(.85) # retain 85% of the variance\nX_r = pca.fit_transform(X_l)\n\n","84bbe216":"X_train, X_test, y_train, y_test = train_test_split(\n    X_r, y, test_size=0.2, random_state=1)\nmc_r = defaultdict(list)\n\n### SVM\ntestClassifier(SVC(kernel='rbf', C=C, gamma=gamma), \"Optimized RBF SVM\",mc_r)\n\n### Random Forest\nclf = RandomForestClassifier()\nacc_scorer = make_scorer(accuracy_score)\ngrid_obj = GridSearchCV(clf, parameters, scoring=acc_scorer).fit(X_train, y_train)\ntestClassifier(grid_obj.best_estimator_, \"Random Forest\",mc_r)\n\n### XGBR\ntestClassifier(XGBRegressor(n_estimators=1000, learning_rate=0.05), \"Gradient Boosting\",mc_r)\n\ncomparison_r = pd.DataFrame(mc_r, columns=['classifier','accuracy','fallout'])\n","f5eb9903":"X_train, X_test, y_train, y_test = train_test_split(\n    X_h, y, test_size=0.2, random_state=1)\nmc_h = defaultdict(list)\n\n### SVM\ntestClassifier(SVC(kernel='rbf', C=C, gamma=gamma), \"Optimized RBF SVM\",mc_h)\n### Random Forest\nclf = RandomForestClassifier()\nacc_scorer = make_scorer(accuracy_score)\ngrid_obj = GridSearchCV(clf, parameters, scoring=acc_scorer).fit(X_train, y_train)\ntestClassifier(grid_obj.best_estimator_, \"Random Forest\",mc_h)\n### XGBR\ntestClassifier(XGBRegressor(n_estimators=1000, learning_rate=0.05), \"Gradient Boosting\",mc_h)\n\ncomparison_h = pd.DataFrame(mc_h, columns=['classifier','accuracy','fallout'])\n\n","8b1d9df0":"comparison_h = comparison_h.set_index(\"classifier\")\ncomparison_r = comparison_r.set_index(\"classifier\")\ncomparison_l = comparison_l.set_index(\"classifier\")\n\n","edd7bcff":"comparison = comparison_h.join(comparison_r,rsuffix = \"_pca\",lsuffix=\"_1hot\")\ncomparison = comparison.join(comparison_l,rsuffix = \"_label\")\n\n","9a9b7a14":"comparison.plot.bar()\nplt.show()\n","62b12c9e":"   # Section 4: classification\n   ## Tested Classifiers:\n    * SVM with radial kernel\n    * K-Nearest Neighbors\n    * Logistic Regession\n    * Decision Tree\n    * Random Forest\n    * Gradient Boost Regressor","cebc63a5":"  ## K-Fold Cross Validation:\n\n  In this method we randomly split the training set into k folds, approximately of the same size. The first fold is considered as a validation set. The procedure is repeated k times, each time a different folder is selected as validation set and produces a different value for the MSE. The effective estimate will be the average of these values.\n\n  $$CV_{(K)} = \\sum\\limits_{k=1}^K {n_k\\over{n}}MSE_k $$\n\n  This approach is very useful for two main reasons (despite other evaluation techniques such as LOOCV):\n  - Is computationally feasible.\n  - It produces low variance.\n\n  ### Mean square error\n  MSE is the average squared loss per example over the whole dataset. To calculate MSE, sum up all the squared losses for individual examples and then divide by the number of examples:\n  $$MSE = {1\\over{N}}\\sum\\limits_{(x,y)\\in D}(y-\\text{prediction}(x))^2$$","6b982b29":"   ## K-Nearest Neighbors Classifier\n   k-nearest neighbors algorithm (k-NN) is a non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:\n\n   In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small).\n\n   If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n\n   The hyperparameters of KNN include the following ones, which can be passed to the KNeighborsClassifier of sklearn.neighbors:\n\n   * n_neighbors: corresponds to K, the number of nearest neighbors considered for the prediction (default=5)\n   * weights:\n       * if uniform, then all neighbors have the same weight for the voting (default)\n       * if distance, then the votes of the neighbors are weighted by the inverse of the distance for the voting\n   * p: the power parameter for the Minkowski metric (default=2)","0be0856e":"   # Conclusion:\n\n  We started our analysis with this aim: to find out the key success factors on the film industry, and to try to use that factors in order to predict if a movie will be succesful or not.\n\n  We found out some interesting facts, for example:\n\n  - older movies had lower runtime\n  - budget slightly increased across the years\n  - longer movies tend to have higher votes\n  - higher budget often means higher revenue and popularity\n\n  **Results**:\n\n  Our **Top Director** is **Steven Spielberg**, with almost all of his production is considered a success.\n\n  The most used **genre** is **Drama**, althought **Comedy** movies has very great aspect: **100%** of them are succesful.\n\n  **Universal Pictures** is the most prolific company, and also the most successful.\n\n  As we could expect, **USA** is the most prolific and successful country, with no rivals pratically.\n\n  The Top Actor is **Samuel L. Jackson** with 30+ succesful movies. Robert De Niro represents an interesting case, passing from 2nd position, when counting the amount of attendees, to out of top-5, if we consider attendees in successful movies only.\n\n  Classifiers:\n\n  The best classifier overall seems the Gradient Boosting classifier, for accuracy and false positive rate (who was selected as critical measure arbitrarily).\n\n  But it's a pretty fair competition between XGB, RF and **SVM**, who outperforms the two ensemble methods in a reduced space of approximately 20 features (against the 40+ of the initial space)","1b277321":"### Women only:","75f1bddb":"   ## Logistic Regression\n   Logistic regression allow us to use regression as classifier.\n   It models the probabilities for classification problems with two possible outcomes.\n   It\u2019s an extension of the linear regression model for classification problems.\n   Instead of fitting a straight line or hyperplane, the logistic regression model uses the logistic(sigmoid) function to squeeze the output of a linear equation between 0 and 1.\n\n   **Sigmoid**: $g(h) = \\frac{1}{1+e^{-h}} $\n\n   We define $x_i$ as the n-dimensional feature vector of a given sample and $\u03b2_0,\u03b2=(\u03b2_1,...,\u03b2_n)^T$ as the model parameters. Then the logistic regression model is defined as:\n\n   $$P(Y=1|x_i)={{exp(\u03b2_0+x^T_i\u03b2)}\\over{1+exp(\u03b2_0+x^{T}_i\u03b2)}}={1\\over{1+exp(\u2212(\u03b2_0+x^T_i\u03b2))}}$$\n\n   The interpretation of the weights in logistic regression differs from the interpretation of the weights in linear regression, since the outcome in logistic regression is a probability between 0 and 1. The weights do not influence the probability linearly any longer. The weighted sum is transformed by the logistic function to a probability, so\n  the logistic regression model is not only a classification model, but also gives you probabilities.\n\n  Note that both\u00a0linear regression\u00a0and\u00a0logistic regression\u00a0give you a straight line (or a higher order polynomial) but those lines have different meaning:\n  - h(x)\u00a0for linear regression interpolates, or extrapolates, the output and predicts the value for\u00a0x\u00a0we haven't seen. It's simply like plugging a new\u00a0x\u00a0and getting a raw number, and is more suitable for tasks like predicting, say car price based on\u00a0{car size, car age}\u00a0etc.\n\n  - h(x)\u00a0for **logistic regression** tells you the\u00a0probability\u00a0that\u00a0x\u00a0belongs to the \"positive\" class. This is why it is called a regression algorithm - it estimates a continuous quantity, the probability. However, if you set a threshold on the probability, such as\u00a0$h(x)>0.5$ , you obtain a classifier, and in many cases this is what is done with the output from a logistic regression model. This is equivalent to putting a line on the plot: all points sitting above the classifier line belong to one class while the points below belong to the other class.\n\n\n   The hyperparameters of a logistic regression include the following ones, which can be passed to the LogisticRegression of sklearn.linear_model:\n\n   * penalty: the norm used for penalization (default='l2')\n   * C: the inverse of the regularization strength (default=1.0)","4985a1fd":"   ## Gradient Boosting Classifier\n   XGBoost is an ensemble learning method. Ensemble learning offers a systematic solution to combine the predictive power of multiple learners. The resultant is a single model which gives the aggregated output from several models.\n\n   #### Bagging\n\n   While decision trees are one of the most easily interpretable models, they exhibit highly variable behavior. Consider a single training dataset that we randomly split into two parts. Now, let\u2019s use each part to train a decision tree in order to obtain two models.\n\n   When we fit both these models, they would yield different results. Decision trees are said to be associated with high variance due to this behavior. Bagging or boosting aggregation helps to reduce the variance in any learner. Several decision trees which are generated in parallel, form the base learners of bagging technique. Data sampled with replacement is fed to these learners for training. The final prediction is the averaged output from all the learners.\n\n   #### Boosting\n   It is an ensemble technique where new models are added to correct the errors made by existing models, sequentially until no further improvements can be made.\n\n   The base learners in boosting are weak learners in which the bias is high, and the predictive power is just a tad better than random guessing, but each of them contributes some vital information for prediction, enabling the boosting technique to produce a strong learner by effectively combining these weak learners, which brings down both the bias and the variance.\n\n   In contrast to bagging techniques like Random Forest, in which trees are grown to their maximum extent, boosting makes use of trees with fewer splits, not very deep, and highly interpretable.\n\n   Parameters like the number of trees or iterations, the rate at which the gradient boosting learns, and the depth of the tree, could be optimally selected through validation techniques like k-fold cross validation.\n\n   Having a large number of trees might lead to overfitting, so it is necessary to carefully choose the stopping criteria for boosting.\n\n   Boosting consists of three simple steps:\n\n   1. An initial model $F_0$ is defined to predict the target variable y. This model will be associated with a residual $(y \u2013 F_0)$\n   2. A new model $h_1$ is fit to the residuals from the previous step\n   3. Now,$ F_0$ and $h_1$ are combined to give $F_1$, the boosted version of $F_0$. The mean squared error from $F_1$ will be lower than that from$ F_0$:\n\n   $$F_1(x)\\leftarrow F_0(x)+h_1(x)$$\n\n   To improve the performance of $F_1$, we could model after the residuals of $F_1$ and create a new model $F_2$:\n\n   $$F_2(x)\\leftarrow F_1(x)+h_2(x)$$\n\n   This can be done for \u2018m\u2019 iterations, until residuals have been minimized as much as possible:\n   $$F_m(x)\\leftarrow F_{m-1}(x)+h_{m}(x)$$\n\n   Here, the additive learners do not disturb the functions created in the previous steps. Instead, they impart information of their own to bring down the errors.\n\n\n   **Gradient boosting** is an approach where new models are created that predict the residuals or errors of prior models and then added together to make the final prediction. It is called gradient boosting because **it uses a gradient descent algorithm** to minimize the loss when adding new models.\n\n   This approach supports both regression and classification predictive modeling problems.","cea54cee":"  Basic idea of support vector machines:\n\n  \u2013 **Hard margin classifier**: to find optimal hyperplane who maximizes the margin (the distance between the two classes) for linearly separable patterns\n\n  \u2013 **Soft margin classifier**: introducing some tolerance factor in the boundary in order to improve performance\n\n  \u2013 **The Kernel trick**: extend to patterns that are not linearly separable by transformations of original data to map into new space\n\n  ---\n  ### Hard margin classifier\n\n   In order to define the optimal separating hyperplane we define two planes:\n\n   $H_1:w\u22c5x_i+b= +1$\n\n\n   $H_2:w\u22c5x_i+b=\u22121$\n\n\n   Furthemore, there is a plane in between:\n   $H_0:w\u22c5x_i+b=0$\n\n   We want a linear separator with the biggest margin possible.\n\n   The distance between $H_0$ and $H_1$ is $\\frac{1}{\\|w\\|}$\n\n   we want $ \\underset{w,b}{\\text{maximize}} {1\\over{\\|w\\|}} \\text{ subject to  } y_i[\\langle{x_i,w}\\rangle + b]\\ge 1 $\n\n   This becomes a Quadratic programming problem and can be solved by optimization techniques (we use Lagrange multipliers to get this problem into a form that can be solved analytically).\n\n   Because it is quadratic, the surface is a paraboloid, with just a single global minimum (thus avoiding a problem we had with neural nets!)\n\n  ---\n  ### Soft margin classifier\n\n  Sometimes data has errors or is just non linearly separable and we want to be less strict separing them to obtain a better solution. So we want to be permissive for certain examples, allowing that their classification by the separator diverge from the real class\n\n  This can be obtained by adding to the problem what is called slack variables $\\xi$ that represent the deviation of the examples from the margin\n\n  Doing this we obtain some kind of *relaxed* **soft margin**\n\n  We need to introduce this slack variables on the original problem, we have now:\n\n   $$ \\text{minimize}{1\\over{2}}\\|w\\|^2+C\\sum\\limits_{i=1}^{n}\\xi_i \\text{ subject to  } y_i[\\langle{x_i,w}\\rangle + b]\\ge 1-\\xi_i,     \\forall i,\\xi_i\\ge 0 $$\n\n   Now we have an additional parameter C that is the tradeoff between the error and the margin\n\n  ---\n  ### The Kernel trick\n\n  In order to have better performance we have to be able to obtain non-linear boundaries, the idea is to transform the data from the input space (the original attributes of the examples) to a higher dimensional space, using a function $\\phi(x)$, called the **feature space**. The advantage of the transformation is that linear operations in the feature space are equivalent to non-linear operations in the input space.\n\n  Working directly in the feature space can be costly because we have to explicitly create the feature space and operate in it. Furthemore we may need infinite features to make a problem linearly separable.\n\n  So we use what is called the Kernel trick:\n\n  In the problem that define a SVM only the inner product of the examples is needed, so if we can define how to compute this product in the feature space, then it won't be necessary to explicitly build it, but we just need to define a kernel function $$K \\text{  such that  } K(x_i,x_j)=\\phi(x_i)\\cdot\\phi(x_j)$$\n\n  then we do not need to know or compute \u03c6 at all! The kernel function defines inner products in the transformed space, in practice it defines a measure of similarity in the transformed space.\n\n   **Radial Kernel** $$ K(x_i,x_{i'}) = exp(-\\gamma \\sum\\limits_{j=1}^{p}(x_{ij}-x_{i'j})^2) $$\n   thanks to kernels we can project our unseparable data in a non-linear space where they become separable\n\n   The hyperparameters of a SVM include the following ones, which can be passed to the SVC of sklearn.svm:\n\n   * C: the inverse of the regularization strength\n   * kernel: the kernel used (default='rbf')\n   * gamma: The higher the gamma value it tries to exactly fit the training data set","b3909183":"   # Movies Dataset Analysis\n\n   ### Section 1: data cleaning and manipulation\n   ### Section 2: data exploration\n   ### Section 3: data pre-processing for classification\n   ### Section 4: classification\n   ---","595ac5ad":" ## One hot encoding\n It is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction.","82bba3f0":"   # Section 3: data pre-processing for classification\n   ### applying \"bag of words\" technique to list columns, binning and encoding categorical labels","5ea19e01":" ## About the datasets:\n We are going to analyze two datasets, *tmdb_5000_credits* and *tmdb_5000_movies*.\n\n First one contains **4803 observations** with the following columns:\n  * movie_id\n  * title\n  * cast\n  * crew\n\nSecond one contains 4803 observations with the following columns:\n  * budget, genres, homepage, id, keywords, original_language\n  * original_title, overview, popularity, production_companies\n  * production_countries, release_date, revenue runtime\n  * spoken_languages, status, tagline, title, vote_average, vote_count\n\nWe will merge the two datasets in order to get all the informations about the actors and the directors of their relative movie.\n\nWe will try to find out which are the key factors who affect the success of a movie.\n\n **In the end we will use them to predict if a movie is successful or not**.","94133bf9":"   ## Random Forest\n   A random forest is an ensemble model that fits a number of decision tree classifiers on various sub-samples of the dataset which are created by the use of bootstrapping. In the inference stage it uses a majority vote over all trees to obtain the prediction. This improves the predictive accuracy and controls over-fitting.\n\n   The hyperparameters of a random forest include the following ones, which can be passed to the RandomForestClassifier of sklearn.ensemble:\n\n   * n_estimators: the number of trees\n   * criterion: the criterion which decides the feature and the value at the split (default='gini')\n   * max_depth: the maximum depth of each tree (default=None)\n   * min_samples_split: the minimum number of samples in a node to be considered for further splitting (default=2)\n   * max_features: the number of features which are considered for a split (default='sqrt')","a88ce6ec":"   # Section 2: data exploration\n   ### looking for correlations between variables, finding new *successful* movies and compare them","5623c7c5":"   ## Decision Tree Classifier\n   A decision tree for classification consists of several splits, which determine for a input sample, the predicted class, which is a leaf node in the tree. The construction of the decision trees is done with a greedy algorithm, because the theoretical minimum of function exists but it is NP-hard to determine it, because number of partitions has a factorial growth. Specifically, a greedy top-down approach is used which chooses a variable at each step that best splits the set of items. For measuring the \"best\" different metrics can be used, which generally measure the homogeneity of the target variable within the subsets.\n\n   For this analysis we consider the following two metrics:\n\n   * Gini impurity: Let j be the number of classes and $p_i$ the fraction of items of class i in a subset p, for i\u2208{1,2,...,j}. Then the gini impurity is defined as follows:\n   $$I_G(p)=1-\\sum\\limits_{i=1}^{j}p_i^2$$\n   * Information gain: It measures the reduction in entropy when applying the split. The entropy is defined as $$H(t)=\u2212\\sum\\limits_{i=1}^{j}p_i\\log[2]p_i$$\n\n   Then we define the information gain to split n samples in parent node p into k partitions, where $n_i$ is the number of samples in partition i as $$I_G=H(p)-\\sum\\limits_{i=1}^{k}{n_i\\over{n}}H(i)$$\n\n   The hyperparameters of a Decision Tree include the following ones, which can be passed to the DecisionTreeClassifier of sklearn.tree:\n\n   * criterion: the criterion which decides the feature and the value at the split (default='gini')\n   * max_depth: the maximum depth of each tree (default=None)\n   * min_samples_split: the minimum number of samples in a node to be considered for further splitting (default=2)","aa916e7d":" ## [How much money does a movie need to make to be profitable?](https:\/\/io9.gizmodo.com\/how-much-money-does-a-movie-need-to-make-to-be-profitab-5747305)\n\n >  a rule of thumb seems to be that the film needs to make twice its production budget globally","48ab2d45":"   ## Binning\n   Data binning (also called Discrete binning or bucketing) is a data pre-processing technique used to reduce the effects of minor observation errors. The original data values which fall in a given small interval, a bin, are replaced by a value representative of that interval, often the central value. It is a form of quantization, the process of transform numerical Variable into categorical counterparts.\n\n   ## Label Encoder\n   It is used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels.","269def48":"   In order to investigate the pair-wise correlations between two variables X and Y, we use the Pearson correlation. Let $\\sigma_X,\\sigma_Y$ be the standard deviation of $X,Y$ and $cov(X,Y)=E[(X\u2212E[X])(Y\u2212E[Y])]$.\n\n  Then we can define the Pearson correlation as the following:\n\n\n   $$\\rho_{X,Y} = \\frac {cov(X,Y)}{\\sigma_X\\sigma_Y}$$\n\n\n    It has a value between +1 and \u22121, where 1 is total positive linear correlation, 0 is no linear correlation, and \u22121 is total negative linear correlation.","dccd252f":"   ## Bag of Words\n   Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.\n\n   In order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:\n\n   *     **tokenizing** strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators.\n   *     **counting** the occurrences of tokens in each document.\n   *     **normalizing** and weighting with diminishing importance tokens that occur in the majority of samples \/ documents.\n\n   A corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus.\n\n   We call **vectorization** the general process of turning a collection of text documents into numerical feature vectors.\n   This specific strategy (tokenization, counting and normalization) is called the **Bag of Words** or \u201cBag of n-grams\u201d representation.\n   Documents are described by word occurrences while completely ignoring the relative position information of the words in the document.","7efca5d5":"  ## Principal Component Analysis\n  PCA is used to find orthonormal basis for data. It sorts dimensions in order of \"Importance\" and enable you to discard lower significance dimensions.\n\n  We have to find a lower dimension linear space that\n  \t1. Maximizes variance of projected data\n  \t2. Minimizes mean squared distance between data points and projections\n\n  That lower dimensional space must preserves as much information as possible, in particular it minimizes the squared error in reconstructing the orgianl data\n\n  Each PCA vector originates from the center of mass. The first one points in the direction of the largest variance. Each subsequent principal component is orthogonal to the previous ones and points in the direction of the largest variance of the residual subspace\n\n  **PCA steps:**\n  * calculate the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix, or perform Singular Vector Decomposition.\n  * sort the Eigenvectors according to their Eigenvalues in decreasing order\n  * build the $n\u00d7k-dimensional$ projection matrix $W$ by putting the top $k$ Eigenvectors into the columns of $W$\n  * transform the dataset $X$ by multiplying it with $W: X_t=XW$","ff72be39":"   # Section 1: data cleaning and manipulation\n   ### Removing duplicates, useless columns, filtering records, and parsing json","62a99868":"   ## Our question is:\n   ** Which are the most important factors that make a movie succesful?**\n\n   and of course, can we predict in advance if a movie will be successful?\n\n   ### Let's start the investigation!","0c6751f1":"  ## Grid Search:\n\n  After that we've seen the meaning and the differents possibilities to build our hyperplane, the big question is **\"How to choose the hyper parameters?\"**\n\n  We cannot choose the optimal values based on our test set; in real problems we won\u2019t have access to it!\n\n  The proper way to evaluate a method is to split the data into 3 parts:\n\n  * We choose some parameters and train your model on the training set.\n  * We then evaluate your performances on the validation set.\n  * Once we find the parameters which work best on the validation set, we can apply the same model on the test set.\n\n  This is the correct estimate of the accuracy you will get\n  on unseen data.\n\n  If there are two (or more) parameters to optimize could be tricky to find the best one, so we must estimate all the possible combinations of those parameters. In this case, we should put all the results in a table (the grid), with $\\gamma$ as column and $C$ in the rows, look for the best combination and finally select the best couple of C and gamma.\n\n  Luckily, **GridSearchCV** of sklearn package will handle this for us (with an additional feature like stratified CV)!","08d8a32c":"   ## Support Vector Classifier:\n   ### using grid search cross validation in order to find the best parameters"}}