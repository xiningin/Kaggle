{"cell_type":{"260ca69e":"code","e64cf03d":"code","69558a10":"code","16cdd586":"code","7691d0f6":"code","d9838901":"code","31070dad":"code","207f9fd1":"code","c847cb2d":"code","1b94b890":"code","85ee0e96":"code","a710bfd0":"code","f4cd65b7":"code","e1550e0b":"code","dce4d6cb":"code","189d1aa2":"code","dccc7139":"code","73671a2d":"code","bf428f7b":"code","ac58b428":"code","f5c5f6d4":"code","e4095a2c":"code","d545f458":"code","0c9015bf":"code","addccb94":"code","1fa4591e":"code","22acb908":"code","a58c242e":"code","bade3e4d":"code","a6a12a94":"code","30cb077f":"code","0a7b69cc":"code","d62a46ef":"code","bd8faac1":"code","f5b14339":"markdown","81f4f704":"markdown","fe4bd446":"markdown","6c1e30a2":"markdown","6106e3ed":"markdown","efb0f094":"markdown","3720e0b4":"markdown","c852e7a5":"markdown","c6334079":"markdown","480eb2a3":"markdown","53007498":"markdown","9e98f35c":"markdown","2d26169d":"markdown"},"source":{"260ca69e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(color_codes=True)\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e64cf03d":"data=pd.read_csv(\"..\/input\/us-airbnb-open-data\/AB_US_2020.csv\",low_memory=False)","69558a10":"data.head()","16cdd586":"data.info()","7691d0f6":"A=['neighbourhood_group','last_review','reviews_per_month']\n\nmissing = data[A].isna().sum()\/data.shape[0]*100\n\nmissing = missing.to_frame().rename(columns={0:'Percentage of missing values'})\nmissing","d9838901":"data=data.drop(['id','name','host_id','host_name','neighbourhood_group',\\\n                'last_review', \"reviews_per_month\"], axis=1)","31070dad":"data.describe()","207f9fd1":"plt.figure(figsize=(20,2))\nplt.title(\"Horizontal boxplot of price\", size=18)\nsns.boxplot(x=\"price\", data=data, showfliers = False, showmeans=True, palette=\"Set2\")\nplt.show()","c847cb2d":"data['price'].quantile(.95)","1b94b890":"# Removing outliers\nlower_bound = .0\nupper_bound = .95\ndata = data[data['price'].between(data['price'].quantile(lower_bound), \\\n            data['price'].quantile(upper_bound), inclusive=True)].reset_index(drop=True)","85ee0e96":"# Removing Outliers\n\niqr = data.copy()\niqr = iqr[iqr['calculated_host_listings_count'] < 10]\niqr = iqr[iqr['number_of_reviews'] < 200]\niqr = iqr[iqr['minimum_nights'] < 10]","a710bfd0":"numeric_ix =data.select_dtypes(include=['int64', 'float64']).columns\n\nfig, axes = plt.subplots(nrows=2, ncols=4)\naux = 0\nfig.set_figheight(17)\nfig.set_figwidth(25)\nfor row in axes:\n    for col in row:\n        iqr[numeric_ix[aux]].plot(kind='kde',ax=col)\n        col.set_title(numeric_ix[aux] +' Distribution',fontsize=16,fontweight='bold')\n        aux+=1\n        if aux==len(numeric_ix):\n            break","f4cd65b7":"plt.figure(figsize=(8,2))\nsns.countplot(y=\"room_type\", data=data)\nplt.title(\"Counts for room typs\", size=15)\n#plt.xlabel('count')\nplt.show()","e1550e0b":"plt.figure(figsize=(20,6))\nsns.violinplot(x=\"price\", y=\"room_type\", showfliers = False, data=data)\nplt.title(\"Distributions of prices depending from room type\", size=18)\nplt.show()","dce4d6cb":"plt.figure(figsize=(20,8))\nsns.countplot(y=\"neighbourhood\", data=data, order=data.neighbourhood.value_counts().iloc[:20].index)\nplt.title(\"Counts for Top populated neighbourhouds\", size=18)\nplt.show()","189d1aa2":"A=list(data.neighbourhood.value_counts().iloc[:20].index) # Top 20 neighbourhoods\n\nplt.figure(figsize=(20,8))\nsns.boxplot(x=\"price\", y=\"neighbourhood\", data=data.loc[data['neighbourhood'].isin(A)], \\\n            showfliers = False, palette=\"Set2\")\nplt.title(\"Boxplots of price for 20 most popular neighbourhoods\", size=18)\nplt.ylabel('')\nplt.show()","dccc7139":"#plt.figure(figsize=(20,2))\n#plt.title(\"Horizontal boxplot of price\", size=18)\n#sns.boxplot(x=\"minimum_nights\", data=data, showfliers = False, showmeans=True, palette=\"Set2\")\n#plt.show()","73671a2d":"plt.figure(figsize=(20,8))\nsns.countplot(y=\"city\", data=data, order=data.city.value_counts().index)\nplt.title(\"Counts for cities \", size=18)\nplt.show()","bf428f7b":"B=list(data.city.value_counts().iloc[:20].index) # Top 20 cities\n\nplt.figure(figsize=(20,8))\nsns.boxplot(x=\"price\", y=\"city\", data=data.loc[data['city'].isin(B)], \\\n            showfliers = False, palette=\"Set2\")\nplt.title('Boxplots of price for 10 most popular cities', size=18)\nplt.ylabel('Cities')\nplt.show()","ac58b428":"#Transforming categories of categorical features into numbers\n\nnumeric_ix=data.select_dtypes(include=['int64', 'float64']).columns.drop('price')\n\nto_categorical_list = ['neighbourhood','room_type','city']\nfor i in to_categorical_list:\n    data[i]=data[i].astype('category')\n    \nlabelencoder = LabelEncoder()\nfor i in to_categorical_list:\n    data[i] = labelencoder.fit_transform(data[i])\ndata.head()","f5c5f6d4":"train_set, test_set = train_test_split(data, test_size=0.2, random_state=42)","e4095a2c":"x_train=train_set.drop(['price'], axis=1)\ny_train=train_set['price']\n\nx_test=test_set.drop(['price'], axis=1)\ny_test=test_set['price']","d545f458":"# Choose a sample of train set to search best parameters for XGBRegressor, to spend less time\n\ntrain_sample=train_set.sample(frac=0.4, replace=True, random_state=42)","0c9015bf":"x_train_sample=train_sample.drop(['price'], axis=1)\ny_train_sample=train_sample['price']","addccb94":"#One Hot Encoding\n\nt = [('cat', OneHotEncoder(), ['room_type','city'])]\ncol_transform = ColumnTransformer(transformers=t,remainder='passthrough')\n\nx_train = pd.DataFrame(col_transform.fit_transform(x_train).toarray())\nx_test = pd.DataFrame(col_transform.fit_transform(x_test).toarray())\n\nx_train_sample = pd.DataFrame(col_transform.fit_transform(x_train_sample).toarray())","1fa4591e":"# Normalizing numerical data\n\nmean = x_train.mean(axis=0)\nx_train -= mean\nstd = x_train.std(axis=0)\nx_train \/= std\n\nx_test -= mean\nx_test \/= std\n\nx_train_sample -= mean\nx_train_sample \/= std","22acb908":"import xgboost as xgb","a58c242e":"booster = xgb.XGBRegressor()","bade3e4d":"from sklearn.model_selection import GridSearchCV\n\n# create Grid\nparam_grid = {'n_estimators': [100, 200, 300],\n              'learning_rate': [0.01, 0.05, 0.1], \n              'max_depth': [5, 7, 10],\n              'colsample_bytree': [0.6, 0.7, 1],\n              'gamma': [0.0, 0.1, 0.2]}\n\n# instantiate the tuned random forest\nbooster_grid_search = GridSearchCV(booster, param_grid, cv=3, n_jobs=-1)\n\n# train the tuned random forest\nbooster_grid_search.fit(x_train_sample, y_train_sample)\n\n# print best estimator parameters found during the grid search\nprint(booster_grid_search.best_params_)","a6a12a94":"# instantiate xgboost with best parameters\nbooster = xgb.XGBRegressor(colsample_bytree=1, gamma=0.0, learning_rate=0.1, \n                           max_depth=10, n_estimators=500, random_state=4)\n\n# train\nbooster.fit(x_train, y_train)\n\n# predict\ny_pred_train = booster.predict(x_train)\ny_pred_test = booster.predict(x_test)","30cb077f":"RMSE = np.sqrt(mean_squared_error(y_test, y_pred_test))\nprint(f\"RMSE: {round(RMSE, 4)}\")","0a7b69cc":"MEA = mean_absolute_error(y_test, y_pred_test)\nprint(f\"MAE: {round(MEA, 4)}\")","d62a46ef":"plt.figure(figsize=(20,8))\nsns.residplot(x=y_test[:1000],y=y_pred_test[:1000])\nplt.title('The residuals of a linear regression .', size=18)\nplt.show()","bd8faac1":"d=y_pred_test-y_test\n\nplt.figure(figsize=(20,8))\nplt.hist(d, bins=100)\nplt.title('The histogram of residuals of a linear regression.', size=18)\nplt.show()","f5b14339":"## 5. Build, Train and Evaluate Model","81f4f704":"We have a dataset of $226$K rows, describing different features of each appartement. It has numerical and categorical features. The target variable is the price. In first part we visualize and gain insights about dataset. In second part we do feature engineering. We use XGRegressor to build and train model to predict price. ","fe4bd446":"To better visualise distribution of numerical features we will remove outliers.","6c1e30a2":"### Table of Contents\n\n1.  [Quick Look at Data Structure](#1)\n\n2.  [Analysis of numerical features](#2)\n\n3.  [Analysis of categorical features](#3)\n\n4. [Feature Engineering](#4)\n\n5. [Build, Train and Evaluate Model](#5)","6106e3ed":"## 4. Feature Engineering","efb0f094":"## 2. Analysis of numerical features","3720e0b4":"We can observe that several attributes have outliers. For instance target variable price, while $75\\%$ of prices are inferior to $201$, maximum price is $25$K. Notice that squared-error loss places much more emphasis on observations with large absolute residuals $|y_i \u2212f(x_i)|$ during the fitting process. It is thus far less robust, and its performance severely degrades for long-tailed error distributions and especially for grossly mis- measured $y$-values (\u201coutliers\u201d). To avoid this problem we will we will remove top $5\\%$ gross values. Let visualize the boxplot of price and see $0.95$-quantile.","c852e7a5":"There are 226030 instances in the dataset. We observe that there are missing values for atributes nighbourhood_group, last_review and reviews_per_month.","c6334079":"We will remove this features and try to build a classifier on dataset without using this features. ","480eb2a3":"## 3. Analysis of categorical features","53007498":"## 1. Quick Look at Data Structure","9e98f35c":"The most expensive cities are Hawaii and Rhode Island.","2d26169d":"We see that within most popular neighbourhouds on airbnb Lahaina and Khei-Makena are the most expensive ones."}}