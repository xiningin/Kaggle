{"cell_type":{"e0d82a3f":"code","2bd11560":"code","deb14261":"code","6abe656d":"code","925658eb":"code","0d8059f9":"code","12d26c61":"code","2eeb0e28":"code","6e60c4d5":"code","7c49bee2":"code","f9335e38":"code","0346fa9c":"code","4b3079bc":"code","2fe5e393":"code","ae9a244c":"code","d85a58ea":"code","4c826f6b":"code","85d011f6":"code","e02232d9":"code","df1bb0e5":"code","277c8b96":"code","42c67daf":"code","d3f15598":"code","62f58c4e":"code","efc0cb47":"code","f95e352e":"code","3f76ae42":"code","423d643d":"code","20ebf39c":"code","698cfe6c":"code","da400f08":"code","e1c7e620":"code","f000b54d":"code","4e0bf3e3":"code","d1af6e51":"code","0c1b0ffb":"code","48ef6922":"code","fb4e35b2":"code","1d44e62c":"code","09412130":"code","7e5e6dd1":"code","7ef3d0d2":"code","329d5ef0":"code","6931c096":"code","a0392db0":"code","93470394":"code","73c20e93":"code","19026126":"code","62283d28":"code","80d07fb1":"code","35254def":"code","8018de4b":"code","9f93a9a5":"code","84b32e0c":"code","1abe351a":"code","bdafb98f":"code","d2fbe519":"code","c40ef413":"code","a09232af":"code","b6bf54e2":"code","c1a145db":"code","02bc8253":"code","9869f954":"code","645e04af":"code","6b2c1373":"code","18fc4319":"code","823989f8":"code","3ac4886d":"code","a8973e33":"code","cbc60294":"code","e367313a":"code","f7fd2206":"code","b8177361":"code","c96fdcd4":"code","0f836070":"code","83e20e6e":"code","58d82b82":"code","3f079ee1":"code","65a348af":"code","924c8ab8":"code","4455d9e8":"code","e07248cc":"code","d8a4e1f3":"code","ba5e1aff":"code","3ba49ed9":"code","ec22bc77":"code","cf6acb13":"code","5435b957":"code","f5fcdbaf":"code","7ee1545f":"code","6900b0f2":"code","663994cc":"code","6b8f27d1":"code","86bf7ed8":"code","92099711":"code","f9d22a0c":"code","425eec7d":"code","a6b395b0":"code","86f236a9":"code","49448303":"code","af7e9df7":"code","dff11ef6":"code","6d59821c":"code","6449174d":"code","b75069b1":"code","01d055fe":"code","e69bf81b":"code","89f6d74e":"code","fb223a7c":"code","92b12107":"code","3dcf5102":"code","a5761837":"code","90e77b7f":"code","9f631cc5":"code","3bd023bc":"code","7467f668":"code","6f4a9cba":"code","1ad05da7":"code","cdd3ad02":"code","024d0fa8":"code","935eb810":"code","581a8765":"code","819b56a0":"code","f10b0adc":"code","762d73b8":"code","46780402":"code","6006c251":"code","0dfb8c70":"code","ad1856ed":"code","92a0ab1d":"code","6f8ba2b4":"code","5ad143ac":"code","8c132662":"code","f283006b":"code","74821111":"code","d276c41f":"code","f4f43544":"code","36979517":"code","5bf05abd":"code","b853497e":"code","56f04361":"code","030a5d6d":"code","3e511810":"code","5035f9c5":"code","218c1003":"code","e3093060":"code","563cdcf3":"code","599fdedb":"code","c212b755":"code","7422e835":"code","0213c0c6":"markdown","fdbf32f7":"markdown","609ed235":"markdown","609b3518":"markdown","6bf68015":"markdown","463f5e17":"markdown","10447af5":"markdown","b868ff1f":"markdown","e963b147":"markdown","8362b902":"markdown","c414af58":"markdown","705bc11a":"markdown","b03d1ba1":"markdown","b55171d3":"markdown","8b81b6cf":"markdown","c03e9338":"markdown","4c89df7f":"markdown","748bf493":"markdown","d7550730":"markdown","46fee116":"markdown","2d60f724":"markdown","a259410e":"markdown","b91ca8ac":"markdown","286d2dc0":"markdown","c40271a7":"markdown","163485be":"markdown","b36c66c8":"markdown","50905b9b":"markdown","b77e8a87":"markdown","f312c38f":"markdown","f88e5948":"markdown","be9522c5":"markdown","e50b28f9":"markdown","c1218cb9":"markdown","777b32ff":"markdown","3cb644d8":"markdown","4878b89f":"markdown","94ef7c5a":"markdown","e30dc38e":"markdown","02f331eb":"markdown","f4a320c7":"markdown","1a364615":"markdown","a3052124":"markdown","6e0e129c":"markdown","86451330":"markdown","90d6b18d":"markdown","f512764d":"markdown","8184d09f":"markdown","3fbc3066":"markdown"},"source":{"e0d82a3f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# For data visualization\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns; sns.set()\n\n# plotly\n# import plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly as py\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\n# Disabling warnings\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2bd11560":"data = pd.read_csv(\"\/kaggle\/input\/videogamesales\/vgsales.csv\")\ndata1 = data.copy()","deb14261":"display(data1.head())\ndisplay(data1.tail())","6abe656d":"data1.info()","925658eb":"data1.isnull().sum()","0d8059f9":"display(data1.Platform.unique())\ndisplay(data1.Genre.unique())\ndisplay(data1.Year.unique())","12d26c61":"display(len(data1.Platform.unique()))\ndisplay(len(data1.Genre.unique()))\ndisplay(len(data1.Publisher.unique()))","2eeb0e28":"data1['Platform'].replace('2600', 'Atari', inplace=True)","6e60c4d5":"sorted((data1.Year.unique()))","7c49bee2":"data1[data1.Year>2017]","f9335e38":"data1 = data1[data1.Year<2018]","0346fa9c":"data1.head()","4b3079bc":"data1[data1.Publisher.isna()]","2fe5e393":"data1.Publisher.fillna('Unknown', inplace=True)","ae9a244c":"data1.Year.fillna(data1.Year.mode()[0], inplace=True)","d85a58ea":"data1.isnull().sum()","4c826f6b":"data1.Year = data1.Year.astype('int64')\ndata1.head()","85d011f6":"data1.info()","e02232d9":"data1.describe().T","df1bb0e5":"data1.corr()","277c8b96":"data1.head()","42c67daf":"display(data1.Platform.value_counts())\ndisplay(data1.Genre.value_counts())\ndisplay(data1.Publisher.value_counts())","d3f15598":"plt.subplots(1,1)\nsns.countplot(data1.Platform, order=data1.Platform.value_counts().iloc[:14].index)\nplt.xticks(rotation= 45)\nplt.title(\"Video Games by Platform Top15\",color = 'blue',fontsize=15)\nplt.show()\n\nplt.subplots(1,1)\nsns.countplot(data1.Genre, order=data1.Genre.value_counts().index)\nplt.title(\"Video Games by Genre\",color = 'blue',fontsize=15)\nplt.xticks(rotation= 75)\nplt.show()\n\nplt.subplots(1,1)\nsns.countplot(data1.Publisher, order=data1.Publisher.value_counts().iloc[0:14].index)\nplt.title(\"Video Games by Publisher Top15\",color = 'blue',fontsize=15)\nplt.xticks(rotation= 90)\nplt.show()","62f58c4e":"# data1.groupby('Genre')['Global_Sales'].mean().sort_values(ascending=False)","efc0cb47":"order_genre = data1.groupby('Genre')['Global_Sales'].mean().sort_values(ascending=False).index\norder_genre","f95e352e":"plt.figure(figsize=(10, 5))\nsns.barplot(x=data1.Genre, y=data1.Global_Sales, order=order_genre);\nplt.xticks(rotation= 45)\nplt.xlabel('Genre', fontsize=14)\nplt.ylabel('Average Global Sales (Million)', fontsize=14)\nplt.title('Average Global Sales by Genre', color = 'blue', fontsize=15)\nplt.show()","3f76ae42":"# data1.groupby('Platform')['Global_Sales'].mean().sort_values(ascending=False).head(15)","423d643d":"order_platform = data1.groupby('Platform')['Global_Sales'].mean().sort_values(ascending=False).head(15).index\norder_platform","20ebf39c":"plt.figure(figsize=(10, 5))\nsns.barplot(x=data1.Platform, y=data1.Global_Sales, order=order_platform);\nplt.xticks(rotation= 45)\nplt.xlabel('Platform', fontsize=14)\nplt.ylabel('Average Global Sales (Million)', fontsize=14)\nplt.title('Average Global Sales by Platform Top15', color = 'blue', fontsize=15)\nplt.show()","698cfe6c":"# data1.groupby('Publisher')['Global_Sales'].mean().sort_values(ascending=False).head(15)","da400f08":"order_publisher = data1.groupby('Publisher')['Global_Sales'].mean().sort_values(ascending=False).head(15).index\norder_publisher","e1c7e620":"plt.figure(figsize=(10, 5))\nsns.barplot(x=data1.Publisher, y=data1.Global_Sales, order=order_publisher)\nplt.xticks(rotation= 90)\nplt.xlabel('Genre', fontsize=14)\nplt.ylabel('Average Global Sales (Million)', fontsize=14)\nplt.title('Average Global Sales by Publisher', color = 'blue', fontsize=15)\nplt.show()","f000b54d":"trace1 = go.Histogram(\n    x=data1.Global_Sales,\n    opacity=0.75,\n    name = \"2011\",\n    marker=dict(color='rgba(171, 50, 96, 0.6)'))\ndata = [trace1]\nlayout = go.Layout(barmode='overlay',\n                   title='global sales distribution',\n                   xaxis=dict(title='Sales (Million)'),\n                   yaxis=dict( title='Count'),\n)\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","4e0bf3e3":"import scipy.stats as stats\nstats.describe(data1.Global_Sales)","d1af6e51":"# data1.groupby('Year')['Global_Sales'].sum()","0c1b0ffb":"global_sales_year = data1.groupby('Year')['Global_Sales'].sum()\nglobal_sales_year_index = data1.groupby('Year')['Global_Sales'].sum().index","48ef6922":"plt.figure(figsize=(15, 5))\nsns.barplot(x=global_sales_year_index, y=global_sales_year)\nplt.xticks(rotation= 45)\nplt.xlabel('Genre', fontsize=14)\nplt.ylabel('Global Sales (Million)', fontsize=14)\nplt.title('Total Global Sales by Genre over the Years', color = 'blue', fontsize=15)\nplt.show()","fb4e35b2":"data1.head()","1d44e62c":"na_sales_year = data1.groupby('Year')['NA_Sales'].sum()\nna_sales_year_index = data1.groupby('Year')['NA_Sales'].sum().index\neu_sales_year = data1.groupby('Year')['EU_Sales'].sum()\neu_sales_year_index = data1.groupby('Year')['EU_Sales'].sum().index\njp_sales_year = data1.groupby('Year')['JP_Sales'].sum()\njp_sales_year_index = data1.groupby('Year')['JP_Sales'].sum().index\nother_sales_year = data1.groupby('Year')['Other_Sales'].sum()\nother_sales_year_index = data1.groupby('Year')['Other_Sales'].sum().index\n\n# visualization\nf,ax = plt.subplots(figsize = (15,10))\nsns.barplot(y=global_sales_year, x=global_sales_year_index, color='yellow',alpha = 0.3,label='Global' )\nsns.barplot(y=na_sales_year, x=na_sales_year_index,color='green',alpha = 0.5,label='NA' )\nsns.barplot(y=eu_sales_year, x=eu_sales_year_index, color='blue',alpha = 0.5,label='EU')\nsns.barplot(y=jp_sales_year, x=jp_sales_year_index,color='red',alpha = 0.7,label='JP')\nsns.barplot(y=other_sales_year, x=other_sales_year_index, color='cyan',alpha = 0.5,label='Other')\n\nplt.xticks(rotation= 45)\nax.legend(loc='upper right',frameon = True)\nax.set(xlabel='Year', ylabel='Sales (Million)',title = \"Global and Regional Total Sales over the Years\")\nplt.show()","09412130":"import plotly.graph_objs as go\n\ntrace1 = go.Scatter(\n                    x = global_sales_year_index,\n                    y = global_sales_year,\n                    mode = \"lines\",\n                    name = \"Global\")\ntrace2 = go.Scatter(\n                    x = global_sales_year_index,\n                    y = na_sales_year,\n                    mode = \"lines+markers\",\n                    name = \"NA\")\n\n\ntrace3 = go.Scatter(\n                    x = global_sales_year_index,\n                    y = eu_sales_year,\n                    mode = \"lines\",\n                    name = \"EU\",\n                    line = dict(dash=\"dot\"))\n\ntrace4 = go.Scatter(\n                    x = global_sales_year_index,\n                    y = jp_sales_year,\n                    mode = \"lines\",\n                    name = \"JP\",\n                    line = dict(dash=\"dash\"))\n\ntrace5 = go.Scatter(\n                    x = global_sales_year_index,\n                    y = other_sales_year,\n                    mode = \"lines\",\n                    name = \"Other\")\n\n\ndata = [trace1, trace2, trace3, trace4, trace5]\nlayout = dict(title = 'Global and Regional Total Sales over the Years',\n              xaxis= dict(title= 'Year',ticklen= 5,zeroline= False), \n              yaxis= dict(title= 'Millon',ticklen= 5,zeroline= False))\nfig = dict(data = data, layout = layout)\niplot(fig)","7e5e6dd1":"sales_region = [data1.NA_Sales.sum(),data1.EU_Sales.sum(),data1.JP_Sales.sum(),data1.Other_Sales.sum()]\nlabels = ['NA', 'EU', 'JP', 'Other']\ncolors = ['cyan','red','yellow','green']\n\n# visual\nplt.figure(figsize = (7,7))\nplt.pie(sales_region, labels=labels, colors=colors, autopct='%1.1f%%')\nplt.title('Total Share of Regions in Global Sales', color = 'blue', fontsize = 15)\nplt.show()","7ef3d0d2":"global_sales_genre = data1.groupby('Genre')['Global_Sales'].sum().sort_values(ascending=False)\norder_sales_genre = data1.groupby('Genre')['Global_Sales'].sum().sort_values(ascending=False).index\n\nplt.figure(figsize=(15, 5))\nsns.barplot(x=order_sales_genre, y=global_sales_genre, order=order_sales_genre)\nplt.xticks(rotation= 0)\nplt.xlabel('Genre', fontsize=14)\nplt.ylabel('Global Sales (Million)', fontsize=14)\nplt.title('Global Sales by Genre', color = 'blue', fontsize=15)\nplt.show()","329d5ef0":"global_sales_platform = data1.groupby('Platform')['Global_Sales'].sum().sort_values(ascending=False).iloc[0:10]\norder_sales_platform = global_sales_platform.index\n\nplt.figure(figsize=(15, 5))\nsns.barplot(x=order_sales_platform, y=global_sales_platform, order=order_sales_platform)\nplt.xticks(rotation= 0)\nplt.xlabel('Platform', fontsize=14)\nplt.ylabel('Global Sales (Million)', fontsize=14)\nplt.title('Global Sales by Platform', color = 'blue', fontsize=15)\nplt.show()","6931c096":"global_sales_platform = data1.groupby('Publisher')['Global_Sales'].sum().sort_values(ascending=False).iloc[0:10]\norder_sales_platform = global_sales_platform.index\n\nplt.figure(figsize=(15, 5))\nsns.barplot(x=order_sales_platform, y=global_sales_platform, order=order_sales_platform)\nplt.xticks(rotation= 60)\nplt.xlabel('Publisher', fontsize=14)\nplt.ylabel('Global Sales (Million)', fontsize=14)\nplt.title('Global Sales by Publisher', color = 'blue', fontsize=15)\nplt.show()","a0392db0":"# na_sales_year_genre = data1.pivot_table(index='Year',columns='Genre', aggfunc = {'NA_Sales': sum})\n# na_sales_year_genre","93470394":"from sklearn.preprocessing import LabelEncoder","73c20e93":"# label encoding of categorical variables\nlbe = LabelEncoder()\ndata1['Genre_Cat'] = lbe.fit_transform(data1['Genre'])\ndata1['Platform_Cat'] = lbe.fit_transform(data1['Platform'])\ndata1['Publisher_Cat'] = lbe.fit_transform(data1['Publisher'])\ndata1.head()","19026126":"from sklearn.preprocessing import StandardScaler\nfrom sklearn import preprocessing\nimport statsmodels.api as sm \nimport statsmodels.formula.api as smf \nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score","62283d28":"data2 = data1.loc[:,'Global_Sales':]\ndata2.head()","80d07fb1":"# Defining independent and dependent variables and splitting the data into two groups as train and test data\ndata2 = preprocessing.normalize(data2)\nx = data2[:,1:]\ny = data2[:,0]\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state= 42)","35254def":"# Defining independent and dependent variables and splitting the data into two groups as train and test data\n# x = data1[['Genre_Cat','Platform_Cat','Publisher_Cat']]\n# y = data1['Global_Sales']\n\n# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state= 42)","8018de4b":"# Multilinear Regression model woth statmodels and model summary\nlm = sm.OLS(y_train, x_train)\nmodel = lm.fit()\nmodel.summary()","9f93a9a5":"# Multilinear Regression model with skilearn \nlm1 = LinearRegression()\nmodel1 = lm1.fit(x_train, y_train)","84b32e0c":"# Coefficients\nmodel1.coef_ ","1abe351a":"# Intercept\nmodel1.intercept_","bdafb98f":"# R2 score\nmodel1.score(x,y)","d2fbe519":"# RMSE score of train data\nrmse = np.sqrt(mean_squared_error(y_train, model1.predict(x_train)))\nrmse","c40ef413":"# RMSE score of test data\nrmse = np.sqrt(mean_squared_error(y_test, model1.predict(x_test)))\nrmse","a09232af":"# RMSE average score of train data after cross-validation\nnp.sqrt(-cross_val_score(model1, \n                x_train, \n                y_train, \n                cv = 10, \n                scoring = \"neg_mean_squared_error\")).mean()","b6bf54e2":"# R2 average for differents situation since each time the algorithm selects different %80 as train data \ncross_val_score(model1, x_train, y_train, cv = 10, scoring = \"r2\").mean()","c1a145db":"# RMSE average score of test data after cross-validation\nreg_final_rmse = np.sqrt(-cross_val_score(model1, \n                x_test, \n                y_test, \n                cv = 10, \n                scoring = \"neg_mean_squared_error\")).mean()\nreg_final_rmse","02bc8253":"# R2 average of test data after cross validation\nreg_final_r2 = cross_val_score(model1, x_test, y_test, cv = 10, scoring = \"r2\").mean()\nreg_final_r2","9869f954":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import scale ","645e04af":"# PCA model instantiation and transformation for PCA\npca = PCA()\nx_reduced_train = pca.fit_transform(scale(x_train))","6b2c1373":"# PCA components \nx_reduced_train[0:1,:]","18fc4319":"# Cumulative percentage of explained variance as we add each component\nnp.cumsum(np.round(pca.explained_variance_ratio_, decimals = 4)*100)[0:5]","823989f8":"# PCA instantiation of regression model\nlm2 = LinearRegression()\npcr_model = lm2.fit(x_reduced_train, y_train)","3ac4886d":"# PCA model intercept\npcr_model.intercept_","a8973e33":"# PCA model coefficients\npcr_model.coef_","cbc60294":"# PCA Regression model with statmodels\nlm3 = sm.OLS(y_train, x_reduced_train)\nmodel2 = lm3.fit()\nmodel2.summary()","e367313a":"# PCA model prediction\ny_pred = pcr_model.predict(x_reduced_train)","f7fd2206":"# PCA RMSE score for train data\nnp.sqrt(mean_squared_error(y_train, y_pred))","b8177361":"# PCA R2 for train data\nr2_score(y_train, y_pred)","c96fdcd4":"# PCA instantiation of model for test data\npca2 = PCA()\nx_reduced_test = pca2.fit_transform(scale(x_test))","0f836070":"# PCA prediction with test data\ny_pred = pcr_model.predict(x_reduced_test)","83e20e6e":"# PCA RMSE score for test data\npca_final_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\npca_final_rmse","58d82b82":"# PCA R2 for test data\npca_final_r2 = r2_score(y_test, y_pred)\npca_final_r2","3f079ee1":"from sklearn import model_selection","65a348af":"# Illustraion of chage in RMSE score as we add each component into the model.\n\ncv_10 = model_selection.KFold(n_splits = 10,\n                             shuffle = True,\n                             random_state = 1)\n\nlm4 = LinearRegression()\n\nRMSE = []\n\nfor i in np.arange(1, x_reduced_train.shape[1] + 1):\n    \n    score = np.sqrt(-1*model_selection.cross_val_score(lm4, \n                                                       x_reduced_train[:,:i], \n                                                       y_train.ravel(), \n                                                       cv=cv_10, \n                                                       scoring='neg_mean_squared_error').mean())\n    RMSE.append(score)","924c8ab8":"# We see that the RMSE score decreases as we add all three components into the model.\n# So, we can decide to keep all three components in the model.\nplt.plot(RMSE, '-v')\nplt.xlabel('Number of Components')\nplt.ylabel('RMSE')\nplt.title('PCR Model Tuning');","4455d9e8":"from sklearn.cross_decomposition import PLSRegression, PLSSVD","e07248cc":"# PLS model instantiation\npls_model = PLSRegression().fit(x_train, y_train)","d8a4e1f3":"# PLS model coefficients\npls_model.coef_","ba5e1aff":"# PLS model predictions based on train data\ny_pred = pls_model.predict(x_train)","3ba49ed9":"# PLS RMSE score for train data\nnp.sqrt(mean_squared_error(y_train, y_pred))","ec22bc77":"# PLS R2 for train data\nr2_score(y_train, y_pred)","cf6acb13":"# PLS prediction based on test data\ny_pred = pls_model.predict(x_test)","5435b957":"# PLS RMSE test score\nnp.sqrt(mean_squared_error(y_test, y_pred))","f5fcdbaf":"# PLS R2 for test data\nr2_score(y_test, y_pred)","7ee1545f":"# Illustraion of change in RMSE score as the model adds one additional component to the model in each loop.\ncv_10 = model_selection.KFold(n_splits=10, shuffle=True, random_state=1)\n\n\nRMSE = []\n\nfor i in np.arange(1, x_train.shape[1] + 1):\n    pls = PLSRegression(n_components=i)\n    score = np.sqrt(-1*cross_val_score(pls, x_train, y_train, cv=cv_10, scoring='neg_mean_squared_error').mean())\n    RMSE.append(score)\n\nplt.plot(np.arange(1, x_train.shape[1] + 1), np.array(RMSE), '-v', c = \"r\")\nplt.xlabel('Number of Components')\nplt.ylabel('RMSE')\nplt.title('Components and RMSE');","6900b0f2":"# PLS model with two components\npls_model2 = PLSRegression(n_components = 3).fit(x_train, y_train)","663994cc":"# PLS prediction based on test data after cross validation\ny_pred2 = pls_model2.predict(x_test)","6b8f27d1":"# PLS RMSE test score after cross validation\npls_final_rmse = np.sqrt(mean_squared_error(y_test, y_pred2))\npls_final_rmse","86bf7ed8":"pls_final_r2 = r2_score(y_test, y_pred2)\npls_final_r2","92099711":"from sklearn.linear_model import Ridge","f9d22a0c":"# Ridge model instantiation and model details\nridge_model = Ridge(alpha = 0.1).fit(x_train, y_train)\nridge_model","425eec7d":"# Ridge model details\nridge_model.coef_","a6b395b0":"# Illustration of how weights of independent variables approaches to 0 as the alpha value increases. \n\nlambdas = 10**np.linspace(10,-2,100)*0.5\n\nridge_model = Ridge()\ncoefficients = []\n\nfor i in lambdas:\n    ridge_model.set_params(alpha = i)\n    ridge_model.fit(x_train, y_train) \n    coefficients.append(ridge_model.coef_)\n        \nax = plt.gca()\nax.plot(lambdas, coefficients) \nax.set_xscale('log') \n\nplt.xlabel('Lambda(Alpha) Values')\nplt.ylabel('Coefficients')\nplt.title('Ridge Coefficients');","86f236a9":"# Ridge prediction based on test data\ny_pred = ridge_model.predict(x_test)","49448303":"# Ridge RMSE test score\nnp.sqrt(mean_squared_error(y_test, y_pred))","af7e9df7":"# Ridge R2 \nr2_score(y_test, y_pred)","dff11ef6":"from sklearn.linear_model import RidgeCV","6d59821c":"# Ridge instantiation of cross validation model and model details\nridge_cv = RidgeCV(alphas = lambdas, \n                   scoring = \"neg_mean_squared_error\",\n                   normalize = True)\nridge_cv.fit(x_train, y_train)\nridge_model","6449174d":"# Ridge cross validation alpha score\nridge_cv.alpha_","b75069b1":"# Ridge tuned model after cross validation\nridge_tuned = Ridge(alpha = ridge_cv.alpha_, \n                   normalize = True).fit(x_train,y_train)","01d055fe":"# Ridge model coefficients after cross validation\nridge_tuned.coef_","e69bf81b":"# Ridge RMSE test score after cross validation\nridge_final_rmse = np.sqrt(mean_squared_error(y_test, ridge_tuned.predict(x_test)))\nridge_final_rmse","89f6d74e":"# Ridge R2 after cross validation\nridge_final_r2 = r2_score(y_test, ridge_tuned.predict(x_test))\nridge_final_r2","fb223a7c":"from sklearn.linear_model import Lasso","92b12107":"# Lasso model instantation and model details\nlasso_model = Lasso(alpha = 1.0).fit(x_train, y_train)\nlasso_model","3dcf5102":"# Lasso model coefficients\nlasso_model.coef_","a5761837":"# The weight of independent variables comes to value of zero as the alpha score changes. \n# However, we cannot see this change since we have coefficients close to 0 before cross validation. \n\nlasso = Lasso()\nlambdas = 10**np.linspace(10,-2,100)*0.5 \ncoefficients = []\n\nfor i in lambdas:\n    lasso.set_params(alpha=i)\n    lasso.fit(x_train, y_train)\n    coefficients.append(lasso.coef_)\n    \nax = plt.gca()\nax.plot(lambdas*2, coefficients)\nax.set_xscale('log')\nplt.axis('tight')\nplt.xlabel('alpha')\nplt.ylabel('weights')","90e77b7f":"# Lasso model prediction based on test data\ny_pred = lasso_model.predict(x_test)","9f631cc5":"# Lasso RMSE score\nnp.sqrt(mean_squared_error(y_test, y_pred))","3bd023bc":"# Lasso R2\nr2_score(y_test, y_pred)","7467f668":"from sklearn.linear_model import LassoCV","6f4a9cba":"# Lasso instantiation of cross validation model\nlasso_cv_model = LassoCV(alphas = None, \n                         cv = 10, \n                         max_iter = 10000, \n                         normalize = True)","1ad05da7":"# Lasso cross validation model details\nlasso_cv_model.fit(x_train,y_train)","cdd3ad02":"# Lasso cross validation model alpha score\nlasso_cv_model.alpha_","024d0fa8":"# Lasso tuned model after cross validation\nlasso_tuned = Lasso(alpha = lasso_cv_model.alpha_)\nlasso_tuned.fit(x_train, y_train)","935eb810":"# Lasso predictions of tuned model base on test data\ny_pred = lasso_tuned.predict(x_test)","581a8765":"# Lasso model coefficients after cross validation\nlasso_tuned.coef_","819b56a0":"# Lasso RMSE test score after cross validation\nlasso_final_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\nlasso_final_rmse","f10b0adc":"# Lasso R2 after cross validation\nlasso_final_r2 = r2_score(y_test, y_pred)\nlasso_final_r2","762d73b8":"from sklearn.linear_model import ElasticNet","46780402":"# Elasticnet model instantiation\nenet_model = ElasticNet().fit(x_train, y_train)","6006c251":"# Elasticnet model coefficients\nenet_model.coef_","0dfb8c70":"#  Elasticnet intercept\nenet_model.intercept_","ad1856ed":"# Elasticnet model details\nenet_model","92a0ab1d":"# Elasticnet model predictions based on test data\ny_pred = enet_model.predict(x_test)","6f8ba2b4":"# Elasticnet RMSE test score\nnp.sqrt(mean_squared_error(y_test, y_pred))","5ad143ac":"# Elasticnet R2\nr2_score(y_test, y_pred)","8c132662":"from sklearn.linear_model import ElasticNetCV","f283006b":"# Elasticnet cross validation model instantiation\nenet_cv_model = ElasticNetCV(cv = 10, random_state = 0).fit(x_train, y_train)","74821111":"# Elasticnet cross validation alpha value\nenet_cv_model.alpha_","d276c41f":"# Elasticnet cross validation model details\nenet_cv_model","f4f43544":"# Elasticnet tuned model based on alpha score\nenet_tuned = ElasticNet(alpha = enet_cv_model.alpha_).fit(x_train,y_train)","36979517":"# Elasticnet predictions based on the tuned model\ny_pred = enet_tuned.predict(x_test)","5bf05abd":"# Elasticnet model coefficients after cross validation\nenet_tuned.coef_","b853497e":"# Elasticnet RMSE test score after cross validation\nenet_final_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\nenet_final_rmse","56f04361":"# Elasticnet R2 after cross validation\nenet_final_r2 = r2_score(y_test, y_pred)\nenet_final_r2","030a5d6d":"print(f\"\"\"Multilinear Regression RMSE: {reg_final_rmse}, R2: {reg_final_r2}\nPCA Regression RMSE: {pca_final_rmse}, R2: {pca_final_r2}\nPLS Regression RMSE: {pls_final_rmse}, R2: {pls_final_r2}\nRidge Regression RMSE: {ridge_final_rmse}, R2: {ridge_final_r2}\nLasso Regression RMSE: {lasso_final_rmse}, R2: {lasso_final_r2}\nElasticNet Regression RMSE: {enet_final_rmse}, R2: {enet_final_r2}\"\"\")","3e511810":"from sklearn.preprocessing import PolynomialFeatures","5035f9c5":"# we change the degree value for model tuning\npoly_features = PolynomialFeatures(degree=3)","218c1003":"x_train_poly = poly_features.fit_transform(x_train)","e3093060":"poly_model = LinearRegression()\npoly_model.fit(x_train_poly, y_train)","563cdcf3":"y_train_pred = poly_model.predict(x_train_poly)","599fdedb":"# Polynomial Regression RMSE score for train data\nrmse_train = np.sqrt(mean_squared_error(y_train,y_train_pred))\nr2_train = r2_score(y_train, y_train_pred)\nprint(rmse_train,r2_train)","c212b755":"y_test_pred = poly_model.predict(poly_features.fit_transform(x_test))","7422e835":"# Polynomial Regression RMSE score for test data\nrmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\nr2_test = r2_score(y_test, y_test_pred)\nprint(rmse_test,r2_test)","0213c0c6":"### Elasticnet Regression Cross Validation","fdbf32f7":"### Action, sports, shooter, role-playing and platform games are the top 5 video games in global sales. ","609ed235":"### There is steady increase in global sales until 2008-2009 and we see a decrease since then. However, we should be cautious about the sales in 2016 and 2017. There may some missing data there since we observe a sharp decrease in 2016 and very low level of sum of global sales in 2017.","609b3518":"## Multilinear Regression","6bf68015":"## Global Video Game Sales by Categories","463f5e17":"### Cross Validation","10447af5":"### Defining data and splitting with normalization","b868ff1f":"# Data Cleaning","e963b147":"### Nintendo, Electronic Arts, Activision, Sony and Ubisoft are the top 5 platforms with respect to total global sales.","8362b902":"### When we look at the sum of global sales together with the regional sum of sales, we observe the increase in the share of NA Sales and EU Sales. ","c414af58":"## Lasso Regression","705bc11a":"# General Information about the dataset","b03d1ba1":"# Conclusion:\n### The data is not the most desired data since we do not have quantitative independent variables and we have to encode categorical variables. As we have a lot of sub-categories, it would a better approach to use label encoding. However, this approach has some downsides as mentioned above.\n### I have tried to use the most prominent linear regression models and compared them with and without normalized data. \n### Multilinear Regression has better RMSE test scores and the rest of the approaches have very close RMSE test scores. In this respect, Multilinear Regression model appears to be the best model compared to others. Ridge Regression follows the former, if we check the decimals. \n### On the other hand, we have very low R2 values in linear regression models.\n### Finally, I have used polynomial regression model. We see that we have more robust model with polynomial regression model with respect to both RMSE test score and R2 value. And when we check these values at different degrees, it looks like that 3rd degree has a lower RMSE. ","b55171d3":"## Ridge Regression","8b81b6cf":"## Analysis of Global and Regional Sales","c03e9338":"* __degree__ = 2; __RMSE__ = 0.007746125544948684, __R2__ = 0.8062692011851837\n* __degree__ = 3; __RMSE__ = 0.0075387615891847806, __R2__ = 0.816502722805112\n* __degree__ = 4; __RMSE__ = 0.007870531850137641, __R2__ = 0.7999964208296635","4c89df7f":"### PLS Cross Validation","748bf493":"This dataset contains a list of video games with sales greater than 100,000 copies. It was generated by a scrape of vgchartz.com.\n\nFields include\n\nRank - Ranking of overall sales\n\nName - The games name\n\nPlatform - Platform of the games release (i.e. PC,PS4, etc.)\n\nYear - Year of the game's release\n\nGenre - Genre of the game\n\nPublisher - Publisher of the game\n\nNA_Sales - Sales in North America (in millions)\n\nEU_Sales - Sales in Europe (in millions)\n\nJP_Sales - Sales in Japan (in millions)\n\nOther_Sales - Sales in the rest of the world (in millions)\n\nGlobal_Sales - Total worldwide sales.","d7550730":"* __Multilinear Regression RMSE__: 0.015001624864342897, __R2__: 0.07397442844946144\n* __PCA Regression RMSE__        : 0.01674123699336149,  __R2__: 0.09509144293622152\n* __PLS Regression RMSE__        : 0.016741898509377524, __R2__: 0.09501992810961701\n* __Ridge Regression RMSE__      : 0.016739899842261558, __R2__: 0.09523599033741226\n* __Lasso Regression RMSE__      : 0.016741833987041262, __R2__: 0.0950269035808824\n* __ElasticNet Regression RMSE__ : 0.0167404713962502,   __R2__: 0.09517420617962535","46fee116":"### Statmodels","2d60f724":"### Platform, shooter and role-playing are the top 3 genres by average global sales. ","a259410e":"### Palcom, Red Orb, Nintendo are the top 3 publishers by average global sales.","b91ca8ac":"### Sci Kit Learn","286d2dc0":"### DS and PS2 are the top 2 platforms for video games. \n### Action and Sports are the top 2 genres of video games.\n### Electronic Arts is the top publisher of video games.","c40271a7":"### We see that there is a gap between 2017 and 2020 and there is only one observation in 2020, so we drop that observation.","163485be":"### Comparison of RMSE and R2 values across different models with normalization","b36c66c8":"### We check the unique values in categorical variables and years.","50905b9b":"## PLS Regression","b77e8a87":"* __Multilinear Regression RMSE__: 1.7105964805711504, __R2__: -0.015022400745020936\n* __PCA Regression RMSE__: 2.0668894045476174, __R2__: -0.00018219613950942737\n* __PLS Regression RMSE__: 2.0646621120360353, __R2__: 0.0019722471705171385\n* __Ridge Regression RMSE__: 2.0649740466339517, __R2__: 0.0016706550587132218\n* __Lasso Regression RMSE__: 2.0646617632879796, __R2__: 0.0019725843300040236\n* __ElasticNet Regression RMSE__: 2.0647315517730767, __R2__: 0.0019051137158706544","f312c38f":"## ElasticNet Regression","f88e5948":"### PS2, X360, PS3, Wii and DS are the top 5 platforms with respect to total global sales. ","be9522c5":"### We get lower RMSE test scores and much higher R2 values with polynomial regression model. \n### We can do model tuning by changing the degree and it looks that degree = 3 is the ideal value since we have a lower RMSE. ","e50b28f9":"### As it can be seen from the pie-chart below that NA has the largest share in cumulative global sales between 1980-2016. The sales revenue from NA is almost half of the global sales. Sales in EU is the second with approximately the quarter of global sales. Thus, these two regions are the most defining regions in global sales. ","c1218cb9":"# EDA","777b32ff":"### We see that there are nan values in year and publisher columns.","3cb644d8":"### Comparison of RMSE and R2 values across different models without normalization","4878b89f":"### As expected we have high correlation between the global sales and regional sales. Because, global sales is the sum of the regional sales. Thus, we cannot use these variables in our analysis and we need to use encoded categorical variables for ML analysis. ","94ef7c5a":"### Ridge Regression Cross Validation","e30dc38e":"### As it is shown below through the histogram graph and kurtosis and skewness values regarding global sales, it is not normally distributed. In fact, the sales numbers for most of the games are below 1 million and there are also top selling video games in the data as well. Thus, it would be good to compare the regression model test errors before and after normalization of the data. ","02f331eb":"## Countplots of Categorical Variables","f4a320c7":"### There is a category in Platform feature called 2600. When we look that in google search, we see that the actual name is Atari 2600. Thus, we replace 2600 with Atari.","1a364615":"### GB, NES and GEN are the top 3 platforms by average global sales.","a3052124":"### Model Tuning","6e0e129c":"### Lasso Regression Cross Validation","86451330":"# Polynomial Regression","90d6b18d":"### We check the observations with nan values. Although the number of observations with nan values is relatively low, I have decided to keep them. Therefore, we replace nan values in Publisher feature with Unknown, and nan values in Year feature with the most frequent year. ","f512764d":"### Defining data and splitting without normalization as an alternative for comparison","8184d09f":"# Linear Regression Models\n### Our dependent variable is global sales and it will be our main focus to make predictions through different linear regression models. \n### As mentioned earlier, we do not have quantitative independent variables for regression analysis. Regional sales are just the sub-totals of global sales. They are highly correlated with the dependent variable. Thus, we need to convert our categorical variables, Platform, Genre and Publisher, into numerical variables and use them in the regression models.\n### I am using the label encoding approach here. I am aware of the fact that it is not the most desired approach for categorical encoding since it has the disadvantage of misinterpretation by algorithms as having some sort of order in them. But, in one-hot method we have the dummy variable trap and curse of dimensionality problems as we have a lot of sub-categories in each of our categorical variables. \n### In this framework, I stick to label encoding approach. ","3fbc3066":"## PCA"}}