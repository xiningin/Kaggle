{"cell_type":{"8e7587fa":"code","a3e4098f":"code","dbc4698c":"code","24cb99ba":"code","1e143d7a":"code","2f7eade9":"code","f31dd016":"code","e4153863":"code","70edaba1":"code","242f3309":"code","2839eebd":"code","396a0942":"code","000942d2":"code","b2b2ba12":"code","e2edd412":"code","ba239ba2":"code","757c95c4":"code","04c6003e":"code","136931ff":"code","cd36606b":"code","f61cef1b":"code","f008d51c":"code","59dd74b8":"code","e04806eb":"code","089c0a47":"code","f4164288":"code","978d20eb":"code","851da65a":"code","28a9b5d7":"code","64ddb33f":"code","68cab07f":"code","423a7111":"code","6146e6f6":"code","31323155":"code","61910c19":"code","5d5303f4":"code","4c0010fe":"code","95824a4e":"markdown","c5749b89":"markdown","860f7385":"markdown","d88072fd":"markdown","73254f99":"markdown","e735a295":"markdown","6c4aadea":"markdown","c18ab9e0":"markdown","e2db4403":"markdown","2d2f6688":"markdown","e76ed0e7":"markdown","b4a08831":"markdown","c016bda0":"markdown","79dcfd1f":"markdown","39de2f34":"markdown","df4bf506":"markdown","747965cf":"markdown","dcbea4af":"markdown","df70ff22":"markdown","718d0719":"markdown"},"source":{"8e7587fa":"import os\ncwd = os.getcwd()\nos.chdir(cwd)\nprint(os.listdir(\"..\/input\"))","a3e4098f":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader","dbc4698c":"# Hyperparameters etc.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nLEARNING_RATE = 5e-5  # could also use two lrs, one for gen and one for disc\nBATCH_SIZE = 64\nIMAGE_SIZE = 64\nCHANNELS_IMG = 3\nNOISE_DIM = 100\nNUM_EPOCHS = 10\nFEATURES_DISC = 64\nFEATURES_GEN = 64\nCRITIC_ITERATIONS = 5\nWEIGHT_CLIP = 0.02","24cb99ba":"device","1e143d7a":"transforms = transforms.Compose([\n    transforms.Resize([64,64]),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5 for i in range(CHANNELS_IMG)], [0.5 for i in range(CHANNELS_IMG)])\n])","2f7eade9":"IMAGE_PATH = '..\/input\/celeba-dataset\/img_align_celeba'\nIMAGE_PATH","f31dd016":"dataset = datasets.ImageFolder(IMAGE_PATH, transform = transforms)","e4153863":"def split_indices(n, val_per, seed = 0):\n    n_val = int(n * val_per)\n    np.random.seed(seed)\n    idx = np.random.permutation(n)\n    return idx[n_val : ], idx[: n_val]","70edaba1":"import numpy as np\nimport matplotlib.pyplot as plt","242f3309":"val_per = 0.4\nrand_seed = 42\n\ntrain_indices, val_indices = split_indices(len(dataset), val_per, rand_seed)\n\nprint(len(train_indices), len(val_indices))","2839eebd":"print(\"Validation Indices: \", val_indices[:20])\nprint(\"Training Indices: \", train_indices[:20])","396a0942":"from torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.utils.data.dataloader import DataLoader","000942d2":"# training data loader\ntrain_sampler = SubsetRandomSampler(train_indices)\nloader = DataLoader(dataset, BATCH_SIZE, sampler = train_sampler)","b2b2ba12":"len(dataset), len(loader)","e2edd412":"class Discriminator(nn.Module):\n    \n    def __init__(self, channels_img, features_d):\n        super(Discriminator, self).__init__()\n        \n        self.disc = nn.Sequential(\n            #size = 3*64*64\n            nn.Conv2d(channels_img, features_d, kernel_size = 4, stride = 2, padding = 1), # Size : 32*32\n            nn.LeakyReLU(0.2),\n            \n            nn.Conv2d(features_d, features_d*2, kernel_size = 4, stride = 2, padding = 1), # size = 16*16\n            nn.BatchNorm2d(features_d*2),\n            nn.LeakyReLU(0.2),\n            \n            nn.Conv2d(features_d*2, features_d*4, kernel_size = 4, stride = 2, padding = 1), # size = 8*8\n            nn.BatchNorm2d(features_d*4),\n            nn.LeakyReLU(0.2),\n            \n            nn.Conv2d(features_d*4, features_d*8, kernel_size = 4, stride = 2, padding = 1), # size = 4*4\n            nn.BatchNorm2d(features_d*8),\n            nn.LeakyReLU(0.2),\n            \n            nn.Conv2d(features_d*8, 1, kernel_size = 4, stride = 2, padding = 0) #1*1\n            \n        )\n        \n        \n    def forward(self, x):\n        return self.disc(x)","ba239ba2":"class Generator(nn.Module):\n    \n    def __init__(self, z_dim, channels_img, features_g):\n        super(Generator, self).__init__()\n        \n        self.net = nn.Sequential(\n            nn.ConvTranspose2d(z_dim, features_g*16, kernel_size = 4, stride = 1, padding = 0), # size = 4*4\n            nn.BatchNorm2d(features_g*16),\n            nn.ReLU(),\n            \n            nn.ConvTranspose2d(features_g*16, features_g*8, kernel_size = 4, stride = 2, padding = 1), # size = 8*8\n            nn.BatchNorm2d(features_g*8),\n            nn.ReLU(),\n            \n            nn.ConvTranspose2d(features_g*8, features_g*4, kernel_size = 4, stride = 2, padding = 1), # size = 16*16\n            nn.BatchNorm2d(features_g*4),\n            nn.ReLU(),\n            \n            nn.ConvTranspose2d(features_g*4, features_g*2, kernel_size = 4, stride = 2, padding = 1), # size = 32*32\n            nn.BatchNorm2d(features_g*2),\n            nn.ReLU(),\n            \n            nn.ConvTranspose2d(features_g*2, channels_img, kernel_size = 4, stride = 2, padding = 1),\n            nn.Tanh()  # [-1, 1]\n        )\n        \n    \n    def forward(self, x):\n        return self.net(x)","757c95c4":"def initialize_weights(model):\n    # Initializes weights according to the DCGAN paper\n    for m in model.modules():\n        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n            nn.init.normal_(m.weight.data, 0.0, 0.02)","04c6003e":"def test():\n    N, in_channels, H, W = 8, 3, 64, 64\n    noise_dim = 100\n    x = torch.randn((N, in_channels, H, W))\n    disc = Discriminator(in_channels, 8)\n    assert disc(x).shape == (N, 1, 1, 1), \"Discriminator test failed\"\n    gen = Generator(noise_dim, in_channels, 8)\n    z = torch.randn((N, noise_dim, 1, 1))\n    assert gen(z).shape == (N, in_channels, H, W), \"Generator test failed\"\n    print('Success')","136931ff":"test()","cd36606b":"dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\ngen = Generator(z_dim = NOISE_DIM, channels_img = CHANNELS_IMG, features_g = FEATURES_GEN).to(device)\ndisc = Discriminator(CHANNELS_IMG, FEATURES_DISC).to(device)\ninitialize_weights(gen)\ninitialize_weights(disc)","f61cef1b":"opt_gen = optim.RMSprop(gen.parameters(), lr = LEARNING_RATE)\nopt_disc = optim.RMSprop(disc.parameters(), lr = LEARNING_RATE)","f008d51c":"def reset_grad():\n    opt_disc.zero_grad()\n    opt_gen.zero_grad()","59dd74b8":"def train_discriminator(images):\n    # create labels, for real image label is 1, for fake \n    \n    # loss for real images\n    \n    for _ in range(CRITIC_ITERATIONS):\n        \n        disc_real = disc(images).reshape(-1)\n        #d_loss_real = torch.mean(disc_real)\n        real_score = disc_real\n        \n        \n        z = torch.randn(BATCH_SIZE, NOISE_DIM, 1, 1).to(device)\n        fake_images = gen(z)\n        disc_fake = disc(fake_images).reshape(-1)\n        #d_loss_fake = torch.mean(disc_fake)\n        fake_score = disc_fake\n        \n        loss_disc = - (torch.mean(disc_real) - torch.mean(disc_fake))\n        \n        reset_grad()\n        \n        loss_disc.backward()\n        \n        opt_disc.step()\n        \n        for p in disc.parameters():\n                p.data.clamp_(-WEIGHT_CLIP, WEIGHT_CLIP)\n        \n        return loss_disc, real_score, fake_score ","e04806eb":"def train_generator():\n    # Generate fake images and calculate loss\n    z = torch.randn(BATCH_SIZE, NOISE_DIM, 1, 1).to(device)\n    fake_images = gen(z)\n    labels = torch.ones(BATCH_SIZE, 1).to(device)\n    output = disc(fake_images).reshape(-1)\n    g_loss = - torch.mean(output)\n\n    # Backprop and optimize\n    reset_grad()\n    g_loss.backward()\n    opt_gen.step()\n    return g_loss, fake_images","089c0a47":"import os\n\nsample_dir = 'samples'\nif not os.path.exists(sample_dir):\n    os.makedirs(sample_dir)","f4164288":"def show_img(img, label):\n    print('Label: ', label)\n    plt.imshow(img.permute(1,2,0), cmap = 'gray')","978d20eb":"def denorm(x):\n  out = (x + 1) \/ 2\n  return out.clamp(0, 1)","851da65a":"from IPython.display import Image\nfrom torchvision.utils import save_image\n\n# Save some real images\nfor images, _ in loader:\n    images = images.reshape(images.size(0), 3, 64, 64)\n    save_image(denorm(images), os.path.join(sample_dir, 'real_images.png'), nrow=16)\n    break\n   \nImage(os.path.join(sample_dir, 'real_images.png'))","28a9b5d7":"sample_vectors = torch.randn(BATCH_SIZE, NOISE_DIM, 1, 1).to(device)\n\ndef save_fake_images(index):\n    fake_images = gen(sample_vectors)\n    fake_images = fake_images.reshape(fake_images.size(0), 3, 64, 64)\n    fake_fname = 'fake_images-{0:0=4d}.png'.format(index)\n    print('Saving', fake_fname)\n    save_image(denorm(fake_images), os.path.join(sample_dir, fake_fname), nrow=16)\n    \n# Before training\nsave_fake_images(0)\nImage(os.path.join(sample_dir, 'fake_images-0000.png'))","64ddb33f":"%%time\n\n\ntotal_step = len(loader)\nd_losses, g_losses, real_scores, fake_scores = [], [], [], []\n\nfor epoch in range(NUM_EPOCHS):\n    for i, (images, _) in enumerate(loader):\n        # Load a batch & transform to vectors\n        images = images.to(device)\n        \n        # Train the discriminator and generator\n        d_loss, real_score, fake_score = train_discriminator(images)\n        g_loss, fake_images = train_generator()\n        \n        # Inspect the losses\n        if (i+1) \/ 100 == 0:\n            d_losses.append(d_loss.item())\n            g_losses.append(g_loss.item())\n            real_scores.append(real_score.mean().item())\n            fake_scores.append(fake_score.mean().item())\n            print('Epoch [{}\/{}], Step [{}\/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}' \n                  .format(epoch, num_epochs, i+1, total_step, d_loss.item(), g_loss.item(), \n                          real_score.mean().item(), fake_score.mean().item()))\n        \n    # Sample and save images\n    save_fake_images(epoch+1)","68cab07f":"# Save the model checkpoints \ntorch.save(gen.state_dict(), 'G.ckpt')\ntorch.save(disc.state_dict(), 'D.ckpt')","423a7111":"Image('.\/samples\/fake_images-0000.png')","6146e6f6":"Image('.\/samples\/fake_images-0002.png')","31323155":"Image('.\/samples\/fake_images-0004.png')","61910c19":"Image('.\/samples\/fake_images-0006.png')","5d5303f4":"Image('.\/samples\/fake_images-0008.png')","4c0010fe":"Image('.\/samples\/fake_images-0010.png')","95824a4e":"Image Transformaition after 10th Epoch","c5749b89":"Image transformation after 2nd Epoch","860f7385":"# Image View","d88072fd":"There seems to be some issue with Print function, but model has trained really well.\n\nEven for 10 Epoch, it took around 2 Hours to train (Thanks to GPU from Kaggle).\n\nLet's observe some of images from initial to final epoch","73254f99":"# Discriminator Model Building (Critic)","e735a295":"Image Transformaition after 6th Epoch","6c4aadea":"Image before training","c18ab9e0":"# Hyper parameter Set","e2db4403":"Image Transformaition after 8th Epoch","2d2f6688":"# Save Model","e76ed0e7":"# Image Saving Code","b4a08831":"This is remarkable, how from Nothing to something meaningful within 10 Iterations. With 100 Epochs, we can get almost 99% similar image as of original Celeb Face Dataset.\n\nI would like if someone can run this for 100 Epochs and result obtained after that, In case anyone able to run for 100, Kindly share link of image transformation. \n\nThat's It for this Notebook, I will come up with more advanced version of GAN with next notebook, for now Wessertein GAN, worked excellently.\n\nThanks and Consider Upvoting !! Keep learnong !!","c016bda0":"# Training of Model","79dcfd1f":"# Import Required Libraries & Load Data","39de2f34":"# Generator and Discriminator (Critic) Initiation","df4bf506":"Image Transformaition after 4h Epoch","747965cf":"# Verify Images After Every Epoch","dcbea4af":"# Define Generator Neural Network","df70ff22":"# Training Discriminator (Critic)","718d0719":"# Training Generator"}}