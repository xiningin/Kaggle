{"cell_type":{"06c7ffb3":"code","4ab074c7":"code","c734961d":"code","f4e9133b":"code","adb3af48":"code","dc371920":"code","6a650824":"code","90c6c709":"code","fddc3cb3":"code","1af62dd3":"code","e2363b3e":"code","2f10b2d0":"code","9b2d0014":"code","e1cdb233":"code","9cc3ba4d":"code","edbc57de":"code","de4b0b2c":"code","75e76c1e":"code","4179fc11":"code","5eb144d2":"code","4ee48c42":"code","26cf5a5a":"code","041ce49f":"code","ab29fd64":"code","627f6978":"code","f9261cba":"code","ac19b60a":"code","ded38a67":"code","3e0655d3":"code","28f21e57":"code","30b26a7e":"code","69fe6f35":"code","d69b9ea3":"code","a1d82c01":"markdown","037a906a":"markdown","bf47e3f0":"markdown","b30baf98":"markdown","126a99bc":"markdown","76f4bfa5":"markdown","c477a0cb":"markdown","74d57879":"markdown","dc6ff875":"markdown","98425a4b":"markdown","a5229746":"markdown","106bf860":"markdown","49e63c3c":"markdown","64d46f64":"markdown","bdaa8618":"markdown","d040c39f":"markdown","666ddbfc":"markdown","f7c7f9d7":"markdown","6cdd8633":"markdown","7dcd7d08":"markdown","abab04ee":"markdown","99d8fd5b":"markdown","2e0ccabd":"markdown","88f221f5":"markdown","92e58134":"markdown","4a12bff9":"markdown","ee4921a2":"markdown","a0a6b2aa":"markdown","e6f94d03":"markdown","b2166a1f":"markdown","8f4ddb5e":"markdown","429ac354":"markdown","1d192908":"markdown","6b8df5ba":"markdown","529e4dc0":"markdown"},"source":{"06c7ffb3":"import pandas as pd\nimport numpy as np","4ab074c7":"import seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt","c734961d":"sns.set_theme(rc = {'grid.linewidth': 0.5,\n                    'axes.linewidth': 0.75, 'axes.facecolor': '#ECECEC', \n                    'axes.labelcolor': 'black',\n                    'figure.facecolor': 'white',\n                    'xtick.color': 'black', 'ytick.color': 'black'})","f4e9133b":"Set = pd.read_csv('..\/input\/housing-prices-visual\/Housing_prices_visual.csv')","adb3af48":"df_train = Set.iloc[:1451]\ndf_test = Set.iloc[1451:]","dc371920":"from sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler","6a650824":"Num_vars = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', \n            'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', \n            '2ndFlrSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', \n            'OpenPorchSF', 'EnclosedPorch', 'ScreenPorch', 'MiscVal', \n            '3SsnPorch' , 'PoolArea' , 'LowQualFinSF']","90c6c709":"def K_means_claster_tuning(df_train, Vars_list, max_cluster = 15):\n    \n    ### Scaling data\n    \n    scaler = StandardScaler()\n    data_scaled = scaler.fit_transform(df_train[Vars_list])\n    \n    ### Calculating inertia_ for each number of clusters\n    \n    SSE = []\n    \n    for cluster in range(1, max_cluster):\n        \n        kmeans = KMeans(n_clusters = cluster, init = 'k-means++', random_state = 999)\n        kmeans.fit(data_scaled)\n        SSE.append(kmeans.inertia_)\n        \n    df_plot = pd.DataFrame({'Cluster': range(1, max_cluster), 'SSE': SSE})\n    \n    return(df_plot)","fddc3cb3":"Cluster_tuning = K_means_claster_tuning(df_train, Num_vars, max_cluster = 20)","1af62dd3":"with plt.rc_context(rc = {'figure.dpi': 110, 'axes.labelsize': 8, \n                          'xtick.labelsize': 6, 'ytick.labelsize': 6}):\n    \n    fig_1, ax_1 = plt.subplots(1, 1, figsize = (5, 3.5))\n    \n    sns.lineplot(x = [1, 19], y = [27569, 10481], color = '#86b9cf',\n                 linewidth = 1)\n\n    sns.lineplot(x = Cluster_tuning['Cluster'].astype('int64'), \n                 y = Cluster_tuning['SSE'], color = '#146964', \n                 marker = 'o', linewidth = 1)\n    \n    plt.xticks(range(1, 20))\n          \nplt.show()","e2363b3e":"def K_means_clastering(df_train, df_test, Vars_list, n_clusters = 10):\n    \n    ### Scaling data\n    \n    scaler = StandardScaler()\n    df_train_scaled = scaler.fit_transform(df_train[Vars_list])\n    df_test_scaled = scaler.transform(df_test[Vars_list])\n    \n    ### Initiating KMeans algorithm\n    \n    kmeans = KMeans(n_clusters = n_clusters, init = 'k-means++', random_state = 999)\n    \n    ### Getting clusters\n    \n    kmeans_train = kmeans.fit_predict(df_train_scaled)\n    kmeans_test = kmeans.predict(df_test_scaled)\n    \n    ### Getting the distance to the cluster centres\n    \n    Cluster_space = []\n    Cols = [f'Clust_space_{i}' for i in range(n_clusters)]\n    \n    Cluster_space.append(kmeans.fit_transform(df_train_scaled))\n    Cluster_space.append(kmeans.transform(df_test_scaled))\n    \n    ### Saving results\n    \n    df_clusters_train = pd.DataFrame(Cluster_space[0], columns = Cols)\n    df_clusters_train['Cluster'] = kmeans_train\n    \n    df_clusters_test = pd.DataFrame(Cluster_space[1], columns = Cols)\n    df_clusters_test['Cluster'] = kmeans_test\n    \n    return(df_clusters_train, df_clusters_test)","2f10b2d0":"Result_1 = K_means_clastering(df_train, df_test, Num_vars[0:5], n_clusters = 5)","9b2d0014":"df_clusters_train = Result_1[0]\ndf_clusters_test = Result_1[1]","e1cdb233":"df_clusters_train.round(2).head(3)","9cc3ba4d":"import pandas as pd\nfrom sklearn.preprocessing import KBinsDiscretizer","edbc57de":"def discretizer(df_train, df_test, Vars_list, n_bins = 5, \n                encode = 'onehot-dense', strategy = 'kmeans'):\n    \n    ### Initiating KBinsDiscretizer algorithm\n    \n    KBins_d = KBinsDiscretizer(n_bins = n_bins, encode = encode, \n                               strategy = strategy)\n    \n    ### Getting binned variables\n    \n    df_train_binned = KBins_d.fit_transform(df_train[Vars_list])\n    df_test_binned = KBins_d.transform(df_test[Vars_list])\n    \n    ### NOT the best way of creating column names :)\n    \n    if encode == 'onehot-dense':\n        \n        Cols = df_train[Vars_list].shape[1] * n_bins\n        \n    else: Cols = df_train[Vars_list].shape[1]\n    \n    df_train_binned = pd.DataFrame(df_train_binned, \n                                   columns = [\"col\" + str(i) for i in range(0, Cols)])\n\n    df_test_binned = pd.DataFrame(df_test_binned, \n                                  columns = [\"col\" + str(i) for i in range(0, Cols)])\n    \n    return(df_train_binned, df_test_binned)","de4b0b2c":"Result_2 = discretizer(df_train, df_test, Num_vars[0:5],\n                       n_bins = 3, encode = 'onehot-dense', strategy = 'kmeans')","75e76c1e":"df_binned_train = Result_2[0]\ndf_binned_test = Result_2[1]","4179fc11":"df_binned_train.head(3)","5eb144d2":"from itertools import combinations","4ee48c42":"Cat_vars = ['ExterCond', 'MasVnrType', 'ExterQual']","26cf5a5a":"def cat_var_combinations(df, Vars_list):\n\n    Comb_train = []\n    Cols = []\n    \n    for \u0441_1, c_2 in combinations(df[Vars_list], 2):\n    \n        Comb_train.append(df[\u0441_1].astype(str) + \" | \" + df[c_2].astype(str))\n    \n        Cols.append(str(\u0441_1) + \" | \" + str(c_2))\n    \n        df_final = pd.DataFrame(Comb_train).T\n        df_final.columns = Cols\n        \n    return(df_final)","041ce49f":"df_comb_cat_train = cat_var_combinations(df_train, Cat_vars)\ndf_comb_cat_test = cat_var_combinations(df_test, Cat_vars)","ab29fd64":"df_comb_cat_train.head(3)","627f6978":"df_comb_cat_train['SalePrice'] = df_train['SalePrice']\n\nVisual_vars = df_comb_cat_train.columns.tolist()\nVisual_vars.remove('SalePrice')","f9261cba":"with plt.rc_context(rc = {'figure.dpi': 500, 'axes.labelsize': 7.5, \n                          'xtick.labelsize': 5.5, 'ytick.labelsize': 5.5}):\n\n    fig_2, ax_2 = plt.subplots(3, 1, figsize = (8, 10))\n\n    for idx, (column, axes) in list(enumerate(zip(list(df_comb_cat_train.columns), ax_2.flatten()))):\n    \n        order = df_comb_cat_train.groupby(column)['SalePrice'].mean().sort_values(ascending = True).index\n    \n        sns.violinplot(ax = axes, x = df_comb_cat_train[column], \n                       y = np.log(df_comb_cat_train['SalePrice']),\n                       order = order, scale = 'width',\n                       linewidth = 0.5, palette = 'viridis',\n                       inner = None)\n    \n        plt.setp(axes.collections, alpha = 0.3)\n    \n        sns.stripplot(ax = axes, x = df_comb_cat_train[column], \n                      y = np.log(df_comb_cat_train['SalePrice']),\n                      palette = 'viridis', s = 1.5, alpha = 1,\n                      order = order, jitter = 0.2)\n        \n        sns.pointplot(ax = axes, x = df_comb_cat_train[column],\n                      y = np.log(df_comb_cat_train['SalePrice']),\n                      order = order,\n                      color = '#ff5736', scale = 0.2,\n                      estimator = np.mean, ci = 'sd',\n                      errwidth = 0.5, capsize = 0.15, join = True)\n    \n        plt.setp(axes.lines, zorder = 100)\n        plt.setp(axes.collections, zorder = 100)\n    \n        if df_comb_cat_train[column].nunique() > 5: \n        \n            plt.setp(axes.get_xticklabels(), rotation = 90)\n    \n    else:\n    \n        [axes.set_visible(False) for axes in ax_2.flatten()[idx + 1:]]\n\nplt.tight_layout(pad = 1)\nplt.show()","ac19b60a":"import featuretools as ft","ded38a67":"All_vars = ['GrLivArea', 'LotArea', 'Neighborhood']\n\nCat_vars_only = ['Neighborhood']","3e0655d3":"def f_tools(df, Vars_list_0, Vars_list_1):\n    \n    df_ft = df[Vars_list_0]\n    df_ft['ID'] = list(range(0, df_ft.shape[0]))\n    \n    ### Loading data and creating an entity\n    \n    ES = ft.EntitySet(id = 'SalePrice_data')\n    ES = ES.entity_from_dataframe(entity_id = 'df_ft',                       \n                                  dataframe = df_ft, index = 'ID')\n    \n    ### Creating relationships\n    \n    for column in Vars_list_1:\n        \n        ES = ES.normalize_entity(base_entity_id = 'df_ft', \n                             new_entity_id = str(column), index = str(column))\n        \n    ### Creating features via aggregations\n    \n    features, feature_names = ft.dfs(entityset = ES,\n                                     target_entity = 'df_ft',\n                                     agg_primitives = ['max', 'min', 'mean'],\n                                     max_depth = 2)\n    \n    features = features.drop(Vars_list_0, axis = 1)\n    features = features.dropna(axis = 1)\n    \n    return(features)","28f21e57":"df_ft_train = f_tools(df_train, All_vars, Cat_vars_only)\ndf_ft_test = f_tools(df_test, All_vars, Cat_vars_only)","30b26a7e":"df_ft_train[['Neighborhood.MAX(df_ft.LotArea)']].head(3)","69fe6f35":"pd.DataFrame({'Check': df_train.groupby('Neighborhood')['LotArea'].transform('max')}).head(3)","d69b9ea3":"with plt.rc_context(rc = {'figure.dpi': 250, 'axes.labelsize': 6, \n                          'xtick.labelsize': 5, 'ytick.labelsize': 5}):\n\n    fig, ax = plt.subplots(2, 3, figsize = (6.5, 4.5), sharey = True)\n\n    for idx, (column, axes) in list(enumerate(zip(list(df_ft_train.columns), \n                                                  ax.flatten()))):\n    \n        sns.scatterplot(ax = axes, x = df_ft_train[column], \n                        y = np.log(df_train['SalePrice']),\n                        hue = np.log(df_train['SalePrice']),\n                        palette = 'viridis', alpha = 0.7, s = 7)\n    \n        axes.legend([], [], frameon = False)\n    \n    else:\n    \n        [axes.set_visible(False) for axes in ax.flatten()[idx + 1:]]\n\n    plt.tight_layout(pad = 1)\n    plt.show()","a1d82c01":"Useful articles and documents:\n\n- <a href=\"https:\/\/medium.com\/greyatom\/using-clustering-for-feature-engineering-on-the-iris-dataset-f438366d0b4b\">Clustering for feature engineering<\/a>;\n- <a href=\"https:\/\/www.analyticsvidhya.com\/blog\/2019\/08\/comprehensive-guide-k-means-clustering\/\">A guide to k-means clustering<\/a>;\n- <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.KMeans.html\">Sklearn documentation<\/a>","037a906a":"## 3. Importing data <a class=\"anchor\" id = \"I_3\"><\/a>","bf47e3f0":"Useful articles and documents:\n\n- <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.KBinsDiscretizer.html\">Sklearn documentation<\/a>","b30baf98":"To make my example cleaner, I took only the first 5 numeric variables.","126a99bc":"## 1. K-means clustering <a class=\"anchor\" id = \"II_1\"><\/a>","76f4bfa5":"Well, <span style=\"color:#E85E40\"> featuretools <\/span> allows you to do a lot. In this notebook, I covered only feature engineering with the help of aggregations.","c477a0cb":"## 2. K-bins discretisation <a class=\"anchor\" id = \"II_2\"><\/a>","74d57879":"It is evident that after 12 clusters a decrease in the SSE became insignificant.","dc6ff875":"For that purpose, I created a simple utility function:","98425a4b":"<div style = \"text-align: justify\"> Sometimes, binning continuous variables can be quite valuable. This technique allows you to deal with outliers and features that are not homogeneous, thus preventing overfitting. Nevertheless, binning always results in information loss. <\/div>","a5229746":"Lastly, you can visualise new variables:","106bf860":"## 3. Categorical variables interactions <a class=\"anchor\" id = \"II_3\"><\/a>","49e63c3c":"## 4. Featuretools <a class=\"anchor\" id = \"II_4\"><\/a>","64d46f64":"<h1><center> Table of contents <\/center><\/h1>","bdaa8618":"<h1><center> I. Preparation <\/center><\/h1>","d040c39f":"You can always plot new variables against the target and explore them more meticulously:","666ddbfc":"<div style = \"text-align: justify\"> The name of this feature essentially speaks for itself. \"LotArea\" area was aggregated by \"Neighborhood\", and the aggregation function was the mean. We can get the same result with the help of <code style = \"background-color: #faedde\">.groupby()<\/code>:<\/div>","f7c7f9d7":"## Thanks for reading!","6cdd8633":"<h1><center> II. Feature engineering <\/center><\/h1>","7dcd7d08":"<div style = \"text-align: justify\"> Before generating new variables, we have to determine the optimal number of clusters. One way to do it is to draw a so called <span style=\"color:#E85E40\">elbow curve<\/span>. We plot a number of clusters against <span style=\"color:#E85E40\"> inertia_ <\/span> \u2013 the sum of squared distances of samples to their closest cluster centre. Our task is to find the smallest number of clusters while keeping <span style=\"color:#E85E40\"> inertia_ <\/span> as low as possible. <\/div>","abab04ee":"<div style = \"text-align: justify\">We can use the output of <span style=\"color:#E85E40\">k-means clustering<\/span> as features for our models. On top of that, not only clusters themselves can be treated as variables but also the distance to the cluster centres can be used as potential predictors.<\/div>","99d8fd5b":"<div style = \"text-align: justify\"> I used only 3 basic primitives <code style = \"background-color: #faedde\">agg_primitives = ['max', 'min', 'mean']<\/code>. If you want to learn more about other options, you should either type <code style = \"background-color: #faedde\">ft.primitives.list_primitives()<\/code> or consult this <a href=\"https:\/\/docs.featuretools.com\/en\/stable\/api_reference.html#feature-primitives\">website<\/a>. <\/div>","2e0ccabd":"### I. Preparation\n\n* [1. Importing libraries](#I_1)\n\n\n* [2. Data preparation](#I_2)\n\n    \n* [3. Data preparation](#I_3)\n        \n        \n### II. Feature engineering\n\n* [1. K-means clustering](#II_1)\n\n\n* [2. K-bins discretisation](#II_2)\n\n\n* [3. Categorical variables interactions](#II_3)\n\n\n* [4. Featuretools](#II_4)","88f221f5":"## 2. Setting global parameters for plots <a class=\"anchor\" id = \"I_2\"><\/a>","92e58134":"Finally, I created a utility function for engineering features from <span style=\"color:#E85E40\">k-means clustering<\/span>:","4a12bff9":"<div style = \"text-align: justify\"> First, I defined all variables that would participate in generating features <code style = \"background-color: #faedde\">['GrLivArea', 'LotArea', 'Neighborhood']<\/code>. Following that, I isolated a single categorical variable <code style = \"background-color: #faedde\">['Neighborhood']<\/code> that was used to aggregate the rest of the continuous variables.<\/div>","ee4921a2":"First, we have to define numeric variables that we are going to use for clustering.","a0a6b2aa":"First and foremost, you should create a list of categorical variables that you want to combine:","e6f94d03":"## 1. Importing basic libraries <a class=\"anchor\" id = \"I_1\"><\/a>","b2166a1f":"<div style = \"color: #000000;\n             display: fill;\n             padding: 8px;\n             border-radius: 5px;\n             border-style: solid;\n             border-color: #a63700;\n             background-color: rgba(235, 125, 66, 0.3)\">\n    \n<span style = \"font-size: 20px; font-weight: bold\">Note:<\/span> \nThe purpose of this kernel is to familiarise you with some feature engineering techniques. I used cleaned training and test sets from one of my <a href=\"https:\/\/www.kaggle.com\/suprematism\/top-7-useful-graphs-and-encoding-techniques\">notebooks<\/a>. Thus, should you want to explore data cleaning and model building, refer to the link placed above.\n<\/div>","8f4ddb5e":"Useful articles and documents:\n\n- <a href=\"https:\/\/featuretools.alteryx.com\/en\/stable\/index.html\">Featuretools documentation<\/a>\n- <a href=\"https:\/\/www.kaggle.com\/liananapalkova\/automated-feature-engineering-for-titanic-dataset\">Automated feature engineering for Titanic dataset<\/a>","429ac354":"<div style = \"text-align: justify\"> If you want, you can change a strategy of binning. For instance, pass <code style = \"background-color: #faedde\">strategy = 'uniform'<\/code> instead of <code style = \"background-color: #faedde\">strategy = 'kmeans'<\/code>. Also, it is up to you to define how variables are going to be encoded. Use either <code style = \"background-color: #faedde\">encode = 'onehot-dense'<\/code> or <code style = \"background-color: #faedde\">encode = 'ordinal'<\/code>. To get more details, consult the official documentation. <\/div>","1d192908":"Let's explore a particular variable:","6b8df5ba":"Features for only 5 clusters were calculated in order to keep the notebook less cluttered.","529e4dc0":"Useful articles and documents:\n\n- <a href=\"https:\/\/www.coursera.org\/lecture\/competitive-data-science\/feature-interactions-yt5t3\">Feature interactions<\/a>"}}