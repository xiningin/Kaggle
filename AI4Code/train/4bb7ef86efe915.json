{"cell_type":{"3598da78":"code","a0d93133":"code","fbf367e2":"code","ef61bc1e":"code","252677a8":"code","d50c2a2a":"code","5708d3b5":"code","afefd87a":"code","cd6df192":"code","0ca23f98":"code","7dc3e672":"code","48e7c9ec":"code","786a4221":"code","411d12fa":"code","981d8e16":"code","0c6101c5":"code","316aec86":"code","914e5d49":"code","2aa9160c":"code","e850f462":"code","ba79f23e":"code","8380e055":"code","9330d1b3":"code","22c5fc73":"code","855bc681":"code","be8f3b35":"code","45f133b9":"markdown","c49da653":"markdown","ea6a3ac0":"markdown","fe8dfec4":"markdown","21f558a2":"markdown","e832667a":"markdown","a0b7ab17":"markdown","64ad1e4f":"markdown","4f55024b":"markdown","92ac4295":"markdown","7eda81df":"markdown","d2675019":"markdown","982e2d4a":"markdown","7fde9743":"markdown","175c7be5":"markdown","c12a5fbd":"markdown"},"source":{"3598da78":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a0d93133":"# import\n# \u300a\u30a4\u30f3\u30dd\u30fc\u30c8\u300b\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n%matplotlib inline\n\nnp.random.seed(2)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.layers import BatchNormalization\n\n\nsns.set(style='white', context='notebook', palette='deep')","fbf367e2":"# Load the data \u300a\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\u300b\ntrain = pd.read_csv(\"..\/input\/fashionmnist\/fashion-mnist_train.csv\")\ntest = pd.read_csv(\"..\/input\/fashionmnist\/fashion-mnist_test.csv\")","ef61bc1e":"# Check the test\n# \u300a\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u78ba\u8a8d\u3059\u308b\u3002\u300b\n# test.head()\n\n# Drop the Answer\n# \u300a\u89e3\u7b54\u3092\u524a\u9664\u3059\u308b\u300b\ntest = test.drop(labels = [\"label\"],axis = 1)\n\n# Check the test without label \n# \u300a\u89e3\u7b54\u304c\u6d88\u3048\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d\u300b\ntest.head()","252677a8":"# Check the train\n# \u300a\u4eca\u56de\u306e\u30b3\u30f3\u30da\u3067\u4e0a\u304c\u3063\u3066\u3044\u308b\u30c7\u30fc\u30bf\u304c\u5165\u3063\u3066\u3044\u308b\u304b\u78ba\u8a8d\u3059\u308b\u3002\u300b\ntrain.head()\n\n# divide train into X_train and Y_train\n# \u300aX_train\u3068Y_train\u3067\u5206\u5272\u3059\u308b\u3002\u300b\nY_train = train[\"label\"]\nX_train = train.drop(labels =[\"label\"],axis = 1)","d50c2a2a":"# Normalize the data\n# \u300a\u30c7\u30fc\u30bf\u3092\u6a19\u6e96\u5316\u3059\u308b\u3002\u300b\nX_train = X_train \/ 255.0\ntest = test \/ 255.0\n\n# Check the data\n# \u300a\u30c7\u30fc\u30bf\u306e\u78ba\u8a8d\u300b\nX_train.head()\ntest.head()","5708d3b5":"# Check the data of null\n# null\u5024\u306e\u78ba\u8a8d\nX_train.isnull().any().describe()","afefd87a":"test.isnull().any().describe()","cd6df192":"# Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1)\n# \u30a4\u30e1\u30fc\u30b8\u30d5\u30a1\u30a4\u30eb\u309228\u00d728\u00d71\u306e\u5f62\u5f0f\u306b\u3059\u308b\u3002\nX_train = X_train.values.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1)","0ca23f98":"# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\n# \u30e9\u30d9\u30eb\u3092 one hot \u30d9\u30af\u30c8\u30eb\u3078\u5909\u63db\u3059\u308b\u3002(\u4f8b : 2 -> [0,0,1,0,0,0,0,0,0,0])\nY_train = to_categorical(Y_train, num_classes = 10)","7dc3e672":"# Set the random seed\n# random seed\u306e\u8a2d\u5b9a\nrandom_seed = 2","48e7c9ec":"# Split the train and the validation set for the fitting\n# \u8a13\u7df4\u30c7\u30fc\u30bf\u3078\u306e\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\u306e\u305f\u3081\u306b\u3001\u8a13\u7df4\u30c7\u30fc\u30bf\u3092train\u3068validation\u306b\u5206\u5272\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=random_seed)","786a4221":"# Some examples\n# \u30b5\u30f3\u30d7\u30eb\u306e\u78ba\u8a8d\ng = plt.imshow(X_train[0][:,:,0])","411d12fa":"# Set the CNN model \n# my CNN architechture is In -> [[Conv2D->relu]*3 -> MaxPool2D -> Dropout]*4 -> Flatten -> Dense -> Dropout -> Out\n# CNN\u30e2\u30c7\u30eb\u306e\u8a2d\u5b9a\n# \u79c1\u306eCNN\u306e\u69cb\u9020\u306f\u6b21\u306e\u901a\u308a\u3067\u3059\u3002-> [[Conv2D->relu]*3 -> MaxPool2D -> Dropout]*4 -> Flatten -> Dense -> Dropout -> Out\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(10, activation = \"softmax\"))","981d8e16":"# Define the optimizer\n# \u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u306e\u5b9a\u7fa9\noptimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)","0c6101c5":"# Compile the model\n# \u30e2\u30c7\u30eb\u306e\u30b3\u30f3\u30d1\u30a4\u30eb\nmodel.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","316aec86":"# Set a learning rate annealer\n# learning rate annealer\u306e\u8a2d\u5b9a\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","914e5d49":"epochs = 50\nbatch_size = 86","2aa9160c":"# Without data augmentation\n# \u30c7\u30fc\u30bf\u306e\u6c34\u5897\u3057\u3092\u3057\u306a\u3044\u5834\u5408\u306f\u4ee5\u4e0b\u3092\u5b9f\u884c\n# history = model.fit(X_train, Y_train, batch_size = batch_size, epochs = epochs, \n#           validation_data = (X_val, Y_val), verbose = 2)","e850f462":"# With data augmentation to prevent overfitting\n# \u904e\u5b66\u7fd2\u3092\u907f\u3051\u308b\u305f\u3081\u3001\u30c7\u30fc\u30bf\u306e\u6c34\u5897\u3057\n\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.01, # Randomly zoom image \n        width_shift_range=0.03,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.03,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(X_train)","ba79f23e":"# Fit the model\n# \u30e2\u30c7\u30eb\u306e\u5b66\u7fd2\nhistory = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (X_val,Y_val),\n                              verbose = 2, steps_per_epoch=X_train.shape[0] \/\/ batch_size\n                              , callbacks=[learning_rate_reduction])","8380e055":"# Plot the loss and accuracy curves for training and validation \n# \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3068\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u306e\u640d\u5931\u30fb\u6b63\u78ba\u6027\u66f2\u7dda\nfig, ax = plt.subplots(2,1)\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['acc'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_acc'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","9330d1b3":"# Look at confusion matrix \n# \u6df7\u540c\u884c\u5217\u3092\u898b\u3066\u307f\u307e\u3057\u3087\u3046\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Predict the values from the validation dataset\n# \u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304b\u3089\u4e88\u6e2c\nY_pred = model.predict(X_val)\n# Convert predictions classes to one hot vectors \n# \u4e88\u6e2c\u7d50\u679c\u3092one hot\u30d9\u30af\u30c8\u30eb\u3078\u5909\u63db\nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\n# \u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u306e\u6b63\u89e3\u5024\u3092one hot\u30d9\u30af\u30c8\u30eb\u3078\u5909\u63db\nY_true = np.argmax(Y_val,axis = 1) \n# compute the confusion matrix\n# \u6df7\u540c\u884c\u5217\u3092\u8a08\u7b97\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\n# \u6df7\u540c\u884c\u5217\u3092\u8868\u793a\nplot_confusion_matrix(confusion_mtx, classes = range(10)) ","22c5fc73":"# Display some error results \n# \u4f55\u7a2e\u985e\u304b\u306e\u30a8\u30e9\u30fc\u3092\u78ba\u8a8d\u3059\u308b\u3002\n# Errors are difference between predicted labels and true labels\n# \u30a8\u30e9\u30fc\u306f\u6b63\u89e3\u5024\u3068\u4e88\u6e2c\u5024\u3067\u7570\u306a\u3063\u3066\u3044\u307e\u3059\u3002\nerrors = (Y_pred_classes - Y_true != 0)\n\nY_pred_classes_errors = Y_pred_classes[errors]\nY_pred_errors = Y_pred[errors]\nY_true_errors = Y_true[errors]\nX_val_errors = X_val[errors]\n\ndef display_errors(errors_index,img_errors,pred_errors, obs_errors):\n    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n    n = 0\n    nrows = 2\n    ncols = 3\n    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n    for row in range(nrows):\n        for col in range(ncols):\n            error = errors_index[n]\n            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n            n += 1\n\n# Probabilities of the wrong predicted numbers\n# \u8aa4\u3063\u305f\u4e88\u6e2c\u5024\u3092\u53d6\u308b\u53ef\u80fd\u6027\nY_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n\n# Predicted probabilities of the true values in the error set\n# \ntrue_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n\n# Difference between the probability of the predicted label and the true label\n# \ndelta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n\n# Sorted list of the delta prob errors\n# \nsorted_dela_errors = np.argsort(delta_pred_true_errors)\n\n# Top 6 errors \n# 6\u3064\u306e\u30a8\u30e9\u30fc\nmost_important_errors = sorted_dela_errors[-6:]\n\n# Show the top 6 errors\n\ndisplay_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)","855bc681":"# predict results\n# \u4e88\u6e2c\u5024\u306e\u7b97\u51fa\nresults = model.predict(test)\n\n# select the indix with the maximum probability\n# \u6700\u3082\u53d6\u308a\u3046\u308b\u5024\u3092\u9078\u629e\nresults = np.argmax(results,axis = 1)\n\nresults = pd.Series(results,name=\"Label\")","be8f3b35":"submission = pd.concat([pd.Series(range(1,10001),name = \"id\"),results],axis = 1)\n\nsubmission.to_csv(\"cnn_mnist_datagen.csv\",index=False)","45f133b9":"## 4.2 Confusion matrix\u300a\u6df7\u540c\u884c\u5217\u300b","c49da653":"# 3. CNN\n## 3.1 Define the model\u300a\u30e2\u30c7\u30eb\u306e\u5b9a\u7fa9\u300b","ea6a3ac0":"## 2.4 Check for null and missing values\u300anull\u5024\u3084\u5024\u306a\u3057\u306e\u30c7\u30fc\u30bf\u304c\u306a\u3044\u304b\u78ba\u8a8d\u3059\u308b\u300b\n","fe8dfec4":"## 3.3 Data augmentation \u300a\u30c7\u30fc\u30bf\u306e\u6c34\u5897\u3057\u300b","21f558a2":"# 2. Data preparation\u300a\u30c7\u30fc\u30bf\u306e\u4e0b\u51e6\u7406\u300b\n## 2.1 Load data\u3000\u300a\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\u300b","e832667a":"## 2.3 Normalization\u300a\u6a19\u6e96\u5316\u300b\nWe perform a normalization to reduce the effect of illumination's differences. \n\nMoreover the CNN converg faster on [0..1] data than on [0..255].\n\n\u300a\u8272\u306b\u3088\u308b\u30e2\u30c7\u30eb\u3078\u306e\u5f71\u97ff\u3092\u6392\u9664\u3059\u308b\u305f\u3081\u3001\u30b0\u30ec\u30fc\u30b9\u30b1\u30fc\u30eb\u306b\u3088\u308b\u6a19\u6e96\u5316\u3092\u884c\u3044\u307e\u3059\u3002\u307e\u305f\u3001\u5404\u30ab\u30e9\u30e0\u306b\u5165\u3063\u3066\u3044\u308b\u5024\u30920-255\u304b\u30890-1\u3078\u6a19\u6e96\u5316\u3059\u308b\u3053\u3068\u306b\u3088\u308a\u30e2\u30c7\u30eb\u304c\u65e9\u304f\u52d5\u304d\u307e\u3059\u3002\u300b","a0b7ab17":"## 2.2 Spliting Data Set\u3000\u300a\u30c7\u30fc\u30bf\u306e\u5206\u5272\u300b\nFashion MNIST datasets have the answer In fashion-mnist_test.csv. So we have to split fashion-mnist_test.csv.\n\u300aFashion MNIST\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306ffashion-mnist_test.csv\u5185\u306b\u89e3\u7b54\u3092\u6301\u3063\u3066\u3044\u307e\u3059\u3002\u305d\u306e\u305f\u3081\u3001fashion-mnist_test.csv\u3092\u5206\u5272\u3057\u307e\u3059\u3002\u300b","64ad1e4f":"## 2.7 Split training and valdiation set \u300a\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3068\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u306e\u5206\u5272\u300b","4f55024b":"## 2.5 Reshape\u300a\u30c7\u30fc\u30bf\u5f62\u5f0f\u306e\u5909\u66f4\u300b","92ac4295":"# 1. Introduction\u300a\u306f\u3058\u3081\u306b\u300b\nThis kernel is Convolutional Neural Network for fashion image recognition trained on Fashion MNIST dataset. I choosed to build it with keras API (Tensorflow backend) which is very intuitive. Firstly, I will prepare the data (fashion images) then I will focus on the CNN modeling and evaluation.\n\n\u300a\u3053\u306e\u30ab\u30fc\u30cd\u30eb\u306fFashion MNIST\u3068\u3044\u3046\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u3082\u3068\u306b\u8a13\u7df4\u3057\u305f\u3001Convolutional Neural Network\u3092\u7528\u3044\u305f\u30d5\u30a1\u30c3\u30b7\u30e7\u30f3\u753b\u50cf\u3092\u5224\u5225\u3059\u308b\u3082\u306e\u3067\u3059\u3002\u79c1\u306fKeras\u3068\u3044\u3046\u76f4\u611f\u7684\u306b\u7528\u3044\u308b\u3053\u3068\u306e\u3067\u304d\u308bAPI\u3092\u7528\u3044\u3066\u3053\u306e\u30ab\u30fc\u30cd\u30eb\u3092\u4f5c\u6210\u3057\u307e\u3057\u305f\u3002\u307e\u305a\u306f\u6700\u521d\u306b\u30c7\u30fc\u30bf\uff08\u30d5\u30a1\u30c3\u30b7\u30e7\u30f3\u753b\u50cf\uff09\u306e\u4e0b\u6e96\u5099\u3092\u3057\u3001\u305d\u306e\u3042\u3068\u3001CNN\u306e\u30e2\u30c7\u30ea\u30f3\u30b0\u3068\u8a55\u4fa1\u3092\u3057\u3066\u3044\u304d\u307e\u3059\u3002\u300b\n\nThis Notebook follows three main parts:\n* The data preparation\n* The CNN modeling and evaluation\n* The results prediction and submission\n\n\u300a\u3053\u306eNoteBook\u306f\uff13\u3064\u306e\u30d1\u30fc\u30c8\u306b\u308f\u304b\u308c\u3066\u3044\u307e\u3059\u3002\n* \u30c7\u30fc\u30bf\u306e\u4e0b\u51e6\u7406\n* CNN\u306e\u30e2\u30c7\u30ea\u30f3\u30b0\u3068\u8a55\u4fa1\n* \u4e88\u6e2c\u7d50\u679c\u3068\u30b5\u30d6\u30df\u30c3\u30c8\n\u300b\n\nI refered [very helpful kernel](https:\/\/www.kaggle.com\/yassineghouzam\/introduction-to-cnn-keras-0-997-top-6\/notebook)\nI am grateful for it and the kaggler(Dr.Yassine Ghouzam).\nThanks for making this kernel , Dr.Yassine Ghouzam!!!\n\n\u300a\u3053\u306e[\u6700\u9ad8\u306b\u308f\u304b\u308a\u3084\u3059\u3044](https:\/\/www.kaggle.com\/yassineghouzam\/introduction-to-cnn-keras-0-997-top-6\/notebook)\u3092\u53c2\u8003\u306b\u3057\u307e\u3057\u305f\u3002\n\u3053\u306e\u30ab\u30fc\u30cd\u30eb\u3092\u4f5c\u3063\u3066\u304f\u3060\u3055\u3063\u305fDr.Yassine Ghouzam\u306b\u975e\u5e38\u306b\u611f\u8b1d\u3057\u3066\u3044\u307e\u3059\u3002\n\u3053\u306e\u30ab\u30fc\u30cd\u30eb\u304c\u306a\u304b\u3063\u305f\u3089\u79c1\u306f\u304d\u3063\u3068CNN\u304c\u7406\u89e3\u3067\u304d\u306a\u3044\u307e\u307e\u3060\u3063\u305f\u3067\u3057\u3087\u3046\u3002\u300b","7eda81df":"## 2.6 Label encoding\u300a\u30e9\u30d9\u30eb\u306e\u5909\u63db\u300b","d2675019":"## 3.2 Set the optimizer and annealer\u300a\u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u3068\u30a2\u30cb\u30fc\u30e9\u30fc\u306e\u8a2d\u5b9a\u300b\n\u203b\u3044\u3044\u65e5\u672c\u8a9e\u304c\u601d\u3044\u3064\u304d\u307e\u305b\u3093\u3002\u3088\u308d\u3057\u3051\u308c\u3070\u30b3\u30e1\u30f3\u30c8\u3067\u6559\u3048\u3066\u3044\u305f\u3060\u3051\u307e\u3059\u3068\u5e78\u3044\u3067\u3059\u3002","982e2d4a":"# 4. Evaluate the model\u300a\u30e2\u30c7\u30eb\u306e\u8a55\u4fa1\u300b\n## 4.1 Training and validation curves\u300a\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3068\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u306e\u66f2\u7dda\u300b","7fde9743":"# 5. Prediction and submition\u300a\u4e88\u6e2c\u3068\u30b5\u30d6\u30df\u30c3\u30c8\u300b\n## 5.1 Predict and Submit results\u300a\u4e88\u6e2c\u3068\u7d50\u679c\u306e\u30b5\u30d6\u30df\u30c3\u30c8\u300b","175c7be5":"# CNN with Data Augmentation by Keras (Fashion MNIST)\n\n#### 21\/08\/2018\nOnly Explanation Written by English\u300aJapanese\u300b\n\u8aac\u660e\u6587\u306e\u307f\u8868\u8a18\u306f\u82f1\u8a9e\u300a\u65e5\u672c\u8a9e\u300b\u3067\u3059\u3002\n\n* **1. Introduction\u300a\u306f\u3058\u3081\u306b\u300b**\n* **2. Data preparation\u300a\u30c7\u30fc\u30bf\u306e\u4e0b\u51e6\u7406\u300b**\n    * 2.1 Load data\u3000\u300a\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\u300b\n    * 2.2 Spliting Data Set\u3000\u300a\u30c7\u30fc\u30bf\u306e\u5206\u5272\u300b\n    * 2.3 Normalization\u300a\u6a19\u6e96\u5316\u300b\n    * 2.4 Check for null and missing values\u300anull\u5024\u3084\u5024\u306a\u3057\u306e\u30c7\u30fc\u30bf\u304c\u306a\u3044\u304b\u78ba\u8a8d\u3059\u308b\u300b\n    * 2.5 Reshape\u300a\u30c7\u30fc\u30bf\u5f62\u5f0f\u306e\u5909\u66f4\u300b\n    * 2.6 Label encoding\u300a\u30e9\u30d9\u30eb\u306e\u5909\u63db\u300b\n    * 2.7 Split training and valdiation set \u300a\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3068\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u306e\u5206\u5272\u300b\n* **3. CNN**\n    * 3.1 Define the model\u300a\u30e2\u30c7\u30eb\u306e\u5b9a\u7fa9\u300b\n    * 3.2 Set the optimizer and annealer\u300a\u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u3068\u30a2\u30cb\u30fc\u30e9\u30fc\u306e\u8a2d\u5b9a\u300b\n    * 3.3 Data augmentation \u300a\u30c7\u30fc\u30bf\u306e\u6c34\u5897\u3057\u300b\n* **4. Evaluate the model\u300a\u30e2\u30c7\u30eb\u306e\u8a55\u4fa1\u300b**\n    * 4.1 Training and validation curves\u300a\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3068\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u306e\u66f2\u7dda\u300b\n    * 4.2 Confusion matrix\u300a\u6df7\u540c\u884c\u5217\u300b**\n* **5. Prediction and submition\u300a\u4e88\u6e2c\u3068\u30b5\u30d6\u30df\u30c3\u30c8\u300b\n    * 5.1 Predict and Submit results\u300a\u4e88\u6e2c\u3068\u7d50\u679c\u306e\u30b5\u30d6\u30df\u30c3\u30c8\u300b","c12a5fbd":"you found this notebook helpful or you just liked it , some upvotes would be very much appreciated - That will keep me motivated :)"}}