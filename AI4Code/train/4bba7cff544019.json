{"cell_type":{"1cd9b50b":"code","8aaafdbc":"code","490c9f5c":"code","b28d08e1":"code","141a33bd":"code","e109675d":"code","124a62b2":"code","ce9ba966":"code","03189196":"code","f59f415a":"code","719d527b":"code","ff3663af":"code","96e7c167":"code","3b228516":"code","324d0889":"code","0a166fb6":"code","aa877b67":"code","078cebd5":"code","4550319b":"code","f254e1a9":"code","49b5115c":"code","ab817dcb":"code","d7624b47":"code","f2a532ed":"code","ae86245e":"code","07afb1c7":"code","23908189":"code","467c8ff0":"code","cc12bdad":"code","2e20b014":"code","511dca61":"markdown","1cd3f8f7":"markdown","e0625560":"markdown","2cb77d12":"markdown","f40784fb":"markdown","2f532bc5":"markdown","44fa93b2":"markdown","b54696bd":"markdown","4c22f871":"markdown"},"source":{"1cd9b50b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8aaafdbc":"train = pd.read_csv(\"\/kaggle\/input\/hackerearth-machine-learning-exhibit-art\/dataset\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/hackerearth-machine-learning-exhibit-art\/dataset\/test.csv\")","490c9f5c":"train","b28d08e1":"train_Y = train['Cost'].abs()\ntrain_X = train.drop(['Cost'], axis=1)\ntest_X = test\ntrain_X","141a33bd":"import pandas_profiling ","e109675d":"train.profile_report()","124a62b2":"train_X = train_X.drop(['Customer Id', 'Artist Name', 'Scheduled Date', 'Delivery Date', 'Customer Location', 'Transport'], axis = 1)\ntest_X = test.drop(['Customer Id', 'Artist Name', 'Scheduled Date', 'Delivery Date', 'Customer Location', 'Transport'], axis = 1)","ce9ba966":"#1. Function to replace NAN values with mode value\ndef impute_nan_most_frequent_category(DataFrame,ColName):\n    # .mode()[0] - gives first category name\n     most_frequent_category=DataFrame[ColName].mode()[0]\n    \n    # replace nan values with most occured category\n     DataFrame[ColName + \"_Imputed\"] = DataFrame[ColName]\n     DataFrame[ColName + \"_Imputed\"].fillna(most_frequent_category,inplace=True)\n#2. Call function to impute most occured category\nfor Columns in ['Material', 'Remote Location']:\n    impute_nan_most_frequent_category(train_X,Columns)\n    impute_nan_most_frequent_category(test_X,Columns)\n    \n# Display imputed result\ntrain_X[['Material','Material_Imputed','Remote Location','Remote Location_Imputed']].head(10)\n#3. Drop actual columns\ntrain_X = train_X.drop(['Material', 'Remote Location'], axis = 1)\ntest_X = test_X.drop(['Material', 'Remote Location'], axis = 1)","03189196":"#test_X.profile_report()","f59f415a":"train_X","719d527b":"# Get list of categorical variables\ns = (train_X.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","ff3663af":"from sklearn.preprocessing import LabelEncoder\n\n# Make copy to avoid changing original data \nlabel_X_train = train_X.copy()\nlabel_X_test = test_X.copy()\n\n# Apply label encoder to each column with categorical data\nlabel_encoder = LabelEncoder()\nfor col in object_cols:\n    label_X_train[col] = label_encoder.fit_transform(train_X[col])\n    label_X_test[col] = label_encoder.transform(test_X[col])\n","96e7c167":"from sklearn.impute import SimpleImputer\n\n# Imputation\nmy_imputer = SimpleImputer()\nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(label_X_train))\nimputed_X_test = pd.DataFrame(my_imputer.transform(label_X_test))\n\n# Imputation removed column names; put them back\nimputed_X_train.columns = train_X.columns\nimputed_X_test.columns = test_X.columns","3b228516":"imputed_X_train","324d0889":"import sklearn\nscaler = sklearn.preprocessing.StandardScaler()\n\nX_train = pd.DataFrame(scaler.fit_transform(imputed_X_train), columns=imputed_X_train.columns)\nX_test = pd.DataFrame(scaler.transform(imputed_X_test), columns=imputed_X_test.columns)\n","0a166fb6":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\nrf.fit(X_train, train_Y)","aa877b67":"y_prediction_rf = rf.predict(X_test)","078cebd5":"y_prediction_rf","4550319b":"#from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV","f254e1a9":"param_grid = [\n{'n_estimators': [50,100,250,500], \n 'max_depth': [10, 50, 100], 'bootstrap': [True, False]}\n]\n","49b5115c":"random_search_forest = RandomizedSearchCV(rf, param_grid, cv=10)\nrandom_search_forest.fit(X_train, train_Y)","ab817dcb":"tuned_rf_best_random = random_search_forest.best_estimator_","d7624b47":"tuned_rf_best_random","f2a532ed":"tuned_rf_best_random.fit(X_train, train_Y)","ae86245e":"tuned_rf_random_pred = tuned_rf_best_random.predict(X_test)","07afb1c7":"tuned_rf_random_pred","23908189":"df = pd.DataFrame()","467c8ff0":"df['Customer Id'] = test['Customer Id']\ndf['Cost'] = y_prediction_rf\ndf['Cost'] = df['Cost'].abs()","cc12bdad":"df","2e20b014":"df.to_csv('prediction1.csv', index = False)","511dca61":"**Tuning RF**","1cd3f8f7":"**Data Normalisation**","e0625560":"**Imputing missing values**","2cb77d12":"**RandomForest Regressor**","f40784fb":"# Used Pandas profiling for the first time and just amazed with it's utility, used RandomForest Regressor as model, also did it's hyperparameter tuning but base model is giving better score","2f532bc5":"# Suggestions and views are really needed and most welcomed","44fa93b2":"**Dealing with categorical columns having missing values**","b54696bd":"**Label Encoding Categorical Data**","4c22f871":"# Using Pandas Profiling for EDA "}}