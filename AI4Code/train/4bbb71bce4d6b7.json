{"cell_type":{"368099f3":"code","670541b0":"code","5736808a":"code","9235ef4e":"code","ef6048c3":"code","8c76ed7d":"code","5df290a5":"code","1b073dee":"code","9f3f113f":"code","a0244842":"code","724a35dd":"code","7bc3af0b":"code","6ff1feea":"code","ca68231f":"code","ed659565":"code","b791db93":"code","db39a377":"code","18fbd188":"code","832fb98c":"code","e483df76":"code","db363519":"code","8eb4a7f0":"code","bbf3bbe9":"code","1a4528fb":"code","83041899":"code","bc020342":"code","e7926cbc":"markdown","c1e32932":"markdown","258ce5db":"markdown","5c7cd09f":"markdown","88c6b811":"markdown","41f6168b":"markdown","8a34b405":"markdown","d6418cdc":"markdown","2cc8697d":"markdown","b06de307":"markdown","acf0d9c9":"markdown","df1f4a5e":"markdown","4ba23006":"markdown","efb16882":"markdown","367427fe":"markdown","265a7184":"markdown","b58a3703":"markdown","3e510055":"markdown","6fea7df8":"markdown","d29dfc93":"markdown","5eda3b4b":"markdown","2f58593a":"markdown","0e56c18b":"markdown","ebf6c339":"markdown","4c928810":"markdown","85867470":"markdown","73d9a39c":"markdown","3c31d8f7":"markdown"},"source":{"368099f3":"import pandas as pd \nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Avoid warning messages\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#plotly libraries\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\nfrom plotly.subplots import make_subplots\nimport cufflinks\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable=True, theme='pearl')\n\nfrom sklearn.feature_extraction.text import CountVectorizer","670541b0":"train = pd.read_csv(\"\/kaggle\/input\/shopee-sentiment-analysis\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/shopee-sentiment-analysis\/test.csv\")\ntraintest = train.append(test).reset_index(drop = True)","5736808a":"all_text = '||'.join(traintest['review'])\n# print(\"TOTAL CHARS include out separator '||' : \", len(all_text))","9235ef4e":"#create list of unique chars\nchars = list(set(all_text))\nprint(\"TOTAL CHARS : \", len(chars))","ef6048c3":"#sneak peak into chars\nchars[:10] ","8c76ed7d":"chars_count = {}\n\ndef append_chars_count(text):\n    for char in list(set(text)): # I count repeated character in one review as one\n        chars_count[char] = chars_count.get(char, 0) + 1\n        \ntraintest['review'].map(append_chars_count);","5df290a5":"# #ensure the length is same\n# len(chars_count), len(chars)","1b073dee":"# make it into datarame so we can sort it easily\ndf_chars = pd.DataFrame()\ndf_chars['char'] = list(chars_count.keys())\ndf_chars['count'] = list(chars_count.values())\ndf_chars = df_chars.sort_values(\"count\", ascending = False).reset_index(drop = True)","9f3f113f":"\ndf_chars['count'].value_counts().head(10)","a0244842":"df_chars.loc[(df_chars['count'] >= 2) & (df_chars['count'] < 4)].sample(10, random_state = 100)","724a35dd":"df_chars.loc[(df_chars['count'] >= 5) & (df_chars['count'] < 10)].sample(10, random_state = 100)","7bc3af0b":"df_chars.loc[(df_chars['count'] >= 10) & (df_chars['count'] < 20)].sample(20, random_state = 100)","6ff1feea":"df_chars = df_chars.loc[df_chars['count'] >= 10]","ca68231f":"# exclude chracters\nexclude_chars = ['\\n', ' ', '\\\\', '\u00b0', '.', \"'\", '\"', '-', '!', '-', ')', '(', ':', ';',\n                ',', '\\u200b', '\ufe0f', '\/', '\\xa0', \"?\", \"&\", '\u2019', '~', '+', '*', '%', '_', \n                 '#', '=', '@', ']', '['] # from manual observation \n\nmask = df_chars['char'].map(lambda x: not (x.isalnum() or str(x) in exclude_chars))\ndf_chars_special = df_chars.loc[mask]","ed659565":"def counting_char_with_sentiment(char, sentiment = 1):\n    return sum([char in review_ for review_ in list(train.loc[train['rating'] == sentiment]['review'])])\n\nfor i in range(1,6):\n    df_chars_special[f'rating_{i}'] = np.vectorize(counting_char_with_sentiment)(df_chars_special['char'], i)","b791db93":"df_chars_special.sample(20, random_state = 100).style.background_gradient( cmap='Greens')","db39a377":"rating_cols = [f'rating_{i}' for i in range(1,6)]\nweighted_score = list(np.arange(-1,1.5, 0.5))\n\ndf_chars_special['score'] = np.array([list(df_chars_special[a] * b) for a,b in zip(rating_cols, weighted_score)]).sum(axis = 0) \/  df_chars_special[rating_cols].sum(axis = 1)","18fbd188":"df_chars_special.sort_values('score', ascending= False)[['char', 'score'] + rating_cols].head(10).style.background_gradient( cmap='Greens')","832fb98c":"df_chars_special.sort_values('score', ascending= True)[['char', 'score'] + rating_cols].head(10).style.background_gradient(cmap='Reds_r')","e483df76":"def ngrams_top(corpus,ngram_range,n=None):\n    \"\"\"\n    List the top n words in a vocabulary according to occurrence in a text corpus.\n    \"\"\"\n    vec = CountVectorizer(stop_words = 'english',ngram_range=ngram_range, min_df= 1).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    total_list=words_freq[:n]\n    df=pd.DataFrame(total_list,columns=['text','count'])\n    return df\n\ndef merge_sentiment_ngrams(df, ngram_range,n=None):\n    df_full = ngrams_top(df['review'],ngram_range,n)\n    \n    for i in range(1,6):\n        df_temp = ngrams_top(df.loc[df['rating'] == i,'review'],ngram_range,n)\n        df_temp.columns = ['text', f'rating_{i}']\n        \n        df_full = df_full.merge(df_temp, on = 'text', how = 'outer')\n        df_full = df_full.fillna(0)\n        df_full[f'rating_{i}'] = df_full[f'rating_{i}'].astype(int)\n        \n    \n    df_full = df_full.loc[df_full['count'] >= 3 ]\n    \n    rating_cols = [f'rating_{i}' for i in range(1,6)]\n    weighted_score = list(np.arange(-1,1.5, 0.5))\n\n    df_full['score'] = np.array([list(df_full[a] * b) for a,b in zip(rating_cols, weighted_score)]).sum(axis = 0) \/  df_full[rating_cols].sum(axis = 1)\n    return df_full","db363519":"df_1grams = merge_sentiment_ngrams(train, ngram_range = (1,1))\ndf_2grams = merge_sentiment_ngrams(train, ngram_range = (1,2))\n\ndf_1grams = df_1grams.loc[df_1grams['count'] >= 100 ].sort_values('score', ascending= False)\ndf_2grams = df_2grams.loc[df_2grams['count'] >= 100 ].sort_values('score', ascending= False)\n","8eb4a7f0":"df_1grams.sort_values('score', ascending= False)[['text', 'score'] + rating_cols].head(10).style.background_gradient( cmap='Greens')","bbf3bbe9":"df_1grams.sort_values('score', ascending= True)[['text', 'score'] + rating_cols].head(10).style.background_gradient(cmap='Reds_r')","1a4528fb":"df_2grams.sort_values('score', ascending= False)[['text', 'score'] + rating_cols].head(10).style.background_gradient( cmap='Greens')","83041899":"df_2grams.sort_values('score', ascending= True)[['text', 'score'] + rating_cols].head(10).style.background_gradient(cmap='Reds_r')","bc020342":"df_2grams.loc[((df_2grams['score'] < 0.1) & (df_2grams['score'] > 0))].sort_values('score', ascending= True)[['text', 'score'] + rating_cols].head(10)","e7926cbc":"# Import Packages and Load Data","c1e32932":"Lets get the character a score with this formula: \n> score = ((#rating1 * -1) + (#rating2 * -0.5) + (#rating3 * 0) + (#rating4 * 0.5) + (#rating5 *1)) \/ (occurance in train)\n\nSo we get -1 if the sentiment is full of bad rating, and 1 when its full of good rating","258ce5db":"See composition of character by their occurance (from column : 'count')","5c7cd09f":"<font color='red'>Bad rating characters words <\/font>","88c6b811":"Lets See if a character belong to unique sentiment or not","41f6168b":"SEE this worring face \ud83d\ude27 in count 10!!! and this adorable \ud83d\ude38 and heartbreak \ud83d\udc94 !!! should be something. \n\nWe curate our chracter exploration until 10 count ","8a34b405":"## 1.2. Chracters with sentiments\ud83d\ude32","d6418cdc":"1 gram <font color='red'>bad<\/font> rating words","2cc8697d":"\n\n> I count repeated character in one review as one to avoid emoji spamming in one review","b06de307":"We can ignore char that happen once, and still considering char which happen 2x or 3x","acf0d9c9":"2 gram neutral words","df1f4a5e":"# If these visualization impress you or give you useful insight, give them an Upvote\ud83e\udd1f\t","4ba23006":"5 until 10 still do not tell anything \ud83e\udd26","efb16882":"Do as far as sample view (just change the `random_state` if you want to see other samples) the character is not really depict sentiment\n\ntry larger !!!","367427fe":"## 1.1. See Character Composition \ud83e\udd26","265a7184":"1 gram <font color='green'>good<\/font> rating words","b58a3703":"# 2. Words Insights? \ud83e\udd10\t","3e510055":"1 table tells more than thousand words","6fea7df8":"### **LETS VISIT CHARS WHO PICK SIDE!!!** \ud83d\ude20","d29dfc93":"There are 1468, there should be a lot of emojis","5eda3b4b":"See... told yaa... EMOJI !!!","2f58593a":"# 1. Character exploration \ud83d\ude21\ud83d\ude20","0e56c18b":"<font color='green'>Good rating characters words <\/font>","ebf6c339":"The goals is to check whether there are emoji \/ special characters and how useful they are","4c928810":"Observation:\n\n* <font color='green'>Positive<\/font> ratings produces adjective words like awesome, *mantap* (awesome in bahasa), and talking about seller good performance\n* <font color='red'>Negative<\/font> ratings produces words like poor, terible, refund. it is divided by 2 bad sides, product or seller quality\n","85867470":"2 grams <font color='red'>bad<\/font> rating words","73d9a39c":"I would like to focus on non aplha num chracter","3c31d8f7":"2 grams <font color='green'>good<\/font> rating words"}}