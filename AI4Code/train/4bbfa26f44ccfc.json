{"cell_type":{"898bc3c7":"code","fcbe2e86":"code","30a52f41":"code","1d485566":"code","cba653e4":"code","204f3b74":"code","79c78e4b":"code","ec7eff10":"code","c89528c1":"code","75b9f3c5":"code","755ef233":"code","21e5d27b":"code","e5407f44":"code","6f937583":"code","bc30c028":"code","8278bf6c":"code","d0462b9f":"code","de19e190":"code","d9752de6":"code","1fc1c9b5":"code","b55539f1":"code","dc645d35":"code","58ac7422":"code","9359b028":"code","9a7f318d":"code","25d3fd4f":"code","d637fd25":"code","c888e13b":"code","27795fa9":"code","9d4f1801":"code","a5958cb5":"code","3c79766d":"code","02b4e9a0":"code","d8f6a9c8":"code","c6f74dd4":"code","7521865e":"code","b15e6731":"code","06599205":"code","f8c39d73":"code","d170c40e":"code","38cb556c":"code","e305926d":"code","5e340e29":"code","43d7e903":"code","b1f77a6c":"code","20e232b0":"code","32289c3b":"code","45caa671":"markdown","75d8344d":"markdown","463cef5b":"markdown","18deaf0d":"markdown","03804c41":"markdown","0b26920c":"markdown","30f33cd7":"markdown","e5f2401c":"markdown","3762f211":"markdown","f77a05f7":"markdown"},"source":{"898bc3c7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats","fcbe2e86":"raw_df = pd.read_csv(\"..\/input\/housesalesprediction\/kc_house_data.csv\")","30a52f41":"raw_df","1d485566":"raw_df.shape","cba653e4":"raw_df.info()","204f3b74":"raw_df.head()","79c78e4b":"raw_df.describe()","ec7eff10":"raw_df.columns","c89528c1":"df = raw_df[['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n             'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above',\n             'sqft_basement', 'yr_built', 'yr_renovated', 'sqft_living15', 'sqft_lot15']].copy()","75b9f3c5":"df","755ef233":"df.isna().sum()","21e5d27b":"df.dropna(inplace=True)","e5407f44":"df.isna().sum()","6f937583":"df.duplicated().sum()","bc30c028":"df.drop_duplicates(inplace=True)","8278bf6c":"df","d0462b9f":"df.columns","de19e190":"from scipy.stats import skew\n#Top skewed columns\nnumeric_features = df.dtypes[df.dtypes != 'object'].index\nskewed_features = df[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skew = skewed_features[abs(skewed_features) > 0.5]\nprint(high_skew)","d9752de6":"#Transforming skewed columns\n# for feature in high_skew.index:\n#     if feature in ['waterfront', 'sqft_lot']:\n#         raw_df[feature] = np.log1p(raw_df[feature])","1fc1c9b5":"df.head()","b55539f1":"#Converting categorical data to numerical\ndata = pd.get_dummies(df)\ndata.head()","dc645d35":"df.head()","58ac7422":"# lets visualize the dependent variable (normal distribution)\n# Supress Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nsns.distplot(df['price'], fit=stats.norm)\nfig = plt.figure()\nres = stats.probplot(df['price'], plot=plt)","9359b028":"df.columns","9a7f318d":"# data seems to be skewed towards left here,\n# Let's check for the logarithmic distribution of price\n\n# ignoring this for better performance\nattr = 'price'\ndf_attr = np.log(df[attr])\nsns.distplot(df_attr, fit = stats.norm)\n\nfig = plt.figure()\nres = stats.probplot(df_attr, plot=plt)","25d3fd4f":"df['price'] = np.log(raw_df['price'])\n# df['price'] = np.log1p(df['price'])","d637fd25":"df","c888e13b":"# Outlier Analysis\nfig, axs = plt.subplots(12, figsize = (10,10))\ncols = df.columns\nfor i in range(0,len(cols)):\n    plt_i = sns.boxplot(raw_df[cols[i]], ax = axs[i])\nplt.tight_layout()","27795fa9":"# sns.pairplot(raw_df, \n#              x_vars=['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n#                      'floors', 'waterfront', 'view', 'condition', 'sqft_above',\n#                      'sqft_basement', 'yr_built', 'yr_renovated'], \n#              y_vars=['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n#                      'floors', 'waterfront', 'view', 'condition', 'sqft_above',\n#                      'sqft_basement', 'yr_built', 'yr_renovated'], \n#              kind='scatter')\n# plt.show()","9d4f1801":"# sns.pairplot(raw_df, \n#              x_vars=['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n#                      'floors', 'waterfront', 'view', 'condition', 'sqft_above',\n#                      'sqft_basement', 'yr_built', 'yr_renovated'], \n#              y_vars='price', \n#              kind='scatter')\n# plt.show()","a5958cb5":"plt.figure(figsize=(16, 6))\nsns.heatmap(df.corr(), annot=True)\nplt.show()","3c79766d":"df.head()","02b4e9a0":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor","d8f6a9c8":"df.columns","c6f74dd4":"X = df[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n       'waterfront', 'view', 'condition', 'grade',\n       'sqft_basement', 'yr_built', 'yr_renovated', 'sqft_living15',\n       'sqft_lot15']]\ny = df['price']\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, test_size = 0.3, random_state = 100)","7521865e":"model = RandomForestRegressor()  \nmodel.fit(X_train, y_train)","b15e6731":"model_predictions = model.predict(X_test)\nprint(model.score(X_test, y_test))","06599205":"from sklearn.metrics import mean_absolute_error, mean_squared_error, explained_variance_score, r2_score\nprint(mean_absolute_error(y_test, model_predictions))\nprint(mean_squared_error(y_test, model_predictions))\nprint(explained_variance_score(y_test, model_predictions))\nprint(r2_score(y_test, model_predictions))","f8c39d73":"from pprint import pprint\npprint(model.get_params())","d170c40e":"from sklearn.model_selection import RandomizedSearchCV # Number of trees in random forest\n\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 500, num = 50)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]# Create the random grid\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\npprint(random_grid)","38cb556c":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestRegressor()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)# Fit the random search model\nrf_random.fit(X_train, y_train)","e305926d":"rf_random.best_params_","5e340e29":"optimized_model = RandomForestRegressor(n_estimators=236, min_samples_split=5, min_samples_leaf=1, max_features='sqrt', max_depth=20, bootstrap=False)  \noptimized_model.fit(X_train, y_train)","43d7e903":"opt_model_predictions = optimized_model.predict(X_test)\nprint(optimized_model.score(X_test, y_test))","b1f77a6c":"from sklearn.metrics import mean_absolute_error, mean_squared_error, explained_variance_score, r2_score\nprint(mean_absolute_error(y_test, opt_model_predictions))\nprint(mean_squared_error(y_test, opt_model_predictions))\nprint(explained_variance_score(y_test, opt_model_predictions))\nprint(r2_score(y_test, opt_model_predictions))","20e232b0":"importances = optimized_model.feature_importances_\nprint(importances)","32289c3b":"featureImp= []\nfor feat, importance in zip(X_train.columns, importances):  \n    temp = [feat, importance*100]\n    featureImp.append(temp)\n\nfT_df = pd.DataFrame(featureImp, columns = ['Feature', 'Importance'])\nprint (fT_df.sort_values('Importance', ascending = False))","45caa671":"The independent variables like bedrooms, bathrooms, sqft_living, sqft_lot, etc. all are looking relevant here except for date, street, city, statezip, and country.","75d8344d":"For now, let's assume that there are no outliers, and review the model prediction to recheck outliers.","463cef5b":"The logarithmic distribution of price looks normal now. Let's move ahead with that.","18deaf0d":"We can see some correlations here, like sqft_above is positively correlated to sqft_living.","03804c41":"# Section B - ML","0b26920c":"# Section 1 - EDA","30f33cd7":"**price** being the dependent variable is normally distributed here.","e5f2401c":"# References\n\n* https:\/\/www.kaggle.com\/kunalprompt\/simple-linear-regression-advertising-and-sales\n* https:\/\/towardsdatascience.com\/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n* https:\/\/medium.com\/hackerdawn\/house-prices-prediction-using-random-forest-aa8722347276\n* https:\/\/www.kaggle.com\/subhradeep88\/house-price-predict-decision-tree-random-forest","3762f211":"# Random Forest Regression\n\nPredict the price of a house using Random Forest Regression.","f77a05f7":"**Important Hyperparameters in Random Forest Regression**\n\n**n_estimators** = number of trees in the foreset\n\n**max_features** = max number of features considered for splitting a node\n\n**max_depth** = max number of levels in each decision tree\n\n**min_samples_split** = min number of data points placed in a node before the node is split\n\n**min_samples_leaf** = min number of data points allowed in a leaf node\n\n**bootstrap** = method for sampling data points (with or without replacement)\n\nLet's adjust the hyperparameters to see if the model prediction changes."}}