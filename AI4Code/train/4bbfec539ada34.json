{"cell_type":{"59f52e4e":"code","8d9da846":"code","f75c5620":"code","c0ac518b":"code","cbc9d06a":"code","2d809911":"code","f4dd5d6f":"code","c369f7d4":"code","b2b5bf55":"code","3eda92c4":"code","a9a6d15e":"code","08f55275":"code","ab3e8fdd":"code","e246f74b":"code","c0303e9b":"code","d72d9d4d":"code","25728928":"code","a6e8e0aa":"code","58c85948":"code","19025d65":"code","6f8f4173":"code","3fc77d96":"code","690a28be":"code","0362e0a9":"code","ce29419d":"code","a5436f07":"markdown","cb7bc1fe":"markdown","e8ed2943":"markdown","1c3845b2":"markdown","4aa1bbe4":"markdown","a550ef73":"markdown","468cbc8a":"markdown","6d5bb160":"markdown","cb6a3e19":"markdown","fc7d27a7":"markdown","f71bce55":"markdown","21ea7093":"markdown","83c776e7":"markdown","ca2396db":"markdown","12d5f0ab":"markdown","fc8787a3":"markdown","f377381d":"markdown","97d3064f":"markdown"},"source":{"59f52e4e":"from IPython.display import clear_output\nimport warnings\n!pip install sweetviz\n!pip install --upgrade pandas\nwarnings.filterwarnings('ignore')\nclear_output()","8d9da846":"from pathlib import Path\ndata_dir = Path(\"\/kaggle\/input\/titanic\/\")\noutput_dir = Path(\"\/kaggle\/working\/\")","f75c5620":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n\nfor dirname, _, filenames in os.walk(data_dir):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c0ac518b":"train_data = pd.read_csv(data_dir \/ \"train.csv\")\ntest_data = pd.read_csv(data_dir \/ \"test.csv\")","cbc9d06a":"train_data.iloc[:3]","2d809911":"data_cleaner = [train_data, test_data]","f4dd5d6f":"import sweetviz as sv\nreport = sv.compare([data_cleaner[0], \"Train\"], [data_cleaner[1], \"Test\"], target_feat=\"Survived\", pairwise_analysis=\"on\")\nreport.show_notebook()","c369f7d4":"for dataset in data_cleaner:\n    dataset[\"Age\"].fillna(dataset[\"Age\"].median(), inplace=True)\n    dataset[\"Embarked\"].fillna(dataset[\"Embarked\"].mode()[0], inplace=True)\n    dataset[\"Fare\"].fillna(dataset[\"Fare\"].median(), inplace = True)\n\n    drop_column = ['Cabin', 'Ticket']\n    dataset.drop(drop_column, axis=1, inplace=True)\n\ntrain_data = train_data.drop(columns=[\"PassengerId\"])\ntest_ids = test_data.pop(\"PassengerId\")\n\ndata_cleaner = [train_data, test_data]","b2b5bf55":"for dataset in data_cleaner:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n    dataset['IsAlone'] = 1  #initialize to 1 = is alone\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0  # now update to no if family size is greater than 1\n\n    # Get \"Mr\", \"Miss\", \"Mrs\", and many titles from the name column.\n    dataset['Title'] = dataset[\"Name\"].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n    title_counts = dataset['Title'].value_counts() < 10\n    dataset[\"Title\"] = dataset[\"Title\"].apply(lambda x: \"Misc\" if title_counts.loc[x] == True else x)\n\n    # Divide the `Fare` into 4 intervals with similar quantities.\n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4, labels=False)\n\n    # Divide the `Age` into 4 discrete intervals according to its value.\n    dataset['AgeBin'] = pd.cut(dataset['Age'], [0, 25, 50, 75, 100], labels=False)\n\n    dataset.drop(columns=[\"Name\", \"Age\", \"Fare\"], inplace=True)\n\ntrain_data.sample(5)","3eda92c4":"train_data = pd.get_dummies(train_data)\ntrain_data.sample(5)","a9a6d15e":"test_data = pd.get_dummies(test_data)\ntest_data.sample(5)","08f55275":"report = sv.compare([train_data, \"Train\"], [test_data, \"Test\"], target_feat=\"Survived\")\nreport.show_notebook()","ab3e8fdd":"y = train_data.pop(\"Survived\")\nX = train_data","e246f74b":"!pip install lazypredict\nclear_output()","c0303e9b":"from lazypredict.Supervised import LazyClassifier\nfrom sklearn.model_selection import train_test_split\n\ndef lazy_predict(X, y, test_size=0.2):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n    clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n    models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n    return models, predictions\n\nlazy_predict(X, y)[0][:10]","d72d9d4d":"from sklearn.linear_model import RidgeClassifier","25728928":"from sklearn.feature_selection import RFECV\n\none_clf = RidgeClassifier()\nselector = RFECV(one_clf)\nselector.fit(X, y)","a6e8e0aa":"new_features = train_data.columns.values[selector.get_support()]\nnew_features","58c85948":"lazy_predict(X[new_features], y)[0][:10]","19025d65":"from sklearn.model_selection import GridSearchCV\n\ntuning_params = {\n    \"alpha\": [0.2, 0.4, 0.6, 0.8, 1],\n    \"normalize\": [False, True],\n    \"tol\": [1e-2, 1e-3, 1e-4, 1e-5],\n    \"solver\": [\"svd\", \"cholesky\", \"lsqr\", \"sparse_cg\", \"sag\", \"saga\"],\n    }\n\nsearch = GridSearchCV(one_clf, tuning_params, \"accuracy\")\nsearch.fit(X[new_features], y)\nsearch.best_params_","6f8f4173":"from sklearn.model_selection import ShuffleSplit, cross_validate\n\none_clf = RidgeClassifier(**search.best_params_).fit(X[new_features], y)\n\ncv_split = ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 42)\nresult = cross_validate(one_clf, X[new_features], y, cv=cv_split, return_train_score=True)\n\ntrain_score = result[\"train_score\"].mean()\ntest_score = result[\"test_score\"].mean()\n\nprint(\"CV Train Score mean:\", train_score)\nprint(\"CV Test Score mean:\", test_score)","3fc77d96":"predictions = one_clf.predict(test_data[new_features])\noutput = pd.DataFrame({'PassengerId': test_ids, 'Survived': predictions})\noutput.to_csv(output_dir \/ \"single_model_submission.csv\", index=False)","690a28be":"from xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier, RidgeClassifierCV\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import NearestCentroid\nfrom sklearn.ensemble import AdaBoostClassifier, VotingClassifier\n\nestimators=[\n        (\"xgb\", XGBClassifier()),\n        (\"lr\", LogisticRegression()),\n        (\"rc\", RidgeClassifier()),\n        (\"rccv\", RidgeClassifierCV()),\n        (\"lda\", LinearDiscriminantAnalysis()),\n        (\"cccv\", CalibratedClassifierCV()),\n        (\"svc\", SVC()),\n        (\"lsvc\", LinearSVC()),\n        (\"nc\", NearestCentroid()),\n        (\"ada\", AdaBoostClassifier())\n    ]\n\ngrid_n_estimator = [10, 50, 100, 300]\ngrid_ratio = [0.1, 0.25, 0.5, 0.75, 1.0]\ngrid_lr = [0.01, 0.03, 0.05, 0.1, 0.25]\ngrid_max_depth = [1, 2, 3, 4, 6, 8]\ngrid_min_samples = [5, 10, 0.03, 0.05, 0.1]\ngrid_bool = [True, False]\ngrid_C = [0.01, 0.1, 1, 10, 100]\n\nmultiple_params = [\n    {\n        \"learning_rate\": grid_lr,\n        \"max_depth\": grid_max_depth,\n        \"n_estimators\": grid_n_estimator,\n    },\n    {\n        \"C\": grid_C,\n        \"penalty\": [\"l1\", \"l2\", \"elasticnet\"],\n        \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"],\n    },\n    {\n        \"alpha\": grid_ratio,\n        \"normalize\": grid_bool,\n        \"solver\": [\"auto\", \"svd\", \"cholesky\", \"lsqr\", \"sparse_cg\", \"sag\", \"saga\"],\n    },\n    {\n        \"fit_intercept\": grid_bool,\n        \"normalize\": grid_bool,\n    },\n    {\n        \"solver\": [\"svd\", \"lsqr\", \"eigen\"],\n    },\n    {\n        \"method\": [\"sigmoid\", \"isotonic\"],\n    },\n    {\n        \"C\": grid_C,\n        \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n        \"degree\": grid_max_depth,\n        \"gamma\": [\"scale\", \"auto\"],\n    },\n    {\n        \"penalty\": [\"l1\", \"l2\"],\n        \"loss\": [\"hinge\", \"squared_hinge\"],\n        \"C\": grid_C,\n        \"fit_intercept\": grid_bool,\n    },\n    {},\n    {\n        \"n_estimators\": grid_n_estimator,\n        \"learning_rate\": grid_lr,\n        \"algorithm\": [\"SAMME\", \"SAMME.R\"],\n    },\n]\n\nfor (algo, clf), params in zip(estimators, multiple_params):\n    search = GridSearchCV(clf, params)\n    search.fit(X[new_features], y)\n\n    print(algo, \":\", search.best_params_)\n    clf.set_params(**search.best_params_)","0362e0a9":"vote_clf = VotingClassifier(\n    estimators=estimators,\n    voting='hard'\n)\n\nvote_clf.fit(X[new_features], y)\n\ncv_split = ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 42)\nresult = cross_validate(vote_clf, X[new_features], y, cv=cv_split, return_train_score=True)\n\ntrain_score = result[\"train_score\"].mean()\ntest_score = result[\"test_score\"].mean()\n\nprint(\"CV Train Score mean:\", train_score)\nprint(\"CV Test Score mean:\", test_score)","ce29419d":"predictions = vote_clf.predict(test_data[new_features])\noutput = pd.DataFrame({'PassengerId': test_ids, 'Survived': predictions})\noutput.to_csv(output_dir \/ 'ensemble_submission.csv', index=False)","a5436f07":"## Data Creating (Feature Engineering)\n\nWe extract the information from `SibSp` and `Parch`, then create two new features: `FamilySize` and `IsAlone`.\n\nFrom the `Name` feature, we can get the `Title` of each person. After the extraction, we can drop the `Name` column.\n\nFinally, we use `qcut()` and `cut()` from pandas, to convert the `Age` and `Fare` into discrete intervals, then we drop the `Age` and `Fare`.\n\n> - [pandas\u7684cut&qcut\u51fd\u6578](https:\/\/medium.com\/@morris_tai\/pandas%E7%9A%84cut-qcut%E5%87%BD%E6%95%B8-93c244e34cfc)\n> - [pandas\u7684cut\uff0cqcut\u51fd\u6570\u7684\u4f7f\u7528\u548c\u533a\u522b](https:\/\/zhuanlan.zhihu.com\/p\/68194655)\n","cb7bc1fe":"# Lazypredict","e8ed2943":"# EDA & Preparing Data","1c3845b2":"## Voting Ensembling","4aa1bbe4":"## Data Converting\n\nIn this step, we have to convert the features: `Sex`, `Title` into **discrete form** by calling `pandas.get_dummies()`.","a550ef73":"## Data Correcting and Completing\n\nWe fill the missing values of `age`, `embarked`, `fare` in both `train_data` and `test_data` with median and mode respectively.\n\nThen we drop the `id`, `cabin`, `ticket` columns from `train_data` because:\n\n1. `PassengerId` has no meaning.\n2. `Cabin` has too many missing values.\n3. `Ticket` has no meaning from its distribution.","468cbc8a":"First, we use library `sweetviz` to do a quick data exploration on the titanic dataset.\n\n1. Values in `age` are missing 177 (20%) in `train_data` and 86 (21%) in `test_data`. \n2. Values in `cabin` are missing 687 (77%) and 327 (78%) in both datasets.\n3. The features that have the most impact on `Survived` are `Sex` and `Fare`.\n","6d5bb160":"# Modelling","cb6a3e19":"## Prediction \ud83c\udf89","fc7d27a7":"# Table of Contents\n\n* [ML Approach (Sklearn + Pandas + Sweetviz + LazyPredict + Feature Engineering + Feature Selection + Model Selection + Model Ensembling)](https:\/\/www.kaggle.com\/windsuzu\/sklearn-eda-lazypredict-feat-engineering-ensemble#ML-Approach-(Sklearn-+-Pandas-+-Sweetviz-+-LazyPredict-+-Feature-Engineering-+-Feature-Selection-+-Model-Selection-+-Model-Ensembling))\n* [Table of Contents](https:\/\/www.kaggle.com\/windsuzu\/sklearn-eda-lazypredict-feat-engineering-ensemble#Table-of-Contents)\n* [Loading Data](https:\/\/www.kaggle.com\/windsuzu\/sklearn-eda-lazypredict-feat-engineering-ensemble#Loading-Data)\n* [EDA & Preparing Data](https:\/\/www.kaggle.com\/windsuzu\/sklearn-eda-lazypredict-feat-engineering-ensemble#EDA-&-Preparing-Data)\n    * [Data Correcting and Completing](https:\/\/www.kaggle.com\/windsuzu\/sklearn-eda-lazypredict-feat-engineering-ensemble#Data-Correcting-and-Completing)\n    * [Data Creating (Feature Engineering)](https:\/\/www.kaggle.com\/windsuzu\/sklearn-eda-lazypredict-feat-engineering-ensemble#Data-Creating-(Feature-Engineering))\n    * [Data Converting](https:\/\/www.kaggle.com\/windsuzu\/sklearn-eda-lazypredict-feat-engineering-ensemble#Data-Converting)\n* [Modelling](https:\/\/www.kaggle.com\/windsuzu\/sklearn-eda-lazypredict-feat-engineering-ensemble#Modelling)\n* [Lazypredict](https:\/\/www.kaggle.com\/windsuzu\/sklearn-eda-lazypredict-feat-engineering-ensemble#Lazypredict)\n* [Single Model](https:\/\/www.kaggle.com\/windsuzu\/sklearn-eda-lazypredict-feat-engineering-ensemble#Single-Model)\n    * [Feature Selection](https:\/\/www.kaggle.com\/windsuzu\/sklearn-eda-lazypredict-feat-engineering-ensemble#Feature-Selection)\n    * [Model Selection (tuning)](https:\/\/www.kaggle.com\/windsuzu\/sklearn-eda-lazypredict-feat-engineering-ensemble#Model-Selection-(tuning))\n    * [Prediction \ud83c\udf89](https:\/\/www.kaggle.com\/windsuzu\/sklearn-eda-lazypredict-feat-engineering-ensemble#Prediction-%F0%9F%8E%89)\n* [Ensemble](https:\/\/www.kaggle.com\/windsuzu\/sklearn-eda-lazypredict-feat-engineering-ensemble#Ensemble)\n    * [Model Selection (tuning)](https:\/\/www.kaggle.com\/windsuzu\/sklearn-eda-lazypredict-feat-engineering-ensemble#Model-Selection-(tuning))\n    * [Voting Ensembling](https:\/\/www.kaggle.com\/windsuzu\/sklearn-eda-lazypredict-feat-engineering-ensemble#Voting-Ensembling)\n    * [Prediction \ud83c\udf89](https:\/\/www.kaggle.com\/windsuzu\/sklearn-eda-lazypredict-feat-engineering-ensemble#Prediction-%F0%9F%8E%89)","f71bce55":"## Model Selection (tuning)","21ea7093":"# Single Model","83c776e7":"## Prediction \ud83c\udf89","ca2396db":"## Model Selection (tuning)","12d5f0ab":"# Ensemble","fc8787a3":"# ML Approach (Sklearn + Pandas + Sweetviz + LazyPredict + Feature Engineering + Feature Selection + Model Selection + Model Ensembling)\n\nIn this notebook, I use `sklearn` as my machine learning framework; `sweetviz` for the exploratory data analysis (EDA); `pandas` for **data cleaning, data correcting, data engineering, data labeling**; `lazypredict` for the overview of machine learning model performance. I also use `sklearn` to implement **feature selection, model selection, and voting ensemble**. \n\nWe will go through the following process in this notebook: \n\n1. Loading data\n2. Using `sweetviz` to check whether the data missing or imbalance \n3. Implementing data cleaning, correcting, completing, and creating (also known as `feature engineering`) \n\nBy this stage, we will get the **features** and **labels** for machine learning training.\n\n4. Using `lazypredict` to find the best models for this problem\n5. Select a model that performs best to do the following implementation:\n    1. In `feature selection`, we will eliminate the redundant or less useful features\n    2. In `model selection`, we will use grid search to find the best hyperparameters\n6. Prediction\n\nBy this stage, we will have chosen the best model and implement both feature and model selection by eliminating some features and find the most suitable hyperparameters of the model; We then use the trained model to predict the testset.\n\n7. Using `lazypredict` to select multiple models that perform well and ensemble them using `VotingClassifier`\n    1. We skip the `feature selection` but implement `model selection` before we ensemble the models\n8. Prediction\n\n> Ref: https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\/notebook","f377381d":"## Feature Selection","97d3064f":"# Loading Data"}}