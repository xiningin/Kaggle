{"cell_type":{"e26def25":"code","0753e03b":"code","82cef41f":"code","41b49f41":"code","3221df8d":"code","32fa62f1":"code","0328c0ee":"code","2e6f2087":"code","00e50a5d":"code","6e9bade6":"code","41d80c92":"code","62686582":"code","d548ac7d":"code","4e797cbe":"code","c80fbb2c":"code","00af7085":"code","b78b913e":"code","78f61992":"code","649e3999":"code","ac12c915":"code","8c1ab551":"code","a24da77d":"code","67538380":"code","701aa5e1":"code","7553cbe9":"code","e88cfd94":"code","364d92f0":"code","3ea6d91e":"code","cf4e84d0":"code","22a10172":"code","b062f64d":"code","1cc95be2":"code","ee5b47cf":"code","1236e352":"code","620463e9":"code","8f50ad0c":"code","ac5c536d":"code","b54347e4":"code","ba278dfd":"code","06f2b86e":"code","929e3036":"code","bec0d47e":"code","221598d7":"code","d241602e":"code","d55468ff":"code","9c2130c3":"code","4c639a64":"code","c2a83b38":"code","8b07f60c":"code","85735c0a":"code","473782b2":"code","2db18d8d":"code","128dd27a":"code","0f0b8271":"code","4f201d4f":"code","71f309c7":"code","f918f47b":"code","cd124386":"code","61966b01":"code","7c3b3e5d":"code","3538eea6":"code","a589e7d8":"code","0f7cd202":"code","1c157a8a":"code","27e2bca9":"code","15bfa8ea":"code","fddc7611":"code","8c79f643":"code","eb61b481":"code","7fd31d74":"code","35410dc0":"code","a3f0c920":"code","6db05f69":"code","dcb17c63":"code","713ab969":"code","75612909":"code","4826dd93":"code","c002b1aa":"code","7e00fda7":"code","0005bc1e":"code","b5a00551":"code","e5ef7aac":"code","13705d94":"markdown","c8b9d988":"markdown","78b00371":"markdown","b1301c08":"markdown","6788f690":"markdown","0d9ea047":"markdown","bc7303af":"markdown","43fdef9a":"markdown","4ca35c45":"markdown","b37e527c":"markdown","9349c0ca":"markdown","b8ac3803":"markdown","2ac8899b":"markdown","b8a0c81f":"markdown","38e28669":"markdown","e8b6b77a":"markdown","bbb80c44":"markdown"},"source":{"e26def25":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0753e03b":"#make compatible with Python 2 and Python 3\nfrom __future__ import print_function, division, absolute_import \n\n# Remove warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline","82cef41f":"#import packages\n\nimport bs4 as bs\nimport nltk\nnltk.download('stopwords')\n\nimport re\nfrom nltk.tokenize import sent_tokenize # tokenizes sentences\nfrom nltk.stem import PorterStemmer\nfrom nltk.tag import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.util import ngrams\n\neng_stopwords = stopwords.words('english')","41b49f41":"merge = pd.read_csv(\"\/kaggle\/input\/live-score\/242project_merge_finalversion_5_13.csv\")\nmerge","3221df8d":"df_raw = pd.read_csv(\"\/kaggle\/input\/live-score\/Compiled_Yelp_Scraped_Reviews_Komplete_5_13.csv\")\n\n# Clean the dataframe by getting rid of non-essential columns \ndf = df_raw.drop(columns = ['0','alias', 'address2', 'formatted_address'], axis = 1)\n\n# Print data shape\nprint(\"Review data shape is {}\".format(df.shape))\n\n#Preview data\ndf.head()","32fa62f1":"live_raw = pd.read_csv(\"\/kaggle\/input\/live-score\/Restaurant_Scores_-_LIVES_Standard.csv\")\n# Use the necessary columns:\nnecessary_list = ['business_id','business_name','business_address','inspection_id','inspection_date','inspection_score','inspection_type','violation_id','violation_description','risk_category']\nlive = live_raw.loc[:,necessary_list]\n#Drop the missing value for inspection_score:\nlive = live.dropna()\nlive.head()","0328c0ee":"def review_cleaner(reviews, lemmatize=True, stem=True):\n    '''\n    Clean and preprocess a review.\n\n    1. Remove HTML tags\n    2. Use regex to remove all special characters (only keep letters)\n    3. Make strings to lower case and tokenize \/ word split reviews\n    4. Remove English stopwords\n    5. Rejoin to one string\n    '''\n    ps = PorterStemmer()\n    wnl = WordNetLemmatizer()\n    cleaned_reviews=[]\n    \n    for i, review in enumerate(df['text']):\n        # print progress\n        if((i+1)%500 == 0):\n            print(\"Done with %d reviews\" %(i+1))\n            \n        #1. Remove HTML tags\n        review = bs.BeautifulSoup(review).text\n\n        #2. Use regex to find emojis\n        emojis = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', review)\n\n        #3. Remove punctuation\n        review = re.sub(\"[^a-zA-Z]\", \" \",review)\n\n        #4. Tokenize into words (all lower case)\n        review = review.lower().split()\n\n        #5. Remove stopwords\n        eng_stopwords = set(stopwords.words(\"english\"))\n            \n        clean_review=[]\n        for word in review:\n            if word not in eng_stopwords:\n                if lemmatize is True:\n                    word=wnl.lemmatize(word)\n                elif stem is True:\n                    if word == 'oed':\n                        continue\n                    word=ps.stem(word)\n                clean_review.append(word)\n\n        #6. Join the review to one sentence\n        \n        review_processed = ' '.join(clean_review+emojis)\n        cleaned_reviews.append(review_processed)\n    \n\n    return(cleaned_reviews)","2e6f2087":"original_clean_reviews=review_cleaner(df['text'], lemmatize = True, stem = True)\n# for j in range(len(original_clean_reviews)):\n#     print(j)\n#     #original_clean_reviews[j]","00e50a5d":"## Add a clean review column\ndf['clean_review'] = original_clean_reviews\ndf['clean_review'][0]","6e9bade6":"df","41d80c92":"df.to_csv(\"After_Clean_Reviews_5_13.csv\")","62686582":"merge.head()","d548ac7d":"grouped_df = df.groupby(['id'])['clean_review'].apply(lambda x: ' '.join(x)).reset_index()\n\nunique_df = df[['id','name','avg_rating','total_review_count']].drop_duplicates()\n\nnew_yelp = pd.merge(grouped_df,unique_df,how='left',on='id')","4e797cbe":"new_yelp.head(40)","c80fbb2c":"new_yelp = new_yelp.drop(list(range(0,37)))","00af7085":"new_yelp = new_yelp.reset_index()","b78b913e":"new_yelp","78f61992":"new_yelp = new_yelp.drop(columns=['index'])","649e3999":"new_merge = merge[['inspection_score','inspection_result','id_yelp']]","ac12c915":"new_review = pd.merge(new_yelp,new_merge,how='inner',left_on='id',right_on='id_yelp')","8c1ab551":"new_review","a24da77d":"new_review = new_review.drop(columns=['id_yelp'])","67538380":"new_review.to_csv('Merged_Cleaned_Reviews_5_13_2155_businesses.csv')","701aa5e1":"reviews = pd.read_csv(\"\/kaggle\/input\/live-score\/Merged_Cleaned_Reviews_5_13_2155_businesses.csv\")\nindividual_reviews = pd.read_csv(\"\/kaggle\/input\/live-score\/After_Clean_Reviews_5_13.csv\")","7553cbe9":"# # Before Joining\n# df.head()","e88cfd94":"reviews['clean_review_sep'] = reviews['clean_review'].apply(lambda x: x.split())","364d92f0":"reviews.head()","3ea6d91e":"threshold = 86\nreviews['inspection_result'] = reviews['inspection_score'].apply(lambda x: 1 if x>=threshold else 0)","cf4e84d0":"print(\"There is {} 0\".format(len(reviews[reviews['inspection_result']==0])))\nprint(\"There is {} 1\".format(len(reviews[reviews['inspection_result']==1])))\nlabel_0 = len(reviews[reviews['inspection_result']==0])\nlabel_1 = len(reviews[reviews['inspection_result']==1])\nbase_accuracy = label_1\/(label_0+label_1)\nprint(\"Baseline Accuarcy is {}\".format(base_accuracy))","22a10172":"from collections import defaultdict\nword_freq = defaultdict(int)\nfor sent in reviews['clean_review_sep']:\n    for i in sent:\n        word_freq[i] += 1\nlen(word_freq)","b062f64d":"sorted(word_freq, key=word_freq.get, reverse=True)[:120]","1cc95be2":"#pip install gensim","ee5b47cf":"sentences = reviews['clean_review_sep']\n# Set values for various parameters\nnum_features = 100    # Word vector dimensionality                      \nmin_word_count = 40   # ignore all words with total frequency lower than this                       \nnum_workers = 4       # Number of threads to run in parallel\ncontext = 10          # Context window size                                                                                    \n\n\n# Initialize and train the model (this will take some time)\nfrom gensim.models import word2vec\n\nprint(\"Training word2vec model... \")\nmodel = word2vec.Word2Vec(sentences,\n                          workers=num_workers,\n                          size=num_features,\n                          min_count=min_word_count,\n                          window=context)\nprint(\"Training finished!\")\n\n# save the model for later use. You can load it later using Word2Vec.load()\nmodel_name = \"100features_40minwords_10context\"\nmodel.save(model_name)","1236e352":"# Get vocabulary count of the model\nvocab_tmp = list(model.wv.vocab)\nprint('Vocab length:',len(vocab_tmp))","620463e9":"# Get Vocabulary words\nvocab_tmp","8f50ad0c":"# Get the Word embedding \nmodel['dirty']","ac5c536d":"# Get cosine similarity of words\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nmodel.similarity('dish', 'plate')","b54347e4":"model.similarity('gross', 'dirty')","ba278dfd":"# model.similarity('rat', 'infested') # etc ","06f2b86e":"model.most_similar(positive=['good', 'great'], negative=['bad'])","929e3036":"model.most_similar(\"health\")","bec0d47e":"model.most_similar(\"awful\")","221598d7":"from gensim.models import Word2Vec\n# Load the trained modelNumeric Representations of Words\nmodel = Word2Vec.load(\"100features_40minwords_10context\")","d241602e":"model.wv.syn0","d55468ff":"model.wv.syn0.shape","9c2130c3":"model.corpus_count","4c639a64":"# Get vocabulary count of the model\nvocab_tmp = list(model.wv.vocab)\nprint('Vocab length:',len(vocab_tmp))","c2a83b38":"# Get distributional representation of each word\nX = model[vocab_tmp]","8b07f60c":"X.shape","85735c0a":"from sklearn import decomposition\n# get two principle components of the feature space\npca = decomposition.PCA(n_components=2).fit_transform(X)","473782b2":"pca","2db18d8d":"pca.shape","128dd27a":"# set figure settings\nplt.figure(figsize=(20,20))\n\n# save pca values and vocab in dataframe df\ndf = pd.concat([pd.DataFrame(pca),pd.Series(vocab_tmp)],axis=1)\ndf.columns = ['x', 'y', 'word']\n\nplt.xlabel(\"1st principal component\", fontsize=14)\nplt.ylabel('2nd principal component', fontsize=14)\n\nplt.scatter(x=df['x'], y=df['y'],s=3)\nfor i, word in enumerate(df['word'][0:100]):\n    plt.annotate(word, (df['x'].iloc[i], df['y'].iloc[i]))\nplt.title(\"PCA Embedding\", fontsize=18)\nplt.show()","0f0b8271":"## A popular non-linear dimensionality reduction technique that preserves greatly the local \n## and global structure of the data. Essentially tries to reconstruct the subspace in which the \n## data exists\n\nfrom sklearn import manifold\ntsne = manifold.TSNE(n_components=2)\nX_tsne = tsne.fit_transform(X)","4f201d4f":"# set figure settings\nplt.figure(figsize=(10,10), dpi=100)\n\n# save pca values and vocab in dataframe df\ndf2 = pd.concat([pd.DataFrame(X_tsne),pd.Series(vocab_tmp)], axis=1)\ndf2.columns = ['x', 'y', 'word']\n\nplt.xlabel(\"1st principal component\", fontsize=14)\nplt.ylabel('2nd principal component', fontsize=14)\n\nplt.scatter(df2['x'], y=df2['y'], s=3)\nfor i, word in enumerate(df2['word'][0:100]):\n    plt.annotate(word, (df2['x'].iloc[i], df2['y'].iloc[i]))\nplt.title(\"tSNE Embedding\", fontsize=18)\nplt.show()","71f309c7":"def makeFeatureVec(review, model):\n    # Function to average all of the word vectors in a given paragraph\n    featureVec =[]\n    \n    # Index2word is a list that contains the names of the words in \n    # the model's vocabulary. Convert it to a set, for speed \n    index2word_set = set(model.wv.index2word)\n    \n    # Loop over each word in the review and, if it is in the model's\n    # vocaublary, add its feature vector to the total\n    for n,word in enumerate(review):\n        if word in index2word_set: \n            featureVec.append(model[word])\n            \n    # Average the word vectors for a \n    featureVec = np.mean(featureVec,axis=0)\n    return featureVec\n\n\ndef getAvgFeatureVecs(reviews, model):\n    # Given a set of reviews (each one a list of words), calculate \n    # the average feature vector for each one \n    \n    reviewFeatureVecs = []\n    # Loop through the reviews\n    for counter,review in enumerate(reviews):\n        \n        # Print a status message every 5000th review\n        if counter%5000. == 0.:\n            print(\"Review %d of %d\" % (counter, len(reviews)))\n\n        # Call the function (defined above) that makes average feature vectors\n        vector= makeFeatureVec(review, model)\n        reviewFeatureVecs.append(vector)\n            \n    return reviewFeatureVecs","f918f47b":"from sklearn.ensemble import RandomForestClassifier\n# # CountVectorizer can actucally handle a lot of the preprocessing for us\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import metrics # for confusion matrix, accuracy score etc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\n\n\nnp.random.seed(0)\n\n\ndef train_sentiment(cleaned_reviews, y,max_features=1000):\n    '''This function will:\n    y=reviews[\"label\"]\n    \n    1. Convert reviews into feature vectors using word2vec.\n    2. split data into train and test set.\n    3. train a random forest model using train n-gram counts and y (labels)\n    4. test the model on your test split\n    5. print accuracy of sentiment prediction on test and training data\n    6. print confusion matrix on test data results\n\n    To change n-gram type, set value of ngram argument\n    To change the number of features you want the countvectorizer to generate, set the value of max_features argument'''\n\n    print(\"1.Creating Feature vectors using word2vec...\\n\")\n\n    trainDataVecs = getAvgFeatureVecs(cleaned_reviews, model)\n    \n   \n    print(\"\\n2.Splitting dataset into train and test sets...\\n\")\n    X_train, X_test, y_train, y_test = train_test_split(\\\n    trainDataVecs, y, random_state=0, test_size=.2)\n\n   \n    print(\"3. Training the random forest classifier...\\n\")\n    \n    # Initialize a Random Forest classifier with 75 trees\n    forest = RandomForestClassifier(n_estimators = 100) \n    \n    # Fit the forest to the training set, word2vecfeatures \n    # and the sentiment labels as the target variable\n    forest = forest.fit(X_train, y_train)\n\n\n    train_predictions = forest.predict(X_train)\n    test_predictions = forest.predict(X_test)\n    \n    train_acc = metrics.accuracy_score(y_train, train_predictions)\n    valid_acc = metrics.accuracy_score(y_test, test_predictions)\n    print(\"=================Training Statistics======================\\n\")\n    print(\"The training accuracy is: \", train_acc)\n    print(\"The validation accuracy is: \", valid_acc)\n    print()\n    print('CONFUSION MATRIX:')\n    print('         Predicted')\n    print('          neg pos')\n    print(' Actual')\n    c=confusion_matrix(y_test, test_predictions)\n    print('  neg  ',c[0])\n    print('  pos  ',c[1])","cd124386":"train_sentiment(cleaned_reviews = sentences, y=reviews[\"inspection_result\"],max_features=1000)","61966b01":"AvgFeatureVecs = getAvgFeatureVecs(reviews=sentences, model=model)","7c3b3e5d":"AvgFeatureVecs_np = np.array(AvgFeatureVecs)","3538eea6":"AvgFeatureVecs_np.shape","a589e7d8":"from sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef rfr_model(X, y):\n# Perform Grid-Search\n    gsc = GridSearchCV(\n        estimator=RandomForestRegressor(),\n        param_grid={\n            'max_depth': range(3,7),\n            'n_estimators': (50, 100, 200),\n        },\n        cv=5, scoring='neg_mean_squared_error', verbose=5, n_jobs=-1)\n    \n    grid_result = gsc.fit(X, y)\n    best_params = grid_result.best_params_\n    \n    rfr = RandomForestRegressor(max_depth=best_params[\"max_depth\"], n_estimators=best_params[\"n_estimators\"],                               random_state=False, verbose=False)\n# Perform K-Fold CV\n    scores = cross_val_score(rfr, X, y, cv=5, scoring='neg_root_mean_squared_error')\n\n    return scores,best_params","0f7cd202":"scores, best = rfr_model(X=AvgFeatureVecs_np,y=reviews[\"inspection_score\"])","1c157a8a":"scores","27e2bca9":"import seaborn as sns\nsns.swarmplot(reviews[\"inspection_score\"])","15bfa8ea":"best","fddc7611":"from sklearn.ensemble import RandomForestClassifier\n# CountVectorizer can actucally handle a lot of the preprocessing for us\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn import metrics # for confusion matrix, accuracy score etc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_recall_curve\n\nfrom sklearn.model_selection import KFold, cross_val_score\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nimport xgboost as xgb","8c79f643":"from sklearn.ensemble import RandomForestClassifier\n# CountVectorizer can actucally handle a lot of the preprocessing for us\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import metrics # for confusion matrix, accuracy score etc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix","eb61b481":"count_vect = CountVectorizer(analyzer=\"word\")\nX_count = count_vect.fit_transform(reviews['clean_review'])\nX_features = pd.DataFrame(X_count.toarray())\nX_features.columns = count_vect.get_feature_names()\nprint(X_features.shape)\nX_features.head()","7fd31d74":"X_train, X_test, y_train, y_test = train_test_split(X_features, reviews['inspection_result'], test_size=0.2)\n\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","35410dc0":"logreg = LogisticRegression()                           # instantiate\nlogreg.fit(X_train, y_train)                            # fit\nY_pred = logreg.predict(X_test)                         # predict\nacc_logreg = logreg.score(X_test, y_test)                # evaluate\n\nprint('Logistic Regression labeling accuracy:', str(round(acc_logreg*100,2)),'%')","a3f0c920":"knn = KNeighborsClassifier(n_neighbors = 3)                  # instantiate\nknn.fit(X_train, y_train)                                    # fit\nacc_knn = knn.score(X_test, y_test)                          # predict + evaluate\n\nprint('K-Nearest Neighbors labeling accuracy:', str(round(acc_knn*100,2)),'%')","6db05f69":"rf = RandomForestClassifier(n_estimators=50, max_depth=10, n_jobs=-1) # instantiate\nrf_model = rf.fit(X_train, y_train) # fit\naccuracy_rf = rf_model.score(X_test, y_test) # predict + evaluate\n\nprint('Random Forest Regression labeling accuracy:', str(round(accuracy_rf*100,2)),'%')\nsorted(zip(rf_model.feature_importances_, X_train.columns), reverse=True)[0:10]","dcb17c63":"# Look at importnace of features for random forest\n\ndef plot_model_var_imp(model , X , y):\n    imp = pd.DataFrame( \n        model.feature_importances_  , \n        columns = [ 'Importance' ] , \n        index = X.columns \n    )\n    imp = imp.sort_values( [ 'Importance' ] , ascending = True )\n    imp[ : 10 ].plot( kind = 'barh' )\n    print ('Training accuracy Random Forest:',model.score( X , y ))\n\nplot_model_var_imp(rf, X_train, y_train)","713ab969":"# XGBoost, same API as scikit-learn\ngradboost = xgb.XGBClassifier(n_estimators=1000)             # instantiate\ngradboost.fit(X_train, y_train)                              # fit\naccuracy_xgboost = gradboost.score(X_test, y_test)           # predict + evalute\n\nprint('XGBoost labeling accuracy:', str(round(acc_xgboost*100,2)),'%')","75612909":"## ALEX'S CHUNK \n\nX_train, X_test, y_train, y_test = train_test_split(X_features, reviews['label'], test_size=0.2)\n\nrf = RandomForestClassifier(n_estimators=50, max_depth=10, n_jobs=-1)\nrf_model = rf.fit(X_train, y_train)\n\nsorted(zip(rf_model.feature_importances_, X_train.columns), reverse=True)[0:10]","4826dd93":"from sklearn.metrics import precision_recall_fscore_support as score","c002b1aa":"y_pred = rf_model.predict(X_test)\nprecision, recall, fscore, support = score(y_test, y_pred, pos_label=1, average='binary')\n\nprint('Precision: {} \/ Recall: {} \/ Accuracy: {}'.format(round(precision, 3),\n                                                        round(recall, 3),\n                                                        round((y_pred==y_test).sum() \/ len(y_pred),3)))","7e00fda7":"y_train1=y_train\nX_test1=X_test\ny_test1 =y_test\nX_train1 = X_train\n\n#Import svm model\nfrom sklearn import svm\n\n#Create a svm Classifier\nclf = svm.SVC(kernel='rbf', gamma = 'scale') # rbf Kernel\n\n#Train the model using the training sets\nclf.fit(X_train1, y_train1)\n\n#Predict the response for test dataset\ny_pred = clf.predict(X_test1)\n\n#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n\n# Model Accuracy: how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test1, y_pred))\n\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test1, y_pred)","0005bc1e":"from sklearn.ensemble import RandomForestClassifier\n# CountVectorizer can actucally handle a lot of the preprocessing for us\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import metrics # for confusion matrix, accuracy score etc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\n\n\nnp.random.seed(100)\n\n\ndef train_predict_sentiment(cleaned_reviews, y=df[\"...\"], ngram=1, max_features=1000):\n    '''This function will:\n    1. split data into train and test set.\n    2. get n-gram counts from cleaned reviews \n    3. train a random forest model using train n-gram counts and y (labels)\n    4. test the model on your test split\n    5. print accuracy of sentiment prediction on test and training data\n    6. print confusion matrix on test data results\n\n    To change n-gram type, set value of ngram argument\n    To change the number of features you want the countvectorizer to generate, set the value of max_features argument'''\n\n    print(\"Creating the bag of words model!\\n\")\n    # CountVectorizer is scikit-learn's bag of words tool, here we show more keywords \n    vectorizer = CountVectorizer(ngram_range=(1, ngram), analyzer = \"word\",\n                                 tokenizer = None,\n                                 preprocessor = None,\n                                 stop_words = None,\n                                 max_features = max_features) \n    \n    X_train, X_test, y_train, y_test = train_test_split(cleaned_reviews, y, random_state=0, test_size=.2)\n\n    # Then we use fit_transform() to fit the model \/ learn the vocabulary,\n    # then transform the data into feature vectors.\n    # The input should be a list of strings. .toarraty() converts to a numpy array\n    \n    train_bag = vectorizer.fit_transform(X_train).toarray()\n    test_bag = vectorizer.transform(X_test).toarray()\n    # print('TOP 20 FEATURES ARE: ',(vectorizer.get_feature_names()[:20]))\n\n    print(\"Training the random forest classifier!\\n\")\n    # Initialize a Random Forest classifier with 75 trees\n    forest = RandomForestClassifier(n_estimators = 50) \n\n    # Fit the forest to the training set, using the bag of words as \n    # features and the sentiment labels as the target variable\n    forest = forest.fit(train_bag, y_train)\n\n\n    train_predictions = forest.predict(train_bag)\n    test_predictions = forest.predict(test_bag)\n    \n    train_acc = metrics.accuracy_score(y_train, train_predictions)\n    valid_acc = metrics.accuracy_score(y_test, test_predictions)\n    print(\" The training accuracy is: \", train_acc, \"\\n\", \"The validation accuracy is: \", valid_acc)\n    print()\n    print('CONFUSION MATRIX:')\n    print('         Predicted')\n    print('          neg pos')\n    print(' Actual')\n    c=confusion_matrix(y_test, test_predictions)\n    print('  neg  ',c[0])\n    print('  pos  ',c[1])\n\n    #Extract feature importnace\n    print('\\nTOP TEN IMPORTANT FEATURES:')\n    importances = forest.feature_importances_\n    indices = np.argsort(importances)[::-1]\n    top_10 = indices[:20]\n    print([vectorizer.get_feature_names()[ind] for ind in top_10])","b5a00551":"# Clean the reviews in the training set 'train' using review_cleaner function defined above\n# Here we use the original reviews without lemmatizing and stemming\noriginal_clean_reviews = review_cleaner(df['text'],lemmatize=False,stem=False)","e5ef7aac":"train_predict_sentiment(cleaned_reviews = original_clean_reviews, y = df[\"...\"], ngram=1, max_features=1000)","13705d94":"### 3.3 Word Count Classification","c8b9d988":"Srijit's Block","78b00371":"## Build A Model","b1301c08":"### 2.1 Text Cleaning","6788f690":"## 4. Train and Test Model on the Restaurant data","0d9ea047":"##  3. Train and Validate a Sentiment Analysis ","bc7303af":"##  2. Data Cleaning: Prepare the Dataset for Classification \n","43fdef9a":"We'll create a function called `review_cleaner` that reads in a review and:\n\n- Removes HTML tags (using beautifulsoup)\n- Extract emojis (emotion symbols, aka :D )\n- Removes non-letters (using regular expression)\n- Converts all words to lowercase letters and tokenizes them (using .split() method on the review strings, so that every word in the review is an element in a list)\n- Removes all the English stopwords from the list of movie review words\n- Join the words back into one string seperated by space, append the emoticons to the end\n\n**NOTE: Transform the list of stopwords to a set before removing the stopwords. I.e. assign `eng_stopwords = set(stopwords.words(\"english\"))`. Use the set to look up stopwords. This will speed up the computations A LOT (Python is much quicker when searching a set than a list).**","4ca35c45":"## Please from here to Load Data : \n1. Merged_Cleaned_Reviews_5_13_2155_Businesses.csv\n2. After_Clean_Review_5_13","b37e527c":"## 2.3 Word Embedding + PCA intro\n#### Test Trial Need Help Figuring Out Vocab","9349c0ca":"### Look at Word Freq","b8ac3803":"## 1. Load the Data \n","2ac8899b":"### 3.2 Word Embedding Random Forest Regressor","b8a0c81f":"### 3.1 Word Embedding Random Forest Classification","38e28669":"### 2.2 Join Data\n#### Test how many business name overlap","e8b6b77a":"Let's test the threshold of 86","bbb80c44":"### Kiki's block"}}