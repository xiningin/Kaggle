{"cell_type":{"1373d476":"code","478b81f1":"code","9a619eaa":"code","a82391e5":"code","d85a2d34":"code","e9e2533b":"code","d3383d62":"code","abbad502":"code","65ef7ea9":"code","d6fb2557":"markdown","ed99b030":"markdown","d89c2619":"markdown"},"source":{"1373d476":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","478b81f1":"# Several preprocessing classes and functions that I shall later use. They are all inspired by the Hands-on machine learning book - by ageron\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler,LabelBinarizer,LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom scipy.sparse import lil_matrix,csr_matrix\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom scipy import stats as ss\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import ClassifierMixin\n\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attr_names):\n        self.attribute_names=attr_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names].values\n\nclass MyLabelFillNA(TransformerMixin):\n    def __init__(self,fill_with=\"unknown\", *args, **kwargs):\n        self.fill_with = fill_with\n    def fit(self, x,y=0):\n        return self\n    def transform(self, x, y=0):\n        retval=None\n        if isinstance(x,pd.DataFrame):\n            retval = x.fillna(self.fill_with)\n        elif isinstance(x, np.ndarray):\n            retval = pd.DataFrame(x).fillna(self.fill_with)\n        else:\n            raise Exception(\"input arg needs to be pandas DataFrame or numpy array\")\n        return retval.values\n\nclass MyLabelEncoder(TransformerMixin):\n    def __init__(self, *args, **kwargs):\n        self.encoder = LabelEncoder(*args, **kwargs)\n    def fit(self, x, y=0):\n        self.encoder.fit(x)\n        return self\n    def transform(self, x, y=0):\n        return self.encoder.transform(x)\n\nclass MyMultiLabelEncoder(TransformerMixin):\n    def __init__(self, label_encoder_args_array=None ):\n        def f( i):\n            if label_encoder_args_array==None or label_encoder_args_array[i] ==None: return MyLabelEncoder()\n            else: return MyLabelBinarizer(*label_encoder_args_array[i])\n        self.label_encoder_args_array= label_encoder_args_array\n        self.encoders=None\n        if label_encoder_args_array is not  None:\n            self.encoders = [f(i) for i in range(len(label_encoder_args_array))]\n            \n    def fit(self,x,y=0):\n        xt = x.transpose()\n        if self.encoders==None:\n            self.encoders = [MyLabelEncoder() for i in range(len(xt))]\n        print(xt.shape,len(xt),len(self.encoders))\n        for i in range(len(xt)):\n            arr=xt[i]\n            enc=self.encoders[i]\n            #y=arr.reshape(-1,1)\n            enc.fit(arr)\n        return self\n    \n    def transform(self,x,y=0):\n        xx=None\n        xt=x.transpose()\n        for i in range(len(xt)):\n            enc = self.encoders[i]\n            arr= xt[i]\n            #y=arr.reshape(-1,1)\n            z=enc.transform(arr).reshape(-1,1)\n            if i==0:\n                xx=z\n            else:\n                xx=np.concatenate((xx,z),axis=1)\n        print('xx shape is',xx.shape)\n        return lil_matrix(xx)\n        \nclass MyLabelBinarizer(TransformerMixin):\n    def __init__(self, *args, **kwargs):\n        self.encoder = LabelBinarizer(*args, **kwargs)\n    def fit(self, x, y=0):\n        self.encoder.fit(x)\n        return self\n    def transform(self, x, y=0):\n        return self.encoder.transform(x)\n\nclass MyMultiLabelBinarizer(TransformerMixin):\n    \n    def __init__(self, binarizer_args_array=None ):\n        def f( i):\n            if binarizer_args_array==None or binarizer_args_array[i] ==None: return MyLabelBinarizer()\n            else: return MyLabelBinarizer(*binarizer_args_array[i])\n        self.binarizer_args_array= binarizer_args_array\n        self.encoders=None\n        if binarizer_args_array is not  None:\n            self.encoders = [f(i) for i in range(len(binarizer_args_array))]\n    def fit(self,x,y=0):\n        xt = x.transpose()\n        if self.encoders==None:\n            self.encoders = [MyLabelBinarizer() for i in range(len(xt))]\n        print(xt.shape,len(xt),len(self.encoders))\n        for i in range(len(xt)):\n            arr=xt[i]\n            enc=self.encoders[i]\n            y=arr.reshape(-1,1)\n            enc.fit(y)\n        return self\n    \n    def transform(self,x,y=0):\n        xx=None\n        xt=x.transpose()\n        for i in range(len(xt)):\n            enc = self.encoders[i]\n            arr= xt[i]\n            y=arr.reshape(-1,1)\n            z=enc.transform(y)\n            if i==0:\n                xx=z\n            else:\n                xx=np.concatenate((xx,z),axis=1)\n        print('xx shape is',xx.shape)\n        return lil_matrix(xx)\n        \nclass FullPipeline:\n\n    def full_pipeline_apply_features(self,data, non_num_attrs=None, num_attrs=None):\n        num_pipeline=None\n        full_pipeline=None\n        if num_attrs != None:\n            num_pipeline = Pipeline([('num_selector', DataFrameSelector(num_attrs)),('imputer',SimpleImputer(strategy='median')), ('std_scaler',StandardScaler() )])\n            full_pipeline= num_pipeline\n            print('numattrs is not None')\n\n        cat_pipeline=None\n        if non_num_attrs != None:\n            cat_pipeline = Pipeline([\n                ('selector', DataFrameSelector(non_num_attrs)),\n                ('na_filler', MyLabelFillNA(\"Unknown\")),\n                ('label_encoder', MyMultiLabelBinarizer())\n            ])\n            full_pipeline=cat_pipeline\n\n\n        #num_pipeline.fit_transform(data)\n        #cat_pipeline.fit_transform(data)\n        #MyLabelBinarizer().fit_transform(selected_data)\n        if num_pipeline != None and cat_pipeline != None:\n            print('Both num_pipeline and cat_pipeline exist')\n            full_pipeline = FeatureUnion(transformer_list=[\n            (\"num_pipeline\", num_pipeline),\n            (\"cat_pipeline\", cat_pipeline),\n            ])\n        if full_pipeline != None:\n            self.full_features_pipeline_=full_pipeline\n            return full_pipeline.fit_transform(data)\n        return None\n\n    def full_pipeline_apply_labels(self,data, label_data_non_num):\n        label_binarized_pipeline = Pipeline([('selector', DataFrameSelector(list(label_data_non_num))),\n        ('na_filler', MyLabelFillNA(\"Unknown\")),\n        ('label_encoder', MyLabelBinarizer())])\n        label_binarized_data_prepared = label_binarized_pipeline.fit_transform(data)\n        self.label_pipeline_ = label_binarized_pipeline\n        return label_binarized_data_prepared\n    \ndef cramers_v(x, y):\n    confusion_matrix = pd.crosstab(x,y)\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2\/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))\/(n-1))\n    rcorr = r-((r-1)**2)\/(n-1)\n    kcorr = k-((k-1)**2)\/(n-1)\n    return np.sqrt(phi2corr\/min((kcorr-1),(rcorr-1)))\n\ndef conditional_probabilities(data,xattr,yattr):\n    d=data[[xattr,yattr]]\n    dg=d.groupby(yattr)\n    return dg.value_counts()\/dg.count()\n\ndef plot_precision_recall_vs_threshold(precisions, recalls,thresholds):\n    plt.plot(thresholds, precisions[:-1],\"b--\",label=\"Precision\")\n    plt.plot(thresholds,recalls[:-1], \"g-\",label=\"Recall\")\n    plt.legend(loc=\"upper left\")\n    plt.ylim([0,1])\n    \ndef plot_roc_curve(fpr, tpr, label=None):\n    plt.plot(fpr, tpr,linewidth=2, label=label) #tpr is the recall or true positives rate\n    plt.plot([0,1],[0,1],'k--')\n    plt.axis([0,1,0,1])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n  \n\n# Stacking classifier inspired by the Hands-on machine learning book - by ageron\nclass SingleLayerStackingClassifier(ClassifierMixin):\n    def __init__(self, subset_ratio, blender, *estimators):\n        if (blender==None or estimators==None):\n            raise \"Both stacking_estimator and at least one estimator required\"\n        self.estimators=estimators\n        self.blender = blender\n        self.subset_ratio=subset_ratio\n    \n    def fit(self,X,y):\n        X1,X2,y1,y2=train_test_split(X,y,test_size=self.subset_ratio,random_state=42)\n        X_intermediate=pd.DataFrame()\n        for est,i in zip(self.estimators,range(len(self.estimators))):\n            est.fit(X1,y1)\n            X_intermediate[\"estimator_\"+str(i)+\"_prediction\"] = est.predict(X2)\n        self.blender.fit(X=X_intermediate,y=y2)\n    \n    def predict(self,X):\n        X_intermediate=pd.DataFrame()\n        for est,i in zip(self.estimators,range(len(self.estimators))):\n            X_intermediate[\"estimator_\"+str(i)+\"_prediction\"] = est.predict(X)\n        return self.blender.predict(X_intermediate)\n         \n            ","9a619eaa":"data = pd.read_csv(\"..\/input\/train.csv\")\ndata['Age'].fillna(-1,inplace=True)\nfull_pipel = FullPipeline()\ndata_prepared = full_pipel.full_pipeline_apply_features(data,non_num_attrs=[\"Sex\",\"Ticket\",\"Cabin\",\"Embarked\"], num_attrs=[\"Pclass\",\"Age\",\"SibSp\",\"Parch\",\"Fare\"])\nlabels_prepared = full_pipel.full_pipeline_apply_labels(data,label_data_non_num=[\"Survived\"])\nprint('data_prepared.shape',data_prepared.shape)\nprint(\"labels_prepared.shape\",labels_prepared.shape)\n\ndata_train,data_test,label_train,label_test = train_test_split(data_prepared,labels_prepared,test_size=0.2,random_state=42)\nlabel_train = label_train.ravel()\nlabel_test= label_test.ravel()","a82391e5":"#Some visualization\ndata.info()","d85a2d34":"data.corr()  #Strong correlation of Pclass and Fare with Survived","e9e2533b":"data.hist(bins=50,figsize=(20,15))","d3383d62":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import precision_recall_curve, roc_curve, roc_auc_score\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\n\nada_classifier = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2),learning_rate=0.5,n_estimators=150,random_state=49,algorithm=\"SAMME.R\")\nada_classifier.fit(data_train,label_train)\nerrors = [mean_squared_error(label_test, y_pred) for y_pred in ada_classifier.staged_predict(data_test)]\n\nbst_estimator = np.argmin(errors) +1\nprint('best n_estimators',bst_estimator)\nada_best = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2),learning_rate=0.5,n_estimators=bst_estimator,random_state=49,algorithm=\"SAMME.R\")\nada_best.fit(data_train,label_train.ravel())\nprint('score ',ada_best.score(data_test,label_test))\nprint('cross_val_score',cross_val_score(ada_best, data_prepared, labels_prepared.ravel(),cv=10))\npred_proba=list(ada_best.staged_predict_proba(data_test))[-1][:,1]\nprecisions,recalls,thresholds = precision_recall_curve(probas_pred=pred_proba, y_true=label_test)\nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\nfpr,tpr,thresholds = roc_curve(label_test, pred_proba)\nplt.figure()\nplot_roc_curve(fpr,tpr)\nprint('roc_auc_score',roc_auc_score(label_test, pred_proba))","abbad502":"# Unlike earlier we train on the entire train.csv\n\nada_classifier = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2),learning_rate=0.5,n_estimators=150,random_state=49,algorithm=\"SAMME.R\")\nada_classifier.fit(data_train,label_train)\nerrors = [mean_squared_error(label_test, y_pred) for y_pred in ada_classifier.staged_predict(data_test)]\n\nbst_estimator = np.argmin(errors)\nprint(bst_estimator)\nada_best = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2),learning_rate=0.5,n_estimators=bst_estimator,random_state=49,algorithm=\"SAMME.R\")\n# Unlike earlier we train on the entire train.csv\nada_best.fit(data_prepared,labels_prepared.ravel())\nprint(ada_best.score(data_test,label_test)) #should be very high since data_test is part of data_prepared\ncross_val_score(ada_best, data_prepared, labels_prepared.ravel(),cv=10)","65ef7ea9":"testdata = pd.read_csv(\"..\/input\/test.csv\")\ntestdata_prepared = full_pipel.full_features_pipeline_.transform(testdata)\ntestdata['Survived']=ada_best.predict(testdata_prepared)\n#testdata[['PassengerId','Survived']].to_csv(path_or_buf=\"..\/input\/results.csv\",header=True,index=False)\ntestdata[['PassengerId','Survived']].set_index('PassengerId')\n","d6fb2557":"## Checkout the roc and precision recall curves","ed99b030":"# Ok let us take a time off here and look at the data harder","d89c2619":"# Using Adaboost ensemble classifier. First find the best n_estimators by using a large one (150) to start with and then getting the staged_predictions using stage_predict. Select the n_estimators with the least error\n\n### This gave me a score above 80% on the Titanic Leaderboard"}}