{"cell_type":{"4eb6eae5":"code","67f48d48":"code","58ba02d1":"code","06bc38e0":"code","6d08f66c":"code","d7a1a92b":"code","4f42d4ad":"code","56c8a442":"code","4cca7f9c":"code","4a3eaf28":"code","45620ada":"code","260ba220":"code","5a33daf5":"code","39b20390":"code","e39ec3c3":"code","a65a3cca":"code","60ab1eb7":"code","fdf41953":"code","f83152e8":"code","cc9acfdc":"code","f5356a6d":"code","5f1ac5e9":"code","b170e5a0":"code","114518ab":"code","f021858a":"code","b878300c":"code","136f3773":"markdown","4abd5d5a":"markdown","ab24dbe6":"markdown","68d9f970":"markdown","3b5a559e":"markdown","ab722b95":"markdown","7c674555":"markdown","1c3deda9":"markdown","82c9ddc3":"markdown","00791262":"markdown","a501095d":"markdown"},"source":{"4eb6eae5":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_columns', None)","67f48d48":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler,OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","58ba02d1":"# Read the data\nsample_submission_file = pd.read_csv(\"..\/input\/home-data-for-ml-course\/sample_submission.csv\")\ntrain_data = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv', index_col='Id')\nX_test = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv', index_col='Id')\nX = train_data.copy()","06bc38e0":"X.head()","6d08f66c":"plt.figure()\nsns.distplot(X.SalePrice)\nplt.title('Distribution of SalePrice')\nplt.show()","d7a1a92b":"sns.distplot(np.log(X.SalePrice))\nplt.title('Distribution of Log-transformed SalePrice')\nplt.xlabel('log(SalePrice)')\nplt.show()","4f42d4ad":"# Perform log transformation on SalePrice\n\nX['SalePrice'] = np.log(X['SalePrice'])\nX = X.rename(columns={'SalePrice': 'SalePrice_log'})","56c8a442":"# Separate target from predictors\ny = X.SalePrice_log","4cca7f9c":"correlation = X.corr()\n\nf, ax = plt.subplots(figsize=(14,12))\nplt.title('Correlation of numerical attributes', size=16)\nsns.heatmap(correlation)\nplt.show()","4a3eaf28":"# Top correlated attributes to the target SalePrice_log\nround(correlation['SalePrice_log'].sort_values(ascending=False).head(15), 2)","45620ada":"# Remove attributes that were identified for excluding when viewing corr values\nattributes_drop = ['MiscVal', 'MSSubClass', 'MoSold', 'YrSold', \n                   'GarageArea', 'GarageYrBlt']","260ba220":"X.drop(attributes_drop, axis=1, inplace=True)","5a33daf5":"X.drop('SalePrice_log', axis=1, inplace=True)","39b20390":"X.head()","e39ec3c3":"# X and X_test should have the same columns\nX_test.drop(attributes_drop, axis=1, inplace=True)","a65a3cca":"# Numerical and categorical columns shoud be treated separately\nnum_columns = X.select_dtypes(exclude='object').columns\ncat_columns = X.select_dtypes(include='object').columns","60ab1eb7":"# Imputation to null values of these numerical columns need to be 'constant'\nconstant_num_cols = ['MasVnrArea']\n\n# Imputation to null values of these numerical columns need to be 'mean'\nmean_num_cols = list(set(num_columns).difference(set(constant_num_cols)))\n\n# Imputation to null values of these categorical columns need to be 'constant'\nconstant_categorical_cols = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu',\n                     'GarageCond', 'GarageQual', 'GarageFinish', 'GarageType',\n                     'BsmtFinType2', 'BsmtExposure', 'BsmtFinType1', 'BsmtQual', 'BsmtCond',\n                     'MasVnrType']\n\n# Imputation to null values of these categorical columns need to be 'most_frequent'\nmf_categorical_cols = list(set(cat_columns).difference(set(constant_categorical_cols)))\n\nmy_cols = constant_num_cols + mean_num_cols + constant_categorical_cols + mf_categorical_cols","fdf41953":"# Define transformers\n# Preprocessing for numerical data\n\nnumerical_transformer_m = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())])\n\nnumerical_transformer_c = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n    ('scaler', StandardScaler())])\n\n\n# Preprocessing for categorical data for most frequent\ncategorical_transformer_mf = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown = 'ignore', sparse = False))\n])\n\n# Preprocessing for categorical data for constant\ncategorical_transformer_c = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='NA')),\n    ('onehot', OneHotEncoder(handle_unknown = 'ignore', sparse = False))\n])\n\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num_mean', numerical_transformer_m, mean_num_cols),\n        ('num_constant', numerical_transformer_c, constant_num_cols),\n        ('cat_mf', categorical_transformer_mf, mf_categorical_cols),\n        ('cat_c', categorical_transformer_c, constant_categorical_cols)\n    ])","f83152e8":"# Preprocessing of training data\nX_cv = X[my_cols].copy()\nX_sub = X_test[my_cols].copy()","cc9acfdc":"X_cv.head()","f5356a6d":"# Test different models\n\nfrom xgboost import XGBRegressor\n\nxgb_model = XGBRegressor(learning_rate = 0.05,\n                            n_estimators=1000,\n                            max_depth=5,\n                            min_child_weight=2,\n                            gamma=0,\n                            subsample=0.7,\n                            colsample_bytree=0.7,\n                            reg_alpha = 0,\n                            reg_lambda = 1,\n                            random_state=0)\n\n# Create the Pipeline\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline_xgb = Pipeline(steps=[('preprocessor', preprocessor),\n                            ('model', xgb_model)\n                             ])\n\n# Multiply by -1 since sklearn calculates *negative* MAE\nscores = -1 * cross_val_score(my_pipeline_xgb, X_cv, y,\n                              cv=5,\n                              scoring='neg_mean_absolute_error')\n\nprint(\"MAE score:\\n\", scores)\nprint(\"MAE mean: {}\".format(scores.mean()))","5f1ac5e9":"from sklearn.linear_model import Lasso\n\nlasso_model = Lasso(alpha=0.0005, random_state=0)\n\nmy_pipeline_lasso = Pipeline(steps=[('preprocessor', preprocessor),\n                            ('model', lasso_model)\n                             ])\n\nscores = -1 * cross_val_score(my_pipeline_lasso, X_cv, y,\n                              cv=5,\n                              scoring='neg_mean_absolute_error')\n\nprint(\"MAE score:\\n\", scores)\nprint(\"MAE mean: {}\".format(scores.mean()))","b170e5a0":"from sklearn.linear_model import Ridge\n\nridge_model = Ridge(alpha=0.002, random_state=5)\n\nmy_pipeline_ridge = Pipeline(steps=[('preprocessor', preprocessor),\n                            ('model', ridge_model)\n                             ])\n\nscores = -1 * cross_val_score(my_pipeline_ridge, X_cv, y,\n                              cv=5,\n                              scoring='neg_mean_absolute_error')\n\nprint(\"MAE score:\\n\", scores)\nprint(\"MAE mean: {}\".format(scores.mean()))","114518ab":"# Fit the best model\nmy_pipeline_xgb.fit(X_cv, y)\n\n# Get predictions\npreds = my_pipeline_xgb.predict(X_sub)","f021858a":"# target is SalePrice_log, we need to inverse-transform to obtain SalePrice\ndef inv_y(transformed_y):\n    return np.exp(transformed_y)","b878300c":"output = pd.DataFrame({'Id': sample_submission_file.Id,\n                       'SalePrice': inv_y(preds)})\n\noutput.to_csv('submission.csv', index=False)","136f3773":"**XGBRegressor** delivers the lowest mean absolute error, the target is still SalePrice_log and not SalePrice.\n\nWe can play with models and **try different settings**, the alternative to it is of course **GridSearchCV**.","4abd5d5a":"At first we should try to minimise the skew of the dataset. The reason given is that skewed data adversely affects the prediction accuracy of regression models. ","ab24dbe6":"The **log-transformed** SalePrice improves the skew.","68d9f970":"# 3. Choosing a model and saving results","3b5a559e":"Top correlation attributes to the Sale Price are:\n\n1. **GrLivArea**: Above grade (ground) living area square feet\n1. **OverallQual**: Overall material and finish quality\n1. **GarageCars**: Size of garage in car capacity\n\nWe can exclude **GarageArea** because it's highly correlated with **GarageCars**, which has a higher correlation with **SalePrice_log**.\n\nIn most cases house and garage are built in the same year thus we can exclude **GarageYrBlt**.\n\nWe can aslo exclude attributes with low or unclear correlation with SalePrice_log: **MSSubClass**, **MoSold**, **YrSold**, **MiscVal**.","ab722b95":"# 1. Quick Data Analysis","7c674555":"Let's check the correlation of the numerical columns.","1c3deda9":"In this notebook I'll practice **pipeline** and **cross-validation** on the Housing Prices Dataset for Kaggle Learn Users. The challenge is to create a model of **house price prediction**.","82c9ddc3":"![](https:\/\/images.unsplash.com\/photo-1490197415175-074fd86b1fcc?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1053&q=80)","00791262":"This work is based on the [Intermediate Maching Learning Course](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning) and two kernels:\n\n[House Prices: 1st Approach to Data Science Process](https:\/\/www.kaggle.com\/cheesu\/house-prices-1st-approach-to-data-science-process)\n\n[Housing Prices: GridSearchCV Example](https:\/\/www.kaggle.com\/erkanhatipoglu\/housing-prices-gridsearchcv-example)","a501095d":"# 2. Data preprocessing"}}