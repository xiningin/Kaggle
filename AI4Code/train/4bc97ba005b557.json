{"cell_type":{"aec8242a":"code","e7b099c1":"code","12f3ff5f":"code","a1572ea6":"code","09586c82":"code","e04345f8":"code","f174e855":"markdown","cc3721b0":"markdown","65ab1745":"markdown","975bc261":"markdown"},"source":{"aec8242a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e7b099c1":"# To use the Adam optimizer, we should train and validate our data in batches. For batching we create this class.\nimport numpy as np\n# Class for loading the datasets *.npz files and do the batching for the algorithm\n# This code is reusable. You should just change Liver_disease_data everywhere in the code\nclass Liver_Disease_Data_Reader():\n    # dataset is a mandatory arugment, while the batch_size is optional. dataset values can be 'train', 'valuatoin' or 'test'\n    # If you don't input batch_size, it will automatically take the value: None\n    def __init__(self, dataset, batch_size = None):\n    \n        # The dataset that loads is one of \"train\", \"validation\", \"test\".\n        # e.g. if I call this class with x('train',5), it will load 'Liver_disease_data_train.npz' with a batch size of 5.\n        npz = np.load('..\/input\/Liver_disease_data_{0}.npz'.format(dataset))\n        \n        # Two variables that take the values of the inputs and the targets. Inputs are floats, targets are integers\n        self.inputs, self.targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n        \n        # Counts the batch number, given the size you feed it later\n        # If the batch size is None, we are either validating or testing, so we want to take the data in a single batch\n        if batch_size is None:\n            self.batch_size = self.inputs.shape[0]\n        else:\n            self.batch_size = batch_size\n        self.curr_batch = 0\n        self.batch_count = self.inputs.shape[0] \/\/ self.batch_size\n    \n    # A method which loads the next batch\n    def __next__(self):\n        if self.curr_batch >= self.batch_count:\n            self.curr_batch = 0\n            raise StopIteration()\n            \n        # You slice the dataset in batches and then the \"next\" function loads them one after the other\n        batch_slice = slice(self.curr_batch * self.batch_size, (self.curr_batch + 1) * self.batch_size)\n        inputs_batch = self.inputs[batch_slice]\n        targets_batch = self.targets[batch_slice]\n        self.curr_batch += 1\n        \n        # One-hot encode the targets. In this example it's a bit superfluous since we have a 0\/1 column \n        # as a target already. But this will be useful for any classification task with more than one target column\n        classes_num = 2\n        targets_one_hot = np.zeros((targets_batch.shape[0], classes_num))\n        targets_one_hot[range(targets_batch.shape[0]), targets_batch] = 1\n        \n        # The function will return the inputs batch and the one-hot encoded targets\n        return inputs_batch, targets_one_hot\n    \n        \n    # A method needed for iterating over the batches, as we will put them in a loop\n    # This tells Python that the class we're defining is iterable, i.e. that we can use it like:\n    # for input, output in data: \n        # do things\n    # An iterator in Python is a class with a method __next__ that defines exactly how to iterate through its objects\n    def __iter__(self):\n        return self","12f3ff5f":"import tensorflow as tf\n\ninput_size = 10        # Input size depends on the number of input variables. We have 10 of them\noutput_size = 2        # Output size is 2, as we one-hot encoded the targets.\nhidden_layer_size = 20 # width of hidden layer\n\n# Reset the default graph, so you can fiddle with the hyperparameters and then rerun the code.\ntf.reset_default_graph()\n\n######--- NN Building block 1.DATA (Placeholders for data) ---######\n\ninputs = tf.placeholder(tf.float32, [None, input_size])\ntargets = tf.placeholder(tf.int32, [None, output_size])\n\n######--- NN Building block 2.Layer (Model + Activation function) ---######\n\n# Outline the model. We will create a net with 2 hidden layers\n\nweights_1 = tf.get_variable(\"weights_1\", [input_size, hidden_layer_size])\nbiases_1 = tf.get_variable(\"biases_1\", [hidden_layer_size])\noutputs_1 = tf.nn.relu(tf.matmul(inputs, weights_1) + biases_1)\n\nweights_2 = tf.get_variable(\"weights_2\", [hidden_layer_size, hidden_layer_size])\nbiases_2 = tf.get_variable(\"biases_2\", [hidden_layer_size])\n\noutputs_2 = tf.nn.sigmoid(tf.matmul(outputs_1, weights_2) + biases_2)\n\nweights_3 = tf.get_variable(\"weights_3\", [hidden_layer_size, output_size])\nbiases_3 = tf.get_variable(\"biases_3\", [output_size])\n\noutputs = tf.matmul(outputs_2, weights_3) + biases_3\n\n######--- NN Building block 3.Objective function ---######\n\n# Use of objective function(softmax_cross_entropy_with_logits) since this is a classification problem \nloss = tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=targets)\nmean_loss = tf.reduce_mean(loss)\n\n# Get a 0 or 1 for every input indicating whether it output the correct answer\nout_equals_target = tf.equal(tf.argmax(outputs, 1), tf.argmax(targets, 1))\naccuracy = tf.reduce_mean(tf.cast(out_equals_target, tf.float32))\n\n######--- NN Building block 4.Optimization Algorithm ---######\n\noptimize = tf.train.AdamOptimizer(learning_rate=0.002).minimize(mean_loss)\n\n### Please note, the above NN building blocks will run and learn only when you explicitly call them in a session ###\n\n# Create a session\nsess = tf.InteractiveSession()\n\n# Initialize the variables\ninitializer = tf.global_variables_initializer()\nsess.run(initializer)\n\n# Choose the batch size\nbatch_size = 20\n\n# Set early stopping mechanisms\nmax_epochs = 100\nprev_validation_loss = 9999999.\n\n# Load the first batch of training and validation, using the class we created. \n# Arguments are ending of 'Liver_disease_data_<...>', where for <...> we input 'train', 'validation', or 'test'\n# depending on what we want to load\ntrain_data = Liver_Disease_Data_Reader('train', batch_size)\nvalidation_data = Liver_Disease_Data_Reader('validation')\n\n# Create the loop for epochs \nfor epoch_counter in range(max_epochs):\n    \n    # Set the epoch loss to 0, and make it a float\n    curr_epoch_loss = 0.\n    \n    # Iterate over the training data \n    # Since train_data is an instance of the Liver_Disease_Data_Reader class,\n    # we can iterate through it by implicitly using the __next__ method we defined above.\n    # As a reminder, it batches samples together, one-hot encodes the targets, and returns\n    # inputs and targets batch by batch\n    for input_batch, target_batch in train_data:\n        _, batch_loss = sess.run([optimize, mean_loss], \n            feed_dict={inputs: input_batch, targets: target_batch})\n        \n        #Record the batch loss into the current epoch loss\n        curr_epoch_loss += batch_loss\n    \n    # Find the mean curr_epoch_loss\n    # batch_count is a variable, defined in the Liver_Disease_Data_Reader class\n    curr_epoch_loss \/= train_data.batch_count\n    \n    # Set validation loss and accuracy for the epoch to zero\n    validation_loss = 0.\n    validation_accuracy = 0.\n    \n    # Use the same logic of the code to forward propagate the validation set\n    # There will be a single batch, as the class was created in this way\n    for input_batch, target_batch in validation_data:\n        validation_loss, validation_accuracy = sess.run([mean_loss, accuracy],\n            feed_dict={inputs: input_batch, targets: target_batch})\n    \n    # Print statistics for the current epoch\n    print('Epoch '+str(epoch_counter+1)+\n          '. Training loss: '+'{0:.3f}'.format(curr_epoch_loss)+\n          '. Validation loss: '+'{0:.3f}'.format(validation_loss)+\n          '. Validation accuracy: '+'{0:.2f}'.format(validation_accuracy * 100.)+'%')\n    \n    # Trigger early stopping if validation loss begins increasing.\n    if validation_loss > prev_validation_loss:\n        break\n        \n    # Store this epoch's validation loss to be used as previous in the next iteration.\n    prev_validation_loss = validation_loss\n    \nprint('End of training.')","a1572ea6":"# Load the test data, following the same logic as we did for the train_data and validation data\ntest_data = Liver_Disease_Data_Reader('test')\n\n# Forward propagate through the training set. This time we only need the accuracy\nfor inputs_batch, targets_batch in test_data:\n    test_accuracy = sess.run([accuracy],\n                     feed_dict={inputs: inputs_batch, targets: targets_batch})\n\n# Get the test accuracy in percentages\n# When sess.run is has a single output, we get a list (that's how it was coded by Google), rather than a float.\n# Therefore, we must take the first value from the list (the value at position 0)\ntest_accuracy_percent = test_accuracy[0] * 100.\n\n# Print the test accuracy\nprint('Test accuracy: '+'{0:.2f}'.format(test_accuracy_percent)+'%')","09586c82":"inputs_batch.shape","e04345f8":"targets_batch.shape","f174e855":"#### Create a class that will batch the data\n","cc3721b0":"## Create the machine learning algorithm (i.e Deep Neural Network, here)\n\nThe building blocks of ML are:\n#### 1. Data\n     We take a historical dataset and use it to train the NN. \n     We split this data into 'train' and 'validation' and use them to prevent overfitting.\n     We feed the 'train' data in batches if you want to use Adam optimizer as your optimization algorithm\n     We use the 'test' data to find the accuracy of the model.\n#### 2. Model\n     Model is a function chosen by us, of which the parameters are weights and biases. e.g. y = x1w1 + ..+ xkwk + b (w-weight, b-bias)\n     Essentially, the idea of the ML is to find those parameters(weights and biases) for which the model has the highest predictive power\n     Note: To create a deep neural network, we should add an activation function to our model for each layer. Activation function adds non-linearity to the layer. \n     # Common activation functions are:\n        1. sigmoid(logistic)\n        2. TanH(Hyperbolic Tangent)\n        3. ReLU(Rectified Linear Unit)\n        4. Softmax ( This function usually uses in the output layer.)\n#### 3. Objective function \n     Objective function measures the predictive power[i.e variation from the model output(y) and the target(t)] of our model. \n     Objective functions are split into 1. loss (supervised learning) and 2. reward(reinforcement learning)\n     Based on the problem at hand, we aim to minimize the loss function OR maximize the reward function\n     This is a supervised learning problem, so we try to minimize the loss function, and the minimization happens by adjusting the parameters of the model (weights and biases). This adjustment is made by the optimization algorithm.\n     \n     #Common objective(loss) functions in Supervised Learning are:\n        1. For Regression problems\n            a. Mean Square Error\/Quadratic Loss\/L2 Loss\n            b. Mean Absolute Error\/L1 Loss\n            c. Mean Bias Error\n        2. For Classification problems\n            a. Cross Entropy Loss\/Negative Log Likelihood\n            b. Hinge Loss\/Multi class SVM Loss             \n#### 4. Optimization algorithm\n     Optimization algorithm adjusts parameters (weights and biases) and the modified model iterate through the steps.\n     Iteration is repeated until we find the values of the parameters (weights and biases), for which the objective function is optimal.\n     # Adam is the latest and widely using optimizer, now.\n","65ab1745":"## Test the model","975bc261":"# Indian Lever Patients Analysis-Deep Neural Networks-Algorithm"}}