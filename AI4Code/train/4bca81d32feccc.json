{"cell_type":{"6d36b633":"code","eae5bbe7":"code","20392b0e":"code","60cabb39":"code","bc946aa5":"code","084e33ce":"code","c7b59931":"code","2f53abba":"code","720c4d69":"code","d8de7ae5":"code","84438979":"code","25cab8d6":"code","280451d9":"code","2b4361f2":"code","680609f4":"code","059009ec":"code","15701798":"code","86e2dffe":"code","b14bf66b":"code","d5543468":"code","9c715acb":"code","f1d8cea1":"code","d3d8a634":"code","2dc368de":"code","78a596f8":"code","6a2feda9":"code","5416cb8b":"code","bf426737":"markdown","788aa3a7":"markdown","ad5436aa":"markdown","8ad32ef1":"markdown","05ac3e4d":"markdown","03ff5e3e":"markdown","0f8c5134":"markdown","1d226243":"markdown","42db42fa":"markdown","8f050479":"markdown","9a8bbf16":"markdown","05eb62b4":"markdown","602ed48b":"markdown","5f9d3dfe":"markdown","aded12df":"markdown","555143ee":"markdown","5e472fd4":"markdown","8ba9dc2e":"markdown","f693b82b":"markdown","bd03bc83":"markdown","c0439422":"markdown"},"source":{"6d36b633":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","eae5bbe7":"#Configure plotting options\nsns.set_style('darkgrid')\nplt.rc('axes', labelsize=14, labelweight='bold', titlesize=16, titleweight='bold')\nplt.rc('figure', figsize=(8,5))","20392b0e":"#Display float values in 4 decimal points\npd.set_option('display.float_format', lambda x: '%.4f' % x)\n\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ndf = pd.concat([train,test])\ndf.head()","60cabb39":"print('Training dimensions:', train.shape)\nprint('Testing dimensions:', test.shape)\nprint('Full dimension:', df.shape)","bc946aa5":"df.info()","084e33ce":"df.describe().T","c7b59931":"#Display categories of each feature\ndef displayUniqueFeatures(col):\n    count = df[col].nunique()\n    print('Unique features for %s: %d' % (col, count))\n    if count < 50:\n        print(df[col].unique())\n\nfor col in df.select_dtypes('object'):\n    displayUniqueFeatures(col)","2f53abba":"#Drop unneeded columns\ncat_cols = df.columns[df.dtypes == 'object']\ncols_to_drop = [col for col in cat_cols if df[col].nunique() > 10]\ncols_to_drop\n\ntrain.drop(columns=['PassengerId'] + cols_to_drop, axis=1)\ntest.drop(columns=['PassengerId'] + cols_to_drop, axis=1)","720c4d69":"#Display correlation matrix\nsns.heatmap(df.corr(), fmt='.2f', annot=True, cmap='Greens', linewidth=.1, annot_kws={'size': 14});","d8de7ae5":"#It's easier to make a function displaying plots, so we don't have to enter the same parameters again\ndef make_pie(column):\n    values = df[column].value_counts()\n    plt.pie(values.values, labels=values.index, autopct='%1.1f%%', startangle=90, shadow=True)","84438979":"make_pie('Survived')","25cab8d6":"make_pie('Pclass')","280451d9":"make_pie('Sex')","2b4361f2":"make_pie('Embarked')","680609f4":"pd.crosstab(df.Sex,df.Survived).plot(kind='bar', stacked=True);","059009ec":"pd.crosstab(df.Embarked,df.Survived).plot(kind='bar', stacked=True);","15701798":"sns.histplot(df.SibSp);","86e2dffe":"sns.histplot(df.Parch);","b14bf66b":"sns.boxplot(x='Embarked', y='Fare', data=df);","d5543468":"sns.boxplot(x='Pclass', y='Fare', data=df);","9c715acb":"def get_null_info(df):\n    null_sum = [df[col].isnull().sum() for col in df.columns]\n    null_prob = [df[col].isnull().sum() \/ len(df) for col in df.columns]\n    null_vals = pd.DataFrame({'amount': null_sum, 'probability': null_prob}, index=df.columns)\n    return null_vals.sort_values('amount', ascending=False)\n\nget_null_info(train)","f1d8cea1":"get_null_info(test)","d3d8a634":"#Fill in missing values\ntrain.Cabin.fillna('Unknown', inplace=True)\ntest.Cabin.fillna('Unknown', inplace=True)\ntrain.Age.fillna(df.Age.median(), inplace=True)\ntest.Age.fillna(df.Age.median(), inplace=True)\ntrain.Embarked.fillna('Unknown', inplace=True)\ntest.Embarked.fillna('Unknown', inplace=True)\ntest.Fare.fillna(df.Fare.mean(), inplace=True)","2dc368de":"#Using sklearn's Pipeline to build the model\npipe = Pipeline([('ohe', OneHotEncoder(handle_unknown='ignore', sparse=False)), \n                 ('rf', RandomForestClassifier(random_state=0))])\n\n#Hyperparameters for our model\nparam_grid = {'rf__n_estimators': [10,20,30,50,100,500,1000], \n              'rf__criterion': ['gini','entropy'], \n              'rf__bootstrap': [True,False]}\n\n#Data splitting\nX_train = train.drop('Survived', axis=1)\ny_train = train.Survived\n\n#Model building\nmodel = GridSearchCV(pipe, param_grid, scoring='accuracy')\nmodel.fit(X_train,y_train)\nprint('Highest score possible: {:.4f}'.format(model.best_score_))","78a596f8":"print('Best parameters:')\nfor k, v in model.best_params_.items():\n    print('%s: %s' % (k, v))","6a2feda9":"#This will create a table of scores with each hyperparameter combination\nparams = pd.DataFrame(model.cv_results_['params'])\nparams['score'] = model.cv_results_['mean_test_score']\nparams.sort_values('score', ascending=False)","5416cb8b":"#Create CVS file for submission\noutput = pd.DataFrame({'PassengerId': test.index + 892, 'Survived': model.predict(test)})\noutput.to_csv('gender_submission.csv', index=False)\noutput","bf426737":"## Intro\nThis is my exploratory data analysis and model building for the Titanic dataset. The titanic dataset is one of the most widely known sample datasets where it can be used to predict who would survive using binary classification, and as also stated, this is my entry for the Kaggle competition. I will start by analyzing the data, then doing some data cleaning and model building to determine how accurate our model will be. We will start by loading our packages:","788aa3a7":"First class individuals paid the most out of everyone else, as suspected. The correlation table proves this fact.\n\n### Data Cleaning","ad5436aa":"So it turns out that using Random Forest Classifier, along with some hyperparameters, slightly boosted the score which makes our model a little bit better. Comment for any feedback.","8ad32ef1":"Next, I will configure the plotting for my convenience. Tampering with the plotting features is really beneficial and it will make your visualizations beautiful. Using the rc() function from Matplotlib's Pyplot is more efficient, so there wouldn't be a need to repeatedly enter the same functions over and over.","05ac3e4d":"This dataset has no correlations, except for ticket class (Pclass) and fare, where more than 50% and less than -50% is considered a strong correlation.\n\n### Visualizations\n\nAgain, I will use a function to plot pies for efficiency.","03ff5e3e":"Just like this histogram, the Parch feature takes in the amount of both parents and\/or children, which can be really confusing. This could be interpreted as individuals with children, since you can't have more than two parents. So it could be said that the tiny portion of people on the ship had more than two children, and the majority have no children.","0f8c5134":"We got an accuracy score of almost 83%, which is above the average (77%) of the scores in this competition. The hyperparameters definiely helped. The code below displays the parameters used to obtain the score:","1d226243":"It turns out that most of the individuals in the Titanic did not survive the catastrophe.","42db42fa":"The Titanic data has missing values, more specifically the Cabin of each individual, which heavily lacks non-null values. The \"Survived\" column is nonexistent in the testing data. The Age and Embarked columns also have missing values in both datasets. We can fill the Cabin and Embarked entries with \"Unknown\", fill the missing fare value with the mean, and fill the missing ages with the median, because it makes no sense for ages to have an average, which doesn't represent a whole number.\n\n*NOTE: The column transformer can also be used, but I have discovered that it gave my model a lower accuracy score.*","8f050479":"And here are all of the possible scores:","9a8bbf16":"Now that we are done, we can create our submission:","05eb62b4":"Given the fact that this feature counts both siblings and spouses (according to the description of the data), it can be hard to tell if the individual was a sibling or somebody's spouse, especially with this histogram. So it is known that it's uncommon to have alot of siblings, and obviously, no person has more than one spouse, especially in 1912. So we could say that in the first two bars, most of the people in the Titanic were either single or didn't have any siblings. The second bar could indicate that those people were married and\/or had a slight chance of having only one sibling. The other bars could indicate that a tiny portion of individuals had more than one sibling.","602ed48b":"I will create a function for displaying the column's categories to preserve code usage.","5f9d3dfe":"Cherbourg residents appeared to have paid the most expensive fares for the ship, while the other embarkments have paid the same amounts.","aded12df":"Now that we have set up our packages and configurations, we can now load and analyze the data.\n\n## Data Analysis","555143ee":"Most of the individuals are men.","5e472fd4":"This plot says that it was mostly females that survived the incident. The length of the bars indicate the frequency of that category.","8ba9dc2e":"3rd class appears to be the most frequent class. This could assume that the classes are price-based and\/or have perks, meaning that this a lower-ranked class.","f693b82b":"The ship mostly consisted of Southamption individuals.","bd03bc83":"Most of the casualties and survivals were from Southampton folks.","c0439422":"## Building the Model\n\nNow that we have imputed the missing values, we can now build our model and get an accuracy score. I will use the Pipeline to encode the categorical data and then use the Random Forest Classifier, along with some hyperparameters to get the highest score."}}