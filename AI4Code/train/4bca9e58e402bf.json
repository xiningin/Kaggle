{"cell_type":{"d370c2d0":"code","5df4dc9d":"code","9cb48380":"code","a0093cc3":"code","e1fa322e":"code","e6fc35d7":"code","3874110d":"code","6fad6c3f":"code","e08756c4":"code","c923669a":"code","c2f2cd53":"code","59d762c7":"code","051ebeb1":"code","66ef45e9":"code","affe357e":"code","1c07cd42":"code","f8c94dd9":"code","184a0db3":"code","8f22666c":"code","d79b1064":"code","eca51eee":"code","9466e521":"markdown","117ddb20":"markdown","828cd7fd":"markdown","0e7648ea":"markdown","1a7a5cc5":"markdown","114a7328":"markdown"},"source":{"d370c2d0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns","5df4dc9d":"#reading the data \ndata = pd.read_csv('..\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv')\ndata.head()","9cb48380":"# exploring the data to get a basic idea of the data\ndata.describe()\nprint(data.shape)\nprint(data.dtypes)\nprint(data.dtypes.value_counts())\n# dropping the columns ['Over18','StandardHours','EmployeeCount','EmployeeNumber'] as they all have the same value.\ndata=data.drop(['Over18','StandardHours','EmployeeCount','EmployeeNumber'],axis=1)\ndata.shape","a0093cc3":"#creating dummy values for categorical variable.\nchar_cols = data.dtypes.pipe(lambda x: x[x == 'object']).index\nlabel_mapping = {}\n\nfor c in char_cols:\n    data[c], label_mapping[c] = pd.factorize(data[c])\ndata.head()","e1fa322e":"#now changing data type of categorical variables into category\ndata[['BusinessTravel','Department','EducationField','Gender',\n      'JobRole','MaritalStatus','OverTime']]=data[['BusinessTravel','Department','EducationField','Gender',\n      'JobRole','MaritalStatus','OverTime']].astype('category')\nprint(data.dtypes)","e6fc35d7":"#obtaining the descriptive statistics of the data.\ndata.describe().T","3874110d":"# creating boxplot to check for outliers\ndata.boxplot(column=['Age', 'DailyRate','DistanceFromHome', 'Education','EnvironmentSatisfaction'])","6fad6c3f":"data.boxplot(column=['HourlyRate', 'JobInvolvement','JobLevel', 'JobSatisfaction','MonthlyIncome'])","e08756c4":"data.boxplot(column=['TotalWorkingYears', 'TrainingTimesLastYear','WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole',\n       'YearsSinceLastPromotion', 'YearsWithCurrManager'])","c923669a":"data.boxplot(column=['NumCompaniesWorked','PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction',\n                     'StockOptionLevel'])","c2f2cd53":"#from the above boxplot we can see that Monthly income has more outliers ouliers compared to other varbales\n#below is a function that filters out outliers row from our data.\ndef outlier(df,col_name):\n    Q1 = df[col_name].quantile(0.25)\n    Q3 = df[col_name].quantile(0.75)\n    IQR = Q3 - Q1\n    print(\"IQR: \",IQR)\n    print(\"Q1: \",Q1)\n    print(\"Q3: \",Q3)\n    lw=Q1-(1.5*IQR)\n    up=Q3+(1.5*IQR)\n    print(\"the limits of outlier is: \",(lw,up))\n    print(\"% of observation above upperlimit is: \",(df[col_name]>up).value_counts(normalize=True)*100)\n    print(\"% of observation below lowerlimit is: \",(df[col_name]<lw).value_counts(normalize=True)*100)\n    df=df.loc[(df[col_name]>lw)&(df[col_name]<up)]\n    return(df)\ndata=outlier(data,'MonthlyIncome')\ndata.shape","59d762c7":"#splitting the data into independent var and target variables\ny=data['Attrition']\nx=data.drop('Attrition',axis=1)\nx.head()","051ebeb1":"# we can apply feature selection method to select the features to be included in our model.\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n#apply SelectKBest class to extract top 20 best features using chi-square \nbestfeatures = SelectKBest(score_func=chi2, k=20)\nfit = bestfeatures.fit(x,y)\ndf_scores=pd.DataFrame(fit.scores_)\ndf_columns=pd.DataFrame(x.columns)\nfeature_scores=pd.concat([df_columns,df_scores],axis=1)\nfeature_scores.columns=['variable','score']\nfinal_variables=feature_scores.nlargest(20,'score')\nfinal_variables","66ef45e9":"#filtering out the important columns alone for building our model\nfiltered_data=data[['MonthlyIncome','MonthlyRate','DailyRate','TotalWorkingYears','YearsAtCompany','YearsInCurrentRole',\n                   'YearsWithCurrManager','Age','DistanceFromHome','StockOptionLevel','OverTime','JobLevel','MaritalStatus',\n                   'EducationField','YearsSinceLastPromotion','JobSatisfaction','EnvironmentSatisfaction','NumCompaniesWorked',\n                   'JobInvolvement','TrainingTimesLastYear','Attrition']]\nfiltered_data.shape","affe357e":"# splitting the data into train and test\nfrom sklearn.model_selection import train_test_split\ny=filtered_data['Attrition']\nx=filtered_data.drop('Attrition',axis=1)\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=50)","1c07cd42":"# buildng a decision tree model using 'entropy' criterion\nfrom sklearn.tree import DecisionTreeClassifier\nmodel=DecisionTreeClassifier(criterion='entropy',max_depth=5,random_state=100)\nmodel.fit(x_train,y_train)","f8c94dd9":"predictions=model.predict(x_test)\nfrom sklearn.metrics import accuracy_score\naccuracy=accuracy_score(y_test,predictions)*100\nprint(accuracy)","184a0db3":"# model evaluation using cross validation score to check our accuracy.\nfrom sklearn.model_selection import cross_val_score\ndt_score=cross_val_score(model,x_train,y_train,scoring=\"accuracy\",cv=10)\nprint(dt_score)\nprint(\"mean_accuracy:\",dt_score.mean())\nprint(\"std_deviation of accuracy:\",dt_score.std())\n#the mean accuracy of our model after running it 10 times is 80.3%","8f22666c":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score,recall_score\nprint(confusion_matrix(y_test, predictions))\nprint(\"precision_score: \",precision_score(y_test,predictions))\nprint(\"recall_score: \",recall_score(y_test,predictions))\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,predictions))","d79b1064":"# building a random forest classifier for the data\nfrom sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier(random_state=500)\nrf.fit(x_train,y_train)\npredictions_rf=rf.predict(x_test)\nprint(accuracy_score(y_test,predictions_rf))\n# model evaluation\nfrom sklearn.model_selection import cross_val_score\nrf_score=cross_val_score(rf,x_train,y_train,scoring=\"accuracy\",cv=10)\nprint(rf_score)\nprint(\"mean_accuracy:\",rf_score.mean())\nprint(\"std_deviation of accuracy:\",rf_score.std())","eca51eee":"#confusion matrix\nprint(confusion_matrix(y_test, predictions))\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,predictions_rf))","9466e521":"Random Forest performs on the data.","117ddb20":"This is my first kernel.Any suggestions and comments are most welcomed. This is just a beginners classification model using decision tree and random forest.","828cd7fd":"Since there are more variables, I have reduced the dimension of the data by selecting important features using chi square method for checking the association with the target variable(attrition).","0e7648ea":"Model Evaluation","1a7a5cc5":"**Importing libraries**","114a7328":"To avoid overfitting , I am using cross validation to check my mean accuracy score of the model."}}