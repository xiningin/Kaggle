{"cell_type":{"1366002d":"code","12ad0acd":"code","71d3c5d9":"code","b88b6857":"code","8df1ec66":"code","da859278":"markdown"},"source":{"1366002d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom sklearn.datasets import make_classification,make_blobs\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","12ad0acd":"class NaiveBayes():\n    def prior(self,c):\n        prior_prob = np.mean(self.y==c)\n        return prior_prob\n    \n    def likelihood(self,mu,var,x):\n        eps = 1e-4\n        coef = 1.0 \/ np.sqrt(2.0 * np.pi * var + eps)\n        exponent = np.exp(-(np.power(x - mu, 2) \/ (2 * var + eps)))\n        likelihood_prob = coef*exponent\n        return likelihood_prob\n    \n    def fit(self,X,y):\n        self.x = X\n        self.y = y\n        self.classes = np.unique(y)\n        \n        self.parameters=[]\n        \n        for i, c in enumerate(self.classes):\n            x_given_c = X[np.where(y == c)]\n            self.parameters.append([])\n            for col in x_given_c.T:\n                parameter = {\"mean\": col.mean(), \"var\": col.var()}\n                self.parameters[i].append(parameter)\n\n    def classify(self,x):\n        posteriors = []\n        for i, c in enumerate(self.classes):\n            posterior_prob = self.prior(c)\n            for feature_value, params in zip(x, self.parameters[i]):\n                likelihood_prob = self.likelihood(params[\"mean\"], params[\"var\"], feature_value)\n                posterior_prob *= likelihood_prob\n            posteriors.append(posterior_prob)\n        # Return the class with the largest posterior probability\n        return self.classes[np.argmax(posteriors)]\n    \n    def predict(self,X):\n        y_pred = np.array([self.classify(sample) for sample in X])\n        return y_pred\n        \n        ","71d3c5d9":"random_seed = 21\nX, y = make_blobs(n_samples=1000, centers=2,random_state=random_seed)\nnaive_bayes = NaiveBayes()\nnaive_bayes.fit(X, y)","b88b6857":"y_pred= naive_bayes.predict(X)","8df1ec66":"print(\"Accuracy: \", np.mean(y==y_pred))","da859278":"## Day 3: Naive Bayes classifier\nIn this notebook we will implement a naive Bayes classfier"}}