{"cell_type":{"fa9d40c1":"code","88d40fc5":"code","4df26acb":"code","dec8d954":"code","f82d943c":"code","c70b926f":"code","b6882d46":"code","71b5e030":"code","787e3626":"code","d0eaefc2":"code","386e0dda":"code","c8e9a027":"code","36052c48":"code","263a2c15":"code","9118846d":"code","71ede063":"code","c844639f":"code","e33834c1":"code","d516befa":"code","55211009":"code","2435b1c0":"code","8ac5f1ba":"code","829d6d52":"code","70906119":"code","5c53928c":"code","39f490d9":"code","2b7c260a":"code","135f8fb7":"code","8d61705f":"code","58f20c4f":"code","e9d37202":"code","ba9e3761":"code","dae409f2":"code","556f05ce":"code","9c283e1d":"code","476b445d":"code","97da1840":"code","7e600d0e":"code","fecc55f3":"code","556e5b7a":"code","558210ad":"code","66d439cd":"code","57c51d25":"code","205e29c2":"code","b24639cf":"code","f26ac287":"code","5d7f4b60":"code","a40b9497":"code","50dea04d":"code","fc26cbb1":"code","62f71192":"code","f9553e00":"code","119e170a":"code","a5df62e4":"code","c9330444":"code","25991961":"code","4cd1aaf3":"code","8f4a064d":"code","a0c55220":"code","451ef198":"code","81f73240":"code","ed9eef4a":"code","a0f68a78":"markdown","2e779faa":"markdown","425ff313":"markdown","edbdca5d":"markdown","a99a422a":"markdown","3e76d944":"markdown","a4218bcd":"markdown","4a0b22a1":"markdown","3f8623e0":"markdown","01df9d45":"markdown","e1dd7dd1":"markdown","9f428aa3":"markdown","984545a4":"markdown","bbc5ebdb":"markdown","9043224b":"markdown","6e6187f1":"markdown","3d332661":"markdown","b78fd474":"markdown","26f1851f":"markdown","070a063c":"markdown","cee65abc":"markdown","da70a39b":"markdown","ad2f51df":"markdown","bde37c50":"markdown","de87d98c":"markdown","835cbcb2":"markdown","1b1396e2":"markdown","916d919c":"markdown","2915b815":"markdown","24bedcbd":"markdown","5c067ea0":"markdown","a019ae59":"markdown","a90642f9":"markdown","7e7146f1":"markdown","5d65a696":"markdown","df9fa896":"markdown","d0dc2c33":"markdown","ee98b500":"markdown","bdc43c8f":"markdown"},"source":{"fa9d40c1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","88d40fc5":"import pandas as pd","4df26acb":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","dec8d954":"train.head()","f82d943c":"train.shape","c70b926f":"test.head()","b6882d46":"test.shape","71b5e030":"train.isnull().sum()","787e3626":"test.isnull().sum()","d0eaefc2":"## we will compare survival rate using pivot table function\npd.pivot_table(train,index=\"Survived\",values = [\"Age\", \"SibSp\", \"Parch\", \"Fare\"])","386e0dda":"print(pd.pivot_table(train,index=\"Survived\", columns = \"Pclass\", values =\"Ticket\", aggfunc = \"count\"))\nprint(pd.pivot_table(train,index=\"Survived\", columns = \"Sex\", values =\"Ticket\", aggfunc = \"count\"))\nprint(pd.pivot_table(train,index=\"Survived\", columns = \"Embarked\", values =\"Ticket\", aggfunc = \"count\"))\n# here's another way to print those values\n# print(pd.pivot_table(train,index=\"Survived\", columns = [\"Pclass\", \"Sex\", \"Embarked\"], values =\"Ticket\", aggfunc = \"count\"))","c8e9a027":"### Now Let's figure those values","36052c48":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n# setting seaborn default for plots\nsns.set()","263a2c15":"def bar_chart(feature):\n    survived = train[train['Survived']==1][feature].value_counts()\n    dead = train[train['Survived']==0][feature].value_counts()\n    df = pd.DataFrame([survived,dead])\n    df.index = ['Survived','Dead']\n    df.plot(kind='bar',stacked=True, figsize=(10,5))","9118846d":"bar_chart('Pclass')","71ede063":"bar_chart('Sex')","c844639f":"bar_chart('Embarked')","e33834c1":"bar_chart('SibSp')","d516befa":"bar_chart('Parch')","55211009":"## Now Let's Fill missing values in train data","2435b1c0":"train.head()","8ac5f1ba":"comb_data = [train, test] #combining train and test dataset\n\nfor dataset in comb_data:\n    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)","829d6d52":"train['Title'].value_counts()","70906119":"test['Title'].value_counts()","5c53928c":"### Now let's map new values express those titles and check if this will help us in our analysis\ntitle_mapping = {\"Mr\": 0, \"Miss\": 1, \"Mrs\": 2, \n                 \"Master\": 3, \"Dr\": 3, \"Rev\": 3, \"Col\": 3, \"Major\": 3, \"Mlle\": 3,\"Countess\": 3,\n                 \"Ms\": 3, \"Lady\": 3, \"Jonkheer\": 3, \"Don\": 3, \"Dona\" : 3, \"Mme\": 3,\"Capt\": 3,\"Sir\": 3 }\nfor dataset in comb_data:\n    dataset['Title'] = dataset['Title'].map(title_mapping)","39f490d9":"bar_chart('Title')","2b7c260a":"# delete unnecessary feature from dataset\ntrain.drop('Name', axis=1, inplace=True)\ntest.drop('Name', axis=1, inplace=True)","135f8fb7":"sex_mapping = {\"male\": 0, \"female\": 1}","8d61705f":"for dataset in comb_data:\n    dataset['Sex'] = dataset['Sex'].map(sex_mapping)","58f20c4f":"bar_chart('Sex')","e9d37202":"# impute nulls for continuous data and getting mean values for missing age values\ntrain.dropna(subset=['Embarked'],inplace = True)\ntest.dropna(subset=['Embarked'],inplace = True)\n\ntrain[\"Age\"].fillna(train.groupby(\"Title\")[\"Age\"].transform(\"mean\"), inplace=True)\ntest[\"Age\"].fillna(test.groupby(\"Title\")[\"Age\"].transform(\"mean\"), inplace=True)","ba9e3761":"Facet_Grid= sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nFacet_Grid.map(sns.kdeplot,'Age',shade= True)\nFacet_Grid.set(xlim=(0, train['Age'].max()))\nFacet_Grid.add_legend()\nplt.show()","dae409f2":"for dataset in comb_data:\n    dataset.loc[ dataset['Age'] <= 16 , 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 26), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 26) & (dataset['Age'] <= 36), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 36) & (dataset['Age'] <= 62), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 62, 'Age'] = 4","556f05ce":"bar_chart('Age')","9c283e1d":"Pclass1 = train[train['Pclass']==1]['Embarked'].value_counts()\nPclass2 = train[train['Pclass']==2]['Embarked'].value_counts()\nPclass3 = train[train['Pclass']==3]['Embarked'].value_counts()\ndf = pd.DataFrame([Pclass1, Pclass2, Pclass3])\ndf.index = ['1st class','2nd class', '3rd class']\ndf.plot(kind='bar',stacked=True, figsize=(10,5))","476b445d":"embarked_mapping = {\"S\": 0, \"C\": 1, \"Q\": 2}\nfor dataset in comb_data:\n    dataset['Embarked'] = dataset['Embarked'].map(embarked_mapping)","97da1840":"# fill missing Fare with mean fare for each Pclass\ntrain[\"Fare\"].fillna(train.groupby(\"Pclass\")[\"Fare\"].transform(\"mean\"), inplace=True)\ntest[\"Fare\"].fillna(test.groupby(\"Pclass\")[\"Fare\"].transform(\"mean\"), inplace=True)","7e600d0e":"facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Fare',shade= True)\nfacet.set(xlim=(0, train['Fare'].max()))\nfacet.add_legend()\nplt.show()","fecc55f3":"for dataset in comb_data:\n    dataset.loc[ dataset['Fare'] <= 17, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 17) & (dataset['Fare'] <= 30), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 30) & (dataset['Fare'] <= 100), 'Fare'] = 2\n    dataset.loc[ dataset['Fare'] > 100, 'Fare'] = 3","556e5b7a":"train.Cabin.value_counts()","558210ad":"for dataset in comb_data:\n    dataset['Cabin'] = dataset['Cabin'].str[:1]","66d439cd":"Pclass1 = train[train['Pclass']==1]['Cabin'].value_counts()\nPclass2 = train[train['Pclass']==2]['Cabin'].value_counts()\nPclass3 = train[train['Pclass']==3]['Cabin'].value_counts()\ndf = pd.DataFrame([Pclass1, Pclass2, Pclass3])\ndf.index = ['1st class','2nd class', '3rd class']\ndf.plot(kind='bar',stacked=True, figsize=(10,5))","57c51d25":"cabin_mapping = {\"A\": 0, \"B\": 0.4, \"C\": 0.8, \"D\": 1.2, \"E\": 1.6, \"F\": 2, \"G\": 2.4, \"T\": 2.8}\nfor dataset in comb_data:\n    dataset['Cabin'] = dataset['Cabin'].map(cabin_mapping)","205e29c2":"# fill missing Cabin with mean for each Pclass\ntrain[\"Cabin\"].fillna(train.groupby(\"Pclass\")[\"Cabin\"].transform(\"mean\"), inplace=True)\ntest[\"Cabin\"].fillna(test.groupby(\"Pclass\")[\"Cabin\"].transform(\"mean\"), inplace=True)","b24639cf":"train[\"Parch_n\"] = train[\"SibSp\"] + train[\"Parch\"] + 1\ntest[\"Parch_n\"] = test[\"SibSp\"] + test[\"Parch\"] + 1","f26ac287":"facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Parch_n',shade= True)\nfacet.set(xlim=(0, train['Parch_n'].max()))\nfacet.add_legend()\nplt.xlim(0)","5d7f4b60":"f_mapping = {1: 0, 2: 0.4, 3: 0.8, 4: 1.2, 5: 1.6, 6: 2, 7: 2.4, 8: 2.8, 9: 3.2, 10: 3.6, 11: 4}\nfor dataset in comb_data:\n    dataset['Parch_n'] = dataset['Parch_n'].map(f_mapping)","a40b9497":"features_drop = ['Ticket', 'SibSp', 'Parch']\ntrain = train.drop(features_drop, axis=1)\ntest = test.drop(features_drop, axis=1)\ntrain = train.drop(['PassengerId'], axis=1)","50dea04d":"train_data = train.drop('Survived', axis=1)\ntarget = train['Survived']\n\ntrain_data.shape, target.shape","fc26cbb1":"train_data.describe().columns","62f71192":"## Now let's import relevant libraries\n\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score","f9553e00":"reg = LogisticRegression(max_iter = 2000)\nscoring = 'accuracy'\nscore = cross_val_score(reg, train_data, target, n_jobs=1, scoring=scoring)\nprint(score)\nround(np.mean(score)*100,2)","119e170a":"k_fold = KFold(n_splits=10, shuffle=True, random_state=0)","a5df62e4":"kNN = KNeighborsClassifier(n_neighbors = 13)\nscoring = 'accuracy'\nscore_1 = cross_val_score(kNN, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score_1)\nround(np.mean(score_1)*100, 2)","c9330444":"DTC = DecisionTreeClassifier()\nscoring = 'accuracy'\nscore_2 = cross_val_score(DTC, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score_2)\nround(np.mean(score_2)*100, 2)","25991961":"RFC = RandomForestClassifier(n_estimators=13)\nscoring = 'accuracy'\nscore_3 = cross_val_score(RFC, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score_3)\nround(np.mean(score_3)*100, 2)","4cd1aaf3":"GNB = GaussianNB()\nscoring = 'accuracy'\nscore_4 = cross_val_score(GNB, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score_4)\nround(np.mean(score_4)*100, 2)","8f4a064d":"SVC = SVC()\nscoring = 'accuracy'\nscore_5 = cross_val_score(SVC, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score_5)\nround(np.mean(score_5)*100,2)","a0c55220":"SVC.fit(train_data, target)\ntest_data = test.drop(\"PassengerId\", axis=1).copy()\nprediction = SVC.predict(test_data)","451ef198":"submission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": prediction\n    })\n\nsubmission.to_csv('submission.csv', index=False)\nsubmission = pd.read_csv('submission.csv')","81f73240":"pd.options.display.max_columns = None\npd.options.display.max_rows = None\ndisplay(submission)","ed9eef4a":"submission.shape","a0f68a78":"The Chart confirms **C** more likely survivied than **Q , S**","2e779faa":"### Results\n\nLogistic Regression = 81.89\n\nkNN Classifier = 82.13\n\nDecision Tree Classifier = 79.2\n\nRandom Forset Classifier = 80.55\n\nNaive Bayes Classifier = 77.85\n\nSVM Classifier = 83.36","425ff313":"There are 177 rows with missing **Age**, 687 rows with missing **Cabin** and 2 rows with missing **Embarked** information.","edbdca5d":"### Now Numerical bar charts","a99a422a":"**Total rows and columns**\n\nWe can see that there are 418 rows and 11 columns in our training dataset.\n\n**Now Let's check how much null we got in our sets**","3e76d944":"### 5.2.5 Random Forest Classifier","a4218bcd":"### 5.2.6 Naive Bayes Classifier","4a0b22a1":"### 5.2 Cross Validation (K-fold)","3f8623e0":"## 7. Testing","01df9d45":"## References\n\nThis notebook is created by learning from the following:\n\n- [Mukesh ChapagainTitanic Solution: A Beginner's Guide](https:\/\/www.kaggle.com\/chapagain\/titanic-solution-a-beginner-s-guide?scriptVersionId=1473689)\n- [How to score 0.8134 in Titanic Kaggle Challenge](http:\/\/ahmedbesbes.com\/how-to-score-08134-in-titanic-kaggle-challenge.html)\n- [Titanic: factors to survive](https:\/\/olegleyz.github.io\/titanic_factors.html)\n- [Titanic Survivors Dataset and Data Wrangling](http:\/\/www.codeastar.com\/data-wrangling\/)\n- [Kenjee : Titanic Project Example](https:\/\/www.kaggle.com\/kenjee\/titanic-project-example)\n- [Minsuk Heo \ud5c8\ubbfc\uc11d](https:\/\/www.youtube.com\/user\/TheEasyoung)","e1dd7dd1":"## Now We Will Plot those 2 Categories\n\n### Bar Chart for Categorical Features\n- Pclass\n- Sex\n- Survived\n- Embarked\n\n\n### Bar Chart for Numerical Features\n- Age\n- SibSp\n- Parch\n- Fare","9f428aa3":"## 2. Collecting the data\n\n### load train, test dataset using Pandas","984545a4":"### 5.2.3 kNN Classifier","bbc5ebdb":"### 4.1 Name","9043224b":"### 4.2 Sex\n\nmale: 0\n\nfemale: 1","6e6187f1":"The Chart confirms **a person aboarded with more than 2 parents or children** more likely survived  \nThe Chart confirms **a person aboarded alone** more likely dead","3d332661":"## 4. Feature engineering\n\nFeature engineering is the process of using domain knowledge of the data  \nto create features that make machine learning algorithms work. ","b78fd474":"### 4.6 Cabin","26f1851f":"#### Converting\nNow we start Converting Numerical Age to Categorical Variable  \n\nfeature vector map:  \nchild: 0  \nyoung: 1  \nadult: 2  \nmid-age: 3  \nsenior: 4","070a063c":"As we can see for \"Pclass\" \"1st\" class tend to have more surviving chance than 2nd and 3rd\n\nFemales tend to have more surviving chances than Males\n\nsurviving_chance_for_southamptons = 24    % chance of surviving\n\nsurviving_chance_for_Queenstown   = 3.4   % chance of surviving\n\nsurviving_chance_for_Cherbourg    = 9.32  % chance of surviving","cee65abc":"The Chart confirms **Women** more likely survivied than **Men**","da70a39b":"The Chart confirms **a person aboarded with more than 2 siblings or spouse** more likely survived  \nThe Chart confirms ** a person aboarded without siblings or spouse** more likely dead","ad2f51df":"### 5.1 LogisticRegression","bde37c50":"There are 86 rows with missing **Age**, 327 rows with missing **Cabin** .","de87d98c":"### 4.4 Embarked","835cbcb2":"## 5. Modeling","1b1396e2":"# Titanic: Machine Learning from Disaster\n### Predict survival on the Titanic\n\n### RoadMap\n- Import Relevant Libraries\n- Collecting the data\n- Exploratory data analysis\n- Feature engineering\n- Modelling\n- Testing","916d919c":"As we can see \n\n1- Younger Age tend to survive more\n\n2- Higher Fare tend to survive more\n\n3- Less SibSp and Higher Parch means more chance to survive","2915b815":"**Total rows and columns**\n\nWe can see that there are 891 rows and 12 columns in our training dataset.","24bedcbd":"### Models from Higher to Worst\n\n- SVM Classifier\n- kNN Classifier\n- Logistic Regression\n- Random Forest Classifier\n- Decision Tree Classifier\n- Naive Bayes Classifier\n\nSo the best result was using SVM Classifier so we will test accuracy with it","5c067ea0":"### 4.5 Fare","a019ae59":"### 5.2.4 Decision Tree Classifier","a90642f9":"### 4.7 Parch","7e7146f1":"### Data Dictionary\n- Survived: \t0 = No, 1 = Yes  \n- pclass: \tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd  \t\n- sibsp:\t# of siblings \/ spouses aboard the Titanic  \t\n- parch:\t# of parents \/ children aboard the Titanic  \t\n- ticket:\tTicket number\t\n- cabin:\tCabin number\t\n- embarked:\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton  ","5d65a696":"### 4.3 Age","df9fa896":"### 5.2.7 SVM Classifier","d0dc2c33":"## 3. Exploratory data analysis\nPrinting first 5 rows of the train dataset.","ee98b500":"more than 50% of 1st class are from S embark  \nmore than 50% of 2nd class are from S embark  \nmore than 50% of 3rd class are from S embark","bdc43c8f":"## 1. Import Relevant Libraries :-"}}