{"cell_type":{"9db5e4e4":"code","7a0fee69":"code","d2194f64":"code","e4636aad":"code","22eef90c":"code","dcd8f315":"code","606bae9a":"code","659900c5":"code","b19fb587":"code","377ef864":"code","04d43ad4":"code","94078488":"code","3b257fe8":"code","c11d0d03":"code","858791c4":"code","08cc13bb":"code","504c142a":"code","c7d0e00e":"code","565366bb":"markdown","0c4a607e":"markdown","a1424bb3":"markdown","66ca2aed":"markdown","672755df":"markdown","a459cd90":"markdown","065c44b0":"markdown","83c40f1c":"markdown","95d576b4":"markdown"},"source":{"9db5e4e4":"import numpy as np \n%matplotlib inline","7a0fee69":"import os\nimport sys","d2194f64":"print(f\"os.environ.get('PYTHONPATH') == '{os.environ.get('PYTHONPATH','Does not exist')}'\")\n!set | grep PYTHONPATH\n\nnewPath = os.environ.get('PYTHONPATH') + ':\/kaggle\/input'\nos.putenv('PYTHONPATH',newPath)\n\nprint(f\"\\nos.environ.get('PYTHONPATH') == '{os.environ.get('PYTHONPATH','Does not exist')}'\")\n!set | grep PYTHONPATH\nprint(\"-> os.putenv does NOT work for os.environ.get, but DOES change the environment variable for 'set'\")","e4636aad":"print(f\"os.environ.get('PYTHONPATH') == '{os.environ.get('PYTHONPATH','Does not exist')}'\")\n!set | grep PYTHONPATH\n\nos.environ['PYTHONPATH'] += ':\/kaeggle\/input\/pyutilities'\n\nprint(f\"\\nos.environ.get('PYTHONPATH') == '{os.environ.get('PYTHONPATH','Does not exist')}'\")\n!set | grep PYTHONPATH\nprint(\"-> os.environ can be directly modified with '+='. Works for os.environ.get and changes the environment variable for 'set'\")","22eef90c":"sys.path += [\"\/kaggle\/inputs\/pyutilities\"]\nprint(sys.path)","dcd8f315":"print('I could not find any solution how to define a new environment variable, which is also found by os.environ.get!')\n!export MY_VAR=doesNotWork\n!set | grep MY_VAR\n\nnewVar = os.environ.get('MY_VAR', 'NA') + ':this worked for set only'\nos.putenv('MY_VAR',newVar)\n\nprint()\n!set | grep MY_VAR\nprint(f\"os.environ.get('MY_VAR') = '{os.environ.get('MY_VAR','Does not exist')}'\")","606bae9a":"# Note: import still fails, although \/inputs\/pyutilities was added to PYTHONPATH!\n# Solution: copy the file to the working directory\n!cp \/kaggle\/input\/pyutilities\/utils2.py .\n\nfrom utils2 import *","659900c5":"# ----------------------------------------------------------------------------\nclass controlStrategy(controlBox):\n    \n    def __init__(self, fromFile=False, fName='.\/tmp'):\n        super().__init__()\n        self.ALPHA = 10             # Learning rate for action weights, w\n                                    # -tp-: ALPHA = 1000\/2 = 500 bec. y=+\/-1 (not +\/-0.5) -> e is doubled\n                                    # -tp- 2nd change: ALPHA=10; factor 50 included in pForce\n        self.BETA = 0.5             # Learning rate for critic weights, z\n        self.GAMMA = 0.95           # Discount factor for critic\n        self.EPS = 0.0001           # \n        self.LAMBDAw = 0.9          # Decay rate for w eligibility trace\n        self.LAMBDAz = 0.8          # Decay rate for z eligibility trace\n        self.FORCE_MAG = 10\n        \n        # define 4 default color scales for w, z, xBar and e      \n        self.colSel = ['darkred',    'brightgreen',\n                       'blue',       'bluegreen',\n                       'orange',     'yellow',\n                       'black',      'red']\n        \n        if fromFile:\n            fn = fName + f\"_{self.bins}.npz\"\n            with open(fn, 'rb') as f:\n                npzfile = np.load(f, allow_pickle=True)\n                self.w = npzfile['w'] # shape [self.bins]\n                self.z = npzfile['z'] # shape [self.bins]\n                self.xBar = npzfile['xBar'] # shape [self.bins]\n                self.e = npzfile['e'] # shape [self.bins]\n                self.oldState = npzfile['s']\n                self.iBox = self.getBox(self.oldState)\n                print(f\"\\nTrained Parameters w \/ z \/ xBar \/ e [{self.bins}] and state =\\n\",\n                      f\"{[1,1,180\/np.pi,180\/np.pi]*np.array(self.oldState)}\")\n                if debugFlag:\n                    self.histBoxes = set(npzfile['hB'])\n                    print(f\"and box-history loaded from file {fn}\\n\")\n                else:\n                    self.histBoxes = set()\n                    print(f\"loaded from file {fn}. Box-history ignored.\\n\")\n        else:\n            self.histBoxes = set()\n            self.w = np.zeros(self.bins)\n            self.z = np.ones(self.bins)*(-0.05)\n            self.xBar = np.zeros(self.bins)\n            self.e = np.zeros(self.bins)\n            self.oldState = (None, None)\n            self.iBox = 0 # initialized by call to getBox; value needed for oldP\n    \n    def action(self):\n        '''\n        Determines force, based on state (iBox) and weight w[iBox]\n        Returns:\n        1 for max. force; -1 for min. force, depending on w[iBox]\n        '''\n        pForce = lambda w: 1 \/ (1 + np.exp(50*np.max([-1, np.min([ w, 1 ])])))\n        randVal = np.random.rand()\n        return 1 if randVal > pForce(self.w[self.iBox]) else -1\n    \n    def update(self, stateVec, y): #, tH):\n        '''\n        Parameters\n        ----------\n        stateVec: state vector (x,v,th,om)\n        y: force indicator -1 for negative & 1 for positive direction\n        Output:\n        ----------\n        - updates self.iBox, based on stateVec\n        - updates all policy-parameters (w, z, xBar and e)\n        - stores self.oldP for next call\n        - returns: p and rHat\n        '''\n        # for OLD iBox, update traces after action step and remember oldP\n        # xBar_next = (xBar_prev*LAMBDAz + (1-LAMBDAz)) \n        #           = xBar_prev + a*(1-xBar_prev); a=1-LAMBDAz\n        # similarly, e_next = e_prev + b*(y-e_prev); b=1-LAMBDAw\n        # -> Sutton Barto p57 (2.9); Barto1983 p841 (3)\n        # Note: the limit t->inf would be 1 for xBar and y for e, \n        #   IF the state remains in the same iBox. \n        #   In practice, the decay (*LAMBDAw\/z, below) drives the value to zero.\n        self.e[self.iBox] += (1 - self.LAMBDAw) * y; # eligibility; LAMBDAw = 0.9\n        self.xBar[self.iBox] += (1 - self.LAMBDAz)   # LAMBDAz = 0.8\n        self.oldP = self.z[self.iBox] \n        \n        # update self.iBox based on stateVec\n        _ = self.getBox(stateVec, update=True) \n        \n        # Failure likelihood estimate = -(r+p) = 1 if failure else z[iBox]\n        if self.failed: # Failure occurred\n            r = -1\n            p = 0\n        else:           # Not a failure\n            # Reinforcement is zero. Prediction of failure given by z weight\n            r = 0\n            p = self.z[self.iBox] # = prediction of eventual reinforcement\n        \n        # Heuristic reinforcement: rHat =\n        #  failure reinforcement + gamma * new failure pred - prev failure pred\n        rHat = r + self.GAMMA * p - self.oldP    # GAMMA = 0.95; discount factor, provides decay of prediction\n        \n        # Update ALL weights\n        self.w = (1-1e-6)*self.w + self.ALPHA * rHat * self.e       # ALPHA = 10\n        self.z += self.BETA  * rHat * self.xBar    # z = prediction of reinforcement; BETA  = 0.5\n        self.z[self.iBox] -= self.BETA*self.GAMMA*self.EPS\n        if np.min(self.z) < -1:\n            print(f'Warning: {np.sum(self.z<-1)} z-elements < -1 encountered!')\n            self.z[self.z<-1] = -1\n        # Note: If oldP > 2.95*zP, then z will become positive!\n        assert np.max(self.z) < 0, f\"Error: bin {np.argmax(self.z)} has positive z-value: {np.max(self.z)}; oldP {self.oldP}!\\nPlease re-start for another try!\"\n        \n        if self.failed: # If failure, zero traces e\/xBar for NEW iBox\n            self.e[self.iBox] = 0\n            self.xBar[self.iBox] = 0\n            \n        # Update (decay) ALL traces\n        self.e *= self.LAMBDAw                      # LAMBDAw = 0.9\n        self.xBar *= self.LAMBDAz                   # LAMBDAz = 0.8\n        \n        return p, rHat\n    \n    # end of class controlStrategy(controlBox)","b19fb587":"# Note: base_288.npz contains successful parameters, which were generated with the program below (unchanged).\n#       You may need several runs to achieve 100k steps!\n# If you want to start 'from scratch' just comment out the copy-statement and set fromFile=False; and vice versa.\n\n!cp \/kaggle\/input\/pyutilities\/base_288.npz .\/base_288.npz\nfromFile = True\n!ls -algtr .\n#debugFlag = False # tracking of state-parameters, traces, weights etc. and display after run","377ef864":"    MAX_STEPS = 99999\n    MAX_FAILURES = 500\n\n    useState = True\n    policy = controlStrategy(fromFile=fromFile, fName='.\/base')\n    initS = policy.oldState if useState else (None,None)\n    state = stateInc(initState=initS)\n    \n    # initialize iBox\n    _ = policy.getBox(state.get(), update=True)\n\n    if debugFlag:\n        binCount = np.zeros(policy.bins)\n        binCount[policy.iBox] += 1\n        histLength = MAX_STEPS+1\n        history = np.ones((histLength, 6))*[policy.iBox, *state.get(), 0]\n        histIdx = 0\n        p1000Queue = np.zeros((1000,4,policy.bins))\n        queueIndex = 0 \n        p1000Queue[0] = [policy.w, policy.z, policy.xBar, policy.e]\n        prHat = np.zeros((1000, 2)) # collection of p and rHat values\n        prHatIdx = 0\n        prHat[0] = [0,0] # ignore the -1 reinforcement for plotting purposes","04d43ad4":"    # RL-loop until either max-failures or max-steps (= success) has been reached\n    while policy.failures < MAX_FAILURES:\n        policy.failed = False\n        for stp in range(MAX_STEPS):\n    \n            # get action\n            y = policy.action()\n            \n            # Apply action to the simulated cart-pole\n            f = state.step(y)\n            \n            # update policy states w, z, xBar, e and iBox\n            #tH = set(history[:,0])\n            pVal, rHat = policy.update(state.get(), y) #, tH = tH)\n            \n            if debugFlag:\n                binCount[policy.iBox] += 1\n                histIdx = (histIdx + 1) % histLength\n                history[histIdx] = [policy.iBox, *state.get(), f]\n                queueIndex = (queueIndex + 1) % 1000\n                p1000Queue[queueIndex] = [policy.w, policy.z, policy.xBar, policy.e]\n                prHatIdx = (prHatIdx + 1) % 1000\n                prHat[prHatIdx] = [pVal, rHat]\n            \n            # exit stp-loop, if out of bounds failure occurred\n            if policy.failed:\n                print(f\"Trial {policy.failures} was {stp+1} steps long\")\n                if policy.failures != MAX_FAILURES:\n                    #print(\"Resetting state.\")\n                    state.reset()\n                    _ = policy.getBox(state.get(), update=True)\n                    if debugFlag:\n                        history[histIdx] = [policy.iBox, *state.get(), f]\n                        prHat[prHatIdx, 1] += 1 # add 1 to remove r_t=-1; for plotting of prHat\n                break\n            \n        # end of stp-loop\n        stp += 1\n        if stp==MAX_STEPS and not policy.failed:\n            break # exit loops, SUCCESS!\n            \n        # reset binCount (comment out to determine total count)\n        if debugFlag and (policy.failures != MAX_FAILURES):\n            binCount = np.zeros(policy.bins) \n            binCount[policy.iBox] += 1\n        \n    # end of failure-loop\n    \n    if (policy.failures == MAX_FAILURES):\n        print(f\"Pole not balanced. Stopping after {policy.failures} failures.\")\n    else:\n        print(f\"\\nSUCCESS: After {policy.failures} failures, pole balanced \",\n              f\"successfully for >{stp} steps.\")","94078488":"    topBins = [None]\n    histBoxes = [None]\n    if debugFlag:\n        # shift history-buffer by (histOffset-stp)%histLength \n        # history[0] = first timestep of last iteration\n        history = shiftList(history, histIdx-stp)\n\n        print(f'Initial state: x={history[0,1]:.5} v={history[0,2]:.5} ',\n              f'th={history[0,3]\/np.pi*180:.5} om={history[0,4]\/np.pi*180:.5}')\n        print(f'Final state:   x={history[stp,1]:.5} v={history[stp,2]:.5} ',\n              f'th={history[stp,3]\/np.pi*180:.5} om={history[stp,4]\/np.pi*180:.5}')\n        \n        # shift ring-buffer to get time-sequence from index 0..999\n        p1000Queue = shiftList(p1000Queue, queueIndex-stp)\n        # transpose to get [0] = bin, [1] = (w,z,xB,e), [2] = t]\n        p1000Queue = p1000Queue.transpose(2,1,0)\n        \n        # shift p-rHat-stateBin history\n        prHat = shiftList(prHat, prHatIdx-stp)\n        \n        nVals = 1000\n        plotHist(history, stp, nV=nVals, bins=policy.bins, prH=prHat)\n    \n        # truncate history to last nVals timesteps and get min\/max state values\n        lenHist = stp + 1 # since history[stp] is the last valid entry\n        lastNvals = lenHist - nVals if lenHist>nVals else 0\n        history = np.array( history[lastNvals:lenHist] )\n        _, x0, v0, th0, om0, _ = np.min(history, axis=0)\n        _, x1, v1, th1, om1, _ = np.max(history, axis=0)\n        policy.histBoxes = set(history[:,0])\n        policy.histBoxes.discard(-1)\n        print(f'\\n{len(policy.histBoxes)} bins were visited during last',\n              f'{lenHist-lastNvals} timesteps.', \n              f'\\nx-Range    \\t[{x0:.5}, {x1:.5}]\\tm\/s',\n              f'\\ntheta-Range\\t[{th0\/np.pi*180:.5}, {th1\/np.pi*180:.5}]\\tdeg',\n              f'\\nv-Range    \\t[{v0:.5}, {v1:.5}]\\tm\/s',\n              f'\\nomega-Range\\t[{om0\/np.pi*180:.5}, {om1\/np.pi*180:.5}]\\tdeg\/s')\n    \n        topBins, tresholdBinFreq, nSum = \\\n            paretoChart(binCount, truncate=True, treshold=80)\n        print(f'\\nVisit-frequency of treshold bin {topBins[-1]}:',\n              f'{tresholdBinFreq*100:.4}% of total bin-visits (={nSum})\\n')\n        for rank, i in enumerate(topBins):\n            stateI = [1, 1, 180\/np.pi, 180\/np.pi] * np.array(policy.midState[i])\n            print(f'({rank+1}) Bin {i} has midpoint',\n                  f'[{stateI[0]:.3}, {stateI[1]:.3}, {stateI[2]:.3}, {stateI[3]:.3}]',\n                  f'w = {policy.w[i]:.3}, z = {policy.z[i]:.3}, e = {policy.e[i]:.3}, xBar = {policy.xBar[i]:.3}')\n        \n        print(f'\\n(w, z, xBar, e)-history of top-{len(topBins)} visited states:')\n        policy.histogram(figsize=(8,8), stateBins=topBins)\n\n    histBoxes = policy.histBoxes\n    policy.visualize(policy.histBoxes)\n    printTop80 = f'\\nTop-80% visited states: {len(topBins)}'\n    if (policy.failures != MAX_FAILURES):\n        policy.saveParameters(state=state.get(), fName='.\/base')\n        if debugFlag:\n            policy.histBoxes = policy.symParameters(topBins)\n            print('\\nAfter updating states, based on the symmetry-rule:')\n            policy.visualize(policy.histBoxes, legend=False)\n            print(f\"(The {len(policy.histBoxes)} states include top-{len(topBins)} states plus their 'sym-states')\")\n            policy.saveParameters(state=state.get(), fName='.\/sym')\n            printTop80 = f'\\nTop-80% visited states (incl. sym. states): {len(topBins)} ({len(policy.histBoxes)})'\n        \n    if debugFlag:\n        plotParaHist(p1000Queue, topBins, history, lenHist)\n        print(printTop80)\n        print(f'\\nNumber of states visited during last 1,000 steps: {len(histBoxes)}')\n\n    print(\"Done.\")","3b257fe8":"getBin((0,0,0,0))","c11d0d03":"xStateBins(getBin((0,0,0,0)))","858791c4":"vStateBins(187)","08cc13bb":"symBin(187)","504c142a":"getMidPoint(0)","c7d0e00e":"        state = stateInc(initState=(0,0.1,0,0))\n        newCartMu = 0.07\n        newPoleMu = 0.025\n        textCart = ''\n        textPole = ''\n        if state.cartTrackMu != newCartMu:\n            textCart = f'\\nCart: from {state.cartTrackMu} to {newCartMu}'\n        if state.poleCartMu != newPoleMu:\n            textPole = f'\\nPole: from {state.poleCartMu} to {newPoleMu}'\n        if textCart+textPole != '':\n            print('Friction values changed from default:', textCart, textPole)\n        state.cartTrackMu = newCartMu\n        state.poleCartMu = newPoleMu\n        n = 1000\n        y = np.zeros((n, 4))\n        y[0] = state.get()\n        for i in range(1,n):\n            state.step(0)\n            y[i] = state.get()\n        \n        x = state.dT * np.array(range(n))\n        \n        plt.figure(figsize=(10,8))\n        plt.subplot(4,1,1)\n        plt.plot(x, y[:,0])\n        plt.subplot(4,1,2)\n        plt.plot(x, y[:,1])\n        plt.subplot(4,1,3)\n        plt.plot(x, y[:,2]*180\/np.pi)\n        plt.subplot(4,1,4)\n        plt.plot(x, y[:,3]*180\/np.pi)\n        \n        plt.show() ","565366bb":"(2) Demon state is current active state: $ x_{i,t} = 1 \\land x_{i,t-1} = 0$\n\n$\ne_{i,t+1} = e_{i,t} + \\left( 1 - \\delta \\right) \\left( y_i - e_{i,t} \\right)  \\\\\n\\bar{x}_{i,t+1} = \\bar{x}_{i,t} + \\left( 1 - \\lambda \\right) \\left( 1 - \\bar{x}_{i,t} \\right)  \\\\\n\\hat{r}_t = \\gamma \\left( z_{i,t} - {\\color{red}\\epsilon}\\right)  - p_{t-1} + r_t \\\\\nz_{i,t+1} = \\left( 1 + \\beta \\gamma \\right) z_{i,t} + \\beta \\left( r_t - p_{t-1} \\right) \n- \\beta \\gamma {\\color{red}\\epsilon} \n\\to \\left( \\frac{p_{t-1} - r_t}{\\gamma} - {\\color{red}\\epsilon} \\right)\n$\n\nSpecial case: $r_t = -1 \\\\\n\\hat{r}_t \\approx -1 \\\\\nw_{i,t+1} \\approx w_{i,t} - \\alpha e_{i,t} \\to \\pm \\infty; \\quad\nz_{i,t+1} \\approx z_{i,t} - \\beta \\bar{x}_{i,t} \\to - \\infty\n$\n\nNote that all three limits - by definition of the case - do not make sense. However, a typical stable control will end up in a limit cycle. So, alternation between the first and the second case is possible for some short period of time.","0c4a607e":"(3) Demon state is current and was previous active state: $x_{i,t} = x_{i,t-1} = 1$\n\n$\n\\hat{r}_t = \\gamma \\left( z_{i,t} - {\\color{red}\\epsilon}\\right) - z_{i,t-1} + r_t\n$\n\n$\nz_{i,t+1} = \\left( 1 + \\beta \\gamma \\right) z_{i,t} + \\beta \\left( r_t - z_{i,t-1} \\right) \n- \\beta \\gamma {\\color{red}\\epsilon} \n\\to \\left( - \\frac{ \\gamma {\\color{red}\\epsilon} - r_t}{1 - \\gamma}  \\right) \n=\n  \\begin{cases}\n    \\frac{ \\gamma{\\color{red}\\epsilon} }{\\gamma - 1}  = -0.019      & \\quad \\text{if } r_t = 0\\\\\n    \\frac{ \\gamma{\\color{red}\\epsilon} + 1 }{\\gamma - 1} = -19.98  & \\quad \\text{if } r_t = -1\n  \\end{cases}\n$\n \nTherefore, the parameter $ {\\color{red}  \\epsilon} $ prevents diminishing $z$-values.\n\nParameters: $\\alpha=10, \\beta=0.5, \\gamma=0.95, \\delta=0.9, \\lambda=0.8, \\epsilon=0.001, \\kappa=50, F=10\\text{N}$","a1424bb3":"**Controller output:**\n\nNote that \n\n$\n\\sum_{i=1}^{n} x_{i,t} w_{i,t} = w_{j_t,t}. \n$\n\nLet \n$\nc_t = e^{-\\kappa w_{j_t,t}}\n$ \nthen the controller output is\n\n$\ny_t =\n  \\begin{cases}\n    1       & \\quad \\text{if $\\displaystyle \\frac{1}{1 - c_t} \\ge \\xi$} \\\\\n   -1      & \\quad \\text{otherwise}\n  \\end{cases}\n$\n\nWith the uniform distributed random variable $\\xi \\in \\mathsf{U} (0,1)$.\n\nThe force $f_t$ is then determined by $f_t = F y_t$, where $F$ is the absolute value of the bang-bang control force applied on the cart.\n","66ca2aed":"Based on\n \n Barto, Sutton, and Anderson, \"Neuronlike Adaptive Elements That Can Solve\n Difficult Learning Control Problems,\" IEEE Trans. Syst., Man, Cybern.,\n Vol. SMC-13, pp. 834--846, Sept.--Oct. 1983.\n (http:\/\/www.derongliu.org\/adp\/adp-cdrom\/Barto1983.pdf)\n \n C-Code \n http:\/\/incompleteideas.net\/sutton\/book\/code\/pole.c\n \n - Friction based on R.V. Florian - 2007 https:\/\/coneural.org\/florian\/papers\/05_cart_pole.pdf. Additionally, a stick-slip effect is added.\n \n - Bins changed; now 288 bins:\n        xBox = np.array([-0.8, 0.8])\n        thetaBox = np.array([-6, -1, 0, 1, 6]) * np.pi \/ 180\n        vBox = np.array([-0.5, 0, 0.5])\n        omegaBox = np.array([-50, 0, 50]) * np.pi \/ 180","672755df":"Below is a simulation, with force=0 and initial state = (0, 0.1, 0, 0)","a459cd90":"**Three cases for calculating the next demon-state**\n\n(1) Demon state is not current active state: $ x_{i,t} = 0$\n\n$\ne_{i,t+1} = \\delta e_{i,t} \\to 0 \\\\\n\\bar{x}_{i,t+1} = \\lambda \\bar{x}_{i,t} \\to 0\n$\n\nSpecial case, when failure occurs: $r_t = -1 \\\\\n\\hat{r}_t \\approx -1 \\\\\nw_{i,t+1} \\approx w_{i,t} - \\alpha e_{i,t}; \\quad\nz_{i,t+1} \\approx z_{i,t} - \\beta \\bar{x}_{i,t} \n$","065c44b0":"**Notes regarding the classes defined in \/kaggle\/input\/pyutilities\/utils2.py:**\n\nThe Class **stateInc** implements the dynamics of the cart and pole. It can be found in **utils2.py**.\n\n* Input: Action ('bang-bang control' force +10N or -10N) and current state. \n* Output: Next state \n\nThe Class **controlStrategy** (below) implements the reinforcement learning system. \nIt inherits from class **controlBox**, which contains the 'boxes' system and several utility functions; see **utils.py**.","83c40f1c":"**Reinforcement Learning example: cartPole-control with state-boxes**\n\nThomas Plocher \n\nSeptember 27, 2020\n\n**Formulas for propagation of weights, traces and reinforcement**\n\nStates:\n\nWith $n$ state-boxes $x_{i,t}$, let $j_t$ denote the system state at time $t$ such that for\n$1 \\le i \\le n: \\quad\nx_{i,t} =\n  \\begin{cases}\n   1      & \\quad \\text{if $i=j_t$} \\\\\n   0      & \\quad \\text{otherwise}\n  \\end{cases} $\n \nTraces:\n\n$\ne_{i,t+1} = e_{i,t} + \\left( 1 - \\delta \\right) \\left( y_t x_{i,t} - e_{i,t} \\right)  \\\\\n\\bar{x}_{i,t+1} = \\bar{x}_{i,t} + \\left( 1 - \\lambda \\right) \\left(  x_{i,t} - \\bar{x}_{i,t} \\right)\n$\n\nTrace $i$ is activated when the system enters state $x_i$. If the system continuously remains in the state $i$, then $e_{i,t} \\to y_t$ and $\\bar{x}_{i,t} \\to 1$. After exiting state $i$, $x_{i,t}=0$, and the traces of box $i$ will exponentially decay to zero. \n\nWeights:\n\n$\nw_{i,t+1} = w_{i,t} + \\alpha \\hat{r}_t e_{i,t} \\\\\nz_{i,t+1} = z_{i,t} + \\beta \\hat{r}_t \\bar{x}_{i,t}\n$\n\nPrediction of reinforcement:\n\n$\np_t = \\sum_{i=1}^{n} x_{i,t} z_{i,t} = z_{j_t,t}\n$\n\nReinforcement estimate:\n\n$\n\\hat{r}_t = \\gamma \\left( p_t - {\\color{red}\\epsilon}\\right) - p_{t-1} + r_t \n$\n\nNote: The parameter $ {\\color{red}  \\epsilon} $ has been introduced to prevent diminishing $z$-weights.\n\nThe external reinforcement $r_t = -1$ is triggered by system failure (position or angle exceeding limits). In normal control mode, $r_t = 0$.","95d576b4":"**utils2.py** also contains some utility functions, e.g.:"}}