{"cell_type":{"ccf475f5":"code","0652584b":"code","dd77e70b":"code","10d88ba1":"code","8df3d1e5":"code","561e31a8":"code","f6de4c49":"code","dfbc5e92":"code","a93491f2":"code","e2733b2d":"code","2092521d":"code","3ab6cd17":"code","a77598d2":"code","fca04b7d":"code","c61e277e":"code","3ee638fb":"code","8b9dbfc4":"code","26663266":"code","a9cdcce8":"code","805ef19b":"code","864c4dcc":"code","bcc03358":"code","afdc9458":"code","183d98f6":"code","a69a2fe6":"code","31915357":"code","dbdad149":"code","5e1894e2":"code","ca0b8894":"code","0c0098e6":"code","1a03ec39":"markdown","543e9bec":"markdown","25287747":"markdown","79f8c386":"markdown","ac60d8b8":"markdown","8b260be9":"markdown","5c4a694e":"markdown"},"source":{"ccf475f5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelBinarizer, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\n\nfrom sklearn.svm import SVC \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score as score\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n\n \n\n","0652584b":"#loading and looing at the data \ntrain_data =pd.read_csv(\"..\/input\/train.csv\", parse_dates =['Dates'])\ntest_data = pd.read_csv(\"..\/input\/test.csv\", parse_dates =['Dates'])\n\nprint(\"The size of the train data is:\", train_data.shape)\nprint(\"The size of the test data is:\", test_data.shape)","dd77e70b":"#Lets take a look at the train set\ntrain_data.head()","10d88ba1":"#Take a look at the test data set\ntest_data.head()","8df3d1e5":"train_data.dtypes.value_counts()","561e31a8":"test_data.dtypes.value_counts()","f6de4c49":"#First we check for any nans or missing values\ntrain_data.isnull().sum()","dfbc5e92":"test_data.isnull().sum()","a93491f2":"train_data.columns","e2733b2d":"test_data.columns","2092521d":"#The category is what we need to predict\ntrain_data.Category.value_counts()","3ab6cd17":"#encoding the Category features\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\ntrain_data['Category'] = le.fit_transform(train_data.Category)\ntrain_data.Category.head()\n","a77598d2":"train_data.PdDistrict.value_counts()","fca04b7d":"#i will use the panda get dummies to one hot encode the remaining categorical features\nfeature_cols =['DayOfWeek', 'PdDistrict']\ntrain_data = pd.get_dummies(train_data, columns=feature_cols)\ntest_data = pd.get_dummies(test_data, columns=feature_cols)\n\ntrain_data","c61e277e":"test_data","3ee638fb":"for x in [train_data, test_data]:\n    x['years'] = x['Dates'].dt.year\n    x['months'] = x['Dates'].dt.month\n    x['days'] = x['Dates'].dt.day\n    x['hours'] = x['Dates'].dt.hour\n    x['minutes'] = x['Dates'].dt.minute\n    x['seconds'] = x['Dates'].dt.second","8b9dbfc4":"train_data.head()","26663266":"\ntest_data.head()","a9cdcce8":"#no need for Dtes column anymore so we drop it\n#i will also dropthe adresses on both data\ntrain_data = train_data.drop(['Dates', 'Address','Resolution'], axis = 1)","805ef19b":"train_data = train_data.drop(['Descript'], axis = 1)\ntrain_data.head()","864c4dcc":"test_data = test_data.drop(['Dates', 'Address'], axis = 1)\ntest_data.head()","bcc03358":"#First up spitting the data into train and validation sets\n\nfeature_cols = [x for x in train_data if x!='Category']\nX = train_data[feature_cols]\ny = train_data['Category']\nX_train, x_test,y_train, y_test = train_test_split(X, y)","afdc9458":"#Logisticregressioncv\nLR_L2 = LogisticRegression()\nLR_L2 = LR_L2.fit(X_train, y_train)\ny_pred_LR = LR_L2.predict(x_test)\ny_pred_test_LR = LR_L2.predict(X_train)\n\n\nprint(\"score is {:.3f}\".format (score(y_test, y_pred_LR, average = 'micro')*100))\nprint(\"Accuracy for the test data is: {:.3f} \".format (accuracy_score(y_test, y_pred_LR)*100))\nprint(\"Accuracy for the train data is: {:.3f} \".format (accuracy_score(y_train, y_pred_test_LR)*100))\n","183d98f6":"#Naive bayes - best for very large datasets\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\ny_pred_gnb = gnb.predict(x_test)\ny_pred_test_gnb = gnb.predict(X_train)\n\nprint(\"score is {:.3f}\".format (score(y_test, y_pred_gnb, average = 'micro')*100))\nprint(\"Accuracy for the test data is {:.3f}\".format (accuracy_score(y_test, y_pred_gnb)*100))\nprint(\"Accuracy for the train data is {:.3f} \".format (accuracy_score(y_train, y_pred_test_gnb)*100))\n","a69a2fe6":"#SVC since there are so manyfaetures (>100k) i will use it with a nystroem kernel\nfrom sklearn.kernel_approximation import Nystroem\nnystroemSVC = Nystroem(kernel = 'rbf')\nsgd = SGDClassifier()\n\nX_train_svc = nystroemSVC.fit_transform(X_train)\nX_test_svc = nystroemSVC.transform(x_test)\n\nlinSVC = sgd.fit(X_train_svc, y_train)\ny_pred_svc = linSVC.predict(X_test_svc)\ny_pred_test_svc = linSVC.predict(X_train_svc)\n\nprint(\"score is {:.3f}\".format (score(y_test, y_pred_svc, average = 'micro')*100))\nprint(\"Accuracy for the test data is {:.3f}\".format (accuracy_score(y_test, y_pred_svc)*100))\nprint(\"Accuracy for the train data is {:.3f}\".format (accuracy_score(y_train, y_pred_test_svc)*100))\n\n","31915357":"#Decision Tree\nDTC = DecisionTreeClassifier(criterion = 'gini', max_features = 10, max_depth = 5)\nDTC = DTC.fit(X_train, y_train)\ny_pred_DTC = DTC.predict(x_test)\ny_pred_test_DTC = DTC.predict(X_train)\n\nprint(\"score is {:.3f}\".format (score(y_test, y_pred_DTC, average = 'micro')*100))\nprint(\"Accuracy for the test data is {:.3f} \".format (accuracy_score(y_test, y_pred_DTC)*100))\nprint(\"Accuracy for the train data is {:.3f} \".format (accuracy_score(y_train, y_pred_test_DTC)*100))\n","dbdad149":"# #Random Forest\n# RC =RandomForestClassifier(n_estimators = 100, max_features = 10)\n# RC =RC.fit(X_train, y_train)\n# y_pred_RC = RC.predict(x_test)\n# y_pred_proba_RC = RC.predict_proba(x_test)\n\n# print(\"score is {}\".format (score(y_test, y_pred_RC, average = 'micro')))\n# print(\"Accuracy is {}\".format (accuracy_score(y_test, y_pred_RC)))\n","5e1894e2":"#so we know we are going to use the decision tree\n#lets creat a submission form in a form needed\n","ca0b8894":"#for now the best model with an accuracy of ~24 is the deciaion tree\n\nX_test =test_data.drop(['Id'], axis = 1)\n\nmy_prediction = DTC.predict(X_test)","0c0098e6":"SFCC_submission_final = pd.DataFrame({'Id': test_data.Id, 'Category': my_prediction})\nprint(SFCC_submission_final.shape)\nSFCC_submission_final.to_csv('SFCC_prediction.csv', index = False)","1a03ec39":"While the test data has 4 object, 2 floats(longitudes and latitudes), and 1 int(id)","543e9bec":"No missing data in both trainig and test data. Sso we can proceed to data preprocessing","25287747":"We can see that the train data has 7 objects and 2 floats(longitude and latitudes)","79f8c386":"##Neural networks with Keras","ac60d8b8":"# data cleaning and processing","8b260be9":"For this classification problem.\nI will be using thye following classification algorithms:\n\n1-Logistic Regression\n2-Niave Bayes\n3-SVMS \n4-Decision trees\n5-Random forest\n6-Neural nets    ","5c4a694e":"## Modelling"}}