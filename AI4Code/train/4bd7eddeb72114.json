{"cell_type":{"f124dcfc":"code","77878d28":"code","e8b53614":"code","4774028e":"code","7302953e":"code","7fb3807b":"code","7554aec9":"code","b84cb62d":"code","604e9ca2":"code","584e1668":"code","d8dff1b2":"code","7e986d25":"code","3b8cd5c6":"code","6e8bb148":"code","2eafc7dc":"code","3b36033c":"code","fa907f3a":"code","c8db4a93":"code","6c0d340c":"code","53e4bd95":"code","6b8b83a7":"code","0c9c6058":"code","1bd73d22":"code","6eb9a7b9":"code","9ec92d3c":"code","4558b0b6":"code","830b6ef2":"code","b4a78d36":"code","857d5959":"code","718a0b2d":"markdown","8f87f0e9":"markdown","2eba85d5":"markdown","502c074a":"markdown","f7b8f6da":"markdown","92c96dc6":"markdown","4325cefd":"markdown","b535f65b":"markdown","a5142b21":"markdown","30d8ab64":"markdown","dfe6679f":"markdown","40e00465":"markdown","405d32d4":"markdown","31cae118":"markdown","cff728af":"markdown","b109cc1e":"markdown","db8451ab":"markdown","4b58bab4":"markdown","cb362e07":"markdown","16a081d2":"markdown","77e93bae":"markdown","d843525c":"markdown","3bb7bdd8":"markdown","f71e7954":"markdown","a271f2bf":"markdown"},"source":{"f124dcfc":"!pip install kaggle-environments --upgrade -q","77878d28":"from kaggle_environments import make\nenv = make(\"mab\", debug=True)","e8b53614":"# best of five testing\ndef bo5(file1, file2):\n    env = make(\"mab\", debug=True)\n\n    total_1 = 0\n    total_2 = 0\n    for i in range(5):\n        env.run([file1, file2])\n        p1_score = env.steps[-1][0]['reward']\n        p2_score = env.steps[-1][1]['reward']\n        env.reset()\n        print(f\"Round {i+1}: {p1_score} - {p2_score}\")\n        total_1 += p1_score\n        total_2 += p2_score\n            \n    print(f\"\\nMean Scores: {total_1\/5} - {total_2\/5}\")","4774028e":"agent_file = \"optimistic_epsilon_greedy.py\"","7302953e":"%%writefile {agent_file}\n\nimport math\nimport numpy as np\nimport random\n\n\n\"\"\"\n    Helper Functions\n\"\"\"\n\n# return the index of the largest value in the supplied list\n# - arbitrarily select between the largest values in the case of a tie\n# (the standard np.argmax just chooses the first value in the case of a tie)\ndef random_argmax(value_list):\n  \"\"\" a random tie-breaking argmax\"\"\"\n  values = np.asarray(value_list)\n  return int(np.argmax(np.random.random(values.shape) * (values==values.max())))","7fb3807b":"%%writefile -a {agent_file}\n\nclass CandyMachine:\n    \"\"\" the base candy machine class \"\"\"\n    \n    Q = 0   # the estimate of this machine's reward value                \n    n = 0   # the number of times this machine has been tried      \n    \n    def __init__(self, **kwargs):       \n        # get the initial estimate from the kwargs\n        self.initial_estimate = kwargs.pop('initial_estimate', 0.)         \n        self.initialize() # reset the machine                         \n        \n    def initialize(self):        \n        # estimate of this machine's reward value \n        # - set to supplied initial value\n        self.Q = self.initial_estimate                  \n        \n        # the number of times this machine has been tried \n        # - set to 1 if an initialisation value is supplied\n        self.n = 1 if self.initial_estimate  > 0 else 0        \n        \n                    \n    def update(self,R):\n        \"\"\" update this machine after it has returned reward value 'R' \"\"\"     \n    \n        # increment the number of times this machine has been tried\n        self.n += 1\n\n        # the new estimate of the mean is calculated from the old estimate\n        self.Q = (1 - 1.0\/self.n) * self.Q + (1.0\/self.n) * R\n    \n    def sample(self):\n        \"\"\" return an estimate of the machine's reward value \"\"\"\n        return self.Q","7554aec9":"%%writefile -a {agent_file}\n\nclass MachineTester():\n    \"\"\" create and test a set of machines over a single test run \"\"\"\n\n    # the index of the last machine chosen\n    machine_index = -1    \n    \n    # the total reward accumulated so far\n    total_reward = 0\n    \n    def __init__(self, configuration, **kwargs):\n        self.machine_count = configuration.banditCount        \n        self.machines = [CandyMachine(**kwargs) for i in range(self.machine_count )]\n        \n    def __call__(self, observation):  \n        \n        if self.machine_index > -1:\n            # the observation reward is the total reward plus the last reward received\n            # - subtract the total reward to the find the reward received from the last machine\n            machine_reward = observation.reward - self.total_reward        \n\n            # update reward estimate of the machine that was last used\n            self.machines[self.machine_index].update( machine_reward )\n\n            # update the total reward\n            self.total_reward = observation.reward\n        \n        # choose a new machine\n        self.machine_index = self.select_machine()\n        return self.machine_index\n\n    def select_machine( self ):\n        \"\"\" choose the machine with the current highest mean reward \n            or arbitrarily select a machine in the case of a tie \"\"\"\n        return random_argmax([machine.sample() for machine in self.machines])  ","b84cb62d":"%%writefile -a {agent_file}\n\nclass EpsilonGreedyTester( MachineTester ):\n\n    def __init__(self, configuration, **kwargs ):  \n        \n        # create a machine tester\n        super().__init__(configuration, **kwargs) \n        \n        # get the probability of selecting a non-greedy action from the kwargs\n        self.epsilon = kwargs.pop('epsilon', 0.0)        \n        \n    \n    def select_machine( self ):\n        \"\"\" Epsilon-Greedy Selection\"\"\"\n        \n        # probability of selecting a random machine\n        p = np.random.random()\n\n        # if the probability is less than epsilon then a random machine is chosen from the complete set\n        if p < self.epsilon:\n            machine_index = np.random.choice(self.machine_count)\n        else:\n            # choose the machine with the current highest mean reward or arbitrary select a machine in the case of a tie            \n            machine_index = random_argmax([machine.sample() for machine in self.machines])                 \n        \n        return machine_index","604e9ca2":"%%writefile -a {agent_file}\n\nmachine_tester = None\n\ndef agent(observation, configuration):    \n    global machine_tester    \n    if machine_tester is None: \n        machine_tester = EpsilonGreedyTester(configuration,initial_estimate = 1.1,epsilon = 0.1)                \n    return machine_tester(observation) ","584e1668":"# play against the default agent that's provided by the competition\nenv.run([\"..\/input\/santa-2020\/submission.py\", f\"{agent_file}\"])\nenv.render(mode=\"ipython\", width=800, height=800)","d8dff1b2":"# best of 5\nprint(f'Default vs {agent_file}')\nbo5(\"..\/input\/santa-2020\/submission.py\", f\"{agent_file}\")","7e986d25":"agent_file = \"upper_confidence_bounds.py\"","3b8cd5c6":"%%writefile {agent_file}\n\nimport math\nimport numpy as np\nimport random\n\n\n\"\"\"\n    Helper Functions\n\"\"\"\n\n# return the index of the largest value in the supplied list\n# - arbitrarily select between the largest values in the case of a tie\n# (the standard np.argmax just chooses the first value in the case of a tie)\ndef random_argmax(value_list):\n  \"\"\" a random tie-breaking argmax\"\"\"\n  values = np.asarray(value_list)\n  return int(np.argmax(np.random.random(values.shape) * (values==values.max())))","6e8bb148":"%%writefile -a {agent_file}\n\nclass CandyMachine:\n    \"\"\" the base candy machine class \"\"\"\n    \n    Q = 0   # the estimate of this machine's reward value                \n    n = 0   # the number of times this machine has been tried      \n    \n    def __init__(self, **kwargs):       \n        # get the initial estimate from the kwargs\n        self.initial_estimate = kwargs.pop('initial_estimate', 0.)         \n        self.initialize() # reset the machine                         \n        \n    def initialize(self):        \n        # estimate of this machine's reward value \n        # - set to supplied initial value\n        self.Q = self.initial_estimate                  \n        \n        # the number of times this machine has been tried \n        # - set to 1 if an initialisation value is supplied\n        self.n = 1 if self.initial_estimate  > 0 else 0        \n        \n                    \n    def update(self,R):\n        \"\"\" update this machine after it has returned reward value 'R' \"\"\"     \n    \n        # increment the number of times this machine has been tried\n        self.n += 1\n\n        # the new estimate of the mean is calculated from the old estimate\n        self.Q = (1 - 1.0\/self.n) * self.Q + (1.0\/self.n) * R\n    \n    def sample(self):\n        \"\"\" return an estimate of the machine's reward value \"\"\"\n        return self.Q","2eafc7dc":"%%writefile -a {agent_file}\n\nclass UCBCandyMachine( CandyMachine ):\n\n    def __init__( self, **kwargs ):    \n        \"\"\" initialize the UCB Candy Machine \"\"\"                  \n        \n        # store the confidence level controlling exploration\n        self.confidence_level = kwargs.pop('confidence_level', 2.0)       \n                \n        # initialize the base Candy Machine\n        super().__init__()           \n        \n    def uncertainty(self, t): \n        \"\"\" calculate the uncertainty in the estimate of this machine's mean \"\"\"\n        if self.n == 0: return float('inf')                         \n        return self.confidence_level * (np.sqrt(np.log(t) \/ self.n))         \n        \n    def sample(self,t):\n        \"\"\" the UCB reward is the estimate of the mean reward plus its uncertainty \"\"\"\n        return self.Q + self.uncertainty(t)","3b36033c":"%%writefile -a {agent_file}\n\nclass MachineTester():\n    \"\"\" create and test a set of machines over a single test run \"\"\"\n\n    # the index of the last machine chosen\n    machine_index = -1    \n    \n    # the total reward accumulated so far\n    total_reward = 0\n    \n    def __init__(self, configuration, **kwargs):\n        self.machine_count = configuration.banditCount        \n        self.machines = [UCBCandyMachine(**kwargs) for i in range(self.machine_count )]\n        \n    def __call__(self, observation):  \n        \n        if self.machine_index > -1:\n            # the observation reward is the total reward plus the last reward received\n            # - subtract the total reward to the find the reward received from the last machine\n            machine_reward = observation.reward - self.total_reward        \n\n            # update reward estimate of the machine that was last used\n            self.machines[self.machine_index].update( machine_reward )\n\n            # update the total reward\n            self.total_reward = observation.reward\n        \n        # choose a new machine\n        self.machine_index = self.select_machine(observation.step)\n        return self.machine_index\n\n    def select_machine( self, t ):\n        \"\"\" choose the machine with the current highest mean reward \n            or arbitrarily select a machine in the case of a tie \"\"\"\n        return random_argmax([machine.sample(t+1) for machine in self.machines])  ","fa907f3a":"%%writefile -a {agent_file}\n\nmachine_tester = None\n\ndef agent(observation, configuration):    \n    global machine_tester    \n    if machine_tester is None: \n        machine_tester = MachineTester(configuration,confidence_level=0.9)                \n    return machine_tester(observation) ","c8db4a93":"# play against the default agent that's provided by the competition\nenv.run([\"..\/input\/santa-2020\/submission.py\", f\"{agent_file}\"])\nenv.render(mode=\"ipython\", width=800, height=800)","6c0d340c":"# best of 5\nprint(f'Default vs {agent_file}')\nbo5(\"..\/input\/santa-2020\/submission.py\", f\"{agent_file}\")","53e4bd95":"agent_file = \"bernoulli_thompson_sampling.py\"","6b8b83a7":"%%writefile {agent_file}\n\nimport math\nimport numpy as np\nimport random\n\n\n\"\"\"\n    Helper Functions\n\"\"\"\n\n# return the index of the largest value in the supplied list\n# - arbitrarily select between the largest values in the case of a tie\n# (the standard np.argmax just chooses the first value in the case of a tie)\ndef random_argmax(value_list):\n  \"\"\" a random tie-breaking argmax\"\"\"\n  values = np.asarray(value_list)\n  return int(np.argmax(np.random.random(values.shape) * (values==values.max())))","0c9c6058":"%%writefile -a {agent_file}\n\nclass CandyMachine:\n    \"\"\" the base candy machine class \"\"\"\n    \n    Q = 0   # the estimate of this machine's reward value                \n    n = 0   # the number of times this machine has been tried      \n    \n    def __init__(self, **kwargs):       \n        # get the initial estimate from the kwargs\n        self.initial_estimate = kwargs.pop('initial_estimate', 0.)         \n        self.initialize() # reset the machine                         \n        \n    def initialize(self):        \n        # estimate of this machine's reward value \n        # - set to supplied initial value\n        self.Q = self.initial_estimate                  \n        \n        # the number of times this machine has been tried \n        # - set to 1 if an initialisation value is supplied\n        self.n = 1 if self.initial_estimate  > 0 else 0        \n        \n                    \n    def update(self,R):\n        \"\"\" update this machine after it has returned reward value 'R' \"\"\"     \n    \n        # increment the number of times this machine has been tried\n        self.n += 1\n\n        # the new estimate of the mean is calculated from the old estimate\n        self.Q = (1 - 1.0\/self.n) * self.Q + (1.0\/self.n) * R\n    \n    def sample(self,t):\n        \"\"\" return an estimate of the machine's reward value \"\"\"\n        return self.Q","1bd73d22":"%%writefile -a {agent_file}\n\nclass BernoulliThompsonCandyMachine( CandyMachine ):\n    def __init__( self, **kwargs ):             \n                \n        self.\u03b1 = 1  # the number of times this machine returned a candy cane\n        self.\u03b2 = 1  # the number of times no candy was returned\n            \n        super().__init__(**kwargs)          \n                    \n    def update(self,R):\n        \"\"\" increase the number of times this machine has been used and \n            update the counts of the number of times it has and has not \n            returned some candy (alpha and beta) \"\"\"\n        self.n += 1    \n        self.\u03b1 += R\n        self.\u03b2 += (1-R)\n        \n    def sample(self,t):\n        \"\"\" return a value sampled from the beta distribution \"\"\"\n        return np.random.beta(self.\u03b1,self.\u03b2)","6eb9a7b9":"%%writefile -a {agent_file}\n\nclass MachineTester():\n    \"\"\" create and test a set of machines over a single test run \"\"\"\n\n    # the index of the last machine chosen\n    machine_index = -1    \n    \n    # the total reward accumulated so far\n    total_reward = 0\n    \n    def __init__(self, configuration, **kwargs):\n        self.machine_count = configuration.banditCount        \n        self.machines = [BernoulliThompsonCandyMachine(**kwargs) for i in range(self.machine_count )]\n        \n    def __call__(self, observation):  \n        \n        if self.machine_index > -1:\n            # the observation reward is the total reward plus the last reward received\n            # - subtract the total reward to the find the reward received from the last machine\n            machine_reward = observation.reward - self.total_reward        \n\n            # update reward estimate of the machine that was last used\n            self.machines[self.machine_index].update( machine_reward )\n\n            # update the total reward\n            self.total_reward = observation.reward\n        \n        # choose a new machine\n        self.machine_index = self.select_machine(observation.step)\n        return self.machine_index\n\n    def select_machine( self, t ):\n        \"\"\" choose the machine with the current highest mean reward \n            or arbitrarily select a machine in the case of a tie \"\"\"\n        return random_argmax([machine.sample(t+1) for machine in self.machines]) ","9ec92d3c":"%%writefile -a {agent_file}\n\nmachine_tester = None\n\ndef agent(observation, configuration):    \n    global machine_tester    \n    if machine_tester is None: \n        machine_tester = MachineTester(configuration)                \n    return machine_tester(observation) ","4558b0b6":"# best of 5\nprint(f'Default vs {agent_file}')\nbo5(\"..\/input\/santa-2020\/submission.py\", f\"{agent_file}\")","830b6ef2":"# best of 5\nprint(f'Optimistic Epsilon-Greedy vs UCB')\nbo5(\"optimistic_epsilon_greedy.py\", \"upper_confidence_bounds.py\")","b4a78d36":"print(f'Optimistic Epsilon-Greedy vs Bernoulli Thompson Sampling')\nbo5(\"optimistic_epsilon_greedy.py\", \"bernoulli_thompson_sampling.py\")","857d5959":"print(f'UCB vs Bernoulli Thompson Sampling')\nbo5(\"upper_confidence_bounds.py\", \"bernoulli_thompson_sampling.py\")","718a0b2d":"# Bernoulli Thompson (Bayesian) Sampling\n\nUp until now, all of the methods we\u2019ve seen for tackling the Bandit Problem have selected their actions based on the current averages of the rewards received from those actions. [Thompson Sampling](https:\/\/towardsdatascience.com\/thompson-sampling-fc28817eacb8) (also sometimes referred to as the <i>Bayesian Bandits<\/i> algorithm) takes a slightly different approach; rather than just refining an estimate of the mean reward it extends this, to instead build up a probability model from the obtained rewards, and then samples from this to choose an action.\n\nIn this way, not only is an increasingly accurate estimate of the possible reward obtained, but the model also provides a level of confidence in this reward, and this confidence increases as more samples are collected. This process of updating your beliefs as more evidence becomes available is known as <i>Bayesian Inference<\/i>.\n\nWhen a random variable has only two possible outcomes its behaviour can be described by the Bernoulli distribution. When, as in this case, the available rewards are binary (win or lose) then the Beta distribution is ideal to model this type of probability. The Beta distribution takes two parameters, \u2018\u03b1\u2019 (alpha) and \u2018\u03b2\u2019 (beta). In the simplest terms these parameters can be thought of as respectively the count of successes and failures. So, when we win a candy cane that machine's alpha value will be increased. When instead we lose, beta will be increased.","8f87f0e9":"### Optimistic Epsilon-Greedy Implementation\n\nCreate a specialized CandyMachine that uses the [Epsilon Greedy](https:\/\/towardsdatascience.com\/bandit-algorithms-34fd7890cb18) algorithm.\n\n* On most time steps this will choose the machine with the current best reward.\n* Now and again a machine will be chosen at random, from the set of all machines, if a random value is less than a value 'Epsilon'.\n\nThis uses the main MachineTester class as its base, so will inherit the main functionality from there.","2eba85d5":"# Create a machine tester\n\n* This keeps a list of all the machines that the Elves can choose from.\n* At each time step it calls the 'select_machine' function to choose one of the machines based on the current rewards.","502c074a":"# Create an agent\n\n\nCreate the agent that will be tested by making an instance of the EpsilonGreedyTester class.\nAt each time step this will be called to select a new machine.\nAdditionally it will be passed the reward obtained from the previous time step, so that the last selected machine can be updated, to keep track of how its performing.","f7b8f6da":"# The Exploration-Exploitation Dilemma\n\nWhen trying the candy machines we're faced with the problem of not knowing which machine will give the most candy. We therefore need to explore the possible choices in search of the best one.\n\nHowever, because we've only got a limited number of tries, we can\u2019t spend all our time searching for the best machine. To get the maximum amount of candy we'll need to exploit the knowledge we've gained, so that we don\u2019t waste time trying bad machines that don't return any candy.\n\nThis is an example of the classic exploration-exploitation dilemma, in which you want to explore the possible options in search of the best one while, at the same time, wanting to exploit the information that has already been obtained, so that you can gain the maximum possible overall reward.\n","92c96dc6":"![Photo by Ferenc Almasi on Unsplash](https:\/\/cdn-images-1.medium.com\/max\/800\/0*WUKevSQJrdR5Xjwr)\n\nPhoto by __[Ferenc Almasi](https:\/\/medium.com\/r\/?url=https%3A%2F%2Funsplash.com%2F%40flowforfrank%3Futm_source%3Dmedium%26utm_medium%3Dreferral)__ on __[Unsplash](https:\/\/medium.com\/r\/?url=https%3A%2F%2Funsplash.com%3Futm_source%3Dmedium%26utm_medium%3Dreferral)__","4325cefd":"The CandyMachine base class is identical to the one used in Epsilon Greedy.\nIt is only repeated here to put it into the UCB python file.","b535f65b":"# Multi-Armed Bandit Strategies\n\nSo, to get the maximum amount of candy for the Elves, we've got to try the machines to find the good ones and, once found, we then need to use those machines. This problem, of finding the balance between exploration and exploitation, is what multi-armed bandit algorithms try to solve.\n\nThere are many such algorithms, but below I have implemented:\n\n* **Optimistic Epsilon Greedy** \n* **Upper Confidence Bound (UCB)**\n* **Bernoulli Thompson (Bayesian) Sampling**\n","a5142b21":"Specify the name of the file to create. We'll create this over a few cells, so after the first cell the attribute '-a' will be used to append to the file","30d8ab64":"The MachineTester class is pretty much identical to the one used for Epsilon Greedy, except now the time step is passed when selecting a machine.","dfe6679f":"## The Code\n\nTo implement the code for this problem I've broken it down into a couple of main classes:\n\n* one to represent a candy machine & keep track of that machines performance\n* one to keep the complete set of machines and decide which machine to try next\n\nThe various strategies for choosing a machine will then build on top of these classes.","40e00465":"### Setup test components","405d32d4":"# What are Multi-Armed Bandits?\n\nWhen faced with a choice of various options, where each option gives you a different degree of reward, how do you find which is the best?\nThis type of problem is commonly referred to as the multi-armed bandit.\n\nIn the multi-armed bandit you are trying to win as much money as possible from playing a set of one-armed bandits (otherwise known as slot machines or fruit machines), each of which can give a different payout. You need to find which machine gives the biggest payout, so you can make as much money as possible in the allocated time.\n\nEach play of a machine (or pull of the bandit\u2019s arm) corresponds to one time slot and you only get to play for a fixed number of time slots.\n\nIn this competition we're not playing slot machines, trying to win as much money as possible, instead we're using candy machines and trying to win Santa's Elves as many candy canes as possible. A much more important task!\n","31cae118":"# Test the agent\n\n- run the algorithm against the supplied default","cff728af":"# **Upper Confidence Bounds (UCB)**\n\nWhen choosing which machine to get candy from it would be best to pick the best one every time (i.e. the one with the highest probability of giving us some candy). This is what's known as the *optimal policy*. As you may have realised, the optimal policy is only theoretical, since we don't actually know which is the best machine and so have to spend some time exploring, during which we'll be using sub-optimal machines.\n\nThe optimal policy, although only theoretical, can however be used to evaluate other policies, to see how close they come to being optimal. The difference between the return that would be achieved by the optimal policy and the amount of return actually achieved by the policy under test is known as the <b><i>regret<\/i><\/b>.\n\nEpsilon-Greedy has linear regret. It continues to explore the set of all actions, long after it has gained sufficient knowledge to know which of these actions are bad actions to take.\n\nA better approach, in terms of maximising the total reward, would be to restrict the sampling over time to the actions showing the best performance. This is the exact approach taken by the [Upper Confidence Bound (UCB)](https:\/\/towardsdatascience.com\/the-upper-confidence-bound-ucb-bandit-algorithm-c05c2bf4c13f) strategy. UCB is based on the principle of \u201c<i>optimism in the fact of uncertainty<\/i>\u201d, which basically means if you don\u2019t know which action is best then choose the one that currently looks to be the best.","b109cc1e":"### UCB Implementation","db8451ab":"# Baby Robot's Guide to Multi-Armed Bandits\n\nThe code in this notebook comes from a Towards Data Science series I've written on [Multi-Armed Bandits](https:\/\/towardsdatascience.com\/multi-armed-bandits-part-1-b8d33ab80697).\n\nI've adapted it so that, rather than picking the best socket to charge up a Baby Robot, it instead tries to pick the best Candy Cane machines for Santa's Elves.","4b58bab4":"The UCBCandyMachine extends the CandyMachine base class. It overrides the \"sample\" function to create one that is based on uncertainty. \n(Note this now takes the time step, since uncertainty decreases with time)","cb362e07":"# Test the agent\n\n- run the Optimistic Epsilon Greedy algorithm against the supplied default","16a081d2":"### Install the kaggle environment for the competition","77e93bae":"# Create the agent file","d843525c":"## Head-to-Head Test","3bb7bdd8":"# **Optimistic Epsilon Greedy** \n\nA greedy strategy would just always choose the machine that gives the most candy. Obviously, since this approach doesn't search the machines to find which is the best, it has a very good chance of not using the best machine. Epsilon-Greedy tries to fix this by introducing a measure of exploration. By default it will use the machine its so far found to be th best, but at random intervals it will try one of the other machines. The degree of exploration is governed by its Epsilon parameter.\n\nAdditionally, if you start with the assumption that none of the machines are going to return any candy (i.e. all machines initially have an average reward of zero) then, as soon as you find a machine that does return some, it will instantly be better than all the others and will therefore become the default machine. As a result, all the other machines will have to wait for the epsilon-greedy search to reach them. So its going to take a very long time to find the best machine and discount bad machines. \n\nThe optimistic greedy approach tries to fix this problem by, instead of assigning an initial reward of zero to each machine, it assigns a high initial expected reward. The effect of this is to make all machines be tried once during the first round of testing. In this way bad machines will be tried and rejected and potentially good machines will be left for further testing.","f71e7954":"# Define a Candy Machine\n\nEach machine keeps track of:\n\n* its average reward\n* the number of times its been tried\n\n\nTo allow an [Optimistic Greedy](https:\/\/towardsdatascience.com\/bandit-algorithms-34fd7890cb18) algorithm to be used, the initial reward estimate can be set to a value other than zero.","a271f2bf":"# Test the agent\n\n- run the UCB algorithm against the supplied default"}}