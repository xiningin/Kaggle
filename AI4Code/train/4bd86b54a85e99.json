{"cell_type":{"a0e0ebc5":"code","b6a2cda7":"code","76107ded":"code","fabe490d":"code","93d5a3d1":"code","66142566":"code","8ecf9c8c":"code","e4d0216d":"code","4c79d338":"code","422ac125":"code","60f1d867":"code","8a7eaef0":"code","7604ded5":"code","2478cc38":"code","c0380ef0":"code","54d3ca68":"code","65850263":"code","3988a023":"code","a80c2136":"code","fecc4070":"code","cd4e61c9":"code","b39d2e9e":"code","23867cef":"code","16919d4b":"code","15ebaef5":"code","c04757dc":"code","15eb2b06":"code","1746e8f3":"code","230e1754":"code","4b6300e1":"code","4fa60d71":"code","c5de0325":"code","824f1aba":"code","118d71fc":"code","9e563401":"code","757a8676":"code","09be2bb4":"code","14d76a81":"markdown","418adf2e":"markdown","5190a787":"markdown","0b464acf":"markdown","81c6f55a":"markdown","bc8afee6":"markdown","ec458da6":"markdown","5b9cd78b":"markdown","ab664079":"markdown","680b2bfc":"markdown","03e189ca":"markdown","1ece19d3":"markdown","a27eab8e":"markdown","08edf7fa":"markdown","c9655cf8":"markdown","e67da0a1":"markdown","03152403":"markdown","35607677":"markdown","122ffdf3":"markdown","75ab80f3":"markdown","ba98e998":"markdown","8473e4c9":"markdown","00b4464b":"markdown","8036dfad":"markdown","83d31231":"markdown","3c3facad":"markdown","7756d1fc":"markdown","42f8980d":"markdown","25ff43ab":"markdown","0dd265ee":"markdown","b16adb0f":"markdown","dba38b0b":"markdown","6655c5a2":"markdown","31c204d1":"markdown","4792bbb7":"markdown"},"source":{"a0e0ebc5":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport seaborn as sbn\nimport matplotlib.pyplot as plt\n\nimport sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")","b6a2cda7":"dataFrame = pd.read_csv(\"..\/input\/used-car-dataset-ford-and-mercedes\/bmw.csv\")\ndataFrame.head()","76107ded":"sbn.distplot(dataFrame[\"price\"])","fabe490d":"sbn.countplot(dataFrame[\"year\"])","93d5a3d1":"dataFrame.corr()","66142566":"dataFrame.corr()[\"price\"].sort_values()","8ecf9c8c":"# now the part about shaving off some of the data breaking the 'normal distribution'.\n# I previously showed that region in yellow on the graph. First, let's sort the dataset by 'price'.\ndataFrame.sort_values(\"price\",ascending = False).head(20)","e4d0216d":"len(dataFrame) * 0.01","4c79d338":"dataFrame.sort_values([\"price\"],ascending=False).iloc[131:]","422ac125":"trimmedDf = dataFrame.sort_values(\"price\",ascending=False).iloc[131:]\ntrimmedDf.head(15)","60f1d867":"sbn.distplot(trimmedDf[\"price\"])","8a7eaef0":"trimmedDf.sort_values(\"engineSize\",ascending = True).head(10)","7604ded5":"trimmedDf = trimmedDf[trimmedDf.engineSize != 0]\ntrimmedDf.sort_values(\"engineSize\",ascending = True)\n","2478cc38":"dataFrame = trimmedDf\ndataFrame.describe()","c0380ef0":"dataFrame = pd.get_dummies(dataFrame, columns=[\"model\"])\ndataFrame = pd.get_dummies(dataFrame, columns=[\"transmission\"])\ndataFrame = pd.get_dummies(dataFrame, columns=[\"fuelType\"])\ndataFrame","54d3ca68":"dataFrame.head()","65850263":"y = dataFrame[\"price\"].values\n\nx = dataFrame.drop(\"price\",axis=1).values\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=10)","3988a023":"len(x_train)","a80c2136":"len(x_test)","fecc4070":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.fit_transform(x_test)\n","cd4e61c9":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense","b39d2e9e":"x_train.shape","23867cef":"model = Sequential()\n\nmodel.add(Dense(40,activation=\"relu\"))\nmodel.add(Dense(40,activation=\"relu\"))\nmodel.add(Dense(40,activation=\"relu\"))\nmodel.add(Dense(40,activation=\"relu\"))\nmodel.add(Dense(40,activation=\"relu\"))\nmodel.add(Dense(40,activation=\"relu\"))\nmodel.add(Dense(40,activation=\"relu\"))\nmodel.add(Dense(40,activation=\"relu\"))\nmodel.add(Dense(40,activation=\"relu\"))\n\nmodel.add(Dense(1))\n\nmodel.compile(optimizer=\"adam\",loss=\"mse\")","16919d4b":"model.fit(x = x_train, y = y_train, batch_size=250, validation_data=(x_test,y_test), epochs=300)\n","15ebaef5":"lostData = pd.DataFrame(model.history.history)","c04757dc":"lostData.head(10)","15eb2b06":"lostData.plot()","1746e8f3":"from sklearn.metrics import mean_squared_error, mean_absolute_error\n\nguessArray = model.predict(x_test)","230e1754":"guessArray","4b6300e1":"mean_absolute_error(y_test,guessArray)   # we see that there is a difference of 2597 pounds","4fa60d71":"dataFrame[\"price\"].mean()","c5de0325":"plt.scatter(y_test,guessArray)\nplt.plot(y_test,y_test,color=\"r\")","824f1aba":"dataFrame.iloc[150][\"price\"]","118d71fc":"newCarSeries = dataFrame.drop(\"price\",axis=1).iloc[150]","9e563401":"newCarSeries","757a8676":"newCarSeries = scaler.transform(newCarSeries.values.reshape(-1,37))","09be2bb4":"model.predict(newCarSeries)","14d76a81":"We plotted the regression graph using mathplotlib.pyplot's .scatter () and .plot () functions.                             We got a nice regression graph, a good result overall, even though there were big deviations in a small number of data.\n\nFinally, let's test our model by giving it a real feature set and finish our work. For this test to get a realistic result, the features we will give must also be realistic and logical, so we will use without 'price' a row  from the dataFrame and give it to the model.","418adf2e":"We used 'iloc'(index-based-location) and our new data frame was created, since extracting 107 rows would be the same as extracting the data from the 107th index.","5190a787":"After data cleaning, the parts on the right that disrupted the normal distribution were almost shaved and became more normal distrubition.\nIf we trim even more, we'll ruin it, so there's no need.\n\nI discovered something about engineSize, now I want to share it.\nWhen I looked at the lowest engineSize and saw 0, this was interesting and after doing some research on the internet, I learned that engineSize cannot be 0. So after sorting the data according to 'engineSize', I will detect 0 ones and discard them from the data.","0b464acf":"Reading data and an overview to look how data looks like:","81c6f55a":"And as you can see, our new data frame has become a data frame with the highest price of 65 thousand, with the first 107 data discarded in descending order, and the cars with a price of 123 thousand were discarded.\nNow let's look at the distribution again.","bc8afee6":"In this way, we looked at the correlation information of 'price' specifically, we added 'sort_values of ( )' so that we can see what the most and least correlated feature is.\nSo what do we understand by looking at this table? The correlation of price with price is 1 this is already normal, then the year with the highest correlation value of 0.62, it seems that price affects the most 'year', in other words, 'mpg' in other words, mile per gallon correlation is negative. It shows that it affects negatively, that is, the higher the mpg rises, the lower the price, so the more the vehicle consumes, the more the price increases and people do not want to buy the vehicle, the most negative effect is 'mileage', that is, how many kilometers the car travels, which is logical anyway. Finally, we can say that the negative correlation creates an oppositional relationship as if it increases, it decreases, if it decreases, it increases.","ec458da6":"mean_absolute_error is equal to 2597. This means that there may be an average difference of 2597 pounds in the car estimates. So for example; our model can predict a 58k vehicle to be about 60,6k or 55,4k.\nLet's do something else to better understand the error.","5b9cd78b":"We did what we said above using the ' pd.get_dummies( ) '  function.","ab664079":"I am new to Data Science, so I would like to thank At\u0131l Samancioglu and Murat Mert for their help.","680b2bfc":"Very good, we've deleted those that are equal to 0.\nNow it's time to change our dataFrame.","03e189ca":"--- Let's Move to Creating Model After Data Cleaning---","1ece19d3":"Graphic as it should be.\n\nLet's look at error values \u200b\u200bto understand exactly how well our model works.","a27eab8e":"After dummy variables, the number of columns increased, we remembered this again.\n\nLet's create our model.","08edf7fa":"Using .predict( ), we give our new feature set to the model, and the prediction data came out 53918.\n\n\nExample Result =>\n\n# Real Price     : ~ 53000\n# Predicted Price: ~ 54700","c9655cf8":"Some importings","e67da0a1":"We changed our dataFrame and when we looked at the 'count' line, we understood that yes it has changed.\n\n--- Now, as I said at the beginning, it's time to convert non-numeric columns into numeric with dummy variables! ---\n","03152403":"![qweerty.PNG](attachment:eaee0cd2-bb74-4006-bff4-5c9fe14cd990.PNG)\nThe region I specified in the yellow part distorts the 'normal distribution', it is a choice to remove that part from the data, I will remove that part for my model to work more accurately. \nBut first, let's get a little more insight into the data.","35607677":"We have defined the lostData. Let's have a glance.","122ffdf3":"Mean of cars' price is 22168 and mean_absolute_error is 2597.\n\n(2597 \/ 22168 ) * 100 = 10.52 \n\nSo our error rate is ~ %11.71\n\nWhat if we didn't get the result we wanted? \n\nWe can go back and clean data, change test size split size, increase epochs, change number of neurons, number of layers, etc. We can make changes in the form, but of course, we should be careful not to overfitting while doing these, because maybe a model that will work very well according to the data we have, may be ridiculous when we enter any feature set, so we should test the model's operation by entering new feature sets.\n\nIt's time to look at the regression graph.\n","75ab80f3":"We looked at which 'year' and how many cars were sold, the year information is mixed on the x-axis in the graph, but that's okay, we already know that it is between 1970-2020.\nWhat we can understand from here is that there are mostly cars in 2019, there are few cars based on old years, for now we will not do anything about it.\nLet's take a look at the correlation information.","ba98e998":"In our work, the steps will be as follows:\n\n* 1- data overview and data cleaning\n* 2- splitting the data into two parts\n* 3- scaling of data\n* 4- training the data\n* 5- run the model\n* 6- evaluation of the model","8473e4c9":"We see that there are cars with engineSize equal to 0, and we don't know the number of them, so let's drop them below.","00b4464b":"This is how we can look at the correlation of all columns to each other, but in this example we are only concerned with the correlation between price and other data.\n\n","8036dfad":"We understand from the above that after sorting our dataset in descending order of 'price', we can discard the first 107 rows. So we will get 'more normal' distrubition.","83d31231":"Since we will give the data in the 150. index to the model, we looked at its 'price'=52995","3c3facad":"We looked at the size of x_train and x_test to check that the data was split correctly.\n\n7422 + 3181 = 10603\n10603 * 0.3 = 3181 \u2713\n\nBefore giving the data to the model, we need to scale the data, thanks to this process, the data is placed between 0-1 in a way that the model can understand. The scaling process allows the model to run fast, for example, when a car price is around 60,000, sizing it between 0-1 makes the model run better.","7756d1fc":"We can think of 'batch_size' as the capacity of the mouth of the model, giving the model 1 million data at a time can crash the model, so we give it little by little, but if we give it little by little, this time it works slowly, so we have to find the appropriate value, here is how much capacity we will give the batch size. The higher the data number, the higher our batch size should be.\n\n'Epochs' is the number of revolutions, while the model is being trained, we determine how many times the model will go over the data set. Too many epochs lead to 'overfitting'. Too few epochs cause the model to be under-trained, we don't want that. So epochs need to be at an optimal value, there are some methods for this but I won't use it for this example because the number of epochs=300 will be enough for us. (Overfitting: When the model deviates from being based on price estimation according to the feature set to be given to it and only gives the data within itself, it is built based on predicting the correct results. this is also ridiculous and unnecessary, we already know the prices, the important thing is that when we give a new feature set, it can tell us the right price.)\n\nWhile the model is being trained, we can give the validation data to the model with the help of tensorflow, that is, it can be validated while it is being trained, which automatically gives us the x_test - y_test comparison that we can do manually. Since we do the comparison with the test data, we give the test data into validation_data(), the remaining train data was already used while the model was being trained.","42f8980d":"As you can see, loss and val_loss are close to each other, this is what we wanted.","25ff43ab":"We are going to solve a regression problem, but there are 3 non-numeric columns (model, transmission, fuel type).\nIn this case, we will either delete these columns or convert the data to a numeric form. \nInstead of deleting the columns, using 'dummy variables' to convert them to a numerical form will make our model work more accurately.\nBut we're going to do this right before we build the model, now let's get some idea of of the data.","0dd265ee":"We created a variable named 'model' from the Sequential class. This is the name of our model.\n\nThen we add hidden layers to the model, we specify the number of neurons in the layers, I set 40 for now. In the future, we can reduce or increase this by looking at the working performance of the model. We also add activation functions of neurons, 4 of the most known are RelU , Tanh , Sigmoid , Linear. My choice for this example was 'relu'. \nWe chose the loss function as 'mse' because we usually use mse when working with regression.\n\n\nNow it's time to run the model, so let's train the model.","b16adb0f":"As seen here, there are some dirty data in our data set, we only showed 20 of them, generally in such cases, if 99% of the data is taken, we can continue without disturbing the general table of the data.\nSo we can subtract 1%.","dba38b0b":"Since the row we will use will be a Series variable, we defined a variable called newCarSeries, and we have assigned the row in the 150th index of the dataFrame to the newCarSeries we just created.","6655c5a2":"After importing, we created a variable named scaler and created it as a variable from the MinMaxScaler class, just like an empty list creates an empty array. Then we set the scaler to fit the x_train by saying scaler.fit(x_train), and then we transform the variables related to x.\nInstead of first fitting and then transforming, we can also fit and transform at the same time, using .fit_transform( ). I chose this way.\n\nAfter the scale process is finished, we can move on to creating our model.\nIn this particular example, we will import two things for our model, Sequential, which is the type of our model, and Dense for our layers.","31c204d1":"Initially we defined x and y, while defining y we dropped the unwanted column with drop( ) instead of giving each column one by one, so it was more practical.\n\nWe split the data into x and y with train_test_split( ), remember y = ax + b, where y is output, x is feature, b is bias.","4792bbb7":"Before giving the data to the model, we need to scale it, so we converted the data to 37 column shape and then scaled the data."}}