{"cell_type":{"ac3827c7":"code","2b3645b1":"code","6fc6e6ca":"code","7c4a40ca":"code","12e4da1c":"code","fac0c9ca":"code","d7cb1acc":"code","c4128184":"code","01dd86fb":"code","3b9ac561":"code","52473a89":"code","245fc1f9":"code","646389f8":"code","795b738a":"code","75aa88ad":"code","69d9264f":"code","a539eac0":"code","782ac920":"code","5222bb6e":"code","3ce03282":"code","52090fb0":"code","d77f9d12":"code","1346785e":"code","beb8425d":"code","b53edf1d":"code","e1973ca9":"code","a6d6e909":"code","3677173f":"code","e7277fb0":"code","9b4c6c37":"code","9b9dbc32":"code","a71ffae2":"code","b6ade47c":"code","60aee3d8":"code","acffaf4d":"code","5e26d081":"code","35c8ccd7":"code","82e530b3":"code","f104bde8":"code","98e015f3":"code","a5a03342":"code","05972c50":"code","31b26d4d":"code","3fca4387":"code","ed45b5fc":"code","853980a3":"code","b6ae98da":"code","8552147c":"code","4aba680c":"code","ccb64ac8":"code","97651721":"code","783fab83":"markdown","adc96c4d":"markdown","6dab8a9b":"markdown","b8e0ee29":"markdown","0c835ec4":"markdown","e5180c07":"markdown","ebae4e47":"markdown","644fdb86":"markdown","89b2f1a8":"markdown"},"source":{"ac3827c7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport datetime","2b3645b1":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6fc6e6ca":"#1. Covert csv files to dataframe and take a quick look of those dataset\nsales=pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\ntest=pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/test.csv\")\nitems=pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/items.csv\")\nitem_categories=pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")\nshops=pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/shops.csv\")","7c4a40ca":"sales.head()\n#sales.shape (2935949, 6) i.e. 2,935,849 rows and 6 columns(variables)","12e4da1c":"#covert date format from \"date, month, year\" to \"year, month, date\"\nsales.date=sales.date.apply(lambda x:datetime.datetime.strptime(x, '%d.%m.%Y'))\nsales.head()\n#print sales.info()","fac0c9ca":"sales.info()","d7cb1acc":"#check if there is null values in the dataframe\n\nsales.isna() #or you can use sales.isnull() They are the same","c4128184":"sales.isnull().sum().sum() #to check if there is null value in the whole dataframe","01dd86fb":"sales.describe() \n","3b9ac561":"sales.loc[sales['item_id'] == 6675]\nprint(sales['item_cnt_day'].agg(['sum']))\n#print(output)","52473a89":"test.head()\n#test.size #642500  number of rows * number of columns\n#test.shape (214200,3) number of rows, number of columns","245fc1f9":"items.tail()","646389f8":"items.isnull().sum(axis = 0)# check null value in every column","795b738a":"items.isnull().sum(axis = 1)# check null value in every row","75aa88ad":"item_categories.head()\n#item_categories.shape (84,2)","69d9264f":"shops.head()\n#shops.shape (60,2)","a539eac0":"#we need item_categroy_id during aggregation process. We can merge table Sales with table Items, then we drop the unneeded column item-names \nnewSales = pd.merge(sales, items, on=['item_id','item_id']).drop(['item_name'],1) \nnewSales.head(70)","782ac920":"#It runs a bit slow now. So I borrow the idea from https:\/\/www.kaggle.com\/kyakovlev\/1st-place-solution-part-1-hands-on-data to downcast the dataframe\ndef downcast_dtypes(df):\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int16)\n    return df","5222bb6e":"dsize_sales = downcast_dtypes(newSales)\nprint(dsize_sales.info())","3ce03282":"dsize_sales.head()","52090fb0":"#add one new column \"revenue\", we will soon figure out which shops sell the most and what is the most popular items.\ndsize_sales['revenue']=dsize_sales['item_cnt_day'] * dsize_sales['item_price']\ndsize_sales.head()","d77f9d12":"#select only the needed column for analyzing task 1. \n\nrankRevenue=dsize_sales[['shop_id','revenue']]\nrankRevenue['revenue']=round(rankRevenue['revenue'])\ntopShops=rankRevenue.groupby(['shop_id']).sum().sort_values('revenue',ascending=False)\ntopShops.head()","1346785e":"#convert the renenue to milloin and keep two digits after dot\n#check out the top 10 shops in revenues\n\ntopShops['revenue']=round(topShops['revenue']\/1000000,2)\n\ntopShops.head()","beb8425d":"#Shops sold least \n\ntopShops.tail()","b53edf1d":"#let's now find the best sold items\n\nrankItems=dsize_sales[['item_id','revenue']]\n#rankItems['revenue']=round(rankItems['revenue'])\ntopItems=rankItems.groupby(['item_id'],as_index=False).sum().sort_values('revenue',ascending=False)\ntopItems['revenue(m)']=round(topItems['revenue']\/1000000)\ntopItems['index'] = range(1, len(topItems) + 1)\ntopItems.head()","e1973ca9":"topItems.tail()","a6d6e909":"topItems[\"item_id\"]=np.where(topItems[\"index\"]<=20, topItems[\"item_id\"],\"Other\")\nnewTop=topItems.groupby('item_id',as_index=False).sum().sort_values('revenue',ascending=False).reset_index()\nnewTop=newTop[['item_id','revenue(m)']]\nnewTop.head()","3677173f":"print(newTop['revenue(m)'].agg(['sum']))","e7277fb0":"newTop['percent']=(newTop['revenue(m)']\/2631*100).astype(float).round(2)\nimport plotly.graph_objects as go\n\nfig = go.Figure(data=[go.Table(\n    header=dict(values=list(newTop.columns),\n                fill_color='green',\n                font=dict(color='white', size=12),\n                align='left'),\n    cells=dict(values=[newTop.item_id,newTop['revenue(m)'],newTop.percent],\n               line_color='darkslategray',\n               fill_color='light green',\n               align='left')),\n     \n])\nfig.update_layout(title='Top-selling items by resevene(millions ruble)from 2013 January to 2015 October',\n                  yaxis_zeroline=False, xaxis_zeroline=False)\nfig.show()","9b4c6c37":"#items that generate the least revenues\nnewTop.tail()","9b9dbc32":"top20Items=topItems.nlargest(20,['revenue(m)'])\nimport plotly.express as px\n\nfig = px.pie(top20Items, values='revenue(m)', names='item_id', title='The top 20 items by revenue from 2013 January to 2015 October (Ruple-millions)')\n\nfig.show()","a71ffae2":"\nrankSoldPieces=dsize_sales[['item_id','item_cnt_day']]\ntopSoldPCs=rankSoldPieces.groupby(by=['item_id'],as_index=False).sum().sort_values('item_cnt_day',ascending=False)\ntopSoldPCs.columns=['item_id','sold_pcs']\ntopSoldPCs=topSoldPCs.nlargest(20,['sold_pcs'])\ntopSoldPCs['sold_pcs']=topSoldPCs['sold_pcs']=round(topSoldPCs['sold_pcs']\/1000)","b6ade47c":"import plotly.graph_objects as go\nfig = go.Figure(data=[go.Table(\n    header=dict(values=list(topSoldPCs.columns),\n                fill_color='light blue',\n                align='left'),\n    cells=dict(values=[topSoldPCs.item_id,topSoldPCs.sold_pcs],\n               fill_color='orange',\n               align='left')),\n     \n])\nfig.update_layout(title='The most sold items(thousand pcs) by item_id from 2013 January to 2015 October',\n                  yaxis_zeroline=False, xaxis_zeroline=False)\nfig.show()","60aee3d8":"#let's now find the best sold category_id \n\nrankItems=dsize_sales[['item_category_id','revenue']]\nrankItems['revenue']=round(rankItems['revenue'])\ntopCatogries=rankItems.groupby(['item_category_id'],as_index=False).sum().sort_values('revenue')\ntopCatogries['revenue']=round(topCatogries['revenue']\/1000000)","acffaf4d":"#Top 20 best-selling categories by revenue from 2013 January to 2015 October (Ruple-millions)\ntop20Catogries=topCatogries.nlargest(20,['revenue'])\nCatogries=top20Catogries[['item_category_id','revenue']]\nCatogries.head()","5e26d081":"import plotly.graph_objects as go\ndf = px.data.tips()\n\nlabels =Catogries['item_category_id']\nvalues=Catogries['revenue']\n\nfig = go.Figure(data=[go.Pie(labels=labels,values=values, hole=.3)])\nfig.update_traces(textposition='inside', textinfo='label+percent')\nfig.update_layout(title_text='Top 20 best-selling categories by revenue from 2013 January to 2015 October (Ruple-millions)')\nfig.show()","35c8ccd7":"#The most sold categories by quantity(thousand pcs) from 2013 January to 2015 October\nrankSoldCat=dsize_sales[['item_category_id','item_cnt_day']]\ntopSoldPCat=rankSoldCat.groupby(by=['item_category_id'],as_index=False).sum().sort_values('item_cnt_day',ascending=False)\ntopSoldPCat.columns=['item_category_id','sold_pcs']\ntopSoldPCat=topSoldPCat.nlargest(20,['sold_pcs'])\ntopSoldPCat['sold_pcs']=topSoldPCat['sold_pcs']=round(topSoldPCat['sold_pcs']\/1000)","82e530b3":"import plotly.graph_objects as go\n\nfig = go.Figure(data=[go.Table(\n    header=dict(values=list(topSoldPCat.columns),\n                fill_color='purple',\n                font=dict(color='white', size=12),\n                align='left'),\n    cells=dict(values=[topSoldPCat.item_category_id,topSoldPCat.sold_pcs],\n               line_color='black',\n               fill_color='rgba(152, 0, 0, .3)',\n               align='left')),\n     \n])\nfig.update_layout(title='The most sold products(thousand pcs) by category_id from 2013 January to 2015 October',\n                  yaxis_zeroline=False, xaxis_zeroline=False)\nfig.show()","f104bde8":"#the most unpopular items_categories\ntopSoldPCat.tail()","98e015f3":"# Show the best selling months\ndsize_sales[\"month\"] = pd.to_datetime(dsize_sales[\"date\"]).dt.month\ndsize_sales[\"year\"] = pd.to_datetime(dsize_sales[\"date\"]).dt.year\nnewMonthSales=dsize_sales[['year','month','revenue']]\nnewMonthSales.head()","a5a03342":"#Christmas seems to be the best season\nrankMonths=newMonthSales[['month','revenue']]\nrankMonths['revenue']=round(rankMonths['revenue'])\ntopMonths=rankMonths.groupby(['month']).sum().sort_values('revenue',ascending=False)\ntopMonths['revenue']=round(topMonths['revenue']\/1000000)\ntopMonths","05972c50":"#select the top 3 shops by revenue\n\nplotTopFourShops=dsize_sales.loc[(dsize_sales['shop_id']==31) | (dsize_sales['shop_id']==25)|(dsize_sales['shop_id']==28) |(dsize_sales['shop_id']==42)]\nplotTopFourShops.head()","31b26d4d":"# now I want to make a table for plotting the top 4 shops \n\nplotTopFourShops=plotTopFourShops[['year','month','shop_id','revenue']]\nsep=plotTopFourShops.groupby(['year','month','shop_id'],as_index=False).sum()\nsep['revenue(m)']=round(sep['revenue']\/1000000)\nsep.head()","3fca4387":"#subset shop25\nshop25=sep.loc[sep['shop_id']==25]\nshop25=shop25[['year','month','revenue(m)']]\nshop25.columns = ['year', 'month','shop25']\nshop25.head()","ed45b5fc":"#do for the other 3 shops\nshop28=sep.loc[sep['shop_id']==28]\nshop28=shop28[['year','month','revenue(m)']]\nshop28.columns = ['year', 'month','shop28']\n\nshop31=sep.loc[sep['shop_id']==31]\nshop31=shop31[['year','month','revenue(m)']]\nshop31.columns = ['year', 'month','shop31']\n\n#shop42=sep.loc[sep['shop_id']==42]\n#shop42=shop42[['year','month','revenue(m)']]\n#shop42.columns = ['year', 'month','shop42']","853980a3":"#join the subset tables\n\nfrom functools import reduce\nplotTopThreeShops = [shop25, shop28, shop31]\nshop_final = reduce(lambda left,right: pd.merge(left,right,on=('year','month')), plotTopThreeShops).reset_index()","b6ae98da":"shop_final.head()","8552147c":"#plot ...\nimport plotly.graph_objects as go\nfig = go.Figure()\nx=['2013-01','2013-02','2013-03','2013-04','2013-05','2013-06','2013-07','2013-08','2013-09','2013-10','2013-11','2013-12',\n   '2014-01','2014-02','2014-03','2014-04','2014-05','2014-06','2014-07','2014-08','2014-09','2014-10','2014-11','2014-12',\n   '2015-01','2015-02','2015-03','2015-04','2015-05','2015-06','2015-07','2015-08','2015-09','2015-10'  \n  ]\n\nfig.add_trace(go.Scatter(x=x, y=shop_final['shop25'],\n                    mode='lines+markers',\n                    name='shop25',\n                    line_color='Red',\n                    #line={'smoothing': 1.3}\n                    ))\nfig.add_trace(go.Scatter(x=x, y=shop_final['shop28'],\n                    mode='lines+markers',\n                    name='shop28',\n                    line_color=\"Green\"))\n\nfig.add_trace(go.Scatter(x=x, y=shop_final['shop31'],\n                    mode='lines+markers',\n                    name='shop31',\n                    line_color='Blue'))\n#fig.add_trace(go.Scatter(x=x, y=shop_final['shop42'],\n            #        mode='lines+markers',\n              #      name='shop42',\n               #     line_color=\"Purple\"))\nfig.update_layout(title_text='Top three best selling shops from 2013 January to 2015 October',\n                 \n                  #xaxis_title=\"\"\n                  yaxis_title=\"Revenue in Ruble (millions)\" \n                 )\n\nfig.show()\n","4aba680c":"# we need to have a dataset that contains information about date, shop_id, item_id as well as sold quantity\ncombinedTable=dsize_sales[['date','shop_id','item_id','item_cnt_day']]\ncombinedTable.head(20)","ccb64ac8":"# here we aggregate the dataset based on date, shop_id and item_id\n\nbefore_final=combinedTable.assign(month=combinedTable.date + pd.to_timedelta(1 - combinedTable.date.dt.day, 'D')).groupby(\n    ['month']).apply(lambda x: x.groupby(['shop_id','item_id']).sum()).reset_index()\nbefore_final.head()","97651721":"#Now we have the dataset for predicting. Let's export it to csv for further analyzing. \nbefore_final.to_csv('processed.csv', sep='\\t')","783fab83":"![](http:\/\/)**3.Taking a quick look at datasets**","adc96c4d":"**1. Importing libraries**","6dab8a9b":"Generate descriptive statistics. Descriptive statistics include those that summarize the central tendency, dispersion\nand shape of a dataset\u2019s distribution, excluding NaN values.\n* Count: Count number of non-NA\/null observations.\n* Mean: Mean of the values.\n* Std: Standard deviation of the observations. \n* Min: Minimum of the values in the object.\n* Percentiles: All should fall between 0 and 1. The default is [.25, .5, .75], which returns the 25th, 50th, and 75th percentiles.\n* Max:n**Maximum of the values in the object.","b8e0ee29":"**2.Loading data**","0c835ec4":"In this series, we are going to figure out:\n\n* The most popular items by revenue\n* The most sold items by quantity\n* The most popular categories by revenue\n* The most sold category_ids by quantity\n* Top selling months\n* Three best shops by revenue","e5180c07":"Thanks for visiting my time-consuming series;). Before diving deep into the code, let's think about this task for a second. What are the most significant features that you would like to know if you are the CEO of this company? Perhaps the best selling seasons, shops that bring more revenues, the most popular items, how much money I could get in the coming years, right:) Well, we know our target now. Let's start. BTW, I 'm not sure yet how many kernels I will create. But here is part one-level zero ","ebae4e47":"Now we got answers to some of our questions. In the next series, we are going to cover ARIMA models using the pre-processed CSV file that we just generated. See you next time \ud83d\ude42","644fdb86":"**The best selling months**","89b2f1a8":"[](http:\/\/)**4. Manupulating dataframe**"}}