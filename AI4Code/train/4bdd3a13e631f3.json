{"cell_type":{"dc2ab586":"code","2c7c1437":"code","b5ea29fc":"code","1626ad88":"code","fda2913f":"code","97d2bf94":"code","95d2dc6d":"code","a3e35ff4":"code","1d7252f4":"code","eef844fa":"code","3b9d88a1":"code","b655b937":"code","26b0406c":"code","3c64aca4":"code","951f14c5":"code","3968986d":"code","36d87d94":"code","bf667ef6":"code","4bc28fb0":"code","2f7cb281":"code","eca84112":"code","ffef246b":"code","7b165c8c":"code","a576edb1":"code","dbc787e3":"code","cb52c9d5":"code","2dee432d":"code","83ac44bd":"code","8777bddb":"code","4b892fb4":"code","49304297":"code","46c034a2":"code","7e4c1928":"code","f24c8f10":"code","240e8c26":"code","6dd0808e":"code","2c979da5":"markdown","d6afc9b8":"markdown","8d37dfeb":"markdown","dd7f1973":"markdown","a6614bad":"markdown","ca37aaaa":"markdown","95f74e75":"markdown","4428c133":"markdown","51a3cf2e":"markdown","be39782b":"markdown","9b6ace26":"markdown","f2ffc284":"markdown","3e1bee3a":"markdown"},"source":{"dc2ab586":"#general imports\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np # linear algebra\nimport seaborn as sns # plotting\nimport matplotlib.pyplot as plt # plotting\n%matplotlib inline\nimport os # accessing directory structure\n\n#NLP processing imports\nfrom wordcloud import WordCloud\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import sent_tokenize, word_tokenize\nfrom wordcloud import WordCloud, STOPWORDS\nfrom collections import Counter\nfrom nltk.tokenize import RegexpTokenizer\nfrom stop_words import get_stop_words\nimport re\nimport spacy\n\n###Vader Sentiment\n#To install vaderSentiment\n!pip install vaderSentiment \nfrom vaderSentiment import vaderSentiment\nfrom textblob import TextBlob\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\n####Lemmatization\nfrom nltk.stem import WordNetLemmatizer\n# Lemmatize with POS Tag\nfrom nltk.corpus import wordnet","2c7c1437":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b5ea29fc":"data = pd.read_csv(\"\/kaggle\/input\/twcs.csv\")","1626ad88":"data.shape","fda2913f":"data = data.loc[:10000]","97d2bf94":"data.shape","95d2dc6d":"data.head()","a3e35ff4":"pd.set_option('display.max_colwidth', -1)","1d7252f4":"data.head(10)","eef844fa":"nRow, nCol = data.shape\nprint(f'There are {nRow} rows and {nCol} columns')","3b9d88a1":"#DataTypes\ndata.dtypes","b655b937":"data[\"text\"] = data[\"text\"].astype(str)","26b0406c":"def remove_urls(text):\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+|@[^\\s]+')\n    return url_pattern.sub(r'', text)","3c64aca4":"data[\"textclean\"] = data[\"text\"].apply(lambda text: remove_urls(text))","951f14c5":"data.head()","3968986d":"top_N = 100 #top 100 words\n\n#convert list of list into text\na = data['textclean'].str.lower().str.cat(sep=' ')\n\n# removes punctuation,numbers and returns list of words\nb = re.sub('[^A-Za-z]+', ' ', a)","36d87d94":"#remove all the stopwords from the text\nstop_words = list(get_stop_words('en'))         \nnltk_words = list(stopwords.words('english'))   \nstop_words.extend(nltk_words)","bf667ef6":"word_tokens = word_tokenize(b) # Tokenization\nfiltered_sentence = [w for w in word_tokens if not w in stop_words]\nfiltered_sentence = []\nfor w in word_tokens:\n    if w not in stop_words:\n        filtered_sentence.append(w)","4bc28fb0":"# Remove characters which have length less than 2  \nwithout_single_chr = [word for word in filtered_sentence if len(word) > 2]\n\n# Remove numbers\ncleaned_data_title = [word for word in without_single_chr if not word.isnumeric()]","2f7cb281":"def get_wordnet_pos(word):\n    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n    tag = nltk.pos_tag([word])[0][1][0].upper()\n    tag_dict = {\"J\": wordnet.ADJ,\n                \"N\": wordnet.NOUN,\n                \"V\": wordnet.VERB,\n                \"R\": wordnet.ADV}\n    return tag_dict.get(tag, wordnet.NOUN)\n\n# 1. Init Lemmatizer\nlemmatizer = WordNetLemmatizer()","eca84112":"lemmatized_output = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in cleaned_data_title]\nlemmatized_output = [word for word in lemmatized_output if not word.isnumeric()]","ffef246b":"word_dist = nltk.FreqDist(lemmatized_output)\ntop100_words = pd.DataFrame(word_dist.most_common(top_N),\n                    columns=['Word', 'Frequency'])","7b165c8c":"plt.figure(figsize=(10,10))\nsns.set_style(\"whitegrid\")\nax = sns.barplot(x=\"Frequency\",y=\"Word\", data=top100_words.head(10))","a576edb1":"def wc(data,bgcolor,title):\n    plt.figure(figsize = (80,80))\n    wc = WordCloud(background_color = bgcolor, max_words = 100,  max_font_size = 50)\n    wc.generate(' '.join(data))\n    plt.imshow(wc)\n    plt.axis('off')","dbc787e3":"wc(lemmatized_output,'black','Common Words' )","cb52c9d5":"sent_analyser = SentimentIntensityAnalyzer()\ndef sentiment(text):\n    return (sent_analyser.polarity_scores(text)[\"compound\"])","2dee432d":"data[\"Polarity\"] = data[\"textclean\"].apply(sentiment)","83ac44bd":"data.head()","8777bddb":"data.dtypes","4b892fb4":"def senti(data):\n    if data['Polarity'] >= 0.05:\n        val = \"Positive\"\n    elif data['Polarity'] <= -0.05:\n        val = \"Negative\"\n    else:\n        val = \"Neutral\"\n    return val","49304297":"data['Sentiment'] = data.apply(senti, axis=1)","46c034a2":"plt.figure(figsize=(10,10))\nsns.set_style(\"whitegrid\")\nax = sns.countplot(x=\"Sentiment\", data=data, \n                  palette=dict(Neutral=\"blue\", Positive=\"Green\", Negative=\"Red\"))","7e4c1928":"#import spacy\nnlp = spacy.load(\"en_core_web_sm\")","f24c8f10":"def pos(text):\n    doc = nlp(text)\n    # You want list of Verb tokens \n    aspects = [token.text for token in doc if token.pos_ == \"NOUN\"]\n    return aspects","240e8c26":"data[\"Aspects\"] = data[\"textclean\"].apply(pos)","6dd0808e":"data.head()","2c979da5":"# Introduction\nGreetings! This is the first public kernel. In this kernel, I  will demonstrate few steps of NLP, few plots like wordcloud, frequency plot.","d6afc9b8":"**P.S This is my first Kaggle Kernel and I am fairly new to python programming as well, hence my non usage of list comprehensions and functions might be evident. I highly encourage everyone to fork my code and add your own twists to increase the accuracy of both aspect extractions and sentiment analysis.**","8d37dfeb":"# WordCloud","dd7f1973":"# TEXT CLEANING","a6614bad":"# VADER + TEXTBLOB Sentiment Analysis","ca37aaaa":"#### I am using Wordnet Lemmatizer with appropriate POS tag. \n#### Function to map word with its POS tag","95f74e75":"### Remove urls,@mention, https","4428c133":"# Find out the top 100 words which are getting used in the text of the data","51a3cf2e":"# Frequency distribution","be39782b":"There is scope of improvement. I will learn and update it accordingly","9b6ace26":"# ASPECT MINING\/ OPINION MINING","f2ffc284":"**#### *Lemmatization is the process of converting a word to its base form. The difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors.*","3e1bee3a":"# Lemmatization"}}