{"cell_type":{"0364da4b":"code","1e2994f2":"code","279bef83":"code","40c7ee06":"code","5cecaefa":"code","336f054f":"code","d8a812e8":"code","80fce4ac":"code","4f2aa103":"code","89edec52":"code","028a7155":"code","53bc6284":"code","f78b47bd":"code","5c5f8fa7":"code","f9765aa4":"code","b7a41e3a":"code","de61f44e":"code","20000b86":"code","27323aab":"code","97dd165e":"code","fdd2ff78":"code","81a1d809":"code","e9103890":"code","db4d7817":"code","ca692f69":"code","41d505b6":"code","21873c26":"code","fceec85d":"code","158fd0a5":"code","78ee110b":"code","95732db0":"code","cce0a687":"code","f60db1b5":"code","15a37ccf":"code","6d2666b4":"code","e2ce50ad":"code","ae78dcb6":"code","b82d5b78":"code","c263b659":"code","a00a7dc0":"code","c762aa01":"code","483e76a0":"code","cb3ceddf":"code","59e6b190":"code","7fc5d549":"code","628e8cb5":"code","f4b41e55":"code","3db3792c":"code","bd2407c9":"code","29c668c6":"code","b70c73a6":"code","1bf21095":"code","452aa984":"code","168be025":"code","5c07c055":"code","7ea8cc1d":"code","4b20f499":"code","6996988d":"code","c59f9946":"code","f8295405":"code","4cab11ac":"code","b1f5d13a":"code","32238264":"code","142668a1":"markdown","4f701e29":"markdown","978c64ce":"markdown","2a03d104":"markdown","16643ed8":"markdown","c461802e":"markdown","b98565d0":"markdown","bfe5bce0":"markdown","4339eb2b":"markdown","b1cac1de":"markdown","424c0e80":"markdown","1d062e19":"markdown","10a4895c":"markdown","0588d2fa":"markdown","c41eeab4":"markdown","e796ef8e":"markdown","1e7639b5":"markdown","54960e16":"markdown","8be0aa70":"markdown","6b1ae98d":"markdown","86e5654b":"markdown","5800fb9e":"markdown","8cd67877":"markdown","80cfce85":"markdown","15fa3936":"markdown","825486e7":"markdown","06ef79c3":"markdown","b25fd2f6":"markdown","34059989":"markdown","bd17a521":"markdown","9dd9499e":"markdown","3175e199":"markdown","18685ba8":"markdown","f247b522":"markdown","9dad77b2":"markdown","f35898ea":"markdown","bc2d56b6":"markdown","2cfe2b9a":"markdown","64f63221":"markdown","4f7053f9":"markdown","90c0151c":"markdown","c43dac6d":"markdown","58be609b":"markdown","99384066":"markdown","a6c33400":"markdown","5567d906":"markdown","5b48ddd5":"markdown","c08afd25":"markdown","83a2d7f4":"markdown"},"source":{"0364da4b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1e2994f2":"# importing necessary libraries at one place\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import GridSearchCV\n\n# mean_squared_error module for applying RMSE\nfrom sklearn.metrics import mean_squared_error as mse\n\n# hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')","279bef83":"data = pd.read_csv('\/kaggle\/input\/car-price\/CarPrice_Assignment.csv')","40c7ee06":"data.shape","5cecaefa":"# first 5 \ndata.head()","336f054f":"# summary of the dataset: 205 rows, 26 columns, no null values\ndata.info()","d8a812e8":"for i in data.columns:\n    #print(i,'\\n',data[i].value_counts(),'\\n','***'*20,'\\n')\n    print(f'Column name: [{i}]\\n{data[i].value_counts()}\\n','***'*20,'\\n')","80fce4ac":"# ploting wheetbase distribution\nplt.figure(figsize=(10,4))\nsns.distplot(data.wheelbase)\n\nplt.show()","4f2aa103":"# ploting wheetbase distribution\nplt.figure(figsize=(10,4))\nsns.distplot(data.curbweight)\n\nplt.show()","89edec52":"# ploting wheetbase distribution\nplt.figure(figsize=(10,4))\nsns.distplot(data.stroke)\n\nplt.show()","028a7155":"# ploting wheetbase distribution\nplt.figure(figsize=(10,4))\nsns.distplot(data.compressionratio)\n\nplt.show()","53bc6284":"# ploting price distribution\nplt.figure(figsize=(10,4))\nsns.distplot(data.price)\n\nplt.show()","f78b47bd":"### 1.\n# all numeric (float and int) variables in dataset\ndata_only_numeric_cols = data.select_dtypes(include=['float64','int64'])\n\n#head\ndata_only_numeric_cols.head()","5c5f8fa7":"data_only_numeric_cols.symboling.value_counts()","f9765aa4":"plt.figure(figsize=(10,4))\ndata_only_numeric_cols.car_ID.value_counts().hist()\nplt.show()","b7a41e3a":"# dropping symboling and car_ID \ndata_only_numeric_cols.drop(['car_ID','symboling'],axis=1,inplace=True)\n\ndata_only_numeric_cols.head()","de61f44e":"# paiwise scatter plot for all variables in data_only_numeric_cols\n\nsns.pairplot(data_only_numeric_cols)\n","20000b86":"# correlation matrix\ncorr = data_only_numeric_cols.corr()\n\n#print cor\ncorr","27323aab":"plt.figure(figsize=(16,8))\nsns.heatmap(corr,cmap='rainbow',annot=True)\n\nplt.show()","97dd165e":"data.symboling.dtype","fdd2ff78":"data.symboling.value_counts()","81a1d809":"# converting symboling to categorical datatype by changing its datatype to object\ndata['symboling'] = data.symboling.astype('object')\ndata.symboling.dtype","e9103890":"# CarName: first few entries (upto 30)\n\ndata.CarName[:30]","db4d7817":"# Method 1: str.split() by space\ncars_company_name = data.CarName.apply(lambda x:x.split(' ')[0])\n\n# first 30 cars name\nprint(cars_company_name[:30])","ca692f69":"# Method 2: Use regular expressions\nimport re\n\n# regex: any alphanumeric sequence before a space, may contain a hyphen\np = re.compile(r'\\w+-?\\w+')\n\n#apply above regex pattern to CarName\ncars_company_name = data.CarName.apply(lambda x:re.findall(p,x)[0])\n\n# first 30 cars name\nprint(cars_company_name[:30])","41d505b6":"# value_counts for each compnay\ndata['cars_company_name'] = cars_company_name\ndata['cars_company_name'].value_counts()","21873c26":"# volkswagen\ndata.loc[(data.cars_company_name=='vw')|(data.cars_company_name=='vokswagen'),'cars_company_name']='volkswagen'\n\n# porsche\ndata.loc[(data.cars_company_name=='porcshce'),'cars_company_name']='porsche'\n\n# toyota\ndata.loc[(data.cars_company_name=='toyouta'),'cars_company_name']='toyota'\n\n# nissan\ndata.loc[(data.cars_company_name=='Nissan'),'cars_company_name']='nissan'\n\n# mazda\ndata.loc[(data.cars_company_name=='maxda'),'cars_company_name']='mazda'","fceec85d":"# value_counts for each compnay\ndata['cars_company_name'].value_counts()","158fd0a5":"# drop CarName variable\ndata.drop('CarName',axis=1,inplace=True)","78ee110b":"# data statistical discription\ndata.describe()","95732db0":"# defining X\nX = data.drop('price',axis=1)\n\n# sefining y\ny = data.price","cce0a687":"X.head()","f60db1b5":"y.head()","15a37ccf":"# subset all categorical variables\ndata_only_cat = X.select_dtypes(include='object')\n\ndata_only_cat.head()","6d2666b4":"# convert into dummies\ndata_only_cat_dummies = pd.get_dummies(data_only_cat,drop_first=True)\n\ndata_only_cat_dummies.head()","e2ce50ad":"# droping categorical variables from X\n\nX.drop(list(data_only_cat),axis=1,inplace=True)","ae78dcb6":"X.head()","b82d5b78":"# concat dummy variables with X\n\nX = pd.concat([X,data_only_cat_dummies],axis=1)","c263b659":"X.head()","a00a7dc0":"from sklearn.preprocessing import scale","c762aa01":"X = pd.DataFrame(scale(X),columns=X.columns)\nX.head()","483e76a0":"from sklearn.model_selection import train_test_split","cb3ceddf":"# split into train and test with test_size=30% and random_state=108\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=108)","59e6b190":"X_train.shape,X_test.shape,y_train.shape,y_test.shape","7fc5d549":"# list of alpha to tune\nparams = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, \n                    10.0, 20, 50, 100, 500, 1000 ]}\n\n#initialising Ridge() function\nridge = Ridge()\n# defining cross validation folds as 5\nfolds = 5","628e8cb5":"# Defining GridSearchCV\ngrid_cv_model = GridSearchCV(estimator=ridge,\n                       param_grid=params,\n                       scoring='neg_mean_absolute_error',\n                       cv=folds,\n                       return_train_score=True,\n                       verbose=1)\n\n# fiting GridSearchCV() with X_train and y_train\ngrid_cv_model.fit(X_train,y_train)","f4b41e55":"# Saving GridSearchCV results into a dataframe \ncv_results = pd.DataFrame(grid_cv_model.cv_results_)\n\n# filter cv_results with all param_alpha less than or equal to 200\ncv_results = cv_results[cv_results['param_alpha']<=200]\n\n# cv_results head\ncv_results.head()","3db3792c":"# changing datatype of 'param_alpha' into int\ncv_results['param_alpha'] = cv_results['param_alpha'].astype('int32')\n\n# plotting\nplt.figure(figsize=(16,8))\nplt.plot(cv_results['param_alpha'],cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'],cv_results['mean_test_score'])\n\nplt.title('Negative Meam Absolute Error and alpha')\nplt.xlabel('alpha')\nplt.ylabel('Negative Meam Absolute Error')\nplt.legend(['train score','test score'],loc='upper right')\n\nplt.show()","bd2407c9":"# checking best alpha from model_cv\n\ngrid_cv_model.best_params_","29c668c6":"#seting alpha as 20\nalpha = 20 \n\n# Initialising Ridge() with above alpha\nridge =Ridge(alpha=alpha)\n\n#fitting model\nridge.fit(X_train,y_train)\n\n#printing ridge coeficients\nridge.coef_","b70c73a6":"# Initialising Lasso()\nlasso = Lasso()\n\n#usig same attributes used for Ridge tuning except estimator here would be lasso\ngrid_cv_model = GridSearchCV(estimator=lasso,\n                       param_grid=params,\n                       scoring='neg_mean_absolute_error',\n                       cv=folds,\n                       return_train_score=True,\n                       verbose=1)\n#fiting model_cv\ngrid_cv_model.fit(X_train,y_train)","1bf21095":"# Saving model_cv results into a dataframe\ncv_results = pd.DataFrame(grid_cv_model.cv_results_)\n\n# cv_results head\ncv_results.head()","452aa984":"# changing param_alpha datatype to float\ncv_results['param_alpha'] = cv_results['param_alpha'].astype('int32')\n\n# plotting\nplt.figure(figsize=(16,8))\nplt.plot(cv_results['param_alpha'],cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'],cv_results['mean_test_score'])\n\nplt.title('Negative Meam Absolute Error and alpha')\nplt.xlabel('alpha')\nplt.ylabel('Negative Meam Absolute Error')\nplt.legend(['train score','test score'],loc='upper left')\n\nplt.show()","168be025":"# Checking best  alpha from model_cv\ngrid_cv_model.best_params_","5c07c055":"# Seting alpha =100\nalpha =100\n\n# Defining lasso with above alpha\nlasso =Lasso(alpha=alpha)\n  \n# fiting lasso\nlasso.fit(X_train,y_train)","7ea8cc1d":"# printing lasso coeficients\nlasso.coef_","4b20f499":"# Initialising ElasticNet()\nelasticnet = ElasticNet() \n\n#using same attributes used for Ridge tuning except estimator here would be ElasticNet\ngrid_cv_model = GridSearchCV(estimator=elasticnet,\n                       param_grid=params,\n                       scoring='neg_mean_absolute_error',\n                       cv=folds,\n                       return_train_score=True,\n                       verbose=1)\n#fitingmodel_cv\ngrid_cv_model.fit(X_train,y_train)","6996988d":"# Saving model_cv results into a dataframe\ncv_results = pd.DataFrame(grid_cv_model.cv_results_)\n\n# cv_results head\ncv_results.head()","c59f9946":"# change param_alpha datatype to float\ncv_results['param_alpha'] = cv_results['param_alpha'].astype('int32')\n\n# plotting\nplt.figure(figsize=(16,8))\nplt.plot(cv_results['param_alpha'],cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'],cv_results['mean_test_score'])\n\nplt.title('Negative Meam Absolute Error and alpha')\nplt.xlabel('alpha')\nplt.ylabel('Negative Meam Absolute Error')\nplt.legend(['train score','test score'],loc='upper left')\n\nplt.show()","f8295405":"# Checking best  alpha from model_cv\ngrid_cv_model.best_params_","4cab11ac":"# Seting alpha=0.3\nalpha =0.3\n\n# Defining ElasticNet with above alpha\nelasticnet = ElasticNet(alpha=alpha)\n  \n# fiting elastic net\nelasticnet.fit(X_train,y_train)","b1f5d13a":"# printing ElasticNet coeficients\nelasticnet.coef_","32238264":"# Calculating all 3 predictions \npred_by_lasso =  lasso.predict(X_test)\npred_by_ridge = ridge.predict(X_test)\npred_by_elasticnet = elasticnet.predict(X_test)\n\n\n# printing RMSE for all 3 techniques\nprint(f'Lasso RMSE: {np.sqrt(mse(y_test,pred_by_lasso))}')\nprint(f'Ridge RMSE: {np.sqrt(mse(y_test,pred_by_ridge))}')\nprint(f'ElasticNet RMSE: {np.sqrt(mse(y_test,pred_by_elasticnet))}')","142668a1":"# Model Evaluation\nI am trying to compare all three model result using error term `RMSE` Root Mean Squared Error","4f701e29":"# Data Cleaning\nThere are no missing values in the dataset\nVariables are in correct format except `symboling` \n* `symboling` should rather be a Categorical Variable (so that dummy variable are created for that)","978c64ce":"**`Extracting Cars_name`**","2a03d104":"Ploting distribution plot for target variable `price`: this is price of car","16643ed8":"Ploting distribution plot for `compressionratio`: this is ratio of volume of compression chamber at largest capacity to least capacity","c461802e":"Ploting distribution plot for `wheelbase`: this is distance between centre of front and rarewheels","b98565d0":"`Observation` <br>\n* `symboling`: shows -2 (least risky) to +3 most risky but most of the cars are 0,1,2.\n* `aspiration`: (internal combustion) engine property showing whether oxygen intake is through standard(atmospheric pressure) or through turbocharging(pressurised oxygen intake)\n* `drivewheel`: frontwheel, rarewheel or four-wheel drive <br>\ntry to observe other by yourself, or ping me","bfe5bce0":"# Data Exploration\nTo perform Linear Regression(numeric) target variable should be linearly related to at least one another numeric variable\n* Let's see whether that's true in this case\n\n`Steps`:<br>\n1. subseting list of all (independent)numeric variables and then \n2. making a pairwise plot","4339eb2b":"**`plotting mean train and test scores with alpha`**","b1cac1de":"# Lasso\n**`Cross Validation and Hyperparameter Tuning`**","424c0e80":"**Ploting a `Pairwise Scatter plot` and observe Linear Relationships**","1d062e19":"Now `cars_company_name` variable looks cool to me\n\n**`Let's now drop CarName variable`**","10a4895c":"# Data Understanding and Exploration","0588d2fa":"Notice it that carname occurs before a space e.g. bmw 320i, chevrolet impala etc\n\nI am tring to simply extract string before a space, I can do it in multiple ways ","c41eeab4":"### Understanding Unique value distribution\nChecking various attributes in a feature and its contribution in dataset","e796ef8e":"**Points on `\u237a`**: <br>\n* Higher values of \u237a force coefficients to move towards zero and increases the restriction on the model. This decreases training performance, but also increases the generalizability of the model. Setting \u237a too high could lead to a model that is too simple and underfits the data\n* With lower values of \u237a the coefficients are less restricted. When \u237a is very small the model becomes more similar to linear regression above and we risk overfitting","1e7639b5":"You can find out when and where to use Lasso and Ridge in MODEL heading which is below there in this notebook","54960e16":"Observe that train and test scores start to become parallel to each other after apha crosses 0.3 \n* So lets check our Elastic model on alpha 0.3","8be0aa70":"# ElasticNet Regression\n**`Cross Validation and Hyperparameter Tuning`**","6b1ae98d":"**Extracting `company_name` from `CarName` column**","86e5654b":"# Problem\n`Car Price Prediction`","5800fb9e":"`Note`:<br> \nTraining results depend on the way train data is splitted in cross validation \n\nEach time I run, data is splitted randomly and hence can observe minor differences in your answer","8cd67877":"Observe that test and train scores start to become parallel to each other after apha crosses 20 \n\n* So lets check our ridge model on alpha 20","80cfce85":"Observe that train and test scores start to become parallel to each other after apha crosses 100 \n\n* So lets check our Lasso model on alpha 100","15fa3936":"**I am tring to apply some Advance Linear Regression like:**\n* `Lasso Regression` or `L1 Regularization`\n* `Ridge Regression` or `L2 Regularization` regularization parameter `\u237a`alpha => controls complexity of the model\n* `ElasticNet Regression` combination of L1 and L2 => if `l1_ratio=0` then L2 Reg is activated else for `l1_ratio=1` then L1 Reg => `Values between 0 and 1 give us a combination of both L1 and L2 regularization`\n* `Polynomial Regression`\n\n**also I will apply `GridSearchCV` and `Hyperparameter Tuning`** here we have `\u237a` and `l1_ratio`","825486e7":"----\n----\n\n# other notebook links:\n* [Simple Linear Regression](https:\/\/www.kaggle.com\/mukeshmanral\/linear-regression-basic)\n* [Multiple Linear Regression](https:\/\/www.kaggle.com\/mukeshmanral\/multiple-linear-regression-basic)\n* [Polynomial Regression](https:\/\/www.kaggle.com\/mukeshmanral\/polynomial-regression-basic)\n* [Advanced Linear Regression](https:\/\/www.kaggle.com\/mukeshmanral\/advance-linear-regression-basic-gridsearchcv-hpt)\n\n\n\n\n\n\n* [Feature Engineering 1](https:\/\/www.kaggle.com\/mukeshmanral\/feature-engineering-dataset-1-basic)\n* [Feature Engineering 2](https:\/\/www.kaggle.com\/mukeshmanral\/feature-engineering-dataset-2-basic)\n* [Feature Engineering 3](https:\/\/www.kaggle.com\/mukeshmanral\/feature-engineering-dataset-3-basic)\n* [Feature Engineering 4](https:\/\/www.kaggle.com\/mukeshmanral\/feature-engineering-dataset-4-basic)\n\n----\n----","06ef79c3":"Observe for our problem statement Ridge as a regularization technique giving best result \n\n**`NOTE`** <br>\n`What I can do further is also I can check for other metrics, as to choose the best model`\n\nFor now I am going any further on this dataset, I will update it more in Future....Stay connected and keep learnig and by any chance if you love the work I have done please `UPVOTE`","b25fd2f6":"![image.png](attachment:90da7012-64ea-4c89-8cc0-9011a35a0217.png)","34059989":"**Plotting `Correlations on a Heatmap`**","bd17a521":"**`Solving Above Data Quality Issue`**\n\nTrying to Replacing misspelled car_company names using loc","9dd9499e":"# Data Preparation\nNow preparing data and build the model\n\nSpliting into X and y","3175e199":"Variable `symboling` is numeric(int) treating it as categorical, since it has only 6 discrete values\n* Dont want `car_ID`.. Why?? Wait","18685ba8":"Try to observe histogram of `car_ID` column, if you can find anything inform me then\n* Most of the time ID columns are of note use, but one example where I found ID column to be usefull is `Big Mart Sales`. ","f247b522":"# Feature Scaling ","9dad77b2":"`Observations:`\n1. `Corr Among Independent Variables:`\n    * Most of independent variables are highly correlated (look at top-left part of matrix): `wheelbase`, `carlength`, `carbwidht`, `enginesize` etc are all measures of 'size\/weight' and are positively correlated\n2. `Corr price with independent variables:`\n    * Price is `highly(positively) correlated` with `wheelbase`, `carlength`, `carwidth`, `curbweight`, `enginesize`, `horsepower`(notice how all of these variables represent size\/weight\/engine power of car)\n    * Price is `negatively correlated` to `citympg` and `highwaympg` (-0.70 approximately)\n        * This suggest that cars having high mileage may fall in 'economy' cars category and are priced lower(think Maruti Alto\/Swift type of cars, which are designed to be affordable for middle class, who value mileage more than horsepower\/size of car)\n        \n\nWhile building model I'll have to pay attention to `Multicollinearity`(especially Linear Models, such as Linear Regression and Logistic Regression as they suffers more from multicollinearity)","f35898ea":"**`plotting mean train and test scores with alpha`**","bc2d56b6":"# Creating dummy variables for categorical variables","2cfe2b9a":"# Ridge Regression (L2 Regularization)\n**`Cross Validation and Hyperparameter Tuning`**","64f63221":"**`Creating new column to store Compnay Name`**","4f7053f9":"**`plotting mean train and test scores with alpha`**","90c0151c":"Quite hard to interpret and I can rather plot correlations between variables also a heatmap is pretty useful to visualise multiple correlations in one plot","c43dac6d":"# Model Building\nBefore going into `Ridge, Lasso and ElasticNet Regression`, one must understand `Bias and Variance` along with [Linear Regression](https:\/\/www.kaggle.com\/mukeshmanral\/linear-regression-basic)\n\nBut I will try to sum up everything for now:\n\n----\n* In `Linear Regression` we have Cost fucntion(ERROR) or Sum of Residual .ie `(y-yhat)^2`\n\n----\n* In `Ridge Regression` we update Cost Function(ERROR) or Sum of Residual .ie `(y-yhat)^2 + lambda * (slope)^2` => `lambda` can take 0 to 1\/0, cant be -ve value, => Usually lambda value is taken small as if lambda value will increse `slope` will try to become || to x-axis somehow but will never be 0\n    * Used to Reduce Overfitting Problem which can be understood as if you learn some topic in class say 100%(training set) and in class_test you only wrote say 50% of what you learned(test_set), now i can tell you you are Overfitting\n    * It basically penalize features having higher `slope`, as a reult Error reduces\n    * for say more then one feature values of slope will be added and remainig formula remains the same i.e (slope_1^2 + slope_2^2)\n\n----\n* In `Lasso Regression` we update Cost Function(ERROR) or Sum of Residual .ie `(y-yhat)^2 + lambda * |slope|` => magnitude of slope not square fo slope\n    * it helps in reducing Overfitting fore sure\n    * Also helps in `Feature Selection`\n        * Feature having less slope value will be removed, that means those removed feature were not important for predicting best fit line\n    * here slope moves toward 0 constantly and at some point becomes 0, so in the process feature are selected, => in case of Ridge slope only srinks but never becom 0\n    * `Good choice when we have a large number of features but expect only a few to be important`\n\n-----\n* `Elastic-Net Regression` is a linear regression model that `combines penalties of Lasso and Ridge`\n    * `l1_ratio` parameter is used to control combination of L1 and L2 regularization\n        * `l1_ratio = 0` we have L2 regularization (Ridge)\n        * `l1_ratio = 1` we have L1 regularization (Lasso)\n        * `Values between 0 and 1 give a combination of both L1 and L2 regularization`","58be609b":"Ploting distribution plot for `stroke`: this is volume of engine (distance traveled by piston in each cycle)","99384066":"----\n----\n\n# other notebook links:\n* [Simple Linear Regression](https:\/\/www.kaggle.com\/mukeshmanral\/linear-regression-basic)\n* [Multiple Linear Regression](https:\/\/www.kaggle.com\/mukeshmanral\/multiple-linear-regression-basic)\n* [Polynomial Regression](https:\/\/www.kaggle.com\/mukeshmanral\/polynomial-regression-basic)\n* [Advanced Linear Regression](https:\/\/www.kaggle.com\/mukeshmanral\/advance-linear-regression-basic-gridsearchcv-hpt)\n\n\n\n\n\n\n* [Feature Engineering 1](https:\/\/www.kaggle.com\/mukeshmanral\/feature-engineering-dataset-1-basic)\n* [Feature Engineering 2](https:\/\/www.kaggle.com\/mukeshmanral\/feature-engineering-dataset-2-basic)\n* [Feature Engineering 3](https:\/\/www.kaggle.com\/mukeshmanral\/feature-engineering-dataset-3-basic)\n* [Feature Engineering 4](https:\/\/www.kaggle.com\/mukeshmanral\/feature-engineering-dataset-4-basic)\n\n----\n----","a6c33400":"Try to Observe you will find some car-company names are misspelled like\n* `vw and vokswagen` should be `volkswagen`\n* `porcshce` should be `porsche` \n* `toyouta` should be `toyota`\n* `Nissan` should be `nissan`\n* `maxda` should be `mazda` so oon","5567d906":"# Generalised Regression using Polynomial Regression\nFor this I am thinking of taking a new dataset as it will help the learner to Explore more DataSet\n\nSoon I will update new Notebook Link so stay connected","5b48ddd5":"## Splitting into train test","c08afd25":"Ploting distribution plot for `curbweight`: this is weight of car without occupants or baggage","83a2d7f4":"![image.png](attachment:dd0d64ba-9525-4302-a5a0-7a4cf9812f14.png)"}}