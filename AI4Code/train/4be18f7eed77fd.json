{"cell_type":{"1b54ef86":"code","8e54f1dd":"code","dd342afe":"code","9fa7b078":"code","fc11ab0d":"code","76540a1b":"code","cfaf6545":"code","6f284134":"code","35da141e":"code","902eef27":"code","001b86b5":"code","cf746d16":"code","a423394f":"code","27c23c3b":"code","ba4727e4":"code","770170b8":"code","52822bda":"code","acf0dd6c":"code","9a64480b":"code","ad513cd4":"code","f7fb75c0":"code","a5c4851c":"code","79db5f26":"code","3bfcb0c1":"code","f1a0d4f1":"code","bacda764":"code","3849f9c6":"code","d71e62f8":"code","16db0c9d":"code","e6e76e7b":"code","9f7f9170":"code","3951c603":"code","6c13d3ba":"code","809d723d":"code","65b8682d":"code","736126e9":"code","b43bee26":"code","bd89239d":"code","2f561291":"code","8ab8e11e":"markdown","b550fc24":"markdown","0449d7ff":"markdown","4857493a":"markdown","3e7995e8":"markdown","e114c779":"markdown","d0802bfd":"markdown","adc481bf":"markdown","f5c4412c":"markdown","331f3cc3":"markdown","d1ef5bc9":"markdown","00863c55":"markdown","e3eed656":"markdown","15bfb446":"markdown","5ed9e1c8":"markdown","ded20189":"markdown","ce992719":"markdown","ed496f16":"markdown","0355133e":"markdown","2561e411":"markdown","4ac82ead":"markdown","6d967f29":"markdown","602dce6f":"markdown","b1fc8e4a":"markdown","a8925812":"markdown","abca8ed1":"markdown","80411f40":"markdown","0418dc1f":"markdown","e007a546":"markdown","fb23afd1":"markdown","163b5c86":"markdown"},"source":{"1b54ef86":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","8e54f1dd":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")","dd342afe":"train.info()","9fa7b078":"train.drop(columns=['Id'], inplace=True)","fc11ab0d":"import seaborn as sns\nplt.figure(figsize=(20,20))\ng = sns.heatmap(train.corr(), annot=True, cmap=\"RdYlGn\")","76540a1b":"train.isnull().sum().sort_values(ascending=False)[:30]","cfaf6545":"has_too_much_null_columns = [\n    'PoolQC',\n    'MiscFeature',\n    'Alley',\n    'Fence'\n]\n\ntrain.drop(columns=has_too_much_null_columns, inplace=True)","6f284134":"has_null_columns = [\n    'FireplaceQu',\n    'LotFrontage',\n    'GarageCond',\n    'GarageType',\n    'GarageYrBlt',\n    'GarageFinish',\n    'GarageQual',\n    'BsmtExposure',\n    'BsmtFinType2',\n    'BsmtFinType1',\n    'BsmtCond',\n    'BsmtQual',\n    'MasVnrArea',\n    'MasVnrType',\n    'Electrical'\n];\n\nfor col in has_null_columns:\n    if (train[col].dtype == np.object):\n        train[col].fillna(0, inplace=True)\n    else:\n        train[col].fillna(train[col].median(), inplace=True)","35da141e":"object_columns = list(train.select_dtypes(include=['object']).columns)","902eef27":"train_encoded = pd.get_dummies(train.iloc[:, :-1], columns=object_columns)","001b86b5":"X = train_encoded.iloc[:, :-1]\ny = train.iloc[:, -1]","cf746d16":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2020)","a423394f":"from sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\nlr.fit(X_train, y_train)\ny_preds_lr = lr.predict(X_test)","27c23c3b":"from sklearn.metrics import r2_score\nr2_score(y_test, y_preds_lr)","ba4727e4":"def adj_r2(r2score, train):\n    return (1 - (1 - r2score) * ((train.shape[0] - 1) \/ (train.shape[0] - train.shape[1] - 1)))\n\nadj_r2(r2_score(y_test, y_preds_lr), X_train)","770170b8":"import statsmodels.api as sm\nregressor_OLS = sm.OLS(endog = y, exog = X).fit()\nregressor_OLS.summary()","52822bda":"def comparing_preds_and_test(y_test, y_preds):\n    plt.scatter(y_test, y_preds)\n    plt.xlabel('y_test')                       \n    plt.ylabel('y_preds_lr')\n    plt.show()\n\ncomparing_preds_and_test(y_test, y_preds_lr)","acf0dd6c":"from sklearn.svm import SVR\n\nsvr = SVR()\nsvr.fit(X_train, y_train)\ny_preds_svr = svr.predict(X_test)","9a64480b":"r2_score(y_test, y_preds_svr), adj_r2(r2_score(y_test, y_preds_svr), X_train)","ad513cd4":"comparing_preds_and_test(y_test, y_preds_svr)","f7fb75c0":"from sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(random_state=2020)\nrf.fit(X_train, y_train)\ny_preds_rf = rf.predict(X_test)","a5c4851c":"r2_score(y_test, y_preds_rf), adj_r2(r2_score(y_test, y_preds_rf), X_train)","79db5f26":"comparing_preds_and_test(y_test, y_preds_rf)","3bfcb0c1":"from sklearn.tree import DecisionTreeRegressor\n\ndt = DecisionTreeRegressor(random_state=2020)\ndt.fit(X_train,y_train)\ny_preds_dt = dt.predict(X_test)","f1a0d4f1":"r2_score(y_test, y_preds_dt), adj_r2(r2_score(y_test, y_preds_dt), X_train)","bacda764":"comparing_preds_and_test(y_test, y_preds_dt)","3849f9c6":"from xgboost.sklearn import XGBRegressor\n\nxgb = XGBRegressor()\nxgb.fit(X_train, y_train)\ny_preds_xgb = xgb.predict(X_test)","d71e62f8":"r2_score(y_test, y_preds_xgb), adj_r2(r2_score(y_test, y_preds_xgb), X_train)","16db0c9d":"comparing_preds_and_test(y_test, y_preds_xgb)","e6e76e7b":"test = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","9f7f9170":"test.head()","3951c603":"test.drop(columns=has_too_much_null_columns, inplace=True)","6c13d3ba":"test_has_null_columns = test.isnull().sum().sort_values(ascending=False)[:30]\ntest_has_null_columns","809d723d":"test_has_null_columns = [\n    'FireplaceQu',\n    'LotFrontage',\n    'GarageCond',\n    'GarageQual',\n    'GarageYrBlt',\n    'GarageFinish',\n    'GarageType',\n    'BsmtCond',\n    'BsmtQual',\n    'BsmtExposure',\n    'BsmtFinType1',\n    'BsmtFinType2',\n    'MasVnrType',\n    'MasVnrArea',\n    'MSZoning',\n    'BsmtHalfBath',\n    'Utilities',\n    'Functional',\n    'BsmtFullBath',\n    'BsmtUnfSF',\n    'SaleType',\n    'BsmtFinSF2',\n    'BsmtFinSF1',\n    'Exterior2nd',\n    'Exterior1st',\n    'TotalBsmtSF',\n    'GarageCars',\n    'KitchenQual',\n    'GarageArea'\n]\n\nfor col in test_has_null_columns:\n    if (test[col].dtype == np.object):\n        test[col].fillna(0, inplace=True)\n    else:\n        test[col].fillna(test[col].median(), inplace=True)","65b8682d":"test_object_columns = list(test.select_dtypes(include=['object']).columns)\ntest_encoded = pd.get_dummies(test, columns=test_object_columns)\ntest_encoded","736126e9":"for col in test_encoded.columns:\n    if (col not in X.columns):\n        test_encoded.drop(columns=[col], inplace=True)\n\nX.shape, test_encoded.shape","b43bee26":"for col in X.columns:\n    if (col not in test_encoded.columns):\n        test_encoded[col] = 0\n        \nX.shape, test_encoded.shape","bd89239d":"y_preds_lr_res = lr.predict(test_encoded)\ny_preds_svr_res = svr.predict(test_encoded)\ny_preds_rf_res = rf.predict(test_encoded)\ny_preds_dt_res = dt.predict(test_encoded)","2f561291":"i = 0\nrows_list = []\nfor pred in y_preds_lr_res:\n    row = {'Id': test[\"Id\"][i], 'SalePrice': pred}\n    i += 1\n    rows_list.append(row)\ndf = pd.DataFrame(rows_list) \ndf.to_csv(\"y_preds_lr_res.csv\", index=False)\n\ni = 0\nrows_list = []\nfor pred in y_preds_svr_res:\n    row = {'Id': test[\"Id\"][i], 'SalePrice': pred}\n    i += 1\n    rows_list.append(row)\ndf = pd.DataFrame(rows_list) \ndf.to_csv(\"y_preds_svr_res.csv\", index=False)\n\ni = 0\nrows_list = []\nfor pred in y_preds_rf_res:\n    row = {'Id': test[\"Id\"][i], 'SalePrice': pred}\n    i += 1\n    rows_list.append(row)\ndf = pd.DataFrame(rows_list) \ndf.to_csv(\"y_preds_rf_res.csv\", index=False)\n\ni = 0\nrows_list = []\nfor pred in y_preds_dt_res:\n    row = {'Id': test[\"Id\"][i], 'SalePrice': pred}\n    i += 1\n    rows_list.append(row)\ndf = pd.DataFrame(rows_list) \ndf.to_csv(\"y_preds_dt_res.csv\", index=False)","8ab8e11e":"## RandomForestRegressor algorithm","b550fc24":"We see that our target column has some high correlation values with several columns. Let us keep in mind it. Now we gonna find null values:","0449d7ff":"# My score: 11.1444% (hmm... cool number :D)\n# Final | Zhandos Ainabek CSSE-192M [ID=24506]","4857493a":"The results are not so good enough as in `RegressionTreeClassifier`. It is not so surprisingly.","3e7995e8":"`XGBRegressor` shows good resutls. We could use `GridSearch` in order to make better results for this type of algorithm.","e114c779":"For other columns those have less null values, we will fill `median` value for float columns (because median is much better than mean if there are outliers) and fill `0` for object columns.","d0802bfd":"Our prediction values are closer to our actual values. This good result so let us move to next algorith.","adc481bf":"We also can calculate so called `Adjusted R^2`:","f5c4412c":"And we can also look at on `Ordinary least squares` summary:","331f3cc3":"And calculate `R^2` score:","d1ef5bc9":"Let us separate our encoded set as independent variables and target variables:","00863c55":"## Contents\n- Data preparation\n- Feature selection and data cleaning\n- Data preprocessing\n- LineaerRegression algorithm\n- SVR algorithm\n- RandromForestRegressor algorithm\n- DecisionTreeRegressor algorithm\n- XGBRegressor algorithm\n- Submitting results","e3eed656":"Our prediction values are closer to our actual values. This good result so let us move to next algorithm.","15bfb446":"## DecisionTreeRegressor algorithm","5ed9e1c8":"And start encoding those object columns (one-hot encoding):","ded20189":"Then let us look at correlation situation of our dataset:","ce992719":"## Linear Regression algorithm\nLet us initialize Linear Regression object, fit our model and predict local values:","ed496f16":"## SVR algorithm\nWe gonna perform the same operation as in `Logisitin Regression` section:","0355133e":"## The objectives of this final\nIn this competition we have to build a regression model that predicts the selling price of a house depending on the parameters of X. We have to make feature selection, select those parameters that should be left in the model and which we should get rid of.","2561e411":"## Data preparation\nLet us import basic libraries in order to start making EDA on our dataset:","4ac82ead":"The result is not so good as we expect. Maybe our default parameters for SVR is not so good. So maybe we had to use `Grid Search` to find best parameters in order to improve our results.","6d967f29":"Let us see information about the dataset:","602dce6f":"Then we have to import our dataset to train our future model:","b1fc8e4a":"In order to compare our actual test values and prediction values we can create method that will help us visualize our results:","a8925812":"## Data preprocessing\nNow let us identidy which columns are object columns:","abca8ed1":"And preparing local train and test variables to evaluate final result:","80411f40":"We see that we have about 4 columns that has too much null values. We gonna drop them:","0418dc1f":"## Submitting results\nIn order to submit our results by predicting using test variables we will perform all necessary operation on test dataset.","e007a546":"We see that our dataset has `81` columns where amount of object columns is half of whole amount of the columns (43). If there were no categorical values we could start building our model without doing dataset preprocessing steps such as one-hot encoding. However in our case we have to perform above operations. Therefore let us start from data cleaning.","fb23afd1":"## Feature selection and data cleaning\nHalf amount of columns has categorical values, that is why, it seems that we may drop some of them if they are useless. First we will drop ID column because intuitively this column does not have influence on target column:","163b5c86":"## XGBRegressor algorithm"}}