{"cell_type":{"961db603":"code","94f2cace":"code","3a346e1d":"code","0b976682":"code","5b8a61c3":"code","148037cb":"code","5a874b2c":"code","54cb257c":"code","d41cd97e":"code","610d0727":"code","f1f305f7":"code","41d22543":"code","2431f706":"code","cfc365ba":"code","129d5116":"code","41a3d0c8":"code","b9f3f72b":"code","78d92133":"code","19272fbc":"code","8e472fdb":"code","00e728f7":"code","58d04e47":"code","1b51c2c8":"code","91d1d0cc":"code","924ede38":"code","ad255aa5":"code","f2184c5c":"code","2a927e9f":"code","a22a3072":"code","4752d9c9":"code","8288735b":"code","70063cd0":"code","aa34ff7c":"code","7c82019f":"code","a6b61e8a":"code","1484f720":"code","5f0fe6c2":"code","db09e988":"code","f85fbed0":"code","a3e62c43":"code","39875a5d":"code","8b3dfcd9":"code","257114ac":"code","faf20f25":"code","6eceb9a9":"code","f1556633":"code","f17fa5d7":"code","9bebe812":"code","c32f5a34":"code","27dc3d5d":"code","7837380f":"code","2945b3fd":"code","ed48b3f1":"code","a281ba58":"code","7d5212d0":"code","cb8cec07":"markdown","9335ae1d":"markdown","6fd46c84":"markdown","a079f727":"markdown","2f9ec11e":"markdown","45275a36":"markdown","3c79a465":"markdown","bb03f5c9":"markdown","bd9b8eb9":"markdown","f38c6efb":"markdown","6dc6408f":"markdown","dbc6bd2a":"markdown","cb159e28":"markdown","0e82e1e5":"markdown","3b0b307b":"markdown","44543b7b":"markdown","675b383c":"markdown","c7a88bc2":"markdown","f7c66344":"markdown","2187e587":"markdown","6cd18f0a":"markdown","58883d3a":"markdown","625cd838":"markdown","7b6702c5":"markdown","e82bf8e1":"markdown"},"source":{"961db603":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","94f2cace":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\n%matplotlib inline","3a346e1d":"bankchurners = pd.read_csv(\"\/kaggle\/input\/credit-card-customers\/BankChurners.csv\")\nbankchurners.head(10)","0b976682":"bankchurners.columns","5b8a61c3":"bankchurners.drop(['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',\n       'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'], axis=1, inplace=True)","148037cb":"bankchurners.isnull().sum()","5a874b2c":"bankchurners.describe()","54cb257c":"bankchurners.drop(['CLIENTNUM'],axis=1,inplace=True)","d41cd97e":"numerical_features = bankchurners._get_numeric_data().columns\nnumerical_features","610d0727":"# Categorical Columns\ncategorical_features = bankchurners.select_dtypes(include='object').columns\ncategorical_features","f1f305f7":"sns.countplot('Income_Category', data=bankchurners)","41d22543":"bankchurners['Income_Category'] = bankchurners['Income_Category'].replace({\n    'Unknown':0,\n    'Less than $40K':1,\n    '$40K - $60K':2,\n    '$60K - $80K':3,\n    '$80K - $120K':4,\n    '$120K +':5\n})","2431f706":"sns.countplot('Attrition_Flag', data=bankchurners)","cfc365ba":"bankchurners['Attrition_Flag'] = bankchurners['Attrition_Flag'].replace({\n    'Existing Customer':0,\n    'Attrited Customer':1,\n})","129d5116":"categorical = bankchurners.select_dtypes(include='object').columns\ncategorical","41a3d0c8":"bankchurners = pd.get_dummies(bankchurners, columns = categorical, drop_first=True)","b9f3f72b":"bankchurners.info()","78d92133":"bankchurners.rename(columns={\n    \"Education_Level_High School\": \"Education_Level_High_School\",\n    \"Education_Level_Post-Graduate\": \"Education_Level_Post_Graduate\",\n},inplace=True)\nbankchurners.columns.values","19272fbc":"from sklearn.model_selection import train_test_split\ny = bankchurners['Attrition_Flag']\nX = bankchurners.drop(['Attrition_Flag'], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42,stratify =y)","8e472fdb":"train = pd.concat([X_train, y_train],axis=1)\ntest = pd.concat([X_test, y_test],axis=1)","00e728f7":"sns.set(font_scale=1.5)\ndef box_plot(key):\n    fig = plt.figure(figsize=(30, 20));\n    sns.boxplot(x='Attrition_Flag', y=key, data=train[['Attrition_Flag', key]])","58d04e47":"box_plot('Customer_Age')","1b51c2c8":"box_plot('Dependent_count')","91d1d0cc":"box_plot('Months_on_book')","924ede38":"box_plot('Total_Relationship_Count')","ad255aa5":"box_plot('Months_Inactive_12_mon')","f2184c5c":"box_plot('Contacts_Count_12_mon')","2a927e9f":"box_plot('Credit_Limit')","a22a3072":"box_plot('Total_Revolving_Bal')","4752d9c9":"box_plot('Avg_Open_To_Buy')","8288735b":"box_plot('Total_Amt_Chng_Q4_Q1')","70063cd0":"box_plot('Total_Trans_Amt')","aa34ff7c":"box_plot('Total_Trans_Ct')","7c82019f":"box_plot('Total_Ct_Chng_Q4_Q1')","a6b61e8a":"box_plot('Avg_Utilization_Ratio')","1484f720":"from sklearn.base import BaseEstimator, TransformerMixin\nclass RemoveOutliers(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, feature, min_value, max_value):\n        self.feature = feature\n        self.min_value = min_value\n        self.max_value = max_value\n        \n    def fit(self, X, y=None):\n        return self # nothing else to do\n    \n    def transform(self, X, y=None):\n        \n        feature_values = X[self.feature]\n        feature_values[feature_values < self.min_value ] = self.min_value\n        feature_values[feature_values > self.max_value ] = self.max_value\n        \n        X[self.feature] = feature_values\n        \n        return X","5f0fe6c2":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_selection import VarianceThreshold\n\nnumerical_pipeline = Pipeline([\n    ('age_outliers',RemoveOutliers('Customer_Age',0,70)),\n    ('varianceThreshold', VarianceThreshold()),\n    ('std_scaler', StandardScaler())])\n\nfull_pipeline = ColumnTransformer([(\"num\", numerical_pipeline, numerical_features)])\n\nX_train_values = full_pipeline.fit_transform(X_train)\nX_test_values = full_pipeline.transform(X_test)","db09e988":"X_train[numerical_features] = X_train_values\nX_test[numerical_features] = X_test_values","f85fbed0":"train = pd.concat([X_train, y_train],axis=1)\ntest = pd.concat([X_test, y_test],axis=1)","a3e62c43":"def remove_columns(df, columns):\n    for c in columns:\n        if c in df.columns:\n            df.drop(c, axis=1, inplace=True)","39875a5d":"from scipy.stats import pearsonr\ndef highly_correleted_columns(df, columns_to_preserve, threshold):\n    corr_columns=[]\n    for c in df.columns:\n        # column to preserve\n        if c in corr_columns:\n            continue\n        # correlation with pval\n        for cc in df.columns:\n            if cc == c:\n                continue\n            if cc in columns_to_preserve:\n                continue\n            if cc in corr_columns:\n                continue\n            corrtest = pearsonr(df[c], df[cc])\n            corr = corrtest[0]\n            pval = corrtest[1]\n            if abs(corr) > threshold and pval < 0.05:\n                corr_columns.append(cc)\n    return corr_columns","8b3dfcd9":"columns_to_remove = highly_correleted_columns(train,['Attrition_Flag'], 0.70)\ncolumns_to_remove","257114ac":"remove_columns(train, columns_to_remove)\nremove_columns(test, columns_to_remove)","faf20f25":"from patsy import dmatrices\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\ndef compute_variance_inflation_factor(df, column_to_predict):\n\n    feature_columns = list(df.columns.values)\n    # always remove the column to predict\n    feature_columns.remove(column_to_predict)\n    features = \"+\".join(feature_columns)\n\n    # get y and X dataframes based on this regression:\n    y, X = dmatrices(column_to_predict + '~' + features, data=df, return_type='dataframe')\n\n    # Calculate VIF Factors, for each X, calculate VIF and save in dataframe\n    vif = pd.DataFrame()\n    vif[\"VIF_Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    vif[\"features\"] = X.columns\n\n    # Inspect VIF Factors\n    print(vif.sort_values('VIF_Factor'))\n    return vif","6eceb9a9":"vif = compute_variance_inflation_factor(train, 'Attrition_Flag')","f1556633":"nans_columns = vif[vif.isin([np.nan, np.inf, -np.inf]).any(1)].features.values\nremove_columns(train, nans_columns)\nremove_columns(test, nans_columns)","f17fa5d7":"highly_collinear = vif.loc[vif.VIF_Factor > 5.0].features.values\nremove_columns(train, highly_collinear)\nremove_columns(test, highly_collinear)","9bebe812":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, recall_score\nimport optuna","c32f5a34":"import itertools\ndef plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","27dc3d5d":"import xgboost as xgb\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom imblearn.over_sampling import BorderlineSMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline as imblearn_pipeline\n\nover = BorderlineSMOTE(sampling_strategy=0.3)\nunder = RandomUnderSampler(sampling_strategy=0.6)\nsteps = [('oversampling', over), ('undersampling', under)]\n\ndef objectiveXGBoost(trial):\n    \n    over = BorderlineSMOTE(sampling_strategy=0.3)\n    under = RandomUnderSampler(sampling_strategy=0.6)\n    \n    gamma_int = trial.suggest_float('gamma', 0.01, 10,log=True)\n    max_depth = trial.suggest_int('max_depth', 1, 5)\n    clf = xgb.XGBClassifier(n_jobs=3,seed=42,gamma=gamma_int,max_depth=max_depth)\n    \n    # by using a pipeline the metric is computed on the original, not balanced dataset\n    full_pipeline = imblearn_pipeline( [('oversampling', over), ('undersampling', under),('model',clf)])\n    \n    # stratified k-fold cross-validation \n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n    \n    return cross_val_score(full_pipeline, X_train,y_train, n_jobs = 3, cv=cv, scoring='neg_log_loss').mean()","7837380f":"import optuna\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objectiveXGBoost, n_trials=30)\ntrial = study.best_trial\nprint(trial.params)","2945b3fd":"over = BorderlineSMOTE(sampling_strategy=0.3)\nunder = RandomUnderSampler(sampling_strategy=0.6)\nsteps = [('oversampling', over), ('undersampling', under)]\nsampling_pipeline = imblearn_pipeline(steps=steps)\nX_train_fit, y_train_fit = sampling_pipeline.fit_resample(X_train, y_train)","ed48b3f1":"clf = xgb.XGBClassifier()\nclf.set_params(**study.best_trial.params)\nclf.fit(X_train_fit,y_train_fit)","a281ba58":"yhat = clf.predict(X_test)","7d5212d0":"cnf_matrix = confusion_matrix(y_test, yhat, labels=[0,1])\nplot_confusion_matrix(cnf_matrix, classes=['Existing Customer','Attrited Customer'],normalize=True,  title='Confusion matrix')\nprint('Recall is ', recall_score(y_test, yhat, average='macro'))","cb8cec07":"# Predict Churning customers","9335ae1d":"## Remove highly correlated columns","6fd46c84":"## Define a pre-processing pipeline","a079f727":"## Check outliers on numerical features","2f9ec11e":"## How many NaNs values are present?","45275a36":"# Common imports","3c79a465":"The goal of this project is to predict the customers who want to cancel a credit card program, such that actions can be taken to prevent the event from happening.\n\nThe top priority is to identify customers who are getting churned. Even if we predict non-churning customers as churned, it won\u2019t harm our business. But predicting churning customers as non-churning will do. So recall (True positives\/(True positives + False negatives) must be high.\n\nThe dataset is strongly un-balanced: only 16% of customers churned.\n\nThe notebook is organized as follow:\n\n+ In Section 1, the dataset is explored, checking if null values are present.\n\n+ In Section 2, feature engineering is performed as follow:\n\n    + The categorical target feature (the Attrition_Flag) is converted to numerical.\n    + Other categorical features are one-hot encoded.\n    + The dataset is divided into train and test, using stratified sampling.\n    + The outliers in the training dataset are identified.\n    + A data preprocessing pipeline is built, removing the outliers identified in the previous step and standardizing each feature.\n    + Highly correlated features are removed, identifing highly correleted feature pairs\n    + Highly multicollinear features are removed, estimating the variance inflation factor\n\n+ In Section 3, XGBoost is used as a model for predicting churned\/not churned customers. Model hyperparameters are searched as follow:\n\n    + An objective function is defined. The objective function computes the average value of the cross-validation score on the training dataset, using the negative log loss as a scoring metric\n    + The maximum value of the objective function is searched using the Bayesian framework Optuna [https:\/\/optuna.org\/]\n    + On the test dataset, the recall value is 0.927","bb03f5c9":"# 3. Modelling","bd9b8eb9":"## CLIENTNUM will not have predictive power, it is an id","f38c6efb":"## XGBoost classifier","6dc6408f":"## Plot confusion matrix","dbc6bd2a":"## Convert attrition_flag to numerical","cb159e28":"## Which columns are numerical and which categorical?","0e82e1e5":"# Function to remove columns in both train and test set","3b0b307b":"### Write a custom transform to remove outliers","44543b7b":"## Use SMOTE and RandomUnderSampler to reduce imbalance","675b383c":"## For the other categorical variables, use dummies (OneHot encoding)","c7a88bc2":"## Ignore the last 2 columns (as suggested by the data description section)","f7c66344":"## Convert the income into ordinal features, it makes sense for this categorical variable","2187e587":"## The dataset used for fitting should be re-balanced","6cd18f0a":"## Raname columns to remove white spaces and other charaters incompatible with patsy (used in the next section)","58883d3a":"## Remove highly collinear features","625cd838":"## Split the dataset in train and dev, before any other analysis","7b6702c5":"# 2. Feature Engineering","e82bf8e1":"## 1. Explore dataset"}}