{"cell_type":{"0204ee60":"code","a7ed11e7":"code","9cbfb0d9":"code","20bd1517":"code","ace5e38e":"code","59e313e5":"code","922c450f":"code","1d4adc1c":"code","d27b826c":"code","1853f762":"code","afcbbbb9":"code","ea3942e5":"code","ae00faed":"code","8ca06918":"code","27e3e530":"code","c887338f":"code","77704ead":"code","9b6831b1":"code","03a89cab":"code","c75cd294":"markdown"},"source":{"0204ee60":"# We will use the official tokenization script created by the Google team\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","a7ed11e7":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input, LSTM, Bidirectional\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\nimport tokenization","9cbfb0d9":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = ['[CLS]'] + text + ['[SEP]']\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","20bd1517":"def build_model(bert_layer, max_len=512):\n    def inner_build_model():\n        input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name='input_word_ids')\n        input_mask = Input(shape=(max_len,), dtype=tf.int32, name='input_mask')\n        segment_ids = Input(shape=(max_len,), dtype=tf.int32, name='segment_ids')\n\n        _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n#         clf_output = Bidirectional(LSTM(128))(sequence_output)\n        clf_output = sequence_output[:, 0, :]\n        out = Dense(1, activation='sigmoid')(clf_output)\n\n        model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n        model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy', 'mse'])\n    \n        return model\n    return inner_build_model","ace5e38e":"%%time\nmodule_url = 'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1'\n# module_url = 'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1'\n\nbert_layer = hub.KerasLayer(module_url, trainable=True)","59e313e5":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","922c450f":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","1d4adc1c":"train['token_len'] = train['text'].apply(lambda x: len(tokenizer.tokenize(x)))\ntest['token_len'] = test['text'].apply(lambda x: len(tokenizer.tokenize(x)))","d27b826c":"token_max_len = max(train['token_len'].max(), test['token_len'].max()) + 2\ndisplay(token_max_len)","1853f762":"from sklearn.model_selection import train_test_split, KFold, cross_val_score\n\ndef get_kfold_sets(train, k=5):\n    kf = KFold(n_splits=k, shuffle=True)\n    for train_texts, train_labels in kf.split(train.text.values, train.target.values):\n        train_texts, valid_texts, train_labels, valid_labels = train_test_split(train.text.values, train.target.values, test_size=0.2)\n        train_input = bert_encode(train_texts, tokenizer, max_len=token_max_len)\n        valid_input = bert_encode(valid_texts, tokenizer, max_len=token_max_len)\n        \n        yield train_input, train_labels, valid_input, valid_labels\n\ndef get_train_sets(train):\n    train_input = bert_encode(train.text.values, tokenizer, max_len=token_max_len)\n    train_labels = train.target.values\n    \n    return train_input, train_labels","afcbbbb9":"test_input = bert_encode(test.text.values, tokenizer, max_len=token_max_len)","ea3942e5":"from sklearn.metrics import f1_score\nfrom keras.callbacks import Callback\n\nclass F1Callback(Callback):\n    def __init__(self, model, X_val, y_val):\n        self.model = model\n        self.X_val = X_val\n        self.y_val = y_val\n\n    def on_epoch_end(self, epoch, logs):\n        pred = self.model.predict(self.X_val)\n        f1_val = f1_score(self.y_val, np.round(pred))\n        print('\\n f1_val = ', f1_val)","ae00faed":"model_template = build_model(bert_layer, max_len=token_max_len)()\nmodel_template.summary()","8ca06918":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.models import clone_model\n\ndef cross_val_score(train, k=3, epochs=10, batch_size=16):\n    f1_vals = []\n    models = []\n    i = 0\n    for train_input, train_labels, valid_input, valid_labels in get_kfold_sets(train, k=k):\n        model = clone_model(model_template)\n        model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy', 'mse'])\n        train_history = model.fit(\n            train_input, train_labels,\n            validation_data=(valid_input, valid_labels),\n            epochs=epochs,\n            batch_size=batch_size,\n            callbacks=[EarlyStopping(patience=1, monitor='val_mse', mode='min', verbose=True)]\n        )\n        pred = model.predict(valid_input)\n        f1_val = f1_score(valid_labels, np.round(pred))\n        print(f'f1-val: {f1_val}')\n        f1_vals.append(f1_val)\n        models.append(model)\n        df = pd.DataFrame(train_history.history)\n        df['f1-val'] = f1_val\n        df.to_csv(f'history_{i}.csv')\n        i += 1\n    return np.array(f1_vals).mean(), models\n\nk = 5\nf1_val, models = cross_val_score(train, k=k)\nprint(f'f1-mean: {f1_val}')","27e3e530":"train_input, train_labels = get_train_sets(train)","c887338f":"def calc_best_threshold(pred, labels):\n    f1_vals = []\n    ts = []\n    for t in np.arange(0.1, 1, 0.1):\n        f1_val = f1_score(train_labels, [1 if p >= t else 0 for p in train_pred])\n        f1_vals.append(f1_val)\n        ts.append(t)\n    return ts[np.argmax(f1_vals)]\n\nbest_ts = []\nfor model in models:\n    train_pred = model.predict(train_input)\n    tmp = calc_best_threshold(train_pred, train_labels)\n    best_ts.append(tmp)\n\nprint(f'best ts: {best_ts}')","77704ead":"test_preds = []\nfor model in models:\n    test_pred = model.predict(test_input)\n    test_preds.append(test_pred)\n\ntest_preds = np.array(test_preds)\nprint(test_preds.shape)","9b6831b1":"test_size = test_preds.shape[1]\nmean_pred = []\nfor s in range(test_size):\n    tmp = []\n    for i in range(k):\n#         tmp.append(test_preds[i][s][0].round())\n        tmp.append(1 if test_preds[i][s][0] >= best_ts[i] else 0)\n    mean = np.mean(tmp)\n    mean_pred.append(mean)\n\nmean_pred = np.array(mean_pred)\nprint(mean_pred.shape)\nprint(mean_pred[20:])\nprint(mean_pred[:20])","03a89cab":"submission['target'] = mean_pred.round().astype(int)\nsubmission.to_csv('submission.csv', index=False)","c75cd294":"# BERT with KFold\n\n## References\n\n* https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub\n* https:\/\/qiita.com\/koshian2\/items\/81abfc0a75ea99f726b9"}}