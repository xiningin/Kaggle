{"cell_type":{"459284e1":"code","1cc9aa8d":"code","3522030d":"code","08612cb6":"code","e28e83d5":"code","f6faa47b":"code","657d4f46":"code","e4d39b47":"code","fd0e17ad":"code","6574b9db":"code","e3e195fe":"code","8457887f":"code","5c67d5ef":"code","561b9254":"code","e4f6039a":"code","cd5b3076":"code","d3a92894":"code","1107984d":"code","994971cc":"code","1196083d":"code","ca49fd65":"code","6b24be07":"code","58f67ab7":"code","c7d3d568":"code","0e66248b":"code","57af2b89":"markdown","5de440d9":"markdown","a8f45139":"markdown","c1427659":"markdown","82b6e9ef":"markdown","6ba41a9e":"markdown","4dd8f073":"markdown","b46fdf5e":"markdown","6e710dbf":"markdown","efe601bb":"markdown","bffadaae":"markdown","f8ce93f4":"markdown","1f28c160":"markdown","18f685d1":"markdown","a1ea18cf":"markdown","989cdb99":"markdown","5009323f":"markdown","f38748ae":"markdown","7c2ea85f":"markdown","382a26a9":"markdown","9a23d56b":"markdown","6f1b76e7":"markdown","a6e2d1be":"markdown","db0a86c1":"markdown","1c123acd":"markdown","a90183b9":"markdown","b2e383e3":"markdown","9901b39b":"markdown","7f6f4365":"markdown","c2c67c88":"markdown","8279ff94":"markdown","3169f102":"markdown","12c2710e":"markdown","844ffe0d":"markdown","6298c15d":"markdown","17de943c":"markdown","cba3e105":"markdown","85c1b519":"markdown","8b53062a":"markdown","9175fc09":"markdown","dc82c11c":"markdown","c91ffb85":"markdown","33c9a4d3":"markdown","24fa2bd9":"markdown","e1f64311":"markdown","eb223782":"markdown","e7c70955":"markdown","988de831":"markdown"},"source":{"459284e1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1cc9aa8d":"import spacy\nimport re\n\nnlp = spacy.load('en_core_web_sm')  # Load the pre-trained model\n\ntext = \"Hello! I don't know khat I'm doing here.\"\n\ndoc = nlp(text)","3522030d":"print(doc)\nprint(type(doc))","08612cb6":"tokens = [token.text for token in doc]\nprint(tokens)\n# SpaCy knows exactly what to do!!","e28e83d5":"tokens = [token.lemma_ for token in doc]\nprint(tokens)\n# SpaCy knows exactly what to do!!","f6faa47b":"print('dog cat'.isalpha(), end=' ')\nprint('U.S.A'.isalpha(), end=' ')\nprint('AKB48'.isalpha(), end=' ')","657d4f46":"string = \"\"\"OMG!!! This is like   the best thing ever \\t\\n\nWow, such    an amazing  song ! I'm hooked.  Top 5 definitely. ? BNK48 is also my favorite!\"\"\"\n\nnlp = spacy.load('en_core_web_sm')\ndoc = nlp(string)","e4d39b47":"tokens = [token.text for token in doc]\nprint(tokens)","fd0e17ad":"lemmas = [token.lemma_ for token in doc]\nprint(lemmas)","6574b9db":"new_token = [e for e in lemmas if e.isalpha() or e=='-PRON-' or re.match(r'\\w+\\d+', e) is not None]\nprint(new_token)\nprint('\\n' + ' '.join(new_token))","e3e195fe":"import nltk\n\nstopwords_spacy = spacy.lang.en.stop_words.STOP_WORDS\nstopwords_nltk = nltk.corpus.stopwords.words('english')\n\nnew_token_stop_spacy = [e for e in new_token if e not in stopwords_spacy]\nnew_token_stop_nltk = [e for e in new_token if e not in stopwords_nltk]","8457887f":"print('SpaCy stopwords\\n', new_token_stop_spacy)\nprint('\\nnltk stopwords\\n', new_token_stop_nltk)","5c67d5ef":"nlp = spacy.load('en_core_web_sm')\n\ntext_1 = \"The bear is an animal\"\ntext_2 = \"Please bear with me\"\ntext_3 = \"Jane is an amazing player\"\n\nfor text in [text_1, text_2, text_3]:\n    print(text)\n    doc = nlp(text)\n    print([(e.text, e.pos_) for e in doc],'\\n')","561b9254":"string = \"\"\"Jamie Vardy, a chairman at Woodbull.com, analyzed the Sharpe ratio of Bitcoin in contrast with \nother assets to calculate its reward-to-risk profile.\"\"\"\n\nnlp = spacy.load('en_core_web_sm')\ndoc = nlp(string)\n\n[(e.text, e.label_) for e in doc.ents]","e4f6039a":"# EXAMPLE\ntext_1 = \"The lion is the king of jungle. The lion is an dangerous species\"\ntext_2 = \"Lions have lifespans of a decade\"\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncorpus = [text_1, text_2]\nvectorizer = CountVectorizer(stop_words='english').fit(corpus)\n\nprint('word : its index')\nvectorizer.vocabulary_","cd5b3076":"vec_1 = vectorizer.transform([text_1])\nprint(vec_1.A)","d3a92894":"vectors = vectorizer.transform(corpus)\nvectors.toarray()","1107984d":"# Return an array of word in the same order of word in vector.\nvectorizer.get_feature_names()","994971cc":"corpus = [\"The movie was good and not boring\", \"The movie was not good and boring\"]\n\nvectorizer = CountVectorizer(stop_words=None).fit(corpus)\nvectors = vectorizer.transform(corpus)\nvectors.A","1196083d":"# Create 1, 2 grams\nvectorizer = CountVectorizer(ngram_range=(1,2)).fit(corpus)\nvectors = vectorizer.transform(corpus)\n\nprint(vectorizer.get_feature_names())\nprint(vectors.A)","ca49fd65":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntext_1 = \"The lion is the king of jungle. The lion is an dangerous species\"\ntext_2 = \"Lions have lifespans of a decade\"\n\ncorpus = [text_1, text_2]\nvectorizer = TfidfVectorizer(stop_words='english').fit(corpus)\n\nvectors = vectorizer.transform(corpus)\nprint(vectors.toarray())","6b24be07":"from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\nimport numpy as np\n\nX = np.array([[4, 7, 1], [5, 2, 3]])\n\nscore = cosine_similarity([[4, 7, 1]], [[5, 2, 3]])\nprint(score, end='\\n\\n')\n\nscore = cosine_similarity(X)\nprint(score)","58f67ab7":"import pandas as pd\n\ndata = pd.read_csv('\/kaggle\/input\/movie-plots\/movie_overviews.csv')\ndata = data.drop_duplicates()\ndata = data.dropna()\ndata = data.reset_index()\ndata.head()","c7d3d568":"vectorizer = TfidfVectorizer(stop_words='english').fit(data['overview'])\n\ndef preprocess(X,Y):\n    return vectorizer.transform(X), vectorizer.transform(Y)\n\ndef get_recommend(title, k=10):\n    \n    if title not in data['title'].values: return None\n    \n    X = data['overview']\n    Y = data[data['title']==title]['overview']\n    \n    X, Y = preprocess(X,Y)\n    \n    s = linear_kernel(X,Y)\n    s_t = list(zip(s.flatten(), data['title'].values)) \n    \n    return sorted(s_t, reverse=True)[1:k+1]","0e66248b":"get_recommend('Star Wars')","57af2b89":"# PART1\n\n## Introduction\nNumerical data is straightforward, Categorical one is have its own way. <br>\nBut text ??? <br>\n\nText preprocessing\n1. Convert to lowercase\n2. Convert to base-form\n3. Convert from *String* to *Vector* in order to use ML \n\nIn order to feed the data into any model, we have to make it tabular <br>\nthat is having ***features as a header***, observations as rows in the table.<br>\n\nThis notebook is about constructing these features from our text!!\n\n---","5de440d9":"-> To word-sense disambiguous <br>\nApplication : Sentimental analysis, Question answering, Fake news detection, Spam detection.","a8f45139":"*Always see other third party stopwords list. <br>*\n\n\nOther text preprocessing techniques <br>\n- Removing HTML\/XML tags\n- Replacing accented characters(such as Qu\u00e9bec)\n- Spelling correction\n- Expand contractions & Abbreviations","c1427659":"# PART2\n## 1.1) Tokenization & Lemmatization\nWe need to make our data machine-friendly\n(Dogs, dog), (reduction, REDUCING, Reduce), (don't, do not), (won't, will not) are the same for machine\n\nby performing text-preprocessing\n- Lowercasing\n- Tokenization\n- Remove punctuation, stopwords\n- Expanding contractions","82b6e9ef":"Context of the words is lost!! Sentimental depends on the position of words. <br>\n\n**Application**\n- Sentence completion\n- Spelling correction\n- Machine translation","6ba41a9e":"Problem with Bag-of-word and Tf-idf <br>\n\"I'm happy\", \"I'm joyous\", \"I'm sad\" 2 of 3 of them have the exact same similarity! <br>\nThis beacuse 'happy', 'joyous', 'sad' are considered to be completely different words. It's something that the techniques we've learned can't capture.","4dd8f073":"### - CountVectorizer's preprocessing\n- **strip_accents** : *{\u2018ascii\u2019, \u2018unicode\u2019}*, default=None\n- **lowercase** : *bool*, default=True\n- **stop_words** : *string {\u2018english\u2019}, list*, default=None\n- **token_pattern** : *regex* denoting what constitutes a \u201ctoken\u201d\n- **ngram_range** : *tuple (min_n, max_n)*, default=(1, 1)\n- **max_df** : *float in range [0.0, 1.0] or int*, default=1.0 <br>\nWhen building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold\n- **min_df** : *float in range [0.0, 1.0] or int*, default=1 <br>\nWhen building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold\n- **vocabulary** : *Mapping or iterable*, default=None <br>\nA dict where keys are terms and values are indices in the feature matrix.","b46fdf5e":"# PART3","6e710dbf":"A word of caution : Always use only those techniques that are relevant to our application. Punctuations, numbers, emojis, if they're important to our usecase, we should keep them.","efe601bb":"***SpaCy has done a very good job!!*** <br>\nStopwords, words that occur so commonly. It's often a good idea to ignore them.","bffadaae":".vocabulary_ = all words appeared in an entire corpus of documents. If we have 15 unique word in our corpus, our vector will have 15 dimensions. Each dimension corresponds to the count of the token of that dimension.<br>\n\n*ith member* = *count of ith index's word* <br>\nFor example, index 5 has a value of 2 meaning that \"Text_1 has 5th word which is 'lion' 2 times.\"","f8ce93f4":"## - Movie recommender","1f28c160":"Text cleanning techniques\nIt's common to remove non-alphabetic tokens, words that occur so commonly that they're not very useful for analysis <br>\nThose tokens such as necessary whitespaces, escape sequences, punctuations, special char.(numbers, emojis, ...), and stopwords.","18f685d1":"See [SpaCy annotation](https:\/\/spacy.io\/api\/annotation) for full annotation doc.","a1ea18cf":"## - Part-of-speech tagging","989cdb99":"**Shortcomings**\n- Curse of dimensionality : ngram_range=(1,4) is building 1,2,3,4 gram -> even more increases number of dimensions. Usually, n>3 are rare. Keep n small.","5009323f":"Application : Keyword search, Question answering (what, where, ..), News article classification, Customer service.","f38748ae":"- Text preprocessing : lowercasing, tokenization, delete non-alphabets &stopwords, ... \n- Vectorization : count-vectorize(bag-of-word model), tfidf-vectorize, ...\n- Basic features\n- Part-of-speech tagging\n- Named Entity recognition\n___","7c2ea85f":"A word of caution <br>\n- Not perfect!!\n- This is a result of pre-trained model. So, the performance totally depends on training and test data.\n- Pre-trained model is not very good in nuanced cases.\n- And language specific.\n\nWe may train our models with specialized data for nuanced cases.","382a26a9":"## - Text Cleaning\nString -> list of lemmas. Now we're in a good position to perform text cleaning <br>","9a23d56b":"Attributes\n- stop_words_ : *set* return Terms that were ignored because they either:\n    - occurred in too many documents (max_df)\n    - occurred in too few documents (min_df)\n    - were cut off by feature selection (max_features).","6f1b76e7":"# Note:\n- This is my lecture note from the course [NLP feature engineering in DataCamp](https:\/\/learn.datacamp.com\/courses\/feature-engineering-for-nlp-in-python)\n- If you found this notebook useful, please kindly upvote.\n- I also have created some DataCamp course notebooks. please check it out!\n___","a6e2d1be":"We might need to know how 2 text are alphabetically similar. Cosine similarity is a metric to measure alphabetical similarity of 2 \ntext. <br>\n\nCosine similarity of vector $A$ and $B$ is : $cos(\\theta) = \\frac{A\\,\\cdot\\,B}{|A||B|}$ <br>\nThe value is between -1 and 1. In NLP, value between 0 and 1 because our weights in vector usually be positive.","db0a86c1":"We definitely have to preprocess our texts before turning them into vectors. We can preprocess right in the CountVectorizer constructor -> [see the doc](https:\/\/scikit-learn.org\/0.23\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) <br>\nPreprocessing our text not only to make out data cleaned, it's also reduce the dimension of our data.","1c123acd":"# PART4","a90183b9":"-> A word of caution!! , isalpha() will return False for some useful words which are not all-alphabetic such as Abbreviations, some proper nouns.<br>\n= write our custom function (using regex) for the nuanced cases. (to ensure we're not inadvertently removing useful words)","b2e383e3":"CountVectorizer can't perform certain steos such as lemmatization, stemming.","9901b39b":"Bag-of-word = Extract tokens from text + Compute frequency of each token + Construct 1vector\/1sentence out of these frequencies and vocabulary of an entire corpus.","7f6f4365":"## - Basic feature extraction","c2c67c88":"Word with **higher tf-idf weight** = *the more important the word is in characterizing the document.* = *reletively exclusive to that particular docuument* = *occur commonly in that document but rarely be found in others.*","8279ff94":"Bag-of-word has some issues.","3169f102":"## - Building Tfidf vectors","12c2710e":"## - Cosine similarity","844ffe0d":"Weights of each dimension in bag-of-word(Count vector) and N-gram are dependent on the frequency of the word corresponding to the dimension. Document contains the word \"human\" 5 times. Dimension corresponding to \"human\" has weight 5.","6298c15d":"## - Lemmatization\nreducing, reduced, reduces -> reduce <br>\nis, am, are -> be <br>\nn't -> not <br>\n've -> have","17de943c":"## - Tokenization","cba3e105":"Below is a movie recommender function that recommends k movies based on the similarity of their overview. The output of this fuction is a list of tuple where the first element of tuple is a similarity score between the queried movie and the second element of tuple.","85c1b519":"## - N-gram","8b53062a":".lemma_ := base form of each word <br>\n-PRON_ is a standard behavior. (lemmatized version of text isn't very human-readable but it's in a much more convenient format for machine to process.) \n___","9175fc09":"### Term frequency-inverse document frequency","dc82c11c":"## - Named Entity Recognition","c91ffb85":"$$w_{i,j} = t_{i,j}log(\\frac{N}{d_i})$$\n\nweight of term $i$ in document $j$ : $w_{i,j}$ <br>\nfrequency of term $i$ in document $j$ : $t_{i,j}$ <br>\nnumber of documents : $N$ <br>\nnumber of documents containing term $i$ : $d_i$","33c9a4d3":"**Motivation** <br>\nSome words occur so commonly across all documents. As a result, the vector representations get more characterized by these dimensions. <br>\n\nCorpus of documents about universe\n- \"universe\" occurs so commonly.\n- \"Jupiter\" rarely occurs in some documents.\n- bag-of-word model will gives much more weight to \"universe\".\n\nWe should give \"Jupiter\" more weight on account of exclusivity. \"universe\" is, in some sense, like a very low variance feature in normal tabular data. But we should not drop it, giving it less interest(weight) is enough.","24fa2bd9":"\"The movie was good and not boring\" -> Positive <br>\n\"The movie was not good and boring\" -> Negative <br>\n\nBut both have the exact same vector when using bag-of-word model.","e1f64311":"## - Redability test\n\nDetermine reability of an English passage. Scale ranging from primary school up to college graduate level.<br>\nUsed in fake news and spam detection<br>\n\n- Flesch reading ease : oldest &most widely used test\n- Gunning fog index\n- SMOG\n- Dale-Chall score\n\nUsing textatstic.Textatistic\n___","eb223782":"## - Bag-of-words model\n\"Vectorization\" = techniques converting text into vector. Bow is one of those techniques. <br>\n\n**Data format for any ML** <br>\n- Data must be in tabular form\n- Data fed to model must be numerical","e7c70955":"Number of characters, Number of words, Average word length, <br>\nSpecial features auch as Number of hashtags, Number of mentions <br>\nothers such as Number of sentences, Number of paragraph, Numeric quantity.","988de831":"## - Word embedding"}}