{"cell_type":{"cc375ff9":"code","146fc660":"code","e3fb157b":"code","31657043":"code","8216cd6e":"code","a168d21e":"code","e883805c":"code","35674db8":"code","4d653efa":"code","ba2f1c23":"code","db8641db":"code","d4d135cb":"code","c79459ed":"code","9256dced":"code","e259d7a4":"code","754cbd5d":"code","b1e3a51d":"code","8af95eca":"code","2a25c474":"code","d17da37b":"code","10d6b30f":"code","3dcd9214":"code","8aa25f7d":"code","317cb6df":"code","ef798747":"code","fe69b846":"code","97ebf8cf":"code","0ed7db62":"code","d27469a9":"code","3df5bfd6":"code","4a03d042":"code","59dcd8c8":"code","57a202b1":"code","905a4677":"code","e228dd8b":"code","e0258775":"code","02d9342b":"code","c40e074b":"code","868f4dfc":"code","2599ea0e":"code","a9a46303":"markdown","c29be98d":"markdown","b5125982":"markdown","6c976b6c":"markdown","3ff907ff":"markdown","97584449":"markdown","43afc8bc":"markdown","7f4b9212":"markdown","2a9d1ac5":"markdown","c56d3f0a":"markdown","d711d33d":"markdown","d9efbc30":"markdown","487c9c3c":"markdown","78206549":"markdown","22ecdef0":"markdown","36183e9b":"markdown","f75ba4a5":"markdown","31e92d09":"markdown","33a42892":"markdown","ae420fd3":"markdown","47f2dfb9":"markdown","8c04869a":"markdown","098c2f71":"markdown","19b67360":"markdown","f650fdac":"markdown","68de8abf":"markdown"},"source":{"cc375ff9":"#import libraries\nimport os\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\n#import visualization libs\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#import preprocessing libraries\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV\n\n#import dimensions related libraries\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.pipeline import Pipeline\n\n#import algos\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\nimport xgboost as xgb\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#import error metrics\nfrom sklearn.metrics import plot_confusion_matrix, plot_roc_curve\n","146fc660":"#get the path of our dataset\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e3fb157b":"#read and analyze the data\ndata = pd.read_csv('\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n\nprint(\"Shape\",data.shape)\n\nprint(\"Info\", data.info())\n\ndata.head()","31657043":"#check the null values\n((data.isnull().sum()*100)\/len(data)).sort_values(ascending=False)","8216cd6e":"#check the unique values\ndata.nunique()","a168d21e":"#Lets do some EDA\nchurn = data[data.Churn == 'Yes']\nnon_churn = data[data.Churn == 'No']","e883805c":"#Churn Vs Gender\n\nsns.set_palette(sns.color_palette('muted', 7))\nplt.figure(figsize=(8,4))\nsns.countplot('gender', data=data, hue='Churn')","35674db8":"#Churn Vs SeniorCitizen\n\nsns.set_palette(sns.color_palette('hls', 7))\nplt.figure(figsize=(8,4))\nsns.countplot('SeniorCitizen', data=data, hue='Churn')","4d653efa":"#tenure relation with churn\n\nsns.set_palette(sns.color_palette('Blues_d', 2))\nsns.boxplot(x='Churn' ,y='tenure', data=data)\n\n#seems most of the people who left service had less tenure","ba2f1c23":"sns.set_palette(sns.color_palette('RdBu', 2))\n\nsns.boxplot(x='Churn',y='MonthlyCharges',data=data)","db8641db":"#First convert the object column into type float\ndata['TotalCharges'] = data['TotalCharges'].replace('[^\\d.]', '', regex = True).replace('',np.nan).astype(float)\nsns.boxplot(x='Churn',y='TotalCharges',data=data)","d4d135cb":"sns.kdeplot(data[data['Churn'] == 'No']['TotalCharges'], label= 'Churn: No')\nsns.kdeplot(data[data['Churn'] == 'Yes']['TotalCharges'], label= 'Churn: Yes')","c79459ed":"sns.kdeplot(data[data['Churn'] == 'No']['tenure'], label= 'Churn: No')\nsns.kdeplot(data[data['Churn'] == 'Yes']['tenure'], label= 'Churn: Yes')","9256dced":"#now check the numeric columns and check the distribution of data\nt= (data.dtypes != 'object')\n#now convert the data into list\nnum_cols = list(t[t].index)\nnum_cols","e259d7a4":"#Balance the data for numeric columns after EDA\n\nsns.set_palette(sns.color_palette('muted', 1))\nsns.distplot(data['tenure'], bins=20, hist=True,label='tenure')","754cbd5d":"sns.distplot(data['MonthlyCharges'], bins=20, hist=True,label='MonthlyCharges')","b1e3a51d":"sns.distplot(data['TotalCharges'], bins=10, hist=True,label='TotalCharges')","8af95eca":"sns.set_palette(sns.color_palette('hls', 4))\nsns.catplot(y=\"Churn\", x=\"MonthlyCharges\", row=\"PaymentMethod\", kind=\"box\", data=data, height=2, aspect=4, orient='h')","2a25c474":"sns.boxplot(x=data['MonthlyCharges'])\n","d17da37b":"sns.boxplot(x=data['TotalCharges'])","10d6b30f":"#Tenure V\/s Monthly Charges\n\nsns.set(style=\"ticks\")\nx = data['tenure']\ny = data['MonthlyCharges']\nsns.jointplot(x, y, kind=\"hex\", color=\"#4CB391\")","3dcd9214":"# Show the joint distribution using kernel density estimation\n\ng = sns.jointplot(x, y, kind=\"kde\",\n                  height=4, space=0)","8aa25f7d":"sns.set(style=\"darkgrid\")\ng = sns.jointplot(\"TotalCharges\", \"tenure\", \n                   data=data, kind=\"reg\",\n                   color=\"m\", height=6)","317cb6df":"#lets calculate the IQR\nQ1 = data['TotalCharges'].quantile(0.25)\nQ3 = data['TotalCharges'].quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","ef798747":"print((data['TotalCharges'] < (Q1 - 1.5 * IQR)) |(data['TotalCharges'] > (Q3 + 1.5 * IQR)))","fe69b846":"totalcharge_out = ((data['TotalCharges'] < (Q1 - 1.5 * IQR)) |(data['TotalCharges'] > (Q3 + 1.5 * IQR)))\nTotalCharges = data['TotalCharges'][~totalcharge_out]\nTotalCharges.shape","97ebf8cf":"#Lets check with zscore\nz = np.abs(stats.zscore(data['TotalCharges']))\nprint(z)","0ed7db62":"threshold = 3\nprint(np.where(z > 3))","d27469a9":"#Create a heatmap\nplt.figure(figsize=(15, 10))\ndata.drop(['customerID'], axis=1, inplace=True)\n# We will get the numeric representation of the object columns to numeric ones by applying pd.factorize.\ncorr = data.apply(lambda x: pd.factorize(x)[0]).corr()\nax = sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, \n                 linewidths=.2, cmap=\"YlGnBu\", annot=True)\n\n\n\n#lets first use the pearson coefficient to check the correlation\nprint(data.apply(lambda x : pd.factorize(x)[0]).corr(method='pearson', min_periods=1))  \n\n#check the correlation between columns which is greater than 0.95\ncorr = data.corr()\ncolumns = np.full((corr.shape[0],), True, dtype=bool)\nfor i in range(corr.shape[0]):\n    for j in range(i+1, corr.shape[0]):\n        if corr.iloc[i,j] >= 0.95:\n            if columns[j]:\n                columns[j] = False\n                \nselected_columns = corr.columns[columns]\ncorr = corr[selected_columns]\ncorr.head()","3df5bfd6":"#now check the object columns\nob= (data.dtypes == 'object')\n#now convert the data into list\nobj_cols = list(ob[ob].index)\nprint(\"List of Object columns\",obj_cols)\nprint()\nprint(\"Number of Unique Values\\n\",data.nunique())","4a03d042":"#lets split the data into train and validation set\ndata_x, data_y = data.drop(columns = 'Churn'), data['Churn']\nskf = StratifiedKFold(n_splits=20, shuffle=True, random_state= 142)\nfor train_index, val_index in skf.split(data_x,data_y):\n    train_x, val_x = data_x.iloc[train_index], data_x.iloc[val_index]\n    train_y, val_y = data_y.iloc[train_index], data_y.iloc[val_index]\n    \ntrain_x.shape, val_x.shape    ","59dcd8c8":"train_x = pd.get_dummies(train_x)\nprint(train_x.head())\nval_x = pd.get_dummies(val_x)\nval_x.head()","57a202b1":"#Lets first start with Logistic Regression\nlogReg = LogisticRegression()\nsc = StandardScaler()\npca = PCA()\n\n# Create a pipeline of three steps. First, standardize the data.\n# Second, tranform the data with PCA.\n# Third, train a logistic regression on the data.\npipe = Pipeline(steps=[('sc', sc),\n                       ('pca', pca),\n                       ('logistic', logReg)])\n\n# Create Parameter Space\n# Create a list of a sequence of integers from 1 to 30 (the number of features in X + 1)\nn_components = list(range(1,train_x.shape[1]+1,1))\n# Create a list of values of the regularization parameter\nC = np.logspace(-4, 4, 50)\n# Create a list of options for the regularization penalty\npenalty = ['l2']\n# Create a dictionary of all the parameter options \n# Note has you can access the parameters of steps of a pipeline by using '__\u2019\nparameters = dict(pca__n_components=n_components,\n                  logistic__C=C,\n                  logistic__penalty=penalty)\n\n# Conduct Parameter Optmization With Pipeline\n# Create a grid search object\nclf = GridSearchCV(pipe, parameters)","905a4677":"train_x = train_x.reset_index()\nval_x = val_x.reset_index()\n\n#np.nan_to_num(train_x)\ntrain_x = train_x.fillna(train_x.mean())\nval_x = val_x.fillna(val_x.mean())\nprint(np.all(np.isfinite(train_x)))\nprint(np.any(np.isnan(train_x)))\nprint(train_x.isnull().sum())\n","e228dd8b":"clf.fit(train_x,train_y)\n\n# View The Best Parameters\nprint('Best Penalty:', clf.best_estimator_.get_params()['logistic__penalty'])\nprint('Best C:', clf.best_estimator_.get_params()['logistic__C'])\nprint('Best Number Of Components:', clf.best_estimator_.get_params()['pca__n_components'])\nprint();print(clf.best_estimator_.get_params()['logistic'])\n\nclf_pred = clf.best_estimator_.predict(val_x)\n\n# Use Cross Validation To Evaluate Model\nCV_Result = cross_val_score(clf, val_x, val_y, cv=4, n_jobs=-1)\nprint(CV_Result)\nprint(CV_Result.mean())\nprint(CV_Result.std())","e0258775":"# Check the accuracy of the model\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix( val_y, clf_pred)\nprint(cm)\n\n# Lets check classification report also \nfrom sklearn.metrics import classification_report\ncr = classification_report( val_y, clf_pred)\nprint(cr)","02d9342b":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\nle = LabelEncoder()\nle_val = le.fit(val_y)\nle_val = le.transform(val_y)\nclf_pred = le.transform(clf_pred)\n\n# Now we will check ROC curve\ndef plot_roc_curve(fpr, tpr):\n    plt.plot(fpr, tpr, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()\n    \nprint('AUC: %.2f'% roc_auc_score(le_val, clf_pred))\n\n# plot the curve\nfpr, tpr, thresholds = roc_curve(le_val, clf_pred)\nplot_roc_curve(fpr, tpr)","c40e074b":"from sklearn.ensemble import RandomForestClassifier\n\nrfc_model=RandomForestClassifier(n_estimators=200, criterion='entropy', random_state=42)\n\nrfc_model.fit(train_x,train_y)\n\nrfc_pred = rfc_model.predict(val_x)","868f4dfc":"rfc_pred = le.transform(rfc_pred)\n\nprint('AUC: %.2f'% roc_auc_score(le_val, rfc_pred))\n\n# plot the curve\nfpr, tpr, thresholds = roc_curve(le_val, rfc_pred)\nplot_roc_curve(fpr, tpr)","2599ea0e":"#Import Gaussian Naive Bayes model\nfrom sklearn.naive_bayes import GaussianNB\n\ngnb_model = GaussianNB()\n\ngnb_model.fit(train_x,train_y)\n\ngnb_pred = gnb_model.predict(val_x)\n\ngnb_pred = le.transform(gnb_pred)\n\n# Now check the accuracy of the model\nprint('AUC: %.2f'% roc_auc_score(le_val, gnb_pred))\n\n# plot the curve\nfpr, tpr, thresholds = roc_curve(le_val, gnb_pred)\nplot_roc_curve(fpr, tpr)","a9a46303":"# Topics Covered: \n\n1. EDA\n2. Outliers Detection\n3. Checking Correlation\n4. PCA\n5. Pipeline\n6. Logistic Algorithm\n7. Random Forest Classifier\n8. Naive Bayes","c29be98d":"From the above graph, we can see that the people who left had higher monthly charges.","b5125982":"As we can see that we have less unique values for object columns. Lets use get_dummies to convert the values","6c976b6c":"Before starting on algorithms, lets understand some theory about these topics:\n\n**PCA:** Principal Component Analysis is used basically for dimensionality reduction. The idea of PCA is simple \u2014 reduce the number of variables of a data set, while preserving as much information as possible.\nSteps for PCA:\n* Standardization: Standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. Otherwise the variables with larger ranges will dominate the ones with smaller range.\n* Covariance matrix calculation: The covariance matrix is a p \u00d7 p symmetric matrix (where p is the number of dimensions) which tells us how the variables are correlated to each other. The covariance of a variable with itself is its variance (Cov(a,a)=Var(a)) and also the covariance is commutative (Cov(a,b)=Cov(b,a)). \n* Compute the Eigenvectors and Eigenvalues of the Covriance Matrix to identify the PRINCIPAL COMPONENTS: The eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component.\n* Feature Vector: Choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector.\n* Recast the data along the PC axes: The aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis).\n\n![image.png](attachment:image.png)\n\n> Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible.\n\n> An important thing to realize here is that, the principal components are less interpretable and don\u2019t have any real meaning since they are constructed as linear combinations of the initial variables.\n\n![](https:\/\/builtin.com\/sites\/default\/files\/inline-images\/Principal%20Component%20Analysis%20second%20principal.gif)","3ff907ff":"# 1. EDA","97584449":"Total charges were less as compared to overall population for the people who left but there are multiple outliers.","43afc8bc":"> **First Approach: Using IQR** -- The IQR is calculated as the difference between the 75th and the 25th percentiles of the data.","7f4b9212":"![](https:\/\/miro.medium.com\/max\/1050\/1*kU_mPtwcjfEUSkSflUikxw.jpeg)","2a9d1ac5":"We can see that the AUC is around 0.7 which is not a great value for our accuracy. \n\nSo let's try with some other algorithm. We will try with Naive Bayes","c56d3f0a":"Logistic Regression: It is linear model which is used for classification problems. It makes use of the sigmoid function which returns the value between 0 and 1. Based on the threshold, the class is determined for that dependent variable. ","d711d33d":"# Naive Bayes\n\nNaive Bayes is a statistical classification technique based on Bayes Theorem. Naive Bayes classifiers have high accuracy and speed on large datasets.\n\nNaive Bayes classifier assumes that the effect of a particular feature in a class is independent of other features.Even if these features are interdependent, these features are still considered independently. This assumption simplifies computation, and that's why it is considered as naive. This assumption is called class conditional independence.\n\n![](https:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1543836882\/image_3_ijznzs.png)\n\n* P(h): the probability of hypothesis h being true (regardless of the data). This is known as the prior probability of h.\n* P(D): the probability of the data (regardless of the hypothesis). This is known as the prior probability.\n* P(h|D): the probability of hypothesis h given the data D. This is known as posterior probability.\n* P(D|h): the probability of data d given that the hypothesis h was true. This is known as posterior probability.\n\n**Working of Naive Bayes**:\n\n* Step 1: Calculate the prior probability for given class labels\n* Step 2: Find Likelihood(conditional) probability with each attribute for each class\n* Step 3: Multiply same class conditional probability. \n* Step 4: Multiply prior probability with Step 3 probability.\n* Step 5: See which class has a higher probability, higher probability class belongs to given input set step.","d9efbc30":"> **StratifiedKFold**: StratifiedKFold is a variation of KFold and ensure each fold is a good representative of the whole. \n\n* First, StratifiedKFold shuffles your data, after that splits the data into n_splits parts. Now, it will use each part as a test set. Note that it only and always shuffles data one time before splitting.\n* With shuffle = True, the data is shuffled by your random_state. Otherwise, the data is shuffled by np.random (as default).","487c9c3c":"# 4. PCA and Pipeline","78206549":"# Few EDA Conclusions:\n\n1. Gender is not a clear indicator of Churn. \n2. Senior Citizens have a higher Churn rate as compared to other. \n3. Most of the people who left, had left in the initial 24 months with a mean of around 11 months.\n4. Monthly charge is a main reason of Churn. \n5. People having extremely high monthly charges have also left the services.\n6. People having online payment mode are more likely to Churn.","22ecdef0":"Pipeline: There are a lot of steps which we need to perform for the ML model. By using Pipeline, we will just tied them together and produce the results. ","36183e9b":"So no outliers are there for TotalCharges","f75ba4a5":"**Second Approach: Using ZScore**: Technically it's a measure of how many standard deviations below or above the population mean a raw score is, which in return gives you an idea of how far from the mean a data point is.","31e92d09":"# Random Forest Classifier\n\nRandom forests creates decision trees on randomly selected data samples, gets prediction from each tree and selects the best solution by means of voting.\n\nIt technically is an ensemble method (based on the divide-and-conquer approach) of decision trees generated on a randomly split dataset. This collection of decision tree classifiers is also known as the forest. The individual decision trees are generated using an attribute selection indicator such as information gain, gain ratio, and Gini index for each attribute. Each tree depends on an independent random sample. In a classification problem, each tree votes and the most popular class is chosen as the final result.\n\n**Working**\n\n1. Select random samples from a given dataset.\n2. Construct a decision tree for each sample and get a prediction result from each decision tree.\n3. Perform a vote for each predicted result.\n4. Select the prediction result with the most votes as the final prediction.\n\n![image.png](attachment:image.png)\n\nAdvantages: \n1. It does not suffer from the overfitting problem. The main reason is that it takes the average of all the predictions, which cancels out the biases.\n2. Random forests can also handle missing values. There are two ways to handle these: using median values to replace continuous variables, and computing the proximity-weighted average of missing values.\n3. Random forests is considered as a highly accurate and robust method because of the number of decision trees participating in the process.\n\n**Finding important features:**\n\nRandom forest uses gini importance or mean decrease in impurity (MDI) to calculate the importance of each feature. Gini importance is also known as the total decrease in node impurity. This is how much the model fit or accuracy decreases when you drop a variable. The larger the decrease, the more significant the variable is. Here, the mean decrease is a significant parameter for variable selection. The Gini index can describe the overall explanatory power of the variables.","33a42892":"We will implement the confusion matrix but before that, lets understand some theory about this: \n\n**Confusion Matrix** : A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known. There are 4 possible outcomes:\n\n* True Positives(TP) : The cases in which we predicted YES and the actual output was also YES.\n* True Negatives(TN) : The cases in which we predicted NO and the actual output was also NO.\n* False Positives(FP) : The cases in which we predicted YES whereas the actual output was NO.\n* False Negatives(FN) : The cases in which we predicted NO whereas the actual output was YES.\n\n![](https:\/\/skappal7.files.wordpress.com\/2018\/08\/confusion-matrix.jpg?w=748)\n\n**Accuracy Score** : Accuracy or classification accuracy tells the number of correct predictions made by the model. It is the ratio of the number of correct predictions to the total number of input samples. For binary classification the accuracy can be defined as:\n\nAccuracy = (TP + TN) \/ (TP + TN + FP + FN)","ae420fd3":"![](https:\/\/miro.medium.com\/max\/623\/1*uR09zTlPgIj5PvMYJZScVg.png)","47f2dfb9":"**Churn rate is the percentage of subscribers to a service that discontinue their subscription to that service in a given time period. With growing pressure from competition and government mandates improving retention rates of profitable customers has become an increasingly urgent to telecom service providers.**","8c04869a":"**Lets check for the outliers, there are many methods but we will go for IQR and Z-Scores and will see the difference**","098c2f71":"So we can say that on this data Naive Bayes is performing better than other 2 algorithms. \n\n* Reference: https:\/\/www.kaggle.com\/jsaguiar\/exploratory-analysis-with-seaborn\n* PCA : https:\/\/builtin.com\/data-science\/step-step-explanation-principal-component-analysis\n* Naive Bayes: https:\/\/www.datacamp.com\/community\/tutorials\/naive-bayes-scikit-learn","19b67360":"**Convert the object columns into numerical** by applying encoding.","f650fdac":"# 2. Outliers Detection","68de8abf":"# 3. Checking Correlation"}}