{"cell_type":{"61f2150e":"code","b8098e12":"code","f5efd783":"code","c4f044d6":"code","5d2ba749":"code","650b0532":"code","14b05e80":"code","ad606914":"code","34cf1bb8":"code","49b30440":"code","e138d9fb":"code","4e5eb522":"code","e4fb5a7a":"code","0f3167e8":"code","77748c25":"code","c93d0940":"code","0512ef7f":"code","0a2eb2b5":"code","862b4bc6":"code","0b66ae2d":"code","b80685f9":"code","fa145fe2":"code","e897b302":"code","39199460":"code","d6568df2":"code","1c6d68b2":"code","de74106f":"code","77efe5fb":"code","550e8589":"code","d860860a":"markdown","6c975f35":"markdown","2f5ffae5":"markdown","09480955":"markdown","3479944d":"markdown","b46a5b23":"markdown","e86a3c8f":"markdown","6caa8fa4":"markdown","da24ef3b":"markdown","7b0ac345":"markdown","78ab3b77":"markdown"},"source":{"61f2150e":"# Importing necessary libraries\n\nimport sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n%matplotlib inline\n\nfrom scipy.io import loadmat\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split","b8098e12":"# Loading MNIST data\n\nmnist = loadmat(\"..\/input\/mnist-original\/mnist-original.mat\")\nmnist_data = mnist[\"data\"].T\nmnist_label = mnist[\"label\"][0]","f5efd783":"mnist_data","c4f044d6":"image_size_px = int(np.sqrt(mnist_data.shape[1]))\nprint(\"The images size is (\",image_size_px,'x',image_size_px,')')","5d2ba749":"# Viewing a random MNIST image\n\ndef mnist_random_example():\n    idx = np.random.randint(70000)\n    exp = mnist_data[idx].reshape(image_size_px,image_size_px)\n    print(\"The number in the image below is:\", mnist_label[idx])\n    plt.imshow(exp)","650b0532":"mnist_random_example()","14b05e80":"# creating a normalization function\n\ndef normalize(data):\n    mean = np.mean(data, axis=1, keepdims=True)\n    std = np.std(data, axis=1, keepdims=True)\n    data_normalized = (data - mean)\/std\n    return data_normalized","ad606914":"# Normalizing the data\n\nmnist_data_normalized = normalize(mnist_data)","34cf1bb8":"# Splitting the data into Train and Test datasets\n\nX_train, X_test, Y_train, Y_test = train_test_split(mnist_data_normalized, mnist_label, test_size=0.20, random_state=42)\nY_train = Y_train.reshape(Y_train.shape[0],1)\nY_test = Y_test.reshape(Y_test.shape[0],1)\nprint(\"The shape of the training set feature matrix is:\", X_train.shape)\nprint(\"The shape of the training label vector is:\", Y_train.shape)\nprint(\"The shape of the test set feature matrix is:\", X_test.shape)\nprint(\"The shape of the test label vector is:\", Y_test.shape)","49b30440":"# Creating new training label vectors for each digit for the one-vs-all methode\n\nY_train_0=(Y_train==0).astype(int)\nY_train_1=(Y_train==1).astype(int)\nY_train_2=(Y_train==2).astype(int)\nY_train_3=(Y_train==3).astype(int)\nY_train_4=(Y_train==4).astype(int)\nY_train_5=(Y_train==5).astype(int)\nY_train_6=(Y_train==6).astype(int)\nY_train_7=(Y_train==7).astype(int)\nY_train_8=(Y_train==8).astype(int)\nY_train_9=(Y_train==9).astype(int)","e138d9fb":"# Creating new testing label vectors for each digit for the one-vs-all methode\n\nY_test_0=(Y_test==0).astype(int)\nY_test_1=(Y_test==1).astype(int)\nY_test_2=(Y_test==2).astype(int)\nY_test_3=(Y_test==3).astype(int)\nY_test_4=(Y_test==4).astype(int)\nY_test_5=(Y_test==5).astype(int)\nY_test_6=(Y_test==6).astype(int)\nY_test_7=(Y_test==7).astype(int)\nY_test_8=(Y_test==8).astype(int)\nY_test_9=(Y_test==9).astype(int)","4e5eb522":"# Creating initilizer function to initialize weights and bias\n\ndef initializer(nbr_features):\n    W = np.zeros((nbr_features,1))\n    B = 0\n    return W, B","e4fb5a7a":"# Creating a Sigmoid function\n\ndef sigmoid(x):\n    s = 1\/(1+np.exp(-x))\n    return s","0f3167e8":"# Creating the Forward and backward propagation function which calculates J, dW, and dB\n\ndef ForwardBackProp(X, Y, W, B):\n    m = X.shape[0] \n    dw = np.zeros((W.shape[0],1))\n    dB = 0\n    \n    Z = np.dot(X,W)+B\n    Yhat = sigmoid(Z) \n    J = -(1\/m)*(np.dot(Y.T,np.log(Yhat))+np.dot((1-Y).T,np.log(1-Yhat)))\n    dW = (1\/m)*np.dot(X.T,(Yhat-Y))\n    dB = (1\/m)*np.sum(Yhat-Y)\n    return J, dW, dB\n","77748c25":"# Creating a prediction function which predicts the labels of the input images\n\ndef predict(X,W,B):\n    Yhat_prob = sigmoid(np.dot(X,W)+B)\n    Yhat = np.round(Yhat_prob).astype(int)\n    return Yhat, Yhat_prob","c93d0940":"# Creating the gradient descent optimizer function\n\ndef gradient_descent(X, Y, W, B, alpha, max_iter):\n    i=0\n    RMSE = 1\n    cost_history=[]\n    \n    # setup toolbar\n    toolbar_width = 20\n    sys.stdout.write(\"[%s]\" % (\"\" * toolbar_width))\n    sys.stdout.flush()\n    sys.stdout.write(\"\\b\" * (toolbar_width+1)) # return to start of line, after '['\n    \n    while (i<max_iter)&(RMSE>10e-6):\n        J, dW, dB = ForwardBackProp(X,Y,W,B)\n        W = W - alpha*dW\n        B = B - alpha*dB\n        cost_history.append(J)\n        Yhat, _ = predict(X,W,B)\n        RMSE = np.sqrt(np.mean(Yhat-Y)**2)\n        i+=1\n        if i%50==0:\n            sys.stdout.write(\"=\")\n            sys.stdout.flush()\n    \n    sys.stdout.write(\"]\\n\") # this ends the progress bar\n    return cost_history, W, B, i","0512ef7f":"# Creating the model function which trains a model and return its parameters. \n\ndef LogRegModel(X_train, X_test, Y_train, Y_test, alpha, max_iter):\n    \n    nbr_features = X_train.shape[1]\n    W, B = initializer(nbr_features)\n    cost_history, W, B, i = gradient_descent(X_train, Y_train, W, B, alpha, max_iter)\n    Yhat_train, _ = predict(X_train, W, B)\n    Yhat, _ = predict(X_test, W, B)\n    \n    train_accuracy = accuracy_score(Y_train, Yhat_train)\n    test_accuracy = accuracy_score(Y_test, Yhat)\n    conf_matrix = confusion_matrix(Y_test, Yhat, normalize='true')\n    \n    model = {\"weights\": W,\n            \"bias\": B,\n            \"train_accuracy\": train_accuracy,\n            \"test_accuracy\": test_accuracy,\n            \"confusion_matrix\": conf_matrix,\n            \"cost_history\": cost_history}\n    return model","0a2eb2b5":"# Testing the model function by training a classifier for the digit '0'\n\nprint('Progress bar: 1 step each 50 iteration')\nmodel_0 = LogRegModel(X_train, X_test, Y_train_0, Y_test_0, alpha=0.01, max_iter=1000)\nprint('Training completed!')","862b4bc6":"# Viewing the cost evolution over time of the trained model\n\ncost = np.concatenate(model_0['cost_history']).ravel().tolist()\nplt.plot(list(range(len(cost))),cost)\nplt.title('Evolution of the cost by iteration')\nplt.xlabel('Iteration')\nplt.ylabel('Cost');","0b66ae2d":"# Checking the accuracy of the model\n\nprint('The training accuracy of the model',model_0['train_accuracy'])\nprint('The test accuracy of the model',model_0['test_accuracy'])","b80685f9":"# Creating  afunction that shows a random image with the true and predicted label\n\ndef check_random_pred(datum,Y,model,label):\n    W = model['weights']\n    B = model['bias']\n    Yhat, _ = predict(datum,W,B)\n    if Yhat == 1:\n        pred_label = label\n    else:\n        pred_label = 'Not '+ label\n    if Y == 1:\n        true_label = label\n    else:\n        true_label = 'Not '+ label\n    print(\"The number in the image below is:\", true_label, ' and predicted as:', pred_label)\n    image = datum.reshape(image_size_px,image_size_px)\n    plt.imshow(image)","fa145fe2":"# Checking some random image predictions\n\nidx = np.random.randint(X_test.shape[0])\ndatum = X_test[idx]\nY = Y_test_0[idx]\ncheck_random_pred(datum,Y,model_0, '0')","e897b302":"# Creating and training a model for each digit\n\nmodels_list=[]\nmodels_name_list=['model_0','model_1','model_2','model_3','model_4','model_5','model_6',\n                 'model_7','model_8','model_9']\nY_train_list=[Y_train_0,Y_train_1,Y_train_2,Y_train_3,Y_train_4,Y_train_5,Y_train_6,\n             Y_train_7,Y_train_8,Y_train_9]\nY_test_list = [Y_test_0,Y_test_1,Y_test_2,Y_test_3,Y_test_4,Y_test_5,Y_test_6,Y_test_7,\n              Y_test_8,Y_test_9]\nprint('Training of a classifier for each digit:')\nfor i in range(10):\n    print('Training of the model: ', models_name_list[i],', to recognize the digit: ',i)\n    print('Training progress bar: 1 step each 50 iteration')\n    model = LogRegModel(X_train, X_test, Y_train_list[i], Y_test_list[i], alpha=0.01, max_iter=1000)\n    print('Training completed!')\n    print('Accuracy:', model['test_accuracy'])\n    print('-'*60)\n    models_list.append(model)","39199460":"# Calculatin gthe average accuracy of all the models\n\naccuracy_list=[]\nfor i in range(len(models_list)):\n    accuracy_list.append(models_list[i]['test_accuracy'])\nove_vs_all_accuracy=np.mean(accuracy_list)\nprint('The accuracy of the Onve-Vs-All model is:', ove_vs_all_accuracy)","d6568df2":"# Creating a one-vs-all function that uses all the trained models to predict the label of a random image\n\ndef one_vs_all(data, models_list):\n    pred_matrix = np.zeros((data.shape[0],10))\n    for i in range(len(models_list)):\n        W = models_list[i]['weights']\n        B = models_list[i]['bias']\n        Yhat, Yhat_prob = predict(data,W,B)\n        pred_matrix[:,i] = Yhat_prob.T\n    max_prob_vec = np.amax(pred_matrix, axis=1, keepdims=True)\n    pred_matrix_max_prob = (pred_matrix == max_prob_vec).astype(int)\n    labels=[]\n    for j in range(pred_matrix_max_prob.shape[0]):\n        idx = np.where(pred_matrix_max_prob[j,:]==1)\n        labels.append(idx)\n    labels = np.vstack(labels).flatten()\n    return labels","1c6d68b2":"pred_label = one_vs_all(X_test, models_list)\nconf_matrix = confusion_matrix(Y_test, pred_label)\n\ndef plot_cm(mat,y_ture,ax,case):\n    if case == 0:\n        df_cm = pd.DataFrame(mat, columns=np.unique(y_ture), index = np.unique(y_ture))\n        df_cm.index.name = 'True Label'\n        df_cm.columns.name = 'Predicted Label'\n        sb.heatmap(df_cm, cmap=\"Blues\", cbar=False, annot=True,annot_kws={\"size\": 10}, ax=ax)\n        plt.yticks(fontsize=10)\n        plt.xticks(fontsize=10)\n    else:\n        l_lab=['Goalkeeper','Defender','Midfielder','Forward']\n        df_cm = pd.DataFrame(mat, columns=np.array(l_lab), index = np.unique(l_lab))\n        df_cm.index.name = 'True Label'\n        df_cm.columns.name = 'Predicted Label'\n        sb.heatmap(df_cm, cmap=\"Blues\", cbar=False, annot=True,annot_kws={\"size\": 10}, ax=ax)\n        plt.yticks(fontsize=10)\n        plt.xticks(fontsize=10)","de74106f":"plt.figure(figsize=(10,7))\nax1 = plt.subplot(111)\nplt.title('Confusion matrix of the one-vs-all logistic regression classifier', fontsize=16)\nplot_cm(conf_matrix, Y_test, ax1,0)","77efe5fb":"# Testing the one-vs-all function on 6 random examples \n\nexamples_number = 6 \nindex_random_sample = np.random.randint(70000, size=(1,examples_number))\nexample = mnist_data_normalized[index_random_sample].reshape(examples_number ,784)\ntrue_labels = mnist_label[index_random_sample].flatten().astype(int)\nlabel = one_vs_all(example, models_list)","550e8589":"# Viewing results \n\nplt.figure(figsize=(14,8))\nfor i in range(examples_number):\n    image = example[i].reshape(image_size_px,image_size_px)\n    plt.subplot(2,3,i+1)\n    plt.imshow(image)\n    title = f\"True label is: {true_labels[i]}, predicted as: {label[i]}\"\n    plt.title(title)","d860860a":"**Partial derivatives of the cost functions:**\n\nThe vectorized form of the partial derivatives of the cost function J with respect to the weights\nand the bias are given under a vectorized form by:\n$$\n\\frac{\\partial J}{\\partial w}=\\frac{1}{m}(X^T(\\hat{Y}-Y)), \\quad \\frac{\\partial J}{\\partial b}=\\frac{1}{m}Y^T\\hat{Y}\n$$\nWhere, \n$$\nX=\\left( \n\\begin{array}{c}\n\\cdots Image_1\\cdots\\\\\n\\cdots Image_2\\cdots\\\\\n.\\\\\n.\\\\\n.\\\\\n\\cdots Image_{m-1}\\cdots\\\\\n\\cdots Image_{m}\\cdots\n\\end{array}\n\\right)_{(m\\times n)},\\quad\nY= \\left( \n\\begin{array}{c}\ny^{(1)}\\\\\ny^{(2)}\\\\\n.\\\\\n.\\\\\n.\\\\\ny^{(m-1)}\\\\\ny^{(m)}\n\\end{array}\n\\right)_{(m\\times 1)},\\quad\n\\hat{Y}= \\left( \n\\begin{array}{c}\n\\hat{y}^{(1)}\\\\\n\\hat{y}^{(2)}\\\\\n.\\\\\n.\\\\\n.\\\\\n\\hat{y}^{(m-1)}\\\\\n\\hat{y}^{(m)}\n\\end{array}\n\\right)_{(m\\times 1)}\n$$","6c975f35":"## Importing and preprocessing data","2f5ffae5":"# Logistic regression with a neural network mindset: MNIST digits classification\n\nIn this project we're going to implement from scratch a one-vs-all logistic regression classifier for the [MNIST digits dataset](https:\/\/www.kaggle.com\/avnishnish\/mnist-original) with a neural network mindset. The neural network aspect of this implementation is the use of a forward and backward propagation to claculate the value of the cost function and the partial derivatives of the cost function with respect to weights and the bias.","09480955":"## Creating model functions ","3479944d":"## Final model for digit classification","b46a5b23":"## Training a model for each digit","e86a3c8f":"As for the training process, the forward propagation calculates the cost using a loss function\nand the backward propagation calculates the partial derivates of the cost function with respect\nthe the weights and bias using the chain rule. This way of implementation mimic the way a\nneural network train using forward and backward propagation.\nThe following diagram show the two phases of the training process:\n\n![ForwardBackProp1.png](attachment:ForwardBackProp1.png)","6caa8fa4":"We're going to implement the logistic regression algorithm using a Neural Netwotk mindset,\nwhich means that we're going to use a forward and backward propagation system in order to\ntrain the model.\nThe process of predicting the digit appearing in a certain image is represented in the following\ndiagram:\n\n![MNIST%20Diagram.png](attachment:MNIST%20Diagram.png)","da24ef3b":"In order to create a logistic regression model that classifies 10 digits (i.e from 0 to 9). We need\nto create 10 models, a model for each digit, this method is called the One-vs-all classification\nmethod. After calculating the probability y\u0302 of a certain image with each model, the model with\nhighest probability is used to classify the given image.\nThe following diagram illustrate the steps of the minimization of the cost function using gradient\ndescent.\n\n![Gradientdescentdiag.png](attachment:Gradientdescentdiag.png)","7b0ac345":"**Loss and cost functions:**\n* Loss function:  $$ \\mathcal{L}(\\hat{y},y)=-\\left( y\\log(\\hat{y})+(1-y)\\log(1-\\hat{y}) \\right)$$\n* Cost function: $$ J(w,b) = \\frac{1}{m}\\sum\\limits_{i=1}^m\\mathcal{L}(\\hat{y}^{(i)},y^{(i)})$$","78ab3b77":"## Training a test model for the digit \"0\""}}