{"cell_type":{"2749e6e6":"code","35a305cc":"code","0dc05156":"code","545bc5b0":"code","811320e6":"code","9cfaaebe":"code","2bd49643":"code","2fd6f135":"markdown","7a09a760":"markdown","ef2767b3":"markdown","82b200d7":"markdown","7da28d4f":"markdown","632bec90":"markdown","2404fb48":"markdown","e6049d6a":"markdown","207196dc":"markdown","567e37e2":"markdown","0dafc29c":"markdown","66a273a0":"markdown","8de50d8a":"markdown","099962d1":"markdown","2f4e9c85":"markdown","c30aec29":"markdown"},"source":{"2749e6e6":"#Given baseline data\nimport pandas as pd\n\nbaseline = {\"Metric Name\": [\"Unique cookies view course overview per Day\", \n                            \"Unique cookies click on 'free trial' per Day\", \n                            \"User-ids enroll per Day\", \n                            \"Click-through-probability on 'free trial' per Day\",\n                            \"Probability of enrolling, given click\", \n                            \"Probability of payment, given enroll\", \n                            \"Probability of payment, given click\"], \n     \"Baseline Value\": [40000, 3200, 660, 0.08, 0.20625, 0.53, 0.109313],\n     \"dmin practical*\": [3000, 240, 50, 0.01, 0.01, 0.01, 0.0075]}\npd_base = pd.DataFrame(baseline, index = ['Cookies','Clicks','User-ids','CTP','Gross Conversion','Retention','Net Conversion'])\npd_base","35a305cc":"print(\"Experiment Duration with all traffic=\", 685325\/40000, \"Experiment Duration with 80% traffic=\", 17\/0.8)","0dc05156":"#!pip install openpyxl","545bc5b0":"import pandas as pd\ncon = pd.read_excel(\"..\/input\/experiment-result\/Final Project.xlsx\",sheet_name=\"Control\")\nexp = pd.read_excel(\"..\/input\/experiment-result\/Final Project.xlsx\",sheet_name=\"Experiment\")\ncon.head()","811320e6":"import math\nz = 1.96\nPageviews_Sample_Size = con[\"Pageviews\"].sum() + exp[\"Pageviews\"].sum()\nClicks_Sample_Size = con[\"Clicks\"].sum() + exp[\"Clicks\"].sum()\nPageviews_hat = con[\"Pageviews\"].sum()\/(con[\"Pageviews\"].sum()+exp[\"Pageviews\"].sum())\nClicks_hat = con[\"Clicks\"].sum()\/(con[\"Clicks\"].sum()+exp[\"Clicks\"].sum())\nCTP_hat = (con[\"Clicks\"].sum()+exp[\"Clicks\"].sum())\/(con[\"Pageviews\"].sum()+exp[\"Pageviews\"].sum())\n\n# 1.pageviews\nSE_pageviews = math.sqrt((Pageviews_hat*(1-Pageviews_hat))\/Pageviews_Sample_Size)\nm_pageviews = z*SE_pageviews\n#m_pageviews\nCI_pageviews_low = 0.5-m_pageviews\nCI_pageviews_high = 0.5+m_pageviews\nOBS_pageviews = con[\"Pageviews\"].sum()\/Pageviews_Sample_Size\n\n# 2.clicks\nSE_clicks = math.sqrt((Clicks_hat*(1-Clicks_hat))\/Clicks_Sample_Size)\nm_clicks = z*SE_clicks\n#m_clicks\nCI_clicks_low = 0.5-m_clicks\nCI_clicks_high = 0.5+m_clicks\nOBS_clicks = con[\"Clicks\"].sum()\/Clicks_Sample_Size\n\n# 3. CTP\nSE_CTP = math.sqrt((CTP_hat*(1-CTP_hat))*(1\/con[\"Pageviews\"].sum() + 1\/exp[\"Pageviews\"].sum()))\nm_CTP = z*SE_CTP\n#m_CTP\nCI_CTP_low = -m_CTP\nCI_CTP_high = m_CTP\nOBS_CTP = (con[\"Clicks\"].sum()\/con[\"Pageviews\"].sum()) - (exp[\"Clicks\"].sum()\/exp[\"Pageviews\"].sum())\n\n\n#create empty dataframe to store sanity check results\nsanity_check = pd.DataFrame(columns=[\"CI_Low\", \"CI_High\", \"OBS\",\"Significant\"], index=[\"Pageviews\", \"Clicks\", \"CTP\"])\n\nsanity_check.loc['Pageviews']['CI_Low'] = CI_pageviews_low\nsanity_check.loc['Pageviews']['CI_High'] = CI_pageviews_high\nsanity_check.loc['Pageviews']['OBS'] = OBS_pageviews\n\nsanity_check.loc['Clicks']['CI_Low'] = CI_clicks_low\nsanity_check.loc['Clicks']['CI_High'] = CI_clicks_high\nsanity_check.loc['Clicks']['OBS'] = OBS_clicks\n\nsanity_check.loc['CTP']['CI_Low'] = CI_CTP_low\nsanity_check.loc['CTP']['CI_High'] = CI_CTP_high\nsanity_check.loc['CTP']['OBS'] = OBS_CTP\n\nfor i in range(len(sanity_check)):\n    if sanity_check.iloc[i][2]>=sanity_check.iloc[i][0] and sanity_check.iloc[i][2]<=sanity_check.iloc[i][1]:\n        sanity_check.iloc[i][3] = 'No'\n    else:\n        sanity_check.iloc[i][3] = 'Yes';\n        \nsanity_check","9cfaaebe":"#given \ndmin_GC = 0.01\ndmin_NC = 0.075\n\n#gross conversion = enroll\/clicks\nz=1.96\ndprac_GC = 0.01\ndprac_NC = 0.075\nd_GC = ((exp[\"Enrollments\"].sum()\/exp[\"Clicks\"][0:23].sum() - con[\"Enrollments\"].sum()\/con[\"Clicks\"][0:23].sum()))\nd_NC = ((exp[\"Payments\"].sum()\/exp[\"Clicks\"][0:23].sum() - con[\"Payments\"].sum()\/con[\"Clicks\"][0:23].sum()))\n\nP_GC_pool = (exp[\"Enrollments\"].sum() + con[\"Enrollments\"].sum()) \/ (exp[\"Clicks\"][0:23].sum() + con[\"Clicks\"][0:23].sum())\nSE_GC_pool = math.sqrt(P_GC_pool*(1-P_GC_pool)*(1\/exp[\"Clicks\"][0:23].sum()+1\/con[\"Clicks\"][0:23].sum()))\nm_GC = z*SE_GC_pool\nm_GC\n\n# net conversion = payments\/clicks\nP_NC_pool = (exp[\"Payments\"].sum() + con[\"Payments\"].sum()) \/ (exp[\"Clicks\"][0:23].sum() + con[\"Clicks\"][0:23].sum())\nSE_NC_pool = math.sqrt(P_NC_pool*(1-P_NC_pool)*(1\/exp[\"Clicks\"][0:23].sum()+1\/con[\"Clicks\"][0:23].sum()))\nm_NC = z*SE_NC_pool\nm_NC\n\n# compute a dataframe to contain the evaluation metrics\neffect_size = pd.DataFrame(columns=[\"dmin_prac\",\"OBS Diff\",\"CI_Low\", \"CI_High\", \"Practical\",\"Statistical\"], index=[\"Gross Conversion\", \"Net Conversion\"])\n\neffect_size.loc['Gross Conversion'][0] = dprac_GC\neffect_size.loc['Gross Conversion'][1] = d_GC\neffect_size.loc['Gross Conversion'][2] = d_GC - m_GC\neffect_size.loc['Gross Conversion'][3] = d_GC + m_GC\n\neffect_size.loc['Net Conversion'][0] = dprac_NC\neffect_size.loc['Net Conversion'][1] = d_NC\neffect_size.loc['Net Conversion'][2] = d_NC - m_NC\neffect_size.loc['Net Conversion'][3] = d_NC + m_NC\n\n\nfor i in range(len(effect_size)):\n    if effect_size.iloc[i][0]<effect_size.iloc[i][2] or effect_size.iloc[i][0]>effect_size.iloc[i][3]:\n        effect_size.iloc[i][4] = 'Yes'\n    elif abs(effect_size.iloc[i][0]) <effect_size.iloc[i][2] or abs(effect_size.iloc[i][0])>effect_size.iloc[i][3]:\n        effect_size.iloc[i][4] = 'Yes'\n    else:\n        effect_size.iloc[i][4] = 'No';\n        \n\nfor i in range(len(effect_size)):\n    if effect_size.iloc[i][2]<0 and effect_size.iloc[i][3]<0:\n        effect_size.iloc[i][5] = 'Yes'\n    elif effect_size.iloc[i][2]>0 and effect_size.iloc[i][3]>0:\n        effect_size.iloc[i][5] = 'Yes'\n    else:\n        effect_size.iloc[i][5] = 'No';\n\neffect_size","2bd49643":"#1. success means when the Observed Experiment has a higher Gross Coversion than Control Group\nSign = pd.DataFrame(columns=[\"Gross Conversion\",\"Net Conversion\"])\n\nSign['Gross Conversion']= exp[\"Enrollments\"]\/exp[\"Clicks\"] - con[\"Enrollments\"]\/con[\"Clicks\"] \nSign['Net Conversion'] = exp[\"Payments\"]\/exp[\"Clicks\"] - con[\"Payments\"]\/con[\"Clicks\"] \n\n# success in gross conversion:\nsuccess_GC = 0\ncount_GC= 0\nfor i in Sign['Gross Conversion']:\n    count_GC +=1\n    if i>0:\n        success_GC+=1\nprint(success_GC, count_GC)\n\nsuccess_NC = 0\ncount_NC= 0\nfor i in Sign['Net Conversion']:\n    count_NC += 1\n    if i>0:\n        success_NC+=1\nprint(success_NC, count_NC)","2fd6f135":"#### 3.1 Sanity Check <a id='section3.1'> <\/a>\nThe purpose of the sanity checks is to make sure the **invariant metrics** are equivalent for the Control group and the Experiment group. That is, we expect the invariant metrics in both groups are 50\/50 split. The number may varied, but we hope there is no significant differences between the groups <br><\/br>\n\nTheoretically, we want 50\/50 split between the control group and the experiment group, or we want the probability of the control group minus the probabiliyt of the experiment group is 0. Therefore,\n- $P_{pageviews} = 0.5$, &nbsp; $P_{clicks} = 0.5$, &nbsp; $P_{con}-P_{exp} = 0$  <br><\/br>\n\nOur observed results for Pageviews, Clicks, and CTP should be: \n- $\\hat{P_{OBS}} = \\frac{n_{con}}{n_{con}+n_{exp}}$ <br><\/br>\n\nHere is the formula to calculate the experiment result: <br><\/br>\n- $P(Pageviews_{con\/exp}) = \\frac{\\sum{Pageviews_{con\/exp}}}{\\sum{Pageviews_{con}+Pageviews_{exp}}}$ , &nbsp; $P(Clicks_{con\/exp}) = \\frac{\\sum{Clicks_{con\/exp}}}{\\sum{Clicks_{con}+Clicks_{exp}}}$ <br> <\/br>\n     $CTP_{con\/exp} = \\frac{\\sum{Clicks_{con\/exp}}}{\\sum{Pageviews_{con}+Pageviews_{exp}}}$ <br><\/br>\n     \nWe are expecting the experiment result falls into the 95% Confidence Interval (CI) <br> <\/br>\nIf the numerator and denominator are the same, for both the control group and the experiment group:\n- $\\hat{P_{OBS}} = \\frac{n_{con}}{n_{con}+n_{exp}}$, &nbsp; $SE =\\sqrt{\\frac{\\hat{P}*(1-\\hat{P})}{n}}$ <br> <\/br>\n\nIf the numerator and denominator are not the same, for both the control group and the experiment group: \n- $\\hat{P_{pool}}= \\frac{\\sum{Clicks_{con}}+\\sum{Clicks_{exp}}} {\\sum{Pageviews_{con}}+\\sum{Pageviews_{exp}}}$, &nbsp; $SE =\\sqrt{\\hat{P_{pool}}*(1-\\hat{P_{pool}})*(\\sum(\\frac{1}{Pageviews_{con}})+\\sum(\\frac{1}{Pageviews_{exp}}))}$ <br><\/br>\n- $MOE = z*SE$ <br><\/br>\n- $CI = \\hat{P} \\pm z*SE$  <br><\/br>\n    \n<u>As the calculation and the table below, we are 95% confident that our invariant metrics are similar for both groups, thus, the sanity check is passed<\/u>","7a09a760":"#### 2.5 Determine the Experiment Sample Size <a id='section2.5'> <\/a>\nBased on the variability of the evaluation metrics, we can determine our experiment sample size now.\nUse $\\alpha=0.05$ and $\\beta=0.2$ Click [here](https:\/\/www.evanmiller.org\/ab-testing\/sample-size.html) to access the sample size calculator\n\n**Gross Coversion**:\n- Given Baseline Value = $20.625\\%$, and Detectable Effect$_{min}$ = $1\\%$. <br><\/br>\n    Using the sample size calculator, the sample size for one group for the number of clicks on \"start free trial\" is $25,835$. Since our A\/B testing experiment have one control group and one experiment group, the sample size for the clicks $n = 25,835*2 = 51,670$\n    \n- Given CTP(${\\frac{\\#Clicks\\: on\\: Free\\: Trial}{\\#Course\\:Overview}}$) = 0.08, the Sample Size of the pageviews $N = \\frac{51,670}{0.08} = 645,875$\n\n**Net Conversion**:\n- Given Baseline Value = $10.9313\\%$, and Min Detectable Effect = $0.75\\%$. <br><\/br>\n    Using the sample size calculator, the sample size for one group for the number of user enrollment is $27,413$. Since our A\/B testing experiment have one control group and one experiment group, the sample size for the clicks $n = 27,413*2 = 54,826$\n    \n- Given CTP(${\\frac{\\#Clicks\\: on\\: Free\\: Trial}{\\#Course\\:Overview}}$) = 0.08, the Sample Size of the pageviews $N = \\frac{54,826}{0.08} = 685,325$\n\n<u> To test for the two metrics, we should use Sample size $N\\ge 685,325$ <\/u>","ef2767b3":"### 1. Context <a id='section1'><\/a>\nUdacity courses currently have two options on the course overview page: **\"start free trial\"**, and **\"access course materials\"**\n- If the student clicks  **\"start free trial\"**, they will be asked to enter their credit card information, and then they will be enrolled in a free trial for the paid version of the course. After 14 days, they will automatically be charged unless they cancel first. \n\n- If the student clicks **\"access course materials\"**, they will be able to view the videos and take the quizzes for free, but they will not receive coaching support or a verified certificate, and they will not submit their final project for feedback. \n\nUdacity tested a change where if the student clicked **\"start free trial\"**, they were asked how much time they had available to devote to the course. \n- If the student indicated 5+ hours per week, they would be taken through the checkout process as usual. \n- If they indicated 5- hours per week, a message would appear indicating that Udacity courses usually require a greater time commitment for successful completion, and suggesting that the student might like to access the course materials for free.\n\nAt this point, the student would have the option to continue enrolling in the free trial, or access the course materials for free instead. This [screenshot](https:\/\/drive.google.com\/file\/d\/0ByAfiG8HpNUMakVrS0s4cGN2TjQ\/view?resourcekey=0-6_dPu8BRM1XlRgV51nIbtA) shows what the experiment looks like.\n\n[Project Iinstruction](https:\/\/docs.google.com\/document\/u\/1\/d\/1aCquhIqsUApgsxQ8-SQBAigFDcfWVVohLEXcV6jWbdI\/pub)","82b200d7":"#### 3. Experiment Analysis <a id='section3'> <\/a>\nAfter chosing and calculating:\n1. the invariant metrics (Number of cookies, Number of clicks, CTP)\n2. the evaluation metrics (Gross Conversion, Net Conversion)\n3. the sample size (685,325) and days(22) that we needed\n\nThe experiment results are provided in the table below, or you can access the excel sheet by clicking [here](https:\/\/docs.google.com\/spreadsheets\/d\/1Mu5u9GrybDdska-ljPXyBjTpdZIUev_6i7t4LRDfXM8\/edit#gid=0).\n\nFor the variables, we have: \n1. <i> Experiment Date <\/i> \n2. <i> Number of Pageviews <\/i> \n3. <i> Number of Clicks on \"start free trial\" <\/i>\n4. <i> Number of Enrollments <\/i>\n5. <i> Number of Payments per day <\/i>","7da28d4f":"#### 3.3 Sign Tests <a id='section3.3'> <\/a>\nThe sign test is to check whether the signs of the difference of the metrics between the experiment and control groups agree with the confidence interval of the difference. Click [here](https:\/\/www.graphpad.com\/quickcalcs\/binomial1\/) for the p-value calculator.setting probability as 0.5.\n- For Gross Conversion, experience group are higher than control groups for 4 times out of 37, p-value=0.0026, which is significant.\n- For Net Conversion, experience group are higher than the control group for 10 times out of 37, p-value=0.6776, that is not significant. \n\nCalculation is done below:","632bec90":"#### 1.1 Purpose of Experiment <a id='section1.1'> <\/a>\nThe purpose of experiment is to help reducing the number of frustrated students who left the free trial because they didn't have enough time, without significantly reducing the number of students to continue past the free trial and eventually complete the course. \n\nIf this hypothesis held true, Udacity could improve the overall student experience and improve coaches' capacity to support students who are likely to complete the course.","2404fb48":"#### 2.3 Measure Variability: SE = $\\sqrt{\\frac{p*(1-p)}{n}}$ <a id='section2.3'> <\/a>\nFor each of the metrics, the standard deviation is calculated for a sample size of 5000 unique cookies visiting the course overview page. We want to see if the **anlaytic estimate** would be comparable to the **empirical variability**. i.e. standard deviation \n\nBaseline values is given to calculate SE:\n\n<table>\n  <tr>\n      <th> Metrics<\/th> <th>Formula<\/th> <th>Baseline value<\/th> <th>dmin*<\/th>\n  <\/tr>\n  <tr> \n      <td>Gross Conversion<\/td> \n      <td>$\\frac{Enrolls}{Clicks}$<\/td>\n      <td>0.20625<\/td>\n      <td> 0.01 <\/td>\n  <\/tr>\n  <tr>\n      <td>Net Conversion<\/td>\n      <td>$\\frac{Payment}{Clicks}$<\/td>\n      <td> 0.109313 <\/td>\n      <td>0.075<\/td>\n  <\/tr>\n<\/table>\n","e6049d6a":"#### 2.Experiment Deisgn - Metric Choice <a id='section2'> <\/a>\nAs the code shows below. The practical significance boundary for each metric, that is, the difference that would have to be observed before that was a meaningful change for the business, is given as **dmin**. All practical significance boundaries are given as **absolute changes**.\n\n- Any place \"unique cookies\" are mentioned, the uniqueness is determined by day. (That is, the same cookie visiting on different days would be counted twice.) \n- User-ids are automatically unique since the site does not allow the same user-id to enroll twice.","207196dc":"#### 3.2 Effect Size Tests <a id='section3.2'> <\/a>\nAfter the sanity check, we will continue examinate the evaluationi metrics. We want to check whether each metric is statically or practically significant on 95% confidence interval.\n- A metric is statistically significant if the CI does not include 0 (that is, you can be confident there was a change)\n- A metric is practically significant if the CI does not include the practical significance boundary (that is, you can be confident there is a change that matters to the business.)\n\nPreviously, we had chosen the following evaluation metrics: **1.Gross Conversion** and **2.Net Conversion**\n- $d^*_{Gross} = 0.01$, &nbsp; $d^*_{Net} = 0.075$ are given earlier. \n- $d_{OBS} = P_{exp}-P_{con}$\n\n<u> According to the calculation at the table below, we conclude that Gross Conversion is both Statistically and Practically significant while Net Conversion is only Practically Significant. <\/u>","567e37e2":"#### 2.6 Experiment Duration and Exposure <a id='section2.6'> <\/a>\n- The experiment indicates that the payments are made 14 days after enrollment. Therefore, we can expect the experiment to run for at least 14 days. Using the analytical variance we calculated above, we can decide the duration of the experiment.\n\n- Theoretically, we could divert 100% of the traffic to our experiment (i.e. about 50% of all visitors would then be in the experiment group. \n    Given 40,000 pageviews per day, it would take $\\frac{685,325}{40,000} = 17$ days \n\n- However, it might be too risky to run on all traffic, also 17 days are too short. Therefore, we can divert the traffic to, let's say 80%, to run the experiment longer.Therefore, we can run the experiment for $\\frac{17}{0.8} = 22$ days\n\n\n\n\n","0dafc29c":"#### 1.2 Population of Study <a id='section1.2'><\/a>\nUdacity is interested in users who already have an Udacity account. For users that **do not enroll**, their user-id is not tracked in the experiment, even if they were signed in when they visited the course overview page. \n\nThe **unit of diversion** is a cookie, although if the student enrolls in the free trial, they are tracked by user-id from that point forward. The same user-id cannot enroll in the free trial twice.\n\n","66a273a0":"#### 2.4 Variability of Evaluation Metrics <a id='section2.4'> <\/a>\n**Gross Conversion**:\n- Given sample size N=5000, and Unit of Analysis is Clicks on \"start free trial\" = CTP(${\\frac{\\#Clicks\\: on\\: Free\\: Trial}{\\#Course\\:Overview}}$) = 0.08, \\\n    n = 5000*0.08 = 400\n- Given P(gross_conversion)= 0.20625. Assuming binomial distribution, SE = $\\sqrt{\\frac{p*(1-p)}{n}}$ \\\n    SE = $\\sqrt{\\frac{0.20625*(1-0.20625)}{200}}$ = 0.02023\n\n**Net Conversion**: \n- Given sample size N=5000, and Unit of Analysis is Clicks on \"start free trial\" = ${\\frac{660}{40000}}$ = 0.08, \\\n    n = 5000*0.08 = 400\n- Given P(net_conversion)= 0.109313. Assuming binomial distribution, SE = $\\sqrt{\\frac{p*(1-p)}{n}}$ \\\n    SE = $\\sqrt{\\frac{0.109313*(1-0.109313)}{200}}$ = 0.02206","8de50d8a":"## Udacity \"Free Trial Screener\" A\/B testing \n### Table of Content\n1. [Context](#section1) <br\/>\n    1.1 [Purpose of Experiment](#section1.1) <br\/>\n    1.2 [Population of Study](#section1.2) <br\/>\n2. [Experiment Design](#section2) <br\/>\n    2.1 [Choosing Invariant Metrics](#section2.1) <br\/>\n    2.2 [Choosing Evaluation Metrics](#section2.2) <br\/>\n    2.3. [Measure Varaibility: The Standard Error](#section2.3) <br\/>\n    2.4 [For the Evaluation Metrics](#section2.4) <br\/>\n    2.5 [Determine the Sample Size](#section2.5) <br\/>\n    2.6. [Experiment Duration and Exposure](#section2.6) <br\/>\n3. [Experiment Analysis](#section3) <br\/>\n    3.1 [Sanity Check](#section3.1) <br\/>\n    3.2 [Effect Size Tests](#section3.2) <br\/>\n    3.3 [Sign Tests](#section3.3) <br\/>\n4. [Conclusion and Recommendation](#section4) <br\/>\n","099962d1":"#### 2.1 Choosing Invariant Metrics <a id='section2.1'><\/a>\nInvariant metrics are one of the sanity checks, that is, they are the metrics have no significant changes during the experiment. Given the metrics table above, the following invariant metrics are chosen: <\/br>\n1. **Number of Cookies**: #unique cookies to view the course overview page per day. \n2. **Number of Clicks**: #unique cookies to click the \u201cstart free trial\u201d button per day.\n3. **CTP**: #unique clicks on \"start free trial\" divided by #unique cookies to view the course overview page per day\n\n**Reasoning:** \n- The experiment is triggered after a user click on the \"start free trial\". Therefore, there is no new changes when a unique cookie visit the course overview page as before, and the number of clicks should be similar when implementing the experiment. ","2f4e9c85":"#### 2.2 Choosing Evaluation Metrics <a id='section2.2'><\/a>\n1. **Gross conversion**: number of user-ids to complete checkout and enroll in the free trial divided by number of unique cookies to click the \u201cStart free trial\u201d button. \n2. **Net conversion**: number of user-ids to remain enrolled past the 14-day boundary (and thus make at least one payment) divided by the number of unique cookies to click the \u201cStart free trial\u201d button.\n\n**Reasoning:**\n- Since our experiment is expecting changes after students click on the \"start free trial button\". Therefore, after the click, the number of enrollment (#user-ids enroll in the free trial) and the number of payments could reasonably been affected.","c30aec29":"#### 4. Conclusion and Recommendation <a id='section4'> <\/a>\nBack to the purpose of the experiment, our hypothesis is to help Udacity filtering out students who wouldn\u2019t commit to the study time, while not reducing the number of students who will make the payment after completing their free trial. The final results show that **Gross Conversion** (Enrollments\/Clicks) will be reduced significantly. However, **Net Conversion** (Payments\/Clicks) has no significant changes. \n\nAt this point, we can conclude that the new feature will help reduce the enrollments, but not enough evidence to show that there will be more students who make the payments. \n\nMy recomendation is not to launch this experiment. Instead, we can design a follow-up experiment to determine whether the screener would decrease the number of early payment cancelation in the 14 days free trial compared with before. "}}