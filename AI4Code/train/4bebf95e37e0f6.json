{"cell_type":{"52093f71":"code","f6792cb3":"code","24227ba1":"code","03396861":"code","ec97dd63":"code","7c821ba2":"code","eab10355":"code","10241c31":"code","3d61cb4c":"markdown","433ae2af":"markdown","319dfee0":"markdown","81e08f89":"markdown"},"source":{"52093f71":"# Read test data. I have previously split the data into K-folds, and also joined with `users.csv`\n\n# The K-splits are stored in this folder\nDATA_FOLDER = '\/kaggle\/input\/shopee-w8\/kfolds'\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\n\ntest = pd.read_csv(os.path.join(DATA_FOLDER, 'test.csv'), parse_dates=['grass_date'])\ntest.head()","f6792cb3":"NFOLDS = 5\n\n\ntrains = []\nvalids = []\nfor fold_id in range(NFOLDS):\n    trains.append(pd.read_csv(os.path.join(DATA_FOLDER, str(fold_id), 'train.csv'), parse_dates=['grass_date']))\n    valids.append(pd.read_csv(os.path.join(DATA_FOLDER, str(fold_id), 'valid.csv'), parse_dates=['grass_date']))\n\n\ntrains[0].head()","24227ba1":"# Fit OneHotEncoders\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n\nCATEGORICAL_COLS = ['domain', 'country_code']\n\nOH_encoders = [OneHotEncoder(handle_unknown='ignore') for i in range(NFOLDS)]\nfor i in range(NFOLDS):\n    print('Processing fold', i)\n    OH_encoders[i].fit(trains[i][CATEGORICAL_COLS])","03396861":"# Build features\n\n\nLAST_DAY_COLS = ['last_open_day', 'last_login_day', 'last_checkout_day']\nDROP_COLS = ['user_id', 'row_id', 'subject_line_length']\n\nX_trains = [None for i in range(NFOLDS)]\ny_trains = [None for i in range(NFOLDS)]\nX_valids = [None for i in range(NFOLDS)]\ny_valids = [None for i in range(NFOLDS)]\nX_tests = [None for i in range(NFOLDS)]\n\n\ndef convert_last(s):\n    if s in ['Never open', 'Never checkout', 'Never login']:\n        return -1\n    return int(s)\n\n\ndef build_features(fold_id, dataset):\n    target = None\n    res = dataset.drop(columns=['user_id', 'row_id', 'subject_line_length'])\n    if 'open_flag' in dataset.columns:\n        target = res['open_flag']\n        res.drop(columns=['open_flag'], inplace=True)\n    \n    # Last day columns: convert to int\n    for col in LAST_DAY_COLS:\n        res[col] = res[col].apply(convert_last)\n    \n    # Grass date: convert to day of week\n    res['grass_day_of_week'] = res['grass_date'].apply(lambda x: x.weekday())\n    res.drop(columns=['grass_date'], inplace=True)\n\n    # Process one-hot columns\n    OH_cols = pd.DataFrame(OH_encoders[fold_id].transform(res[CATEGORICAL_COLS]).toarray())\n    OH_cols.index = res.index\n    res.drop(columns=CATEGORICAL_COLS, inplace=True)\n    res = pd.concat([res, OH_cols], axis=1)\n\n    # Process columns with NA\n    for col in ['attr_1', 'attr_2', 'age']:\n        res[col].fillna(-1, inplace=True)\n\n    return res, target\n\n\nfor fold_id in range(NFOLDS):\n    print('Processing fold', fold_id)\n    X_trains[fold_id], y_trains[fold_id] = build_features(fold_id, trains[fold_id])\n    X_valids[fold_id], y_valids[fold_id] = build_features(fold_id, valids[fold_id])\n    X_tests[fold_id], _ = build_features(fold_id, test)\n\n\nX_trains[0].head()","ec97dd63":"from imblearn.pipeline import make_pipeline, Pipeline\nfrom imblearn.over_sampling import SMOTE\nimport optuna\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import matthews_corrcoef, make_scorer\nfrom sklearn.ensemble import RandomForestClassifier","7c821ba2":"\"\"\"\nResults:\n\n===== Done fold 0 =====\n0.5178508619346296\n{'n_estimators': 817, 'max_depth': 38, 'min_samples_split': 26, 'min_samples_leaf': 8, 'smoth_n_neighbors': 9}\n\n===== Done fold 1 =====\n0.5109257631569604\n{'n_estimators': 200, 'max_depth': 33, 'min_samples_split': 147, 'min_samples_leaf': 9, 'smoth_n_neighbors': 9}\n\n===== Done fold 2 =====\n0.5095566726766917\n{'n_estimators': 705, 'max_depth': 29, 'min_samples_split': 4, 'min_samples_leaf': 43, 'smoth_n_neighbors': 8}\n\n===== Done fold 3 =====\n0.5130338493405462\n{'n_estimators': 274, 'max_depth': 33, 'min_samples_split': 41, 'min_samples_leaf': 14, 'smoth_n_neighbors': 9}\n\n===== Done fold 4 =====\n0.5115307558584887\n{'n_estimators': 613, 'max_depth': 18, 'min_samples_split': 70, 'min_samples_leaf': 18, 'smoth_n_neighbors': 6}\n\n\"\"\"\n\n\nstudies = [None for i in range(NFOLDS)]\n\n\n# Change following line to range(NFOLDS) to run & find best params.\nfor fold_id in range(NFOLDS, NFOLDS):\n    print('========== Processing fold', fold_id, '==========')\n\n    def objective(trial:optuna.trial.Trial):\n        params = {\n            'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n            'max_depth': trial.suggest_int('max_depth', 4, 50),\n            'min_samples_split': trial.suggest_int('min_samples_split', 1, 150),\n            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 60),\n        }\n\n        smoth_n_neighbors = trial.suggest_int('smoth_n_neighbors', 5, 10)\n        sampler = SMOTE(random_state=42, k_neighbors=smoth_n_neighbors)\n\n        clf = RandomForestClassifier(random_state=42, **params)\n        pipeline = make_pipeline(sampler, clf)\n        scores = cross_validate(pipeline, X_trains[fold_id], y_trains[fold_id], verbose=1,\n                    n_jobs=-1, scoring=make_scorer(matthews_corrcoef), cv=4)\n        return scores[\"test_score\"].mean()\n\n\n    studies[fold_id] = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n    studies[fold_id].optimize(objective, n_trials=20)\n\n    print(f'===== Done fold {fold_id} =====')\n    print(studies[fold_id].best_value)\n    print(studies[fold_id].best_params)","eab10355":"# Build the final models with best params found above.\n\nrf_best_params = [\n    {'n_estimators': 817, 'max_depth': 38, 'min_samples_split': 26, 'min_samples_leaf': 8},\n    {'n_estimators': 200, 'max_depth': 33, 'min_samples_split': 147, 'min_samples_leaf': 9},\n    {'n_estimators': 705, 'max_depth': 29, 'min_samples_split': 4, 'min_samples_leaf': 43},\n    {'n_estimators': 274, 'max_depth': 33, 'min_samples_split': 41, 'min_samples_leaf': 14},\n    {'n_estimators': 613, 'max_depth': 18, 'min_samples_split': 70, 'min_samples_leaf': 18},\n]\nsmote_best_params = [\n    {'smoth_n_neighbors': 9},\n    {'smoth_n_neighbors': 9},\n    {'smoth_n_neighbors': 8},\n    {'smoth_n_neighbors': 9},\n    {'smoth_n_neighbors': 6},\n]\n\nrfs = []\nsamplers = []\nmcc_sum = 0.0\nfor i in range(NFOLDS):\n    print('Processing fold', i)\n    samplers.append(SMOTE(random_state=42, k_neighbors=smote_best_params[i][\"smoth_n_neighbors\"]))\n    rf = RandomForestClassifier(random_state=42, **rf_best_params[i])\n    rfs.append(rf)\n\n    pipeline = Pipeline([(\"sampler\", samplers[i]), (\"clf\", rf)])\n    pipeline.fit(X_trains[i], y_trains[i])\n    preds_valid = rf.predict(X_valids[i].values)\n    mcc = matthews_corrcoef(y_valids[i], preds_valid)\n    print('MCC:', mcc)\n    mcc_sum += mcc\n\n\nprint('Avg MCC:', mcc_sum \/ NFOLDS)","10241c31":"from tqdm.notebook import tqdm\n\n\ndef print_result(clfs, Xs):\n    probs = np.zeros(shape=(Xs[0].shape[0], 2))\n\n    for fold_id in tqdm(range(len(clfs))):\n        probs += clfs[fold_id].predict_proba(Xs[fold_id]) \/ NFOLDS\n    preds = np.argmax(probs, axis=1)\n\n    submission = pd.DataFrame({\n        'row_id': test['row_id'],\n        'open_flag': preds,\n    })\n    submission.to_csv('submission.csv', index=False)\n\n\nprint_result(rfs, X_tests)","3d61cb4c":"# Getting prediction results\n\nHere I simply get the average probability of K-folds, and use it as prediction","433ae2af":"# Modeling\n\nNow it's time to build our models. I used Optuna for hyper parameter tuning.","319dfee0":"# Prepare Data\n\nI have previously split the data into K-folds, and also joined with `users.csv`. The K-folds are used for 2 purposes:\n\n1. Get better validation result\n2. Build K models, and combine the results of these K models.\n","81e08f89":"# Build features\n\nHere what I did was:\n\n1. Convert `last_X_day` columns to `int`, and also change all `Never` values to `-1`.\n2. One hot encoding for all categorical columns\n3. Drop column `subject_line_length`. I think this feature will overfit your model: 2 subject with same length doesn't necessary have the same content.\n4. Change `grass_date` to day of week. Though I don't think this feature is necessary because it has very low correlation with `open_flag`.\n5. Fill other null values with `-1`."}}