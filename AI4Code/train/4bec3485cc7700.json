{"cell_type":{"015f65a9":"code","7f5b7be1":"code","cb39f080":"code","f9234980":"code","21643db3":"code","6103a7b7":"code","ada3f6a8":"code","1d0d6248":"code","d3efb58d":"code","8d151431":"code","75324d44":"code","add5e0b6":"code","f0e68a3d":"code","884ee1f0":"code","c41f4f41":"code","8c852281":"code","21b43369":"code","7d2e2f9b":"code","ce7db872":"code","032ee615":"code","b31d5f1b":"code","59d493d7":"code","dabee7f1":"code","016da563":"code","b7e0320c":"code","c8b21770":"code","bb627649":"code","29386891":"code","aea4682e":"code","5501eb57":"code","85020963":"code","cbc4300d":"code","68c795ff":"code","c208a693":"code","0b5a5cfb":"code","9d86e1e5":"code","90dda9e4":"code","d1484371":"code","91bd1091":"code","9b8fda00":"code","4c5b4ffd":"code","cb0c934e":"code","6ab6d77b":"code","5ea63177":"code","895d0d6b":"code","988ed3dc":"code","836f51a0":"code","14bf5f51":"code","fd233cc6":"markdown","cbf0b429":"markdown","e3bba5cf":"markdown","2c2d85a0":"markdown","8a941131":"markdown","626e8fa8":"markdown","242fdd45":"markdown","874210e3":"markdown","a78537f6":"markdown","c1cc5b40":"markdown","a134663c":"markdown","ff3de994":"markdown","a17df85c":"markdown","3c06c5d7":"markdown","946ef1bb":"markdown","c0336dfa":"markdown","0106466d":"markdown","c22b00f7":"markdown","e1fcab81":"markdown","500ada06":"markdown","37e28db1":"markdown","c7581d21":"markdown","bd7b6087":"markdown","983be461":"markdown","4f625058":"markdown","ef57b263":"markdown","573565d0":"markdown","11135c6b":"markdown","caffee7d":"markdown","b61a1cc7":"markdown","e31a44c6":"markdown","827b2a40":"markdown","86c1e388":"markdown","01c7739e":"markdown","26795669":"markdown","21ee09b2":"markdown","3652d55a":"markdown","96f5cc8d":"markdown","326666ba":"markdown","5ccbfeac":"markdown","f8dc1c40":"markdown","be5d7d36":"markdown","90a0b31a":"markdown","2984bf80":"markdown"},"source":{"015f65a9":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\n\nplt.rcParams.update({'font.size': 14})\n\n# Load data\ntrain = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsub_sample = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","7f5b7be1":"print (train.shape, test.shape, sub_sample.shape)","cb39f080":"train.head()","f9234980":"train.duplicated().sum()","21643db3":"\nsns.countplot(y=train.target);","6103a7b7":"train.target.value_counts()","ada3f6a8":"# NA data\ntrain.isnull().sum()","1d0d6248":"test.isnull().sum()","d3efb58d":"# Check number of unique keywords, and whether they are the same for train and test sets\nprint (train.keyword.nunique(), test.keyword.nunique())\nprint (set(train.keyword.unique()) - set(test.keyword.unique()))","8d151431":"# Most common keywords\nplt.figure(figsize=(9,6))\nsns.countplot(y=train.keyword, order = train.keyword.value_counts().iloc[:15].index)\nplt.title('Top 15 keywords')\nplt.show()\n# train.keyword.value_counts().head(10)","75324d44":"kw_d = train[train.target==1].keyword.value_counts().head(10)\nkw_nd = train[train.target==0].keyword.value_counts().head(10)\n\nplt.figure(figsize=(13,5))\nplt.subplot(121)\nsns.barplot(kw_d, kw_d.index, color='c')\nplt.title('Top keywords for disaster tweets')\nplt.subplot(122)\nsns.barplot(kw_nd, kw_nd.index, color='y')\nplt.title('Top keywords for non-disaster tweets')\nplt.show()","add5e0b6":"top_d = train.groupby('keyword').mean()['target'].sort_values(ascending=False).head(10)\ntop_nd = train.groupby('keyword').mean()['target'].sort_values().head(10)\n\nplt.figure(figsize=(13,5))\nplt.subplot(121)\nsns.barplot(top_d, top_d.index, color='pink')\nplt.title('Keywords with highest % of disaster tweets')\nplt.subplot(122)\nsns.barplot(top_nd, top_nd.index, color='yellow')\nplt.title('Keywords with lowest % of disaster tweets')\nplt.show()","f0e68a3d":"# Check number of unique keywords and locations\nprint (train.location.nunique(), test.location.nunique())","884ee1f0":"# Most common locations\nplt.figure(figsize=(9,6))\nsns.countplot(y=train.location, order = train.location.value_counts().iloc[:15].index)\nplt.title('Top 15 locations')\nplt.show()","c41f4f41":"raw_loc = train.location.value_counts()\ntop_loc = list(raw_loc[raw_loc>=10].index)\ntop_only = train[train.location.isin(top_loc)]\n\ntop_l = top_only.groupby('location').mean()['target'].sort_values(ascending=False)\nplt.figure(figsize=(14,6))\nsns.barplot(x=top_l.index, y=top_l)\nplt.axhline(np.mean(train.target))\nplt.xticks(rotation=80)\nplt.show()","8c852281":"# Fill NA values\nfor col in ['keyword','location']:\n    train[col] = train[col].fillna('None')\n    test[col] = test[col].fillna('None')\n\ndef clean_loc(x):\n    if x == 'None':\n        return 'None'\n    elif x == 'Earth' or x =='Worldwide' or x == 'Everywhere':\n        return 'World'\n    elif 'New York' in x or 'NYC' in x:\n        return 'New York'    \n    elif 'London' in x:\n        return 'London'\n    elif 'Mumbai' in x:\n        return 'Mumbai'\n    elif 'Washington' in x and 'D' in x and 'C' in x:\n        return 'Washington DC'\n    elif 'San Francisco' in x:\n        return 'San Francisco'\n    elif 'Los Angeles' in x:\n        return 'Los Angeles'\n    elif 'Seattle' in x:\n        return 'Seattle'\n    elif 'Chicago' in x:\n        return 'Chicago'\n    elif 'Toronto' in x:\n        return 'Toronto'\n    elif 'Sacramento' in x:\n        return 'Sacramento'\n    elif 'Atlanta' in x:\n        return 'Atlanta'\n    elif 'California' in x:\n        return 'California'\n    elif 'Florida' in x:\n        return 'Florida'\n    elif 'Texas' in x:\n        return 'Texas'\n    elif 'United States' in x or 'USA' in x:\n        return 'USA'\n    elif 'United Kingdom' in x or 'UK' in x or 'Britain' in x:\n        return 'UK'\n    elif 'Canada' in x:\n        return 'Canada'\n    elif 'India' in x:\n        return 'India'\n    elif 'Kenya' in x:\n        return 'Kenya'\n    elif 'Nigeria' in x:\n        return 'Nigeria'\n    elif 'Australia' in x:\n        return 'Australia'\n    elif 'Indonesia' in x:\n        return 'Indonesia'\n    elif x in top_loc:\n        return x\n    else: return 'Others'\n    \ntrain['location_clean'] = train['location'].apply(lambda x: clean_loc(str(x)))\ntest['location_clean'] = test['location'].apply(lambda x: clean_loc(str(x)))","21b43369":"top_l2 = train.groupby('location_clean').mean()['target'].sort_values(ascending=False)\nplt.figure(figsize=(14,6))\nsns.barplot(x=top_l2.index, y=top_l2)\nplt.axhline(np.mean(train.target))\nplt.xticks(rotation=80)\nplt.show()","7d2e2f9b":"leak = pd.read_csv(\"..\/input\/disasters-on-social-media\/socialmedia-disaster-tweets-DFE.csv\", encoding='latin_1')\nleak['target'] = (leak['choose_one']=='Relevant').astype(int)\nleak['id'] = leak.index\nleak = leak[['id', 'target','text']]\nmerged_df = pd.merge(test, leak, on='id')\nsub1 = merged_df[['id', 'target']]\nsub1.to_csv('submit_1.csv', index=False)","ce7db872":"import re\n\ntest_str = train.loc[417, 'text']\n\ndef clean_text(text):\n    text = re.sub(r'https?:\/\/\\S+', '', text) # Remove link\n    text = re.sub(r'\\n',' ', text) # Remove line breaks\n    text = re.sub('\\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces\n    return text\n\nprint(\"Original text: \" + test_str)\nprint(\"Cleaned text: \" + clean_text(test_str))","032ee615":"def find_hashtags(tweet):\n    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"#\\w+\", tweet)]) or 'no'\n\ndef find_mentions(tweet):\n    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"@\\w+\", tweet)]) or 'no'\n\ndef find_links(tweet):\n    return \" \".join([match.group(0)[:] for match in re.finditer(r\"https?:\/\/\\S+\", tweet)]) or 'no'\n\ndef process_text(df):\n    \n    df['text_clean'] = df['text'].apply(lambda x: clean_text(x))\n    df['hashtags'] = df['text'].apply(lambda x: find_hashtags(x))\n    df['mentions'] = df['text'].apply(lambda x: find_mentions(x))\n    df['links'] = df['text'].apply(lambda x: find_links(x))\n    # df['hashtags'].fillna(value='no', inplace=True)\n    # df['mentions'].fillna(value='no', inplace=True)\n    \n    return df\n    \ntrain = process_text(train)\ntest = process_text(test)","b31d5f1b":"from wordcloud import STOPWORDS\n\ndef create_stat(df):\n    # Tweet length\n    df['text_len'] = df['text_clean'].apply(len)\n    # Word count\n    df['word_count'] = df[\"text_clean\"].apply(lambda x: len(str(x).split()))\n    # Stopword count\n    df['stop_word_count'] = df['text_clean'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n    # Punctuation count\n    df['punctuation_count'] = df['text_clean'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n    # Count of hashtags (#)\n    df['hashtag_count'] = df['hashtags'].apply(lambda x: len(str(x).split()))\n    # Count of mentions (@)\n    df['mention_count'] = df['mentions'].apply(lambda x: len(str(x).split()))\n    # Count of links\n    df['link_count'] = df['links'].apply(lambda x: len(str(x).split()))\n    # Count of uppercase letters\n    df['caps_count'] = df['text_clean'].apply(lambda x: sum(1 for c in str(x) if c.isupper()))\n    # Ratio of uppercase letters\n    df['caps_ratio'] = df['caps_count'] \/ df['text_len']\n    return df\n\ntrain = create_stat(train)\ntest = create_stat(test)\n\nprint(train.shape, test.shape)","59d493d7":"train.corr()['target'].drop('target').sort_values()","dabee7f1":"from nltk import FreqDist, word_tokenize\n\n# Make a set of stop words\nstopwords = set(STOPWORDS)\n# more_stopwords = {'https', 'amp'}\n# stopwords = stopwords.union(more_stopwords)","016da563":"# Unigrams\nword_freq = FreqDist(w for w in word_tokenize(' '.join(train['text_clean']).lower()) if \n                     (w not in stopwords) & (w.isalpha()))\ndf_word_freq = pd.DataFrame.from_dict(word_freq, orient='index', columns=['count'])\ntop20w = df_word_freq.sort_values('count',ascending=False).head(20)\n\nplt.figure(figsize=(8,6))\nsns.barplot(top20w['count'], top20w.index)\nplt.title('Top 20 words')\nplt.show()","b7e0320c":"plt.figure(figsize=(16,7))\nplt.subplot(121)\nfreq_d = FreqDist(w for w in word_tokenize(' '.join(train.loc[train.target==1, 'text_clean']).lower()) if \n                     (w not in stopwords) & (w.isalpha()))\ndf_d = pd.DataFrame.from_dict(freq_d, orient='index', columns=['count'])\ntop20_d = df_d.sort_values('count',ascending=False).head(20)\nsns.barplot(top20_d['count'], top20_d.index, color='c')\nplt.title('Top words in disaster tweets')\nplt.subplot(122)\nfreq_nd = FreqDist(w for w in word_tokenize(' '.join(train.loc[train.target==0, 'text_clean']).lower()) if \n                     (w not in stopwords) & (w.isalpha()))\ndf_nd = pd.DataFrame.from_dict(freq_nd, orient='index', columns=['count'])\ntop20_nd = df_nd.sort_values('count',ascending=False).head(20)\nsns.barplot(top20_nd['count'], top20_nd.index, color='y')\nplt.title('Top words in non-disaster tweets')\nplt.show()","c8b21770":"# Bigrams\n\nfrom nltk import bigrams\n\nplt.figure(figsize=(16,7))\nplt.subplot(121)\nbigram_d = list(bigrams([w for w in word_tokenize(' '.join(train.loc[train.target==1, 'text_clean']).lower()) if \n              (w not in stopwords) & (w.isalpha())]))\nd_fq = FreqDist(bg for bg in bigram_d)\nbgdf_d = pd.DataFrame.from_dict(d_fq, orient='index', columns=['count'])\nbgdf_d.index = bgdf_d.index.map(lambda x: ' '.join(x))\nbgdf_d = bgdf_d.sort_values('count',ascending=False)\nsns.barplot(bgdf_d.head(20)['count'], bgdf_d.index[:20], color='pink')\nplt.title('Top bigrams in disaster tweets')\nplt.subplot(122)\nbigram_nd = list(bigrams([w for w in word_tokenize(' '.join(train.loc[train.target==0, 'text_clean']).lower()) if \n              (w not in stopwords) & (w.isalpha())]))\nnd_fq = FreqDist(bg for bg in bigram_nd)\nbgdf_nd = pd.DataFrame.from_dict(nd_fq, orient='index', columns=['count'])\nbgdf_nd.index = bgdf_nd.index.map(lambda x: ' '.join(x))\nbgdf_nd = bgdf_nd.sort_values('count',ascending=False)\nsns.barplot(bgdf_nd.head(20)['count'], bgdf_nd.index[:20], color='yellow')\nplt.title('Top bigrams in non-disaster tweets')\nplt.show()","bb627649":"import category_encoders as ce\n\n# Target encoding\nfeatures = ['keyword', 'location_clean']\nencoder = ce.TargetEncoder(cols=features)\nencoder.fit(train[features],train['target'])\n\ntrain = train.join(encoder.transform(train[features]).add_suffix('_target'))\ntest = test.join(encoder.transform(test[features]).add_suffix('_target'))","29386891":"from sklearn.feature_extraction.text import CountVectorizer\n\n# CountVectorizer\n\n# Links\nvec_links = CountVectorizer(min_df = 5, analyzer = 'word', token_pattern = r'https?:\/\/\\S+') # Only include those >=5 occurrences\nlink_vec = vec_links.fit_transform(train['links'])\nlink_vec_test = vec_links.transform(test['links'])\nX_train_link = pd.DataFrame(link_vec.toarray(), columns=vec_links.get_feature_names())\nX_test_link = pd.DataFrame(link_vec_test.toarray(), columns=vec_links.get_feature_names())\n\n# Mentions\nvec_men = CountVectorizer(min_df = 5)\nmen_vec = vec_men.fit_transform(train['mentions'])\nmen_vec_test = vec_men.transform(test['mentions'])\nX_train_men = pd.DataFrame(men_vec.toarray(), columns=vec_men.get_feature_names())\nX_test_men = pd.DataFrame(men_vec_test.toarray(), columns=vec_men.get_feature_names())\n\n# Hashtags\nvec_hash = CountVectorizer(min_df = 5)\nhash_vec = vec_hash.fit_transform(train['hashtags'])\nhash_vec_test = vec_hash.transform(test['hashtags'])\nX_train_hash = pd.DataFrame(hash_vec.toarray(), columns=vec_hash.get_feature_names())\nX_test_hash = pd.DataFrame(hash_vec_test.toarray(), columns=vec_hash.get_feature_names())\nprint (X_train_link.shape, X_train_men.shape, X_train_hash.shape)","aea4682e":"_ = (X_train_link.transpose().dot(train['target']) \/ X_train_link.sum(axis=0)).sort_values(ascending=False)\nplt.figure(figsize=(10,6))\nsns.barplot(x=_, y=_.index)\nplt.axvline(np.mean(train.target))\nplt.title('% of disaster tweet given links')\nplt.show()","5501eb57":"_ = (X_train_men.transpose().dot(train['target']) \/ X_train_men.sum(axis=0)).sort_values(ascending=False)\nplt.figure(figsize=(14,6))\nsns.barplot(x=_.index, y=_)\nplt.axhline(np.mean(train.target))\nplt.title('% of disaster tweet given mentions')\nplt.xticks(rotation = 50)\nplt.show()","85020963":"hash_rank = (X_train_hash.transpose().dot(train['target']) \/ X_train_hash.sum(axis=0)).sort_values(ascending=False)\nprint('Hashtags with which 100% of Tweets are disasters: ')\nprint(list(hash_rank[hash_rank==1].index))\nprint('Total: ' + str(len(hash_rank[hash_rank==1])))\nprint('Hashtags with which 0% of Tweets are disasters: ')\nprint(list(hash_rank[hash_rank==0].index))\nprint('Total: ' + str(len(hash_rank[hash_rank==0])))","cbc4300d":"# Tf-idf for text\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvec_text = TfidfVectorizer(min_df = 10, ngram_range = (1,2), stop_words='english') \n# Only include >=10 occurrences\n# Have unigrams and bigrams\ntext_vec = vec_text.fit_transform(train['text_clean'])\ntext_vec_test = vec_text.transform(test['text_clean'])\nX_train_text = pd.DataFrame(text_vec.toarray(), columns=vec_text.get_feature_names())\nX_test_text = pd.DataFrame(text_vec_test.toarray(), columns=vec_text.get_feature_names())\nprint (X_train_text.shape)","68c795ff":"# Joining the dataframes together\n\ntrain = train.join(X_train_link, rsuffix='_link')\ntrain = train.join(X_train_men, rsuffix='_mention')\ntrain = train.join(X_train_hash, rsuffix='_hashtag')\ntrain = train.join(X_train_text, rsuffix='_text')\ntest = test.join(X_test_link, rsuffix='_link')\ntest = test.join(X_test_men, rsuffix='_mention')\ntest = test.join(X_test_hash, rsuffix='_hashtag')\ntest = test.join(X_test_text, rsuffix='_text')\nprint (train.shape, test.shape)","c208a693":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\n\nfeatures_to_drop = ['id', 'keyword','location','text','location_clean','text_clean', 'hashtags', 'mentions','links']\nscaler = MinMaxScaler()\n\nX_train = train.drop(columns = features_to_drop + ['target'])\nX_test = test.drop(columns = features_to_drop)\ny_train = train.target\n\nlr = LogisticRegression(solver='liblinear', random_state=777) # Other solvers have failure to converge problem\n\npipeline = Pipeline([('scale',scaler), ('lr', lr),])\n\npipeline.fit(X_train, y_train)\ny_test = pipeline.predict(X_test)\n\nsubmit = sub_sample.copy()\nsubmit.target = y_test\nsubmit.to_csv('submisson1.csv',index=False)","0b5a5cfb":"print ('Training accuracy: %.4f' % pipeline.score(X_train, y_train))","9d86e1e5":"# F-1 score\nfrom sklearn.metrics import f1_score\n\nprint ('Training f-1 score: %.4f' % f1_score(y_train, pipeline.predict(X_train)))","90dda9e4":"\nfrom sklearn.metrics import confusion_matrix\npd.DataFrame(confusion_matrix(y_train, pipeline.predict(X_train)))","d1484371":"# Cross validation\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import ShuffleSplit\n\ncv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=123)\ncv_score = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='f1')\nprint('Cross validation F-1 score: %.3f' %np.mean(cv_score))","91bd1091":"# Top features\nplt.figure(figsize=(16,7))\ns1 = pd.Series(np.transpose(lr.coef_[0]), index=X_train.columns).sort_values(ascending=False)[:20]\ns2 = pd.Series(np.transpose(lr.coef_[0]), index=X_train.columns).sort_values()[:20]\nplt.subplot(121)\nsns.barplot(y=s1.index, x=s1)\nplt.title('Top positive coefficients')\nplt.subplot(122)\nsns.barplot(y=s2.index, x=s2)\nplt.title('Top negative coefficients')\nplt.show()","9b8fda00":"# Feature selection\nfrom sklearn.feature_selection import RFECV\n\nsteps = 20\nn_features = len(X_train.columns)\nX_range = np.arange(n_features - (int(n_features\/steps)) * steps, n_features+1, steps)\n\nrfecv = RFECV(estimator=lr, step=steps, cv=cv, scoring='f1')\n\npipeline2 = Pipeline([('scale',scaler), ('rfecv', rfecv)])\npipeline2.fit(X_train, y_train)\nplt.figure(figsize=(10,6))\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(np.insert(X_range, 0, 1), rfecv.grid_scores_)\nplt.show()","4c5b4ffd":"print ('Optimal no. of features: %d' % np.insert(X_range, 0, 1)[np.argmax(rfecv.grid_scores_)])","cb0c934e":"selected_features = X_train.columns[rfecv.ranking_ == 1]\nX_train2 = X_train[selected_features]\nX_test2 = X_test[selected_features]","6ab6d77b":"# lr2 = LogisticRegression(solver='liblinear', random_state=37)\npipeline.fit(X_train2, y_train)\ncv2 = ShuffleSplit(n_splits=5, test_size=0.2, random_state=456)\ncv_score2 = cross_val_score(pipeline, X_train2, y_train, cv=cv2, scoring='f1')\nprint('Cross validation F-1 score: %.3f' %np.mean(cv_score2))","5ea63177":"from sklearn.model_selection import GridSearchCV\n\ngrid={\"C\":np.logspace(-2,2,5), \"penalty\":[\"l1\",\"l2\"]}\nlr_cv = GridSearchCV(LogisticRegression(solver='liblinear', random_state=20), grid, cv=cv2, scoring = 'f1')\n\npipeline_grid = Pipeline([('scale',scaler), ('gridsearch', lr_cv),])\n\npipeline_grid.fit(X_train2, y_train)\n\nprint(\"Best parameter: \", lr_cv.best_params_)\nprint(\"F-1 score: %.3f\" %lr_cv.best_score_)","895d0d6b":"# Submit fine-tuned model\n\ny_test2 = pipeline_grid.predict(X_test2)\nsubmit2 = sub_sample.copy()\nsubmit2.target = y_test2\nsubmit2.to_csv('submission2.csv',index=False)","988ed3dc":"# Top features with fine-tuned model\nplt.figure(figsize=(16,7))\ns1 = pd.Series(np.transpose(lr.coef_[0]), index=X_train2.columns).sort_values(ascending=False)[:20]\ns2 = pd.Series(np.transpose(lr.coef_[0]), index=X_train2.columns).sort_values()[:20]\nplt.subplot(121)\nsns.barplot(y=s1.index, x=s1)\nplt.title('Top positive coefficients')\nplt.subplot(122)\nsns.barplot(y=s2.index, x=s2)\nplt.title('Top negative coefficients')\nplt.show()","836f51a0":"# Error analysis\ny_hat = pipeline_grid.predict_proba(X_train2)[:,1]\nchecker = train.loc[:,['text','keyword','location','target']]\nchecker['pred_prob'] = y_hat\nchecker['error'] = np.abs(checker['target'] - checker['pred_prob'])\n\n# Top 50 mispredicted tweets\nerror50 = checker.sort_values('error', ascending=False).head(50)\nerror50 = error50.rename_axis('id').reset_index()\nerror50.target.value_counts()","14bf5f51":"pd.options.display.max_colwidth = 200\n\nerror50.loc[0:10,['text','target','pred_prob']]","fd233cc6":"## <a id='encode'>8. Encoding and Vectorizers<\/a>\n\nAs part of feature generation, we will:\n- Apply target encoding to keyword and location (cleaned)\n- Count Vectorize cleaned text, links, hashtags and mentions columns","cbf0b429":"# Confusion matrix","e3bba5cf":"This forms the basis of evaluating and modifying our models in the next section.","2c2d85a0":"## <a id='gram'>7. Most frequent words and bigrams<\/a>\n\nWhat are the most common unigrams (single word) and bigrams (two-word sequence)?","8a941131":"# Visualization with the class imbalance thing","626e8fa8":"# Datasets:\n\n**Datasets Used In This Notebook Are As Follows**\n\n1. [Real or Not? NLP with Disaster Tweets](http:\/\/www.kaggle.com\/c\/nlp-getting-started\/data)\n2. [Disasters on social media](http:\/\/www.kaggle.com\/jannesklaas\/disasters-on-social-media)\n\n\n\n","242fdd45":"# Hypertuning","874210e3":"# Cleaning the Data","a78537f6":"# Import Libraries\n","c1cc5b40":"## <a id='location'>3. Locations<\/a>","a134663c":"From the above evaluation we understand that Text is all non-null. Only a small percentage of tweets have no keyword. Location has much more null values.","ff3de994":"Train and test have the same set of keywords","a17df85c":"> **When we first started learning about NLP, we chose to write this kernel. It is essentially the stuff we have learned documented in Kaggle Notebook format.**\n> \n> **It can be helpful for you if you are looking for *data analysis on competition data, feature engineering ideas for NLP, cleaning and text processing ideas, and Logistic Regression*.**\n> \n","3c06c5d7":"### <a id='simple'>9. Logistic Regression<\/a>\n\nWe try the simplest model with logistic regression, based on all features we created above. Before we fit a model, we first transform the features into the same scale with minimum 0 and maximum 1. We do this in the form of pipeline.","946ef1bb":"Findings:\n- 'keyword_target' is the top positive coefficient, meaning the keyword column made a good feature\n- hiroshima both as text and hashtag made the top 20 positive coefficients\n- Punctuation count and stop word count are among top 20 negative coefficients\n- None of the bigrams made the top features","c0336dfa":"## <a id='improve'>10. Evaluate and Improve Our Model<\/a>\n\nSeveral things will be done:\n- Cross validation with shuffle split\n- Feature selections\n- Grid search for hyperparameters\n- Identify errors\n\nReference: [scoring parameters](https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter)","0106466d":"There is no common top 10 keywords between disaster and non-disaster tweets.","c22b00f7":"## <a id='stat'>6. Create statistics from texts<\/a>","e1fcab81":"# Competition Description\n\n**Twitter has become an important communication channel in times of emergency. The ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).**\n\n**But, it\u2019s not always clear whether a person\u2019s words are actually announcing a disaster.**","500ada06":"## <a id='text'>5. Clean up Text Column<\/a>\n\nHere we clean up the text column by:\n- Making a 'clean' text column, removing links and unnecessary white spaces\n- Creating separate columns containing lists of hashtags, mentions, and links","37e28db1":"# Data Preprocessing Part","c7581d21":"Findings:\n- Top two words in disaster tweets: 'fire' and 'news', don't make the top 20 on unreal disaster tweets.\n- Words are more specific for real disaster tweets (e.g. 'califonia', 'hiroshima', 'fire', 'police', 'suicide', 'bomb').","bd7b6087":"A little improvement from the earlier model","983be461":"Next, we inspect the tweets that predicted probability differs the most from target outcome","4f625058":"The top 3 locations with highest % of disaster tweets are **Mumbai, India, and Nigeria**. As the location data is not clean, we see some interesting cases, such as **'London, UK' saw a higher-than-average % of disaster tweets, but 'London' is below average**. We try to clean up the location and see if there is any difference:","ef57b263":"Among the top 50 mispredicted tweets, only 4 are false positive","573565d0":"Default hyperparameters gave the best score.","11135c6b":"# F1 SCORE","caffee7d":"# Real or Not? NLP with Disaster Tweets\ud83d\udd25\ud83d\udd25\n\n**Predict which Tweets are about real disasters and which ones are not**","b61a1cc7":"# TF-IDF Count","e31a44c6":"As location is free text, the data is not clean, you can see both 'USA' and 'United States' in top locations. We than have a look at % of disaster tweets for common locations.","827b2a40":"There are 0 duplicate rows.","86c1e388":"We then pick up the 1133 selected features to do Grid Search CV to find optimal hyperparameters","01c7739e":"# Output data for Submission","26795669":"We try to distinguish disaster and non-disaster tweets:","21ee09b2":"# Feature Engineering","3652d55a":"# Regression Technique","96f5cc8d":"**<span style=\"color:blue\">Please Upvote If You Like, Use Or Learn From This. If you have any idea that might improve this kernel, please be sure to comment, or fork and experiment as you like. If you don't understand any part, feel free to ask in the comment section.**\ud83d\ude4c\ud83d\ude4c\ud83d\ude4c","326666ba":"# <span style=\"color:blue\">**Please Upvote If You Like, Use Or Learn From This.**\n\n# <span style=\"color:blue\">**Also Comment For Improvisation Of The Notebook.**\n\n\n# <span style=\"color:blue\">**Thank You!!!**\n\n","5ccbfeac":"Mumbai and Nigeria are still on the top. Other than the strange 'ss', London and New York made the bottom of % of disaster tweets.","f8dc1c40":"Findings:\n- Most top bigrams in disaster tweets show certain kinds of catestrophe (e.g. suicide bomber, oil spill, california wildfire); for non-disaster tweets, only 'burning buildings' as top bigram look like a disaster;\n- Top bigrams in disaster tweets have a more casual tone;\n- 'youtube' appears in three of the twenty bigrams for non-disaster tweets; none in disaster tweets","be5d7d36":"## <a id='keyword'>2. Keywords<\/a>","90a0b31a":"All of the statistics have very low correlation with the target variable","2984bf80":"## <a id='basic'>1. Basic Exploration<\/a>"}}