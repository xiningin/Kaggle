{"cell_type":{"87632c18":"code","21648b03":"code","eee9ee83":"code","1e6a1d2b":"code","22d425ae":"code","66f8625b":"code","0a69c654":"code","e0aaa0dc":"code","9faabe46":"code","f0ee0fec":"code","419fd0a4":"code","10fe9d27":"code","e62b1b59":"code","11dc164a":"code","0d4c81e7":"code","9c06ed05":"code","203a5c57":"code","5b0288b6":"code","67baf444":"code","e51efdfa":"code","4c96b8c2":"code","c366017a":"code","4cc69607":"code","aea396bb":"code","b7784c38":"code","f9112c61":"code","43fde996":"code","7b9b4ee2":"code","4a7a8d41":"code","e0b5b7a3":"code","174f28aa":"code","b811e98e":"code","c5344a16":"code","6f67e498":"code","0214ff2e":"code","8a746cab":"code","c8bbdba9":"code","395ecbbb":"code","c7ad3a19":"code","ad6520ef":"code","df0d67b2":"code","d56a86a0":"code","0aa3c781":"markdown","da393209":"markdown","789d2bf0":"markdown","7f6d5c1c":"markdown","922c0c32":"markdown","a0706eb9":"markdown","562cf695":"markdown","0ad80f28":"markdown","9398528c":"markdown","ddcef573":"markdown","a53c1aff":"markdown","5edd9a44":"markdown","bfdaa7be":"markdown","0ca9960b":"markdown","f39102a5":"markdown","985b75ea":"markdown","b081ac5f":"markdown","7c14e717":"markdown","14253872":"markdown"},"source":{"87632c18":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import svm\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier,GradientBoostingClassifier, VotingClassifier\nfrom sklearn.naive_bayes import GaussianNB","21648b03":"train=pd.read_csv('..\/input\/carInsurance_train.csv')\ntest=pd.read_csv('..\/input\/carInsurance_test.csv')","eee9ee83":"print('The train dataset has %d observations and %d features' % (train.shape[0], train.shape[1]))\nprint('The test dataset has %d observations and %d features' % (test.shape[0], test.shape[1]))","1e6a1d2b":"# First check out correlations among numeric features\n# Heatmap is a useful tool to get a quick understanding of which variables are important\ncolormap = plt.cm.viridis\ncor = train.corr()\ncor = cor.drop(['Id'],axis=1).drop(['Id'],axis=0)\nplt.figure(figsize=(12,12))\nsns.heatmap(cor,vmax=0.8,cmap=colormap,annot=True,fmt='.2f',square=True,annot_kws={'size':10},linecolor='white',linewidths=0.1)","22d425ae":"imp_feats = ['CarInsurance','Age','Balance','HHInsurance', 'CarLoan','NoOfContacts','DaysPassed','PrevAttempts']\nsns.pairplot(train[imp_feats],hue='CarInsurance',palette='viridis',size=2.5)\nplt.show()","66f8625b":"# Next check out categorical features\ncat_feats = train.select_dtypes(include=['object']).columns\nplt_feats = cat_feats[(cat_feats!= 'CallStart') & (cat_feats!='CallEnd')]\n\nfor feature in plt_feats:\n    plt.figure(figsize=(10,6))\n    sns.barplot(feature,'CarInsurance', data=train,palette='Set2')","0a69c654":"# Check outliers\n# From the pairplot, we can see there is an outlier with extreme high balance. Drop that obs here.\ntrain[train['Balance']>80000]\ntrain = train.drop(train[train.index==1742].index)","e0aaa0dc":"# merge train and test data here in order to impute missing values all at once\nall=pd.concat([train,test],keys=('train','test'))\nall.drop(['CarInsurance','Id'],axis=1,inplace=True)\nprint(all.shape)\n","9faabe46":"total = all.isnull().sum()\npct = total\/all.isnull().count()\nNAs = pd.concat([total,pct],axis=1,keys=('Total','Pct'))\nNAs[NAs.Total>0].sort_values(by='Total',ascending=False)","f0ee0fec":"all_df = all.copy()\n\n# Fill missing outcome as not in previous campaign\nall_df[all_df['DaysPassed']==-1].count()\nall_df.loc[all_df['DaysPassed']==-1,'Outcome']='NoPrev'\n\n# Fill missing communication with none \nall_df['Communication'].value_counts()\nall_df['Communication'].fillna('None',inplace=True)\n\nall_df['Education'].value_counts()\n\n","419fd0a4":"# Create job-education level mode mapping\nedu_mode=[]\njob_types = all_df.Job.value_counts().index\nfor job in job_types:\n    mode = all_df[all_df.Job==job]['Education'].value_counts().nlargest(1).index\n    edu_mode = np.append(edu_mode,mode)\nedu_map=pd.Series(edu_mode,index=all_df.Job.value_counts().index)\nedu_map","10fe9d27":"#  Apply the mapping to missing eductaion obs\nfor j in job_types:\n    all_df.loc[(all_df['Education'].isnull()) & (all_df['Job']==j),'Education'] = edu_map.loc[edu_map.index==j][0]\nall_df['Education'].fillna('None',inplace=True)\n\n# Fill missing job with none\nall_df['Job'].fillna('None',inplace=True)\n\n# Double check if there is still any missing value\nall_df.isnull().sum().sum()","e62b1b59":"# First simplify some client features\n\n# Create age group based on age bands\nall_df['AgeBand']=pd.cut(all_df['Age'],5)\nprint(all_df['AgeBand'].value_counts())","11dc164a":"all_df.loc[(all_df['Age']>=17) & (all_df['Age']<34),'AgeBin'] = 1\nall_df.loc[(all_df['Age']>=34) & (all_df['Age']<49),'AgeBin'] = 2\nall_df.loc[(all_df['Age']>=49) & (all_df['Age']<65),'AgeBin'] = 3\nall_df.loc[(all_df['Age']>=65) & (all_df['Age']<80),'AgeBin'] = 4\nall_df.loc[(all_df['Age']>=80) & (all_df['Age']<96),'AgeBin'] = 5\nall_df['AgeBin'] = all_df['AgeBin'].astype(int)","0d4c81e7":"# Create balance groups\nall_df['BalanceBand']=pd.cut(all_df['Balance'],5)\nprint(all_df['BalanceBand'].value_counts())\nall_df.loc[(all_df['Balance']>=-3200) & (all_df['Balance']<17237),'BalanceBin'] = 1\nall_df.loc[(all_df['Balance']>=17237) & (all_df['Balance']<37532),'BalanceBin'] = 2\nall_df.loc[(all_df['Balance']>=37532) & (all_df['Balance']<57827),'BalanceBin'] = 3\nall_df.loc[(all_df['Balance']>=57827) & (all_df['Balance']<78122),'BalanceBin'] = 4\nall_df.loc[(all_df['Balance']>=78122) & (all_df['Balance']<98418),'BalanceBin'] = 5\nall_df['BalanceBin'] = all_df['BalanceBin'].astype(int)","9c06ed05":"all_df = all_df.drop(['AgeBand','BalanceBand','Age','Balance'],axis=1)","203a5c57":"#  Convert education level to numeric \nall_df['Education'] = all_df['Education'].replace({'None':0,'primary':1,'secondary':2,'tertiary':3})","5b0288b6":"# Next create some new communication Features. This is the place feature engineering coming into play\n\n# Get call length\nall_df['CallEnd'] = pd.to_datetime(all_df['CallEnd'])\nall_df['CallStart'] = pd.to_datetime(all_df['CallStart'])\nall_df['CallLength'] = ((all_df['CallEnd'] - all_df['CallStart'])\/np.timedelta64(1,'m')).astype(float)\n","67baf444":"all_df['CallLenBand']=pd.cut(all_df['CallLength'],5)\nprint(all_df['CallLenBand'].value_counts())\n\n# Create call length bins\nall_df.loc[(all_df['CallLength']>= 0) & (all_df['CallLength']<11),'CallLengthBin'] = 1\nall_df.loc[(all_df['CallLength']>=11) & (all_df['CallLength']<22),'CallLengthBin'] = 2\nall_df.loc[(all_df['CallLength']>=22) & (all_df['CallLength']<33),'CallLengthBin'] = 3\nall_df.loc[(all_df['CallLength']>=33) & (all_df['CallLength']<44),'CallLengthBin'] = 4\nall_df.loc[(all_df['CallLength']>=44) & (all_df['CallLength']<55),'CallLengthBin'] = 5\nall_df['CallLengthBin'] = all_df['CallLengthBin'].astype(int)\nall_df = all_df.drop('CallLenBand',axis=1)","e51efdfa":"# Get call start hour\nall_df['CallStartHour'] = all_df['CallStart'].dt.hour\nall_df[['CallStart','CallEnd','CallLength','CallStartHour']].head()","4c96b8c2":"# Get workday of last contact based on call day and month, assuming the year is 2016\nall_df['LastContactDate'] = all_df.apply(lambda x:datetime.datetime.strptime(\"%s %s %s\" %(2018,x['LastContactMonth'],x['LastContactDay']),\"%Y %b %d\"),axis=1)\nall_df['LastContactWkd'] = all_df['LastContactDate'].dt.weekday\nall_df['LastContactWkd'].value_counts()\nall_df['LastContactMon'] = all_df['LastContactDate'].dt.month\nall_df = all_df.drop('LastContactMonth',axis=1)","c366017a":"# Get week of last contact\nall_df['LastContactWk'] = all_df['LastContactDate'].dt.week\nMonWk = all_df.groupby(['LastContactWk','LastContactMon'])['Education'].count().reset_index()\n","4cc69607":"MonWk = MonWk.drop('Education',axis=1)\nMonWk['LastContactWkNum']=0\nfor m in range(1,13):\n    k=0\n    for i,row in MonWk.iterrows():\n        if row['LastContactMon']== m:\n            k=k+1\n            row['LastContactWkNum']=k","aea396bb":"def get_num_of_week(df):\n    for i,row in MonWk.iterrows():\n        if (df['LastContactWk']== row['LastContactWk']) & (df['LastContactMon']== row['LastContactMon']):\n            return row['LastContactWkNum']\n\nall_df['LastContactWkNum'] = all_df.apply(lambda x: get_num_of_week(x),axis=1)\nall_df[['LastContactWkNum','LastContactWk','LastContactMon']].head(10)","b7784c38":"# Spilt numeric and categorical features\ncat_feats = all_df.select_dtypes(include=['object']).columns\nnum_feats = all_df.select_dtypes(include=['float64','int64']).columns\nnum_df = all_df[num_feats]\ncat_df = all_df[cat_feats]\nprint('There are %d numeric features and %d categorical features\\n' %(len(num_feats),len(cat_feats)))\nprint('Numeric features:\\n',num_feats.values)\nprint('Categorical features:\\n',cat_feats.values)","f9112c61":"cat_df = pd.get_dummies(cat_df)\nall_data = pd.concat([num_df,cat_df],axis=1)\n","43fde996":"# Split train and test\nidx=pd.IndexSlice\ntrain_df=all_data.loc[idx[['train',],:]]\ntest_df=all_data.loc[idx[['test',],:]]\ntrain_label=train['CarInsurance']\nprint(train_df.shape)\nprint(len(train_label))\nprint(test_df.shape)","7b9b4ee2":"# Train test split\nx_train, x_test, y_train, y_test = train_test_split(train_df,train_label,test_size = 0.005,random_state=3)","4a7a8d41":"x_train.shape","e0b5b7a3":"# Create a cross validation function \ndef get_best_model(estimator, params_grid={}):\n    \n    model = GridSearchCV(estimator = estimator,param_grid = params_grid,cv=3, scoring=\"accuracy\", n_jobs= -1)\n    model.fit(x_train,y_train)\n    print('\\n--- Best Parameters -----------------------------')\n    print(model.best_params_)\n    print('\\n--- Best Model -----------------------------')\n    best_model = model.best_estimator_\n    print(best_model)\n    return best_model","174f28aa":"# Create a model fitting function\ndef model_fit(model,feature_imp=True,cv=5):\n\n    # model fit   \n    clf = model.fit(x_train,y_train)\n    \n    # model prediction     \n    y_pred = clf.predict(x_test)\n    \n    # model report     \n    cm = confusion_matrix(y_test,y_pred)\n    plot_confusion_matrix(cm, classes=class_names, title='Confusion matrix')\n\n    print('\\n--- Train Set -----------------------------')\n    print('Accuracy: %.5f +\/- %.4f' % (np.mean(cross_val_score(clf,x_train,y_train,cv=cv)),np.std(cross_val_score(clf,x_train,y_train,cv=cv))))\n    print('AUC: %.5f +\/- %.4f' % (np.mean(cross_val_score(clf,x_train,y_train,cv=cv,scoring='roc_auc')),np.std(cross_val_score(clf,x_train,y_train,cv=cv,scoring='roc_auc'))))\n    print('\\n--- Validation Set -----------------------------')    \n    print('Accuracy: %.5f +\/- %.4f' % (np.mean(cross_val_score(clf,x_test,y_test,cv=cv)),np.std(cross_val_score(clf,x_test,y_test,cv=cv))))\n    print('AUC: %.5f +\/- %.4f' % (np.mean(cross_val_score(clf,x_test,y_test,cv=cv,scoring='roc_auc')),np.std(cross_val_score(clf,x_test,y_test,cv=cv,scoring='roc_auc'))))\n    print('-----------------------------------------------') \n\n    # feature importance \n    if feature_imp:\n        feat_imp = pd.Series(clf.feature_importances_,index=all_data.columns)\n        feat_imp = feat_imp.nlargest(15).sort_values()\n        plt.figure()\n        feat_imp.plot(kind=\"barh\",figsize=(6,8),title=\"Most Important Features\")","b811e98e":"# The confusion matrix plotting function is from the sklearn documentation below:\n# http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\nclass_names = ['Success','Failure']","c5344a16":"# Let's start with KNN. An accuracy of 0.76 is not very impressive. I will just take this as the model benchmark. \nknn = KNeighborsClassifier()\nparameters = {'n_neighbors':[5,6,7], \n              'p':[1,2],\n              'weights':['uniform','distance']}\nclf_knn = get_best_model(knn,parameters)\nmodel_fit(model=clf_knn, feature_imp=False)","6f67e498":"# As expected, Naive Bayes classifier doesn't perform well here. \n# There are multiple reasons. Some of the numeric features are not normally distributed, which is a strong assemption hold by Naive Bayes. \n# Also, features are definitely not independent.  \nclf_nb = GaussianNB()\nmodel_fit(model=clf_nb,feature_imp=False)","0214ff2e":"# We're making progress here. Logistic regression performs better than KNN. \nlg = LogisticRegression(random_state=3)\nparameters = {'C':[0.8,0.9,1], \n              'penalty':['l1','l2']}\nclf_lg = get_best_model(lg,parameters)\nmodel_fit(model=clf_lg, feature_imp=False)","8a746cab":"# I did some manual parameter tuning here. This is the best model so far. \n# Based on the feature importance report, call length, last contact week, and previous success are strong predictors of cold call success\nrf = RandomForestClassifier(random_state=3)\nparameters={'n_estimators':[100],\n            'max_depth':[10],\n            'max_features':[13,14],\n            'min_samples_split':[11]}\nclf_rf= get_best_model(rf,parameters)\nmodel_fit(model=clf_rf, feature_imp=True)","c8bbdba9":"# try a SVM RBF model \nsvc = svm.SVC(kernel='rbf', probability=True, random_state=3)\nparameters = {'gamma': [0.005,0.01,0.02],\n              'C': [0.5,1,5]}\nclf_svc = get_best_model(svc, parameters)\nmodel_fit(model=clf_svc,feature_imp=False)","395ecbbb":"# Finally let's try out XBGoost. As expected, it outperforms all other algorithms. \n# Also, based on feature importances, some of the newly created features such as call start hour, last contact week and weekday \n# have been picked as top features. \n\nimport xgboost as xgb\nxgb = xgb.XGBClassifier()\nparameters={'n_estimators':[900,1000,1100],\n            'learning_rate':[0.01],\n            'max_depth':[8],\n            'min_child_weight':[1],\n            'subsample':[0.8],\n            'colsample_bytree':[0.3,0.4,0.5]}\nclf_xgb= get_best_model(xgb,parameters)\nmodel_fit(model=clf_xgb, feature_imp=True)","c7ad3a19":"# Compare model performance\nclfs= [clf_knn, clf_nb, clf_lg, clf_rf, clf_svc, clf_xgb]\nindex =['K-Nearest Neighbors','Naive Bayes','Logistic Regression','Random Forest','Support Vector Machines','XGBoost']\nscores=[]\nfor clf in clfs:\n    score = np.mean(cross_val_score(clf,x_test,y_test,cv=5,scoring = 'accuracy'))\n    scores = np.append(scores,score)\nmodels = pd.Series(scores,index=index)\nmodels.sort_values(ascending=False)","ad6520ef":"#XGBoost and Random Forest show different important features, implying that those models are capturing different aspects of the data\n# To get the final model, I ensembled different classifiers based on majority voting.\n# XGBoost and Random Forest are given larger weights due to their better performance. \n\nclf_vc = VotingClassifier(estimators=[('xgb', clf_xgb),                                       \n                                      ('rf', clf_rf),\n                                      ('lg', clf_lg), \n                                      ('svc', clf_svc)], \n                          voting='hard',\n                          weights=[4,4,1,1])\nclf_vc = clf_vc.fit(x_train, y_train)","df0d67b2":"clf_xgb = clf_xgb.fit(x_train, y_train)","d56a86a0":"print('Final Model Accuracy: %.5f'%(accuracy_score(y_test, clf_xgb.predict(x_test))))\n","0aa3c781":"# Modeling","da393209":"## Support Vector Machines","789d2bf0":"# Import Libraries","7f6d5c1c":"# Ensemble Voting","922c0c32":"# Model Evaluation\u00b6\n","a0706eb9":"# Data Exploration & Visualization\u00b6","562cf695":"## XGBoost","0ad80f28":"## Naive Bayes Classifier","9398528c":"Features are fairly independent, except DaysPassed and PreAttempts. Cold call success is positively correlated with PreAttemps,DaysPassed,Age and Balance, and negatively correlated with default, HHInsurance, CarLoan, LastContactDay and NoOfContacts.","ddcef573":"Age: It's interesting to see that seniors are more likely to buy car insurance.\n\nBalance: For balance, the data point at the upper right corner might be an outlier \n\nHHInsurance: Households insured are less likely to buy car insurance \n\nCarLoan: People with car loan are less likely to buy \n\nNoOfContacts: Too many contacts causes customer attrition \n\nDaysPassed: It looks like the more day passed since the last contact, the better \n\nPrevAttempts: Also, more previous attempts, less likely to buy. There is a potential outlier here","a53c1aff":"## k-Nearest Neighbors (KNN)","5edd9a44":"# Cold Calls","bfdaa7be":"Job: Student are most likely to buy insurance, followed by retired and unemployed folks.This is aligned with the age distribution. There might be some promotion targeting students? \n\nMarital status: Married people are least likely to buy car insurance. Opportunities for developing family insurance business\n\nEducation: People with higher education are more likely to buy \n\nCommunication: No big difference between cellular and telephone \n\nOutcome in previous campaign: Success in previous marketing campaign is largely associated with success in this campaign \n\nContact Month: Mar, Sep, Oct, and Dec are the hot months. It might be associated with school season?","0ca9960b":"## Logistic Regression","f39102a5":"### Handling Miss Data\n","985b75ea":"There are three types of features: \n\nClient features: Age, Job, Marital, Education, Default, Balance, HHInsurance, CarLoan \n\nCommunication features: LastContactDay, LastContactMonth, CallStart, CallEnd, Communication, NoOfContacts, DaysPassed \n\nPrevious campaign features: PrevAttempts, Outcome","b081ac5f":"# Feature Engineering\n","7c14e717":"## Random Forest","14253872":"# Import Data"}}