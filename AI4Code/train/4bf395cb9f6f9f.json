{"cell_type":{"8dc049a9":"code","65131d71":"code","216c8185":"code","aa46b061":"code","4fde80d3":"code","efe03812":"code","5e9e9487":"code","3b4b565c":"code","ab3197fc":"code","31fe5045":"code","eafe788a":"code","a8ae9659":"code","d4ce50f4":"code","880968dd":"code","be2d03e3":"code","106a42e6":"code","1113a1f7":"code","63c322d3":"code","7a8ee4e5":"code","50a6a24e":"code","102a62d6":"code","42f18d0c":"code","1a5a072b":"code","8ebaacc2":"code","0c1025f5":"code","8883ec5f":"code","f2bcb1f4":"code","d28abd15":"code","1c4430ac":"code","86911518":"code","6867891b":"code","f4e1f211":"code","235045ce":"code","c514319a":"code","69e9515e":"code","89546e7d":"code","086edbef":"code","6eb1f088":"code","35dd0e03":"code","3369af02":"code","c53e66e8":"code","85a37be9":"code","54bee28d":"code","3ad86cdd":"code","46e1a7da":"code","773741df":"code","2e6551f1":"code","82c83087":"code","3f799b3b":"code","11308ad1":"code","ed054380":"code","1195a619":"code","d4505576":"code","9b6e6461":"code","495e1b3c":"code","39c5ad4c":"code","159bd126":"code","d0996a07":"markdown","0e508b40":"markdown","67b682d1":"markdown","295863cb":"markdown","a755b72a":"markdown","82001eca":"markdown","8221391a":"markdown","e8ef2974":"markdown","0f096601":"markdown","e26f88fd":"markdown","aae7d180":"markdown","93575ee7":"markdown","5275d982":"markdown","c4e74a9a":"markdown"},"source":{"8dc049a9":"# Import relevant libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\npd.set_option('max_columns', None)","65131d71":"# Load the dataset\n\ndataset = pd.read_csv('..\/input\/credit-card-customers\/BankChurners.csv')","216c8185":"# Take a first glimpse at the data\n\ndataset.head()","aa46b061":"# Get rid of the last two columns because I don't need them\n\ndataset = dataset.iloc[:,:-2]","4fde80d3":"# Check for missing values\n\ndataset.isna().sum()","efe03812":"# Explore the variables\n\ndataset.describe(include =  'all')","5e9e9487":"dataset['Education_Level'].value_counts()","3b4b565c":"dataset['Marital_Status'].value_counts()","ab3197fc":"dataset['Income_Category'].value_counts()","31fe5045":"dataset['Card_Category'].value_counts()","eafe788a":"dataset['Customer_Age'].plot(kind = 'hist', figsize = (10, 8))\nplt.title(\"Customer Age\", size = 20)\nplt.show()","a8ae9659":"dataset['Dependent_count'].plot(kind = 'hist', figsize = (10, 8))\nplt.title(\"Dependent Count\", size = 20)\nplt.show()","d4ce50f4":"dataset['Months_on_book'].plot(kind = 'hist', figsize = (10, 8))\nplt.title(\"Months on book\", size = 20)\nplt.show()","880968dd":"dataset['Total_Relationship_Count'].plot(kind = 'hist', figsize = (10, 8))\nplt.title(\"Total Relationship Count\", size = 20)\nplt.show()","be2d03e3":"dataset['Months_Inactive_12_mon'].plot(kind = 'hist', figsize = (10, 8))\nplt.title(\"Months Inactive in the last 12 months\", size = 20)\nplt.show()","106a42e6":"dataset['Contacts_Count_12_mon'].plot(kind = 'hist', figsize = (10, 8))\nplt.title(\"Contacts in the last 12 months\", size = 20)\nplt.show()","1113a1f7":"dataset['Credit_Limit'].plot(kind = 'hist', figsize = (10, 8))\nplt.title(\"Credit Limit\", size = 20)\nplt.show()","63c322d3":"dataset['Total_Revolving_Bal'].plot(kind = 'hist', figsize = (10, 8))\nplt.title(\"Total Revolving Balance\", size = 20)\nplt.show()","7a8ee4e5":"dataset['Avg_Open_To_Buy'].plot(kind = 'hist', figsize = (10, 8))\nplt.title(\"Open to Buy Credit (Avg. Last 12 months)\", size = 20)\nplt.show()","50a6a24e":"dataset['Total_Amt_Chng_Q4_Q1'].plot(kind = 'hist', figsize = (10, 8))\nplt.title(\"Change in Transaction Amount (Q4 over Q1))\", size = 20)\nplt.show()","102a62d6":"dataset['Total_Trans_Amt'].plot(kind = 'hist', figsize = (10, 8))\nplt.title(\"Total Transaction Amount\", size = 20)\nplt.show()","42f18d0c":"dataset['Total_Trans_Ct'].plot(kind = 'hist', figsize = (10, 8))\nplt.title(\"Total Transaction Count\", size = 20)\nplt.show()","1a5a072b":"dataset['Total_Ct_Chng_Q4_Q1'].plot(kind = 'hist', figsize = (10, 8))\nplt.title(\"Change in Transaction Count (Q4 over Q1)\", size = 20)\nplt.show()","8ebaacc2":"dataset['Avg_Utilization_Ratio'].plot(kind = 'hist', figsize = (10, 8))\nplt.title(\"Avg. Card Utiliation\", size = 20)\nplt.show()","0c1025f5":"X = dataset.iloc[:,2:]","8883ec5f":"y = dataset.iloc[:,1]","f2bcb1f4":"# Split the data into train and test sets \n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)","d28abd15":"# Encode the response variables to 0s and 1s\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny_train = le.fit_transform(y_train)\ny_test = le.fit_transform(y_test)\nprint(y_train)\nprint(y_test)","1c4430ac":"# Perform feature scaling to the continuous variables \n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train.iloc[:,[0,2,7,8,9,10,11,12,13,14,15,16,17,18]] = sc.fit_transform(X_train.iloc[:,[0,2,7,8,9,10,11,12,13,14,15,16,17,18]])\nX_test.iloc[:,[0,2,7,8,9,10,11,12,13,14,15,16,17,18]] = sc.transform(X_test.iloc[:,[0,2,7,8,9,10,11,12,13,14,15,16,17,18]])","86911518":"# Turn the categorical variables into dummy variables \n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1, 3, 4, 5, 6])], remainder='passthrough')\nX_train = np.array(ct.fit_transform(X_train))\nX_test = np.array(ct.fit_transform(X_test))","6867891b":"# Train the model\n\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train, y_train)","f4e1f211":"# Test the model\n\ny_pred = classifier.predict(X_test)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","235045ce":"# Build the confusion matrix\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","c514319a":"# Train the model\n\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nclassifier.fit(X_train, y_train)","69e9515e":"# Test the model\n\ny_pred = classifier.predict(X_test)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","89546e7d":"# Build the confusion matrix\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","086edbef":"# Train the model\n\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel = 'linear', random_state = 0)\nclassifier.fit(X_train, y_train)","6eb1f088":"# Test the model\n\ny_pred = classifier.predict(X_test)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","35dd0e03":"# Build the confusion matrix\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","3369af02":"# Train the model\n\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)","c53e66e8":"# Test the model\n\ny_pred = classifier.predict(X_test)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","85a37be9":"# Build the confusion matrix\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","54bee28d":"# Train the model\n\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)","3ad86cdd":"# Test the model\n\ny_pred = classifier.predict(X_test)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","46e1a7da":"# Build the confusion matrix\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","773741df":"# Train the model\n\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)","2e6551f1":"# Test the model\n\ny_pred = classifier.predict(X_test)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","82c83087":"# Build the confusion matrix\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","3f799b3b":"# Import tensorflow\n\nimport tensorflow as tf","11308ad1":"# Initialzing the ANN \n\nann = tf.keras.models.Sequential()\n\n# Adding the input layer and the first hidden layer\n\nann.add(tf.keras.layers.Dense(units=12, activation='relu'))\n\n# Adding the second hidden layer\n\nann.add(tf.keras.layers.Dense(units=12, activation='relu'))\n\n# Adding the output layer\n\nann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))","ed054380":"# Compile the ANN\n\nann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","1195a619":"# Train the ANN\n\nann.fit(X_train, y_train, batch_size = 32, epochs = 100)","d4505576":"# Test the model\n\ny_pred = ann.predict(X_test)\ny_pred = (y_pred > 0.5)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","9b6e6461":"# Build the confusion matrix\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","495e1b3c":"# Train the model\n\nfrom xgboost import XGBClassifier\nclassifier = XGBClassifier()\nclassifier.fit(X_train, y_train)","39c5ad4c":"# Test the model\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","159bd126":"# Apply k-fold cross validation\n\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","d0996a07":"## Preparing the data for analysis","0e508b40":"##### It looks like the best performing model is the XGBoost with a 97.16% accuracy score","67b682d1":"### Naive Bayes","295863cb":"### XGBoost","a755b72a":"## Starting with some EDA","82001eca":"### Random Forest","8221391a":"### K-NN","e8ef2974":"### Logistic Regression","0f096601":"##### The data looks pretty normal, there are no outliers or erroneous entries","e26f88fd":"## Test various models to see which one performs best","aae7d180":"### ANN","93575ee7":"### Decision Tree","5275d982":"Business Case Description\n\nA manager at the bank is disturbed with more and more customers leaving their credit card services. They would really appreciate if one could predict for them who is gonna get churned so they can proactively go to the customer to provide them better services and turn customers' decisions in the opposite direction","c4e74a9a":"### SVM"}}