{"cell_type":{"b6388d72":"code","a1f4fd66":"code","86ad9c97":"code","b912ca17":"code","ab74efe0":"code","0f1d6184":"code","a3702b87":"code","befcba1a":"code","f9e49879":"code","3e0777f2":"code","4ec07a46":"code","ff06e271":"code","2d40403c":"code","c8c42f8f":"code","f9848335":"code","474b5c84":"code","142cec59":"code","41f65b2a":"code","77bfddc0":"markdown","8a40cee7":"markdown","06aa0b59":"markdown","298be8cd":"markdown"},"source":{"b6388d72":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","a1f4fd66":"train = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","86ad9c97":"train.isnull().sum()","b912ca17":"train[\"length\"] = train[\"text\"].apply(len)","ab74efe0":"sns.countplot(x = \"target\",data = train,palette=\"icefire\")\nplt.title('Label Counts')\nplt.show()","0f1d6184":"sns.barplot(x = \"target\", y = \"length\", data = train, palette=\"icefire\")\nplt.title(\"Avg. length of each target\")\nplt.show()","a3702b87":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\nsns.histplot(train[train[\"target\"] == 1][\"length\"],bins = 30,ax = ax1, kde=True).set(title = \"disaster tweets\")\nsns.histplot(train[train[\"target\"] == 0][\"length\"],bins = 30,ax = ax2, kde = True).set(title = \"Not disaster tweets\")\nplt.show()","befcba1a":"import string\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nimport re\nfrom nltk.corpus import stopwords\n\nlemma = WordNetLemmatizer()\ndef process_text(text):\n    text = re.sub(\"(@[A-Za-z0-9_]+)|([^0-9A-Za-z \\t])\", \" \",text.lower())\n    words = nltk.word_tokenize(text)\n    words = [lemma.lemmatize(word) for word in words if word not in set(stopwords.words(\"english\"))]\n    text = \" \".join(words)\n        \n    return text\n\ntrain[\"text\"] = train[\"text\"].apply(process_text)","f9e49879":"import string\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nimport re\nfrom nltk.corpus import stopwords\n\nlemma = WordNetLemmatizer()\ndef process_text(text):\n    text = re.sub(\"(@[A-Za-z0-9_]+)|([^0-9A-Za-z \\t])\", \" \",text.lower())\n    words = nltk.word_tokenize(text)\n    words = [lemma.lemmatize(word) for word in words if word not in set(stopwords.words(\"english\"))]\n    text = \" \".join(words)\n        \n    return text\n\ntest[\"text\"] = test[\"text\"].apply(process_text)","3e0777f2":"from os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","4ec07a46":"text = \" \".join(review for review in train.text)\nwordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"black\").generate(text)\nfig = plt.figure(figsize = (10, 10)) \nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"To Create Cloud of words for all words in train data\")\nplt.show()","ff06e271":"text = \" \".join(review for review in test.text)\nwordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"black\").generate(text)\nfig = plt.figure(figsize = (10, 10)) \nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"To Create Cloud of words for all words in test data\")\nplt.show()","2d40403c":"from sklearn.model_selection import train_test_split\nX, y = train['text'], train['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","c8c42f8f":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(ngram_range=(1,2))\nX_train_vec = vectorizer.fit_transform(X_train).toarray()\nX_test_vec = vectorizer.transform(X_test).toarray()","f9848335":"\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier,BaggingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score\n\nclf_A = LogisticRegression()\nclf_B = AdaBoostClassifier()\nclf_C = DecisionTreeClassifier()\nclf_D = SVC()\nclf_E = RandomForestClassifier()\nclf_F = MultinomialNB()\nclfs = [clf_A,clf_B,clf_C,clf_D,clf_E,clf_F]","474b5c84":"df_score = pd.DataFrame(index=None, columns=['model','f1_score'])\nfor clf in clfs:\n    clf.fit(X_train_vec, y_train)\n    pred = clf.predict(X_test_vec)\n    score3 = f1_score(y_test,pred)\n  \n    df_score = df_score.append(pd.Series({\n                \"model\" : clf.__class__.__name__,\n                \"f1_score\" : score3}),ignore_index = True)\n\ndf_score","142cec59":"test_vec = vectorizer.transform(test['text']).toarray()\npredictions = clf.predict(test_vec)","41f65b2a":"submission = pd.DataFrame(predictions, columns=['target'])\nsubmission['id'] = test['id']\nsubmission.set_index('id', inplace=True)\n\nsubmission.to_csv('submission.csv')","77bfddc0":"## Natural Language Processing with Disaster Tweets","8a40cee7":"**Please upvote and share if this helps you!! Also, feel free to fork this kernel to play around with the code and test it for yourself.**","06aa0b59":"In this competition, you\u2019re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one\u2019s aren\u2019t.","298be8cd":"**The distribution of both seems to be almost same.120 to 140 characters in a tweet are the most common among both.**"}}