{"cell_type":{"38657c16":"code","13cc73a3":"code","78811a1e":"code","ac08810b":"code","992024f6":"code","b72e09de":"code","a3afbe38":"code","3afac052":"code","64eb5981":"code","b2f42208":"code","0223757f":"code","46cd0467":"code","c69824ad":"code","e2c4af46":"code","311e321b":"code","b82741d2":"markdown","ddc12437":"markdown","3b834470":"markdown","44734914":"markdown","834a9db9":"markdown","cd306290":"markdown","f5672985":"markdown"},"source":{"38657c16":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport category_encoders\nfrom sklearn.model_selection import train_test_split, cross_validate, KFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import metrics\nfrom hyperopt import hp, tpe, fmin, space_eval\nimport os\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, MaxAbsScaler\nfrom sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\nfrom sklearn.cluster import FeatureAgglomeration\n\nnp.random.seed(123)\n","13cc73a3":"train = pd.read_csv(os.path.join('..', 'input', 'train.csv'), index_col='ID')\ntrain.head()","78811a1e":"test = pd.read_csv(os.path.join('..', 'input', 'test.csv'), index_col='ID')\ntest.head()","ac08810b":"train.info()","992024f6":"def test_model(x_train, x_test, y_train, y_test, model):\n    \"\"\" fit the model and print the train and test result \"\"\"\n    np.random.seed(1)\n    model.fit(x_train, y_train)\n    print('train score: ', model.score(x_train, y_train))\n    print('test score: ', model.score(x_test, y_test))","b72e09de":"# Split to X and y and then to train and test sets:\nX = train.drop('y', axis=1)\ny = train['y']\nx_train, x_test, y_train, y_test = train_test_split(X, y)","a3afbe38":"# One hot encoding to the categorical columns in the data:\none_hot = category_encoders.OneHotEncoder(cols=['X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X8'], drop_invariant=True, use_cat_names=True)\nx_train_one_hot = one_hot.fit_transform(x_train)\nx_test_one_hot = one_hot.transform(x_test)","3afac052":"test_model(x_train_one_hot, x_test_one_hot, y_train, y_test, model=SVR())","64eb5981":"def get_model(args):\n    \"\"\"Construct the mode based on the args choosen in the current step of the bayesian optimization process\"\"\"\n    feature_selector = args['selection']\n        \n    model = Pipeline([\n        ('scaler', args['scaler']()),\n        ('selection', feature_selector['selection_algo'](**feature_selector['selection_params'])),\n        ('clf', args['clf'](**args['clf_params']))\n    ])\n\n    return model","b2f42208":"def objective_func(args, x_train=x_train_one_hot, y_train=y_train):\n    \"\"\"\n    Run a cross validation on the train data and return the mean test score.\n    This function output will be value the bayesian optimization process will try to minimize.\n    \"\"\"\n    np.random.seed(123)\n    model = get_model(args)\n\n    cv_results = cross_validate(estimator=model, X=x_train, y=y_train, n_jobs=-1, scoring='r2',\n                                cv=KFold(n_splits=4))\n    return - cv_results['test_score'].mean() # minus is because we optimize to the minimum","0223757f":"search_space = {\n    'scaler': hp.choice('scaler', [StandardScaler, RobustScaler, MinMaxScaler, MaxAbsScaler]),\n    'selection':  hp.choice('selection',[\n        {\n        'selection_algo': SelectKBest,\n        'selection_params': \n            {\n            'k': hp.choice('k', ['all'] + list(range(1, x_train_one_hot.shape[1]))),\n            'score_func': hp.choice('score_func', [f_regression, mutual_info_regression])\n            }\n        },\n        {\n            'selection_algo': PCA,\n            'selection_params': {'n_components': hp.uniformint('n_components', 1, x_train_one_hot.shape[1])}\n        },\n        {\n            'selection_algo': FeatureAgglomeration,\n            'selection_params': {'n_clusters': hp.uniformint('n_clusters', 1, x_train_one_hot.shape[1])}\n        }\n    ]),\n\n    'clf': SVR,\n    'clf_params': \n        {\n            'kernel': hp.choice('kernel', ['rbf', 'poly', 'linear']),\n            'C': hp. uniform('C', 0.0001, 30)\n        }\n\n}","46cd0467":"np.random.seed(123)\nbest_space = fmin(objective_func, space=search_space, algo=tpe.suggest, max_evals=100)\nbest_model =  get_model(space_eval(search_space, best_space))\nprint(best_model)","c69824ad":"space_eval(search_space, best_space)","e2c4af46":"test_model(x_train_one_hot, x_test_one_hot, y_train, y_test, model=best_model)","311e321b":"# Run on the real test\n# X_one_hot = one_hot.fit_transform(X)\n# test_one_hot = one_hot.transform(test)\n\n# best_model.fit(X_one_hot, y)\n# pd.DataFrame({'ID':test.index, 'y': best_model.predict(test_one_hot)}).to_csv(r'subs.csv', index=False)","b82741d2":"#### Test a first inilized model for a baseline","ddc12437":"### Load the train data and the test data:","3b834470":"Our init model is not bad, lets do a simple hyperparams search","44734914":"One hot encoding to the categorical columns in the data:","834a9db9":"#### A few notes about the search space:\n- You need to specify for each parameter it's distribution.<br\/>I offen user uniformal distribution if I'm not sure which is the right distribution (**Do you know a better way? I'll be happy to learn, please leave a comment**\n- I'm considering the choise of which data scaler to use as a hyperparameter\n- I assume I need some feature selection but I'm not sure which method will be the best.<br\/>So I have three different methods which have different params and all this will be considered as hyperparam as well.\n- There is more options and maybe better models to use ..","cd306290":"**Great** improvement only by searching some hyperparms (100 evaluations, which in my opinion is a low amount) .\n\nOf course a simple grid search would find the same params as well and if you are any lucky even random search would. But it would be a question of running time.<br\/>\nI believe that this bayesian way improves the random searching and offers a bit better searching method.","f5672985":"# How To Use Bayesian Optimization\nIn this kernel I'll try to demonstrate how easy it is to use 'hyperopt' to do hyperparams search using bayesian optimization.\nThis is not supposed to be an in-depth tutorial but more a simple notebook to show how to use this great searching method.\n\n\nI intentionally won't do any EDA or feature extraction from the data.\nI'll do a simple one hot encoding to categorical features and run a model !\n\n\n#### We actually need only 2 things:\n1. The parameters' values space to search\n2. An objective function to minimize"}}