{"cell_type":{"ed577cd3":"code","0d3cd531":"code","71ff59d9":"code","d341fb2e":"code","4218cced":"code","07d9c514":"code","a2baac84":"code","9251213c":"code","63af80b6":"code","10efcd32":"code","360184a6":"code","4c427d42":"code","d1fda6d5":"code","a218089e":"code","24b2cba9":"code","9f001532":"code","85ee1d1b":"code","edffb34a":"code","0b8655e5":"code","bec0bc00":"markdown","c57c8b3a":"markdown","a7c7c151":"markdown","334e6aeb":"markdown","a5ce69f3":"markdown","c76fac25":"markdown","78955c90":"markdown","d523c87c":"markdown","6d14b101":"markdown","faa258bf":"markdown","ca3b626c":"markdown","2a4b9ce8":"markdown"},"source":{"ed577cd3":"import os, re, math\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib import transforms as plttrans\n#%tensorflow_version 2.x # not yet supported on TPU (it will be in TF 2.1)\nimport tensorflow as tf\nprint(\"Tensorflow version: \" + tf.__version__)\n\nMAX_DATE = 18262 # 50 years of data, Tmin, Tmax each day","0d3cd531":"#title Data formatting and display utilites\n\n# Training data sequencer: this version works from TFRecords and uses tf.data.Dataset\ndef rnn_dataset_sequencer3tfrec(filenames, resample_by=1, batch_size=9999, seqlen=MAX_DATE, n_forward=0, nb_epochs=1, tminmax_only=False, drop_remainder=False, ordered=False):\n  # drop_remainder applies to batch size only,\n  # the remainders introduced by \"resample_by\" and \"seqlen\" are always dropped\n  \n  def truncate(x, n):\n    return x[:x.get_shape()[0] \/\/ n * n]\n  \n  def read_tfrecord(example):\n      feature = {\n        \"date\": tf.io.FixedLenFeature([MAX_DATE], tf.string),\n        \"tmin\": tf.io.FixedLenFeature([MAX_DATE], tf.float32),\n        \"tmax\": tf.io.FixedLenFeature([MAX_DATE], tf.float32),\n        \"interpolated\": tf.io.FixedLenFeature([MAX_DATE], tf.int64)\n      }\n      example = tf.io.parse_single_example(example, feature)\n      temps = tf.stack([example['tmin'], example['tmax']], axis=1)\n      return temps, example['date'], example['interpolated']\n\n  def truncate(x, n):\n    return x[:x.get_shape()[0] \/\/ n * n]\n  \n  def process_metadata(x, mean=False):\n    x = truncate(x, resample_by)\n    x = tf.reshape(x, [-1, resample_by])\n    if mean:\n      x = tf.math.greater(tf.reduce_mean(tf.cast(x, tf.float32), axis=1), 0.0)\n    else:\n      x = x[:,0]\n    x = x[:x.get_shape()[0]-n_forward] # allows n_forward=0\n    x = truncate(x, seqlen)\n    x = tf.reshape(x, [-1, seqlen])\n    return x\n  \n  def process_data(temps, dates, interpolated):\n    temps = truncate(temps, resample_by)\n    temps = tf.reduce_mean(tf.reshape(temps, [-1, resample_by, 2]), axis=1)\n    temps, targets = temps[:temps.get_shape()[0]-n_forward], temps[n_forward:] # allows n_forward=0\n    temps, targets = truncate(temps, seqlen), truncate(targets, seqlen)\n    temps, targets = tf.reshape(temps, [-1, seqlen, 2]), tf.reshape(targets, [-1, seqlen, 2])\n    return temps, targets, process_metadata(dates), process_metadata(interpolated, mean=True)\n  \n  dataset = tf.data.TFRecordDataset(filenames, \"GZIP\", num_parallel_reads=1 if ordered else 16) # 16 or 32 same perf\n  dataset = dataset.map(read_tfrecord, num_parallel_calls=32)\n  dataset = dataset.map(process_data, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n  dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)\n  dataset = dataset.map(lambda x, y, z, t: (tf.transpose(x, [1, 0, 2, 3]), tf.transpose(y, [1, 0, 2, 3]),  tf.transpose(z, [1, 0, 2]),  tf.transpose(t, [1, 0, 2])))\n  dataset = dataset.unbatch()\n  \n  if tminmax_only:\n    dataset = dataset.map(lambda temps, targets, date, interp: (temps, targets))\n    \n  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n  dataset = dataset.cache()\n  dataset = dataset.repeat(nb_epochs)\n  return dataset\n\n\n# override default matplotlib styles\nplt.rcParams['figure.figsize']=(16.8,6.0)\nplt.rcParams['axes.grid']=True\nplt.rcParams['axes.linewidth']=0\nplt.rcParams['grid.color']='#DDDDDD'\nplt.rcParams['axes.facecolor']='white'\nplt.rcParams['xtick.major.size']=0\nplt.rcParams['ytick.major.size']=0\nplt.rcParams['axes.titlesize']=15.0\n\ndef display_lr(lr_schedule, nb_epochs):\n  x = np.arange(nb_epochs)\n  y = [lr_schedule(i) for i in x]\n  plt.figure(figsize=(9,5))\n  plt.step(x,y, where='post')\n  plt.title(\"Learning rate schedule\\nmax={:.2e}, min={:.2e}\".format(np.max(y), np.min(y)),\n            y=0.85)\n  plt.xlabel('EPOCH')\n  plt.show()\n  \ndef display_loss(history, full_history, nb_epochs):\n  plt.figure()\n  plt.plot(np.arange(0, len(full_history['loss']))\/steps_per_epoch, full_history['loss'], label='detailed loss')\n  plt.plot(np.arange(1, nb_epochs+1), history['loss'], color='red', linewidth=3, label='average loss per epoch')\n  plt.ylim(0,3*max(history['loss'][1:]))\n  plt.xlabel('EPOCH')\n  plt.ylabel('LOSS')\n  plt.xlim(0, nb_epochs+0.5)\n  plt.legend()\n  for epoch in range(nb_epochs\/\/2+1):\n    plt.gca().axvspan(2*epoch, 2*epoch+1, alpha=0.05, color='grey')\n  plt.show()\n\ndef dataset_to_numpy(dataset, batches=1):\n  data = []\n  \n  # In eager mode, iterate on the Datset directly.\n  if tf.executing_eagerly():\n    for i, data_batch in zip(range(batches), dataset): # will stop on shortest\n      data.append(data_batch)\n\n      # parse dates\n    data = [(samples.numpy(),\n            targets.numpy(),\n            np.array(dates.numpy(), dtype='datetime64'),\n            interpolated.numpy())\n            for samples, targets, dates, interpolated in data]\n\n  # In non-eager mode, must get the TF node that\n  # yields the next item and run it in a tf.Session.\n  else:\n    get_next_item = tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n    with tf.Session() as ses:\n      for _ in range(batches):\n        data.append(ses.run(get_next_item))\n      \n    # parse dates\n    data = [(samples,\n            targets,\n            np.array(dates, dtype='datetime64'),\n            interpolated)\n            for samples, targets, dates, interpolated in data]\n\n  return data\n\n        \ndef picture_this_4(temperatures, dates, interpolated):\n  min_temps = temperatures[:,0]\n  max_temps = temperatures[:,1]\n  #interpolated = temperatures[:,2]\n\n  interpolated_sequence = False\n  #plt.plot(dates, max_temps)\n  for i, date in enumerate(dates):\n    if interpolated[i]:\n      if not interpolated_sequence:\n        startdate = date\n      interpolated_sequence = True\n      stopdate = date\n    else:\n      if interpolated_sequence:\n        # light shade of red just for visibility\n        plt.axvspan(startdate+np.timedelta64(-5, 'D'), stopdate+np.timedelta64(6, 'D'), facecolor='#FFCCCC', alpha=1)\n        # actual interpolated region\n        plt.axvspan(startdate+np.timedelta64(-1, 'D'), stopdate+np.timedelta64(1, 'D'), facecolor='#FF8888', alpha=1)\n      interpolated_sequence = False\n  plt.fill_between(dates, min_temps, max_temps).set_zorder(10)\n  plt.show()\n  \ndef picture_this_5(visu_data, station):\n  subplot = 231\n  for samples, targets, dates, _ in visu_data:\n    plt.subplot(subplot)\n    h1 = plt.fill_between(dates[station], samples[station,:,0], samples[station,:,1], label=\"features\")\n    h2 = plt.fill_between(dates[station], targets[station,:,0], targets[station,:,1], label=\"labels\")\n    h2.set_zorder(-1)\n    if subplot == 231:\n      plt.legend(handles=[h1, h2])\n    subplot += 1\n    if subplot==237:\n      break\n  plt.show()\n  \ndef picture_this_6(evaldata, evaldates, prime_data, results, primelen, runlen, offset, rmselen):\n  disp_data = evaldata[offset:offset+primelen+runlen]\n  disp_dates = evaldates[offset:offset+primelen+runlen]\n  colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n  displayresults = np.ma.array(np.concatenate((np.zeros([primelen,2]), results)))\n  displayresults = np.ma.masked_where(displayresults == 0, displayresults)\n  sp = plt.subplot(212)\n  p = plt.fill_between(disp_dates, displayresults[:,0], displayresults[:,1])\n  p.set_alpha(0.8)\n  p.set_zorder(10)\n  trans = plttrans.blended_transform_factory(sp.transData, sp.transAxes)\n  plt.text(disp_dates[primelen],0.05,\"DATA |\", color=colors[1], horizontalalignment=\"right\", transform=trans)\n  plt.text(disp_dates[primelen],0.05,\"| +PREDICTED\", color=colors[0], horizontalalignment=\"left\", transform=trans)\n  plt.fill_between(disp_dates, disp_data[:,0], disp_data[:,1])\n  plt.axvspan(disp_dates[primelen], disp_dates[primelen+rmselen], color='grey', alpha=0.1, ymin=0.05, ymax=0.95)\n  plt.show()\n\n  rmse = math.sqrt(np.mean((evaldata[offset+primelen:offset+primelen+rmselen] - results[:rmselen])**2))\n  print(\"RMSE on {} predictions (shaded area): {}\".format(rmselen, rmse))\n\ndef display_training_curves(training, validation, title, subplot):\n  if subplot%10==1: # set up the subplots on the first call\n    plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n    plt.tight_layout()\n  ax = plt.subplot(subplot)\n  ax.set_facecolor('#F8F8F8')\n  ax.plot(training)\n  ax.plot(validation)\n  ax.set_title('model '+ title)\n  ax.set_ylabel(title)\n  ax.set_xlabel('epoch')\n  ax.legend(['train', 'valid.'])","71ff59d9":"try: # TPU detection\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\nelse:\n    strategy = tf.distribute.get_strategy()\n    print(\"Running on GPU or CPU\")","d341fb2e":"N_FORWARD = 8       # train the network to predict N in advance (traditionnally 1)\nRESAMPLE_BY = 5     # averaging period in days (training on daily data is too much)\nRNN_CELLSIZE = 128  # size of the RNN cells\nSEQLEN = 166        # unrolled sequence length\nBATCHSIZE = 256     # mini-batch size\nDROPOUT = 0.3       # dropout regularization: probability of neurons being dropped. Should be between 0 and 0.5\n\nDATA_DIR = 'gs:\/\/good-temperatures-public\/'\nDATA_DIR_TFREC = 'gs:\/\/good-temperatures-public-tfrecords\/'\nALL_FILEPATTERN = DATA_DIR_TFREC + 'temperatures_*.tfrec'\nEVAL_FILEPATTERN = DATA_DIR_TFREC + 'temperatures_*_nb_?.tfrec'","4218cced":"all_filenames = tf.io.gfile.glob(ALL_FILEPATTERN)\neval_filenames = tf.io.gfile.glob(EVAL_FILEPATTERN)\ntrain_filenames = list(set(all_filenames) - set(eval_filenames))\n\n# TFRecord files contain data for multiple weather stations each. The number is in the file name.\nnb_finder = re.compile(r'nb_([0-9]*)\\.')  # nb of stations in 'temperatures_weather_stations_0000_to_0003_nb_4.tfrec'\nnb_stations = sum([int(nb_finder.search(s).group(1)) for s in all_filenames])\nnb_eval_stations = sum([int(nb_finder.search(s).group(1)) for s in eval_filenames])\nnb_train_stations = sum([int(nb_finder.search(s).group(1)) for s in train_filenames])\n\nprint('Pattern \"{}\" matches {} files containing {} weather stations'.format(ALL_FILEPATTERN, len(all_filenames), nb_stations))\nprint('Eval pattern \"{}\" matches {} evaluation files containing {} weather stations'.format(EVAL_FILEPATTERN, len(eval_filenames), nb_eval_stations))\nprint('The remaining {} training files contain {} weather stations'.format(len(train_filenames), nb_train_stations))\n\n# By default, this utility function loads all the weather stations and places\n# data from them as-is in an array, one station per line. Later, we will use\n# it to shape the dataset as needed for training.\n\neval_dataset = rnn_dataset_sequencer3tfrec(eval_filenames)\nevtemps, evtargets, evdates, evinter = dataset_to_numpy(eval_dataset)[0]\n\nprint()\nprint(\"Initial shape of the evaluation dataset: \" + str(evtemps.shape))\nprint(\"{} weather stations, {} data points per station (50 years), {} values per data point (Tmin, Tmax) \".format(evtemps.shape[0], evtemps.shape[1],evtemps.shape[2]))","07d9c514":"# You can adjust the visualisation range and dataset here.\n# Interpolated regions of the dataset are marked in red.\nWEATHER_STATION = 0 # 0 to 7 in default eval dataset\nSTART_DATE = 0      # 0 = Jan 2nd 1950\nEND_DATE = MAX_DATE # 18262 = Dec 31st 2009\nvisu_temperatures = evtemps[WEATHER_STATION, START_DATE:END_DATE]\nvisu_dates        = evdates[WEATHER_STATION, START_DATE:END_DATE]\nvisu_interpolated = evinter[WEATHER_STATION, START_DATE:END_DATE]\n\npicture_this_4(visu_temperatures, visu_dates, visu_interpolated)","a2baac84":"# This time we ask the utility function to average temperatures over RESAMPLE_BY day periods (in days)\neval_dataset = rnn_dataset_sequencer3tfrec(eval_filenames, RESAMPLE_BY, seqlen=MAX_DATE\/\/RESAMPLE_BY)\nevaluation_data = dataset_to_numpy(eval_dataset)[0] # all data returned as one batch\nevaltemps, _, evaldates, _ = evaluation_data","9251213c":"# display five years worth of data\nWEATHER_STATION = 0              # 0 to 7 in default eval dataset\nSTART_DATE = 0                   # 0 = Jan 2nd 1950\nEND_DATE = 365*5\/\/RESAMPLE_BY    # 5 years\nvisu_temperatures = evaltemps[WEATHER_STATION, START_DATE:END_DATE]\nvisu_dates        = evaldates[WEATHER_STATION, START_DATE:END_DATE]\nplt.fill_between(visu_dates, visu_temperatures[:,0], visu_temperatures[:,1])\nplt.show()","63af80b6":"# The sequencing function puts one weather station per line in a batch and\n# continues with data from the same station in corresponding lines in the next batch.\n# Features and labels are returned with shapes [BATCHSIZE, SEQLEN, 2]\n\neval_dataset = rnn_dataset_sequencer3tfrec(eval_filenames,\n                                           RESAMPLE_BY,\n                                           BATCHSIZE,\n                                           SEQLEN,\n                                           N_FORWARD)\n\nvisu_data = dataset_to_numpy(eval_dataset, batches=6)","10efcd32":"# Check that consecutive training sequences from the same weather station are indeed consecutive\nWEATHER_STATION = 0\npicture_this_5(visu_data, WEATHER_STATION)","360184a6":"def keras_model(batchsize, seqlen, stateful):\n  \n    model = tf.keras.Sequential([\n      tf.keras.layers.InputLayer(batch_size=batchsize, input_shape=[seqlen, 2]), # 2 for (Tmin, Tmax). Batch size is mandatory in Keras stateful RNN.\n      tf.keras.layers.GRU(RNN_CELLSIZE, stateful=stateful, return_sequences=True),\n      tf.keras.layers.GRU(RNN_CELLSIZE, stateful=stateful, return_sequences=True),\n      tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(2))\n    ])\n\n    # Tip: use  CuDNN optimized GRU cells on GPU\n    # tf.keras.layers.CuDNNGRU(RNN_CELLSIZE, stateful=stateful, return_sequences=True)\n\n    model.compile(\n      optimizer='rmsprop',\n      loss='mean_squared_error',\n      metrics=['RootMeanSquaredError'])\n\n    return model","4c427d42":"# Keras model callbacks\n\n# This callback records a per-step loss history instead of the average loss per\n# epoch that Keras normally reports. It allows you to see more problems.\nclass LossHistory(tf.keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.history = {'loss': []}\n    def on_batch_end(self, batch, logs={}):\n        self.history['loss'].append(logs.get('loss'))\n\n# Learning Rate decay absolutely necessary on this model\ndef lr_schedule(epoch): return 0.0001 + 0.01 * math.pow(0.6, epoch)\nlr_decay = tf.keras.callbacks.LearningRateScheduler(lr_schedule, verbose=True)","d1fda6d5":"NB_EPOCHS = 8 # number of times the model sees all the data during training\n\n# The model\nwith strategy.scope():\n    model = keras_model(BATCHSIZE, SEQLEN, stateful=False) # stateful RNNs not (yet) supported on TPUs\n\n# this prints a description of the model\nmodel.summary()\n\ndisplay_lr(lr_schedule, NB_EPOCHS)\n\n# The dataset\ndataset = rnn_dataset_sequencer3tfrec(train_filenames,\n                                      RESAMPLE_BY,\n                                      BATCHSIZE,\n                                      SEQLEN,\n                                      N_FORWARD,\n                                      NB_EPOCHS,\n                                      tminmax_only=True,\n                                      drop_remainder=True) # this is needed on GPU (bug?)\n  \n# Training\nsteps_per_epoch = (nb_train_stations\/\/BATCHSIZE) * (((MAX_DATE\/\/RESAMPLE_BY)-(N_FORWARD))\/\/SEQLEN)\nfull_history = LossHistory()\nhistory = model.fit(dataset,\n                    steps_per_epoch=steps_per_epoch,\n                    epochs=NB_EPOCHS,\n                    callbacks=[lr_decay, full_history])","a218089e":"display_loss(history.history, full_history.history, NB_EPOCHS)","24b2cba9":"inference_model = keras_model(1, N_FORWARD, stateful=True) # here, the stateful RNN is required\ninference_model.set_weights(model.get_weights()) # copy weights from TPU to CPU (inference will be CPU-based)","9f001532":"# Inference from stateful model\ndef keras_prediction_run(model, prime_data, run_length):\n    model.reset_states()\n\n    data_len = prime_data.shape[0]\n\n    Yin = prime_data[-(data_len\/\/N_FORWARD)*N_FORWARD:]  # trim start of sequence to make length divisible by N_FORWARD\n    Yin = np.reshape(Yin, [data_len\/\/N_FORWARD, N_FORWARD, 2])\n\n    # prime the state from data\n    if data_len\/\/N_FORWARD > 0:\n        for i in range(data_len\/\/N_FORWARD - 1): # keep last N_FORWARD samples to serve as the input sequence for predictions\n            model.predict(np.expand_dims(Yin[i], axis=0))\n\n        # prime predictions with real data: the last N_FORWARD samples\n        Yout = np.expand_dims(Yin[data_len\/\/N_FORWARD - 1], axis=0)\n        # Yout shape [1, N_FORWARD, 2]: batch of a single sequence of length N_FORWARD of (Tmin, Tmax) data pointa\n\n    # prediction run\n    results = []\n    for i in range(run_length\/\/N_FORWARD+1):\n        Yout = model.predict(Yout)\n        results.append(Yout[0]) # shape [N_FORWARD, 2]\n\n    return np.concatenate(results, axis=0)[:run_length]","85ee1d1b":"QYEAR = 365\/\/(RESAMPLE_BY*4)\nYEAR = 365\/\/(RESAMPLE_BY)\n\n# Try starting predictions from January \/ March \/ July (resp. OFFSET = YEAR or YEAR+QYEAR or YEAR+2*QYEAR)\n# Some start dates are more challenging for the model than others.\nOFFSET = 30*YEAR+1*QYEAR\nPRIMELEN=5*YEAR\nRUNLEN=3*YEAR\nRMSELEN=3*365\/\/(RESAMPLE_BY*2) # accuracy of predictions 1.5 years in advance","edffb34a":"selected_evaluation_data = tuple(data[np.array([1, 2, 3, 4, 5, 6, 7, 12])] for data in evaluation_data) # selecting 8 interesting stations\neval_temps, _, eval_dates, _ = selected_evaluation_data\nfor temp_seq, date_seq in zip(eval_temps, eval_dates):\n    prime_data = temp_seq[OFFSET:OFFSET+PRIMELEN]\n    results = keras_prediction_run(inference_model, prime_data, RUNLEN)\n    picture_this_6(temp_seq, date_seq, prime_data, results, PRIMELEN, RUNLEN, OFFSET, RMSELEN)","0b8655e5":"rmses = []\nbad_ones = 0\neval_temps, _, eval_dates, _ = selected_evaluation_data # use evaluation_data for a more accurate eval\nfor offset in [YEAR, YEAR+QYEAR, YEAR+2*QYEAR]:\n    for evaldata in eval_temps:\n        prime_data = evaldata[offset:offset+PRIMELEN]\n        results = keras_prediction_run(inference_model, prime_data, RUNLEN)\n        rmse = math.sqrt(np.mean((evaldata[offset+PRIMELEN:offset+PRIMELEN+RMSELEN] - results[:RMSELEN])**2))\n        rmses.append(rmse)\n        if rmse>5: bad_ones += 1\n        print(\"RMSE on {} predictions (shaded area): {}\".format(RMSELEN, rmse))\nprint(\"Average RMSE on {} weather stations: {} ({} really bad ones, i.e. >5.0)\".format(len(evaltemps), np.mean(rmses), bad_ones))","bec0bc00":"# Temperature data\nThis is what our temperature datasets looks like: sequences of daily (Tmin, Tmax) from 1960 to 2010. They have been cleaned up and eventual missing values have been filled by interpolation. Interpolated regions of the dataset are marked in red on the graph.","c57c8b3a":"# Keras model\n![alt text](https:\/\/googlecloudplatform.github.io\/tensorflow-without-a-phd\/docs\/images\/RNN2.svg)","a7c7c151":"# Resampling\nIn this playground notebook, resampling is initially set to RESAMPLE_BY=1 (no resampling).\nThis is what (Tmin, Tmax) temperatures look like.","334e6aeb":"# TPU detection","a5ce69f3":"## Data formatting and display utilites\nYou can skip this code. Nothing of interest here.","c76fac25":"# The training loop\n\nTraining a stateful RNN theoretically requires data from weather stations to be arranged into batches so that data from one weather station continues \non the same line across all batches. This way, the RNN state resulting from one batch is the correct RNN state for the next batch. In practice, training with bad input states introduces noise at the begining of each sequence, but with sequences long enough, the impact can be small. Most data scientists do not bother with correct sequence ordering because of the complexity. You can have a look at the code of the rnn_dataset_sequencer3tfrec function in the \"utilities\" section. It does the batch sequencing correctly for stateful RNNs but it is a bit complicated.\n![alt text](https:\/\/googlecloudplatform.github.io\/tensorflow-without-a-phd\/docs\/images\/batching.svg)\n\nOn top of that, on TPUs, stateful RNNs are not (yet) supported.","78955c90":"# Visualize training sequences\nThis is what the neural network will see during training.","d523c87c":"# Inference model","6d14b101":"# Hyperparameters\nN_FORWARD = 1: works but model struggles to predict from some positions<br\/>\nN_FORWARD = 4: better but still bad occasionnally<br\/>\nN_FORWARD = 8: works perfectly ","faa258bf":"# Visual validation","ca3b626c":"# An RNN model for temperature data\nThis time we will be working with real data: daily (Tmin, Tmax) temperature series from 1666 weather stations spanning 50 years. It is to be noted that a pretty good predictor model already exists for temperatures: the average of temperatures on the same day of the year in N previous years. It is not clear if RNNs can do better but we will se how far they can go.","2a4b9ce8":"Copyright 2019 Google LLC\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n[http:\/\/www.apache.org\/licenses\/LICENSE-2.0](http:\/\/www.apache.org\/licenses\/LICENSE-2.0)\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License."}}