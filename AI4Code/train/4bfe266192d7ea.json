{"cell_type":{"8700c054":"code","5bca798a":"code","8259f81b":"code","f8532bef":"code","0d3a88ff":"code","de5edc33":"code","23fed341":"code","aca03373":"code","545aee25":"code","220f5aca":"code","15d19cd6":"code","065b891f":"code","620a9db4":"code","6c30684e":"code","dd146775":"code","edf93a6d":"code","3a3b4e2b":"code","19eaf9e8":"code","834c4c8c":"code","c89f341c":"code","360a433d":"code","12fbe1d4":"code","878b9040":"code","e20ce2e6":"code","ce571c9f":"code","f6272b65":"code","c5c9b817":"code","12ddb3df":"code","396566d8":"code","16e58a45":"code","2db28f3e":"code","8e9b5b47":"code","dd7c5a8d":"code","e25bbc45":"code","4d242a3c":"code","df00f5cb":"code","d28db87e":"code","05c44313":"code","de625115":"code","6989084a":"code","23e7dc92":"code","b851eaae":"code","ae1a1c62":"code","f0c48257":"code","db0d845f":"code","cb5f2b4b":"code","5cf1c75e":"code","fd665b2c":"code","f2d70bb1":"code","cc1ebc54":"code","ea30f850":"code","ab02d6aa":"code","f1581656":"code","81aefba1":"code","425c6656":"code","fbb0429e":"code","9b3763be":"code","b22bd395":"code","c4838c8b":"code","33c0dbfd":"code","ad5d694a":"code","ca023a99":"code","25836e72":"code","8ff14cd2":"code","760a2182":"code","d98f0557":"code","70ace9da":"code","c8cb58fb":"code","fcf0e876":"code","55e06d3d":"code","c86889c2":"code","d3d7e979":"code","f52f3596":"code","817dcc44":"code","09b42dab":"code","caa4057a":"code","acd785fa":"code","e733fa1b":"code","5cc782db":"code","cf2ab14b":"code","d58de195":"code","21135fc1":"code","b1e08d78":"code","9b47ed7f":"code","0c9f9985":"code","61f97e35":"code","25c30fb7":"code","b5eb8264":"markdown","7c49903c":"markdown","9611d4db":"markdown","f5051a7f":"markdown","ea30b1a6":"markdown","d9ca3948":"markdown","0a378c6c":"markdown","224e1970":"markdown","be3e5bfa":"markdown","af9f27a8":"markdown","be6920a5":"markdown","573480a6":"markdown","481c84f8":"markdown","6d9b41b7":"markdown","f2ff945c":"markdown","43b80258":"markdown","9a948bcf":"markdown","4c1e7c22":"markdown","b072bb9a":"markdown","8bd50efb":"markdown","96023c09":"markdown","067d8ece":"markdown","9dae8419":"markdown","6c28ef15":"markdown","cab41f8f":"markdown","4010cf7d":"markdown","7fceca06":"markdown","f01edf84":"markdown","7700bc67":"markdown","a24c6a82":"markdown","9da3e7ba":"markdown","a3ebf217":"markdown","ba8c9d80":"markdown","d4ba75df":"markdown","c42f82b9":"markdown","2da55b04":"markdown","02c6126b":"markdown","79de9d39":"markdown","6d8940c7":"markdown","e7b52daf":"markdown","68d1c16c":"markdown","729cac27":"markdown","4e86dc85":"markdown","6f2201fa":"markdown","360a3ae3":"markdown","d47525c4":"markdown","258efd93":"markdown","f5444fb0":"markdown","4efb17b7":"markdown","eb64b4b1":"markdown","4a98a2b2":"markdown","d52ff4d9":"markdown","5114613f":"markdown"},"source":{"8700c054":"from google.colab import drive\ndrive.mount('\/content\/gdrive')","5bca798a":"### Data handling libraries ###\n\nimport numpy as np\nimport pandas as pd\nimport os\n\n### Plotting Libraries ### \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.pyplot import figure\nplt.style.use('ggplot')\nplt.rcParams['figure.figsize'] = (12,8)\n\n### Date Time ###\n\nimport datetime\nimport time\nimport pytz\n\n### Warnings ###\nimport warnings\nwarnings.filterwarnings('ignore')\n\n### Progress Bar ###\nfrom tqdm import tqdm\n\n### Model Building, Model Evaluvation, Model Preprocessing ###\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Models Imbalance # \n\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline\n\n# ML MODELS #\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Scoring Dependancies #\n\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score \n# from sklearn.metrics import average_precision_score,make_scorer\nfrom sklearn.model_selection import cross_val_score, cross_validate, KFold\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import confusion_matrix\n\n# Models Saving #\n\nimport pickle\n\n# Other #\nfrom collections import Counter\nfrom sklearn.utils import shuffle","8259f81b":"PATH = r'\/content\/gdrive\/My Drive\/MLPipeLine'\nREADFILE = 'investments_VC.csv'\nROWS = 0\nCOLUMNS = 0\nSEARCHFIT = 0\nLISTCOLUMNNAME= []\nLISTFUNDINGCOL = ['seed', 'venture',\n       'equity_crowdfunding', 'undisclosed', 'convertible_note',\n       'debt_financing', 'grant', 'private_equity', 'post_ipo_equity',\n       'post_ipo_debt', 'secondary_market', 'product_crowdfunding', 'round_A',\n       'round_B', 'round_C', 'round_D', 'round_E', 'round_F', 'round_G',\n       'round_H']\n\nMAINDF = pd.read_csv(os.path.join(PATH, READFILE), encoding='latin1')\nLISTCOLUMNNAME = list(MAINDF.columns)\nROWS, COLUMNS = MAINDF.shape","f8532bef":"def funcCustomCVScore(fncp_X_train, fncp_y_train, fncpKFold,fncpBaseModel, fncpBaseModelParam=None, fncpRandomState=123, fncpScoreAverage='weighted'):\n\n  '''\n  This function splits the X_train and y_train into folds for cross calulating recall, precesion and f1 score's of each fold and returns the scores \n  and prints the mean score and the 95% confidence interval of the score estimate.\n\n  input:\n     fncp_X_train - X_train\n     fncp_y_train - y_train\n     fncpKFold - No of folds\n     fncpBaseModel - Base model. Ex: RandomForestClassifier\n     (Optional) (dict) - Parameters to be used in the base model\n     (Optional)  fncpRandomState\n     (Optional) fncpScoreAverage\n  output:\n    recallScores\n    precisionScores\n    f1Scores\n\n  '''\n  kfold = KFold(n_splits=fncpKFold, random_state=fncpRandomState)\n  recallScores = []\n  precisionScores = []\n  f1Scores = []\n  for train_index, test_index in tqdm(kfold.split(fncp_X_train)):\n    cv_X_train = fncp_X_train[fncp_X_train.index.isin(train_index)]\n    cv_X_test = fncp_X_train[fncp_X_train.index.isin(test_index)]\n\n    cv_y_train = fncp_y_train[fncp_y_train.index.isin(train_index)]\n    cv_y_test = fncp_y_train[fncp_y_train.index.isin(test_index)]\n\n    if fncpBaseModelParam == None:\n      model = fncpBaseModel()\n    else:\n      model = fncpBaseModel(**fncpBaseModelParam)\n    model.fit(cv_X_train,cv_y_train)\n\n    tempScore = round(recall_score(cv_y_test, model.predict(cv_X_test), average=fncpScoreAverage)*100,2)\n    precisionScores.append(tempScore)\n\n    tempScore = round(precision_score(cv_y_test, model.predict(cv_X_test), average=fncpScoreAverage)*100,2)\n    recallScores.append(tempScore)\n\n    tempScore = round(f1_score(cv_y_test, model.predict(cv_X_test), average=fncpScoreAverage)*100,2)\n    f1Scores.append(tempScore)\n  print('\\n')\n  print(f'The mean score and the 95% confidence interval of the score estimate are')\n  print(\"Recall: %0.2f (+\/- %0.2f)\" % (np.array(recallScores).mean(), np.array(recallScores).std() * 2))\n  print(\"Precision: %0.2f (+\/- %0.2f)\" % (np.array(precisionScores).mean(), np.array(precisionScores).std() * 2))\n  print(\"F1-Score: %0.2f (+\/- %0.2f)\" % (np.array(f1Scores).mean(), np.array(f1Scores).std() * 2))\n  return recallScores, precisionScores, f1Scores","0d3a88ff":"def funcCustomCVScore(fncp_X_train, fncp_y_train, fncpKFold,fncpBaseModel, fncpBaseModelParam=None, fncpRandomState=123, fncpScoreAverage='weighted'):\n\n  '''\n  This function splits the X_train and y_train into folds for cross calulating recall, precesion and f1 score's of each fold and returns the scores \n  and prints the mean score and the 95% confidence interval of the score estimate.\n\n  input:\n     fncp_X_train - X_train\n     fncp_y_train - y_train\n     fncpKFold - No of folds\n     fncpBaseModel - Base model. Ex: RandomForestClassifier\n     (Optional) (dict) - Parameters to be used in the base model\n     (Optional)  fncpRandomState\n     (Optional) fncpScoreAverage\n  output:\n    recallScores\n    precisionScores\n    f1Scores\n\n  '''\n  kfold = KFold(n_splits=fncpKFold, random_state=fncpRandomState)\n  recallScores = []\n  precisionScores = []\n  f1Scores = []\n  for train_index, test_index in tqdm(kfold.split(fncp_X_train)):\n    cv_X_train = fncp_X_train[fncp_X_train.index.isin(train_index)]\n    cv_X_test = fncp_X_train[fncp_X_train.index.isin(test_index)]\n\n    cv_y_train = fncp_y_train[fncp_y_train.index.isin(train_index)]\n    cv_y_test = fncp_y_train[fncp_y_train.index.isin(test_index)]\n\n    if fncpBaseModelParam == None:\n      model = fncpBaseModel()\n    else:\n      model = fncpBaseModel(**fncpBaseModelParam)\n    model.fit(cv_X_train,cv_y_train)\n\n    tempScore = round(recall_score(cv_y_test, model.predict(cv_X_test), average=fncpScoreAverage)*100,2)\n    precisionScores.append(tempScore)\n\n    tempScore = round(precision_score(cv_y_test, model.predict(cv_X_test), average=fncpScoreAverage)*100,2)\n    recallScores.append(tempScore)\n\n    tempScore = round(f1_score(cv_y_test, model.predict(cv_X_test), average=fncpScoreAverage)*100,2)\n    f1Scores.append(tempScore)\n  print('\\n')\n  print(f'The mean score and the 95% confidence interval of the score estimate are')\n  print(\"Recall: %0.2f (+\/- %0.2f)\" % (np.array(recallScores).mean(), np.array(recallScores).std() * 2))\n  print(\"Precision: %0.2f (+\/- %0.2f)\" % (np.array(precisionScores).mean(), np.array(precisionScores).std() * 2))\n  print(\"F1-Score: %0.2f (+\/- %0.2f)\" % (np.array(f1Scores).mean(), np.array(f1Scores).std() * 2))\n  return recallScores, precisionScores, f1Scores","de5edc33":"def funcPreprocessing(fncpDF):\n    \n    '''\n    This function\n    1) Removes leading and trailing white spaces in the column names.\n    2)Removes leading and trailing white spaces in the values present in object datatype columns.\n    \n    input\n        Dataframe\n    output\n        None\n    '''\n\n    dictCol = {}\n    print('Column preprocessing...')\n    for col in fncpDF.columns:\n        dictCol[col] = col.strip()\n    fncpDF.rename(columns=dictCol, inplace=True) \n    print('Object datatype preprocessing...\\n')\n    for col in fncpDF.columns:\n        if fncpDF[col].dtype == 'object':\n            fncpDF[col] = fncpDF[col].str.strip()\n    print('Sucessfully preprocessed the dataframe!')   ","23fed341":"def fncpModelEvaluvate(fncpActual, fncpPredicted, fncpBoolHeatMap=False, fncpMultiClass=True,fncpAverageType='weighted'):\n  '''\n  This function prints the various evaluvation metric of a models and also prints the confusion matrix\n  input:\n    fncpActual - Actual Values\n    funPredictedValues - Predicted Values\n    (optional) (bool) fncpBoolHeatMap - To display or not display confusion matrix\n    (optional) (bool) fncpMultiClass - Is it a multiclass problem or binary class problem\n    (optional) (bool) fncpAverageType - Average type for multiclass problem   \n  '''\n\n  # Heat Map #\n  if  fncpBoolHeatMap == True:\n    cf_matrix = confusion_matrix(fncpActual, fncpPredicted)\n    make_confusion_matrix(cf_matrix, figsize=(8,6), cbar=True, cmap='BrBG')\n    print('\\n\\n')\n\n  print('Evaluation Metrics\\n')\n  # print(f'Accuracy Score :{round(accuracy_score(fncpActual, fncpPredicted)*100,2)}%')\n  if fncpMultiClass == True:\n    print(f'Recall Score :{round(recall_score(fncpActual, fncpPredicted, average=fncpAverageType)*100,2)}%')\n    print(f'Precision Score :{round(precision_score(fncpActual, fncpPredicted, average=fncpAverageType)*100,2)}%')\n    print(f'F1 Score :{round(f1_score(fncpActual, fncpPredicted, average=fncpAverageType)*100,2)}%')\n  else:\n    print(f'Recall Score :{round(recall_score(fncpActual, fncpPredicted)*100,2)}%')\n    print(f'Precision Score :{round(precision_score(fncpActual, fncpPredicted)*100,2)}%')\n    print(f'F1 Score :{round(f1_score(fncpActual, fncpPredicted)*100,2)}%')\n","aca03373":"def funcFeatureImportance(fncpModel, fncpTrainSet, fncpCV=True):\n\n  '''\n  This function prints the top 20 and bottom 20 important features and returns an dataframe with important features sorted in descending order\n  input:\n    (model) fncpModel\n    (dataframe) fncpTrainSet\n    (optional) (bool) fncpCV\n  ouput:\n    dataframe\n  '''\n  if fncpCV == True:\n    feature_importances = pd.DataFrame(fncpModel.best_estimator_.feature_importances_,\n                                      index = fncpTrainSet.columns,\n                                        columns=['importance']).sort_values('importance', ascending=False)\n  else:\n    feature_importances = pd.DataFrame(fncpModel.feature_importances_,\n                              index = fncpTrainSet.columns,\n                                columns=['importance']).sort_values('importance', ascending=False)                         \n  print(\"Top 20 Important Feature\\n\")\n  print(feature_importances.head(20))\n  print('\\n')\n  print(\"Bottom 20 Important Feature\\n\")\n  print(feature_importances.tail(20))\n  return feature_importances","545aee25":"def funcDescription(fncpDF, fncpPrnt=False):\n    \n    '''\n    This function prints some of the general characterstics of the dataset and returns a list of tuples about the columns.\n    input: \n        DataFrame\n        (bool) (optional) To print or not\n    output: \n        List of general characterstics\n    '''\n    \n    lstTemp = []\n    for col in tqdm(fncpDF.columns):\n        if fncpDF[col].dtype == 'object':\n            colUniqueValue = fncpDF[col].nunique()\n            colMax = 0\n            colMin = 0\n            colMean = 0\n            colType = 'object'\n            lstTemp.append((col.strip(), colUniqueValue, colMax,colMin,round((fncpDF[col].isna().sum()\/ROWS)*100, 2), colType))\n        else:\n            colUniqueValue = 0\n            colMax = fncpDF[col].max()\n            colMin = fncpDF[col].min()\n            colMean = round(fncpDF[col].mean(),2)\n            colType = 'float'\n            lstTemp.append((col, colUniqueValue, colMax,colMin,round((fncpDF[col].isna().sum()\/ROWS)*100, 2), colType))           \n    lstTemp = sorted(lstTemp, key=lambda x: x[4], reverse=True)\n    if fncpPrnt:\n        print(f'Total no of rows : {fncpDF.shape[0]} \\nTotal no of columns : {fncpDF.shape[1]}')\n        totalNaNRows = fncpDF[fncpDF.isna().any(axis=1)].shape[0]\n        print(f'Rows with atleast one of the columns with a NaN value : {totalNaNRows}')\n        print('\\n')\n        for item in lstTemp:\n            print(f'------------------- {item[0]}-------------------')\n            print(f'Unique values in column : {item[1]}')\n            print(f'Max value in column : {item[2]}')\n            print(f'Min value in column : {item[3]}')\n            print(f'Mean Value in column : {item[4]}')\n            print('\\n')\n    return lstTemp","220f5aca":"### Date preprocessing ###\n\ndef funcDateManipulation(fncpDF, fncpColName, funcStartsWith='00'):\n    '''\n    The expected date format is yyyy-mm-dd but few of the dates are not in this format. This function changes the value to the \/ \n    specified format. The startsWith searches for all the dates that begin with 00 and modifies them into the required format.\n    \n    input: \n        DataFrame\n        (str) fncpColName, \n        (str) (optional) funcStartsWith\n    output: \n        None\n    '''\n    print(f'Processing column {fncpColName}....')\n    lstTemp = fncpDF.loc[fncpDF[fncpColName].str.startswith(funcStartsWith, na=False), [fncpColName]].values.tolist()\n    print(lstTemp)\n    for dte in lstTemp:\n        year = str(dte[0][-2:])\n        month = str(dte[0][5:7])\n        day = str(dte[0][2:4])\n        newDate = '20'+year+'-'+month+'-'+day\n        fncpDF.loc[fncpDF[fncpColName].str.contains(dte[0], na=False),[fncpColName]] = newDate\n    print(f'Sucessfully changed the date for {fncpColName} variable')","15d19cd6":"def funcCustomOneHotEncode(fncpDF, fncpColName,fncpTop=5, fncpBottom=5, fncpMiddle=10, fncpDropOriginal=True):\n    \n    '''\n    This function takes the fncpTop, fncpMiddledle and fncpBottom most occuring values in a feature and one hot encodes the values.\n    input: \n        DataFrame, \n        (str) Variable\/Feature,\n        (int) (optional) fncpTop n categories, \n        (int) (optional) fncpBottom n categories, \n        (int) (optional) fncpMiddle n categories, \n        (bool) (optional) To drop original variable\n    output: \n        1 (Success) \n        -1 (Failure)\n    '''\n    \n    if fncpColName not in fncpDF.columns:\n        return -1\n    \n    uniqueMarketsCount = len(fncpDF[fncpColName].unique())\n    print(f'There are in total {uniqueMarketsCount} unique categories in {fncpColName} feature \\n')\n\n    # Top X\n    listfncpTopXCategories = []\n    listfncpTopXCategories = fncpDF[fncpColName].value_counts().sort_values(ascending = False).head(fncpTop).index.tolist()\n    if 'Unknown' in listfncpTopXCategories:\n        listfncpTopXCategories = fncpDF[fncpColName].value_counts().sort_values(ascending = False).head(fncpTop + 1).index.tolist()\n        listfncpTopXCategories.remove('Unknown') # Removing the 'Unknown' market since it is a imputed value\n    print(f'fncpTop {len(listfncpTopXCategories)} markets are:\\n ')\n    print({*listfncpTopXCategories}, sep = \", \")\n\n    # Mid X\n    listfncpMiddleXCategories = []\n    startPos = int(uniqueMarketsCount\/2)\n    endPos = startPos + fncpMiddle\n    listfncpMiddleXCategories = fncpDF[fncpColName].value_counts().sort_values(ascending = True)[startPos:endPos].index.tolist()\n    if 'Unknown' in listfncpMiddleXCategories:\n        listfncpMiddleXCategories = fncpDF[fncpColName].value_counts().sort_values(ascending = True)[startPos:endPos + 1].index.tolist()        \n        listfncpMiddleXCategories.remove('Unknown') # Removing the 'Unknown' market since it is a imputed value\n    print(f'\\nfncpMiddle {len(listfncpMiddleXCategories)} markets are:\\n ')\n    print({*listfncpMiddleXCategories}, sep = \", \")\n\n    # Bottom X\n    listfncpBottomXCategories = []\n    listfncpBottomXCategories = fncpDF[fncpColName].value_counts().sort_values(ascending = True).head(fncpBottom).index.tolist()\n    if 'Unknown' in listfncpTopXCategories:\n        listfncpBottomXCategories = fncpDF[fncpColName].value_counts().sort_values(ascending = True).head(fncpBottom + 1).index.tolist()\n        listfncpBottomXCategories.remove('Unknown') # Removing the 'Unknown' market since it is a imputed value\n    print(f'\\nfncpBottom {len(listfncpBottomXCategories)} markets are:\\n ')\n    print({*listfncpBottomXCategories}, sep = \", \")\n\n    listFinalXCategories = listfncpTopXCategories + listfncpMiddleXCategories + listfncpBottomXCategories\n    for label in listFinalXCategories:\n      fncpDF['f_'+fncpColName+'_'+label] = np.where(fncpDF[fncpColName] == label,1,0)\n    \n    if fncpDropOriginal == True:\n        fncpDF.drop(fncpColName, axis=1, inplace=True)\n    print(f'Sucessfully implemented custom one hot encoding for {fncpColName} variable...')\n    return listFinalXCategories","065b891f":"def funcColumnsToDrop(fncpDF, fncpLstColToDrop = []):\n    \n    '''\n    This function drops a list of column(s) in the dataframe and returns the new unique columns in the dataframe\n    input: Dataframe, list of coulumns to drop\n    output: list of new columns (Sucess) \/ -1 (Failure)\n    '''\n    \n    if len(fncpLstColToDrop) > 0 and set(fncpLstColToDrop).issubset(fncpDF.columns):\n        fncpDF.drop(fncpLstColToDrop, axis=1, inplace=True) \n        return fncpDF.columns\n    else:\n        print('No columns to drop or one of the columns present in the list is not available in the dataframe')\n        return -1","620a9db4":"def funcHeatMap(funcpX, funcpY, funcpSize):\n  '''\n  This function creates a custom heatmap.\n  Please refer https:\/\/towardsdatascience.com\/better-heatmaps-and-correlation-matrix-plots-in-python-41445d0f2bec for in depth explanation of this function.\n  '''\n\n  fig, ax = plt.subplots()\n  \n  # Mapping from column names to integer coordinates\n  x_labels = [v for v in sorted(funcpX.unique())]\n  y_labels = [v for v in sorted(funcpY.unique())]\n  x_to_num = {p[1]:p[0] for p in enumerate(x_labels)} \n  y_to_num = {p[1]:p[0] for p in enumerate(y_labels)} \n  \n  size_scale = 270\n  ax.scatter(\n      x=funcpX.map(x_to_num), # Use mapping for x\n      y=funcpY.map(y_to_num), # Use mapping for y\n      s=funcpSize * size_scale, # Vector of square sizes, proportional to size parameter \n      c='green',\n      alpha=0.8,\n      marker='s' # Use square as scatterplot marker\n  )\n  \n  # Show column labels on the axes\n  ax.set_xticks([x_to_num[v] for v in x_labels])\n  listTempXLabel = [name.replace('_', ' ') for name in x_labels]\n  ax.set_xticklabels(listTempXLabel, rotation=90, horizontalalignment='right')\n  ax.set_yticks([y_to_num[v] for v in y_labels])\n  listTempYLabel = [name.replace('_', ' ') for name in y_labels]\n  ax.set_yticklabels(listTempYLabel)\n  ax.tick_params(axis='x', colors='white',labelsize=12)\n  ax.tick_params(axis='y', colors='white')\n  ax.grid(False, 'major')\n  ax.grid(True, 'minor')\n  ax.set_xticks([t + 0.5 for t in ax.get_xticks()], minor=True)\n  ax.set_yticks([t + 0.5 for t in ax.get_yticks()], minor=True)\n  ax.set_xlim([-0.5, max([v for v in x_to_num.values()]) + 0.5]) \n  ax.set_ylim([-0.5, max([v for v in y_to_num.values()]) + 0.5])","6c30684e":"### Renaming the columns to remove the white spaces ###    \ndfProcessed = MAINDF.copy()\nfuncPreprocessing(dfProcessed)\nLISTCOLUMNNAME = dfProcessed.columns","dd146775":"### Checking and deleting rows where all the values are NaN ###\n\nallNAValues = dfProcessed[dfProcessed.isna().all(axis=1)].shape[0]\nprint(f'There are {allNAValues} rows that have only NaN as values')\ndfProcessed = dfProcessed.dropna(how='all')\ndfProcessed = dfProcessed.reset_index(drop=True)","edf93a6d":"### Checking duplicate records in a dataframe ###\n\nduplicateRecords = dfProcessed[dfProcessed.duplicated()].shape[0]\nif duplicateRecords == 0:\n  print(f'There are {duplicateRecords} duplicate records in the dataframe')\nelse:\n  print(f'{duplicateRecords} duplicate records are dropped from the dataframe')\n  dfProcessed = dfProcessed[~dfProcessed.duplicated()]","3a3b4e2b":"### Dropping rows where target variable 'status' is NaN ### \n\ntargetNAValues = dfProcessed.loc[dfProcessed['status'].isna() == True,].shape[0]\nprint(f'There are {targetNAValues} rows for which target variable has NaN as values')\ndfProcessed = dfProcessed.loc[dfProcessed['status'].isna() == False,]","19eaf9e8":"### Removing the rows for which we dont have total funding and the breakup of the funding is null as well ###\n\nprint('The list of variables associated with funding are \\n', LISTFUNDINGCOL)\ndfProcessed['f_SumCol'] = dfProcessed[LISTFUNDINGCOL].sum(axis = 1)\nrowsTotalSumCheck = dfProcessed.loc[(dfProcessed['funding_total_usd'] == '-'),['funding_total_usd']].shape[0]\nprint(f'There are {rowsTotalSumCheck} rows whose total funding is none (\"-\")')\nrowsTotalSumCheck = dfProcessed.loc[(dfProcessed['funding_total_usd'] == '-') & (dfProcessed['f_SumCol'] > 0),].shape[0]\nprint(f'There are {rowsTotalSumCheck} rows whose total funding is none but sum of the total funding is more than zero')\n\n### Selecting only the rows for which the variable funding_total_usd is not equal to '-' ###\ndfProcessed = dfProcessed.loc[~(dfProcessed['funding_total_usd'] == '-'),]","834c4c8c":"# ### ROWS WITH HIGH NaN percetage ###\n\nlstHighNanRow = []\nfor i in tqdm(range(len(dfProcessed.index)), desc='Rows processed'):\n    temp = round((dfProcessed.iloc[i].isnull().sum()\/dfProcessed.shape[1])*100,2)\n    if temp >= 80:\n      lstHighNanRow.append(i)\nprint(f'\\nNumber of rows deleted is {len(lstHighNanRow)}')\ndfProcessed = dfProcessed.drop(dfProcessed.index[lstHighNanRow])","c89f341c":"### Removing permalink and name since this would not contribute to the model ###\n\nlstDropColumns = ['permalink', 'name']\nLISTCOLUMNNAME = funcColumnsToDrop(dfProcessed, lstDropColumns)","360a433d":"### Dropping region based columns ###\n\n# Replacing Nan with 'Unknown' where all the region based columns have NaN values \nlstcolumns = ['country_code', 'state_code','region', 'city']\ndfProcessed.loc[(dfProcessed[lstcolumns].isna().all(axis=1) == True), lstcolumns] = 'Unknown'\n\ntxt = 'Total Null values is '\nprint('country_code')\nprint(txt, dfProcessed[ 'country_code'].isnull().sum())\nprint('\\n')\nprint('state_code')\nprint(txt, dfProcessed[ 'state_code'].isnull().sum())\nprint('\\n')\nprint('region')\nprint(txt, dfProcessed[ 'region'].isnull().sum())\nprint('\\n')\nprint('city')\nprint(txt, dfProcessed[ 'city'].isnull().sum())\nprint('\\n')\n\nprint(dfProcessed.loc[(dfProcessed['state_code'].isnull() == False), lstcolumns]['country_code'].value_counts())\n\n# Since the state_code has the maximum number of null values and remaing non-null values belong to US and Canada,\n# we will be removing the state_code variable\n\nlstDropColumns = ['state_code']\nLISTCOLUMNNAME = funcColumnsToDrop(dfProcessed, lstDropColumns)","12fbe1d4":"### Dropping City column ###\n\nuniqueCityCount = len(dfProcessed['city'].unique())\nuniqueRatioPercent = round(uniqueCityCount\/len(dfProcessed['city'])*100,2)\nprint(f'There are {uniqueCityCount} unique cities and hence dropping the column since uniquness ratio is low ({uniqueRatioPercent})')\nlstDropColumns = ['city']\nLISTCOLUMNNAME = funcColumnsToDrop(dfProcessed, lstDropColumns)","878b9040":"### Removing funding_total_usd since this is just an addition of all the funding columns###\n\nlstDropColumns = ['funding_total_usd'] #, 'f_SumCol']\nLISTCOLUMNNAME = funcColumnsToDrop(dfProcessed, lstDropColumns)\n\n### Combining Seed investment and Angel Investment into one since both are the same.\n\ndfProcessed['seed'] = dfProcessed['seed'] + dfProcessed['angel']\nlstDropColumns = ['angel']\nLISTCOLUMNNAME = funcColumnsToDrop(dfProcessed, lstDropColumns)","e20ce2e6":"listColCharacterstics = funcDescription(MAINDF, fncpPrnt=False)","ce571c9f":"### Imbalanced Dataset ###\n\n\nplt.figure(figsize = (10,5), facecolor = 'black', dpi=70) \nsns.set(rc={'axes.facecolor':'black', \n            'figure.facecolor':'black', \n            'axes.spines.top': False\n\n            })\nax = dfProcessed['status'].value_counts(dropna = False).plot(kind='bar', color = 'green', alpha = 0.8)\nax.set_xlabel('')\nax.set_ylabel('CATEGORYWISE COUNT')\nax.set()\nax.yaxis.label.set_color('white')\nax.xaxis.label.set_color('white')\nax.tick_params(axis='x', colors='white',labelsize=12)\nax.tick_params(axis='y', colors='white',labelsize=12)\nax.tick_params(axis='y', colors='white')\nsns.despine()\nplt.xticks(rotation=45)\nplt.show()","f6272b65":"### Status VS Fundings ###\n\nprint(LISTFUNDINGCOL)\n\nfig = plt.figure(figsize = (21,21), facecolor = 'black', dpi=70) \n\nfor c,num in zip(LISTFUNDINGCOL, range(1,22)):\n  ax = fig.add_subplot(7,3,num)\n  x = np.random.rand()\n  y = np.random.rand()\n  z = np.random.rand()\n  ax =  dfProcessed.groupby('status')[c].agg('mean').plot(kind='bar',alpha = .8, color=(x,y,z))\n\n  ax.set_xlabel('')\n  temp = 'Median \"' + c.replace('_',' ') + '\" value'\n  ax.set_ylabel(temp)\n  ax.set_title('')\n  ax.xaxis.label.set_color('white')\n  ax.yaxis.label.set_color('white')\n  ax.tick_params(axis='x', colors='white',labelsize=12)\n  ax.tick_params(axis='y', colors='white')\n  sns.despine()\n  plt.xticks(rotation=45)\n\nplt.tight_layout()\nplt.subplots_adjust(hspace=0.7, wspace = 0.5)","c5c9b817":"### Correlation HeatMap ###\n\nlistTempCol = LISTFUNDINGCOL.copy()\nlistTempCol.append('funding_rounds')\ncorr = dfProcessed[listTempCol].corr()\ncorr = pd.melt(corr.reset_index(), id_vars='index') # Unpivot the dataframe, so we can get pair of arrays for x and y\ncorr.columns = ['x', 'y', 'value']\nfuncHeatMap(\n    funcpX=corr['x'],\n    funcpY=corr['y'],\n    funcpSize=corr['value'].abs()\n)","12ddb3df":"### Found out while converting the object type to date type ###\n\nwrongDateCount = dfProcessed.loc[dfProcessed['founded_at'].str.contains('1636', na=False),].shape[0]\nprint(f'There are {wrongDateCount} record(s) whose year is 1636')\ndfProcessed = dfProcessed.loc[~dfProcessed['founded_at'].str.contains('1636', na=False),]","396566d8":"### List of years starting with '00'. This was found while trying to convert the the column datatyp to datetime ###\n\nprint(dfProcessed.loc[dfProcessed['first_funding_at'].str.startswith('00', na=False), ['first_funding_at']])\nprint('\\n')\nfuncDateManipulation(dfProcessed, 'first_funding_at')\nprint('\\n')\nprint(dfProcessed.loc[dfProcessed['last_funding_at'].str.startswith('00', na=False), ['last_funding_at']])\nprint('\\n')\nfuncDateManipulation(dfProcessed, 'last_funding_at')","16e58a45":"# ## Found out while converting the object type to date type ###\n\ndfProcessed.loc[dfProcessed['first_funding_at'].str.contains('201-01-01', na=False),['first_funding_at']] = '2010-01-01'\ndfProcessed.loc[dfProcessed['last_funding_at'].str.contains('201-01-01', na=False),['last_funding_at']] = '2010-01-01'\nprint('Date sucessfully changed')","2db28f3e":"### Changing object type to date type ###\n\ndfProcessed['founded_at'] = dfProcessed['founded_at'].astype('datetime64[ns]')\ndfProcessed['first_funding_at'] = dfProcessed['first_funding_at'].astype('datetime64[ns]')\ndfProcessed['last_funding_at'] = dfProcessed['last_funding_at'].astype('datetime64[ns]')","8e9b5b47":"### Market variable ###\n \ndfProcessed['market'] = dfProcessed['market'].str.strip() \nmarketNAValues = dfProcessed.loc[(dfProcessed['market'].isna() == True),].shape[0]\nprint(f'There are in total {marketNAValues} rows for which the market has NaN value')\ndfProcessed.loc[(dfProcessed['market'].isna() == True),['market']] = 'Unknown' # Replacing NaN as UnKnown","dd7c5a8d":"### Market variable selecting only top 5, bottom 5 and mid 10 and one-hot encoding###\n\nmarket_variableOneHotCol = funcCustomOneHotEncode(dfProcessed, 'market')","e25bbc45":"### country_code variable selecting only top 5, bottom 5 and mid 10 and one-hot encoding###\n\ncountry_code_variableOneHotCol = funcCustomOneHotEncode(dfProcessed, 'country_code')","4d242a3c":"### region variable selecting only top 5, bottom 5 and mid 10 and one-hot encoding###\n\nregion_variableOneHotCol = funcCustomOneHotEncode(dfProcessed, 'region')","df00f5cb":"### Correcting total funding_rounds ###\n\nlstCol = ['seed', 'venture', 'equity_crowdfunding', 'undisclosed', \n       'convertible_note', 'debt_financing', 'grant', 'private_equity', 'post_ipo_equity', \n       'post_ipo_debt', 'secondary_market', 'product_crowdfunding', 'round_A', 'round_B', \n       'round_C', 'round_D', 'round_E', 'round_F', 'round_G', 'round_H']\n    \nprint('The list of variables associated with funding are \\n', LISTFUNDINGCOL)\ndfProcessed['r_Count']=dfProcessed.loc[:,lstCol].ge(1).sum(axis=1) # Counts the value across the rows only if the value is greater than 1 only\nincorrectcount = dfProcessed.loc[dfProcessed['r_Count'] != dfProcessed['funding_rounds'], ['r_Count', 'funding_rounds']].shape[0]\nprint(f'\\nThere are {incorrectcount} rows whose to funding rounds do not match with the actual funding rounds')\ndfProcessed['funding_rounds'] = dfProcessed['r_Count']\nlstDropColumns = ['r_Count']\nLISTCOLUMNNAME = funcColumnsToDrop(dfProcessed, lstDropColumns)","d28db87e":"### Handling outliers if any exist in int and float datatypes ###\n\ndictOutlierTuple = {}\n\nfor col in dfProcessed.columns:\n    if (((dfProcessed[col].dtype)=='float64') | ((dfProcessed[col].dtype)=='int64')):\n        percentiles = dfProcessed[col].quantile([.25, .75]).values\n        lowerQuantileValue = percentiles[0]\n        upperQuantileValue = percentiles[1]\n        dfProcessed.loc[dfProcessed[col] < lowerQuantileValue,col] = lowerQuantileValue\n        lowCount = dfProcessed.loc[dfProcessed[col] < lowerQuantileValue,col].shape[0]\n        dfProcessed.loc[dfProcessed[col] > upperQuantileValue, col] = upperQuantileValue\n        highCount = dfProcessed.loc[dfProcessed[col] > upperQuantileValue, col].shape[0]\n        dictOutlierTuple[col] = (lowerQuantileValue, upperQuantileValue, lowCount, highCount)\nprint(f'Rows affected (LowerQuantile Value, UpperQuantlie Value, LowerQuantlie Rows, Upperquantile Rows)\\n\\n {dictOutlierTuple}')","05c44313":"### Replaces NaN with single pipe and then checks the number of pipes in the string. If greater than two multi category \n### is marked as 1 and if not marked as 0.\n\ndfProcessed.loc[dfProcessed['category_list'].isna() == True,['category_list']] = '|'\ndfProcessed['f_Multi_Category'] = [1 if x.count('|') > 2 else 0 for x in dfProcessed['category_list']] \nlstDropColumns = ['category_list'] \nLISTCOLUMNNAME = funcColumnsToDrop(dfProcessed, lstDropColumns)","de625115":"dfProcessed.loc[dfProcessed['homepage_url'].isna() == True,['homepage_url']] = 'NAN'\ndfProcessed['f_URL'] = [1 if len(x) > 3 else 0 for x in dfProcessed['homepage_url']]\nlstDropColumns = ['homepage_url'] \nLISTCOLUMNNAME = funcColumnsToDrop(dfProcessed, lstDropColumns)","6989084a":"### Checking to impute founded year using the other coulmns\n\nprint(dfProcessed.loc[((dfProcessed['founded_month'].isna() == False) & (dfProcessed['founded_at'].isna() == True)) ,['founded_at','founded_month']])\nprint('\\n')\nprint(dfProcessed.loc[((dfProcessed['founded_quarter'].isna() == False) & (dfProcessed['founded_at'].isna() == True)) ,['founded_at', 'founded_quarter']])\nprint('\\n')\nprint(dfProcessed.loc[((dfProcessed['founded_year'].isna() == False) & (dfProcessed['founded_at'].isna() == True)) ,['founded_at', 'founded_year']])","23e7dc92":"### Calculating the age of the company and removing all the companies whose age is greater than 30 years\n\ndfProcessed['r_first_funding_year'] = pd.DatetimeIndex(dfProcessed['first_funding_at']).year\ndfProcessed['r_last_funding_year'] = pd.DatetimeIndex(dfProcessed['last_funding_at']).year\ndfProcessed['r_founded_year'] = pd.DatetimeIndex(dfProcessed['founded_at']).year\n\ndfProcessed['r_diff'] = dfProcessed['r_first_funding_year'] - dfProcessed['r_founded_year']\ndfProcessed['r_diff_mean'] = round(dfProcessed['r_diff'].mean(skipna=True),0)\ndfProcessed.loc[dfProcessed['r_founded_year'].isna() == True,['r_founded_year']] = dfProcessed['r_first_funding_year'] - dfProcessed['r_diff_mean']\n\ndfProcessed['f_age'] = 2020 - dfProcessed['r_founded_year']\nageCondRows = dfProcessed.loc[((dfProcessed['f_age'] > 30) | (dfProcessed['f_age'] < 0)),].shape[0]\nprint(f'There are {ageCondRows} whose age is greater than 30 or less than zero')\ndfProcessed = dfProcessed.loc[((dfProcessed['f_age'] <= 30) & (dfProcessed['f_age'] >= 0)),]","b851eaae":"### Changing the 'first funding year' whose value is less than 'founded year' to be the same as 'founded year' ###\n\ndfProcessed.loc[dfProcessed['r_first_funding_year'] < dfProcessed['r_founded_year'],['r_first_funding_year']] = dfProcessed['r_founded_year']\ndfProcessed['f_yearstoFirstFunding'] = dfProcessed['r_first_funding_year'] - dfProcessed['r_founded_year']","ae1a1c62":"### Changing the 'last funding year' whose value is less than 'first funding year' to be the same as 'first funding year' ###\n\ndfProcessed.loc[dfProcessed['r_last_funding_year'] < dfProcessed['r_first_funding_year'] ,['r_last_funding_year']] = dfProcessed['r_first_funding_year']\ndfProcessed['f_FirstFundingToLastFunding'] = dfProcessed['r_last_funding_year'] - dfProcessed['r_first_funding_year']","f0c48257":"### Dropping columns related to dates ###\n\nlstDropColumns = ['r_first_funding_year', 'r_last_funding_year', 'r_founded_year', 'r_diff', 'r_diff_mean', 'founded_at', 'founded_at', 'founded_month',\n       'founded_quarter', 'founded_year', 'first_funding_at','last_funding_at']\nLISTCOLUMNNAME = funcColumnsToDrop(dfProcessed, lstDropColumns)","db0d845f":"### Label Encoding Target Variable ### \n\nlabel_encoder = preprocessing.LabelEncoder() \ndfProcessed['status']= label_encoder.fit_transform(dfProcessed['status']) ","cb5f2b4b":"ROWS = dfProcessed.shape[0]\nCOLUMNS = dfProcessed.shape[1]\nprint(f'Total number of rows in the final dataset : {ROWS} \\nTotal number of columns in the final dataset : {COLUMNS}')","5cf1c75e":"### TRAIN TEST SPLIT ###\nX = dfProcessed.loc[:, ~dfProcessed.columns.isin(['status'])] \ny = dfProcessed.loc[:, ['status']] \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\ndel X, y","fd665b2c":"dummy_clf = DummyClassifier(strategy=\"most_frequent\")","f2d70bb1":"dummy_clf.fit(X_train, y_train)","cc1ebc54":"predictedValues = dummy_clf.predict(X_test)\nprint('Sucessfully predicted the values')","ea30f850":"recallScores, precisionScores, f1Scores = funcCustomCVScore(fncp_X_train=X_train, \n                                                            fncp_y_train=y_train, \n                                                            fncpKFold=10,\n                                                            fncpBaseModel=DummyClassifier,\n                                                            fncpBaseModelParam={'strategy':'most_frequent'},\n                                                            fncpRandomState=123,\n                                                            fncpScoreAverage='weighted')","ab02d6aa":"fncpModelEvaluvate(fncpActual=y_test, \n                   fncpPredicted=predictedValues, \n                   fncpBoolHeatMap=False, \n                   fncpMultiClass=True,\n                   fncpAverageType='weighted')","f1581656":"classiRandomForest = RandomForestClassifier()","81aefba1":"classiRandomForest.fit(X_train, y_train)\npredictedValues = classiRandomForest.predict(X_test)\nprint('Sucessfully predicted the values')","425c6656":"classiRandomForest.fit(X_train, y_train)","fbb0429e":"predictedValues = classiRandomForest.predict(X_test)\nprint('Sucessfully predicted the values')","9b3763be":"recallScores, precisionScores, f1Scores = funcCustomCVScore(fncp_X_train=X_train, \n                                                            fncp_y_train=y_train, \n                                                            fncpKFold=10,\n                                                            fncpBaseModel=RandomForestClassifier,\n                                                            fncpBaseModelParam=None,\n                                                            fncpRandomState=123,\n                                                            fncpScoreAverage='weighted')","b22bd395":"fncpModelEvaluvate(fncpActual=y_test, \n                   fncpPredicted=predictedValues, \n                   fncpBoolHeatMap=False, \n                   fncpMultiClass=True,\n                   fncpAverageType='weighted')","c4838c8b":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'log2']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(f'The hyperparameters are \\n{random_grid}')","33c0dbfd":"start_time = time.time()\nmodel = RandomForestClassifier()\ncurrent_time = datetime.datetime.now(pytz.timezone('Asia\/Kolkata')).strftime(\"%d%m%Y_%H%M%S\")\nmodelFileName = PATH+'\/Model\/cv_model_'+ current_time +'.sav'\nprint(f'Model training start time : {current_time}\\n')\nrf_random = RandomizedSearchCV(estimator=model, param_distributions=random_grid, n_iter=100, cv=3, verbose=3, random_state=42, n_jobs=-1)\nrf_random.fit(X_train, y_train)\n\n# Saving Model #\npickle.dump(rf_random, open(modelFileName, 'wb'))\n\nprint(f'Minutes taken to complete training : {(time.time() - start_time)\/60}')","ad5d694a":"### Loading saved model ###\nchosenFilePath = r'\/content\/gdrive\/My Drive\/AIML\/DIY_Programs\/MLPipeLine\/Model\/cv_model_24082020_110100.sav'\nloaded_model = pickle.load(open(chosenFilePath, 'rb'))\nprint('Sucessfully loaded the model!!!')","ca023a99":"for key, val in loaded_model.best_params_.items():\n  print(f'The best \"{key}\" hyperparameter is: {val} ')","25836e72":"### Feature Importance ###\ndfImportantFeature = funcFeatureImportance(loaded_model, X_train, fncpCV=True)","8ff14cd2":"recallScores, precisionScores, f1Scores = funcCustomCVScore(fncp_X_train=X_train, \n                                                            fncp_y_train=y_train, \n                                                            fncpKFold=10,\n                                                            fncpBaseModel=RandomForestClassifier, \n                                                            fncpBaseModelParam=loaded_model.best_params_,\n                                                            fncpRandomState=123,\n                                                            fncpScoreAverage='weighted')","760a2182":"predictedValues = loaded_model.best_estimator_.predict(X_test)\nprint('Sucessfully predicted the values')","d98f0557":"# Test Dataset\n\nfncpModelEvaluvate(y_test, predictedValues, fncpBoolHeatMap=False, fncpMultiClass=True,fncpAverageType='weighted')","70ace9da":"print('Before under and over SMOTE')\ncounter = Counter(y_train['status'].array)\nprint(counter)\n\n# define pipeline\ndictOver = {0: 12000, 1:12000}\nover = SMOTE(sampling_strategy=dictOver)\ndictUnder = {2: 20000}\nunder = RandomUnderSampler(sampling_strategy=dictUnder)\nsteps = [('o', over), ('u', under)]\npipeline = Pipeline(steps=steps)\n\n# transform the dataset\nX_new, y_new = pipeline.fit_resample(X_train, y_train)\nX_new, y_new = shuffle(X_new, y_new) # Shuffles the arrays\n\nprint('\\n')\nprint('After under and over SMOTE')\ncounter = Counter(y_new)\nprint(counter)\n\n# Converts the array into a dataframe \nX_new = pd.DataFrame(data=X_new, columns=X_train.columns.to_list())\ny_new = pd.DataFrame(data=y_new, columns=y_train.columns.to_list())\nprint('\\nSMOTE dataframe sucessfully created')","c8cb58fb":"dummy_clf = DummyClassifier(strategy=\"most_frequent\")","fcf0e876":"dummy_clf.fit(X_new, y_new)","55e06d3d":"predictedValues = dummy_clf.predict(X_test)\nprint('Sucessfully predicted the values')","c86889c2":"recallScores, precisionScores, f1Scores = funcCustomCVScore(fncp_X_train=X_new, \n                                                            fncp_y_train=y_new, \n                                                            fncpKFold=10,\n                                                            fncpBaseModel=DummyClassifier,\n                                                            fncpBaseModelParam={'strategy':'most_frequent'},\n                                                            fncpRandomState=123,\n                                                            fncpScoreAverage='weighted')","d3d7e979":"fncpModelEvaluvate(fncpActual=y_test, \n                   fncpPredicted=predictedValues, \n                   fncpBoolHeatMap=False, \n                   fncpMultiClass=True,\n                   fncpAverageType='weighted')","f52f3596":"classiRandomForest = RandomForestClassifier()","817dcc44":"classiRandomForest.fit(X_new, y_new)","09b42dab":"predictedValues = classiRandomForest.predict(X_test)\nprint('Sucessfully predicted the values')","caa4057a":"recallScores, precisionScores, f1Scores = funcCustomCVScore(fncp_X_train=X_new, \n                                                            fncp_y_train=y_new, \n                                                            fncpKFold=10,\n                                                            fncpBaseModel=RandomForestClassifier,\n                                                            fncpBaseModelParam=None,\n                                                            fncpRandomState=123,\n                                                            fncpScoreAverage='weighted')","acd785fa":"fncpModelEvaluvate(fncpActual=y_test, \n                   fncpPredicted=predictedValues, \n                   fncpBoolHeatMap=False, \n                   fncpMultiClass=True,\n                   fncpAverageType='weighted')","e733fa1b":"print(f'The hyperparameters are \\n{random_grid}')","5cc782db":"start_time = time.time()\nmodel = classiRandomForest\ncurrent_time = datetime.datetime.now(pytz.timezone('Asia\/Kolkata')).strftime(\"%d%m%Y_%H%M%S\")\nmodelFileName = PATH+'\/Model\/cv_SMOTE_model_'+ current_time +'.sav'\nprint(f'Model training start time : {current_time}\\n')\nrf_random = RandomizedSearchCV(estimator=model, param_distributions=random_grid, n_iter=100, cv=3, verbose=3, random_state=42, n_jobs=-1)\nrf_random.fit(X_new, y_new)\n\n# Saving Model #\npickle.dump(rf_random, open(modelFileName, 'wb'))\nsmote_loaded_model = pickle.load(open(modelFileName, 'rb'))\nprint(f'Minutes taken to complete training : {(time.time() - start_time)\/60}')","cf2ab14b":"### Loading Model ###\nfileName = r'\/content\/gdrive\/My Drive\/AIML\/DIY_Programs\/MLPipeLine\/Model\/cv_SMOTE_model_04092020_140033.sav'\nsmote_loaded_model = pickle.load(open(fileName, 'rb'))\nprint('Sucessfully loaded the model!!!')","d58de195":"for key, val in smote_loaded_model.best_params_.items():\n  print(f'The best \"{key}\" hyperparameter is: {val} ')","21135fc1":"### Feature Importance ###\ndfSMOTEImportantFeature = funcFeatureImportance(smote_loaded_model, X_train, fncpCV=True)","b1e08d78":"recallScores, precisionScores, f1Scores = funcCustomCVScore(fncp_X_train=X_new, \n                                                            fncp_y_train=y_new, \n                                                            fncpKFold=10,\n                                                            fncpBaseModel=RandomForestClassifier, \n                                                            fncpBaseModelParam=smote_loaded_model.best_params_,\n                                                            fncpRandomState=123,\n                                                            fncpScoreAverage='weighted')","9b47ed7f":"smotePredictedValues = smote_loaded_model.best_estimator_.predict(X_test)\nprint('Sucessfully predicted the values')","0c9f9985":"fncpModelEvaluvate(y_test, smotePredictedValues, fncpBoolHeatMap=False, fncpMultiClass=True,fncpAverageType='weighted')","61f97e35":"UserInput = [[1.0e+00, 0.0e+00, 7.0e+06, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00,\n        0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00,\n        0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00,\n        1.4e+07, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00,\n        0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00,\n        0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00,\n        1.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00,\n        0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00,\n        0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00,\n        0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00,\n        0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00,\n        0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 0.0e+00, 1.0e+00,\n        2.4e+01, 1.7e+01, 0.0e+00]]","25c30fb7":"predictedValue = smote_loaded_model.best_estimator_.predict(UserInput)\nprdictedClass = label_encoder.inverse_transform(predictedValue)\nprint(f'The predicted value is \"{prdictedClass.item()}\"')","b5eb8264":"# Exploratory Data Analysis\n\n","7c49903c":"#### Cross Validation Score","9611d4db":"# Data Preprocessing","f5051a7f":"###### Random Search CV Training","ea30b1a6":"###### Gridsearch Parameters","d9ca3948":"#### Cross Validation Score","0a378c6c":"###### Test Model Evaluation\n","224e1970":"#### Date variable\n","be3e5bfa":"#### Cross Validation score","af9f27a8":"##### Cross Validation Score","be6920a5":"#### Test Model Evaluation","573480a6":"### Random forest with optimum parameters","481c84f8":"## Imbalanced dataset","6d9b41b7":"# Model Building","f2ff945c":"#### Model Training","43b80258":"# Constants","9a948bcf":"#### Model Prediction","4c1e7c22":"#### Test Model Evaluation","b072bb9a":"### Vanilla Randomforest Classifier\n","8bd50efb":"#### Test Model Evaluation","96023c09":"###### Model Prediction","067d8ece":"### Vanilla Randomforest Classifier","9dae8419":"### Dummy Classifier ###\n","6c28ef15":"##### Test Model Evaluation","cab41f8f":"#### Model Prediction","4010cf7d":"#### Market variable","7fceca06":"#### Random Search CV Training","f01edf84":"#### Model Prediction","7700bc67":"#### Funding Rounds","a24c6a82":"#### Handling Outliers","9da3e7ba":"#### Dropping Rows and Columns ###\n","a3ebf217":"#### Cross Validation Score","ba8c9d80":"#### Strippping white spaces","d4ba75df":"#### Region Variables","c42f82b9":"#### Model Prediction ","2da55b04":"##### Model Prediction","02c6126b":"# Dependancies","79de9d39":"##### Model Training","6d8940c7":"## Balanced Dataset","e7b52daf":"### Random forest with optimum parameters","68d1c16c":"#### Year Related Columns","729cac27":"# Model Deployment","4e86dc85":"#### Homepage_URL","6f2201fa":"# Feature Engineering","360a3ae3":"# Functions","d47525c4":"#### Model Training","258efd93":"#### Grid Search Parameters","f5444fb0":"###### Cross Validation Score","4efb17b7":"#### Target Variable","eb64b4b1":"#### Test Model Evaulation  ","4a98a2b2":"#### Category List","d52ff4d9":"#### Model Training","5114613f":"### Dummy Classifier"}}