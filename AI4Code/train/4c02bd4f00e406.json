{"cell_type":{"0beaff2d":"code","eca7002b":"code","582d472f":"code","bb797553":"code","a4b60eeb":"code","f85df10d":"code","c48e0e29":"markdown","0477c14e":"markdown","43bdf472":"markdown","8d1ce167":"markdown"},"source":{"0beaff2d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","eca7002b":"import pandas as pd\nimport numpy as nm\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nimport cv2 as cv2\nimport os","582d472f":"\nallFeatures=[]\n\nfor filename in os.listdir('..\/input\/'):\n    img = cv2.imread('..\/input\/'+filename)\n    img1 = cv2.resize(img, (240,240), interpolation = cv2.INTER_AREA)\n    mask = nm.zeros(img1.shape[:2],nm.uint8)\n    bgdModel = nm.zeros((1,65),nm.float64)\n    fgdModel = nm.zeros((1,65),nm.float64)\n    rect = (5,5,235,235)\n    cv2.grabCut(img1,mask,rect,bgdModel,fgdModel,10,cv2.GC_INIT_WITH_RECT)\n    mask2 = nm.where((mask==2)|(mask==0),0,1).astype('uint8')\n    img1 = img1*mask2[:,:,nm.newaxis]\n    img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n    #img1= cv2.GaussianBlur(img1,(5,5),cv2.BORDER_DEFAULT)\n    ll=cv2.equalizeHist(img1)\n    orb = cv2.ORB_create(nfeatures=200)\n    keypoints, descriptors = orb.detectAndCompute(ll, None)\n    allFeatures.append(descriptors)#array[ImageNb][FeatureNb]","bb797553":"kmeans = KMeans(n_clusters = 200, n_init=10, init='random')\ngg=[item for sublist in allFeatures for item in sublist]\nkmeans.fit(gg)#vocabulary","a4b60eeb":"documentsPerFeature=[0] * 200\n\nalla=[]\nfor i in allFeatures:\n    clusters=[]\n    for u in i:\n        cluster=kmeans.predict([u])  \n        clusters.append(cluster[0])\n        \n    nums=pd.Series(clusters).value_counts()\n    tf=nums.apply(lambda a:a\/nums.sum())\n    alla.append(tf.to_dict())\n#tf for each feature in the document\n\nfor i in alla:\n    for k in i:\n        documentsPerFeature[k-1]=documentsPerFeature[k-1]+1\n#number of documents containing a given feature","f85df10d":"import math as m\nfr= pd.DataFrame(columns= range(1,201))\nfor i in alla:\n    row=[0]*200\n    totalWords=0\n    for k in i:\n        idf=m.log(len(alla)\/documentsPerFeature[k-1])\n        res=i[k]*idf\n        row[k]=res\n    fr=fr.append(pd.Series(row, range(1,201)),ignore_index=True )  \nfr       \n    ","c48e0e29":"Calculate TF IDF\nCreate bag of Visual Words ","0477c14e":"Calculate TF and the number of documents per a given feature","43bdf472":"Create 200 clusters with a help of K-means","8d1ce167":"- Load photos containing food images\n- Remove background\n- Convert to grayscale\n- Apply histogram equalization\n- Extract image features with help of ORB "}}