{"cell_type":{"86e30a06":"code","9d810f3c":"code","21ea7c63":"code","c9602951":"code","b5a1a016":"code","e4928903":"code","7d8322e4":"code","4b486c68":"code","615dcde4":"code","81928c7b":"code","d1a68e89":"code","84fba998":"code","e830b430":"code","06078f48":"code","dea7f00e":"code","66a9ca7c":"code","d1e877b6":"code","b55d0587":"code","ba3f3cc8":"code","14831186":"code","84039c29":"code","7ab9d9f0":"code","58d33ca9":"code","c2e2ccbf":"code","2247405c":"code","89dd6e8b":"code","f8757a52":"code","60bbee42":"code","e1fe6147":"code","1def96c7":"code","6ea6c06f":"code","0cf8f6e7":"code","9fdffd74":"code","6fb8d6ea":"code","e8da4ab3":"code","809d08c4":"code","162a1334":"code","309deb70":"code","9f8f87d7":"code","e83a2b3e":"code","f7fbed10":"code","c78bf51a":"code","a0bec20f":"code","c1d7753f":"code","471d1cd3":"code","1d318734":"code","5bb34266":"code","4ff9f772":"code","d1ed545b":"code","cf9b2e80":"code","da0e478c":"code","f4dabd2e":"code","24872a33":"code","ba65789d":"code","1db7a893":"code","c5515a93":"code","195d0cb7":"code","dedbcea5":"code","d888e34d":"code","2ed374a7":"code","2cf81761":"code","672d760e":"code","1847e3d2":"code","635e23cd":"code","960d6270":"code","ee24cc07":"code","08f96cc5":"code","03cf584c":"code","fedc25a1":"code","191f6839":"code","7328d1e9":"code","9586d55d":"code","ab91a1b0":"code","53de6140":"code","21f6caaf":"code","168beaf2":"code","436275df":"code","32445b23":"code","6b6454c0":"code","87ba79bf":"code","89e4e816":"code","6578ab74":"code","6ab5d44b":"code","0241a6b4":"code","77ae45e6":"code","ab0addd0":"code","abad8b04":"code","e267f3e9":"code","914fb3f1":"code","ea914f81":"code","63c86943":"code","5eef0a86":"code","6fb5e5e6":"code","a9b3b971":"code","f8b7a613":"code","6547b1a9":"code","b23902cf":"code","46c9b157":"code","fb63c505":"code","fae84397":"code","82342ab4":"code","24a709b6":"code","8fa9f5bb":"code","67f18d13":"code","583ae8bc":"code","169ea071":"code","f5e0458a":"code","54fdb680":"code","67ad9a1d":"code","828990d1":"code","cec845b1":"code","433aff04":"code","2cba3be4":"code","164dfaf8":"code","f1b99d7f":"code","fa8a64a3":"markdown","38ae8831":"markdown","c9242f1b":"markdown","6be65f26":"markdown","4b1a2af4":"markdown","876b2fa7":"markdown","ac9688ea":"markdown","21b55052":"markdown","72f00ff1":"markdown","b8ef340a":"markdown","3e3053c7":"markdown","4520dbdd":"markdown","fa576e07":"markdown","286e686f":"markdown","121999f5":"markdown","9906d989":"markdown","1e889bf0":"markdown","80a798af":"markdown","bde008cc":"markdown","a862b78a":"markdown","09cfed57":"markdown","9219397c":"markdown","0d15cc0f":"markdown","67500f02":"markdown","611b31e1":"markdown","f9df08a5":"markdown","bbd7669a":"markdown","a7582a9e":"markdown","8596e09d":"markdown","91736183":"markdown","2b29feaa":"markdown","e059db5c":"markdown","942be906":"markdown","191c5df2":"markdown","dc086bd9":"markdown","34fda744":"markdown","88ab2bd7":"markdown","56f0f3e4":"markdown","932c07ca":"markdown","9cd4d31a":"markdown","521a6707":"markdown","c25eec75":"markdown","d6203184":"markdown","b27eb0de":"markdown","576c9a56":"markdown","be9f1c50":"markdown","6e2d99e0":"markdown","81e3f42f":"markdown","702c23fe":"markdown","bd120815":"markdown","282e0962":"markdown","0f052d8b":"markdown","502b2221":"markdown","df712e0f":"markdown","bd7e79fe":"markdown","e6560874":"markdown","1cf0d620":"markdown","d7f61e2a":"markdown","688e3f8d":"markdown","6fcc2eb6":"markdown","488ffec2":"markdown","2d5c79f3":"markdown","14582e70":"markdown","8df895c5":"markdown","43229851":"markdown","22633153":"markdown","2cf97667":"markdown","ab1191fb":"markdown","9911f7b7":"markdown","cbd9f9cc":"markdown","0e1e682b":"markdown","5439e59f":"markdown","e6cfcad4":"markdown","67e97732":"markdown","3b8d9979":"markdown","795c7b35":"markdown","dbf124eb":"markdown","b8bd1a1c":"markdown","f2a58d0f":"markdown","a16e49bf":"markdown","071e7afa":"markdown","4318f541":"markdown","8d11adc2":"markdown","f53210e2":"markdown","15f090dd":"markdown","645b9101":"markdown","06ee3d0d":"markdown","6c7d0ff0":"markdown","05c678d2":"markdown","6a20f220":"markdown","bff4b7ea":"markdown","536f45e3":"markdown","30ac636c":"markdown","fb99d95b":"markdown","d12f9ef0":"markdown","b626efb9":"markdown","afd76ce8":"markdown"},"source":{"86e30a06":"# General tools\nimport os\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom IPython.display import Image, HTML\nfrom plotnine import *\nimport pydot\nimport math\nimport warnings\nimport time\nwarnings.filterwarnings(\"ignore\")\n\n# For transformations and predictions\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.svm import LinearSVC, SVC\nfrom xgboost import XGBClassifier\n\n# For scoring\nfrom sklearn.metrics import confusion_matrix, cohen_kappa_score\nfrom sklearn.metrics import plot_confusion_matrix, confusion_matrix\nfrom sklearn.metrics import make_scorer\n\n# For validation\nfrom sklearn.model_selection import train_test_split as split\n\nsns.set(style='whitegrid', palette='deep', font='sans-serif', font_scale=1, color_codes=True, rc=None)","9d810f3c":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","21ea7c63":"path = '..\/input\/wine-quality\/winequalityN.csv'\ndf = pd.read_csv(path)","c9602951":"# Showing 5 first lines of the data\ndf.head()","b5a1a016":"df.info()","e4928903":"numeric_columns = df.columns[df.dtypes != 'object']\nstring_columns = df.columns[df.dtypes == 'object']\nprint(f'There are {len(numeric_columns)} numeric columns & {len(string_columns)} string columns')","7d8322e4":"# Numeric Heatmap\nnumeric_df = pd.DataFrame(data=df, columns=numeric_columns, index=df.index)\ncorr = np.abs(numeric_df.corr())\nfig, ax = plt.subplots(figsize=(8, 8))\ncmap = sns.color_palette(\"rainbow\")\nsns.heatmap(corr, cmap=cmap, square=True)\nplt.title('Correlation between numerical features: abs values')\nplt.show()","4b486c68":"series = np.abs(corr['quality']).sort_values(ascending=False)\nprint('The most linear correlated features to QUALITY are:')\nfor i, row in enumerate(series):\n    if 0.2 <= row < 1:\n      print(f'{series.index[i]:17} --> {row: .2f} (abs)')","615dcde4":"# Seaborn pairplot\nsns_plot = sns.pairplot(df, hue='type', height=3)\nplt.show()","81928c7b":"df.duplicated().sum()","d1a68e89":"df['type'].loc[df.duplicated()==1].value_counts()","84fba998":"df.groupby('type')['quality'].count().to_frame()","e830b430":"fig, ax = plt.subplots(figsize=(12,4))\npd.options.display.float_format = '{:,.2f}'.format\n\nbar_chart = df.groupby(['type','quality'])['quality'].count().unstack('type')\nbar_chart= (bar_chart.T\/bar_chart.T.sum()).T\nax = bar_chart.plot(kind='bar', stacked=True, color=['r','w'], edgecolor='black', ax=ax)\n\nlabels = []\nfor j in bar_chart.columns:\n    for i in bar_chart.index:\n          label = str('{0:.2%}'.format(bar_chart.loc[i][j]))\n          labels.append(label)\n\npatches = ax.patches\n\nfor label, rect in zip(labels, patches):\n    width = rect.get_width()\n    if width > 0:\n        x = rect.get_x()\n        y = rect.get_y()\n        height = rect.get_height()\n        ax.text(x + width\/2., y + height\/2., label, ha='center', va='center', color='black')\n\nax.set_xticklabels(labels=ax.get_xticklabels(), rotation=0)\nax.set_yticklabels(labels='')\nax.set_ylabel('% of records')\nplt.legend(bbox_to_anchor = (1, 1.01), edgecolor='black')\nplt.show()","06078f48":"data = df.groupby('type')['quality'].count()\nfig, ax = plt.subplots(figsize=[10,6])\nlabels = ['red','white']\nax = plt.pie(x=data, autopct=\"%.1f%%\", explode=[0.05]*2, labels=labels, colors=['darkred','white'],\n             wedgeprops={\"edgecolor\":\"black\"},pctdistance=0.5)\nplt.show()","dea7f00e":"fig, ax = plt.subplots(figsize=(10,5))\ncolors = ['w', 'r']\nsns.set_palette(sns.color_palette(colors))\nax = sns.violinplot(data=df ,x=df.type ,y=df.quality)\n\nax.set_ylabel('Quality', fontsize=14)\nplt.tight_layout()","66a9ca7c":"df['fixed acidity'].isnull().sum()","d1e877b6":"df['fixed acidity'].describe()","b55d0587":"colors = ['r']\nsns.set_palette(sns.color_palette(colors))\ndf['fixed acidity'].hist(bins=40)\nplt.show()","ba3f3cc8":"colors = ['r', 'w']\nsns.set_palette(sns.color_palette(colors))\nfig, ax = plt.subplots(1, figsize=(12, 5), sharey=True, sharex = True)\ndata =  df.groupby(['quality','type'])['fixed acidity'].mean().reset_index()\ns = df.groupby(['quality','type'])['fixed acidity'].count().reset_index()\ndata = pd.concat([data, s['fixed acidity']], axis=1)\ndata.columns = ['quality', 'type', 'fixed acidity', 'size']\nax = sns.scatterplot(x='quality', y='fixed acidity', hue='type', cmap=colors, data=data, size='size', sizes=(100,1000), legend='brief')\nax.legend(bbox_to_anchor=(1.12,1), markerscale=0.4, facecolor ='lightgrey')\nax.set_title('')\nax.set_facecolor('black')\nax.set_ylabel('Mean fixed acidity', fontsize=12)\nplt.tight_layout()","14831186":"df['volatile acidity'].isnull().sum()","84039c29":"df['volatile acidity'].describe()","7ab9d9f0":"colors = ['r']\ndf['volatile acidity'].hist(bins=40)\nplt.show()","58d33ca9":"fig, ax = plt.subplots(1, figsize=(12, 5), sharey=True, sharex = True)\nax.set_facecolor('lightgrey')\ncolors = ['w', 'r']\nsns.set_palette(sns.color_palette(colors))\nsns.boxplot(data=df,y='volatile acidity',x='quality',hue='type')\nplt.show()","c2e2ccbf":"df['citric acid'].isnull().sum()","2247405c":"df['citric acid'].describe()","89dd6e8b":"colors = ['r']\nsns.set_palette(sns.color_palette(colors))\ndf['citric acid'].hist(bins=40)\nplt.show()","f8757a52":"fig, ax = plt.subplots(1, figsize=(12, 5), sharey=True, sharex = True)\nax.set_facecolor('lightgrey')\ncolors = ['w', 'r']\nsns.set_palette(sns.color_palette(colors))\nsns.boxplot(data=df,y='citric acid',x='quality',hue='type')\nplt.show()","60bbee42":"colors = ['r', 'w']\nsns.set_palette(sns.color_palette(colors))\nfig, ax = plt.subplots(1, figsize=(12, 5), sharey=True, sharex = True)\ndata =  df.groupby(['quality','type'])['citric acid'].mean().reset_index()\ns = df.groupby(['quality','type'])['citric acid'].count().reset_index()\ndata = pd.concat([data, s['citric acid']], axis=1)\ndata.columns = ['quality', 'type', 'citric acid', 'size']\nax = sns.scatterplot(x='quality', y='citric acid', hue='type', cmap=colors, data=data, size='size', sizes=(100,1000), legend='brief')\nax.legend(bbox_to_anchor=(1.12,1), markerscale=0.4, facecolor ='lightgrey')\nax.set_title('')\nax.set_facecolor('black')\nax.set_ylabel('Mean citric acid', fontsize=12)\nplt.tight_layout()","e1fe6147":"df['residual sugar'].isnull().sum()","1def96c7":"df['residual sugar'].describe()","6ea6c06f":"colors = ['r']\nsns.set_palette(sns.color_palette(colors))\ndf['residual sugar'].hist(bins=30)\nplt.show()","0cf8f6e7":"fig, ax = plt.subplots(1, figsize=(12, 5), sharey=True, sharex = True)\nax.set_facecolor('lightgrey')\ncolors = ['w', 'r']\nsns.set_palette(sns.color_palette(colors))\nsns.boxplot(data=df[df['residual sugar']<40],y='residual sugar',x='quality',hue='type')\nplt.show()","9fdffd74":"df['chlorides'].isnull().sum()","6fb8d6ea":"df['chlorides'].describe()","e8da4ab3":"colors = ['r']\nsns.set_palette(sns.color_palette(colors))\ndf['chlorides'].hist(bins=30)\nplt.show()","809d08c4":"fig, ax = plt.subplots(1, figsize=(12, 5), sharey=True, sharex = True)\nax.set_facecolor('lightgrey')\ncolors = ['w', 'r']\nsns.set_palette(sns.color_palette(colors))\nsns.boxplot(data=df,y='chlorides',x='quality',hue='type')\nplt.show()","162a1334":"colors = ['r', 'w']\nsns.set_palette(sns.color_palette(colors))\nfig, ax = plt.subplots(1, figsize=(12, 5), sharey=True, sharex = True)\ndata =  df.groupby(['quality','type'])['chlorides'].mean().reset_index()\ns = df.groupby(['quality','type'])['chlorides'].count().reset_index()\ndata = pd.concat([data, s['chlorides']], axis=1)\ndata.columns = ['quality', 'type', 'chlorides', 'size']\nax = sns.scatterplot(x='quality', y='chlorides', hue='type', cmap=colors, data=data, size='size', sizes=(100,1000), legend='brief')\nax.legend(bbox_to_anchor=(1.12,1), markerscale=0.4, facecolor ='lightgrey')\nax.set_title('')\nax.set_facecolor('black')\nax.set_ylabel('Mean chlorides', fontsize=12)\nplt.tight_layout()","309deb70":"df['free sulfur dioxide'].isnull().sum()","9f8f87d7":"df['free sulfur dioxide'].describe()","e83a2b3e":"# Looking at the outlier\ndf.loc[df['free sulfur dioxide']>150]","f7fbed10":"# Histogram without the outlier\nFSD = df.loc[df['free sulfur dioxide']<150]\nplt.hist(FSD['free sulfur dioxide'], bins=20, color='r')\nplt.xlabel('FSD level')\nplt.ylabel('# of records')\nplt.show()","c78bf51a":"# Creating the canvas & colors\nfig, ax = plt.subplots(figsize=(15, 5))\ncolors = ['w', 'r']\nsns.set_palette(sns.color_palette(colors))\nax.set_facecolor('lightgrey')\n\n# data\nax = sns.boxplot(data=FSD, x='quality', y='free sulfur dioxide', hue='type')\nax.set_title('Free SO2 levels vs. quality category')\nax.set_ylabel('Free SO2')\nax.set_xlabel('Quality')\nplt.show()","a0bec20f":"colors = ['r', 'w']\nsns.set_palette(sns.color_palette(colors))\nfig, ax = plt.subplots(1, figsize=(12, 5), sharey=True, sharex = True)\ndata =  df.groupby(['quality','type'])['free sulfur dioxide'].mean().reset_index()\ns = df.groupby(['quality','type'])['free sulfur dioxide'].count().reset_index()\ndata = pd.concat([data, s['free sulfur dioxide']], axis=1)\ndata.columns = ['quality', 'type', 'free sulfur dioxide', 'size']\nax = sns.scatterplot(x='quality', y='free sulfur dioxide', hue='type', cmap=colors, data=data, size='size', sizes=(100,1000), legend='brief')\nax.legend(bbox_to_anchor=(1.12,1), markerscale=0.4, facecolor ='lightgrey')\nax.set_title('')\nax.set_facecolor('black')\nax.set_ylabel('Mean free sulfur dioxide', fontsize=12)\nplt.tight_layout()","c1d7753f":"df['total sulfur dioxide'].isnull().sum()","471d1cd3":"df['total sulfur dioxide'].describe()","1d318734":"plt.hist(df['total sulfur dioxide'], bins=40)\nplt.xlabel('Total SO2')\nplt.ylabel('# of records')\nplt.show()","5bb34266":"df.loc[df['total sulfur dioxide']>325,:]","4ff9f772":"# Creating the canvas & colors\nfig, ax = plt.subplots(figsize=(15, 5))\ncolors = ['w', 'r']\nsns.set_palette(sns.color_palette(colors))\nax.set_facecolor('lightgrey')\n\n# Data\nax = sns.boxplot(data=df, x='quality', y='total sulfur dioxide', hue='type')\nax.set_ylabel('Total SO2')\nax.set_xlabel('Quality')\nplt.show()","d1ed545b":"fig, ax = plt.subplots(figsize=(15,4))\nax.set_facecolor('darkgrey')\ncolors = ['r', 'w']\nsns.set_palette(sns.color_palette(colors))\n\nax = sns.lineplot(x='quality',\n                  y='total sulfur dioxide',\n                  hue='type', \n                  data = df.groupby(['quality','type'])['total sulfur dioxide'].mean().reset_index())\nax.set_ylabel('Mean total sulfur dioxide')\nplt.show()","cf9b2e80":"df.density.isnull().sum()","da0e478c":"df.density.describe()","f4dabd2e":"fig, ax = plt.subplots(figsize=(12,5))\nax.set_facecolor('darkgrey')\ncolors = ['w', 'r']\nsns.set_palette(sns.color_palette(colors))\n\nsns.boxplot(data=df[df['density']<1.03], y='density', x='quality', hue='type')\nplt.show()","24872a33":"df.loc[df.density>=1.005,:]","ba65789d":"fig, ax = plt.subplots(figsize=(15,4))\nax.set_facecolor('darkgrey')\ncolors = ['r', 'w']\nsns.set_palette(sns.color_palette(colors))\n\nax = sns.lineplot(x='quality',\n                  y='density',\n                  hue='type', \n                  data = df.groupby(['quality','type'])['density'].mean().reset_index())\nax.set_ylabel('Mean density')\nplt.show()","1db7a893":"fig, ax = plt.subplots(figsize=(12,5))\nax.set_facecolor('darkgrey')\ncolors = ['w', 'r']\nsns.set_palette(sns.color_palette(colors))\n\nsns.boxplot(data=df, y='pH', x='quality', hue='type')\nplt.show()","c5515a93":"# Null values\ndf.loc[df['pH'].isnull() == 1, :]","195d0cb7":"df['pH'].loc[df['type']=='red'].mean()","dedbcea5":"df['pH'].loc[df['type']=='white'].mean()","d888e34d":"colors = ['r', 'w']\nsns.set_palette(sns.color_palette(colors))\nfig, ax = plt.subplots(1, figsize=(12, 5), sharey=True, sharex = True)\ndata =  df.groupby(['quality','type'])['pH'].mean().reset_index()\ns = df.groupby(['quality','type'])['pH'].count().reset_index()\ndata = pd.concat([data, s['pH']], axis=1)\ndata.columns = ['quality', 'type', 'pH', 'size']\nax = sns.scatterplot(x='quality', y='pH', hue='type', cmap=colors, data=data, size='size', sizes=(100,1000), legend='brief')\nax.legend(bbox_to_anchor=(1.12,1), markerscale=0.4, facecolor ='lightgrey')\nax.set_title('')\nax.set_facecolor('black')\nax.set_ylabel('Mean pH', fontsize=12)\nplt.tight_layout()","2ed374a7":"# Looking at null values\ndf.loc[df.sulphates.isnull()==1]","2cf81761":"df['sulphates'].loc[df['type']=='white'].mean()","672d760e":"df['sulphates'].loc[df['type']=='red'].mean()","1847e3d2":"fig, ax = plt.subplots(figsize=(12,5))\nax.set_facecolor('darkgrey')\ncolors = ['w', 'r']\nsns.set_palette(sns.color_palette(colors))\nsns.stripplot(data=df, y='sulphates', x='quality', hue='type', alpha=0.5, orient='v')\nplt.show()","635e23cd":"fig, ax = plt.subplots(figsize=(15,4))\nax.set_facecolor('darkgrey')\ncolors = ['r', 'w']\nsns.set_palette(sns.color_palette(colors))\nax = sns.lineplot(x='quality',\n                  y='sulphates',\n                  hue='type', \n                  data = df.groupby(['quality','type'])['sulphates'].mean().reset_index())\nax.set_ylabel('Mean sulphates')\nplt.show()","960d6270":"df.alcohol.describe()","ee24cc07":"sns.distplot(df.alcohol, bins=20, kde=True)\nax.set_title('Histogram')\nax.set_xlabel('% Alcohol')\nplt.show()","08f96cc5":"fig, ax = plt.subplots(figsize=(15,4))\nax.set_facecolor('darkgrey')\ncolors = ['r', 'w']\nsns.set_palette(sns.color_palette(colors))\nax = sns.lineplot(x='quality',\n                  y='alcohol',\n                  hue='type', \n                  data = df.groupby(['quality','type'])['alcohol'].mean().reset_index())\nax.set_ylabel('Mean alcohol')\nplt.show()","03cf584c":"rw = df.loc[df['type']=='red'].quality.describe()\nww = df.loc[df['type']=='white'].quality.describe()\ndata = pd.DataFrame(data=[rw,ww], index=['Red Wines', 'White Wines'])\ndata","fedc25a1":"rw = df.loc[df['type']=='red'].quality.value_counts(normalize=True).sort_index()\nww = df.loc[df['type']=='white'].quality.value_counts(normalize=True).sort_index()\ndata = pd.DataFrame(data=[rw,ww], index=['Red Wines', 'White Wines'])\ndata","191f6839":"# Quality = 9 --> 5 records\ndf.loc[df.quality==9]","7328d1e9":"df['sulfur dioxide ratio'] = df['free sulfur dioxide']\/df['total sulfur dioxide']","9586d55d":"numeric_columns=['free sulfur dioxide','total sulfur dioxide','sulfur dioxide ratio','quality']\nnumeric_df = pd.DataFrame(data=df, columns=numeric_columns, index=df.index)\ncorr = np.abs(numeric_df.corr())\nfig, ax = plt.subplots(figsize=(6, 6))\ncmap = sns.color_palette(\"rainbow\")\nsns.heatmap(corr, cmap=cmap, square=True, annot=True)\nplt.title('Correlation between numerical features: abs values')\nplt.show()","ab91a1b0":"colors = ['r', 'w']\nsns.set_palette(sns.color_palette(colors))\nfig, ax = plt.subplots(1, figsize=(12, 5), sharey=True, sharex = True)\ndata =  df.groupby(['quality','type'])['sulfur dioxide ratio'].mean().reset_index()\ns = df.groupby(['quality','type'])['sulfur dioxide ratio'].count().reset_index()\ndata = pd.concat([data, s['sulfur dioxide ratio']], axis=1)\ndata.columns = ['quality', 'type', 'sulfur dioxide ratio', 'size']\nax = sns.scatterplot(x='quality', y='sulfur dioxide ratio', hue='type', cmap=colors, data=data, size='size', sizes=(100,1000), legend='brief')\nax.legend(bbox_to_anchor=(1.12,1), markerscale=0.4, facecolor ='lightgrey')\nax.set_title('')\nax.set_facecolor('black')\nax.set_ylabel('Mean sulfur dioxide ratio', fontsize=12)\nplt.tight_layout()","53de6140":"df['acidity factor'] = df['fixed acidity']*df['volatile acidity']*df['citric acid']","21f6caaf":"numeric_columns=['fixed acidity','volatile acidity','citric acid','acidity factor','quality']\nnumeric_df = pd.DataFrame(data=df, columns=numeric_columns, index=df.index)\ncorr = np.abs(numeric_df.corr())\nfig, ax = plt.subplots(figsize=(6, 6))\ncmap = sns.color_palette(\"rainbow\")\nsns.heatmap(corr, cmap=cmap, square=True, annot=True)\nplt.title('Correlation between numerical features:\\n abs values')\nplt.show()","168beaf2":"colors = ['r']\nsns.set_palette(sns.color_palette(colors))\ndf['acidity factor'].hist(bins=40)\nplt.show()","436275df":"colors = ['r', 'w']\nsns.set_palette(sns.color_palette(colors))\nfig, ax = plt.subplots(1, figsize=(12, 5), sharey=True, sharex = True)\ndata =  df.groupby(['quality','type'])['acidity factor'].mean().reset_index()\ns = df.groupby(['quality','type'])['acidity factor'].count().reset_index()\ndata = pd.concat([data, s['acidity factor']], axis=1)\ndata.columns = ['quality', 'type', 'acidity factor', 'size']\nax = sns.scatterplot(x='quality', y='acidity factor', hue='type', cmap=colors, data=data, size='size', sizes=(100,1000), legend='brief')\nax.legend(bbox_to_anchor=(1.12,1), markerscale=0.4, facecolor ='lightgrey')\nax.set_title('')\nax.set_facecolor('black')\nax.set_ylabel('Mean acidity factor', fontsize=12)\nplt.tight_layout()","32445b23":"df = pd.read_csv(path)","6b6454c0":"def limit_value(X, **val_dict):\n  \"\"\"This function recieves a dataframe and returns\n     it without the outliers\n  \"\"\"\n  for col, val in val_dict.items():\n      X = X.loc[X[col] < val, :]\n  return X","87ba79bf":"# Dictionary of outliers\noutlier_dict = {'free sulfur dioxide': 150,\n                'total sulfur dioxide': 400,\n                'density': 1.01,\n                'sulphates': 1.75,\n                'volatile acidity':1.5,\n                'citric acid': 1.2,\n                'residual sugar': 50,\n                'chlorides': 0.5}\n\n# Clean outliers\noutlier_limit = FunctionTransformer(limit_value, kw_args = outlier_dict)\ndf = outlier_limit.transform(df)","89e4e816":"def feature_engineering(X):\n  \"\"\"This function creates two new features\n  \"\"\"\n  X['sulfur dioxide ratio'] = X['free sulfur dioxide']\/X['total sulfur dioxide']\n  X['acidity factor'] = X['fixed acidity']**0.7 * X['volatile acidity']*0.55 * X['citric acid']*50\n  X['type'] = np.where(X['type']=='red', 0, 1)\n  return X","6578ab74":"fe = FunctionTransformer(feature_engineering)\ndf = fe.transform(df)","6ab5d44b":"col_to_drop = ['free sulfur dioxide', 'total sulfur dioxide']\ndf = df.drop(labels=col_to_drop, axis=1)","0241a6b4":"# split the data using 25% test and stratify according to quality distribution\nX_train, X_test, y_train, y_test =                    \\\n     split(df.drop('quality', axis=1), df['quality'], \n          test_size = 0.25, \n          random_state = 12345, \n          stratify=df['quality'])\n    \n# Examine train & test shapes\nprint('X_Train: ', X_train.shape, '| y_Train: ', y_train.shape[0])\nprint('X_Test:  ', X_test.shape, '| y_Test: ', y_test.shape[0])","77ae45e6":"imputer = SimpleImputer(strategy = 'mean')\nscaler = MinMaxScaler()\ncols = ['fixed acidity', 'volatile acidity', 'citric acid',\n       'residual sugar', 'chlorides', 'density', 'pH', \n        'sulphates', 'alcohol', 'acidity factor']\nX_train.loc[X_train['type'] == 0] = imputer.fit_transform(X_train.loc[X_train['type'] == 0])\nX_train.loc[X_train['type'] == 1] = imputer.fit_transform(X_train.loc[X_train['type'] == 1])\n\nX_train.loc[X_train['type'] == 0,cols] = scaler.fit_transform(X_train.loc[X_train['type'] == 0,cols])\nX_train.loc[X_train['type'] == 1,cols] = scaler.fit_transform(X_train.loc[X_train['type'] == 1,cols])\n\nX_test.loc[X_test['type'] == 0] = imputer.transform(X_test.loc[X_test['type'] == 0])\nX_test.loc[X_test['type'] == 1] = imputer.transform(X_test.loc[X_test['type'] == 1])\n\nX_test.loc[X_test['type'] == 0,cols] = scaler.transform(X_test.loc[X_test['type'] == 0,cols])\nX_test.loc[X_test['type'] == 1,cols] = scaler.transform(X_test.loc[X_test['type'] == 1,cols])","ab0addd0":"print('X_train: ', X_train.shape)\nprint('X_test: ', X_test.shape)","abad8b04":"X_train.to_csv('X_train_tomodel.csv')\nX_test.to_csv('X_test_tomodel.csv')\ny_train.to_csv('y_train_tomodel.csv')\ny_test.to_csv('y_test_tomodel.csv')","e267f3e9":"kappa_scorer = make_scorer(cohen_kappa_score)","914fb3f1":"# X_train = pd.read_csv('X_train_tomodel.csv')\n# X_test = pd.read_csv('X_test_tomodel.csv')\n# y_train = pd.read_csv('y_train_tomodel.csv')\n# y_test = pd.read_csv('y_test_tomodel.csv')","ea914f81":"# --- Confusion Matrix Plot to evaluate multiple model results ---\n\ndef cm_plot(cm, model=None):\n  fig, ax = plt.subplots(figsize = (10,4))\n  df_cm = pd.DataFrame(cm, index = [i for i in model.classes_],\n            columns = [i for i in model.classes_])\n  sns.set(font_scale=1.2)\n  cmap = sns.color_palette(\"rainbow\")\n  ax = sns.heatmap(df_cm, cmap = cmap, annot=True,\n                  fmt = 'd',\n                  linecolor = 'w', linewidth = 1,\n                  annot_kws={\"size\": 12})\n  return plt.show()","63c86943":"# Run Baseline Logistic Regression with all data\nmodel = LogisticRegression(multi_class = 'ovr' )\nmodel.fit(X_train, y_train)\ny_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\n\n# CrossValidation\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\nscores = cross_val_score(model, X_train, y_train, cv=cv, scoring=kappa_scorer)\n\n# Print the results\nprint('CV:', scores)\nprint('CV mean:', scores.mean())\nprint(f'Train Cohen kappa score is: {cohen_kappa_score(y_train, y_train_pred):.3}')\nprint(f'Test Cohen kappa score is: {cohen_kappa_score(y_test, y_test_pred):.3}')\ncm_plot(confusion_matrix(y_test, y_test_pred),model)","5eef0a86":"class run_estimator (BaseEstimator, TransformerMixin):\n  \"\"\" This estimator recives 2 models, split the data into 2 classes, \n      apply each population its unique model and returns a unite prediction \n      \n      PARAMETERS:\n      ----------\n      model_r (scikit learn estimator): model to apply on red wines\n      model_w (scikit learn estimator): model to apply on white wines\n      classes (list): optional. default refers to 0,1 as names of populations\n\n      RERTURN:\n      ----------\n      A pd.Series with the prediction of the estimator\n  \"\"\"\n\n  def __init__(self, model_r, model_w, classes=[0,1]):\n      self.red_model_ = model_r\n      self.white_model_ = model_w\n      self.classes = classes\n      \n  \n  def fit (self, X, y=None):\n      X_red = X[X.type==0].copy()\n      y_red = y[X.type==0].copy()\n      self.red_model_.fit(X_red, y_red)\n      X_white = X[X.type==1].copy()\n      y_white = y[X.type==1].copy()\n      self.white_model_.fit(X_white, y_white)\n      return self\n\n  def predict(self, X):\n      X_red = X[X.type==0].copy()\n      X_white = X[X.type==1].copy()\n      y_red_pred = pd.Series(self.red_model_.predict(X_red),index=X_red.index)\n      y_white_pred = pd.Series(self.white_model_.predict(X_white),index=X_white.index)\n      y_pred = pd.concat([y_red_pred, y_white_pred], axis=0)\n      return y_pred.reindex_like(X)\n\n  def score(self, X, y):\n      pass\n    \n  def get_params(self, deep=True):\n      return {'model_w': self.white_model_,\n              'model_r': self.red_model_,\n              'classes': self.classes}\n  \n  def set_params(self, **parameters):\n      for parameter, value in parameters.items():\n          setattr(self, parameter, value)\n      return self","6fb5e5e6":"# initialize the model\nrfc = RandomForestClassifier(random_state=42,\n                            class_weight = {3:2, 4:2, 5:7, 6:6, 7:9, 8:4, 9:1},\n                            max_depth=15, n_estimators = 200)\nrfc.fit(X_train, y_train)\n\n#prediction\ny_train_pred = rfc.predict(X_train)\ny_test_pred = rfc.predict(X_test)\n\n# CrossValidation\nmy_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores_rfc = cross_val_score(rfc, X_train, y_train, cv=my_cv, scoring=kappa_scorer)\n\n# GridSearch\n# my_param_grid= {'n_estimators':[150, 200], \n#                 'class_weight': [{3:2, 4:2, 5:7, 6:6, 7:9, 8:4, 9:1}],\n#                 'max_depth':[5, 8, 12, 15, 18]}\n# rf_clf = GridSearchCV(rf, my_param_grid, cv=my_cv)\n# rf_clf.fit(X_train, y_train)\n# print(\"Best parameters:\", rf_clf.best_params_)\n\n#score\nprint(f'TrainCV  cross: {scores_rfc}')\nprint(\"Train Mean cross-validation score: %.2f\" % scores_rfc.mean()+'\\n')\nprint(f'Train Cohen kappa score is: {cohen_kappa_score(y_train, y_train_pred):.3}')\nprint(f'Test Cohen kappa score is: {cohen_kappa_score(y_test, y_test_pred):.3}')\ncm_plot(confusion_matrix(y_test, y_test_pred), rfc)","a9b3b971":"# start_time = time.time()\nxgb = XGBClassifier(alpha=1, colsample_bytree=0.9,\n                    max_depth=10, n_extimator=100,\n                    objective='multi:softmax')\nxgb.fit(X_train, y_train)\n\n#prediction\ny_train_pred = xgb.predict(X_train)\ny_test_pred = xgb.predict(X_test)\n\n#Cross validation\nmy_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscoresXgb = cross_val_score(xgb, X_train, y_train, cv=my_cv, scoring=kappa_scorer)\n\n#GridSearch\n# my_param_grid = {'colsample_bytree': [0.8,0.9],\n#                  'max_depth': [9,10], 'alpha': [1],\n#                  'n_extimator':[100,150],\n#                  'objective' :['multi:softmax']}\n# gs_xgb = GridSearchCV(xgb, my_param_grid ,scoring=kappa_scorer, cv=my_cv)\n# gs_xgb.fit(X_train, y_train)\n# print(\"Best parameters:\", gs_xgb.best_params_)\n\n#score\nprint(f'TrainCV  cross: {scoresXgb}')\nprint(\"Train Mean cross-validation score: %.2f\" % scoresXgb.mean()+'\\n')\nprint(f'Train Cohen kappa score is: {cohen_kappa_score(y_train, y_train_pred):.3}')\nprint(f'Test Cohen kappa score is: {cohen_kappa_score(y_test, y_test_pred):.3}')\ncm_plot(confusion_matrix(y_test, y_test_pred), xgb)","f8b7a613":"clf1 = LogisticRegression(multi_class='ovr', C=1.6)\nclf2 = DecisionTreeClassifier(max_depth=12)\nclf3 = SVC(probability=True)\n\nclassifiers = [('LR', clf1), ('DT', clf2), ('SVM', clf3)]\nvoting = VotingClassifier(estimators=classifiers, voting='soft')\nvoting.fit(X_train, y_train)\n\n#prediction\ny_train_pred = voting.predict(X_train)\ny_test_pred = voting.predict(X_test)\n\n# crossvalidation\nmy_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscoresV = cross_val_score(voting, X_train, y_train, cv=my_cv, scoring=kappa_scorer)\n\n#GridSearch\n# my_param_grid = {'LR__C':[1, 1.2, 1.6],'DT__max_depth':[6, 8, 12,15] }\n# voting_gs = GridSearchCV(voting, my_param_grid, cv=3)\n# voting_gs.fit(X_train, y_train)\n# print(\"Best parameters:\", voting_gs.best_params_)\n\n# Scores Print\nprint(f'TrainCV  cross: {scoresV}')\nprint(\"Train Mean cross-validation score: %.2f\" % scoresV.mean()+'\\n')\nprint(f\"Train Cohen kappa score is: {cohen_kappa_score(y_train, y_train_pred):.3}\")\nprint(f'Test Cohen kappa score is: {cohen_kappa_score(y_test, y_test_pred):.3}')\ncm_plot(confusion_matrix(y_test, y_test_pred), voting)","6547b1a9":"# CREATE RED DATA\n#==================\nX_train_r = X_train[X_train.type==0].copy()\ny_train_r = y_train[X_train.type==0].copy()\nX_test_r=  X_test[X_test.type==0].copy()\ny_test_r = y_test[X_test.type==0].copy()\n\n# CREATE WHITE DATA\n#==================\nX_train_w = X_train[X_train.type==1].copy()\ny_train_w = y_train[X_train.type==1].copy()\nX_test_w=  X_test[X_test.type==1].copy()\ny_test_w = y_test[X_test.type==1].copy()","b23902cf":"# WHITE MODEL\nmodel_w = LogisticRegression(multi_class = 'ovr', random_state=42, penalty='l2', C=1,\n                             class_weight={3:2, 4:5, 5:6, 6:6, 7:8, 8:4, 9:1})\nmodel_w.fit(X_train_w, y_train_w)\ny_train_w_pred = model_w.predict(X_train_w)\ny_test_w_pred = model_w.predict(X_test_w)\n\n# # CrossValidation\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(model_w, X_train_w, y_train_w, cv=cv, scoring=kappa_scorer)\n\n# GridSearchCV\n# my_params = {'C': [1,0.1,5]}\n# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n# clf = GridSearchCV(model, my_params, scoring=kappa_scorer, cv=cv)\n# search = clf.fit(X_train, y_train)\n# print(search.best_params_)\n\n# Print the results\nprint('CV:', scores)\nprint('CV Mean:', scores.mean().round(3))\nprint(f'Train Cohen kappa score is: {cohen_kappa_score(y_train_w, y_train_w_pred):.3}')\nprint(f'Test Cohen kappa score is: {cohen_kappa_score(y_test_w, y_test_w_pred):.3}')\ncm_plot(confusion_matrix(y_test, y_test_pred), model_w)","46c9b157":"# RED MODEL\nmodel_r = LogisticRegression(multi_class = 'ovr', random_state=42, penalty='l2', C=1.6,\n                             class_weight={3: 1, 4: 3, 5: 5, 6: 7, 7: 9, 8: 1})\nmodel_r.fit(X_train_r, y_train_r)\ny_train_r_pred = model_r.predict(X_train_r)\ny_test_r_pred = model_r.predict(X_test_r)\n\n# # CrossValidation\ncv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\nscores = cross_val_score(model_r, X_train_r, y_train_r, cv=cv, scoring=kappa_scorer)\n\n# # GridSearchCV\n# my_params = {'solver':['newton-c','lbfgs', 'liblinear'],'penalty':['l2','l1']}\n# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n# clf = GridSearchCV(model, my_params, scoring=kappa_scorer, cv=cv)\n# search = clf.fit(X_train, y_train)\n# print(search.best_params_)\n\n# Print the results\nprint('CV:', scores)\nprint('CV Mean:', scores.mean().round(3))\nprint(f'Train Cohen kappa score is: {cohen_kappa_score(y_train_r, y_train_r_pred):.3}')\nprint(f'Test Cohen kappa score is: {cohen_kappa_score(y_test_r, y_test_r_pred):.3}')\ncm_plot(confusion_matrix(y_test_r, y_test_r_pred), model_r)","fb63c505":"def cm_plot_run_estimator(cm, model=None):\n  fig, ax = plt.subplots(figsize = (10,4))\n  df_cm = pd.DataFrame(cm, index = np.arange(3,10),\n            columns = np.arange(3,10))\n  sns.set(font_scale=1.2)\n  cmap = sns.color_palette(\"rainbow\")\n  ax = sns.heatmap(df_cm, cmap = cmap, annot=True,\n                  fmt = 'd',\n                  linecolor = 'w', linewidth = 1,\n                  annot_kws={\"size\": 12})\n  return plt.show()","fae84397":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Logistic Regression Classifier\nmodel_w = LogisticRegression(multi_class = 'ovr', random_state=42, penalty='l2', C=1,\n                             class_weight={3:2, 4:5, 5:6, 6:6, 7:8, 8:4, 9:1})\nmodel_r = LogisticRegression(multi_class = 'ovr', random_state=42, penalty='l2', C=1.6,\n                             class_weight={3: 1, 4: 3, 5: 5, 6: 7, 7: 9, 8: 1})\nmodel = run_estimator(model_r, model_w)\nmodel.fit(X_train, y_train)\ny_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\n\n# CrossValidation\ncv = StratifiedKFold(n_splits=3, shuffle=True, random_state=123)\nscores = cross_val_score(model, X_train, y_train, cv=cv, scoring=kappa_scorer)\n\n# Print the results\nprint('CV:', scores)\nprint('CV mean:', scores.mean())\nprint(f'Train Cohen kappa score is: {cohen_kappa_score(y_train, y_train_pred):.3}')\nprint(f'Test Cohen kappa score is: {cohen_kappa_score(y_test, y_test_pred):.3}')\ncm_plot_run_estimator(confusion_matrix(y_test, y_test_pred), model)","82342ab4":"# WHITE MODEL\nmodel_w = RandomForestClassifier(random_state=0, max_depth = 12, \n                                 class_weight={3:2, 4:2, 5:7, 6:6, 7:9, 8:4, 9:1})\nmodel_w.fit(X_train_w, y_train_w)\ny_train_w_pred = model_w.predict(X_train_w)\ny_test_w_pred = model_w.predict(X_test_w)\n\n# # CrossValidation\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(model_w, X_train_w, y_train_w, cv=cv, scoring=kappa_scorer)\n\n# # # GridSearchCV\n# my_params = {'max_depth': [5,8,12,15]}\n# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n# clf = GridSearchCV(model_w, my_params, scoring=kappa_scorer, cv=cv)\n# search = clf.fit(X_train, y_train)\n# print(search.best_params_)\n\n# Print the results\nprint('CV:', scores)\nprint('CV Mean:', scores.mean().round(3))\nprint(f'Train Cohen kappa score is: {cohen_kappa_score(y_train_w, y_train_w_pred):.3}')\nprint(f'Test Cohen kappa score is: {cohen_kappa_score(y_test_w, y_test_w_pred):.3}')\ncm_plot(confusion_matrix(y_test_w, y_test_w_pred), model_w)","24a709b6":"# RED MODEL\nmodel_r = RandomForestClassifier (n_estimators=111, random_state=0, max_depth = 5)\nmodel_r.fit(X_train_r, y_train_r)\ny_train_r_pred = model_r.predict(X_train_r)\ny_test_r_pred = model_r.predict(X_test_r)\n\n# # CrossValidation\ncv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\nscores = cross_val_score(model_r, X_train_r, y_train_r, cv=cv, scoring=kappa_scorer)\n\n# # GridSearchCV\n# my_params = {'bootstrap':[False,True], 'max_depth': [4,6,8,10,12]}\n# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n# clf = GridSearchCV(model, my_params, scoring=kappa_scorer, cv=cv)\n# search = clf.fit(X_train, y_train)\n# print(search.best_params_)\n\n# Print the results\nprint('CV:', scores)\nprint('CV Mean:', scores.mean().round(3))\nprint(f'Train Cohen kappa score is: {cohen_kappa_score(y_train_r, y_train_r_pred):.3}')\nprint(f'Test Cohen kappa score is: {cohen_kappa_score(y_test_r, y_test_r_pred):.3}')\ncm_plot(confusion_matrix(y_test_r, y_test_r_pred), model_r)","8fa9f5bb":"# RandomForestClassifier\nmodel_r = RandomForestClassifier (n_estimators=111, random_state=0, max_depth = 5)\nmodel_w = RandomForestClassifier(random_state=0, max_depth = 12, \n                                 class_weight={3:2, 4:2, 5:7, 6:6, 7:9, 8:4, 9:1})\nmodel = run_estimator(model_r, model_w)\nmodel.fit(X_train, y_train)\ny_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\n\n# CrossValidation\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(model, X_train, y_train, cv=cv, scoring=kappa_scorer)\n\n# Print the results\nprint('CV:', scores)\nprint('CV Mean:', scores.mean().round(3))\nprint(f'Train Cohen kappa score is: {cohen_kappa_score(y_train, y_train_pred):.3}')\nprint(f'Test Cohen kappa score is: {cohen_kappa_score(y_test, y_test_pred):.3}')\ncm_plot_run_estimator(confusion_matrix(y_test, y_test_pred), model=model)","67f18d13":"# RandomForestClassifier\nmodel_r = RandomForestClassifier (n_estimators=111, random_state=0, max_depth = 5)\nmodel_w = RandomForestClassifier(random_state=0,\n                            class_weight = {3:2, 4:2, 5:7, 6:6, 7:9, 8:4, 9:1},\n                            max_depth=15, n_estimators = 200)\nmodel = run_estimator(model_r, model_w)\nmodel.fit(X_train, y_train)\ny_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\n\n# CrossValidation\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\nscores = cross_val_score(model, X_train, y_train, cv=cv, scoring=kappa_scorer)\n\n# Print the results\nprint('CV:', scores)\nprint('CV Mean:', scores.mean().round(3))\nprint(f'Train Cohen kappa score is: {cohen_kappa_score(y_train, y_train_pred):.3}')\nprint(f'Test Cohen kappa score is: {cohen_kappa_score(y_test, y_test_pred):.3}')\ncm_plot_run_estimator(confusion_matrix(y_test, y_test_pred), model=model)","583ae8bc":"vals_r, vals_w = [],[]\nfor (val_r, col) in [*zip(model_r.feature_importances_,X_train.columns)]:\n  vals_r.append(val_r)\nfor (val_w, col) in [*zip(model_w.feature_importances_,X_train.columns)]:\n  vals_w.append(val_w)\n\nfig, ax = plt.subplots(figsize=(12,4))\nmy_data_r = pd.Series(index=X_train.columns, data=vals_r)\nmy_data_w = pd.Series(index=X_train.columns, data=vals_w)\nmy_data=pd.concat([my_data_r,my_data_w], axis=1)\nmy_data.columns=['Red','White']\nmy_data.drop('type', inplace=True)\nax = my_data.plot(kind='bar', color=['r','w'],edgecolor='black',ax=ax)\nplt.title('Feature Importances: Random Forest Combined Model')\nplt.show()","169ea071":"# WHITE MODEL\nmodel_w = XGBClassifier(random_state=0, max_depth=5, colsample_bytree=0.6,\n                        reg_alpha=0.5)\nmodel_w.fit(X_train_w, y_train_w)\n\ny_train_w_pred = model_w.predict(X_train_w)\ny_test_w_pred = model_w.predict(X_test_w)\n\n# CrossValidation\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(model_w, X_train_w, y_train_w, cv=cv, scoring=kappa_scorer)\n\n# # GridSearchCV\n# my_params = {'colsample_bytree ': [3,4,5,6], 'gamma':[0,0.01,0.05]}\n# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n# clf = GridSearchCV(model, my_params, scoring=kappa_scorer, cv=cv)\n# search = clf.fit(X_train, y_train)\n# print(search.best_params_)\n\n# Print the results\nprint('CV:', scores)\nprint('CV Mean:', scores.mean().round(3))\nprint(f'Train Cohen kappa score is: {cohen_kappa_score(y_train_w, y_train_w_pred):.3}')\nprint(f'Test Cohen kappa score is: {cohen_kappa_score(y_test_w, y_test_w_pred):.3}')\ncm_plot(confusion_matrix(y_test_w, y_test_w_pred), model_w)","f5e0458a":"# RED MODEL\nmodel_r = XGBClassifier(random_state=0, max_depth=2, \n                        n_estimators=30, colsample_bytree=0.8,\n                        subsample = 0.3)\n                        \nmodel_r.fit(X_train_r, y_train_r)\ny_train_r_pred = model_r.predict(X_train_r)\ny_test_r_pred = model_r.predict(X_test_r)\n#objective='multi:softprob',           scale_pos_weight = 1)              eval_metric='mlogloss', learning_rate=0.05,\n\n# # CrossValidation\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(model_r, X_train_r, y_train_r, cv=cv, scoring=kappa_scorer)\n\n# GridSearchCV\n# my_params = {'learning_rate':[0.05,0.1,0.2]}\n# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n# clf = GridSearchCV(model, my_params, scoring=kappa_scorer, cv=cv)\n# search = clf.fit(X_train, y_train)\n# print(search.best_params_)\n\n# Print the results\nprint('CV:', scores)\nprint('CV Mean:', scores.mean().round(3))\nprint(f'Train Cohen kappa score is: {cohen_kappa_score(y_train_r, y_train_r_pred):.3}')\nprint(f'Test Cohen kappa score is: {cohen_kappa_score(y_test_r, y_test_r_pred):.3}')\ncm_plot(confusion_matrix(y_test_r, y_test_r_pred), model_r)","54fdb680":"# XGBoost\nmodel_r = XGBClassifier(random_state=0, max_depth=2, \n                        n_estimators=30, colsample_bytree=0.8,\n                        subsample = 0.3)\nmodel_w = XGBClassifier(random_state=0, max_depth=5, colsample_bytree=0.6,\n                        reg_alpha=0.5)\n\nmodel = run_estimator(model_r, model_w)\nmodel.fit(X_train, y_train)\ny_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\n\n# CrossValidation\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\nscores = cross_val_score(model, X_train, y_train, cv=cv, scoring=kappa_scorer)\n\n# Print the results\nprint('CV:', scores)\nprint('CV mean:', scores.mean())\nprint(f'Train Cohen kappa score is: {cohen_kappa_score(y_train, y_train_pred):.3}')\nprint(f'Test Cohen kappa score is: {cohen_kappa_score(y_test, y_test_pred):.3}')\ncm_plot_run_estimator(confusion_matrix(y_test, y_test_pred), model=model)","67ad9a1d":"model_r = XGBClassifier(random_state=0, max_depth=2, \n                        n_estimators=30, colsample_bytree=0.8,\n                        subsample = 0.3)\nmodel_w = XGBClassifier(alpha=1, colsample_bytree=0.9,\n                    max_depth=10, n_extimator=100,\n                    objective='multi:softmax')\n\nmodel = run_estimator(model_r, model_w)\nmodel.fit(X_train, y_train)\ny_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\n\n# CrossValidation\ncv = StratifiedKFold(n_splits=3, shuffle=True, random_state=123)\nscores = cross_val_score(model, X_train, y_train, cv=cv, scoring=kappa_scorer)\n\n# Print the results\nprint('CV:', scores)\nprint('CV mean:', scores.mean())\nprint(f'Train Cohen kappa score is: {cohen_kappa_score(y_train, y_train_pred):.3}')\nprint(f'Test Cohen kappa score is: {cohen_kappa_score(y_test, y_test_pred):.3}')\ncm_plot_run_estimator(confusion_matrix(y_test, y_test_pred), model=model)","828990d1":"vals_r, vals_w = [],[]\nfor (val_r, col) in [*zip(model_r.feature_importances_,X_train.columns)]:\n  vals_r.append(val_r)\nfor (val_w, col) in [*zip(model_w.feature_importances_,X_train.columns)]:\n  vals_w.append(val_w)\n\nfig, ax = plt.subplots(figsize=(15,6))\nmy_data_r = pd.Series(index=X_train.columns, data=vals_r)\nmy_data_w = pd.Series(index=X_train.columns, data=vals_w)\nmy_data=pd.concat([my_data_r,my_data_w], axis=1)\nmy_data.columns=['Red','White']\nmy_data.drop('type', inplace=True)\nax = my_data.plot(kind='bar', color=['r','w'],edgecolor='black',ax=ax)\nplt.title('Feature Importances: XGBoost Combined Model')\nplt.show()","cec845b1":"classifiers = [('LR', LogisticRegression(multi_class='ovr', random_state=42)),\n               ('DTC', DecisionTreeClassifier(max_depth=5, random_state=42)),\n               ('SVC', SVC(probability=True, random_state=42))]","433aff04":"# WHITE MODEL\nmodel_w = VotingClassifier(estimators=classifiers, voting='soft')\nmodel_w.set_params(LR__C=1)\nmodel_w.fit(X_train_w, y_train_w)\ny_train_w_pred = model_w.predict(X_train_w)\ny_test_w_pred = model_w.predict(X_test_w)\n\n# CrossValidation\ncv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\nscores = cross_val_score(model_w, X_train_w, y_train_w, cv=cv, scoring=kappa_scorer)\n\n# GridSearchCV\n# my_params = [{'LR__C':[0.1,1,3], 'DTC__max_depth':[4,5,6], \n#                'LR__class_weight': [{3:2, 4:5, 5:6, 6:6, 7:8, 8:4, 9:1}],\n#               'LR__penalty': ['l2']}]\n# clf = GridSearchCV(model_w, my_params, cv=5, scoring=kappa_scorer)\n# search = clf.fit(X_train, y_train)\n# print(\"Best parameters:\", clf.best_params_)\n\n# Print the results\nprint('CV:', scores)\nprint('CV Mean:', scores.mean().round(3))\nprint(f'Train Cohen kappa score is: {cohen_kappa_score(y_train_w, y_train_w_pred):.3}')\nprint(f'Test Cohen kappa score is: {cohen_kappa_score(y_test_w, y_test_w_pred):.3}')\ncm_plot(confusion_matrix(y_test_w, y_test_w_pred), model_w)","2cba3be4":"# RED MODEL\nmodel_r = VotingClassifier(estimators=classifiers, voting='soft')\nmodel_r.set_params(LR__C=2.5)\nmodel_r.fit(X_train_r, y_train_r)\ny_train_r_pred = model_r.predict(X_train_r)\ny_test_r_pred = model_r.predict(X_test_r)\n\n# # CrossValidation\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(model_r, X_train_r, y_train_r, cv=cv, scoring=kappa_scorer)\n\n# GridSearchCV\n# my_params = {}\n# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n# clf = GridSearchCV(model, my_params, scoring=kappa_scorer, cv=cv)\n# search = clf.fit(X_train, y_train)\n# search.best_params_\n\n# Print the results\nprint('CV:', scores)\nprint('CV Mean:', scores.mean().round(3))\nprint(f'Train Cohen kappa score is: {cohen_kappa_score(y_train_r, y_train_r_pred):.3}')\nprint(f'Test Cohen kappa score is: {cohen_kappa_score(y_test_r, y_test_r_pred):.3}')\ncm_plot(confusion_matrix(y_test_r, y_test_r_pred), model_r)","164dfaf8":"# Voting Classifier\nmodel_r = VotingClassifier(estimators=classifiers, voting='soft')\nmodel_r.set_params(LR__C=2.5)\nmodel_w = VotingClassifier(estimators=classifiers, voting='soft')\nmodel_w.set_params(LR__C=1)\nmodel = run_estimator(model_r, model_w)\nmodel.fit(X_train, y_train)\ny_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\n\n# CrossValidation\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\nscores = cross_val_score(model, X_train, y_train, cv=cv, scoring=kappa_scorer)\n\n# Print the results\nprint('CV:', scores)\nprint('CV mean:', scores.mean())\nprint(f'Train Cohen kappa score is: {cohen_kappa_score(y_train, y_train_pred):.3}')\nprint(f'Test Cohen kappa score is: {cohen_kappa_score(y_test, y_test_pred):.3}')\ncm = confusion_matrix(y_test, y_test_pred)\ncm_plot_run_estimator(cm, model=model)","f1b99d7f":"# Voting Classifier\nmodel_r = VotingClassifier(estimators=classifiers, voting='soft')\nmodel_r.set_params(LR__C=2.5)\nmodel_w = VotingClassifier(estimators=classifiers, voting='soft')\nmodel_w.set_params(DTC__max_depth=12,LR__multi_class='auto')\n\nmodel = run_estimator(model_r, model_w)\nmodel.fit(X_train, y_train)\ny_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\n\n# CrossValidation\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\nscores = cross_val_score(model, X_train, y_train, cv=cv, scoring=kappa_scorer)\n\n# Print the results\nprint('CV:', scores)\nprint('CV mean:', scores.mean())\nprint(f'Train Cohen kappa score is: {cohen_kappa_score(y_train, y_train_pred):.3}')\nprint(f'Test Cohen kappa score is: {cohen_kappa_score(y_test, y_test_pred):.3}')\ncm = confusion_matrix(y_test, y_test_pred)\ncm_plot_run_estimator(cm, model=model)","fa8a64a3":"Pairplot representation of the data with differentiation between the only categorical feature, the wine type, reveals some intresting observations regarding the correlation of features of each type. \n\nIt can also be seen quite easily that almost most of the features have one or more suspected outliers.\n\nTherefore, for each feature we will explore the wine type differntiation and outliers detection.","38ae8831":"## **pH**\n\nAlso known as the potential of hydrogen, this is a numeric scale to specify the acidity or basicity the wine. Fixed acidity contributes the most towards the pH of wines. You might know, solutions with a pH less than 7 are acidic, while solutions with a pH greater than 7 are basic. With a pH of 7, pure water is neutral. Most wines have a pH between 2.9 and 3.9 and are therefore acidic.","c9242f1b":"We implement each population its best model after gridsearch, and the results are:\n* Train Cohen kappa score is: 0.301\n* Test Cohen kappa score is: 0.269","6be65f26":"**Run_Estimator Model**","4b1a2af4":"### **White Model**","876b2fa7":"Up until now, we tried to find each population its best hyperparameters. In the combined method we used the red and white wines best hyperparameters to get a single result. We now try to use the best model we found for the whole data and inplement it as the white best hyperparameters. This model will be refered to as combined + single.","ac9688ea":"* At quality levels <5 - mean citric acid levels are relatively different\n* 3 null values\n* Outlier detected","21b55052":"> This project is the 4th assignment required in *Data Science Course, Naya College* and submitted to the instructor Dror Geva.\n> The project main purpose was to explore and practice the ML classification processes along with advanced technicques of fearures selections, grid search, validation methods and demonstrate deep understanding of score method selection. \n","72f00ff1":"First, we will create a clean copy of the data","b8ef340a":"## **Apply imputer and scaler**","3e3053c7":"# **Pre-Processing**","4520dbdd":"Voting Classifier results:\n![image.png](attachment:image.png)","fa576e07":"* Red wines have lower free sulfor dioxide conc. at the same quality level\n* Red wines do not exceed FSD conc. > 80 in this dataset\n* White wines has higher free SO2 levels\n* Mean level of SO2 in white wines with quality = 4 is showing anomalous\n* No null values\n* Min = 1, Max = 289, Mean = 30.52\n* 1 outlier detected with FDS=289, quality is 3 (the lowest).","286e686f":"## **Scoring Method**","121999f5":"> Excess of volatile acids are undesirable and lead to unpleasant flavour.","9906d989":"### **Red Model**","1e889bf0":"Looking closely at suspected outliers reveals 3 records: 2 of them are identical, and one distinguished.","80a798af":"## **RandomForestClassifier**\n\nInitally, applying simple RandomForestClassifier [random_state=0, max_depth = 3] with our split model we got:\n* Train Cohen kappa score is: 0.28\n* Test Cohen kappa score is: 0.224\n\nThis is slightly worse than the single model. We will implement each population best model, and the results are:\n* Train Cohen kappa score is: 0.318\n* Test Cohen kappa score is: 0.269","bde008cc":"### **Combined: White = Best Single Model**","a862b78a":"The average sulphates level vary in white and red wines. We will use simple imputer to replace NaN values with the mean values according to the type.","09cfed57":"# **Models: Split Populations**\n\nIn this section we will try to find the most suitable model to predict the quality label while using different models for red and white wines, and combining it to a single prediction vector.\n\nWe will use the Logistic Regression as a baseline, and try to achieve higher Cohens Kappa's score.\nWe will implement the following models:\n* RandomForestClassifier\n* XGBoost\n* VotingClassifier","9219397c":"## **Feature Engineering**\n\nWe create the 2 new features and replace the type with [0,1].","0d15cc0f":"## **Sulphates**\n\nThese are mineral salts containing sulfur. They are a regular part of the winemaking around the world and are considered essential. They are connected to the fermentation process and affects the wine aroma and flavour.","67500f02":"* Combined data failed to improve model scores\n* Nevertheless, splitting the data into 2 populations was worth the effort, except for in the VotingClassifier model\n* Applying grid search on each population and joined data seperately, gave added value\n\nFurther options to explore the data are:\n* Oversampling \\ undersampling should be executed on the split model\n* Combining 2 different models (instead of one model with different hyper-parameters)\n* More time to GridSearch and improve the scores  ;-)\n* Binning the quality target into good, medium and bad\n* Predicting wine type \n\n","611b31e1":"There is a difference with pH 'behaviour' between red and white wines.\n\nThe average pH is not the same for white and red wines. Later, We will use an imputer to replace NaN values with the mean values according to the type.","f9df08a5":"* Total mean SO2 levels are higher in white wines than in red wines, but remain relatively constant among quality groups.\n* No null values\n* min = 6, max = 440, mean = 115","bbd7669a":"### **Acidity factor**\n\nLooking at citric acid feature, there isn't a clear sepration between red and white wines. We decided to multiply the 3 acid features to create a new feature which demonstrates a differntiation between red and white wines.\n","a7582a9e":"Wines with higher acidity feel lighter-bodied because they come across as \u201cspritzy\u201d. Reducing acids significantly might lead to wines tasting flat. If you prefer a wine that is richer and rounder, you enjoy slightly less acidity.\n\n * 10 null values will later be filled with average\n * It seems like very high\\low fixed acidity represent very poor or very good quality\n * Values between 3.8 and 15.9: feature will need scaling","8596e09d":"### **Sulfur dioxide ratio**\n\nSince free sulfur dioxide is the unbound part of total sulfur dioxide, we will caculate the ratio of this two features.\nThis feature has higher correlation to quality than each of the individuals ","91736183":"* Mean chloride level decline with quality increase\n* Red Wines have higher chloride levels than white wines\n* 2 nulls\n* Outliers detected","2b29feaa":"## **Chlorides**","e059db5c":"This feature doesn't demonsrate a difference between white and red wines.\n\nHigher mean alcohol volumes are associated with higher quality.","942be906":"### **Combined: White = Best Single Model**","191c5df2":"## **Voting**\nVoting model is based on Logistc Regression,Decision Tree classifier & SVM. We used GridSearch to select best hyperparameter combinations","dc086bd9":"Mean density values decline as quality increase in both types of wines.","34fda744":"# **Models: Single Population**","88ab2bd7":"## **Alcohol**\n\nIt's usually measured in % vol or alcohol by volume (ABV)","56f0f3e4":"### **Combined: White = Best Single Model**","932c07ca":"The max value of 289 is outstanding, and actually there is only one value above 150. ","9cd4d31a":"## **Split the data**","521a6707":"## **Baseline Model**","c25eec75":"## **Import the data**","d6203184":"Logistic Regression results![image.png](attachment:image.png)","b27eb0de":"This typically refers to the natural sugar from grapes which remains after the fermentation process is stopped.\n* 2 Null values\n* Outliers detected","576c9a56":"## **XGBoost**\n\nInitally, applying simple XGBoost [random_state=0, max_depth = 3] with our split model we got:\n\n* Train Cohen kappa score is: 0.528\n* Test Cohen kappa score is: 0.286\n\nThis is slightly _______ than the single model. We will implement each population best model, and the results are:\n* Train Cohen kappa score is: 0.686\n* Test Cohen kappa score is: 0.387","be9f1c50":"## **Fixed acidity**","6e2d99e0":"### Optional: Files import","81e3f42f":"### **Combined**","702c23fe":"### **Red Model**","bd120815":"### **White Model**","282e0962":"We build a general estimator that takes as input a model (inc. kwargs) and output is cohen's kappa score","0f052d8b":"Density values are actually densed and varied between 0.987 and 1.03. ","502b2221":"### **White Model**","df712e0f":"## **RandomForestClassifier** ","bd7e79fe":"# **EDA**\n---","e6560874":"## **Residual sugar**","1cf0d620":"## **Outliers**","d7f61e2a":"## **Select Features**\n\nWe drop the 2 original sulfur features since it is replaced by its ratio.","688e3f8d":"Random Forest conclusions:\n![image.png](attachment:image.png)","6fcc2eb6":"# **At first glance...**\n---","488ffec2":"## XGBoost","2d5c79f3":"## **Save files for model**","14582e70":"## **Free Sulfur Dioxide (FSD)**\n\nThis is the part of the sulphur dioxide that when added to a wine is said to be free after the remaining part binds. Winemakers will always try to get the highest proportion of free sulphur to bind. They are also known as sulfites and too much of it is undesirable and gives a pungent odour. ","8df895c5":"We have decided to use the cohen's kappa score to evaluate our different models. The reason for this is that our data is highly imbalanced and we use the models to predict multiclass predictions. We could also use log-loss or AUC ROC.\nFor more reading regarding this score method please refer to: [https:\/\/towardsdatascience.com\/cohens-kappa-9786ceceab58](http:\/\/).\n\nAlso, since this scoring method is not an option using scikit-learn scoring during cross validation and gridsearch, we used the make scorer function to enable this feature.","43229851":"### **Combined**","22633153":"## **Duplicates**\n\nLooking for duplicates in the dataset, reveals 1168 records marked as duplicates, almost 20% of the data.\nThe [original](https:\/\/archive.ics.uci.edu\/ml\/datasets\/wine+quality) introduction of this dataset has 4898 records. Our Kaggle dataset contains 6497 records. \n\nThe duplicates can not explain the gap between the copies of the dataset. Since the duplicates contain red & white wine with 3:1 ratio we believe that along the way someone added sampling data to the original dataset, and hence the dulicates. \n\nWe have decided to keep the duplicates, since part of the challenge in this project is to deal with imbalanced data, and the current setting of the data support this purpose.","2cf97667":"## **Total Sulfur Dioxide (TSD)**\n\nThis is the sum total of the bound and the free sulfur dioxide. This is mainly added to kill harmful bacteria and preserve quality and freshness. There are usually legal limits for sulfur levels in wines and excess of it can even kill good yeast and give out undesirable odour.","ab1191fb":"Looking at linear correlation among numerical features as a heatmap, reveals small to medium correlation between different features.\n\nAs for the correlation to our target - the highest correlation is to alcohol with 0.44.","9911f7b7":"* Red wine volatile acidity is higher than white wines and tends to descent with quality\n* Mean white wines fixed acidity is almost linear\n* 8 null values\n* Outliers detected\n* Correlation to quality --> 0.27 (abs)","cbd9f9cc":"## **Density**\n\nThis can be represented as a comparison of the weight of a specific volume of wine to an equivalent volume of water. It is generally used as a measure of the conversion of sugar to alcohol. Usually, wine has higher density from water by 8-9%.","0e1e682b":"This is one of the fixed acids which gives a wine its freshness. Usually most of it is consumed during the fermentation process and sometimes it is added separately to give the wine more freshness.","5439e59f":"Chloride concentration in the wine is influenced by terroir and its highest levels are found in wines coming from countries where irrigation is carried out using salty water or in areas with brackish terrains.","e6cfcad4":"### **White Model**","67e97732":"### **Combined**","3b8d9979":"# **Conclusions**","795c7b35":"* Most features show different behavior regarding the wine type, but the type by itself can\u2019t predict the quality. \n* Should we predict the wine type, one could expect high prediction rates. \n* Quality prediction rate is expected to be low, due to imbalanced dataset\n* The ability to predict categories 3, 4, 8 & 9 is expected to be very low due to small sample size \n* Imputing nulls must take into consideration the wine type\n* Outliers removals is essential in almost 50% of features\n* Scaling is required due to different values scale (<1, 10s, 100s)\n* The engineered features were aimed to unite similar indicative feature to a more solid feature. But while in sulfur we decided to drop the original features, at acidity we will use it all.\n* We suspect that applying different prediction models on white and red wines may get higher prediction score.\n","dbf124eb":"Examining the high values (above 325) of total sulfur dioxide reveals that one point (id 4745) is the outlier from FSD section with 440 level. ","b8bd1a1c":"Red wines have higher sulphates values than white wines.\n","f2a58d0f":"## **Citric acid**","a16e49bf":"## **Type**","071e7afa":"# **Data Insights**","4318f541":"## **VotingClassifier**","8d11adc2":"## **Feature Engineering**","f53210e2":"## **Quality: The Target**\n\nWine experts graded the wine quality between 0 (very bad) and 10 (very excellent). The eventual quality score is the median of at least three evaluations made by the same wine experts. De facto values begin at 3, and ends at 8 or 9 (red and white wines respectively).","15f090dd":"This is the single categorical feature in the dataset, except for the target. There are two types of wines - red wine and white wine.\n\n* There are no null values \n\n* The two type of wine are divided 75%-25% (white\/red)","645b9101":"### **Red Model**","06ee3d0d":"XGBoost conclusions:\n![image.png](attachment:image.png)","6c7d0ff0":"## **Volatile acidity**","05c678d2":"The purpose of this data set is to predict the quality of a wine based on its chemical features. ","6a20f220":"At this point we tried Logistic Regression. We got a valid model with\n* Train Cohen kappa score is: 0.251\n* Test Cohen kappa score is: 0.239\n\n**Therefore, Logistic Regression will be our baseline model**","bff4b7ea":"# **Wine Quality Classification**\n---\n**Processed by: Guy Kahana & Anat Peled**\n\nSource: [Wine-quality](https:\/\/www.kaggle.com\/rajyellow46\/wine-quality)\n\nAcknowledgements: \nP. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. *Modeling wine preferences by data mining from physicochemical properties*. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.\n\nPDF presentaion is available here: [Presentation](https:\/\/github.com\/pelanat1207\/Project_4-Wine-Quality\/blob\/main\/wine_quality_pdf.pdf)","536f45e3":"![image.png](attachment:image.png)","30ac636c":"This model use differently the features for red and white wines. ","fb99d95b":"### **Combined**","d12f9ef0":"We define a function that removes outliers according to a preset dictionary.","b626efb9":"## **Logistic Regreesion**\n\n \n","afd76ce8":"### **Red Model**"}}