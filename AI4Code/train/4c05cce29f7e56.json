{"cell_type":{"9cb9cd99":"code","5e2cf173":"code","dbd788ae":"code","119f8abe":"code","22bb7667":"code","2c63c89c":"code","9e3fabd2":"code","5dd898cc":"code","37e2ad9c":"code","2daa5c2a":"code","c64a5842":"code","71a0e6c0":"code","eb19ae3c":"code","c36b5ac3":"code","5908c429":"code","d56dc93f":"code","2632b5c9":"code","ca0b6840":"code","b4c1a9d7":"code","981c8d96":"code","5bc16a62":"code","1a73bca9":"code","f81f19cb":"code","61acdecd":"code","ca0d6682":"code","90e5950b":"code","83fa6492":"code","53fdb20d":"code","c51c21ca":"code","f748ee4b":"code","ceb49c52":"code","8f27f7a1":"code","8c908908":"code","d6c03fa4":"code","6f3aca9d":"code","fc16894d":"code","df335b0f":"code","2e63b61c":"code","aee9ed68":"code","a1d2b35e":"markdown","d8d3d9aa":"markdown","b6bd458d":"markdown","55f8f3d2":"markdown","adef3ada":"markdown","6aec0f5a":"markdown","9c8da3b0":"markdown","101949fa":"markdown","47064061":"markdown","b9b1cea1":"markdown","f1509f0e":"markdown","1706102b":"markdown","83048c0f":"markdown","eecb093d":"markdown","cd3ac447":"markdown","5cce7744":"markdown","d4f62901":"markdown","709fe4e7":"markdown","529c0bef":"markdown","1ce41573":"markdown","d24bc932":"markdown","5d2a0c8a":"markdown","5f09d0c1":"markdown","a4a061f4":"markdown","7fa7de05":"markdown","ac46c084":"markdown","96587716":"markdown"},"source":{"9cb9cd99":"import numpy as np   \nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd    \nimport matplotlib.pyplot as plt \n%matplotlib inline \nimport seaborn as sns\nfrom scipy.stats import zscore\nfrom sklearn import preprocessing\n#Test Train Split\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\n#Feature Scaling library\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn import svm\n\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\n#ROC Curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n# Suppressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","5e2cf173":"vData = pd.read_csv(\"..\/input\/vehicle\/vehicle.csv\")  \nvData.shape","dbd788ae":"vData.head()","119f8abe":"vData.dtypes","22bb7667":"vData.describe().T","2c63c89c":"vData.isnull().values.any() # If there are any null values in data set","9e3fabd2":"# Checking the presence of missing values\nnull_counts = vData.isnull().sum()  # This prints the columns with the number of null values they have\nprint (null_counts)","5dd898cc":"#instead of dropping the rows, lets replace the missing values with median value. \nvData.median()","37e2ad9c":"# function to replace missing values with median values\ndef impute_with_median (df):\n#    \"\"\"Iterate through columns of Pandas DataFrame.\n#    Where NaNs exist replace with median\"\"\"\n    \n   # Get list of DataFrame column names\n    cols = list(df)\n    # Loop through columns\n    for column in cols:\n        # Transfer column to independent series\n        col_data = df[column]\n        # Look to see if there is any missing numerical data\n        missing_data = sum(col_data.isna())\n        if missing_data > 0:\n            # Get median and replace missing numerical data with median\n            col_median = col_data.median()\n            col_data.fillna(col_median, inplace=True)\n            df[column] = col_data\n    return df  ","2daa5c2a":"impute_with_median (vData)","c64a5842":"vData.isnull().values.any() # If there are any null values in data set","71a0e6c0":"# studying the distribution of continuous attributes mean, median, mode defining the central tendency, \n# standard deviation refecting the spread and skewness reflecting the tail \ncols = list(vData.columns)\ncols.remove('class')\nfor i in np.arange(len(cols)):\n    sns.distplot(vData[cols[i]], color='blue')\n    #plt.xlabel('Experience')\n    plt.show()\n    print('Distribution of ',cols[i])\n    print('Mean is:',vData[cols[i]].mean())\n    print('Median is:',vData[cols[i]].median())\n    print('Mode is:',vData[cols[i]].mode())\n    print('Standard deviation is:',vData[cols[i]].std())\n    print('Skewness is:',vData[cols[i]].skew())\n    print('Maximum is:',vData[cols[i]].max())\n    print('Minimum is:',vData[cols[i]].min())\n    \n","eb19ae3c":"# Getting the distribution of the target class variable\nprint(vData['class'].value_counts())\nvData[\"class\"].value_counts().plot.bar(title='Freq dist of vehicle types')","c36b5ac3":"sns.pairplot(vData, hue=\"class\")\nplt.show()","5908c429":"df_corr = vData.corr()\nplt.subplots(figsize =(12, 7)) \nsns.heatmap(df_corr,annot=True)","d56dc93f":"# Checking the presence of outliers\nl = len(vData)\ncol = list(vData.columns)\ncol.remove('class')\nfor i in np.arange(len(col)):\n    sns.boxplot(x= vData[col[i]], color='cyan')\n    plt.show()\n    print('Boxplot of ',col[i])\n    #calculating the outiers in attribute \n    Q1 = vData[col[i]].quantile(0.25)\n    Q2 = vData[col[i]].quantile(0.50)\n    Q3 = vData[col[i]].quantile(0.75) \n    IQR = Q3 - Q1\n    L_W = (Q1 - 1.5 *IQR)\n    U_W = (Q3 + 1.5 *IQR)    \n    print('Q1 is : ',Q1)\n    print('Q2 is : ',Q2)\n    print('Q3 is : ',Q3)\n    print('IQR is:',IQR)\n    print('Lower Whisker, Upper Whisker : ',L_W,',',U_W)\n    bools = (vData[col[i]] < (Q1 - 1.5 *IQR)) |(vData[col[i]] > (Q3 + 1.5 * IQR))\n    print('Out of ',l,' rows in data, number of outliers are:',bools.sum())   #calculating the number of outliers","2632b5c9":"#Removing outliers by removing data below lower whisker and above upper whisker\nl = len(vData)\nfor i in np.arange(len(col)):\n    Q1 = vData[col[i]].quantile(0.25)\n    Q3 = vData[col[i]].quantile(0.75)\n    IQR = Q3 - Q1\n    vData = vData[(vData[col[i]] > (Q1 - 1.5 *IQR)) & (vData[col[i]] < (Q3 + 1.5 *IQR))]","ca0b6840":"# verifying for imbalance after treating the outliers, since percentage less than 25%(imbalance ratio of UCI dataset) is imbalanced\nvData['class'].value_counts() ","b4c1a9d7":"# Creating scores dataframe to store respective model scores for assessment\ndf_score = pd.DataFrame({\"accuracy_score\":[0,0,0,0,0,0]})\ndata = {'accuracy_score':[0,0,0,0,0,0]}\ndf_score = pd.DataFrame(data, index=['SVM_18', 'SVM_Cross_Validation_18','SVM_11','SVM_Cross_Validation_11', 'SVM_PCA_7', 'SVM_Cross_Validation_PCA_7'])","981c8d96":"replaceStruct = {\n                \"class\":     {\"car\": 0, \"bus\": 1, \"van\": 2},\n                }","5bc16a62":"vData1 = vData.replace(replaceStruct)","1a73bca9":"X_vData = vData1.drop(['class'], axis=1)\nX = vData1.drop(['class'], axis=1)\ny = vData1['class']","f81f19cb":"XScaled = X.apply(zscore)","61acdecd":"# Split the data into training and test set in the ratio of 70:30 respectively  \n\nX_train18, X_test18, y_train, y_test = train_test_split(XScaled, y, train_size=0.7, test_size=0.3, random_state=56)","ca0d6682":"#verifying to ensure there are some records with 'status' = 1 for training and testing both.\nprint('y_train value counts are:\\n',y_train.value_counts())\nprint('y_test value counts are:\\n',y_test.value_counts())","90e5950b":"# Calculating SVM model score with 18 dependent variables\nsvm1 = svm.SVC(gamma=0.025, C=3)   \nsvm1.fit(X_train18 , y_train)\ny_pred18 = svm1.predict(X_test18)\n# Score of the Model\nprint('SVM score on training data :', svm1.score(X_train18, y_train))\nprint(\"Accuracy score on test data before PCA  = \", accuracy_score(y_test, y_pred18))\ndf_score.loc['SVM_18', 'accuracy_score'] = accuracy_score(y_test, y_pred18)\ndf_score","83fa6492":"#Perform K-fold cross validation on SVM model with 18 dependent features\nnum_folds = 50\nseed = 7\nkfold = KFold(n_splits=num_folds, random_state=seed)\n\nresults18 = cross_val_score(svm1, XScaled, y, cv=kfold)\nprint(\"Cross validation Accuracy: %.3f%% Std dev:%.3f%%\" % (results18.mean()*100.0, results18.std()*100.0))\ndf_score.loc['SVM_Cross_Validation_18', 'accuracy_score'] = results18.mean()\ndf_score","53fdb20d":"# Use PCA and extract Principal Components from the 18 dependent features\ncovMatrix18 = np.cov(XScaled,rowvar=False)\npca18 = PCA(n_components=18)\npca18.fit(XScaled)\nprint(pca18.explained_variance_)\nprint(pca18.components_)","c51c21ca":"print(pca18.explained_variance_ratio_)\nplt.figure(figsize= (20,4))\nplt.bar(list(range(1,19)),pca18.explained_variance_ratio_,alpha=0.5, align='center')\nplt.ylabel('Variation explained')\nplt.xlabel('eigen Value')\nplt.show()","f748ee4b":"plt.step(list(range(1,19)),np.cumsum(pca18.explained_variance_ratio_), where='mid')\nplt.ylabel('Cum of variation explained')\nplt.xlabel('eigen Value')\nplt.show()","ceb49c52":"# considering the correlation heat map we drop the highly correlated coulmns and select 11 features\n\nXScaled11 = XScaled.drop(['distance_circularity','elongatedness','pr.axis_rectangularity','max.length_rectangularity','scaled_variance','scaled_variance.1','scaled_radius_of_gyration'], axis=1)\n\n\n# Split the data into training and test set in the ratio of 70:30 respectively  \n\nX_train11, X_test11, y_train, y_test = train_test_split(XScaled11, y, train_size=0.7, test_size=0.3, random_state=56)","8f27f7a1":"svm1.fit(X_train11 , y_train)\ny_pred11 = svm1.predict(X_test11)\n# Score of the Model\nprint('SVM score :', svm1.score(X_test11, y_test))\nprint(\"Accuracy score with 11 dependent variables  = \", accuracy_score(y_test, y_pred11))\ndf_score.loc['SVM_11', 'accuracy_score'] = accuracy_score(y_test, y_pred11)\ndf_score","8c908908":"#Perform K-fold cross validation on SVM model with 11 dependent features\nnum_folds = 50\nseed = 7\nkfold = KFold(n_splits=num_folds, random_state=seed)\n\nresults11 = cross_val_score(svm1, XScaled11, y, cv=kfold)\nprint(\"Cross validation for 11 features, Accuracy: %.3f%% Std dev:%.3f%%\" % (results11.mean()*100.0, results11.std()*100.0))\ndf_score.loc['SVM_Cross_Validation_11', 'accuracy_score'] = results11.mean()\ndf_score","d6c03fa4":"#Fitting and transforming the train & test data created earlier using 7 principal components\npca7 = PCA(n_components= 7)\npca7.fit(XScaled)\n#Xpca7 = pca7.fit(XScaled)\n","6f3aca9d":"print(pca7.components_)\nprint(pca7.explained_variance_ratio_)","fc16894d":"Xpca7 = pca7.transform(XScaled)\nXpca7.shape\n","df335b0f":"# Calculating SVM model score with 7 principal components\n#svm = svm1.SVC(gamma=0.025, C=3)   \nX_train_pca7, X_test_pca7, y_train, y_test = train_test_split(Xpca7, y, train_size=0.7, test_size=0.3, random_state=56)\nsvm1.fit(X_train_pca7, y_train)\ny_pred7 = svm1.predict(X_test_pca7)\nprint('SVM score on training after PCA :', svm1.score(X_train_pca7, y_train))\nprint(\"Accuracy score on test after PCA  = \", accuracy_score(y_test, y_pred7))\ndf_score.loc['SVM_PCA_7', 'accuracy_score'] = accuracy_score(y_test, y_pred7)\ndf_score","2e63b61c":"#Perform K-fold cross validation on SVM model with 7 principal components\nnum_folds = 50\nseed = 7\nkfold = KFold(n_splits=num_folds, random_state=seed)\n\nresults7 = cross_val_score(svm1,Xpca7, y, cv=kfold)\nprint(\"Cross validation for principal 7 components, Accuracy: %.3f%% Std dev:%.3f%%\" % (results7.mean()*100.0, results7.std()*100.0))\ndf_score.loc['SVM_Cross_Validation_PCA_7', 'accuracy_score'] = results7.mean()\ndf_score","aee9ed68":"# SVM_18 is the score of the raw data trained on svm model\n#SVM_Cross_Validation_18 is the cross validation score when all 18 components are used on svm.\n# SVM_11 is the score of the 11 attributes used to train the svm model. These attributes were selected by removing highly correlated ones.  \n#SVM_Cross_Validation_11 is the cross validation score of the above 11 atributes on an svm model.\n#SVM_PCA_7 is the score of the 7 attributes (selected after principal component analysis of raw data) on svm\n#SVM_Cross_Validation_PCA_7 is the cross validation score of the above 7 atributes on an svm model.\n\nsc = df_score.loc[['SVM_18', 'SVM_Cross_Validation_18','SVM_11','SVM_Cross_Validation_11', 'SVM_PCA_7', 'SVM_Cross_Validation_PCA_7']]\nplt.figure(figsize= (15,6))\nplt.subplot(1, 1, 1)\nsc['accuracy_score'].plot.bar()\nplt.title('Accuracy Scores')\ndf_score","a1d2b35e":"the histogram above shows that the counts of car,bus and van are in the descending order.","d8d3d9aa":"# Dropping coulmns based on high correlation, for feature selection. Calculating score of the model with selected features and verifying with cross validation.","b6bd458d":"radius_ratio, pr.axis_aspect_ratio, max.length_aspect_ratio,scaled_variance,scaled_variance.1,scaled_radius_of_gyration.1,\nskewness_about, skewness_about.1 have outliers which needs to be removed.","55f8f3d2":"As in the data, the count of 'car' is more compared to 'bus' and 'van'.There are chances that the model perdiction may get affected.\n    \nLooks like the 'bus' has lesser elongatedness and scatter ratio is highest but the 'van' is vice versa where elongatedness\nis higher and scatter ratio is less.\n    \n'car' has a wide range of cicularity and also max.length_rectangularity whereas the 'bus' and 'van' do'nt. \nSo cicularity and max.lenght_rectangularity can be used to classify mainly 'bus' and 'van' but not 'cars'.","adef3ada":"The percentage variation explained by first 7 dimensions covers about 96% variation in the data. Hence we an use, 7 dimensions  to train the model and do prediction on test data.","6aec0f5a":"Apart from 'class' rest are all numeric attributes.","9c8da3b0":"Assigning the class variable with numeric values as SVM & PCA operates on numeric data","101949fa":"7. Repeat steps 3,4 and 5 but this time, use Principal Components instead of the original data. And the accuracy score \n   should be on the same rows of test data that were used earlier.","47064061":"1. Data pre-processing \u2013 Perform all the necessary preprocessing on the data ready to be fed to an Unsupervised algorithm","b9b1cea1":"# Using the 7 principal components to train, test and cross validate the svm scores against those of all 18 dependent features.","f1509f0e":"'max.length_aspect_ratio' has highly positive skewed data.\n\n'circularity','scatter_ratio','pr.axis_rectangularity','scaled_radius_of_gyration' are right skewed.\n\n'distance_circularity' is left skewed.\n\n'scaled_variance.1','scatter_ratio' have high variance.\n\n'max.length_rectangularity' is trimodal.\n\n'distance_circularity','elongatedness','max.length_aspect_ratio','scatter_ratio','pr.axis_rectangularity','scaled_variance' ,\n'scaled_variance.1','hollows_ratio' are bimodal.","1706102b":"DATA SETUP","83048c0f":"8.  Compare the accuracy scores and cross validation scores of Support vector machines \u2013 one trained using raw data and\nthe other using Principal Components, and mention your findings.","eecb093d":"'compactness','circularity','distance_circularity','radius_ratio','scatter_ratio'  has positive correlation with most \n of the attributes and is not much correlated to others.\n \n'pr.axis_aspect_ratio' and 'max.length_aspect_ratio' shows a line plot with most of the attributes, showing the counts \n of all three classes.\n \n'elongatedness' has negative correlation with most of the attributes and is not much correlated to others. \n\n'scaled_radius_of_gyration.1' has negative correlation with 'skewness_about.2' and 'hollows_ratio'.\n\n'skewness_about' and 'skewness_about.1' are not much correlated with 'skewness_about.2' and 'hollows_ratio'.\n\n'skewness_about.2' and 'hollows_ratio' have high positive correlation.\n","cd3ac447":"The number of missing values for every attribute are mentioned above.","5cce7744":"# Case Study : Classification of a given silhouette as one of three types of vehicle","d4f62901":"6. Use PCA from Scikit learn, extract Principal Components that capture about 95% of the variance in the data ","709fe4e7":"5. Perform K-fold cross validation and get the cross validation score of the model ","529c0bef":"Looking at the scores from the above histogram, \n\nThe raw data and it's cross validation gave 98% accuracy.\n\nThe 11 attributes selected after removing the highy correlated ones gave approximately 97% accuracy.\n\nThe 7 principal components that were selected, show 93% accuracy since they contribute more in classifying the silhouette as car, bus or van.Their cross validation too gives the same score with 6% standard deviation.\n\n\nThe PCA ensures reduction in correlation and thus reduction in dimensional complexity such that there is not much of data loss and around 96% of the feature data were retained.\n\n","1ce41573":"3. Split the data into train and test (Suggestion: specify \u201crandom state\u201d if you are using train_test_split from Sklearn)","d24bc932":"2. Understanding the attributes - Find relationship between different attributes (Independent variables) and \n   choose carefully which all attributes have to be a part of the analysis and why ","5d2a0c8a":"Bar Chart of all the Eigen values vs the Variance shows maximum variance is covered under first 6 to 7 dimensions","5f09d0c1":"Data Description:  \nThe data contains features extracted from the silhouette of vehicles in different angles. Four \"Corgie\" model vehicles\nwere used for the experiment: a double decker bus, Cheverolet van, Saab 9000 and an Opel Manta 400 cars. This particular\ncombination of vehicles was chosen with the expectation that the bus, van and either one of the cars would be readily\ndistinguishable, but it would be more difficult to distinguish between the cars. \n\nContext: \nThe purpose is to classify a given silhouette as one of three types of vehicle, using a set of features extracted from \nthe silhouette. The vehicle may be viewed from one of many different angles. \n\nDomain:  \nObject recognition \n\nAttribute Information: \n    \n    \u25cf All the features are geometric features extracted from the silhouette.  \n    \u25cf All are numeric in nature. \n \n\nObjective:  \nApply dimensionality reduction technique \u2013 PCA and train a model using principle components instead of training the \nmodel using just the raw data. \n","a4a061f4":"The above plot showing the cumulative variance, which also indicates first 7 dimensions covers maximum variation in the data","7fa7de05":"'circularity' has high correlation with 'scaled_radius_of_gyration' and 'max.length_rectangularity' and vice versa.\n\n'distance_circularity' has high correlation with 'elongatedness' and 'scatter_ratio'.\n\n'scatter_ratio' has high correlation with 'pr.axis_rectangularity','scaled_variance.1','scaled_variance' and 'elongatedness', 'distance_circularity'.\n\n'pr.axis_rectangularity' has high correlation with 'scaled_variance.1','scaled_variance', 'scatter_ratio' and 'elongatedness'.\n\n'scaled_variance' and 'scaled_variance.1' have high correlation with each other and 'scatter_ratio','pr.axis_rectangularity'.\n","ac46c084":"Looks like there are null values in the data, since the count of data in the attributes is different.\nMean of the scaled_variance.1 is far from the 50% value, it is near the minimum value and far from the max, which means\nthe data is skewed and variance is high. High variance is also justified by highest standard deviation of scaled_variance.1 \nof all attributes.","96587716":"4. Train a Support vector machine using the train set and get the accuracy on the test set"}}