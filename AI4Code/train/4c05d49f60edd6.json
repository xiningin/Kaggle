{"cell_type":{"69768365":"code","9febb0a5":"code","07c31b93":"code","b6d8dd05":"code","84c85693":"code","c1a78dc8":"code","682e02e0":"code","f9cb791d":"code","716dcaaa":"code","f1bd8e50":"code","5f3b5f94":"code","2b82e6c7":"code","83543449":"code","a822b45a":"code","4ea8d131":"code","09ff37b3":"code","7ff5d072":"code","9b98e409":"code","a5f11408":"code","52cd970c":"code","2fb8981b":"code","88911781":"code","27c17759":"code","b57ed880":"code","4589f581":"code","f78274fc":"code","28dea3f5":"code","7c5df9fc":"code","3e8b6157":"code","45079335":"code","88725fb0":"code","06d097c1":"code","ce4211ba":"code","5b1c069c":"code","8b07db2a":"code","d6211dbf":"code","8af12a04":"code","3edb27da":"code","115a5123":"code","1bb00f99":"code","915c798d":"code","5a3f728c":"code","c96d1f59":"code","87d26a8b":"code","a218c48e":"code","c4d95a6e":"code","a261eda9":"code","5ba89f41":"code","a3c54f53":"code","93215e4f":"code","817c0e1a":"code","63a11904":"code","0f4dad55":"code","4ae2a55f":"code","9e8fbc0c":"code","765500d0":"code","faeb64d2":"code","fa1ee069":"code","7779d452":"code","33700e5a":"code","a0f9ba1a":"code","0da041a5":"code","71767c0b":"code","79090c67":"code","0822b2d1":"code","9d113e37":"code","6a777e03":"code","dbce2a5b":"code","d6b00a82":"code","95561391":"code","65886705":"code","3fd42654":"code","9195c7bb":"code","3e44dce3":"code","b6eaf17b":"code","3e89a89c":"code","4df37a4c":"code","d723bb3c":"code","eda9d0f9":"markdown","1d5c4af3":"markdown","f3087ab8":"markdown","14047355":"markdown","c1a82192":"markdown","03d077af":"markdown","16180bf1":"markdown","ae291c4a":"markdown","2bb2f42f":"markdown","3d9355b5":"markdown","7e543db4":"markdown","c99a8ee5":"markdown","2d82c508":"markdown","226833ae":"markdown","674bf6ee":"markdown","6e5d039c":"markdown","681426d4":"markdown","93d72183":"markdown","e6b4a87a":"markdown","d9bdba13":"markdown","7e31c460":"markdown"},"source":{"69768365":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', 500)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.feature_selection import SelectPercentile, univariate_selection, RFE\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, cross_val_score, RepeatedStratifiedKFold,\\\nStratifiedKFold\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler, OneHotEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.metrics import classification_report, plot_confusion_matrix, precision_recall_curve, plot_precision_recall_curve, confusion_matrix\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\nimport category_encoders as ce\nfrom scipy import stats\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9febb0a5":"telco = pd.read_excel('\/kaggle\/input\/telco-customer-churn-ibm-dataset\/Telco_customer_churn.xlsx')","07c31b93":"telco.head()","b6d8dd05":"telco.info()","84c85693":"telco.describe()","c1a78dc8":"def report(df):\n    col = []\n    d_type = []\n    uniques = []\n    n_uniques = []\n    \n    for i in df.columns:\n        col.append(i)\n        d_type.append(df[i].dtypes)\n        uniques.append(df[i].unique()[:5])\n        n_uniques.append(df[i].nunique())\n    \n    return pd.DataFrame({'Column': col, 'd_type': d_type, 'unique_sample': uniques, 'n_uniques': n_uniques})","682e02e0":"report(telco)","f9cb791d":"# one time run\nnew_col = telco.columns.str.replace(' ', '_')\ntelco.columns = new_col","716dcaaa":"telco.columns","f1bd8e50":"telco.drop('CustomerID Count City Country State Zip_Code Lat_Long Latitude Longitude'.split(), axis=1, inplace=True)","5f3b5f94":"telco.head()","2b82e6c7":"telco.columns","83543449":"plt.figure(figsize=(10,5))\nplt.pie(telco['Churn_Label'].value_counts(), labels=telco['Churn_Label'].unique(), autopct='%.2f%%')\nplt.show()","a822b45a":"by_gender = telco.groupby('Churn_Label')['Gender'].value_counts().to_frame().rename(columns={'Gender': 'Freq'}).reset_index().sort_values('Churn_Label')\n\n# Make data: I have 3 groups and 7 subgroups\ngroup_names=telco['Churn_Label'].value_counts().index\ngroup_size=telco['Churn_Label'].value_counts()\nsubgroup_names=by_gender['Gender']\nsubgroup_size=by_gender['Freq']\n \n# Create colors\na, b =[plt.cm.Blues, plt.cm.Reds]\n \n# First Ring (outside)\nfig, ax = plt.subplots()\nfig.suptitle('Churn by Gender')\nax.axis('equal')\nmypie, _ = ax.pie(group_size, radius=1.3, labels=group_names, colors=[a(0.6), b(0.6)])\nplt.setp( mypie, width=0.3, edgecolor='white')\n \n# Second Ring (Inside)\nmypie2, _ = ax.pie(subgroup_size, radius=1.3-0.3, labels=subgroup_names, labeldistance=0.7, colors=[a(0.5), a(0.4), b(0.5), b(0.4)])\nplt.setp( mypie2, width=0.4, edgecolor='white')\nplt.margins(0,0)\n \n# show it\nplt.show()\n","4ea8d131":"telco.groupby('Churn_Label')['Gender'].value_counts(normalize=True).to_frame().rename(columns={'Gender': 'Ratio'}).reset_index().sort_values('Churn_Label')","09ff37b3":"by_senior = telco.groupby('Churn_Label')['Senior_Citizen'].value_counts().to_frame().rename(columns={'Senior_Citizen': 'Freq'}).reset_index().sort_values('Churn_Label')\n\n# Make data: I have 3 groups and 7 subgroups\ngroup_names=telco['Churn_Label'].value_counts().index\ngroup_size=telco['Churn_Label'].value_counts()\nsubgroup_names=by_senior['Senior_Citizen']\nsubgroup_size=by_senior['Freq']\n \n# Create colors\na, b =[plt.cm.Blues, plt.cm.Reds]\n \n# First Ring (outside)\nfig, ax = plt.subplots()\nfig.suptitle('Churn by Senior Citizen')\nax.axis('equal')\nmypie, _ = ax.pie(group_size, radius=1.3, labels=group_names, colors=[a(0.6), b(0.6)])\nplt.setp( mypie, width=0.3, edgecolor='white')\n \n# Second Ring (Inside)\nmypie2, _ = ax.pie(subgroup_size, radius=1.3-0.3, labels=subgroup_names, labeldistance=0.7, colors=[a(0.5), a(0.4), b(0.5), b(0.4)])\nplt.setp( mypie2, width=0.4, edgecolor='white')\nplt.margins(0,0)","7ff5d072":"plt.figure(figsize=(10,5))\nplt.pie(telco['Senior_Citizen'].value_counts(), labels=telco['Senior_Citizen'].unique(), autopct='%.2f%%')\nplt.show()","9b98e409":"telco['Senior_Citizen'].value_counts()","a5f11408":"sns.catplot(x='Gender', hue='Churn_Label', col='Senior_Citizen', kind='count', data=telco)","52cd970c":"by_gender_senior = telco.groupby(['Senior_Citizen', 'Gender'])['Churn_Label'].value_counts(normalize=True).to_frame().rename(columns={'Churn_Label': 'Ratio'}).reset_index().sort_values('Senior_Citizen')\nby_gender_senior","2fb8981b":"plt.figure(figsize=(10,5))\nplt.pie(telco['Partner'].value_counts(), labels=telco['Partner'].unique(), autopct='%.2f%%')\nplt.show()","88911781":"by_partner = telco.groupby('Churn_Label')['Partner'].value_counts().to_frame().rename(columns={'Partner': 'Freq'}).reset_index().sort_values('Churn_Label')\n\n# Make data: I have 3 groups and 7 subgroups\ngroup_names=telco['Churn_Label'].value_counts().index\ngroup_size=telco['Churn_Label'].value_counts()\nsubgroup_names=by_partner['Partner']\nsubgroup_size=by_partner['Freq']\n \n# Create colors\na, b =[plt.cm.Blues, plt.cm.Reds]\n \n# First Ring (outside)\nfig, ax = plt.subplots()\nfig.suptitle('Churn by Partner')\nax.axis('equal')\nmypie, _ = ax.pie(group_size, radius=1.3, labels=group_names, colors=[a(0.6), b(0.6)])\nplt.setp( mypie, width=0.3, edgecolor='white')\n \n# Second Ring (Inside)\nmypie2, _ = ax.pie(subgroup_size, radius=1.3-0.3, labels=subgroup_names, labeldistance=0.7, colors=[a(0.5), a(0.4), b(0.5), b(0.4)])\nplt.setp( mypie2, width=0.4, edgecolor='white')\nplt.margins(0,0)","27c17759":"sns.catplot(x='Gender', hue='Churn_Label', col='Partner', kind='count', data=telco)\nplt.show()","b57ed880":"by_gender_partner = telco.groupby(['Partner', 'Gender'])['Churn_Label'].value_counts(normalize=True).to_frame().rename(columns={'Churn_Label': 'Ratio'}).reset_index().sort_values('Partner')\nby_gender_partner","4589f581":"by_gp = telco.groupby(['Partner', 'Gender', 'Churn_Label'])\nby_gp_mc = by_gp['Monthly_Charges'].mean().to_frame().reset_index().sort_values('Partner')\nby_gp_mc_no = by_gp_mc[by_gp_mc['Partner'] == 'No']\nby_gp_mc_yes = by_gp_mc[by_gp_mc['Partner'] == 'Yes']","f78274fc":"plt.figure(figsize=(10,5))\nplt.title('Tenure (Months) distribution')\nsns.distplot(telco[telco['Churn_Label'] == 'Yes']['Tenure_Months'], label='Churn')\nsns.distplot(telco[telco['Churn_Label'] == 'No']['Tenure_Months'], label='Retain')\nplt.legend(loc= 'upper right')\nplt.show()","28dea3f5":"plt.figure(figsize=(16,5))\nplt.subplot(1,2,1)\nplt.title('Tenure (Months) distribution')\nsns.distplot(telco[telco['Churn_Label'] == 'Yes']['Tenure_Months'], label='Churn')\nplt.subplot(1,2,2)\nplt.title('Tenure (Months) distribution Split by Gender')\nsns.distplot(telco[(telco['Churn_Label'] == 'Yes') & (telco['Gender'] == 'Male')]['Tenure_Months'], label='Male')\nsns.distplot(telco[(telco['Churn_Label'] == 'Yes') & (telco['Gender'] == 'Female')]['Tenure_Months'], label='Female')\nplt.legend()\nplt.show()","7c5df9fc":"plt.figure(figsize=(10,5))\nplt.title('Monthly Charge distribution')\nsns.distplot(telco[telco['Churn_Label'] == 'Yes']['Monthly_Charges'], label='Churn')\nsns.distplot(telco[telco['Churn_Label'] == 'No']['Monthly_Charges'], label='Retain')\nplt.legend(loc= 'upper right')\nplt.show()","3e8b6157":"plt.figure(figsize=(20,16))\nplt.subplot(331)\ntelco[(telco['Monthly_Charges'] <= 25) & (telco['Churn_Value']==0)]['Gender'].value_counts().plot(kind='barh')\nplt.xlabel('frequency')\nplt.subplot(332)\ntelco[(telco['Monthly_Charges'] <= 25) & (telco['Churn_Value']==0)]['Tenure_Months'].value_counts().plot(kind='hist')\nplt.axvline(50, label='tenure 50 months', color='red')\nplt.legend()\nplt.xlabel('tenure months')\nplt.subplot(333)\ntelco[(telco['Monthly_Charges'] <= 25) & (telco['Churn_Value']==0)]['Dependents'].value_counts().plot(kind='pie')\nplt.subplot(334)\ntelco[(telco['Monthly_Charges'] <= 25) & (telco['Churn_Value']==0)]['Partner'].value_counts().plot(kind='pie')\nplt.subplot(335)\ntelco[(telco['Monthly_Charges'] <= 25) & (telco['Churn_Value']==0)]['Senior_Citizen'].value_counts().plot(kind='pie')\nplt.subplot(336)\ninternet_service_df = telco[(telco['Monthly_Charges'] <= 25) & (telco['Churn_Value']==0)]['Internet_Service']\nsns.countplot(internet_service_df)\nplt.subplot(337)\nstreaming_movie_df = telco[(telco['Monthly_Charges'] <= 25) & (telco['Churn_Value']==0)]['Streaming_Movies']\nsns.countplot(streaming_movie_df)","45079335":"plt.figure(figsize=(16,5))\nplt.subplot(1,2,1)\nplt.title('Monthly Charge distribution')\nsns.distplot(telco[telco['Churn_Label'] == 'Yes']['Monthly_Charges'], label='Churn')\nplt.subplot(1,2,2)\nplt.title('Monthly Charge distribution Split by Gender')\nsns.distplot(telco[(telco['Churn_Label'] == 'Yes') & (telco['Gender'] == 'Male')]['Monthly_Charges'], label='Male')\nsns.distplot(telco[(telco['Churn_Label'] == 'Yes') & (telco['Gender'] == 'Female')]['Monthly_Charges'], label='Female')\nplt.legend()\nplt.show()","88725fb0":"sns.catplot(x='Payment_Method', hue='Churn_Label', kind='count', data=telco)\nplt.xticks(rotation=45)\nplt.show()","06d097c1":"pd.crosstab(telco['Payment_Method'], telco['Churn_Label'], normalize=0)","ce4211ba":"telco.info()","5b1c069c":"telco['Total_Charges'].value_counts()","8b07db2a":"telco['Total_Charges'].replace(' ', np.nan, inplace=True)","d6211dbf":"telco['Total_Charges'].value_counts()","8af12a04":"sns.heatmap(telco.isna())","3edb27da":"pd.crosstab(telco['Payment_Method'], telco['Churn_Label'], values=telco['Total_Charges'], aggfunc=np.mean)","115a5123":"telco.head()","1bb00f99":"telco2 = telco.drop(['Churn_Label', 'Churn_Score', 'CLTV', 'Churn_Reason'], axis=1).copy()","915c798d":"telco2[['Monthly_Charges', 'Total_Charges']].corr()","5a3f728c":"summary_df = report(telco2)\none_hot_cols = summary_df[summary_df['d_type']=='object']['Column']","c96d1f59":"summary_df","87d26a8b":"telco2['Churn_Value'].value_counts(normalize=True).plot(kind='pie', figsize=(5,5))","a218c48e":"X = telco2.drop(columns='Churn_Value')\ny = telco2['Churn_Value']\n\nX_train_val, X_test, y_train_val, y_test = train_test_split(X,y,test_size=.2, stratify=y)","c4d95a6e":"logit = LogisticRegression(random_state=2021)\nrfc = RandomForestClassifier(max_depth=10)\nknn = KNeighborsClassifier(n_neighbors=5)\nsmote = SMOTE()","a261eda9":"telco2.head(1)","5ba89f41":"telco2.isna().sum()\/len(telco2)*100","a3c54f53":"telco2['Total_Charges'].plot(kind='hist')","93215e4f":"test, pval = stats.shapiro(telco2['Total_Charges'])\n\nif pval < 0.05:\n    print('normal')\nelse:\n    print('not normal')","817c0e1a":"# for monthly and total charges\nlogit_pipe_num = Pipeline([\n    ('imputer', SimpleImputer(strategy='median', missing_values=np.nan)),\n    ('scaler', StandardScaler()),\n    ('pca', PCA(n_components=1))\n])\n\n# for all object columns\nlogit_pipe_cat = Pipeline([\n    ('onehot', OneHotEncoder(drop='first')),\n])\n\n# transforming all columns\nlogit_transformer = ColumnTransformer([\n    ('pipe_num', logit_pipe_num, ['Monthly_Charges', 'Total_Charges']),\n    ('pipe_cat', logit_pipe_cat, one_hot_cols)\n])\n\n# combine all pipeline\nlogit_pipe_combine = Pipeline([\n    ('transformer', logit_transformer),\n    ('rfe', RFE(logit)),\n    ('resampling', smote),\n    ('logit', logit)\n])","63a11904":"# for monthly and total charges\nrfc_pipe_num = Pipeline([\n    ('imputer', SimpleImputer(strategy='median', missing_values=np.nan)),\n    ('scaler', StandardScaler()),\n    ('pca', PCA(n_components=1))\n])\n\n# for all object columns\nrfc_pipe_cat = Pipeline([\n    ('onehot', OneHotEncoder(drop='first')),\n])\n\n# transforming all columns\nrfc_transformer = ColumnTransformer([\n    ('pipe_num', rfc_pipe_num, ['Monthly_Charges', 'Total_Charges']),\n    ('pipe_cat', rfc_pipe_cat, one_hot_cols)\n])\n\n# combine all pipeline\nrfc_pipe_combine = Pipeline([\n    ('transformer', rfc_transformer),\n    ('rfe', RFE(rfc)),\n    ('resampling', smote),\n    ('rfc', rfc)\n])","0f4dad55":"# for monthly and total charges\nknn_pipe_num = Pipeline([\n    ('imputer', SimpleImputer(strategy='median', missing_values=np.nan)),\n    ('scaler', StandardScaler()),\n    ('pca', PCA(n_components=1))\n])\n\n# for all object columns\nknn_pipe_cat = Pipeline([\n    ('onehot', OneHotEncoder(drop='first')),\n])\n\n# transforming all columns\nknn_transformer = ColumnTransformer([\n    ('pipe_num', knn_pipe_num, ['Monthly_Charges', 'Total_Charges']),\n    ('pipe_cat', knn_pipe_cat, one_hot_cols)\n])\n\n# combine all pipeline\nknn_pipe_combine = Pipeline([\n    ('transformer', knn_transformer),\n    ('resampling', smote),\n    ('knn', knn)\n])","4ae2a55f":"import sklearn\nsklearn.metrics.SCORERS.keys()","9e8fbc0c":"rskf = RepeatedStratifiedKFold(n_splits=5, random_state=2021)\n\nlogit_score = cross_val_score(logit_pipe_combine, X_train_val, y_train_val, scoring='recall', cv=rskf, n_jobs=-1, verbose=1)\nrfc_score = cross_val_score(rfc_pipe_combine, X_train_val, y_train_val, scoring='recall', cv=rskf, n_jobs=-1, verbose=1)\nknn_score = cross_val_score(knn_pipe_combine, X_train_val, y_train_val, scoring='recall', cv=rskf, n_jobs=-1, verbose=1)","765500d0":"print('Logit Val Score:', logit_score)\nprint('RFC Val Score:', rfc_score)\nprint('KNN Val Score:', knn_score)","faeb64d2":"print('Logit Val Score:', logit_score.mean())\nprint('RFC Val Score:', rfc_score.mean())\nprint('KNN Val Score:', knn_score.mean())","fa1ee069":"logit_pipe_combine.get_params()","7779d452":"rfc_pipe_combine.get_params()","33700e5a":"knn_pipe_combine.get_params()","a0f9ba1a":"logit_params = {\n    'logit__C': np.linspace(0,1,10),\n    'logit__solver': ['lbfgs', 'liblinear', 'newton-cg'],\n    'transformer__pipe_num__imputer__strategy': ['median', 'mean']\n}\n\nrfc_params = {\n    'rfc__max_depth': np.arange(10,16),\n    'rfc__min_samples_split': np.arange(7,10),\n    'transformer__pipe_num__imputer__strategy': ['median', 'mean']\n}\n\nknn_params = {\n    'knn__n_neighbors': np.arange(3,8),\n    'transformer__pipe_num__imputer__strategy': ['median', 'mean']\n}","0da041a5":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\nlogit_grid = GridSearchCV(logit_pipe_combine, param_grid=logit_params, scoring='recall', cv=skf, verbose=1, n_jobs=-1)\nlogit_grid.fit(X_train_val, y_train_val)","71767c0b":"logit_tuned = logit_grid.best_estimator_\nlogit_tuned_score = cross_val_score(logit_tuned, X_train_val, y_train_val, scoring='recall', cv=rskf, n_jobs=-1, verbose=1)\nlogit_tuned_score","79090c67":"plt.plot(np.arange(len(logit_tuned_score)), logit_tuned_score)\nplt.ylim(0,1)\nplt.show()","0822b2d1":"logit_tuned_score.mean() # tuned dari cross val score","9d113e37":"logit_tuned.fit(X_train_val, y_train_val)\nplot_confusion_matrix(logit_tuned, X_test, y_test)","6a777e03":"logit_score.mean() #benchmark hasil dari cross val score","dbce2a5b":"logit_pipe_combine.fit(X_train_val, y_train_val)\nplot_confusion_matrix(logit_pipe_combine, X_test, y_test)","d6b00a82":"plot_precision_recall_curve(logit_tuned, X_test, y_test)","95561391":"p, r, t = precision_recall_curve(y_test, logit_tuned.predict_proba(X_test)[:,1])\nlen(p[:-1]), len(r[:-1]), len(t)\npr_df = pd.DataFrame({'precision': p[:-1], 'recall': r[:-1], 'threshold': t})\npr_df","65886705":"pr_df[(pr_df['recall'].between(0.8, 0.91)) & (pr_df['precision'].between(0.43, 0.5))].head(20)","3fd42654":"proba1 = logit_tuned.predict_proba(X_test)[:,1]\ny_pred = logit_tuned.predict(X_test)\nthresh =0.341275\npred_03 = np.where(proba1 > thresh, 1, 0)\nres_df = pd.DataFrame({'proba1': proba1, 'y_pred': y_pred, 'y_pred03': pred_03})\nplt.figure(figsize=(16,8))\nplt.subplot(121)\nsns.scatterplot(x=range(len(res_df)), y=res_df['proba1'], hue=res_df['y_pred03'])\nplt.axhline(thresh, color='red')\n\nplt.subplot(122)\nsns.scatterplot(x=range(len(res_df)), y=res_df['proba1'], hue=res_df['y_pred'])\nplt.axhline(0.5, color='red')","9195c7bb":"logit_pipe_combine.fit(X_train_val, y_train_val)\nplot_confusion_matrix(logit_pipe_combine, X_test, y_test)","3e44dce3":"logit_tuned.fit(X_train_val, y_train_val)\nplot_confusion_matrix(logit_tuned, X_test, y_test)","b6eaf17b":"print(\"Customer who don't get a treatment and run away:\", round(63\/(745+63)*100,2),\"%\")\nprint(\"Customer who get a treatment but actually still with us:\", round(290\/(290+311)*100,2),\"%\")","3e89a89c":"def make_conf_matrix(confusion):\n    tn, fp, fn, tp = confusion.flatten()\n    conf_df = pd.DataFrame({'p_0': [tn, fn, tn+fn], 'p_1': [fp, tp, fp+tp], 'total_actual': [tn+fp, fn+tp, tn+fn+tp+fp]},\n                           index=['a_0', 'a_1', 'total_prediction'])\n    return conf_df","4df37a4c":"make_conf_matrix(confusion_matrix(y_test, pred_03))","d723bb3c":"print(\"Customer who don't get a treatment and run away:\", round(31\/(621+31)*100,2),\"%\")\nprint(\"Customer who get a treatment but actually still with us:\", round(414\/(414+343)*100,2),\"%\")","eda9d0f9":"# Final Result","1d5c4af3":"**Note:** From the visualization above, we can see that, the proportion for male and female to churn or retain is almost the same.","f3087ab8":"**Note:** From the visualization above, we can see that, the longer a person uses the provider, the probability of churn decreases.","14047355":"There is a very small increase in the recall score. We will try to continue to find the best threshold using the tuned Logistic Regression.","c1a82192":"# __EDA__","03d077af":"For the modeling stage, this churn prediction will focus on as much as possible in predicting which customers will churn. As we know, if this model predicts 'retain' for a customer that will actually 'churn', then we will lose the customer. And according to some opinions, the cost of customer acquisition is 5 times greater than the cost of retaining the customer.\n\nSo that in modeling this time we will focus on recall. And we will try to use PR (Precision Recall) Curve to find the optimal threshold.","16180bf1":"## Pipeline For LogisticRegression","ae291c4a":"## Pipeline For KNeighborsClassifier","2bb2f42f":"**Note:** From the visualization above, we can see that the churn and retain ratio for senior citizens, both male and female, is almost the same. Likewise, the churn and retain ratio for non-senior citizens. But if we look, the churn ratio has increased in senior citizens when compared to non-senior citizens.","3d9355b5":"Initially I will try to make a brief report on this data. From this report I will determine which columns to delete.\n\nFrom the dataframe above, there are several columns that have one unique value, namely the column [Count, Country, State]. In addition, I will not use the CustomerID column because the customerID does not determine the probability that someone will churn or not.\n\nZip code, Lat Long, Latitude, Longitude will also be deleted. I won't use it to build Machine Learning.\n\nI will delete Churn Score and Churn Reason in the end, because this will leak information for the model later. But for now, I will use it just for looking some insights.","7e543db4":"We can see that there is a high correlation between monthly charges and total charges. We can do PCA in these two columns.","c99a8ee5":"From the visualization above, both female and male who do not have a partner are more likely to churn.","2d82c508":"There is a peak for customer retainers whose monthly charge is around 20 dollars. From the visualization above, we can see that the majority of customers whose monthly charges are still around 20 dollars are customers with tenure months of under 50 months. Most of them are Non-Senior Citizens. They don't have an internet access and haven't subscribed to many products from the provider.","226833ae":"## We did it!\n\nWe managed to lower our chances of letting off a churning customer.\n\n__Summary:__\n\nFrom the confusion matrix above, we can conclude that, of the 100 people that our machine learning model predicts, only 4.75% or only 5 people will pass or they won't get any treatment from us in order to keep using our services. Compared to the previous model, out of 100 people who are predicted to churn, 7.8% or about 8 people will pass. There was a decrease of up to 37.5%.\n\nBut this decline has consequences. Of the 100 people who are predicted to churn, 54.69% or about 55 people will get treatment from us, but actually they have no problem with our services. And even they won't actually churn. This possibility increases when compared to the previous model, which is only 48.25% or only 48 people who get treatment to stay with us. The increase is up to 12.72%.\n\n__In other words:__\n\nBased on the initial explanation, the cost of acquiring new customers is 5x the cost of retaining customers. So, suppose we have a budget to keep customers, of USD 50. So that automatically, we have a budget to get a new customer of USD 250.\n\nUsing the previous machine learning model, we would need USD 2000 to acquire new customers. Whereas using the new machine learning model, we only need USD 1250. Or we can save 37.5%.\n\nMeanwhile, the cost of retaining customers (USD 50) will increase. From USD 2400 to USD 2750 or there is a 12% increase in costs.\n\n__But if we look at the whole story:__\n\n1. The total cost of using the first machine learning model was USD 4400.\n2. The total cost of using the second machine learning model is USD 4000.\n3. In other words, using optimized machine learning can save costs by 9%.","674bf6ee":"## To be continued","6e5d039c":"## Pipeline For RandomForestClassifier","681426d4":"**Note:** From the visualization above, customers who make transactions per month on average more than $ 60, will be more likely to churn than the average transaction below that.","93d72183":"**Problem Statement**\n\n> In a telco company, there are promotional costs known as Acquisition Cost and Retention Cost. Acquisition Cost is the cost for a company to acquire new customers. Meanwhile, Retention Cost is the cost for the company to retain existing customers.\n> \n> Due to human limitations, we are often wrong to predict which customers will churn and which customers will retain. So that the allocation of funds can be wrong so that the funds issued become larger.\n> \n> Moreover, according to some sources, the acquisition cost is 5x greater than the retantion cost. If we are wrong in predicting a customer who will actually churn, but it turns out that we predict as a customer who will retain, then we need to spend more than it should be.\n\n**What to do**\n\n> I will try to create a Machine Learning model to predict customer churn and retantion.\n\n**Goal**\n\n> Machine Learning has a goal so that cost allocation can be done as precisely as possible.\n\n**Value**\n\n> There is no wasted cost allocation.","e6b4a87a":"**Note:** From the visualization above, the odds between men and women to churn is almost the same in every level of monthly charges.","d9bdba13":"**Note:** From the visualization above, most female users whose subscription period is less than 10 months, will be more likely to churn than male. However, the longer the subscription period, male are more likely to churn than female.","7e31c460":"## Modelling"}}