{"cell_type":{"cacf9b83":"code","fb73dd2a":"code","a47d9611":"code","d9d43758":"code","4988f6b7":"code","a297ab60":"code","8d32802e":"code","e634db83":"code","48294880":"code","fd5d11e7":"code","bb2ecc28":"code","8863bb58":"code","4eab08a2":"code","6406918e":"code","2a862d08":"code","d14e9d97":"code","2615a6d5":"code","8af5ffc1":"code","acfa081b":"code","9a861341":"code","ce0532e2":"code","968147b2":"code","0a465b69":"code","2f07edca":"code","e6fa2362":"code","f18d4c99":"code","523b7c58":"code","83e13766":"code","20944d32":"code","b86690ad":"code","14d483f2":"code","6ab4790a":"markdown","8dc6ccf1":"markdown","64439cf1":"markdown","1bdce2ab":"markdown","3c52c83c":"markdown","4857b9ce":"markdown","3954dd20":"markdown","d6fcb395":"markdown","0d8ed464":"markdown","8991db5b":"markdown","35100d93":"markdown","829b6be5":"markdown","4c59cbd3":"markdown","29afd981":"markdown","9447f3d6":"markdown","db9a9cc8":"markdown","da848154":"markdown","1b55e055":"markdown","b302e5bb":"markdown"},"source":{"cacf9b83":"#importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n\nimport os\nprint(os.listdir(\"..\/input\"))","fb73dd2a":"data = pd.read_csv(\"..\/input\/passenger-list-for-the-estonia-ferry-disaster\/estonia-passenger-list.csv\")","a47d9611":"data.head()","d9d43758":"sns.set_style('whitegrid')\nplt.rcParams['font.size'] = 14\nplt.rcParams['figure.figsize'] = (9, 5)\nplt.rcParams['figure.facecolor'] = '#00000000'\n","4988f6b7":"f, axes = plt.subplots(1,1)\ng1 = sns.histplot(data[\"Age\"], color=\"red\",ax = axes,kde=True)\nplt.title(\"Distribution of age\");","a297ab60":"sns.violinplot(x=\"Survived\",y=\"Age\",data=data);","8d32802e":"# is the chance of survival different for different countries of origin?\ndata.groupby(\"Country\")[\"Survived\"].mean().plot(kind=\"bar\");","e634db83":"plotp=data.groupby(\"Survived\")[\"Survived\"].count()\nplotp.plot.pie(autopct=\"%.1f%%\");","48294880":"data = data[['Sex','Age','Category', 'Survived',\"Country\"]]\n#we remove the name collumns since they hold no value for the model","fd5d11e7":"from sklearn.preprocessing import LabelEncoder\nlabelencoder=LabelEncoder()\ndata.Category=labelencoder.fit_transform(data[\"Category\"])\ndata.Sex=labelencoder.fit_transform(data[\"Sex\"])\nprint(data)\n# Female=0 male=1, Crew=0, passenger=1","bb2ecc28":"#since the variable Country is not binary we need to make dummies\ndata = pd.get_dummies(data,drop_first=True)\ndata.head()","8863bb58":"data.isna().sum()\n#fortunatly there are no missing values","4eab08a2":"y = data['Survived']\nX = data.drop(columns=['Survived'])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=6)","6406918e":"# Setup the parameters and distributions to sample from: param_dist\nparam_dist = {\"min_samples_leaf\": range(1, 9),\n              \"criterion\": [\"gini\", \"entropy\"]}\n\n# Instantiate a Decision Tree classifier: tree\ntree = DecisionTreeClassifier()\n\ntree_cv = GridSearchCV(tree, param_dist, cv=5)\n\n# Fit it to the data\ntree_cv.fit(X_train, y_train)\n\n# Print the tuned parameters and score\nprint(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\nprint(\"Best score is {}\".format(tree_cv.best_score_))","2a862d08":"# Predict the labels of the test data: y_pred\ny_pred = tree_cv.predict(X_test)\n\n# Generate the confusion matrix \ncm0=confusion_matrix(y_test, y_pred)\nprint(classification_report(y_test, y_pred))","d14e9d97":"f, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm0, annot=True, linewidth=0.7, linecolor='cyan', fmt='g', ax=ax, cmap=\"BuPu\")\nplt.title('Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","2615a6d5":"#Import the SMOTE-NC\nfrom imblearn.over_sampling import SMOTENC\n#Create the oversampler. For SMOTE-NC we need to pinpoint the column position where is the categorical features are.\nsmotenc = SMOTENC([0,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17],random_state = 101)\n\nX_oversample, y_oversample = smotenc.fit_resample(X_train, y_train)","8af5ffc1":"# Re-Fit it to the oversampled data\ntree_cv.fit(X_oversample, y_oversample)\n\n\nprint(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\nprint(\"Best score is {}\".format(tree_cv.best_score_))","acfa081b":"model = tree_cv.best_estimator_\n","9a861341":"# Predict the labels of the test data: y_pred\ny_pred = model.predict(X_test)\n\n# Generate the confusion matrix \ncm=confusion_matrix(y_test, y_pred)\n\nprint(classification_report(y_test, y_pred))","ce0532e2":"f, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='g', ax=ax, cmap=\"BuPu\")\nplt.title('Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","968147b2":"from sklearn.metrics import roc_curve\n\n# Compute predicted probabilities: y_pred_prob\ny_pred_prob = model.predict_proba(X_test)[:,1]\n\n# Generate ROC curve values: fpr, tpr, thresholds\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()","0a465b69":"fig = plt.figure(figsize=(15,15))\nax1 = fig.add_subplot(2, 2, 1) \nax1.set_title('Decision tree no oversampling') \nax2 = fig.add_subplot(2, 2, 2) \nax2.set_title('Decision tree with oversampling')\n\n\nsns.heatmap(cm0, annot=True, linewidth=0.7, linecolor='red',cmap=\"BuPu\" ,fmt='g', ax=ax1)\nsns.heatmap(cm, annot=True, linewidth=0.7, linecolor='red',cmap=\"BuPu\" ,fmt='g', ax=ax2)  \nplt.show()","2f07edca":"model.feature_importances_","e6fa2362":"dataf=data.drop([\"Survived\"], axis=1)","f18d4c99":"def plot_feature_importance(importance,names,model_type):\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n\n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data)\n\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n\n    plt.figure(figsize=(10,8))\n\n    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n\n    plt.title(model_type + 'FEATURE IMPORTANCE')\n    plt.xlabel('FEATURE IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')\n\n\n    \nplot_feature_importance(model.feature_importances_,dataf.columns,'Decision Tree ')","523b7c58":"y = data['Survived']\nX = data[['Age',\"Sex\",\"Category\",\"Country_Sweden\",\"Country_Latvia\",\"Country_Russia\",\"Country_Estonia\"]]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=6)","83e13766":"smotenc = SMOTENC([0,2,3,4,5,6],random_state = 101)\n\nX_oversample, y_oversample = smotenc.fit_resample(X_train, y_train)","20944d32":"tree_cv.fit(X_oversample, y_oversample)\n\n\nprint(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\nprint(\"Best score is {}\".format(tree_cv.best_score_))","b86690ad":"# Predict the labels of the test data: y_pred\ny_pred = tree_cv.predict(X_test)\n\n# Generate the confusion matrix \ncm3=confusion_matrix(y_test, y_pred)\n\nprint(classification_report(y_test, y_pred))","14d483f2":"fig = plt.figure(figsize=(15,15))\nax1 = fig.add_subplot(3, 3, 1) \nax1.set_title('Decision tree no oversampling') \nax2 = fig.add_subplot(3, 3, 2) \nax2.set_title('Decision tree with oversampling')\nax3 = fig.add_subplot(3, 3, 3) \nax3.set_title('Decision tree final')\n\nsns.heatmap(cm0, annot=True, linewidth=0.7, linecolor='red',cmap=\"BuPu\" ,fmt='g', ax=ax1)\nsns.heatmap(cm, annot=True, linewidth=0.7, linecolor='red',cmap=\"BuPu\" ,fmt='g', ax=ax2)  \nsns.heatmap(cm3, annot=True, linewidth=0.7, linecolor='red',cmap=\"BuPu\" ,fmt='g', ax=ax3)  \nplt.show()","6ab4790a":"# Oversampling and re-fit","8dc6ccf1":"# Intro","64439cf1":"**Introduction On September 27 1994 the ferry Estonia set sail on a night voyage across the Baltic Sea from the port of Tallin in Estonia to Stockholm. She departed at 19.00 carrying 989 passengers and crew, as well as vehicles, and was due to dock at 09.30 the following morning, Tragically, the Estonia never arrived.**\n\n","1bdce2ab":"# train_test_split","3c52c83c":"# Model","4857b9ce":"# Visualize the data","3954dd20":"As we can see the oversampling worked. Our model is now better at predicting the class Survived=1. unfortunately recall for the first class, slightly dropped.","d6fcb395":"Compairing the models with and without oversampling\n\n","0d8ed464":"# Final re-fit\n","8991db5b":"We will remove the unnecessary features and re fit the model\n\n","35100d93":"As we can see from the plot, the median age for those who survived is lower, and there also seems to be smaller variation in these ages.","829b6be5":"The pie plot not only shows the magnitute of the disaster, it also hints us that the data are not balanced and it may cause problems to our model.","4c59cbd3":"As we can see, for most countries, the origin of the passenger plays no role in the prediction.\n\n","29afd981":"# Final comparison of the 3 models","9447f3d6":"# Metrics","db9a9cc8":"# Pre Processing","da848154":"We chose a Decision Tree Classifier for our model\n\n","1b55e055":"* In the end, the model that we will choose depends on its future usage and the cost of the false positives for each class.\n* **All in all i would say that the last model is the better of the 3 since it has better average recall and f1-score.**","b302e5bb":"**As we saw in the vizualization stage our data suffer from inbalance. As such our model can not work properly because even though we have a high score, recall for the class Survived=1 is 0,07. In order to solve this, we use oversampling.**"}}