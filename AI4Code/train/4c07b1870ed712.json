{"cell_type":{"ced19539":"code","9aaf46e7":"code","3e128550":"code","c9e4c6ac":"code","9602b7f4":"code","ce1fc495":"code","24f46509":"code","8cd94eef":"code","23ea979c":"code","da67129a":"code","55bc060d":"code","2ca43a46":"code","40b05481":"code","48120bf0":"code","79415001":"code","ac0023cc":"code","ad0bb73c":"code","5a184edc":"code","ff36f670":"code","e96069da":"code","ae785cfa":"code","53a800db":"code","e527abda":"code","251f3ba7":"code","9e5826e5":"code","4ca3e26c":"code","b5740e29":"code","be2f7e2f":"code","2eac094c":"code","022614d3":"code","3cac66e7":"code","d3962d11":"code","f91c1358":"code","ff7f8b8f":"code","8698b71f":"code","cb0f0e4f":"code","ab63880a":"code","933b14f2":"code","2acfa87b":"code","e685d9a8":"code","47d89e8e":"code","aabf95e8":"code","6e35292e":"code","b407bd88":"code","ccc98d3c":"code","37f6e1af":"code","a89a67ae":"code","75e398f1":"code","f8e5057f":"code","f5c9dba3":"code","37f41ea6":"code","acc78e9d":"code","7a191731":"code","e804d454":"code","72c1c54f":"code","73c9871a":"code","ba57a17e":"code","37153c15":"code","e1b339a8":"code","e29b39e6":"code","e96750dd":"code","de13fc86":"code","c4d1747b":"code","efe614d0":"code","bf87dfd6":"code","3539e5a9":"code","bfbfa16c":"markdown","a7373cab":"markdown","73fb5c81":"markdown","0a2b7ac4":"markdown","a1d874c7":"markdown","f6a4c9e5":"markdown","ced54dac":"markdown","f3174a3c":"markdown","df3e432e":"markdown","846ef311":"markdown","c9e2e7c7":"markdown","c7a0a9fc":"markdown","16c3e435":"markdown","880a3cbd":"markdown","e2d18b40":"markdown","fb86c561":"markdown","80c956d2":"markdown","77c72bd1":"markdown","9702a08a":"markdown","1919fe04":"markdown","45e0d89f":"markdown","5e31b8d1":"markdown","2498a95d":"markdown","2a3518fb":"markdown","2003d6b4":"markdown","bca00b28":"markdown","7204e20e":"markdown","8b21d352":"markdown","b0d48ea3":"markdown","0445cb37":"markdown","abcb419f":"markdown","0baf3e7b":"markdown","43355cb2":"markdown","e644b18c":"markdown","108ef8e6":"markdown","48670e12":"markdown","c78cffb7":"markdown","694bf07e":"markdown","88c833ac":"markdown","60fd83b4":"markdown","ea1f5d24":"markdown","63ea4bc6":"markdown"},"source":{"ced19539":"def getfeat(df):\n    \"\"\"\n    Returns lists of numeric & categorical features\n    \"\"\"\n    numfeat, catfeat = list(df.select_dtypes(include=np.number)), list(df.select_dtypes(exclude=np.number))\n    return numfeat, catfeat","9aaf46e7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.rcParams[\"figure.figsize\"] = (8,6) ## preferred size of plots\nplt.style.use('fivethirtyeight')\n\nimport seaborn as sns\nsns.set(style=\"darkgrid\") ## beautiful plots\n\nfrom scipy import stats\n\nimport warnings\nwarnings.filterwarnings('ignore')\n","3e128550":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nsample = pd.read_csv('..\/input\/sample_submission.csv')","c9e4c6ac":"train.info()","9602b7f4":"## Dropping the redundant column of 'Loan_ID'\ntrain.drop('Loan_ID', axis=1, inplace=True)\ntest.drop('Loan_ID', axis=1, inplace=True)","ce1fc495":"## Seperating features by type and target column\ntarget = 'Interest_Rate'\nnumfeat, catfeat = getfeat(train)\nnumfeat.remove(target)\n\ntemp = 0    ## a temporary variable for random usage","24f46509":"train.shape, test.shape","8cd94eef":"## Mapping target variable to categories as integer type won't give best results. \nmatch = {1:'Cat_1', 2:'Cat_2', 3:'Cat_3'}  \nunmatch = {'Cat_1': 1, 'Cat_2': 2, 'Cat_3':3} ## to be used during submission to unmap\ntrain[target] = train[target].map(match)","23ea979c":"train[target].value_counts().plot(kind='bar', figsize=(8,6))\nplt.show()  ","da67129a":"# drop_indices = np.random.choice(train[train[target]=='Cat_2'].index, 30000, replace=False)\n# train = train.drop(drop_indices).reset_index()\n# drop_indices = np.random.choice(train[train[target]=='Cat_3'].index, 25000, replace=False)\n# train = train.drop(drop_indices).reset_index()\n# train[target].value_counts()\n# # train.head()","55bc060d":"train_labels = train[target]\ndf = pd.concat([train, test], keys = ['train', 'test'])\ndf","2ca43a46":"# for temp in range(len(df['Loan_Amount_Requested'])):\n#     df['Loan_Amount_Requested'][temp] = int(df['Loan_Amount_Requested'][temp].replace(',', ''))\n# df['Loan_Amount_Requested'] = df['Loan_Amount_Requested'].astype('int64')\n\n## Fixing 'Loan_Amount_Requested' \ndf['Loan_Amount_Requested'] = df['Loan_Amount_Requested'].str.replace(\",\",\"\").astype('int64')","40b05481":"## Creating new features before moving towards transformation\ndf['Loan_Amount_Requested_range'] = pd.cut(df['Loan_Amount_Requested'], \n                                             bins= range(0, df['Loan_Amount_Requested'].max(), 2000)).astype('object')\ndf['Loan_Amount_Requested_category'] = pd.cut(df['Loan_Amount_Requested'], \n                                                 bins=[int(x) for x in np.linspace(0,df['Loan_Amount_Requested'].max(), 8)], \n                                                 labels=['Very Low', 'Low', 'Medium-Low', 'Medium', 'Medium-High', 'High', 'Very High']).astype('object')","48120bf0":"sns.countplot(x = 'Loan_Amount_Requested_category', data = df.xs('train'), hue = 'Interest_Rate')\nplt.show()","79415001":"sns.distplot(df['Loan_Amount_Requested'], fit = stats.norm)\nplt.legend(['Normal fit', 'Skew: {:.4f}'.format(df['Loan_Amount_Requested'].skew())])\nplt.show()","ac0023cc":"temp = 'Length_Employed'\ndf[temp].isnull().sum()\/len(df[temp])*100","ad0bb73c":"df[temp].unique()","5a184edc":"sns.countplot(x=temp, data = df)\nplt.xticks(rotation=45)\nplt.show()","ff36f670":"## My guesses for null values --> either unemployed i.e. a housewife or student in that case '0 year' \n##                            --> fill with mode \n\ndf[temp].fillna('0 year', inplace=True)","e96069da":"temp = 'Home_Owner'\ndf[temp].isnull().sum()\/len(df)*100","ae785cfa":"df[temp].value_counts()","53a800db":"sns.countplot(temp, data=df, hue='Length_Employed')\nplt.show()","e527abda":"## My guesses for null values --> people let go of filling just because none of it applies\n##                            --> fill with mode \n\ndf[temp].fillna('None', inplace = True)","251f3ba7":"temp = 'Annual_Income'\ndf[temp].isnull().sum()\/len(df)*100","9e5826e5":"df[temp].describe()","4ca3e26c":"## My guesses for null values --> fill with median as data has an extreme outlier\n##                            -->  \ndf[temp].fillna(df[temp].median(), inplace=True)","b5740e29":"df['Annual_Income_range'] = pd.cut(df[temp], bins=[int(x) for x in np.linspace(0, df[temp].max(), 15)]).astype('object')\ndf['Annual_Income_cat'] = pd.cut(df[temp], bins=[int(x) for x in np.linspace(0, df[temp].max(), 8)],\n                                 labels = ['Very Low', 'Low', 'Medium-Low', 'Medium', 'Medium-High', 'High', 'Very High']).astype('object')","be2f7e2f":"f,a = plt.subplots(1,2,figsize=(16,6))\n\nsns.distplot(df[temp], fit=stats.norm, ax=a[0])\na[0].set_title(\"Distribution with Skew: {:.4f}\".format(df[temp].skew()))\n\ntempdf = pd.Series(stats.boxcox(1+df[temp], lmbda=0))\n\nsns.distplot(tempdf, fit=stats.norm, ax=a[1])\na[1].set_title(\"Transformed distribution with Skew: {:.4f}\".format(tempdf.skew()))\n\ndf[temp] = stats.boxcox(1+df[temp], lmbda=0)\nplt.show()","2eac094c":"temp = 'Income_Verified'\ndf[temp].isnull().sum()","022614d3":"df[temp].unique()","3cac66e7":"temp = 'Purpose_Of_Loan'\ndf[temp].isnull().sum()","d3962d11":"df[temp].unique()","f91c1358":"sns.countplot(df[temp])\nplt.xticks(rotation=90)\nplt.show()","ff7f8b8f":"temp = 'Debt_To_Income'\ndf[temp].isnull().sum()","8698b71f":"df[temp].describe(percentiles=[0.20,0.40,0.60,0.9])","cb0f0e4f":"df['Debt'] = (df[temp]*df['Annual_Income']).astype('float')","ab63880a":"f,a = plt.subplots(1,2,figsize=(16,6))\n\nsns.distplot(df[temp], fit=stats.norm, ax=a[0])\na[0].set_title(\"Distribution with Skew: {:.4f}\".format(df[temp].skew()))\n\ntempdf = pd.Series(stats.boxcox(1+df[temp], lmbda=0.817))  ## I prefer setting lambda values manually although 'boxcox_normmax' gives good results\n\nsns.distplot(tempdf, fit=stats.norm, ax=a[1])\na[1].set_title(\"Transformed distribution with Skew: {:.4f}\".format(tempdf.skew()))\n\ndf[temp] = stats.boxcox(1+df[temp], lmbda=0.817) ##    ONE TIME RUN\nplt.show()","933b14f2":"temp = 'Inquiries_Last_6Mo'\ndf[temp].isnull().sum()","2acfa87b":"sorted(df[temp].unique())","e685d9a8":"df['Requirement'] = pd.cut(df[temp], bins=[0,1,5,8], labels = ['least', 'wanted', 'highly wanted']).astype('object')","47d89e8e":"temp = 'Months_Since_Deliquency'\ndf[temp].isnull().sum()\/len(df)*100","aabf95e8":"df.drop(temp, axis=1,inplace=True)","6e35292e":"temp = 'Number_Open_Accounts'\nprint(df[temp].isnull().sum(), sorted(df[temp].unique()))","b407bd88":"df['Number_Open_Accounts_cat'] = pd.cut(df[temp], bins = [0,9,20,45,85], labels=['few', 'ok_ok', 'many', 'far_too_many']).astype('object')","ccc98d3c":"f,a = plt.subplots(1,2,figsize=(16,6))\n\nsns.distplot(df[temp], fit=stats.norm, ax=a[0])\na[0].set_title(\"Distribution with Skew: {:.4f}\".format(df[temp].skew()))\n\ntempdf = pd.Series(stats.boxcox(1+df[temp], lmbda=0.15))  ## I prefer setting lambda values manually although 'boxcox_normmax' gives good results\n\nsns.distplot(tempdf, fit=stats.norm, ax=a[1])\na[1].set_title(\"Transformed distribution with Skew: {:.4f}\".format(tempdf.skew()))\n\ndf[temp] = stats.boxcox(1+df[temp], lmbda=0.15) ##    ONE TIME RUN\nplt.show()","37f6e1af":"temp = 'Total_Accounts'\ndf[temp].isnull().sum()","a89a67ae":"df[temp].describe()","75e398f1":"df['Total_Accounts_cat'] = pd.cut(df[temp], bins = [0,9,20,45,85,156], labels=['few', 'ok_ok', 'many', 'very_many','far_too_many']).astype('object')","f8e5057f":"f,a = plt.subplots(1,2,figsize=(16,6))\n\nsns.distplot(df[temp], fit=stats.norm, ax=a[0])\na[0].set_title(\"Distribution with Skew: {:.4f}\".format(df[temp].skew()))  ## I prefer setting lambda values manually although 'boxcox_normmax' gives good results\n\ntempdf = pd.Series(stats.boxcox(1+df[temp], lmbda=0.348))\n\nsns.distplot(tempdf, fit=stats.norm, ax=a[1])\na[1].set_title(\"Transformed distribution with Skew: {:.4f}\".format(tempdf.skew()))\n\ndf[temp] = stats.boxcox(1+df[temp], lmbda=0.348) ##    ONE TIME RUN\nplt.show()","f5c9dba3":"temp = 'Gender'\ndf[temp].isnull().sum()","37f41ea6":"import lightgbm as lgb\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.metrics import f1_score, confusion_matrix, accuracy_score, plot_confusion_matrix\nfrom sklearn.model_selection import RandomizedSearchCV","acc78e9d":"## Rebuilding test and training set\n\nnumfeat, catfeat = getfeat(df)\ncatfeat.remove(target) \n\nX = df.xs('train').drop(target, axis=1)\ny = df.xs('train')[target]\n\nX_test = df.xs('test').drop(target, axis=1)","7a191731":"X.shape, y.shape, X_test.shape","e804d454":"## Encoding categorical variables\n\nX = pd.get_dummies(X, columns=catfeat)\nX_test = pd.get_dummies(X_test, columns=catfeat)\n\nfrom sklearn import preprocessing as prep\n\n## Scaling for better results on NN \n\nminmaxscalar = prep.MinMaxScaler()  \nX = pd.DataFrame(minmaxscalar.fit_transform(X), columns = X.columns)\nX_test = pd.DataFrame(minmaxscalar.fit_transform(X_test), columns = X_test.columns)\n\nX.shape, y.shape","72c1c54f":"## Before submitting dataframe to lgbm, replacing all non-alphanumeric characters in the column names\n## The problem is that few of the features contain a comma (,) which lgbm doesn't accept anymore.\n\nX.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X.columns]\nX_test.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_test.columns]\n\nX_train, x_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=100)","73c9871a":"learning_rate = [float(x) for x in np.linspace(0.005, 0.05, 1000)]\nn_estimators = [int(x) for x in range(500,5000,500)]\nmax_depth = [int(x) for x in range(2,30,2)]\nnum_leaves = [int(x) for x in range(1,5000,100)]\nbagging_fraction = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n\nearly_stopping_rounds = 50\nmin_data_in_leaf = 100\nlambda_l1 = 0.5\n\nlgbgrid = {'num_leaves': num_leaves,\n           'bagging_fraction': bagging_fraction,\n           'early_stopping_rounds': early_stopping_rounds,\n           'min_data_in_leaf': min_data_in_leaf,\n           'lambda_l1': lambda_l1,\n           'max_depth': max_depth,\n           'n_estimators': n_estimators,\n           'learning_rate': learning_rate\n           }","ba57a17e":"# lgbm = lgb.LGBMClassifier(objective='multiclass', random_state=5, bagging_freq=1)\n# clf = RandomizedSearchCV(lgbm, lgbgrid, cv=5, n_iter=300, n_jobs=1)\n# search = clf.fit(X_train,y_train)\n# search.best_params_","37153c15":"scores = {'last':0,'current':0}","e1b339a8":"lgbm = lgb.LGBMClassifier(boost = 'dart', objective='multiclass', num_class = 3,\n                          learning_rate = 0.075, max_bin = 470,\n                          max_depth = 38, min_data_in_leaf = 60, num_leaves = 3501,\n                          lambda_l1 = 0.5                          \n                          )\n\nlgbm.fit(X_train, y_train,eval_set = (x_test,y_test), early_stopping_rounds=100, verbose=200)\n\ny_pred = lgbm.predict(x_test)\n\nscores['current'] = f1_score(y_test, y_pred, average='weighted')\nprint('previous score = ', scores['last'],\n      '\\ncurrent score = ', scores['current'])\nscores['last'] = scores['current']","e29b39e6":"plot_confusion_matrix(lgbm, x_test, y_test, normalize='pred')\nplt.show()","e96750dd":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier","de13fc86":"from keras.utils import np_utils\nfrom sklearn.preprocessing import LabelEncoder\n\n## encoding y variables for multiclass NN prediction\nencoded_y = LabelEncoder().fit_transform(y_train)\ndummy_y = np_utils.to_categorical(encoded_y)\n\nencoded_y_test = LabelEncoder().fit_transform(y_test)\ndummy_y_test = np_utils.to_categorical(encoded_y_test)","c4d1747b":"## Creating metrics to be used in epochs as the evaluation metric for the competition is F1-score\n\nfrom keras import backend as K\n\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))\n","efe614d0":"model = Sequential()\nmodel.add(Dense(12, input_dim=92 ,activation='relu'))\nmodel.add(Dense(24, activation='relu'))\nmodel.add(Dense(3, activation='softmax'))\n## using softmax as final layer activation layer as it is a multiclass prediction. Sigmoid would've been good had it been a binary classification.\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[f1_m])\n\nmodel.fit(X_train,dummy_y,epochs=100, validation_split=0.1,batch_size=128, verbose=100)\n","bf87dfd6":"f1_sc = model.evaluate(x_test, dummy_y_test, verbose=0, batch_size=32)\nf1_sc[1]","3539e5a9":"# sub = pd.Series(lgbm.predict(X_test))\n# sample.iloc[:,1] = sub.map(unmatch)\n# sample.to_csv('..\/input\/submission.csv', index=False)","bfbfa16c":"# SUBMISSION","a7373cab":"#### Model training","73fb5c81":"Above I had to transform the entire data for better results. Standardization works better on data which is already normalized.\n\nI have normalized the data before while fixing the skewness values. [This](https:\/\/sebastianraschka.com\/Articles\/2014_about_feature_scaling.html) article here explains **'Feature Scaling and Normalization \u2013 and the effect of standardization for machine learning algorithms'** which helped me learn about the better fit here.","0a2b7ac4":"Creating a basic NN as the data set is huge. A very simple tutorial available at the Tensorflow Website [here](https:\/\/www.tensorflow.org\/tutorials\/keras\/classification) worked for starters.","a1d874c7":"#### Parameter tuning\n","f6a4c9e5":"\nThe data looks imbalanced as class '2' has around 70000 cases and class '1' has around 33000 cases. \nOne might be motivated to balance out the data to remove bias from the model for starters. But [this](https:\/\/matloff.wordpress.com\/2015\/09\/29\/unbalanced-data-is-a-problem-no-balanced-data-is-worse\/) article explains quite well how 'the problem of (artificially) balanced data is worse than the unbalanced case'. \n\nHence, moving on with the data.\n\n---\n---","ced54dac":"## LightGB ","f3174a3c":"# Beginners' Mistakes 01 - A comprehensive parameter tuned LGBM model.","df3e432e":"#### 4: Annual_Income","846ef311":"Fixed the litte bit of skewness.","c9e2e7c7":"Fixed a bit of skewness.","c7a0a9fc":"# MODEL FITTING","16c3e435":"Category 2 seems to be most famous loan for a lot of loan amounts (except the highest one). Category 3 shows an interesting and unidentifiable trend. Category 1 is the least favoured for all types.\n\n","880a3cbd":"#### 5: Income_Verified","e2d18b40":"Clearly LGB is the winner here. Made a very basic NN to start off with. ","fb86c561":"#### 10: Number_Open_Accounts","80c956d2":"### BEGINNER MISTAKES\n\nI tried dropping a few columns to balance the data and check the results. Turned out to be the same. \n\n![](http:\/\/)![](http:\/\/)Later on a bit of research taught me about sampling but it was too late. ","77c72bd1":"#### 8: Inquiries_Last_6Mo","9702a08a":"This was my first approach to lgbm tuning. A few articles listed here helped me a lot in the process. \n\n [Lightgbm parameters guide](https:\/\/neptune.ai\/blog\/lightgbm-parameters-guide)\n    \n [xgboost\/LightGBM parameters](https:\/\/sites.google.com\/view\/lauraepp\/parameters) \n \n \nThey cover the parameters and their effects comprehensively and proved to be very useful.","1919fe04":"Hello everyone. Below is my approach to 'Machine Learning for Banking' posted as a competition at [Analytics Vidhya](https:\/\/datahack.analyticsvidhya.com\/contest\/all\/) which got me a rank 241 in the leaderboard. \n\nI am sharing this notebook to fellow beginners who start off with a problem and face issues such as what to do next, am I going right. I have provided links which were useful to me while completing my code, listed my thought process while applying different techniques and keeping my mistakes right here. I understood that this UNFILTERED approach is way more helpful to any beginner and me alike.\nI hope any experienced person would have some great tips\/suggestions for me about my approach and I request you to please comment below about the same. It would be really helpful. \n\n\n\nAll said, if you like my approach please upvote - it helps stay motivated and would make me continue this approach as a series. Do ping if you're interested in connecting with a like minded beginner who shares the same passion as you. \n\nThank you :)\n\n","45e0d89f":"# DATA PREPROCESSING","5e31b8d1":"## Neural Network (a novice try)","2498a95d":"### Utility functions\n\n\n---\n\n","2a3518fb":"### BEGINNER MISTAKES\n\nFirst I was using the loop to fix the string elements of 'Loan_Amount_Requested'. But found efficient way of doing it from a friend.","2003d6b4":"#### 7: Debt_To_Income","bca00b28":"Around 4.5% data is null for Length_Employed. First job is to fill the null values. Looking for a method to fill the values.","7204e20e":"#### 1: Loan_Amount_Requested","8b21d352":"Fixed the skewness.","b0d48ea3":"#### 11: Total_Accounts","0445cb37":"# IMPORTS","abcb419f":"#### 12: Gender","0baf3e7b":"## Problem Statement:\n\nThe process, defined as \u2018risk-based pricing\u2019, uses a sophisticated algorithm that leverages different determining factors of a loan applicant. Selection of significant factors will help develop a prediction algorithm which can estimate loan interest rates based on clients\u2019 information. On one hand, knowing the factors will help consumers and borrowers to increase their credit worthiness and place themselves in a better position to negotiate for getting a lower interest rate. On the other hand, this will help lending companies to get an immediate fixed interest rate estimation based on clients information. Here, your goal is to use a training dataset to predict the loan rate category (1 \/ 2 \/ 3) that will be assigned to each loan in our test set.","43355cb2":"\n\n---\n","e644b18c":"#### 6: Purpose_Of_Loan","108ef8e6":"## Data Prepping","48670e12":"#### 2: Length_Employed","c78cffb7":"\n\n---\n\n\n","694bf07e":"#### 3: Home_Owner","88c833ac":"#### 9: Months_Since_Deliquency","60fd83b4":"#### Target - Interest_Rate","ea1f5d24":"A highly skewed feature.\n\n---","63ea4bc6":"The plotted normalized confusion matrix on the prediction set helps understand the effect of the earlier thought of imbalance in the data with respect to target variables. \n\nEven after randomly removing a few rows the results did not show much change. It just satisfies the hypothesis - **Introducing artificial balance in data does not lead to better model accuracy.**"}}