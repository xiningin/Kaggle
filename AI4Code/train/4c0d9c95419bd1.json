{"cell_type":{"e9572c45":"code","05aacb43":"code","82eab8be":"code","a1cd1d63":"code","ac715d93":"code","6514aa21":"code","4d143371":"code","b25aa7f2":"code","17a849ca":"code","79c62764":"code","8d94ebab":"code","6f5f5ae4":"code","40395638":"code","9eca0e11":"code","aa523fcd":"code","fda6526a":"code","fcbe73b8":"code","3031b003":"code","afa0e4e8":"code","e5d1163e":"code","cfac97cb":"code","5a9fba8c":"code","0a37fe1b":"code","badee36a":"code","0c123671":"markdown","5e86e70c":"markdown","ac726b0b":"markdown","0b87fa1c":"markdown","f5445201":"markdown","5e9df25d":"markdown","93a4938b":"markdown"},"source":{"e9572c45":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\nimport tqdm\nfrom tqdm.auto import tqdm as tqdmp\ntqdmp.pandas()\n\n# NLP\nimport unicodedata, string, re, nltk, json\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom wordcloud import WordCloud, STOPWORDS\nfrom collections import Counter, defaultdict\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.dates as mdates\n\n# ignoring warnings\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n# Some settings for visualizations\nsns.set(rc={'axes.facecolor':'black', 'figure.facecolor':'black', \n            'xtick.color': 'white', 'ytick.color': 'white', \n            'grid.color': 'white', 'axes.labelcolor': 'white',\n            'figure.dpi': 150, 'grid.linestyle': ':', 'grid.alpha': .6,\n            'font.family': 'fantasy'})","05aacb43":"train = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/train.csv')\nss = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv')\ntrain.head()","82eab8be":"ss.head()","a1cd1d63":"train.info()","ac715d93":"train.describe()","6514aa21":"# Note that ALL ground truth texts have been cleaned for matching purposes using the following code:\ndef text_cleaner(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n\ndef show_wordcloud(data, stop, mask = None, title = None, color = 'black'):\n    \"\"\"\n    Function for creating wordclouds (with or without mask)\n    \"\"\"\n    from wordcloud import WordCloud, ImageColorGenerator\n    wordcloud = WordCloud(background_color = color,\n                         stopwords = stop,\n                         mask = mask,\n                         max_words = 100,\n                         scale = 3,\n                         width = 4000, \n                         height = 2000,\n                         collocations = False,\n                         colormap = 'rainbow',\n                         random_state = 1)\n    \n    wordcloud = wordcloud.generate(data)\n    \n    plt.figure(1, figsize = (16, 8), dpi = 300)\n    plt.title(title, size = 15)\n    plt.axis('off')\n    if mask is None:\n        plt.imshow(wordcloud, interpolation = \"bilinear\")\n        plt.show()\n    else:\n        image_colors = ImageColorGenerator(mask)\n        plt.imshow(wordcloud.recolor(color_func = image_colors), \n                   interpolation = \"bilinear\")\n        plt.show()","4d143371":"title_cleaned = train['pub_title'].apply(lambda x: text_cleaner(x))","b25aa7f2":"title_length = title_cleaned.str.len()\n\nplt.title('Title length', size = 15, color = 'white')\nsns.distplot(title_length, kde = False, color = 'blue', \n             hist_kws = dict(alpha = 1))\nplt.xlabel('Length of title (symbols)')\nplt.show()","17a849ca":"title_words = title_cleaned.str.split().map(lambda x: len(x))\n\nplt.title('Title words', size = 15, color = 'white')\nsns.distplot(title_words, kde = False, color = 'blue', \n             hist_kws = dict(alpha = 1))\nplt.xlabel('Length of title (words)')\nplt.show()","79c62764":"title_word_len = title_cleaned.str.split().apply(lambda x: [len(i) for i in x]).map(lambda x: np.mean(x))\n\nplt.title('Title words length', size = 15, color = 'white')\nsns.distplot(title_word_len, kde = False, color = 'blue', \n             hist_kws = dict(alpha = 1))\nplt.xlabel('Mean word length in title (symbols)')\nplt.show()","8d94ebab":"words = title_cleaned.str.split().values.tolist()\ntitle_corpus = [word for i in words for word in i]\n\ntitle_counter = Counter(title_corpus)\ntitle_most = title_counter.most_common()\n\nstop = set(stopwords.words('english'))\n\ntitle_top_words, title_top_words_count = [], []\nfor word, count in title_most[:100]:\n    if word not in stop:\n        title_top_words.append(word)\n        title_top_words_count.append(count)","6f5f5ae4":"plt.title('TOP-10 title words', color = 'white', size = 15)\nsns.barplot(y = title_top_words[:10], x = title_top_words_count[:10], \n            edgecolor = 'black', color = 'blue')\nplt.show()","40395638":"title_word_string = ' '.join(title_corpus)\nshow_wordcloud(title_word_string, stop)","9eca0e11":"def text_extractor(url_id):\n    url = '..\/input\/coleridgeinitiative-show-us-the-data\/train\/{}.json'.format(url_id)\n    return ' '.join(pd.read_json(url).text)","aa523fcd":"train_texts = train.drop_duplicates(subset = ['Id'])['Id'].progress_apply(lambda x: text_extractor(x))","fda6526a":"train_texts","fcbe73b8":"text_cleaned = train_texts.progress_apply(lambda x: text_cleaner(x))","3031b003":"text_cleaned","afa0e4e8":"text_length = text_cleaned.str.len()\n\nplt.title('Pub text length', size = 15, color = 'white')\nsns.distplot(text_length, kde = False, color = 'blue', \n             hist_kws = dict(alpha = 1))\nplt.xlabel('Length of pub text (symbols)')\nplt.show()","e5d1163e":"text_words = text_cleaned.str.split().map(lambda x: len(x))\n\nplt.title('Pub text words', size = 15, color = 'white')\nsns.distplot(text_words, kde = False, color = 'blue', \n             hist_kws = dict(alpha = 1))\nplt.xlabel('Length of pub text (words)')\nplt.show()","cfac97cb":"text_word_len = text_cleaned.str.split().progress_apply(lambda x: [len(i) for i in x]).map(lambda x: np.mean(x))\n\nplt.title('Pub text words length', size = 15, color = 'white')\nsns.distplot(text_word_len, kde = False, color = 'blue', \n             hist_kws = dict(alpha = 1))\nplt.xlabel('Mean word length in pub text (symbols)')\nplt.show()","5a9fba8c":"words = text_cleaned.str.split().values.tolist()\ntext_corpus = [word for i in words for word in i]\n\ntext_counter = Counter(title_corpus)\ntext_most = title_counter.most_common()\n\nstop = set(stopwords.words('english'))\n\ntext_top_words, text_top_words_count = [], []\nfor word, count in text_most[:100]:\n    if word not in stop:\n        text_top_words.append(word)\n        text_top_words_count.append(count)","0a37fe1b":"plt.title('TOP-20 text words', color = 'white', size = 15)\nsns.barplot(y = text_top_words[:20], x = text_top_words_count[:20], \n            edgecolor = 'black', color = 'blue')\nplt.show()","badee36a":"def jaccard_similarity(s1, s2):\n    l1 = s1.split(\" \")\n    l2 = s2.split(\" \")    \n    intersection = len(list(set(l1).intersection(l2)))\n    union = (len(l1) + len(l2)) - intersection\n    return float(intersection) \/ union","0c123671":"<h1 style='color:white; background:blue; border:0'><center>Show US the Data: Start Our Study<\/center><\/h1>\n\n![](https:\/\/oerc.osu.edu\/sites\/oerc\/themes\/oerc\/images\/projects\/coleridge.png)\n\nThis competition challenges data scientists to show how publicly funded data and evidence are used to serve science and society. Data, evidence, and science are critical if government is address the many threats facing society: pandemics, climate change and coastal inundation, Alzheimer\u2019s disease, child hunger, and support science and innovation, increase food production, maintain biodiversity, and address many other challenges. Yet much of the information about data necessary to inform evidence and science is locked inside publications.\n\nCan natural language processing find the hidden-in-plain-sight data citations? Can machine learning find the link between the words used in research articles and the data referenced in the article?","5e86e70c":"## Extract all publication texts\n\nPublications are provided in JSON format, broken up into sections with section titles. Let's extract all the texts with a little function.","ac726b0b":"# EDA","0b87fa1c":"Note that there are multiple rows for some training documents, indicating multiple mentioned datasets","f5445201":"<h1 style='color:white; background:blue; border:0'><center>WORK IN PROGRESS...<\/center><\/h1>","5e9df25d":"The goal in this competition is not just to match known dataset strings but to generalize to datasets that have never been seen before using NLP and statistical techniques. Not all datasets have been identified in train, but you have been provided enough information to generalize.\n\nThe objective of the competition is to identify the mention of datasets within scientific publications. Your predictions will be short excerpts from the publications that appear to note a dataset.\n\nSubmissions are evaluated on a Jaccard-based FBeta score between predicted texts and ground truth texts, with Beta = 0 (an F0 or precision score). Multiple predictions are delineated with a pipe (|) character in the submission file.\n\nThe following is Python reference code for the Jaccard score:","93a4938b":"Let's at first analyze the train pub_title data."}}