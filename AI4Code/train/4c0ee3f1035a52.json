{"cell_type":{"a432db94":"code","519841dc":"code","c22ef7f5":"code","e6e4a068":"code","589cafc9":"code","82417227":"code","76d744c3":"code","737522f9":"code","e52be3fc":"code","d4e62cab":"code","3d08c96f":"code","a8f455da":"code","a39ea360":"code","dd62fb9b":"code","83634821":"code","64b17eb7":"code","0017bd35":"code","54d2f674":"code","c825ee93":"code","326da767":"code","260ed22b":"code","7601807e":"code","c0b7d320":"code","c49afeec":"code","3b783b8f":"code","9683d420":"code","ce5df820":"code","ed7237f3":"code","c1bdc92e":"code","f79d778f":"code","495eda25":"code","84a717db":"code","126b0517":"code","fa198d87":"code","2102e04a":"code","f1453e33":"code","7bbb9a67":"code","4c1c7baa":"code","fd46ef4b":"code","b1165096":"code","5460af8a":"code","48a5fcc7":"code","7ba92439":"code","8184e057":"code","fcd6e26c":"code","f5a0e576":"code","7d987420":"code","a53e8e27":"code","19c5f8be":"code","4284609b":"code","4029ec27":"code","ca1333e4":"code","b9f8a497":"code","578e8170":"code","60dcb407":"code","09aed1b5":"code","27651de3":"code","b63bba00":"code","dadec9e9":"code","80b6b001":"code","c73607d0":"code","e08db580":"code","e2dc9fd2":"code","f4b0701f":"code","59fab479":"code","48b3125c":"code","d57a9cb4":"code","0cbd9cdc":"code","329a5b47":"code","4a50c6d0":"code","11c3767a":"code","edaed138":"code","c72aeb34":"code","48231837":"code","ac77aa5f":"code","242e5980":"code","97f5ef84":"code","754e906d":"code","ca566af9":"code","e92494fa":"code","401e552c":"code","8717ac4d":"code","257ac08b":"code","2e5f7736":"code","81504bbf":"code","ef19e404":"code","14e9362d":"code","85eacc48":"code","335796ae":"code","b5e55980":"code","8e094eaa":"code","6502d0f7":"code","ab9fecc9":"code","494c2f8c":"code","bb742c33":"code","6e5afaa1":"code","51e8c80e":"code","04f24049":"code","4b6d7e99":"code","2de9c242":"code","7f03554d":"code","85b72f4d":"code","90604aba":"code","72874269":"code","afa218db":"code","0bd88830":"code","2a8d356d":"code","a21671a2":"code","e2339cb7":"code","585f7d14":"code","f564c7ed":"code","77ab8402":"code","91a1419a":"code","667a3a62":"code","75d2f0ef":"code","e34fa814":"code","27ee5722":"code","64b0ea73":"code","5dd90503":"markdown","71cd8361":"markdown","f446db3a":"markdown","8c7b75be":"markdown","760cdd85":"markdown","3e25d168":"markdown","5f66a83e":"markdown","97030f0e":"markdown","21ce4f40":"markdown","4caf0e4c":"markdown","2da9f5db":"markdown","6d059489":"markdown","cd2afc7e":"markdown","f9d361fb":"markdown","969bffcf":"markdown","f1dd91cd":"markdown","e1254e05":"markdown","700ac3ae":"markdown","df581229":"markdown","374f95f8":"markdown","251dd4f1":"markdown","6e65f17c":"markdown","1abd1e1e":"markdown","d07fc9f7":"markdown","9fe271ba":"markdown","880b9c9f":"markdown","fa9a953a":"markdown","28ebe0bb":"markdown","4a7638c9":"markdown","d08bc3dc":"markdown","228f8121":"markdown","8f66c4c5":"markdown","ac9f6c25":"markdown","744f7031":"markdown","0b7ebf14":"markdown","1a144e85":"markdown","478afcda":"markdown","01c438ca":"markdown","5cab8469":"markdown","d1fe2470":"markdown","d06d81fe":"markdown","45c557c4":"markdown","58111221":"markdown","e65e6c43":"markdown","229400d7":"markdown","20594e22":"markdown","4e9e7754":"markdown","888e05d5":"markdown","8a0175ce":"markdown","91240144":"markdown","198ed9cc":"markdown","0366059b":"markdown","cea559bd":"markdown","1f9c3644":"markdown","9a4f50d2":"markdown","d179a264":"markdown","067b0aa4":"markdown","201d4d9e":"markdown","a94ec46b":"markdown","6250062f":"markdown","68177da1":"markdown","6d720cff":"markdown","2abae76b":"markdown","e6c1cd31":"markdown","60d630e2":"markdown","ec4f9112":"markdown","df6b3971":"markdown","22d5bedc":"markdown","e90006c4":"markdown"},"source":{"a432db94":"!pip install feature_engine","519841dc":"import os\nfrom io import StringIO\n#from pandasql import sqldf\n\nimport pandas as pd\nimport numpy as np\nimport math\nimport decimal  \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import OrderedDict\n\nimport nltk\nimport scipy.stats as stats\nfrom feature_engine.encoding import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import PCA\nimport umap\nimport umap.plot\n\n%matplotlib inline\n# Auto reloads notebook when changes are made\n%reload_ext autoreload\n%autoreload 2","c22ef7f5":"pd.options.display.max_columns = None","e6e4a068":"#Read in data\nEvent_Details = pd.read_csv(r'\/kaggle\/input\/us-figure-skating-data-2020-regionals-int-ladies\/Scraped_Event_Details.csv')\nOfficial_Details = pd.read_csv(r'\/kaggle\/input\/us-figure-skating-data-2020-regionals-int-ladies\/Scraped_Official_Details.csv')\nSkater_Details = pd.read_csv(r'\/kaggle\/input\/us-figure-skating-data-2020-regionals-int-ladies\/Scraped_Skater_Details.csv')\nComponent_Scores = pd.read_csv(r'\/kaggle\/input\/us-figure-skating-data-2020-regionals-int-ladies\/Scraped_Component_Scores.csv')\nTechnical_Scores = pd.read_csv(r'\/kaggle\/input\/us-figure-skating-data-2020-regionals-int-ladies\/Scraped_Technical_Scores.csv')","589cafc9":"#Read in supplemental data\nUSFS_Sections_Regions = pd.read_csv(r'\/\/kaggle\/input\/us-figure-skating-data-2020-regionals-int-ladies\/SupplementalData_SectionsRegions.csv')\nOfficial_Supplemental = pd.read_csv(r'\/kaggle\/input\/us-figure-skating-data-2020-regionals-int-ladies\/SupplementalData_OfficialLocations.csv')\n#Starting_BaseValues = pd.read_csv(r'\/kaggle\/input\/us-figure-skating-data-2020-regionals-int-ladies\/SupplementalData_StartingBaseValues.csv')","82417227":"#Append Official Section and Region, first by attaching the Other_Boundary field, then by joining to USFS_Sections_Regions\nOfficial_Details=Official_Details.merge(Official_Supplemental, how='left', on=['Official_StateOrCountry','Official_City'])\nOfficial_Details=Official_Details.merge(USFS_Sections_Regions, how='left', left_on=['Official_StateOrCountry','Other_Boundary'],\n                                       right_on=['State_Abbrev','Other_Boundary'])\nOfficial_Details.drop(['State', 'State_Abbrev', 'Other_Boundary'], axis='columns', inplace=True)\nOfficial_Details.rename(columns={'Section': 'Official_Section', 'Region': 'Official_Region'}, inplace=True)\ndel Official_Supplemental, USFS_Sections_Regions","76d744c3":"#Remove any skaters that withdrew (and therefore would not have gotten scored)\nwithdrawn_bool = Skater_Details['Skater_Placement']=='WD'\nSkater_Details = Skater_Details[~withdrawn_bool].copy()\nSkater_Details['Skater_Placement']=Skater_Details['Skater_Placement'].astype('int64')\nSkater_Details['Skate_Order']=Skater_Details['Skate_Order'].astype('int64')\ndel withdrawn_bool","737522f9":"#Append event details to skater details\nSkater_Details=Skater_Details.merge(Event_Details, how='left', on=['Event_Index'])","e52be3fc":"#Technical official slice\/pivot to append to skater details\nbool_judge=Official_Details['Official'].str.contains('Judge')\nTechPanelPlusReferee= Official_Details[~bool_judge]\n\nTechPanelPlusReferee=TechPanelPlusReferee.pivot(index='Event_Index', columns='Official', \n                     values=['Official_Name','Official_City','Official_StateOrCountry', 'Official_Section', 'Official_Region'])\n\n#some help from stackoverflow to collapse the pivot headers\n#https:\/\/stackoverflow.com\/questions\/37087020\/collapse-a-pandas-multiindex\nTechPanelPlusReferee.columns = TechPanelPlusReferee.columns.to_series().apply('_'.join)\n\nSkater_Details=Skater_Details.merge(TechPanelPlusReferee, how='left', on='Event_Index')\ndel TechPanelPlusReferee","d4e62cab":"Technical_Scores['I']=np.where(Technical_Scores['I'].isin(['*','!','e','<','<<','q']), Technical_Scores['I'], None)","3d08c96f":"Skater_Details['Number_Falls']=np.where(Skater_Details['Deductions_Falls']==0,0,-1*Skater_Details['Deductions_Falls']\/.5)\nSkater_Details['Number_Falls']=Skater_Details['Number_Falls'].astype('int')","a8f455da":"#Turns out, rounding in the world of computer science is not like Excel... \n#I had a hard time using the decimal library to use regular old math rounding... \n#this may be something for another day to dig into further, but for now just added something insignificant figure \n#to make sure .05's rounded up properly; ironically doing the calculations here were intended to save me time from \n#copying and pasting in the final values - oops!\ncols_to_agg = ['J1_PC', 'J2_PC', 'J3_PC', 'J4_PC', 'J5_PC', 'J6_PC', 'J7_PC']\ncumulative_PC = Component_Scores[cols_to_agg].sum(axis=1)\nmax_PC = Component_Scores[cols_to_agg].max(axis=1)\nmin_PC = Component_Scores[cols_to_agg].min(axis=1)\ncount_PC = Component_Scores[cols_to_agg].count(axis=1)\nComponent_Scores['PC_avg']= cumulative_PC \/ count_PC\nComponent_Scores['PC_avg_excl_hi_lo'] = ((cumulative_PC - max_PC - min_PC) \/ (count_PC - 2)) \nComponent_Scores['PC_avg_excl_hi_lo']+=.0000001 #fudge\nComponent_Scores['PC_avg_excl_hi_lo']=round(Component_Scores['PC_avg_excl_hi_lo'],2)\nComponent_Scores['PanelScores_FactoredPC']=(Component_Scores['PC_avg_excl_hi_lo']*Component_Scores['PC_Factor'])\nComponent_Scores['PanelScores_FactoredPC']+=.0000001 #fudge\nComponent_Scores['PanelScores_FactoredPC']=round(Component_Scores['PanelScores_FactoredPC'],2)\nComponent_Scores['PanelScores_FactoredPC']=(Component_Scores['PanelScores_FactoredPC']*Component_Scores['PC_Factor_General'])\nComponent_Scores['PanelScores_FactoredPC']+=.0000001 #fudge\nComponent_Scores['PanelScores_FactoredPC']=round(Component_Scores['PanelScores_FactoredPC'],2)\ndel cols_to_agg, cumulative_PC, max_PC, min_PC, count_PC","a39ea360":"#The scraped data already contains the adjusted technical scores, so I just need to aggregate those.  \n#However, I do want a raw average excluding high\/low for the GOE:\ncols_to_agg = ['J1_GOE', 'J2_GOE', 'J3_GOE', 'J4_GOE', 'J5_GOE', 'J6_GOE', 'J7_GOE']\n\n#When elements are completely asterisked, the GOE for the judges is shown in the protocol as a '-'\n#naturally python gets huffy when you ask it to consider a dash as a number so cleaning this up\nfor col in cols_to_agg:\n    Technical_Scores[col]=Technical_Scores[col].replace('-', np.nan)\n    Technical_Scores[col]=Technical_Scores[col].replace('\u00a0\u00a0', np.nan)\n    Technical_Scores[col]=pd.to_numeric(Technical_Scores[col])\n\ncumulative_GOE = Technical_Scores[cols_to_agg].sum(axis=1)\nmax_GOE = Technical_Scores[cols_to_agg].max(axis=1)\nmin_GOE = Technical_Scores[cols_to_agg].min(axis=1)\ncount_GOE = Technical_Scores[cols_to_agg].count(axis=1)\nTechnical_Scores['GOE_avg'] = cumulative_GOE  \/ count_GOE\nTechnical_Scores['GOE_avg_excl_hi_lo'] = ((cumulative_GOE - max_GOE - min_GOE) \/ (count_GOE - 2)) \nTechnical_Scores['GOE_avg_excl_hi_lo']+=.0000001 #fudge\nTechnical_Scores['GOE_avg_excl_hi_lo']=round(Technical_Scores['GOE_avg_excl_hi_lo'],2)\ndel cols_to_agg, cumulative_GOE, max_GOE, min_GOE, count_GOE","dd62fb9b":"#append event PC scores, tech scores, and combined total scores\nPC_FinalScores=Component_Scores.groupby(['Event_Index','Skater_Placement']).agg({'PanelScores_FactoredPC': sum}).reset_index()\nTech_FinalScores=Technical_Scores.groupby(['Event_Index','Skater_Placement']).agg({'PanelScores_Technical': sum}).reset_index()\nSkater_Details=Skater_Details.merge(PC_FinalScores, how='left', on=['Event_Index','Skater_Placement'])\nSkater_Details=Skater_Details.merge(Tech_FinalScores, how='left', on=['Event_Index','Skater_Placement'])\n\n#get event total score as sum of PC, tech, and deductions\nSkater_Details['Final_Score']=Skater_Details['PanelScores_FactoredPC']+Skater_Details['PanelScores_Technical']+ Skater_Details['Deductions_Total']\ndel PC_FinalScores, Tech_FinalScores\n\n#check if any issues in the scraped totals versus calculated\nSkater_Details['Final_Score_diff']=(Skater_Details['Final_Score']-Skater_Details['Total_Segment_Score']).round()\nSkater_Details['Tech_Score_diff']=(Skater_Details['PanelScores_Technical']-Skater_Details['Total_Element_Score']).round()\nSkater_Details['PC_Score_diff']=(Skater_Details['PanelScores_FactoredPC']-Skater_Details['Total_Component_Score']).round()\nSkater_Details[['Final_Score_diff', 'Tech_Score_diff', 'PC_Score_diff']].describe()","83634821":"Skater_Details[['Event_Index','Skater_Name','Tech_Score_diff','PanelScores_Technical',\n                'Total_Element_Score']].sort_values(by=['Tech_Score_diff']).head()","64b17eb7":"#remove Leila Slayton, who has some data issue in the raw protocols\noutlier_skater = Skater_Details['Skater_Name']=='Leila Slayton'\nSkater_Details = Skater_Details[~outlier_skater]\ndel outlier_skater","0017bd35":"#Create a table that has just the final round events to cmbine short program and free skate total scores,\n#then join back onto skater details\n#Not sure I need\/want this for this particular project, but adding for now while I'm thinking of it and want to expand scope!\nCombinedEventFinal=Skater_Details[Skater_Details['Round']=='Final'].groupby(['Region', 'Level', 'Gender', 'Skater_Name', 'Skater_Club']).agg({\n    'PanelScores_FactoredPC': sum,'PanelScores_Technical': sum,'Deductions_Total':sum,'Final_Score':sum}).reset_index()\nCombinedEventFinal.rename(columns={'PanelScores_FactoredPC':'FinalCombinedEvent_PC', \n                                   'PanelScores_Technical':'FinalCombinedEvent_Tech',\n                                   'Deductions_Total':'FinalCombinedEvent_Ded',\n                                   'Final_Score':'FinalCombinedEvent_Total'},inplace=True)\n\nSkater_Details=Skater_Details.merge(CombinedEventFinal, how='left', on=['Region','Level','Gender','Skater_Name','Skater_Club'])\ndel CombinedEventFinal","54d2f674":"#Adjust technical score raw data so that judges scores are on each row rather than separate columns\nTechnical_Scores = Technical_Scores.melt(id_vars=['Event_Index','Skater_Placement', 'Element_Number', 'Element', 'I', \n        'Base_Value', 'FreeSkate2ndHalfJump_Flag', 'GOE' , 'GOE_avg', 'GOE_avg_excl_hi_lo'], \n        value_vars=['J1_GOE', 'J2_GOE', 'J3_GOE', 'J4_GOE', 'J5_GOE', 'J6_GOE', 'J7_GOE', 'PanelScores_Technical'],\n        var_name='Judge', value_name='Raw_GOE')\n\n#In reality, the value housed for PanelScores_Technical are the combined base value and scaled GOEs, not the average raw GOE\nTechnical_Scores['BaseValue_Plus_GOE']=np.where(Technical_Scores['Judge']=='PanelScores_Technical', \n                                                Technical_Scores['Raw_GOE'], np.nan)\nTechnical_Scores['Raw_GOE']=np.where(Technical_Scores['Judge']=='PanelScores_Technical', \n                                     Technical_Scores['GOE_avg'], Technical_Scores['Raw_GOE'])\n\n#replace raw data judge labels with ones that will facilitate a join to the Judge_Details dataframe later\nTechnical_Scores['Judge'].replace({'J1_GOE': 'Judge 1', 'J2_GOE': 'Judge 2', 'J3_GOE': 'Judge 3', 'J4_GOE': 'Judge 4', \n                                   'J5_GOE': 'Judge 5', 'J6_GOE': 'Judge 6', 'J7_GOE': 'Judge 7'}, inplace=True)\n\n#Join judge name and location data\nTechnical_Scores=Technical_Scores.merge(Official_Details,how='left',\n                                        left_on=['Event_Index','Judge'],right_on=['Event_Index','Official'])\nTechnical_Scores.drop(['Official', 'GOE_avg', 'GOE_avg_excl_hi_lo'],axis='columns', inplace=True)","c825ee93":"#Adjust component score raw data so that judges scores are on each row rather than separate columns\nComponent_Scores = Component_Scores.melt(id_vars=['Event_Index','Skater_Placement','Program_Components','PC_avg'], \n                      value_vars=['J1_PC', 'J2_PC', 'J3_PC', 'J4_PC', 'J5_PC', 'J6_PC', 'J7_PC', 'PanelScores_PC'],\n                      var_name='Judge', value_name='Component_Score')\n\n#this is the average excluding hi\/lo\nComponent_Scores['PanelScores_PC_excl_HiLo']=np.where(Component_Scores['Judge']=='PanelScores_PC', \n                                     Component_Scores['Component_Score'], np.nan)\n\n#this is the average of the jduges scores before hi\/lo removed\nComponent_Scores['PanelScores_PC']=np.where(Component_Scores['Judge']=='PanelScores_PC', \n                                     Component_Scores['PC_avg'], np.nan)\n\n\n#replace raw data judge labels with ones that will facilitate a join to the Judge_Details dataframe later\nComponent_Scores['Judge'].replace({'J1_PC': 'Judge 1', 'J2_PC': 'Judge 2', 'J3_PC': 'Judge 3', 'J4_PC': 'Judge 4',\n                                   'J5_PC': 'Judge 5', 'J6_PC': 'Judge 6', 'J7_PC': 'Judge 7'}, inplace=True)","326da767":"#Pivot so that each program component is column rather than row level, then remove multindices created from pivoting\nComponent_Scores_temp1=Component_Scores.pivot(index=['Event_Index','Skater_Placement','Judge'],\n                       columns='Program_Components', \n                       values='Component_Score').reset_index().rename_axis(None, axis=1)\n\n#Same thing but using the average before hi\/low\nComponent_Scores_temp2=Component_Scores.pivot(index=['Event_Index','Skater_Placement','Judge'],\n                       columns='Program_Components', \n                       values='PanelScores_PC').reset_index().rename_axis(None, axis=1)\nComponent_Scores_temp2.rename(columns={'Composition': 'Composition_avg', \n                                       'Interpretation of the Music': 'Interpretation of the Music_avg',\n                                       'Performance': 'Performance_avg' , 'Skating Skills': 'Skating Skills_avg'}, inplace=True)","260ed22b":"#Combine the component scores and the average scores before removing hi\/low\nComponent_Scores=Component_Scores_temp1.merge(Component_Scores_temp2, how='left', on=['Event_Index','Skater_Placement','Judge'])\n\n#Join judge name and location data\nComponent_Scores=Component_Scores.merge(Official_Details,how='left',\n                                        left_on=['Event_Index','Judge'],right_on=['Event_Index','Official'])","7601807e":"del Component_Scores_temp1, Component_Scores_temp2","c0b7d320":"#append event details to technical scores and component scores data\nTechnical_Scores=Technical_Scores.merge(Skater_Details, how='inner', on=['Event_Index', 'Skater_Placement'])\nComponent_Scores=Component_Scores.merge(Skater_Details, how='inner', on=['Event_Index', 'Skater_Placement'])","c49afeec":"#Handy field for later EDA\nTechnical_Scores['Round_and_ProgramType']=Technical_Scores['Round']+' ' +Technical_Scores['Program_Type']","3b783b8f":"judge_bool = Technical_Scores['Judge']!='PanelScores_Technical'\ntemp_table1= pd.DataFrame(Technical_Scores[~judge_bool].groupby(['Event_Index','Skater_NameClub','Number_Falls'])['Raw_GOE'].rank(\"first\"))\ntemp_table1.rename(columns={'Raw_GOE': 'Ranked_Raw_GOE'}, inplace=True)","9683d420":"temp_table2= Technical_Scores[~judge_bool][['Event_Index','Skater_NameClub','Element_Number','Number_Falls']]\ntemp_table= temp_table2.join(temp_table1)","ce5df820":"temp_table['Fall_Indicator']=np.where(temp_table['Number_Falls']<temp_table['Ranked_Raw_GOE'], 0, 1)\ntemp_table['Fall_Indicator']=np.where(temp_table['Ranked_Raw_GOE'].isna(), 0, temp_table['Fall_Indicator'])","ed7237f3":"Technical_Scores=Technical_Scores.merge(temp_table, how='left', on=['Event_Index','Skater_NameClub','Element_Number','Number_Falls'])","c1bdc92e":"del judge_bool, temp_table, temp_table1, temp_table2","f79d778f":"judge_bool = Technical_Scores['Judge']!='PanelScores_Technical'\ntemp_table1=Technical_Scores[~judge_bool].groupby(['Event_Index','Skater_NameClub']).agg(\n    Max_Element_Number=pd.NamedAgg(column='Element_Number', aggfunc=np.max)).reset_index()","495eda25":"temp_table2=Technical_Scores[~judge_bool][['Event_Index','Skater_NameClub', 'Element_Number']]","84a717db":"temp_table=temp_table2.merge(temp_table1, how='left', on=['Event_Index','Skater_NameClub'])\ntemp_table['Max_Element_Flag']=np.where(temp_table['Max_Element_Number']==temp_table['Element_Number'],1,0)\ntemp_table.drop(columns=['Max_Element_Number'], inplace=True)","126b0517":"Technical_Scores=Technical_Scores.merge(temp_table, how='left', on=['Event_Index','Skater_NameClub','Element_Number'])\nTechnical_Scores['TimeViolation_Flag']= np.where(((Technical_Scores['Deductions_TimeViolation']<0) & \n                                                  (Technical_Scores['Max_Element_Flag']==1)), 1, 0)","fa198d87":"del judge_bool, temp_table1, temp_table2, temp_table","2102e04a":"#Step 0 - Create Element Category \nTechnical_Scores['Element_Category']=np.where(Technical_Scores['Element'].isin(['StSqB','StSq1','StSq2','StSq3','StSq4']), 'Step Sequence',\n                                     np.where(Technical_Scores['Element'].str.contains('Sp|SpB|Sp1|Sp2|Sp3|Sp4'), 'Spin',\n                                     'Jump'))","f1453e33":"#First, remove jump errors\nTechnical_Scores['Downgrade_count']=Technical_Scores['Element'].str.count('\\\\<<')\nTechnical_Scores['Element_mod']=Technical_Scores['Element'].str.replace('<<', '', regex=False)\n\nTechnical_Scores['UnderRotate_count']=Technical_Scores['Element'].str.count('\\\\<')\nTechnical_Scores['Element_mod']=Technical_Scores['Element_mod'].str.replace('<', '', regex=False)\n\nTechnical_Scores['Element_mod']=Technical_Scores['Element_mod'].str.replace('e', '', regex=False)\nTechnical_Scores['Element_mod']=Technical_Scores['Element_mod'].str.replace('!', '', regex=False)\n\nTechnical_Scores['Starred_count']=Technical_Scores['Element'].str.count('\\\\*')\nTechnical_Scores['Element_mod']=Technical_Scores['Element_mod'].str.replace('*', '', regex=False)","7bbb9a67":"#Second, create a field to capture what level the step sequence was, and remove that information from Element_mod\nTechnical_Scores['Step_Level']=Technical_Scores['Element_mod'].where(Technical_Scores['Element_Category']=='Step Sequence', np.nan)\nTechnical_Scores['Step_Level'].replace({'StSqB': 'B', \n                                        'StSq1': 1, \n                                        'StSq2': 2, \n                                        'StSq3': 3, \n                                        'StSq4': 4}, inplace=True)\n\nTechnical_Scores['Element_mod'].replace({'StSqB': 'StSq', \n                                         'StSq1': 'StSq', \n                                         'StSq2': 'StSq', \n                                         'StSq3': 'StSq', \n                                         'StSq4': 'StSq'}, inplace=True)\n\nTechnical_Scores['Step_Level']=np.where(Technical_Scores['Step_Level']=='B',0, Technical_Scores['Step_Level'])\nTechnical_Scores['Step_Level'].fillna(0,inplace=True)","4c1c7baa":"#Third, remove +SEQ and +REP and create a +REP Flag\nTechnical_Scores['Unintended_SEQ_Flag']=Technical_Scores['Element_mod'].str.contains('+SEQ+', regex=False).astype(int)\nTechnical_Scores['Element_mod']=Technical_Scores['Element_mod'].str.replace('+SEQ+', '+', regex=False)\n\nTechnical_Scores['Intended_SEQ_Flag']=Technical_Scores['Element_mod'].str.contains('+SEQ', regex=False).astype(int)\nTechnical_Scores['Element_mod']=Technical_Scores['Element_mod'].str.replace('+SEQ', '', regex=False)\n\nTechnical_Scores['REP_Flag']=Technical_Scores['Element_mod'].str.contains('+REP', regex=False).astype(int)\nTechnical_Scores['Element_mod']=Technical_Scores['Element_mod'].str.replace('+REP', '', regex=False)","fd46ef4b":"#Fourth, handle bonuses\nTechnical_Scores['Bonus_Count']=Technical_Scores['Element_mod'].str.count('b1')\nTechnical_Scores['Element_mod']=Technical_Scores['Element_mod'].str.replace('2Ab1', '2A', regex=False)\nTechnical_Scores['Element_mod']=Technical_Scores['Element_mod'].str.replace('3Sb1', '3S', regex=False)\nTechnical_Scores['Element_mod']=Technical_Scores['Element_mod'].str.replace('3Tb1', '3T', regex=False)\nTechnical_Scores['Element_mod']=Technical_Scores['Element_mod'].str.replace('3Lob1', '3Lo', regex=False)\nTechnical_Scores['Element_mod']=Technical_Scores['Element_mod'].str.replace('3Fb1', '3F', regex=False)\nTechnical_Scores['Element_mod']=Technical_Scores['Element_mod'].str.replace('3Lzb1', '3Lz', regex=False)","b1165096":"#Fifth, handle spin V errors\nTechnical_Scores['Spin_V_Flag']=np.where(Technical_Scores['Element_mod'].str.endswith('V'), 1, 0)\nTechnical_Scores['Element_mod']=Technical_Scores['Element_mod'].str.replace('V', '', regex=False)","5460af8a":"#Sixth, spin level and type\nTechnical_Scores['Spin_Flag']=np.where(Technical_Scores['Element_Category']=='Spin','Spin','Not A Spin')\nTechnical_Scores['Spin_Details']=Technical_Scores['Element_mod'].where(Technical_Scores['Element_Category']=='Spin', np.nan)\nTechnical_Scores['Spin_Details']=Technical_Scores['Spin_Details'].str.partition('Sp', expand=False)\nTechnical_Scores['Spin_Level']=Technical_Scores['Spin_Details'].str[2]\nTechnical_Scores['Spin_Type']=Technical_Scores['Spin_Details'].str[0]\nTechnical_Scores['Spin_Sp']= 'Sp'\nTechnical_Scores['Spin_Type']=Technical_Scores['Spin_Type']+Technical_Scores['Spin_Sp']\nTechnical_Scores.drop(['Spin_Sp', 'Spin_Details'], axis='columns', inplace=True)\n\nTechnical_Scores['Element_mod']=np.where(Technical_Scores['Spin_Type'].isnull(), Technical_Scores['Element_mod'],\n                                         Technical_Scores['Spin_Type'])\n\nTechnical_Scores['Spin_Level']=np.where(Technical_Scores['Element'].isin(['FSSp', 'CCoSp', 'CoSp', 'Sp', 'SSp', 'FCoSp']),0,\n                                        Technical_Scores['Spin_Level'])  #to capture no value spins, need another tweak\n\nTechnical_Scores['Spin_Level_num']=pd.to_numeric(np.where(Technical_Scores['Spin_Level'].isin(['B','0','']), 0, Technical_Scores['Spin_Level']))\nTechnical_Scores['Spin_Level_num'].fillna(0,inplace=True)","48a5fcc7":"#Seventh, create jump combo flag and count of jumps in combo\nTechnical_Scores['JumpCombo_Flag']=Technical_Scores['Element_mod'].str.contains('+', regex=False).astype(int)\nTechnical_Scores['JumpCombo_Count']=np.where(Technical_Scores['Element_mod'].str.count('\\\\+')>0,\n                                             Technical_Scores['Element_mod'].str.count('\\\\+')+1,0)\n\nTechnical_Scores['JumpCombo_element1']=Technical_Scores['Element_mod'].str.split('+', expand=True)[0]\nTechnical_Scores['JumpCombo_element1']=np.where(Technical_Scores['JumpCombo_Flag']==False, \n                                                np.nan, Technical_Scores['JumpCombo_element1'])\n\nTechnical_Scores['JumpCombo_element2']=Technical_Scores['Element_mod'].str.split('+', expand=True)[1]\nTechnical_Scores['JumpCombo_element2']=np.where(Technical_Scores['JumpCombo_Count']<2, \n                                                np.nan, Technical_Scores['JumpCombo_element2'])\n\nTechnical_Scores['JumpCombo_element3']=Technical_Scores['Element_mod'].str.split('+', expand=True)[2]\nTechnical_Scores['JumpCombo_element3']=np.where(Technical_Scores['JumpCombo_Count']<3, \n                                                np.nan, Technical_Scores['JumpCombo_element3'])","7ba92439":"#Eighth, create a flag for solo jump elements\nbool_step = Technical_Scores['Element_mod']=='StSq'\nbool_spin = (Technical_Scores['Spin_Type'].str.endswith('Sp')).replace(np.nan,False)\nbool_jumpcombo = Technical_Scores['JumpCombo_Flag']\nbool_jumpsolo = ~(bool_step | bool_spin | bool_jumpcombo)\nTechnical_Scores['SoloJump_Flag']=np.where(bool_jumpsolo,1,0)\nTechnical_Scores['SoloJump_element']=np.where(Technical_Scores['SoloJump_Flag']==1,Technical_Scores['Element_mod'],np.nan)\ndel bool_step, bool_spin, bool_jumpcombo, bool_jumpsolo","8184e057":"#create additional elements to do bag of words transformations\nTechnical_Scores['Element_mod2']=Technical_Scores['Element_mod'].str.replace('+', ' ', regex=False)\nTechnical_Scores['Element_mod3']=Technical_Scores['Element'].str.replace('+', ' ', regex=False)\n\n#CountVectorizer does not cope with special characters for the new feature names, so I am swapping these for letters\nTechnical_Scores['Element_mod3']=Technical_Scores['Element_mod3'].str.replace('e', 'Edge', regex=False)\nTechnical_Scores['Element_mod3']=Technical_Scores['Element_mod3'].str.replace('<<', 'Down', regex=False)\nTechnical_Scores['Element_mod3']=Technical_Scores['Element_mod3'].str.replace('<', 'Under', regex=False)\nTechnical_Scores['Element_mod3']=Technical_Scores['Element_mod3'].str.replace('!', 'Unclear', regex=False)\nTechnical_Scores['Element_mod3']=Technical_Scores['Element_mod3'].str.replace('*', 'Star', regex=False)\nTechnical_Scores['Element_mod3']=Technical_Scores['Element_mod3'].str.replace('REP', '', regex=False)\nTechnical_Scores['Element_mod3']=Technical_Scores['Element_mod3'].str.replace('SEQ', '', regex=False)","fcd6e26c":"#path_name = r'C:\\Skating_Data'\n#Skater_Details.to_excel(os.path.join(path_name, r'Skater_Event_Details3.xlsx'))\n#Component_Scores.to_excel(os.path.join(path_name, r'Component_Scores3.xlsx'))\n#Technical_Scores.to_excel(r'C:\\Skating_Data\\Technical_Scores3.xlsx')","f5a0e576":"bagofwords = CountVectorizer(lowercase=False)\nindividual_element_array = bagofwords.fit_transform(Technical_Scores['Element_mod2'])\ncol_names= [col+'_BaseFlag' for col in bagofwords.get_feature_names()]\nindividual_elements=pd.DataFrame(individual_element_array.toarray(),columns=col_names)\nTechnical_Scores=Technical_Scores.join(individual_elements,how='left')","7d987420":"bagofwords2 = CountVectorizer(lowercase=False)\nindividual_element_array2 = bagofwords2.fit_transform(Technical_Scores['Element_mod3'])\ncol_names2= [col+'_Flag' for col in bagofwords2.get_feature_names()]\nindividual_elements2=pd.DataFrame(individual_element_array2.toarray(),columns=col_names2)\nTechnical_Scores=Technical_Scores.join(individual_elements2,how='left')","a53e8e27":"#get a running total of the elements performed, and the base values\ntemp=Technical_Scores[Technical_Scores.Judge=='PanelScores_Technical'].groupby(['Event_Index', \n            'Round', 'Program_Type', 'Skater_Name'])[col_names + col_names2 + ['Base_Value']].cumsum()\n\n\n\n#rename the columns so we don't bring back duplicate names\nfor col in temp.columns:\n    temp.rename(columns={col:col.replace(\"Flag\",\"Flag_cumsum\")},inplace=True)\ntemp.rename(columns={'Base_Value':'Base_Value_cumsum'},inplace=True)\ncumsum_list=[]\ncumsum_list= [col for col in temp.columns]\n\n#get the unique things we just did our cumsum by\ntemp2=Technical_Scores[Technical_Scores.Judge=='PanelScores_Technical'][['Event_Index', \n            'Round', 'Program_Type', 'Skater_Name']]\n\n#tack the cumulative sums back onto the event index, round, program type, and skater name\ntemp3=Technical_Scores[Technical_Scores.Judge=='PanelScores_Technical'][['Event_Index','Round', 'Program_Type', \n                                                                         'Skater_Name','Element_Number']]\ntemp4 = pd.concat([temp2, temp], axis=1)\ntemp5 = temp4.join(temp3['Element_Number'])\n\n#join the cumulative information back onto Technical Scores\nTechnical_Scores=Technical_Scores.merge(temp5, how='left', on=['Event_Index', 'Round', 'Program_Type', \n                                                               'Skater_Name', 'Element_Number'])","19c5f8be":"del bool_judge, bagofwords, bagofwords2, individual_element_array, \\\n    individual_element_array2, individual_elements, individual_elements2, temp, temp2, temp3, temp4, temp5","4284609b":"cumsum_list.pop() #remove Base_Value_cumsum","4029ec27":"Y_list = ['GOE', 'Total_Segment_Score', 'Base_Value_cumsum'] #let's use this to color code our 'clusters'\n\nrandom_effects_list = ['Event_Index', 'Official_Name', 'Skater_NameClub'] \n\ncategorical_vars=['Round' ,'Group' ,'Program_Type','Skater_Club','Region','Section','Official_Region','Official_Section']\n\n#numerical_vars=['Deductions_Total', 'Deductions_Falls', 'Deductions_CostumeOrPropFailure', 'Deductions_TimeViolation'] \n\n#in theory can also bring forward to last element row:\n#step level, avg spin level, etc.\n#but for now, just leaving as-is to get an idea how this overall will work\n\nvar_list = Y_list + random_effects_list + categorical_vars + cumsum_list\n#cumsum_list now no longer inclues Base_Value_cumsum, which we popped off and instead put up in the Y list","ca1333e4":"cat_Technical_Scores = Technical_Scores[categorical_vars].fillna(value='Null')\nTechnical_Scores.drop(columns=categorical_vars, inplace=True)\nTechnical_Scores=pd.concat([Technical_Scores,cat_Technical_Scores],axis=1)","b9f8a497":"#GPBoost complained about white spaces in names, so fixing problem variables before OHE\nTechnical_Scores['Program_Type']=np.where(Technical_Scores['Program_Type']=='Free Skate', 'Free_Skate', \n                                          Technical_Scores['Program_Type'])\nTechnical_Scores['Element_Category']=np.where(Technical_Scores['Element_Category']=='Step Sequence','Step_Sequence',\n                                              Technical_Scores['Element_Category'])\nTechnical_Scores['Group']=np.where(Technical_Scores['Group']==' ','Null',Technical_Scores['Group'])\nTechnical_Scores['Region']=Technical_Scores['Region'].str.replace(' ', '_')\nTechnical_Scores['Official_Region']=Technical_Scores['Official_Region'].str.replace(' ', '_')\nTechnical_Scores['Skater_Club']=Technical_Scores['Region'].str.replace('-', '')\nTechnical_Scores['Skater_Club']=Technical_Scores['Region'].str.replace(' ', '_')","578e8170":"#XGBoost Grumbles about < and << in names\nTechnical_Scores['I']=np.where(Technical_Scores['I']=='<', 'under',Technical_Scores['I'])\nTechnical_Scores['I']=np.where(Technical_Scores['I']=='<<', 'down',Technical_Scores['I'])","60dcb407":"test_bool = Technical_Scores['Region']== 'Southwestern'\npanel_bool = Technical_Scores['Judge']=='PanelScores_Technical'\nlastrow_bool = Technical_Scores['Max_Element_Flag']==1\n\ntrain = Technical_Scores[~test_bool & panel_bool & lastrow_bool][var_list].reset_index(drop=True)\ntest = Technical_Scores[test_bool & panel_bool & lastrow_bool][var_list].reset_index(drop=True)\n\ntrain_X = train.drop(columns=Y_list)\ntest_X = test.drop(columns=Y_list)","09aed1b5":"#we will use these to lable our clusters\ntrain_y = train['GOE']\ntest_y = test['GOE']\n\ntrain_y2 = train['Total_Segment_Score']\ntest_y2 = test['Total_Segment_Score']\n\ntrain_y3 = train['Base_Value_cumsum']\ntest_y3 = test['Base_Value_cumsum']\n\ntrain_y4 = train['Region']\ntest_y4 = test['Region']\n\ndel train, test","27651de3":"ohe_encoder = OneHotEncoder(variables=categorical_vars, drop_last=True)\n\ntrain_X = ohe_encoder.fit_transform(train_X)\ntest_X = ohe_encoder.transform(test_X)\n\n#ohe_encoder.encoder_dict_","b63bba00":"SS = StandardScaler()\n\nX_SS_train = SS.fit_transform(train_X[cumsum_list])\nX_SS_test = SS.transform(test_X[cumsum_list])","dadec9e9":"pca = PCA(n_components=0.9, random_state=42)","80b6b001":"PCA_train_X = pca.fit_transform(X_SS_train)\nPCA_test_X = pca.transform(X_SS_test)","c73607d0":"pca_col_names = ['PCA_' + str(i+1) for i in range(0,pca.n_components_)]\nPCA_train_X=pd.DataFrame(PCA_train_X,  columns=pca_col_names)\nPCA_test_X=pd.DataFrame(PCA_test_X,  columns=pca_col_names)","e08db580":"#Typically we'd want to zap the stuff we transformed -- but for UMAP, I want to see if PCA does better or worse\ntrain_X = train_X.drop(columns=cumsum_list)\ntest_X = test_X.drop(columns=cumsum_list)\n\ntrain_X = pd.concat([train_X, PCA_train_X], axis=1)\ntest_X = pd.concat([test_X, PCA_test_X], axis=1)","e2dc9fd2":"del X_SS_train, X_SS_test, pca, PCA_train_X, PCA_test_X","f4b0701f":"#break apart fixed and random effects\ntrain_X_fixed=train_X.drop(columns=random_effects_list)\ntrain_X_random=train_X[random_effects_list]\n\ntest_X_fixed=test_X.drop(columns=random_effects_list)\ntest_X_random=test_X[random_effects_list]","59fab479":"def pca_plot(dataframe, color_labels, title):\n    fig, ax = plt.subplots(figsize=(14,8))\n    scatter= ax.scatter(\n        dataframe['PCA_1'],\n        dataframe['PCA_2'],\n        c=color_labels, cmap='Spectral', s=25)\n    plt.gca().set_aspect('equal', 'datalim')\n    legend1 = ax.legend(*scatter.legend_elements(),\n                    loc=\"lower right\", title= title+\" Legend\")\n    ax.add_artist(legend1)\n    plt.title('First 2 PCAs:  '+title, fontsize=24)","48b3125c":"pca_plot(train_X_fixed, train_y3, 'Total Base Value')","d57a9cb4":"pca_plot(train_X_fixed, train_y2, 'Total Segment Score')","0cbd9cdc":"pca_plot(train_X_fixed, train_y, 'GOE')","329a5b47":"#let's just take the first 20 components (which explain the most variance in the data anyways)\npca_list = ['PCA_' + str(i+1) for i in range(0,20)]","4a50c6d0":"mapper = umap.UMAP(random_state=42, metric='euclidean', n_neighbors=60).fit_transform(train_X_fixed[pca_list])","11c3767a":"def umap_plot(umap_name, color_labels, title):\n    fig, ax = plt.subplots(figsize=(14,8))\n    scatter= ax.scatter(\n        umap_name[:, 0],\n        umap_name[:, 1],\n        c=color_labels, cmap='Spectral', s=18)\n    plt.gca().set_aspect('equal', 'datalim')\n    legend1 = ax.legend(*scatter.legend_elements(), loc=\"lower right\", title=title + \" Legend\")\n    ax.add_artist(legend1)\n    plt.title('UMAP projection of the Skating dataset:  ' + title, fontsize=24)","edaed138":"umap_plot(mapper, train_y3, \"Total Base Value\")","c72aeb34":"umap_plot(mapper, train_y2, \"Total Segment Score\")","48231837":"umap_plot(mapper, train_y, \"GOE\")","ac77aa5f":"mapper2 = umap.UMAP(random_state=42, metric='hamming', n_neighbors=75, min_dist=.8).fit_transform(train_X_fixed.iloc[:, 4:28])","242e5980":"umap_plot(mapper2, train_y3, \"Total Base Value\")","97f5ef84":"umap_plot(mapper2, train_y2, \"Total Segment Score\")","754e906d":"umap_plot(mapper2, train_y, \"GOE\")","ca566af9":"train_y4.unique()","e92494fa":"region_dict={'Eastern_Great_Lakes':0, 'Central_Pacific':1, 'North_Atlantic':2,'Upper_Great_Lakes':3, 'Northwest_Pacific':4, \n             'Southwest_Pacific':5,'New_England':6, 'South_Atlantic':7}\n\nfig, ax = plt.subplots(figsize=(14,8))\nscatter= ax.scatter(\n    mapper2[:, 0],\n    mapper2[:, 1],\n    c=[sns.color_palette()[x] for x in train_y4.map(region_dict)], \n    cmap='Spectral', s=18)\nplt.gca().set_aspect('equal', 'datalim')\nplt.title('UMAP projection of the Skating dataset:  Region', fontsize=24)","401e552c":"union_mapper = mapper + mapper2\ncontrast_mapper = mapper - mapper2\n#intersection_mapper = mapper * mapper2","8717ac4d":"umap_plot(union_mapper, train_y3, \"Total Base Value\")","257ac08b":"umap_plot(union_mapper, train_y2, \"Total Segment Score\")","2e5f7736":"umap_plot(union_mapper, train_y, \"GOE\")","81504bbf":"umap_plot(contrast_mapper, train_y3, \"Total Base Value\")","ef19e404":"umap_plot(contrast_mapper, train_y2, \"Total Segment Score\")","14e9362d":"umap_plot(contrast_mapper, train_y, 'GOE')","85eacc48":"Y_list = ['Raw_GOE','Total_Segment_Score']\n\nrandom_effects_list = ['Event_Index', 'Official_Name', 'Skater_NameClub'] \n\ncategorical_vars=['Round' ,'Group' ,'Program_Type','Skater_Club','Region','Section','Official_Region','Official_Section'\n                 ,'Element_Category','I','Spin_Type', 'FreeSkate2ndHalfJump_Flag'\n                 ,'SoloJump_element','JumpCombo_element1','JumpCombo_element2','JumpCombo_element3']\n\nflag_vars=['JumpCombo_Flag','Spin_V_Flag','Unintended_SEQ_Flag','Intended_SEQ_Flag','REP_Flag','Fall_Indicator', 'TimeViolation_Flag']\n\nnumerical_vars=['Element_Number','Base_Value','Base_Value_cumsum','Bonus_Count','Downgrade_count','UnderRotate_count'\n                ,'Starred_count','JumpCombo_Count','Spin_Level_num','Step_Level'] \n\nvar_list = Y_list + random_effects_list + categorical_vars + flag_vars + numerical_vars + cumsum_list","335796ae":"cat_Technical_Scores = Technical_Scores[categorical_vars].fillna(value='Null')\nTechnical_Scores.drop(columns=categorical_vars, inplace=True)\nTechnical_Scores=pd.concat([Technical_Scores,cat_Technical_Scores],axis=1)","b5e55980":"test_bool = Technical_Scores['Region']== 'Southwestern'\npanel_bool = Technical_Scores['Judge']=='PanelScores_Technical'\nna_bool = ~(Technical_Scores['Raw_GOE'].isna())\n\ntrain = Technical_Scores[~test_bool & panel_bool & na_bool][var_list].reset_index(drop=True)\ntest = Technical_Scores[test_bool & panel_bool& na_bool][var_list].reset_index(drop=True)\n\ntrain_X = train.drop(columns=Y_list)\ntest_X = test.drop(columns=Y_list)","8e094eaa":"train_y = train['Raw_GOE']\ntest_y = test['Raw_GOE']\n\ntrain_y2 = train['Total_Segment_Score']\ntest_y2 = test['Total_Segment_Score']\n\ndel train, test","6502d0f7":"ohe_encoder = OneHotEncoder(variables=categorical_vars, drop_last=True)\n\ntrain_X = ohe_encoder.fit_transform(train_X)\ntest_X = ohe_encoder.transform(test_X)\n\n#ohe_encoder.encoder_dict_","ab9fecc9":"SS = StandardScaler()\n\nX_SS_train = SS.fit_transform(train_X[cumsum_list])\nX_SS_test = SS.transform(test_X[cumsum_list])","494c2f8c":"pca = PCA(n_components=0.9, random_state=42)","bb742c33":"PCA_train_X = pca.fit_transform(X_SS_train)\nPCA_test_X = pca.transform(X_SS_test)","6e5afaa1":"pca_col_names = ['PCA_' + str(i+1) for i in range(0,pca.n_components_)]\nPCA_train_X=pd.DataFrame(PCA_train_X,  columns=pca_col_names)\nPCA_test_X=pd.DataFrame(PCA_test_X,  columns=pca_col_names)","51e8c80e":"train_X = train_X.drop(columns=cumsum_list)\ntest_X = test_X.drop(columns=cumsum_list)\n\ntrain_X = pd.concat([train_X, PCA_train_X], axis=1)\ntest_X = pd.concat([test_X, PCA_test_X], axis=1)","04f24049":"del X_SS_train, X_SS_test, pca, PCA_train_X, PCA_test_X","4b6d7e99":"#break apart fixed and random effects\ntrain_X_fixed=train_X.drop(columns=random_effects_list)\ntrain_X_random=train_X[random_effects_list]\n\ntest_X_fixed=test_X.drop(columns=random_effects_list)\ntest_X_random=test_X[random_effects_list]","2de9c242":"pca_plot(train_X_fixed, train_y, 'GOE')","7f03554d":"pca_plot(train_X_fixed, train_y2, 'Total_Segment_Score')","85b72f4d":"#let's just take the first 20 components (which explain the most variance in the data anyways)\npca_list = ['PCA_' + str(i+1) for i in range(0,20)]\nscale_list = pca_list + numerical_vars","90604aba":"SS2 = StandardScaler()\n\ntrain_num = SS2.fit_transform(train_X[scale_list])\ntest_num = SS2.transform(test_X[scale_list])","72874269":"mapper = umap.UMAP(random_state=42, metric='euclidean', min_dist=0.15, n_neighbors=20).fit_transform(train_num)\n#just manually fiddled with some parameters for n_neighbors and min_dist -- I'm sure a more systematic approach is advisable \numap_plot(mapper, train_y, \"GOE\")","afa218db":"mapper = umap.UMAP(random_state=42, metric='minkowski', min_dist=0.15, n_neighbors=20).fit_transform(train_num)\n#just manually fiddled with some parameters for n_neighbors and min_dist -- I'm sure a more systematic approach is advisable \numap_plot(mapper, train_y, \"GOE\")","0bd88830":"mapper = umap.UMAP(random_state=42, metric='chebyshev', min_dist=0.15, n_neighbors=20).fit_transform(train_num)\n#just manually fiddled with some parameters for n_neighbors and min_dist -- I'm sure a more systematic approach is advisable \numap_plot(mapper, train_y, \"GOE\")","2a8d356d":"umap_plot(mapper, train_X['Downgrade_count'], \"Downgrade Count\")\numap_plot(mapper, train_X['Spin_Level_num'], \"Spin Level\")\numap_plot(mapper, train_X['Step_Level'], \"Step Level\")","a21671a2":"#put all columns names of our dataset in a list\ncols_list=train_X.columns.to_list()\n\n#put all pca column names in a list\n#https:\/\/stackoverflow.com\/questions\/27275236\/pandas-best-way-to-select-all-columns-whose-names-start-with-x\npca_col = [col for col in train_X if col.startswith('PCA')]\n\n#get rid of random effects, pca, numeric variables to just list the categorical vars\n#includes categorical variables that were OHE'd, and our flag variables\nohe_cat_var_list=list(set(cols_list) - set(pca_col ) - set(numerical_vars) - set(random_effects_list))","e2339cb7":"mapper2 = umap.UMAP(random_state=42, metric='dice', min_dist=.9).fit_transform(train_X[ohe_cat_var_list])\n#just manually fiddled with some parameters for n_neighbors and min_dist -- I'm sure a more systematic approach is advisable \numap_plot(mapper2, train_y, \"GOE\")","585f7d14":"mapper2 = umap.UMAP(random_state=42, metric='hamming', min_dist=.9).fit_transform(train_X[ohe_cat_var_list])\n#just manually fiddled with some parameters for n_neighbors and min_dist -- I'm sure a more systematic approach is advisable \numap_plot(mapper2, train_y, \"GOE\")","f564c7ed":"union_mapper = mapper + mapper2\ncontrast_mapper = mapper - mapper2\n#intersection_mapper = mapper * mapper2","77ab8402":"umap_plot(union_mapper, train_y, \"GOE\")","91a1419a":"umap_plot(contrast_mapper, train_y, \"GOE\")","667a3a62":"mapper = umap.UMAP(random_state=42, metric='chebyshev', min_dist=0.15, n_neighbors=20).fit(train_num)\nmapper_test = mapper.transform(test_num)\n\nmapper2 = umap.UMAP(random_state=42, metric='hamming', min_dist=.9).fit(train_X[ohe_cat_var_list])\nmapper2_test=mapper2.transform(test_X[ohe_cat_var_list])\n\nunion_mapper_test = mapper_test + mapper2_test\ncontrast_mapper_test = mapper_test - mapper2_test","75d2f0ef":"umap_plot(mapper_test, test_y, \"GOE\")","e34fa814":"umap_plot(mapper2_test, test_y, \"GOE\")","27ee5722":"umap_plot(union_mapper_test, test_y, \"GOE\")","64b0ea73":"umap_plot(contrast_mapper_test, test_y, \"GOE\")","5dd90503":"4) Next, we create a field to capture if any elements were bonused and remove the 'b1' code from the element. Double axels and triples were eligible in this event to receive bonuses, so long as not downgraded ('<<').  \n\n","71cd8361":"If the skater has a timing deduction, it would be likely that there could be a GOE penalty on the last element.  For example, consider a spin that goes past the end of the music so the skater can try to get a spin level.","f446db3a":"Here we will try to see if we can use an unsupervised approach, UMAP, for 'modeling' GOE.  This is done by using our design matrix and using the UMAP method as a dimension reduction technique, and scatter plotting the first 2 UMAP components with GOE as the color.  \n\nThe first set of UMAP analyses in this notebook is done on data that has been summarized\/aggregated, to the level of detail of 1 row per skater per event.  The second set of UMAP analyses is done on data that is 1 row per element per skater per event -- so, it is summarized to the official panel level, and is not as granular as 1 row per element per skater per event <i> per judge <\/i>.  ","8c7b75be":"Plotting the first 2 PCA values here we don't see a major pattern pop out for GOE but we do for Total Segment Score - which is similar to the the aggregated data flavor.  Again, this is not surprising given the data content.","760cdd85":"The USFS Section and Region definitions can be found in the USFS Rulebook (rule 1010). The Official Supplemental data was found by pivoting on the official city and states to see which were in states that may not have a 1-to-1 mapping to a USFS Region.  Then, I assigned each city\/state an \"Other_Boundary\" level, which can then facilitate a 1-to-1 join to the USFS_Sections_Regions data.\n\nFor the international judge living in Bristol, United Kingdom (i.e. the author, Katherine Werner), her Section and Region have been assigned to be \"International\".  In reality, her USFS club affiliation would land her in the Midwestern\/Upper Great Lakes region, however this data would not necesssarily be readily available for the casual data analyst and so has been kept more reflective of the data as sort of anomolous\/weird.","3e25d168":"## UMAP on Categorical Variables:  Aggregated Data","5f66a83e":"## Add Combined Event Totals (Short Program + Free Skate) ","97030f0e":"Nowadays, the technical panel will mark if an element had a fall in the \"I\" column using the code \"F\".  At the time of this competition series, the technical panel did not mark fall indicators for each element, but only tallied them in total in the deductions section.  An earlier draft of this project had poor model fits for GOE on validation.  I believe in large part this was due to not having falls per element coded.  In an attempt to reflect the current codification system and improve the modelling, I'm going to impute the falls by element. [In some sense this is cheating by using the target variable to assign the falls, but since the new coding system would have this, I feel it is \"fair\" for the purposes of this fun excercise.]\n\nTo do the imputation, I'll take the total fall deductions to get the # of falls.  I'll assume an element could at most accumulate 1 fall deduction.  I'll sort the overall GOEs and assign the falls to the ranked GOEs.  So, if a skater had 3 falls, then the elements with the 3 lowest GOEs will each get an imputed \"F\" code.\n\nSimilarly, it would be nice to impute 'q' codes, which are new codes the technical panel uses nowawadays, but that would be much hard to do.  However, given the improvement in the models when the imputed fall code was added, it seems reasonable to assume that if we had the q code, the GOE models would be more accurate as well.","21ce4f40":"## Add Event Details to Technical & Component DFs","4caf0e4c":"We now want to create a few modified verions of the \"Element\" field in the Technical Scores data.  This field contains many pieces of information we will want to interrogate separately. \n\n1)  We first remove the no value element asterisks and the jump error marks.  Asterisks indicate that a jump will not count - this could be the result of a number of things, such as too many repeats of this jump, occurring after a +SEQ incident (more on that later), or in short program not being of the required jump type.  A single carrot '<' indicates that the jump is underrotated by more than a 1\/4 rotation.  A double carrot '<<' indicates that that the jump is underrotated by more than a 1\/2 rotation.  An 'e' means that a Lutz or a Flip jump have taken off from an incorrect edge.  An exclamation point '!' indicates that a Lutz or Flip jump have taken off from an unclear edge.  A jump may have a rotational and an edge error (e.g. 2Fe<<).  A jump combination has each jumps errors itemised (e.g. 2Fe<<+2T<<).  <br><br>\n<i>Note that since this competition, there are revised error codes (such as landing on the quarter 'q', or 'F' for falls).  Revisions include not only additional codes but also the effects on the elemtn base values and GOEs.  This is not a focus for this exercise, but noting it for anyone reviewing more recent protocol sheets.<\/i>\n\n ","2da9f5db":"## Calculate Number of Falls","6d059489":"## UMAP on Top 20 PCA Variables:  Aggregated Data","cd2afc7e":"## One Hot Encoding ","f9d361fb":"5) Moving onto spins, the next adjustment removes the 'V' error code.  This is a technical error on the spin, and can be related to a few issues - number of revolutions in a position or the quality of the fly on flying spins, for example.  Without getting into too much detail, the 'V' is a technical error which may or may not result in a GOE reduction.\n\n","969bffcf":"7) Moving back to jumps, we create a flag to identify if the element is a jump combination, and if so whether it was a 2 or 3 jump combination.  From there, we create fields to identify which jump in the combination was first, second, or third.  \n\n","f1dd91cd":"## Union of UMAPS:  Aggregated Data ","e1254e05":"As we can see, looks like there is something going on in the technical scores because there is at least 1 skater iwth a 20 point difference in their calculated versus scraped scores.  \n\nTo isolate the issue, sort the data by the technical score differences to see how many records are impacted.","700ac3ae":"## Bag of Words","df581229":"Next, we create a bag of words representation of the elements.","374f95f8":"# Purpose","251dd4f1":"Use our Element_mod2 and Element_mod3 to get counts of each sub-element performed -- before and after any technical deductions applied.\n\n[This is slightly cheating in that I'm using the entire dataset for the bag of words, but it's not a huge leakage issue and can be pipelined more appropriately later once the proof of concept is established.]","6e65f17c":"3) Thirdly, we remove the +SEQ and +REP codes.  <br>\n\n+SEQ can have 2 different meanings depending on where in the element code it is situated.  If it is at the end of the element code, it means the skater has performed a jump combination where the last jump performed was an axel type jump.  It indicates that the skater had a weight transfer in order to step into the axel entry edge.  It does not deduct from the scores in any way.  If the +SEQ code apears sandwiched between 2 jump element codes, it means that the skater had to transfer their weight on their jump landing - e.g. a fall or step out - before attemping the next jump in the combination.  Any jumps performed after the +SEQ as part of this element will be asterisked and receive no value in the technical base value.  The error will also result in GOE deductions on judging side.\n\n+REP is a code where the application will depend on the event.  The code has to do with jump repetition, and penalizing where jumps are repeated as individual elements.  At the intermediate level, double and triple jumps may be repeated, but at least one of the repeated jumps must occur in a combination.  If both jump attempts are individual elements, then the +REP will reduce the base value:  original base value is multiplied by 70%.  If additional attempts of the same jump occur, these will be asterisked.  \n\nIn the Short Program, if the combination jump is not performed, a '+COMBO' is code is used to indicate that a second jump was not performed.  I have NOT removed +COMBO from the data as it represents a jump, it is just that the jump identification is missing\/null.\n\n","1abd1e1e":"## Further Technical 'Element' Feature Engineering","d07fc9f7":"## Split into Train and Test","9fe271ba":"Actually, we see top-to-bottom \/ left-to-right GOE relationship when transformed on test data for GOE when looking at the union map \/ contrast map.  In conclusion, even in an unsupervised approach we are able to detect some pattern in GOE from just 2 UMAP, that generalizes alright to unseen data.","880b9c9f":"2)  Secondly, we clean up the step sequence to remove the level.  And, we store the level of the step sequence in a new field.  \n\n","fa9a953a":"Don't really need this right now, but worth highlighting these variables are a little different in nature...","28ebe0bb":"## One Hot Encoding ","4a7638c9":"Looks like the issue is isolated to 1 skater.  A zero technical is basically impossible... so, I will remove this skater from the dataset.  (She did not make the final round so I can just remove all of her records.)","d08bc3dc":"Some weird stuff when scraped for a few skaters.","228f8121":"Before we do UMAP, let's just plot the first 2 PCA variables.  Our PCAs were constructed from bags of words on just the elements performed.  Each element performed has a specfic base value, so it makes sense that obvious structure shows up on the plot for Base Value.  And, the total segment score is largely influenced by the base value, and the base value is highly correlated with the components scores, so again a lot of structure is expected to be visible there.  We know from the EDA nontebook that harder elements don't correlate with higher (or lower) GOE, so we don't expect as much structure to be visible with respect to GOE. ","8f66c4c5":"# Import Libraries and Raw Data","ac9f6c25":"## Reshape Technical Scores and Component Scores","744f7031":"If we also plot spin level and step level, we can see where these variables are lighting up the plot.","0b7ebf14":"# UMAP Clustering:  Aggregated Data","1a144e85":"## Cross-Check Information Sourced from 2 Different Website Locations ","478afcda":"Just playing around with a few flavors of UMAPs and it seems like we might be able to tease out some relationship left-to-right with the UMAP unions, but it's not super obviously clustered.  Still this seems a lot better in terms of finding **something** in terms of relationships as compared to the earlier attempt using the aggregated data.","01c438ca":"## PCA for Cumulative Sum Columns","5cab8469":"## PCA for Cumulative Sum Columns","d1fe2470":"Let's see how it visualises on unseen data...","d06d81fe":"For fun, will try a few different metrics.","45c557c4":"#  Background and Disclaimers","58111221":"## Use Supplemental Data to obtain Officials' Region and Section","e65e6c43":"## UMAP Numerical Variables:  Elementwise Data","229400d7":"# Preprocessing:  1 Row per Skater & Event","20594e22":"The author used data scraped from the web which was subsequently saved in csv files, which are read in below into a few different data frames.  See https:\/\/www.kaggle.com\/katiewerner\/us-figure-skating-data-beautiful-soup for more details.<br>\n\nThe protocols are reflective of the rules in effect at the time of the competitions. Scales of values used this season can be found here:  <br>\n&nbsp;&nbsp;&nbsp;&nbsp;ISU 2253 S&P SOV 2019-20.pdf (usfigureskating.org) <br>\n&nbsp;&nbsp;&nbsp;&nbsp;https:\/\/www.isu.org\/inside-isu\/isu-communications\/communications\/21210-2253-s-p-sov-2019-20\/file <br><br>\nA copy of the US Figure Skating Rulebook in effect at the time can be found here: <br>\n&nbsp;&nbsp;&nbsp;&nbsp;    https:\/\/scboston.org\/wp-content\/uploads\/2019\/10\/2019-20-U.S.-Figure-Skating-Rulebook.pdf <br>\nRules of interest include:  <br>\n&nbsp;&nbsp;&nbsp;&nbsp;\u2022 6440\/6441\/6442 for Intermediate Level Required Elements, <br>\n&nbsp;&nbsp;&nbsp;&nbsp;\u2022 1070\/1071\/1072\/1073 for IJS scoring rules, <br>\n&nbsp;&nbsp;&nbsp;&nbsp;\u2022 1010\/1011\/1012\/1013\/1014 for Geographic Regions\/Sections <br>\nPlease note that rules of sport frequently change and the rules in effect at the time may no longer necessarily effective.\n\n<b>Disclaimers and Caveats:<\/b><br>\nThe author has created this notebook as a fun data project, being data she is intimately familiar and fascinated with the creation of -- indeed, she helped create this data back in October 2019 by serving as a judge one of these regional competitions!\n\nThis notebook is strictly intended to serve as an example to demonstrate the author's approach to working with data and to be a fun way to share her passion for the sport. It is ABSOLUTELY NOT intended to be used to draw any conclusions about the sport of figure skating, the judges, the technical officials, the skaters, the coaches, the skating clubs, US Figure Skating, any region or section of US Figure Skating, etc.","4e9e7754":"Earlier I have popped off the cumsum_list the Base_Value_cumsum, since I wanted to use this as as color-coding label later on.","888e05d5":"# UMAP Clustering:  Elementwise Data","8a0175ce":"The test set will serve as a final validation at the very end.  I've chosen to hold out the Southwestern region events (last region in the dataset).\n\nThe enoder I am using does not like np.nan values so I replace the values with text.  I also kept some cleanup that other models like.","91240144":"For fun, let's try a few different metrics.","198ed9cc":"8) Next, we create a field to identify if the element is a solo jump.  ","0366059b":"6) Then, we create fields to identify the type of spin and the level of the spin.\n\n","cea559bd":"## UMAPs on Test Data:  Elementwise Data","1f9c3644":"The event details website stores the total segment scores, total element scores, total component scores, and total deductions.  However, the itemized element scores and itemized component scores are also sourced from the protocol websites.  So, this step essentially aggregates the individual element and component scores to cross-check against the data scraped from the event detail website.  These should be pretty consistent, but it is possible for mismatches.","9a4f50d2":"I believe the data was over-summarized above for attempting to find clusters that relfect GOE, so let's go back to 1 row per element performed.  We'll still keep this to the overall panel figures, rather than individual judge scores.  The GOEs here for labelling will be the average of the panel's raw GOEs, excluding high\/low (before being applied as a % of base value).","d179a264":"# Preprocessing:  Each Row is an Element","067b0aa4":"## Remove Erroneous \"I\" Values","201d4d9e":"In the end, I guess I'm not particularly wowed by anything I've done here.  I have 3 thoughts one this:  <br>\n1) This is probably in large part a result of being a neophyte with UMAP, and would welcome constructive suggestions, <br>\n2) I think another driving factor is the data.  The GOEs being used as the label are aggregated across all skated elements, and also are based on the % of base value rather than raw -5, -4, ..., +4, +5 values, so it seems like we're trying to squeeze lemondate out of something that no longer remotely resembles a lemon. <br>\n3) Also on the data front, we crammed individual element details like jump downgrade and spin levels into the PCA computations - might have been trying to compress too much into an aggregated view that is better served at a more granular level (element level).","a94ec46b":"## Add a Timing Flag to Last Element if a Time Deduction ","6250062f":"## Separate Fixed and Random Effects ","68177da1":"## UMAP Categorical Variables:  Elementwise Data","6d720cff":"It isn't surprising that these clusters seem kind of unrelated to the scores as the variables were more generic like region or official region.  Nothing really to differentiate the skating performed.  Let's try to plot region that as the color to see if this hunch is correct.  [Yes, looks pretty correct.]","2abae76b":"## Remove Withdrawn Skaters, Append Event & Official Details","e6c1cd31":"## Split into Train and Test","60d630e2":"## Impute Fall Code by Element ","ec4f9112":"# Initial Data Prep","df6b3971":"## Separate Fixed and Random Effects  ","22d5bedc":"We will not want to use the same metric for numerical variables as categorical variables in UMAP.  To start off, let's just take the first 20 PCA variables we created to see if we can get some pattern via UMAP.","e90006c4":"## Union of UMAPS:  Elementwise Data"}}