{"cell_type":{"64aaf597":"code","e27ebcd2":"code","8b70c961":"code","9409190a":"code","f42e7864":"code","c9f0c797":"code","775e3b0a":"code","3316aceb":"code","c6c47f38":"code","b2fe1c09":"code","2cae587f":"code","b7b99abd":"code","09b63c87":"code","2a086d2b":"code","590db165":"code","6f5106f4":"code","72c814f8":"code","69092adb":"code","46e7a8de":"code","e1f0a188":"code","264b6a66":"code","00713a98":"code","c1b8ff5b":"code","d99c04d8":"code","bbee0565":"code","1708ec17":"code","adbc8e4f":"code","83c5cb3b":"code","770108f8":"code","e24c98d4":"code","de10fd8d":"code","5a037b6e":"code","f11256f4":"code","b0101ae9":"code","f157b818":"code","758a8acd":"code","96432e61":"code","01903696":"code","9e2c9478":"code","aadfdf85":"code","81296ac5":"code","db4189b4":"code","f8b26ee0":"code","b7aa948a":"code","88318872":"code","eac1c75a":"code","a5c1ab91":"code","172f728d":"code","59a96444":"code","871c7ec7":"code","362609ca":"code","20e27174":"code","6c85c03e":"code","ad85edc0":"code","5d0cecad":"code","f82246c0":"code","7376ec48":"code","f0d33b97":"code","b47150e5":"markdown","c5314e2b":"markdown","89b04095":"markdown","3dd27535":"markdown","cf7dbca7":"markdown","51ff8402":"markdown","a6a93927":"markdown","0e56103b":"markdown","b2edf407":"markdown","072109ab":"markdown","5bfecd30":"markdown","a8edc213":"markdown","1c8440b5":"markdown","b5f2b1d2":"markdown","0284be1d":"markdown","671ec9a3":"markdown","5f89d66f":"markdown","3aa28069":"markdown","c98e8acd":"markdown","de5d48b7":"markdown","a426fea8":"markdown","84dfba36":"markdown","e014bc86":"markdown"},"source":{"64aaf597":"import time\n\n#Analysis\nimport math\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\n#Visualization\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\nimport mpl_toolkits.mplot3d.axes3d as p3\n\n#Processing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import KNNImputer\n\n#Modelling\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.neighbors import kneighbors_graph","e27ebcd2":"#Configuration\npd.set_option('max_columns',50)","8b70c961":"df = pd.read_csv('..\/input\/customer-personality-analysis\/marketing_campaign.csv',sep=\"\\t\")","9409190a":"cat_cols = ['Education','Marital_Status','Complain','AcceptedCmp1','AcceptedCmp2',\n            'AcceptedCmp3','AcceptedCmp4','AcceptedCmp5','Response']\n\ndate_cols = ['Year_Birth','Dt_Customer']\n\nnum_cols = ['Year_Birth','Income','Kidhome','Teenhome','Recency','MntWines','MntFruits','MntMeatProducts','MntFishProducts',\n            'MntSweetProducts','MntGoldProds','NumDealsPurchases','NumWebPurchases','NumCatalogPurchases',\n            'NumStorePurchases','NumWebVisitsMonth']","f42e7864":"df.head()","c9f0c797":"df.info()","775e3b0a":"#type conversion\ndf['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'],format='%d-%m-%Y')","3316aceb":"df.describe()","c6c47f38":"df[num_cols].skew()","b2fe1c09":"#NaN values show\ndf[df['Income'].isna()]","2cae587f":"#duplicated rows\ndf.loc[df.duplicated()]","b7b99abd":"df.nunique()","09b63c87":"#unique values show \nfor col_name in df.columns:\n    if df[col_name].dtype == \"object\":\n        print(df[col_name].unique())","2a086d2b":"#get sum as total spend\ndf[\"Total_Spend\"] = df[\"MntWines\"] + df[\"MntFruits\"] + df[\"MntMeatProducts\"] +\\\n                        df[\"MntFishProducts\"] + df[\"MntSweetProducts\"] + df['MntGoldProds']\n#add it to num_cols\nnum_cols.append('Total_Spend')","590db165":"#get sum as total AcceptedCmp\ndf[\"Total_AcceptedCmp\"] = df[\"AcceptedCmp1\"] + df[\"AcceptedCmp2\"] + df[\"AcceptedCmp3\"] +\\\n                        df[\"AcceptedCmp4\"] + df[\"AcceptedCmp5\"]\n\n#add it to num_cols\nnum_cols.append('Total_AcceptedCmp')","6f5106f4":"#get sum as total AcceptedCmp\ndf[\"Total_NumWaysPurchases\"] = df['NumWebPurchases'] + df['NumCatalogPurchases'] + df['NumStorePurchases'] \n\n#add it to num_cols\nnum_cols.append('Total_NumWaysPurchases')","72c814f8":"def set_xticklabels_rotation(ax,rotation = 20):\n    \n    for label in ax.get_xticklabels():\n        label.set_rotation(rotation)\n        label.set_horizontalalignment('right')","69092adb":"_, ax1 = plt.subplots(1,2, figsize=(13,5))\nplt.suptitle('Histograms of date columns')\n\nfor i, col in enumerate(date_cols):\n    sns.histplot(x=col, data=df, ax=ax1[i])\n    set_xticklabels_rotation(ax1[i])\n    \nplt.show()","46e7a8de":"_, ax1 = plt.subplots(2,5, figsize=(25,10))\nplt.suptitle('Histograms of catagory columns')\n\nfor i, col in enumerate(cat_cols):\n    sns.histplot(x=col, data=df, ax=ax1[i\/\/5, i%5],discrete=True)\n    set_xticklabels_rotation(ax1[i\/\/5, i%5])\n\nax1[1,4].set_visible(False)\nplt.show()","e1f0a188":"_, ax1 = plt.subplots(4,5, figsize=(25,18))\nplt.suptitle('Histograms of numerial columns')\n\nfor i, col in enumerate(num_cols):\n    sns.histplot(x=col, data=df, kde=True, ax=ax1[i\/\/5, i%5])\n    set_xticklabels_rotation(ax1[i\/\/5, i%5])\n    \nplt.show()","264b6a66":"def num_plot(df, col):\n    fig = px.histogram(df, x=col, marginal=\"box\")\n    fig.update_layout(height=300, width=500, showlegend=True)\n    fig.update_traces(marker_line_width=1,marker_line_color=\"black\")\n    fig.show()\n    \nnum_plot(df,'Income')","00713a98":"df.loc[df.Income > 600000]","c1b8ff5b":"df.loc[df.Income > 600000, 'Income'] = np.nan","d99c04d8":"num_plot(df,'Year_Birth')","bbee0565":"df.loc[df['Year_Birth']<1940]","1708ec17":"df.loc[df['Year_Birth']<1940, 'Year_Birth'] = np.nan","adbc8e4f":"df_pcs = pd.get_dummies(df, drop_first=True)\ndf_pcs.head()","83c5cb3b":"#NaN values of Income's Index\nnan_index_Income = df[df_pcs['Income'].isna()].index\nnan_index_Year_Birth = df[df_pcs['Year_Birth'].isna()].index\n#Drop useless cols\ndf_pcs = df_pcs.drop(['Dt_Customer','Z_CostContact','Z_Revenue','ID'],axis=1)","770108f8":"#StandardScaler\nscaler = StandardScaler()\nscaler_pcs = scaler.fit_transform(df_pcs)\nscaler_df_pcs = pd.DataFrame(scaler_pcs,columns=df_pcs.columns)\n#set nan values after scaler\nscaler_df_pcs['Income'][nan_index_Income] = np.nan\nscaler_df_pcs['Income'][nan_index_Year_Birth] = np.nan","e24c98d4":"#KNNImputer\nimputer = KNNImputer(n_neighbors=8)\nimputed_pcs = imputer.fit_transform(scaler_df_pcs) ","de10fd8d":"#Standardscaler inversing\ndf_pcs = pd.DataFrame(scaler.inverse_transform(imputed_pcs),\n                    columns=df_pcs.columns).round(1)","5a037b6e":"print('original: mean:{}, var:{}'.format(df['Income'].mean(),df['Income'].var()))\nprint('after imputation: mean:{}, var:{}'.format(df_pcs['Income'].mean(),df_pcs['Income'].var()))\nprint('imputation data: mean:{}'.format(df_pcs.loc[nan_index_Income,'Income'].mean()))\nprint(stats.kstest(df['Income'],df_pcs['Income']))","f11256f4":"num_plot(df,'Income')","b0101ae9":"num_plot(df_pcs,'Income')","f157b818":"stats.probplot(df_pcs['Income'].sort_values(ascending=True), dist='norm', plot=plt)\nplt.show()","758a8acd":"print('original: mean:{}, var:{}'.format(df['Year_Birth'].mean(),df['Year_Birth'].var()))\nprint('after imputation: mean:{}, var:{}'.format(df_pcs['Year_Birth'].mean(),df_pcs['Year_Birth'].var()))\nprint('imputation data: mean:{}'.format(df_pcs.loc[nan_index_Year_Birth,'Year_Birth'].mean()))\nprint(stats.kstest(df['Year_Birth'],df_pcs['Year_Birth']))","96432e61":"num_plot(df,'Year_Birth')","01903696":"num_plot(df_pcs,'Year_Birth')","9e2c9478":"stats.probplot(df_pcs['Year_Birth'].sort_values(ascending=True), dist='norm', plot=plt)\nplt.show()","aadfdf85":"df['Income'] = df_pcs['Income']\ndf['Year_Birth'] = df_pcs['Year_Birth']\ndf.isna().sum()","81296ac5":"_, ax2 = plt.subplots(figsize=(15, 12))\nsns.heatmap(df[num_cols].corr(), annot=True, linewidths=.5, ax=ax2)","db4189b4":"sns.scatterplot(data=df, x=\"Total_Spend\", y=\"Total_NumWaysPurchases\")","f8b26ee0":"sns.scatterplot(data=df, x=\"Income\", y=\"Total_NumWaysPurchases\")","b7aa948a":"sns.scatterplot(data=df, x=\"Income\", y=\"Total_Spend\")","88318872":"x = df[\"Total_NumWaysPurchases\"]\ny = df[\"Income\"]\nz = df[\"Total_Spend\"]\n\nfig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(111, projection=\"3d\")\nax.scatter(x, y, z, c=\"maroon\", marker=\"o\")\nplt.show()","eac1c75a":"X = pd.concat(\n    [\n        (df[\"Total_NumWaysPurchases\"]-df[\"Total_NumWaysPurchases\"].mean())\/df[\"Total_NumWaysPurchases\"].std(),\n        (df[\"Income\"]-df[\"Income\"].mean())\/df[\"Income\"].std(),\n        (df[\"Total_Spend\"]-df[\"Total_Spend\"].mean())\/df[\"Total_Spend\"].std()\n    ],axis=1)","a5c1ab91":"X_original = pd.concat(\n    [\n        x,\n        y,\n        z\n    ],axis=1)","172f728d":"connectivity = kneighbors_graph(X, n_neighbors=4, include_self=True)\n\nward = AgglomerativeClustering(\n    n_clusters=12, connectivity=connectivity, linkage=\"ward\",\n).fit(X)\n\nlabel = ward.labels_\n\nfig = px.scatter_3d(X, x='Total_NumWaysPurchases', y='Income', z='Total_Spend',\n              color=label)\nfig.show()","59a96444":"label_selected_tag = label.copy()\nlabel_selected_tag[label==0] = 1\nlabel_selected_tag[(label == 4)|(label == 7)] = 2\nlabel_selected_tag[(label == 3)|(label == 2)|(label == 1)] = 3\nlabel_selected_tag[(label == 8)|(label == 11)|(label == 5)|(label == 9)] = 4\n\nX_original = pd.concat(\n    [\n        X_original,\n        pd.Series(label_selected_tag,name=\"Cluster\")\n    ],axis=1)\n\nfig = px.scatter_3d(X_original, x='Total_NumWaysPurchases', y='Income', z='Total_Spend',\n              color=label_selected_tag)\nfig.show()","871c7ec7":"sns.violinplot(data=X_original, x=\"Cluster\", y=\"Total_Spend\",\n               split=True, palette=\"Set3\",linewidth=0.5)","362609ca":"sns.violinplot(data=X_original, x=\"Cluster\", y=\"Total_NumWaysPurchases\",\n               split=True, palette=\"Set3\",linewidth=0.5)","20e27174":"sns.violinplot(data=X_original, x=\"Cluster\", y=\"Income\",\n               split=True, palette=\"Set3\",linewidth=0.5)","6c85c03e":"sns.pairplot(X_original, hue=\"Cluster\")","ad85edc0":"df_withlabel = pd.concat(\n    [\n        df,\n        pd.Series(label_selected_tag,name=\"Cluster\")\n    ],axis=1)","5d0cecad":"_, ax = plt.subplots(figsize=(20, 5))\n\nax = sns.violinplot(x=\"Total_AcceptedCmp\", y=\"Income\", hue=\"Cluster\",\n                 data=df_withlabel, palette=\"Set3\",linewidth=0.5)\n\nax = sns.jointplot(data=df_withlabel, x=\"Total_AcceptedCmp\", y=\"Income\", hue=\"Cluster\")","f82246c0":"ax = sns.jointplot(data=df_withlabel, x=\"Total_AcceptedCmp\", y=\"Year_Birth\", hue=\"Cluster\")\n\nax = sns.jointplot(data=df_withlabel, x=\"Total_Spend\", y=\"Year_Birth\", hue=\"Cluster\")","7376ec48":"_, ax1 = plt.subplots(2,5, figsize=(25,20))\nplt.suptitle('Histograms of catagory columns')\n\nfor i, col in enumerate(cat_cols):\n    sns.histplot(x=col, data=df_withlabel, ax=ax1[i\/\/5, i%5],discrete=True,hue='Cluster')\n    set_xticklabels_rotation(ax1[i\/\/5, i%5])\n\nax1[1,4].set_visible(False)\nplt.show()","f0d33b97":"_, ax1 = plt.subplots(4,5, figsize=(25,18))\nplt.suptitle('Histograms of numerial columns')\n\nfor i, col in enumerate(num_cols):\n    sns.histplot(x=col, data=df_withlabel, kde=True, ax=ax1[i\/\/5, i%5],hue='Cluster')\n    set_xticklabels_rotation(ax1[i\/\/5, i%5])\n    \nplt.show()","b47150e5":"### If you like this analysis, please leave your UPVOTE.\n### THANKS and Having a good day!","c5314e2b":"# 2. Data Exploring","89b04095":"### Conclusion:\n\n**Conclusion:** we could consider that our most ideal customers are in cluster 1 and 2, The second are 3.\n\n\nto be continued...","3dd27535":"**Attention:** There is a 666.666k record, it looks like a joke input, we could consider it as wrong input, making it as nan","cf7dbca7":"## 2.2 Data Processing","51ff8402":"**Attention:** We could pay more attention in NumWebPurchases and NumWebVisitsMonth. Cluster 1 and 2 are more like purchasing on website. Their number of visiting web is low. We guess ideal customers buy thing on website more impulsively","a6a93927":"**Attention:** 24 records of NaN in Income, almost 1%, are replace by mean, median, mode or number by using special method, it depends on what we find in data exploring","0e56103b":"**Attention:** Income has positive correlations with most of columns which are Mnt or Num prefix , KidHome and NumWebVisitsMonth have nagative correlations with most of columns. Recency is a noteworthy feature which has uncorrelation with other feature in num_cols. we could check the distribution of Recency with different catogory columns on the next step. It's a funny thing about NumWebVisitsMonth has negative correlations with most of columns, it reflect that the more people visit, the less they pay??? \n\n**That's a amazing thing that Income has a strong correlation with Total_Spend. The score is almost 0.8. Total Spend has a strong correlation with Total NumWaysPurchases**\n\n**Let's focus on target. What we really want is knowing clusters about summarize customers, is knowing who is the ideal customer. We need to find out who have power of purchasing so that we can reduce cost with accurate advertising and find out different customers adopt different strategies.**\n\nwho is our ideal customer, I think that we could use **Total_Spend, Total_AcceptedCmp, Total_NumWaysPurchases and NumDealsPurchases** as first grade indicators, and **NumWebVisitsMonth, Response and NumDealsPurchases** as second grade indicators, those indicators reflect people's power of purchasing.","b2edf407":"**Attention:** we found that Z_CostContact and Z_Revenue both are single catagory, so we can drop them in data processing","072109ab":"### Attributes\n\n**People**\n- ID: Customer's unique identifier\n- Year_Birth: Customer's birth year\n- Education: Customer's education level\n- Marital_Status: Customer's marital status\n- Income: Customer's yearly household income\n- Kidhome: Number of children in customer's household\n- Teenhome: Number of teenagers in customer's household\n- Dt_Customer: Date of customer's enrollment with the company\n- Recency: Number of days since customer's last purchase\n- Complain: 1 if the customer complained in the last 2 years, 0 otherwise\n\n**Products**\n- MntWines: Amount spent on wine in last 2 years\n- MntFruits: Amount spent on fruits in last 2 years\n- MntMeatProducts: Amount spent on meat in last 2 years\n- MntFishProducts: Amount spent on fish in last 2 years\n- MntSweetProducts: Amount spent on sweets in last 2 years\n- MntGoldProds: Amount spent on gold in last 2 years\n\n**Promotion**\n- NumDealsPurchases: Number of purchases made with a discount\n- AcceptedCmp1: 1 if customer accepted the offer in the 1st campaign, 0 otherwise\n- AcceptedCmp2: 1 if customer accepted the offer in the 2nd campaign, 0 otherwise\n- AcceptedCmp3: 1 if customer accepted the offer in the 3rd campaign, 0 otherwise\n- AcceptedCmp4: 1 if customer accepted the offer in the 4th campaign, 0 otherwise\n- AcceptedCmp5: 1 if customer accepted the offer in the 5th campaign, 0 otherwise\n- Response: 1 if customer accepted the offer in the last campaign, 0 otherwise\n\n**Place**\n- NumWebPurchases: Number of purchases made through the company\u2019s website\n- NumCatalogPurchases: Number of purchases made using a catalogue\n- NumStorePurchases: Number of purchases made directly in stores\n- NumWebVisitsMonth: Number of visits to company\u2019s website in the last month","5bfecd30":"**Year_Birth looks normal distribution in different dimensions**","a8edc213":"#### Generate Summary Index","1c8440b5":"## 2.3 Data Exploring","b5f2b1d2":"### Target\n\nNeed to perform clustering to summarize customer segments.","0284be1d":"**Attention:** the skewnesses of num_cols are mostly positive skewness. Recency looks like normal or evenly.","671ec9a3":"## 2.1 Data Overview","5f89d66f":"**The larger Total AcceptedCmp, the higer proportion of Cluster 1 and 2.** Cluster 1 and 2 are the most ideal customers, 3 and 4 are the second. Ideal customers are mostly in the interval of 75000-100000","3aa28069":"**Attention:** There are three records,  we could consider it as wrong input, making it as nan","c98e8acd":"# 1. Libraries Importing and Data Loading","de5d48b7":"Now we finished NaN values imputation, data cleaning part, let's exploring","a426fea8":"## Target and Content","84dfba36":"**Attention:** As shown above, maybe the measures of two distributions are different. But it doesn't matter. we couldn't reject the null hypothesis after KS-test, and we have to consider that we could use the Income after imputation to data exploring, even if this statement got some wrong. Generally we consider it is ok when replace df'Income' with df_pcs'Income'  ","e014bc86":"### 2.2.1 Data Cleaning"}}