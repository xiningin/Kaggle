{"cell_type":{"9e0d3c36":"code","658bd3b2":"code","5fae3148":"code","b3c3587c":"code","8a4047e7":"code","61cf6515":"code","d77a4266":"code","8cefaab7":"code","af42748e":"code","8de286fb":"code","b59be33f":"code","e6f8ecea":"code","957b8a3d":"code","8f61a73e":"code","260ee1ca":"code","e7532137":"code","59969553":"code","1aa8de35":"code","801da90c":"code","8e5f79eb":"code","4ecb361d":"code","014bb522":"code","fd4409a6":"markdown","6d95314d":"markdown","7bb6c9b6":"markdown","2fc78720":"markdown","474cebcb":"markdown","52ca605b":"markdown","2b958b83":"markdown","3d1666f5":"markdown","bbdd682b":"markdown","6f2929ee":"markdown","44e3bead":"markdown","f68fb0e5":"markdown","edea26d3":"markdown"},"source":{"9e0d3c36":"import pandas as pd \nimport numpy as np\n","658bd3b2":"data = pd.read_csv(\"..\/input\/Iris.csv\")","5fae3148":"data.head()","b3c3587c":"data.drop([\"Id\"],axis=1)","8a4047e7":"data.info()","61cf6515":"y=data.Species.values\nx_data=data.drop([\"Species\"],axis=1).values\n","d77a4266":"x=(x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))","8cefaab7":"from sklearn.model_selection import train_test_split \nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=1)","af42748e":"# NN- algorithm\nfrom sklearn.neighbors import KNeighborsClassifier\nknn =KNeighborsClassifier(n_neighbors =7)\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)","8de286fb":"print(\"{} nn score:{}\".format(7,knn.score(x_test,y_test)))","b59be33f":"import matplotlib.pyplot as plt\nscore_list=[]\nfor each in range(1,30):\n    knn2=KNeighborsClassifier(n_neighbors=each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\nplt.plot(range(1,30),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel=(\"accuracy\")\nplt.show()","e6f8ecea":"import pandas as pd \nimport numpy as np \n\ndata = pd.read_csv(\"..\/input\/Iris.csv\")\n\ndata.info()","957b8a3d":"data.drop(\"Id\",axis=1)","8f61a73e":"x_data = data.iloc[:, :-1].values\ny = data.iloc[:, -1].values","260ee1ca":"x=(x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))","e7532137":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=100)","59969553":"# importing cross_validation functions\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report","1aa8de35":"# Training on linear model\n\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression()\nclassifier.fit(x_train,y_train)\ny_pred = classifier.predict(x_test)\n\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nfrom sklearn.metrics import accuracy_score\nprint('accuracy is',accuracy_score(y_pred,y_test))\ny_true = y_test\n# confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n\n\n#  cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.show()","801da90c":"from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(x_train, y_train)\n\ny_pred = classifier.predict(x_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nfrom sklearn.metrics import accuracy_score\nprint('accuracy is',accuracy_score(y_pred,y_test))\n\ny_true = y_test\n#%% confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n\n\n# %% cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.show()","8e5f79eb":"from sklearn.svm import SVC\n\nclassifier = SVC()\nclassifier.fit(x_train, y_train)\n\ny_pred = classifier.predict(x_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nfrom sklearn.metrics import accuracy_score\nprint('accuracy is',accuracy_score(y_pred,y_test))\n\ny_true = y_test\n#confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n\n\n#  cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.show()","4ecb361d":"from sklearn.tree import DecisionTreeClassifier\ndt=DecisionTreeClassifier()\ndt.fit(x_train,y_train)\nprint(\"score\",dt.score(x_test,y_test))\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n\n","014bb522":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators = 100,random_state = 1)\nrf.fit(x_train,y_train)\nprint(\"random forest algo result: \",rf.score(x_test,y_test))\n\n\ny_pred = rf.predict(x_test)\ny_true = y_test\n# confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n\n\n#  cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.show()","fd4409a6":"# Now , we make normalization to bring data between 1 and 0 do\n# normalization formula is (x - min(x))\/(max(x)-min(x))","6d95314d":"# Now, we examine Naive bayas classification","7bb6c9b6":"# Random Forest ","2fc78720":"# CONCLUSION\n- Except of logistic regression, all classification algorithms  have  accuracy of 1.0 meaning all test data ","474cebcb":"# this step is train and test split","52ca605b":"Train and tet split","2b958b83":"# training on support vector machine\n","3d1666f5":"# Decision Tree's\n","bbdd682b":"Id is unnecessary  column. Therefore, we must drop from iris dataset","6f2929ee":"Normalization of data","44e3bead":"first step is read csv file using pandas \nsecond step is analyse data  and  droping unnecessary columns ","f68fb0e5":"# KNN algorithm","edea26d3":"# Now , we will examine the dataset using logistic regression methods"}}