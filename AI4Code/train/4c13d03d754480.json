{"cell_type":{"33e92c1a":"code","062fdfaf":"code","dc69d413":"code","34ad06b0":"code","3160edf9":"code","7111c03c":"code","06b0ac95":"code","c041fb40":"code","970d1b66":"code","6b5905c2":"code","eb3328c5":"code","58f94854":"code","97d71d10":"code","becdd526":"code","eb8a74bf":"code","676dbe1c":"code","97d529c6":"code","c760e71e":"code","6e7d1c27":"code","fa14501a":"code","a6276f56":"code","c7f19f08":"code","876cf284":"code","a56aba4a":"code","64ee3499":"code","f823f35a":"code","004f5c6d":"code","cd718228":"code","909233db":"code","d01e3929":"code","c09b6007":"code","973ddcae":"code","9f5b5232":"code","e4eed70f":"code","857bfb30":"code","b76f8756":"code","b252721d":"code","66652389":"code","00216399":"code","930952e5":"code","3d640de0":"code","e8480009":"code","c7c18ed2":"code","c8761094":"code","dcc7e4a4":"code","149e51b0":"code","ab4bce67":"code","ffa10742":"code","fd150fd1":"code","34c78cdb":"code","dd81dd6a":"markdown","b50b37c7":"markdown","4a525fc4":"markdown","cee5650c":"markdown","d5437721":"markdown","669dafcd":"markdown","16943be5":"markdown","1726af8a":"markdown","c76dda9f":"markdown","1f352c3c":"markdown","d59342e6":"markdown","35e5861e":"markdown","3d65b099":"markdown","698035fe":"markdown","8d32d17b":"markdown","acb66e8f":"markdown","65a54eae":"markdown","eba64ac6":"markdown","bf7fc039":"markdown","463391c5":"markdown","7ddc0c6e":"markdown","30544562":"markdown","fb31918d":"markdown","a97edc0c":"markdown","9d5475ca":"markdown","55e417a0":"markdown","6ff4762d":"markdown","761f66ea":"markdown","bf4c1a2a":"markdown","3f36f53c":"markdown","eb0463f6":"markdown","c76af8de":"markdown","6e186775":"markdown"},"source":{"33e92c1a":"# this may need to be installed separately with\n# !pip install category-encoders\nimport category_encoders as ce\n\n# python general\nimport pandas as pd\nimport numpy as np\nfrom collections import OrderedDict\n\n#scikit learn\n\nimport sklearn\nfrom sklearn.base import clone\n\n# model selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\n# ML models\nfrom sklearn import tree\nfrom sklearn.linear_model import LinearRegression \nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n# error metrics\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, median_absolute_error\n\n# plotting and display\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib as mpl\n\nfrom IPython.display import display\npd.options.display.max_columns = None\n\n# widgets and widgets based libraries\nimport ipywidgets as widgets\nfrom ipywidgets import interact, interactive","062fdfaf":"def rmse(y_true, y_pred):\n    res = np.sqrt(((y_true - y_pred) ** 2).mean())\n    return res\n\ndef mape(y_true, y_pred):\n    y_val = np.maximum(np.array(y_true), 1e-8)\n    return (np.abs(y_true -y_pred)\/y_val).mean()","dc69d413":"metrics_dict_res = OrderedDict([\n            ('mean_absolute_error', mean_absolute_error),\n            ('median_absolute_error', median_absolute_error),\n            ('root_mean_squared_error', rmse),\n            ('mean abs perc error', mape)\n            ])","34ad06b0":"def regression_metrics_yin(y_train, y_train_pred, y_test, y_test_pred,\n                           metrics_dict, format_digits=None):\n    df_results = pd.DataFrame()\n    for metric, v in metrics_dict.items():\n        df_results.at[metric, 'train'] = v(y_train, y_train_pred)\n        df_results.at[metric, 'test'] = v(y_test, y_test_pred)\n\n    if format_digits is not None:\n        df_results = df_results.applymap(('{:,.%df}' % format_digits).format)\n\n    return df_results","3160edf9":"def describe_col(df, col):\n    display(df[col].describe())\n\ndef val_count(df, col):\n    display(df[col].value_counts())\n\ndef show_values(df, col):\n    print(\"Number of unique values:\", len(df[col].unique()))\n    return display(df[col].value_counts(dropna=False))","7111c03c":"def plot_distribution(df, col, bins=100, figsize=None, xlim=None, font=None, histtype='step'):\n    if font is not None:\n        mpl.rc('font', **font)\n\n    if figsize is not None:\n        plt.figure(figsize=figsize)\n    else:\n        plt.figure(figsize=(10, 6))\n    dev = df[col]    \n    dev.plot(kind='hist', bins=bins, density=True, histtype=histtype, color='b', lw=2,alpha=0.99)\n    print('mean:', dev.mean())\n    print('median:', dev.median())\n    if xlim is not None:\n        plt.xlim(xlim)\n    return plt.gca()","06b0ac95":"def plot_feature_importances(model, feature_names=None, n_features=20):\n    if feature_names is None:\n        feature_names = range(n_features)\n    \n    importances = model.feature_importances_\n    importances_rescaled = 100 * (importances \/ importances.max())\n    xlabel = \"Relative importance\"\n\n    sorted_idx = np.argsort(-importances_rescaled)\n\n    names_sorted = [feature_names[k] for k in sorted_idx]\n    importances_sorted = [importances_rescaled[k] for k in sorted_idx]\n\n    pos = np.arange(n_features) + 0.5\n    plt.barh(pos, importances_sorted[:n_features], align='center')\n\n    plt.yticks(pos, names_sorted[:n_features])\n    plt.xlabel(xlabel)\n\n    plt.title(\"Feature importances\")\n\n    return plt.gca()","c041fb40":"def plot_act_vs_pred(y_act, y_pred, scale=1, act_label='actual', pred_label='predicted', figsize=None, xlim=None,\n                     ylim=None, font=None):\n    \n    if font is not None:\n        mpl.rc('font', **font)\n\n    if figsize is not None:\n        plt.figure(figsize=figsize)\n    else:\n        plt.figure(figsize=(10, 6))\n    plt.scatter(y_act\/scale, y_pred\/scale)\n    x = np.linspace(0, y_act.max()\/scale, 10)\n    plt.plot(x, x)\n    plt.xlabel(act_label)\n    plt.ylabel(pred_label)\n    if xlim is not None:\n        plt.xlim(xlim)\n    else:\n        plt.xlim([0, 1e2])\n    if ylim is not None:\n        plt.ylim(ylim)\n    else:\n        plt.ylim([0, 1e2])\n    return plt.gca()","970d1b66":"def compute_perc_deviation(y_act, y_pred, absolute=False):\n    dev = (y_pred - y_act)\/y_act * 100\n    if absolute:\n        dev = np.abs(dev)\n        dev.name = 'abs % error'\n    else:\n        dev.name = '% error'\n    return dev\n\ndef plot_dev_distribution(y_act, y_pred, absolute=False, bins=100, figsize=None, xlim=None, font=None):\n    if font is not None:\n        mpl.rc('font', **font)\n\n    if figsize is not None:\n        plt.figure(figsize=figsize)\n    else:\n        plt.figure(figsize=(10, 6))\n    dev = compute_perc_deviation(y_act, y_pred, absolute=absolute)\n    dev.plot(kind='hist', bins=bins, density=True)\n    print('mean % dev:', dev.mean())\n    print('median % dev:', dev.median())\n    # plt.vlines(dev.mean(), 0, 0.05)\n    plt.title('Distribution of errors')\n    plt.xlabel('% deviation')\n    if xlim is not None:\n        plt.xlim(xlim)\n    else:\n        plt.xlim([-1e2, 1e2])\n    return plt.gca()","6b5905c2":"categorical_features = [\n    'Body_Type',\n    'Driven_Wheels',\n    'Global_Sales_Sub-Segment',\n    'Brand',\n    'Nameplate',\n    'Transmission',\n    'Turbo',\n    'Fuel_Type',\n    'PropSysDesign',\n    'Plugin',\n    'Registration_Type',\n    'country_name'\n]\n\nnumeric_features = [\n    'Generation_Year',\n    'Length',\n    'Height',\n    'Width',\n    'Engine_KW',\n    'No_of_Gears',\n    'Curb_Weight',\n    'CO2',\n    'Fuel_cons_combined',\n    'year'\n]\n\nall_numeric_features = list(numeric_features)\nall_categorical_features = list(categorical_features)\n\ntarget = [\n    'Price_USD'\n]\n\ntarget_name = 'Price_USD'","eb3328c5":"#ml_model_type = 'Linear Regression'\nml_model_type = 'Decision Tree'\n# ml_model_type = 'Random Forest'\n\nregression_metric = 'mean abs perc error'\n\ndo_grid_search_cv = False\nscoring_greater_is_better = False  # THIS NEEDS TO BE SET CORRECTLY FOR CV GRID SEARCH\n\ndo_retrain_total = True\nwrite_predictions_file = True\n\n# relative size of test set\ntest_size = 0.3\nrandom_state = 33","58f94854":"df = pd.read_csv('\/kaggle\/input\/ihsmarkit-hackathon-june2020\/train_data.csv',index_col='vehicle_id')\ndf['date'] = pd.to_datetime(df['date'])","97d71d10":"# basic commands on a dataframe\n# df.info()\ndf.head(5)\n# df.shape\n# df.head()\n# df.tail()","becdd526":"df['country_name'].value_counts()","eb8a74bf":"df.groupby(['year', 'country_name'])['date'].count()","676dbe1c":"df_oos = pd.read_csv('\/kaggle\/input\/ihsmarkit-hackathon-june2020\/oos_data.csv', index_col='vehicle_id')\ndf_oos['date'] = pd.to_datetime(df_oos['date'])\ndf_oos['year'] = df_oos['date'].map(lambda d: d.year)","97d529c6":"# df_oos.shape\ndf_oos.head()","c760e71e":"df_oos.groupby(['year', 'country_name'])['date'].count()","6e7d1c27":"# unique values, categorical variables\nfor col in all_categorical_features:\n    print(col, len(df[col].unique()))","fa14501a":"interactive(lambda col: show_values(df, col), col=all_categorical_features)","a6276f56":"# summary statistics\ndf[numeric_features + target].describe()","c7f19f08":"figsize = (16,12)\nsns.set(style='whitegrid', font_scale=2)\n\nbins = 1000\nbins = 40\n#xlim = [0,100000]\nxlim = None\nprice_mask = df['Price_USD'] < 100000\ninteractive(lambda col: plot_distribution(df[price_mask], col, bins=bins, xlim=xlim), col=sorted(all_numeric_features + target))\n#interactive(lambda col: plot_distribution(df, col, bins=bins, xlim=xlim), col=sorted(all_numeric_features + target))","876cf284":"# this is quite slow\nsns.set(style='whitegrid', font_scale=1)\n# sns.pairplot(df[numeric_features[:6] + target].iloc[:10000])\n#sns.pairplot(df[['Engine_KW'] + target].iloc[:10000])\nprice_mask = df['Price_USD'] < 100000\ndf_temp = df[price_mask].copy()\nsns.pairplot(df_temp[['Engine_KW'] + target])","a56aba4a":"additional_numeric_features = []","64ee3499":"features_drop = []\n\nif ml_model_type == 'Linear Regression':\n    features_drop = categorical_features + numeric_features\n    features_to_use = ['Engine_KW']\n    # features_to_use = ['country_name', 'Engine_KW']\n    for feature in features_to_use:\n        features_drop.remove(feature)\nelse:\n    features_drop = categorical_features + numeric_features\n    #features_to_use = ['Brand', 'country_name', 'Engine_KW']\n    features_to_use = ['country_name', 'Engine_KW']\n    for feature in features_to_use:\n        features_drop.remove(feature)\n    # features_drop = ['Nameplate']\n    \n\ncategorical_features = list(filter(lambda f: f not in features_drop, categorical_features))\nnumeric_features = list(filter(lambda f: f not in features_drop, numeric_features))","f823f35a":"features = categorical_features + numeric_features + additional_numeric_features\nmodel_columns = features + [target_name]\nlen(model_columns)","004f5c6d":"#dataframe for further processing\ndf_proc = df[model_columns].copy()\ndf_proc.shape","cd718228":"# One-hot encoding\nencoder = ce.OneHotEncoder(cols=categorical_features, handle_unknown='value', \n                           use_cat_names=True)\nencoder.fit(df_proc)\ndf_comb_ext = encoder.transform(df_proc)\nfeatures_ext = list(df_comb_ext.columns)\nfeatures_ext.remove(target_name)","909233db":"#del df_proc\ndf_comb_ext.head()","d01e3929":"# df_comb_ext.memory_usage(deep=True).sum()\/1e9\n#features_model\ndf_comb_ext.shape","c09b6007":"X_train, X_test, y_train, y_test = train_test_split(df_comb_ext[features_ext], df_comb_ext[target_name], \n                                                    test_size=test_size, random_state=random_state)\n\nprint(X_train.shape)\nprint(X_test.shape)","973ddcae":"if ml_model_type == 'Linear Regression':\n    model_hyper_parameters_dict = OrderedDict(fit_intercept=True, normalize=False)\n    regressor =  LinearRegression(**model_hyper_parameters_dict)\n\nif ml_model_type == 'Decision Tree':\n    model_hyper_parameters_dict = OrderedDict(max_depth=3, random_state=random_state)\n    regressor =  DecisionTreeRegressor(**model_hyper_parameters_dict)\n      \nif ml_model_type == 'Random Forest':\n\n    model_hyper_parameters_dict = OrderedDict(n_estimators=10, \n                                              max_depth=4, \n                                              min_samples_split=2, \n                                              max_features='sqrt',\n                                              min_samples_leaf=1, \n                                              random_state=random_state, \n                                              n_jobs=4)\n   \n    regressor = RandomForestRegressor(**model_hyper_parameters_dict)\n        \nbase_regressor = clone(regressor)\n    \nif do_grid_search_cv:\n    \n    scoring = make_scorer(metrics_dict_res[regression_metric], greater_is_better=scoring_greater_is_better)\n    \n    if ml_model_type == 'Random Forest':\n        \n\n        grid_parameters = [{'n_estimators': [10], 'max_depth': [3, 5, 10], \n                             'min_samples_split': [2,4], 'min_samples_leaf': [1]} ]\n        \n    n_splits = 4\n    n_jobs = 4\n    cv_regressor = GridSearchCV(regressor, grid_parameters, cv=n_splits, scoring=scoring, return_train_score=True,\n                                refit=True, n_jobs=n_jobs)    ","9f5b5232":"if do_grid_search_cv:\n    cv_regressor.fit(X_train, y_train)\n    regressor_best = cv_regressor.best_estimator_\n    model_hyper_parameters_dict = cv_regressor.best_params_\n    train_scores = cv_regressor.cv_results_['mean_train_score']\n    test_scores = cv_regressor.cv_results_['mean_test_score']\n    test_scores_std = cv_regressor.cv_results_['std_test_score']\n    cv_results = cv_regressor.cv_results_\nelse:\n    regressor.fit(X_train, y_train)","e4eed70f":"if do_grid_search_cv:\n    # print(cv_results)\n    print(model_hyper_parameters_dict)\n    plt.plot(-train_scores, label='train')\n    plt.plot(-test_scores, label='test')\n    plt.xlabel('Parameter set #')\n    plt.legend()\n    regressor = regressor_best","857bfb30":"y_train_pred = regressor.predict(X_train)\ny_test_pred = regressor.predict(X_test)","b76f8756":"if ml_model_type == 'Linear Regression':\n    df_reg_coef = (pd.DataFrame(zip(['intercept'] + list(X_train.columns), \n                               [regressor.intercept_] + list(regressor.coef_)))\n                 .rename({0: 'feature', 1: 'coefficient value'}, axis=1))\n    display(df_reg_coef)","b252721d":"if hasattr(regressor, 'feature_importances_'):\n    sns.set(style='whitegrid', font_scale=1.5)\n    plt.figure(figsize=(12,10))\n    plot_feature_importances(regressor, features_ext, n_features=np.minimum(20, X_train.shape[1]))","66652389":"df_regression_metrics = regression_metrics_yin(y_train, y_train_pred, y_test, y_test_pred,\n                                               metrics_dict_res, format_digits=3)\n\ndf_output = df_regression_metrics.copy()\ndf_output.loc['Counts','train'] = len(y_train)\ndf_output.loc['Counts','test'] = len(y_test)\ndf_output","00216399":"figsize = (16,10)\nxlim = [0, 250]\nfont={'size': 20}\nsns.set(style='whitegrid', font_scale=2.5)\nact_label = 'actual price [k$]'\npred_label='predicted price [k$]'\nplot_act_vs_pred(y_test, y_test_pred, scale=1000, act_label=act_label, pred_label=pred_label, \n                 figsize=figsize, xlim=xlim, ylim=xlim, font=font)\nprint()","930952e5":"figsize = (14,8)\nxlim = [0, 100]\n#xlim = [-100, 100]\n# xlim = [-50, 50]\n#xlim = [-20, 20]\n\nfont={'size': 20}\nsns.set(style='whitegrid', font_scale=1.5)\n\np_error = (y_test_pred - y_test)\/y_test *100\ndf_p_error = pd.DataFrame(p_error.values, columns=['percent_error'])\n#display(df_p_error['percent_error'].describe().to_frame())\n\nbins=1000\nbins=500\n#bins=100\nabsolute = True\n#absolute = False\nplot_dev_distribution(y_test, y_test_pred, absolute=absolute, figsize=figsize, \n                      xlim=xlim, bins=bins, font=font)\nprint()","3d640de0":"if do_retrain_total:\n    cv_opt_model = clone(base_regressor.set_params(**model_hyper_parameters_dict))\n    # train on complete data set\n    X_train_full = df_comb_ext[features_ext].copy()\n    y_train_full = df_comb_ext[target_name].values\n    cv_opt_model.fit(X_train_full, y_train_full) \n    regressor = cv_opt_model","e8480009":"# df_oos.head()","c7c18ed2":"df_proc_oos = df_oos[model_columns[:-1]].copy()\ndf_proc_oos[target_name] = 1","c8761094":"df_comb_ext_oos = encoder.transform(df_proc_oos)","dcc7e4a4":"df_comb_ext_oos.drop(target_name, axis=1, inplace=True)","149e51b0":"y_oos_pred = regressor.predict(df_comb_ext_oos)","ab4bce67":"id_col = 'vehicle_id'\ndf_out = (pd.DataFrame(y_oos_pred, columns=[target_name], index=df_comb_ext_oos.index)\n            .reset_index()\n            .rename({'index': id_col}, axis=1))","ffa10742":"df_out.head()","fd150fd1":"df_out.shape","34c78cdb":"if write_predictions_file:\n    df_out.to_csv('submission.csv', index=False)","dd81dd6a":"##  Model Performance plots","b50b37c7":"#  ML data preparation","4a525fc4":"# Import libraries","cee5650c":"## Train test split","d5437721":"## Pair plot","669dafcd":"# Model evaluation","16943be5":"# Apply model to OOS data","1726af8a":"## ML model training","c76dda9f":"# Locally defined functions","1f352c3c":"# Machine learning model\n\nSupervised learning\n\nhttps:\/\/scikit-learn.org\/stable\/supervised_learning.html\n\nEnsemble methods in scikit learn\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/ensemble.html\n\n\nDecision trees\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/tree.html\n","d59342e6":"## Apply categorical encoding","35e5861e":"# Load  data\n","3d65b099":"# Feature generation","698035fe":"## Regression coefficients\/Feature importance","8d32d17b":"## Out of sample data (to predict)","acb66e8f":"# Global options","65a54eae":"## Categorical feature encoding","eba64ac6":"## Drop features (optional)","bf7fc039":"## Optionally retrain on the whole data set","463391c5":"## Define features","7ddc0c6e":"## Numerical features","30544562":"## Metrics","fb31918d":"## Categorical features","a97edc0c":"## Apply model and produce output","9d5475ca":"## Train, test predictions","55e417a0":"##  Model definition","6ff4762d":"# Feature exploration","761f66ea":"## Metrics","bf4c1a2a":"## Subset to relevant columns","3f36f53c":"## Training data","eb0463f6":"## Display functions","c76af8de":"# Feature selection\n\nYou can read about feature selection here\nhttps:\/\/scikit-learn.org\/stable\/modules\/feature_selection.html#","6e186775":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-libraries\" data-toc-modified-id=\"Import-libraries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Import libraries<\/a><\/span><\/li><li><span><a href=\"#Locally-defined-functions\" data-toc-modified-id=\"Locally-defined-functions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Locally defined functions<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Metrics\" data-toc-modified-id=\"Metrics-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;<\/span>Metrics<\/a><\/span><\/li><li><span><a href=\"#Display-functions\" data-toc-modified-id=\"Display-functions-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;<\/span>Display functions<\/a><\/span><\/li><li><span><a href=\"#Define-features\" data-toc-modified-id=\"Define-features-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;<\/span>Define features<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Global-options\" data-toc-modified-id=\"Global-options-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Global options<\/a><\/span><\/li><li><span><a href=\"#Load--data\" data-toc-modified-id=\"Load--data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Load  data<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Training-data\" data-toc-modified-id=\"Training-data-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;<\/span>Training data<\/a><\/span><\/li><li><span><a href=\"#Out-of-sample-data-(to-predict)\" data-toc-modified-id=\"Out-of-sample-data-(to-predict)-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;<\/span>Out of sample data (to predict)<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Feature-exploration\" data-toc-modified-id=\"Feature-exploration-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Feature exploration<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Categorical-features\" data-toc-modified-id=\"Categorical-features-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;<\/span>Categorical features<\/a><\/span><\/li><li><span><a href=\"#Numerical-features\" data-toc-modified-id=\"Numerical-features-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;<\/span>Numerical features<\/a><\/span><\/li><li><span><a href=\"#Pair-plot\" data-toc-modified-id=\"Pair-plot-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;<\/span>Pair plot<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Feature-generation\" data-toc-modified-id=\"Feature-generation-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Feature generation<\/a><\/span><\/li><li><span><a href=\"#Feature-selection\" data-toc-modified-id=\"Feature-selection-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>Feature selection<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Drop-features-(optional)\" data-toc-modified-id=\"Drop-features-(optional)-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;<\/span>Drop features (optional)<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#ML-data-preparation\" data-toc-modified-id=\"ML-data-preparation-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;<\/span>ML data preparation<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Categorical-feature-encoding\" data-toc-modified-id=\"Categorical-feature-encoding-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;<\/span>Categorical feature encoding<\/a><\/span><\/li><li><span><a href=\"#Train-test-split\" data-toc-modified-id=\"Train-test-split-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;<\/span>Train test split<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Machine-learning-model\" data-toc-modified-id=\"Machine-learning-model-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;<\/span>Machine learning model<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Model-definition\" data-toc-modified-id=\"Model-definition-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;<\/span>Model definition<\/a><\/span><\/li><li><span><a href=\"#ML-model-training\" data-toc-modified-id=\"ML-model-training-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;<\/span>ML model training<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Model-evaluation\" data-toc-modified-id=\"Model-evaluation-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;<\/span>Model evaluation<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Train,-test-predictions\" data-toc-modified-id=\"Train,-test-predictions-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;<\/span>Train, test predictions<\/a><\/span><\/li><li><span><a href=\"#Regression-coefficients\/Feature-importance\" data-toc-modified-id=\"Regression-coefficients\/Feature-importance-10.2\"><span class=\"toc-item-num\">10.2&nbsp;&nbsp;<\/span>Regression coefficients\/Feature importance<\/a><\/span><\/li><li><span><a href=\"#Metrics\" data-toc-modified-id=\"Metrics-10.3\"><span class=\"toc-item-num\">10.3&nbsp;&nbsp;<\/span>Metrics<\/a><\/span><\/li><li><span><a href=\"#Model-Performance-plots\" data-toc-modified-id=\"Model-Performance-plots-10.4\"><span class=\"toc-item-num\">10.4&nbsp;&nbsp;<\/span>Model Performance plots<\/a><\/span><\/li><li><span><a href=\"#Optionally-retrain-on-the-whole-data-set\" data-toc-modified-id=\"Optionally-retrain-on-the-whole-data-set-10.5\"><span class=\"toc-item-num\">10.5&nbsp;&nbsp;<\/span>Optionally retrain on the whole data set<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Apply-model-to-OOS-data\" data-toc-modified-id=\"Apply-model-to-OOS-data-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;<\/span>Apply model to OOS data<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Subset-to-relevant-columns\" data-toc-modified-id=\"Subset-to-relevant-columns-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;<\/span>Subset to relevant columns<\/a><\/span><\/li><li><span><a href=\"#Apply-categorical-encoding\" data-toc-modified-id=\"Apply-categorical-encoding-11.2\"><span class=\"toc-item-num\">11.2&nbsp;&nbsp;<\/span>Apply categorical encoding<\/a><\/span><\/li><li><span><a href=\"#Apply-model-and-produce-output\" data-toc-modified-id=\"Apply-model-and-produce-output-11.3\"><span class=\"toc-item-num\">11.3&nbsp;&nbsp;<\/span>Apply model and produce output<\/a><\/span><\/li><\/ul><\/li><\/ul><\/div>"}}