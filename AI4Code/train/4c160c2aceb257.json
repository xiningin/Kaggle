{"cell_type":{"1dc800ba":"code","15a7a856":"code","8b744998":"code","9b9e73d9":"code","746b42ba":"code","a4f8d64c":"code","22e468b6":"markdown","fd5fb0fe":"markdown","fd21b495":"markdown","81a6659c":"markdown","d0602c68":"markdown","2f31b812":"markdown","95d0fd13":"markdown"},"source":{"1dc800ba":"!pip install keras","15a7a856":"import numpy as np\nimport tensorflow as tf\nimport os\nimport distutils","8b744998":"CORPUS_URL = \"..\/input\/pt-text-generation-dataset-with-desciclopedia-text\/desciclopedia-articles.txt\"\nEMBEDDING_DIM = 512\nLINES_NUM = 229\nEPOCHS_NUM = 20\nseed_txt = 'Pooh, Taki pariu! '","9b9e73d9":"def transform(txt):\n  return np.asarray([ord(c) for c in txt if ord(c) < 255], dtype=np.int32)\n\ndef input_fn(seq_len=100, batch_size=1024):\n  \"\"\"Return a dataset of source and target sequences for training.\"\"\"\n  txt = open(CORPUS_URL).read()\n  txt = txt.lower()\n\n\n  source = tf.constant(transform(txt), dtype=tf.int32)\n\n  ds = tf.data.Dataset.from_tensor_slices(source).batch(seq_len+1, drop_remainder=True)\n\n  def split_input_target(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text\n\n  BUFFER_SIZE = 10000\n  ds = ds.map(split_input_target).shuffle(BUFFER_SIZE).batch(batch_size, drop_remainder=True)\n\n  return ds.repeat()\n\ndef lstm_model(seq_len=100, batch_size=None, stateful=True):\n  \"\"\"Language model: predict the next word given the current word.\"\"\"\n  source = tf.keras.Input(\n      name='seed', shape=(seq_len,), batch_size=batch_size, dtype=tf.int32)\n\n  embedding = tf.keras.layers.Embedding(input_dim=256, output_dim=EMBEDDING_DIM)(source)\n  lstm_1 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(embedding)\n  lstm_2 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(lstm_1)\n  predicted_char = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(256, activation='softmax'))(lstm_2)\n  return tf.keras.Model(inputs=[source], outputs=[predicted_char])","746b42ba":"tf.keras.backend.clear_session()\n\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=os.environ['TPU_NAME'])\ntf.config.experimental_connect_to_cluster(resolver)\n# This is the TPU initialization code that has to be at the beginning.\ntf.tpu.experimental.initialize_tpu_system(resolver)\nprint(\"All devices: \", tf.config.list_logical_devices('TPU'))\n\nstrategy = tf.distribute.experimental.TPUStrategy(resolver)\n\nwith strategy.scope():\n  training_model = lstm_model(seq_len=100, stateful=False)\n  training_model.compile(\n      optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.01),\n      loss='sparse_categorical_crossentropy',\n      metrics=['sparse_categorical_accuracy'])\n\ntraining_model.fit(\n    input_fn(100,LINES_NUM),\n    steps_per_epoch=LINES_NUM,\n    epochs=EPOCHS_NUM\n)\n#    steps_per_epoch=100,\ntraining_model.save_weights('weights.tf', overwrite=True)","a4f8d64c":"BATCH_SIZE = 5\nPREDICT_LEN = 250\n\n# Keras requires the batch size be specified ahead of time for stateful models.\n# We use a sequence length of 1, as we will be feeding in one character at a \n# time and predicting the next character.\nprediction_model = lstm_model(seq_len=1, batch_size=BATCH_SIZE, stateful=True)\nprediction_model.load_weights('weights.h5')\n\n# We seed the model with our initial string, copied BATCH_SIZE times\nseed = transform(seed_txt)\nseed = np.repeat(np.expand_dims(seed, 0), BATCH_SIZE, axis=0)\n\n# First, run the seed forward to prime the state of the model.\nprediction_model.reset_states()\nfor i in range(len(seed_txt) - 1):\n  prediction_model.predict(seed[:, i:i + 1])\n\n# Now we can accumulate predictions!\npredictions = [seed[:, -1:]]\nfor i in range(PREDICT_LEN):\n  last_word = predictions[-1]\n  next_probits = prediction_model.predict(last_word)[:, 0, :]\n  \n  # sample from our output distribution\n  next_idx = [\n      np.random.choice(256, p=next_probits[i])\n      for i in range(BATCH_SIZE)\n  ]\n  predictions.append(np.asarray(next_idx, dtype=np.int32))\n  \n\nfor i in range(BATCH_SIZE):\n  print('PREDICTION %d\\n\\n' % i)\n  p = [predictions[j][i] for j in range(PREDICT_LEN)]\n  generated = ''.join([chr(c) for c in p])  # Convert back to text\n  print(\"\\033[94m\" + seed_txt + \"\\033[0m\" + generated)\n  print()\n  assert len(generated) == PREDICT_LEN, 'Generated text too short'","22e468b6":"### Edit in your need:","fd5fb0fe":"## Articles Used:\n* [Aspirador de p\u00f3](http:\/\/desciclopedia.org\/wiki\/Aspirador_de_p%C3%B3) (Lines 1-56)\n* [Sof\u00e1](http:\/\/desciclopedia.org\/wiki\/Sof%C3%A1) (Lines 57-75)\n* [Rifa](http:\/\/desciclopedia.org\/wiki\/Rifa) (Lines 76-104)\n* [Outdoor](http:\/\/desciclopedia.org\/wiki\/Outdoor) (Lines 105-122)\n* [Piscina de 100 litros](http:\/\/desciclopedia.org\/wiki\/Piscina_de_1000_litros) (Lines 123-154)\n* [Touro mec\u00e2nico](http:\/\/desciclopedia.org\/wiki\/Touro_mec%C3%A2nico) (Lines 155-172)\n* [RMS Olympic](http:\/\/desciclopedia.org\/wiki\/RMS_Olympic) (Lines 174-195)\n* [Pacu](http:\/\/desciclopedia.org\/wiki\/Pacu) (Lines 196-206)\n* [Desmanuais: Como bater punheta na hora da aula](http:\/\/desciclopedia.org\/wiki\/Desmanuais:Como_bater_punheta_na_hora_da_aula) (Lines 207-229)","fd21b495":"### Start Training:","81a6659c":"### Import dependencies:","d0602c68":"### Run AI:","2f31b812":"### Functions:","95d0fd13":"### Install dependencies:"}}