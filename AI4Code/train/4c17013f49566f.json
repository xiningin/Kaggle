{"cell_type":{"e4e28419":"code","eb98338f":"code","4a42b97d":"code","d3387ade":"code","f370cfb9":"code","ae9a1e5b":"code","bf8c9a9a":"code","590f5372":"code","2ec91c6d":"code","b9420ec0":"code","bd8f7419":"code","365a51da":"code","35b152c2":"code","2761e011":"code","b79c13fd":"code","3dbb90c3":"code","e287ae22":"code","f2dc94e8":"code","ea0b1a88":"code","24a2eb37":"code","e6722a25":"code","10add888":"code","9fbca6ac":"code","441d9012":"code","e409ee83":"code","bac67e80":"code","c02929c5":"code","9a43bc63":"code","bc1a073b":"code","90a34c94":"code","52087b9e":"code","781823e8":"code","36e83f3c":"code","ad471f98":"code","1c5fc8fa":"code","2ec4ed7f":"code","acccc045":"code","94cc929d":"code","a8bb092a":"code","5d91ff58":"code","315a1304":"code","8201939f":"code","065cdb83":"code","9f2d3e1e":"code","e4229bd4":"code","8475c0c4":"code","857e214b":"code","f1c56c0c":"code","e1aea540":"code","9e5ad8bf":"code","c9c21c48":"code","511d512a":"code","63b3b958":"code","893762ed":"code","2b4a8bc8":"code","a0bc3f5d":"code","b64d1402":"code","253d4db5":"code","176197b4":"code","37b5cfb0":"code","ba41a5b9":"code","023f2912":"code","f37ffcc9":"code","3c25e45c":"code","7eee68c3":"markdown","2e391b5f":"markdown","1aab2e32":"markdown","dfc5d298":"markdown","411df08d":"markdown","8265cc0a":"markdown","2079274f":"markdown","d2c45951":"markdown","e75e7949":"markdown","da7e53a7":"markdown","7084918e":"markdown","1883bfa2":"markdown","7e77e140":"markdown","138632d3":"markdown","ed4dd63e":"markdown","59f2e82c":"markdown","5c0c779c":"markdown","193eb3f6":"markdown","54e9f58d":"markdown","a41df810":"markdown","4dc2e10b":"markdown","119c6560":"markdown","e5220a0b":"markdown","25313024":"markdown","ef72c966":"markdown","bf447336":"markdown","cfafc7d7":"markdown","2e51ed4f":"markdown","3b3c7b5b":"markdown","c89cfbdd":"markdown","7e589b89":"markdown","92e03543":"markdown","34bc1edd":"markdown","0e6b91ec":"markdown","9c68dd92":"markdown","00cd9f30":"markdown","4179a4eb":"markdown","59c1ec7c":"markdown","a55ea428":"markdown","6dc2cd41":"markdown","cb3058a8":"markdown","71388f19":"markdown","23a925cc":"markdown"},"source":{"e4e28419":"# Standard libaries\n\n# For data structures\n\nimport pandas as pd\n\n# For basic plottting\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n# For advanced plotting\n\nimport seaborn as sns\n\n# For scientific computing\n\nimport numpy as np\n\n# Config option for displaying output\n# Default is 'last_expr'\n\n# from IPython.core.interactiveshell import InteractiveShell\n# InteractiveShell.ast_node_interactivity = \"all\"\n\n# Set display options to max column width and all columns and rows\n\npd.set_option('display.max_colwidth', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\n# For python system functions\n\nimport sys\n\n# For listing files in current working directory\n\n# import os\n# pd.DataFrame([os.getcwd()], columns=['Current Working Directory:'])\n# pd.DataFrame(os.listdir(), columns=['Current Working Directory Contents:'])\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfor dirname, _, filenames in os.walk('\/kaggle\/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Additional Libraries\n\n# For unziping files\n\nfrom zipfile import ZipFile\n\n# For plotting style\n\nplt.style.use('fivethirtyeight')\n\n# For models and evaluation\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\n\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_squared_log_error\n\nfrom scipy import stats\n\n\n","eb98338f":"# Load data\n\ndf_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_comp = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n","4a42b97d":"# Explore training dataset\n\ndf_train.head()\n","d3387ade":"# Explore training dataset\n\ndf_train.shape\n","f370cfb9":"# Explore competition dataset\n\ndf_comp.head()\n","ae9a1e5b":"# Explore competition dataset\n\ndf_comp.shape\n","bf8c9a9a":"# Compare features between datasets\n\nlst_train_diff_features = list(set(df_train.columns.values)-set(df_comp.columns.values))\nlst_train_diff_features\n","590f5372":"# Compare features between datasets\n\nlst_comp_diff_features = list(set(df_comp.columns.values)-set(df_train.columns.values))\nlst_comp_diff_features\n","2ec91c6d":"# Explore data format training dataset\n\ndf_train.info(verbose=True)\n","b9420ec0":"# Explore data format competition dataset\n\ndf_comp.info(verbose=True)\n","bd8f7419":"# Explore missing data in training dataset\n\nnull_data_train = df_train.columns[df_train.isnull().any()]\ndf_train[null_data_train].isnull().sum()\n","365a51da":"# Explore missing data in competition dataset\n\nnull_data_comp = df_comp.columns[df_comp.isnull().any()]\ndf_comp[null_data_comp].isnull().sum()\n    ","35b152c2":"# Drop features with high level of missing values from training dataset\n\ndf_train.drop(['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis=1, inplace=True)\ndf_train.shape\n","2761e011":"# Drop features with high level of missing values from the competition dataset\n\ndf_comp.drop(['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis=1, inplace=True)\ndf_comp.shape\n","b79c13fd":"# Explore missing data in training dataset\n\nnull_data_train = df_train.columns[df_train.isnull().any()]\ndf_train[null_data_train].isnull().sum()\n","3dbb90c3":"# Explore missing data in competition dataset\n\nnull_data_comp = df_comp.columns[df_comp.isnull().any()]\ndf_comp[null_data_comp].isnull().sum()\n","e287ae22":"# Reverify features between datasets\n\ndf_train.head()\n","f2dc94e8":"# Reverify features between datasets\n\ndf_train.shape","ea0b1a88":"# Reverify features between datasets\n\ndf_comp.head()\n","24a2eb37":"# Reverify features between datasets\n\ndf_comp.shape\n","e6722a25":"# Reverify features between datasets\n\nlst_train_diff_features = list(set(df_train.columns.values)-set(df_comp.columns.values))\nlst_train_diff_features\n","10add888":"# Reverify features between datasets\n\nlst_comp_diff_features = list(set(df_comp.columns.values)-set(df_train.columns.values))\nlst_comp_diff_features\n","9fbca6ac":"# Create new dataframe of categorical variables for training dataset\n\ndf_train_cat_var = df_train[['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities',\n                       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n                       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', \n                       'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n                       'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', \n                       'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', \n                       'Functional', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', \n                       'PavedDrive', 'SaleType', 'SaleCondition']]\ndf_train_cat_var.head()\ndf_train_cat_var.shape\n","441d9012":"# Loop through categorical variables and convert to numeric values for training dataset\n\nfor column in df_train_cat_var:\n    df_dummy_col = pd.get_dummies(df_train[column], prefix=column)\n    df_train = pd.concat([df_train, df_dummy_col], axis=1)\n    df_train.drop(column, axis=1, inplace=True)  \n\n# Convert all data types to floats\n\ndf_train = df_train.astype(float)\n\n# Explore the new dataset\n\ndf_train.head()\n","e409ee83":"# Create new dataframe of categorical variables for competition dataset\n\ndf_comp_cat_var = df_comp[['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities',\n                       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n                       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', \n                       'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n                       'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', \n                       'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', \n                       'Functional', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', \n                       'PavedDrive', 'SaleType', 'SaleCondition']]\ndf_comp_cat_var.head()\n","bac67e80":"# Loop through categorical variables and convert to numeric values for competition dataset\n\nfor column in df_comp_cat_var:\n    df_dummy_col = pd.get_dummies(df_comp[column], prefix=column)\n    df_comp = pd.concat([df_comp, df_dummy_col], axis=1)\n    df_comp.drop(column, axis=1, inplace=True)  \n\n# Convert all data types to floats\n\ndf_comp = df_comp.astype(float)\n\n# Explore the new dataset\n\ndf_comp.head()\ndf_comp.shape\n","c02929c5":"# Create difference list of features uncommon between training and comp datasets\n\nlst_train_diff_features = list(set(df_train.columns.values)-set(df_comp.columns.values))\nlst_train_diff_features\n","9a43bc63":"# Drop SalesPrice from difference list\n\nlst_train_diff_features.remove('SalePrice')\nlst_train_diff_features\n","bc1a073b":"# Drop difference list from training dataset\n\ndf_train.drop(lst_train_diff_features, axis=1, inplace=True)\n","90a34c94":"# Reverify difference between training and comp datasets\n\nlst_train_diff_features = list(set(df_train.columns.values)-set(df_comp.columns.values))\nlst_train_diff_features\n","52087b9e":"# Reverify difference between training and comp datasets\n\nlst_comp_diff_features = list(set(df_comp.columns.values)-set(df_train.columns.values))\nlst_comp_diff_features\n","781823e8":"# Sort features alphabetically and explore new datasets\n\ndf_train = df_train.reindex(sorted(df_train.columns), axis=1)\ndf_train.head()\n","36e83f3c":"# Sort features alphabetically and explore new datasets\n\ndf_train.shape\n","ad471f98":"# Sort features alphabetically and explore new datasets\n\ndf_comp = df_comp.reindex(sorted(df_comp.columns), axis=1)\ndf_comp.head()\n","1c5fc8fa":"# Sort features alphabetically and explore new datasets\n\ndf_comp.shape\n","2ec4ed7f":"# Explore missing data in training dataset\n\nnull_data_train = df_train.columns[df_train.isnull().any()]\ndf_train[null_data_train].isnull().sum()\n","acccc045":"# Explore missing data in competition dataset\n\nnull_data_comp = df_comp.columns[df_comp.isnull().any()]\ndf_comp[null_data_comp].isnull().sum()\n","94cc929d":"# Loop through training dataset and replace missing values with feature mean\n\nlst_null_train = ['GarageYrBlt', 'LotFrontage', 'MasVnrArea']\n\nfor n in lst_null_train:\n    df_train[n].fillna(df_train[n].mean(), axis=0, inplace=True)\n","a8bb092a":"# Loop through competition dataset and replace missing values with feature mean\n\nlst_null_comp = ['GarageYrBlt', 'LotFrontage', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n            'BsmtFullBath', 'BsmtHalfBath', 'BsmtUnfSF', 'GarageArea', 'GarageCars', \n            'TotalBsmtSF', 'GarageYrBlt']\n\nfor n in lst_null_comp:\n    df_comp[n].fillna(df_comp[n].mean(), axis=0, inplace=True)\n","5d91ff58":"# Explore missing data in training dataset\n\nnull_data_train = df_train.columns[df_train.isnull().any()]\ndf_train[null_data_train].isnull().sum()\n","315a1304":"# Explore missing data in competition dataset\n\nnull_data_comp = df_comp.columns[df_comp.isnull().any()]\ndf_comp[null_data_comp].isnull().sum()\n","8201939f":"# Reverify features between datasets\n\ndf_train.head()\n","065cdb83":"# Reverify features between datasets\n\ndf_train.shape\n","9f2d3e1e":"# Reverify features between datasets\n\ndf_comp.head()\n","e4229bd4":"# Reverify features between datasets\n\ndf_comp.shape\n","8475c0c4":"# Reverify features between datasets\n\nlst_train_diff_features = list(set(df_train.columns.values)-set(df_comp.columns.values))\nlst_train_diff_features\n","857e214b":"# Reverify features between datasets\n\nlst_comp_diff_features = list(set(df_comp.columns.values)-set(df_train.columns.values))\nlst_comp_diff_features\n","f1c56c0c":"# Export training dataset features for easy input into model\n\nlst_train_col = df_train.columns\nlst_train_features = []\n\nfor f in lst_train_col:\n    f = \"'\" + f + \"'\"\n    lst_train_features.append([f])\n\npd.DataFrame(lst_train_features).to_csv('\/kaggle\/working\/training_features.csv', index=False)\n","e1aea540":"# Explore correlation of training dataset\n\ndf_train_corr = df_train.corr()\ndf_train_corr_sale = df_train_corr[['SalePrice']]\ndf_train_corr_sale = df_train_corr_sale.sort_values(['SalePrice'], ascending=False).head(10)\ndf_train_corr_sale\n","9e5ad8bf":"# Determine key features\n\nlst_ind_var = ['BedroomAbvGr',  \n                'BsmtCond_Fa', \n                'BsmtCond_Gd', \n                'BsmtCond_Po', \n                'BsmtCond_TA', \n                'ExterCond_Ex', \n                'ExterCond_Fa', \n                'ExterCond_Gd',\n                'ExterCond_Po', \n                'ExterCond_TA', \n                'ExterQual_Ex', \n                'ExterQual_Fa',\n                'ExterQual_Gd', \n                'ExterQual_TA', \n                'FullBath', \n                'GarageCars', \n                'GarageQual_Fa',\n                'GarageQual_Gd',\n                'GarageQual_Po',\n                'GarageQual_TA',\n                'GrLivArea',\n                'HalfBath',\n                'KitchenQual_Ex',\n                'KitchenQual_Fa',\n                'KitchenQual_Gd',\n                'KitchenQual_TA',\n                'LotArea',\n                'MSSubClass',\n                'MSZoning_C (all)',\n                'MSZoning_FV',\n                'MSZoning_RH',\n                'MSZoning_RL',\n                'MSZoning_RM',\n                'Neighborhood_Blmngtn',\n                'Neighborhood_Blueste',\n                'Neighborhood_BrDale',\n                'Neighborhood_BrkSide',\n                'Neighborhood_ClearCr',\n                'Neighborhood_CollgCr',\n                'Neighborhood_Crawfor',\n                'Neighborhood_Edwards',\n                'Neighborhood_Gilbert',\n                'Neighborhood_IDOTRR',\n                'Neighborhood_MeadowV',\n                'Neighborhood_Mitchel',\n                'Neighborhood_NAmes',\n                'Neighborhood_NPkVill',\n                'Neighborhood_NWAmes',\n                'Neighborhood_NoRidge',\n                'Neighborhood_NridgHt',\n                'Neighborhood_OldTown',\n                'Neighborhood_SWISU',\n                'Neighborhood_Sawyer',\n                'Neighborhood_SawyerW',\n                'Neighborhood_Somerst',\n                'Neighborhood_StoneBr',\n                'Neighborhood_Timber',\n                'Neighborhood_Veenker',\n                'OverallCond',\n                'OverallQual',\n                'Street_Grvl',\n                'Street_Pave',\n              ]\n\n# Root Mean Squared Error 39735.084240\n# R2 score 0.771371\n","c9c21c48":"# Plot key features\n\n# Plot the data\n\nfig, axes = plt.subplots(2, 2, sharey=True, figsize=(10, 6))\nfig.subplots_adjust(wspace=0.4, hspace=0.4)\n\nlst_plot_features = ['GrLivArea','LotArea', 'OverallQual', 'OverallCond']\n\nfor ax, features in zip(axes.ravel(), lst_plot_features):\n    ax.scatter(df_train[features], df_train['SalePrice'])\n    ax.set_title(features, fontsize=10)\n    ax.set_xticklabels([])\n    ax.set_ylabel('Sale Price', fontsize=10)\n    ax.set_yticklabels([])\n\nfig.suptitle('Plot Key Features Compared to Sale Price', fontsize=20)\n\n# Show plots\n\nplt.show()\n","511d512a":"# Prepare the data\n\nlm = LinearRegression()\n\nX_data = df_train[lst_ind_var]\ny_data = df_train['SalePrice']\n\n# Train and fit the model\n\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=.20, random_state=0)\nlm.fit(X_train, y_train)\ny_hat = lm.predict(X_test)\n","63b3b958":"print('Training set coef: ', lm.coef_)\nprint('Training set intercept: ', lm.intercept_)\nprint('Training set y_hat: ', y_hat[0:5])\ndf_coef = pd.DataFrame(lm.coef_, index=X_data.columns, columns=['Coefficients'])\ndf_coef\n","893762ed":"# Explore training model actual vs predicted house prices\n\ndf_train_comp = pd.DataFrame({'Actual (y_test)':y_test, 'Predicted (y_hat)':y_hat})\ndf_train_comp.head()\n","2b4a8bc8":"# Plot training model actual vs predicted house prices\n\n# Number of records to display\n\nn = 10\n\n# Colors for bars\n\nblue = '#0000ff'\ngold = '#ffd700'\n\n# Plot data\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nact_hse_price = df_train_comp['Actual (y_test)'].head(n)\npred_hse_price = df_train_comp['Predicted (y_hat)'].head(n)\n\nbar_width = 0.25\nx_bar_pos = np.arange(1, n+1)\nx_label = df_train_comp.head(n).index\nmax_hse_price = df_train.SalePrice.max()\n\nax.bar(x_bar_pos, act_hse_price, color=blue, width=bar_width, label='Actual (y_test)')\nax.bar(x_bar_pos+bar_width, pred_hse_price, color=gold, width=bar_width, label='Predicted (y_hat)')\n\nax.set_xlabel('House ID', fontsize=10)\nax.set_xticks(x_bar_pos+(.5*bar_width))\nax.set_xticklabels(x_label, fontsize=10)\n\nax.set_ylabel('House Price', fontsize=10)\nax.set_yticks([0, 500000,  900000])\nax.set_ylim([0, 900000])\n\nax.set_title('Training Model: Actual vs Predicted Price (Sample)', fontsize=20)\nax.legend()\n\n# Show plot\n\nplt.show()\n","a0bc3f5d":"# Evaluate the model with Root Mean Squared Error, R2 score, Mean Squared Log Error, and Root Mean Squared Log Error\n\nlst_eval = [np.sqrt(mean_squared_error(y_test, y_hat)), r2_score(y_test, y_hat), mean_squared_log_error(y_test, y_hat), np.sqrt(mean_squared_log_error(y_test,y_hat))]\n\ndf_eval = pd.DataFrame({'Evaluation':lst_eval}, index=['Root Mean Squared Error', 'R2 score', 'Mean Squared Log Error', 'Root Mean Squared Log Error'])\ndf_eval\n","b64d1402":"# Use Ridge Regression to improve model\n\nrr = Ridge(alpha=0.10)\nrr.fit(X_train, y_train)\n\n# Evaluate the model with Root Mean Squared Error, R2 score, Mean Squared Log Error, and Root Mean Squared Log Error\n\nlst_eval = [np.sqrt(mean_squared_error(y_test, y_hat)), r2_score(y_test, y_hat), mean_squared_log_error(y_test, y_hat), np.sqrt(mean_squared_log_error(y_test,y_hat))]\n\ndf_eval = pd.DataFrame({'Evaluation':lst_eval}, index=['Root Mean Squared Error', 'R2 score', 'Mean Squared Log Error', 'Root Mean Squared Log Error'])\ndf_eval\n","253d4db5":"# Use lasso regression to improve model\n\nlasso = Lasso()\nlasso.fit(X_train,y_train)\n\n# Evaluate the model with Root Mean Squared Error, R2 score, Mean Squared Log Error, and Root Mean Squared Log Error\n\nlst_eval = [np.sqrt(mean_squared_error(y_test, y_hat)), r2_score(y_test, y_hat), mean_squared_log_error(y_test, y_hat), np.sqrt(mean_squared_log_error(y_test,y_hat))]\n\ndf_eval = pd.DataFrame({'Evaluation':lst_eval}, index=['Root Mean Squared Error', 'R2 score', 'Mean Squared Log Error', 'Root Mean Squared Log Error'])\ndf_eval\n","176197b4":"# Loop through all elements in key independent variables list and print Pearson Coefficent and P-value\n\nlst_pearson = []\nfor i in lst_ind_var:\n    pearson_coef, p_value = stats.pearsonr(df_train[i], df_train['SalePrice'])\n    lst_pearson.append([i, pearson_coef, p_value])\n\ndf_pearson = pd.DataFrame(lst_pearson, columns=['Key Indpendent Variable', 'Pearson Coefficient', 'P-value'])\ndf_pearson = df_pearson.set_index(['Key Indpendent Variable'])\ndf_pearson\n","37b5cfb0":"# Descriptive stats of training set saleprice\n\ndf_train_desc = df_train.describe()\ndf_train_desc = df_train_desc[['SalePrice']]\ndf_train_desc\n","ba41a5b9":"# Predict sale prices using comp data\n\ny_hat_comp = lm.predict(df_comp[lst_ind_var])\n\n# Load the results into dataframe\n\ndf_comp_sub = pd.DataFrame({'Id':df_comp.Id, 'SalePrice':y_hat_comp})\n","023f2912":"# Explore submission dataset\n\ndf_comp_sub['Id'] = df_comp_sub['Id'].astype(int)\ndf_comp_sub.head()\n","f37ffcc9":"# Explore submission dataset\n\ndf_comp_sub.shape\n","3c25e45c":"# Export sales price data to submission csv file\n\ndf_comp_sub.to_csv('\/kaggle\/working\/house_prices_submission.csv', index=False)\n","7eee68c3":"### Descriptive statistics of SalePrice\n\nHere are the descriptive stats for SalePrice.\n","2e391b5f":"### Evaluate statistical significance with Pearson Coefficient and P-value","1aab2e32":"### Train and fit model with key features\n\nWe train our multiple linear regression model with our key features.\n","dfc5d298":"# kaggle-competition-predict-house-prices\n---\n## Project: Kaggle Competition Predict House Prices\n\n### Summary\n\n* Provided a solution to the business problem of predicting house prices using the Ames Housing dataset. The solution also served as an entry into the kaggle closed competition [House Prices: Advanced Regression Techniques](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/overview 'House Prices: Advanced Regression Techniques').\n* Used a variety of data wrangling and feature engineering techniques to prepare and analyze the datasets. \n* Employed multiple linear regression to build a model using the training dataset to the predict the SalePrice of houses.\n* Compared actual vs predicted SalePrice using the model. Results were mixed, indicating the model could be improved.\n\n### Notebooks\n\n> [Jupyter notebook on GitHub](https:\/\/github.com\/burrittresearch\/kaggle-competition-predict-house-prices\/blob\/master\/kaggle-competition-predict-house-prices.ipynb 'Notebook')  \n> [Jupyter notebook on kaggle](https:\/\/www.kaggle.com\/jonathanburritt\/kaggle-competition-predict-house-prices 'Notebook')\n\n### Wayne Burritt, Burritt Research\n\n> [wburritt@burrittresearch.com](mailto:wburritt@burrittresearch.com?subject=Info)  \n> [burrittresearch.com](https:\/\/burrittresearch.com 'Burritt Research Website')  \n> [github.com\/burrittresearch](https:\/\/github.com\/burrittresearch 'Burritt Research GitHub')  \n> [linkedin.com\/in\/burrittresearch](https:\/\/www.linkedin.com\/in\/burrittresearch 'Burritt Research LinkedIn')  \n> [@burrittresearch](https:\/\/twitter.com\/burrittresearch\/ 'Burritt Research Twitter')\n","411df08d":"### Drop uncommon features","8265cc0a":"# Data Wrangling","2079274f":"### Analysis of model results\n\n> Actual vs predicted values of SalePrice are close, but the model is imprecise.\n\n> RMSE is 39,735, which is .22 of the mean of all sale prices 180,921, indicating the model is imprecise.\n\n> The R-squared value is .77, which indicates that 77% of the variation in sales prices is due to our key independent variables, which is acceptable.\n\n> The pearson coefficients of the independent variables indicate both strong and weak linear\/predictor relationship.\n\n> The p-value of of the independent variables are extremely low, indicating the correlation between the  \nindependent and dependent variables is significant.\n","d2c45951":"### Explore new datasets\n\nWe explore the new datasets.\n","e75e7949":"### Determine key features\n\nBased on domain research and experience, we select the following features for our model.\n","da7e53a7":"### Load libraries","7084918e":"### Explore remaining missing data","1883bfa2":"### Encode categorical variables: Competition dataset","7e77e140":"# Explore and compare datasets\n\nWe explore the datasets for an idea of the scope of the datasets.\n\n> The training dataset has 81 features and 1,460 instances.\n\n> The competition dataset has 80 features and 1,459 instances.","138632d3":"### Replace missing data training dataset\n\nNow, we want to replace missing data with the feature mean.\n","ed4dd63e":"### Plot sample key features\n\nLet's plot a sample of our features again SalePrice.\n","59f2e82c":"### Encode categorical variables: Training dataset\n\nNow that we have a good idea of our data types, we encode categorical variables into suitable numeric variables for both datasets.\n","5c0c779c":"# Model development","193eb3f6":"### Use lasso regression to improve model\n\n> Lasso regression did not improve the model.\n","54e9f58d":"### Use ridge regression to improve model\n\n> Ridge regression did not improve the model.\n","a41df810":"### Evaluate the model with Root Mean Squared Error, R2 score, Mean Squared Log Error, and Root Mean Squared Log Error\n\n> Actual vs predicted values of SalePrice are close, but the model is imprecise.\n\n> The R-squared value is .77, which indicates that 77% of the variation in sales prices is due to our key independent variables, which is acceptable.","4dc2e10b":"### Export sales price data to submission csv file\n\nFinally, we export the sale price data for submission.\n","119c6560":"### Explore missing data\n\nNext we explore the missing data for each feature.","e5220a0b":"### Reverify features between datasets","25313024":"### Verify features between datasets\n\nWe verify the difference in features, which is 'SalePrice' as expected.\n","ef72c966":"### Plot training model actual vs predicted house prices (sample)\n\nNow, we'll plot a sample of actual vs predicted house prices.\n","bf447336":"### Explore model results","cfafc7d7":"### Explore data","2e51ed4f":"### Load data\n\nWe begin by loading data.","3b3c7b5b":"### Export training dataset features\n\nWe export the training dataset features to a csv file so we can easily adjust the model features by hand.\n","c89cfbdd":"### Reverify features between datasets\n","7e589b89":"### Apply model to competition data\n\nWe apply the model to the competition data.\n","92e03543":"### Explore data type\n\nWe take a look at the data type for each dataset to discover our numerical and categorical variables.","34bc1edd":"# Acquire and load data","0e6b91ec":"### Reverify features between datasets\n\nWe now verify the datasets again.","9c68dd92":"### Explore training model actual vs predicted house prices\n\nLet's take a look at a sample of actual vs predicted house prices.\n","00cd9f30":"### Explore correlation\n\nWe begin our model development with an exploration of correlation between a sample of features and SalePrice.\n","4179a4eb":"# Competition submission","59c1ec7c":"### Replace missing data competition dataset","a55ea428":"# Model evaluation","6dc2cd41":"### Explore remaining missing data\n\n\nFollowing our encoding of categorical features, we want to reverify our datasets. \n\n> The training dataset now has 256 features and 1,460 instances.\n\n> The competition dataset has 255 features and 1,459 instances.\n","cb3058a8":"# Model results","71388f19":"# Feature engineering","23a925cc":"### Drop features\n\nNow, we drop features that have a high level of missing values from each dataset.\n"}}