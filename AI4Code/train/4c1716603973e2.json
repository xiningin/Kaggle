{"cell_type":{"4e181cab":"code","756c1088":"code","1e3a4de5":"code","f9190f26":"code","22e5cebb":"code","5b1ab9d0":"code","cdb67ad5":"code","acaac7cd":"code","2b8bc137":"code","09b8c584":"code","94719707":"code","9c3997f9":"markdown","1b56f0fc":"markdown"},"source":{"4e181cab":"from tensorflow.keras.datasets import imdb\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\nfrom tensorflow.keras.models import model_from_json\n\nimport numpy as np\nprint(np.__version__)\nimport math","756c1088":"(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)","1e3a4de5":"# [print('{}\\n'.format(X_train[i])) for i in range(len(X_train)) if i < 5]\n\nprint('=== Data ====\\n{}\\n\\n=== Sentiment ====\\n{}\\n\\n{}'.format(\n    X_train[5], y_train[5], type(X_train)))","f9190f26":"# Default value in load_data\nINDEX_FROM = 3\n\n# Download word index and prepare word id\nword2id = imdb.get_word_index()\nword2id = {word:(word_id + INDEX_FROM) for (word, word_id) in word2id.items()}\n# Labelling predefined value to prevent error\nword2id[\"<PAD>\"] = 0\nword2id[\"<START>\"] = 1\nword2id[\"<UNK>\"] = 2\nword2id[\"<UNUSED>\"] = 3\n\n# Prepare id to word\nid2word = {value:key for key, value in word2id.items()}\n\nprint('=== Tokenized sentences words ===')\nprint(' '.join(id2word[word_id] for word_id in X_train[5]))","22e5cebb":"pad_size = 1000\nX_train_pad = pad_sequences(X_train, maxlen=pad_size)\nX_test_pad = pad_sequences(X_test, maxlen=pad_size)","5b1ab9d0":"vocab_size = len(word2id)\ninput_dim = math.ceil(vocab_size \/ 2)\nprint('Len Vocab: {}, input_dim: {}'.format(len(word2id), input_dim))\nembedding_size = math.ceil(vocab_size**0.25)\noutput_units = 1\n\nmodel=Sequential()\nmodel.add(Embedding(input_dim=input_dim, output_dim=embedding_size,\n    input_length=pad_size))\nmodel.add(LSTM(units=100))\nmodel.add(Dense(output_units, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', \n             optimizer='adam', \n             metrics=['accuracy'])\n\nprint(model.summary())","cdb67ad5":"train_size = math.ceil(0.8 * len(X_train))\n\nX, y = X_train_pad[:train_size], y_train[:train_size]\nX_val, y_val = X_train_pad[train_size:], y_train[train_size:]","acaac7cd":"batch_size = 64\nepochs = 5\n\nmodel.fit(X, y, validation_data=(X_val, y_val), batch_size=batch_size,\n          epochs=epochs, shuffle=True)","2b8bc137":"# Saving structure and weights\nmodel_structure = model.to_json()\nwith open('model_structure.json', 'w') as f:\n    f.write(model_structure)\n    \nmodel.save_weights('model_weights.h5')","09b8c584":"# Load and compile model\n\nwith open('model_structure.json', 'r') as f:\n    loaded_model_json = f.read()\n    loaded_model = model_from_json(loaded_model_json)\n\nloaded_model.load_weights('model_weights.h5')\nloaded_model.compile(loss='binary_crossentropy', \n             optimizer='adam', \n             metrics=['accuracy'])","94719707":"scores = loaded_model.evaluate(X_test_pad, y_test, verbose=0)\nprint('Model Accuracy:', scores[1])","9c3997f9":"## Train Model","1b56f0fc":"## Exploring Data"}}