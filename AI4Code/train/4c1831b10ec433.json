{"cell_type":{"599891ed":"code","42222b73":"code","3c286557":"code","6a5b5a45":"code","992d488a":"code","146abe6b":"markdown","c3b6d3ef":"markdown","e3393896":"markdown","703cffd2":"markdown","61cd9e81":"markdown","e1391af3":"markdown"},"source":{"599891ed":"import pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2021\/test.csv\")\ntrain","42222b73":"cat_features = [\n    \"cat0\", \"cat1\", \"cat2\", \"cat3\", \"cat4\", \"cat5\", \"cat6\", \"cat7\", \n    \"cat8\", \"cat9\"\n]\n\ncont_features = [\n    \"cont0\", \"cont1\", \"cont2\", \"cont3\", \"cont4\",\n    \"cont5\", \"cont6\", \"cont7\", \"cont8\", \"cont9\", \"cont10\", \n    \"cont11\", \"cont12\", \"cont13\"\n]","3c286557":"from sklearn.preprocessing import LabelEncoder\n\nnew_cat_features = []\n\ndef label_encode(train_df, test_df, column):\n    le = LabelEncoder()\n    new_feature = \"{}_le\".format(column)\n    le.fit(train_df[column])\n    train_df[new_feature] = le.transform(train_df[column])\n    test_df[new_feature] = le.transform(test_df[column])\n    return new_feature\n\nfor feature in cat_features:\n    new_cat_features.append(label_encode(train, test, feature))\n\ntrain","6a5b5a45":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom lightgbm import LGBMRegressor\n\nn_folds = 10\n\nskf = KFold(n_splits=n_folds, random_state=2021, shuffle=True)\n\ntrain_oof = np.zeros((300000,))\ntest_preds = 0\n\nfull_features = []\nfull_features.extend(new_cat_features)\nfull_features.extend(cont_features)\n\nlgbm_params = {\n    \"random_state\": 2021,\n    \"metric\": \"rmse\",\n    \"n_jobs\": 6,\n    \"early_stopping_round\": 200,\n    \"cat_features\": [x for x in range(len(new_cat_features))],\n    \"reg_alpha\": 9.03513073170552,\n    \"reg_lambda\": 0.024555737897445917,\n    \"colsample_bytree\": 0.2185112060137363,\n    \"learning_rate\": 0.003049106861273527,\n    \"max_depth\": 65,\n    \"num_leaves\": 51,\n    \"min_child_samples\": 177,\n    \"n_estimators\": 1600000,\n    \"cat_smooth\": 93.60968300634175,\n    \"max_bin\": 537,\n    \"min_data_per_group\": 117,\n    \"bagging_freq\": 1,\n    \"bagging_fraction\": 0.6709049555262285,\n    \"cat_l2\": 7.5586732660804445,\n}\n\nfor fold, (train_index, test_index) in enumerate(skf.split(train, train[\"target\"])):\n    print(\"-------> Fold {} <--------\".format(fold + 1))\n    x_train, x_valid = pd.DataFrame(train.iloc[train_index]), pd.DataFrame(train.iloc[test_index])\n    y_train, y_valid = train[\"target\"].iloc[train_index], train[\"target\"].iloc[test_index]\n    \n    x_train_features = pd.DataFrame(x_train[full_features])\n    x_valid_features = pd.DataFrame(x_valid[full_features])\n\n    model = LGBMRegressor(\n        **lgbm_params\n    )\n    model.fit(\n        x_train_features[full_features], \n        y_train,\n        eval_set=[(x_valid_features[full_features], y_valid)],\n        verbose=100,\n    )\n    oof_preds = model.predict(x_valid_features[full_features])\n    test_preds += model.predict(test[full_features]) \/ n_folds\n    train_oof[test_index] = oof_preds\n    print(\"\")\n    \nprint(\"--> Overall results for out of fold predictions\")\nprint(\": RMSE = {}\".format(mean_squared_error(train_oof, train[\"target\"], squared=False)))","992d488a":"preds = test_preds.tolist()\ntest_ids = test[\"id\"].tolist()\n\nsubmission = pd.DataFrame({\"id\": test_ids, \"target\": preds})\nsubmission.to_csv(\"submission.csv\", index=False)","146abe6b":"If you find this kernel useful, please upvote!","c3b6d3ef":"# Introduction\n\nA simple LightGBM model using `LabelEncoder` for categorical values. Further tuned by hand to find a good balance between fit and LB score. Note that the model is very likely overfit. This model may be most useful if combined with other ensemble methods. Note that no significant feature engineering was employed by this model.\n\n## Credits\n\n* [LGBM Goes brrr!](https:\/\/www.kaggle.com\/maunish\/lgbm-goes-brrr) for initial tuning of LGBM parameters.","e3393896":"# Generate Model\n\nHere we'll use `KFold` cross validation, but produce predictions out-of-fold for each fold.","703cffd2":"Categories must be converted to `int` types for LightGBM to use them as categorical. We'll use `LabelEncoder` to do this.","61cd9e81":"# Feature Definitions","e1391af3":"# Generate Results\n\nThe test predictions were generated from each fold. Collect them here and build submission file."}}