{"cell_type":{"b29d953f":"code","eecc2747":"code","96df26aa":"code","83cfdcb4":"code","fd269ed2":"code","ce24be92":"code","b4501a2e":"markdown","65b63cf5":"markdown","2fd7daea":"markdown","14760f3f":"markdown","173cfc5d":"markdown","f3a0ff88":"markdown","0e475887":"markdown"},"source":{"b29d953f":"# neural network in pytorch\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom math import sqrt, exp, pi, floor\n\ntorch.manual_seed(1)","eecc2747":"class Data(Dataset):\n    # Constructor\n    def __init__(self, x, y):\n        self.x = torch.Tensor(np.asarray(x).astype(np.float16))\n        # define output\n        self.y = torch.Tensor(y.reshape([-1, 1]).astype(np.float16))\n        self.len = self.x.shape[0]\n\n    # Getter\n    def __getitem__(self, index):\n        return self.x[index], self.y[index]\n\n    # Get Length\n    def __len__(self):\n        return self.len\n\n\nclass Net(nn.Module):\n    def __init__(self, dropout_p, signallength):\n        super(Net, self).__init__()\n        self.drop = nn.Dropout(p=dropout_p)\n        self.batchnorm1 = nn.BatchNorm1d(num_features=signallength)\n        self.CNN1 = nn.Conv1d(in_channels=1, out_channels=4, kernel_size=30, stride=5)\n        self.maxpool1 = nn.MaxPool1d(kernel_size=5, stride=2)\n        self.CNN2 = nn.Conv1d(in_channels=4, out_channels=16, kernel_size=30, stride=5)\n        self.maxpool2 = nn.MaxPool1d(kernel_size=5, stride=2)\n        self.sigmoid = nn.Sigmoid()\n        self.relu = nn.functional.leaky_relu\n        self.linear = nn.Linear(16*11, 100)\n        self.linear2 = nn.Linear(100, 10)\n        self.linear3 = nn.Linear(10, 1)\n\n\n    def forward(self, activation):\n        activation = activation.view([activation.shape[0], 1, activation.shape[1]])\n        activation = self.CNN1(activation)\n        # print(activation.shape)\n        activation = self.maxpool1(activation)\n        # print(activation.shape)\n        # activation = self.relu(activation)\n        # print(activation.shape)\n        activation = self.CNN2(activation)\n        # print(activation.shape)\n        activation = self.maxpool2(activation)\n        # print(activation.shape)\n        # activation = self.relu(activation)\n        # print(activation.shape)\n        activation = activation.view(-1, 16*11)\n        # print(activation.shape)\n        activation = self.relu(self.linear(activation), negative_slope=0.1)\n        # print(activation.shape)\n        activation = self.relu(self.linear2(activation), negative_slope=0.1)\n        activation = self.sigmoid(self.linear3(activation))\n        return activation","96df26aa":"def accuracy(model, data_set, test_set, epoch):\n    model.eval()\n    mr = model(data_set.x).detach().numpy()\n    mt = model(test_set.x).detach().numpy()\n    yr = data_set.y.detach().numpy()\n    yt = test_set.y.detach().numpy()\n\n    error_r = np.abs((yr - np.round(mr, 0))).mean()\n    error_t = np.abs((yt - np.round(mt, 0))).mean()\n    mean_positive = mr[yr == 1].mean()\n    mean_negative = mr[yr == 0].mean()\n    ratio = mean_positive\/mean_negative\n    model.train()\n    if (epoch)%300 == 0:\n        plt.figure(1)\n        plt.subplot(211)\n        plt.plot(mt, '-o')\n        plt.subplot(212)\n        plt.plot(mr, '-o')\n        plt.title(str(epoch))\n        plt.savefig(str(epoch) + '.png')\n        plt.close()\n    return error_r, mean_positive, mean_negative, ratio, error_t","83cfdcb4":"def train(f, data_set, test_set, model, criterion, train_loader, optimizer, epochs=100):\n    LOSS = []\n    ACC_train = []\n    ACC_test = []\n    for epoch in range(epochs):\n        for x, y in train_loader:\n            optimizer.zero_grad()\n            yhat = model(x)\n            loss = criterion(yhat, y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        LOSS.append(loss.item())\n        error_r, mean_positive, mean_negative, ratio, error_t = accuracy(model, data_set, test_set, epoch)\n        ACC_train.append(1 - error_r)\n        ACC_test.append(1 - error_t)\n        newstr = ','.join([str(epoch), str(error_r),str(mean_positive),str(mean_negative), str(ratio),str(error_t), str(loss.item())])\n        f.write(newstr + '\\n')\n        if epoch % 500 == 0:\n            print(newstr)\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = param_group['lr']*.99\n\n    # Final Plot for results\n    from sklearn.metrics import precision_recall_curve\n\n    y_true = test_set.y.detach().numpy()\n    model.eval()\n    y_scores = model(test_set.x).detach().numpy()\n    precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n    fig, ax1 = plt.subplots()\n    color = 'tab:red'\n    ax1.plot(LOSS, color=color)\n    ax1.set_xlabel('Iteration', color=color)\n    ax1.set_ylabel('total loss', color=color)\n    ax1.set_ylim([0,0.1])\n    ax1.tick_params(axis='y', color=color)\n\n    ax2 = ax1.twinx()\n    color = 'tab:blue'\n    ax2.set_ylabel('accuracy', color=color)  # we already handled the x-label with ax1\n    ax2.plot(ACC_test, color=color)\n    ax2.tick_params(axis='y', color=color)\n    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n    plt.title('Precision: {:0.2f}, Recall: {:0.2f}'.format(precision[0], recall[0]))\n    plt.savefig('Percision-Recall.png')\n    plt.close()\n    return LOSS\n","fd269ed2":"def gauss(n=11,sigma=1):\n    r = range(-int(n\/2),int(n\/2)+1)\n    return [1 \/ (sigma * sqrt(2*pi)) * exp(-float(x)**2\/(2*sigma**2)) for x in r]\n\ndef format_into_xy(df):\n    gfilt = np.asarray(gauss())\n    rowi = df.loc[0, :]._ndarray_values[1:]\n    rowi = np.convolve(rowi, gfilt)\n    xfft = np.abs(np.fft.fft(rowi)[:floor(len(rowi) \/ 2)])\n    # xfft= rowi\n    arr_len = xfft.shape[0]\n    arr_wid = len(df)\n    sl = [] # Use this to delete values from the data set if there is a problem with them\n    y = np.zeros([arr_wid - len(sl), 1])\n    x = np.zeros([arr_wid - len(sl), arr_len])\n    skip = 0\n    for i in range(arr_wid):\n        if i in sl:\n            print('skip')\n            skip +=1\n        else:\n            rowi = df.loc[i, :]._ndarray_values[1:]\n            rowi= np.convolve(rowi, gfilt)\n            xfft = np.abs(np.fft.fft(rowi)[:floor(len(rowi) \/ 2)])\n            x[i - skip, :] = ((xfft - np.mean(xfft)) \/ np.std(xfft))\n            y[i - skip] = df.loc[i - skip, :]._ndarray_values[0]\n            if y[i - skip] == 2:\n\n                y[i - skip] = 1\n            else:\n                y[i - skip] = 0\n    return x, y\n","ce24be92":"import os\nprint(os.listdir(\"..\/input\"))\ndf = pd.read_csv('..\/input\/exoTrain.csv', header=0, index_col=False)\ndf2 = pd.read_csv('..\/input\/exoTest.csv', header=0, index_col=False)\nx, y = format_into_xy(df)\nx2, y2 = format_into_xy(df2)\nf = open('log.csv', 'w+')\nfor col in ['margin']:\n    data_set = Data(x, y)\n    test_set = Data(x2, y2)\n    train_loader = DataLoader(dataset=data_set, batch_size=data_set.len)\n    criterion = nn.BCELoss()\n    learning_rate = 0.1\n    model = Net(0.05, x.shape[1])\n    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.5)\n    model.train()\n    LOSS = train(f, data_set, test_set, model, criterion, train_loader, optimizer, epochs=4000)\nf.close()","b4501a2e":"Nothing left to do but import the test and train datasets. Define the loss function and the model metaparameters and see how well the model can perform.","65b63cf5":"Defining data class and neural net - 1D CNN","2fd7daea":"Define Training Method with a plot summarizing performance at the end","14760f3f":"Importing Libraries","173cfc5d":"A 1D CNN model of the intensity data was created using the training data and validated against the test data. Since the data set is very skewed (<1:100 positive:negative) the evaluation criteria used for the model was the Precision and Recall of 0.833 and 1.0 respectively.\n\nTo increase accuracy the raw input data had two modifications. A Gaussian filter along the time signal to clean up outliers and the time-domain singals were converted into frequency domain since the sampling was at a consistent rate this should be an acceptable change. The final model performed reasonably well but had one FP on the test set. The Confusion Matrix of the model looks like this:\nTN = 564;\nFP = 1;\nFN = 0;\nTP = 5;\n\nThe model has a high degree of certainty that star 372 (index 0) of the test set has an ExoPlanet, or needs an explaination as to the cyclical behavior if it is not an exoplanet. I am okay with this (only) misclassfication because if it isn't an exoplanet, it is still the exact behavior we are trying to detect. I am calling this a win. See the star below\n![Star372.png](attachment:Star372.png)","f3a0ff88":"Helper Function to define model performance along the way. Every 100 epochs the predictions are being logged of both the training and the testing data sets to monitor progress. Since the data sets are ordered (Positive values first, and negative values second) performance is easy to see throughout.\n![169.png](attachment:169.png)\n![959.png](attachment:959.png)\n![3999.png](attachment:3999.png)","0e475887":"Functions to pass a gaussian smoothing filter over the data set then convert it to frequency space. Since we are looking for orbits, the effects we are looking for should happen at a fixed frequency, therefore the frequency-space format of the data might be more interesting. After converting to frequency space, the amplitudes are normallized.\n"}}