{"cell_type":{"13b1b4c6":"code","245ecf46":"code","db66fbf1":"code","e42c260a":"code","019e7b85":"code","6db3a0de":"code","f7d46d38":"code","c1767c89":"code","5a742f44":"code","a9353c80":"code","33ed3619":"code","dccd86c0":"code","c4970732":"code","653f2b38":"code","1ce97000":"code","1c6fcee9":"code","cbfa604d":"code","cdde1ebd":"code","a381c75b":"code","fa762a5c":"code","ef917ee3":"code","b6be19ba":"code","152e36a7":"code","ebdf6d18":"code","204c8a83":"code","7ddfb560":"code","e3cd12ed":"code","675e32c8":"code","a2ffeb97":"code","ab6a5c30":"code","729744ca":"code","2314297e":"code","7cccde48":"code","29d41556":"code","5ae6c12b":"code","9a53f2fa":"code","f696eb5f":"code","795cd945":"code","641b57c7":"code","eb1b017f":"code","b10b0fcc":"code","7a5d7ac8":"code","23e9b6b2":"code","7a165258":"code","1d92e2bc":"code","a19c9c6e":"code","63dbeefb":"code","3ea7cd26":"code","ccd8481d":"code","0cf8aa32":"code","a7cbee89":"code","cb70fa60":"code","4d0b4f1d":"code","b7f2041a":"code","c3ca506e":"code","7e79edd1":"code","f8862b80":"code","9011c927":"code","494f734a":"code","c3739697":"code","485aaa5c":"code","a88ac578":"code","917b8f54":"code","f32795a4":"code","2628b240":"code","5599609c":"code","536364f7":"code","15b2a1c9":"code","4b315838":"code","1e0b6560":"code","6fd3385e":"code","758b47dd":"code","8e4f9eb0":"code","8b58427a":"code","3fd23520":"code","1a0973d3":"code","7e242946":"code","932cee95":"code","b2e77418":"code","cb6ab024":"code","9000d6d8":"code","5266131e":"code","05622044":"code","08651b2c":"code","1a74909b":"markdown","f6899df5":"markdown","a5b6a2e6":"markdown","2009539c":"markdown","ae6334c5":"markdown","2f38052b":"markdown","df481c14":"markdown","6991753a":"markdown","ce654358":"markdown","e288c8b0":"markdown","c8bea0d2":"markdown","9f540a78":"markdown","8a58b0c3":"markdown","9e03891a":"markdown","85fb71c2":"markdown","9f557fa3":"markdown","884b2c67":"markdown","5c06c724":"markdown","49f51af2":"markdown","5f7c9364":"markdown","42354ba6":"markdown","cbebc6c8":"markdown"},"source":{"13b1b4c6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas_profiling\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","245ecf46":"# Download the data from repository for EDA\ndf1=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\n\ndf3=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n","db66fbf1":"# Get frist hand knowledge about the data\n#profile = df1.profile_report(title='Pandas Profiling Report')\n#profile.to_file(output_file=\"train_pandas_profiling.html\")\n#profile ","e42c260a":"df1.head(5)","019e7b85":"\n    ## Columns with numerical values are PassengerId,Survived(0 or 1),Pclass(1,2 or 3),Age(.42 to 80 yrs),SibSp(0 to 8),Parch,Fare(0 to 512) \ndf1.columns.values","6db3a0de":"df1.info()","f7d46d38":"df1.describe()","c1767c89":"df1[['Cabin']].groupby('Cabin', as_index= False).sum().sort_values(by= 'Cabin', ascending= False)","5a742f44":"#Adding a new Column 'has_cabin'\ndf1['has_cabin']=df1['Cabin'].apply(lambda x:0 if type(x)==float else 1)\ndf1= df1.drop('Cabin', axis=1)\ndf1.head()","a9353c80":"#Adding a new column as 'has_family'by combining SibSp and Parch\ndf1['has_family']= df1['SibSp']+df1['Parch']+1\ndf1['has_family']=df1['has_family'].apply(lambda x:0 if x==1 else 1)\ndf1=df1.drop(['SibSp', 'Parch'], axis=1)\ndf1.head()","33ed3619":"#Lets treat the missing values in Age column by imputing with mean value\nmean_1=df1['Age'].mean()\nmean_1","dccd86c0":"df1['Age']= df1['Age'].fillna(mean_1)","c4970732":"#Adding a new column with age group\ndef age_group(i):\n    if (i <= 1):\n        return 'Infant'\n    elif (i>1) & (i <=3):\n        return 'Toddler'\n    elif (i>3) & (i<=12):\n        return 'Kid'\n    elif (i>12) & (i<=18):\n        return'Teen'\n    elif (i>18) & (i<= 60):\n        return 'Adult'\n    else:\n        return'Old'\ndf1['age_group']=df1.apply(lambda x: age_group(x['Age']),axis=1)\ndf1=df1.drop('Age', axis=1)\ndf1.head()","653f2b38":"df1.info()","1ce97000":"#df1 = df1.dropna(subset=['Age','Embarked'])\n#df1 = df1.dropna(subset=['Embarked'])\ndf1.groupby('Embarked').count()","1c6fcee9":"# as Emabrked S has maximum values, lets impute Null values with S\ndf1['Embarked']=df1['Embarked'].fillna('S')","cbfa604d":"# Lets generalise Fare column\ndef fare_group(i):\n    if i <= 8:\n        return 'group1'\n    elif (i>8) & (i <=24):\n        return 'group2'\n    elif (i>24) & (i <=50):\n        return 'group3'\n    elif (i>50) & (i<=100):\n        return 'group4'\n    else:\n        return 'group5'\ndf1['fare_group']= df1.apply(lambda x:fare_group(x['Fare']), axis=1)\ndf1=df1.drop('Fare', axis=1)\ndf1.info()\n    ","cdde1ebd":"\"\"\"\"\"total = df1.isnull().sum().sort_values(ascending=False)\npercent = (df1.isnull().sum()\/df1.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Missing Percent'])\nmissing_data['Missing Percent'] = missing_data['Missing Percent'].apply(lambda x: x * 100)\nmissing_data.loc[missing_data['Missing Percent'] > 0][:10]\"\"\"\"\"\n","a381c75b":"#The above data shows, there are about 6 object type variables in df1, need to assign numbers prior to our anlaysis\nfrom sklearn import preprocessing\ndf1_catagorical= df1.select_dtypes(include=['object'])\ndf1_catagorical\n","fa762a5c":"# assigning numbers to object columns entries-apply label encoder to df1_catagorical\nle = preprocessing.LabelEncoder()\ndf1_catagorical = df1_catagorical.apply(le.fit_transform)\ndf1_catagorical.head()\n","ef917ee3":"#dropping the catagorical columns and adding the new Catagorical columns\ndf1= df1.drop(df1_catagorical.columns, axis =1)\ndf1=pd.concat([df1, df1_catagorical], axis=1)\ndf1.info()","b6be19ba":"df1.sample(5)","152e36a7":"df1=df1.drop('Name', axis=1)\ndf1=df1.drop('Ticket', axis=1)","ebdf6d18":"X= df1.drop('Survived',axis=1)\ny= df1['Survived'].astype('category')","204c8a83":"#Lets check the Testing data\ndf3.info()","7ddfb560":"#Impute missing Age values with mean\nmean_a=df3['Age'].mean()\nmean_a","e3cd12ed":"df3['Age']=df3['Age'].fillna(mean_a)\ndf3.info()\n","675e32c8":"#lets add a column with Cabin info\ndf3['has_cabin']=df3['Cabin'].apply(lambda x:0 if type(x)==float else 1)\ndf3= df3.drop('Cabin', axis=1)\ndf3.head()","a2ffeb97":"#Adding a new column as 'has_family'by combining SibSp and Parch\ndf3['has_family']= df3['SibSp']+df3['Parch']+1\ndf3['has_family']=df3['has_family'].apply(lambda x:0 if x==1 else 1)\ndf3=df3.drop(['SibSp', 'Parch'], axis=1)\ndf3.head()","ab6a5c30":"#dding a new column for age group\ndf3['age_group']=df3.apply(lambda x: age_group(x['Age']),axis=1)\ndf3=df3.drop('Age', axis=1)\ndf3.head()","729744ca":"df3.info()","2314297e":"#Lets choose 'Fare' column first to impute the value\ndf3[df3['Fare'].isnull()] ","7cccde48":"mean_b=df3['Fare'].mean()\nmean_b","29d41556":"#impute missing Fare values with mean_b\ndf3['Fare']=df3['Fare'].fillna(mean_b)","5ae6c12b":"df3.info()","9a53f2fa":"df3['fare_group']= df3.apply(lambda x:fare_group(x['Fare']), axis=1)\ndf3=df3.drop('Fare', axis=1)\ndf3.info()","f696eb5f":"#df3 has 6 object type coulmns , lets assighn numbers wrt labels\ndf3_catagorical= df3.select_dtypes(include=['object'])\ndf3_catagorical","795cd945":"\ndf3_catagorical = df3_catagorical.apply(le.fit_transform)\ndf3_catagorical.head()","641b57c7":"#dropping the catagorical columns and adding the new Catagorical columns\ndf3= df3.drop(df1_catagorical.columns, axis =1)\ndf3=pd.concat([df3, df3_catagorical], axis=1)\ndf3.info()","eb1b017f":"df3=df3.drop('Name', axis=1)\ndf3=df3.drop('Ticket', axis=1)","b10b0fcc":"df3","7a5d7ac8":"X_test_final=df3","23e9b6b2":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test= train_test_split(X,y, test_size=.3, random_state=50)\ndt= RandomForestClassifier()\ndt.fit(X_train,y_train)","7a165258":"y_pred=dt.predict(X_test)\ny_pred\n","1d92e2bc":"y_pred.size","a19c9c6e":"#converting y_pred to data frame\ny_pred= pd.DataFrame(y_pred)\ny_pred","63dbeefb":"X_test","3ea7cd26":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nprint(classification_report(y_test, y_pred))","ccd8481d":"# Printing confusion matrix\nconfusion_matrix(y_test, y_pred)","0cf8aa32":"# Printing accuracy_score\naccuracy_score(y_test, y_pred)","a7cbee89":"# GridSearchCV to find optimal tuning gdata\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV","cb70fa60":"param_grid = {\n    'max_depth': range(5, 15, 5),\n    'min_samples_leaf': range(50, 150, 50),\n    'min_samples_split': range(50, 150, 50),\n    'criterion': [\"entropy\", \"gini\"]\n}\n\nn_folds = 5\n\n# Instantiate the grid search model\nrfc = RandomForestClassifier()\ngrid_search = GridSearchCV(estimator = rfc, param_grid = param_grid, \n                          cv = n_folds, verbose = 1)\n\n# Fit the grid search to the data\ngrid_search.fit(X_train,y_train)","4d0b4f1d":"# printing the optimal accuracy score and hyperparameters\nprint(\"best accuracy\", grid_search.best_score_)\nprint(grid_search.best_estimator_)","b7f2041a":"# model with mdified optimal hyperparameters\nclf_gini = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n                       max_depth=10, max_features='auto', max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=50, min_samples_split=50,\n                       min_weight_fraction_leaf=0.0, n_estimators=10,\n                       n_jobs=None, oob_score=False, random_state=None,\n                       verbose=0, warm_start=False)\nclf_gini.fit(X_train, y_train)","c3ca506e":"#Lets do the predictions after tuning the Hyper parameters\ny_pred_new=clf_gini.predict(X_test)","7e79edd1":"# Printing accuracy_score prior to hyper parameter Tuning\naccuracy_score(y_test, y_pred)","f8862b80":"# Printing accuracy_score after Hyper parameter tuning\naccuracy_score(y_test, y_pred_new)","9011c927":"print(classification_report(y_test, y_pred))","494f734a":"print(classification_report(y_test, y_pred_new))","c3739697":"# Printing confusion matrix Prior to Hyper parameter Tuning\nconfusion_matrix(y_test, y_pred)","485aaa5c":"# Printing confusion matrix post Hyper parameter Tuning\nconfusion_matrix(y_test, y_pred_new)","a88ac578":"import sklearn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn import metrics\n\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nimport gc # for deleting unused variables\n%matplotlib inline\n\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\nmodel= XGBClassifier()\nmodel.fit(X_train,y_train)","917b8f54":"# make predictions for test data\n# use predict_proba since we need probabilities to compute auc\ny_pred = model.predict_proba(X_test)\ny_pred[:10]","f32795a4":"# evaluate predictions\nroc = metrics.roc_auc_score(y_test, y_pred[:, 1])\nprint(\"AUC: %.2f%%\" % (roc * 100.0))","2628b240":"# hyperparameter tuning with XGBoost\n\n# creating a KFold object \nfolds = 3\n\n# specify range of hyperparameters\nparam_grid = {'learning_rate': [0.2, 0.6], \n             'subsample': [0.3, 0.6, 0.9]}          \n\n\n# specify model\nxgb_model = XGBClassifier(classifier_max_depth=2, n_estimators=100)\n\n# set up GridSearchCV()\nmodel_cv = GridSearchCV(estimator = xgb_model, \n                        param_grid = param_grid, \n                        scoring= 'roc_auc', \n                        cv = folds, \n                        verbose = 1,\n                        return_train_score=True)","5599609c":"# fit the model\nmodel_cv.fit(X_train, y_train)","536364f7":"# cv results\ncv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results","15b2a1c9":"# convert parameters to int for plotting on x-axis\ncv_results['param_learning_rate'] = cv_results['param_learning_rate'].astype('float')\n#cv_results['param_max_depth'] = cv_results['param_max_depth'].astype('float')\ncv_results.head()","4b315838":"# # plotting\nplt.figure(figsize=(16,6))\n\nparam_grid = {'learning_rate': [0.2, 0.6], \n             'subsample': [0.3, 0.6, 0.9]} \n\n\nfor n, subsample in enumerate(param_grid['subsample']):\n    \n\n    # subplot 1\/n\n    plt.subplot(1,len(param_grid['subsample']), n+1)\n    df = cv_results[cv_results['param_subsample']==subsample]\n\n    plt.plot(df[\"param_learning_rate\"], df[\"mean_test_score\"])\n    plt.plot(df[\"param_learning_rate\"], df[\"mean_train_score\"])\n    plt.xlabel('learning_rate')\n    plt.ylabel('AUC')\n    plt.title(\"subsample={0}\".format(subsample))\n    plt.ylim([0.60, 1])\n    plt.legend(['test score', 'train score'], loc='upper left')\n    plt.xscale('log')","1e0b6560":"params = {'learning_rate': 0.2,\n          'max_depth': 2, \n          'n_estimators':100,\n          'subsample':0.9,\n         'objective':'binary:logistic'}\n\n# fit model on training data\nmodel = XGBClassifier(params = params)\nmodel.fit(X_train, y_train)","6fd3385e":"# predict\ny_pred_boost = model.predict_proba(X_test)\ny_pred_boost[:10]","758b47dd":"auc = sklearn.metrics.roc_auc_score(y_test, y_pred[:, 1])\nauc","8e4f9eb0":"#converting y_pred_boost to data frame\ny_pred_boost= pd.DataFrame(y_pred)","8b58427a":"#lets convert y_pred_new to data frame\n\ny_pred_new=pd.DataFrame(y_pred_new)\n","3fd23520":"y_pred_final=model.predict(X_test_final)\ny_pred_final=pd.DataFrame(y_pred_final)\n","1a0973d3":"y_pred_final.info()","7e242946":"X_test_final.info()","932cee95":"# adding Test & Predicted data into a single dataframe\noutput=X_test_final.join(y_pred_final)\noutput\n","b2e77418":"output.columns","cb6ab024":"\noutput.columns.values[8] = 'Survived'\noutput = output.reset_index()\noutput.head()","9000d6d8":"predictions=output[['PassengerId','Survived']]\npredictions","5266131e":"predictions.to_csv('predictions.csv', index= False)","05622044":"feature_importances = pd.DataFrame(model.feature_importances_,\n                                   index = X_train.columns,\n                                    columns=['importance']).sort_values('importance',ascending=False)\nfeature_importances # Used initial model for imprtance feature calculations","08651b2c":"output[['Sex', 'Survived']].groupby('Sex', as_index= False).sum().sort_values(by= 'Survived', ascending= False)\n","1a74909b":"Model accuracy dcreased after fitting the model with optimum Hyper tuning parameters . But model performance certainly improved","f6899df5":"**Conclusion:** From the above results it appears that individual's Sex category had linked to maximum survival. If we look at the survival rate further interms of gender classification , it appears female had the max Survival rate in that category. The next significant factor was 'Pclass' followed by 'Cabin','Fare', 'Age','Embarked','PassengerId' and 'Parch'.","a5b6a2e6":"**Plotting few prediction**","2009539c":"**Feature Engineering**\nLets derive some features as\n1)Has_Cabin\n2)Family\n3)Age_group","ae6334c5":"Lets drop column'Cabin' as it has maximum mising value which is more tha 50 % of total data","2f38052b":"Making predictions","df481c14":"Now lets predict the model with test data provided(X_test_final), Will use the XGBoost model for prediction","6991753a":"The results show that a subsample size of 0.6 and learning_rate of about 0.2 seems optimal. Also, XGBoost has resulted in the highest ROC AUC obtained (across various hyperparameters).\n\nLet's build a final model with the chosen hyperparameters.","ce654358":"Data Cleaning: Check columns with NaN or blank entries- Below output shows Age column has missing entries. Its better to get rid of rows with zero missing entries","e288c8b0":"**Lets see the feature contributing for the prediction and accuracy**","c8bea0d2":"Above data shows missing values.Need to impute null entries","9f540a78":"**Lets run grid search to get the optimal Hyper parameters**","8a58b0c3":"**Prepare the data for Modelling**","9e03891a":"**Lets call & fit the training data to Random forest classifier **\n","85fb71c2":"Lets use boosting method to increase the accuary . Will use XG Boost for this application","9f557fa3":"**Introduction:** This problem statement expects a catagorical out-put as a solution. Passengers survival is expected as out-put based on provided features.\nI will be using RandomForest technique to evaluate as it will show the factors leading to a prediction.Boosting also can be used but again data size matters to get a quality output to increase accuracy.","884b2c67":"df3 which is data to be used for prediction, also needs cleaning as we did for training data set","5c06c724":"We\u2019ll just print 10 ordered features with missing values percentage:","49f51af2":"Accuracy is dropping with hyper parameter tuning. So, will look for boosting opportunities","5f7c9364":"Now lets get rid off rows which do not have entries in 'Age' & 'Embarked Columns","42354ba6":"Let's now try tuning the hyperparameters using k-fold CV. We'll then use grid search CV to find the optimal values of hyperparameters.","cbebc6c8":"**Running the model with best parameters obtained from grid search.**"}}