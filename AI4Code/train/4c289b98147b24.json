{"cell_type":{"109f3994":"code","101e96cc":"code","dfd60d7e":"code","555c591d":"code","55335663":"code","6d10cf83":"code","dcf4d16a":"code","1ebe5067":"code","8a097267":"code","3300c9b6":"code","405b8d0b":"code","38215dd8":"code","8d61a682":"code","74c6a99e":"code","a713f2b3":"code","3baa5e44":"code","d086d96f":"code","6285ae3f":"code","bde5cc64":"code","0b87a9e4":"code","ce4332b1":"code","ff17cbbb":"code","65bbb0a2":"markdown","b24ad070":"markdown","ae09e893":"markdown","9f2916a4":"markdown","3720505a":"markdown","26e3bc5a":"markdown","bb017aa0":"markdown","cb56ed06":"markdown","b0aeef48":"markdown","eb67f2c6":"markdown","4f726f7f":"markdown","4fe7eb1d":"markdown","597551e2":"markdown","bb5964ef":"markdown","cde73c81":"markdown","a0e2df09":"markdown","ea759ec0":"markdown","467dd44f":"markdown","0aa82ea6":"markdown"},"source":{"109f3994":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nfrom sklearn import neighbors\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","101e96cc":"data = pd.read_csv(\"..\/input\/bank-marketing-term-deposit\/bank_customer_survey.csv\")","dfd60d7e":"data.head()","555c591d":"data.isnull().sum()","55335663":"data.describe()","6d10cf83":"data[data['y']==0].describe()","dcf4d16a":"data[data['y']==1].describe()","1ebe5067":"data[ data['age'] > 90 ].head()","8a097267":"data[ data['duration'] < 8 ]['y'].count()","3300c9b6":"from sklearn.preprocessing import LabelEncoder\n\nnewdata = data\nle = LabelEncoder()\nfor col in newdata.columns:\n    if(newdata[col].dtype == 'object'):\n        newdata.loc[:,col] = le.fit_transform(newdata.loc[:,col])\n\nnewdata.head()","405b8d0b":"corr = newdata.corr()\ncorr.style.background_gradient(cmap='coolwarm').set_precision(2)","38215dd8":"X = newdata.iloc[:,:-1].values\ny = newdata.iloc[:,-1].values\n","8d61a682":"#If you want to try KNeighborsClassifier uncomment lines 13,21,and comment  14 and 22\n\nscoresAc = []\nscoresF1 = []\n\npreds = []\nactual_labels = []\n# Initialise the 5-fold cross-validation\nkf = KFold(n_splits=10,shuffle=True)\n\n\nfor i in range(1,15):\n  #model1= neighbors.KNeighborsClassifier(n_neighbors = i)\n  model2= RandomForestClassifier(max_depth=i ,n_estimators = 200,n_jobs= 5) \n  aux1 =[]\n  aux2 = []\n  for train_index,test_index in kf.split(X):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    #estimator = model1.fit(X_train,y_train)\n    estimator = model2.fit(X_train,y_train)\n    \n    predictions = estimator.predict(X_test)\n    scoreF1 = metrics.f1_score(y_test,predictions)\n    accuracy = metrics.accuracy_score(y_test,predictions)\n    aux1.append(accuracy)\n    aux2.append(scoreF1)\n  \n  scoresAc.append(np.average(aux1))\n  scoresF1.append(np.average(aux2))\n\n#print(\"F1 score: {0}\".format((scoresF1)))\n\n#print(\"accuracy score: {0}\".format((scoresAc)))\nreport = classification_report(y_test, predictions)\nprint(report)\n\nplt.plot(range(1,15), scoresAc, label=\"training accuracy\")\nplt.plot(range(1,15), scoresF1, label=\"F1 accuracy\") \nplt.ylabel(\"score\")\nplt.xlabel(\"max_depth\")\nplt.legend()\n","74c6a99e":"#If you want to try SVC uncomment lines 21,22,and comment  18 and 19\n\n\n\nscoresAc = []\nscoresF1 = []\n\npreds = []\n\n# Initialise the 5-fold cross-validation\nkf = KFold(n_splits=10,shuffle=True)\n\nfor train_index,test_index in kf.split(X):\n  X_train, X_test = X[train_index], X[test_index]\n  y_train, y_test = y[train_index], y[test_index]\n\n  \n  model3 = GaussianNB()\n  estimator = model3.fit(X_train, y_train)\n\n  #model4 = SVC(C=1000, kernel = \"rbf\")\n  #estimator = model4.fit(X_train, y_train)\n\n  predictions = estimator.predict(X_test)\n  scoreF1 = metrics.f1_score(y_test,predictions)\n  accuracy = metrics.accuracy_score(y_test,predictions)\n\n  \nscoresAc.append(accuracy)\nscoresF1.append(scoreF1)\n\nprint(\"F1 score: {0}\".format((np.average(scoreF1))))\n\nprint(\"accuracy score: {0}\".format(np.average(scoresAc)))\n\nreport = classification_report(y_test, predictions)\nprint(report)\n","a713f2b3":"sizeY = data['y'].count()\nprint (\"number of observations:\",sizeY)\n\n#using undersampling\nsizeClass0=data[data['y']==0]['y'].count()\nprint (\"number of observations with class 0:\",sizeClass0)\n\nsizeClass1=data[data['y']==1]['y'].count()\nprint (\"number of observations with class 1:\",sizeClass1)\n\n\n#preprocesing\nnewdata= data\nle = LabelEncoder()\nfor col in newdata.columns:\n    if(newdata[col].dtype == 'object'):\n        newdata.loc[:,col] = le.fit_transform(newdata.loc[:,col])\n\n\n# We are going to create dt with class1 and 5000 elements of class0\n# in testclass0 We save the remaining elements of class0\ndataClass0 = newdata[newdata['y']==0]\ndataClass1 = newdata[newdata['y']==1]\n\nperm = np.random.permutation(sizeClass0)\n\n# underSampling  with sizeClass1 \n\nsplit_point = sizeClass1\n#split_point = int(np.ceil(sizeClass0*0.5))\n\ndataClass0 = dataClass0.values\ndataClass1 = dataClass1.values\n\n\nclass0ForTrain = dataClass0[perm[:split_point].ravel(),:] \n\ntestClass0 = dataClass0[perm[split_point:].ravel(),:] \n\ndt = np.concatenate((class0ForTrain,dataClass1))\n\n\nprint('length of dt contains class1 and 5000 elements of class0', len(dt))\nprint('length of remaining elements of class0 :', len(testClass0))\n\nX = dt[:,:-1]\ny = dt[:,-1]\n\nXTestClass0 = testClass0[:,:-1]\nyTestClass0 = testClass0[:,-1]\n","3baa5e44":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nscoresAc = []\nscoresF1 = []\n\npreds = []\n\nkf = KFold(n_splits=10,shuffle=True)\n\nfor train_index,test_index in kf.split(X):\n  X_train, X_test = X[train_index], X[test_index]\n  y_train, y_test = y[train_index], y[test_index]\n\n  model1 = GaussianNB()\n  estimator = model1.fit(X_train, y_train)\n\n  #model1 = SVC(C=30, kernel = \"rbf\")\n  #estimator = model1.fit(X_train, y_train)\n\n  #model1= RandomForestClassifier(max_depth=16 ,n_estimators = 200) \n  #estimator = model1.fit(X_train, y_train)\n\n  #model1 = neighbors.KNeighborsClassifier(n_neighbors = 2)\n  #estimator = model1.fit(X_train, y_train)\n\n\n  XfinalTest = np.concatenate((X_test,XTestClass0))\n  yfinalTest = np.concatenate((y_test,yTestClass0))\n\n  predictions = model1.predict(XfinalTest)\n\n  scoreF1 = metrics.f1_score(yfinalTest,predictions)\n  accuracy = metrics.accuracy_score(yfinalTest,predictions)\n\n  #predictions = estimator.predict(X_test)\n  #scoreF1 = metrics.f1_score(y_test,predictions)\n  #accuracy = metrics.accuracy_score(y_test,predictions)\n\n  \nscoresAc.append(accuracy)\nscoresF1.append(scoreF1)\n\nprint(\"F1 score: {0}\".format((np.average(scoreF1))))\n\nprint(\"accuracy score: {0}\".format(np.average(scoresAc)))\n\nreport = classification_report(yfinalTest, predictions)\nprint(report)","d086d96f":"sizeY = data['y'].count()\nprint (\"number of observations:\",sizeY)\n\n#undersampling\nsizeClass0=data[data['y']==0]['y'].count()\nprint (\"number of observations with class 0:\",sizeClass0)\n\nsizeClass1=data[data['y']==1]['y'].count()\nprint (\"number of observations with class 1:\",sizeClass1)\n\n\n#preprocesing\nnewdata= data\nle = LabelEncoder()\nfor col in newdata.columns:\n    if(newdata[col].dtype == 'object'):\n        newdata.loc[:,col] = le.fit_transform(newdata.loc[:,col])\n\n\n# We are going to create dt with class1 and 19961 elements of class0\n# in testclass0 We save the remaining elements of class0\ndataClass0 = newdata[newdata['y']==0]\ndataClass1 = newdata[newdata['y']==1]\n\nperm = np.random.permutation(sizeClass0)\n\n# underSampling  with class0ForTrain = sizeClass0 \/ 2\n\n\nsplit_point = int(np.ceil(sizeClass0*0.5))\n\ndataClass0 = dataClass0.values\ndataClass1 = dataClass1.values\n\n\nclass0ForTrain = dataClass0[perm[:split_point].ravel(),:] \n\ntestClass0 = dataClass0[perm[split_point:].ravel(),:] \n\ndt = np.concatenate((class0ForTrain,dataClass1))\n\n\nprint('length of dt contains class1 and 19961 elements of class0', len(dt))\nprint('length of remaining elements of class0 :', len(testClass0))\n\nX = dt[:,:-1]\ny = dt[:,-1]\n\nXTestClass0 = testClass0[:,:-1]\nyTestClass0 = testClass0[:,-1]","6285ae3f":"scoresAc = []\nscoresF1 = []\n\npreds = []\n\nkf = KFold(n_splits=10,shuffle=True)\n\nfor train_index,test_index in kf.split(X):\n  X_train, X_test = X[train_index], X[test_index]\n  y_train, y_test = y[train_index], y[test_index]\n\n  #model1 = GaussianNB()\n  #estimator = model1.fit(X_train, y_train)\n\n  #model1 = SVC(C=30, kernel = \"rbf\")\n  #estimator = model1.fit(X_train, y_train)\n\n  model1= RandomForestClassifier(max_depth=16 ,n_estimators = 200,n_jobs= 5) \n  estimator = model1.fit(X_train, y_train)\n\n  #model1 = neighbors.KNeighborsClassifier(n_neighbors = 2)\n  #estimator = model1.fit(X_train, y_train)\n\n\n  XfinalTest = np.concatenate((X_test,XTestClass0))\n  yfinalTest = np.concatenate((y_test,yTestClass0))\n\n  predictions = model1.predict(XfinalTest)\n\n  scoreF1 = metrics.f1_score(yfinalTest,predictions)\n  accuracy = metrics.accuracy_score(yfinalTest,predictions)\n\n  #predictions = estimator.predict(X_test)\n  #scoreF1 = metrics.f1_score(y_test,predictions)\n  #accuracy = metrics.accuracy_score(y_test,predictions)\n\n  \nscoresAc.append(accuracy)\nscoresF1.append(scoreF1)\n\nprint(\"F1 score: {0}\".format((np.average(scoreF1))))\n\nprint(\"accuracy score: {0}\".format(np.average(scoresAc)))\n\nreport = classification_report(yfinalTest, predictions)\nprint(report)","bde5cc64":"\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import to_categorical\n\nX = newdata.iloc[:,:-1].values\ny = newdata.iloc[:,-1].values\n\nperm = np.random.permutation(y.size)\n\nPRC = 0.80\nsplit_point = int(np.ceil(y.shape[0]*PRC))\nX_train = X[perm[:split_point].ravel(),:] \ny_train = y[perm[:split_point].ravel()]\n\nX_test = X[perm[split_point:].ravel(),:]\n\ny_test = y[perm[split_point:].ravel()]\n\n\ny1 = to_categorical(y)\n\ny_train1 = y1[perm[:split_point].ravel()]\ny_test1 = y1[perm[split_point:].ravel()]\n\n#We standardize features by removing the mean and scaling to unit variance\nsc = StandardScaler()\nX_trainS = sc.fit_transform(X_train)\nX_testS = sc.fit_transform(X_test)","0b87a9e4":"\nmodel = Sequential()\n\n#get number of columns in training data\nn_cols_2 = X.shape[1]\n\n#add layers to model\nmodel.add(Dense(250, activation='relu', input_shape=(n_cols_2,)))\nmodel.add(Dense(250, activation='relu'))\nmodel.add(Dense(250, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))\n\n\n#compile model using accuracy to measure model performance\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","ce4332b1":"model.fit(X_train, y_train1, epochs=20, batch_size=32)\n\npredictions = model.predict(X_test)\n\n\ny_pred1 = predictions[:,0] < 0.5\n\nk = classification_report(y_test, y_pred1)\nprint (k)","ff17cbbb":"#Same neural network but after standardize their features.\nmodel.fit(X_trainS, y_train1, epochs=20, batch_size=32)\n\npredictions = model.predict(X_testS)\n\n\ny_pred1 = predictions[:,0] < 0.5\n\nk = classification_report(y_test, y_pred1)\nprint (k)","65bbb0a2":"![](http:\/\/)","b24ad070":"Now, We are going to try some algorithms with different hyper-parameters.These are :\n* Random Forest\n* k-nearest neighbors \n* Naive Bayes\n* Support vector machine","ae09e893":" Next code we'll transform raw data into an understandable format","9f2916a4":"now we can said sometime more about our data:\n* We have 39922 rows of class 0 and 5289 rows of class 1\n* We have an imbalance dataset (95% vs 5%)\n* Duration looks quite different in  the last 2 tables (mean:221 vs 537 )\n","3720505a":"The subject of this work is to create a **binary classification model** to know whether a client would subscribe to a term deposit.\n\nThis information would be based on phone calls made by marketing campaigns. \n\nOne of the issues that make this dataset interesting is that the dataset is unbalanced, which means we have a frequency bigger in class 0 than in class 1.\n\nTo achieve this model we will explore different algorithms like **RF, NB, KNN SVM , Neural Networks,** and techniques like **\"undersampling\"**.\n\nWe will also have to pay attention to the way we measure the model.\n","26e3bc5a":"tabla![US.png](attachment:US.png)","bb017aa0":"A correlation matrix is a table showing correlation coefficients between variables. Each cell in the table shows the correlation between the two variables.\n\n  If we look at the last column we can look at the correlation of all variables with \"y\". Duration has the biggest correlation with \"y\".Also, the matrix shows us a bigger correction. between attributes related to time.\n","cb56ed06":"\n![NN.png](attachment:NN.png)\n","b0aeef48":"After all these experiments, we learned some lessons.\n\n\nFirstly, as we discussed in class, we should avoid  measuring accuracy for unbalanced data, instead, we should use other metrics such F1, recall, and precision,  since these metrics give us more information about the behavior of our model.\nIn our first try, our  4 models achieved had an accuracy of up to 85%. (Random Forest had  an accuracy of 91%) but F1 was not very well. So, we tried some techniques of undersampling and we got better results.\n\n\nSecondly, we have many tools that choose the best hyperparameters for us\n(like grid-search) . But it is still good, making a chart to show different scores with different hyperparameters.\n.For example in random Forest, we learned that increasing the depth of the tree gives us a better F1 up until  it  starts to converge.\n\nFinally, it is important to understand our dataset, but also what is the interest or  the purpose for our dataset?. Are we more concerned about precision? Or instead, we want to be sure of having the bigger amount of buyers. In particular, we have to think about the cost. For example, The cost of lending to a defaulter is far greater than the loss-business cost of refusing a loan to a non-defaulter. By the other side in promotional mailing: the cost of sending junk mail to a household that doesn't respond is far less than the lost-business cost of not sending it to a household that would have responded.\n\n We should ask these questions. I think we are particularly interested in class 1 (the buyers). Next, we can do other analyzes to arrive at a better model like precision or recall).\n\nIf we are interested in maximizing the rate of calls to buyers; We will choose a model that pays more attention to the sensitivity of class 1. It could be NB or Randomn Forest\n using undersampling .these have a sensitivity (recall) of 79% and 68% respectively.\n\nBut suppose we decided to hire a specialist for the calls that we predicted as buyers. In this case, we are interested in precision. Consequently, Randon Forest and  3-layer neural network have the best accuracy (69% and 61% respectively)\n\nIf I have to choose, I'll say Random forest. Although RF using undersampling could be a good option too. If we observe our different models we have a trade-off between precision and recall.\n\n In conclusion, some of the steps to follow could be:\n* Keep experimenting with neural networks, the ones we used gave us good results. But other variants are worth trying.\n* explore other techniques of undersampling. We applied one of these techniques and they got good results \n* get more information from the dataset and the client.\n* we could think of making a  priority queue with the elements that we predict as class 1. The criteria to choose  elements( that we predicted as class one) would be the probability that they are 1\n","eb67f2c6":"Now, we are gonna do the same, but this time we are using  Undersampling techniques. Undersampling can be defined as removing some observations of the majority class. So we are going to remove a bunch of files (to train our model).","4f726f7f":"**but what happen when  we separete by class \"y\" ?**","4fe7eb1d":"We have 7 numeric attributes. \"Describe()\" give us important information like the mean, min , max , percentiles ,and standard deviation.","597551e2":"First .We check for missing values.","bb5964ef":"The dataset has information about customers and telemarketing campaigns made by a bank. There is a column named \"y\"  in the dataset which tells us whether or not the customer opted for term deposit. This is divided into 17 attributes, being 8 categorical and 9 of factors.\n\nWhat's more exciting is the distribution of \"y\". We have 39922 \"no\" vs 5289 \"yes\".\n\nThat means we have 95% of class 0 and just 5% of class 1.\n\n Finally, the size of the dataset (45211 rows) is big enough for working with some algorithms.\n \nBelow you can see the meaning of each attribute:\n\n\nAge :numeric\n\nJob :Type of job (categorical)\n\nMarital : Marital status\n\nEducation : Level of education of the customer\n\ndefault : Has credit in default? (binary: \"yes\",\"no\")\n\nBalance : average yearly balance, in euros (numeric)\n\nHousing : has housing loan? (binary: \"yes\",\"no\")\n\nLoan : has personal loan? (binary: \"yes\",\"no\")\n\nContact: contact communication type (categorical: \"unknown\",\"telephone\",\"cellular\")\n\nDay : last contact day of the month (numeric)\n\nMonth : last contact month of year (categorical: \"jan\", \"feb\", \"mar\", ..., \"nov\", \"dec\")\n\nDuration : last contact duration, in seconds (numeric), important\n\nCampaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n\nPdays : number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted)\n\nPrevious :  number of contacts performed before this campaign and for this client (numeric)\n\nPoutcome : outcome of the previous marketing campaign (categorical: \"unknown\",\"other\",\"failure\",\"success\")\n\nY : has the client subscribed a term deposit? (binary : 0, 1)\n\n","cde73c81":"![US05.png](attachment:US05.png)","a0e2df09":"Calls with a duration less than 8 are class 0. it means they don't subscribe to a term deposit. It makes sense.","ea759ec0":"We didn't improve the performance of F1-score.Although we improved the sensitivity (recall of NB: 78%).\n\nSo instead of reducing the observations of class 0 to the size of class 1. We are going to removing samples from the majority class to half. ","467dd44f":"Ultimately we are going to try  using a neural network.","0aa82ea6":"tabla con resultados .\n![allALG.png](attachment:allALG.png)"}}