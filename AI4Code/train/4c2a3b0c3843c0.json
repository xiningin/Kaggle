{"cell_type":{"ebfdbbc7":"code","c416add0":"code","b0add49d":"code","45ec443c":"code","99ba32ad":"code","efb91a48":"code","e7add238":"code","7e6530a5":"code","b50a287e":"code","5a1bdac7":"code","b8320b66":"code","639b1822":"code","2762efb6":"code","378d1d5e":"code","55d64e97":"code","711ffd93":"code","4e63239d":"code","a9ac5649":"code","8261fa18":"code","ceb18f67":"code","660c8b30":"code","9f5d3ddc":"code","7f2e74cb":"code","33a76f5d":"code","c64de542":"code","e4d2b582":"code","ace0de45":"code","9bfa8418":"code","16b2d7e2":"code","d24c6da6":"code","fb13483d":"code","6d0c4b0e":"code","f6ac32f1":"code","f6179b56":"code","97426a8f":"code","8a1fac1b":"code","cc781525":"code","b424da0f":"code","ed79cc8f":"code","a590faf9":"code","e16d62de":"code","4f09711e":"code","6858b32a":"code","a9b33b4b":"code","ccb86fa8":"code","89a2bfce":"code","3c3afeab":"code","cdbc38f7":"code","510b6c51":"code","f80078bf":"code","317d8d2b":"code","0b798e28":"markdown","2d080999":"markdown","9f8ef63a":"markdown","ce6f45b4":"markdown","df240349":"markdown","6f87151b":"markdown","bd29b4de":"markdown","c151c086":"markdown","8c5bd8ae":"markdown","3540c85b":"markdown","bac8068c":"markdown","f06f8f44":"markdown","e4db711c":"markdown","b476ec9b":"markdown","2a438056":"markdown","429719b2":"markdown","1017eb60":"markdown","1ccf6fa1":"markdown","0e2e9776":"markdown","a27bd26a":"markdown","e7ef9126":"markdown","a6988bfb":"markdown","7300d618":"markdown","ff41b740":"markdown","7f373f23":"markdown","a46fbd11":"markdown","451926e1":"markdown","9163ebfd":"markdown","aba38952":"markdown","2f727f29":"markdown","2a4e3220":"markdown","1faa3d8c":"markdown","0fcc2e7f":"markdown","996474e5":"markdown","5e4276c9":"markdown","31186253":"markdown","89a05e9e":"markdown","4e7ec6ed":"markdown","b029d315":"markdown","eebe4fa2":"markdown","9794da4c":"markdown","45a898e3":"markdown","65d2975f":"markdown","d3d41790":"markdown","04eae393":"markdown","45f7b587":"markdown","a4bb58f0":"markdown","155f1f11":"markdown","91c3676d":"markdown","f8e64de3":"markdown"},"source":{"ebfdbbc7":"import numpy as np \nimport pandas as pd\nfrom scipy.stats import skew\nimport gc\n\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly.subplots import make_subplots\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c416add0":"%%time\n\n#reading de train dataset\ntrain = pd.read_pickle('..\/input\/riiid-train-data-multiple-formats\/riiid_train.pkl.gzip')","b0add49d":"train.head(10)","45ec443c":"train.info()","99ba32ad":"train.memory_usage(deep=True)","efb91a48":"train.prior_question_had_explanation = train.prior_question_had_explanation.astype('bool')","e7add238":"train.info()","7e6530a5":"train.isnull().sum()","b50a287e":"train.prior_question_elapsed_time = train.prior_question_elapsed_time.fillna(value = train.prior_question_elapsed_time.mean())","5a1bdac7":"# checking the result\ntrain.isnull().sum()","b8320b66":"fig, ax = plt.subplots(figsize = (18, 8))\nax.hist(train.timestamp, bins = 70)\n\n\nax.set_xlabel('Timestamps', fontsize=18);\nax.set_ylabel('Frequency', fontsize=18);\nax.set_title('Timestamp Distribution', fontsize = 20)\nplt.yticks(fontsize = 14)\nplt.xticks(fontsize = 14)\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['left'].set_visible(False);\n\nplt.show()","639b1822":"fig, ax = plt.subplots(figsize = (18, 8))\nax.plot(np.arange(1,100), 1\/np.arange(1,100));\n\nax.set_xlabel('x', fontsize=18);\nax.set_ylabel('1\/x', fontsize=18);\nax.set_title('1\/x behaviour', fontsize = 20)\nplt.yticks(fontsize = 14)\nplt.xticks(fontsize = 14)\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['left'].set_visible(False);\n\nplt.show()","2762efb6":"fig, ax = plt.subplots(figsize = (18, 8))\nax.hist(train.content_id, bins = 200)\n\n\nax.set_xlabel('IDs', fontsize=18);\nax.set_ylabel('Frequency', fontsize=18);\nax.set_title('Content_Id Distribution', fontsize = 20)\nplt.yticks(fontsize = 14)\nplt.xticks(fontsize = 14)\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['left'].set_visible(False);\n\nplt.show()","378d1d5e":"proportion_content_type_id = (train.content_type_id.value_counts().values\/\n                              train.content_type_id.value_counts().values.sum())\n\n# plotting\nfig, ax = plt.subplots(figsize = (18, 8))\nax.bar(['Question', 'Lecture'], proportion_content_type_id)\n\n# layout setup\nax.set_xlabel('Content Type', fontsize=18);\nax.set_ylabel('Proportion', fontsize=18);\nax.set_title('Proportion between Questions and Lectures', fontsize = 20)\nplt.yticks(fontsize = 14)\nplt.xticks(fontsize = 14)\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['left'].set_visible(False);\n\n# adding values on the top of each bar\nrects = ax.patches\nlabels = [str(\"{:.2f}\".format(x)) for x in proportion_content_type_id]\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width() \/ 2, height, label,\n            fontsize=14, ha = 'center', va = 'bottom')\n    \nplt.show()","55d64e97":"train.task_container_id.value_counts()","711ffd93":"frequency_task_container = pd.Series(train.task_container_id.value_counts().values,\n                                     index = (train.task_container_id.value_counts().index).astype('str')).sort_values(ascending=True)\n\n# plotting\nfig, ax = plt.subplots(figsize = (18, 10))\nax.barh(frequency_task_container.tail(40).index, frequency_task_container.tail(40))\n#plt.gca().invert_yaxis()\n\n# layout setup\nax.set_xlabel('Frequency', fontsize=18);\nax.set_ylabel('Task Container Ids', fontsize=18);\nax.set_title('30 Most frequent Task Container', fontsize = 20)\nplt.yticks(fontsize = 14)\nplt.xticks(fontsize = 14)\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['left'].set_visible(False);\n\nplt.show()","4e63239d":"# plotting\nfig, ax = plt.subplots(figsize = (18, 8))\nax.bar(train.user_answer.value_counts().index, train.user_answer.value_counts().values)\n#plt.gca().invert_yaxis()\n\n# layout setup\nax.set_xlabel('Answers', fontsize=18);\nax.set_ylabel('Frequency', fontsize=18);\nax.set_title('Total of each answer was choosen', fontsize = 20)\nplt.yticks(fontsize = 14)\nplt.xticks(fontsize = 14)\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['left'].set_visible(False);\n\nplt.show()","a9ac5649":"answer = train.user_answer.value_counts().drop(index = -1)\nanswer_percent = answer.values \/ answer.values.sum()\n\n# plotting\nfig, ax = plt.subplots(figsize = (18, 8))\nax.bar(answer.index, answer_percent)\n#plt.gca().invert_yaxis()\n\n# layout setup\nax.set_xlabel('Frequency', fontsize=18);\nax.set_ylabel('Answers', fontsize=18);\nax.set_title('Total of each answer was choosen in %', fontsize = 20)\nplt.yticks(fontsize = 14)\nplt.xticks(fontsize = 14)\nplt.xticks([0, 1, 2, 3])\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['left'].set_visible(False);\n\n# adding values on the top of each bar\nrects = ax.patches\nlabels = [str(\"{:.2f}\".format(x)) + '%' for x in answer_percent]\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width() \/ 2, height , label,\n            fontsize=14, ha = 'center', va = 'bottom')\n\n\nplt.show()","8261fa18":"train.head()","ceb18f67":"answered_right = train.answered_correctly.value_counts().drop(index = -1)\nanswered_right_percent = answered_right \/ answered_right.values.sum()\n\n\n# plotting\nfig, ax = plt.subplots(figsize = (18, 8))\nax.bar(answered_right.index, answered_right_percent)\n\n# layout setup\nax.set_xlabel('Answers', fontsize=18);\nax.set_ylabel('Proportion', fontsize=18);\nax.set_title('Proportion of correct answers in all interations', fontsize = 20)\nplt.yticks(fontsize = 14)\nplt.xticks(fontsize = 14)\nplt.xticks([0, 1], ['Wrong', 'Right'])\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['left'].set_visible(False);\n\n# adding values on the top of each bar\nrects = ax.patches\nlabels = [str(\"{:.2f}\".format(x)) for x in answered_right_percent]\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width() \/ 2, height , label,\n            fontsize=14, ha = 'center', va = 'bottom')\n\n\nplt.show()","660c8b30":"fig, ax = plt.subplots(figsize = (18, 8))\nax.hist(train.prior_question_elapsed_time, bins = 100)\n\n\nax.set_xlabel('Time (miliseconds)', fontsize=18);\nax.set_ylabel('Frequency', fontsize=18);\nax.set_title('Time demanded to answer each question in the previous bundle', fontsize = 20)\nplt.yticks(fontsize = 14)\nplt.xticks(fontsize = 14)\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['left'].set_visible(False);\n\nplt.show()","9f5d3ddc":"skew(train.prior_question_elapsed_time)","7f2e74cb":"train.prior_question_elapsed_time.describe()","33a76f5d":"del train\ngc.collect()","c64de542":"train = pd.read_pickle('..\/input\/riiid-train-data-multiple-formats\/riiid_train.pkl.gzip')\ntrain.prior_question_had_explanation = train.prior_question_had_explanation.astype('bool')\ntrain.prior_question_elapsed_time = train.prior_question_elapsed_time.fillna(value = train.prior_question_elapsed_time.median())","e4d2b582":"explanation = train.prior_question_had_explanation.value_counts()\nexplanation_percent = explanation \/ explanation.values.sum()\n\n\n# plotting\nfig, ax = plt.subplots(figsize = (18, 8))\nax.bar(explanation.index, explanation_percent)\n\n# layout setup\nax.set_xlabel('Had Explanation', fontsize=18);\nax.set_ylabel('Proportion', fontsize=18);\nax.set_title( 'Proportion of interations where the user saw explanation after answering', fontsize = 20)\nplt.yticks(fontsize = 14)\nplt.xticks(fontsize = 14)\nplt.xticks([0, 1], ['No', 'Yes'])\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['left'].set_visible(False);\n\n# adding values on the top of each bar\nrects = ax.patches\nlabels = [str(\"{:.2f}\".format(x)) for x in explanation_percent]\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width() \/ 2, height , label,\n            fontsize=14, ha = 'center', va = 'bottom')\n\n\nplt.show()","ace0de45":"correlation = train.corr()\ncorrelation","9bfa8418":"import plotly.figure_factory as ff\n\nz = np.around(correlation.values, decimals = 4)\nx = list(correlation.columns)\ny = list(correlation.columns)\n\nfig = ff.create_annotated_heatmap(z, x = x, y = y,\n                                  colorscale = 'RdBu',\n                                  font_colors = ['black'],\n                                  showscale = True)\n\nfig.data[0].update(zmin = -1, zmax = 1)\nfig.layout.title = 'Features Correlation'\nfig.update_layout(width=800, height=900)\nfig.show()","16b2d7e2":"# 1% of the dataset\ntrain_sample = train.sample(1000000, random_state = 42)\ntrain_sample","d24c6da6":"fig = px.scatter(train_sample, train_sample.timestamp, train_sample.task_container_id,\n                 marginal_x=\"histogram\", marginal_y=\"histogram\", opacity = 0.1)\nfig.show()","fb13483d":"fig = make_subplots(rows = 1, cols = 2, \n                   subplot_titles = ('Boxplot of the content_id for Questions and Lectures',\n                                     'Distribution of content_id (for comparison)'),\n                   column_widths=[0.7, 0.3])\n\nfig.add_trace(go.Violin(x = train_sample.content_type_id, \n                        y = train_sample.content_id, \n                        box_visible=True), row = 1, col = 1)\n\n              \nfig.add_trace(go.Histogram(x = train_sample.content_id), row = 1, col = 2)   \n\n\nfig.update_layout(xaxis = {'tickmode':'array',\n                           'tickvals':[0, 1],\n                           'ticktext':['Question', 'Lecture']},\n                  title_text = \"Analysing the relationship between content_id and content_type_id\",\n                  showlegend = False)\n\nfig.update_xaxes(title_text = \"content_type_id\", row=1, col=1)\nfig.update_yaxes(title_text = \"content_id\", row=1, col=1)\nfig.update_xaxes(title_text = \"content_id\", row=1, col=2)\n\n              \nfig.show()","6d0c4b0e":"train.prior_question_had_explanation.value_counts()","f6ac32f1":"questions = train[train['content_type_id'] == False]['prior_question_had_explanation']\nlectures = train[train['content_type_id'] == True]['prior_question_had_explanation']\nexplanation = [\"Hadn't Explanation\", \"Had Explanation\"]\n\ntrace1 = go.Bar(x = explanation, \n                y = questions.value_counts().values,\n                name = 'Questions')\n\ntrace2 = go.Bar(x = explanation, \n                y = lectures.value_counts().values,\n                name = 'Lectures')\n\ndata = [trace1, trace2]\n\nlayout = go.Layout(title = 'Relationship of Content Type and if the previous question had explanation')\n\nfig = go.Figure(data = data, layout = layout)\nfig.show()","f6179b56":"del [[questions, lectures]]\ngc.collect()","97426a8f":"incorrect = train[train['answered_correctly'] == 0]['prior_question_had_explanation']\ncorrect = train[train['answered_correctly'] == 1]['prior_question_had_explanation']\nexplanation = [\"Hadn't Explanation\", \"Had Explanation\"]\n\ntrace1 = go.Bar(x = explanation, \n                y = incorrect.value_counts().values,\n                name = \"Incorrect\")\n\ntrace2 = go.Bar(x = explanation, \n                y = correct.value_counts().values,\n                name = \"Correct\")\n\ndata = [trace1, trace2]\n\nlayout = go.Layout(title = 'Relationship of Content Type and if the previous question had explanation')\n\nfig = go.Figure(data = data, layout = layout)\nfig.show()","8a1fac1b":"del [[incorrect, correct]]\ngc.collect()","cc781525":"incorrect = train[train['answered_correctly'] == 0]['task_container_id'].value_counts().reset_index()\nincorrect.columns = ['task_container', 'count']\nincorrect['task_container'] = incorrect['task_container'].astype(str) + ' -'\nincorrect = incorrect.sort_values(['count']).tail(30)\n\n\ncorrect = train[train['answered_correctly'] == 1]['task_container_id'].value_counts().reset_index()\ncorrect.columns = ['task_container', 'count']\ncorrect['task_container'] = correct['task_container'].astype(str) + ' -'\ncorrect = correct.sort_values(['count']).tail(30)\n\nfig = make_subplots(rows = 1, cols = 2, subplot_titles = ('Incorrect','Correct'))\n\nfig.add_trace(go.Bar(x = incorrect['count'], \n                     y = incorrect['task_container'], \n                     orientation='h'), row = 1, col = 1)\n\nfig.add_trace(go.Bar(x = correct['count'], \n                     y = correct['task_container'], \n                     orientation='h'), row = 1, col = 2)  \n\nfig.update_yaxes(title_text = 'task_container_id', tickvals = incorrect['task_container'], row=1, col=1)\nfig.update_yaxes(tickvals = correct['task_container'], row=1, col=2)\nfig.update_layout(title_text = '30 most frequent task containers for incorrect and correct answers', showlegend = False)\n\nfig.show()","b424da0f":"for i in range(0, 10):\n    print(\"Top \" + str(i+1) + \" incorrect: \" + str(incorrect['task_container'][i]) + \n          \" Top \" + str(i+1) + \" correct: \"  + str(correct['task_container'][i][:-2]))","ed79cc8f":"del [[incorrect, correct]]\ngc.collect()","a590faf9":"incorrect = train[train['answered_correctly'] == 0]['task_container_id'].value_counts().reset_index()\nincorrect.columns = ['task_container', 'count']\nincorrect = incorrect.sort_values(['task_container'])\n\n\ncorrect = train[train['answered_correctly'] == 1]['task_container_id'].value_counts().reset_index()\ncorrect.columns = ['task_container', 'count']\ncorrect = correct.sort_values(['task_container'])\n\ntask_container_difficulty = []\nfor i in correct['task_container']:\n    correct_value = correct.loc[correct['task_container'] == i]['count'].values\n    incorrect_value = incorrect.loc[incorrect['task_container'] == i]['count'].values\n    \n    if incorrect_value >= (correct_value * 1.2):\n        task_container_difficulty.append(\"very hard\")\n       \n    elif incorrect_value >= (correct_value * 1.05):\n        task_container_difficulty.append(\"hard\")\n        \n    elif (incorrect_value * 1.5) <= correct_value:\n        task_container_difficulty.append(\"very easy\")\n        \n    elif (incorrect_value * 1.05) <= correct_value:\n        task_container_difficulty.append(\"easy\")\n        \n    else:\n        task_container_difficulty.append(\"middle\")\n        \ntask_container_difficulty = pd.DataFrame(task_container_difficulty, columns = [\"difficulty\"])","e16d62de":"task_container_difficulty","4f09711e":"task_container_difficulty.value_counts()","6858b32a":"train = train.join(task_container_difficulty, on = \"task_container_id\")","a9b33b4b":"custom_dict = {\"very easy\":0,\n             \"easy\":1,\n             \"middle\":2,\n             \"hard\":3,\n             \"very hard\":4}\n\ndifficulty_value_counts = train[\"difficulty\"].value_counts().sort_index(key=lambda x: x.map(custom_dict))\ndifficulty_value_counts","ccb86fa8":"fig = px.bar(x = difficulty_value_counts.index, \n             y = difficulty_value_counts.values)\n\nfig.update_layout(title_text = \"Level of Difficulty Distribution\")\nfig.update_xaxes(title_text = \"Container level of difficulty\")\nfig.update_yaxes(title_text = \"Frequency\")\n\nfig.show()","89a2bfce":"fig = px.bar(x = difficulty_value_counts.index[1:], \n             y = difficulty_value_counts.values[1:])\n\nfig.update_layout(title_text = 'Level of Difficulty Distribution without \"Very Easy\" level')\nfig.update_xaxes(title_text = \"Container level of difficulty\")\nfig.update_yaxes(title_text = \"Frequency\")\n\nfig.show()","3c3afeab":"incorrect = train[train['answered_correctly'] == False]['difficulty']\ncorrect = train[train['answered_correctly'] == True]['difficulty']\ndifficulty_level = [\"Very Easy\", \"Easy\", \"Middle\", \"Hard\", \"Very Hard\"]\n\ncustom_dict = {\"very easy\":0,\n             \"easy\":1,\n             \"middle\":2,\n             \"hard\":3,\n             \"very hard\":4}\n\ntrace1 = go.Bar(x = difficulty_level, \n                y = incorrect.value_counts().sort_index(key = lambda x: x.map(custom_dict)).values,\n                name = \"Incorrect\")\n\ntrace2 = go.Bar(x = difficulty_level, \n                y = correct.value_counts().sort_index(key = lambda x: x.map(custom_dict)).values,\n                name = \"Correct\")\n\ndata = [trace1, trace2]\n\nlayout = go.Layout(title = 'Relationship of Task Container level of Difficulty and Answers')\n\nfig = go.Figure(data = data, layout = layout)\nfig.show()","cdbc38f7":"hadnt_explanation = train[train['prior_question_had_explanation'] == 0]['difficulty']\nhad_explanation = train[train['prior_question_had_explanation'] == 1]['difficulty']","510b6c51":"had_explanation.value_counts()","f80078bf":"difficulty_level = [\"Very Easy\", \"Easy\", \"Middle\", \"Hard\", \"Very Hard\"]\n\ncustom_dict = {\"very easy\":0,\n              \"easy\":1,\n              \"middle\":2,\n              \"hard\":3,\n              \"very hard\":4}\n\ntrace1 = go.Bar(x = difficulty_level, \n                y = hadnt_explanation.value_counts().sort_index(key = lambda x: x.map(custom_dict)).values,\n                name = \"Hadn't Explanation\")\n\ntrace2 = go.Bar(x = difficulty_level, \n                y = had_explanation.value_counts().sort_index(key = lambda x: x.map(custom_dict)).values,\n                name = \"Had Explanationt\")\n\ndata = [trace1, trace2]\n\nlayout = go.Layout(title = 'Relationship of Task Container level of Difficulty and if Prior Question Had Explanation')\n\nfig = go.Figure(data = data, layout = layout)\nfig.show()","317d8d2b":"trace1 = go.Bar(x = difficulty_level[1:], \n                y = hadnt_explanation.value_counts().sort_index(key = lambda x: x.map(custom_dict)).values[1:],\n                name = \"Hadn't Explanation\")\n\ntrace2 = go.Bar(x = difficulty_level[1:], \n                y = had_explanation.value_counts().sort_index(key = lambda x: x.map(custom_dict)).values[1:],\n                name = \"Had Explanationt\")\n\ndata = [trace1, trace2]\n\nlayout = go.Layout(title = 'Relationship of Task Container level of Difficulty and if Prior Question Had Explanation')\n\nfig = go.Figure(data = data, layout = layout)\nfig.show()","0b798e28":"Great! Now we may plot it and understand the distribution of the difficulty","2d080999":"Here we could point out two interesting points:\n- The amount of \"had explanation\" are pretty much stable around the value 375k interactions. It may show that just a few users are interested in dive deepper or have difficulty enough to watch explanations about the previous question bundle.\n- The amount of \"hadn't explanation\" grows as the level of difficulty also grows. It may show that as the difficulty the users may lost interest and just don't dive deeper into the explanations. To confirm this hypothesis, a multivariate plot with correct and incorrect answers will comes in handy.","9f8ef63a":"Let's check of the answers examining the feature \"user_answer\"","ce6f45b4":"Seems that the Feature has the wrong type. It should be **bool** but it is **object** and it is undesirably consuming RAM.  \nLet's cast it to bool.","df240349":"Now let's check how much missing values do we have here","6f87151b":"In order to understand a little bit more about the correlation between the features, and to have some relationship to start exploring, let's create a correlation heatmap!","bd29b4de":"The graphic above shows an unexpected behaviour, if the student didn't see an explanation after solving the last question bundle it is more likely that he will answer the question correctly. But if he had an explanation, the likelyhood it's about the same. That is pretty unexpected to me.  \nWe might think as a cause of this behaviour to be that the student move to the next question bundle but his thoughts still at the last problems and he probabily still have some questions about the subject. It suggests also that a great next bundle would be something that still related to the previous explanation in order reforce the content and improve the learning process.","c151c086":"Assuming that only correlations where z > 0.4 or z < -0.4 are strong enough to worth an exploration, the relatioships that we will explore are:\n- timestamp & task_container_id - z = 0.4334\n- content_id & content_type_id - z = 0.4146\n- content_type_id & prior_question_had_explanation - z = -0.3915\n\nAs expected the features user_id and row_id have z = 1","8c5bd8ae":"Well, it looks like a normal distribution but skewed to the right. Let's check it out.","3540c85b":"Now let's investigate the task_container_id feature and check if we can extract some insight about the containers.","bac8068c":"# 3. NEXT STEPS \nSo far we explore one variate and bivariate relationships, so the next steps should be:\n- Explore multivariate relationships;\n- Explore the lectures and questions datasets in the same way that was done for the train dataset;\n- Explore the relationships between the 3 datasets!\n\nTo be continued...!","f06f8f44":"By looking the graph above, we see that there is around 66% of the answers would be correct. That is important because if we decide to bild a baseline that only say that the answer will be correct, we expect to be correct on 66% of the time. So our model must perform above this value or if we decide for another baseline, it must be better than 66% of correct predictions! ","e4db711c":"As expected, we do se some relationship between this two variables but weak.  \n\nWe can also see that there are a greater concentration of the points in the left down side of the graph, that means, most of the timestamps were between 0 and 40 bilions and the most frequent task cointainers were between 0 an 4k.","b476ec9b":"**Exploring the correlation between content_type_id & prior_question_had_explanation (z = -0.3915).**","2a438056":"It is clear that the giant proportion of interation was solving questions than taking lectures. If we could see a relationship between this proportion and the proportion of corect answers for the user, it could be a great clue of the way that we should build our ML solution. I will test it latter when it gets on bivariate analysis.","429719b2":"Now let's check how many questions (in proportion) were answered right and wrong!","1017eb60":"Looking at the graph we can say that IDs until 11000 are the most frequent and in this range there is some points bins that appear way more thant the others. That means this IDs are more acessed by the users.   \nIt is also important to say that we need to check which of those IDs stands for Questions and Lectures. This analysis is done in the next section where we explore bivariate behaviour.","1ccf6fa1":"Let's check which contents are most frequent and how content_id distribution looks like","0e2e9776":"Analysing the graph we can say that:\n- For Questions, the content_id range is from 0 to 14k. We can also, in the second graph, that most part of the content are from this range. It goes right away with we saw before, where 98% of the interactions were of questions and only 2% of lectures.\n- Question content_id distribution is very heterogen.\n- For Lectures, 75% of the data are from ids over 8,6k and as content_id grows, there are less interactions. It explains why we see this tail with small frequency: the major part of this interactions are from questions and because only 2% of instances are of lectures, this tail must present small frequency values.\n- Lectures content_id distribution are very homogen, so we can supose there are no prefered lectures.","a27bd26a":"# Exploring *train* dataset","e7ef9126":"Let's take a closer look at the top 10 most frequent task containers answered correctly and incorrectly. ","a6988bfb":"A value greater than 0 means that the distribution is right skewed. A value of 4.83 means that this skewness is pretty strong actualy. The mean is much bigger than the mode and to fill null values with mean could lead to a undesired behaviour because we want to our descriptive statistics to be the most concentrate aroud the majority of the data as possible. So, in this case the right tail will lead to a erroneous assumption.\nHere we conclude: \n1. The fillna should be with the median and not the mean;\n2. As we can see, the standard deviation is almost twice as bigger than the mean and that is pretty bad because mean is very sensitive to outliers or too skewed distribution.\n3. It would be better to use the median. So let's reload the data and use median in the fillna.\n","7300d618":"**Checking the relationship between task_container_id & answered_correctly**  \nThe main ideia is to check if there are somo containers that has ","ff41b740":"### Description of the dataset\n- row_id: (int64) ID code for the row.\n\n- timestamp: (int64) the time in milliseconds between this user interaction and the first event completion from that user.\n\n- user_id: (int32) ID code for the user.\n\n- content_id: (int16) ID code for the user interaction\n\n- content_type_id: (int8) 0 if the event was a question being posed to the user, 1 if the event was the user watching a lecture.\n\n- task_container_id: (int16) Id code for the batch of questions or lectures. For example, a user might see three questions in a row before seeing the explanations for any of them. Those three would all share a task_container_id.\n\n- user_answer: (int8) the user's answer to the question, if any. Read -1 as null, for lectures.\n\n- answered_correctly: (int8) if the user responded correctly. Read -1 as null, for lectures.\n\n- prior_question_elapsed_time: (float32) The average time in milliseconds it took a user to answer each question in the previous question bundle, ignoring any lectures in between. Is null for a user's first question bundle or lecture. Note that the time is the average time a user took to solve each question in the previous bundle.\n\n- prior_question_had_explanation: (bool) Whether or not the user saw an explanation and the correct response(s) after answering the previous question bundle, ignoring any lectures in between. The value is shared across a single question bundle, and is null for a user's first question bundle or lecture. Typically the first several questions a user sees were part of an onboarding diagnostic test where they did not get any feedback","7f373f23":"*Now let's check out other possible relationships.*","a46fbd11":"So we need to manage missing value in the column \"prior_question_elapsed_time\".  \nLet's fill the missing value for the mean value of the column.\n","451926e1":"Great! Now let's analyse em plot the distribution.","9163ebfd":"Looking at the distribution above, we may say that after a step into a subject with very easy and easy question, the user prefer hard and very hard questions and it makes sense because to get sharp in an area, it demands the solving of complex tasks!","aba38952":"**Exploring the correlation between content_id & content_type_id (z = 0.4146).**","2f727f29":"Analysing the graph we can conclude that, in one hand if the user didn't wacht an explanation of after solving the previous question bundle, it is more likely that the next interaction will be with another question posed. On the other hand, if the student had explanation, we can say that the next definitivly won't a lecture.","2a4e3220":"We see that only answer 2 was less chosen than the others in around 9%. We see also that the other answers were chosen around 27% of the times. **The question here is why?** It should have the same value assuming that there are 101 milion interactions, enough samples to see a equal distribution between the answers.","1faa3d8c":"Well it does already say something but let's put it in %","0fcc2e7f":"# 2. Bivariate Exploration","996474e5":"As expected, the behaviour match with the equation.","5e4276c9":"**Exploring the correlation between timestamp & task_container_id (z = 0.4334).**","31186253":"Seems that very easy container taks are the greatest part of the interaction and goes directly to which we've said before (more easy task must be present to the user so they can step into new subjects without get scaried and abandon the program), but it still to much very easy questions, which indicates that we should change the thresholds.\n\nIn order to see de distribution of the other levels, let's plot them without the very easy level of difficulty.","89a05e9e":"Let's finally take a look at the feature *\"prior_question_had_explanation\"* in order to understand in how many interactions did the users see an explanation after answering a question bundle.","4e7ec6ed":"As we can see, for very easy task containers there are much more correct questions than incorrect. It is something that we already expect to see and confirms the hypothesis so far.\n\nBy looking closer to the other levels, we see that the distribution presents a trasitin where for easy containers there are still more correct answer, for middle difficulty the amount for correct and incorrect was about the same and for hard and very hard there are more incorret than correct answers.  \nThis behaviour is exactly what we expect to see in real world and it is a great clue that the analysis is going in the right path! ","b029d315":"**Checking the relationship between prior_question_had_explanation & answered_correctly**","eebe4fa2":"Now let's visualize the distribution of timestamp column","9794da4c":"**Checking the relationship between difficulty & answered_correctly**","45a898e3":"As we can  see, the users in around 90% of the interactions saw an explanation after answering a question bundle.","65d2975f":"First thing to notice is that the \"very easy\" questions are so much more than the other levels. It may make sense when we thing that students need to solve easy questions to step on new subject in a way that they don't feel overwhelmed and afraid to dive into the different areas.  \n\nAnother point is that this number is still too bigger an we expected to see more questions of other levels. It may be good to work with the thresholds a bit! I think here is a great opportunity to go crazy and test whatever thresholds combination that comes in mind.\n\nNow let's insert this results in our train dataset and check how were the distribution of the interactions based on their level of difficulty!","d3d41790":"Analysing the graphs and the print above we can conclude the following:\n- if the taks container, like container 6, has a greater frequency in incorrect answers than in correct answers, we might say that this container has hard difficulty.\n- if the task container, like container 15, has more or less the same frequency in both incorrect and correct answers count, we might say that this container has middle difficulty.\n- if the task container, like container 14, has a greater frequency in correct answers than in incorrect answers, we might say that this container has a easy difficulty.\n\nThis conclusion could be the most important so far, because it allows us to build a new feaure where we classify the level of difficulty of the container (could be 3 levels or whatever we want), and this new feature might help to improve our machine learning models!  \nLet's create this feature and explore it a little!","04eae393":"Looking at the distribution, we can conclude that the most amount of interactions was done by users that was not active for very long time.   \n**We can say that the behaviour of this distribution could be discrebed by the fuction f(x) = 1\/x where x is the amount of the timestamps in that bin.**","45f7b587":"Let's look first at very easy containers. Here, the amount amount of task containers where the user had an explanation in the previous question bundle is way more bigger thant the amount of users that hadn't an explanation.  \nIt matches with our hypothesis of the very easy containers are more used for beginners in a subject, which leads to the necessity to watch some explanation after solving a question bundle and before to move to the next container. It also explains why the biggest amount\/rate of \"had explanation\" is for very easy containers than for very hard.\n\nNow, in order to get a better understanding of the behaviuor for the other levels, let's plot them with the very easy columns.","a4bb58f0":"Because the train dataframe has 101 milion intances, let's use some samples to be able to explore it better","155f1f11":"Now let's check the distribution for user_id in order to get to know more about this feature","91c3676d":"**Checking the relationship between difficulty & prior_question_had_explanation**  \nWe expect, intuitively, to see more explanation for more difficulty task containers, let's check if it matches with the reality","f8e64de3":"# 1. Univariate Exploration"}}