{"cell_type":{"12de0fa9":"code","907d2afe":"code","d76bce00":"code","10695c04":"code","9373c4b7":"code","07ca90de":"code","4b5f7a4f":"code","73c2b502":"code","3ba86b6b":"code","e1ac25b4":"code","cc51d08f":"code","0211200b":"code","08a21bd2":"code","9eb38c07":"code","ba953a98":"code","a47bc121":"code","5c6fe334":"code","ef3dda8b":"code","c0ce16e0":"code","da0cab5c":"code","688dc434":"code","c0dd0307":"code","82c42913":"code","745410a5":"markdown","d0dbb7f6":"markdown","f7cfb289":"markdown","c7bc004c":"markdown","c627fa22":"markdown","b1b226ff":"markdown","800c5816":"markdown","1eaa30f6":"markdown","8310723e":"markdown","15ed36ea":"markdown","3b11e59a":"markdown","d500370c":"markdown","90acb663":"markdown","764359ca":"markdown","f7b124bc":"markdown","5f983c1d":"markdown","dede66f3":"markdown","afba405a":"markdown","cbeeabbe":"markdown","cdec41c1":"markdown","aa7d3577":"markdown","5a10c0ff":"markdown","10100fea":"markdown","2add5cdd":"markdown","49d5a614":"markdown","dbdb9eb9":"markdown","2137ba93":"markdown","5409daa3":"markdown","f6fb94a5":"markdown","13d0b8e7":"markdown"},"source":{"12de0fa9":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n","907d2afe":"##### Load dataset\ntitanic_data = pd.read_csv(\"..\/input\/titanic\/titanic.csv\")\ntitanic_data.head()                                 #head prints only the first 5 rows\n\n# The columns below such as PassengerId, Sex, Age are referred to as features","d76bce00":"# Check for missing values\ntitanic_data.isnull().sum()","10695c04":"# 2.2.1 Filling missing value of Age with mean value\ntrainFareSum=titanic_data[\"Age\"].isnull().sum()\nprint('Total age null values in the dataset before fill = ', trainFareSum)\ntitanic_data[\"Age\"] = titanic_data[\"Age\"].fillna(titanic_data[\"Age\"].mean())\ntrainFareSum=titanic_data[\"Age\"].isnull().sum()\nprint('Total age null values in the dataset after fill = ', trainFareSum)","9373c4b7":"# Correlation matrix between numerical values (SibSp Parch Age and Fare values) and Survived \ng = sns.heatmap(titanic_data[[\"Survived\",\"SibSp\",\"Parch\",\"Age\",\"Fare\"]].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")","07ca90de":"# Explore SibSp feature vs Survived\ng = sns.catplot(x=\"SibSp\",y=\"Survived\",data=titanic_data,kind=\"bar\", height = 6, palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","4b5f7a4f":"# Explore Parch feature vs Survived\ng  = sns.catplot(x=\"Parch\",y=\"Survived\",data=titanic_data,kind=\"bar\", height = 6 , \npalette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","73c2b502":"# Explore Age vs Survived\ng = sns.FacetGrid(titanic_data, col='Survived')\ng = g.map(sns.distplot, \"Age\")","3ba86b6b":"g = sns.barplot(x=\"Sex\",y=\"Survived\",data=titanic_data)\ng = g.set_ylabel(\"Survival Probability\")","e1ac25b4":"# convert Sex into categorical value 0 for male and 1 for female\ntitanic_data[\"Sex\"] = titanic_data[\"Sex\"].map({\"male\": 0, \"female\":1})","cc51d08f":"# Drop extraneous features, since we would like to demonstrate machine learning with minimal dataset\n# For advanced training, we would include more features based on further analysis.\ntitanic_data.drop([\"Name\",  \"SibSp\", \"Parch\", \"Cabin\", \"PassengerId\", \"Pclass\", \"Embarked\", \"Ticket\"], axis = 1, inplace = True)","0211200b":"# Check the data for total number of features\ntitanic_data.head()","08a21bd2":"# Check the correlation between features after data cleaning\ng = sns.heatmap(titanic_data[[\"Age\",\"Sex\",\"Fare\"]].corr(),cmap=\"BrBG\",annot=True)","9eb38c07":"## Separate train dataset for model to see features and outcomes\nfrom sklearn.model_selection import train_test_split\n\nX=titanic_data.drop(\"Survived\",axis=1)   # list of features selected for training predicton model\ny=titanic_data[\"Survived\"]               # list of features to be predicted after training the model\nX_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size=0.20, random_state=10)\n                                    # test size indicates percent of dataset used as test set, here it is set to 20%                                    ","ba953a98":"# DecisionTree Parameters tunning \nfrom sklearn.tree import DecisionTreeClassifier\nDTC = DecisionTreeClassifier(random_state=10)\nDTC.fit(X_train,Y_train)\n","a47bc121":"# RFC Parameters tunning \nfrom sklearn.ensemble import RandomForestClassifier\nRFC = RandomForestClassifier(n_estimators=10,random_state=10)\nRFC.fit(X_train,Y_train)","5c6fe334":"# Decision Tree feature importance\nindices = np.argsort(DTC.feature_importances_)[::-1][:40]        \ng = sns.barplot(y=X_train.columns[indices][:40],x = DTC.feature_importances_[indices][:40] , orient='h')\ng.set_xlabel(\"Relative importance\",fontsize=12)\ng.set_ylabel(\"Features\",fontsize=12)\ng.tick_params(labelsize=9)\ng.set_title(\"Decision Tree feature importance\")","ef3dda8b":"# Random Forest feature importance\nindices = np.argsort(RFC.feature_importances_)[::-1][:40]        \ng = sns.barplot(y=X_train.columns[indices][:40],x = RFC.feature_importances_[indices][:40] , orient='h')\ng.set_xlabel(\"Relative importance\",fontsize=12)\ng.set_ylabel(\"Features\",fontsize=12)\ng.tick_params(labelsize=9)\ng.set_title(\"Random Forest feature importance\")","c0ce16e0":"# Predict the results for both models\ntest_Survived_RFC = pd.Series(RFC.predict(X_test), name=\"RFC\")\ntest_Survived_DTC = pd.Series(DTC.predict(X_test), name=\"DTC\")","da0cab5c":"from sklearn.metrics import classification_report\nprint(classification_report(Y_test,test_Survived_DTC))","688dc434":"print(classification_report(Y_test,test_Survived_RFC))","c0dd0307":"print(\"Training samples: {}, testing samples: {}\".format(len(X_train), len(X_test)))\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(Y_test,test_Survived_DTC)\ng = sns.heatmap(cm,annot=True)\ng.set_xlabel(\"Predicted label - Survived\",fontsize=12)\ng.set_ylabel(\"True label - Survived\",fontsize=12)\ng.tick_params(labelsize=9)\ng.set_title('Confusion Matrix - Decision Tree Model')\n","82c42913":"from sklearn.metrics import confusion_matrix\ncm=confusion_matrix(Y_test,test_Survived_RFC)\ng = sns.heatmap(cm,annot=True)\ng.set_xlabel(\"Predicted label - Survived\",fontsize=12)\ng.set_ylabel(\"True label - Survived\",fontsize=12)\ng.tick_params(labelsize=9)\ng.set_title('Confusion Matrix - Random Forest Model')","745410a5":"### 2.2 Massage\/clean the data","d0dbb7f6":"Small families have more chance to survive, more than single (Parch 0), medium (Parch 3,4) and large families (Parch 5,6).","f7cfb289":"## 3. Split into Training\/Test Set","c7bc004c":"At this stage, we have 3 features.","c627fa22":"Since we have one missing value , we decided to fill it with the mean value which will not have an important effect on the prediction.\nYou will notice we checked total number of null values for age before and after filling the null values.\nAnother option to fill null values is using median value of the feature.","b1b226ff":"Only Fare feature seems to have a significative correlation with the survival probability.\n\nIt doesn't mean that the other features are not useful. Subpopulations in these features can be correlated with the survival. To determine this, we need to explore in detail these features","800c5816":"We decided to choose the DecisionTree and RandomForest models as they are the stepping stones to quickly understand predictive modeling in machine learning.","1eaa30f6":"At this time, prediction results for both models are ready for evaluation.","8310723e":"We note that the two models have top features according to the relative importance. When several features are considered, the top features that are important may differ.","15ed36ea":"At this time, you'll be familiar with how to load dataset, clean dataset, visualize features using plots, check and fill null values, train models, and predict the results.","3b11e59a":"### 2.3.2 Categorical values feature analysis\n#### 2.3.2.1 Sex","d500370c":"# Hands-on ML using Titanic Kernel\n### Uttara Sawant and Thalanayar Muthukumar\n#### April 2019\n\n* **1 Introduction**\n* **2 Load and check data**\n    * 2.1 load data\n    * 2.2 massage\/clean the data\n        * 2.2.1 Check missing values and fill them\n    * 2.3 Feature analysis\n        * 2.3.1 numerical values\n        * 2.3.2 categorical values\n    * 2.4 feature engineering\n* **3 Split into Training\/Test Set**\n\n* **4 Train model on data**\n    * 4.1 Two models\n    * 4.2 Feature importance from model\n* **5 Score & Evaluate**\n    * 5.1 Evaluation using classification report\n    * 5.2 Evaluation using confusion matrix\n    ","90acb663":"It seems that passengers having a lot of siblings\/spouses have less chance to survive\n\nSingle passengers (0 SibSP) or with two other persons (SibSP 1 or 2) have more chance to survive.","764359ca":"##### 2.3.1.3 Age feature","f7b124bc":"##### 2.3.1.2 Parch feature","5f983c1d":"It seems that very young passengers have more chance to survive.","dede66f3":"## 1. Introduction\n\nWe will use the Titanic dataset to walk through the machine learning steps. Before we walk through the ML steps, we need to import the packages (pandas, numpy, matplotlib and seaborn). We have disabled printing of warning messages using the warnings package.\n","afba405a":"### 5.2 Evaluation using confusion matrix","cbeeabbe":"## 5 Score & Evaluate\n### 5.1 Evaluation using classification report\n\nWe choose classification report to determine the model precision.","cdec41c1":"The precision column in the report indicates the Random Forest model predicted 'Survived' (1) to 74%, and 'Not Survived' (0) to 84%.","aa7d3577":"The precision column in the report indicates the Decision Tree model predicted 'Survived' (1) to 67%, and 'Not Survived' (0) to 85%.","5a10c0ff":"##### 2.3.1.1 SibSp feature","10100fea":"### 2.4 Feature engineering","2add5cdd":"### 4.2 Feature importance of models\n\nIn order to see the most informative features for the prediction of passengers survival, we displayed the feature importance for the 2 models.","49d5a614":"### 4 Train models\n#### 4.1 Two Models\n\nWe compared 2 popular models and evaluate them. \n\n* Decision Tree: Simplest predictive learning model to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves)\n* Random Forest: Collective  predictive learning model which combines multitude of decision trees","dbdb9eb9":"## 2. Machine Learning Steps\nWe will walk through the following steps i. Load data, ii. Clean data, iii. Feature analysis, iv. Split into train \/ test set, v. Train model on data, vi. Score and evaluate\n\n### 2.1 Load and check data","2137ba93":"### 2.2.1. Check missing values and fill them","5409daa3":"Age and Cabin features have an important part of missing values.","f6fb94a5":"### 2.3 Feature analysis\nYou understand the correlation of features (numerical and categorical) with the value being predicted\n\n#### 2.3.1 Numerical values","13d0b8e7":"It is clearly obvious that Male have less chance to survive than Female.\n\nSo Sex, might play an important role in the prediction of the survival.\n\nFor those who have seen the Titanic movie (1997), I am sure, we all remember this sentence during the evacuation : \"Women and children first\". "}}