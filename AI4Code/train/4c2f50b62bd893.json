{"cell_type":{"50ba8c3a":"code","cc8f723e":"code","69357808":"code","c6195155":"code","9758415d":"code","539f50da":"code","3b0957fb":"code","3287e725":"code","ca0a65f9":"code","f5d5a889":"code","47e0af45":"code","b488b61a":"code","10585e8e":"code","d224e200":"code","f10dd023":"code","7be19a08":"code","1bee78d9":"code","66d83227":"code","eae558e8":"code","14a807c9":"code","29ddbf05":"code","753ab9ac":"code","56b2c57e":"code","dc919a10":"code","52e1f942":"code","2db519d2":"code","f4bc8981":"code","50f1f973":"code","b6a77931":"code","b90ba43a":"code","cc27c795":"code","8cfeda95":"code","55cbf212":"code","453559af":"code","ae889683":"code","ce52bf1a":"code","826f5a56":"code","63cc1e7f":"code","88a24d7b":"code","6a9e05a3":"code","780d6d2a":"code","4d9b18a0":"code","cca3ee77":"code","935aa468":"code","1bd7c958":"code","1d736c59":"code","c0d4c79e":"markdown","de33b212":"markdown","b5db2d70":"markdown","66c51b2b":"markdown","d7a3ff9f":"markdown","292962c6":"markdown","99e38d06":"markdown","f0717f39":"markdown","4e28cbf6":"markdown","45ff54f4":"markdown","c9589df6":"markdown","db26aa73":"markdown","723018ee":"markdown","af5e0790":"markdown","8691f39f":"markdown","dcd0a126":"markdown","3c1f2d0f":"markdown","db0ad676":"markdown","7a997322":"markdown","793f3940":"markdown","cf9a1c18":"markdown"},"source":{"50ba8c3a":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, explained_variance_score, mean_squared_log_error\n%matplotlib inline\ndf = pd.read_csv('..\/input\/housesalesprediction\/kc_house_data.csv')\npd.set_option('float_format', '{:f}'.format)","cc8f723e":"df.shape","69357808":"df.head()","c6195155":"df.info()","9758415d":"df.describe()","539f50da":"df.drop(columns = ['id'], inplace= True)","3b0957fb":"df['date']","3287e725":"df['date'] = df['date'].apply(lambda x: x[0:8])\ndf['date'] = pd.to_datetime(df['date'], format = '%Y%m%d') # convert to datetime object to analyze time patterns\nrecent = df['date'].min()\nlatest = df['date'].max()\nprint(recent, latest) #date range of this dataset is around one year starting on May 2nd, 2014 to May 27th, 2015","ca0a65f9":"df['date']","f5d5a889":"df['month'] = df[\"date\"].dt.month\ndf['day'] = df['date'].dt.day","47e0af45":"plt.figure(figsize = (18, 10))\nsns.lineplot(x = df['date'], y = df['price'])","b488b61a":"plt.figure(figsize = (18, 10))\nsns.boxplot(x = df['month'], y = df['price'])","10585e8e":"plt.figure(figsize = (18, 10))\nsns.boxplot(x = df['day'], y=  df['price'])","d224e200":"df.drop(columns = ['date', 'day', 'month'], inplace = True)","f10dd023":"cols = df.columns\nfig, ax = plt.subplots(nrows = 5, ncols = 4, figsize = (18, 18))\nfor i, ax in enumerate(fig.axes):\n    if i >= len(cols):\n        fig.delaxes(ax)\n    else:\n        sns.histplot(x = df[cols[i]], ax = ax)","7be19a08":"# waterfront\nprint(len(df[df['waterfront'] == 0]) \/ len(df['waterfront'])) # percentage of 0s\nsns.boxplot(x = df['waterfront'], y = df['price'])","1bee78d9":"# view\nprint(len(df[df['view'] == 0]) \/ len(df['view']))\nsns.boxplot(x = df['view'], y = df['price'])","66d83227":"print(df[df['sqft_basement'] == 0]['price'].describe())\nprint('-------------------------')\nprint(df[df['sqft_basement'] != 0]['price'].describe())","eae558e8":"# yr_renovated\nprint(len(df[df['yr_renovated'] == 0]) \/ len(df['yr_renovated']))\ndf2 = df[df['yr_renovated'] != 0]\nsns.regplot(x = df2['yr_renovated'], y = df2['price'], scatter_kws = {'alpha': .3, 's': 10})","14a807c9":"print(df[df['yr_renovated'] == 0]['price'].describe())\nprint('--------------------------')\nprint(df[df['yr_renovated'] != 0]['price'].describe())","29ddbf05":"# sqft_basement\nprint(len(df[df['sqft_basement'] == 0]) \/ len(df['sqft_basement']))\ndf2 = df[df['sqft_basement'] != 0]\nsns.regplot(x = df2['sqft_basement'], y = df2['price'], scatter_kws = {'alpha': .3, 's': 10})","753ab9ac":"df2[df2['price'] > 7000000]","56b2c57e":"# dropping outliers\ndf.drop([3914, 7252], inplace = True)","dc919a10":"df.columns\nnum = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement','sqft_living15', 'sqft_lot15']\nfig, ax = plt.subplots(nrows = 2, ncols = 3, figsize = (18, 9))\nfor i, ax in enumerate(fig.axes):\n    if i >= len(num):\n        fig.delaxes(ax)\n    else:\n        sns.regplot(x = df[num[i]], y = df['price'], scatter_kws = {'alpha': .3, 's': 10}, ax = ax)","52e1f942":"df.drop(columns = ['sqft_lot', 'sqft_lot15'], inplace = True)","2db519d2":"df[df['sqft_living'] > 12000]","f4bc8981":"df.drop([12777], inplace = True)","50f1f973":"df.columns\ncategorical = ['bedrooms', 'floors', 'waterfront', 'view', 'condition', 'grade', 'bathrooms']\nfig, ax = plt.subplots(nrows = 2, ncols = 4, figsize = (25, 9))\nfor i, ax in enumerate(fig.axes):\n    if i >= len(categorical):\n        fig.delaxes(ax)\n    else:\n        sns.boxplot(x = df[categorical[i]], y = df['price'], ax = ax)","b6a77931":"df[df['bedrooms'] == 33]","b90ba43a":"# drop bedroom outlier\ndf.drop([15870], inplace = True)","cc27c795":"df.drop(columns = ['floors'], inplace = True)","8cfeda95":"sns.boxplot(x = df['price'])\ndf[df['price'] > 4000000].index\ntemp = df.drop([1164, 1315, 1448, 2626, 4411, 8092, 8638, 9254, 12370])\ndf['price'] = np.log2(df['price'])","55cbf212":"sns.histplot(x = df['price'])","453559af":"sns.boxplot(x = df['sqft_living'])\ndf['sqft_living'] = np.log2(df['sqft_living'])","ae889683":"sns.histplot(x = df['sqft_living'])","ce52bf1a":"sns.boxplot(x = df['sqft_above'])\ndf[df['sqft_above'] > 8000]\ndf.drop([18302], inplace = True)\ndf['sqft_above'] = np.log2(df['sqft_above'])","826f5a56":"sns.histplot(x = df['sqft_above'])","63cc1e7f":"from sklearn.metrics import r2_score\nX = df.drop(columns = ['price'])\ny = df['price']\nmodels = ['Linear', 'Ridge', 'Lasso', 'RandomForest', 'XGBoost']\nfinal = []","88a24d7b":"# Linear Regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits = 10)\nscores = []\nfor train_index, test_index in kf.split(X):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    mod = LinearRegression().fit(X_train, y_train)\n    pred = mod.predict(X_test)\n    scores.append(r2_score(pred, y_test))\nfinal.append(np.mean(scores))","6a9e05a3":"# Ridge Regression\nfrom sklearn import linear_model\nkf = KFold(n_splits = 10)\nscores = []\nfor train_index, test_index in kf.split(X):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    mod = linear_model.Ridge(alpha = 2).fit(X_train, y_train)\n    pred = mod.predict(X_test)\n    scores.append(r2_score(pred, y_test))\nfinal.append(np.mean(scores))","780d6d2a":"#Lasso Regression\nfrom sklearn.ensemble import RandomForestRegressor\nkf = KFold(n_splits = 10)\nscores = []\nfor train_index, test_index in kf.split(X):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    mod = linear_model.Lasso().fit(X_train, y_train)\n    pred = mod.predict(X_test)\n    scores.append(r2_score(pred, y_test))\nfinal.append(np.mean(scores))","4d9b18a0":"# Random Forest Regression \nfrom sklearn.ensemble import RandomForestRegressor\nkf = KFold(n_splits = 10)\nscores = []\nfor train_index, test_index in kf.split(X):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    mod = RandomForestRegressor().fit(X_train, y_train)\n    pred = mod.predict(X_test)\n    scores.append(r2_score(pred, y_test))\nfinal.append(np.mean(scores))","cca3ee77":"# XGBoost\nfrom xgboost import XGBRegressor\nimport warnings\nwarnings.filterwarnings(action='ignore', category=UserWarning)\nkf = KFold(n_splits = 10)\nscores = []\nfor train_index, test_index in kf.split(X):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    mod = XGBRegressor().fit(X_train, y_train)\n    pred = mod.predict(X_test)\n    scores.append(r2_score(pred, y_test))\nfinal.append(np.mean(scores))","935aa468":"data = {'Model': models, 'R2 Score': final}\ndf2 = pd.DataFrame(data)\nsns.barplot(x = df2['R2 Score'], y = df2['Model'], orient = 'h')","1bd7c958":"from sklearn.model_selection import GridSearchCV\nmod = XGBRegressor(n_jobs = 5)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\nparams = {'n_estimators': [100, 250, 500, 750], 'max_depth': [3, 5, 7, 9], 'learning_rate' : [.1, .3, .5, .7]}\ngrid = GridSearchCV(mod, scoring = 'r2', cv = 10, param_grid = params, n_jobs = 5)\ngrid.fit(X, y)","1d736c59":"print(grid.best_score_)","c0d4c79e":"# Looking into variables w\/ a lot of 0s","de33b212":"# Model Testing","b5db2d70":"XGBoost (without any parameter changes) seems to be the best model with an R2 score of .88, so I will optimize its parameters using GridSearch to get the best R2 score","66c51b2b":"# Normalizing distribution of numeric variables","d7a3ff9f":"# Bivariate analysis between independent and dependent variables","292962c6":"We can also see that the difference in mean between houses w\/ and w\/out basements is around $140000 which is significant enough to keep this variable.","99e38d06":"95% of the houses in this dataset were never renovated, and the regression plot (ignoring houses that haven't been renovated) shows a slightly positive relationship between *price* and *yr_renovated*  ","f0717f39":"* No missing values to deal with\n* Dataset has 21,613 rows and 20 columns \n* ID variable doesn't seem to be very helpful for predicting house prices so I will remove it","4e28cbf6":"# Distributions Of Each Variable","45ff54f4":"Looking at these graphs, we can see that the date of house purchase can't really explain the price of a house, therefore we will remove these columns.","c9589df6":"With GridSearch, my final R2 score was around .89","db26aa73":"90% of the houses in this dataset have a *view* grade of 0. The boxplot shows that there is as slight increase in price as the view rating increases so we will keep this variable.","723018ee":"Most categorical variables seem to be linearly correlated with *price*. Outlier in *bedrooms* where a house has 33 bedrooms.*floors* sees an increase in price until it reaches 3 floors where it unusally drops. We saw in univariate analysis that *floors* was not distributed well, which may have caused the unusual drop in price, therefore I will drop this column. I will also drop the outlier in *bedrooms*.","af5e0790":"Most numerical variables seem to be well linearly correlated with *price*. Big outlier in *sqft_living*, will drop this. *sqft_lot* and *sqft_lot15* do not look like good predictor variables as the data doesn't seem consistent enough with *price*, therefore I will drop these variables. ","8691f39f":"# Analyzing date of sale vs price","dcd0a126":"99% of the houses in this dataset are not near waterfronts. The boxplot shows that the difference in price between 0s and 1s is significant enough so we will keep this variable.","3c1f2d0f":"60% of houses in this dataset do not have basements. The regression plot (ignoring houses with no basements) shows that there is a slightly positive relationship between *sqft_basement* and *price*. There are a couple outliers that are way off the other datapoints and the reg line which may affect our model performance later on, so I will remove those. ","db0ad676":"# Exploring The Data","7a997322":"1. Variables like waterfront, view, sqft_basement, yr_renovated have a lot of 0s, and some categorical variables like floors and view are unevenly distributed\n    \n    a. May affect data visualization and model performance\n    \n    b. Remove variable if there is no significant difference in price between 0s and non 0s\n    \n    c. Remove some of the categorical variables if seen necessary later on\n\n2. Numeric variables like price, sqft_living, sqft_above are skewed to the left\n    \n    a. Removing outliers may fix the distribution\n    \n    b. Normalizing the distribution may benefit specific models","793f3940":"To normalize *price*, *sqft_above*, and *sqft_living*, I will remove some outliers and use log transformation.","cf9a1c18":"Although the count of houses that have been renovated is way less than the count of houses that haven't (5%:95%), we can still see a slightly higher price mean with houses that have been renovated. "}}