{"cell_type":{"2caf553f":"code","3495ca08":"code","b38d5321":"code","c3071254":"code","9b2343f5":"code","10e8eeae":"code","34a70148":"code","8659b4a8":"code","4b1a9e62":"code","b6c07a14":"code","15a93b12":"code","1a7d8467":"code","6221b54b":"code","f288645b":"code","dd138a2a":"code","ed0d9627":"code","8bfaa9fb":"code","ab000c3e":"code","f553f84c":"code","cf42ff12":"code","621cb0da":"code","bca6bc1f":"code","0e1dcf73":"code","898b3ea0":"code","ecbf1a52":"code","51a106a0":"code","42dbec3c":"code","83965645":"code","97cf4e61":"code","bb84f2e8":"code","f8395770":"code","bb5dd8dd":"code","8170d87a":"code","080ba50f":"code","40355a50":"code","b71caec8":"code","937e1b15":"markdown","cd4d69df":"markdown","3b4d6963":"markdown"},"source":{"2caf553f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3495ca08":"import nltk\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\nps = PorterStemmer()","b38d5321":"df = pd.read_csv(\"..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\")\ndf.head()","c3071254":"stopwords_eng = stopwords.words('english')\nstopwords_eng.append('br')\nstopwords_eng","9b2343f5":"sent = nltk.sent_tokenize(df.iloc[4]['review'])\nsent","10e8eeae":"corpus = []\nreview = []\nfor i in sent:\n    review.extend(nltk.word_tokenize(re.sub('[^a-zA-Z]', ' ', i).lower()))\n    #ps.stem(word)\n    review = [ps.stem(word) for word in review if word not in stopwords_eng]\n    \nprint(review)","34a70148":"corpus=[]\nfor i in range(5000):\n    #review = re.sub('[^a-zA-Z]',' ',df['review'][i]) # chalega\n    #review = review.lower()\n    #review = review.split()\n    #review = [ps.stem(word) for word in review if word not in set(stopwords.words('english'))]\n    #review=' '.join(review)\n    review = nltk.word_tokenize(re.sub('[^a-zA-Z]', ' ', df['review'][i]).lower())\n    #ps.stem(word)\n    review = [ps.stem(word) for word in review if word not in stopwords_eng]\n    review=' '.join(review)\n    corpus.append(review)\n#corpus","8659b4a8":"import pandas as pd\nprep_data = pd.read_csv(\"..\/input\/imdbmoviereviewpreprocessedstemming50k\/prepd_data.csv\")\nprep_data.head()","4b1a9e62":"corpus = list(prep_data['review'])\nlen(corpus)","b6c07a14":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features=5000)\nX = cv.fit_transform(corpus).toarray()\n\n# rows of X will be corresponding sentences and columns will be corresponding to word\ny = prep_data['sentiment_label']","15a93b12":"print(X[0].sum())\nprint(X.shape)","1a7d8467":"#corpus has 68992 unique words , how many of them you want to consider(depending upon frequency)?????\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.5, shuffle=True)","6221b54b":"test_corpus=[]\nfor i in range(45000,50000):    \n    review1 = nltk.word_tokenize(re.sub('[^a-zA-Z]', ' ', df['review'][i]).lower())\n    #ps.stem(word)--\n    review1 = [ps.stem(word) for word in review1 if word not in stopwords_eng]\n    review1 =' '.join(review1)\n    test_corpus.append(review1)","f288645b":"X2 = cv.transform(test_corpus).toarray()\n\n# rows of X will be corresponding sentences and columns will be corresponding to word\n    \ny2 = pd.get_dummies(df['sentiment'])\ny2 = y2.iloc[45000:50000,1].values\nX2.shape","dd138a2a":"y_pred2 = model1.predict(X2)\ncm2 = confusion_matrix(y2,y_pred2)\naccuracy2= accuracy_score(y2,y_pred2)","ed0d9627":"accuracy2","8bfaa9fb":"from sklearn.feature_extraction.text import TfidfVectorizer\ncv = TfidfVectorizer(max_features=5000, ngram_range=(1, 3))\nX = cv.fit_transform(list(prep_data['review'])).toarray()\ny = prep_data['sentiment_label']","ab000c3e":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.5, shuffle=True)\nprint(x_train.shape,y_train.shape,x_test.shape,y_test.shape)","f553f84c":"from sklearn.linear_model import SGDClassifier \nmodel2 = SGDClassifier().fit(x_train,y_train,sample_weight=None)","cf42ff12":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\ny_pred_svm = model2.predict(x_test)\ncm = confusion_matrix(y_test,y_pred_svm)\naccuracy= accuracy_score(y_test,y_pred_svm)","621cb0da":"cm","bca6bc1f":"accuracy","0e1dcf73":"\"\"\"\nMulticlass SVMs (Crammer-Singer formulation).\nA pure Python re-implementation of:\nLarge-scale Multiclass Support Vector Machine Training via Euclidean Projection onto the Simplex.\nMathieu Blondel, Akinori Fujino, and Naonori Ueda.\nICPR 2014.\nhttp:\/\/www.mblondel.org\/publications\/mblondel-icpr2014.pdf\n\"\"\"\n\nimport numpy as np\n\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils import check_random_state\nfrom sklearn.preprocessing import LabelEncoder\n\n\ndef projection_simplex(v, z=1):\n    \"\"\"\n    Projection onto the simplex:\n        w^* = argmin_w 0.5 ||w-v||^2 s.t. \\sum_i w_i = z, w_i >= 0\n    \"\"\"\n    # For other algorithms computing the same projection, see\n    # https:\/\/gist.github.com\/mblondel\/6f3b7aaad90606b98f71\n    n_features = v.shape[0]\n    u = np.sort(v)[::-1]\n    cssv = np.cumsum(u) - z\n    ind = np.arange(n_features) + 1\n    cond = u - cssv \/ ind > 0\n    rho = ind[cond][-1]\n    theta = cssv[cond][-1] \/ float(rho)\n    w = np.maximum(v - theta, 0)\n    return w\n\n\nclass MulticlassSVM(BaseEstimator, ClassifierMixin):\n\n    def __init__(self, C=1, max_iter=50, tol=0.05,\n                 random_state=None, verbose=0):\n        self.C = C\n        self.max_iter = max_iter\n        self.tol = tol,\n        self.random_state = random_state\n        self.verbose = verbose\n\n    def _partial_gradient(self, X, y, i):\n        # Partial gradient for the ith sample.\n        g = np.dot(X[i], self.coef_.T) + 1\n        g[y[i]] -= 1\n        return g\n\n    def _violation(self, g, y, i):\n        # Optimality violation for the ith sample.\n        smallest = np.inf\n        for k in range(g.shape[0]):\n            if k == y[i] and self.dual_coef_[k, i] >= self.C:\n                continue\n            elif k != y[i] and self.dual_coef_[k, i] >= 0:\n                continue\n\n            smallest = min(smallest, g[k])\n\n        return g.max() - smallest\n\n    def _solve_subproblem(self, g, y, norms, i):\n        # Prepare inputs to the projection.\n        Ci = np.zeros(g.shape[0])\n        Ci[y[i]] = self.C\n        beta_hat = norms[i] * (Ci - self.dual_coef_[:, i]) + g \/ norms[i]\n        z = self.C * norms[i]\n\n        # Compute projection onto the simplex.\n        beta = projection_simplex(beta_hat, z)\n\n        return Ci - self.dual_coef_[:, i] - beta \/ norms[i]\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n\n        # Normalize labels.\n        self._label_encoder = LabelEncoder()\n        y = self._label_encoder.fit_transform(y)\n\n        # Initialize primal and dual coefficients.\n        n_classes = len(self._label_encoder.classes_)\n        self.dual_coef_ = np.zeros((n_classes, n_samples), dtype=np.float64)\n        self.coef_ = np.zeros((n_classes, n_features))\n\n        # Pre-compute norms.\n        norms = np.sqrt(np.sum(X ** 2, axis=1))\n\n        # Shuffle sample indices.\n        rs = check_random_state(self.random_state)\n        ind = np.arange(n_samples)\n        rs.shuffle(ind)\n\n        violation_init = None\n        for it in range(self.max_iter):\n            violation_sum = 0\n\n            for ii in range(n_samples):\n                i = ind[ii]\n\n                # All-zero samples can be safely ignored.\n                if norms[i] == 0:\n                    continue\n\n                g = self._partial_gradient(X, y, i)\n                v = self._violation(g, y, i)\n                violation_sum += v\n\n                if v < 1e-12:\n                    continue\n\n                # Solve subproblem for the ith sample.\n                delta = self._solve_subproblem(g, y, norms, i)\n\n                # Update primal and dual coefficients.\n                self.coef_ += (delta * X[i][:, np.newaxis]).T\n                self.dual_coef_[:, i] += delta\n\n            if it == 0:\n                violation_init = violation_sum\n\n            vratio = violation_sum \/ violation_init\n\n            if self.verbose >= 1:\n                print(\"iter\", it + 1, \"violation\", vratio)\n\n            if vratio < self.tol:\n                if self.verbose >= 1:\n                    print(\"Converged\")\n                break\n\n        return self\n\n    def predict(self, X):\n        decision = np.dot(X, self.coef_.T)\n        pred = decision.argmax(axis=1)\n        return self._label_encoder.inverse_transform(pred)\n\n\n'''if __name__ == '__main__':\n    from sklearn.datasets import load_iris\n\n    iris = load_iris()\n    X, y = iris.data, iris.target\n\n    clf = MulticlassSVM(C=0.1, tol=0.01, max_iter=100, random_state=0, verbose=1)\n    clf.fit(X, y)\n    print(clf.score(X, y))\n    '''","898b3ea0":"classifier = MulticlassSVM(C=0.1, tol=0.01, max_iter=100, random_state=0, verbose=1)\nclassifier.fit(x_train, y_train)\nprint(classifier.score(x_train, y_train))","ecbf1a52":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\ny_pred1=classifier.predict(x_test)\ncm = confusion_matrix(y_test,y_pred1)\naccuracy= accuracy_score(y_test,y_pred1)","51a106a0":"cm\n","42dbec3c":"accuracy","83965645":"x = x_train[:5000]\nx.shape","97cf4e61":"#k_value1 = np.array(x@x.T + np.identity(5)*1e-12)\n#k_value1","bb84f2e8":"np.linalg.cholesky(k_value)","f8395770":"class SVM:\n\n    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n        self.lr = learning_rate\n        self.lambda_param = lambda_param\n        self.n_iters = n_iters\n        self.w = None\n        self.b = None\n\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        \n        y_ = np.where(y <= 0, -1, 1)\n        \n        self.w = np.zeros(n_features)\n        self.b = 0\n\n        for _ in range(self.n_iters):\n            for idx, x_i in enumerate(X):\n                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n                if condition:\n                    self.w -= self.lr * (2 * self.lambda_param * self.w)\n                else:\n                    self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))\n                    self.b -= self.lr * y_[idx]\n\n\n    def predict(self, X):\n        approx = np.dot(X, self.w) - self.b\n        return np.sign(approx)","bb5dd8dd":"classifier2 = SVM()\nclassifier2.fit(x_train,y_train)\n","8170d87a":"y_pred2 = classifier2.predict(x_test)\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\ncm2 = confusion_matrix(y_test,y_pred2)\naccuracy= accuracy_score(y_test,y_pred1)","080ba50f":"import numpy as np\nfrom sklearn import svm\nimport itertools\n\n\nclass MulticlassSVM:\n\n    def __init__(self):\n\n        self.labels = None\n        self.binary_svm = None\n        self.W = None\n        \n    def fit_cs(self, X, y):\n        self.labels = np.unique(y)\n        X_intercept = np.hstack([X, np.ones((len(X), 1))])\n\n        N, d = X_intercept.shape\n        K = len(self.labels)\n\n        W = np.zeros((K, d))\n\n        n_iter = 1500\n        learning_rate = 1e-8\n        for i in range(n_iter):\n            W -= learning_rate * self.grad_student(W, X_intercept, y)\n\n        self.W = W\n        \n    def predict_cs(self, X):\n        X_intercept = np.hstack([X, np.ones((len(X), 1))])\n        return self.labels[np.argmax(self.W.dot(X_intercept.T), axis=0)]\n\n    def loss_student(self, W, X, y, C=1.0):\n        \"\"\"\n        Compute loss function given W, X, y.\n        For exact definitions, please check the MP document.\n        Arguments:\n            W: Weights. Numpy array of shape (K, d)\n            X: Features. Numpy array of shape (N, d)\n            y: Labels. Numpy array of shape N\n            C: Penalty constant. Will always be 1 in the MP.\n        Returns:\n            The value of loss function given W, X and y.\n        \"\"\"\n        if self.labels is None:\n            self.labels = np.unique(y)\n\n        # loss of regularization term\n        l2_loss = 0.5 * np.sum(W**2)\n\n        # gradient of the other term\n        # get the matrix of term 1 - delta(j, y_i) + w_j^T * x_i\n        loss_aug_inf = 1 - (self.labels[:, None] == y[None, :]) + np.matmul(W, np.transpose(X))  # (K, N)\n        # sum over N of max value in loss_aug_inf\n        loss_aug_inf_max_sum = np.sum(np.max(loss_aug_inf, axis=0))\n        # sum over N of w_{y_i}^T * x_i\n        wx_sum = np.sum(W[y] * X)\n        multiclass_loss = C * (loss_aug_inf_max_sum - wx_sum)\n\n        total_loss = l2_loss + multiclass_loss\n        return total_loss\n    def grad_student(self, W, X, y, C=1.0):\n        \"\"\"\n        Compute gradient function w.r.t. W given W, X, y.\n        For exact definitions, please check the MP document.\n        Arguments:\n            W: Weights. Numpy array of shape (K, d)\n            X: Features. Numpy array of shape (N, d)\n            y: Labels. Numpy array of shape N\n            C: Penalty constant. Will always be 1 in the MP.\n        Returns:\n            The gradient of loss function w.r.t. W,\n            in a numpy array of shape (K, d).\n        \"\"\"\n        if self.labels is None:\n            self.labels = np.unique(y)\n\n        # gradient of regularization term\n        l2_grad = W\n\n        # gradient of the other term\n        # get the matrix of term 1 - delta(j, y_i) + w_j^T * x_i\n        loss_aug_inf = 1 - (self.labels[:, None] == y[None, :]) + np.matmul(W, np.transpose(X))  # (K, N)\n        # get the j_max that maximizes the above matrix for every sample\n        j_max = np.argmax(loss_aug_inf, axis=0)  # (N,)\n        # gradient of sum(...) is:   x_i, if k == j_max_i and k != y_i  (pos_case)\n        #                           -x_i, if k != j_max_i and k == y_i  (neg_case)\n        #                              0, otherwise\n        pos_case = np.logical_and((self.labels[:, None] == j_max[None, :]), (self.labels[:, None] != y[None, :]))\n        neg_case = np.logical_and((self.labels[:, None] != j_max[None, :]), (self.labels[:, None] == y[None, :]))\n        multiclass_grad = C * np.matmul(pos_case.astype(int) - neg_case.astype(int) , X)\n\n        total_grad = l2_grad + multiclass_grad\n        return total_grad","40355a50":"print('Training self Crammer-Singer...')\nself_cs = MulticlassSVM()\nself_cs.fit_cs(x_train, y_train)\n","b71caec8":"from sklearn import metrics\nprint('Self Crammer-Singer Accuracy (train):',metrics.accuracy_score(y_train, self_cs.predict_cs(x_train)))\nprint('Self Crammer-Singer Accuracy (test) :',metrics.accuracy_score(y_test, self_cs.predict_cs(x_test)))","937e1b15":"### Linear SVM from Scratch ###","cd4d69df":"# Sentiment Analysis on IMDB data using SVM","3b4d6963":"### Dataset Exploration and preprocessing"}}