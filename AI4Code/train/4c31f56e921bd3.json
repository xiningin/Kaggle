{"cell_type":{"45add160":"code","2a7058e5":"code","0ed17453":"code","f96cfba9":"code","63635106":"code","b749961d":"code","4647f625":"code","f9d7ff8d":"code","9d41830c":"code","044735a5":"code","a279b940":"code","96dd65de":"code","9585a0ca":"code","ebcceecc":"code","682d134e":"code","9bb2bed2":"code","d27969cb":"code","a52556c0":"code","388a3460":"code","814d955c":"code","b1291471":"code","88c42a3d":"code","3aaed7d8":"code","4bd87b2d":"code","9a77647e":"code","4ae7ae49":"code","a689d3fb":"code","b4ecec30":"code","b474f051":"code","4aee938c":"code","a1e7f58d":"code","2a594b3e":"code","010aebd8":"code","695dfd59":"code","4556d3f7":"code","826849ef":"code","26ae04d0":"code","0e3fb27a":"code","88996660":"code","148c40d3":"code","4f2bdded":"code","3a82e1f7":"code","661ed8c4":"code","1b120b90":"code","6d69e5ad":"code","f66563ff":"code","db93fb7f":"code","41a9db6e":"code","620f1466":"code","d8b20b14":"code","7834654b":"code","73c51a6b":"code","aef038ef":"code","2000a183":"code","6fb83de2":"code","86af3acd":"markdown","cf2c61b8":"markdown","866a365b":"markdown","a399f310":"markdown","32f4a749":"markdown","2ac05e6a":"markdown","c0afc045":"markdown","11c306f9":"markdown","a4db6ca9":"markdown","937477df":"markdown","19bf1a8f":"markdown","73dd4ca8":"markdown","fe9034dd":"markdown","522f75d4":"markdown","2e5a2422":"markdown","1330fc60":"markdown","a03aafaa":"markdown","dffab0df":"markdown","a2b44932":"markdown","133e626f":"markdown","c5d76af9":"markdown","3ebca5c2":"markdown","cd9ebcde":"markdown","8c28541a":"markdown","0b98f3b3":"markdown","d3265686":"markdown","5296bff1":"markdown","8caf37c0":"markdown"},"source":{"45add160":"import numpy as np\nimport pandas as pd\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #\u7edf\u8ba1\u5e38\u7528\u51fd\u6570\n\n# \u5bfc\u5165\u53ef\u89c6\u5316\u5de5\u5177\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport matplotlib.pylab as pylab\nimport seaborn as sns\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8","2a7058e5":"data_train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndata_test = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ndata_test_ID = data_test['Id']\n# \u89c2\u5bdf\u524d5\u884c\ndata_train.head()","0ed17453":"print(\"\u8bad\u7ec3\u96c6\u7684\u5927\u5c0f\u4e3a\uff1a{}\\n\u6d4b\u8bd5\u96c6\u7684\u5927\u5c0f\u4e3a\uff1a{}\".format(data_train.shape, data_test.shape))","f96cfba9":"data_train.info()","63635106":"# DataFrame\u81ea\u5e26\u7d22\u5f15\uff0c\u8fd9\u91cc\u53bb\u6389'Id'\u4e00\u5217\ndata_train = data_train.drop(['Id'], axis=1)\ndata_test = data_test.drop(['Id'], axis=1)\ndata_train.head()","b749961d":"target = 'SalePrice'\ndata_cleaner = [data_train, data_test]","4647f625":"plt.figure(figsize=(8,6), dpi=80)\nplt.scatter(x = data_train['GrLivArea'], y = data_train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","f9d7ff8d":"# \u5982\u56fe\u793a\uff0c\u53f3\u4e0b\u65b9\u51fa\u73b0\u4e24\u4e2a\u5927\u9762\u79ef\u4f4e\u623f\u4ef7\u7684\u6837\u672c\uff0c\u8fd9\u91cc\u8fdb\u884c\u6e05\u9664\ndata_train = data_train.drop(data_train[(data_train['GrLivArea']>4000) & (data_train['SalePrice']<300000)].index)\n\n# \u518d\u6b21\u68c0\u67e5\nplt.figure(figsize=(8,6), dpi=80)\nplt.scatter(data_train['GrLivArea'], data_train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","9d41830c":"# \u67e5\u770b\u8bad\u7ec3\u96c6\u5927\u5c0f\uff0c\u5982\u671f\u5c11\u4e862\u6761\u8bb0\u5f55\ndata_train.shape","044735a5":"# \u83b7\u53d6\u6982\u7387\u5206\u5e03\u53c2\u6570\u03bc\uff08mu\uff09\u548c\u03c3\uff08sigma\uff09\n(mu, sigma) = norm.fit(data_train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n# \u753b\u51fa\u6982\u7387\u5206\u5e03\u56fe\nsns.distplot(data_train[target] , fit=norm)\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n# \u753b\u51fa\u6b63\u6001\u6982\u7387\u56feQQ-plot\nfig = plt.figure()\nstats.probplot(data_train['SalePrice'], plot=plt)\nplt.show()","a279b940":"# skewness & Kurtosis\nprint(\"Skewness: %f\" % data_train[target].skew())\nprint(\"Kurtosis: %f\" % data_train[target].kurt())","96dd65de":"# \u5bf9\u6570\u636e\u8fdb\u884cbox cox\u8f6c\u6362\uff0c\u7528numpy log1p\u8fdb\u884clog(1+x)\u7684\u53d8\u6362\uff0c\u4ea7\u751f\u65b0\u7684column [SalePrice_trans]\ndata_train['SalePrice_trans'] = np.log1p(data_train[target])\ntarget_trans = 'SalePrice_trans'\n# \u753b\u56fe\u89c2\u5bdf\nsns.distplot(data_train[target_trans])\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n# QQ-plot\nfig = plt.figure()\nstats.probplot(data_train[target_trans], plot=plt)\nplt.show()\n# skewness & Kurtosis\nprint(\"Skewness: %f\" % data_train[target_trans].skew())\nprint(\"Kurtosis: %f\" % data_train[target_trans].kurt())","9585a0ca":"# \u5b9a\u4e49\u76ee\u6807\u503c\uff0c\u7b2c3\u90e8\u5206\u8fdb\u884c\u8bad\u7ec3\u65f6\u8c03\u7528\ndata_target = data_train['SalePrice_trans']","ebcceecc":"missing_train = pd.DataFrame({'Missing Number': data_train.isnull().sum().sort_values(ascending=False)})\nmissing_train = missing_train.drop(missing_train[missing_train['Missing Number']==0].index)\nmissing_test = pd.DataFrame({'Missing Number':data_test.isnull().sum().sort_values(ascending=False)})\nmissing_test = missing_test.drop(missing_test[missing_test['Missing Number']==0].index)","682d134e":"missing_train","9bb2bed2":"columns1 = ['PoolQC', 'FireplaceQu','GarageQual','GarageCond','FireplaceQu','GarageFinish','Fence','GarageType', 'GarageCars', 'GarageArea',\n            'BsmtFinType2','BsmtQual','BsmtCond','BsmtFinType1','MasVnrType','MiscFeature','Alley','Fence','BsmtExposure']\ncolumns2 = ['LotFrontage','MasVnrArea']\ncolumns3 = ['Electrical','MasVnrType', 'MSZoning','KitchenQual','Exterior1st','Exterior2nd','SaleType']","d27969cb":"for column1 in columns1:\n    data_train[column1] = data_train[column1].fillna(\"None\")\n    data_test[column1] = data_test[column1].fillna(\"None\")","a52556c0":"# LotFrontage\u662f\u8857\u9053\u79bb\u623f\u5b50\u7684\u8ddd\u79bb\uff0c\u8fde\u7eed\u578b\uff0c\u5176\u7ed3\u679c\u4e0e\u5176\u90bb\u5c45\u6709\u5173\u3002\u901a\u8fc7\u5206\u7c7b\u540e\u586b\u5145\u540c\u7c7b\u6c11\u5c45\u7684\u5e73\u5747\u503c\ndata_train['LotFrontage'] = data_train['LotFrontage'].groupby(by=data_train['Neighborhood']).apply(lambda x: x.fillna(x.mean()))\ndata_train['MasVnrArea'] = data_train['MasVnrArea'].fillna(data_train['MasVnrArea'].median())\ndata_test['LotFrontage'] = data_test['LotFrontage'].groupby(by=data_test['Neighborhood']).apply(lambda x: x.fillna(x.mean()))\ndata_test['MasVnrArea'] = data_test['MasVnrArea'].fillna(data_test['MasVnrArea'].median())","388a3460":"# Electrical, MasVnrType\u662f\u7c7b\u522b\u578b\uff0c\u7f3a\u5931\u901a\u8fc7mode\u586b\u8865\nfor column3 in columns3:\n    data_train[column3] = data_train[column3].fillna(data_train[column3].mode()[0])\n    data_test[column3] = data_test[column3].fillna(data_train[column3].mode()[0])","814d955c":"# \u7f3a\u5931GarageYrBlt\uff0c\u662f\u56e0\u4e3a\u4e0d\u5b58\u5728\u8f66\u5e93\uff0c\u8fd9\u91cc\u7531\u4e8e\u5e74\u4efd\u662f\u6570\u503c\u578b\uff0c\u8fd9\u91cc\u7528\u8f83\u8001\u5e74\u4efd1920\u4ee3\u66ff\ndata_train['GarageYrBlt'] = data_train['GarageYrBlt'].fillna(1920)\ndata_test['GarageYrBlt'] = data_test['GarageYrBlt'].fillna(1920)","b1291471":"# Functional : \u63d0\u4f9b\u7684\u6570\u636e\u63cf\u8ff0\u4e2d\uff0c\u8868\u793aNA\u662ftypical\ndata_train[\"Functional\"] = data_train[\"Functional\"].fillna(\"Typ\")\ndata_test[\"Functional\"] = data_test[\"Functional\"].fillna(\"Typ\")","88c42a3d":"# Utilities : \u5177\u4f53\u7684\u7279\u5f81\u503c\u4e3b\u8981\u662f\"AllPub\", \u9664\u4e86\u4e00\u4e2a\"NoSeWa\" \u548c\u4e24\u4e2aNA\u3002\u6ca1\u6709\u592a\u5927\u7684\u9884\u6d4b\u4ef7\u503c\uff0c\u8fd9\u91cc\u5c06\u8be5\u7279\u5f81\u79fb\u9664\ndata_train = data_train.drop(['Utilities'], axis=1)\ndata_test = data_test.drop(['Utilities'], axis=1)","3aaed7d8":"# BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath : \u7f3a\u5931\u7684\u6570\u636e\u662f\u7531\u4e8e\u6ca1\u6709basement\uff0c\u8fd9\u91cc\u75280\u586b\u5145\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    data_train[col] = data_train[col].fillna(0)\n    data_test[col] = data_test[col].fillna(0)","4bd87b2d":"missing_train = pd.DataFrame({'Missing Number': data_train.isnull().sum().sort_values(ascending=False)})\nmissing_train = missing_train.drop(missing_train[missing_train['Missing Number']==0].index)\nmissing_test = pd.DataFrame({'Missing Number':data_test.isnull().sum().sort_values(ascending=False)})\nmissing_test = missing_test.drop(missing_test[missing_test['Missing Number']==0].index)","9a77647e":"missing_train","4ae7ae49":"missing_test","a689d3fb":"data_train['TotalSF'] = data_train['TotalBsmtSF'] + data_train['1stFlrSF'] + data_train['2ndFlrSF']\ndata_test['TotalSF'] = data_test['TotalBsmtSF'] + data_test['1stFlrSF'] + data_test['2ndFlrSF']","b4ecec30":"for dataset in data_cleaner:\n#MSSubClass\n    dataset['MSSubClass'] = dataset['MSSubClass'].astype(str)\n\n#OverallCond\n    dataset['OverallCond'] = dataset['OverallCond'].astype(str)\n\n#Year and month sold\n    dataset['YrSold'] = dataset['YrSold'].astype(str)\n    dataset['MoSold'] = dataset['MoSold'].astype(str)","b474f051":"cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\nfrom sklearn.preprocessing import LabelEncoder\nfor c in cols:\n    label = LabelEncoder()\n    data_train[c] = label.fit_transform(data_train[c])\n    data_test[c] = label.fit_transform(data_test[c])","4aee938c":"#\u67e5\u770b\u6570\u636e\u4ecd\u662f\u5b57\u7b26\u4e32\u7684\u7279\u5f81\u6709\u54ea\u4e9b\ndata_train.dtypes[data_train.dtypes == 'object'].index","a1e7f58d":"feats_numeric = data_train.dtypes[data_train.dtypes != \"object\"].index\n\n# \u68c0\u67e5train\u8bad\u7ec3\u96c6\u4e2d\u9700\u8981\u504f\u5dee\u7684\u7279\u5f81\u503c\u6709\u54ea\u4e9b\ndata_skewness = pd.DataFrame({'Skew' :data_train[feats_numeric].apply(lambda x: skew(x)).sort_values(ascending=False)})\ndata_skewness = data_skewness[abs(data_skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(data_skewness.shape[0]))\ndata_skewness.head(5)","2a594b3e":"# \u6682\u65f6\u4e0d\u7528\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u6c42\u6700\u4f73lam\u503c\uff0c\u56e0\u4e3a\u7279\u5f81\u4e2d\u51fa\u73b0\u5f88\u591a\u7279\u5f81\u503c\u4e3a0\u7684\u60c5\u51b5\n# def lam_best(y):\n#     lam_range = np.linspace(-2,5,100)  # default nums=50\n#     for i,lam in enumerate(lam_range):\n#         llf = np.zeros(lam_range.shape, dtype=float)\n#         llf[i] = stats.boxcox_llf(lam, y) # y>0\n#         lam_best = lam_range[llf.argmax()]\n#     return(lam_best)\n\nfrom scipy.special import boxcox1p\nfeats_skewed = data_skewness.index\nlam = 0.15\nfor feat in feats_skewed:\n    #all_data[feat] += 1\n    data_train[feat] = boxcox1p(data_train[feat], lam)","010aebd8":"# \u5bf9test\u6d4b\u8bd5\u96c6\u7684\u504f\u5dee\u7279\u5f81\u503c\u8fdb\u884c\u53d8\u6362\nfeats_numeric_test = data_test.dtypes[data_test.dtypes != \"object\"].index\ndata_skewness = pd.DataFrame({'Skew' :data_test[feats_numeric_test].apply(lambda x: skew(x)).sort_values(ascending=False)})\ndata_skewness = data_skewness[abs(data_skewness) > 0.75]\nfrom scipy.special import boxcox1p\nfeats_skewed = data_skewness.index\nlam = 0.15\nfor feat in feats_skewed:\n    #all_data[feat] += 1\n    data_test[feat] = boxcox1p(data_test[feat], lam)","695dfd59":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost\nimport lightgbm","4556d3f7":"data_feats = data_train.drop(['SalePrice', 'SalePrice_trans'], axis=1)","826849ef":"data_all = pd.concat((data_feats, data_test))\nnum = data_feats.shape[0]\ndata_all = pd.get_dummies(data_all)\ndata_x = data_all[:num]\ndata_test = data_all[num:]","26ae04d0":"# \u68c0\u67e5\u8bad\u7ec3\u96c6\u7279\u5f81\u503c\u4e0e\u76ee\u6807\u503c\u662f\u5426\u80fd\u5bf9\u4e0a\n# \u8bad\u7ec3\u96c6\u7279\u5f81\u503c\ndata_x.shape","0e3fb27a":"# \u8bad\u7ec3\u96c6\u76ee\u6807\u503c\ndata_target.shape","88996660":"# \u6d4b\u8bd5\u96c6\ndata_test.shape","148c40d3":"n_folds = 5\ndef RMSE(alg):\n    kf = KFold(n_folds, shuffle=True, random_state=20).get_n_splits(data_x)\n    rmse= np.sqrt(-cross_val_score(alg, data_x, data_target, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","4f2bdded":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nscore = RMSE(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","3a82e1f7":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\nscore = RMSE(ENet)\nprint(\"\\nENet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","661ed8c4":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nscore = RMSE(KRR)\nprint(\"\\nKRR score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","1b120b90":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\nscore = RMSE(GBoost)\nprint(\"\\nGBoost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","6d69e5ad":"model_xgb = xgboost.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nscore = RMSE(model_xgb)\nprint(\"\\nXGBoost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","f66563ff":"model_lgb = lightgbm.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\nscore = RMSE(model_lgb)\nprint(\"\\nLGBoost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","db93fb7f":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)  ","41a9db6e":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = RMSE(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","620f1466":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=42)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X.iloc[train_index], y.iloc[train_index])\n                y_pred = instance.predict(X.iloc[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","d8b20b14":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\nscore = RMSE(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","7834654b":"# StackingAveragedModels\u5f97\u5206\u6700\u4f73","73c51a6b":"model_fit = stacked_averaged_models.fit(data_x, data_target)","aef038ef":"result = np.expm1(stacked_averaged_models.predict(data_test)) # log(1+x)\u9006\u8fd0\u7b97","2000a183":"# \u4fdd\u5b58\u9884\u6d4b\u7ed3\u679c\nsubmission = pd.DataFrame()\nsubmission['Id'] = data_test_ID\nsubmission['SalePrice'] = result\nsubmission.to_csv('\/kaggle\/working\/submission',index=False)","6fb83de2":"import pandas as pd\nsample_submission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")","86af3acd":"#2.2.3 \u518d\u6b21\u786e\u8ba4\u7f3a\u5931\u6570\u636e\u60c5\u51b5","cf2c61b8":"#1.\u6570\u636e\u51c6\u5907\u4e0e\u5904\u7406\n#1.1 \u5bfc\u5165\u6570\u636e\u5904\u7406\u9700\u8981\u7684\u5e93","866a365b":"\u4ece\u6982\u7387\u5206\u5e03\u56fe\uff0c\u6b63\u6001\u6982\u7387\u5206\u5e03\u56fe\uff0c\u53caskewness>0\uff0c\u53ef\u4ee5\u5224\u65ad\u76ee\u6807\u503c\u6570\u636e\u53f3\u504f\n\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u4e2d\u7684\u4e00\u4e2a\u524d\u63d0\u5047\u8bbe\u5c31\u662f\u8981\u6c42\u6837\u672c\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u7684\u6b8b\u5dee\u670d\u4ece\u6b63\u6001\u5206\u5e03\uff080\u5747\u503c\uff09\uff0c\u4e5f\u5373\u56e0\u53d8\u91cf\u4e5f\u670d\u4ece\u6b63\u6001\u5206\u5e03\n\u5bf9\u6570\u636e\u8fdb\u884c\u6b63\u6001\u5316\u53d8\u6362\u66f4\u5229\u4e8e\u540e\u9762\u8fdb\u884c\u56de\u5f52\u6a21\u578b\u8bad\u7ec3","a399f310":"#3.6 \u7ed3\u679c\u9884\u6d4b\u4e0e\u4fdd\u5b58","32f4a749":"#3.3 \u7279\u5f81\u503conehot\u7f16\u7801\u8f6c\u6362","2ac05e6a":"#2.2 Completing\uff1a\u586b\u8865\u7f3a\u5931\u6570\u636e\n#2.2.1 \u67e5\u770b\u7f3a\u5931\u6570\u636e","c0afc045":"#3.5.7 AveragingModels\u5b9a\u4e49\u4e0e\u5f97\u5206","11c306f9":"#2.2.2 \u5206\u7c7b\u586b\u8865\u7f3a\u5931\u6570\u636e\n#PoolQC\u662f\u6cf3\u6c60\u7684\u7b49\u7ea7\uff0c\u7c7b\u522b\u578b\uff0c\u8fd9\u91cc\u7f3a\u5931\u662f\u56e0\u4e3a\u6cf3\u6c60\uff0c\u6240\u4ee5\u901a\u8fc7\u586b\u8865\u201cNA\u201d\u5b57\u6bb5\u4ee3\u66ff\uff0c\u540c\u6837\u5730\uff0c\u76f8\u540c\u5904\u7406\u7279\u5f81FireplaceQu, GarageQual, GarageCond, FireplaceQu, GarageFinish, Fence\uff0cGarageType\uff0cBsmtFinType2\uff0cBsmtQual\uff0cBsmtCond\uff0cBsmtFinType1\uff0cMasVnrType\uff0cMiscFeature\uff0cAlley\uff0cFence, BsmtExposure\n#LotFrontage\u662f\u8857\u9053\u79bb\u623f\u5b50\u7684\u8ddd\u79bb\uff0c\u8fde\u7eed\u578b\uff0c\u7f3a\u5931\u901a\u8fc7median\u586b\u8865\uff0c\u7c7b\u4f3c\u7684\u6709 MasVnrArea\n#Electrical, MasVnrType, MSZoning, KitchenQual, Exterior1st, Exterior2nd, SaleType\u662f\u7c7b\u522b\u578b\uff0c\u7f3a\u5931\u901a\u8fc7mode\u586b\u8865","a4db6ca9":"#3.5.5 XGboost\u6a21\u578b\u9884\u6d4b\u5f97\u5206\n","937477df":"#2.4 Converting","19bf1a8f":"#2 \u6570\u636e\u6e05\u6d17","73dd4ca8":"#3.5.8 StackingAveragedModels\u5b9a\u4e49\u4e0e\u5f97\u5206","fe9034dd":"#3.5.3 KRR\u6a21\u578b\u9884\u6d4b\u53ca\u5f97\u5206","522f75d4":"#2.4.2 Categorial\u7c7b\u522b\u578b\u6570\u636e\u8f6c\u6362\u6210\u6570\u503c\u578b\u8fdb\u884c\u8ba1\u7b97\uff08\u90e8\u5206\u9002\u5408OneHot\u7f16\u7801\u7684\uff0c\u5728\u5212\u5206\u7279\u5f81\u96c6\u548c\u6d4b\u8bd5\u96c6\u540e\u518d\u8fdb\u884c\u8f6c\u6362\uff09","2e5a2422":"#2.1.1 Correcting\u2014\u2014\u79bb\u7fa4\u70b9\u89c2\u5bdf\u53ca\u6e05\u9664\n\u6839\u636e\u751f\u6d3b\u7ecf\u9a8c\uff0c\u5f71\u54cd\u623f\u4ef7\u6700\u5927\u7684\u56e0\u7d20\u662f\u9762\u79ef\uff0c\u6240\u4ee5\u89c2\u5bdf\u9762\u79ef\u4e0e\u4ef7\u683c\u7684\u5173\u7cfb\uff0c\u5bb9\u6613\u5224\u65ad\u79bb\u7fa4\u70b9","1330fc60":"#2.3 Creating\uff1a \u5b9e\u9645\u7ecf\u9a8c\u544a\u8bc9\u6211\u4eec\uff0c\u623f\u5b50\u7684\u6240\u6709\u5b9e\u7528\u9762\u79ef\u5927\u5c0f\uff0c\u4f1a\u6700\u5927\u7a0b\u5ea6\u5730\u5f71\u54cd\u623f\u4ef7\uff0c\u6240\u4ee5\u8fd9\u91cc\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u7279\u5f81TotalSF","a03aafaa":"#3.2 \u7279\u5f81\u503c\u5212\u5206","dffab0df":"#1.2 \u8bfb\u53d6\u6570\u636e\u53ca\u521d\u6b65\u89c2\u5bdf","a2b44932":"\u521d\u6b65\u4fe1\u606f\uff1a1\u3001\u7279\u5f81\u503c\u975e\u5e38\u591a\uff0c\u96be\u4ee5\u9010\u4e00\u5206\u6790  2\u3001\u90e8\u5206\u7279\u5f81\u7f3a\u5931\u503c\u5f88\u591a","133e626f":"#2.1.2 Correcting\u2014\u2014\u76ee\u6807\u503c\u6b63\u6001\u6027\u68c0\u6d4b\n\u5bf9\u76ee\u6807\u503cSalePrice\u8fdb\u884c\u6982\u7387\u5206\u5e03\u5206\u6790\u5904\u7406","c5d76af9":"#2.4.3 \u504f\u5dee\u7279\u5f81\u503c\u5904\u7406","3ebca5c2":"#2.4.1 \u6570\u503c\u578b\u7279\u5f81\u8f6c\u6362\u6210\u5b57\u7b26\u578b\uff0c\u540e\u9762\u65b9\u4fbf\u8f6c\u6362\u6210onehot\u5f62\u5f0f\u3002\u6709\u90e8\u5206\u6570\u503c\u578b\u7684\u7279\u5f81\uff0c\u5b9e\u9645\u4e0a\u662f\u7c7b\u522b\u578b\u7684\u6570\u636e\uff0c\u8fd9\u7c7b\u8fdb\u884c\u9002\u5f53\u7684\u8f6c\u6362","cd9ebcde":"#3.5.4 GradientBoostingRegressor\u6a21\u578b\u9884\u6d4b\u5f97\u5206","8c28541a":"#3.5.2 ENet\u6a21\u578b\u9884\u6d4b\u53ca\u5f97\u5206","0b98f3b3":"#3.\u8bad\u7ec3\u6a21\u578b\n#3.1 \u5bfc\u5165\u5e93","d3265686":"#3.4 \u5b9a\u4e49\u635f\u5931\u51fd\u6570RMSE\uff0c\u56e0\u4e3a\u623f\u4ef7\u9884\u6d4b\uff0c\u624d\u7528\u5747\u65b9\u6839\u8bef\u5dee\u80fd\u53cd\u5e94\u6570\u503c\u5927\u5c0f\u4e0a\u7684\u76f4\u63a5\u5dee\u522b","5296bff1":"#3.5 \u6a21\u578b\u9884\u6d4b\n#3.5.1 Lasso\u6a21\u578b\u9884\u6d4b\u53ca\u5f97\u5206","8caf37c0":"#3.5.6 LightGBM\u6a21\u578b\u9884\u6d4b\u5f97\u5206"}}