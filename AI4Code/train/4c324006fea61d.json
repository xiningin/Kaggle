{"cell_type":{"0c911b4a":"code","7b126123":"code","d91ac176":"markdown"},"source":{"0c911b4a":"import numpy as np\n\nclass HMM:\n    def __init__(self, A, B, pi):\n        self.A = A\n        self.B = B\n        self.pi = pi\n\n    def _forward(self, obs_seq):\n        N = self.A.shape[0]\n        T = len(obs_seq)\n\n        F = np.zeros((N,T))\n        F[:,0] = self.pi * self.B[:, obs_seq[0]]\n\n        for t in range(1, T):\n            for n in range(N):\n                F[n,t] = np.dot(F[:,t-1], (self.A[:,n])) * self.B[n, obs_seq[t]]\n\n        return F\n\n    def _backward(self, obs_seq):\n        N = self.A.shape[0]\n        T = len(obs_seq)\n\n        X = np.zeros((N,T))\n        X[:,-1:] = 1\n\n        for t in reversed(range(T-1)):\n            for n in range(N):\n                X[n,t] = np.sum(X[:,t+1] * self.A[n,:] * self.B[:, obs_seq[t+1]])\n\n        return X\n\n    def observation_prob(self, obs_seq):\n        \"\"\" P( entire observation sequence | A, B, pi ) \"\"\"\n        return np.sum(self._forward(obs_seq)[:,-1])\n\n    def state_path(self, obs_seq):\n        \"\"\"\n        Returns\n        -------\n        V[last_state, -1] : float\n            Probability of the optimal state path\n        path : list(int)\n            Optimal state path for the observation sequence\n        \"\"\"\n        V, prev = self.viterbi(obs_seq)\n\n        # Build state path with greatest probability\n        last_state = np.argmax(V[:,-1])\n        path = list(self.build_viterbi_path(prev, last_state))\n\n        return V[last_state,-1], reversed(path)\n\n    def viterbi(self, obs_seq):\n        \"\"\"\n        Returns\n        -------\n        V : numpy.ndarray\n            V [s][t] = Maximum probability of an observation sequence ending\n                       at time 't' with final state 's'\n        prev : numpy.ndarray\n            Contains a pointer to the previous state at t-1 that maximizes\n            V[state][t]\n        \"\"\"\n        N = self.A.shape[0]\n        T = len(obs_seq)\n        prev = np.zeros((T - 1, N), dtype=int)\n\n        # DP matrix containing max likelihood of state at a given time\n        V = np.zeros((N, T))\n        V[:,0] = self.pi * self.B[:,obs_seq[0]]\n\n        for t in range(1, T):\n            for n in range(N):\n                seq_probs = V[:,t-1] * self.A[:,n] * self.B[n, obs_seq[t]]\n                prev[t-1,n] = np.argmax(seq_probs)\n                V[n,t] = np.max(seq_probs)\n\n        return V, prev\n\n    def build_viterbi_path(self, prev, last_state):\n        \"\"\"Returns a state path ending in last_state in reverse order.\"\"\"\n        T = len(prev)\n        yield(last_state)\n        for i in range(T-1, -1, -1):\n            yield(prev[i, last_state])\n            last_state = prev[i, last_state]\n\n    def simulate(self, T):\n\n        def draw_from(probs):\n            return np.where(np.random.multinomial(1,probs) == 1)[0][0]\n\n        observations = np.zeros(T, dtype=int)\n        states = np.zeros(T, dtype=int)\n        states[0] = draw_from(self.pi)\n        observations[0] = draw_from(self.B[states[0],:])\n        for t in range(1, T):\n            states[t] = draw_from(self.A[states[t-1],:])\n            observations[t] = draw_from(self.B[states[t],:])\n        return observations,states\n\n    def baum_welch_train(self, observations, criterion=0.05):\n        n_states = self.A.shape[0]\n        # \u89c2\u5bdf\u5e8f\u5217\u7684\u957f\u5ea6T\n        n_samples = len(observations)\n\n        done = False\n        while not done:\n            # alpha_t(i) = P(o_1,o_2,...,o_t,q_t = s_i | hmm)\n            # Initialize alpha\n            # \u83b7\u5f97\u6240\u6709\u524d\u5411\u4f20\u64ad\u8282\u70b9\u503c alpha_t(i)\n            alpha = self._forward(observations)\n\n            # beta_t(i) = P(o_t+1,o_t+2,...,o_T | q_t = s_i , hmm)\n            # Initialize beta\n            # \u83b7\u5f97\u6240\u6709\u540e\u5411\u4f20\u64ad\u8282\u70b9\u503c beta_t(i)\n            beta = self._backward(observations)\n\n            # \u8ba1\u7b97 xi_t(i,j) -> xi(i,j,t)\n            xi = np.zeros((n_states, n_states, n_samples - 1))\n            # \u5728\u6bcf\u4e2a\u65f6\u523b\n            for t in range(n_samples - 1):\n                # \u8ba1\u7b97P(O | hmm)\n                denom = sum(alpha[:, -1])\n                for i in range(n_states):\n                    # numer[1,:] = \u884c\u5411\u91cf\uff0calpha[i,t]=\u5b9e\u6570\uff0cslef.A[i,:] = \u884c\u5411\u91cf\n                    # self.B[:,observations[t+1]].T = \u884c\u5411\u91cf,beta[:,t+1].T = \u884c\u5411\u91cf\n                    numer = alpha[i, t] * self.A[i, :] * self.B[:, observations[t + 1]].T * beta[:, t + 1].T\n                    xi[i, :, t] = numer \/ denom\n\n                # \u8ba1\u7b97gamma_t(i) \u5c31\u662f\u5bf9j\u8fdb\u884c\u6c42\u548c\n                gamma = np.sum(xi, axis=1)\n                # need final gamma elements for new B\n                prod = (alpha[:, n_samples - 1] * beta[:, n_samples - 1]).reshape((-1, 1))\n                # \u5408\u5e76T\u65f6\u523b\u7684\u8282\u70b9\n                gamma = np.hstack((gamma, prod \/ np.sum(prod)))\n                # \u5217\u5411\u91cf\n                newpi = gamma[:, 0]\n                newA = np.sum(xi, 2) \/ np.sum(gamma[:, :-1], axis=1).reshape((-1, 1))\n                newB = np.copy(self.B)\n\n                # \u89c2\u6d4b\u72b6\u6001\u6570\n                num_levels = self.B.shape[1]\n                sumgamma = np.sum(gamma, axis=1)\n                for lev in range(num_levels):\n                    mask = observations == lev\n                    newB[:, lev] = np.sum(gamma[:, mask], axis=1) \/ sumgamma\n\n                if np.max(abs(self.pi - newpi)) < criterion and \\\n                                np.max(abs(self.A - newA)) < criterion and \\\n                                np.max(abs(self.B - newB)) < criterion:\n                    done = 1\n                self.A[:], self.B[:], self.pi[:] = newA, newB, newpi\n","7b126123":"# \u72b6\u6001\u8f6c\u79fb\u6982\u7387\nA = np.array([\n    [  0,   1,   0,   0],\n    [0.4,   0, 0.6,   0],\n    [  0, 0.4,   0, 0.6],\n    [  0,   0, 0.5, 0.5],\n])\n# \u72b6\u6001\u89c2\u6d4b\u6982\u7387\nB = np.array([\n    [0.5,0.5],\n    [0.3,0.7],\n    [0.6,0.4],\n    [0.8,0.2],\n])\n# \u521d\u59cb\u72b6\u6001\u6982\u7387\npi = np.array([0.25,0.25,0.25,0.25])\n\n\n# \u4f7f\u7528\u6211\u4eec\u8bbe\u5b9a\u7684\u6a21\u578b\u53c2\u6570\u751f\u6210\u89c2\u6d4b\u5e8f\u5217\nh = HMM(A, B, pi)\nobservations_data, states_data = h.simulate(1000)\n\n\n# \u8bad\u7ec3\u6a21\u578b\nguess = HMM(np.full((4,4),0.25),np.full((4,2),0.5),np.full((4),0.25))\nguess.baum_welch_train(observations_data)\nprint(guess.A)\nprint(guess.B)\nprint(guess.pi)\n\n# \u9884\u6d4b\u5e76\u6253\u5370\u6b63\u786e\u7387\nstates_out = guess.state_path(observations_data)[1]\np = 0.0\nfor s in states_data:\n    if next(states_out) == s: p += 1\n\nprint(p \/ len(states_data))\n\n# \u6d4b\u8bd5\u9884\u6d4b\u51fd\u6570\nstates_out = h.state_path(observations_data)[1]\np = 0.0\nfor s in states_data:\n    if next(states_out) == s: p += 1\n\nprint(p \/ len(states_data))","d91ac176":"\u914d\u5957\u6587\u7ae0\u89c1[\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b](https:\/\/drivingc.com\/p\/5b4eda402392ec0cef4dbc92)"}}