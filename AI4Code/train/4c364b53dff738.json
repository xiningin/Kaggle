{"cell_type":{"d7da070d":"code","3225b5cc":"code","1c93c936":"code","acef601e":"code","67bb1250":"code","89dbbc2b":"code","0028c126":"code","395949c8":"code","0a0f5d03":"code","cf6e47f5":"code","4549d414":"code","190bf9d3":"code","bf79c6c5":"code","b64c6605":"code","b385d414":"code","0ff7498d":"code","44231b0d":"code","19d43b42":"code","392fe7f3":"markdown","518707fc":"markdown","cd7cf02b":"markdown","d83d3501":"markdown","5ea4ee2e":"markdown","1c04c69b":"markdown","bc7d47b7":"markdown","a636be74":"markdown","a936a070":"markdown","289b907c":"markdown"},"source":{"d7da070d":"# ====================================================\n# Directory settings\n# ====================================================\nimport os\n\nOUTPUT_DIR = '.\/'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)","3225b5cc":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    model_dir='..\/input\/jigsaw-deberta-v3-base-train-model-3'\n    num_workers=4\n    model=\"..\/input\/deberta-v3-base\/deberta-v3-base\"\n    batch_size=128\n    fc_dropout=0.0000001\n    text=\"text\"\n    target=\"target\"\n    target_size=1\n    head=32\n    tail=32\n    seed=2021\n    n_fold=5\n\n\nCFG.max_len = CFG.head + CFG.tail","1c93c936":"# ====================================================\n# Library\n# ====================================================\nimport os\nimport gc\nimport re\nimport sys\nimport json\nimport time\nimport math\nimport string\nimport pickle\nimport random\nimport joblib\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport logging\nlogging.basicConfig(level=logging.ERROR)\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\nimport transformers\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig \nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","acef601e":"# ====================================================\n# Utils\n# ====================================================\ndef get_score(df):\n    score = len(df[df['less_toxic_pred'] < df['more_toxic_pred']]) \/ len(df)\n    return score\n\n\ndef get_logger(filename=OUTPUT_DIR+'train'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = get_logger()\n\ndef seed_everything(seed=2021):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=2021)","67bb1250":"# ====================================================\n# Data Loading\n# ====================================================\ndf = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv')\nsub  = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/sample_submission.csv')\n","89dbbc2b":"tokenizer = AutoTokenizer.from_pretrained(CFG.model, lowercase=True)\ntokenizer.save_pretrained(OUTPUT_DIR+'tokenizer\/')\nCFG.tokenizer = tokenizer","0028c126":"# ====================================================\n# Dataset\n# ====================================================\ndef prepare_input(text, cfg):\n    if cfg.tail == 0:\n        inputs = cfg.tokenizer.encode_plus(text, \n                                           return_tensors=None, \n                                           add_special_tokens=True, \n                                           max_length=cfg.max_len,\n                                           pad_to_max_length=True,\n                                           truncation=True)\n        for k, v in inputs.items():\n            inputs[k] = torch.tensor(v, dtype=torch.long)\n    else:\n        inputs = cfg.tokenizer.encode_plus(text,\n                                           return_tensors=None, \n                                           add_special_tokens=True, \n                                           truncation=True)\n        for k, v in inputs.items():\n            v_length = len(v)\n            if v_length > cfg.max_len:\n                v = np.hstack([v[:cfg.head], v[-cfg.tail:]])\n            if k == 'input_ids':\n                new_v = np.ones(cfg.max_len) * cfg.tokenizer.pad_token_id\n            else:\n                new_v = np.zeros(cfg.max_len)\n            new_v[:v_length] = v \n            inputs[k] = torch.tensor(new_v, dtype=torch.long)\n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.text = df[cfg.text].fillna(\"none\").values\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, item):\n        text = str(self.text[item])\n        inputs = prepare_input(text, self.cfg)\n        return inputs","395949c8":"class AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector","0a0f5d03":"# ====================================================\n# Model\n# ====================================================\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        self.head = AttentionHead(self.config.hidden_size,self.config.hidden_size)\n        self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(self.config.hidden_size,1)\n\n    def forward(self, xb):\n        x = self.model(**xb)[0]\n        x = self.head(x)\n        x = self.dropout(x)\n        x = self.linear(x)\n        return x\n\nclass CustomModel_Legacy(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, cfg.target_size)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        feature = torch.mean(last_hidden_states, 1)\n        return feature\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        return output","cf6e47f5":"# ====================================================\n# inference\n# ====================================================\ndef inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.sigmoid().to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions","4549d414":"test_dataset = TestDataset(CFG, df)\ntest_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False, \n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\nconfig_path = CFG.model_dir+\"config.pth\"\npredictions = []\nfor fold in range(CFG.n_fold):\n    model = CustomModel(CFG, config_path=config_path, pretrained=False)\n    state = torch.load(CFG.model_dir+f\"\/jigsaw_fold{fold}_best.pth\", map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    predictions.append(prediction)\n    del model, state; gc.collect()\n    torch.cuda.empty_cache()","190bf9d3":"df['score'] = np.mean(predictions, axis=0)\ndf.to_csv(\"submission_nlp_raw.csv\", index=False)\ndf['score'] = df['score'].rank(method='first')\ndf[['comment_id', 'score']].to_csv(\"submission_ranked.csv\", index=False)\ndf.head()\n# 0.766  with this above logic ","bf79c6c5":"# Open the file\ndf_train = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\nprint('Dim Train :', df_train.columns)\n\n# If the pair has been ranked by multiple worker, we keep the order that is most unanimous\ndf_train['TEXT_ranked'] = df_train.apply(lambda row : row['less_toxic'] + ':' + row['more_toxic'], axis = 1)\ndf_train['TEXT_paire'] = df_train.apply(lambda row : min(row['less_toxic'], row['more_toxic']) + ':' + max(row['less_toxic'], row['more_toxic']), axis = 1)\ndf_train['Count_paire_ranked'] = df_train.groupby(['TEXT_ranked'])['TEXT_ranked'].transform('count')\ndf_train['Count_paire'] = df_train.groupby(['TEXT_paire'])['TEXT_ranked'].transform('count')\ndf_train['count_max'] = df_train.groupby(['TEXT_paire'])['Count_paire_ranked'].transform(max)\n\n# Selection\ndf_train = df_train[df_train['Count_paire_ranked'] == df_train['count_max']]\ndf_train = df_train[df_train['Count_paire_ranked'] == 3] # every workers agreed\n\n# Delete duplicates\ndf_train = df_train.drop(columns = ['worker'])\ndf_train = df_train.drop_duplicates()\n\n# Results\ndf_train = df_train.sort_values(by = ['TEXT_ranked'])\ndf_train = df_train.drop(columns = ['Count_paire', 'count_max', 'TEXT_ranked', 'TEXT_paire']).drop_duplicates()\nprint('Dim APRES :', df_train.shape)\ndf_train.head()\n# Add score to the validation dataset\ndf_train = df_train.merge(df[['text', 'score']], left_on = 'less_toxic', right_on = 'text', how = 'left').drop_duplicates()\ndf_train = df_train.rename(columns = {'score' : 'score_less'})\ndf_train = df_train.drop(columns = ['text'])\ndf_train = df_train.merge(df[['text', 'score']], left_on = 'more_toxic', right_on = 'text', how = 'left').drop_duplicates()\ndf_train = df_train.rename(columns = {'score' : 'score_more'})\ndf_train = df_train.drop(columns = ['text'])\n\n# Stats\ndf_train.head()","b64c6605":"# Stats\nprint(len(df_train[df_train['score_more'] < df_train['score_less']]), '\/', len(df_train))","b385d414":"# Test\ndf_train[df_train['score_more'] < df_train['score_less']].sort_values(['less_toxic'])","0ff7498d":"# Correction of scores\ndf_train['score_max_du_less_toxic'] = df_train.groupby(['less_toxic'])['score_more'].transform(min) # score_min des textes + toxics\ndf_train['score_min_du_more_toxic'] = df_train.groupby(['more_toxic'])['score_less'].transform(max) # score_max des textes - toxics\n\n# Join\ndf = df.merge(df_train[['less_toxic', 'score_less', 'score_max_du_less_toxic']], left_on = ['text', 'score'], right_on = ['less_toxic', 'score_less'], how = 'left')\ndf = df.drop(columns = ['less_toxic', 'score_less'])\ndf = df.merge(df_train[['more_toxic', 'score_more', 'score_min_du_more_toxic']], left_on = ['text', 'score'], right_on = ['more_toxic', 'score_more'], how = 'left')\ndf = df.drop(columns = ['more_toxic', 'score_more'])\n\n# Rename\ndf = df.rename(columns = {'score_max_du_less_toxic' : 'borne_max', 'score_min_du_more_toxic' : 'borne_min'}) # le score doit est + petit que borne_max\ndf = df[['comment_id', 'text', 'score', 'borne_min', 'borne_max']].drop_duplicates()\n\n# Aper\u00e7u\ndf.head()\n# CORRECTION of the scores\ndef corrige(row) :\n    score, borne_min, borne_max = row['score'], row['borne_min'], row['borne_max']\n    \n    if not(pd.isna(borne_min)) and not(pd.isna(borne_max)) :\n        if borne_max < borne_min : return (borne_max + borne_min ) \/\/ 2 # return score\n        if score < borne_min : return borne_min+1\n        if score > borne_max : return borne_max-1\n        else :\n            return score\n        \n    elif not(pd.isna(borne_min)) :\n        if score < borne_min : return borne_min+1\n        else : return score\n\n    elif not(pd.isna(borne_max)) :\n        if score > borne_max : return borne_max-1\n        else : return score\n        \n    else :\n        return score\n    \n# --------------------\n\n# Application of correction\ndf['score_corrige'] = df.apply(lambda row : corrige(row), axis=1)\ncorrections = df[df['score'] != df['score_corrige']]\nprint(\"Nb of corrections : {}\/{}.\".format(len(corrections), len(df)))\n\n\n# Show\ncorrections[['comment_id', 'text', 'score', 'score_corrige']].head()","44231b0d":"# Rank first\ndf = df[['comment_id', 'text', 'score_corrige']].drop_duplicates()\ndf['score'] = df['score_corrige'].rank(method='first')\ndf = df[['comment_id', 'text', 'score']].drop_duplicates()\n\n# Show\ndf.head()","19d43b42":"df = df[['comment_id', 'score']].drop_duplicates()\nprint(df.shape)\ndf.to_csv(\"submission.csv\", index=False)","392fe7f3":"# Dataset","518707fc":"# deberta-v3-base Inf \n\n**Train notebook** https:\/\/www.kaggle.com\/gauravbrills\/jigsaw-deberta-v3-base-train-model-3?scriptVersionId=80522297","cd7cf02b":"# inference","d83d3501":"# Data Loading","5ea4ee2e":"# submission","1c04c69b":"# Utils","bc7d47b7":"# Library","a636be74":"# CFG","a936a070":"# tokenizer","289b907c":"# Model"}}