{"cell_type":{"58e9ca68":"code","7b3f07b2":"code","219b9826":"code","e2037d03":"code","c0af3964":"code","4dc89d2f":"code","bfab01c5":"code","b09a858b":"code","d9537c34":"code","abb9ef5a":"code","b62fafee":"code","1d840888":"code","2f8e9f25":"code","6c366869":"code","4dcac782":"code","a52fd1e0":"code","691dc86d":"code","aa4ee1c0":"code","8f64e711":"code","64660a27":"code","2a3305c6":"code","3cbdf3b8":"code","df97efd1":"code","5959ee7a":"code","e9e22895":"code","2d781055":"code","94547e7e":"code","57768979":"code","fcd19a91":"code","4757c6bd":"code","dc2c29ee":"code","7ff6b5c9":"code","fab1d5d6":"markdown","70dfc7c1":"markdown","2ccd65a0":"markdown","f80c87a6":"markdown","b89892f6":"markdown","bf59876d":"markdown","a939e813":"markdown","25d20449":"markdown","79910e98":"markdown","7ab94750":"markdown","efa1dada":"markdown"},"source":{"58e9ca68":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","7b3f07b2":"#Reading the CSV files into respective dataframes\nquestions = pd.read_csv('\/kaggle\/input\/kaggle-survey-2019\/questions_only.csv')\nmcr = pd.read_csv('\/kaggle\/input\/kaggle-survey-2019\/multiple_choice_responses.csv')\notr = pd.read_csv('\/kaggle\/input\/kaggle-survey-2019\/other_text_responses.csv')\nschema = pd.read_csv('\/kaggle\/input\/kaggle-survey-2019\/survey_schema.csv')","219b9826":"#Converting the string characters to lower case to bring in uniformity for the field Q26_OTHER_TEXT\ntext_lower_cv = pd.DataFrame(otr['Q26_OTHER_TEXT'].str.lower())\ntext_cv = text_lower_cv.Q26_OTHER_TEXT.unique()\n","e2037d03":"# Installing NLTK to analyse the textual content of the field Q27_OTHER_TEXT\n!pip install nltk","c0af3964":"#Load nltk\nimport nltk","4dc89d2f":"#Creating a list with all the responses\nsentence_cv = ['']\ni = 0\nfor i in range(len(text_cv)):\n    sentence_cv.append(text_cv[i])\nsentence_cv\n","bfab01c5":"#Deleting some of the responses that doesn't give any relevant methods\ndel sentence_cv[0:3]\nsentence_cv.remove('am learning this')\ndel sentence_cv[11]\ndel sentence_cv[12]\n\n\n","b09a858b":"#Replacing spaces with hyphen to get full names\nsentence_cv[0] = 'time-based, lstm, i3d'\nsentence_cv[3] ='video-analysis'\nsentence_cv[10] ='glcm-wavelet'\nsentence_cv[12] ='triplet-loss'\nsentence_cv[13] ='triplet-loss'\nsentence_cv[18] ='pose-estimation'\nsentence_cv[19] ='pose-estimation'\nsentence_cv[23] ='triplet-loss'\nsentence_cv[29] ='i3d'\nsentence_cv","d9537c34":"#Creating onse single sentence with the unique value for further tokenization purpose\none_sentence_cv = \"\" \nfor index, value in enumerate(sentence_cv):\n    one_sentence_cv += (str(value)+\",\")\nprint(one_sentence_cv)","abb9ef5a":"#text.dropna()\n#Implement Word Tokenization\nfrom nltk.tokenize import word_tokenize\ntokenized_word_cv = word_tokenize(one_sentence_cv)\n\n#Stopwords\nfrom nltk.corpus import stopwords\nstop_words_cv =  stopwords.words('english')\nnewStopWords_cv = [',','.','(',')','?','-']\nstop_words_cv.extend(newStopWords_cv)\n\n#Removing StopWords\nfiltered_sent_cv = []\nfor w in tokenized_word_cv:\n    if w not in stop_words_cv:\n        filtered_sent_cv.append(w)\n        \n#Frequency Distribution\nfrom nltk.probability import FreqDist\nfdist_cv = FreqDist(filtered_sent_cv)\nprint(fdist_cv)\n\nfdist_cv.most_common(65)\n    ","b62fafee":"#Creating a frequency distribution dataframe of the most used CV Methods\nfreq_words_cv = pd.DataFrame(filtered_sent_cv)\nfreq_words_cv.columns =['words']\nfreq_words_cv.head()\ntype(freq_words_cv)","1d840888":"#install wordcloud package using pip\n! pip install wordcloud","2f8e9f25":"#Importing Matplotlib for plotting purpose\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline","6c366869":"#import sub features\nfrom subprocess import check_output\nfrom wordcloud import WordCloud, STOPWORDS","4dcac782":"#WordCloud figure parameters\nmpl.rcParams['figure.figsize']=(10.0,14.0)    #(6.0,4.0)\nmpl.rcParams['font.size']=24              #10 \nmpl.rcParams['savefig.dpi']=250           #72 \nmpl.rcParams['figure.subplot.bottom']=.1","a52fd1e0":"#Generating the worldcloud with the website name\nwordcloud_cv = WordCloud(\n                          background_color='white',\n                          max_words=100,\n                          max_font_size=40, \n                          random_state=42\n                         ).generate(str(freq_words_cv['words']))","691dc86d":"#Printing the wordcloud and storing in a png file\nprint(wordcloud_cv)\nfig = plt.figure(1)\nplt.imshow(wordcloud_cv)\nplt.axis('off')\nplt.show()","aa4ee1c0":"#Frequency Distribution plot\nimport matplotlib.pyplot as plt\nplt.tick_params(axis='x', which='major', labelsize=10)\nplt.title('Frequently used CV Methods')\nfdist_cv.plot(60,cumulative = False)\nplt.show()","8f64e711":"#Converting the string characters to lower case to bring in uniformity for the field Q27_OTHER_TEXT\ntext_lower = pd.DataFrame(otr['Q27_OTHER_TEXT'].str.lower())\ntext = text_lower.Q27_OTHER_TEXT.unique()","64660a27":"#Creating a list with all the response\nsentence_nlp = ['']\ni = 0\nfor i in range(len(text)):\n    sentence_nlp.append(text[i])\nsentence_nlp\n ","2a3305c6":"#Deleting the sentences that are noise and doesn't give any relevant methods\ndel sentence_nlp[0:3]\ndel sentence_nlp[3]\nsentence_nlp.remove('am learning this ')\nsentence_nlp","3cbdf3b8":"#Replacing few spaces with hyphens to preserve the full form of the methods\nsentence_nlp[23] = 'topic-modeling'\nsentence_nlp[25] = 'stopwords, lemmatization, tfidf, bow'\nsentence_nlp[27] = 'text-mining by r and python libraries only'\nsentence_nlp","df97efd1":"#Converting the unque responses into one sentence for further tokenization purpose\none_sentence_nlp = \"\" \nfor index, value in enumerate(sentence_nlp):\n    one_sentence_nlp += (str(value)+\",\")\nprint(one_sentence_nlp)","5959ee7a":"#text.dropna()\n#Implement Word Tokenization\nfrom nltk.tokenize import word_tokenize\ntokenized_word_nlp = word_tokenize(one_sentence_nlp)\n\n#Stopwords\nfrom nltk.corpus import stopwords\nstop_words_nlp =  stopwords.words('english')\nnewStopWords_nlp = [',','.','(',')','?','-']\nstop_words_nlp.extend(newStopWords_nlp)\n\n#Removing StopWords\nfiltered_sent_nlp = []\nfor w in tokenized_word_nlp:\n    if w not in stop_words_nlp:\n        filtered_sent_nlp.append(w)\n        \n#Frequency Distribution\nfrom nltk.probability import FreqDist\nfdist_nlp = FreqDist(filtered_sent_nlp)\nprint(fdist_nlp)\n\nfdist_nlp.most_common(65)\n    ","e9e22895":"#Creating a Dataframe with the frequency of each word detected\nfreq_words_nlp = pd.DataFrame(filtered_sent_nlp)\nfreq_words_nlp.columns =['words']\nfreq_words_nlp.head()\ntype(freq_words_nlp)","2d781055":"#Generating the worldcloud with the website name\nwordcloud_nlp = WordCloud(\n                          background_color='white',\n                          max_words=100,\n                          max_font_size=40, \n                          random_state=42\n                         ).generate(str(freq_words_nlp['words']))","94547e7e":"#Printing the wordcloud and storing in a png file\nprint(wordcloud_nlp)\nfig = plt.figure(1)\nplt.imshow(wordcloud_nlp)\nplt.axis('off')\nplt.show()\n","57768979":"#Frequency Distribution plot\nimport matplotlib.pyplot as plt\nplt.tick_params(axis='x', which='major', labelsize=14)\nplt.title('Frequently used NLP Methods')\nfdist_nlp.plot(25,cumulative = False)\nplt.show()\n","fcd19a91":"#Identify number of NON NULL responses for CV and NLP Question\notr.count(axis = 0)","4757c6bd":"# initialize list of lists \ndata_count = [['Kaggle',mcr['Q17_Part_1'].count()]\n               ,['Colab',mcr['Q17_Part_2'].count()]\n               ,['GCloud',mcr['Q17_Part_3'].count()]\n               ,['MAzure',mcr['Q17_Part_4'].count()]\n               ,['Paperspace',mcr['Q17_Part_5'].count()]\n               ,['FloydHub',mcr['Q17_Part_6'].count()]\n               ,['Bynder',mcr['Q17_Part_7'].count()]\n               ,['IBMWatson',mcr['Q17_Part_8'].count()]\n               ,['CodeOcean',mcr['Q17_Part_9'].count()]\n               ,['AWS',mcr['Q17_Part_10'].count()]\n               ,['None',mcr['Q17_Part_11'].count()]\n               ,['Other',mcr['Q17_Part_12'].count()]]\nnotebook_type_count = pd.DataFrame(data_count, columns = ['Notebook', 'Users_Count'])        \nnotebook_type_count.head()","dc2c29ee":"# x-coordinates of left sides of bars  \nleft = notebook_type_count['Notebook']\n  \n# heights of bars \nheight = notebook_type_count['Users_Count']\n  \n# labels for bars \nplt.tick_params(axis='x', which='major', labelsize=8)\n  \n# plotting a bar chart \nplt.bar(left, height, \n        width = 0.8) \n  \n# naming the x-axis \nplt.xlabel('Notebook') \n# naming the y-axis \nplt.ylabel('Height') \n# plot title \nplt.title('Popularity of Hosted Notebooks') \n  \n# function to show the plot \nplt.show() \n","7ff6b5c9":"import seaborn as sns\npt1 = mcr[['Q5']]\npt1 = pt1.rename(columns={\"Q5\": \"Title\"})\npt1.drop(0, axis=0, inplace=True)\n\n# plotting to create pie chart \nplt.figure(figsize=(38,36))\nplt.subplot(221)\npt1[\"Title\"].value_counts().plot.pie(autopct = \"%1.0f%%\",colors = sns.color_palette(\"prism\",5),startangle = 60,wedgeprops={\"linewidth\":2,\"edgecolor\":\"k\"},shadow =True)\nplt.title(\"Title Distribution\")\n","fab1d5d6":"**Observation:** The common methods we can identify from this WordCloud are 'lemmatization','tfidf','scdv','ulmfit','svm','clustering','lstm' etc. To understand them better let us try to understand them in a frequency plot. ","70dfc7c1":"From the description of data we understand the fields `Q26_OTHER_TEXT` and `Q27_OTHER_TEXT` of `other_text_responses.csv` gives the responses for the methods used for CV and NLP. So our primary analysis will be done on these fields. ","2ccd65a0":"**Conclusion:** With a number of applications increasing in CV and NLP Kaggle is a promising platform for developement and research in the field of CV and NLP. On reaching out to more Kaggle community members in future surveys it gives the budding NLP and CV scientists to find newer methods and popular methods to build promising application","f80c87a6":"**Observation:** The most popular methods used for CV are 'CNN','triplet-loss' with four and three members responding respectively followed by 'i3d','openCV','classification'. \nHowever we see that the frequency of users using these methods confine to single digit.\nSo, lets also understand the usage of NLP Methods. ","b89892f6":"**Observation:** Out of 19717 surveytakers only 38 users responded for the methods they use for NLP and 34 responded for the methods they use for CV. \nBased only on this data we can say the NLP and CV methods are very unpopular and there aren't much users working on these fields. \nHowever, with the huge number of competitions being held on Kaggle itself based on the fields of NLP and CV we are quite assured that there are immensely large number of users across Kaggle itself who work in these fields, but the probability is that we have missed out that population in this survey. \n\nOur goal is to target these missed out population in the future surveys. To understand this we can analyze the popular hosted notebooks among the survey takers and analyzing the profession of the survey takers can tell us whom whom we missed. \n\nOur Hypotheisis is that: With the profession we can understand whether they are NLP or CV engineers, students or professors who work in the field of NLP and CV. In the case of university students and professors the chances for them to respond for the methods based on NLP or CV are extremely high.\nWith the hosted notebook medium we can understand which are the other popular forums where the survey links needs to be shares through emails, notifications or ads to reach to more users in this field. Combining these twoinformation for the future survey we can identify which professionals or students to target in future to get accurate information on their application on CV and NLP.","bf59876d":"**Observation:** Followed by Kaggle is Google Colab and some of them who are not using any hosted Notebook.\n\n**Possible community to be targetted:** As we see a lot users using Google Colab, various google forums can be platforms for sharing the survey links in future so that we can get more responses from people working in CV and NLP. ","a939e813":"**Observation:** The most popular method is 'tfidf' and 'word2vec' which is given as a response by two users. Rest methods of NLP such as spacy, sparse, scdv, flair, lstm,ocr,spss,ulmfit are all responded only by one user per method.\n\nThis makes us our reliability on the data very frail. This calls for fresh methods for survey to reach out to the community who are actively involved in handling NLP for their work or research. ","25d20449":"**Understanding the popularity of Computer Vision and Natural Language Processing application among the Kaggle Survey takers**\n\nComputer Vision and Natural Language Processing are two application fields of Machine Learning and Data Science which is finding increasing applications in building reallife application products. Ranging from building chatbots for customer support to document analysis NLP has found place in many user friendly products. On the other hand Self-Driving cars, surgical robots are all applications of Computer Vision field along with Robotics. \n\n**How is NLP and CV relevant for Kaggle?**\nMost of the popular courses offered on CV and NLP through platforms like Udacity and Coursera encourages the students to work on realtime application data available in Kaggle for developing skills in this field. Being the biggest hub for open source data, Kaggle plays an important role for fetching openly available data as well as using the notebook and hardware ( both GPU and TPU) service provided. This obiviously makes us believe that the recent Kaggle survey can tell us more about the popularity of CV and NLP among the users and their advanced methods. \n\n**Goal of this analysis:** Understand the frequency of users availing the CV and NLP resources available across the internet, the popular methods they are employing for building meaningful products. With this analysis we also look forward to find channels, groups etc to target for more CV and NLP users in the future surveys of Kaggle.\n\n****Approach:**** The survey had questionaire about the popular NLP methods and CV methods where the responses were typed in as text replies by the users. From the typed sentence we are trying to tokenize the words using NLTK Python library and extract the methods terms. With these method terms we try to build a WordCloud to spot the methods in the response and a frequency chart to understand their popularity. ","79910e98":"**Observation:** Here we find 21% of student users contributing to this survey. However this community can probably consist of undergraduates pursuing any degree or not specifically doing research on NLP or CV. Similarly we do not see Professors or trainers who are guiding students. \n\n**Possible community to be targetted:** Sending survey links to universities doing research and projects on NLP and CV can help in increasing more people to respond with greater frequency and even newer methods. ","7ab94750":"We know that the survey was taken from about 19717 survey takers. However the unique responses for the methods used for CV and NLP seems to be very less compared to the total number of survey takers. This calls for us to explore more on the reason for this less responses for these questions as the response can give us an idea about:\n* popularity of CV and NLP among Kaggle users\n* understand scope of Kaggle usage among these users","efa1dada":"Among the terms tokenized from the user responses we can see some of the terms like `i3d`,`lstm`,`cnn` etc are commonly known CV methods. To understand all the methods and frequency we can plot a frequency plot using these terms. "}}