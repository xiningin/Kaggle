{"cell_type":{"4439426d":"code","570c04fd":"code","8269f8eb":"code","7a19d1ee":"code","35a9fdc4":"code","b9103a16":"code","c01a8449":"code","d23f8dd7":"code","b521ac45":"code","acb962dd":"code","a396d03b":"code","b808e024":"code","e39bce8c":"code","34ebac41":"code","494ff050":"code","cc019edf":"markdown","5aded4d6":"markdown","370ea34a":"markdown","91d78bc8":"markdown","f13b221d":"markdown","7e8e2467":"markdown","6247cbaf":"markdown","a7741507":"markdown","592999d9":"markdown","43241449":"markdown","8c9fc8b0":"markdown","3a4d39c7":"markdown","80a30a68":"markdown","35ad3466":"markdown","7aa53607":"markdown","a7ca3104":"markdown"},"source":{"4439426d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n\n\n# The following packages were used:\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport string\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, make_scorer, confusion_matrix, classification_report\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\n\nimport re\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport category_encoders as ce\n\n!pip install pyspellchecker\nfrom spellchecker import SpellChecker\nfrom sklearn import decomposition\n\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","570c04fd":"train_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ntrain_data.head(50)","8269f8eb":"# Shuffeling the training data\ntrain_data = train_data.sample(frac=1, random_state=1)\ntrain_data.head(25)","7a19d1ee":"# some basic data explorations showing the amount of samples in the data sets,\n# unique keywords and locations and number of missing values in keywords and locations \nprint(\"train data (cols, rows):\\n\", train_data.shape)\nprint(\"test data (cols, rows):\\n\", test_data.shape)\nprint(\"Unique keywords in train data:\\n\", len(pd.unique(train_data['keyword'])))  \nprint(\"Unique locations in train data:\\n\", len(pd.unique(train_data['location']))) \nprint(\"Missing keywords in train data:\\n\", train_data['keyword'].isnull().sum())\nprint(\"Missing locations in train data:\\n\", train_data['location'].isnull().sum())","35a9fdc4":"# plotting the distribution between the number of disaster and non disaster tweets.\nlabels = train_data.target.value_counts()\n\n# Creating a nice graph for visualiziation\nsns.barplot(labels.index, labels)\nplt.gca().set_title('Number of Tweets per category')\nplt.gca().set_ylabel('samples')\nplt.gca().set_xticklabels(['0: No disaster', '1: Disaster'])","b9103a16":"# The URL's like youtube links do not provide valuable info so are removed\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\n# HTML code can also be removed.\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\n# emoji are not written in plain text and are thus removed\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n# combines the above 3 functions\ndef clean_text(text):\n    text = remove_URL(text)\n    text = remove_html(text)\n    text = remove_emoji(text)\n    return text\n\n# Tweets are written fast with lots of spelling errors.\n# the function belows does some basic spell checking.\n# Hopefully we can now categorize misspelled and correctly spelled words together. \n# Spellchecking slightly improves model performance\nspell = SpellChecker(distance=1) # distance=2 is standard but very slow\ndef correct_spelling(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n\n\n# The following function applies the clean text and correct spelling functions\n# furthermore featuers are created like the number of punctuation and word count.\ndef feature_engineer(dataframe):\n    # apply the clean text function\n    dataframe['text'] = dataframe.text.apply(lambda x: clean_text(x))\n    num_char = dataframe.text.apply(lambda x: len(x))\n    num_space = dataframe.text.apply(lambda x: x.count(' '))\n    \n    # 4 extra features are created that may be valluable for the model\n    dataframe['num_punc'] = dataframe.text.apply(lambda x: len([c for c in x if c in string.punctuation]))\n    dataframe['num_upper'] = dataframe.text.apply(lambda x: len([letter for letter in x if letter.isupper()]))\n    dataframe['mean_word_length'] = dataframe['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n    dataframe['word_count'] = dataframe['text'].apply(lambda x: len(str(x).split()))\n    \n    # Punctuation is removed and the spelling corrector is used\n    dataframe['text'] = dataframe.text.apply(lambda x: \"\".join([c for c in x if c not in string.punctuation]))\n    dataframe['text'] = dataframe.text.apply(lambda x: correct_spelling(x))\n    dataframe = dataframe.drop(columns=['id'])\n    return dataframe  # return a dataframe with clean text and new features\n    \n","c01a8449":"# applying the Feature engineer function above to both the train and test data set.\ntrain = feature_engineer(train_data)\ntest = feature_engineer(test_data)\n\n# Here we split the train data in an X and Y (target) variable.\ny = train['target']\nx = train.drop(columns=['target'])\n\n# showing the new cleaned train dataframe.\n# Upper case letters are tranformed to lower case in a later stage\ntrain.head(10)","d23f8dd7":"f, axes = plt.subplots(2,2, figsize=(10,10))\nsns.distplot(train.num_upper[train.target == 0], label='Not Disaster', color='green', ax=axes[0][0])\nsns.distplot(train.num_upper[train.target == 1], label='Disaster', color='red', ax=axes[0][0])\naxes[0][0].legend()\naxes[0][0].set_title('Distribution of upper case letters')\naxes[0][0].set_xlabel('Number of upper case letters')\n\nsns.distplot(train.num_punc[train.target == 0], label='Not Disaster', color='green', ax=axes[0][1])\nsns.distplot(train.num_punc[train.target == 1], label='Disaster', color='red', ax=axes[0][1])\naxes[0][1].legend()\naxes[0][1].set_title('Distribution of punctuation marks')\naxes[0][1].set_xlabel('Number of punctuation marks')\n\nsns.distplot(train.word_count[train.target == 0], label='Not Disaster', color='green', ax=axes[1][0])\nsns.distplot(train.word_count[train.target == 1], label='Disaster', color='red', ax=axes[1][0])\naxes[1][0].legend()\naxes[1][0].set_title('Distribution of words per tweet')\naxes[1][0].set_xlabel('Number of words per tweet')\n\nsns.distplot(train.mean_word_length[train.target == 0], label='Not Disaster', color='green', ax=axes[1][1])\nsns.distplot(train.mean_word_length[train.target == 1], label='Disaster', color='red', ax=axes[1][1])\naxes[1][1].legend()\naxes[1][1].set_title('Distribution of word lenght per tweet')\naxes[1][1].set_xlabel('Average word lenght per tweet')\n\nplt.tight_layout()\nplt.show()","b521ac45":"# stop wrods are used a lot but contain little information\nstop_words = set(stopwords.words('english'))\ndef remove_stopwords(tokens):\n    nostop = [word for word in tokens if word not in stop_words]\n    return nostop\n\n# stemming and lemmatizer function\nwnl = WordNetLemmatizer()\ndef lemmatizer(tokens):\n    lem_tokens = [wnl.lemmatize(word) for word in tokens]\n    return lem_tokens\n\nps = PorterStemmer()\ndef stemmer(tokens):\n    stem_tokens = [ps.stem(word) for word in tokens]\n    return stem_tokens\n\n\n# This analyzer function is used to process the text in the model piplein\ndef analyzer(text):\n    tokens = word_tokenize(text.lower())  # Creates a list of all word in a tweet in lower case\n    tokens = remove_stopwords(tokens)     # removes the stop words\n    \n    ## The lematizer OR the stemmer is used (not both): Stemming provides better predictions\n    #     tokens = lemmatizer(tokens)\n    tokens = stemmer(tokens)\n    text =  ' '.join([w for w in tokens])\n    return text\n","acb962dd":"## TRANSFORMERS\n\n# Preprocessing for keyword (and Location) data\n# Location data does not improve the modle\nkeyword_transformer = ce.TargetEncoder()\n\n# PREPROCESSING FOR TEXT DATA\n# A vectorizer is used to change the words in numbers (like one hot encoding)\n# TFIDF is more sophisticated. However the more simple count vectorizer provides better results\n# min_df=2 removes all words that are only used once in all tweets (slight improvement in model performance) \n# Ngrams did not improve model performance\ntext_transformer = CountVectorizer(preprocessor=analyzer, min_df = 2)\n# text_transformer = TfidfVectorizer(analyzer=analyzer, min_df = 2, ngram_range = (1,2))\n\n\n# Numeric collumns are scaled with standardscalar\n# Number of upper case letters and punctuation marks did not improve the model and are thus not used\nnumeric_cols = ['word_count', 'mean_word_length']\nnumeric_transformer = StandardScaler()\n\n\n# Bundle preprocessing for numerical, text and keyword data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('key', keyword_transformer, 'keyword'),   # Location does not provide an improvement\n        ('tex', text_transformer, 'text'),\n        ('num', numeric_transformer, numeric_cols)],\n        remainder = 'drop')\n\n\n## MODEL SELECTION\n# 6 different types of models were tried (manually). (see the from .... import .... list below)\n# A basic LogisticRegression classification model provided the best results and is fast to run\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\n# I manually changed the model type here\n# The model below was optimized a bit, in a later stage of this notebook\nmodel = LogisticRegression(C = 0.3, penalty= 'l2', random_state = 1, solver='lbfgs')\n\n## FEATURE SELECTION\n# The count vectorizer creates a new feature for each unique word.\n# In total this is more than 5000 features.\n# I tried to select the K best features with a variety of metrics but this did not improve the model performance\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif, chi2, mutual_info_classif\n# selector = SelectKBest(f_classif, k=300)\n\n\n# The LogisticRegression (LR) pipeline\nLR_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n#                             ('selector', selector),  # The feature selection (selectKbest) is not used\n                              ('model', model)\n                             ])\n\n# 5 fold Cross validation to evaluate model performance\nscores = cross_val_score(LR_pipeline, x, y,\n                         cv=5,\n                         scoring='f1',  # evaluation metric of the competition\n                         n_jobs=-1)\n# Printing the f1 score results\nprint(scores)\nprint(np.mean(scores))\n","a396d03b":"# calculating cross validation predictions\nLR_pred = cross_val_predict(LR_pipeline, x, y,\n                            cv=5,\n                            n_jobs=-1)\n\n# calculating the confusion matrix\nconf = confusion_matrix(y, LR_pred, labels=[1, 0])\n\n# Making a better readable and flashy looking confusion matrix\nax= plt.subplot()\nsns.heatmap(conf, annot=True, ax = ax)\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels')\nax.set_title('Confusion Matrix')\nax.xaxis.set_ticklabels(['Disaster', 'No disaster']); ax.yaxis.set_ticklabels(['Disaster', 'No disaster'])\n\n# showing the classification report\nprint(classification_report(y, LR_pred))","b808e024":"# Run the data preprocessing of the Pipeline\nPCA_fit = LR_pipeline.fit(x,y)\nprocessed_data = PCA_fit.named_steps['preprocessor'].transform(x)\n\n# Calculate the Principal components and select the 2 most important ones\npca2 = decomposition.PCA(n_components=2)\nX_pca2 = pca2.fit_transform(processed_data.toarray())\nX_pca2 = pd.DataFrame(X_pca2, columns=['pca1', 'pca2']) #create small df for seaborn\nX_pca2['Disaster?'] = y #add the target classes for plotting\n\nsns.scatterplot(x=X_pca2['pca1'], y=X_pca2['pca2'], hue=X_pca2['Disaster?'])\nplt.title('First two principle components')\n\nprint(\"Percentage of variance explained by first 2 PCA components:\\n\", pca2.explained_variance_ratio_)\n","e39bce8c":"# SimpleImputer hasn no get_feature_names function. These features are removed from this pipeline\n# I am more interested in the most importantd words in the text anyway.\nimportance_preprocessor = ColumnTransformer(transformers=[\n                                            ('key', keyword_transformer, 'keyword'),\n                                            ('tex', text_transformer, 'text')],\n#                                           ('num', numeric_transformer, numeric_cols)],\n                                            remainder = 'drop')\n\n# Create a pipeline with the new Columntranformer\nimportance_pipeline = Pipeline(steps=[('preprocessor', importance_preprocessor),\n                                      ('model', model)\n                                     ])\n\n# Fit the new pipeline\nimportance_pipeline.fit(x, y)    \n\n# Calculate a logistic regression coefficient and the corresponding feature names\nfeature_importance = importance_pipeline.named_steps['model'].coef_[0]\nfeature_names = importance_pipeline.named_steps['preprocessor'].get_feature_names()\n\n# Place the feature names and coefficients in a dataframe to sort the values in ascending order\n# Show the top 25 coefficient in a table\ndf = pd.DataFrame({'feature_names': feature_names, 'feature_coefficient': feature_importance})\ndf = df.sort_values(by='feature_coefficient', ascending=False)\ndf.head(25)","34ebac41":"# Create hyperparameter options for regualrization strength C\n# hyperparameters = {'model__C': [0.1, 0.25, 0.5, 1.0, 2.0, 5.0]} # Ran this first, result was around 0.5\nhyperparameters = {'model__C': [0.2, 0.3, 0.4, 0.5, 0.6]} # More finetuning around 0.5\n\n# Create grid search using 5-fold cross validation\nlogistic_gs = GridSearchCV(LR_pipeline, hyperparameters, cv=5, scoring = make_scorer(f1_score), verbose=0, n_jobs=-1)\n\n# Fitting the results\nLR_best_model = logistic_gs.fit(x, y)\n\n# Printing the accuracy for each fold\n# Printing the average model accuracy over all 3 folds\nprint('Best score and parameter combination = ')\nprint(LR_best_model.best_score_)    \nprint(LR_best_model.best_params_) \n\n\n###################################################\n# Best score and parameter combination = \n# 0.7604063775130128\n# {'model__C': 0.6, 'model__penalty': 'l2'}","494ff050":"# LR_pipeline_fit = LR_pipeline.fit(x,y)\ntest_pred = LR_best_model.predict(test)\n\n\nsample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nsample_submission[\"target\"] = test_pred\nsample_submission.to_csv(\"submission.csv\", index=False)\nsample_submission.head(25)","cc019edf":"> **TEXT CLEANING AND CREATING EXTRA FEATURES**\n\nWith the functions below the text is first cleaned and afterwards the tweet text is cleaned. The text is cleaned to all lowercase lettered words, without any punctuation.","5aded4d6":"> **MODEL OPTIMIZATION**\n\nThe Logistic regression model is optimized with gridsearchCV to gain slightly better predictive results. The best parameters are used to make final predictions on the test set.","370ea34a":"> **PLOTTING THE NEW FEATURES**\n\nDistribution plots are made for the 4 newly created features. When the distribution plots are different for the disaster and no disaster class they can be used for making predictions.","91d78bc8":"The F1 scores are fairly consistant between 0.75 and 0.77. I think these results are quite decent for a simple logistic regression model (I have seen some neural nets reach scores of 0.8x). \n\nA more detailed classification report and confusion matrix show more in-depth information of the results. From the confusion matrix and recall scores is vissible that the model is slightly better at predicting non-disaster tweets.","f13b221d":"> **REDUCE NUMBER OF UNIQUE WORDS IN TWEETS**\n\nThe following functions remove stop words and stems or lemmatizes each word in all tweets. Stop words are used often but contain little information. Stemming or lemmatization reduce variations of the same word to there root form. This reduces the vocabulary from all tweets combined and improves moddel performance. ","7e8e2467":"The code block below shuffles the training dataset. The text data needs a lot of cleaning: lots of punctuation, spelling errors, website links and other weird looking letters. The Keywords look fairly organized in classes. I think the keyword can contain a lot of valuable information for the predictive model. The location field contains more missing values and random locations like \"land OF The Kings\" (id=9982). I do not think the location category contains a lot of predictive power.\n\nThere are numerous tweets with desceptive keywords and texts. id9982 contains 'tsunamie' as keyword but is not listed as a disaster for example. A similar vocabulary is used in the disaster and non disaster tweets. Words like 'windstorm', 'burned', 'hostage' and 'fatality' can be used in disaster tweets but also show up in the non disaster tweets in the data shown below. This can make it a lot harder to make accurate predictions. ","6247cbaf":"> **INTRODUCTION**\n\nThis notbook is part of the \"Real or Not? NLP with Disaster Tweets\" competition on Kaggle ($ 10.000 prize money, submission deadline 23 march 2020).  In this notebook tweets are catagorized in 2 classes: Tweets about disasters and tweets that are not about disasters. The data contains a Keyword, Location and the text around 10000 tweets. Natural Language Processing (NLP) techniques are used to make predictions with the text data. There are about 7000 Tweets in the train set and about 3000 tweets in the test set. Competition results are based on the F1 score as evaluation metric.\n\nI started this project as it looked fun, interesting and challenging with the skills I had at the moment. The goal of this project was not to gain the highest position on the kaggle competition ladder, but to improve my data science skills. ","a7741507":"> **PREDICTING THE TEST SET**\n\nPredictions are made for the test set. The submission.csv file is created to see how well this model performs on the competition leaderbord.","592999d9":"> **PACKAGES USED**\n\nBelow all packages used in this project are imported:","43241449":"> **DATA SEPERATION WITH PCA**\n\nI used principle component analysis to see how well the disaster and non disaster tweets are seperated. The PCA analysis scatterplot does not show any seperation in the data. The first PCA component only explains 13% of the variance and the second component only 5%.","8c9fc8b0":"The results here are different from what I expected. I expected more upper case letters and more punctuation marks in the Disaster tweets but the opposite(although it is very close). I don't expect the number of upper case letters and number of punctuation marks to improve the model.\n\nThe distribution plot for the word count and mean word lenght look quite different between the disaster and non disaster tweets. I expect an small improvement in model performance when using these two features.","3a4d39c7":"> **CLASS DISTRIBUTION**\n\nLets check the class distribution between 0 (non disaster tweets) and 1 (disaster tweets). An somewhat even distribution in the target category would be preferred. There are a bit more non disaster tweets but overall the distribution is fairly even.","80a30a68":"> **DATA EXPLORATION**\n\nIn the following code blocks some basic data exploration is done","35ad3466":"> **lOADING THE DATA FILES**\n\nThe code block below loads the data train and test data file. As can be seen is the train data set ordered by keyword.  ","7aa53607":"> **CLASSIFICATION PIPELINE**\n\nThe codeblock below shows the pipeline used to classify the tweets. The comments in the codeblock explain the steps step by step. 5 Fold cross validation is used to evaluate the model performance and to reduce the risk of overfitting compared to the train_test_split method. ","a7ca3104":"> **IMPORTANT WORDS**\n\nIt is usefull to know what words are used by the model to predict if a tweet falls in the disaster category or the non-disaster category. The coefficients of the logistic regression model are usefull for this purpose. A higher coefficient corresponds to an outcome 1 (Disaster). The keyword is an important featuer. From the vocabulary: words like Hiroshima, earthquake, storm, tornado and wildfire correspond to a disaster tweet according to the logistic regression coefficients. These are words I would categorize as a disaster by myself as well."}}