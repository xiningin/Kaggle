{"cell_type":{"cc50577f":"code","ef2cf89c":"code","cf727540":"code","e31caf5c":"code","c65b3eb3":"code","c8a9bf2c":"code","3dda8850":"code","f1fae086":"code","9c488ad5":"code","857aac93":"code","739e7933":"code","231ba5f3":"code","e554cada":"code","166453fc":"code","dc7a1b97":"code","506a6b42":"code","0ce40617":"code","16f7fb49":"code","7261ac67":"code","39c9ae1a":"markdown","b119dacd":"markdown","654e231c":"markdown","c30e688f":"markdown","d7cc4d59":"markdown","a8a48d95":"markdown","818eeb3e":"markdown","f9610a92":"markdown","0d7f7699":"markdown","f954bb0f":"markdown","ab95e1ec":"markdown","549c3fe5":"markdown","8c9317fb":"markdown","9da05674":"markdown"},"source":{"cc50577f":"# Suppress multiple warnings\nimport warnings\nwarnings.filterwarnings(action='once')","ef2cf89c":"# Import libraries and modules\nimport numpy as np\nimport pandas as pd\nimport os\nimport datetime","cf727540":"# Files in directory\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e31caf5c":"# Import train\nTrain = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\nprint(Train.shape)\n\n# Import test\nX_test= pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")\nprint(X_test.shape)\n\n# Quick look at the data\nTrain.head()","c65b3eb3":"# Split train into X & Y\ny_train = Train.label\nX_train = Train.drop('label',axis=1)","c8a9bf2c":"# Convert all into numpy arrays\ny_train = y_train.values\nX_train = X_train.values\nX_test = X_test.values","3dda8850":"# Split train data into t_train and t_test sets\nfrom sklearn.model_selection import train_test_split\nXt_train, Xt_test, yt_train, yt_test = train_test_split(X_train, y_train, test_size=0.3, random_state=42)","f1fae086":"# Return shape of 1st item in list\nXt_train[0].shape","9c488ad5":"# Visualise data\n\nimport matplotlib.pyplot as plt \ndata_no = 6\nplt.imshow(Xt_train[data_no].reshape(28, 28))\nplt.title('Label: ' + str(yt_train[data_no]))","857aac93":"# Preprocess input data\n\n# Good practice (and better performance) to ensure the values are less than 1. To do this we'll calculate the max value and scale all numbers by that.\nscaling_value = max(Xt_train.max().max(),Xt_test.max().max(),X_test.max().max())\nXt_train = Xt_train\/ scaling_value\nXt_test = Xt_test\/ scaling_value\nX_test = X_test\/ scaling_value\n\n# Convert inputs into right form for model shape (n, width, height) to (n, depth, width, height), where n is number of records.\nXt_train = Xt_train.reshape(Xt_train.shape[0], 28, 28, 1)\nXt_test = Xt_test.reshape(Xt_test.shape[0], 28, 28, 1)\nX_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n# Ensure inputs are correct type\nXt_train = Xt_train.astype('float32')\nXt_test = Xt_test.astype('float32')\nX_test = X_test.astype('float32')\n\n# Preprocess class labels (we require these to be one-hot encoded i.e. for 3 [0,0,1,0,0,0,0,0,0])\nfrom keras.utils import np_utils\nyt_train = np_utils.to_categorical(yt_train, 10)\nyt_test = np_utils.to_categorical(yt_test, 10)\n\n# Sense check some parameters\nprint('Scaling value: ' + str(scaling_value))\nprint(Xt_train.shape)\nprint(yt_train.shape)","739e7933":"from keras.models import Sequential\n\n# Define model architecture\nmodel = Sequential() ","231ba5f3":"from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n\n# Add layers\n\n# The objective of the Convolution Operation is to extract the high-level features such as edges, from the input image.\n# A 3x3 kernal (or filter) is passed across the image\nmodel.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28,28,1)))\nprint(model.output_shape)\nmodel.add(Conv2D(32, kernel_size=3, activation='relu'))\nprint(model.output_shape)\n\n# MaxPooling2D is a way to reduce the number of parameters in our model by sliding a 2x2 pooling filter\n# across the previous layer and taking the max of the 4 values in the 2x2 filter\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nprint(model.output_shape)\n\n# Dropout is a method for regularizing our model in order to prevent overfitting.\nmodel.add(Dropout(0.25))\nprint(model.output_shape)\n\n# Deep network requires to be converted into a 1-D array\nmodel.add(Flatten())\nprint(model.output_shape)\n\n# Add a couple of fully connected dense layers\nmodel.add(Dense(128, activation='relu'))\nprint(model.output_shape)\n# Last layer is out output so a 'softmax' outputs a probability distribution between 0 and 1\nmodel.add(Dense(10, activation='softmax'))\nprint(model.output_shape)","e554cada":"# Compile model\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","166453fc":"# Fit model on training data\nstart = datetime.datetime.now()\nmodel.fit(Xt_train, yt_train, batch_size=32, nb_epoch=10, verbose=1)\nprint('Time taken: ' + str((datetime.datetime.now()-start).total_seconds()) + ' seconds')","dc7a1b97":"# Evaluate model on test data\nscore = model.evaluate(Xt_test, yt_test, verbose=0)\nprint(model.metrics_names[0] + \": \" + str(score[0]))\nprint(model.metrics_names[1] + \": \" + str(score[1]))","506a6b42":"# Fit model on ALL training data\nX_train = np.concatenate([Xt_train,Xt_test],axis=0)\ny_train = np.concatenate([yt_train,yt_test],axis=0)\nstart = datetime.datetime.now()\nmodel.fit(X_train, y_train, batch_size=32, nb_epoch=10, verbose=1)\nprint('Time taken: ' + str((datetime.datetime.now()-start).total_seconds()) + ' seconds')","0ce40617":"# Predict results on test set\nresults = model.predict(X_test)\n\n# Select the index with the maximum probability\nresults = np.argmax(results,axis = 1)\n\n# Put into dataframe and add Id column\nresults = pd.Series(results,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\n# Check submission looks correct\nsubmission.head(5)","16f7fb49":"# Check one result\ntest_no = 0\nplt.imshow(X_test[test_no].reshape(28, 28))\nplt.title('Predicted Label: ' + str(submission.Label[test_no]))","7261ac67":"# Output to csv for submission\nsubmission.to_csv(\"submission.csv\",index=False)","39c9ae1a":"# Compile model","b119dacd":"We'll split the training data into t_train (80%) and t_test (20%) sets. The t_test is sometimes referred to as the 'validation' set.","654e231c":"# Fit model on 70% of training data","c30e688f":"# Model architectural setup ","d7cc4d59":"# Preprocessing","a8a48d95":"Tensorflow requires inputs to not be in Pandas dataframe format. Numpy arrays are okay.","818eeb3e":"Using some techniques from the following sources:\n* https:\/\/elitedatascience.com\/keras-tutorial-deep-learning-in-python\n* https:\/\/towardsdatascience.com\/building-a-convolutional-neural-network-cnn-in-keras-329fbbadc5f5\n* https:\/\/www.kaggle.com\/poonaml\/deep-neural-network-keras-way","f9610a92":"# Evaluate model on unseen (30%) training data","0d7f7699":"# Make predictions on test data","f954bb0f":"We know that the images are (28x28) images. If we wanted to visualise an image we would have to reshape it from (784x1) to (29x28). Note they are greyscale.","ab95e1ec":"The extra column in Train is for the label. Otherwise both the same.","549c3fe5":"Each image is currently stored as a (784x1) array. See below.","8c9317fb":"# Import data & split into test and train","9da05674":"# Fit model on ALL training data"}}