{"cell_type":{"c4d3fc08":"code","3fcbff2c":"code","f2001cb5":"code","8be23002":"code","216fb0ce":"code","4a856332":"code","9d9c983b":"code","824532ed":"code","e611b008":"code","a76aafd5":"code","7352a739":"code","9c01731c":"code","9518e80c":"code","8618b65b":"code","63e648db":"code","d6e69f1a":"code","a713d36e":"code","ab8b9458":"code","ed46db26":"code","d4ef9c23":"code","7abc93ee":"code","6420a92b":"code","ffdab276":"code","06920ae1":"code","0df12f47":"markdown","98a6c8d8":"markdown","a7b8cc57":"markdown","418789ff":"markdown","aba25191":"markdown","aea2ac31":"markdown","d73e1eee":"markdown","a1dbc1c7":"markdown","881f46d3":"markdown","b5456b80":"markdown","06cca6da":"markdown","03dfc14c":"markdown","03573a6a":"markdown","3ac93fbe":"markdown","ee0da7a8":"markdown","bb5781e5":"markdown","7dc17625":"markdown","16cca493":"markdown","06523b99":"markdown","67a0b142":"markdown"},"source":{"c4d3fc08":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport scipy as sp\nimport tensorflow as tf \nimport tensorflow_hub as hub\nimport tensorflow_datasets as tfds\nimport pathlib\nimport os\nfrom sklearn.metrics import accuracy_score\nimport PIL\nimport requests\nfrom io import BytesIO\nimport urllib\nimport cv2\nfrom skimage import io","3fcbff2c":"training_dataset_path = pathlib.Path(r\"..\/input\/cats-vs-dogs\/data\/train\")\nvalidation_dataset_path = pathlib.Path(r\"..\/input\/cats-vs-dogs\/data\/validation\")","f2001cb5":"img_height = 224\nimg_width = 224\n\n\ntraining_data_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1\/255.0,\n                                                                   rotation_range=40,\n                                                                   zoom_range=0.2,\n                                                                   width_shift_range=0.2,\n                                                                   height_shift_range=0.2,\n                                                                   shear_range=0.2,\n                                                                   horizontal_flip=True,\n                                                                   vertical_flip=True)\n\ntraining_generator = training_data_gen.flow_from_directory(training_dataset_path,\n                                                          target_size=(224, 224),\n                                                          batch_size=32,\n                                                          class_mode='binary',\n                                                          seed=101)\n#------------------------------------------------------------------------------------------------------\nval_data_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1\/255.0)\nvalidation_generator = val_data_gen.flow_from_directory(validation_dataset_path,\n                                                        target_size=(224, 224),\n                                                        batch_size=32,\n                                                        class_mode='binary',\n                                                        seed=101)\n","8be23002":"my_model = tf.keras.models.Sequential()\n\nmy_model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\nmy_model.add(tf.keras.layers.MaxPooling2D(2))\n\n\nmy_model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\nmy_model.add(tf.keras.layers.MaxPooling2D(2))\n\n\nmy_model.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu'))\nmy_model.add(tf.keras.layers.MaxPooling2D(2))\n\n\nmy_model.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu'))\nmy_model.add(tf.keras.layers.MaxPooling2D(2))\n\nmy_model.add(tf.keras.layers.Conv2D(256, (3, 3), activation='relu'))\nmy_model.add(tf.keras.layers.MaxPooling2D(2))\n\n\nmy_model.add(tf.keras.layers.GlobalAveragePooling2D())\n\nmy_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))","216fb0ce":"my_model.summary()","4a856332":"my_model.compile(loss='binary_crossentropy',\n                 optimizer=tf.keras.optimizers.SGD(),\n                 metrics=['acc'])","9d9c983b":"history = my_model.fit(training_generator, steps_per_epoch=500,\n                       validation_data=validation_generator, \n                       validation_steps=281, epochs=15) ","824532ed":"my_model.layers","e611b008":"validation_generator.class_indices","a76aafd5":"# final convolution layer\nprint(my_model.layers[-3].name)\n\n# global average pooling layer\nprint(my_model.layers[-2].name)\n\n# output of the classifier\nprint(my_model.layers[-1].name)","7352a739":"# same as previous model but with an additional output\ncam_model  = tf.keras.Model(inputs=my_model.input,outputs=(my_model.layers[-3].output,my_model.layers[-1].output))\ncam_model.summary()","9c01731c":"# get the features and results of the test images using the newly created model\nfeatures, results = cam_model.predict(validation_generator)\n\n# shape of the features\nprint(\"features shape: \", features.shape)\nprint(\"results shape: \", results.shape)","9518e80c":"# these are the weights going into the softmax layer\nlast_dense_layer = my_model.layers[-1]\n\n# get the weights list.  index 0 contains the weights, index 1 contains the biases\ngap_weights_l = last_dense_layer.get_weights()\n\nprint(\"gap_weights_l index 0 contains weights \", gap_weights_l[0].shape)\nprint(\"gap_weights_l index 1 contains biases \", gap_weights_l[1].shape)\n\n# shows the number of features per class, and the total number of classes\n# Store the weights\ngap_weights = gap_weights_l[0]\n\nprint(f\"There are {gap_weights.shape[0]} feature weights and {gap_weights.shape[1]} classes.\")","8618b65b":"def Make_a_prediction_on_an_image_from_the_web(url):\n    \n    response = requests.get(url)\n    img = PIL.Image.open(BytesIO(response.content))\n    img = img.resize((224, 224))\n    img_array = np.array(img)\n    img_array_exp = np.expand_dims(img_array, axis=0)\n\n    features_web, results_web = cam_model.predict(img_array_exp)\n    \n    # takes the features of the chosen image\n    features_for_img = features_web[0,:,:,:]\n    # get the class with the highest output probability\n    prediction_prob = results_web[0]\n\n    if prediction_prob >= 0.5: \n        prediction = \"Dog\"\n        prob = prediction_prob[0]\n    else:\n        prediction = \"Cat\"\n        prob = 1 - prediction_prob[0]\n\n    # get the gap weights at the predicted class\n    class_activation_weights = gap_weights[:,0]\n\n    # upsample the features to the image's original size (224 x 224)\n    class_activation_features = sp.ndimage.zoom(features_for_img, (224\/5, 224\/5, 1), order=2)\n\n    # compute the intensity of each feature in the CAM\n    cam_output  = np.dot(class_activation_features,class_activation_weights)\n    \n\n    print('Predicted Class = ' +str(prediction)+ ', Probability = ' + str(prob))\n\n    #--------------------------------------------\n    \n    fig=plt.figure(figsize=(12, 7))\n    \n    fig.add_subplot(2,2,1)\n    plt.imshow(cam_output, cmap='Reds')\n    plt.colorbar()\n    \n    fig.add_subplot(2,2,2)\n    plt.imshow(img_array)\n    \n    fig.add_subplot(2,2,3)\n    #plt.figure(figsize = (7,7))\n    plt.imshow(cam_output, cmap='Reds') #, alpha=1.5)\n    plt.imshow(img_array, alpha=0.5)\n    plt.axis('off')\n    ","63e648db":"url_1 = \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/1\/18\/Dog_Breeds.jpg\"\nMake_a_prediction_on_an_image_from_the_web(url_1)","d6e69f1a":"url_2 = \"https:\/\/i.pinimg.com\/originals\/cd\/94\/f0\/cd94f0089700a1c01407d9f409e2d9b6.jpg\"\nMake_a_prediction_on_an_image_from_the_web(url_2)","a713d36e":"url_3 = \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/7\/74\/Golden_Retrievers_dark_and_light.jpg\"\nMake_a_prediction_on_an_image_from_the_web(url_3)","ab8b9458":"url_4 = \"https:\/\/hips.hearstapps.com\/hmg-prod.s3.amazonaws.com\/images\/cats-that-look-like-tigers-bengal-1562690663.jpg\"\nMake_a_prediction_on_an_image_from_the_web(url_4)","ed46db26":"from tensorflow.keras.preprocessing import image\n\n\ndef visualize (image_path):\n    \n    #take image path and load the image\n    img = image.load_img(image_path, target_size=(224, 224))\n    #trasform the image to array\n    img_tensor = image.img_to_array(img)\n    img_tensor = np.expand_dims(img_tensor, axis=0)\n    #scaling the image\n    img_tensor \/= 255.0\n\n    # model that takes the input image and return the activations for the\n    # conv and pool layers in the trained model \n    layer_outputs = [layer.output for layer in my_model.layers[:10]]\n    activation_model = tf.keras.models.Model(inputs=my_model.input, outputs=layer_outputs)\n    \n    activation = activation_model.predict(img_tensor)\n    \n    layer_names = []\n\n    for layer in my_model.layers[:10]:\n        layer_names.append(layer.name)\n\n    images_per_row = 16\n\n    for layer_name, layer_activation in zip(layer_names, activation):\n        n_features = layer_activation.shape[-1]\n\n        size = layer_activation.shape[1]\n\n        n_cols = n_features \/\/ images_per_row\n        display_grid = np.zeros((size * n_cols, images_per_row * size))\n\n        for col in range(n_cols):\n            for row in range(images_per_row):\n                channel_image = layer_activation[0,\n                                                :, :,\n                                                col * images_per_row + row]\n\n                channel_image -= channel_image.mean()\n                channel_image \/= channel_image.std()\n                channel_image *= 64\n                channel_image += 128\n                channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n                display_grid[col * size : (col + 1) * size, \n                            row * size : (row + 1) * size] = channel_image\n\n        scale = 1. \/ size\n        plt.figure(figsize=(scale * display_grid.shape[1],\n                            scale * display_grid.shape[0]))\n\n        plt.title(layer_name)\n        plt.grid(False)\n        plt.imshow(display_grid, aspect='auto', cmap='viridis')   ","d4ef9c23":"cat_img_path = r\"..\/input\/cats-vs-dogs\/data\/validation\/cats\/cat.10088.jpg\"\nvisualize (cat_img_path)","7abc93ee":"dog_img_path = r\"..\/input\/cats-vs-dogs\/data\/validation\/dogs\/dog.10068.jpg\"\nvisualize (dog_img_path)","6420a92b":"# the_class is 0 for cats and 1 for dogs\ndef veiw_saliency_map_for_binary_classification_model(url, the_class):\n    \n    #The image needs to be preprocessed before being fed to the model\n    \n    # read the image\n    img = io.imread(url)\n\n    # resize to 300x300 and normalize pixel values to be in the range [0, 1]\n    img = cv2.resize(img, (300, 300)) \/ 255.0\n\n    # add a batch dimension in front\n    image = np.expand_dims(img, axis=0)\n    #------------------------------------------------------------------------\n    # compute the gradient of the loss with respect to the input image,\n    # this should tell us how the loss value changes with respect to\n    # a small change in input image pixels\n    \n    expected_output = tf.constant(the_class, shape=(1, 1))\n    with tf.GradientTape() as tape:\n        # cast image to float\n        inputs = tf.cast(image, tf.float32)\n\n        # watch the input pixels\n        tape.watch(inputs)\n\n        # generate the predictions\n        predictions = my_model(inputs)\n\n        # get the loss\n        loss = tf.keras.losses.binary_crossentropy(\n            expected_output, predictions\n        )\n\n    # get the gradient with respect to the inputs\n    gradients = tape.gradient(loss, inputs)\n    # The gradient tells us how much the categorical loss changes when the input change,\n    #also it can indicate when certain parts of the input image have a\n    #significat effect on the categorical prediction\n    #------------------------------------------------------------------------\n    # reduce the RGB image to grayscale\n    grayscale_tensor = tf.reduce_sum(tf.abs(gradients), axis=-1)\n\n    # normalize the pixel values to be in the range [0, 255].\n    # the max value in the grayscale tensor will be pushed to 255.\n    # the min value will be pushed to 0.\n    normalized_tensor = tf.cast(\n        255\n        * (grayscale_tensor - tf.reduce_min(grayscale_tensor))\n        \/ (tf.reduce_max(grayscale_tensor) - tf.reduce_min(grayscale_tensor)),\n        tf.uint8,\n    )\n\n    # remove the channel dimension to make the tensor a 2d tensor\n    normalized_tensor = tf.squeeze(normalized_tensor)\n    #------------------------------------------------------------------------\n    gradient_color = cv2.applyColorMap(normalized_tensor.numpy(), cv2.COLORMAP_HOT)\n    gradient_color = gradient_color \/ 255.0\n    super_imposed = cv2.addWeighted(img, 0.5, gradient_color, 0.5, 0.0)\n\n    plt.figure(figsize=(8, 8))\n    plt.imshow(super_imposed)\n    plt.axis('off')\n    plt.show()","ffdab276":"url = \"https:\/\/hips.hearstapps.com\/hmg-prod.s3.amazonaws.com\/images\/cats-that-look-like-tigers-bengal-1562690663.jpg\"\nveiw_saliency_map_for_binary_classification_model(url, 0)","06920ae1":"url = \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/9\/97\/Dog_for_Senior_Dog_Food_Diet_Wikipedia_Page.jpg\/1200px-Dog_for_Senior_Dog_Food_Diet_Wikipedia_Page.jpg\"\nveiw_saliency_map_for_binary_classification_model(url, 1)","0df12f47":"Visualizing intermediate convnet outputs is the process of displaying the feature maps that output by various convolution and pooling layers in the netweork given an input. ","98a6c8d8":"# 1- Imports","a7b8cc57":"### create your CAM model","418789ff":"# 2- ImageDataGenerator","aba25191":"# 3- Building the model","aea2ac31":"# 5- Visualizing intermediate activations","d73e1eee":"**Have you ever asked about the responce map what looks like?**\n\nAfter the training of the Convolutional network, if we pass an image to it we can extract the feature maps that produced\nby the proceesing of this image using network layers. \n\nEach layer contain learnable filters that trained to detect certain featire from the input of the layer. The layer may contain \nany number of filters 32, 64, ... each of them detect certain feature and produce a feature map, so a layer that have \n64 filter will produce 64 feature map. \n\nin this notebook we will see the feature maps produced by each convolutional and pooling layer in a simple network we will train. \n\n\n**The code referance: Deep Learning with Python by Fran\u00e7ois Chollet**","a1dbc1c7":"So in the image above we can see that the model use the dog eyes nose and mouth to identify is class ","881f46d3":"Class Activation Map (CAM): is a matrix that shows what parts of the image the model was paying attension to when deciding what class assign to the image. its a map of which pixels of the image sent activation that we use to determine the class of the object ","b5456b80":"we can generate the CAM by getting the dot product of the class activation features and the class activation weights.\n\nwe will need the weights from the Global Average Pooling layer (GAP) to calculate the activations of each feature given a particular class.\n\n- Note that you'll get the weights from the dense layer that follows the global average pooling layer.\n\n- The last conv2D layer has (h,w,depth) of (5 x 5 x 256), so there are 256 features.\n\n- The global average pooling layer collapses the h,w,f (5 x 5 x 256) into a dense layer of 256 neurons (1 neuron per feature).\n\n- The activations from the global average pooling layer get passed to the last dense layer.\n\n- The last dense layer assigns weights to each of those 256 features (for each of the 10 classes), \n\n- So the weights of the last dense layer (which immmediately follows the global average pooling layer) are referred to in this context as the \"weights of the global average pooling layer\".\n\nFor each of the 10 classes, there are 256 features, so there are 256 feature weights, one weight per feature.","06cca6da":"# 7- Thank you ","03dfc14c":"to generate the class activation map, we want to get the features detected in the last convolution layer and see which ones are most active when generating the output probabilities.","03573a6a":"# 4- Generate the Class Activation Map (CAM)","3ac93fbe":"Thank you for reading, I hope you enjoyed and benefited from it.\n\nIf you have any questions or notes please leave it in the commont section.\n\nIf you like this notebook please press upvote and thanks again.","ee0da7a8":"- Saliency map: is a representation of every pixel in the image in way that make sense for our particular context. \n    \n- it's define the pixels in the image that end up having an impact on the classification of the image","bb5781e5":"We will start by making our image data generator for the training, validation, test sets then structuring a ConvNet, it's a simple and traditional network, series of Conolutional and pooling layers then at the top we add Dense layers (The classifier)","7dc17625":"# 6- Saliency\n","16cca493":"The network act like Information distillation pipeline where irrelevant information is filtered out and the useful information is refined. As we can see as we go deeper in the network activations become less visuallyinterpretable.","06523b99":"Use the CAM model to predict on the test set, so that it generates the features and the predicted probability for each class (results).","67a0b142":"Neural networks are black boxes where we can not extract the learning represntations in a form that the human can understand, \nbut it in not true for ConvNets, Where we can for example visualize intermediate convnet outputs, convnet filters, and heatmaps of class activation in an image. "}}