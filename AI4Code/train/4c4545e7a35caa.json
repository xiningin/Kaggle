{"cell_type":{"d38e3d38":"code","52e36cee":"code","aff8bbd9":"code","620d755c":"code","f2708eda":"code","19d952a9":"code","af8398e7":"code","2c6cb2c4":"code","ae185d1f":"code","2e5244c6":"code","c076cff3":"code","a19fc868":"code","4bd21f93":"code","13247b16":"code","d5897213":"code","938d12e6":"code","60f40c91":"code","36072cc9":"code","42c4b99b":"code","da74425e":"code","822181d1":"code","7e0a6593":"code","f4988d88":"code","507f0993":"code","a84cb5e1":"code","5bb90df3":"code","f2c96b8c":"code","f123fa6d":"code","5236089c":"code","e6c3619b":"code","392cac07":"code","ee420820":"code","b3d57da6":"code","01689424":"markdown","aaddee0d":"markdown","8a7e164b":"markdown","9fff099b":"markdown","333fe6dd":"markdown","9b41c3a1":"markdown","2530f323":"markdown","411842ad":"markdown","30623f53":"markdown","fcea9181":"markdown","b68cbfc1":"markdown","0ed4468c":"markdown","6769ab32":"markdown","8314c564":"markdown","9518ced8":"markdown"},"source":{"d38e3d38":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\nimport optuna\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\n\npd.set_option(\"display.max_columns\", 30)\npd.set_option(\"display.max_rows\", 30)","52e36cee":"train_df= pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ntest_df= pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsub_df= pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")","aff8bbd9":"test_df.head()","620d755c":"def plot_model_comparison(models,results,title):\n    \"\"\" \n        Compares the results of different models and plots box plots for the algorithms.\n        models: list of names of models\n        results: training results\n        title: title for the graph\n        \n    \"\"\"\n    fig = plt.figure()\n    fig.suptitle(title)\n    ax = fig.add_subplot(111)\n    plt.boxplot(results)\n    ax.set_xticklabels(models)\n    plt.show()\n\ndef timer(start_time=None):\n    \"\"\" \n        Helps  to keep track of time elapsed while training.\n        start time: if none then start time tracking\n                    if not none tracks time from start time         \n    \"\"\"\n    if not start_time:\n        print(datetime.now())\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print(\"Time taken: %i hours %i minutes and %s seconds.\" % (thour, tmin, round(tsec, 2)))\n\ndef getbounds(col):\n    ''' \n    This function returns the upper bound and the lower bound using the IQR for the column \"col\".\n    '''\n    sorted(col)\n    q1,q3 = np.percentile(col,[25,75]) # quartailes\n    iqr = q3-q1 # inter quartile range\n    lb = q1 -(1.5*iqr) # lower bound\n    ub = q3 +(1.5*iqr) # upper bound\n    return lb,ub","f2708eda":"print(train_df.shape, test_df.shape)","19d952a9":"train_df.dtypes","af8398e7":"train_df.columns","2c6cb2c4":"train_df.isnull().sum()","ae185d1f":"train_df.duplicated().sum()","2e5244c6":"train_df.describe().T","c076cff3":"train_df.head()","a19fc868":"cat_cols = [\"cat0\", \"cat1\", \"cat2\", \"cat3\", \"cat4\", \"cat5\", \"cat6\",\n\"cat7\",\"cat8\", \"cat9\"]\ncont_cols= [\"cont0\", \"cont1\", \"cont2\", \"cont3\", \"cont4\", \"cont5\",\n\"cont6\", \"cont7\", \"cont8\", \"cont9\", \"cont10\", \"cont11\", \"cont12\",\n\"cont13\"]","4bd21f93":"train_df[cont_cols].hist(figsize=(20,12))\nplt.show()","13247b16":"for col in cat_cols:\n    print (col,sorted(train_df[col].unique()))","d5897213":"for col in cat_cols:\n    print (col,sorted(train_df[col].unique()))","938d12e6":"train_df[cat_cols].apply(pd.Series.value_counts).plot(kind='bar',figsize=(15,5))\nplt.show()","60f40c91":"plt.figure(figsize=(15,15))\ncont_hm = train_df[cont_cols].corr()\nsns.heatmap(cont_hm, annot= True)\nplt.show()","36072cc9":"plt.figure(figsize=(12,10))\nsns.boxplot(data=train_df[cont_cols])\nplt.show()","42c4b99b":"plt.figure(figsize=(12,10))\nsns.boxplot(data=test_df[cont_cols])\nplt.show()","da74425e":"cont0_lb,cont0_ub = getbounds(train_df['cont0'])\ncont6_lb,cont6_ub = getbounds(train_df['cont6'])\ncont8_lb,cont8_ub = getbounds(train_df['cont8'])","822181d1":"train_df.loc[train_df['cont0']>=cont0_ub,'cont0']=cont0_ub\ntrain_df.loc[train_df['cont0']<=cont0_lb,'cont0']=cont0_lb\ntrain_df.loc[train_df['cont6']>=cont6_ub,'cont6']=cont6_ub\ntrain_df.loc[train_df['cont6']<=cont6_lb,'cont6']=cont6_lb\ntrain_df.loc[train_df['cont8']>=cont8_ub,'cont8']=cont8_ub\ntrain_df.loc[train_df['cont8']<=cont8_lb,'cont8']=cont8_lb","7e0a6593":"cont0_lb,cont0_ub = getbounds(test_df['cont0'])\ncont6_lb,cont6_ub = getbounds(test_df['cont6'])\ncont8_lb,cont8_ub = getbounds(test_df['cont8'])\n\ntest_df.loc[test_df['cont0']>=cont0_ub,'cont0']=cont0_ub\ntest_df.loc[test_df['cont0']<=cont0_lb,'cont0']=cont0_lb\ntest_df.loc[test_df['cont6']>=cont6_ub,'cont6']=cont6_ub\ntest_df.loc[test_df['cont6']<=cont6_lb,'cont6']=cont6_lb\ntest_df.loc[test_df['cont8']>=cont8_ub,'cont8']=cont8_ub\ntest_df.loc[test_df['cont8']<=cont8_lb,'cont8']=cont8_lb","f4988d88":"plt.figure(figsize=(12,10))\nsns.boxplot(data=train_df[cont_cols])\nplt.show()","507f0993":"plt.figure(figsize=(12,10))\nsns.boxplot(data=test_df[cont_cols])\nplt.show()","a84cb5e1":"for col in cat_cols:\n        values=train_df[col].unique()\n        labels_ordered = { k:i for i,k in enumerate(sorted(values),0)}\n        train_df[col]=train_df[col].map(labels_ordered)","5bb90df3":"for col in cat_cols:\n        values=test_df[col].unique()\n        labels_ordered = { k:i for i,k in enumerate(sorted(values),0)}\n        test_df[col]=test_df[col].map(labels_ordered)","f2c96b8c":"X = train_df.drop(columns=[\"id\",\"target\"])\nY = train_df[\"target\"]\ntest_df.drop(columns='id',inplace=True)","f123fa6d":"x_train,x_test,y_train,y_test=train_test_split(X, Y, test_size = 0.1, random_state = 21)","5236089c":"sc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\ntest_df = sc.transform(test_df)","e6c3619b":"def tuner(trial):    \n    params = {        \n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 7500, 12000, step=100),\n        \"max_depth\":trial.suggest_int(\"max_depth\", 4, 10),\n        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 7, 8),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True), \n        \"subsample\": trial.suggest_float(\"subsample\", 0.7, 1.0),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.1, 1.0),\n        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-2, 10.),\n        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-2, 10.),\n        \"gamma\": trial.suggest_float(\"gamma\", 0.7, 1.0, step=0.1),\n    }    \n    \n    model = XGBRegressor(\n        **params,\n        n_jobs=-1, \n        objective=\"reg:squarederror\",\n        tree_method='gpu_hist', \n        gpu_id=0\n    )\n    \n    model.fit(        \n        x_train, \n        y_train, \n        early_stopping_rounds=500, \n        eval_set=[(x_test,y_test)],        \n        verbose=0)\n    \n    y_hat = model.predict(x_test)\n    \n    return np.sqrt(mean_squared_error(y_test, y_hat))\n\nstart_time=timer(None)\nstudy = optuna.create_study()\nstudy.optimize(tuner, n_trials=100)\ntimer(start_time)","392cac07":"study.best_params","ee420820":"final_xgb_params = study.best_params\nXGBmodel = XGBRegressor(\n    **final_xgb_params,\n    n_jobs=-1, \n    tree_method='gpu_hist', \n    gpu_id=0\n)\n\nXGBmodel.fit(\n    x_train,\n    y_train, \n    early_stopping_rounds=300, \n    eval_set=[(x_test,y_test)],\n    verbose=0)","b3d57da6":"target_xgb = XGBmodel.predict(test_df)\nsub_df[\"target\"] = target_xgb\nsub_df[[\"id\", \"target\"]].to_csv(\"sub_XGB.csv\", index=False)\nsub_df.head()","01689424":"## 1.b) Load Dataset","aaddee0d":"### Outlier Treatment","8a7e164b":"#### Some Functions the we would use","9fff099b":"## 1.a) Load Libraries","333fe6dd":"# 3. Preparing Data","9b41c3a1":"### Neither we have any missing values nor any duplicates","2530f323":"# 2. Summarize Data","411842ad":"## 4.a) Split-out validation dataset","30623f53":"### After Outlier Treatment","fcea9181":"## 2.a) Descriptive statistics","b68cbfc1":"# 4. Evaluate Algorithms","0ed4468c":"# Observations - \n* There  are 300000 training rows  and 200000 test rows in the data\n* There are 10 categorical columns \n* There are 14 continuous numerical columns\n* No NULL values in the data\n* No duplicate values present\n* The unique category values are same in the train and test dataset\n* Columns Cont0, Cont6, and Cont8 have outliers - we cap the ouliers at the upper \/ lower bounds\n* For the categorical variables we map the values to numbers ","6769ab32":"# 1. Preparation","8314c564":"## 3.a) Data Transformation","9518ced8":"## Tuning XGBoost using optuna with GPU"}}