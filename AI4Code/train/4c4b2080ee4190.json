{"cell_type":{"b0e140d5":"code","7ee4658b":"code","a4e47361":"code","e6522d82":"code","3d0b164b":"code","3be6d732":"code","d5f2dde8":"code","102c2cc3":"code","1d74b944":"code","89e421c4":"code","99437ddc":"code","8f830731":"code","8ad8f347":"code","d74114f9":"code","844117f4":"code","73150d3e":"code","c2adcad1":"code","f438f4fd":"code","1aef0caf":"code","b145eb5b":"code","3bac7282":"code","e26b4cec":"code","a6288d21":"code","71944ebe":"code","9f829358":"code","d2758148":"code","e9a3522c":"code","6a6847e5":"code","f254a6dc":"code","0cd9ee23":"code","690b8e12":"code","7f728211":"code","9582dc37":"code","62a2c155":"code","e4edfc2d":"code","65dabe43":"code","318f7666":"code","e3f091c6":"markdown","ce4d44f1":"markdown","ebc20586":"markdown","600b9b8a":"markdown","a6011a18":"markdown","87ddb87b":"markdown","6f0c7a7f":"markdown","7dff8c98":"markdown","2bde6ae8":"markdown","33ed69ea":"markdown","0bdac7ae":"markdown","721ec852":"markdown","f5ca890a":"markdown"},"source":{"b0e140d5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7ee4658b":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nfrom sklearn.preprocessing import MinMaxScaler, scale, StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer, precision_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn import svm\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix\nwarnings.filterwarnings('ignore')\n%matplotlib inline","a4e47361":"data = pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')","e6522d82":"data.head()","3d0b164b":"data.info()","3be6d732":"data.describe()","d5f2dde8":"sns.pairplot(data)","102c2cc3":"# Feature correlation heatmap\nfig, ax = plt.subplots(1, 1, figsize=(10, 10))\ncorr = data.drop(columns=['target']).corr()\ncorr = corr.round(decimals=2)\ncorr = corr.where(np.tril(np.ones(corr.shape)).astype(np.bool)) # make heatmap lower triangular (remove redundant info)\nsns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, ax=ax, cmap = 'coolwarm')\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nax.set_ylim(len(corr)+0.5, -0.5)\nplt.show()","1d74b944":"# Histogram for all features\n\nfig, ax = plt.subplots(1, 1, figsize=(15, 15))\ndata.drop(columns=['target']).hist(ax=ax)\nplt.show()","89e421c4":"sns.countplot(data['sex'], hue=data['target'])","99437ddc":"sns.countplot(data['cp'], hue=data['target'])","8f830731":"sns.scatterplot(x=data['age'], y=data['chol'], hue=data['target'])","8ad8f347":"sns.countplot(data['thal'], hue=data['target'])","d74114f9":"cp = pd.get_dummies(data['cp'], drop_first=True)\nrestecg = pd.get_dummies(data['restecg'], drop_first=True)\nslope = pd.get_dummies(data['slope'], drop_first=True)\nca = pd.get_dummies(data['ca'], drop_first=True)","844117f4":"cp.columns = ['cp_1', 'cp_2', 'cp_3']\nrestecg.columns = ['restecg_1', 'restecg_2']\nslope.columns = ['slope_1', 'slope_2']\nca.columns = ['ca_1', 'ca_2', 'ca_3', 'ca_4']","73150d3e":"data.drop(['cp', 'restecg', 'slope', 'ca'], axis=1, inplace=True)","c2adcad1":"data = pd.concat([data, cp, restecg, slope,ca], axis=1)","f438f4fd":"data.head()","1aef0caf":"X = data.drop(['target'], axis=1)\ny = data['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","b145eb5b":"train_score_array = []\ntest_score_array = []\n\nfor k in range(1, 20):\n    knn = KNeighborsClassifier(k)\n    knn.fit(X_train, y_train)\n    train_score_array.append(knn.score(X_train, y_train))\n    test_score_array.append(knn.score(X_test, y_test))\n    \nx_axis = range(1, 20)\nplt.subplots(figsize = (20, 5))\nplt.plot(train_score_array, label='Train score array', c='g')\nplt.plot(test_score_array, label='Test score array', c='b')\nplt.xlabel('N neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(x_axis, np.arange(20))\nplt.grid()\nplt.legend()\nplt.show()","3bac7282":"param_grid = {'n_neighbors' : np.arange(1, 20)}\ngrid_search = GridSearchCV(knn, param_grid, cv=5, scoring= 'precision_macro' , return_train_score=True)\ngrid_search.fit(X_train, y_train)\n\ngrid_search.best_params_","e26b4cec":"knn = KNeighborsClassifier(n_neighbors=11)\nknn.fit(X_train, y_train)\nprint('Training score:', knn.score(X_train, y_train))\nprint('Testing score:', knn.score(X_test, y_test))","a6288d21":"y_pred_train = knn.predict(X_train)\ny_pred_test = knn.predict(X_test)\n\nknn_train_precision_score = precision_score(y_train, y_pred_train, average='macro')\nknn_test_precision_score = precision_score(y_test, y_pred_test, average='macro')\n\nprint('Train Precision score:', knn_train_precision_score)\nprint('Test Precision score:', knn_test_precision_score)\nconfusion_matrix(y_test, y_pred_test)","71944ebe":"knn = KNeighborsClassifier(n_neighbors=11)\nknn.fit(X_train, y_train)\ntrain_score = cross_val_score(knn, X_train, y_train)\ntest_score = cross_val_score(knn, X_test, y_test)\nprint('Cross-validation scores:', train_score)\nprint('Cross-validation scores:', test_score)\nprint('Average Train score:', train_score.mean())\nprint('Average Test score:', test_score.mean())","9f829358":"c_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\ntrain_score_l1 = []\ntrain_score_l2 = []\ntest_score_l1 = []\ntest_score_l2 = []\n\nfor c in c_range:\n    log_l1 = LogisticRegression(penalty = 'l1', C = c, solver = 'liblinear', max_iter = 500)\n    log_l2 = LogisticRegression(penalty = 'l2', C = c, solver = 'lbfgs', max_iter = 500)\n    log_l1.fit(X_train, y_train)\n    log_l2.fit(X_train, y_train)\n    train_score_l1.append(log_l1.score(X_train, y_train))\n    train_score_l2.append(log_l2.score(X_train, y_train))\n    test_score_l1.append(log_l1.score(X_test, y_test))\n    test_score_l2.append(log_l2.score(X_test, y_test))\n    \nplt.subplots(figsize = (20,5))\nplt.plot(c_range, train_score_l1, label = 'Train score, penalty = l1')\nplt.plot(c_range, test_score_l1, label = 'Test score, penalty = l1')\nplt.plot(c_range, train_score_l2, label = 'Train score, penalty = l2')\nplt.plot(c_range, test_score_l2, label = 'Test score, penalty = l2')\nplt.legend()\nplt.xlabel('Regularization parameter: C')\nplt.ylabel('Accuracy')\nplt.xscale('log')","d2758148":"lreg_clf = LogisticRegression()\n\nparam_grid = {'C': [0.00001,0.0001,0.001,0.01,0.1,1,10,100], 'penalty': ['l1', 'l2']}\n\ngrid_search = GridSearchCV(lreg_clf, param_grid, cv=5, scoring= 'precision_macro' ,return_train_score=True)\ngrid_search.fit(X_train, y_train)\n\ngrid_search.best_params_","e9a3522c":"lreg_clf = LogisticRegression(C=10, penalty= 'l2')\nlreg_clf.fit(X_train, y_train)\n\ny_pred_train = lreg_clf.predict(X_train)\ny_pred_test = lreg_clf.predict(X_test)\n\nlreg_train_precision_score = precision_score(y_train, y_pred_train, average='macro')\nlreg_test_precision_score = precision_score(y_test, y_pred_test, average='macro')\n\nprint('Train Precision score:', lreg_train_precision_score)\nprint('Test Precision score:', lreg_test_precision_score)\n\nconfusion_matrix(y_test, y_pred_test)","6a6847e5":"lreg_clf = LogisticRegression(C=10, penalty= 'l2')\nlreg_clf.fit(X_train, y_train)\ntrain_score = cross_val_score(lreg_clf, X_train, y_train)\ntest_score = cross_val_score(lreg_clf, X_test, y_test)\nprint('Cross-validation scores:', train_score)\nprint('Cross-validation scores:', test_score)\nprint('Average Train score:', train_score.mean())\nprint('Average Test score:', test_score.mean())","f254a6dc":"LSVC_clf = LinearSVC()\n\nparam_grid = {'C': [0.00001,0.0001,0.001,0.01,0.1,1,10,100]}\n\ngrid_search = GridSearchCV(LSVC_clf, param_grid, cv=5, scoring='precision_macro', return_train_score=True, iid=False)\ngrid_search.fit(X_train, y_train)\n\ngrid_search.best_params_","0cd9ee23":"LSVC_clf = LinearSVC(C=0.1)\nLSVC_clf.fit(X_train, y_train)\n\ny_pred_train = LSVC_clf.predict(X_train)\ny_pred_test = LSVC_clf.predict(X_test)\n\nLSVC_train_precision_score = precision_score(y_train, y_pred_train, average='macro')\nLSVC_test_precision_score = precision_score(y_test, y_pred_test, average='macro')\n\nprint('Train Precision score:', LSVC_train_precision_score)\nprint('Test Precision score:', LSVC_test_precision_score)\nconfusion_matrix(y_test, y_pred_test)","690b8e12":"LSVC_clf = LinearSVC(C=0.1)\nLSVC_clf.fit(X_train, y_train)\ntrain_score = cross_val_score(LSVC_clf, X_train, y_train)\ntest_score = cross_val_score(LSVC_clf, X_test, y_test)\nprint('Cross-validation scores:', train_score)\nprint('Cross-validation scores:', test_score)\nprint('Average Train score:', train_score.mean())\nprint('Average Test score:', test_score.mean())","7f728211":"KSVC_clf = svm.SVC(kernel='rbf', random_state=0)\n\nparam_grid = {'C': [0.0001,0.001,0.01,0.1,1,10],\n          'gamma': [0.0001,0.001,0.1,1,10]}\n\ngrid_search = GridSearchCV(KSVC_clf, param_grid, cv=5, scoring= 'precision_macro', return_train_score=True, iid=False)\ngrid_search.fit(X_train, y_train)\n\ngrid_search.best_params_","9582dc37":"KSVC_clf = svm.SVC(kernel='rbf', C=10, gamma=0.001, probability=True)\nKSVC_clf.fit(X_train, y_train)\n\ny_pred_train = KSVC_clf.predict(X_train)\ny_pred_test = KSVC_clf.predict(X_test)\n\nKSVC_train_precision_score = precision_score(y_train, y_pred_train, average='macro')\nKSVC_test_precision_score = precision_score(y_test, y_pred_test, average='macro')\n\nprint('Train Precision score:', KSVC_train_precision_score)\nprint('Test Precision score:', KSVC_test_precision_score)\nconfusion_matrix(y_test, y_pred_test)","62a2c155":"KSVC_clf = svm.SVC(kernel='rbf',C=10, gamma=0.001, probability=True)\nKSVC_clf.fit(X_train, y_train)\ntrain_score = cross_val_score(KSVC_clf, X_train, y_train)\ntest_score = cross_val_score(KSVC_clf, X_test, y_test)\nprint('Cross-validation scores:', train_score)\nprint('Cross-validation scores:', test_score)\nprint('Average Train score:', train_score.mean())\nprint('Average Test score:', test_score.mean())","e4edfc2d":"dt_clf = DecisionTreeClassifier()\nparam_grid = {'max_depth': np.arange(1,20)}\n\ngrid_search = GridSearchCV(dt_clf, param_grid, return_train_score=True, scoring='precision_macro', iid=False)\ngrid_search.fit(X_train, y_train)\n\ngrid_search.best_params_","65dabe43":"dt_clf = DecisionTreeClassifier(max_depth=3)\ndt_clf.fit(X_train, y_train)\n\ny_pred_train = dt_clf.predict(X_train)\ny_pred_test = dt_clf.predict(X_test)\n\ndt_train_precision_score = precision_score(y_train, y_pred_train, average='macro')\ndt_test_precision_score = precision_score(y_test, y_pred_test, average='macro')\n\nprint('Train Precision score:', dt_train_precision_score)\nprint('Test Precision score:', dt_test_precision_score)\nconfusion_matrix(y_test, y_pred_test)","318f7666":"dt_clf = DecisionTreeClassifier(max_depth=3)\ndt_clf.fit(X_train, y_train)\ntrain_score = cross_val_score(dt_clf, X_train, y_train)\ntest_score = cross_val_score(dt_clf, X_test, y_test)\nprint('Cross-validation scores:', train_score)\nprint('Cross-validation scores:', test_score)\nprint('Average Train score:', train_score.mean())\nprint('Average Test score:', test_score.mean())","e3f091c6":"Creating dummy variables for columns with classes","ce4d44f1":"Cholestrol at a young age is dangerous","ebc20586":"**Logistic Regression**","600b9b8a":"**KNN Classification**","a6011a18":"**Kernel SVM**","87ddb87b":"Chest Pain type 2 is the most dangerous type","6f0c7a7f":"Males are more likely to suffer from heart disease","7dff8c98":"Thank you! :)","2bde6ae8":"**Linear SVM**","33ed69ea":"Splitting and Transforming data","0bdac7ae":"**Decision Tree Classification**","721ec852":"Column description:\n\n1. age: age\n2. sex: sex (1 = male, 0 = female)\n3. cp: chest pain type (4 values)\n4. trestbps: resting blood pressure\n5. chol: serum cholestoral in mg\/dl\n6. fbs: fasting blood sugar > 120 mg\/dl\n7. restecg: resting electrocardiographic results (values 0,1,2)\n8. thalach: maximum heart rate achieved\n9. exang: exercise induced angina\n10. oldpeak: ST depression induced by exercise relative to rest\n12. slope: the slope of the peak exercise ST segment\n13. ca: number of major vessels (0-3) colored by flourosopy\n14. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect","f5ca890a":"**Data Analytics and Visualization**"}}