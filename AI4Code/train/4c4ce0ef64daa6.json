{"cell_type":{"07421eb6":"code","82ec2e5f":"code","96cc5c23":"code","31cd1422":"code","b46aef35":"code","aca50ad6":"code","192fbf74":"code","14224f15":"code","e9413655":"code","09c1c8a3":"code","2807d620":"code","b3d2cf81":"code","314f1521":"code","e432cdcd":"code","39ea724e":"code","fb6ba6dd":"code","e38907c8":"code","ec33e1ff":"code","e65bafc8":"code","d4547fad":"code","70619e14":"code","ab184e3f":"code","637a92a2":"code","8bff3b34":"code","2e3d06da":"code","4a8f8e07":"code","4529876c":"code","3bf5f8f6":"code","36a0175b":"code","da23b481":"code","a75c9d9e":"code","504d4a8a":"code","5f40ecfc":"code","8febc29e":"code","9258637e":"code","5f077934":"code","e6ec372c":"code","c3789def":"code","d8ff1bae":"code","272b4b67":"code","5c3694cd":"code","131bfc26":"code","142bf02d":"code","2d5287bf":"code","44b7c59f":"code","31100a95":"code","87c1be1e":"code","1919b033":"code","354a17f0":"code","01b8be91":"code","4722b898":"code","40f1ad0e":"code","63ab6bd2":"code","cdb99ea7":"markdown","26c2a82c":"markdown","add5773a":"markdown","e402518c":"markdown","b9147f61":"markdown","97daaaeb":"markdown","cc50936d":"markdown","2b7dd001":"markdown","68f4b34e":"markdown","9193a8c0":"markdown","22c2b195":"markdown","b51784b7":"markdown","d484b14d":"markdown","dae0b495":"markdown","6cefc223":"markdown","f4774579":"markdown","bda5836d":"markdown","3f7d6877":"markdown","2bd2f2e6":"markdown","766a9a3a":"markdown","1a87949a":"markdown","4966e8f1":"markdown","f803a353":"markdown","eac9e4ac":"markdown"},"source":{"07421eb6":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')","82ec2e5f":"train_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\ncombine = [train_df, test_df]\n\n#combine= pd.concat() another way to combine the data sets","96cc5c23":"print(train_df.columns.values)","31cd1422":"train_df.head()","b46aef35":"train_df.tail()","aca50ad6":"train_df.info()\nprint('_ '*40)\ntest_df.info()","192fbf74":"train_df.isnull().sum()","14224f15":"test_df.isnull().sum()","e9413655":"train_df.describe()","09c1c8a3":"train_df.describe(include=['O'])","2807d620":"train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","b3d2cf81":"train_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","314f1521":"train_df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","e432cdcd":"train_df[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","39ea724e":"g = sns.FacetGrid(train_df, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","fb6ba6dd":"grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","e38907c8":"grid = sns.FacetGrid(train_df, row='Embarked', size=2.2, aspect=1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()","ec33e1ff":"grid = sns.FacetGrid(train_df, row='Embarked', col='Survived', size=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()","e65bafc8":"print(\"Before: \", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)\n\ntrain_df = train_df.drop(['Ticket', 'Cabin'], axis=1)\ntest_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\ncombine = [train_df, test_df]\n\nprint(\"After: \", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)","d4547fad":"for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_df['Title'], train_df['Sex'])","70619e14":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Dr', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace(['Mlle','Lady','Ms','Countess'], 'Miss')\n    dataset['Title'] = dataset['Title'].replace(['Mme','Sir','Don', 'Capt', 'Col','Major', 'Jonkheer', 'Rev','Master'], 'Mrs')\n    \ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","ab184e3f":"#converting the categorical titles to ordinal.\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Rare\": 4}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_df.head()","637a92a2":"train_df = train_df.drop(['Name', 'PassengerId'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.shape, test_df.shape","8bff3b34":"for dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ntrain_df.head()","2e3d06da":"grid = sns.FacetGrid(train_df, row='Pclass', col='Sex', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","4a8f8e07":"guess_ages = np.zeros((2,3))\nfor dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n            age_guess = guess_df.median()\n            \n            guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain_df.head()","4529876c":"train_df['AgeBand'] = pd.cut(train_df['Age'], 5)\ntrain_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","3bf5f8f6":"# Let us replace Age with ordinals based on these bands.\nfor dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\ntrain_df.head()","36a0175b":"# remove the ageband column\ntrain_df = train_df.drop(['AgeBand'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.head()","da23b481":"for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","a75c9d9e":"for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","504d4a8a":"train_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ntest_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ncombine = [train_df, test_df]\n\ntrain_df.head()","5f40ecfc":"for dataset in combine:\n    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n\ntrain_df.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)","8febc29e":"freq_port = train_df.Embarked.dropna().mode()[0]\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \ntrain_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","9258637e":"#  now convert the EmbarkedFill feature by creating a new numeric Port feature\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntrain_df.head()","5f077934":"test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\ntest_df.head()","e6ec372c":"train_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","c3789def":"for dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain_df = train_df.drop(['FareBand'], axis=1)\ncombine = [train_df, test_df]\n    \ntrain_df.head(5)","d8ff1bae":"test_df.head(10)","272b4b67":"X_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","5c3694cd":"# Logistic Regression\n\nLR = LogisticRegression()\nLR.fit(X_train, Y_train)\nY_pred = LR.predict(X_test)\nacc_log = round(LR.score(X_train, Y_train) * 100, 2)\nacc_log","131bfc26":"coeff_df = pd.DataFrame(train_df.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(LR.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)","142bf02d":"# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","2d5287bf":"# k-Nearest Neighbors algorithm\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","44b7c59f":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","31100a95":"# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron","87c1be1e":"# Linear SVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc","1919b033":"# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd","354a17f0":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","01b8be91":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","4722b898":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)","40f1ad0e":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('submission.csv', index=False)","63ab6bd2":"submission.head()","cdb99ea7":"__Observations of Embark__\n\n* Female passengers had much better survival rate than males. Confirms classifying (#1).\n* Exception in Embarked=C where males had higher survival rate. This could be a correlation between Pclass and Embarked and in turn Pclass and Survived, not necessarily direct correlation between Embarked and Survived.\n* Males had better survival rate in Pclass=3 when compared with Pclass=2 for C and Q ports. Completing (#2).\n* Ports of embarkation have varying survival rates for Pclass=3 and among male passengers. Correlating (#1).","26c2a82c":"**Distribution of numerical feature values across the samples**\n\n1. Total samples are 891 or 40% of the actual number of passengers on board the Titanic (2,224).\n2. Survived is a categorical feature with 0 or 1 values.\n3. Around 38% samples survived representative of the actual survival rate at 32%.\n4. Most passengers (> 75%) did not travel with parents or children.\n5. Nearly 30% of the passengers had siblings and\/or spouse aboard.\n6. Fares varied significantly with few passengers (<1%) paying as high as $512.\n7. Few elderly passengers (<1%) within age range 65-80.","add5773a":"Categorical features: \n    Normal: **Survived, Sex,** and __Embarked__. Ordinal: **Pclass**.\n\nNumerical freatures:\n    Continous: **Age, Fare.** Discrete: **SibSp, Parch.**\n\n**Ticket** is a mix of numeric and alphanumeric data types. **Cabin** is alphanumeric","e402518c":"Now we can safely drop the Name feature from training and testing datasets. We also do not need the PassengerId feature in the training dataset.","b9147f61":"**Distribution of categorical features**\n\n1. Names are unique across the dataset (count=unique=891)\n2. Sex variable as two possible values with 65% male (top=male, freq=577\/count=891).\n3. Cabin values have several dupicates across samples. Alternatively several passengers shared a cabin.\n4. Embarked takes three possible values. S port used by most passengers (top=S)\n5. Ticket feature has high ratio (22%) of duplicate values (unique=681).","97daaaeb":"* Sex is highest positivie coefficient, implying as the Sex value increases (male: 0 to female: 1), the probability of Survived=1 increases the most.\n* Inversely as Pclass increases, probability of Survived=1 decreases the most.\n* This way Age*Class is a good artificial feature to model as it has second highest negative correlation with Survived.\n* So is Title as second highest positive correlation.","cc50936d":"* Logistic Regression\n* KNN or k-Nearest Neighbors\n* Support Vector Machines\n* Naive Bayes classifier\n* Decision Tree\n* Random Forrest\n* Perceptron\n* Artificial neural network\n* RVM or Relevance Vector Machine","2b7dd001":"**Observations**\n\nWhen we plot Title, Age, and Survived, we note the following observations.\n\n* Most titles band Age groups accurately. For example: Master title has Age mean of 5 years.\n* Survival among Title Age bands varies slightly.\n* Certain titles mostly survived (Mme, Lady, Sir) or did not (Don, Rev, Jonkheer).\n\n'Mme','Sir','Don', 'Capt', 'Col','Major', 'Jonkheer', 'Rev','Master', 'Mlle','Lady','Ms','Countess', 'Miss', 'Mrs','Dr' are extracted from name columns and given Title name which helps to classify with the female and male \n* 'Mme','Sir','Don', 'Capt', 'Col','Major', 'Jonkheer', 'Rev','Master' all are replaced with 'Mrs'\n* 'Mlle','Lady','Ms','Countess' are replaced as 'Miss'\n* 'Dr' is mixed with both genders so relacing it as 'Rare'\n\n**The above titles are differentiated with respect to genders**","68f4b34e":"__Pclass__: We observe significant correlation (>0.5) among Pclass=1 and Survived (classifying #3). We decide to include this feature in our model.\n\n__Sex__: We confirm the observation during problem definition that Sex=female had very high survival rate at 74% (classifying #1).\n\n__SibSp and Parch__: These features have zero correlation for certain values. It may be best to derive a feature or a set of features from these individual features (creating #1).","9193a8c0":"create a new feature for FamilySize which combines Parch and SibSp. This will enable us to drop Parch and SibSp from our datasets.","22c2b195":"__Observation of Age column__\n\n* Infants (Age <=4) had high survival rate.\n* Oldest passengers (Age = 80) survived.\n* Large number of 15-25 year olds did not survive.\n* Most passengers are in 15-35 age range.","b51784b7":"# Analyzing Pivoting features","d484b14d":"Let us create Age bands and determine correlations with Survived.","dae0b495":"Seven features are integer or floats. Six in case of test dataset.\nFive features are strings (object).","6cefc223":"# Loading the data\n\nThe Python Pandas packages helps us work with our datasets. We start by loading the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.","f4774579":"# Data wrangling","bda5836d":"# Analysing the dataset","3f7d6877":"In train data set the __Age__ have 177, __Cabin__ have 687 and __Embarked__ have 2 __Missing \/ Null values__","2bd2f2e6":"# Analyze by visualizing data","766a9a3a":"In test data set the **Age** have 86, **Cabin** have 327 and **Fare** have 2 **Missing \/ Null values**","1a87949a":"Let us start by converting Sex feature to a new feature called Gender where female = 1 and male = 0.","4966e8f1":"__Observations:__\n\n* Higher fare paying passengers had better survival. Confirms our assumption for creating (#4) fare ranges.\n* Port of embarkation correlates with survival rates. Confirms correlating (#1) and completing (#2).","f803a353":"# ML Models","eac9e4ac":"__Observation of PClass__\n\n* Pclass=3 had most passengers, however most did not survive. Confirms our classifying assumption #2.\n* Infant passengers in Pclass=2 and Pclass=3 mostly survived. Further qualifies our classifying assumption #2.\n* Most passengers in Pclass=1 survived. Confirms our classifying assumption #3.\n* Pclass varies in terms of Age distribution of passengers."}}