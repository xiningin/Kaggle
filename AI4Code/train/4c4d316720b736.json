{"cell_type":{"165b64bf":"code","4423f364":"code","be241810":"code","f135087a":"code","50fb4054":"code","5bd7132e":"code","13bbbcbe":"code","c5172d73":"code","50b8841d":"code","9aeff717":"code","6a498682":"code","f6910ce0":"code","1a247522":"code","c5ce3253":"code","d1016005":"code","673be7d4":"code","b20e0b50":"code","2c106b03":"code","113ee0f8":"code","8c5dac0a":"code","00fd79da":"code","fd549bdb":"code","5457c608":"code","84708d39":"code","86fe96db":"code","c35a66df":"code","59f8fa4b":"code","782af486":"code","27a37db0":"code","3250cfd9":"code","5811f702":"code","abb68f83":"code","bb2c9046":"code","3f87e88d":"code","095b0a12":"code","5b7f2bf5":"code","d1cd7a9f":"code","36a3f3fc":"markdown","64515f62":"markdown","c5c47c5d":"markdown","07827677":"markdown","bf14db30":"markdown","9bc86af5":"markdown","a3756a57":"markdown","22437a3f":"markdown","0cc43ccc":"markdown"},"source":{"165b64bf":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","4423f364":"heart_df = pd.read_csv(\"..\/input\/heart-clean\/heart_clean.csv\")","be241810":"heart_df.shape","f135087a":"heart_df.info()","50fb4054":"heart_df.head()","5bd7132e":"heart_df.tail()","13bbbcbe":"heart_df.describe(include='all')","c5172d73":"heart_df.sample(10)","50b8841d":"heart_df.isnull().sum()","9aeff717":"heart_df.dtypes","6a498682":"heart_df.hist(figsize=(10,15))","f6910ce0":"sns.pairplot(heart_df, diag_kind='kde', hue='HeartDisease')","1a247522":"corr = heart_df.corr()\ncorr","c5ce3253":"sns.heatmap(corr, annot=False)","d1016005":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split","673be7d4":"x=heart_df.drop(\"HeartDisease\",axis=1)\ny=heart_df[\"HeartDisease\"]","b20e0b50":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.30, random_state = 42)","2c106b03":"model=LogisticRegression(solver='liblinear')\nmodel.fit(x_train,y_train)","113ee0f8":"for idx, col_name in enumerate(x_train.columns):\n    print(\"The coefficient for {} is {}\".format(col_name, model.coef_[0][idx]))","8c5dac0a":"intercept = model.intercept_[0]\nprint(\"The intercept for our model is {}\".format(intercept))","00fd79da":"predictions = model.predict(x_test)","fd549bdb":"model_score = model.score(x_train, y_train)\nprint(model_score)","5457c608":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test, predictions)","84708d39":"from sklearn import metrics\nprint(metrics.classification_report(y_test, predictions))","86fe96db":"cm=metrics.confusion_matrix(y_test, predictions, labels=[0, 1])\n\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"No\",\"Yes\"]],\n                  columns = [i for i in [\"No\",\"Yes\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot=True,fmt='g')","c35a66df":"from sklearn.tree import DecisionTreeClassifier\ndTree = DecisionTreeClassifier(criterion = 'gini', random_state = 1)\ndTree.fit(x_train, y_train)","59f8fa4b":"print(dTree.score(x_train, y_train))\nprint(dTree.score(x_test, y_test))","782af486":"dTreeR = DecisionTreeClassifier(criterion = 'gini', max_depth = 3, random_state = 1)\ndTreeR.fit(x_train, y_train)\nprint(dTreeR.score(x_train, y_train))\nprint(dTreeR.score(x_test, y_test))","27a37db0":"print(dTreeR.score(x_test, y_test))\ny_predict = dTreeR.predict(x_test)\n\ncm = metrics.confusion_matrix(y_test, y_predict, labels = [0,1])\n\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"No\", \"Yes\"]],\n                    columns = [i for i in [\"No\", \"Yes\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot = True, fmt = 'g')","3250cfd9":"from sklearn.ensemble import BaggingClassifier\n\nbgcl = BaggingClassifier(n_estimators=50, bootstrap=True, oob_score=True, random_state=1)\nbgcl = bgcl.fit(x_train, y_train)","5811f702":"y_predict = bgcl.predict (x_test)\nprint(bgcl.score(x_test, y_test))\ncm = metrics.confusion_matrix(y_test, y_predict, labels = [0,1])\n\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"No\", \"Yes\"]],\n                    columns = [i for i in [\"No\", \"Yes\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot = True, fmt = 'g')","abb68f83":"from sklearn.ensemble import AdaBoostClassifier\nabcl = AdaBoostClassifier (n_estimators = 10, random_state = 1)\n#abcl = AdaBoostClassisifier (n_estimators = 50, random_state = 1)\nabcl = abcl.fit(x_train, y_train)","bb2c9046":"y_predict = abcl.predict (x_test)\nprint(abcl.score(x_test, y_test))\ncm = metrics.confusion_matrix(y_test, y_predict, labels = [0,1])\n\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"No\", \"Yes\"]],\n                    columns = [i for i in [\"No\", \"Yes\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot = True, fmt = 'g')","3f87e88d":"from sklearn.ensemble import GradientBoostingClassifier\ngbcl = GradientBoostingClassifier(n_estimators = 50, random_state=1)\ngbcl = gbcl.fit(x_train, y_train)","095b0a12":"y_predict = gbcl.predict (x_test)\nprint(gbcl.score(x_test, y_test))\ncm = metrics.confusion_matrix(y_test, y_predict, labels = [0,1])\n\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"No\", \"Yes\"]],\n                    columns = [i for i in [\"No\", \"Yes\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot = True, fmt = 'g')","5b7f2bf5":"from sklearn.ensemble import RandomForestClassifier\nrfcl = RandomForestClassifier(n_estimators = 50, random_state = 1, max_features = 8)\nrfcl = rfcl.fit(x_train, y_train)","d1cd7a9f":"y_predict = rfcl.predict (x_test)\nprint(rfcl.score(x_test, y_test))\ncm = metrics.confusion_matrix(y_test, y_predict, labels = [0,1])\n\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"No\", \"Yes\"]],\n                    columns = [i for i in [\"No\", \"Yes\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot = True, fmt = 'g')","36a3f3fc":"### Split the Data","64515f62":"### Ensebmle Learning - AdaBoosting","c5c47c5d":"### Ensebmle Learning - GradientBoostingClassifier","07827677":"### Reducing over fitting (Regularization) - Decision Tree","bf14db30":"### Fit the Model - Logistic Regression","9bc86af5":"#### The confusion matrix\n\n#### No, No = True Positives (TP): We correctly predicted that they do not have Heart Disease 100.\n\n#### Yes, Yes = True Negatives (TN): We correctly predicted that they have Heart Disease 138.\n\n#### No, Yes = False Positives (FP): We incorrectly predicted that they don't have Heart Disease (a \"Type I error\") 26 Falsely predict positive Type I error.\n\n#### Yes, No = False Negatives (FN): We incorrectly predicted that they have Heart Disease (a \"Type II error\") 12 Falsely predict negative Type II error.","a3756a57":"### Build Decision Tree Model","22437a3f":"### Ensemble RandomForest Classifier","0cc43ccc":"### Ensebmle Learning - Bagging"}}