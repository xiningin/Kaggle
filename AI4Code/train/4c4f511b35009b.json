{"cell_type":{"e0052fde":"code","b8f8932a":"code","04c6fe9d":"code","f952402a":"code","1ef663df":"code","10cdf51c":"code","28a3a54e":"code","b592f439":"code","69919342":"code","01cb920f":"code","3fe6625c":"code","880d90ce":"code","bd645cbb":"code","666fd32f":"code","9b926958":"code","b3f41da2":"code","66574939":"code","a9d0cd0f":"code","4f244b7e":"code","227e7be9":"markdown","ab936f41":"markdown","b2f4b6c6":"markdown","6bc94f9c":"markdown","0f7d1d40":"markdown","ca0d9f03":"markdown","1c457306":"markdown","15ce22cc":"markdown","2af424da":"markdown"},"source":{"e0052fde":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport nltk \nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n\nimport re","b8f8932a":"train_df = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntrain_df.head()","04c6fe9d":"test_df = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\ntest_df.head()","f952402a":"train_df.shape, test_df.shape","1ef663df":"train_df.info()","10cdf51c":"train_df.isnull().sum()","28a3a54e":"test_df.isnull().sum()","b592f439":"train_df.drop([\"location\"], axis=1, inplace=True)\n\ntest_df.drop([\"location\"], axis=1, inplace=True)\n\ntrain_df['keyword'] = train_df['keyword'].fillna(train_df['keyword'].value_counts().idxmax())\n\ntest_df['keyword'] = test_df['keyword'].fillna(test_df['keyword'].value_counts().idxmax())","69919342":"print(train_df.isnull().sum())\nprint(test_df.isnull().sum())","01cb920f":"sns.countplot(x = \"target\", data = train_df)","3fe6625c":"train_df.drop([\"id\"], axis=1, inplace=True)\ntest_df.drop([\"id\"], axis=1, inplace=True)","880d90ce":"## Remove extra punctuations\ntrain_df[\"text\"].replace(\"[^a-zA-Z]\", \" \",regex = True, inplace = True)\ntest_df[\"text\"].replace(\"[^a-zA-Z]\", \" \",regex = True, inplace = True)\n\n## Convert Upper case to Lower case\ntrain_df[\"text\"] = train_df[\"text\"].str.lower()\ntest_df[\"text\"] = test_df[\"text\"].str.lower()\n\n## Remove stop words\nstop_words = stopwords.words('english')\n\ntrain_df['text'].apply(lambda x: [item for item in x if item not in stop_words])\ntest_df['text'].apply(lambda x: [item for item in x if item not in stop_words])\n","bd645cbb":"from wordcloud import WordCloud\n\nplt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(train_df[train_df.target == 0].text))\nplt.imshow(wc , interpolation = 'bilinear')","666fd32f":"from wordcloud import WordCloud\n\nplt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(train_df[train_df.target == 1].text))\nplt.imshow(wc , interpolation = 'bilinear')","9b926958":"X = train_df.drop(['target'],axis = 1)\nY = train_df['target']\n\nX[\"sentence\"] = X['keyword'] + \" \" + X['text']\nXtrain = np.array(X[\"sentence\"])\n\ntest_df[\"sentence\"] = test_df['keyword'] + \" \" + test_df['text']\nXtest = np.array(test_df[\"sentence\"])","b3f41da2":"from sklearn.feature_extraction.text import CountVectorizer,  TfidfVectorizer, HashingVectorizer\n\nvectorizer = TfidfVectorizer()\nvectorizer.fit_transform(Xtrain)\n\nkeyword = vectorizer.get_feature_names()\nx_train = vectorizer.transform(Xtrain)\nx_test = vectorizer.transform(Xtest)","66574939":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import ComplementNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\n\nmodel1 = LogisticRegression()\nmodel2 = ComplementNB()\nmodel3 = SVC()","a9d0cd0f":"final_model = VotingClassifier(estimators=[('lOG', model1), ('NB', model2), ('SVC',model3)], voting='hard')\n\nfinal_model.fit(x_train, Y)","4f244b7e":"pred = final_model.predict(x_test)\n\n## filling submission.csv\n\nsubmission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\n\nsubmission[\"target\"] = pred\nsubmission.to_csv(\"submission.csv\", index=False)\n\nsubmission.head()","227e7be9":"## Importing Dependencies","ab936f41":"## Dataset Preprocessing & Cleaning","b2f4b6c6":"## Voting Ensemble\n1. LogisticRegression\n2. ComplementNB\n3. SVC","6bc94f9c":"## Data Vectorizer","0f7d1d40":"## Converting data to numpy array","ca0d9f03":"We can see that 2533 are missing which is huge amount which will make huge prediction errors, so we can drop location column\nAnd fill keyword column with Most Frequent Value","1c457306":"****It is clean now. Now let's see some word cloud visualizations of it.****","15ce22cc":"## Model Prediction","2af424da":"## Text Preprocessing\n1. Remove extra punctuations\n2. Convert Upper case to Lower case\n3. Remove StopWords"}}