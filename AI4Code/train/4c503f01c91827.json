{"cell_type":{"e7b7929b":"code","7a8e306f":"code","0f4ed011":"code","a4eeafb4":"code","3840040c":"code","5242a8c8":"code","6ab58a3c":"code","fdc702ba":"code","4ba9e2cc":"code","4021a869":"code","f3c83bdc":"code","d9ff2234":"code","a2a49509":"code","5aded100":"code","4c3d935d":"code","8282a398":"code","338557cb":"code","181c2635":"code","bfa21899":"code","c8850920":"code","06ed056a":"code","c98a6321":"code","aef10a78":"code","e315bd14":"code","1c048f5f":"code","d81356b9":"code","a10f6ea0":"code","dab7cbc9":"code","33da19fd":"code","d5242d4f":"code","51bc84fc":"code","d44406a0":"code","6a8af4e3":"code","dca3f01d":"code","ee248705":"code","f9f5144c":"code","e70b6c27":"code","805ea43c":"code","b267ac49":"code","153bade2":"code","225f1809":"code","5a0b2b9d":"code","282628d3":"code","0e91fe0f":"code","0d3859fa":"code","142c0c28":"code","e26a4b34":"code","84a19999":"code","d32fef35":"code","3bbbaa0a":"code","45e30e03":"code","fd9e05ae":"code","a90fd113":"code","1b39e691":"code","ad8e5a00":"code","c14f8211":"code","31f7491b":"code","479ec8d2":"markdown","5aa49fb8":"markdown","17a1f4e6":"markdown","cc87322c":"markdown","fb34f0ff":"markdown","b1a5fad8":"markdown","81fd20f2":"markdown","b0d075bf":"markdown","9512d33c":"markdown","f7fd62da":"markdown","6f9f866d":"markdown","88122cc4":"markdown","4fe0f2f2":"markdown","4cd5ceba":"markdown","faefda25":"markdown","2b2a7961":"markdown","cf3df7f1":"markdown","3e24b4a8":"markdown","3b5cd5d6":"markdown","3a56456f":"markdown","0f6ce799":"markdown","784f5db3":"markdown","cf1ebd76":"markdown","796086d3":"markdown","d3e138cb":"markdown","5ac9befa":"markdown","d8595f6c":"markdown","2086c199":"markdown","44647353":"markdown","4ca0ccb2":"markdown"},"source":{"e7b7929b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy as sp\nimport warnings\nimport os \nwarnings.filterwarnings(\"ignore\")\nimport datetime\n","7a8e306f":"data=pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')\n","0f4ed011":"data.head()      #displaying the head of dataset","a4eeafb4":"data.describe()      #description of dataset ","3840040c":"data.info()","5242a8c8":"data.shape       #569 rows and 33 columns","6ab58a3c":"data.columns     #displaying the columns of dataset","fdc702ba":"data.value_counts","4ba9e2cc":"data.dtypes","4021a869":"data.isnull().sum()","f3c83bdc":"data.drop('Unnamed: 32', axis = 1, inplace = True)\n","d9ff2234":"data","a2a49509":"data.corr()","5aded100":"plt.figure(figsize=(18,9))\nsns.heatmap(data.corr(),annot = True, cmap =\"Accent_r\")\n\n\n\n","4c3d935d":"sns.barplot(x=\"id\", y=\"diagnosis\",data=data[160:190])\nplt.title(\"Id vs Diagnosis\",fontsize=15)\nplt.xlabel(\"Id\")\nplt.ylabel(\"Diagonis\")\nplt.show()\nplt.style.use(\"ggplot\")\n","8282a398":"sns.barplot(x=\"radius_mean\", y=\"texture_mean\", data=data[170:180])\nplt.title(\"Radius Mean vs Texture Mean\",fontsize=15)\nplt.xlabel(\"Radius Mean\")\nplt.ylabel(\"Texture Mean\")\nplt.show()\nplt.style.use(\"ggplot\")\n","338557cb":" \nmean_col = ['diagnosis','radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']\n\nsns.pairplot(data[mean_col],hue = 'diagnosis', palette='Accent')\n","181c2635":"sns.violinplot(x=\"smoothness_mean\",y=\"perimeter_mean\",data=data)","bfa21899":"plt.figure(figsize=(14,7))\nsns.lineplot(x = \"concavity_mean\",y = \"concave points_mean\",data = data[0:400], color='green')\nplt.title(\"Concavity Mean vs Concave Mean\")\nplt.xlabel(\"Concavity Mean\")\nplt.ylabel(\"Concave Points\")\nplt.show()\n\n","c8850920":"worst_col = ['diagnosis','radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']\n\nsns.pairplot(data[worst_col],hue = 'diagnosis', palette=\"CMRmap\")","06ed056a":"# Getting Features\n\nx = data.drop(columns = 'diagnosis')\n\n# Getting Predicting Value\ny = data['diagnosis']\n","c98a6321":"\n#train_test_splitting of the dataset\nfrom sklearn.model_selection import train_test_split \nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=0)\n","aef10a78":"print(len(x_train))\n","e315bd14":"print(len(x_test))","1c048f5f":"print(len(y_train))","d81356b9":"print(len(y_test))","a10f6ea0":"from sklearn.linear_model import LogisticRegression\nreg = LogisticRegression()\nreg.fit(x_train,y_train)                         \n","dab7cbc9":"y_pred=reg.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",reg.score(x_train,y_train)*100)\n\n\n","33da19fd":"data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata\n\n\n\n\n","d5242d4f":"print(accuracy_score(y_test,y_pred)*100)","51bc84fc":"from sklearn.model_selection import GridSearchCV\nparam = {\n         'penalty':['l1','l2'],\n         'C':[0.001, 0.01, 0.1, 1, 10, 20,100, 1000]\n}\nlr= LogisticRegression(penalty='l1')\ncv=GridSearchCV(reg,param,cv=5,n_jobs=-1)\ncv.fit(x_train,y_train)\ncv.predict(x_test)\n","d44406a0":"print(\"Best CV score\", cv.best_score_*100)","6a8af4e3":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier(max_depth=6, random_state=123)\n\ndtree.fit(x_train,y_train)\n\n#y_pred = dtree.predict(x_test)\n","dca3f01d":"y_pred=dtree.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",dtree.score(x_train,y_train)*100)\n\n","ee248705":"print(accuracy_score(y_test,y_pred)*100)","f9f5144c":"from sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier()\nrfc.fit(x_train,y_train)\n\n","e70b6c27":"y_pred=rfc.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",rfc.score(x_train,y_train)*100)\n","805ea43c":"print(accuracy_score(y_test,y_pred)*100)","b267ac49":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=7)\n\nknn.fit(x_train,y_train)\n","153bade2":"y_pred=knn.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",knn.score(x_train,y_train)*100)\nprint(knn.score(x_test,y_test))\n","225f1809":"print(accuracy_score(y_test,y_pred)*100)\n","5a0b2b9d":"from sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\n","282628d3":"y_pred=svc.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",svc.score(x_train,y_train)*100)\nprint(svc.score(x_test,y_test))\n","0e91fe0f":"print(\"Training Score: \",svc.score(x_train,y_train)*100)","0d3859fa":"from sklearn.ensemble import AdaBoostClassifier\nadb = AdaBoostClassifier(base_estimator = None)\nadb.fit(x_train,y_train)\n\n\n\n\n\n","142c0c28":"y_pred=adb.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",adb.score(x_train,y_train)*100)","e26a4b34":"print(accuracy_score(y_test,y_pred)*100)","84a19999":"from sklearn.ensemble import GradientBoostingClassifier\ngbc=GradientBoostingClassifier()\ngbc.fit(x_train,y_train)\n","d32fef35":"y_pred=gbc.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",gbc.score(x_train,y_train)*100)\nprint(gbc.score(x_test,y_test))\n","3bbbaa0a":"print(accuracy_score(y_test,y_pred)*100)","45e30e03":"from xgboost import XGBClassifier\n\nxgb =XGBClassifier(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 10)\n\nxgb.fit(x_train, y_train)\n","fd9e05ae":"y_pred=xgb.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",xgb.score(x_train,y_train)*100)\nprint(xgb.score(x_test,y_test))\n","a90fd113":"print(\"Training Score: \",xgb.score(x_train,y_train)*100)","1b39e691":"data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata","ad8e5a00":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(x_train,y_train)","c14f8211":"y_pred=gnb.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(accuracy_score(y_test,y_pred))\nprint(\"Training Score: \",gnb.score(x_train,y_train)*100)\nprint(gnb.score(x_test,y_test))\n","31f7491b":"data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata","479ec8d2":"**Ada Boost Classifier got the highest accuracy**","5aa49fb8":"# Breast Cancer Wisconsin (Diagnostic) Data Set\n","17a1f4e6":"**So we get a accuracy score of 96.49 % using Random Forest Classifier**","cc87322c":"**So now we conclude the accuracy of different models:**\n\n**1. AdaBoost Classifier = 98.24 %**\n\n**2. XGB Classifier= 97.84 %**\n\n**3. Random Forest Classifier =96.57 %**\n\n**4. Gradient Boosting Classifier= 95.66%**\n\n**5. Decision Tree Classifier= 94.78 %**\n\n**6. K Neighbours Classifier= 70.18 %**\n\n**7. SVC = 63.80 %**\n\n**8. Naiye Bayes= 63.30 %**\n\n**9. Logistic Regression = 58.82%**\n","fb34f0ff":"# MODELS","b1a5fad8":"**So we get a accuracy score of 58.7 % using logistic regression**","81fd20f2":"What Are the Symptoms of Breast Cancer?\n\nNew lump in the breast or underarm (armpit).\n\nThickening or swelling of part of the breast.\n\nIrritation or dimpling of breast skin.\n\n\nRedness or flaky skin in the nipple area or the breast.\n\nPulling in of the nipple or pain in the nipple area.\n\nNipple discharge other than breast milk, including blood.\n","b0d075bf":"# 5. SVC","9512d33c":"**So we get a accuracy score of 70.17 % using KNeighborsClassifier**","f7fd62da":"# 1. Logistic Regression","6f9f866d":"**So we get a accuracy score of 95.61 % using GradientBoostingClassifier**","88122cc4":"**So we get a accuracy score of 94.73 % using Decision Tree Classifier**","4fe0f2f2":"# 3. Random Forest Classifier","4cd5ceba":"# VISUALIZING THE DATA","faefda25":"# TRAINING AND TESTING DATA","2b2a7961":"# IMPORTING THE LIBRARIES","cf3df7f1":"**So we get a accuracy score of 63.7 % using SVC**","3e24b4a8":"**So we get a accuracy score of 63.29 % using Naive Bayes**","3b5cd5d6":"**So we get a accuracy score of 98.24 % using AdaBoostClassifier**","3a56456f":"**So we get a accuracy score of 97.80 % using  XGBClassifier**","0f6ce799":"#  7. Gradient Boosting Classifier","784f5db3":"# 2. DECISION TREE CLASSIFIER","cf1ebd76":"**Task : To predict whether the cancer is benign or malignant**","796086d3":"# If you liked this notebook, please UPVOTE it.","d3e138cb":"# 6. AdaBoostClassifier","5ac9befa":"# 9. Naive Bayes","d8595f6c":"# LOADING THE DATASET","2086c199":"# 4. KNeighborsClassifier\n\n","44647353":"# 8. XGBClassifier","4ca0ccb2":"**So we have to drop the Unnamed: 32 coulumn which contains NaN values**"}}