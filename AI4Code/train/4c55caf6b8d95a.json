{"cell_type":{"a14f501b":"code","a3a7995b":"code","09fcb159":"code","c1ce4f61":"code","1839fc2a":"code","e3a20dd5":"code","23e77920":"code","b7397893":"code","4c9db234":"code","64ac68b6":"code","2b11e9b9":"code","2059d8ac":"code","8cf0ae46":"code","0ba0bc95":"code","dc14d879":"code","e9ee0254":"code","50cab0e3":"code","fe3982a0":"code","5a095e80":"code","92ef6ccb":"code","84ee0a8b":"code","16aa160e":"code","8844e8c2":"code","43d2a9eb":"code","13c5baab":"code","e536cf3e":"code","3a7a3f8b":"code","7f238652":"code","66ebebda":"code","61f22d9a":"code","437a22e5":"code","65ca9786":"code","13d191a2":"code","e9efdf40":"code","d372e064":"code","966308e8":"code","6704e040":"code","67d37e1e":"code","643a2902":"code","1d249ba5":"code","61668b7d":"code","45111704":"code","d6904de6":"code","64484ebf":"code","e9513d29":"code","94bf7f3e":"code","a2990728":"code","53b12299":"code","ac6b29ce":"code","5b5ae4e0":"markdown","cde6e518":"markdown","d08eb03e":"markdown","560732e2":"markdown","e9cbc2f7":"markdown","bac63aec":"markdown","16f0f1f6":"markdown","54f174ad":"markdown","37809b13":"markdown","a53bff16":"markdown","1b4ebbf5":"markdown","58a5a83e":"markdown","50413130":"markdown","115d2e13":"markdown","8b770d1c":"markdown","420bc21c":"markdown","371e8dc0":"markdown","b5a46bf2":"markdown","278fb284":"markdown","67faea85":"markdown","662e319b":"markdown","739a9d78":"markdown","5ff3ba78":"markdown","f9d0ac66":"markdown","fc0b6fbf":"markdown","a32587ca":"markdown","efc7cc58":"markdown","42c91f20":"markdown","21e8274c":"markdown"},"source":{"a14f501b":"import re\nimport sys\nimport os\n\nimport time\nimport datetime\n\nimport numpy as np\nimport pandas as pd\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.model_selection import train_test_split","a3a7995b":"import matplotlib.pyplot as plt\n%matplotlib inline\nfrom pylab import rcParams","09fcb159":"apps = pd.read_csv('..\/input\/google-play-store-apps\/googleplaystore.csv')","c1ce4f61":"apps.head()","1839fc2a":"apps.describe()","e3a20dd5":"apps.shape","23e77920":"#Setting options to display all rows and columns\n\npd.options.display.max_columns=None\npd.options.display.max_rows=None\npd.options.display.width=None","b7397893":"# missing data\ntotal = apps.isnull().sum().sort_values(ascending=False)\npercent = (apps.isnull().sum()\/apps.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(4)","4c9db234":"# The best way to fill missing values might be using the median instead of mean.\napps['Rating'] = apps['Rating'].fillna(apps['Rating'].median())","64ac68b6":"# Before filling null values we have to clean all non numerical values & unicode charachters \nreplaces = [u'\\u00AE', u'\\u2013', u'\\u00C3', u'\\u00E3', u'\\u00B3', '[', ']', \"'\"]\nfor i in replaces:\n    apps['Current Ver'] = apps['Current Ver'].astype(str).apply(lambda x : x.replace(i, ''))\n\nregex = [r'[-+|\/:\/;(_)@]', r'\\s+', r'[A-Za-z]+']\nfor j in regex:\n    apps['Current Ver'] = apps['Current Ver'].astype(str).apply(lambda x : re.sub(j, '0', x))\n\napps['Current Ver'] = apps['Current Ver'].astype(str).apply(lambda x : x.replace('.', ',',1).replace('.', '').replace(',', '.',1)).astype(float)\napps['Current Ver'] = apps['Current Ver'].fillna(apps['Current Ver'].median())","2b11e9b9":"# Unwanted record of caetgory - which is 1.9\napps.Category.unique()","2059d8ac":"# Check the record  of unreasonable value which is 1.9\ni = apps[apps['Category'] == '1.9'].index\napps.loc[i]","8cf0ae46":"# Drop this bad column\napps = apps.drop(i)","0ba0bc95":"# Removing NaN values\napps = apps[pd.notnull(apps['Last Updated'])]\napps = apps[pd.notnull(apps['Content Rating'])]","dc14d879":"# Contribution in null value by Cuurent Ver, Android ver, Content Rating & Type is hardly make .5% so it's better to drop null values of them. \napps.dropna(how='any', inplace=True)","e9ee0254":"# Changed dimension:\napps.shape","50cab0e3":"apps = apps[apps['Content Rating'] != 'Unrated']","fe3982a0":"apps['Content Rating'].unique()","5a095e80":"# Content rating features encoding\ncontent_list = apps['Content Rating'].unique().tolist() \ncontent_list = ['con_' + word for word in content_list]\napps = pd.concat([apps, pd.get_dummies(apps['Content Rating'], prefix='con')], axis=1)","92ef6ccb":"apps['Price'] = apps['Price'].str.replace('$',\"\")\napps['Price'] = apps['Price'].apply(lambda x: float(x))","84ee0a8b":"# App values encoding\nle = preprocessing.LabelEncoder()\napps['App'] = le.fit_transform(apps['App'])\n# This encoder converts the values into numeric values","16aa160e":"apps.head()","8844e8c2":"# Category features encoding\ncategory_list = apps['Category'].unique().tolist() \ncategory_list = ['cat_' + word for word in category_list]\napps = pd.concat([apps, pd.get_dummies(apps['Category'], prefix='cat')], axis=1)\n","43d2a9eb":"apps['Genres'] = apps['Genres'].str.split(';').str[0]","13c5baab":"apps['Genres'].value_counts().tail()","e536cf3e":"apps['Genres'].replace('Music & Audio', 'Music',inplace = True)","3a7a3f8b":"apps['Genres'].value_counts().tail()","7f238652":"# Genres features encoding\nle = preprocessing.LabelEncoder()\napps['Genres'] = le.fit_transform(apps['Genres'])","66ebebda":"apps.head(2)","61f22d9a":"# Removing punchuations ',' & '+' sign from Intalls column:\napps.Installs = apps.Installs.apply(lambda x: x.replace(',',''))\napps.Installs = apps.Installs.apply(lambda x: x.replace('+',''))\napps.Installs = apps.Installs.apply(lambda x: int(x))","437a22e5":"apps.head(2)","65ca9786":"# Type encoding\napps['Type'] = pd.get_dummies(apps['Type'])","13d191a2":"# Convert kbytes to Mbytes \nk_indices = apps['Size'].loc[apps['Size'].str.contains('k')].index.tolist()\nconverter = pd.DataFrame(apps.loc[k_indices, 'Size'].apply(lambda x: x.strip('k')).astype(float).apply(lambda x: x \/ 1024).apply(lambda x: round(x, 3)).astype(str))\napps.loc[k_indices,'Size'] = converter","e9efdf40":"# Size cleaning\napps['Size'] = apps['Size'].apply(lambda x: x.strip('M'))\napps[apps['Size'] == 'Varies with device'] = 0\napps['Size'] = apps['Size'].astype(float)","d372e064":"apps.head()","966308e8":"# I'm just not considering this parameter for my model building","6704e040":"apps['Reviews'] = apps['Reviews'].apply(lambda x: float(x))","67d37e1e":"# Split data into training and testing sets\nfeatures = ['App', 'Reviews', 'Size', 'Installs', 'Type', 'Price', 'Genres', 'Current Ver']\nfeatures.extend(category_list)\nfeatures.extend(content_list)\nX = apps[features]\ny = apps['Rating']","643a2902":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 10)","1d249ba5":"# Look at the 10 closest neighbors\nmodel = KNeighborsRegressor(n_neighbors=10)","61668b7d":"# Find the mean accuracy of knn regression using X_test and y_test\nmodel.fit(X_train, y_train)","45111704":"# Calculate the mean accuracy of the KNN model\naccuracy = model.score(X_test,y_test)\n'Accuracy of this model is ' + str(np.round(accuracy*100, 2)) + '%'","d6904de6":"# We can try with different numbers of n_estimators or combination of various neighbors - this will take a minute or so\nn_neighbors = np.arange(1, 22, 1)\nscores = []\nfor n in n_neighbors:\n    model.set_params(n_neighbors=n)\n    model.fit(X_train, y_train)\n    scores.append(model.score(X_test, y_test))\nplt.figure(figsize=(8, 6))\nplt.title(\"Effect of Estimators\")\nplt.xlabel(\"Number of Neighbors K\")\nplt.ylabel(\"Score\")\nplt.plot(n_neighbors, scores)","64484ebf":"model = RandomForestRegressor(n_jobs=-1)\n# Try different numbers of n_estimators - this will take a minute or so\nestimators = np.arange(10, 200, 10)\nscores = []\nfor n in estimators:\n    model.set_params(n_estimators=n)\n    model.fit(X_train, y_train)\n    scores.append(model.score(X_test, y_test))\nplt.figure(figsize=(7, 5))\nplt.title(\"Effect of Estimators\")\nplt.xlabel(\"no. estimator\")\nplt.ylabel(\"score\")\nplt.plot(estimators, scores)\nresults = list(zip(estimators,scores))\nresults\n","e9513d29":"predictions = model.predict(X_test)\n'Mean Absolute Error:', metrics.mean_absolute_error(y_test, predictions)","94bf7f3e":"'Mean Squared Error:', metrics.mean_squared_error(y_test, predictions)","a2990728":"'Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, predictions))","53b12299":"from sklearn.metrics import accuracy_score","ac6b29ce":"model3 = XGBRegressor(n_jobs=-1)\n# Try different numbers of n_estimators - this will take a minute or so\nestimators = np.arange(10, 50, 2)\nscores = []\nfor n in estimators:\n    model3.set_params(n_estimators=n)\n    model3.fit(X_train, y_train)\n    scores.append(model3.score(X_test, y_test))\nplt.figure(figsize=(7, 5))\nplt.title(\"Effect of Estimators\")\nplt.xlabel(\"no. estimator\")\nplt.ylabel(\"score\")\nplt.plot(estimators, scores)\nresults = list(zip(estimators,scores))\nresults","5b5ae4e0":"## Last Updated:","cde6e518":"The k-nearest neighbors algorithm is based around the simple idea of predicting unknown values by matching them with the most similar known values. Building the model consists only of storing the training dataset. To make a prediction for a new data point, the algorithm finds the closest data points in the training dataset \u2014 its \"nearest neighbors\".","d08eb03e":"Unrated rating has on 1 application so its better to drop that category for future simplification of calculation & model building:","560732e2":"The RandomForestRegressor class of the sklearn.ensemble library is used to solve regression problems via random forest. The most important parameter of the RandomForestRegressor class is the n_estimators parameter. This parameter defines the number of trees in the random forest.","e9cbc2f7":"# KNN Model: K-Nearest Neighbors Model","bac63aec":"# Thank You","16f0f1f6":"![image.png](attachment:image.png)","54f174ad":"This can be done by selecting all k values from the \"Size\" column and replace those values by their corresponding M values, and since k indices belong to a list of non-consecutive numbers, a new dataframe (converter) will be created with these k indices to perform the conversion, then the final values will be assigned back to the \"Size\" column.","37809b13":"## Reviews","a53bff16":"# Data Analysis on Google Playstore Apps - Model Building ","1b4ebbf5":"# XGBoost Algorithm","58a5a83e":"### $ For-EDA-   part - please-  refer-- to-  my --- $'Analysis-of- Google- PlayStore-Apps-EDA' script $","50413130":"## Content Rating","115d2e13":"Price is actually a numeric column and present in Object type, remove '$'sign and ',' from the character.","8b770d1c":"## Genres","420bc21c":"## Category:","371e8dc0":"## Installs:","b5a46bf2":"## After getting done with 3 models we've concluded that all above these 3 models come up with the highest accuracy of:\n### KNN - 91.80\n### Random Forest - 92.67\n### XGboost - 92.64\n## So, we can see that, when you're building your model on such type of target variable wherein you've multiple classes, we should go with the regressor models to get efficient amount of accuracy. furthermore, you can see over here with all the 3 models, wherein all of them has its outstanding capability of handling large chunk of data, with multiple iterations and random sample (using bagging and boosting), nearest neighbor & avoiding overfitting\/biasness, gives us an decent result to come up with the conclusion that we've successfully predicted our $Rating Attribute$ with random forest (for a consideration), along with low MSE- 19% & MAE- 25%. This is because we've not included our one attribute 'Last Updated Date' in our model building part.\n","278fb284":"## Apps","67faea85":"## Size:","662e319b":"#### Thank you \" A. - kaggle expert\" for your reference kernel","739a9d78":"## Price","5ff3ba78":"The above line drops the reference column and just keeps only one of the two columns as retaining this extra column does not add any new information for the modeling process, this line is exactly the same as setting drop_first parameter to True","f9d0ac66":"# $Model Building:$___________________________","fc0b6fbf":"\n\nIn this section shows how k-nearest neighbors and random forests can be used to predict app ratings based on the other matrices. First, the dataset has to separate into dependent and independent variables (or features and labels). Then those variables have to split into a training and test set.\n\nDuring training stage we give the model both the features and the labels so it can learn to classify points based on the features.\n","a32587ca":"XGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. In prediction problems involving unstructured data (images, text, etc.) artificial neural networks tend to outperform all other algorithms or frameworks. However, when it comes to small-to-medium structured\/tabular data, decision tree based algorithms are considered best-in-class right now. Please see the chart below for the evolution of tree-based algorithms over the years.","efc7cc58":"# Random Forest Model","42c91f20":"### The above script splits the dataset into 70% train data and 30% test data","21e8274c":"## Type:"}}