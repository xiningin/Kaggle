{"cell_type":{"a5777cce":"code","b2887c77":"code","b6f39024":"code","75d6af97":"code","0ede0b79":"code","a4b8fa48":"code","83f64f56":"code","13ce5417":"code","13f2fe96":"code","8980cca6":"code","8dd4d276":"code","8b2b617a":"code","d58973c3":"code","0b10bffe":"code","0c3d3006":"code","2cbbe23e":"code","d53b9ab8":"code","d71da1c1":"code","a7989c27":"code","0d10ed40":"code","c22303b2":"code","c3535c34":"code","d1c9f61f":"markdown","06f0f01d":"markdown","fe745508":"markdown","ad140f32":"markdown","8b595457":"markdown","5a83225e":"markdown","9c4fd7b0":"markdown","06bab0a8":"markdown","a0214f89":"markdown","add4088e":"markdown","d51dd4c6":"markdown","09307185":"markdown","c698580e":"markdown","25ab331f":"markdown","fc49f1c5":"markdown","a07ee656":"markdown","26a418ad":"markdown","7cb9ecf2":"markdown"},"source":{"a5777cce":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b2887c77":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport re  # Regular expression operations\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords  # for stopword\nfrom nltk.stem.porter import PorterStemmer  # for steming\n\n#The sklearn.feature_extraction module can be used to extract features in a format supported by machine learning algorithms from datasets consisting of formats such as text and image.\n\nfrom sklearn.feature_extraction.text import CountVectorizer","b6f39024":"\ndata = pd.read_csv('..\/input\/restaurant-reviewstsv\/Restaurant_Reviews.tsv', delimiter= '\\t', quoting=3 ) # quoting=3 is for ignor al double quoting","75d6af97":"data.head()","0ede0b79":"# make a clean corpus. remove punctuation and stop word. steming then join with space\ncorpus =[] \nfor i in range(0,len(data)):\n\n  review = re.sub('[^a-zA-Z]',' ', data['Review'][i])  # for any replace in string, here we replace punctuation by space. ^a-zA-Z means any thing except a-z or A-Z\n  review = review.lower() # make all lower case\n\n  review = review.split ()  # split every word\n\n  ps = PorterStemmer() # steming object  # about steming https:\/\/towardsdatascience.com\/stemming-corpus-with-nltk-7a6a6d02d3e5\n  all_stopwords = stopwords.words('english') \n  all_stopwords.remove('not')  ## all stop word except 'not' \n  \n  review = [ps.stem(word) for word in review if not word in set(all_stopwords) ]  # steming of word and remove the stop words \n  review = ' '.join(review)\n  corpus.append(review)\n\n\n\n","a4b8fa48":"corpus[:5]\n","83f64f56":"# create countvectorizer \n\ncv = CountVectorizer(max_features= 1500)  # max_features is how many frequent word that stay in word of bag\n\n#independent variable fro analysis \n\n# make a bag of word where every word has numeric position\nx = cv.fit_transform(corpus).toarray()  # need 2D array\n\n# output variable\ny = data['Liked'].values  #need 2D array","13ce5417":"len(x[0])","13f2fe96":"from sklearn.model_selection import train_test_split\n\nx_train,x_test, y_train,y_test = train_test_split(x,y, test_size= .2)","8980cca6":"from sklearn.naive_bayes import GaussianNB\n\nclassifier = GaussianNB()\n\nclassifier.fit(x_train,y_train)\n\n","8dd4d276":"y_pre = classifier.predict(x_test)","8b2b617a":"from sklearn.metrics import confusion_matrix, accuracy_score\n\ncm= confusion_matrix (y_test,y_pre)\nscore = accuracy_score (y_test,y_pre)\n\nprint(cm)\nprint(score)","d58973c3":"new_review = 'I  love this restaurant so much'\nnew_review = re.sub('[^a-zA-Z]', ' ', new_review)\nnew_review = new_review.lower()\nnew_review = new_review.split()\nps = PorterStemmer()\nall_stopwords = stopwords.words('english')\nall_stopwords.remove('not')\nnew_review = [ps.stem(word) for word in new_review if not word in set(all_stopwords)]\nnew_review = ' '.join(new_review)\nnew_corpus = [new_review]\nnew_X_test = cv.transform(new_corpus).toarray()\nnew_y_pred = classifier.predict(new_X_test)\nprint(new_y_pred)","0b10bffe":"new_review = 'I hate this restaurant so much'\nnew_review = re.sub('[^a-zA-Z]', ' ', new_review)\nnew_review = new_review.lower()\nnew_review = new_review.split()\nps = PorterStemmer()\nall_stopwords = stopwords.words('english')\nall_stopwords.remove('not')\nnew_review = [ps.stem(word) for word in new_review if not word in set(all_stopwords)]\nnew_review = ' '.join(new_review)\nnew_corpus = [new_review]\nnew_X_test = cv.transform(new_corpus).toarray()\nnew_y_pred = classifier.predict(new_X_test)\nprint(new_y_pred)","0c3d3006":"from   sklearn.linear_model import LogisticRegression\nfrom   sklearn.tree import DecisionTreeClassifier\nfrom   sklearn.ensemble import RandomForestClassifier\nfrom   sklearn.svm import SVC\nfrom   sklearn.neighbors import KNeighborsClassifier\nfrom   sklearn.linear_model import SGDClassifier","2cbbe23e":"def get_models():\n  models = dict()\n  #models['XGB'] = XGBClassifier()\n  #models['LGBM'] = LGBMClassifier()\n  models['logreg'] = LogisticRegression()\n  models['knn'] = KNeighborsClassifier()\n  models['cart']= DecisionTreeClassifier()\n  models['Rndf'] = RandomForestClassifier()\n  models['svm'] = SVC(probability=True)\n  models['bayes'] = GaussianNB()\n  models['SGD'] = SGDClassifier()\n\n  return models","d53b9ab8":"#xt means test data\ndef model_predict(x,y,model,xt):\n  model.fit(x,y)\n  Y_pre = model.predict(xt)\n  # Y_pred_prob = model.predict_proba(X_test_1)\n  return Y_pre\n","d71da1c1":"models = get_models()","a7989c27":"# Evalution library\nfrom sklearn.metrics import accuracy_score,confusion_matrix, classification_report\nimport sklearn.metrics","0d10ed40":"## Performance test for all model\nnames =[]\nExact_Match_Ratio=[]\nHamming_loss=[]\nRecall=[]\nPrecision =[]\nF1_Measure =[]\nScore = []\n\nfor name, model in models.items():\n  names.append(name)\n  Y_pre = model_predict(x_train,y_train,model,x_test)\n  \n  Score.append(accuracy_score(y_test,Y_pre) )\n\n  Exact_Match_Ratio.append(' {0}'.format(sklearn.metrics.accuracy_score(y_test, Y_pre, normalize=True, sample_weight=None)))\n\n  Hamming_loss.append(' {0}'.format(sklearn.metrics.hamming_loss(y_test, Y_pre))) \n  #\"samples\" applies only to multilabel problems. It does not calculate a per-class measure, instead calculating the metric over the true and predicted classes \n  #for each sample in the evaluation data, and returning their (sample_weight-weighted) average.\n\n  Recall.append('{0}'.format(sklearn.metrics.precision_score(y_true=y_test, y_pred=Y_pre, average='weighted'))) \n  \n  Precision.append('{0}'.format(sklearn.metrics.recall_score(y_true=y_test, y_pred=Y_pre, average='weighted')))\n  \n  F1_Measure.append('{0}'.format(sklearn.metrics.f1_score(y_true=y_test, y_pred=Y_pre, average='weighted'))) \n\n\nPerformance = pd.DataFrame(list(zip(names,Score, Exact_Match_Ratio,Hamming_loss,Recall,Precision,F1_Measure)), columns=['Algorithom','Accuracy','Exact Match Ratio','Hamming loss','Recall','Precision','F1 Measure'] )","c22303b2":"print (Performance)","c3535c34":"Performance.to_csv(\"Performance.csv\")\nos.listdir('\/kaggle\/')","d1c9f61f":"## Creating the Bag of Words model","06f0f01d":"### Positive review","fe745508":"## Importing the libraries","ad140f32":"Vectorization the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the Bag of Words or \u201cBag of n-grams\u201d representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document.\n\nCountVectorizer implements both tokenization and occurrence counting in a single class.\nText Analysis is a major application field for machine learning algorithms.\nHowever the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.","8b595457":"#Accurecy measure with all clssification model","5a83225e":"## Importing the dataset","9c4fd7b0":"## Training the Naive Bayes model on the Training set","06bab0a8":"## Splitting the dataset into the Training set and Test set","a0214f89":"# Sentiment Analysis \n","add4088e":"Analysis of restaurant reviews by natural language processing. \n","d51dd4c6":"## Predicting the Test set results","09307185":"## Making the Confusion Matrix","c698580e":"## Cleaning the texts","25ab331f":"### Negative review","fc49f1c5":"Use our model to predict if the following review:\n\n\"I love this restaurant so much\"\nis positive or negative.","a07ee656":"## Predicting if a single review is positive or negative","26a418ad":"Use our model to predict if the following review:\n\n\"I hate this restaurant so much\"\n\nis positive or negative.","7cb9ecf2":"##Performance test"}}