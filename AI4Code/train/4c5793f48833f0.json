{"cell_type":{"2f2f9b33":"code","d040ea8f":"code","1f110285":"code","4d136805":"code","ae85496d":"code","10dd3858":"code","6c47317c":"code","c71a13b3":"code","852d22e6":"code","d52b5f57":"code","00f9b9b8":"code","951867fa":"code","dbff5ff2":"code","4d53d220":"markdown","73a86436":"markdown","7c654ddc":"markdown","8f25b2a2":"markdown","032b1dca":"markdown","45f3bc34":"markdown","bfa149be":"markdown","1b7c9b62":"markdown","ef299178":"markdown","439a3c77":"markdown","0ccefdf9":"markdown","f1488367":"markdown","42b03609":"markdown","7843837f":"markdown"},"source":{"2f2f9b33":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nimport os\nimport sys\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport pywt\nfrom sklearn.preprocessing import StandardScaler","d040ea8f":"compute_features = False \n# The computed features are saved in an hdf file along with the time_to_failure to \n# save the time spend reading the training data and the feature computation\n\nif 'KAGGLE_URL_BASE' in os.environ:\n    # If we are in a kaggle kernel, read from the csv file.\n    train_data_format = 'csv'\nelse:\n    # This only work in my local setup. \n    # Loading the training dataset from the feather file is much faster than CSV\n    train_data_format = 'feather' \n","1f110285":"def load_train_data(file_format):\n    \"\"\"Load the training dataset.\"\"\"\n    print(f\"Loading data from {file_format} file:\", end=\"\")\n    if file_format.lower() == 'feather':\n        train_file_name = '..\/input\/train_4mhz.feather'\n        train_df = pd.read_feather(train_file_name)\n    else:\n        train_df = pd.read_csv('..\/input\/train.csv', \n                               dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})\n    print(\"Done\")\n    return train_df","4d136805":"sampling_frequency = 4e6 #4mhz\nsampling_period = 1.\/sampling_frequency\n\ndef generate_features(segment_signal):\n    \n    segment_signal = segment_signal - segment_signal.mean()\n    \n    features = pd.Series()\n    \n    windows = 10\n    signal_roll_std = segment_signal.rolling(windows).std().dropna().values\n    features['std_roll_std_10'] = signal_roll_std.std()\n    features['mean_roll_std_10'] = signal_roll_std.mean()\n    features['q05_roll_std_10'] = np.quantile(signal_roll_std, 0.05)\n    features['min_roll_std_10'] = signal_roll_std.min()\n    features['q95_roll_std_10'] = np.quantile(signal_roll_std, 0.95)           \n    \n    return features","ae85496d":"saved_files_present = ( os.path.isfile('..\/alternate_input\/x_train.hdf') and \n                        os.path.isfile('..\/alternate_input\/y_train.hdf') and \n                        os.path.isfile('..\/alternate_input\/earthquakes_id.hdf') )\n\nif (not compute_features) and saved_files_present:\n    print(\"Reading hdf files:\", end=\"\")\n    x_train = pd.read_hdf('..\/alternate_input\/x_train.hdf','data')\n    y_train = pd.read_hdf('..\/alternate_input\/y_train.hdf','data')\n    earthquakes_id = pd.read_hdf('..\/alternate_input\/earthquakes_id.hdf','data')  \n    print(\"Done\")\n    \nelse:    \n        \n    x_train = pd.DataFrame()\n    earthquakes_id = pd.Series()\n    y_train = pd.Series()\n    \n    train_df = load_train_data(train_data_format)    \n    chunksize = 150_000\n\n    segments = int(np.floor(train_df.shape[0] \/ chunksize))\n\n    current_quake_id = 0\n    last_time_to_failure = train_df.iloc[0]['time_to_failure']   \n    \n    print(\"Computing features:\", end=\"\")  \n    sys.stdout.flush()\n    for segment_number in tqdm(range(segments)):\n\n        segment_df = train_df.iloc[segment_number*chunksize:\n                                   segment_number*chunksize+chunksize]\n\n        times_to_failure = segment_df['time_to_failure'].values\n\n        # Ignore segments with an earthquake in it.\n        if np.abs(times_to_failure[0]-times_to_failure[-1])>1:\n            continue\n\n        y_train.loc[segment_number] = times_to_failure[-1]\n\n        features = generate_features(segment_df['acoustic_data'])\n        x_train=x_train.append(features, ignore_index=True)    \n\n        if np.abs(times_to_failure[-1]-last_time_to_failure)>1:\n            current_quake_id += 1\n\n        earthquakes_id.loc[segment_number] = current_quake_id\n        last_time_to_failure = times_to_failure[-1]\n    print(\"Done\")\n    \n    print(\"Saving features, earthquake ids, and time_to_failure to hdf files:\", end=\"\")      \n    if not os.path.isdir(\"..\/alternate_input\"):\n        os.makedirs(\"..\/alternate_input\")\n        \n    x_train.to_hdf(f'..\/alternate_input\/x_train.hdf','data')\n    y_train.to_hdf('..\/alternate_input\/y_train.hdf','data')\n    earthquakes_id.to_hdf('..\/alternate_input\/earthquakes_id.hdf','data')\n    print(\"Done\")\n    \n    del train_df","10dd3858":"np.unique(earthquakes_id)","6c47317c":"fig, ax1 = plt.subplots(figsize=(9,6))\nax1.plot(np.arange(y_train.size), y_train, color='r', label=\"Time to failure\")\nax1.set_ylabel(\"Time to failure\")\nax1.set_xlabel(\"Segment #\")\n\nax2 = ax1.twinx() \nax2.plot(np.arange(y_train.size), earthquakes_id, color='b', label=\"Earthquake #\")\nax2.set_ylabel(\"Earthquake #\")\nax1.legend(loc='center left', bbox_to_anchor=(0.05,0.95))\nax2.legend(loc='center', bbox_to_anchor=(0.7,0.95))\nplt.show()\n","c71a13b3":"correlation_coefficients = np.abs(x_train.corrwith(y_train)).sort_values(ascending=False)\n\nimport seaborn as sns\nsns.barplot(x=correlation_coefficients.values, y=correlation_coefficients.index)\nplt.title('Features correlation with quaketime')\nplt.tight_layout()","852d22e6":"from catboost import CatBoostRegressor\n\n# Fit the scaler for the data\nscaler = StandardScaler()\nscaler.fit(x_train)\n\ndef get_model():\n    model = CatBoostRegressor(iterations=2000, \n                              loss_function='MAE', \n                              boosting_type='Plain')\n    fit_kwargs = dict(silent=True)    \n    return model, fit_kwargs    ","d52b5f57":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import LeaveOneGroupOut\n\ngroup_kfold = LeaveOneGroupOut()\nfold_splitter = group_kfold.split(x_train, y_train, earthquakes_id)\n\ntrain_errors = list()\nvalidation_errors = list()\nfor group_out, (train_index, valid_index) in enumerate(fold_splitter):\n    if group_out in [0,16]:\n        continue\n        \n    print('Validation Earthquake:', group_out,end=\"   \")\n    \n    train_signal, validation_signal = x_train.iloc[train_index], x_train.iloc[valid_index]\n    train_quaketime, validation_quaketime = y_train.iloc[train_index], y_train.iloc[valid_index]\n    \n    train_signal = scaler.transform(train_signal)\n    validation_signal = scaler.transform(validation_signal)    \n            \n    selected_model , fit_kwargs = get_model()\n    selected_model.fit(train_signal, train_quaketime, **fit_kwargs)\n    \n    train_error = mean_absolute_error(selected_model.predict(train_signal), train_quaketime)\n    validation_error = mean_absolute_error(selected_model.predict(validation_signal), validation_quaketime)\n    print(f'train_error: {train_error:.2f}',end=\"  \")\n    print(f'validation_error:{validation_error:.2f}', )\n    \n    train_errors.append(train_error)\n    validation_errors.append(validation_error)\n    \nmean_train_error = np.asarray(train_errors).mean()\nmean_validation_error = np.asarray(validation_errors).mean()\n\nprint(f\"\\nMean train_error: {mean_train_error:.2f}\")\nprint(f\"Mean validation_errors: {mean_validation_error:.2f}\")","00f9b9b8":"# First we shuffle the data\nfrom sklearn.utils import shuffle\n\nshuffled_features = shuffle(x_train)\nshuffled_quaketime = y_train.iloc[shuffled_features.index]\n            \ncatboost_reg_model = CatBoostRegressor(iterations=1300, loss_function='MAE', boosting_type='Plain')\ncatboost_reg_model.fit(shuffled_features, shuffled_quaketime, silent=True)","951867fa":"from bokeh.plotting import figure, show\nfrom bokeh.io import output_notebook\noutput_notebook()\n\nquaketime_predict = catboost_reg_model.predict(x_train)\n\n# create a new plot with a title and axis labels\np = figure(title=\"Time to failure prediction\", x_axis_label='x', y_axis_label='y')\n\n# add a line renderer with legend and line thickness\np.line (np.arange(quaketime_predict.shape[0]), quaketime_predict, line_color='red')\n\np.line (np.arange(y_train.values.shape[0]), y_train, line_color='blue')\n\n# show the results\nshow(p)\n","dbff5ff2":"selected_model = catboost_reg_model\n\nsubmission = pd.read_csv('..\/input\/sample_submission.csv', index_col='seg_id')\nX_test = pd.DataFrame()\n\nfor seg_id in submission.index:\n    seg = pd.read_csv('..\/input\/test\/' + seg_id + '.csv')\n    ch = generate_features(seg['acoustic_data'])\n    X_test = X_test.append(ch, ignore_index=True)\n\nsubmission['time_to_failure'] = selected_model.predict(X_test).clip(0, 16)\nsubmission.to_csv('submission.csv')   ","4d53d220":"## Features correlations with Time_to_failure\n\nLets see how well the features are correlated with the time to failure.","73a86436":"The q05, q95 quantiles have the best correlation with the time to failure, suggesting that they are very important features.","7c654ddc":"## Train the model using all the available data","8f25b2a2":"### CV validation strategy\n\n\nFollowing the discussions in https:\/\/www.kaggle.com\/c\/LANL-Earthquake-Prediction\/discussion\/78809 we will use the LeaveOneGroupOut validation strategy.\n- Use one earthquake for validation\n- Use the rest for training","032b1dca":"# Submit prediction","45f3bc34":"## Test model performance using a KFold validation strategy using the earthquakes IDs","bfa149be":"## Feature generator","1b7c9b62":"## Load the data","ef299178":"## Generate features\n\nHere we will generate the features and also identify each segment with the corresponding earthquake that should be predicted by it. Each earthquake will be identified by an unique ID (earthquake #).\n\nIf **compute_features** is set to False, the features, time_to_failure, and the earthquake ids are loaded from the hdf files. Otherwise, they are computed and saved in hdf format for future use.","439a3c77":"# CV splitting by earthquake ID\n\nThis kernels implements a Cross Validation (CV) strategy that separates the segments in the training data by Earthquake ID.\n\nFor this, we first identify the earthquakes in the training dataset and we assign an unique ID to every earthquake (the earthquake #). Then we use the \"Leave One Group Out cross-validator\" to test a simple model. \n\nThis kernel follows many of the suggestions in [this discussion](https:\/\/www.kaggle.com\/c\/LANL-Earthquake-Prediction\/discussion\/78809) started by [Elliot](https:\/\/www.kaggle.com\/tclf90).  ","0ccefdf9":"## Global parameters\n\nTo save time and memory, this notebooks save the features, time_to_failure, and the earthquake ids in hdf files.\nHence, once the data was computed, it can be loaded from these files quickly.\nThe files are saved in the **alternate_input** directory.","f1488367":"In the previous plot, we see that the Earthquake 0 and 16 only contain a small number of segments. Hence, during the the CV we will only use those segment for training but not for validation.","42b03609":"### Define model","7843837f":"### Lets see how many earthquakes we have in the training set"}}