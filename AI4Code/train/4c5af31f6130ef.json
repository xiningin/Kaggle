{"cell_type":{"0c837d66":"code","1fe37424":"code","fad79872":"code","ae6d4b9f":"code","84e07e0c":"code","c54eb91b":"code","6d9b45d2":"code","af9ad2fc":"code","77bf1dd3":"code","61747d1f":"code","a0808b32":"code","0762bc08":"code","63856b96":"code","19948159":"code","3f10bbc3":"code","51fc8f1d":"code","0a21d51e":"code","b87465e4":"code","cbe11cf2":"code","aa2a590a":"code","e680c7a7":"code","fb18f6c9":"code","47d98f45":"code","70cc432d":"code","c864b55e":"code","ce9f3fc8":"code","b0580603":"code","7c56f02d":"code","1482b87f":"code","f9537a97":"code","e85f848a":"code","b25f0b89":"code","f958a45e":"code","48f05aab":"code","6b9154bd":"code","53fe75d2":"code","709de9b9":"code","ce6eaec2":"code","c792c56e":"code","0237be84":"code","7a11d69d":"code","53853697":"code","4014b315":"code","012bff92":"code","ce482448":"code","5eb4d452":"code","588f3cbb":"code","fd0cca6a":"code","3e7d8d75":"code","5271a3a0":"code","a53d0ad4":"code","edee4b6a":"code","7e8f4431":"code","25a9c111":"code","1ce2678c":"code","9d8af93c":"code","63605a3e":"code","f119c32c":"markdown","1afdd26d":"markdown","5860f4e0":"markdown"},"source":{"0c837d66":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1fe37424":"import networkx as nx\nimport sklearn\nfrom hmmlearn import hmm\nimport gensim\nimport nltk\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer, porter\n#from nltk.stem import PorterStemmer.ORIGINAL_ALGORITHM as PSOA\nfrom nltk.stem.porter import *\nfrom scipy.cluster.vq import vq, kmeans, whiten\nfrom nltk import StemmerI","fad79872":"from nltk.stem import PorterStemmer","ae6d4b9f":"porter_stemmer = PorterStemmer.NLTK_EXTENSIONS","84e07e0c":"dir(nltk)","c54eb91b":"np.random.seed(42)","6d9b45d2":"DJIA = pd.read_csv('..\/input\/stocknews\/upload_DJIA_table.csv')\ncombined_DJIA = pd.read_csv('..\/input\/stocknews\/Combined_News_DJIA.csv')\nReddit_News = pd.read_csv('..\/input\/stocknews\/RedditNews.csv')","af9ad2fc":"#Intuitively, a document titlet combined_DJIA would have a relationship to the DJIA - oddly, I am not seeing it here.","77bf1dd3":"Reddit_News.columns","61747d1f":"R_text = Reddit_News[['News']]\nR_text['index'] = R_text.index\nR_docs = R_text","a0808b32":"combined_DJIA.columns\n#There is clearly some sort of advanced technique involved wherein I can weight each next level of news story\n#for now I can mash them all into one big word salad\/date","0762bc08":"R_text.columns","63856b96":"combined_DJIA['All News'] = combined_DJIA['Top1'] + combined_DJIA['Top2'] + combined_DJIA['Top3'] + combined_DJIA['Top4']\ncombined_DJIA['All News'] + combined_DJIA['Top5'] + combined_DJIA['Top6'] + combined_DJIA['Top7'] + combined_DJIA['Top8']\ncombined_DJIA['All News'] + combined_DJIA['Top9'] + combined_DJIA['Top10'] + combined_DJIA['Top11'] + combined_DJIA['Top12']\ncombined_DJIA['All News'] + combined_DJIA['Top13'] + combined_DJIA['Top14'] + combined_DJIA['Top15'] + combined_DJIA['Top16']\ncombined_DJIA['All News'] + combined_DJIA['Top17'] + combined_DJIA['Top18'] + combined_DJIA['Top19'] + combined_DJIA['Top20']\ncombined_DJIA['All News'] + combined_DJIA['Top21'] + combined_DJIA['Top22'] + combined_DJIA['Top23'] + combined_DJIA['Top24'] \n+ combined_DJIA['Top25']\n#for simplicity sake, I am collapsing all news into one single column per day","19948159":"#How can I rate these different levels of information - this should be useful for nexttime\n","3f10bbc3":"%pip install nltk","51fc8f1d":"print(combined_DJIA['All News'].head())","0a21d51e":"text = str(combined_DJIA['All News'])","b87465e4":"# here I clean the data to make them all lower case - a little unclear because proper nouns can be extremely informative\n#\n#\n#\n#","cbe11cf2":"token_words = [ ]","aa2a590a":"from nltk.tokenize import sent_tokenize, word_tokenize\ndef stemSentence(text):\n    token_words = word_tokenize(text)\n    token_words\n    print(token_words)","e680c7a7":"stemSentence(text)","fb18f6c9":"stem_sentence = []","47d98f45":"for word in token_words:\n        stem_sentence.append(porter_stemmer(word))\n        stem_sentence.append(\" \")\n        \"\".join(stem_sentence)\n\nx=stemSentence(text)\nprint(x)","70cc432d":"def lemmatize_stemming(text):\n    return((WordNetLemmatizer().lemmatize(text, pos='v')))\n#lemmatizing_stemmers are useful, in general, however, in this case, common sense would suggest words like: Israel, United States,\n#President, Minister, impeach, genocide, war, ozone, European Union, petroleum, etc.. would be the words I am looking \n#for..","c864b55e":"def preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocessing(text):\n        if token in gensim.preprocessing.STOPWORDS() and len(token)> 3:\n            result.append(lemmatize_steming(token))\n        return(result)","ce9f3fc8":"doc_sample = combined_DJIA['All News'] \n#Assign as a variable so as *not* to change the original\n\nprint('original document: ')","b0580603":"words = []\nfor word in doc_sample:\n   #words.append(word)\n   print(words)\n   #print('\\n\\n tokenized and lemmatized document: ')\n   print(preprocess(doc_sample))","7c56f02d":"#lemmatize_stemming(doc_sample)","1482b87f":"preprocessed_docs = DJIA['All News'].map(preprocess)\npreprocessed_docs[:10]","f9537a97":"dictionary = gensim.corpora.Dictionary(processed_docs)","e85f848a":"count = 0\nfor k,v in dictionary.iteritems():\n    print(k, v)\n    count += 1\n    if count > 10:\n        break","b25f0b89":"dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)","f958a45e":"bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\nbow_corpus[4310]","48f05aab":"#Keep it very simple, since this is first base\nnum_topics = 3\nchunksize = 100 \npasses = 10 \niterations = 10\neval_every = 1  \n\n# Make a index to word dictionary.\ntemp = dictionary[0]  # This is only to \"load\" the dictionary.\nid2word = dictionary.id2token\n\nmodel = LdaModel(corpus=bow_corpus, id2word=id2word, chunksize=chunksize, \n                       alpha='auto', eta='auto', \n                       iterations=iterations, num_topics=num_topics, \n                       passes=passes, eval_every=eval_every)","6b9154bd":"#I will test this by simply seeing, cutting off the last 500 lines, and treating it as an out-of-sample test ","53fe75d2":"#DJIA.head()","709de9b9":"#The way that someone else did this online was simply to just apply an HMM to 40 days and then use it to prodict the future","ce6eaec2":"open_price = np.array(DJIA['Open'])\nclose_price= np.array(DJIA['Close'])\nhigh_price= np.array(DJIA['High'])\nlow_price= np.array(DJIA['Low'])","c792c56e":"frac_change = np.array((close_price-open_price)\/open_price)\n","0237be84":"frac_high = np.array((high_price-open_price)\/open_price)","7a11d69d":"frac_low = np.array((open_price-low_price)\/open_price)","53853697":"#So far, my model is that there is one node for the entre stock market index and each node goes to one of: very high, small high, no change, small low, very low\n#The guy who did his senior thesis on this said the best fit was at 40 days, so I'll have a 5X40 markov chain\n# I start with just one stock, it should be easy to upgrade complexity from there","4014b315":"DJIA['frac_change'] = frac_change\nDJIA['frac_high'] = frac_high\nDJIA['frac_low'] = frac_low","012bff92":"#DJIA['frac_change'].max(),\n#DJIA['frac_change'].min()","ce482448":"#DJIA['normalized_change'] = DJIA['frac_change']\/(DJIA['frac_high']-DJIA['frac_low'])\n#I got something wrong with this equation and it actually exacerbates changes","5eb4d452":"#Vector Quantizer here: Separate it into 5 distinct states\ncode_book = np.array([-2, -1, 0, 1, 2])","588f3cbb":"DJIA['vectorized_change'] == vq(DJIA['frac_change'], code_book)","fd0cca6a":"print(vectorized_changes)\n#This does not look very good yet... I must tinker","3e7d8d75":"#DJIA['normalized_change'].max()\n#DJIA['normalized_change'].min()\n#They don't seem normalized at all","5271a3a0":"#print(np.column_stack((frac_change, frac_high, frac_low)))","a53d0ad4":"\n#It seems like he solves this problem by extracting the columns into individual arrays first...\n\n#","edee4b6a":"model = hmm.GaussianHMM(n_components=5, n_iter=40, covariance_type='tied')\n#Are these inputs correct though?","7e8f4431":"#model.startprob_= np.array()\n#model.transmat_ = np.array()\n#model.means_ = np.array()\n#model.covars_ = np.tile(np.identity(), (,,,))\n#X, Y = model.sample(100)\n#This will be generated by the LDA in the previous section","25a9c111":"#just tinkering here \n#vec_change = np.array(DJIA['vectorized_change'])\n#vec_change = vec_change.reshape(1, -1)","1ce2678c":"DJIA.columns","9d8af93c":"model.fit(DJIA[['Date','vectorized_change']])\n","63605a3e":"#I will use a Chi-Square test to test the Goodness of Fit","f119c32c":"**Test Performance**","1afdd26d":"**Hidden Markov Model**","5860f4e0":"**Test Goodness of Fit**"}}