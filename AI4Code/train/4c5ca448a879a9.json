{"cell_type":{"75f23f83":"code","0233e651":"code","8e064382":"code","ae38b620":"code","9238f170":"code","4692d25f":"code","0ab9c311":"code","c6940a12":"code","3c69dc02":"code","8c98cc18":"code","c58295d2":"code","422d790e":"code","016e25a1":"code","113eb96d":"code","2fb1c928":"code","7a7665d5":"code","e750dc90":"code","4f98150a":"code","e97800b5":"code","a545551c":"code","d31abc25":"code","a0101348":"code","23b0ef56":"code","e375242d":"code","3eb747f1":"code","41138d1d":"code","cc9ff409":"code","b4343056":"code","678c0b92":"code","8daa85a2":"code","720b8375":"code","49be4030":"code","020b8259":"code","8aeff79b":"code","d6f38f0e":"code","6aaff361":"code","a652240d":"code","89cfc9f3":"code","48decba1":"code","20832798":"code","de25925c":"code","2503d558":"code","70350285":"markdown","33ae162b":"markdown","d27410ef":"markdown","72aba425":"markdown","1b1c1dc6":"markdown","b97f071a":"markdown","d6d4b77e":"markdown","8b4725f1":"markdown","4516bcd3":"markdown","a3d9ef53":"markdown","97d1a43c":"markdown","3e391479":"markdown","e04b5050":"markdown","c58bd7e8":"markdown","65c8e05d":"markdown","532a7ce1":"markdown","88052a14":"markdown","59eda1f1":"markdown","41405631":"markdown","85e13f90":"markdown","00a066db":"markdown","f4eddd80":"markdown","7ca89c40":"markdown","212aa442":"markdown","023e3772":"markdown","0227f15a":"markdown","dff84195":"markdown"},"source":{"75f23f83":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0233e651":"import pandas as pd\nimport seaborn as sns\nimport statsmodels.tsa.api as smt  \nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom sklearn import metrics\nimport requests\nimport io\n%matplotlib inline\n\n\ndf=pd.read_csv('..\/input\/weatherAUS.csv')\ndf.shape","8e064382":"df.head()","ae38b620":"total = df.isnull().sum().sort_values(ascending=False)\npercent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)","9238f170":"missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])","4692d25f":"f, ax = plt.subplots(figsize=(15, 6))\nplt.xticks(rotation='90')\nsns.barplot(x=missing_data.index, y=missing_data['Percent'])\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)\nmissing_data","0ab9c311":"df=df.drop(['Sunshine','Evaporation','Cloud3pm','Cloud9am'],axis=1)","c6940a12":"df=df.drop(['Date','Location','RISK_MM'],axis=1)","3c69dc02":"df['RainTomorrow']=df['RainTomorrow'].map({'No':0,'Yes':1})","8c98cc18":"df['RainToday']=df['RainToday'].map({'No':0,'Yes':1})","c58295d2":"df=df.dropna(how='any')  ## also able to fill nan values using mean,median.","422d790e":"df['WindDir9am']=df['WindDir9am'].map({'W':0, 'NNW':1, 'SE':2, 'ENE':3, 'SW':4, 'SSE':5, 'S':6, 'NE':7, 'SSW':8, 'N':9, 'WSW':10,\n       'ESE':11, 'E':12, 'NW':13, 'WNW':14,\n       'NNE':15\n})","016e25a1":"df['WindDir3pm']=df['WindDir3pm'].map({'WNW':0, 'WSW':1, 'E':2, 'NW':3, 'W':4, 'SSE':5, 'ESE':6, 'ENE':7, 'NNW':8, 'SSW':9,\n       'SW':10, 'SE':11, 'N':12, 'S':13, 'NNE':14,\n        'NE':15})","113eb96d":"df['WindGustDir']=df['WindGustDir'].map({'W':0, 'WNW':1, 'WSW':2, 'NE':3, 'NNW':4, 'N':5, 'NNE':6, 'SW':7, 'ENE':8, 'SSE':9,\n       'S':10, 'NW':11, 'SE':12, 'ESE':13, \n       'E':14, 'SSW':15})","2fb1c928":"df.head()","7a7665d5":"df.isnull().sum()","e750dc90":"y=df['RainTomorrow']\nX=df.drop('RainTomorrow',axis=1)","4f98150a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=20)","e97800b5":"from statsmodels.tools import add_constant as add_constant\ndf_constant = add_constant(df)\ndf_constant.head()","a545551c":"cols=df_constant.columns[:-1]\nmodel=sm.Logit(df.RainTomorrow,df_constant[cols])\nresult=model.fit()\nresult.summary()","d31abc25":"df['RainTomorrow'].value_counts(normalize=True)","a0101348":"from sklearn.metrics import accuracy_score,roc_curve,roc_auc_score,confusion_matrix,classification_report","23b0ef56":"def imb_predict(algo,xtrain,ytrain,xtest,ytest):\n    \n    algo.fit(xtrain,ytrain)\n    ypred=algo.predict(xtest)\n    yprob=algo.predict_proba(xtest)[:,1]\n    \n    acc=accuracy_score(ytest,ypred)\n    print('Accuracy Score: ',acc)\n    \n    con = confusion_matrix(ytest,ypred)\n    print('Confusion matrix: \\n',con)\n    \n    auc=roc_auc_score(ytest,yprob)\n    print('AUC: ',auc)\n    \n    cr=classification_report(ytest,ypred)\n    print('Classification report:\\n ',cr)\n    \n    fpr,tpr,thresh=roc_curve(ytest,yprob)\n    plt.plot(fpr,tpr,'b--')\n    plt.plot(fpr,fpr,'r--')\n    plt.show()  ","e375242d":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression(solver='liblinear')\n\nimb_predict(lr,X_train,y_train,X_test,y_test)","3eb747f1":"from sklearn.neighbors import KNeighborsClassifier  \nknn = KNeighborsClassifier()\nimb_predict(knn,X_train,y_train, X_test,y_test)","41138d1d":"from sklearn.tree import DecisionTreeClassifier      \ndtree=DecisionTreeClassifier(max_depth=5)\nimb_predict(dtree,X_train,y_train, X_test,y_test)","cc9ff409":"from sklearn.ensemble import RandomForestClassifier \nrf=RandomForestClassifier(n_estimators=20)\nimb_predict(rf,X_train,y_train, X_test,y_test)","b4343056":"Xy_train=pd.concat([X_train,y_train],axis=1)                         ##unsersampling\nXy_train0=Xy_train[Xy_train['RainTomorrow']==0]\nXy_train1=Xy_train[Xy_train['RainTomorrow']==1]\nlen1=len(Xy_train1)\nlen0=len(Xy_train0)\nXy_train0_us=Xy_train0.sample(n=len1)\nXy_train_us=pd.concat([Xy_train1,Xy_train0_us],axis=0)\n\nX_train_us=Xy_train_us.drop('RainTomorrow',axis=1)\ny_train_us=Xy_train_us['RainTomorrow']","678c0b92":"imb_predict(lr,X_train_us,y_train_us, X_test,y_test)","8daa85a2":"imb_predict(knn,X_train_us,y_train_us, X_test,y_test) ","720b8375":"imb_predict(dtree,X_train_us,y_train_us, X_test,y_test) ","49be4030":"imb_predict(rf,X_train_us,y_train_us, X_test,y_test) ","020b8259":"Xy_train=pd.concat([X_train,y_train],axis=1)\nXy_train0=Xy_train[Xy_train['RainTomorrow']==0]                     ##oversampling\nXy_train1=Xy_train[Xy_train['RainTomorrow']==1]\nlen1=len(Xy_train1)\nlen0=len(Xy_train0)\nXy_train1_os=Xy_train1.sample(n=len0,replace=True)\nXy_train_os=pd.concat([Xy_train0,Xy_train1_os],axis=0)\n\nX_train_os=Xy_train_os.drop('RainTomorrow',axis=1)\ny_train_os=Xy_train_os['RainTomorrow']","8aeff79b":"imb_predict(lr,X_train_os,y_train_os, X_test,y_test)","d6f38f0e":"imb_predict(knn,X_train_os,y_train_os, X_test,y_test)","6aaff361":"imb_predict(dtree,X_train_os,y_train_os, X_test,y_test) ","a652240d":"imb_predict(rf,X_train_os,y_train_os, X_test,y_test)  ","89cfc9f3":"from imblearn.over_sampling import SMOTE\nsmote=SMOTE(ratio='minority')\nX_train_sm,y_train_sm=smote.fit_sample(X_train,y_train)","48decba1":"imb_predict(lr,X_train_sm,y_train_sm, X_test,y_test)    ","20832798":"imb_predict(knn,X_train_sm,y_train_sm, X_test,y_test) ","de25925c":"imb_predict(dtree,X_train_sm,y_train_sm, X_test,y_test)","2503d558":"imb_predict(rf,X_train_sm,y_train_sm, X_test,y_test) ","70350285":"Decision Tree Classifier","33ae162b":"Random Forest Classifier","d27410ef":"Starting with imbalancing and checking the AUC score and after that proceeding with balancing of data.","72aba425":"Logistic Regression","1b1c1dc6":"Now performing undersampling","b97f071a":"Decision Tree Classifier","d6d4b77e":"Random Forest Classifier","8b4725f1":"KNN Classifier","4516bcd3":"SMOTE","a3d9ef53":"Over sampling","97d1a43c":"K-NN Classifier","3e391479":"Since the data is imbalance Accuracy is not a good metric. So Area under curve is good metric.","e04b5050":"KNN Classifier","c58bd7e8":"Random Forest Classifier","65c8e05d":"The above bar graph gives the missing value percentage for each features. so droping 'Sunshine','Evaporation','Cloud3pm','Cloud9am', Date and Location (since it is only for australia so not required).","532a7ce1":"In the next version, with different techniques of filling missing values and applying PCA will be done.","88052a14":"Decision Tree Classifier","59eda1f1":"> Logistic Regression","41405631":"Since the target variable is highly imbalance. so balancing of the data is needed by using undersampling,oversampling and SMOTE and check for recall and AUC score.","85e13f90":"From the above base model it shows the pseudo r square as 0.331, by fadden r square it says that if pseudo r square is between 0.2 to 0.4 it is a good model and also LLR p value is less than 0.05 means the model is significant.","00a066db":"Logistic Regression","f4eddd80":"KNN Classifier","7ca89c40":"Logistic Regression","212aa442":"Conclusion: From the above balancing techniques undersampling performs better comparing with recall. From all Logistic regression performs better with AUC score of 0.864.","023e3772":"Decision Tree Classifier","0227f15a":"BASE MODEL with logistic:","dff84195":"Random Forest Classifier"}}