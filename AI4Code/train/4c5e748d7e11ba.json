{"cell_type":{"9b90c217":"code","a3fd0926":"code","61b2e2f1":"code","21fb0660":"code","209175c6":"code","517594bb":"code","342146a3":"code","260ae8fb":"code","e04c4d59":"code","564ed6a9":"code","d11cb83c":"code","caeb95c6":"code","fe9a7611":"code","bca862fb":"code","30374546":"code","18bb5c51":"code","cd616cdd":"code","a8f1248b":"code","175c3391":"markdown","33b54693":"markdown","6ea12b55":"markdown","d360a711":"markdown","754f42ad":"markdown","ab754174":"markdown","374595b7":"markdown","431e42ea":"markdown"},"source":{"9b90c217":"from PIL import Image \nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nf = plt.figure(figsize=(18.0,12.0))\n\nimg = mpimg.imread('.\/rnn_hiddenlayer.png') ### Dont think about name, just taken from stackoverflow.\nplt.imshow(img)\nplt.show()\n","a3fd0926":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","61b2e2f1":"import csv\nimport dask as ddt\nimport xgboost as xgb\nimport pandas as pd\nimport pandas as pd\n\n\nload_data = True\nif load_data:\n    train = pd.read_csv('..\/input\/jane-street-market-prediction\/train.csv')\n\n\n\nfeatures = train.columns[train.columns.str.contains('feature')]\ntarget = 'action'\nprint(\"features read\")\nfillnamean = train.mean()\ntrain = train[train['weight'] != 0]\ntrain['action'] = ((train['weight'].values * train['resp'].values) > 0).astype('int')\ntrain.fillna(train.mean(),inplace=True)\nfillnamean  = train.mean()\n\n#X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, random_state=666, test_size=0.2)\n\n","21fb0660":"train.head(4)","209175c6":"import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torch.nn import CrossEntropyLoss, MSELoss\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\n\n\nall_feat_cols = features\ntarget_cols = \"action\"\n##### Model&Data fnc\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.batch_norm0 = nn.BatchNorm1d(len(all_feat_cols))\n        hidden_size = 130\n        self.dense0 = nn.Linear(len(all_feat_cols), hidden_size)\n\n        dropout_rate = 0.0\n        hidden_sizeLOCAL1 = 65\n        hidden_sizeLOCAL2 = 65\n        hidden_sizeLOCAL3 = 65\n        hidden_sizeLOCAL4 = 65\n        hidden_sizeLOCAL5 = 65\n        hidden_sizeLOCAL6 = 65\n        hidden_sizeLOCAL7 = 65\n        hidden_sizeLOCAL8 = 65\n        hidden_sizeLOCAL9 = 65\n        hidden_sizeLOCAL10 = 65\n        hidden_sizeLOCAL11= 65\n        hidden_sizeLOCAL12= 65\n        hidden_sizeLOCAL13= 65\n        hidden_sizeLOCAL14= 65\n        hidden_sizeLOCAL15= 65\n\n        \n        \n        allhiddens  = hidden_sizeLOCAL1 +hidden_sizeLOCAL2 + hidden_sizeLOCAL3 + hidden_sizeLOCAL4 +   hidden_sizeLOCAL5+ hidden_sizeLOCAL6+ hidden_sizeLOCAL7+ hidden_sizeLOCAL8+hidden_sizeLOCAL9+hidden_sizeLOCAL10\n        self.dense1 = nn.Linear(hidden_size, hidden_sizeLOCAL1)\n        self.batch_norm1 = nn.BatchNorm1d(hidden_sizeLOCAL1)\n        self.dropout1 = nn.Dropout(dropout_rate)\n        self.denseout1 = nn.Linear(hidden_sizeLOCAL1, 1)\n        self.tanh1 = nn.Tanh()\n        self.dropoutensemble1 = nn.Dropout(0.3)\n        \n        self.dense2 = nn.Linear(hidden_size, hidden_sizeLOCAL2)\n        self.batch_norm2 = nn.BatchNorm1d(hidden_sizeLOCAL2)\n        self.dropout2 = nn.Dropout(dropout_rate)\n        self.denseout2 = nn.Linear(hidden_sizeLOCAL2, 1)\n        self.tanh2 = nn.Tanh()\n        self.dropoutensemble2 = nn.Dropout(0.3)\n\n        self.dense3 = nn.Linear(hidden_size, hidden_sizeLOCAL3)\n        self.batch_norm3 = nn.BatchNorm1d(hidden_sizeLOCAL3)\n        self.dropout3 = nn.Dropout(dropout_rate)\n        self.denseout3 = nn.Linear(hidden_sizeLOCAL3, 1)\n        self.tanh3 = nn.Tanh()\n        self.dropoutensemble3 = nn.Dropout(0.3)\n\n        \n        \n        self.dense4 = nn.Linear(hidden_size, hidden_sizeLOCAL4)\n        self.dropout4 = nn.Dropout(dropout_rate)\n\n        self.batch_norm4 = nn.BatchNorm1d(hidden_sizeLOCAL4)\n        self.denseout4 = nn.Linear(hidden_sizeLOCAL4, 1)\n        self.tanh4 = nn.Tanh()\n        self.dropoutensemble4 = nn.Dropout(0.3)\n\n\n        \n        self.dense5 = nn.Linear(hidden_size, hidden_sizeLOCAL5)\n        self.batch_norm5 = nn.BatchNorm1d(hidden_sizeLOCAL5)\n        self.dropout5 = nn.Dropout(dropout_rate)\n        self.denseout5 = nn.Linear(hidden_sizeLOCAL5, 1)\n        self.tanh5 = nn.Tanh()\n        self.dropoutensemble5 = nn.Dropout(0.3)\n        \n        self.dense6 = nn.Linear(hidden_size, hidden_sizeLOCAL6)\n        self.batch_norm6 = nn.BatchNorm1d(hidden_sizeLOCAL6)\n        self.dropout6 = nn.Dropout(dropout_rate)\n        self.denseout6 = nn.Linear(hidden_sizeLOCAL6, 1)\n        self.tanh6 = nn.Tanh()\n        self.dropoutensemble6 = nn.Dropout(0.3)\n\n        self.dense7 = nn.Linear(hidden_size, hidden_sizeLOCAL7)\n        self.batch_norm7 = nn.BatchNorm1d(hidden_sizeLOCAL7)\n        self.dropout7 = nn.Dropout(dropout_rate)\n        self.denseout7 = nn.Linear(hidden_sizeLOCAL7, 1)\n        self.tanh7 = nn.Tanh()\n        self.dropoutensemble7 = nn.Dropout(0.3)\n\n        self.dense8 = nn.Linear(hidden_size, hidden_sizeLOCAL8)\n        self.batch_norm8 = nn.BatchNorm1d(hidden_sizeLOCAL8)\n        self.dropout8 = nn.Dropout(dropout_rate)\n        self.denseout8 = nn.Linear(hidden_sizeLOCAL8, 1)\n        self.tanh8 = nn.Tanh()\n        self.dropoutensemble8 = nn.Dropout(0.3)\n\n        self.dense9 = nn.Linear(hidden_size, hidden_sizeLOCAL9)\n        self.batch_norm9 = nn.BatchNorm1d(hidden_sizeLOCAL9)\n        self.denseout9 = nn.Linear(hidden_sizeLOCAL9, 1)\n        self.tanh9 = nn.Tanh()\n        self.dropout9 = nn.Dropout(dropout_rate)\n\n        self.dropoutensemble9 = nn.Dropout(0.3)\n        \n        self.dense10 = nn.Linear(hidden_size, hidden_sizeLOCAL10)\n        self.batch_norm10 = nn.BatchNorm1d(hidden_sizeLOCAL10)\n        self.dropout10 = nn.Dropout(dropout_rate)\n        self.denseout10= nn.Linear(hidden_sizeLOCAL10, 1)\n        self.tanh10 = nn.Tanh()\n\n        self.dropoutensemble10 = nn.Dropout(0.3)\n        \n        self.dense11 = nn.Linear(hidden_size, hidden_sizeLOCAL11)\n        self.batch_norm11 = nn.BatchNorm1d(hidden_sizeLOCAL11)\n        self.dropout11 = nn.Dropout(dropout_rate)\n        self.denseout11= nn.Linear(hidden_sizeLOCAL11, 1)\n        self.tanh11 = nn.Tanh()\n\n        self.dropoutensemble11 = nn.Dropout(0.3)\n        \n        self.dense12 = nn.Linear(hidden_size, hidden_sizeLOCAL12)\n        self.batch_norm12 = nn.BatchNorm1d(hidden_sizeLOCAL12)\n        self.dropout12 = nn.Dropout(dropout_rate)\n        self.denseout12= nn.Linear(hidden_sizeLOCAL12, 1)\n        self.tanh12 = nn.Tanh()\n\n        self.dropoutensemble12 = nn.Dropout(0.3)\n\n        self.dense13 = nn.Linear(hidden_size, hidden_sizeLOCAL13)\n        self.batch_norm13 = nn.BatchNorm1d(hidden_sizeLOCAL13)\n        self.dropout13 = nn.Dropout(dropout_rate)\n        self.denseout13= nn.Linear(hidden_sizeLOCAL13, 1)\n        self.tanh13 = nn.Tanh()\n\n        self.dropoutensemble13 = nn.Dropout(0.3)       \n        self.dense14 = nn.Linear(hidden_size, hidden_sizeLOCAL14)\n        self.batch_norm14 = nn.BatchNorm1d(hidden_sizeLOCAL14)\n        self.dropout14 = nn.Dropout(dropout_rate)\n        self.denseout14= nn.Linear(hidden_sizeLOCAL14, 1)\n        self.tanh14 = nn.Tanh()\n        self.dropoutensemble14 = nn.Dropout(0.3)\n\n        self.dense15 = nn.Linear(hidden_size, hidden_sizeLOCAL15)\n        self.batch_norm15 = nn.BatchNorm1d(hidden_sizeLOCAL15)\n        self.dropout15 = nn.Dropout(dropout_rate)\n        self.denseout15= nn.Linear(hidden_sizeLOCAL15, 1)\n        self.tanh15 = nn.Tanh()\n\n        self.dropoutensemble15 = nn.Dropout(0.3)\n        self.Relu = nn.ReLU(inplace=True)\n        self.PReLU = nn.PReLU()\n        self.LeakyReLU = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n        self.RReLU = nn.RReLU()\n\n    def forward(self, x):\n        x = self.batch_norm0(x)\n        x = self.dense0(x)\n        x = self.dense0(x)\n\n        x1 = self.dense1(x)\n        x1 = self.batch_norm1(x1)\n        x1 = self.LeakyReLU(x1)\n        x1 = self.denseout1(x1)\n        x1 = self.tanh1(x1)\n        x1 = self.dropoutensemble1(x1)\n\n        \n        x2 = self.dense2(x)\n        x2 = self.batch_norm2(x2)\n        x2 = self.LeakyReLU(x2)\n        x2 = self.denseout2(x2)\n        x2 = self.tanh2(x2)\n        x2 = self.dropoutensemble2(x2)\n\n\n        \n        x3 = self.dense3(x)\n        x3 = self.batch_norm3(x3)\n        x3 = self.LeakyReLU(x3)\n        x3 = self.denseout3(x3)\n        x3 = self.tanh3(x3)\n        x3 = self.dropoutensemble3(x3)\n\n\n        x4 = self.dense4(x)\n        x4 = self.LeakyReLU(x4)\n        x4 = self.batch_norm4(x4)\n        x4 = self.denseout4(x4)\n        x4 = self.tanh4(x4)\n        x4 = self.dropoutensemble4(x4)\n\n        x5 = self.dense5(x)\n        x5 = self.LeakyReLU(x5)\n        x5 = self.batch_norm5(x5)\n        x5 = self.denseout5(x5)\n        x5 = self.tanh5(x5)        \n        x5 = self.dropoutensemble5(x5)\n\n        x6 = self.dense6(x)\n        x6 = self.LeakyReLU(x6)\n        x6 = self.batch_norm6(x6)\n        x6 = self.denseout6(x6)\n        x6 = self.tanh6(x6)\n        x6 = self.dropoutensemble6(x6)\n\n        x7 = self.dense7(x)\n        x7 = self.LeakyReLU(x7)\n        x7 = self.batch_norm7(x7)\n        x7 = self.denseout7(x7)\n        x7 = self.tanh7(x7)\n        x7 = self.dropoutensemble7(x7)\n\n        x8 = self.dense8(x)\n        x8 = self.LeakyReLU(x8)\n        x8 = self.batch_norm8(x8)\n        x8 = self.denseout8(x8)\n        x8 = self.tanh8(x8)\n        x8 = self.dropoutensemble8(x8)\n\n        x9 = self.dense9(x)\n        x9 = self.LeakyReLU(x9)\n        x9 = self.batch_norm9(x9)\n        x9 = self.denseout9(x9)\n        x9 = self.tanh9(x9)\n        x9 = self.dropoutensemble9(x9)\n        \n        x10 = self.dense10(x)\n        x10 = self.LeakyReLU(x10)\n        x10 = self.batch_norm10(x10)\n        x10 = self.denseout10(x10)\n        x10 = self.tanh10(x10)\n        x10 = self.dropoutensemble10(x10)\n        \n        x11 = self.dense11(x)\n        x11 = self.LeakyReLU(x11)\n        x11 = self.batch_norm11(x11)\n        x11 = self.denseout11(x11)\n        x11 = self.tanh11(x11)\n        x11 = self.dropoutensemble11(x11)\n        \n        x12 = self.dense12(x)\n        x12 = self.LeakyReLU(x12)\n        x12 = self.batch_norm12(x12)\n        x12 = self.denseout12(x12)\n        x12 = self.tanh12(x12)\n        x12 = self.dropoutensemble12(x12)\n        \n        x13 = self.dense13(x)\n        x13 = self.LeakyReLU(x13)\n        x13 = self.batch_norm13(x13)\n        x13 = self.denseout13(x13)\n        x13 = self.tanh13(x13)\n        x13 = self.dropoutensemble13(x13)\n        \n        x14 = self.dense14(x)\n        x14 = self.LeakyReLU(x14)\n        x14 = self.batch_norm14(x14)\n        x14 = self.denseout14(x14)\n        x14 = self.tanh14(x14)\n        x14 = self.dropoutensemble14(x14)\n        \n        \n        x15 = self.dense15(x)\n        x15 = self.LeakyReLU(x15)\n        x15 = self.batch_norm15(x15)\n        x15 = self.denseout15(x15)\n        x15 = self.tanh15(x15)\n        x15 = self.dropoutensemble15(x15)\n        \n        \n        #x = torch.cat([x3, x4], 1)\n        x = torch.sum(torch.cat([x1,x2,x3, x4,x5,x6,x7,x8,x9,x10,x11,x12,x13,x14,x15], 1),1)\n        x = x.squeeze()\n\n        return x","517594bb":"\n\ndef return_model(load_model= False, train = True):\n    model = Model()\n\n    if load_model != False:\n        model.load_state_dict(torch.load(load_model))\n        print(\"Model loaded.\")\n\n    if torch.cuda.is_available():\n        print('using device: cuda')\n        torch.device(\"cuda:0\")\n    else:\n        print('using device: cpu')\n        device = torch.device('cpu')\n        model.to(device)\n    if train != True:\n        model.eval()\n        for param in model.parameters():\n            param.grad = None\n\n    \n    return model\n","342146a3":"from torch.utils.data import TensorDataset, DataLoader\nimport torch.utils.data as data_utils\n\ndef return_dataloaders(X_train,y_train,X_valid,y_valid):\n\n    traininputs  = torch.tensor(X_train.values .astype(np.float32))\n    traintargets = torch.LongTensor(y_train.values.flatten() .astype(np.float32))\n    batch_size = 4096\n\n    traindataset =TensorDataset(traininputs, traintargets)\n    trainloader = DataLoader(traindataset, batch_size, shuffle = False)\n\n    validinputs  = torch.tensor(X_valid.values.astype(np.float32))\n    validtargets = torch.LongTensor(y_valid.values.flatten().astype(np.float32))\n    batch_size = 4096\n    validdataset =TensorDataset(validinputs, validtargets)\n    validloader = DataLoader(validdataset, batch_size, shuffle = False)\n    return trainloader,validloader\n","260ae8fb":"class SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss\n\n#  https:\/\/www.kaggle.com\/a763337092\/neural-network-starter-pytorch-version\nclass EarlyStopping: \n    def __init__(self, patience=7, mode=\"max\", delta=0.):\n        self.patience = patience\n        self.counter = 0\n        self.mode = mode\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n        if self.mode == \"min\":\n            self.val_score = np.Inf\n        else:\n            self.val_score = -np.Inf\n\n    def __call__(self, epoch_score, model, model_path):\n\n        if self.mode == \"min\":\n            score = -1.0 * epoch_score\n        else:\n            score = np.copy(epoch_score)\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n        elif score < self.best_score: #  + self.delta\n            self.counter += 1\n            print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            # ema.apply_shadow()\n            self.save_checkpoint(epoch_score, model, model_path)\n            # ema.restore()\n            self.counter = 0\n\n    def save_checkpoint(self, epoch_score, model, model_path):\n        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n            print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n            torch.save(model.state_dict(), model_path)\n        self.val_score = epoch_score\n","e04c4d59":"import torch.optim as optim\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import roc_auc_score, roc_curve, log_loss\n\n\n\n#    trainloader,validloader = return_dataloaders(X_train,y_train,X_valid,y_valid)\n\n\n\ndef train_model(save_name,trainloader,validloader, load_model= False):\n    model = return_model(load_model)\n\n    \n    \n    label_smoothing=0.05\n    criterion = SmoothBCEwLogits(smoothing=label_smoothing)\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\n    base_path = save_name\n    \n    \n\n\n\n    es = EarlyStopping(patience=10, mode=\"max\")\n\n    for epoch in range(100):  # loop over the dataset multiple times\n        print(\"EPOCH:\",epoch)\n        running_loss = 0.0\n        trainauc = np.array([])\n        for i, data in enumerate(trainloader, 0):\n            # get the inputs; data is a list of [inputs, labels]\n            inputs, labels = data\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # forward + backward + optimize\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            trainauc = np.append(trainauc,roc_auc_score(labels.detach().cpu().numpy(),outputs.detach().cpu().numpy()))\n\n\n        print(\"TrainAUC:\",np.mean(trainauc))\n\n        valrunning_loss = 0.0\n        valauc = np.array([])\n        for i, data in enumerate(validloader, 0):\n\n            # get the inputs; data is a list of [inputs, labels]\n            valinputs, vallabels = data\n\n            # zero the parameter gradients\n\n            # forward + backward + optimize\n            optimizer.zero_grad() #zero the parameter gradients\n            model.eval()   # Set model to evaluate mode\n\n\n            valoutputs = model(valinputs)\n            valloss = criterion(valoutputs, vallabels)\n            valauc = np.append(valauc,roc_auc_score(vallabels.detach().cpu().numpy(),valoutputs.detach().cpu().numpy()))\n            # print statistics\n        print(\"ValAUC:\",np.mean(valauc))\n\n        es(np.mean(valauc), model, model_path=base_path)\n        if es.early_stop:\n            print(\"Early stopping\")\n            break\n\n\n    print('Finished Training')\n\n\n    ## earlystopping https:\/\/www.kaggle.com\/a763337092\/neural-network-starter-pytorch-version\n","564ed6a9":"train_indexes = np.tile(np.repeat([True,False],4096),reps=2000)[:train.shape[0]]*np.arange(train.shape[0])\ntrain_indexes = np.sort(train_indexes)\ntrain_indexes = np.trim_zeros(train_indexes)\ntrain_indexes\n\nvalid_indexes = np.tile(np.repeat([False, True],4096),reps=2000)[:train.shape[0]]*np.arange(train.shape[0])\nvalid_indexes = np.sort(valid_indexes)\nvalid_indexes = np.trim_zeros(valid_indexes)\nvalid_indexes","d11cb83c":"trainingmode = False\nif trainingmode:\n    total_index = train.shape[0]\n    fold = 8\n    step = int(total_index\/(fold))\n    trainstep = int(step*0.50)\n    validstep = step- trainstep\n\n    for i in range(fold):#for i in range(fold-2):\n\n        train_ = train[int(i*step):int((i+1)*step-validstep)]\n        valid_ = train[int((i+1)*step-validstep):(i+1)*step]\n        X_train = train_.loc[:, features].astype(np.float32)\n        y_train = train_.loc[:, 'action'].astype(np.float32)\n        X_valid = valid_.loc[:, features].astype(np.float32)\n        y_valid = valid_.loc[:, 'action'].astype(np.float32)\n\n\n\n        trainloader,validloader = return_dataloaders(X_train,y_train,X_valid,y_valid)\n\n        train_model(\"fold_{}.pth\".format(i),trainloader,validloader)","caeb95c6":"trainingmode = False\nif trainingmode:\n    total_index = train.shape[0]\n    fold = 8\n    step = int(total_index\/(fold))\n    trainstep = int(step*0.85)\n    validstep = step- trainstep\n\n    for i in range(fold):#for i in range(fold-2):\n\n        train_ = train[int(i*step):int((i+1)*step-validstep)]\n        valid_ = train[int((i+1)*step-validstep):(i+1)*step]\n        X_train = train_.loc[:, features].astype(np.float32)\n        y_train = train_.loc[:, 'action'].astype(np.float32)\n        X_valid = valid_.loc[:, features].astype(np.float32)\n        y_valid = valid_.loc[:, 'action'].astype(np.float32)\n\n\n\n        trainloader,validloader = return_dataloaders(X_train,y_train,X_valid,y_valid)\n\n        train_model(\"fold_{}.pth\".format(i),trainloader,validloader)","fe9a7611":"\n\n\ntrainingmode = False\nif trainingmode:\n    Xy_train = train[train.index.isin(train_indexes)].copy(deep=True)\n    Xy_valid = train[train.index.isin(valid_indexes)].copy(deep=True)\n\n    X_train = Xy_train[features]\n    X_valid = Xy_valid[features]\n    y_train = Xy_train[\"action\"]\n    y_valid = Xy_valid[\"action\"]\n    \n\n    \n    X_train = X_train.astype(np.float32)\n    y_train = y_train.astype(np.float32)\n    X_valid = X_valid.astype(np.float32)\n    y_valid = y_valid.astype(np.float32)\n\n\n\n    trainloader,validloader = return_dataloaders(X_train,y_train,X_valid,y_valid)\n\n    train_model(\"final_ensemble_3.pth\",trainloader,validloader\n               )","bca862fb":"total_index = train.shape[0]\nfold = 8\nstep = int(total_index\/(fold))\ntrainstep = int(step*0.85)\nvalidstep = step- trainstep\n\nfor i in range(fold):#for i in range(fold-2):\n\n    train_ = train[int(i*step):int((i+1)*step-validstep)]\n    valid_ = train[int((i+1)*step-validstep):(i+1)*step]\n    X_train = train_.loc[:, features].astype(np.float32)\n    y_train = train_.loc[:, 'action'].astype(np.float32)\n    X_valid = valid_.loc[:, features].astype(np.float32)\n    y_valid = valid_.loc[:, 'action'].astype(np.float32)\n\n\n\n    trainloader,validloader = return_dataloaders(X_train,y_train,X_valid,y_valid)\n\n    #train_model(\"fold_{}.pth\".format(i),trainloader,validloader, load_model= \"..\/input\/basemodel-validauc059\/base_model_ (26).pth\")","30374546":"!pip install torchviz\n","18bb5c51":"#!pip install torchviz\nfrom torchviz import make_dot\nmodel = return_model(load_model= False, train = False)\nmake_dot(Variable(torch.from_numpy(X_train[0:1].values)))# plot graph of variable, not of a nn.Module\nprint(\"Note, PyTorchViz doesn't seem to be working. Check this out for alternatives, where I found hiddenlayer<: https:\/\/github.com\/ashishpatel26\/Tools-to-Design-or-Visualize-Architecture-of-Neural-Network\")\n","cd616cdd":"!pip install hiddenlayer\nimport hiddenlayer as hl\n\ntransforms = [ hl.transforms.Prune('Constant') ] # Removes Constant nodes from graph.\n\ngraph = hl.build_graph(model, Variable(torch.from_numpy(X_train[0:1].values)), transforms=transforms)\ngraph.theme = hl.graph.THEMES['blue'].copy()\ngraph.save('rnn_hiddenlayer', format='png')\ngraph","a8f1248b":"\nimport importlib\nimport janestreet\nfrom importlib import reload\nreload(janestreet)\njanestreet.make_env.__called__ = False\nenv = janestreet.make_env() # initialize the environment\n\nenv_iter = env.iter_test() # an iterator which loops over the test set\n\nfillnamean_array = fillnamean[features].values\n\nfrom os import listdir\nfrom os.path import isfile, join\n\"\"\"mypath = \"..\/input\/foldensembles\"\nonlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\nselecteds = [ ]\nmodels = []\nfor model_path in selecteds : \n    models.append(return_model(load_model= False,train = False))\n\n\"\"\"    \n\nmodel = return_model(load_model= '..\/input\/finalfinal\/final_ensemble_3 (1).pth' ,train = False)","175c3391":"#### Some Functions for data handling, models, etc.\nI have functions to train\/returning model\/ returning dataloaders, SmoothBCE function is not mine. However, nothing fancy there. Reading the text,  looking at the NN graph, for complete  2-3 mins will be sufficient. Functions may be better for practitioners, but there are better notebooks published.\n\nIdea explanation is at the bottom. You can skip the code  part.","33b54693":"#### Use of Activation Functions\n\nKey point to mention is tanh and sigmoid layer on the bottom. ,\nTanH produces values between (-1 , 1), centered at 0. (https:\/\/mathworld.wolfram.com\/HyperbolicTangent.html)\n![image.png](attachment:image.png)\n\nSo if the sum of all outputs are larger than zero, we may conclude that prediction is 1. \nSum of tanh's are also compatible with sigmoid function, that produces values between 0.5-1 for inputs larger than zero. Why is this important, this allows us ensemble desired number of models. \n\nOne criticisim, sum of a lot of tanh's couldhave very strict output values, it may be hard to converge, dividing sum to number of models could be better for convergence.\n\nAlso another thing.\n","6ea12b55":"#### Results\nNot able to do full of it due to short time. LB sucks. However, this notebook is intended to give an idea for time-limited submissions, ensembling inside NNs, using activations\/loss  for easier ensembling.  ","d360a711":"#### What happens in this graph?\n\nIt's just bunch of NNs that uses same layer as a feature extractor, each sub-NN is actually a part of ensemble but tried to avoid overfitting by not sending data to some of the networks by dropout(sorry for explaining dropout very bad),adjusted for ensembling by sum(tanh(output of sub-NNs)). With that way, infinite amount of sub-NNs can be ensembled.","754f42ad":"\nBetter visualisation is at very bottom, thanks to https:\/\/github.com\/waleedka\/hiddenlayer\/ .\nWhat I did there? This is just a trick, in order to have fast inference and may better be criticised.\nWhen I saw that a lot of people tries to find a way to ensemble, I just wanted to try ensemble inside a neural network.  ","ab754174":"### An idea to ensemble inside NNs\n#### Why does ensembling worth to talk, what I observed during contest?\n##### Note; Don't read it to have good score. This is unaccomplished ideas that I thought due to time limited submisions and seeing people not ensembling because of that.\n\nJane Street Competition had a time limit on submissions, it should work under 4 mins for 15 thousand rows. Prior to that, I was confused to see single models in public notebooks, since, you know, ensembling is a good thing :D \n\nI wasn't able to successfully run two Pytorch, and there are some people that ensembles Tensorflow+PyTorch(really don't know what people tried to achieve), Keras+LGBM and others. Number of combinations didn't exceed 2 as I see, due to the time limit. Tree methods were lil bit slower than NNs.\n\nEnsembling XGB+LGB in order to obtain model diversity is OK, might be useful since their *default* tree creation algorithms are different, but probably it would hit to the time limit.\n\nDuring the contest, at last week probably, one TF+PyTorch ensembling public notebook showed high score in LB, then a lot of people used the notebook and idea with same\/different seeds\/different CVs and leaderboard then filled with same scores.\n\nPeople used TFLite in order to train more than one model(since it's, you know, lite) and  ensemble them with fast inference speed. I was using PyTorch, worked little on the contest,tried one idea to ensemble in NNs. \n####  ","374595b7":"#### Use of dropouts for ensembling and CV\n\nFor the last layer of sub-NNs, using dropout won't harm getting ensemble results, again, sum of tanh's are zero centered again. \n\nTo be honest, I couldn't have time to finalize my ideas. But, I thought that each sub-NN can be responsible for its own fold, by either\n- training NN with full data in order to get feature extractor first layer(this will be freezed then), then randomize(re-initialize) sub-NNs ), then training each sub-NN with only one fold(keeping other layers freezed, and always using same feature extractor with same nontrainable weights. With that way I can have full NN with its sub-NNs are good at their folds.\n(param.requires_grad = False, https:\/\/discuss.pytorch.org\/t\/how-the-pytorch-freeze-network-in-some-layers-only-the-rest-of-the-training\/7088)\n\nor \n- using fixed seed dropout on  PyTorch with manual seeding in order to mimic the procedure above. https:\/\/stackoverflow.com\/questions\/52730405\/does-pytorch-seed-affect-dropout-layers\n\n\nIt should be better thought, I couldn't complete it since I started competitions very late, as I stated, this might give some of you some ideas, just wanted to share to hear comments,  critics, smilar applications.","431e42ea":"#### Model Architecture\n"}}