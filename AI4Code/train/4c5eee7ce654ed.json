{"cell_type":{"1877c329":"code","db3cb76b":"code","d3418418":"code","79e9325d":"code","eb727348":"code","fb5fb6e5":"code","f77518e5":"code","44742a37":"code","eff341db":"code","0751b331":"code","b9d415d8":"code","fa5db9f7":"code","9c523812":"code","75267f13":"code","b0fd2382":"code","bdaba988":"code","b6408e3b":"code","51d27b08":"code","c68a957c":"code","984ddfa0":"code","70e27a5c":"code","bd3d6e6e":"code","d24a1934":"code","5e16d3fa":"code","dad86889":"code","2c660fe1":"code","f2c35f54":"code","136033e8":"code","25a8857b":"code","5011a386":"code","479f0016":"code","5d77f2c8":"code","505d361a":"code","7d6ab03d":"code","0a67af64":"code","51dab7fc":"code","7a81e974":"code","9c964e2d":"markdown","81bd041b":"markdown","0c01809a":"markdown","8cbd71bb":"markdown","9ec60ffc":"markdown","6ea467ea":"markdown","87696554":"markdown","79813935":"markdown","073b5db5":"markdown","9a5f8fec":"markdown","617cdb15":"markdown","55b26487":"markdown","6ad592a3":"markdown","5f2f5b96":"markdown","b07e1185":"markdown","ce273e43":"markdown","692ac7a2":"markdown","5b39da83":"markdown","a6a8b6ac":"markdown","ebc982c6":"markdown","7f132d1e":"markdown","8f0b0e84":"markdown","bb3e014a":"markdown","27cfaf81":"markdown","d24315b1":"markdown","7ee86808":"markdown","d0a1bd58":"markdown","51278ca0":"markdown","01499b79":"markdown"},"source":{"1877c329":"import numpy as np\nimport pandas as pd\n\n%pylab inline\nimport glob\nimport matplotlib.pyplot as plt\nimport os\nfrom collections import defaultdict\nimport string\nfrom PIL import Image\nfrom skimage import io\n\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\n\nfrom nltk.translate.bleu_score import sentence_bleu\n\nfrom tqdm import tqdm","db3cb76b":"img_folder = os.path.join('\/','kaggle','input','flickr8k','Images')\ncap_file = os.path.join('\/','kaggle','input','flickr8k','captions.txt')\nimg_files = glob.glob(os.path.join(img_folder,'*.jpg'),recursive=True)\nwork_folder = os.path.join('\/','kaggle','working')\nprint(f'Total images in archive : {len(img_files)}')","d3418418":"# Load the captions in to a data frame.\ncap_df = pd.read_csv(cap_file)\nprint(f'Total Captions in file : {cap_df.shape[0]}')\ncap_df.head()","79e9325d":"# Group all captions together having the same image ID.\n# Add \"start\" and \"end\" tokens to each caption.\nstart_token = '<start>'\nend_token = '<end>'\noov_token = '<unk>'\nimage_path_to_caption = defaultdict(list)\nfor idx, row in cap_df.iterrows():\n    caption = f'{start_token} {row.caption} {end_token}'\n    im_path = os.path.join(img_folder,row['image'])\n    image_path_to_caption[im_path].append(caption)\nprint(f\"Total captions present in the dataset: {len(cap_df['caption'])}\")\nprint(f\"Total images present in the dataset: {len(img_files)}\")","eb727348":"image_paths = list(image_path_to_caption.keys())\nall_captions = []\nimg_name_vector = []\nfor image_path in image_paths:\n    caption_list = image_path_to_caption[image_path]\n    all_captions.extend(caption_list)\n    img_name_vector.extend([image_path] * len(caption_list))\n\nf, axes = plt.subplots(1, 2)\nplt.axis('off')\nf.set_figwidth(10)\naxes[0].imshow(io.imread(img_name_vector[0]))\naxes[0].axis('off')\naxes[1].set_ylim(0,5)\nfor i in range(0,5):\n    axes[1].text(0,i,all_captions[i])","fb5fb6e5":"# Select Top 5000 tokens\ntop_k = 5000\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = top_k,\n                                                  oov_token = oov_token,\n                                                  filters = '!\"#$%&()*+.,-\/:;=?@[\\]^_`{|}~ ')\ntokenizer.fit_on_texts(all_captions)","f77518e5":"pad_token = '<pad>'\ntokenizer.word_index[pad_token] = 0\ntokenizer.index_word[0] = pad_token","44742a37":"# Get top 30 words by frequency.\nvals = sorted(tokenizer.word_counts.items(),key=lambda x:x[1],reverse=True)\nwords = [x[0] for x in vals[:32] if x[0] not in [start_token,end_token]]\ncnts = [x[1] for x in vals[:32] if x[0] not in [start_token,end_token]]\nplt.figure(figsize=(20,5))\nf = plt.bar(words,cnts)","eff341db":"# Create the tokenized vectors\ntxt_seqs = tokenizer.texts_to_sequences(all_captions)","0751b331":"def calc_max_length(tensor):\n    '''Get maximum length of any caption in the data set'''\n    return max(len(t) for t in tensor)\n# Pad each vector to the# max_length of the captions\ncap_vector = tf.keras.preprocessing.sequence.pad_sequences(txt_seqs, padding='post')\nmax_length = calc_max_length(txt_seqs)\nprint(\"The shape of Caption vector is :\" + str(cap_vector.shape))","b9d415d8":"TRAIN_SIZE = 0.8\nRAND_STATE = 42\ntf.random.set_seed(RAND_STATE)\nimg_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,cap_vector, train_size = TRAIN_SIZE, random_state = RAND_STATE)","fa5db9f7":"len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)","9c523812":"def load_image(image_path):\n    '''Load image for feature extraction. Below operations are performed:\n    1. Rescaling to 299x299 pixels\n    2. Normalization of image to range of (-1,1)\n    '''\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, (299, 299))\n    # preprocess_input will normalize input to range of -1 to 1\n    img = tf.keras.applications.inception_v3.preprocess_input(img)\n    return img, image_path","75267f13":"image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\nnew_input = image_model.input\nhidden_layer = image_model.layers[-1].output\n\nimage_features_extract_model = tf.keras.Model(new_input, hidden_layer)","b0fd2382":"img_set = sorted(set(img_name_vector))\nnp_cache = os.path.join(work_folder,'npy_cache')\nif not os.path.isdir(np_cache):\n    os.mkdir(np_cache) \n\nIMG_BATCH_SIZE = 64\nimg_dataset = tf.data.Dataset.from_tensor_slices(img_set)\nimg_dataset = img_dataset.map(load_image, num_parallel_calls = tf.data.AUTOTUNE).batch(IMG_BATCH_SIZE)\nfor img, path in tqdm(img_dataset):\n    batch_features = image_features_extract_model(img)\n    batch_features = tf.reshape(batch_features,(batch_features.shape[0], -1, batch_features.shape[3]))\n    \n    for bf, p in zip(batch_features, path):        \n        path_of_feature = p.numpy().decode(\"utf-8\")\n        op_path = os.path.join(np_cache,os.path.basename(path_of_feature)+\".npy\")\n        np.save(op_path, bf.numpy())","bdaba988":"sample_img_batch, sample_cap_batch = next(iter(img_dataset))\nprint(sample_img_batch.shape)\nprint(sample_cap_batch.shape)","b6408e3b":"BATCH_SIZE = 64\nBUFFER_SIZE = 1000\nembedding_dim = 784\nunits = 512\n\nvocab_size = top_k + 1\ntrain_num_steps = len(img_name_train) \/\/ BATCH_SIZE\ntest_num_steps = len(img_name_val) \/\/ BATCH_SIZE\n\n# Shape of the vector extracted from InceptionV3 is (64, 2048)\n# These two variables represent that vector shape\nfeatures_shape = 2048\nattention_features_shape = 64","51d27b08":"def load_img_features(img_name, cap):\n    '''Function to load previously stored npy cache files from np cache folder.\n    Each image will have one npy file.\n    '''\n    op_path = os.path.join(np_cache,os.path.basename(img_name.decode('utf-8'))+\".npy\")\n    img_tensor = np.load(op_path)\n    return img_tensor, cap","c68a957c":"train_dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n# Use map to load the numpy files in parallel\ntrain_dataset = train_dataset.map(lambda item1, item2: tf.numpy_function(load_img_features, [item1, item2], \n                                                                         [tf.float32, tf.int32]))\n# Shuffle and batch\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE,reshuffle_each_iteration=True).batch(BATCH_SIZE)\ntrain_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)","984ddfa0":"test_dataset = tf.data.Dataset.from_tensor_slices((img_name_val, cap_val))\n# Use map to load the numpy files in parallel\ntest_dataset = test_dataset.map(lambda item1, item2: tf.numpy_function(load_img_features, [item1, item2], [tf.float32, tf.int32]))\n# Shuffle and batch\ntest_dataset = test_dataset.shuffle(BUFFER_SIZE,reshuffle_each_iteration=False).batch(BATCH_SIZE)\ntest_dataset = test_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)","70e27a5c":"class Encoder(tf.keras.Model):\n    '''Derived from Model super class'''\n    def __init__(self, embedding_dim):\n        '''Constructor takes embedding dimensions and prepares encoder layer.'''\n        super(Encoder, self).__init__()\n        self.dense = tf.keras.layers.Dense(embedding_dim)\n\n    def call(self, features, training=False):\n        '''Call to Encoder layer'''\n        features = self.dense(features)\n        features = tf.nn.relu(features)\n        return features","bd3d6e6e":"encoder = Encoder(embedding_dim)","d24a1934":"class BahdanauAttention(tf.keras.Model):\n    '''Derived from Model super class'''\n    def __init__(self, units):\n        '''Constructor'''\n        super(BahdanauAttention, self).__init__()\n        self.units = units\n        self.W1 = tf.keras.layers.Dense(self.units)\n        self.W2 = tf.keras.layers.Dense(self.units)\n        self.V = tf.keras.layers.Dense(1)        \n\n    def call(self, features, hidden):\n        '''Call to attention layer'''\n        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n        attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n                                             self.W2(hidden_with_time_axis)))\n        \n        score = self.V(attention_hidden_layer)\n        \n        attention_weights = tf.nn.softmax(score, axis=1)\n\n        context_vector = attention_weights * features\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n\n        return context_vector, attention_weights","5e16d3fa":"class Decoder(tf.keras.Model):\n    '''Derived from Model super class'''\n    def __init__(self, embedding_dim, units, vocab_size):\n        '''Constructor'''\n        super(Decoder, self).__init__()\n        self.units = units\n        # Add attention layer object.\n        self.attention = BahdanauAttention(self.units)\n        self.embed = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        self.gru = tf.keras.layers.GRU(self.units,\n                                       return_sequences = True,\n                                       return_state = True,\n                                       recurrent_initializer = 'glorot_uniform')\n        self.d1 = tf.keras.layers.Dense(self.units)\n        self.d2 = tf.keras.layers.Dense(vocab_size)\n        \n    def call(self, x, features, hidden):\n        '''Call to decoder layer.'''\n        context_vector, attention_weights = self.attention(features, hidden)\n        \n        embed = self.embed(x)\n        embed = tf.concat([tf.expand_dims(context_vector, 1), embed], axis=-1)\n\n        # passing the concatenated vector to the GRU\n        output, state = self.gru(embed)\n\n        x = self.d1(output)\n        x = tf.reshape(x, (-1, x.shape[2]))\n        x = self.d2(x)\n        \n        return x, state, attention_weights\n\n    def reset_state(self, batch_size):\n        '''Reset decoder state.'''\n        return tf.zeros((batch_size, self.units))","dad86889":"decoder = Decoder(embedding_dim, units, vocab_size)","2c660fe1":"LR = 2e-4\n# Using Adam optimizer and Sparse Categorical Cross entropy for loss.\noptimizer = tf.keras.optimizers.Adam(learning_rate = LR)\n\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction='none')","f2c35f54":"checkpoint_path = os.path.join(work_folder,'checkpoint')\nckpt = tf.train.Checkpoint(encoder=encoder,\n                           decoder=decoder,\n                           optimizer = optimizer)\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n\nstart_epoch = 0\nif ckpt_manager.latest_checkpoint:\n    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])","136033e8":"def loss_function(real, pred):\n    '''Define function to calculate loss value based on real and prediction values.'''\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss = loss_object(real, pred)\n    mask = tf.cast(mask, dtype=loss.dtype)\n    loss *= mask\n    return tf.reduce_mean(loss)","25a8857b":"@tf.function\ndef train_step(img_tensor, target):\n    '''Step function for training.'''\n    loss = 0\n    # initializing the hidden state for each batch\n    # because the captions are not related from image to image\n    hidden = decoder.reset_state(batch_size = target.shape[0])\n\n    dec_input = tf.expand_dims([tokenizer.word_index[start_token]] * target.shape[0], 1)\n\n    with tf.GradientTape() as tape:\n        features = encoder(img_tensor)\n\n        for i in range(1, target.shape[1]):\n            # passing the features through the decoder\n            predictions, hidden, _ = decoder(dec_input, features, hidden)\n            loss += loss_function(target[:, i], predictions)\n            # using teacher forcing\n            dec_input = tf.expand_dims(target[:, i], 1)\n\n    total_loss = (loss \/ int(target.shape[1]))\n\n    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n\n    gradients = tape.gradient(loss, trainable_variables)\n\n    optimizer.apply_gradients(zip(gradients, trainable_variables))\n    \n    return loss, total_loss","5011a386":"@tf.function\ndef test_step(img_tensor, target):\n    '''Step function for test'''\n    loss = 0\n\n    hidden = decoder.reset_state(batch_size=target.shape[0])\n    dec_input = tf.expand_dims([tokenizer.word_index[start_token]] * target.shape[0], 1)\n    features = encoder(img_tensor)\n    for i in range(1, target.shape[1]):\n        predictions, hidden, _ = decoder(dec_input, features, hidden)\n        loss += loss_function(target[:, i], predictions)\n        predicted_id = tf.argmax(predictions[0])\n        dec_input = tf.expand_dims([predicted_id] * target.shape[0] , 1)\n\n    avg_loss = (loss \/ int(target.shape[1]))\n\n    return loss, avg_loss","479f0016":"def test_loss_cal(dataset):\n    '''Function to calculate total test loss.'''\n    total_loss = 0\n\n    for (batch,(img_tensor,target)) in enumerate(dataset):\n        batch_loss, t_loss = test_step(img_tensor, target)\n        total_loss += t_loss\n    \n    return total_loss\/int(target.shape[1])","5d77f2c8":"loss_plot = []\ntest_loss_plot = []\nEPOCHS = 15\nbest_test_loss = 100\nfor epoch in tqdm(range(0, EPOCHS)):\n    start = time.time()\n    total_loss = 0\n        \n    for (batch, (img_tensor, target)) in enumerate(train_dataset):\n        batch_loss, t_loss = train_step(img_tensor, target)\n        total_loss += t_loss\n        avg_train_loss=total_loss \/ train_num_steps\n        \n    loss_plot.append(avg_train_loss)    \n    test_loss = test_loss_cal(test_dataset)\n    test_loss_plot.append(test_loss)\n    \n    print ('For epoch: {}, the train loss is {:.3f}, & test loss is {:.3f}'.format(epoch+1,avg_train_loss,test_loss))\n    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n    \n    if test_loss < best_test_loss:\n        print('Test loss has been reduced from %.3f to %.3f' % (best_test_loss, test_loss))\n        best_test_loss = test_loss\n        ckpt_manager.save()","505d361a":"plt.figure(figsize=[10,5])\nplt.subplot(1,2,1)\nplt.plot(loss_plot)\nplt.title('Training Loss')\nplt.subplot(1,2,2)\nplt.plot(test_loss_plot)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Validation Loss')\nplt.show()","7d6ab03d":"def evaluate(image):\n    '''Evaluates the model prediction by comparing real and prediction'''\n    attention_plot = np.zeros((max_length, attention_features_shape))\n\n    hidden = decoder.reset_state(batch_size=1)\n\n    temp_input = tf.expand_dims(load_image(image)[0], 0)\n    img_tensor_val = image_features_extract_model(temp_input)\n    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n\n    features = encoder(img_tensor_val)\n\n    dec_input = tf.expand_dims([tokenizer.word_index[start_token]], 0)\n    result = []\n\n    for i in range(max_length):\n        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n\n        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n\n        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n        if tokenizer.index_word[predicted_id] == oov_token:\n            # Ignore out of vocabulary predictions.\n            continue\n            \n        result.append(tokenizer.index_word[predicted_id])\n\n        if tokenizer.index_word[predicted_id] == end_token:\n            return result, attention_plot\n\n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    attention_plot = attention_plot[:len(result), :]\n    return result, attention_plot","0a67af64":"def plot_attention(image, result, attention_plot):\n    '''Plot attention detail from each word in predicted sequence'''\n    temp_image = np.array(Image.open(image))\n\n    fig = plt.figure(figsize=(10, 10))\n\n    len_result = len(result)\n    for l in range(len_result):\n        temp_att = np.resize(attention_plot[l], (8, 8))\n        if l+1>(len_result\/\/2)*(len_result\/\/2): continue\n        ax = fig.add_subplot(len_result\/\/2, len_result\/\/2, l+1)\n        ax.set_title(result[l])\n        img = ax.imshow(temp_image)\n        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n\n    plt.tight_layout()\n    plt.show()","51dab7fc":"def show_prediction(random_id,captions_set,img_names,tokenizer):\n    '''Display prediction of image.'''\n    image = img_name_val[random_id]\n    real_caption = ' '.join([tokenizer.index_word[i] for i in captions_set[random_id] if i not in [0]])\n    result, attention_plot = evaluate(image)\n    \n    print(f'Image {random_id} : {image}')\n    print ('Real Caption:', real_caption)\n    print ('Prediction Caption:', ' '.join(result))\n    pred_caption = ' '.join(result)\n    real_appn = []\n    real_appn.append(real_caption.split())\n    reference = real_appn\n    candidate = pred_caption.split()\n\n    score = sentence_bleu(reference, candidate, weights=(0.2, 0.2, 0.45, 0.15))\n    print(f\"BELU score: {score*100}\")\n    plot_attention(image, result, attention_plot)","7a81e974":"rid = np.random.randint(0, len(img_name_val))\nshow_prediction(rid,cap_val,img_name_val,tokenizer)\nImage.open(img_name_val[rid])","9c964e2d":"Define decoder object","81bd041b":"#### 2.6 Define Bahdanau Attention Model","0c01809a":"#### 2.5 Define RNN encoder class","8cbd71bb":"### 2. Model building\n#### 2.1 Prepare training and test data sets.","9ec60ffc":"Prepare training data set","6ea467ea":"#### 5.2 Define function for plotting attention detail from prediction","87696554":"### 4 Train RNN Encoder-Decoder model","79813935":"Store image ids and captions in a dictionary. Since for each image, there are multiple captions, we will concatenate captions together to form one big string.\n\nCaptions are differentiated by tags \"< start >\" and \"< end >\"","073b5db5":"In image data set there will be repetitions. So, we will select only unique images.","9a5f8fec":"#### 2.4 Prepare RNN training and test data sets.","617cdb15":"#### 3.1 Define Loss function","55b26487":"#### 1.6 Make all caption vectors equal sized","6ad592a3":"#### 2.7 Define RNN Decoder layer","5f2f5b96":"### 5. Model evaluation\n#### 5.1 Define function for model evaluation","b07e1185":"#### 1.4 Visualize caption and image","ce273e43":"#### 1.2 Define input, intermediate and output paths","692ac7a2":"#### 2.3 Define parameters for RNN (Encoder-Decoder) model","5b39da83":"Define encoder object.","a6a8b6ac":"#### 5.3 Evaluation with random image from test set","ebc982c6":"### 3. Build model","7f132d1e":"#### 3.2 Define Train step function","8f0b0e84":"## Image Captioning Capstone project\n\n* Author : S S B Phani Pradeep Miriyala\n\n### 1. Pre processing\n\n#### 1.1 Load libraries","bb3e014a":"Use InceptionV3 model as base model. We will use features extracted by inception model.","27cfaf81":"Prepare test data set","d24315b1":"#### 4.1 Training and test loss visualization","7ee86808":"#### 1.5 Vecotrize words","d0a1bd58":"#### 1.3 Load captions from captions text file.\nCaptions text file will contain 5 captions for each image.","51278ca0":"#### 3.3 Define test step and loss function","01499b79":"#### 2.2 Image processing"}}