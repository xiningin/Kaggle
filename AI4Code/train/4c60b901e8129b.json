{"cell_type":{"8689cc8b":"code","c3e15952":"code","88e013f1":"code","33ee2115":"code","769d6f54":"code","8a58ce67":"code","cfd78e7b":"code","944c8750":"code","9039ca67":"code","3480c14a":"code","c7f6b51f":"code","e2893cb9":"code","5a6453b7":"code","284ece3e":"code","52a564be":"code","405de8d3":"code","c0281b0a":"code","72ff71a4":"code","5e3090a5":"code","133dca22":"code","0b3890c9":"code","b81391ce":"code","f3594d09":"code","18125f77":"code","e508e201":"code","dbd283f3":"code","ff6977b0":"code","fe99e909":"code","53ab37c6":"code","e1f0c286":"code","38803992":"code","67d3c8a2":"code","403650bf":"code","0ae40e37":"code","7f8cca6d":"code","54bb1740":"code","fb8a9daf":"code","f9f71f4b":"code","c0263264":"code","974854b6":"markdown","3cc7a8cd":"markdown","4b8f7ece":"markdown","b2387117":"markdown","69fb2ea0":"markdown","9810ce69":"markdown","83649083":"markdown","b6e8bba0":"markdown","66434c79":"markdown","1da33a16":"markdown","f9132fd6":"markdown","8622fa38":"markdown","bce9b084":"markdown","24a6d179":"markdown","0046802b":"markdown","e1619178":"markdown","2f6fba78":"markdown","a14db70a":"markdown","7c3ea25b":"markdown","8df314de":"markdown"},"source":{"8689cc8b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport random, pickle\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c3e15952":"train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('..\/input\/digit-recognizer\/test.csv')","88e013f1":"train.head()","33ee2115":"print(\"There are {0} rows and {1} columns in the given Training dataset\".format(train.shape[0],train.shape[1]))","769d6f54":"print(\"There are {0} rows and {1} columns in given test dataset\".format(test.shape[0],test.shape[1]))","8a58ce67":"y = train['label']\nX = train.drop('label',axis=1)","cfd78e7b":"y = pd.get_dummies(y)\ny.head()","944c8750":"X.describe()","9039ca67":"X = X\/255\ntest = test\/255","3480c14a":"X = X.values.reshape(-1,28,28,1)","c7f6b51f":"test = test.values.reshape(-1,28,28,1)","e2893cb9":"fig = plt.figure(figsize = (8,8))\nfor i in range(16):\n    plt.subplot(4,4,i+1).set_title(\"Number\")\n    plt.imshow((X[i][:,:,0]))#,cmap='gray')\n    plt.axis(\"off\")\n    \nplt.show()","5a6453b7":"seed = random.randint(5,99)\nseed","284ece3e":"X_train, X_valid, y_train, y_valid = train_test_split(X,y,test_size = 0.2, random_state = seed)","52a564be":"X_train.shape","405de8d3":"def conv2D():\n    model = tf.keras.Sequential()\n    model.add(layers.Conv2D(36, (3,3), padding = 'same', input_shape = (28,28,1)))\n    model.add(layers.LeakyReLU())\n    \n    model.add(layers.Conv2D(72, (3,3), strides = (2,2), padding = 'same'))\n    model.add(layers.MaxPool2D(pool_size=(2,2), strides=(2,2)))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3)) #dropout to regularize\n    \n    model.add(layers.Conv2D(144, (3,3), strides = (2,2), padding = 'same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3)) #dropout to regularize\n    \n    #model.add(layers.Conv2D(72, (2,2), strides = (2,2), padding = 'same'))\n    #model.add(layers.LeakyReLU())\n    #model.add(layers.Dropout(0.3))\n    \n    model.add(layers.Conv2D(72, (3,3), strides = (2,2), padding = 'same'))\n    model.add(layers.MaxPool2D(pool_size=(2,2), strides=(2,2)))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.25)) #dropout to regularize\n    \n    model.add(layers.Flatten()) #coalescing the convolution into a single dimension\n    model.add(layers.Dense(216, activation = 'relu')) #head of the model starts\n    #model.add(layers.Dense(108, activation = 'relu'))\n    model.add(layers.Dense(10, activation = 'softmax'))\n    return model","c0281b0a":"model = conv2D()","72ff71a4":"optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)","5e3090a5":"model.compile(optimizer=optimizer,\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\nmodel.summary()","133dca22":"#callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_loss',min_delta = 0.002,patience=5)\ncallbacks_LR = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss',verbose=1, min_lr=0.00001,factor=0.5,patience=3)","0b3890c9":"epochs = 50\nhistory = model.fit(X_train,y_train,\n                    epochs=epochs, verbose = 1,\n                    callbacks=[callbacks_LR]\n                    )","b81391ce":"df = pd.DataFrame(history.history)\ndf.plot()","f3594d09":"model.evaluate(X_valid,y_valid)","18125f77":"#dataaug.fit(X_train)","e508e201":"#epochs = 30\n#history = model.fit(dataaug.flow(X_train,y_train, batch_size=32),validation_data=(X_valid,y_valid),\n#  epochs=epochs,steps_per_epoch=X_train.shape[0]\/\/32,\n          #          verbose = 2, callbacks=[callbacks_LR]\n#)","dbd283f3":"#df = pd.DataFrame(history.history)\n#df.plot()","ff6977b0":"predicted = model.predict(test).astype(int)","fe99e909":"predicted","53ab37c6":"a = np.array([0,1,2,3,4,5,6,7,8,9])\na.T","e1f0c286":"b = np.matmul(predicted,a.T)\nfor i in range(4):\n    print(b[i])","38803992":"plt.figure(figsize=(10,10))\nfor i in range(16):\n    plt.subplot(4,4,i+1).set_title(\"Predicted Number:{}\".format(round(b[i],0)))\n    plt.imshow((test[i][:,:,0]),cmap='gray')\n    plt.axis(\"off\")","67d3c8a2":"predicted = np.argmax(predicted, axis =1)","403650bf":"predicted","0ae40e37":"predicted = pd.Series(predicted, name = \"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),predicted],axis = 1)\n\nsubmission.to_csv(\"mnist_submission.csv\",index=False)","7f8cca6d":"#save model for future usage.\n#model.save(\"cnn_dc\")","54bb1740":"#model = tf.keras.models.load_model(\"cnn_dc\") #loading back the model","fb8a9daf":"#model.evaluate(X_valid,y_valid) #validating the performance","f9f71f4b":"#pred = model.predict(test)","c0263264":"#plt.imshow(test[0][:,:,0],cmap=\"gray\")\n#plt.title(\"True Label: {}\".format(np.argmax(pred[0])))","974854b6":"# Defining our NN model","3cc7a8cd":"From above it can be seen that our prediction is in OHC format, we can change it to label with two ways. I have shown both below.","4b8f7ece":"The below code turns our 10 numbers as an array of 10 classes to be classified by NN model.","b2387117":"def conv2D():\n    model = tf.keras.Sequential()\n    model.add(layers.Conv2D(36, (3,3), padding = 'same', input_shape = (28,28,1)))\n   # model.add(layers.LeakyReLU())\n    \n    model.add(layers.Conv2D(72, (3,3), strides = (2,2), padding = 'same',activation='relu'))\n    model.add(layers.MaxPool2D(pool_size=(2,2), strides=(2,2)))\n    #model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3)) #dropout to regularize\n    \n    model.add(layers.Conv2D(144, (3,3), strides = (2,2), padding = 'same',activation='relu'))\n   # model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3)) #dropout to regularize\n    \n    #model.add(layers.Conv2D(72, (2,2), strides = (2,2), padding = 'same'))\n    #model.add(layers.LeakyReLU())\n    #model.add(layers.Dropout(0.3))\n    \n    model.add(layers.Conv2D(72, (3,3), strides = (2,2), padding = 'same',activation='relu'))\n    model.add(layers.MaxPool2D(pool_size=(2,2), strides=(2,2)))\n    #model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.25)) #dropout to regularize\n    \n    model.add(layers.Flatten()) #coalescing the convolution into a single dimension\n    model.add(layers.Dense(108, activation = 'relu')) #head of the model starts\n    model.add(layers.Dense(108, activation = 'relu'))\n    model.add(layers.Dense(10, activation = 'softmax'))\n    return model\n              ","69fb2ea0":"Below code defines a function which will return a model consisting of CNN layers as a base and a head of dense layer which predicts the class of given image.","9810ce69":"Usually we think about having more data to improve accuracy, but with images it's simply easier to augment them to generate data which model has not seen before, giving it a taste of variety. Though it's tricky to decide what techniques to apply, we cannot simply perform all operations which are available. I will be using ImageDataGenerator available in **Keras** here, follow the code.","83649083":"The below code converts each row with 784 columns into a matrix of 28X28 dimension,(1 X n squared matrix to an n X n matrix). We perform the operation on both training and test data.","b6e8bba0":"# Reshaping into arrays of 28X28 pixels.","66434c79":"Looks we have activation of each pixels ranging from 0 to 255, which isn't a good input for a NN model. NN models usually require scaled data. One way to scale is to simply divide by 255 which will return values ranging from 0-1, instead of 0-255(There are other ways as well for scaling, but for simplicity we will just divide by 255).","1da33a16":"Let's just drop the label and make define our input csv as training set and label as target variable.","f9132fd6":"Let's see some details about our training data.","8622fa38":"What we can see is that in the training csv there are 784 pixels plus label represented as columns here. Now, can we convert these row representation to an image? Let's find out!","bce9b084":"Compile and get an overview of our model","24a6d179":"Define Optimizer","0046802b":"**2. ArgMax:** It simply returns the index value of the array. Much simple and easier, no head cracker!","e1619178":"**1. Matrix Multiplication:** Being a linear algebra fan I thought let's put some matrix multiplication into action and down below I have simply multiplied each row with a column matrix. Now a is a 1 X 10 matrix and b is resultant of predicted multiplied by a transpose. since all other values are 0 in predicted the returned value will be of the predicted class.","2f6fba78":"callbacks_LR = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',verbose=1,\n                                min_lr=0.00001,factor=0.3,patience=3)","a14db70a":"Callbacks for early stopping so our model stops training if loss starts oscillating at some value.","7c3ea25b":"dataaug = ImageDataGenerator(\n    rotation_range=10,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    zoom_range=0.2,\n    horizontal_flip=False,\n    vertical_flip=False\n    )","8df314de":"# Data Augmentation"}}