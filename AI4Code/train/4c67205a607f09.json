{"cell_type":{"9e75ec94":"code","08c6067c":"code","3a0e0709":"code","761fefc1":"code","e6032ee3":"code","bb29b77a":"code","0a5ef4c8":"code","35e8357e":"code","69cf65c7":"code","8b0cdba5":"code","f2e40d10":"code","ddff146c":"code","07c6132d":"code","5afc7b26":"code","960558d1":"code","8af931af":"code","e3c691f1":"code","445c0c0e":"code","eb9b1d3d":"code","a48cc6ea":"code","8a21e6bb":"code","53fa2e6a":"code","b72ea133":"code","f81b39e6":"code","c9ec8b78":"code","a56f6954":"code","07a10629":"code","027f24ea":"code","63d11c69":"code","e1ae9ce9":"code","d66e439b":"code","871115b3":"code","7752029f":"code","973e2338":"code","b12d531e":"code","ef3a9882":"code","7ea069dc":"code","f7085bc4":"code","ec2c5cb2":"code","f391d87a":"code","20d97aa3":"code","b09d7daa":"code","5a2f003f":"code","e477543a":"code","c0257461":"code","3a75dca9":"code","bf15d66e":"markdown","568d6768":"markdown","272f9466":"markdown","a49dafa1":"markdown","f364f654":"markdown","32496c3f":"markdown","b82c333e":"markdown","05809586":"markdown","50628caa":"markdown","37ba0a51":"markdown","0ab6302b":"markdown","b9ab8510":"markdown","b0217db6":"markdown","b9a91f34":"markdown","a674bf88":"markdown","530f8595":"markdown","ea5acd06":"markdown","8f85b79f":"markdown","02be540a":"markdown","fe9765b3":"markdown","203971f8":"markdown","e99c4ec1":"markdown","8bd60619":"markdown","11dbfd7b":"markdown","32c86adb":"markdown","bb4bc077":"markdown","2f36e29d":"markdown","e2fd2759":"markdown","61e88e8c":"markdown","6bda04b9":"markdown","f299d023":"markdown"},"source":{"9e75ec94":"import pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings \nimport os\nwarnings.filterwarnings('ignore')\n%matplotlib inline","08c6067c":"data_train=pd.read_csv('..\/input\/black-friday-sales-prediction\/train_oSwQCTC (1)\/train.csv')\ndata_test=pd.read_csv('..\/input\/black-friday-sales-prediction\/test_HujdGe7 (1)\/test.csv')","3a0e0709":"data_train=data_train[0:10000]\ndata_test=data_test[0:10000]","761fefc1":"print(data_train.columns.values)","e6032ee3":"print(data_train.dtypes)","bb29b77a":"data_train.head(5)","0a5ef4c8":"data_train.isna().sum()","35e8357e":"import pylab \nimport scipy.stats as stats","69cf65c7":"stats.probplot(data_train['Purchase'], dist=\"norm\", plot=pylab)\npylab.show()","8b0cdba5":"plt.hist(data_train['Purchase'])\nplt.show()","f2e40d10":"data_train['Purchase_log']=np.log(data_train['Purchase'])\nstats.probplot(data_train['Purchase_log'], dist=\"norm\", plot=pylab)\npylab.show()","ddff146c":"data_train['Purchase'], lmbda = stats.boxcox(data_train['Purchase'])\nstats.probplot(data_train['Purchase_log'], dist=\"norm\", plot=pylab)\npylab.show()\nplt.hist(data_train['Purchase'])\nplt.show()","07c6132d":"category_cols=['Gender', 'Age', 'Occupation', 'City_Category', 'Stay_In_Current_City_Years',\n               'Marital_Status', 'Product_Category_1', 'Product_Category_2', 'Product_Category_3']","5afc7b26":"for col in category_cols:\n    data_train[col] = pd.Categorical(data_train[col])\n    \nfor col in category_cols:\n    data_test[col] = pd.Categorical(data_test[col])","960558d1":"cat_uniques = pd.DataFrame([[i, len(data_train[i].unique())] for i in data_train[category_cols].columns], columns=['Variable', 'Unique Values']).set_index('Variable')\nprint(cat_uniques)\n    \n### Visualisation\nn=len(category_cols)\nfig,ax = plt.subplots(n,1, figsize=(6,n*2), sharex=True)\nfor i in range(n):\n    plt.sca(ax[i])\n    col = category_cols[i]\n    sns.barplot(x=col, y='Purchase', data=data_train)\n    \n## Encoding\nfrom sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()","8af931af":"for col in category_cols:\n    data_train[col] = le.fit_transform(data_train[col])\n    \nfor col in category_cols:\n    data_test[col] = le.fit_transform(data_test[col])\n    ","e3c691f1":"X=data_train.drop('Purchase', axis=1).drop(['Purchase_log', 'User_ID', 'Product_ID'], axis=1)\ny=data_train['Purchase']","445c0c0e":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.2, random_state=42)","eb9b1d3d":"from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import Lasso, Ridge","a48cc6ea":"from sklearn.metrics import r2_score","8a21e6bb":"def rmse(y_true, y_preds):\n    return np.sqrt(((y_preds - y_true) ** 2).mean())","53fa2e6a":"def mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100","b72ea133":"reg_algos = [\n    RandomForestRegressor(),\n    SVR(kernel='rbf'),#\n    DecisionTreeRegressor(),\n    GradientBoostingRegressor(),\n    MLPRegressor(),\n    Ridge(),\n    Lasso()]","f81b39e6":"for algo in reg_algos:\n    algo.fit(X_train, y_train)\n    name = algo.__class__.__name__\n    \n    print(\"_\"*30)\n    print(name)\n    \n    print('****Results****')\n    train_predictions = algo.predict(X_test)\n    \n    # calculate score\n    RMSE=rmse(y_test, train_predictions)\n    r2=r2_score(y_test, train_predictions)\n    MAPE=mean_absolute_percentage_error(y_test, train_predictions)\n    \n    print(\"RMSE: {:.4}\".format(RMSE))\n    print(\"R^2: {:.4}\".format(r2))\n    print(\"MAPE: {:.4}\".format(MAPE))\n    \nprint(\"_\"*30)","c9ec8b78":"from sklearn.model_selection import RandomizedSearchCV","a56f6954":"n_estimators=[100, 500, 1000, 1500, 2000] # Define params\nmax_features=['auto', 'sqrt']\nmax_depth=[1, 10,  50, 100]\nmax_depth.append(None)\nmin_samples_split=[2,5,10,20,50]\nmin_samples_leaf=[1,5,10]","07a10629":"grid_params={'n_estimators':n_estimators,\n             'max_features':max_features,\n             'max_depth':max_depth,\n             'min_samples_split':min_samples_split,\n             'min_samples_leaf':min_samples_leaf}","027f24ea":"rf=RandomForestRegressor(random_state=40) # Initiate base model","63d11c69":"rf_rand = RandomizedSearchCV(estimator=rf, \n                             param_distributions=grid_params, \n                             scoring='neg_root_mean_squared_error',\n                             n_iter=500,\n                             cv=3,\n                             random_state=40,\n                             verbose = 2,n_jobs=-1) ","e1ae9ce9":"rf_rand.fit(X_train, y_train)\nprint(\"Best parameter (CV score=:\",  rf_rand.best_score_*-1)\nprint(\"Best RF params:\")\nprint (rf_rand.best_params_)","d66e439b":"tuned_rf=RandomForestRegressor(**rf_rand.best_params_)","871115b3":"learning_rate = [1, 0.5, 0.1, 0.05, 0.01, 0.001]\nn_estimators = [100, 500, 1000, 1500, 2000]\nmax_depths = [1, 10,  50, 100]\nmin_samples_splits = [2,5,10,20,50]\nmin_samples_leafs = [1,5,10]","7752029f":"grid_params={'n_estimators':n_estimators,\n             'learning_rate':learning_rate,\n             'max_depth':max_depth,\n             'min_samples_split':min_samples_split,\n             'min_samples_leaf':min_samples_leaf}","973e2338":"gb=GradientBoostingRegressor(random_state=40)\ngb_rand = RandomizedSearchCV(estimator=gb, \n                             param_distributions=grid_params, \n                             scoring='neg_root_mean_squared_error',\n                             n_iter=500,\n                             cv=3,\n                             random_state=40,\n                             verbose = 2,n_jobs=-1) ","b12d531e":"gb_rand.fit(X_train, y_train)\nprint(\"Best parameter (CV score=:\",  gb_rand.best_score_*-1)\nprint(\"Best GB params:\")\nprint (gb_rand.best_params_)","ef3a9882":"tuned_gb=GradientBoostingRegressor(**gb_rand.best_params_)","7ea069dc":"from sklearn.ensemble import VotingRegressor","f7085bc4":"ensemble_reg = VotingRegressor(estimators=[('tuned_rf',tuned_rf), \n               ('tuned_gb',tuned_gb)])","ec2c5cb2":"final_algos = [\n    tuned_rf,\n    tuned_gb,\n    ensemble_reg]","f391d87a":"performance=[]\nfor algo in final_algos:\n    \n    algo.fit(X_train, y_train)\n    name = algo.__class__.__name__\n    \n    print(\"_\"*30)\n    print(name)\n    \n    print('****Results****')\n    train_predictions = algo.predict(X_test)\n    \n    # calculate score\n    RMSE=rmse(y_test, train_predictions)\n    r2=r2_score(y_test, train_predictions)\n    MAPE=mean_absolute_percentage_error(y_test, train_predictions)\n    \n    print(\"RMSE: {:.4}\".format(RMSE))\n    print(\"R^2: {:.4}\".format(r2))\n    print(\"MAPE: {:.4}\".format(MAPE))\n    \n    cols=[\"Algo\", \"RMSE\"]\n    performance_df = pd.DataFrame([[name, RMSE]], columns=cols)\n    performance.append(performance_df)\n    \nprint(\"_\"*30)","20d97aa3":"performance=pd.concat(performance, axis=0).sort_values(by='RMSE')\nbest_algo=performance['Algo'].values[0] # get best algo","b09d7daa":"print('The best performing algorithm is: ' , best_algo)","5a2f003f":"if best_algo=='VotingRegressor':\n    print('Ensembling via voting improved performance')\nelse:\n    print('Ensembling via voting did not improve performance')\n    \n# Assign best algo for final predictions\nif best_algo=='VotingRegressor':\n    final_algo=ensemble_reg\nif best_algo=='GradientBoostingRegressor':\n    final_algo=tuned_gb\nelse:\n    final_algo=tuned_rf\n    \n### Final Predictions\n# Lastly, I will predict on the test data\nuser_ids=data_test['User_ID']\nproduct_ids=data_test['Product_ID']\ndata_test=data_test.drop(['User_ID', 'Product_ID'], axis=1)\nfinal_preds = final_algo.predict(data_test)","e477543a":"from scipy.special import inv_boxcox\nfinal_preds=inv_boxcox(final_preds, lmbda)","c0257461":"final_df=pd.DataFrame({'Purchase':final_preds,\n                      'User_ID':user_ids,\n                      'Product_ID':product_ids})","3a75dca9":"final_df.to_csv('submission.csv')","bf15d66e":"## Load Data","568d6768":"Fit RF CV Search","272f9466":"Due to memory limits of Kaggle notebooks I am forced to subset the data","a49dafa1":"RMSE is required as the primary performance metric. We'll also define some others.","f364f654":" Formatting categories","32496c3f":"Now lets test how each of these perform on the hold out set","b82c333e":"Histogram","05809586":"From Kaggle:<br>\n\nA retail company \u201cABC Private Limited\u201d wants to understand the customer purchase behaviour (specifically, purchase amount) against various products of different categories. They have shared purchase summary of various customers for selected high volume products from last month.<br>\nThe data set also contains customer demographics (age, gender, marital status, citytype, stayincurrentcity), product details (productid and product category) and Total purchaseamount from last month.","50628caa":"The top performing algos are GB and RF. This is most likely because there are many interactions between variables which I did not feature engineer.<br>\nLets train and stack these.","37ba0a51":"# Tuning Gradient Boost","0ab6302b":"# Baseline Regression Algo Testing<br>\nImport regressors","b9ab8510":"The data now appears reasonably normal. We will need to remember to back-transform the outcome using inv boxcox later.","b0217db6":"Inverse BoxCox","b9a91f34":"Is the target variable normally distributed?","a674bf88":"# Tuning Random Forest","530f8595":"Are there NAs in cols?","ea5acd06":" # Predicting Black Friday Sale Values","8f85b79f":"How many unique values are in each cat? If too many, we may not be able to grab dummies.","02be540a":"This did not improve the distribution. A boxcox transformation would be more appropriate here...","fe9765b3":"Now, they want to build a model to predict the purchase amount of customer against various products which will help them to create personalized offer for customers against different products","203971f8":"## Import Packages","e99c4ec1":"What types are they?","8bd60619":"# Split data","11dbfd7b":"Not very normal - let's log transform. ","32c86adb":"Format for submission","bb4bc077":"# Brief look at the data<br>\nWhat columns do we have? ","2f36e29d":"# Voting Regressor","e2fd2759":"View the data ","61e88e8c":"Make final rf object","6bda04b9":"Only NAs in product category, but since products can have multiple categories this is okay.","f299d023":"Make final gb object"}}