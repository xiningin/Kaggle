{"cell_type":{"9f050a6d":"code","60eb0074":"code","8317eb3f":"code","14708f24":"code","5614be6d":"code","e463a460":"code","67de7456":"code","4f2ebc3d":"code","8bc28f02":"code","81b32fb6":"code","2cff5ecb":"code","4cd7d8d1":"code","a63015c5":"code","01e3ac83":"markdown","c4db5769":"markdown","5315fb9c":"markdown","50ca2e0c":"markdown","3e92ada1":"markdown","15f61e57":"markdown","47f6e7cf":"markdown","653ac4b9":"markdown","828aa692":"markdown","88f44007":"markdown","c7e23d7a":"markdown","95dca9d4":"markdown","1559b993":"markdown","6d577e0c":"markdown","69963de7":"markdown","74798126":"markdown","5aadeb70":"markdown","7fff3298":"markdown","6560883e":"markdown"},"source":{"9f050a6d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport nltk\nfrom nltk import FreqDist\nfrom nltk.corpus import stopwords\nimport re\n\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\n# Load the data\ndf = pd.read_csv('..\/input\/videogamesales\/vgsales.csv')\n# Reduce dataframe to feature 'Name', and label 'Genre'\ndf = df[['Name','Genre']].copy()\ndf.head()","60eb0074":"tokens = nltk.word_tokenize(' '.join(df.Name.values))\ntext = nltk.Text(tokens)\nfdist = FreqDist(text)\nprint(fdist)\nfdist.most_common(50)","8317eb3f":"fdist.plot(50,cumulative=True,title='Top 50 Word Frequency Cumulative Plot')","14708f24":"text.collocations()","5614be6d":"genres = list(df.Genre.unique())\nd = {}\nfor g in genres:\n    names = list(df[df.Genre == g].Name.values)\n    tokens = nltk.word_tokenize(' '.join(names))\n    types = set(tokens)\n    lexical_diversity = round(len(types) \/ len(tokens),3)\n    d[g] = (len(tokens), len(types), lexical_diversity)\n    \n    #print(f\"{g}: TOKENS: {len(tokens)}, TYPES: {len(types)}, LEXICAL DIVERSITY: {lexical_diversity}\")\ntable = pd.DataFrame.from_dict(d,orient='index',columns=['tokens','type','lexical_diversity'])\ndisplay(table.sort_values('lexical_diversity'))","e463a460":"def clean_text(text):\n    text = text.lower()\n    tokens = nltk.word_tokenize(text)\n    tokens = [t for t in tokens if t.isalpha()]\n    tokens = [t for t in tokens if t not in stopwords.words('english')]\n    # Remove roman numerals using regex\n    roman_re = r'\\bM{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})\\b'\n    tokens = [t for t in tokens if not re.match(roman_re,t,flags=re.IGNORECASE).group()]\n    \n    text = ' '.join(tokens).strip()\n    \n    return text\n\ndf.Name = df.Name.apply(lambda n: clean_text(n))\ndf.sample(20)","67de7456":"df.Genre.value_counts(ascending=True).plot(kind='barh')\nplt.show()","4f2ebc3d":"genres = list(df.Genre.unique())\nd = {}\nfor g in genres:\n    names = list(df[df.Genre == g].Name.values)\n    tokens = nltk.word_tokenize(' '.join(names))\n    types = set(tokens)\n    lexical_diversity = round(len(types) \/ len(tokens),3)\n    d[g] = (len(tokens), len(types), lexical_diversity)\n    \n    #print(f\"{g}: TOKENS: {len(tokens)}, TYPES: {len(types)}, LEXICAL DIVERSITY: {lexical_diversity}\")\ntable = pd.DataFrame.from_dict(d,orient='index',columns=['tokens','type','lexical_diversity'])\ndisplay(table.sort_values('lexical_diversity'))","8bc28f02":"tokens = nltk.word_tokenize(' '.join(df.Name.values))\ntext = nltk.Text(tokens)\nfdist = FreqDist(text)\nprint(fdist)\nfdist.most_common(50)","81b32fb6":"fdist.plot(50,cumulative=True,title='Top 50 Word Frequency Cumulative Plot')","2cff5ecb":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\ntfidf_vectorizer = TfidfVectorizer(analyzer='word',ngram_range=(1,2))\n\n# split dataset into training and validation set\ny = df.Genre\nx = df.Name\nxtrain, xval, ytrain, yval = train_test_split(x,y, test_size = 0.2)\n\n# create the TF-IDF features\nxtrain_tfidf = tfidf_vectorizer.fit_transform(xtrain)\nxval_tfidf = tfidf_vectorizer.transform(xval)\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import f1_score\nfrom sklearn import metrics\n\nmnb = MultinomialNB()\nlr = LogisticRegression(max_iter=1000)\n\n# fit model on train data\nmnb.fit(xtrain_tfidf, ytrain)\nlr.fit(xtrain_tfidf, ytrain)\n\n# make predictions for validation set\nmnb_pred = mnb.predict(xval_tfidf)\nlr_pred = lr.predict(xval_tfidf)\n\n# evaluate performance\nmnb_acc = metrics.accuracy_score(yval, mnb_pred)\nmnb_acc = round(mnb_acc,2)\nlr_acc = metrics.accuracy_score(yval, lr_pred)\nlr_acc = round(lr_acc,2)","4cd7d8d1":"print(f\"Accuracy Scores:\\nMultinomial Naive Bayes: {mnb_acc}\")\nprint(f\"Logistic Regression: {lr_acc}\")","a63015c5":"pred_df = pd.DataFrame(xval)\npred_df['actual'] = yval\npred_df['prediction'] = lr_pred\npred_df.sample(10)","01e3ac83":"Logistic Regression achieved a higher accuracy score of over 70% and will be chosen.","c4db5769":"With over 70% accuracy, the above sample shows the model was able to correctly classify most of the video game titles except there are cases of mis-classification for games in similar genres where the word use will likely be similar. Such as Strategy, Action and Adventure to name a few.","5315fb9c":"Since we removed the commonly occuring and less useful words the lexical diversity for all of the genres tended to increase. I predict that the genres with the highest lexical diversity will be the hardest to predict for.","50ca2e0c":"### Tokens, type and lexical diversity","3e92ada1":"## Data Cleaning and auditing\n\nIn this phase since we will only be working with text, non-alphabetic, stopwords and roman numerals will be removed from the titles.","15f61e57":"## Sample predictions","47f6e7cf":"The above plot, shows the top 50 most frequent words and the cumulative distribution. The 50 most common words currently make up over 22.5K of the 75K words in the corpus, or about 30%. There are also several stop words that will be removed.","653ac4b9":"The above table shows the 'word richness' of the video game names by genre. The lexical diversity can suggest how varied in word choice the names are within their respective genres. Sports and Action names have the lowest lexical diversity ratios while Adventure and Puzzle have the highest.","828aa692":"## Class balance","88f44007":"Now the 50 most common words make up nearly 8K of the 50K words, or 16%.","c7e23d7a":"## Text Classification Model\n\n**Problem Definition**\n\nNow that we understand the dataset more we can define the problem to be a Multi-class Classification problem since the video games in the dataset all have one genre from a list of multiple subjects.\n\n![image.png](attachment:image.png)\n\n**Feature Extraction**: TF-IDF algorithm will be used to give each word in our titles weight to input into the models.\n\n**Prediction models**: Multinomial Naive Bayes and Logistic Regression will be compared.","95dca9d4":"# Video Game Classification using NLP\n\n\nAn interesting use case of NLP is text classification. We have a dataset of video game titles and their genres, the question I would like to answer is: \n\n**Can we devise a model that can accurately predict what the genre is of a video game by just the title alone?**\n\nTo answer this question we will go through the following steps:\n\n1. Exploratory Analysis - this phase will explore the structure of the data and underlying trends in the text.\n2. Data quality audit and cleaning - After the initial EDA, the quality of the data will be reviewed and will be transformed to fit our requirements for the model.\n3. Text Classification and evaluation - Text classification model will be trained on the testing set and evaluated on the validation set.","1559b993":"It can be seen from the above plot that, there is some class imbalance. We can employ an over and undersampling strategy here. However I will continue with this distribution for now. Should we wish to improve the accuracy of the model this can be a point to come back to.","6d577e0c":"The dataset now has one feature, `Name` and the target label `Genre`. The problem can also be considered a multi-class classification problem as opposed to binary classification (Only 2 label values) and multi-label classification (Text can have more than one label.) In this case a video game only has one genre and there is more than two genres.","69963de7":"### Collocations\n\nCollocations are uniquely commonly occuring bigrams, or words that tend to follow one another. Sometimes bigrams are more helpful in determining classification.","74798126":"## Conclusion\n\nFrom this it can be seen that with just the titles as features it is possible to capture the correlation with Genre and predict on unseen data. This can be further built upon should there be synopsis data on video games, and other mediums.\n\nSome steps to be taken to improve classification accuracy:\n - Employ Under \/ Over sampling strategy to reduce class imbalance.\n - Explore other models\n - Use cross validation and hyperparameter tune models\n\nA use case for this type of classification was automating data entry tasks as data can come without classification.","5aadeb70":"In total there are 9.4K unique words and over 75K total words. The above are the 50 most commonly occuring words in the titles.","7fff3298":"## Post Cleaning analysis\n\nWith non-alphabetic, stop words and roman numerals removed we can observe how the data was changed.","6560883e":"## Text Data Exploration \/ Text Analysis\n\n"}}