{"cell_type":{"455c38b6":"code","eed55683":"code","8d3c8ddb":"code","af09aeac":"code","841ec51d":"code","849996b4":"code","09e12fdc":"code","03a9b932":"code","6d2a1020":"code","c08916dd":"code","e28f7bee":"code","d51cd45c":"code","0ca8c95f":"code","abb75c38":"code","07621efe":"code","3251bf41":"code","0b34b5e0":"code","f9ed94d1":"markdown","3fd8418f":"markdown","5dbfc3a5":"markdown","2a78e49f":"markdown","a2416488":"markdown","7e0dee05":"markdown","3cebe2a9":"markdown"},"source":{"455c38b6":"import numpy as np\nimport pandas as pd\nimport nltk\nimport re\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\n\nimport matplotlib.pyplot as plt","eed55683":"path_traindata = '\/kaggle\/input\/nlp-getting-started\/train.csv'\npath_testdata = '\/kaggle\/input\/nlp-getting-started\/test.csv'\n\ntrain_data = pd.read_csv(path_traindata)\ntest_data = pd.read_csv(path_testdata)\n\nprint('Data imported')","8d3c8ddb":"#Total rows and columns\ntrain_data_shape = train_data.shape\ntest_data_shape = test_data.shape\n#Total of nulls per column\ntrain_total_nulls = train_data.isnull().sum()\ntest_total_nulls = test_data.isnull().sum()\n\nprint('Train Rows: ', train_data_shape[0],', Train Columns: ', train_data_shape[1])\nprint('Test Rows: ', test_data_shape[0],', Test Columns: ', test_data_shape[1])\n\nprint('\\nTrain data total nulls per column:\\n', train_total_nulls)\nprint('\\nTest data total nulls per column:\\n', test_total_nulls)","af09aeac":"#Drop columns that will not be used for the model\n\ntrain_data.drop(columns=['keyword'], inplace=True)\ntrain_data.drop(columns=['location'], inplace=True)\ntrain_data.drop(columns=['id'], inplace=True)\ntest_data.drop(columns=['keyword'], inplace=True)\ntest_data.drop(columns=['location'], inplace=True)","841ec51d":"train_data.head()","849996b4":"#Check if target column is balanced\nsum_disaster = train_data.loc[train_data['target'] == 1].target.sum()\nsum_notdisaster = train_data.loc[train_data['target'] == 0].target.add(1).sum()\n\nprint('Total tweets of disasters: ', sum_disaster)\nprint('Total tweets of not disasters: ',sum_notdisaster)","09e12fdc":"#lowercase all words\ntrain_data[\"lowercase_text\"] = train_data[\"text\"].str.lower()\ntest_data[\"lowercase_text\"] = test_data[\"text\"].str.lower()\n\ntrain_data.head()","03a9b932":"#Remove stopwords\n\n\ndef remove_stopwords(text):\n    text_tokens = word_tokenize(text)\n    tokens_without_sw = [word for word in text_tokens if not word in stopwords.words('english')]\n    return ' '.join(tokens_without_sw)\n\ntrain_data['lowercase_text']=train_data['lowercase_text'].apply(lambda x : remove_stopwords(x))\ntest_data['lowercase_text']=test_data['lowercase_text'].apply(lambda x : remove_stopwords(x))\n\ntrain_data.head()","6d2a1020":"#Remove urls and special characters\n\nstr_url = 'This is the tweet 1234 with an url: http:\/\/docs.python.org\/3\/library\/re.html #test'\n\ndef clean_text(text):\n    url = re.compile('http.?:\/\/[A-Za-z0-9.\/-]*')\n    clean_string = url.sub(r'',text)                   #Remove url\n    spchars = re.compile('[\\x24-\\x40]*')   \n    clean_string = spchars.sub(r'',clean_string)      #Remove special characters\n    repspace = re.compile('[ ]{2,}')\n    clean_string = repspace.sub(r' ',clean_string)   #Remove double space between words\n    repspace = re.compile('# ')\n    clean_string = repspace.sub(r'#',clean_string)   #Remove space after hashtag\n    return clean_string\n\nexmp_str = clean_text(str_url)\nprint(exmp_str)","c08916dd":"train_data['lowercase_text']=train_data['lowercase_text'].apply(lambda x : clean_text(x))\ntest_data['lowercase_text']=test_data['lowercase_text'].apply(lambda x : clean_text(x))\n\nprint('Clean text completed')\n","e28f7bee":"#Get x and y\ny = train_data[\"target\"].copy()\nx = train_data.drop(\"target\", axis=1)\nx.drop(columns=['text'], inplace=True)  #x to train the model\nx2 = test_data.drop(\"text\", axis=1) #x for the submission\nx2.drop(columns=['id'], inplace=True)\n\nx.head()","d51cd45c":"vectorizer =TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n#vectorizer2 =TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n\nx_vec = vectorizer.fit_transform(x['lowercase_text'])\nx2_vec = vectorizer.transform(x2['lowercase_text'])\n\nx_vec.todense()[0]\nx2_vec.todense()[0]\n\nx_vec_df = pd.DataFrame(\n    x_vec.todense(),\n    columns=vectorizer.get_feature_names()\n)\n\nx2_vec_df = pd.DataFrame(\n    x2_vec.todense(),\n    columns=vectorizer.get_feature_names()\n)\n\nx_vec_df.head()","0ca8c95f":"#Train\/Test will be used to validate the model\n#Train 80% y Test 20%\n\nX_train, X_test, y_train, y_test = train_test_split(x_vec_df, y, test_size=0.20, random_state=42)\n\nprint(\"Training set size\", X_train.shape)\nprint(\"Test set size\", X_test.shape)","abb75c38":"#Define models\nlogregression_model = LogisticRegression(solver='liblinear', random_state=10, C=2.5, intercept_scaling=1500)\nknn_model = KNeighborsClassifier(n_neighbors=5,n_jobs=-1)\nbayes_model = GaussianNB()\nrfmodel = RandomForestClassifier(max_depth=100,min_samples_split=100,random_state=10)\ndtmodel = DecisionTreeClassifier(max_depth=100,min_samples_split=100,random_state=10)","07621efe":"models_list = [logregression_model,knn_model,bayes_model,rfmodel,dtmodel]\n\n\nfor m in models_list:\n    m.fit(X_train, y_train)\n    score = m.score(X_test, y_test)\n    print(m, '\\n\\nSCORE: ', score,'\\n\\n\\n')","3251bf41":"# Plot non-normalized confusion matrix for model with best score - Logistic Regression\ndisp = plot_confusion_matrix(logregression_model, X_test, y_test, display_labels=['Not Disaster', 'Disaster'],normalize=None)\nplt.show()","0b34b5e0":"\npredictions = logregression_model.predict(x2_vec_df)\n\nsubmission = pd.DataFrame({'id':test_data['id'], 'target':predictions})\n\nsubmission.to_csv('submission.csv',index=False)\n\nsubmission.head(10)\n\n","f9ed94d1":"# Model","3fd8418f":"# Data analysis","5dbfc3a5":"# Data Cleaning","2a78e49f":"# Import Data","a2416488":"# Submission","7e0dee05":"# NLP\n\nC\u00e9sar Jim\u00e9nez Mena","3cebe2a9":"# Imports"}}