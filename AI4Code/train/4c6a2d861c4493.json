{"cell_type":{"bd40d057":"code","fd7d06cc":"code","2a536953":"code","0c79fc6a":"code","0f552d4f":"code","61b32ccb":"code","87643dcc":"code","a7c5eebc":"code","2ac23080":"code","7b1bcc2c":"code","1ecda923":"code","9ddebb08":"markdown","9d5913dd":"markdown","fc2969d8":"markdown","b5ee7bfb":"markdown","880a9d43":"markdown","600bcedf":"markdown","8702d11e":"markdown","52449c9b":"markdown","72c96f03":"markdown","9163c67a":"markdown","635c7e65":"markdown","68698030":"markdown"},"source":{"bd40d057":"# imports kaggle environment\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import (Observation, \n                                                                Configuration, \n                                                                Action, \n                                                                row_col, \n                                                                translate, \n                                                                greedy_agent)\nfrom kaggle_environments import evaluate, make, utils\n# import other necessary packages\nimport numpy as np\nimport random\nfrom tqdm import tqdm\n# install tf-agents\n!pip install -q tf-agents\n# other \nimport abc\nimport tensorflow as tf\nimport numpy as np\n\nfrom tf_agents.environments import py_environment\nfrom tf_agents.environments import tf_environment\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.environments import utils\nfrom tf_agents.specs import array_spec\nfrom tf_agents.environments import wrappers\nfrom tf_agents.environments import suite_gym\nfrom tf_agents.trajectories import time_step as ts\n\ntf.compat.v1.enable_v2_behavior()","fd7d06cc":"class TFHungryGoose(py_environment.PyEnvironment):\n    \"\"\"The TF-Agent ported env of HungryGoose\"\"\"\n\n    def __init__(self):\n        \"\"\"Init the env with basic info\n        \"\"\"\n        # first create the kaggle hungry goose env\n        env = make(\"hungry_geese\")\n        # init the trainer (wth 3 greedy adv)\n        self.trainer = env.train([None, greedy_agent, greedy_agent, greedy_agent])\n        obs = self.trainer.reset()\n        # there are 4 actions -- the 4 directions\n        self._action_spec = array_spec.BoundedArraySpec(\n            shape=(), dtype=np.int32, minimum=0, maximum=3, name='action')\n        # observation is a 7x11 2d matrix\n        self._observation_spec = array_spec.BoundedArraySpec(\n            shape=(1, 7, 11, 1), dtype=np.float32, minimum=0, maximum=10, name='observation')\n        # init state\n        self._state = self.create_grid_from_geese_position(obs)\n        # init ending state == False\n        self._episode_ended = False\n        # action mapping\n        self.action_name_mapping = {0: 'NORTH', 1: 'SOUTH', 2: 'EAST', 3: 'WEST'}\n\n    def create_grid_from_geese_position(self, obs, grid_cols=11, grid_rows=7):\n        \"\"\"Create a grid form the given geese positions and game board dimensions \n        Identifier ---\n        {0}: free space; {1, 2, 3, 4}: for 4 geese bodies, where 1 is me; {5}: food; \n        {6} for head of other agents and 7 for the head of mine agent\n        \"\"\"\n        # extract the reuired info frm obs\n        geese_position = obs.geese\n        foods = obs.food\n        my_index = obs.index # which should be 1\n        # create matrix with all a free space\n        matrix = np.zeros((grid_rows, grid_cols))\n        # for each geese position add a specific idenitfier in matrix\n        goose_id = [1, 2, 3, 4]\n        for i, goose_position in enumerate(geese_position):\n            for j, pos in enumerate(goose_position):\n                row, col = row_col(pos, grid_cols)\n                if j == 0:\n                    if i!=my_index: # mark as head\n                        matrix[row][col] = 6\n                    else:\n                        matrix[row][col] = 7\n                else:# normal body\n                    matrix[row][col] = goose_id[i]\n        # add identifier for the food    \n        np.put(matrix, foods, [5])\n        # return \n        return matrix.reshape(1, 7, 11, 1).astype('float32')\n\n    def action_spec(self):\n        return self._action_spec\n\n    def observation_spec(self):\n        return self._observation_spec\n\n    def _reset(self):\n        \"\"\"Reset the env\"\"\"\n        obs = self.trainer.reset()\n        self._state = self.create_grid_from_geese_position(obs)\n        self._episode_ended = False\n        return ts.restart(self._state)\n\n    def __reward_manager(self, reward, step, geese):\n        \"\"\"Modifying the default reward of the env\n        Mods:\n        1. Every step you survive, you get 50 rewards\n        2. Every food you eat gives you additonal 50 points\n        3. If you loose, you get -1000 points\n        4. If you win, you get 1000 points\n        5. First step reward is 200! -- remove this\n        \"\"\"\n        # \n        if step == 1 and (reward != 0): # first step and survived, return only survive reward\n            return 50\n        elif (reward == 0) or (len(geese[0])==0): # you loose, hence large neg reward\n            return -1000\n        # check if you won or not\n        elif (max([len(goose) for goose in geese[1:]]) == 0) and (reward != 0): #you just won\n            return 1000\n        elif (reward%100)==0: # you survived but not won nor ate, hence only survive reward\n            return 50\n        else: # you survived and ate, hence\n            return 100\n        \n    def _step(self, action):\n        \"\"\"Define the operation on env provided some action\"\"\"\n        \n        if self._episode_ended:\n            # The last action ended the episode. Ignore the current action and start\n            # a new episode.\n            return self.reset()\n        \n        # map the action to the env Action\n        action = self.action_name_mapping[int(action)]\n        \n        # perform the action; returned are -- (obs, reward, done, info)\n        obs, reward, self._episode_ended, info = self.trainer.step(action)\n        \n        # mod the reward\n        reward = self.__reward_manager(reward, obs.step, obs.geese)\n        # modify the state\n        self._state = self.create_grid_from_geese_position(obs)\n        \n        # handle the env termination or transition based on the outcome of action on the env\n        if self._episode_ended:\n            return ts.termination(self._state, reward)\n        else:\n            return ts.transition(self._state, reward=reward, discount=1.0)        ","2a536953":"## test the env on sample episodes\nenv = TFHungryGoose()\nutils.validate_py_environment(env, episodes=5)","0c79fc6a":"import base64\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom tf_agents.agents.dqn import dqn_agent\nfrom tf_agents.drivers import dynamic_step_driver\nfrom tf_agents.environments import suite_gym\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.networks import sequential\nfrom tf_agents.policies import random_tf_policy, policy_saver\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.trajectories import trajectory, time_step, policy_step\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.utils import common\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Conv2D, Flatten\nfrom keras.optimizers import Adam\n\n# !pip install wandb\nimport wandb","0f552d4f":"# Start a run, tracking hyperparameters\nconfig = {\n    'num_iterations' : 20000 ,\n\n    'initial_collect_steps' : 100  ,\n    'collect_steps_per_iteration' : 1  ,\n    'replay_buffer_max_length' : 100000  ,\n    \"replay_buffer_num_steps\": 2,\n    \n    'batch_size' : 64  ,\n    'learning_rate' : 1e-3  ,\n    'log_interval' : 1000  ,\n\n    'num_eval_episodes' : 10  ,\n    'eval_interval' : 10000  ,\n    }","61b32ccb":"# The QNetwork model\nmodel = sequential.Sequential([\n    Conv2D(64, kernel_size=3, activation=\"relu\"),\n    Conv2D(32, kernel_size=3, activation=\"relu\"),\n    Flatten(),\n    Dense(48, activation=\"relu\"),\n    Dense(24, activation=\"relu\"),\n    Dense(4, activation=None), # 4 actions hence last layer outputs 4\n])","87643dcc":"# create the envs to test and eval\ntrain_py_env = TFHungryGoose()\neval_py_env = TFHungryGoose()\n# convert the env to TF\ntrain_env = tf_py_environment.TFPyEnvironment(train_py_env)\neval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n\n# set the optimizer\noptimizer = tf.keras.optimizers.Adam(learning_rate=config['learning_rate'])\n\n# step counter\ntrain_step_counter = tf.Variable(0)\n\n# create the DQN agent\nagent = dqn_agent.DqnAgent(\n    train_env.time_step_spec(),\n    train_env.action_spec(),\n    q_network=model,\n    optimizer=optimizer,\n    td_errors_loss_fn=common.element_wise_squared_loss,\n    train_step_counter=train_step_counter)\n\n# init the agent\nagent.initialize()","a7c5eebc":"# get the policies\neval_policy = agent.policy\ncollect_policy = agent.collect_policy\n\n# func to get avg reward\ndef compute_avg_return(environment, policy, num_episodes=10):\n\n  total_return, won = 0.0, 0\n  for _ in range(num_episodes):\n\n    time_step = environment.reset()\n    episode_return = 0.0\n\n    while not time_step.is_last():\n      action_step = policy.action(time_step)\n      time_step = environment.step(action_step.action)\n      episode_return += time_step.reward\n    total_return += episode_return\n    if episode_return > 1000:\n        won+=1\n\n  avg_return = total_return \/ num_episodes\n  return avg_return, won \/ num_episodes\n\n# test the policy\n# compute_avg_return(TFHungryGoose(), eval_policy)\n\n## Expeience Replay\nreplay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n    data_spec=agent.collect_data_spec,\n    batch_size=train_env.batch_size,\n    max_length=config['replay_buffer_max_length'])\n\ndef collect_step(environment, policy, buffer):\n  time_step = environment.current_time_step()\n  action_step = policy.action(time_step)\n  next_time_step = environment.step(action_step.action)\n  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n  # Add trajectory to the replay buffer\n  buffer.add_batch(traj)\n\ndef collect_data(env, policy, buffer, steps):\n  for _ in range(steps):\n    collect_step(env, policy, buffer)\n\n# collect some initial random experiences\ncollect_data(train_env, collect_policy, replay_buffer, config['initial_collect_steps'])\n\n# Dataset generates trajectories with shape [Bx2x...]\ndataset = replay_buffer.as_dataset(\n#     num_parallel_calls=3, \n    sample_batch_size=config['batch_size'], \n    num_steps=config['replay_buffer_num_steps']).prefetch(3)\n\n# create an iterator\niterator = iter(dataset)\n\n# create a policy saver\nsaver = policy_saver.PolicySaver(eval_policy, batch_size=None)\n# checkpointer \ntrain_checkpointer = common.Checkpointer(\n        ckpt_dir=\".\/agent_checkpoint\",\n        max_to_keep=1,\n        agent=agent,\n        policy=agent.policy,\n        replay_buffer=replay_buffer,\n        global_step=train_step_counter\n    )\n# init or reset the check point -- load existing model if present\ntrain_checkpointer.initialize_or_restore()","2ac23080":"# (Optional) Optimize by wrapping some of the code in a graph using TF function.\nagent.train = common.function(agent.train)\n\n# Reset the train step\nagent.train_step_counter.assign(0)\n\n# Evaluate the agent's policy once before training.\navg_return = compute_avg_return(eval_env, agent.policy, config['num_eval_episodes'])\nreturns = [avg_return]\n\nfor _ in range(config['num_iterations']):\n\n  # Collect a few steps using collect_policy and save to the replay buffer.\n  collect_data(train_env, agent.collect_policy, replay_buffer, config['collect_steps_per_iteration'])\n\n  # Sample a batch of data from the buffer and update the agent's network.\n  experience, unused_info = next(iterator)\n  train_loss = agent.train(experience).loss\n\n  step = agent.train_step_counter\n\n  if step % config['log_interval'] == 0:\n    print('step = {0}: loss = {1}'.format(int(step), train_loss))\n\n  if step % config['eval_interval'] == 0:\n    avg_return, won = compute_avg_return(eval_env, agent.policy, config['num_eval_episodes'])\n    print('step = {0}: Average Return = {1}'.format(int(step), avg_return))\n    returns.append(avg_return)\n    # save checkpoint\n    train_checkpointer.save(train_step_counter)","7b1bcc2c":"# create TF-agent playing function\ndef tf_agent_play(obs, conf):\n    # get the state info\n    state = TFHungryGoose().create_grid_from_geese_position(obs)\n    # create timestep\n    timestep = time_step.TimeStep(\n        np.array(0, dtype='int32'),\n        np.array(0, dtype='float32'),\n        np.array(0, dtype='float32'),\n        np.array(state, dtype='float32')\n    )\n    # run the policy\n    action_step = agent.policy.action(timestep)\n    # extract the action\n    action = TFHungryGoose().action_name_mapping[int(action_step.action)]\n    #\n    return action\n\n# def go_west(obs, conf):\n#     return 'WEST'\n    \n# # test the agent\n# env_test = make(\"hungry_geese\", debug=True)\n# _ = env_test.run([tf_agent_play, go_west, go_west, go_west])\n# env_test.render(mode=\"ipython\", width=500, height=450)","1ecda923":"## evaluate the agent over multiple trials\nfrom joblib import Parallel, delayed\n\n# variable\ntrials = 10\n\n# run parallel test for 100\nresults = Parallel()( \n    delayed(evaluate)(\"hungry_geese\", [\n        tf_agent_play, \n        greedy_agent, \n        greedy_agent, \n        greedy_agent, \n    ], num_episodes=1) \nfor _ in range(trials) )\n\nmean_score = np.mean(results, axis=0).astype(np.int).flatten()\nprint(\"mean\", mean_score)\n\nmax_score = np.max( results, axis=0).astype(np.int).flatten()\nprint(\"max \", max_score)","9ddebb08":"### Import packages","9d5913dd":"### Observation\n\n- As observed from the evalutions, the trained model is not winning any competitions to say the least :)\n- This is expected, as we trained the agent for just 20k iterations and also we used quite simple architecture and hyperparameter.\n- That said, the motive was to port HungryGoose to Tf-Agent, so that we can try out more sophisticated algorithms and training procedures.\n- I have trained this same agent for ~1M iterations with little improvements.\n- Some areas I am planning to explore to improve the result are, \n    - Train for more iterations.\n    - Try other algorithms like DDQN with priority replay.\n    - Modify the state to encode more information\n    - Use more sophisticated Q_network (maybe more CNN layers)\n    - Modify the reward function.\n    \n    \n#### If you learned anything new please upvote the notebook. Thnx!!!","fc2969d8":"### Create the DQN agent","b5ee7bfb":"## Step 2: DQN agent on the ported HungryGoose env","880a9d43":"### Test the agent","600bcedf":"### Test the env, it should not throw any errors!","8702d11e":"### Create the Q-Network for the DQN\n\n- At the heart of the DQN is a NN which takes state as input layer and return Actions in the last layer.\n- We will use a simple Convolution based NN as we have a 2D grid as input.","52449c9b":"### Set the hyperparameter","72c96f03":"## TF-Agent based HungryGoose using DQN\n\n- Port the hungry goose env to Tf-Agent\n    - We create a 7x11 2D grid as the input to the DQN\n    - Output is the Q-value for each of the 4 possible actions.\n    - Note, tutorial link: https:\/\/www.tensorflow.org\/agents\/tutorials\/2_environments_tutorial\n   \n   \n- Train a simple DQN policy on the new environment\n    - Note, tutorial link: https:\/\/www.tensorflow.org\/agents\/tutorials\/1_dqn_tutorial","9163c67a":"### Train the agent!!","635c7e65":"### Create policies and ReplayBuffer for Experience Replay\n\n- Define the policies and create func to calculate avg reard from a policy.\n    Agents contain two policies:\n    - agent.policy \u2014 The main policy that is used for evaluation and deployment.\n    - agent.collect_policy \u2014 A second policy that is used for data collection.\n\n\n- Create the Experience Replay module\n    \n    - Each row of the replay buffer only stores a single observation step. \n    But since the DQN Agent needs both the current and next observation to compute the loss, \n    the dataset pipeline will sample two adjacent rows for each item in the batch (num_steps=2).\n    This dataset is also optimized by running parallel calls and prefetching data.\n\n","68698030":"## Step 1: Port HungryGoose to TF-Agent Env"}}