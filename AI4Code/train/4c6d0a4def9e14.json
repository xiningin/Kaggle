{"cell_type":{"e0480b97":"code","b2d0f2e5":"code","3c47796f":"code","36b59019":"code","d55db486":"code","2aaece56":"code","14b195ac":"code","9a6892da":"code","4e99a133":"code","93301fe7":"code","a417137f":"code","9a6e5b78":"code","f7bb988a":"code","13245b24":"code","c17d71ad":"code","a2d64cef":"code","1a5d6be5":"code","6b2ef4ff":"code","312569b1":"code","5aeb9ca5":"code","f552a8ff":"code","17140033":"code","f8551756":"code","ddca6e77":"code","0c2b60b3":"code","5729cfaf":"markdown","f21129c7":"markdown","7f44fddc":"markdown","d94d2a00":"markdown","655fcefc":"markdown","b2a9eb36":"markdown","8a70e6fc":"markdown","a8e00a9d":"markdown","079278f1":"markdown","7232ab5d":"markdown","4141b99b":"markdown"},"source":{"e0480b97":"import pickle\nimport numpy as np\nimport pandas as pd\n\nfrom scipy.sparse import csr_matrix, hstack\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()","b2d0f2e5":"times = ['time%s' % i for i in range(1, 11)]\nsites = ['site%s' % i for i in range(1, 11)]\n\ntrain = pd.read_csv('..\/input\/train_sessions.csv', parse_dates = times, index_col='session_id')\ntest = pd.read_csv('..\/input\/test_sessions.csv', parse_dates = times, index_col='session_id')\n\ntrain.sort_values(by='time1', inplace=True)\n\nidx = train.shape[0]\ndata = pd.concat([train, test], sort=False) # leave train.target for eda\n\ntrain.shape, test.shape, data.shape","3c47796f":"data[sites] = data[sites].fillna(0).astype(np.uint16) # float->int (55.0 -> 55)\n\n# for each row combine site_ids into one string separated by space\ndata['words'] = data[sites].astype(np.str).apply(' '.join, axis=1)\n\n#words = CountVectorizer(max_features=50000, ngram_range=(1, 3)).fit_transform(data['words'])\nwords = TfidfVectorizer(max_features=50000, ngram_range=(1, 3)).fit_transform(data['words'])\n\ndata.drop(['words'], inplace=True, axis=1)\nwords","36b59019":"model = LogisticRegression(random_state=17, solver='liblinear')\ntime_split = TimeSeriesSplit(n_splits=10)\ntrain.time1.min(), train.time1.max(), test.time1.min(), test.time1.max()","d55db486":"X_train = words[:idx]\ny_train = train.target\n\ncv_scores = cross_val_score(model, X_train, y_train, cv=time_split, scoring='roc_auc')\ncv_scores, cv_scores.mean()\n\n# 0.8670500571969433 CountVectorizer\n# 0.8664051910501502 TfidfVectorizer","2aaece56":"data['min'] = data[times].min(axis=1)\ndata['max'] = data[times].max(axis=1)\ndata['seconds'] = ((data['max'] - data['min']) \/ np.timedelta64(1, 's'))\ndata['minutes'] = ((data['max'] - data['min']) \/ np.timedelta64(1, 'm')).round(2)\ndata.drop(['min','max'], inplace=True, axis=1)\n\ndata['month'] = data['time1'].apply(lambda ts: ts.month+(12*(ts.year-2013))).astype(np.int8)\ndata['yyyymm'] = data['time1'].apply(lambda ts: 100 * ts.year + ts.month).astype(np.int32) # wtf! why this works?\ndata['mm'] = data['time1'].apply(lambda ts: ts.month).astype(np.int8)\ndata['yyyy'] = data['time1'].apply(lambda ts: ts.year).astype(np.int8)\n\ndata['dayofweek'] = data['time1'].apply(lambda ts: ts.dayofweek).astype(np.int8)\ndata['weekend'] = data['time1'].apply(lambda ts: ts.dayofweek > 5).astype(np.int8)\n\ndata['hour'] = data['time1'].apply(lambda ts: ts.hour).astype(np.int8)","14b195ac":"hosts = pd.read_pickle('..\/input\/site_dic.pkl')\nhosts = pd.DataFrame(data=list(hosts.keys()), index=list(hosts.values()), columns=['name']) # switch key and value\n\nhosts['split'] = hosts['name'].str.split('.')\nhosts['len'] = hosts['split'].map(lambda x: len(x)).astype(np.int8)\nhosts['domain'] = hosts['split'].map(lambda x: x[-1])\n\nhosts.drop(['name','split'], inplace=True, axis=1)\nhosts.index.rename('site1', inplace=True) # rename index for the future merge\ndata = pd.merge(data, hosts, how='left', on='site1')","9a6892da":"data.columns","4e99a133":"_, axes = plt.subplots(nrows=1, ncols=3, figsize=(22,6))\nsns.boxplot(x='target', y='minutes', data=data[:idx], ax=axes[0])\nsns.violinplot(x='target', y='minutes', data=data[:idx], ax=axes[1])\nsns.boxplot(x='target', y='seconds', data=data[:idx], ax=axes[2])\naxes[0].set_ylim(-1,5), axes[1].set_ylim(-1,5), axes[2].set_ylim(-30,300);","93301fe7":"data['short'] = data['minutes'].map(lambda x: x < 0.8).astype(np.int8)\ndata['long'] = data['minutes'].map(lambda x: x >= 0.8).astype(np.int8)","a417137f":"_, axes = plt.subplots(nrows=1, ncols=3, figsize=(22,4))\nsns.countplot(x=\"dayofweek\", data=data[data.target==1][:idx], ax=axes[0]) # Alice\nsns.countplot(x=\"dayofweek\", data=data[data.target==0][:idx], ax=axes[1]) # Not Alice\nsns.countplot(x=\"dayofweek\", data=data[idx:], ax=axes[2]); # Test","9a6e5b78":"data[\"online_day\"] = data['time1'].apply(lambda ts: ts.dayofweek in [0,1,3,4]).astype(np.int8)\ndata[\"mon\"] = data['time1'].apply(lambda ts: ts.dayofweek in [0]).astype(np.int8) # monday\ndata[\"wen\"] = data['time1'].apply(lambda ts: ts.dayofweek in [2]).astype(np.int8) # wensday\ndata[\"sun\"] = data['time1'].apply(lambda ts: ts.dayofweek in [6]).astype(np.int8) # sunday","f7bb988a":"_, axes = plt.subplots(nrows=1, ncols=3, figsize=(22,4))\nsns.countplot(x=\"month\", data=data[data.target==1][:idx], ax=axes[0]) # Alice\nsns.countplot(x=\"month\", data=data[data.target==0][:idx], ax=axes[1]) # Not Alice\nsns.countplot(x=\"month\", data=data[idx:], ax=axes[2]); # Test","13245b24":"agg = data[data.target==1].groupby(['mm']).seconds.agg({ 'mean', 'sum', 'count'})\nagg\n\n# TODO exploit aggregates\n# data = pd.merge(data, agg, how='left', on='mm')","c17d71ad":"_, axes = plt.subplots(nrows=1, ncols=3, figsize=(22,4))\nsns.countplot(x=\"hour\", data=data[data.target==1][:idx], ax=axes[0]) # Alice\nsns.countplot(x=\"hour\", data=data[data.target==0][:idx], ax=axes[1]) # Not Alice\nsns.countplot(x=\"hour\", data=data[idx:], ax=axes[2]); # Test","a2d64cef":"''' wtf?\ndata['morning'] = data['time1'].apply(lambda ts: (ts.hour >= 8) & (ts.hour < 12)).astype(np.int8)\ndata['day'] = data['time1'].apply(lambda ts: (ts.hour >= 12) & (ts.hour < 15)).astype(np.int8)\ndata['evening'] = data['time1'].apply(lambda ts: (ts.hour >= 15) & (ts.hour < 19)).astype(np.int8)\ndata['night'] = data['time1'].apply(lambda ts: (ts.hour >= 19) | (ts.hour < 8)).astype(np.int8) # or!\n'''\n\ndata['morning'] = data['time1'].apply(lambda ts: (ts.hour >= 7) & (ts.hour < 12)).astype(np.int8)\ndata['day'] = data['time1'].apply(lambda ts: (ts.hour >= 12) & (ts.hour < 18)).astype(np.int8)\ndata['evening'] = data['time1'].apply(lambda ts: (ts.hour >= 18) & (ts.hour < 23)).astype(np.int8)\ndata['night'] = data['time1'].apply(lambda ts: (ts.hour >= 23) | (ts.hour < 7)).astype(np.int8) # or!","1a5d6be5":"_, axes = plt.subplots(nrows=1, ncols=3, figsize=(22,4))\nsns.countplot(x=\"len\", data=data[data.target==1][:idx], ax=axes[0]) # Alice\nsns.countplot(x=\"len\", data=data[data.target==0][:idx], ax=axes[1]) # Not Alice\nsns.countplot(x=\"len\", data=data[idx:], ax=axes[2]); # Test","6b2ef4ff":"data['big_site'] = data['len'].apply(lambda x: x > 5).astype(np.int8)\ndata['typical_site'] = data['len'].apply(lambda x: x == 3).astype(np.int8)","312569b1":"_, axes = plt.subplots(nrows=1, ncols=1, figsize=(22,4))\nsns.countplot(x=\"domain\", data=data[data.target==1][:idx], ax=axes); # Alice","5aeb9ca5":"data['typical_domain'] = data['domain'].map(lambda x: x in ('com', 'fr', 'net', 'uk', 'org', 'tv')).astype(np.int)","f552a8ff":"data.drop(times + sites + ['target'], inplace=True, axis=1)\ndata.to_pickle('dump.pkl')\ndata.columns","17140033":"data = pd.read_pickle('dump.pkl')\n\n# commented columns go to model\ndata.drop([\n    'seconds', \n    'minutes', \n    'month', \n    #'yyyymm', \n    'mm', \n    'yyyy', \n    'dayofweek',\n    'weekend', \n    'hour', \n    'len', \n    'domain', \n    'short', \n    'long',\n    'online_day',\n    'mon',\n    'wen',\n    'sun',\n    #'morning', \n    #'day', \n    #'evening', \n    #'night', \n    'big_site',\n    'typical_site',\n    'typical_domain',\n], inplace=True, axis=1)\n\ndata = pd.get_dummies(data, columns=[\n    #'yyyy',\n    #'mm',\n    #'dayofweek',\n    #'hour',\n    #'len'\n])\n\nfeatures_to_scale = [\n    #'seconds',\n    #'minutes',\n    #'month',\n    'yyyymm',\n    #'dayofweek',\n    #'hour',\n    #'len',\n]\ndata[features_to_scale] = StandardScaler().fit_transform(data[features_to_scale])","f8551756":"X_train = csr_matrix(hstack([words[:idx], data[:idx]]))\ncv_scores = cross_val_score(model, X_train, y_train, cv=time_split, scoring='roc_auc')   \ndata.columns, cv_scores, cv_scores.mean()","ddca6e77":"X_train = csr_matrix(hstack([words[:idx], data[:idx]]))\ny_train = train.target\n\nparams = {\n    'C': np.logspace(-2, 2, 10),\n    'penalty': ['l1','l2']\n}\n\ngrid = GridSearchCV(estimator=model, param_grid=params, scoring='roc_auc', cv=time_split, verbose=1, n_jobs=-1)\ngrid.fit(X_train, y_train)\n\ngrid.best_estimator_, grid.best_score_, grid.best_params_","0c2b60b3":"model = grid.best_estimator_\nmodel.fit(X_train, y_train)\n\nX_test = csr_matrix(hstack([words[idx:], data[idx:]]))\ny_test = model.predict_proba(X_test)[:, 1]\n\nsubmission = pd.DataFrame({\"session_id\": test.index, \"target\": y_test})\nsubmission.to_csv('submission.csv', index=False)","5729cfaf":"# Alice","f21129c7":"Alice's sessions generaly shorter.","7f44fddc":"Noone is online at night. Alice has special hours.","d94d2a00":"Alice is active at at spring and some autumn months. She is a student.","655fcefc":"## Exploratory data analysis","b2a9eb36":"Alice is offline due wensday and weekend, but she is active on monday.","8a70e6fc":"## Try first model\nTest set follows train set on the timeline. Thats why we need TimeSeriesSplit.","a8e00a9d":"*In order to follow the rules of the course, which prohibit the publication of high performance notebooks, I left only a [basic](https:\/\/www.kaggle.com\/kashnitsky\/correct-time-aware-cross-validation-scheme) set of features (plus yyyymm from the 4th assignment).*","079278f1":"## Time and hosts features","7232ab5d":"## Sites\nThe main idea is to consider site_ids as words and sessions as sentences of words. This way we can use text processing tools like CountVectorizer and TfidfVectorizer with sessions.","4141b99b":"## Finding perfect features"}}