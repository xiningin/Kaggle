{"cell_type":{"e7930db2":"code","1b654046":"code","279f1abe":"code","b31f076d":"code","be59ec9d":"code","4a6cb990":"code","56ecd54c":"code","22948217":"code","627fef34":"code","16f842d0":"code","44917e7c":"code","2f649997":"code","c4ac8c67":"code","9f1104ad":"code","059d66f5":"code","f71ec9cd":"code","0f0dc2f2":"code","3ca0825d":"code","d7e8b34e":"code","13080d34":"code","a3f93b96":"code","80af901d":"code","27747915":"code","d9551825":"code","4979b4b7":"code","8629f5eb":"code","e4f52764":"code","cf629ed9":"code","4d5659a6":"code","5d60f217":"code","6a27c8f4":"code","d295389d":"code","c2105f54":"code","96a62f04":"code","c9850b6c":"code","964ddb80":"code","b8970b77":"code","eb40f21a":"code","34ae4605":"code","3c28fe19":"code","c817706b":"code","d3e76cc4":"code","2bbbdc58":"code","14e202c4":"code","76bd88f2":"code","5e1e36bf":"code","933f9782":"code","6369fcab":"code","b74f23e9":"code","00f0bf0b":"code","4fe8e1dc":"code","5d0a3e2c":"code","27c35e5e":"code","f5501e27":"code","cbd58135":"code","64c25c2c":"code","351676a5":"code","a42bb19b":"code","d8e4a254":"code","5ccbe3f6":"code","4113cd1b":"code","cffddefc":"code","09fbaae3":"code","ccd0ac3e":"code","02fe4e9c":"code","b118243b":"code","b21f948f":"code","2d9e84ec":"code","cad3e8a8":"code","3633c5fc":"code","fda1ec4c":"code","29115735":"code","9ca611d3":"code","0d7bb7b9":"code","1825d88e":"code","ea834bf1":"code","7ab8f436":"code","44d6e06c":"code","860cf7c5":"code","ab9e28c4":"code","4d6ddfce":"markdown","6d26c592":"markdown","8ac92814":"markdown","fc944ad0":"markdown","45e1fa1c":"markdown","d18aaa9b":"markdown","673f9cb8":"markdown","defb4090":"markdown","806afa71":"markdown","a4a3338c":"markdown","a3cb9425":"markdown","d6dc2af0":"markdown","ae2d75b8":"markdown","949e9d43":"markdown","480c6b9f":"markdown","33302ad1":"markdown","3cf304c4":"markdown","62298c93":"markdown","f412073b":"markdown","f802d8b8":"markdown","33994476":"markdown","573f2880":"markdown","da1b9b56":"markdown","e42f7852":"markdown","1a587fe1":"markdown","376f2c24":"markdown","8248cb86":"markdown","67994abb":"markdown","ffbd6d01":"markdown","d3ac0135":"markdown","28cec871":"markdown","cf94d70f":"markdown","9d23446f":"markdown","027d7e26":"markdown","1beed109":"markdown","706a02c6":"markdown","d4a786cb":"markdown","9654cd73":"markdown","a54ef4f7":"markdown","9c9c60c2":"markdown","34ef46ce":"markdown","52161045":"markdown","ab7e8fde":"markdown","26820514":"markdown","bd89f292":"markdown"},"source":{"e7930db2":"import time\nstart = time.time()\nimport os\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import StandardScaler\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom textblob import TextBlob\n\n#plotting\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#statistics & econometrics\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\n\n#model fiiting and selection\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import make_scorer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.linear_model import Lasso, Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor","1b654046":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","279f1abe":"df = pd.read_csv(\"..\/input\/stocknews\/Combined_News_DJIA.csv\",low_memory=False,\n                    parse_dates=[0])\n\nfull_stock = pd.read_csv(\"..\/input\/stocknews\/DJIA_table.csv\",low_memory=False,\n                    parse_dates=[0])\n\n#add the closing stock value to the df - this will be the y variable\ndf[\"Close\"]=full_stock.Close\n\n#show how the dataset looks like\ndf.head(5)","b31f076d":"#drop the label column\ndf = df.drop([\"Label\"], axis=1)","be59ec9d":"#check for NAN\ndf.isnull().sum()","4a6cb990":"df = df.replace(np.nan, ' ', regex=True)\n\n#sanity check\ndf.isnull().sum().sum()","56ecd54c":"df = df.replace('b\\\"|b\\'|\\\\\\\\|\\\\\\\"', '', regex=True)\ndf.head(2)","22948217":"Anakin = SentimentIntensityAnalyzer()\n\nAnakin.polarity_scores(\" \")","627fef34":"def detect_subjectivity(text):\n    return TextBlob(text).sentiment.subjectivity\n\ndetect_subjectivity(\" \") #should return 0","16f842d0":"#get the headline columns' names\ncols = []\nfor i in range(1,26):\n    col = (\"Top{}\".format(i))\n    cols.append(col)","44917e7c":"start_vect=time.time()\nprint(\"ANAKIN: 'Intializing the process..'\")\n\n#get the name of the headline columns\ncols = []\nfor i in range(1,26):\n    col = (\"Top{}\".format(i))\n    cols.append(col)\n\n\nfor col in cols:\n    df[col] = df[col].astype(str) # Make sure data is treated as a string\n    df[col+'_comp']= df[col].apply(lambda x:Anakin.polarity_scores(x)['compound'])\n    df[col+'_sub'] = df[col].apply(detect_subjectivity)\n    print(\"{} Done\".format(col))\n    \nprint(\"VADER: Vaderization completed after %0.2f Minutes\"%((time.time() - start_vect)\/60))","2f649997":"#the text isn't required anymore\ndf = df.drop(cols,axis=1)\ndf.head(5)","c4ac8c67":"comp_cols = []\nfor col in cols:\n    comp_col = col + \"_comp\"\n    comp_cols.append(comp_col)\n\nw = np.arange(1,26,1).tolist()\nw.reverse()\n\nweighted_comp = []\nmax_comp = []\nmin_comp = []\nfor i in range(0,len(df)):\n    a = df.loc[i,comp_cols].tolist()\n    weighted_comp.append(np.average(a, weights=w))\n    max_comp.append(max(a))\n    min_comp.append(min(a))\n\ndf['compound_mean'] = weighted_comp\ndf['compound_max'] = max_comp\ndf['compound_min'] = min_comp\n\n\nsub_cols = []\nfor col in cols:\n    sub_col = col + \"_sub\"\n    sub_cols.append(sub_col)\n\n\nweighted_sub = []\nmax_sub = []\nmin_sub = []\nfor i in range(0,len(df)):\n    a = df.loc[i,sub_cols].tolist()\n    weighted_sub.append(np.average(a, weights=w))\n    max_sub.append(max(a))\n    min_sub.append(min(a))\n\ndf['subjectivity_mean'] = weighted_sub\ndf['subjectivity_max'] = max_sub\ndf['subjectivity_min'] = min_sub\n\nto_drop = sub_cols+comp_cols\ndf = df.drop(to_drop, axis=1)","9f1104ad":"df.head(5)","059d66f5":"fig1 = go.Figure()\nfig1.add_trace(go.Scatter(x=df.Date, y=df.Close,\n                    mode='lines'))\ntitle = []\ntitle.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,\n                              xanchor='left', yanchor='bottom',\n                              text='Development of stock values from Aug, 2008 to Jun, 2016',\n                              font=dict(family='Arial',\n                                        size=30,\n                                        color='rgb(37,37,37)'),\n                              showarrow=False))\nfig1.update_layout(xaxis_title='Date',\n                   yaxis_title='Closing stock value (in $)',\n                  annotations=title)\nfig1.show()","f71ec9cd":"#function for quick plotting and testing of stationarity\ndef stationary_plot(y, lags=None, figsize=(12, 7), style='bmh'):\n    \"\"\"\n        Plot time series, its ACF and PACF, calculate Dickey\u2013Fuller test\n        \n        y - timeseries\n        lags - how many lags to include in ACF, PACF calculation\n    \"\"\"\n    if not isinstance(y, pd.Series):\n        y = pd.Series(y)\n        \n    with plt.style.context(style):    \n        fig = plt.figure(figsize=figsize)\n        layout = (2, 2)\n        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n        acf_ax = plt.subplot2grid(layout, (1, 0))\n        pacf_ax = plt.subplot2grid(layout, (1, 1))\n        \n        y.plot(ax=ts_ax)\n        p_value = sm.tsa.stattools.adfuller(y)[1]\n        ts_ax.set_title('Time Series Analysis Plots\\n Dickey-Fuller: p={0:.5f}'.format(p_value))\n        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n        plt.tight_layout()","0f0dc2f2":"stationary_plot(df.Close)","3ca0825d":"diff = df.Close - df.Close.shift(7)\nstationary_plot(diff[7:])","d7e8b34e":"diff2 = diff - diff.shift(1)\nstationary_plot(diff2[7+1:], lags=60)","13080d34":"fig2 = go.Figure()\nfig2.add_trace(go.Scatter(x=df.Date, y=df.compound_mean,\n                    mode='lines',\n                    name='Mean'))\nfig2.add_trace(go.Scatter(x=df.Date, y=df.compound_max,\n                    mode='lines',\n                    name='Maximum'))\nfig2.add_trace(go.Scatter(x=df.Date, y=df.compound_min,\n                    mode='lines',\n                    name='Minimum'))\ntitle = []\ntitle.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,\n                              xanchor='left', yanchor='bottom',\n                              text='Development of sentiment compound score',\n                               font=dict(family='Arial',\n                                       size=30,\n                                        color='rgb(37,37,37)'),\n                              showarrow=False))\nfig2.update_layout(xaxis_title='Date',\n                   yaxis_title='Compound score',\n                  annotations=title)\nfig2.show()","a3f93b96":"compm_hist = px.histogram(df, x=\"compound_mean\")\ncompm_hist.show()","80af901d":"fig3 = go.Figure()\nfig3.add_trace(go.Scatter(x=df.Date, y=df.subjectivity_mean,\n                    mode='lines',\n                    name='Mean'))\nfig3.add_trace(go.Scatter(x=df.Date, y=df.subjectivity_min,\n                    mode='lines',\n                    name='Min'))\nfig3.add_trace(go.Scatter(x=df.Date, y=df.subjectivity_max,\n                    mode='lines',\n                    name='Max'))\ntitle = []\ntitle.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,\n                              xanchor='left', yanchor='bottom',\n                              text='Development of subjectivity score',\n                              font=dict(family='Arial',\n                                        size=30,\n                                        color='rgb(37,37,37)'),\n                              showarrow=False))\nfig3.update_layout(xaxis_title='Date',\n                   yaxis_title='Subjectivity score',\n                  annotations=title)\nfig3.show()","27747915":"subm_hist = px.histogram(df, x=\"subjectivity_mean\")\nsubm_hist.show()","d9551825":"df.describe()","4979b4b7":"def unique_ratio (col):\n    return len(np.unique(col))\/len(col)\n\ncols = ['Close', 'compound_mean', 'compound_max', 'compound_min', 'subjectivity_mean', 'subjectivity_max', 'subjectivity_min']\n\nur = []\nvar = []\nfor col in cols:\n    ur.append(unique_ratio(df[col]))\n    var.append(np.var(df[col]))\n    \nfeature_sel = pd.DataFrame({'Column': cols, \n              'Unique': ur,\n              'Variance': var})\nfeature_sel","8629f5eb":"sel_fig = go.Figure(data=go.Scatter(\n    x=feature_sel.Column,\n    y=feature_sel.Unique,\n    mode='markers',\n    marker=dict(size=(feature_sel.Unique*100)),\n))\nsel_fig.update_layout(title='Ratio of unique values', \n                      yaxis_title='Unique ratio')\nsel_fig.show()","e4f52764":"drop = ['subjectivity_min', 'subjectivity_max']\nclean_df = df.drop(drop,axis=1)","cf629ed9":"lag_df = clean_df.copy()\nlag_df.head(3)","4d5659a6":"to_lag = list(lag_df.columns)\nto_lag_4 = to_lag[1]\nto_lag_1 = to_lag[2:len(to_lag)]","5d60f217":"#lagging text features two days back\nfor col in to_lag_1:\n    for i in range(1,3):\n        new_name = col + ('_lag_{}'.format(i))\n        lag_df[new_name] = lag_df[col].shift(i)\n    \n#lagging closing values 4 days back\nfor i in range(1, 5):\n    new_name = to_lag_4 + ('_lag_{}'.format(i))\n    lag_df[new_name] = lag_df[to_lag_4].shift(i)","6a27c8f4":"#Show many rows need to be removed\nlag_df.head(10) ","d295389d":"lag_df = lag_df.drop(lag_df.index[[np.arange(0,4)]])\nlag_df = lag_df.reset_index(drop=True)\n\n#sanity check for NaNs\nlag_df.isnull().sum().sum()","c2105f54":"lag_df.head(5)","96a62f04":"# for time-series cross-validation set 10 folds \ntscv = TimeSeriesSplit(n_splits=10)","c9850b6c":"def mape(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100\nscorer = make_scorer(mean_squared_error)\nscaler = StandardScaler()   ","964ddb80":"def ts_train_test_split(X, y, test_size):\n    \"\"\"\n        Perform train-test split with respect to time series structure\n    \"\"\"\n    \n    # get the index after which test set starts\n    test_index = int(len(X)*(1-test_size))\n    \n    X_train = X.iloc[:test_index]\n    y_train = y.iloc[:test_index]\n    X_test = X.iloc[test_index:]\n    y_test = y.iloc[test_index:]\n    \n    return X_train, X_test, y_train, y_test","b8970b77":"X = lag_df.drop(['Close'],axis=1)\nX.index = X[\"Date\"]\nX = X.drop(['Date'],axis=1)\ny = lag_df.Close\n\nX_train, X_test, y_train, y_test = ts_train_test_split(X, y, test_size = 0.2)\n\n#sanity check\n(len(X_train)+len(X_test))==len(X)","eb40f21a":"#function for plotting coeficients of models (lasso and XGBoost)\ndef plotCoef(model,train_x):\n    \"\"\"\n        Plots sorted coefficient values of the model\n    \"\"\"\n    \n    coefs = pd.DataFrame(model.coef_, train_x.columns)\n    coefs.columns = [\"coef\"]\n    coefs[\"abs\"] = coefs.coef.apply(np.abs)\n    coefs = coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\n    \n    plt.figure(figsize=(15, 7))\n    coefs.coef.plot(kind='bar')\n    plt.grid(True, axis='y')\n    plt.hlines(y=0, xmin=0, xmax=len(coefs), linestyles='dashed');","34ae4605":"econ_cols = list(X_train.columns)\necon_cols = econ_cols[12:17]\nX_train_e = X_train[econ_cols]\nX_test_e = X_test[econ_cols]\ny_train_e = y_train\ny_test_e = y_test","3c28fe19":"econ_perf = pd.DataFrame(columns=['Model','MSE', 'SD'])\necon_perf","c817706b":"ridge_param = {'model__alpha': list(np.arange(0.001,1,0.001))}\nridge = Ridge(max_iter=5000)\npipe = Pipeline([\n    ('scale', scaler),\n    ('model', ridge)])\nsearch_ridge = GridSearchCV(estimator=pipe,\n                          param_grid = ridge_param,\n                          scoring = scorer,\n                          cv = tscv,\n                          n_jobs=4\n                         )\nsearch_ridge.fit(X_train_e, y_train_e)","d3e76cc4":"ridge_e = search_ridge.best_estimator_\n\n#get cv results of the best model + confidence intervals\nfrom sklearn.model_selection import cross_val_score\ncv_score = cross_val_score(ridge_e, X_train_e, y_train_e, cv=tscv, scoring=scorer)\necon_perf = econ_perf.append({'Model':'Ridge', 'MSE':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)\nridge_e","2bbbdc58":"plotCoef(ridge_e['model'], X_train_e)","14e202c4":"coefs = ridge_e['model'].coef_\nridge_coefs = pd.DataFrame({'Coef': coefs,\n                           'Name': list(X_train_e.columns)})\nridge_coefs[\"abs\"] = ridge_coefs.Coef.apply(np.abs)\nridge_coefs = ridge_coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\nridge_coefs","76bd88f2":"econ_perf","5e1e36bf":"rf_param = {'model__n_estimators': [10, 100, 300],\n            'model__max_depth': [10, 20, 30, 40],\n            'model__min_samples_split': [2, 5, 10],\n            'model__min_samples_leaf': [1, 2, 3],\n            'model__max_features': [\"auto\", 'sqrt']}","933f9782":"rf = RandomForestRegressor()\npipe = Pipeline([\n    ('scale', scaler),\n    ('model', rf)])\ngridsearch_rf = GridSearchCV(estimator=pipe,\n                          param_grid = rf_param,\n                          scoring = scorer,\n                          cv = tscv,\n                          n_jobs=4,\n                          verbose=3\n                         )","6369fcab":"gridsearch_rf.fit(X_train_e, y_train_e)","b74f23e9":"rf_e = gridsearch_rf.best_estimator_\n\n#get cv results of the best model + confidence intervals\ncv_score = cross_val_score(rf_e, X_train_e, y_train_e, cv=tscv, scoring=scorer)\necon_perf = econ_perf.append({'Model':'RF', 'MSE':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)","00f0bf0b":"xgb_param = {'model__lambda': list(np.arange(0.1,3, 0.1)), #L2 regularisation\n             'model__alpha': list(np.arange(0.1,3, 0.1)),  #L1 regularisation\n            }","4fe8e1dc":"xgb = XGBRegressor(booster='gblinear', feature_selector='shuffle', objective='reg:squarederror')\n\npipe = Pipeline([\n    ('scale', scaler),\n    ('model', xgb)])\ngridsearch_xgb = GridSearchCV(estimator=pipe,\n                          param_grid = xgb_param,\n                          scoring = scorer,\n                          cv = tscv,\n                          n_jobs=4,\n                          verbose=3\n                         )","5d0a3e2c":"gridsearch_xgb.fit(X_train_e, y_train_e)","27c35e5e":"xgb_e = gridsearch_xgb.best_estimator_\n\n#get cv results of the best model + confidence intervals\ncv_score = cross_val_score(xgb_e, X_train_e, y_train_e, cv=tscv, scoring=scorer)\necon_perf = econ_perf.append({'Model':'XGB', 'MSE':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)\nxgb_e","f5501e27":"print(econ_perf)","cbd58135":"econ_fig = px.scatter(econ_perf, x=\"Model\", y='MSE', color='Model', error_y=\"SD\")\necon_fig.update_layout(title_text=\"Performance of models trained on lags of y\")\necon_fig.show()","64c25c2c":"X_train_n = X_train.drop(econ_cols, axis=1)\nX_test_n = X_test.drop(econ_cols, axis=1)\ny_train_n = y_train\ny_test_n = y_test","351676a5":"nlp_perf = pd.DataFrame(columns=['Model','MSE', 'SD'])\nnlp_perf","a42bb19b":"ridge_param = {'model__alpha': list(np.arange(1,10,0.1))}\nridge = Ridge(max_iter=5000)\npipe = Pipeline([\n    ('scale', scaler),\n    ('model', ridge)\n])\nsearch_ridge = GridSearchCV(estimator=pipe,\n                          param_grid = ridge_param,\n                          scoring = scorer,\n                          cv = tscv,\n                          n_jobs=4\n                         )\nsearch_ridge.fit(X_train_n, y_train_n)","d8e4a254":"ridge_n = search_ridge.best_estimator_\n\n#get cv results of the best model + confidence intervals\ncv_score = cross_val_score(ridge_n, X_train_n, y_train_n, cv=tscv, scoring=scorer)\nnlp_perf = nlp_perf.append({'Model':'Ridge', 'MSE':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)\nridge_n","5ccbe3f6":"plotCoef(ridge_n['model'], X_train_n)\n\ncoefs = ridge_n['model'].coef_\nridge_coefs = pd.DataFrame({'Coef': coefs,\n                           'Name': list(X_train_n.columns)})\nridge_coefs[\"abs\"] = ridge_coefs.Coef.apply(np.abs)\nridge_coefs = ridge_coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\nridge_coefs","4113cd1b":"mape(y_test, ridge_n.predict(X_test_n))","cffddefc":"rf_param = {'model__n_estimators': [10, 100, 300],\n            'model__max_depth': [10, 20, 30, 40],\n            'model__min_samples_split': [2, 5, 10],\n            'model__min_samples_leaf': [1, 2, 3],\n            'model__max_features': [\"auto\", 'sqrt']}\nrf = RandomForestRegressor()\npipe = Pipeline([\n    ('scale', scaler),\n    ('model', rf)])\ngridsearch_rf = GridSearchCV(estimator=pipe,\n                          param_grid = rf_param,\n                          scoring = scorer,\n                          cv = tscv,\n                          n_jobs=4,\n                          verbose=3\n                         )\ngridsearch_rf.fit(X_train_n, y_train_n)","09fbaae3":"rf_n = gridsearch_rf.best_estimator_\n\n#get cv results of the best model + confidence intervals\ncv_score = cross_val_score(rf_n, X_train_n, y_train_n, cv=tscv, scoring=scorer)\nnlp_perf = nlp_perf.append({'Model':'RF', 'MSE':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)","ccd0ac3e":"xgb_param = {'model__lambda': list(np.arange(1,10, 1)), #L2 regularisation\n             'model__alpha': list(np.arange(1,10, 1)),  #L1 regularisation\n            }\nxgb = XGBRegressor(booster='gblinear', feature_selector='shuffle', objective='reg:squarederror')\n\npipe = Pipeline([\n    ('scale', scaler),\n    ('model', xgb)])\ngridsearch_xgb = GridSearchCV(estimator=pipe,\n                          param_grid = xgb_param,\n                          scoring = scorer,\n                          cv = tscv,\n                          n_jobs=4,\n                          verbose=3\n                         )\ngridsearch_xgb.fit(X_train_n, y_train_n)","02fe4e9c":"xgb_n = gridsearch_xgb.best_estimator_\n\n#get cv results of the best model + confidence intervals\ncv_score = cross_val_score(xgb_n, X_train_n, y_train_n, cv=tscv, scoring=scorer)\nnlp_perf = nlp_perf.append({'Model':'XGB', 'MSE':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)\nxgb_n","b118243b":"print(nlp_perf)","b21f948f":"nlp_fig = px.scatter(nlp_perf, x=\"Model\", y='MSE', color='Model', error_y=\"SD\")\n#nlp_fig.update_layout(title_text=\"Performance of models trained on NLP features\",\nnlp_fig.show()","2d9e84ec":"en_perf = pd.DataFrame(columns=['Model','MSE', 'SD'])\nen_perf","cad3e8a8":"ridge_param = {'model__alpha': list(np.arange(0.1,1,0.01))}\nridge = Ridge(max_iter=5000)\npipe = Pipeline([\n    ('scale', scaler),\n    ('model', ridge)])\nsearch_ridge = GridSearchCV(estimator=pipe,\n                          param_grid = ridge_param,\n                          scoring = scorer,\n                          cv = tscv,\n                          n_jobs=4,\n                          verbose=3\n                         )\nsearch_ridge.fit(X_train, y_train)","3633c5fc":"ridge_en = search_ridge.best_estimator_\n\n#get cv results of the best model + confidence intervals\ncv_score = cross_val_score(ridge_en, X_train, y_train, cv=tscv, scoring=scorer)\nen_perf = en_perf.append({'Model':'Ridge', 'MSE':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)\nridge_en","fda1ec4c":"coefs = ridge_en['model'].coef_\nridge_coefs = pd.DataFrame({'Coef': coefs,\n                           'Name': list(X_train.columns)})\nridge_coefs[\"abs\"] = ridge_coefs.Coef.apply(np.abs)\nridge_coefs = ridge_coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\nridge_coefs","29115735":"plotCoef(ridge_en['model'], X_train)","9ca611d3":"rf_param = {'model__n_estimators': [10, 100, 300],\n            'model__max_depth': [10, 20, 30, 40],\n            'model__min_samples_split': [2, 5, 10],\n            'model__min_samples_leaf': [1, 2, 3],\n            'model__max_features': [\"auto\", 'sqrt']}\nrf = RandomForestRegressor()\npipe = Pipeline([\n    ('scale', scaler),\n    ('model', rf)])\ngridsearch_rf = GridSearchCV(estimator=pipe,\n                          param_grid = rf_param,\n                          scoring = scorer,\n                          cv = tscv,\n                          n_jobs=4,\n                          verbose=3\n                         )\ngridsearch_rf.fit(X_train, y_train)","0d7bb7b9":"rf_en = gridsearch_rf.best_estimator_\n\n#get cv results of the best model + confidence intervals\ncv_score = cross_val_score(rf_en, X_train, y_train, cv=tscv, scoring=scorer)\nen_perf = en_perf.append({'Model':'RF', 'MSE':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)\nrf_en","1825d88e":"xgb_param = {'model__lambda': list(np.arange(1,10, 1)), #L2 regularisation\n             'model__alpha': list(np.arange(1,10, 1)),  #L1 regularisation\n            }\nxgb = XGBRegressor(booster='gblinear', feature_selector='shuffle', objective='reg:squarederror')\n\npipe = Pipeline([\n    ('scale', scaler),\n    ('model', xgb)])\ngridsearch_xgb = GridSearchCV(estimator=pipe,\n                          param_grid = xgb_param,\n                          scoring = scorer,\n                          cv = tscv,\n                          n_jobs=4,\n                          verbose=3\n                         )\ngridsearch_xgb.fit(X_train, y_train)","ea834bf1":"xgb_en = gridsearch_xgb.best_estimator_\n\n#get cv results of the best model + confidence intervals\ncv_score = cross_val_score(xgb_en, X_train, y_train, cv=tscv, scoring=scorer)\nen_perf = en_perf.append({'Model':'XGB', 'MSE':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)\nxgb_en","7ab8f436":"from sklearn.model_selection import cross_val_predict\n\nX_train_stack = pd.DataFrame(pd.DataFrame(columns=['econ_r', 'nlp_r']))\nX_train_stack['econ_r'] = cross_val_predict(ridge_e, X_train_e, y_train, cv=10)\nX_train_stack['nlp_r'] = cross_val_predict(ridge_n, X_train_n, y_train, cv=10)\n\nX_test_stack = pd.DataFrame(pd.DataFrame(columns=['econ_r', 'nlp_r']))\nX_test_stack['econ_r'] = ridge_e.predict(X_test_e)\nX_test_stack['nlp_r'] = ridge_n.predict(X_test_n)\n\nX_train_stack.to_csv(\"Stack_train.csv\")\nX_test_stack.to_csv(\"Stack_test.csv\")\n\nfrom sklearn.linear_model import ElasticNetCV\nstack = ElasticNetCV(cv=tscv)\nstack.fit(X_train_stack, y_train)\ncv_score = cross_val_score(stack, X_train_stack, y_train, cv=tscv, scoring=scorer)\nstack_performance = {'Model':'XGB', 'MSE':np.mean(cv_score), 'SD':(np.std(cv_score))}\nstack_performance\n\nmape(y_test, stack.predict(X_test_stack))","44d6e06c":"coefs = stack.coef_\nridge_coefs = pd.DataFrame({'Coef': coefs,\n                           'Name': list(X_train_stack.columns)})\nridge_coefs[\"abs\"] = ridge_coefs.Coef.apply(np.abs)\nridge_coefs = ridge_coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\nprint(ridge_coefs)\nplotCoef(stack, X_train_stack)","860cf7c5":"prediction_compare = pd.DataFrame(pd.DataFrame(columns=['y_true', 'econ_r', 'econ_rf', 'econ_x', 'nlp_r', 'nlp_rf', 'nlp_x', 'comb_r', 'comb_rf', 'comb_x', 'stack']))\nprediction_compare['y_true'] = y_test\nprediction_compare['econ_r'] = ridge_e.predict(X_test_e)\nprediction_compare['econ_rf'] = rf_e.predict(X_test_e)\nprediction_compare['econ_x'] = xgb_e.predict(X_test_e)\nprediction_compare['nlp_r'] = ridge_n.predict(X_test_n)\nprediction_compare['nlp_rf'] = rf_n.predict(X_test_n)\nprediction_compare['nlp_x'] = xgb_n.predict(X_test_n)\nprediction_compare['comb_r'] = ridge_en.predict(X_test)\nprediction_compare['comb_rf'] = rf_en.predict(X_test)\nprediction_compare['comb_x'] = xgb_en.predict(X_test)\nprediction_compare['stack'] = stack.predict(X_test_stack)\n\nprediction_compare.sample(3)","ab9e28c4":"econ_perf.to_csv(\"econ_perf.csv\")\nnlp_perf.to_csv(\"nlp_perf.csv\")\nen_perf.to_csv(\"en_perf.csv\")\nprediction_compare.to_csv('compare_predictions.csv')\nX_test.to_csv('X_test.csv')","4d6ddfce":"Above we can see that the first 7 rows now have missing values. Let's delete them and reset index.","6d26c592":"# 2. Data cleaning","8ac92814":"Next we'll look at some descriptive statistics about the data.","fc944ad0":"Now we plot distribution of the subjectivity scores as well.","45e1fa1c":"Next we look at the compound sentiment scores.","d18aaa9b":"##\u00a0Ridge regression","673f9cb8":"## Random Forest","defb4090":"There are a few headlines missing. Let's fill them in with a whitespace.","806afa71":"### Remove the HTML tags\nThere are several non-word tags in the headlines that would just bias the sentiment analysis so we need to remove them and replace with ''. This can be done with regex.","a4a3338c":"# 1. Load the data","a3cb9425":"## Random forest","d6dc2af0":"# 8. Model training\nLet's train 3 ML models. We'll do this in 2 rounds. First, using the econometric features alone (7 lags of y). Second, including the information extracted from the headlines (compound, subjectivity and their lags)\n\n**Models**\n- Ridge regression - punish model for using too many features but doesn't allow the coeficients drop to zero completely\n- Random forest\n- XGBoost\n\nWe'll score all models by mean squared error as it gives higher penalty to larger mistakes.\nAnd before each model training we'll standardize the training data.\n","ae2d75b8":"# Stacking classification and regression\nUsing NLP features in the above models turned out to by highly ineffective. There are, however some problems of the econometric models that NLP features might be able to solve. From other kernels you can see that they can be used for predicting whether the stock value will go up or down. In following section, we'll train direction classifier with NLP features and use the output of this model to improve the econometric models.","949e9d43":"# 8.1 Econometric models\nFirst let's train models using only the lags of the y variable (i.e. Close).","480c6b9f":"### NA treatment\nWe'll simply fill the NAs in the numerical features (Date, Close). \nIn the text features we'll fill the missing values with ''.","33302ad1":"# Sentiment and subjectivity score extraction\nNow I run the sentiment analysis extracting the compound score that goes from -0.5 (most negative) to 0.5 (most positive). I'm going to use the \"dirty\" texts in this part because VADER can utilize the information such as ALL CAPS, punctuation, etc. I'll also calculate the subjectivity of each headline using the TextBlob package.\n\nInitialise the VADER analyzer.","3cf304c4":"Let's also plot the distribution of the compound score.","62298c93":"The 3 models are performing quite similary. They might be useful candidates for stacking.","f412073b":"# 5. Lag the extracted features\nTo allow the models to look into the past, we'll add features which are essentially just copies of rows from n-steps back. In order to not create too many new features we'll add only features from 1 week prior to the current datapoint.\n","f802d8b8":"## XGBoost","33994476":"# NLP for stock market prediction","573f2880":"Plot performance of econ models including error bars","da1b9b56":"The cost function to minimize is mean squared error because this function assigns cost proportionally to the error size. The mean absolute percentage error will be used for plotting and easier interpretation. It's much easier to understand the errors of a model in terms of percentage.\nEach training set is scaled (normalized) independently to minimize data leakage.","e42f7852":"### Ridge regression","1a587fe1":"# Try stacking econometric and NLP models","376f2c24":"In this process, rows with NAs were created. Unfortunately these rows will have to be removed since we simply don't have the data from the future.","8248cb86":"Summarise the compound and subjectivity scores weighted by rating of the headline (top1 has the most weight)","67994abb":"Write a function to save the subjectivity score directly from TextBlob function's output. Subjectivity score might detect direct quotes in the headlines and positive stuff is rarely quoted in the headline.","ffbd6d01":"## NLP models\nLet's try now predict the stock value using only information from the news headlines.","d3ac0135":"# Model comparison","28cec871":"## Random Forest","cf94d70f":"# Econ+NLP models\nLet's use all features now\n\n## Ridge regression","9d23446f":"The first step will be creating folds for cross-validation. We'll use the same folds for all models in order to allow for creating a meta-model. Since we're working with timeseries the folds cannot be randomly selected. Instead a fold will be a sequence of data so that we don't lose the time information.","027d7e26":"# Feature selection\nI'm not going to use many FS methods since the features were mostly handcrafted. So we'll simply look at their variance and proportion of unique values.","1beed109":"Compound maximum and minimum are potentially less interesting as only ~18% of their values are unique. Also maximum of subjectivity has very low values. Minimum subjectivity has contains almost only 0. We'll drop the subjectivity min and max.","706a02c6":"So that's a very unstationary timeseries. Although we won't need it for our models it might still be interesting to try to make the data stationary. Challenge accepted.","d4a786cb":"It is quite obvious that the timeseries isn't stationary at all. There just seems to be a downwards trend over the time.","9654cd73":"Now this is a relatively stationary...well it's something oscilating around zero. From these plots the parameters for a SARIMA model can be inferred. However, this is for now beond the scope of this kernel.","a54ef4f7":"# 4. Explorative Data Analysis\n\nFirst, the timeseries of the y (to be predicted) variable will be explored. It's likely the the timeseries isn't stationary which however doesn't worry us in this case as the models won't be of the classical timeseries methods family.\n","9c9c60c2":"So let's look at how much unstationary the timeseries actually is ;-)","34ef46ce":"Next we split the dataset into training and testing. 20% of the data will be used for testing.","52161045":"It seems that removing weekly seasonality helped a bit but the autocorrelation plot still shows many significant lags.","ab7e8fde":"## XGBoost","26820514":"### XGBoost\nUsing linear booster because tree methods don't work with timeseries very well","bd89f292":"And the subjectivity scores"}}