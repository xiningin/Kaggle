{"cell_type":{"66da773f":"code","6dcb20d7":"code","155b518b":"code","9a7266cb":"code","817624e9":"code","a74398b4":"code","63c4eacb":"code","7f1da861":"code","2800529e":"code","492f9cb2":"code","1f6f51f6":"code","7c02f1a8":"code","62b95e76":"code","d524ad31":"code","81fbd4c3":"code","0f5f6c20":"code","743b1e2a":"code","1e8f5894":"code","83fc7c50":"code","55634b1b":"code","1cbb7ca9":"code","3e5e0a7a":"code","b6bb8a58":"code","9fa61cdc":"code","078e4e7a":"code","e134bfba":"code","079db84d":"code","3292e1cc":"code","544904e0":"code","80c21b92":"code","2e664610":"code","6b1f1455":"code","ade6b7d2":"code","1b88bb4b":"code","25789726":"code","4ba70b96":"code","ba406834":"code","d4ca871a":"code","315de7a6":"code","caed721d":"code","bc813845":"code","df178f60":"code","cc09bb5c":"code","cc7cc828":"code","5bc6c106":"code","0ea03a7b":"code","e1d74f4d":"code","4a3dab5c":"code","c4d205bd":"code","89309888":"code","4346b058":"code","a8c1e947":"code","1066f51d":"code","948a7b0b":"code","b7b1f67f":"code","a6b25a21":"code","19052da3":"code","35edaad0":"code","93e77a1f":"code","0470bcd0":"markdown","3b6d7a0d":"markdown","9c9e5f71":"markdown","d67e795b":"markdown","2dd5e7db":"markdown","487a3508":"markdown","35aea6e3":"markdown","39b5bcda":"markdown","3ec9d4f5":"markdown","c865a5ee":"markdown","3d5b8ca8":"markdown","bd49f9ed":"markdown","66713569":"markdown","e90b0970":"markdown","51bfcb3d":"markdown","89cfb2e9":"markdown","466f6a3f":"markdown","3195210a":"markdown","e0da536a":"markdown","19426d85":"markdown","4594b5ab":"markdown","97738953":"markdown","0e9971b6":"markdown","572df898":"markdown","f7181c68":"markdown","3aae2597":"markdown","e1495729":"markdown","60c7a4c8":"markdown","5d6ba8ef":"markdown","a572d10b":"markdown","a877dd9b":"markdown","3c6338a6":"markdown","9c145292":"markdown","06fc8957":"markdown","5f0f8c84":"markdown","ca83d9a0":"markdown","513698f8":"markdown","0d5dd2a9":"markdown","006536f8":"markdown","c54f01e5":"markdown","ac96d61e":"markdown","1703c2ed":"markdown","32e5fea1":"markdown"},"source":{"66da773f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport random, os\n\n%matplotlib inline\nplt.style.use(\"dark_background\")","6dcb20d7":"def seed_everything(seed=1234): \n    random.seed(seed) \n    os.environ['PYTHONHASHSEED'] = str(seed) \n    tf.random.set_seed(seed)\n    np.random.seed(seed) \n    \nseed_everything(2020)","155b518b":"# kaggle \nmain_dir = \"..\/input\/biobytes-contest\"\n\n# Jupyter lab\n# main_dir = \".\"","9a7266cb":"!ls {main_dir}","817624e9":"with open(f\"{main_dir}\/Main_data.txt\") as f:\n    temp = f.read()\n    temp = temp[:temp.index('>', 1)]\n    print (\"When printed it looks like so:\\n\\n\".upper() + temp)\n    print (\"Raw string:\\n\\n\".upper() + repr(temp))","a74398b4":"data = pd.read_csv(\n    f\"{main_dir}\/Main_data.txt\", \n    # each 'unit' internally is seperated by a \\n\n    sep='\\n', names=['Name', 'P_Seq', 'Target'], \n    # Each new 'unit' starts with a '>'\n    lineterminator='>', \n    # The txt file doesn't contain an index\n    # first column is the name of protein\n    index_col=False\n)\n\n# let's see if we were successful in reading to pandas DF\nfor name, value in zip(['Name', 'P_seq', 'Target'], data.iloc[0]):\n    print (f\"{name}: {repr(value)}\\n\")","63c4eacb":"# Apply a function to a Dataframe elementwise.\ndata = data.applymap(lambda x: x.rstrip('\\r'))\n\nfor name, value in zip(['Name', 'P_seq', 'Target'], data.iloc[0]):\n    print (f\"{name}: {repr(value)}\\n\")","7f1da861":"test = (\n    pd.read_csv(\n        f\"{main_dir}\/Test_data.txt\", \n        sep='\\n', names=['Name', 'P_Seq'], \n        lineterminator='>', index_col=False)\n    .applymap(lambda x: x.rstrip('\\r'))\n)\n\nfor name, value in zip(['Name', 'P_seq', 'Target'], test.iloc[0]):\n    print (f\"{name}: {repr(value)}\\n\")","2800529e":"sample_sub = pd.read_csv(f\"{main_dir}\/Sample_Solution.csv\")\nsample_sub.head()","492f9cb2":"# shapes of loaded Frames\ndata.shape, test.shape, sample_sub.shape","1f6f51f6":"# places where our above assumption is false\n(data.apply(lambda x: len(x[1]) - len(x[2]), axis=1) != 0).sum()","7c02f1a8":"# The Protein Class, Dunno if something like this \n# indeed exists. Correct me if I am wrong ofc!\ndata['P_Main_Class'] = data.Name.str.extract(\":(\\w)\")\ndata['P_Sub_Class'] = data.Name.str.extract(\"(.+):\\w\")\n\n# length of sequence\ndata['Seq_len'] = data.P_Seq.apply(len)\n\n# Number of unique peptides per sequence\ndata['Uniq_seq_Count'] = data.P_Seq.apply(lambda x: len(set(x)))\n\n# Number of peptides bound\ndata['B_Site_Count'] = data.Target.str.count('1')\n\n# Percentage of peptides that are bound\ndata['B_Site_percent'] = data['B_Site_Count'] \/ data['Seq_len']\n\n# Count of peptides that are unbound\ndata['Non_B_Site_Count'] = data['Seq_len'] - data['B_Site_Count']\n\ndata.head()","62b95e76":"uniq = set()\nfor _, seq in data.P_Seq.iteritems():\n    uniq |= set(seq)\n    \nprint (uniq)\nlen(uniq)","d524ad31":"# some basic stats\ndata.describe()","81fbd4c3":"f, ax = plt.subplots(ncols=2, nrows=2, figsize=(15, 10))\n(data[['Seq_len', 'Uniq_seq_Count', 'B_Site_Count', 'Non_B_Site_Count']]\n .plot(kind='kde', subplots=True, ax=ax, grid=True));","0f5f6c20":"data.B_Site_percent.plot(kind='kde', figsize=(15, 5), title='Percentage Bound for sequences');","743b1e2a":"data.plot(kind='box', figsize=(20, 10), subplots=True);","1e8f5894":"temp = data.P_Main_Class.value_counts()\nprint (\"Unique classes per Total Classes: {}\/{}\".format(len(temp), len(data)))\n\nplt.figure(figsize=(15, 5))\nplt.yticks(range(0, 21, 2))\nplt.bar(temp.index, temp.values)\n\nfor index, value in temp.iteritems():\n    plt.text(index, value, value)","83fc7c50":"pmc_mapper = dict(zip(temp[temp > 1].index, range(len(temp))))\nprint (pmc_mapper)","55634b1b":"temp = data['P_Sub_Class'].value_counts()\nprint (\"Unique classes per Total Classes: {}\/{}\".format(len(temp), len(data)))\n\nplt.figure(figsize=(15, 5))\nplt.bar(temp.index, temp.values)\nplt.xticks(rotation=90)\n\nfor index, value in temp.iteritems():\n    plt.text(index, value, value)","1cbb7ca9":"psc_mapper = dict(zip(temp[temp > 1].index, range(len(temp))))\nprint (psc_mapper)","3e5e0a7a":"temp = data.apply(lambda x: np.array(list(x[1]))[np.array(list(x[2])).astype(bool)], axis=1)\ntemp = temp.apply(pd.Series).stack().reset_index(level=1, drop=True)\n\nfreq_occured = data.P_Seq.apply(lambda x: pd.Series(list(x)).value_counts()).sum()\nfreq_bound = temp.value_counts()\n\nfreq = pd.merge(\n    pd.DataFrame(freq_occured).reset_index().rename({\"index\": \"Peptide\", 0: \"Occured\"}, axis=1), \n    pd.DataFrame(freq_bound).reset_index().rename({\"index\": \"Peptide\", 0: \"Bound\"}, axis=1),\n    on='Peptide'\n)\n\nfreq['Percent'] = freq['Bound'] \/ freq['Occured'] * 100\n\nfreq.head()","b6bb8a58":"(freq[['Peptide', 'Percent']]\n .set_index('Peptide')\n .sort_values('Percent')\n .plot(kind='bar', figsize=(20, 5), title='Percentage Plot (SORTED)', rot=0)\n);","9fa61cdc":"ax = freq.set_index(\"Peptide\")['Occured'].plot(\n    kind='bar', figsize=(20, 5), \n    title='Peptide Occuring vs Being Bound',\n    legend=True,\n)\n\nfreq.set_index(\"Peptide\")['Bound'].plot(kind='bar', ax=ax, color='r', legend=True)\n\nfor index, percent, value in freq[['Percent', 'Occured']].itertuples():\n    ax.text(index-0.2, value, f\"{percent:.0f}%\")","078e4e7a":"freq['Percent_bin'] = pd.cut(freq['Percent'], bins=5).cat.codes\nfreq = freq.sort_values(['Percent_bin', 'Occured'], ascending=[False, True])\n\n(freq.set_index(\"Peptide\")[['Occured', 'Bound']]\n .plot(kind='bar', \n       stacked=True,\n       title='Peptides sorted by Importance (Percent & Rareness)',\n      figsize=(20, 5), rot=0)\n);","e134bfba":"len(sample_sub), test.P_Seq.map(len).sum()","079db84d":"sub = test['P_Seq'].apply(list).explode().reset_index()\nsub = sub.rename({'index': 'Seq_No', \"P_Seq\": 'Peptide'}, axis=1)\nsub['Id'] = sub.index\nsub = sub.iloc[:, [-1, 0, 1]]\nsub.head()","3292e1cc":"sub[\"Expected\"] = sub.groupby(\"Seq_No\")['Id'].transform(lambda x: (np.random.random(len(x)) < 0.35).astype(int))\nsub.head()","544904e0":"ax = sub.Expected.value_counts().plot(\n    kind='bar', figsize=(10, 5), \n    color=['g', 'r'], title='Naive Prediction'\n)\n\nax.set_xticks([0, 1])\nax.set_xticklabels(['Unbound', 'Bound'], rotation=0);","80c21b92":"sub[['Id', 'Expected']].to_csv(\"Naive_submission.csv\", index=False)","2e664610":"# mapper containing the frequencies for each peptide\nmapper = dict(zip(freq.Peptide, freq.Percent))\nprint (mapper)","6b1f1455":"sub['Expected'] = (\n    sub.Peptide.map(mapper) \/ 100 \n    # calculate random for each peptide, say 'A' together\n    > sub.groupby('Peptide')['Id'].transform(lambda x: np.random.random(len(x)))\n).astype(int)","ade6b7d2":"# let's simply check if the random function has been used properly\n# I was confused with using >, < \n# here both should match for any value of temp (Nearly)\ntemp = np.random.choice(list(mapper.keys()))\nmapper[temp] \/ 100, sub.loc[sub.Peptide == temp, 'Expected'].mean()","1b88bb4b":"ax = sub.Expected.value_counts().plot(\n    kind='bar', figsize=(10, 5), \n    color=['g', 'r'], \n    title='Peptide Freq based Prediction'\n)\n\nax.set_xticklabels(['Unbound', 'Bound'], rotation=0);","25789726":"sub[['Id', 'Expected']].to_csv(\"Peptide_Based_Fprediction.csv\", index=False)","4ba70b96":"from sklearn.metrics import roc_auc_score, accuracy_score\n\nY = data.Target.apply(list).explode().values.astype(int)\ny_hat_naive = (np.random.random(len(Y)) < 0.35).astype(int)\n\ny_hat_freq_based = (\n    data.P_Seq.apply(list).explode().reset_index(drop=True).map(mapper) \/ 100 \n    > \n    (data.P_Seq.apply(list).explode().to_frame().reset_index().groupby(\"P_Seq\")\n     .transform(lambda x: np.random.random(len(x)))['index'])\n).astype(int)\n\nprint (\"Our estimates on TRAINING DATA:\")\nprint (\"\\n\\tNaive model accuracy score: {:12.2f}\\n\\tNaive model ROC score: {:17.2f}\"\n       .format(\n           accuracy_score(Y, y_hat_naive), \n           roc_auc_score(Y, y_hat_naive))\n      )\n\nprint (\"\\n\\tNuanced Naive model accuracy score: {:.2f}\\n\\tNuanced Naive model ROC score: {:9.2f}\"\n       .format(\n           accuracy_score(Y, y_hat_freq_based), \n           roc_auc_score(Y, y_hat_freq_based))\n      )","ba406834":"flat_data = pd.DataFrame({\n    \"P_Seq\": data.P_Seq.apply(list).explode(),\n    \"Target\": data.Target.apply(list).explode().astype(int)\n})\n\nflat_data.head()","d4ca871a":"peptide_mapper = dict(zip(list(mapper.keys()), range(len(mapper))))\nprint (peptide_mapper)","315de7a6":"# simplest possible linear model\ntf.keras.backend.clear_session()\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Input(shape=(len(peptide_mapper),)))\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=tf.keras.metrics.AUC())\nmodel.summary()","caed721d":"hist = model.fit(\n    tf.one_hot(flat_data.P_Seq.map(peptide_mapper), depth=20), \n    flat_data.Target, \n    validation_split=0.2, \n    callbacks=tf.keras.callbacks.EarlyStopping(patience=10),\n    epochs=100, \n    verbose=0)\n\nprint(\"Train Best ROC_AUC Score: {:.2f}\".format(hist.history['auc'][-1]))\nprint (\"Val Best ROC_AUC Score: {:6.2f}\".format(hist.history['val_auc'][-1]))\n\npd.DataFrame(hist.history).iloc[:, [1, 3]].plot(figsize=(20, 5), title='Model Performance');","bc813845":"sub['Expected'] = (\n    model.predict(tf.one_hot(\n        sub.Peptide.map(peptide_mapper), \n        depth=len(peptide_mapper)))\n)","df178f60":"sub[['Id', 'Expected']].to_csv(\"Linear_model_one_hot.csv\", index=False)","cc09bb5c":"flat_data['Percent'] = flat_data.P_Seq.map(mapper).astype(float) \/ 100\nflat_data = flat_data.merge(data.iloc[:, 3:7], right_index=True, left_index=True)\nflat_data['Position'] = flat_data.groupby(flat_data.index)['P_Seq'].transform(lambda x: np.arange(len(x)) \/ len(x))\nflat_data.head()","cc7cc828":"# Main and sub class for the test data as well\ntest['P_Main_Class'] = test.Name.str.extract(\":(\\w)\")\ntest['P_Sub_Class'] = test.Name.str.extract(\"(.+):\\w\")\n\n# length of sequence\ntest['Seq_len'] = test.P_Seq.apply(len)\n\n# Number of unique peptides per sequence\ntest['Uniq_seq_Count'] = test.P_Seq.apply(lambda x: len(set(x)))\n\n# merge the test data with Sub dataFrame\nsub = sub.set_index(\"Seq_No\").merge(test.iloc[:, 2:], right_index=True, left_index=True)\n\n# we reuse column 'expected' as our percentage expected\nsub['Expected'] = sub.Peptide.map(mapper) \/ 100\n\n# rename the columns accordingly\nsub = sub.rename({\"Peptide\": \"P_Seq\", \"Expected\": \"Percent\"}, axis=1)\n\n# the position of peptide in sequence\nsub['Position'] = sub.groupby(sub.index)['P_Seq'].transform(lambda x: np.arange(len(x)) \/ len(x))\n\n# how does it look?\nsub.head(3)","5bc6c106":"def process_flat_df(\n    data, \n    ohc=[('P_Sub_Class', len(psc_mapper)), ('P_Main_Class', len(pmc_mapper))], \n    dc=['P_Main_Class', 'P_Sub_Class'], \n    seq_shift=0,\n    pep_freq=None, \n    as_df=False):\n    \n    '''One hot enocdes the categorical data after mapping them to the respective mappers.\n    The numeric columns, futher more are scaled.\n    ohc      -> One hot columns, other than `P_Seq`\n    dc       -> columns to drop\n    pep_freq -> does the dataframe contain the peptide frequency, (we pass in the series)\n    as_Df    -> Return output as a df or tensor\n    '''\n    \n    df = data.copy()\n    \n    # copy and create mapper for null values\n    pmapper = peptide_mapper.copy()\n    pmapper['0'] = max(peptide_mapper.values()) + 1\n    \n    # reusing pipeline in the future, skip for now\n    if pep_freq is not None:\n        freq_mat = get_freq(pep_freq[0], fit=pep_freq[1])\n        df = df.merge(freq_mat, left_index=True, right_index=True, how='left')\n        df.iloc[:, -freq_mat.shape[1]:] = df.iloc[:, -freq_mat.shape[1]:] \/ df['Seq_len'].values.reshape(-1, 1)\n    \n    # normalize numeric values\n    df['Seq_len'] = df['Seq_len'] \/ max(sub.Seq_len.max(), flat_data.Seq_len.max())\n    df['Uniq_seq_Count'] = df['Uniq_seq_Count'] \/ 20\n    \n    # ordinal encoding the categorical data\n    # df['P_Seq'] = df['P_Seq'].map(pmapper).fillna(len(pmapper))\n    df['P_Main_Class'] = df['P_Main_Class'].map(pmc_mapper).fillna(len(pmc_mapper))\n    df['P_Sub_Class'] = df['P_Sub_Class'].map(psc_mapper).fillna(len(psc_mapper))  \n\n    one_hot = []\n    \n    for i in range(-seq_shift, seq_shift+1):\n        df[f'P_Seq_{i}'] = df.groupby(df.index)['P_Seq'].shift(-i).fillna('0').map(pmapper)\n        \n    seq_one_hot = tf.reduce_sum(tf.one_hot(df.iloc[:, -((seq_shift*2)+1):], depth=len(pmapper)), axis=1)\n    seq_one_hot = seq_one_hot \/ ((seq_shift * 2) + 1)\n    one_hot.append(seq_one_hot)\n    \n    for key, depth in ohc:\n#         one_hot.append(tf.one_hot(df[key], depth=depth+1))\n        one_hot.append(tf.one_hot(df[key], depth=depth))\n    \n    one_hot = tf.concat(one_hot, axis=1)\n    \n    # drop the columns we had one hot encoded to\n    # always drop ['P_Seq'] since it is 1hencoded\n    dc = dc + df.columns[df.columns.str.contains(\"P_Seq\", na=False)].tolist()\n    df.drop(dc, axis=1, inplace=True)\n    \n    if as_df:\n        return (df.reset_index(drop=True).merge(\n                pd.DataFrame(one_hot.numpy()), right_index=True, left_index=True))\n    \n    else:\n        return tf.concat([df, one_hot], axis=1)","0ea03a7b":"# working good?\nshape = process_flat_df(\n    flat_data.drop(\"Target\", 1), \n    ohc=[('P_Sub_Class', len(psc_mapper))]\n).shape[1]\n\nshape","e1d74f4d":"# linear model with more features added\ntf.keras.backend.clear_session()\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Input(shape=(shape,)))\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=tf.keras.metrics.AUC())\n\nhist = model.fit(\n    process_flat_df(flat_data.drop(\"Target\", 1), \n                    ohc=[('P_Sub_Class', len(psc_mapper))]\n    ),\n    flat_data.Target, \n    validation_split=0.2, \n    callbacks=tf.keras.callbacks.EarlyStopping(patience=10),\n    epochs=100, \n    verbose=0)\n\nprint(\"Train Best ROC_AUC Score: {:.2f}\".format(hist.history['auc'][-1]))\nprint (\"Val Best ROC_AUC Score: {:6.2f}\".format(hist.history['val_auc'][-1]))\n\npd.DataFrame(hist.history).iloc[:, [1, 3]].plot(figsize=(20, 5), title='Model Performance');","4a3dab5c":"predictions = model.predict(\n    process_flat_df(\n        sub.drop(['Id'], axis=1), \n        ohc=[('P_Sub_Class', len(psc_mapper))])\n)\n\nsub['Expected'] = predictions\nsub.head()","c4d205bd":"sub[['Id', 'Expected']].to_csv(\"Linear_model_with_meta.csv\", index=False)","89309888":"def get_freq(series, min_thresh=0.65, fit=False):\n    'Returns a 20 * n matrix containing frequencies for each sequence'\n    from sklearn.feature_extraction.text import CountVectorizer\n    global cnt\n    \n    if fit:\n        cnt = CountVectorizer(analyzer='char', ngram_range=(1, 2), min_df=min_thresh, lowercase=False)\n        return pd.DataFrame(cnt.fit_transform(series).todense(), columns=cnt.get_feature_names())\n    else:\n        return pd.DataFrame(cnt.transform(series).todense(), columns=cnt.get_feature_names())\n\n    # pervious code, counts only 1 character\n    '''\n    matrix = pd.DataFrame()\n    for key in peptide_mapper.keys():\n        matrix[key] = series.str.count(key)\n    return matrix\n    '''","4346b058":"# defining the columns we would be dropping\ndc=[\n    'P_Sub_Class', \n#     'P_Main_Class', \n#     'Seq_len', \n#     'Uniq_seq_Count', \n#     'Percent',\n   ]\n\n# defining columns we would be one hot encoding along with their mappers\nohc=[\n    ('P_Sub_Class', len(psc_mapper)), \n#     ('P_Main_Class', len(pmc_mapper)),\n]\n\n# working good?\ntemp = process_flat_df(\n    flat_data.drop(\"Target\", 1), \n    ohc=ohc,\n    dc=dc,\n    pep_freq=(data.P_Seq, True),\n    as_df=True\n).head(3)\n\nshape = temp.shape[1]\ntemp","a8c1e947":"# linear model with more features added\ntf.keras.backend.clear_session()\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Input(shape=(shape,)))\nmodel.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', \n              optimizer=tf.keras.optimizers.Adam(0.0025), \n              metrics=tf.keras.metrics.AUC())\n\nhist = model.fit(\n    process_flat_df(\n        flat_data.drop(\"Target\", 1), \n        ohc=ohc,\n        dc=dc,\n        pep_freq=(data.P_Seq, True)\n    ),\n    \n    flat_data.Target, \n    batch_size=128,\n    validation_split=0.2, \n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(patience=8, monitor='val_auc', mode='max'),\n        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_auc', patience=5, factor=0.5, mode='max')],\n    epochs=100, \n    verbose=0)\n\nprint(\"Train Best ROC_AUC Score: {:.2f}\".format(hist.history['auc'][-1]))\nprint (\"Val Best ROC_AUC Score: {:6.2f}\".format(hist.history['val_auc'][-1]))\n\npd.DataFrame(hist.history).iloc[:, [1, 3]].plot(figsize=(20, 5), title='Model Performance');","1066f51d":"predictions = model.predict(\n    process_flat_df(\n        sub.drop([\"Expected\", \"Id\"], 1), \n        ohc=ohc,\n        dc=dc,\n        pep_freq=(test.P_Seq, False)),\n)\n\nsub['Expected'] = predictions\nsub.head(3)","948a7b0b":"sub[['Id', 'Expected']].to_csv(\"Linear_model_with_Seq_freq.csv\", index=False)","b7b1f67f":"# Seq in future (and in past) to consider as input\nSHIFT = 2\n\n# defining the columns we would be dropping\ndc=[\n#     'P_Sub_Class', \n    'P_Main_Class',\n#     'Seq_len',\n    'Uniq_seq_Count',\n#     'Percent',\n#     'Position',\n   ]\n\n# defining columns we would be one hot encoding along with their mappers\nohc=[\n#     ('P_Sub_Class', len(psc_mapper)),\n#     ('P_Main_Class', len(pmc_mapper)),\n]\n\n# working good?\ntemp = process_flat_df(\n    flat_data.drop(\"Target\", 1), \n    ohc=ohc,\n    dc=dc,\n    # we can tune the value of seq_shift \n    # to see which one performs better\n    pep_freq=(data.P_Seq, True),\n    seq_shift=SHIFT,\n    as_df=True\n).head(5)\n\nshape = temp.shape[1]\nprint (\"The number of columns in data that would be fit to our model is:\", shape)\n\n# how does the one hot encoded labels look?\ntemp.iloc[:3, 29:29+21]","a6b25a21":"flat_data['P_Seq'].head(5).map(peptide_mapper).values","19052da3":"# resetting the shift value\nSHIFT = 3\n\n# linear model with more features added\ntf.keras.backend.clear_session()\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Input(shape=(shape,)))\nmodel.add(tf.keras.layers.Dropout(0.1))\nmodel.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', \n              optimizer=tf.keras.optimizers.Adam(0.005), \n              metrics=tf.keras.metrics.AUC())\n\nhist = model.fit(\n    process_flat_df(\n        flat_data.drop(\"Target\", 1), \n        ohc=ohc,\n        dc=dc,\n        pep_freq=(data.P_Seq, True),\n        seq_shift=SHIFT),\n    flat_data.Target,\n    \n    validation_split=0.25,\n    batch_size=128,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_auc', mode='max'),\n        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_auc', patience=5, factor=0.25, mode='max')],\n    epochs=100, \n    verbose=0)\n\nprint(\"Train Best ROC_AUC Score: {:.2f}\".format(hist.history['auc'][-1]))\nprint (\"Val Best ROC_AUC Score: {:6.2f}\".format(hist.history['val_auc'][-1]))\n\npd.DataFrame(hist.history).iloc[:, [1, 3]].plot(figsize=(20, 5), title='Model Performance');","35edaad0":"predictions = model.predict(\n    process_flat_df(\n        sub.drop([\"Expected\", \"Id\"], 1), \n        ohc=ohc,\n        dc=dc,\n        pep_freq=(test.P_Seq, False)\n    ),\n)\n\nsub['Expected'] = predictions\nsub.head(3)","93e77a1f":"sub[['Id', 'Expected']].to_csv(\"Linear_model_with_hist.csv\", index=False)","0470bcd0":"The problem of course is that the each column entry has a '\\r' appended to its end. We need to remove it before we can proceed further.","3b6d7a0d":"Let's see which _Protein Classes_ are more frequent?","9c9e5f71":"What are the files we have been given?","d67e795b":"A single Main Class `1jj2` occurs almost 22 times. The remaining names occur just once! We repeat the same procedure that we had done earlier, retaining just the most popular classes:","2dd5e7db":"### LB Scores and Analysis: \n\n1. `Naive_submission.csv` managed to get a score of 0.498\n2. `Peptide_Based_Fprediction.csv` managed to get a score of 0.50\n\nAlthough our predictions are more nuanced the second time, the score isn't as well as we had hoped. However, Since the competition uses an ROC metric, random predictions should rightly yield a score of 0.5. Let's verify that with the training data we have:","487a3508":"Cool, so let's make our own submission file containing the Peptides as well. Our naive predictions are going to need it.","35aea6e3":"Making a simple count Plot:","39b5bcda":"Higher values of shift, seems to perform well on the validation data, however it seems to 'overfit' to the training dataset & doesn't score well on the Public LB ;(\n\nHope you liked reading this! ","3ec9d4f5":"### Feature Engineering:\nAll good, let's move on with some feature engineering for gather cool insights.","c865a5ee":"It has improved very little compared to our previous model, the n-gram approach performs marginally better than the previous approach.\n\nOur next approach is to add in the previous sequences to the data and factor it as well for the predictions. Let's see if this approach works well. We would be one hot encoding consequetive sequences and summing them up together:\n\nThe P_Seq's are shifted using `shift` function, and the one_hot encoded. This one encoded results are then added together and resultant would be akin to a multihot encoded sequence. Lets see how this would look like:\n\n`Note`: **Do not use freq_distribution with this method, since the pep freq joining relies on the index. The index are all jumbled and warped up after the train\/Val split.**","3d5b8ca8":"## EDA:\n\nUsually we are provided with csv data which comes preformated. Here we are provided with txt data that needs a little reformating. Let's see how we can save the txt file to csv files using pandas. \n\nHow a single \"unit\" looks like?","bd49f9ed":"Let's now try to see which peptide has been bound the most & also try to calculate their _chances_ for being bound:","66713569":"Nuanced random predictions are expected to do slightly better, but owing to the very small test data (5k) compared to 14k in training set, the random predictions aren't that better than our initial naive model. At the end of the day, Random predictions, esp. when used against an ROC metric must yeild a score of ~0.5. \n\n    \n#### Let's move on to using machine learning models:\n\nA simplest ML model would simply predict whether a particular peptide would be bound or unbound. It doesn't consider the entire sequence.","e90b0970":"Let's create a mapper for the peptides as well:","51bfcb3d":"It has managed to reach a ROC of .76. Let's make our predictions on the test dataset:","89cfb2e9":"Let's improve on our naive idea a bit. One improvement would be predict whether a particular entry in submission would be bound, depending on the freq insights we had extracted from the train data.","466f6a3f":"These are the unique peptide links we have overall:","3195210a":"We can observe that column 20 which denotes the padding, has a value of 20 for the first row. This is because the first row has no sequences that precede it (Shift value of 2). We can better verify this if we also have our unprocesssed dataFrame: `flat_data`:","e0da536a":"### Box Plots:","19426d85":"### KDE Plots:\n\nLet's visualize their distributions.","4594b5ab":"Let's also check out with our Protein sub class:","97738953":"It has managed a score of 0.58 on LB. Let's see if we can do still better.\n\n`Note`: It is possible to submit, float targets as our predictions.\n\n#### Linear model with meta features:\n\nLet's also add in the other meta features we had extracted previously plus some more columns, to our `flat_data`:","0e9971b6":"First row shifted twice back and forward has the sequence: [20, 20, 7, 5, 7]\n<br>Note that 20 for is pad values. Therefore on summing up the first row makes sense.\n1. 20th column has value of 2 (normalized as per shift value)\n2. 7th column has a value of 2 (normalized as per shift value)\n3. 5th column has a value of 1 (normalized as per shift value)\n\nSO on and so forth. Let's now fit the data to a new model:","572df898":"Each unit of Main_data.txt has the following format:\n\n    \\>Protein_name:Class\\nProtein_sequence\\nBinary_sequence\\n\n  \nThe challenge is the fact that each \"unit\" does not have a fixed size. We could use pandas's read_csv method for reading the txt file with some slight modifications.","f7181c68":"### Keeping the End Goal in Mind:\nLet's now explore how the submission file  ought to be:\n\nFor each test Seq's peptide, we predict correspondingly whether it would be bound or otherwise, therefore the sum of all test sequence's length must equal the len of rows in sample_submission. Let's verify if what I said just now is correct:","3aae2597":"A function to perform the categorical to ordinal plus one hot encoding for us:","e1495729":"This model has managed to reach a ROC of ~0.77. Let's make submissions and see how it scores:","60c7a4c8":"Let's repeat the same for test dataset as well:","5d6ba8ef":"For those peptides with higher chance of occuring, rarer the peptide occuring, more important it should be. \n\nFor example: We should note that even though 'C' has very little probability, it is still more important than other peptides with similar percentages that occur much more often, say: 'L'. See plot below to see what I mean:","a572d10b":"Let's set random seed to produce reproducible results:","a877dd9b":"We need to create the above features for the test data as well:","3c6338a6":"Load Sample submission file as well:","9c145292":"Count Plot, Again:","06fc8957":"Let's plot the above freq Df as a box plot:","5f0f8c84":"This would be our first submission:","ca83d9a0":"'A' Class occurs the most. More than half of the classes occur only once! Let's group the less popular classes to 'others'. We create a mapper to help us do this in the future.","513698f8":"Given 69 sequences of proteins, we have to predict binding sites for 17 protein molecules.\n\nLets do some simple checks. By our understanding, the len of P_seq - len of Target should equal 0 for every row. Since target is essentially telling us if that particular `Peptide` is RNA binding or not.","0d5dd2a9":"The above bar plot only gives us half the picture. Although percentage is a good way of identifying the important peptides, we also need to consider how another feature in ordering them by importance: \"How rarely does that peptide occurs?\". \n\nWe can do that by a stacked bar plot:","006536f8":"A very naive idea would be to predict 35% of peptides to be randomly bound:","c54f01e5":"# Objective:\nThis notebook's objective has changed ;) Right now its to show you what I did My EDA plus data modelling.\n\n### Import necessary modules:","ac96d61e":"This managed an LB score of 0.75 in the Public leaderboard. \n\nNow before moving on to more powerful architectures, let's try adding the entire sequence frequency to each row in the `flat_data`. For each sequence, we create a variable that holds the normalized value of different peptides contained in it. Let's write a simple function to get the peptide value counts for each sequence:","1703c2ed":"It has managed to reach an ROC_AUC of 0.63, let's now make predictions on the test data:","32e5fea1":"#### Our observation:\n- Every protein sequence we have been given with have a minimum of 15 peptides and a max of 20 unique peptides.\n- Sequence length is widely varying for each protein, ranging from min of 46 to a max of 862.\n- .75 quantile is much smaller than max for all columns. Meaning that certain proteins are very different from the rest of the group.\n- B_Site_Percent has similar mean and median, we observe that **most sequences have 35% of the links bound.**\n- There has been a protein for which only one peptide was not bound :)\n- All columns have a longer right tail (with the exception of Uniq_seq_count)\n\nLooking at B_site_percent, a naive model would be one which randomly predicts links to be bound roughly 35% of time."}}