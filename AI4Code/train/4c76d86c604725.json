{"cell_type":{"02359820":"code","faf814ea":"code","b804cd40":"code","b6bd8327":"code","c20b3ae4":"code","23ed2a1f":"code","a598309c":"code","4a2e85e7":"code","9167a01a":"code","7b659bfc":"code","371ed6bb":"code","ed705a45":"code","37d1063e":"code","1bf962d2":"code","24efeaf3":"code","45b55cfa":"code","a9e8a2f2":"code","c14134f3":"code","42b5543b":"code","9fb8af1b":"code","1941641a":"code","b47440a7":"code","0a013120":"code","fa9904b3":"code","db1a28b0":"code","1d0ce506":"code","e652241b":"code","9617b93e":"code","2b41ab8f":"code","023d8437":"code","12f5a849":"code","ef8a4365":"code","a3547417":"code","c1af9fdb":"code","152fa601":"code","7d86626a":"code","52eb68f3":"code","2829fd33":"code","b0c0633a":"code","52409efb":"code","95ac7a52":"code","268a8796":"code","b9ad519e":"code","3bc69656":"code","11275499":"code","b6c10438":"code","929cf54e":"code","2a93cd6d":"code","8d79d976":"code","a83a3cbe":"code","83fda7ff":"code","6f1de7e6":"code","a5bdeb01":"code","062268c5":"code","40b98f46":"code","5f557b20":"code","4cf75523":"code","3f91a630":"code","5489fbbd":"code","7dc3f596":"code","86d4eabb":"markdown","47cc2c88":"markdown","25584a58":"markdown","d5f61f69":"markdown","b9c8f487":"markdown","8aa0be6d":"markdown","533ef81b":"markdown","054298cf":"markdown","9cb52886":"markdown","87d90395":"markdown","f8eba962":"markdown","3c29f7a5":"markdown","13e7faf7":"markdown","9eacfc70":"markdown","63bf7992":"markdown","875e588e":"markdown"},"source":{"02359820":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_log_error\nfrom scipy import stats","faf814ea":"dir_path = '\/kaggle\/input\/seoul-bike-rental-ai-pro-iti'\n\ndf_train = pd.read_csv(os.path.join(dir_path, 'train.csv'))\ndf_test = pd.read_csv(os.path.join(dir_path, 'test.csv'))\n\ndf_test_ids = df_test['ID']\ndf_train = df_train.drop(columns = ['ID'])\ndf_test = df_test.drop(columns = ['ID'])","b804cd40":"df_train.head()","b6bd8327":"df_test.head()","c20b3ae4":"df_train.columns","23ed2a1f":"df_train.dtypes","a598309c":"df_train = df_train.rename(columns = {df_train.columns[3] : 'Temperature (C)', df_train.columns[7] : 'Dew point temperature (C)'})\ndf_test = df_test.rename(columns = {df_test.columns[2] : 'Temperature (C)', df_test.columns[6] : 'Dew point temperature (C)'})\n#df_train = df_train[df_train['Functioning Day'] != 'No']\n#df_train.drop(columns='Functioning Day', inplace=True)\ndf_train.columns","4a2e85e7":"df_train.describe()","9167a01a":"df_train.isna().sum()","7b659bfc":"df_train.var().sort_values(ascending = False)","371ed6bb":"# we should drop after visualization so this cell should be moved\n#df_temp = df_train.drop(columns=[\"Holiday\", \"Date\", \"Rainfall(mm)\", \"Wind speed (m\/s)\", Snowfall (cm)\"])","ed705a45":"def add_working_hour_column(df):\n    df[\"working_hour\"] = 0\n    df[\"working_hour\"] = ((df[\"Hour\"] >= 5) & (df[\"Hour\"] <= 20)).astype(int)\n    return df","37d1063e":"df_train['Month'] = pd.DatetimeIndex(df_train['Date']).month\ndf_train['Day'] = pd.DatetimeIndex(df_train['Date']).day\ndf_train['Weekday'] = pd.DatetimeIndex(df_train['Date']).weekday\ndf_test['Month'] = pd.DatetimeIndex(df_test['Date']).month\ndf_test['Day'] = pd.DatetimeIndex(df_test['Date']).day\ndf_test['Weekday'] = pd.DatetimeIndex(df_test['Date']).weekday","1bf962d2":"#df_train[\"m_d_h\"] = df_train[\"Month\"] * 30 + df_train[\"Day\"] * 24 + df_train[\"Hour\"]\n#df_test[\"m_d_h\"] = df_test[\"Month\"] * 30 + df_test[\"Day\"] * 24 + df_test[\"Hour\"]\ndf_train.head()","24efeaf3":"df_train=add_working_hour_column(df_train)\ndf_test=add_working_hour_column(df_test)","45b55cfa":"# feature=['y', 'Solar Radiation (MJ\/m2)']\n# # IQR\n# Q1 = np.percentile(df_train[feature], 25, \n#                    interpolation = 'midpoint',axis=0) \n# print(Q1) \n# Q3 = np.percentile(df_train[feature], 75,\n#                    interpolation = 'midpoint',axis=0) \n# print(Q3) \n# IQR = Q3 - Q1 \n  \n# print(\"Old Shape: \", df_train.shape) \n  \n# # Upper bound\n# upper = np.where(df_train[feature] >= (Q3+1.5*IQR))\n# # Lower bound\n# lower = np.where(df_train[feature] <= (Q1-1.5*IQR))\n  \n# #Removing the Outliers\n# df_train.drop(upper[0], inplace = True, axis=0)\n# df_train.drop(lower[0], inplace = True, axis=0)\n  \n# print(\"New Shape: \", df_train.shape)","a9e8a2f2":"def get_temp_range(temp_val):\n    counter=1\n    for i in range(-20,41,10):\n        if temp_val <= i :\n            return counter\n        counter+=1\n    return 0","c14134f3":"# df_train[\"temp_range\"]=df_train['Temperature (C)'].apply(get_temp_range)\n# df_test[\"temp_range\"]=df_test['Temperature (C)'].apply(get_temp_range)\ndf_train.head()","42b5543b":"def encode_categroical_features(df):\n    df[\"Seasons\"] = df[\"Seasons\"].astype(\"category\").cat.codes\n    df[\"Functioning Day\"] = df[\"Functioning Day\"].astype(\"category\").cat.codes\n    df[\"Holiday\"] = df[\"Holiday\"].astype(\"category\").cat.codes\n    return df\n    ","9fb8af1b":"def pca_3_components(df, feature1, feature2, feature3,  new_col_name,df_test):\n    to_be_transformed = df[[feature1, feature2, feature3]]\n    to_be_transformed_test = df_test[[feature1, feature2, feature3]]\n    pca = PCA(n_components=1)\n    transformed_components = pca.fit_transform(to_be_transformed)\n    df[new_col_name] = transformed_components\n    df_test[new_col_name]=pca.transform(to_be_transformed_test)\n    df.drop(columns=[feature1, feature2, feature3],inplace=True)\n    df_test.drop(columns=[feature1, feature2, feature3],inplace=True)\n    return df","1941641a":"def filter_functioning_day(df):\n    df_columns=df.columns\n    for col in df_columns:\n        df[col]=df[col]*df['Functioning Day']\n    return df","b47440a7":"def replace_outlaires(df):\n    for feature in df.drop(columns=[\"Hour\", \"Month\", \"Day\",'Functioning Day','Seasons']).columns:\n        # IQR\n        Q1 = np.percentile(df[feature], 25, \n                           interpolation = 'midpoint') \n        Q3 = np.percentile(df[feature], 75,\n                           interpolation = 'midpoint') \n        IQR = Q3 - Q1 \n\n        upperL = Q3 + 1.5*IQR\n        lowerL = Q1 - 1.5*IQR\n        df[feature] = df[feature].map(lambda val: (val if val < upperL else upperL))\n        df[feature] = df[feature].map(lambda val: (val if val > lowerL else lowerL))\n\n    print(\"New Shape: \", df.shape)\n    return df","0a013120":" def pre_processing(df):\n    columns_to_drop=['Date', 'Snowfall (cm)', 'Holiday', 'Wind speed (m\/s)']\n    df=encode_categroical_features(df)  \n    df=df.drop(columns=columns_to_drop)\n#     _ = pca_3_components(df, \"Day\", \"Month\", \"m_d_h\", \"D_M\",df_test)\n    #df=filter_functioning_day(df)\n    return df","fa9904b3":"#corr_mat = df_train.corr()\n#fig = plt.figure(figsize = (14, 14))\n#sns.heatmap(corr_mat, annot= True)\n#plt.show()","db1a28b0":"from sklearn.decomposition import PCA\ndef pca_2_components(df, feature1, feature2,  new_col_name , df_test):\n    to_be_transformed = df[[feature1, feature2]]\n    to_be_transformed_test = df_test[[feature1, feature2]]\n    pca = PCA(n_components=1)\n    transformed_components = pca.fit_transform(to_be_transformed)\n    df[new_col_name] = transformed_components\n    df_test[new_col_name]= pca.transform(to_be_transformed_test)\n    df.drop(columns=[feature1,feature2],inplace=True)\n    df_test.drop(columns=[feature1,feature2],inplace=True)\n    return df","1d0ce506":"pca_2_components(df_train, 'Dew point temperature (C)', 'Temperature (C)', 'temp_pca',df_test)\n#pca_2_components(df_train, 'Solar Radiation (MJ\/m2)', 'working_hour', 'solar_work_pca',df_test)\ncolumns_to_drop_aftePCA=['Dew point temperature (C)', 'Temperature (C)']","e652241b":"df_temp =pre_processing(df_train)","9617b93e":"df_temp=replace_outlaires(df_temp)","2b41ab8f":"# corr_mat = df_temp.corr()\n# fig = plt.figure(figsize = (14, 14))\n# sns.heatmap(corr_mat, annot= True)\n# plt.show()","023d8437":"\ndf_temp[df_temp['Functioning Day']==0]","12f5a849":"#df_temp['y'] = np.log1p(df_temp['y'])","ef8a4365":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\n\nX_train, X_test, y_train, y_test = train_test_split(df_temp.drop(columns=['y', 'Month', 'Day']), df_temp[\"y\"], test_size=0.005, random_state=42)","a3547417":"df_temp.describe()","c1af9fdb":"\n# scaler=MinMaxScaler()\n# X_train=scaler.fit_transform(X_train)\n# X_test=scaler.transform(X_test)\n","152fa601":"scaler=StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)\n","7d86626a":"#print(pd.DataFrame(X_train).describe())","52eb68f3":"'''\n\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nregr = ExtraTreesRegressor(random_state=0)\nregr.fit(X_train, y_train)\n\ny_pred=regr.predict(X_test)\n\nprint(regr.score(X_test, y_test))\n\ny_test, y_pred = np.expm1(y_test), np.expm1(y_pred)\nrmsle = np.sqrt(mean_squared_log_error(y_test, y_pred))\nprint(rmsle)\n'''","2829fd33":"#len(np.where(y_pred < y_test)[0])","b0c0633a":"from xgboost import XGBRegressor\nXGBModel = XGBRegressor(objective=\"reg:tweedie\", tweedie_variance_power=1.6, gamma=2, max_depth=6, subsample=.7, reg_alpha=0.15, reg_lambda=1, learning_rate= 0.15)","52409efb":"X_train=scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)\nXGBModel = XGBModel.fit(X_train, y_train, verbose=False)\nprint(XGBModel.score(X_test, y_test))","95ac7a52":"y_test","268a8796":"y_pred = XGBModel.predict(X_test)\nrmsle = np.sqrt(mean_squared_log_error(y_test, y_pred))\nprint(rmsle)","b9ad519e":"len(np.where(y_pred < y_test)[0])","3bc69656":"print(df_temp[df_temp[\"y\"] < 0])","11275499":"sns.histplot(df_temp[\"y\"])","b6c10438":"sns.histplot(y_train, bins=20)","929cf54e":"sns.histplot(y_test, bins=20)","2a93cd6d":"sns.histplot(y_pred, bins=20)","8d79d976":"sns.scatterplot(y_pred, y_test)","a83a3cbe":"sns.histplot(y_pred-y_test)","83fda7ff":"print(np.sum(np.abs(y_pred - y_test) > 100))","6f1de7e6":"len(y_test)","a5bdeb01":"df_test.head()","062268c5":"\ndf_test=pre_processing(df_test)","40b98f46":"df_test.head()","5f557b20":"df_test[df_test['Functioning Day']==0]","4cf75523":"X_test = df_test.drop(columns=['Month','Day'])\nX_test=scaler.transform(X_test)\n# You should update\/remove the next line once you change the features used for training\ny_test_predicted = XGBModel.predict(X_test)\ndf_test['y'] = y_test_predicted\ndf_test['ID']=df_test_ids\ndf_test.head()\ndf_test[['ID', 'y']].to_csv('\/kaggle\/working\/submission.csv', index=False)","3f91a630":"X_test = df_test.drop(columns=['Month','Day'])\nX_test.head()","5489fbbd":"df_test.describe()","7dc3f596":"df_test[df_test['Functioning Day']==0]","86d4eabb":"# **XGB**","47cc2c88":"There are no missing values, so..That's a good start","25584a58":"# Seoul Bike Rental","d5f61f69":"#### Importing necessary libraries","b9c8f487":"#### Reviewing data","8aa0be6d":"# # **Model**","533ef81b":"### Apply Feature Scaling","054298cf":"# **Encoding Categorical Columns**","9cb52886":"# Some EDA ","87d90395":"#### Loading datasets into notebook","f8eba962":"### Fixing names of Temperature columns \n","3c29f7a5":"* We can see strong corrolation between temp and dew point temp, so we can drop dew point\n* Snowfall, Rainfall, Holiday and FunctioningDay can be droped","13e7faf7":"We should keep an eye for 'Snowfall' and 'Solar Radiation' columns, Cause with a variance this low they might be adding an insignificant amount of information ","9eacfc70":"# **Now let's see heat map to check for corrolation**","63bf7992":"#### We'll plot the scatter plot for some selected columns","875e588e":"### Functioning Day filteration"}}