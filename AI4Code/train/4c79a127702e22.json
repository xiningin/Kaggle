{"cell_type":{"730a9b3a":"code","d41761ff":"code","aa741beb":"code","ad94a8ba":"code","331937df":"code","da288e2e":"code","7f7715ff":"code","3dbd6b5f":"code","807ad088":"code","7f9b277d":"markdown","0c137e66":"markdown","4b420bb7":"markdown","e314359e":"markdown","44b266a3":"markdown","98b0eb97":"markdown","5c4e5359":"markdown","a48d6128":"markdown","77b1bc1b":"markdown","a5234826":"markdown"},"source":{"730a9b3a":"import os\nimport gc\nimport sys\nimport cv2\nimport math\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import KFold,StratifiedKFold\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader,Sampler\n\nimport multiprocessing\nimport more_itertools\n\n\nfrom transformers import (AutoModel, AutoTokenizer, AutoModelForSequenceClassification)\n\nfrom colorama import Fore, Back, Style\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA\nc_ = Fore.CYAN\nsr_ = Style.RESET_ALL","d41761ff":"train_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nsample = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')","aa741beb":"config = {\n    'learning_rate':2e-5,\n    'batch_size':16,\n    'epochs':3,\n    'nfolds':5,\n    'seed':1, # 42\n    'max_len':256,\n}\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=config['seed'])","ad94a8ba":"class CLRPDataset(Dataset):\n    def __init__(self,df,tokenizer):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],return_tensors='pt',\n                                max_length=config['max_len'],\n                                padding='max_length',truncation=True)\n        return encode\n    \n    def __len__(self):\n        return len(self.excerpt)","331937df":"class AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim, num_targets):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n\n        score = self.V(att)\n\n        attention_weights = torch.softmax(score, dim=1)\n\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector","da288e2e":"class Model(nn.Module):\n    def __init__(self):\n        super(Model,self).__init__()\n        self.electra = AutoModel.from_pretrained('..\/input\/clrp-electra-large\/clrp_electra_large')\n        self.head = AttentionHead(256,256,1) \n        self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(256,1)\n\n    def forward(self,**xb):\n        x = self.electra(**xb)[0]\n        x = self.head(x)\n        x = self.dropout(x)\n        x = self.linear(x)\n        return x","7f7715ff":"def get_prediction(df,path,device='cuda'):        \n    model = Model()\n    model.load_state_dict(torch.load(path,map_location=device))\n    model.to(device)\n    model.eval()\n    \n    tokenizer = AutoTokenizer.from_pretrained('..\/input\/electra-tokenizer-files')\n    \n    test_ds = CLRPDataset(df,tokenizer)\n    test_dl = DataLoader(test_ds,\n                        batch_size = config[\"batch_size\"],\n                        shuffle=False,\n                        num_workers = 4,\n                        pin_memory=True)\n    \n    predictions = list()\n    for i, (inputs) in tqdm(enumerate(test_dl)):\n        inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n        outputs = model(**inputs)\n        outputs = outputs.cpu().detach().numpy().ravel().tolist()\n        predictions.extend(outputs)\n        \n    torch.cuda.empty_cache()\n    return np.array(predictions)","3dbd6b5f":"pred1 = get_prediction(test_data,'..\/input\/electra-large-fit8\/model0\/model0.bin')\npred2 = get_prediction(test_data,'..\/input\/electra-large-fit8\/model1\/model1.bin')\npred3 = get_prediction(test_data,'..\/input\/electra-large-fit8\/model2\/model2.bin')\n\npredictions1 = (pred1 + pred2 + pred3)\/3","807ad088":"sample['target'] = predictions1\nsample.to_csv('submission.csv',index=False)\nsample","7f9b277d":"# Original context from @maunish\n\nThis is inference notebooks that is trained using below notebooks.\n\nThis notebook uses the model created in pretrain any model notebook.\n\n1. Pretrain Roberta Model: https:\/\/www.kaggle.com\/maunish\/clrp-pytorch-roberta-pretrain\n2. Finetune Roberta Model: https:\/\/www.kaggle.com\/maunish\/clrp-pytorch-roberta-finetune <br\/>\n   Finetune Roberta Model TPU: https:\/\/www.kaggle.com\/maunish\/clrp-pytorch-roberta-finetune-tpu\n3. Inference Notebook: this notebook\n4. Roberta + SVM: https:\/\/www.kaggle.com\/maunish\/clrp-roberta-svm\n","0c137e66":"# Dataset class","4b420bb7":"# Electra Model","e314359e":"# Prediction","44b266a3":"# Submission","98b0eb97":"# Predict","5c4e5359":"# Imports","a48d6128":"# About this notebook\n\nThis uses the implementation from @maunish as a basis and makes an inference using Electra-large model from google, scoring 0.504 on LB.\n\nIf you like it, please upvote maunish's notebooks mentioned below.\n\nThis uses a dataset that contains Electra pre-trained for 3 epochs, trained for 8 epochs, 3 folds","77b1bc1b":"# Load data","a5234826":"# Config"}}