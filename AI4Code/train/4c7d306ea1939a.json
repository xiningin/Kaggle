{"cell_type":{"67909313":"code","3ae6b9a6":"code","ed68b75f":"code","2f76db25":"code","b96fc6cd":"code","fa2cf403":"code","b91637f6":"code","2734abc7":"code","67846fc1":"code","a303e2c2":"code","1abcb005":"code","39f71673":"code","0853ca8f":"code","fd3cd834":"code","f9bf67fa":"code","2a97e0aa":"code","6a11e2cd":"code","2927959c":"markdown","d21e6e0f":"markdown","825a6066":"markdown","14887548":"markdown","53fd99ef":"markdown","4f914e8d":"markdown","43900996":"markdown"},"source":{"67909313":"# To store data\nimport pandas as pd\n\n# To do linear algebra\nimport numpy as np\n\n# To create plots\nimport matplotlib.pyplot as plt\n\n# To create nicer plots\nimport seaborn as sns\n\n# To search directories\nimport os\n\n# To get progression bars\nfrom tqdm import tqdm\n\n# To play sound in notebooks\nimport IPython.display as ipd\n\n# To create models\nfrom keras.models import Sequential\nfrom keras.layers import Conv1D, Dense, Dropout, MaxPool1D, Flatten","3ae6b9a6":"# Load sample and ids\ntrain = pd.read_csv('..\/input\/train.csv')\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv')\n\n# Path to files\ntrain_path = '..\/input\/audio_train\/'\ntest_path = '..\/input\/audio_test\/'\n\nprint('Each file in the csv-submission has three possible concatenated labels.')\nprint('Sample Submission Shape:\\t{}'.format(sample_submission.shape))\nsample_submission.head()","ed68b75f":"print('Each file has a label and a marker weather it has been verified by a human.')\nprint('Train Shape:\\t{}'.format(train.shape))\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntrain_df.head()","2f76db25":"train_files = os.listdir(train_path)\nprint('Number Of Train Files:\\t{}'.format(len(train_files)))\n\ntest_files = os.listdir(test_path)\nprint('Number Of Test Files:\\t{}'.format(len(test_files)))","b96fc6cd":"title = 'Distribution Of Labels'\nlabels_grouped = train_df.groupby(['label', 'manually_verified']).count().rename(columns={'fname':'Verified'})\nlabels_grouped = labels_grouped.unstack().reindex(labels_grouped.unstack().sum(axis=1).sort_values(ascending=False).index)\nlabels_grouped.columns = ['Unverified', 'Verified']\nlabels_grouped.plot(kind='barh', stacked=True, title=title, figsize=(16,9))\nplt.xlabel('Count')\nplt.ylabel('Label')\nplt.show()","fa2cf403":"from scipy.io import wavfile\nfname, label, verified = train_df.sample(1).values[0]\nrate, data = wavfile.read(train_path+fname)\nprint(label)\nprint('Sampling Rate:\\t{}'.format(rate))\nprint('Total Frames:\\t{}'.format(data.shape[0]))\nprint(data)","b91637f6":"n = 3\nfig, axarr = plt.subplots(n, 1, figsize=(16, 2*n))\nfor i, (fname, label) in enumerate(train_df.sample(n)[['fname', 'label']].values):\n    rate, data = wavfile.read(train_path+fname)\n    axarr[i].plot(data)\n    axarr[i].set_title(label)\nplt.tight_layout()\nplt.show()","2734abc7":"print('Sound:\\t{}'.format(train[train['fname']==fname]['label'].values[0]))\nipd.Audio(train_path+fname)","67846fc1":"file_length = []\nfor file in tqdm([train_path+file for file in os.listdir(train_path)] + [test_path+file for file in os.listdir(test_path)]):\n    rate, data = wavfile.read(file)\n    file_length.append([len(data)\/rate, rate, file])\nlength_df = pd.DataFrame(file_length, columns=['length', 'rate', 'file'])\nlength_df['data'] = 'test'\nlength_df.loc[:train_df.shape[0], 'data'] = 'train'\n\nfig, axarr = plt.subplots(1, 2, figsize=(16,4))\nsns.distplot(length_df[length_df['data']=='train']['length'], ax=axarr[0])\naxarr[0].set_title('Train: Distribution File-Lengths')\naxarr[0].set_xlabel('Seconds')\nsns.distplot(length_df[length_df['data']=='test']['length'], ax=axarr[1])\naxarr[1].set_title('Test: Distribution File-Lengths')\naxarr[1].set_xlabel('Seconds')\nplt.show()","a303e2c2":"train_df['duration'] = length_df[length_df['data']=='train']['length']\n\nplt.figure(figsize=(16,4))\nsns.violinplot(data=train_df, y='duration', x='label')\nplt.title('File-Lengths Per Label')\nplt.xlabel('Label')\nplt.ylabel('Seconds')\nplt.xticks(rotation=90)\nplt.show()","1abcb005":"# Setup variables\ninput_length = 44100*10 # First 10 seconds for classification\nn_classes = train['label'].unique().shape[0]\n\n# Create model\nmodel = Sequential()\nmodel.add(Conv1D(filters=4, kernel_size=16, activation='relu', padding='same', input_shape=(input_length, 1)))\nmodel.add(MaxPool1D(pool_size=5))\nmodel.add(Dropout(rate=0.1))\nmodel.add(Conv1D(filters=6, kernel_size=16, activation='relu', padding='same'))\nmodel.add(MaxPool1D(pool_size=5))\nmodel.add(Dropout(rate=0.1))\nmodel.add(Conv1D(filters=9, kernel_size=16, activation='relu', padding='same'))\nmodel.add(MaxPool1D(pool_size=5))\nmodel.add(Dropout(rate=0.1))\nmodel.add(Conv1D(filters=14, kernel_size=16, activation='relu', padding='same'))\nmodel.add(MaxPool1D(pool_size=5))\nmodel.add(Dropout(rate=0.1))\nmodel.add(Conv1D(filters=21, kernel_size=16, activation='relu', padding='same'))\nmodel.add(MaxPool1D(pool_size=5))\nmodel.add(Dropout(rate=0.1))\nmodel.add(Conv1D(filters=31, kernel_size=16, activation='relu', padding='same'))\nmodel.add(MaxPool1D(pool_size=5))\nmodel.add(Dropout(rate=0.1))\nmodel.add(Conv1D(filters=46, kernel_size=16, activation='relu', padding='same'))\nmodel.add(MaxPool1D(pool_size=5))\nmodel.add(Dropout(rate=0.1))\nmodel.add(Flatten())\nmodel.add(Dense(units=100, activation='relu'))\nmodel.add(Dense(units=n_classes, activation='softmax'))\n\n# Compile model\nmodel.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\nmodel.summary()","39f71673":"# Map files to label\nfile_label_dict = {fname:label for fname, label in train[['fname', 'label']].values}\n\nexample_file = '6a446a35.wav'\nprint('File Label \"{}\":\\n{}'.format(example_file, file_label_dict[example_file]))\n\n\n# Create vector encoded labels\nlabelEncoder = {}\nfor i, label in enumerate(train['label'].unique()):\n    label_array = np.zeros(n_classes)\n    label_array[i] = 1\n    labelEncoder[label] = label_array\n\nexample_label = 'Cello'\nprint('\\nEncoded Label \"{}\":\\n{}'.format(example_label, labelEncoder[example_label]))\n\n# Remap predictions to label\nprediction_to_label = {np.argmax(array):label for label, array in labelEncoder.items()}","0853ca8f":"# Define batch generator to yield random data batches\ndef batchGenerator(files, batch_size):\n    # Generate infinite random batches\n    while True:\n        # Get random files\n        batch_files = np.random.choice(files, batch_size, replace=False)\n\n        # Get labels and data\n        batch_label = []\n        batch_data = []\n        # Combine batch\n        for file in batch_files:\n            # Get label and data\n            label = file_label_dict[file]\n            rate, data = wavfile.read(train_path+file)\n            # Trim data to get uniform length\n            data_uniform_length = np.zeros(input_length)\n            minimum = min(input_length, data.shape[0])\n            data_uniform_length[:minimum] = data[:minimum]\n            # Encode label\n            encoded_label = labelEncoder[label]\n            # Create label and data batch\n            batch_label.append(encoded_label)\n            batch_data.append(data_uniform_length)\n        # Format batches\n        batch_label = np.array(batch_label)\n        batch_data = np.array(batch_data).reshape(-1, input_length, 1)\n\n        # Batch normalisation\n        minimum, maximum = batch_data.min().astype(float), batch_data.max().astype(float)\n        batch_data = (batch_data - minimum) \/ (maximum - minimum)\n\n        # Yield batches for training\n        yield batch_data, batch_label","fd3cd834":"# Create random mask to split files in train and validation set\ntrain_val_split_mask  = np.zeros(train.shape[0], dtype=bool)\ntrain_val_split_mask[:8500] = True\nnp.random.shuffle(train_val_split_mask)\n\n# Get train and validation files\ntrain_files = train['fname'][train_val_split_mask]\nval_files = train['fname'][~train_val_split_mask]\n\n\n# Specify train and validation generators\nbatch_size = 50\ntrain_generator = batchGenerator(train_files, batch_size=batch_size)\nval_generator = batchGenerator(val_files, batch_size=50)","f9bf67fa":"model.fit_generator(generator=train_generator, validation_data=val_generator, validation_steps=10, epochs=20, steps_per_epoch=train.shape[0]\/\/batch_size)","2a97e0aa":"prediction = []\ntest_data = []\ntest_files = os.listdir(test_path)\nfor fname in tqdm(test_files):\n    rate, data = wavfile.read(test_path + fname)\n    # Trim data to get uniform length\n    data_uniform_length = np.zeros(input_length)\n    minimum = min(input_length, data.shape[0])\n    data_uniform_length[:minimum] = data[:minimum]\n    test_data.append(data_uniform_length)\n    \n    if len(test_data)==50:\n        test_data = np.array(test_data).reshape(-1, input_length, 1)\n        prediction.extend(model.predict(test_data))\n        test_data = []\ntest_data = np.array(test_data).reshape(-1, input_length, 1)\nprediction.extend(model.predict(test_data))\n\n#prediction = model.predict(test_data)\nprediction = np.array(prediction)\nbest_prediction = np.flip(prediction.argsort(), axis=1)[:, :3]\n\nfinal_prediction = []\nfor entry in best_prediction:\n    best_file_predictions = []\n    for label in entry:\n        best_file_predictions.append(prediction_to_label[label])\n    final_prediction.append(' '.join(best_file_predictions))\nfinal_prediction[:10]","6a11e2cd":"submission = pd.DataFrame()\nsubmission['fname'] = test_files\nsubmission['label'] = final_prediction\nsubmission.to_csv('Submission.csv', index=False)\nsubmission.head()","2927959c":"# Single Example Exploration","d21e6e0f":"# Train Model","825a6066":"# Import Libraries","14887548":"# File Lengths","53fd99ef":"# Load Data","4f914e8d":"# Label Exploration","43900996":"# Create Model"}}