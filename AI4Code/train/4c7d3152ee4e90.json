{"cell_type":{"4f437002":"code","f00f2c6b":"code","f59c7090":"code","c9557203":"code","df399cea":"code","77a0a261":"code","21dfa99e":"code","23468876":"code","04f28cdf":"code","bf1d5c05":"code","cff1b382":"code","78a0767d":"code","81d2c435":"code","714b6f53":"code","2876d864":"code","14abcd83":"code","3b9dd455":"code","e563b3cb":"code","b51316ec":"code","d2f9fd78":"code","d569debb":"code","70381e21":"code","38c7fd32":"code","d3a9d52a":"code","ae06c1f5":"code","50e63735":"code","b47dbd99":"code","4129625d":"code","c7c5f03a":"code","e0e9b935":"code","4acb4b64":"code","44c73656":"code","d32aed80":"code","6be58112":"code","8d5a8f2d":"code","abdb9205":"code","676a24d4":"code","4476d839":"code","b8799c0e":"code","47cbfa1a":"code","26629b3b":"code","606562a6":"code","d326952a":"code","08fca88c":"code","2386dcf0":"code","cf97122c":"code","c773fbf7":"code","bf652526":"code","8fba7144":"code","7a73e8fd":"code","d047b043":"code","a7112d3c":"code","c7a4c209":"code","ee496da4":"code","9c929caf":"code","8e49cbb1":"code","aed24fa3":"code","deec864a":"code","9357f52e":"code","ac5f990a":"code","60ca0aaf":"code","92c102da":"code","811777df":"markdown","d45a9eca":"markdown","7494976b":"markdown","c04bea98":"markdown","36fa2599":"markdown","164301b0":"markdown","36725c31":"markdown","9ba5fce4":"markdown","d76346ad":"markdown","d49c5b00":"markdown","5ea7bfd9":"markdown","4025b340":"markdown","24bc496a":"markdown","2eee6844":"markdown","87d6527c":"markdown","0640a8a9":"markdown","3fdd55f7":"markdown","e7e7b7e5":"markdown","5b21fd2d":"markdown","7e8b00b1":"markdown","181b938e":"markdown","a9424d75":"markdown","94e02f5f":"markdown","4462a9a4":"markdown","e8d48a92":"markdown"},"source":{"4f437002":"\n\n# # color palletes\n# male_female_pal = ['#3489d6', '#e64072']\n# survival_pal = ['#2a2a2a', '#ff0000']\n# sns.set_palette(survival_pal)\n# sns.set_style(\"whitegrid\")\n\n# from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n# from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, LabelBinarizer, scale, Normalizer, PowerTransformer, MaxAbsScaler\n# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n# from sklearn.svm import SVC, NuSVC, LinearSVC\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.tree import DecisionTreeClassifier\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.neighbors import KNeighborsClassifier\n\n# import lightgbm as lgb\n\n# import eli5\n# from eli5.sklearn import PermutationImportance","f00f2c6b":"# numerical analysis\nimport numpy as np\n# storing and processing in dataframes\nimport pandas as pd\n\n# basic plotting\nimport matplotlib.pyplot as plt\n# advanced plotting\nimport seaborn as sns\n\n# splitting dataset into train and test\nfrom sklearn.model_selection import train_test_split\n# scaling features\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelBinarizer, LabelEncoder\n# selecting important features\nfrom sklearn.feature_selection import RFECV\n# k nearest neighbors model\nfrom sklearn.neighbors import KNeighborsClassifier\n# accuracy\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc","f59c7090":"# plot style\nsns.set_style('whitegrid')\n\n# color palettes\nmale_female_pal = ['#3489d6', '#e64072']\nsurvival_pal = ['#2a2a2a', '#ff0000']\nsns.set_palette(survival_pal)","c9557203":"# get training dataset\ntrain = pd.read_csv('..\/input\/train.csv')\n\n# first few rows of train dataset\ntrain.head()","df399cea":"# get test dataset\ntest = pd.read_csv('..\/input\/test.csv')\n\n# first few rows of test dataset\ntest.head()","77a0a261":"# no. of rows and columns\ntrain.shape","21dfa99e":"#  columns names\ntrain.columns","23468876":"# consise summary of dataframe\ntrain.info()","04f28cdf":"# descriptive statistics\ntrain.describe(include='all')","bf1d5c05":"def get_missing_vals_info(df):\n    '''get no. of missing values information'''\n    \n    # no. of missing values in each column of the dataframe\n    print(df.isna().sum())\n    \n    # visualizing missing values in each column\n    \n    # plot figure\n    plt.figure(figsize=(12, 6))\n    # plot missing values heatmap\n    sns.heatmap(df.isna(), cbar=False, cmap='cividis')\n    # title\n    plt.title('Missing values in each columns')\n    # show the plot\n    plt.show()","cff1b382":"# missing values in train dataset\nget_missing_vals_info(train)","78a0767d":"# missing values in train dataset\nget_missing_vals_info(test)","81d2c435":"# Class distribution\n\nplt.figure(figsize=(4, 5))\nsns.countplot(x='Survived', data=train)\nplt.show()","714b6f53":"# How being in different categories resulted in the survival ?\n\ncat_cols = ['Pclass', 'Sex', 'Embarked']\n\nfig, ax = plt.subplots(1, 3, figsize=(15, 4))\nfor ind, val in enumerate(cat_cols):\n    sns.countplot(x=val, hue='Survived', data=train, ax=ax[ind])\n    ax[ind].legend(['Did not survived', 'Survived'])","2876d864":"# Did people hold on to their families ?\n\ncat_cols = ['SibSp', 'Parch']\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 4))\nfor ind, val in enumerate(cat_cols):\n    sns.countplot(x=val, hue='Survived', data=train, ax=ax[ind])\n    ax[ind].legend(['Did not survived', 'Survived'])","14abcd83":"fig, ax = plt.subplots(1, 2, figsize=(20, 5))\nfor ind, col in enumerate(['Age', 'Fare']):\n    ax[ind] = sns.kdeplot(train.loc[train['Survived']==0, col].dropna(), shade=True, ax=ax[ind])\n    ax[ind] = sns.kdeplot(train.loc[train['Survived']==1, col].dropna(), shade=True, ax=ax[ind])\n    ax[ind].set_xlabel(col)\n    ax[ind].legend(['Did not survived', 'Survived'])","3b9dd455":"# Correlation between columns\n\nplt.figure(figsize=(8, 6))\ndf_corr = train.drop('PassengerId', axis=1).corr()\nsns.heatmap(df_corr, annot=True, fmt='.2f', cmap='RdBu', vmax=0.8, vmin=-0.8)\nplt.show()","e563b3cb":"# Pairplot\n\nplt.figure(figsize=(7, 7))\nsns.pairplot(train.drop('PassengerId', axis=1), hue=\"Survived\", palette=survival_pal)\nplt.plot()","b51316ec":"# Filling Embarked with most frequent value\n# =========================================\n\nprint(train['Embarked'].value_counts())\nmost_freq = train['Embarked'].value_counts().index[0]\ntrain['Embarked'].fillna(most_freq, inplace=True)","d2f9fd78":"# Filling age with respect to title\n# ==================================\n\n# extracting the title\ntrain[\"Title\"] = train[\"Name\"].str.extract('([A-Za-z]+)\\.',expand=False)\ntest[\"Title\"] = test[\"Name\"].str.extract('([A-Za-z]+)\\.',expand=False)\n\n# replacing similar titles\nfor i in [train, test]:\n    i['Title'] = i['Title'].replace('Mr', 'Mr')\n    i['Title'] = i['Title'].replace(('Mme', 'Ms'), 'Mrs')\n    i['Title'] = i['Title'].replace('Mlle', 'Miss')\n    i['Title'] = i['Title'].replace(('Capt', 'Col', 'Major', 'Dr','Rev'), 'Officer')\n    i['Title'] = i['Title'].replace(('Jonkheer', 'Don', 'Sir', 'Countess','Dona', 'Lady'), 'Royalty')","d569debb":"# Title vs Age Distribution\n\nsns.set_palette('Paired')\nplt.figure(figsize=(15, 6))\n\nax = sns.kdeplot(train[train['Title']=='Mr']['Age'], shade=True, label='Mr')\nax = sns.kdeplot(train[train['Title']=='Mrs']['Age'], shade=True, label='Mrs')\nax = sns.kdeplot(train[train['Title']=='Miss']['Age'], shade=True, label='Miss')\nax = sns.kdeplot(train[train['Title']=='Master']['Age'], shade=True, label='Master')\nax = sns.kdeplot(train[train['Title']=='Officer']['Age'], shade=True, label='Officer')\n\nax.set_xlim(-10, 90)\nax.set_xlabel('Age')\nax.set_title('Distribution of Age of based on Title')\nplt.show()","70381e21":"# fill age wrt title\nfor df in [train, test]:\n    for title in df['Title'].unique():\n        age = df.loc[df['Title']==title, 'Age'].mean()\n        df[df['Title']==title].fillna(age, inplace=True)","38c7fd32":"train.info()","d3a9d52a":"# converting to catogorical values into categorical columns \n\ncat_cols = ['Pclass', 'Sex', 'Embarked']\nfor i in cat_cols:\n    train[i] = train[i].astype('category')\n    test[i] = test[i].astype('category')\n    \n# train.info()","ae06c1f5":"# Class - Gender - Survival\n\ng = sns.FacetGrid(train, col='Embarked', size=4)\ng.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', order=[1, 2, 3], \n      hue_order=['male', 'female'], palette=male_female_pal)\ng.add_legend()\nplt.show()","50e63735":"plt.figure(figsize=(10, 6))\nsns.countplot(x=\"Embarked\", hue=\"Title\", data=train)\nplt.show()","b47dbd99":"plt.figure(figsize=(10, 6))\nsns.countplot(x=\"Pclass\", hue=\"Title\", data=train)\nplt.show()","4129625d":"# Mean of of age wrt Title\n\ntr = train[['Age', 'Title']]\nts = test[['Age', 'Title']]\ntr_ts = pd.concat([tr, ts])\n\nprint(tr_ts.groupby('Title').mean())","c7c5f03a":"# plt.figure(figsize=(60,5))\n# ax = sns.countplot(x='Age', hue='Survived', data=train)\n# plt.legend(['Not Survived', 'Survived'])\n# plt.show()\n\n# plt.figure(figsize=(200, 5))\n# ax = sns.countplot(x='Fare', hue='Survived', data=train)\n# plt.legend(['Not Survived', 'Survived'])\n# plt.show()","e0e9b935":"# Binning Age and Fare\n\ntrain['age_cat'] = pd.cut(train['Age'], \n                          bins = [0, 0.99, 7, 23, 58, 100],\n                          labels = [\"infant\", \"child\", \"young\", \"adult\", \"senior\"],\n                          include_lowest=True)\ntest['age_cat'] = pd.cut(test['Age'], \n                         bins = [0, 0.99, 7, 23, 58, 100],\n                         labels = [\"infant\", \"child\", \"young\", \"adult\", \"senior\"],\n                         include_lowest=True)\n\ntrain['fare_cat'] = pd.cut(train['Fare'], \n                           bins = [0, 12, 40, 80, 1000],\n                           labels = ['least', 'low', 'mid', 'high'],\n                           include_lowest=True)\ntest['fare_cat'] = pd.cut(test['Fare'], \n                           bins = [0, 12, 40, 80, 1000],\n                           labels = ['least', 'low', 'mid', 'high'],\n                           include_lowest=True)","4acb4b64":"# Extracting Cabin Type from Cabin name\n\nc_train_type = train['Cabin'].str[0]\ntrain['c_type'] = c_train_type\ntrain['c_type'] = train['c_type'].fillna('unknown')\n\nc_test_type = test['Cabin'].str[0]\ntest['c_type'] = c_test_type\ntest['c_type'] = test['c_type'].fillna('unknown')","44c73656":"# Age, Fare, Cabin category vs Survival\n\nfig, ax = plt.subplots(1, 3, figsize=(24, 5))\nfor ind, val in enumerate(['age_cat', 'fare_cat', 'c_type']):\n    sns.countplot(x=val, hue='Survived', data=train, ax=ax[ind])","d32aed80":"# Family member count and Family Size and is alone\n\ntrain['fam_count'] = train['SibSp']+train['Parch']\ntest['fam_count'] = test['SibSp']+test['Parch']\n\nsize = {\n    0:'alone',\n    1:'small',\n    2:'small',\n    3:'small',\n    4:'large',\n    5:'large',\n    6:'large',\n    7:'large',\n    10:'large'\n}\n\ntrain['fam_size'] = train['fam_count'].map(size)\ntest['fam_size'] = test['fam_count'].map(size)\n\ntrain['is_alone'] = train['fam_size']=='alone'\ntest['is_alone'] = test['fam_size']=='alone'","6be58112":"fig, ax = plt.subplots(1, 3, figsize=(20, 5))\nfor ind, val in enumerate(['fam_count', 'fam_size', 'is_alone']):\n    sns.countplot(x=val, hue='Survived', data=train, ax=ax[ind])","8d5a8f2d":"# dataframe\n\ntrain.head()","abdb9205":"# Scaling Age and Fare\n\nmm = MinMaxScaler()\nfor i in ['Age', 'Fare']:\n    train[i] =  mm.fit_transform(train[i].values.reshape(-1,1))\n    test[i] =  mm.fit_transform(test[i].values.reshape(-1,1))","676a24d4":"# Label Binerizer Sex\n\nlb = LabelBinarizer()\nfor i in ['Sex', 'is_alone']:\n    train[i] =  lb.fit_transform(train[i])\n    test[i] =  lb.fit_transform(test[i])","4476d839":"# Label Encoding Pclass\n\nen = LabelEncoder()\ntrain['Pclass'] =  en.fit_transform(train['Pclass'])\ntest['Pclass'] =  en.fit_transform(test['Pclass'])","b8799c0e":"# Create dummies for nominal categorical columns\n\ndef create_dummies(df, column_name):\n    dummies = pd.get_dummies(df[column_name], prefix=column_name)\n    df = pd.concat([df,dummies],axis=1)\n    df.drop(column_name, axis=1, inplace=True)\n    return df\n\n# for i in ['Sex', 'SibSp', 'Parch', 'Embarked', 'Title', 'age_cat', 'fare_cat', 'c_type', 'fam_count', 'fam_size']:\n#     train = create_dummies(train, i)\n#     test = create_dummies(test, i)","47cbfa1a":"# Droping columns\n\ntrain.drop(['Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\ntest.drop(['Name', 'Ticket', 'Cabin'], axis=1, inplace=True)","26629b3b":"# dataframe\n\ntrain.head()","606562a6":"# Final correlation heatmap\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(train.drop('PassengerId', axis=1).corr(), annot=True, fmt='.1f', cmap='RdBu', vmax=0.8, vmin=-0.8)\nplt.show()","d326952a":"# X = train.drop(['Survived', 'PassengerId'], axis=1)\nfeatures = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'fam_count', 'is_alone']\nX = train[features]\ny = train['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","08fca88c":"# naive bayes\n\nnb = GaussianNB()\nnb.fit(X_train, y_train)\ny_pred = nb.predict(X_test)\naccuracy_score(y_pred, y_test)\n#print(classification_report(y_pred, y_test))","2386dcf0":"plt.figure(figsize=(3,3))\nsns.heatmap(confusion_matrix(y_pred, y_test), annot=True, cbar=False, fmt='1d', cmap='Blues')\nplt.show()","cf97122c":"perm = PermutationImportance(nb, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","c773fbf7":"# Logistic regression\n\nlr = LogisticRegression(C = 1, penalty= 'l2', solver= 'liblinear')\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\naccuracy_score(y_pred, y_test)","bf652526":"#print(classification_report(y_pred, y_test))\nplt.figure(figsize=(3,3))\nsns.heatmap(confusion_matrix(y_pred, y_test), annot=True, cbar=False, fmt='1d', cmap='Blues')\nplt.show()","8fba7144":"perm = PermutationImportance(lr, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","7a73e8fd":"# svm\n\nmodel = SVC()\n\nhyperparameters = {\n    'C': [0.1, 1, 10, 100],\n    'gamma': [1, 0.1, 0.01],\n    'kernel': ['rbf', 'linear']\n}\n\ngrid = GridSearchCV(model, param_grid=hyperparameters, cv=10)\ngrid.fit(X, y)\n\nbest_params = grid.best_params_\nbest_score = grid.best_score_\n\nsvc = grid.best_estimator_\ny_pred = svc.predict(X_test)\n\nprint(grid.best_params_)\nprint(grid.best_estimator_)\nprint(grid.best_score_)","d047b043":"#print(classification_report(y_pred, y_test))\nplt.figure(figsize=(3,3))\nsns.heatmap(confusion_matrix(y_pred, y_test), annot=True, cbar=False, fmt='1d', cmap='Blues')\nplt.show()","a7112d3c":"perm = PermutationImportance(svc, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","c7a4c209":"# K Nearest Neighbours\n\nmodel = KNeighborsClassifier()\n\nhyperparameters = {\n    \"n_neighbors\" : range(1,20,2),\n    'weights' : ['uniform', 'distance'],\n    'p' : [1, 2]\n}\n\ngrid = GridSearchCV(model, param_grid=hyperparameters, cv=10)\ngrid.fit(X, y)\n\nbest_params = grid.best_params_\nbest_score = grid.best_score_\n\nknn = grid.best_estimator_\ny_pred = knn.predict(X_test)\n\nprint(grid.best_params_)\nprint(grid.best_estimator_)\nprint(grid.best_score_)","ee496da4":"#print(classification_report(y_pred, y_test))\nplt.figure(figsize=(3,3))\nsns.heatmap(confusion_matrix(y_pred, y_test), annot=True, cbar=False, fmt='1d', cmap='Blues')\nplt.show()","9c929caf":"perm = PermutationImportance(knn, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","8e49cbb1":"# Decision Tree\n\ndt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)\ny_pred = dt.predict(X_test)\naccuracy_score(y_pred, y_test)\n#print(classification_report(y_pred, y_test))","aed24fa3":"from sklearn import tree\nimport graphviz \n\ndot_data = tree.export_graphviz(dt, out_file=None) \ngraph = graphviz.Source(dot_data) \ngraph.render(\"iris\")\n\ndot_data = tree.export_graphviz(dt, out_file=None, \n                     feature_names=X_train.columns,  \n                     class_names=['Survived', 'Not Survived'],  \n                     filled=True, rounded=True,  \n                     special_characters=True)  \ngraph = graphviz.Source(dot_data)  \ngraph ","deec864a":"hyperparameters = {\"criterion\": [\"entropy\", \"gini\"],\n                   \"max_depth\": [3, 5, 7, 10],\n                   \"max_features\": [\"log2\", \"sqrt\", 'auto'], \n                   'min_samples_leaf' : [2, 3, 4, 5],\n                   'min_samples_split' : [2, 3, 4, 5]\n}\n\ngrid = GridSearchCV(dt, param_grid=hyperparameters, cv=10)\ngrid.fit(X, y)\n\nbest_params = grid.best_params_\nbest_score = grid.best_score_\n\ndt = grid.best_estimator_\ny_pred = dt.predict(X_test)\n\nprint(grid.best_params_)\nprint(grid.best_score_)\n\n#print(classification_report(y_pred, y_test))\nplt.figure(figsize=(3,3))\nsns.heatmap(confusion_matrix(y_pred, y_test), annot=True, cbar=False, fmt='1d', cmap='Blues')\n\nperm = PermutationImportance(dt, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","9357f52e":"# Random Forest\n\nmodel = RandomForestClassifier()\n\nhyperparameters = {\"criterion\": [\"entropy\", \"gini\"],\n                   \"max_depth\": [5, 10],\n                   \"max_features\": [\"log2\", \"sqrt\"],\n                   'min_samples_leaf' : [2, 3, 4, 5],\n                   'min_samples_split' : [2, 3, 4, 5],\n                   \"n_estimators\": [6, 9]\n}\n\ngrid = GridSearchCV(model, param_grid=hyperparameters, cv=10)\ngrid.fit(X, y)\n\nbest_params = grid.best_params_\nbest_score = grid.best_score_\n\nrf = grid.best_estimator_\ny_pred = rf.predict(X_test)\n\nprint(grid.best_params_)\nprint(grid.best_score_)\n\n#print(classification_report(y_pred, y_test))\nplt.figure(figsize=(3,3))\nsns.heatmap(confusion_matrix(y_pred, y_test), annot=True, cbar=False, fmt='1d', cmap='Blues')\n\nperm = PermutationImportance(rf, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","ac5f990a":"# test.head()","60ca0aaf":"# test.isnull().sum()","92c102da":"holdout_ids = test[\"PassengerId\"]\nholdout_features = test[features]\nholdout_predictions = lr.predict(holdout_features)\n\nsubmission = pd.DataFrame({\"PassengerId\": holdout_ids, \n                           \"Survived\": holdout_predictions})\nprint(submission.head())\n\nsubmission.to_csv(\"submission.csv\",index=False)","811777df":"# EDA","d45a9eca":"* Survival\n    > * Survival\n        0 - No - Did not survived\n        1 - Yes - Survived\n\n\n* Pclass\n    > * Ticket class  \n    > * A proxy for socio-economic status (SES) \n        1 - 1st - Upper   \n        2 - 2nd - Middle   \n        3 - 3rd - Lower \n        \n       \n* Sex\n    > * Gender\n    > * Male or Female\n    \n   \n* Age\n    > * Age in years\n    > * Age is fractional if less than 1. \n    > * If the age is estimated, is it in the form of xx.5\n    \n    \n* Sibsp\n    > * No. of siblings \/ spouses aboard the Titanic \n    > * Sibling = brother, sister, stepbrother, stepsister\n    > * Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n    \n    \n* Parch\n    > * No. of parents \/ children aboard the Titanic \n    > * Parent = mother, father\n    > * Child = daughter, son, stepdaughter, stepson\n    > * Some children travelled only with a nanny, therefore parch=0 for them.\n    \n    \n* Ticket\n    > * Ticket number \n    \n    \n* Fare\n    > * Passenger fare \n    \n    \n* Cabin\n    > * Cabin number \n    \n    \n* Embarked\n    > * Port of Embarkation \n        C = Cherbourg\n        Q = Queenstown\n        S = Southampton","7494976b":"## KNN","c04bea98":"# Inspecting Dataframes","36fa2599":"## Decision Tree","164301b0":"# Data properties","36725c31":"## Logistic Regression","9ba5fce4":"# Visual Exploration","d76346ad":"## Naive Bayes","d49c5b00":"# Importing Libraries","5ea7bfd9":"### Theme","4025b340":"### Datatypes","24bc496a":"### Class distribution","2eee6844":"### Task\n* test dataset doesn't have the column 'Survived'\n* we need to come up with a ML model that can predict 'Survived' value\n* we need to find wether the passenger survived or did not survived\n* this is a classic Classification task","87d6527c":"# Train - Test data splitting","0640a8a9":"# Holdout Prediction","3fdd55f7":"## SVM","e7e7b7e5":"# Preprocessing","5b21fd2d":"### Missing values","7e8b00b1":"# Models","181b938e":"# Preprocessing","a9424d75":"### Features \/ Columns","94e02f5f":"# Data","4462a9a4":"### No. of values in each category","e8d48a92":"### Missing values"}}