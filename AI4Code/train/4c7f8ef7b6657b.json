{"cell_type":{"97d87c85":"code","4a8610ef":"code","cd00b9d2":"code","7ddba0cb":"code","ef8045fd":"code","8ac32b76":"code","9aebd33a":"code","c107c0b6":"code","2d8a0a04":"code","0a0d1dca":"code","e4618631":"code","00173552":"code","24932ccf":"code","2aa32d0f":"code","e8dd0f85":"code","c5c8287f":"code","c5b3efbe":"code","91372174":"code","5e58723f":"code","a67b81ff":"code","59f0ed36":"code","767a098c":"code","8bac6302":"code","8e655675":"code","1949d766":"code","b79a2c8d":"code","70c19051":"code","f1731916":"code","b6ec10c2":"markdown","b111a053":"markdown","86cb2463":"markdown","af9e52b5":"markdown","99864e9d":"markdown","ac16dcdd":"markdown","da507b99":"markdown"},"source":{"97d87c85":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt","4a8610ef":"dataset = pd.DataFrame(pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv'))","cd00b9d2":"dataset.tail(10)","7ddba0cb":"dataset[dataset.Time.isnull() == True ]","ef8045fd":"dataset[dataset.Time.isna() == True ]","8ac32b76":"dataset.describe()","9aebd33a":"dataset.info()","c107c0b6":"dataset.isnull().sum()","2d8a0a04":"y = dataset['Class']","0a0d1dca":"y","e4618631":"dataset.drop(columns='Class', inplace=True)","00173552":"dataset.shape","24932ccf":"# 142,400 record for trainning set\n# 85,440 records for Testing set\n# 56,960 records for validation\nX_train = dataset[:142400]\nX_test = dataset[142400:227840]\nX_val = dataset[227840:]","2aa32d0f":"X_train.shape","e8dd0f85":"Y_train = y[:142400]\nY_test = y[142400:227840]\nY_val = y[227840:]","c5c8287f":"Y_train.shape","c5b3efbe":"from sklearn.preprocessing import StandardScaler\nX_train['Amount'] = StandardScaler().fit_transform(X_train['Amount'].values.reshape (-1,1))\nX_test['Amount'] = StandardScaler().fit_transform(X_test['Amount'].values.reshape (-1,1))\nX_val['Amount'] = StandardScaler().fit_transform(X_val['Amount'].values.reshape (-1,1))","91372174":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.regularizers import l2","5e58723f":"network = Sequential()","a67b81ff":"network.add(layers.Dense(10, activation='relu', kernel_regularizer=l2(0.001), input_shape=(X_train.shape[1],)))\nnetwork.add(layers.Dense(8, activation='relu', kernel_regularizer=l2(0.001)))\nnetwork.add(layers.Dense(1, activation='sigmoid'))","59f0ed36":"network.compile(optimizer='adam', loss='binary_crossentropy', metrics='accuracy')","767a098c":"network_history = network.fit(X_train, Y_train, epochs=50, batch_size=500, validation_data=(X_val, Y_val))","8bac6302":"train_loss = network_history.history['loss']","8e655675":"val_loss = network_history.history['val_loss']","1949d766":"epochs = range(1,51)","b79a2c8d":"plt.plot(epochs, train_loss, 'r', label='Training Loss' )\nplt.plot(epochs, val_loss, 'g', label='Validation Loss')\nplt.title('Training and Validation loss Plot.')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","70c19051":"network.predict(X_test)","f1731916":"network.evaluate(X_test, Y_test)","b6ec10c2":"<center>Thanks \ud83d\ude1d<\/center>","b111a053":"#Visualizing Training and Validation Loss","86cb2463":"# Normalization\n","af9e52b5":"Split into 50% Training(Samples,Labels) , 30% Test(Samples,Labels) and 20% Validation Data(Samples,Labels).","99864e9d":"#Splitting dataset into Train and Test set","ac16dcdd":"#data preprocessing","da507b99":"#Building the ANN\nImporting Regularizers to add a penalty for weight size to the loss function and avoiding overfitting."}}