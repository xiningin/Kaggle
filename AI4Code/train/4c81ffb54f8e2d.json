{"cell_type":{"93941776":"code","253ca876":"code","7860d2af":"code","1af42b48":"code","96d176ba":"code","4b87edf0":"code","11021391":"code","c4f18bde":"code","daab9be7":"code","1938c732":"code","4197d75a":"code","64bfc422":"code","d47fc4e6":"markdown","6454d438":"markdown","03565bbd":"markdown","2a46c021":"markdown","9b30054b":"markdown","89ec7d93":"markdown","b2061cbc":"markdown","162975db":"markdown","6de52eee":"markdown","93fd4990":"markdown"},"source":{"93941776":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\nfrom tqdm import tqdm\nprint(os.listdir(\"..\/input\"))","253ca876":"from keras import Sequential\nfrom keras import optimizers\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential,Model\nfrom keras.layers import LSTM, Dense, Bidirectional, Input,Dropout,BatchNormalization,CuDNNLSTM, GRU, CuDNNGRU, Embedding, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom keras.layers import *\nfrom sklearn.metrics import *","7860d2af":"# https:\/\/www.kaggle.com\/qqgeogor\/keras-lstm-attention-glove840b-lb-0-043\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim\n","1af42b48":"train = pd.read_json('..\/input\/train.json')\ntest = pd.read_json('..\/input\/test.json')\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv')\nxtrain = [k for k in train['audio_embedding']]\ntest_data = test['audio_embedding'].tolist()\nytrain = train['is_turkey'].values\n# Pad the audio features so that all are \"10 seconds\" long\nx_train = pad_sequences(xtrain, maxlen=10)\ny_train = np.asarray(ytrain)","96d176ba":"train.head()","4b87edf0":"def eva_plot(History, epoch):\n    plt.figure(figsize=(20,10))\n    sns.lineplot(range(1, epoch+1), History.history['acc'], label='Train Accuracy')\n    sns.lineplot(range(1, epoch+1), History.history['val_acc'], label='Test Accuracy')\n    plt.legend(['train', 'validaiton'], loc='upper left')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.show()\n    plt.figure(figsize=(20,10))\n    sns.lineplot(range(1, epoch+1), History.history['loss'], label='Train loss')\n    sns.lineplot(range(1, epoch+1), History.history['val_loss'], label='Test loss')\n    plt.legend(['train', 'validaiton'], loc='upper left')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.show()\n\n\ndef get_model():\n    model = Sequential()\n    model.add(BatchNormalization(momentum=0.98,input_shape=(10, 128)))\n    model.add(Bidirectional(GRU(128, return_sequences = True)))\n    # model.add(Bidirectional(CuDNNLSTM(1, return_sequences = True)))\n    model.add(Attention(10))\n    model.add(Dense(1,activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer = optimizers.Adam(lr=0.001), metrics=['accuracy'])\n#     print(model.summary())\n    return model","11021391":"kf = KFold(n_splits=10, shuffle=True, random_state=42069)\npreds = []\ntest_data = pad_sequences(test_data)\nfold = 0\naucs = 0\nfor train_idx, val_idx in kf.split(x_train):\n    x_train_f = x_train[train_idx]\n    y_train_f = y_train[train_idx]\n    x_val_f = x_train[val_idx]\n    y_val_f = y_train[val_idx]\n    model = get_model()\n    History = model.fit(x_train_f, y_train_f,\n              batch_size=256,\n              epochs=12,\n              verbose = 0,\n              validation_data=(x_val_f, y_val_f))\n    eva_plot(History, epoch = 12)\n    # Get accuracy of model on validation data. It's not AUC but it's something at least!\n    preds_val = model.predict([x_val_f], batch_size=512)\n    preds.append(model.predict(test_data))\n    fold+=1\n    fpr, tpr, thresholds = roc_curve(y_val_f, preds_val, pos_label=1)\n    aucs += auc(fpr,tpr)\n    print('Fold {}, AUC = {}'.format(fold,auc(fpr, tpr)))\nprint(\"Cross Validation AUC = {}\".format(aucs\/10))","c4f18bde":"preds = np.asarray(preds)[...,0]\npreds = np.mean(preds, axis=0)\nsub_df = pd.DataFrame({'vid_id':test['vid_id'].values,'is_turkey':preds})\n# sub_df.to_csv('submission.csv', index=False)","daab9be7":"probs = sub_df.is_turkey.values\nn,bins,_ = plt.hist(probs,bins=100)\nprint(n, bins)\npos_threshold = 0.99\nneg_threshold = 0.01\npseudo_index = np.argwhere(np.logical_or(probs > pos_threshold, probs < neg_threshold ))[:,0]","1938c732":"pseudo_x_train = test_data[pseudo_index]\npseudo_y_train = probs[pseudo_index]\npseudo_y_train[pseudo_y_train > 0.5] = 1\npseudo_y_train[pseudo_y_train <= 0.5] = 0\nx_train = np.concatenate([x_train, pseudo_x_train],axis=0)\ny_train = np.concatenate([y_train,pseudo_y_train])\nprint(x_train.shape, y_train.shape)","4197d75a":"kf = KFold(n_splits=10, shuffle=True, random_state=42069)\npreds = []\ntest_data = pad_sequences(test_data)\nfold = 0\naucs = 0\nfor train_idx, val_idx in kf.split(x_train):\n    x_train_f = x_train[train_idx]\n    y_train_f = y_train[train_idx]\n    x_val_f = x_train[val_idx]\n    y_val_f = y_train[val_idx]\n    model = get_model()\n    History = model.fit(x_train_f, y_train_f,\n              batch_size=256,\n              epochs=12,\n              verbose = 0,\n              validation_data=(x_val_f, y_val_f))\n    eva_plot(History, epoch = 12)\n    # Get accuracy of model on validation data. It's not AUC but it's something at least!\n    preds_val = model.predict([x_val_f], batch_size=512)\n    preds.append(model.predict(test_data))\n    fold+=1\n    fpr, tpr, thresholds = roc_curve(y_val_f, preds_val, pos_label=1)\n    aucs += auc(fpr,tpr)\n    print('Fold {}, AUC = {}'.format(fold,auc(fpr, tpr)))\nprint(\"Cross Validation AUC = {}\".format(aucs\/10))","64bfc422":"preds = np.asarray(preds)[...,0]\npreds = np.mean(preds, axis=0)\nsub_df = pd.DataFrame({'vid_id':test['vid_id'].values,'is_turkey':preds})\nsub_df.to_csv('submission.csv', index=False)","d47fc4e6":"## 6.Result Distribution","6454d438":"## 8.Final Model Training","03565bbd":"## 7.Filter Data Using Threshold","2a46c021":"## 1.Import Dataset","9b30054b":"## 3.Split Dataset","89ec7d93":"## 5.KFold LSTM Model Training","b2061cbc":"# **Happy Thanks Giving!**\n\n---\nOutline of Notebook\n---\n* [**1.Import Dataset**](#1.Import-Dataset)\n* [**2.Design Attention Layer**](#2.Design-Attention-Layer)\n* [**3.Split Dataset**](#3.Split-Dataset)\n* [**4.Model Desing and Result Evaluation**](#4.Model-Desing-and-Result-Evaluation)\n* [**5.KFold LSTM Model Training**](#5.KFold-LSTM-Model-Training)\n* [**6.Result Distribution**](#6.Result-Distribution)\n* [**7.Filter Data Using Threshold**](#7.Filter-Data-Using-Threshold)\n* [**8.Final Model Training**](#8.Final-Model-Training)\n* [**9.Submission**](#9.Submission)\n\n---","162975db":"## 4.Model Desing and Result Evaluation","6de52eee":"## 9.Submission","93fd4990":"## 2.Design Attention Layer"}}