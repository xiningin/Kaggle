{"cell_type":{"d1e6802c":"code","6a4af05d":"code","500488f7":"code","478464d1":"code","c09e7b3f":"code","66188523":"code","bc4adab9":"code","bce9ee8f":"code","e1cf6239":"code","08c54d7a":"code","d8a9cf4f":"code","1ca21cff":"code","ae52ba5d":"code","de9d65c0":"code","28eae67c":"code","fad66e3b":"code","3af172e7":"code","b6f66e31":"markdown","0586e91b":"markdown","8715eab4":"markdown","9d7a9218":"markdown","60cd413f":"markdown","9dd5ab9e":"markdown","17af5e2f":"markdown","36dc6d6d":"markdown","bc10d334":"markdown"},"source":{"d1e6802c":"!pip install ktrain","6a4af05d":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport ktrain\nfrom ktrain import text\nimport tensorflow as tf","500488f7":"tf.__version__","478464d1":"predictor1 = ktrain.load_predictor('..\/input\/imdbbert\/')\n\n#sample dataset to test on\n\ndata = ['Cinematography was amazing, however the story was boring. One time watch',\n        'The protogonist acted very well. Director has done amazing job. Always nice to wacth Nolan movies',\n        'They are both immature, selfish, and self-centered people. They hurt EVERYBODY around them playing their silly game']\n\npredictor1.predict(data)","c09e7b3f":"import os\nos.chdir(r'\/kaggle\/working')","66188523":"!git clone https:\/\/github.com\/laxmimerit\/IMDB-Movie-Reviews-Large-Dataset-50k.git","bc4adab9":"#loading the train dataset\ndata_train = pd.read_excel('IMDB-Movie-Reviews-Large-Dataset-50k\/train.xlsx', dtype = str)\n\n#loading the test dataset\ndata_test = pd.read_excel('IMDB-Movie-Reviews-Large-Dataset-50k\/test.xlsx', dtype = str)\n\n#dimension of the dataset\n\nprint(\"Size of train dataset: \",data_train.shape)\nprint(\"Size of test dataset: \",data_test.shape)","bce9ee8f":"#printing last rows of train dataset\n\ndata_train.tail()","e1cf6239":"# text.texts_from_df return two tuples\n# maxlen means it is considering that much words and rest are getting trucated\n# preprocess_mode means tokenizing, embedding and transformation of text corpus(here it is considering BERT model)\n\n\n(X_train, y_train), (X_test, y_test), preproc = text.texts_from_df(train_df=data_train,\n                                                                   text_column = 'Reviews',\n                                                                   label_columns = 'Sentiment',\n                                                                   val_df = data_test,\n                                                                   maxlen = 500,\n                                                                   preprocess_mode = 'bert')","08c54d7a":"# name = \"bert\" means, here we are using BERT model.\n\nmodel = text.text_classifier(name = 'bert',\n                             train_data = (X_train, y_train),\n                             preproc = preproc)","d8a9cf4f":"#here we have taken batch size as 6 as from the documentation it is recommend to use this with maxlen as 500\n\nlearner = ktrain.get_learner(model=model, train_data=(X_train, y_train),\n                   val_data = (X_test, y_test),\n                   batch_size = 6)","1ca21cff":"# find out best learning rate?\n# learner.lr_find(max_epochs=2)","ae52ba5d":"# learner.lr_plot(n_skip_beginning=2200, n_skip_end=4100)\n","de9d65c0":"#Essentially fit is a very basic training loop, whereas fit one cycle uses the one cycle policy callback\n## After running the lr_find learnin rate with 2e-5 was having minimum loss. So using it for this dataset\n\nlearner.fit_onecycle(lr = 2e-5, epochs = 1)\n\npredictor = ktrain.get_predictor(learner.model, preproc)\n\n","28eae67c":"## Save the model for future use. Also use FileLink to download the files, so that you can upload it later\npredictor.save('\/kaggle\/working\/bert')\n!ls bert\/\nFileLink(r'bert\/tf_model.preproc')","fad66e3b":"#sample dataset to test on\n\ndata = ['Cinematography was amazing, however the story was boring. One time watch',\n        'The protogonist acted very well. Director has done amazing job. Always nice to wacth Nolan movies',\n        'They are both immature, selfish, and self-centered people. They hurt EVERYBODY around them playing their silly game']","3af172e7":"predictor.predict(data)","b6f66e31":"# Save the BERT model","0586e91b":"# Try and load the model, if the model is saved\n\nLet us try to load the model already trained. ","8715eab4":"Splitting data into Train and Test:","9d7a9218":"Observation: Both train and test dataset is having 25000 rows and 2 columns\n\nSee last few rows","60cd413f":"Loading the Data","9dd5ab9e":"# Build the BERT model\n\nIf the model is not ready, please follow on and build the model and save it for future use","17af5e2f":"Observation:\n\nWe can see that has detected language as an English\nAlso, this is not a multilabel classification","36dc6d6d":"Let us find best Learning Rate","bc10d334":"Clone the Dataset from Large movie Dataset from IMDB"}}