{"cell_type":{"aedbcf3a":"code","6a37affb":"code","6de8fa79":"code","3e2bca04":"code","f31d354f":"code","405ac404":"code","adf346d2":"code","4fd9f13e":"code","5661cd16":"code","54be32a5":"code","837c87e4":"code","cd023583":"code","a22a465a":"code","30ae3429":"code","caf1f67e":"code","1ef25c0f":"code","f4b599fd":"code","b6501208":"code","18ff1ac2":"code","87dab398":"code","1a330831":"code","393c810b":"code","50de1dd4":"code","e17fbcca":"code","8ecc68b6":"code","b9ef9cc4":"code","470070fb":"code","cfd72341":"code","f94e87f2":"code","13386135":"code","d6a2b489":"code","d74348b1":"code","4d7dca59":"code","210389d4":"code","235582c7":"code","73d094f0":"code","fcd29459":"code","16689ea9":"code","9042dfaf":"code","8a745b09":"code","e5142161":"code","d7a5cae5":"code","7f48b9a7":"code","9ae7b341":"code","65c37c26":"code","a83f866d":"code","a62c62ff":"code","b49f2df0":"code","ce809485":"code","9d88659e":"code","5d06d743":"code","43387f97":"code","7622da2e":"code","760b607b":"code","e38285e4":"code","c5360434":"code","619fddfd":"code","236e1b57":"code","1a020ee9":"code","fac2a6b4":"code","a3207334":"code","8ac9d5d1":"code","b1e2b83a":"code","e862c68b":"code","e26d9778":"code","bcbc45f1":"code","b21794f2":"code","bdceaaa7":"markdown","4198e411":"markdown","ee13965a":"markdown","288c76c2":"markdown","176bcbd9":"markdown","d4098b66":"markdown","aca72c22":"markdown","432fefd1":"markdown","a0aeac05":"markdown","3757547e":"markdown","b30d1165":"markdown","a5095e8d":"markdown","f0ee8cc6":"markdown","d832d0e2":"markdown","e83e8c16":"markdown","9a6d1e51":"markdown","2bbc20ad":"markdown","e32dfa2a":"markdown","cd8e65d8":"markdown","efe1d6d8":"markdown","3e41e9f9":"markdown","05da610b":"markdown","4631ddc5":"markdown","8a713a90":"markdown","4d2cff16":"markdown","b88a7e8e":"markdown","681fda55":"markdown","3a36963d":"markdown","bc85c8ea":"markdown","6d8886c3":"markdown","a1df6fec":"markdown","3c8f1d23":"markdown","40d13549":"markdown","40078a04":"markdown","db3a1216":"markdown"},"source":{"aedbcf3a":"# Importing all the tools we need.\n\n# Regular EDA (exploratory Data analysis) and visualisation libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#To make our plots visible on notebook\n%matplotlib inline  \n\n# Models from Scikit-Learn.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Model Evaluations \nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve","6a37affb":"df = pd.read_csv('..\/input\/heart-disease.csv')\ndf","6de8fa79":"df.shape # (rows, columns)\n","3e2bca04":"df.tail()\n","f31d354f":"# How many of the each class\ndf['target'].value_counts()\n","405ac404":"# Visualising the targets\nax = sns.countplot(x='target',data=df);\n#df['target'].value_counts().plot(kind='bar',color=['green','red']);\ndict1={0:'Safe',1:'danger'}\nax.legend(dict1.values())\n\nplt.title('Targets count 1\/0');","adf346d2":"df.info()\n","4fd9f13e":"df.describe()\n","5661cd16":"df.isna().sum()\n","54be32a5":"df.sex.value_counts()\n","837c87e4":"# compare target column with sex column\npd.crosstab(df.target,df.sex)","cd023583":"print(f'Chance of a male having a disease: {((93\/207)*100):.2f}%')\nprint(f'Chance of a female having a disease: {((72\/96)*100):.2f}%')\n","a22a465a":"#create a plot of crosstab\npd.crosstab(df.target,df.sex).plot(kind='bar',figsize=(10,6),color=['salmon','lightblue']);\nplt.title('Heart Disease frequency for sex');\n#plt.xlabel('0 = No heart Disease, 1 = Heart Disease');\nplt.ylabel('Count');\nplt.legend(['Female','Male']);\nplt.xticks([0,1],['No heart disease','heart disease'],rotation=0);","30ae3429":"plt.figure(figsize=(10,6))\n\n# scatter with positive values\nplt.scatter(df.age[df.target==1],df.thalach[df.target==1],c='salmon');\n\n# scatter with Negative values\nplt.scatter(df.age[df.target==0],df.thalach[df.target==0],c='lightblue');\n\n# customizing the plot\nplt.title('Heart Disease as the function of age and Max Heart rate.');\nplt.xlabel('Age');\nplt.ylabel('Max Heart rate');\nplt.legend(['Heart Disease','No Heart Disease']);","caf1f67e":"df.age.plot(kind='hist');\n","1ef25c0f":"pd.crosstab(df.cp,df.target)\n","f4b599fd":"pd.crosstab(df.cp,df.target).plot.bar(color=['lightblue','salmon'],figsize=(10,6));\nplt.title('Heart Disease frequency per Chest pain types');\nplt.xlabel('Chest pain Type (CP)');\nplt.ylabel('Frequency');\nplt.legend(['No Heart Disease','Heart Disease']);\nplt.yticks(rotation=0);\nplt.xticks(rotation=0);","b6501208":"df.corr()\n","18ff1ac2":"corr_matrix = df.corr()\nfig, ax = plt.subplots(figsize=(15,12))\nax = sns.heatmap(corr_matrix,\n                 annot=True,\n                 linewidths=0.5,\n                 fmt='.2f',\n                 cmap='YlGnBu');\nax.set_title('Correlation Matrix of Independent Features and Dependent Feature',fontsize=20);","87dab398":"df.head()\n","1a330831":"X = df.drop('target',axis=1)\ny = df['target']","393c810b":"X","50de1dd4":"y","e17fbcca":"\n# split into train and rest set\n\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.2)","8ecc68b6":"X_train,len(X_train)\n","b9ef9cc4":"y_train,len(y_train)\n","470070fb":"models = {'Logistic Regression':LogisticRegression(),\n          'KNN':KNeighborsClassifier(),\n          'Random Forest':RandomForestClassifier()}","cfd72341":"def fit_and_score(models, X_train, X_test,y_train,y_test):\n    '''\n    Fits and evaluates given machine learing models.\n    models: A dictionary of dofferent Scikit-Learn machine learning models\n    X_train : training data (no labels)\n    X_test : testing data (no labels)\n    y_train : training labels\n    y_test : testing labels\n    '''\n    # set random seed\n    np.random.seed(42)\n    # make a dictionary to keep model scores\n    model_scores={}\n    # Loop through models\n    for name, model in models.items():\n        #Fit the model to the data\n        model.fit(X_train,y_train)\n        # Evaluate the model and append its score to the model_scores\n        model_scores[name]=model.score(X_test,y_test)\n    return model_scores","f94e87f2":"scores = fit_and_score(models,X_train,X_test,y_train,y_test) \nscores\n","13386135":"model_compare = pd.DataFrame(data=scores.values(),index=scores.keys(),columns=['Accuracy'])\nmodel_compare.plot.bar();\nplt.xticks(rotation=0);","d6a2b489":"# Let's tune KNN\n\ntrain_scores=[]\ntest_scores=[]\n\n# create a list of different values for n_neighbours\nneighbours = range(1,21)\n\n# setup KNN instance\n\nknn = KNeighborsClassifier()\n\n# Loop through different n_neighbours\nfor i in neighbours:\n    knn.set_params(n_neighbors=i)\n    \n    # fit the model to the data\n    knn.fit(X_train,y_train)\n    \n    #update the training scores list\n    train_scores.append(knn.score(X_train,y_train))\n    \n    #update the testing scores list\n    test_scores.append(knn.score(X_test,y_test))","d74348b1":"train_scores\n","4d7dca59":"test_scores\n","210389d4":"plt.plot(neighbours,train_scores,label='Train score');\nplt.plot(neighbours,test_scores,label='Test score');\nplt.xlabel('Number of neighbors');\nplt.ylabel('Model score');\nplt.xticks(np.arange(0,21,1));\nplt.legend();\n\nprint(f'Maximum KNN score on the Test data: {max(test_scores)*100:.2f}%')\n","235582c7":"log_reg_grid =  {'C':np.logspace(-4,4,20),\n                 'solver':['liblinear']}\n","73d094f0":"rf_grid = {'n_estimators':np.arange(10,1000,50),\n           'max_depth':[None,3,5,10],\n           'min_samples_split':np.arange(2,20,2),\n           'min_samples_leaf':np.arange(1,20,2)}\n","fcd29459":"# Tune LogisticRegression\n\nnp.random.seed(42)\n\n# setup random hyperparameter search for LogisticRegression\nrs_log_reg = RandomizedSearchCV(LogisticRegression(),\n                               param_distributions=log_reg_grid,\n                               cv=5,\n                               n_iter=20,\n                               verbose=True)\n\n#Fit random hyperparameter search model for LogisticRegression\n\nrs_log_reg.fit(X_train,y_train)","16689ea9":"rs_log_reg.best_params_\n","9042dfaf":"rs_log_reg.score(X_test,y_test)\n","8a745b09":"\nnp.random.seed(42)\n\n# setup random hyperparameter search for RandomForestClassifier\nrs_rf = RandomizedSearchCV(RandomForestClassifier(),\n                               param_distributions=rf_grid,\n                               cv=5,\n                               n_iter=20,\n                               verbose=True)\n\n#Fit random hyperparameter search model for LogisticRegression\n\nrs_rf.fit(X_train,y_train)","e5142161":"rs_rf.best_params_\n","d7a5cae5":"rs_rf.score(X_test,y_test)\n","7f48b9a7":"scores\n","9ae7b341":"# Different Hyperparameters for our LogisticRegression model\nlog_reg_grid = {'C': np.logspace(-4,4,30),\n                'solver': ['liblinear']}\n\n# setup grid hyperparameter search for LogisticRegression\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                          param_grid=log_reg_grid,\n                          cv=5,\n                          verbose=True)\n# Fit Grid search Logistic Regression model to our training data\ngs_log_reg.fit(X_train,y_train)","65c37c26":"# check the best hyperparameters\ngs_log_reg.best_params_\n","a83f866d":"# Evaluate the grid search LogisticRegression model\ngs_log_reg.score(X_test,y_test)\n","a62c62ff":"y_preds = gs_log_reg.predict(X_test)\n","b49f2df0":"y_preds","ce809485":"y_test","9d88659e":"# Plot ROC curve and calculate and calculate AUC metric\nplot_roc_curve(gs_log_reg,X_test,y_test);","5d06d743":"print(confusion_matrix(y_test,y_preds))\n","43387f97":"# visualise confusion matrix:\n# we create a function for plotting ROC curve given y_test and y_preds\ndef plot_conf_mat(y_test,y_preds):\n    '''\n    Plots a nice looking confusion matrix using seaborns heatmap()\n    '''\n    fig, ax = plt.subplots(figsize=(3,3))\n    ax = sns.heatmap(confusion_matrix(y_test,y_preds),\n                     annot=True,\n                     cbar=False)\n    plt.xlabel('True label')\n    plt.ylabel('Predicted label')\n\nplot_conf_mat(y_test,y_preds)","7622da2e":"print(classification_report(y_test,y_preds))\n","760b607b":"# Check best hyperparameters\ngs_log_reg.best_params_\n","e38285e4":"# create a new classifier with best parameters\n\nclf = LogisticRegression(C=0.20433597178569418,solver='liblinear')","c5360434":"def cal_cv_scores(clf,X,y,metric):\n    '''\n    Calculates the cross-validated evaluation metrics of the given classifier as the metric,X and y are provided.\n    returns : cross-validated score  \n    '''\n\n    score = cross_val_score(clf,\n                            X,\n                            y,\n                            cv=5,\n                            scoring=metric )\n    return np.mean(score)*100\n","619fddfd":"# cross-validated accuracy\ncv_accuracy = cal_cv_scores(clf,X,y,'accuracy')\nprint(f'Cross-validated accuracy: {cv_accuracy:.2f}%')","236e1b57":"# cross-validated Precision\ncv_precision = cal_cv_scores(clf,X,y,'precision')\nprint(f'Cross-validated Precision: {cv_precision:.2f}%')","1a020ee9":"# cross-validated Recall\ncv_recall = cal_cv_scores(clf,X,y,'recall')\nprint(f'Cross-validated Recall: {cv_recall:.2f}%')","fac2a6b4":"# cross-validated F1-score\ncv_f1 = cal_cv_scores(clf,X,y,'f1')\nprint(f'Cross-validated F1-score: {cv_f1:.2f}%')","a3207334":"cv_metrics = pd.Series([cv_accuracy,cv_precision,cv_recall,cv_f1])\ncv_metrics_acc=pd.DataFrame(cv_metrics)\ncv_metrics_acc.rename(columns={0:'Score'},inplace=True)\ncv_metrics_acc.rename(index={0:'Accuracy',1:'Precision',2:'Recall',3:'F1'},inplace=True)\ncv_metrics_acc\n","8ac9d5d1":"cv_metrics_acc.plot.bar();\n","b1e2b83a":"df.head()\n","e862c68b":"# Fit an instance of Logistic Regression\n\nclf = LogisticRegression(C=0.20433597178569418,\n                         solver='liblinear') \nclf.fit(X_train,y_train)","e26d9778":"# check coef_\nclf.coef_\n","bcbc45f1":"# Match coef's of features to columns\nfeatures_dict = dict(zip(df.columns,list(clf.coef_[0])))\nfeatures_dict\n","b21794f2":"plt.figure(figsize=(10,6));\nplt.bar(features_dict.keys(),features_dict.values());\nplt.title('Feature Importance',fontsize=20);","bdceaaa7":"Create a Hyperparameter grid for Randomforest:\n\n","4198e411":"Now we've got our data into training and test sets, it's time to build a machine learning model. As we train the model on training set, we'll test it on test set.\n\nWe're going to try 3 different machine learning models:\n\n> \n* Logistic Regression.\n* K-Nearest Neighbours Classifier\n* Random Forest Classifier\n\nPut Models in a dictionary:","ee13965a":"Create a function to fit and score models:\n\n","288c76c2":"Model comparision\u00b6\n","176bcbd9":"Confusion Matrix","d4098b66":"We'll use GridSearchCV now for hyperparameter tuning of our LogisticRegression Model\u00b6\n","aca72c22":"ROC curve and AUC score\n\n","432fefd1":"Calculate evaluation metrics using cross-validation\u00b6\n","a0aeac05":"Now we've got our baseline model.. We know a model's first predictions aren't always what we should proceed with.\n\nLet's look at the following:\n> \n* Hyperparameter Tuning\n* Feature importance\n* Confusion matrix\n* Cross-validation\n* Precision\n* Recall\n* F1 score\n* Classification report\n* ROC curve\n* Area under the Curve(AUC)","3757547e":"### Exploratory Data Analysis (EDA)\n##### The goal here is to be find out more about the data that we're working with.\n\n* What questions are we trying to solve?\n* what kind of data do we have and how do we treat different types\n* what's missing from the data and how do we deal with them.\n* Where are the outliers and why should we care about them?\n* How can we add, change or remove features.","b30d1165":"visualize feature importance\n\n","a5095e8d":"Feature Importance\u00b6\n> \"Which features in our data contributed most to the outcomes of our model and how did they contribute?\"\n\nFinding feature importance is different for each machine learning model.\n\n","f0ee8cc6":"Evaluating our tuned machine learning classifier beyond accuracy:\n>\n* ROC curve and AUC curve\n* Confusion matrix\n* Classification report\n* Precision\n* Recall\n* F1-score","d832d0e2":"We've tuned Logistic Regression, Let's do the same for our RandomForest Classifier\n\n","e83e8c16":"## Predicting Heart Disease using machine learning\n\nThis notebook looks into using various python-based machine learning and data science libraries in an attempt to build a machine learning model capable of predicting whether or not someone is having a heart disease based on their medical attributes.\n\nWe're going to take the following approach:\n\n1. Problem definition.\n2. Data\n3. Evaluation\n4. Features\n5. Modelling\n6. Experimentation\n\n\n1. Problem Definition\n> Given clinical parameters about a patient, can we predict whether or not they have heart disease\n\n2. Data\n> This database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to this date. The \"goal\" field refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4. source:https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci\n\n3. Evaluation\n> We need to try atleast 85% accuracy of the model to pursue the project.\n\n4. Features\nThis is where we get different information about each of the features in our data\n\n**Data dictionary**\n* age - age in years\n* sex - (1 = male; 0 = female)\n* cp - chest pain type\n>  0: Typical Angina: Chest pain related decrease blood supply to the heart\n1: Atypical Angeina: Chest pain not related to heart\n2: Non-anginal pain: typically esophagal spasms(non heart related)\n3: Asymptomatic: chest pain not showing signs of disease\n* trestbps - resting blood pressure (in mm Hg on admission to the hospital)\n* chol - serum cholestoral in mg\/dl\ns* erum = LDL + HDL +.2*triglvcerides\n> above 200 is cause for concern\nfbs - (fasting blood sugar > 120 mg\/dl) - (1 = true; 0 = false)\n'>126' mg\/dL signals diabetes\n* restecg - resting electrocardiographic results\n> 0: Nothing to note\n1: ST-T wave abnormality\ncan range from mild symptoms to severe problems\nsignals non-normal heart beat\n2: Possible or definite left ventricular hypertrophy.\nEnlarged heart's main pumping chamber\n* thalach - maximum heart rate achieved\n* exangexercise induced angina - (1 = yes; 0 = no)\n* oldpeakST - depression induced by exercise relative to rest\n* slopethe - slope of the peak exercise ST segment\n* canumber of major vessels - (0-3) colored by flourosopy\n* thal3 = normal; 6 = fixed defect; 7 = reversable defect\n* target - 1 or 0\n ","9a6d1e51":"**Visualizing the correlation matrix:**","2bbc20ad":"Split the data into X and y\n\n","e32dfa2a":"6. Experimentation\n> * Could we collect more data?\n  * Could we try a better model? like CatBoost or XGBoost\n  * Could we improve our current model?","cd8e65d8":"### Heart disease Frequncey per Chest pain type:\n> cp - chest pain type\n* 0: Typical Angina: Chest pain related decrease blood supply to the heart\n* 1: Atypical Angeina: Chest pain not related to heart\n* 2: Non-anginal pain: typically esophagal spasms(non heart related)\n* 3: Asymptomatic: chest pain not showing signs of disease\n\n","efe1d6d8":"we will create a function that takes input X and y and calculates cross validated scored for us\n\n","3e41e9f9":"import pickle\n\npickle.dump(clf,open('heart_disease_classifier.pkl','wb'))\n","05da610b":"Heart Disease frequency according to Gender\u00b6\n","4631ddc5":"**Correllation Matrix**","8a713a90":"**checking for missing values:**\n\n","4d2cff16":"Now we've got hyperparameter grids setup for each of our models, let's tune them using RandomizedSearchCV..\n\n","b88a7e8e":"Hyperparameter tuning (by hand)\u00b6\n","681fda55":"Value Counts of Targets\n\n* Samples with disease: 165\n* Samples without disease: 138","3a36963d":"Checking the age distribution using histogram for outliers\n\n","bc85c8ea":"> Due to unsatisfied performance of KNN model inspite of hyperparameter tuning when compared to that of Logistic regression and RandomForest Classifier, we would not consider KNN model\n\n","6d8886c3":"### 5. Modelling\u00b6\n","a1df6fec":"Saving our model\u00b6\n","3c8f1d23":"Classification report:","40d13549":"Split the data into training and test sets\n\n","40078a04":"Let's find the features importance for our Logistic Regression model.\n\n","db3a1216":"### Load the data\u00b6\n"}}