{"cell_type":{"d99ee519":"code","46b1bca2":"code","acb71ccb":"code","96a85a69":"code","01fef9ad":"code","09aebde4":"code","4875844f":"code","497d81f3":"code","1c753065":"code","6d036695":"code","7a9643b4":"code","71be2ce1":"markdown"},"source":{"d99ee519":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import cross_val_score\nfrom tqdm import tqdm\nfrom scipy.stats import ks_2samp\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","46b1bca2":"#train = pd.read_csv('..\/input\/dont-overfit-ii\/train.csv', index_col = 'id') # new data\n#test = pd.read_csv('..\/input\/dont-overfit-ii\/test.csv', index_col = 'id') # new data\n\ntrain = pd.read_csv('..\/input\/older-dataset-for-dont-overfit-ii-challenge\/train.csv', index_col = 'id') # old data\ntest = pd.read_csv('..\/input\/older-dataset-for-dont-overfit-ii-challenge\/test.csv', index_col = 'id') # old data\ntrain.head(4)","acb71ccb":"train_y = train['target']\ntrain = train.drop(columns='target')\ntrain.head(4)\n","96a85a69":"plt.bar(range(2), (train.shape[0], test.shape[0]), align='center', alpha=0.8)\nplt.xticks(range(2), ('train','test'))\nplt.ylabel('Number of data') \nplt.title('Can we avoid overfitting')\nplt.show()\n","01fef9ad":"plt.figure(figsize=(15,15))\nfor i in range(5):\n    for j in range(5):\n        plt.subplot(5,5,5*i+j+1)\n        plt.hist(test[str(5*i+j)],bins=100)\n        plt.title('Column '+str(5*i+j))\nplt.show()\n","09aebde4":"# You could see all caraible columns has gaussian distribution\n# with mean 0 and std 1 \n# But will there any differnece with test data?\n\nprint(train.mean().sum()\/300)\nprint(train.std().sum()\/300)\n","4875844f":"#from kernal  \"https:\/\/www.kaggle.com\/nanomathias\/distribution-of-test-vs-training-data\"\n\ndef get_diff_columns(train_df, test_df, show_plots=True, show_all=False, threshold=0.1):\n    \"\"\"Use KS to estimate columns where distributions differ a lot from each other\"\"\"\n\n    # Find the columns where the distributions are very different\n    diff_data = []\n    for col in tqdm(train_df.columns):\n        statistic, pvalue = ks_2samp(\n            train_df[col].values, \n            test_df[col].values\n        )\n        if pvalue > 0.05 and np.abs(statistic) < threshold:\n            diff_data.append({'feature': col, 'p': np.round(pvalue, 5), 'statistic': np.round(np.abs(statistic), 2)})\n\n    # Put the differences into a dataframe\n    diff_df = pd.DataFrame(diff_data).sort_values(by='statistic', ascending=False)\n    print(f\"number of features with diff distribution : {len(diff_df)}\")\n    if show_plots:\n        # Let us see the distributions of these columns to confirm they are indeed different\n        n_cols = 5\n        n_rows = 5\n        _, axes = plt.subplots(n_rows, n_cols, figsize=(20, 3*n_rows))\n        axes = [x for l in axes for x in l]\n\n        # Create plots\n        for i, (_, row) in enumerate(diff_df.iterrows()):\n            if i >= len(axes):\n                break\n            extreme = np.max(np.abs(train_df[row.feature].tolist() + test_df[row.feature].tolist()))\n            train_df.loc[:, row.feature].apply(np.log1p).hist(\n                ax=axes[i], alpha=0.5, label='Train', density=True,\n                bins=np.arange(-extreme, extreme, 0.25)\n            )\n            test_df.loc[:, row.feature].apply(np.log1p).hist(\n                ax=axes[i], alpha=0.5, label='Test', density=True,\n                bins=np.arange(-extreme, extreme, 0.25)\n            )\n            axes[i].set_title(f\"Statistic = {row.statistic}, p = {row.p}\")\n            axes[i].set_xlabel(f'Log({row.feature})')\n            axes[i].legend()\n\n        plt.tight_layout()\n        plt.show()\n        \n    return diff_df\n\n# Get the columns which differ a lot between test and train\ndiff_df = get_diff_columns(train, test)\n","497d81f3":"# As distribution of train and test are different, we use roboust scaler for scaling\n\ndata = RobustScaler().fit_transform(np.concatenate((train, test), axis=0))\ntrain = data[:250]\ntrain += np.random.normal(0, 0.01, train.shape)\ntest = data[250:]","1c753065":"clf = LogisticRegression(class_weight='balanced', solver='liblinear', penalty ='l1', C= 0.1, max_iter=10000)\nclf.fit(train, train_y)\nprint(f'5-fold val score : {cross_val_score(clf, train, train_y, cv=5)}')","6d036695":"clf.fit(train, train_y)\nans = clf.predict_proba(test)\nans","7a9643b4":"submit = pd.read_csv('..\/input\/dont-overfit-ii\/sample_submission.csv')\nsubmit['target'] = ans[:,1]\nsubmit.to_csv('submit.csv', index = False)","71be2ce1":"## Note\n\nI have discovered that kaggle has new data set but using older data for evaluation\n\nThus, the competition scores wrongly.\n\nThat's why I am using old version data for training.\n\n### discussion about this issue\n\nhttps:\/\/www.kaggle.com\/c\/dont-overfit-ii\/discussion\/169948"}}