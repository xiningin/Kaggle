{"cell_type":{"89180e3b":"code","0f68aa0e":"code","b66f69a6":"code","5f4f8834":"code","6ce8c9e7":"code","ca7f47d0":"code","12ea2b89":"code","3c096d43":"code","1fe99340":"code","6f122a73":"code","1fbad871":"code","3de01ac7":"code","1639ed16":"code","bde428db":"code","41f36548":"code","40aa7e95":"code","e9b2eb4c":"code","9204fe38":"code","f13546dd":"code","118ae541":"code","50a1e68c":"code","7b72356a":"code","90650644":"code","63cc24ef":"code","8c4f421d":"code","298b599e":"code","20dd2ecb":"code","e4180f51":"code","78c920e4":"markdown","1813f597":"markdown","09638162":"markdown","190a4e21":"markdown","d491428a":"markdown","6d7646af":"markdown","0ddad0cf":"markdown","5828d21c":"markdown","7b608d1f":"markdown","bcb78574":"markdown","39073300":"markdown","bdcd7476":"markdown","2f478e2b":"markdown","b1534ac0":"markdown","d2344eef":"markdown","263d5e00":"markdown","57c26939":"markdown","62ab9cd0":"markdown","9f6d4881":"markdown","d677afeb":"markdown","b4c3cd51":"markdown"},"source":{"89180e3b":"import pandas as pd\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.linear_model  import Ridge,Lasso,RidgeCV, LassoCV, ElasticNet, ElasticNetCV, LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as sm \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","0f68aa0e":"df =pd.read_csv('..\/input\/admission-prediction\/Admission_Prediction.csv')\ndf.head()","b66f69a6":"df.describe(include='all')","5f4f8834":"df.isna().sum()","6ce8c9e7":"df['University Rating'] = df['University Rating'].fillna(df['University Rating'].mode()[0])\ndf['TOEFL Score'] = df['TOEFL Score'].fillna(df['TOEFL Score'].mean())\ndf['GRE Score']  = df['GRE Score'].fillna(df['GRE Score'].mean())","ca7f47d0":"df= df.drop(columns = ['Serial No.'])\ndf.head()","12ea2b89":"df","3c096d43":"plt.figure(figsize=(20,25), facecolor='white')\nplotnumber = 1\n\nfor column in df:\n    if plotnumber<=16 :\n        ax = plt.subplot(4,4,plotnumber)\n        sns.distplot(df[column])\n        plt.xlabel(column,fontsize=20)\n        #plt.ylabel('Salary',fontsize=20)\n    plotnumber+=1\nplt.tight_layout()","1fe99340":"y = df['Chance of Admit']\nX =df.drop(columns = ['Chance of Admit'])","6f122a73":"plt.figure(figsize=(20,30), facecolor='white')\nplotnumber = 1\n\nfor column in X:\n    if plotnumber<=15 :\n        ax = plt.subplot(5,3,plotnumber)\n        plt.scatter(X[column],y)\n        plt.xlabel(column,fontsize=20)\n        plt.ylabel('Chance of Admit',fontsize=20)\n    plotnumber+=1\nplt.tight_layout()","1fbad871":"scaler =StandardScaler()\n\nX_scaled = scaler.fit_transform(X)","3de01ac7":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvariables = X_scaled\n\n# we create a new data frame which will include all the VIFs\n# note that each variable has its own variance inflation factor as this measure is variable specific (not model specific)\n# we do not include categorical values for mulitcollinearity as they do not provide much information as numerical ones do\nvif = pd.DataFrame()\n\n# here we make use of the variance_inflation_factor, which will basically output the respective VIFs \nvif[\"VIF\"] = [variance_inflation_factor(variables, i) for i in range(variables.shape[1])]\n# Finally, I like to include names so it is easier to explore the result\nvif[\"Features\"] = X.columns\nvif","1639ed16":"x_train,x_test,y_train,y_test = train_test_split(X_scaled,y,test_size = 0.25,random_state=355)","bde428db":"regression = LinearRegression()\n\nregression.fit(x_train,y_train)","41f36548":"regression.score(x_train,y_train)","40aa7e95":"def adj_r2(x,y):\n    r2 = regression.score(x,y)\n    n = x.shape[0]\n    p = x.shape[1]\n    adjusted_r2 = 1-(1-r2)*(n-1)\/(n-p-1)\n    return adjusted_r2","e9b2eb4c":"adj_r2(x_train,y_train)","9204fe38":"regression.score(x_test,y_test)","f13546dd":"adj_r2(x_test,y_test)","118ae541":"lasscv = LassoCV(alphas = None,cv =10, max_iter = 100000, normalize = True)\nlasscv.fit(x_train, y_train)","50a1e68c":"alpha = lasscv.alpha_\nalpha","7b72356a":"lasso_reg = Lasso(alpha)\nlasso_reg.fit(x_train, y_train)","90650644":"lasso_reg.score(x_test, y_test)","63cc24ef":"import numpy as np\nalphas = np.random.uniform(low=0, high=10, size=(50,))\nridgecv = RidgeCV(alphas = alphas,cv=10,normalize = True)\nridgecv.fit(x_train, y_train)","8c4f421d":"ridgecv.alpha_","298b599e":"ridge_model = Ridge(alpha=ridgecv.alpha_)\nridge_model.fit(x_train, y_train)\nridge_model.score(x_test, y_test)","20dd2ecb":"elasticCV = ElasticNetCV(alphas = None, cv =10)\n\nelasticCV.fit(x_train, y_train)\nelasticCV.alpha_","e4180f51":"elasticnet_reg = ElasticNet(alpha = elasticCV.alpha_,l1_ratio=0.5)\nelasticnet_reg.fit(x_train, y_train)\nelasticnet_reg.score(x_test, y_test)","78c920e4":"### 1. Lets Fill this columns\n### 2. As University Rating consist of Categorry is fill null values using mode of the data TOFEL Score & GRE Score Category consist of Continues values so fill null values using mean of the data","1813f597":"If you find Kernel Helpful please try to make upvote!!","09638162":"### Computing method for Adjusted R\n#### 1. n= no of rows\n#### 2. p= np.of columns","190a4e21":"### So, we can see by using different type of regularization, we still are getting the same r2 score. That means our OLS model has been well trained over the training data and there is no overfitting","d491428a":"## Why use Regularization?\n\n### 1. Regularization helps to reduce the variance of the model, without a substantial increase in the bias. ,If there is variance in the model that means that the model won\u2019t fit well for dataset different that training data. \n### 2.The tuning parameter \u03bb controls this bias and variance tradeoff. When the value of \u03bb is increased up to a certain limit, it reduces the variance without losing any important properties in the data. But after a certain limit, the model will start losing some important properties which will increase the bias in the data. \n### 3.Thus, the selection of good value of \u03bb is the key. The value of \u03bb is selected using cross-validation methods. A set of \u03bb is selected and cross-validation error is calculated for each value of \u03bb and that value of \u03bb is selected for which the cross-validation error is minimum.","6d7646af":"## Difference between Ridge and Lasso\n### 1.Ridge regression shrinks the coefficients for those predictors which contribute very less in the model but have huge weights, very close to zero. But it never makes them exactly zero. Thus, the final model will still contain all those predictors, though with less weights. This doesn\u2019t help in interpreting the model very well. \n### 2.This is where Lasso regression differs with Ridge regression. In Lasso, the L1 penalty does reduce some coefficients exactly to zero when we use a sufficiently large tuning parameter \u03bb. So, in addition to regularizing, lasso also performs feature selection.","0ddad0cf":"# Questions","5828d21c":"###  Now let's check if our model is overfitting our data using regularization.","7b608d1f":"## When should you use plain Linear Regression (i.e., without any regularization), Ridge, Lasso, or Elastic Net?\n\nAccording to the Hands-on Machine Learning book, it is almost always preferable to have at least a little bit of regularization, so generally you should avoid plain Linear Regression. Ridge is a good default, but if you suspect that only a few features are actually useful, you should prefer Lasso or Elastic Net since they tend to reduce the useless features\u2019 weights down to zero as we have discussed. In general, Elastic Net is preferred over Lasso since Lasso may behave erratically when the number of features is greater than the number of\ntraining instances or when several features are strongly correlated.\n\n### 1. If your Data is highly correlated than Lasso regression helps \n### 2. If Data is highly Overfitted than Ridge Regression helps\n### 3. Elastic net is mixture of Both Lasso + Rigde So if you are not sure about Data is highly correlated or Not!\n","bcb78574":"### Now the data looks good and there are no missing values. Also, the first cloumn is just serial numbers, so we don' need that column. Let's drop it from data and make it more clean.","39073300":"### 1. From the above Graph we can determine Linear Relation can be achieved with in the feat\n### 2. Now Normalize data by using StandardScaler which convert values in range of -1 to 1","bdcd7476":"LASSO regression penalizes the model based on the sum of magnitude of the coefficients. The regularization term is given by\n\n regularization=$ \\lambda *\\sum  |\\beta_j| $\n\nWhere, \u03bb is the shrinkage factor.\n\nand hence the formula for loss after regularization is:\n[Image Of Eqaution](https:\/\/www.google.com\/url?sa=i&url=https%3A%2F%2Fwww.statisticshowto.com%2Flasso-regression%2F&psig=AOvVaw18TJGNlVtL8KAgVAHZKMMm&ust=1594790247405000&source=images&cd=vfe&ved=0CAIQjRxqFwoTCIj4xuP-y-oCFQAAAAAdAAAAABAD)","2f478e2b":"##### Elastic Net\n\nAccording to the Hands-on Machine Learning book, elastic Net is a middle ground between Ridge Regression and Lasso Regression. The regularization term is a simple mix of both Ridge and Lasso\u2019s regularization terms, and you can control the mix ratio \u03b1. \n\n\nwhere \u03b1 is the mixing parameter between ridge (\u03b1\u2004=\u20040) and lasso (\u03b1\u2004=\u20041).\n[Image Of Eqaution](https:\/\/www.google.com\/url?sa=i&url=https%3A%2F%2Fhackernoon.com%2Fan-introduction-to-ridge-lasso-and-elastic-net-regression-cca60b4b934f&psig=AOvVaw2Ina_ONvgxzteADm2_A8EZ&ust=1594790911533000&source=images&cd=vfe&ved=0CAIQjRxqFwoTCPihyqKBzOoCFQAAAAAdAAAAABAD)","b1534ac0":"#### Ridge Regression (L2 Form)\nRidge regression penalizes the model based on the sum of squares of magnitude of the coefficients. The regularization term is given by\n\n regularization=$ \\lambda *\\sum  |\\beta_j ^ 2| $\n\nWhere, \u03bb is the shrinkage factor.\n\nand hence the formula for loss after regularization is:[Image Of Equation](https:\/\/www.google.com\/url?sa=i&url=http%3A%2F%2Fbusinessforecastblog.com%2Festimation-and-variable-selection-with-ridge-regression-and-the-lasso%2F&psig=AOvVaw2d2qzJhAROB5Wqfm2_s6fH&ust=1594791191867000&source=images&cd=vfe&ved=0CAIQjRxqFwoTCNDytbGCzOoCFQAAAAAdAAAAABAD)","d2344eef":"our r2_score for test data (75.34%) comes same as before using regularization. So, it is fair to say our OLS model did not overfit the data.","263d5e00":"#### Generating numbers between 0 to 10","57c26939":"## L1 Reguralization","62ab9cd0":"1. Our r2 score is 84.15% and adj r2 is 83.85% for our training et., so looks like we are not being penalized by use of any feature.\n\n2. Let's check how well model fits the test data.\n ","9f6d4881":"1. So it looks like our model r2 score is less on the test data.\n\n2. Let's see if our model is overfitting our training data.","d677afeb":"**Lets import the required library**","b4c3cd51":"### 1. Here, we have the correlation values for all the features. As a thumb rule, a VIF value greater than 5 means a very severe multicollinearity. We don't any VIF greater than 5 , so we are good to go.\n\n### 2. Great. Let's go ahead and use linear regression and see how good it fits our data. But first. let's split our data in train and test."}}