{"cell_type":{"9ef6a709":"code","0d3a2203":"code","f47a9535":"code","9209fac9":"code","11991d71":"code","78cfb9b2":"code","0ba26ab0":"code","873ce30c":"code","6b25877d":"code","6b4c3c9d":"code","159e61ad":"code","aeb91375":"code","b7f9b38b":"code","23ac3265":"markdown","14e5af17":"markdown","106528a3":"markdown","e4d0a343":"markdown","253bc733":"markdown","07a8a7c8":"markdown","ffadcc6b":"markdown","e5f118f9":"markdown","79b783f5":"markdown","14ce962d":"markdown","5d624123":"markdown","bd167ded":"markdown","9b21c735":"markdown","ebc3d382":"markdown","9e1373d6":"markdown","1960db6b":"markdown"},"source":{"9ef6a709":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom keras import backend as K\n\n\nnp.random.seed(0)\ntf.set_random_seed(0)\nsession_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\nsess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\nK.set_session(sess)","0d3a2203":"df = pd.read_csv('..\/input\/kaggle-survey-2019\/multiple_choice_responses.csv', skiprows=1)\n\nrename_cols = {\"Duration (in seconds)\": \"Duration\",\n               \"What is your age (# years)?\": \"Age\",\n               \"What is your gender? - Selected Choice\": \"Gender\",\n               \"In which country do you currently reside?\": \"Country\",\n               \"What is the highest level of formal education that you have attained or plan to attain within the next 2 years?\": \"Education\",\n               \"Select the title most similar to your current role (or most recent title if retired): - Selected Choice\": \"JobTitle\",\n               \"What is your current yearly compensation (approximate $USD)?\": \"Salary\",\n               \"Who\/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Kaggle (forums, blog, social media, etc)\": \"FavoriteKaggle\",\n               \"What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Python\": \"PythonUser\",\n               \"What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - R\": \"RUser\",\n               \"Which types of specialized hardware do you use on a regular basis?  (Select all that apply) - Selected Choice - GPUs\": \"GPUUser\",\n               \"Which of the following cloud computing platforms do you use on a regular basis? (Select all that apply) - Selected Choice - None\": \"CloudUser\",\n               \"Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice -   TensorFlow \": \"TensorflowUser\",\n               \"Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice -  Keras \": \"KerasUser\",\n               \"Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice -  PyTorch \": \"PytorchUser\",\n               \"Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice -  Fast.ai \": \"FastaiUser\",\n               \"Which automated machine learning tools (or partial AutoML tools) do you use on a regular basis?  (Select all that apply) - Selected Choice - None\": \"AutoMLUser\"\n              }\n\ndf = df.rename(columns=rename_cols)[list(rename_cols.values())]\n\nfor col in [\"FavoriteKaggle\", \"PythonUser\", \"RUser\", \"GPUUser\", \n            \"TensorflowUser\", \"KerasUser\", \"PytorchUser\", \"FastaiUser\", \"AutoMLUser\", \"CloudUser\"]:\n    df[col] = df[col].notnull()\n    if col in [\"AutoMLUser\", \"CloudUser\"]:\n        df[col] = ~df[col]\n        \ndf[\"TensorflowKerasUser\"] = df[\"TensorflowUser\"] | df[\"KerasUser\"]\ndf[\"PytorchFastaiUser\"] = df[\"PytorchUser\"] | df[\"FastaiUser\"]\n\ndf = df.drop([\"TensorflowUser\", \"KerasUser\", \"PytorchUser\", \"FastaiUser\"], axis=1)\nbinary_columns = [\"FavoriteKaggle\", \"PythonUser\", \"RUser\", \"GPUUser\", \n                  \"TensorflowKerasUser\", \"PytorchFastaiUser\", \"AutoMLUser\", \"CloudUser\"]\n\ndf.head()","f47a9535":"import re\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\nnumber_re = re.compile(r\"[\\d,]+\")\nCOUNT_LIMIT = 100\nNO_SALARY_REPLACE = \"100\"\ncategoricals = [\"Age\", \"Gender\", \"Country\", \"Education\", \"JobTitle\"]\nle_dict = dict()\nss_dict = dict()\n\ndef extract_log_salary(x):\n    x = str(x)\n    x = number_re.findall(x)\n    if len(x) == 0:\n        return None\n    x = [int(num.replace(\",\", \"\")) for num in x]\n    return np.log10(np.mean(x))\n\n# replace long country names with their short form for better visualization\ndf.loc[df[\"Country\"] == \"United Kingdom of Great Britain and Northern Ireland\", \"Country\"] = \"UK\"\ndf.loc[df[\"Country\"] == \"United States of America\", \"Country\"] = \"USA\"\n\n# log scale salaries so that our model doesn't focus on only rich countries (considering that the loss is MSE)\ndf.loc[df[\"JobTitle\"] == \"Not employed\", \"Salary\"] = NO_SALARY_REPLACE\ndf.loc[df[\"JobTitle\"] == \"Student\", \"Salary\"] = NO_SALARY_REPLACE\ndf[\"Salary\"] = df[\"Salary\"].apply(extract_log_salary)\ndf[\"Salary\"].fillna(df[\"Salary\"].median(), inplace=True)\n\n# People who spend more than 30 minutes on the survey assumed to be done after 30 minutes\ndf[\"Duration\"] = np.clip(df[\"Duration\"], 0, 1800)*1.0\n\nfor col in df.columns:\n    if col in categoricals:\n        # Merge all the rare categories into one category to prevent overfitting\n        df.loc[df[col].isnull(), col] = \"No {}\".format(col)\n        df.loc[df.groupby(col)[\"Duration\"].transform(\"count\") < COUNT_LIMIT, col] = \"Other {}\".format(col)\n        le_dict[col] = LabelEncoder()\n        df[col + \"_le\"] = le_dict[col].fit_transform(df[col])\n    elif col in [\"Salary\", \"Duration\"]:\n        ss_dict[col] = StandardScaler()\n        df[col + \"_ss\"] = ss_dict[col].fit_transform(df[col].values.reshape(-1, 1))\n\ndf.head()","9209fac9":"from IPython.display import SVG\nfrom keras.utils import model_to_dot\nfrom keras.layers import *\nfrom keras.models import Model\n\nEMB_SIZE = 8\ndense_outputs = [\"Duration_ss\", \"Salary_ss\"]\n\ndef nn_block(input_layer, size, dropout_rate, activation):\n    out_layer = Dense(size, activation=None)(input_layer)\n    out_layer = BatchNormalization()(out_layer)\n    out_layer = Activation(activation)(out_layer)\n    out_layer = Dropout(dropout_rate)(out_layer)\n    return out_layer\n\n\ndef get_model():\n    cat_inputs = []\n    cat_embs = []\n    for cat in categoricals:\n        cat_input = Input(shape=(1,), name=cat + \"_input\")\n        cat_emb = Embedding(len(le_dict[cat].classes_), EMB_SIZE, name=cat)(cat_input)\n        cat_embs.append(Flatten(name=cat + \"_1D\")(cat_emb))\n        cat_inputs.append(cat_input)\n    \n    hidden_layer = concatenate(cat_embs)\n    hidden_layer = nn_block(hidden_layer, 64, 0.1, \"relu\")\n    hidden_layer = nn_block(hidden_layer, 16, 0.1, \"relu\")\n    dense_out = Dense(len(dense_outputs), name=\"linear_out\")(hidden_layer)\n    binary_out = Dense(len(binary_columns), activation=\"sigmoid\", name=\"binary_out\")(hidden_layer)\n    \n    model = Model(inputs=cat_inputs, outputs=[dense_out, binary_out])\n    return model\n\ndef get_input(df):\n    return [df[cat + \"_le\"].values for cat in categoricals]\n\ndef get_output(df):\n    return [df[dense_outputs].values, df[binary_columns].values]\n\nSVG(model_to_dot(get_model(), dpi=64).create(prog='dot', format='svg'))","11991d71":"from sklearn.model_selection import train_test_split\nfrom keras.optimizers import Nadam\n\n\nclass ModelConfig:\n    batch_size = 64\n    training_scheme = [(0.002, 10), (0.0005, 5), (0.0001, 5)] # lr, epochs\n    loss = [\"mean_squared_error\", \"binary_crossentropy\"] # MSE for linear output, bce for binary output\n    optimizer = Nadam\n\ntrain_df, val_df = train_test_split(df, shuffle=True, random_state=0, test_size=0.2)\nmodel = get_model()\nfor lr, epochs in ModelConfig.training_scheme:\n    model.compile(loss=ModelConfig.loss, optimizer=ModelConfig.optimizer(lr=lr))\n    hist = model.fit(get_input(train_df), get_output(train_df), batch_size=ModelConfig.batch_size, epochs=epochs,\n                     validation_data=(get_input(val_df), get_output(val_df)),\n                     verbose=2, shuffle=True)\n    \nmodel = get_model()\nfor lr, epochs in ModelConfig.training_scheme:\n    model.compile(loss=ModelConfig.loss, optimizer=ModelConfig.optimizer(lr=lr))\n    hist = model.fit(get_input(df), get_output(df), batch_size=ModelConfig.batch_size, epochs=epochs,\n                     verbose=0, shuffle=True)","78cfb9b2":"from sklearn.decomposition import PCA\nfrom plotly.offline import init_notebook_mode, iplot \nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)\n\n\ndef plot_emb(model, cat, exclude=None):\n    L = le_dict[cat].classes_\n    W = model.get_layer(cat).get_weights()[0]\n    if exclude:\n        include = np.where([l not in exclude for l in L])[0]\n        L = L[include]\n        W = W[include]\n    \n    W = PCA(n_components=2, random_state=0).fit_transform(W)\n\n    pobj = [go.Scatter(x=W[:, 0], y=W[:, 1], mode = 'markers+text', text=L, hoverinfo=\"none\", textposition=\"bottom center\",\n                      marker = dict(size = 20, color = 'rgba(64, 64, 192, .7)'))]\n\n    fig = go.Figure(pobj, layout=go.Layout(title=go.layout.Title(text=cat)))\n\n    iplot(fig)\n    \nplot_emb(model, \"JobTitle\", exclude=[\"Student\", \"Not employed\", \"No JobTitle\"])","0ba26ab0":"plot_emb(model, \"Education\")","873ce30c":"plot_emb(model, \"Country\")","6b25877d":"plot_emb(model, \"Age\")","6b4c3c9d":"def make_prediction(model, df):\n    df = df.copy()\n    for col in categoricals:\n        df[col + \"_le\"] = le_dict[col].transform(df[col])\n    dense_result, bin_result = model.predict(get_input(df))\n    \n    for i, col in enumerate(binary_columns):\n        df[col] = bin_result[:, i]\n        df[col] = df[col].apply(lambda x: \"{}%\".format(int(x*100)))\n\n    df[\"Duration\"] = ss_dict[\"Duration\"].inverse_transform(dense_result[:, 0])\n    df[\"Salary\"] = np.power(10, ss_dict[\"Salary\"].inverse_transform(dense_result[:, 1]))\n    df[\"Salary\"] = df[\"Salary\"].apply(lambda x: \"{}000\".format(int(x\/\/1000)))\n    return df\n\nuniverses = [{\"Universe\": \"C-137 (Original)\", \"Age\":\"25-29\", \"Gender\":\"Male\", \"Country\":\"Netherlands\", \n                  \"Education\":\"Master\u2019s degree\", \"JobTitle\":\"Data Scientist\"},\n             {\"Universe\": \"C-510 (Turkey)\", \"Age\":\"25-29\", \"Gender\":\"Male\", \"Country\":\"Turkey\", \n                  \"Education\":\"Master\u2019s degree\", \"JobTitle\":\"Data Scientist\"},\n             {\"Universe\": \"C-425 (Old)\", \"Age\":\"35-39\", \"Gender\":\"Male\", \"Country\":\"Netherlands\", \n                  \"Education\":\"Master\u2019s degree\", \"JobTitle\":\"Data Scientist\"},\n             {\"Universe\": \"C-841 (PhD)\", \"Age\":\"25-29\", \"Gender\":\"Male\", \"Country\":\"Netherlands\", \n                  \"Education\":\"Doctoral degree\", \"JobTitle\":\"Data Scientist\"},\n             {\"Universe\": \"C-707 (SE)\", \"Age\":\"25-29\", \"Gender\":\"Male\", \"Country\":\"Netherlands\", \n                  \"Education\":\"Master\u2019s degree\", \"JobTitle\":\"Software Engineer\"},\n             {\"Universe\": \"C-210 (Female)\", \"Age\":\"25-29\", \"Gender\":\"Female\", \"Country\":\"Netherlands\", \n                  \"Education\":\"Master\u2019s degree\", \"JobTitle\":\"Data Scientist\"}]\n\nuniverses = pd.DataFrame(universes)\nuniverses","159e61ad":"def compare(res, topics, title):\n    fig = go.Figure(data=[\n        go.Bar(name=topic, x=res[topic], y=res[\"Universe\"], orientation=\"h\") for topic in topics\n    ], layout=go.Layout(title=go.layout.Title(text=title)))\n    iplot(fig)\n\nres = make_prediction(model, universes)\n\ncompare(res, [\"Salary\"], \"Salary\")","aeb91375":"compare(res, [\"Duration\"], \"Duration\")","b7f9b38b":"compare(res, binary_columns, \"Technology\")","23ac3265":"### First let's make our Numpy-Tensorflow deterministic. God does not play dice, right?\nSetting the seeds and disabling the parallelism. With different seeds, you may get slightly different embeddings.","14e5af17":"* It seems **Data Analyst** and **Business Analyst** are very similar to each other.\n* **Statistician** and **Research Scientist** are the outliers which are not similar to other jobs.\n* We also observe a triangle with **Software Engineer**, **Data Engineer** and **Data Scientist**.","106528a3":"* **18-21** and **22-24** are probably university students and they are very similar.\n* **25-29** are mostly in the beginning of their careers.\n* After 30 years old, there is no significant difference between the age groups.","e4d0a343":"### Reading the data that is relevant to us","253bc733":"### Preprocessing the data\n\n* Replacing nans\n* Clipping Survey Duration and log scaling salaries\n* Getting ready for Neural Network: Standard scaling and label encoding","07a8a7c8":"## Salary Conclusion\n* Salary data may be a bit noisy because I believe while some people provide their gross salary, some might have provided their net salary.\n* Me in **Turkey** is earning significantly less than me in the original universe. As far as I know, cost of living in Istanbul is around 3 times cheaper than Amsterdam but the same person in Amsterdam seems to earn almost 6 times more. So purchasing power for a data scientist in Istanbul is like half of Amsterdam. This can be explained by the dramatic loss of value in Turkish Lira in the last couple of years.\n* Having a **PhD** degree seems to make the salary worse. I guess this is due to data bias. People who are around 25-29 years old are proably still doing their PhD and PhD positions are paid less compared to private sector jobs. So we should interpret it as \"doing PhD\" instead of \"having PhD\".\n* **Software Engineer** version of me also makes less money. Even though both Software Engineering and Data Science are equally challenging, recent hype in Data Science probably made the data scientists more expensive.\n* **10 years older** version of me has the highest compensation. Given that age and years of experience are highly correlated, it makes sense.\n* The most surprising result was the **female** one for me. I always hear that there are fewer females interested in Data Science and companies try to keep the gender balance, therefore try to hire more females. So it looks like a low supply high demand situation. My basic economy knowledge tells me that women should earn more than men. But the data tells the opposite. There is a clear **gender bias** here. I hope there can be a reasonable hidden factor that can explain this because I kept all other attributes the same while changing the gender. Otherwise, if women are getting less when it comes to job offer, it is totally not fair.","ffadcc6b":"## Duration Conclusion\nDuration (Survey Time) was very experimental one. I wanted to see if there are any patterns in how much time it takes for people to complete the survey. I have re-run this experiment offline with other countries too. My hypothesis is that Duration measures the fluency in English and being detail oriented. People whose mother tongues are not from the same root as English seem to spend more time on the survey. In addition, having PhD or being old slightly increases the time spent on the survey.","e5f118f9":"### Thanks for reading! Enjoyed the notebook? You can fork it and:\n* Play with the model and get more experience on Representation Learning\n* Export the learned embeddings for Transfer Learning\n* Try the model with your age, gender, education etc and learn about your profile\n* Thinking about doing a PhD? Do you have a job offer in another country? You can get insights on your next move.","79b783f5":"### Neural Network Architecture\n\n* Embeddings of size 8 for each category are concatenated\n* 2 Dense-BN-Relu-Dropout hidden layers to add some non-linearity\n* 2 outputs: Duration and Salary as linear output. FavoriteKaggle, PythonUser, RUser, GPUUser, TensorflowKerasUser, PytorchFastaiUser, AutoMLUser, CloudUser as binary output.","14ce962d":"## Technology Conclusion\n* Living in **Turkey** makes it more likely that **Kaggle** is the favorite ML media source. Knowing that Turkish education system is more competitive than The Netherlands, competitive nature of Kaggle may be making it better learning place for Turkish people.\n* **Software Engineers** use both **Python** and **R** less while they use **AutoML** tools more. Some of them might be building machine learning applications on Java\/C++ etc using AutoML tools without the need of knowing machine learning in detail.\n* **Keras-Tensorflow** usage is correlated with **Pytorch-Fastai**. At least out of 5 attributes, none of them seems to be related to the choice between Tensorflow and Pytorch. \n* **Cloud** usage is another metric that is insensitive to the 5 attributes.\n* **PhDs** have the highest **GPU** usage. In an era which Neural Networks are SOTA in many tasks, this makes sense.","5d624123":"# Representation Learning\n\n## What is Representation Learning?\n\nI would call Representation Learning as transforming the raw data to machine understandable vector representations which are learned from the relations within the data. One very well-known example is word vectors. Words are represented as vectors while preserving their semantic similarities. Here is a nice example from [GLOVE Embeddings](https:\/\/nlp.stanford.edu\/projects\/glove\/):\n\n<img src=\"https:\/\/nlp.stanford.edu\/projects\/glove\/images\/man_woman.jpg\" style=\"width: 400px;\"\/>\n\n\n## What are we going to do with Representation Learning?\n\nRepresentation learning has two main benefits:\n* It allows us to visually understand the data in 2D plots.\n* Learned vectors can later be used in other Machine Learning tasks. This is called Transfer Learning. It is especially useful if the representation is learned on a big data and used in a problem with small data.\n\nWe are going to use Age, Country, Gender, Job Title and Education information of the survey respondents as categories. Then we will train a model to predict how much time it takes for them to complete the survey, their salaries and if they use Python, R, GPUs, AutoML, Cloud Services, TF-Keras, Pytorch-Fast.ai and if their favorite media source is Kaggle. So all these input and output elements will interact to get better representations of input categories.\n\n## How are we going to do that?\n\nWe will train a neural network model on Keras with Tensorflow backend and feed these categories to embedding layers.\n\n![tfkeras](https:\/\/i.imgur.com\/aPTRfSn.png)","bd167ded":"### Visualizing the embeddings\n\n* Since embeddings are trained with size 8, we need PCA to visualize it in 2 dimensions (top principal components).\n* Plotly is used to create interactive plots so that you can zoom in\/out etc.","9b21c735":"# Parallel Universes\n\nUsing our model, we can actually do some sensitivity analysis. I wonder about the expected salary and skills for my profile. I also wonder how much my salary and skills would be different if I made different decisions in my life. Let's have a look at me in 6 parallel universes.\n\n* **C-137**: The original universe. 28 years old male data scientist living in The Netherlands with Master's degree.\n* **C-510**: I was living in Turkey before. I have decided to stay in Turkey instead of moving to Netherlands.\n* **C-425**: I was born 10 years earlier.\n* **C-841**: I have decided to do a PhD after my MSc.\n* **C-707**: I was working as Software Engineer before. I have decided to stay as Software Engineer.\n* **C-210**: This universe went a bit radical. It seems I have decided to change gender in that reality.\n\n**DISCLAIMER:** During this session, I may have some very subjective theories and opinions on the outcome.","ebc3d382":"### Model Training\n\n* First we make sure that model is not overfitting by splitting it into train\/validation sets\n* Then the model is trained on whole dataset to learn better embeddings","9e1373d6":"* Country embeddings seem to cover geographical, economical and cultural similarities.\n* x axis seems to be an economical axis. It can correlate with the salaries.\n* y axis seems more geographical and cultural. Over zero, we see more Mediterranean or South Asian countries. Below zero, we see more East Asian and South American countries.\n* USA, Canada, Australia and West European countries are clustered together. Rest of EU has another cluster. There are more diversity on y axis on the middle of x axis.","1960db6b":"* There is no significant difference between **Bachelor's degree** and **Master's degree**.\n* People who **prefer not to answer** this question is most similar to the ones with **no formal education past high school**."}}