{"cell_type":{"0117e72f":"code","8039910b":"code","0a74958d":"code","9411af73":"code","cd2af97c":"code","3835239c":"code","3024bde7":"code","6358088f":"code","9e145022":"code","fdaed579":"code","1901c79f":"code","d7853901":"code","a0e2e2a4":"code","fd40e904":"code","05eedbc4":"code","6cbc8747":"code","fecdb756":"code","36b44269":"code","8ce0d518":"code","e4d830e4":"code","249720ac":"code","91db0506":"code","6f0e2e37":"code","e3a2ce43":"code","3b0ee9f2":"code","852c2270":"code","d70001af":"code","079028f6":"code","23ece240":"code","ee3d3596":"code","853c405f":"code","f588031b":"code","7447b5f8":"code","ee0be845":"code","1ab16d5a":"code","381530ab":"code","2df6458d":"code","c7bef7f3":"code","442adbd3":"code","37b3a653":"code","4445c385":"code","2be0d8d7":"code","5d77ab8b":"code","32dd1a13":"code","5ce888e8":"code","8f6af14f":"code","6a207622":"code","772cecd8":"code","b2ce4bb5":"code","57b5a321":"code","124ffb7a":"code","c6dbe693":"code","4bd6c6ff":"code","30534658":"code","652daa20":"code","69d0545f":"code","1bccb57c":"code","d59eed20":"code","e13583b0":"code","7758fb78":"code","d0c103f3":"code","71c10559":"code","4f0b51ea":"code","c3156ff2":"code","3febb20c":"code","80bec8f2":"code","99b235a9":"code","aa4903f6":"code","69087826":"code","a79d6700":"code","78b51b91":"code","4d55ae6a":"code","3af76ac6":"code","035543f2":"code","c72c846d":"code","644e949f":"code","7c51ba85":"code","30d7e987":"code","2c56cc99":"code","bf972422":"code","4c14cf3f":"code","808567a7":"code","b3ac10ba":"code","2ed26860":"code","9f4e5eb5":"code","27ab1f5a":"code","45921de3":"code","479ddb53":"code","146472c8":"code","500af8f2":"code","3ed89cd9":"code","42f553af":"code","81ca8631":"code","60f8b23a":"code","43d23641":"code","c8c848d6":"code","de9c1755":"code","b8a0db9e":"code","5655a38d":"code","47d8fb0e":"code","74f2e112":"code","47d2ceca":"code","e8170009":"code","05815a67":"code","b4f2af94":"markdown","a2d92055":"markdown","135eede9":"markdown","1cb1e1c9":"markdown","e0225187":"markdown","dc25b7cc":"markdown","56971b55":"markdown","e0211c0f":"markdown","8e7fc6ee":"markdown","2073cb53":"markdown","6ccefe33":"markdown","dad9cb06":"markdown","0453532a":"markdown","0227f4d0":"markdown","7c291620":"markdown","164c2789":"markdown","a68c3d54":"markdown","127df218":"markdown","f63e866d":"markdown","4d9a84e0":"markdown","aaa0deb5":"markdown","e61db09a":"markdown","62824023":"markdown","ccccf198":"markdown","e3a17ff4":"markdown","e52d6a87":"markdown","bb4f00bb":"markdown","7e605309":"markdown","ef83b0de":"markdown","9408535b":"markdown","a3909ef5":"markdown","7462c519":"markdown","4c5187a9":"markdown","67175149":"markdown","8c01e0aa":"markdown","dc88d2a2":"markdown","f481fdd8":"markdown","06a41767":"markdown","63ee64f1":"markdown","0855896d":"markdown","7c68804e":"markdown","862b8eec":"markdown","06110ac7":"markdown","bdca030d":"markdown","fb6997bf":"markdown","84042dcb":"markdown","ed993919":"markdown","6d575376":"markdown","60e3e199":"markdown","6c57a8f1":"markdown","b9ec944e":"markdown","2cb390d0":"markdown","fd913e11":"markdown","00a40201":"markdown","2c1283b8":"markdown","b6a1e7f2":"markdown","6f2b4663":"markdown","8e8cccf0":"markdown","396c73ef":"markdown","a03113af":"markdown","59171159":"markdown","2a2a360b":"markdown","75164be8":"markdown"},"source":{"0117e72f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8039910b":"import pandas as pd\nfrom io import StringIO\n\ndata = (\"\"\"\n\nid,param,def_value,descr\n\n1,base_score,0.5,initial prediction score of all instances\n\n2,booster,gbtree,Can be gbtree gblinear or dart. gbtree is tree-based model\n\n3,num_estimates,100,number of estimates\n\n4,max_depth, 6,maximum depth of a tree (too high a value leads to overfitting)\n\n5,eta, 0.3, step size shrinkage (learning rate) used in update to prevent overfitting\n\n6,gamma, 0, alias of min_split_loss with min loss reduction required to make a further partition on a leaf node of the tree\n\n7,min_child_weight,1,min sum of instance weight (hessian) needed in a child\n\n8,max_delta_step, 0, max delta step allowed each output to be\n\n9,subsample,1,subsample ratio of the training instances\n\n10, sampling_method, uniform, method used to sample the training instances\n\n11, colsample_bytree, 1, subsampling\n\n12, colsample_bylevel, 1, subsampling\n\n13, colsample_bynode,1,subsampling\n\n14, lambda,1, alias of reg_lambda the L2 regularization term on weights\n\n15, alpha, 0, alias of reg_alpha the L1 regularization term on weights\n\n16, tree_method, auto, tree construction alg used in xgboost\n\n17, sketch_eps, 0.03,-\n\n18, scale_pos_weight, 1, controls balance of positive and negative weights\n\n19, updater, grow_colmaker prun, -\n\n20, refresh_leaf, 1, refresh updater\n\n21, process_type, default, choices of default and update\n\n22, grow_policy, depthwise, controls way new nodes are added to the tree\n\n23, max_leaves,0, max number of nodes to be added\n\n24, max_bin, 256, only used if tree_method is set to hist or gpu_hist\n\n25, verbosity, 1, verbosity of printing messages\n\n26, num_feature, automatically discovered, feature dimension used in boosting set to max dim of the feature\n\n\"\"\")\n\nmapper = pd.read_csv(StringIO(data))\n\nd = dict(selector=\"th\",\n    props=[('text-align', 'center')])\n\nmapper = mapper.set_index('id')\n\nmapper.style.set_properties(**{'width':'10em', 'text-align':'center'})\\\n        .set_table_styles([d])","0a74958d":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplt.style.use(\"fivethirtyeight\")\nsns.set_style(\"darkgrid\")","9411af73":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom sklearn.ensemble import *\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, VotingRegressor\n\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\n\nimport lightgbm\nfrom lightgbm import LGBMRegressor\n\nfrom catboost import CatBoostRegressor\n\nfrom sklearn.neighbors import KNeighborsRegressor\n\nfrom sklearn.svm import SVC, SVR\n\nfrom sklearn.metrics import accuracy_score,mean_squared_error, mean_absolute_error, r2_score\n\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder","cd2af97c":"train = pd.read_csv(\"\/kaggle\/input\/autompg-dataset\/auto-mpg.csv\")","3835239c":"display (train.head())","3024bde7":"display(\"Rows and Columns of train:\", train.shape)","6358088f":"display(train.info())\ndisplay(\"Column names in Train:\\n\", train.columns.tolist())","9e145022":"train.describe()","fdaed579":"train['horsepower'].value_counts().sum()","1901c79f":"# It is mentioned in the description of the dataset that some of the values in the horsepower are marked as  \"?\"\n\ntrain[train['horsepower']=='?']","d7853901":"# Filling the missing values in the horsepower column\n\ntrain['horsepower'][32] = 82\ntrain['horsepower'][126] = 70\ntrain['horsepower'][330] = 45\ntrain['horsepower'][336] = 86\ntrain['horsepower'][354] = 75\ntrain['horsepower'][374] = 82","a0e2e2a4":"train['horsepower'] = train['horsepower'].astype(int)","fd40e904":"train['car name'].value_counts()","05eedbc4":"# some of the names as written in short so converitng it a common name for each name\n\ntrain['car name'] = train['car name'].replace(['volkswagen','vokswagen','vw'],'volkswagen')\ntrain['car name'] = train['car name'].replace('maxda','mazda')\ntrain['car name'] = train['car name'].replace('toyouta','toyota')\ntrain['car name'] = train['car name'].replace('mercedes-benz','mercedes')\ntrain['car name'] = train['car name'].replace('nissan','datsun')\ntrain['car name'] = train['car name'].replace('capri','ford')\ntrain['car name'] = train['car name'].replace(['chevroelt','chevy'],'chevrolet')","6cbc8747":"train['car name'].value_counts()","fecdb756":"le = LabelEncoder()\ntrain[\"car name\"] = le.fit_transform(train[\"car name\"])","36b44269":"train['origin'].value_counts()","8ce0d518":"x = ['1', '2', '3']\ny = train['origin'].value_counts()\nplt.bar(x, y, color='orangered')\nplt.title('origin Distribution', fontweight='bold', fontsize=20)\nplt.xlabel('origin', fontweight='bold', fontsize=15)\nplt.ylabel('Frequency', fontweight='bold', fontsize=15)\nplt.show();","e4d830e4":"plt.figure(figsize=(7,5))\nsns.barplot(x=\"origin\", y=\"mpg\",data=train);","249720ac":"train['cylinders'].value_counts()","91db0506":"train[\"cylinders\"].value_counts(normalize = True)","6f0e2e37":"plt.figure(figsize = (8,6))\nsns.countplot(data = train, x = \"cylinders\")\nplt.title(\"Cylinders\")\nplt.show()","e3a2ce43":"train['mpg'].value_counts()","3b0ee9f2":"sns.distplot(train[\"mpg\"]);","852c2270":"display(train.isnull().sum())\nprint(train.isnull().sum().sum())","d70001af":"# Pearson Correlation\nplt.figure(figsize=(10,9))\nsns.heatmap(train.corr(method='pearson'), cbar=False, annot=True, fmt='.1f', linewidth=0.2, cmap='summer');","079028f6":"# Spearman Correlation\nplt.figure(figsize=(10,9))\nsns.heatmap(train.corr(method='spearman'), cbar=False, annot=True, fmt='.1f', linewidth=0.2, cmap='viridis');","23ece240":"# kendall\nfig, ax = plt.subplots(1, 3, figsize=(17 , 5))\n\nfeature_lst = ['mpg', 'cylinders', 'displacement','horsepower','weight', 'acceleration', 'model year', 'origin']\n\ncorr = train[feature_lst].corr()\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\nfor idx, method in enumerate(['pearson', 'kendall', 'spearman']):\n    sns.heatmap(train[feature_lst].corr(method=method), ax=ax[idx],\n            square=True, annot=True, fmt='.1f', center=0, linewidth=2,\n            cbar=False, cmap=sns.diverging_palette(240, 10, as_cmap=True),\n            mask=mask\n           ) \n    ax[idx].set_title(f'{method.capitalize()} Correlation', loc='left', fontweight='bold')     \n\nplt.show()","ee3d3596":"train.corr()['mpg'].sort_values(ascending=False)","853c405f":"a = train.drop([\"origin\", \"mpg\"], axis=1)\na.corrwith(train.origin).plot(kind='bar', grid=True, figsize=(15,10), color='salmon')\nplt.title(\"Correlation with target\", size=20)\nplt.xticks(size=15, rotation=90)\nplt.yticks(size=15)\nplt.show()","f588031b":"train.skew()","7447b5f8":"plt.figure(figsize=(15,10))\nsns.boxplot(data=train, orient='h', palette='Set2');","ee0be845":"sns.scatterplot(x=\"model year\", y=\"mpg\",data=train)\n# shows model year 80 has highest mpg","1ab16d5a":"sns.pairplot(data=train)","381530ab":"X = train.iloc[:, 1:]\ny = train['mpg']","2df6458d":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,shuffle=True,random_state=0)","c7bef7f3":"X_train.shape","442adbd3":"scaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","37b3a653":"rf1 = RandomForestRegressor()\nrf1.fit(X_train, y_train)","4445c385":"y_pred_rf = rf1.predict(X_test)","2be0d8d7":"print('Mean Squared Error:', np.sqrt(mean_squared_error(y_test, y_pred_rf)))\nprint(\"Train Score {:.2f} & Test Score {:.2f}\".format(rf1.score(X_train,y_train),rf1.score(X_test,y_test)))","5d77ab8b":"plt.figure(figsize=(8,6))\nfeature_imp = pd.Series(rf1.feature_importances_, index=X.columns).sort_values(ascending=False)\n\n# Creating a bar plot\nsns.barplot(x=feature_imp, y=feature_imp.index)\n\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.legend()\nplt.show()","32dd1a13":"print(\"Model\\t\\t\\t\\t RMSE \\t\\t MSE \\t\\t MAE \\t\\t R2\")\nprint(\"\"\"Random Forest Regressor \\t {:.2f} \\t\\t {:.2f} \\t\\t{:.2f} \\t\\t{:.2f}\"\"\".format(\n            np.sqrt(mean_squared_error(y_test, y_pred_rf)),\n            mean_squared_error(y_test, y_pred_rf),\n            mean_absolute_error(y_test, y_pred_rf),\n            r2_score(y_test, y_pred_rf)))\n\nplt.figure(figsize=(8,6))\nplt.scatter(y_test, y_pred_rf)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.title(\"Random Forest Regressor\")\n\nplt.show()","5ce888e8":"# reg:linear is now deprecated in favor of reg:squarederror\nxgb1 = XGBRegressor(objective='reg:squarederror')\nxgb1.fit(X_train, y_train)","8f6af14f":"# make predictions for test set\ny_pred_xgb1 = xgb1.predict(X_test)","6a207622":"print('Mean Squared Error:', np.sqrt(mean_squared_error(y_test, y_pred_xgb1)))\nprint(\"Train Score {:.2f} & Test Score {:.2f}\".format(xgb1.score(X_train,y_train),xgb1.score(X_test,y_test)))","772cecd8":"# Random Forest we would do the same to get importances\nprint(xgb1.feature_importances_)","b2ce4bb5":" #To have even better plot, let\u2019s sort the features based on importance value:\n\nplt.figure(figsize=(9,8))\nsorted_idx = xgb1.feature_importances_.argsort()\nplt.barh(X.columns[sorted_idx], xgb1.feature_importances_[sorted_idx]);\nplt.xlabel(\"Xgboost Feature Importance\")\nplt.show()","57b5a321":"print(\"Model\\t\\t\\t\\t RMSE \\t\\t MSE \\t\\t MAE \\t\\t R2\")\nprint(\"\"\"Random Forest Regressor \\t {:.2f} \\t\\t {:.2f} \\t\\t{:.2f} \\t\\t{:.2f}\"\"\".format(\n            np.sqrt(mean_squared_error(y_test, y_pred_xgb1)),\n            mean_squared_error(y_test, y_pred_xgb1),\n            mean_absolute_error(y_test, y_pred_xgb1),\n            r2_score(y_test, y_pred_xgb1)))\n\nplt.figure(figsize=(8,6))\nplt.scatter(y_test, y_pred_xgb1)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.title(\"Random Forest Regressor\")\n\nplt.show()","124ffb7a":"xgb2 = XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=0.7, colsample_bytree=0.8)\n\nxgb2 = cross_val_score(xgb2, X_train, y_train, cv=10, scoring=('neg_mean_absolute_error'))\nxgb2","c6dbe693":"print(\"Average XGB Cross Validation MAE: {0}\".format(np.abs(xgb2.mean())))\nprint(\"Best XGB Cross Validation MAE: {0}\".format(np.abs(xgb2.max())))","4bd6c6ff":"# reg:linear is now deprecated in favor of reg:squarederror\n\nparams = {\n    # Parameters that we are going to tune.\n    'max_depth':6,\n    'min_child_weight': 1,\n    'eta':.3,\n    'subsample': 1,\n    'colsample_bytree': 1,\n    # Other parameters\n    'objective':'reg:squarederror',\n}","30534658":"params['eval_metric'] = \"mae\"\nnum_boost_round = 999","652daa20":"# Loading data into DMatrices\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)","69d0545f":"model = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dtest, \"Test\")],\n    early_stopping_rounds=10\n)","1bccb57c":"print(\"Best MAE: {:.2f} with {} rounds\".format(\n                 model.best_score,\n                 model.best_iteration+1))","d59eed20":"cv_results = xgb.cv(\n    params,\n    dtrain,\n    num_boost_round = num_boost_round,\n    seed=42,\n    nfold=5,\n    metrics={'mae'},\n    early_stopping_rounds=10\n)\ncv_results","e13583b0":"cv_results['test-mae-mean'].min()","7758fb78":"gridsearch_params = [\n    (max_depth, min_child_weight)\n    for max_depth in range(9,12)\n    for min_child_weight in range(5,8)\n]\n\n#gridsearch_params = {'max_depth': np.range(9,12),\n#                     'min_child_weight': np.range(5,8),\n #              }","d0c103f3":"# Define initial best params and MAE\nmin_mae = float(\"Inf\")\nbest_params = None\nfor max_depth, min_child_weight in gridsearch_params:\n    print(\"CV with max_depth={}, min_child_weight={}\".format(\n                             max_depth,\n                             min_child_weight))\n    # Update our parameters\n    params['max_depth'] = max_depth\n    params['min_child_weight'] = min_child_weight\n    # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        seed=42,\n        nfold=5,\n        metrics={'mae'},\n        early_stopping_rounds=10\n    )\n    # Update best MAE\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].argmin()\n    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = (max_depth,min_child_weight)\nprint(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))","71c10559":"gridsearch_params = [\n    (subsample, colsample)\n    for subsample in [i\/10. for i in range(7,11)]\n    for colsample in [i\/10. for i in range(7,11)]\n]","4f0b51ea":"min_mae = float(\"Inf\")\nbest_params = None\n# We start by the largest values and go down to the smallest\nfor subsample, colsample in reversed(gridsearch_params):\n    print(\"CV with subsample={}, colsample={}\".format(\n                             subsample,\n                             colsample))\n    # We update our parameters\n    params['subsample'] = subsample\n    params['colsample_bytree'] = colsample\n    # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        seed=42,\n        nfold=5,\n        metrics={'mae'},\n        early_stopping_rounds=10\n    )\n    # Update best score\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].argmin()\n    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = (subsample,colsample)\nprint(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))","c3156ff2":"%time\n\n# This can take some time\u2026\nmin_mae = float(\"Inf\")\nbest_params = None\n\nfor eta in [.3, .2, .1, .05, .01, .005]:\n    print(\"CV with eta={}\".format(eta))\n    \n    # We update our parameters\n    params['eta'] = eta\n    \n    # Run and time CV\n    %time cv_results = xgb.cv(params, dtrain, num_boost_round=num_boost_round, seed=42, nfold=5, metrics=['mae'], early_stopping_rounds=10)\n    \n    # Update best score\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].argmin()\n    print(\"\\tMAE {} for {} rounds\\n\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = eta\n        \nprint(\"Best params: {}, MAE: {}\".format(best_params, min_mae))","3febb20c":"params\n{'colsample_bytree': 1.0,\n 'eta': 0.05,\n 'eval_metric': 'mae',\n 'max_depth': 10,\n 'min_child_weight': 6,\n 'objective': 'reg:squarederror',\n 'subsample': 0.8}","80bec8f2":"model = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dtest, \"Test\")],\n    early_stopping_rounds=10\n)","99b235a9":"print(\"Best MAE: {:.2f} in {} rounds\".format(model.best_score, model.best_iteration+1))","aa4903f6":"xgb3 = XGBRegressor(colsample_bytree= 1.0,\n                    learning_rate = 0.01,\n                    eta = 0.05,\n                    max_depth = 10,\n                    min_child_weight = 6,\n                    subsample = 0.8,\n                    n_estimators= 1000,\n                    objective= 'reg:squarederror',\n                    eval_metric = 'mae'\n                  )\n                   \nxgb3.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_test, y_test)], verbose=False)","69087826":"print(\"Train Score {:.2f} & Test Score {:.2f}\".format(xgb3.score(X_train,y_train),xgb3.score(X_test,y_test)))","a79d6700":"y_pred_xgb3 = xgb3.predict(X_test)","78b51b91":"print(\"Model\\t\\t\\t\\t RMSE \\t\\t MSE \\t\\t MAE \\t\\t R2\")\nprint(\"\"\"Random Forest Regressor \\t {:.2f} \\t\\t {:.2f} \\t\\t{:.2f} \\t\\t{:.2f}\"\"\".format(\n            np.sqrt(mean_squared_error(y_test, y_pred_xgb3)),\n            mean_squared_error(y_test, y_pred_xgb3),\n            mean_absolute_error(y_test, y_pred_xgb3),\n            r2_score(y_test, y_pred_xgb3)))\n\nplt.figure(figsize=(8,6))\nplt.scatter(y_test, y_pred_xgb3)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.title(\"Random Forest Regressor\")\n\nplt.show()","4d55ae6a":"# To have even better plot, let\u2019s sort the features based on importance value:\n\nplt.figure(figsize=(9,8))\nsorted_idx = xgb3.feature_importances_.argsort()\nplt.barh(X.columns[sorted_idx], xgb3.feature_importances_[sorted_idx]);\nplt.xlabel(\"Xgboost Feature Importance\")\nplt.show()","3af76ac6":"# The plot_importance function allows to see the relative importance of all features in our model\nplt.figure(figsize=(10,8))\nxgb.plot_importance(xgb3, ax=plt.gca())","035543f2":"# Plot_tree allows to visualize the trees that were built by XGBoost\nplt.figure(figsize=(20,25))\nxgb.plot_tree(xgb3, ax=plt.gca())","c72c846d":"print(\"Number of boosting trees: {}\".format(xgb3.n_estimators))\nprint(\"Max depth of trees: {}\".format(xgb3.max_depth))\nprint(\"Objective function: {}\".format(xgb3.objective))","644e949f":"parameters = {'objective':['reg:squarederror'],\n              'booster':['gbtree','gblinear'],\n              'learning_rate': [0.1], \n              'max_depth': [7,10,15,20],\n              'min_child_weight': [10,15,20,25],\n              'colsample_bytree': [0.8, 0.9, 1],\n              'n_estimators': [300,400,500,600],\n              \"reg_alpha\"   : [0.5,0.2,1],\n              \"reg_lambda\"  : [2,3,5],\n              \"gamma\"       : [1,2,3]}\n\nxgb_model = XGBRegressor(random_state=30)\n\ngrid_obj_xgb = RandomizedSearchCV(xgb_model,parameters, cv=5, n_iter=10, scoring='neg_mean_absolute_error', verbose=3, n_jobs=12)\n\ngrid_obj_xgb.fit(X_train, y_train,verbose = 1)","7c51ba85":"y_pred_test = grid_obj_xgb.predict(X_test)","30d7e987":"print('Best Parameters:\\n\\n', grid_obj_xgb.best_params_)\nprint('\\n\\nBest Score:\\n\\n', grid_obj_xgb.best_score_)","2c56cc99":"bst = grid_obj_xgb.best_estimator_\nprint(bst)","bf972422":"# reg:linear is now deprecated in favor of reg:squarederror\nxgb4 = XGBRegressor(objective='reg:squarederror',\n                   reg_lambda = 2,\n                   reg_alpha = 0.2,\n                   n_estimators = 600,\n                   min_child_weight = 20,\n                   max_depth = 20,\n                   learning_rate = 0.1,\n                   gamma = 3,\n                   colsample_bytree = 0.8,\n                   booster = 'gbtree')\nxgb4.fit(X_train, y_train)","4c14cf3f":"y_pred_xgb4 = xgb4.predict(X_test)","808567a7":"print('Mean Squared Error:', np.sqrt(mean_squared_error(y_test, y_pred_xgb4)))\nprint(\"Train Score {:.2f} & Test Score {:.2f}\".format(xgb4.score(X_train,y_train),xgb4.score(X_test,y_test)))","b3ac10ba":"xgb4.get_params(deep=True)","2ed26860":"xgb4.get_xgb_params()","9f4e5eb5":"xgb4.feature_importances_","27ab1f5a":"# Plot_tree allows to visualize the trees that were built by XGBoost\nplt.figure(figsize=(15,10))\nxgb.plot_tree(xgb4, ax=plt.gca());","45921de3":"import graphviz\nfrom IPython.display import Image, display_png\ngraph = xgb.to_graphviz(bst)\ngraph.format = \"png\"\ngraph.render(\"tree\")\ndisplay_png(Image(\"tree.png\"))","479ddb53":"xgb.to_graphviz(xgb4, num_trees=1, rankdir='LR')","146472c8":"xgb.to_graphviz(xgb4, num_trees=2, rankdir='LR')","500af8f2":"from matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 13,10\nfor i in range(10):\n    xgb.plot_tree(xgb4, num_trees=i)\n    fig = plt.gcf()","3ed89cd9":"fig, ax = plt.subplots(figsize=(9,8))\nxgb.plot_importance(xgb4, ax=ax); ","42f553af":"plt.style.use('dark_background')\n\nfig, ax = plt.subplots(figsize=(11,9))\n\nmy_range=range(0,8)\nmy_size=range(0,8)\norb_size = [.2]*8\n\n# Create a color if the group is \"B\"\nmy_color = np.where(xgb3.feature_importances_ > 0.0, 'orange', 'skyblue')\n\nplt.hlines(y=X.columns , xmin=0, xmax= xgb4.feature_importances_, color=my_color, alpha=0.4)\nplt.scatter(xgb4.feature_importances_, my_range, color=my_color, alpha=.9)\n\nplt.title(\"\\nCalculated Feature Importance (Raw Value) per individual feature\", loc='center', fontsize=20)\nplt.xlabel(\"\\n Feature Importance 'Score'\\n\", fontsize=15)\nplt.ylabel('')\nax.spines['top'].set_linewidth(.3)  \nax.spines['left'].set_linewidth(.3)  \nax.spines['right'].set_linewidth(.3)  \nax.spines['bottom'].set_linewidth(.3)  \nsns.despine(top=True, right=True, left=True, bottom=True)\nplt.axvline(x=0.08, color='lightgrey', ymin = .03, ymax=.97, linestyle=\"--\", linewidth=.3)\nplt.axvline(x=0.16, color='lightgrey', ymin = .03, ymax=.97, linestyle=\"--\", linewidth=.3)\nplt.axvline(x=0.24, color='lightgrey', ymin = .03, ymax=.97, linestyle=\"--\", linewidth=.3)\nplt.axvline(x=0.32, color='lightgrey', ymin = .03, ymax=.97, linestyle=\"--\", linewidth=.3)\nplt.axvline(x=0.4, color='lightgrey', ymin = .03, ymax=.97, linestyle=\"--\", linewidth=.3)\nplt.axvline(x=0.48, color='lightgrey', ymin = .03, ymax=.97, linestyle=\"--\", linewidth=.3)\nplt.axvline(x=0.56, color='lightgrey', ymin = .03, ymax=.97, linestyle=\"--\", linewidth=.3)\nplt.tight_layout()\nplt.show();","81ca8631":"def plot_feature_importances(model):\n    n_features = X.shape[1]\n    plt.barh(range(n_features), xgb4.feature_importances_, align=\"center\")\n    plt.yticks(np.arange(n_features),X)\n    plt.xlabel(\"importance\")\n    plt.ylabel(\"features\")\n    plt.show\nplot_feature_importances(bst)\nplt.savefig(\"Features Importances\")","60f8b23a":"# To have even better plot, let\u2019s sort the features based on importance value:\n\nplt.figure(figsize=(9,8))\nsorted_idx = xgb4.feature_importances_.argsort()\nplt.barh(X.columns[sorted_idx], xgb4.feature_importances_[sorted_idx]);\nplt.xlabel(\"Xgboost Feature Importance\")\nplt.show()","43d23641":"importances = pd.DataFrame({'feature':X.columns,'importance':np.round(xgb4.feature_importances_,3)})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')\nimportances.head(10)","c8c848d6":"importances.plot.bar(figsize=(11,9));","de9c1755":"lgbm_params = {\"learning_rate\": [0.001,0.01,0.1,1,1.5],\n               \"n_estimators\": [300,500,700,1000],\n               \"max_depth\": [3, 5, 8, 10],\n              \"num_leaves\":[32,64,128],\n              \"min_data_in_leaf\":[100,1000]}\n\n\nlgbm = LGBMRegressor()\n\nlgbm_grid_search = GridSearchCV(estimator = lgbm, param_grid = lgbm_params,\n                              scoring = \"neg_root_mean_squared_error\",cv = 5,\n                              n_jobs = -1, verbose = 2, refit = True).fit(X_train, y_train)","b8a0db9e":"y_pred = lgbm_grid_search.predict(X_test)\n\nlgbm_tunned_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n\nprint(\"Tunned LGBM RMSE: \",lgbm_tunned_rmse )\nprint(\"Best Parameters : \",lgbm_grid_search.best_estimator_)","5655a38d":"lgbm = LGBMRegressor(max_depth=3, min_data_in_leaf=100, n_estimators=300, num_leaves=32)\nlgbm.fit(X_train, y_train)","47d8fb0e":"plt.rcParams[\"figure.figsize\"] = (8,6)\nlightgbm.plot_importance(lgbm, max_num_features = 60, height=.9)","74f2e112":"r1 = GradientBoostingRegressor(random_state=0,n_estimators=400,subsample=0.7)\nr2 = LGBMRegressor(learning_rate=0.3,boosting_type='dart',random_state=0,n_estimators=400)\nr3 = RandomForestRegressor(n_estimators=100, random_state=42)\nr4 = CatBoostRegressor()\nr5 = ExtraTreesRegressor(n_estimators=400,random_state=0,min_samples_split=3)\nr6 = KNeighborsRegressor(n_neighbors=2,p=1)\n#model_name = ['LGBM', 'RandomForestRegressor', 'CatBoost', 'Final']\n\nvoting_clf = VotingRegressor([('grb', r1), ('lgbm', r2),('rf',r3),('cat',r4),('etr',r5), ('knn', r6)],weights=[1,0.25,0.23,0.23,0.04,0.04])\n\ny_pred = voting_clf.fit(X_train, y_train).predict(X_test)","47d2ceca":"# Train classifiers\nreg1 = GradientBoostingRegressor(random_state=1)\nreg2 = RandomForestRegressor(random_state=1)\nreg3 = LinearRegression()\n\nreg1.fit(X_train, y_train)\nreg2.fit(X_train, y_train)\nreg3.fit(X_train, y_train)\n\nereg = VotingRegressor([('gb', reg1), ('rf', reg2), ('lr', reg3)])\nereg.fit(X_train, y_train)","e8170009":"pred1 = reg1.predict(X_test)\npred2 = reg2.predict(X_test)\npred3 = reg3.predict(X_test)\npred4 = ereg.predict(X_test)","05815a67":"plt.figure(figsize=(14,12))\nplt.plot(pred1, 'gd', label='GradientBoostingRegressor')\nplt.plot(pred2, 'b^', label='RandomForestRegressor')\nplt.plot(pred3, 'ys', label='LinearRegression')\nplt.plot(pred4, 'r*', ms=10, label='VotingRegressor')\n\nplt.tick_params(axis='x', which='both', bottom=False, top=False,\n                labelbottom=False)\nplt.ylabel('predicted')\nplt.xlabel('training samples')\nplt.legend(loc=\"best\")\nplt.title('Regressor predictions and their average')\n\nplt.show()","b4f2af94":"#### For more information on **scoring** refer:\n\n  - https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html","a2d92055":"- **Note:** There is a difference between **'feature importance'** and XGBoost's feature importance plot (which is actually based on F-Score)**","135eede9":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left;\"> 6.2.2) Cross Validation <\/h1>","1cb1e1c9":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:170%; text-align:left;\"> 6.3) LGBM <\/h1>","e0225187":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left;\"> 5.6) Data Visualization <\/h1>","dc25b7cc":"<h2 style=color:green align=\"left\"> Method 3: <\/h2>","56971b55":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> Table of Contents <\/h1>\n\n- 1) What are hyperparameters\n\n- 2) XGBoost hyperparameters\n\n - 2.1 General Parameters\n   - 2.1.1 booster\n   - 2.1.2 verbosity\n   - 2.1.3 nthread\n   \n - 2.2 Booster Parameters\n   - 2.2.1 eta\n   - 2.2.2 gamma\n   - 2.2.3 max_depth\n   - 2.2.4 min_child_weight\n   - 2.2.5 max_delta_step\n   - 2.2.6 subsample\n   - 2.2.7 colsample_bytree, colsample_bylevel, colsample_bynode\n   - 2.2.8 lambda\n   - 2.2.9 alpha\n   - 2.2.10 tree_method\n   - 2.2.11 scale_pos_weight\n   - 2.2.12 max_leaves\n   \n - 2.3 Learning Task Parameters\n   - 2.3.1 objective\n   - 2.3.2 eval_metric\n   - 2.3.3 seed\n   \n- 3) Load Required Libraries\n\n- 4) Load data\n\n- 5) EDA(Exploratory Data Analysis)\n\n    - 5.1) Feature Analysis\n    \n    - 5.2) Missing Values\n    \n    - 5.3) Relationship between features\n      - 5.3.1) The correlation between the continuos variables\n      - 5.3.2) The correlation between this continuos features and the target\n      \n    - 5.4) Skewness and Kurtesis\n    \n    - 5.5) Outliers\n    \n    - 5.6) Data Visualization\n      - 5.6.1) Univariate Analysis\n      - 5.6.2) Bivariate Analysis\n      - 5.6.3) Multivariate Analysis\n          \n- 6) Model building and Evaluation\n\n    - 6.1) Random Forest\n      - 6.1.1) Base Model\n      - 6.1.2) Hyperparameter tuning\n         \n    - 6.2) XGBoost Regressor\n      - 6.2.1) Base Model\n      - 6.2.2) Hyperparameter Tuning (Tune 6 main Parameters)\n        - 6.2.2.1) Parameters num_boost_round and early_stopping_rounds\n        - 6.2.2.2) Using XGBoost\u2019s CV\n        - 6.2.2.3) Parameters max_depth and min_child_weight\n        - 6.2.2.4) Parameters subsample and colsample_bytree\n        - 6.2.2.5) Parameter ETA\n        - 6.2.2.6) Results\n      - 6.2.3) Hyperparameter Tuning\n\n  - 6.3) LGBM\n  \n  - 6.4) Voting Classifier","e0211c0f":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:170%; text-align:left;\"> 6.2.3.5) Parameter ETA <\/h1>\n\n- The ETA parameter controls the learning rate. It corresponds to the shrinkage of the weights associated to features after each round, in other words it defines the amount of \"correction\" we make at each step (remember how each boosting round is correcting the errors of the previous? if not, check our first tutorial here).\n\n- In practice, having a lower eta makes our model more robust to overfitting thus, usually, the lower the learning rate, the best. But with a lower eta, we need more boosting rounds, which takes more time to train, sometimes for only marginal improvements. Let's try a couple of values here, and time them with the notebook command:","8e7fc6ee":"<h2 style=color:green align=\"left\"> Method 2: <\/h2>","2073cb53":"<h2 style=color:green align=\"left\"> MAE (Mean Absolute Error) <\/h2>\n\n - We are going to use **mean absolute error (MAE)** to evaluate the quality of our predictions. MAE is a common and simple metric that has the advantage of being in the **same unit as our target**, which means it can be compared to target values and easily interpreted. \n \n<h2 style=color:green align=\"left\"> num_boost_round <\/h2>\n\n- XGBoost provides a nice way to find the best number of rounds whilst training. Since trees are built sequentially, instead of fixing the number of rounds at the beginning, we can test our model at each step and see if adding a new tree\/round improves performance.\n\n- To do so, we define a test dataset and a metric that is used to assess performance at each round. If performance haven\u2019t improved for N rounds (N is defined by the variable early_stopping_round), we stop the training and keep the best number of boosting rounds. Let's see how to use it.\n\n- In order to automatically find the best number of boosting rounds, we need to pass extra parameters on top of the params dictionary, the training DMatrix and num_boost_round:\n\n- **evals:** a list of pairs **(test_dmatrix, name_of_test)**. Here we will use our dtest DMatrix.\n\n- **early_stopping_rounds:** The number of rounds without improvements after which we should stop, here we set it to 10.","6ccefe33":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:170%; text-align:left;\"> 6.4) Voting Classifier <\/h1>","dad9cb06":"### Key hyper parameters (Default Values)\n\n - **min_child_weight:** 1\n \n - **max_depth:** 6\n \n - **gamma:** 0\n \n - **subsample:** 100\n \n - **colsample_bytree:** 1\n \n - **learning_rate:** 0.3\n \n - **eta:** 0.3","0453532a":"<h1 style=\"background-color:DeepPink; font-family:newtimeroman; font-size:170%; text-align:left;\"> 5.1.3) Origin <\/h1>","0227f4d0":"- In order to tune the other hyperparameters, we will use the **cv** function from XGBoost. It allows us to run cross-validation on our training dataset and returns a mean MAE score.\n\n- We don\u2019t need to pass a **test** dataset here. It\u2019s because the cross-validation function is splitting the **train** dataset into **nfolds** and iteratively keeps one of the folds for test purposes. \n\n### params: \n - dictionary of parameters.\n\n- dtrain matrix.\n\n### num_boost_round:\n - number of boosting rounds. Here we will use a large number again and count on early_stopping_rounds to find the optimal number of rounds before reaching the maximum.\n\n### seed: \n - random seed. It's important to set a seed here, to ensure we are using the same folds for each step so we can properly compare the scores with different parameters.\n\n### nfold:\n - The number of folds to use for cross-validation\n\n### metrics:\n - The metrics to use to evaluate our model, here we use MAE.","7c291620":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:170%; text-align:left;\"> 5.2) Missing Values <\/h1>","164c2789":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left;\"> 6.1.2) Hyperparameter Tuning <\/h1>","a68c3d54":"- a) XGBoost InBuilt function --> **plot_tree**\n\n- b) Graphviz --> **to_graphviz(xgb)**","127df218":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 5) EDA(Exploratory Data Analysis) <\/h1>","f63e866d":"<h1 style=\"background-color:DeepPink; font-family:newtimeroman; font-size:180%; text-align:left; border-radius: 4px 4px;\"> 6.2.4.1) Plotting Decision Trees: <\/h1>","4d9a84e0":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left;\"> 5.3.1) The correlation between the continuos variables <\/h1>","aaa0deb5":"<h2 style=color:green align=\"left\"> Method 4: <\/h2>","e61db09a":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:170%; text-align:left;\"> 6.2.3.6) Results <\/h1>","62824023":"- As you can see we **stopped before reaching the maximum** number of boosting rounds, that\u2019s because after the **7th tree**, **adding more rounds did not lead to improvements** of MAE on the test dataset.","ccccf198":"<h1 style=\"background-color:DeepPink; font-family:newtimeroman; font-size:180%; text-align:left; border-radius: 4px 4px;\"> 6.2.4.2) Feature Importance: <\/h1>\n\n - XGBoost actually has a **built in plot_importance function** that is relatively useful, but its default visualization is somewhat unappealing.","e3a17ff4":"- **Regression Voting Ensemble:** Predictions are the average of contributing models.\n\n- **Classification Voting Ensemble:** Predictions are the majority vote of contributing models.\n\n##### There are two approaches to the majority vote prediction for classification; they are hard voting and soft voting.\n\n- **Hard Voting:** Predict the class with the largest sum of votes from models\n\n- **Soft Voting:** Predict the class with the largest summed probability from models.\n\n-----------------\n\n#### Reference:\n\n - https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_voting_regressor.html","e52d6a87":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left;\"> 5.4) Skewness and Kurtesis <\/h1>","bb4f00bb":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 1) What are hyperparameters <\/h1>","7e605309":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 4) Load data <\/h1>","ef83b0de":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:190%; text-align:left;\"> 6.2) XGBoost Regressor <\/h1>","9408535b":"- Again, we update our params dictionary\n\n       params['subsample'] = .8\n       params['colsample_bytree'] = 1","a3909ef5":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:170%; text-align:left;\"> 6.2.3.1) Parameters num_boost_round and early_stopping_rounds <\/h1>","7462c519":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:190%; text-align:left;\"> 6.1) Random Forest <\/h1>","4c5187a9":"<h1 style=\"background-color:DeepPink; font-family:newtimeroman; font-size:170%; text-align:left;\"> 5.1.2) Car Name <\/h1>","67175149":"<h2 style=color:green align=\"left\"> Attribute Information: <\/h2>\n\n - **mpg:** continuous cylinders: multi-valued discrete\n \n - **displacement:** continuous\n \n - **horsepower:** continuous\n \n - **weight:** continuous\n \n - **acceleration:** continuous\n \n - **model year:** multi-valued discrete\n \n - **origin:** multi-valued discrete\n \n - **car name:** string (unique for each instance)","8c01e0aa":"<h1 style=\"background-color:DeepPink; font-family:newtimeroman; font-size:170%; text-align:left;\"> 5.1.4) cylinders <\/h1>","dc88d2a2":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:170%; text-align:left;\"> 5.1) Feature Analysis <\/h1>","f481fdd8":"- We get the best score with a **max_depth** of **9** and **min_child_weight** of **6**, so let's update our params\n\n        params['max_depth'] = 10\n        params['min_child_weight'] = 6","06a41767":"train = train.drop(\"car name\", axis = 1)","63ee64f1":"       params['eta'] = 0.05","0855896d":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:170%; text-align:left;\"> 6.2.3.2) Using XGBoost\u2019s CV <\/h1>","7c68804e":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 6) Model building and Evaluation <\/h1>","862b8eec":"### 1) Mean Absolute Error (MAE)\n\n         # from sklearn.metrics import mean_absolute_error\n         # print(mean_absolute_error(actual, prediction))\n        \n### 2) Mean Squared Error (MSE)\n\n         # from sklearn.metrics import mean_squared_error\n         # print(mean_squared_error(actual, prediction))\n        \n### 3) Root Mean Squared Error (RMSE)\n\n         # from sklearn.metrics import mean_squared_error\n         # print(np.sqrt(mean_squared_error(actual, prediction)))\n                             (or)\n         # print(mean_squared_error(actual, prediction, squared=False))\n        \n### 4) R-squared\n\n         # from sklearn.metrics import r2_score\n         # print(r2_score(actual, prediction))\n         \n------------------\n\n### rmse (root mean squared error)\n\n - The **default** values are\n \n  - **rmse** for **regression**\n  - **error** for **classification**\n  - **mean average precision** for **ranking**\n\n\n - RMSE is the standard deviation of the error. In other words it\u2019s a measure to check spread out of error.\n\n - **Lower values of RMSE indicate better fit**.","06110ac7":"<h2 style=color:green align=\"left\"> Method 1: Inbuilt Function <\/h2>","bdca030d":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left;\"> 6.2.3) Hyperparameter Tuning (Tune 6 main Parameters) <\/h1>","fb6997bf":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:170%; text-align:left;\"> 6.2.3.3) Parameters max_depth and min_child_weight <\/h1>","84042dcb":"- In the following, we are going to see methods to **tune the main parameters** of your XGBoost model. In an ideal world, with infinite resources and where time is not an issue, you could run a giant **grid search** with all the parameters together and find the optimal solution.\n\n- In fact, you might even be able to do that with really **small datasets**, but as the **data grows bigger, training time grows too**, and each step in the tuning process becomes **more expensive**. For this reason it is important to understand the role of the parameters and focus on the steps that we expect to impact our results the most. Here we will tune **6 of the hyperparameters** that are usually having a **big impact on performance**.","ed993919":"#### i) Univariate Analysis","6d575376":"<h1 style=\"background-color:DeepPink; font-family:newtimeroman; font-size:170%; text-align:left;\"> 5.1.5) mpg <\/h1>","60e3e199":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left;\"> 6.1.1) Base Model <\/h1>","6c57a8f1":"<h1 style=\"background-color:DeepPink; font-family:newtimeroman; font-size:170%; text-align:left;\"> 5.1.1) Horse Power <\/h1>","b9ec944e":"#### iii) Multivariate Analysis","2cb390d0":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:170%; text-align:left;\"> 6.2.3.4) Parameters subsample and colsample_bytree <\/h1>\n\n- Those parameters control the sampling of the dataset that is done at each boosting round.\n\n- Instead of using the whole training set every time, we can build a tree on slightly different data at each step, which makes it less likely to overfit to a single sample or feature.\n\n### subsample:\n- corresponds to the fraction of observations (the rows) to subsample at each step. By default it is set to 1 meaning that we use all rows.\n\n### colsample_bytree:\n- corresponds to the fraction of features (the columns) to use. By default it is set to 1 meaning that we will use all features.","fd913e11":"- Generally, the XGBoost hyperparameters have been divided into 4 categories. They are as follows \n\n - General parameters\n - Booster parameters\n - Learning task parameters\n - Command line parameters\n \n\n- Before running a XGBoost model, we must set three types of parameters - general parameters, booster parameters and task parameters.\n\n- The fourth type of parameters are command line parameters. They are only used in the console version of XGBoost. So, we will skip these parameters and limit our discussion to the first three type of parameters.\n\n#### 2.1 General Parameters \n\n- These parameters guide the overall functioning of the XGBoost model.\n\n- In this section, we will discuss three hyperparameters - booster, verbosity and nthread.\n\n- Please visit XGBoost General Parameters for detailed discussion on general parameters.\n\n##### 2.1.1 booster \n\n- **booster[default = gbtree]**\n\n - **booster** parameter helps us to choose which booster to use.\n - It helps us to select the type of model to run at each iteration.\n - It has 3 options - **gbtree, gblinear or dart.**\n\n      - **gbtree and dart** - use tree-based models, while\n      - **gblinear** uses linear models.\n\n##### 2.1.2 verbosity \n\n- **verbosity[default = 1]**\n\n  - Verbosity of printing messages.\n  - Valid values are **0 (silent), 1 (warning), 2 (info), 3 (debug).**\n\n##### 2.1.3 nthread \n\n- **nthread [default = maximum number of threads available if not set]**\n\n    - This is number of parallel threads used to run XGBoost.\n    - This is used for parallel processing and number of cores in the system should be entered.\n    - If you wish to run on all cores, value should not be entered and algorithm will detect automatically.\n\n- There are other general parameters like disable_default_eval_metric [default=0], num_pbuffer [set automatically by XGBoost, no need to be set by user] and num_feature [set automatically by XGBoost, no need to be set by user].\n\n- So, these parameters are taken care by XGBoost algorithm itself. Hence,we will not discuss these further.\n\n#### 2.2 Booster Parameters \n\n- We have 2 types of boosters - **tree booster and linear booster.**\n- We will limit our discussion to **tree booster** because it always outperforms the **linear booster** and thus the later is rarely used.\n- Please visit, [Parameters for Tree Booster](https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html#parameters-for-tree-booster), for detailed discussion on booster parameters.\n\n##### 2.2.1 eta \n\n- **eta [default=0.3, alias: learning_rate]**\n\n    - It is analogous to learning rate in GBM.\n    - It is the step size shrinkage used in update to prevent overfitting.\n    - After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative.\n    - It makes the model more robust by shrinking the weights on each step.\n    - range : [0,1]\n    - Typical final values : 0.01-0.2.\n\n##### 2.2.2 gamma \n\n- **gamma [default=0, alias: min_split_loss]**\n\n     - A node is split only when the resulting split gives a positive reduction in the loss function.\n     - Gamma specifies the minimum loss reduction required to make a split.\n     - It makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n     - The larger gamma is, the more conservative the algorithm will be.\n     - Range: [0,\u221e]\n\n##### 2.2.3 max_depth \n\n- **max_depth [default=6]**\n\n   - The maximum depth of a tree, same as GBM.\n   - It is used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n   - Increasing this value will make the model more complex and more likely to overfit.\n   - The value 0 is only accepted in lossguided growing policy when tree_method is set as hist and it indicates no limit on depth.\n   - We should be careful when setting large value of max_depth because XGBoost aggressively consumes memory when training a deep tree.\n   - range: [0,\u221e] (0 is only accepted in lossguided growing policy when tree_method is set as hist.\n   - Should be tuned using CV.\n   - Typical values: 3-10\n\n##### 2.2.4 min_child_weight\n- used to control over-fitting, this parameter is the **sample size** under which the **model can not split a node**. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n\n- **min_child_weight [default=1]**\n\n    - It defines the minimum sum of weights of all observations required in a child.\n    - This is similar to min_child_leaf in GBM but not exactly. This refers to min \u201csum of weights\u201d of observations while GBM has min \u201cnumber of observations\u201d.\n    - It is used to control over-fitting.\n    - Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n    - Too high values can lead to under-fitting.\n    - Hence, it should be tuned using CV.\n    - The larger min_child_weight is, the more conservative the algorithm will be.\n    - range: [0,\u221e]\n\n##### 2.2.5 max_delta_step \n\n- **max_delta_step [default=0]**\n\n    - In maximum delta step we allow each tree\u2019s weight estimation to be.\n    - If the value is set to 0, it means there is no constraint.\n    - If it is set to a positive value, it can help making the update step more conservative.\n    - Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced.\n   - Set it to value of 1-10 might help control the update.\n   - range: [0,\u221e]\n\n##### 2.2.6 subsample\n- defines the ratio of the training instances. For example setting it to **0.5** means that XGBoost would **randomly sample half of the training data** prior to growing trees, preventing overfitting\n\n- **subsample [default=1]**\n\n   - It denotes the fraction of observations to be randomly samples for each tree.\n   - Subsample ratio of the training instances.\n   - Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. - This will prevent overfitting.\n   - Subsampling will occur once in every boosting iteration.\n   - Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.\n   - Typical values: 0.5-1\n   - range: (0,1]\n\n##### 2.2.7 colsample_bytree, colsample_bylevel, colsample_bynode \n\n- **colsample_bytree, colsample_bylevel, colsample_bynode [default=1]**\n\n   - This is a family of parameters for subsampling of columns.\n\n   - All **colsample_by** parameters have a range of (0, 1], the default value of 1, and specify the fraction of columns to be subsampled.\n\n   - **colsample_bytree** is the subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed.\n\n   - **colsample_bylevel** is the subsample ratio of columns for each level. Subsampling occurs once for every new depth level reached in a tree. Columns are subsampled from the set of columns chosen for the current tree.\n\n  - **colsample_bynode** is the subsample ratio of columns for each node (split). Subsampling occurs once every time a new split is evaluated. Columns are subsampled from the set of columns chosen for the current level.\n\n  - **colsample_by** parameters work cumulatively. For instance, the combination {'colsample_bytree':0.5, 'colsample_bylevel':0.5, 'colsample_bynode':0.5} with 64 features will leave 8 features to choose from at each split.\n\n##### 2.2.8 lambda \n\n- **lambda [default=1, alias: reg_lambda]**\n\n    - **L2 regularization** term on weights (analogous to **Ridge regression**).\n    - This is used to handle the regularization part of XGBoost.\n    - Increasing this value will make model more conservative.\n\n##### 2.2.9 alpha \n\n- **alpha [default=0, alias: reg_alpha]**\n\n   - **L1 regularization** term on weights (analogous to **Lasso regression**).\n   - It can be used in case of **very high dimensionality** so that the algorithm runs faster when implemented.\n   - Increasing this value will make model more conservative.\n   \n##### 2.2.10 tree_method \n\n- **tree_method string [default= auto]**\n\n   - The tree construction algorithm used in XGBoost.\n\n   - XGBoost supports approx, hist and gpu_hist for distributed training. Experimental support for external memory is available for approx and gpu_hist.\n\n   - Choices: auto, exact, approx, hist, gpu_hist\n\n   - **auto:** Use heuristic to choose the fastest method.\n\n     - For small to medium dataset, exact greedy (exact) will be used.\n\n     - For very large dataset, approximate algorithm (approx) will be chosen.\n\n     - Because old behavior is always use exact greedy in single machine, user will get a message when approximate algorithm is chosen to notify this choice.\n\n   - **exact:** Exact greedy algorithm.\n\n   - **approx:** Approximate greedy algorithm using quantile sketch and gradient histogram.\n\n   - **hist:** Fast histogram optimized approximate greedy algorithm. It uses some performance improvements such as bins caching.\n\n   - **gpu_hist:** GPU implementation of hist algorithm.\n   \n##### 2.2.11 scale_pos_weight \n\n- **scale_pos_weight [default=1]**\n\n  - It controls the balance of positive and negative weights,\n  - It is useful for imbalanced classes.\n  - A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence.\n  - A typical value to consider: sum(negative instances) \/ sum(positive instances).\n\n##### 2.2.12 max_leaves \n\n- **max_leaves [default=0]**\n\n  - Maximum number of nodes to be added.\n  - Only relevant when grow_policy=lossguide is set.\n  - There are other hyperparameters like sketch_eps,updater, refresh_leaf, process_type, grow_policy, max_bin, predictor and num_parallel_tree.\n  - For detailed discussion of these hyperparameters, please visit [Parameters for Tree Booster](https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html#parameters-for-tree-booster)\n  \n### 2.3 Learning Task Parameters \n\n - These parameters are used to define the optimization objective the metric to be calculated at each step.\n\n - They are used to specify the learning task and the corresponding learning objective. The objective options are below:\n\n#### 2.3.1 objective \n\n - **objective [default=reg:squarederror]**\n\n - It defines the loss function to be minimized. Most commonly used values are given below -\n\n    - **reg:squarederror :** regression with squared loss.\n\n    - **reg:squaredlogerror:** regression with squared log loss 1\/2[log(pred+1)\u2212log(label+1)]2. - All input labels are required to be greater than -1.\n\n    - **reg:logistic :** logistic regression\n\n    - **binary:logistic :** logistic regression for binary classification, output probability\n\n    - **binary:logitraw:** logistic regression for binary classification, output score before logistic transformation\n\n    - **binary:hinge :** hinge loss for binary classification. This makes predictions of 0 or 1, rather than producing probabilities.\n\n    - **multi:softmax :** set XGBoost to do multiclass classification using the softmax objective, you also need to set num_class(number of classes)\n\n    - **multi:softprob :** same as softmax, but output a vector of ndata nclass, which can be further reshaped to ndata nclass matrix. The result contains predicted probability of each data point belonging to each class.\n    \n#### 2.3.2 eval_metric \n\n - **eval_metric [default according to objective]**\n - The metric to be used for validation data.\n - The **default** values are\n      - **rmse** for **regression**\n      - **error** for **classification**\n      - **mean average precision** for **ranking**\n - We can add multiple evaluation metrics.\n - Python users must pass the metrices as list of parameters pairs instead of map.\n - The most common values are given below -\n\n     - **rmse :** root mean square error\n     - **mae :** mean absolute error\n     - **logloss :** negative log-likelihood\n     - **error :** Binary classification error rate (0.5 threshold). It is calculated as #(wrong cases)\/#(all cases). For the predictions, the evaluation will regard the instances with prediction value larger than 0.5 as positive instances, and the others as negative instances.\n     - **merror :** Multiclass classification error rate. It is calculated as #(wrong cases)\/#(all cases).\n     - **mlogloss :** Multiclass logloss\n     - **auc:** Area under the curve\n     - **aucpr :** Area under the PR curve\n\n#### 2.3.3 seed \n\n - **seed [default=0]**\n\n   - The random number seed.\n   - This parameter is ignored in R package, use set.seed() instead.\n   - It can be used for generating reproducible results and also for parameter tuning.","00a40201":"### max_depth:\n- is the maximum number of nodes allowed from the root to the farthest leaf of a tree. Deeper trees can model more complex relationships by adding more nodes, but as we go deeper, splits become less relevant and are sometimes only due to noise, causing the model to overfit.\n\n### min_child_weight:\n- is the minimum weight (or number of samples if all samples have a weight of 1) required in order to create a new node in the tree. A smaller min_child_weight allows the algorithm to create children that correspond to fewer samples, thus allowing for more complex trees, but again, more likely to overfit.","2c1283b8":"### What are hyperparameters\n\n- In this kernel, we will discuss the critical problem of hyperparameter tuning in XGBoost model.\n\n- Hyperparameters are certain values or weights that determine the learning process of an algorithm.\n\n- As stated earlier, XGBoost provides large range of hyperparameters. We can leverage the maximum power of XGBoost by tuning its hyperparameters.\n\n- The most powerful ML algorithm like XGBoost is famous for picking up patterns and regularities in the data by automatically tuning thousands of learnable parameters.\n\n- In tree-based models, like XGBoost the learnable parameters are the choice of decision variables at each node.\n\n- XGBoost is a very powerful algorithm. So, it will have more design decisions and hence large hyperparameters. These are parameters specified by hand to the algo and fixed throughout a training phase.\n\n- In tree-based models, hyperparameters include things like the maximum depth of the tree, the number of trees to grow, the number of variables to consider when building each tree, the minimum number of samples on a leaf and the fraction of observations used to build a tree.\n\n- Although we focus on optimizing XGBoost hyperparameters in this kernel, the concepts discussed in this kernel applies to any other advanced ML algorithm as well.\n\n### Reference:\n\n- https:\/\/info.cambridgespark.com\/latest\/getting-started-with-xgboost\n\n- https:\/\/blog.cambridgespark.com\/hyperparameter-tuning-in-xgboost-4ff9100a3b2f\n\n- https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/\n\n- https:\/\/www.analyticsvidhya.com\/blog\/2016\/02\/complete-guide-parameter-tuning-gradient-boosting-gbm-python\/","b6a1e7f2":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left;\"> 6.2.1) Base Model <\/h1>","6f2b4663":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left;\"> 5.3.2) The correlation between this continuos features and the target <\/h1>","8e8cccf0":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:170%; text-align:left;\"> 5.3) Relationship between features <\/h1>","396c73ef":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 3) Load Required Libraries <\/h1>","a03113af":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 2) XGBoost hyperparameters <\/h1>","59171159":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left;\"> 6.2.4) RandomizedSearchCV <\/h1>","2a2a360b":"#### ii) Bivariate Analysis","75164be8":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left;\"> 5.5) Outliers <\/h1>"}}