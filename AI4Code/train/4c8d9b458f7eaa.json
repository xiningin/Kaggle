{"cell_type":{"30a9a90a":"code","11b54b4e":"code","84627923":"code","6d8fd3ac":"code","ca897f21":"code","4793b5aa":"code","6966e795":"code","167bcb8a":"code","e456515a":"code","8ef2b4ca":"code","8460c346":"code","7a3fd3fe":"code","d36802ed":"code","9abf28f7":"code","a4b07f58":"code","2aa57e85":"code","03c90caa":"code","bcf0dc8e":"code","55cc460f":"code","9605feb5":"code","f6b2c02b":"code","f1e01df8":"code","93432d23":"code","76d5f619":"code","9f2f85ed":"markdown","59781a8b":"markdown","03f3a232":"markdown","e4af233a":"markdown","d77e8666":"markdown","6a2e4cd1":"markdown","f0018944":"markdown","6d7f853c":"markdown","f56d11ec":"markdown"},"source":{"30a9a90a":"# Loading packages\nimport pandas as pd #Analysis \nimport matplotlib.pyplot as plt #Visulization\nimport seaborn as sns #Visulization\nimport numpy as np #Analysis \nfrom scipy.stats import norm #Analysis \nfrom sklearn.preprocessing import StandardScaler #Analysis \nfrom scipy import stats #Analysis \nimport warnings \nwarnings.filterwarnings('ignore')\n%matplotlib inline\nimport gc\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\nimport xgboost as xgb\nfrom scipy.optimize import minimize","11b54b4e":"train = pd.read_csv(\"..\/input\/train.csv\", parse_dates=[\"first_active_month\"])\nprint(\"shape of train : \",train.shape)","84627923":"test = pd.read_csv(\"..\/input\/test.csv\", parse_dates=[\"first_active_month\"])\nprint(\"shape of test : \",test.shape)","6d8fd3ac":"import datetime\n\nfor df in [train,test]:\n    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n    df['year'] = df['first_active_month'].dt.year\n    df['month'] = df['first_active_month'].dt.month\n    df['elapsed_time'] = (datetime.date(2018, 2, 1) - df['first_active_month'].dt.date).dt.days\n\ntarget = train['target']\ndel train['target']","ca897f21":"ht = pd.read_csv(\"..\/input\/historical_transactions.csv\")\nprint(\"shape of historical_transactions : \",ht.shape)","4793b5aa":"ht['authorized_flag'] = ht['authorized_flag'].map({'Y':1, 'N':0})","6966e795":"def aggregate_historical_transactions(history):\n    \n    history.loc[:, 'purchase_date'] = pd.DatetimeIndex(history['purchase_date']).\\\n                                      astype(np.int64) * 1e-9\n    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'merchant_id': ['nunique'],\n        'city_id': ['nunique'],\n        'purchase_amount': ['sum', 'median', 'max', 'min', 'std'],\n        'installments': ['sum', 'median', 'max', 'min', 'std'],\n        'purchase_date': [np.ptp],\n        'month_lag': ['min', 'max']\n        }\n    agg_history = history.groupby(['card_id']).agg(agg_func)\n    agg_history.columns = ['hist_' + '_'.join(col).strip() \n                           for col in agg_history.columns.values]\n    agg_history.reset_index(inplace=True)\n    \n    df = (history.groupby('card_id')\n          .size()\n          .reset_index(name='hist_transactions_count'))\n    \n    agg_history = pd.merge(df, agg_history, on='card_id', how='left')\n    \n    return agg_history\n\nhistory = aggregate_historical_transactions(ht)\ndel ht\ngc.collect()","167bcb8a":"train = pd.merge(train, history, on='card_id', how='left')\ntest = pd.merge(test, history, on='card_id', how='left')","e456515a":"merchant = pd.read_csv(\"..\/input\/merchants.csv\")\nprint(\"shape of merchant : \",merchant.shape)","8ef2b4ca":"new_merchant = pd.read_csv(\"..\/input\/new_merchant_transactions.csv\")\nprint(\"shape of new_merchant_transactions : \",new_merchant.shape)","8460c346":"new_merchant['authorized_flag'] = new_merchant['authorized_flag'].map({'Y':1, 'N':0})","7a3fd3fe":"def aggregate_new_transactions(new_trans):    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'merchant_id': ['nunique'],\n        'city_id': ['nunique'],\n        'purchase_amount': ['sum', 'median', 'max', 'min', 'std'],\n        'installments': ['sum', 'median', 'max', 'min', 'std'],\n        'month_lag': ['min', 'max']\n        }\n    agg_new_trans = new_trans.groupby(['card_id']).agg(agg_func)\n    agg_new_trans.columns = ['new_' + '_'.join(col).strip() \n                           for col in agg_new_trans.columns.values]\n    agg_new_trans.reset_index(inplace=True)\n    \n    df = (new_trans.groupby('card_id')\n          .size()\n          .reset_index(name='new_transactions_count'))\n    \n    agg_new_trans = pd.merge(df, agg_new_trans, on='card_id', how='left')\n    \n    return agg_new_trans\n\nnew_trans = aggregate_new_transactions(new_merchant)","d36802ed":"train = pd.merge(train, new_trans, on='card_id', how='left')\ntest = pd.merge(test, new_trans, on='card_id', how='left')","9abf28f7":"use_cols = [col for col in train.columns if col not in ['card_id', 'first_active_month']]\n\ntrain = train[use_cols]\ntest = test[use_cols]\n\nfeatures = list(train[use_cols].columns)\ncategorical_feats = [col for col in features if 'feature_' in col]","a4b07f58":"from sklearn.preprocessing import LabelEncoder\nfor col in categorical_feats:\n    print(col)\n    lbl = LabelEncoder()\n    lbl.fit(list(train[col].values.astype('str')) + list(test[col].values.astype('str')))\n    train[col] = lbl.transform(list(train[col].values.astype('str')))\n    test[col] = lbl.transform(list(test[col].values.astype('str')))","2aa57e85":"df_all = pd.concat([train, test])\ndf_all = pd.get_dummies(df_all, columns=categorical_feats)\n\nlen_train = train.shape[0]\n\ntrain = df_all[:len_train]\ntest = df_all[len_train:]","03c90caa":"lgb_params = {\"objective\" : \"regression\", \"metric\" : \"rmse\", \n               \"max_depth\": 9, \"min_child_samples\": 20, \n               \"reg_alpha\": 1, \"reg_lambda\": 1,\n               \"num_leaves\" : 64, \"learning_rate\" : 0.005, \n               \"subsample\" : 0.8, \"colsample_bytree\" : 0.8, \n               \"verbosity\": -1}\n\nFOLDs = KFold(n_splits=10, shuffle=True, random_state=1989)\n\noof_lgb = np.zeros(len(train))\npredictions_lgb = np.zeros(len(test))\n\nfeatures_lgb = list(train.columns)\nfeature_importance_df_lgb = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train)):\n    trn_data = lgb.Dataset(train.iloc[trn_idx], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx], label=target.iloc[val_idx])\n\n    print(\"LGB \" + str(fold_) + \"-\" * 50)\n    num_round = 10000\n    clf = lgb.train(lgb_params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 50)\n    oof_lgb[val_idx] = clf.predict(train.iloc[val_idx], num_iteration=clf.best_iteration)\n\n    fold_importance_df_lgb = pd.DataFrame()\n    fold_importance_df_lgb[\"feature\"] = features_lgb\n    fold_importance_df_lgb[\"importance\"] = clf.feature_importance()\n    fold_importance_df_lgb[\"fold\"] = fold_ + 1\n    feature_importance_df_lgb = pd.concat([feature_importance_df_lgb, fold_importance_df_lgb], axis=0)\n    predictions_lgb += clf.predict(test, num_iteration=clf.best_iteration) \/ FOLDs.n_splits\n    \nprint(np.sqrt(mean_squared_error(oof_lgb, target)))","bcf0dc8e":"cols = (feature_importance_df_lgb[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n\nbest_features = feature_importance_df_lgb.loc[feature_importance_df_lgb.feature.isin(cols)]\n\nplt.figure(figsize=(14,14))\nsns.barplot(x=\"importance\",\n            y=\"feature\",\n            data=best_features.sort_values(by=\"importance\",\n                                           ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')","55cc460f":"xgb_params = {'eta': 0.005, 'max_depth': 9, 'subsample': 0.8, 'colsample_bytree': 0.8, \n          'objective': 'reg:linear', 'eval_metric': 'rmse', 'silent': True}\n\nFOLDs = KFold(n_splits=10, shuffle=True, random_state=1989)\n\noof_xgb = np.zeros(len(train))\npredictions_xgb = np.zeros(len(test))\n\nfor fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train)):\n    trn_data = xgb.DMatrix(data=train.iloc[trn_idx], label=target.iloc[trn_idx])\n    val_data = xgb.DMatrix(data=train.iloc[val_idx], label=target.iloc[val_idx])\n    watchlist = [(trn_data, 'train'), (val_data, 'valid')]\n    print(\"xgb \" + str(fold_) + \"-\" * 50)\n    num_round = 10000\n    xgb_model = xgb.train(xgb_params, trn_data, num_round, watchlist, early_stopping_rounds=50, verbose_eval=1000)\n    oof_xgb[val_idx] = xgb_model.predict(xgb.DMatrix(train.iloc[val_idx]), ntree_limit=xgb_model.best_ntree_limit+50)\n\n    predictions_xgb += xgb_model.predict(xgb.DMatrix(test), ntree_limit=xgb_model.best_ntree_limit+50) \/ FOLDs.n_splits\n\nnp.sqrt(mean_squared_error(oof_xgb, target))","9605feb5":"print('lgb', np.sqrt(mean_squared_error(oof_lgb, target)))\nprint('xgb', np.sqrt(mean_squared_error(oof_xgb, target)))","f6b2c02b":"def find_best_weight(preds, target):\n    def _validate_func(weights):\n        ''' scipy minimize will pass the weights as a numpy array '''\n        final_prediction = 0\n        for weight, prediction in zip(weights, preds):\n                final_prediction += weight * prediction\n        return np.sqrt(mean_squared_error(final_prediction, target))\n\n    #the algorithms need a starting value, right not we chose 0.5 for all weights\n    #its better to choose many random starting points and run minimize a few times\n    starting_values = [0.5]*len(preds)\n\n    #adding constraints and a different solver as suggested by user 16universe\n    #https:\/\/kaggle2.blob.core.windows.net\/forum-message-attachments\/75655\/2393\/otto%20model%20weights.pdf?sv=2012-02-12&se=2015-05-03T21%3A22%3A17Z&sr=b&sp=r&sig=rkeA7EJC%2BiQ%2FJ%2BcMpcA4lYQLFh6ubNqs2XAkGtFsAv0%3D\n    cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n    #our weights are bound between 0 and 1\n    bounds = [(0, 1)] * len(preds)\n    \n    res = minimize(_validate_func, starting_values, method='Nelder-Mead', bounds=bounds, constraints=cons)\n    \n    print('Ensemble Score: {best_score}'.format(best_score=(1-res['fun'])))\n    print('Best Weights: {weights}'.format(weights=res['x']))\n    \n    return res","f1e01df8":"res = find_best_weight([oof_lgb, oof_xgb], target)","93432d23":"total_sum = 0.71044189 * oof_lgb + 0.36912984 * oof_xgb\nprint(\"CV score: {:<8.5f}\".format(mean_squared_error(total_sum, target)**0.5))","76d5f619":"sub_df = pd.read_csv('..\/input\/sample_submission.csv')\nsub_df[\"target\"] = 0.71044189 * predictions_lgb + 0.36912984 * predictions_xgb\nsub_df.to_csv(\"submission_ensemble.csv\", index=False)","9f2f85ed":"# Feature engineering","59781a8b":"## XGBoost","03f3a232":"# References\n* https:\/\/www.kaggle.com\/rooshroosh\/simple-data-exploration-with-python-lb-3-762\n* https:\/\/www.kaggle.com\/youhanlee\/hello-elo-ensemble-will-help-you\n* https:\/\/www.kaggle.com\/truocpham\/feature-engineering-and-lightgbm-starter\n* https:\/\/www.kaggle.com\/youhanlee\/hello-elo-ensemble-will-help-you","e4af233a":"**Merge history to train and test**","d77e8666":"## History Transactions Processing","6a2e4cd1":"# Import libs and Load data","f0018944":"## Ensemble and make submission","6d7f853c":"## LightGBM","f56d11ec":"## Merchants Processing"}}