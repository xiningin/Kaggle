{"cell_type":{"435f4103":"code","4251b298":"code","30f7ee94":"code","515c0d68":"code","b15c9ae6":"code","967556a8":"code","78a4b4ce":"code","1a0358c7":"code","9646fb0a":"code","e005afa6":"code","30917421":"code","c24c0730":"code","ee9339d5":"code","79917676":"code","628b709d":"code","91e727e7":"code","27ce5c0d":"code","6a4851aa":"code","19862754":"code","07c8c7c2":"code","d6c9eefb":"code","5f0af5a7":"code","7de40dc3":"code","a1d815ee":"code","57fd4e17":"code","27dd35a4":"code","46f477b9":"code","27b2e45c":"code","a3d92c58":"code","72bff74a":"code","e5e3d37c":"code","4df4d33c":"code","dbbf406d":"code","899c0cdc":"code","6f2a5966":"code","bdef42ac":"code","53557cc7":"code","e9fa0736":"code","e54cfa2f":"code","c5499006":"code","dcf036ea":"code","ba8953d6":"code","097b5dbd":"code","6ee69b52":"code","8c3efeea":"code","dc179e56":"code","6b9f65bf":"code","919a858f":"code","daf4c90a":"code","6ba4ff44":"code","d62a49be":"code","1d0a59c6":"code","8d34dd43":"code","f5476577":"code","3ae57c37":"code","30bf3d13":"code","250994aa":"code","6f6f60c6":"code","9938153e":"code","605a0239":"code","9c3b396e":"code","726f9265":"code","93da2f40":"code","1d8753a9":"code","2585259b":"code","9108f961":"code","03e4c9b1":"code","e1941100":"code","62fbbe66":"code","e1dbcac3":"code","67d1b1a9":"code","a877f5b0":"code","a6f85fef":"code","3c8e08f4":"code","1c5105f8":"code","904197b7":"code","d7b79256":"code","a1468e08":"code","d91830fe":"code","31a6338f":"code","1b3b386b":"code","f13aa8fc":"code","e685d444":"code","cf5e2247":"code","2a519fd0":"code","e7b2540e":"code","5ffc2991":"code","07b7295e":"code","0864bc48":"code","4b28d043":"code","ec1faa04":"code","75541b0c":"code","945404ff":"code","02e92245":"code","0d590b26":"code","cf1b8aa9":"code","48478a30":"code","605f669c":"markdown","5cb06c4d":"markdown","a62dc590":"markdown","90b7979f":"markdown","32e900d7":"markdown","88f569d7":"markdown","da367979":"markdown","1781fb7a":"markdown","324baaed":"markdown","e53cc358":"markdown","be812f71":"markdown","9fc520f4":"markdown","f4277ee5":"markdown","70449dd5":"markdown","b888e1d5":"markdown","85ee852c":"markdown","82261561":"markdown","f467b512":"markdown","480b3493":"markdown","0f4c946c":"markdown","ec9bd362":"markdown","9128a917":"markdown","2a62a893":"markdown","c036b38e":"markdown","935eb89a":"markdown","14b72361":"markdown","62f4d285":"markdown","491d4485":"markdown","1ed6d780":"markdown","b10859ea":"markdown","95ef1712":"markdown","6b7e73bd":"markdown","bfef82d5":"markdown"},"source":{"435f4103":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4251b298":"import json\nfrom pandas.io.json import json_normalize\n\n\nwith open(\"\/kaggle\/input\/housedata\/data.dat\") as json_file:\n    json_data = json.load(json_file)\n    houses=pd.DataFrame(json_data)","30f7ee94":"df = json_normalize(json_data['houses'])","515c0d68":"print (df.head(5))","b15c9ae6":"df['date'].value_counts()","967556a8":"%matplotlib inline\nimport matplotlib.pyplot as plt\n","78a4b4ce":"df['date'].value_counts().plot(kind = \"line\" , figsize=(15,5))\n\nplt.show()","1a0358c7":"df['address_list'] = df['address'].str.split(',')\n#'address' is split according to \",\" and stored into 'address_list'","9646fb0a":"df['address_list']","e005afa6":"df['street'] = df['address_list'].apply( lambda col : col[0])\ndf['city'] = df['address_list'].apply( lambda col : col[1])\ndf['statezip'] = df['address_list'].apply( lambda col : col[2])\ndf['country'] = df['address_list'].apply( lambda col : col[3])","30917421":"df.head()","c24c0730":"df.drop('address', axis=1, inplace= True)\ndf.drop('address_list', axis=1 , inplace= True)\n\ndf.head(3)","ee9339d5":"df['bathrooms'] = df.rooms.str.extract('Number of bathrooms: (\\d.\\d+)', expand= True )\n#df['bedrooms'] = df.rooms.str.extract('Number of bedrooms: (\\d+)', expand= True )\n\n#df.head(10)","79917676":"df.head(5)","628b709d":"df.drop( 'rooms' , axis=1 , inplace = True)\ndf.head(1)","91e727e7":"#splitting the values of 'area.sqft_living\/sqft_lot' according to \"=\"\ndf['area.sqft_living\/sqft_lot_list'] = df['area.sqft_living\/sqft_lot'].str.split('=')\n\n# col[1] has the values of sqft_living and sqft_lot, hence storing it in 'area.sqft_living\/sqft_lot_list_list1'\ndf['area.sqft_living\/sqft_lot_list_list1'] = df['area.sqft_living\/sqft_lot_list'].apply(lambda col: col[1])\ndf['area.sqft_living\/sqft_lot_list_list2'] = df['area.sqft_living\/sqft_lot_list_list1'].str.split('\\ ')\n\ndf['sqft_living']=df['area.sqft_living\/sqft_lot_list_list2'].apply(lambda col: col[0])\ndf['sqft_lot']=df['area.sqft_living\/sqft_lot_list_list2'].apply(lambda col: col[1])","27ce5c0d":"df.head(5)","6a4851aa":"#dropping all dummy list used to store while splitting values of 'area.sqft_living\/sqft_lot'\ndf.drop('area.sqft_living\/sqft_lot_list_list1',axis=1, inplace=True)\ndf.drop('area.sqft_living\/sqft_lot_list_list2', axis=1, inplace = True)\ndf.drop('area.sqft_living\/sqft_lot_list', axis=1, inplace = True)\ndf.drop('area.sqft_living\/sqft_lot', axis=1, inplace = True)","19862754":"df.head(5)","07c8c7c2":"df.rename(index = str, columns= {\"area.sqft_above\" :\"sqft_above\" , \"area.sqft_basement\" :\"sqft_basement\"}, inplace= True)\n\ndf.head(3)","d6c9eefb":"df['sqft_living'] = df['sqft_living'].map(lambda x : x.rstrip('\\\\'))\ndf.head(2)","5f0af5a7":"df['temp'] = df[['sqft_basement' , 'sqft_above']].sum(axis=1)\n\ndf[df['temp'] != df['sqft_living']].index","7de40dc3":"# there is a different date formate for \"23052014T000000\" replacing it with \"20140523T000000\"\ndf['date'].replace(['23052014T000000'],['20140523T000000'],inplace=True) \n\n# there is a date value with 20140631T000000, where as June month does not contain 31st and hence considering it has Irregularities. changing date 20140631T000000 to 20140701T000000  \ndf['date'].replace(['20140631T000000'],['20140701T000000'],inplace=True)","a1d815ee":"df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%dT%H:%m:%s')\ndf.head(2)","57fd4e17":"df[['bedrooms', 'bathrooms']] = df[['bedrooms', 'bathrooms']].astype(float)\ndf[['sqft_lot', 'sqft_living']] =df[['sqft_lot', 'sqft_living']].astype(np.int64)\ndf['price'] = df['price'].apply(np.int64)\n\ndf.info()","27dd35a4":"df = df[['date', 'price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', \n         'condition', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'street', 'city', 'statezip', \n         'country']]","46f477b9":"df.head()","27b2e45c":"df.describe(include = \"all\")","a3d92c58":"df['city'].value_counts()","72bff74a":"#City\n# there is a Lexical error in city \"sammamish\" which is replaced with the average value of city named \"Sammamish\"\ndf['city'].replace(['sammamish'],['Sammamish'],inplace=True) \n\n# there is a Lexical error in city \"Samamish\" which is replaced with the average value of city named \"Sammamish\"\ndf['city'].replace(['Samamish'],['Sammamish'],inplace=True) \n\n# there is a Lexical error in city \"Seaattle\" which is replaced with the average value of city named \"Seattle\"\ndf['city'].replace(['Seaattle'],['Seattle'],inplace=True) \n\n# there is a Lexical error in city \"Seatle\" which is replaced with the average value of city named \"Seattle\"\ndf['city'].replace(['Seatle'],['Seattle'],inplace=True) \n \n# there is a Lexical error in city \"seattle\" which is replaced with the average value of city named \"Seattle\"\ndf['city'].replace(['seattle'],['Seattle'],inplace=True)\n\n# there is a Lexical error in city \"Issaguah\" which is replaced with the average value of city named \"Issaquah\"\ndf['city'].replace(['Issaguah'],['Issaquah'],inplace=True)\n\n# there is a Lexical error in city \"Woodenville\" which is replaced with the average value of city named \"Woodinville\"\ndf['city'].replace(['Woodenville'],['Woodinville'],inplace=True)\n \n# there is a Lexical error in city \"redmond\" which is replaced with the average value of city named \"Redmond\"\ndf['city'].replace(['redmond'],['Redmond'],inplace=True)\n\n# there is a Lexical error in city \"Redmund\" which is replaced with the average value of city named \"Redmond\"\ndf['city'].replace(['Redmund'],['Redmond'],inplace=True)\n\n# there is a Lexical error in city \"Redmund\" which is replaced with the average value of city named \"Redmond\"\ndf['city'].replace(['Redmonde'],['Redmond'],inplace=True)\n\n# there is a Lexical error in city \"auburn\" which is replaced with the average value of city named \"Auburn\"\ndf['city'].replace(['auburn'],['Auburn'],inplace=True)\n\n# there is a Lexical error in city \"Auburnt\" which is replaced with the average value of city named \"Auburn\"\ndf['city'].replace(['Auburnt'],['Auburn'],inplace=True)\n\n# there is a Lexical error in city \"Sureline\" which is replaced with the average value of city named \"Shoreline \"\ndf['city'].replace(['Sureline'],['Shoreline'],inplace=True)\n\n# there is a Lexical error in city \"Bellvue\" which is replaced with the average value of city named \"Bellevue \"\ndf['city'].replace(['Bellvue'],['Bellevue'],inplace=True)\n\n# there is a Lexical error in city \"Belleview\" which is replaced with the average value of city named \"Bellevue \"\ndf['city'].replace(['Belleview'],['Bellevue'],inplace=True)\n\n# there is a Lexical error in city \"Snogualmie\" which is replaced with the average value of city named \"Snoqualmie\"\ndf['city'].replace(['Snogualmie'],['Snoqualmie'],inplace=True)\n\n# there is a Lexical error in city \"Coronation\" which is replaced with the average value of city named \"Carnation\"\ndf['city'].replace(['Coronation'],['Carnation'],inplace=True)\n\n# there is a Lexical error in city \"Kirklund\" which is replaced with the average value of city named \"Kirkland\"\ndf['city'].replace(['Kirklund'],['Kirkland'],inplace=True)\n\n#The above changes can aslo be done as show in below code,\n#df.city.replace({\"sammamish\":\"Sammamish\", \"Samamish\": \"Sammamish\", \"Seaattle\":\"Seattle\", \"Seatle\":\"Seattle\",\n#\"seattle\":\"Seattle\", \"Issaguah\":\"Issaquah\"}, inplace=True) \ndf.city.value_counts()","e5e3d37c":"df['bathrooms'].value_counts()","4df4d33c":"#Bathroom\ndf['bathrooms'].replace([1.70],[1.75],inplace=True) \n\ndf['bathrooms'].replace([1.05],[1.50],inplace=True) \n\ndf['bathrooms'].replace([2.55],[2.50],inplace=True) \n\ndf['bathrooms'].replace([2.30],[2.25],inplace=True) \n\ndf['bathrooms'].replace([2.57],[2.75],inplace=True) \n\ndf['bathrooms'].value_counts()","dbbf406d":"df[df.duplicated(keep=False)]","899c0cdc":"#dropping the row which are duplictaes\ndf.drop_duplicates(keep=\"first\", inplace=True)","6f2a5966":"df.info()","bdef42ac":"df.isnull().sum()","53557cc7":"df['yr_renovated'].unique()","e9fa0736":"df_dummy = df.copy()\ndf_dummy.head()","e54cfa2f":"df_dummy['yr_renovated'] = np.nan_to_num(df_dummy['yr_renovated']).astype(np.int64)","c5499006":"df_dummy","dcf036ea":"from sklearn.linear_model import LinearRegression\n\nreg = LinearRegression()","ba8953d6":"labels = df_dummy['yr_renovated']\n\n#also converting date to 0 or 1 so it doesnt influence our data that much\n\nconv_dates = [1 if values == 2014 else 0 for values in df_dummy.date]","097b5dbd":"df_dummy['date'] = conv_dates\n\ntrain_df1 = df_dummy.drop(['city', 'street', 'country', 'statezip', 'price'], axis=1)","6ee69b52":"train_df1.head(5)","8c3efeea":"from sklearn.model_selection import train_test_split","dc179e56":"X_train, X_test, y_train, y_test = train_test_split(train_df1, labels, test_size=0.10, random_state=2)","6b9f65bf":"reg.fit(X_train, y_train)","919a858f":"reg.score(X_test, y_test)","daf4c90a":"df['yr_renovated'].fillna(df.groupby([\"yr_built\", \"condition\"])[\"yr_renovated\"].transform(\"mean\"), inplace= True)","6ba4ff44":"df.yr_renovated.describe()","d62a49be":"df.isnull().sum()","1d0a59c6":"df['yr_renovated'].fillna(0, inplace= True)","8d34dd43":"df_dummy = df.copy()","f5476577":"#we know that 'yr_renovated' are to be predicted , hence we set labels (output) as 'yr_renovated' column\nlabels = df_dummy['yr_renovated']\n#Converting dates to 1\u2019s and 0\u2019s so that it doesn\u2019t influence our data much\n#We use 0 for houses which are new that is built after 2014.\nconv_dates = [1 if values == 2014 else 0 for values in df_dummy.date]\ndf_dummy['date'] = conv_dates\ntrain_df1 = df_dummy.drop(['city','street','country','statezip','price'],axis=1)","3ae57c37":"reg.fit(X_train,y_train)","30bf3d13":"reg.score(X_test,y_test)","250994aa":"df_imptd1 = df.copy()","6f6f60c6":"#we know that 'yr_renovated' are to be predicted , hence we set labels (output) as 'yr_renovated' column\nlabels = df_imptd1['yr_renovated']\n#Converting dates to 1\u2019s and 0\u2019s so that it doesn\u2019t influence our data much\n#We use 0 for houses which are new that is built after 2014.\nconv_dates = [1 if values == 2014 else 0 for values in df_imptd1.date]\ndf_imptd1['date'] = conv_dates\ntrain1 = df_imptd1.drop(['city','street','country','statezip','price'],axis=1)","9938153e":"#train data is set to 90% and 10% of the data to be my test data , and randomized the splitting of data by using random_state.\nX_train, X_test, y_train, y_test = train_test_split(train1,labels,test_size = 0.10, random_state=2)","605a0239":"reg.fit(X_train,y_train)","9c3b396e":"reg.score(X_test,y_test)","726f9265":"df['yr_renovated'] = df_dummy['yr_renovated'].astype(np.int64)","93da2f40":"df.head(5)","1d8753a9":"df.price.value_counts()","2585259b":"df_dummy[df_dummy['price'] < 1] ","9108f961":"df[[\"price\",\"bedrooms\",\"bathrooms\",\"sqft_living\",\"sqft_lot\",\"sqft_above\",\"yr_built\",\"sqft_living\",\"sqft_lot\"]].describe()","03e4c9b1":"#replacing all the 0.0 to NaN\ndf['price'] = df['price'].replace(0.0, np.nan)","e1941100":"#replacing all NaN to mean values of price \ndf[\"price\"].fillna(df_dummy.groupby([\"bedrooms\",\"bathrooms\",\"city\",\"statezip\"])[\"price\"].transform(\"mean\"), inplace=True)\n","62fbbe66":"df.price.describe()","e1dbcac3":"df.isnull().sum()","67d1b1a9":"df.dropna(subset=['price'],axis=0,inplace=True)","a877f5b0":"df_final = df.copy()","a6f85fef":"import matplotlib.pyplot as plt\n%matplotlib inline\n\ndf.boxplot(figsize=(15,10))","3c8e08f4":"bp = df.boxplot(column='price',figsize=(10,20))","1c5105f8":"# We can see a bunch of price above 0.5, then something around 2.5, the outliers are:\ndf[df['price'] > 2.0] ","904197b7":"# plotting baoxplot to check outliers price vs bedrooms\nbp = df.boxplot(column='price', by = 'bedrooms',figsize=(15,10))","d7b79256":"# price according to sqft_lot\ndf.boxplot(column='sqft_lot', figsize=(10,10))","a1468e08":"\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\nimport seaborn as sns\nfrom matplotlib import rcParams\n\n%matplotlib inline \n%pylab inline \n\n","d91830fe":"sns.pairplot(data=df, x_vars=['bathrooms','bedrooms','sqft_living','sqft_lot','sqft_above','waterfront'], y_vars=[\"price\"])","31a6338f":"sns.jointplot('sqft_living','price', data=df, xlim=(500,3500), ylim=(100000,1000000), size=10, alpha=.5)","1b3b386b":"df.groupby('statezip')['price'].mean()","f13aa8fc":"import seaborn as sns\nimport mpl_toolkits","e685d444":"df['bedrooms'].value_counts().plot(kind= 'bar')\nplt.title('Number of Bedrooms')\nplt.xlabel('Bedroom')\nplt.ylabel('House Count')\nplt.show()","cf5e2247":"#PRIcE Vs SQFT_LIVING\nplt.scatter(df.price,df.sqft_living)\nplt.title(\"Price Vs Square feet\")\nplt.xlabel('Square feet')\nplt.ylabel('Price')\nplt.show()","2a519fd0":"#PRICE Vs BEDROOMS\nplt.scatter(df.bedrooms,df.price)\nplt.title(\"Bedroom and Price\")\nplt.xlabel(\"Bedrooms\")\nplt.ylabel(\"Price\")\nplt.show()","e7b2540e":"#Total sqft including basement vs price and waterfront vs price\nplt.scatter((df['sqft_living']+df['sqft_basement']),df['price'])\nplt.title(\"sqft_living and sqft_basement vs Price\")\nplt.xlabel(\"sqft_living and sqft_basement\")\nplt.ylabel(\"Price\")\nplt.show()","5ffc2991":"plt.scatter(df.waterfront,df.price)\nplt.title(\"Waterfront Vs Price (0 = No Waterfront )\")\nplt.xlabel(\"waterfront\")\nplt.ylabel(\"Price\")\nplt.show()","07b7295e":"plt.scatter(df.floors,df.price)\nplt.title(\"floors Vs Price\")\nplt.xlabel(\"floors\")\nplt.ylabel(\"Price\")\nplt.show()","0864bc48":"plt.scatter(df.condition,df.price)\nplt.title(\"Condition Vs Price\")\nplt.xlabel(\"Condition\")\nplt.ylabel(\"Price\")\nplt.show()","4b28d043":"from sklearn.linear_model import LinearRegression\n#Initializing Linear Regression to a variable reg\nreg = LinearRegression()\n\nlabels = df['price']\nconv_dates = [1 if values == 2014 else 0 for values in df.date]\ndf['date'] = conv_dates\ntrain1 = df.drop(['city','street','country','statezip','price'],axis=1)","ec1faa04":"X_train, X_test, y_train, y_test = train_test_split(train1,labels,test_size = 0.10, random_state=2)","75541b0c":"map(pd.np.shape,[X_train, X_test, y_train, y_test])","945404ff":"reg.fit(X_train,y_train)","02e92245":"reg.score(X_test,y_test)","0d590b26":"df_final.info()","cf1b8aa9":"df_final.head(4)","48478a30":"\n\nfilename = 'Predicted_pricing.csv'\ndf_final.to_csv(filename, encoding='utf-8', index=False)\n\n","605f669c":"# Auditing and cleansing the loaded data\n\nWe are inspecting and auditind the data to identify the data problems, and then fix the problems. Different generic and major data problems could be found in the data might include:\n\n    Lexical errors, e.g., typos and spelling mistakes\n    Irregularities, e.g., abnormal data values and data formats\n    Violations of the Integrity constraint.\n    Outliers\n    Duplications\n    Missing values\n    Inconsistency, e.g., inhomogeneity in values and types in representing the same data\n\n","5cb06c4d":"We need to change the date values now","a62dc590":"Dropping all nan values from price","90b7979f":"Now we can predict some prices","32e900d7":"Now we should check how many values are zero","88f569d7":"We can find out average house price according to the zip code","da367979":"Preparing test and train data","1781fb7a":"Loading the .dat file into  A PYTHON dictionary for my better understanding and usuability\nNow using a json_normalise i will flatten the JSON dictionary data into a table and will save into a dataframe ","324baaed":"Lets see how the address_list looks like","e53cc358":"We can replace the 'NaN' values by two ways \n1.  with zeros and \n1.  with mean values","be812f71":"#renaming the columns from \"area.sqft_above\" to \"sqft_above\" and \"area.sqft_basement\" to \"sqft_basement\"","9fc520f4":"Now we need to split the address into 'Statezip' and 'Street\" value","f4277ee5":"Now we can drop the rooms col\n","70449dd5":"Now we need to investigate for duplicate entry","b888e1d5":"We can replace the zero value of house prices with the mean of the prices","85ee852c":"This model predicts only 60%, Not much efficient , in future i will learn to perfect the model more than this ","82261561":"We need to remove the '\\' value from  'sqft_living'","f467b512":"we can check whether the value of total living area is sum of sqft_basement + sqft_above or not","480b3493":"we should create a dummy dataframe for using it the null values for yr_renovate by using mean","0f4c946c":"\nInvestigating Lexical errors for all the columes in dataframe","ec9bd362":"# Investigating the missing values\n\n'nan' doesn't occur in counts()\n\n    checking how many 'NaN' values are there\n    checking how many 0\n    checking how many < 1\n\n","9128a917":"Now we will separate the rooms value into bathrooms from 'Number of bathrooms:' value and also bedrooms value from 'Number of bedrooms' value","2a62a893":"Predicting the 'yr_renovated' value","c036b38e":"Our final dataframe is ready","935eb89a":"Using boxplot to detect outlier","14b72361":"We know that 'yr_renovated' needs to be predicted so we can store it in a labels","62f4d285":"# Working with the regression model","491d4485":"We can plot some graphs for better understanding","1ed6d780":"droping the 'address' and'address_list' col","b10859ea":"# Time to put down some formulas to predict","95ef1712":"some bathroom values should be set to standard values for better understanding","6b7e73bd":"We should rearrange the dataframe for clear understanding or our better understanding","bfef82d5":"We will split the address_list 0th col will be 'street' code ,1st col will be 'city', 2nd col will be 'statezip' and 3rd col will be country"}}