{"cell_type":{"a7c14080":"code","79ad2de4":"code","b5ba3afa":"code","52225ecc":"code","630bf61c":"code","9a725453":"code","1d85dc63":"code","e5c94bbb":"code","53179dcd":"code","6464232b":"code","bcd902e3":"code","60ba867c":"code","f0ebb4de":"code","51d7082f":"code","dedd392d":"code","80c97774":"code","d7bad113":"code","1203c8c6":"code","700849dc":"code","8c580bff":"code","20d3f715":"code","9306f5f0":"code","5ef73f74":"markdown","ee3b28a6":"markdown","c1872ea6":"markdown","9324e2a3":"markdown","bf9ba782":"markdown","c4bb8c84":"markdown","6288a800":"markdown","01eb8260":"markdown","762d7a93":"markdown","3d381bbe":"markdown","47cb707d":"markdown","cb37bee0":"markdown","c22587ad":"markdown","fc33570c":"markdown","ab28c56c":"markdown","1a4ba9ba":"markdown","fe8fea3b":"markdown","4d43a1b8":"markdown","29f76afd":"markdown","f3b6e5eb":"markdown","40c4bd32":"markdown","4aef241f":"markdown","b0fac7e3":"markdown","f34ee8d4":"markdown"},"source":{"a7c14080":"from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso, LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import LeaveOneOut, cross_validate, cross_val_predict\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom yellowbrick.model_selection import LearningCurve, FeatureImportances\nfrom yellowbrick.regressor import AlphaSelection\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n!pip install openpyxl","79ad2de4":"A = pd.read_excel(\"..\/input\/machining\/A.xlsx\") \nB = pd.read_excel(\"..\/input\/machining\/B.xlsx\")\n\nA.head()","b5ba3afa":"B.head()","52225ecc":"df = pd.concat([A, B], axis=0, ignore_index=True)\n\ndf.drop('Exp no.', axis=1, inplace=True)\n\ndf.info()","630bf61c":"# Center in zero so we can understand correlations better\nsns.heatmap(df.corr(), cmap = 'RdBu', center=0., fmt = '.2f', annot=True);","9a725453":"inputs = ['DOC (mm)', 'CS (m\/min)','FR (mm\/rev)']\noutputs = ['Tool wear (\u00b5m)', 'Surface Roughness (\u00b5m)']\n\n\nfor i in inputs:\n    fig, ax = plt.subplots(1, 2, figsize=(15,6))\n    sns.boxplot(x=i, y=outputs[0], data=df, ax=ax[0])\n    sns.boxplot(x=i, y=outputs[1], data=df, ax=ax[1]);","1d85dc63":"fig, ax = plt.subplots(1, 2, figsize=(15,6))\n\nsns.scatterplot(data=df, x=inputs[0], y=outputs[0], hue=inputs[1], size=inputs[2],\n                sizes=(20, 250),ax=ax[0], alpha=0.8, palette='viridis_r')\nsns.scatterplot(data=df, x=inputs[0], y=outputs[1], hue=inputs[1], size=inputs[2],\n                sizes=(20, 250),ax=ax[1], alpha=0.8, palette='viridis_r')\nax[0].grid(False)\nax[1].grid(False);","e5c94bbb":"fig, ax = plt.subplots(1, 2, figsize=(15,6))\nsns.histplot(data=df, x=outputs[0], ax=ax[0])\nsns.histplot(data=df, x=outputs[1], ax=ax[1]);\nax[0].grid(False)\nax[1].grid(False);","53179dcd":"df['log(Tool wear (\u00b5m))'] = np.log(df['Tool wear (\u00b5m)'])\n\noutputs.append('log(Tool wear (\u00b5m))')\n\nsns.histplot(data=df, x=outputs[2]);\nplt.grid(False);","6464232b":"lin_model = LinearRegression()\nX = df[inputs]\ny_tw = df[outputs[0]]\ny_sr = df[outputs[1]]\n\nscaler = StandardScaler()\nX_std = scaler.fit_transform(X)\n\n\nmetric = 'neg_root_mean_squared_error'\n\ncv_tw = cross_validate(lin_model, X_std, y_tw, cv=LeaveOneOut(), \n                       scoring=metric, return_train_score=True)\n\ncv_sr = cross_validate(lin_model, X_std, y_sr, cv=LeaveOneOut(), \n                       scoring=metric, return_train_score=True)","bcd902e3":"viz = FeatureImportances(lin_model, absolute=True, labels=inputs)\nviz.fit(X_std, y_tw)\nviz.show();","60ba867c":"viz = FeatureImportances(lin_model, absolute=True, labels=inputs)\nviz.fit(X_std, y_sr)\nviz.show();","f0ebb4de":"print(f'TW train score: {cv_tw[\"train_score\"].mean():.3f} +- {cv_tw[\"train_score\"].std():.3f}')\nprint(f'TW test score: {cv_tw[\"test_score\"].mean():.3f} +- {cv_tw[\"test_score\"].std():.3f}\\n')\nprint(f'SR train score: {cv_sr[\"train_score\"].mean():.3f} +- {cv_sr[\"train_score\"].std():.3f}')\nprint(f'SR test score: {cv_sr[\"test_score\"].mean():.3f} +- {cv_sr[\"test_score\"].std():.3f}')","51d7082f":"ts = np.linspace(0.1, 1.0, 8)\n\nviz1 = LearningCurve(lin_model, train_sizes=ts, cv=LeaveOneOut(), scoring=metric)\nviz1.fit(X, y_tw)\nviz1.show();","dedd392d":"viz2 = LearningCurve(lin_model, train_sizes=ts, cv=LeaveOneOut(), scoring=metric)\nviz2.fit(X, y_sr)\nviz2.show();","80c97774":"alphas = np.logspace(-5, 1, 400)\n\nmodel_ridge = RidgeCV(alphas=alphas) \nviz3 = AlphaSelection(model_ridge)\nviz3.fit(X_std, y_tw)\nviz3.show();","d7bad113":"ridge_model = RidgeCV(alphas=alphas)\nviz4 = AlphaSelection(ridge_model)\nviz4.fit(X_std, y_sr)\nviz4.show()","1203c8c6":"alphas = np.logspace(-5, -1, 400)\nlasso_model = LassoCV(alphas=alphas, cv=LeaveOneOut())\nviz5 = AlphaSelection(lasso_model)\nviz5.fit(X_std, y_tw)\nviz5.show();","700849dc":"alphas = np.logspace(-5, -1, 400)\nlasso_model = LassoCV(alphas=alphas, cv=LeaveOneOut())\nviz6 = AlphaSelection(lasso_model)\nviz6.fit(X_std, y_sr)\nviz6.show();","8c580bff":"lasso_model_tw = Lasso(alpha=0.009)\nridge_model_tw = Ridge(alpha=3.081)\nridge_model_sr = Ridge(alpha=3.190)\n\ncv_lasso_tw = cross_validate(lasso_model_tw, X_std, y_tw, cv=LeaveOneOut(), \n                       scoring=metric, return_train_score=True)\n\ncv_ridge_tw = cross_validate(ridge_model_tw, X_std, y_tw, cv=LeaveOneOut(), \n                       scoring=metric, return_train_score=True)\n\ncv_ridge_sr = cross_validate(ridge_model_sr, X_std, y_sr, cv=LeaveOneOut(), \n                       scoring=metric, return_train_score=True)","20d3f715":"print(f'TW Linear test score: {cv_tw[\"test_score\"].mean():.3f} +- {cv_tw[\"test_score\"].std():.3f}')\nprint(f'TW Lasso test score: {cv_lasso_tw[\"test_score\"].mean():.3f} +- {cv_lasso_tw[\"test_score\"].std():.3f}')\nprint(f'TW Ridge test score: {cv_ridge_tw[\"test_score\"].mean():.3f} +- {cv_ridge_tw[\"test_score\"].std():.3f}\\n')\nprint(f'SR Linear test score: {cv_sr[\"test_score\"].mean():.3f} +- {cv_sr[\"test_score\"].std():.3f}')\nprint(f'SR Ridge test score: {cv_ridge_sr[\"test_score\"].mean():.3f} +- {cv_ridge_sr[\"test_score\"].std():.3f}')","9306f5f0":"tw_model = ridge_model_tw\nsr_model = LinearRegression()\n\ntw_model.fit(X_std, y_tw)\nsr_model.fit(X_std, y_sr);\n\ny_pred_tw = tw_model.predict(X_std)\ny_pred_sr = sr_model.predict(X_std)\n\nrmse_tw = (mean_squared_error(y_pred_tw, df[outputs[0]]))**0.5\nrmse_sr = (mean_squared_error(y_pred_sr, df[outputs[1]]))**0.5\n\nr2_tw = (r2_score(df[outputs[0]], y_pred_tw))\nr2_sr = (r2_score(df[outputs[1]], y_pred_sr))\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\nax[0].set_title('Tool Wear')\nax[0].plot(df[outputs[0]], y_pred_tw, lw=0, marker='o', label=f'rmse = {rmse_tw:.3f}, r2 = {r2_tw:.3f}')\nax[0].plot([0, 200],[0, 200], ls = '--', color='r', label='Equivalent Line')\nax[0].set_ylim(df[outputs[0]].iloc[0], df[outputs[0]].iloc[-1])\nax[0].set_xlim(df[outputs[0]].iloc[0], df[outputs[0]].iloc[-1]);\nax[0].set_ylabel('Predicted Value')\nax[0].set_xlabel('True Value')\n\nax[1].plot(df[outputs[1]], y_pred_sr, lw=0, marker='o',  label=f'rmse = {rmse_sr:.3f}, r2 = {r2_sr:.3f}')\nax[1].set_title('Surface Roughness')\nax[1].plot([0, 2.5],[0, 2.5], ls = '--', color='r', label='Equivalent Line')\nax[1].set_ylim(df[outputs[1]].min(), df[outputs[1]].max())\nax[1].set_xlim(df[outputs[1]].min(), df[outputs[1]].max());\nax[1].set_ylabel('Predicted Value')\nax[1].set_xlabel('True Value')\n\n\nax[0].grid(False)\nax[1].grid(False)\nax[0].legend()\nax[1].legend()\n\nfig.tight_layout();","5ef73f74":"Let's see how is our data:","ee3b28a6":"Now, observe that the first outlier have a high FR and moderate CS, therefore, it seems reasonable that it will have a good surface quality even with high DOC. For the other two outliers, they have small FR, although having high CS and DOC.","c1872ea6":"## Load Dataset","9324e2a3":"## Machine Learning: Linear Regression","bf9ba782":"Given the small number of instances and features avaliable, let's use simpler models which are more versatile and robust. For this datasets, we are going to use Linear, Rigde and Lasso Regression.","c4bb8c84":"## Machine Learning: Ridge and Lasso Regression","6288a800":"## Conclusion","01eb8260":"More improvements can be done for linear models, like the log transformation, the combination of ridge and lasso (elastic net), and even polynomial transformations, but I recommend gathering more data on the problem so we can understand more about it. Also, it should present the materials that the workpiece and tools are made of, this causes a lot of impact on the dependent variables (Tool Wear and Surface Roughness).","762d7a93":"Ridge Regression slighty changed the Tool Wear model, reducing the bias and variance of the test score, but does not cause any significant improvement in Surface Roughness model.","3d381bbe":"The output increases and decreases according to our conclusions. We can spot outliers in the Sufarce Roughness output though, but we can't explain them with visualization tools help, or can we? Of course we can, with very little code:","47cb707d":"Now, let's train a Linear Regression Model. To evaluate it, we are going to use de Leave-One-Out Cross Validation, since the dataset is small. The standardzation of the input variables won't improve the model, but will give us insight about the influence of each feature and their influence and are necessary for regularization models.","cb37bee0":"Now, the L1 Regression, also know as Lasso.","c22587ad":"To improve the Linear Regression, we are going to apply regularization,  but, for that, we have to choose the best hyperparameters, in others words, which alphas gives us the lowest error. First, the L2 regularization, also know as Ridge (Note: the metric used by yellowbrick is MSE).","fc33570c":"- As expected, all control variables are independent of one another. \n- All variables are positive correlated with tool wear. \n- Surface Roughness is negative correlated with CS and FR, increasing them will increase surface quality.\n- In both cases, DOC seems to be predominant. \n- Both Outputs are correlated with one another, the more you wear your tool, the better your workpiece will be.","ab28c56c":"## Introduction","1a4ba9ba":"Based on this heatmap, we can have the following conclusions:","fe8fea3b":"What we have is a dataset with a combination of different values of variables of control that were repetead twice so the data could be more reliable. let's concatenate both datasets, dropping the column \"Exp no.\" in the process.","4d43a1b8":"The learning curve is different too. One can see and conclude that more dataset isn't necessary, since the training score is very close to the test score, but it have to approach with more caution: The variance is not stable. Therefore the model doesn't have a reliable error margin to make predictions.","29f76afd":"The tool wear isn't normally distributed, most likely due to the restricitions of the experiment, it doesn't make sense evaluate combinations that will wear too much of your tool. A log transformation on the output might improve the linear model performance. In this notebook, we are not testing it, just show how log transformation reduce skewness.","f3b6e5eb":"Lasso Regression doesn't seems to improve the Surface Roughness model, therefore, we are not considering it.","40c4bd32":"According to our previous analysis, DOC is predominant on both outputs, followed by CS and FR.","4aef241f":"## EDA (Exploratory Data Analysis)","b0fac7e3":"This results is quite unusual, since the test score is higher than the train score, this is due to the fact that is a Leave-One-Out Cross Validation, which every instance is a testing set by itself, resulting in lower bias. This also explains the higher variance, since each instance is evaluated separately.","f34ee8d4":"## Import Libraries "}}