{"cell_type":{"96d1ce96":"code","7b18380d":"code","d8c12066":"code","ebba79a2":"code","3954a01a":"code","5bde32bb":"code","f57e2eb2":"code","69d1482c":"code","c15705f6":"code","42d9dc85":"code","771fd2b6":"code","1f150881":"code","c351fa17":"code","d211beee":"code","8db4d4ce":"code","463a046f":"code","efe57e96":"code","a755fccb":"code","f672bafb":"code","350c2cf6":"code","60a225d4":"code","7ffa2ee9":"code","a0767db5":"code","81b3aa39":"code","9b10f644":"code","a3ddf3ae":"code","8a927193":"code","6170943f":"code","64100c1a":"code","b636e497":"code","8837c5cb":"code","dd251364":"code","b3f85898":"code","d11fe301":"code","e3b58db0":"code","ffc08b23":"code","8dbd1699":"code","51c0ab1e":"code","1b848394":"code","f2de0c2c":"code","09f57280":"code","b73937b9":"code","58319e50":"code","c15e9f8a":"code","a248c306":"code","60071a72":"code","edbe58a6":"code","40d9bd0a":"code","6b608f9a":"markdown","f7ba3ee0":"markdown","99b4e4e2":"markdown","783ba0e6":"markdown","369ab3a9":"markdown","f361ba16":"markdown","c5e73a69":"markdown","d4e99224":"markdown","bc9e00f2":"markdown","bb76c82c":"markdown","e5baedd7":"markdown","cb05dcb0":"markdown","b3cfbbd4":"markdown","e5ee29d4":"markdown","532e55d3":"markdown","53ab2bdf":"markdown","eec12128":"markdown","b0a59d70":"markdown","b9a13667":"markdown","295978dd":"markdown","162ececb":"markdown","880ea7f9":"markdown","a4e86ee9":"markdown","862e9172":"markdown","912fe92a":"markdown","8e8f30cb":"markdown","a7d04603":"markdown","8074ff47":"markdown","9bf10a21":"markdown","8c446997":"markdown","f228c236":"markdown"},"source":{"96d1ce96":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport cv2 \nimport copy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nimport glob\nimport os\nfrom tqdm import tqdm \nimport matplotlib.image as mpimg\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport itertools","7b18380d":"dataset = pd.read_csv(\"\/kaggle\/input\/devanagari-character-set\/data.csv\")","d8c12066":"dataset.head()","ebba79a2":"dataset.shape","3954a01a":"def read_data():\n\n    data_set_full = [] \n    for folder_path in glob.glob('\/kaggle\/input\/devanagari-character-set\/Images\/Images\/*'):\n\n        class_name = folder_path.split('\/')[-1]\n        print(class_name)\n\n        for img in tqdm(os.listdir(folder_path)): \n            path = os.path.join(folder_path, img)\n            img = cv2.imread(path)\n            data_set_full.append([np.array(img), class_name])\n            #print(data_set_full)\n            #break\n        \n    return data_set_full","5bde32bb":"## Commented due to long run time once after the data had been generated.\n##full_data = read_data()","f57e2eb2":"##len(full_data)","69d1482c":"plt.figure(figsize = (10,20))\n_ = sns.countplot(x=None, y=dataset.character)","c15705f6":"dataset.drop(['character'], axis =1).iloc[0].max()","42d9dc85":"img = (dataset.drop(['character'],axis=1).iloc[0]).to_numpy().reshape(32,32)\n_ = plt.imshow(img, cmap = 'gray')","771fd2b6":"#import Image\n\ndef is_grey_scale(img_path):\n    img = Image.open(img_path).convert('RGB')\n    w,h = img.size\n    for i in range(w):\n        for j in range(h):\n            r,g,b = img.getpixel((i,j))\n            if r != g != b: return False\n    return True\n\n\n### Basically, check every pixel to see if it is grayscale (R == G == B)","1f150881":"ones_array = np.ones([100, 100, 3], dtype=np.uint8)\n_ = plt.imshow(ones_array)","c351fa17":"red_array = copy.deepcopy(ones_array)\nred_array[:,:,0] = 255\nred_array[:,:,1] = 0\nred_array[:,:,2] = 0\n\n_ = plt.imshow(red_array)","d211beee":"any_gray_array = copy.deepcopy(ones_array)\nany_gray_array[:,:,0] = 200\nany_gray_array[:,:,1] = 200\nany_gray_array[:,:,2] = 200\n\n_ = plt.imshow(any_gray_array)","8db4d4ce":"any_gray_array = copy.deepcopy(ones_array)\nany_gray_array[:,:,0] = 150\nany_gray_array[:,:,1] = 150\nany_gray_array[:,:,2] = 150\n\n_ = plt.imshow(any_gray_array)","463a046f":"any_gray_array = copy.deepcopy(ones_array)\nany_gray_array[:,:,0] = 100\nany_gray_array[:,:,1] = 100\nany_gray_array[:,:,2] = 100\n\n_ = plt.imshow(any_gray_array)","efe57e96":"x = dataset.drop(['character'],axis = 1)\ny_text = dataset.character","a755fccb":"x.shape","f672bafb":"from sklearn.preprocessing import LabelBinarizer\nbinencoder = LabelBinarizer()\ny = binencoder.fit_transform(y_text)","350c2cf6":"y[0]","60a225d4":"x = x.values.reshape(x.shape[0],32,32,1)","7ffa2ee9":"x.shape","a0767db5":"print(x.max())\nprint(x.mean())\nprint(x.sum())","81b3aa39":"x = x\/255.0","9b10f644":"print(x.max())\nprint(x.mean())\nprint(x.sum())","a3ddf3ae":"x.shape","8a927193":"y.shape","6170943f":"(unique, counts) = np.unique(y, return_counts=True, axis = 0)\nfrequencies = np.asarray((binencoder.inverse_transform(unique), counts)).T","64100c1a":"frequencies","b636e497":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.33, random_state=42, stratify = y)","8837c5cb":"x_train.shape","dd251364":"x_test.shape","b3f85898":"y_train.shape","d11fe301":"y_test.shape","e3b58db0":"(unique, counts) = np.unique(y_train, return_counts=True, axis = 0)\nfrequencies = np.asarray((binencoder.inverse_transform(unique), counts)).T","ffc08b23":"frequencies","8dbd1699":"(unique, counts) = np.unique(y_test, return_counts=True, axis = 0)\nfrequencies = np.asarray((binencoder.inverse_transform(unique), counts)).T","51c0ab1e":"frequencies","1b848394":"from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau","f2de0c2c":"model = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (32,32,1)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(46, activation = \"softmax\"))","09f57280":"# Define the optimizer\noptimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)","b73937b9":"# Compile the model\nmodel.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","58319e50":"model.summary()","c15e9f8a":"# Set a learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","a248c306":"epochs = 7\nbatch_size = 86","60071a72":"history = model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, \n       validation_data = (x_test, y_test), verbose = 2)","edbe58a6":"scores = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","40d9bd0a":"# Plot the loss and accuracy curves for training and validation \nfig, ax = plt.subplots(2,1)\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","6b608f9a":"## Reading Images","f7ba3ee0":"# Identifying Grayscale Images","99b4e4e2":"## A grayscale image is formed when the red, blue and green component of the image have the same value for each pixel. So that means we should have a multidimensional array with values like m x n x 3, 3 for representing the 3 channels. But here we can see from the dataset that each row represents a single image and has only one channel. Also the pixels have values other than 0 and 1 so its not binary. So does that mean that this image format is unknown to us and we can't identify it?\n## No, the image format is very well known to us by now and its none other than grayscale. Since the red, blue and green channels of all grayscale images have the same value at each pixel, it is smart to store the image in a single 2-D array rather than having 3 2-D arrays having the same values. Though when the image is read, the final result is a merge of the 3 channels (having same values of pixels) basically a grayscale image.","783ba0e6":"### Before going to the theory let's look at a plain old pythonic way of checking whether an image is grayscale","369ab3a9":"# Image Normalisation","f361ba16":"# Class Distribution","c5e73a69":"1. Introduction\n2. Reading Images from Folder with class name\n3. Class Distribution\n4. Image Characteristics\n5. Identifying Grayscale Images\n6. Splitting Input and Target Data\n7. Target Label Binarization\n8. Input Image Reshaping\n9. Image Normalisation\n10. Splitting into Training and Testing set\n11. Stratified split\n12. Split Frequency check\n13. Modelling\n14. Evaluation","d4e99224":"### Stratified split","bc9e00f2":"## Normalization is a process that changes the range of pixel intensity values","bb76c82c":"### Frequency check","e5baedd7":"### Shape check","cb05dcb0":"## Updating With More Details Very Soon","b3cfbbd4":"# Reading Images from Folder with class name","e5ee29d4":"# Modelling","532e55d3":"## Theory","53ab2bdf":"# Evaluation","eec12128":"### This is a grayscale image and I have used cmap = 'gray' to for best suiting representation of the image. Also notice the shades of gray in the image. Had it been a binary image there would be just black and white colors. But how did I identify its type?","b0a59d70":"### Every class is equally distributed, very well. We won't encounter the problem of class imbalance like [here](https:\/\/www.kaggle.com\/shrutimechlearn\/pokemon-classification-and-smote-for-imbalance)","b9a13667":"### Now let's come back to the modelling pipeline","295978dd":"### Thanks! Do upvote if you found it helpful. ","162ececb":"### So the train data has 1340 examples and test has 660 examples of each of the 46 classes.","880ea7f9":"# Image Characteristics","a4e86ee9":"### Now to make any shade of gray","862e9172":"## Target Label Binarization","912fe92a":"# Splitting Input and Target Data","8e8f30cb":"# Introduction\n","a7d04603":"## A few points:\n\n#### 1. Firstly the dataset originally has images stored in a flattened manner. Two dimensional array has been flattened into a one dimension array.\n#### 2. I have come across notebooks that take the given 1-D array as input to build models. I appreciate the approach but believe that atleast some crucial information about the spacial arrangement of pixels is lost in doing so. Hence my modelling is based on 2-D image input.\n#### 3. Though I have used the given csv from the dataset, I have added a portion briefly on how to read images from specific folders where the folder name is the class name. I hope this will act as a baseline for many others who are required to take input in this manner for other competitions and datasets as well.","8074ff47":"# Start Note:\n### I would like to thank the data provider for providing this hard to get by dataset.\nSource - https:\/\/web.archive.org\/web\/20160105230017\/http:\/\/cvresearchnepal.com\/wordpress\/dhcd\/","9bf10a21":"## Splitting into Training and Testing set","8c446997":"## Practical Example","f228c236":"## Input Image Reshaping"}}