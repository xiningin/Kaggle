{"cell_type":{"7af92e6d":"code","1b2bc729":"code","69aba9cb":"code","eda1b173":"code","a3ba2744":"code","75b5db63":"code","b670bbaf":"code","f1d19413":"code","594e8928":"code","ea46a89b":"code","125294f7":"code","15c56c03":"code","86d618c5":"code","03d09351":"code","6ef42cbc":"code","0be8a53b":"code","d0bbda26":"code","d71dd2a4":"code","b9cba240":"code","1ce613b2":"code","bd577e44":"code","01536686":"code","ba62dd59":"code","3aa07d70":"code","0d590018":"code","f35474c7":"code","b1f73d4f":"code","c9b643f9":"code","7ca95721":"code","6e1d1c5b":"code","3ccc7247":"code","eee1085f":"code","28b00a34":"code","c217c41a":"code","af517087":"code","3b23c695":"code","1cf05fc6":"code","9c6d2510":"code","743319af":"code","499e6613":"code","986f4e26":"code","673245a1":"code","4aeb2f7d":"code","7355125a":"code","e61d970d":"code","7b368500":"code","bb727ec4":"code","e82ad456":"code","6bc2ee94":"code","0a50c14d":"code","de6c5b21":"code","f59edf3f":"code","0dae694d":"code","e5e3d9ba":"code","38319bcf":"code","1055a62e":"code","d12989d5":"code","f18b7de8":"code","3d6827c1":"code","24fc5ab3":"code","6fa03757":"code","d8f16b15":"code","26052077":"code","3aee920d":"code","2efb5d22":"code","06aafe8a":"code","8307065f":"code","2c9898c7":"code","2a249b94":"code","99acaaa0":"code","51b488ee":"code","926e6027":"code","5fc85803":"code","70eb5a07":"code","26293013":"code","de7a9999":"markdown","582449e4":"markdown","a9807b9f":"markdown"},"source":{"7af92e6d":"import torch\nimport numpy as np\nimport torchvision\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom torch import nn\nfrom torch.nn import functional as F\nimport torch.optim as optim\nimport pandas as pd\nfrom torch.utils.data import random_split\nfrom torchvision.utils import make_grid\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline","1b2bc729":"# torch.Tensor.ndim = property(lambda x: len(x.shape))","69aba9cb":"!pwd","eda1b173":"!unzip digit-recognizer.zip","a3ba2744":"# path = \"\/content\/Mnist\"\ntrain_path = \"..\/input\/digit-recognizer\/train.csv\"\ntest_path = \"..\/input\/digit-recognizer\/test.csv\"","75b5db63":"train = pd.read_csv(train_path)\ntest = pd.read_csv(test_path)","b670bbaf":"# val_size = 8000\n# train_size = len(train) - val_size\n# train_data, val_data = random_split(train, [train_size, val_size])","f1d19413":"Y = train[\"label\"]\nX = train.drop(labels = [\"label\"],axis = 1)","594e8928":"#Splitting\nX_train, X_val, Y_train, Y_val = train_test_split(X,Y, test_size=0.2)\nX_train.shape, Y_train.shape, X_val.shape, Y_val.shape","ea46a89b":"train_size = X_train.shape[0]\nval_size = X_val.shape[0]\nprint(\"Training size {}, Validation size {} \".format(train_size, val_size))","125294f7":"#Normalization \nX_train = X_train \/ 255.0\nX_val = X_val \/ 255.0\ntest = test \/ 255.0","15c56c03":"X_train = np.array(X_train, np.float32)\ntest = np.array(test, np.float32)\nY_train = np.array(Y_train, np.long)\nX_val = np.array(X_val, np.float32)\nY_val = np.array(Y_val, np.long)\nprint(X_train.shape)\nprint(test.shape)\nprint(X_val.shape)","86d618c5":"plt.imshow(X_train[10].reshape(28, 28), cmap='gray')\nprint(Y_train[10])","03d09351":"type(X_train)","6ef42cbc":"X_train = torch.from_numpy(X_train)\nY_train = torch.from_numpy(Y_train)\nX_val = torch.from_numpy(X_val)\nY_val = torch.from_numpy(Y_val)\ntest = torch.from_numpy(test)","0be8a53b":"# transform_img = torchvision.transforms.Compose([\n# #                                           torchvision.transforms.Resize((224, 224)),\n#                                           torchvision.transforms.ToTensor(),\n#                                           torchvision.transforms.Normalize(mean=[0.485], std=[0.229])\n#                                           ])","d0bbda26":"# print(transform_img)","d71dd2a4":"# data = torchvision.datasets.MNIST(path, train=True, transform=transform_img, target_transform=None, download=True)","b9cba240":"train_data = torch.utils.data.TensorDataset(X_train, Y_train)\nval_data = torch.utils.data.TensorDataset(X_val, Y_val)","1ce613b2":"# inputs = data.data\n# labels = data.targets\n# print(inputs.shape)\n\n\n# print(labels[2])\n# plt.imshow(inputs[2].numpy())","bd577e44":"# dir(data)","01536686":"batch = 32","ba62dd59":"train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch, shuffle=True, num_workers=4)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=2*batch, shuffle=True, num_workers=4)","3aa07d70":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))","0d590018":"class MLP(torch.nn.Module):\n    def __init__(self):\n        super(MLP, self).__init__()\n        self.fc1 = torch.nn.Linear(784, 32)\n        # nn.ReLU(),\n        self.fc2 = torch.nn.Linear(32, 16)\n        # nn.ReLU(),\n        self.fc3 = torch.nn.Linear(16, 10)\n        # nn.Softmax()\n#         self.fc1 = torch.nn.Linear(784, 10)\n          \n    def forward(self, x):\n        \n        x = x.view(x.size(0), -1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        x = F.relu(x)\n        x = self.fc3(x)\n#         x = self.fc1(x)\n        return x\n\n    def training_step(self, batch):\n        images, labels = batch\n        out = self(images)\n        loss = F.cross_entropy(out, labels)\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch\n        preds = self(images)\n        loss = F.cross_entropy(preds, labels)\n        acc = accuracy(preds, labels)\n        return {'val_loss': loss, 'val_acc': acc}\n    \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))","f35474c7":"model = MLP()","b1f73d4f":"print(model)","c9b643f9":"for para in model.parameters():\n  print(para.shape)","7ca95721":"# loss_fn = F.cross_entropy\n# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)","6e1d1c5b":"if torch.cuda.is_available():\n  device = torch.device('cuda')\nelse:\n  device = torch.device('cpu')\nprint(device)","3ccc7247":"def to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)","eee1085f":"class DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","28b00a34":"train_loader = DeviceDataLoader(train_loader, device)\nval_loader = DeviceDataLoader(val_loader, device)","c217c41a":"# model.to(device=device)","af517087":"# images, labels = next(iter(train_loader))","3b23c695":"def evaluate(model, val_loader):\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)","1cf05fc6":"def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    \n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase \n        for batch in train_loader:\n            loss = model.training_step(batch)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","9c6d2510":"to_device(model, device)","743319af":"history = [evaluate(model, val_loader)]\nhistory","499e6613":"history += fit(5, 0.5, model, train_loader, val_loader)","986f4e26":"history += fit(5, 0.1, model, train_loader, val_loader)","673245a1":"losses = [x['val_loss'] for x in history]\nplt.plot(losses, '-x')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.title('Loss vs. No. of epochs');","4aeb2f7d":"accuracies = [x['val_acc'] for x in history]\nplt.plot(accuracies, '-x')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.title('Accuracy vs. No. of epochs');","7355125a":"# test_data = torchvision.datasets.MNIST(path, train=False, transform=transform_img, target_transform=None, download=True)","e61d970d":"# inputs = test_data.test_data\n# labels = test_data.test_labels\n# print(inputs.shape)\n\n\n# print(labels[1])\n# plt.imshow(inputs[1].numpy())","7b368500":"test_data = torch.utils.data.TensorDataset(test)","bb727ec4":"test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=False, num_workers=1)","e82ad456":"train_loader = DeviceDataLoader(train_loader, device)","6bc2ee94":"model.eval()","0a50c14d":"def predict_image(img, model):\n    xb = to_device(img.unsqueeze(0), device)\n    yb = model(xb)\n    _, preds  = torch.max(yb, dim=1)\n    return preds[0].item()","de6c5b21":"img = test_data[1]\nplt.imshow(img[0].reshape(28, 28), cmap='gray')\nprint('Predicted: ', predict_image(img[0], model))","f59edf3f":"img = test_data[1]\nprint(type(img))\nprint(type(img[0]))","0dae694d":"# evaluate(model, test_loader)","e5e3d9ba":"# correct_ans = 0","38319bcf":"# def correct_pred_count(pred, answer):\n#     pred = torch.argmax(pred, dim=1)\n#     correct_count_vector = (pred.data == answer.data)\n#     correct_count = correct_count_vector.sum()\n#     return correct_count","1055a62e":"# for images, labels in test_loader:\n    \n#     images = images.view(images.size(0), -1)\n\n#     images = images.to(device=device)\n#     labels = labels.to(device=device)\n\n#     preds = model(images)\n    \n#     correct_ans += correct_pred_count(preds, labels)","d12989d5":"# accuracy = (correct_ans \/  10000.0)","f18b7de8":"# accuracy","3d6827c1":"# import csv","24fc5ab3":"# csv_path = \"mnist.csv\"\n# file = open(csv_path, 'w')\n# writer = csv.writer(file)\n# writer.writerow([\"ImageId\", \"Label\"])","6fa03757":"prediction = []\nfor i, images in enumerate(test_loader, 1):\n  images = images[0]\n#   images = images.to(device=device)\n\n  preds = model(images)\n#   print(i)\n  preds = torch.argmax(preds, dim=1)\n  prediction.append(preds.item())\n#   writer.writerow([i, preds.item()])","d8f16b15":"len(prediction)","26052077":"path = \"CNN_submission2.csv\"","3aee920d":"sample_sub = pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')","2efb5d22":"sample_sub = pd.DataFrame({\"ImageId\": list(range(1,len(prediction)+1)),\n                         \"Label\": prediction})\nsample_sub.to_csv('CNN_submission2.csv', index=False)\nsample_sub.head()","06aafe8a":"file = [row.strip().split() for row in open(path)]","8307065f":"len(file)","2c9898c7":"file[len(file) - 1]","2a249b94":"len(test_loader)","99acaaa0":"saved_weights_fname='MNIST-feedforward.pth'","51b488ee":"torch.save(model.state_dict(), saved_weights_fname)","926e6027":"!pip install jovian --upgrade --quiet","5fc85803":"import jovian","70eb5a07":"project_name = \"MNISt Feed forward\"","26293013":"jovian.commit(project=project_name, environment=None, outputs=[saved_weights_fname])","de7a9999":"## Save and upload","582449e4":"## Training the model","a9807b9f":"## Prediction on Samples"}}