{"cell_type":{"0cf2ca1f":"code","574948b4":"code","e94e83f3":"code","914f42e9":"code","005c381f":"code","63d49444":"code","db6a3d5b":"code","af34557a":"code","46bc33bf":"code","63b7641e":"code","05dc175a":"code","4d5668c6":"code","9751fab3":"code","4d711384":"code","e91cea19":"code","e6674f2b":"code","3227f738":"code","fe2ed01a":"code","eabd2ace":"code","b9c53461":"code","ae177825":"code","aaf520ad":"code","7f224ce5":"code","b14a910e":"code","4c730616":"code","54237d8b":"code","4a0ab7fa":"code","f2f0bc8d":"code","95bca5a3":"markdown","f1f55678":"markdown","f6664ac8":"markdown","001f5619":"markdown","be99f25f":"markdown","6f99e251":"markdown","765b932c":"markdown","691d7e2a":"markdown","bd882ab3":"markdown","45c735ed":"markdown","cbf84e5c":"markdown","81427d23":"markdown","b53e85da":"markdown","bb2a0cc6":"markdown","f211c836":"markdown","aa800f76":"markdown","b1382f56":"markdown","a1bce1ae":"markdown","0136e5b1":"markdown","cec7b776":"markdown","02b74f0b":"markdown","d110a6d5":"markdown","7b30accd":"markdown","e69c5d69":"markdown","3c199c81":"markdown","0d7b952a":"markdown","0385dde8":"markdown"},"source":{"0cf2ca1f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.express as px\ninit_notebook_mode(connected=True)\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","574948b4":"pokemons = pd.read_csv('\/kaggle\/input\/pokemon-images-and-types\/pokemon.csv')\nnumbers = []\nfor i in range(1,pokemons.shape[0]+1):\n    numbers.append(i)\npokemons['pkn'] = numbers\nIMG_DIR = '\/kaggle\/input\/pokemon-images-dataset\/pokemon\/pokemon'\nfrom os import listdir\nfrom os.path import isfile, join\nonlyfiles = [f for f in listdir(IMG_DIR) if isfile(join(IMG_DIR, f))]","e94e83f3":"import re\ndataframe_img = pd.DataFrame([])\nimages = []\npokemon_number = []\nfor img in onlyfiles:\n    if not re.search('-', img):\n        pkn = img.split('.')\n        n = re.sub(\"[^0-9]\", \"\", pkn[0])\n        path = IMG_DIR +'\/' +str(img)\n        images.append(path)\n        pokemon_number.append(n)\ndataframe_img['images'] = images\ndataframe_img['pkn'] = pokemon_number\ndataframe_img['pkn'] = dataframe_img['pkn'].astype(int)\ndataframe_img['pkn'] = dataframe_img['pkn'].astype(int)\nresult = pokemons.merge(dataframe_img, left_on='pkn', right_on='pkn')\nresult.head()","914f42e9":"select = ['Water', 'Fire']\nresult = result[result['Type1'].isin(select)]\nfig = go.Figure()\n\nfig.add_trace(go.Bar(x=[result['Type1'].value_counts().index[0]],\n                     y=[result['Type1'].value_counts()[0]],\n                     marker_color='blue',\n                     name='water'\n                     ))\n\nfig.add_trace(go.Bar(x=[result['Type1'].value_counts().index[1]],\n                     y=[result['Type1'].value_counts()[1]],\n                     marker_color='red',\n                     name='fire'\n                    ))\nfig.update_layout({\n        'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n        'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n        'title': 'Pokemon Distribution',\n        'width': 500, \n        'height': 400\n})\nfig.show()","005c381f":"import os\nfrom shutil import copyfile\nos.mkdir('train\/')\nos.mkdir('test\/')\nos.mkdir('val\/')\nfor class_ in result['Type1'].unique():\n    os.mkdir('train\/'+str(class_)+'\/')\n    os.mkdir('test\/'+str(class_)+'\/')\n    os.mkdir('val\/'+str(class_)+'\/')","63d49444":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    result, result['Type1'],test_size=0.33, stratify=result['Type1'])\n\nX_test, X_val, y_test, y_val = train_test_split(\n    X_test, y_test, test_size=0.33,stratify=y_test)","db6a3d5b":"from shutil import copyfile, copy2\n\nfor image,type_  in zip(X_train['images'], y_train):\n    copy2(image, 'train\/'+type_)\n\nfor image,type_ in zip(X_test['images'], y_test):\n    copy2(image, 'test\/'+type_)\n    \nfor image,type_ in zip(X_val['images'], y_val):\n    copy2(image, 'val\/'+type_)","af34557a":"from keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator()\n\ntrain = datagen.flow_from_directory('train\/')\ntest = datagen.flow_from_directory('test\/')\nval = datagen.flow_from_directory('val\/')\n","46bc33bf":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization, Lambda\nfrom keras.preprocessing.image import ImageDataGenerator\n\ndef build():\n    model = Sequential()\n    IMAGE_WIDTH = 256\n    IMAGE_HEIGHT = 256\n    IMAGE_CHANNELS = 3\n    model.add(Lambda(lambda x: x, input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)))\n    model.add(Conv2D(32, (2, 2), activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(3, 3)))\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(64, (2, 2), activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(3, 3)))\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(128, (2, 2), activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(3, 3)))\n    model.add(Dropout(0.25))\n\n    model.add(Flatten())\n    model.add(Dense(512, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    model.add(Dense(2, activation='softmax')) \n\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n\n    model.summary()\n    return model\nmodel = build()\nhistory = model.fit_generator(train, epochs=30, validation_data=val)","63b7641e":"predict = model.predict_generator(test)","05dc175a":"\nfig = go.Figure()\nepochs = []\nfor i in range(len(history.history['acc'])):\n    epochs.append(i)\nfig.add_trace(go.Scatter(x=epochs,y=history.history['acc'], mode='lines',name='train'))\nfig.add_trace(go.Scatter(x=epochs,y=history.history['val_acc'], mode='lines',name='val'))\nfig.update_layout({\n        'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n        'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n        'width': 500, \n        'height': 400\n\n})\nfig.show()","4d5668c6":"from sklearn.metrics import classification_report\npredict_frame = pd.DataFrame([])\npredict_frame['category'] = np.argmax(predict, axis=-1)\nlabels = dict((v,k) for k,v in val.class_indices.items())\npredict_frame['category'] = predict_frame['category'].replace(labels)\nprint(classification_report(y_test, predict_frame['category']))","9751fab3":"def show_wrong_classification(y_test, predict, result):\n    tmp = result[result.index.isin(y_test.index)]\n    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 20))\n    i=0\n    for imag, true, pred in zip(tmp['images'], tmp['Type1'], predict):\n        if true!=pred:\n            if i <3:\n                img = Image.open(imag)\n                fig = plt.figure()\n                ax[i].imshow(img)\n                ax[i].set_title(str(pred))\n                i+=1","4d711384":"from PIL import Image\nshow_wrong_classification(y_test, predict_frame['category'], result)","e91cea19":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(2,3,figsize=(15,10))\nk =0\nimport cv2\nlist_b =[]\nlist_r = []\nlist_g = []\nfrom tqdm import tqdm\nfor type_ in tqdm(result['Type1'].unique()):\n    tmp = result[result['Type1']==type_]\n    for img in tmp['images']:\n        img = cv2.imread(img)\n        b, g, r = cv2.split(img)\n        color = 'blue'\n        for i in b:\n            for j in i:\n                if j != 0:\n                    list_b.append(j)\n        color = 'green'\n        for i in g:\n            for j in i:\n                if j != 0:\n                    list_g.append(j)\n        color = 'red'\n        for i in r:\n            for j in i:\n                if j != 0:\n                    list_r.append(j)\n    sns.distplot(list_g, ax=axes[k, 0], color='g')\n    sns.distplot(list_b, ax=axes[k, 1], color='b')\n    sns.distplot(list_r, ax=axes[k, 2], color='r')\n    axes[k, 0].set_title('Pokemon type color channel ' + type_)\n    if type_ =='Fire':\n        list_g_f = list_g\n        list_b_f = list_b\n        list_r_f = list_r\n    else:\n        list_g_w = list_g\n        list_b_w = list_b\n        list_r_w = list_r\n    list_b =[]\n    list_r = []\n    list_g = []\n    \n    k += 1","e6674f2b":"import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\ng_fire = np.std(np.array(list_g_f), axis=0)\nr_fire = np.std(np.array(list_r_f), axis=0)\nb_fire = np.std(np.array(list_b_f), axis=0)\n\n\ng_water = np.std(np.array(list_g_w), axis=0)\nr_water = np.std(np.array(list_r_w), axis=0)\nb_water = np.std(np.array(list_b_w), axis=0)\n\nfig = make_subplots(rows=1, cols=2, subplot_titles=(\"Fire std channel\",\"Water std channel\"))\nfig.add_trace(go.Scatter(\n    y=[b_fire, g_fire, g_fire],\n    x=['blue', 'green', 'red'],\n    mode='markers',\n    marker=dict(size=[b_fire, g_fire, r_fire],\n                color=['blue', 'green', 'red'],\n                showscale=True)\n), row=1, col=1)\n\nfig.add_trace(go.Scatter(\n    y=[b_water, g_water, r_water],\n    x=['blue', 'green', 'red'],\n    mode='markers',\n    marker=dict(size=[b_water, g_water, r_water],\n                color=['blue', 'green', 'red'],\n                showscale=True)\n), row=1, col=2)\nfig.update_layout(showlegend=False)\nfig.update_layout({\n        'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n        'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n        'width': 850, \n        'height': 400\n\n})\nfig.show()","3227f738":"water = result[result['Type1']=='Water']\nstds = []\nvalues = []\nfor image in water['images']:\n    img = cv2.imread(image)\n    b, g, r = cv2.split(img)\n    for i in r:\n        for j in i:\n            if j != 0:\n                stds.append(j)       \n    std = np.mean(np.array(stds), axis=0)\n    values.append(std)\n    stds = []\nwater['stds'] = values","fe2ed01a":"water = water.sort_values(by='stds', ascending=False)","eabd2ace":"i = 0\nj = 0\nfig, ax = plt.subplots(nrows=2, ncols=3, figsize=(20, 20))\nfor imag in water['images']:\n    if i == 1 and j ==3:\n        break\n    if j > 2:\n        i =1\n        j = 0\n    img = Image.open(imag)\n    fig = plt.figure()\n    ax[i][j].imshow(img)\n    j+=1","b9c53461":"fire = result[result['Type1']=='Fire']\nstds = []\nvalues = []\nfor image in fire['images']:\n    img = cv2.imread(image)\n    b, g, r = cv2.split(img)\n    for i in b:\n        for j in i:\n            if j != 0:\n                stds.append(j)       \n    std = np.mean(np.array(stds), axis=0)\n    values.append(std)\n    stds = []\nfire['stds'] = values\nfire = fire.sort_values(by='stds', ascending=False)\ni = 0\nj = 0\nfig, ax = plt.subplots(nrows=2, ncols=3, figsize=(20, 20))\nfor imag in fire['images']:\n    if i == 1 and j ==3:\n        break\n    if j > 2:\n        i =1\n        j = 0\n    img = Image.open(imag)\n    fig = plt.figure()\n    ax[i][j].imshow(img)\n    j+=1","ae177825":"# example of horizontal shift image augmentation\nfrom numpy import expand_dims\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom matplotlib import pyplot\nimg = load_img(water['images'][170])\ndata = img_to_array(img)\nsamples = expand_dims(data, 0)\ndatagen = ImageDataGenerator(brightness_range=[0.2,1.5])\nit = datagen.flow(samples, batch_size=1)\nfig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 20))\nfor i in range(3):\n    fig = plt.figure()\n    batch = it.next()\n    image = batch[0].astype('uint8')\n    ax[i].imshow(image)\npyplot.show()","aaf520ad":"img = load_img(fire['images'][156])\ndata = img_to_array(img)\nsamples = expand_dims(data, 0)\ndatagen = ImageDataGenerator(zoom_range=[0.5, 1.0])\nit = datagen.flow(samples, batch_size=1)\nfig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 20))\nfor i in range(3):\n    fig = plt.figure()\n    batch = it.next()\n    image = batch[0].astype('uint8')\n    ax[i].imshow(image)\npyplot.show()","7f224ce5":"img = load_img(water['images'][118])\ndata = img_to_array(img)\nsamples = expand_dims(data, 0)\ndatagen = ImageDataGenerator(rotation_range=35)\nit = datagen.flow(samples, batch_size=1)\nfig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 20))\nfor i in range(3):\n    fig = plt.figure()\n    batch = it.next()\n    image = batch[0].astype('uint8')\n    ax[i].imshow(image)\npyplot.show()","b14a910e":"datagentrain = ImageDataGenerator(rotation_range=35, \n                                 zoom_range=[0.5, 1.0], \n                                 brightness_range=[0.2,1.5])\n\ndatagen = ImageDataGenerator()\n\ntrain = datagentrain.flow_from_directory('train\/')\ntest = datagen.flow_from_directory('test\/')\nval = datagen.flow_from_directory('val\/')","4c730616":"model = build()\nhistory = model.fit_generator(train, epochs=30, validation_data=val)","54237d8b":"fig = go.Figure()\nepochs = []\nfor i in range(len(history.history['acc'])):\n    epochs.append(i)\nfig.add_trace(go.Scatter(x=epochs,y=history.history['acc'], mode='lines',name='train'))\nfig.add_trace(go.Scatter(x=epochs,y=history.history['val_acc'], mode='lines',name='val'))\nfig.update_layout({\n        'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n        'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n        'width': 500, \n        'height': 400\n\n})\nfig.show()","4a0ab7fa":"predict_frame = pd.DataFrame([])\npredict = model.predict_generator(test)\npredict_frame['category'] = np.argmax(predict, axis=-1)\nlabels = dict((v,k) for k,v in val.class_indices.items())\npredict_frame['category'] = predict_frame['category'].replace(labels)\nprint(classification_report(y_test, predict_frame['category']))","f2f0bc8d":"show_wrong_classification(y_test, predict_frame['category'], result)","95bca5a3":"* the purpose of this kernel is to be fun and to test the use of a convolutional neural network in the identification of pokemon we know that pokemon usually has characteristics of animals that belong to their types and very striking colors such as, for example, fire pokemons are usually red already the grass ones are green the dark ones are black and so on, so it usually has striking features to represent its types, but this is not always confirmed as in the following examples:\n","f1f55678":"* the model clearly had difficulties in classifying the fire type pokemons, and while the water ones managed to perform better","f6664ac8":"* prepare our data to train CNN, lets merge each pokemons type with your image and construct the data set","001f5619":"* As we can see we have pokemons of different types badly classified but visually ignoring their animal characteristics they are similar in relation to their colors, so let's see how the pixel rgb channels are distributed among the fire and water pokemons\n\n","be99f25f":"## zoom","6f99e251":"<img src='https:\/\/img.rankedboost.com\/wp-content\/uploads\/2016\/07\/Pokemon-Go-Pok%C3%A9dex.png' style='height:400px'>","765b932c":"* So a classification of the fire type pokemon is impaired due to the red channel of the water type pokemon being very difficult to perform the classification correctly even with augmentation\n","691d7e2a":"<img src='https:\/\/images.uncyc.org\/pt\/thumb\/d\/df\/Octilery.png\/230px-Octilery.png' style='height:100px'>\n\n* Octilery: it is a water type pokemon but has striking red colors its water features are striking because the pokemon is an octopus.\n\n\n<img src='https:\/\/assets.pokemon.com\/assets\/cms2\/img\/pokedex\/full\/324.png' style='height:150px'>\n\n* Torkoal: it is a turtle pokemon but its fire type which draws attention to its flame shape is the fact that it is a turtle with a volcano on its back.\n\n","bd882ab3":"* Split data","45c735ed":"* But how we see that pokemon of diferents types can be similiar, we can have we also have pokemon with big differences between them:\n\n<img src='https:\/\/cdn.bulbagarden.net\/upload\/thumb\/f\/fe\/055Golduck.png\/250px-055Golduck.png' style='height:150px'>\n\n\n<img src='https:\/\/assets.pokemon.com\/assets\/cms2\/img\/pokedex\/full\/077.png' style='height:150px'>","cbf84e5c":"\n* the model converged at 0.76 in val data, lets see whats happen in test data","81427d23":"<img src='https:\/\/miro.medium.com\/max\/2510\/1*vkQ0hXDaQv57sALXAJquxA.jpeg' style='height:200px'>\n\n<br\/>\n<br\/>\n<br\/>\n<br\/>\n\n* Now we have train, test and val we can start modelling de CNN. The architecture used will be simple with 3 convolution layer, maxpooling and Dropout\n\n* Convolution Layer, maxpooling, dropout:\n","b53e85da":"* The distribution of pixels in the channels is similar, mainly between the green and red channels, the blue channel is the one with the most difference in its distribution where for fire type pokemon there is a low occurrence between 200 and 255 pixel value.\n","bb2a0cc6":"## brightness","f211c836":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">&nbsp;Summary:<\/h1>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#1\" role=\"tab\" aria-controls=\"profile\">1. Introduction<span class=\"badge badge-primary badge-pill\">1<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#2\" role=\"tab\" aria-controls=\"messages\">2. Data <span class=\"badge badge-primary badge-pill\">2<\/span><\/a>\n   <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#3\" role=\"tab\" aria-controls=\"messages\">3. Model<span class=\"badge badge-primary badge-pill\">3<\/span><\/a>\n   <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#4\" role=\"tab\" aria-controls=\"messages\">4. Data Augmentation<span class=\"badge badge-primary badge-pill\">4<\/span><\/a>\n<\/div>","aa800f76":"* Lets see pokemons with higher averages in the red channel","b1382f56":"* Water pokemon has high std in red channel and fire has low std in blue channel. Thus the sensation that water type pokemons have a red presence which ends up mischaracterizing pokemons. So lets know identify pokemons that has red presence.\n","a1bce1ae":"* lets see wrongs model classfication","0136e5b1":"* Now i have the data with pokemons type and your images, my objective is classify Water and Fire type know i will select them","cec7b776":"* To try improve our result, i will apply some image augmentation","02b74f0b":"<a id=\"1\"><\/a> <br>\n<font size=\"+3\" color=\"black\"><b>1 - Introduction<\/b><\/font><br><a id=\"1\"><\/a>\n<br> ","d110a6d5":"## rotation","7b30accd":"<a id=\"3\"><\/a> <br>\n<font size=\"+3\" color=\"black\"><b>3 - model<\/b><\/font><br><a id=\"3\"><\/a>\n<br> ","e69c5d69":"* So let's evaluate the performance of a CNN in classifying pokemon types based only on its images and ","3c199c81":"<a id=\"2\"><\/a> <br>\n<font size=\"+3\" color=\"black\"><b>2 - Data<\/b><\/font><br><a id=\"2\"><\/a>\n<br> ","0d7b952a":"* We have in our selection we have twice as many water type pokemon. Now i will split data in train, test and val to evaluate the CNN. So first I need construct the folder relation that keras need to split data and train my model.\n\n\n\n<br\/>\n<br\/>\n\n<br\/>\n<br\/>\n\n<img src='https:\/\/miro.medium.com\/max\/1682\/1*HpvpA9pBJXKxaPCl5tKnLg.jpeg' style='height:200px'>\n\n\n<br\/>\n<br\/>\n\n<br\/>\n<br\/>\n\n<br\/>\n<br\/>\n\n* As you can see above a path is required for each split of the data and within that path a sub path with the classification classes and their sampling.\n","0385dde8":"<a id=\"4\"><\/a> <br>\n<font size=\"+3\" color=\"black\"><b>4 - Data Augmentation<\/b><\/font><br><a id=\"4\"><\/a>\n<br> "}}