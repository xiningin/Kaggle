{"cell_type":{"94bb7be3":"code","52792d8e":"code","fc7e3f05":"code","06b69c47":"code","5167928a":"code","c1faa1d0":"code","d5c685bd":"code","a78d4b68":"code","a269b49b":"code","c118bc5f":"code","4c504d9b":"code","d8655b69":"code","de85cc84":"code","327880a0":"code","962f32d4":"code","e4822171":"code","1fd36db8":"code","043b8824":"code","62ebfc54":"code","edac67a6":"code","971f4ca3":"code","5928fad7":"code","6995d3c2":"markdown","a260abd3":"markdown","0c4a11c2":"markdown","6663e959":"markdown","c600d2ec":"markdown","a08e5e8b":"markdown","a7a24448":"markdown","79de8e33":"markdown","d2416332":"markdown","20a097e3":"markdown","279ac0bb":"markdown","151605e7":"markdown","fc060bf8":"markdown","0c32e966":"markdown","77f8700e":"markdown","6ac53b64":"markdown","f089803f":"markdown","00b82f41":"markdown","fe1b917f":"markdown","931a5310":"markdown","df6309d5":"markdown","3923a039":"markdown"},"source":{"94bb7be3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","52792d8e":"# Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Pandas global settings\npd.set_option(\"display.max_columns\", None)\npd.set_option(\"display.max_rows\", None)","fc7e3f05":"# Data\ntrain = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/test.csv')\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')","06b69c47":"# Let's start with rows and columns\nprint('Train: ' , train.shape)\nprint('Test: ', test.shape)","5167928a":"# Let's take a look at the first few rows in each of our data sets\ntrain.head()","c1faa1d0":"test.head()","d5c685bd":"# Dropping id column\ntrain.drop(['id'], axis=1, inplace= True)\ntest.drop(['id'], axis=1, inplace= True)","a78d4b68":"# Number of missing values\nprint(f'Number of Missing Values in Train: {sum(train.isnull().sum())}')\nprint(f'Number of Missing Values in Test: {sum(test.isnull().sum())}')","a269b49b":"train.dtypes","c118bc5f":"test.dtypes","4c504d9b":"train.nunique().sort_values(ascending=True)","d8655b69":"test.nunique().sort_values(ascending=True)","de85cc84":"# Creating our list of integer fields \ninteger_fields = ['f1', 'f16', 'f27', 'f55', 'f60', 'f86']\n\n# We now create a dataframe with the count of our unique values per field\ntrain_unique = pd.DataFrame(train[integer_fields].nunique())\ntrain_unique = train_unique.reset_index(drop=False) #This line is necessary so it know to take our list of fields as a field itself and not as index\ntrain_unique.columns = ['Features', 'Count']\n\n\n# Do the same for our test data\ntest_unique = pd.DataFrame(test[integer_fields].nunique())\ntest_unique = test_unique.reset_index(drop=False) #This line is necessary so it know to take our list of fields as a field itself and not as index\ntest_unique.columns = ['Features', 'Count']\n","327880a0":"# Creating our plot\nsns.set_style(\"dark\")\nplot = sns.barplot(x= train_unique.Features, y=train_unique.Count)\n\nplt.title('Count of Unique Values - Train')\nplt.xlabel('Features')\nplt.ylabel('Count')\nplt.xticks(rotation=30, horizontalalignment=\"center\")\nplt.figure(figsize=(20,20))\n\nx = train_unique['Features'].values.tolist()\ny = train_unique['Count'].values.tolist()\n\n# Annotations\nfor bar in plot.patches:\n    plot.annotate(format(bar.get_height(), '.0f'), \n                  (bar.get_x() + bar.get_width()\/2, bar.get_height()), \n                  ha='center', va='center', size='10', xytext=(0,8), \n                  textcoords='offset points')\nplt.show()","962f32d4":"# Plot\nsns.set_style(\"dark\")\nplot2 = sns.barplot(x= test_unique.Features, y=test_unique.Count)\n\nplt.title('Count of Unique Values - Test')\nplt.xlabel('Features')\nplt.ylabel('Count')\nplt.xticks(rotation=30, horizontalalignment=\"center\")\nplt.figure(figsize=(20,20))\n\n# Annotations\nfor i in plot2.patches:\n    plot2.annotate(format(i.get_height(), '.0f'), \n                  (i.get_x() + i.get_width()\/2, i.get_height()), \n                  ha='center', va='center', size='10', xytext=(0,8), \n                  textcoords='offset points')\nplt.show()","e4822171":"fig, axes = plt.subplots(10,10,figsize=(12, 12))\naxes = axes.flatten()\n\nfor idx, ax in enumerate(axes):\n    sns.kdeplot(data=train, x=f'f{idx}', \n                fill=True, \n                ax=ax)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlabel('')\n    ax.set_ylabel('')\n    ax.spines['left'].set_visible(False)\n    ax.set_title(f'f{idx}', loc='right', weight='bold', fontsize=10)\n\nfig.supxlabel('Distribution by feature', ha='center', fontweight='bold')\n\nfig.tight_layout()\nplt.show()\n","1fd36db8":"# Plot\nsns.set_style(\"dark\")\nsns.histplot(data=train.loss)\n\nplt.title('Distribution of Loss')\nplt.figure(figsize=(20,20))","043b8824":"# Sepparating our matrix of features and prediction vector\nX = train.iloc[:, 0:-1].values\ny = train.iloc[:, -1].values","62ebfc54":"# Importing the library and fitting the model\nfrom sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(n_estimators = 10, random_state = 0)\nregressor.fit(X,y)","edac67a6":"# Creating predictions\npreds = regressor.predict(test)","971f4ca3":"#resetting the index to the correct number\ntest.index += 250000","5928fad7":"#Creating Submission File\nsubmission = pd.DataFrame({\"id\": test.index, \"loss\": preds})\nsubmission.to_csv(\"submission.csv\", index=False)","6995d3c2":"### What about unique values?\n\nLet's take a look at the unique values of our different fields","a260abd3":"### Let's load our Libraries and Data\n\nFirst step to start exploring the data we got is to bring in our \"tools\". We will use the basic stack:","0c4a11c2":"Like I mentioned earlier, this is a **Regression** problem. That means we will be predicting a number. There are variety of models that we can apply for this purpose, but some of them will require the data to meet some especific criteria. \n\nFor example, one of the criteria to apply a Linear Regression model is that the data is normally distributed. Let's check for that:","6663e959":"In both of our data sets, the integers fields seems to have significantly smaller amount of unique values. Regardless, the smallest one is 284 values, which is still a lot to consider it a categorical. \n\n### Let's compare the number of unique values in our integer fields.","c600d2ec":"# 4. Conclusion \ud83d\udd1a\n\nTo summarize our findings:\n* This is a **large** data set, with 100 features\n* All of the **features are numerical**, and will be treated as such in our prediction models\n    * Our integer features, even though they had a smaller amount of unique values, will not be treated as categoricals\n* Most of our features **don't have a normal distribution**\n* Our prediction vector is **positively skewed**","a08e5e8b":"#### Perfect, we found something to fix! Let's get rid of the id column in both of the data sets:","a7a24448":"# Data Science, my $0.02 \ud83d\udcb0\n\nAs a person looking to **start** getting involved with data science, it can be a bit daunting to just jump into it. \n\nMost of what you find in articles and blog posts are explanations of very complicated machine learning models, neural networks, deep learning, and many other things I am yet to understand. \n\nBrowsing through the expert's notebooks can be a very enriching experience, but **very overwhelming** at the same time. \n\nI wish I can bring a refreshing (and hopefully insightful) view to my fellow beginners with this attempt at Exploratory Data Analysis, and hopefully show you that many others are in the same boat as you are.\n\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana;\">\n    \ud83d\udccc I would greatly appreciate ANY feedback you might have regarding my code, though process, approach, etc. \n<\/div>","79de8e33":"# EDA Closing Thoughts \ud83d\udcad\n\nThank you so much for taking the time to read this notebook. I hope you found it somewhat informative and helpful. Please leave a comment if you have any recommendations or thoughts!","d2416332":"#### Now, let's check how many missing values we have","20a097e3":"As shown on some of our other features, this feature's distribution is also skewed.","279ac0bb":"#### Checkpoint\n\nThe train data set has one more column than our test data set. This makes sense since we will be using the test data set to create our competition submission. In other words, we are predicting that missing column. \n\nAlso, A HUNDRED COLUMNS! Wow ","151605e7":"#### Finally, lets look at the datatypes","fc060bf8":"## Random Forrest Regressor\nWe mentioned earlier when we dove into the features that the data was not normally distributed. For this reason we will start with tree-based models, see their performance, and go from there. ","0c32e966":"# 3. Exploring \ud83d\udd0d\n## Now that we have the data...\n\nWe want to do simple exploration on our two main data sets. How many columns\/rows do they have? What kind of values they contain? Are there any missing values? \n\nThat sort of thing...","77f8700e":"# 5. Predictions","6ac53b64":"Here we can clearly see that we cannot treat these integer features as categoricals because of their magnitude of unique values. The amount of unique values is referred to as **Cardinality**. Features with high cardinailty don't tend to be the best categorical variables.","f089803f":"#### Datatype summary\n\nWe have a combination of integers and floats for our features. It would be interesting to explore the number of unique values in each columns and see how it compares to hour total amount of rows. Maybe this would allow us to treat some of our integer columns as categorical?\n\nAlso, our integer columns are f1, f16, f27, f55, f60 and f86.","00b82f41":"# 1. Problem - What are we even trying to do? \ud83c\udfaf\n\nI am not going to lie, I had to read through several discussion posts to understand what data we are dealing with and what we are trying to achieve. Long story short:\n\n**We are trying to predict HOW MUCH loss is associated to loan default**\n\nIn human, we want to know how much money will a lending entity (bank, credit union, etc.) lose if one of their customers decides to give up on paying their loan. \n\nNow, I like to usually start with a couple of **hypothesis or predictions** about the data on hand and try to prove them right\/wrong. For that I usually create a relationship between the features using prior knowledge and common sense (ex: Amount of excercise is positively correlated with calories burnt). In this case, since the features are anonymized, we will have to skip that part and find some insight on the numbers themselves. \n\nAlso, notice that we are trying to predict **how much loss** is associated to a person defaulting. Personally, I more commonly see this kind of financial data associated to a classification prediction: *Will this person default on their loan?*\n\nGiven that we are going to predict a value and not a category, it makes this a **Regression** problem.","fe1b917f":"## What about our prediction value?\n\nI wonder what the distribution of our **loss** field looks like","931a5310":"## So, what magic spell will we use to predict? ","df6309d5":"# 2. Getting Started \ud83d\udc69\u200d\ud83d\udcbb","3923a039":"Even though they are very small, you can see each of the features distribution:\n* We have some **Normally Distributed** features\n* Some of the features are **bimodal or trimodal**\n* A lot of our features are **skewed**\n\n**Normal Distribution** - When the distribution of the values is a symetrical bell-shaped graph\n\n**Multimodal (bimodal, trimodal, etc.)** - When the distribution shows several \"peaks\" (local maxima)\n\n**Skewed** - refers from a distortion that deviates from the ideal symetrical bell curve. The curve is shifted to  the left or right. \n\nSo, if we are planning on applying any linear models, we will ahve to address the distribution issue."}}