{"cell_type":{"72450a2c":"code","0b25b4d0":"code","d52be845":"code","191baffd":"code","17460510":"code","e5220e6d":"code","425b45f8":"code","8a251445":"code","a60c57f8":"code","dc387571":"code","77987643":"code","5af8f5e1":"code","6020e016":"code","4f094d47":"code","dfc3f1e2":"code","a820e310":"code","71e439bf":"code","a02f6d03":"code","c35b30ef":"code","5df66d48":"code","2785442c":"code","5c648bc7":"code","b7697a6f":"code","a5393824":"code","0e820c76":"code","6d7f0293":"code","573d424a":"code","7208c4c8":"code","037f2c07":"code","4b8f4894":"code","08087744":"code","9e422149":"code","59f4890d":"code","e49e07f5":"code","cb9fa9b4":"code","273d4671":"code","4082d5ed":"markdown"},"source":{"72450a2c":"import pandas as pd\ndataset=pd.read_csv('..\/input\/company-database\/promotion_dataset.csv')\ndataset.head()","0b25b4d0":"dataset.info()","d52be845":"dataset.describe()","191baffd":"dataset.skew()","17460510":"dataset.nunique()","e5220e6d":"dataset.isnull().sum().sort_values(ascending=False)","425b45f8":"round((dataset[['previous_year_rating','education']].isnull().sum()*100\/len(dataset)),2)","8a251445":"dataset[['previous_year_rating','education']].info()","a60c57f8":"dataset['previous_year_rating'].value_counts()","dc387571":"import numpy as np\ndataset[dataset['previous_year_rating'].isna()].head()","77987643":"dataset['education'].value_counts()","5af8f5e1":"dataset[dataset['education'].isna()].head()","6020e016":"round(((dataset['is_promoted'].value_counts()*100)\/len(dataset)),2)","4f094d47":"!pip install pandas-profiling\n","dfc3f1e2":"from pandas_profiling import ProfileReport\nprofile = ProfileReport(dataset, title=\"EDA Report\")\nprofile","a820e310":"data = dataset.sample(frac=0.99, random_state=42)\ndata_unseen = dataset.drop(data.index)\ndata.reset_index(inplace=True, drop=True)\ndata_unseen.reset_index(inplace=True, drop=True)\nprint('Data for Modeling: ' + str(data.shape))","71e439bf":"print('Unseen Data For Predictions: ' + str(data_unseen.shape))","a02f6d03":"!pip install pycaret","c35b30ef":"from pycaret.classification import *","5df66d48":"data.info()","2785442c":"data.columns","5c648bc7":"categorical = []\nfor i in data.columns:\n    if (data[i].dtype=='object'):\n        categorical.append(i)\nprint(\"Categorical Attribute : {}\\n \".format(len(categorical)))\nfor x in range(len(categorical)): \n    print(categorical[x])","b7697a6f":"data[categorical].nunique()","a5393824":"round(data['region'].value_counts()*100\/len(data),2)","0e820c76":"round(data['department'].value_counts()*100\/len(data),2)","6d7f0293":"promotion = setup(data = data, target = 'is_promoted',\n                  session_id=1,\n                  train_size = 0.8, # training over 80% of available data\n                  ordinal_features = {'education' : [\"Below Secondary\", \"Bachelor's\", \"Master's & above\"]}, #ordinal feature\n                  ignore_features=['employee_id'], # not to consider this feature for training model\n                  categorical_features=['department','gender','recruitment_channel','awards_won?'], #categorical features\n                  numeric_features=['no_of_trainings', 'age', 'previous_year_rating',\n                                    'length_of_service',  'avg_training_score'],# numerical feature\n                  transformation = True,#Transformation changes the shape of the distribution such that the transformed data can be represented by normal distribution\n                  normalize = True, #rescale the values of numeric columns\n                  handle_unknown_categorical = True, \n                  unknown_categorical_method = 'most_frequent',\n                  remove_outliers = True,\n                  remove_multicollinearity = True, #drop one of the two features that are highly correlated with each other\n                  ignore_low_variance = True,#all categorical features with statistically insignificant variances are removed from the dataset.\n                  combine_rare_levels = True,# all levels in categorical features below the threshold defined in rare_level_threshold param are combined together as a single level\n                  high_cardinality_features = ['region','department'],#compressed into fewer levels by passing them as a list of column names with high cardinality. \n                  fix_imbalance = True,# to fix the imbalance\n                  numeric_imputation='median',\n                  categorical_imputation='mode')","573d424a":"best=compare_models()","7208c4c8":"catboost= create_model('catboost')","037f2c07":"print(catboost)","4b8f4894":"tuned_catboost = tune_model(catboost,optimize = 'AUC') #tuned on  AUC ","08087744":"evaluate_model(tuned_catboost) #Graphical plot ","9e422149":"predict_model(tuned_catboost)# Test data evaluation ","59f4890d":"final_catboost = finalize_model(tuned_catboost) # Final model \nfinal_catboost","e49e07f5":"predict_model(final_catboost)#final test model evaluation ","cb9fa9b4":"unseen_predictions = predict_model(final_catboost, data=data_unseen)#evaluation on unseen data\nunseen_predictions.head()","273d4671":"print(\"Confidence Score :   {}\".format(round(unseen_predictions.Score.mean(),2)))#Confidence Score","4082d5ed":"We opt for CatBoost Classifier for it's high AUC\n\nFor binary classification problem, AUC, graph of true positive vs false positive gives information about how our classifier is as compared to random guesses. If classifier is good AUC will be close to 1. AUC is better measure of classifier performance than accuracy because it does not bias on size of test or evaluation data. \n\nFor better understanding, we can read at https:\/\/datascience.stackexchange.com\/questions\/806\/advantages-of-auc-vs-standard-accuracy#:~:text=AUC%20is%20in%20fact%20often,a%20number%20of%20different%20reasons.&text=slightly%20non%2Dintuitive.-,The%20implicit%20goal%20of%20AUC%20is%20to%20deal%20with%20situations,overfit%20to%20a%20single%20class."}}