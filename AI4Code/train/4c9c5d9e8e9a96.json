{"cell_type":{"098128d9":"code","95f7aa6a":"code","8b986c1b":"code","024d6f43":"code","d08538ac":"code","f27afd16":"code","756a7ef4":"code","b1febf90":"code","095b2c01":"code","1b1dc172":"code","b8d2da27":"code","648bb16d":"code","f8d9c995":"code","19864103":"code","a7f847bc":"code","21172d24":"code","d8797f6e":"code","b26bb127":"code","c1db138d":"code","4d2cd837":"code","86e3d560":"code","703ba038":"code","e202186d":"code","608d010b":"code","eb8b356e":"code","2e2600e4":"markdown","dcf2bac9":"markdown","0d2f65a8":"markdown","68cc7867":"markdown","f35b0412":"markdown","53a86a30":"markdown","5ef48b33":"markdown","0f813402":"markdown","6c37d59f":"markdown"},"source":{"098128d9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report,confusion_matrix\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\nimport matplotlib.pyplot as plt\nfrom mlxtend.plotting import plot_decision_regions\nimport time\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","95f7aa6a":"df = pd.read_excel(\"\/kaggle\/input\/bank-loan-modelling\/Bank_Personal_Loan_Modelling.xlsx\",\"Data\")\ndf.head()","8b986c1b":"percent_missing = df.isnull().sum() * 100 \/ len(df)\nmissing_values = pd.DataFrame({'percent_missing': percent_missing})\nmissing_values.sort_values(by ='percent_missing' , ascending=False)","024d6f43":"X = df.drop(['Personal Loan'], axis=1)\nX = StandardScaler().fit_transform(X)\nY = df['Personal Loan'].values\nX_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size = 0.30, random_state = 101)","d08538ac":"clf = RandomForestClassifier(max_depth=2, random_state=0)\nclf.fit(X_Train, Y_Train)\ny_pred = clf.predict(X_Test)\naccuracy_score(Y_Test, y_pred)","f27afd16":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\nPCA_df = pd.DataFrame(data = X_pca, columns = ['PC1', 'PC2'])\nPCA_df = pd.concat([PCA_df, df['Personal Loan']], axis = 1)\nPCA_df.head()","756a7ef4":"X = PCA_df.drop(['Personal Loan'], axis=1)\nX = StandardScaler().fit_transform(X)\nY = PCA_df['Personal Loan'].values\nX_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size = 0.30, random_state = 101)","b1febf90":"clf = RandomForestClassifier(max_depth=2, random_state=0)\nclf.fit(X_Train, Y_Train)\ny_pred = clf.predict(X_Test)\naccuracy_score(Y_Test, y_pred)","095b2c01":"import matplotlib.gridspec as gridspec\nimport itertools\ngs = gridspec.GridSpec(2, 2)\n\nfig = plt.figure(figsize=(10,8))\n\nclf1 = LogisticRegression(random_state=1,\n                          solver='newton-cg',\n                          multi_class='multinomial')\nclf2 = RandomForestClassifier(random_state=1, n_estimators=100)\nclf3 = GaussianNB()\nclf4 = SVC(gamma='auto')\n\nlabels = ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'SVM']\nfor clf, lab, grd in zip([clf1, clf2, clf3, clf4],\n                         labels,\n                         itertools.product([0, 1], repeat=2)):\n\n    clf.fit(X_Train, Y_Train)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X_Train, Y_Train, clf=clf, legend=2)\n    plt.title(lab)\n\nplt.savefig(\"boundaries.png\", dpi=300)\nplt.show()","1b1dc172":"from yellowbrick.contrib.classifier import DecisionViz\nfrom sklearn.neighbors import KNeighborsClassifier\n\nviz = DecisionViz(\n    KNeighborsClassifier(3), title=\"Nearest Neighbors\",\n    features=['PCA1', 'PCA2'], classes=['0', '1']\n)\nviz.fit(X_Train, Y_Train)\nviz.draw(X_Test, Y_Test)\nviz.show()","b8d2da27":"viz = DecisionViz(\n    RandomForestClassifier(3), title=\"Random Forest\",\n    features=['PCA1', 'PCA2'], classes=['0', '1']\n)\nviz.fit(X_Train, Y_Train)\nviz.draw(X_Test, Y_Test)\nviz.show()","648bb16d":"from itertools import product\n\nclf1 = RandomForestClassifier(max_depth=4)\nclf2 = KNeighborsClassifier(n_neighbors=7)\nclf3 = SVC(gamma=.1, kernel='rbf', probability=True)\neclf = VotingClassifier(estimators=[('rf', clf1), ('knn', clf2),\n                                    ('svc', clf3)],\n                        voting='soft', weights=[2, 1, 2])\n\nclf1.fit(X_Train, Y_Train)\nclf2.fit(X_Train, Y_Train)\nclf3.fit(X_Train, Y_Train)\neclf.fit(X_Train, Y_Train)\n\n# Plotting decision regions\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                     np.arange(y_min, y_max, 0.1))\n\nf, axarr = plt.subplots(2, 2, sharex='col', sharey='row', figsize=(10, 8))\n\nfor idx, clf, tt in zip(product([0, 1], [0, 1]),\n                        [clf1, clf2, clf3, eclf],\n                        ['Random Forest (depth=4)', 'KNN (k=7)',\n                         'Kernel SVM', 'Soft Voting']):\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.4)\n    axarr[idx[0], idx[1]].scatter(X_Train[:, 0], X_Train[:, 1], c=Y_Train,\n                                  s=20, edgecolor='k')\n    axarr[idx[0], idx[1]].set_title(tt)\n\nplt.show()","f8d9c995":"# Following https:\/\/www.kaggle.com\/arthurtok\/decision-boundaries-visualised-via-python-plotly\nfrom plotly import tools\nimport plotly.offline as py\n\ntrees = RandomForestClassifier(max_depth=4, n_estimators=20, random_state=0)\ntrees.fit(X_Train, Y_Train)\nh = 0.02\n\ntrees_overfit = RandomForestClassifier(max_depth=50, n_estimators=5, random_state=0)\ntrees_overfit.fit(X_Train, Y_Train)\n\nx_min, x_max = X_Train[:, 0].min() - 1, X_Train[:, 0].max() + 1\ny_min, y_max = X_Train[:, 1].min() - 1, X_Train[:, 1].max() + 1\n\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h)\n                     , np.arange(y_min, y_max, h))\ny_ = np.arange(y_min, y_max, h)\n\nZ = trees.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n#Z = Z.reshape((xx.shape[0], xx.shape[1], 3))\n\nfig = tools.make_subplots(rows=1, cols=2,\n                          subplot_titles=(\"Random Forest (Depth = 4)\",\n                                          \"Random Forest (Depth = 50)\")\n                         )\n\ntrace1 = go.Heatmap(x=xx[0], y=y_, z=Z,\n                  colorscale='Viridis',\n                  showscale=False)\n\ntrace2 = go.Scatter(x=X_Train[:, 0], y=X_Train[:, 1], \n                    mode='markers',\n                    showlegend=False,\n                    marker=dict(size=10,\n                                color=Y_Train, \n                                colorscale='Viridis',\n                                line=dict(color='black', width=1))\n                    )\n                  \nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 1)\n\nZ = trees_overfit.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\n\ntrace3 = go.Heatmap(x=xx[0], y=y_, \n                    z=Z,\n                    colorscale='Viridis',\n                    showscale=True)\n\ntrace4 = go.Scatter(x=X_Train[:, 0], y=X_Train[:, 1],\n                    mode='markers',\n                    showlegend=False,\n                    marker=dict(size=10,\n                                color=Y_Train, \n                                colorscale='Viridis',\n                                line=dict(color='black', width=1))\n                   )\nfig.append_trace(trace3, 1, 2)\nfig.append_trace(trace4, 1, 2)\n\nfor i in map(str, range(1, 3)):\n    x = 'xaxis' + i\n    y2 = 'yaxis' + i\n    fig['layout'][x].update(showgrid=False, zeroline=False,\n                                   showticklabels=False, ticks='', autorange=True)\n    fig['layout'][y2].update(showgrid=False, zeroline=False,\n                                   showticklabels=False, ticks='', autorange=True)\n\npy.iplot(fig)","19864103":"def forest_test(X, Y):\n    X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size = 0.30, random_state = 101)\n    start = time.process_time()\n    trainedforest = RandomForestClassifier(n_estimators=700).fit(X_Train,Y_Train)\n    print(time.process_time() - start)\n    predictionforest = trainedforest.predict(X_Test)\n    print(confusion_matrix(Y_Test,predictionforest))\n    print(classification_report(Y_Test,predictionforest))","a7f847bc":"def complete_test_2D(X, Y, plot_name = ''):\n    Small_df = pd.DataFrame(data = X, columns = ['C1', 'C2'])\n    Small_df = pd.concat([Small_df, df['Personal Loan']], axis = 1)\n    Small_df['Personal Loan'] = LabelEncoder().fit_transform(Small_df['Personal Loan'])\n    forest_test(X, Y)\n    data = []\n    for clas, col, name in zip((1, 0), ['red', 'darkblue'], ['0', '1']):\n\n        trace = dict(\n            type='scatter',\n            x= Small_df.loc[Small_df['Personal Loan'] == clas, 'C1'],\n            y= Small_df.loc[Small_df['Personal Loan'] == clas, 'C2'],\n            mode= 'markers',\n            name= name,\n            marker=dict(\n                color=col,\n                size=12,\n                line=dict(\n                    color='rgba(217, 217, 217, 0.14)',\n                    width=0.5),\n                opacity=0.8)\n        )\n        data.append(trace)\n\n    layout = dict(\n            title= plot_name + ' 2D Dimensionality Reduction',\n            xaxis=dict(title='C1', showline=False),\n            yaxis=dict(title='C2', showline=False)\n    )\n    fig = dict(data=data, layout=layout)\n    iplot(fig)","21172d24":"complete_test_2D(X_pca, Y, 'PCA')","d8797f6e":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\ninput_shape2 = [X_Train.shape[1]]\nmodel = Sequential()\nmodel.add(Dense(50, input_shape=input_shape2, activation='relu'))\nmodel.add(Dense(20, activation='relu'))\nmodel.add(Dense(1, activation='tanh'))\nmodel.compile(loss='squared_hinge', optimizer='adam', metrics=['accuracy'])\nmodel.fit(X_Train, Y_Train, epochs=20, batch_size=64, verbose=2, validation_split=0.3)\nmodel.evaluate(X_Test, Y_Test, verbose=1)","b26bb127":"plot_decision_regions(X_Train, Y_Train, clf=model, legend=2)\n\n# Working with more than 2 dimensions\n# value=1.5\n# width=0.75\n# plot_decision_regions(X_Train, Y_Train, clf=model, legend=2, feature_index=[0,2],\n#                       filler_feature_values={1: value, 3:value, 4: value, 5:value, \n#                                              6: value, 7:value, 8: value, 9:value, \n#                                              10: value, 11: value, 12:value},\n#                       filler_feature_ranges={1: width, 3: width, 4: width, 5: width, \n#                                              6: width, 7: width, 8: width, 9: width, \n#                                              10: width, 11: width, 12: width})\n\nplt.show()","c1db138d":"plot_decision_regions(X_Test, Y_Test, clf=model, legend=2)\nplt.show()","4d2cd837":"# Following https:\/\/towardsdatascience.com\/simple-method-of-creating-animated-graphs-127c11f58cc5\n# boundary of the graph\nGRID_X_START = -7\nGRID_X_END = 9\nGRID_Y_START = -1.0\nGRID_Y_END = 8.5\n# output directory (the folder must be created on the drive)\nOUTPUT_DIR = \".\/\"\ngrid = np.mgrid[GRID_X_START:GRID_X_END:100j,GRID_X_START:GRID_Y_END:100j]\ngrid_2d = grid.reshape(2, -1).T\nXX, YY = grid","86e3d560":"# the function making up the graph of a dataset\ndef make_plot(X, y, plot_name, file_name=None, XX=None, YY=None, preds=None, dark=False):\n    if (dark):\n        plt.style.use('dark_background')\n    else:\n        sns.set_style(\"whitegrid\")\n    plt.figure(figsize=(16,12))\n    axes = plt.gca()\n    axes.set(xlabel=\"$X_1$\", ylabel=\"$X_2$\")\n    plt.title(plot_name, fontsize=30)\n    plt.subplots_adjust(left=0.20)\n    plt.subplots_adjust(right=0.80)\n    if(XX is not None and YY is not None and preds is not None):\n        plt.contourf(XX, YY, preds[:,0].reshape(XX.shape), 25, alpha = 1, cmap=cm.Spectral)\n        plt.contour(XX, YY, preds[:,0].reshape(XX.shape), levels=[.5], cmap=\"Greys\", vmin=0, vmax=.6)\n    plt.scatter(X[:, 0], X[:, 1], c=y.ravel(), s=40, cmap=plt.cm.Spectral, edgecolors='black')\n    if(file_name):\n        plt.savefig(file_name)\n        plt.close()","703ba038":"def make_gif(input_folder, save_filepath):\n    episode_frames = []\n    time_per_step = 0.25\n    for root, _, files in os.walk(input_folder):\n        file_paths = [os.path.join(root, file) for file in files]\n        #sorted by modified time\n        file_paths = sorted(file_paths, key=lambda x: os.path.getmtime(x))\n        episode_frames = [imageio.imread(file_path) for file_path in file_paths if file_path.endswith('.png')]\n    episode_frames = np.array(episode_frames)\n    imageio.mimsave(save_filepath, episode_frames, duration=time_per_step)","e202186d":"import seaborn as sns\nfrom matplotlib import cm\n\ndef callback_keras_plot(epoch, logs):\n    plot_title = \"Keras Model - It: {:05}\".format(epoch)\n    file_name = \"keras_model_{:05}.png\".format(epoch)\n    file_path = os.path.join(OUTPUT_DIR, file_name)\n    prediction_probs = model.predict_proba(grid_2d, batch_size=32, verbose=0)\n    make_plot(X_Test, Y_Test, plot_title, file_name=file_path, XX=XX, YY=YY, preds=prediction_probs)\n    \n# Adding callback functions that they will run in every epoch\ntestmodelcb = keras.callbacks.LambdaCallback(on_epoch_end=callback_keras_plot)\n\n# Building a model\nmodel = Sequential()\nmodel.add(Dense(25, input_dim=2,activation='relu'))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(25, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer=\"sgd\", metrics=['accuracy'])\n\n# Training\nhistory = model.fit(X_Train, Y_Train, epochs= 118, callbacks=[testmodelcb])","608d010b":"prediction_probs = model.predict_proba(grid_2d, batch_size=32, verbose=0)\nmake_plot(X_Test, Y_Test, \"Keras Model\", file_name=None, XX=XX, YY=YY, preds=prediction_probs)","eb8b356e":"import imageio\n\ninput_folder = '.\/'\nsave_filepath = '.\/Keras_gif.gif'\nmake_gif(input_folder, save_filepath)","2e2600e4":"### Sklearn","dcf2bac9":"## PCA Reduction ","0d2f65a8":"### Mlxtend","68cc7867":"# Machine Learning Visualization 5","f35b0412":"### Neural Network Animated Decision Boundary","53a86a30":"### Plotly","5ef48b33":"### PCA Plot","0f813402":"### Using Deep Learning instead of Machine Learning","6c37d59f":"### Yellowbrick"}}