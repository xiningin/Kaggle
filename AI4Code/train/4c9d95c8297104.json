{"cell_type":{"86abb0dc":"code","d943e30e":"code","69fb1c5f":"code","c933b81a":"code","1446e710":"code","65a8269f":"code","4e33b6b1":"code","743d52a3":"code","465f761e":"code","382bfb67":"code","2f526447":"code","f6f694af":"code","5037f274":"code","b464684d":"code","2e77485f":"code","cfface4d":"markdown","97ba5272":"markdown","28fc238f":"markdown","68f16309":"markdown","df481070":"markdown","630bca6b":"markdown","ea83f7c4":"markdown","18ef8a78":"markdown","1ee3ed6e":"markdown","07e6f138":"markdown","509f1954":"markdown","dd5eb7bf":"markdown","89f146db":"markdown","d0becde2":"markdown","2af1e546":"markdown"},"source":{"86abb0dc":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom scipy.spatial.distance import cdist\nfrom mpl_toolkits.axes_grid1 import host_subplot\nfrom sklearn import metrics\nimport matplotlib.gridspec as gridspec\nimport math\nimport seaborn as sns\n\n\n#import os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n","d943e30e":"path = '\/kaggle\/input\/customer-segmentation-tutorial-in-python\/Mall_Customers.csv'\ndata = pd.read_csv(path)","69fb1c5f":"for column in data.columns:\n    print(column, ' - ', data[column].dtype)","c933b81a":"for col in data.columns:\n    nan = round(data[col].isnull().sum() \/ len(data[col]) * 100, 2)\n    print(col, ' : ', nan, '%')","1446e710":"data[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']].hist(figsize = (20,20))\nplt.show()","65a8269f":"corr = data[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']].corr()\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=1,vmin = -1, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","4e33b6b1":"# Standarize data\nscaler = StandardScaler()\ndata_sc = scaler.fit_transform(data[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']])\ndata_sc = pd.DataFrame(data_sc, columns = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)'])","743d52a3":"def elbow_plot_for_k_means(X, k_range = range(2,15)):\n    distortions = []\n    for k in k_range:\n        kmeanModel = KMeans(n_clusters=k).fit(X)\n        kmeanModel.fit(X)\n        distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) \/ X.shape[0]) \n    reduction = [distortions[i-1] - distortions[i] for i in range(1,len(distortions))]\n    reduction.insert(0,np.nan)\n    # Plot    \n    plt.figure(figsize=[12,8])\n    host = host_subplot(111)\n    par = host.twinx()\n    host.set_title(\"Optimal number of clusters\")\n    host.set_xlabel(\"k\")\n    host.set_ylabel(\"Distortion\")\n    par.set_ylabel(\"Distortion reduction\")\n    plt.xticks(k_range)\n    p1, = host.plot(k_range, distortions)\n    p2, = par.plot(k_range, reduction)\n    host.yaxis.get_label().set_color(p1.get_color())\n    par.yaxis.get_label().set_color(p2.get_color())\n    plt.show()","465f761e":"elbow_plot_for_k_means(data_sc)","382bfb67":"def ch_plot_for_k_means(X, k_range = range(2,15), resample = 3):\n    plt.figure(figsize=[12,8])\n    for i in range(3):\n        scores = []\n        for k in k_range:\n            kmeansModel = KMeans(n_clusters = k).fit(X)\n            labels = kmeansModel.labels_\n            scores.append(metrics.calinski_harabasz_score(X, labels)) \n        plt.plot(k_range, scores)\n    plt.xticks(k_range)\n    plt.title(\"Optimal number of clusters Calinski-Harabasz criterion\")\n    plt.xlabel(\"k\")\n    plt.ylabel(\"Calinski - Harabasz statistic\")\n    plt.show()","2f526447":"ch_plot_for_k_means(data_sc, k_range = range(2,10))","f6f694af":"class KMeansCust(KMeans):\n    \"\"\"\n    Custumized KMeans Class. Add new prediction method, returned cluster label and distances to each cluster. \n    It's usefull when using distances as explanatory variables in other model.\n    \"\"\"    \n    def predict_dist(self, X, columns = None, scaler = True, join = True):\n        \"\"\"\n        There arent missing values in X. \n        IN: X - data (pandas, numpy)\n            columns - if X is pandas obiect then its possible to indicate columns and X can have redundant columns.\n            scaler - if True then StandardScaler\n            inplace - if True results are joined to X and then \n        \"\"\"\n        if columns != None: data = X[columns]\n        else: data = X\n        if scaler:\n            scaler = StandardScaler()\n            data = scaler.fit_transform(data)\n        df = pd.DataFrame(np.array(cdist(data, self.cluster_centers_, 'euclidean')))\n        df.columns = ['dist_clu_' + str(i) for i in range(len(df.columns))]\n        labels = self.predict(data)\n        df['clu_label'] = list(labels)\n        if join:\n            X_ = X.copy()\n            X_.reset_index(inplace = True, drop = True)\n            df.reset_index(inplace = True, drop = True)\n            return X_.join(df, how = 'left')\n        else:\n            return df","5037f274":"kmeans_model = KMeansCust(n_clusters = 6).fit(data_sc)\ndata_pred = kmeans_model.predict_dist(X = data, columns = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)'])","b464684d":"def kmeans_present_cluster_stats(data, label, cols = None):\n    \"\"\"\n    \"\"\"\n    means = data.groupby(label).mean()\n    rows = math.ceil(len(means.columns)\/5)\n    fig = plt.figure(constrained_layout=False, figsize=(10, rows * 5))\n    gs = gridspec.GridSpec(rows, 5, figure=fig)\n    i=0\n    for col in means.columns:\n        if col in cols:\n            ax = fig.add_subplot(gs[i])\n            if i % 5 == 0:\n                ax.set_yticks([i + 0.5 for i in means.index])\n                ax.set_yticklabels(['Cluster ' + str(i) for i in means.index])\n                ax.set_ylabel('Mean by cluster')\n            else:\n                ax.set_yticks([])\n            ax.set_xticks([0.5])\n            ax.set_xticklabels([col], rotation = 45)\n            col_t = [[i] for i in list(means[col])] \n            im = ax.pcolormesh(col_t, cmap=\"RdYlBu_r\")\n            plt.colorbar(im)\n            i+=1\n    plt.tight_layout()\n    plt.show()","2e77485f":"kmeans_present_cluster_stats(data = data_pred, label = 'clu_label', cols = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)'])","cfface4d":"Histogram's for numeric variables.","97ba5272":"Calinski - Harabasz plot","28fc238f":"Correlation plot for numeric variables.","68f16309":"Interpretation: <br>\nCluster 0 - medium age, high income, economical <br>\nCluster 1 - medium age, high income, high spending <br>\nCluster 2 - old, avarage income, avarage spending <br>\nCluster 3 - young, avarage income, avarage spending <br>\nCluster 4 - young, low income, high spending - intresting :) <br> \nCluster 5 - medium age, low income, low spending","df481070":"### 3. Choose right number of clusters\nElbow plot","630bca6b":"### 1. Preprocessing\n#### Include necessary libraries.","ea83f7c4":"Heatmap presents no significant correlation between any of features.","18ef8a78":"#### Fit model","1ee3ed6e":"#### Missing valeus\n It's important to deal with missing values. If there're NA's, it's important to replace or delete rows. ","07e6f138":"### 4. Present business interpretation of clusters","509f1954":"### 2. Data exploration\n#### Data types","dd5eb7bf":"#### Stanrarize data\nIf features are measured on diffrent scales it's good to standarize data. Diffrent scale may cause that one point is close to another in one direction but very far in other. Data standarization prevent this.","89f146db":"CH plot shows that we can expect 6 optimal clusters. Elbow plot is less clear, but line stabilizes after 6 clusters. ","d0becde2":"1. Preprocessing\n2. Data exploration\n3. Choose right number of clusters\n4. Present business interpretation of clusters","2af1e546":"#### Load data "}}