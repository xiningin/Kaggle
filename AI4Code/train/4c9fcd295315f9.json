{"cell_type":{"559edf53":"code","8adb5455":"code","7cd3a730":"code","24296371":"code","b4178ae9":"code","cf3fdfb7":"code","1dc8840e":"code","b64beb18":"code","c6e378d8":"code","ba89b954":"code","52088724":"code","00d43f69":"code","660008a5":"code","1cfbb29e":"code","840974cd":"markdown","6dd51d39":"markdown","9f5bc95a":"markdown","70d53315":"markdown","bf2e01e9":"markdown","0f72b8f7":"markdown","d9d061bd":"markdown","f725e778":"markdown","857bc39e":"markdown","d78c3a57":"markdown","983e4cba":"markdown","487122c5":"markdown","f348fd7a":"markdown","021b7a88":"markdown","68e6d416":"markdown","6ca286fe":"markdown","831239bb":"markdown","ce169fae":"markdown"},"source":{"559edf53":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n","8adb5455":"!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","7cd3a730":"import tokenization","24296371":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","b4178ae9":"def build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","cf3fdfb7":"%%time\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","1dc8840e":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","b64beb18":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","c6e378d8":"train_input = bert_encode(train.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(test.text.values, tokenizer, max_len=160)\ntrain_labels = train.target.values","ba89b954":"model = build_model(bert_layer, max_len=160)\nmodel.summary()","52088724":"train_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=3,\n    batch_size=16\n)\n\nmodel.save('model.h5')","00d43f69":"test_pred = model.predict(test_input)","660008a5":"submission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('submission.csv', index=False)","1cfbb29e":"submission.head()","840974cd":"## 4.4 Loading data:","6dd51d39":"<a id='top'><\/a>\n<h1 style=\"text-align:center;font-size:200%;;\">Understanding BERT for Disaster NLP<\/h1>\n<img src=\"https:\/\/cx3digital.com\/wp-content\/uploads\/2019\/11\/google-bert.jpg\" width=\"700\" height=\"700\" align=\"center\"\/>","9f5bc95a":"## Table of Contents:<br>\n1. Intution behind RNN based Sequence-to-Sequence Model<br>\n   1.1 Limitations of RNN'S <br>\n2. Introduction to the Transformers <br>\n   2.1 Transformer\u2019s Model Architecture<br>\n   2.2 Understanding self attention<br>\n   2.3 Limitations of the Transformer<br>\n3. Understanding BERT <br>\n   3.1 How Does BERT Work?<br>\n   3.2 Text processing for BERT<br>\n   3.3 How to use BERT (Fine-tuning)<br>\n   3.4 Takeaways<br>\n   3.5 Compute considerations (training and applying)<br>\n4. Implementation of BERT using TFHub<br>\n   4.1 Importing necessary modules<br>\n   4.2 Helper Functions<br>\n   4.3 Loading BERT from the Tensorflow Hub<br>\n   4.4 Loading data<br>\n   4.5 Loading tokenizer from the bert layer<br>\n   4.6 Encoding the text into tokens, masks, and segment flags<br>\n   4.7 Model: Build, Train, Predict, Submit<br>\n   \n   ","70d53315":"# 3. Understanding BERT (Bidirectional Encoder Representations from Transformers): <br>\nBERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.<br>\n\nBERT is a **deeply bidirectional** model. Bidirectional means that BERT learns information from both the left and the right side of a token\u2019s context during the training phase.<br>\n\nThe bidirectionality of a model is important for truly understanding the meaning of a language. Let\u2019s see an example to illustrate this. There are two sentences in this example and both of them involve the word \u201cbank\u201d:<br>\n\n![](https:\/\/s3-ap-south-1.amazonaws.com\/av-blog-media\/wp-content\/uploads\/2019\/09\/sent_context.png)<br>\n\nIf we try to predict the nature of the word \u201cbank\u201d by only taking either the left or the right context, then we will be making an error in at least one of the two given examples.One way to deal with this is to consider both the left and the right context before making a prediction. That\u2019s exactly what BERT does. <br>\n\n## 3.1 How Does BERT Work? <br>\n### 3.1.1 BERT\u2019s Architecture: <br>\n\nThe BERT architecture builds on top of Transformer. We currently have two variants available:<br>\n\n* BERT Base: 12 layers (transformer blocks), 12 attention heads, and 110 million parameters\n* BERT Large: 24 layers (transformer blocks), 16 attention heads and, 340 million parameters\n\n![](https:\/\/s3-ap-south-1.amazonaws.com\/av-blog-media\/wp-content\/uploads\/2019\/09\/bert_encoder.png\/)<br>\n\nAll of these Transformer layers are Encoder-only blocks.<br>\n\nNow that we know the overall architecture of BERT, let\u2019s see what kind of text processing steps are required before we get to the model building phase.<br>\n\n## 3.2 Text processing for BERT:<br>\n![](https:\/\/s3-ap-south-1.amazonaws.com\/av-blog-media\/wp-content\/uploads\/2019\/09\/bert_emnedding.png)<br>\nFor bert,every input embedding is a combination of 3 embeddings:<br>\n\n**Position Embeddings**: BERT learns and uses positional embeddings to express the position of words in a sentence. These are added to overcome the limitation of Transformer which, unlike an RNN, is not able to capture \u201csequence\u201d or \u201corder\u201d information<br>\n**Segment Embeddings**: BERT can also take sentence pairs as inputs for tasks (Question-Answering). That\u2019s why it learns a unique embedding for the first and the second sentences to help the model distinguish between them. In the above example, all the tokens marked as EA belong to sentence A (and similarly for EB)<br>\n**Token Embeddings**: These are the embeddings learned for the specific token from the WordPiece token vocabulary<br>\n\nFor a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings.Such a comprehensive embedding scheme contains a lot of useful information for the model.These combinations of preprocessing steps make BERT so versatile.<br>\n\nWhen training language models, there is a challenge of defining a prediction goal. Many models predict the next word in a sequence, a directional approach which inherently limits context learning. To overcome this challenge, BERT uses two training strategies:<br>\n\n### 1. Masked LM (MLM): <br>\nBefore feeding word sequences into BERT, 15% of the words in each sequence are replaced with a [MASK] token. The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence. In technical terms, the prediction of the output words requires:\n1. Adding a classification layer on top of the encoder output.<br>\n2. Multiplying the output vectors by the embedding matrix, transforming them into the vocabulary dimension.<br>\n3. Calculating the probability of each word in the vocabulary with softmax.<br>\n\n![](https:\/\/miro.medium.com\/max\/986\/0*ViwaI3Vvbnd-CJSQ.png)<br>\nThe BERT loss function takes into consideration only the prediction of the masked values and ignores the prediction of the non-masked words. As a consequence, the model converges slower than directional models, a characteristic which is offset by its increased context awareness.<br>\n\n### 2. Next Sentence Prediction (NSP): <br>\nIn the BERT training process, the model receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence. The assumption is that the random sentence will be disconnected from the first sentence.<br>\nTo help the model distinguish between the two sentences in training, the input is processed in the following way before entering the model:<br>\n1. A [CLS] token is inserted at the beginning of the first sentence and a [SEP] token is inserted at the end of each sentence.\n2. A sentence embedding indicating Sentence A or Sentence B is added to each token. Sentence embeddings are similar in concept to token embeddings with a vocabulary of 2.\n3. A positional embedding is added to each token to indicate its position in the sequence. The concept and implementation of positional embedding are presented in the Transformer paper.\n![](https:\/\/miro.medium.com\/max\/1321\/0*m_kXt3uqZH9e7H4w.png)<br>\n\nTo predict if the second sentence is indeed connected to the first, the following steps are performed:\n1. The entire input sequence goes through the Transformer model.\n2. The output of the [CLS] token is transformed into a 2\u00d71 shaped vector, using a simple classification layer (learned matrices of weights and biases).\n3. Calculating the probability of IsNextSequence with softmax.<br>\n\nWhen training the BERT model, Masked LM and Next Sentence Prediction are trained together, with the goal of minimizing the combined loss function of the two strategies.\n\n## 3.3 How to use BERT (Fine-tuning):\nUsing BERT for a specific task is relatively straightforward:<br>\nBERT can be used for a wide variety of language tasks, while only adding a small layer to the core model:<br>\n1. Classification tasks such as sentiment analysis are done similarly to Next Sentence classification, by adding a classification layer on top of the Transformer output for the [CLS] token.<br>\n2. In Question Answering tasks (e.g. SQuAD v1.1), the software receives a question regarding a text sequence and is required to mark the answer in the sequence. Using BERT, a Q&A model can be trained by learning two extra vectors that mark the beginning and the end of the answer.<br>\n3. In Named Entity Recognition (NER), the software receives a text sequence and is required to mark the various types of entities (Person, Organization, Date, etc) that appear in the text. Using BERT, a NER model can be trained by feeding the output vector of each token into a classification layer that predicts the NER label.<br>\n\n## 3.4 Takeaways:\n1. Model size matters, even at huge scale. BERT_large, with 345 million parameters, is the largest model of its kind. It is demonstrably superior on small-scale tasks to BERT_base, which uses the same architecture with \u201conly\u201d 110 million parameters.<br>\n2. With enough training data, more training steps == higher accuracy. For instance, on the MNLI task, the BERT_base accuracy improves by 1.0% when trained on 1M steps (128,000 words batch size) compared to 500K steps with the same batch size.<br>\n3. BERT\u2019s bidirectional approach (MLM) converges slower than left-to-right approaches (because only 15% of words are predicted in each batch) but bidirectional training still outperforms left-to-right training after a small number of pre-training steps.<br>\n\n![](https:\/\/miro.medium.com\/max\/1773\/0*KONsqvDohE7ytu_E.png)\n\n## 3.5 Compute considerations (training and applying):<br>\n![](https:\/\/miro.medium.com\/max\/875\/1*LgbpLsRUGbtTPmMSUO2Drw.png)<br>","bf2e01e9":"## 4.1 Importing necessary modules","0f72b8f7":"<a href=\"#top\" class=\"btn btn-primary btn-lg active\" role=\"button\" aria-pressed=\"true\">Go to TOP<\/a>","d9d061bd":"# 2. Introduction to the Transformer:<br>\nThe Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. The idea behind Transformer is to handle the dependencies between input and output with attention and recurrence completely.<br>\n## 2.1 Transformer\u2019s Model Architecture:<br>\n![](https:\/\/s3-ap-south-1.amazonaws.com\/av-blog-media\/wp-content\/uploads\/2019\/06\/Screenshot-from-2019-06-17-19-53-10.png) <br>\nLet\u2019s first focus on the Encoder and Decoder parts only.Now focus on the below image. The Encoder block has 1 layer of a Multi-Head Attention followed by another layer of Feed Forward Neural Network. The decoder, on the other hand, has an extra Masked Multi-Head Attention.<br>\n![](https:\/\/s3-ap-south-1.amazonaws.com\/av-blog-media\/wp-content\/uploads\/2019\/06\/Screenshot-from-2019-06-17-20-01-32.png)<br>\n\n\n**Encoder**: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection around each ofthe two sub-layers, followed by layer normalization. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)),where Sublayer(x) is the function implemented by the sub-layeritself. To facilitate these residual connections, all sub-layers in the model, as well as the embeddinglayers, produce outputs of dimension dmodel = 512.<br>\n\n\n**Decoder**: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i.<br>\n\nThe encoder and decoder blocks are actually multiple identical encoders and decoders stacked on top of each other. Both the encoder stack and the decoder stack have the same number of units.The number of encoder and decoder units is a hyperparameter <br>\n\n![](https:\/\/s3-ap-south-1.amazonaws.com\/av-blog-media\/wp-content\/uploads\/2019\/06\/Screenshot-from-2019-06-17-20-03-14.png)<br>\n\nLet\u2019s see how this setup of the encoder and the decoder stack works:<br>\n\n* The word embeddings of the input sequence are passed to the first encoder <br>\n* These are then transformed and propagated to the next encoder<br>\n* The output from the last encoder in the encoder-stack is passed to all the decoders in the decoder-stack as shown in the figure below:<br>\n\nAn important thing to note here \u2013 in addition to the self-attention and feed-forward layers, the decoders also have one more layer of Encoder-Decoder Attention layer. This helps the decoder focus on the appropriate parts of the input sequence.<br>\n\n## 2.2 Understanding self attention: <br>\nSelf-attention, sometimes called intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.<br>\n![](https:\/\/s3-ap-south-1.amazonaws.com\/av-blog-media\/wp-content\/uploads\/2019\/06\/Screenshot-from-2019-06-17-22-31-11.png)<br>\nTake a look at the above image. Can you figure out what the term \u201cit\u201d in this sentence refers to?<br>\n\nIs it referring to the street or to the animal? It\u2019s a simple question for us but not for an algorithm. When the model is processing the word \u201cit\u201d, self-attention tries to associate \u201cit\u201d with \u201canimal\u201d in the same sentence.<br>\n\nSelf-attention allows the model to look at the other words in the input sequence to get a better understanding of a certain word in the sequence.<br>\n\nSelf-attention is computed not once but multiple times in the Transformer\u2019s architecture, in parallel and independently. It is therefore referred to as Multi-head Attention. The outputs are concatenated and linearly transformed as shown in the figure below:\n![](https:\/\/s3-ap-south-1.amazonaws.com\/av-blog-media\/wp-content\/uploads\/2019\/06\/Screenshot-from-2019-06-17-22-47-53.png)<br>\n\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions.<br>\n\n## 2.3 Limitations of the Transformer:<br>\nAttention can only deal with fixed-length text strings. The text has to be split into a certain number of segments or chunks before being fed into the system as input\nThis chunking of text causes context fragmentation. For example, if a sentence is split from the middle, then a significant amount of context is lost. In other words, the text is split without respecting the sentence or any other semantic boundary <br>\n\nBut,undoutedly transformer inspired BERT and all the following breakthroughs in NLP.","f725e778":"### Getting tokenizer","857bc39e":"# Introduction:<br>\n### Welcome! to this kernel.In this kernel we are going to explore Google's BERT.I will try to cover all the topics which helps to understand BERT clearly.Finally,we will see how to implement BERT for disaster NLP \ud83d\ude0a .<br>\n###  <font color='blue'>Without wasting time lets get started.<\/font> <br>\n","d78c3a57":"### References and credits:\n1. [BERT Explained: State of the art language model for NLP](https:\/\/towardsdatascience.com\/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)<br>\n2. [How do Transformers Work in NLP? A Guide to the Latest State-of-the-Art Models](https:\/\/www.analyticsvidhya.com\/blog\/2019\/06\/understanding-transformers-nlp-state-of-the-art-models\/)<br>\n3. [Demystifying BERT: A Comprehensive Guide to the Groundbreaking NLP Framework](https:\/\/www.analyticsvidhya.com\/blog\/2019\/09\/demystifying-bert-groundbreaking-nlp-framework\/)\n4. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https:\/\/arxiv.org\/pdf\/1810.04805.pdf)<br>\n5. [Attention Is All You Need](https:\/\/arxiv.org\/pdf\/1706.03762.pdf)<br>\n6. [Disaster NLP: Keras BERT using TFHub](https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub)<br>\nThanks to all the above blogs and article contributors.Thanks you very much for you open source contibutions.","983e4cba":"## 4.2 Helper Functions:","487122c5":"## 4.3 Loading BERT from the Tensorflow Hub:","f348fd7a":"## 4.5 Loading tokenizer from the bert layer:","021b7a88":"## 4.6 Encoding the text into tokens, masks, and segment flags:<br>","68e6d416":"## 4.7 Model: Build, Train, Predict, Submit:","6ca286fe":"# 4. Implementation of BERT using TFHub: <br>\n#### Credits : This part was taken from [this kernel](https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub) please refer to this awesome kernel and consider upvoting.Thanks to the author of the kernel [xhlulu](https:\/\/www.kaggle.com\/xhlulu)","831239bb":"\n# <font color='red'>Please consider Upvoting if you like this kernel.<\/font> <br>\n# Thank you for reading this kernel.<br>\n# Happy learning & sharing \ud83d\ude0a<br>","ce169fae":"# 1. Intution behind RNN based Sequence-to-Sequence Model:\nSequence-to-sequence (seq2seq) models in NLP are used to convert sequences of Type A to sequences of Type B. For example, translation of English sentences to German sentences is a sequence-to-sequence task.<br>\nLet\u2019s take a simple example of a sequence-to-sequence model. Check out the below illustration:<br>\n![](https:\/\/s3-ap-south-1.amazonaws.com\/av-blog-media\/wp-content\/uploads\/2019\/06\/seq2seq.gif)<br>\nThe above seq2seq model is converting a German phrase to its English counterpart. Let\u2019s break it down:<br>\n\n* Both Encoder and Decoder are RNNs\n* At every time step in the Encoder, the RNN takes a word vector (xi) from the input sequence and a hidden state (Hi) from the previous time step\n* The hidden state is updated at each time step\n* The hidden state from the last unit is known as the context vector. This contains information about the input sequence\n* This context vector is then passed to the decoder and it is then used to generate the target sequence (English phrase)\n* If we use the Attention mechanism, then the weighted sum of the hidden states are passed as the context vector to the decoder\n\n# 1.1 Limitations of RNN'S:\n* Dealing with long-range dependencies is still challenging\n* The sequential nature of the model architecture prevents parallelization.\n* Vanishing Gradient Problem\n* Issue of increasing gradients at each step called as exploding gradients."}}