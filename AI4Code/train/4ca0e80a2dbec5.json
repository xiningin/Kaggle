{"cell_type":{"d0aa8065":"code","cbdb0984":"code","da258802":"code","d057ff44":"code","1ddbdd10":"code","366ec002":"code","1ffeda9e":"code","c12b77af":"code","df65aee4":"code","38d0a916":"code","aa209db6":"code","272796bd":"code","34f50770":"code","251d627c":"code","3c3a308c":"code","e8d5aac7":"code","856f48fa":"markdown","76cdf141":"markdown","f7759397":"markdown","eb595b5a":"markdown","c50908b6":"markdown","e855579e":"markdown","30b59b68":"markdown","1389fb19":"markdown","ba378877":"markdown","7ad94926":"markdown","7d4e1628":"markdown","aae9fdda":"markdown","aa330b5a":"markdown","d081f026":"markdown","2c94e91d":"markdown"},"source":{"d0aa8065":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cbdb0984":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","da258802":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","d057ff44":"women = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\n\nprint(\"% of women who survived:\", rate_women)","1ddbdd10":"men = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\n\nprint(\"% of men who survived:\", rate_men)","366ec002":"# from sklearn.ensemble import RandomForestClassifier\n\n# y = train_data[\"Survived\"]\n\n# features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\n# X = pd.get_dummies(train_data[features])\n# X_test = pd.get_dummies(test_data[features])\n\n# model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\n# model.fit(X, y)\n# predictions = model.predict(X_test)\n\n# output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n# output.to_csv('my_submission.csv', index=False)\n# print(\"Your submission was successfully saved!\")","1ffeda9e":"# from sklearn.ensemble import RandomForestClassifier\n\n# y = train_data[\"Survived\"]\n\n# features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Age\"]\n# X = pd.get_dummies(train_data[features])\n# X_test = pd.get_dummies(test_data[features]) \n\n# X.Age.fillna(X.Age.mean(), inplace=True) # Fill missing values in Age with mean value\n# X_test.Age.fillna(X_test.Age.mean(), inplace=True)\n\n# model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\n# model.fit(X, y)\n# predictions = model.predict(X_test)\n\n# output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n# output.to_csv('my_submission.csv', index=False)\n# print(\"Your submission was successfully saved!\")","c12b77af":"from sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Age\", \"Embarked\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features]) \n\nX.Age.fillna(X.Age.mean(), inplace=True) # Fill missing values in Age with mean value\nX_test.Age.fillna(X_test.Age.mean(), inplace=True)\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","df65aee4":"# from sklearn.ensemble import RandomForestClassifier\n\n# y = train_data[\"Survived\"]\n\n# features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Age\", \"Embarked\"]\n# X = pd.get_dummies(train_data[features])\n# X_test = pd.get_dummies(test_data[features]) \n\n# X.Age.fillna(X.Age.mean(), inplace=True) # Fill missing values in Age with mean value\n# X_test.Age.fillna(X_test.Age.mean(), inplace=True)\n\n# model = RandomForestClassifier(n_estimators=1000, max_depth=None, random_state=1)\n# model.fit(X, y)\n# predictions = model.predict(X_test)\n\n# output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n# output.to_csv('my_submission.csv', index=False)\n# print(\"Your submission was successfully saved!\")","38d0a916":"# from sklearn.linear_model import LogisticRegression\n\n# y = train_data[\"Survived\"]\n\n# features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Age\", \"Embarked\"]\n# X = pd.get_dummies(train_data[features])\n# X_test = pd.get_dummies(test_data[features]) \n\n# X.Age.fillna(X.Age.mean(), inplace=True) # Fill missing values in Age with mean value\n# X_test.Age.fillna(X_test.Age.mean(), inplace=True)\n\n# model = LogisticRegression(max_iter=1000) # Increase max_iter=1000 (from default 100) for convergence\n# model.fit(X, y)\n# predictions = model.predict(X_test)\n\n# output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n# output.to_csv('my_submission.csv', index=False)\n# print(\"Your submission was successfully saved!\")","aa209db6":"# from sklearn.neighbors import KNeighborsClassifier\n\n# y = train_data[\"Survived\"]\n\n# features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Age\", \"Embarked\"]\n# X = pd.get_dummies(train_data[features])\n# X_test = pd.get_dummies(test_data[features]) \n\n# X.Age.fillna(X.Age.mean(), inplace=True) # Fill missing values in Age with mean value\n# X_test.Age.fillna(X_test.Age.mean(), inplace=True)\n\n# model = KNeighborsClassifier(n_neighbors=5) # Use default settings for all other parameters\n# model.fit(X, y)\n# predictions = model.predict(X_test)\n\n# output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n# output.to_csv('my_submission.csv', index=False)\n# print(\"Your submission was successfully saved!\")","272796bd":"# y = train_data[\"Survived\"]\n\n# features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Age\", \"Embarked\"]\n# X = pd.get_dummies(train_data[features])\n# X_test = pd.get_dummies(test_data[features]) \n\n# X.Age.fillna(X.Age.mean(), inplace=True) # Fill missing values in Age with mean value\n# X_test.Age.fillna(X_test.Age.mean(), inplace=True)\n\n# from sklearn.ensemble import RandomForestClassifier\n# model_rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\n# model_rf.fit(X, y)\n\n# from sklearn.linear_model import LogisticRegression\n# model_lr = LogisticRegression(max_iter=1000) # All other parameters default, default max_iter=100\n# model_lr.fit(X, y)\n\n# from sklearn.neighbors import KNeighborsClassifier\n# model_knn = KNeighborsClassifier(n_neighbors=5) # Use default settings for all other parameters\n# model_knn.fit(X, y)\n\n# from sklearn.ensemble import VotingClassifier\n# estimators =[('rf', model_rf), ('lr', model_lr), ('knn', model_knn)]\n# ensemble = VotingClassifier(estimators, voting='hard') # 'hard' = majority vote\n# ensemble.fit(X, y)\n\n# predictions = ensemble.predict(X_test)\n\n# output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n# output.to_csv('my_submission.csv', index=False)\n# print(\"Your submission was successfully saved!\")","34f50770":"# from sklearn.ensemble import RandomForestClassifier\n\n# y = train_data[\"Survived\"]\n\n# names_train = train_data[['Name']].copy() # Split Name into Name_First, Name_Title, Name_Last\n# names_train [['Name_Last', 'Name_Temp']] = names_train['Name'].str.split(',', expand=True)\n# names_train [['Name_Title', 'Name_First']] = names_train['Name_Temp'].str.split('.', n=1, expand=True)\n# names_train = names_train.drop('Name_Temp', 1)\n\n# names_test = test_data[['Name']].copy() # Split Name into Name_First, Name_Title, Name_Last\n# names_test [['Name_Last', 'Name_Temp']] = names_test['Name'].str.split(',', n=1, expand=True)\n# names_test [['Name_Title', 'Name_First']] = names_test['Name_Temp'].str.split('.', n=1, expand=True)\n# names_test = names_test.drop('Name_Temp', 1)\n\n# features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Age\", \"Embarked\"]\n# X = pd.get_dummies(train_data[features])\n# X_test = pd.get_dummies(test_data[features]) \n\n# X['Name_Title'] = names_train['Name_Title'] # Append Name_Title to train_data and test_data\n# X_test['Name_Title'] = names_test['Name_Title']\n\n# X['Age'] = X['Age'].fillna(X.groupby(['Name_Title', 'Pclass'])['Age'].transform('mean')) # Fill missing Age with mean of Name_Title\/Pclass combination e.g. Mr\/3\n# X['Age'] = X['Age'].fillna(X['Age'].mean()) # Fill any remaining blanks with simple mean of all Ages\n\n# X_test['Age'] = X_test['Age'].fillna(X_test.groupby(['Name_Title', 'Pclass'])['Age'].transform('mean'))\n# X_test['Age'] = X_test['Age'].fillna(X_test['Age'].mean()) \n\n# X = X.drop('Name_Title', 1) # Drop Name_Title as only used for filling missing Age, not for modelling\n# X_test = X_test.drop('Name_Title', 1)\n\n# model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\n# model.fit(X, y)\n# predictions = model.predict(X_test)\n\n# output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n# output.to_csv('my_submission.csv', index=False)\n# print(\"Your submission was successfully saved!\")","251d627c":"# from sklearn.linear_model import LogisticRegression\n\n# y = train_data[\"Survived\"]\n\n# names_train = train_data[['Name']].copy() # Split Name into Name_First, Name_Title, Name_Last\n# names_train [['Name_Last', 'Name_Temp']] = names_train['Name'].str.split(',', expand=True)\n# names_train [['Name_Title', 'Name_First']] = names_train['Name_Temp'].str.split('.', n=1, expand=True)\n# names_train = names_train.drop('Name_Temp', 1)\n\n# names_test = test_data[['Name']].copy() # Split Name into Name_First, Name_Title, Name_Last\n# names_test [['Name_Last', 'Name_Temp']] = names_test['Name'].str.split(',', n=1, expand=True)\n# names_test [['Name_Title', 'Name_First']] = names_test['Name_Temp'].str.split('.', n=1, expand=True)\n# names_test = names_test.drop('Name_Temp', 1)\n\n# features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Age\", \"Embarked\"]\n# X = pd.get_dummies(train_data[features])\n# X_test = pd.get_dummies(test_data[features]) \n\n# X['Name_Title'] = names_train['Name_Title'] # Append Name_Title to train_data and test_data\n# X_test['Name_Title'] = names_test['Name_Title']\n\n# X['Age'] = X['Age'].fillna(X.groupby(['Name_Title', 'Pclass'])['Age'].transform('mean')) # Fill missing Age with mean of Name_Title\/Pclass combination e.g. Mr\/3\n# X['Age'] = X['Age'].fillna(X['Age'].mean()) # Fill any remaining blanks with simple mean of all Ages\n\n# X_test['Age'] = X_test['Age'].fillna(X_test.groupby(['Name_Title', 'Pclass'])['Age'].transform('mean'))\n# X_test['Age'] = X_test['Age'].fillna(X_test['Age'].mean()) \n\n# X = X.drop('Name_Title', 1) # Drop Name_Title as only used for filling missing Age, not for modelling\n# X_test = X_test.drop('Name_Title', 1)\n\n# model = LogisticRegression(max_iter=1000) # Increase max_iter=1000 (from default 100) for convergence\n# model.fit(X, y)\n# predictions = model.predict(X_test)\n\n# output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n# output.to_csv('my_submission.csv', index=False)\n# print(\"Your submission was successfully saved!\")","3c3a308c":"# from sklearn.neighbors import KNeighborsClassifier\n\n# y = train_data[\"Survived\"]\n\n# names_train = train_data[['Name']].copy() # Split Name into Name_First, Name_Title, Name_Last\n# names_train [['Name_Last', 'Name_Temp']] = names_train['Name'].str.split(',', expand=True)\n# names_train [['Name_Title', 'Name_First']] = names_train['Name_Temp'].str.split('.', n=1, expand=True)\n# names_train = names_train.drop('Name_Temp', 1)\n\n# names_test = test_data[['Name']].copy() # Split Name into Name_First, Name_Title, Name_Last\n# names_test [['Name_Last', 'Name_Temp']] = names_test['Name'].str.split(',', n=1, expand=True)\n# names_test [['Name_Title', 'Name_First']] = names_test['Name_Temp'].str.split('.', n=1, expand=True)\n# names_test = names_test.drop('Name_Temp', 1)\n\n# features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Age\", \"Embarked\"]\n# X = pd.get_dummies(train_data[features])\n# X_test = pd.get_dummies(test_data[features]) \n\n# X['Name_Title'] = names_train['Name_Title'] # Append Name_Title to train_data and test_data\n# X_test['Name_Title'] = names_test['Name_Title']\n\n# X['Age'] = X['Age'].fillna(X.groupby(['Name_Title', 'Pclass'])['Age'].transform('mean')) # Fill missing Age with mean of Name_Title\/Pclass combination e.g. Mr\/3\n# X['Age'] = X['Age'].fillna(X['Age'].mean()) # Fill any remaining blanks with simple mean of all Ages\n\n# X_test['Age'] = X_test['Age'].fillna(X_test.groupby(['Name_Title', 'Pclass'])['Age'].transform('mean'))\n# X_test['Age'] = X_test['Age'].fillna(X_test['Age'].mean()) \n\n# X = X.drop('Name_Title', 1) # Drop Name_Title as only used for filling missing Age, not for modelling\n# X_test = X_test.drop('Name_Title', 1)\n\n# model = KNeighborsClassifier(n_neighbors=10) # Use default settings for all other parameters\n# model.fit(X, y)\n# predictions = model.predict(X_test)\n\n# output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n# output.to_csv('my_submission.csv', index=False)\n# print(\"Your submission was successfully saved!\")","e8d5aac7":"# y = train_data[\"Survived\"]\n\n# names_train = train_data[['Name']].copy() # Split Name into Name_First, Name_Title, Name_Last\n# names_train [['Name_Last', 'Name_Temp']] = names_train['Name'].str.split(',', expand=True)\n# names_train [['Name_Title', 'Name_First']] = names_train['Name_Temp'].str.split('.', n=1, expand=True)\n# names_train = names_train.drop('Name_Temp', 1)\n\n# names_test = test_data[['Name']].copy() # Split Name into Name_First, Name_Title, Name_Last\n# names_test [['Name_Last', 'Name_Temp']] = names_test['Name'].str.split(',', n=1, expand=True)\n# names_test [['Name_Title', 'Name_First']] = names_test['Name_Temp'].str.split('.', n=1, expand=True)\n# names_test = names_test.drop('Name_Temp', 1)\n\n# features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Age\", \"Embarked\"]\n# X = pd.get_dummies(train_data[features])\n# X_test = pd.get_dummies(test_data[features]) \n\n# X['Name_Title'] = names_train['Name_Title'] # Append Name_Title to train_data and test_data\n# X_test['Name_Title'] = names_test['Name_Title']\n\n# X['Age'] = X['Age'].fillna(X.groupby(['Name_Title', 'Pclass'])['Age'].transform('mean')) # Fill missing Age with mean of Name_Title\/Pclass combination e.g. Mr\/3\n# X['Age'] = X['Age'].fillna(X['Age'].mean()) # Fill any remaining blanks with simple mean of all Ages\n\n# X_test['Age'] = X_test['Age'].fillna(X_test.groupby(['Name_Title', 'Pclass'])['Age'].transform('mean'))\n# X_test['Age'] = X_test['Age'].fillna(X_test['Age'].mean()) \n\n# X = X.drop('Name_Title', 1) # Drop Name_Title as only used for filling missing Age, not for modelling\n# X_test = X_test.drop('Name_Title', 1)\n\n# from sklearn.ensemble import RandomForestClassifier\n# model_rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\n# model_rf.fit(X, y)\n\n# from sklearn.linear_model import LogisticRegression\n# model_lr = LogisticRegression(max_iter=1000) # All other parameters default, default max_iter=100\n# model_lr.fit(X, y)\n\n# from sklearn.neighbors import KNeighborsClassifier\n# model_knn = KNeighborsClassifier(n_neighbors=10) # Use default settings for all other parameters\n# model_knn.fit(X, y)\n\n# from sklearn.ensemble import VotingClassifier\n# estimators =[('rf', model_rf), ('lr', model_lr), ('knn', model_knn)]\n# ensemble = VotingClassifier(estimators, voting='hard') # 'hard' = majority vote\n# ensemble.fit(X, y)\n\n# predictions = ensemble.predict(X_test)\n\n# output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n# output.to_csv('my_submission.csv', index=False)\n# print(\"Your submission was successfully saved!\")","856f48fa":"# Mark 11 [0.71770]\nTry k-Nearest Neighbour model using 10 neighbours <br\/><br\/>\nFeatures = Sex, Pclass, SibSp, Parch, Age (fill blanks with mean of Name_Title\/Pclass combo), Embarked <br\/><br\/> \nModel = k-Nearest Neighbour (n_neighbours=10)\n","76cdf141":"# Mark 09 [0.77990]\nChange Age missing data filling approach to mean of each Name_Title\/Pclass combination, where Name_Title (Mr, Miss, Master etc) was extracted from Name; and reverting back to Random Forest model from Mark 03 <br\/><br\/>\nFeatures = Sex, Pclass, SibSp, Parch, Age (fill blanks with mean of Name_Title\/Pclass combo), Embarked <br\/><br\/> \nModel = Random Forest (n_estimators=100, max_depth=5)","f7759397":"# Taking the Shotgun Approach *(a.k.a. throwing pasta on the ceiling and seeing what sticks)* \n\nAfter completing Alexis Cook's very useful Titanic Tutorial, I couldn't help myself and spent a couple of days hacking around to try and improve my score without going through the usual data science workflow of EDA, feature engineering, model selection, hyperparameter tuning, train\/test iterations. \n\nI know it's not the proper way of doing data science, but like I said, I just couldn't help myself.\n=)\n\nThe Titantic Tutorial provided two possible models: Gender Submission and Random Forest, which I will label as Mark 01 and Mark 02, that scored 0.76555 and 0.77511 respectively.\n\nGiven that I had a ready notebook to play with, I decided to make small adjustments to see how they would change the score. Changes would be made along two paths: (i) changing the input features and filling missing data, and (ii) changing the machine learning model.\n\nHere's what I tried:\n* **Mark 01**: Download and submit gender_submission.csv from Titanic Tutorial <br\/>*i.e. Features = Sex; Model = All women survied, all men died*\n* **Mark 02**: Copy\/paste Python code from Titanic Tutorial <br\/>*i.e. Features = Sex, Pclass, SibSp, Parch; Model = Random Forest (max_iter=100, depth=5)*\n* **Mark 03**: Extend features by adding Age, with missing data filled using mean of available data <br\/>*i.e. Features = Sex, Pclass, SibSp, Parch, Age (fill blanks with mean); Model = Random Forest (n_estimators=100, max_depth=5)*\n* **Mark 04**: Extend features by adding Embarked <br\/>*i.e. Features = Sex, Pclass, SibSp, Parch, Age (fill blanks with mean), Embarked; Model = Random Forest (n_estimators=100, max_depth=5)*\n* **Mark 05**: Tweak Random Forest model by increasing number of trees and removing cap on depth <br\/>*i.e. Features = Sex, Pclass, SibSp, Parch, Age (fill blanks with mean), Embarked; Model = Random Forest (n_estimators=1000, max_depth=None)*\n* **Mark 06**: Try Logistic Regression model <br\/>*i.e. Features = Sex, Pclass, SibSp, Parch, Age (fill blanks with mean), Embarked; Model = Logistic Regression (max_iteration=1000)*\n* **Mark 07**: Try k-Nearest Neighbour (kNN) model <br\/>*i.e. Features = Sex, Pclass, SibSp, Parch, Age (fill blanks with mean), Embarked; Model = k-Nearest Neighbour (n_neighbours=5)*\n* **Mark 08**: Try Ensemble model using Mark 05, 06 and 07 <br\/>*i.e. Features = Sex, Pclass, SibSp, Parch, Age (fill blanks with mean), Embarked; Model = Ensemble of Mark 05, 06, 07 (voting='hard')*\n* **Mark 09**: Change Age missing data filling approach to mean of each Name_Title\/Pclass combination, where Name_Title (Mr, Miss, Master etc) was extracted from Name; and reverting back to Random Forest model from Mark 03 <br\/>*i.e. Features = Sex, Pclass, SibSp, Parch, Age (fill blanks with mean of Name_Title\/Pclass combo), Embarked; Model = Random Forest (n_estimators=100, max_depth=5)* \n* **Mark 10**: Try Logistic Regression model <br\/>*i.e. Features = Sex, Pclass, SibSp, Parch, Age (fill blanks with mean of Name_Title\/Pclass combo), Embarked; Model = Logistic Regression (max_iteration=1000)* \n* **Mark 11**: Try k-Nearest Neighbour model using 10 neighbours <br\/>*i.e. Features = Sex, Pclass, SibSp, Parch, Age (fill blanks with mean of Name_Title\/Pclass combo), Embarked; Model = k-Nearest Neighbour (n_neighbours=10)*\n* **Mark 12**: Try Ensemble model using Mark 09, 10, 11 <br\/>*i.e. Features = Sex, Pclass, SibSp, Parch, Age (fill blanks with mean of Name_Title\/Pclass combo), Embarked; Model = Ensemble of Mark 09, 10, 11 (voting='hard')*\n\n**Phew!**\n\nIf you're still reading this, thank you! And congratulations for having so much patience. \n\nIt sounds like a long list of attempts, but given that each successive try was a tweak of the earlier one, the process was basically a repetition of copy-paste-tweak-check-save-submit to get a new score. \n\nThis didn't take much time, except that I'm new to all this and had to pick up some simple Python and scikit-learn syntax along the way, courtesy of Google and stackoverflow.com. \n\nYes, I know that I should invest some time to pick up Python properly, and I plan to do that soon, but I managed to learn just enough to get the job done.\n\nOh, and since Kaggle restricts the number of submissions to 10 per 24-hour cycle, I had to do this over two days. \n\nIf you plan to fork this notebook and repeat what I did, note that you'll have to uncomment the code for the specific Mark you want to try, then \"Save & Run All (Commit)\" and submit the output in order to get a new score. \n\nRunning a different Mark means having to comment out the previously-run Mark and uncommenting the new one. Clunky, I know, but that's what I did. \n\nTip: To comment\/uncomment many lines of code quickly, select and use Ctrl-\/ \n\nI'll indicate the score for each Mark in [square brackets], and summarise them for Marks 01 to 12 in a table at the end of the notebook. So, keep reading if you're interested to know what they were; or simply scroll all the way down to the bottom.\n\n","eb595b5a":"# Mark 05 [0.74641]\nTweak Random Forest model by increasing number of trees and removing cap on depth <br\/><br\/>\nFeatures = Sex, Pclass, SibSp, Parch, Age (fill blanks with mean), Embarked <br\/><br\/>\nModel = Random Forest (n_estimators=1000, max_depth=None)","c50908b6":"# Mark 10 [0.76794]\nTry Logistic Regression model <br\/><br\/>\nFeatures = Sex, Pclass, SibSp, Parch, Age (fill blanks with mean of Name_Title\/Pclass combo), Embarked <br\/><br\/>\nModel = Logistic Regression (max_iteration=1000)\n","e855579e":"# Mark 02 [0.77511]\nCopy\/paste Python code from Titanic Tutorial <br\/><br\/>\nFeatures = Sex, Pclass, SibSp, Parch <br\/><br\/>\nModel = Random Forest (max_iter=100, depth=5)","30b59b68":"# Mark 06 [0.76794]\nTry Logistic Regression model <br\/><br\/>\nFeatures = Sex, Pclass, SibSp, Parch, Age (fill blanks with mean), Embarked <br\/><br\/>\nModel = Logistic Regression (max_iteration=1000)","1389fb19":"# Mark 08 [0.77990]\nTry Ensemble model using Mark 05, 06 and 07 <br\/><br\/>\nFeatures = Sex, Pclass, SibSp, Parch, Age (fill blanks with mean), Embarked <br\/><br\/>\nModel = Ensemble of Mark 05, 06, 07 (voting='hard')\n","ba378877":"# Copy\/Paste from Titanic Tutorial","7ad94926":"# Summary\nMark 01 = 0.76555 <br\/>\nMark 02 = 0.77511 <br\/>\nMark 03 = 0.77033 <br\/>\nMark 04 = 0.78468 * <br\/>\nMark 05 = 0.74641 <br\/>\nMark 06 = 0.76794 <br\/>\nMark 07 = 0.72248 <br\/>\nMark 08 = 0.77990 <br\/>\nMark 09 = 0.77990 <br\/>\nMark 10 = 0.76794 <br\/>\nMark 11 = 0.71770 <br\/>\nMark 12 = 0.77751\n\n\\**highest score among Marks 01-12*\n\nIt's easier to visualise the results in a heatmap table, with lowest score in yellow and highest in dark green. \n\n<img src=\"https:\/\/firefortysix.files.wordpress.com\/2020\/08\/titanic-competition-shotgun-approach.jpg\" align=\"left\" width=\"800px\" alt=\"Titanic competition: Comparing scores of different feature\/model choices - firefortysix.com\/data-science\"\/>\n\n<br clear=\"all\"\/><br\/>\n\nSome quick observations:\n* Adding features seem to improve scores more than using different models. Although it could be that the Random Forest model in Titanic Tutorial was already quite good, better than Logistic Regression or kNN.\n* Changing the filling method of missing Age data from a simple mean of all available vs a slightly more involved mean of Name_Title\/Pclass combo didn't improve scores, in fact it made them slightly worse.\n* Using an Ensemble model did not improve scores of the best performing Random Forest model. Again, it could be that the Random Forest model was much better than the other two other models that using an ensemble made final predictions worse.\n* It seems that using a fairly complex Random Forest model with many features didn't really improve the super simple \"all women survive, all men die\" model by a lot. The score only increased by a small 2.5%, showing that sometimes, simple is best. \n\nWhile this hacking around is clearly not the right way to approach a data science problem, it did help scratch my itch of quickly trying different settings and seeing how they performed vs each other. I hope you found these results entertaining at least. \n\nNow, back to doing things \"right\" and starting from scratch with EDA in a new notebook.","7d4e1628":"# Mark 07 [0.72248]\nTry k-Nearest Neighbour (kNN) model <br\/><br\/>\nFeatures = Sex, Pclass, SibSp, Parch, Age (fill blanks with mean), Embarked <br\/><br\/> \nModel = k-Nearest Neighbour (n_neighbours=5)","aae9fdda":"# Mark 04 [0.78468]\nExtend features by adding Embarked <br\/><br\/>\nFeatures = Sex, Pclass, SibSp, Parch, Age (fill blanks with mean), Embarked <br\/><br\/>\nModel = Random Forest (n_estimators=100, max_depth=5)","aa330b5a":"# Mark 12 [0.77751]\nTry Ensemble model using Mark 09, 10, 11 <br\/><br\/>\nFeatures = Sex, Pclass, SibSp, Parch, Age (fill blanks with mean of Name_Title\/Pclass combo), Embarked <br\/><br\/>\nModel = Ensemble of Mark 09, 10, 11 (voting='hard')","d081f026":"# Mark 01 [0.76555]\nDownload and submit gender_submission.csv from Titanic Tutorial <br\/><br\/>\nFeatures = Sex <br\/><br\/>\nModel = All women survied, all men died","2c94e91d":"# Mark 03 [0.77033]\nExtend features by adding Age, with missing data filled using mean of available data <br\/><br\/>\nFeatures = Sex, Pclass, SibSp, Parch, Age (fill blanks with mean) <br\/><br\/>\nModel = Random Forest (n_estimators=100, max_depth=5)"}}