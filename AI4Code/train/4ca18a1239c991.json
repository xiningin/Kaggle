{"cell_type":{"f9e00a8f":"code","25555463":"code","4f036113":"code","3c844d8f":"code","329738e0":"code","48e5e629":"code","844c06d1":"code","ceef3398":"code","ad972a83":"code","383139b5":"code","f34835be":"code","bb054591":"code","6a54b22b":"code","70e0a04f":"code","d8f050d2":"code","af59beb9":"code","602a259a":"code","367d1ae3":"code","f330c4cb":"code","3801292d":"code","1817ef59":"code","ea9f0fa2":"code","e7a01753":"code","338fc7e5":"code","c8aef1cd":"code","6c20fa71":"code","f06be4d9":"code","93297407":"code","308f3bcc":"code","55b4e430":"code","6eeea7e4":"code","52d97581":"code","aec79522":"code","20aa3f05":"code","3131d058":"code","b76654ca":"code","4f59c360":"code","aad0ecc2":"code","c613b9c7":"code","9dc9b1a0":"code","34ccb5fd":"code","aa81a529":"code","70b7a862":"markdown","c6acbff2":"markdown","12a40869":"markdown","6f4a126a":"markdown","e0247fd7":"markdown","859d8e5d":"markdown","1ef29f87":"markdown","39e2e5f8":"markdown","5fb521ca":"markdown","a7916db3":"markdown","1827ddc6":"markdown","941f059a":"markdown","fef86989":"markdown","1398b63e":"markdown","b0fb9eba":"markdown"},"source":{"f9e00a8f":"import numpy as np\nimport pandas as pd\npd.set_option(\"display.max_columns\" , None)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport random\nimport os\nfrom tqdm import notebook\nimport gc\nfrom string import punctuation\nimport chardet\nimport re\nfrom datetime import datetime\n\nfrom sklearn.pipeline import Pipeline , make_pipeline\n\nnp.random.seed(0)\nrandom.seed(0)\npd.set_option(\"display.max_columns\" , None)\n\nfrom wordcloud import WordCloud\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer , PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import TweetTokenizer","25555463":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4f036113":"FILE_PATH = \"..\/input\/mbti-type\/mbti_1.csv\"\ntry:\n    #import data\n    data = pd.read_csv(FILE_PATH)\nexcept Exception:\n    with open(FILE_PATH , \"rb\") as f:\n        encoding = chardet.detect(f.read(10000))","3c844d8f":"data.head()","329738e0":"data.shape","48e5e629":"plt.rcParams[\"font.size\"] = 14","844c06d1":"plt.figure(figsize=(13,9))\nsns.countplot(data.type);","ceef3398":"data.head(3)","ad972a83":"# #Lowercase ALL\n# data[\"posts\"] = data.posts.str.lower()\n# #Strip the strings of spaces then apostrophe then  spaces again\n# data[\"posts\"] = data[\"posts\"].str.strip()\n# data[\"posts\"] = data[\"posts\"].str.strip(\"'\")\n# data[\"posts\"] = data[\"posts\"].str.strip()\n#Strip the type feature of spaces","383139b5":"# Just striping the string incase of any whitespace before or after the string\ndata[\"type\"] = data[\"type\"].str.strip()\n# Seperate the the label into four different parts\ntarget_multi_label = data[\"type\"].str.split(\"\" , expand=True)\ntarget_multi_label = target_multi_label.iloc[: , 1:-1]\ntarget_multi_label.columns = [\"Personality-1\",\"Personality-2\",\"Personality-3\",\"Personality-4\"]\ndata = pd.concat([data,target_multi_label] , axis=1)","f34835be":"data.head()","bb054591":"# Let's map them to their actual meaning to make more sense out of it\npersonality_map = {\n    \"I\":\"Introvert\",\n    \"E\":\"Extrovert\",\n    \"N\":\"Intuitive\",\n    \"S\":\"Sensitive\",\n    \"F\":\"Emotional\",\n    \"T\":\"Thinker\",\n    \"J\":\"Judgemental\",\n    \"P\":\"Perceiving\"\n}\n\nfor col in data.loc[: , \"Personality-1\":\"Personality-4\"].columns:\n    data[col] = data[col].map(personality_map)","6a54b22b":"data.head()","70e0a04f":"personalities = data.loc[: , \"Personality-1\":\"Personality-4\"].columns\nfor personality in personalities:\n    sns.countplot(data[personality])\n    plt.show()","d8f050d2":"def clean_data(text,clean_stopwords=True,clean_puntuation=True,clean_numbers=True):\n    text = text.lower()\n    text = text.strip()\n    text = text.strip(\"'\")\n    text = text.strip()\n    pattern = pattern = r\"\\|\\|\\|\"\n    f = lambda x : re.sub(pattern , \" \" , x)\n    text = f(text)\n    #Clean web links\n    pattern = r\"(https?:\\\/\\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*\"\n    f = lambda x: re.sub(pattern , \"\" , x)\n    text = f(text)\n    if clean_stopwords == True:\n        #Clean Stopwords\n        stopword = r\"|\".join([f\"\\s{word}\\s\" for word in stopwords.words(\"english\")])\n        f = lambda x : re.sub(stopword, \" \" , x)\n        text = f(text)\n    if clean_puntuation == True:\n        #Clean punctuations\n        #Let's not remove apostrohspe \"'' so our text can make more sense\n        punctuations = punctuation.replace(\"'\" , \"\")\n        punctuations = f\"[{punctuations}]\"\n        f = lambda x : re.sub(punctuations , \"\" , text)\n        text = f(text)\n    if clean_numbers == True:\n        #Clean Numbers\n        f = lambda x : re.sub(r\"[0-9]+\" , \"\" , x)\n        text = f(text)\n    return text","af59beb9":"data[\"clean-posts\"] = data[\"posts\"].apply(clean_data)","602a259a":"data.head()","367d1ae3":"introverts_clean_text_corpus = \" \".join([text for text in data[\"clean-posts\"][data[\"Personality-1\"] == \"Introvert\"]])\nextroverts_clean_text_corpus = \" \".join([text for text in data[\"clean-posts\"][data[\"Personality-1\"] == \"Extrovert\"]])\nintutives_clean_text_corpus = \" \".join([text for text in data[\"clean-posts\"][data[\"Personality-2\"] == \"Intuitive\"]])\nsensitive_clean_text_corpus = \" \".join([text for text in data[\"clean-posts\"][data[\"Personality-2\"] == \"Sensitive\"]])\nemotional_clean_text_corpus = \" \".join([text for text in data[\"clean-posts\"][data[\"Personality-3\"] == \"Emotional\"]])\nthinkers_clean_text_corpus = \" \".join([text for text in data[\"clean-posts\"][data[\"Personality-3\"] == \"Thinker\"]])\njudgemental_clean_text_corpus = \" \".join([text for text in data[\"clean-posts\"][data[\"Personality-4\"] == \"Judgemental\"]])\nperceivers_clean_text_corpus = \" \".join([text for text in data[\"clean-posts\"][data[\"Personality-4\"] == \"Perceiving\"]])","f330c4cb":"introverts_word_cloud = WordCloud(random_state=0).generate(introverts_clean_text_corpus)\nextroverts_word_cloud = WordCloud(random_state=0).generate(extroverts_clean_text_corpus)\nintuitives_word_cloud = WordCloud(random_state=0).generate(intutives_clean_text_corpus)\nsensitive_word_cloud = WordCloud(random_state=0).generate(sensitive_clean_text_corpus)\njudgemental_word_cloud = WordCloud(random_state=0).generate(judgemental_clean_text_corpus)\nperceivers_word_cloud = WordCloud(random_state=0).generate(perceivers_clean_text_corpus)","3801292d":"fig , ax = plt.subplots(3,2,figsize=(30,50))\nax = ax.flatten()\n\nax[0].imshow(introverts_word_cloud)\nax[0].set_title(\"Introverts\" , fontsize = 20)\nax[0].axis(False)\n\nax[1].imshow(extroverts_word_cloud)\nax[1].set_title(\"Extroverts\" , fontsize = 20)\nax[1].axis(False)\n\nax[2].imshow(intuitives_word_cloud)\nax[2].set_title(\"Intuitives\" , fontsize = 20)\nax[2].axis(False)\n\nax[3].imshow(sensitive_word_cloud)\nax[3].set_title(\"Sensitives\" , fontsize = 20)\nax[3].axis(False)\n\nax[4].imshow(judgemental_word_cloud)\nax[4].set_title(\"Judgemental\" , fontsize = 20)\nax[4].axis(False)\n\nax[5].imshow(perceivers_word_cloud)\nax[5].set_title(\"Perceivers\" , fontsize = 20)\nax[5].axis(False)\n\nplt.show()","1817ef59":"from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer\nfrom sklearn.model_selection import train_test_split","ea9f0fa2":"countVectorizer = CountVectorizer()\ntfidfVectorizer = TfidfVectorizer()","e7a01753":"train , test = train_test_split(data , test_size=0.2 , shuffle = False)\ntrain , val = train_test_split(train , test_size=0.1 , shuffle=False)","338fc7e5":"train.shape , val.shape , test.shape","c8aef1cd":"#For Bag of Words Vector\ntrain_bow = countVectorizer.fit_transform(train[\"clean-posts\"])\nval_bow = countVectorizer.transform(val[\"clean-posts\"])\ntest_bow = countVectorizer.transform(test[\"clean-posts\"])\n\n#For Tfidf Vector\ntrain_tfidf = tfidfVectorizer.fit_transform(train[\"clean-posts\"])\nval_tfidf = tfidfVectorizer.transform(val[\"clean-posts\"])\ntest_tfidf = tfidfVectorizer.transform(test[\"clean-posts\"])","6c20fa71":"train_bow.shape , val_bow.shape , test_bow.shape","f06be4d9":"train_tfidf.shape , val_tfidf.shape , test_tfidf.shape","93297407":"from sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.multiclass import OneVsOneClassifier , OneVsRestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score , precision_score , recall_score , f1_score , log_loss , confusion_matrix , classification_report","308f3bcc":"y_train = train.loc[: , \"Personality-1\":\"Personality-4\"].values\ny_val = val.loc[: , \"Personality-1\":\"Personality-4\"].values\ny_test = test.loc[: , \"Personality-1\":\"Personality-4\"].values\n\nmulitLabelBinarizer = MultiLabelBinarizer()\n\ny_train = mulitLabelBinarizer.fit_transform(y_train)\ny_val = mulitLabelBinarizer.transform(y_val)\ny_test = mulitLabelBinarizer.transform(y_test)\n\nclasses = mulitLabelBinarizer.classes_","55b4e430":"mulitLabelBinarizer.classes_","6eeea7e4":"y_train.shape","52d97581":"len(y_train[: , 0])","aec79522":"for num , personalti in enumerate(mulitLabelBinarizer.classes_):\n    X = train_bow\n    y = y_train[: , num]\n    lr = LogisticRegression()\n    lr.fit(X , y)\n    val_pred = lr.predict(val_bow)\n    test_pred = lr.predict(test_bow)\n    val_score = precision_score(y_val[:,num] , val_pred)\n    test_score = precision_score(y_test[:,num] , test_pred)\n    print(f\"Processing for {personalti}\")\n    print(f\"Validation Score: {val_score}\")\n    print(f\"Test Score: {test_score}\\n\")","20aa3f05":"ovr = OneVsRestClassifier(MultinomialNB() , n_jobs = -1)\novr.fit(train_bow , y_train)","3131d058":"#Validation\nval_pred = ovr.predict(val_bow)\nval_pred_proba = ovr.predict_proba(val_bow)\n# pd.DataFrame(val_pred , columns = classes)","b76654ca":"#Test\ntest_pred = ovr.predict(test_bow)\ntest_pred_proba = ovr.predict_proba(test_bow)\n# pd.DataFrame(test_pred , columns=classes)","4f59c360":"def check_scores(y_true , y_pred):\n    accuracy = accuracy_score(y_true , y_pred)\n    print(f\"Accuracy Score: {accuracy}\")\n    precision = precision_score(y_true , y_pred , average=\"samples\")\n    print(f\"Precision Score: {precision}\")\n    recall = recall_score(y_true , y_pred , average=\"samples\")\n    print(f\"Recall Score: {recall}\")\n    f1 = f1_score(y_true , y_pred , average=\"samples\")\n    print(f\"F1 Score: {f1}\")","aad0ecc2":"check_scores(y_val , val_pred)","c613b9c7":"ovr = OneVsRestClassifier(MultinomialNB() , n_jobs = -1)\novr.fit(train_tfidf , y_train)","9dc9b1a0":"#Validation\nval_pred = ovr.predict(val_tfidf)\nval_pred_proba = ovr.predict_proba(val_tfidf)\n","34ccb5fd":"#Test\ntest_pred = ovr.predict(test_tfidf)\ntest_pred_proba = ovr.predict_proba(test_tfidf)","aa81a529":"check_scores(y_val , val_pred)","70b7a862":"### Let's Split into Train and Test","c6acbff2":"There are 4 Personality Preferences each with 2 sub-category each (the two sub-category are of negating personalities so an Individual can only fall into one), in which individuals falls into a sub-category in the 4 preferences:\n1. Introverts or Extroverts\n2. Intuitives or Sensors (Sensitive)\n3. Feelers(Emotional) or Thinkers\n4. Judgers(Judgemental) or Perceivers(Perceiving)\n\nSo for example a person can be an **introvert** who is **sensitive** and **thinks** and is **judgemental**\n\nWe can see the way a person falls into 4 categories","12a40869":"## Tfidf","6f4a126a":"### Let's create the Bag of Words vectors and the Tfidf vectors","e0247fd7":"### Data Preprocessing","859d8e5d":"### Bag of Words","1ef29f87":"## Modelling","39e2e5f8":"## One versus Rest","5fb521ca":"![70929366_102936361110910_2831855586759409664_n.jpg](attachment:70929366_102936361110910_2831855586759409664_n.jpg)","a7916db3":"![https___i.pinimg.com_originals_8b_c5_73_8bc57357ccbc1bae1e9094c1b64947dd.jpg](attachment:https___i.pinimg.com_originals_8b_c5_73_8bc57357ccbc1bae1e9094c1b64947dd.jpg)","1827ddc6":"# <div align=\"center\">Personality Type Prediction<\/div>","941f059a":"### One Versus Rest Second Approach","fef86989":"#### Cleaning our data","1398b63e":"### One versus Rest First Approach","b0fb9eba":"There are 2 unique types of Personalities in each column(feature)...this could be 2 negating(two opposite personalities), so they were put together in the same column"}}