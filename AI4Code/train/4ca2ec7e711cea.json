{"cell_type":{"d88948c6":"code","16e702d6":"code","9ff7dc69":"code","149dfb9b":"code","c85e5811":"code","21ee2653":"code","dead1e04":"code","2dc45395":"code","2cf02729":"code","334d9a12":"code","9e925c10":"code","86c17299":"code","cb88eb48":"code","d1ecdc2a":"code","e98c54e6":"code","acdacf21":"code","65e564c7":"code","44ad9430":"code","d9bdec40":"code","683b82fe":"code","65b33538":"code","7959aa8a":"code","4881f278":"code","27732952":"code","d099af40":"code","e7336bd6":"code","a4c6aa5e":"code","8e637af2":"code","b894c866":"code","99c0da25":"code","cb88ca5d":"code","45023eda":"code","405261fd":"code","1cc98afe":"code","3695a905":"code","447ba7fb":"markdown","42bbc1ad":"markdown","f3105630":"markdown","9b520515":"markdown","8b9140ca":"markdown","2d1be8c9":"markdown","f5e1d8cf":"markdown","7522b54c":"markdown","54f3b565":"markdown","1d9380fe":"markdown","cd8a0bf3":"markdown","93c765d6":"markdown","d1534ce7":"markdown","a0a93ea0":"markdown","d7ddf410":"markdown"},"source":{"d88948c6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas.api.types import is_string_dtype\nfrom pandas.api.types import is_numeric_dtype\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport scipy.stats as stats\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","16e702d6":"from itertools import combinations\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection  import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_log_error","9ff7dc69":"train_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\nprint(\"Shape of training dataset: \", train_df.shape)","149dfb9b":"X, y = train_df[train_df.columns[:-1]], train_df['SalePrice']","c85e5811":"print(X.head(10))","21ee2653":"# Check the number of unique categories in each category\n# Decide which categorical variables need to be encoded\nfor col_name in X.columns:\n    if X[col_name].dtypes == 'object':\n        unique_cat = len(X[col_name].unique())\n        print(\"Feature '{col_name}' has {unique_cat} unique categories\".format(\n            col_name=col_name, unique_cat=unique_cat))","dead1e04":"# Get names of columns to encode (all columns in this case)\ncols_to_encode = []\nfor col_name in X.columns:\n    if X[col_name].dtypes == 'object':\n        unique_cat = len(X[col_name].unique())\n        cols_to_encode.append(col_name)","2dc45395":"# Function to create ordinal encoding for all categorical colums\n# This function will be used on the test data as well\ndef create_ordinal_encoding(df, dummy_list):\n    for x in dummy_list:\n        col_name_code = x + '_code'\n        \n        # Replace missing values in categorical column with mode\n        # Need to do this step for ordinal encoding\n        df[x].fillna(df[x].value_counts().index[0], inplace=True)\n        \n        ord_enc = OrdinalEncoder()\n        df[col_name_code] = ord_enc.fit_transform(df[[x]])\n        df = df.drop(x, 1)\n        \n    return df","2cf02729":"X = create_ordinal_encoding(X, cols_to_encode)","334d9a12":"print(\"Shape of training dataset: \", X.shape)","9e925c10":"# Check to see if there is missing data in the dataset\nX.isnull().sum().sort_values(ascending=False).head()","86c17299":"# Function to replace missing value with median\n# This function will be used on the test data as well\ndef impute_missing_values(df):\n    imp = SimpleImputer(missing_values=np.nan, strategy='median')\n    imp.fit(df)\n    df = pd.DataFrame(data=imp.transform(df), columns=df.columns)\n    return df","cb88eb48":"X = impute_missing_values(X)","d1ecdc2a":"# Now check again to see if you still have missing data\nX.isnull().sum().sort_values(ascending=False).head()","e98c54e6":"print(\"Shape of training dataset: \", X.shape)","acdacf21":"def remove_ID_col(df):\n    df = df.drop('Id', 1)  \n    return df","65e564c7":"X = remove_ID_col(X)","44ad9430":"print(\"Shape of training dataset: \", X.shape)","d9bdec40":"# Check on the kurtosis & the skewness of SalePrice\nprint(\"Kurtosis: {}\".format(y.kurt()))\nprint(\"Skewness: {}\".format(y.skew()))","683b82fe":"fig = plt.figure(constrained_layout=True, figsize=(8,4))\ngrid = gridspec.GridSpec(ncols=1, nrows=1, figure=fig)\n\n# Plot a QQplot (data before normalization)\nax = fig.add_subplot()\nstats.probplot(y, plot=ax)\nax.set_title(\"QQplot of SalePrice before normalization\")","65b33538":"# Normalize the dependent variable (SalePrice)\ny = np.log1p(y)","7959aa8a":"fig = plt.figure(constrained_layout=True, figsize=(8,4))\ngrid = gridspec.GridSpec(ncols=1, nrows=1, figure=fig)\n\n# Plot a QQplot (data after normalization)\nax = fig.add_subplot()\nstats.probplot(y, plot=ax)\nax.set_title(\"QQplot of SalePrice after normalization\")","4881f278":"# Function to normalize the predictors\n# This function will be used on the test data as well\ndef normalize_values(df):\n    col_names = list(df)\n    min_max_scaler = preprocessing.MinMaxScaler()\n    df_scaled = min_max_scaler.fit_transform(df)\n    df_norm = pd.DataFrame(df_scaled)\n    df_norm.columns = col_names\n    return df_norm","27732952":"X = normalize_values(X)","d099af40":"# Function to create to two-way interactions for all features\ndef add_interactions(df, prune=True):\n    # Get feature names\n    combos = list(combinations(list(df.columns), 2))\n    colnames = list(df.columns) + ['_'.join(x) for x in combos]\n    \n    # Find interactions\n    poly = PolynomialFeatures(interaction_only=True, include_bias=False)\n    df = poly.fit_transform(df)\n    df = pd.DataFrame(df)\n    df.columns = colnames\n    \n    # Remove interaction terms with all 0 values\n    if (prune):\n        noint_indices = [i for i, x in enumerate(list((df == 0).all())) if x]\n        df = df.drop(df.columns[noint_indices], axis=1)\n    \n    return df","e7336bd6":"X = add_interactions(X, False)","a4c6aa5e":"print(\"Shape of training dataset: \", X.shape)","8e637af2":"pca = PCA(n_components=100)\nX_pca = pd.DataFrame(pca.fit_transform(X))","b894c866":"# Use train_test_split  to split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X_pca, y, train_size=0.70, random_state=1)","99c0da25":"linreg = LinearRegression()\nlinreg.fit(X_train, y_train)","cb88ca5d":"y_pred = linreg.predict(X_test)","45023eda":"print(\"Mean Absolute Error\", metrics.mean_absolute_error(y_test, y_pred))\nprint(\"Mean Sqaured Error\", metrics.mean_squared_error(y_test, y_pred))\nprint(\"Root Mean Squared Error\", np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\nprint(\"Root Mean Squared Logarithmic Error\", np.sqrt(metrics.mean_squared_log_error(y_test, y_pred)))","405261fd":"test_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nprint(\"Shape of test dataset: \", test_df.shape)","1cc98afe":"test_df = test_df.reindex(columns = test_df.columns, fill_value=0)\ntest_df = create_ordinal_encoding(test_df, cols_to_encode)\ntest_df = impute_missing_values(test_df)\ntest_df = remove_ID_col(test_df)\ntest_df = normalize_values(test_df)\ntest_df = add_interactions(test_df, False)\ntest_df_pca = pca.transform(test_df)\nprint(\"Shape of test dataset: \", test_df_pca.shape)","3695a905":"# Create submission file\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(linreg.predict(test_df_pca)))\nsubmission.to_csv(\"submission.csv\", index=False)","447ba7fb":"A symmetrical dataset will have a skewness equal to 0. So, a normal distribution will have a skewness of 0. The kurtosis of a normal distribution is equal to 3. If the kurtosis is greater than 3, then the dataset has heavier tails than a normal distribution (more in the tails).","42bbc1ad":"Outlier detection is not part of the data cleaning and preparation process. I checked for outliers but removing\/imputing outlier values deterioriated performance on the test dataset. Therefore, I have not included it in the kernel.","f3105630":"<h2>3.2 Dimensionality Reduction using PCA<\/h2>\n\nPCA is a great method for dimensionality reduction. Another good method is feature selection (not shown in this notebook).","9b520515":"# 5. Predictions on Test Data","8b9140ca":"<h2>2.4 Normalize Data<\/h2>\n\nAnalysis show that the target variable is not normally distributed. We can confirm that with a QQ-plot.","2d1be8c9":"<h2>2.3 Remove ID column<\/h2>\n\nThe ID column in the training dataset is an unique identifier for each record and does not have any predictive power.","f5e1d8cf":"We are not removing any interaction features at this stage (not even the ones with very low values) beacuse we will be performing PCA later.","7522b54c":"# 1. Data Loading","54f3b565":"# 4. Model Building","1d9380fe":"<h2>3.1 Create Interactions<\/h2>","cd8a0bf3":"We need to normalize the predictors as well. Since we are going to use linear regression in this notebook, it is a good idea to normalize the predictors.","93c765d6":"# 3. Feature Engineering","d1534ce7":"<h2>2.2 Handle Missing Values<\/h2>\n\nSince all the columns have been encoded, now the data in training dataset is completely numerical.","a0a93ea0":"# 2. Data Cleaning","d7ddf410":"<h2>2.1 Encode Categorical Values<\/h2>"}}