{"cell_type":{"88e7d628":"code","62dce33e":"code","3e15ffb3":"code","0aa251e6":"code","edd8d4bd":"code","b5360fde":"code","f0235237":"code","d6b79082":"code","1b93014d":"code","ca797511":"code","5fae96fc":"code","efa9bef4":"code","0962ba54":"code","9ab75a61":"code","3938d557":"code","27052222":"code","1ce90e51":"code","f0a61149":"code","0d4b8581":"code","a07852f5":"code","026c7f39":"code","a58a7ac0":"code","3a630bce":"code","f4b2bdda":"code","859a8c5d":"code","46658b95":"code","8ceee9f1":"code","40a74517":"code","b1040e24":"code","2e847a30":"code","d6d4ae00":"code","a8a5eeb8":"code","c9e603eb":"code","ccb8f5fc":"code","ebcc47e7":"code","e4bab250":"code","4fc0a634":"code","6201fafb":"code","17d86a41":"code","4bd2c833":"code","477b6be3":"code","7da7a7a6":"code","1bdc01b0":"code","6b529eda":"code","6719a8ac":"code","27c597b5":"code","83cafc6b":"code","dc57fa82":"code","dc3f6218":"code","b252dc94":"code","b433a7fc":"markdown","6db1202d":"markdown","9dc46222":"markdown","df2ace1b":"markdown","b66f31f7":"markdown","0ae587f1":"markdown","0e5111a2":"markdown","43e6b78d":"markdown","1df3126d":"markdown","a0e624d6":"markdown","abf4ed08":"markdown","615c4def":"markdown","366e91af":"markdown","9d843d45":"markdown","8df678c3":"markdown","0eaac991":"markdown","d7082c8d":"markdown","f39e9ae8":"markdown","1a16910f":"markdown","fc2afec7":"markdown","82f3ac03":"markdown","61a0ec54":"markdown","2a5855ea":"markdown","dc0ad075":"markdown","57ef52b7":"markdown","80ad27fb":"markdown","9123ad1b":"markdown","8c3db5ff":"markdown","c5a690a4":"markdown"},"source":{"88e7d628":"%%javascript\n$.getScript('https:\/\/kmahelona.github.io\/ipython_notebook_goodies\/ipython_notebook_toc.js')","62dce33e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3e15ffb3":"# Importing basic libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly \nimport sklearn\n\n# use following command to download shap & eli5\n# conda install -c conda-forge shap\n# conda install -c conda-forge eli5","0aa251e6":"# Import libraries for data processing & modeling\n\nfrom sklearn.metrics import confusion_matrix\n\nfrom sklearn.svm import SVC\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n\nfrom imblearn.over_sampling import SMOTE","edd8d4bd":"# Importing libraries for Visualization \nimport plotly\nfrom plotly import tools\nimport plotly.offline as py\nimport plotly.graph_objs as go\n\nimport plotly.figure_factory as ff\nimport plotly_express as px\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected = True)\n\n!pip install bubbly\nfrom bubbly.bubbly import bubbleplot\n\n!pip install joypy\nimport joypy","b5360fde":"train = pd.read_csv('\/kaggle\/input\/fraudclaims\/insurance_claims.csv')\n\n# let's take a look at the data\npd.set_option('display.max_columns', None)\ntrain.head()\n# train.shape\n# train.info()","f0235237":"train.describe()","d6b79082":"# Check for missing data \ntrain_na = (train.isnull().sum() \/ len(train)) * 100\ntrain_na = train_na.drop(train_na[train_na == 0].index).sort_values(ascending=False)[:30]\nmiss_train = pd.DataFrame({'Train Missing Ratio' :train_na})\nmiss_train.head()","1b93014d":"# Unique values in each variables\ntrain.nunique()","ca797511":"train = train.replace('?',np.NaN)\ntrain.isnull().any()","5fae96fc":"# Replace the '?' with mode of collision_type feature as it is categorical and we are unaware of the type.\ntrain['collision_type'].fillna(train['collision_type'].mode()[0], inplace = True)\n\n# Replace missing information as 'No' for property_damage. Feature can have only 2 possibles values. Either Yes or No. \n# If information is nor available we will assume it was not reported.\ntrain['property_damage'].fillna('NO', inplace = True)\n\n# USe the assumption above to replace missing information as 'No' for police_report_available\ntrain['police_report_available'].fillna('NO', inplace = True)\n\ntrain.isnull().any().any()","efa9bef4":"  # Let's start with some basic plots to analyze relationship between different features\nfig = px.scatter(train, x = 'months_as_customer', y = 'age', color = 'fraud_reported', marginal_x = 'rug',\n                 marginal_y = 'histogram')\nfig.show()","0962ba54":"fig = px.scatter(train, x = 'months_as_customer', y = 'policy_annual_premium', color = 'fraud_reported',\n                 marginal_x = 'rug', marginal_y = 'histogram')\nfig.show()","9ab75a61":"fig = px.scatter(train, x = 'months_as_customer', y = 'total_claim_amount', color = 'fraud_reported', marginal_x = 'rug', marginal_y = 'histogram')\nfig.show()","3938d557":"fig = px.scatter_matrix(train, dimensions=[\"total_claim_amount\", \"injury_claim\", \"property_claim\", \"vehicle_claim\"], color = \"fraud_reported\")\nfig.show()","27052222":"fig, axes = joypy.joyplot(train,\n                         column = ['incident_hour_of_the_day','number_of_vehicles_involved', 'witnesses'],\n                         by = 'incident_city',\n                         ylim = 'own',\n                         figsize = (20, 10),\n                         alpha = 0.5, \n                         legend = True)\n\nplt.title('Incident hour, No. of vehicles, witnesses & Incident City', fontsize = 20)\nplt.show()","1ce90e51":"# let's encode the fraud reported to numerical values\ntrain['fraud_reported'] = train['fraud_reported'].replace(('Y','N'),(1,0))","f0a61149":"# Count the fraudulent transactions by gender\nax = sns.countplot(x=\"insured_sex\", hue=\"fraud_reported\", data=train)\n\n# Analyze fraudulent transactions by gender\ntrain[[\"insured_sex\", \"fraud_reported\"]].groupby(['insured_sex']).count().sort_values(by='fraud_reported', ascending=False)","0d4b8581":"# Analyze the Probability of fraud by Gender\ng = sns.barplot(x=\"insured_sex\",y=\"fraud_reported\",data=train)\ng = g.set_ylabel(\"Fraud Probability\")\ntrain[[\"insured_sex\", \"fraud_reported\"]].groupby(['insured_sex']).mean().sort_values(by='fraud_reported', ascending=False)","a07852f5":"# Let's explore the distribution of age by response variable (Fraud Reported)\nfig = plt.figure(figsize=(10,8),)\naxis = sns.kdeplot(train.loc[(train['fraud_reported'] == 1),'age'] , color='g',shade=True, label='Fraudulent Claims')\naxis = sns.kdeplot(train.loc[(train['fraud_reported'] == 0),'age'] , color='b',shade=True,label='Nonfraudulent Claims')\nplt.title('Age Distribution - Fraud V.S. Non Fraud Claims', fontsize = 20)\nplt.xlabel(\"Age\", fontsize = 12)\nplt.ylabel('Frequency', fontsize = 12);","026c7f39":"# Count the fraudulent transactions by insured_education_level\nfig, ax = plt.subplots(figsize=(12,6))\nax = sns.countplot(x=\"insured_education_level\", hue=\"fraud_reported\", data=train)\n\n# Analyze the fraudulent transactions by insured_education_level\ntrain[[\"insured_education_level\", \"fraud_reported\"]].groupby(['insured_education_level']).count().sort_values(by='fraud_reported', ascending=False)","a58a7ac0":"# Analyze the Probability of fraud by education level\nfig, ax = plt.subplots(figsize=(12,6))\ng = sns.barplot(x=\"insured_education_level\",y=\"fraud_reported\",data=train)\ng = g.set_ylabel(\"Fraud Probability\")\ntrain[[\"insured_education_level\", \"fraud_reported\"]].groupby(['insured_education_level']).mean().sort_values(by='fraud_reported', ascending=False)","3a630bce":"# Count the fraudulent transactions by insured_relationship\nfig, ax = plt.subplots(figsize=(12,6))\nax = sns.countplot(x=\"insured_relationship\", hue=\"fraud_reported\", data=train)\n\n# Analyze the fraudulent transactions by insured_relationship\ntrain[[\"insured_relationship\", \"fraud_reported\"]].groupby(['insured_relationship']).count().sort_values(by='fraud_reported', ascending=False)","f4b2bdda":"# Analyze the Probability of fraud by insured_relationship\nfig, ax = plt.subplots(figsize=(12,6))\ng = sns.barplot(x=\"insured_relationship\",y=\"fraud_reported\",data=train)\ng = g.set_ylabel(\"Fraud Probability\")\ntrain[[\"insured_relationship\", \"fraud_reported\"]].groupby(['insured_relationship']).mean().sort_values(by='fraud_reported', ascending=False)","859a8c5d":"# Analyze the Probability of fraud by incident_type\n\ng = sns.barplot(x=\"incident_type\",y=\"fraud_reported\",data=train)\ng = g.set_ylabel(\"Fraud Probability\")\ntrain[[\"incident_type\", \"fraud_reported\"]].groupby(['incident_type']).mean().sort_values(by='fraud_reported', ascending=False)","46658b95":"# Analyze the Probability of fraud by collision_type\ng = sns.barplot(x=\"collision_type\",y=\"fraud_reported\",data=train)\ng = g.set_ylabel(\"Fraud Probability\")\ntrain[[\"collision_type\", \"fraud_reported\"]].groupby(['collision_type']).mean().sort_values(by='fraud_reported', ascending=False)","8ceee9f1":"# Analyze the Probability of fraud by incident_severity\n\ng = sns.barplot(x=\"incident_severity\",y=\"fraud_reported\",data=train)\ng = g.set_ylabel(\"Fraud Probability\")\ntrain[[\"incident_severity\", \"fraud_reported\"]].groupby(['incident_severity']).mean().sort_values(by='fraud_reported', ascending=False)","40a74517":"# Analyze the Probability of fraud by incident_state\n\ng = sns.barplot(x=\"incident_state\",y=\"fraud_reported\",data=train)\ng = g.set_ylabel(\"Fraud Probability\")\ntrain[[\"incident_state\", \"fraud_reported\"]].groupby(['incident_state']).mean().sort_values(by='fraud_reported', ascending=False)","b1040e24":"import warnings\nwarnings.filterwarnings('ignore')\n\nfigure = bubbleplot(dataset = train, x_column = 'policy_annual_premium', y_column = 'total_claim_amount',\n                    bubble_column = 'fraud_reported', time_column = 'auto_year', size_column = 'months_as_customer',\n                    color_column = 'fraud_reported', x_title = \"Annual Policy Premium\", y_title = \"Total Claim Amount\",\n                    title = 'Total Claim Amount, Annual Premium and Months as Customer',\n    x_logscale = False, scale_bubble = 3, height = 650)\n\npy.iplot(figure, config={'scrollzoom': True})","2e847a30":"# Analyze the Probability of fraud by incident_state\nfig, ax = plt.subplots(figsize=(12,6))\ng = sns.barplot(x=\"auto_make\",y=\"fraud_reported\",data=train)\ng = g.set_ylabel(\"Fraud Probability\")\ntrain[[\"auto_make\", \"fraud_reported\"]].groupby(['auto_make']).mean().sort_values(by='fraud_reported', ascending=False)","d6d4ae00":"bins = [-1, 3, 6, 9, 12, 17, 20, 24] \ncatg = [\"past_midnight\", \"early_morning\", \"morning\", 'fore-noon', 'afternoon', 'evening', 'night']\ntrain['incident_period'] = pd.cut(train.incident_hour_of_the_day, bins, labels=catg).astype(object)\ntrain[['incident_hour_of_the_day', 'incident_period']].head(20)","a8a5eeb8":"#Transforming some numerical variables that are really categorical\ntrain['number_of_vehicles_involved'] = train['number_of_vehicles_involved'].apply(str)\ntrain['witnesses'] = train['witnesses'].apply(str)\ntrain['bodily_injuries'] = train['bodily_injuries'].apply(str)","c9e603eb":"dummies = pd.get_dummies(train[['policy_state', 'insured_sex', 'insured_education_level', 'insured_occupation', \n                                'insured_hobbies', 'insured_relationship', 'incident_type',\n                                'collision_type', 'incident_severity', 'authorities_contacted', 'incident_state',\n                                'incident_city', 'number_of_vehicles_involved', 'property_damage', 'bodily_injuries',\n                                'witnesses', 'police_report_available', 'auto_make', 'auto_model', 'incident_period']])\n\ndummies.head(5)","ccb8f5fc":"#let's drop unnecessary variables\n\ntrain = train.drop(columns = ['policy_number', 'policy_csl','insured_zip','policy_bind_date', 'incident_date', \n                              'incident_location', 'auto_year', 'incident_hour_of_the_day', 'policy_state', \n                              'insured_sex', 'insured_education_level', 'insured_occupation', 'insured_hobbies',\n                              'insured_relationship', 'incident_type', 'collision_type', 'incident_severity',\n                              'authorities_contacted', 'incident_state', 'incident_city', 'number_of_vehicles_involved',\n                              'property_damage', 'bodily_injuries', 'witnesses', 'police_report_available',\n                              'auto_make', 'auto_model', 'incident_period'])","ebcc47e7":"x = pd.concat([dummies, train], axis=1)\nx.info()","e4bab250":"x_unscaled = x.iloc[:, 0:-1]  # predictor variables\ny = x.iloc[:, -1]  # target variable","4fc0a634":"# let's split the dataset into train and test sets\n\nx_train, x_test, y_train, y_test = train_test_split(x_unscaled, y, test_size = 0.2, random_state = 0)\n\nprint(\"Shape of x_train :\", x_train.shape)\nprint(\"Shape of x_test :\", x_test.shape)\nprint(\"Shape of y_train :\", y_train.shape)\nprint(\"Shape of y_test :\", y_test.shape)","6201fafb":"x_train.info()","17d86a41":"# standardize for easy computing\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler(with_mean=False)\nX_train = scaler.fit_transform(x_train)\nx_test = scaler.fit_transform(x_test)\nY_train= y_train\n\n#Y_train= y","4bd2c833":"# Cross validate model with Kfold stratified cross val\nkfold = StratifiedKFold(n_splits=10)\n# Modeling differents algorithms. Thanks Yassine for this great piece of code. \n\nrandom_state = 2\nclassifiers = []\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(LinearDiscriminantAnalysis())\n\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, X_train, Y_train , scoring = \"accuracy\", cv = kfold, n_jobs=4))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"KNeighboors\",\"LogisticRegression\",\"LinearDiscriminantAnalysis\"]})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","477b6be3":"# Adaboost\nDTC = DecisionTreeClassifier()\n\nadaDTC = AdaBoostClassifier(DTC, random_state=7)\n\nada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[1,2],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n\ngsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\ngsadaDTC.fit(X_train,Y_train)\nada_best = gsadaDTC.best_estimator_\ngsadaDTC.best_params_\ngsadaDTC.best_score_","7da7a7a6":"#ExtraTrees \nExtC = ExtraTreesClassifier()\n## Search grid for optimal parameters\nex_param_grid = {\"max_depth\": [None],\n                 \"max_features\": [1, 3, 10],\n                 \"min_samples_split\": [2, 3, 10],\n                 \"min_samples_leaf\": [1, 3, 10],\n                 \"bootstrap\": [False],\n                 \"n_estimators\" :[100,300],\n                 \"criterion\": [\"gini\"]}\n\ngsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\ngsExtC.fit(X_train,Y_train)\nExtC_best = gsExtC.best_estimator_\ngsExtC.best_params_\n# Best score\ngsExtC.best_score_","1bdc01b0":"# RFC Parameters tunning \nRFC = RandomForestClassifier()\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [None],\n                 \"max_features\": [1, 3, 10],\n                 \"min_samples_split\": [2, 3, 10],\n                 \"min_samples_leaf\": [1, 3, 10],\n                 \"bootstrap\": [False],\n                 \"n_estimators\" :[100,300],\n                 \"criterion\": [\"gini\"]}\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\ngsRFC.fit(X_train,Y_train)\nRFC_best = gsRFC.best_estimator_\n\n# Best score\ngsRFC.best_score_","6b529eda":"# Gradient boosting \nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1]}\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\ngsGBC.fit(X_train,Y_train)\nGBC_best = gsGBC.best_estimator_\n\n# Best score\ngsGBC.best_score_","6719a8ac":"### SVC classifier\nSVMC = SVC(probability=True)\nsvc_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\n\ngsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\ngsSVMC.fit(X_train,Y_train)\nSVMC_best = gsSVMC.best_estimator_\n\n# Best score\ngsSVMC.best_score_","27c597b5":"y_pred_rf = gsSVMC.predict(x_test)\nprint(\"Training Accuracy: \", gsSVMC.score(X_train, Y_train))\nprint('Testing Accuarcy: ', gsSVMC.score(x_test, y_test))\n\n# making a classification report\ncr = classification_report(y_test,  y_pred_rf)\nprint(cr)\n\n# making a confusion matrix\nplt.rcParams['figure.figsize'] = (5, 5)\ncm = confusion_matrix(y_test, y_pred_rf)\nsns.heatmap(cm, annot = True, cmap = 'spring')\nplt.show()","83cafc6b":"from imblearn.over_sampling import SMOTE\nx_resample, y_resample  = SMOTE().fit_sample(x_unscaled, y.values.ravel())\nprint(\"Shape of x_resample :\",x_resample.shape)\nprint(\"Shape of y_resample :\",y_resample.shape)","dc57fa82":"x_train2, x_test2, y_train2, y_test2 = train_test_split(x_resample, y_resample, test_size = 0.2, random_state = 0)\nprint(\"Shape of x_train2 :\", x_train2.shape)\nprint(\"Shape of y_train2 :\", y_train2.shape)\nprint(\"Shape of x_test2 :\", x_test2.shape)\nprint(\"Shape of y_test2 :\", y_test2.shape)","dc3f6218":"# standardization\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train2 = sc.fit_transform(x_train2)\nx_test2 = sc.transform(x_test2)","b252dc94":"### SVC classifier\nSVMC = SVC(probability=True)\nsvc_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\ngsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\ngsSVMC.fit(x_train2,y_train2)\n\ny_pred_rf = gsSVMC.predict(x_test2)\nprint(\"Training Accuracy: \", gsSVMC.score(x_train2, y_train2))\nprint('Testing Accuarcy: ', gsSVMC.score(x_test2, y_test2))\n\n# making a classification report\ncr = classification_report(y_test2,  y_pred_rf)\nprint(cr)\n\n# making a confusion matrix\nplt.rcParams['figure.figsize'] = (5, 5)\ncm = confusion_matrix(y_test2, y_pred_rf)\nsns.heatmap(cm, annot = True, cmap = 'spring')\nplt.show()\n","b433a7fc":"# **Data Visualization**\n\n![Visualization.png](attachment:Visualization.png)","6db1202d":"\n# **Problem Identification**\n![Prob%20Ident.png](attachment:Prob%20Ident.png)\n\n\nThe objective of this use case would be to build a predictive model to identify and tag fraudulent claims. So this would be a classification problem.","9dc46222":"# **Developing a model**","df2ace1b":"# **Exploratory data analysis**\n\n![analysis.png](attachment:analysis.png)","b66f31f7":"The claims related analysis suffers from imbalanced class data which means there could be a significant difference between the majority and minority classes. In other words, there are too few examples of the minority class for a model to effectively learn the decision boundary.\nOne way to tackle such a situation is to oversample the minority class (fraud = 1 in this case). The approach involves simply duplicating the minority class examples. This is a type of data processing for the minority class is referred to as the Synthetic Minority Oversampling Technique or SMOTE for short.","0ae587f1":"let's encode the fraud reported to numerical values for additional charts","0e5111a2":"The average months_as_customer is around 200. We can see that recency is linked with frauds here. Let's analyze this with policy_annual_premium..\n\nahhh not that conclusive!!!","43e6b78d":"**ExtraTrees Classifier**\n\nET is a meta estimator that fits a number of randomized decision trees on various sub-samples of the dataset and then uses averaging method to improve the predictive accuracy and control over-fitting.","1df3126d":"As we can see oversampling using SMOTE helped in improving the predictions in test dataset.  ","a0e624d6":"<h1 id=\"tocheading\">Table of Contents<\/h1>\n<div id=\"toc\"><\/div>","abf4ed08":"**AdaBoost classifier**\n\nAdaboost begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.","615c4def":"# **What would be the workflow?**\nI will keep it simple & crisp rather than using buzz words & useless data science frameworks. Frankly speaking no one cares.\n\nThis will help you to stay on track. So here is the workflow.\n\n**1. Problem Identification**\n\n**2. What data do we have?**\n\n**3. Exploratory data analysis**\n\n**4. Data preparation including feature engineering**\n\n**5. Developing a model**\n\n**6. Model evaluation**\n\n**7. Conclusions**\n\nThat's all you need to solve a data science problem.\n","366e91af":"* **Months as customer** \u2014 Are old customers more loyal and truthful? Can a minor difference between policy issuance date and incident date be an indication of fraud?\n\n* **Age** \u2014 Could age be an important factor to indicate fraudulent claims? Are younger people more likely to commit fraud as compared to the old generation? If yes, how can you define a threshold?\n\n* **Gender** \u2014 Are men more likely to commit fraud?\n\n* **Insured Occupation** \u2014 Are blue-collar workers more likely to be involved in fraud than white-collar workers?\n\n* **Hobbies and relationship** \u2014 Does hobby and relationship with the insured make a difference?\n\n* **Annual Premium** \u2014 Can high premium coax a claimant to exaggerate repair costs after an accident?\n\n* **Insured Zip and State** \u2014 Did you observe any specific pattern with zip code and state? Are people from a specific zip code or state more likely to file claims as compared to others?\n\n* **Incident, Collision and Severity Type** \u2014 What is the probability of a fraudulent claim being associated with a particular incident or collision type?\n\n* **Witnesses** \u2014 How the presence or absence of a witness related to frauds? Does the presence of witnesses could indicate a staged accident? Or could the absence of witnesses be a potential indicator of fraud?\n\n* **Total claim vs Injury and vehicle claim** \u2014 Does the ratio of injury to vehicle claim reflect anything? Let\u2019s say a serious accident with extensive physical damage happened, but only a minor injury was diagnosed with subjective findings that required little or no medical treatment. What does it mean to the claim investigator or a Data Scientist?","9d843d45":"Let\u2019s print a confusion matrix using the SVC predictions. The training accuracy for the model is 94.75% and the testing accuracy is 83%. Can we improve this further using SMOTE?","8df678c3":"> **Applying Over Sampling Techniques Using SMOTE**\n","0eaac991":"The dataset doesn\u2019t have null values, however, few features have \u2018?\u2019 as values which should be appropriately treated.","d7082c8d":"# **Model evaluation**\n\nEvaluating multiple models using GridSearch optimization method.\n\nHyper-parameters are key parameters that are not directly learnt within the estimators. We have to pass these as arguments. Different hyper parameters can result in different model with varying performance\/accuracy. To find out what paparmeters are resulting in best score, we can use Grid Search method and use the optimum set of hyper parameters to build and select a good model.\n\nA search consists of:\n\n* an estimator (regressor or classifier)\n* a parameter space;\n* a method for searching or sampling candidates;\n* a cross-validation scheme; and\n* a score function.","f39e9ae8":"**Random Forest Classifier**\n\nSimilar to Extra Tree Classifier a Random Forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).\n\nHow ET differes from RF -\n\n1) When choosing variables at a split, samples are drawn from the entire training set instead of a bootstrap sample of the training set.\n\n2) Splits are chosen completely at random from the range of values in the sample at each split.","1a16910f":"Let's see how Total Claim, Injury Claim, Property Claim, and Vehicle Claim are related with fraud_reported","fc2afec7":"**Gradient Boosting**\n\nGradient boosting is one of the most powerful techniques for building predictive models. Boosting is a method of converting weak learners into strong learners by building an additive model in a forward stage-wise fashion. In boosting, each new tree is a fit on a modified version of the original data set.","82f3ac03":"# **What is Insurance Fraud**\n\nInsurance fraud is a deliberately false or misrepresented claim by an insured\/claimant or entity for the purpose of financial gain. Insurance fraud can be committed at different touchpoints in the insurance lifecycle by insured applicants, policyholders, third-party claimants or professionals such as insurance agencies\/agents who provide such services.\n\n![Fraud.jpg](attachment:Fraud.jpg)\n\nToday Insurance industry looks much different than it did ten years ago due to technological advancements & innovations but needless to say, fraudsters have also evolved. Insurance fraud has been a constant challenge for this industry, however, a foundational framework along with a robust AI strategy can help in identifying suspicious claims in advance.\n\nTo illustrate the actual implementation, we can pick up a dummy data from the Auto Insurance industry to understand how Machine Learning can help in identifying and predicting fraudulent claims.\n\n","61a0ec54":"**If you liked the notebook, please share and upvote. **\n![Good%20Bye.png](attachment:Good%20Bye.png)\n\nStay tuned for more. Happy Learning !!!","2a5855ea":"**Cross Validation**\n![CV.png](attachment:CV.png)\n\nCross Validation is one of the most powerful tool in Data Scientist's tool box. It helps you to understand the performance of your model and fight with overfitting. As we all know that Learning the model parameters and testing it on the same data is a big mistake. Such a model would have learned everything about the training data and would give result in a near perfect test score as it has already seen the data. The same model would fail terribly when tested on unseen data. This situation is called overfitting. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set X_test, y_test.\n\nThe general approach is as follows:\n\n1. Split the dataset into k groups\n2. For each unique group:\n\n     a. Keep one group as a hold out or test data set\n     \n     b. Use the remaining groups as training data set\n     \n     c. Build the model on the training set and evaluate it on the test set\n     \n     d. Save the evaluation score \n \n3. Summarize the performance of the model using the sample of model evaluation scores\n\nYou can access following link and read about Cross Validation in detail.\n\nhttps:\/\/medium.com\/datadriveninvestor\/k-fold-cross-validation-6b8518070833\nhttps:\/\/www.analyticsvidhya.com\/blog\/2018\/05\/improve-model-performance-cross-validation-in-python-r\/","dc0ad075":"A casual look of data will reveal that few features have '?' as values. For example Collision_type has 178 such values which need to be replaced ","57ef52b7":"**Support Vector Machines**\n\nSVM builds a hyperplane in multidimensional space to separate different classes. SVM generates optimal hyperplane in an iterative manner, which is used to minimize the error. The idea behind SVM is to find a maximum marginal hyperplane(MMH) that best divides the dataset into classes.","80ad27fb":"# **What data do we have?**\n\n![Data.jpg](attachment:Data.jpg)\n\nThe raw data has 39 variables including the target \u2018Fraud Reported\u2019. It\u2019s a small dataset with just 1000 observations (rows).","9123ad1b":"# **Conclusions**\n\n![Conclusion.png](attachment:Conclusion.png)\n\nFor the sake of keeping the notebook short, I didn\u2019t add too many plots and analysis. While analyzing the claims data, several decisions could be taken based on the business and availability of data.\n\nHaving said that, I would like to leave some Food For Thought for you!!!","8c3db5ff":"# **My blog on Fraud Analytics on Medium**\n\nThe notebook is being written to support my article on Insurance Fraud. The purpose of this notebook is to leverage the \nsample dataset uploaded on Kaggle and perform ML to identify and predict fraudulent claims.\n\nThe article link is - [Predicting Insurance Fraud with Machine Learning (SMOTE)](http:\/\/medium.com\/analytics-vidhya\/predicting-insurance-fraud-with-machine-learning-smote-da94adf8fb62)\n","c5a690a4":"# **Feature engineering**\n\n![FE.png](attachment:FE.png)"}}