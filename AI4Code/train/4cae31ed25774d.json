{"cell_type":{"6d0f565b":"code","239addb3":"code","a9877824":"code","5ce9cbff":"code","eff201f5":"code","0315cd7e":"code","0713c7ed":"code","c92dcb8a":"code","a200cb72":"code","3eb5ab5f":"code","7245bd3d":"code","879692c4":"code","02c150a5":"code","f3e82b2f":"code","3238d8a3":"code","8dbbb805":"code","213cfe6c":"code","5202a29d":"code","9089c91b":"code","31639cb3":"code","711b0dcd":"code","0848e92a":"code","b377d02f":"code","6daef86f":"code","3acb1082":"code","68337455":"code","cce4ffaa":"code","ca37157d":"code","02c51238":"code","4ae34097":"code","b0b02a01":"code","bf7f27ea":"code","3ba773d4":"code","d50e83c7":"code","057928e0":"code","537903cc":"code","d97b0203":"code","2582f1e7":"code","308815d2":"code","22eda546":"code","575d25f5":"code","bd3dbad2":"code","7f9510df":"code","a9e0657c":"code","9870ea0e":"markdown","c1b9bfd4":"markdown","dad21270":"markdown","4ed9e7dc":"markdown","b79cbd4a":"markdown","38da58a4":"markdown","82c7df2a":"markdown","54ebbcd1":"markdown","9d7b3e4b":"markdown","b97302c5":"markdown","cd816058":"markdown","001e8c1c":"markdown","d7585cc1":"markdown","a434dfe9":"markdown","55ffc83f":"markdown","95bc0c46":"markdown","eefadfec":"markdown","dd133ff1":"markdown","cde3ff47":"markdown","53eb5700":"markdown","dc021757":"markdown","b37db698":"markdown","74be0b79":"markdown","82d455ef":"markdown","59f64cf5":"markdown","9bd8c196":"markdown","be0aafb7":"markdown","ea77a98d":"markdown","5e07046e":"markdown","2ffc26e3":"markdown","a4aefcf3":"markdown","30c20c0f":"markdown","d3a7d8a2":"markdown","aba14338":"markdown","b3ba11d7":"markdown","39ac373b":"markdown","40fb661b":"markdown","cee73a80":"markdown","a9c6569b":"markdown","46f18aa2":"markdown","cdb08a88":"markdown","d3d070c9":"markdown","5a90fe33":"markdown"},"source":{"6d0f565b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","239addb3":"#Import the data into a dataframe\ndf = pd.read_csv('..\/input\/911.csv')","a9877824":"df.info()","5ce9cbff":"df.head()","eff201f5":"#check for any missing data\ndf.isnull().sum()","0315cd7e":"#The top 5 zip codes for 911 calls with the data we have\ndf['zip'].value_counts().head()","0713c7ed":"#The top 5 townships for 911 calls with the data we have\ndf['twp'].value_counts().head()","c92dcb8a":"#Let's see how many unique title codes there are.\ndf['title'].nunique()","a200cb72":"df['Reason'] = df['title'].apply(lambda st: st.split(':')[0])","3eb5ab5f":"df.head()","7245bd3d":"df['Reason'].value_counts()","879692c4":"#let's visualize the above result to visually compare these numbers\nsns.countplot(x='Reason',data=df)","02c150a5":"#checking the timestamp column datatype\ntype(df['timeStamp'].iloc[0])","f3e82b2f":"#convert them to DateTime objects\ndf['timeStamp'] = pd.to_datetime(df['timeStamp'])","3238d8a3":"type(df['timeStamp'].iloc[0])","8dbbb805":"df['Hour'] = df['timeStamp'].apply(lambda time: time.hour)","213cfe6c":"df['Month'] = df['timeStamp'].apply(lambda time: time.month)","5202a29d":"dmap = {0:'Mon',1:'Tue',2:'Wed',3:'Thu',4:'Fri',5:'Sat',6:'Sun'}\ndf['Day of Week'] = df['timeStamp'].apply(lambda time: time.dayofweek).map(dmap)\n#There is a time.weekday_name attribute that could have produced an equivalent solution, but I wanted to practice mapping a dictionary","9089c91b":"#check to see our new dataframe\ndf.head()","31639cb3":"sns.countplot(x='Day of Week', data = df, hue = 'Reason', palette = 'Set2')\nplt.legend(loc='lower left',bbox_to_anchor=(1.0,0.5))","711b0dcd":"sns.countplot(x='Month', data = df, hue = 'Reason', palette = 'Set2')\nplt.legend(loc='lower left',bbox_to_anchor=(1.0,0.5))","0848e92a":"df.groupby('Month').count()","b377d02f":"#let's turn this into a graph to better understand calling trends per month\ndf.groupby('Month').count().plot.line(use_index = True,y = 'title',legend = None)\nplt.ylabel('count')","6daef86f":"sns.lmplot(x='Month',y = 'title', data = df.groupby('Month').count().reset_index())\nplt.ylabel('count')","3acb1082":"#let's use the timestamp information to create a new column\ndf['Date'] = df['timeStamp'].apply(lambda ts: ts.date())","68337455":"df.head()","cce4ffaa":"df.groupby('Date').count().plot.line(use_index = True, y = 'title', figsize= (15,2), legend = None)\nplt.ylabel('count')","ca37157d":"df['Date'] = pd.to_datetime(df['Date'])","02c51238":"df.groupby(df[df['Date'].dt.year>=2018]['Date']).count().plot.line(use_index = True, y = 'title', legend = None)\nplt.ylabel('count')","4ae34097":"df.groupby(df[(df['Date'].dt.year>= 2018) & (df['Date'].dt.month==3)]['Date']).count()","b0b02a01":"#Checking the reasons to see if it's distributed according to the entire dataset.\ndf[df['Date']=='2018-03-02']['Reason'].value_counts()\n","bf7f27ea":"sns.countplot(x='Reason',data=df[df['Date']=='2018-03-02'])","3ba773d4":"#reusing the same code from before\ndf.groupby(df[(df['Date'].dt.year>= 2018) & (df['Date'].dt.month==11)]['Date']).count()","d50e83c7":"sns.countplot(x='Reason',data=df[df['Date']=='2018-11-15'])","057928e0":"#unstacking and viewing the groupby table so we can find out how to select our data.\ndf.groupby(['Date','Reason']).count().unstack()","537903cc":"#Traffic\ndf.groupby(['Date','Reason']).count()['title'].unstack().plot.line(use_index = True, y = 'Traffic', figsize= (15,2), legend = None)\nplt.title('Traffic')\nplt.ylabel('count')","d97b0203":"#EMS\ndf.groupby(['Date','Reason']).count()['title'].unstack().plot.line(use_index = True, y = 'EMS', figsize= (15,2), legend = None)\nplt.title('EMS')\nplt.ylabel('count')","2582f1e7":"#Fire\ndf.groupby(['Date','Reason']).count()['title'].unstack().plot.line(use_index = True, y = 'Fire', figsize= (15,2), legend = None)\nplt.title('Fire')\nplt.ylabel('count')","308815d2":"#First need to change the dataframe to a pivot table with days of week and hours in day\ndfht = df.groupby(['Day of Week','Hour']).count().unstack()['title']\ndfht","22eda546":"fig, ax = plt.subplots(figsize=(12,6))\nsns.heatmap(dfht, cmap='coolwarm',ax = ax)","575d25f5":"sns.clustermap(dfht, cmap = 'coolwarm', figsize = (12,10))","bd3dbad2":"#Creating the dataframe we'll use\ndfmt = df.groupby(['Day of Week','Month']).count().unstack()['title']\ndfmt","7f9510df":"#Heatmap\nfig, ax = plt.subplots(figsize=(12,6))\nsns.heatmap(dfmt, cmap='coolwarm', ax = ax)","a9e0657c":"#let's make a cluster map of the same information\nsns.clustermap(dfmt, cmap = 'coolwarm', figsize = (12,10))","9870ea0e":"# Heat Maps\nFinally, let's move to looking at how the time of day and day of week interact with the number of 911 calls. For this, we'll be creating a heat map using seaborn.","c1b9bfd4":"Let's get a better understanding of the total number of calls per month.","dad21270":"Again, we see that something happened on November 15th. Because the November 15th graph also shows a high count of traffic calls, we have a sense that it could be similar to the March 2nd incident where the cause was extreme weather. ","4ed9e7dc":"# Extracting 911 Call Reasons\nThere seems to be a variety of, what seems like, the reason\/results of the 911 call under the title column. I assume that this is what they use to quickly describe what kind of incident occured. ","b79cbd4a":"We see that this count distribution looks very different than our original total count distribution for the entire dataset. We can draw from this that there was most likely an event that caused more traffic calls to happen, maybe a big sports game, weather issue, or something else. ","38da58a4":"I enjoyed learning utilizing my first kaggle kernel and getting to practice data visualization with this dataset. I would be grateful for any constructive feedback you may have for me!","82c7df2a":"It seems that, at a quick glance at these two graphs, we see that Traffic calls are generally reduced on the weekends, and that calls because of fire are much lower in number per month than EMS and Traffic. ","54ebbcd1":"We see that there are many missing data points for zip code and some for the township. Therefore, if we look at the top values in zip codes and townships, we have to keep in mind that much of the zip code data and some of the township data is missing.","9d7b3e4b":"After some quick research, it does appear that Montgomery county was experiencing extreme weather and power outages on this day. It is very likely that this was the cause of the anomoly from March 2, 2018. \n(Source:https:\/\/www.pema.pa.gov\/about\/publicinformation\/Daily%20Incident%20Reports\/20180303%20Daily%20Report.pdf)","b97302c5":"We see the first one in March. Let's track it down to what day.","cd816058":"After quickly researching the date again, we see that it was most likely due to extreme weather, just as it was March 2nd, 2018. (Source:https:\/\/patch.com\/pennsylvania\/norristown\/more-1-200-montgomery-co-peco-customers-without-power)","001e8c1c":"# Extracting Time Data\nLet's look into what time frame this dataset covers","d7585cc1":"First, let's explore the data a little.","a434dfe9":"Now, let's check the other anomoly from November of 2018. ","55ffc83f":"This cluster map more clearly shows that the hours and days of the week that have the most density are the weekdays during conventional working hours of 9 am to 6 pm. ","95bc0c46":"# Graphing Reason and Time Data\nNow, let's look at this time data with our Reason column.","eefadfec":"We see from above that the trendline is slightly negative with large variance towards the beginning and ending months of the data set. ","dd133ff1":"It appears that the values in the title column are preceded with a category. Let's split this into another column to make it easier to understand what is going on.","cde3ff47":"We see that the biggest density is on Friday in March. This is surely influenced by the weather incident that we looked at earlier that fell on Friday, March 2, 2018.\nIt's important to note that, although we expected to see a heavier density in November due to the other incident, at the time of this analysis, our data stops mid-November, making the months of November and December less valuable to look at due to the lower amount of data in the dataset. ","53eb5700":"# Dataset\nLet's get some info on this dataset","dc021757":"Now that the time stamps have been converted, we can begin adding new columns based on the time information.","b37db698":"# 911 Calls Capstone Project","74be0b79":"Start by importing the relevant libraries I plan on using as well as the dataset","82d455ef":"We see that, scanning the above table, that on the 2nd there were about 4-5 times the normal amount of calls for the rest of the month. Let's see if we can understand what might have happened on that day from the data we have.","59f64cf5":"There's a lot of spikes in the above graph, so let's do a linear regression to see the general trendline and understand our data better.","9bd8c196":"*From here, another place you could explore, is the question following question: \"Why were there more fire calls in the March 2,2018 weather incident but not the November 11th, 2018 weather incident?\". This question is beyond the scope of this analysis but it's also worth mentioning where you could go because of this data, potentially leading to more information on how to specifically prevent more fire accidents by studying these two dates and analyzing what occured and why. *","be0aafb7":"Let's find out what the heatmap of the month and the day of the week looks like.","ea77a98d":"Out of curiosity, let's continue exploring the number of calls by date, but, this time, let's break it down by reason.","5e07046e":"I will be making visualizations of this data set in order to analyze and extract insights. This is my first kernel using Kaggle.","2ffc26e3":"What we didn't notice until now, though, was that, along with the number of traffic calls being high on March 2, 2018, we also see that the number of calls for fire were abnormally high as well. With the count plot earlier, we were simply looking for a difference in distribution, not necessarily in quantity, over each category to denote it being abnormal. Now, however, going back to the count plot for March 2, 2018, we see that the fire count is around 600, much larger than the percieved average in the fire calls graph above. This shows the importance of checking each of the major categories in your data, especially ones that could lead you to conclusions, so that you can more accurately see what is going on. In this case, it wasn't just that severe weather most likely attributed to a higher traffic call count, it also attributed to higher fire count for March 2, 2018. ","a4aefcf3":"We notice giant outliers in March of 2018 and in November of 2018.","30c20c0f":"We can quickly note, from the above graph, that fires are less represented in our data set. ","d3a7d8a2":"# Investigating Outliers\nLet's investigate these two major outliers.","aba14338":"We see that most of the call density comes during the day and most prevelent on business days, which are both expected. Let's look at a cluster map of this data to better understand the similarities.","b3ba11d7":"**2018\/11\/23**\n___","39ac373b":"*It's interesting to note that, of the 3 years of data, both of the anomolies caused by extreme weather were in 2018. Further investigation beyond the scope of this analysis could be done to see if the weather in 2018 for montegomery county was significantly greater than the previous two years. It would be interesting to see what specifically caused the comparively large increase in 911 calls for those weather events and not, presumably, for any in 2016 and 2017. There could be a number of factors (power outages, awareness of incoming conditions, severity of the weather, etc.) that could be further investigated to try to find the source of what caused the increase in 911 calls. Again, this is not within the scope of this analysis, but it is worth mentioning where this could lead and the potential benefit of knowing the cause(s) to gain insight on how to better prepare for extreme weather conditions in the future..*","40fb661b":"The most clear observation we can make from the clustermap is that sunday is generally the lowest day of the week for 911 calls.","cee73a80":"# Conclusion\nIn this visual analysis we were able to practice many different visualization techniques while exploring this dataset. We used pandas to create dataframes and sift through our data, reorganizing, extracting, and graphing important data categories that we want to visualize. For the dataset, we found the EMS-related calls represented the most 911 calls, followed by traffic and then fire. We found 2 outliers that occured on March 2nd, 2018 and November 15th, 2018, both likely due to sever weather conditions, and mentioned how, with more research, you could draw insights from the investigation of these two dates and the data behind it. ","a9c6569b":"Now let's plot the total 911 calls by date. ","46f18aa2":"# Graphing Timelines\nTo continue exploring, let's find out what the actual data looks like for each reason given the date. ","cdb08a88":"We see that the first two graphs show us roughly what we expected - the traffic graph having two large outliers that we investigated earlier, and the EMS graph being about average, with the exception of a few datapoints having less than average (because these are not zero, indicating something wrong with the data, and seem like reasonable decreases, we're going to assume that they are part of the natural outliers that you would see in any dataset).","d3d070c9":"Now that we have this new column, let's look at what is happening a little closer","5a90fe33":"I will be looking at the Emergency 911 Calls Montegomery County Data set as part of one of the capstone projects for the Udemy course \"Python for data science and machine learning bootcamp\""}}