{"cell_type":{"bf186017":"code","64bc46c5":"code","801ad215":"code","b7307a1d":"code","fee01682":"code","520af1cc":"code","f93b72d2":"code","7d616c3a":"code","5ec12008":"code","5108e3e3":"code","62ff67a8":"code","fdf78d3e":"code","cf7b57fd":"code","e18842c2":"code","677ab699":"code","08953f09":"code","58eeead0":"code","ac7bf152":"code","5e43d996":"code","13114a63":"code","d089e880":"code","b0bedaa0":"code","afcdb278":"code","75ea28b6":"code","9e3dc973":"code","8e3913aa":"code","9af249ab":"code","668e0673":"code","90209c07":"code","d7bd4af4":"code","6025c367":"code","9ab306ae":"code","e8b89bda":"code","984bdc29":"code","97c09ebf":"code","2d59fb81":"code","142a6bbc":"markdown","221046a8":"markdown","67de0622":"markdown","e294a811":"markdown","4ff01f69":"markdown","47fb0d32":"markdown","fbe2d276":"markdown","b812c781":"markdown","7c3c67a4":"markdown"},"source":{"bf186017":"import pandas as pd\n\ntrain_data = pd.read_csv(\"..\/input\/zeemee-micro-competition-data\/zeemee_train.csv\")\ntest_data = pd.read_csv(\"..\/input\/zeemee-micro-competition-data\/zeemee_test.csv\")","64bc46c5":"train_data.columns","801ad215":"train_data.describe()","b7307a1d":"test_data.describe()","fee01682":"train_data.head()","520af1cc":"train_data.shape","f93b72d2":"## This section is so that I can view the types of data being used for each catagory and look for missing values\n## This was also useful for figuring out which values were catagorical and which were ordinal\n##some sections have been commented out as the output is lengthy and not frequently useful to view\n\n\nprint(\"college: \")\ndisplay(train_data.college.unique())\nprint(\"\\n\\npublic profile enabled: \")\ndisplay(train_data.public_profile_enabled.unique())\nprint(\"\\n\\ngoing: \")\ndisplay(train_data.going.unique())\nprint(\"\\n\\ninterested: \")\ndisplay(train_data.interested.unique())\nprint(\"\\n\\nstart term: \")\ndisplay(train_data.start_term.unique())\nprint(\"\\n\\ncohort year: \")\ndisplay(train_data.cohort_year.unique())\nprint(\"\\n\\ncreated by csv: \")\ndisplay(train_data.created_by_csv.unique())\n#print(\"\\n\\nlast login: \") \n#display(train_data.last_login.unique()) ##found Nan here\nprint(\"\\n\\nschools followed: \")\ndisplay(train_data.schools_followed.unique())\nprint(\"\\n\\nhigh school: \")\ndisplay(train_data.high_school.unique())\nprint(\"\\n\\ntransfer status: \")\ndisplay(train_data.transfer_status.unique())\nprint(\"\\n\\nroommate match quiz: \")\ndisplay(train_data.roommate_match_quiz.unique())\n#print(\"\\n\\nchat messages sent: \")\n#display(train_data.chat_messages_sent.unique())\n#print(\"\\n\\nchat viewed: \")\n#display(train_data.chat_viewed.unique())\n#print(\"\\n\\nvideos liked: \")\n#display(train_data.videos_liked.unique())\n#print(\"\\n\\nvideos viewed: \")\n#display(train_data.videos_viewed.unique())\n#print(\"\\n\\nvideos veiwed unique: \")\n#display(train_data.videos_viewed_unique.unique())\n#print(\"\\n\\ntotal official videos: \")\n#display(train_data.total_official_videos.unique())\nprint(\"\\n\\nengaged: \")\ndisplay(train_data.engaged.unique())\nprint(\"\\n\\nfinal funnel stage: \")\ndisplay(train_data.final_funnel_stage.unique())","7d616c3a":"display(test_data.cohort_year.unique())\ndisplay(test_data.college.unique())\ndisplay(test_data.start_term.unique())","5ec12008":"## train_data = train_data[train_data.cohort_year != 2017]\n## this line lost accuracy in the model, interesting hypothesis though","5108e3e3":"print('is null in training data: ')\ndisplay(train_data.isnull().sum())\nprint('\\n\\nis null in test data: ')\ndisplay(test_data.isnull().sum())","62ff67a8":"train_data['last_login'] = train_data['last_login'].fillna(0)\n## I tried running the model by filling the null entry with 0 and then again with 1500\n## the assumption was that the null entry was either from the time being too long \n## to record or the entry being null because the user was loging on at the time of capture\ntest_data['last_login'] = test_data['last_login'].fillna(0)","fdf78d3e":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\nprint('Setup Complete')","cf7b57fd":"def final_funnel_num(val):\n    if val == 'Inquired':\n        return 0\n    \n    if val == 'Applied':\n        return 0\n    \n    if val == 'Accepted':\n        return 0\n    \n    if val == 'Deposited':\n        return 1\n    \n    if val == 'Application_Complete':\n        return 0\n    \n    if val == 'Enrolled':\n        return 1\n    \n    else:\n        return 'error'\n    \n## at this point in the process you may be wondering why I'm not using one of the in built encoding methods in sklearn over making my own\n## When dealing with ordinal values (values that can be converted to integers and still hold meaning) it is benificial to hand encode values to ensure your model is not loosing some of the meaning in the data\n\n## Also worth pointing out here, initially I had thought i needed to predict all 6 outcomes for the model, when I double checked the rules I discovered that the model was supposed to predict a binary outcome of either or for a subset of the outcomes (Enrolled and Deposited)\n## which is why this function seems a bit silly, about an hour and a half before the deadline i discovered this error and went for the simplest fix.\n## interestingly enough when I was predicting 6 outcomes the model was 54% accurate in testing (17% is the accuracy that dice would have had)\n## I'm also curious how it would have effected accuracy if i had the algorithm predict all six outcomes and then translated that result to the desired binary output","e18842c2":"def num_bool(val):\n    if val == True:\n        return 1\n    elif val == False:\n        return 0\n    else:\n        return 'error'\n## The return code 'error' would actually be an error when the data is fed into the model\n## as a random forrest cannot handle values that are non numeric","677ab699":"def goin_num(val):\n    if val == 'undecided':\n        return 0\n    if val == 'going':\n        return 1\n    if val == 'notgoing':\n        return -1\n    else:\n        return 'error'","08953f09":"def col_num(val):\n    if val == 'college1':\n        return 1\n    if val == 'college2':\n        return 2\n    if val == 'college4':\n        return 4\n    if val == 'college3':\n        return 3\n    if val == 'college5':\n        return 5\n    if val == 'college6':\n        return 6\n    if val == 'college7':\n        return 7\n    if val == 'college8':\n        return 8\n    else:\n        return 'error'","58eeead0":"def term_num(val):\n    if val == 'fall':\n        return 1\n    if val == 'spring':\n        return 2\n    if val == 'summer':\n        return 3\n    else:\n        return 'error'","ac7bf152":"## since I'm using a random forest model for this project I will be converting all values I wish to use to numbers\n## most likely i will have to drop some of these values for my final project, but its nice to have options\ntrain_data['funnel_num'] = pd.Series([final_funnel_num(x) for x in train_data.final_funnel_stage], index=train_data.index)\ntrain_data['transfer_status_num'] = pd.Series([num_bool(x) for x in train_data.transfer_status], index=train_data.index)\ntrain_data['public_profile_enabled_num'] = pd.Series([num_bool(x) for x in train_data.public_profile_enabled], index=train_data.index)\ntrain_data['interested_num'] = pd.Series([num_bool(x) for x in train_data.interested], index=train_data.index)\ntrain_data['created_by_csv_num'] = pd.Series([num_bool(x) for x in train_data.created_by_csv], index=train_data.index)\ntrain_data['roommate_match_quiz_num'] = pd.Series([num_bool(x) for x in train_data.roommate_match_quiz], index=train_data.index)\ntrain_data['going_num'] = pd.Series([goin_num(x) for x in train_data.going], index=train_data.index)\ntrain_data['college_num'] = pd.Series([col_num(x) for x in train_data.college], index=train_data.index)\ntrain_data['start_term_num'] = pd.Series([term_num(x) for x in train_data.start_term], index=train_data.index)\n## Also worth mentioning: isn't it wonderful not having to scale features for a random forest model","5e43d996":"train_data.describe()","13114a63":"y = train_data.funnel_num\nrf_features = ['cohort_year',  'going_num', 'chat_messages_sent', 'schools_followed', 'videos_liked',  'chat_viewed', 'total_official_videos',  'videos_viewed','transfer_status_num',  'videos_viewed_unique', \n               'public_profile_enabled_num', 'created_by_csv_num', 'interested_num', 'roommate_match_quiz_num', 'college_num']\nX = train_data[rf_features]\n\n## what isn't pictured here is the trial and error as I remove various components, and reran the next 2 cells to compare accuracy.\n## This process also taught me something new, the order the features are in affects the model created\n\n## As a worthwhile point to mention, I had initially guessed that using label encoding for the college would have confused a random forrest and that i was going to need to retry using one hot encoding or a sparse vector instead, \n## however using label encoding did have a positive end result (assuming I didn't overfit the model)\n\n## start_term_num and last_login was removed as it did not improve the accuracy of the model\n\n## Other notes:  roommate_match_quiz_num and public_profile_enabled_num had an almost insignifigant impact on accuracy, I chose to leave them in because the impact was positive\n## however if this were a production enviorment i might drop it for computational efficiency\n\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)","d089e880":"forest_model = RandomForestRegressor(random_state=0)\nforest_model.fit(train_X, train_y)\ntd_preds = forest_model.predict(val_X)\nprint(mean_squared_error(val_y, td_preds))","b0bedaa0":"td_preds = td_preds.round()","afcdb278":"print(td_preds.max(), td_preds.min())\n## Checking to make sure there are no nonsensicle outputs","75ea28b6":"print(mean_squared_error(val_y, td_preds))","9e3dc973":"## this cell I added after the competition. Since the data sample has a split of about 1 positive case in 10\n## I wanted to compare other evaluation methods\nfrom sklearn.metrics import roc_auc_score\n\nprint(roc_auc_score(val_y, td_preds))","8e3913aa":"full_preds = forest_model.predict(train_data[rf_features])\n\ntrain_data['preds'] = full_preds.round()","9af249ab":"train_data.head(20)","668e0673":"y = 0\nx = 0\nfor index, row in train_data.iterrows():\n#    print(row['funnel_num'])\n    if row['funnel_num'] == row['preds']:\n        x = x + 1\n    else:\n        y = y + 1\nprint(\"Percent accuracy of model is: \", (x\/(x+y))*100)","90209c07":"y = 0\nx = 0\nn = 0\nq = 0\nfor index, row in train_data.iterrows():\n#    print(row['funnel_num'])\n    if row['funnel_num'] == 1:\n        x = x + 1\n    else:\n        y = y + 1\n    if row['preds'] == 1:\n        n = n + 1\n    else:\n        q = q + 1\n        \nprint('Percentage of positives in sample: ', (x\/(x+y))*100, '\\nPercentage of predicted positives in sample: ', (n\/(n+q))*100)","d7bd4af4":"test_data['transfer_status_num'] = pd.Series([num_bool(x) for x in test_data.transfer_status], index=test_data.index)\ntest_data['public_profile_enabled_num'] = pd.Series([num_bool(x) for x in test_data.public_profile_enabled], index=test_data.index)\ntest_data['interested_num'] = pd.Series([num_bool(x) for x in test_data.interested], index=test_data.index)\ntest_data['created_by_csv_num'] = pd.Series([num_bool(x) for x in test_data.created_by_csv], index=test_data.index)\ntest_data['roommate_match_quiz_num'] = pd.Series([num_bool(x) for x in test_data.roommate_match_quiz], index=test_data.index)\ntest_data['going_num'] = pd.Series([goin_num(x) for x in test_data.going], index=test_data.index)\ntest_data['college_num'] = pd.Series([col_num(x) for x in test_data.college], index=test_data.index)","6025c367":"test_preds = forest_model.predict(test_data[rf_features])\nprint('values should be 0  and 1.0: ', test_preds.min(), test_preds.max())\ntest_data['preds'] = test_preds.round()\ntest_data.head(20)","9ab306ae":"#def funnel_final(val):    \n#    if val == 1:\n#        return 'Inquired'\n#    \n#    if val == 2:\n#        return 'Applied'\n#    \n#    if val == 3:\n#        return 'Accepted'\n#    \n#    if val == 4:\n#        return 'Deposited'\n#    \n#    if val == 5:\n#        return 'Application_Complete'\n#    \n#    if val == 6:\n#        return 'Enrolled'","e8b89bda":"## Now to bring this hot mess full circle\ntest_data['final_funnel_stage'] = pd.Series([x for x in test_data.preds], index=test_data.index)","984bdc29":"test_data.head()","97c09ebf":"test_data = test_data[['college', 'public_profile_enabled', 'going', 'interested',\n       'start_term', 'cohort_year', 'created_by_csv', 'last_login',\n       'schools_followed', 'high_school', 'transfer_status',\n       'roommate_match_quiz', 'chat_messages_sent', 'chat_viewed',\n       'videos_liked', 'videos_viewed', 'videos_viewed_unique',\n       'total_official_videos', 'engaged', 'final_funnel_stage']]\nprint(test_data.shape) ## checking to make sure we're back to 20 columns and that i didn't screw something up","2d59fb81":"#test_data.to_csv(\"zeemee_test_output.csv\")\n#output file for the copetition","142a6bbc":"So we can see the test data doesn't have entrys from 2017 (which the training set has), I'm curious how much the data will be affected by seasonality. More than 75% of the entries are from 2019 in both sets either way.\n\nIf there is more time I might want to build a different model for each year to see if that will prevent overfitting.","221046a8":"ZeeMee Mini-Hackathon by MatrixDS\n\nGet started by fork lifting this project! (Green button in upper right). Then just build an R or Python tool. Make sure if you are on a team, just use one project and add your other team members!\n\nZeeMee is a fast growing silicon valley startup that has a social network for high school students looking for colleges. Students use the ZeeMee platform, through Android and iOS apps, to connect with others who are interested in the same colleges.\n\nThe goal of this competition is to use data collected about the students behavior on the zeemee platform to predict if they will enroll in a specific college. The dataset contains 19 features which are described here.\n\n    college: The college of interest that a particular student is following\n    public_profie_enabled: If the student has made their zeemee profile public\n    going: If the student has stated (in a non-binding way) on the zeemee app that they are going to the college\n    interested: If the student has stated they are interested in the college on the zeemee app\n    start_term: Which term the student is projected to begin class\n    cohort_year: Which year the student is projected to begin class\n    created_by_csv: If the students zeemee account associated to the college as part of a batch upload\n    last_login: Number of days since the last login\n    schools_followed: Number of schools followed on the zeemee platform\n    high_school: Which high school the student attends\n    transfer_status: If the student is transferring from another college\n    roomate_match_quiz: If the student filled out a ZeeMee provided quiz to match with a roomate at the college of interest\n    chat_messages_sent: Number of messages sent\n    chat_viewed: Number of chats viewed\n    videos_liked: Number of videos liked\n    videos_viewed: Number of videos viewed\n    videos_viewed_unique: Number of unique videos viewed\n    offical_videos: Number of videos produced by the college of interest\n    engaged: If the student is engaged with the college on the zeemee app\n    final_funnel_stage: What stage in the enrolment process did the student end\n\nThe goal of your model is to predict final_funnel_stage. The funnel is a series of steps that a student moves through on their way to actually showing up to class. The stages are thought of in the following progression:\n\n    Inquired: Expressed interest in the college on the zeemee app\n    Applied: Filled out some part of an application from the college\n    Application_Complete: Completed an application from the college\n    Accepted: Accepted by the college\n    Deposited: Paid a deposit to the college\n    Enrolled: Enrolled in class at the college\n\nThe prediction of interest for this competition is to focus on identifying students that enroll (funnel stage Enrolled or Deposited). This is a binary prediction, either the student does or does not enroll. Use the two csv files in the data folder to build your model. There is a training data file and a test data file.\n\n----\n\nGrading\n\nYou will be graded on two areas each consisting of a possible 5 points.\n\nFirst the average accuracy of your predictions on the testing data. Average accuracy is defined as the arithmetic average of accuracy for both classes of enrolled students and non-enrolled students. Predictions should be added to the zeemee_test.csv file. The best accuracy will receive all 5 points and other submissions will receive a relative portion of points to the best performer.\n\nSecond you must submit an exploration of important features for your prediction model. The explanation of features and feature engineering will receive a subjective score from a panel from ZeeMee and MatrixDS. This score will be out of a possible 5 points. Submissions can be in the format of a notebook or rmarkdown file.\n\nTotal scores for the two areas will be added and the individual and teams with the highest talley will receive the cash and interview prizes.\n\n----\n\nSubmission. You will submit your solution using a public MatrixDS project at the end of the hackathon. We will grade the predictions that you append to the zeemee_test.csv file and any feature explaining documents in the analysis folder in your public project. Please represent your predictions as a numeric\/binary value 0 (wont enroll) or 1 (will enroll) in the submission file. The submission form will be provided on the day of the competition.","67de0622":"Thanks for taking the time to review my submission. \n\nThe following is a copy of the notebook which I submitted to the ZeeMee Mini-Hackathon competition, The model had an accuracy of .936 on the test data and a score of .949 on the training data. The model itself had gotten third place for accuracy, however it did not rank for the analysis, because I ran out of time. I have cleaned up some of the cells but have otherwise left the code intact. My notes will seem a bit critical of my entrys shortcomings, this is because I am looking specifically for areas of improvement, the success of my model wasn't lost on me.\n\n\nThe purpose of this is mostly to demonstrate my thought process when aproaching these sorts of challenges.\n\nI chose a random forest model to predict the outcomes for two simple reasons. The first is that I have a fair bit of practice with random forests and would be able to troubleshoot any issues that arose and refine the model to get the best outcome. The second reason is the time limit, since I have 6 hours to work on this I wanted to make sure I was able to deliver a complete final product.\n\nIf the competition were to go for longer I would be tempted to use either a nueral net, as those can produce increadible accuracy if you train it right, or a support vector machine since I've wanted to get some practice with one of those to broaden my skillset. However, in a production enviorment there are two other models I would have perfered for use: a decision tree or a perceptron model. Both the decision tree and the perceptron model have one perticular area that they work very well in: explainability. Both models could be deconstructed for answer further questions, instead of determining what the likelyhood of a perticular student enrolling in college, why not find what perticular area of study would prove the greatest impact of their success (ex: 'by taking on one more extra corricular activity you increase the likelihood of being excepted to school by 12%').","e294a811":"\nThe cell below was copied directly from the MatrixDS page for the event","4ff01f69":"Now that we have an idea of the accuracy of the model (and that its better than throwing darts at a board), let's wrap this hot mess up into the test set","47fb0d32":"As you can see the greatest failing of the project was that I did not ration my time effectively, I didn't finish my EDA before I jumped into the model and had to use trial and error in its place. I had optimistically imported seaborn and matplotlib, but failed to use them. Especially with the encoding methods I used I missed out on a golden opportunity to review the differences in the training and test dataset.\n\nSecondly with testing, I did not check for overfitting and simply submitted and crossed my fingers (as a result of running out of time). In hindsight I know that the model hadn't overfit and was similairly effective in both the training and test data.\n\nThird I would have liked to capture false positive, and fale negitive rate for the model as well [use a confusion matrix] as opposed to just capturing MAE and percentage (these at least provided quick feedback to adjust the various itterations of the model). Ideally in a production enviorment I would have liked to tweek the model so that it had very little false negitives even if it meant that the model became too optimistic (loosing just a couple students would outweigh the benifits of cutting advertising to the one who wouldn't be attainable).","fbe2d276":"Part -1:\nDrop everything and focus on finishing one successful model as fast a possible entry! oh god how is it half way to time already.\n\nBy this point in the competition I realized i hadn't be executing with the right sence of urgency and needed to rush towards a solution else risk having nothing to present.","b812c781":"Part 1:\nExploritory Data Analysis","7c3c67a4":"*As one quick note:\nthe version of the test csv was overritten at the end of the competition so the test set no longer has missing values, it also has answers in it (which are actually my predictions). The part below was written during the competition*\n\nOdd that there are missing values but that they are so rare. I'm curious if those three entry's are outliers in the dataset or just the result of a bug.\nI had experimented with changing the missing values to 0, mean, and 1600 (max value), in the end using 0 had the most positive effect on the accuracy of the model (It's also strange that 2 missing values would even have a measurable impact)"}}