{"cell_type":{"5e9a6152":"code","26560cfc":"code","b0bcc8c7":"code","b653c870":"code","20101457":"code","80b53917":"code","6a48a336":"code","f5139f9e":"code","a015ce9c":"markdown","3d07a2db":"markdown","29260e6a":"markdown"},"source":{"5e9a6152":"!pip install pyspark","26560cfc":"import os\nimport pandas as pd\nimport numpy as np\n\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession, SQLContext\n\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import udf, col\n\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.mllib.evaluation import RegressionMetrics\n\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nimport matplotlib.pyplot as plt\n\nspark = SparkSession.builder.master(\"local[2]\").appName(\"Linear-Regression-California-Housing\").getOrCreate()\npath = '..\/input\/hausing-data\/cal_housing.data'\n\nschema = StructType([\n    StructField(\"long\", FloatType(), nullable=True),\n    StructField(\"lat\", FloatType(), nullable=True),\n    StructField(\"medage\", FloatType(), nullable=True),\n    StructField(\"totrooms\", FloatType(), nullable=True),\n    StructField(\"totbdrms\", FloatType(), nullable=True),\n    StructField(\"pop\", FloatType(), nullable=True),\n    StructField(\"houshlds\", FloatType(), nullable=True),\n    StructField(\"medinc\", FloatType(), nullable=True),\n    StructField(\"medhv\", FloatType(), nullable=True)]\n)\n\nhousing_df = spark.read.csv(path=path, schema=schema).cache()","b0bcc8c7":"housing_df.show(5)","b653c870":"## Maybe create some features here ##\n\n## YOUR CODE HERE ##","20101457":"feature_cols = ## YOUR CODE HERE ##\n\nassembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\nassembled_df = assembler.transform(housing_df)\n\n##Maybe scale your features here ##\n## YOUR CODE HERE ##\n\ntrain_data, test_data = assembled_df.randomSplit([.8,.2])","80b53917":"## YOUR CODE HERE: Fit a regression model to the data ##","6a48a336":"evaluator = RegressionEvaluator(\n    labelCol=\"medhv\", predictionCol=\"prediction\", metricName=\"rmse\")\n\nmse = evaluator.evaluate(predictions)\n\nprint(\"MSE:\",mse)","f5139f9e":"spark.stop()","a015ce9c":"# Task 4\n\nREMEMBER TO SWITCH OVER TO KAGGLE AT https:\/\/www.kaggle.com\/mehulraheja\/keras-apache-spark-task-4\n\nNow, we'll try to use spark functions to improve the quality of our predictions on the california housing data. For convenience, much of the code to load data and run the model is copied from the previous part. This activity is pretty free form and will be graded for effort instead of accuracy. Here are some ideas of feature engineering you can do to improve the quality of your predictions:\n\n1. Try different types of regression (which can be found here) https:\/\/spark.apache.org\/docs\/2.1.1\/ml-classification-regression.html\n2. Think of other features which may be indicative of this data. Some of the most important ones could be rooms, population, or bedrooms per household (which is more helpful than rooms, populations, or bedrooms per block group)\n3. Try to scale the features using StandardScaler\n","3d07a2db":"### CREDITS:\nTHIS NOTEBOOK IS HEAVILY INSPIRED BY THE ONE BY FATMAKURSUN WHICH YOU CAN FIND HERE https:\/\/www.kaggle.com\/fatmakursun\/pyspark-ml-tutorial-for-beginners. SOME BLOCKS OF CODE, FOR EXAMPLE LOADING THE DATASET, ARE TAKEN DIRECTLY FROM IT.","29260e6a":"## Housing Data Set\n\nThe California Housing data set appeared in a 1997 paper titled *Sparse Spatial Autoregressions*, written by Pace, R. Kelley and Ronald Barry and published in the Statistics and Probability Letters journal. The researchers built this data set by using the 1990 California census data.\n\nThe data contains one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). In this sample a block group on average includes 1425.5 individuals living in a geographically compact area.\n\nThese spatial data contain 20,640 observations on housing prices with 9 economic variables:\n\n<p style=\"text-align: justify;\"><\/p>\n<pre><strong>Longitude:<\/strong>refers to the angular distance of a geographic place north or south of the earth\u2019s equator for each block group\n<strong>Latitude :<\/strong>refers to the angular distance of a geographic place east or west of the earth\u2019s equator for each block group\n<strong>Housing Median Age:<\/strong>is the median age of the people that belong to a block group. Note that the median is the value that lies at the midpoint of a frequency distribution of observed values\n<strong>Total Rooms:<\/strong>is the total number of rooms in the houses per block group\n<strong>Total Bedrooms:<\/strong>is the total number of bedrooms in the houses per block group\n<strong>Population:<\/strong>is the number of inhabitants of a block group\n<strong>Households:<\/strong>refers to units of houses and their occupants per block group\n<strong>Median Income:<\/strong>is used to register the median income of people that belong to a block group\n<strong>Median House Value:<\/strong>is the dependent variable and refers to the median house value per block group\n<\/pre>\n\nWhat's more, we also learn that all the block groups have zero entries for the independent and dependent variables have been excluded from the data.\n\nThe Median house value is the dependent variable and will be assigned the role of the target variable in our ML model."}}