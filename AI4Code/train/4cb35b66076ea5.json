{"cell_type":{"8a515a53":"code","9ec8e172":"code","692d5626":"code","0f54199b":"code","b43dd0e6":"code","c6a92e4b":"code","84211525":"code","f6c2435d":"code","6bc57823":"code","e7d72294":"code","1be7dead":"code","f1fb59b8":"code","91f307a0":"code","7abe1483":"code","920a8832":"code","ca5a569b":"code","2ee80beb":"code","0c50ec0d":"code","9e30a2ac":"code","7772d935":"code","3380d33a":"code","f54a7b5e":"code","2c806974":"markdown","8bca15b5":"markdown","46be7ad1":"markdown","f02761c8":"markdown","702d2e32":"markdown","08c431a9":"markdown","bda17934":"markdown"},"source":{"8a515a53":"import pandas as pd\ntrain_data = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_data = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntrain_data.head()","9ec8e172":"data = pd.concat([train_data,test_data],axis=0)\nprint(data.describe())\nimport matplotlib.pyplot as plt\nplt.hist(data.target)\nplt.show()","692d5626":"import re\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","0f54199b":"import spacy\nnlp = spacy.load('en')\ndef preprocessing(text):\n  text = text.replace('#','')\n  text = decontracted(text)\n  text = re.sub('\\S*@\\S*\\s?','',text)\n  text = re.sub('http[s]?:(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',text)\n\n  token=[]\n  result=''\n  text = re.sub('[^A-z]', ' ',text.lower())\n  \n  text = nlp(text)\n  for t in text:\n    if not t.is_stop and len(t)>2:  \n      token.append(t.lemma_)\n  result = ' '.join([i for i in token])\n\n  return result.strip()","b43dd0e6":"data.text = data.text.apply(lambda x : preprocessing(x))","c6a92e4b":"from nltk.tokenize import word_tokenize\nfrom tqdm import tqdm\nfrom nltk.corpus import stopwords\nstop=set(stopwords.words('english'))\ndef create_corpus(df):\n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n        corpus.append(words)\n    return corpus","84211525":"corpus=create_corpus(data)","f6c2435d":"import numpy as np\nembedding_dict={}\nwith open('..\/input\/glove6b200d\/glove.6B.200d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","6bc57823":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nMAX_LEN=40\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","e7d72294":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","1be7dead":"\nnum_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,200))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec","f1fb59b8":"from keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D,Dropout\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam\nmodel=Sequential()\n\nembedding=Embedding(num_words,200,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(64,dropout=0.2, recurrent_dropout=0.2))\n\n\nmodel.add(Dense(1, activation='sigmoid'))\n\n\n\noptimzer=Adam(learning_rate=0.001)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])\nmodel.summary()","91f307a0":"from keras.callbacks import ModelCheckpoint,EarlyStopping\nfilepath=\"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\nes = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=30)\ncallbacks_list = [checkpoint,es]","7abe1483":"\ntweet =data.iloc[:7613,:]\ntest = data.iloc[7613:,:]","920a8832":"\ntrain=tweet_pad[:tweet.shape[0]]\ntest=tweet_pad[tweet.shape[0]:]","ca5a569b":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.2)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","2ee80beb":"history=model.fit(X_train,y_train,batch_size=100,epochs=20,callbacks=callbacks_list,validation_data=(X_test,y_test),verbose=1)","0c50ec0d":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","9e30a2ac":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","7772d935":"# from keras.models import load_model\n# model = load_model('..\/input\/finalmodel\/weights-improvement-21-0.84.hdf5')","3380d33a":"sample_sub=pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","f54a7b5e":"y_pre = model.predict(test)\ny_pre=np.round(y_pre).astype(int).reshape(3263)\nsub=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pre})\nsub.to_csv('submission.csv',index=False)","2c806974":"# **IMPLEMENTING MODEL**","8bca15b5":"# **IMPORT DATA**","46be7ad1":"# **USING PRE-TRAINED GLOVE WEIGHTS**","f02761c8":"# **TEXT PREPROCESSING**","702d2e32":"##DON'T FORGET TO UPVOTE##\n##1ST NOTEBOOK KERNEL##","08c431a9":"# **LOADING BEST MODEL AND PREDICTING RESULTS**","bda17934":"# **USING CALLBACKS**"}}