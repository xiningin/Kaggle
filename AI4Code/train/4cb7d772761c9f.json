{"cell_type":{"2e1b4e78":"code","31cefdeb":"code","29958eeb":"code","93a89468":"code","d81bc367":"code","3d87a137":"code","499b19a2":"code","ddd3ab03":"code","73c8ea85":"code","af12028d":"code","6f96d100":"code","ad66e6eb":"code","8763e3ed":"code","930a2e35":"code","6c25b0a3":"code","a1444aa9":"code","0cabcca3":"code","11867792":"code","d483776a":"code","b69dc5d6":"code","7c0da5ab":"code","9e06bb81":"code","15ef43f3":"code","edee7169":"code","05c95bea":"code","dc51679f":"code","f0d3d43c":"code","cf0acea0":"code","4ab0379b":"code","477ae79d":"code","77a003ae":"code","0ab301a9":"markdown","ca0d2da1":"markdown","1ff45e40":"markdown","4be70fd0":"markdown","583c0442":"markdown","0be70951":"markdown","509ebd28":"markdown","e286c8d1":"markdown","8609103b":"markdown","f5083667":"markdown","f9f5c9de":"markdown","75420e52":"markdown","f998061c":"markdown","3ef76349":"markdown","fc485f0f":"markdown","61ddb8df":"markdown","666198a5":"markdown","88362d82":"markdown","4000af73":"markdown"},"source":{"2e1b4e78":"!pip install -q scanpy\n\nimport scanpy as sc\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score\nfrom matplotlib import pyplot as plt\n# magic incantation to help matplotlib work with our jupyter notebook\n%matplotlib inline ","31cefdeb":"adata = sc.read('..\/input\/theory-introto-singlecell-rnaseq-images\/brain_normalized\/brain_normalized.h5ad')","29958eeb":"adata.var.head()","93a89468":"adata.obs.head()","d81bc367":"adata.X.shape","3d87a137":"sc.tl.tsne(adata, perplexity=30, learning_rate=1000, random_state=0)  # add X_tsne in obsm\n\nsc.pl.tsne(adata, color='cell_ontology_class')  # Plots using X_tsne which is tSNE coordinates of data.","499b19a2":"adata","ddd3ab03":"sc.pp.neighbors(adata) # UMAP is based on the neighbor graph; we'll compute this first\n# |_ Compute a neighborhood graph of observations\n# Depending on copy, updates or returns adata with the following: connectivities and distances.\nsc.tl.umap(adata, min_dist=0.5, spread=1.0, random_state=1, n_components=2)  # # add X_umap in obsm","73c8ea85":"sc.pl.umap(adata, color='cell_ontology_class')","af12028d":"adata","6f96d100":"adata.X.shape","ad66e6eb":"adata.write('brain_embeddings.h5ad')","8763e3ed":"adata = sc.read('brain_embeddings.h5ad')","930a2e35":"import scanpy as sc\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score\nfrom matplotlib import pyplot as plt\n%matplotlib inline","6c25b0a3":"adata","a1444aa9":"umap_coordinates = adata.obsm['X_umap'] # extract the UMAP coordinates for each cell\nkmeans = KMeans(n_clusters=4, random_state=0).fit(umap_coordinates) # fix the random state for reproducibility\n\nadata.obs['kmeans'] = kmeans.labels_ # retrieve the labels and add them as a metadata column in our AnnData object\nadata.obs['kmeans'] = adata.obs['kmeans'].astype(str)\n\nsc.pl.umap(adata, color='kmeans') # plot the results, Ploting the umap with the color as the cluster label","0cabcca3":"print(kmeans.labels_)\nadata.obs.head()","11867792":"adata.obs['cell_ontology_class'].unique()","d483776a":"rand_index = adjusted_rand_score(labels_true = adata.obs['cell_ontology_class'], labels_pred = adata.obs['kmeans'])\nprint('The Rand index is', round(rand_index, 2))","b69dc5d6":"# Saving this results with column_name = kmeans_7\n\numap_coordinates = adata.obsm['X_umap'] # extract the UMAP coordinates for each cell\nkmeans = KMeans(n_clusters=7, random_state=0).fit(umap_coordinates) # fix the random state for reproducibility\n\nadata.obs['kmeans_7'] = kmeans.labels_ # retrieve the labels and add them as a metadata column in our AnnData object\nadata.obs['kmeans_7'] = adata.obs['kmeans_7'].astype(str)\n\nsc.pl.umap(adata, color='kmeans_7') # plot the results, Ploting the umap with the color as the cluster label","7c0da5ab":"rand_index = adjusted_rand_score(labels_true = adata.obs['cell_ontology_class'], labels_pred = adata.obs['kmeans_7'])\nprint('The Rand index is', round(rand_index, 2))","9e06bb81":"!pip install louvain","15ef43f3":"from scanpy.tl import louvain\nlouvain(adata)  # Cluster cells into subgroups # adata.obs['louvain']\nsc.pl.umap(adata, color='louvain')","edee7169":"adata.obs.head()\n# louvain column is added","05c95bea":"adata","dc51679f":"rand_index = adjusted_rand_score(adata.obs['cell_ontology_class'], adata.obs['louvain'])\nprint('The rand index is ', round(rand_index, 2))","f0d3d43c":"louvain(adata, resolution = 0.1)  # Cluster cells into subgroups # adata.obs['louvain']\nsc.pl.umap(adata, color='louvain')\n\nrand_index = adjusted_rand_score(adata.obs['cell_ontology_class'], adata.obs['louvain'])\nprint('The rand index is ', round(rand_index, 2))","cf0acea0":"cerebellum = adata[adata.obs['subtissue'] == 'Cerebellum']\nsc.pp.neighbors(cerebellum)\nsc.tl.umap(cerebellum)\n\nsc.tl.louvain(cerebellum)\nsc.pl.umap(cerebellum, color='louvain')","4ab0379b":"cerebellum","477ae79d":"sc.tl.louvain(adata, resolution=0.1)\nadata.write('brain_clusters.h5ad')","77a003ae":"!zip \"brain_clusters.zip\" \".\/brain_clusters.h5ad\"","0ab301a9":"# Graph-based methods \n- Graph-based methods attempt to partition a pre-computed neighhbor graph into modules (i.e., groups \/ clusters of cells) based on their connectivity. \n- Currently, the most widely used graph-based methods for single cell data are variants of the louvain algorithm. The intuition behind the louvain algorithm is that it looks for areas of the \n![image.png](attachment:84023260-87e7-496c-87ee-47c69b93b271.png)","ca0d2da1":"<br>\n<h1 style = \"font-size:60px; font-family:Garamond ; font-weight : normal; background-color: #f6f5f5 ; color : #fe346e; text-align: center; border-radius: 100px 100px;\">Dimensionality reduction & Clustering in Single cell RNA-seq data<\/h1>\n<br>\n\n<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Dimensionality Reduction<\/h1>\n\n- Dimensionality reduction methods seek to take a large set of variables and return a smaller set of components that still contain most of the information in the original dataset. \n- This implies an inherent tradeoff between information loss and improved interpretability: all dimensionality reduction methods discard some information, but they also play an important role in helping us make sense of a giant matrix of values.\n\nWe already saw one example of dimensionality reduction in PCA in the <a href=\"https:\/\/www.kaggle.com\/aayush9753\/3-normalization-pca-in-single-cell-rna-seq-data\/notebook?select=brain_normalized.zip\">previous notebook<\/a> \n\nLet's look at two other common approaches to dimensionality reduction: tSNE and UMAP.\n\n# tSNE \n- tSNE (t-Distributed Stochastic Neighbor Embedding) combines dimensionality reduction (e.g. PCA) with random walks on the nearest-neighbour network to map high dimensional data (i.e. our 18,585 dimensional expression matrix) to a 2-dimensional space. \n- In contrast with PCA, tSNE can capture nonlinear structure in the data, and tries to preserve the local distances between cells. - - Due to the non-linear and stochastic nature of the algorithm, tSNE is more difficult to intuitively interpret: while tSNE faithfully represents local relationships, it doesn't always capture the relatioships between more distant cells correctly.\n-  The method when run multiple times on the same dataset will result in different plots because tSNE is a stochastic algorithm. \n- To ensure reproducibility, we fix the \"seed\" of the random-number generator in the code below so that we always get the same plot.","1ff45e40":"Here, we see that tSNE generally does a good job of grouping similar cell types together (much better than PCA alone), but there are still some neurons that are not grouped together.","4be70fd0":"- Sometimes, we may want to look at clusters within a given tissue or cell type designation. This can surface interesting heterogeneity between subpopulations, although it can also make our results more noisy.\n- **Looking into the cells having Cerebellum subtissue.**","583c0442":"Here, we see that UMAP generally does a a better job of grouping like cells together and achieving clean separation between cell types. \n\nLet's save our anndata object with our new embeddings to file for later use.","0be70951":"<br>\n<h1 style = \"font-size:60px; font-family:Garamond ; font-weight : normal; background-color: #f6f5f5 ; color : #fe346e; text-align: center; border-radius: 100px 100px;\">Summary<\/h1>\n<br>\n\n#### Dimensionality Reduction\n1. tSNE\n2. UMAP\n\n#### Clustering\n- k-Means on tSNE\n- Evaluating the k-means clustering\n- Playing with No of cluster in k-means\n- **Graph Based Clustering Method** - Louvain \n- Tuning thr resolution parameter\n- Seeing clusters in cells of a perticular sybtissue\n- Saving the adata\n","509ebd28":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Clustering<\/h1>\n\n- Grouping cells based on the similarity of their expression profiles allows us to identify cell types and states, as well as infer differences between groups. This is done either via clustering or community detection.\n\n- Some of the most popular approaches are hierarchical clustering and k-means clustering. These methods compute a **distance metric between cells (often based on a low-dimensional representation, such as PCA, tSNE or UMAP), and then iteratively group cells together based on these distances.**\n\n- **Community detection methods (also referred to as 'graph-based clustering') partition the neighbor graph. The neighbor graph treats each cell as a node, with edges connecting each node to its k nearest neighbors (based on similar distance metrics described above). The graph is then partitioned into modules based on these connectivities. These methods are typically faster than other clustering methods with similar effectiveness.**\n\n- All clustering or community detection methods have is a resolution parameter that controls how fine- or coarse-grained the inferred clusters are. This parameter can have major effects on your results.\n\n- Here, we'll explore k-means clustering and the graph-based louvain clustering method.","e286c8d1":"We see that the score does not improve.","8609103b":"## Evaluating clustering \n\n- Intuitively, **we can see from the plot that our value of k (the number of clusters) is probably too low.**\n- This dataset has **\"ground truth\" cell type labels available (This might not be with every case) We can use these to assess our cluster labels a bit more rigorously using the adjusted Rand index.**  \n- This index is a measure between (0, 1) which indicates the similarity between two sets of categorical labels (e.g., our cell type labels and cluster labels). A value of 1 means the two clusterings are identical, and 0 means the level of similarity expected by random chance.","f5083667":"### Loading the Normalized data","f9f5c9de":"**Try rerunning k-means clustering with several different values of k.**\n- **Lets try with K=7 as the total cell types is also 7**","75420e52":"- The Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.\n- The adjusted Rand index is thus ensured to have a value close to 0.0 for random labeling independently of the number of clusters and samples and exactly 1.0 when the clusterings are identical (up to a permutation).\n- **Higher the score, better the clustering**","f998061c":"You can see that adata.obsm is updated with X_tsne which is tSNE coordinates of data.","3ef76349":"- **The low rand index with the default resolution parameter is quite low (0.34), but this increases to 0.9 when we set resolution=0.1.**\n- Higher resolution means finding more and smaller clusters","fc485f0f":"# k-means - Applied on UMAP\n- In k-means clustering, the goal is to partition N cells into k different clusters. This is done in an iterative manner, cluster centers are assigned and each cell is assigned to its nearest cluster:\n- Let's try this out on the umap representation of our dataset. Scanpy doesn't include a method for k-means clustering, so we'll extract the umap coordinates that we calculated earlier and use scikit-learn for this task instead.","61ddb8df":"### Saving the Clustered adata","666198a5":"\nhttps:\/\/chanzuckerberg.github.io\/scRNA-python-workshop\/preprocessing\/01-basic-qc.html","88362d82":"# UMAP \n- UMAP (Uniform Approximation and Projection) is another nonlinear dimensionality reduction method. \n- Like tSNE, UMAP is nondeterministic and requires that we fix the random seed to ensure reproducibility. \n- While tSNE optimizes for local structure, UMAP tries to balance the preservation of local and global structure. For this reason, we prefer UMAP over tSNE for exploratory analysis and general visualization.","4000af73":"### Installations"}}