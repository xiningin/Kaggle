{"cell_type":{"73fc18a9":"code","aedddfd4":"code","d727785a":"code","191a54f7":"code","275df03a":"code","5c9ed1b1":"code","be1def5f":"code","d380dbcd":"code","cc37d537":"code","628a7592":"code","9bb42bd8":"code","765208c3":"code","46daa6b1":"code","3592c875":"code","c0f5d445":"code","b28d2843":"code","e65c711d":"code","cb3f3bb6":"code","c0c9d4bc":"code","340a6622":"code","de1ee365":"code","5258c980":"code","e86d58c0":"code","a11013a3":"code","f472bb36":"code","9a3b295a":"code","11be2c92":"code","92644765":"code","14d6a148":"code","bb92bcb1":"code","8762d4ca":"code","29086429":"code","22f67fdc":"code","fb257a77":"code","c7f96c25":"code","3f1d3dbf":"code","ff82c2d1":"code","15c10fe1":"code","7c6eb4cf":"code","06231542":"code","78c8f568":"code","e760d5ae":"code","519cb22f":"code","2dd18bb2":"code","c8cff5cf":"code","f2bd43c3":"code","e2d84f61":"code","444c451d":"code","4ee36cb5":"code","ed40973c":"code","287e00bf":"code","a968dd2e":"code","36707b58":"markdown","d5712710":"markdown","b9bc33cc":"markdown","cc8c4ef1":"markdown","365ac915":"markdown","a997bc54":"markdown","5516d9ca":"markdown","c9045f86":"markdown","5e6f8b56":"markdown","779651c2":"markdown","5a22ce0b":"markdown","43706c52":"markdown","cbcd2b4a":"markdown","1a9da276":"markdown","4a2a3328":"markdown","519d5a62":"markdown","d47b3611":"markdown","820528c2":"markdown","565b1145":"markdown","90c71f10":"markdown","0b2ed6b2":"markdown","a88e9355":"markdown"},"source":{"73fc18a9":"import numpy as np \nimport pandas as pd\n\nimport sys\nsys.path.append('..\/input\/gsdmm-short-text-clustering')\nfrom gsdmm import MovieGroupProcess\n\nfrom gensim.models.coherencemodel import CoherenceModel\nfrom gensim import corpora, models\nfrom gensim.utils import simple_preprocess\nimport gensim, spacy\nfrom gensim.models.ldamulticore import LdaMulticore\nimport re\n\n\nfrom gensim.models import Phrases\nfrom gensim.models.phrases import Phraser\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","aedddfd4":"data = pd.read_csv('\/kaggle\/input\/trip-advisor-hotel-reviews\/tripadvisor_hotel_reviews.csv', encoding='utf-8')\ndata.head()","d727785a":"data['length'] = data.Review.apply(lambda row: len(row.split()))\nprint('Mean length: ', data['length'].mean())","191a54f7":"import seaborn as sns\nsns.set_style(style=\"darkgrid\")\n\nsns.distplot(data['length'])","275df03a":"data['review_list'] = data.Review.values.tolist()\n\n# remove characters\ndata['review_list'] = [re.sub('\\s+', ' ', sent) for sent in data['review_list']]\ndata['review_list'] = [re.sub(\"\\'\", \"\", sent) for sent in data['review_list']]\n\ndef sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))","5c9ed1b1":"# create N-grams\ndef make_n_grams(texts):\n    bigram = gensim.models.Phrases(texts, min_count=5, threshold=100)  # higher threshold fewer phrases.\n    bigram_mod = gensim.models.phrases.Phraser(bigram)\n    trigram = gensim.models.Phrases(bigram[texts], threshold=100)\n    trigram_mod = gensim.models.phrases.Phraser(trigram)\n    bigrams_text = [bigram_mod[doc] for doc in texts]\n    trigrams_text =  [trigram_mod[bigram_mod[doc]] for doc in bigrams_text]\n    return trigrams_text","be1def5f":"tokens_reviews = list(sent_to_words(data['review_list']))","d380dbcd":"tokens_reviews = make_n_grams(tokens_reviews)","cc37d537":"nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n\n# I use gensim stop-words and add me own stop-words, based on texts\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in gensim.parsing.preprocessing.STOPWORDS.union(set(['also', 'meanwhile','however', 'time', \n                                                                                                                           'hour', 'soon', 'day', 'book',\n                                                                                                                           'there', 'hotel', 'room', 'leave',\n                                                                                                                           'arrive',\n                                                                                                                           'place', 'stay', 'staff', 'location',\n                                                                                                                          'service', 'come', 'check',\n                                                                                                                          'ask', 'lot', 'thing', \n                                                                                                                          'soooo', 'add', 'rarely',\n                                                                                                                          'use', 'look', 'minute',\n                                                                                                                          'bring', 'need', 'world',\n                                                                                                                          'think', 'value', 'include']))] for doc in texts]\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out","628a7592":"! python -m spacy download en_core_web_sm","9bb42bd8":"# do lemmatization keeping only noun, vb, adv\n# because adj is not informative for reviews topic modeling\nreviews_lemmatized = lemmatization(tokens_reviews, allowed_postags=['NOUN', 'VERB', 'ADV'])\n\n# remove stop words after lemmatization\nreviews_lemmatized = remove_stopwords(reviews_lemmatized)","765208c3":"np.random.seed(0)","46daa6b1":"mgp = MovieGroupProcess(K=6, alpha=0.01, beta=0.01, n_iters=30)\n\nvocab = set(x for review in reviews_lemmatized for x in review)\nn_terms = len(vocab)\nmodel = mgp.fit(reviews_lemmatized, n_terms)","3592c875":"def top_words(cluster_word_distribution, top_cluster, values):\n    for cluster in top_cluster:\n        sort_dicts =sorted(mgp.cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]\n        print(\"\\nCluster %s : %s\"%(cluster,sort_dicts))","c0f5d445":"doc_count = np.array(mgp.cluster_doc_count)\nprint('Number of documents per topic :', doc_count)\n\n# topics sorted by the number of document they are allocated to\ntop_index = doc_count.argsort()[-10:][::-1]\nprint('\\nMost important clusters (by number of docs inside):', top_index)\n# show the top 5 words in term frequency for each cluster \ntop_words(mgp.cluster_word_distribution, top_index, 10)","b28d2843":"# I don`t rename the clusters\n\ntopic_dict = {}\ntopic_names = ['type 1',\n               'type 2',\n               'type 3',\n               'type 4',\n               'type 5',\n               'type 6',\n              ]\nfor i, topic_num in enumerate(top_index):\n    topic_dict[topic_num]=topic_names[i] ","e65c711d":"def create_topics_dataframe(data_text=data.Review,  mgp=mgp, threshold=0.3, topic_dict=topic_dict, lemma_text=reviews_lemmatized):\n    result = pd.DataFrame(columns=['Text', 'Topic', 'Rating', 'Lemma-text'])\n    for i, text in enumerate(data_text):\n        result.at[i, 'Text'] = text\n        result.at[i, 'Rating'] = data.Rating[i]\n        result.at[i, 'Lemma-text'] = lemma_text[i]\n        prob = mgp.choose_best_label(reviews_lemmatized[i])\n        if prob[1] >= threshold:\n            result.at[i, 'Topic'] = topic_dict[prob[0]]\n        else:\n            result.at[i, 'Topic'] = 'Other'\n    return result","cb3f3bb6":"result = create_topics_dataframe(data_text=data.Review, mgp=mgp, threshold=0.3, topic_dict=topic_dict, lemma_text=reviews_lemmatized)\nresult.head(5)","c0c9d4bc":"import plotly.express as px\n\nfig = px.pie(result, names='Topic',  title='Topics', color_discrete_sequence=px.colors.sequential.Burg)\nfig.show()","340a6622":"import matplotlib.pyplot as plt\n\nrating_counts = result.Rating.value_counts()\ntypes_counts = result.Topic.value_counts()\nfig, ax = plt.subplots(1, 2, figsize=(15,5))\nrating = sns.barplot(x = rating_counts.index, y = rating_counts.values, palette=\"pastel\", ax=ax[0])\ntypes = sns.barplot(x = types_counts.index, y = types_counts.values, palette=\"pastel\", ax=ax[1])\n","de1ee365":"fig = px.sunburst(result, path=['Topic', 'Rating'], title='Topics and ratings', color_discrete_sequence=px.colors.sequential.Burg)\nfig.show()","5258c980":"result['len'] = result.Text.apply(lambda row: len(row.split()))\nresult.head()","e86d58c0":"types_len = result.groupby(by=['Topic']).mean()\nrating_len = result.groupby(by=['Rating']).mean()\n\nfig, ax = plt.subplots(1, 2, figsize=(15,5))\nax[0].set_title('Types')\nax[1].set_title('Ratings')\ntypes = sns.barplot(x = types_len.index, y = types_len['len'], palette=\"pastel\",  ax=ax[0])\nrating = sns.barplot(x = rating_len.index, y = rating_len['len'], palette=\"pastel\", ax=ax[1])","a11013a3":"result.Rating = pd.to_numeric(result.Rating)","f472bb36":"types_rating = result.drop('len', axis='columns').groupby(by=['Topic']).mean()\n\nfig, ax = plt.subplots(figsize=(8,5))\nax.set_title('Mean rating for each type')\ntypes = sns.barplot(x = types_rating.index, y = types_rating.Rating, palette=\"pastel\")","9a3b295a":"result['Lemma_text'] = result['Lemma-text'].apply(lambda row: ' '.join(row))","11be2c92":"result = result.drop('Lemma-text', axis=1)","92644765":"from wordcloud import WordCloud ","14d6a148":"def create_WordCloud(data, title=None):\n    wordcloud = WordCloud(width = 500, height = 500,\n                          background_color ='white',\n                          min_font_size = 15\n                         ).generate(\" \".join(data.values))\n                      \n    plt.figure(figsize = (5, 5), facecolor = None) \n    plt.imshow(wordcloud, interpolation='bilinear') \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n    plt.title(title,fontsize=20)\n    plt.show() ","bb92bcb1":"create_WordCloud(result['Lemma_text'].loc[result.Topic == 'type 1'], title=\"Most used words in topic 1\")","8762d4ca":"create_WordCloud(result['Lemma_text'].loc[result.Topic == 'type 2'], title=\"Most used words in topic 2\")","29086429":"create_WordCloud(result['Lemma_text'].loc[result.Topic == 'type 3'], title=\"Most used words in topic 3\")","22f67fdc":"create_WordCloud(result['Lemma_text'].loc[result.Topic == 'type 4'], title=\"Most used words in topic 4\")","fb257a77":"create_WordCloud(result['Lemma_text'].loc[result.Topic == 'type 5'], title=\"Most used words in topic 5\")","c7f96c25":"create_WordCloud(result['Lemma_text'].loc[result.Topic == 'type 6'], title=\"Most used words in topic 6\")","3f1d3dbf":"create_WordCloud(result['Lemma_text'].loc[result.Topic == 'Other'], title=\"Most used words in Other\")","ff82c2d1":"id2word = corpora.Dictionary(reviews_lemmatized)\ntexts = reviews_lemmatized\ncorpus = [id2word.doc2bow(text) for text in texts]","15c10fe1":"# Use TF-IDF\ntfidf = models.TfidfModel(corpus)\ncorpus_tfidf = tfidf[corpus]","7c6eb4cf":"from gensim.models.ldamulticore import LdaMulticore\n\ndef calc_coherence_values(dictionary, corpus, texts, limit = 12, start = 1, step = 1):\n    coherence_values = []\n    model_list = []\n    for num_topics in range(start, limit, step):\n        model = LdaMulticore(corpus=corpus,id2word = dictionary, num_topics = num_topics, alpha=.1, eta=0.1, random_state = 42)\n        model_list.append(model)\n        print('model created')\n        coherencemodel = CoherenceModel(model = model, texts = texts, dictionary = dictionary, coherence = 'c_v')\n        print(coherencemodel.get_coherence())\n        coherence_values.append(coherencemodel.get_coherence())\n    return model_list, coherence_values\n\nmodel_list, coherence_values = calc_coherence_values(dictionary = id2word, corpus = corpus_tfidf, texts = texts, start = 20, limit = 30, step = 2)","06231542":"import matplotlib.pyplot as plt","78c8f568":"limit, start, step = 30, 20, 2\nx = range(start, limit, step)\nplt.plot(x, coherence_values)\nplt.xlabel(\"Number of topics\")\nplt.ylabel(\"Coherence\")\nplt.legend((\"coherence_values\"), loc='best')\nplt.show()","e760d5ae":"index = coherence_values.index(max(coherence_values))\nmodel_list[index].show_topics()","519cb22f":"import pyLDAvis.gensim\n\nlda_display = pyLDAvis.gensim.prepare(model_list[index], corpus_tfidf, id2word, sort_topics = False)\npyLDAvis.display(lda_display)","2dd18bb2":"def format_topics_sentences(lda_model, corpus, data):\n    sent_topics_df = pd.DataFrame()\n    for i, row in enumerate(lda_model[corpus]):\n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = lda_model.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)","c8cff5cf":"df_topic_sents_keywords = format_topics_sentences(model_list[index], corpus_tfidf, texts)\n\n# Format\ndf_dominant_topic = df_topic_sents_keywords.reset_index()\ndf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n\n# Show\ndf_dominant_topic.head(10)","f2bd43c3":"topic_counts = df_dominant_topic.Dominant_Topic.value_counts().sort_values(ascending=True)","e2d84f61":"plt.figure(figsize=(20,5))\nax = sns.barplot(x=topic_counts.index, y=topic_counts, data=topic_counts,  palette=\"ch:.25\")\nfor p in ax.patches:\n    ax.annotate(format(p.get_height(), '.1f'), \n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\nax = ax.set_xticklabels(topic_counts.index, rotation = 45, ha=\"right\")","444c451d":"sent_topics_sorteddf_mallet = pd.DataFrame()\nsent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n\nfor i, grp in sent_topics_outdf_grpd:\n    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], axis=0)\n\nsent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\nsent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\nsent_topics_sorteddf_mallet.head(24)","4ee36cb5":"from gensim.models import LsiModel","ed40973c":"def calc_coherence_values_Lsi(dictionary, corpus, texts, limit, start = 2, step = 2):\n    coherence_values = []\n    model_list = []\n    for num_topics in range(start, limit, step):\n        model = LsiModel(corpus=corpus, id2word = dictionary, num_topics = num_topics)\n        print('model created')\n        model_list.append(model)\n        coherencemodel = CoherenceModel(model = model, texts = texts, dictionary = dictionary, coherence = 'c_v')\n        print(coherencemodel.get_coherence())\n        coherence_values.append(coherencemodel.get_coherence())\n    return model_list, coherence_values\n\n\nmodel_list, coherence_values_Lsi = calc_coherence_values_Lsi(dictionary = id2word, corpus=corpus_tfidf, texts=texts, start = 2, limit = 30, step =2)","287e00bf":"limit, start, step = 30, 2, 2\nx = range(start, limit, step)\nplt.plot(x, coherence_values_Lsi)\nplt.xlabel(\"Number of topics\")\nplt.ylabel(\"Coherence\")\nplt.legend((\"coherence_values\"), loc='best')\nplt.show()","a968dd2e":"index = coherence_values_Lsi.index(max(coherence_values_Lsi))\nmodel_list[index].show_topics()","36707b58":"# Approaches for topic modeling \n\n\n<br>\n<br>\n\n##  What is topic modeling? \n   \n<br>\n<br>\n\n Topic Modeling is a technique to extract the hidden topics from large volumes of text.\n\n    \nAt the moment, the most popular algorithm is LDA (Latent Dirichlet Allocation), but there are a lot of other approaches for different purposes.\n\nFor example, topic modeling for short text is a very difficult task with special algorithms.\n\nAlso, topic modeling is not a simple process, because we need clear text data at the start.\n\nWell, in this notebook three algorithms will be introduced:\n<br>\n<br>\n\n- Topic modeling for short texts (GSDMM);\n- Classical approach: LDA-model;\n- LSI.\n\n<p style=\"color:red\"> If you find this notebook useful, upvote it, please <\/p>","d5712710":"# **What is the GSDMM?**\n\nShort text clustering is a challenging problem due to its sparse, high-dimensional, and large-volume characteristics. The GSDMM has the following nice properties:\n\n- GSDMM can infer the number of clusters automatically;\n- GSDMM has a clear way to balance the completeness and homogeneity of the clustering results; \n- GSDMM is fast to converge; \n- Unlike the Vector Space Model (VSM)-based approaches, GSDMM can cope with the sparse and highdimensional problem of short texts; \n- Like Topic Models (e.g., PLSA and LDA), GSDMM can also obtain the representative words of each cluster.","b9bc33cc":"# Explore the data:","cc8c4ef1":"<p style=\"color:red; font-size:150%\"> At the next: more visualizations for LDA<\/p>","365ac915":"Let`s explore the mean of length for each type of topics\/ratings:","a997bc54":"For the topic model very important feature is the length of the text, for example, LDA-model maybe gives a bad quality for short text and we need to try another approach.\nSee the mean text length of reviews:","5516d9ca":"# GSDMM for the topic modeling: ","c9045f86":"Hyper-parameters of the GSDMM model:\n\n- K = 6. It is the number of clusters. I set this value after several experiments in which I started from 20 clusters, and as a result, I had 6 filled clusters and 14 empty clusters.\n\n- alpha =0.01 1 and beta = 0.01. Hyper-parametrs. I chosen that values after experements.\n\n- n_iters = 30.  Number of iteration.","5e6f8b56":"# Preprocessing the data:\n\nThe quality of our data is important for the quality of our model. I like to do preprocess with Gensim and Spacy library.\n\nMy preprocessing steps are:\n- removing symbols with regular expressions;\n- tokenization and delete punctuation;\n- create N-grams: bigrams and trigrams;\n- lemmatization;\n- removing stop-words.","779651c2":"<p style=\"color:red; font-size:150%\"> At the next: more visualizations for LSI<\/p>","5a22ce0b":"# LDA-model","43706c52":"# Visualization for GSDM model:","cbcd2b4a":"Means of LDA coherence score:\n\n- 0.3 is bad\n- 0.4 is low\n- 0.55 is okay\n- 0.65 might be as good as it is going to get\n- 0.7 is nice\n- 0.8 is unlikely\n- 0.9 is probably wrong","1a9da276":"It is medium text length, but the classical approach, LDA-model, poorly performs on short texts like Tweets, Reddit posts and etc. I suggest another approach, which created especially for short text, - GSDMM (Gibbs sampling algorithm for a Dirichlet Mixture Model).\n\nBut we also check LDA-model results and compare with GSDMM result.\n","4a2a3328":"**LSI (Latent Semantic Indexing)** \n\nis a dimensionality reduction technique that projects documents to a lower-dimensional semantic space and, in doing so, causes documents with similar topical content to be close to one another in the resulting space. LSI uses the singular value decomposition (SVD) of the large term-by-document matrix which represents a document collection in an IR system. The latent space is generated automatically based on word co-occurrence in the collection of documents, so the amount of semantic relatedness between documents in the latent space will depend on other documents in the collection. For example, in the document collection consisting of the portion of the worldwide web indexed by Google, the subject matter is diverse, and two documents that both contain words about computer programming languages will be close in the latent space. However, in the collection of documents on the intranet of a software development company, there will be many documents with programming terminology and two documents will only be close if they share many terms.","519d5a62":"# LSI model","d47b3611":"Create LDA-model:","820528c2":"Let`s take a closer look at the last point (about comparing it with the most popular topic models):\n\n- In LDA, documents are considered to be a mixture of topics. Short text only deals with one topic (most of the time) so this assumption is not reliable anymore;\n- Shorter text means fewer data to rely on during the LDA inference steps.","565b1145":"Well, the Gibbs Sampling Dirichlet Mixture Model (GSDMM) is an \u201cextended\u201d LDA algorithm, that makes the initial assumption: **1 topic is 1 document**. \n\nThe words within a document are generated using the same unique topic, and not from a mixture of topics as it was in the original LDA.\n\nGSDMM is a good choice for short text topic modeling.","90c71f10":"Top texts in each topic:","0b2ed6b2":"The best number of topics is 24.","a88e9355":"## WordClouds:"}}