{"cell_type":{"365f48cd":"code","ec497bd8":"code","e8418f63":"code","6e216a65":"code","62e18ba5":"code","3b06f895":"code","f69fd597":"code","729148ec":"code","7224c96d":"code","6f9c297a":"code","78735ac9":"code","53d88c5c":"code","07459927":"code","a50c04d9":"code","f9ab2c97":"code","fa4a34a6":"code","cdad0c0a":"code","e1c11bef":"code","fa22f614":"code","c8c1b056":"markdown","8a41c23f":"markdown","e2bb3fce":"markdown","f7d712d2":"markdown","222e9640":"markdown","63f70f43":"markdown","5e2eef70":"markdown","511a5e00":"markdown","f295f282":"markdown","a8261f87":"markdown","a3499b90":"markdown","e7f4643e":"markdown","4f40dd82":"markdown","b471fa22":"markdown"},"source":{"365f48cd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ec497bd8":"!pip install pycaret","e8418f63":"#from pycaret.datasets import get_data\ndf=pd.read_csv(r'\/kaggle\/input\/heart-disease-uci\/heart.csv')\ndf.head()","6e216a65":"from pycaret.classification import *\nclf=setup(df,target='target')","62e18ba5":"compare_models()","3b06f895":"l=create_model('lr')","f69fd597":"dt=create_model('dt')","729148ec":"##tunned_lr=tune_model('lr')\n## The latest version of pycaret has made some changes in the tune model function \n## So let's explore more before performing this step","7224c96d":"boosted_dt=ensemble_model(dt,method='Boosting')","6f9c297a":"lr=create_model('lr')\nlda=create_model('lda')\ngbc=create_model('gbc')","78735ac9":"blender=blend_models(estimator_list=[lr,lda,gbc],method='soft')","53d88c5c":"blender.estimators_","07459927":"plot_model(blender)","a50c04d9":"plot_model(blender,plot='confusion_matrix')","f9ab2c97":"plot_model(blender,plot='threshold')","fa4a34a6":"plot_model(blender,plot='boundary')","cdad0c0a":"interpret_model(dt)","e1c11bef":"interpret_model(dt,plot='correlation')","fa22f614":"##deploy_model(dt,model_name='dt-for-aws',platform='aws',authentication={'bucket':'pycaret-test'})","c8c1b056":"# 7)ensemble model","8a41c23f":"# 4)comparing the models","e2bb3fce":"### After all the above steps we can deploy our model using aws s3 bucket instace","f7d712d2":"# 3) setting up the environment","222e9640":"# 5)creating the model","63f70f43":"# 6)tune the model","5e2eef70":"# 10)interpret the model","511a5e00":"# 2)importing dataset","f295f282":"### we can observe the detailed description and info of the data","a8261f87":"# 1) installing PyCaret","a3499b90":"# 11) deploy model","e7f4643e":"### As we observe it is compared with 14 different classification models and out of that logistic regression is the best, we will create logistic regression model","4f40dd82":"# 8) blending models","b471fa22":"# 9)Analize model"}}