{"cell_type":{"83c7e610":"code","71b473bf":"code","fd873de0":"code","5431932d":"code","f2563b90":"code","a807b289":"code","199e2dc5":"code","d4b7c3e8":"code","7074c601":"code","4687bf6d":"code","1fd80335":"markdown","bde176bb":"markdown","7c1e4965":"markdown","857629da":"markdown","4fa8f050":"markdown","f837beca":"markdown","45788a34":"markdown","1ec24b2d":"markdown","ef669af2":"markdown","31e8ebc8":"markdown","bef59729":"markdown","b4dcf49f":"markdown","c58c2c94":"markdown"},"source":{"83c7e610":"import librosa\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom matplotlib import pyplot as plt\nimport math\n\n%matplotlib inline","71b473bf":"slice_len   = 2\nslice_count = 1\nsr          = 44100\nn_mels      = 256\nfmin        = 20\nhop_length  = int(sr\/(n_mels\/slice_len)) # ensures square mel-spectrogram slice\nfmax        = sr\/\/2\n\ny = librosa.effects.trim(librosa.load('..\/input\/train_noisy\/42f7abb4.wav' , sr)[0])[0]\n\ns = librosa.feature.melspectrogram(y, \n                                   sr         = sr,\n                                   n_mels     = n_mels,\n                                   hop_length = hop_length,\n                                   n_fft      = n_mels*20,\n                                   fmin       = fmin,\n                                   fmax       = fmax)","fd873de0":"gain          = 0.6\nbias          = 0.1 \npower         = 0.2 \ntime_constant = 0.4 \neps           = 1e-9\n\ntime_constant = 0.4\n\npower_to_db = librosa.power_to_db(s)\n\npcen_librosa = librosa.core.pcen(s, \n                                 sr            = sr,\n                                 hop_length    = hop_length,\n                                 gain          = gain,\n                                 bias          = bias,\n                                 power         = power,\n                                 time_constant = time_constant,\n                                 eps           = eps)\n\n","5431932d":"fig = plt.figure(figsize=(20,3))\nfig.suptitle(\"Power to Db\")\nplt.imshow(power_to_db)\n\n\nfig = plt.figure(figsize=(20,3))\nfig.suptitle(\"PCEN\")\nplt.imshow(pcen_librosa)","f2563b90":"def pcen(x, eps=1E-6, s=0.025, alpha=0.98, delta=2, r=0.5, training=False):\n    frames = x.split(1, -2)\n    m_frames = []\n    last_state = None\n    for frame in frames:\n        if last_state is None:\n            last_state = s * frame\n            m_frames.append(last_state)\n            continue\n        if training:\n            m_frame = ((1 - s) * last_state).add_(s * frame)\n        else:\n            m_frame = (1 - s) * last_state + s * frame\n        last_state = m_frame\n        m_frames.append(m_frame)\n    M = torch.cat(m_frames, 1)\n    if training:\n        pcen_ = (x \/ (M + eps).pow(alpha) + delta).pow(r) - delta ** r\n    else:\n        pcen_ = x.div_(M.add_(eps).pow_(alpha)).add_(delta).pow_(r).sub_(delta ** r)\n    return pcen_\n\n\nclass PCENTransform(nn.Module):\n\n    def __init__(self, eps=1E-6, s=0.025, alpha=0.98, delta=2, r=0.5, trainable=True):\n        super().__init__()\n        if trainable:\n            self.log_s = nn.Parameter(torch.log(torch.Tensor([s])))\n            self.log_alpha = nn.Parameter(torch.log(torch.Tensor([alpha])))\n            self.log_delta = nn.Parameter(torch.log(torch.Tensor([delta])))\n            self.log_r = nn.Parameter(torch.log(torch.Tensor([r])))\n        else:\n            self.s = s\n            self.alpha = alpha\n            self.delta = delta\n            self.r = r\n        self.eps = eps\n        self.trainable = trainable\n\n    def forward(self, x):\n        x = x.permute((0,1,3,2)).squeeze(dim=1)\n        if self.trainable:\n            x = pcen(x, self.eps, torch.exp(self.log_s), torch.exp(self.log_alpha), torch.exp(self.log_delta), torch.exp(self.log_r), self.training and self.trainable)\n        else:\n            x = pcen(x, self.eps, self.s, self.alpha, self.delta, self.r, self.training and self.trainable)\n        x = x.unsqueeze(dim=1).permute((0,1,3,2))\n        return x","a807b289":"t = torch.tensor(s)\n\nT = time_constant * sr \/ hop_length\n\nb = (math.sqrt(1 + 4* T**2) - 1) \/ (2 * T**2)    # as per librosa documentation\n\npcen_torch = pcen(t[None,...].permute((0,2,1)),  # change the shape of the mels appropriately for the PyTorch pcen function\n                  eps      = eps, \n                  s        = b, \n                  alpha    = gain,\n                  delta    = bias, \n                  r        = power, \n                  training = True)\n\npcen_torch = pcen_torch.permute((0,2,1)).squeeze().numpy()     # change the shape back and convert to numpy array from tensor","199e2dc5":"np.allclose(pcen_librosa, pcen_torch)","d4b7c3e8":"width  = 256\nstart  = np.random.randint(0,s.shape[1]-width) \nend    = start + width\n\ncrop_then_slice = librosa.core.pcen(s[:,start:end],\n                                    sr            = sr,\n                                    hop_length    = hop_length,\n                                    gain          = gain,\n                                    bias          = bias,\n                                    power         = power,\n                                    time_constant = time_constant,\n                                    eps           = eps)\n\nslice_then_crop = pcen_librosa[:,start:end]","7074c601":"np.allclose(crop_then_slice, slice_then_crop)","4687bf6d":"fig, ax = plt.subplots(1,3,figsize=[20,9])\nax[0].set_title(\"Crop then slice\")\nax[0].imshow(crop_then_slice)\n\nax[1].set_title(\"Slice then crop\")\nax[1].imshow(slice_then_crop)\n\nax[2].set_title(\"Difference\")\nax[2].imshow(crop_then_slice-slice_then_crop)","1fd80335":"### Compute PCEN from mels","bde176bb":"I read this discussion thread about PCEN as an alternative to mel-scale filters - [Anyone tried PCEN in front end?](https:\/\/www.kaggle.com\/c\/freesound-audio-tagging-2019\/discussion\/91859) and thought I'd give it a go.\n","7c1e4965":"### Problem: Slice then PCEN \u2260 PCEN then Slice\n\nFor this competition to ensure constant input size for my networks I was using the approach of randomly slicing a section of the spectrogram and zero-padding if necessary.\n\nI realised that the order of slicing and PCEN matters:","857629da":"I used the ```PCENTransform``` module above as the first layer in my CNN. \n\nSomething like:\n\n\n```\nclass MyNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.pcen = PCENTransform(eps=1E-6, s=0.025, alpha=0.6, delta=0.1, r=0.2, trainable=True)\n   \n    ...\n    \n    def forward(self, x):\n        p = self.pcen(x)\n\n    ...\n\n```","4fa8f050":"When the PCEN is computed on the entire spectrogram information is carried through from the start of the spectrogram, whereas when the slicing is done first this information is not available.","f837beca":"### Load an example wav file and compute the melspectrogram features","45788a34":"### Compare plots of the Power to Db and the PCEN mel-spectrogram ","1ec24b2d":"### Check that the PyTorch and Librosa PCEN are the same","ef669af2":"Using a PCEN layer in my network with the above slicing approach posed a bit of a challenge, the PCEN will be computed on the sliced spectrogram not on the whole spectrogram and this seemed to make a difference to my training results.\n\nI found that I could not get the trainable PCEN to perform as well as the static parameters I chose with librosa.\n\nI tried setting my PCEN layer to not be trainable and gave it the same static parameters, but the performance was still worse.\n\nThe only thing I can think to attribute this to is the problem I highlighted above.","31e8ebc8":"\nAs suggested in the thread I tried out a few different sets of parameters using the [librosa implementation](https:\/\/librosa.github.io\/librosa\/generated\/librosa.core.pcen.html) mentioned in the thread. I found some parameters that seemed to give me a bit of a boost over the '''power_to_db''' function that I had been using. Those parameters were:\n```\ngain          = 0.6\nbias          = 0.1 \npower         = 0.2 \ntime_constant = 0.4 \neps           = 1e-9\n```","bef59729":"### Learn the PCEN parameters\n\nThe idea of incorporating the tuning of these parameters was appealing so I adapted and simplified code for a PyTorch PCEN layer that I found [here](https:\/\/github.com\/daemon\/pytorch-pcen).\n\nThe code there uses different names for the PCEN parameters and it took me a little while to figure exactly how to change the parameter values to be equivalent to the librosa implementation.\n\nA change I found necessary to make was to ensure that the parameters remained positive during training. Initially using the code from github, my gradients would become undefined because the PCEN calculation went wrong if the paramters became negative. \n\nI followed the approach suggested in the [original paper](https:\/\/static.googleusercontent.com\/media\/research.google.com\/en\/\/pubs\/archive\/8a75d472dc7286653a5245a80a7603a1db308af0.pdf) that proposed PCEN, and learnt the logarithms of the parameters and then exponentiated them in the PCEN calculation.","b4dcf49f":"### Trainable PCEN CNN performance","c58c2c94":"### Test PyTorch PCEN definition\n\nThe librosa pcen documentation states that:\n\n> If b is not provided, it is calculated as:\n> \n> b = (sqrt(1 + 4* T**2) - 1) \/ (2 * T**2)\n> where T = time_constant * sr \/ hop_length."}}