{"cell_type":{"e448f3bd":"code","a748ca0d":"code","0420bbf0":"code","1eed5a37":"code","fa05c573":"code","404e9513":"code","80f4d37c":"code","1287ebb1":"code","9410091c":"code","63c4c796":"code","42e4cf4a":"code","104e4c26":"code","5657cf7c":"code","c82a18d1":"code","770d3faf":"code","07386eef":"code","d139c5db":"markdown","9d7b0af6":"markdown","957ca47a":"markdown"},"source":{"e448f3bd":"import pickle\nimport pandas as pd\nimport numpy as np\nimport os, sys, gc \nfrom plotnine import *\nimport plotnine\n\nfrom tqdm import tqdm_notebook\nimport seaborn as sns\nimport warnings\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\nimport matplotlib as mpl\nfrom matplotlib import rc\nimport re\nfrom matplotlib.ticker import PercentFormatter\nimport datetime\nfrom math import log # IDF \uacc4\uc0b0\uc744 \uc704\ud574","a748ca0d":"path = \"..\/input\/t-academy-recommendation\/\"","0420bbf0":"# pd.read_json : json \ud615\ud0dc\uc758 \ud30c\uc77c\uc744 dataframe \ud615\ud0dc\ub85c \ubd88\ub7ec\uc624\ub294 \ucf54\ub4dc \nmagazine = pd.read_json(path + 'magazine.json', lines=True) # lines = True : Read the file as a json object per line.\nmetadata = pd.read_json(path + 'metadata.json', lines=True)\nusers = pd.read_json(path + 'users.json', lines=True)","1eed5a37":"%%time \nimport itertools\nfrom itertools import chain\nimport glob\nimport os \n\ndef chainer(s):\n    return list(itertools.chain.from_iterable(s))\n\nread_rowwise = pd.read_csv(path + \"read_rowwise.csv\")","fa05c573":"from datetime import datetime \n\nmetadata['reg_datetime'] = metadata['reg_ts'].apply(lambda x : datetime.fromtimestamp(x\/1000.0))\nmetadata.loc[metadata['reg_datetime'] == metadata['reg_datetime'].min(), 'reg_datetime'] = datetime(2090, 12, 31)\nmetadata['reg_dt'] = metadata['reg_datetime'].dt.date\nmetadata['type'] = metadata['magazine_id'].apply(lambda x : '\uac1c\uc778' if x == 0.0 else '\ub9e4\uac70\uc9c4')\nmetadata['reg_dt'] = pd.to_datetime(metadata['reg_dt'])","404e9513":"read_rowwise = read_rowwise.merge(metadata[['id', 'reg_dt']], how='left', left_on='article_id', right_on='id')\nread_rowwise = read_rowwise[read_rowwise['article_id'] != '']\n\n# \uc0ac\uc6a9\uc790\uac00 \uc77d\uc740 \uae00\uc758 \ubaa9\ub85d\ub4e4\uc744 \uc800\uc7a5 \nread_total = pd.DataFrame(read_rowwise.groupby(['user_id'])['article_id'].unique()).reset_index()\nread_total.columns = ['user_id', 'article_list']","80f4d37c":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nmetadata = metadata[metadata['keyword_list'].notnull()].reset_index()\nmetadata = metadata[metadata['reg_dt'] >= '2019-01-01']","1287ebb1":"user_total = pd.DataFrame(read_rowwise.groupby(['article_id'])['user_id'].unique()).reset_index()\nuser_total.columns = ['article_id', 'user_list']\n\nuser_total['user_len'] = user_total['user_list'].apply(lambda x: len(x))\ncold_article = user_total[user_total['user_len'] <= 20]['article_id'].unique()\nmetadata = metadata[~metadata['id'].isin(cold_article)]","9410091c":"article2idx = {}\nfor i, l in enumerate(metadata['id'].unique()):\n    article2idx[l] = i\n    \nidx2article = {i: item for item, i in article2idx.items()}\narticleidx = metadata['articleidx'] = metadata['id'].apply(lambda x: article2idx[x]).values","63c4c796":"import scipy\n\ndocs = metadata['keyword_list'].apply(lambda x: ' '.join(x)).values\ntfidv = TfidfVectorizer(use_idf=True, smooth_idf=False, norm=None).fit(docs)\ntfidv_df = scipy.sparse.csr_matrix(tfidv.transform(docs))\ntfidv_df = tfidv_df.astype(np.float32)","42e4cf4a":"print(tfidv_df.shape)","104e4c26":"popular_rec_model = read_rowwise['article_id'].value_counts().index[0:100]","5657cf7c":"del read_rowwise\ngc.collect()","c82a18d1":"from sklearn.metrics.pairwise import cosine_similarity\n\n# \uba54\ubaa8\ub9ac \ubb38\uc81c \ubc1c\uc0dd \ncos_sim = cosine_similarity(tfidv_df, tfidv_df)","770d3faf":"valid = pd.read_csv(path + '\/predict\/dev.users', header=None)","07386eef":"%%time \ntop_n = 100\nwith open('.\/recommend.txt', 'w') as f:\n    for user in tqdm_notebook(valid[0].values):\n        seen = chainer(read_total[read_total['user_id'] == user]['article_list'])\n        for seen_id in seen:\n            # 2019\ub144\ub3c4 \uc774\uc804\uc5d0 \uc77d\uc5b4\uc11c \ud639\uc740 \uba54\ud0c0\ub370\uc774\ud130\uc5d0 \uae00\uc774 \uc5c6\uc5b4\uc11c \uc720\uc0ac\ub3c4 \uacc4\uc0b0\uc774 \uc548\ub41c \uae00\n            cos_sim_sum = np.zeros(len(cos_sim))\n            try:\n                cos_sim_sum += cos_sim[article2idx[seen_id]]\n            except:\n                pass\n\n        recs = []\n        for rec in cos_sim_sum.argsort()[-(top_n+100):][::-1]:\n            if (idx2article[rec] not in seen) & (len(recs) < 100):\n                recs.append(idx2article[rec])\n\n        f.write('%s %s\\n' % (user, ' '.join(recs[0:100])))","d139c5db":"![](https:\/\/github.com\/choco9966\/T-academy-Recommendation\/blob\/master\/figure\/Contents_Based_Score.PNG?raw=true)","9d7b0af6":"\ub370\uc774\ud130\uac00 Sparse \ud615\ud0dc\uc778 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc74c","957ca47a":"## \ucf58\ud150\uce20 \uae30\ubc18\uc758 \ucd94\ucc9c\uc2dc\uc2a4\ud15c\n- Model\uc758 \ub2e8\uc5b4\ub97c \uc774\uc6a9\ud55c \ubc29\uc2dd\n- TF-IDF \ud615\uc2dd\n    - index : \ubb38\uc11c\uc758 \uc544\uc774\ub514 \n    - column : \ub2e8\uc5b4 \n\n\ud558\uc9c0\ub9cc, \ubb38\uc11c\uac00 \ucd1d 64\ub9cc\uac1c\ub85c \ub108\ubb34 \ub9ce\uace0 data.0\uc758 \ud30c\uc77c\uc744 \uc77d\uc5b4\ubcf4\uba74 \ub2e8\uc5b4 \ub610\ud55c \ub108\ubb34 \ub9ce\uc544\uc11c \uc0ac\uc6a9\ud558\uae30\uac00 \uc5b4\ub824\uc6b4 \uc0c1\ud669\n\n### \ud574\uacb0\ubc29\uc2dd\n\uc704\uc640 \uac19\uc740 \ubb38\uc81c\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud574\uc11c \ud574\ub2f9 \ub300\ud68c\uc758 1\ub4f1\ud300\uc778 NAFMA\ud300\uc740 \uae00\uc758 \ud0a4\uc6cc\ub4dc\ub97c \ud65c\uc6a9\ud574\uc11c Embedding\uc744 \uad6c\uc131 \n- \ucc38\uace0\uc790\ub8cc : https:\/\/github.com\/JungoKim\/brunch_nafma"}}