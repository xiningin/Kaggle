{"cell_type":{"a6783e4a":"code","4d469eea":"code","8634975a":"code","d5b1edfe":"code","4746e9a2":"code","26ea377a":"code","a674ece4":"code","bff629cb":"code","2f8e6f40":"code","7617890a":"code","13ee0d35":"code","a4863880":"code","6bbb43c4":"code","6e7cd394":"code","377030b6":"code","e2bc4c92":"code","bd6618a3":"code","4fd282a1":"code","14a7ec0f":"code","4087ed98":"code","55d0a57b":"code","aa910881":"code","2eb9253b":"code","b97b51b0":"code","79a1aa81":"code","8b5f363a":"code","c76683bf":"code","65efffc6":"code","56e771d6":"code","ead04565":"code","d0424dd7":"code","31787a5c":"code","eaf9fd24":"code","2267e212":"code","654713e1":"code","0cd44bf6":"code","2dffa181":"code","62070c87":"code","7b1787cb":"code","f6f98870":"code","2345ddf7":"code","07ac09e9":"code","19072fc0":"code","257461a8":"code","d870cb33":"code","9d4dd793":"code","d4f9ec9e":"code","5a1a1786":"code","db636d69":"code","da5082d4":"code","6ab5ac20":"code","a7501980":"code","85f19a25":"code","63f8b76c":"code","18311fc9":"code","32be4a2d":"code","583d35b2":"code","4bcb5656":"code","728c8bec":"code","b31c527a":"code","9850c66f":"code","0ab45efd":"markdown","0578473f":"markdown","8d03007f":"markdown","50607b2d":"markdown","87ff86f3":"markdown","f62f9f62":"markdown","5f2a8414":"markdown","85e84487":"markdown","8b4e02f7":"markdown","15bcd218":"markdown","e48e7935":"markdown","b19696d9":"markdown","090931d5":"markdown","c03d2190":"markdown","10162d3c":"markdown","b7dd4009":"markdown","0b7e7655":"markdown","d64aca51":"markdown","7e918a3b":"markdown","95c63d45":"markdown","3673f9af":"markdown","de91a61f":"markdown","b90b1fe3":"markdown","199c919b":"markdown","a5f34641":"markdown","e78fcffc":"markdown","21cb09dd":"markdown","2f0f4ad5":"markdown","98061f17":"markdown","3b9add81":"markdown","de528b77":"markdown","7d51cb5f":"markdown","a5aad677":"markdown","ad9bf8b9":"markdown","18cbb736":"markdown","5a859013":"markdown"},"source":{"a6783e4a":"import numpy as np # np is short for numpy\nimport pandas as pd # pandas is so commonly used, it's shortened to pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns # seaborn gets shortened to sns\n\n# We want our plots to appear in the notebook\n%matplotlib inline \n\n## Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n## Model evaluators\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve","4d469eea":"df = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\") # 'DataFrame' shortened to 'df'\ndf.shape # (rows, columns)","8634975a":"df.head()","d5b1edfe":"df.target.value_counts()","4746e9a2":"# Normalized value counts\ndf.target.value_counts(normalize=True)","26ea377a":"# Plot the value counts with a bar graph\ndf.target.value_counts().plot(kind=\"bar\", color=[\"salmon\", \"lightblue\"]);","a674ece4":"df.info()","bff629cb":"df.describe()","2f8e6f40":"df.sex.value_counts()","7617890a":"# Compare target column with sex column\npd.crosstab(df.target, df.sex)","13ee0d35":"# Create a plot\npd.crosstab(df.target, df.sex).plot(kind=\"bar\", \n                                    figsize=(10,6), \n                                    color=[\"salmon\", \"lightblue\"]);\n\n# Add some attributes to it\nplt.title(\"Heart Disease Frequency for Sex\")\nplt.xlabel(\"0 = No Disease, 1 = Disease\")\nplt.ylabel(\"Amount\")\nplt.legend([\"Female\", \"Male\"])\nplt.xticks(rotation=0); # keep the labels on the x-axis vertical","a4863880":"# Create another figure\nplt.figure(figsize=(10,6))\n\n# Start with positve examples\nplt.scatter(df.age[df.target==1], \n            df.thalach[df.target==1], \n            c=\"salmon\") # define it as a scatter figure\n\n# Now for negative examples, we want them on the same plot, so we call plt again\nplt.scatter(df.age[df.target==0], \n            df.thalach[df.target==0], \n            c=\"lightblue\") # axis always come as (x, y)\n\n# Add some helpful info\nplt.title(\"Heart Disease in function of Age and Max Heart Rate\")\nplt.xlabel(\"Age\")\nplt.legend([\"Disease\", \"No Disease\"])\nplt.ylabel(\"Max Heart Rate\");","6bbb43c4":"# Histograms to check the distribution of a variable\ndf.age.plot.hist();","6e7cd394":"pd.crosstab(df.cp, df.target)","377030b6":"# Create a new crosstab and base plot\npd.crosstab(df.cp, df.target).plot(kind=\"bar\", \n                                   figsize=(10,6), \n                                   color=[\"lightblue\", \"salmon\"])\n\n# Add attributes to the plot to make it more readable\nplt.title(\"Heart Disease Frequency Per Chest Pain Type\")\nplt.xlabel(\"Chest Pain Type\")\nplt.ylabel(\"Frequency\")\nplt.legend([\"No Disease\", \"Disease\"])\nplt.xticks(rotation = 0);","e2bc4c92":"corr_matrix = df.corr()\nplt.figure(figsize=(15, 10))\nsns.heatmap(corr_matrix, \n            annot=True, \n            linewidths=0.5, \n            fmt= \".2f\", \n            cmap=\"YlGnBu\");","bd6618a3":"#split the target variable from the rest\n\n# Everything except target variable\nX = df.drop(\"target\", axis=1)\n\n# Target variable\ny = df.target.values","4fd282a1":"# Independent variables (no target column)\nX.head()","14a7ec0f":"# Targets\ny","4087ed98":"# Random seed for reproducibility\nnp.random.seed(42)\n\n# Split into train & test set\nX_train, X_test, y_train, y_test = train_test_split(X, # independent variables \n                                                    y, # dependent variable\n                                                    test_size = 0.2) # percentage of data to use for test set","55d0a57b":"X_train.head()","aa910881":"y_train, len(y_train)","2eb9253b":"X_test.head()\n","b97b51b0":"y_test, len(y_test)","79a1aa81":"# Put models in a dictionary\nmodels = {\"KNN\": KNeighborsClassifier(),\n          \"Logistic Regression\": LogisticRegression(), \n          \"Random Forest\": RandomForestClassifier()}\n\n# Create function to fit and score models\ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n    \"\"\"\n    Fits and evaluates given machine learning models.\n    models : a dict of different Scikit-Learn machine learning models\n    X_train : training data\n    X_test : testing data\n    y_train : labels assosciated with training data\n    y_test : labels assosciated with test data\n    \"\"\"\n    # Random seed for reproducible results\n    np.random.seed(42)\n    # Make a list to keep model scores\n    model_scores = {}\n    # Loop through models\n    for name, model in models.items():\n        # Fit the model to the data\n        model.fit(X_train, y_train)\n        # Evaluate the model and append its score to model_scores\n        model_scores[name] = model.score(X_test, y_test)\n    return model_scores","8b5f363a":"model_scores = fit_and_score(models=models,\n                             X_train=X_train,\n                             X_test=X_test,\n                             y_train=y_train,\n                             y_test=y_test)\nmodel_scores","c76683bf":"model_compare = pd.DataFrame(model_scores, index=['accuracy'])\nmodel_compare.T.plot.bar();","65efffc6":"# Create a list of train scores\ntrain_scores = []\n\n# Create a list of test scores\ntest_scores = []\n\n# Create a list of different values for n_neighbors\nneighbors = range(1, 21) # 1 to 20\n\n# Setup algorithm\nknn = KNeighborsClassifier()\n\n# Loop through different neighbors values\nfor i in neighbors:\n    knn.set_params(n_neighbors = i) # set neighbors value\n    \n    # Fit the algorithm\n    knn.fit(X_train, y_train)\n    \n    # Update the training scores\n    train_scores.append(knn.score(X_train, y_train))\n    \n    # Update the test scores\n    test_scores.append(knn.score(X_test, y_test))","56e771d6":"train_scores","ead04565":"plt.plot(neighbors, train_scores, label=\"Train score\")\nplt.plot(neighbors, test_scores, label=\"Test score\")\nplt.xticks(np.arange(1, 21, 1))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model score\")\nplt.legend()\n\nprint(f\"Maximum KNN score on the test data: {max(test_scores)*100:.2f}%\")","d0424dd7":"# Different LogisticRegression hyperparameters\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n                \"solver\": [\"liblinear\"]}\n\n# Different RandomForestClassifier hyperparameters\nrf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n           \"max_depth\": [None, 3, 5, 10],\n           \"min_samples_split\": np.arange(2, 20, 2),\n           \"min_samples_leaf\": np.arange(1, 20, 2)}","31787a5c":"# Setup random seed\nnp.random.seed(42)\n\n# Setup random hyperparameter search for LogisticRegression\nrs_log_reg = RandomizedSearchCV(LogisticRegression(),\n                                param_distributions=log_reg_grid,\n                                cv=5,\n                                n_iter=20,\n                                verbose=True)\n\n# Fit random hyperparameter search model\nrs_log_reg.fit(X_train, y_train);","eaf9fd24":"rs_log_reg.best_params_","2267e212":"rs_log_reg.score(X_test, y_test)","654713e1":"# Setup random seed\nnp.random.seed(42)\n\n# Setup random hyperparameter search for RandomForestClassifier\nrs_rf = RandomizedSearchCV(RandomForestClassifier(),\n                           param_distributions=rf_grid,\n                           cv=5,\n                           n_iter=20,\n                           verbose=True)\n\n# Fit random hyperparameter search model\nrs_rf.fit(X_train, y_train);","0cd44bf6":"# Find the best parameters\nrs_rf.best_params_","2dffa181":"# Evaluate the randomized search random forest model\nrs_rf.score(X_test, y_test)","62070c87":"# Different LogisticRegression hyperparameters\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n                \"solver\": [\"liblinear\"]}\n\n# Setup grid hyperparameter search for LogisticRegression\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                          param_grid=log_reg_grid,\n                          cv=5,\n                          verbose=True)\n\n# Fit grid hyperparameter search model\ngs_log_reg.fit(X_train, y_train);","7b1787cb":"# Check the best parameters\ngs_log_reg.best_params_","f6f98870":"# Evaluate the model\ngs_log_reg.score(X_test, y_test)","2345ddf7":"# Make preidctions on test data\ny_preds = gs_log_reg.predict(X_test)","07ac09e9":"y_preds","19072fc0":"y_test","257461a8":"# Import ROC curve function from metrics module\nfrom sklearn.metrics import plot_roc_curve\n\n# Plot ROC curve and calculate AUC metric\nplot_roc_curve(gs_log_reg, X_test, y_test);","d870cb33":"# Display confusion matrix\nprint(confusion_matrix(y_test, y_preds))","9d4dd793":"\n# Import Seaborn\nimport seaborn as sns\nsns.set(font_scale=1.5) # Increase font size\n\ndef plot_conf_mat(y_test, y_preds):\n    \"\"\"\n    Plots a confusion matrix using Seaborn's heatmap().\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n                     annot=True, # Annotate the boxes\n                     cbar=False)\n    plt.xlabel(\"true label\")\n    plt.ylabel(\"predicted label\")\n    \nplot_conf_mat(y_test, y_preds)","d4f9ec9e":"# Show classification report\nprint(classification_report(y_test, y_preds))","5a1a1786":"# Check best hyperparameters\ngs_log_reg.best_params_","db636d69":"# Import cross_val_score\nfrom sklearn.model_selection import cross_val_score\n\n# Instantiate best model with best hyperparameters (found with GridSearchCV)\nclf = LogisticRegression(C=0.23357214690901212,\n                         solver=\"liblinear\")","da5082d4":"# Cross-validated accuracy score\ncv_acc = cross_val_score(clf,\n                         X,\n                         y,\n                         cv=5, # 5-fold cross-validation\n                         scoring=\"accuracy\") # accuracy as scoring\ncv_acc","6ab5ac20":"cv_acc = np.mean(cv_acc)\ncv_acc","a7501980":"# Cross-validated precision score\ncv_precision = np.mean(cross_val_score(clf,\n                                       X,\n                                       y,\n                                       cv=5, # 5-fold cross-validation\n                                       scoring=\"precision\")) # precision as scoring\ncv_precision\n","85f19a25":"# Cross-validated recall score\ncv_recall = np.mean(cross_val_score(clf,\n                                    X,\n                                    y,\n                                    cv=5, # 5-fold cross-validation\n                                    scoring=\"recall\")) # recall as scoring\ncv_recall","63f8b76c":"# Cross-validated F1 score\ncv_f1 = np.mean(cross_val_score(clf,\n                                X,\n                                y,\n                                cv=5, # 5-fold cross-validation\n                                scoring=\"f1\")) # f1 as scoring\ncv_f1","18311fc9":"# Visualizing cross-validated metrics\ncv_metrics = pd.DataFrame({\"Accuracy\": cv_acc,\n                            \"Precision\": cv_precision,\n                            \"Recall\": cv_recall,\n                            \"F1\": cv_f1},\n                          index=[0])\ncv_metrics.T.plot.bar(title=\"Cross-Validated Metrics\", legend=False);","32be4a2d":"# Fit an instance of LogisticRegression (taken from above)\nclf.fit(X_train, y_train);","583d35b2":"# Check coef_\nclf.coef_","4bcb5656":"# Match features to columns\nfeatures_dict = dict(zip(df.columns, list(clf.coef_[0])))\nfeatures_dict","728c8bec":"# Visualize feature importance\nfeatures_df = pd.DataFrame(features_dict, index=[0])\nfeatures_df.T.plot.bar(title=\"Feature Importance\", legend=False);","b31c527a":"pd.crosstab(df[\"sex\"], df[\"target\"])","9850c66f":"# Contrast slope (positive coefficient) with target\npd.crosstab(df[\"slope\"], df[\"target\"])","0ab45efd":"Now we've got an instantiated classifier, let's find some cross-validated metrics.","0578473f":"#Load Data","8d03007f":"Now we've tuned LogisticRegression using RandomizedSearchCV, we'll do the same for RandomForestClassifier.","50607b2d":"Logistic Regression - LogisticRegression()\n\nK-Nearest Neighbors - KNeighboursClassifier()\n\nRandomForest - RandomForestClassifier()","87ff86f3":"# **Predicting Heart Disease using Machine Learning**\n## 1. Problem Definition\n> Given clinical parameters about a patient, can we predict whether or not they have heart disease?\n### Dataset\n(https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci\/)\n## Data Dictionary\n1. age - age in years \n2. sex - (1 = male; 0 = female) \n3. cp - chest pain type \n    * 0: Typical angina: chest pain related decrease blood supply to the heart\n    * 1: Atypical angina: chest pain not related to heart\n    * 2: Non-anginal pain: typically esophageal spasms (non heart related)\n    * 3: Asymptomatic: chest pain not showing signs of disease\n4. trestbps - resting blood pressure (in mm Hg on admission to the hospital)\n    * anything above 130-140 is typically cause for concern\n5. chol - serum cholestoral in mg\/dl \n    * serum = LDL + HDL + .2 * triglycerides\n    * above 200 is cause for concern\n6. fbs - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false) \n    * '>126' mg\/dL signals diabetes\n7. restecg - resting electrocardiographic results\n    * 0: Nothing to note\n    * 1: ST-T Wave abnormality\n        - can range from mild symptoms to severe problems\n        - signals non-normal heart beat\n    * 2: Possible or definite left ventricular hypertrophy\n        - Enlarged heart's main pumping chamber\n8. thalach - maximum heart rate achieved \n9. exang - exercise induced angina (1 = yes; 0 = no) \n10. oldpeak - ST depression induced by exercise relative to rest \n    * looks at stress of heart during excercise\n    * unhealthy heart will stress more\n11. slope - the slope of the peak exercise ST segment\n    * 0: Upsloping: better heart rate with excercise (uncommon)\n    * 1: Flatsloping: minimal change (typical healthy heart)\n    * 2: Downslopins: signs of unhealthy heart\n12. ca - number of major vessels (0-3) colored by flourosopy \n    * colored vessel means the doctor can see the blood passing through\n    * the more blood movement the better (no clots)\n13. thal - thalium stress result\n    * 1,3: normal\n    * 6: fixed defect: used to be defect but ok now\n    * 7: reversable defect: no proper blood movement when excercising \n14. target - have disease or not (1=yes, 0=no) (= the predicted attribute)\n\n## Preparing the tools\n\n* [pandas](https:\/\/pandas.pydata.org\/) for data analysis.\n* [NumPy](https:\/\/numpy.org\/) for numerical operations.\n* [Matplotlib](https:\/\/matplotlib.org\/)\/[seaborn](https:\/\/seaborn.pydata.org\/) for plotting or data visualization.\n* [Scikit-Learn](https:\/\/scikit-learn.org\/stable\/) for machine learning modelling and evaluation.","f62f9f62":"Now we've matched the feature coefficients to different features, let's visualize them.","5f2a8414":"###Model choices","85e84487":"###Heart Disease Frequency per Chest Pain Type","8b4e02f7":"\nSince there are 5 metrics here, we'll take the average.","15bcd218":"#ROC Curve and AUC Scores","e48e7935":"###Model Comparison","b19696d9":"Let's create a hyperparameter grid (a dictionary of different hyperparameters) for each and then test them out.","090931d5":"###Training and test split","c03d2190":"It seems the younger someone is, the higher their max heart rate (dots are higher on the left of the graph) and the older someone is, the more green dots there are. But this may be because there are more dots all together on the right side of the graph (older participants).","10162d3c":"#Hyperparameter tuning and cross-validation","b7dd4009":"##Confusion matrix","0b7e7655":"We'll pass it the different hyperparameters from log_reg_grid as well as set n_iter = 20. This means, RandomizedSearchCV will try 20 different combinations of hyperparameters from log_reg_grid and save the best ones.","d64aca51":"##Classification report","7e918a3b":"##Feature importance\nSince we're using LogisticRegression, we'll look at one way we can calculate feature importance for it.","95c63d45":"The larger the value (bigger bar), the more the feature contributes to the models decision.\n\nIf the value is negative, it means there's a negative correlation. And vice versa for positive values.\n\nFor example, the sex attribute has a negative value of -0.904, which means as the value for sex increases, the target value decreases.\n\nWe can see this by comparing the sex column to the target column.","3673f9af":"###Tuning models with with RandomizedSearchCV","de91a61f":"###Age vs Max Heart rate for Heart Disease\nLet's try combining a couple of independent variables, such as, age and thalach (maximum heart rate) and then comparing them to our target variable heart disease.","b90b1fe3":"<a href=\"https:\/\/colab.research.google.com\/github\/sujikathir\/Heart-disease-Identification\/blob\/master\/HEART_DISEASE.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a>","199c919b":"Evaluating a classification model, beyond accuracy\n\nROC curve and AUC score - plot_roc_curve()\n\nConfusion matrix - confusion_matrix()\n\nClassification report - classification_report()\n\nPrecision - precision_score()\n\nRecall - recall_score()\n\nF1-score - f1_score()","a5f34641":"We can see it's a normal distribution but slightly swaying to the right","e78fcffc":"##Tune KNeighborsClassifier (K-Nearest Neighbors or KNN)","21cb09dd":"Let's visualize them.","2f0f4ad5":"What can we infer from this? Let's make a simple heuristic.\n\n* Since there are about 100 women and 72 of them have a postive value of heart disease being present, we might infer, based on this one variable if the participant is a woman, there's a 75% chance she has heart disease.\n\n* As for males, there's about 200 total with around half indicating a presence of heart disease. So we might predict, if the participant is male, 50% of the time he will have heart disease.\n\n* Averaging these two values, we can assume, based on no other parameters, if there's a person, there's a 62.5% chance they have heart disease.","98061f17":"#Modeling","3b9add81":"##Making our crosstab visual","de528b77":"We'll make predictions on the test data.","7d51cb5f":"Slope is the \"slope of the peak exercise ST segment\" where:\n\n0: Upsloping: better heart rate with excercise (uncommon)\n\n1: Flatsloping: minimal change (typical healthy heart)\n\n2: Downslopins: signs of unhealthy heart\n\nAccording to the model, there's a positive correlation of 0.470, not as strong as sex and target but still more than 0.\n\nThis positive correlation means our model is picking up the pattern that as slope increases, so does the target value.","a5aad677":"#Correlation between independent variables","ad9bf8b9":"###Heart Disease Frequency according to Gender\n* Male - 1\n* Female - 0","18cbb736":"###Tuning a model with GridSearchCV","5a859013":"When sex is 0 (female), there are almost 3 times as many (72 vs. 24) people with heart disease (target = 1) than without.\n\nAnd then as sex increases to 1 (male), the ratio goes down to almost 1 to 1 (114 vs. 93) of people who have heart disease and who don't.\n\nIt means the model has found a pattern which reflects the data. Looking at these figures and this specific dataset, it seems if the patient is female, they're more likely to have heart disease."}}