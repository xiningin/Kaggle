{"cell_type":{"f9de65a0":"code","cb17a89a":"code","66deebf9":"code","078ecbaf":"code","7a97e830":"code","ae563699":"code","547dffa0":"code","579821ef":"code","41154bed":"code","f215fe25":"code","60f1e8b4":"code","6d4c3ec4":"code","472febc9":"code","599bf9f1":"code","3ba45a33":"code","f97e6f56":"code","47ac1c20":"code","b2b187ae":"code","253da5e4":"code","c65aa9ab":"code","1c2679d2":"code","c6f393bb":"code","c3e4a5bc":"code","4b36942e":"code","0020d74f":"code","cd639cb9":"code","f941d53f":"code","7658e62e":"code","7ee74d23":"code","62f48aba":"code","5a834d69":"code","5ebf0868":"code","ac3677e0":"code","59ff5542":"code","1127b2af":"code","c936a45b":"code","568658c3":"code","82574047":"code","5e77415f":"code","de6b3eec":"code","7f6b6841":"code","96c5cd57":"code","0e18a79a":"code","4ccf0e72":"code","c191bca7":"code","2eb3bdec":"code","1036c95c":"code","203e57ce":"code","6cdb1767":"code","f114a92f":"code","cc382ee5":"code","a45afaa5":"code","f4c6a028":"markdown","e58114f7":"markdown","2e12399a":"markdown","4ed1592b":"markdown","68a38108":"markdown","f51c48fe":"markdown","3a039aa8":"markdown","042bf1e9":"markdown","1083f1c6":"markdown","dd0a777a":"markdown","aecb43bd":"markdown","4887a11f":"markdown","6851b6cc":"markdown","fcadee58":"markdown","2506fbf5":"markdown","f3e04d3b":"markdown","4948ab0b":"markdown","ddabd794":"markdown","20c07def":"markdown","372032b6":"markdown","01c47729":"markdown","4caf9c83":"markdown","34835347":"markdown","1d7004a3":"markdown","a55694f2":"markdown","1495b619":"markdown"},"source":{"f9de65a0":"#importing all important libraries \n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import confusion_matrix,accuracy_score,roc_curve,classification_report\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nimport time\nimport datetime as dt\n\nfrom sklearn.metrics import silhouette_samples,silhouette_score\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom collections import Counter\nimport pandas_profiling as pp\n# data preprocessing\nfrom sklearn.preprocessing import StandardScaler","cb17a89a":"data = pd.read_csv('\/kaggle\/input\/ecommerce-data\/data.csv',encoding=\"ISO-8859-1\")\ndata","66deebf9":"print(data.shape)","078ecbaf":"# checking null values\ndata.isnull().sum()","7a97e830":"data.dropna(axis = 0, inplace = True)\nprint(data.shape)   #removing the null values","ae563699":"sns.heatmap(data.isnull())","547dffa0":"pp.ProfileReport(data)","579821ef":"data = data[data['Quantity']>=0]\ndata = data[data['UnitPrice']>=0]\n\nprint(data.shape)","41154bed":"data.describe()","f215fe25":"data['TotalQuantity'] = data['Quantity']*data['UnitPrice']\ndata.head()","60f1e8b4":"print(data[['InvoiceNo','Country']].groupby('Country').count().sort_values(\"InvoiceNo\",ascending = False))","6d4c3ec4":"'''Top 5 countries sales count wise in the cleaned up data.'''\ndata['Country'].value_counts().head(10).plot(kind='bar')","472febc9":"data[data['TotalQuantity']==data['TotalQuantity'].max()]","599bf9f1":"items = data['Description'].value_counts().head()\nprint(items)","3ba45a33":"print(data[['InvoiceNo','Country','CustomerID','TotalQuantity']].sort_values('TotalQuantity',ascending = False).head(15))","f97e6f56":"print(data[['InvoiceNo','Country','CustomerID','TotalQuantity']].sort_values('TotalQuantity',ascending = False).head(15))","47ac1c20":"print(data[['InvoiceNo','Country','CustomerID','TotalQuantity']].sort_values('TotalQuantity',ascending = False).head(15))","b2b187ae":"data['InvoiceDate'] = pd.to_datetime(data['InvoiceDate'])\ndata['Date'] = data['InvoiceDate'].apply(lambda x: x.date())\ndata.head()","253da5e4":"data['Month']=data['InvoiceDate'].apply(lambda x:x.month)\ndata['Year']=data['InvoiceDate'].apply(lambda x:x.year)\ndata=data.sort_values(by=['Year','Month'])\n\nmmap={1:'Jan11',2:'Feb11',3:'Mar11',4:'Apr11', 5:'May11', 6:'Jun11', 7:'Jul11',8:'Aug11',9:'Sep11',10:'Oct11',11:'Nov11',12:'Dec11'}\ndata['Month_name']=data['Month'].map(mmap)\n","c65aa9ab":"def my(x):\n    Month=x[0]\n    Year=x[1]\n    \n    if Year==2010:\n        Month='Dec10'\n        return Month\n    else:\n        return Month","1c2679d2":"data['Month_name']=data[['Month_name','Year']].apply(my, axis=1)","c6f393bb":"data.head()","c3e4a5bc":"monthly=data.groupby(['Year','Month','Month_name']).sum()\nmonthly.head()","4b36942e":"#recency dataframe\nrecency_df = data.groupby(by='CustomerID', as_index=False)['Date'].max()\nrecency_df.columns = ['CustomerID','LastPurchaseDate']\nrecency_df.head(5)\n","0020d74f":"current = dt.date(2021,12,1)\nprint(current)\n","cd639cb9":"recency_df['Recency'] = recency_df['LastPurchaseDate'].apply(lambda x: (current - x).days)\nrecency_df.drop('LastPurchaseDate',axis = 1,inplace=True)\nrecency_df.head(5)","f941d53f":"temp = data.copy()\ntemp.drop_duplicates(['InvoiceNo','CustomerID'],keep='first',inplace=True)\nfrequency_df = temp.groupby(by=['CustomerID'], as_index=False)['InvoiceNo'].count()\nfrequency_df.columns = ['CustomerID','Frequency']\nfrequency_df.head()","7658e62e":"monetary_df = data.groupby(by = 'CustomerID',as_index=False).agg({'TotalQuantity':'sum'})\nmonetary_df.columns = ['CustomerID','TotalQuanity']\nmonetary_df.head(5)","7ee74d23":"rfm_df = recency_df.merge(frequency_df,on='CustomerID').merge(monetary_df,on='CustomerID')\nrfm_df.set_index('CustomerID',inplace=True)\nrfm_df.head(5)","62f48aba":"features = rfm_df.columns\nrfm_df.shape","5a834d69":"print(rfm_df.corr())\nsns.heatmap(rfm_df.corr(),cmap=\"YlGnBu\",annot=True)","5ebf0868":"from sklearn.preprocessing import PowerTransformer\npt = PowerTransformer()\nrfm_df = pd.DataFrame(pt.fit_transform(rfm_df))\nrfm_df.columns = features\nrfm_df.head()","ac3677e0":"sc = StandardScaler()\nrfm_scaled = sc.fit_transform(rfm_df)\nrfm_scaled","59ff5542":"from sklearn.decomposition import PCA\npca = PCA()\npca_tranformed_data = pca.fit_transform(rfm_scaled)","1127b2af":"pca.components_","c936a45b":"pca.explained_variance_","568658c3":"var_exp = pca.explained_variance_ratio_\nvar_exp","82574047":"X = rfm_df.copy()\npca = PCA(n_components = 2)\ndf_pca = pca.fit_transform(X)\n\ndf_pca = pd.DataFrame(df_pca)\ndf_pca.head(5)","5e77415f":"X = df_pca.copy()","de6b3eec":"from sklearn.cluster import KMeans \n\ncluster_range = range(1, 15)\ncluster_errors = []\ncluster_sil_scores = []\n\nfor num in cluster_range: \n    clusters = KMeans(num, n_init = 100,init='k-means++',random_state=0)\n    clusters.fit(X)\n    # capture the cluster lables\n    labels = clusters.labels_  \n    # capture the centroids\n    centroids = clusters.cluster_centers_ \n    # capture the intertia\n    cluster_errors.append( clusters.inertia_ )    \nclusters_df = pd.DataFrame({ \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors} )\nclusters_df[0:10]","7f6b6841":"plt.figure(figsize=(15,6))\nplt.plot(clusters_df[\"num_clusters\"],clusters_df[\"cluster_errors\"],marker = 'o')\nplt.xlabel('count of clusters')\nplt.ylabel('error')","96c5cd57":"for num in range(2,16):\n    clusters = KMeans(n_clusters=num,random_state=0)\n    labels = clusters.fit_predict(df_pca)\n    \n    sil_avg = silhouette_score(df_pca, labels)\n    print('For',num,'The Silhouette Score is =',sil_avg)","0e18a79a":"kmeans = KMeans(n_clusters = 6)\nkmeans = kmeans.fit(df_pca)\nlabels = kmeans.predict(df_pca)\ncentroids = kmeans.cluster_centers_\n\nprint(labels)\nprint()\nprint('Cluster Centers')\nprint(centroids)","4ccf0e72":"df_pca['Clusters'] = labels\ndf_pca.head()","c191bca7":"df_pca['Clusters'].value_counts()","2eb3bdec":"df_pca.head(5)","1036c95c":"X = df_pca[[0,1]]\nY = df_pca['Clusters']\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42, stratify=Y)\n\n# instantiate the model\ndc=DecisionTreeClassifier()\nknn=KNeighborsClassifier(1)\n# train a Gaussian Naive Bayes classifier on the training set\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\n\n\nm6 = 'DecisionTreeClassifier'\ndt = DecisionTreeClassifier(criterion = 'entropy',random_state=0,max_depth = 6)\ndt.fit(X_train, y_train)\ndt_predicted = dt.predict(X_test)\ndt_conf_matrix = confusion_matrix(y_test, dt_predicted)\ndt_acc_score = accuracy_score(y_test, dt_predicted)\nprint(\"confussion matrix\")\nprint(dt_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of DecisionTreeClassifier:\",dt_acc_score*100,'\\n')\nprint(classification_report(y_test,dt_predicted))","203e57ce":"# Print the Confusion Matrix and slice it into four pieces\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, dt_predicted)\n\nprint('Confusion matrix\\n\\n', cm)\n\nprint('\\nTrue Positives(TP) = ', cm[0,0])\n\nprint('\\nTrue Negatives(TN) = ', cm[1,1])\n\nprint('\\nFalse Positives(FP) = ', cm[0,1])\n\nprint('\\nFalse Negatives(FN) = ', cm[1,0])","6cdb1767":"m5 = 'K-NeighborsClassifier'\nknn = KNeighborsClassifier(n_neighbors=10)\nknn.fit(X_train, y_train)\nknn_predicted = knn.predict(X_test)\nknn_conf_matrix = confusion_matrix(y_test, knn_predicted)\nknn_acc_score = accuracy_score(y_test, knn_predicted)\nprint(\"confussion matrix\")\nprint(knn_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of K-NeighborsClassifier:\",knn_acc_score*100,'\\n')\nprint(classification_report(y_test,knn_predicted))","f114a92f":"# Print the Confusion Matrix and slice it into four pieces\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, knn_predicted)\n\nprint('Confusion matrix\\n\\n', cm)\n\nprint('\\nTrue Positives(TP) = ', cm[0,0])\n\nprint('\\nTrue Negatives(TN) = ', cm[1,1])\n\nprint('\\nFalse Positives(FP) = ', cm[0,1])\n\nprint('\\nFalse Negatives(FN) = ', cm[1,0])\n\n\n\n\n","cc382ee5":"m2 = 'Naive Bayes'\ngnb.fit(X_train,y_train)\nnbpred = gnb.predict(X_test)\nnb_conf_matrix = confusion_matrix(y_test, nbpred)\nnb_acc_score = accuracy_score(y_test, nbpred)\nprint(\"confussion matrix\")\nprint(nb_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Naive Bayes model:\",nb_acc_score*100,'\\n')\nprint(classification_report(y_test,nbpred))","a45afaa5":"# Print the Confusion Matrix and slice it into four pieces\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test,nbpred)\n\nprint('Confusion matrix\\n\\n', cm)\n\nprint('\\nTrue Positives(TP) = ', cm[0,0])\n\nprint('\\nTrue Negatives(TN) = ', cm[1,1])\n\nprint('\\nFalse Positives(FP) = ', cm[0,1])\n\nprint('\\nFalse Negatives(FN) = ', cm[1,0])\n","f4c6a028":"# Conclusion\nWe saw that using classification models like Logisitc Regression, KNeighborsClassifier ,DecisionTree we predicted the clusters for customers using RFM dataset as independent variables and Cluster as the target variable. The clusters predicted by the classification models perfectly aligns with K-Means clustering. So, we can conclude that our clusters are correct.","e58114f7":"* [1. Introduction](#1)\n* [2. Data Reading and Analysis](#2)\n* [3. Data Processing and Cleansing](#3) \n* [4. Model Training](#5) <br>\n\n<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0' role=\"tab\" aria-controls=\"home\"><center>Introduction<\/center><a id=1><\/a><\/h3>","2e12399a":"# **Recency**\n\nTo calculate recency, we need to choose a date point from which we evaluate how many days ago was the customer's last purchase.","4ed1592b":"# Summary\n\nThe work described in this notebook is based on a database providing details on purchases made on an E-commerce platform over a period of one year. Each entry in the dataset describes the purchase of a product, by a particular customer and at a given date. In total, approximately  **\u223c 4000** clients appear in the database. Given the available information, I decided to develop a classifier that allows to anticipate the type of purchase that a customer will make, as well as the number of visits that he will make during a year, and this from its first visit to the E-commerce site.\n\nThe next part of the analysis consisted of some basic **data visualization**. This was done in order to get insights regarding the country which was using the E-commerce website the most. I used basic plots in order to show the results of my analysis. I also tried to analyse other important factors such as the Gross Purcahse by a country as well as which following description was used the most. \n\nThe final part of the analysis was the customer segmentation part. The main way to go around with this procces is to use the **RFM (Recency, Frequency, Monetory) table** to sort the customer in the groups. After creating the RFM table I used **K-Means clustering (Elbow curve and Silhoutte scores)** in order to create 4 clusters in which the customers should be Segmented. After each of the customers were segmented into their respective groups. I used models such as **Logisitc Regression, KNeighborsClassifier ,DecisionTree** in order the cross the accuracy of the clustering which resulted in an accuracy score **0.98**. Hence, I conclude the customer segmentation was done which effective methods and high accuracy. ","68a38108":"> This dataframe contains 8 variables that correspond to:\n\n**InvoiceNo:** Invoice number. Nominal, a 6-digit integral number uniquely assigned to each \ntransaction. If this code starts with letter 'c', it indicates a cancellation.\n\n**StockCode:** Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.\n\n**Description:** Product (item) name. Nominal.\n\n**Quantity:** The quantities of each product (item) per transaction. Numeric.\n\n**InvoiceDate:** Invice Date and time. Numeric, the day and time when each transaction was generated.\n\n**UnitPrice:** Unit price. Numeric, Product price per unit in sterling.\n\n**CustomerID:** Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.\n\n**Country:** Country name. Nominal, the name of the country where each customer resides.","f51c48fe":"Now we will use the models to find the number of","3a039aa8":"# RFM Table Visualisation\n\nNow we will look at the correlation between the the Recency, Frequency and Monetary part of the RFM table which will be an integral part of customer segmentation","042bf1e9":"<a id=\"top\"><\/a>\n\u200b\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:Blue; border:0' role=\"tab\" aria-controls=\"home\"><center>Quick navigation<\/center><\/h3>","1083f1c6":"\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0' role=\"tab\" aria-controls=\"home\"><center>Data Reading and Analysis<\/center><a id=2><\/a><\/h3>","dd0a777a":"## Gaussian Naive Bayes Classifier","aecb43bd":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0' role=\"tab\" aria-controls=\"home\"><center>Data Processing and Cleansing<\/center><a id=3><\/a><\/h3>","4887a11f":"# Inferences:\n\nWe observe from the elbow plot a sharp bend after the number of clusters increase by 2.\nSilhoutte Score is also the highest for 2 clusters.\n\nBut, there is also a significant reduce in cluster error as number of clusters increase from 2 to 4 and after 4, the reduction is not much.\n\nSo, we will choose n_clusters = 4 to properly segment our customers.","6851b6cc":"# K-Means Clustering \n","fcadee58":"# Create RFM Table\n","2506fbf5":"# <center>Online Shoppers Intension Data set<\/center>","f3e04d3b":"Now we will look at the details of the countries through which most of the orders were placed\n","4948ab0b":"# Frequency\nFrequency helps us to know how many times a customer purchased from us. To do that we need to check how many invoices are registered by the same customer.","ddabd794":"\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0' role=\"tab\" aria-controls=\"home\"><center>Model Training<\/center><a id=5><\/a><\/h3>","20c07def":"## KNN","372032b6":"## Total Transaction's monthly","01c47729":"Now we can observe that difference before droping null values and after droping null values","4caf9c83":"# Conclusion\nTo gain even further insight into customer behavior, we can dig deeper in the relationship between RFM variables.\n\n**RFM model** can be used in conjunction with certain predictive models like **K-means clustering, Logistic Regression and Recommendation Engines** to produce better informative results on customer behavior.\n\nWe will go for **K-means since it has been widely used for Market Segmentation** and it offers the advantage of being simple to implement.","34835347":"# Monetary\nMonetary attribute answers the question: How much money did the customer spent over time?\n\nTo do that, first, we will create a new column total cost to have the total price per invoice.","1d7004a3":"Fortunately, there are no missing values. If there were missing values we will have to fill them with the median, mean or mode. I tend to use the median but in this scenario there is no need to fill any missing values. This will definitely make our job easier!","a55694f2":"# **RFM Analysis**\n\n**RFM (Recency, Frequency, Monetary)** analysis is a customer segmentation technique that uses past purchase behaviour to divide customers into groups.\nRFM helps divide customers into various categories or clusters to identify customers who are more likely to respond to promotions and also for future personalization services.\n\n**RECENCY (R)**: Days since last purchase\n\n**FREQUENCY (F)**: Total number of purchases\n\n**MONETARY VALUE (M)**: Total money this customer spent.\nWe will create those 3 customer attributes for each customer.","1495b619":"# PCA\n\nApplying PCA to reduce the the dimensions and the correlation between Frequency and Monetary features."}}