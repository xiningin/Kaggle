{"cell_type":{"daaa6813":"code","76ec570c":"code","92c4e567":"code","5e5a1797":"code","08cb6041":"code","06a4a000":"code","a9ce1838":"code","0ca872cc":"code","2892fb91":"code","96cbb989":"code","67e66a36":"code","68fdf39b":"markdown","1df97ec1":"markdown","5fb10326":"markdown","6a32ac9e":"markdown","ef6892a5":"markdown","fa13d7a2":"markdown","05e4c154":"markdown","86ba8af6":"markdown","6f68d63d":"markdown","fc715485":"markdown","edb247f0":"markdown","9368f436":"markdown","0bd12fd3":"markdown","f1276096":"markdown","95463cb4":"markdown"},"source":{"daaa6813":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nfrom statsmodels.api import  qqplot\n\nfrom sklearn import preprocessing as preprocessing\nfrom sklearn.linear_model import LinearRegression, RidgeCV\nfrom sklearn.metrics import mean_absolute_error\nimport types\n\n\nfrom scipy.stats import shapiro, normaltest, anderson\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\ndef load_data(path):\n    return pd.read_csv(path+'train.csv', index_col='Id'), pd.read_csv(path+'test.csv', index_col='Id')\n\ndef init_sns():\n    # set seaborn style dark\n    sns.set_style('dark')\n    \ndef qq_plot(x, **kwargs):\n    qqplot(x, ax=plt.gca(), **kwargs)\n\ndef cat_cols(df):\n    \"\"\"return categorical columns\n    \"\"\"\n    return [cname for cname in df.columns if\n                    df[cname].dtype == \"O\"]\n\ndef num_cols(df):\n    \"\"\"return numerical columns\n\n        total columns = categorical columns + numerical columns\n    \"\"\"\n    return df.columns[~df.columns.isin(cat_cols(df))]\n\ndef get_cols_with_(x, data):\n    \"\"\" return columns of DataFrame 'data', which contains str 'x'\n    \"\"\"\n    cols = []\n    for c in data.columns:\n        if x in c:\n            cols.append(c)\n    return cols\n\ndef save(index, preds):\n    # Save test predictions to file\n    output = pd.DataFrame({'Id': index,\n                           'SalePrice': preds})\n    output.to_csv('submission.csv', index=False)\n    \ndef nomality_tests(data):\n    \"\"\"\n    return shpiro test result(s1, p1),\n    D\u2019Agostino\u2019s K^2 Test result(s2,p2), \n    Anderson-Darling Test result\n    in tuple\n    \"\"\"\n    s1, p1 = shapiro(data)\n    s2, p2 = normaltest(data)\n    result = anderson(data)\n    return (s1, p1), (s2, p2), result\n\ndef linear_regression(data):\n    lr = LinearRegression()\n    x_train, y_train = data[n_feats], data['SalePrice']\n    lr.fit(x_train, y_train)\n    return lr\n\ndef fill_garageyrblt(data, c):\n    rows = data['GarageYrBlt'].isnull()\n    data.loc[rows, 'GarageYrBlt']= \\\n    data.loc[rows, 'YearBuilt']\n    \ndef miss_val_handler(data):\n    \n    def _handle_(data, c, v):\n        if isinstance(v, types.FunctionType):\n            v(data, c)\n        else:\n            null_r = data[c].isnull()\n            if null_r.sum() == 0: return\n            data.loc[null_r, c] = v        \n\n    for c in MISS_VAL:\n        v = MISS_VAL[c]\n        if isinstance(v, tuple):\n            for e in v:\n                _handle_(data, c, e)\n        else:\n            _handle_(data, c, v)\n            \n    return data\n            \ndef BsmtExposure_no(data,c):\n    #data.loc[949, 'BsmtExposure']='No'\n    pass\n\ndef median_impute(data, c):\n    null_r = data[c].isnull()\n    if null_r.sum() == 0: return\n    data.loc[null_r, c] = data[c].median()\n    \ndef most_freq_impute(data, c):\n    null_r = data[c].isnull()\n    if null_r.sum() == 0: return\n    data.loc[null_r, c] = data[c].mode()[0]\n    \n# load data to TRAIN_DATA, TEST_DATA\nPATH = '\/kaggle\/input\/home-data-for-ml-course\/'\nTRAIN_DATA, TEST_DATA = load_data(PATH)\n\n## Chosen numeric features\nN_FEATS = ['BedroomAbvGr', 'GrLivArea', 'LowQualFinSF', 'YearBuilt', 'FullBath',\n           'EnclosedPorch', '1stFlrSF', 'BsmtFinSF2', 'OverallQual', 'YearRemodAdd',\n           'BsmtUnfSF', 'LotArea', 'KitchenAbvGr', 'Fireplaces', 'OverallCond',\n           'TotRmsAbvGrd', 'PoolArea', 'LotFrontage', 'OpenPorchSF', 'BsmtFinSF1',\n           'GarageYrBlt', 'TotalBsmtSF', 'GarageCars', '3SsnPorch', 'BsmtHalfBath',\n           'WoodDeckSF', '2ndFlrSF', 'GarageArea', 'BsmtFullBath', 'MiscVal', 'ScreenPorch',\n           'HalfBath', 'MasVnrArea', 'YrSold']\n\n## Chosen categrical featrues\nC_FEATS = ['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities',\n           'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', \n           'BldgType','HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n           'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n           'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n           'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n           'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n           'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType',\n           'SaleCondition', 'MoSold', 'MSSubClass']\n\n## Doc of Values for missing value to fill.\n## where key is column name and value to fill missing value\nMISS_VAL = {'PoolQC': 'NA',\n            'FireplaceQu': 'NA',\n            'GarageCond': 'NA',\n            'GarageType': 'NA',\n            'GarageFinish': 'NA',\n            'GarageQual': 'NA',\n            'BsmtExposure': ('NA', BsmtExposure_no),\n            'BsmtFinType2': 'NA',\n            'BsmtFinType1': 'NA',\n            'BsmtQual': 'NA',\n            'BsmtCond': 'NA',\n            'MasVnrType': 'None',\n            'Electrical': 'SBrkr',\n            'MasVnrArea': 0,\n            'LotFrontage': median_impute,\n            'GarageYrBlt': fill_garageyrblt,\n            'BsmtFullBath':0,\n            'BsmtHalfBath':0,\n            'TotalBsmtSF': 0,\n            'BsmtFinSF1': 0,\n            'BsmtUnfSF': 0, \n            'BsmtFinSF2': 0,\n            'Exterior1st': most_freq_impute,\n            'Exterior2nd': most_freq_impute,\n            'MSZoning': most_freq_impute,\n            'Functional': most_freq_impute,\n            'Utilities': most_freq_impute,\n            'KitchenQual': most_freq_impute,\n            'SaleType': most_freq_impute,\n            'GarageArea': 0,\n            'GarageCars': 0}\n\n## Ordinal features\nORDINAL = ['ExterQual','BsmtQual', 'KitchenQual',\n           'GarageQual', 'HeatingQC', 'PoolQC',\n           'GarageType', 'GarageFinish', 'GarageCond',\n           'BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n           'BsmtFinType2', 'FireplaceQu', 'Electrical',\n           'LotShape', 'Functional', 'ExterCond',\n           'Utilities', 'PavedDrive']\n\n## Features to be one-hot encoded\nONE_HOT = ['LandSlope', 'LotConfig', 'Foundation', 'MSSubClass', 'SaleType', 'MSZoning',\n           'Neighborhood', 'MoSold', 'Condition1', 'Exterior2nd', 'BldgType', 'RoofStyle',\n           'HouseStyle', 'CentralAir', 'Heating', 'MasVnrType', 'LandContour', 'Street',\n           'Exterior1st', 'Condition2', 'SaleCondition', 'RoofMatl']\n\nQT = preprocessing.QuantileTransformer(output_distribution='normal', random_state=0)\nOHE =  preprocessing.OneHotEncoder(dtype='int', drop='first')\nORDE = preprocessing.OrdinalEncoder()\nCT = ColumnTransformer([('ohe', OHE, ONE_HOT),\n                        ('orde', ORDE, ORDINAL),\n                        ('qt', QT, N_FEATS)],\n                       remainder='drop')\n\nFEATS = N_FEATS + ORDINAL + ONE_HOT\n\nREGRESSOR = TransformedTargetRegressor(regressor=LinearRegression(),\n                                       transformer=QT)\n\nMISS_VAL_IMPUTER = preprocessing.FunctionTransformer(miss_val_handler, validate=False)\n\nPIPELINE = Pipeline(memory=None,\n                    steps=[('imputer', MISS_VAL_IMPUTER),\n                           ('col_trans', CT),\n                           ('linear_reg', REGRESSOR)],\n                    verbose=True)\n\ndef ols_regression(x_train, x_test):\n    X_train = x_train.copy()\n    X_test = x_test.copy()\n\n    # test + train data merge\n    merged = pd.concat([X_train, X_test], sort=True)\n    miss_val_handler(merged)\n    # missing value handle\n    X_train[949, 'BsmtExposure'] = 'No'\n\n    PIPELINE.fit(merged.loc[:1460,:].copy(), merged.loc[:1460,'SalePrice'].copy())\n    return mean_absolute_error(X_train['SalePrice'],\n                              PIPELINE.predict(merged.loc[:1460,:].copy()))\n\nMSSUBCLASS_OHE =  preprocessing.OneHotEncoder(dtype='int', handle_unknown='ignore')\n\nCT2 = ColumnTransformer([('mssubclass_ohe', MSSUBCLASS_OHE, ['MSSubClass']),\n                        ('ohe', OHE, list(set(ONE_HOT) - set(['MSSubClass']))),\n                        ('orde', ORDE, ORDINAL),\n                        ('qt', QT, N_FEATS)],\n                       remainder='drop')\n\nRIDGE_REGRESSOR = TransformedTargetRegressor(\n    regressor=RidgeCV(alphas=[1e-6, 1e-3, 1e-2, 1e-1, 1], scoring='neg_mean_absolute_error'),\n    transformer=QT)\n\nRIDGE_PIPELINE = Pipeline(memory=None,\n                    steps=[('imputer', MISS_VAL_IMPUTER),\n                           ('col_trans', CT2),\n                           ('ridge_reg', RIDGE_REGRESSOR)],\n                    verbose=True)\n\ndef ridge_regression(x_train, x_test, save_test=False):\n    X_train = x_train.copy()\n    X_test = x_test.copy()\n\n    # test + train data merge\n    merged = pd.concat([X_train, X_test], sort=True)\n    miss_val_handler(merged)\n    # missing value handle\n    X_train[949, 'BsmtExposure'] = 'No'\n\n    RIDGE_PIPELINE.fit(merged.loc[:1460,:].copy(), merged.loc[:1460,'SalePrice'].copy())\n    \n        \n    if save_test:\n        preds = RIDGE_PIPELINE.predict(merged.loc[1461:,:].copy())\n        save(X_test.index, preds)\n        \n    return mean_absolute_error(X_train['SalePrice'],\n                              RIDGE_PIPELINE.predict(merged.loc[:1460,:].copy()))","76ec570c":"RIDGE_PIPELINE = Pipeline(memory=None,\n                    steps=[('imputer', MISS_VAL_IMPUTER),\n                           ('col_trans', CT2),\n                           ('ridge_reg', RIDGE_REGRESSOR)],\n                    verbose=True)\n\nmae = ridge_regression(TRAIN_DATA, TEST_DATA)\nprint(mae)","92c4e567":"RIDGE_PIPELINE = Pipeline(memory=None,\n                    steps=[('imputer', MISS_VAL_IMPUTER),\n                           ('col_trans', CT),\n                           ('ridge_reg', RIDGE_REGRESSOR)],\n                    verbose=True)\n\nmae = ridge_regression(TRAIN_DATA, TEST_DATA)\nprint(mae)","5e5a1797":"PIPELINE = Pipeline(memory=None,\n                    steps=[('imputer', MISS_VAL_IMPUTER),\n                           ('col_trans', CT2),\n                           ('linear_reg', REGRESSOR)],\n                    verbose=True)\n\nmae = ols_regression(TRAIN_DATA, TEST_DATA)\nprint(mae)","08cb6041":"PIPELINE = Pipeline(memory=None,\n                    steps=[('imputer', MISS_VAL_IMPUTER),\n                           ('col_trans', CT),\n                           ('linear_reg', REGRESSOR)],\n                    verbose=True)\n\nmae = ols_regression(TRAIN_DATA, TEST_DATA)\nprint(mae)","06a4a000":"RIDGE_REGRESSOR.regressor_.alpha_","a9ce1838":"RIDGE_REGRESSOR =TransformedTargetRegressor(\n    regressor=RidgeCV(alphas=[1e-6], scoring='neg_mean_absolute_error'),\n    transformer=QT)\n\nRIDGE_PIPELINE = Pipeline(memory=None,\n                    steps=[('imputer', MISS_VAL_IMPUTER),\n                           ('col_trans', CT2),\n                           ('ridge_reg', RIDGE_REGRESSOR)],\n                    verbose=True)\n\nmae = ridge_regression(TRAIN_DATA, TEST_DATA)\nprint(mae)","0ca872cc":"RIDGE_REGRESSOR =TransformedTargetRegressor(\n    regressor=RidgeCV(alphas=[1e-6, 1e-3, 1e-2, 1e-1, 1, 10, 20], scoring='neg_mean_absolute_error'),\n    transformer=QT)","2892fb91":"RIDGE_PIPELINE = Pipeline(memory=None,\n                    steps=[('imputer', MISS_VAL_IMPUTER),\n                           ('col_trans', CT2),\n                           ('ridge_reg', RIDGE_REGRESSOR)],\n                    verbose=True)\n\nmae = ridge_regression(TRAIN_DATA, TEST_DATA)\nprint(mae)","96cbb989":"RIDGE_REGRESSOR.regressor_.alpha_","67e66a36":"mae = ridge_regression(TRAIN_DATA, TEST_DATA, True)\nprint(mae)","68fdf39b":"Selected alpha was 1.0. What if we set alphas=\\[1e-6\\]?","1df97ec1":"##  'MSSubClass' one-hot-encode with option handle_unknown='ignore'","5fb10326":"# Data load and utility functions","6a32ac9e":"Ridge regression objective\n\n\n$$\\min_{w} \\| X w - y\\|_2^2 + \\alpha \\|w\\|_2^2$$\n\n\nL2 ( $\\alpha \\|w\\|_2^2$ ) of Ridge regression is works.","ef6892a5":"Although the mae is worse than OLS, LB score (14286.74819) is better than earler. Scoring with train data is not good, since fitted model already know the data.","fa13d7a2":"# Next plan\n\n* LASSO - L1 regularization\n* Train, Validation separation\n* Grid search for optimal hyper parameter auto searching\n* Feature selection and Feature engineering","05e4c154":"##  'MSSubClass' one-hot-encode with option handle_unknown='ignore'","86ba8af6":"# More on ridge regression\n\nAny way, which value was selected? We set alphas=\\[1e-6, 1e-3, 1e-2, 1e-1, 1\\]","6f68d63d":"Select best alpha is 10.0 by RidgeCV. Result submitted with alpha 1.0, the LB score is worse than OLS.","fc715485":"# OLS linear regression","edb247f0":"##  'MSSubClass' one-hot-encode without option handle_unknown='ignore'","9368f436":"# Ridge regrssion","0bd12fd3":"Result is almost same as OLS.\n\nWhat if we set alphas=\\[1e-6, 1e-3, 1e-2, 1e-1, 1, 10, 20, 30\\]","f1276096":"In last notebook [Housing price: Ridge Regression](https:\/\/www.kaggle.com\/angelndevil2\/housing-price-ridge-regression), result (MAE: 12708.169406839932) is not improved compared to OLS ([Housing price: OLS Linear Regression - 3](https:\/\/www.kaggle.com\/angelndevil2\/housing-price-ols-linear-regression-3)) (MAE: 12051.77159387892).\n\nDiffernce\n\n* OLS vs Ridge\n* In ridge regression, MSSubClass featrues are ignored for some samples (MSSubClass 150) when predict using test data.","95463cb4":"##  'MSSubClass' one-hot-encode without option handle_unknown='ignore'"}}