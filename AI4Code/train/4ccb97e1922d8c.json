{"cell_type":{"8ee11822":"code","6c391022":"code","67399efc":"code","f4654023":"code","9ef5e68f":"code","402c3b80":"code","4e16712f":"code","b8bbcb85":"code","8957df5e":"code","34cf784a":"code","c9a92d2d":"code","355932d7":"code","2983198b":"code","af5e7fe8":"code","5d302745":"code","56a95af8":"code","669e7cac":"code","7b5b04e3":"code","5b6fb751":"code","2ac06a92":"code","efa0d9c0":"code","63c6c75e":"code","3497b401":"code","ec4da5f5":"code","075e00c5":"code","04bfd400":"code","1112e5d2":"code","2a21d3d1":"code","6731d049":"code","e6bc6e0d":"code","026158f5":"code","16f818d9":"code","4a66b1e2":"code","1616d348":"code","d5a63ccf":"code","e6aab11d":"code","b93b0e50":"code","fca35dbc":"code","ce5e4ee7":"code","e6045349":"markdown","b4cb1216":"markdown","0f6d02f3":"markdown","f84aa991":"markdown","54a3b10d":"markdown","5fdc7797":"markdown","751da617":"markdown","640731a2":"markdown","2ef0d619":"markdown","e3bf6d7b":"markdown","adea4bc8":"markdown","d2e5d0a3":"markdown","a2f2e8f8":"markdown","7aa9019a":"markdown","3bb07d16":"markdown","f4cc11c2":"markdown","6866770e":"markdown","c88efcc6":"markdown","6edbfbcd":"markdown","33e8760d":"markdown","21b5cece":"markdown","b91f0f74":"markdown","3960d290":"markdown","3c70403b":"markdown","960f82ef":"markdown","fd551a80":"markdown","b8e61026":"markdown","e54025a8":"markdown"},"source":{"8ee11822":"import keras\nfrom keras.models import Sequential,Input,Model\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\n\nimport os\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import confusion_matrix\nfrom IPython.display import SVG","6c391022":"def list_images(basePath, contains=None):\n    # return the set of files that are valid\n    return list_files(basePath, validExts=(\".jpg\", \".jpeg\", \".png\", \".bmp\"), contains=contains)\n\ndef list_files(basePath, validExts=(\".jpg\", \".jpeg\", \".png\", \".bmp\"), contains=None):\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and the filename does not contain\n            # the supplied string, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n\n            # check to see if the file is an image and should be processed\n            if ext.endswith(validExts):\n                # construct the path to the image and yield it\n                imagePath = os.path.join(rootDir, filename).replace(\" \", \"\\\\ \")\n                yield imagePath","67399efc":"def load_images(directory='', size=(150,150)):\n    images = []\n    labels = []  # Integers corresponding to the categories in alphabetical order\n    label = 0\n    \n    imagePaths = list(list_images(directory))\n    \n    for path in imagePaths:\n        if 'buildings' in path:\n            label = 0\n            \n        elif 'forest' in path:\n            label = 1\n            \n        elif 'glacier' in path:\n            label = 2\n            \n        elif 'mountain' in path:\n            label = 3\n            \n        elif 'sea' in path:\n            label = 4\n        \n        elif 'street' in path:\n            label = 5\n            \n        path = path.replace('\\\\','')\n            \n        image = cv2.imread(path) #Reading the image with OpenCV\n        image = cv2.resize(image,size) #Resizing the image, in case some are not of the same size\n        \n        images.append(image)\n        labels.append(label)\n    \n    return shuffle(images,labels,random_state=42) #Shuffles the dataset.","f4654023":"images, labels = load_images(directory='..\/input\/seg_train') #Extract the training images\n\nimages = np.array(images)\nlabels = np.array(labels)","9ef5e68f":"print(images.shape)\nprint(labels.shape)","402c3b80":"label_to_class={\n    0 : 'buildings',\n    1 : 'forest',\n    2 : 'glacier',\n    3 : 'mountain',\n    4 : 'sea',\n    5 : 'street'\n}","4e16712f":"_,ax = plt.subplots(4,5, figsize = (15,15)) \nfor i in range(4):\n    for j in range(5):\n        ax[i,j].imshow(images[5*i+j])\n        ax[i,j].set_title(label_to_class[labels[5*i+j]])\n        ax[i,j].axis('off')","b8bbcb85":"train_data_path = '..\/input\/seg_train\/seg_train'\ntest_data_path = '..\/input\/seg_test\/seg_test'\n\nsize=(150,150)\nepochs = 30\nbatch_size = 32\nnum_of_train_samples = 14000\nnum_of_test_samples = 3000\n\n#Image Generator\ntrain_datagen = ImageDataGenerator(rescale=1. \/ 255,\n                                   rotation_range=40,\n                                   width_shift_range=0.2,\n                                   height_shift_range=0.2,\n                                   shear_range=0.2,\n                                   zoom_range=0.2,\n                                   horizontal_flip=True,\n                                   fill_mode='nearest')\n\ntest_datagen = ImageDataGenerator(rescale=1. \/ 255)\n\n\ntrain_generator = train_datagen.flow_from_directory(train_data_path,\n                                                    target_size=size,\n                                                    batch_size=batch_size,\n                                                    class_mode='categorical')\n\nvalidation_generator = test_datagen.flow_from_directory(test_data_path,\n                                                        target_size=size,\n                                                        batch_size=batch_size,\n                                                        class_mode='categorical',\n                                                        shuffle=False)","8957df5e":"num_classes = 6\n\nsimple_model = Sequential()\n\nsimple_model.add(Conv2D(128,kernel_size=(3,3),activation='relu',input_shape=(150,150,3)))\nsimple_model.add(MaxPooling2D(2,2))\n\n#The batch normalization allows the model to converge much faster\nsimple_model.add(BatchNormalization())\nsimple_model.add(Conv2D(128,kernel_size=(3,3),activation='relu'))\nsimple_model.add(MaxPooling2D(2,2))\n\nsimple_model.add(BatchNormalization())\nsimple_model.add(Conv2D(64,kernel_size=(3,3),activation='relu'))\nsimple_model.add(MaxPooling2D(5,5))\nsimple_model.add(Flatten())\n\nsimple_model.add(Dense(128, activation='relu'))\nsimple_model.add(Dense(128, activation='relu'))\nsimple_model.add(Dropout(rate=0.3))\n\nsimple_model.add(Dense(num_classes,activation='softmax'))\n\nsimple_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(0.0001),metrics=['accuracy'])\n\nsimple_model.summary()","34cf784a":"training = simple_model.fit_generator(train_generator,\n                                      steps_per_epoch=num_of_train_samples \/\/ batch_size,\n                                      epochs=epochs,\n                                      validation_data=validation_generator,\n                                      validation_steps=num_of_test_samples \/\/ batch_size)","c9a92d2d":"plt.plot(training.history['acc'])\nplt.plot(training.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\nplt.plot(training.history['loss'])\nplt.plot(training.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","355932d7":"validation_generator = test_datagen.flow_from_directory(test_data_path,\n                                                        target_size=size,\n                                                        batch_size=batch_size,\n                                                        class_mode='categorical',\n                                                        shuffle=False)\n\nY_pred = simple_model.predict_generator(validation_generator, num_of_test_samples \/\/ batch_size+1)\ny_pred = np.argmax(Y_pred, axis=1)\n\n\nc=0\nfor i in range(len(validation_generator.classes)):\n  if validation_generator.classes[i]==y_pred[i]:\n    c+=1\n    \nprint(\"Accuracy\")\nprint(c\/len(y_pred))\n\nconf_mx=confusion_matrix(validation_generator.classes, y_pred)\nprint('Confusion Matrix')\nprint(conf_mx)","2983198b":"def plot_confusion_matrix(matrix):\n    fig = plt.figure(figsize=(8,8))\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(matrix)\n    fig.colorbar(cax)","af5e7fe8":"plot_confusion_matrix(conf_mx)","5d302745":"row_sums = conf_mx.sum(axis=1, keepdims=True)\nnorm_conf_mx = conf_mx \/ row_sums\n\nnp.fill_diagonal(norm_conf_mx, 0)\n\nplot_confusion_matrix(norm_conf_mx)","56a95af8":"#Shows errors of prediction between two classes, limited to n images\n#Not symetric, will show images from class cl1 predicted as images from class cl2\ndef errors(predictions, cl1, cl2, n):\n  _,ax = plt.subplots(n\/\/5,5, figsize = (15,15)) \n  c=0\n  for k in range(len(validation_generator.classes)):\n    if validation_generator.classes[k]==cl1 and predictions[k]==cl2 and c<n:\n      path = validation_generator.filepaths[k]\n      image = cv2.imread(path)\n      image = cv2.resize(image,size)\n      i=c\/\/5\n      j=c%5\n      ax[i,j].imshow(image)\n      ax[i,j].set_title('predicted : '+label_to_class[cl2])\n      ax[i,j].axis('off')\n      c+=1","669e7cac":"#Images of buildings classified as streets\nerrors(y_pred,0,5,10)","7b5b04e3":"#Images of streets classified as buildings\nerrors(y_pred,5,0,10)","5b6fb751":"#Images of glaciers classified as mountains\nerrors(y_pred,2,3,10)","2ac06a92":"#Images of mountains classified as glaciers\nerrors(y_pred,3,2,10)","efa0d9c0":"train_data_path = '..\/input\/seg_train\/seg_train'\ntest_data_path = '..\/input\/seg_test\/seg_test'\n\nsize=(150,150)\nepochs = 30\nbatch_size = 32\nnum_of_train_samples = 14000\nnum_of_test_samples = 3000\n\n#Image Generator\ntrain_datagen = ImageDataGenerator(rescale=1. \/ 255,\n                                   rotation_range=40,\n                                   width_shift_range=0.2,\n                                   height_shift_range=0.2,\n                                   shear_range=0.2,\n                                   zoom_range=0.2,\n                                   horizontal_flip=True,\n                                   fill_mode='nearest')\n\ntest_datagen = ImageDataGenerator(rescale=1. \/ 255)\n\n\ntrain_generator = train_datagen.flow_from_directory(train_data_path,\n                                                    target_size=size,\n                                                    batch_size=batch_size,\n                                                    class_mode='categorical')\n\nvalidation_generator = test_datagen.flow_from_directory(test_data_path,\n                                                        target_size=size,\n                                                        batch_size=batch_size,\n                                                        class_mode='categorical',\n                                                        shuffle=False)","63c6c75e":"model = Sequential()\n\nmodel.add(Conv2D(200,kernel_size=(3,3),activation='relu',input_shape=(150,150,3)))\n\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(200,kernel_size=(3,3),activation='relu'))\nmodel.add(MaxPooling2D(5,5))\n\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(150,kernel_size=(3,3),activation='relu'))\n\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(150,kernel_size=(3,3),activation='relu'))\n\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(100,kernel_size=(3,3),activation='relu'))\n\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(50,kernel_size=(3,3),activation='relu'))\nmodel.add(MaxPooling2D(5,5))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(200,activation='relu'))\nmodel.add(Dense(100,activation='relu'))\nmodel.add(Dense(50,activation='relu'))\nmodel.add(Dropout(rate=0.5))\n\nmodel.add(Dense(num_classes,activation='softmax'))\n\nmodel.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(0.0001),metrics=['accuracy'])\n\nmodel.summary()\n","3497b401":"training_1 = model.fit_generator(train_generator,\n                                 steps_per_epoch=num_of_train_samples \/\/ batch_size,\n                                 epochs=epochs,\n                                 validation_data=validation_generator,\n                                 validation_steps=num_of_test_samples \/\/ batch_size)","ec4da5f5":"plt.plot(training_1.history['acc'])\nplt.plot(training_1.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\nplt.plot(training_1.history['loss'])\nplt.plot(training_1.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","075e00c5":"validation_generator = test_datagen.flow_from_directory(test_data_path,\n                                                        target_size=size,\n                                                        batch_size=batch_size,\n                                                        class_mode='categorical',\n                                                        shuffle=False)\n\nY_pred_1 = model.predict_generator(validation_generator, num_of_test_samples \/\/ batch_size+1)\ny_pred_1 = np.argmax(Y_pred_1, axis=1)\n\nc=0\nfor i in range(len(validation_generator.classes)):\n  if validation_generator.classes[i]==y_pred_1[i]:\n    c+=1\n    \nprint(\"Accuracy\")\nprint(c\/len(y_pred_1))\n\nconf_mx_1=confusion_matrix(validation_generator.classes, y_pred_1)\nprint('Confusion Matrix')\nprint(conf_mx_1)","04bfd400":"plot_confusion_matrix(conf_mx_1)","1112e5d2":"row_sums_1 = conf_mx_1.sum(axis=1, keepdims=True)\nnorm_conf_mx_1 = conf_mx_1 \/ row_sums_1\n\nnp.fill_diagonal(norm_conf_mx_1, 0)\n\nplot_confusion_matrix(norm_conf_mx_1)","2a21d3d1":"errors(y_pred_1,2,3,10)","6731d049":"errors(y_pred_1,5,0,10)","e6bc6e0d":"train_data_path = '..\/input\/seg_train\/seg_train'\ntest_data_path = '..\/input\/seg_test\/seg_test'\n\nsize=(150,150)\nepochs = 20\nbatch_size = 32\nnum_of_train_samples = 14000\nnum_of_test_samples = 3000\n\n#Image Generator\ntrain_datagen = ImageDataGenerator(rescale=1. \/ 255,\n                                   rotation_range=40,\n                                   width_shift_range=0.2,\n                                   height_shift_range=0.2,\n                                   shear_range=0.2,\n                                   zoom_range=0.2,\n                                   horizontal_flip=True,\n                                   fill_mode='nearest')\n\ntest_datagen = ImageDataGenerator(rescale=1. \/ 255)\n\n\ntrain_generator = train_datagen.flow_from_directory(train_data_path,\n                                                    target_size=size,\n                                                    batch_size=batch_size,\n                                                    class_mode='categorical')\n\nvalidation_generator = test_datagen.flow_from_directory(test_data_path,\n                                                        target_size=size,\n                                                        batch_size=batch_size,\n                                                        class_mode='categorical',\n                                                        shuffle=False)","026158f5":"from keras.applications.resnet50 import ResNet50","16f818d9":"num_classes = 6\n\ntransfer_model = Sequential()\n\n# 1st layer as the lumpsum weights from resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n# NOTE that this layer will be set below as NOT TRAINABLE, i.e., use it as is\ntransfer_model.add(ResNet50(include_top = False, pooling = 'avg', weights = 'imagenet'))\n\n# 2nd layer as Dense for 2-class classification, i.e., dog or cat using SoftMax activation\ntransfer_model.add(Dense(num_classes, activation = 'softmax'))\n\n# We choose to train the ResNet model\ntransfer_model.layers[0].trainable = True\n\ntransfer_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(0.0001),metrics=['accuracy'])\n\ntransfer_model.summary()","4a66b1e2":"#The network has already been trained to recognize patterns, it will converge much faster\n#This is why we only train it during 20 epochs\ntraining_2 = transfer_model.fit_generator(train_generator,\n                                          steps_per_epoch=num_of_train_samples \/\/ batch_size,\n                                          epochs=epochs,\n                                          validation_data=validation_generator,\n                                          validation_steps=num_of_test_samples \/\/ batch_size)","1616d348":"plt.plot(training_2.history['acc'])\nplt.plot(training_2.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\nplt.plot(training_2.history['loss'])\nplt.plot(training_2.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","d5a63ccf":"validation_generator = test_datagen.flow_from_directory(test_data_path,\n                                                        target_size=size,\n                                                        batch_size=batch_size,\n                                                        class_mode='categorical',\n                                                        shuffle=False)\n\nY_pred_2 = transfer_model.predict_generator(validation_generator, num_of_test_samples \/\/ batch_size+1)\ny_pred_2 = np.argmax(Y_pred_2, axis=1)\n\nc=0\nfor i in range(len(validation_generator.classes)):\n  if validation_generator.classes[i]==y_pred_2[i]:\n    c+=1\n    \nprint(\"Accuracy\")\nprint(c\/len(y_pred_2))\n\nconf_mx_2=confusion_matrix(validation_generator.classes, y_pred_2)\nprint('Confusion Matrix')\nprint(conf_mx_2)","e6aab11d":"plot_confusion_matrix(conf_mx_2)","b93b0e50":"row_sums_2 = conf_mx_2.sum(axis=1, keepdims=True)\nnorm_conf_mx_2 = conf_mx_2 \/ row_sums_2\n\nnp.fill_diagonal(norm_conf_mx_2, 0)\n\nplot_confusion_matrix(norm_conf_mx_2)","fca35dbc":"errors(y_pred_2,0,5,15)","ce5e4ee7":"errors(y_pred_2,2,3,15)","e6045349":"This matrix shows us that the most common confusions are between **streets and buildings**, there are also confusions between **glaciers and mountains**.\n\n**Let's look at some badly classified images**","b4cb1216":"## Creating the model","0f6d02f3":"**The results are way better.**","f84aa991":"We will represent some arbitrary images and their labels, since we shuffled the dataset, we can simply show the 20 first images.","54a3b10d":"## Trying a more powerful model","5fdc7797":"Some of these images are hard to classify, even for a human.\n\nHowever there are very few errors with the images of forests.\n\n**A solution would be to train a One v One classifier to recognize a mountain from a glacier and a street from a building.**","751da617":"## Importing libraries","640731a2":"As you can see, most of the images wrongly classified are ambiguous. The images of buildings also present streets and the images of glaciers also have mountains.\n\n**This ambiguity in the data makes it very hard to get better performances.**","2ef0d619":"## Inspection of the data","e3bf6d7b":"**There are still some confusions between the buildings and the streets as well as between the glaciers and the mountains.**","adea4bc8":"**This network has the same difficulties to distinguish those specific classes.**","d2e5d0a3":"**The confusion matrix will show us the most frequent mistakes made by this classifier.**","a2f2e8f8":"**First, let's try a simple model with 3 convolutional layers and 2 fully connected layers**","7aa9019a":"There are still some errors between the previous problematic classes (0 vs 5 and 2 vs 3).","3bb07d16":"The goal is to classify images of size 150x150 into 6 disctinct categories : 'buildings', 'forest', 'glacier', 'mountain', 'sea' and 'street'.\n\nThe data was provided by Intel.","f4cc11c2":"## Trying transfer learning","6866770e":"**The results are promising considering the complexity of the task.**","c88efcc6":"The huge size of the network led to an important overfitting.\n\n**The solutions to reduce overfitting are the augmentation of the dataset or the addition of regularization (with dropout layers or L1\/L2 regularization)**","6edbfbcd":"## Analysis of the predictions","33e8760d":"**The model does not seem to overfit, it has a better accuracy on the validation set because the data augmentation creates images more difficult to classify.**","21b5cece":"**There is an improvement from the previous model.**","b91f0f74":"There is no obvious overfitting.","3960d290":"**This new model contains 6 convolutional layers, it will detect more complex patterns.**","3c70403b":"## Loading the images and looking at some data","960f82ef":"Using transfer learning with a huge trained CNN alows us to detect very complex patterns with an important training speed, as we do not need to massively change the weights in the layers.\n\n**I chose not to freeze the model because the pattern detection in ResNet50 is not adapted to our classification problem.\nThe network was trained on image net which is a very different problem.**\n\nFreezing the weights leads to a massive overfitting (more than 90% accuracy on the training set with less than 20% on the validation set).","fd551a80":"## Preparation of the data generators","b8e61026":"# Intel Image Classification","e54025a8":"## Training the model"}}