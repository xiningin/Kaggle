{"cell_type":{"efa2c0de":"code","ae53bdd2":"code","d237d77c":"code","0da7e6a1":"code","036f8bc1":"code","136083fe":"code","e148a329":"code","80b351b6":"code","6934adeb":"code","ea0bd8b3":"code","792fd848":"code","b1af12e5":"code","8cf891a1":"code","0cf2c4f1":"code","a3901728":"code","c3df04d2":"code","a86b1220":"code","abce09ac":"code","8cae976e":"code","0a52cba3":"code","974596a7":"code","384291e6":"code","e8fa9d88":"code","02862d6f":"code","d855df5b":"code","ba20d0c6":"code","8f74bbaf":"code","c5e9f9eb":"code","802f59ff":"code","e51a097e":"code","2ec00eea":"code","d0762ad9":"code","5146b6b8":"code","4dcc22a2":"code","b29189ec":"code","5d0c1ed5":"code","6a00d48c":"code","9a145276":"code","65df57f0":"code","b607ab3d":"code","80f735e1":"markdown","8cd3c899":"markdown","42e6488e":"markdown","b558cd40":"markdown","c405a34f":"markdown","0863bb66":"markdown","812626c1":"markdown","254a3bf4":"markdown","c88e6ebe":"markdown","44dd6eb4":"markdown","fb2b2e0d":"markdown","c9a5b22d":"markdown","3e9c95d8":"markdown","a51c022a":"markdown","0a2d6e61":"markdown","e8ed08a2":"markdown","a501ce44":"markdown","8c78936a":"markdown","61901bcb":"markdown","5db91189":"markdown","308ff967":"markdown","dabc028f":"markdown","ef0fcd62":"markdown","bd793950":"markdown","94f82552":"markdown"},"source":{"efa2c0de":"#! pip install branca==0.4.1 #0.3.1\n#! pip install wordcloud\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport os\nimport string\nimport re\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport branca.colormap as cm\nfrom mpl_toolkits.basemap import Basemap\nimport requests\nimport folium\nfrom folium import plugins\nfrom folium.plugins import HeatMap\nimport branca.colormap\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk import pos_tag, ne_chunk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom wordcloud import WordCloud\nfrom tqdm import tqdm, notebook\n%matplotlib inline\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.max_colwidth', None)\npd.set_option('display.width', None)","ae53bdd2":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d237d77c":"df = pd.read_csv(os.path.join(dirname, filename))\ndf.head()","0da7e6a1":"df.info()","036f8bc1":"df.date.min(), df.date.max()","136083fe":"# There are no retweets in the data\ndf['is_retweet'].value_counts()","e148a329":"# ~15% tweets have been tweeted by verified users\ndf['user_verified'].value_counts(normalize=True)","80b351b6":"df.describe()","6934adeb":"sns.heatmap(df.corr())","ea0bd8b3":"# Make a copy of dataframe before making any changes\ntweets = df.copy()","792fd848":"# Convert date columns to datetime data type from object\ntweets['date'] = pd.to_datetime(tweets['date'])\ntweets['user_created'] = pd.to_datetime(tweets['user_created'])\ntweets['date_ext'] = tweets['date'].dt.date","b1af12e5":"# Take care of nulls in location and description\ntweets.user_location.fillna('Unknown', inplace=True)\ntweets.user_description.fillna('Unknown', inplace=True)\ntweets.source.fillna('Unknown', inplace=True)\ntweets.hashtags.fillna('None', inplace=True)","8cf891a1":"# Verify\ntweets.info()","0cf2c4f1":"# Attempt tp clean the location column. There are many other business rules that can be applied to \n# improve the accuracy of different cases. We can also use regular expressions\n\n# If both country and city is mentioned\n\ntweets[\"country\"] = tweets.user_location.apply(lambda x: x.split(\",\")[-1].strip() \\\n                                            if (\",\" in x) else x)\ntweets[\"city\"] = tweets.user_location.apply(lambda x: x.split(\",\")[0].strip() \\\n                                            if (\",\" in x) else x)\n\n# Replacing 2 digit US states with USA except UK\ntweets[\"country\"] = tweets.country.apply(lambda x: 'USA' if len(x.lower().strip())<3 and x!='uk' else x)\n\n# Standarizing case senstive cases\ntweets[\"country\"] = tweets.country.apply(lambda x: 'USA' if x.lower().strip() in \\\n                                      (\"united states, usa, us\") else x)\ntweets[\"country\"] = tweets.country.apply(lambda x: 'India' if x.lower().strip() in \\\n                                      (\"india\") else x)\n\n# Removing countries from city column\ntweets[\"city\"] = tweets.city.apply(lambda x: 'Unknown' if x.lower() in \\\n                                   ['india', 'united states', 'united kingdom', 'uk', 'usa', 'us'] \\\n                                   else x)","a3901728":"# Taking top 100 countries and cities excluding the unknown at index 0\ntop100_count = tweets.country.value_counts(sort=True, ascending=False)[1:101]\ntop100_count = pd.DataFrame(top100_count)\n\ntop100_city= tweets.city.value_counts(sort=True, ascending=False)[1:101]\ntop100_city = pd.DataFrame(top100_city)","c3df04d2":"def get_coordinates(input_type, name, output_as='center'):\n    \"\"\"\n    Function to get coordinates of country\/ city\n\n    Attributes\n    ----------\n    input_type : str\n        Pass 'country' or 'city' to generate the respective URL\n    name : str\n        Name of the country or city we need the coordinates for\n    output_as : str\n        Pass 'center' or 'boundingbox' depending upon what coordinates type to fetch\n    \n    Methods\n    -------\n        Returns the coordinates of the country or city\n    \"\"\"\n    # create url\n    url = '{0}{1}{2}'.format('http:\/\/nominatim.openstreetmap.org\/search?'+input_type+'='\\\n                             ,name,'&format=json&polygon=0')\n    response = requests.get(url)\n    try:\n        response = response.json()[0]\n        # parse response to list\n        if output_as == 'center':\n            lst = [response.get(key) for key in ['lat','lon']]\n            output = [float(i) for i in lst]\n        if output_as == 'boundingbox':\n            lst = response[output_as]\n            output = [float(i) for i in lst]\n        return output\n    \n    except (IndexError, ValueError):\n        # this will log the whole traceback\n        return [0,0]","a86b1220":"# Get the latitudes and longitudes for the top 100 countries and cities\nlat_long_count = [get_coordinates(\"country\",coun) for coun in top100_count.index]\nlat_long_city = [get_coordinates(\"city\",city) for city in top100_city.index]","abce09ac":"# Call the get_coodinates() and drop the duplicates from dataframe\ndef top100_df(input_type,df,lat_long):\n    # merge lat and long with original dataframe\n    for i, pair in zip(df.index, lat_long):\n        df.loc[i,'lat'] = pair[0]\n        df.loc[i,'long'] = pair[1] \n    # reset the index as country names\n    df.reset_index(level=0, inplace=True)\n    if input_type==\"country\":\n        df.rename(columns={\"country\":\"# of tweets\",\"index\":\"country\"}, inplace=True)\n    elif input_type==\"city\":\n        df.rename(columns={\"city\":\"# of tweets\",\"index\":\"city\"}, inplace=True)\n    # drop the countries\/ cities with unidetified and duplicated latitudes and longitudes\n    df.drop_duplicates(subset=['lat','long'],inplace=True)\n    return df","8cae976e":"# Call the top100_df() to finalize the country and city dataframes \ntop100_count = top100_df(\"country\",top100_count,lat_long_count)\ntop100_city = top100_df(\"city\",top100_city,lat_long_city)","0a52cba3":"# Tweets trend by Country and City\nfig, (ax1, ax2) = plt.subplots(2, 1, squeeze=True, figsize=(16,12))\n\nax1.fill_between(top100_count.country, top100_count['# of tweets'], color=\"skyblue\", alpha=0.4)\nax1.plot(top100_count.country, top100_count['# of tweets'], color=\"Slateblue\", alpha=0.8)\nax1.set_title(\"\\n\\n Tweets by Country\", fontsize=16)\n\nax2.fill_between(top100_city.city, top100_city['# of tweets'],  color=\"aqua\", alpha=0.4)\nax2.plot(top100_city.city, top100_city['# of tweets'], color=\"darkblue\", alpha=0.8)\nax2.set_title(\"\\n\\n Tweets by City\", fontsize=16)\n\nfor ax in fig.axes:\n    ax.tick_params(labelrotation=90)\n    \nfig.tight_layout()\nfig.show()","974596a7":"# Create a heatmap using folium\ndef color(magnitude):\n    if magnitude>=2000:\n        col='red'\n    elif (magnitude>=500 and magnitude<2000):\n        col='beige'\n    elif magnitude<500:\n        col='green'\n    return col\n\ndef generateBaseMap(input_type,df,default_location=[40.693943, -73.985880], default_zoom_start=2):\n    \"\"\"\n    Function to generate the heatmap\n\n    Attributes\n    ----------\n    input_type : str\n        Pass 'country' or 'city' to generate the respective heatmap\n    df : str\n        Name of the dataframe having the country\/city coordinates and other details\n    default_location : int\n        Pass the default location for the displayed heatmap\n    default_zoom_start: int\n        Pass the default zoom for the displayed heatmap\n    \n    Methods\n    -------\n        Returns the base_map\n    \"\"\"\n        \n    base_map = folium.Map(location=default_location, control_scale=True, zoom_start=default_zoom_start)\n    marker_cluster = plugins.MarkerCluster().add_to(base_map)\n    \n    HeatMap(data=df[['lat','long']].values.tolist(),radius=20,max_zoom=13).add_to(base_map)\n    for lat,lan,tweet,name in zip(df['lat'],df['long'],df['# of tweets'],df.iloc[:,0]): \n        # Marker() takes location coordinates as a list as an argument \n        folium.Marker(location=[lat,lan],popup = [name,tweet], \n                      icon= folium.Icon(color=color(tweet), \n                      icon_color='white', icon='twitter', prefix='fa')).add_to(marker_cluster)\n        \n    #specify the min and max values of your data\n    min, max = df['# of tweets'].min(), df['# of tweets'].max()\n    colormap = cm.LinearColormap(colors=['green','beige','red'], vmin=min,vmax=max)\n   \n    colormap.caption = input_type.title() +' distribution of COVID-19 tweets'\n    colormap.add_to(base_map)\n    return base_map","384291e6":"generateBaseMap('country',top100_count)","e8fa9d88":"generateBaseMap('city',top100_city)","02862d6f":"# Compare the tweet source\ntop10_source = tweets.source.value_counts().nlargest(10)\nfig = plt.figure(figsize = (20,5))\nplt.bar(top10_source.index, top10_source.values, color=\"lightblue\", edgecolor=\"darkblue\")\nplt.ylabel('# of Tweets')\nplt.title(\"Tweets by source\", fontsize=16);","d855df5b":"# Most trended hashtags\ntop10_hashtags = tweets.hashtags.str.lower().value_counts().nlargest(10)\n# initiate the figure with it's size\nfig = plt.figure(figsize = (10,5))\nplt.barh(top10_hashtags.index, top10_hashtags.values)\nplt.xlabel('# of Tweets')\nplt.title(\"Tweets by hashtags\", fontsize=16);","ba20d0c6":"# Daily tweet trend\ndaily_tweets = tweets.groupby(['date_ext'])['text'].count()\n\nfig = plt.figure(figsize = (15,5))\nplt.plot(daily_tweets.index,daily_tweets.values)\nplt.title('Daily Tweets\\' Trend', fontsize=16)\nplt.xlabel('Dates')\nplt.ylabel('# of Tweets')\nplt.show()","8f74bbaf":"# Top 10 twitter accounts being followed\ntop10users = tweets.groupby(by=[\"user_name\"])['user_followers'].max().sort_values(ascending=False)[:10]\ntop10users.to_frame().style.bar()","c5e9f9eb":"# Convert everything other than a-z, A-Z, 0-9 to space and remove the link from tweets\ntweets['clean_tweet'] = tweets['text'].apply(lambda x: re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)\",\" \", x))","802f59ff":"def createWordCloud(input_type, text):\n    \"\"\"\n    Function to generate the wordcloud\n\n    Attributes\n    ----------\n    input_type : str\n        Pass 'words' or 'entities' to update the chart title based on the text passed\n    text : str\n        Name of the string text to make the wordcloud\n    \n    Methods\n    -------\n        Returns the wordcloud\n    \"\"\"\n    wordcloud = WordCloud(width = 1000, height = 600, \n                      #colormap = 'Paired',\n                      background_color ='white',\n                      collocations = False,\n                      stopwords=stop_words\n                     ).generate(text)\n\n    plt.figure(figsize = (12, 12), facecolor = None)\n    plt.title(\"Most common \"+ input_type +\" in the tweets \\n\", fontsize=20, color='Black')\n    plt.imshow(wordcloud, interpolation='bilinear') \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n    plt.show()","e51a097e":"# Create a wordcloud for most popular words in tweets\ntext = '' \nstop_words = set(stopwords.words(\"english\"))\nfor row in tweets['clean_tweet']:\n    # typecaste each row to string and split it to get tokens\n    tokens = str(row).split()\n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower()\n    text += \" \".join(tokens)+\" \"\n\ncreateWordCloud(\"words\",text)","2ec00eea":"# We are using Compound score to detect the tweet sentiment which is a metric that calculates the sum of\n# all the lexicon ratings which have been normalized between \n# -1(most extreme negative) and +1 (most extreme positive)\n# positive: (compound score >= 0.05), negative : (compound score <= -0.05), neutral otherwise\nsid = SentimentIntensityAnalyzer()\nfor index, row in tqdm(tweets.iterrows()): #tqdm \n    ss = sid.polarity_scores(row['clean_tweet'])\n    if ss['compound'] >= 0.05 : \n        tweets.at[index,'sentiment'] = \"Positive\"\n    elif ss['compound'] <= - 0.05 : \n        tweets.at[index,'sentiment'] = \"Negative\"\n    else : \n        tweets.at[index,'sentiment'] = \"Neutral\"","d0762ad9":"# Show distribution of tweet sentiments\nsentiment_dist = tweets.sentiment.value_counts()\n\nplt.pie(sentiment_dist, labels=sentiment_dist.index, explode= (0.1,0,0),\n        colors=['yellowgreen', 'gold', 'lightcoral'],\n        autopct='%1.1f%%', shadow=True, startangle=140)\nplt.title(\"Tweets\\' Sentiment Distribution \\n\", fontsize=16, color='Black')\nplt.axis('equal')\nplt.tight_layout()\nplt.show()","5146b6b8":"# Function to filter top 10 tweets by sentiment\ndef top10AccountsBySentiment(sentiment):\n    df = tweets.query(\"sentiment==@sentiment\")\n    top10 = df.groupby(by=[\"user_name\"])['sentiment'].count().sort_values(ascending=False)[:10]\n    return(top10)","4dcc22a2":"# Top 10 tweets by each sentiment\ntop10_pos = top10AccountsBySentiment(\"Positive\")\ntop10_neg = top10AccountsBySentiment(\"Negative\")\ntop10_neu = top10AccountsBySentiment(\"Neutral\")\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, squeeze=True, figsize=(16,8))\nfig.suptitle('Top 10 Twitter Accounts \\n', fontsize=20)\n\nax1.barh(top10_pos.index, top10_pos.values, color='yellowgreen')\nax1.set_title(\"\\n\\n Positive Tweets\", fontsize=16)\n\nax2.barh(top10_neg.index, top10_neg.values, color='lightcoral')\nax2.set_title(\"\\n\\n Negative Tweets\", fontsize=16)\n\nax3.barh(top10_neu.index, top10_neu.values, color='gold')\nax3.set_title(\"\\n\\n Neutral Tweets\", fontsize=16);\n\nfig.tight_layout()\nfig.show()","b29189ec":"# Sentiment of top 10 favoritized tweets\ntop10tweets = tweets.nlargest(10, ['user_favourites'])[['user_name','clean_tweet','user_verified','sentiment']]\ntop10tweets.groupby(by=['user_name','sentiment'])['clean_tweet'].count().to_frame().style.bar()","5d0c1ed5":"top10tweets","6a00d48c":"# Create dictionary of entities and their frequency in the tweets then create a wordcloud\ntt = TweetTokenizer()\nentities={}\n\nfor sent in tqdm(tweets.clean_tweet):\n    for chunk in ne_chunk(pos_tag(tt.tokenize(sent)), binary=True):\n        if hasattr(chunk, 'label'):\n            if chunk[0][0] in entities.keys():\n                entities[chunk[0][0]] = entities[chunk[0][0]]+1\n            else:\n                entities[chunk[0][0]]=1\n                \n#sorted by value, return a list of tuples   \ntop50_entities = sorted(entities.items(), key=lambda x: x[1], reverse=True)[:50]\nentities_text = \" \".join([(k + \" \")*v for k,v in dict(top50_entities).items()])","9a145276":"# Plot the most frequent entities to show the frequency trend line\ncreateWordCloud(\"entities\",entities_text)\n\nx, y = zip(*top50_entities) # unpack a list of pairs into two tuples\nplt.figure(figsize=[15,8])\nplt.plot(x, y)\nplt.xticks(rotation=90)\nplt.title(\"Most frequent entities mentioned in tweets\", fontsize=16)\nplt.show();","65df57f0":"# Tweets that has Trump mentioned in them\ntrump_tweets = tweets[tweets['clean_tweet'].str.contains(\"realDonaldTrump|Trump\")]\ntrump_tweets.query(\"sentiment=='Negative'\")['clean_tweet'].tail(10)","b607ab3d":"trump_tweets.sentiment.value_counts(normalize=True)","80f735e1":"####  A total of ~70k tweets have a positive sentiment making the largest pie in the chart with ~40% tweets.","8cd3c899":"## Detailed Analysis","42e6488e":"#### We see that US, India, and England are the top 3 countries with the hightest tweets with USA having significantly higher tweets (~20%). The curve is almost flattened beyond the top 3 countries with ~2k (or less) tweets per country. <br> The top 5 cities with highest tweets are London, New Delhi, New York, Mumbai, Washington with each having 2k-3k tweets.","b558cd40":"#### In terms of entities that have been most talked about, as expected, COVID tops the list again. Interestingly, other entities to notice are Donald Trump (3rd and 7th on the list), Joe Biden, Boris Johnson, Congress, WHO, CDC. <br> We also see a few countries like India, US, China, Russia, and cities\/ states like Odisha (a city in India), Florida, Texas etc. that have been quite talked about.","c405a34f":"### Create WordCloud","0863bb66":"## Exploration Data Analysis (EDA)","812626c1":"### Check Most Frequent Entities","254a3bf4":"#### We see quite a spike in number of tweets in the last week of July. This could possibly be because of the highest number of corona cases during that time as reported by Worldometers. July 24, had the highest daily corona cases till date with ~290k globally and ~80k in the US. This could probably explain the high volume of tweets in that week and especially on July 25th. \n[Source: Worldometers daily cases (US)](http:\/\/www.worldometers.info\/coronavirus\/country\/us\/)\n![image.png](attachment:image.png)\n","c88e6ebe":"#### Out of all the tweets that mention 'Trump', almost 50% have a negative tone. When inspected most of these tweets had people talking about the increasing corona cases. Some of the tweets talk about country's inefficiency in promoting awareness about coronavirus, having faulty ventilators, lack of much needed preventive measure etc.","44dd6eb4":"### Looking into tweets talking about 'Trump'","fb2b2e0d":"# COVID19 Tweets\n\n## We are using a twitter data set that has been collected using Twitter API with a high-frequency hashtag (#covid19). We are using a sample set of ~180k tweets for the period of July 24 to August 30. More details on the data and python script can be accessed on [Kaggle](http:\/\/www.kaggle.com\/gpreda\/covid19-tweets).\n\n## Using this data, we have tried to answer 10 business questions:\n>#### 1. Which countries are most people tweeting from? <br> 2. What sources have people used more commonly to tweet <br> 3. What are the common hash tags used in these tweets? <br> 4. Is there any trend in the tweets on a daily basis? <br> 5. Who are the most followed people\/ accounts amongst these tweets? <br> 6. What are the most common words in the tweets? <br> 7. What is the sentiment in the tweets? <br> 8. Which accounts mostly have a positive and negative tweets? <br> 9. Does most favoritized tweets have any sentiment pattern\/ trend? <br> 10. Which entities (people, locations, organizations) have been talked about the most.","c9a5b22d":"## References\n- https:\/\/stackoverflow.com\/questions\/44173624\/how-to-apply-nltk-word-tokenize-library-on-a-pandas-dataf**rame-for-twitter-data\n- https:\/\/basemaptutorial.readthedocs.io\/en\/latest\/plotting_data.html#text\n- https:\/\/zapcircle.net\/geomapping-with-python\/\n- https:\/\/matplotlib.org\/basemap\/users\/geography.html\n- https:\/\/queirozf.com\/entries\/add-labels-and-text-to-matplotlib-plots-annotation-examples\n- https:\/\/colab.research.google.com\/drive\/1HJB7UGj7YuUEJi-cKZRAr3O4Dlym2KrP#scrollTo=0H-uRYF1iIjS\n- https:\/\/stackoverflow.com\/questions\/56876620\/unsure-how-to-use-colormap-with-folium-marker-plot\n- https:\/\/www.geeksforgeeks.org\/generating-word-cloud-python\/\n- https:\/\/www.nltk.org\/howto\/sentiment.html\n- https:\/\/www.geeksforgeeks.org\/python-sentiment-analysis-using-vader\/\n    ","3e9c95d8":"#### In the heatmaps above, we can see a high concentration in regions with higher tweets. As we zoom in the heatmap, we see the number of tweets for each country and city. The color (red,yellow,green) of icon tells the intensity of tweets from high to low and as we hover over each icon, we see the name of country\/ city and it's number of tweets.","a51c022a":"#### As expected, ~70% of the tweets mention either covid19 or coronavirus as the hastag.","0a2d6e61":"#### 32% of people have tweeted using the Web App, closely followed by Android users with 22%, and iPhone users with 20%.","e8ed08a2":"## Check sentiment of tweets\n### Preprocessing text","a501ce44":"## Closing thoughts\n\n1.  US (New York, Washinton), India (New Delhi, Mumbai), and England (London) have the hightest tweets. These are also the most populated and metropolitan cities clearly indicating that people in these cities are more active on Twitter.\n2. People using twitter on mobile are 1.5x of the ones using web when combined the Android and iPhone users collectively making ~45% of tweets.\n3. Looking at the tweets' trend, people have tweeted more following a spike in covid19 cases especially when cases on June 24 were the highest till date, we see a huge spike in number of tweets on June 25.\n4. As expected, most of the news channels like CNN, National Geographic, CGTN, NDTV, and Times of India are the top 5 twitter accounts to be followed with CNN having over 50m followers, and National Geographic having ~25m followers.\n5. While doing text analysis, we captured the words like covid19, coronavirus, pandemic, vaccine, death, mask, etc. which were mostly talked about.\n6. Overall there were more positive tweets (~40%) than negative or neutral, indicating people still have great hopes for the world to become normal soon again. When analyzing positive tweets further, we found 'GlobalPandemic.NET' leads the list for having the highest number of positive tweets while 'Open Letters' leads the negative tweet list and 'Coronavirus Updates' for neutral tweets.\n7. Amongst the top 10 favorited tweets, tweeted by three (non-verified) accounts pop-up Patty, Paolo, and Chelsea:\n    * The negative tweets showcased concerns about the rising COVID-19 cases and US' inability to contain it.\n    * The positive tweets talked about how the situation might improve once a vaccine is made.\n    * The neutral tweets were mostly an update on the number of covid19 cases.\n8. Apart from the most obvious word 'COVID', the other entities to be most talked about were Donald Trump, Joe Biden, Boris Johnson, Congress, WHO, CDC. A few countries like India, US, China, Russia, and cities\/ states like Odisha (a city in India), Florida, Texas etc. were very frequent mentioned as well.","8c78936a":"### Calculate Sentiments","61901bcb":"## Import Libraries and Data Load","5db91189":"#### Amongst the top favoritized tweets, three (non-verified) accounts pop-up Patty, Paolo, and Chelsea. The negative tweets here showcase concerns about the rising COVID-19 cases and US' inability to contain it. On the other hand, there are some positive tweets talking about how the situation might improve once a vaccine is made. The other neutral tone tweets are mostly an update on the number of covid19 cases.","308ff967":"### I hope you enjoyed my kernel. You can use this dataset for any text based analysis. Can't wait to see your work! Please upvote if you like my work \ud83d\ude03","dabc028f":"#### As expected, the most common words that pop up in these tweets are all corona related viz. covid19, coronavirus, pandemic, vaccine, death, mask, etc.","ef0fcd62":"#### CNN, and National Geographic are the top 2 twitter accounts to be followed the most with CNN having over 50m followers, and National Geographic having ~25m followers. Next in line are CGTN, NDTV, and Times of India with each having slightly over 13m followers.","bd793950":"## Plot heatmap to see the geographical distribution based on number of tweets","94f82552":"#### 'GlobalPandemic.NET' has the hightest number of positive tweets while 'Open Letters' leads the list for having maximum negative tweets and 'Coronavirus Updates' for neutral tweets."}}