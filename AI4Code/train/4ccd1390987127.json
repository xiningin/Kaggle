{"cell_type":{"8d325c82":"code","92fa9a00":"code","65a57824":"code","503a28cb":"code","21897db7":"code","4897ad97":"code","5f890042":"code","146edd8e":"code","b18c8b47":"code","6299edc1":"code","c82a5484":"code","ede0ccd8":"code","6a3aa9ac":"code","2cdc26fa":"code","f780f437":"code","1987bf0d":"code","45464e4c":"code","5dd4cb30":"code","f4fb3321":"code","3bf05227":"code","fb1da13c":"code","e1570f25":"code","53626710":"code","b8e33c23":"code","ef159ac5":"code","d0caffcf":"code","fd852467":"code","f94010ae":"code","06340b24":"code","2654f12a":"code","b9be17da":"code","ac811a93":"code","265fc8c0":"code","5b8a0e6f":"code","61679ee1":"code","3d7f6815":"code","c87b61c1":"code","b40df3c0":"code","ddda04a7":"code","6cba3726":"code","69ebbe63":"code","0ec62f4c":"code","01589b92":"code","490b3a50":"code","654516c9":"markdown","8b9866b3":"markdown","ffbc2c07":"markdown","73379705":"markdown","47568779":"markdown","6d7c49f1":"markdown","c699147f":"markdown","957f69e8":"markdown","6b4c57f6":"markdown","987d668c":"markdown","bcf3ea65":"markdown","63ab7822":"markdown","6626edc7":"markdown","7266e3aa":"markdown","01063f8b":"markdown","f811b3bb":"markdown","f630b6dd":"markdown","7950ebf8":"markdown","6d346547":"markdown","2f110e36":"markdown","103c8e61":"markdown","524f48a1":"markdown","e3d0ac91":"markdown","e3a59d26":"markdown","db0fe2bf":"markdown","8723def0":"markdown","78e81424":"markdown","08e18819":"markdown","7a5ef676":"markdown","260dc35c":"markdown","499971da":"markdown","f30055a0":"markdown","05e11342":"markdown","4371fc0e":"markdown","ca1bb146":"markdown","928863b8":"markdown","04e499ad":"markdown","c7446a6b":"markdown","026a9644":"markdown"},"source":{"8d325c82":"import pandas as pd\n\nloans_raw = pd.read_csv(\n    \"..\/input\/lending-club\/accepted_2007_to_2018q4.csv\/accepted_2007_to_2018Q4.csv\",\n    low_memory=False,\n)\n\nloans_raw.shape","92fa9a00":"dictionary_df = pd.read_excel(\"https:\/\/resources.lendingclub.com\/LCDataDictionary.xlsx\")\n\n# Drop blank rows, strip white space, convert to Python dictionary, fix one key name\ndictionary_df.dropna(axis=\"index\", inplace=True)\ndictionary_df = dictionary_df.applymap(lambda x: x.strip())\ndictionary_df.set_index(\"LoanStatNew\", inplace=True)\ndictionary = dictionary_df[\"Description\"].to_dict()\ndictionary[\"verification_status_joint\"] = dictionary.pop(\"verified_status_joint\")\n\n# Print in order of dataset columns (which makes more sense than dictionary's order)\nfor col in loans_raw.columns:\n    print(f\"\u2022{col}: {dictionary[col]}\")\n\n# Hiding the output because it's quite a few lines, but feel free to take a peek by\n# clicking the \"Output\" button","65a57824":"cols_for_output = [\"term\", \"installment\", \"total_rec_prncp\", \"total_rec_int\", \"total_rec_late_fee\", \"recoveries\", \"collection_recovery_fee\"]","503a28cb":"loans_raw[\"emp_title\"].nunique()","21897db7":"cols_to_drop = [\"id\", \"member_id\", \"funded_amnt\", \"funded_amnt_inv\", \"int_rate\", \"grade\", \"sub_grade\", \"emp_title\", \"pymnt_plan\", \"url\", \"desc\", \"title\", \"zip_code\", \"addr_state\", \"initial_list_status\", \"out_prncp\", \"out_prncp_inv\", \"total_pymnt\", \"total_pymnt_inv\", \"last_pymnt_d\", \"last_pymnt_amnt\", \"next_pymnt_d\", \"last_credit_pull_d\", \"last_fico_range_high\", \"last_fico_range_low\", \"policy_code\", \"hardship_flag\", \"hardship_type\", \"hardship_reason\", \"hardship_status\", \"deferral_term\", \"hardship_amount\", \"hardship_start_date\", \"hardship_end_date\", \"payment_plan_start_date\", \"hardship_length\", \"hardship_dpd\", \"hardship_loan_status\", \"orig_projected_additional_accrued_interest\", \"hardship_payoff_balance_amount\", \"hardship_last_payment_amount\", \"disbursement_method\", \"debt_settlement_flag\", \"debt_settlement_flag_date\", \"settlement_status\", \"settlement_date\", \"settlement_amount\", \"settlement_percentage\", \"settlement_term\"]\n\nloans = loans_raw.drop(columns=cols_to_drop)","4897ad97":"loans.groupby(\"loan_status\")[\"loan_status\"].count()","5f890042":"credit_policy = \"Does not meet the credit policy. Status:\"\nlen_credit_policy = len(credit_policy)\nremove_credit_policy = (\n    lambda status: status[len_credit_policy:]\n    if credit_policy in str(status)\n    else status\n)\nloans[\"loan_status\"] = loans[\"loan_status\"].map(remove_credit_policy)\n\nrows_to_drop = loans[\n    (loans[\"loan_status\"] != \"Charged Off\") & (loans[\"loan_status\"] != \"Fully Paid\")\n].index\nloans.drop(index=rows_to_drop, inplace=True)\n\nloans.groupby(\"loan_status\")[\"loan_status\"].count()","146edd8e":"loans[cols_for_output].info()","b18c8b47":"loans.groupby(\"term\")[\"term\"].count()","6299edc1":"onehot_cols = [\"term\"]\n\nloans[\"term\"] = loans[\"term\"].map(lambda term_str: term_str.strip())\n\nextract_num = lambda term_str: float(term_str[:2])\nloans[\"term_num\"] = loans[\"term\"].map(extract_num)\ncols_for_output.remove(\"term\")\ncols_for_output.append(\"term_num\")","c82a5484":"received = (\n    loans[\"total_rec_prncp\"]\n    + loans[\"total_rec_int\"]\n    + loans[\"total_rec_late_fee\"]\n    + loans[\"recoveries\"]\n    - loans[\"collection_recovery_fee\"]\n)\nexpected = loans[\"installment\"] * loans[\"term_num\"]\nloans[\"fraction_recovered\"] = received \/ expected\n\nloans.groupby(\"loan_status\")[\"fraction_recovered\"].describe()","ede0ccd8":"import numpy as np\n\nloans[\"fraction_recovered\"] = np.where(\n    (loans[\"loan_status\"] == \"Fully Paid\") | (loans[\"fraction_recovered\"] > 1.0),\n    1.0,\n    loans[\"fraction_recovered\"],\n)\nloans.groupby(\"loan_status\")[\"fraction_recovered\"].describe()","6a3aa9ac":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nsns.kdeplot(\n    data=loans[\"fraction_recovered\"][loans[\"loan_status\"] == \"Charged Off\"],\n    label=\"Charged Off\",\n    shade=True,\n)\nplt.axis(xmin=0, xmax=1)\nplt.title('Distribution of \"fraction recovered\"')\nplt.show()","2cdc26fa":"loans.drop(columns=cols_for_output, inplace=True)\nloans.info(verbose=True, null_counts=True)","f780f437":"negative_mark_cols = [\"mths_since_last_delinq\", \"mths_since_last_record\", \"mths_since_last_major_derog\", \"mths_since_recent_bc_dlq\", \"mths_since_recent_inq\", \"mths_since_recent_revol_delinq\", \"mths_since_recent_revol_delinq\", \"sec_app_mths_since_last_major_derog\"]\njoint_cols = [\"annual_inc_joint\", \"dti_joint\", \"verification_status_joint\", \"revol_bal_joint\", \"sec_app_fico_range_low\", \"sec_app_fico_range_high\", \"sec_app_earliest_cr_line\", \"sec_app_inq_last_6mths\", \"sec_app_mort_acc\", \"sec_app_open_acc\", \"sec_app_revol_util\", \"sec_app_open_act_il\", \"sec_app_num_rev_accts\", \"sec_app_chargeoff_within_12_mths\", \"sec_app_collections_12_mths_ex_med\", \"sec_app_mths_since_last_major_derog\"]\nconfusing_cols = [\"open_acc_6m\", \"open_act_il\", \"open_il_12m\", \"open_il_24m\", \"mths_since_rcnt_il\", \"total_bal_il\", \"il_util\", \"open_rv_12m\", \"open_rv_24m\", \"max_bal_bc\", \"all_util\", \"inq_fi\", \"total_cu_tl\", \"inq_last_12m\"]","1987bf0d":"loans[\"issue_d\"] = loans[\"issue_d\"].astype(\"datetime64[ns]\")\n\n# Check date range of confusing columns\nloans[confusing_cols + [\"issue_d\"]].dropna(axis=\"index\")[\"issue_d\"].agg(\n    [\"count\", \"min\", \"max\"]\n)","45464e4c":"# Compare to all entries from Dec 2015 onward\nloans[\"issue_d\"][loans[\"issue_d\"] >= np.datetime64(\"2015-12-01\")].agg(\n    [\"count\", \"min\", \"max\"]\n)","5dd4cb30":"new_metric_cols = confusing_cols","f4fb3321":"mths_since_last_cols = [\n    col_name\n    for col_name in loans.columns\n    if \"mths_since\" in col_name or \"mo_sin_rcnt\" in col_name\n]\nmths_since_old_cols = [\n    col_name for col_name in loans.columns if \"mo_sin_old\" in col_name\n]\n\nfor col_name in mths_since_last_cols:\n    loans[col_name] = [\n        0.0 if pd.isna(months) else 1 \/ 1 if months == 0 else 1 \/ months\n        for months in loans[col_name]\n    ]\nloans.loc[:, mths_since_old_cols].fillna(0, inplace=True)\n\n# Rename inverse columns\nrename_mapper = {}\nfor col_name in mths_since_last_cols:\n    rename_mapper[col_name] = col_name.replace(\"mths_since\", \"inv_mths_since\").replace(\n        \"mo_sin_rcnt\", \"inv_mo_sin_rcnt\"\n    )\nloans.rename(columns=rename_mapper, inplace=True)\n\n\ndef replace_list_value(l, old_value, new_value):\n    i = l.index(old_value)\n    l.pop(i)\n    l.insert(i, new_value)\n\n\nreplace_list_value(new_metric_cols, \"mths_since_rcnt_il\", \"inv_mths_since_rcnt_il\")\nreplace_list_value(\n    joint_cols,\n    \"sec_app_mths_since_last_major_derog\",\n    \"sec_app_inv_mths_since_last_major_derog\",\n)","3bf05227":"loans.groupby(\"application_type\")[\"application_type\"].count()","fb1da13c":"joint_loans = loans[:][loans[\"application_type\"] == \"Joint App\"]\njoint_loans[joint_cols].info()","e1570f25":"joint_new_metric_cols = [\"revol_bal_joint\", \"sec_app_fico_range_low\", \"sec_app_fico_range_high\", \"sec_app_earliest_cr_line\", \"sec_app_inq_last_6mths\", \"sec_app_mort_acc\", \"sec_app_open_acc\", \"sec_app_revol_util\", \"sec_app_open_act_il\", \"sec_app_num_rev_accts\", \"sec_app_chargeoff_within_12_mths\", \"sec_app_collections_12_mths_ex_med\", \"sec_app_inv_mths_since_last_major_derog\"]\njoint_loans[joint_new_metric_cols + [\"issue_d\"]].dropna(axis=\"index\")[\"issue_d\"].agg(\n    [\"count\", \"min\", \"max\"]\n)","53626710":"# Check without `sec_app_revol_util` column\njoint_new_metric_cols_2 = [\"revol_bal_joint\", \"sec_app_fico_range_low\", \"sec_app_fico_range_high\", \"sec_app_earliest_cr_line\", \"sec_app_inq_last_6mths\", \"sec_app_mort_acc\", \"sec_app_open_acc\", \"sec_app_open_act_il\", \"sec_app_num_rev_accts\", \"sec_app_chargeoff_within_12_mths\", \"sec_app_collections_12_mths_ex_med\", \"sec_app_inv_mths_since_last_major_derog\"]\njoint_loans[joint_new_metric_cols_2 + [\"issue_d\"]].dropna(axis=\"index\")[\"issue_d\"].agg(\n    [\"count\", \"min\", \"max\"]\n)","b8e33c23":"joint_loans[\"issue_d\"].agg([\"count\", \"min\", \"max\"])","ef159ac5":"onehot_cols.append(\"application_type\")\n\n# Fill joint columns in individual applications\nfor joint_col, indiv_col in zip(\n    [\"annual_inc_joint\", \"dti_joint\", \"verification_status_joint\"],\n    [\"annual_inc\", \"dti\", \"verification_status\"],\n):\n    loans[joint_col] = [\n        joint_val if app_type == \"Joint App\" else indiv_val\n        for app_type, joint_val, indiv_val in zip(\n            loans[\"application_type\"], loans[joint_col], loans[indiv_col]\n        )\n    ]\n\nloans.info(verbose=True, null_counts=True)","d0caffcf":"cols_to_search = [\n    col for col in loans.columns if col not in new_metric_cols + joint_new_metric_cols\n]\nloans.dropna(axis=\"index\", subset=cols_to_search).shape","fd852467":"loans.dropna(axis=\"index\", subset=cols_to_search, inplace=True)","f94010ae":"loans[[\"earliest_cr_line\", \"sec_app_earliest_cr_line\"]]","06340b24":"def get_credit_history_age(col_name):\n    earliest_cr_line_date = loans[col_name].astype(\"datetime64[ns]\")\n    cr_hist_age_delta = loans[\"issue_d\"] - earliest_cr_line_date\n    MINUTES_PER_MONTH = int(365.25 \/ 12 * 24 * 60)\n    cr_hist_age_months = cr_hist_age_delta \/ np.timedelta64(MINUTES_PER_MONTH, \"m\")\n    return cr_hist_age_months.map(\n        lambda value: np.nan if pd.isna(value) else round(value)\n    )\n\n\ncr_hist_age_months = get_credit_history_age(\"earliest_cr_line\")\ncr_hist_age_months","2654f12a":"loans[\"earliest_cr_line\"] = cr_hist_age_months\nloans[\"sec_app_earliest_cr_line\"] = get_credit_history_age(\n    \"sec_app_earliest_cr_line\"\n).astype(\"Int64\")\nloans.rename(\n    columns={\n        \"earliest_cr_line\": \"cr_hist_age_mths\",\n        \"sec_app_earliest_cr_line\": \"sec_app_cr_hist_age_mths\",\n    },\n    inplace=True,\n)\nreplace_list_value(\n    joint_new_metric_cols, \"sec_app_earliest_cr_line\", \"sec_app_cr_hist_age_mths\"\n)","b9be17da":"categorical_cols = [\"term\", \"emp_length\", \"home_ownership\", \"verification_status\", \"purpose\", \"verification_status_joint\"]\nfor i, col_name in enumerate(categorical_cols):\n    print(\n        loans.groupby(col_name)[col_name].count(),\n        \"\\n\" if i < len(categorical_cols) - 1 else \"\",\n    )","ac811a93":"loans.drop(\n    columns=[\n        \"verification_status\",\n        \"verification_status_joint\",\n        \"issue_d\",\n        \"loan_status\",\n    ],\n    inplace=True,\n)","265fc8c0":"onehot_cols += [\"home_ownership\", \"purpose\"]\nordinal_cols = {\n    \"emp_length\": [\n        \"< 1 year\",\n        \"1 year\",\n        \"2 years\",\n        \"3 years\",\n        \"4 years\",\n        \"5 years\",\n        \"6 years\",\n        \"7 years\",\n        \"8 years\",\n        \"9 years\",\n        \"10+ years\",\n    ]\n}","5b8a0e6f":"loans_1 = loans.drop(columns=new_metric_cols + joint_new_metric_cols)\nloans_2 = loans.drop(columns=joint_new_metric_cols)\nloans_2.info(verbose=True, null_counts=True)","61679ee1":"loans_2[\"il_util\"][loans_2[\"il_util\"].notna()].describe()","3d7f6815":"query_df = loans[[\"il_util\", \"total_bal_il\", \"total_il_high_credit_limit\"]].dropna(\n    axis=\"index\", subset=[\"il_util\"]\n)\nquery_df[\"il_util_compute\"] = (\n    query_df[\"total_bal_il\"] \/ query_df[\"total_il_high_credit_limit\"]\n).map(lambda x: float(round(x * 100)))\nquery_df[[\"il_util\", \"il_util_compute\"]]","c87b61c1":"(query_df[\"il_util\"] == query_df[\"il_util_compute\"]).describe()","b40df3c0":"query_df[\"compute_diff\"] = abs(query_df[\"il_util\"] - query_df[\"il_util_compute\"])\nquery_df[\"compute_diff\"][query_df[\"compute_diff\"] != 0].describe()","ddda04a7":"loans[\"il_util_imputed\"] = [\n    True if pd.isna(util) & pd.notna(bal) & pd.notna(limit) else False\n    for util, bal, limit in zip(\n        loans[\"il_util\"], loans[\"total_bal_il\"], loans[\"total_il_high_credit_limit\"]\n    )\n]\nnew_metric_onehot_cols = [\"il_util_imputed\"]\nloans[\"il_util\"] = [\n    0.0\n    if pd.isna(util) & pd.notna(bal) & (limit == 0)\n    else float(round(bal \/ limit * 100))\n    if pd.isna(util) & pd.notna(bal) & pd.notna(limit)\n    else util\n    for util, bal, limit in zip(\n        loans[\"il_util\"], loans[\"total_bal_il\"], loans[\"total_il_high_credit_limit\"]\n    )\n]\n\nloans_2 = loans.drop(columns=joint_new_metric_cols)\nloans_2.info(verbose=True, null_counts=True)","6cba3726":"loans_2.dropna(axis=\"index\", inplace=True)\n\nloans_3 = loans.dropna(axis=\"index\")\nloans_3.info(verbose=True, null_counts=True)","69ebbe63":"from sklearn.model_selection import train_test_split\nfrom sklearn_pandas import DataFrameMapper\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\nfrom tensorflow.keras import Sequential, Input\nfrom tensorflow.keras.layers import Dense, Dropout\n\n\ndef run_pipeline(\n    data,\n    onehot_cols,\n    ordinal_cols,\n    batch_size,\n    validate=True,\n):\n    X = data.drop(columns=[\"fraction_recovered\"])\n    y = data[\"fraction_recovered\"]\n    X_train, X_valid, y_train, y_valid = (\n        train_test_split(X, y, test_size=0.2, random_state=0)\n        if validate\n        else (X, None, y, None)\n    )\n\n    transformer = DataFrameMapper(\n        [\n            (onehot_cols, OneHotEncoder(drop=\"if_binary\")),\n            (\n                list(ordinal_cols.keys()),\n                OrdinalEncoder(categories=list(ordinal_cols.values())),\n            ),\n        ],\n        default=StandardScaler(),\n    )\n\n    X_train = transformer.fit_transform(X_train)\n    X_valid = transformer.transform(X_valid) if validate else None\n\n    input_nodes = X_train.shape[1]\n    output_nodes = 1\n\n    model = Sequential()\n    model.add(Input((input_nodes,)))\n    model.add(Dense(64, activation=\"relu\"))\n    model.add(Dropout(0.3, seed=0))\n    model.add(Dense(32, activation=\"relu\"))\n    model.add(Dropout(0.3, seed=1))\n    model.add(Dense(16, activation=\"relu\"))\n    model.add(Dropout(0.3, seed=2))\n    model.add(Dense(output_nodes))\n    model.compile(optimizer=\"adam\", loss=\"mean_squared_logarithmic_error\")\n\n    history = model.fit(\n        X_train,\n        y_train,\n        batch_size=batch_size,\n        epochs=100,\n        validation_data=(X_valid, y_valid) if validate else None,\n        verbose=2,\n    )\n\n    return history.history, model, transformer\n\n\nprint(\"Model 1:\")\nhistory_1, _, _ = run_pipeline(\n    loans_1,\n    onehot_cols,\n    ordinal_cols,\n    batch_size=128,\n)\nprint(\"\\nModel 2:\")\nhistory_2, _, _ = run_pipeline(\n    loans_2,\n    onehot_cols + new_metric_onehot_cols,\n    ordinal_cols,\n    batch_size=64,\n)\nprint(\"\\nModel 3:\")\nhistory_3, _, _ = run_pipeline(\n    loans_3,\n    onehot_cols + new_metric_onehot_cols,\n    ordinal_cols,\n    batch_size=32,\n)","0ec62f4c":"sns.lineplot(x=range(1, 101), y=history_1[\"loss\"], label=\"loss\")\nsns.lineplot(x=range(1, 101), y=history_1[\"val_loss\"], label=\"val_loss\")\nplt.xlabel(\"epoch\")\nplt.title(\"Model 1 loss metrics during training\")\nplt.show()","01589b92":"import joblib\n\n_, final_model, final_transformer = run_pipeline(\n    loans_1,\n    onehot_cols,\n    ordinal_cols,\n    batch_size=128,\n    validate=False,\n)\n\nfinal_model.save(\"loan_risk_model\")\njoblib.dump(final_transformer, \"data_transformer.joblib\")","490b3a50":"# Exports for \"Can I Grade Loans Better than LendingClub?\"\nexpected.rename(\"expected_return\", inplace=True)\nloans_for_eval = loans_1.join([loans_raw[[\"issue_d\", \"grade\", \"sub_grade\"]], expected])\njoblib.dump(loans_for_eval, \"loans_for_eval.joblib\")\n\n# Exports for \"Improving Loan Risk Prediction With Natural Language Processing\"\nloans_for_nlp = loans_1.join(loans_raw[[\"issue_d\", \"title\", \"desc\"]])\njoblib.dump(loans_for_nlp, \"loans_for_nlp.joblib\")","654516c9":"First, in researching income verification, I learned that LendingClub only tries to [verify income](https:\/\/www.lendingclub.com\/investing\/investor-education\/income-verification) on a subset of loan applications based on the content of the application, so this feature is a source of target leakage. I'll remove the two offending columns (and a couple more I don't need anymore).","8b9866b3":"Ah, so `term` is a categorical feature with two options. I'll treat it as such when I use it as an input to the model, but to calculate the output variable I'll create a numerical column from it.\n\nAlso, I need to trim the whitespace from the beginning of those values\u2014that's no good.","ffbc2c07":"It seems there may be a case of newer metrics for joint applications as well. I'll investigate.","73379705":"_Now_ I can create the output variable.","47568779":"With 2,260,701 loans to look at and 151 potential variables, my goal is to create a neural network model to predict the fraction of an expected loan return that a prospective borrower will pay back. (Spoiler alert: my model can pick grade A loans [better than LendingClub](https:\/\/www.kaggle.com\/tywmick\/can-i-grade-loans-better-than-lendingclub).)\n\nAfterward, I'll create a public API to serve that model.\n\n## Data cleaning\n\nI'll first look at the data dictionary (downloaded directly [from LendingClub's website](https:\/\/resources.lendingclub.com\/LCDataDictionary.xlsx)) to get an idea of how to create the desired output variable and which remaining features are available at the point of loan application (to avoid data leakage).","6d7c49f1":"Newer than the previous set of new metrics, even\u2014these didn't start getting used till March 2017. Now I wonder when joint loans were first introduced.","c699147f":"There is at least one odd outlier on the right in both categories. But also, many of the \"fully paid\" loans do not quite reach 1. One potential explanation is that when the last payment comes in, the system just flips `loan_status` to \"Fully Paid\" without adding the payment amount to the system itself, or perhaps simply multiplying `installation` by the `term` number leaves off a few cents in the actual total. If I were performing this analysis for Lending Club themselves, I'd ask them, but this is just a personal project. I'll consider every loan marked \"Fully Paid\" to have fully recovered the expected return.\n\nFor that matter, I'll cap my `fraction_recovered` values for charged off loans at 1.0 as well, since at least one value is above that for some reason.","957f69e8":"Before creating the output variable, however, I must take a closer look at `loan_status`, to see if any loans in the dataset are still open.","6b4c57f6":"For practical purposes, I'll consider loans with statuses that don't contain \"Fully Paid\" or \"Charged Off\" to still be open, so I'll remove those from the dataset. I'll also merge the \"credit policy\" columns with their matching status.","987d668c":"Then actually I'll tackle `earliest_cr_line` and its joint counterpart first before looking at the categorical features.","bcf3ea65":"Now to look closer at joint loans.","63ab7822":"Good. Ready to drop rows with nulls in `loans_2` and move on to the DataFrame for the model that adds the new metrics for joint applications.","6626edc7":"For the sake of curiosity, I'll plot the distribution of fraction recovered for charged-off loans.","7266e3aa":"Peeking back up to the data dictionary, `il_util` is the \"ratio of total current balance to high credit\/credit limit on all install acct\". The relevant balance (`total_bal_il`) and credit limit (`total_il_high_credit_limit`) metrics appear to already be in the data, so perhaps this utilization metric doesn't contain any new information. I'll compare `il_util` (where it's present) to the ratio of the other two variables.","01063f8b":"Several other columns contain either irrelevant demographic data or data not created until after a loan is accepted, so those will need to be removed. I'll hold onto `issue_d` (the month and year the loan was funded) for now, though, in case I want to compare variables to the date of the loan.\n\n`emp_title` (the applicant's job title) _does_ seem relevant in the context of a loan, but it may have too many unique values to be useful.","f811b3bb":"## Saving the final model\n\nFirst I need to _create_ the final model, training `model_1`'s architecture on the full dataset. Then I'll save the model to disk with its `save` function and save the data transformer using joblib so I can use it in the API.","f630b6dd":"-----\n\nOne of the best\/worst things about machine learning is that your models _always_ have room for improvement. I mentioned a couple ideas along the way above for how I could improve the model in the future, but what's the first thing you would tweak in this model? I'd love to hear in the comments below.","7950ebf8":"Phew, the data's all clean now! Time for the fun part.\n\n## Building the neural networks\n\nAfter a good deal of trial and error, I found that a network architecture with three hidden layers, each followed by a dropout layer of rate 0.3, was as good as I could find. I used ReLU activation in those hidden layers, and adam optimization and a loss metric of mean squared error in the model as a whole. I also settled on a mean squared logarithmic error loss function, since it performed better than mean absolute error, mean squared error, and mean absolute percentage error.\n\nThe dataset being so large, I had great results increasing the batch size for the first couple models.","6d346547":"Now to create the output variable. I'll start by checking the null counts of the variables involved.","2f110e36":"Every remaining row has each of these seven variables, but `term`'s data type is `object`, so that needs to be fixed first.","103c8e61":"<div style=\"font-size: 16px; font-weight: 500; line-height: 1.4; font-style: italic; color: #6c757d; margin-bottom: 1em;\">\n  or, Ty Goes Into Far Too Much Detail About Cleaning Data\n<\/div>\n\n1. **[Introduction](#Introduction)**\n2. **[Data cleaning](#Data-cleaning)**\n3. **[Building the neural networks](#Building-the-neural-networks)**\n4. **[Saving the final model](#Saving-the-final-model)**\n5. **[Building the API](#Building-the-API)**\n6. **[Further reading](#Further-reading)**\n\n## Introduction\n\n[LendingClub](https:\/\/www.lendingclub.com\/) is the world's largest peer-to-peer lending platform. Until recently (through the end of 2018), LendingClub published a public dataset of all loans issued since the company's launch in 2007. I'm accessing the dataset [via Kaggle](https:\/\/www.kaggle.com\/wordsforthewise\/lending-club).","524f48a1":"The first model performed best, settling around a mean squared logarithmic error of 0.0230 (though it seems even after setting `random_state` inside `train_test_split` and `seed` inside the dropout layers, there's still a bit of entropy left in the training of the model, so if you run this notebook yourself, the course of your training may look a little different). Apparently the additional _records_ in the first dataset did more to aid in training than the additional _metrics_ in the subsequent sets. And the dropout layers didn't stop the third model from overfitting anyway.","e3d0ac91":"That should cover all the cleaning necessary for the first model's data. I'll save the columns that'll be used in the first model to a new DataFrame, and while I'm at it, I'll start formatting the DataFrames for the two additional models adding the two sets of new metrics.","e3a59d26":"Yes, still 1,110,171. That'll do.","db0fe2bf":"Before I drop a bunch of rows with nulls from `loans_2`, I'm concerned about `il_util`, as it's missing values in about 50,000 more rows than the rest of the new metric columns. Why would that be?","8723def0":"Now that the output is formatted, it's time to clean up the inputs. I'll check the null counts of each variable.","78e81424":"As for the derogatory\/delinquency metrics, taking a cue [from Michael Wurm](https:\/\/towardsdatascience.com\/intelligent-loan-selection-for-peer-to-peer-lending-575dfa2573cb#a0a0), I'm going to take the inverse of all the \"months since recent\/last\" fields, which will turn each into a proxy for the frequency of the event and also let me set all the null values (when an event has never happened) to 0. For the \"months since oldest\" fields, I'll just set the null values to 0 and leave the rest untouched.","08e18819":"Too many unique values indeed. In a future version of this model I could perhaps try to generate a feature from this column by aggregating job titles into categories, but that effort may have a low return on investment, since there are already columns for annual income and length of employment.\n\nTwo other interesting columns that I'll also remove are `title` and `desc` (\"description\"), which are both freeform text entries written by the borrower. These could be fascinating subjects for natural language processing, but that's outside the scope of the current project. Perhaps in the future I could generate additional features from these fields using measures like syntactic complexity, word count, or keyword inclusion. (I ended up exploring this line of thinking in [a later notebook](https:\/\/www.kaggle.com\/tywmick\/natural-language-processing-for-loan-risk \"Natural Language Processing for Loan Risk\") using document vectors.)","7a5ef676":"2015\\. I think I'll save the newer joint metrics for perhaps a third model, but I believe I can include `annual_inc_joint`, `dti_joint`, and `verification_status_joint` in the main model\u2014I'll just binary-encode `application_type`, and for individual applications I'll set `annual_inc_joint`, `dti_joint`, and `verification_status_joint` equal to their non-joint counterparts.","260dc35c":"I'll first look at those more confusing columns to find out whether or not they're a newer set of metrics. That'll require converting `issue_d` to date format first.","499971da":"## Building the API\n\nI first tried building this API and its demonstrational front end on [Glitch](https:\/\/glitch.com\/), which, officially, only supports Node.js back ends, but unofficially you can get a Python server running there (which I've [done before](https:\/\/ty-metricimpconverter-python.glitch.me\/ \"Python metric-imperial converter microservice \u2013 Ty Mick\") using [Flask](https:\/\/flask.palletsprojects.com\/)). When I was almost finished, though, I tried importing TensorFlow to load my model, and it was then that I discovered that unlike Node.js dependencies, Python dependencies get installed to your project's disk space on Glitch, and not even their pro plan provides enough space to contain the entire TensorFlow library. Which totally makes sense\u2014I certainly wasn't using the platform as intended.\n\nThen I discovered [PythonAnywhere](https:\/\/www.pythonanywhere.com\/)! They have plenty of common Python libraries already installed out-of-the-box, including TensorFlow, so I got everything working perfectly there.\n\nSo [head on over](https:\/\/tywmick.pythonanywhere.com\/ \"Neural Network Loan Risk Prediction API \u2013 Ty Mick\") if you'd like to check it out; the front end includes a form where you can fill in all the parameters for the API request, and there are a couple of buttons that let you fill the form with typical examples from the dataset (since there are a _lot_ of fields to fill in). Or you can send a GET request to `https:\/\/tywmick.pythonanywhere.com\/api\/predict` if you really want to include every parameter in your query string. In either case, you're also more than welcome to take a look at its source [on GitHub](https:\/\/github.com\/tywmick\/loan-risk-neural-network).\n\n## Further reading\n\n- [Can I Grade Loans Better Than LendingClub?](https:\/\/www.kaggle.com\/tywmick\/can-i-grade-loans-better-than-lendingclub)\n- [Natural Language Processing for Loan Risk](https:\/\/www.kaggle.com\/tywmick\/natural-language-processing-for-loan-risk)","f30055a0":"For the output variable (the fraction of expected return that was recovered), I'll calculated the _expected return_ by multiplying the monthly payment amount (`installment`) by the number of payments on the loan (`term`), and I'll calculate the _amount actually received_ by summing the total principle, interest, late fees, and post-chargeoff gross recovery received (`total_rec_prncp`, `total_rec_int`, `total_rec_late_fee`, `recoveries`) and subtracting any collection fee (`collection_recovery_fee`).","05e11342":"Now the only remaining steps should be removing rows with null values (in columns that aren't new metrics) and encoding categorical features.\n\nI'm _removing_ rows with null values in those columns because that should still leave the vast majority of rows intact, over 1 million, which is still plenty of data. But I guess I should make sure before I overwrite `loans`.","4371fc0e":"That's weird. `il_util` is equal to the computed ratio three-quarters of the time, but when it's off, the median difference is 10 points off. Perhaps there's new information there sometimes after all. Maybe whatever credit bureau is reporting the utilization rate uses a different formula than just a simple ratio? Again, something I could ask if I were performing this analysis for a client, but that's not the case. I'll assume that this variable is still valuable, and where `il_util` is null I'll impute the value to make it equal to the ratio of `total_bal_il` to `total_il_high_credit_limit` (or 0 if the limit is 0). And I'll add one more boolean field to mark the imputed entries.\n\nAlso, that 1,108 is a doozy of an outlier, but I think I'll just leave it be, as it appears that [outliers aren't too big a deal](https:\/\/medium.com\/analytics-vidhya\/effect-of-outliers-on-neural-networks-performance-ca1d9185dce9) if the neural network architecture is sufficiently deep.","ca1bb146":"Remaining columns with lots of null values seem to fall into three categories:\n\n1. Derogatory\/delinquency metrics (where null means the borrower doesn't have any such marks)\n  - I'll also add `mths_since_recent_inq` to this list, since its non-null count is below what seems to be the threshold for complete data, which is around 1,277,783. I'll assume a null value here means no recent inquiries.\n2. Metrics that only apply to joint applications (where null means it was a single application)\n3. An inexplicable series of 14 credit history\u2013related columns that only have around 537,000 entries. Are these newer metrics?","928863b8":"I should convert that to the age of the credit line at the time of application (or the time of loan issuing, more precisely).","04e499ad":"_Now_ a look at those categorical features.","c7446a6b":"It appears that these are indeed newer metrics, their use only beginning in December 2015, but even after that point usage is spotty. I'm curious to see if these additional metrics would make a model more accurate, though, so once I'm done cleaning the data I'll copy the rows with these new metrics into a new dataset and create another model using the new metrics.","026a9644":"Once I create my pipeline, I'll binary encode `term`, one-hot encode `home_ownership` and `purpose`, and since `emp_length` is an ordinal variable, I'll convert it to the integers 0\u201310."}}