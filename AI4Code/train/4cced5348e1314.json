{"cell_type":{"0f351b4f":"code","156a4709":"code","66b30cbf":"code","d45822d2":"code","7090278b":"code","1c8dd1f4":"code","58017128":"code","64650ab7":"code","9fc50142":"markdown","e23291b4":"markdown","9a2d2bfc":"markdown","9893e9f0":"markdown","84b10d40":"markdown","f07f0cb4":"markdown","e9d5d6c0":"markdown","57cdd058":"markdown","e46e560a":"markdown"},"source":{"0f351b4f":"import numpy as np # linear algebra","156a4709":"def function(x, y):\n    return(-2*x + x**2 - 12 * y + y**3)","66b30cbf":"def get_gradient(x, y):\n    dx = -2 + 2*x\n    dy = -12 + 3*y**2\n    return(np.array([dx, dy]))","d45822d2":"def new_point(x, y, grad):\n    a = np.array([x,y])\n    b = a - step * grad\n    return(b[0], b[1])","7090278b":"# Starting Point \nx_0, y_0 = 4, 4\n# Stepsize\nstep = 0.10","1c8dd1f4":"# Initial Step\nfunction(x_0, y_0)\ngrad_0 = get_gradient(x_0, y_0)\nx_1, y_1 = new_point(x_0, y_0, grad_0)","58017128":"# Second Step\nfunction(x_1, y_1)\ngrad_1 = get_gradient(x_1, y_1)\nx_2, y_2 = new_point(x_1, y_1, grad_1)","64650ab7":"x, y = 4, 4\n\nfor i in range(1, 10):\n     grad = get_gradient(x, y)\n     x, y = new_point(x, y, grad)\n     print('\\n' + \"x: \", x) \n     print(\"y: \", y)\n     print(\"z: \", function(x, y))","9fc50142":"Calculating our next point gives us:\n\\begin{align}\nb = a -  \\gamma  * \\nabla f(a) = {4 \\choose 4} -  0.10  {-2 + 2*4 \\choose -12 + 3*4^2} = {3.4 \\choose 0.4}\n\\end{align}","e23291b4":"The next point b is always:\n\\begin{align}\nb = a -  \\gamma *  \\nabla f(a) = {x \\choose y} -  \\gamma  {-2 + 2x \\choose -12 + 3y^2}\n\\end{align}","9a2d2bfc":"We can do this iterative, until the result is close enough to the optimum. Let's try it for example ten times.","9893e9f0":"**How it works**\n\nGradient Descent can be thought of climbing down to the bottom of a valley, instead of climbing up a hill. This is because it is a minimization algorithm that minimizes a given function.\n\nThe equation below describes what Gradient Descent does: \u201eb\u201c describes the next position of our climber, while \u201ea\u201c represents his current position. The minus sign refers to the minimization part of gradient descent. The \u201egamma\u201c in the middle is a waiting factor and the gradient term ( \u0394f(a) ) is simply the direction of the steepest descent.","84b10d40":"\\begin{align}\nb = a -  \\gamma *  \\nabla f(a)\n\\end{align}\n\nFor further reading see hear:\n[https:\/\/towardsdatascience.com\/gradient-descent-in-a-nutshell-eaf8c18212f0](http:\/\/)","f07f0cb4":"We want to minimize the function:\n\n\\begin{align}\nf(x,y) = -2x + x^2 - 12y + y^3\n\\end{align}\n\n\nThe partial derivatives are:\n\\begin{align}\n\\frac{\\partial{{f(x,y)}}}{\\partial x} = -2 + 2x\n \\end{align}\n and\n\\begin{align}\n\\frac{\\partial{{f(x,y)}}}{\\partial y} = -12 + 3y^2\n \\end{align}\n \n Spoiler Alert! At the Minimum both equations have to be zero:\n \\begin{align}\n0 = -2 + 2x\n \\end{align}\n  \\begin{align}\nx = 1\n  \\end{align}\n  \\begin{align}\n0 = -12 + 3y^2\n \\end{align}\n  \\begin{align}\ny = 2\n  \\end{align}\n \nSo the minimum Value of our function is:\n\\begin{align}\nf(1,2) = -2*1 + 1^2 - 12*2 + 2^3 = -17\n\\end{align}\nBut let's ignore that for a now, since we are hear to learn somehing about gradient descent. ","e9d5d6c0":"The gradient follows from the partial derivatives:\n\\begin{align}\n\\nabla f(a) = \\nabla f(x,y) = {-2 + 2x \\choose -12 + 3y^2}\n \\end{align}\n","57cdd058":"Next we define our starting point: Let's for example start at  x = 4, y = 4. So point a is (4, 4) and we are searching for our next point b. As stepsize we define 0.10.","e46e560a":"Now we can take the point x = 3.4, y = 0.4 as our new point a and calculate our next point b, by repeating the step.\n\\begin{align}\nb = {3.4 \\choose 0.4} -  0.10  {-2 + 2*3.4 \\choose -12 + 3*0.4^2} = { 2.92 \\choose 1.552}\n\\end{align}"}}