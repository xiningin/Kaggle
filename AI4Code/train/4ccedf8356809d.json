{"cell_type":{"ea27a96c":"code","59c5b8af":"code","df903c6a":"code","adfe1c1b":"code","f20ec0cb":"code","4f615256":"code","1820ebf1":"code","6f41a5a3":"code","a021d587":"code","ffe3513e":"code","aad85c74":"code","70ab5ca1":"code","7f15735b":"code","8fe01512":"code","06623f19":"code","457f610d":"code","f4a17bed":"code","cdac8ff9":"code","28e49fba":"code","61dae9cd":"code","74623b21":"code","9c164c1b":"code","49c8e067":"code","ce541e55":"code","db162311":"code","e5c0ff9d":"code","8be39bc1":"code","004a2453":"code","34825cc6":"code","a8dc55a3":"code","5978c9fd":"code","e9f540ed":"code","9cc0dfd5":"code","0353d7c6":"code","a759d84f":"code","abee911e":"code","ac989ed6":"code","8b1298c4":"code","f0c4708e":"code","cd9f203d":"code","a7755c2c":"code","80374f13":"code","96e4c049":"code","5b005d07":"code","e684e6d2":"code","9628c17a":"code","e907cf18":"code","9393d5ed":"code","9071ff1f":"code","946430d2":"markdown","a8f8e244":"markdown","ace87ff6":"markdown","4fda44dd":"markdown","06597505":"markdown","c9a1cfed":"markdown","0a85099f":"markdown","26cf1399":"markdown","fe8ce5cd":"markdown","89eb0490":"markdown","b76f18c6":"markdown","fc2703c3":"markdown","0821f359":"markdown","7a9dac99":"markdown","891a3b57":"markdown","43ca1c8c":"markdown","6b0b078e":"markdown","dec9fa45":"markdown","764b0ab4":"markdown","a6c94265":"markdown","3cd76919":"markdown","b5fa2bf4":"markdown","02554709":"markdown","95ad7b8c":"markdown","e28526ea":"markdown","d97f66d4":"markdown","e7b6f87e":"markdown","d60b098a":"markdown","e5a61a6e":"markdown","bb8897bc":"markdown","5994cbee":"markdown","6ff71b44":"markdown","ef88f6aa":"markdown","973446f1":"markdown","50c13a47":"markdown"},"source":{"ea27a96c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","59c5b8af":"train_data = pd.read_csv(\"..\/input\/titanic\/train.csv\",index_col='PassengerId')\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\", index_col='PassengerId')","df903c6a":"train_data.head()","adfe1c1b":"train_data['Royalty'] = train_data['Name'].str.split(', ').str[1].str.split('.').str[0]\ntest_data['Royalty'] = test_data['Name'].str.split(', ').str[1].str.split('.').str[0]\ntrain_data['Royalty'].unique()","f20ec0cb":"train_data['MarStatus'] = train_data['Royalty']\ntest_data['MarStatus'] = test_data['Royalty']\n\ntrain_data['MarStatus'] = train_data['MarStatus'].replace(['Ms','Mlle'], 'Miss')\ntrain_data['MarStatus'] = train_data['MarStatus'].replace(['Mme','Dona','the Countess','Lady'], 'Mrs')\ntrain_data['MarStatus'] = train_data['MarStatus'].replace(['Rev','Jonkheer','Dr','Capt','Don','Col','Major','Sir'], 'Mr')\n\ntest_data['MarStatus'] = test_data['MarStatus'].replace(['Ms','Mlle'], 'Miss')\ntest_data['MarStatus'] = test_data['MarStatus'].replace(['Mme','Dona','the Countess','Lady'], 'Mrs')\ntest_data['MarStatus'] = test_data['MarStatus'].replace(['Rev','Jonkheer','Dr','Capt','Don','Col','Major','Sir'], 'Mr')","4f615256":"train_data['FamilySize'] = train_data['SibSp'] + train_data['Parch'] + 1\ntest_data['FamilySize'] = test_data['SibSp'] + test_data['Parch'] + 1","1820ebf1":"train_data['isAlone'] = train_data['FamilySize']\ntrain_data['isAlone'].replace({2 : 0, 5 : 0, 3 : 0, 7 : 0, 6 : 0, 4 : 0, 8 : 0, 11 : 0}, inplace = True)   \ntest_data['isAlone'] = test_data['FamilySize']\ntest_data['isAlone'].replace({2 : 0, 5 : 0, 3 : 0, 7 : 0, 6 : 0, 4 : 0, 8 : 0, 11 : 0}, inplace = True)   ","6f41a5a3":"train_data.isnull().sum()","a021d587":"test_data.isnull().sum()","ffe3513e":"#Categorical Encoding\n\ntrain_data['Sex'].replace({'male':1 , 'female':0},inplace=True)\ntrain_data['Embarked'].replace({'C': 1, 'S': 2 ,'Q': 3},inplace=True)\ntrain_data['MarStatus'].replace({'Miss' : 0, 'Mr' : 1, 'Mrs' : 2, 'Master' : 0}, inplace=True)\ntrain_data['Royalty'].replace({'Mr' : 0, 'Mrs': 0, 'Miss' : 0, 'Master' : 0, \n                               'Don' : 1, 'Rev' : 1, 'Dr' : 1, 'Mme' : 0, 'Ms' : 0,\n                               'Major' : 1, 'Lady' : 1, 'Sir' : 1, 'Mlle' : 0, \n                               'Col' : 1, 'Capt' : 1, 'the Countess' : 1, 'Jonkheer' : 1}, inplace = True)\n\ntest_data['MarStatus'].replace({'Miss' : 0, 'Mr' : 1, 'Mrs' : 2, 'Master' : 0}, inplace = True)\ntest_data['Sex'].replace({'male':1 , 'female':0},inplace=True)\ntest_data['Embarked'].replace({'C': 1, 'S': 2 ,'Q': 3},inplace=True)\ntest_data['Royalty'].replace({'Mr' : 0, 'Mrs': 0, 'Miss' : 0, 'Master' : 0, \n                               'Don' : 1, 'Dona' : 1, 'Rev' : 1, 'Dr' : 1, 'Mme' : 0, 'Ms' : 0,\n                               'Major' : 1, 'Lady' : 1, 'Sir' : 1, 'Mlle' : 0, \n                               'Col' : 1, 'Capt' : 1, 'the Countess' : 1, 'Jonkheer' : 1}, inplace = True)","aad85c74":"#Handling the missing values\n\ntrain_data['Age'].fillna(train_data['Age'].median(), inplace = True)\ntrain_data['Embarked'].fillna(1,inplace=True)\ntest_data['Age'].fillna(test_data['Age'].median(),inplace = True )\ntest_data['Fare'].fillna(test_data['Fare'].mean(), inplace = True)","70ab5ca1":"train_data['Embarked'] = train_data['Embarked'].astype(int)\ntest_data['Embarked'] = test_data['Embarked'].astype(int)","7f15735b":"train_data.head()","8fe01512":"train_data.isnull().sum()","06623f19":"test_data.isnull().sum()","457f610d":"sns.barplot(x = train_data['Pclass'] , y = 1 - train_data['Survived'])\nplt.title(\"No.of deaths according to Pclass\")\nplt.ylabel('Deaths')\n     \n# It can be observed that as the people from the 3rd class suffered the highest deaths","f4a17bed":"sns.barplot(x = train_data['Sex'] , y = train_data['Survived'])\nplt.title(\"No.of survivals according to Sex\")\n\n#It is observed that most of the survivors were female","cdac8ff9":"sns.barplot(x = train_data['SibSp'] , y = train_data['Survived'])\nplt.title(\"Effect of sibling(s) and spouse on survival\")\n\n# It can be observed that people with 0,1,2 spouse\/sibling(s) had the highest survival rate","28e49fba":"sns.barplot(x = train_data['Parch'] , y = train_data['Survived'])\nplt.title(\"Effect of parent and children on survival\")\n\n# It can be observed that parents with 0,1,2,3 children had the highest survival rate","61dae9cd":"sns.barplot(x = train_data['Embarked'] , y = train_data['Survived'])\nplt.title(\"Ports Embarked and Survival\")\n\n# It can be observed that people who Embarked from Cherbourg had the highest survival rate","74623b21":"sns.barplot(x = train_data['Royalty'] , y = train_data['Survived'])\nplt.title(\"Royal Titles and Survival\")\n\n# It can be observed that royal and non royals had a similar fate","9c164c1b":"sns.barplot(x = train_data['FamilySize'] , y = train_data['Survived'])\nplt.title(\"Effect of Family size on Survival\")\n\n# It can be observed that people with 4 or less membered families survived more","49c8e067":"sns.barplot(x = train_data['MarStatus'] , y = train_data['Survived'])\nplt.title(\"Maritial Status and Survival\")\n\n# It can be observed that Unmarried and Married females along with kids had higher chances of Survival","ce541e55":"plt.figure(figsize=(15,6))\n\n#outlier detection in Age column\nplt.subplot(1, 2, 1)\nfig = train_data.boxplot(column='Age')\nfig.set_title('')\nfig.set_ylabel('Age')\n\n#outlier detection in Fare column\nplt.subplot(1, 2, 2)\nfig = train_data.boxplot(column='Fare')\nfig.set_title('')\nfig.set_ylabel('Fare')","db162311":"plt.figure(figsize=(15,6))\n\n#Visualizing the Age columns\nplt.subplot(1, 2, 1)\nfig = train_data.Age.hist(bins=20)\nfig.set_ylabel('Number of passengers')\nfig.set_xlabel('Age')\n\n##Visualizing the Fare columns\nplt.subplot(1, 2, 2)\nfig = train_data.Fare.hist(bins=20)\nfig.set_ylabel('Number of passengers')\nfig.set_xlabel('Fare')","e5c0ff9d":"#Gaussian Assumption\n#defining upper and lower boundaries\nUpper_boundary = train_data.Age.mean() + 3* train_data.Age.std()\nLower_boundary = train_data.Age.mean() - 3* train_data.Age.std()\nAge_limit = round(Upper_boundary)\n\n#Constraining the upper limit of the data\ntrain_data['Age'] = np.where(train_data['Age'] > Age_limit, Age_limit, train_data['Age'])\ntest_data['Age'] = np.where(test_data['Age'] > Age_limit, Age_limit, test_data['Age'])\ntrain_data['Age'].max()","8be39bc1":"#Inter-Quartile Range Adjustments\n#defining the upper and lower break limits\nIQR = train_data.Fare.quantile(0.75) - train_data.Fare.quantile(0.25)\nLower_fence = train_data.Fare.quantile(0.25) - (IQR * 3)\nUpper_fence = train_data.Fare.quantile(0.75) + (IQR * 3)\nFare_limit = round(Upper_fence)\n\n#Constraining the lower limits of the data\ntrain_data['Fare'] = np.where(train_data['Fare'] > Fare_limit, Fare_limit, train_data['Fare'])\ntest_data['Fare'] = np.where(test_data['Fare'] > Fare_limit, Fare_limit, test_data['Fare'])\ntrain_data['Fare'].max()","004a2453":"#Separating the Age column into 5 different bins \ntrain_data['Age_binned'] = pd.cut(train_data['Age'],5)\ntrain_data['Age_binned'].unique()","34825cc6":"#Applying categorical encoding according to the binned train data\ntrain_data.loc[(train_data['Age'] <= 13.936), 'Age'] = 1\ntrain_data.loc[(train_data['Age'] > 13.936) & (train_data['Age'] <= 27.452), 'Age'] = 2\ntrain_data.loc[(train_data['Age'] > 27.452) & (train_data['Age'] <= 40.968), 'Age'] = 3\ntrain_data.loc[(train_data['Age'] > 40.968) & (train_data['Age'] <= 54.484), 'Age'] = 4\ntrain_data.loc[(train_data['Age'] > 54.484) & (train_data['Age'] <= 68.0), 'Age'] = 5","a8dc55a3":"#Applying categorical encoding to the test data\ntest_data.loc[(test_data['Age'] <= 13.936), 'Age'] = 1\ntest_data.loc[(test_data['Age'] > 13.936) & (test_data['Age'] <= 27.452), 'Age'] = 2\ntest_data.loc[(test_data['Age'] > 27.452) & (test_data['Age'] <= 40.968), 'Age'] = 3\ntest_data.loc[(test_data['Age'] > 40.968) & (test_data['Age'] <= 54.484), 'Age'] = 4\ntest_data.loc[(test_data['Age'] > 54.484) & (test_data['Age'] <= 68.0), 'Age'] = 5","5978c9fd":"train_data.drop('Age_binned', axis = 1, inplace = True)\ntrain_data['Age'] = train_data['Age'].astype(int)\ntest_data['Age'] = test_data['Age'].astype(int)","e9f540ed":"#Separating the Fare column into different bins \ntrain_data['Fare_binned'] = pd.cut(train_data['Fare'],4)\ntrain_data['Fare_binned'].unique()","9cc0dfd5":"#Applying categorical encoding according to the binned data\ntrain_data.loc[(train_data['Fare'] <= 25.00), 'Fare'] = 1\ntrain_data.loc[(train_data['Fare'] > 25.00) & (train_data['Fare'] <= 50.00), 'Fare'] = 2\ntrain_data.loc[(train_data['Fare'] > 50.00) & (train_data['Fare'] <= 75.00), 'Fare'] = 3\ntrain_data.loc[(train_data['Fare'] > 75.00) & (train_data['Fare'] <= 100.00), 'Fare'] = 4","0353d7c6":"#Applying categorical encoding according to the binned test data\ntest_data.loc[(test_data['Fare'] <= 25.00), 'Fare'] = 1\ntest_data.loc[(test_data['Fare'] > 25.00) & (test_data['Fare'] <= 50.00), 'Fare'] = 2\ntest_data.loc[(test_data['Fare'] > 50.00) & (test_data['Fare'] <= 75.00), 'Fare'] = 3\ntest_data.loc[(test_data['Fare'] > 75.00) & (test_data['Fare'] <= 100.00), 'Fare'] = 4","a759d84f":"train_data.drop('Fare_binned', axis = 1, inplace = True)\ntrain_data['Fare'] = train_data['Fare'].astype(int)\ntest_data['Fare'] = test_data['Fare'].astype(int)","abee911e":"plt.figure(figsize = (12,6))\nsns.heatmap(train_data.corr(), annot = True, linewidth = 3, cmap=\"YlGnBu\")","ac989ed6":"features = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'Royalty', 'MarStatus', 'FamilySize', 'isAlone']\nX = train_data[features]\ny = train_data.Survived\nX_final = test_data[features]","8b1298c4":"combine = pd.concat([X,X_final])\ncombine = pd.get_dummies(columns = features, data = combine)\nn_cols = combine.shape[1]\ncombine.head()","f0c4708e":"#Sanity check to see if any missing values exist\nprint(combine.isnull().sum().sum())","cd9f203d":"#Splitting the combined data into train and test set\nX_encoded = combine.iloc[: X.shape[0]]\nX_final_encoded = combine.iloc[X.shape[0] :]","a7755c2c":"#importing Standard Scaler\nfrom sklearn.preprocessing import StandardScaler\n\n#instanting object and fitting the train data\nscaler = StandardScaler()\nscaler.fit(X_encoded)\n\n#scaling the train and test data\nX_scaled = scaler.transform(X_encoded)\nX_final_scaled = scaler.transform(X_final_encoded)","80374f13":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.utils import to_categorical","96e4c049":"def Titanic_model(n):\n    \n    model = Sequential()\n    \n    model.add(Dense(40, activation = 'relu', input_dim = n))\n    model.add(Dense(20, activation = 'relu'))\n    model.add(Dense(10, activation = 'relu'))\n    model.add(Dense(1, activation = 'sigmoid'))\n    \n    model.compile(optimizer = 'adam', \n                  loss = 'binary_crossentropy',\n                  metrics = ['accuracy'])\n    \n    return model","5b005d07":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom statistics import mean\n\nmy_model = Titanic_model(n_cols)\nhistory = my_model.fit(X_scaled, y, \n                       batch_size = 16, epochs  = 100, \n                       validation_split = 0.2, shuffle = True, verbose = 2)","e684e6d2":"history.history.keys()","9628c17a":"#plotting validation and training loss\nplt.plot(history.history['loss'], label = 'training loss')\nplt.plot(history.history['val_loss'], label = 'validation loss')\nplt.legend(loc = 'upper left')\nplt.title('Training and Validation loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')","e907cf18":"#plotting validation and training accuracy\nplt.plot(history.history['accuracy'], label = 'training accuracy')\nplt.plot(history.history['val_accuracy'], label = 'validation accuracy')\nplt.legend(loc = 'upper left')\nplt.title('Training and Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')","9393d5ed":"my_model = Titanic_model(n_cols)\nhistory = my_model.fit(X_scaled, y, batch_size = 16, epochs = 800, verbose = 2)","9071ff1f":"y_pred = my_model.predict_classes(X_final_scaled)\n\noutput = pd.DataFrame(columns = ['PassengerId','Survived'])\n\nfor i in range(0,418):\n    output = output.append({'PassengerId': X_final_encoded.index[i], 'Survived': y_pred[i][0] }, ignore_index = True)\n\noutput.to_csv('Titanic_keras.csv', index=False)","946430d2":"Visualizaing the data distribution of Age and Fare via histograms","a8f8e244":"## Categorical Encoding and Handling Missing Values","ace87ff6":"## Feature Selection","4fda44dd":"The Age and Fare values have been handled by constraining the maximum limit of the data. It can be seen that only the upper limit has been constrained and the lower limit hasn't been touched. This is because, in both cases, the lower limit is negative and in practical situations it is not possible to have negative values of Fare and Age and hence they are automatically constrained to be positive.","06597505":"## One Hot Encoding the complete data","c9a1cfed":"As it can be seen that both the Age and the Fare column has a lot of outliers.\n\nIt is important to note that Outliers can only be present in continous values and hence other columns have been ignored","0a85099f":"## EDA and Data Visualization","26cf1399":"The model performs satisfactorily with a public score of 0.75.","fe8ce5cd":"## Feature Engineering\n\nThe data contains valuable information in the Name column, which could be used to create new features. Similarly, by combining SibSp and Parch, we can estimate the size of the family, which is also another important feature","89eb0490":"## Outlier Detection\n\n\nFor this section, I referred to this kernel - https:\/\/www.kaggle.com\/anuragnegi\/feature-engineering-outliers-handling-ensembling","b76f18c6":"Outlier detection via boxplots","fc2703c3":"It can be seen from the plot that the model is highly overfitting on the validation data. It is performing well on the training data but fails to generate inferences for the validation data.\n\nThere are methods to take care of overfitting but that is not the focus of this kernel, since it's just an introductory one to Keras.","0821f359":"## Data Binning - Age\n\nSeparating the Age column into different groups and hence applying categorical encoding to the data","7a9dac99":"## Making predictions and saving the output","891a3b57":"## Data Binning - Fare\n\nSeparating the Age column into different groups and hence applying categorical encoding to the data","43ca1c8c":"## Data Normalization","6b0b078e":"## Training the Nueral Network and checking for over\/under fitting","dec9fa45":"We will be using all the features, since the number of features are less and training data is also less. \n\nHowever, since FamilySize is very highly corelated with SibSp and Parch, we will be using FamilySize only and not be using the other two","764b0ab4":"Loading the data and checking for missing values","a6c94265":"Handling the ***Age*** column ","3cd76919":"## Training on the whole data with increased epochs","b5fa2bf4":"Creating Family Size feature from SibSp and Parch columns","02554709":"Extracting royal titles and maritial status from the Name column","95ad7b8c":"## Visualizing model output","e28526ea":"## DATA GATHERING","d97f66d4":"## Defining the Neural Network","e7b6f87e":"Checking for new missing values that could be created","d60b098a":"## Outlier Handling","e5a61a6e":"Handling the ***Fare*** column","bb8897bc":"The model performs decently with increased epochs, however it's still overfitting the data.","5994cbee":"No new missing values were created.\n\nThree new features have been created. We will be converting them to usable values in the Categorical Encoding section","6ff71b44":"As it can be seen from the histograms, the ***Age*** values have Normal distribution, while the ***Fare*** values are Left Skewed.\nHence we will be using Gaussian Assumption and Interquartile range calculations respectively to correct the outliers.","ef88f6aa":"### ----- THANK YOU ! ------ ","973446f1":"Creating a new feature called isAlone ","50c13a47":"Using co-relation matrix and heatmap to identify important features"}}