{"cell_type":{"89e34615":"code","51d97a17":"code","d57d6f1f":"code","7e90bdcb":"code","4aae6935":"code","eaebc5c4":"code","98db2214":"code","3dd810a6":"code","f19c1f3c":"code","6d5eccca":"code","28425cfa":"code","4d7b2588":"code","3945c581":"code","513f7aa9":"code","3482e41e":"code","0b82560c":"code","76a9a5a3":"code","e3afcb51":"code","5f3fe608":"code","82653652":"code","9454a5a8":"code","c90bf832":"markdown","68a21737":"markdown","133b61fd":"markdown","51f7ec78":"markdown","1da03587":"markdown","9f747d62":"markdown","8a923f5e":"markdown","e9cb6405":"markdown","02f7f8e5":"markdown","d8773eea":"markdown","89986358":"markdown","054b05a3":"markdown","40d6b49e":"markdown","c73b7590":"markdown"},"source":{"89e34615":"# Let's start importing the necessary python modules.\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nimport tensorflow\nfrom tensorflow import keras\nfrom tensorflow.keras.optimizers import  Adam\nfrom tensorflow.keras.models import Sequential, Model, model_from_json\nfrom tensorflow.keras.layers import Dense, Conv2D, Activation, MaxPool2D, Flatten, Dropout, BatchNormalization\nfrom tensorflow.keras.layers import Reshape,Conv2DTranspose, LeakyReLU, Dropout\nfrom tensorflow.keras.utils import load_img, img_to_array, array_to_img, to_categorical\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport glob\n\nfrom IPython import display\nprint (\"tensorflow version: \", tensorflow.__version__ )","51d97a17":"#let's check the CPU\/GPU devices used and print it to the screen\n\ntensorflow.test.gpu_device_name()\nfrom tensorflow.python.client import device_lib\ndevice_lib.list_local_devices()","d57d6f1f":"# Reading dataset with pandas and import to notebook\ndata= pd.read_csv(\"..\/input\/facial-expression-recognitionferchallenge\/fer2013\/fer2013\/fer2013.csv\") \n\n# checking data shape\ndata.shape","7e90bdcb":"# shows all columns and first 5 rows of dataset\ndata.head() ","4aae6935":"# In the fer2013 dataset, the pixel values in the \"pixels\" column were created with spaces between them. let's arrange\n\nx_train_pixels=data.pixels.str.split(\" \").tolist()   # We remove the spaces and add the pixel values to the list.\nx_train_pixels=pd.DataFrame(x_train_pixels, dtype= int)   # let's convert the pixel list to dataframe","eaebc5c4":"train_images = x_train_pixels.values #Let's assign the pixel values to a new variable\n\nx_train = train_images.astype(np.float)  #Let's change the type of the variable\n\nprint(x_train.shape)  #let's see the variable size","98db2214":"x_train=x_train.reshape(x_train.shape[0],48,48,1)\nx_train.shape","3dd810a6":"# show sample images from dataset\n\nplt.figure(figsize=(10,10))\nfor i in range (16):\n  plt.subplot(4,4,i+1)\n  plt.xticks([])\n  plt.yticks([])\n  plt.grid(False)\n  plt.imshow(x_train[np.random.randint(0,35880)] )\n\nplt.show()","f19c1f3c":"# Discriminator Network\n\ndef create_discriminator(in_shape=(48,48,1)):\n    \n    model= Sequential() # create  discriminator model\n    \n    # 1st convolution layer\n    model.add(Conv2D(64,(3,3), strides=(2,2), padding= \"same\", input_shape= in_shape )) \n    model.add(LeakyReLU(alpha=0.2))\n    \n    # 2nd convolution layer\n    model.add(Conv2D(128,(3,3), strides=(2,2), padding= \"same\"))\n    model.add(LeakyReLU(alpha=0.2))\n    \n    # 3rd convolution layer\n    model.add(Conv2D(128,(3,3), strides=(2,2), padding= \"same\"))\n    model.add(LeakyReLU(alpha=0.2))\n    \n    # 4th convolution layer    \n    model.add(Conv2D(64,(3,3), strides=(2,2), padding= \"same\"))\n    model.add(LeakyReLU(alpha=0.2))\n\n    \n    # Full connected layers\n    model.add(Flatten())\n    model.add(Dropout(0.4))\n    model.add(Dense(1, activation=\"sigmoid\"))\n    \n    #Optimizer\n    opt= Adam(learning_rate= 0.0001, beta_1=0.5)\n    \n    # Once the model is created, you can config the model with losses and metrics with model.compile()\n    model.compile(loss= \"binary_crossentropy\",\n                 optimizer=opt,\n                 metrics=[\"accuracy\"])\n    \n    return model\n\ndiscriminatorx= create_discriminator()  #Assign the discriminator to a variable to display layers\ndiscriminatorx.summary()","6d5eccca":"# Generator Network\n\ndef create_generator( latent_dim):\n    \n    model= Sequential() # create generator model\n    \n    n_nodes=128*6*6 # number of node\n    \n    model.add(Dense(n_nodes, input_dim=latent_dim))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Reshape((6, 6, 128)))\n    \n    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding= \"same\")) \n    model.add(LeakyReLU(alpha=0.2))\n\n    model.add(Conv2DTranspose(64, (4,4), strides=(2,2), padding= \"same\")) \n    model.add(LeakyReLU(alpha=0.2))\n              \n    model.add(Conv2DTranspose(64, (4,4), strides=(2,2), padding= \"same\")) \n    model.add(LeakyReLU(alpha=0.2))\n              \n    model.add(Conv2D(1, (8,8), activation= \"tanh\", padding= \"same\"))\n              \n    return model\n              \ngeneratorx= create_generator(100) # Assign the generator to a variable to display layers\ngeneratorx.summary()","28425cfa":"# Create Generative Adversarial Networks \n\ndef create_gan(generator, discriminator):\n    \n    discriminator.trainable= False  #stop discriminator training\n    \n    model= Sequential() # create GAN model\n    \n    # add generator and discriminator to model\n    \n    model.add(generator)\n    model.add(discriminator)\n    \n    model.compile(loss= \"binary_crossentropy\",\n                 optimizer=Adam(learning_rate= 0.0001, beta_1=0.5))\n    \n    return model","4d7b2588":"# Load training images------> x_train\n\n# normalize pixels to values between -1 and 1 -----> X\n\ndef load_real_samples():\n    x_train\n    X= (x_train.astype(\"float32\")-127.5)\/127.5\n    \n    return X","3945c581":"# set variable values\n\n# size of the latent space\nlatent_dim=100\n\n# calling defined functions and creating models.\ndiscriminator =create_discriminator() \ngenerator = create_generator(latent_dim)\ngan_model = create_gan(generator, discriminator)\n\ndataset= load_real_samples() # new dataset","513f7aa9":"# A random group of images will be selected for gan training\n# We assign 1 tag for each real image and 0 tags for fake.\n\ndef generate_real_samples(dataset, n_samples): # n_sample---> number of sample images\n    \n    ix= np.random.randint(0, dataset.shape[0], n_samples)  # choose random samples\n    \n    X= dataset[ix] \n    \n    y= np.ones((n_samples,1)) # let's assign 1 to real examples\n    \n    return X,y","3482e41e":"# Create latent point\n\ndef generate_latent_points(latent_dim, n_samples):\n\n    x_input = np.random.randn(latent_dim * n_samples) # Create latent point\n\n    x_input = x_input.reshape(n_samples, latent_dim) # reshape latent points for input\n    return x_input","0b82560c":"# Create fake images\n\ndef generate_fake_samples(generator, latent_dim, n_samples):\n    \n    x_input= generate_latent_points(latent_dim, n_samples) #call the function to create latent points\n    \n    X= generator.predict(x_input) # Generating new images from latent points\n    \n    y= np.zeros((n_samples,1)) #assign 0 tags to generated images\n    \n    return X,y","76a9a5a3":"def generate_and_save_images(model, epoch, test_input):\n\n  predictions= model(test_input, training=False) # Model training is stopped to infer all layers.\n\n  fig= plt.figure(figsize=(10,10))\n\n  for i in range(16):\n    plt.subplot(4,4,i+1)\n    plt.imshow(predictions[i,:,:,0]*127.5*127.5 ) #cmap=\"binary\"\n    plt.axis(\"off\")\n\n  plt.savefig(\".\/produced_images_epoch_{:04d}.png\".format(epoch)) # save images produced in each epoch\n  plt.show()","e3afcb51":"n_batch=128 \n\nnum_epochs=int(dataset.shape[0]\/n_batch)\n\nseed= tensorflow.random.normal(shape=[num_epochs,100])\n\nplt.imshow(seed, cmap=\"binary\")#show seed\nplt.axis(\"off\")\nplt.title(\"SEED\")","5f3fe608":"# train the generator and discriminator\n\ndef train (g_model, d_model, gan_model, dataset, latent_dim, n_epochs=5, n_batch=128):\n    \n    batch_per_epoch= int(dataset.shape[0]\/n_batch) # How many batches will be created per epoch.\n    half_batch= int(n_batch\/2) #half of the batches will generate real data in the fake half\n    \n    for i in range (n_epochs):\n        \n        for j in range(batch_per_epoch):\n            \n            # generate images from real dataset,\n            X_real, y_real= generate_real_samples(dataset, half_batch)\n            d_loss_real, _ = d_model.train_on_batch(X_real, y_real)\n            \n            #generate fake images using latent points\n            X_fake, y_fake= generate_fake_samples(g_model,latent_dim, half_batch)\n            d_loss_fake, _ = d_model.train_on_batch(X_fake, y_fake)\n            \n            #generate latent points\n            X_gan= generate_latent_points(latent_dim, n_batch)\n            y_gan= np.ones((n_batch,1))\n            \n            g_loss= gan_model.train_on_batch(X_gan, y_gan)\n\n        display.clear_output(wait=True) # clear output on screen\n        generate_and_save_images(generator, i+1, seed) # generate, display and save images in new epoch\n\n        print( \"epochs: \", i)\n    \n    display.clear_output(wait=True) # clear output on screen\n    generate_and_save_images(generator, i, seed) # generate, display and save images in new epoch\n            \n    g_model.save(\".\/face_generator.h5\") #save network weights","82653652":"train(generator, discriminator, gan_model, dataset, latent_dim, n_epochs= 100) ","9454a5a8":"import imageio\nimport glob\n\nanim_file = '.\/dcgan_face.gif'\n\nwith imageio.get_writer(anim_file, mode='I') as writer:\n  filenames = glob.glob('.\/produced_images_epoch_*.png')\n  filenames = sorted(filenames)\n  last = -1\n  for i,filename in enumerate(filenames):\n    frame = 2*(i)\n    if round(frame) > round(last):\n      last = frame\n    else:\n      continue\n    image = imageio.imread(filename)\n    writer.append_data(image)\n  image = imageio.imread(filename)\n  writer.append_data(image)\n\nfrom IPython.display import Image\nImage(open(anim_file,'rb').read())","c90bf832":"## Load datasets","68a21737":"#### Run this part to show the results as a GIF.\n\nhttps:\/\/www.udemy.com\/course\/derin-ogrenmeye-giris\/learn\/lecture\/20317005#questions\n\nhttps:\/\/www.tensorflow.org\/tutorials\/generative\/dcgan#create_a_gif","133b61fd":"## Deep Convolutional Generative Adversarial Network (DCGAN)","51f7ec78":"### Data preprocessing","1da03587":"### Visualize","9f747d62":"## Discriminator\n\nUse the (as yet untrained) discriminator to classify the generated images as real or fake. The model will be trained to output positive values for real images, and negative values for fake images.","8a923f5e":"# Face generation with DCGAN\n\n![produced_images_epoch_0050.png](attachment:21e04ae4-e285-4354-9000-8444798b8be9.png)\n\n*Figure: Evolution of faces generated in 100 epochs*\n\n## Deep Convolutional Generative Adversarial Network (DCGAN)\n\nA DCGAN is a direct extension of the GAN, except that it explicitly uses convolutional and convolutional-transpose layers in the discriminator and generator, respectively.  It was first described by Radford et. al. in the paper Unsupervised Representation Learning With Deep Convolutional Generative Adversarial Networks. The discriminator is made up of strided convolution layers, batch norm layers, and LeakyReLU activations. For detailed information, you can refer to the following resources.\n* [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https:\/\/arxiv.org\/abs\/1511.06434)\n* [Devoloper.google.com](https:\/\/developers.google.com\/machine-learning\/gan\/generator)\n* [TensorFlow](https:\/\/www.tensorflow.org\/tutorials\/generative\/dcgan)\n* [PyTorch](https:\/\/pytorch.org\/tutorials\/beginner\/dcgan_faces_tutorial.html)\n\n\n## FER2013 Dataset:\n\nThe data consists of 48x48 pixel grayscale images of faces. The faces have been automatically registered so that the face is more or less centred and occupies about the same amount of space in each image. The task is to categorize each face based on the emotion shown in the facial expression into one of seven categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral). The training set consists of 28,709 examples and the public test set consists of 3,589 examples. A large amount of data is required for generative adverserial network training. That's why we combined training, testing and validation data. We created a training set of ***35887 images*** in total.\n\n*In this tutorial, new face images will be produced using the  *Facial Expression Recognition (FER2013)* dataset. Since this data set includes different emotional states, the data diversity is quite high. This is positive for GAN training. However, since not all photos are taken from the same angle, the success of face production may be affected.*\n\n\n\n","e9cb6405":"For detailed information about [tensorflow and keras libraries](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras), please go to the link. It contains very useful information for the use of modules.","02f7f8e5":"## Generator\n\nThe generator uses tf.keras.layers.Conv2DTranspose (upsampling) layers to produce an image from a seed (random noise). Start with a Dense layer that takes this seed as input, then upsample several times until you reach the desired image size of 48x48x1. Notice the tf.keras.layers.LeakyReLU activation for each layer, except the output layer which uses tanh.","d8773eea":"## Reference\n \n* https:\/\/github.com\/bnsreenu\/python_for_microscopists\/blob\/master\/248_keras_implementation_of_GAN\/248-cifar_GAN.py\n\n* https:\/\/github.com\/ayyucekizrak\/GAN_UreticiCekismeliAglar_ile_SentetikVeriUretme\n\n* https:\/\/www.tensorflow.org\/tutorials\/generative\/dcgan","89986358":"## Training","054b05a3":"## Import modules","40d6b49e":"### Model train function\n","c73b7590":"**Prepared by Nurullah Yuksel**"}}