{"cell_type":{"6e54f498":"code","747c73fd":"code","0c003d90":"code","035fc5b3":"code","22d71753":"code","e18fb28f":"code","378ce0b4":"code","31336bac":"code","33822e24":"code","7f892647":"code","ed9a491c":"code","e77f2682":"code","7d78c0e3":"code","1740198d":"code","3db678ca":"code","cbf8bcb2":"code","dfe150bf":"code","e434eac4":"code","feec4782":"code","c0794a6a":"code","d76b05a9":"code","c13cc762":"code","5c7784d3":"code","5cd3681a":"code","fcf2f870":"code","0d141bff":"code","d16129e8":"code","30eb9d2b":"code","175fc876":"code","44398e19":"code","551aa0b4":"code","e2a27ec9":"code","318ba93d":"code","d62d0528":"code","633504fd":"code","bd61b30d":"code","c271d10c":"code","afafbf82":"code","e9fb0058":"code","44fb18e7":"code","f820744f":"code","043d2b99":"code","cc73a04c":"code","2a45d4aa":"code","5f0c996f":"code","829d698d":"code","5646a33b":"markdown","4f3c1913":"markdown","a033c5d7":"markdown","6d1aa491":"markdown","22ea97d3":"markdown","ddad4750":"markdown","db32b8be":"markdown","2b39146f":"markdown"},"source":{"6e54f498":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","747c73fd":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0c003d90":"file = '\/kaggle\/input\/iris-flower-dataset\/IRIS.csv'\ndf = pd.read_csv(file)","035fc5b3":"df.head()","22d71753":"df.shape # Understand the row and column of dataset","e18fb28f":"df.isnull().sum() # Check the null value of dataset","378ce0b4":"df['species'].value_counts() # check how many categorical values present","31336bac":"df['species'].value_counts().plot.bar()","33822e24":"sns.countplot(df['species'])","7f892647":"sns.histplot(df['sepal_length'])","ed9a491c":"sns.distplot(df['sepal_length'])","e77f2682":"sns.barplot(x=df['species'],y=df['sepal_length'])","7d78c0e3":"sns.distplot(df['sepal_width'])","1740198d":"sns.barplot(x=df['species'],y=df['sepal_width'])","3db678ca":"sns.distplot(df['petal_length'])","cbf8bcb2":"sns.barplot(x=df['species'],y=df['petal_length'])","dfe150bf":"sns.distplot(df['petal_width'])","e434eac4":"sns.barplot(x=df['species'],y=df['petal_width'])","feec4782":"# We have observed that petal length and petal width are not normalised so i can handle in later stages also handle\n# target varibale","c0794a6a":"fig, ax = plt.subplots(figsize=(15,10))\nsns.boxplot(data=df, width= 0.5,  fliersize=6)\n","d76b05a9":"df['species']=df['species'].map({'Iris-setosa':0 ,'Iris-versicolor':1,'Iris-virginica':2 })\n","c13cc762":"df['species'].value_counts()","5c7784d3":"df.corr()","5cd3681a":"X = df.drop(columns = ['species'])\ny = df['species']","fcf2f870":"# let's see how data is distributed for every column\nplt.figure(figsize=(20,25), facecolor='white')\nplotnumber = 1\n\nfor column in X:\n    if plotnumber<=9 :\n        ax = plt.subplot(3,3,plotnumber)\n        sns.stripplot(y,X[column])\n    plotnumber+=1\nplt.tight_layout()","0d141bff":"from sklearn.preprocessing import StandardScaler \nscalar = StandardScaler()\nX_scaled = scalar.fit_transform(X)","d16129e8":"from sklearn.linear_model  import Ridge,Lasso,RidgeCV, LassoCV, ElasticNet, ElasticNetCV, LogisticRegression\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor \nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score","30eb9d2b":"vif = pd.DataFrame()\nvif[\"vif\"] = [variance_inflation_factor(X_scaled,i) for i in range(X_scaled.shape[1])]\nvif[\"Features\"] = X.columns\n\n#let's check the values\nvif","175fc876":"X_new = df[['sepal_length','sepal_width','petal_width']]","44398e19":"X_scaled_new = scalar.fit_transform(X_new)","551aa0b4":"vif = pd.DataFrame()\nvif[\"vif\"] = [variance_inflation_factor(X_scaled_new,i) for i in range(X_scaled_new.shape[1])]\nvif[\"Features\"] = X_new.columns\n\n#let's check the values\nvif","e2a27ec9":"x_train,x_test,y_train,y_test = train_test_split(X_scaled_new,y, test_size= 0.25, random_state = 355)","318ba93d":"log_reg = LogisticRegression()\n\nlog_reg.fit(x_train,y_train)","d62d0528":"y_pred = log_reg.predict(x_test)","633504fd":"accuracy = accuracy_score(y_test,y_pred)\naccuracy","bd61b30d":"# Confusion Matrix\nconf_mat = confusion_matrix(y_test,y_pred)\nconf_mat","c271d10c":"from sklearn.tree import DecisionTreeClassifier, export_graphviz\nclf = DecisionTreeClassifier()\nclf.fit(x_train,y_train)","afafbf82":"clf.score(x_test,y_test)","e9fb0058":"grid_param = {\n    'criterion': ['gini', 'entropy'],\n    'max_depth' : range(2,32,1),\n    'min_samples_leaf' : range(1,10,1),\n    'min_samples_split': range(2,10,1),\n    'splitter' : ['best', 'random']\n    \n}","44fb18e7":"grid_search = GridSearchCV(estimator=clf,\n                     param_grid=grid_param,\n                     cv=5,\n                    n_jobs =-1)","f820744f":"grid_search.fit(x_train,y_train)","043d2b99":"best_parameters = grid_search.best_params_\nprint(best_parameters)","cc73a04c":"grid_search.best_score_","2a45d4aa":"# Now try this parameter","5f0c996f":"clf = DecisionTreeClassifier(criterion = 'gini', max_depth =21, min_samples_leaf= 4, min_samples_split= 4, splitter ='random')\nclf.fit(x_train,y_train)","829d698d":"clf.score(x_test,y_test) # But possible may be low bias and high variance present for unknown dataset","5646a33b":"# Feature Selection","4f3c1913":"# Feature Engineering","a033c5d7":"Great!! Our test score has improved after using Gridsearch.\n\nNote : we must understand that giving all the hyperparameters in the gridSearch doesn't gurantee the best result. We have to do hit and trial with parameters to get the perfect score.\n\n","6d1aa491":"feature of sepal_length ,petal_length and petal_width have highest multicollinearity so we can delete this","22ea97d3":"\nAll the VIF values are less than 5 and are very low. That means no multicollinearity. Now, we can go ahead with fitting our data to the model. Before that, let's split our data in test and training set.","ddad4750":"\nAll the VIF values are less than 5 and are very low. That means no multicollinearity. Now, we can go ahead with fitting our data to the model. Before that, let's split our data in test and training set.","db32b8be":"# EXPLORATORY DATA ANALYSIS","2b39146f":"Let's now try to tune some hyperparameters using the GridSearchCV algorithm.\n\nGridSearchCV is a method used to tune our hyperparameters. We can pass different values of hyperparameters as parameters for grid search. It does a exhaustive generation of combination of different parameters passed. Using cross validation score, Grid Search returns the combination of hyperparameters for which the model is performing the best."}}