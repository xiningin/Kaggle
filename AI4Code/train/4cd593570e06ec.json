{"cell_type":{"278c9132":"code","4626bc26":"code","63db1e8b":"code","b9f7c2c5":"code","5e6194bb":"code","7859393f":"code","f1d8c58d":"code","09ff8b57":"code","36eec183":"code","264c83c6":"code","e2c18460":"code","fcc0e489":"code","5388b223":"code","22fc3dd6":"code","ff416381":"code","e6b483f3":"code","48e7ab58":"code","168830b9":"code","a41e2061":"code","5dc14430":"code","2685aba8":"code","502ac4f3":"code","6e6fe833":"code","e33643db":"code","69726c7f":"code","0f2a3a1f":"code","84770cb0":"markdown","9405fa11":"markdown","9663fb84":"markdown","4f7d55f4":"markdown","eada505c":"markdown","31aead2a":"markdown","806a2ca8":"markdown"},"source":{"278c9132":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport pandas_profiling as pp\nfrom sklearn.preprocessing import StandardScaler, Normalizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,accuracy_score,roc_curve,classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom imblearn.over_sampling import SMOTE\n#ensembling\nfrom mlxtend.classifier import StackingCVClassifier","4626bc26":"data = pd.read_csv('..\/input\/heart-disease-prediction-using-logistic-regression\/framingham.csv')\ndata.head()","63db1e8b":"# data.dropna(inplace = True) # ignore na\ndata = data.apply(lambda x: x.fillna(x.mean())) #replace na with mean\ntarget = data[\"TenYearCHD\"]\nfeatures = data.drop('TenYearCHD',axis=1)\nsns.histplot(target)\ndata.info()","b9f7c2c5":"pp.ProfileReport(data)","5e6194bb":"smote = SMOTE()\nx_smote ,y_smote = smote .fit_resample(features, target)\nsns.histplot(y_smote)","7859393f":"x_train, x_test, y_train, y_test = train_test_split(features, target, test_size = 0.2, random_state = 42, shuffle = True)\nxs_train ,xs_test ,ys_train ,ys_test = train_test_split(x_smote ,y_smote , test_size = 0.2 , random_state = 42 ,shuffle = True) ","f1d8c58d":"kernels = ['rbf', 'poly', 'sigmoid']\nbest_svm = None\nbest_score = 0\nbest_pred = None\nall_results = []\nfor kernel in kernels:\n    print(\"Using kernel =\", kernel)\n    for c in range(1, 11):\n        print(\"\\tusing C={}\".format(c), end = \" \")\n        svc =  SVC(kernel=kernel, C=c, probability = True)\n        svc.fit(x_train, y_train)\n        y_pred = svc.predict(x_test)\n        score = accuracy_score(y_test, y_pred)\n        all_results.append([c, score, kernel])\n        print(\"score = \", score)\n        if score > best_score:\n            best_score = score\n            best_svm = svc\n            best_pred = y_pred\nall_results = np.array(all_results, dtype=object)","09ff8b57":"print(\"Best score using kernel={} with c = {}\".format(best_svm.kernel, best_svm.C))\ncm = confusion_matrix(y_test,best_pred)\nprint(classification_report(y_test,best_pred))\nprint(\"Accuracy score\", accuracy_score(y_test, best_pred))\nsns.heatmap(cm\/np.sum(cm), annot=True, fmt = '.2%', cmap = 'Blues')","36eec183":"kernels = ['rbf', 'poly', 'sigmoid']\nbest_svm_smote = None\nbest_score_smote = 0\nbest_pred_smote = None\nall_results_smote = []\nfor kernel in kernels:\n    print(\"Using kernel =\", kernel)\n    for c in range(1, 11):\n        print(\"\\tusing C={}\".format(c), end = \" \")\n        svc =  SVC(kernel=kernel, C=c, probability = True)\n        svc.fit(xs_train, ys_train)\n        ys_pred = svc.predict(xs_test)\n        score = accuracy_score(ys_test, ys_pred)\n        all_results_smote.append([c, score, kernel])\n        print(\"score = \", score)\n        if score > best_score_smote:\n            best_score_smote = score\n            best_svm_smote = svc\n            best_pred_smote = y_pred\nall_results_smote = np.array(all_results_smote, dtype=object)","264c83c6":"print(\"Best score using kernel={} with c = {}\".format(best_svm_smote.kernel, best_svm_smote.C))\ncm = confusion_matrix(y_test,best_pred_smote)\nprint(classification_report(y_test,best_pred_smote))\nprint(\"Accuracy score\", accuracy_score(y_test, best_pred_smote))\nsns.heatmap(cm\/np.sum(cm), annot=True, fmt = '.2%', cmap = 'Blues')","e2c18460":"plt.plot(all_results[:10][:, 0], all_results[:10][:, 1], label = \"kernel(RBF)\")\nplt.plot(all_results[10:20][:, 0], all_results[10:20][:, 1], label = \"kernel(Poly)\")\nplt.plot(all_results[20:][:, 0], all_results[20:][:, 1], label = \"kernel(sigmoid)\")\nplt.plot(all_results_smote[:10][:, 0], all_results_smote[:10][:, 1], label = \"kernel(RBF)-SMOTE\")\nplt.plot(all_results_smote[10:20][:, 0], all_results_smote[10:20][:, 1], label = \"kernel(Poly)-SMOTE\")\nplt.plot(all_results_smote[20:][:, 0], all_results_smote[20:][:, 1], label = \"kernel(sigmoid)-SMOTE\")\nplt.title(\"Scores on SVM by C value\")\nplt.ylabel(\"score\")\nplt.xticks(range(1, 11))\nplt.xlabel(\"C\")\nplt.legend()\nplt.show()","fcc0e489":"all_results = []\nbest_lr = None\nbest_score = 0\nbest_pred = None\nworst_param = None\nfor param in range((x_train.shape[1]) + 1):\n    if param != x_train.shape[1]:\n        print(\"Without using param\",features.columns[param], end = \" \")\n        train = x_train.drop(x_train.columns[param], axis=1)\n        test = x_test.drop(x_test.columns[param], axis=1)\n    else:\n        print(\"Using all parameters\", end = \" \")\n    lr_model = LogisticRegression(random_state = 42, max_iter = 10000)\n    lr_model.fit(train , y_train)\n    y_pred = lr_model.predict(test)\n    score = accuracy_score(y_test, y_pred)\n    all_results.append([param, score])\n    print(\"score\", score)\n    if score > best_score:\n        best_lr = lr_model\n        best_score = score\n        best_pred = y_pred\n        worst_param = param\nprint(\"Best without using\", features.columns[worst_param])\ncm = confusion_matrix(y_test,best_pred)\nprint(classification_report(y_test,best_pred))\nsns.heatmap(cm\/np.sum(cm), annot=True, fmt = '.2%', cmap = 'Blues')\nall_results = np.array(all_results, dtype=object)","5388b223":"all_results_smote = []\nbest_lr_smote = None\nbest_score_smote = 0\nbest_pred_smote = None\nworst_param = None\nfor param in range((xs_train.shape[1]) + 1):\n    if param != xs_train.shape[1]:\n        print(\"Without using param\",xs_train.columns[param], end = \" \")\n        train = xs_train.drop(xs_train.columns[param], axis=1)\n        test = xs_test.drop(xs_test.columns[param], axis=1)\n    else:\n        print(\"Using all parameters\", end = \" \")\n    lr_model = LogisticRegression(random_state = 42, max_iter = 10000)\n    lr_model.fit(train , ys_train)\n    ys_pred = lr_model.predict(test)\n    score = accuracy_score(ys_test, ys_pred)\n    all_results_smote.append([param, score])\n    print(\"score\", score)\n    if score > best_score_smote:\n        best_lr_smote = lr_model\n        best_score_smote = score\n        best_pred_smote = ys_pred\n        worst_param = param\nprint(\"Best without using\", features.columns[worst_param])\ncm = confusion_matrix(ys_test,best_pred_smote)\nprint(classification_report(ys_test,best_pred_smote))\nsns.heatmap(cm\/np.sum(cm), annot=True, fmt = '.2%', cmap = 'Blues')\nall_results_smote = np.array(all_results_smote, dtype=object)","22fc3dd6":"labels = np.insert(x_train.columns, len(x_train.columns), \"None\")\nplt.plot(all_results[:, 0], all_results[:, 1], label = \"Original data\")\nplt.plot(all_results_smote[:, 0], all_results_smote[:, 1], label = \"after SMOTE\")\nplt.title(\"Scores on SVM by C value\")\nplt.ylabel(\"score\")\nplt.xticks(range(len(labels)), labels = labels, rotation=90)\nplt.xlabel(\"parameter removed\")\nplt.legend()\nplt.show()","ff416381":"criterions = ['gini', 'entropy']\nbest_score = -1\nbest_pred = []\nbest_forest = None\nall_results = []\nfor criterion in criterions:\n    print(\"Using\", criterion)\n    for estimators in range(10, 201, 10):\n        print(\"\\t{} estimators\".format(estimators), end = \" \")\n        forest = RandomForestClassifier(n_estimators=estimators, criterion = criterion, random_state = 42)\n        forest.fit(x_train, y_train)\n        y_pred = forest.predict(x_test)\n        score = accuracy_score(y_test, y_pred)\n        all_results.append([forest, y_pred, score, estimators])\n        print(\"score = {}\".format(score))\n        if score > best_score:\n            best_pred = y_pred\n            best_score = score\n            best_forest = forest\nall_results = np.array(all_results, dtype=object)","e6b483f3":"criterions = ['gini', 'entropy']\nbest_score_smote = -1\nbest_pred_smote = []\nbest_forest_smote = None\nall_results_smote = []\nfor criterion in criterions:\n    print(\"Using\", criterion)\n    for estimators in range(10, 201, 10):\n        print(\"\\t{} estimators\".format(estimators), end = \" \")\n        forest = RandomForestClassifier(n_estimators=estimators, criterion = criterion, random_state = 42)\n        forest.fit(xs_train, ys_train)\n        ys_pred = forest.predict(xs_test)\n        score = accuracy_score(ys_test, ys_pred)\n        all_results_smote.append([forest, ys_pred, score, estimators])\n\n        print(\"score = {}\".format(score))\n        if score > best_score_smote:\n            best_pred_smote = ys_pred\n            best_score_smote = score\n            best_forest_smote = forest\nall_results_smote = np.array(all_results_smote, dtype=object)","48e7ab58":"plt.plot(all_results[:20][:, 3], all_results[:20][:, 2], label = \"Gini\")\nplt.plot(all_results[20:][:, 3], all_results[20:][:, 2], label = 'Entropy')\nplt.plot(all_results_smote[:20][:, 3], all_results_smote[:20][:, 2], label = \"Gini - smote\")\nplt.plot(all_results_smote[20:][:, 3], all_results_smote[20:][:, 2], label = 'Entropy - smote')\nplt.title(\"Scores on random forest by number of estimators(using smote)\")\nplt.ylabel(\"score\")\nplt.xlabel(\"number of estimators\")\nplt.legend()\nplt.show()","168830b9":"print(\"\\t\\tOriginal data\")\nprint(\"Best score using criterion={} with {} estimators\".format(best_forest.criterion, len(best_forest\n                                                                                         .estimators_)))\ncm = confusion_matrix(y_test,best_pred)\nprint(classification_report(y_test,best_pred))\nprint(\"Accuracy score\", accuracy_score(y_test, best_pred))\nsns.heatmap(cm\/np.sum(cm), annot=True, fmt = '.2%', cmap = 'Blues')","a41e2061":"print(\"\\t\\tafter smote\")\n\nprint(\"Best score using criterion={} with {} estimators\".format(best_forest_smote.criterion, len(best_forest_smote\n                                                                                         .estimators_)))\ncm = confusion_matrix(ys_test,best_pred_smote)\nprint(classification_report(ys_test,best_pred_smote))\nprint(\"Accuracy score\", accuracy_score(ys_test, best_pred_smote))\nsns.heatmap(cm\/np.sum(cm), annot=True, fmt = '.2%', cmap = 'Blues')","5dc14430":"best_score = -1\nbest_pred = []\nbest_knn = None\nall_results = []\nfor p in [1, 2]:\n    print(\"Using l{}\".format(p))\n    for k in range(2, 31):\n        print(\"\\tUsing k={}\".format(k), end = ' ')\n        knn = KNeighborsClassifier(n_neighbors=k, p = p)\n        knn.fit(x_train, y_train)\n        y_pred = knn.predict(x_test)\n        score = accuracy_score(y_test, y_pred)\n        all_results.append([k, score])\n        print(\"score = {}\".format(score))\n        if score > best_score:\n            best_pred = y_pred\n            best_score = score\n            best_knn = knn\nall_results = np.array(all_results, dtype=object)","2685aba8":"best_score_smote = -1\nbest_pred_smote = []\nbest_knn_smote = None\nall_results_smote = []\nfor p in [1, 2]:\n    print(\"Using l{}\".format(p))\n    for k in range(2, 31):\n        print(\"\\tUsing k={}\".format(k), end = ' ')\n        knn = KNeighborsClassifier(n_neighbors=k, p = p)\n        knn.fit(xs_train, ys_train)\n        ys_pred = knn.predict(xs_test)\n        score = accuracy_score(ys_test, ys_pred)\n        all_results_smote.append([k, score])\n        print(\"score = {}\".format(score))\n        if score > best_score_smote:\n            best_pred_smote = ys_pred\n            best_score_smote = score\n            best_knn_smote = knn\nall_results_smote = np.array(all_results_smote, dtype=object)","502ac4f3":"plt.plot(all_results[:29][:, 0], all_results[:29][:, 1], label = \"l1\")\nplt.plot(all_results[29:][:, 0], all_results[29:][:, 1], label = 'l2')\nplt.plot(all_results_smote[:29][:, 0], all_results_smote[:29][:, 1], label = 'l1 - smote')\nplt.plot(all_results_smote[29:][:, 0], all_results_smote[29:][:, 1], label = 'l2 - smote')\nplt.title(\"Scores on KNN by k value\")\nplt.ylabel(\"score\")\nplt.xticks(range(2, 31), rotation=90)\nplt.xlabel(\"k\")\nplt.legend()\nplt.show()","6e6fe833":"print(\"\\t\\tOriginal data\")\nprint(\"Best score using l{} with {} neighbors\".format(best_knn.p, best_knn.n_neighbors))\ncm = confusion_matrix(y_test,best_pred)\nprint(classification_report(y_test,best_pred))\nprint(\"Accuracy score\", accuracy_score(y_test, best_pred))\nsns.heatmap(cm\/np.sum(cm), annot=True, fmt = '.2%', cmap = 'Blues')","e33643db":"print(\"\\t\\tAfter smote\")\nprint(\"Best score using l{} with {} neighbors\".format(best_knn_smote.p, best_knn_smote.n_neighbors))\ncm = confusion_matrix(ys_test,best_pred_smote)\nprint(classification_report(ys_test,best_pred_smote))\nprint(\"Accuracy score\", accuracy_score(ys_test, best_pred_smote))\nsns.heatmap(cm\/np.sum(cm), annot=True, fmt = '.2%', cmap = 'Blues')","69726c7f":"ensemble = [best_forest_smote, best_knn, best_lr, best_svm]\npred = []\nfor model in ensemble:\n    print(model)\n    if model is best_lr and worst_param is not features.shape[1]:\n        pred.append(model.predict_proba(x_test.drop(x_test.columns[worst_param], axis=1)))\n    else:\n        pred.append(model.predict_proba(x_test))\nprobs = sum(pred)\/len(ensemble)\nfinal_pred = [0 if p[0] > p[1] else 1 for p in probs]","0f2a3a1f":"cm = confusion_matrix(y_test,final_pred)\nprint(classification_report(y_test,final_pred))\nprint(accuracy_score(y_test, final_pred))\nsns.heatmap(cm\/np.sum(cm), annot=True, fmt = '.2%', cmap = 'Blues')","84770cb0":"# Logistic Regression\n\n## Original Data","9405fa11":"## Using data after oversampling","9663fb84":"# Random forest, different criterions and #estimators\u00b6\n## Using the original dataset","4f7d55f4":"# KNN - different K's\n## Original dataset","eada505c":"## Using data after oversampling","31aead2a":"# SVM \n## Original Data","806a2ca8":"## Using data after oversampling"}}