{"cell_type":{"24f68e6e":"code","b77793cc":"code","10e970e5":"code","a07face3":"code","e2bb0a8e":"code","b5c7f9aa":"code","da1acefd":"code","e69e112a":"code","1c9081bb":"code","ae5dd0c7":"code","144797fd":"code","9af9af28":"code","5a0e06bd":"code","4c77ac8f":"code","112808b3":"code","3889af34":"code","94b608e3":"code","33d2b09f":"code","2938bb33":"code","daba701f":"code","931ff597":"code","24e7123d":"code","51cbc4b0":"code","9f80a9b6":"code","94cc83b6":"code","cc1b2a37":"code","198dd338":"code","f5842291":"code","d20d43bc":"code","7503f543":"code","ac55d24c":"markdown","ae69dfc0":"markdown","32ef2183":"markdown","f91d1ff5":"markdown","a29693fd":"markdown","2aaf0a5a":"markdown","61ebf218":"markdown","b7583b43":"markdown","16aba45f":"markdown","23e5a8a0":"markdown","697a9ac7":"markdown","2b873fd9":"markdown","74e9ed69":"markdown","fcd6c658":"markdown","aa3ceefa":"markdown","ef9bf0a5":"markdown","310c62f4":"markdown","1bf4e9a0":"markdown","463ab95f":"markdown","ce30262f":"markdown","8ae88914":"markdown"},"source":{"24f68e6e":"import pandas as pd\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn","b77793cc":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","10e970e5":"train_ID = train['Id']\ntest_ID = test['Id']\n\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)","a07face3":"corrmat = train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","e2bb0a8e":"fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","b5c7f9aa":"train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\nfig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","da1acefd":"from scipy.stats import skew, kurtosis\nsns.distplot(train['SalePrice']);\nprint(train[\"SalePrice\"].skew(),\"   \", train[\"SalePrice\"].kurtosis())","e69e112a":"train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\nsns.distplot(train['SalePrice']);","1c9081bb":"print(train[\"SalePrice\"].skew(),\"  \", train[\"SalePrice\"].kurtosis())","ae5dd0c7":"corr = train.corrwith(train['SalePrice']).abs().sort_values(ascending=False)[2:]\n\ndata = go.Bar(x=corr.index, \n              y=corr.values )\n       \nlayout = go.Layout(title = 'Which variables are most correlated to Sale Price?', \n                   xaxis = dict(title = ''), \n                   yaxis = dict(title = 'correlation'),\n                   autosize=False, width=750, height=500,)\n\nfig = dict(data = [data], layout = layout)\niplot(fig)","144797fd":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)","9af9af28":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data","5a0e06bd":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","4c77ac8f":"all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\nall_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')\nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\nall_data = all_data.drop(['Utilities'], axis=1)\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")","112808b3":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","3889af34":"all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","94b608e3":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))","33d2b09f":"all_data = pd.get_dummies(all_data)\n\ntrain = all_data[:ntrain]\ntest = all_data[ntrain:]","2938bb33":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","daba701f":"def rmsle(model):\n    kfold = KFold(5, shuffle = True, random_state = 1).get_n_splits(train.values)\n    rmse = np.sqrt(-cross_val_score(model, train.values, y_train, scoring = \"neg_mean_squared_error\", cv = kfold))\n    return(rmse)","931ff597":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree = 2, coef0 = 2.5)\nKRR_score = rmsle(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(KRR_score.mean(), KRR_score.std()))","24e7123d":"lasso = make_pipeline(RobustScaler(), Lasso(alpha = 0.0005, random_state = 1))\nlasso_score = rmsle(lasso)\nprint(\"Lasso score: {:.4f} ({:.4f})\\n\".format(lasso_score.mean(), lasso_score.std()))","51cbc4b0":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=1))\nENet_score = rmsle(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(ENet_score.mean(), ENet_score.std()))","9f80a9b6":"GBoost = GradientBoostingRegressor(n_estimators = 3000, learning_rate = 0.05,\n                                   max_depth = 4, max_features = 'sqrt',\n                                   min_samples_leaf = 15, min_samples_split = 10, \n                                   loss = 'huber', random_state = 1)\nGBoost_score = rmsle(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(GBoost_score.mean(), GBoost_score.std()))","94cc83b6":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nXGB_score = rmsle(model_xgb)\nprint(\"XGBoost score: {:.4f} ({:.4f})\\n\".format(XGB_score.mean(), XGB_score.std()))","cc1b2a37":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\nlgb_score = rmsle(model_lgb)\nprint(\"LGB score: {:.4f} ({:.4f})\\n\".format(lgb_score.mean(), lgb_score.std()))","198dd338":"bayridge = BayesianRidge(compute_score=True)\nbayridge_score = rmsle(bayridge)\nprint(\"Bayesian Ridge score: {:.4f} ({:.4f})\\n\".format(bayridge_score.mean(), bayridge_score.std()))","f5842291":"LassoMd = lasso.fit(train.values,y_train)\nENetMd = ENet.fit(train.values,y_train)\nGBoostMd = GBoost.fit(train.values,y_train)\nXGBMd = model_xgb.fit(train.values,y_train)\nLGBMd = model_lgb.fit(train.values,y_train)\nBayRidgeMd = bayridge.fit(train.values,y_train)","d20d43bc":"final_model = (np.expm1(LassoMd.predict(test.values)) + np.expm1(ENetMd.predict(test.values)) + np.expm1(GBoostMd.predict(test.values)) + np.expm1(XGBMd.predict(test.values)) + np.expm1(LGBMd.predict(test.values)) + np.expm1(BayRidgeMd.predict(test.values)) ) \/ 6","7503f543":"submission = pd.DataFrame()\nsubmission['Id'] = test_ID\nsubmission['SalePrice'] = final_model\nsubmission.to_csv('submission.csv',index=False)","ac55d24c":"Plot of the distribution of the Sale Price and observe the skew and kurtosis","ae69dfc0":"Based on the errors, we will select all of the models, except for Kernel Ridge, and take their averages","32ef2183":"Take the log of the Sale Price in order to center the distribution","f91d1ff5":"Plot the variables most correlated to SalePrice Log in descending order","a29693fd":"Plot the features by their missing data percentages in descending order","2aaf0a5a":"Get dummy variables and split the merged data into training and test sets","61ebf218":"Concatenate both train and test values.","b7583b43":"With the dataset of housing prices, we will be:\n\n1. Loading our data.\n2. Exploring the variables with visualization.\n3. Cleaning the data.\n4. Transforming the features.\n5. Utilizing different regression models for our prediction.\n","16aba45f":"Calculate the percentage of missing values in the dataset per feature.","23e5a8a0":"Import libraries for modeling","697a9ac7":"Check if any missing values still exist","2b873fd9":"Remove the outliers and show the new scatterplot","74e9ed69":"Create validation function with 5-fold validation","fcd6c658":"We shall generate a scatter plot of the GrLivArea vs SalePrice.","aa3ceefa":"Check the skew and kurtosis after the log scale","ef9bf0a5":"Modify data types by converting them into strings","310c62f4":"Now, we replace the missing values.","1bf4e9a0":"We will use the following regression models: Kernel Ridge, Lasso, ElasticNet, Gradient Boost, XGBoost, LighBGM, and Bayesian Ridge","463ab95f":"Encode the labels in order to make categorical data more understandable.","ce30262f":"Save the ID columns for later and remove them from the datasets.","8ae88914":"Generate a correlation matrix for all variables"}}