{"cell_type":{"8d7566d5":"code","01cd7af0":"code","8dfa741f":"code","93ccb1ea":"code","864a0ddc":"code","201e71da":"code","9ecbbc0a":"code","bdeb44d8":"code","ee9856d7":"code","2b5f3bfd":"code","9caa9898":"code","6bbf5a62":"code","40cd11da":"markdown","4ef5f3e9":"markdown","8a57ac65":"markdown","2efd8bb4":"markdown","80ba6414":"markdown","a897df41":"markdown","60384c02":"markdown"},"source":{"8d7566d5":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport os\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Model","01cd7af0":"rock_dir = os.path.join('..\/input\/rock-paper-scissor\/rps\/rps\/rock')\npaper_dir = os.path.join('..\/input\/rock-paper-scissor\/rps\/rps\/paper')\nscissors_dir = os.path.join('..\/input\/rock-paper-scissor\/rps\/rps\/scissors')\n\nprint('Total training rock images:', len(os.listdir(rock_dir)))\nprint('\\nTotal training paper images:', len(os.listdir(paper_dir)))\nprint('\\nTotal training scissors images:', len(os.listdir(scissors_dir)))","8dfa741f":"rock_files = os.listdir(rock_dir)\nprint(\"Some Rock File Names\\n\", rock_files[:5])\n\npaper_files = os.listdir(paper_dir)\nprint(\"\\nSome Paper File Names\\n\",paper_files[:5])\n\nscissors_files = os.listdir(scissors_dir)\nprint(\"\\nSome Scissor File Names\\n\",scissors_files[:5])","93ccb1ea":"import matplotlib.image as mpimg\n\npic_index = 3\n\nnext_rock = [os.path.join(rock_dir, fname) \n                for fname in rock_files[pic_index-2:pic_index]]\nnext_paper = [os.path.join(paper_dir, fname) \n                for fname in paper_files[pic_index-2:pic_index]]\nnext_scissors = [os.path.join(scissors_dir, fname) \n                for fname in scissors_files[pic_index-2:pic_index]]\n\nfor i, img_path in enumerate(next_rock+next_paper+next_scissors):\n    img = mpimg.imread(img_path)\n    plt.imshow(img)\n    plt.axis('Off')\n    plt.show()","864a0ddc":"import tensorflow as tf\nimport keras_preprocessing\nfrom keras_preprocessing import image\nfrom keras_preprocessing.image import ImageDataGenerator","201e71da":"TRAINING_DIR = \"..\/input\/rock-paper-scissor\/rps\/rps\/\"\ntraining_datagen = ImageDataGenerator(\n        rescale = 1.\/255,\n        rotation_range=40,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        fill_mode='nearest')\n\nVALIDATION_DIR = \"..\/input\/rock-paper-scissor\/rps-test-set\/rps-test-set\/\"\nvalidation_datagen = ImageDataGenerator(rescale = 1.\/255)\n\ntrain_generator = training_datagen.flow_from_directory(\n    TRAINING_DIR,\n    target_size=(150,150),\n    class_mode='categorical',\n  batch_size=100\n)\n\nvalidation_generator = validation_datagen.flow_from_directory(\n    VALIDATION_DIR,\n    target_size=(150,150),\n    class_mode='categorical',\n    batch_size=50\n)","9ecbbc0a":"!wget --no-check-certificate \\\n    https:\/\/storage.googleapis.com\/mledu-datasets\/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n    -O \/tmp\/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\n\nlocal_weights_file = '\/tmp\/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n\npre_trained_model = InceptionV3(input_shape = (150, 150, 3), \n                                include_top = False, \n                                weights = None)\n\npre_trained_model.load_weights(local_weights_file)\n\nfor layer in pre_trained_model.layers:\n    layer.trainable = False","bdeb44d8":"#pre_trained_model_tra.summary()","ee9856d7":"last_layer = pre_trained_model.get_layer('mixed7')\nprint('last layer output shape: ', last_layer.output_shape)\nlast_output = last_layer.output","2b5f3bfd":"from tensorflow.keras.optimizers import RMSprop\nadam = tf.keras.optimizers.Adam(\n    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False)\n\nx = layers.Flatten()(last_output)\nx = layers.Dense(1024, activation='relu')(x)\nx = layers.Dropout(0.2)(x)                  \nx = layers.Dense  (3, activation='softmax')(x)        \n\nmodel = Model( pre_trained_model.input, x) \n\nmodel.compile(optimizer = adam, \n              loss = 'categorical_crossentropy', \n              metrics = ['accuracy'])","9caa9898":"history = model.fit(\n            train_generator,\n            validation_data = validation_generator,\n            steps_per_epoch = 25,\n            epochs = 10,\n            validation_steps = 10,\n            verbose = 1)","6bbf5a62":"import matplotlib.pyplot as plt\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.style.use('seaborn-whitegrid')\nplt.figure(dpi = 100)\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend(loc=0)\nplt.show();","40cd11da":"# Introduction\n\n> I will use PreTrained Model Inception Netowrk to train my model. Off Course because we need to go deeper :)\n\n![](https:\/\/miro.medium.com\/max\/1656\/1*uW81y16b-ptBDV8SIT1beQ.png)\n\nInceptionv3 is a convolutional neural network for assisting in image analysis and object detection, and got its start as a module for Googlenet. It is the third edition of Google's Inception Convolutional Neural Network, originally introduced during the ImageNet Recognition Challenge. Just as ImageNet can be thought of as a database of classified visual objects, Inception helps classification of objects in the world of computer vision. One such use is in life sciences, where it aids in the research of Leukemia. It was \"codenamed 'Inception' after the film of the same name\"","4ef5f3e9":"# Inception V3\n\nLet us load Inception V3 Pre-trained model.","8a57ac65":"# DNN Model\n\nLet us add our own model at last layer of Inception.","2efd8bb4":"> I am using 'mixed7' as last layer from Inception network and i am freezing layers to not retrain them.\n\n**Use summery() to see layers.**","80ba6414":"> Let us see few images :)","a897df41":"# Image Preprocessing\n\nI will use ImageDataGenerator to make it easier to work with images on the go as well as i will also use Data Augmentation.","60384c02":"> Do not worry why curve is fluctuating so much, this is because our data is different than what is is in Inception Netowrk. We can try much deeper network to our data to increase accuracy on Validation set. Can you see how good it is doing with adding just one layer after inception. That's the beauty of Transfer Learning.\n\n**Thank you for reading my Notebook. I hope you learned something new. You can try tweaking parameters and play around them.**"}}