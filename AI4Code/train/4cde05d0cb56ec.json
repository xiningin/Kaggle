{"cell_type":{"06609862":"code","a41a0772":"code","cfb3328f":"code","e411d296":"code","8379f435":"code","4526eb84":"code","d72f0e8e":"code","3322a503":"code","42263029":"markdown","bcc7665d":"markdown","81bb84ea":"markdown","f5e3f150":"markdown","c4f3912c":"markdown","916b9d2f":"markdown","d981f477":"markdown"},"source":{"06609862":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom kaggle.competitions import twosigmanews #TwoSigmaAPI\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.base import clone\n\nimport random\nimport math\nfrom datetime import timedelta","a41a0772":"#Crete our environment\nenv = twosigmanews.make_env()\n(market_label_df, news_label_df) = env.get_training_data()","cfb3328f":"def kFoldIndices(df, folds):\n    \"\"\"\n    This function creates a list of training and validation indices for our K-Folds based on splitting the data on year-months\n    \"\"\"\n    #Get a set of all the available year months\n    df.loc[:, 'yearmonth'] = df.loc[:, 'time'].dt.strftime('%Y-%m')\n    yearmonths = set(df['yearmonth'].unique())\n    #Work out the fold size from the total number of folds available and the number of folds we want to use\n    #tk - Need to add error handling here for number of folds greather than available even though this is unlikely to occur\n    foldSize = int(round(len(yearmonths)\/folds, 0))\n    #Get the training yearmonths\n    foldYearMonths = []\n    for i in range(folds):\n        #Final fold\n        if i == (folds-1):\n            #Need to expand to a single list of yearmonths as otherwise we have a list of lists\n            foldYearMonthsExpanded = [j for i in foldYearMonths for j in i]\n            #Calculate the yearmonths for our final fold\n            fold = list(yearmonths-set(foldYearMonthsExpanded))\n            foldYearMonths.append(fold)\n        else:\n            #Take a random sample from the remaining yearmonths \n            if len(foldYearMonths) > 0:\n                #Need to expand to a single list of yearmonths as otherwise we have a list of lists\n                foldYearMonthsExpanded = [j for i in foldYearMonths for j in i]\n                options = yearmonths - set(foldYearMonthsExpanded)\n                fold = random.sample(options, foldSize)\n                foldYearMonths.append(fold)\n            #If it is our first fold just take a random sample of the yearmonths\n            else: \n                fold = random.sample(yearmonths, foldSize)\n                foldYearMonths.append(fold)\n    \n    #Now that we have our folds we need to split them into K combinations of training and validiation sets \n    kFolds = []\n    \n    for i in range(folds):\n        #Set our training months to be all except for the ith fold\n        trainMonths = foldYearMonths[:i] + foldYearMonths[i+1:]\n        #Expand into single list from list of lists\n        trainMonths = [j for i in trainMonths for j in i]\n        #Set our valid months to be the ith fold\n        validMonths = foldYearMonths[i]\n        #Set our indices from the months\n        trainIndices = df[df['yearmonth'].isin(trainMonths)].index\n        validIndices = df[df['yearmonth'].isin(validMonths)].index\n        #Create our tuple and append \n        kFold = (trainMonths, validMonths, trainIndices, validIndices)\n        kFolds.append(kFold)\n    \n    return kFolds","e411d296":"def purgeTrain(train, valid, yearmonths, validMonths):\n    \"\"\"\n    This function purges all training data within 10 days of any validation data. This prevents leakage between\n    the two data sets. \n    \"\"\"\n    # Get a list of our yearmonths \n    ymSort = list(yearmonths)\n    # Sort in ascending order\n    ymSort.sort()\n    # Make a copy of our training data\n    trn = train.copy(deep=True)\n    # Iterate over our validation months\n    for month in validMonths:\n        # Get the position of the month in our list of year months\n        index = ymSort.index(month)\n        # Get the position of the month before it and after it i.e under and over\n        under = index-1\n        over = index+1\n        \n        # If no months are before or after this month set it that side to false\n        if under < 0:\n            under = False\n        if over > (len(ymSort) - 1):\n            over = False\n        \n        # Look for the first training month before our validition month, this handled multiple validation months joined together\n        while True:\n            if ymSort[under] not in validMonths:\n                break\n            if (under - 1) < 0:\n                under = False\n                break\n            else:\n                under -= 1\n            \n        # Look for the first training month after our validition month, this handled multiple validation months joined together\n        while True:\n            if ymSort[over] not in validMonths:\n                break\n            if (over + 1) > (len(ymSort) - 1):\n                over = False\n                break\n            else:\n                over += 1\n        \n        # If there is a training month before our validation month, remove all training observations from the 20th onwards of that month\n        if under != False:\n            monthUnder = ymSort[under]\n            test = trn[trn['yearmonth'] == monthUnder]\n            test = test[test['time'].dt.day > 20]\n            trn = trn.drop(test.index)          \n            \n        # If there is a training month after our validation month, remove all training observations up to the 10th of that month\n        if over != False:\n            monthOver = ymSort[over]\n            test = trn[trn['yearmonth'] == monthOver]\n            test = test[test['time'].dt.day < 10]\n            trn = trn.drop(test.index)\n    \n    # Return our indices to purge from our training sets\n    return trn.index","8379f435":"# Create our KFolds\nkFolds = kFoldIndices(market_label_df, 10)\n\n# Get the indices and year months for our first fold\ntrainMonths = kFolds[0][0]\ntrainIndices = kFolds[0][2]\nvalidMonths = kFolds[0][1]\nvalidIndices = kFolds[0][3]\n\n# Create our training and validation sets\ntrain = market_label_df.loc[trainIndices, :]\nvalid = market_label_df.loc[validIndices, :]\nprint (train.shape, valid.shape)\n\n# Work out the indices we need to purge to ensure no data leakage\ntrainIndicesPurged = purgeTrain(train, valid, trainMonths+validMonths, validMonths)\n\n# Purge our training set to ensure no data leakage\ntrainPurged = train.loc[trainIndicesPurged, :]\nprint (trainIndicesPurged.shape)\nprint (trainPurged.shape)","4526eb84":"# Plot our fold\nplt.figure(figsize=(40,10))\nplt.plot_date(trainPurged.groupby('time').count().index, trainPurged.groupby('time').count()['close'], markersize=3)\nplt.plot_date(valid.groupby('time').count().index, valid.groupby('time').count()['close'], markersize=3)","d72f0e8e":"# Initialise our list for holding our scores\nscores = []\n\n# Create a simple decision tree model\ndecisionTree = DecisionTreeClassifier(max_depth=10)\n\n# Iterate over our folds we created earlier\nfor kFold in kFolds:\n    trainMonths = kFold[0]\n    validMonths = kFold[1]\n    trainIndices = kFold[2]\n    validIndices = kFold[3]\n    \n    train = market_label_df.loc[trainIndices, :]\n    valid = market_label_df.loc[validIndices, :]\n    \n    trainIndicesPurged = purgeTrain(train, valid, trainMonths+validMonths, validMonths)\n    \n    valid = valid.dropna()\n    train = market_label_df.loc[trainIndicesPurged, :].dropna()\n    \n    X_train = train.drop(['returnsOpenNextMktres10', 'universe', 'yearmonth', 'time', 'assetName', 'assetCode'], axis=1).values\n    X_valid = valid.drop(['returnsOpenNextMktres10', 'universe', 'yearmonth', 'time', 'assetName', 'assetCode'], axis=1).values\n    y_train = train.loc[:, 'returnsOpenNextMktres10']\n    y_train[y_train < 0] = 0\n    y_train[y_train > 0] = 1\n    y_train = y_train.values.squeeze()\n    y_valid = valid.loc[:, 'returnsOpenNextMktres10']\n    y_valid[y_valid < 0] = 0\n    y_valid[y_valid > 0] = 1\n    y_valid = y_valid.values.squeeze()\n    \n    print ('got kfold')\n    estimatorCopy = clone(decisionTree)\n    print ('cloned')\n    estimatorCopy.fit(X_train, y_train)\n    print ('fit')\n    score = estimatorCopy.score(X_valid, y_valid)\n    print ('scored')\n    scores.append(score)","3322a503":"print (scores)","42263029":"**Function A) Create our K-Folds by splitting the data on year-months**","bcc7665d":"**Function B) Purge the training data so that there are no training observations within 10 days of a validation observation**","81bb84ea":"**Get our data**","f5e3f150":"In the chart above our validation sets are in orange and our training sets are in blue. If you look closely you can see the gap either side of the validation set that has no observations.","c4f3912c":"A big challenge in working with time series data is finding a stable cross validation that you can use to test the accuracy of your models. It can be very frustrating when you think you have a great model but when you submit it for scoring it fails. Furthermore it is very annoying when you get a great result on a model but can't understand why.\n\nThis is especially import for us in this competition as we are dealing with time series data. Without a stable cross validation we will end up overfitting the public leaderboard. When it comes time to test our model on the private leaderboard it is likely to fall apart.\n\nFor this reason we will use Purged Time Series K-Fold Cross Validation to build a stable cross validation framework for evaluationg our model.\n\nLet's break that down quickly:\n\nTime Series - We are working with time series data and will thus split our training and validation sets based on a time window, in this case we will use year-months e.g. February 2008, March 2009 etc.\nPurged - Means that we will ensure there is a gap between our training and validation sets so that there is no data leakage","916b9d2f":"**Import Libraries**","d981f477":"**Create our KFolds and inspect the first fold**"}}