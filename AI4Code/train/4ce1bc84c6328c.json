{"cell_type":{"e5b88f7f":"code","e676ba47":"code","61744172":"code","d8577228":"code","b19b013b":"code","9b67ff19":"code","57042d31":"code","b91a6610":"code","3a3f253f":"code","e0998f08":"code","cfb02e28":"code","293a9d0d":"code","5e86ada7":"code","97db2aab":"code","9ebe52ec":"code","44055e4f":"code","4a0d056f":"code","d4d9a19c":"code","4f9350a4":"code","19e90fba":"code","beb58de0":"code","356c4fc3":"code","56d9fcd0":"code","b96c6722":"code","cb2006cb":"code","639b0774":"code","74349da6":"code","66777a8a":"code","a5f4aeb1":"code","040eaaa5":"code","b88c2dfb":"code","6a738885":"markdown","ddf243ac":"markdown","8d6845ab":"markdown","11c3f81f":"markdown","76832d72":"markdown"},"source":{"e5b88f7f":"from gensim.utils import simple_preprocess\nfrom gensim.sklearn_api.phrases import PhrasesTransformer # phrases\/ coallocations - https:\/\/radimrehurek.com\/gensim\/sklearn_api\/phrases.html\nfrom gensim.sklearn_api import phrases\nfrom gensim.models.phrases import Phrases #, ENGLISH_CONNECTOR_WORDS\nfrom gensim.models import Word2Vec\n\n# import re\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')","e676ba47":"interesting_words_list = [\"salt\",\"pepper\",\"spice\",\"spices\",\"herbs\",\"herbal\",\"sweet\",\"spicy\",\"salty\",\"moist\",\n                          \"paprika\",\"saffron\",\"mace\",\"lavender\",\"honey\",\"honeysuckle\",\"chile\",\"marjoram\",\"sugar\",\"tea\",\"mint\",\n                          \"taste\",\"smell\",\"aroma\",\n                          \"cinnamon\",\"cardamom\",\"peppercorn\",\"turmeric\",\"anise\",\"zaatar\",\n                          \"fork\",\"knife\",\"dish\",\"food\",\"plate\",\n                         \"basil\",\"cilantro\",\"chili\",\"cumin\",\"onion\",\"garlic\",\"dill\",\"horseradish\",\"radish\",\"mustard\",\"peppermint\",\"pepper\",\"sage\",\"vanilla\",\"wasabi\"]","61744172":"df = pd.read_csv(\"..\/input\/gutenberg-poetry-dataset\/Gutenberg-Poetry.csv\",\n#                  nrows=1234,\n                 usecols=[\"s\"]).drop_duplicates().rename(columns={\"s\":\"text\"})\ndf = df.loc[df[\"text\"].str.split().str.len()>1]\ndf","d8577228":"## https:\/\/stackoverflow.com\/questions\/51049568\/attributeerror-on-spacy-token-pos\n## we could also lemmatize\n# from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS\n\ndf[\"text\"] = df[\"text\"].apply(lambda x: simple_preprocess(x, deacc=True, max_len=50,min_len=2))\n# df.drop_duplicates(\"text\",inplace=True)\nprint(df.shape)\n# ## drop duplicates - on list\n# df = df[~df[\"text\"].apply(pd.Series).duplicated()]\n# print(df.shape)\n\n## phrases coallocation\nm = PhrasesTransformer(min_count=6,max_vocab_size=30000000)\n# df[\"text\"] = m.fit_transform(df[\"text\"].values)\nm.fit(df[\"text\"])\ndf[\"text\"]  = m.transform(df[\"text\"])\n\n\nsentences = df[\"text\"]","b19b013b":"df[\"text\"].str.len().describe()","9b67ff19":"#an example sentence in the data\nprint(sentences.iloc[7])","57042d31":"#Word2Vec\n#training the gensim on the data\n#Using the Cbow architecture for the word2Vec\n\nmodel_cbow = Word2Vec(sentences, min_count = 2, size = 200, workers = 3, window = 6)","b91a6610":"#Any example word vector\nprint('chief\\n:',model_cbow['chief']) ","3a3f253f":"# Similarity of the words\nprint(model_cbow.similarity('chief', 'indian'))","e0998f08":"print('the 10 most similar words to indian:\\n')\nmodel_cbow.most_similar('indian')","cfb02e28":"for w in interesting_words_list:\n    try: print(w,\"\\n\",[i[0] for i in model_cbow.most_similar(w, topn=13)])\n    except: pass","293a9d0d":"# defining a tsne function to visualize\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\ndef plot_tsne(model, num):\n    labels = []\n    tokens = []\n    for word in model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    tsne = TSNE(perplexity = 40, n_components = 2, init = 'pca', n_iter = 1500, random_state = 23) # orig 2500 n_iter\n    data = tsne.fit_transform(tokens[:num])\n    x = []\n    y = []\n    for each in data:\n        x.append(each[0])\n        y.append(each[1])\n    plt.figure(figsize = (13, 13))\n    for i in range(num):\n        plt.scatter(x[i], y[i])\n        plt.annotate(labels[i],\n                     xy = (x[i], y[i]),\n                     xytext = (5,2),\n                     textcoords = 'offset points',\n                     ha = 'right',\n                     va = 'bottom')\n    plt.show()","5e86ada7":"#visualising the cbow architecture(only the first 120)\nplot_tsne(model_cbow, 120)","97db2aab":"## let's see how the skipgram model works on the data\nmodel_skipgram = Word2Vec(sentences, min_count = 2, size = 200, workers = 3, window = 6, sg = 1)","9ebe52ec":"#Computing the similarities of the words\nprint(model_skipgram.similarity('indian', 'chief'))","44055e4f":"for w in interesting_words_list:\n    try: print(w,\"\\n\",[i[0] for i in model_skipgram.most_similar(w, topn=13)])\n    except: pass","4a0d056f":"print('the 10 most similar words to indian:\\n')\nmodel_skipgram.most_similar('indian')","d4d9a19c":"#visualising the skipgram archtecture(only the first 100)\nplot_tsne(model_skipgram,100)","4f9350a4":"#using the glove package for embeddings\n!pip install glove_python","19e90fba":"from glove import Corpus, Glove\ncorpus = Corpus()\ncorpus.fit(sentences, window = 5)\nglove = Glove(no_components = 150, learning_rate = 0.05)\nglove.fit(corpus.matrix, epochs = 30, no_threads = 4, verbose = False)\nglove.add_dictionary(corpus.dictionary)","beb58de0":"#Computing the similarities of the words\nprint(glove.most_similar('indian', number = 9))","356c4fc3":"for w in interesting_words_list:\n    try: print(w,\"\\n\",[i[0] for i in glove.most_similar(w, number=13)])\n    except: pass","56d9fcd0":"# now visualising first 80 words using tsne\ndef plot_tsne_glove(model, num):\n    labels = []\n    tokens = []\n    for word in model.wv.vocab:\n        tokens.append(glove.word_vectors[glove.dictionary[word]])\n        labels.append(word)\n    tsne = TSNE(perplexity = 40, n_components = 2, init = 'pca', n_iter = 1500, random_state = 23) # was n_iter 2500 originally\n    data = tsne.fit_transform(tokens[:num])\n    x = []\n    y = []\n    for each in data:\n        x.append(each[0])\n        y.append(each[1])\n    plt.figure(figsize = (12, 12))\n    for i in range(num):\n        plt.scatter(x[i], y[i])\n        plt.annotate(labels[i],\n                     xy = (x[i], y[i]),\n                     xytext = (5,2),\n                     textcoords = 'offset points',\n                     ha = 'right',\n                     va = 'bottom')\n    plt.title('Word vectorization using Glove')\n    plt.show()","b96c6722":"plot_tsne_glove(model_skipgram, 120)","cb2006cb":"import gensim\nfrom gensim.models import Word2Vec \nfrom gensim.models import KeyedVectors","639b0774":"## load pretrained conceptnet numberatch + Clean it's format (\"remove \"\/c\/en\" and similar prefixes of language?)\n## takes ~ 2 minutes to load\nmodel = KeyedVectors.load_word2vec_format(\"..\/input\/conceptnet\/numberbatch-en-19.08.txt\",binary=False, unicode_errors='ignore',limit=800000)","74349da6":"model.most_similar('indian')","66777a8a":"print(\"Pretrained model, without finetuning on poetry:\")\nfor w in interesting_words_list:\n    try: print(w,\"\\n\",[i[0] for i in model.most_similar(w, topn=10)])\n    except: pass","a5f4aeb1":"model_2 = Word2Vec(size=300, min_count=1)\nmodel_2.build_vocab(sentences)\ntotal_examples = model_2.corpus_count\nmodel_2.build_vocab([list(model.vocab.keys())], update=True)\n\nmodel_2.intersect_word2vec_format(\"..\/input\/conceptnet\/numberbatch-en-19.08.txt\",binary=False, unicode_errors='ignore')\nmodel_2.train(sentences, total_examples=total_examples, epochs=model_2.iter)","040eaaa5":"print(\"Finetuned model on poetry:\")\nfor w in interesting_words_list:\n    try: print(w,\"\\n\",[i[0] for i in model_2.most_similar(w, topn=13)])\n    except: pass","b88c2dfb":"# model.train(sentences, total_examples=total_examples, epochs=model_2.iter)\n### AttributeError: 'Word2VecKeyedVectors' object has no attribute 'train' \n# print(\"Pretrained model, without finetuning on poetry:\")\n# for w in interesting_words_list:\n#     try: print(w,\"\\n\",[i[0] for i in model.most_similar(w, topn=12)])\n#     except: pass","6a738885":"#### Finetuning a pretrained model\n* Example : https:\/\/www.kaggle.com\/rtatman\/fine-tuning-word2vec\n* We will use conceptnet numberbatch embeddings - can be downloaded manually, via [Gensim's downloader api](https:\/\/radimrehurek.com\/gensim\/auto_examples\/howtos\/run_downloader_api.html), or imported from one of the kaggle datasets (as done here)\n    * https:\/\/www.kaggle.com\/joeskimo\/conceptnet\n    * https:\/\/www.kaggle.com\/blackitten13\/gensim-embeddings-dataset\n    * Example numberbatch loading + \"cleaning\" code snippets: https:\/\/gist.github.com\/ixaxaar\/9fc209e7ba1c88b87f287028396609f1","ddf243ac":"### glove based model\/embeddings","8d6845ab":"#### Most similar words to a word\n* CBOW model","11c3f81f":"#### skipgram - most similar words","76832d72":"# Word embeddings & analysis of Poetry\n* Digital humanities\n* Gutenberg corpus includes preamble\/non poetry. Could be filtered.\n* Primarily English. \n* Data may further be cleaned by using lemmatization etc'. +- pretrained w2v models, multilingual. ([Example of loading a pretrained W2V model and finetuning it](https:\/\/www.kaggle.com\/rtatman\/fine-tuning-word2vec)"}}