{"cell_type":{"03518b04":"code","102a5642":"code","cbfb658d":"code","970fbbd8":"code","2798fe84":"code","760d65ce":"code","680484de":"code","4a0d86bd":"code","788a7048":"code","8a348943":"code","a09deae2":"code","356b9573":"code","885856ee":"code","0227ff46":"code","5589cd11":"code","044b25b7":"code","3fdaadf4":"code","4ae9b786":"code","42193122":"code","f701e149":"code","659ab5ab":"code","144b0820":"code","13a0d1e7":"code","b6d37560":"code","fbf6a70c":"code","ebb39ddc":"code","53b28df0":"code","852d1b62":"code","bcce56ad":"code","5aba9987":"code","eda38f4a":"code","27f8ce7e":"code","7aca30d1":"code","fc6a9370":"code","30399eda":"code","4f239699":"code","a1911814":"code","39bb4c54":"code","473b504e":"code","92b9ed8f":"code","d2a9fbf6":"markdown","c09a961e":"markdown"},"source":{"03518b04":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","102a5642":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import recall_score\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import LogisticRegression","cbfb658d":"df = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv',sep=',')","970fbbd8":"df.head()","2798fe84":"df.describe()\n","760d65ce":"df.info()\n","680484de":"df.isnull().sum()\n","4a0d86bd":"df.shape\n","788a7048":"plt.figure(figsize=(15,5))\nplt.title('Amount Distribution')\nsns.distplot(df.Amount)","8a348943":"df.Class.value_counts()\n","a09deae2":"plt.figure(figsize=(15,5))\nplt.title('Time Distribution')\nsns.distplot(df.Time)","356b9573":"counts = df.Class.value_counts()\nnormal = counts[0]\nfraud = counts[1]\nsum = normal+fraud\nper_normal = (normal\/(sum)) * 100\nper_fraud = (fraud\/(sum)) * 100","885856ee":"print('{} non-fraud ({:.3f}%) and {} fraud ({:.3f}%).'.format(normal, per_normal, fraud, per_fraud))","0227ff46":"plt.subplots(figsize = (30,20))\nsns.heatmap(df.corr(),cmap=sns.diverging_palette(20, 220, n=200),annot=True, center =0)\nplt.title('Heatmap of all features',fontsize=25)","5589cd11":"f,axes = plt.subplots(nrows=3,ncols=3,figsize=(25,15))\n\nf.suptitle('Fetures with high Negative Corr',size = 35)\n\nsns.boxplot(x=\"Class\", y=\"V3\", data=df, ax=axes[0,0])\nsns.boxplot(x=\"Class\", y=\"V7\", data=df, ax=axes[0,1])\nsns.boxplot(x=\"Class\", y=\"V10\", data=df, ax=axes[0,2])\nsns.boxplot(x=\"Class\", y=\"V12\", data=df, ax=axes[1,0])\nsns.boxplot(x=\"Class\", y=\"V14\", data=df, ax=axes[1,1])\nsns.boxplot(x=\"Class\", y=\"V16\", data=df, ax=axes[1,2])\nsns.boxplot(x=\"Class\", y=\"V17\", data=df, ax=axes[2,0])\nsns.boxplot(x=\"Class\", y=\"V18\", data=df, ax=axes[2,1])\nf.delaxes(axes[2,2])","044b25b7":"f, axes = plt.subplots(nrows=1, ncols=2, figsize=(14,5))\n\nf.suptitle('Features With High Positive Corr', size=20)\nsns.boxplot(x=\"Class\", y=\"V4\", data=df, ax=axes[0])\nsns.boxplot(x=\"Class\", y=\"V11\", data=df, ax=axes[1])","3fdaadf4":"sns.scatterplot(x=df['V13'], y=df['V17'], hue=df['Class'])\n","4ae9b786":"sns.scatterplot(x=df['V13'], y=df['V14'], hue=df['Class'])\n","42193122":"sns.scatterplot(x=df['V13'], y=df['V12'], hue=df['Class'])\n","f701e149":"sns.kdeplot(data=df[df['Class'] == 0]['V17'], label=\"Class 0\", shade=True)\nsns.kdeplot(data=df[df['Class'] == 1]['V17'], label=\"Class 1\", shade=True)\nplt.legend()","659ab5ab":"sns.kdeplot(data=df[df['Class'] == 0]['V14'], label=\"Class 0\", shade=True)\nsns.kdeplot(data=df[df['Class'] == 1]['V14'], label=\"Class 1\", shade=True)","144b0820":"sns.kdeplot(data=df[df['Class'] == 0]['V12'], label=\"Class 0\", shade=True)\nsns.kdeplot(data=df[df['Class'] == 1]['V12'], label=\"Class 1\", shade=True)","13a0d1e7":"from sklearn.preprocessing import StandardScaler\ndf['Amount'] = StandardScaler().fit_transform(df['Amount'].values.reshape (-1,1))\ndf = df.drop (['Time'], axis = 1);","b6d37560":"X = df.iloc[:,:-1].values\ny = df.iloc[:,-1].values","fbf6a70c":"number_records_fraud = len (df[df['Class']==1])\n","ebb39ddc":"fraud_indices = np.array (df[df['Class']==1].index)\nnormal_indices = np.array (df[df['Class']==0].index)\n","53b28df0":"random_normal_indices = np.random.choice (normal_indices, number_records_fraud, replace = False )\nunder_sample_indices = np.concatenate ([fraud_indices, random_normal_indices])","852d1b62":"under_sample_data = df.iloc[under_sample_indices,:]\nX_undersample = under_sample_data.iloc [:, under_sample_data.columns != 'Class'];\ny_undersample = under_sample_data.iloc [:, under_sample_data.columns == 'Class'];","bcce56ad":"\nX_train, X_test, y_train, y_test = train_test_split (X,y, test_size = 0.3, random_state = 0)\nX_train_under, X_test_under, y_train_under, y_test_under = train_test_split (X_undersample,y_undersample, test_size = 0.3, random_state = 0)","5aba9987":"MLPC = MLPClassifier(hidden_layer_sizes=(200,), max_iter=10000)\nMLPC.fit(X_train_under, y_train_under)\ny_pred = MLPC.predict(X_test)\nrecall_acc = recall_score (y_test,y_pred)\nrecall_acc","eda38f4a":"accuracy_score(y_test,y_pred)\n","27f8ce7e":"roc_auc_score(y_test,y_pred)\n","7aca30d1":"print(classification_report(y_test,y_pred))\n","fc6a9370":"from imblearn.over_sampling import SMOTE\n\nprint(\"Transaction Number x_train dataset: \", X_train.shape)\nprint(\"Transaction Number y_train dataset: \", y_train.shape)\nprint(\"Transaction Number x_test dataset: \", X_test.shape)\nprint(\"Transaction Number y_test dataset: \", y_test.shape)\n\nsm = SMOTE(random_state=2)\nx_train_s, y_train_s = sm.fit_sample(X_train, y_train.ravel())\nsns.countplot(x=y_train_s, data=df, palette='CMRmap')","30399eda":"x_train_s.shape\n","4f239699":"y_train_s.shape\n","a1911814":"logreg = LogisticRegression()\nlogreg.fit(x_train_s, y_train_s)\ny_pred = logreg.predict(X_test)\ncnf_matrix = confusion_matrix(y_test, y_pred)\n\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\", fmt='g')\nplt.ylabel('Actual Label')\nplt.xlabel('Predicted Label')\n\nprint(classification_report(y_test, y_pred))","39bb4c54":"from sklearn.ensemble import RandomForestClassifier as rfc\nrand_f = rfc(n_estimators=100, min_samples_split=10, min_samples_leaf=1,\n           max_features='auto', max_leaf_nodes=None,\n           oob_score=True, n_jobs=-1, random_state=1)\nrand_f.fit(x_train_s, y_train_s)\ny_pred = rand_f.predict(X_test)\n\ncnf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\", fmt='g')\nplt.ylabel('Actual Label')\nplt.xlabel('Predicted Label')\n\nprint(classification_report(y_test, y_pred))","473b504e":"roc_auc_score(y_test,y_pred)","92b9ed8f":"accuracy_score(y_test,y_pred)","d2a9fbf6":"We got a high recall which means our model is able to detect the highest number of fraud transactions, while the precision is very low which is not good because it means that the model classifies a lot of non-fraud transactions as fraud. The customers of a financial firm are not going to be satisfied with that and may even stop using the service of that financial firm. So in this case it's also important to have a high precision, which we are going to try to achieve using oversampling over the unbalanced data and then trying ML model over it to see the difference.\n\n","c09a961e":"Random Forest gave us the best results being able to detect more than 80% fraud transactions and at the same time not classifying a lot of non-fraud transactions as fraud."}}