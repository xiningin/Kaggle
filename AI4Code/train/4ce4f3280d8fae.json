{"cell_type":{"0120a0a5":"code","528e1be7":"code","024c1bd4":"code","3aa217ee":"code","02b28c82":"code","e74fdb40":"code","3fdf0bdd":"code","2d894dab":"code","069ea8a5":"code","aef240fb":"code","e3a223e4":"code","ffefbb26":"code","d06f3e63":"code","1871a9b8":"code","efe0c98a":"code","0d05326f":"code","b93b2c72":"code","85c571de":"code","96c0e2b9":"code","e849eebf":"code","16d8dcca":"code","0f927dad":"code","80e846fa":"code","fd34dd30":"code","dd3e260e":"code","3be1af9d":"code","fce1d699":"code","c64335cc":"code","895f6f47":"code","b21ab8f9":"code","4a26197e":"code","c6cd830a":"code","b6ef5f49":"code","e146809f":"code","d7eb3f07":"code","6598177b":"code","03be16b6":"code","4be67eab":"code","0ec5b128":"code","87f54b7d":"code","d16afc1e":"code","91c8fc32":"code","4e8244db":"code","1b761602":"code","f68dbb87":"code","7e4dcd9b":"code","31f988d6":"code","82e94bda":"markdown"},"source":{"0120a0a5":"!pip install ..\/input\/pytorchlightning-071\/pytorch-lightning-0.7.1\/pytorch-lightning-0.7.1","528e1be7":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nimport os\nimport random\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom collections import Counter\nfrom transformers import BertForQuestionAnswering\nfrom transformers import BertTokenizer\nfrom transformers import AlbertModel, AlbertTokenizer,RobertaForQuestionAnswering,BertPreTrainedModel,RobertaTokenizer\nimport tokenizers\nimport pytorch_lightning as pl\nfrom transformers import RobertaTokenizer, RobertaForQuestionAnswering,RobertaConfig,RobertaModel,RobertaForMaskedLM\nimport torch\n\nfrom tqdm import tqdm_notebook as tqdm\nimport itertools\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup","024c1bd4":"pl.__version__","3aa217ee":"!ls ..\/input\/tweet-sentiment-extraction\n","02b28c82":"!ls ..\/input\/roberta-base","e74fdb40":"#df_train = pd.read_csv('..\/input\/tweet-train-folds\/train_folds.csv')\ndf_train = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\ndf_test = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')\nsubmission = pd.read_csv('..\/input\/tweet-sentiment-extraction\/sample_submission.csv')","3fdf0bdd":"df_train = df_train.dropna()\ndf_train.shape","2d894dab":"def remove_multiple_dot(line):\n   # print(line)\n    line = re.sub('\\.\\.+', ' ', line) \n    #line = re.sub('\\.', '', line)\n    return line","069ea8a5":"def remove_single_multiple_dot(line):\n    line = re.sub('\\.\\.+', ' ', line) \n    line = re.sub('\\.', '', line)\n    return line","aef240fb":"#df_train['text'] = df_train['text'].apply(lambda x :remove_multiple_dot(x))\n#df_train['selected_text'] = df_train['selected_text'].apply(lambda x :remove_single_multiple_dot(x))","e3a223e4":"#df_test['text'] = df_test['text'].apply(lambda x :remove_multiple_dot(x))\n","ffefbb26":"df_train, df_val = train_test_split(df_train, train_size=0.8,stratify=df_train['sentiment'])","d06f3e63":"df_train.head()","1871a9b8":"df_train.shape","efe0c98a":"df_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)","0d05326f":"for col in df_train:\n    if sum(df_train[col].isnull()) >  0:\n        print(col,sum(df_train[col].isnull()) )","b93b2c72":"for col in df_val:\n    if sum(df_val[col].isnull()) >  0:\n        print(col,sum(df_val[col].isnull()) )","85c571de":"for col in df_test:\n    if sum(df_test[col].isnull()) >  0:\n        print(col,sum(df_test[col].isnull()) )","96c0e2b9":"df_val.head()","e849eebf":"df_test.head()","16d8dcca":"def process_data1(tweet, selected_text, sentiment, tokenizer, max_len):\n    #print(tweet)\n    #print(selected_text)\n    #tweet = \" \" + \" \".join(str(tweet).split())\n    #selected_text = \" \" + \" \".join(str(selected_text).split())\n    tweet = tweet.strip()\n    selected_text = selected_text.strip()\n    print(tweet)\n    print(selected_text)\n    tweet_encoded = tokenizer.encode(tweet, add_special_tokens=True)\n    print(tweet_encoded)\n    selected_text_encoded = tokenizer.encode(selected_text, add_special_tokens=True)\n    print(selected_text_encoded)\n    sentiment_encoded = tokenizer.encode(sentiment, add_special_tokens=True)\n    input_ids = tokenizer.build_inputs_with_special_tokens(sentiment_encoded,tweet_encoded)\n    token_type_ids = tokenizer.create_token_type_ids_from_sequences(sentiment_encoded,tweet_encoded)\n    start_id = None\n    end_id = None\n    for i in range(0,len(input_ids)):\n        #print(selected_text_encoded[1:-1],input_ids[i:len(selected_text_encoded)+i],input_ids[i:len(selected_text_encoded)+i][1:-1])\n        if selected_text_encoded[1:-1] == input_ids[i:len(selected_text_encoded)+i][1:-1]:\n            start_id = i\n            end_id = i + len(selected_text_encoded)\n            print(start_id,end_id)\n            break\n        #else:\n            #print(selected_text_encoded[1:-1] , input_ids[i:len(selected_text_encoded)+i][1:-1])\n    mask = [1] * len(token_type_ids)\n    padding_length = max_len - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([0] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([1] * padding_length)\n    #print('--------')\n    return {\n        'ids': input_ids,\n        'mask': mask,\n        'token_type_ids': token_type_ids,\n        'target_start_id': start_id,\n        'target_end_id': end_id,\n        'tweet': tweet,\n        'selected_text': selected_text,\n        'sentiment': sentiment\n    }","0f927dad":"def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n    tweet = \" \" + \" \".join(str(tweet).split())\n    selected_text = \" \" + \" \".join(str(selected_text).split())\n    #print(tweet)\n    #print(selected_text)\n    len_st = len(selected_text) - 1\n    idx0 = None\n    idx1 = None\n\n    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n        if \" \" + tweet[ind: ind+len_st] == selected_text:\n            idx0 = ind\n            idx1 = ind + len_st - 1\n            break\n\n    char_targets = [0] * len(tweet)\n    if idx0 != None and idx1 != None:\n        for ct in range(idx0, idx1 + 1):\n            char_targets[ct] = 1\n    \n    tok_tweet = tokenizer.encode(tweet)\n    input_ids_orig = tok_tweet.ids\n    tweet_offsets = tok_tweet.offsets\n    \n    target_idx = []\n    for j, (offset1, offset2) in enumerate(tweet_offsets):\n        if sum(char_targets[offset1: offset2]) > 0:\n            target_idx.append(j)\n    \n    targets_start = target_idx[0]\n    targets_end = target_idx[-1]\n\n    sentiment_id = {\n        'positive': 1313,\n        'negative': 2430,\n        'neutral': 7974\n    }\n    \n    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n    token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n    mask = [1] * len(token_type_ids)\n    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n    targets_start += 4\n    targets_end += 4\n    #print(targets_start,targets_end)\n    padding_length = max_len - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([1] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n    return {\n        'ids': input_ids,\n        'mask': mask,\n        'token_type_ids': token_type_ids,\n        'target_start_id': targets_start,\n        'target_end_id': targets_end,\n        'tweet': tweet,\n        'selected_text': selected_text,\n        'sentiment': sentiment,\n        'offsets': tweet_offsets\n    }","80e846fa":"def process_data_test1(tweet, selected_text, sentiment, tokenizer, max_len):\n    tweet_encoded = tokenizer.encode(tweet, add_special_tokens=True)\n    sentiment_encoded = tokenizer.encode(sentiment, add_special_tokens=True)\n    input_ids = tokenizer.build_inputs_with_special_tokens(sentiment_encoded,tweet_encoded)\n    token_type_ids = tokenizer.create_token_type_ids_from_sequences(sentiment_encoded,tweet_encoded)\n    mask = [1] * len(token_type_ids)\n    padding_length = max_len - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([0] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([1] * padding_length)\n    \n    return {\n        'ids': input_ids,\n        'mask': mask,\n        'token_type_ids': token_type_ids,\n        'tweet': tweet,\n        'sentiment': sentiment\n    }","fd34dd30":"def process_data_test(tweet, sentiment, tokenizer, max_len):\n    tweet = \" \" + \" \".join(str(tweet).split())\n    #len_st = len(selected_text) - 1\n    tok_tweet = tokenizer.encode(tweet)\n    input_ids_orig = tok_tweet.ids\n    tweet_offsets = tok_tweet.offsets\n\n    sentiment_id = {\n        'positive': 1313,\n        'negative': 2430,\n        'neutral': 7974\n    }\n    \n    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n    token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n    mask = [1] * len(token_type_ids)\n    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n\n    padding_length = max_len - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([1] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n    \n    return {\n        'ids': input_ids,\n        'mask': mask,\n        'token_type_ids': token_type_ids,\n        'tweet': tweet,\n        'sentiment': sentiment,\n        'offsets': tweet_offsets\n    }","dd3e260e":"class Training_Dataset(Dataset):\n    def __init__(self,df):\n        super().__init__()\n        self.tweet = df['text']\n        self.sentiment = df['sentiment']\n        self.selected_text = df['selected_text']\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n        self.text_id = df['textID'].values\n\n    def __len__(self):\n        return len(self.tweet)\n\n    def __getitem__(self, item):\n        data = process_data(\n            self.tweet[item], \n            self.selected_text[item], \n            self.sentiment[item],\n            self.tokenizer,\n            self.max_len\n        )\n        return {\n#            'idx': torch.tensor(item, dtype=torch.long),\n            'idx':item,\n            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n            'target_start_id': torch.tensor(data[\"target_start_id\"], dtype=torch.long),\n            'target_end_id': torch.tensor(data[\"target_end_id\"], dtype=torch.long),\n            'tweet': data[\"tweet\"],\n            'selected_text': data[\"selected_text\"],\n            'sentiment': data[\"sentiment\"],\n            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n        }","3be1af9d":"#textID\ttext\tselected_text\tsentiment\tkfold","fce1d699":"class Test_Dataset(Dataset):\n    def __init__(self,df):\n        super().__init__()\n        self.tweet = df['text'].values\n        self.sentiment = df['sentiment'].values\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n        self.text_id = df['textID'].values\n\n    def __len__(self):\n        return len(self.tweet)\n\n    def __getitem__(self, item):\n        data = process_data_test(\n            self.tweet[item], \n            self.sentiment[item],\n            self.tokenizer,\n            self.max_len\n        )\n        \n        return {\n         #   'idx': torch.tensor(item, dtype=torch.long),\n            'idx':self.text_id[item],\n            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n            'tweet': data[\"tweet\"],\n            'sentiment': data[\"sentiment\"],\n            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n        }","c64335cc":"class TweetBaseSuperModule(pl.LightningModule):\n    def __init__(self, model, tokenizer, prediction_save_path):\n        super().__init__()\n        self.model = model\n        self.tokenizer = tokenizer\n        self.prediction_save_path = prediction_save_path\n        self.drop_out = nn.Dropout(0.1)\n        self.l0 = nn.Linear(1024 * 2, 2)\n        torch.nn.init.normal_(self.l0.weight, std=0.02)\n        self.loss_average_val =0\n        self.loss_average_train = 0\n\n    def get_device(self):\n        return self.bertmodel.state_dict()['bert.embeddings.word_embeddings.weight'].device\n\n    def save_predictions(self, idx,start_positions, end_positions,filtered_output):\n        d = pd.DataFrame({'text_ID':idx,'start_position':start_positions, 'end_position':end_positions,'selected_text':filtered_output})\n        d.to_csv(self.prediction_save_path, index=False)\n        \n    def save_predictions1(self, idx,start_positions, end_positions):\n        d = pd.DataFrame({'text_ID':idx,'start_position':start_positions, 'end_position':end_positions})\n        d.to_csv(self.prediction_save_path, index=False)\n        \n    def forward(self, ids, mask, token_type_ids):\n        _, _, out = self.model(\n            ids,\n            mask,\n            token_type_ids=token_type_ids\n        ) \n        out = torch.cat((out[-1], out[-2]), dim=-1) \n        out = self.drop_out(out) \n        logits = self.l0(out) \n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1) \n        end_logits = end_logits.squeeze(-1) \n        return start_logits, end_logits\n    \n    \n    def loss(self,start_logits, end_logits, start_positions, end_positions):\n        \"\"\"\n        Return the sum of the cross entropy losses for both the start and end logits\n        \"\"\"\n        loss_fct = nn.CrossEntropyLoss()\n        start_loss = loss_fct(start_logits, start_positions)\n        end_loss = loss_fct(end_logits, end_positions)\n        total_loss = (start_loss + end_loss)\n        return total_loss\n    \n    def extract_selected_text(self,original_tweet, sentiment_val,idx_start, idx_end,offsets):\n        #print(original_tweet)\n        #print(sentiment_val)\n        #print(idx_start)\n        if idx_end < idx_start:\n            idx_end = idx_start\n        filtered_output  = \"\"\n        for ix in range(idx_start, idx_end + 1):\n            filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n            if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n                filtered_output += \" \"\n        if sentiment_val == \"neutral\" or len(original_tweet.split()) < 2:\n            filtered_output = original_tweet\n        return filtered_output\n    \n    def extract_selected_text_batch(self,tweet, sentiment,idx_start, idx_end,offsets):\n        filtered_output_ls = []\n        for tweet_value,sentiment_value,start_value,end_value,offset_value in zip(tweet,sentiment,idx_start,idx_end,offsets):\n            filtered_output = self.extract_selected_text(\n                original_tweet=tweet_value,\n                sentiment_val=sentiment_value,\n                idx_start=start_value,\n                idx_end=end_value,\n                offsets=offset_value\n            )\n            filtered_output_ls.append(filtered_output)\n        return filtered_output_ls\n            \n\n    def training_step(self, batch, batch_nb):\n        \"\"\"\n        (batch) -> (dict or OrderedDict)\n        # Caution: key for loss function must exactly be 'loss'.\n        \"\"\"\n        idx = batch['idx']\n        #ids, mask, token_type_ids\n        start_logits, end_logits = self.forward(batch['ids'],batch['mask'],batch['token_type_ids'])\n        loss = self.loss(start_logits, end_logits,batch['target_start_id'],batch['target_end_id'])\n        self.loss_average_train = (self.loss_average_train + loss)\/(batch_nb+1)\n        if batch_nb % 200 == 0: \n            print(f\"TRAIN : Batch {batch_nb} Average Loss {self.loss_average_train} batch loss: {loss}\")\n        return {'loss':loss, 'idx':idx,'total_average_train_loss':self.loss_average_train}\n\n    def validation_step(self, batch, batch_nb):\n        \"\"\"\n        (batch) -> (dict or OrderedDict)\n        # Caution: key for loss function must exactly be 'loss'.\n        \"\"\"\n        idx = batch['idx']\n        start_logits, end_logits = self.forward(batch['ids'],batch['mask'],batch['token_type_ids'])\n        loss = self.loss(start_logits, end_logits,batch['target_start_id'],batch['target_end_id'])\n        self.loss_average_val = (self.loss_average_val + loss)\/(batch_nb+1)\n        if batch_nb % 100 == 0: \n            print(f\"VAL : Batch {batch_nb} Average Loss {self.loss_average_val} batch loss: {loss}\")\n        return {'loss':loss, 'idx':idx,'total_average_val_loss':self.loss_average_val}\n\n    def test_step(self, batch, batch_nb):\n        \"\"\"\n        (batch) -> (dict or OrderedDict)\n        \"\"\"\n        idx = batch['idx']\n        start_scores = self.forward(batch['ids'],batch['mask'],batch['token_type_ids'])[0]\n        end_scores = self.forward(batch['ids'],batch['mask'],batch['token_type_ids'])[1]\n        tweet = batch['tweet']\n        offsets = batch['offsets']\n        sentiment = batch['sentiment']\n        return {'start_scores':start_scores, 'end_scores':end_scores, 'idx':idx,'tweet':tweet,'offsets':offsets,'sentiment':sentiment}\n\n    def training_end(self, outputs):\n        \"\"\"\n        outputs(dict) -> loss(dict or OrderedDict)\n        # Caution: key must exactly be 'loss'.\n        \"\"\"\n        #train_num_steps = int(len(df_train) \/ TRAIN_BATCH_SIZE * EPOCHS)\n        #l = outputs['loss']\n        #tl = outputs['total_train_loss']\/train_num_steps\n        #print(f\"TRAIN STEP END : Total Return Loss {l}, TOTAL LOSS {tl}, TRAIN STEPS : {train_num_steps}\")\n        return {'loss':outputs['loss']}\n\n    def validation_end(self, outputs):\n        \"\"\"\n        For single dataloader:\n            outputs(list of dict) -> (dict or OrderedDict)\n        For multiple dataloaders:\n            outputs(list of (list of dict)) -> (dict or OrderedDict)\n        \"\"\"        \n        return {'loss':torch.mean(torch.tensor([output['loss'] for output in outputs])).detach()}\n\n    def test_end(self, outputs):\n        \"\"\"\n        For single dataloader:\n            outputs(list of dict) -> (dict or OrderedDict)\n        For multiple dataloaders:\n            outputs(list of (list of dict)) -> (dict or OrderedDict)\n        \"\"\"\n        start_scores = torch.cat([output['start_scores'] for output in outputs]).detach().cpu().numpy()\n        start_positions = np.argmax(start_scores, axis=1) - 1\n\n        end_scores = torch.cat([output['end_scores'] for output in outputs]).detach().cpu().numpy()\n        end_positions = np.argmax(end_scores, axis=1) - 1\n        idx = [output['idx'] for output in outputs]\n        idx = list(itertools.chain.from_iterable(idx))\n        \n        tweet = [output['tweet'] for output in outputs]\n        tweet = list(itertools.chain.from_iterable(tweet))\n\n        offsets = [output['offsets'] for output in outputs]\n        offsets = list(itertools.chain.from_iterable(offsets))\n        \n        sentiment = [output['sentiment'] for output in outputs]\n        sentiment = list(itertools.chain.from_iterable(sentiment))\n        \n        filtered_output= self.extract_selected_text_batch(tweet, sentiment,start_positions, end_positions,offsets)\n        self.save_predictions(idx,start_positions, end_positions,filtered_output)\n        return {}\n\n    #def configure_optimizers(self):\n    #    return optim.Adam(self.parameters(), lr=2e-5)\n    \n    def configure_optimizers(self):\n        num_train_steps = int(len(df_train) \/ TRAIN_BATCH_SIZE * EPOCHS)\n        param_optimizer = list(self.model.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n        optimizer_parameters = [\n            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n        ]\n        \n        optimizer = AdamW(optimizer_parameters, lr=3e-5)\n        scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=num_train_steps)\n        return [optimizer], [scheduler]\n\n\n    @pl.data_loader\n    def train_dataloader(self):\n        pass\n\n    @pl.data_loader\n    def val_dataloader(self):\n        pass\n\n    @pl.data_loader\n    def test_dataloader(self):\n        pass","895f6f47":"np.array([1,2,4]).shape","b21ab8f9":"class Tweet_Model(TweetBaseSuperModule):\n    def __init__(self, bertmodel, tokenizer, prediction_save_path):\n        super().__init__(bertmodel, tokenizer, prediction_save_path)\n\n    @pl.data_loader\n    def train_dataloader(self):\n        return train_dl\n\n    @pl.data_loader\n    def val_dataloader(self):\n        return val_dl\n\n    @pl.data_loader\n    def test_dataloader(self):\n        return test_dl","4a26197e":"ls ..\/input\/roberta-transformers-pytorch\/roberta-large","c6cd830a":"MAX_LEN = 128\nTRAIN_BATCH_SIZE = 16\nVALID_BATCH_SIZE = 8\nEPOCHS = 2\nDEBUG_MODE = False\n\nROBERTA_PATH = \"..\/input\/roberta-transformers-pytorch\/roberta-large\/\"\nTOKENIZER = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=f\"{ROBERTA_PATH}vocab.json\", \n    merges_file=f\"{ROBERTA_PATH}merges.txt\", \n    lowercase=True,\n    add_prefix_space=True\n)\n\n#tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\nmodel_config = RobertaConfig.from_pretrained(f\"{ROBERTA_PATH}config.json\")\nmodel_config.output_hidden_states = True\nmodel = RobertaModel.from_pretrained(f\"{ROBERTA_PATH}pytorch_model.bin\", config=model_config)\n","b6ef5f49":"#tweet, sentiment, selected_text,tokenizer,max_len\ntrain_ds = Training_Dataset(df_train)\nval_ds = Training_Dataset(df_val)\ntest_ds = Test_Dataset(df_test)","e146809f":"train_dl = DataLoader(train_ds, batch_size=TRAIN_BATCH_SIZE, num_workers=4)\nval_dl = DataLoader(val_ds, batch_size=VALID_BATCH_SIZE, num_workers=4)\ntest_dl = DataLoader(test_ds, batch_size=VALID_BATCH_SIZE, num_workers=4)","d7eb3f07":"model = Tweet_Model(model, TOKENIZER, 'pred.csv')\nmodel","6598177b":"1024*2","03be16b6":"trainer = pl.Trainer(gpus=1,max_nb_epochs=EPOCHS, fast_dev_run=DEBUG_MODE)","4be67eab":"trainer.fit(model)","0ec5b128":"trainer.test()","87f54b7d":"pred = pd.read_csv('pred.csv')","d16afc1e":"pred = pred[['text_ID','selected_text']]","91c8fc32":"pred.columns=['textID','selected_text']","4e8244db":"def fillna_by_text(val):\n    if val['selected_text']=='CODECXXX001':\n        return ' '.join(val['text'].split(' ')[0:3])\n    else:\n        return val['selected_text']","1b761602":"submission_final = pred.merge(df_test,on='textID',how='inner')\nsubmission_final = submission_final.fillna('CODECXXX001')\nsubmission_final['selected_text1']=submission_final.apply(lambda x : fillna_by_text(x),axis=1)\nsubmission_final = submission_final[['textID','selected_text1']]\nsubmission_final.columns=['textID','selected_text']\nsubmission_final.to_csv('submission.csv',index=False)","f68dbb87":"#pred.to_csv('submission.csv',index=False)","7e4dcd9b":"submission_final.shape,submission.shape","31f988d6":"submission_final","82e94bda":"        input_ids,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        start_positions=None,\n        end_positions=None,"}}