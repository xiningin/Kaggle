{"cell_type":{"a5cfafb9":"code","9565cef4":"code","77f08503":"code","5fc88af7":"code","a6fc70da":"code","d37236f9":"code","61eac953":"code","3b31d4ed":"code","a4464ded":"code","e252e704":"code","80ad6f7c":"code","35bebbf9":"code","d3eb4d25":"code","2ba295c8":"code","590fa49b":"code","b77d5c9d":"code","c00cc4e2":"code","bcd7a566":"code","40863f66":"code","cf56ed9d":"code","c10f9cab":"code","230d6cdb":"markdown","3331c48a":"markdown","3f5e9a99":"markdown","db4fd4d9":"markdown"},"source":{"a5cfafb9":"#Importing required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy.optimize as opt\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns","9565cef4":"df = pd.read_csv(\"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/iris\/iris.data\", sep = \",\" , \n                 header = None, names  = [\"SepalLength\", \"SepalWidth\", \"PetalLength\", \"PetalWidth\", \"Species\"], index_col=None)","77f08503":"display(df.head())\n#Distribution of each variable in dataset\ndisplay(df.describe())","5fc88af7":"#Plotting bivariate plots to get insights of our taget variable\nsns.set_context('paper', font_scale = 1.5)\nfig, ax =plt.subplots(1,2)\nfig.set_size_inches(16,5)\nsns.scatterplot(x= 'SepalLength', y = 'SepalWidth', data = df, hue = 'Species',ax =ax[0])\nsns.scatterplot(x= 'PetalWidth', y = 'PetalLength', data = df, hue = 'Species',ax =ax[1])","a6fc70da":"#Making a different dataset for the target variable\ny = df.Species\nY = y.copy()","d37236f9":"df.drop('Species', axis=1, inplace=True)","61eac953":"Y.value_counts()","3b31d4ed":"#Converting categories to numeircal data for implementation of a vectorized model\nY.replace({\"Iris-setosa\":0,\"Iris-versicolor\":1, \"Iris-virginica\":2}, inplace = True);","a4464ded":"print(df.head())\nprint(df.columns)","e252e704":"#Feature Scaling\n#Unscaled features results in dominance of a particular feature\/features having higher \"weight\" \n#and thus reduces the accuracy of the model on data the model is not tested on.\n#Performing mean normalisation\nfor i in df.columns:\n    df[i] = (df[i]-(df[i].mean()))\/(df[i].std())","80ad6f7c":"#Dataset with normalised values\ndf.head()","35bebbf9":"display(df.head())\ndisplay(df.describe())","d3eb4d25":"x = df.to_numpy()\ny = Y.to_numpy()","2ba295c8":"numExamples = x.shape[0] \nnumFeatures = x.shape[1]","590fa49b":"#Adding unit bias\nx = np.append(np.ones((numExamples,1)),x,axis=1)\nx.shape","b77d5c9d":"xtrain, xtest, ytrain , ytest = train_test_split(x, y, test_size = .1, random_state = 42)","c00cc4e2":"#Sigmoid Function which always return values between 0 and 1 \ndef  sigmoid(z):\n    sig = 1\/(1+np.exp(-z))\n    return sig","bcd7a566":"#Cost function of Logistic Regression Model. Goal is to minimize this Cost function\ndef cost(theta, x, y):\n    m = x.shape[0]\n    predictions = sigmoid(np.matmul(x,theta))\n    error = ((-y* np.log(predictions)) - ((1-y)*np.log(1-predictions)))\n    cost = (1\/m)*sum(error)\n    return cost\n#Gradient Descent\ndef grad(theta, x , y):\n    m = x.shape[0]\n    predictions = sigmoid(np.matmul(x,theta))\n    grad = np.matmul(x.transpose(), (predictions - y)) \/ len(y)\n    return grad","40863f66":"#Final thetas for each category (species) will be stored here\nclassifiers = np.zeros(shape=(3, numFeatures + 1))","cf56ed9d":"#Getting optimum values of theta for each category by\n#minimizing the Cost Function. \nfor c in range(0,3):\n    label = (ytrain == c).astype(int)\n    lable = np.reshape(label, (label.shape[0],1))\n    initial_theta = np.zeros((xtrain.shape[1]))\n    initial_theta = np.reshape(initial_theta,(initial_theta.shape[0],1))\n    classifiers[c, :] = opt.fmin_cg(cost, initial_theta, grad, (xtrain, label), disp=0)","c10f9cab":"#Making predictions on the entire dataset. \nclassProbabilities = sigmoid(x @ classifiers.transpose())\n#This will return us the category for whichever category the input variables returns \n#the highest probability  \npredictions = classProbabilities.argmax(axis=1)\nprint(\"Accuracy:\", str(100 * np.mean(predictions == y)) + \"%\")","230d6cdb":"### Converting datasets to matrices using Numpy as Vectorized implementation requires matrix operations. ","3331c48a":"## These scatter plots show a very distinct Linear separation between the species Iris Setosa and the other two species named Iris Versicolor and Iris Virginica.","3f5e9a99":"## Iris-Setosa = 0\n## Iris-Versicolor = 1\n## Iris-Virginica = 2","db4fd4d9":"# Iris species \n### OneVsAll Logistic Regeression model implemented solely by Numpy and Pandas. \n### Aim for this project was to implement vectorized Cost Function and Gradient descent and make a OneVsAll Logistic Regeression model (3 categories) without the use of any libraries but Numpy and Pandas. I have used the Scipy library to use its fmincg funtion which returns the values of theta by optimizing the Cost funtion. "}}