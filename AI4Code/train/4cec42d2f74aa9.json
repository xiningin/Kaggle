{"cell_type":{"b631848f":"code","4734a08e":"code","60d85113":"code","23228648":"code","f55a1475":"code","edff0368":"code","db46e516":"code","cfb1ebb9":"code","749fc607":"code","cb438d03":"code","33dc2d55":"code","49e49262":"code","e5e456e7":"code","54e50ee7":"code","f8f333b1":"code","7bbdccd1":"code","fa1628d0":"code","ff51cde9":"code","2d3fd956":"code","c5271ad1":"code","fc4aaf51":"code","15be1e6b":"code","dee03b12":"code","06895f87":"code","dc2e4cbf":"code","dcf5f026":"code","88974894":"code","d303b612":"markdown","b085fa17":"markdown","71732100":"markdown","87dbb163":"markdown","b64bdee4":"markdown","f9b36342":"markdown","78d31f7a":"markdown","16a376a8":"markdown","a4eeb6c5":"markdown","8c4af3ca":"markdown","ff74b313":"markdown","cc8a632c":"markdown","01de44e1":"markdown","273f283b":"markdown","a6adb145":"markdown","3a72d04e":"markdown","1e452dff":"markdown","9b7bc76e":"markdown","89456a6f":"markdown","6229a59d":"markdown","af14d247":"markdown","1cbbf0ef":"markdown","07c67826":"markdown","5c51a888":"markdown"},"source":{"b631848f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom pandas_profiling import ProfileReport","4734a08e":"candy = pd.read_csv('..\/input\/the-ultimate-halloween-candy-power-ranking\/candy-data.csv')","60d85113":"candy.head()","23228648":"candy.shape","f55a1475":"candy.info()","edff0368":"def count(feature):\n    \n    # Show the counts of observations in each categorical bin using bars\n    sns.countplot(x=feature,data=candy)\n    ","db46e516":"candy.head()","cfb1ebb9":"fig, ax = plt.subplots(3, 3,figsize=(15,20))\nplt.subplot(3,3,1)\ncount('chocolate')\nplt.subplot(3,3,2)\ncount('fruity')\nplt.subplot(3,3,3)\ncount('caramel')\nplt.subplot(3,3,4)\ncount('peanutyalmondy')\nplt.subplot(3,3,5)\ncount('nougat')\nplt.subplot(3,3,6)\ncount('crispedricewafer')\nplt.subplot(3,3,7)\ncount('bar')\nplt.subplot(3,3,8)\ncount('pluribus')\nplt.subplot(3,3,9)\ncount('hard')","749fc607":"def box(var):\n    # this function take the variable and return a boxplot for each type of fish\n    sns.boxplot(x=\"chocolate\", y=var, data=candy,palette='rainbow')","cb438d03":"fig, ax = plt.subplots(3, 1,figsize=(15,20))\nplt.subplot(3,1,1)\nbox('sugarpercent')\nplt.subplot(3,1,2)\nbox('pricepercent')\nplt.subplot(3,1,3)\nbox('winpercent')","33dc2d55":"candy.head()","49e49262":"X = candy.drop(['chocolate','competitorname'],axis=1) #independent columns\ny = candy['chocolate']   #target column i.e price range\nfrom sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesClassifier()\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nplt.figure(figsize=(10,10))\nfeat_importances.nlargest(8).plot(kind='barh')\nplt.show()","e5e456e7":"plt.figure(figsize=(15,10))\nsns.heatmap(candy.corr(),cmap='coolwarm',annot=True,linecolor='white',linewidths=4)","54e50ee7":"candy.info()","f8f333b1":"competitorname = pd.get_dummies(candy['competitorname'],drop_first=True)","7bbdccd1":"candy.drop('competitorname',axis=1,inplace=True)","fa1628d0":"candy=pd.concat([candy,competitorname],axis=1)","ff51cde9":"candy.head()","2d3fd956":"X = candy.drop('chocolate',axis=1)\ny = candy['chocolate']","c5271ad1":"#spliting the dataset into training set and test set\nfrom sklearn.model_selection import train_test_split\nX_train ,X_test , y_train , y_test =train_test_split(X,y, test_size = 0.2 , random_state=4)","fc4aaf51":"from sklearn.linear_model import LogisticRegression\nlog = LogisticRegression()","15be1e6b":"log.fit(X_train,y_train)","dee03b12":"predictions = log.predict(X_test)","06895f87":"from sklearn.metrics import classification_report,confusion_matrix","dc2e4cbf":"print(classification_report(y_test,predictions))","dcf5f026":"confusion_matrix(y_test,predictions)","88974894":"from sklearn import metrics\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, predictions))","d303b612":"We can see that the dessert that contains chocolate has an average sugar level,\n\nmore expensive than the rest,\n\nand the percentage of winning it according to the larger matching cases","b085fa17":"# Feature Selection","71732100":"Reporting data to know more details about it","87dbb163":"Let's move on to evaluate our model!","b64bdee4":"# Correlation Matrix with Heatmap","f9b36342":"# Training and Predicting","78d31f7a":"# Evaluation","16a376a8":"## Converting Categorical Features ","a4eeb6c5":"# Building a Logistic Regression model","8c4af3ca":"Correlation states how the features are related to each other or the target variable.\nCorrelation can be positive (increase in one value of feature increases the value of the target variable) or negative (increase in one value of feature decreases the value of the target variable)\nHeatmap makes it easy to identify which features are most related to the target variable, we will plot heatmap of correlated features ","ff74b313":"# The dataset","cc8a632c":"\nContent\ncandy-data.csv includes attributes for each candy along with its ranking. For binary variables, 1 means yes, 0 means no. The data contains the following fields:\n\nchocolate: Does it contain chocolate?\n\nfruity: Is it fruit flavored?\n\ncaramel: Is there caramel in the candy?\n\npeanutalmondy: Does it contain peanuts, peanut butter or almonds?\nnougat: Does it contain nougat?\n\ncrispedricewafer: Does it contain crisped rice, wafers, or a cookie component?\n\nhard: Is it a hard candy?\n\nbar: Is it a candy bar?\n\npluribus: Is it one of many candies in a bag or box?\n\nsugarpercent: The percentile of sugar it falls under within the data set.\n\npricepercent: The unit price percentile compared to the rest of the set.\n\nwinpercent: The overall win percentage according to 269,000 matchups.","01de44e1":"I hope you enjoy this study and I would appreciate if you add your comments below.\n\nSeif Mohamed","273f283b":"### Let's find out the percentages","a6adb145":"### fit the model","3a72d04e":"We'll need to convert categorical features to dummy variables using pandas! Otherwise our machine learning algorithm won't be able to directly take in those features as inputs.","1e452dff":"Let's load the file and see what the data looks like.","9b7bc76e":"# EDA","89456a6f":"# model is ready","6229a59d":"Feature importance is an inbuilt class that comes with Tree Based Classifiers, we will be using Extra Tree Classifier for extracting the top 8 features for the dataset.","af14d247":"Now we can see the 8 most important variables in this data","1cbbf0ef":"import the necessary libraries.","07c67826":"I can notice that most candies do not have a filling inside\n\nThe most common filling is chocolate and fruit","5c51a888":"### Training and Testing Data"}}