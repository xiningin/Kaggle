{"cell_type":{"d426ea61":"code","81dedcb9":"code","80ac5211":"code","5b34ba9e":"code","791f79ea":"code","32ebcf43":"code","0c1860c7":"code","e3f82be9":"code","1296539a":"code","007ad4c0":"code","cd7f2842":"code","08a44808":"code","541a4561":"code","50be041b":"code","36d4e253":"code","9b72c627":"code","10867671":"code","a86784ec":"code","d0b3b5ad":"code","35e8d18a":"code","f8907b38":"code","d5ff4063":"code","3af55175":"code","9a4d790c":"code","cd2b23de":"code","e3a8d07c":"code","ca5e92a5":"code","d7568d50":"code","b7ffbfc1":"code","084e97b9":"code","18f2343c":"code","6de26869":"code","f2539fc1":"code","c0345e3d":"code","90d27faf":"code","45f73913":"code","8fb2bee1":"markdown","b8c19a2c":"markdown","89b5fce4":"markdown","7dc1d2ab":"markdown","57f2946c":"markdown","e5531e89":"markdown","8cf4cd00":"markdown","ade4bf25":"markdown","85c7184b":"markdown","17d7cdbd":"markdown","7dee782d":"markdown","69712c50":"markdown","c560d3be":"markdown","0353345a":"markdown","312ebba4":"markdown","a94d4a7e":"markdown","64a84222":"markdown","75c2c4bf":"markdown","c9e4a748":"markdown","ed8c9b50":"markdown","42934017":"markdown","0e29329d":"markdown","922a01bf":"markdown","b5e28121":"markdown","018b4ab1":"markdown","0137f662":"markdown","746e2102":"markdown"},"source":{"d426ea61":"def get_categorical_features(data_df):\n    return data_df.select_dtypes(include='object')\n\n\ndef get_numerical_features(data_df):\n    return data_df.select_dtypes(exclude='object')\n\n\ndef read_train_test_data():\n    train_df = pd.read_csv('..\/input\/train.csv', index_col='Id')\n    test_df = pd.read_csv('..\/input\/test.csv', index_col='Id')\n    \n    print(\"Shape of Train Data: \" + str(train_df.shape))\n    print(\"Shape of Test Data: \" + str(test_df.shape))\n\n    return train_df, test_df\n\ndef inv_y(transformed_y):\n    return np.exp(transformed_y)\n\ndef MissingandUniqueStats(df):\n    total_entry_list = []\n    total_missing_list = []\n    missing_value_ratio_list = []\n    datatype_list = []\n    unique_values_list = []\n    number_of_unique_values_list = []\n    variable_names_list = []\n    \n    for col in df.columns:\n        total_entry_list.append(df[col].shape[0] - df[col].isna().sum())\n        total_missing_list.append(df[col].isna().sum())\n        missing_value_ratio_list.append(round((df[col].isna().sum() \/ len(df[col])), 4))\n        datatype_list.append(df[col].dtype)\n        unique_values_list.append(df[col].unique())\n        number_of_unique_values_list.append(len(df[col].unique()))\n        variable_names_list.append(col)\n    \n    all_info_df = pd.DataFrame({'#TotalEntry':total_entry_list, '#TotalMissingValue':total_missing_list,\\\n                               '%MissingValueRatio':missing_value_ratio_list, 'DataType':datatype_list, 'UniqueValues':unique_values_list,\\\n                               '#UniqueValues':number_of_unique_values_list})\n    \n    all_info_df.index = variable_names_list\n    all_info_df.index.name='Columns'\n    \n    return all_info_df.sort_values(by=\"#TotalMissingValue\",ascending=False)","81dedcb9":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","80ac5211":"train_df, test_df = read_train_test_data()","5b34ba9e":"target = train_df.SalePrice\n\nplt.figure(figsize=(8, 6))\nsns.distplot(target)\nplt.title('Distribution of SalePrice')\nplt.show()","791f79ea":"sns.distplot(np.log10(target))\nplt.title('Distribution of Log10-transformed SalePrice')\nplt.xlabel('log10(SalePrice)')\nplt.show()","32ebcf43":"sns.distplot(np.log(target))\nplt.title('Distribution of Log-transformed SalePrice')\nplt.xlabel('log(SalePrice)')\nplt.show()","0c1860c7":"sns.distplot(np.sqrt(target))\nplt.title('Distribution of Square root-transformed SalePrice')\nplt.xlabel('sqrt(SalePrice)')\nplt.show()","e3f82be9":"info_df = MissingandUniqueStats(train_df)\ninfo_df.head(20)","1296539a":"num_correlation = train_df.select_dtypes(exclude='object').corr()\nplt.figure(figsize=(20,20))\nplt.title('High Correlation')\nsns.heatmap(num_correlation > 0.8, annot=True, square=True, cmap=\"YlGnBu\")","007ad4c0":"num_columns = train_df.select_dtypes(exclude='object').columns\ncorr_to_price = num_correlation['SalePrice']\nn_cols = 5\nn_rows = 8\nfig, ax_arr = plt.subplots(n_rows, n_cols, figsize=(20,20), sharey=True)\nplt.subplots_adjust(bottom=-0.9)\nfor j in range(n_rows):\n    for i in range(n_cols):\n        plt.sca(ax_arr[j, i])\n        index = i + j*n_cols\n        if index < len(num_columns):\n            plt.scatter(train_df[num_columns[index]], train_df.SalePrice)\n            plt.xlabel(num_columns[index])\n            plt.title('Corr to SalePrice = '+ str(np.around(corr_to_price[index], decimals=3)))\nplt.show()\n","cd7f2842":"numerical_features = get_numerical_features(train_df).drop(['SalePrice'], axis=1).copy()\ninfo_numeric = MissingandUniqueStats(numerical_features)\ninfo_numeric","08a44808":"fig = plt.figure(figsize=(12,18))\nfor i in range(len(numerical_features.columns)):\n    fig.add_subplot(9,4,i+1)\n    sns.boxplot(y=numerical_features.iloc[:,i])\n\nplt.tight_layout()\nplt.show()","541a4561":"f = plt.figure(figsize=(18,30))\n\nfor i in range(len(numerical_features.columns)):\n    f.add_subplot(9, 4, i+1)\n    sns.scatterplot(numerical_features.iloc[:,i], target)\n    \nplt.tight_layout()\nplt.show()","50be041b":"categorical_features = get_categorical_features(train_df).copy()\ninfo_categorical = MissingandUniqueStats(categorical_features)\ninfo_categorical","36d4e253":"col = train_df['Fence']\nf, ax = plt.subplots(figsize=(10,6))\nsns.boxplot(y=target, x=col)\nplt.show()","9b72c627":"col = train_df['FireplaceQu']\nf, ax = plt.subplots(figsize=(10,6))\nsns.boxplot(y=target, x=col)\nplt.show()","10867671":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split","a86784ec":"train_df_copy = train_df.copy()\n# MasVnrArea column(8 missing)\ntrain_df_copy.MasVnrArea = train_df_copy.MasVnrArea.fillna(0)\n\n# Categorical columns:\ncat_cols_fill_none = ['Fence', 'FireplaceQu', 'GarageCond', 'GarageQual', 'GarageFinish', 'GarageType',\n                     'BsmtFinType2', 'BsmtExposure', 'BsmtFinType1', 'BsmtQual', 'BsmtCond', 'MasVnrType']\nfor col in cat_cols_fill_none:\n    train_df_copy[col] = train_df_copy[col].fillna(\"None\")","d0b3b5ad":"print(train_df_copy.shape)\ntrain_df_copy = train_df_copy.drop(train_df_copy[train_df_copy['LotFrontage'] > 200].index)\ntrain_df_copy = train_df_copy.drop(train_df_copy[train_df_copy['LotArea'] > 100000].index)\ntrain_df_copy = train_df_copy.drop(train_df_copy[train_df_copy['BsmtFinSF1'] > 4000].index)\ntrain_df_copy = train_df_copy.drop(train_df_copy[train_df_copy['TotalBsmtSF'] > 6000].index)\ntrain_df_copy = train_df_copy.drop(train_df_copy[train_df_copy['LowQualFinSF'] > 550].index)\ntrain_df_copy = train_df_copy.drop(train_df_copy[(train_df_copy['GrLivArea'] > 4000) & (target < 300000)].index)\nprint(train_df_copy.shape)","35e8d18a":"train_df_copy['SalePrice'] = np.log(train_df_copy['SalePrice'])\ntrain_df_copy = train_df_copy.rename(columns={'SalePrice': 'SalePrice_log'})\n\n# train_df_copy['SalePrice'] = np.log10(train_df_copy['SalePrice'])\n# train_df_copy = train_df_copy.rename(columns={'SalePrice': 'SalePrice_log10'})","f8907b38":"train_df_copy.drop(columns=['GarageArea','TotRmsAbvGrd','GarageYrBlt', 'MoSold', 'YrSold', '1stFlrSF'],axis=1,inplace=True) \ntest_df.drop(columns=['GarageArea','TotRmsAbvGrd','GarageYrBlt', 'MoSold', 'YrSold','1stFlrSF'],axis=1,inplace=True)","d5ff4063":"train_df_copy.drop(columns=['PoolQC', 'MiscFeature', 'Alley'], axis=1, inplace=True)\ntest_df.drop(columns=['PoolQC', 'MiscFeature', 'Alley'], axis=1, inplace=True)","3af55175":"y = train_df_copy.SalePrice_log\ntrain_df_copy.drop('SalePrice_log', axis=1, inplace=True)","9a4d790c":"X = train_df_copy\n# One-hot encoding\nX = pd.get_dummies(X)\n# Train-Validation split\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, test_size= 0.25 ,random_state=1)\nprint(\"Train_X shape:\",train_X.shape,\"val_X shape:\",val_X.shape)\nmy_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\ntrain_X = my_imputer.fit_transform(train_X)\nval_X = my_imputer.transform(val_X)","cd2b23de":"from sklearn.metrics import mean_absolute_error\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor","e3a8d07c":"# Series to collate mean absolute errors for each algorithm\nmae_compare = pd.Series()\nmae_compare.index.name = 'Algorithm'\n\n# Random Forest. Define the model. =============================\nrf_model = RandomForestRegressor(random_state=5)\nrf_model.fit(train_X, train_y)\nrf_val_predictions = rf_model.predict(val_X)\nrf_val_mae = mean_absolute_error(inv_y(rf_val_predictions), inv_y(val_y))\n\nmae_compare['RandomForest'] = rf_val_mae\n\n# XGBoost. Define the model. ======================================\nxgb_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\nxgb_model.fit(train_X, train_y, early_stopping_rounds=5, \n              eval_set=[(val_X,val_y)], verbose=False)\nxgb_val_predictions = xgb_model.predict(val_X)\nxgb_val_mae = mean_absolute_error(inv_y(xgb_val_predictions), inv_y(val_y))\n\nmae_compare['XGBoost'] = xgb_val_mae\n\n# Linear Regression =================================================\nlinear_model = LinearRegression()\nlinear_model.fit(train_X, train_y)\nlinear_val_predictions = linear_model.predict(val_X)\nlinear_val_mae = mean_absolute_error(inv_y(linear_val_predictions), inv_y(val_y))\n\nmae_compare['LinearRegression'] = linear_val_mae\n\n# Lasso ==============================================================\nlasso_model = Lasso(alpha=0.0005, random_state=5)\nlasso_model.fit(train_X, train_y)\nlasso_val_predictions = lasso_model.predict(val_X)\nlasso_val_mae = mean_absolute_error(inv_y(lasso_val_predictions), inv_y(val_y))\n\nmae_compare['Lasso'] = lasso_val_mae\n\n# Ridge ===============================================================\nridge_model = Ridge(alpha=0.002, random_state=5)\nridge_model.fit(train_X, train_y)\nridge_val_predictions = ridge_model.predict(val_X)\nridge_val_mae = mean_absolute_error(inv_y(ridge_val_predictions), inv_y(val_y))\n\nmae_compare['Ridge'] = ridge_val_mae\n\n# ElasticNet ===========================================================\nelastic_net_model = ElasticNet(alpha=0.02, random_state=5, l1_ratio=0.7)\nelastic_net_model.fit(train_X, train_y)\nelastic_net_val_predictions = elastic_net_model.predict(val_X)\nelastic_net_val_mae = mean_absolute_error(inv_y(elastic_net_val_predictions), inv_y(val_y))\n\nmae_compare['ElasticNet'] = elastic_net_val_mae\n\n# Gradient Boosting Regression ==========================================\ngbr_model = GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, \n                                      max_depth=4, random_state=5)\ngbr_model.fit(train_X, train_y)\ngbr_val_predictions = gbr_model.predict(val_X)\ngbr_val_mae = mean_absolute_error(inv_y(gbr_val_predictions), inv_y(val_y))\n\nmae_compare['GradientBoosting'] = gbr_val_mae\n\n\nprint('MAE values for different algorithms:')\nmae_compare.sort_values(ascending=True).round()","ca5e92a5":"from sklearn.model_selection import cross_val_score\n\nimputer = SimpleImputer()\nimputed_X = imputer.fit_transform(X)\nn_folds = 10","d7568d50":"scores = cross_val_score(lasso_model, imputed_X, y, scoring='neg_mean_squared_error', \n                         cv=n_folds)\nlasso_mae_scores = np.sqrt(-scores)\n\nprint('For LASSO model:')\nprint(lasso_mae_scores.round(decimals=2))\n\nprint('Mean RMSE = ' + str(lasso_mae_scores.mean().round(decimals=3)))","b7ffbfc1":"scores = cross_val_score(gbr_model, imputed_X, y, scoring='neg_mean_squared_error', \n                         cv=n_folds)\ngbr_mae_scores = np.sqrt(-scores)\n\nprint('For Gradient Boosting model:')\n# print(lasso_mae_scores.round(decimals=2))\nprint('Mean RMSE = ' + str(gbr_mae_scores.mean().round(decimals=3)))","084e97b9":"scores = cross_val_score(xgb_model, imputed_X, y, scoring='neg_mean_squared_error', \n                         cv=n_folds)\nmae_scores = np.sqrt(-scores)\n\nprint('For XGBoost model:')\n# print(mae_scores.round(decimals=2))\nprint('Mean RMSE = ' + str(mae_scores.mean().round(decimals=3)))","18f2343c":"scores = cross_val_score(rf_model, imputed_X, y, scoring='neg_mean_squared_error', \n                         cv=n_folds)\nmae_scores = np.sqrt(-scores)\n\nprint('For Random Forest model:')\n# print(mae_scores.round(decimals=2))\nprint('Mean RMSE = ' + str(mae_scores.mean().round(decimals=3)))","6de26869":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [{'alpha': [0.0007, 0.0005, 0.005]}]\ntop_reg = Lasso()\n\ngrid_search = GridSearchCV(top_reg, param_grid, cv=5, \n                           scoring='neg_mean_squared_error')\n\ngrid_search.fit(imputed_X, y)\n\ngrid_search.best_params_","f2539fc1":"_ , test_df = read_train_test_data()","c0345e3d":"# create test_X which to perform all previous pre-processing on\ntest_X = test_df.copy()\n\n# Repeat treatments for missing\/null values =====================================\n# Numerical columns:\ntest_X.MasVnrArea = test_X.MasVnrArea.fillna(0)\n\n# Categorical columns:\nfor cat in cat_cols_fill_none:\n    test_X[cat] = test_X[cat].fillna(\"None\")\n\ntest_X = test_df.drop(['GarageArea','TotRmsAbvGrd','GarageYrBlt', 'MoSold', 'YrSold', '1stFlrSF'], axis=1)\n\n# One-hot encoding for categorical data =========================================\ntest_X = pd.get_dummies(test_X)\n\n\n# ===============================================================================\n# Ensure test data is encoded in the same manner as training data with align command\nfinal_train, final_test = X.align(test_X, join='left', axis=1)\n\n# Imputer for all other missing values in test data. Note: Do not 'fit_transform'\nfinal_test_imputed = my_imputer.transform(final_test)","90d27faf":"# Create model - on full set of data (training & validation)\n# Best model = Lasso\nfinal_model = Lasso(alpha=0.0005, random_state=5)\n# final_model = XGBRegressor(n_estimators=1500, learning_rate=0.03)\nfinal_train_imputed = my_imputer.fit_transform(final_train)\n\n# Fit the model using all the data - train it on all of X and y\nfinal_model.fit(final_train_imputed, y)","45f73913":"# make predictions which we will submit. \ntest_preds = final_model.predict(final_test_imputed)\n\n# The lines below shows you how to save your data in the format needed to score it in the competition\n# Reminder: predictions are in log(SalePrice). Need to inverse-transform.\noutput = pd.DataFrame({'Id': test_df.index,\n                       'SalePrice': inv_y(test_preds)})\n\noutput.to_csv('submission.csv', index=False)","8fb2bee1":"### We will drop highly correlated  features as mentioned above.","b8c19a2c":"#### Our validation-set is relatively small, and because of that, validation scores might change significantly depending on data points in the validation set. Using K-fold validation could represent data with less variance.","89b5fce4":"- #### GarageCond, GarageQual, GarageFinish, GarageType have the same MissingValueRatio(81), so probably these houses don't have a garage. We'll fill the missing values with \"None\".\n- #### BsmtExposure, BsmtFinType2, BsmtCond, BsmtFindType1, BsmtQual have the same MissingValueRatio(37-38), so probably these houses don't have a basement. We'll fill the missing values with \"None\".\n- #### MasVnrArea(numeric), MasVnrType(categorical) have the same MissingValueRatio(8), so probably these houses don't have a masonry veneer. We'll fill the missing values with \"None\".","7dc1d2ab":"### Highly corrolated features:\n- YearBuilt - GarageYrBlt\n- TotRmsAbvGrd - GrLivArea\n- GarageArea - GarageCars\n- TotalBsmtSF - 1stFlrSF","57f2946c":"## Select algorithm and hyperparameters","e5531e89":"# 2. Data Cleaning","8cf4cd00":"## 2.2 Outliers","ade4bf25":"## Testing regression algorithms","85c7184b":"## 2.3 Normalizing Data","17d7cdbd":"## 2.1 Missing\/Null Values","7dee782d":"## Reusable Methods","69712c50":"# 1. Exploring Data","c560d3be":"## Make predictions","0353345a":"# 3. Feature Selection","312ebba4":"#### Our validation-set is relatively small, and because of that, validation scores might change significantly depending on data points in the validation set. Using K-fold validation could represent data with less variance.","a94d4a7e":"## Cross-validation","64a84222":"## 1.1 Numeric Features","75c2c4bf":"## Final Model","c9e4a748":"### 1.1.1 Correlation Coefficents","ed8c9b50":"### Drop our target feature from training data (SalePrice)","42934017":"- https:\/\/www.kaggle.com\/gowrishankarin\/learn-ml-ml101-rank-4500-to-450-in-a-day \n- https:\/\/www.kaggle.com\/cheesu\/house-prices-1st-approach-to-data-science-process\n- https:\/\/www.kaggle.com\/prestonfan\/ultimate-house-pricing-guide-v5","0e29329d":"## These notebooks were my guide. Especially be sure to check Mr. Chee Su Goh's notebook, which is in the middle one.","922a01bf":"### 1.1.2 Outlier Detection","b5e28121":"- #### The columns which have > %90 missing value ratio probably don't contain any important information. We can drop them.","018b4ab1":"### Also drop > %90 missing value ratio.","0137f662":"- #### MasVnrArea, MasVnrType have the same MissingValueRatio(8), so probably these houses don't have a masonry veneer. We'll fill the missing values with \"None\".","746e2102":"## 1.2 Categorical Features"}}