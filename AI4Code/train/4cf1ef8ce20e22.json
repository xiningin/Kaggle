{"cell_type":{"66aeab62":"code","0bc41630":"code","eeafd0c0":"code","41811952":"code","205d78fd":"code","1ac3c343":"code","5eaee189":"code","b57f7e9c":"code","8a04c465":"code","7be3433c":"code","fc3afdc6":"code","53e967d7":"code","d4589217":"code","8dd66f17":"code","33be22f2":"code","676ece80":"code","af8dca2e":"code","8b6f999a":"code","773d3726":"code","dcc8ca9f":"code","f98c8a8d":"code","b53abe2c":"code","b0ff9eb7":"code","f5943c3e":"code","cc36be55":"code","1fc16373":"code","117d00c8":"code","438e16fc":"code","6eb29ce7":"code","740568b6":"code","03434efa":"markdown","87b0bfae":"markdown","f1a7561c":"markdown","abfd4e17":"markdown","08a442d7":"markdown","a45572da":"markdown","c240e003":"markdown","0564a82d":"markdown","3164f13f":"markdown","84f9a332":"markdown","71d3da09":"markdown","14068828":"markdown","8363b032":"markdown","2fbf2751":"markdown","e8103b2c":"markdown","98fec489":"markdown","553036ed":"markdown","4e22aa45":"markdown","0b9abafa":"markdown","1ede1357":"markdown"},"source":{"66aeab62":"import os\nimport cv2\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras import layers, optimizers\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, Dropout\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n","0bc41630":"# Specify training data directory\nXRay_Directory = '..\/input\/pneumonia-covid-19-xray-dataset\/Xray images dataset\/Dataset'","eeafd0c0":"# List the folders in the directory\nos.listdir(XRay_Directory)","41811952":"# Use image generator to generate tensor images data and normalize them\n# Use 20% of the data for cross-validation  \nimage_generator = ImageDataGenerator(rescale=1.\/255,validation_split=0.2)","205d78fd":"# Generate batches of 40 images\n# Total number of images is 133*4 = 532 images\n# Training is 428 (80%) and validation is 104 (20%)\n# Perform shuffling and image resizing\n\ntrain_generator = image_generator.flow_from_directory(batch_size=40,directory=XRay_Directory,target_size=(256,256),shuffle=True,class_mode='categorical',subset='training')\n# Generate batches of 40 images\n# Total number of images is 133*4 = 532 images\n# Training is 428 (80%) and validation is 104 (20%)\n# Perform shuffling and image resizing\n","1ac3c343":"validation_generator = image_generator.flow_from_directory(batch_size=40,directory=XRay_Directory,target_size=(256,256),shuffle=True,class_mode='categorical',subset='validation')","5eaee189":"# Generate a batch of 40 images and labels\ntrain_images, train_labels = next(train_generator)","b57f7e9c":"train_images.shape, train_labels.shape","8a04c465":"# labels Translator \nlabel_names = {0 : 'Covid-19', 1 : 'Normal' , 2: 'Viral Pneumonia', 3 : 'Bacterial Pneumonia'}","7be3433c":"# Create a grid of 36 images along with their corresponding labels\nL = 6\nW = 6\n\nfig, axes = plt.subplots(L, W, figsize = (12, 12))\naxes = axes.ravel()\n\nfor i in np.arange(0, L*W):\n    axes[i].imshow(train_images[i])\n    axes[i].set_title(label_names[np.argmax(train_labels[i])])\n    axes[i].axis('off')\n\nplt.subplots_adjust(wspace = 0.7)   ","fc3afdc6":"basemodel = ResNet50(weights = 'imagenet', include_top=False, input_tensor=Input(shape=(256,256,3)))","53e967d7":"basemodel.summary()","d4589217":"# freezing layers in the model\nfor layer in basemodel.layers[:-10]:\n  layers.trainable = False","8dd66f17":"# taking output from our trained basemodel and building our classification model over it.\nheadmodel = basemodel.output \nheadmodel = AveragePooling2D(pool_size = (4,4))(headmodel)\nheadmodel = Flatten(name= 'flatten')(headmodel)\nheadmodel = Dense(256, activation = \"relu\")(headmodel)\nheadmodel = Dropout(0.5)(headmodel)\nheadmodel = Dense(256, activation = \"relu\")(headmodel)\nheadmodel = Dropout(0.5)(headmodel)\nheadmodel = Dense(128, activation = \"relu\")(headmodel)\nheadmodel = Dropout(0.5)(headmodel)\nheadmodel = Dense(64, activation = \"relu\")(headmodel)\nheadmodel = Dropout(0.2)(headmodel)\nheadmodel = Dense(4, activation = 'softmax')(headmodel)\n\nmodel = Model(inputs = basemodel.input, outputs = headmodel)","33be22f2":"model.compile(loss = 'categorical_crossentropy', optimizer=optimizers.RMSprop(lr = 1e-4, decay = 1e-6), metrics= [\"accuracy\"])","676ece80":"# using early stopping to exit training if validation loss is not decreasing even after certain epochs (patience)\nearlystopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n\n# save the best model with lower validation loss\ncheckpointer = ModelCheckpoint(filepath=\"weights.hdf5\", verbose=1, save_best_only=True)","af8dca2e":"train_generator = image_generator.flow_from_directory(batch_size = 4, directory= XRay_Directory, shuffle= True, target_size=(256,256), class_mode= 'categorical', subset=\"training\")\nval_generator = image_generator.flow_from_directory(batch_size = 4, directory= XRay_Directory, shuffle= True, target_size=(256,256), class_mode= 'categorical', subset=\"validation\")","8b6f999a":"history = model.fit_generator(train_generator, steps_per_epoch= train_generator.n \/\/ 4, epochs = 16, validation_data= val_generator, validation_steps= val_generator.n \/\/ 4, callbacks=[checkpointer, earlystopping])","773d3726":"history.history.keys()","dcc8ca9f":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['loss'])\n\nplt.title('Model Loss and Accuracy Progress During Training')\nplt.xlabel('Epoch')\nplt.ylabel('Training Accuracy and Loss')\nplt.legend(['Training Accuracy', 'Training Loss'])","f98c8a8d":"plt.plot(history.history['val_loss'])\nplt.title('Model Loss During Cross-Validation')\nplt.xlabel('Epoch')\nplt.ylabel('Validation Loss')\nplt.legend(['Validation Loss'])","b53abe2c":"plt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy Progress During Cross-Validation')\nplt.xlabel('Epoch')\nplt.ylabel('Validation Accuracy')\nplt.legend(['Validation Accuracy'])","b0ff9eb7":"test_directory = '..\/input\/pneumonia-covid-19-xray-dataset\/Xray images dataset\/Test'","f5943c3e":"test_gen = ImageDataGenerator(rescale = 1.\/255)\n\ntest_generator = test_gen.flow_from_directory(batch_size = 40, directory= test_directory, shuffle= True, target_size=(256,256), class_mode= 'categorical')\n\nevaluate = model.evaluate_generator(test_generator, steps = test_generator.n \/\/ 4, verbose =1)\n\nprint('Accuracy Test : {}'.format(evaluate[1]))","cc36be55":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n\nprediction = []\noriginal = []\nimage = []\n\nfor i in range(len(os.listdir(test_directory))):\n  for item in os.listdir(os.path.join(test_directory,str(i))):\n    img= cv2.imread(os.path.join(test_directory,str(i),item))\n    img = cv2.resize(img,(256,256))\n    image.append(img)\n    img = img \/ 255\n    img = img.reshape(-1,256,256,3)\n    predict = model.predict(img)\n    predict = np.argmax(predict)\n    prediction.append(predict)\n    original.append(i)","1fc16373":"len(original)","117d00c8":"score = accuracy_score(original,prediction)\nprint(\"Test Accuracy : {}\".format(score))","438e16fc":"L = 5\nW = 5\n\nfig, axes = plt.subplots(L, W, figsize = (12, 12))\naxes = axes.ravel()\n\nfor i in np.arange(0, L*W):\n    axes[i].imshow(image[i])\n    axes[i].set_title('Guess={}\\nTrue={}'.format(str(label_names[prediction[i]]), str(label_names[original[i]])))\n    axes[i].axis('off')\n\nplt.subplots_adjust(wspace = 1.2) ","6eb29ce7":"print(classification_report(np.asarray(original), np.asarray(prediction)))","740568b6":"cm = confusion_matrix(np.asarray(original), np.asarray(prediction))\nax = plt.subplot()\nsns.heatmap(cm, annot = True, ax = ax)\n\nax.set_xlabel('Predicted')\nax.set_ylabel('Original')\nax.set_title('Confusion_matrix')","03434efa":"# TASK #3: VISUALIZE DATASET","87b0bfae":"![alt text](https:\/\/drive.google.com\/uc?id=1hngDlUf9JnwUhPII-Ah7KTtcvoeTI9m8)","f1a7561c":"![alt text](https:\/\/drive.google.com\/uc?id=19BuQ5m0xZWC7vQN4jX9lukmJ4aE0EkL8)","abfd4e17":"![alt text](https:\/\/drive.google.com\/uc?id=176TJGdJtNZmX4J5QyeI8W_YS5f1gg5VS)","08a442d7":"![alt text](https:\/\/drive.google.com\/uc?id=1nt8iX7H2LEhaWgGCi_NIb05DMQEoJVfI)","a45572da":"# TASK #2: IMPORT LIBRARIES AND DATASET","c240e003":"# TASK #4: UNDERSTAND THE THEORY AND INTUITION BEHIND CONVOLUTIONAL NEURAL NETWORKS","0564a82d":"# TASK #5: UNDERSTAND THE THEORY AND INTUITION BEHIND TRANSFER LEARNING","3164f13f":"![alt text](https:\/\/drive.google.com\/uc?id=1340UvqbXc-sy6cIuVg7ZbOwcga2JxfkP)","84f9a332":"# TASK #7: BUILD AND TRAIN DEEP LEARNING MODEL","71d3da09":"\n<table>\n  <tr><td>\n    <img src=\"https:\/\/drive.google.com\/uc?id=15eGnAbma5Q_j9CZZKi46Gh3-EpgSWYOV\"\n         alt=\"Fashion MNIST sprite\"  width=\"1000\">\n  <\/td><\/tr>\n  <tr><td align=\"center\">\n    <b>Figure 1. Classifying disease using Deep Learning \n  <\/td><\/tr>\n<\/table>\n","14068828":"# TASK #1: UNDERSTAND THE PROBLEM STATEMENT AND BUSINESS CASE","8363b032":"![alt text](https:\/\/drive.google.com\/uc?id=14niGb232X6l8OD1dMT4a_u3fjh_jKuMS)","2fbf2751":"![alt text](https:\/\/drive.google.com\/uc?id=1Chdq0gdnHGYDDb50pMMtcTOZMr0u37Iz)","e8103b2c":"# TASK #8: EVALUATE TRAINED DEEP LEARNING MODEL","98fec489":"![alt text](https:\/\/drive.google.com\/uc?id=10tbeSkGZ0xdHtqTGhYwHhb9PPURw0BfD)","553036ed":"![alt text](https:\/\/drive.google.com\/uc?id=1dye4zWALCDu8a1a-58HfZk4On4nVuizV)","4e22aa45":"# TASK #6: IMPORT MODEL WITH PRETRAINED WEIGHTS","0b9abafa":"![alt text](https:\/\/drive.google.com\/uc?id=1Wnti2DSmA2qMRsgkD7Z_MJkmed0bJZTN)","1ede1357":"**As seen my model has a test_acc of around 78% which is good. But as a deep learning model classifing such important task. I would expect a test_accuracy of more than 95%. I tried playing around with the model. I would be coming back and tuning the model as I want it to be 100% accurate. While detecting Viral Pneunomina its making huge errors, that is what I would never want because in Medical predictions the recall and precision has to be very high. \nI would appreciate if someone can add something to my notebook and let me know feedback.\nAnyways using ResNet50 model for transfer learning is really helpful. I would like to try it with vgg19 too in future.**\nLet me know any good suggestions"}}