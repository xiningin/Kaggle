{"cell_type":{"c0b7184e":"code","4ce5478a":"code","7c5df23d":"code","c963d156":"code","a50cc328":"code","d311ac33":"code","930bf5f8":"code","6b5ac376":"code","90c82047":"code","486b5d5b":"code","00d7dcf2":"code","cab93fb5":"code","2d199487":"code","926f8de6":"code","3ee64717":"code","72c19714":"code","2cc83fa1":"code","552e76d3":"code","a06b604f":"code","d33a313e":"code","b2324457":"code","ddf57c0c":"code","699cbf16":"code","a9bb3fb0":"code","7ad22274":"code","383bc084":"code","bed03322":"code","fce1a1ff":"code","f336c043":"code","2ee4c24e":"code","40d6c0e7":"code","380fe798":"code","b5bc9650":"code","ec1266e7":"code","c62900e5":"code","8195fa89":"code","8f5a7d84":"code","e496f9c2":"code","28617265":"code","fdce0b5f":"code","d0960396":"code","d5d9825f":"code","c10333cb":"code","e29e0ef1":"code","cf50dc8c":"code","8531d62c":"code","2479b7e2":"code","b0cbdf1d":"markdown","36f94cf5":"markdown","28f2e29b":"markdown","a4fd233a":"markdown","76593380":"markdown","31c42340":"markdown","8a775466":"markdown","4347b4a5":"markdown","49193794":"markdown","892abac5":"markdown","0e115efb":"markdown","3f17ea37":"markdown","4beef9c7":"markdown","781ee35f":"markdown","ad411458":"markdown","b2ead198":"markdown","823287f9":"markdown","2c3071e3":"markdown","f1e897ac":"markdown","ac27dcd3":"markdown","870a001a":"markdown","d082ac3c":"markdown","267033cd":"markdown","f1fe72cb":"markdown","76cfde2f":"markdown","adfd7e20":"markdown","f1419d0d":"markdown","584b6ac5":"markdown","d7296860":"markdown","c3c154c1":"markdown","b497b104":"markdown","49885c87":"markdown","6bcbf737":"markdown","0a38d711":"markdown","41e0012c":"markdown","b71d58cc":"markdown","1225418d":"markdown","4aba6a81":"markdown","005c3842":"markdown","58018855":"markdown","c3d2c066":"markdown","64eed677":"markdown","d3f8862f":"markdown","3ee599aa":"markdown","bbf913cc":"markdown"},"source":{"c0b7184e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport itertools\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom keras.utils import plot_model\nfrom sklearn.metrics import classification_report\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4ce5478a":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    #print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()\n    \ndef printcfm(y_test,y_pred,title='confusion matrix'):\n    cnf_matrix = confusion_matrix(y_test, y_pred)\n    np.set_printoptions(precision=2)\n    # Plot non-normalized confusion matrix\n    plt.figure()\n    plot_confusion_matrix(cnf_matrix, classes=['White','Red'],\n                      title=title)","7c5df23d":"# Read in white wine data \nwhite = pd.read_csv(\"http:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/wine-quality\/winequality-white.csv\", sep=';')\n\n# Read in red wine data \nred = pd.read_csv(\"http:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/wine-quality\/winequality-red.csv\", sep=';')\n\n# Add `type` column to `red` with value 1\nred['type'] = 1\n\n# Add `type` column to `white` with value 0\nwhite['type'] = 0\n\n# Append `white` to `red`\nwines = red.append(white, ignore_index=True)","c963d156":"# Print info on white wine\nprint(white.info())\nprint()\n# Print info on red wine\nprint(red.info())","a50cc328":"# First rows of `red` \nred.head()","d311ac33":"# Last rows of `white`\nwhite.tail()","930bf5f8":"# Take a sample of 5 rows of `red`\nred.sample(5)","6b5ac376":"# Describe `white`\nwhite.describe()","90c82047":"# Double check for null values in `red`\npd.isnull(red)","486b5d5b":"fig, ax = plt.subplots(1, 2)\n\nax[0].hist(red.alcohol, 10, facecolor='red', alpha=0.5, label=\"Red wine\")\nax[1].hist(white.alcohol, 10, facecolor='white', ec=\"black\", lw=0.5, alpha=0.5, label=\"White wine\")\n\nfig.subplots_adjust(left=0, right=1, bottom=0, top=0.5, hspace=0.05, wspace=1)\nax[0].set_ylim([0, 1000])\nax[0].set_xlabel(\"Alcohol in % Vol\")\nax[0].set_ylabel(\"Frequency\")\nax[1].set_xlabel(\"Alcohol in % Vol\")\nax[1].set_ylabel(\"Frequency\")\n#ax[0].legend(loc='best')\n#ax[1].legend(loc='best')\nfig.suptitle(\"Distribution of Alcohol in % Vol\")\n\nplt.show()","00d7dcf2":"import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 2, figsize=(8, 4))\n\nax[0].scatter(red['quality'], red[\"sulphates\"], color=\"red\")\nax[1].scatter(white['quality'], white['sulphates'], color=\"white\", edgecolors=\"black\", lw=0.5)\n\nax[0].set_title(\"Red Wine\")\nax[1].set_title(\"White Wine\")\nax[0].set_xlabel(\"Quality\")\nax[1].set_xlabel(\"Quality\")\nax[0].set_ylabel(\"Sulphates\")\nax[1].set_ylabel(\"Sulphates\")\nax[0].set_xlim([0,10])\nax[1].set_xlim([0,10])\nax[0].set_ylim([0,2.5])\nax[1].set_ylim([0,2.5])\nfig.subplots_adjust(wspace=0.5)\nfig.suptitle(\"Wine Quality by Amount of Sulphates\")\n\nplt.show()","cab93fb5":"import matplotlib.pyplot as plt\nimport numpy as np\n\nnp.random.seed(570)\n\nredlabels = np.unique(red['quality'])\nwhitelabels = np.unique(white['quality'])\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 2, figsize=(8, 4))\nredcolors = np.random.rand(6,4)\nwhitecolors = np.append(redcolors, np.random.rand(1,4), axis=0)\n\nfor i in range(len(redcolors)):\n    redy = red['alcohol'][red.quality == redlabels[i]]\n    redx = red['volatile acidity'][red.quality == redlabels[i]]\n    ax[0].scatter(redx, redy, c=redcolors[i])\nfor i in range(len(whitecolors)):\n    whitey = white['alcohol'][white.quality == whitelabels[i]]\n    whitex = white['volatile acidity'][white.quality == whitelabels[i]]\n    ax[1].scatter(whitex, whitey, c=whitecolors[i])\n    \nax[0].set_title(\"Red Wine\")\nax[1].set_title(\"White Wine\")\nax[0].set_xlim([0,1.7])\nax[1].set_xlim([0,1.7])\nax[0].set_ylim([5,15.5])\nax[1].set_ylim([5,15.5])\nax[0].set_xlabel(\"Volatile Acidity\")\nax[0].set_ylabel(\"Alcohol\")\nax[1].set_xlabel(\"Volatile Acidity\")\nax[1].set_ylabel(\"Alcohol\") \n#ax[0].legend(redlabels, loc='best', bbox_to_anchor=(1.3, 1))\nax[1].legend(whitelabels, loc='best', bbox_to_anchor=(1.3, 1))\n#fig.suptitle(\"Alcohol - Volatile Acidity\")\nfig.subplots_adjust(top=0.85, wspace=0.7)\n\nplt.show()","2d199487":"# # Add `type` column to `red` with value 1\n# red['type'] = 1\n\n# # Add `type` column to `white` with value 0\n# white['type'] = 0\n\n# # Append `white` to `red`\n# wines = red.append(white, ignore_index=True)","926f8de6":"import seaborn as sns\nfig = plt.figure(figsize=(20, 10))                         \n#sns.heatmap(pca.inverse_transform(np.eye(n_comp)), cbar=True, annot=True, cmap=\"hot\")\nsns.heatmap(wines.corr(), \n            cbar=True, annot=True, linewidths=.3, xticklabels=wines.columns, cmap=\"hot\")\nplt.ylabel('Principal component', fontsize=20);\n#plt.xlabel('original feature index', fontsize=20);\nplt.tick_params(axis='both', which='major', labelsize=18);\nplt.tick_params(axis='both', which='minor', labelsize=12);\nplt.show()","3ee64717":"wines.type.value_counts().plot(kind='bar', title='Count Wine Type');","72c19714":"# Import `train_test_split` from `sklearn.model_selection`\nfrom sklearn.model_selection import train_test_split\n\n# Specify the data \nX=wines.iloc[:,0:11]\n\n# Specify the target labels and flatten the array\ny= np.ravel(wines.type)\n\n# Split the data up in train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","2cc83fa1":"# Import `StandardScaler` from `sklearn.preprocessing`\nfrom sklearn.preprocessing import StandardScaler\n\n# Define the scaler \nscaler = StandardScaler().fit(X_train)\n\n# Scale the train set\nX_train = scaler.transform(X_train)\n\n# Scale the test set\nX_test = scaler.transform(X_test)","552e76d3":"def classificationModel(input_shape):\n    \n    # Import `Sequential` from `keras.models`\n    from keras.models import Sequential\n\n    # Import `Dense` from `keras.layers`\n    from keras.layers import Dense\n\n    # Initialize the constructor\n    model = Sequential()\n\n    # Add an input layer \n    model.add(Dense(32, activation='relu', input_shape=(input_shape,)))\n\n    # Add one hidden layer \n    model.add(Dense(8, activation='relu'))\n\n    # Add an output layer \n    model.add(Dense(1, activation='sigmoid'))\n    \n    return model","a06b604f":"model = classificationModel(X.shape[1])","d33a313e":"# Model output shape\nmodel.output_shape","b2324457":"# Model summary\nmodel.summary()","ddf57c0c":"# Model config\nmodel.get_config()","699cbf16":"plot_model(model)","a9bb3fb0":"# List the number of weight tensors \nlen(model.get_weights())","7ad22274":"for weight in model.get_weights():\n    print(weight.shape)","383bc084":"model.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","bed03322":"import time\nstart_time = time.time()\nmyEpochs=20\nmyBatch_size = 1\nprint(\"We are going to feed %d matrices in each epoch with %d samples each\" %(X_train.shape[0]\/myBatch_size,myBatch_size))\nprint()\nmodel.fit(X_train, y_train,epochs=myEpochs, batch_size=myBatch_size, verbose=1)\n# We have 4352 samples, with batch_size=1\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","fce1a1ff":"import time\nstart_time = time.time()\nmyEpochs=20\nmyBatch_size = int(X_train.shape[0]\/10)\nprint(\"We are going to feed %d matrices in each epoch with %d samples each\" %(X_train.shape[0]\/myBatch_size,myBatch_size))\nprint()\nmodel1=model.fit(X_train, y_train,epochs=myEpochs, batch_size=myBatch_size, verbose=1)\n# We have 4352 samples, with batch_size=1\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","f336c043":"from IPython.display import HTML\nHTML('<center><iframe width=\"800\" height=\"450\" src=\"https:\/\/www.youtube.com\/embed\/kkWRbIb42Ms\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe><\/center>')","2ee4c24e":"y_pred = model.predict_classes(X_test)","40d6c0e7":"y_pred[:10].T","380fe798":"y_test[:10]","b5bc9650":"score = model.evaluate(X_test, y_test,verbose=1)\n\nprint(score)","ec1266e7":"# Import the modules from `sklearn.metrics`\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, cohen_kappa_score\n\n# Confusion matrix\nprintcfm(y_test,y_pred,title='confusion matrix')","c62900e5":"# Precision \nprecision_score(y_test, y_pred)","8195fa89":"# Recall\nrecall_score(y_test, y_pred)","8f5a7d84":"# F1 score\nf1_score(y_test,y_pred)","e496f9c2":"# Cohen's kappa\ncohen_kappa_score(y_test, y_pred)","28617265":"print(classification_report(y_test, y_pred))","fdce0b5f":"# Isolate target labels\ny = wines.quality\n\n# Isolate data\nX = wines.drop('quality', axis=1) ","d0960396":"# Scale the data with `StandardScaler`\nX = StandardScaler().fit_transform(X)","d5d9825f":"def regressionModel(input_shape):\n    \n    # Import `Sequential` from `keras.models`\n    from keras.models import Sequential\n\n    # Import `Dense` from `keras.layers`\n    from keras.layers import Dense\n\n    # Initialize the constructor\n    model = Sequential()\n\n    # Add an input layer \n    model.add(Dense(32, activation='relu', input_shape=(input_shape,)))\n\n    # Add one hidden layer \n    model.add(Dense(8, activation='relu'))\n\n    # Add an output layer \n    model.add(Dense(1))\n    \n    return model","c10333cb":"from sklearn.model_selection import StratifiedKFold\n\nseed = 7\nnp.random.seed(seed)\n\nmyRegression = regressionModel(X.shape[1])\nmyRegression.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\nfor train, test in kfold.split(X, y):\n    myRegression.fit(X[train], y[train], epochs=10, verbose=1)","e29e0ef1":"myRegression.fit(X[train], y[train], epochs=10, verbose=1)","cf50dc8c":"mse_value, mae_value = myRegression.evaluate(X[test], y[test], verbose=0)\n\nprint(mse_value)","8531d62c":"print(mae_value)","2479b7e2":"from sklearn.metrics import r2_score\n\nr2_score(y_test, y_pred)","b0cbdf1d":"Let's define the new target label:","36f94cf5":"Note: In classification task, the output layer returns a probability. For this reason we use a sigmoid in the output layer. \n\n![Sigmoid function](https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/20190911181329\/Screenshot-2019-09-11-18.05.46.png)","28f2e29b":"Let's import the *train_test_split* from *sklearn.model_selection* and assign the data and the target labels to the variables X and y. ","a4fd233a":"In this Kernel We are going to use the wine quality data set that We can find in the wine quality data set from the UCI Machine Learning Repository. This Kernel is based in this [tutorial](https:\/\/www.datacamp.com\/community\/tutorials\/deep-learning-python).\n\nAnother goal is to understand the **Neural Network Architecture and math**!","76593380":"After compiling the model is time to train!\n\nAfter, you can train the model for *e* epochs or iterations over all the samples in X_train and y_train, in batches of \u00b4n\u00b4 samples. By setting the verbose argument to 1, you indicate that you want to see progress bar logging.\n\nIn other words, the trainning happens for a specified number of epochs (or exposures to the training dataset). An epoch is a single pass through the entire training set, followed by testing of the verification set.\n\nThe batch size that you specify in the code above defines the number of samples that going to be propagated through the network.","31c42340":"# Compile and Fit the model","8a775466":"In this case, there seems to be an imbalance. Afterwards, you can evaluate the model and if it underperforms, It is possible to undersampling or oversampling to cover up the difference in observations.","4347b4a5":"Let's check weight tensors dimensions:","49193794":"Now the data is preprocessed, We can move on to the neural network model building to classify wines and predict the Wine Quality.","892abac5":"Why do we have these weigths' dimensions?<br>\n\nEach layer perform the following operation:\n\n<center>output = activation(dot(input, kernel) + bias).<br> or <br>\n$output = \\sigma(W^T * X + b)$.<\/center> <br>\n\nFor the first layer:<br>\ninput = (11,1)<br>\nkernel = (11, 32)<br>\nbias = (32,)<br>\n\noutput = relu((32,11)*(11,1)+(32,1)) = relu (32,1) = (32,1)\n<br>\n\n(11,1) is the number of features that feed the neural network. 32 is the number of neurons in the second layer. Then the input dimension needs to be transformed to feed the second layer. Matrix multiplication: (32,11)*(11,1)=(32,1). <br> \n(32,) -> Is the bias for the first layer. Matrix sum: (32,1) + (32,1) = (32,1)<br>\n\nThe process continues for the other layers...","0e115efb":"# Loading and understand the Data","3f17ea37":"Define some functions to print confusion matrix:","4beef9c7":"Note: We are not using an activation function in the output layer. This is a typical setup for scalar regression, where you are trying to predict a single continuous value).","781ee35f":"The score is a list that holds the combination of the loss and the accuracy. In this case, you see that both seem very great, but in this case it\u2019s good to remember that your data was somewhat imbalanced: you had more white wine than red wine observations. The accuracy might just be reflecting the class distribution of your data because it\u2019ll just predict white because those observations are abundantly present!\n\nROC (or AUC) could be a better metric!","ad411458":"Alcohol","b2ead198":"Note: The second one was faster because of [vectorization](https:\/\/stackoverflow.com\/questions\/47755442\/what-is-vectorization), but it use more memory! If you want to learn more, take a look in thile following video from Prof. Andrew Ng.:","823287f9":"Let's use some metrics:","2c3071e3":"The data consists of two datasets that are related to red and white variants of the Portuguese \u201cVinho Verde\u201d wine.\n\nhere\u2019s a short description of each variable:\n\n0. Fixed acidity: acids are major wine properties and contribute greatly to the wine\u2019s taste. Usually, the total acidity is divided into two groups: the volatile acids and the nonvolatile or fixed acids. Among the fixed acids that you can find in wines are the following: tartaric, malic, citric, and succinic. This variable is expressed in $g(tartaricacid)\/dm^3$ in the data sets.\n\n1. Volatile acidity: the volatile acidity is basically the process of wine turning into vinegar. In the U.S, the legal limits of Volatile Acidity are 1.2 g\/L for red table wine and 1.1 g\/L for white table wine. In these data sets, the volatile acidity is expressed in $g(aceticacid)\/dm^3$\n\n2. Citric acid is one of the fixed acids that you\u2019ll find in wines. It\u2019s expressed in $g\/dm^3$ in the two data sets.\n\n3. Residual sugar typically refers to the sugar remaining after fermentation stops, or is stopped. It\u2019s expressed in $g\/dm^3$ in the red and white data.\n\n4. Chlorides can be a significant contributor to saltiness in wine. Here, you\u2019ll see that it\u2019s expressed in $g(sodiumchloride)\/dm^3$\n\n5. Free sulfur dioxide: the part of the sulfur dioxide that is added to a wine and that is lost into it is said to be bound, while the active part is said to be free. The winemaker will always try to get the highest proportion of free sulfur to bind. This variable is expressed in $mg\/dm^3$ in the data.\n\n6. Total sulfur dioxide is the sum of the bound and the free sulfur dioxide (SO2). Here, it\u2019s expressed in $mg\/dm^3$. There are legal limits for sulfur levels in wines: in the EU, red wines can only have 160mg\/L, while white and rose wines can have about 210mg\/L. Sweet wines are allowed to have 400mg\/L. For the US, the legal limits are set at 350mg\/L, and for Australia, this is 250mg\/L.\n\n7. Density is generally used as a measure of the conversion of sugar to alcohol. Here, it\u2019s expressed in $g\/cm^3$\n\n8. pH or the potential of hydrogen is a numeric scale to specify the acidity or basicity the wine. As you might know, solutions with a pH less than 7 are acidic, while solutions with a pH greater than 7 are basic. With a pH of 7, pure water is neutral. Most wines have a pH between 2.9 and 3.9 and are therefore acidic.\n\n9. Sulfates are to wine as gluten is to food. You might already know sulfites from the headaches that they can cause. They are a regular part of the winemaking around the world and are considered necessary. In this case, they are expressed in $g(potassiumsulphate)\/dm^3$\n\n10. Alcohol: wine is an alcoholic beverage, and as you know, the percentage of alcohol can vary from wine to wine. It shouldn\u2019t be surprised that this variable is included in the data sets, where it\u2019s expressed in % vol.\n\n11. Quality: wine experts graded the wine quality between 0 (very bad) and 10 (very excellent). The eventual number is the median of at least three evaluations made by those same wine experts.","f1e897ac":"# Evaluating the Model","ac27dcd3":"Standardize The Data","870a001a":"# Create Model for Regression Task: Define Wine Quality.","d082ac3c":"Let's check if our model would generalize using Kfold. Here we have a good explanation about [kfold](https:\/\/stats.stackexchange.com\/questions\/52274\/how-to-choose-a-predictive-model-after-k-fold-cross-validation).\n\nFor the loss function, we are going to use mse (Mean squared error):<p>\n<center> $MSE = \\frac{1}{n} \\Sigma_{i=1}^{n}{\\Big({Y_i -y_i}\\Big)^2}$ <\/center>\n<br>\nFor the metric, we are going to use mae (Mean absolute error):<p>\n<center> $MAE = \\frac{1}{n}\\Sigma_{i=1}^{n}{{|Y_i -y_i|}}$ <\/center>","267033cd":"Creating the model:","f1fe72cb":"# Create Model for Classification Task: Define Wine.","76cfde2f":"Or you can use the sklearn *classification_report*.","adfd7e20":"Compare y_pred and y_test:","f1419d0d":"There is a direct correlation between alcohol and quality \ud83e\udd23!","584b6ac5":"Let\u2019s use the model to make predictions for the labels of the test set:","d7296860":"Standardization is a way to deal with features values that lie so far apart and it is important to improve the models outcome.","c3c154c1":"Understanding the Neural Network Architecture","b497b104":"Now, we are going to plot a correlation matrix. The table shows the correlation coefficients between variables. Each cell in the table shows the correlation between two variables. A correlation matrix is used to summarize data, as an input into a more advanced analysis, and as a diagnostic for advanced analyses.","49885c87":"# Data Exploration","6bcbf737":"Let's use the 'adam' optimizer (for details take a look in this [paper](https:\/\/arxiv.org\/pdf\/1412.6980.pdf)) to find the best weights and the binary_crossentropy as loss function. Additionally, it is possible to monitor the accuracy during the training by passing as ['accuracy'] as the metrics argument.\n\nThe optimizer and the loss are two arguments that are required to compile the model. Some of the most popular optimization algorithms used are the Stochastic Gradient Descent (SGD), ADAM and RMSprop. The choice for a loss function depends on the task: for example, for a **regression**, the Mean Squared Error (MSE) is an option. for a **classification**,  binary_crossentropy is used for the binary classification. In multi-class classification, there is the categorical_crossentropy.","0a38d711":"Sulfates","41e0012c":"**Visualizing The Data**","b71d58cc":"Now let's train our model:","1225418d":"Acidity","4aba6a81":"**Still improving the documentation and refactoring code.\nSorry for the typos!**","005c3842":"Let's check the number of weight tensors ","58018855":"Imbalanced data typically refers to a problem with classification problems where the classes are not represented equally. Let's check if there is a classe imbalance: ","c3d2c066":"# Preprocess Data","64eed677":"Print some metrics:","d3f8862f":"Train and Test Sets","3ee599aa":"# \ud83c\udf77 Predicting wine types and quality: red or white? \ud83c\udf77","bbf913cc":"Let\u2019s put the data to the test and make a scatter plot that plots the alcohol versus the volatile acidity. The data points should be colored according to their rating or quality label:"}}