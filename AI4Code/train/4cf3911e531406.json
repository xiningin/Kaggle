{"cell_type":{"7ab3d921":"code","9469dbb7":"code","798f3161":"code","e33dc94c":"code","bd70f3db":"code","fe8f28cb":"code","679749b0":"code","e91f45cf":"code","b7e38919":"code","bb6f6e54":"code","61641efb":"code","2581f37d":"code","5e5ac8f6":"code","40215344":"code","76b68ef1":"code","3642f918":"code","166dea44":"code","a11a8566":"code","ce7715fe":"code","5c382a15":"code","7d51621a":"code","f476c274":"code","d651232e":"code","18c2e96f":"code","9f0c1072":"code","b1c7a517":"code","45d44895":"code","463058ef":"code","4affc999":"code","a30365b8":"code","3ab3ead4":"code","bc48964f":"code","d576152e":"code","143fc4fd":"code","857c2343":"code","91ee315c":"code","bfb90ebe":"code","88f3d2f9":"code","a888abfa":"code","cdd6111a":"code","baec5036":"code","85f1e36f":"code","066ef9c6":"code","460bea4d":"markdown","7085162c":"markdown","89477403":"markdown","e1cb1498":"markdown","b445f0d8":"markdown","4bb047a0":"markdown","fa090a1a":"markdown","6b32c1b4":"markdown","45aad5fa":"markdown","77e9b34b":"markdown","1745963e":"markdown","abff4a51":"markdown","9b236ff3":"markdown","53d46947":"markdown","d7ef6d99":"markdown","3fb0c058":"markdown","31903862":"markdown","7513b30c":"markdown","77b1cd72":"markdown","c049f28e":"markdown","85c67030":"markdown"},"source":{"7ab3d921":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import mean_squared_error\n\nfrom __future__ import print_function\nimport warnings\nwarnings.filterwarnings(\"ignore\")","9469dbb7":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\ntrain = train.drop('Id', axis = 1)\nprint(train.shape)\nprint(test.shape)","798f3161":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","e33dc94c":"train = train[train.GrLivArea < 4000]\ntrain.shape","bd70f3db":"missing = train.isnull().sum().sort_values(ascending = False)\nmissing_pct = missing \/len(train)\nfeatures_to_discard = list(missing_pct[missing_pct > 0.8].index)\n\nprint('features discared:', features_to_discard)\ntrain.drop(features_to_discard, axis=1, inplace = True)","fe8f28cb":"nominal_features = ['MSSubClass', 'MSZoning', 'LandContour', 'LotConfig', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', \n                   'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation', 'Heating',\n                   'GarageType', 'SaleType', 'SaleCondition']\nordinal_features = ['Street', 'CentralAir', 'LotShape', 'Utilities', 'LandSlope', 'OverallQual', 'OverallCond', 'ExterCond', 'ExterQual', \n                   'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'Electrical', 'KitchenQual',\n                   'Functional', 'FireplaceQu', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive']\n#the propotion of bedroom, bathroom, kitchen compare to total rooms maybe a good feature\ndiscrete_features = ['YearBuilt', 'YearRemodAdd', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr',\n                    'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'YrSold', ]\n#the proportion of living area compare to total area maybe a good feature\ncontinuous_features = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n                      'LowQualFinSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch',\n                      'PoolArea', 'MiscVal', ]\n\nmisc_features = ['MoSold', ]","679749b0":"from sklearn.base import BaseEstimator, TransformerMixin\n\n#Encode ordinal features with order infomation\noridnal_map = {\n    #qual, cond ect.\n    'No':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5, \n    #yes or no\n    'N':0, 'Y': 2, \n    # fence feature\n    'MnWw':1, 'GdWo':2, 'MnPrv':3, 'GdPrv':4,\n    #LotShape\n    'IR3':1, 'IR2':2, 'IR1':3, 'Reg':4,\n    #Utilities\n    'ELO':1, 'NoSeWa':2, 'NoSewr':3, 'AllPub':4,\n    #Land Slope\n    'Sev':1, 'Mod':2, 'Gtl':3,\n    #BsmtFin Type 1\/2\n    'Unf':1, 'LwQ':2, 'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6,\n    #Electrical\n    'Mix':1, 'FuseP':2, 'FuseF':3, 'FuseA':4, 'SBrkr':5,\n    #Functional\n    'Sal':1, 'Sev':2, 'Maj2':3, 'Maj1':4, 'Mod':5, 'Min2':6, 'Min1':7, 'Typ':8,\n    #Garage Finish\n    'Unf':1, 'RFn':2, 'Fin':3,\n    #Paved Drive\n    'N':0, 'P':1, 'Y':2,\n    #Street\n    'Grvl':1, 'Pave':2,\n    #Basement exposure\n    'Mn':2, 'Av':3, 'Gd':4\n}\n\n\n# no fit is needed, function transformer maybe enough\nclass OrdinalTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y = None):\n        return self\n    def transform(self, X, y = None):\n        df_x = pd.DataFrame(X, columns=ordinal_features)\n        df_x = df_x.fillna('No')\n        df_x =  df_x.applymap(lambda x:oridnal_map.get(x,x))\n        return df_x","e91f45cf":"ordinal_transformer = OrdinalTransformer()\nx =ordinal_transformer.fit_transform(train[ordinal_features])\nx","b7e38919":"train[discrete_features].isnull().sum()","bb6f6e54":"from sklearn.preprocessing import Imputer\n\ndiscrete_imputer = Imputer(strategy='most_frequent')\nX = discrete_imputer.fit_transform(train[discrete_features])\ndf_x = pd.DataFrame(X, columns=discrete_features)\ndf_x.isnull().sum()","61641efb":"df_x","2581f37d":"train[continuous_features].isnull().sum()","5e5ac8f6":"continuous_inputer = Imputer(strategy='mean')\nX = continuous_inputer.fit_transform(train[continuous_features])\ndf_x = pd.DataFrame(X, columns=continuous_features)\nprint(df_x.isnull().sum())\ndf_x","40215344":"continuous_inputer.statistics_","76b68ef1":"from scipy.stats import skew\ndf_x.apply(lambda x:skew(x))","3642f918":"def hist_plot(df, features_of_interest):\n    _ = plt.subplots(figsize = (18,18))\n    num_plots_per_row = 4\n    all_plots = len(features_of_interest)\n    for i, feature in enumerate(features_of_interest):\n        ax = plt.subplot(all_plots\/num_plots_per_row + 1, num_plots_per_row, i+1)\n        ax.hist(x = df[feature], bins = 50)\n        ax.set_xlabel(feature)\nhist_plot(df_x, continuous_features)","166dea44":"from sklearn.preprocessing import StandardScaler\n\nstd_transformer = StandardScaler()\nX = std_transformer.fit_transform(df_x)\ndf_x = pd.DataFrame(X, columns=continuous_features)\ndf_x","a11a8566":"hist_plot(df_x, continuous_features)","ce7715fe":"class CatImputer(BaseEstimator, TransformerMixin):\n    \"\"\" Impute categorical features, using most frequent strategy \"\"\"\n    \n    def fit(self, X, y=None):\n        self.fill = pd.Series([X[c].value_counts().index[0]\n            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],\n            index=X.columns)\n\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(self.fill)\n\nnominal_pipeline = Pipeline(steps=[\n    ('imputer', CatImputer()),\n    ('encoder', OneHotEncoder(sparse=False, handle_unknown='ignore'))\n])\n","5c382a15":"train[nominal_features].isnull().sum()","7d51621a":"nominal_X = nominal_pipeline.fit_transform(train[nominal_features])","f476c274":"nominal_X.shape","d651232e":"\ncontinous_pipeline = Pipeline(steps=[\n    ('imputer', Imputer(strategy='mean')),\n    ('log1p', FunctionTransformer(np.log1p)),\n    ('std_scaler', StandardScaler()),\n])\nordinal_pipeline = Pipeline(steps=[\n    ('transformer', OrdinalTransformer()),\n    # std scaler here really dont change the result much\n    #('std_scaler', StandardScaler()),\n])\ndiscrete_pipeline = Pipeline(steps=[\n    ('imputer', Imputer(strategy='most_frequent')),\n    # std scaler here really dont change the result much\n   # ('std_scaler', StandardScaler()),\n])\n\nnominal_pipeline = Pipeline(steps=[\n    ('imputer', CatImputer()),\n    ('encoder', OneHotEncoder(sparse=False, handle_unknown='ignore'))\n])\n\nprepare_pipeline = ColumnTransformer([\n    ('ordinal', ordinal_pipeline, ordinal_features),\n    ('discrete', discrete_pipeline, discrete_features),\n    ('continuous', continous_pipeline, continuous_features),\n    ('nominal', nominal_pipeline, nominal_features),\n])","18c2e96f":"train_prepared = prepare_pipeline.fit_transform(train)\n\ntrain_prepared.shape","9f0c1072":"encoder = prepare_pipeline.named_transformers_.nominal.steps[1][1]\n\n\nprint('Before preprocess, number of nominal features: ', len(nominal_features))\nprint('After preprocess, number of categories:', len(encoder.categories_))\n\ntmp = zip(nominal_features, encoder.categories_)\n\nall_cats = [feature + '_' + str(suffix) for feature, cat in tmp for suffix in cat]\n\ntransformed_features = ordinal_features + discrete_features + continuous_features + all_cats\n\nlen(transformed_features)","b1c7a517":"df = pd.DataFrame(train_prepared, columns=transformed_features)\ndf['Target'] = train.SalePrice","45d44895":"bins = 15\n\nyear_cat = pd.cut(df.YearBuilt, bins = bins)\ndf['year_cat'] = year_cat\n\n_= plt.subplots(figsize = (12,12))\nsns.boxplot(x = 'year_cat', y = 'Target', data = df)\nplt.xticks(rotation = 90)\n\n","463058ef":"x1 = df.YearBuilt > 1890\nx2 = df.YearBuilt < 1900\n\ndf[x1 & x2].loc[:, ['YearBuilt', 'Target']]\n","4affc999":"fig,ax = plt.subplots(figsize = (20,20))\ndf[continuous_features].hist(ax = ax)","a30365b8":"corr_matrix = df.corr()\n\ncorr_matrix.Target.sort_values(ascending=False)","3ab3ead4":"_ = plt.subplots(figsize = (20,20))\nsns.heatmap(corr_matrix)","bc48964f":"from sklearn.model_selection import train_test_split\n\ntrain_set, test_set = train_test_split(train, test_size = 0.2, shuffle= True, random_state = 42)\n\nprint('train set:', train_set.shape)\nprint('test set:', test_set.shape)","d576152e":"step1_y = np.log(train_set.SalePrice.copy())\nstep1_X = prepare_pipeline.fit_transform(train_set)","143fc4fd":"from sklearn.linear_model import Lasso\nfrom sklearn.metrics import mean_squared_error\n\nlasso_reg = Lasso()\n\nlasso_reg.fit(step1_X, step1_y)\npred = lasso_reg.predict(step1_X)\nprint(np.sqrt(mean_squared_error(step1_y, pred)))\n","857c2343":"from sklearn.linear_model import LassoCV\neps = 0.0005\nalphas = np.logspace(-5 , 1, 100)\n\nlasso_cv = LassoCV(alphas=alphas, cv = 5, verbose=1, random_state=42)\nlasso_cv.fit(step1_X,step1_y)\n\nbest_alpha = lasso_cv.alpha_\nprint('best alpha', best_alpha)\n\nalphas = np.arange(0.95, 1.05, 0.001) * best_alpha\nlasso_cv = LassoCV(alphas= alphas, cv = 10, verbose=1, random_state=42)\nlasso_cv.fit(step1_X,step1_y)\n\nbest_alpha = lasso_cv.alpha_\nprint('refined best alpha', best_alpha)","91ee315c":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import make_scorer\n\nscorer = make_scorer(mean_squared_error, greater_is_better = False)\n\nscores = cross_val_score(Lasso(alpha=best_alpha), step1_X, step1_y, scoring=scorer, cv = 10, verbose=2 )\n\nscores = np.sqrt(-scores)\nprint('mean:', scores.mean(), '\\t sdv:', scores.std())","bfb90ebe":"test_X = prepare_pipeline.transform(test_set)\ntest_y = np.log(test_set.SalePrice.copy())\n\nprint(step1_X.shape)\nprint(test_X.shape)\n\nlasso_reg = Lasso(alpha=best_alpha)\nlasso_reg.fit(step1_X,step1_y)","88f3d2f9":"pred = lasso_reg.predict(test_X)\nnp.sqrt(mean_squared_error(test_y, pred))","a888abfa":"all_X = prepare_pipeline.fit_transform(train)\nall_y = np.log(train.SalePrice.copy())\n\nlasso_reg.fit(all_X, all_y)","cdd6111a":"final_X = prepare_pipeline.transform(test)\npred = np.exp(lasso_reg.predict(final_X))","baec5036":"result = pd.DataFrame()\nresult['Id'] = test.Id\nresult['SalePrice'] = pred\nresult.to_csv('it2_result.csv', index=False)","85f1e36f":"coefs = pd.Series(lasso_reg.coef_, index = transformed_features)","066ef9c6":"print(\"Lasso picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  \\\n      str(sum(coefs == 0)) + \" features\")\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                     coefs.sort_values().tail(10)])\n_ = plt.subplots(figsize = (12,12))\nimp_coefs.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Model\")","460bea4d":"* Simple linear model<br>\n**Score:0.13230**\n* log transformation helps a little<br>\n**Score:0.12958**\n* Include nominal features<br>\n**Score:0.12731**\n* Using lasso regression<br>\n**Score:0.12151**","7085162c":"## prepare a test set","89477403":"## Discard outliers","e1cb1498":"### ordinal features preprocessing","b445f0d8":"### nominal features preprocessing","4bb047a0":"### continuouse features preprocessing","fa090a1a":"## Explore(sanity check) preprocessed data","6b32c1b4":"## train on all training data","45aad5fa":"# Make Predictions","77e9b34b":"## evaluate on test set","1745963e":"* Discard outliers\n* Remove features which are missing k(defualt 80) percent of values\n* For ordinal features, fill na with 'No'\n* Map ordinal feature values to int\n* For discrete features, fill na with 'most frequent'\n* For continuous features, fill na with 'median\/ mean'\n* Log transform skewed continuous features\n* Standardize log-transformed continuouse features\n* For nominal features, fill na with 'most frequent'\n* One-Hot encode nominal features","abff4a51":"# Train and fine tune a model","9b236ff3":"Seems a little odd in 1890-1899","53d46947":"# Credit\nThis kernel is largely inspired by juliencs' kernel:<br>[https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset](http:\/\/)<br>\nEven some of the code is directly copied from that kernel. ","d7ef6d99":"## Remove features with large proportion of missing values","3fb0c058":"### discrete features preprocessing","31903862":"## Ensemble the feature preprocessing pipeline","7513b30c":"# Prepare a data preparation pipeline","77b1cd72":"# Some insight into the features","c049f28e":"Next step:\n* Feature engneering<br>\n* Other types of model<br>\n* Ensemble<br>","85c67030":"## Seperate different types of features"}}