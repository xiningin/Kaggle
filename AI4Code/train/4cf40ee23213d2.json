{"cell_type":{"38691031":"code","b33c6d35":"code","bd5dc93d":"code","59625369":"code","983b0f48":"code","f80491dd":"code","915cf5e7":"code","15e694be":"code","71d01b72":"code","9108dabf":"code","f7fd6ddf":"code","f4625fc9":"markdown","a640bfdb":"markdown","4168de41":"markdown","3312f2fd":"markdown","5e438716":"markdown","93bc4925":"markdown","f818573d":"markdown","0c04134d":"markdown","648c56d9":"markdown","3883a486":"markdown","57c214cc":"markdown","a659dcc9":"markdown"},"source":{"38691031":"import numpy as np # We'll be storing our data as numpy arrays\nimport os # For handling directories\nfrom PIL import Image # For handling the images\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg # Plotting\n","b33c6d35":"lookup = dict()\nreverselookup = dict()\ncount = 0\nfor j in os.listdir('..\/input\/leapgestrecog\/leapGestRecog\/00\/'):\n    if not j.startswith('.'): # If running this code locally, this is to \n                              # ensure you aren't reading in hidden folders\n        lookup[j] = count\n        reverselookup[count] = j\n        count = count + 1\nlookup","bd5dc93d":"x_data = []\ny_data = []\ndatacount = 0 # We'll use this to tally how many images are in our dataset\nfor i in range(0, 10): # Loop over the ten top-level folders\n    for j in os.listdir('..\/input\/leapgestrecog\/leapGestRecog\/0' + str(i) + '\/'):\n        if not j.startswith('.'): # Again avoid hidden folders\n            count = 0 # To tally images of a given gesture\n            for k in os.listdir('..\/input\/leapgestrecog\/leapGestRecog\/0' + \n                                str(i) + '\/' + j + '\/'):\n                                # Loop over the images\n                img = Image.open('..\/input\/leapgestrecog\/leapGestRecog\/0' + \n                                 str(i) + '\/' + j + '\/' + k).convert('L')\n                                # Read in and convert to greyscale\n                img = img.resize((320, 120))\n                arr = np.array(img)\n                x_data.append(arr) \n                count = count + 1\n            y_values = np.full((count, 1), lookup[j]) \n            y_data.append(y_values)\n            datacount = datacount + count\nx_data = np.array(x_data, dtype = 'float32')\ny_data = np.array(y_data)\ny_data = y_data.reshape(datacount, 1) # Reshape to be the correct size","59625369":"from random import randint\nfor i in range(0, 10):\n    plt.imshow(x_data[i*200 , :, :])\n    plt.title(reverselookup[y_data[i*200 ,0]])\n    plt.show()\n","983b0f48":"import keras\nfrom keras.utils import to_categorical\ny_data = to_categorical(y_data)","f80491dd":"x_data = x_data.reshape((datacount, 120, 320, 1))\nx_data \/= 255","915cf5e7":"from sklearn.model_selection import train_test_split\nx_train,x_further,y_train,y_further = train_test_split(x_data,y_data,test_size = 0.2)\nx_validate,x_test,y_validate,y_test = train_test_split(x_further,y_further,test_size = 0.5)","15e694be":"from keras import layers\nfrom keras import models","71d01b72":"model=models.Sequential()\nmodel.add(layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu', input_shape=(120, 320,1))) \nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu')) \nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(128, activation='relu'))\nmodel.add(layers.Dense(10, activation='softmax'))","9108dabf":"model.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=10, batch_size=64, verbose=1, validation_data=(x_validate, y_validate))","f7fd6ddf":"[loss, acc] = model.evaluate(x_test,y_test,verbose=1)\nprint(\"Accuracy:\" + str(acc))","f4625fc9":"Now it's time to build our network. We'll use keras.","a640bfdb":"At this point we would typically graph the accuracy of our model on the validation set, and choose a suitable number of epochs to train for to avoid overfitting. We might also consider introducing dropout and regularisation. However, we can see we're getting perfect accuracy on the validation set after just one or two epochs, so we're pretty much done. Let's quickly confirm that this is carrying through to the test set:","4168de41":"As described in the Data Overview, there are 10 folders labelled 00 to 09, each containing images from a given subject. In each folder there are subfolders for each gesture. We'll build a dictionary `lookup` storing the names of the gestures we need to identify, and giving each gesture a numerical identifier. We'll also build a dictionary `reverselookup` that tells us what gesture is associated to a given identifier.","3312f2fd":"# Intro\nThe Hand Gesture Recognition Database is a collection of near-infra-red images of ten distinct hand gestures. In this notebook we use end-to-end deep learning to build a classifier for these images.\n\nWe'll first load some packages required for reading in and plotting the images. ","5e438716":"We need a cross-validation set and a test set, and we'll use the `sklearn` package to construct these. In order to get an 80-10-10 split, we call `train_test_split` twice, first to split 80-20, then to split the smaller chunk 50-50. Note that we do this after the rescaling step above, to ensure that our train and test sets are coming from the same distribution.","93bc4925":"Since our images are big (we chose not to do any cropping) and the classification problem looks quite easy, we're going to downsample fairly aggressively, beginning with a 5 x 5 filter with a stride of 2. Note we have to specify the correct input shape at this initial layer, and keras will figure it out from then on. We won't worry about padding since it's clear that all the useful features are well inside the image. We'll continue with a sequence of convolutional layers followed by max-pooling until we arrive at a small enough image that we can add a fully-connected layer. Since we need to classify between 10 possibilities, we finish with a softmax layer with 10 neurons. ","f818573d":"Next we read in the images, storing them in `x_data`. We store the numerical classifier for each image in `y_data`. Since the images are quite large and are coming from an infra-red sensor, there's nothing really lost in converting them to greyscale and resizing to speed up the computations.","0c04134d":"Let's take a look at some of the pictures. Since each of the subfolders in `00` contained 200 images, we'll use the following piece of code to load one image of each gesture.","648c56d9":"Our set of images has shape `(datacount, 120, 320)`. Keras will be expecting another slot to tell it the number of channels, so we reshape `x_data` accordingly. We also rescale the values in `x_data` to lie between 0 and 1.","3883a486":"The first thing to note is that this is not a difficult classification problem. The gestures are quite distinct, the images are clear, and there's no background whatsoever to worry about. If you weren't comfortable with deep learning, you could do quite well with some straight-forward feature detection -- for example the '07_ok' class could easily be detected with binary thresholding followed by circle detection. \n\nMoreover, the gestures consistently occupy only about 25% of the image, and all would fit snugly inside a square bounding box. Again if you're looking to do basic feature detection, an easy first step would be to write a short script cropping everything to the relevant 120 x 120 square. \n\nBut the point of this notebook is to show how effective it is to just throw a neural network at a problem like this without having to worry about any of the above, so that's what we're going to do. \n\nAt the moment our vector `y_data` has shape `(datacount, 1)`, with `y_data[i,0] = j` if the `i`th image in our dataset is of gesture `reverselookup[j]`. In order to convert it to one-hot format, we use the keras function to_categorical:","57c214cc":"You'll get slightly different numbers each time you run it but you should be getting between 99.9 and 100% accuracy. Great!","a659dcc9":"Finally, we fit the model."}}