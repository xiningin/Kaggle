{"cell_type":{"9e7c6cea":"code","1b25b208":"code","068bebef":"code","5855e672":"code","108037a4":"code","11ed65be":"code","dfafa91a":"code","ef409e62":"code","bc9da15c":"code","94363784":"code","721d0734":"code","1ad83115":"code","d7060a48":"code","38e447f4":"code","2e420bf7":"markdown","0f3bffa6":"markdown","3ca628a4":"markdown","5b1c0072":"markdown","c43f4276":"markdown","9c19cf98":"markdown","539e73cb":"markdown","08a26c33":"markdown","2e76ba9b":"markdown","5c2f35f2":"markdown","d777132a":"markdown","ae46c583":"markdown","13b63460":"markdown","fc245598":"markdown","4ee4fc69":"markdown","1e5333c6":"markdown","15251985":"markdown","447897d4":"markdown","1b1703bf":"markdown"},"source":{"9e7c6cea":"import os\nimport json\nimport math\nimport numpy as np \nimport pandas as pd \nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\nfrom nltk.util import bigrams, trigrams, ngrams\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n","1b25b208":"def textPreprocessing (text):    \n    stop_words = stopwords.words(\"english\")\n    stop_words += [wr for wr in ['one','av','however','moreover','yet']]\n    words = nltk.word_tokenize(text)\n    new_words =[] \n    for word in words: \n        word = word.lower()\n        if ((word not in stop_words) and (word.isalpha())):\n            new_words.append(word)\n    return new_words #list of words in a text","068bebef":"def textReading(file_dir,x=0,y=10):\n    filenames = os.listdir(file_dir)\n#     all_files = []\n    docs_bagOfWords = {}\n    for filename in filenames[x:y]: \n        text = ''\n        file = json.load(open(os.path.join(json_dir,filename), 'rb'))\n        for i in file['body_text']:\n            text += i['text']  \n        docs_bagOfWords[(filename[:-5],file['metadata']['title'])] = textPreprocessing(text)\n    return docs_bagOfWords #dictionary {paper_id:[]}","5855e672":"json_dir = '\/kaggle\/input\/CORD-19-research-challenge\/document_parses\/pdf_json'\ndocs_bagOfWords = textReading(json_dir,2000,2800)","108037a4":"def totalTFIDF(docs_bagOfWords):\n    \"\"\"\n    Calculating TFIDF for the whole documents\n    Args:\n        docs_bagOfWords: dict bag of words of each paper\n    Returns:\n        documentsText: list of strings (the complete body text of each document) \n        feature_names: list of strings (the total vocab unique words)\n        tfidf_dict: dict with paper_id as keys and list of tfidf for this paper\n    \"\"\"\n    vectorizer = TfidfVectorizer()\n    documentsText = []\n    for k in docs_bagOfWords.keys():\n        str1 = ' '\n        str1 = str1.join(docs_bagOfWords[k])\n        documentsText.append(str1)\n    vectors = vectorizer.fit_transform(documentsText)\n    feature_names = vectorizer.get_feature_names()\n    dense = vectors.todense()\n    denselist = dense.tolist()\n    df = pd.DataFrame(denselist, columns=feature_names)\n    tfidf_dict = {}\n    for key,tfidf in zip(docs_bagOfWords.keys(),denselist):\n        tfidf_dict[key] = tfidf\n    return documentsText,feature_names,tfidf_dict\n","11ed65be":"def calculateTFIDF(vec_query):\n    \"\"\"\n    docs_bagOfWords: dict bag of words of each paper\n    \"\"\"\n    vectorizer = TfidfVectorizer()\n    documentsText = []\n    str1 = ' '\n    str1 = str1.join(vec_query)\n    documentsText.append(str1)\n    vectors = vectorizer.fit_transform(documentsText)\n    feature_names = vectorizer.get_feature_names()\n    dense = vectors.todense()\n    denselist = dense.tolist()\n    df = pd.DataFrame(denselist, columns=feature_names)\n    return denselist,feature_names","dfafa91a":"def getTotalVocab(docs):\n    \"\"\"\n    for getting the total_vocab of the given documents in the form of list of words\n    \"\"\"\n    total_vocab = []\n    for i in docs:\n        total_vocab += i.split(' ')\n    return total_vocab #list of total vocab","ef409e62":"d,features,tfidf_dict = totalTFIDF(docs_bagOfWords)\ntotal_vocab = getTotalVocab(d)","bc9da15c":"def getCosineDistance(q_vec,doc_dict):\n    \"\"\"\n    Calculates the cosine distance between a query and documents\n    Args:\n        q_vec: A vector representing the query\n        doc_dict: Dictionary having with - key as a document title\n                                         - value as vector representation for this document\n    Returns:\n        q_norm: The norm of the input query\n        cosDistances: Dictionary containing the documents sorted according to their cosine distances with the query\n    \n    \"\"\"\n    cosDistances = {}\n    q_norm = np.linalg.norm(q_vec)\n    for k in doc_dict.keys():\n        v2 = doc_dict[k]\n        z = np.zeros(((len(v2)-len(q_vec)),))\n        q_vec = np.concatenate((q_vec,z), axis=0)\n        dotProduct = np.dot(q_vec,v2)\n        cosDistances[k] = dotProduct\/(q_norm*np.linalg.norm(v2))\n    cosDistances = {i: j for i, j in sorted(cosDistances.items(), key=lambda item: item[1],reverse=True)}\n    return q_norm, cosDistances\n        ","94363784":"question = \"\"\"What is the covid-19 risk factors? how it affects smokers, pregnants,children and how it\n        influences people with cronic diseases like hypertension, diabetes and cardiologic diseases?\"\"\"\ntext_filtered = textPreprocessing(question)\ntfidfList,words = calculateTFIDF(text_filtered)\n","721d0734":"q_norm,c = getCosineDistance(tfidfList[0],tfidf_dict)","1ad83115":"def pretty(d, indent=0,r=5):\n    for key, i in zip(d.keys(),range(r)):\n        print('\\t' * indent + str(key))\n        if isinstance(d[key], dict):\n            pretty(d[key], indent+1)\n        else:\n            print('\\t' * (indent+1) + str(round(d[key],3)))","d7060a48":"pretty(c)","38e447f4":"paper = 'f084fa3b9768063bc36f08970e7c28a5e3f2f13b.json'\njson.load(open(os.path.join(json_dir,paper), 'rb'))","2e420bf7":"### Retrieving json files of document from the directory","0f3bffa6":"## Implementing TF-IDF Using sklearn","3ca628a4":"The input question is to be placed in 'question' variable","5b1c0072":"#### Displaying the output results:","c43f4276":"![](https:\/\/miro.medium.com\/proxy\/1*A5YGwFpcTd0YTCdgoiHFUw.png)","9c19cf98":"### Computing TF-IDF","539e73cb":"## Preprocessing Data","08a26c33":"### Tokenizing the body text of a paper\nBy removing unnecessary words, punctuation marks, currency symbols and numbers","2e76ba9b":"## Implementing Raw TF-IDF","5c2f35f2":"![](https:\/\/miro.medium.com\/proxy\/1*nSqHXwOIJ2fa_EFLTh5KYw.png)","d777132a":"# About this notebook","ae46c583":"![](https:\/\/miro.medium.com\/proxy\/1*HM0Vcdrx2RApOyjp_ZeW_Q.png)","13b63460":"### Computing IDF","fc245598":"## Importing Important Libraries","4ee4fc69":"![image.png](http:\/\/sites.temple.edu\/tudsc\/files\/2017\/03\/cosine-equation.png)\n","1e5333c6":"#### Cosine similarity measures the similarity between two vectors of an inner product space. It is measured by the cosine of the angle between two vectors and determines whether two vectors are pointing in roughly the same direction.\n![](http:\/\/miro.medium.com\/max\/650\/1*OGD_U_lnYFDdlQRXuOZ9vQ.png)\nA document can be represented by thousands of attributes, each recording the frequency of a particular word (such as a keyword) or phrase in the document. Thus, each document is an object represented by what is called a term-frequency vector.\nThis is what implemented in this model, to find to what extent the input query is similar to the available documents and help us in answering the inquiries about the risk factors of the newly arised COVID-19 virus.","15251985":"### First, TF (Term Frequency Calculation)","447897d4":"> #### The novel **COVID-19** has come and changed how we as humans in this new era of civilization, view diseases. Everything escalated quickly, number of confirmed cases increased exponentially with the R number (which signifies the average number of people which one person infected person will pass the virus to) between 2 and 2.5 at the beginning, and what made it harder is that we don\u2019t understand the disease and more and more lives were lost, we\u2019re in a race with time to try to save as many lives as possible, we want to know more about the disease to flatten the curve, i.e. decrease the R number, and by knowing the risk factors to covid-19, we will be able to do so!! \n\n> #### *And this is what our model \"Corona Explorer\" aims to, by directing the healthcare giver to the most relevant paper that he might find what he\u2019s looking for. And if our solution saved only one life, then we would be very proud that applying some science and using our time did this!*","1b1703bf":"## Calculating Cosine Distance"}}