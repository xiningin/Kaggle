{"cell_type":{"766bee0b":"code","e6ae47e0":"code","5d52925d":"code","cc650d83":"code","df1c03e0":"code","20bbd71d":"code","e6c35074":"code","6811f5fa":"code","2b468491":"code","c8a2bc8e":"code","b1dc3a5b":"code","2071c663":"code","a5bdc856":"code","127d9b5b":"code","94c56ae9":"code","f4b40af8":"code","3da4b3bb":"code","470ba00d":"code","9737654e":"code","52cfa4f5":"code","9bd6716e":"code","24fcbce7":"code","6c0067d8":"code","3ffa2e64":"code","3ac46de5":"code","4f5afc4e":"code","e2c91e63":"code","af250c94":"code","a152ede2":"code","5f524df5":"code","457f476a":"code","72400a4e":"code","27910c6a":"code","9f0fa53c":"markdown","80c43cdf":"markdown","1edbc03a":"markdown","af3b2bcf":"markdown","6561c553":"markdown","0a6e86ed":"markdown","4f323b36":"markdown","7dd4b391":"markdown","248ff429":"markdown","66d9c692":"markdown","e5a1f42f":"markdown","a9ac6b51":"markdown","420ef5ab":"markdown","4772814e":"markdown","41da8ff6":"markdown"},"source":{"766bee0b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns \nimport matplotlib.pyplot as plt \npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e6ae47e0":"train_df = pd.read_csv('..\/input\/us-accidents\/US_Accidents_May19.csv')\ntrain_df.shape\ntrain_df.head()","5d52925d":"train_df.Source.unique()","cc650d83":"states = train_df.State.unique()","df1c03e0":"count_by_state=[]\nfor i in train_df.State.unique():\n    count_by_state.append(train_df[train_df['State']==i].count()['ID'])\n\nfig,ax = plt.subplots(figsize=(16,10))\nsns.barplot(states,count_by_state)","20bbd71d":"missing_df = train_df.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name','missing_count']\nmissing_df = missing_df.ix[missing_df['missing_count']>0]\nmissing_df = missing_df.sort_values(by='missing_count')\n\nind = np.arange(missing_df.shape[0])\nwidth = 0.5\nfig,ax = plt.subplots(figsize=(12,18))\nrects = ax.barh(ind,missing_df.missing_count.values,color='blue')\nax.set_yticks(ind)\nax.set_yticklabels(missing_df.column_name.values, rotation='horizontal')\nax.set_xlabel(\"Count of missing values\")\nax.set_title(\"Number of missing values in each column\")\nplt.show()","e6c35074":"sns.jointplot(x=train_df.Start_Lat.values,y=train_df.Start_Lng.values,height=10)\nplt.ylabel('Start_Lat', fontsize=12)\nplt.xlabel('Start_Lng', fontsize=12)\nplt.show()","6811f5fa":"sns.jointplot(x=train_df.End_Lat.values,y=train_df.End_Lng.values,height=10)\nplt.ylabel('End_Lat', fontsize=12)\nplt.xlabel('End_Lng', fontsize=12)\nplt.show()","2b468491":"fig, ax=plt.subplots(figsize=(16,7))\ntrain_df['Weather_Condition'].value_counts().sort_values(ascending=False).head(5).plot.bar(width=0.5,edgecolor='k',align='center',linewidth=2)\nplt.xlabel('Weather_Condition',fontsize=20)\nplt.ylabel('Number of Accidents',fontsize=20)\nax.tick_params(labelsize=20)\nplt.title('5 Top Weather Condition for accidents',fontsize=25)\nplt.grid()\nplt.ioff()","c8a2bc8e":"dtype_df = train_df.dtypes.reset_index()\ndtype_df.columns = [\"Count\", \"Column Type\"]\ndtype_df","b1dc3a5b":"dtype_df.groupby(\"Column Type\").aggregate('count').reset_index()","2071c663":"missing_df = train_df.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['columns_name','missing_count']\nmissing_df['missing_ratio'] = missing_df['missing_count'] \/train_df.shape[0]\nmissing_df.loc[missing_df['missing_ratio']>0.777]","a5bdc856":"missin = missing_df.loc[missing_df['missing_count']>250000]\nremovelist = missin['columns_name'].tolist()\nremovelist","127d9b5b":"train_df['Start_Time'] = pd.to_datetime(train_df['Start_Time'], errors='coerce')\ntrain_df['End_Time'] = pd.to_datetime(train_df['End_Time'], errors='coerce')\n\n# Extract year, month, day, hour and weekday\ntrain_df['Year']=train_df['Start_Time'].dt.year\ntrain_df['Month']=train_df['Start_Time'].dt.strftime('%b')\ntrain_df['Day']=train_df['Start_Time'].dt.day\ntrain_df['Hour']=train_df['Start_Time'].dt.hour\ntrain_df['Weekday']=train_df['Start_Time'].dt.strftime('%a')\n\n# Extract the amount of time in the unit of minutes for each accident, round to the nearest integer\ntd='Time_Duration(min)'\ntrain_df[td]=round((train_df['End_Time']-train_df['Start_Time'])\/np.timedelta64(1,'m'))","94c56ae9":"neg_outliers=train_df[td]<=0\n\n# Set outliers to NAN\ntrain_df[neg_outliers] = np.nan\n\n# Drop rows with negative td\ntrain_df.dropna(subset=[td],axis=0,inplace=True)","f4b40af8":"feature_lst=['Source','TMC','Severity','Start_Lng','Start_Lat','Distance(mi)','Side','City','County','State','Timezone','Temperature(F)','Humidity(%)','Pressure(in)', 'Visibility(mi)', 'Wind_Direction','Weather_Condition','Amenity','Bump','Crossing','Give_Way','Junction','No_Exit','Railway','Roundabout','Station','Stop','Traffic_Calming','Traffic_Signal','Turning_Loop','Sunrise_Sunset','Hour','Weekday', 'Time_Duration(min)']","3da4b3bb":"df = train_df[feature_lst].copy()\ndf.info()","470ba00d":"x_cols = [col for col in df.columns if col not in ['Severity'] if df[col].dtype=='float64']\n\nlabels = []\nvalues = []\nfor col in x_cols:\n    labels.append(col)\n    values.append(np.corrcoef(df[col].values, df.Severity.values)[0,1])\ncorr_df = pd.DataFrame({'col_labels':labels, 'corr_values':values})\ncorr_df = corr_df.sort_values(by='corr_values')\n\nind = np.arange(len(labels))\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(12,40))\nrects = ax.barh(ind, np.array(corr_df.corr_values.values), color='y')\nax.set_yticks(ind)\nax.set_yticklabels(corr_df.col_labels.values, rotation='horizontal')\nax.set_xlabel(\"Correlation coefficient\")\nax.set_title(\"Correlation coefficient of the variables\")\nplt.show()","9737654e":"corr_zero_columns = ['Turning_Loop','Visibility(mi)','Pressure(in)','Humidity(%)','Temperature(F)','TMC']\nfor col in corr_zero_columns:\n    print(col,len(df[col].unique()))","52cfa4f5":"corr_df_sel = corr_df.loc[(corr_df['corr_values']>0.05) | (corr_df['corr_values'] < -0.05)]\ncorr_df_sel","9bd6716e":"corr_df_ = corr_df_sel.col_labels.tolist()\n\ntem_df = df[corr_df_]\n\ncorrmat = tem_df.corr(method='spearman')\nfig,ax= plt.subplots(figsize=(8,8))\n\nsns.heatmap(corrmat,vmax=1,square = True)\nplt.title('corr map',fontsize=15)\nplt.show()","24fcbce7":"fig=plt.gcf()\nfig.set_size_inches(20,20)\nfig=sns.heatmap(df.corr(),annot=True,linewidths=1,linecolor='k',square=True,mask=False, vmin=-1, vmax=1,cbar_kws={\"orientation\": \"vertical\"},cbar=True)","6c0067d8":"fig = plt.figure(figsize=(10,10)) \nfig_dims = (3, 2)\n\n\nplt.subplot2grid(fig_dims, (0, 0))\ndf['Amenity'].value_counts().plot(kind='bar', \n                                     title='Amenity')\nplt.subplot2grid(fig_dims, (0, 1))\ndf['Crossing'].value_counts().plot(kind='bar', \n                                     title='Crossing')\nplt.subplot2grid(fig_dims, (1, 0))\ndf['Junction'].value_counts().plot(kind='bar', \n                                     title='Junction')\nplt.subplot2grid(fig_dims, (1, 1))\ndf['Junction'].value_counts().plot(kind='bar', \n                                     title='Junction')","3ffa2e64":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndf['Severity'].value_counts().plot.pie(explode=[0,0.1,0.1,0.1,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Percentage Severity Distribution')\nax[0].set_ylabel('Count')\nsns.countplot('Severity',data=df,ax=ax[1],order=df['Severity'].value_counts().index)\nax[1].set_title('Count of Severity')\nplt.show()","3ac46de5":"plt.figure(figsize=(12,8))\nsns.boxplot(x=\"Severity\", y=\"Wind_Chill(F)\", data=train_df)\nplt.ylabel('Wind_Chill(F)', fontsize=12)\nplt.xlabel('Severity', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()","4f5afc4e":"plt.figure(figsize=(12,8))\nsns.violinplot(x='Severity', y='Amenity', data=train_df)\nplt.xlabel('Severity', fontsize=12)\nplt.ylabel('Amenity', fontsize=12)\nplt.show()","e2c91e63":"\nplt.figure(figsize=(12,8))\nsns.violinplot(x='Severity', y='Wind_Chill(F)', data=train_df)\nplt.xlabel('Severity', fontsize=12)\nplt.ylabel('Wind_Chill(F)', fontsize=12)\nplt.show()","af250c94":"plt.figure(figsize=(12,8))\nsns.violinplot(x='Severity', y='Crossing', data=train_df)\nplt.xlabel('Severity', fontsize=12)\nplt.ylabel('Crossing', fontsize=12)\nplt.show()","a152ede2":"plt.figure(figsize=(12,8))\nsns.violinplot(x='Severity', y='Junction', data=train_df)\nplt.xlabel('Severity', fontsize=12)\nplt.ylabel('Junction', fontsize=12)\nplt.show()","5f524df5":"plt.figure(figsize=(12,8))\nsns.violinplot(x='Severity', y='Traffic_Signal', data=train_df)\nplt.xlabel('Severity', fontsize=12)\nplt.ylabel('Traffic_Signal', fontsize=12)\nplt.show()","457f476a":"df.dropna(subset=df.columns[df.isnull().mean()!=0], how='any', axis=0, inplace=True)\ndf.shape","72400a4e":"\ntrain_y = df['Severity'].values\nx_cols = [col for col in df.columns if col not in ['Severity'] if df[col].dtype=='float64']\ntrain_col= df[x_cols]\n\nfearture_name = train_col.columns.values \n\nfrom sklearn import ensemble \n\nmodel = ensemble.ExtraTreesRegressor(n_estimators=25, max_depth=30, max_features=0.3, n_jobs=-1, random_state=0)\nmodel.fit(train_col,train_y)\n\n#plot imp \nimportance = model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_],axis=0)\nindices = np.argsort(importance)[::-1][:20]\n\nplt.figure(figsize=(12,12))\nplt.title(\"Feature importances\")\nplt.bar(range(len(indices)), importance[indices], color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(len(indices)), fearture_name[indices], rotation='vertical')\nplt.xlim([-1, len(indices)])\nplt.show()","27910c6a":"import xgboost as xgb \n\nxgb_prames = {\n    'eta': 0.05,\n    'max_depth': 8,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:linear',\n    'silent': 1,\n    'seed' : 0\n}\n\ndtrain = xgb.DMatrix(train_col,train_y,feature_names=train_col.columns.values)\n\nmodel = xgb.train(dict(xgb_prames, silent=0), dtrain, num_boost_round=50)\n\n\nfig, ax = plt.subplots(figsize=(12,18))\nxgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\nplt.show()","9f0fa53c":"seems like Start_lng . Traffic_signals , start,lat ,TMC are more important Feature and followed by ..\n\nlets check with XGBoost also for Feature_importance_","80c43cdf":"here some data pages with lat and lng","1edbc03a":"this gives the same includes Distance(mi)","af3b2bcf":"Since there are so many variables, let us first take the 'float' variables alone and then get the correlation with the target variable to see how they are related.","6561c553":"this says that California this with high accidents\n\n\nlets go for EDA \n\ncheck for missing values ","0a6e86ed":"get highly correlated columns","4f323b36":"The correlation of the target variable with the given set of variables are low overall.\n\nthere are some variable with no correlation","7dd4b391":"Severity of the accident oue target ","248ff429":"get the ratio and the columns with more missing values above 80%","66d9c692":"to be contunued...soon \n\n**pleace upvote if you like that makes me motive... :)**","e5a1f42f":"lets sapater the datasets based on dtype so that we can make good analysis ","a9ac6b51":"lets once check for all the vriables ","420ef5ab":"lets check for the top5  Weather Condition for accidents","4772814e":"we move on making some changes ... ","41da8ff6":"ohhh ..... i think we have to check for the importance of the feature ... we will go with that ...."}}