{"cell_type":{"80ecd01c":"code","1e698724":"code","00aa1fcc":"code","065eea9e":"code","272b17c8":"code","e470073d":"code","7ec5f5ba":"code","c826cb0c":"code","038884d9":"code","594cbae9":"code","16594260":"code","7fc0c1a4":"code","67caeaa7":"code","a0320f0f":"code","ebb9fb37":"code","ee3f92ff":"code","d63f58b5":"code","1837c524":"code","44ea2855":"code","7bffd2be":"code","d1bc0059":"code","2bc2842a":"code","1f581631":"code","e0348b48":"code","3b507b30":"code","e2e79fb0":"code","c928dfff":"code","6379acca":"code","6d5a8bd5":"markdown","01367b2c":"markdown","ef02332b":"markdown","85bbd71a":"markdown","7804c621":"markdown","9254239d":"markdown","08498858":"markdown","121b9c9a":"markdown","1f695e7b":"markdown","84e3418d":"markdown","461d2d02":"markdown","693b11f1":"markdown","16710bf6":"markdown","2cfa32b9":"markdown","c3e3c272":"markdown","e8175d14":"markdown","9293ac6d":"markdown","d258cef2":"markdown","18d82e74":"markdown","31f230d8":"markdown","3ac8844c":"markdown","cdea1cec":"markdown"},"source":{"80ecd01c":"# Importing the libraries\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","1e698724":"# Reading the file\ndf = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","00aa1fcc":"# Removing the 'Time' column\ndf = df.drop(['Time'], axis = 1)","065eea9e":"# Checking for null values\ndf.isnull().sum().sum()","272b17c8":"# Understanding amount in fraud transactions\nplt.style.use('seaborn')\nfig, ax = plt.subplots(1 ,1, figsize = (10, 6), constrained_layout = True)\n\n# I used median instead of mean to avoid outliers\nax = sns.barplot(x = 'Class', y = 'Amount', data = df, estimator = np.median, ax = ax)\n\nplt.title(\"Average Amount in Fraud Transactions\", size = 16)\n\nax.set_xticklabels(['Fair', 'Fraud'], fontsize = 14)\n\nplt.xlabel(None)\nplt.ylabel('Amount of Transaction', fontsize = 14);","e470073d":"# EDA for the features V1, V2,..., V28\nv_feaures = df.iloc[:,:-2].columns.to_list()\n\nplt.style.use('seaborn')\nfig, ax = plt.subplots(7,4, figsize = (14, 24), constrained_layout = True)\n\nfor col, axis in zip(v_feaures, ax.ravel()): # ax.ravel() kind of flattens the 2d to 1d for iteration\n    axis = sns.kdeplot(x = col, data = df, fill = True, alpha = 0.6, linewidth = 1.5, ax=axis)\n    axis.set_title(col, fontsize = 14)\n    axis.set_xlabel(None)\n    axis.set_ylabel(None)\n    \nfig.suptitle('Distribution of the features V1, V2,..., V28', fontsize = 16, y = 1.01);","7ec5f5ba":"# Checking the skewness values\nplt.style.use('seaborn')\nfig, ax = plt.subplots(1,1, figsize = (10, 6), constrained_layout = True)\nax = df.skew(axis = 0).plot(kind = 'bar')\n\nfor i in ax.patches:\n    ax.text(x = i.get_x() + i.get_width()\/2, y = i.get_height()+0.5, \n            s = f\"{np.round(i.get_height(), 1)}\", \n            ha = 'center', size = 14, rotation = 0, color = 'black')\n    \nax.set_ylabel('Skewness', fontsize = 14)\nax.set_title('Skewness of the features', fontsize = 16);","c826cb0c":"from sklearn.preprocessing import quantile_transform\n\nv_features = df.iloc[:, :-2].columns\ndf[v_features] = pd.DataFrame(quantile_transform(df[v_features], n_quantiles=500), columns = v_features)\n\nprint('Skewness after Quantile Transform')\ndf[v_features].skew(axis = 0)","038884d9":"# Checking for class imbalance \nplt.style.use('seaborn')\nfig = plt.figure(figsize = (10, 6))\n\nax = sns.countplot(x = 'Class', data = df)\n\nfor i in ax.patches:\n    ax.text(x = i.get_x() + i.get_width()\/2, y = i.get_height()\/2, \n            s = f\"{np.round(i.get_height()\/len(df)*100, 3)}%\", \n            ha = 'center', size = 20, weight = 'bold', rotation = 0, color = 'black')\n\nplt.title(\"Credit Card Fraud Count\", size = 16)\n\nax.set_xticklabels(['Fair', 'Fraud'], fontsize = 14)\n\nplt.xlabel(None)\nplt.ylabel('Number of Transactions', fontsize = 14);","594cbae9":"# Splitting data into train and test\nfrom sklearn.model_selection import train_test_split\n\nX = df.drop(['Class'], axis = 1)\ny = df['Class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, stratify = df['Class'], random_state = 99)","16594260":"# Training model on imbalanced class data \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\nmodel = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n\nmatrix = metrics.confusion_matrix(y_test, model.predict(X_test))","7fc0c1a4":"plt.style.use('seaborn')\nfig, axis = plt.subplots(1,1, figsize=(10, 6), constrained_layout = True)\naxis = sns.heatmap(matrix, annot=True, fmt = '.0f', cbar=False, cmap='Blues',\n                        linewidths=3, square=True, ax = axis, annot_kws={\"fontsize\":20})\n\naxis.set_xlabel('Predicted', fontsize=14)\naxis.set_ylabel('Actual', fontsize=14)\naxis.set_xticklabels(['Fair','Fraud'], fontsize=12)\naxis.set_yticklabels(['Fair','Fraud'], fontsize=12, rotation=0);\nplt.title('Model Evluation using Confusion Matrix (Before Class Balancing)', fontsize = 16);","67caeaa7":"from imblearn.over_sampling import SMOTE, ADASYN\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nX_train_new, y_train_new = SMOTE().fit_resample(X_train, y_train)\n\nprint('Before resmapling : ', y_train.value_counts())\nprint('After resmapling : ', y_train_new.value_counts())\n\npipeline = Pipeline([('model', LogisticRegression(solver='liblinear'))])\npipeline.fit(X_train_new, y_train_new)","a0320f0f":"matrix_new = metrics.confusion_matrix(y_test, pipeline.predict(X_test))\nplt.style.use('seaborn')\nfig, axis = plt.subplots(1,1, figsize=(10, 6), constrained_layout = True)\naxis = sns.heatmap(matrix_new, annot=True, fmt = '.0f', cbar=False, cmap='Blues',\n                        linewidths=3, square=True, ax = axis, annot_kws={\"fontsize\":20})\n\naxis.set_xlabel('Predicted', fontsize=14)\naxis.set_ylabel('Actual', fontsize=14)\naxis.set_xticklabels(['Fair','Fraud'], fontsize=12)\naxis.set_yticklabels(['Fair','Fraud'], fontsize=12, rotation=0);\nplt.title('Model Evluation using Confusion Matrix (After Class Balancing)', fontsize = 16);","ebb9fb37":"# Oversampling minority class using ADASYN\nX_train_ada, y_train_ada = ADASYN().fit_resample(X_train, y_train)\n\nprint('Before resmapling : ', y_train.value_counts())\nprint('After resmapling : ', y_train_ada.value_counts())\n\npipeline = Pipeline([('model', LogisticRegression(solver='liblinear'))])\npipeline.fit(X_train_ada, y_train_ada)","ee3f92ff":"matrix_new = metrics.confusion_matrix(y_test, pipeline.predict(X_test))\nplt.style.use('seaborn')\nfig, axis = plt.subplots(1,1, figsize=(10, 6), constrained_layout = True)\naxis = sns.heatmap(matrix_new, annot=True, fmt = '.0f', cbar=False, cmap='Blues',\n                        linewidths=3, square=True, ax = axis, annot_kws={\"fontsize\":20})\n\naxis.set_xlabel('Predicted', fontsize=14)\naxis.set_ylabel('Actual', fontsize=14)\naxis.set_xticklabels(['Fair','Fraud'], fontsize=12)\naxis.set_yticklabels(['Fair','Fraud'], fontsize=12, rotation=0);\nplt.title('Model Evluation using Confusion Matrix (After Class Balancing)', fontsize = 16);","d63f58b5":"from sklearn.linear_model import LogisticRegressionCV\npipeline = Pipeline([('model', LogisticRegressionCV(solver='liblinear', cv = 5))])\npipeline.fit(X_train_ada, y_train_ada)","1837c524":"# Plotting the ROC Curve and finding optimal threshold\n# Function for plotting ROC curve, confurion matrix and finding optimal threshold\ndef my_roc_curve(model, accuracy_weight, title, X_train, y_train):\n    from sklearn import metrics\n    # Plotting AUC ROC curve\n    y_scores = model.predict(X_train)\n    fpr, tpr, thresholds = metrics.roc_curve(y_train, y_scores)\n    \n    fig, (ax1,ax2) = plt.subplots(1,2, figsize = (12, 6), constrained_layout = True)\n    ax1.plot(fpr, tpr, color='skyblue', label='ROC')\n    ax1.plot([0, 1], [0, 1], color='pink', linestyle='--')\n    ax1.text(x = 0.8, y = 0.3,\n            s = f\"AUC : {round(metrics.roc_auc_score(y_train, y_scores),2)}\",\n            ha = 'center', size = 12, rotation = 0, color = 'black',\n            bbox=dict(boxstyle=\"round,pad=0.5\", fc='skyblue', ec=\"skyblue\", lw=2));\n\n    ax1.set_xlabel('False Positive Rate', fontsize = 12)\n    ax1.set_ylabel('True Positive Rate', fontsize = 12)\n    ax1.set_title(f'ROC curve',  fontsize=16, y=1.05)\n\n    # Plotting optimal Confusion Matrix\n    from sklearn import metrics\n    probability = model.predict_proba(X_train)\n    matrix = pd.DataFrame()\n    \n    base_accuracy = metrics.accuracy_score(y_train, model.predict(X_train))\n    base_recall = metrics.recall_score(y_train, model.predict(X_train))\n    best_score = 0.6*base_accuracy + 0.4*base_recall\n    best_thresold = 0.5\n    \n    # Finding optimul thresold to maximize > accuracy_weight*threshold_accuracy + (1-accuracy_weight)*threshold_recall\n    for threshold in np.linspace(0, 1, 100):\n        y_predict = (probability>=threshold).astype(int)[:,1]\n        threshold_accuracy = metrics.accuracy_score(y_train, y_predict)\n        threshold_recall = metrics.recall_score(y_train, y_predict)\n        weighted_score = accuracy_weight*threshold_accuracy + (1-accuracy_weight)*threshold_recall\n        \n        if weighted_score>best_score:\n            best_thresold = threshold\n            best_score = weighted_score\n    \n    y_predict = (probability>=best_thresold).astype(int)[:,1]\n    matrix = metrics.confusion_matrix(y_train, y_predict)\n\n    ax2 = sns.heatmap(matrix, annot=True, fmt = '.0f', cbar=False, cmap='Blues',\n                        linewidths=3, square=True, ax = ax2, annot_kws={\"fontsize\":20})\n    ax2.set_title(f\"Confusion Matrics | Threshold : {round(best_thresold, 2)}\", fontsize=16, y=1.05);\n    ax2.set_xlabel('Predicted', fontsize=12)\n    ax2.set_ylabel('Actual', fontsize=12)\n    ax2.set_xticklabels([0,1], fontsize=12 )\n    ax2.set_yticklabels([0,1], fontsize=12, rotation=0)\n    \n    print('Optimal Threshold for Accuracy and Recall is : ', round(best_thresold, 2))\n    print(f\"Train Accuracy for  {title}: \", round(metrics.accuracy_score(y_train, y_predict),2), \n          f\"| Train Recall for {title}: \", round(metrics.recall_score(y_train, y_predict),2))\n    \n    plt.suptitle(f'{title}', fontsize=19, y=1.05)\n    \n    # For test data\n    probability = model.predict_proba(X_test)\n    y_predict = (probability>=best_thresold).astype(int)[:,1]\n    print(f\"Test Accuracy for {title}: \", round(metrics.accuracy_score(y_test, y_predict),2),\n         f\"| Test Recall for {title}: \", round(metrics.recall_score(y_test, y_predict),2))","44ea2855":"my_roc_curve(pipeline, 0.5, 'Logistic Regression', X_train_ada, y_train_ada)","7bffd2be":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\n\npipeline = Pipeline([('tree', DecisionTreeClassifier(random_state = 99))])\nparms = {'tree__max_depth': [3, 7, 11],\n        'tree__min_samples_leaf':[50, 100, 200]}\n\ntree_model = GridSearchCV(pipeline, parms, cv = 5)\ntree_model.fit(X_train_ada, y_train_ada)","d1bc0059":"# Best Estimator\ntree_model.best_estimator_","2bc2842a":"my_roc_curve(tree_model, 0.5, 'Decision Tree', X_train_ada, y_train_ada)","1f581631":"from catboost import CatBoostClassifier\n\ncat_model = CatBoostClassifier(task_type = 'GPU', od_type = 'Iter')","e0348b48":"# Creating evaluation set for catboost\nX_train_cat, X_eval, y_train_cat, y_eval = train_test_split(X_train_ada, y_train_ada, stratify = y_train_ada,\n                                                            random_state = 99)","3b507b30":"cat_model.fit(X_train_cat, y_train_cat, use_best_model = True,\n              eval_set = (X_eval, y_eval), verbose = 100, early_stopping_rounds = 50)","e2e79fb0":"my_roc_curve(cat_model, 0.5, 'CatBoost', X_train_cat, y_train_cat)","c928dfff":"probability = cat_model.predict_proba(X_test)\ny_predict = (probability>=0.7).astype(int)[:,1]\nmatrix = metrics.confusion_matrix(y_test, y_predict)\nmatrix","6379acca":"plt.style.use('seaborn')\nfig, axis = plt.subplots(1,1, figsize=(10, 6), constrained_layout = True)\naxis = sns.heatmap(matrix, annot=True, fmt = '.0f', cbar=False, cmap='Blues',\n                        linewidths=3, square=True, ax = axis, annot_kws={\"fontsize\":20})\n\naxis.set_xlabel('Predicted', fontsize=14)\naxis.set_ylabel('Actual', fontsize=14)\naxis.set_xticklabels(['Fair','Fraud'], fontsize=12)\naxis.set_yticklabels(['Fair','Fraud'], fontsize=12, rotation=0);\nplt.title('CatBoost Confusion Matrix (Optimal Threshold)', fontsize = 16);","6d5a8bd5":"<h4><center> <div style=\"background-color:violet;border-radius:10px; padding: 10px;\">Decision Tree \ud83c\udf33<\/div><\/center><\/h4>","01367b2c":"> I will be using AUC ROC curve for model evaluation as it is independent of the threshold. Sklearn considers `0.5` as threshold for calculating Accuracy, Precision, Recall, etc. but the value of threshold varies with models, and so I will be using AUC ROC curve to find out the best threshold and use that to predict.","ef02332b":"> Many features have normal distribution. Each of the PCs given by PCA is a linear combination of the original features and the original features were normally distributed , so every linear combination of them i.e. PCs are also normally distributed.\n\n> There are also some features which are skew distributed, so I will use quantile transform for this purprose. I will be considering (-0.5,0.5) range for skewness, any feature having value beyound this range will get transformed.\n\n\u2755 I have tried log transform by adding some constant to negetive values and also inverse hyperbolic tangent transform by scaling features in (-1, 1) range, but none of them reduced the skewness, so I am using quantile transform.","85bbd71a":"<h2><center> <div style=\"background-color:skyblue;border-radius:10px; padding: 10px;\">Data Prepration<\/div><\/center><\/h2>\n\n> The data contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, \u2026 V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are `Time` and `Amount`. Feature `Time` contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature `Amount` is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. Feature `Class` is the response variable and it takes value 1 in case of fraud and 0 otherwise.","7804c621":"#### Overfitting!","9254239d":"<h4><center> <div style=\"background-color:pink;border-radius:10px; padding: 10px;\">If you like it, don't forget to upvote!<\/div><\/center><\/h4>","08498858":"> There are 35 False Negetives out of 148. It means that, out of 148 frauds, our model couldn't identify 35 (24%), this is mainly due to class imbalance. We don't want to let the fraud escape this easily, so let's balance the classes.\n\n> I am using SMOTE and ADASYN methods. They both are oversampling methods, i.e. they create synthetic data for manority class so that it becomes similar in count with majority class.\n\n<center><img src = https:\/\/miro.medium.com\/max\/1200\/1*0jwntVGaj7qQkr-MeueQcQ.jpeg><\/center>\n\nSMOTE (Synthetic Minority Over-sampling Technique) creates the synthetic data for minority class, using linear method. It takes two close minority points and creates a new point on a line joining the two points.\n\nADASYN (Adaptive Synthetic) uses a weighted distribution for different minority class examples according to their level of difficulty in learning, where more synthetic data is generated for minority class examples that are harder to learn compared to those minority examples that are easier to learn. It creates the synthetic data near to the minority points having less density. ","121b9c9a":"<h4><center> <div style=\"background-color:violet;border-radius:10px; padding: 10px;\">Logistic Regression<\/div><\/center><\/h4>","1f695e7b":"> On an average, the amount in fraud transactions is lower than fair transactions. The reason I think can be, the fraud's are smart \ud83e\udde0, they know if they do a fraud of big amount they may get caught \ud83d\ude94, so they do multiple frauds of smaller amount.","84e3418d":"> The recall is slightly improved but on the expense of precision.","461d2d02":"<h4><center> <div style=\"background-color:violet;border-radius:10px; padding: 10px;\">CatBoost \ud83c\udf33\u27a1\ud83c\udf33\u27a1\ud83c\udf33\u27a1\ud83c\udf33<\/div><\/center><\/h4>","693b11f1":"> `Class` 0 : Fair transaction \ud83d\udc4d\ud83c\udffd | 1 : Fraud transaction \ud83e\uddb8\u200d\u2642\ufe0f\n\n> `Time` column won't of much importance while indentifying a fraud, as it only contains the seconds elapsed between each transaction and the first transaction in the dataset, so I will remove it.","16710bf6":"<h4><center> <div style=\"background-color:violet;border-radius:10px; padding: 10px;\">Quantile Transform<\/div><\/center><\/h4>","2cfa32b9":"> There are only 9 False Negetives out of 148. It means that, out of 148 frauds, our model couldn't identify only 9 (6%) frauds. Previously, before class balancing, False Negetives were 35 (24%) and now they are 9, 4 times reduction!\n\n> Let's try with ADASYN.","c3e3c272":"> That looks good! We got a recall of 84% and 97% accuracy!","e8175d14":"<h3><center> <div style=\"background-color:lightpink;border-radius:10px; padding: 10px;\">Class Imbalance \u2696<\/div><\/center><\/h3>\n\n> Imbalanced classifications pose a challenge for predictive modeling as most of the machine learning algorithms used for classification were designed around the assumption of an equal number of examples for each class. This results in models that have poor predictive performance, specifically for the minority class.","9293ac6d":"<h2><center> <div style=\"background-color:skyblue;border-radius:10px; padding: 10px;\">Model Building \ud83e\udd16<\/div><\/center><\/h2>\n\n> I am focusing on recall as, it a good measure of false negetives. If a fair transaction is predicted as fraud, then manual step can mitigate this problem but if fraud is predicted as fair, then it can cause loss to the customer as well as bank.\n\n> But if only recall the prime importance then the threshold will be set at 0 so that we get recall as 100% but very high false positives, so I am giving 50% weightage to accuracy and 50% to recall.","d258cef2":"> Only 0.173% tractions are fraud, which is very much less than other class. If we train our model on this unbalanced class data, then the model won't learn much from Fraud class and will try to give more importance to Fair class, which we don't want. Let's see that in action, I will train a simple logistic regression model and see its performance.","18d82e74":"<h1><center><div style=\"background-color:skyblue;border-radius:10px; padding: 10px;\">Introduction<\/div><\/center><\/h1>\n\n\ud83d\ude45\ud83c\udffd\u200d\u2640\ufe0f Motivation\n> Suppose you get a call \ud83d\udcde from your bank, and the customer care executive informs you that your card is about to expire in a week. Immediately, you check your card details and realise that it will expire in the next eight days. Now, to renew your membership, the executive asks you to verify a few details such as your credit card number, the expiry date and the CVV number. Will you share these details with the executive?\n> In such situations, you need to be careful because the details that you might share with them could grant them unhindered access to your credit card account.\n \n> Banks \ud83c\udfe6 need to be cautious about their customers\u2019 transactions, as they cannot afford to lose their customers\u2019 money to fraudsters \ud83e\uddb8\u200d\u2642\ufe0f. Every fraud is a loss to the bank, as the bank is responsible for the fraudulent transactions if they are reported within a certain time frame by the customer.\n\n\ud83c\udfaf Goal\n> The goal of this notebook is to predict the fraud transactions with high accuracy.\n\n#### Index\n\n- 1. Data preparation\n    - Exploring data, and removing unnecessary columns\n    - Transforming data\n    - Handling class imbalance\n    - Creating training and testing data\n- 2. Model building\n    - Building models\n    - Hyperparameter tuning\n- 3. Model evaluation\n    - Evaluating model using AUC ROC curve","31f230d8":"> The skewness has been reduced close to 0! Now let's check for the class imbalance.","3ac8844c":"<h3><center> <div style=\"background-color:lightpink;border-radius:10px; padding: 10px;\">Data Exploration<\/div><\/center><\/h3>\n\n> The data contains PCA transformed features, so the columns are meaningless except the `Amount` and `Class`. I will only perform the EDA which is necessary for building model.","cdea1cec":"<h3><center> <div style=\"background-color:lightpink;border-radius:10px; padding: 10px;\">\ud83e\udd5a Data Transformation \ud83d\udc23<\/div><\/center><\/h3>"}}