{"cell_type":{"8e131511":"code","727b5bb1":"code","cf9fc545":"code","539e0cc7":"code","499256c4":"code","7cfc3fc3":"markdown","e6f63d4e":"markdown","1f92e46b":"markdown","0ddae79e":"markdown","2e6d08e0":"markdown"},"source":{"8e131511":"#Importing the files\nimport numpy as np\nimport pandas as pd\n\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","727b5bb1":"from sklearn.model_selection import train_test_split\nfeatures = ['PassengerId', 'Pclass', 'Sex', 'Age', 'Fare', 'SibSp']\nX_train, X_valid, y_train, y_valid =  train_test_split(pd.get_dummies(train[features]), train['Survived'])\nX_test = pd.get_dummies(test[features]); y_test = None","cf9fc545":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy='constant')","539e0cc7":"from xgboost import XGBRegressor\nfrom sklearn.pipeline import Pipeline\nxgb = XGBRegressor(n_estimators=500, learning_rate=0.15)\npipe = Pipeline(steps=[('preprocessor', imputer),\n                       ('model', xgb)])","499256c4":"pipe.fit(X=X_train, y=y_train, model__early_stopping_rounds=5, model__eval_set=[(X_valid.values, y_valid.values)], model__verbose=False)\ny_test = pipe.predict(X_test)\nout = []\nfor val in y_test:\n    out.append(1 if val >= .5 else 0)\noutput = pd.DataFrame({'PassengerId': X_test.PassengerId, 'Survived': out})\noutput.to_csv('output.csv', index=False)","7cfc3fc3":"**Step 2:** Addressing null values with the help of an **Imputer** (fits and transforms the data).","e6f63d4e":"**Step 3:** Creating a Pipeline for the model:<br\/>\n**learning_rate**, afaik, restrains the amount of change the model undergoes during each iteration.<br\/>\n**n_estimators** specifies max. number of models that can be added into the ensemble.","1f92e46b":"Just completed the Intermediate ML microcourse, and decided to try out its fundamentals on everyone's favourite dataset...<br\/>\n<b>Model used: Xtreme Gradient Boost<\/b><br\/>\nThough probably not the ideal model for this dataset, its performance is significantly better than that of my previous submissions.<br\/>\n**Edit: Its performance is fluctuating significantly from one run to another, and on average its accuracy is worse than the default model (Random Forests), which isn't really surprising.**","0ddae79e":"**Step 1:** Segregating the Data into 2 groups (X and y), and then into two sets: one for training and the other for validation.<br\/>\nThe pd.get_dummies() converts strings to indicator ints.","2e6d08e0":"**Step 4:** Running the model:<br\/>\n**early_stopping_rounds** specifies the number of rounds after which to stop after either undergoing constant degradation \/ there being little to no change in accuracy."}}