{"cell_type":{"7a073da7":"code","26a2ca8d":"code","8f6720d1":"code","cc2f54d8":"code","6b3cd8bf":"code","ac194410":"code","e1394b38":"code","3056f651":"code","3eac1407":"code","ffda69a6":"code","adcf2124":"code","fd92d0e1":"code","b6f8d2b8":"code","97681424":"code","41b5147d":"code","46c9bb72":"code","42e90e56":"code","0690eaaf":"code","6ece297d":"code","246a3073":"code","701d1502":"code","97f1f9a4":"code","8aac2338":"code","266ee395":"markdown","0dac48f3":"markdown","118fa390":"markdown","bb1236db":"markdown","ee32ab46":"markdown","d427e29b":"markdown","433b4253":"markdown","18165b0c":"markdown","baebce3f":"markdown","a568b519":"markdown","56c08535":"markdown","b2218f2c":"markdown","05291c3d":"markdown","0ccb6987":"markdown","0b6d2d2d":"markdown","93616cab":"markdown","3ddd569e":"markdown"},"source":{"7a073da7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","26a2ca8d":"np.random.seed(0)\nX_train_fpath = '\/kaggle\/input\/ml2020spring-hw2\/data\/X_train'\nY_train_fpath = '\/kaggle\/input\/ml2020spring-hw2\/data\/Y_train'\nX_test_fpath = '\/kaggle\/input\/ml2020spring-hw2\/data\/X_test'\noutput_fpath = '\/kaggle\/working\/output_{}.csv'\n# \u8bfb\u53d6csv\u4fdd\u5b58\u5728numpy\u6570\u7ec4\nwith open(X_train_fpath) as f:\n    next(f)\n    X_train = np.array([line.strip('\\n').split(',')[1:] for line in f], dtype = float)#strip()\u8868\u793a\u5220\u9664\u6389\u6570\u636e\u4e2d\u7684\u6362\u884c\u7b26\uff0csplit\uff08\u2018,\u2019\uff09\u5219\u662f\u6570\u636e\u4e2d\u9047\u5230\u2018,\u2019 \u5c31\u9694\u5f00\nwith open(Y_train_fpath) as f:\n    next(f)\n    Y_train = np.array([line.strip('\\n').split(',')[1] for line in f], dtype = float)\nwith open(X_test_fpath) as f:\n    next(f)\n    X_test = np.array([line.strip('\\n').split(',')[1:] for line in f], dtype = float)\nX_train, Y_train","8f6720d1":"\ndef _normalize(X, train = True, specified_column = None, X_mean = None, X_std = None):\n    # \u8fd9\u4e2a\u51fd\u6570\u5bf9X\u4e2d\u7684\u7279\u5b9a\u5217\u6807\u51c6\u5316.\n    # \u8bad\u7ec3\u96c6\u7684\u5e73\u5747\u503c\u4e0e\u6807\u51c6\u5dee\u4f1a\u5728\u6d4b\u8bd5\u96c6\u4e2d\u518d\u6b21\u88ab\u4f7f\u7528\n    #\n    # \u53c2\u6570:\n    #     X: \u88ab\u9884\u6d4b\u6570\u636e\n    #     train: 'True' \u8bad\u7ec3\u96c6, 'False' \u6d4b\u8bd5\u96c6\n    #     specific_column: \u8981\u88ab\u6807\u51c6\u5316\u7684\u5217\u7d22\u5f15, 'None'\u4ee3\u8868\u6807\u51c6\u5316\u6240\u6709\u5217.\n    #     X_mean: \u8bad\u7ec3\u96c6\u7684\u5e73\u5747\u503c, \u5f53train = 'False'\u5373\u6d4b\u8bd5\u96c6\u4e2d\u4f7f\u7528.\n    #     X_std: \u8bad\u7ec3\u96c6\u7684\u6807\u51c6\u5dee, \u5f53train = 'False'\u5373\u6d4b\u8bd5\u96c6\u4e2d\u4f7f\u7528.\n    # \u8f93\u51fa:\n    #     X: \u6807\u51c6\u5316\u540e\u6570\u636e\n    #     X_mean: \u8bad\u7ec3\u96c6\u7684\u5e73\u5747\u503c\n    #     X_std: \u8bad\u7ec3\u96c6\u7684\u5e73\u5747\u503c\n\n    if specified_column == None:\n        specified_column = np.arange(X.shape[1])#shape[1]\u4ee3\u8868\u77e9\u9635\u7684\u5217\u6570\n    if train:\n        X_mean = np.mean(X[:, specified_column] ,0).reshape(1, -1)#reshape(1,-1)\u8f6c\u6362\u4e3a\u884c\u65701, \u5217\u6570\u6839\u636e\u884c\u6570\u8ba1\u7b97\n        X_std  = np.std(X[:, specified_column], 0).reshape(1, -1)\n\n    X[:,specified_column] = (X[:, specified_column] - X_mean) \/ (X_std + 1e-8)#(x-\u03bc)\/\u03c3\n     \n    return X, X_mean, X_std\n\ndef _train_dev_split(X, Y, dev_ratio = 0.25):\n    # \u8fd9\u4e2a\u51fd\u6570\u5c06\u6570\u636e\u5206\u5272\u4e3a\u8bad\u7ec3\u96c6, \u53d1\u5c55\u96c6\n    train_size = int(len(X) * (1 - dev_ratio))\n    return X[:train_size], Y[:train_size], X[train_size:], Y[train_size:]\n\n# \u5bf9\u8bad\u7ec3\u96c6\u6d4b\u8bd5\u96c6\u6807\u51c6\u5316\nX_train, X_mean, X_std = _normalize(X_train, train = True)\nX_test, _, _= _normalize(X_test, train = False, specified_column = None, X_mean = X_mean, X_std = X_std)#'_'\u505a\u53d8\u91cf\u540d\u5408\u6cd5, \u4f5c\u4e3a\u65e0\u7528\u7684\u4e34\u65f6\u53d8\u91cf\n    \n# \u5206\u5272\u8bad\u7ec3\u96c6, \u53d1\u5c55\u96c6\ndev_ratio = 0.1\nX_train, Y_train, X_dev, Y_dev = _train_dev_split(X_train, Y_train, dev_ratio = dev_ratio)\n\ntrain_size = X_train.shape[0]#\u8bad\u7ec3\u96c6\u884c\u6570\ndev_size = X_dev.shape[0]#\u53d1\u5c55\u96c6\u884c\u6570\ntest_size = X_test.shape[0]#\u6d4b\u8bd5\u96c6\u884c\u6570\ndata_dim = X_train.shape[1]#\u8bad\u7ec3\u96c6\u5217\u6570(\u7ef4\u5ea6)\nprint('Size of training set: {}'.format(train_size))\nprint('Size of development set: {}'.format(dev_size))\nprint('Size of testing set: {}'.format(test_size))\nprint('Dimension of data: {}'.format(data_dim))","cc2f54d8":"def _shuffle(X, Y):\n    # \u8be5\u51fd\u6570\u5bf9\u4e24\u4e2a\u7b49\u957f\u7684\u5217\u8868\u6216\u6570\u7ec4\u6d17\u724c(XY\u540c\u6b65).\n    randomize = np.arange(len(X))#arange(len(X))\u8fd4\u56de0,1...len(X)\n    np.random.shuffle(randomize)#shuffle()\u65b9\u6cd5\u5c06\u5e8f\u5217\u7684\u6240\u6709\u5143\u7d20\u968f\u673a\u6392\u5e8f\n    return (X[randomize], Y[randomize])","6b3cd8bf":"def _sigmoid(z):\n    # Sigmoid\u51fd\u6570\u7528\u6765\u8ba1\u7b97\u6982\u7387.\n    # \u4e3a\u907f\u514d\u6ea2\u51fa, \u9650\u5b9a\u8f93\u51fa\u7684\u6700\u5927\u6700\u5c0f\u503c.\n    return np.clip(1 \/ (1.0 + np.exp(-z)), 1e-8, 1 - (1e-8))#clip\u5c06\u6570\u9650\u5b9a\u5230\u8303\u56f41e-8\u548c1-(1e-8)\u4e2d","ac194410":"def _f(X, w, b):\n    # \u5bf9\u7387\u56de\u5f52\u51fd\u6570, \u7528w b\u4f5c\u4e3a\u53c2\u6570\n    #\n    # \u53c2\u6570:\n    #     X: \u8f93\u5165\u6570\u636e, shape = [\u5c0f\u6279\u6b21\u7684\u5c3a\u5bf8batch_size, data_dimension]\n    #     w: weight \u5411\u91cf, shape = [data_dimension, ]\n    #     b: bias, \u6807\u91cf\n    # \u8f93\u51fa:\n    #     \u6bcf\u4e00\u884cX\u4e3a\u6b63\u4f8b\u7684\u9884\u6d4b\u6982\u7387, shape = [batch_size, ]\n    return _sigmoid(np.matmul(X, w) + b)#matmul\u77e9\u9635\u76f8\u4e58","e1394b38":"def _predict(X, w, b):\n    # \u901a\u8fc7\u628a\u5bf9\u7387\u56de\u5f52\u51fd\u6570\u7684\u7ed3\u679c\u56db\u820d\u4e94\u5165(round), \u5e76\u8f6c\u6362\u7c7b\u578b, \u5f97\u5230\u6bcf\u884cX\u771f\u6b63\u7684\u9884\u6d4b\u503c.\n    return np.round(_f(X, w, b)).astype(np.int)\n    \ndef _accuracy(Y_pred, Y_label):\n    # \u8be5\u51fd\u6570\u8ba1\u7b97\u9884\u6d4b\u7cbe\u786e\u5ea6\n    acc = 1 - np.mean(np.abs(Y_pred - Y_label))#\u8bef\u5dee1\/n*\u03a3|y*-y|\n    return acc","3056f651":"def _cross_entropy_loss(y_pred, Y_label):\n    # \u8be5\u51fd\u6570\u8ba1\u7b97\u4ea4\u53c9\u71b5.\n    #\n    # \u53c2\u6570:\n    #     y_pred: \u9884\u6d4b\u6982\u7387, float vector\n    #     Y_label: \u6807\u51c6\u7b54\u6848(ground truth)\u6807\u8bb0, bool vector\n    # Output:\n    #     cross entropy\u4ea4\u53c9\u71b5, scalar\n    cross_entropy = -np.dot(Y_label, np.log(y_pred)) - np.dot((1 - Y_label), np.log(1 - y_pred))#log\u9ed8\u8ba4\u4ee5e\u4e3a\u5e95\n    return cross_entropy","3eac1407":"def _gradient(X, Y_label, w, b):\n    # \u5173\u4e8eweight w\u548cbias b, \u7528\u4ea4\u53c9\u71b5\u635f\u5931\u8ba1\u7b97\u68af\u5ea6.\n    y_pred = _f(X, w, b)#\u5bf9\u7387\u56de\u5f52\n    pred_error = Y_label - y_pred#\u8bef\u5dee\n    w_grad = -np.sum(pred_error * X.T, 1)#sum\u53c2\u6570axis=1\u662f\u538b\u7f29\u5217,\u5373\u5c06\u6bcf\u4e00\u884c\u7684\u5143\u7d20\u76f8\u52a0,\u5c06\u77e9\u9635\u538b\u7f29\u4e3a\u4e00\u5217\n    b_grad = -np.sum(pred_error)\n    return w_grad, b_grad","ffda69a6":"# def _regularization_gradient(X, Y_label, w, b):\n#     regularization_weight = 1\n#     # \u5173\u4e8eweight w\u548cbias b, \u7528\u4ea4\u53c9\u71b5\u635f\u5931\u8ba1\u7b97\u68af\u5ea6.\n#     y_pred = _f(X, w, b)#\u5bf9\u7387\u56de\u5f52\n#     pred_error = Y_label - y_pred#\u8bef\u5dee\n#     regularization = pred_error+regularization_weight*np.dot(w, w)#\u6b63\u5219\u5316\u635f\u5931, scalar\n# #     w_grad = -np.sum((pred_error+regularization) * X.T, 1)\n#     w_grad = -np.sum((pred_error) * X.T, 1)#sum\u53c2\u6570axis=1\u662f\u538b\u7f29\u5217,\u5373\u5c06\u6bcf\u4e00\u884c\u7684\u5143\u7d20\u76f8\u52a0,\u5c06\u77e9\u9635\u538b\u7f29\u4e3a\u4e00\u5217\n#     b_grad = -np.sum(pred_error)\n# #     print(np.shape(pred_error), np.shape(X.T))\n#     return w_grad, b_grad","adcf2124":"# # baseline-cell\n# # \u5c06weights\u548cbias\u521d\u59cb\u5316\u4e3a0\n# w = np.zeros((data_dim,)) #zeros\u7b2c\u4e00\u4e2a\u53c2\u6570\u4e3a\u5f62\u72b6\n# b = np.zeros((1,))\n\n# # \u8bad\u7ec3\u8d85\u53c2\u6570  \n# # max_iter = 10\n# max_iter = 20\n# batch_size = 8\n# learning_rate = 0.2\n\n# # \u4fdd\u5b58\u6bcf\u4e00\u6b21\u8fed\u4ee3\u7684\u635f\u5931\u4e0e\u7cbe\u786e\u5ea6, \u7528\u6765\u753b\u56fe\n# train_loss = []\n# dev_loss = []\n# train_acc = []\n# dev_acc = []\n\n# # \u8ba1\u53c2\u6570\u66f4\u65b0\u6b21\u6570(\u6b65\u6570)\n# step = 1\n\n# # \u8fed\u4ee3\u5f0f\u8bad\u7ec3\n# for epoch in range(max_iter):\n#     # \u6bcf\u4e00\u4ee3(epoch)\u968f\u673a\u6d17\u724c\n#     X_train, Y_train = _shuffle(X_train, Y_train)\n        \n#     # \u5c0f\u6279\u8bad\u7ec3\n#     for idx in range(int(np.floor(train_size \/ batch_size))):\n#         # \u53d6\u8fd9\u4e00\u5c0f\u6279\u7684X, Y\n#         X = X_train[idx*batch_size:(idx+1)*batch_size]\n#         Y = Y_train[idx*batch_size:(idx+1)*batch_size]\n\n#         # \u8ba1\u7b97\u68af\u5ea6\n#         w_grad, b_grad = _gradient(X, Y, w, b)\n            \n#         # \u5229\u7528\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u53c2\u6570wb\n#         # \u5b66\u4e60\u7387\u968f\u65f6\u95f4(step)\u51cf\u5c11\n#         w = w - learning_rate\/np.sqrt(step) * w_grad\n#         b = b - learning_rate\/np.sqrt(step) * b_grad\n\n#         step = step + 1\n            \n#     # \u8ba1\u7b97\u8bad\u7ec3\u96c6\u4e0e\u53d1\u5c55\u96c6\u7684\u635f\u5931\u4e0e\u7cbe\u786e\u5ea6\n#     y_train_pred = _f(X_train, w, b)\n#     Y_train_pred = np.round(y_train_pred)\n#     train_acc.append(_accuracy(Y_train_pred, Y_train))\n#     train_loss.append(_cross_entropy_loss(y_train_pred, Y_train) \/ train_size)#\/ train_size\u6d88\u9664\u8bad\u7ec3\u96c6\u4e0e\u53d1\u5c55\u96c6\u5927\u5c0f\u4e0d\u540c\u5e26\u6765\u7684\u5f71\u54cd\n\n#     y_dev_pred = _f(X_dev, w, b)\n#     Y_dev_pred = np.round(y_dev_pred)\n#     dev_acc.append(_accuracy(Y_dev_pred, Y_dev))\n#     dev_loss.append(_cross_entropy_loss(y_dev_pred, Y_dev) \/ dev_size)\n\n# print('Training loss: {}'.format(train_loss[-1]))#[-1]\u8868\u793a\u6570\u7ec4\u4e2d\u6700\u540e\u4e00\u4f4d\n# print('Development loss: {}'.format(dev_loss[-1]))\n# print('Training accuracy: {}'.format(train_acc[-1]))\n# print('Development accuracy: {}'.format(dev_acc[-1]))","fd92d0e1":"# report2-cell\n# \u5c06weights\u548cbias\u521d\u59cb\u5316\u4e3a0\nw = np.zeros((data_dim,)) #zeros\u7b2c\u4e00\u4e2a\u53c2\u6570\u4e3a\u5f62\u72b6\nb = np.zeros((1,))\n\n# \u8bad\u7ec3\u8d85\u53c2\u6570  \n# max_iter = 10\nmax_iter = 20\nbatch_size = 8\nlearning_rate = 0.2\nregularization_weight = 0.005\n\n\n# \u4fdd\u5b58\u6bcf\u4e00\u6b21\u8fed\u4ee3\u7684\u635f\u5931\u4e0e\u7cbe\u786e\u5ea6, \u7528\u6765\u753b\u56fe\ntrain_loss = []\ndev_loss = []\ntrain_acc = []\ndev_acc = []\n\n# \u8ba1\u53c2\u6570\u66f4\u65b0\u6b21\u6570(\u6b65\u6570)\nstep = 1\n\n# \u8fed\u4ee3\u5f0f\u8bad\u7ec3\nfor epoch in range(max_iter):\n    # \u6bcf\u4e00\u4ee3(epoch)\u968f\u673a\u6d17\u724c\n    X_train, Y_train = _shuffle(X_train, Y_train)\n        \n    # \u5c0f\u6279\u8bad\u7ec3\n    for idx in range(int(np.floor(train_size \/ batch_size))):\n        # \u53d6\u8fd9\u4e00\u5c0f\u6279\u7684X, Y\n        X = X_train[idx*batch_size:(idx+1)*batch_size]\n        Y = Y_train[idx*batch_size:(idx+1)*batch_size]\n\n        # \u8ba1\u7b97\u68af\u5ea6\n        w_grad, b_grad = _gradient(X, Y, w, b)\n            \n        # \u5229\u7528\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u53c2\u6570wb\n        # \u5b66\u4e60\u7387\u968f\u65f6\u95f4(step)\u51cf\u5c11\n        w = w*(1-(learning_rate\/np.sqrt(step))*regularization_weight) - learning_rate\/np.sqrt(step) * w_grad\n        b = b - learning_rate\/np.sqrt(step) * b_grad\n\n        step = step + 1\n            \n    # \u8ba1\u7b97\u8bad\u7ec3\u96c6\u4e0e\u53d1\u5c55\u96c6\u7684\u635f\u5931\u4e0e\u7cbe\u786e\u5ea6\n    y_train_pred = _f(X_train, w, b)\n    Y_train_pred = np.round(y_train_pred)\n    train_acc.append(_accuracy(Y_train_pred, Y_train))\n    train_loss.append(_cross_entropy_loss(y_train_pred, Y_train) \/ train_size)#\/ train_size\u6d88\u9664\u8bad\u7ec3\u96c6\u4e0e\u53d1\u5c55\u96c6\u5927\u5c0f\u4e0d\u540c\u5e26\u6765\u7684\u5f71\u54cd\n\n    y_dev_pred = _f(X_dev, w, b)\n    Y_dev_pred = np.round(y_dev_pred)\n    dev_acc.append(_accuracy(Y_dev_pred, Y_dev))\n    dev_loss.append(_cross_entropy_loss(y_dev_pred, Y_dev) \/ dev_size)\n\nprint('Training loss: {}'.format(train_loss[-1]))#[-1]\u8868\u793a\u6570\u7ec4\u4e2d\u6700\u540e\u4e00\u4f4d\nprint('Development loss: {}'.format(dev_loss[-1]))\nprint('Training accuracy: {}'.format(train_acc[-1]))\nprint('Development accuracy: {}'.format(dev_acc[-1]))","b6f8d2b8":"# np.shape(w),w","97681424":"import matplotlib.pyplot as plt\n\n# Loss curve\nplt.plot(train_loss)# x\u53ef\u7701\u7565,\u9ed8\u8ba4[0,1..,N-1]\u9012\u589e\nplt.plot(dev_loss)\nplt.title('Loss')\nplt.legend(['train', 'dev'])#\u9ed8\u8ba4\u53c2\u6570: \u56fe\u4f8b\u7684\u540d\u79f0\nplt.savefig('\/kaggle\/working\/loss.png')\nplt.show()\n\n# Accuracy curve\nplt.plot(train_acc)\nplt.plot(dev_acc)\nplt.title('Accuracy')\nplt.legend(['train', 'dev'])\nplt.savefig('acc.png')\nplt.show()","41b5147d":"# \u9884\u6d4b\u6d4b\u8bd5\u96c6\u7684\u6807\u8bb0\npredictions = _predict(X_test, w, b)\nwith open(output_fpath.format('logistic'), 'w') as f:\n    f.write('id,label\\n')\n    for i, label in  enumerate(predictions):#enumerate\u5217\u51fa\u6570\u636e\u548c\u751f\u6210\u6570\u636e\u4e0b\u6807\n        f.write('{},{}\\n'.format(i, label))\n\n# \u7b5b\u9009\u6700\u5927(\u663e\u8457)\u7684\u51e0\u4e2aweight, \u5373\u5f97\u5230\u6700\u6709\u7528\u7684\u7279\u5f81\nind = np.argsort(np.abs(w))[::-1]#argsort\u4ece\u5c0f\u5230\u5927\u6392\u5e8f, ::-1\u4ece\u540e\u5411\u524d\u8bfb\u53d6\nwith open(X_test_fpath) as f:\n    content = f.readline().strip('\\n').split(',')\nfeatures = np.array(content)\nfor i in ind[0:10]:\n    print(features[i], w[i])","46c9bb72":"# Parse csv files to numpy array\nwith open(X_train_fpath) as f:\n    next(f)\n    X_train = np.array([line.strip('\\n').split(',')[1:] for line in f], dtype = float)\nwith open(Y_train_fpath) as f:\n    next(f)\n    Y_train = np.array([line.strip('\\n').split(',')[1] for line in f], dtype = float)\nwith open(X_test_fpath) as f:\n    next(f)\n    X_test = np.array([line.strip('\\n').split(',')[1:] for line in f], dtype = float)\n\n# Normalize training and testing data\nX_train, X_mean, X_std = _normalize(X_train, train = True)\nX_test, _, _= _normalize(X_test, train = False, specified_column = None, X_mean = X_mean, X_std = X_std)","42e90e56":"# \u5c06\u4e24\u4e2a\u7c7b\u522b\u7684\u6837\u672c\u5206\u5f00\u5b58\u653e\nX_train_0 = np.array([x for x, y in zip(X_train, Y_train) if y == 0])\nX_train_1 = np.array([x for x, y in zip(X_train, Y_train) if y == 1])","0690eaaf":"# \u8ba1\u7b97\u7c7b\u5185\u5e73\u5747\u503c\nmean_0 = np.mean(X_train_0, axis = 0)#axis = 0\uff1a\u538b\u7f29\u884c\uff0c\u5bf9\u5404\u5217\u6c42\u5747\u503c\uff0c\u8fd4\u56de 1* n \u77e9\u9635\nmean_1 = np.mean(X_train_1, axis = 0)  ","6ece297d":"# \u534f\u65b9\u5dee\u77e9\u9635\u521d\u59cb\u5316\ncov_0 = np.zeros((data_dim, data_dim))\ncov_1 = np.zeros((data_dim, data_dim))\n# \u8ba1\u7b97\u7c7b\u5185\u534f\u65b9\u5dee\nfor x in X_train_0:\n    cov_0 += np.dot(np.transpose([x - mean_0]), [x - mean_0]) \/ X_train_0.shape[0]#shape[0]\uff1a\u8868\u793a\u77e9\u9635\u7684\u884c\u6570\nfor x in X_train_1:\n    cov_1 += np.dot(np.transpose([x - mean_1]), [x - mean_1]) \/ X_train_1.shape[0]\n\n# \u5bf9\u7c7b\u5185\u534f\u65b9\u5dee\u52a0\u6743\u5e73\u5747, \u83b7\u5f97\u5171\u7528\u534f\u65b9\u5dee.\ncov = (cov_0 * X_train_0.shape[0] + cov_1 * X_train_1.shape[0]) \/ (X_train_0.shape[0] + X_train_1.shape[0])","246a3073":"# \u76ee\u6807\u662f\u8ba1\u7b97\u534f\u65b9\u5dee\u77e9\u9635cov\u7684\u9006. cov\u53ef\u80fd\u662f\u5947\u5f02\u7684, np.linalg.inv()\u6c42\u77e9\u9635\u7684\u9006\u53ef\u80fd\u51fa\u73b0\u8f83\u5927\u8bef\u5dee.\u6240\u4ee5\u901a\u8fc7 SVD \u5206\u89e3\nu, s, v = np.linalg.svd(cov, full_matrices=False)\ninv = np.matmul(v.T * 1 \/ s, u.T)","701d1502":"# np.shape(mean_0),np.shape(inv)","97f1f9a4":"# \u76f4\u63a5\u8ba1\u7b97weights\u548cbias\n#\u7531\u4e8ey=0\u7684\u542b\u4e49\u662f\u7c7b2, \u6240\u4ee5\u6b64\u5904mean0-mean1\u5373\u4ee3\u8868\u03bc2-\u03bc1\n# \u76f8\u5f53\u4e8e\u4ee3\u516c\u5f0f\u4ee3\u53cd\u4e86\n# \u672c\u6765\u662f\n# y---\u7c7b\n# 1---1\n# 0---2\n# \u5f53\u6210\u4e86\n# y---\u7c7b\n# 1---2\n# 0---1\nw = np.dot(mean_0 - mean_1, inv)#\u6b64\u5904inv\u00b7\u03bc=\u03bc\u00b7inv\nb =  (-0.5) * np.dot(mean_0, np.dot(inv, mean_0)) + 0.5 * np.dot(mean_1, np.dot(inv, mean_1))\\\n    + np.log(float(X_train_0.shape[0]) \/ X_train_1.shape[0]) #\u77e9\u9635\u4e0e\u5411\u91cf\u8fdb\u884c\u8fd0\u7b97\u65f6\uff0c\u5411\u91cfmean\u5373\u03bc\u4f1a\u8fdb\u884c\u81ea\u52a8\u8f6c\u7f6e\u64cd\u4f5c\n\n# \u8ba1\u7b97\u8bad\u7ec3\u96c6\u7cbe\u5ea6\nY_train_pred = 1 - _predict(X_train, w, b)#\u4e3a\u4ec0\u4e48\u8981'1-'\u7ffb\u8f6c\u7c7b\nprint('Training accuracy: {}'.format(_accuracy(Y_train_pred, Y_train)))","8aac2338":"# # \u9884\u6d4b\u6d4b\u8bd5\u96c6\u6807\u7b7e\n# predictions = 1 - _predict(X_test, w, b)#\u7531\u4e8e\u516c\u5f0f\u4ee3\u53cd\u4e86, \u6240\u4ee5\u9884\u6d4b\u51fa\u7684\u7c7b\u548c\u5bf9\u7387\u56de\u5f52\u76f8\u53cd. \u75281 -\u8c03\u6574\u8fc7\u6765\n# with open(output_fpath.format('generative'), 'w') as f:\n#     f.write('id,label\\n')\n#     for i, label in  enumerate(predictions):\n#         f.write('{},{}\\n'.format(i, label))\n\n# # \u8f93\u51fa\u6700\u663e\u8457\u7279\u5f81\n# ind = np.argsort(np.abs(w))[::-1]\n# with open(X_test_fpath) as f:\n#     content = f.readline().strip('\\n').split(',')\n# features = np.array(content)\n# for i in ind[0:10]:\n#     print(features[i], w[i])","266ee395":"# \u5bf9\u7387\u56de\u5f52\n## \u6570\u636e\u51c6\u5907\n\u4e0b\u8f09\u8cc7\u6599\uff0c\u4e26\u4e14\u5c0d\u6bcf\u500b\u5c6c\u6027\u505a\u6b63\u898f\u5316\uff0c\u8655\u7406\u904e\u5f8c\u518d\u5c07\u5176\u5207\u5206\u70ba\u8a13\u7df4\u96c6\u8207\u767c\u5c55\u96c6(development set)\u3002","0dac48f3":"### \u4f5c\u635f\u5931, \u7cbe\u5ea6\u66f2\u7ebf","118fa390":"## \u9884\u6d4b\u6d4b\u8bd5\u96c6\n\u9810\u6e2c\u6e2c\u8a66\u96c6\u7684\u8cc7\u6599\u6a19\u7c64\u4e26\u4e14\u5b58\u5728 *output_logistic.csv* \u4e2d\u3002","bb1236db":"# \u6982\u7387\u751f\u6210\u6a21\u578b\n\u5be6\u4f5c\u57fa\u65bc generative model \u7684\u4e8c\u5143\u5206\u985e\u5668\uff0c\u7406\u8ad6\u7d30\u7bc0\u8acb\u53c3\u8003\u674e\u5b8f\u6bc5\u5206\u7c7b\u4e00\u8282\u3002\n## \u51c6\u5907\u6570\u636e\n\u8a13\u7df4\u96c6\u8207\u6e2c\u8a66\u96c6\u7684\u8655\u7406\u65b9\u6cd5\u8ddf logistic regression \u4e00\u6a21\u4e00\u6a23\uff0c\u7136\u800c\u56e0\u70ba generative model \u6709\u53ef\u89e3\u6790\u7684\u6700\u4f73\u89e3\uff0c\u56e0\u6b64\u4e0d\u5fc5\u4f7f\u7528\u5230 development set\u3002(\u5bf9\u7387\u56de\u5f52\u7684\u68af\u5ea6\u4e0b\u964d\u53c2\u6570\u627e\u4e0d\u51c6\u70b9, \u53d1\u5c55\u96c6\u7528\u6765\u6a21\u62df\u6cdb\u5316\u540e\u8bef\u5dee)","ee32ab46":"### \u635f\u5931\u51fd\u6570\u6b63\u5219\u5316\n#### \u539f\u635f\u5931\u51fd\u6570\n\u5bf9\u4e8e\u8bad\u7ec3\u96c6(\ud835\udc65^\ud835\udc5b,\ud835\udc66 hat^\ud835\udc5b )\n\ud835\udc66 hat^\ud835\udc5b: 1-class 1, 0-class 2\n$$ L\\left( f \\right) = {\\sum_{n}{l\\left( {f\\left( x^{n} \\right),{\\hat{y}}^{n}} \\right)}} \\\\=\\sum_{n} - \\left\\lbrack {{\\hat{y}}^{n}lnf\\left( x^{n} \\right) + \\left( {1 - {\\hat{y}}^{n}} \\right) ln\\left( {1 - f\\left( x^{n} \\right)} \\right)} \\right\\rbrack $$\n\u635f\u5931\u51fd\u6570\u4e3a\u6240\u6709\u6837\u672c\u70b9\u7684output(\u5373f)\u548c\u5b9e\u9645target(y hat)\u5728Bernoulli distribution(\u4e24\u70b9\u5206\u5e03)\u4e0b\u7684\u4ea4\u53c9\u71b5\u603b\u548c\n#### \u7ebf\u6027\u56de\u5f52\u635f\u5931\u51fd\u6570\u6b63\u5219\u5316\n\u539f\u6765\u7684\u635f\u5931\u51fd\u5f0f\u53ea\u8003\u8651\u4e86\u9884\u6d4b\u7684\u8bef\u5dee, \u52a0\u4e00\u9879\u8003\u8651w\u5927\u5c0f(\ud835\udf06\u2211(\ud835\udc64_\ud835\udc56 )^2),\u4f46\u4e0d\u7528\u8003\u8651bias, bias\u4e0e\u5e73\u6ed1\u7a0b\u5ea6\u65e0\u5173. \n$$ L = \\sum_{n} ( \\hat{y}^n \u2013 ( b + w*x_{cp}^n ) )^2 + \\lambda \\sum ( w_i )^2 $$\n\u671f\u5f85\u53c2\u6570\u7684\u503c\u66f4\u5c0f\u7684function, \u610f\u5473\u7740\u66f4\u5e73\u6ed1, \u8f93\u51fa\u53d7\u8f93\u5165\u5f71\u54cd\u66f4\u5c0f, \u5f53\u6d4b\u8bd5\u96c6\u7684\u8f93\u5165\u542b\u566a\u58f0\u65f6, \u5e73\u6ed1\u7684\u51fd\u5f0f\u53d7\u66f4\u5c0f\u7684\u5f71\u54cd. \u03bb\u8d8a\u9ad8\u8d8a\u5e73\u6ed1, \u5728\u8bad\u7ec3\u96c6\u4e0a\u5f97\u5230\u7684\u8bef\u5dee\u8d8a\u5927,\u4f46\u662f\u5728\u6d4b\u8bd5\u96c6\u4e0a\u5148\u51cf\u5c0f\u540e\u589e\u52a0. \n#### \u5bf9\u7387\u56de\u5f52\u635f\u5931\u51fd\u6570\u6b63\u5219\u5316\n$$  L\\left( f \\right) = {\\sum_{n}{l\\left( {f\\left( x^{n} \\right),{\\hat{y}}^{n}} \\right)}}+ \\lambda \\sum ( w_i )^2 \\\\=\\sum_{n} - \\left\\lbrack {{\\hat{y}}^{n}lnf\\left( x^{n} \\right) + \\left( {1 - {\\hat{y}}^{n}} \\right) ln\\left( {1 - f\\left( x^{n} \\right)} \\right)} \\right\\rbrack+ \\lambda \\sum ( w_i )^2 $$\n#### \u6b63\u5219\u5316\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u516c\u5f0f\n$$ \\begin{align} w_i & = w_i - \\eta \\frac\\partial{\\partial{w_i}}I(w) \\\\\n& = w_i - \\eta \\left[ \\frac{1}{m} \\sum_{n=1}^m \\left( f_w(x^{(n)}) - y^{(n)} \\right) x_i^{(n)} + \\frac{\\lambda}{m} w_i \\right] \\\\ & = w_i (1 - \\eta \\frac{\\lambda}{m}) - \\eta \\frac{1}{m} \\sum_{n=1}^m  \\left(\\left(f(x^{(n)}) - y^{(n)}\\right)  x_i^{(n)}\\right) \\end{align} $$","d427e29b":"### \u9ad8\u65af\u5e73\u5747\u503c\u516c\u5f0f\n\u534f\u65b9\u5dee\u516c\u5f0f\u8be6\u7ec6\u53ef\u53c2\u89c1\u7b14\u8bb04:\u5206\u7c7b. \u6781\u5927\u4f3c\u7136\u51fd\u6570L\u5206\u522b\u5bf9\u03bc, \u03a3\u6c42\u504f\u5bfc\uff0c\u89e3\u51fa\u5fae\u5206\u662f0\u7684\u70b9\uff0c\u5373\u4f7fL\u6700\u5927\u7684\u90a3\u7ec4\u53c2\u6570.\n\u5f97\u4e24\u8005\u89e3\u6790\u89e3\u7684\u516c\u5f0f, \u03bc\u662f\u6570\u5b66\u671f\u671b:\n$$ \\mu^{*} = E(X)=\\frac{1}{n}{\\sum_{n = 1}^{n}x^{n}} $$","433b4253":"## \u8ba1\u7b97weights, bias\n\u6b0a\u91cd\u77e9\u9663\u8207\u504f\u5dee\u5411\u91cf\u53ef\u4ee5\u76f4\u63a5\u88ab\u8a08\u7b97\u51fa\u4f86\uff0c\u7b97\u6cd5\u53ef\u4ee5\u53c3\u8003\u674e\u5b8f\u6bc5\u5206\u7c7b\u4e00\u8282.\n\n\u7531\u4e8e\u534f\u65b9\u5dee\u77e9\u9635\u53ef\u80fd\u662f\u5947\u5f02\u7684, np.linalg.inv()\u6c42\u77e9\u9635\u7684\u9006\u53ef\u80fd\u51fa\u73b0\u8f83\u5927\u6570\u503c\u8bef\u5dee.\n\n\u901a\u8fc7 SVD \u5206\u89e3, \u53ef\u7528\u6709\u6548\u7cbe\u786e\u5730\u5f97\u5230\u77e9\u9635\u7684\u9006.\n\n\u5947\u5f02\u503c\u5206\u89e3\u516c\u5f0f:\n$$ \\mathbf{S}_w=\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^{\\mathrm T}\\\\ \\mathbf{S}_w^{-1}=\\mathbf{V}\\mathbf{\\Sigma}^{-1}\\mathbf{U}^{\\mathrm T} $$\n\u4ee3\u5165np.linalg.svd\u8fd4\u56de\u503c:\n$$ \\mathbf{cov}=\\mathbf{U}\\mathbf{S}\\mathbf{V}\\\\ \\mathbf{inv}=\\mathbf{cov}^{-1}=\\mathbf{V}^{\\mathrm T}\\mathbf{S}^{-1}\\mathbf{U}^{\\mathrm T} $$","18165b0c":"### sigmoid\u51fd\u6570\n\u7528\u7684\u6982\u7387\u5206\u5e03\u662f\u9ad8\u65af\u5206\u5e03, \u5219P\u53ef\u4ee5\u4ee3\u5165\u03c3, \u5373sigmoid\u51fd\u6570:\n$$ P\\left( C_{1} \\middle| x \\right) = \\sigma\\left( z \\right) \\\\  \\sigma\\left( z \\right)= \\frac{1}{1 + exp\\left( {- z} \\right)} $$","baebce3f":"### \u5bf9\u7387\u56de\u5f52\n\u4f7f\u7528sigmoid\u51fd\u6570\u4f5c\u4e3aP\u65f6, \u8f93\u5165\u7684z\u4e3a:\n$$ z=  {w \\cdot x + b}=\\sum_i w_ix_i+b $$","a568b519":"## \u5e73\u5747\u503c\u548c\u534f\u65b9\u5dee\n\u5728 generative model \u4e2d\uff0c\u6211\u5011\u9700\u8981\u5206\u5225\u8a08\u7b97\u5169\u500b\u985e\u5225\u5167\u7684\u8cc7\u6599\u5e73\u5747\u8207\u5171\u8b8a\u7570(in-class mean and covariance)\u3002","56c08535":"### \u68af\u5ea6\u4e0b\u964d\u7684\u66f4\u65b0\u516c\u5f0f\n$$ w_{i}\\leftarrow w_{i} - \\eta{\\sum_{n}{- \\left( {{\\hat{y}}^{n} - f_{w,b}\\left( x^{n} \\right)} \\right)x_{i}^{n}}} $$\n\u8be5\u516c\u5f0f\u4e2d\u95f4\u7684\u56e0\u5b50, \u4e3a\u4f3c\u7136\u51fd\u6570lnL\u5bf9wi\u7684\u504f\u5fae\u5206:\n$$ \\frac{\\partial lnL\\left( {w,b} \\right)}{\\partial w_{i}} =\\sum_{n}{- \\left( {{\\hat{y}}^{n} - f_{w,b}\\left( x^{n} \\right)} \\right)x_{i}^{n}} $$","b2218f2c":"## \u9884\u6d4b\u6d4b\u8bd5\u96c6\u6807\u7b7e\n\u9810\u6e2c\u6e2c\u8a66\u96c6\u7684\u8cc7\u6599\u6a19\u7c64\u4e26\u4e14\u5b58\u5728 *output_generative.csv* \u4e2d\u3002","05291c3d":"## \u8bad\u7ec3\n\n\u6211\u5011\u4f7f\u7528\u5c0f\u6279\u6b21(batch)\u68af\u5ea6\u4e0b\u964d\u6cd5\u4f86\u8a13\u7df4\u3002\u8a13\u7df4\u8cc7\u6599\u88ab\u5206\u70ba\u8a31\u591a\u5c0f\u6279\u6b21\uff0c\u91dd\u5c0d\u6bcf\u4e00\u500b\u5c0f\u6279\u6b21\uff0c\u6211\u5011\u5206\u5225\u8a08\u7b97\u5176\u68af\u5ea6\u4ee5\u53ca\u640d\u5931\uff0c\u4e26\u6839\u64da\u8a72\u6279\u6b21\u4f86\u66f4\u65b0\u6a21\u578b\u7684\u53c3\u6578\u3002\u7576\u4e00\u6b21\u8ff4\u5708(\u5faa\u73af, \u8fed\u4ee3)\u5b8c\u6210\uff0c\u4e5f\u5c31\u662f\u6574\u500b\u8a13\u7df4\u96c6\u7684\u6240\u6709\u5c0f\u6279\u6b21\u90fd\u88ab\u4f7f\u7528\u904e\u4e00\u6b21\u4ee5\u5f8c\uff0c\u6211\u5011\u5c07\u6240\u6709\u8a13\u7df4\u8cc7\u6599\u6253\u6563\u4e26\u4e14\u91cd\u65b0\u5206\u6210\u65b0\u7684\u5c0f\u6279\u6b21\uff0c\u9032\u884c\u4e0b\u4e00\u500b\u8ff4\u5708\uff0c\u76f4\u5230\u4e8b\u5148\u8a2d\u5b9a\u7684\u8ff4\u5708\u6578\u91cf\u9054\u6210\u70ba\u6b62\u3002","0ccb6987":"## \u68af\u5ea6\u4e0b\u964d\u51fd\u6570\u548c\u635f\u5931\u51fd\u6570\n\n\u53c3\u8003\u674e\u5b8f\u6bc5\u5bf9\u7387\u56de\u5f52PPT, P12\u68af\u5ea6\u53ca\u640d\u5931\u51fd\u6578\u8a08\u7b97\u516c\u5f0f\u3002\n### \u8ba1\u7b97\u4ea4\u53c9\u71b5\u4f5c\u4e3a\u635f\u5931\u51fd\u6570\ny hat=1, \u5c5e\u4e8e\u7c7b\u522b1, y hat=0, \u5c5e\u4e8e\u7c7b\u522b2,\n\n\u5bf9\u4ee5\u4e0b\u4e24\u4e2a\u5206\u5e03(\u5047\u8bbe\u5747\u4e3aBernouli\u4e24\u70b9\u5206\u5e03)\u505a\u4ea4\u53c9\u71b5. Distribution p:\n$$ p\\left( {x = 1} \\right) = {\\hat{y}}^{n}\\\\ p\\left( {x = 0} \\right) = {1 - \\hat{y}}^{n} $$\n\u4ea4\u53c9\u71b5\u53ef\u8861\u91cf\u4e24\u4e2a\u5206\u5e03\u63a5\u8fd1\u7a0b\u5ea6:\n$$ H\\left( {p,q} \\right) = - {\\sum_{x}{p\\left( x \\right)ln\\left( {q\\left( x \\right)} \\right)}} $$\n\u4ee3\u5165\u4f3c\u7136\u51fd\u6570\u53ef\u8868\u793a\u4e3a:\n$$ - lnL\\left( {w,b} \\right) = {\\sum_{n}{- \\left\\lbrack {{\\hat{y}}^{n}lnf_{w,b}\\left( x^{n} \\right) + \\left( {1 - {\\hat{y}}^{n}} \\right) ln\\left( {1 - f_{w,b}\\left( x^{n} \\right)} \\right)} \\right\\rbrack}} $$","0b6d2d2d":"### \u9ad8\u65af\u534f\u65b9\u5dee\u516c\u5f0f\n$$ \\Sigma^{*} = cov(X,X)=E[(X-\\mu)(X-\\mu)^T]=\\frac{1}{n}{\\sum_{n = 1}^{n}\\left( {x^{n} - \\mu^{*}} \\right)}\\left( {x^{n} - \\mu^{*}} \\right)^{T} $$\n","93616cab":"### \u8ba1\u7b97weights\u548cbias\u89e3\u6790\u89e3\n\u8be6\u89c1\u5206\u7c7b\u4e00\u8282\u7684\u540e\u9a8c\u6982\u7387\u5206\u6790.\n$$ P\\left( C_{1} \\middle| x \\right) = \\sigma\\left( z \\right) = \\sigma\\left( {w \\cdot x + b} \\right) \\\\ If~P\\left( C_{1} \\middle| x \\right) > 0.5,~x~belongs~to~class~1 $$\n\u5176\u4e2d\u4ee3\u5165z\u6574\u7406\u5f97:\n$$ w^T=\\left( {\\mu^{1} - \\mu^{2}} \\right)^{T}\\Sigma^{- 1} \\\\ b=- \\frac{1}{2}\\left( \\mu^{1} \\right)^{T}\\Sigma^{- 1}\\mu^{1} + \\frac{1}{2}\\left( \\mu^{2} \\right)^{T}\\Sigma^{- 1}\\mu^{2} + ln\\frac{N_{1}}{N_{2}} $$\n","3ddd569e":"### \u5de5\u5177\u51fd\u6570\n\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u91cd\u590d\u4f7f\u7528"}}