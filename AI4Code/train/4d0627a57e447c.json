{"cell_type":{"457bd4d3":"code","42a46527":"code","df9a77d7":"code","f98402bf":"code","04cd0963":"code","fba5dde2":"code","3e568f80":"code","c843e912":"code","67d41249":"code","c28fcefd":"code","2aaf0637":"code","b918ce63":"code","81422728":"code","1e011e0e":"code","f23aefbd":"code","252423ad":"code","5c8cdb0b":"markdown","dbdaa491":"markdown","538edaf1":"markdown","4903933b":"markdown","94ff7404":"markdown","53eae207":"markdown","1dd57333":"markdown","bf6fde7f":"markdown","a3103c17":"markdown","bcbfa5c0":"markdown","31eb046c":"markdown","e4248d81":"markdown","4c9bb353":"markdown","45f5639b":"markdown","8d5bae8f":"markdown","de919c2c":"markdown","572bade2":"markdown"},"source":{"457bd4d3":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","42a46527":"train = pd.read_csv(\"train.csv\")\ntestset = pd.read_csv(\"test.csv\")\ntestset.info()","df9a77d7":"print((train.isna().sum()\/train.shape[0])*100)","f98402bf":"#Drop cabin given it is missing too much data\ntrain_clean = train.drop(['Cabin'],axis=1)\ntest_clean = test.drop(['Cabin'],axis=1)","04cd0963":"train_clean['Age'].fillna(train_clean.groupby('Sex')['Age'].transform(\"mean\"), inplace=True)\ntest_clean['Age'].fillna(test_clean.groupby('Sex')['Age'].transform(\"mean\"), inplace=True)\n\ntrain_clean['Embarked'] = train_clean['Embarked'].astype('category').cat.codes\ntest_clean['Embarked'] = test_clean['Embarked'].astype('category').cat.codes\n\ntrain_clean['Embarked'].fillna(train_clean.groupby('Pclass')['Embarked'].agg(pd.Series.mode),inplace=True)\ntest_clean['Embarked'].fillna(test_clean.groupby('Pclass')['Embarked'].agg(pd.Series.mode),inplace=True)","fba5dde2":"# Convert to categorical values Title \ndataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in train_clean[\"Name\"]]\ntrain_clean[\"Title\"] = pd.Series(dataset_title)\ntrain_clean[\"Title\"].head()\n\ntrain_clean[\"Title\"] = train_clean[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntrain_clean[\"Title\"] = train_clean[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ntrain_clean[\"Title\"] = train_clean[\"Title\"].astype(int)\n\n\n#Repeat for test set\n#Convert to categorical values Title \ndataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in test_clean[\"Name\"]]\ntest_clean[\"Title\"] = pd.Series(dataset_title)\ntest_clean[\"Title\"].head()\n\ntest_clean[\"Title\"] = test_clean[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntest_clean[\"Title\"] = test_clean[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ntest_clean[\"Title\"] = test_clean[\"Title\"].astype(int)","3e568f80":"train_clean[['C','Q','S']] = pd.get_dummies(train[\"Embarked\"])\ntest_clean[['C','Q','S']] = pd.get_dummies(test[\"Embarked\"])\n\ntrain_clean[['M','F']] = pd.get_dummies(train[\"Sex\"])\ntest_clean[['M','F']] = pd.get_dummies(test[\"Sex\"])\n\n#Combine Sibling Spouse and Parent Child features into one Family Feature\ntrain_clean['FamSize'] = train_clean['SibSp']+train_clean['Parch']+ 1\ntest_clean['FamSize'] = test_clean['SibSp']+test_clean['Parch']+ 1\n\n#Drop Original Feature columns now that we've encoded them\ntrain_clean = train_clean.drop(['Embarked','Sex','Ticket','SibSp','Parch','PassengerId','Name'],axis=1)\ntest_clean = test_clean.drop(['Embarked','Sex','Ticket','SibSp','Parch','PassengerId','Name'],axis=1)\n\ntrain_clean.describe()","c843e912":"from sklearn.preprocessing import RobustScaler\ntrain_clean[[\"Age\",\"Fare\"]] = RobustScaler().fit_transform(train_clean[[\"Age\",\"Fare\"]])\ntest_clean[[\"Age\",\"Fare\"]] = RobustScaler().fit_transform(test_clean[[\"Age\",\"Fare\"]])","67d41249":"g= sns.pairplot(train_clean, hue= \"Survived\")","c28fcefd":"fig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(train_clean.corr(), vmax=1.0, center=0, fmt='.2f', square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\":.70})","2aaf0637":"y_train = train_clean[\"Survived\"].values\nX_train = train_clean.drop(columns=[\"Survived\"])","b918ce63":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import model_selection\n\nmodels = []\nmodels.append(('RF',RandomForestClassifier(n_estimators=100, max_depth=10,random_state=0)))\nmodels.append(('NB',GaussianNB()))\nmodels.append((\"SVM\",SVC(gamma='auto')))\nmodels.append((\"LR\",LogisticRegression(solver='lbfgs')))\n\nresults = []\nnames = []\n\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits =10, random_state = 7)\n    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring =\"accuracy\")\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","81422728":"from sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\n# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, y_train)","1e011e0e":"from sklearn.metrics import accuracy_score\n\nprint(rf_random.best_params_)\nbest_model = rf_random.best_estimator_\npreds = best_model.predict(X_train)\naccuracy_score(y_train, preds)","f23aefbd":"from sklearn.model_selection import GridSearchCV\n\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [int(x) for x in np.linspace(10, 30, num = 10)],\n    'max_features': [2,3],\n    'min_samples_leaf': [1],\n    'min_samples_split': [10],\n    'n_estimators': [int(x) for x in np.linspace(1400, 1800, num = 30)]\n}\n\n# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)\ngrid_search.fit(X_train, y_train)\n","252423ad":"print(grid_search.best_params_)\nbest_model = grid_search.best_estimator_\npreds = best_model.predict(X_train)\naccuracy_score(y_train, preds)","5c8cdb0b":"Initial accuracies aren't bad, random forests and logistic regression and SVMs seem most promising. Let's go with Random Forest and try a random search for the best hyperparamter combination. Then after the random search, we'll use a more refined grid-search to maximize accuracy gains","dbdaa491":"<H3> 1.5) Multiple Imputation by Chained Equations","538edaf1":"<H3> 4) Data Visualization","4903933b":"<H1> B) Data Interpolation and Cleaning","94ff7404":"Try later.","53eae207":"<H1> A) Import Train Data and Examine it","1dd57333":"<H3> 1) Mean in Combination GroupBy Interpolation","bf6fde7f":"To interpolate the missing information, I'm thinking of using two techniques: Mean, Median in combination with Groupby or Multiple Imputation by Chained Equations. There's no right answer here but I assume interpolating the age correlating might influence the classifier's final accuracy.","a3103c17":"<H3> 3) Feature Scaling","bcbfa5c0":"<H3> Try Random Hyperparam Tuning","31eb046c":"<H3> Try a Grid Search using the Random results\n","e4248d81":"Here, I interpolate the missing values by grouping them to the gender. Not sure if this is a good idea yet. Will come back later if need be.","4c9bb353":"<H1> Titanic Dataset Solution<\/H1>\n\nThis is my attempt at solving the Titanic Dataset. Here, I employ techniques such as data interpolation, feature conversion, regular expression on string features, random search and grid search for hyperparameter optimization. ","45f5639b":"<H1> C) Try Quick and Dirty Solutions","8d5bae8f":"<H3> 2) One Hot Encoding for categorical features","de919c2c":"From an initial glance, we can see that the \"Cabin\" feature lacks too many data points for reasonable interpolation. Therefore, we will likely drop it. Other features like *Embarked* and *Age* will probably require some interpolation. Let's first examine the percentage of data missing per feature.","572bade2":"Handling the *Name* feature by extracting important info such as the title"}}