{"cell_type":{"f2db3063":"code","654af62e":"code","29571fce":"code","088dea01":"code","5135ceba":"code","d9cc2bb3":"code","8b2cb6d2":"code","46e59aac":"code","50fb1cc6":"code","5f08583a":"code","b35a00c3":"code","90c1acb0":"code","03acadc8":"code","4751be98":"code","db6fdc8d":"code","3bf254d7":"code","5167d896":"code","7f743441":"code","78df7ce0":"code","810d150d":"code","73a9becb":"code","919913f5":"code","a2e36ef4":"code","3a1facf1":"code","2f56d5d5":"code","6543903f":"code","e1c11bb5":"code","cfb69cc5":"code","c50d0511":"code","3a3fc4b2":"code","cdd2cb18":"code","9599d518":"code","523f84f0":"code","b78bdfb1":"code","7aef5dfc":"code","396a9bc9":"code","6cab3fef":"code","600ac00a":"code","d7b7ae06":"code","269b67ac":"code","040cda59":"code","ad83b1f2":"code","2018e322":"code","8716f76c":"code","ca039629":"code","8f7b510a":"code","6d7b8734":"markdown","44d0adcc":"markdown","cc52b9b4":"markdown","82962b67":"markdown","d476d62f":"markdown","751bdac7":"markdown","0b27ebca":"markdown","6cfee82d":"markdown","eeb693d7":"markdown","1d2fde5b":"markdown","d9de7ac7":"markdown","a5e22067":"markdown","62ba9b35":"markdown","919b746b":"markdown","a3828b67":"markdown","d9eb8d11":"markdown","c492d741":"markdown","c23259e2":"markdown"},"source":{"f2db3063":"## import libraries for EDA\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","654af62e":"## load data, for EDA we'll only use train data\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\npd.set_option('display.max_columns', 100) # Show all columns when looking at dataframe\npd.options.display.float_format='{:,.3f}'.format","29571fce":"test.shape","088dea01":"train.info()","5135ceba":"train.describe()","d9cc2bb3":"## divide data in categorical and continuos\n## I put sale price in both to see how our target relates to categorical and continuos variables \ndf_num = train[['LotFrontage','LotArea','YearBuilt','YearRemodAdd','MasVnrArea','BsmtFinSF1',\n                'BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea',\n                'TotRmsAbvGrd','Fireplaces','GarageYrBlt','GarageCars','GarageArea','WoodDeckSF',\n                'OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal',\n                'YrSold','SalePrice']].copy(deep=True)\ndf_cat = train[['MSSubClass','MSZoning','Street','Alley','LotShape','LandContour','Utilities',\n                'LotConfig','LandSlope','Neighborhood','Condition1','Condition2','BldgType','HouseStyle',\n                'OverallQual','OverallCond','RoofStyle','RoofMatl','Exterior1st','Exterior2nd',\n                'MasVnrType','ExterQual','ExterCond','Foundation','BsmtQual','BsmtCond','BsmtExposure',\n                'BsmtFinType1','BsmtFinType2','Heating','HeatingQC','CentralAir','Electrical',\n                'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\n                'KitchenQual','Functional','FireplaceQu','GarageType','GarageFinish','GarageQual',\n                'GarageCond','PavedDrive','PoolQC','Fence','MiscFeature','MoSold','SaleType',\n                'SaleCondition','SalePrice']].copy(deep=True)","8b2cb6d2":"corr = df_num.corr()\ncorr","46e59aac":"sns.heatmap(corr, cmap='YlGnBu');","50fb1cc6":"## set the style for plots\nstyle = {'axes.facecolor': 'white',\n         'ytick.left': False,\n         'axes.spines.left': True,\n         'axes.spines.right': False,\n         'axes.spines.top': False}\npalette = 'Set2'\nsns.set(style=style, palette=palette)","5f08583a":"vars_ = ['GrLivArea','GarageCars','GarageArea','TotalBsmtSF','1stFlrSF','TotRmsAbvGrd','YearBuilt','YearRemodAdd']\nfor i in vars_:\n    ax = sns.distplot(df_num[i])\n    ax.set(title=i, xlabel='', yticklabels=[])\n    plt.show()\n    ax = sns.regplot(x=df_num[i], y=df_num.SalePrice, fit_reg=True, scatter_kws={\"alpha\": 0.3})\n    ax.set(yticklabels=[], xticklabels=[], xticks=[])\n    plt.show()","b35a00c3":"for i in vars_:\n    sns.jointplot(x=df_num[i], y=df_num.SalePrice, kind='kde')\n    plt.show()","90c1acb0":"## Load package for linear regression \nimport statsmodels.api as sm","03acadc8":"df_num['stFrSF'] = df_num['1stFlrSF']\ndf_num['ndFrSF'] = df_num['2ndFlrSF']\ndf_num['SsnPorch'] = df_num['3SsnPorch']","4751be98":"## define num vars for the loop\nnum_vars = ['LotFrontage', 'LotArea', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea',\n           'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'LowQualFinSF', \n           'GrLivArea', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars',\n           'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', 'ScreenPorch',\n           'PoolArea', 'MiscVal', 'YrSold', 'stFrSF', 'ndFrSF', 'SsnPorch']","db6fdc8d":"## fit each variable and save the R-squared in a list\nnum_rsquared = []\nfor var in num_vars:\n    model = sm.OLS.from_formula(f'SalePrice ~ {var}', data=df_num)\n    result = model.fit()\n    num_rsquared.append((var,result.rsquared))\n    \n## make a series with rsquared data\nrsquared = [i[1] for i in num_rsquared]\nvars_ = [i[0] for i in num_rsquared]\ndf_num_rsquared = pd.Series(rsquared, index=vars_)\n\ndf_num_rsquared.sort_values(ascending=False)","3bf254d7":"## first define our categorical variables\ncat_vars = ['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour',\n       'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1',\n       'Condition2', 'BldgType', 'HouseStyle', 'OverallQual', 'OverallCond',\n       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n       'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond',\n       'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC',\n       'CentralAir', 'Electrical', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'Functional',\n       'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\n       'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'MoSold', 'SaleType',\n       'SaleCondition']","5167d896":"# load package for ANOVA\nfrom statsmodels.formula.api import ols","7f743441":"## ANOVA this prints the number of variables with significant p-values\nanova_pvalues = []\nfor var in cat_vars:\n    model = ols(f'SalePrice ~ C({var})', data=df_cat).fit()\n    anova_table = sm.stats.anova_lm(model, typ=2)\n    pvalue = anova_table.loc[f'C({var})','PR(>F)']\n    if pvalue < 0.05:\n        anova_pvalues.append((var,pvalue))\n    else:\n        continue\n\nprint(len(anova_pvalues))","78df7ce0":"## fit each variable and save the R-squared in a list\ncat_rsquared = []\nfor i in anova_pvalues:\n    model = sm.OLS.from_formula(f'SalePrice ~ {i[0]}', data=df_cat)\n    result = model.fit()\n    cat_rsquared.append((i[0],result.rsquared))\n    \n## make a series with rsquared data\nrsquared = [i[1] for i in cat_rsquared]\nvars_ = [i[0] for i in cat_rsquared]\ndf_cat_rsquared = pd.Series(rsquared, index=vars_)\n\ndf_cat_rsquared.sort_values(ascending=False)","810d150d":"## the selected features and target\nfeatures = ['GrLivArea','GarageCars','GarageArea','TotalBsmtSF','1stFlrSF','TotRmsAbvGrd','YearBuilt',\n            'YearRemodAdd','GarageYrBlt','MasVnrArea','OverallQual','Neighborhood','ExterQual','KitchenQual',\n            'BsmtQual','FullBath','Alley','GarageFinish','Foundation']\nX_test = test[features]\nfeatures.append('SalePrice')\nX_train = train[features]","73a9becb":"## Percentage of NAN values per column in train\nNAN_train = [(column, X_train[column].isna().mean()*100) for column in list(X_train.columns)]\nNAN_train = pd.DataFrame(NAN_train, columns=['column_name','percentage'])\nNAN_train","919913f5":"## Percentage of NAN values per column in test\nNAN_test = [(column, X_test[column].isna().mean()*100) for column in list(X_test.columns)]\nNAN_test = pd.DataFrame(NAN_test, columns=['column_name','percentage'])\nNAN_test","a2e36ef4":"X_train = X_train.drop(['Alley'], axis=1)\nX_test = X_test.drop(['Alley'], axis=1)\n\nX_train['GarageYrBlt'].fillna(X_train.GarageYrBlt.median(), inplace=True)\nX_train['MasVnrArea'].fillna(X_train.MasVnrArea.median(), inplace=True)\nX_train.dropna(inplace=True)\n\nX_test['TotalBsmtSF'].fillna(X_test.TotalBsmtSF.median(), inplace=True)\nX_test['GarageYrBlt'].fillna(X_test.GarageYrBlt.median(), inplace=True)\nX_test['MasVnrArea'].fillna(X_test.MasVnrArea.median(), inplace=True)\nX_test['GarageArea'].fillna(X_test.GarageArea.median(), inplace=True)\nX_test['GarageCars'].fillna(X_test.GarageCars.median(), inplace=True)","3a1facf1":"## separate SalePrice into a new target series and drop it from the train dataset\ny = X_train['SalePrice']\nX_train = X_train.drop(['SalePrice'], axis=1)","2f56d5d5":"## get dummies\nX_train = pd.get_dummies(X_train)\nX_test = pd.get_dummies(X_test)","6543903f":"# Scale numeric data\nfrom sklearn.preprocessing import StandardScaler\n\nnum_data = ['GrLivArea', 'GarageCars', 'GarageArea', 'TotalBsmtSF', '1stFlrSF', 'TotRmsAbvGrd',\n            'YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'MasVnrArea']\nX_train[num_data] = StandardScaler().fit_transform(X_train[num_data])\nX_test[num_data] = StandardScaler().fit_transform(X_test[num_data])","e1c11bb5":"print(X_test.shape)\nprint(X_train.shape)","cfb69cc5":"X_test.drop(X_test.columns.difference(list(X_train.columns)), axis=1, inplace=True)","c50d0511":"print(X_test.shape)\nprint(X_train.shape)","3a3fc4b2":"## import cross_val for the scores\nfrom sklearn.model_selection import cross_val_score","cdd2cb18":"## Linear Regression\nfrom sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\ncv = cross_val_score(linreg, X_train, y, cv=5)\nprint(cv)\nprint(cv.mean())","9599d518":"## k neighbors regressor selecting number of neighbors\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_predict\n\nerror = []\nfor k in range(1,51):\n    knn = KNeighborsRegressor(n_neighbors=k)\n    y_pred = cross_val_predict(knn, X_train, y, cv=5)\n    error.append(mean_squared_error(y,y_pred))\n    \nplt.plot(range(1,51), error);","523f84f0":"## look only between 5 and 20\nerror = []\nfor k in range(5,21):\n    knn = KNeighborsRegressor(n_neighbors=k)\n    y_pred = cross_val_predict(knn, X_train, y, cv=5)\n    error.append(mean_squared_error(y,y_pred))\n    \nplt.plot(range(5,21), error);","b78bdfb1":"## k neighbors regressor\n## smallest error  seems to be at 11 neighbors\nknnreg = KNeighborsRegressor(n_neighbors = 11)\ncv = cross_val_score(knnreg, X_train, y, cv=5)\nprint(cv)\nprint(cv.mean())","7aef5dfc":"## Ridge regression\n## looking at different alpha values\nfrom sklearn.linear_model import Ridge\n\nerror = []\nfor alpha in range(1,51):\n    linridge = Ridge(alpha=float(alpha))\n    y_pred = cross_val_predict(linridge, X_train, y, cv=5)\n    error.append(mean_squared_error(y,y_pred))\n    \nplt.plot(range(1,51), error);","396a9bc9":"## Ridge regression\n## smallest error at alpha=1\nlinridge = Ridge(alpha=1.0)\ncv = cross_val_score(linridge, X_train, y, cv=5)\nprint(cv)\nprint(cv.mean())","6cab3fef":"## Lasso regression\n## looking at different alpha values\nfrom sklearn.linear_model import Lasso\n\nerror = []\nfor alpha in range(1,51):\n    linlasso = Lasso(alpha=float(alpha), max_iter = 10000)\n    y_pred = cross_val_predict(linlasso, X_train, y, cv=5)\n    error.append(mean_squared_error(y,y_pred))\n    \nplt.plot(range(1,51), error);","600ac00a":"## Lasso regression\n## looking at different alpha values\n## looking only between 3 and 12\nfrom sklearn.linear_model import Lasso\n\nerror = []\nfor alpha in range(3,13):\n    linlasso = Lasso(alpha=float(alpha), max_iter = 10000)\n    y_pred = cross_val_predict(linlasso, X_train, y, cv=5)\n    error.append(mean_squared_error(y,y_pred))\n    \nplt.plot(range(3,13), error);","d7b7ae06":"## Lasso regression\n## smallest error at alpha = 9\nlinlasso = Lasso(alpha=9.0)\ncv = cross_val_score(linlasso, X_train, y, cv=5)\nprint(cv)\nprint(cv.mean())","269b67ac":"## Random Forest Regressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(random_state=0, n_estimators=200, max_depth=10)\ncv = cross_val_score(rf, X_train, y, cv=5)\nprint(cv)\nprint(cv.mean())","040cda59":"## Gradient Boosting Regressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\ngb = GradientBoostingRegressor(random_state=0, n_estimators=200, max_depth=3)\ncv = cross_val_score(gb, X_train, y, cv=5)\nprint(cv)\nprint(cv.mean())","ad83b1f2":"## XGB Regressor\nfrom xgboost import XGBRegressor\n\nxgb = XGBRegressor(random_state=0, n_estimators=30, max_depth=3)\ncv = cross_val_score(xgb, X_train, y, cv=5)\nprint(cv)\nprint(cv.mean())","2018e322":"## LGBM Regressor\nfrom lightgbm import LGBMRegressor\n\nlgbm = LGBMRegressor(random_state=0, n_estimators=100, max_depth=3)\ncv = cross_val_score(lgbm, X_train, y, cv=5)\nprint(cv)\nprint(cv.mean())","8716f76c":"gb.fit(X_train, y)\npredictions = gb.predict(X_test)","ca039629":"## make the file to submit\ndf_submition = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\ndf_submition['SalePrice'] = predictions\n\ndf_submition.drop(df_submition.columns.difference(['Id', 'SalePrice']), axis=1, inplace=True) # Selecting only needed columns\n\nprint(df_submition.shape)\n\ndf_submition.head(5) ","8f7b510a":"df_submition.to_csv('my_submission.csv', index=False)\nprint('File created')","6d7b8734":"If we have too many data points the scatterplots can be overplotted to avoid that we can use joinplots which plot the density of points. ","44d0adcc":"### Categorical data","cc52b9b4":"You can see that these values are the same as the ones from the correlation matrix.","82962b67":"## Exploratory Data Analysis","d476d62f":"### Let's start by looking at the numeric variables\n\nFirst I'm going to take a look at the correlation.","751bdac7":"The goal for this project is to predict house prices depending on various variables. I also decided to use this notebook to practice the use of ANOVA and my abilities with visualizations and hypothesis testing so the exploratory data analysis section will be a bit long. ","0b27ebca":"Even after ANOVA we're left with too many variables so given that I want a model with less that 20 features I decided to do the same thing a did with the numeric data to select the variables for the model, that's fitting a simple linear regression for each variable with significant p-value in ANOVA and find the highest coefficients of determination.","6cfee82d":"## Data Cleaning and Preprocessing\n\nIn this section we'll do the next:\n* Keep only selected features in the train and test dataset -the 20 with the highest r-squared-\n* Deal with null values\n* Get dummies for the categorical data\n* Scale data with standard scaler","eeb693d7":"## Overview\n\n1) Exploratory Data Analysis\n\n2) Data Cleaning and Preprocessing\n\n3) Model Building and Evaluation\n\n4) Submission","1d2fde5b":"Given that we have 53 variables I decided not to do plots instead for the selection I used One-way ANOVA in each variable; if there's a significant difference in the mean SalePrice between the categories of a variable we get a p-value < 0.05 so we pick that variable.","d9de7ac7":"## Submission\n\nFor the submission file we'll use Gradient Boosting Regressor given that give us the best score.","a5e22067":"# Regression Project Example","62ba9b35":"If we look at the SalePrice column or row in the correlation matrix we can find some high correlation coefficients, here the ones above 0.5.\n* 0.709 - GrLivArea (Above grade (ground) living area square feet)\n* 0.640 - GarageCars (Size of garage in car capacity)\n* 0.623 - GarageArea (Size of garage in square feet)\n* 0.614 - TotalBsmtSF (Total square feet of basement area)\n* 0.606 - 1stFlrSF (First Floor square feet)\n* 0.534 - TotRmsAbvGrd (Total rooms above grade -does not include bathrooms-)\n* 0.523 - YearBuilt (Original construction date)\n* 0.507 - YearRemodAdd (Remodel date -same as construction date if no remodeling or additions-)","919b746b":"## Model Building and Evaluation\n\nHere we're going to use various model and use 5 fold cross validation to get a baseline for scores. The models are linear regression, k neighbors regressor, ridge regression, lasso regression, random forest, gradient boosting, XGB regressor, LGBM regressor.","a3828b67":"let's look deeper into this variables. Start by plotting histograms to check the distribution and scatterplots for each of them and the target.","d9eb8d11":"We have null values in different columns: for the categorical we'll just drop them given that is a pretty small percentage, for the numeric we'll fill them with the median, in the case of Alley the percentage is really big so I just drop the column. ","c492d741":"X train and X test ended up with different number of columns and predictions can't be made like this so we drop the diference.","c23259e2":"The correlation coefficient is equal to the r-squared of a simple linear regression to show that I'm going to fit a linear regression for each numeric variable and see what's their respective coefficient of determination.\n\nThese three columns ('1stFlrSF', '2ndFlrSF', '3SsnPorch') raise an error when fitting the model because of the numbers in the name so I take their values to new columns with different names ('stFrSF', 'ndFrSF', 'SsnPorch')."}}