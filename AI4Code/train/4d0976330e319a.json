{"cell_type":{"2dc2fd87":"code","8e7151c4":"code","e380ebdf":"code","92905f0d":"code","d3b82133":"code","a91d22e6":"code","e3f46677":"code","32d4f60b":"code","17f79994":"code","bd7a5b01":"code","f53fdb97":"code","2f85e84d":"code","a55044b5":"code","c25a7bc1":"code","b74df6a7":"code","ac96bf61":"code","628182bd":"code","724f7bc2":"code","68be7035":"code","5e04db79":"code","b1a8f363":"code","63d91cb2":"code","00a8d24c":"code","80a93e94":"code","12d1d049":"code","8a2e6e5e":"code","d4bc6f17":"code","12408168":"code","ff00e3d3":"code","0eedc148":"code","36af8bf6":"code","5920296a":"code","c104ebb3":"code","718a3201":"code","1f798759":"code","d932ab73":"markdown","8bdb4dca":"markdown","b13dd55a":"markdown"},"source":{"2dc2fd87":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom tqdm import tqdm\nimport gc","8e7151c4":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n#\ndef autocorrelation(ys, t=1):\n    return np.corrcoef(ys[:-t], ys[t:])","e380ebdf":"#==========================================================================\ndef preprocess_sales(sales, start=1400, upper=1970):\n    if start is not None:\n        print(\"dropping...\")\n        to_drop = [f\"d_{i+1}\" for i in range(start-1)]\n        print(sales.shape)\n        sales.drop(to_drop, axis=1, inplace=True)\n        print(sales.shape)\n    #=======\n    print(\"adding...\")\n    new_columns = ['d_%i'%i for i in range(1942, upper, 1)]\n    for col in new_columns:\n        sales[col] = np.nan\n    print(\"melting...\")\n    sales = sales.melt(id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\",\"scale1\",\n                                \"sales1\",\"start\",\"sales2\",\"scale2\"],\n                        var_name='d', value_name='demand')\n    #\n    return sales\n#===============================================================\ndef preprocess_calendar(calendar):\n    global maps, mods\n    calendar[\"event_name\"] = calendar[\"event_name_1\"]\n    calendar[\"event_type\"] = calendar[\"event_type_1\"]\n\n    map1 = {mod:i for i,mod in enumerate(calendar['event_name'].unique())}\n    calendar['event_name'] = calendar['event_name'].map(map1)\n    map2 = {mod:i for i,mod in enumerate(calendar['event_type'].unique())}\n    calendar['event_type'] = calendar['event_type'].map(map2)\n    calendar['nday'] = calendar['date'].str[-2:].astype(int)\n    maps[\"event_name\"] = map1\n    maps[\"event_type\"] = map2\n    mods[\"event_name\"] = len(map1)\n    mods[\"event_type\"] = len(map2)\n    calendar[\"wday\"] -=1\n    calendar[\"month\"] -=1\n    calendar[\"year\"] -= 2011\n    mods[\"month\"] = 12\n    mods[\"year\"] = 6\n    mods[\"wday\"] = 7\n    mods['snap_CA'] = 2\n    mods['snap_TX'] = 2\n    mods['snap_WI'] = 2\n    \n    calendar[\"nb\"] = calendar.index + 1\n\n    calendar.drop([\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\", \"date\", \"weekday\"], \n                  axis=1, inplace=True)\n    return calendar\n#=========================================================\ndef make_dataset(categorize=False ,start=1400, upper= 1970):\n    global maps, mods\n    print(\"loading calendar...\")\n    calendar = pd.read_csv(\"..\/input\/m5-forecasting-uncertainty\/calendar.csv\")\n    print(\"loading sales...\")\n    sales = pd.read_csv(\"..\/input\/walmartadd\/sales_aug.csv\")\n    cols = [\"item_id\", \"dept_id\", \"cat_id\",\"store_id\",\"state_id\"]\n    if categorize:\n        for col in cols:\n            temp_dct = {mod:i for i, mod in enumerate(sales[col].unique())}\n            mods[col] = len(temp_dct)\n            maps[col] = temp_dct\n        for col in cols:\n            sales[col] = sales[col].map(maps[col])\n        #\n\n    sales =preprocess_sales(sales, start=start, upper= upper)\n    calendar = preprocess_calendar(calendar)\n    calendar = reduce_mem_usage(calendar)\n    print(\"merge with calendar...\")\n    sales = sales.merge(calendar, on='d', how='left')\n    #del calendar\n\n    print(\"reordering...\")\n    sales.sort_values(by=[\"id\",\"nb\"], inplace=True)\n    print(\"re-indexing..\")\n    sales.reset_index(inplace=True, drop=True)\n    gc.collect()\n\n    sales['n_week'] = (sales['nb']-1)\/\/7\n    sales[\"nday\"] -= 1\n    mods['nday'] = 31\n    sales = reduce_mem_usage(sales)\n    calendar = calendar.loc[calendar.nb>=start]\n    gc.collect()\n    return sales, calendar\n#===============================================================================#","92905f0d":"#=========================\ndef qloss_np(y_true, y_pred):\n    # Pinball loss for multiple quantiles\n    qs = [0.005, 0.025, 0.165, 0.250, 0.500, 0.750, 0.835, 0.975, 0.995]\n    q = np.array([qs])\n    e = y_true - y_pred\n    v = np.maximum(q*e, (q-1)*e)\n    return np.mean(v)","d3b82133":"%%time\nCATEGORIZE = True;\nSTART = 1400; UPPER = 1970;\nmaps = {}\nmods = {}\nsales, calendar = make_dataset(categorize=CATEGORIZE ,start=START, upper= UPPER)","a91d22e6":"sales.nb.min(), sales.nb.max()","e3f46677":"sales[\"x\"] = sales[\"demand\"] \/ sales[\"scale1\"]","32d4f60b":"LAGS = [28, 35, 42, 49, 56, 63]\nFEATS = []\ngroups = sales.groupby(\"id\")\nfor lag in tqdm(LAGS):\n    #sales[f\"x_{lag}\"] = sales.groupby(\"id\")[\"x\"].shift(lag)\n    sales[f\"x_{lag}\"] = groups[\"x\"].shift(lag)\n    FEATS.append(f\"x_{lag}\")\n#","17f79994":"NITEMS = 42840\nscale = sales['scale1'].values.reshape((NITEMS, -1))\nids = sales['id'].values.reshape((NITEMS, -1))\nys = sales[['x','sales1']].values.reshape((NITEMS, -1, 2))\nZ = sales[FEATS].values.reshape((NITEMS, -1, len(FEATS)))","bd7a5b01":"scale.shape, ids.shape, ys.shape, Z.shape","f53fdb97":"LEN = 1969 - START + 1\nMAX_LAG = max(LAGS)\nprint(LEN, MAX_LAG)","2f85e84d":"CATCOLS = ['snap_CA', 'snap_TX', 'snap_WI', 'wday', 'month', 'year', 'event_name', 'nday',\n           'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']","a55044b5":"C = sales[CATCOLS].values.reshape((NITEMS, -1, len(CATCOLS)))","c25a7bc1":"C.shape, Z.shape, ys.shape, LEN","b74df6a7":"import tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nimport tensorflow as tf\nimport itertools as it","ac96bf61":"def make_data(c, z, y):\n    x = {\"snap_CA\":c[:,:,0], \"snap_TX\":c[:,:,1], \"snap_WI\":c[:,:,2], \"wday\":c[:,:,3], \n         \"month\":c[:,:,4], \"year\":c[:,:,5], \"event\":c[:,:,6], \"nday\":c[:,:,7], \n         \"item\":c[:,:,8], \"dept\":c[:,:,9], \"cat\":c[:,:,10], \"store\":c[:,:,11], \n         \"state\":c[:,:,12], \"num\":z}\n    t = y\n    return x, t","628182bd":"#========================\nclass DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, x, brks, batch_size=32, shuffle=True):\n        'Initialization'\n        self.batch_size = batch_size\n        self.c = x[0]\n        self.z = x[1]\n        self.y = x[2]\n        self.brks = brks.copy()\n        self.list_IDs = np.array(range(42840))\n        self.shuffle = shuffle\n        self.nb_batch = int(np.ceil(len(self.list_IDs) \/ self.batch_size))\n        self.n_windows = brks.shape[0]\n        self.on_epoch_end()\n        #B+H <= idx <= LEN - 28\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.ceil(len(self.list_IDs) \/ self.batch_size))*(self.n_windows)\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        idx, kx = self.ids[index]\n        batch_ids = self.list_IDs[idx*self.batch_size:(idx+1)*self.batch_size]\n\n        # Generate data\n        X, y = self.__data_generation(batch_ids, kx)\n\n        return X, y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.ids = list( it.product(np.arange(0, self.nb_batch), self.brks )) \n        if self.shuffle == True:\n            np.random.shuffle(self.ids)\n\n    def __data_generation(self, batch_ids, kx):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        x_batch, y_batch = make_data(self.c[batch_ids, kx-28:kx], self.z[batch_ids,kx-28:kx], \n                                     self.y[batch_ids, kx-28:kx])\n        return x_batch, y_batch\n    #\n#=================================================================","724f7bc2":"#xt, yt = make_data(C[:,MAX_LAG:LEN-56,:], Z[:,MAX_LAG:LEN-56,:], ys[:,MAX_LAG:LEN-56]) #train\nxv, yv = make_data(C[:,LEN-56:LEN-28,:], Z[:,LEN-56:LEN-28,:], ys[:,LEN-56:LEN-28]) # val\nxe, ye = make_data(C[:,LEN-28:LEN,:], Z[:,LEN-28:LEN,:], ys[:,LEN-28:LEN]) # test","68be7035":"xv['snap_CA'].shape, xe['snap_CA'].shape","5e04db79":"#========================\ndef wqloss(y_true, y_pred):\n    # Pinball loss for multiple quantiles\n    qs = [0.005, 0.025, 0.165, 0.250, 0.500, 0.750, 0.835, 0.975, 0.995]\n    q = tf.constant(np.array([qs]), dtype=tf.float32)\n    e = y_true[:,:,:1] - y_pred\n    v = tf.maximum(q*e, (q-1)*e)\n    psl = tf.reduce_mean(v, axis=[1,2])\n    weights = y_true[:,0,1] \/ K.sum(y_true[:,0,1])\n    return tf.reduce_sum(psl * weights)\n#========================\ndef qloss(y_true, y_pred):\n    # Pinball loss for multiple quantiles\n    qs = [0.005, 0.025, 0.165, 0.250, 0.500, 0.750, 0.835, 0.975, 0.995]\n    q = tf.constant(np.array([[qs]]), dtype=tf.float32)\n    e = y_true - y_pred\n    v = tf.maximum(q*e, (q-1)*e)\n    return K.mean(v)\n#============================#\ndef make_model(n_in):\n    \n    seq_len = 28\n    num = L.Input((seq_len, n_in,), name=\"num\")\n    \n    ca = L.Input((seq_len,), name=\"snap_CA\")\n    tx = L.Input((seq_len,), name=\"snap_TX\")\n    wi = L.Input((seq_len,), name=\"snap_WI\")\n    wday = L.Input((seq_len,), name=\"wday\")\n    month = L.Input((seq_len,), name=\"month\")\n    year = L.Input((seq_len,), name=\"year\")\n    event = L.Input((seq_len,), name=\"event\")\n    nday = L.Input((seq_len,), name=\"nday\")\n    item = L.Input((seq_len,), name=\"item\")\n    dept = L.Input((seq_len,), name=\"dept\")\n    cat = L.Input((seq_len,), name=\"cat\")\n    store = L.Input((seq_len,), name=\"store\")\n    state = L.Input((seq_len,), name=\"state\")\n    inp = {\"snap_CA\":ca, \"snap_TX\":tx, \"snap_WI\":wi, \"wday\":wday, \n           \"month\":month, \"year\":year, \"event\":event, \"nday\":nday,\n           \"item\":item, \"dept\":dept, \"cat\":cat, \"store\":store, \n           \"state\":state, \"num\":num} \n    #\n    ca_ = L.Embedding(mods[\"snap_CA\"], mods[\"snap_CA\"], name=\"ca_3d\")(ca)\n    tx_ = L.Embedding(mods[\"snap_TX\"], mods[\"snap_TX\"], name=\"tx_3d\")(tx)\n    wi_ = L.Embedding(mods[\"snap_WI\"], mods[\"snap_WI\"], name=\"wi_3d\")(wi)\n    wday_ = L.Embedding(mods[\"wday\"], mods[\"wday\"], name=\"wday_3d\")(wday)\n    month_ = L.Embedding(mods[\"month\"], mods[\"month\"], name=\"month_3d\")(month)\n    year_ = L.Embedding(mods[\"year\"], mods[\"year\"], name=\"year_3d\")(year)\n    event_ = L.Embedding(mods[\"event_name\"], mods[\"event_name\"], name=\"event_3d\")(event)\n    nday_ = L.Embedding(mods[\"nday\"], mods[\"nday\"], name=\"nday_3d\")(nday)\n    item_ = L.Embedding(mods[\"item_id\"], 10, name=\"item_3d\")(item)\n    dept_ = L.Embedding(mods[\"dept_id\"], mods[\"dept_id\"], name=\"dept_3d\")(dept)\n    cat_ = L.Embedding(mods[\"cat_id\"], mods[\"cat_id\"], name=\"cat_3d\")(cat)\n    store_ = L.Embedding(mods[\"store_id\"], mods[\"store_id\"], name=\"store_3d\")(store)\n    state_ = L.Embedding(mods[\"state_id\"], mods[\"state_id\"], name=\"state_3d\")(state)\n    \n    p = [ca_, tx_, wi_, wday_, month_, year_, event_, nday_, item_, dept_, cat_, store_, state_]\n    context = L.Concatenate(name=\"context\")(p)\n    \n    x = L.Concatenate(name=\"x1\")([context, num])\n    x = L.GRU(100, return_sequences=True, name=\"d1\")(x)\n    x = L.Dropout(0.3)(x)\n    x = L.Concatenate(name=\"m1\")([x, context])\n    x = L.GRU(100, return_sequences=True, name=\"d2\")(x)\n    x = L.Dropout(0.3)(x)\n    x = L.Concatenate(name=\"m2\")([x, context])\n    x = L.GRU(100, return_sequences=True, name=\"d3\")(x)\n    preds = L.Dense(9, activation=\"linear\", name=\"preds\")(x)\n    model = M.Model(inp, preds, name=\"M1\")\n    model.compile(loss=wqloss, optimizer=\"adam\")\n    return model","b1a8f363":"net = make_model(Z.shape[2])\nckpt = ModelCheckpoint(\"w.h5\", monitor='val_loss', verbose=1, save_best_only=True,mode='min')\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=5, min_lr=0.0005)\nes = EarlyStopping(monitor='val_loss', patience=20)\nprint(net.summary())","63d91cb2":"n_slices = LEN \/\/ 28\nbrks = np.array([LEN - (n_slices - i)*28 for i in range(n_slices+1)])\nbrks = brks[brks>=max(LAGS)+28]\nprint(LEN, C.shape, Z.shape)\nprint(brks)","00a8d24c":"C.min(), ys.min(), Z[:,66:].min()","80a93e94":"net.fit_generator(DataGenerator((C, Z, ys), brks[:-1], batch_size=10_000), epochs=100, \n                  validation_data=(xv, yv), callbacks=[ckpt, reduce_lr, es])","12d1d049":"nett = make_model(Z.shape[2])\nnett.load_weights(\"w.h5\")","8a2e6e5e":"pv = nett.predict(xv, batch_size=500, verbose=1)\npe = nett.predict(xe, batch_size=500, verbose=1)","d4bc6f17":"nett.evaluate(xv, yv, batch_size=5000)","12408168":"sv = scale[:,LEN-56:LEN-28]\nse = scale[:,LEN-28:LEN]","ff00e3d3":"k = np.random.randint(0, 42840)\n#k = np.random.randint(0, 200)\nprint(ids[k, 0])\nplt.plot(np.arange(28, 56), yv[k,:,0], label=\"future\")\nplt.plot(np.arange(28, 56), pv[k ,:, 3], label=\"q25\")\nplt.plot(np.arange(28, 56), pv[k ,:, 4], label=\"q50\")\nplt.plot(np.arange(28, 56), pv[k, :, 5], label=\"q75\")\nplt.plot(np.arange(0, 28), ys[k, -56:-28, 0], label=\"past\")\nplt.legend(loc=\"best\")\nplt.show()","0eedc148":"names = [f\"F{i+1}\" for i in range(28)]","36af8bf6":"piv = pd.DataFrame(ids[:, 0], columns=[\"id\"])","5920296a":"QUANTILES = [\"0.005\", \"0.025\", \"0.165\", \"0.250\", \"0.500\", \"0.750\", \"0.835\", \"0.975\", \"0.995\"]\nVALID = []\nEVAL = []\n\nfor i, quantile in tqdm(enumerate(QUANTILES)):\n    t1 = pd.DataFrame(pv[:,:, i]*sv, columns=names)\n    t1 = piv.join(t1)\n    t1[\"id\"] = t1[\"id\"] + f\"_{quantile}_validation\"\n    t2 = pd.DataFrame(pe[:,:, i]*se, columns=names)\n    t2 = piv.join(t2)\n    t2[\"id\"] = t2[\"id\"] + f\"_{quantile}_evaluation\"\n    VALID.append(t1)\n    EVAL.append(t2)\n#============#","c104ebb3":"sub = pd.DataFrame()\nsub = sub.append(VALID + EVAL)\ndel VALID, EVAL, t1, t2","718a3201":"sub.head()","1f798759":"sub.to_csv(\"submission.csv\", index=False)","d932ab73":"### Prediction","8bdb4dca":"## It is a baseline model. Feel free to add your own FE magic !!!","b13dd55a":"This notebook is a continuation of https:\/\/www.kaggle.com\/ulrich07\/quantile-regression-with-keras. Our point is to say it is maybe wrong to write off RNN or CNN nets. Maybe this can lead to a good solution. I'm sure that you guys can iterate on it and better this work !!!"}}