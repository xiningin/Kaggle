{"cell_type":{"596d60bb":"code","a2c89a23":"code","9b0ac9eb":"code","878da442":"code","71681052":"code","78d7faff":"code","b44670b1":"code","3ed9b05b":"code","9dd9a34d":"code","ad7e6ef1":"code","8047b604":"code","9942d193":"code","dd3ad1a4":"code","8283ede7":"code","d84ed4a5":"code","11700a1f":"code","7562d17e":"code","419f1db3":"code","1dc5aa45":"code","89c509be":"code","a1ff6996":"code","ae523a1d":"code","85df1e6c":"code","f27f5178":"code","76ec104e":"code","ef8f4d56":"code","ee2b776a":"code","09bb8000":"code","14a64fe8":"code","f15929b6":"code","67949f8e":"code","bf9a441e":"code","b39f9da3":"code","4f6f6fa7":"code","ac36e432":"code","0fde63bc":"code","85a1d7fd":"code","64b82ecc":"code","260232a6":"code","bafcaebc":"code","3a6a956b":"code","bdf4ca02":"code","02bfffbb":"code","8575ce9b":"code","72883204":"code","2d646b1d":"code","80c25116":"code","0915c2c1":"code","63243742":"code","fdc3c982":"code","2224bb89":"code","6e04eea1":"code","40a74f10":"code","b1dfbd83":"code","e9a9039d":"code","3ac044a3":"code","b2cdbff2":"code","1dbb5e70":"code","8c7371c1":"code","ceb30cf5":"code","4425375f":"code","ebe825cf":"code","e8a749f3":"code","4d9fb1e2":"code","9917b604":"code","ce3cdc8d":"code","769d8017":"code","8eb4daa4":"code","0ba7f63d":"code","20574e73":"code","83f6a321":"code","eec3ae88":"code","97384214":"code","0f6a3173":"code","7a289482":"code","f96f40c5":"code","e99e32dc":"code","34fd6b9d":"code","a84a9daa":"code","b56e7f98":"code","715efefc":"code","c868da50":"code","32b688da":"code","da892ba9":"code","eac10a5e":"code","26a1c4d0":"code","54c20a66":"code","54e2e1f6":"code","fde225cc":"code","c421510b":"code","5fa6bcf4":"code","d428dac7":"code","05a4c06c":"code","3da73550":"code","857285c8":"code","2f3f5ae1":"code","1c806d74":"code","1bd0f987":"code","b63ca048":"code","f730b1a4":"code","3744e53e":"code","1a34ec35":"code","e7ee9607":"code","ba9c0c37":"code","21bccf7b":"code","b8116a43":"code","73026fad":"code","f4a2d3ab":"code","344b765d":"code","c8e11c99":"code","c4db1a4e":"code","3b73cd3c":"code","2395770c":"code","8140fbb6":"code","4bf249ab":"code","b70eef86":"code","509b1b5b":"code","eefe5660":"code","8f52b8dd":"code","0735d222":"code","5598f9c1":"code","45a7c53d":"code","ea8a82d3":"code","072e426e":"code","03702097":"code","59dddf04":"code","27335730":"code","935a0d79":"code","4c36fd7c":"code","4cc8bfaf":"code","e7b5af17":"code","26fd971d":"code","adef32c2":"code","3f5805c6":"code","40b05a77":"code","4647f5f2":"code","432d883f":"code","11d5e037":"code","59cac221":"code","7e85eca1":"code","abb9e077":"code","73947e2f":"code","f5bf35da":"code","9ff36ec3":"code","e5b4a0eb":"code","26f5b9cf":"code","6ad81ac6":"code","4569dcdb":"code","c3ffc396":"code","0985c27c":"code","4362b585":"code","52f4f797":"markdown","1b727b4d":"markdown","78e7aba7":"markdown","5029e5f7":"markdown","a9031ea1":"markdown","22280332":"markdown","3609d895":"markdown","b7278bda":"markdown","90bb7080":"markdown","0ac284c3":"markdown","0eadec73":"markdown","8e73eca1":"markdown","51345df2":"markdown","98c35493":"markdown","c1d97fa7":"markdown","dc3ee189":"markdown","48719c42":"markdown","40b4d14a":"markdown","02bc0b69":"markdown","7f071a35":"markdown","bb15d789":"markdown","20131dc6":"markdown","11236a9d":"markdown","70d33eb1":"markdown","f420cbef":"markdown","73f1e555":"markdown","c2662915":"markdown","245943c1":"markdown","222d9e64":"markdown","d6146d0d":"markdown","c1dca775":"markdown","08575085":"markdown","649fb1b0":"markdown","4011c3dc":"markdown","fba6ff50":"markdown","78c54c86":"markdown","5327eb94":"markdown","2120f777":"markdown","813ee407":"markdown","4d8bbb05":"markdown","7e2c29df":"markdown","78bd25d1":"markdown","09625ab9":"markdown","05a38663":"markdown","7a2b3269":"markdown","97dc23c8":"markdown","08937f7f":"markdown","843ec739":"markdown","30afa667":"markdown","eaf72d00":"markdown","d7347522":"markdown","ba3d69b5":"markdown","a475b110":"markdown","0be7b20f":"markdown","75a9132b":"markdown","6781a2fd":"markdown","02abaabe":"markdown","55455eae":"markdown","944bee51":"markdown","50f7aa0c":"markdown","64bd5cff":"markdown","b12b6e1b":"markdown","3b705289":"markdown","7fcd7e69":"markdown","7dce8dfa":"markdown","da6908f6":"markdown","339e4ebb":"markdown"},"source":{"596d60bb":"import pandas as pd, matplotlib.pyplot as plt, numpy as np, datetime as dt, seaborn as sbrn","a2c89a23":"#Model Testers\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import train_test_split","9b0ac9eb":"#MSE metric, as stated in Kaggle Documentation.\nfrom sklearn.metrics import mean_squared_error","878da442":"from sklearn.decomposition import PCA","71681052":"#Estimators\/models.\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor","78d7faff":"#Feature Selection Functions\nfrom sklearn.feature_selection import SelectKBest, SelectFwe, RFE, RFECV\nfrom sklearn.feature_selection import f_regression, mutual_info_regression, VarianceThreshold","b44670b1":"from sklearn.impute import SimpleImputer","3ed9b05b":"from sklearn.preprocessing import minmax_scale\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import FunctionTransformer","9dd9a34d":"from sklearn.datasets import make_sparse_uncorrelated","ad7e6ef1":"from sklearn.metrics import make_scorer","8047b604":"scope_names = dir()\n#scope_names","9942d193":"#Try replacing with %load or %loadpy and use python sript.\n\n#%load standard_functions.py\n\ndef make_name_from_estimator(obj_estimator):\n    \n    name = str(obj_estimator).split('(')\n    name = name[0].strip('(')\n    return name\n\ndef return_repeatedstratifiedkfold(splits = 5, repeats = 5, rand_state = 88):        \n    from sklearn.model_selection import RepeatedStratifiedKFold\n    \n    cv_splitter = RepeatedStratifiedKFold(\n        n_splits = splits,\n        n_repeats = repeats,\n        random_state = rand_state\n        )\n    \n    return cv_splitter\n\ndef run_rfecv(df, lst_X, y, estimator, scoring_method, cv_splitter = None):\n    \n    if \"RFECV\" not in scope_names:\n        from sklearn.feature_selection import RFECV    \n    \n    if cv_splitter == None:\n        cv_splitter = return_repeatedstratifiedkfold()\n\n    rfecv = RFECV(\n        estimator = estimator,\n        cv = cv_splitter,\n        scoring = scoring_method,\n        n_jobs = -1\n        )\n    \n    rfecv.fit(X = df[lst_X], y = df[y])\n    \n    results = {\n        \"features\": df[lst_X].columns[rfecv.support_],\n        \"split_scores\": rfecv.grid_scores_,\n        \"fit_estimator\": rfecv.estimator_,\n        \"rankings\": rfecv.ranking_\n        }\n    \n    return results\n\ndef run_kbest(feature_data, target_values, n_passers = \"all\", score_func = None):\n#     if (score_func == None) and (\"mutual_info_regression\" not in scope_names):\n#         from skelarn.feature_selection import mutual_info_regression\n#         score_func = mutual_info_regression\n    \n#     if \"SelectKBest\" not in scope_names:\n#         from sklearn.feature_selection import SelectKBest\n\n    from sklearn.feature_selection import SelectKBest\n    \n    kbest = SelectKBest(score_func = score_func, k = n_passers)\n    kbest.fit(X = feature_data, y = target_values)\n        \n    results = {\n        \"kbest_scores\": kbest.scores_,\n        \"kbest_pvalues\": kbest.pvalues_,\n        \"kbest_params\": kbest.get_params(),\n        #\"passing_features\": kbest.get_support()\n        }\n    \n    return results\n\ndef manyset_manymethod_kbest(\n    dict_datasets,\n    dict_targets,\n    lst_score_funcs,   \n    n_passers = \"all\"  \n    ):\n    \n    result_dicts = {}\n    \n    for alias, dataset in dict_datasets.items():\n        set_results = {}\n        \n        for method in lst_score_funcs:\n            try:\n                rslt = run_kbest(\n                    feature_data = dataset,\n                    target_values = dict_targets[alias],                    \n                    n_passers = n_passers,\n                    score_func = method\n                )\n                \n                set_results[ str(method).split(' ')[1] ] = rslt\n            except Exception as e:\n                set_results[ str(method).split(' ')[1] ] = {\"Error\":str(e)}\n        \n        result_dicts[alias] = set_results\n    \n    return result_dicts\n\ndef run_randsearch(estimator, X_data, y_data, dict_params,\n                   scoring_method = None,\n                   n_combinations = 128,\n                   cv_splitter = None,\n                   get_trainset_scores = False\n                   ):\n    \n    if cv_splitter == None:\n        cv_splitter = return_repeatedstratifiedkfold()\n    \n    randomized_search = RandomizedSearchCV(\n        estimator = estimator,\n        param_distributions = dict_params,\n        n_iter = n_combinations,\n        scoring = scoring_method,\n        n_jobs = -1,\n        cv = cv_splitter,\n        return_train_score = get_trainset_scores\n        )        \n    \n    randomized_search.fit(X = X_data, y = y_data)\n    dict_cv_results = randomized_search.cv_results_\n    \n    results = {\n        \"cv_results\": dict_cv_results,\n        \"cv_results_best\": dict_cv_results['params'][randomized_search.best_index_],\n        \"best_params\": randomized_search.best_params_,\n        \"best_score\": randomized_search.best_score_,\n        \"best_estimator\": randomized_search.best_estimator_,\n        \"refit_time\": randomized_search.refit_time_\n        }\n    \n    return results\n\ndef manymodel_manyfeatureset_randsearch(\n        dict_estimators_params,\n        dict_feature_sets,\n        dict_target_features,\n        scoring_method = None,\n        n_combinations = 128,\n        cv_splitter = None,\n        get_trainset_scores = False        \n        ):\n    \n    model_results = {}\n    \n    for estimator, param_grid in dict_estimators_params.items():\n        name = make_name_from_estimator(estimator)        \n        featureset_results = {}\n        \n        for alias, feature_data in dict_feature_sets.items():\n            featureset_results[alias] = run_randsearch(\n                estimator = estimator,\n                X_data = feature_data,\n                y_data = dict_target_features[alias],\n                dict_params = param_grid)\n        \n        model_results[name] = featureset_results\n    \n    return model_results\n\ndef run_gridsearch(\n        estimator, X_data, y_data, dict_params,\n        scoring_method = None,\n        cv_splitter = None,\n        get_trainset_scores = False\n        ):\n    if cv_splitter == None:\n        cv_splitter = return_repeatedstratifiedkfold()\n\n    grid_search = GridSearchCV(\n        estimator = estimator,\n        param_grid = dict_params,\n        scoring = scoring_method,\n        n_jobs = -1,\n        cv = cv_splitter,\n        return_train_score = get_trainset_scores\n        )\n    \n    grid_search.fit(X = X_data, y = y_data)\n    cv_results = grid_search.cv_results_\n    \n    results = {\n        \"best_params\": grid_search.best_params_,\n        \"best_score\": grid_search.best_score_,\n        \"best_estimator\": grid_search.best_estimator_,\n        \"cv_results\": cv_results,\n        \"best_of_cv_results\": cv_results['params'][grid_search.best_index_]\n        }    \n    \n    return results\n\ndef manymodel_manyfeatureset_hparam_gridsearch(\n    dict_estimators_params,\n    dict_X_combinations,\n    dict_y_data,\n    scoring_method,\n    cv_splitter = return_repeatedstratifiedkfold(),\n    get_trainset_scores = False):\n    \n    dict_manymodel_gridsearch = {}\n    \n    for model, hparam_grid in dict_estimators_params.items():\n        name = make_name_from_estimator(model)\n        rslt = {}\n        \n        for alias, feature_combination in dict_X_combinations.items():\n            rslt[alias] = run_gridsearch(\n                estimator = model,\n                dict_params = hparam_grid,                \n                y_data = dict_y_data[alias],\n                X_data = feature_combination,\n                scoring_method = scoring_method,\n                cv_splitter = cv_splitter,\n                get_trainset_scores = get_trainset_scores\n            )            \n            \n        dict_manymodel_gridsearch[name] = rslt\n        \n    return dict_manymodel_gridsearch\n    \ndef run_crossvalscore(\n        estimator, train_data, target_feature,\n        scoring_method = None,\n        cv_splitter = None\n        ):\n    \"\"\"    \n\n    Parameters\n    ----------\n    estimator : MODEL OR ESTIMATOR\n        DESCRIPTION.\n    train_data : TYPE\n        DESCRIPTION.\n    target_feature : TYPE\n        DESCRIPTION.\n    scoring_method : TYPE, optional\n        DESCRIPTION. The default is None.\n    cv_splitter : TYPE, optional\n        DESCRIPTION. The default is None.\n\n    Returns\n    -------\n    results : TYPE\n        DESCRIPTION.\n\n    \"\"\"\n    \n    if (cv_splitter == None) and (\"RepeatedStratifiedKFold\" not in scope_names):\n        cv_splitter = return_repeatedstratifiedkfold()\n    \n    cvscores = cross_val_score(\n        estimator = estimator,\n        X = train_data,\n        y = target_feature,\n        scoring = scoring_method,\n        cv = cv_splitter,\n        n_jobs = -1\n        )    \n    \n    mean_of_scores_withnan = np.mean(cvscores)\n    nanmean_of_scores = np.nanmean(cvscores)\n    standard_deviation = np.std(cvscores)\n    variance = np.var(cvscores)    \n    \n    results = {\n        \"mean_score\": mean_of_scores_withnan,\n        \"nanmean_score\": nanmean_of_scores,\n        \"std\": standard_deviation,\n        \"var\": variance\n        }    \n    \n    return results\n\ndef manymodel_manyfeatureset_cvs(\n        lst_estimators,\n        dict_featuresets,        \n        scoring_method,\n        cv_splitter = return_repeatedstratifiedkfold(),\n        ):\n    \"\"\"\n    Parameters\n    ----------\n    lst_stimators: List-like of Estimator Objects\/Models\n        A list with properly instantiated estimators or algorithm models, ex:\n            MLPRegressor(hidden_layer_sizes = (64, 64)).\n    \n    dict_featuresets: Dictionary\n        A dictionary. The keys are arbitrary names\/aliases used to identify the\n        different sets of training data. The values are the corresponding list with\n        feature data to be passed \"as-is\" to an estimator placed on index 0. The\n        target data will be placed on index 1. THe input may be dataframes,\n        series, or NumPy n-dimensional arrays. Format:\n            { \"dataset_name\": [features, target_values] }\n    \n    scoring_method: String\n        A string giving the scoring metric or criteria, such as root mean squared error\n        or accuracy score.\n    \n    cv_splitter: CV Splitter Object, default RepeatedStratifiedKFold(n_repeats = 5, n_splits = 5)\n        An instance of an SKLearn cross-validation splitter.\n    \n    Returns\n    -------\n    dict_results: Dictionary\n        A dictionary whose keys are the estimator names with parenthesis and parameters removed.\n        The values are also dictionaries.\n        The \"second layer\" dictionaries corresponding to the estimator names\n        have the feature set aliases as keys and\n        dictionaries with summary statistics about the cross-validation scores.\n        Sample Structure:\n        {\n            \"LinearRegression\":\n                {\n                    \"df_raw\":\n                        {\"mean_score\":NaN, \"nanmean_score\":0.53, \"std\":3, \"var\":9},\n                    \"df_processed\":\n                        {\"mean_score\": 0.75, \"nanmean_score\":0.75, \"std\":0.5, \"var\":0.70710}\n             },\n            \"MLPRegression\":\n                {\n                    \"df_raw\":\n                        { \"mean_score\":NaN, \"nanmean_score\":0.83, \"std\":3, \"var\":9 },\n                    \"df_processed\":\n                        {\"mean_score\": 0.85, \"nanmean_score\":0.95, \"std\":0.1, \"var\":0.31622}\n                }    \n        }\n\n    \"\"\"\n    \n    dict_results = dict()\n    \n    for estimator in lst_estimators:\n        name = make_name_from_estimator(estimator)        \n        featureset_rslt = dict()\n        \n        for alias, dataset in dict_featuresets.items():\n            featureset_rslt[alias] = run_crossvalscore(\n                estimator = estimator,\n                train_data = dataset[0],\n                target_feature = dataset[1],\n                scoring_method = scoring_method,\n                cv_splitter = cv_splitter\n                )\n        \n        dict_results[name] = featureset_rslt\n    \n    return dict_results\n\n\ndef make_traintest(df, train_fraction = 0.7, random_state_val = 88):\n    df = df.copy()\n    df_train = df.sample(frac = train_fraction, random_state = random_state_val)    \n    bmask_istrain = df.index.isin(df_train.index.values)\n    df_test = df.loc[ ~bmask_istrain ]\n    \n    #return (df_train, df_test)\n    \n    return {\n        \"train\":df_train,\n        \"test\":df_test\n        }\n\ndef train_models(\n        dict_model_and_dataset\n        ):\n    \"\"\"\n\n    Parameters\n    ----------\n    dict_model_and_dataset : TYPE\n        DESCRIPTION: A dictionary with an arbitrary alias\/name for the model and data set\n        as its keys and the corresponding estimator's data set as a list. The list\n        must have the estimator object at index 0,\n        features' data at index 1 and the target variables' data at index 2.\n        Ex:\n            {\"MLPRegressor_baseline\": [regressor_object,\n                                       train[list_of_features],\n                                       train[\"target_feature\"]\n                                       ]\n             }\n\n    Returns\n    -------\n    fit_models : DICTIONARY\n        DESCRIPTION: A dictionary with the estimator objects after their `.fit()`\n        method has been called with the proper corresponding data as `fit` method\n        parameters.\n        In case of failure to train an estimator, the estimator object is set to\n        `None`.\n        The aliases supplied to this function instance are used as keys.\n\n    \"\"\"\n    \n    fit_models = {}\n    \n    for alias, model_and_data in dict_model_and_dataset.items():\n        name = alias\n        estimator = model_and_data[0]\n        try:\n            estimator.fit(X = model_and_data[1], y = model_and_data[2])\n        except:\n            estimator = None\n        finally:\n            fit_models[name] = estimator\n        \n    return fit_models","dd3ad1a4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8283ede7":"df_train = pd.read_csv(\n    #sep = '\\t',\n    filepath_or_buffer = \"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\",\n    #filepath_or_buffer = \"Ames Iowa Housing Data Kaggle\/train.csv\",\n    index_col = \"Id\"    \n)\n\n# df_test = pd.read_csv(\n#     filepath_or_buffer = \"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\",\n#     #filepath_or_buffer = \"Ames Iowa Housing Data Kaggle\/train.csv\",\n#     index_col = \"Id\"\n# )\n\ndf_holdout = pd.read_csv(\n    filepath_or_buffer = \"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\",\n    #filepath_or_buffer = \"Ames Iowa Housing Data Kaggle\/train.csv\",\n    index_col = \"Id\"\n)","d84ed4a5":"df_train.head()","11700a1f":"#df_train.columns.tolist()","7562d17e":"df_train.shape","419f1db3":"df_holdout.shape","1dc5aa45":"# Features below will not be available for unsold houses. They might also \"leak\" information that would not be present in\n# actual data sets of unsold houses.\nleakers = [\n    \"MoSold\",\n    \"YrSold\",\n    \"SaleType\",\n    \"SaleCondition\",\n    \"SaleType\",\n    \"SaleCondition\"\n]\ndf_train.drop(columns = leakers, inplace = True)\n\nfor item in leakers:\n    try:\n        df_holdout.drop(columns = item, inplace = True)\n    except Exception as e:\n        print(\"Error occured processing\", item, ':')\n        print('\\n', e)\n","89c509be":"lst_true_integers = [    \n    \"YearBuilt\",\n    \"YearRemodAdd\",    \n    \"BsmtFullBath\",\n    \"BsmtHalfBath\",\n    \"FullBath\",\n    \"HalfBath\",\n    \"BedroomAbvGr\",\n    \"KitchenAbvGr\",\n    \"TotRmsAbvGrd\",\n    \"Fireplaces\",\n    \"GarageYrBlt\",\n    \"GarageCars\",    \n]\nlst_true_integers.sort()","a1ff6996":"lst_true_floats = [\n    \"LotFrontage\",\n    \"LotArea\",\n    \"MasVnrArea\",\n    \"BsmtFinSF1\",\n    \"BsmtFinSF2\",\n    \"BsmtUnfSF\",\n    \"TotalBsmtSF\",\n    \"1stFlrSF\",\n    \"2ndFlrSF\",\n    \"LowQualFinSF\",\n    \"GrLivArea\",\n    \"GarageArea\",\n    \"WoodDeckSF\",\n    \"OpenPorchSF\",\n    \"PoolArea\",\n    \"EnclosedPorch\",\n    \"3SsnPorch\",\n    \"ScreenPorch\",\n    \"MiscVal\",    \n]\nlst_true_floats.sort()","ae523a1d":"print(\"Count of true integers and true floats:\", \"Ints:\", len(lst_true_integers), \"Floats:\", len(lst_true_floats))\n\nlst_numerics = [n for n in lst_true_integers]\n#print(len(lst_numerics))\n#lst_numeric_dtypes = [n for n in lst_true_integers]\n\nfor item in lst_true_floats:\n    lst_numerics.append(item)\n    #lst_numeric_dtypes.append(item)\n\nfor feature in lst_true_integers:\n    if feature not in lst_numerics:\n        print(\"Integer Feature missing from Numerics List:\", feature)\nelse:\n    print(\"All integer features added.\")\n        \nfor feature in lst_true_floats:\n    if feature not in lst_numerics:\n        print(\"Float Feature missing from Numerics List:\", feature)\nelse:\n    print(\"All floating numeral features added.\")\n#lst_numeric_dtypes.append(\"SalePrice\")\nprint(len(lst_numerics))","85df1e6c":"def make_int(df):\n    df_result = pd.DataFrame(index = df.index)\n    \n    for feature in df.columns.values:\n        try:\n            df_result[feature] = pd.to_numeric(arg = df[feature], errors = \"coerce\", downcast = \"integer\")\n        except Exception as e:\n            print(\"Failed to transform\", feature, \",\\n\", e)\n    \n    return df_result","f27f5178":"def make_float64(df):\n    df_result = pd.DataFrame(index = df.index)\n    \n    for feature in df.columns.values:\n        try:\n            df_result[feature] = df[feature].astype(dtype = np.float64)\n        except Exception as e:\n            print(\"Failed to transform\", feature, \",\\n\", e)\n    \n    return df_result","76ec104e":"ConvertToInt = FunctionTransformer(func = make_int)\nConvertToFloat64 = FunctionTransformer(func = make_float64)","ef8f4d56":"df_train[lst_true_integers] = ConvertToInt.transform(X = df_train[lst_true_integers])\ndf_train[lst_true_floats] = ConvertToFloat64.transform(X = df_train[lst_true_floats])\n\ndf_holdout[lst_true_floats] = ConvertToInt.transform(X = df_holdout[lst_true_floats])\ndf_holdout[lst_true_floats] = ConvertToFloat64.transform(X = df_holdout[lst_true_floats])","ee2b776a":"lst_categoricals = []\nfor feature in df_train.columns.values:\n    if (feature not in lst_numerics) and (str(feature) != \"SalePrice\"):\n        #df_train[feature] = df_train[feature].astype(dtype = str)\n        #df_train[feature] = df_train[feature].astype(dtype = \"category\")\n        lst_categoricals.append(feature)        \n\ndf_train[\"SalePrice\"] = df_train[\"SalePrice\"].astype(dtype = np.float64)\nlst_categoricals.sort()","09bb8000":"df_train.head()","14a64fe8":"df_train[lst_numerics].info()","f15929b6":"def make_age_from_year(integer_year):\n    return int(dt.datetime.now().year) - integer_year","67949f8e":"lst_year_columns = [\"GarageYrBlt\", \"YearBuilt\", \"YearRemodAdd\"]\nfor feature in lst_year_columns:\n    df_train[feature] = df_train[feature].apply(func = make_age_from_year)\n    df_holdout[feature] = df_holdout[feature].apply(func = make_age_from_year)","bf9a441e":"df_train[lst_year_columns].head()","b39f9da3":"def get_incomplete_features(df, threshold = 0.05):\n    row_count = df.shape[0]\n    sr_null_counts = df.isnull().sum()\n    lst_drop = []\n    lst_have_nulls = sr_null_counts.index.values.tolist()\n    end_decorator = \"\\n******\"\n    for feature in lst_have_nulls:\n        threshold = 0.05\n        null_percent = (sr_null_counts[feature] \/ row_count)\n\n        if null_percent > threshold:\n            print(\"To Drop: \", feature, type(df_train[feature]), type(df_train[feature].iloc[0]) )\n            lst_drop.append(feature)            \n                \n    return lst_drop","4f6f6fa7":"lst_missing_value_failers = get_incomplete_features(df = df_train, threshold = 0.05)\n\ndf_train.drop(columns = lst_missing_value_failers, inplace = True)\n\nlst_features = df_train.columns.tolist()\nlst_features.remove(\"SalePrice\")\ndf_holdout = df_holdout[ lst_features ]\n\nfor feature in lst_missing_value_failers:\n    if feature in lst_numerics:\n        lst_numerics.remove(feature)\n    if feature in lst_categoricals:\n        lst_categoricals.remove(feature)","ac36e432":"temp = pd.Series(data = df_train.select_dtypes(include = \"number\").isnull().sum() )\ntemp[ (temp > 0) ]","0fde63bc":"df_train.head()","85a1d7fd":"# for feature in lst_categoricals:\n#     df_train[feature] = df_train[feature].astype(dtype = str)","64b82ecc":"# df_dummy = df_housing.drop(columns = [\"SalePrice\"])\n# lst_features = df_dummy.columns.values.tolist()\n# df_dummy[\"SalePrice\"] = df_housing[\"SalePrice\"]\n\n# df_dummy[lst_numerics] = slctr_var_threshold.fit_transform(X = df_dummy[lst_numerics])\n# slctr_var_threshold.variances_","260232a6":"def var_threshold(df, minimum_variance = 0.20):\n    dict_variances = {}\n    lst_drop = []\n    df = df.copy()\n    \n    for feature in df.columns.values:\n        name = feature\n        var = df[name].var()        \n        dict_variances[name] = var\n        \n        if var < minimum_variance:\n            lst_drop.append(name)\n    \n    return {\"variances\":dict_variances, \"failers\":lst_drop}","bafcaebc":"def label_encode(feature_data):\n    label_encoder = LabelEncoder()    \n    label_encoded = label_encoder.fit_transform(feature_data)    \n    \n    return {\"encoded_data\":label_encoded, \"encoder_object\":label_encoder}","3a6a956b":"#Process is sub-obtimal because it performs some steps of transform before error stops execution.\n#Ideally, target column would be passed in or categoricals would be automatically targeted.\ndef label_encode(df):\n    for feature in df:\n        try:\n            label_encoder = LabelEncoder()\n            label_encoded = label_encoder.fit_transform(df_feature)\n            df[feaure] = label_encoded\n        except Exception as e:\n            print(\"Could not label-encode\", feature, \".\\n\", e)\n    \n    return df","bdf4ca02":"label_encoder = LabelEncoder()\ndf_categoricals_train = df_train[lst_categoricals].dropna()\n\ndict_classes = {}\n\nfor feature in df_categoricals_train.columns.values:\n    df_categoricals_train[feature] = label_encoder.fit_transform(df_categoricals_train[feature].astype(str))    \n    dict_classes[feature] = label_encoder.classes_","02bfffbb":"# for feature, classes in dict_classes.items():\n#     print(feature, \"\\n\", classes)","8575ce9b":"df_categoricals_train.head()","72883204":"lst_features = df_train.columns.tolist()\nlst_features.remove(\"SalePrice\")\n\nlst_current_numeric_features = df_train.select_dtypes(include = \"number\").columns.tolist()\nlst_current_numeric_features.remove(\"SalePrice\")\n\nlst_current_numeric_columns = df_train.select_dtypes(include = \"number\").columns.tolist()\n\ndf_numeric_features = pd.DataFrame(\n    data = minmax_scale(X = df_train[lst_numerics]),\n    index = df_train.index,\n    columns = df_train[lst_numerics].columns\n)\ndf_numeric_features.head()","2d646b1d":"df_categoricals_train[lst_categoricals] = minmax_scale(X = df_categoricals_train[lst_categoricals])\ndf_categoricals_train.head()","80c25116":"results = var_threshold(df = df_numeric_features[lst_numerics], minimum_variance = 0.05)\ndict_variances, lst_variance_failers = results[\"variances\"], results[\"failers\"]","0915c2c1":"categorical_results = var_threshold(df = df_categoricals_train, minimum_variance = 0.05)\ndict_categoricals_variances, lst_variance_failers_categorical = categorical_results[\"variances\"], categorical_results[\"failers\"]","63243742":"for failer in lst_variance_failers_categorical:\n    lst_variance_failers.append(failer)","fdc3c982":"print(len(lst_variance_failers))","2224bb89":"print(len(lst_features))","6e04eea1":"for feature in lst_variance_failers:\n    try:\n        lst_numerics.remove(feature)\n        lst_numeric_dtypes.remove(feature)        \n    except Exception as e:\n        print(\"Error removing\", feature, \"from list of true numerics\/numeric data types.\", '\\n', e)\n        \nfor feature in lst_variance_failers:\n    try:\n        df_train.drop(columns = feature, inplace = True)\n    except Exception as e:\n        print(\"Error removing\", feature, \"from training data.\", '\\n', e)\n        \nfor feature in lst_variance_failers:\n    try:\n        df_holdout.drop(columns = feature, inplace = True)\n    except Exception as e:\n        print(\"Error removing\", feature, \"from holdout data.\", '\\n', e)\n        \nfor feature in lst_variance_failers:\n    try:\n        lst_categoricals.remove(feature)\n    except Exception as e:\n        print(\"Error removing\", feature, \"from lst_categoricals.\", '\\n', e)","40a74f10":"lst_variance_failers.sort()\nlst_variance_failers","b1dfbd83":"split = make_traintest(df_train)\ndf_housing_clean = df_train.copy()\n\ndf_train = split[\"train\"]\ndf_test = split[\"test\"]","e9a9039d":"df_train.shape","3ac044a3":"df_test.shape","b2cdbff2":"df_holdout.shape","1dbb5e70":"def impute_categorical(sr):\n    label_encoder = LabelEncoder()    \n    \n    encoded_data = label_encoder.fit_transform( sr.dropna().astype(dtype = str) )    \n    \n    value_counts = sr.astype(dtype = str).value_counts().sort_values(ascending = False)    \n    mode = value_counts.iloc[0]\n    \n    bmask_isnull = sr.isnull()\n    sr_string_version = sr.astype(dtype = str)\n    sr_string_version[bmask_isnull] = mode\n    \n    ndarr_clean = label_encoder.transform(sr_string_version)\n    ndarr_clean = label_encoder.inverse_transform(encoded_data) \n    \n    return {\"data\":ndarr_clean, \"impute_value\":mode}","8c7371c1":"# for feature in lst_categoricals:\n#     try:\n#         impute_results = impute_categorical(df_train[feature])\n#         df_train[feature] = impute_results[\"data\"]\n        \n# #         bmask_isnull_test = df_test[feature].isnull()\n# #         df_test.loc[bmask_isnull_test, feature] = impute_results[\"impute_value\"]\n        \n    \n#         impute_results = impute_categorical(df_test[feature])\n#         df_test[feature] = impute_results[\"data\"]\n        \n#     except Exception as e:\n#         print(\"Exception in imputing\", feature, \":\")\n#         print(e)","ceb30cf5":"# dict_classes = {}\n# for feature in lst_categoricals:\n#     impute_results = impute_categorical(df_train[feature])\n#     df_train[feature] = impute_results[\"data\"]\n    \n#     encoding_result = label_encode(df_train[feature])\n#     try:\n#         dict_classes[feature] = encoding_result[\"encoder_object\"].classes_\n#     except:\n#         pass\n    \n# #       bmask_isnull_test = df_test[feature].isnull()\n# #       df_test.loc[bmask_isnull_test, feature] = impute_results[\"impute_value\"]\n        \n    \n#     impute_results = impute_categorical(df_test[feature])\n#     df_test[feature] = impute_results[\"data\"]","4425375f":"df_train.isnull().sum()","ebe825cf":"df_test.isnull().sum()","e8a749f3":"df_train_modes = df_train.mode()\nfor feature in df_train_modes.columns.values:\n    df_train[feature] = df_train[feature].fillna(value = df_train_modes[feature].iloc[0])\n    #df_train[feature].fillna(inplace = True, value = df_train_modes[feature].iloc[0])\n\ndf_test_modes = df_test.mode()\nfor feature in df_test_modes.columns.values:\n    df_test[feature] = df_test[feature].fillna(value = df_test_modes[feature].iloc[0])\n    \ndf_holdout_modes = df_holdout.mode()\nfor feature in df_holdout_modes.columns.values:\n    df_holdout[feature] = df_holdout[feature].fillna(value = df_holdout_modes[feature].iloc[0])","4d9fb1e2":"max(df_test[lst_categoricals].isnull().sum())","9917b604":"max(df_train[lst_categoricals].isnull().sum())","ce3cdc8d":"max(df_holdout.isnull().sum())","769d8017":"df_train.head(2)","8eb4daa4":"df_train.shape","0ba7f63d":"df_test.head(2)","20574e73":"df_test.shape","83f6a321":"df_holdout.head(2)","eec3ae88":"df_holdout.shape","97384214":"max(df_train.isnull().sum())","0f6a3173":"max(df_test.isnull().sum())","7a289482":"max(df_holdout.isnull().sum())","f96f40c5":"imputer_mean = SimpleImputer(\n    strategy = \"mean\",\n    verbose = True,\n    add_indicator = False,\n    missing_values = np.nan\n)\n\nlst_current_numeric_columns = df_train.select_dtypes(include = \"number\").columns.tolist()\ndf_train[lst_numerics] = imputer_mean.fit_transform(X = df_train[lst_numerics])\ndf_train[\"SalePrice\"] = imputer_mean.fit_transform(X = df_train[[\"SalePrice\"]])\n\nlst_current_numeric_columns.remove(\"SalePrice\")\ndf_test[lst_numerics] = imputer_mean.fit_transform(X = df_test[lst_numerics])\ndf_test[\"SalePrice\"] = imputer_mean.fit_transform(X = df_test[[\"SalePrice\"]])\n\ndf_holdout[lst_numerics] = imputer_mean.fit_transform(X = df_holdout[lst_numerics])","e99e32dc":"temp = pd.Series(data = df_train.select_dtypes(include = \"number\").isnull().sum() )\ntemp[ (temp > 0) ]","34fd6b9d":"temp = pd.Series(data = df_holdout.select_dtypes(include = \"number\").isnull().sum() )\ntemp[ (temp > 0) ]","a84a9daa":"def print_result_dict(dict_result):\n    \n    for dataset, dataset_results in dict_result.items():\n        print(\"\\n=====\\n\", \"Start\", \"\\n=====\\n\", dataset)\n        \n        for method, result_dict in dataset_results.items():\n            print(\"\\n-----\\n\", method)\n            for result_name, result_value in result_dict.items():\n                print(\":\", result_name, \":\")\n                try:\n                    sr_values = pd.Series(data = result_value)\n                    print(sr_values.sort_values(ascending = False))\n                except:\n                    print(result_value)\n                finally:\n                    print(\"\\n*** End Section***\")\n        print(\"\\n=== End of Results ===\\n\")\n    return None","b56e7f98":"# def manyset_manymethod_kbest(\n#     dict_datasets,\n#     dict_targets,\n#     lst_score_funcs,\n#     y = \"is_benign\",    \n#     n_passers = \"all\"    \n#     ):\n\ndict_kbest_results = manyset_manymethod_kbest(\n    dict_datasets = {\"df_train\" : df_train[lst_numerics] },\n    dict_targets = {\"df_train\" : df_train[\"SalePrice\"] },\n    lst_score_funcs = [f_regression, mutual_info_regression],\n    n_passers = 0\n)","715efefc":"print_result_dict(dict_kbest_results)","c868da50":"def filter_by_threshold(dict_value_series, remove_low = True, dict_thresholds = {\"default\":np.nan}):\n    lst_failers = []    \n    dict_exceptions = {}\n    \n    if remove_low == True: \n        try:\n            for alias, series in dict_value_series.items():\n                for feature in series.index.tolist():\n                    if (series[feature] <= dict_thresholds[alias]) and (feature not in lst_failers):\n                        lst_failers.append(feature)\n        except Exception as e:                \n                dict_exceptions[\"alias\"] = str(e)            \n                    \n    else:        \n        for alias, series in dict_value_series.items():\n            try:\n                for feature in series.index.tolist():                \n                    if (series[feature] <= dict_thresholds[alias]) and (feature not in lst_failers):\n                        lst_failers.append(feature)            \n            except Exception as e:                \n                dict_exceptions[\"alias\"] = str(e)\n    \n    return lst_failers","32b688da":"def extract_result_dict(dict_result):\n    \n    extracted = {}\n    \n    for dataset, dataset_results in dict_result.items():\n        #print(\"\\n=====\\n\", \"Start\", \"\\n=====\\n\", dataset)\n        \n        for method, result_dict in dataset_results.items():\n            #print(\"\\n-----\\n\", method)\n            for result_name, result_value in result_dict.items():\n                item_name = method + '-' + result_name\n                try:\n                    sr_values = pd.Series(data = result_value)\n                    extracted[item_name] = sr_values.sort_values(ascending = False)\n                except Exception as e:\n                    print(\"Error occured processing:\", item_name, e)\n                    continue        \n                    \n    return extracted","da892ba9":"def find_kbest_statistics_max(dictionary_of_series):\n    dict_maximums = {}\n    for alias, series in dictionary_of_series.items():\n        try:\n            dict_maximums[alias] = np.nanmax(series)\n        except:\n            dict_maximums[alias] = np.nan\n    \n    return dict_maximums","eac10a5e":"def find_kbest_statistics_min(dictionary_of_series):\n    dict_minimums = {}\n    for alias, series in dictionary_of_series.items():\n        try:\n            dict_minimums[alias] = np.nanmin(series)\n        except:\n            dict_minimums[alias] = np.nan\n            \n    return dict_minimums","26a1c4d0":"#Documentation: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.make_sparse_uncorrelated.html\n#The first four features are useful for regression. The rest are not.\nfeature_count = len(lst_numerics) + 4\nrow_count = len(df_train[\"SalePrice\"])\nsparse_uncorrelated_X, sparse_uncorrelated_y = make_sparse_uncorrelated(\n    n_samples = row_count,\n    n_features = feature_count,\n    random_state = 0\n)","54c20a66":"df_sparse_uncorr = pd.DataFrame(data = sparse_uncorrelated_X)\ndf_sparse_uncorr = df_sparse_uncorr.loc[ : , 4: ]\ndf_sparse_uncorr[\"SalePrice\"] = sparse_uncorrelated_y","54e2e1f6":"df_sparse_uncorr.head()","fde225cc":"df_sparse_uncorr.shape","c421510b":"dict_sparse_uncorr = manyset_manymethod_kbest(\n    dict_datasets = {\"df_sparse_uncorr\" : df_sparse_uncorr.drop(columns = [\"SalePrice\"]) },\n    dict_targets = {\"df_sparse_uncorr\" : df_sparse_uncorr[\"SalePrice\"] },\n    lst_score_funcs = [f_regression, mutual_info_regression]  \n)","5fa6bcf4":"print_result_dict(dict_sparse_uncorr)","d428dac7":"kbest_stats_for_synthetic = extract_result_dict(dict_sparse_uncorr)\nkbest_stats_for_synthetic.keys()","05a4c06c":"kbest_stats_for_train = extract_result_dict(dict_kbest_results)\n#del(kbest_stats_for_train[\"f_regression-kbest_pvalues\"])\nkbest_stats_for_train.keys()","3da73550":"sparse_uncorr_failers = set(\n    filter_by_threshold(\n        dict_value_series = kbest_stats_for_train,\n        dict_thresholds = {\n            \"f_regression-kbest_scores\": np.max(kbest_stats_for_synthetic[\"f_regression-kbest_scores\"]),\n            \"mutual_info_regression-kbest_scores\": np.max(kbest_stats_for_synthetic[\"mutual_info_regression-kbest_scores\"])\n        }\n    )\n)\nsparse_uncorr_failers","857285c8":"lst_synthetic_data_failers = list(df_train[lst_numerics].columns[list(sparse_uncorr_failers)])","2f3f5ae1":"#List of failers for `mutual info regression` and `f_regression` tests.\nlst_synthetic_data_failers","1c806d74":"#Consistency check. The list `lst_numerics` was used to slice the data frame and get the statistics for the train dataset.\n#The list is sorted, as are columns of the dataframes. DF columns are automatically sorted.\ndef check_iloc_in_lst_numerics(iterable_iloc_indexer, index_names):\n    for iloc_index in iterable_iloc_indexer:\n        print(index_names[ int(iloc_index)] )","1bd0f987":"check_iloc_in_lst_numerics( list(sparse_uncorr_failers), lst_numerics)","b63ca048":"FWE = SelectFwe(score_func = f_regression, alpha = 0.01)\nFWE.fit_transform(X = df_train[lst_numerics], y = df_train[\"SalePrice\"])\nbmask_fwe_passers = FWE.get_support()\nbmask_fwe_failers = ~bmask_fwe_passers","f730b1a4":"lst_fwe_passers = df_train[lst_numerics].columns[bmask_fwe_passers]\nlst_fwe_passers","3744e53e":"lst_fwe_failers = df_train[lst_numerics].columns[bmask_fwe_failers].tolist()\nlst_fwe_failers.sort()\nlst_fwe_failers","1a34ec35":"decomposer_pca = PCA(n_components = None)\ndecomposer_pca.fit(df_sparse_uncorr)","e7ee9607":"uncorrelated_max_explained_variance = max(decomposer_pca.explained_variance_)","ba9c0c37":"uncorrelated_max_explained_variance_ratio = max(decomposer_pca.explained_variance_ratio_)","21bccf7b":"label_encoder = LabelEncoder()\ndf_categoricals_label_encoded = pd.DataFrame(\n    columns = df_train[lst_categoricals].columns,\n    index = df_train[lst_categoricals].index\n)\nfor feature in lst_categoricals:\n    df_categoricals_label_encoded[feature] = label_encoder.fit_transform(y = df_train[feature])\n\ndecomposer_pca.fit(df_categoricals_label_encoded)\nlst_pca_failers = []\nlst_explained_variances = decomposer_pca.explained_variance_\nlst_explained_variance_ratios = decomposer_pca.explained_variance_ratio_\n\nfor feature_iloc in range( len(decomposer_pca.explained_variance_) ):\n    feature = lst_categoricals[feature_iloc]    \n    \n    if (lst_explained_variances[feature_iloc] <= uncorrelated_max_explained_variance) and (feature not in lst_pca_failers):\n        lst_pca_failers.append(feature)\n    \n    if (lst_explained_variance_ratios[feature_iloc] <= uncorrelated_max_explained_variance) and (feature not in lst_pca_failers):\n        lst_pca_failers.append(feature)\n        \nlst_pca_failers        ","b8116a43":"lst_statistical_failers = []\n\nfor item in lst_fwe_failers:\n    if item not in lst_statistical_failers:\n        lst_statistical_failers.append(item)\n\nfor item in lst_synthetic_data_failers:\n    if item not in lst_statistical_failers:\n        lst_statistical_failers.append(item)\n\nfor item in lst_pca_failers:\n    if item not in lst_statistical_failers:\n        lst_statistical_failers.append(item)","73026fad":"df_train = df_train.append(df_test)\ndf_train = df_train.append(df_holdout)","f4a2d3ab":"df_train = pd.get_dummies(data = df_train)\ndf_holdout = df_train.loc[1461: , : ]\ndf_holdout.drop(columns = \"SalePrice\", inplace = True)\ndf_train.drop(index = df_train.loc[1461: , : ].index, inplace = True)\n\nsplit = make_traintest(df = df_train)\ndf_train = split[\"train\"]\ndf_test = split[\"test\"]","344b765d":"df_train.shape","c8e11c99":"df_test.shape","c4db1a4e":"df_test[\"SalePrice\"]","3b73cd3c":"df_holdout.shape","2395770c":"df_train_statclean = df_train.copy()\ndf_test_statclean = df_test.copy()\ndf_holdout_statclean = df_holdout.copy()\n\nfor feature in lst_statistical_failers:\n    try:\n        df_train_statclean.drop(columns = feature, inplace = True)\n    except Exception as e:\n        print(\"Error dropping from train\", feature, '\\n', e)\n\nfor feature in lst_statistical_failers:\n    try:\n        df_test_statclean.drop(columns = feature)\n    except Exception as e:\n        print(\"Error dropping from test\", feature, '\\n', e)\n        \nfor feature in lst_statistical_failers:\n    try:\n        df_holdout_statclean.drop(inplace = True, columns = feature)\n    except Exception as e:\n        print(\"Error dropping from holdout\", feature, '\\n', e)","8140fbb6":"mdl_linreg = LinearRegression()\nmdl_randforest = RandomForestRegressor()\nmdl_nnregressor = MLPRegressor(hidden_layer_sizes = (287, 64, 64) )","4bf249ab":"mdl_linreg.fit(X = df_train.drop(columns = \"SalePrice\"), y = df_train[\"SalePrice\"])\nmdl_randforest.fit(X = df_train.drop(columns = \"SalePrice\"), y = df_train[\"SalePrice\"])\nmdl_nnregressor.fit(X = df_train.drop(columns = \"SalePrice\"), y = df_train[\"SalePrice\"])","b70eef86":"mse_scorer = make_scorer(mean_squared_error)","509b1b5b":"# dict_baseline_cvs = manymodel_manyfeatureset_cvs(\n#     lst_estimators = [mdl_linreg, mdl_randforest, mdl_nnregressor],\n#     dict_featuresets = {\n#         \"df_train\":[\n#             df_train.drop(columns = \"SalePrice\"),\n#             df_train[\"SalePrice\"]\n#         ],\n#         \"df_test\":[\n#             df_test.drop(columns = \"SalePrice\"),\n#             df_test[\"SalePrice\"]\n#         ]\n#     },\n#     scoring_method = mse_scorer\n# )","eefe5660":"def print_cv_result_dict(dict_result):\n    \n    for dataset, dataset_results in dict_result.items():\n        print(\"\\n=====\\n\", \"Start\", \"\\n=====\\n\", dataset)\n        \n        for method, result_dict in dataset_results.items():\n            print(\"\\n-----\\n\", method)\n            for result_name, result_value in result_dict.items():\n                print(\":\", result_name, \":\")\n                res = \"{:.3f}\".format( np.sqrt(result_value) )\n                try:                    \n                    sr_values = pd.Series(data = res)\n                    print(sr_values.sort_values(ascending = False))\n                except:\n                    print( res )\n                finally:\n                    print(\"\\n*** End Section***\")\n        print(\"\\n=== End of Results ===\\n\")\n    return None","8f52b8dd":"#print_cv_result_dict(dict_baseline_cvs)","0735d222":"max(df_train_statclean.isnull().sum())","5598f9c1":"max(df_test_statclean.isnull().sum())","45a7c53d":"max(df_holdout_statclean.isnull().sum())","ea8a82d3":"mdl_randforest_statcleaned = RandomForestRegressor()\nmdl_randforest_statcleaned.fit(\n    X = df_train_statclean.drop(columns = [\"SalePrice\"]),\n    y = df_train_statclean[\"SalePrice\"])","072e426e":"mdl_nnregressor_statcleaned = MLPRegressor(hidden_layer_sizes = (277, 128, 128), max_iter = 2054)\nmdl_nnregressor_statcleaned.fit(X = df_train_statclean.drop(columns = [\"SalePrice\"]), y = df_train_statclean[\"SalePrice\"])","03702097":"# dict_crossval_results = manymodel_manyfeatureset_cvs(\n#     lst_estimators = [mdl_randforest_statcleaned, mdl_nnregressor_statcleaned],\n#     dict_featuresets = {\n#         \"df_train_statclean\": [\n#             df_train_statclean.drop(columns = [\"SalePrice\"]),\n#             df_train_statclean[\"SalePrice\"]\n#         ],\n#         \"df_test_statclean\": [\n#             df_test_statclean.drop(columns = \"SalePrice\"),\n#             df_test_statclean[\"SalePrice\"]\n#         ],\n#         \"df_train_statclean_numeric\":[\n#             df_train_statclean.select_dtypes(include = \"number\").drop(columns = \"SalePrice\"),\n#             df_train_statclean[\"SalePrice\"]\n#         ],\n#         \"df_test_statclean_numeric\": [\n#             df_test_statclean.select_dtypes(include = \"number\").drop(columns = \"SalePrice\"),\n#             df_test_statclean[\"SalePrice\"]\n#         ]\n#     },\n#     scoring_method = mse_scorer\n# )","59dddf04":"#print_cv_result_dict(dict_crossval_results)","27335730":"def save_submission_file(dict_data, filename):\n    df = pd.DataFrame(data = dict_data)\n    timestamp = str( dt.datetime.now() )\n    name = filename + '-' + timestamp + \".csv\"\n    \n    df.to_csv(\n        path_or_buf = name,\n        index = False\n    )\n    \n    return None","935a0d79":"nn_results = mdl_nnregressor_statcleaned.predict(X = df_holdout_statclean)\n#ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 307 is different from 242)\nrandforest_results = mdl_randforest_statcleaned.predict(X = df_holdout_statclean)\n#ValueError: Number of features of the model must match the input. Model n_features is 223 and input n_features is 208 ","4c36fd7c":"baseline_randforest = mdl_randforest.predict(X = df_holdout)\nbaseline_nnregressor = mdl_nnregressor.predict(X = df_holdout)","4cc8bfaf":"save_submission_file(\n    dict_data = {\"Id\":df_holdout.index.tolist(), \"SalePrice\":baseline_randforest},\n    filename = \"Baseline Random Forest Regressor\"\n)","e7b5af17":"save_submission_file(\n    dict_data = {\"Id\":df_holdout.index.tolist(), \"SalePrice\":baseline_nnregressor},\n    filename = \"Baseline Neural Net Regressor\"\n)","26fd971d":"save_submission_file(\n    dict_data = {\"Id\":df_holdout.index.tolist(), \"SalePrice\":nn_results},\n    filename = \"Neural Network Regressor, Stat Clean\"\n)","adef32c2":"save_submission_file(\n    dict_data = {\"Id\":df_holdout.index.tolist(), \"SalePrice\":randforest_results},\n    filename = \"Random Forest Regressor, Stat Clean\"\n)","3f5805c6":"!pip install -U tensorflow","40b05a77":"import tensorflow as tf\ntf.__version__","4647f5f2":"!pip install git+https:\/\/github.com\/keras-team\/keras-tuner.git@1.0.2rc2\n!pip install autokeras","432d883f":"#autokeras.__version__\nfrom autokeras import StructuredDataRegressor","11d5e037":"# define the search\nsearch = StructuredDataRegressor(max_trials = 8, loss='mean_squared_error')\nprint(\"Started search.\")\n# perform the search\nsearch.fit(\n    x = df_train_statclean.drop(columns = \"SalePrice\"),\n    y = df_train_statclean[\"SalePrice\"]\n)","59cac221":"# evaluate the model\nmse = search.evaluate(\n    x = df_test_statclean.drop(columns = \"SalePrice\"),\n    y = df_test_statclean[\"SalePrice\"],\n    verbose = 0)\nprint('RMSE: {.3f}'.format(np.sqrt(mse)))\n\n# use the model to make a prediction\nyhat = search.predict(df_holdout_statclean)\nprint('Predicted: %.3f' % yhat[0])\n\n# get the best performing model\nmodel = search.export_model()\n\n# summarize the loaded model\nmodel.summary()\n\n# save the best performing model to file\nmodel.save('mdl_autokeras_baseline')","7e85eca1":"from tpot import TPOTRegressor\n# define evaluation procedure\ncv_rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=88)\n# define search\nmodel_tpotreg = TPOTRegressor(\n    generations=4,\n    population_size=8,\n    scoring='neg_mean_absolute_error',\n    cv=cv_rskf,\n    verbosity=2,\n    random_state=88,\n    n_jobs=-1\n)","abb9e077":"# perform the search\nmodel_tpotreg.fit(df_train_statclean.drop(columns = \"SalePrice\"), df_train_statclean[\"SalePrice\"])","73947e2f":"# export the best model\nmodel_tpotreg.export('44201762 TPOT Baseline gen=4, popsize=8.py')","f5bf35da":"tpot_baseline = model_tpotreg.predict(df_holdout_statclean)\n\nsave_submission_file(\n    dict_data = {\"Id\":df_holdout.index.tolist(), \"SalePrice\":tpot_baseline},\n    filename = \"44201762 TPOT Baseline, RidgeCV(RobustScaler(input_matrix))\"\n)","9ff36ec3":"from tpot import TPOTRegressor\n\ntpot = TPOTRegressor(\n    generations = 32,\n    population_size = 256,\n    verbosity = 3,\n    random_state = 88)\n\ntpot.fit(df_train_statclean.drop(columns = \"SalePrice\"), df_train_statclean[\"SalePrice\"])","e5b4a0eb":"print(tpot.score(df_train_statclean.drop(columns = \"SalePrice\"), df_train_statclean[\"SalePrice\"]))\ntpot.export('44201762 TPOTRegressor gen 32 pop 256 randstate 88.py')","26f5b9cf":"tpot_results = tpot.predict(df_holdout_statclean)\nsave_submission_file(\n    dict_data = {\"Id\":df_holdout_statclean.index.tolist(), \"SalePrice\":tpot_results},\n    filename = \"44201762 TPOT TPOTRegressor gen 32 pop 256 randstate 88\"\n)","6ad81ac6":"df_train_statclean.to_csv(path_or_buf = \"\/kaggle\/working\/df_train_statclean.csv\" )\ndf_test_statclean.to_csv(path_or_buf = \"\/kaggle\/working\/df_test_statclean.csv\")\ndf_holdout_statclean.to_csv(path_or_buf = \"\/kaggle\/working\/df_holdout_statclean.csv\")","4569dcdb":"from sklearn.svm import SVC, LinearSVC, LinearSVR\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.decomposition import PCA\nfrom hpsklearn import HyperoptEstimator\nimport xgboost\n\nmdl_svc, mdl_linSVC = SVC(), LinearSVC()\nmdl_randforest_class = RandomForestClassifier(\n    n_jobs = -1,\n    random_state=88,\n    max_samples = 0.70,\n    min_samples_leaf=0.05\n    )\nlst_classifiers = [mdl_svc, mdl_linSVC, mdl_randforest_class]\n\nmdl_svr, mdl_xgb_regress = LinearSVR(), xgboost.XGBRegressor()\nmdl_randforest_regress = RandomForestClassifier(\n    n_jobs = -1,\n    random_state=88,\n    max_samples=0.70,\n    min_samples_leaf=0.05\n    )\n#lst_regressors = [mdl_xgb_regress, mdl_svr, mdl_randforest_regress]\nlst_regressors = [mdl_xgb_regress, mdl_svr]\n\nprep_pca, prep_onehot = PCA(), OneHotEncoder()\nimputer_mean = SimpleImputer()\nlst_preprocessors = [prep_pca, prep_onehot, imputer_mean]\n\nestimator_hyperopt = HyperoptEstimator(\n    regressor=lst_regressors,\n    preprocessing=lst_preprocessors,\n    max_evals=32,\n    trial_timeout=300)","c3ffc396":"estimator_hyperopt.fit(\n    X = df_train_statclean.drop(columns = \"SalePrice\").values,\n    y = df_train_statclean[\"SalePrice\"].values)\n\ntest_score = estimator_hyperopt.score(\n    X = df_test_statclean.drop(columns = \"SalePrice\").values,\n    y = df_test_statclean[\"SalePrice\"].values\n    )\n\nmdl_best_hyperopt = estimator_hyperopt.best_model()\nprint(\"Best HyperOpt model:\\n\", mdl_best_hyperopt)","0985c27c":"ndarr_predictions_hyperopt = mdl_best_hyperopt.predict(X = df_holdout.values)\nsave_submission_file(\n    dict_data = {\"Id\":df_holdout.index.tolist(), \"SalePrice\":ndarr_predictions_hyperopt},\n    filename = \"Neural Network Regressor, Stat Clean\"\n)","4362b585":"print(\"Done\")","52f4f797":"### *** Warning: Unsafe practice has to be implemented because test data contains np.nan while train data does not.\n```\nfor feature in df_categoricals_train.columns.values.tolist():\n    df_categoricals_train[feature] = label_encoder.fit_transform(df_train[feature].astype(str))    \n    df_categoricals_test[feature] = label_encoder.fit_transform(df_test[feature].astype(str))\n```\nThe label_encoder should be fit only on train data. After fitting, the transform must be used on test data. The above method does a `fit_transform` separately for train and test.\nCode above should be:\n```\nfor feature in df_categoricals_train.columns.values.tolist():\n    df_categoricals_train[feature] = label_encoder.fit_transform(df_train[feature].astype(str))    \n    df_categoricals_test[feature] = label_encoder.transform(df_test[feature].astype(str))\n```\n\n### Temporary Solution:\n1. The `np.nan` values will be dropped. They will not be considered when calculating variance.\nNote:\n\n```\n>>>print(str(np.nan))\nnan\n>>>df = pd.DataFrame(data = {\"mix\":[\"2\", None, np.nan, \"0\"], \"int\":[1, 2, 3, 4]})\n>>>df[\"mix\"] = df[\"mix\"].astype(np.float64)\n>>>df.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4 entries, 0 to 3\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   mix     2 non-null      float64\n 1   int     4 non-null      int64  \ndtypes: float64(1), int64(1)\nmemory usage: 192.0 bytes\n```\n\n2. The test set may not be label encoded. The label-encoded train set data will be used as a basis for removing low-variance features. The label-ecnoded data will not be used for model training or testing. All data will be one-hot\/dummy encoded. Dummy encoding will automatically ignore `np.nan` values. Preferably, all `NaN` or `None` equivalents will be dropped. For competition purposes, they will be imputed.\n\n### Reasoning Behind Preference for Dropping Over Imputing\nDropping is preferred since both imputing and dropping introduce unpredictable effects to the model. Imputing will allow data to be processed and learned from. However, the learned results may be skewed since it is unknwon whether the imputed values really reflect the target population's data.\n\nOn the other hand, dropping avoids unknown effects coming from potentially inaccurate imputation of values. The issue with dropping is that certain classes or variants of the target population may be ignored. In other words, the trained models may be missing data from sample points that had to be removed along with \"rows\" or sample points that have a missing metric.\n\nThe solution has to be found thru domain knowledge. If the imputation values are statistically prevalent or common among the population or the population's representative sample, imputing will be considered. Next, imputing or dropping will be decided based on model performance. Situational adaptations may be needed for this plan.","1b727b4d":"Data type of some columns was changed to \"str\", cell 27. As a result, feature removal process missed many columns with too many NA type data.\n\n- Replicated. Set categoricals to dtype str.\n- Displayed head and list of nulls. MasVnrArea 8, .head() returns Alley awith several np.nan values.\n- Set single line converting dtypes as comment.\n- Alley, PoolQC and other features dropped as appropriate. MasVnrArea still showing 8 null values.","78e7aba7":"### Removing Columns that Leak Future Information","5029e5f7":"## ** Situational Cleaning: Changing Date Columns to Count Years","a9031ea1":"When step to convert to string was removed, still threw error same as if categorical.\nStep to convert to string was added after dropping columns with null values, notebook ran up to model training and cross-validation. Run cancelled, this step is predicted to cause certain NA type categorical\/object columns to have NA values listed as \"nan\" or \"None\", evading imputation.","22280332":"## ** Fitting to all Available Features","3609d895":"# * Basic Data Formatting and Cleaning","b7278bda":"## ** Dealing With Columns That Have Many Null Values","90bb7080":"## ** Question: When imputing categoricals using most frequent value, should mode of train also be used to impute test set? On the other hand, is it more appropriate to impute each train and test data separately using each data set's mode?","0ac284c3":"# * Feature Selection and Engineering","0eadec73":"# * Function Definitions","8e73eca1":"## ** Statistical Significance Values","51345df2":"# Notes","98c35493":"### *** Getting List of Low-variance Features","c1d97fa7":"## ** Getting Statistical Significance Values of Randomly Shuffled Dataset (Null Hypothesis Surrogate)","dc3ee189":"### *** Kaggle Leaderboard Results","48719c42":"The main sources for the decision are two questions on Analytics Vidhya (slugs `missing-value-threshold` and `what-should-be-the-allowed-percentage-of-missing-vales`) and an article on Statistics Solutions (*Missing Values in Data*).\n\nThe two Analytics Vidhya articles stated that normally a column with five percent missing values should be imputed. Any hiigher and the column may be dropped. However, \"in practice\", it will depend wildly on two things. One is model performance on different data sets with different features removed. The other and seemingly more important one is \"significance\" or \"information\" contained in a feature. If a feature is very important or has plenty information, you should impute it.\n\nEqually siginificant, if you have a reliable way of guessing or anticipating values, such as in time series analysis where a certain column follows a pattern, you may also impute more values. The answer suggested using `pd.Series.interpolate`.\nHowever, since it would be time consuming to do significant research into the significance of each variable in house sale price, the standard \"by the book\" theoritical thresholds will be used to decide whether or not to impute a feature.","40b4d14a":"# * Overall Best Kaggle Entry\n\n\nYou advanced 770 places on the leaderboard!\n\nYour submission scored 0.15808, which is an improvement of your previous score of 0.20054. Great job!","02bc0b69":"# Ames Iowa Housing Price Prediction","7f071a35":"## ** Filtering Features Automatically thru VarianceThreshold()","bb15d789":"## ** TPOT","20131dc6":"# To-do","11236a9d":"### *** Creating \"Synthetic\" Dataset","70d33eb1":"## ** AutoKeras Model","f420cbef":"To function properly, one-hot encoding must be done before a train-test split. However, thei imputation and scaling of data was done on a train-test split before this cell. The split was made to avoid leaking data during cross-validated training on the split train data. The solution is to append the train and test set then split them again using a train-test splitter with the same `random_state` parameter.","73f1e555":"### *** Filtering Categoricals Through Explained Variance Based on PCA Results","c2662915":"=====\nBaseline Scores\n=====\n LinearRegression\n\n-----\n train\n: mean_score :\n0    32440.607\ndtype: object\n\n*** End Section***\n: nanmean_score :\n0    32440.607\ndtype: object\n\n*** End Section***\n: std :\n0    25879.877\ndtype: object\n\n*** End Section***\n: var :\n0    669768019.919\ndtype: object\n\n*** End Section***\n\n-----\n test\n: mean_score :\n0    36856.167\ndtype: object\n\n*** End Section***\n: nanmean_score :\n0    36856.167\ndtype: object\n\n*** End Section***\n: std :\n0    30647.640\ndtype: object\n\n*** End Section***\n: var :\n0    939277856.595\ndtype: object\n\n*** End Section***\n\n=== End of Results ===\n\n\n=====\n Start \n=====\n RandomForestRegressor\n\n-----\n train\n: mean_score :\n0    26985.911\ndtype: object\n\n*** End Section***\n: nanmean_score :\n0    26985.911\ndtype: object\n\n*** End Section***\n: std :\n0    13300.744\ndtype: object\n\n*** End Section***\n: var :\n0    176909798.043\ndtype: object\n\n*** End Section***\n\n-----\n test\n: mean_score :\n0    31196.539\ndtype: object\n\n*** End Section***\n: nanmean_score :\n0    31196.539\ndtype: object\n\n*** End Section***\n: std :\n0    20722.367\ndtype: object\n\n*** End Section***\n: var :\n0    429416485.560\ndtype: object\n\n*** End Section***\n\n=== End of Results ===\n\n\n=====\n Start \n=====\n MLPRegressor\n\n-----\n train\n: mean_score :\n0    37804.388\ndtype: object\n\n*** End Section***\n: nanmean_score :\n0    37804.388\ndtype: object\n\n*** End Section***\n: std :\n0    23975.602\ndtype: object\n\n*** End Section***\n: var :\n0    574829492.030\ndtype: object\n\n*** End Section***\n\n-----\n test\n: mean_score :\n0    41111.603\ndtype: object\n\n*** End Section***\n: nanmean_score :\n0    41111.603\ndtype: object\n\n*** End Section***\n: std :\n0    28282.070\ndtype: object\n\n*** End Section***\n: var :\n0    799875487.929\ndtype: object\n\n*** End Section***\n\n=== End of Results ===","245943c1":"## ** Simplified Imputation of Categoricals Features","222d9e64":"### *** Creating Separate Data Frame of Categorical Objects\n\nThe separate dataframe was created to avoid dropping rows. Removing `np.nan` values is done as part of the process to get the mode and then impute categoricals columns.","d6146d0d":"## When all columns with variance of less than 0.05 were manually removed, these were the results.\nExtremely strange is the ridiculously low score on the test sets for numeric data only inspite of every other set, including numeric data only TRAIN set, having a very high error.","c1dca775":"# * Dropping Features with Failing Statistical Analysis Results","08575085":"### *** Function Definition","649fb1b0":"2020-10-07\n\n=====\n Start \n=====\n LinearRegression\n\n-----\n df_train\n: mean_score :\n0    40349.761\ndtype: object\n\n*** End Section***\n: nanmean_score :\n0    40349.761\ndtype: object\n\n*** End Section***\n: std :\n0    21146.926\ndtype: object\n\n*** End Section***\n: var :\n0    447192475.980\ndtype: object\n\n*** End Section***\n\n-----\n df_test\n: mean_score :\n0    1340694813348.979\ndtype: object\n\n*** End Section***\n: nanmean_score :\n0    1340694813348.979\ndtype: object\n\n*** End Section***\n: std :\n0    2967445419538.622\ndtype: object\n\n*** End Section***\n: var :\n0    8805732317940747073486848.000\ndtype: object\n\n*** End Section***\n\n=== End of Results ===\n\n\n=====\n Start \n=====\n RandomForestRegressor\n\n-----\n df_train\n: mean_score :\n0    41518.478\ndtype: object\n\n*** End Section***\n: nanmean_score :\n0    41518.478\ndtype: object\n\n*** End Section***\n: std :\n0    22768.769\ndtype: object\n\n*** End Section***\n: var :\n0    518416833.504\ndtype: object\n\n*** End Section***\n\n-----\n df_test\n: mean_score :\n0    43467.585\ndtype: object\n\n*** End Section***\n: nanmean_score :\n0    43467.585\ndtype: object\n\n*** End Section***\n: std :\n0    21897.547\ndtype: object\n\n*** End Section***\n: var :\n0    479502557.408\ndtype: object\n\n*** End Section***\n\n=== End of Results ===\n\n\n=====\n Start \n=====\n MLPRegressor\n\n-----\n df_train\n: mean_score :\n0    49293.037\ndtype: object\n\n*** End Section***\n: nanmean_score :\n0    49293.037\ndtype: object\n\n*** End Section***\n: std :\n0    23606.500\ndtype: object\n\n*** End Section***\n: var :\n0    557266850.984\ndtype: object\n\n*** End Section***\n\n-----\n df_test\n: mean_score :\n0    103114.099\ndtype: object\n\n*** End Section***\n: nanmean_score :\n0    103114.099\ndtype: object\n\n*** End Section***\n: std :\n0    44653.811\ndtype: object\n\n*** End Section***\n: var :\n0    1993962871.255\ndtype: object\n\n*** End Section***\n\n=== End of Results ===","4011c3dc":"- 2020-09-16\n- 2020-10-06\n\nFilter categorical features by statistics.\nDone, filter through PCA. 20 days after objective.","fba6ff50":"## ** Dropping Features with 5% or More Missing Values","78c54c86":"### *** TPOT Baseline Results on Stat-cleaned Data\n\nTPOT Baseline RidgeCV(RobustScaler(input_matrix))-2020-10-13 14_51_26.085753.csv\n\n0 seconds\n0 seconds\n0.20517","5327eb94":"## ** Recursive Stratified Cross-validation on Train and Test Datasets","2120f777":"### *** Checking Index Alignment of List of Failing Features","813ee407":"All the features of a particular row will be shuffled twice. First, the order of values in a row will be randomized. After that, the columns will be shuffled. The `mutual_info_regression` and `f_regression` values for the shuffled datasets will be used as minimum limits for the statistical values. The statistical values of the randomly shuffled data set will be *surrogates* for the values of completely unrelated\/independent features.\n\n1. The dataframe values will be accessed as an n-dimensional array through the `values` method. They will be shuffled along the x-axis, or axis 1.\n2. After the intitial row-only shuffle, the columns will be shuffled as well.\n3. A new dtaframe will be created from the shuffled values. The same column names will be passed in as the parameter to `columns`. The individual cells\/values in the new dataframe's columns may or may not be properly aligned.","4d8bbb05":"## ** Converting Columns to Appropriate Data Types","7e2c29df":"# Done","78bd25d1":"- Unknown Start\n- 2020-09-16\n\nGo online, find ways to do variance filtering after splitting into train-test sets. The filtering was done before splitting because getting dummies was also done before splitting. If the varaince filtering were done on the one-hot-encoded data, many dummy columns could be removed. Further, the variance of a column is determined by how varied it is while not one-hot encoded.","09625ab9":"# * Setting Baseline RMSE Scores","05a38663":"### *** Which should be done first: hyper-parameter optimization or recursive feature elimination using RFECV?\nI assume that getting correctly processed data is more important than getting the best hyper-parameters. However, would changing the hyper-parameters of a model significantly change the optimal features selected by RFECV? If so, is that change good? On the other hand, assume that changing the data affects the optimum hyper-parameters. Is the change in hyper-parameters caused by different data useful or beneficial? In other words, is it better to tune your hyper-parameters before running automated recursive feature elimination? Is it better to run RFECV before hyper-parameter optimization using Grid Search or Randomized Search? And last but not least, do I really need to dive deep into documentation and research to find out the answer for each specific use case or data set?","7a2b3269":"# Questions","97dc23c8":"TPOT Regressor with highly increased population and generations gave marginal improvement and was lower than best manually made submission.\n\n```\ntpot = TPOTRegressor(generations = 8, population_size = 64, verbosity = 2, random_state = 88)\nScore:\n0.20517\n\ntpot = TPOTRegressor(generations = 16, population_size = 128, verbosity = 2, random_state = 88)\nScore:\n0.20423\n```","08937f7f":"## ** HyperOpt-SKLearn with XGBoost**","843ec739":"## ** One-hot Encoding of Categorical Features","30afa667":"# * Train-test Validation of Data cleaned with SelectKbest Scores","eaf72d00":"## ** Combining Lists of Statistical Score Failers","d7347522":"## ** Automated Imputation of True Numeric Features","ba3d69b5":"## ** Question: Is it acceptable to calcualte statistical significance of features using SelectKBest before train-test splitting? What about imputation? Should imputation be before or after splitting? Should it be done with the imputation value calculated separately for each data set?","a475b110":"### *** Finding List of Failing Feature Based on Synthetic Sparse Uncorrelated Data","0be7b20f":"The `alpha` parameter in `SelectFwe` is the arbitrarily selected significance value. The significance value has several descriptions.\n1. The significance value or alpha value is the probability of wrongfully rejecting the null hypothesis of an experiment. The null hypothesis states that the independent variables do not cause significant change in the dependent variable. Wrongfully rejecting the null hypothesis is also called a \"Type 1 Error\".\n2. The alpha value is the probability of finding \"observations\" just as extreme as the observations in your data set assuming that the null hypothesis is true. The **lower the probability** of finding similar or more extreme observations **given that the null hypothesis is true**, the lower the chances of wrongfully rejecting the null hypothesis.","75a9132b":"## ** Getting Statistical Analysis of \"Sparse Uncorrelated\" Dataset","6781a2fd":"Another version of this notebook that focuses on AutoML is being developed:\n[Ames Iowa Housing with AutoML](https:\/\/www.kaggle.com\/joachimrives\/ames-iowa-housing-with-automl)\n\nKaggle URLfor new notebook: https:\/\/www.kaggle.com\/joachimrives\/ames-iowa-housing-with-automl","02abaabe":"## ** Train-test Split Before Imputation and Scaling","55455eae":"## ** Selecting Features by P-value","944bee51":"### *** Experiment: Setting Data Type as Category\n\nExperiment goal is to determine if setting dtype of feature as category also prevents `get_incomplete_failures` from dropping columns. Change will not be kept, might have issue with categorical feature imputation steps that accept features as string and were working when features were str dtype.\n\n- Columns still removed as if dtype were not forced into str.\n- Verification. Reverting to old code with forced conversion to str. The np.nan values were kept, columns not removed.\n\nAdditional Info:\nRan whole working set except columns were set as category at the beginning. Error on categorical value imputation at call to `impute_categorical`:\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n\/opt\/conda\/lib\/python3.7\/site-packages\/sklearn\/preprocessing\/_label.py in _encode_python(values, uniques, encode)\n     65         try:\n---> 66             encoded = np.array([table[v] for v in values])\n     67         except KeyError as e:\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sklearn\/preprocessing\/_label.py in <listcomp>(.0)\n     65         try:\n---> 66             encoded = np.array([table[v] for v in values])\n     67         except KeyError as e:\n\nKeyError: 924\n\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\n<ipython-input-207-3d25baeb8aa0> in <module>\n      1 dict_classes = {}\n      2 for feature in lst_categoricals:\n----> 3     impute_results = impute_categorical(df_train[feature])\n      4     df_train[feature] = impute_results[\"data\"]\n      5 \n\n<ipython-input-205-cccc1584e7ed> in impute_categorical(sr)\n     11     sr_string_version[bmask_isnull] = mode\n     12 \n---> 13     ndarr_clean = label_encoder.transform(sr_string_version)\n     14     ndarr_clean = label_encoder.inverse_transform(encoded_data)\n     15","50f7aa0c":"### *** TPOT Baseline Lifted from MachineLearningMastery.com\n\nURL: https:\/\/machinelearningmastery.com\/tpot-for-automated-machine-learning-in-python\/","64bd5cff":"### *** TPOT Lifted from Official Documentation\n\nThe code below is intended for an extremely similar data set: the built-in Boston Housing Prices dta set.\nSource page: https:\/\/epistasislab.github.io\/tpot\/examples\/#boston-housing-prices-modeling","b12b6e1b":"### *** Getting KBest Scores of Synthetic Dataset","3b705289":"### Perform Variance Threshold Elimination Before Getting Dummies\/One-hot Encoding\nIt is not correct to remove only certain dummy-columns. The result will be unpredictable and will depend heavily on chance.\nThe variance of one single feature before being one-hot encoded should be used to decide whether to remove it or not.","7fcd7e69":"### *** Scaling of Feature Values","7dce8dfa":"## ** Installing General Prerequisites for Auto-ML","da6908f6":"## ** Manually Filtering by Variance Threshold","339e4ebb":"## ** Imputing Categorical Features"}}