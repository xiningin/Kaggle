{"cell_type":{"be3b86d0":"code","2167e475":"code","4b91d446":"code","1691a88c":"code","86248b56":"code","6bdc1441":"code","64515f0f":"code","9bc9be0a":"code","6986a450":"code","d807e05a":"markdown"},"source":{"be3b86d0":"import numpy as np\nimport pandas as pd\nimport torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport ast\nimport sys\neffdet_path = \"..\/input\/effdet\"\nsys.path.append(effdet_path)\ntimm_path = \"..\/input\/timm-pytorch-image-models\/pytorch-image-models-master\"\nsys.path.append(timm_path)\nimport timm\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn\nimport os\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nomega_path = \"..\/input\/omegaconf\"\nsys.path.append(omega_path)\nfrom omegaconf import OmegaConf\nimport glob\nimport sklearn\nimport math\nimport random\n\nimport cv2\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch import optim\nfrom torchvision import transforms\n\nfrom transformers import get_cosine_schedule_with_warmup\n\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn import metrics, model_selection, preprocessing\nfrom sklearn.model_selection import GroupKFold\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","2167e475":"!pip install --no-deps '..\/input\/pycocotools202\/pycocotools-2.0.2-cp37-cp37m-linux_x86_64.whl' > \/dev\/null","4b91d446":"IMAGE_SIZE = 768\nfrom effdet import create_model","1691a88c":"model = create_model(\"tf_efficientdet_d7\" , bench_task='predict' , num_classes= 1 , image_size=(IMAGE_SIZE,IMAGE_SIZE),pretrained=False,pretrained_backbone=False)\npath = \"..\/input\/efficientdet-d2-train-pytorch\/Fold 4 model with val loss 0.8070960265697625.pth\"\nmodel.load_state_dict(torch.load(path))\nmodel.eval()\nmodel.to(device)","86248b56":"test_transform = A.Compose([\n   A.Resize(IMAGE_SIZE,IMAGE_SIZE ,p = 1.0),\n            A.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n            ToTensorV2()\n])","6bdc1441":"def format_prediction_string(boxes, scores):\n    # Format as specified in the evaluation page\n    pred_strings = []\n    for j in zip(scores, boxes):\n        pred_strings.append(\"{0:.2f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n\n    return \" \".join(pred_strings)","64515f0f":"prediction_confidence_threshold = 0.15\ndef predict(model, pixel_array,prediction_confidence_threshold):\n    # Predictions for a single image\n    \n    # Apply all the transformations that are required\n    pixel_array = np.array(pixel_array, dtype=np.float32)\n    image = test_transform(image=pixel_array)['image'].unsqueeze(0)\n    \n    # Get predictions\n    #each row representing [x_min, y_min, x_max, y_max, score, class]\n    with torch.no_grad():\n        detections = model(image.to(device)) \n    \n    # Move predictions to cpu and numpy\n    \n    boxes = detections[0].detach().cpu().numpy()[:, :4]\n    scores = detections[0].detach().cpu().numpy()[:, 4]\n    indexes = np.where(scores > prediction_confidence_threshold)[0]\n    boxes = boxes[indexes]\n    scores =scores[indexes]\n    \n    boxes[:, 0] = (boxes[:, 0]*(1280\/IMAGE_SIZE)).astype(np.int32).clip(min=0, max=1280)\n    boxes[:, 1] = (boxes[:, 1]*(720\/IMAGE_SIZE)).astype(np.int32).clip(min=0, max=720)\n    boxes[:, 2] = (boxes[:, 2]*(1280\/IMAGE_SIZE)).astype(np.int32).clip(min=0, max=1280)\n    boxes[:, 3] = (boxes[:, 3]*(720\/IMAGE_SIZE)).astype(np.int32).clip(min=0, max=720)\n    \n    \n    # Go back from x_min, y_min, x_max, y_max to x_min, y_min, w, h\n    boxes[:, 2] = (boxes[:, 2] - boxes[:, 0])\n    boxes[:, 3] = (boxes[:, 3] - boxes[:, 1])\n    \n    \n  \n    # Format results as requested in the Evaluation tab\n    return format_prediction_string(boxes, scores)","9bc9be0a":"import greatbarrierreef\nenv = greatbarrierreef.make_env()\niter_test = env.iter_test() ","6986a450":"for (pixel_array, df_pred) in iter_test:  # iterate through all test set images\n    df_pred['annotations'] = predict(model, pixel_array,prediction_confidence_threshold)\n    env.predict(df_pred)","d807e05a":"## Reference https:\/\/www.kaggle.com\/mrinath\/effdet-infer-pytorch\/notebook#Model"}}