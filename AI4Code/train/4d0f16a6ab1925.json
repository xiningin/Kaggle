{"cell_type":{"0b5c011e":"code","130a64f1":"code","f8eefaff":"code","eb302680":"code","5769c195":"code","fc32a35e":"code","0d1abd7b":"code","fa7a7540":"code","456ae2b5":"code","94e1e233":"code","52b134da":"code","8db86d72":"code","5784e9f7":"code","67ae8ab3":"code","e4419e39":"code","d9c46920":"code","3fb4ce6d":"code","ea3305fa":"code","c0b97266":"code","459d4ea9":"code","67a209ad":"code","892841bf":"code","20989da8":"code","80bf432e":"code","63504890":"code","8a90a3cb":"code","27c047f7":"code","bcf4784b":"code","307dea5c":"code","ad1c842d":"code","58208736":"code","ddbd1c2e":"code","d027c69f":"code","4a58761a":"code","0992b9d4":"code","fe0f873a":"code","04673cd7":"code","cdb04a17":"code","3b053f3d":"code","b5309308":"code","db522e77":"code","3d072e17":"code","472c1cba":"code","0aed01b9":"code","8323097e":"code","78cc491f":"code","6ad9942d":"code","0020159f":"code","7dd40a0c":"code","fcf5579e":"code","42dab0b6":"code","7ce84a01":"code","129a92ca":"code","f531909c":"code","4914fedf":"code","083d1143":"markdown","d58e148c":"markdown","ccd3f279":"markdown","5ccb781e":"markdown","9717cfa6":"markdown","a61600a9":"markdown","bff02dfd":"markdown","13a39ef7":"markdown","66f66af2":"markdown","ec6044f8":"markdown","3874c8ba":"markdown","c8d36a6e":"markdown","1851e034":"markdown","aca74253":"markdown","bf4ab71b":"markdown","466de3c2":"markdown"},"source":{"0b5c011e":"import os\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import scale\n\n#for dirname, _, filenames in os.walk('\/kaggle\/input\/'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))","130a64f1":"input_dir = 'google-cloud-ncaa-march-madness-2020-division-1-mens-tournament'\n\nseeds = pd.read_csv('..\/input\/{}\/MDataFiles_Stage1\/MNCAATourneySeeds.csv'.format(input_dir))\ntourney_results = pd.read_csv('..\/input\/{}\/MDataFiles_Stage1\/MNCAATourneyCompactResults.csv'.format(input_dir))\nregular_results = pd.read_csv('..\/input\/{}\/MDataFiles_Stage1\/MRegularSeasonCompactResults.csv'.format(input_dir))\nregular_results_deets = pd.read_csv('..\/input\/{}\/MDataFiles_Stage1\/MRegularSeasonDetailedResults.csv'.format(input_dir))\nteams = pd.read_csv('..\/input\/{}\/MDataFiles_Stage1\/MTeams.csv'.format(input_dir))\nkenpom = pd.read_csv('\/kaggle\/input\/kenpom-2020\/NCAA2020_Kenpom.csv')","f8eefaff":"def prepare_data(df):\n    dfswap = df[['Season', 'DayNum', 'LTeamID', 'LScore', 'WTeamID', 'WScore', 'WLoc', 'NumOT']]\n\n    dfswap.loc[df['WLoc'] == 'H', 'WLoc'] = 'A'\n    dfswap.loc[df['WLoc'] == 'A', 'WLoc'] = 'H'\n    df.columns.values[6] = 'location'\n    dfswap.columns.values[6] = 'location'         \n    df.columns = [x.replace('W','T1_').replace('L','T2_') for x in list(df.columns)]\n    dfswap.columns = [x.replace('L','T1_').replace('W','T2_') for x in list(dfswap.columns)]\n    output = pd.concat([df, dfswap]).sort_index().reset_index(drop=True)\n    \n    return output","eb302680":"tourney_results = prepare_data(tourney_results)\nregular_results = prepare_data(regular_results)","5769c195":"# convert to str, so the model would treat TeamID them as factors\nregular_results['T1_TeamID'] = regular_results['T1_TeamID'].astype(str)\nregular_results['T2_TeamID'] = regular_results['T2_TeamID'].astype(str)\n\n# make it a binary task\nregular_results['win'] = np.where(regular_results['T1_Score']>regular_results['T2_Score'], 1, 0)\n\ndef team_quality(season):\n    \"\"\"\n    Calculate team quality for each season seperately. \n    Team strength changes from season to season (students playing change!)\n    So pooling everything would be bad approach!\n    \"\"\"\n    formula = 'win~-1+T1_TeamID+T2_TeamID'\n    glm = sm.GLM.from_formula(formula=formula, \n                              data=regular_results.loc[regular_results.Season==season,:], \n                              family=sm.families.Binomial()).fit()\n    \n    # extracting parameters from glm\n    quality = pd.DataFrame(glm.params).reset_index()\n    quality.columns = ['TeamID','beta']\n    quality['Season'] = season\n    # taking exp due to binomial model being used\n    quality['quality'] = np.exp(quality['beta'])\n    # only interested in glm parameters with T1_, as T2_ should be mirroring T1_ ones\n    quality = quality.loc[quality.TeamID.str.contains('T1_')].reset_index(drop=True)\n    quality['TeamID'] = quality['TeamID'].apply(lambda x: x[10:14]).astype(int)\n    return quality","fc32a35e":"import time\nstart = time.time()\nteam_qual = pd.concat([team_quality(2010),\n                       team_quality(2011),\n                       team_quality(2012),\n                       team_quality(2013),\n                       team_quality(2014),\n                       team_quality(2015),\n                       team_quality(2016),\n                       team_quality(2017),\n                       team_quality(2018),\n                       team_quality(2019)]).reset_index(drop=True)\nend = time.time()\nprint(\"time elapsed:\",end - start)","0d1abd7b":"team_quality_T1 = team_qual[['TeamID','Season','quality']]\nteam_quality_T1.columns = ['T1_TeamID','Season','T1_quality']\nteam_quality_T2 = team_qual[['TeamID','Season','quality']]\nteam_quality_T2.columns = ['T2_TeamID','Season','T2_quality']\n\ntourney_results['T1_TeamID'] = tourney_results['T1_TeamID'].astype(int)\ntourney_results['T2_TeamID'] = tourney_results['T2_TeamID'].astype(int)\ntourney_results = tourney_results.merge(team_quality_T1, on = ['T1_TeamID','Season'], how = 'left')\ntourney_results = tourney_results.merge(team_quality_T2, on = ['T2_TeamID','Season'], how = 'left')","fa7a7540":"# we only have tourney results since year 2010\ntourney_results = tourney_results.loc[tourney_results['Season'] >= 2010].reset_index(drop=True)\n\n# not interested in pre-selection matches\ntourney_results = tourney_results.loc[tourney_results['DayNum'] >= 136].reset_index(drop=True)","456ae2b5":"seeds['seed'] = seeds['Seed'].apply(lambda x: int(x[1:3]))\nseeds['division'] = seeds['Seed'].apply(lambda x: x[0])\n\nseeds_T1 = seeds[['Season','TeamID','seed','division']].copy()\nseeds_T2 = seeds[['Season','TeamID','seed','division']].copy()\nseeds_T1.columns = ['Season','T1_TeamID','T1_seed','T1_division']\nseeds_T2.columns = ['Season','T2_TeamID','T2_seed','T2_division']\n\ntourney_results = tourney_results.merge(seeds_T1, on = ['Season', 'T1_TeamID'], how = 'left')\ntourney_results = tourney_results.merge(seeds_T2, on = ['Season', 'T2_TeamID'], how = 'left')","94e1e233":"tourney_results['T1_powerrank'] = tourney_results.groupby(['Season','T1_division'])['T1_quality'].rank(method='dense', ascending=False).astype(int)\ntourney_results['T2_powerrank'] = tourney_results.groupby(['Season','T2_division'])['T2_quality'].rank(method='dense', ascending=False).astype(int)","52b134da":"kpcols = list(kenpom.columns)\na, b = kpcols.index('Season'), kpcols.index('TeamName')\nkpcols[b], kpcols[a] = kpcols[a], kpcols[b]\nkenpom = kenpom[kpcols]\n\nkenpom_T1 = kenpom.copy()\nkenpom_T2 = kenpom.copy()\n\nkpT1cols = []; kpT2cols = [];\nkpT1cols.append('Season');kpT2cols.append('Season')\nfor k in kenpom.columns:\n    if k!='Season':\n        kpT1cols.append('T1_{}'.format(k))\n        kpT2cols.append('T2_{}'.format(k))\n    \nkenpom_T1.columns = kpT1cols\nkenpom_T2.columns = kpT2cols\n\ntourney_results = tourney_results.merge(kenpom_T1, on = ['Season', 'T1_TeamID'], how = 'left')\ntourney_results = tourney_results.merge(kenpom_T2, on = ['Season', 'T2_TeamID'], how = 'left')","8db86d72":"tourney_results.head()","5784e9f7":"def prepare_data_deets(df):\n    dfswap = df.copy()\n\n    dfswap.loc[df['WLoc'] == 'H', 'WLoc'] = 'A'\n    dfswap.loc[df['WLoc'] == 'A', 'WLoc'] = 'H'\n    df.columns.values[6] = 'location'\n    dfswap.columns.values[6] = 'location'         \n    df.columns = [x.replace('W','T1_').replace('L','T2_') for x in list(df.columns)]\n    dfswap.columns = [x.replace('L','T1_').replace('W','T2_') for x in list(dfswap.columns)]\n    output = pd.concat([df, dfswap]).sort_index().reset_index(drop=True)\n    \n    return output","67ae8ab3":"regular_results_deets = prepare_data_deets(regular_results_deets)","e4419e39":"regular_results_deets['T1_FGeff'] = regular_results_deets['T1_FGM']\/regular_results_deets['T1_FGA']\nregular_results_deets['T2_FGeff'] = regular_results_deets['T2_FGM']\/regular_results_deets['T2_FGA']\n\nregular_results_deets['T1_FG3eff'] = regular_results_deets['T1_FGM3']\/regular_results_deets['T1_FGA3']\nregular_results_deets['T2_FG3eff'] = regular_results_deets['T2_FGM3']\/regular_results_deets['T2_FGA3']","d9c46920":"T1_FGeffM_S = regular_results_deets.groupby(['T1_TeamID','Season']).agg({'T1_FGeff':['mean','std']})\nT2_FGeffM_S = regular_results_deets.groupby(['T2_TeamID','Season']).agg({'T2_FGeff':['mean','std']})\n\nT1_FG3effM_S = regular_results_deets.groupby(['T1_TeamID','Season']).agg({'T1_FG3eff':['mean','std']})\nT2_FG3effM_S = regular_results_deets.groupby(['T2_TeamID','Season']).agg({'T2_FG3eff':['mean','std']})","3fb4ce6d":"tourney_results = tourney_results.merge(T1_FGeffM_S, on = ['Season', 'T1_TeamID'], how = 'left')\ntourney_results = tourney_results.merge(T2_FGeffM_S, on = ['Season', 'T2_TeamID'], how = 'left')\ntourney_results = tourney_results.merge(T1_FG3effM_S, on = ['Season', 'T1_TeamID'], how = 'left')\ntourney_results = tourney_results.merge(T2_FG3effM_S, on = ['Season', 'T2_TeamID'], how = 'left')","ea3305fa":"tourney_results['win'] = np.where(tourney_results['T1_Score'] > tourney_results['T2_Score'], 1, 0)","c0b97266":"T1_conf_dum = pd.get_dummies(tourney_results['T1_conference'])\nT2_conf_dum = pd.get_dummies(tourney_results['T2_conference'])\n\nT1_conf_dum.columns = ['T1_{}'.format(i) for i in T1_conf_dum.columns]\nT2_conf_dum.columns = ['T2_{}'.format(i) for i in T2_conf_dum.columns]","459d4ea9":"feats_to_use = ['Season','T1_powerrank','T2_powerrank',\\\n                'T1_seed','T2_seed','T1_rank','T2_rank',\\\n                ('T1_FGeff', 'mean'),('T2_FGeff', 'mean'),\\\n                ('T1_FGeff', 'std'),('T2_FGeff', 'std'),\\\n                ('T1_FG3eff', 'mean'),('T2_FG3eff', 'mean'),\\\n                ('T1_FG3eff', 'std'),('T2_FG3eff', 'std')]\n\nmodel_df = tourney_results[feats_to_use+['win']]\n\nmodel_df = pd.concat([model_df.loc[:, model_df.columns != 'win'],T1_conf_dum,T2_conf_dum,model_df['win']],axis=1)","67a209ad":"model_df[['T1_powerrank','T2_powerrank','T1_seed','T2_seed','T1_rank','T2_rank']] = \\\n        scale(model_df[['T1_powerrank','T2_powerrank','T1_seed','T2_seed','T1_rank','T2_rank']])","892841bf":"from tqdm import tqdm_notebook\n\nscores_lr = []\nscores_rf = []\nscores_nn = []\nscores_ens = []\n\nfor i in tqdm_notebook(np.arange(2010,2020)):\n    X_train = model_df[model_df.Season!=i].iloc[:,1:-1]\n    y_train = model_df[model_df.Season!=i].iloc[:,-1]\n    X_test = model_df[model_df.Season==i].iloc[:,1:-1]\n    y_test = model_df[model_df.Season==i].iloc[:,-1]\n    \n    lr = LogisticRegression(random_state=4351)\n    lr.fit(X_train,y_train)\n\n    rf = RandomForestClassifier(n_estimators=1000,random_state=342)\n    rf.fit(X_train,y_train)\n    \n    nn = MLPClassifier(hidden_layer_sizes=(5,7,),random_state=222)\n    nn.fit(X_train,y_train)\n    \n    lr_yhat_prob = lr.predict_proba(X_test)[:,1]\n    lr_yhat = lr.predict(X_test)\n\n    rf_yhat_prob = rf.predict_proba(X_test)[:,1]\n    rf_yhat = rf.predict(X_test)\n    \n    nn_yhat_prob = nn.predict_proba(X_test)[:,1]\n    nn_yhat = nn.predict(X_test)\n    \n    ens_yhat_prob = 0.33*lr_yhat_prob + 0.33*rf_yhat_prob + 0.33*nn_yhat_prob\n    \n    scores_lr.append(log_loss(y_test.values,lr_yhat_prob))\n    scores_rf.append(log_loss(y_test.values,rf_yhat_prob))\n    scores_nn.append(log_loss(y_test.values,nn_yhat_prob))\n    scores_ens.append(log_loss(y_test.values,ens_yhat_prob))","20989da8":"#plt.figure(figsize=(10,15))\n#y_pos = np.arange(len(X_train.columns))\n#plt.barh(y_pos, lr.coef_[0])\n \n# Create names on the y-axis\n#plt.yticks(y_pos, X_train.columns)\n#plt.show()","80bf432e":"#plt.figure(figsize=(10,15))\n#y_pos = np.arange(len(X_train.columns))\n#plt.barh(y_pos, rf.feature_importances_)\n \n# Create names on the y-axis\n#plt.yticks(y_pos, X_train.columns)\n#plt.show()","63504890":"plt.figure(figsize=(15,8))\nplt.plot(np.arange(2010,2020),scores_lr,'b.')\nplt.plot(np.arange(2010,2020),scores_rf,'r.')\nplt.plot(np.arange(2010,2020),scores_nn,'g.')\nplt.plot(np.arange(2010,2020),scores_ens,'m.')","8a90a3cb":"print(np.mean(scores_lr),np.std(scores_lr))\nprint(np.mean(scores_rf),np.std(scores_rf))\nprint(np.mean(scores_nn),np.std(scores_nn))\nprint(np.mean(scores_ens),np.std(scores_ens))","27c047f7":"def concat_row(r):\n    if r['WTeamID'] < r['LTeamID']:\n        res = str(r['Season'])+\"_\"+str(r['WTeamID'])+\"_\"+str(r['LTeamID'])\n    else:\n        res = str(r['Season'])+\"_\"+str(r['LTeamID'])+\"_\"+str(r['WTeamID'])\n    return res\n\ndef delete_leaked_from_df_train(df_train, df_test):\n    df_train['Concats'] = df_train.apply(concat_row, axis=1)\n    df_train_duplicates = df_train[df_train['Concats'].isin(df_test['ID'].unique())]\n    df_train_idx = df_train_duplicates.index.values\n    df_train = df_train.drop(df_train_idx)\n    df_train = df_train.drop('Concats', axis=1)\n    \n    return df_train ","bcf4784b":"feats_to_use = ['Season','seed_diff','rank_diff','powerrank_diff']","307dea5c":"train_df = pd.read_csv('..\/input\/{}\/MDataFiles_Stage1\/MNCAATourneyCompactResults.csv'.format(input_dir))\ntest_df = pd.read_csv('..\/input\/{}\/MSampleSubmissionStage1_2020.csv'.format(input_dir))","ad1c842d":"train_df = delete_leaked_from_df_train(train_df, test_df)","58208736":"train_df = prepare_data(train_df)\n\ntrain_df['T1_TeamID'] = train_df['T1_TeamID'].astype(int)\ntrain_df['T2_TeamID'] = train_df['T2_TeamID'].astype(int)\ntrain_df = train_df.merge(team_quality_T1, on = ['T1_TeamID','Season'], how = 'left')\ntrain_df = train_df.merge(team_quality_T2, on = ['T2_TeamID','Season'], how = 'left')\n\n# we only have tourney results since year 2010\ntrain_df = train_df.loc[train_df['Season'] >= 2010].reset_index(drop=True)\n\n# not interested in pre-selection matches\ntrain_df = train_df.loc[train_df['DayNum'] >= 136].reset_index(drop=True)\n\ntrain_df = train_df.merge(seeds_T1, on = ['Season', 'T1_TeamID'], how = 'left')\ntrain_df = train_df.merge(seeds_T2, on = ['Season', 'T2_TeamID'], how = 'left')\n\ntrain_df['seed_diff'] = train_df['T1_seed']-train_df['T2_seed']\n\ntrain_df['T1_powerrank'] = train_df.groupby(['Season','T1_division'])['T1_quality'].rank(method='dense', ascending=False).astype(int)\ntrain_df['T2_powerrank'] = train_df.groupby(['Season','T2_division'])['T2_quality'].rank(method='dense', ascending=False).astype(int)\n\ntrain_df = train_df.merge(kenpom_T1, on = ['Season', 'T1_TeamID'], how = 'left')\ntrain_df = train_df.merge(kenpom_T2, on = ['Season', 'T2_TeamID'], how = 'left')\n\ntrain_df['rank_diff'] = train_df['T1_rank']-train_df['T2_rank']\ntrain_df['powerrank_diff'] = train_df['T1_powerrank']-train_df['T2_powerrank']\n\ntrain_df = train_df.merge(T1_FGeffM_S, on = ['Season', 'T1_TeamID'], how = 'left')\ntrain_df = train_df.merge(T2_FGeffM_S, on = ['Season', 'T2_TeamID'], how = 'left')\ntrain_df = train_df.merge(T1_FG3effM_S, on = ['Season', 'T1_TeamID'], how = 'left')\ntrain_df = train_df.merge(T2_FG3effM_S, on = ['Season', 'T2_TeamID'], how = 'left')\n\ntrain_df['win'] = np.where(train_df['T1_Score'] > train_df['T2_Score'], 1, 0)\n\nT1_conf_dum = pd.get_dummies(train_df['T1_conference'])\nT2_conf_dum = pd.get_dummies(train_df['T2_conference'])\n\nT1_conf_dum.columns = ['T1_{}'.format(i) for i in T1_conf_dum.columns]\nT2_conf_dum.columns = ['T2_{}'.format(i) for i in T2_conf_dum.columns]\n\ntrain_df[['T1_powerrank','T2_powerrank','T1_seed','T2_seed','T1_rank','T2_rank','seed_diff','rank_diff','powerrank_diff']] = \\\n            scale(train_df[['T1_powerrank','T2_powerrank','T1_seed','T2_seed','T1_rank','T2_rank','seed_diff','rank_diff','powerrank_diff']])\n\nmodel_df = pd.concat([model_df.loc[:, model_df.columns != 'win'],T1_conf_dum,T2_conf_dum,model_df['win']],axis=1)\n\nmodel_df = train_df[feats_to_use+['win']]","ddbd1c2e":"model_df.head()","d027c69f":"X_train = model_df.iloc[:,1:-1]\ny_train = model_df.iloc[:,-1]\n\nlr.fit(X_train,y_train)\nrf.fit(X_train,y_train)\nnn.fit(X_train,y_train)","4a58761a":"test_df['Season'] = test_df['ID'].map(lambda x: int(x[:4]))\ntest_df['T1_TeamID'] = test_df['ID'].map(lambda x: int(x[5:9]))\ntest_df['T2_TeamID'] = test_df['ID'].map(lambda x: int(x[10:14]))","0992b9d4":"test_dfT1 = test_df.merge(team_quality_T1, on = ['Season','T1_TeamID'], how = 'left')\ntest_dfT1 = test_dfT1.merge(seeds_T1, on = ['Season','T1_TeamID'], how = 'left')\ntest_dfT2 = test_df.merge(team_quality_T2, on = ['Season','T2_TeamID'], how = 'left')\ntest_dfT2 = test_dfT2.merge(seeds_T2, on = ['Season','T2_TeamID'], how = 'left')","fe0f873a":"test_dfT1['T1_powerrank'] = test_dfT1.groupby(['Season','T1_division'])['T1_quality'].rank(method='dense', ascending=False).astype(int)\ntest_dfT2['T2_powerrank'] = test_dfT2.groupby(['Season','T2_division'])['T2_quality'].rank(method='dense', ascending=False).astype(int)","04673cd7":"kenpom_T1['Season'] = kenpom_T1['Season'].astype('int64')\nkenpom_T2['Season'] = kenpom_T2['Season'].astype('int64')\n\ntest_dfT1 = test_dfT1.merge(kenpom_T1, on = ['Season', 'T1_TeamID'], how = 'left')\ntest_dfT2 = test_dfT2.merge(kenpom_T2, on = ['Season', 'T2_TeamID'], how = 'left')","cdb04a17":"test_df = pd.concat([test_dfT1[['Season','T1_TeamID','T1_powerrank','T1_seed','T1_rank']],test_dfT2[['T2_TeamID','T2_powerrank','T2_seed','T2_rank']]],axis=1)","3b053f3d":"test_dfFGeffM_ST1 = test_df.merge(T1_FGeffM_S, on = ['Season', 'T1_TeamID'], how = 'left')\ntest_dfFGeffM_ST2 = test_df.merge(T2_FGeffM_S, on = ['Season', 'T2_TeamID'], how = 'left')","b5309308":"test_dfFGeffM_ST1 = test_dfFGeffM_ST1.merge(T1_FG3effM_S, on = ['Season', 'T1_TeamID'], how = 'left')\ntest_dfFGeffM_ST2 = test_dfFGeffM_ST2.merge(T2_FG3effM_S, on = ['Season', 'T2_TeamID'], how = 'left')","db522e77":"test_df = pd.concat([test_dfFGeffM_ST1[['Season','T1_TeamID','T1_powerrank',\\\n                                        'T1_seed','T1_rank',('T1_FGeff','mean'),('T1_FGeff','std'),\\\n                                        ('T1_FG3eff','mean'),('T1_FG3eff','std')]],\\\n                     test_dfFGeffM_ST2[['T2_TeamID','T2_powerrank',\\\n                                        'T2_seed','T2_rank',('T2_FGeff','mean'),('T2_FGeff','std'),\\\n                                        ('T2_FG3eff','mean'),('T2_FG3eff','std')]]],axis=1)","3d072e17":"T1_conf_dum = pd.get_dummies(test_dfT1['T1_conference'])\nT2_conf_dum = pd.get_dummies(test_dfT2['T2_conference'])\n\nT1_conf_dum.columns = ['T1_{}'.format(i) for i in T1_conf_dum.columns]\nT2_conf_dum.columns = ['T2_{}'.format(i) for i in T2_conf_dum.columns]\n\n# No Pac 10 in this data\nT1_conf_dum.insert(loc=22,column='T1_P10',value = 0)\nT2_conf_dum.insert(loc=22,column='T2_P10',value = 0)","472c1cba":"test_df = pd.concat([test_df['Season'],test_df[['T1_powerrank','T2_powerrank',\\\n                                                'T1_seed','T2_seed',\\\n                                                'T1_rank','T2_rank',\\\n                                               ('T1_FGeff', 'mean'),('T1_FGeff', 'std'),\\\n                                               ('T2_FGeff', 'mean'),('T2_FGeff', 'std'),\\\n                                               ('T1_FG3eff', 'mean'),('T1_FG3eff', 'std'),\\\n                                               ('T2_FG3eff', 'mean'),('T2_FG3eff', 'std')]],\\\n                     T1_conf_dum,T2_conf_dum],axis=1)","0aed01b9":"test_df['seed_diff'] = test_df['T1_seed']-test_df['T2_seed']\ntest_df['rank_diff'] = test_df['T1_rank']-test_df['T2_rank']\ntest_df['powerrank_diff'] = test_df['T1_powerrank']-test_df['T2_powerrank']\n\ntest_df[['T1_powerrank','T2_powerrank','T1_seed','T2_seed','T1_rank','T2_rank','seed_diff','rank_diff','powerrank_diff']] = \\\n    scale(test_df[['T1_powerrank','T2_powerrank','T1_seed','T2_seed','T1_rank','T2_rank','seed_diff','rank_diff','powerrank_diff']])","8323097e":"test_df = test_df[feats_to_use]","78cc491f":"model_df.head()","6ad9942d":"test_df.head()","0020159f":"yhat = 0*nn.predict_proba(test_df.drop('Season', axis=1))+\\\n        0*rf.predict_proba(test_df.drop('Season', axis=1))+\\\n        1.0*lr.predict_proba(test_df.drop('Season', axis=1))","7dd40a0c":"borderlineY = test_df[(yhat[:,1]>0.5) & (yhat[:,1]<0.55)]\nborderlineY['V1'] = np.where(borderlineY['seed_diff']>0,1,0)\nborderlineY['V2'] = np.where(borderlineY['rank_diff']>0,1,0)\nborderlineY['V3'] = np.where(borderlineY['powerrank_diff']>0,1,0)\n\nborderlineY['votes'] = borderlineY['V1']+borderlineY['V2']+borderlineY['V3']\n\nborderlineN = test_df[(yhat[:,1]>0.5) & (yhat[:,1]<0.55)]\nborderlineN['V1'] = np.where(borderlineN['seed_diff']<0,1,0)\nborderlineN['V2'] = np.where(borderlineN['rank_diff']<0,1,0)\nborderlineN['V3'] = np.where(borderlineN['powerrank_diff']<0,1,0)\n\nborderlineN['votes'] = borderlineN['V1']+borderlineN['V2']+borderlineN['V3']","fcf5579e":"plt.hist(yhat[:,1])","42dab0b6":"yhat[borderlineY[borderlineY.votes==3].index,1]+=0.4\nyhat[borderlineY[borderlineY.votes==2].index,1]+=0.3\nyhat[borderlineY[borderlineY.votes==1].index,1]+=0.1\n\nyhat[borderlineN[borderlineN.votes==3].index,1]-=0.4\nyhat[borderlineN[borderlineN.votes==2].index,1]-=0.3\nyhat[borderlineN[borderlineN.votes==1].index,1]-=0.1","7ce84a01":"plt.hist(yhat[:,1])","129a92ca":"submit = pd.read_csv('..\/input\/{}\/MSampleSubmissionStage1_2020.csv'.format(input_dir))","f531909c":"submit['Pred'] = yhat[:,1]","4914fedf":"submit.to_csv('SampleSubmissionStage1_Latimer.csv',index=False)","083d1143":"# Submission Stage 1","d58e148c":"Scaling the continuous variables helps the ML algorithms converge.","ccd3f279":"We can also show feature importances from the Random Forest model.","5ccb781e":"@catadanna made this function to remove test samples from the training set\nhttps:\/\/www.kaggle.com\/catadanna\/delete-leaked-from-training-ncaam-ncaaw-stage1","9717cfa6":"Test","a61600a9":"Team power ranking idea and code is due to @raddar https:\/\/www.kaggle.com\/raddar\/team-power-rankings <br>\nKenPom data is from @paulorzp https:\/\/www.kaggle.com\/paulorzp\/kenpom-scraper-2020 <br>\n","bff02dfd":"We can examine the logistic regression coefficients for clues on relationships. A positive value means it correlated positively with a win for team 1, a negative value means that variable correlated negatively with a win for team 1.","13a39ef7":"# Model","66f66af2":"# Regular season stats","ec6044f8":"Train","3874c8ba":"As you can see, depending on which year you test, a different model performs better.","c8d36a6e":"Dummy variables for the conference","1851e034":"This is where the KenPom data gets applied","aca74253":"Here we convert quality to powerrank, by grouping quality by division so we get a number similar to seed.","bf4ab71b":"This is the team_quality feature, which is essentially a glm fit on the number of wins. Again, idea and code is from @raddar. This takes a while to run.","466de3c2":"Version 3 LB - 0.5917 <br>\nVersion 4 LB - 0.51550 (lr) <br>\nVersion 7 LB - 0.50762 (ensemble, equal weights) <br>\nVersion 12 LB (fixed leaks) - 0.54538 (ens, equal weights), 0.63224 (nn), 0.55683 (rf), 0.55865 (lr) <br>\nBaseline with no team stat features: lr - 0.54262, rf - 0.51148, nn - 0.53148, ens - 0.51415 <br>\nBaseline with only the seed diff: lr - 0.55109 <br>\nBaseline with seed_diff and rank_diff: lr - 0.53932 <br>\nBaseline with seed_diff, rank_diff, and powerrank_diff: lr = 0.53929"}}