{"cell_type":{"782abc48":"code","80a46874":"code","1219f427":"code","34791a7f":"code","8f64f3b8":"code","018bd615":"code","93a857d9":"code","42507fae":"code","08b0598b":"code","453c7492":"code","4fbf8433":"code","29d1ef98":"code","ba908dfa":"code","17f021e9":"code","b83c4385":"code","489c53cb":"code","5d2029ac":"code","0bae81ed":"code","1350e77e":"code","a1c0e1ad":"code","3af54f80":"code","e39add27":"code","2aed0811":"code","ae1df474":"code","6ccf5b50":"code","3c5a25bc":"code","be887ebf":"code","a56d90e0":"code","92f07f1f":"code","537721e9":"code","176fda21":"code","287f8106":"code","372f7904":"code","2afdf040":"code","2115e3c1":"code","8c01c2ff":"code","b09c46ee":"code","4a20d58b":"code","cef33fea":"code","dbc9f56d":"code","f9f4ceec":"code","df249f6f":"code","9dc8c7ba":"code","aba1d169":"code","24938366":"code","50afc260":"code","7527f8a3":"code","5b09300c":"code","acbf611f":"code","fa1298f3":"code","758ef87b":"code","3735d5bc":"code","dfa4002b":"code","5d0fa039":"code","fe5879a0":"code","8ae6a6d0":"code","2a920f5e":"code","5aa1bb91":"code","24598856":"code","77237e1d":"code","38e5d982":"code","590cf488":"code","68ed6930":"code","52d9c7cb":"code","afc467c4":"code","f56e06e8":"code","fbb9931d":"code","44fa4169":"code","a67bf089":"code","fd68e70d":"code","cac9d137":"code","26d75332":"code","e80eea76":"code","41aa0d26":"code","6ee7fbf1":"code","37f9e540":"code","73f0f7bf":"code","a80b0093":"code","264cbe0b":"code","bff101bf":"code","d31bda0a":"code","a24120ff":"code","9ce4f187":"code","baf7be29":"code","1c08042b":"code","fb672bd9":"code","21f98082":"code","26ece63f":"code","1c0b02ae":"code","fb964b0f":"code","35b07736":"code","14a720e1":"code","d0bc41a8":"code","2a602a6e":"code","4ad721ea":"code","53893d3e":"code","2664d99f":"code","0971bfd2":"code","66a26688":"code","97b873fb":"code","f69c26db":"code","30b56596":"code","f8b3a9ee":"code","6ace2c8e":"code","cae15005":"code","c0e1b2c6":"code","b702c03c":"code","b7b980e1":"code","6d57c805":"code","879104e3":"code","bc3251e3":"code","f5d335e9":"code","a58938bd":"code","c8e28cef":"code","7c5466e7":"markdown","deb65a7d":"markdown","e27bd0bc":"markdown","5e62f5da":"markdown","ca51a276":"markdown","9dc5baff":"markdown","67a4ea69":"markdown","922dd770":"markdown","cc7046a0":"markdown","811980d0":"markdown","83b835c5":"markdown","0a218077":"markdown","bfdc021b":"markdown","ccffbd4a":"markdown","5417c592":"markdown","6694d2da":"markdown","a492b31a":"markdown","c5e233f1":"markdown","a7ec8ba3":"markdown","4c624217":"markdown","7741c468":"markdown","f2533cfc":"markdown","e6c12a88":"markdown","10345bdf":"markdown","630d495c":"markdown","d288f9c4":"markdown","7a052e25":"markdown","3958a3ad":"markdown","5c6f893d":"markdown","b18d75be":"markdown","05cac12b":"markdown","76975dea":"markdown"},"source":{"782abc48":"import sklearn\nsklearn.__version__","80a46874":"import os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","1219f427":"from sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.metrics import plot_confusion_matrix, accuracy_score\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder, OneHotEncoder","34791a7f":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_selection import VarianceThreshold, SelectFromModel, RFECV\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.feature_selection import mutual_info_regression, mutual_info_classif\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics","8f64f3b8":"# some imports for my accomplishments\n\n# standard imports\nimport re\nimport time\nimport random\nimport string\n\n# interactive plots\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom plotly import figure_factory as FF\n# pd.options.plotting.backend = \"plotly\"\n\n# progress bars\nfrom tqdm import tqdm\ntqdm.pandas()\n\n# feature importance\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom yellowbrick.model_selection import FeatureImportances\n\n# logging\nimport logging\nlogging.basicConfig()\nlogging.getLogger().setLevel(logging.INFO)","018bd615":"SEED = 42","93a857d9":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n\n\nseed_everything()","42507fae":"def fit_plot_confusion(clf, X_train, y_train, X_test, y_test):\n    clf.fit(X_train, y_train % 2)\n\n    mean, std = clf.cv_results_['mean_test_score'][clf.best_index_], \\\n                clf.cv_results_['std_test_score'][clf.best_index_]\n\n    logging.info(clf.best_params_)\n\n    disp = metrics.plot_confusion_matrix(clf, X_test, y_test % 2, normalize='true')\n    disp.figure_.suptitle(\"Confusion Matrix\")\n    plt.show()\n    \n    return clf.best_estimator_, {\"mean\": mean, \"std\": std}","08b0598b":"# write func for creating submission file\ndef write_to_submission_file(predicted_labels, out_file, train_num=891,\n                    target='Survived', index_label=\"PassengerId\"):\n    # turn predictions into data frame and save as csv file\n    predicted_df = pd.DataFrame(predicted_labels,\n                                index = np.arange(train_num + 1,\n                                                  train_num + 1 +\n                                                  predicted_labels.shape[0]),\n                                columns=[target])\n    predicted_df.to_csv(out_file, index_label=index_label)","453c7492":"# PATH = \".\/data\/\"\nPATH = \"..\/input\/\"","4fbf8433":"test_data = pd.read_csv(os.path.join(PATH, 'titanic', 'test.csv')).set_index('PassengerId')","29d1ef98":"data = pd.read_csv(os.path.join(PATH, 'titanic', 'train.csv')).set_index('PassengerId')","ba908dfa":"data.head()","17f021e9":"## your code\ndata.describe(include=['object'])","b83c4385":"data['Sex'].value_counts()","489c53cb":"fig = px.histogram(data, x=\"Sex\", title=\"Distribution of passengers across gender\")\nfig.show()","5d2029ac":"data['Survived'].value_counts()","0bae81ed":"data[\"Survived_cat\"] = [\"Survived\" if i == 1 else \"Not Survived\" for i in data[\"Survived\"]]","1350e77e":"fig = px.histogram(data, x=\"Survived_cat\", title=\"Survived passengers (target feature)\")\nfig.show()","a1c0e1ad":"fig = make_subplots(rows=1, cols=2, specs=[[{\"type\": \"pie\"}, {\"type\": \"pie\"}]])\n\n\n\nfig.add_trace(\n            go.Pie(labels=data.loc[data['Sex'] == 'male']['Survived_cat'], pull = [.1, .1],\n                   title = 'Male vs. Survived'), row=1, col=1)\n\nfig.add_trace(\n            go.Pie(labels=data.loc[data['Sex'] == 'female']['Survived_cat'], pull = [.07, .07],\n                   title = 'Female vs. Survived'), row=1, col=2)\n\n\nfig.update_layout(height=500, width=800, title_text=\"Gene Expression Features\")\nfig.show()","3af54f80":"survived_or_not_male = [0, 0]\nsurvived_or_not_female = [0, 0]\n\nfor i in data[data[\"Survived_cat\"] != \"Survived\"][\"Sex\"]:\n    if i == \"male\":\n        survived_or_not_male[0] += 1\n    elif i == \"female\":\n        survived_or_not_female[0] += 1\n\nfor i in data[data[\"Survived_cat\"] == \"Survived\"][\"Sex\"]:\n    if i == \"male\":\n        survived_or_not_male[1] += 1\n    elif i == \"female\":\n        survived_or_not_female[1] += 1","e39add27":"survived_sex = pd.DataFrame(\n    data=[\n        [survived_or_not_male[0], survived_or_not_female[0]],\n        [survived_or_not_male[1], survived_or_not_female[1]]\n    ],\n    columns=['male','female'],\n    index=['Not Survived', 'Survived']\n)\n\nsurvived_sex","2aed0811":"## your code\nfig = px.histogram(\n    data, \n    x='Age', \n    nbins=79, \n    marginal='box',\n    title='Passengers age distribution'\n)\n\nfig.show()","ae1df474":"data['Age'].describe()","6ccf5b50":"avg_age_survived = data[data[\"Survived_cat\"] == \"Survived\"][\"Age\"].mean()\nmedian_age_survived = data[data[\"Survived_cat\"] == \"Survived\"][\"Age\"].median()\navg_age_deceased = data[data[\"Survived_cat\"] != \"Survived\"][\"Age\"].mean()\nmedian_age_deceased = data[data[\"Survived_cat\"] != \"Survived\"][\"Age\"].median()","3c5a25bc":"print(f\"Average age of survived passengers: {avg_age_survived}\")\nprint(f\"Median age of survived passengers: {median_age_survived}\")\nprint(f\"Average age of deceased passengers: {avg_age_deceased}\")\nprint(f\"Median age of deceased passengers: {median_age_deceased}\")","be887ebf":"fig = px.histogram(\n    data[data[\"Survived_cat\"] == \"Survived\"], \n    x='Age', \n    nbins=79, \n    marginal='violin',\n    title='Age distributions for survived passengers'\n)\n\nfig.show()","a56d90e0":"fig = px.histogram(\n    data[data[\"Survived_cat\"] != \"Survived\"], \n    x='Age', \n    nbins=80, \n    marginal='violin',\n    title='Age distributions for deceased passengers'\n)\n\nfig.show()","92f07f1f":"x0 = data[data[\"Survived_cat\"] == \"Survived\"][\"Age\"]\nx1 = data[data[\"Survived_cat\"] != \"Survived\"][\"Age\"]\n\nfig = go.Figure()\nfig.add_trace(go.Histogram(x=x0, name=\"Survived\"))\nfig.add_trace(go.Histogram(x=x1, name=\"NOT survived\"))\n\nfig.update_layout(\n    barmode='stack', \n    title='Age distributions differ for survived and deceased passenger'\n)\nfig.show()","537721e9":"## your code","176fda21":"Pclass1 = data[\"Survived\"][data[\"Pclass\"] == 1].value_counts(normalize = True)\nPclass2 = data[\"Survived\"][data[\"Pclass\"] == 2].value_counts(normalize = True)\nPclass3 = data[\"Survived\"][data[\"Pclass\"] == 3].value_counts(normalize = True)\n\nx0 = ['Pclass 1', 'Pclass 2', 'Pclass 3']\ny0 = [Pclass1[1], Pclass2[1], Pclass3[1]]\n\npclass_data = [go.Bar(\n    x=x0,\n    y=y0\n)]\nlayout = go.Layout(\n    yaxis = dict(title = 'Survival Rates'),\n    title = 'Survival by Pclass'\n)\nfig = go.Figure(data=pclass_data, layout=layout)\n\nfig.show()","287f8106":"fig = px.density_contour(\n    data, \n    x=\"Pclass\", \n    y=\"Survived_cat\",\n    height=700, \n    width=700\n)\nfig.update_layout(\n    title='Density contours. Survived for Pclass'\n)\n\nfig.show()","372f7904":"S = data[\"Survived\"][data[\"Embarked\"] == \"S\"].value_counts(normalize = True)\nC = data[\"Survived\"][data[\"Embarked\"] == \"C\"].value_counts(normalize = True)\nQ = data[\"Survived\"][data[\"Embarked\"] == \"Q\"].value_counts(normalize = True)\n\nx0 = ['Southampton', 'Cherbourg', 'Queenstown']\ny0 = [S[1], C[1], Q[1]]\n\nemb_data = [go.Bar(\n    x=x0,\n    y=y0\n)]\nlayout = go.Layout(\n    yaxis = dict(title = 'Survival Rates'),\n    title = 'Survival by Port of Embarkation'\n)\nfig = go.Figure(data=emb_data, layout = layout)\n\nfig.show()","2afdf040":"fig = px.density_contour(\n    data, \n    x=\"Embarked\", \n    y=\"Survived_cat\",\n    height=700, \n    width=700\n)\nfig.update_layout(\n    title='Density contours. Survived for Embarked'\n)\n\nfig.show()","2115e3c1":"fig = px.density_contour(\n    data, \n    x=\"Pclass\", \n    y=\"Embarked\", \n    color='Survived_cat',\n    height=800, \n    width=800\n)\nfig.update_traces(\n    contours_coloring=\"fill\", \n    contours_showlabels=True\n)\nfig.update_layout(\n    title='Density contours. Survived for Embarked vs. Pclass',\n    legend=dict(\n        x=0,\n        y=.5,\n        traceorder=\"normal\",\n        font=dict(\n            family=\"sans-serif\",\n            size=12,\n            color=\"black\"\n        ),\n    )\n)\n\nfig.show()","8c01c2ff":"fig = px.density_heatmap(\n    data, \n    x=\"Embarked\", \n    y=\"Pclass\",\n    height=700, \n    width=700\n)\nfig.update_layout(\n    title='Heatmap'\n)\n\nfig.show()","b09c46ee":"fig = px.scatter_3d(\n    data, \n    x='Pclass', \n    y='Embarked', \n    z='Age',\n    color='Survived',\n    symbol='Sex'\n)\nfig.update_layout(\n    title='3D scatter',\n    legend=dict(\n        x=0,\n        y=.5,\n        traceorder=\"normal\",\n        font=dict(\n            family=\"sans-serif\",\n            size=12,\n            color=\"black\"\n        ),\n    )\n)\n\nfig.show()","4a20d58b":"## your code","cef33fea":"def check_missing(df):\n    df_miss_numerbers = df.isna().sum()\n    df_miss_percents = data.isna().mean().round(4) * 100\n    miss_df=pd.concat([df_miss_numerbers, df_miss_percents], axis=1)\n    miss_df.rename(columns={0: 'numbers',  1: 'percent'}, inplace=True)\n    miss_df = miss_df[miss_df.numbers > 0].sort_values(by=['percent'], ascending=False)\n    display(miss_df.style.background_gradient())\n    miss_df.plot.bar();\n#     return miss_df","dbc9f56d":"check_missing(data)","f9f4ceec":"check_missing(test_data)","df249f6f":"# almost 80% of missing values is very very bad - drop Cabin feature\n# data = data.drop(columns=['Cabin'])\n# test_data = test_data.drop(columns=['Cabin'])\n\n# or not?\n# for dataset in [data, test_data]:\n#     dataset['Cabin'] = dataset['Cabin'].fillna('Unknown')\n\ndeck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\n\nfor dataset in [data, test_data]:\n    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n    dataset['Deck'] = dataset['Deck'].map(deck)\n    dataset['Deck'] = dataset['Deck'].fillna(0)\n    dataset['Deck'] = dataset['Deck'].astype(int)\n    \n# drop Cabin feature\ndata = data.drop(columns=['Cabin'])\ntest_data = test_data.drop(columns=['Cabin'])","9dc8c7ba":"# it's time to work with missing values in Age feature\nage_before_imputation = data['Age'].dropna()\n\n# Impute the missing value with the median\nfor dataset in [data, test_data]:\n    dataset['Age'] = dataset['Age'].fillna(dataset['Age'].median())\n\nhist_data = [age_before_imputation, data['Age']]\n\ngroup_labels = ['Before imputation', 'After imputation']\ncolors = ['#333F44', '#37AA9C']\n\n# Create distplot\nfig = FF.create_distplot(\n    hist_data,\n    group_labels, \n    show_hist=False, \n    colors=colors\n)\n\nfig['layout'].update(title='Age distribution')\n\nfig.show()","aba1d169":"# Confirm missing values of Age have been taken care of\nnp.sum(data['Age'].isnull())","24938366":"data['Embarked'].value_counts()","50afc260":"embarked_before_imputation = go.Histogram(\n    x=data['Embarked'].dropna(),\n    name='Before Imputation',\n)\n\n# Impute the Embarked variable\nfor dataset in [data, test_data]:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n\nembarked_after_imputation = go.Histogram(\n    x=data['Embarked'],\n    name='After Imputation'\n)\n\nembarked_before_after_imputation = [embarked_before_imputation, embarked_after_imputation]\n\nlayout = go.Layout(\n    bargap = 0.5,\n    barmode='group',\n    title='Passenger distribution by Port of Embarkation')\n\nfig = go.Figure(data=embarked_before_after_imputation, layout=layout)\n\nfig.show()","7527f8a3":"# Confirm missing values of Embarked have been taken care of\nnp.sum(data['Embarked'].isnull())","5b09300c":"y = data['Survived']","acbf611f":"## your code","fa1298f3":"# drop unnecessary features\nprepared_data = data.drop(['Name', 'Ticket', 'Survived_cat'], axis=1)","758ef87b":"# encode features\nprepared_data = pd.get_dummies(prepared_data)","3735d5bc":"prepared_data.sample(3)","dfa4002b":"# Looks good, let's now see the correlations for all features\ndef plot_corr_matr(c_matr):\n    f, ax = plt.subplots(figsize=(16, 14))\n    matrix_tr = np.triu(c_matr)\n\n    sns.heatmap(\n        c_matr, \n        mask=matrix_tr, \n        cmap=sns.diverging_palette(220, 10, as_cmap=True),\n        linewidths=0.5,\n        square=True, \n        ax=ax,\n        annot=True\n    );","5d0fa039":"prepared_data.corr().abs().unstack().sort_values(ascending=False)[\n    len(prepared_data.corr().columns):len(prepared_data.corr().columns) + 10\n]","fe5879a0":"corr_matrix = prepared_data.corr(method='pearson').abs()","8ae6a6d0":"plot_corr_matr(corr_matrix)","2a920f5e":"X = prepared_data.drop(columns=['Survived'])","5aa1bb91":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=SEED)","24598856":"print(f'Training sample dimension: {X_train.shape}')\nprint(f'Validation sample dimension: {X_valid.shape}')","77237e1d":"log_reg_model = LogisticRegression(solver='liblinear', penalty='l1')","38e5d982":"log_reg_model.fit(X_train, y_train)\n\nlog_reg_prediction = log_reg_model.predict(X_valid)\naccuracy_score(y_valid, log_reg_prediction)","590cf488":"pipeline_mms = make_pipeline(MinMaxScaler(), log_reg_model)\npipeline_mms.fit(X_train, y_train)\nmms_log_reg_prediction = pipeline_mms.predict(X_valid)\n\naccuracy_score(y_valid, mms_log_reg_prediction)","68ed6930":"pipeline_ss = make_pipeline(StandardScaler(), log_reg_model)\npipeline_ss.fit(X_train, y_train)\nss_log_reg_prediction = pipeline_ss.predict(X_valid)\n\naccuracy_score(y_valid, ss_log_reg_prediction)","52d9c7cb":"forest_model = RandomForestClassifier(random_state=SEED)","afc467c4":"pipeline_mms_fs = make_pipeline(MinMaxScaler(), SelectFromModel(forest_model), log_reg_model) \npipeline_mms_fs.fit(X_train, y_train)\nmms_forest_log_reg_pred = pipeline_mms_fs.predict(X_valid)\n\naccuracy_score(y_valid, mms_forest_log_reg_pred)","f56e06e8":"pipeline_ss_fs = make_pipeline(StandardScaler(), SelectFromModel(forest_model), log_reg_model) \npipeline_ss_fs.fit(X_train, y_train)\nss_forest_log_reg_pred = pipeline_ss_fs.predict(X_valid)\n\naccuracy_score(y_valid, ss_forest_log_reg_pred)","fbb9931d":"knn_model = KNeighborsClassifier(n_neighbors=5)","44fa4169":"knn_model.fit(X_train, y_train)\n\nknn_prediction = knn_model.predict(X_valid)\naccuracy_score(y_valid, knn_prediction)","a67bf089":"pipeline_mms_knn = make_pipeline(MinMaxScaler(), knn_model)\npipeline_mms_knn.fit(X_train, y_train)\nmms_knn_prediction = pipeline_mms_knn.predict(X_valid)\n\naccuracy_score(y_valid, mms_knn_prediction)","fd68e70d":"pipeline_ss_knn = make_pipeline(StandardScaler(), knn_model)\npipeline_ss_knn.fit(X_train, y_train)\nss_knn_prediction = pipeline_ss_knn.predict(X_valid)\n\naccuracy_score(y_valid, ss_knn_prediction)","cac9d137":"pipeline_mms_fs_knn = make_pipeline(MinMaxScaler(), SelectFromModel(forest_model), knn_model) \npipeline_mms_fs_knn.fit(X_train, y_train)\nknn_mms_forest_pred = pipeline_mms_fs_knn.predict(X_valid)\n\naccuracy_score(y_valid, knn_mms_forest_pred)","26d75332":"pipeline_ss_fs_knn = make_pipeline(StandardScaler(), SelectFromModel(forest_model), knn_model) \npipeline_ss_fs_knn.fit(X_train, y_train)\nknn_ss_forest_pred = pipeline_ss_fs_knn.predict(X_valid)\n\naccuracy_score(y_valid, knn_ss_forest_pred)","e80eea76":"pipeline_ss_knn_lr = make_pipeline(StandardScaler(), SelectFromModel(log_reg_model), knn_model) \npipeline_ss_knn_lr.fit(X_train, y_train)\nss_knn_lr_pred = pipeline_ss_knn_lr.predict(X_valid)\n\naccuracy_score(y_valid, ss_knn_lr_pred)","41aa0d26":"pipeline_mms_knn_lr = make_pipeline(MinMaxScaler(), SelectFromModel(log_reg_model), knn_model) \npipeline_mms_knn_lr.fit(X_train, y_train)\nmms_knn_lr_pred = pipeline_mms_knn_lr.predict(X_valid)\n\naccuracy_score(y_valid, mms_knn_lr_pred)","6ee7fbf1":"## your code","37f9e540":"# prepare new set and drop unnecessary features\nfe_prepared_test_data = test_data.copy()\n\nfe_prepared_data = data.drop(['Survived_cat'], axis=1)\nfe_prepared_data.head()","73f0f7bf":"# generate new feature from Name column\n# see honorific and titles in names of passengers.\nfe_prepared_data['Name'].str.extract('([A-Za-z]+)\\.', expand=False).unique()","a80b0093":"# add Title as new feature\nfor dataset in [fe_prepared_data, fe_prepared_test_data]:\n    dataset['Title'] = dataset['Name'].str.extract('([A-Za-z]+)\\.', expand=False)","264cbe0b":"fe_prepared_data['Title'].value_counts()","bff101bf":"# reduce the number of values, otherwise it hurts a lot\n# replace those that are rare with rare ones, the rest will be called common names.\nfor dataset in [fe_prepared_data, fe_prepared_test_data]:\n    dataset['Title'] = dataset['Title'].replace(\n        ['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], \n        'Rare'\n    )\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')","d31bda0a":"# check the survivability of each Title group\nfe_prepared_data[['Title', 'Survived']].groupby(\n    ['Title'], \n    as_index=False\n).mean().sort_values(by='Survived', ascending=False)","a24120ff":"# add IsMarried feature\nfor dataset in [fe_prepared_data, fe_prepared_test_data]:\n    dataset['Is_Married'] = 0\n    dataset['Is_Married'].loc[dataset['Title'] == 'Mrs'] = 1","9ce4f187":"# check the survivability of each Is_Married group\nfe_prepared_data[['Is_Married', 'Survived']].groupby(\n    ['Is_Married'], \n    as_index=False\n).mean().sort_values(by='Survived', ascending=False)","baf7be29":"for dataset in [fe_prepared_data, fe_prepared_test_data]:\n    # create feature FamilySize\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\n    # create feature Alone from FamilySize\n    dataset['Alone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'Alone'] = 1","1c08042b":"# check the survivability of each Alone group\nfe_prepared_data[['Alone', 'Survived']].groupby(\n    ['Alone'], \n    as_index=False\n).mean().sort_values(by='Survived', ascending=False)","fb672bd9":"# check the survivability of each FamilySize group\nfe_prepared_data[['FamilySize', 'Survived']].groupby(\n    ['FamilySize'], \n    as_index=False\n).mean().sort_values(by='Survived', ascending=False)","21f98082":"# creating feature isGreen based on passengers under 16 y.o.\nfor dataset in [fe_prepared_data, fe_prepared_test_data]:\n    dataset['isGreen'] = np.where(dataset['Age'] <= 16, 1, 0)","26ece63f":"# creating features based on some common words in Ticket\nfor dataset in [fe_prepared_data, fe_prepared_test_data]:\n    dataset['CA'] = dataset['Ticket'].str.contains('CA|C.A.').astype(int)\n    dataset['SOTON'] = dataset['Ticket'].str.contains('SOTON|STON').astype(int)\n    dataset['PC'] = dataset['Ticket'].str.contains('PC').astype(int)\n    dataset['SC'] = dataset['Ticket'].str.contains('SC|S.C').astype(int)\n    dataset['C'] = dataset['Ticket'].str.contains('C').astype(int)\n\nfe_prepared_data = fe_prepared_data.drop(['Ticket'], axis=1)\nfe_prepared_test_data = fe_prepared_test_data.drop(['Ticket'], axis=1)","1c0b02ae":"# Impute the Fare variable on test dataset\nfe_prepared_test_data['Fare'] = fe_prepared_test_data['Fare'].fillna(\n    fe_prepared_test_data['Fare'].median()\n)","fb964b0f":"# create temp feature based on Fare\nfe_prepared_data['tempFare'] = pd.qcut(fe_prepared_data['Fare'], 8)\nfe_prepared_data[['tempFare', 'Survived']].groupby(\n    ['tempFare'], \n    as_index=False\n).mean().sort_values(by='tempFare', ascending=True)","35b07736":"# replace Fare value based on tempFare\nfor dataset in [fe_prepared_data, fe_prepared_test_data]:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare'] = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\n# drop temp feature    \nfe_prepared_data = fe_prepared_data.drop(['tempFare'], axis=1)","14a720e1":"# for dataset in [fe_prepared_data, fe_prepared_test_data]:\n#     # create categorical feature of Pclass\n#     dataset[\"Pclass_cat\"] = [\n#         \"1st\" if i == 1 else \"2nd\" if i == 2 else \"3rd\" for i in dataset[\"Pclass\"]\n#     ]","d0bc41a8":"# drop garbage\nfe_prepared_data = fe_prepared_data.drop(['Name', ], axis=1)\n# fe_prepared_data = fe_prepared_data.drop(['Pclass', 'Parch', 'SibSp', 'FamilySize'], axis=1)\n\nfe_prepared_test_data = fe_prepared_test_data.drop(['Name', ], axis=1)\n# fe_prepared_test_data = fe_prepared_test_data.drop(['Pclass', 'Parch', 'SibSp', 'FamilySize'], axis=1)\n\nfe_prepared_data.head()","2a602a6e":"# create temp feature based on Age\nfe_prepared_data['AgeGroup'] = pd.cut(fe_prepared_data ['Age'], 8)\nfe_prepared_data[['AgeGroup', 'Survived']].groupby(\n    ['AgeGroup'], \n    as_index=False\n).mean().sort_values(by='AgeGroup', ascending=True)","4ad721ea":"# replace Age value based on AgeGroup\nfor dataset in [fe_prepared_data, fe_prepared_test_data]:\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4\n    dataset['Age'] = dataset['Age'].astype(int)\n    \n# drop temp feature    \nfe_prepared_data = fe_prepared_data.drop(['AgeGroup'], axis=1)","53893d3e":"for dataset in [fe_prepared_data, fe_prepared_test_data]:\n    dataset['Age*Class'] = dataset['Age'] * dataset['Pclass']\n\nfe_prepared_data.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)","2664d99f":"# Fare per Person\nfor dataset in [fe_prepared_data, fe_prepared_test_data]:\n    dataset['Fare_Per_Person'] = dataset['Fare'] \/ (dataset['FamilySize'])\n    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)","0971bfd2":"for dataset in [fe_prepared_data, fe_prepared_test_data]:\n#     # encode features\n    dataset['Sex'] = dataset['Sex'].map({'male': 1, 'female': 0})\n    dataset['Embarked'] = dataset['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n    dataset['Title'] = dataset['Title'].map(\n        {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    )\n\nfe_prepared_data = pd.get_dummies(fe_prepared_data)\n\nfe_prepared_test_data = pd.get_dummies(fe_prepared_test_data)\n\nfe_prepared_data.head()","66a26688":"fe_prepared_data.columns","97b873fb":"fe_prepared_test_data.columns","f69c26db":"X = fe_prepared_data.drop(columns=['Survived'])","30b56596":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=SEED)","f8b3a9ee":"log_reg_model = LogisticRegression(C=1.1, solver='liblinear', penalty='l2')\n\nfe_pipeline_ss = make_pipeline(StandardScaler(), log_reg_model)\nfe_pipeline_ss.fit(X_train, y_train)\nfe_ss_log_reg_prediction = fe_pipeline_ss.predict(X_valid)\n\naccuracy_score(y_valid, fe_ss_log_reg_prediction)","6ace2c8e":"# best_feats = X.columns.tolist()","cae15005":"best_feats = [\n    'Pclass',\n    'Sex',\n    'Age',\n    'SibSp',\n    'Parch',\n    'Fare',\n    'Embarked',\n    'Title',\n    'Is_Married',\n    'FamilySize',\n    'Alone',\n    'isGreen',\n    'CA',\n    'SOTON',\n    'PC',\n    'SC',\n    'Age*Class',\n]","c0e1b2c6":"X_train[best_feats].sample(3)","b702c03c":"# re-create the model by specifying a solver\nclf = LogisticRegression(solver='liblinear')\n\n# describe the grid on which we will search\nparam_grid = {\n    'logreg__solver': ['liblinear'],\n    'logreg__C': np.logspace(-4, 4, 20),\n    'logreg__penalty': ['l1', 'l2'],\n    'logreg__random_state': [17, 21, 42, 73, 2020],\n    'logreg__fit_intercept': [True, False],\n    'logreg__class_weight': [None, 'balanced'],\n    'logreg__max_iter': [2500, 3000, 3500],\n}\n\npipe_for_grid = Pipeline([('scaler',  StandardScaler()), ('logreg', clf)])\n# create a GridSearchCV object\nlog_reg_grid_search = GridSearchCV(\n    pipe_for_grid, param_grid, n_jobs=-1, cv=5, refit=True, scoring='accuracy'\n)\n\n# run search\nlog_reg_grid_search.fit(X_train[best_feats], y_train)\n\nbest_model = log_reg_grid_search.best_estimator_\n\n# display the best parameters\nprint(f\"Best params: {log_reg_grid_search.best_params_}\")\nprint(f\"Best score: {log_reg_grid_search.best_score_}\")\nprint(f\"Best estimator: {best_model}\")\nlog_reg_grid_prediction = best_model.predict(X_valid[best_feats])\nprint(f\"CV accuracy score: {accuracy_score(y_valid, log_reg_grid_prediction)}\")","b7b980e1":"lr_best_clf, lr_stats = fit_plot_confusion(\n    log_reg_grid_search, \n    X_train[best_feats], \n    y_train, \n    X_valid[best_feats], \n    y_valid\n)","6d57c805":"eli_lr_model = best_model\n\n\neli_lr_model.fit(X_train[best_feats], y_train)\nperm = PermutationImportance(eli_lr_model).fit(X_valid[best_feats], y_valid)\neli5.show_weights(perm, feature_names = best_feats)","879104e3":"model_yb = eli_lr_model[1]\nviz = FeatureImportances(model_yb)\nviz.fit(X_train[best_feats], y_train)\n\nviz.show();\n# https:\/\/www.scikit-yb.org\/en\/latest\/api\/model_selection\/importances.html","bc3251e3":"# KNN\n\nknn_param_grid = {\n    'n_neighbors': [1, 2, 3, 5, 30, 100], \n    'weights': ['uniform', 'distance']\n}\n\nknn_clf = GridSearchCV(KNeighborsClassifier(n_jobs=-1), knn_param_grid)\n\nknn_best_clf, knn_stats = fit_plot_confusion(\n    knn_clf, \n    X_train[best_feats], \n    y_train,\n    X_valid[best_feats], \n    y_valid\n)\n\n# stats and params\nprint(f\"Knn stats: {knn_stats}\")\nprint(f\"Knn Best estimator: {knn_best_clf}\")\n\ny_pred = knn_best_clf.predict(X_valid[best_feats])\n\nprint(f\"CV accuracy score: {metrics.accuracy_score(y_pred=y_pred, y_true=y_valid % 2)}\")","f5d335e9":"eli_lr_model = knn_best_clf\n\neli_lr_model.fit(X_train[best_feats], y_train)\nperm = PermutationImportance(eli_lr_model).fit(X_valid[best_feats], y_valid)\neli5.show_weights(perm, feature_names = best_feats)","a58938bd":"fe_pipeline_full_data = make_pipeline(best_model)\n# fe_pipeline_full_data = make_pipeline(knn_best_clf)\n\nfe_pipeline_full_data.fit(X[best_feats], y)\nfe_log_reg_prediction_full_data = fe_pipeline_full_data.predict(\n    fe_prepared_test_data[best_feats]\n)\n\nwrite_to_submission_file(\n    fe_log_reg_prediction_full_data, \n    'submit.csv'\n)","c8e28cef":"fe_pipeline_full_data","7c5466e7":"# EDA","deb65a7d":"< your thoughts > In the first class, the chance of survival was higher than in the 3rd class. \n\nOn heatmap we see that the most frequent pairing between Embarked and Pclass is S, 3, meaning that most passengers that embarked from Southampton were in third class.\n\nOn 3-d scatter we can clearly see that most passengers is Pclass = 3 did not survive but those that did were female. Lastly, we see that most passengers in Pclass = 1 survived indepedent of Sex","e27bd0bc":"https:\/\/github.com\/rolling-scopes-school\/ml-intro\/tree\/2021\/5_classification_linear_knn","5e62f5da":"https:\/\/www.kaggle.com\/c\/titanic\/data","ca51a276":"![image.png](attachment:image.png)","9dc5baff":"**(0.5 points)** How many females and males are there in the dataset? What about the survived passengers? Is there any relationship between the gender and the survival?","67a4ea69":"Load the test set and make the predictions. Submit them to kaggle and see the results :)\nSelect the best model, load the test set and make the predictions. Submit them to kaggle.\n\n**Note**. X points will depend on your kaggle leaderboard score.\n$$ f(score) = 0.5, \\ \\ 0.76 \\leq score < 0.78,$$\n$$ f(score) = 1.0, \\ \\ 0.78 \\leq score < 0.81,$$ \n$$ f(score) = 2.5, \\ \\ 0.81 \\leq score $$ \nYour code should generate the output submitted to kaggle. Fix random seeds to make the results reproducible.","922dd770":"### Dataset\n\nRead the description here: https:\/\/www.kaggle.com\/c\/titanic\/data. Download the dataset and place it in the *data\/titanic\/* folder in your working directory.\nYou will use train.csv for model training and validation. The test set is used for model testing: once the model is trained, you can predict whether a passenger survived or not for each passenger in the test set, and submit the predictions: https:\/\/www.kaggle.com\/c\/titanic\/overview\/evaluation.  \n","cc7046a0":"# train KNN","811980d0":"Think about the ways to handle these missing values for modelling and write your answer below. Which methods would you suggest? What are their advantages and disadvantages?\n\n< your thoughts >\n\nThinking about ways to handle missing values is a useful thing, but I decided to go straight to business as soon as I saw them =) and, as you can see from the code above, I processed these values.\n\nFilled in the missing values of **Age** with the median. \n\nI filled in the missing values port of **Embarkation** with the most common value - S (Southampton), because the percentage of skips for this feature was extremely low. \n\nFor the missing values of **Cabin**, more radical measures were taken - the entire column was droped, because the percentage of omissions was almost 80%, which is a lot.","83b835c5":"**(0.5 points)** Find the percentage of missing values for each feature. ","0a218077":"< your thoughts >\nAs we can see from the pie charts and the generated data microframe, more women were saved, more men died.","bfdc021b":"< your thoughts > The age distribution of surviving and deceased passengers has a number of differences.\n\nHaving carefully studied the graphs, you can see that babies have a high probability of surviving, obviously they will be given priority to a place in a lifeboat, and people aged ~ 80 years, were saved in 100% of cases.","ccffbd4a":"# try to find best hyperparameters","5417c592":"# Classification. Linear models and KNN","6694d2da":"It's time to deal with Embarked and fill in the missing values. We will do it very simply, find the most frequent value and replace the gaps with it - Southampton(S).","a492b31a":"https:\/\/www.kaggle.com\/c\/titanic\/submit","c5e233f1":"<a href=\".\/submit.csv\"> Download File <\/a>","a7ec8ba3":"## Part 1: Titanic survival prediction","4c624217":"**(0.5 + X points)** Try more feature engineering and hyperparameter tuning to improve the results. You may use either KNN or Logistic Regression (or both).","7741c468":"![image.png](attachment:image.png)","f2533cfc":"**(0.5 points)** Plot age distribution of the passengers. What is the average and the median age of survived and deceased passengers? Do age distributions differ for survived and deceased passengers? Why?","e6c12a88":"**(1.5 points)** Prepare the features and train two models (KNN and Logistic Regression) to predict the survival. Compare the results. Use accuracy as a metric. Don't forget about cross-validation!","10345bdf":"# Modelling","630d495c":"LeaderBoard Score: 0.79425\n\nbest_feats = [\n    'Pclass',\n    'Sex',\n    'Age',\n    'SibSp',\n    'Parch',\n    'Fare',\n    'Embarked',\n    'Title',\n    'Is_Married',\n    'FamilySize',\n    'Alone',\n    'isGreen',\n    'CA',\n    'SOTON',\n    'PC',\n    'SC',\n    'Age*Class',\n]\n\nPipeline(steps=[('pipeline',\n                 Pipeline(steps=[('scaler', StandardScaler()),\n                                 ('logreg',\n                                  LogisticRegression(C=0.23357214690901212,\n                                                     max_iter=3000,\n                                                     random_state=17,\n                                                     solver='liblinear'))]))])","d288f9c4":"# feature enginering","7a052e25":"# feature preparation","3958a3ad":"*What are their advantages and disadvantages?*\n\nTo try to understand the advantages and disadvantages of certain methods of handling missing values, we must understand the reason for the loss of data.\n\n* **Missing at Random (MAR):** Missing at random means that the propensity for a data point to be missing is not related to the missing data, but it is related to some of the observed data\n* **Missing Completely at Random (MCAR):** The fact that a certain value is missing has nothing to do with its hypothetical value and with the values of other variables.\n* **Missing not at Random (MNAR):** Two possible reasons are that the missing value depends on the hypothetical value (e.g. People with high salaries generally do not want to reveal their incomes in surveys) or missing value is dependent on some other variable\u2019s value (e.g. Let\u2019s assume that females generally don\u2019t want to reveal their ages! Here the missing value in age variable is impacted by gender variable)\n\nIn the first two cases, it is safe to remove the data with missing values depending upon their occurrences, while in the third case removing observations with missing values can produce a bias in the model. So we have to be really careful before removing observations. Note that imputation does not necessarily give better results.","5c6f893d":"**(1 point)** Explore \"passenger class\" and \"embarked\" features. What class was \"the safest\"? Is there any relationship between the embarkation port and the survival? Provide the corresponding visualizations.","b18d75be":"# train Logistic Regression","05cac12b":"Results not so good, need to work hard! =)","76975dea":"# Predict on test data"}}