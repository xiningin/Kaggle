{"cell_type":{"f5159295":"code","96a4142e":"code","2757229c":"code","c73ad340":"code","ba3e5981":"code","5d0d3ee6":"code","2a8271b4":"code","b46ba0b8":"code","0ba0319c":"code","3358fc6d":"code","505beebe":"code","ede772e7":"code","372bc1ab":"code","44cbd161":"code","f158c9b2":"code","6d9db183":"code","a9dc47e4":"code","55fa83aa":"code","6f49c522":"code","1994d5bc":"code","21cae622":"code","7f2c811c":"code","d95a99fc":"code","8f291abd":"code","31c36d50":"markdown","014afe8d":"markdown","9f091a09":"markdown","e2f7373c":"markdown","3ea1f488":"markdown","c7f8f2f3":"markdown","c5558625":"markdown","015ae084":"markdown","dcc89f5b":"markdown","d0807640":"markdown","aa6bf47b":"markdown","0455f787":"markdown","fd6dadf7":"markdown","5efeaa5f":"markdown","3c29efe4":"markdown"},"source":{"f5159295":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport operator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import SimpleRNN\nfrom tensorflow.keras.layers import RepeatVector\nfrom tensorflow.keras.layers import TimeDistributed\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.models import load_model","96a4142e":"valid_characters = '0123456789.+*-\/'","2757229c":"val_char_dict = dict((character, index) for index, character in enumerate(valid_characters))","c73ad340":"val_char_dict_inv = dict((index, character) for index, character in enumerate(valid_characters))","ba3e5981":"def one_hot_encode(decoded):\n  encoded = np.zeros((repeat_steps, len(valid_characters)))\n  padding = repeat_steps - len(decoded)\n\n  for index, character in enumerate(decoded):\n    encoded[index+padding, val_char_dict[character]] = 1\n  \n  for index in range(0, padding):\n    encoded[index, val_char_dict['0']] = 1\n  \n  return encoded","5d0d3ee6":"def one_hot_decode(encoded):\n  decoded = [val_char_dict_inv[np.argmax(array)] for index, array in enumerate(encoded)]\n  decoded = ''.join(decoded)\n  \n  return decoded","2a8271b4":"number_max = 100 #Up to this number\nrepeat_steps = len(str(number_max-1)) * 2 + 1","b46ba0b8":"operators = ['+', '*', '-', '\/']\noperators_dict = { \"+\":operator.add, \n                  '*':operator.mul, \n                  \"-\":operator.sub,\n                  '\/':operator.truediv}","0ba0319c":"def data_generator():\n  number_1 = np.random.randint(1,number_max)\n  operator_index = np.random.randint(0,len(operators))\n  operator = operators[operator_index]\n  number_2 = np.random.randint(1,number_max)\n  operation = str(number_1) + operator + str(number_2)\n  result = str(round(operators_dict[operator](number_1,number_2),5))\n  return operation, result","3358fc6d":"data_points = 1000000\ntest_size = 0.2\n\ntraining_size = int(round(data_points * (1-test_size),0))\ntest_size = data_points - training_size\n\nx_train = []\nx_test = []\ny_train = []\ny_test = []\n\nfor i in tqdm(range(0, training_size)):\n  x, y = data_generator()\n  x_e = one_hot_encode(x)\n  y_e = one_hot_encode(y)\n  x_train.append(x_e)\n  y_train.append(y_e)\n\nfor i in tqdm(range(0, test_size)):\n  x, y = data_generator()\n  x_e = one_hot_encode(x)\n  y_e = one_hot_encode(y)\n  x_test.append(x_e)\n  y_test.append(y_e)\n\nx_train = np.array(x_train)\ny_train = np.array(y_train)\nx_test = np.array(x_test)\ny_test = np.array(y_test)","505beebe":"model = Sequential()\n\nmodel.add(SimpleRNN(units=1024, input_shape=(None, len(valid_characters))))\nmodel.add(RepeatVector(repeat_steps))\n\nmodel.add(SimpleRNN(units=1024, return_sequences=True))\nmodel.add(TimeDistributed(Dense(units=len(valid_characters), activation='softmax')))\n","ede772e7":"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","372bc1ab":"plot_model(model)","44cbd161":"model.summary()","f158c9b2":"early_stopping = EarlyStopping(monitor='val_loss', patience=10)\nsave_best = ModelCheckpoint('best_model.hdf5', monitor='val_loss', save_best_only=True, mode='min')","6d9db183":"history = model.fit(x_train,\n                    y_train, \n                    batch_size=512,\n                    epochs=250,\n                    validation_data=(x_test, y_test),\n                    callbacks=[early_stopping, save_best])","a9dc47e4":"plt.figure(figsize=(10,6))\nplt.plot(np.arange(0,len(history.history['loss'])), history.history['loss'], label='train_loss')\nplt.plot(np.arange(0,len(history.history['val_loss'])), history.history['val_loss'], label='test_loss')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.title('Loss vs. Epochs')\nplt.legend()\nplt.show()","55fa83aa":"plt.figure(figsize=(10,6))\nplt.plot(np.arange(0,len(history.history['accuracy'])), history.history['accuracy'], label='Train Accuracy')\nplt.plot(np.arange(0,len(history.history['val_accuracy'])), history.history['val_accuracy'], label='Test Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.title('Accuracy vs. Epochs')\nplt.legend()\nplt.show()","6f49c522":"model = load_model('best_model.hdf5')","1994d5bc":"def predict_result(operation):\n  o = one_hot_encode(operation)\n  o = np.reshape(o, (1, o.shape[0], o.shape[1] ))\n  predictions = model.predict(o)\n  for prediction in predictions:\n    return one_hot_decode(prediction).lstrip(\"0\")","21cae622":"predict_result('14+7')","7f2c811c":"predict_result('99*5')","d95a99fc":"predict_result('2\/8')","8f291abd":"predict_result('75-64')","31c36d50":"Computers have been helping people solve mathematical operations for decades, so the title of this post may sound trivial for some of you. Let me tell you that this problem will have an increased degree of difficulty because I\u2019ll be feeding my algorithm math operations as strings. So basically, the neural network will have to learn the symbolic representation of numbers and basic operators such as +, *, -, \/.\n\nFor the first proof of concept, I decided to use a simple recurrent neural network (RNN) that could compute the sum, multiplication, subtraction, and division of two numbers (each not greater than 100). Recurring neural networks are a type of neural network where the data does not flow in sequential order (unlike feedforward networks,) the data flows in cycles or loops as shown in Figure 1.","014afe8d":"Now, we need a function to be in charge of generating data points. The following function generates an operation string of the type \u201c58\/73\u201d and its result in the same format. Please note that the number_max variable controls the range of the random integers that will be used. This variable has a direct influence in the training times required to achieve reasonable accuracy, for this project I limit it to 100.","9f091a09":"## **References**\n1. \u201cAn Implementation of Sequence to Sequence Learning for Performing Addition.\u201d Addition RNN \u2014 Keras Documentation, keras.io\/examples\/addition_rnn\/.\n1. Zaremba, Wojciech, and Ilya Sutskever. \u201cLearning to Execute.\u201d ArXiv:1410.4615 [Cs], Feb. 2015. arXiv.org, http:\/\/arxiv.org\/abs\/1410.4615.\n","e2f7373c":"Once we have an encoder and decoder, as well as a data generator function, the next step is to put everything together and scale it up to generate as many data points as we considered necessary. The following code generates 1,000,000 data points and uses 80% for the training set and the remaining for the test set.","3ea1f488":"The model was trained using Keras\u2019 EarlyStopping and ModelCheckpoint callbacks to prevent unnecessary waste of time. The model reached its peak accuracy of 96.38% at epoch 12with only 240 seconds of training time, not bad! Take a look at Figure 3 and Figure 4 to see how the loss and accuracy progressed throughout the training process, keep in mind that only the model with the best accuracy was serialized (in this case at epoch 12.)","c7f8f2f3":"## Features","c5558625":"As you can see, the model is working as expected. However, there are cases where the predictions are wrong, especially when the input has decimals or negative numbers as it wasn\u2019t shown any examples of that kind.\n\nThis neural network is impressive, but its performance could be enhanced by including LSTM layers to its architecture and of course, feeding it more data and more varied examples.\n\nFinally, if you enjoyed this article, please feel free to follow me on GitHub and Twitter.\n\nGitHub: https:\/\/github.com\/fescobar96\n\nTwitter: https:\/\/twitter.com\/fescobar96","015ae084":"The strings need to be encoded before they can be fed into a neural network. To do this, we create two functions to encode the data before feeding it to our neural network and to decoded after it makes its predictions.","dcc89f5b":"## Import Libraries","d0807640":"Let\u2019s now try to use the trained model to make some predictions:","aa6bf47b":"## Model","0455f787":"## One-Hot Encoder\/Decoder","fd6dadf7":"## Data Synthesis","5efeaa5f":"The first step of this project was to collect or generate enough data to train the neural network. Unfortunately, I do not believe that there are datasets suitable for this type of task, but generating our data, in this case, is extremely simple.\n\nOur project begins by defining a feature space and we do that by considering all the possible characters that the neural network could encounter. In this case, our feature space is defined in valid_characters. Then, a couple of dictionaries are created, this will come up handy in the next step to one-hot encode and decode the data.","3c29efe4":"The model was the simplest part of the project; using a high-level API as Keras feels like cheating sometimes. The model has an encoder section composed of a SimpleRNN layer followed by a RepeatVector. The decoder has a similar structure, but instead of using a RepeatVector layer, it uses a TimeDistributed layer that encloses a Dense layer."}}