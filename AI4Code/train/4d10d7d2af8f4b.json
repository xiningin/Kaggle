{"cell_type":{"ff377871":"code","9ba9f0c8":"code","79bde2a8":"code","70b717a0":"code","c75ab883":"code","336f9f2e":"code","ed90a550":"code","70fe57a2":"code","970a8cab":"code","b676576e":"code","cb095108":"code","fdb7aa75":"code","a7fe2c77":"code","7e6c8c08":"code","497252f4":"code","d5047202":"code","601690e9":"code","3726bd1d":"code","8198830a":"code","e675757b":"code","eabb91d5":"code","136c1d67":"code","c396c71e":"code","0253bf82":"code","88b9e935":"code","87457771":"code","5e62c478":"code","5091a37f":"code","99000404":"code","fe28a374":"code","c6162491":"code","f4aa4d7e":"code","7b64036b":"code","40c3b3fe":"code","1e7c2a1e":"code","df2df783":"code","fefc8a50":"code","ea48fc35":"code","8289e347":"code","a9abcc99":"code","a4a61cb7":"code","5943f37f":"code","22658aa8":"code","40805dff":"code","8410a74c":"code","6d63a58a":"code","f32e1ca6":"code","9ee7e36a":"code","99b4bdf7":"code","03a5666e":"code","cfc4add0":"code","ce7600ca":"code","f1378ec5":"code","8cc292cb":"code","e6327828":"code","bc403d49":"code","461649e4":"code","3c9551f0":"code","8d1fbb48":"code","fa4b730e":"code","432ef91c":"code","6a8c58c7":"code","e96e20c9":"code","5574cc62":"code","06089e6e":"code","664f205c":"markdown","133a28fb":"markdown","828ecfa8":"markdown","a522548a":"markdown","ead2b382":"markdown","73183108":"markdown","8d247e98":"markdown","e693760d":"markdown","901cd47e":"markdown","53a24f9b":"markdown","371e9ba9":"markdown","4508bf7e":"markdown","98e2e061":"markdown","a4bfa84c":"markdown","9e64c8a3":"markdown","8c898f3e":"markdown","e21c6ee3":"markdown","d9c21975":"markdown","7f521523":"markdown","d40dd2b7":"markdown","5305f845":"markdown","f10dd254":"markdown","be9821bc":"markdown","17a7a049":"markdown","a9085fb0":"markdown"},"source":{"ff377871":"import numpy as np\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport seaborn as sns\n\nfrom copy import deepcopy\nimport gc\n\nimport nltk\nfrom nltk.tokenize import TweetTokenizer  # Twitter-aware tokenizer\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.corpus import stopwords\n\nimport torch\nimport torch.nn as nn\n\nfrom gensim.models.word2vec import Word2Vec\nimport gensim.downloader as api\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n%matplotlib inline","9ba9f0c8":"def _normalize_tweet(text):\n    \"\"\"Returns a normalized versions of text.\"\"\"\n\n    # change hyperlinks to '<url>' tokens\n    output = re.sub(r'http[s]{0,1}:\/\/t.co\/[a-zA-Z0-9]+\\b', '<url>', text)\n    \n    # separate all '#' signs from following word with one whitespace\n    output = re.sub(r'#(\\w+)', r'# \\1', output)\n\n    return output","79bde2a8":"def _tokenize(tokenizer, string):\n    \"\"\"Tokenizes a sentence, but leave hastags (#) and users (@)\"\"\"\n    \n    tokenized = tokenizer.tokenize(string)\n    return tokenized","70b717a0":"def _numbers_to_number_tokens(tokenized_string, num_token='<number>'):\n    \"\"\"Returns the tokenized string (list) with numbers replaced by a numbet token.\"\"\"\n    \n    # create a list of (word, POS-tags) tuples\n    pos_tagged = nltk.pos_tag(tokenized_string)\n    \n    # find indices of number POS tags\n    num_indices = [idx for idx in range(len(pos_tagged)) if pos_tagged[idx][1] == 'CD']\n    \n    # replace numbers by token\n    for idx in num_indices:\n        tokenized_string[idx] = num_token\n        \n    return tokenized_string    ","c75ab883":"def preprocess_text(tokenizer, string):\n    \"\"\"Executes all text cleaning functions.\"\"\"\n    \n    return _numbers_to_number_tokens(_tokenize(tokenizer, _normalize_tweet(string)))","336f9f2e":"def preprocess_keyword(keyword):\n    \"\"\"Returns a clean, tokenized keyword.\"\"\"\n    \n    # return None if keywors is np.nan\n    if type(keyword) == np.float and np.isnan(keyword):\n        return\n    \n    # replace '%20' with whitespace, lower, and tokenize\n    output = re.sub(r'%20', ' ', keyword)\n    output = output.lower()\n    output = output.split()\n    return output","ed90a550":"def count_all_caps(text):\n    \"\"\"Returns an integer denoting number of ALL-CAPS words (e.g. 'CANADA', 'WELCOME').\"\"\"\n\n    return len([word for word in text.split() if word.isupper()])","70fe57a2":"def count_capitalized(text):\n    \"\"\"Returns an integer denoting number of capitalized words (e.g. 'Beer', 'Obama').\"\"\"\n\n    return len([word for word in text.split() if word.istitle()])","970a8cab":"def count_words(text):\n    \"\"\"Returns an integer denoting number of words in tweet (before normalizing).\"\"\"\n\n    return len(text.split())","b676576e":"def sentiment_analyze_df(df, column):\n    \"\"\"Adds 4 columns of sentiment analysis scores to input DataFrame. changes occur inplace.\"\"\"\n\n    # instantiate a sentiment anlayzer\n    sid = SentimentIntensityAnalyzer()\n    \n    # instantiate a matrix and populate it with scores of each of df[column]\n    output_values = np.zeros((len(df), 4))\n    for tup in df.itertuples():\n        output_values[tup.Index, :] = list(sid.polarity_scores(' '.join(getattr(tup, column))).values())\n    \n    # adding column to input DataFrame\n    for idx, col in enumerate(['sent_neg', 'sent_neu', 'sent_pos', 'sent_compound']):\n        df[col] = output_values[:, idx]","cb095108":"def _get_word_vec(embedding_model, use_norm, word):\n    \"\"\"\n    Returns a normalized embedding vector of input word.\n    \n    Takes care of special cases.\n    <url> tokens are already taken care of in normalization.\n    \"\"\"\n\n    if word[0] == '@':\n        return embedding_model.word_vec('<user>', use_norm=use_norm)\n        \n    elif word == '#':\n        return embedding_model.word_vec('<hashtag>', use_norm=use_norm)\n\n    elif word in embedding_model.vocab:\n        return embedding_model.word_vec(word, use_norm=use_norm)\n\n    else:\n        return embedding_model.word_vec('<UNK>', use_norm=use_norm)","fdb7aa75":"def _text_to_vectors(embedding_model, use_norm, tokenized_text):\n    \"\"\"Returns tweet's words' embedding vector.s\"\"\"\n\n    vectors = [_get_word_vec(embedding_model, use_norm, word) for word in tokenized_text]\n    vectors = np.array(vectors)\n    \n    return vectors","a7fe2c77":"def _trim_and_pad_vectors(text_vectors, embedding_dimension, seq_len):\n    \"\"\"Returns a padded matrix of text embedding vectors with dimensions (seq_len, embedding dimensions).\"\"\"\n\n    # instantiate 0's matrix\n    output = np.zeros((seq_len, embedding_dimension))\n\n    # trim long tweets to be seq_len long\n    trimmed_vectors = text_vectors[:seq_len]\n\n    # determine index of end of padding and beginning of tweet embedding\n    end_of_padding_index = seq_len - trimmed_vectors.shape[0]\n\n    # pad if needed, by replacing last rows with the tweet's words' embedding vectors\n    output[end_of_padding_index:] = trimmed_vectors\n\n    return output","7e6c8c08":"def embedding_preprocess(embedding_model, use_norm, seq_len, tokenized_text):\n    \"\"\"Returns an embedding representation of input tokenized text, by executing text embedding functions.\"\"\"\n    \n    # get matrix of tweet's words' embedding vectors (tweet length, embedding_dimension)\n    text_vectors = _text_to_vectors(embedding_model, use_norm, tokenized_text)\n    \n    output = _trim_and_pad_vectors(text_vectors, embedding_model.vector_size, seq_len)\n    \n    return output","497252f4":"def keyword_to_avg_vector(embedding_model, use_norm, tokenized_keyword):\n    \"\"\"Returns keyword(s') average embedding vector.\"\"\"\n    \n    # return a vector of zeros if tokenized_keyword is None\n    if tokenized_keyword is None:\n        return np.zeros((1, embedding_model.vector_size))\n    \n    # otherwise, calculate average embedding vector\n    vectors = [_get_word_vec(embedding_model, use_norm, word) for word in tokenized_keyword]\n    vectors = np.array(vectors)\n    avg_vector = np.mean(vectors, axis=0)\n    avg_vector = avg_vector.reshape((1, embedding_model.vector_size))\n    return avg_vector","d5047202":"# load a pre-trained model, which was trained on twitter\nmodel_glove_twitter = api.load(\"glove-twitter-100\")","601690e9":"# create a random vector, to represent <UNK> token (unseen word)\nrandom_vec_for_unk = np.random.uniform(-1, 1, size=model_glove_twitter.vector_size).astype('float32')\nrandom_vec_for_unk = random_vec_for_unk.reshape(1,model_glove_twitter.vector_size)","3726bd1d":"# add the random vector to model\nmodel_glove_twitter.add(['<UNK>'], random_vec_for_unk, replace=True)","8198830a":"# compute noramlized vectors, and replace originals\nmodel_glove_twitter.init_sims(replace=True)","e675757b":"TRAIN_SET_PATH = '..\/input\/nlp-getting-started\/train.csv'","eabb91d5":"train_df = pd.read_csv(TRAIN_SET_PATH)\ntrain_df.head()","136c1d67":"# create a tokenizer which lowercases, reduces length of and preserves user handles ('@user')\ntokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=False) ","c396c71e":"# normalize and tokenize texts\ntrain_df['tok_norm_text'] = [preprocess_text(tokenizer, text) for text in train_df['text']]","0253bf82":"train_df['keyword'] = train_df['keyword'].apply(preprocess_keyword)","88b9e935":"train_df['num_all_caps'] = train_df['text'].apply(count_all_caps)\ntrain_df['num_caps'] = train_df['text'].apply(count_capitalized)\ntrain_df['num_words'] = train_df['text'].apply(count_words)","87457771":"# create a scaler to make all features be in range [-1, 1], thus suitable for a newural network model\nscaler = MinMaxScaler(feature_range=(-1, 1))\n\ncolumns_to_scale = ['num_all_caps', 'num_caps', 'num_words']\nscaler.fit(train_df[columns_to_scale])\ntrain_df[columns_to_scale] = scaler.transform(train_df[columns_to_scale])","5e62c478":"# create sentiment analysis feautres\nsentiment_analyze_df(train_df, 'tok_norm_text')","5091a37f":"train_df.head()","99000404":"sns.distplot([len(tok) for tok in train_df['tok_norm_text']])","fe28a374":"sequence_max_length = 30","c6162491":"train_df['text_embedding'] = [\n    embedding_preprocess(\n        embedding_model=model_glove_twitter, use_norm=True, seq_len=sequence_max_length, tokenized_text=text)\n    for text in train_df['tok_norm_text']\n]","f4aa4d7e":"train_df['keyword_embedding'] = [\n    keyword_to_avg_vector(embedding_model=model_glove_twitter, use_norm=True, tokenized_keyword=keyword)\n    for keyword in train_df['keyword']\n]","7b64036b":"train_df.head()","40c3b3fe":"def _single_values_repeat(seq_len, static_single_values):\n    \"\"\"Returns a numpy array containing seq_len-repeated values.\"\"\"\n    \n    output = static_single_values.reshape((1, len(static_single_values)))\n    output = np.repeat(output, seq_len, axis=0)\n    return output","1e7c2a1e":"def _static_embedding_repeat(seq_len, static_embedding_values):\n    \"\"\"Return a numpy array os stacked static embedding vectors.\"\"\"\n    \n    horizontally_stacked = np.hstack(static_embedding_values)\n    output = np.repeat(horizontally_stacked, seq_len, axis=0)\n    return output","df2df783":"def concatenate_embeddings(df, embedding_model, seq_len, sequence_embedding_col, static_embedding_cols, static_singles_cols):\n    \"\"\"Returns one embedding representation of all features - main sequence, static embedded featues, and single values.\"\"\"\n    \n    emb_dim = embedding_model.vector_size\n    \n    # instantiate output matrix\n    output = np.zeros((len(df), seq_len, len(static_singles_cols) + len(static_embedding_cols) * emb_dim + emb_dim))\n    \n    for idx, row in df.iterrows():\n        \n        single_vals = _single_values_repeat(seq_len, row[static_singles_cols].values)\n        static_emb_vals = _static_embedding_repeat(seq_len, row[static_embedding_cols])\n        seq_emb_vals = row[sequence_embedding_col]\n\n        # horizontally stack embeddings and features\n        row_embedding = np.hstack((single_vals, static_emb_vals, seq_emb_vals))\n\n        output[idx, :, :] = row_embedding\n        \n    return output","fefc8a50":"# Create one embedding representation of all chosen features\nembedding_matrix = concatenate_embeddings(\n    df=train_df, embedding_model=model_glove_twitter, seq_len=sequence_max_length,\n    sequence_embedding_col='text_embedding',\n    static_embedding_cols=['keyword_embedding'],\n    static_singles_cols=['num_all_caps', 'num_caps', 'num_words', 'sent_neg', 'sent_neu', 'sent_pos', 'sent_compound'])","ea48fc35":"embedding_matrix.shape","8289e347":"class BiLSTM(nn.Module):\n    \"\"\"A pyTorch Bi-Directional LSTM RNN implementation\"\"\"\n\n    def __init__(self, embedding_dim, hidden_dim, num_layers, num_classes, batch_size, dropout, device):\n        super(BiLSTM, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.batch_size = batch_size\n        self.num_layers = num_layers\n\n        self.dropout = nn.Dropout(p=dropout)\n\n        self.lstm = nn.LSTM(\n            input_size=embedding_dim, hidden_size=hidden_dim, num_layers=num_layers,\n            batch_first=True, dropout=dropout, bidirectional=True)\n        \n        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n\n        self.device = device\n        \n        # instantiate lists for evaluating and plotting\n        self.train_loss = []\n        self.train_acc = []\n        self.val_loss = []\n        self.val_acc = []\n        \n        # an attribute to hold model's best weights (used for evaluating)\n        self.best_weights = deepcopy(self.state_dict())\n\n    def _init_hidden(self, current_batch_size):\n        \"\"\"Sets initial hidden and cell states (for LSTM).\"\"\"\n\n        h0 = torch.zeros(self.num_layers * 2, current_batch_size, self.hidden_dim).to(self.device)\n        c0 = torch.zeros(self.num_layers * 2, current_batch_size, self.hidden_dim).to(self.device)\n        return h0, c0\n\n    def forward(self, x):\n        \"\"\"Forward step.\"\"\"\n\n        # Forward propagate LSTM\n        h, c = self._init_hidden(current_batch_size=x.size(0))\n        out, _ = self.lstm(x, (h, c))\n\n        # dropout\n        out = self.dropout(out)\n\n        # Decode the hidden state of the last time step\n        out = self.fc(out[:, -1, :])\n\n        return out\n    \n    def predict(self, x: torch.tensor):\n        \"\"\"Return a tensor of predictions of tensor x.\"\"\"\n\n        class_predictions = self(x).data\n        _, predicted = torch.max(class_predictions, dim=1)\n        return predicted\n\n    def _train_evaluate(self, X_train, y_train, X_val, y_val, criterion):\n        \"\"\"Evaluates model during training time, and returns train_loss, train_acc, val_loss, val_acc.\"\"\"\n\n        # set model to evaluation mode\n        self.eval()\n\n        # calculate accuracy and loss of train set and append to lists\n        epoch_train_acc = (self.predict(X_train) == y_train).sum().item() \/ y_train.shape[0]\n        epoch_train_loss = criterion(self(X_train), y_train).item()\n        self.train_acc.append(epoch_train_acc)\n        self.train_loss.append(epoch_train_loss)\n\n        # calculate accuracy and loss of validation set, and append to lists\n        if X_val is not None and y_val is not None:\n            epoch_val_acc = (self.predict(X_val) == y_val).sum().item() \/ y_val.shape[0]\n            epoch_val_loss = criterion(self(X_val), y_val).item()\n            self.val_acc.append(epoch_val_acc)\n            self.val_loss.append(epoch_val_loss)\n\n            # return all loss and accuracy values\n            return epoch_train_loss, epoch_train_acc, epoch_val_loss, epoch_val_acc\n\n        # return train set loss and accuracy values, if there is no validation set\n        return epoch_train_loss, epoch_train_acc, None, None\n    \n    @staticmethod\n    def _print_progress(epoch, train_loss, train_acc, val_loss, val_acc, improved, verbose=False):\n        \"\"\"Prints the training progress.\"\"\"\n\n        output = f'Epoch {str(epoch + 1).zfill(3)}:'\n        output += f'\\n\\t Training   Loss: {str(train_loss)[:5]} | Accuracy: {str(train_acc)[:5]}.'\n\n        if val_loss is not None and val_acc is not None:\n            output += f'\\n\\t Validation Loss: {str(val_loss)[:5]} | Accuracy: {str(val_acc)[:5]}.'\n\n        if improved:\n            output += f' Improvement!'\n\n        if verbose:\n            print(output)\n\n    def fit(self, X_train, y_train, X_val, y_val, epoch_num, criterion, optimizer, verbose=False):\n        \"\"\"Trains the model.\"\"\"\n\n        # a variable to determine whether to update best weights (and report progress)\n        best_acc = 0.0\n\n        # split dataset to batches\n        X_train_tensor_batches = torch.split(X_train, self.batch_size)\n        y_train_tensor_batches = torch.split(y_train, self.batch_size)\n\n        for epoch in range(epoch_num):\n\n            # set model to train mode\n            self.train()\n\n            for i, (X_batch, y_batch) in enumerate(zip(X_train_tensor_batches, y_train_tensor_batches)):\n\n                # Forward pass\n                outputs = self(X_batch)\n                loss = criterion(outputs, y_batch)\n\n                # Backward and optimize\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n            # calculate accuracy and loss of train and validation set (if validation set is None, values are None)\n            train_loss, train_acc, val_loss, val_acc = self._train_evaluate(X_train, y_train, X_val, y_val, criterion)\n\n            # a boolean to determine the correct accuracy to consider for progress (Validation ot Training)\n            if X_val is not None and y_val is not None:\n                accuracy = val_acc\n            else:\n                accuracy = train_acc\n\n            # if accuracy outperforms previous best accuracy, print and update best accuracy and model's best weights\n            if accuracy > best_acc:\n                self._print_progress(epoch, train_loss, train_acc, val_loss, val_acc, improved=True, verbose=verbose)\n                best_acc = accuracy\n                self.best_weights = deepcopy(self.state_dict())\n\n            # else, print\n            else:\n                self._print_progress(epoch, train_loss, train_acc, val_loss, val_acc, improved=False, verbose=verbose)\n\n        gc.collect()","a9abcc99":"def plot_graphs(model):\n    plt.figure(figsize=(6, 12))\n\n    plt.subplot(311)\n    plt.title(\"Accuracy\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Accuracy\")\n    plt.plot(range(1, len(model.train_acc)+1), model.train_acc, label=\"Train\")\n    plt.plot(range(1, len(model.val_acc)+1), model.val_acc, label=\"Validation\")\n\n    plt.xticks(np.arange(0, len(model.train_acc)+1, 5))\n    plt.legend()\n\n    plt.subplot(312)\n    plt.title(\"Loss\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.plot(range(1, len(model.train_loss)+1), model.train_loss, label=\"Train\")\n    plt.plot(range(1, len(model.val_loss)+1), model.val_loss, label=\"Validation\")\n\n    plt.xticks(np.arange(0, len(model.train_acc)+1, 5))\n    plt.legend()\n\n    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n    plt.show()","a4a61cb7":"X_train_val, X_held_out_set, y_train_val, y_held_out_set = train_test_split(\n    embedding_matrix, train_df['target'].values, test_size=0.1)","5943f37f":"# determines which device to mount the model to\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Using device: {device}')","22658aa8":"# convert above LSTM arrays to tensors to be used in BiLSTM Neural Network\nX_train_val = torch.from_numpy(X_train_val).float().to(device)\nX_held_out_set = torch.from_numpy(X_held_out_set).float().to(device)\ny_train_val = torch.from_numpy(y_train_val).long().to(device)\ny_held_out_set = torch.from_numpy(y_held_out_set).long().to(device)","40805dff":"# network hyprer-parameters\nembedding_dim = embedding_matrix.shape[2]\nhidden_size = 50\nnum_layers = 2\nnum_classes = 2\nbatch_size = 256\ndropout = 0.3","8410a74c":"# learning hyprer-parameters\nnum_epochs = 20\nlearning_rate = 0.0005\nweight_decay = 0.0005","6d63a58a":"# instantiate Model, Loss and Optimizer\nbilstm = BiLSTM(embedding_dim, hidden_size, num_layers, num_classes, batch_size, dropout, device).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(bilstm.parameters(), lr=learning_rate, weight_decay=weight_decay)","f32e1ca6":"# train the model\nbilstm.fit(\n    X_train=X_train_val, y_train=y_train_val, X_val=X_held_out_set, y_val=y_held_out_set,\n    epoch_num=num_epochs, criterion=criterion, optimizer=optimizer, verbose=True)","9ee7e36a":"plot_graphs(bilstm)","99b4bdf7":"del(bilstm)","03a5666e":"X_train = torch.from_numpy(embedding_matrix).float().to(device)\ny_train = torch.from_numpy(train_df['target'].values).long().to(device)","cfc4add0":"# instantiate Model, Loss and Optimizer\nbilstm = BiLSTM(embedding_dim, hidden_size, num_layers, num_classes, batch_size, dropout, device).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(bilstm.parameters(), lr=learning_rate, weight_decay=weight_decay)","ce7600ca":"# train the model\nbilstm.fit(\n    X_train=X_train, y_train=y_train, X_val=None, y_val=None,\n    epoch_num=num_epochs, criterion=criterion, optimizer=optimizer, verbose=True)","f1378ec5":"plot_graphs(bilstm)","8cc292cb":"TEST_SET_PATH = '..\/input\/nlp-getting-started\/test.csv'","e6327828":"test_df = pd.read_csv(TEST_SET_PATH)\ntest_df.head()","bc403d49":"# normalize and tokenize texts and keywords\ntest_df['tok_norm_text'] = [preprocess_text(tokenizer, text) for text in test_df['text']]\ntest_df['keyword'] = test_df['keyword'].apply(preprocess_keyword)","461649e4":"# feature extraction\ntest_df['num_all_caps'] = test_df['text'].apply(count_all_caps)\ntest_df['num_caps'] = test_df['text'].apply(count_capitalized)\ntest_df['num_words'] = test_df['text'].apply(count_words)\n\ntest_df[columns_to_scale] = scaler.transform(test_df[columns_to_scale])\n\nsentiment_analyze_df(test_df, 'tok_norm_text')","3c9551f0":"#text embedding\ntest_df['text_embedding'] = [\n    embedding_preprocess(\n        embedding_model=model_glove_twitter, use_norm=True, seq_len=sequence_max_length, tokenized_text=text)\n    for text in test_df['tok_norm_text']\n]","8d1fbb48":"#keyword embedding\ntest_df['keyword_embedding'] = [\n    keyword_to_avg_vector(embedding_model=model_glove_twitter, use_norm=True, tokenized_keyword=keyword)\n    for keyword in test_df['keyword']\n]","fa4b730e":"test_df.head()","432ef91c":"# Create one embedding representation of all chosen features\ntest_embedding_matrix = concatenate_embeddings(\n    df=test_df, embedding_model=model_glove_twitter, seq_len=sequence_max_length,\n    sequence_embedding_col='text_embedding',\n    static_embedding_cols=['keyword_embedding'],\n    static_singles_cols=['num_all_caps', 'num_caps', 'num_words', 'sent_neg', 'sent_neu', 'sent_pos', 'sent_compound'])","6a8c58c7":"X_test = torch.from_numpy(test_embedding_matrix).float().to(device)","e96e20c9":"# predict\npreds = bilstm.predict(X_test)","5574cc62":"# put predictions and id's in DataFrame\nfinal_preds = preds.cpu().numpy().reshape(-1,1)\nids = test_df['id'].values.reshape(-1,1)\ndata = np.hstack((ids, final_preds))\n\nsubmission_df = pd.DataFrame(data=data,columns = ['id', 'target'])","06089e6e":"submission_df.to_csv('submission.csv', index=False)","664f205c":"It seems like most of the texts are shorter than 30 words.<br>\nThus, a reasonable compromise between data loss and computational complexity will be<br>\nchoosing the maximum length of sequences to be 30, i.e. trimming text after the 30'th word.","133a28fb":"# Word embedding functions","828ecfa8":"I used the train-val set to perform 10-fold cross validation to tune the hyper-parameters of the network. <br>\nThe 10-fold cross validation is left out of the notebook for readability and order. <br>\nThe chosen hyper-parameters are as following:","a522548a":"#### Create one embedding representation of all chosen features","ead2b382":"# Pre-processing","73183108":"# Predicting test set","8d247e98":"As we now know our network is doing it's job properly, I train a network again, using all the training-data:","e693760d":"#### Feature engineering","901cd47e":"#### Keyword cleaning","53a24f9b":"# Feature engineering functions","371e9ba9":"#### Cleaning","4508bf7e":"At first, I split the train data to train and test set (the test set is called held-out set):","98e2e061":"The training graph:","a4bfa84c":"# Embedding model preparation","9e64c8a3":"The training graph (note that this time there is no validation data to test on):","8c898f3e":"#### Keyword embedding","e21c6ee3":"This notebook shows a basic pipeline for the NLP task of text classification.<br>\nSpecifically it tries to classify wheter a tweet describes a real or fake disaster.\n\nThe pipeline is composed of:\n* Cleaning and pre-preocessing\n* Feature engineering\n* Pre-trained word-embedding model\n* BiLSTM RNN model","d9c21975":"#### Textual feautres to word embedding representation","7f521523":"# Neural-network creation and utility functions","d40dd2b7":"# Imports","5305f845":"#### Text embedding","f10dd254":"To show the results, I use the train-val data to train the network, and use the held-out to test it:","be9821bc":"# Running the network","17a7a049":"# Cleaning and pre-processing functions","a9085fb0":"#### Text cleaning"}}