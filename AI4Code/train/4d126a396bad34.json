{"cell_type":{"d79ad23c":"code","a8578813":"code","5a9e32be":"code","1470c9bf":"code","9f7c0827":"code","8e1078ab":"code","78918ba3":"code","50c7f8a6":"code","e8413e11":"code","9fcd4034":"code","2e2f75cf":"code","d1162510":"code","3f1a78ff":"code","29865c59":"code","62904e3f":"code","31eea957":"code","54bed888":"code","8eca0d15":"code","3c721c27":"code","5a87b829":"code","ab8f4cc0":"code","9b211e94":"code","ab82405e":"code","d8b259c8":"code","7db0693c":"code","35d363e9":"code","9495e59e":"code","1f4a0f0a":"code","ab2dd13d":"code","63c9ccb3":"code","a79de424":"code","5bc5adbf":"code","a67e49f5":"code","4ead5700":"markdown","cb77e249":"markdown","bdc28d03":"markdown","96f3ea28":"markdown","84a40147":"markdown"},"source":{"d79ad23c":"import numpy as np \nimport pandas as pd \nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a8578813":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","5a9e32be":"train_data=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\nsub_data=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","1470c9bf":"train_data.head()","9f7c0827":"(train_data.isnull()==True).sum()","8e1078ab":"train_data['target'].value_counts()","78918ba3":"train_data[train_data['keyword'].isnull()==False]","50c7f8a6":"ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\ntrain_data[train_data['id'].isin(ids_with_target_error)]","e8413e11":"train_data.loc[train_data['id'].isin(ids_with_target_error),'target'] = 0\ntrain_data[train_data['id'].isin(ids_with_target_error)]","9fcd4034":"train_data.loc[train_data['keyword'].notnull(), 'text'] = train_data['keyword'] + ' ' + train_data['text']\n# test.loc[test['keyword'].notnull(), 'text'] = test['keyword'] + ' ' + test['text']\n\n# view\ntrain_data[train_data['keyword'].notnull()].head()","2e2f75cf":"import string\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import TweetTokenizer\n\ntknzr = TweetTokenizer(strip_handles=True)\nstop_words = set(stopwords.words('english'))\ncorpus = []\n\ndef clean_data(text):\n    # special characters\n    text = re.sub(r\"\\x89\u00db_\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00d2\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00d3\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00cfWhen\", \"When\", text)\n    text = re.sub(r\"\\x89\u00db\u00cf\", \"\", text)\n    text = re.sub(r\"China\\x89\u00db\u00aas\", \"China's\", text)\n    text = re.sub(r\"let\\x89\u00db\u00aas\", \"let's\", text)\n    text = re.sub(r\"\\x89\u00db\u00f7\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00aa\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\\x9d\", \"\", text)\n    text = re.sub(r\"\u00e5_\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00a2\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00a2\u00e5\u00ca\", \"\", text)\n    text = re.sub(r\"from\u00e5\u00cawounds\", \"from wounds\", text)\n    text = re.sub(r\"\u00e5\u00ca\", \"\", text)\n    text = re.sub(r\"\u00e5\u00c8\", \"\", text)\n    text = re.sub(r\"Jap\u00cc_n\", \"Japan\", text)    \n    text = re.sub(r\"\u00cc\u00a9\", \"e\", text)\n    text = re.sub(r\"\u00e5\u00a8\", \"\", text)\n    text = re.sub(r\"Suru\u00cc\u00a4\", \"Suruc\", text)\n    text = re.sub(r\"\u00e5\u00c7\", \"\", text)\n    text = re.sub(r\"\u00e5\u00a33million\", \"3 million\", text)\n    text = re.sub(r\"\u00e5\u00c0\", \"\", text)\n    \n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    # remove numbers\n    text = re.sub(r'[0-9]', '', text)\n    \n    # remove punctuation and special chars (keep '!')\n    for p in string.punctuation.replace('!', ''):\n        text = text.replace(p, '')\n        \n    # remove urls\n    text = re.sub(r'http\\S+', '', text)\n    \n    # tokenize\n    text = tknzr.tokenize(text)\n    \n    # remove stopwords\n    text = [w.lower() for w in text if not w in stop_words]\n    corpus.append(text)\n    \n    # join back\n    text = ' '.join(text)\n    \n    return text\n\n\n   ","d1162510":"# Thanks to https:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert\nabbreviations = {\n    \"$\" : \" dollar \",\n    \"\u20ac\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c\/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n     \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w\/\" : \"with\",\n    \"w\/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}","3f1a78ff":"def convert_abbrev_in_text(text):\n    t=[]\n    words=text.split()\n    t = [abbreviations[w.lower()] if w.lower() in abbreviations.keys() else w for w in words]\n    return ' '.join(t)    ","29865c59":"train_data['text']=train_data['text'].apply(clean_data)\ntrain_data['text']=train_data['text'].apply(convert_abbrev_in_text)","62904e3f":"import numpy as np\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Embedding,Bidirectional,Dropout,BatchNormalization,GlobalMaxPool1D,Input\n# from keras.layers import Embedding, Input, Dense, CuDNNGRU,Bidirectional, Dropout,SpatialDropout1D, Conv1D, GlobalAveragePooling1D, GlobalMaxPooling1D\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","31eea957":"# max_features = 50000\ntokenizer=Tokenizer()\ntokenizer.fit_on_texts(train_data['text'])\nsequences = tokenizer.texts_to_sequences(train_data['text'])\npad=pad_sequences(sequences,maxlen=120,padding='post',truncating='post')","54bed888":"partition=int(len(train_data)*0.8);\ny_train=train_data['target'].iloc[0:partition]\ny_val=train_data['target'].iloc[partition:len(train_data)]\nx_train=pad[0:partition]\nx_val=pad[partition:len(train_data)]\n\nvocab_size=len(tokenizer.word_index)+1","8eca0d15":"#after clear_data function is used \nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 32, input_length=pad.shape[1]))\nmodel.add(Bidirectional(LSTM(32,return_sequences=True)))\nmodel.add(Dropout(0.7))\n\nmodel.add(Bidirectional(LSTM(32,return_sequences=True)))\nmodel.add(Dropout(0.7))\n\nmodel.add(Bidirectional(LSTM(32)))\nmodel.add(Dropout(0.7))\n\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.7))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\nhistory = model.fit(x_train, y_train, epochs=3, validation_data=(x_val, y_val))","3c721c27":"test_data=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ntest_data.loc[test_data['keyword'].notnull(), 'text'] = test_data['keyword'] + ' ' + test_data['text']\ntest_data['text']=test_data['text'].apply(clean_data)\nt_sequences = tokenizer.texts_to_sequences(test_data['text'])\nt_pad=pad_sequences(t_sequences,maxlen=120,padding='post',truncating='post')","5a87b829":"y_pred=model.predict(t_pad)","ab8f4cc0":"sub_file=pd.DataFrame()\nsub_file['id']=test_data['id']\nsub_file['target']=y_pred.round().astype(int)\nsub_file.head()","9b211e94":"sub_file.to_csv('submission.csv', index=False)","ab82405e":"import numpy as np\nembedding_dict={}\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word = values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","d8b259c8":"all_embs = np.stack(embedding_dict.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\nword_index = tokenizer.word_index\nnb_words = len(word_index)+1\n\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n\nfor word, i in word_index.items():\n    embedding_vector = embedding_dict.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","7db0693c":"from keras.initializers import Constant\nmodel_glove = Sequential()\nmodel_glove.add(Embedding(nb_words,embed_size, input_length=pad.shape[1],weights= [embedding_matrix]))\n\nmodel_glove.add(Bidirectional(LSTM(32,return_sequences=True)))\nmodel_glove.add(Dropout(0.7))\nmodel_glove.add(Bidirectional(LSTM(32,return_sequences=True)))\nmodel_glove.add(Dropout(0.7))\nmodel_glove.add(Bidirectional(LSTM(32)))\nmodel_glove.add(Dropout(0.7))\n\nmodel_glove.add(Dense(32, activation='tanh'))\nmodel_glove.add(Dropout(0.7))\n\nmodel_glove.add(Dense(1, activation='sigmoid'))\nmodel_glove.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\nhistory = model_glove.fit(x_train, y_train, epochs=4, validation_data=(x_val, y_val))","35d363e9":"test_data=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ntest_data.loc[test_data['keyword'].notnull(), 'text'] = test_data['keyword'] + ' ' + test_data['text']\ntest_data['text']=test_data['text'].apply(clean_data)\nt_sequences = tokenizer.texts_to_sequences(test_data['text'])\nt_pad=pad_sequences(t_sequences,maxlen=120,padding='post',truncating='post')","9495e59e":"y_glove_pred=model_glove.predict(t_pad)","1f4a0f0a":"sub_glove_file=pd.DataFrame()\nsub_glove_file['id']=test_data['id']\nsub_glove_file['target']=y_glove_pred.round().astype(int)\nsub_glove_file.head()","ab2dd13d":"sub_glove_file.to_csv(\"submission_glove.csv\", index=False)","63c9ccb3":"from IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create a random sample dataframe\n\n# create a link to download the dataframe\ncreate_download_link(sub_file)","a79de424":"#before clear_data function is used \n# model = Sequential()\n# model.add(Embedding(vocab_size, 64, input_length=pad.shape[1]))\n# model.add(Bidirectional(LSTM(200, return_sequences=True)))\n# model.add(LSTM(200))\n# model.add(Dense(50, activation='relu'))\n# model.add(Dense(1, activation='sigmoid'))\n# model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n# history = model.fit(x_train, y_train, epochs=1, validation_data=(x_val, y_val))","5bc5adbf":"# model = Sequential()\n# model.add(Embedding(vocab_size, 64, input_length=pad.shape[1]))\n# model.add(Bidirectional(LSTM(64,return_sequences=True)))\n# model.add(Bidirectional(LSTM(64)))\n# model.add(Dense(50, activation='relu'))\n# model.add(Dropout(0.7))\n# model.add(Dense(1, activation='sigmoid'))\n# model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n# history = model.fit(x_train, y_train, epochs=1, validation_data=(x_val, y_val))","a67e49f5":"# model = Sequential()\n# model.add(Embedding(vocab_size, 64, input_length=pad.shape[1]))\n# model.add(Bidirectional(LSTM(64,return_sequences=True)))\n# model.add(Bidirectional(LSTM(128)))\n# model.add(Dense(50, activation='relu'))\n# model.add(Dropout(0.7))\n# model.add(Dense(1, activation='sigmoid'))\n# model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])","4ead5700":"# **Tokenizing**","cb77e249":"# **Glove Embedding**","bdc28d03":"# **Download submission.csv file **","96f3ea28":"# **Model Architecture**","84a40147":"# **Cleaning**"}}