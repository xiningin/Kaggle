{"cell_type":{"9ac4f6f7":"code","521ff80e":"code","60ae4c4d":"code","d0378f4d":"code","30d6dacc":"code","8a65c471":"code","88f937ab":"code","7163434c":"code","6d12fd03":"code","01411ccf":"code","baf9e334":"code","dfb92197":"code","a0ad8b31":"code","9d1aef94":"code","f0d5b332":"code","9dc4f3b4":"code","b5a0d380":"code","8f70b764":"code","9b3d4007":"code","0865c33f":"code","714cc645":"code","ebe1693c":"code","e65bc8a5":"code","31fac1bf":"code","bb49aac8":"code","bb5af38f":"code","dad436f0":"code","d558fc25":"code","466eba22":"code","8c9fb264":"markdown","44f30d38":"markdown","79a49591":"markdown","76915fcb":"markdown","7127ec5a":"markdown","f93a7fa5":"markdown","0f7a38b8":"markdown","9c83a487":"markdown","bda0fc0d":"markdown","9dd60611":"markdown","a02ca9b4":"markdown"},"source":{"9ac4f6f7":"import re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\n#\u89c2\u5bdf\u524d\u51e0\u884c\u7684\u6e90\u6570\u636e\uff1a\n\ntrain_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')\n\nsns.set_style('whitegrid')\ntrain_data.head()","521ff80e":"# \u5c06name\u8fd9\u4e00feature\u8f6c\u5316\u4e3a title\u5c5e\u6027\ntrain_data['Title'] = train_data['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\ntest_data['Title'] = test_data['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\nprint(train_data['Title'].value_counts(),test_data['Title'].value_counts())","60ae4c4d":"k1 = train_data['Title'].value_counts()\nk2 = test_data['Title'].value_counts()\nk1 = k1.index\nk2 = k2.index\nk2.isin(k1)","d0378f4d":"# \u66ff\u6362\u6389test\u4e2d \u4e0d\u5305\u542b\u7684\u5012\u6570\u7b2c\u4e8c\u4e2a\ntest_data['Title'] = test_data['Title'].replace('Dona','Don')\n\n# \u66ff\u6362\u4e4b\u540e\u8fdb\u884c\u68c0\u6d4b\u662f\u5426\u4e3a\u5168\u90e8\u5305\u542b\nk1 = train_data['Title'].value_counts()\nk2 = test_data['Title'].value_counts()\nk1 = k1.index\nk2 = k2.index\nk2.isin(k1)","30d6dacc":"train_data = train_data.drop(['Name'], axis=1)\ntest_data = test_data.drop(['Name'], axis=1)\ntest_data","8a65c471":"def GET_CR(X):\n    t1 = X.value_counts()\n    t2 =t1.values\n    t2 = np.unique(t2)\n    for kk in t2:\n        m = t1 == kk\n        OVER_CR = t1.index[m]\n        OVER_CR = list(OVER_CR)\n        k2 = map(lambda x:x in OVER_CR, X)\n        ISIN = np.array(list(k2))\n        ISIN = (1 - ISIN).astype(np.bool)\n        y = np.where(ISIN, X, kk)\n        X = pd.Series(y)\n    return y","88f937ab":"kk = pd.DataFrame({'ticket':train_data['Ticket']})\nkk2 = pd.DataFrame({'ticket':test_data['Ticket']})\ntrain_data['Ticket']= kk.apply(GET_CR)\ntest_data['Ticket']= kk2.apply(GET_CR)\nprint(train_data['Ticket'])\nprint(test_data['Ticket'])","7163434c":"# \u56e0\u4e3acabin\u9879\u76ee\u7f3a\u5931\u6bd4\u7b80\u5927 \u76f4\u63a5\u5ffd\u7565cabin\u9879\ntrain_data = train_data.drop(['Cabin'], axis=1)\ntest_data = test_data.drop(['Cabin'], axis=1)\ntest_data","6d12fd03":"# Embarked \u5728\u4e24\u8005\u4e2d\u4e00\u81f4  \nk1 = train_data['Embarked'].value_counts()\nk2 = test_data['Embarked'].value_counts()\nk1 = k1.index\nk2 = k2.index\nk2.isin(k1)","01411ccf":"train_data.info()\ntest_data.info()\n","baf9e334":"# \u5c06 ticket\u8f6c\u5316\u4e3anum\u7c7b\u578b\ntrain_data['Ticket'] = train_data['Ticket'].astype(np.uint8)","dfb92197":"obj = ['Sex','Embarked','Title']\nfor x in obj:\n    embark_dummies  = pd.get_dummies(train_data[x])\n    train_data = train_data.join(embark_dummies)\n    train_data.drop([x], axis=1,inplace=True)\n\n    embark_dummies  = pd.get_dummies(test_data[x])\n    test_data = test_data.join(embark_dummies)\n    test_data.drop([x], axis=1,inplace=True)","a0ad8b31":"from sklearn.ensemble import RandomForestRegressor\n#choose training data to predict age\nage_df = train_data\nage_df_notnull = age_df.loc[(train_data['Age'].notnull())]\nage_df_isnull = age_df.loc[(train_data['Age'].isnull())]\nY = age_df_notnull.values[:,2]\nage_df_notnull = age_df_notnull.drop(['Age'], axis=1)\nX = age_df_notnull.values\n# use RandomForestRegression to train data\nRFR = RandomForestRegressor(n_estimators=1000, n_jobs=-1)\nRFR.fit(X,Y)\n# \u5229\u7528\u5df2\u6709\u7684 \u5e74\u9f84\u4e0d\u4e3a\u7a7a\u7684\u503c \u6765\u5bf9\u5e74\u9f84\u4e3a\u7a7a\u7684\u503c\u8fdb\u884c\u9884\u6d4b\nage_df_isnull.drop(['Age'], axis=1,inplace=True)\npredictAges = RFR.predict(age_df_isnull.values)\n# \u5c06\u539f\u59cb\u6570\u636e\u96c6\u4e2dage\u4e3a0\u7684\u5730\u65b9\u586b\u5145\uff0c\u9884\u6d4b\u503c\u91cd\u65b0\u5199\u5165\u8bad\u7ec3\u96c6\u4e2d\uff0c\u4f7f\u5f97age\u5168\u90e8\u6709\u503c\ntrain_data.loc[train_data['Age'].isnull(), ['Age']]= predictAges\ntrain_data.info()","9d1aef94":"# \u5bf9\u6d4b\u8bd5\u96c6\u7684age\u8fdb\u884c\u8865\u7a7a\u64cd\u4f5c\ntest_data.info()","f0d5b332":"# \u5bf9\u4e3a\u7a7a\u7684fare\u8fdb\u884c\u5747\u503c\u586b\u5145\ntest_data['Fare'][test_data.Fare.isnull()] = test_data['Fare'].mean()","9dc4f3b4":"test_data['Fare'].describe()","b5a0d380":"#choose training data to predict age\nage_df = test_data\nage_df_notnull = age_df.loc[(test_data['Age'].notnull())]\nage_df_isnull = age_df.loc[(test_data['Age'].isnull())]\nY = age_df_notnull.values[:,2]\nage_df_notnull = age_df_notnull.drop(['Age'], axis=1)\nX = age_df_notnull.values\n# use RandomForestRegression to train data\nRFR = RandomForestRegressor(n_estimators=1000, n_jobs=-1)\nRFR.fit(X,Y)\n# \u5229\u7528\u5df2\u6709\u7684 \u5e74\u9f84\u4e0d\u4e3a\u7a7a\u7684\u503c \u6765\u5bf9\u5e74\u9f84\u4e3a\u7a7a\u7684\u503c\u8fdb\u884c\u9884\u6d4b\nage_df_isnull.drop(['Age'], axis=1,inplace=True)\npredictAges = RFR.predict(age_df_isnull.values)\n# \u5c06\u539f\u59cb\u6570\u636e\u96c6\u4e2dage\u4e3a0\u7684\u5730\u65b9\u586b\u5145\uff0c\u9884\u6d4b\u503c\u91cd\u65b0\u5199\u5165\u8bad\u7ec3\u96c6\u4e2d\uff0c\u4f7f\u5f97age\u5168\u90e8\u6709\u503c\ntest_data.loc[test_data['Age'].isnull(), ['Age']]= predictAges\ntest_data.info()","8f70b764":"# Embarked \u5728\u4e24\u8005\u4e2d\u4e00\u81f4  \nk1 = train_data.columns.values \nk2 = test_data.columns.values \n# print(k1,k2)\nk1 = pd.Series(k1)\nk2 = pd.Series(k2)\nk = list(map(lambda x:not(x),k1.isin(k2)))\n# \u83b7\u53d6test\u4e2d\u4e0d\u5b58\u5728\u7684\u5217\u540d\u79f0\uff0c\u5c06\u5176\u52a0\u5165\u5230test_data\u4e2d\uff0c\u5e76\u586b\u5145\u5176\u503c\u4e3a 0 \npieces = [test_data,pd.DataFrame(columns=k1[k])]\ntest_data = pd.concat(pieces).fillna(value=0)","9b3d4007":"test_data.drop(['Survived'], axis=1,inplace=True)\ntest_data.drop(['PassengerId'], axis=1,inplace=True)\ntrain_data.drop(['PassengerId'], axis=1,inplace=True)","0865c33f":"# \u5bf9\u6570\u636e\u8fdb\u884c\u6392\u5e8f\ntrain_data = train_data.sort_index(axis=1, ascending=True)\ntest_data = test_data.sort_index(axis=1, ascending=True)","714cc645":"train_data.insert(0, 'Survived', train_data.pop('Survived'))\ntrain_data","ebe1693c":"train_x = train_data.iloc[:,1:].to_numpy()\ntrain_y = train_data.iloc[:,0].to_numpy().reshape(891,1)\ntest_x = test_data.to_numpy()\ntrain_x.shape,test_x.shape,train_y.shape","e65bc8a5":"# import keras\n# from keras import models\n# from keras import layers\n# #\u6a21\u578b\u914d\u7f6e\n# model = models.Sequential()\n# # 96%  86%\n# model.add(layers.Dense(256,activation='relu',input_shape=(28,)))\n# model.add(layers.Dense(128,activation='relu'))\n# model.add(layers.Dense(64,activation='relu'))\n# model.add(layers.Dense(32,activation='relu'))\n# model.add(layers.Dense(1,activation='sigmoid'))\n\n# x_train = train_x[:800]\n# y_train = train_y[:800]\n# x_test = train_x[800:]\n# y_test = train_y[800:]\n\n# model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n# #\u6a21\u578b\u8bad\u7ec3\uff0c\u8fd4\u56de\u7684history\u4e3a\u5b57\u5178\u7ed3\u6784\uff0c\u8bb0\u5f55\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u7684\u6240\u6709\u6570\u636e\uff0c\u952e\u5305\u542b['acc'\uff0c'val_acc','val_loss'],history.history.keys()\n# history = model.fit(x_train,y_train,epochs=1000,batch_size=64,validation_data=(x_test,y_test))\n\n# # model.fit(x_train, y_train, epochs=5, batch_size=1)\n# loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n# print(loss_and_metrics)  #  [0.6808906934924961, 0.8144329913293373]","31fac1bf":"train_data['Age'] =(train_data['Age'] -train_data['Age'].min())\/ (train_data['Age'].max() - train_data['Age'].min())\ntrain_data['Fare'] =(train_data['Fare'] -train_data['Fare'].min())\/ (train_data['Fare'].max() - train_data['Fare'].min())","bb49aac8":"train_x = train_data.iloc[:,1:].to_numpy()\ntrain_y = train_data.iloc[:,0].to_numpy().reshape(891,1)","bb5af38f":"x_data = train_x\ny_data = train_y\n# print(x_data,'\/n',y_data)","dad436f0":"from sklearn.utils import shuffle\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n\ntf.reset_default_graph()\nx = tf.placeholder(tf.float32, [None,28], name = \"X\") # 12\u4e2a\u7279\u5f81\u6570\u636e\uff0812\u5217\uff09\ny = tf.placeholder(tf.float32, [None,1], name = \"Y\") # 1\u4e2a\u6807\u7b7e\u6570\u636e\uff081\u5217\uff09\n# \u5b9a\u4e49\u4e86\u4e00\u4e2a\u547d\u540d\u7a7a\u95f4\n\nwith tf.name_scope(\"Model\"):\n    # w \u521d\u59cb\u5316\u503c\u4e3ashape=(12,1)\u7684\u968f\u673a\u6570  \u670d\u4ece\u6307\u5b9a\u6b63\u6001\u5206\u5e03\u7684\u5e8f\u5217\n    w = tf.Variable(tf.random_normal([28,1], stddev=0.01), name=\"W\")\n    \n    # b \u521d\u59cb\u5316\u503c\u4e3a 1.0\n    b = tf.Variable(1.0, name=\"b\")\n    \n    # w\u548cx\u662f\u77e9\u9635\u76f8\u4e58\uff0c\u7528matmul,\u4e0d\u80fd\u7528mutiply\u6216\u8005*\n    def model(x, w, b):\n        # return tf.nn.softmax(tf.matmul(x, w) + b)\n        return tf.sigmoid(tf.matmul(x, w) + b)\n\n    # \u9884\u6d4b\u8ba1\u7b97\u64cd\u4f5c\uff0c\u524d\u5411\u8ba1\u7b97\u8282\u70b9\n    pred= model(x, w, b)\n # \u8fed\u4ee3\u8f6e\u6b21\ntrain_epochs = 500 \n\n# \u5b66\u4e60\u7387\nlearning_rate = 0.01 \n# \u5b9a\u4e49\u635f\u5931\u51fd\u6570\nwith tf.name_scope(\"LossFunction\"):\n    loss_function = tf.reduce_mean(tf.reduce_sum(-y*tf.log(pred)-(1-y)*tf.log(1-pred))) #\u5747\u65b9\u8bef\u5dee\n\n    # loss_function = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred),reduction_indices=1)) #\u5747\u65b9\u8bef\u5dee\n    # m = tf.abs(y)>0.5\n    # m = tf.cast(m,dtype=tf.float32)\n    # k = tf.reduce_sum(tf.abs(m - y))\n    # loss_function = k #\u5747\u65b9\u8bef\u5dee\n# \u521b\u5efa\u4f18\u5316\u5668\noptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss_function)  #83.05%\n# optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss_function)\n\n # \u68c0\u67e5\u9884\u6d4b\u7c7b\u522btf.argmax(pred, 1)\u4e0e\u5b9e\u9645\u7c7b\u522btf.argmax(y, 1)\u7684\u5339\u914d\u60c5\u51b5\npred = pred>0.5\npred =tf.cast(pred,dtype=tf.float32)\n\ncorrect_prediction = tf.equal(pred, y)\n# \u51c6\u786e\u7387\uff0c\u5c06\u5e03\u5c14\u503c\u8f6c\u5316\u4e3a\u6d6e\u70b9\u6570\uff0c\u5e76\u8ba1\u7b97\u5e73\u5747\u503c\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) \n\n\nsess = tf.Session()\n# \u5b9a\u4e49\u521d\u59cb\u5316\u53d8\u91cf\u7684\u64cd\u4f5c\ninit = tf.global_variables_initializer()\n\nsess.run(init)\nloss_list = [] # \u7528\u4e8e\u4fdd\u5b58loss\u503c\u7684\u5217\u8868\n\n# \u8bbe\u7f6e\u8bad\u7ec3\u53c2\u6570\ntrain_epochs = 5000 # \u8bad\u7ec3\u8f6e\u6570\nbatch_size = 16  # \u5355\u6b21\u8bad\u7ec3\u6837\u672c\u6570\uff08\u6279\u6b21\u5927\u5c0f\uff09\ntotal_batch= int(len(y_data)\/batch_size)  # \u4e00\u8f6e\u8bad\u7ec3\u6709\u591a\u5c11\u6279\u6b21\ndisplay_step = 100  # \u663e\u793a\u7c92\u5ea6\nlearning_rate= 0.001  # \u5b66\u4e60\u7387\n\n# \u5f00\u59cb\u8bad\u7ec3\nfor epoch in range(train_epochs ):\n    for batch in range(total_batch):\n        xs = x_data[batch*batch_size:(batch+1)*batch_size]\n        ys = y_data[batch*batch_size:(batch+1)*batch_size]# \u8bfb\u53d6\u6279\u6b21\u6570\u636e\n\n        sess.run(optimizer,feed_dict={x: xs,y: ys}) # \u6267\u884c\u6279\u6b21\u8bad\u7ec3\n    \n    #total_batch\u4e2a\u6279\u6b21\u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u4f7f\u7528\u9a8c\u8bc1\u6570\u636e\u8ba1\u7b97\u8bef\u5dee\u4e0e\u51c6\u786e\u7387\uff1b\u9a8c\u8bc1\u96c6\u6ca1\u6709\u5206\u6279   \n    loss,acc = sess.run([loss_function,accuracy],\n                        feed_dict={x: x_data, y: y_data})\n    # \u6253\u5370\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u8be6\u7ec6\u4fe1\u606f\n    if (epoch+1) % display_step == 0:\n        print(\"Train Epoch:\", '%02d' % (epoch+1), \"Loss=\", \"{:.9f}\".format(loss),\\\n              \" Accuracy=\",\"{:.4f}\".format(acc))\n\nprint(\"Train Finished!\")  ","d558fc25":"test_data['Age'] =(test_data['Age'] -test_data['Age'].min())\/ (test_data['Age'].max() - test_data['Age'].min())\ntest_data['Fare'] =(test_data['Fare'] -test_data['Fare'].min())\/ (test_data['Fare'].max() - test_data['Fare'].min())\n\ntest_x = test_data.to_numpy()\ntest_x.shape","466eba22":"pred1 = sess.run(pred,feed_dict={x: test_x})\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')\nk = pd.DataFrame({'PassengerId':test_data['PassengerId'],'Survived':pred1.astype(np.int64).flatten()})\n\nk.to_csv('submission5.csv',index=0)","8c9fb264":"## \u5408\u5e76\u76f8\u540c\u7968\u53f7\u6570  \u4ee5\u7968\u53f7\u6570\u7684\u6570\u91cf\u4f5c\u4e3a\u4ed6\u4eec\u7684\u503c","44f30d38":"# \u5bf9\u6570\u636e\u8fdb\u884c\u9884\u6d4b","79a49591":"## \u7528CNN\u8fdb\u884c\u5efa\u6a21","76915fcb":"# 1.\u6570\u636e\u6e05\u6d17\n## 1.1 \u5c06\u6570\u636e\u5168\u90e8\u8f6c\u5316\u4e3anum","7127ec5a":"### \u67e5\u770b\u8bad\u7ec3\u96c6\u4e0e\u9a8c\u8bc1\u96c6\u4e2d\u7684title\u662f\u5426\u4e3a\u5305\u542b\u5173\u7cfb","f93a7fa5":"## \u8fdb\u884c\u5f52\u4e00\u5316","0f7a38b8":"## \u586b\u5145test\u4e2d\u7684age","9c83a487":"## \u4e0d\u8fdb\u884c\u5f52\u4e00\u5316","bda0fc0d":"# \u5bf9\u7f3a\u5931\u7684\u6570\u636e\u8fdb\u884c\u5904\u7406","9dd60611":"## \u5c06 Name\u3001Sex\u3001Ticket\u3001Cabin\u3001Embarked \u8f6c\u5316\u4e3a\u6570\u5b57\u7c7b\u578b","a02ca9b4":"## \u5bf9\u7f3a\u5931\u7684age\u8fdb\u884c\u586b\u5145"}}