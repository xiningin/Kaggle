{"cell_type":{"a6c97469":"code","4cebeaf8":"code","35af5e25":"code","feb69cdb":"code","b29877cf":"code","33c2ec4e":"code","1791177b":"code","7f72b0cb":"code","0a651e7e":"code","8c23694b":"code","1e1a1f40":"code","8b9dfbde":"code","74afe77e":"code","602dfce9":"code","f67ed317":"code","fe62981d":"code","35140864":"code","3c283452":"code","4661da9f":"code","aaf48ed2":"code","a25ca172":"markdown","0acad51a":"markdown","ae7b7598":"markdown","7d37eb5a":"markdown","2abafc10":"markdown"},"source":{"a6c97469":"%%time\n\n!pip install --upgrade mxnet-cu100\n!pip install autogluon","4cebeaf8":"import gc\nimport os\nimport shutil\nimport pandas as pd\nfrom autogluon.tabular import TabularDataset, TabularPredictor\n\ntrain_data = TabularDataset('..\/input\/tabular-playground-series-jul-2021\/train.csv')\ntest_data = TabularDataset('..\/input\/tabular-playground-series-jul-2021\/test.csv')\nsubmit = TabularDataset('..\/input\/tabular-playground-series-jul-2021\/sample_submission.csv')","35af5e25":"train_data.head(5)","feb69cdb":"test_data.head(5)","b29877cf":"cols = train_data.columns.tolist()\ncols.remove('target_carbon_monoxide')\ncols.remove('target_benzene')\ncols.remove('target_nitrogen_oxides')\n\nX = train_data[cols]\ny1 = train_data['target_carbon_monoxide']\ny2 = train_data['target_benzene']\ny3 = train_data['target_nitrogen_oxides']\n\ntrain_data1 = pd.concat([X,y1],axis=1)\ntrain_data2 = pd.concat([X,y2],axis=1)\ntrain_data3 = pd.concat([X,y3],axis=1)\n\ntrain_data1.shape, train_data2.shape, train_data3.shape","33c2ec4e":"# Fit AutoGluon on the data, using the 'target' column as the label.\n\ntarget = 'target_carbon_monoxide'\nfit_args = {}\n\n# If you want to speed up training, exclude neural network models via:\nfit_args['excluded_model_types'] = ['NN', 'FASTAI']\n\npredictor1 = TabularPredictor(label=target, eval_metric='rmse').fit(train_data1, time_limit = 60*60\/3, presets='best_quality', auto_stack=True, \n                                                                   keep_only_best=True, save_space=True, **fit_args, verbosity=0)\n\npredictor1.leaderboard(silent=True, extra_info=False)","1791177b":"# Fit AutoGluon on the data, using the 'target' column as the label.\n\ntarget = 'target_benzene'\nfit_args = {}\n\n# If you want to speed up training, exclude neural network models via:\nfit_args['excluded_model_types'] = ['NN', 'FASTAI']\n\npredictor2 = TabularPredictor(label=target, eval_metric='rmse').fit(train_data2, time_limit = 60*60\/3, presets='best_quality', auto_stack=True, \n                                                                   keep_only_best=True, save_space=True, **fit_args, verbosity=0)\n\npredictor2.leaderboard(silent=True, extra_info=False)","7f72b0cb":"# Fit AutoGluon on the data, using the 'target' column as the label.\n\ntarget = 'target_nitrogen_oxides'\nfit_args = {}\n\n# If you want to speed up training, exclude neural network models via:\nfit_args['excluded_model_types'] = ['NN', 'FASTAI']\n\npredictor3 = TabularPredictor(label=target, eval_metric='rmse').fit(train_data3, time_limit = 60*60\/3, presets='best_quality', auto_stack=True, \n                                                                   keep_only_best=True, save_space=True, **fit_args, verbosity=0)\n\npredictor3.leaderboard(silent=True, extra_info=False)","0a651e7e":"submit['target_carbon_monoxide'] = predictor1.predict(test_data)\nsubmit['target_benzene'] = predictor2.predict(test_data)\nsubmit['target_nitrogen_oxides'] = predictor3.predict(test_data)","8c23694b":"submit.head()","1e1a1f40":"submit.to_csv('submission.csv',index=False)","8b9dfbde":"shutil.rmtree('AutogluonModels')\n\ndel predictor1\ndel predictor2\ndel predictor3\n\ngc.collect()","74afe77e":"!pip install -U lightautoml","602dfce9":"# Standard python libraries\nimport os\nimport time\n\n# Essential DS libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_log_error\nimport matplotlib.pyplot as plt\nimport torch\n\n# LightAutoML presets, task and report generation\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\nfrom lightautoml.tasks import Task\nfrom lightautoml.dataset.roles import DatetimeRole\nfrom lightautoml.report.report_deco import ReportDeco","f67ed317":"N_THREADS = 4\nN_FOLDS = 5\nRANDOM_STATE = 42\nTIMEOUT = 60*60\nTARGET_NAME = 'target'","fe62981d":"np.random.seed(RANDOM_STATE)\ntorch.set_num_threads(N_THREADS)","35140864":"train_data = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/train.csv')\ntest_data = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/test.csv')\nsample_sub = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/sample_submission.csv')\n\ndef rmsle_metric(y_true, y_pred, **kwargs):\n    return mean_squared_log_error(y_true, np.clip(y_pred, 0, None), **kwargs) ** 0.5\n\ntask = Task('reg', loss = 'rmsle', metric = rmsle_metric)\n\ntargets_and_drop = {\n    'target_carbon_monoxide': ['target_benzene', 'target_nitrogen_oxides'],\n    'target_benzene': ['target_carbon_monoxide', 'target_nitrogen_oxides'],\n    'target_nitrogen_oxides': ['target_carbon_monoxide', 'target_benzene']\n}\n\nroles = {\n    DatetimeRole(base_date=False, base_feats=True, seasonality=('d', 'wd', 'hour')): 'date_time'\n}\n\nimportances = {}\ndt = pd.to_datetime(train_data['date_time'])\nfor targ in targets_and_drop:\n    print('='*50, '='*50, sep = '\\n')\n    automl = TabularAutoML(task = task, \n                           timeout = TIMEOUT,\n                           cpu_limit = N_THREADS,\n                           reader_params = {'n_jobs': N_THREADS, 'cv': N_FOLDS, 'random_state': RANDOM_STATE},\n                           general_params={'use_algos': [['lgb', 'lgb_tuned', 'cb', 'cb_tuned']]}\n                          )\n\n    roles['target'] = targ\n    roles['drop'] = targets_and_drop[targ]\n    \n    if targ == 'target_nitrogen_oxides':\n        oof_pred = automl.fit_predict(train_data[dt >= np.datetime64('2010-09-01')], roles = roles)\n    else:\n        oof_pred = automl.fit_predict(train_data, roles = roles)\n    print('oof_pred:\\n{}\\nShape = {}'.format(oof_pred, oof_pred.shape))\n    \n    # Fast feature importances calculation\n    fast_fi = automl.get_feature_scores('fast')\n    importances[targ] = fast_fi\n    \n    test_pred = automl.predict(test_data)\n    print('Prediction for te_data:\\n{}\\nShape = {}'.format(test_pred, test_pred.shape))\n    \n    sample_sub[targ] = np.clip(test_pred.data[:, 0], 0, None)","3c283452":"submit = sample_sub.copy()\nsubmit_final = sample_sub.copy()\n\ncols = sample_sub.columns.tolist()\ncols.remove('date_time')\n\nfor i in cols:\n    submit_final[i] = np.mean((submit[i].values, sample_sub[i].values), axis=0)","4661da9f":"submit_final.head(2)","aaf48ed2":"submit_final.to_csv('submission.csv',index=False)","a25ca172":"**Making predictions with the best models trained so far.**","0acad51a":"Some pointers to note about AutoGluon:\n1. You can specify the metric that you want to track. As our evaluation metric is **RMSLE**, but since it is not in the AutoGluon library, we will consider **RMSE** as our metric which can be specified in the <code>eval_metric<\/code> argument.\n2. You can specify which models to fit. Not specifying will iterate over all algorithms in the library.\n3. You can also specify which models to exclude. Models like Neural Networks may take relatively longer to train.\n4. It is very important to specify the time limits. Specifying a time limit of **~2 hours** for each model should be best since the Kaggle run-time limit is **9 hours** and the kernel shall take some time in making predictions beyond 6 hours of training.\n5. Models will run on CPU. **AutoGluon in currently not GPU-compatible**, so don't waste your GPU run-time keeping it on!\n    ","ae7b7598":"### AutoGluon - AutoML framework\n\nAutoGluon is built upon the emphasis of ensembling over hyperparameter tuning. Typically, in order to improve model performance, we can either pursue hyperparameter tuning in order to find the best set of hyperparameters corresponding to data or we can pursue model ensembling - bagging, boosting and stacking.\n\nHowever, performing an exhaustive search among a large space of hyperparameters can be highly time-consuming. At the same time, if your training data changes, the best set of hyperparameters you found out may no longer be the best, and so you would have to find them again.\n\nThis is the reason why AutoGluon focuses on building highly stacked ensembles, believing that you can still achieve optimal model performances without tuning hyperparameters at all.\n\nTutorials: https:\/\/auto.gluon.ai\/dev\/tutorials\/tabular_prediction\/index.html\n\nGitHub: https:\/\/github.com\/awslabs\/autogluon\/","7d37eb5a":"**In order to get best predictions, we need to train on 100% of data.** By default, AutoGluon splits your data as 80\/20 (train\/validation), [reference](https:\/\/auto.gluon.ai\/dev\/tutorials\/image_prediction\/kaggle.html#automatic-training-validation-split). So, you can choose to refit the best model based on validation score to fit on complete data (train+validation) using the <code>set_best_to_refit_full=True<\/code> argument, [reference](https:\/\/auto.gluon.ai\/api\/autogluon.task.html#:~:text=enable%20this%20functionality.-,set_best_to_refit_full,-bool%2C%20default%20%3D%20False).\n\nSome pointers about fit arguments:\n\n1. AutoGluon ensures that the model **predictions made later are with the best model trained in the fitting history**. Nonetheless, we are also explicitly specifying to keep the best model with <code>keep_only_best<\/code> argument.\n2. We have also allowed for stacking using the <code>auto_stack<\/code>. This shall take considerably longer but should also give better predictive performance.\n3. We will also delete all the unused models while keeping the best models to save space, using the <code>save_space<\/code> argument.\n\nFor more information about other arguments, please look at the documentation: https:\/\/auto.gluon.ai\/api\/autogluon.task.html","2abafc10":"### LightAutoML"}}