{"cell_type":{"2b19138d":"code","6d23fd9f":"code","76113735":"code","4f2cddb8":"code","d64e9cbb":"code","3afebfad":"code","1ca341f6":"code","82e7dde0":"code","27f74bb9":"code","d62bce0e":"code","2f32a77a":"code","eba8ea18":"code","dd6dced5":"code","e30f0d6f":"code","d3aa18c7":"code","ce0e5720":"code","09465ea4":"code","a8fc3a96":"code","78a86f71":"code","1ceed35b":"code","a09d3674":"code","6447963a":"code","32b2fdf5":"code","5ff82c3d":"code","ee0590d8":"code","3dad9877":"markdown","83f690f9":"markdown"},"source":{"2b19138d":"#Importing Libraries\nimport re\nimport string\nimport numpy as np\nimport pandas as pd\nimport random\nimport missingno\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import TransformerMixin\nfrom sklearn.metrics import accuracy_score, plot_confusion_matrix, classification_report, confusion_matrix\nfrom wordcloud import WordCloud\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\nfrom sklearn.svm import SVC","6d23fd9f":"#Reading dataset\ndata = pd.read_csv('..\/input\/fake-job-postings\/fake_job_postings.csv')","76113735":"#Shape of the dataset\ndata.shape","4f2cddb8":"#Head of the dataset\ndata.head()","d64e9cbb":"data.interpolate(inplace=True)\ndata.isnull().sum()","3afebfad":"#Fill NaN values with blank in the dataset\ncolumns=['job_id', 'telecommuting', 'has_company_logo', 'has_questions', 'salary_range', 'employment_type']\nfor col in columns:\n    del data[col]\n\ndata.fillna(' ', inplace=True)","1ca341f6":"data.head()","82e7dde0":"#Fraud and Real visualization\nsns.countplot(data.fraudulent).set_title('Real & Fradulent')\ndata.groupby('fraudulent').count()['title'].reset_index().sort_values(by='title',ascending=False)","27f74bb9":"#Visualize job postings by countries\ndef split(location):\n    l = location.split(',')\n    return l[0]\n\ndata['country'] = data.location.apply(split)\n\ncountry = dict(data.country.value_counts()[:11])\ndel country[' ']\nplt.figure(figsize=(8,6))\nplt.title('Country-wise Job Posting', size=20)\nplt.bar(country.keys(), country.values())\nplt.ylabel('No. of jobs', size=10)\nplt.xlabel('Countries', size=10)","d62bce0e":"#Visualize the required experiences in the jobs\nexperience = dict(data.required_experience.value_counts())\ndel experience[' ']\nplt.figure(figsize=(10,5))\nplt.bar(experience.keys(), experience.values())\nplt.title('No. of Jobs with Experience')\nplt.xlabel('Experience', size=10)\nplt.ylabel('No. of jobs', size=10)\nplt.xticks(rotation=35)\nplt.show()","2f32a77a":"#Most frequent jobs\nprint(data.title.value_counts()[:10])","eba8ea18":"#Titles and count of fraudulent jobs\nprint(data[data.fraudulent==1].title.value_counts()[:10])","dd6dced5":"#Titles and count of real jobs\nprint(data[data.fraudulent==0].title.value_counts()[:10])","e30f0d6f":"#combine text in a single column to start cleaning our data\ndata['text']=data['title']+' '+data['location']+' '+data['company_profile']+' '+data['description']+' '+data['requirements']+' '+data['benefits']\ndel data['title']\ndel data['location']\ndel data['department']\ndel data['company_profile']\ndel data['description']\ndel data['requirements']\ndel data['benefits']\ndel data['required_experience']\ndel data['required_education']\ndel data['industry']\ndel data['function']\ndel data['country']","d3aa18c7":"data.head()","ce0e5720":"#Separate fraud and actual jobs\nfraudjobs_text = data[data.fraudulent==1].text\nactualjobs_text = data[data.fraudulent==0].text","09465ea4":"#Fradulent jobs wordcloud\nSTOPWORDS = spacy.lang.en.stop_words.STOP_WORDS\nplt.figure(figsize = (16,14))\nwc = WordCloud(min_font_size = 3,  max_words = 3000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(str(\" \".join(fraudjobs_text)))\nplt.imshow(wc,interpolation = 'bilinear')","a8fc3a96":"#Actual jobs wordcloud\nplt.figure(figsize = (16,14))\nwc = WordCloud(min_font_size = 3,  max_words = 3000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(str(\" \".join(actualjobs_text)))\nplt.imshow(wc,interpolation = 'bilinear')","78a86f71":"#Cleaning and preprocessing\n# Create our list of punctuation marks\npunctuations = string.punctuation\n\n# Create our list of stopwords\nnlp = spacy.load('en')\nstop_words = spacy.lang.en.stop_words.STOP_WORDS\n\n# Load English tokenizer, tagger, parser, NER and word vectors\nparser = English()\n\n# Creating our tokenizer function\ndef spacy_tokenizer(sentence):\n    # Creating our token object, which is used to create documents with linguistic annotations.\n    mytokens = parser(sentence)\n\n    # Lemmatizing each token and converting each token into lowercase\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n\n    # Removing stop words\n    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n\n    # return preprocessed list of tokens\n    return mytokens","1ceed35b":"# Custom transformer using spaCy\nclass predictors(TransformerMixin):\n    def transform(self, X, **transform_params):\n        # Cleaning Text\n        return [clean_text(text) for text in X]\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def get_params(self, deep=True):\n        return {}\n\n# Basic function to clean the text\ndef clean_text(text):\n    # Removing spaces and converting text into lowercase\n    return text.strip().lower()","a09d3674":"# Splitting dataset in train and test\nX_train, X_test, y_train, y_test = train_test_split(data.text, data.fraudulent, test_size=0.3)","6447963a":"#Train-test shape\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","32b2fdf5":"#Support Vector Machine Classifier\n\n# Create pipeline using Bag of Words\npipe = Pipeline([(\"cleaner\", predictors()),\n                 ('vectorizer', CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,3))),\n                 ('classifier', SVC())])\n\n#Training the model.\npipe.fit(X_train,y_train)","5ff82c3d":"# Predicting with a test dataset\ny_pred = pipe.predict(X_test)\n\n# Model Accuracy\nprint(\"Classification Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"Classification Report\\n\")\nprint(classification_report(y_test, y_pred))\nprint(\"Confusion Matrix\\n\")\nprint(confusion_matrix(y_test, y_pred))","ee0590d8":"fig, ax = plt.subplots(figsize=(10, 10))\nplot_confusion_matrix(pipe, X_test, y_test, values_format=' ', ax=ax) \nplt.title('Confusion Matrix')\nplt.show()","3dad9877":"In this implementation, we will train the machine learning classifier on Employment Scam Aegean Dataset (EMSCAD) to identify the fake job advertisements. First, we will visualize the insights from the fake and real job advertisement and then we will use the Support Vector Classifier in this task which will predict the real and fraudulent class labels for the job advertisements after successful training. Finally, we will evaluate the performance of our classifier using several evaluation metrics. ","83f690f9":"# Classifying Fake and Real Job Advertisements using Machine Learning"}}