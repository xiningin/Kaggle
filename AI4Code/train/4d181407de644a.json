{"cell_type":{"2bcd325a":"code","ce4fcc71":"code","c00aa5cb":"code","d3ebab12":"code","85021f1f":"code","54e56423":"code","54e90ca0":"code","c79c0a8d":"code","f5c666e5":"code","9eefac10":"code","62a5d36e":"code","2870e271":"code","3f4952e5":"code","14a4f937":"code","eb60ada2":"code","18a62adc":"code","ac3a8b91":"code","2f3417f4":"code","6ee6314a":"code","6932e74f":"code","68851074":"code","116ed7a8":"code","a70fe374":"code","fb74ced4":"code","6d4e7998":"code","cfc17cb6":"code","8a3744f7":"code","43c1a409":"code","e3f4441c":"code","1d8ecbd6":"code","9219df86":"code","33addca2":"code","a5557501":"code","1b906b99":"code","a7da1038":"code","b7608e1a":"code","11c13165":"code","7aefd29e":"code","34739c8d":"code","9682f4f6":"code","52629f40":"code","18a35cb4":"markdown","31695ae6":"markdown","a04ee5e1":"markdown","18414e22":"markdown","7a8abfb7":"markdown","4d07d668":"markdown","16d933cb":"markdown","01a9b9ba":"markdown","b7c324b6":"markdown","7277158f":"markdown","1079cd03":"markdown","8ffb3abb":"markdown","915df2a3":"markdown","ba17dc51":"markdown","357567db":"markdown","bd23ae5d":"markdown","28516cdc":"markdown","0182f15b":"markdown","169636fe":"markdown","bfaf098e":"markdown","7a7e423f":"markdown","07b5d269":"markdown","74cc3371":"markdown"},"source":{"2bcd325a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ce4fcc71":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nfrom keras.layers import Dense, BatchNormalization, Dropout, LSTM\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom keras.optimizers import Adam\nfrom tensorflow.keras import regularizers\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\nfrom keras import callbacks\n\nnp.random.seed(0)","c00aa5cb":"data = pd.read_csv(\"..\/input\/weather-dataset-rattle-package\/weatherAUS.csv\")\ndata.head()","d3ebab12":"data.info()","85021f1f":"#first of all let us evaluate the target and find out if our data is imbalanced or not\ncols= [\"#C2C4E2\",\"#EED4E5\"]\nsns.countplot(x= data[\"RainTomorrow\"], palette= cols)","54e56423":"# Correlation amongst numeric attributes\ncorrmat = data.corr()\ncmap = sns.diverging_palette(260,-10,s=50, l=75, n=6, as_cmap=True)\nplt.subplots(figsize=(18,18))\nsns.heatmap(corrmat,cmap= cmap,annot=True, square=True)","54e90ca0":"#Parsing datetime\n#exploring the length of date objects\nlengths = data[\"Date\"].str.len()\nlengths.value_counts()","c79c0a8d":"#There don't seem to be any error in dates so parsing values into datetime\ndata['Date']= pd.to_datetime(data[\"Date\"])\n#Creating a collumn of year\ndata['year'] = data.Date.dt.year\n\n# function to encode datetime into cyclic parameters. \n#As I am planning to use this data in a neural network I prefer the months and days in a cyclic continuous feature. \n\ndef encode(data, col, max_val):\n    data[col + '_sin'] = np.sin(2 * np.pi * data[col]\/max_val)\n    data[col + '_cos'] = np.cos(2 * np.pi * data[col]\/max_val)\n    return data\n\ndata['month'] = data.Date.dt.month\ndata = encode(data, 'month', 12)\n\ndata['day'] = data.Date.dt.day\ndata = encode(data, 'day', 31)\n\ndata.head()","f5c666e5":"# roughly a year's span section \nsection = data[:360] \ntm = section[\"day\"].plot(color=\"#800080\")\ntm.set_title(\"Distribution Of Days Over Year\")\ntm.set_ylabel(\"Days In month\")\ntm.set_xlabel(\"Days In Year\")","9eefac10":"cyclic_month = sns.scatterplot(x=\"month_sin\",y=\"month_cos\",data=data, color=\"#C2C4E2\")\ncyclic_month.set_title(\"Cyclic Encoding of Month\")\ncyclic_month.set_ylabel(\"Cosine Encoded Months\")\ncyclic_month.set_xlabel(\"Sine Encoded Months\")","62a5d36e":"cyclic_day = sns.scatterplot(x='day_sin',y='day_cos',data=data, color=\"#C2C4E2\")\ncyclic_day.set_title(\"Cyclic Encoding of Day\")\ncyclic_day.set_ylabel(\"Cosine Encoded Day\")\ncyclic_day.set_xlabel(\"Sine Encoded Day\")","2870e271":"# Get list of categorical variables\ns = (data.dtypes == \"object\")\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","3f4952e5":"# Missing values in categorical variables\n\nfor i in object_cols:\n    print(i, data[i].isnull().sum())","14a4f937":"# Filling missing values with mode of the column in value\n\nfor i in object_cols:\n    data[i].fillna(data[i].mode()[0], inplace=True)","eb60ada2":"# Get list of neumeric variables\nt = (data.dtypes == \"float64\")\nnum_cols = list(t[t].index)\n\nprint(\"Neumeric variables:\")\nprint(num_cols)","18a62adc":"# Missing values in numeric variables\n\nfor i in num_cols:\n    print(i, data[i].isnull().sum())","ac3a8b91":"# Filling missing values with median of the column in value\n\nfor i in num_cols:\n    data[i].fillna(data[i].median(), inplace=True)\n    \ndata.info()","2f3417f4":"#plotting a lineplot rainfall over years\nplt.figure(figsize=(12,8))\nTime_series=sns.lineplot(x=data['Date'].dt.year,y=\"Rainfall\",data=data,color=\"#C2C4E2\")\nTime_series.set_title(\"Rainfall Over Years\")\nTime_series.set_ylabel(\"Rainfall\")\nTime_series.set_xlabel(\"Years\")","6ee6314a":"#Evauating Wind gust speed over years\ncolours = [\"#D0DBEE\", \"#C2C4E2\", \"#EED4E5\", \"#D1E6DC\", \"#BDE2E2\"]\nplt.figure(figsize=(12,8))\nDays_of_week=sns.barplot(x=data['Date'].dt.year,y=\"WindGustSpeed\",data=data, ci =None,palette = colours)\nDays_of_week.set_title(\"Wind Gust Speed Over Years\")\nDays_of_week.set_ylabel(\"WindGustSpeed\")\nDays_of_week.set_xlabel(\"Year\")","6932e74f":"# Apply label encoder to each column with categorical data\nlabel_encoder = LabelEncoder()\nfor i in object_cols:\n    data[i] = label_encoder.fit_transform(data[i])\n    \ndata.info()","68851074":"# Prepairing attributes of scale data\n\nfeatures = data.drop(['RainTomorrow', 'Date','day', 'month'], axis=1) # dropping target and extra columns\n\ntarget = data['RainTomorrow']\n\n#Set up a standard scaler for the features\ncol_names = list(features.columns)\ns_scaler = preprocessing.StandardScaler()\nfeatures = s_scaler.fit_transform(features)\nfeatures = pd.DataFrame(features, columns=col_names) \n\nfeatures.describe().T","116ed7a8":"#Detecting outliers\n#looking at the scaled features\ncolours = [\"#D0DBEE\", \"#C2C4E2\", \"#EED4E5\", \"#D1E6DC\", \"#BDE2E2\"]\nplt.figure(figsize=(20,10))\nsns.boxenplot(data = features,palette = colours)\nplt.xticks(rotation=90)\nplt.show()","a70fe374":"#full data for \nfeatures[\"RainTomorrow\"] = target\n\n#Dropping with outlier\n\nfeatures = features[(features[\"MinTemp\"]<2.3)&(features[\"MinTemp\"]>-2.3)]\nfeatures = features[(features[\"MaxTemp\"]<2.3)&(features[\"MaxTemp\"]>-2)]\nfeatures = features[(features[\"Rainfall\"]<4.5)]\nfeatures = features[(features[\"Evaporation\"]<2.8)]\nfeatures = features[(features[\"Sunshine\"]<2.1)]\nfeatures = features[(features[\"WindGustSpeed\"]<4)&(features[\"WindGustSpeed\"]>-4)]\nfeatures = features[(features[\"WindSpeed9am\"]<4)]\nfeatures = features[(features[\"WindSpeed3pm\"]<2.5)]\nfeatures = features[(features[\"Humidity9am\"]>-3)]\nfeatures = features[(features[\"Humidity3pm\"]>-2.2)]\nfeatures = features[(features[\"Pressure9am\"]< 2)&(features[\"Pressure9am\"]>-2.7)]\nfeatures = features[(features[\"Pressure3pm\"]< 2)&(features[\"Pressure3pm\"]>-2.7)]\nfeatures = features[(features[\"Cloud9am\"]<1.8)]\nfeatures = features[(features[\"Cloud3pm\"]<2)]\nfeatures = features[(features[\"Temp9am\"]<2.3)&(features[\"Temp9am\"]>-2)]\nfeatures = features[(features[\"Temp3pm\"]<2.3)&(features[\"Temp3pm\"]>-2)]\n\n\nfeatures.shape","fb74ced4":"#looking at the scaled features without outliers\n\nplt.figure(figsize=(20,10))\nsns.boxenplot(data = features,palette = colours)\nplt.xticks(rotation=90)\nplt.show()","6d4e7998":"X = features.drop([\"RainTomorrow\"], axis=1)\ny = features[\"RainTomorrow\"]\n\n# Splitting test and training sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\nX.shape","cfc17cb6":"#Early stopping\nearly_stopping = callbacks.EarlyStopping(\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=20, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)\n\n# Initialising the NN\nmodel = Sequential()\n\n# layers\n\nmodel.add(Dense(units = 32, kernel_initializer = 'uniform', activation = 'relu', input_dim = 26))\nmodel.add(Dense(units = 32, kernel_initializer = 'uniform', activation = 'relu'))\nmodel.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n# Compiling the ANN\nopt = Adam(learning_rate=0.00009)\nmodel.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Train the ANN\nhistory = model.fit(X_train, y_train, batch_size = 32, epochs = 150, callbacks=[early_stopping], validation_split=0.2)","8a3744f7":"history_df = pd.DataFrame(history.history)\n\nplt.plot(history_df.loc[:, ['loss']], \"#BDE2E2\", label='Training loss')\nplt.plot(history_df.loc[:, ['val_loss']],\"#C2C4E2\", label='Validation loss')\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(loc=\"best\")\n\nplt.show()","43c1a409":"history_df = pd.DataFrame(history.history)\n\nplt.plot(history_df.loc[:, ['accuracy']], \"#BDE2E2\", label='Training accuracy')\nplt.plot(history_df.loc[:, ['val_accuracy']], \"#C2C4E2\", label='Validation accuracy')\n\nplt.title('Training and Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","e3f4441c":"# Predicting the test set results\ny_pred = model.predict(X_test)\ny_pred = (y_pred > 0.5)","1d8ecbd6":"# confusion matrix\ncmap1 = sns.diverging_palette(260,-10,s=50, l=75, n=5, as_cmap=True)\nplt.subplots(figsize=(12,8))\ncf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(cf_matrix\/np.sum(cf_matrix), cmap = cmap1, annot = True, annot_kws = {'size':15})","9219df86":"print(classification_report(y_test, y_pred))","33addca2":"# uncomment to pycaret\n#!pip install pycaret","a5557501":"train = pd.concat([X_train, y_train], axis=1)\ntest = pd.concat([X_test, y_test], axis=1)","1b906b99":"train.head()","a7da1038":"from pycaret.classification import *","b7608e1a":"exp = setup(data =train, target='RainTomorrow')","11c13165":"compare_models()","7aefd29e":"catboost = create_model('catboost')","34739c8d":"tuned_model = tune_model(catboost, early_stopping=True)","9682f4f6":"predictions  = predict_model(catboost, test)","52629f40":"predictions.head()","18a35cb4":"**Numerical variables**\n\n* Filling missing values with median of the column value","31695ae6":"# Pycaret Approach","a04ee5e1":"# DATA PREPROCESSING\n\n**Steps involved in Data Preprocessing:**\n\n* Label encoding columns with categorical data\n* Perform the scaling of the features\n* Detecting outliers\n* Dropping the outliers based on data analysis","18414e22":"**Categorical variables**\n\n* Filling missing values with mode of the column value","7a8abfb7":"# <h1 style='background:#C2C4E2; border:0; color:black'><center>RAIN PREDICTION<\/center><\/h1> \n![Memphis%20Stop%20Start%20Continue%20Brainstorm%20Presentation%20%281%29.png](attachment:Memphis%20Stop%20Start%20Continue%20Brainstorm%20Presentation%20%281%29.png)\n\n   <a id='top'><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 style='background:#C2C4E2; border:0; color:black'><center>TABLE OF CONTENTS<\/center><\/h1>\n\n[1. IMPORTING LIBRARIES](#1)\n    \n[2. LOADING DATA](#2)    \n\n[3. DATA VISUALIZATION AND CLEANINGS](#3)     \n\n[4. DATA PREPROCESSING](#4)     \n\n[5. MODEL BUILDING](#5) \n\n[6. CONCLUSION](#6) \n\n[7. END](#7) \n\n\n# IMPORTING LIBRARIES","4d07d668":"# CONCLUSIONS\n\n\n**Concluding the model with:**\n\n* Testing on the test set\n* Evaluating the confusion matrix\n* Evaluating the classification report","16d933cb":"As expected, the \"year\" attribute of data repeats. However in this for the true cyclic nature is not presented in a continuous manner. Splitting months and days into Sine and cosine combination provides the cyclical continuous feature. This can be used as input features to ANN. ","01a9b9ba":"**Now I will parse Dates into datetime.**\n\nMy goal is to build an artificial neural network(ANN). I will encode dates appropriately, i.e. I prefer the months and days in a cyclic continuous feature. As, date and time are inherently cyclical. To let the ANN model know that a feature is cyclical I split it into periodic subsections. Namely, years, months and days. Now for each subsection, I create two new features, deriving a sine transform and cosine transform of the subsection feature. ","b7c324b6":"# LOADING DATA","7277158f":"Tuning the model for better perfformance","1079cd03":"# MODEL BUILDING\n\n\n**In this project, we build an artificial neural network.**\n\n**Following steps are involved in the model building**\n\n* Assining X and y the status of attributes and tags\n* Splitting test and training sets\n* Initialising the neural network\n* Defining by adding layers\n* Compiling the neural network\n* Train the neural network","8ffb3abb":"**Label encoding the catagorical varable**","915df2a3":"From the above result, we can say that there is no improvement of metrics after tuning.","ba17dc51":"Looks Good. Up next is building artificial neural network.","357567db":"Plotting training and validation loss over epochs","bd23ae5d":"Next, I will deal with missing values in categorical and numeric attributes separately","28516cdc":"Trying to explore the Rain prediction ANN and using Pycaret to predict the RAIN","0182f15b":"Now, we create a Catboost model for prediction","169636fe":"We can conclude that both ANN and pycaret performance is similar, will update this notebook soon with next version with some features engineering and improved accuracy.","bfaf098e":"Untill then Take care, Upvote if you like the notebook, Thank you.","7a7e423f":"Plotting training and validation accuracy over epochs","07b5d269":"**Points to notice:**\n\n* There are missing values in the dataset\n* Dataset includes numeric and categorical values \n \n# DATA VISUALIZATION AND CLEANING\n\n**Steps involves in this section:**\n\n* Count plot of target column \n* Correlation amongst numeric attributes\n* Parse Dates into datetime\n* Encoding days and months as continuous cyclic features\n","74cc3371":"**About the data:**\n\nThe dataset contains about 10 years of daily weather observations from different locations across Australia. Observations were drawn from numerous weather stations. \n\nIn this project, I will use this data to predict whether or not it will rain the next day. There are 23 attributes including the target variable \"RainTomorrow\", indicating whether or not it will rain the next day or not. "}}