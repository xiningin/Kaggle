{"cell_type":{"8c6bc1af":"code","3e7a713d":"code","fd58fcfa":"code","db11815b":"code","0a1f6ca6":"code","601353ba":"code","170713d8":"code","a310e51f":"code","71d8a4b4":"code","6298dad1":"code","db7c4f57":"code","cc20e164":"code","68b61c81":"code","79c2bb53":"code","09385c53":"code","c6c77188":"code","00b1775b":"code","cdeeb546":"code","d288f917":"code","e9e22429":"code","2ec47be2":"code","43f7670e":"code","abd18eb3":"code","4b9f2007":"code","5bfe065c":"code","7f59dee2":"code","c7712422":"code","748df829":"code","62e33d0f":"code","dc246ada":"code","caec74c5":"code","7c008a85":"code","03cc68af":"code","0023569a":"code","35c492ad":"code","610faabc":"code","956fc06b":"code","264c9b48":"code","3d49817d":"code","c1a10476":"code","afc17887":"code","72bd323d":"code","a9958c59":"code","9dcbd11a":"code","7d0e3e91":"code","ae2f5372":"code","e2f4ce9d":"code","d01e14d8":"code","9d8e32e2":"code","60c37c07":"code","a6565d6c":"code","678f1aac":"code","46b542e1":"code","d797e40f":"code","a24643bf":"code","ac7fb392":"code","a0fe0a81":"code","c373f5a9":"code","1e0c9b7f":"code","a987cb35":"code","506b8905":"code","a5137860":"code","13e694c6":"code","623001fe":"code","33d08205":"code","982aa27d":"code","2e9c5642":"code","86e13e82":"markdown","47e6458c":"markdown","58bf2671":"markdown","05aa4e3a":"markdown","eabf4186":"markdown","d7de63e6":"markdown","5de09301":"markdown","c5793f10":"markdown","17e7602d":"markdown"},"source":{"8c6bc1af":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3e7a713d":"import pandas as pd\nimport numpy as np\nimport os\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\n\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\n\nfrom wordcloud import WordCloud\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nfrom collections import defaultdict\nfrom collections import Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\n\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\n\nfrom tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam","fd58fcfa":"traintw= pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntesttw=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsubmission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")","db11815b":"print('There are {} rows and {} columns in train'.format(traintw.shape[0],traintw.shape[1]))\nprint('There are {} rows and {} columns in train'.format(testtw.shape[0],testtw.shape[1]))","0a1f6ca6":"traintw.head(10)","601353ba":"testtw.head(10)","170713d8":"# extracting the number of examples of each class\ntarget_real = traintw[traintw['target'] == 1].shape[0]\ntarget_not = traintw[traintw['target'] == 0].shape[0]","a310e51f":"# bar plot of the 3 classes\nplt.rcParams['figure.figsize'] = (7, 5)\nplt.bar(10,target_real,3, label=\"Real Tweet\", color='blue')\nplt.bar(15,target_not,3, label=\"Not Real Tweet\", color='red')\nplt.legend()\nplt.ylabel('Number of examples')\nplt.title('Atribute of examples')\nplt.show()","71d8a4b4":"def length(text):    \n    '''a function which returns the length of text'''\n    return len(text)","6298dad1":"traintw['length'] = traintw['text'].apply(length)","db7c4f57":"plt.rcParams['figure.figsize'] = (18.0, 6.0)\nbins = 150\nplt.hist(traintw[traintw['target'] == 0]['length'], alpha = 0.6, bins=bins, label='Not')\nplt.hist(traintw[traintw['target'] == 1]['length'], alpha = 0.8, bins=bins, label='Real')\nplt.xlabel('length')\nplt.ylabel('numbers')\nplt.legend(loc='upper right')\nplt.xlim(0,150)\nplt.grid()\nplt.show()","cc20e164":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=traintw[traintw['target']==1]['text'].str.len()\nax1.hist(tweet_len,color='blue')\nax1.set_title('disaster tweets')\ntweet_len=traintw[traintw['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='red')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","68b61c81":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=traintw[traintw['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='blue')\nax1.set_title('disaster tweets')\ntweet_len=traintw[traintw['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='red')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Words in a tweet')\nplt.show()\n","79c2bb53":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nword=traintw[traintw['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='blue')\nax1.set_title('disaster')\nword=traintw[traintw['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='red')\nax2.set_title('Not disaster')\nfig.suptitle('Average word length in each tweet')","09385c53":"def create_corpus(target):\n    corpus=[]\n    \n    for x in traintw[traintw['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","c6c77188":"def create_corpus_df(traintw, target):\n    corpus=[]\n    \n    for x in traintw[traintw['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","00b1775b":"corpus=create_corpus(0)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]","cdeeb546":"# displaying the stopwords\nnp.array(stop)","d288f917":"plt.rcParams['figure.figsize'] = (18.0, 6.0)\nx,y=zip(*top)\nplt.bar(x,y)","e9e22429":"corpus=create_corpus(1)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n    \n\nplt.rcParams['figure.figsize'] = (18.0, 6.0)\nx,y=zip(*top)\nplt.bar(x,y)","2ec47be2":"plt.figure(figsize=(16,5))\ncorpus=create_corpus(1)\n\ndic=defaultdict(int)\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y)","43f7670e":"plt.figure(figsize=(16,5))\ncorpus=create_corpus(0)\ndic=defaultdict(int)\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y,color='green')","abd18eb3":"plt.figure(figsize=(16,5))\ncounter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:40]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)","4b9f2007":"sns.barplot(x=y,y=x)","5bfe065c":"def get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","7f59dee2":"plt.figure(figsize=(16,5))\ntop_tweet_bigrams=get_top_tweet_bigrams(traintw['text'])[:10]\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=y,y=x)","c7712422":"df=pd.concat([traintw,testtw])\ndf.shape","748df829":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\n","62e33d0f":"df['text']=df['text'].apply(lambda x : remove_URL(x))","dc246ada":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)","caec74c5":"df['text']=df['text'].apply(lambda x : remove_html(x))","7c008a85":"# Reference : https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n","03cc68af":"df['text']=df['text'].apply(lambda x: remove_emoji(x))","0023569a":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n","35c492ad":"df['text']=df['text'].apply(lambda x : remove_punct(x))","610faabc":"# BOW\ncorpus_new1=create_corpus_df(df,1)\nlen(corpus_new1)","956fc06b":"corpus_new1[:10]","264c9b48":"# Generating the wordcloud with the values under the category dataframe\nplt.figure(figsize=(12,8))\nword_cloud = WordCloud(\n                          background_color='black',\n                          max_font_size = 80\n                         ).generate(\" \".join(corpus_new1[:50]))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","3d49817d":"corpus_new0=create_corpus_df(df,0)\nlen(corpus_new0)","c1a10476":"corpus_new0[:10]","afc17887":"# Generating the wordcloud with the values under the category dataframe\nplt.figure(figsize=(12,8))\nword_cloud = WordCloud(\n                          background_color='black',\n                          max_font_size = 80\n                         ).generate(\" \".join(corpus_new0[:50]))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","72bd323d":"df.head(10)","a9958c59":"# Bag of Words Counts\ndef cv(data):\n    count_vectorizer = CountVectorizer()\n\n    emb = count_vectorizer.fit_transform(data)\n\n    return emb, count_vectorizer\n\nlist_corpus = df[\"text\"].tolist()\nlist_labels = df[\"target\"].tolist()\n\nX_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.2, \n                                                                                random_state=42)\n\nX_train_counts, count_vectorizer = cv(X_train)\nX_test_counts = count_vectorizer.transform(X_test)","9dcbd11a":"def plot_LSA(test_data, test_labels, savepath=\"PCA_demo.csv\", plot=True):\n        lsa = TruncatedSVD(n_components=2)\n        lsa.fit(test_data)\n        lsa_scores = lsa.transform(test_data)\n        color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}\n        color_column = [color_mapper[label] for label in test_labels]\n        colors = ['orange','blue']\n        if plot:\n            plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n            orange_patch = mpatches.Patch(color='orange', label='Not')\n            blue_patch = mpatches.Patch(color='blue', label='Real')\n            plt.legend(handles=[orange_patch, blue_patch], prop={'size': 30})\n\nfig = plt.figure(figsize=(16, 16))          \nplot_LSA(X_train_counts, y_train)\nplt.show()","7d0e3e91":"#TF IDF\ndef tfidf(data):\n    tfidf_vectorizer = TfidfVectorizer()\n\n    train = tfidf_vectorizer.fit_transform(data)\n\n    return train, tfidf_vectorizer\n\nX_train_tfidf, tfidf_vectorizer = tfidf(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)","ae2f5372":"fig = plt.figure(figsize=(16, 16))          \nplot_LSA(X_train_tfidf, y_train)\nplt.show()","e2f4ce9d":"# GloVe\ndef create_corpus_new(df):\n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(tweet)]\n        corpus.append(words)\n    return corpus   ","d01e14d8":"corpus=create_corpus_new(df)","9d8e32e2":"embedding_dict={}\nwith open('..\/input\/glove6b100dtxt\/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word = values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","60c37c07":"MAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","a6565d6c":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","678f1aac":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i < num_words:\n        emb_vec=embedding_dict.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i]=emb_vec ","46b542e1":"tweet_pad[0][0:]","d797e40f":"# model with GloVe\nmodel=Sequential()\n\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=3e-4)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])","a24643bf":"model.summary()","ac7fb392":"train=tweet_pad[:traintw.shape[0]]\ntest=tweet_pad[traintw.shape[0]:]","a0fe0a81":"X_train,X_test,y_train,y_test=train_test_split(train,traintw['target'].values,test_size=0.2)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","c373f5a9":"fig = plt.figure(figsize=(16, 16))          \nplot_LSA(train,traintw['target'])\nplt.show()","1e0c9b7f":"# Recomended 10-20 epochs\nhistory=model.fit(X_train,y_train,batch_size=4,epochs=10,validation_data=(X_test,y_test),verbose=2)","a987cb35":"train_pred_GloVe = model.predict(train)\ntrain_pred_GloVe_int = train_pred_GloVe.round().astype('int')","506b8905":"\ntest_pred_GloVe = model.predict(test)\ntest_pred_GloVe_int = test_pred_GloVe.round().astype('int')","a5137860":"# Prediction by GloVe model with my tuning for the training data - for the Confusion Matrix\ntrain_pred_GloVe = model.predict(train)\ntrain_pred_GloVe_int = train_pred_GloVe.round().astype('int')","13e694c6":"pred = pd.DataFrame(test_pred_GloVe, columns=['preds'])\npred.plot.hist()","623001fe":"submission['target'] = test_pred_GloVe_int\nsubmission.head(10)","33d08205":"submission.to_csv(\"submission.csv\", index=False, header=True)","982aa27d":"# Showing Confusion Matrix\ndef plot_cm(y_true, y_pred, title, figsize=(5,5)):\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)","2e9c5642":"# Showing Confusion Matrix for GloVe model\n\nplot_cm(train_pred_GloVe_int, train['target'].values,'Confusion matrix for GloVe model', figsize=(7,7))","86e13e82":"# Baseline Model with some results","47e6458c":"## Confusion Matrices","58bf2671":"I hope you find this kernel useful and for bigginer to learn basic functions. Your comments and feedback are most welcome.\ndont forget if you like this post please upvoke!","05aa4e3a":"## Submission","eabf4186":"# text rpocessing teknique","d7de63e6":"> ### N-gram analysis","5de09301":"## Prediction","c5793f10":"if you want to work on NlP project , its good idea to use this steps:\n    \n    1- first of all import all nessecery library. it means try to use first cell of your notebook for importing library. its clean.\n    2- download and  load dataset in your projects\n    3- spilt your data to train and test. try to  and analyze dataset , and solve partial of problem  your minds by seen\n    4- visulize te data with diffrent teknique to catche better idea to model selection\n    5- find distribution of target class\n    6- for every step try to visualize the results for best choise\n    7- in some of text processing teknique we need do standard preprocessing teknique, but keep in your mind in some of teknique like bert , glove we cant preprocess with common standard teknique , we use unclean text for applay in glove and bert preprocess model directly . here we use both standard and glove teknique becouse i use text procees tekniqe like :  Bag of Words, TF IDF , finally i use Glove\n    8- for this teknique we should get statistical result for word in text or dataset. something like: number of character ,Number of words , Average word length , stopwords , Analyzing punctuations , Common words ,......\n    9- do preprocessing teknique on your dataset\n    10- do text processing teknique and get the result\n    11- Baseline Model with best analysed results from above teknique we use glove here\n    12- predict model\n    13- submission\n    14- confusion matrix\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    ","17e7602d":"# Acknowledgements\n****\nwith special tankx to:\n* https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert\n\n* https:\/\/www.kaggle.com\/christofhenkel\/how-to-preprocessing-for-glove-part2-usage\n\n* https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove"}}