{"cell_type":{"647dd910":"code","d85a4cb4":"code","03119269":"code","5ffce9f8":"code","3f2dc22f":"code","b2ddb2cc":"code","4317703b":"code","2e090849":"code","5ae27660":"code","95501cd4":"code","834e16f9":"code","fcefcd2a":"code","8dc0c20a":"code","0c0c3140":"code","d7da92f2":"code","6eaa1bea":"code","86ad6ca6":"code","68f51cbf":"markdown","ab4edee8":"markdown","60a7bc0b":"markdown","cbe52bb3":"markdown","ad16fe88":"markdown","09113477":"markdown","3ef4d768":"markdown","bc03ccc2":"markdown","5aaa2855":"markdown","10261726":"markdown","3fbf113d":"markdown","57b7af1f":"markdown","b6fe60c1":"markdown","d520aa95":"markdown","6acaf285":"markdown","c249b2d2":"markdown","b0ca02fe":"markdown"},"source":{"647dd910":"import gc\nimport torch\n\ngc.enable()\n\nconfig = {\n    'batch_size': 8, # the batch size for the dataloaders,\n    'best_pretrained_roberta_folder': '..\/input\/pretrain-roberta-large-on-clrp-data\/clrp_roberta_large\/best_model\/',\n    'effective_batch_size': 128, # the number of samples within a gradient accumalation set\n    'lr': 2e-5,\n    'model_name': 'roberta-large',\n    'num_of_folds': 5,\n    'num_of_epochs': 5,\n    'seed': 2021,\n    'sentence_max_length': 256, # the max size of the input for the tokenzier\n    'wd': 0.01\n}\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nfor k, v in config.items():\n    print('The value for {}: {}'.format(k, v))\n\nprint('Device: {}'.format(device))","d85a4cb4":"import os\n\nfor i in range(config['num_of_folds']):\n    os.makedirs(f'model{i}',exist_ok=True)\n    \nprint('Created folders for the models.')","03119269":"import pandas as pd\n\ntrain_csv_path = '\/kaggle\/input\/commonlitreadabilityprize\/train.csv'\ntrain_data = pd.read_csv(train_csv_path)\n\nprint('The total # of samples is {}.'.format(len(train_data)))","5ffce9f8":"import numpy as np\n\nfrom sklearn.model_selection import StratifiedKFold\n\n# create & fill bins column (needed for kfold)\nnum_of_bins = int(np.floor(1 + np.log2(len(train_data))))\ntrain_data.loc[:,'bin'] = pd.cut(train_data['target'], bins=num_of_bins, labels=False)\nbins = train_data.bin.to_numpy()\n\n# kfold\ntrain_data['fold'] = -1\nkfold = StratifiedKFold(n_splits=config['num_of_folds'], shuffle=True, random_state=config['seed'])\nfor k, (train_idx, valid_idx) in enumerate(kfold.split(X=train_data, y=bins)):\n    train_data.loc[valid_idx, 'fold'] = k\n\nprint('Performed K-fold split on training data (K={}).'.format(config['num_of_folds']))","3f2dc22f":"train_data['excerpt'] = train_data['excerpt'].apply(lambda x: x.replace('\\n', ' '))\ntrain_texts = train_data['excerpt'].values.tolist()\n\nprint('Text data has been cleaned.')","b2ddb2cc":"from transformers import RobertaTokenizer\n\ntokenizer = RobertaTokenizer.from_pretrained(config['model_name'])\n\nprint('Tokenizer is ready.')","4317703b":"import torch\n\nclass ReadabilityDataset(torch.utils.data.Dataset):\n    \"\"\"Custom dataset for the Readability task.\"\"\"\n    def __init__(self, encodings, targets):\n        self.encodings = encodings\n        self.targets = targets\n        \n        \n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['target'] = torch.tensor(self.targets[idx])\n        return item\n        \n    def __len__(self):\n        return len(self.targets)\n    \n\nprint(ReadabilityDataset.__doc__)","2e090849":"from torch import nn\n\nclass AttentionHead(nn.Module):\n    \"\"\"Class implementing the attention head of the model.\"\"\"\n    def __init__(self, in_features, hidden_dim):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n       \n    \n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        \n        return torch.sum(context_vector, dim=1)\n\n\nprint(AttentionHead.__doc__)","5ae27660":"from torch import nn\nfrom transformers import RobertaModel\nfrom transformers import RobertaConfig\n\nclass ReadabilityRobertaModel(nn.Module):\n    \"\"\"Custom model for the Readability task containing a Roberta layer and a custom NN head.\"\"\"\n        \n    def __init__(self):\n        super(ReadabilityRobertaModel, self).__init__()\n        \n        self.model_config = RobertaConfig.from_pretrained(config['best_pretrained_roberta_folder'])\n        self.model_config.update({\n            \"output_hidden_states\": True\n        })\n        \n        self.roberta = RobertaModel.from_pretrained(config['best_pretrained_roberta_folder'],\n                                                    config=self.model_config)\n        self.attention_head = AttentionHead(self.model_config.hidden_size, \n                                            self.model_config.hidden_size)\n        self.dropout = nn.Dropout(0.1)\n        self.regressor = nn.Linear(self.model_config.hidden_size, 1)\n        \n        \n    def forward(self, tokens, attention_mask):\n        x = self.roberta(input_ids=tokens, attention_mask=attention_mask)[0]\n        x = self.attention_head(x)\n        x = self.dropout(x)\n        x = self.regressor(x)\n        return x\n    \n    \n    def freeze_roberta(self):\n        \"\"\"\n        Freezes the parameters of the Roberta model so when ReadabilityRobertaModel is \n        trained only the wieghts of the custom regressor are modified.\n        \"\"\"\n        for param in self.roberta.named_parameters():\n            param[1].requires_grad=False\n    \n    def unfreeze_roberta(self):\n        \"\"\"\n        Unfreezes the parameters of the Roberta model so when ReadabilityRobertaModel is \n        trained both the wieghts of the custom regressor and of the underlying Roberta\n        model are modified.\n        \"\"\"\n        for param in self.roberta.named_parameters():\n            param[1].requires_grad=True\n\n    \nprint(ReadabilityRobertaModel.__doc__)","95501cd4":"def loss_fn(y_predicted, y_actual):\n    return torch.sqrt(nn.MSELoss()(y_predicted, y_actual))\n    \nprint('Defined loss function.')","834e16f9":"import torch\n\ndef get_batch_prediction(model, batch):\n    \"\"\"Executes the provided model on the batch to make predictions.\"\"\"\n    input_ids = batch['input_ids'].to(device)\n    attention_mask = batch['attention_mask'].to(device)\n\n    batch_prediction = torch.flatten(model(tokens=input_ids, attention_mask=attention_mask))\n    \n    del input_ids, attention_mask\n    torch.cuda.empty_cache()\n    \n    return batch_prediction\n\nprint(get_batch_prediction.__doc__)","fcefcd2a":"import torch\n\ndef validate_model(model, dataloader, loss_fn):\n    \"\"\"Validates the given model with the data obtained from the provided dataloader. Returns the computed loss\"\"\"\n    model.eval()\n\n    loss = 0\n    size = len(dataloader)\n            \n    with torch.no_grad():\n        for batch in tqdm(dataloader):\n            targets = batch['target'].to(device)\n            predictions = get_batch_prediction(model, batch)\n\n            loss += loss_fn(predictions, targets).item()\n\n    loss = loss \/ size\n    \n    return loss\n\nprint(validate_model.__doc__)","8dc0c20a":"def save_if_best_model(model, fold, best_loss, val_loss):\n    \"\"\"Saves the state dictionary of the provided model if the current validation loss is better than the best loss.\"\"\"\n    if val_loss < best_loss:\n        best_loss = val_loss\n        torch.save(model.state_dict(), f'.\/model{fold}\/model{fold}.bin')\n        tokenizer.save_pretrained(f'.\/model{fold}')\n        print(f\"Model & tokenizer has been saved to .\/model{fold}.\")\n        \n    return best_loss\n\nprint(save_if_best_model.__doc__)","0c0c3140":"from tqdm.auto import tqdm\n\nimport torch\n\ntqdm.pandas()\n\ndef train_and_eval(model, train_dataloader, val_dataloader, optimizer,\n                   loss_fn, best_loss, fold):\n    \"\"\"Trains the model and evaluates it on every validate_after_stepth step.\"\"\"\n    \n    model.train()\n    size = len(train_dataloader.dataset)\n    total_train_loss = 0\n    optimizer.zero_grad()\n    \n    for i, train_batch in enumerate(tqdm(train_dataloader)):\n        train_targets = train_batch['target'].to(device)\n        train_predictions = get_batch_prediction(model, train_batch)     \n        \n        train_loss = loss_fn(train_predictions, train_targets)\n        train_loss.backward()\n                \n        if (i * config['batch_size'] % config['effective_batch_size']) == 0 or ((i + 1) == len(train_dataloader)):\n            print(f\"{i}th batch:\")\n            optimizer.step()\n            optimizer.zero_grad()\n        \n            total_train_loss += train_loss.item()\n        \n            val_loss = validate_model(model, val_dataloader, loss_fn)\n            best_loss = save_if_best_model(model, fold, best_loss, val_loss)\n            \n            if not (val_loss > best_loss):\n                print(f\"Best validation loss: {best_loss}\")\n                print(f\"Training loss: {total_train_loss\/(i+1)}\")\n                \nprint(train_and_eval.__doc__)","d7da92f2":"from torch.utils.data import DataLoader\n\ndef create_dataloader(texts, targets):\n    \"\"\"Converts the provided texts & targets into a dataloader\"\"\"\n    encodings = tokenizer(texts.values.tolist(),\n                         max_length=config['sentence_max_length'],\n                         truncation=True, padding=True)\n    dataset = ReadabilityDataset(encodings, targets.values.tolist())\n    dataloader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)\n    \n    return dataloader\n\nprint(create_dataloader.__doc__)","6eaa1bea":"def train_fold(fold, model, tokenizer, optimizer):\n    \"\"\"Trains the foldth model.\"\"\"\n    for i in range(config['num_of_epochs']):\n        print(f'Epoch # {i+1}:')\n        x_train = train_data.query(f\"fold != {fold}\")\n        x_val = train_data.query(f\"fold == {fold}\")\n        \n        train_dataloader = create_dataloader(x_train['excerpt'], x_train['target'])\n        val_dataloader = create_dataloader(x_val['excerpt'], x_val['target'])\n                \n        train_and_eval(model=model,\n                       train_dataloader=train_dataloader, val_dataloader=val_dataloader,\n                       optimizer=optimizer, loss_fn=loss_fn,\n                       best_loss=9999,\n                       fold=fold)\n        \nprint(train_fold.__doc__)","86ad6ca6":"from transformers import AdamW\n\nprint('Train the models:')\nfor fold in range(config['num_of_folds']):\n    model = ReadabilityRobertaModel()\n    model.to(device)\n    \n    optimizer = AdamW(model.parameters(), lr=config['lr'], weight_decay=config['wd'])\n    \n    print(f\"Fold #: {fold}\")\n    train_fold(fold, model, tokenizer, optimizer)\n    \n    del model\n    torch.cuda.empty_cache()\n    print(f\"Fold # {fold} successfully finished.\")","68f51cbf":"<a id='training_model_fold'><\/a>\n## Training a model for each fold\n[[back to top]](#toc)","ab4edee8":"Note, that the concept of attention is awesomely explained in [Lena Voita](https:\/\/lena-voita.github.io\/)'s excellent notebook [here](https:\/\/lena-voita.github.io\/nlp_course\/seq2seq_and_attention.html).","60a7bc0b":"Effective batch size as described here: https:\/\/stackoverflow.com\/questions\/68479235\/cuda-out-of-memory-error-cannot-reduce-batch-size","cbe52bb3":"<a id=processing_data_folds><\/a>\n## Organize data into folds\n[[back to top]](#toc)\n\nThe following cell splits the data into 5 fold. To do that a new column (named *bin*) is added to the panda dataframe. The new column is produced by using [panda's *cut*](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.cut.html#pandas-cut) which can essentailly be used to convert continous values into discrete values.\n\nOnce there is a column (in this case *bin*) which splits the target values into categories [scikit learn's StratifiedKFold](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.StratifiedKFold.html) can be used. An advantage of stratified k-fold is that \"*the folds are made by preserving the percentage of samples for each class*\".\n\n(This idea was also borrowed from [Maunish' previously referred notebook](https:\/\/www.kaggle.com\/maunish\/clrp-pytorch-roberta-finetune).)","ad16fe88":"<a id='processing_data_clearing_texts'><\/a>\n## Clearing texts\n[[back to top]](#toc)\n\nSo far the only cleaning which takes place is to replace new line characters with spaces.","09113477":"<a id=\"setup_dataset\"><\/a>\n## Data set\n[[back to top]](#toc)","3ef4d768":"Create loss function based on [this](https:\/\/www.kaggle.com\/maunish\/clrp-pytorch-roberta-finetune).","bc03ccc2":"<a id=\"training\"><\/a>\n# Training process\n[[back to top]](#toc)","5aaa2855":"<a id=\"setup_model\"><\/a>\n## Model\n[[back to top]](#toc)\n\nLoads a pretrained Roberta model from another notebook titled [Fine-tuning Roberta-large on CLRP data](https:\/\/www.kaggle.com\/angyalfold\/fine-tuning-roberta-large-on-clrp-data).\n\nThe following model is based on Maunish' [notebook](https:\/\/www.kaggle.com\/maunish\/clrp-pytorch-roberta-finetune\/data)","10261726":"## Introduction\n\nThe aim of this notebook is to train models for the [CommonLit Readability Prize competition](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/overview\/) using K-Fold, i.e. it produces k=5 models which can be used for making predictions.\n\nThis notebook is part of a series:\n1. Pretrain roberta large on the CommonLit dataset [here](https:\/\/www.kaggle.com\/angyalfold\/pretrain-roberta-large-on-clrp-data\/).\n2. Produce k models which can later be used for determining the readability of texts (this notebook).\n3. Make predictions with a custom NN regressor [here](https:\/\/www.kaggle.com\/angyalfold\/roberta-large-with-custom-regressor-pytorch\/).\n4. Ensemble (Roberta large + SVR, Roberta large + Ridge, Roberta large + custom NN head) [here](https:\/\/www.kaggle.com\/angyalfold\/ensemble-for-commonlit\/).\n\nK-Fold ideas & and the approach of producing multiple models are taken from Maunish' [notebook](https:\/\/www.kaggle.com\/maunish\/clrp-pytorch-roberta-finetune).","3fbf113d":"<a id=\"toc\"><\/a>\n# Table of contents\n* [Parameters](#parameters)\n* [Processing data](#processing_data)\n    * [Load data](#processing_data_load)\n    * [Organize data into folds](#processing_data_folds)\n    * [Clearing texts](#processing_data_clearing_texts)\n* [Setup](#setup)\n    * [Tokenizer](#setup_tokenizer)\n    * [Data set](#setup_dataset)\n    * [Model](#setup_model)\n    * [Loss function](#setup_loss)\n* [Training process](#training)\n    * [Iteration to train & validate model](#training_model_iteration)\n    * [Training a model for each fold](#training_model_fold)","57b7af1f":"<a id=\"setup_tokenizer\"><\/a>\n## Tokenizer\n[[back to top]](#toc)","b6fe60c1":"<a id=\"setup_loss\"><\/a>\n## Loss function\n[back to top](#toc)","d520aa95":"<a id=\"setup\"><\/a>\n# Setup\n[[back to top]](#toc)","6acaf285":"<a id='training_model_iteration'><\/a>\n## Iteration to train & validate model\n[[back_to_top]](#toc)","c249b2d2":"<a id=\"processing_data\"><\/a>\n# Processing data\n[[back to top]](#toc)\n\n<a id=\"processing_data_load\"><\/a>\n## Load data\n[[back to top]](#toc)","b0ca02fe":"<a id=\"parameters\"><\/a>\n# Parameters\n[[back to top]](#toc)"}}