{"cell_type":{"53fe7bac":"code","9586a313":"code","ce2e628f":"code","2fbb5778":"code","659a22b0":"code","33d164f6":"code","10e71270":"code","947aed73":"code","f4f13c5a":"code","ece29a71":"code","385f290e":"code","355165a3":"code","4ead5615":"code","74fbcd4c":"code","c2087407":"code","b02e59dc":"code","d55e2e2c":"code","6cb93998":"code","6c4999c9":"code","b62c6c2a":"code","c4585241":"code","9f7705a4":"code","f5bf86c9":"code","3cf21b53":"code","3031e798":"code","ed54dcda":"code","ee51e8ec":"code","ab8e9747":"code","8ede881b":"markdown","38779682":"markdown","ef2dbc26":"markdown"},"source":{"53fe7bac":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9586a313":"# Import the Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats","ce2e628f":"#load the data and overview\ndf = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndf.head()","2fbb5778":"#Show the dimension of the dataset\ndf.shape","659a22b0":"df.info()","33d164f6":"df.describe().T","10e71270":"#Check for missing values\ndf.isnull().any()","947aed73":"for x in df.columns:\n    print(x,len(df[df[x]==0]))","f4f13c5a":"#Replace zero to nan values\ndf[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.nan)\n#showing the count of Nans\nprint(df.isnull().sum())","ece29a71":"#Create a hist for showing the zeros values\np = df.hist(figsize = (15,15))","385f290e":"#Fill the nan values for the columns in accordance with their distribution\ndf['Glucose'].fillna(df['Glucose'].mean(),inplace=True)\ndf['BloodPressure'].fillna(df['BloodPressure'].mean(),inplace=True)\ndf['SkinThickness'].fillna(df['SkinThickness'].median(),inplace=True)\ndf['Insulin'].fillna(df['Insulin'].mean(),inplace=True)\ndf['BMI'].fillna(df['BMI'].median(),inplace=True)","355165a3":"#checking the count of Nans\nprint(df.isnull().sum())","4ead5615":"#Get a count of the number of Have Diabetes(1) and Haven't Diabetes(0) cells\ndf.Outcome.value_counts()","74fbcd4c":"#Visualize the count\nplt.figure(figsize=(5,5))\nsns.countplot(data=df,x='Outcome')\nplt.xticks(ticks=[0,1],labels=[\"Haven't Diabetes\",'Have Diabetes'])","c2087407":"#The percent of people with and without diabetes \npercent_have_diabetes = (len(df[df.Outcome == 1]) \/ len(df)) * 100\npercent_havenot_diabetes = (len(df[df.Outcome == 0]) \/ len(df)) * 100\n\nprint('Percent of people that have diabetes is :',round(percent_have_diabetes,2),'%')\nprint(\"Percent of people that haven't diabetes is :\",round(percent_havenot_diabetes,2),'%')","b02e59dc":"#Visual the correlation\nplt.figure(figsize=(10,10))\nsns.heatmap(df.corr(),annot=True)","d55e2e2c":"#The correlation for the target\n100 * df.corr()['Outcome'].sort_values()","6cb93998":"#Create a dist plot\nfig, ax = plt.subplots(2,4, figsize=(15,15))\nsns.histplot(df.Age, bins = 20, ax=ax[0,0] , color=\"red\", kde=True, stat=\"density\") \nsns.histplot(df.Pregnancies, bins = 20, ax=ax[0,1], color=\"red\", kde=True, stat=\"density\") \nsns.histplot(df.Glucose, bins = 20, ax=ax[0,2], color=\"red\", kde=True, stat=\"density\") \nsns.histplot(df.BloodPressure, bins = 20, ax=ax[0,3], color=\"red\", kde=True, stat=\"density\") \nsns.histplot(df.SkinThickness, bins = 20, ax=ax[1,0], color=\"red\", kde=True, stat=\"density\")\nsns.histplot(df.Insulin, bins = 20, ax=ax[1,1], color=\"red\", kde=True, stat=\"density\")\nsns.histplot(df.DiabetesPedigreeFunction, bins = 20, ax=ax[1,2], color=\"red\", kde=True, stat=\"density\") \nsns.histplot(df.BMI, bins = 20, ax=ax[1,3],color=\"red\", kde=True, stat=\"density\") ","6c4999c9":"#Check for outliers\nplt.figure(figsize=(10,10))\nax = sns.boxplot(data=df, orient=\"h\", palette=\"Set2\",)","b62c6c2a":"#Split the dataset into independent(X) and dependent(y) datasets\nX = df.iloc[:,:-1]\ny = df.iloc[:,-1]","c4585241":"from sklearn.preprocessing import PolynomialFeatures\nfeature = PolynomialFeatures(degree=3)\nX = feature.fit_transform(X)","9f7705a4":"X.shape","f5bf86c9":"#Split the dataset into 85% training and 15% testing\nfrom sklearn.model_selection import train_test_split\nX_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.15,random_state=42)","3cf21b53":"#Scale the data (Feature Scaling)\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","3031e798":"#Create a function for model\ndef models(X_train,y_train):\n    \n    # LogisticRegression\n    from sklearn.linear_model import LogisticRegression\n    log = LogisticRegression(solver='liblinear')\n    log.fit(X_train,y_train)\n    \n    # Random Forest\n    from sklearn.ensemble import RandomForestClassifier\n    rnd = RandomForestClassifier(n_estimators=70,random_state=0,criterion='gini',max_depth=14,bootstrap=False,verbose = 0)\n    rnd.fit(X_train,y_train)\n    \n    # Gradient Boost\n    from sklearn.ensemble import GradientBoostingClassifier\n    gb = GradientBoostingClassifier()\n    gb.fit(X_train,y_train)\n   \n    \n    #Print the model accuracy of training data\n    print('Logistic Regression Training Accuracy : ',log.score(X_train, y_train))\n    print('Random Forest Training Accuracy : ',rnd.score(X_train, y_train))\n    print('Gradient Boosting Classifier Training Accuracy : ',gb.score(X_train, y_train))\n\n    \n    return log,rnd,gb","ed54dcda":"#Get the model\nmodel = models(X_train,y_train)","ee51e8ec":"#Test model accuracy on test data using confusion matrix and accuracy score\nfrom sklearn.metrics import confusion_matrix , accuracy_score\nfor x in range(len(model)):\n    print('Model :',model[x])\n    cm = confusion_matrix(y_test,model[x].predict(X_test))\n    print(cm)\n    print('Accuracy ',accuracy_score(y_test,model[x].predict(X_test)))\n    print('\\n\\n')","ab8e9747":"#Another way to get matrix of the models\nfrom sklearn.metrics import classification_report\nfor i in range (len(model)):\n    print('Model :',model[i])\n    print(classification_report(y_test,model[i].predict(X_test)))\n    print('\\n\\n')","8ede881b":"## In the end, we find that **GradientBoostingClassifier** is more accurate than the other ","38779682":"On these columns, a value of zero does not make sense and thus indicates missing value.\n\nFollowing columns or variables have an invalid zero value:\n\n**Glucose** , **BloodPressure** , **SkinThickness** , **Insulin** , **BMI**\n","ef2dbc26":"# Pima Indians Diabetes Prediction"}}