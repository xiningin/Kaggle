{"cell_type":{"08ef2e1b":"code","026765fd":"code","d199d6f0":"code","baccf9b2":"code","1e0129ea":"code","f72a6db8":"code","e49b90eb":"code","481c67b6":"code","38e5fa5a":"code","618dbd3b":"code","be8ed5fa":"code","e18ea5dd":"code","38644a20":"markdown","51baed6a":"markdown","51ea793c":"markdown","68f9fec0":"markdown","37398865":"markdown"},"source":{"08ef2e1b":"# 1. Th\u00eam c\u00e1c th\u01b0 vi\u1ec7n c\u1ea7n thi\u1ebft\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.utils import np_utils\n# from keras.datasets import mnist","026765fd":"# 2. Load d\u1eef li\u1ec7u MNIST\ndef load_data(path):\n    with np.load(path) as f:\n        x_train, y_train = f['x_train'], f['y_train']\n        x_test, y_test = f['x_test'], f['y_test']\n        return (x_train, y_train), (x_test, y_test)\n\n(X_train, y_train), (X_test, y_test) = load_data('..\/input\/mnist.npz')\n\n# (X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_train \/= 255\nX_test \/= 255\n\nX_val, y_val = X_train[50000:60000,:], y_train[50000:60000]\nX_train, y_train = X_train[:50000,:], y_train[:50000]\nprint(X_train.shape)","d199d6f0":"# 3. Reshape l\u1ea1i d\u1eef li\u1ec7u cho \u0111\u00fang k\u00edch th\u01b0\u1edbc m\u00e0 keras y\u00eau c\u1ea7u\nX_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\nX_val = X_val.reshape(X_val.shape[0], 28, 28, 1)\nX_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\nprint(X_train.shape)","baccf9b2":"# 4. One hot encoding label (Y)\nY_train = np_utils.to_categorical(y_train, 10)\nY_val = np_utils.to_categorical(y_val, 10)\nY_test = np_utils.to_categorical(y_test, 10)\nprint('D\u1eef li\u1ec7u y ban \u0111\u1ea7u ', y_train[0])\nprint('D\u1eef li\u1ec7u y sau one-hot encoding ',Y_train[0])","1e0129ea":"# 5. \u0110\u1ecbnh ngh\u0129a model\nmodel = Sequential()\n \n# Th\u00eam Convolutional layer v\u1edbi 32 kernel, k\u00edch th\u01b0\u1edbc kernel 3*3\n# d\u00f9ng h\u00e0m sigmoid l\u00e0m activation v\u00e0 ch\u1ec9 r\u00f5 input_shape cho layer \u0111\u1ea7u ti\u00ean\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28,28,1)))\n\n# Th\u00eam Convolutional layer\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\n\n# Th\u00eam Max pooling layer\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\n# Flatten layer chuy\u1ec3n t\u1eeb tensor sang vector\nmodel.add(Flatten())\n\n# Th\u00eam Fully Connected layer v\u1edbi 128 nodes v\u00e0 d\u00f9ng h\u00e0m sigmoid\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\n\n# Output layer v\u1edbi 10 node v\u00e0 d\u00f9ng softmax function \u0111\u1ec3 chuy\u1ec3n sang x\u00e1c xu\u1ea5t.\nmodel.add(Dense(10, activation='softmax'))","f72a6db8":"# 6. Compile model, ch\u1ec9 r\u00f5 h\u00e0m loss_function n\u00e0o \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng, ph\u01b0\u01a1ng th\u1ee9c \n# \u0111\u00f9ng \u0111\u1ec3 t\u1ed1i \u01b0u h\u00e0m loss function.\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adadelta',\n              metrics=['accuracy'])","e49b90eb":"# 7. Th\u1ef1c hi\u1ec7n train model v\u1edbi data\nnumOfEpoch = 15\nH = model.fit(X_train, Y_train, validation_data=(X_val, Y_val),\n          batch_size=128, epochs=numOfEpoch, verbose=1)","481c67b6":"# 8. V\u1ebd \u0111\u1ed3 th\u1ecb loss, accuracy c\u1ee7a traning set v\u00e0 validation set\nfig = plt.figure()\nplt.plot(np.arange(0, numOfEpoch), H.history['loss'], label='training loss')\nplt.plot(np.arange(0, numOfEpoch), H.history['val_loss'], label='validation loss')\nplt.plot(np.arange(0, numOfEpoch), H.history['acc'], label='accuracy')\nplt.plot(np.arange(0, numOfEpoch), H.history['val_acc'], label='validation accuracy')\nplt.title('Accuracy and Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss|Accuracy')\nplt.legend()","38e5fa5a":"# 9. \u0110\u00e1nh gi\u00e1 model v\u1edbi d\u1eef li\u1ec7u test set\nscore = model.evaluate(X_test, Y_test, verbose=0)\nprint(score)","618dbd3b":"# 10. D\u1ef1 \u0111o\u00e1n \u1ea3nh\nindex_test = 321\nplt.imshow(X_test[index_test].reshape(28,28), cmap='gray')\n\ny_predict = model.predict(X_test[index_test].reshape(1,28,28,1))\nprint('Gi\u00e1 tr\u1ecb d\u1ef1 \u0111o\u00e1n: ', np.argmax(y_predict))","be8ed5fa":"output = model.predict(X_test)\nY = []\nfor i in range(output.shape[0]):\n    if np.argmax(output[i]) != np.argmax(Y_test[i]): Y.append(i)\n\nprint(Y)","e18ea5dd":"NumIm = 20\nfig = plt.figure(figsize=(15, 30))\ncolumns = 5\nrows = 10\n\nfor index in range(0, NumIm):\n    fig.add_subplot(rows, columns, index + 1)\n    \n    index_test = Y[index]\n    plt.imshow(X_test[index_test].reshape(28,28), cmap='gray')\n    y_predict = model.predict(X_test[index_test].reshape(1,28,28,1))\n    print(index + 1, ': h\u00ecnh \u1ea3nh th\u1ee9 ', index_test, ' c\u00f3 gi\u00e1 tr\u1ecb d\u1ef1 \u0111o\u00e1n v\u00e0 th\u1ef1c t\u1ebf: ', np.argmax(y_predict), ' v\u00e0 ', np.argmax(Y_test[index_test]))\n\nplt.show()","38644a20":"Ta s\u1ebd d\u00f9ng k\u1ebft qu\u1ea3 \u0111\u00e1nh gi\u00e1 c\u1ee7a mode v\u1edbi test set \u0111\u1ec3 l\u00e0m k\u1ebft qu\u1ea3 cu\u1ed1i c\u00f9ng c\u1ee7a model. T\u1ee9c model c\u1ee7a ch\u00fang ta d\u1eef \u0111o\u00e1n ch\u1eef s\u1ed1 c\u00f3 \u0111\u1ed9 ch\u00ednh x\u00e1c 98.92% v\u1edbi MNIST dataset. Ngh\u0129a l\u00e0 d\u1ef1 \u0111o\u00e1n kho\u1ea3ng 100 \u1ea3nh th\u00ec sai 1 \u1ea3nh.","51baed6a":"Load d\u1eef li\u1ec7u t\u1eeb MNIST dataset, bao g\u1ed3m 60.000 training set v\u00e0 10.000 test set. Sau \u0111\u00f3 chia b\u1ed9 traning set th\u00e0nh 2: 50.000 cho training set v\u00e0 10.000 d\u1eef li\u1ec7u cho validation set.","51ea793c":"B\u01b0\u1edbc n\u00e0y chuy\u1ec3n \u0111\u1ed5i one-hot encoding label Y c\u1ee7a \u1ea3nh v\u00ed d\u1ee5 s\u1ed1 5 th\u00e0nh vector [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]","68f9fec0":"B\u01b0\u1edbc n\u00e0y \u0111\u1ecbnh ngh\u0129a model:\n1. Model = Sequential() \u0111\u1ec3 n\u00f3i cho keras l\u00e0 ta s\u1ebd x\u1ebfp c\u00e1c layer l\u00ean nhau \u0111\u1ec3 t\u1ea1o model. V\u00ed d\u1ee5 input -> CONV -> POOL -> CONV -> POOL -> FLATTEN -> FC -> OUTPUT\n2.  \u1ede layer \u0111\u1ea7u ti\u00ean c\u1ea7n ch\u1ec9 r\u00f5 input_shape c\u1ee7a \u1ea3nh, input_shape = (W, H, D), ta d\u00f9ng \u1ea3nh x\u00e1m k\u00edch th\u01b0\u1edbc (28,28) n\u00ean input_shape = (28, 28, 1)\n3. Khi th\u00eam Convolutional Layer ta c\u1ea7n ch\u1ec9 r\u00f5 c\u00e1c tham s\u1ed1: K (s\u1ed1 l\u01b0\u1ee3ng layer), kernel size (W, H), h\u00e0m activation s\u1eed d\u1ee5ng. c\u1ea5u tr\u00fac: model.add(Conv2D(K, (W, H), activation='t\u00ean_h\u00e0m_activation'))\n4. Khi th\u00eam Maxpooling Layer c\u1ea7n ch\u1ec9 r\u00f5 size c\u1ee7a kernel, model.add(MaxPooling2D(pool_size=(W, H)))\n5. B\u01b0\u1edbc Flatten chuy\u1ec3n t\u1eeb tensor sang vector ch\u1ec9 c\u1ea7n th\u00eam flatten layer.\n6. \u0110\u1ec3 th\u00eam Fully Connected Layer (FC) c\u1ea7n ch\u1ec9 r\u00f5 s\u1ed1 l\u01b0\u1ee3ng node trong layer v\u00e0 h\u00e0m activation s\u1eed d\u1ee5ng trong layer, c\u1ea5u tr\u00fac: model.add(Dense(s\u1ed1_l\u01b0\u1ee3ng_node activation='t\u00ean_h\u00e0m activation'))\n","37398865":"D\u1eef li\u1ec7u input cho m\u00f4 h\u00ecnh convolutional neural network l\u00e0 1 tensor 4 chi\u1ec1u (N, W, H, D), trong b\u00e0i n\u00e0y l\u00e0 \u1ea3nh x\u00e1m n\u00ean W = H = 28, D = 1, N l\u00e0 s\u1ed1 l\u01b0\u1ee3ng \u1ea3nh cho m\u1ed7i l\u1ea7n training. Do d\u1eef li\u1ec7u \u1ea3nh \u1edf tr\u00ean c\u00f3 k\u00edch th\u01b0\u1edbc l\u00e0 (N, 28, 28) t\u1ee9c l\u00e0 (N, W, H) n\u00ean r\u1ea7n reshape l\u1ea1i th\u00e0nh k\u00edch th\u01b0\u1edbc N * 28 * 28 * 1 \u0111\u1ec3 gi\u1ed1ng k\u00edch th\u01b0\u1edbc m\u00e0 keras y\u00eau c\u1ea7u."}}