{"cell_type":{"6b6b145c":"code","fb338c90":"code","8722a613":"code","b08b1a00":"code","ba12222d":"code","dc22766d":"code","b37c18d7":"code","24a92795":"code","2c181578":"code","3bdaa828":"code","8bca992f":"code","91071da5":"code","6ee123aa":"code","b5b6f356":"code","e004051a":"code","a6e627ba":"code","6795f599":"code","434ce847":"code","40b53895":"code","f88e18d2":"code","4d16ee90":"code","97fb4c21":"code","a7ab2b79":"code","223b02cb":"code","f997057e":"code","73e39ae6":"code","b712d988":"code","f27a9cc9":"code","a8eefdae":"code","8059b324":"code","88cd870b":"code","7ad18cef":"code","52593fb4":"code","1f471831":"code","d1ba5ae0":"code","6365ea08":"code","bfec4c65":"code","573b9f90":"code","a2ea6937":"code","a1945f19":"code","938c97ac":"code","06efd647":"code","b09c378a":"code","47a7696e":"code","cbffe1c3":"code","1f8eb1d8":"code","352212dd":"code","b5283237":"code","e7753c49":"code","91b60844":"code","0b33472f":"code","536c7169":"code","99c7fc2f":"code","e667fbc8":"code","0dd99014":"code","05a00162":"markdown","c86a1407":"markdown","1534c06b":"markdown","ff0ac1ea":"markdown","ffac1a8e":"markdown","96fe8146":"markdown","58ae2a3d":"markdown","72df2e9e":"markdown","78662621":"markdown","995697a5":"markdown","9924467b":"markdown"},"source":{"6b6b145c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly as py\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fb338c90":"# Reading the sellers dataset\ndf_sellers = pd.read_csv(\"..\/input\/brazilian-ecommerce\/olist_sellers_dataset.csv\")\n\n# Reading the costumers dataset\ndf_costumers = pd.read_csv(\"..\/input\/brazilian-ecommerce\/olist_customers_dataset.csv\")\n\n# Reading the orders datasets\ndf_orders = pd.read_csv(\"..\/input\/brazilian-ecommerce\/olist_orders_dataset.csv\")\ndf_order_items = pd.read_csv(\"..\/input\/brazilian-ecommerce\/olist_order_items_dataset.csv\")\ndf_order_payments = pd.read_csv(\"..\/input\/brazilian-ecommerce\/olist_order_payments_dataset.csv\")\ndf_order_reviews = pd.read_csv(\"..\/input\/brazilian-ecommerce\/olist_order_reviews_dataset.csv\")\n\n# Reading the products dataset\ndf_products = pd.read_csv(\"..\/input\/brazilian-ecommerce\/olist_products_dataset.csv\")\n\n# Reading the localization dataset\ndf_localizations = pd.read_csv(\"..\/input\/brazilian-ecommerce\/olist_geolocation_dataset.csv\")\n\n# Reading the category name translator\ndf_translator = pd.read_csv(\"..\/input\/brazilian-ecommerce\/product_category_name_translation.csv\")","8722a613":"df_products","b08b1a00":"# Reading the leads dataset \ndf_leads = pd.read_csv('..\/input\/marketing-funnel-olist\/olist_marketing_qualified_leads_dataset.csv')\ndf_leads.head(10)","ba12222d":"# Reading the leads dataset \ndf_closed_deals = pd.read_csv('..\/input\/marketing-funnel-olist\/olist_closed_deals_dataset.csv')\ndf_closed_deals.head(10)","dc22766d":"df_text = df_order_reviews[[\"review_score\", \"review_comment_title\"]].dropna()\ndf_text ","b37c18d7":"from sklearn.feature_extraction.text import CountVectorizer\n\ncount_vect = CountVectorizer()\n\nX_counts = count_vect.fit_transform(df_text[\"review_comment_title\"].to_list())\nX_counts.shape","24a92795":"from sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf_transformer = TfidfTransformer()\nX_tfidf = tfidf_transformer.fit_transform(X_counts)\nX_tfidf.shape","2c181578":"# marketing funnel dataset (NaNs are leads that did not close a deal)\ndf_funnel = df_leads.merge(df_closed_deals, on='mql_id', how='left')\ndf_funnel.head(10)","3bdaa828":"df_funnel.dropna(subset=[\"seller_id\"])","8bca992f":"df_funnel_items = df_funnel.dropna(subset=[\"seller_id\"]).merge(df_order_items, on='seller_id', how='left')\ndf_funnel_items = df_funnel_items.dropna(subset=[\"product_id\"])\ndf_funnel_items","91071da5":"df_funnel_products = df_funnel_items.merge(df_products, on=\"product_id\", how=\"left\")\ndf_funnel_products","6ee123aa":"from datetime import datetime, date, timedelta\n\ndf = df_funnel.dropna(subset=[\"seller_id\"])\ndf[\"won_date\"] = df[\"won_date\"].apply(lambda d: datetime.strptime(d,\"%Y-%m-%d %H:%M:%S\"))\ndf.head()\n\nrepresentant_struc = {}\nfor representant_id in df[\"sr_id\"].unique():\n    # Get the seller convertions...\n    df_seller = df.where(df[\"sr_id\"] == representant_id).dropna(subset=[\"won_date\", \"business_segment\"])\n    \n    representant_struc[representant_id] = {\n        \"num_convertions\": len(df_seller),\n        \"first_convertion\": df_seller[\"won_date\"].min(),\n        \"last_convertion\": df_seller[\"won_date\"].max(),\n        \"num_categories\": len(df_seller[\"business_segment\"].unique())\n    }\n    ","b5b6f356":"pd_struc = {\n    \"representant\": [],\n    \"categories\": [],\n    \"convertions\": [],\n    \"performance\": [],\n    \"time_as_representant\": []\n}\n\nfor representant_id in representant_struc:\n    # Time as seller in OList\n    time_as_representant = (representant_struc[representant_id][\"last_convertion\"] - representant_struc[representant_id][\"first_convertion\"]).days\n    \n    # Convertions per time -> Performance metric\n    convertions_per_time = representant_struc[representant_id][\"num_convertions\"] \/ time_as_representant if time_as_representant != 0 else representant_struc[representant_id][\"num_convertions\"]\n    \n    pd_struc[\"representant\"].append(representant_id)\n    pd_struc[\"performance\"].append(convertions_per_time)\n    pd_struc[\"categories\"].append(representant_struc[representant_id][\"num_categories\"])\n    pd_struc[\"convertions\"].append(representant_struc[representant_id][\"num_convertions\"])\n    pd_struc[\"time_as_representant\"].append(time_as_representant)\n\nplot_df = pd.DataFrame(pd_struc)","e004051a":"plot_df","a6e627ba":"fig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x=plot_df[\"categories\"],\n    y=plot_df[\"convertions\"],\n    mode=\"markers\"\n))\n\nfig.update_layout(template=\"xgridoff\",\n                  title=\"Correlations of convertions and categories\",\n                  xaxis_title=\"Number of categories\",\n                  yaxis_title=\"Number of convertions\")\n\niplot(fig)","6795f599":"fig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x=plot_df[\"categories\"],\n    y=plot_df[\"performance\"],\n    mode=\"markers\"\n))\n\nfig.update_layout(template=\"xgridoff\",\n                  title=\"Correlations of categories and performance\",\n                  xaxis_title=\"Number of categories\",\n                  yaxis_title=\"Performance\")\n\niplot(fig)","434ce847":"fig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x=plot_df[\"time_as_representant\"],\n    y=plot_df[\"performance\"],\n    mode=\"markers\"\n))\n\nfig.update_layout(template=\"xgridoff\",\n                  title=\"Correlations of time as represetant and performance\",\n                  xaxis_title=\"Time as representant\",\n                  yaxis_title=\"Performance\")\n\niplot(fig)","40b53895":"df = df_funnel_items.merge(df_orders, on=\"order_id\", how=\"left\")\n\ndf[\"order_approved_at\"] = df[\"order_approved_at\"].apply(lambda d: datetime.strptime(d, \"%Y-%m-%d %H:%M:%S\"))\n\ndf.head()","f88e18d2":"df_performance = df.groupby(by=\"seller_id\").sum().reset_index()\ndf_performance","4d16ee90":"seller_time = {\n    \"seller_id\": [],\n    \"time_as_seller\": []\n}\n\nfor seller_id in df[\"seller_id\"].unique():\n    \n    df_seller = df.where(df[\"seller_id\"] == seller_id).dropna(subset=[\"seller_id\"])\n    \n    first_order_date = df_seller[\"order_approved_at\"].min()\n    last_order_date = df_seller[\"order_approved_at\"].max()\n    \n    time_as_seller = (last_order_date - first_order_date).days \/ 30\n    \n    seller_time[\"seller_id\"].append(seller_id)\n    seller_time[\"time_as_seller\"].append(time_as_seller if time_as_seller != 0 else 1)\n","97fb4c21":"df_performance = df_performance.merge(pd.DataFrame(seller_time), on=\"seller_id\", how=\"left\")\n\ndf_performance[\"performance\"] = df_performance[\"price\"] \/ df_performance[\"time_as_seller\"]\n\ndf_performance_seller = df_performance[[\"price\", \"freight_value\", \"time_as_seller\", \"performance\"]]","a7ab2b79":"df = df_order_items.merge(df_orders, on='order_id', how='left').dropna()\ndf[\"order_approved_at\"] = df[\"order_approved_at\"].apply(lambda d: datetime.strptime(d, \"%Y-%m-%d %H:%M:%S\"))\n\ndf.head()","223b02cb":"df_best_products = df_order_items.groupby(by=\"product_id\").count().sort_values(by=\"order_id\", ascending=False).reset_index()\ndf_best_products = df_best_products[[\"product_id\", \"order_id\"]]\ndf_best_products.columns = [\"product_id\", \"order_count\"]\ndf_best_products.head()","f997057e":"def build_product_history(product_id, df):\n    \"\"\"\n    \"\"\"\n    # Build the dataframe of the particular product\n    prod_df = df.where(df[\"product_id\"] == product_id).dropna(subset=[\"product_id\"])\n    \n    # Create the monthly group\n    prod_df[\"month_group\"] = prod_df[\"order_approved_at\"].apply(lambda d: datetime(d.year, d.month, 1))\n    \n    # Group the product values\n    res_df = prod_df.groupby(by=\"month_group\").count().reset_index()\n    res_df = res_df[[\"month_group\", \"order_id\"]]\n    res_df.columns = [\"month_group\", \"order_count\"]\n    \n    # Product mean price computing \n    res_df[\"mean price\"] = prod_df.groupby(by=\"month_group\").mean().reset_index()[\"price\"]\n    \n    return res_df.sort_values(by=\"month_group\")\n\n\ndef build_product_history_week(product_id, df):\n    \"\"\"\n    \"\"\"\n    # Build the dataframe of the particular product\n    prod_df = df.where(df[\"product_id\"] == product_id).dropna(subset=[\"product_id\"])\n    \n    # Create the monthly group\n    prod_df[\"week\"] = prod_df[\"order_approved_at\"].apply(lambda d: d.week)\n    prod_df[\"year\"] = prod_df[\"order_approved_at\"].apply(lambda d: d.year)\n    \n    # Group the product values\n    res_df = prod_df.groupby(by=[\"week\", \"year\"]).count().reset_index()\n    res_df = res_df[[\"week\", \"year\", \"order_id\"]]\n    res_df.columns = [\"week\", \"year\", \"order_count\"]\n    \n    # Product mean price computing \n    res_df[\"mean price\"] = prod_df.groupby(by=[\"week\", \"year\"]).mean().reset_index()[\"price\"]\n    \n    return res_df","73e39ae6":"my_prod_df = build_product_history(\"aca2eb7d00ea1a7b8ebd4e68314663af\", df)\nmy_prod_df","b712d988":"df.where(df['product_id'] == \"aca2eb7d00ea1a7b8ebd4e68314663af\").dropna().max()","f27a9cc9":"df_sells = df_orders.merge(df_order_items, how=\"left\", on=\"order_id\")\n\ndf_prices = df_sells[[\"order_id\", \"price\"]].groupby(by=\"order_id\").sum()\n\ndf_prices[\"first_digit\"] = df_prices[\"price\"].apply(lambda p: int(str(p).replace(\".\", \"\")[0]))\ndf_prices[\"second_digit\"] = df_prices[\"price\"].apply(lambda p: int(str(p).replace(\".\", \"\")[1]))\n\ndigit_count = []\nfor digit in range(1,10):\n    digit_count.append(\n        df_prices.where(df_prices[\"first_digit\"] == digit).dropna().shape[0]\n    )\n    \ndigit_prop = [100 * digit \/ sum(digit_count) for digit in digit_count]\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(\n    x = list(range(1,10)),\n    y = digit_prop,\n    mode=\"markers+lines\"\n))\n\niplot(fig)","a8eefdae":"df_sells = (df_orders\n    .merge(df_order_items, on=\"order_id\", how=\"left\")\n    .merge(df_products, on=\"product_id\", how=\"left\"))\n\nprint(f\"DataFrame shape: {df_sells.shape}\")","8059b324":"df_sells.columns","88cd870b":"columns = ['order_purchase_timestamp', 'product_id', 'order_delivered_customer_date', 'seller_id', \n           'shipping_limit_date', 'price', 'freight_value', 'product_category_name',\n           'product_name_lenght', 'product_description_lenght',\n           'product_photos_qty', 'product_weight_g', 'product_length_cm',\n           'product_height_cm', 'product_width_cm']\n\ndf_sells = df_sells[columns]","7ad18cef":"df_sells[\"order_purchase_timestamp\"][0], df_sells[\"order_delivered_customer_date\"][0]","52593fb4":"from datetime import datetime\n\ndf_sells.dropna(subset=[\"order_purchase_timestamp\", \"order_delivered_customer_date\", \"shipping_limit_date\"], inplace=True)\ndf_sells[\"order_purchase_timestamp\"] = df_sells[\"order_purchase_timestamp\"].apply(lambda d: datetime.strptime(d, \"%Y-%m-%d %H:%M:%S\"))\ndf_sells[\"order_delivered_customer_date\"] = df_sells[\"order_delivered_customer_date\"].apply(lambda d: datetime.strptime(str(d), \"%Y-%m-%d %H:%M:%S\"))\ndf_sells[\"shipping_limit_date\"] = df_sells[\"shipping_limit_date\"].apply(lambda d: datetime.strptime(str(d), \"%Y-%m-%d %H:%M:%S\"))\n\n# Creating delivery time\ndef compute_delivery(df):\n    \n    delivery_time_list = list()\n    for initial, final in zip(df[\"order_purchase_timestamp\"].to_list(), df[\"order_delivered_customer_date\"].to_list()):\n        delivery_time_list.append((final - initial).days)\n    \n    df[\"delivery_time\"] = delivery_time_list\n    return df\n\n# Compute the delivery time for order\ndf_sells = compute_delivery(df_sells)\n\n# Compute if the delivery was late or not\ndf_sells[\"late_delivery\"] = df_sells[\"order_delivered_customer_date\"] > df_sells[\"shipping_limit_date\"]\n\n# Compute the time informations\ndf_sells[\"day\"] = df_sells[\"order_purchase_timestamp\"].apply(lambda d: d.day)\ndf_sells[\"month\"] = df_sells[\"order_purchase_timestamp\"].apply(lambda d: d.month)\ndf_sells[\"year\"] = df_sells[\"order_purchase_timestamp\"].apply(lambda d: d.year)","1f471831":"df_sells.columns","d1ba5ae0":"product_cols = ['product_id', 'seller_id',\n       'price', 'freight_value', 'product_category_name',\n       'product_name_lenght', 'product_description_lenght',\n       'product_photos_qty', 'product_weight_g', 'product_length_cm',\n       'product_height_cm', 'product_width_cm', 'delivery_time',\n       'late_delivery', 'day', 'month', 'year']\n\ndf_prod = df_sells[product_cols]","6365ea08":"group_df = (df_prod\n    .groupby(\n        by=[\"product_id\", \"seller_id\", \"product_category_name\", \"month\", \"year\"])\n    .agg({\n        \"product_id\": [\n            \"count\"\n        ],\n        \"price\": [\n            \"mean\", \"std\", \"max\", \"min\"\n        ],\n        \"freight_value\": [\n            \"mean\", \"std\", \"max\", \"min\"\n        ],\n        \"product_name_lenght\": [\n            \"mean\", \"std\", \"max\", \"min\"\n        ],\n        \"product_description_lenght\": [\n            \"mean\", \"std\", \"max\", \"min\"\n        ],\n        \"product_photos_qty\": [\n            \"mean\", \"std\", \"max\", \"min\"\n        ],\n        \"product_weight_g\": [\n            \"mean\", \"std\", \"max\", \"min\"\n        ],\n        \"product_length_cm\": [\n            \"mean\", \"std\", \"max\", \"min\"\n        ],\n        \"product_height_cm\": [\n            \"mean\", \"std\", \"max\", \"min\"\n        ],\n        \"product_width_cm\": [\n            \"mean\", \"std\", \"max\", \"min\"\n        ],\n        \"delivery_time\": [\n            \"mean\", \"std\", \"max\", \"min\"\n        ],\n        \"late_delivery\": [\n            \"sum\"\n        ]\n    }))\n\ngroup_df.columns = [\"_\".join(x) for x in group_df.columns.ravel()]\ngroup_df = group_df.fillna(0)","bfec4c65":"informative_cols = list()\nfor col in group_df.columns:\n    if group_df[col].std() != 0.0:\n        informative_cols.append(col)\n        \ngroup_df = group_df[informative_cols]\n\n# Creating time reference column for sorting\ngroup_df.reset_index(inplace=True)\ngroup_df[\"time_reference\"] = [datetime(day=1,month=m,year=y) for m, y in zip(group_df[\"month\"], group_df[\"year\"])]","573b9f90":"group_df","a2ea6937":"pid_sorted_df = group_df.where(group_df[\"product_id\"] == \"aca2eb7d00ea1a7b8ebd4e68314663af\").dropna().sort_values(by=\"time_reference\", ascending=False)\n\npid_sorted_df[\"target\"] = [-1] + pid_sorted_df[\"product_id_count\"][:-1].to_list()","a1945f19":"test_size = round(len(pid_sorted_df) * 0.3)\n\npid_sorted_df = pid_sorted_df[1:]\npid_test_df = pid_sorted_df.iloc[:test_size]\npid_train_df = pid_sorted_df.iloc[test_size:]","938c97ac":"# from tqdm import tqdm\n\n# # nx = 4\n# # dummy_values = [0 for k in range(len(group_df))]\n# # for n in range(nx):\n# #     group_df[f\"number_of_sells_{n}\"] = dummy_values\n\n# train_df = pd.DataFrame(columns=group_df.columns.to_list() + [\"target\"])\n# test_df = pd.DataFrame(columns=group_df.columns.to_list() + [\"target\"])\n\n# for pid in tqdm(group_df[\"product_id\"].unique()):\n#     pid_sorted_df = (group_df\n#                      .where(group_df[\"product_id\"] == pid)\n#                      .dropna()\n#                      .sort_values(by=\"time_reference\", ascending=False))\n    \n#     if len(pid_sorted_df) >= 5:\n        \n#         pid_sorted_df[\"target\"] = [-1] + pid_sorted_df[\"product_id_count\"][:-1].to_list()\n        \n#         test_size = round(len(pid_sorted_df) * 0.3)\n\n#         pid_sorted_df = pid_sorted_df[1:]\n#         pid_test_df = pid_sorted_df.iloc[:test_size]\n#         pid_train_df = pid_sorted_df.iloc[test_size:]\n\n#         train_df = pd.concat((train_df, pid_train_df), axis=0)\n#         test_df = pd.concat((test_df, pid_test_df), axis=0)\n    ","06efd647":"prod_hist_df = group_df.groupby(by=[\"product_id\"]).count()[\"seller_id\"].reset_index().sort_values(by=\"seller_id\", ascending=False)\nprint(f\"Number of products: {len(prod_hist_df)}\")\n\nimp_products = prod_hist_df.where(prod_hist_df[\"seller_id\"] > 12).dropna()[\"product_id\"].to_list()\nprint(f\"Number of important products: {len(imp_products)}\")","b09c378a":"group_df[\"important\"] = group_df[\"product_id\"].apply(lambda pid: pid in imp_products)\nimp_products_df = group_df.where(group_df[\"important\"]).dropna()","47a7696e":"from tqdm import tqdm\n\n# nx = 4\n# dummy_values = [0 for k in range(len(group_df))]\n# for n in range(nx):\n#     group_df[f\"number_of_sells_{n}\"] = dummy_values\n\ntrain_df = pd.DataFrame(columns=imp_products_df.columns.to_list() + [\"target\"])\ntest_df = pd.DataFrame(columns=imp_products_df.columns.to_list() + [\"target\"])\n\nfor pid in tqdm(imp_products_df[\"product_id\"].unique()):\n    pid_sorted_df = (imp_products_df\n                     .where(imp_products_df[\"product_id\"] == pid)\n                     .dropna()\n                     .sort_values(by=\"time_reference\", ascending=False))\n\n    pid_sorted_df[\"target\"] = [-1] + pid_sorted_df[\"product_id_count\"][:-1].to_list()\n    pid_sorted_df = pid_sorted_df.iloc[1:,:]\n    \n    test_size = round(len(pid_sorted_df) * 0.3)\n\n    pid_test_df = pid_sorted_df.iloc[:test_size]\n    pid_train_df = pid_sorted_df.iloc[test_size:]\n\n    train_df = pd.concat((train_df, pid_train_df), axis=0)\n    test_df = pd.concat((test_df, pid_test_df), axis=0)","cbffe1c3":"from sklearn.preprocessing import LabelEncoder\n\n# Creating label_encoders\nle_ = dict()\nfor col in train_df.columns.to_list():\n    if train_df[col].dtype == object:\n        le_[col] = LabelEncoder().fit(pd.concat(\n            (train_df[col], test_df[col]), axis=0))\n        train_df[col] = le_[col].transform(train_df[col])\n        test_df[col] = le_[col].transform(test_df[col])","1f8eb1d8":"# Droping unused columns\nrm_columns = [\"time_reference\", \"important\"]\n\ntrain_df.drop(columns=rm_columns, inplace=True)\ntest_df.drop(columns=rm_columns, inplace=True)","352212dd":"from sklearn.preprocessing import StandardScaler\n\n\n\n","b5283237":"X_train, y_train = train_df.iloc[:, :-1], train_df.iloc[:, -1]\nX_test, y_test = test_df.iloc[:, :-1], test_df.iloc[:, -1]","e7753c49":"from xgboost import XGBRegressor\n\n\nreg = XGBRegressor(n_estimators=10, max_depth=2)\nreg.fit(X_train, y_train)","91b60844":"from scipy.optimize import differential_evolution, dual_annealing, shgo, minimize\n\ndef cost_function(pars, X_train, X_test, y_train, y_test):\n    \"\"\"\n    \"\"\"\n    params = {\n        'min_child_weight': int(pars[0]),\n        'gamma': pars[1],\n        'subsample': pars[2],\n        'colsample_bytree': pars[3],\n        'max_depth': int(pars[4]),\n        'n_estimators': int(pars[5])\n    }\n    \n    reg = XGBRegressor(**params)\n    reg.fit(X_train, y_train)\n    \n    y_pred = reg.predict(X_test)\n    \n    error = mean_squared_error(y_test, y_pred)\n    print(f\"Error: {error}\")\n    \n    return error\n\nbounds = [(1, 100),\n          (0.1, 100),\n          (0.3, 1.0),\n          (0.3, 1.0),\n          (2, 100),\n          (2, 400)]\n\nsummary = shgo(cost_function, \n               bounds=bounds, \n               args=(X_train, X_test, y_train, y_test))","0b33472f":"summary","536c7169":"pars = summary.x\n\nparams = {\n    'min_child_weight': int(pars[0]),\n    'gamma': pars[1],\n    'subsample': pars[2],\n    'colsample_bytree': pars[3],\n    'max_depth': int(pars[4]),\n    'n_estimators': int(pars[5])\n}\n\nreg = XGBRegressor(**params)\nreg.fit(X_train, y_train)","99c7fc2f":"from sklearn.metrics import r2_score, mean_squared_error, explained_variance_score\n\ny_pred = reg.predict(X_test)\n\nprint(f\"R2 score: {r2_score(y_test, y_pred, multioutput='variance_weighted')}\")\nprint(f\"MSE: {mean_squared_error(y_test, y_pred, squared=True)}\")\nprint(f\"EV score: {explained_variance_score(y_test, y_pred, multioutput='variance_weighted')}\")\n\ny_m = y_test.mean()\nnum = sum([(y_t - y_p)**2 for y_t, y_p in zip(y_test, y_pred)]) ** 0.5\nden = sum([(y_t - y_m)**2 for y_t in y_test]) ** 0.5\nbf_rate = 100 * ( 1 - (num \/ den) ) \nprint(f\"BFR: {bf_rate}\")","e667fbc8":"import plotly.graph_objects as go\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    name=\"Real data\",\n    x=list(range(len(y_test))),\n    y=y_test\n))\n\nfig.add_trace(go.Scatter(\n    name=\"Prediction\",\n    x=list(range(len(y_test))),\n    y=y_pred\n))\n\nfig.show()","0dd99014":"import numpy as np\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    name=\"Error\",\n    x=list(range(len(y_test))),\n    y=y_test - y_pred\n))\n\nm_error = np.mean(y_test - y_pred)\n\nfig.add_trace(go.Scatter(\n    name=\"Mean error\",\n    x=list(range(len(y_test))),\n    y=[m_error for k in range(len(y_test))]\n))\n\nfig.show()","05a00162":"# Customer revenue\n\n---\n\nUse the information of a particular customer to predict how much performatic this costumer will be in the future if this costumer converts during the representant approach.","c86a1407":"# **Acessando os dados**\n\n---\n\n## Brazillian ECommerce ","1534c06b":"# Product prediction\n\n---\n\n\nPredict the number of products will be sold of a particular `product_id`, using historic information and the product particular informations.","ff0ac1ea":"# Natural Language Processing - Comments-> Ranking\n\n---\n\nThe simplest problem would be to use the comments and ranking dataset to build a comment model capable of ranking the comments of the users as satisfied or unsatisfied with the provided product. This is particularlly interesting considering that there are few portuguese datasets for costumer ranking. For that we only need to use the dataset of `order_reviews` and use the `review_score` and `review_comment_title` fields, respectivelly as target and feature.","ffac1a8e":"## Marketing deals ","96fe8146":"## *If we wanted to check the correlation between number of departments and number of salles convertions?*","58ae2a3d":"## Generating text features from comments","72df2e9e":"### Preparing and analysing dates","78662621":"# **Closing deals - Analysis of Sellers**\n\n---\n\n## Describe the reasons and characteristics of a great representant\n\nCreate descriptive analysis or clustering models to characterize each Representant and understand their performance rates. This is nothing much more than a Personas analysis with each particular representant, that are described by the `sr_id`. With this information you probably will have the information of several comportamental information of each representant and with this information it is probably possible to use the cluster to create more analysis...\n\n\n## Find the best represetant for each deal!\n\nFor this you probably will have to create a model capable of using the information of the `seller_id` from the Brazillian OList ECommerce database to characterize each particular seller, and their products, to them find the best cluster of representants to use in a particular client\/seller.\n","995697a5":"# Introducing the Benford Law Analysis\n\n---","9924467b":"# Predicting number of product sales\n\n---"}}