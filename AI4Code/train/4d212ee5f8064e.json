{"cell_type":{"36b21d36":"code","af7319a2":"code","0421e090":"code","e2b74543":"code","2491ce8e":"code","f7f335a6":"code","a1d508cc":"code","661ddf4d":"code","37a44d4f":"code","88ca0b98":"code","48a0509a":"code","c82c14b7":"code","3fd6aae5":"code","6773757c":"code","34dc2d66":"code","469bb685":"code","650e5ce1":"code","c912b6e1":"code","a40a5b17":"code","3cd2b96c":"code","c5724200":"markdown"},"source":{"36b21d36":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","af7319a2":"import numpy as np \nimport random\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom collections import Counter\nimport nltk\nfrom nltk.corpus import stopwords\n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nfrom tqdm import tqdm\nimport os\nimport nltk\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch","0421e090":"train = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv')","e2b74543":"train.head()","2491ce8e":"#Distribution of Sentiment column\ntrain['sentiment'].value_counts(normalize=True)","f7f335a6":"plt.figure(figsize=(12,8))\nsns.countplot(x='sentiment',data=train)","a1d508cc":"import re\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\ntrain['text'] = train['text'].apply(lambda x:clean_text(x))\ntrain['selected_text'] = train['selected_text'].apply(lambda x:clean_text(x))","661ddf4d":"train.head()","37a44d4f":"train['text_len'] = train['text'].astype(str).apply(len)\ntrain['text_word_count'] = train['text'].apply(lambda x: len(str(x).split()))","88ca0b98":"train['temp_list'] = train['selected_text'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in train['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","48a0509a":"from nltk.stem.snowball import SnowballStemmer\n\n# create an object of stemming function\nstemmer = SnowballStemmer(\"english\")\n\ndef stemming(text):    \n    '''a function which stems each word in the given text'''\n    text = [stemmer.stem(word) for word in text.split()]\n    return \" \".join(text) ","c82c14b7":"train['text'] = train['text'].apply(stemming)\ntrain['selected_text'] = train['selected_text'].apply(stemming)\ntrain.head(10)","3fd6aae5":"Positive_sent = train[train['sentiment']=='positive']\nNegative_sent = train[train['sentiment']=='negative']\nNeutral_sent = train[train['sentiment']=='neutral']\n#MosT common positive words\ntop = Counter([item for sublist in Positive_sent['temp_list'] for item in sublist])\ntemp_positive = pd.DataFrame(top.most_common(20))\ntemp_positive.columns = ['Common_words','count']\ntemp_positive.style.background_gradient(cmap='Greens')","6773757c":"fig = px.bar(temp_positive, x=\"count\", y=\"Common_words\", title='Most Commmon Positive Words', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","34dc2d66":"count_vectorizer = CountVectorizer()\ntrain_vectors = count_vectorizer.fit_transform(train['text'])\ntest_vectors = count_vectorizer.transform(test[\"text\"])\n\n## Keeping only non-zero elements to preserve space \nprint(train_vectors[0].todense())","469bb685":"tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\ntrain_tfidf = tfidf.fit_transform(train['text'])\ntest_tfidf = tfidf.transform(test[\"text\"])","650e5ce1":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntvec = TfidfVectorizer(stop_words=None, max_features=100000, ngram_range=(1, 3))\nlr = LogisticRegression()\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\ndef lr_cv(splits, X, Y, pipeline, average_method):\n    \n    kfold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=777)\n    accuracy = []\n    precision = []\n    recall = []\n    f1 = []\n    for train, test in kfold.split(X, Y):\n        lr_fit = pipeline.fit(X[train], Y[train])\n        prediction = lr_fit.predict(X[test])\n        scores = lr_fit.score(X[test],Y[test])\n        \n        accuracy.append(scores * 100)\n        precision.append(precision_score(Y[test], prediction, average=average_method)*100)\n        print('              negative    neutral     positive')\n        print('precision:',precision_score(Y[test], prediction, average=None))\n        recall.append(recall_score(Y[test], prediction, average=average_method)*100)\n        print('recall:   ',recall_score(Y[test], prediction, average=None))\n        f1.append(f1_score(Y[test], prediction, average=average_method)*100)\n        print('f1 score: ',f1_score(Y[test], prediction, average=None))\n        print('-'*50)\n\n    print(\"accuracy: %.2f%% (+\/- %.2f%%)\" % (np.mean(accuracy), np.std(accuracy)))\n    print(\"precision: %.2f%% (+\/- %.2f%%)\" % (np.mean(precision), np.std(precision)))\n    print(\"recall: %.2f%% (+\/- %.2f%%)\" % (np.mean(recall), np.std(recall)))\n    print(\"f1 score: %.2f%% (+\/- %.2f%%)\" % (np.mean(f1), np.std(f1)))","c912b6e1":"from sklearn.pipeline import Pipeline\noriginal_pipeline = Pipeline([\n    ('vectorizer', tvec),\n    ('classifier', lr)\n])\nlr_cv(5, train.text, train.sentiment, original_pipeline, 'macro')","a40a5b17":"def predict_entities(text, lr):\n    doc = lr(text)\n    ent_array = []\n    for ent in doc.ents:\n        start = text.find(ent.text)\n        end = start + len(ent.text)\n        new_int = [start, end, ent.label_]\n        if new_int not in ent_array:\n            ent_array.append([start, end, ent.label_])\n    selected_text = text[ent_array[0][0]: ent_array[0][1]] if len(ent_array) > 0 else text\n    return selected_text\n","3cd2b96c":"sample = pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/sample_submission.csv\")\nsubmission['selected_text'] = test['text']\nsubmission.to_csv(\"submission.csv\", index=False)\ndisplay(submission.head(10))","c5724200":"SOme pretty basic data Cleaning"}}