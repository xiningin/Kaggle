{"cell_type":{"8a85d07a":"code","0aaddf65":"code","e9131718":"code","007a29d7":"code","5a6fd529":"code","ad44a723":"code","2a0d8124":"code","05ca97f8":"code","b4fb542b":"code","ccfe1216":"code","b5c1f4b3":"code","614b3341":"code","7a98bce3":"code","7a24c9d3":"code","1721ca47":"code","ed73250b":"code","2dfbcde4":"code","5570dc76":"code","5d221843":"code","6c2ecae7":"code","f94171b2":"code","2fd5997d":"code","54d6e5b5":"code","1c7ddb28":"code","16a27ff8":"code","8b56ecbe":"code","c93023ec":"code","1c06d37b":"code","873ae285":"code","61512dbe":"code","0b9c4696":"code","dbb52df3":"code","291c0169":"code","9f3fd14b":"code","9e0245d1":"code","5aabd859":"code","02ff1890":"code","9cfe3638":"code","74a4b9e1":"code","23bd766d":"code","8cf960de":"code","1682cf93":"code","9383674e":"code","b677b10f":"code","817fc857":"code","8debd179":"code","43bca4fb":"code","b2d27124":"code","f0f930c5":"code","e61463d9":"code","12261aee":"code","1dd77284":"code","d5bce387":"code","edf29932":"code","bf7635f5":"code","eb495400":"code","e2c0de38":"code","ad956e2b":"code","28e26035":"code","2fa51693":"code","165784f6":"code","11d644a5":"code","5b4ebe01":"code","0755da01":"code","34fa2104":"code","229b9a09":"code","c115d1e0":"code","0dea2c5a":"code","bab150d0":"code","0b4c79eb":"code","d1867caf":"code","02f9e6c3":"code","e6b86f4d":"markdown","705f95d5":"markdown","5dca7e1e":"markdown","09a5cb06":"markdown","ee75b0d5":"markdown","cb10f928":"markdown","8cebdea4":"markdown","3cf8c5ef":"markdown","338598aa":"markdown","6deb5137":"markdown","2aefa983":"markdown","54a9327b":"markdown","c043025f":"markdown","53ecb751":"markdown","336982aa":"markdown","9d96004c":"markdown","536db735":"markdown","b67d2e6d":"markdown","d31baf2b":"markdown","212cbf40":"markdown","7512fb0e":"markdown","a036b6ef":"markdown","d68c60d0":"markdown","df9567a6":"markdown","d3740f9d":"markdown","5f24ac66":"markdown","b931e049":"markdown","732dbbb3":"markdown","57d078cb":"markdown","f312f69c":"markdown"},"source":{"8a85d07a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","0aaddf65":"train = pd.read_csv(\"..\/input\/train\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test\/test.csv\")\nbreed_labels = pd.read_csv(\"..\/input\/breed_labels.csv\")\ncolor_labels = pd.read_csv(\"..\/input\/color_labels.csv\")\nstate_labels = pd.read_csv(\"..\/input\/state_labels.csv\")\nsubmission = pd.read_csv(\"..\/input\/test\/sample_submission.csv\")","e9131718":"train.head()\n\n","007a29d7":"\nbreed_labels.head()\n","5a6fd529":"color_labels.head()\n","ad44a723":"state_labels.head()\n","2a0d8124":"#Copy training set to train_eda\ntrain_ed = train.copy()\ntrain_ed.head()","05ca97f8":"# Joining train_eda with state labels, breed_labels and color labels datasets\nbreed_labels.rename(columns={'BreedID':'Breed1'}, inplace=True)\nstate_labels.rename(columns={'StateID':'State'}, inplace=True)\ntrain_ed = train_ed.merge(state_labels,on=\"State\",how='left')\ntrain_ed = train_ed.merge(breed_labels,on=\"Breed1\",how='left')\ntrain_ed.shape","b4fb542b":"train_ed.rename(columns={'BreedName':'MainBreed'}, inplace=True)\ntrain_ed.rename(columns={'Type_x':'Type'}, inplace=True)","ccfe1216":"train_ed= train_ed.drop([\"Breed1\",'Type_y'],axis = 1)\ntrain_ed.columns","b5c1f4b3":"breed_labels.rename(columns={'Breed1':'Breed2',\"BreedName\":\"SecondaryName\"},inplace = True)\n","614b3341":"train_ed=train_ed.merge(breed_labels,on=\"Breed2\",how=\"left\")\ntrain_ed.rename(columns={'Type_x':'Type'},inplace = True)\ntrain_ed=train_ed.drop([\"Type_y\",'Breed2'],axis = 1)\ntrain_ed.columns\ntrain_ed.shape","7a98bce3":"color_labels.rename(columns={'ColorID':'Color1','ColorName':'ColorName1'},inplace = True)\ntrain_ed=train_ed.merge(color_labels,on='Color1',how='left')\ntrain_ed.shape","7a24c9d3":"color_labels.rename(columns={'Color1':'Color2','ColorName1':'ColorName2'},inplace = True)\n","1721ca47":"train_ed=train_ed.merge(color_labels,on='Color2',how='left')\ntrain_ed.columns","ed73250b":"color_labels.rename(columns={'Color2':'Color3','ColorName2':'ColorName3'},inplace = True)\ntrain_ed=train_ed.merge(color_labels,on='Color3',how='left')","2dfbcde4":"\ntrain_ed = train_ed.drop([\"Color1\",'Color2','Color3'],axis = 1)\ntrain_ed.columns\n","5570dc76":"train_ed.shape","5d221843":"train_ed = train_ed.drop([\"State\"],axis = 1)\n","6c2ecae7":"train_ed.head()","f94171b2":"train_ed[\"Gender\"]=train_ed[\"Gender\"].replace({1:'Male',2:'Female',3:'Mixed'})\ntrain_ed[\"MaturitySize\"]=train_ed[\"MaturitySize\"].replace({1:'Small',2:'Medium',3:'Large',4:'Extra Large',0:\"Not Specified\"})\ntrain_ed[\"FurLength\"]=train_ed[\"FurLength\"].replace({1:'Short',2:'Medium',3:'Long',0:\"Not Specified\"})\ntrain_ed[\"Vaccinated\"]=train_ed[\"Vaccinated\"].replace({1:'Yes',2:'No',3:'Not Sure'})\ntrain_ed[\"Dewormed\"]=train_ed[\"Dewormed\"].replace({1:'Yes',2:'No',3:'Not Sure'})\ntrain_ed[\"Sterilized\"]=train_ed[\"Sterilized\"].replace({1:'Yes',2:'No',3:'Not Sure'})\ntrain_ed[\"Health\"]=train_ed[\"Health\"].replace({1:'Healthy',2:'Minor Injury',3:'Major Injury',0:\"Not Specified\"})\ntrain_ed[\"Type\"]=train_ed[\"Type\"].replace({1:'Dog',2:'Cat'})\ntrain_ed[\"AdoptionSpeed\"]=train_ed[\"AdoptionSpeed\"].replace({0:'Adopted on the same day',1:'Adopted within 1 week',2:'Adopted within 1 month',3:'Adopted within 2-3 months',4:'Didnt get adopted'})\ntrain_ed.head()\n","2fd5997d":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ng=sns.countplot(x=\"Type\",data=train_ed,palette=sns.color_palette(\"muted\"))\n#Turns out cats are listed more on the site\nplot_dict={}\nval_counts=dict(train_ed[\"Type\"].value_counts().sort_index())\nfor k,v in val_counts.items():\n    if k not in plot_dict:\n        plot_dict[val_counts[k]] = val_counts[k]\n    else:\n        plot_dict[0] = 0\nfor x in g.patches:\n    height = x.get_height()\n    g.text(x.get_x() + x.get_width()\/2.0,height,plot_dict[height],ha = \"center\", va = \"bottom\", fontsize = 8,weight=\"semibold\",size=\"large\")\n\n","54d6e5b5":"#function to create AdoptionSpeed plots using a grouper \nimport math\ndef make_plot (df,grouper,col='AdoptionSpeed',title='',palette = ''):\n    g=sns.countplot(x=col,hue=grouper,data=df,palette=palette) \n    g.set_xticklabels(labels = df[col].unique(),rotation=-60,ha='center')\n    g.set_ylabel(\"Number of Adoptions\")\n    g.set_title(title)\n    g.legend(frameon=False)\n    sns.set_style({\"axes.facecolor\":\"white\"})\n##adding annotations\n    total_counts_dict = dict(df[grouper].value_counts().sort_index())\n    plot_dict={}\n   \n    for i in df[col].unique():\n        val_counts = dict(df.loc[df[col] == i, grouper].value_counts())\n        for k,v in val_counts.items():\n            if k not in plot_dict:\n                plot_dict[val_counts[k]] = (val_counts[k]\/total_counts_dict[k])*100\n            else:\n                plot_dict[0]=0       \n    for p in g.patches:\n        height = p.get_height()\n        if math.isnan(height):\n            continue\n        g.text(p.get_x()+p.get_width()\/2.0,\n         height + 0.5,\n         f\"{plot_dict[height]:.0f}%\" , ha = \"center\", va = \"bottom\", fontsize = 8,weight=\"semibold\",size=\"large\")\n \n\n\n\n\n","1c7ddb28":"#We can also see which Type gets adopted faster based on given data'        \n\n\nmake_plot(train_ed,\"Type\",palette=sns.color_palette(\"Spectral_r\"))\n\n","16a27ff8":"import matplotlib.pyplot as plt1 \nplt1.figure(figsize=(25,15))\nplt1.subplots_adjust(hspace=0.7,wspace=0.7)\nplt1.subplot(2,2,1)\nmake_plot(df=train_ed,grouper=\"Vaccinated\",title='Adoption Rate vs Vaccinated',palette='rainbow')\nplt1.subplot(2,2,2)\nmake_plot(df=train_ed,grouper=\"Sterilized\",title='Adoption Rate vs Sterilized',palette='Spectral')\nplt1.subplot(2,2,3)\nmake_plot(df=train_ed,grouper=\"Dewormed\",title='Adoption Rate vs Dewormed',palette='icefire')\nplt1.subplot(2,2,4)\nmake_plot(df=train_ed,grouper=\"Health\",title='Adoption Rate vs Health',palette='winter_r')\n            \n        \n    \n    ","8b56ecbe":"plt1.figure(figsize=(20,10))\nplt1.subplots_adjust(hspace=0.7,wspace=0.7)\nplt1.subplot(2,2,1)\ndf=train_ed.loc[train_ed['Type'] == \"Cat\"]\nax1=make_plot(df=df,grouper=\"MaturitySize\",title='Adoption Rate vs MaturitySize for Cats',palette='summer')\n\nplt1.subplot(2,2,2)\ndf=train_ed.loc[train_ed['Type'] == \"Dog\"]\nax2=make_plot(df=df,grouper=\"MaturitySize\",title='Adoption Rate vs MaturitySize for Dogs',palette='winter')\n\nplt1.subplot(2,2,3)\n\nax2=make_plot(df=train_ed,grouper=\"MaturitySize\",title='Adoption Rate vs MaturitySize',palette='OrRd_r')\n\nplt1.subplot(2,2,4)\nax2=make_plot(df=train_ed,col='Type',grouper='MaturitySize',palette='Set2',title='Counts per MaturitySize type')\n\n","c93023ec":"plt1.figure(figsize=(20,10))\nplt1.subplots_adjust(hspace=0.7,wspace=0.4)\nplt1.subplot(2,2,1)\ndf=train_ed.loc[train_ed['Type'] == \"Cat\"]\nax1=make_plot(df=df,grouper=\"FurLength\",title='Adoption Rate vs FurLength for Cats',palette='summer')\n\nplt1.subplot(2,2,2)\ndf=train_ed.loc[train_ed['Type'] == \"Dog\"]\nax2=make_plot(df=df,grouper=\"FurLength\",title='Adoption Rate vs FurLength for Dogs',palette='winter')\n\nplt1.subplot(2,2,3)\n\nax2=make_plot(df=train_ed,grouper=\"FurLength\",title='Adoption Rate vs FurLength',palette='OrRd_r')\n\nplt1.subplot(2,2,4)\nax2 = make_plot(col='Type',grouper='FurLength',df=train_ed,palette='Set3',title='Counts per FurLength type')","1c06d37b":"plt1.figure(figsize=(20,7))\nplt1.subplots_adjust(hspace=0.7,wspace=0.4)\n\nplt1.subplot(1,2,1)\nax=make_plot(df=test,col='Type',grouper='FurLength',palette='OrRd_r',title='FurLength vs Count of pets per type for test set')\n\n# plt1.subplot(1,2,2)\n# ax=make_plot(df=test,col='Type',grouper='MaturitySize',palette='OrRd_r',title='MaturitySize vs Count of pets per type for test set')\n\n#test[\"MaturitySize\".groupby(\"Type\")\nfor i in test['Type'].unique():\n        val_counts = dict(test.loc[test['Type'] == i, 'FurLength'].value_counts())\nval_counts    ","873ae285":"color_labels.rename(columns={'Color3':'ColorID','ColorName3':'ColorName'},inplace = True)\npalette_=['Black', 'Brown', '#FFFDD0', 'Gray', 'Gold', 'White', 'Yellow']\nplt.figure(figsize=(20,10))\nplt.subplot(1,2,1)\nmake_plot(df=train_ed,grouper='ColorName1',palette=palette_,col='Type',title='Counts for training set based on color')\n# Now we check for test set\ncolors_dict = {k: v for k, v in zip(color_labels['ColorID'], color_labels['ColorName'])}\ntest[\"ColorName1_\"] = test[\"Color1\"].apply(lambda x: colors_dict[x] if x in colors_dict else '')\nplt.subplot(1,2,2)\nmake_plot(df=test,grouper='ColorName1_',palette=palette_,col='Type',title='Counts for test set based on color')\n\n","61512dbe":"plt.figure(figsize=(18,8))\nplt.subplots_adjust(hspace=0.9,wspace = 0.5)\n# palette_=['Black', 'Brown', '#FFFDD0', 'Gray', 'Gold', 'White', 'Yellow']\nfor i,v in enumerate(train_ed[\"ColorName1\"].unique()):\n    plt.subplot(2,4,i+1)\n    g=sns.countplot(x='AdoptionSpeed',data=train_ed[train_ed[\"ColorName1\"]==v])\n    g.set_title(v + \" vs Adoption Speed\")\n    g.set_xticklabels(labels = train_ed[\"AdoptionSpeed\"].unique(),rotation = -60,ha='left')\n    colors_dict = dict(train_ed.loc[train_ed[\"ColorName1\"] == v, \"AdoptionSpeed\"].value_counts())\n    plot_dict={}\n    for k,v in colors_dict.items():\n        if k not in plot_dict:\n                plot_dict[colors_dict[k]]=colors_dict[k]\/sum(colors_dict.values())*100\n        else:\n            plot_dict[0]=0\n    for x in g.patches:\n        height = x.get_height()\n        g.text(x.get_x() + x.get_width()\/2,height,f\"{plot_dict[height]:.0f}%\",\n               ha='center',va='bottom',fontsize = 8,weight=\"semibold\",size=\"large\")","0b9c4696":"train_ed[\"All_Colors\"] = train_ed[\"ColorName1\"] + train_ed[\"ColorName2\"] + train_ed[\"ColorName3\"]\ntrain_ed[\"All_Colors\"].unique()","dbb52df3":"#Selecting the top 5 values\ntop_5_colors = train_ed[\"All_Colors\"].value_counts()[:5]\ntop_5_color = dict(top_5_colors)\ntop_5_color","291c0169":"sum_fee = train_ed[\"Fee\"].value_counts()[:10]\nsum_fee","9f3fd14b":"print(sum_fee.sum())","9e0245d1":"plt.figure(figsize=(12,8))\nsns.scatterplot(x='AdoptionSpeed',y='Fee',data=train_ed,palette=\"summer\",hue='Type')","5aabd859":"plt.figure(figsize=(16, 10));\nsns.scatterplot(x=\"Fee\", y=\"Quantity\", hue=\"Type\",data=train_ed);\nplt.title('Quantity of pets and Fee');","02ff1890":"train_ed.columns","9cfe3638":"plt.figure(figsize=(12, 10));\nsns.scatterplot(x=\"AdoptionSpeed\", y=\"Age\", hue=\"Type\",data=train_ed);\nplt.title('Age vs Adoption speed for each Pet type');","74a4b9e1":"train_ed.groupby('MainBreed')[\"Type\"].value_counts().sort_values(ascending=False)\n","23bd766d":"top_5_index= [x for x in train_ed[\"StateName\"].value_counts()[:5].index]\ntop_5_values= [x for x in train_ed[\"StateName\"].value_counts()[:5].values]\ntop_5_states = {k:v for k,v in zip(top_5_index,top_5_values)}\nplt.figure(figsize = (20,10))\nplt.subplots_adjust(hspace = 0.9,wspace = 0.3)\ncount = 1\nfor k,v in top_5_states.items():\n    plt.subplot(2,3,count)\n    g = sns.countplot(x='AdoptionSpeed',data=train_ed[train_ed[\"StateName\"]==k],hue='Type')\n    g.set_title(\"StateName:\" + k )\n    g.set_xticklabels(labels = train_ed[\"AdoptionSpeed\"].unique(),rotation =-60,ha='left')\n    count = count + 1\n    g.legend(frameon=False)\n    ","8cf960de":"top_5_index= [x for x in train_ed[\"RescuerID\"].value_counts()[:5].index]\ntop_5_values= [x for x in train_ed[\"RescuerID\"].value_counts()[:5].values]\ntop_5_rescuers = {k:v for k,v in zip(top_5_index,top_5_values)}\nplt.figure(figsize = (20,10))\nplt.subplots_adjust(hspace = 0.9,wspace = 0.8)\ncount = 1\nfor k,v in top_5_rescuers.items():\n    plt.subplot(2,3,count)\n    g = sns.countplot(x='AdoptionSpeed',data=train_ed[train_ed[\"RescuerID\"]==k],hue='Type',palette = 'cool')\n    g.set_title(\"RescuerID: \" + k )\n    g.set_xticklabels(labels = train_ed[\"AdoptionSpeed\"].unique(),rotation =-60,ha='left')\n    count = count + 1\n    g.legend(frameon=False)\n","1682cf93":"plt.figure(figsize=(12,10))\ng=sns.scatterplot(y='AdoptionSpeed',x='PhotoAmt',data=train_ed,palette = 'Green')\ng.set_title('Adoption Speed vs Number of Photos Uploaded')","9383674e":"plt.figure(figsize=(12,10))\ng=sns.scatterplot(y='AdoptionSpeed',x='VideoAmt',data=train_ed)\ng.set_title('Adoption Speed vs Number of Videos Uploaded')","b677b10f":"train_image_files = sorted(glob.glob('..\/input\/train_images\/*.jpg'))\ntrain_metadata_files = sorted(glob.glob('..\/input\/train_metadata\/*.json'))\ntrain_sentiment_files = sorted(glob.glob('..\/input\/train_sentiment\/*.json'))\n\nprint('num of train images files: {}'.format(len(train_image_files)))\nprint('num of train metadata files: {}'.format(len(train_metadata_files)))\nprint('num of train sentiment files: {}'.format(len(train_sentiment_files)))\n\n\ntest_image_files = sorted(glob.glob('..\/input\/test_images\/*.jpg'))\ntest_metadata_files = sorted(glob.glob('..\/input\/test_metadata\/*.json'))\ntest_sentiment_files = sorted(glob.glob('..\/input\/test_sentiment\/*.json'))\n\nprint('num of test images files: {}'.format(len(test_image_files)))\nprint('num of test metadata files: {}'.format(len(test_metadata_files)))\nprint('num of test sentiment files: {}'.format(len(test_sentiment_files)))","817fc857":"#Train Set\n\n#Sentiment\ntrain_sentiment_df = pd.DataFrame(train_sentiment_files,columns={'File'})\ntrain_sentiment_df[\"PetID\"] = train_sentiment_df[\"File\"].apply(lambda x : x.split(\"\/\")[-1].split('.')[0])\nprint(\"Number of pets with sentiments {}\" .format(len(train_sentiment_df[\"PetID\"].unique())) )\npets_with_sentiments = len(np.intersect1d(train_sentiment_df[\"PetID\"].unique(),train_ed[\"PetID\"].unique()))\nprint(\"fraction of pets with sentiments {:.4f}\".format(pets_with_sentiments\/len(train_ed[\"PetID\"])))\n\n#Images\n\ntrain_image_df = pd.DataFrame(train_image_files,columns={'File'})\ntrain_image_df[\"PetID\"] = train_image_df[\"File\"].apply(lambda x : x.split(\"\/\")[-1].split('.')[0].split('-')[0])\nprint(\"Number of pets with images {}\" .format(len(train_image_df[\"PetID\"].unique())) )\npets_with_images = len(np.intersect1d(train_image_df[\"PetID\"].unique(),train_ed[\"PetID\"].unique()))\nprint(\"fraction of pets with images {:.4f}\".format(pets_with_images\/len(train_ed[\"PetID\"])))\n\n#Metadata\ntrain_metadata_df = pd.DataFrame(train_metadata_files,columns={'File'})\ntrain_metadata_df[\"PetID\"] = train_metadata_df[\"File\"].apply(lambda x : x.split(\"\/\")[-1].split('.')[0].split('-')[0])\nprint(\"Number of pets with metadata {}\" .format(len(train_metadata_df[\"PetID\"].unique())) )\npets_with_metadata = len(np.intersect1d(train_metadata_df[\"PetID\"].unique(),train_ed[\"PetID\"].unique()))\nprint(\"fraction of pets with metadata {:.4f}\".format(pets_with_metadata\/len(train_ed[\"PetID\"])))\n\n\n","8debd179":"#Test Set\n\n#Sentiment\ntest_sentiment_df = pd.DataFrame(test_sentiment_files,columns={'File'})\ntest_sentiment_df[\"PetID\"] = test_sentiment_df[\"File\"].apply(lambda x : x.split(\"\/\")[-1].split('.')[0])\nprint(\"Number of pets with sentiments {}\" .format(len(test_sentiment_df[\"PetID\"].unique())) )\npets_with_sentiments = len(np.intersect1d(test_sentiment_df[\"PetID\"].unique(),test[\"PetID\"].unique()))\nprint(\"fraction of pets with sentiments {:.4f}\".format(pets_with_sentiments\/len(test[\"PetID\"])))\n\n#Images\n\ntest_image_df = pd.DataFrame(test_image_files,columns={'File'})\ntest_image_df[\"PetID\"] = test_image_df[\"File\"].apply(lambda x : x.split(\"\/\")[-1].split('.')[0].split('-')[0])\nprint(\"Number of pets with images {}\" .format(len(test_image_df[\"PetID\"].unique())) )\npets_with_images = len(np.intersect1d(test_image_df[\"PetID\"].unique(),test[\"PetID\"].unique()))\nprint(\"fraction of pets with images {:.4f}\".format(pets_with_images\/len(test[\"PetID\"])))\n\n#Metadata\ntest_metadata_df = pd.DataFrame(test_metadata_files,columns={'File'})\ntest_metadata_df[\"PetID\"] = test_metadata_df[\"File\"].apply(lambda x : x.split(\"\/\")[-1].split('.')[0].split('-')[0])\nprint(\"Number of pets with metadata {}\" .format(len(test_metadata_df[\"PetID\"].unique())) )\npets_with_metadata = len(np.intersect1d(test_metadata_df[\"PetID\"].unique(),test[\"PetID\"].unique()))\nprint(\"fraction of pets with metadata {:.4f}\".format(pets_with_metadata\/len(test[\"PetID\"])))\n","43bca4fb":"import json\n#opening the json file:\ndef open_sentiment_file(file):\n    with open(file, 'r') as f:\n        sentiment_file = json.load(f)\n    return sentiment_file\n\ndef open_metadata_file(file):\n    with open(file,'r') as f:\n        metadata_file = json.load(f)\n    return metadata_file\n\n\ndef get_sentiment_features(file):\n    file_content = [x[\"text\"][\"content\"] for x in file[\"sentences\"]]\n    file_content =  \"\".join(file_content)\n    file_entities = [x['name'] for x in file['entities']]\n    file_entities = \" \".join(file_entities)\n    sentences_sentiment = [x[\"sentiment\"] for x in file[\"sentences\"]]\n    file_sentiment_df = pd.DataFrame.from_dict(sentences_sentiment,orient = 'columns').sum()\n    file_sentiment_df = file_sentiment_df.to_dict()\n    sentiment_df = pd.DataFrame.from_dict(file_sentiment_df,orient = 'index').T\n    sentiment_df[\"Entities\"] = file_entities\n    sentiment_df[\"Description\"] = file_content \n    \n    return sentiment_df    \n\n\ndef get_metadata_features(file):\n    if 'labelAnnotations' in list(file.keys()):\n        file_score = np.mean([x['score'] for x in file['labelAnnotations']])\n        file_top_desc = [x['description'] for x in file['labelAnnotations']]\n    else:\n        file_score = np.nan\n        file_top_desc = ['']\n    color_ = file['imagePropertiesAnnotation']['dominantColors']['colors'] \n    color_score = np.mean([x['score'] for x in color_])\n    color_pixelfraction = np.mean([x['pixelFraction'] for x in color_])\n    crop_ = file['cropHintsAnnotation']['cropHints']\n    confidence = np.mean([x['confidence'] for x in crop_])\n    if 'importanceFraction' in crop_[0].keys():\n        imp_fraction = np.mean([x['importanceFraction'] for x in crop_])\n    else:\n        imp_fraction = np.nan\n    \n    metadata_df = {'score':file_score,\n                  'color_score': color_score,\n                  'color_pixelfraction':color_pixelfraction,\n                  'confidence':confidence,\n                  'imp_fraction':imp_fraction,\n                   'annots_top_desc':\" \".join(file_top_desc)\n                  }\n    metadata_df = pd.DataFrame.from_dict(metadata_df,orient = 'index').T\n    return metadata_df\n    \n\n#Final Function for extracting features from metadata and sentiment files\ndef extract_features(sentiment_files,metadata_files):\n    final_metadata_df = pd.DataFrame()\n    final_sentiment_df = pd.DataFrame()\n    metadata_petid = []\n    sentiment_petid = []\n    for i in metadata_files:\n        f = open_metadata_file(i)\n        df = get_metadata_features(f)    \n        final_metadata_df = pd.concat([final_metadata_df,df],ignore_index = True)\n        pet = i.split('\/')[-1].split('.')[0].split('-')[0]\n        metadata_petid.append(pet)\n    final_metadata_df[\"PetID\"] = metadata_petid\n        \n    for i in sentiment_files:\n        f = open_sentiment_file(i)\n        df = get_sentiment_features(f)    \n        final_sentiment_df = pd.concat([final_sentiment_df,df],ignore_index = True)\n        pet = i.split('\/')[-1].split('.')[0]\n        sentiment_petid.append(pet)\n    final_sentiment_df[\"PetID\"] = sentiment_petid\n        \n    dfs = [final_sentiment_df,final_metadata_df]\n    return dfs\ndfs_train = extract_features(train_sentiment_files,train_metadata_files)\nprint(dfs_train[0].shape,dfs_train[1].shape)\n\n\n","b2d27124":"dfs_test = extract_features(test_sentiment_files,test_metadata_files)\nprint(dfs_test[0].shape,dfs_test[1].shape)","f0f930c5":"dfs_train[0].head()","e61463d9":"metadata_train_df = dfs_train[1]\nmetadata_test_df = dfs_test[1]\nsentiment_train_df = dfs_train[0]\nsentiment_test_df = dfs_test[0]\n\n\n","12261aee":"#Train\n#Metadata\nmetadata_train_desc = metadata_train_df.groupby(['PetID'])['annots_top_desc'].unique()\nmetadata_train_desc = metadata_train_desc.reset_index()\nmetadata_train_desc[\n    'annots_top_desc'] = metadata_train_desc[\n    'annots_top_desc'].apply(lambda x: ' '.join(x))\nmetadata_train_grouper= metadata_train_df.drop([\"annots_top_desc\"],axis = 1)\nfor i in metadata_train_grouper.columns:\n    if 'PetID' not in i :\n        metadata_train_grouper[i]=metadata_train_grouper[i].astype('float')\nmetadata_train_grouper = metadata_train_grouper.groupby(\"PetID\").agg(['mean','sum'])\nmetadata_train_grouper.columns = pd.Index(['{}_{}_{}'.format('metadata',x[0],x[1].upper()) for x in metadata_train_grouper.columns ])\n\n #Sentiment\n    \nsentiment_train_desc = sentiment_train_df[[\"Entities\",\"PetID\"]]\n#     'Entities'] = sentiment_train_desc[\n#     'Entities'].apply(lambda x: ' '.join(x))\nsentiment_train_grouper= sentiment_train_df.drop([\"Entities\",\"Description\"],axis = 1)\nfor i in sentiment_train_grouper.columns:\n    if 'PetID' not in i :\n        sentiment_train_grouper[i]=sentiment_train_grouper[i].astype('float')\nsentiment_train_grouper = sentiment_train_grouper.groupby(\"PetID\").agg(['mean','sum'])\nsentiment_train_grouper.columns = pd.Index(['{}_{}_{}'.format('sentiment',x[0],x[1].upper()) for x in sentiment_train_grouper.columns ])\n\n        \n#Test   \nmetadata_test_desc = metadata_test_df.groupby(['PetID'])['annots_top_desc'].unique()\nmetadata_test_desc = metadata_test_desc.reset_index()\nmetadata_test_desc[\n    'annots_top_desc'] = metadata_test_desc[\n    'annots_top_desc'].apply(lambda x: ' '.join(x))\nmetadata_test_grouper= metadata_test_df.drop([\"annots_top_desc\"],axis = 1)\nfor i in metadata_test_grouper.columns:\n    if 'PetID' not in i :\n        metadata_test_grouper[i]=metadata_test_grouper[i].astype('float')\nmetadata_test_grouper = metadata_test_grouper.groupby(\"PetID\").agg(['mean','sum'])\nmetadata_test_grouper.columns = pd.Index(['{}_{}_{}'.format('metadata',x[0],x[1].upper()) for x in metadata_test_grouper.columns ])\n\n #Sentiment\n    \nsentiment_test_desc = sentiment_test_df[[\"Entities\",\"PetID\"]]\n# sentiment_test_desc.reset_index()\n# sentiment_test_desc[\n#     'Entities'] = sentiment_test_desc[\n#     'Entities'].apply(lambda x: ' '.join(x))\nsentiment_test_grouper= sentiment_test_df.drop([\"Entities\",\"Description\"],axis = 1)\nfor i in sentiment_test_grouper.columns:\n    if 'PetID' not in i :\n        sentiment_test_grouper[i]=sentiment_test_grouper[i].astype('float')\nsentiment_test_grouper = sentiment_test_grouper.groupby(\"PetID\").agg(['mean','sum'])\nsentiment_test_grouper.columns = pd.Index(['{}_{}_{}'.format('sentiment',x[0],x[1].upper()) for x in sentiment_test_grouper.columns ])\n\n","1dd77284":"#Train \n\ntrain_final = train.copy()\ntrain_final = train_final.merge(metadata_train_grouper,on='PetID',how = 'left')\ntrain_final = train_final.merge(sentiment_train_grouper,on='PetID',how = 'left')\ntrain_final = train_final.merge(metadata_train_desc,on='PetID',how = 'left')\ntrain_final = train_final.merge(sentiment_train_desc,on='PetID',how = 'left')\n\n#Test\n \ntest_final = test.copy()\ntest_final = test_final.merge(metadata_test_grouper,on='PetID',how = 'left')\ntest_final = test_final.merge(sentiment_test_grouper,on='PetID',how = 'left')\ntest_final = test_final.merge(metadata_test_desc,on='PetID',how = 'left')\ntest_final = test_final.merge(sentiment_test_desc,on='PetID',how = 'left')\n\nprint(train_final.shape)\nprint(test_final.shape)\n","d5bce387":"X = pd.concat([train_final,test_final],ignore_index=True,sort=False)\n","edf29932":"print(\"NaN structure is: \", X.isna().sum())","bf7635f5":"col_types = X.dtypes\nprint(\"Integer columns: \",col_types[col_types=='int'])\nprint(\"Float Columns: \",col_types[col_types=='float'])\nprint(\"Categorical Columns: \",col_types[col_types=='object'])\n","eb495400":"X_temp = X.copy()\n# Count RescuerID occurrences:\nrescuer_count = X.groupby(['RescuerID'])['PetID'].count().reset_index()\nrescuer_count.columns = ['RescuerID', 'RescuerID_COUNT']\n\n# Merge as another feature onto main DF:\nX_temp = X_temp.merge(rescuer_count, how='left', on='RescuerID')","e2c0de38":"text_columns = ['Description','Entities','annots_top_desc']\n#PetID won't make relevance as a feature\n#RescuerID will be dropped as one more feature will be made using this in the coming markdown\n#Names are all unique in the dataset, hence it makes no sense to keep it as a feature\nto_drop_columns = ['PetID','RescuerID','Name']","ad956e2b":"\nX_text = X_temp[text_columns]\n\nfor i in X_text.columns:\n    X_text.loc[:, i] = X_text.loc[:, i].fillna('<MISSING>')","28e26035":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import SparsePCA, TruncatedSVD, LatentDirichletAllocation, NMF\n\nn_components = 5\ntext_features = []\n\n\n# Generate text features:\nfor i in X_text.columns:\n    \n    # Initialize decomposition methods:\n    print('generating features from: {}'.format(i))\n    svd_ = TruncatedSVD(\n        n_components=n_components, random_state=1337)\n    nmf_ = NMF(\n        n_components=n_components, random_state=1337)\n    \n    tfidf_col = TfidfVectorizer().fit_transform(X_text.loc[:, i].values)\n    svd_col = svd_.fit_transform(tfidf_col)\n    svd_col = pd.DataFrame(svd_col)\n    svd_col = svd_col.add_prefix('SVD_{}_'.format(i))\n    \n    nmf_col = nmf_.fit_transform(tfidf_col)\n    nmf_col = pd.DataFrame(nmf_col)\n    nmf_col = nmf_col.add_prefix('NMF_{}_'.format(i))\n    \n    text_features.append(svd_col)\n    text_features.append(nmf_col)\n\n    \n# Combine all extracted features:\ntext_features = pd.concat(text_features, axis=1)\n\n# Concatenate with main DF:\nX_temp = pd.concat([X_temp, text_features], axis=1)\n\n# Remove raw text columns:\nfor i in X_text.columns:\n    X_temp = X_temp.drop(i, axis=1)","2fa51693":"X_temp.shape","165784f6":"X_temp = X_temp.drop(to_drop_columns,axis = 1)\nX_temp = X_temp.drop('ColorName1_',axis = 1)\n\n","11d644a5":"# Split into train and test again:\nX_train = X_temp.loc[np.isfinite(X_temp.AdoptionSpeed), :]\nX_test = X_temp.loc[~np.isfinite(X_temp.AdoptionSpeed), :]\n\n# Remove missing target column from test:\nX_test = X_test.drop(['AdoptionSpeed'], axis=1)\n\n\nprint('X_train shape: {}'.format(X_train.shape))\nprint('X_test shape: {}'.format(X_test.shape))\n\nassert X_train.shape[0] == train.shape[0]\nassert X_test.shape[0] == test.shape[0]\n\n\n# Check if columns between the two DFs are the same:\ntrain_cols = X_train.columns.tolist()\ntrain_cols.remove('AdoptionSpeed')\n\ntest_cols = X_test.columns.tolist()\n\nassert np.all(train_cols == test_cols)","5b4ebe01":"np.sum(X_train.isnull())","0755da01":"np.sum(X_test.isnull())","34fa2104":"import scipy as sp\n\nfrom collections import Counter\nfrom functools import partial\nfrom math import sqrt\n\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn.metrics import confusion_matrix as sk_cmatrix\n\n\n# FROM: https:\/\/www.kaggle.com\/myltykritik\/simple-lgbm-image-features\n\n# The following 3 functions have been taken from Ben Hamner's github repository\n# https:\/\/github.com\/benhamner\/Metrics\ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the confusion matrix between rater's ratings\n    \"\"\"\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the counts of each type of rating that a rater made\n    \"\"\"\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(y, y_pred):\n    \"\"\"\n    Calculates the quadratic weighted kappa\n    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n    value, which is a measure of inter-rater agreement between two raters\n    that provide discrete numeric ratings.  Potential values range from -1\n    (representing complete disagreement) to 1 (representing complete\n    agreement).  A kappa value of 0 is expected if all agreement is due to\n    chance.\n    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n    each correspond to a list of integer ratings.  These lists must have the\n    same length.\n    The ratings should be integers, and it is assumed that they contain\n    the complete range of possible ratings.\n    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n    is the minimum possible rating, and max_rating is the maximum possible\n    rating\n    \"\"\"\n    rater_a = y\n    rater_b = y_pred\n    min_rating=None\n    max_rating=None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = confusion_matrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              \/ num_scored_items)\n            d = pow(i - j, 2.0) \/ pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] \/ num_scored_items\n            denominator += d * expected_count \/ num_scored_items\n\n    return (1.0 - numerator \/ denominator)\n\nclass OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n\n        ll = quadratic_weighted_kappa(y, X_p)\n        return -ll\n\n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n        return X_p\n\n    def coefficients(self):\n        return self.coef_['x']\n    \ndef rmse(actual, predicted):\n    return sqrt(mean_squared_error(actual, predicted))","229b9a09":"import lightgbm as lgb\n\nparams = {'application': 'regression',\n          'boosting': 'gbdt',\n          'metric': 'rmse',\n          'num_leaves': 70,\n          'max_depth': 9,\n          'learning_rate': 0.01,\n          'bagging_fraction': 0.85,\n          'feature_fraction': 0.8,\n          'min_split_gain': 0.02,\n          'min_child_samples': 150,\n          'min_child_weight': 0.02,\n          'lambda_l2': 0.0475,\n          'verbosity': -1,\n          'data_random_seed': 17}\n\n# Additional parameters:\nearly_stop = 500\nverbose_eval = 100\nnum_rounds = 10000\nn_splits = 5","c115d1e0":"from sklearn.model_selection import StratifiedKFold\n\n\nkfold = StratifiedKFold(n_splits=n_splits, random_state=1337)\n\n\noof_train = np.zeros((X_train.shape[0]))\noof_test = np.zeros((X_test.shape[0], n_splits))\n\n\ni = 0\nfor train_index, valid_index in kfold.split(X_train, X_train['AdoptionSpeed'].values):\n    \n    X_tr = X_train.iloc[train_index, :]\n    X_val = X_train.iloc[valid_index, :]\n    \n    y_tr = X_tr['AdoptionSpeed'].values\n    X_tr = X_tr.drop(['AdoptionSpeed'], axis=1)\n    \n    y_val = X_val['AdoptionSpeed'].values\n    X_val = X_val.drop(['AdoptionSpeed'], axis=1)\n    \n    print('\\ny_tr distribution: {}'.format(Counter(y_tr)))\n    \n    d_train = lgb.Dataset(X_tr, label=y_tr)\n    d_valid = lgb.Dataset(X_val, label=y_val)\n    watchlist = [d_train, d_valid]\n    \n    print('training LGB:')\n    model = lgb.train(params,\n                      train_set=d_train,\n                      num_boost_round=num_rounds,\n                      valid_sets=watchlist,\n                      verbose_eval=verbose_eval,\n                      early_stopping_rounds=early_stop)\n    \n    val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n    test_pred = model.predict(X_test, num_iteration=model.best_iteration)\n    \n    oof_train[valid_index] = val_pred\n    oof_test[:, i] = test_pred\n    \n    i += 1","0dea2c5a":"plt.hist(oof_train)","bab150d0":"# Compute QWK based on OOF train predictions:\noptR = OptimizedRounder()\noptR.fit(oof_train, X_train['AdoptionSpeed'].values)\ncoefficients = optR.coefficients()\npred_test_y_k = optR.predict(oof_train, coefficients)\nprint(\"\\nValid Counts = \", Counter(X_train['AdoptionSpeed'].values))\nprint(\"Predicted Counts = \", Counter(pred_test_y_k))\nprint(\"Coefficients = \", coefficients)\nqwk = quadratic_weighted_kappa(X_train['AdoptionSpeed'].values, pred_test_y_k)\nprint(\"QWK = \", qwk)","0b4c79eb":"# Manually adjusted coefficients:\n\ncoefficients_ = coefficients.copy()\n\ncoefficients_[0] = 1.645\ncoefficients_[1] = 2.115\ncoefficients_[3] = 2.84\n\ntrain_predictions = optR.predict(oof_train, coefficients_).astype(int)\nprint('train pred distribution: {}'.format(Counter(train_predictions)))\n\ntest_predictions = optR.predict(oof_test.mean(axis=1), coefficients_)\nprint('test pred distribution: {}'.format(Counter(test_predictions)))","d1867caf":"# Distribution inspection of original target and predicted train and test:\n\nprint(\"True Distribution:\")\nprint(pd.value_counts(X_train['AdoptionSpeed'], normalize=True).sort_index())\nprint(\"\\nTrain Predicted Distribution:\")\nprint(pd.value_counts(train_predictions, normalize=True).sort_index())\nprint(\"\\nTest Predicted Distribution:\")\nprint(pd.value_counts(test_predictions, normalize=True).sort_index())","02f9e6c3":"# Generate submission:\n\nsubmission = pd.DataFrame({'PetID': test['PetID'].values, 'AdoptionSpeed': test_predictions.astype(np.int32)})\nsubmission.head()\nsubmission.to_csv('submission.csv', index=False)","e6b86f4d":"Based on the plot, Fees and Quantity have an inverted relationship. Lesser the number of pets, higher is the fee.","705f95d5":"As we can see, more than 14,000 pets in the data are bought in under 300 Singapore Dollars. Most of the Animals (Around 12.5k) in the listing who are adopted are free of cost. Let's analyse the Adoption Speed Based on the Fee charged and Type for the Animal","5dca7e1e":"From the plot, it is pretty clear that most pets within fee range below 1000 dollars  are adopted more. There is only 1 dog costing 3000 $ who also is adopted within 2-3 months. This basically means that generally people adopt pets who are mid-range expensive\n\nWe now analyse the relationship between Quantity and Fee","09a5cb06":"We'll have a look at how top 5 State Names of the pet does\/doesn't affect the adoption Speed","ee75b0d5":"These are the top 5 colors which are listed in the dataset.\nNow let's analyse the effect of Fee on Adoption Speed","cb10f928":"Pets who have less photos\/videos uploaded are adopted early(within a week), this might be so because the person adopting wouldn't have taken much time to go over all the photos\/videos for pets with higher number for the same. \n","8cebdea4":"Pets are generally adopted within 0-100 month range. A few older cats are adopted too. But there are dogs above 150 months who aren't adopted. As cats live longer than dogs, it would make sense to adopt an older cat than a dog.","3cf8c5ef":"Based on the graphs, we can be sure that animals that are vaccinated in the earlier listing period are less likely to be adopted as compared to animals who aren't vaccinated. \nMore than 30% of Dogs who are vaccinated are not even Adopted.\nOn the other hand, more than 75% of pets who aren't Vaccinated are Adopted.\n\nPeople tend to prefer non.sterilized pets as we can see that almost 80% are adopted within 90 days of their listing.\n\nIt can be considered a general conception that people would prefer Healthy Cats and Dogs, and the data proves it too! \nThe number of animals with injuries (Major or Minor) are significantly less and a major chunk of them also don't get adopted.\n","338598aa":"As we'll be doing EDA on training set. We need to change the numerical notation with text.\n","6deb5137":"There aren't any special observations except the fact that most common Maturity Size is Medium.\nSo it might be the case that there is bias in the data for number of observations. (Graph 4 shows that **medium** Maturity Size pets are higher in Number )\n","2aefa983":"Similar observations go for furlength feature **(Short Fur animals are most common in the listings, Hence introducing bias)**\nSo what we can do is, check for the same bias in the test set.","54a9327b":"Same observations are seen, i.e, Medium Sized or Small Furred pets are most common in the listings.\nSo to prevent bias, we'll have to do PCA or regularization to prevent overfitting in later stages.\n\nWe now see the effect of pet color on Adoption Speed","c043025f":"We can now see that Metadata dataframe has multiple numerical feature values for 1 pet ID, So we can perform some mathematical operations for aggregation purposes.","53ecb751":"So now it's safe to assume that we can extract features from json as well as jpg files","336982aa":"The most common animals in both the datasets are Black and Brown in color.\n\nWe can now see how various colors affect adoption speed","9d96004c":"Now we analyse relationship for MaturitySize and Furlength with AdoptionSpeed.\nFirst we look into MaturitySize for Cats and Dogs individually\n","536db735":"We can now join train and test dataset and do some feature engineering.","b67d2e6d":"Let's import the metadata, image and the sentiment data","d31baf2b":"Even though Black and Brown are the most common colors for pets in the dataset, There are more than 2000 black colored pets who are unadopted.\n\nLet's also analyse AdoptionSpeed across various combinations of animal color. (ColorName1,ColorName2 and ColorName3)","212cbf40":"We can see that more number of dogs are listed on the website.\nNow let's see what percentage of Cat\/Dog are getting adopted, and at what speed.","7512fb0e":"Reference for below code: https:\/\/www.kaggle.com\/wrosinski\/baselinemodeling","a036b6ef":"Now that we have converted our data for EDA purpose, let's drive insights from the same.\nFirst we'll see whether which animal has more listings in the set.","d68c60d0":"Let's see which colored animals are the highest in number","df9567a6":"Now we can merge the extracted features with our initial Training data as well as test data\n        ","d3740f9d":"Let's get an idea of what the number of pets are in train and test sets in terms of Sentiment, Metadata and images so that we can add them to main Dataframe for adding features.","5f24ac66":"Based on the above plot, we can infer that Cat and Dog adoption increases sharply within a week of their listings.\nBut  a quarter of the total Cats and one-third of the Dogs still remain unadopted. There are other factors to consider like age, health etc as to why this is happening.\n\nNow we can further analyse how Health factors like Vaccinated, Dewormed, Sterilized, Health affected Adoption Speeds.","b931e049":"There aren't any particular insights to this  except the fact that unadopted pets in Kula Lampur are high in number. Plus most of the pets in Selangor, Pulau Pinang nad Perak are adopted.\n\nNext, we look into the various Rescuers.","732dbbb3":"Dogs are rescued generally more in number by the top 5 rescuers. Consequently they are adopted more. One of the rescuers(Plot 4) hasn't even adopted any cats So which would mean that  in most cases, cats who are rescued must be rescued independently. Whereas dogs are rescued by organizations who have high rescue numbers.\n\nNow analysing pet adoption rates based on photos and videos uploaded for the same.","57d078cb":"We can check whether age of a pet(Cat or Dog) affects Adoption Speed or not","f312f69c":"Now we can divide our dataset into test and training set"}}