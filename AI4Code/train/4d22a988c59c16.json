{"cell_type":{"d7937988":"code","7e1c82b8":"code","818665dd":"code","22831b4c":"code","3744ae98":"code","a904e588":"code","12332c2d":"code","952053d4":"code","c03ad407":"code","59573615":"code","254bb399":"code","2c7363ee":"code","e01e4495":"code","524b9511":"code","1647b2d4":"code","589eba79":"code","a1c2fdc6":"code","92f7554c":"code","cab1ff04":"code","7437b38c":"code","a7855e14":"code","e45f67fc":"code","4bc51e3f":"code","c38d5660":"code","b72a2e5a":"code","8e84b522":"code","09046dde":"code","ad53ef62":"code","77784eb8":"code","02e5bc5b":"code","ea51bcfc":"code","229eed6b":"code","beb7231c":"code","6ba2b854":"code","5a653c8e":"code","11677e4e":"markdown","98695d15":"markdown","ed138a8b":"markdown","37051c70":"markdown","82146c28":"markdown","cce93063":"markdown","6c22dd41":"markdown","123701a2":"markdown","7e0ae303":"markdown","3b34ada9":"markdown","49e22254":"markdown","a719a448":"markdown","ca4df52d":"markdown","45387dc8":"markdown","02339f67":"markdown","8247083a":"markdown","88bb77dc":"markdown","8b67becb":"markdown","4840e29b":"markdown","c33f09d3":"markdown","41e7d64b":"markdown","53d4c19b":"markdown","136855a8":"markdown","d516488f":"markdown","07349c6e":"markdown","96c20b40":"markdown","e94098da":"markdown"},"source":{"d7937988":"import torch\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"{device}\" \" is available.\")","7e1c82b8":"df = pd.read_csv('..\/input\/charleston-sea-level-daily-dataset-1921-2014\/daily Charleston 1921-2014.csv')","818665dd":"# drop data in first line because its not technically data that we need\ndf.drop([0], inplace=True)\ndf.drop(columns=['depth', 'latitude', 'longitude', 'sensor_type_code'], inplace=True)\n\n# change data types\ndf['time'] = pd.to_datetime(df['time'])\ndf['sea_surface_height_above_reference_level'] = pd.to_numeric(df['sea_surface_height_above_reference_level'])\ndf.dtypes","22831b4c":"# check missing value and percentage\ndf_notna = pd.DataFrame(df.notna().sum()).rename(columns={0:'notna'}).reset_index()\ndf_isna = pd.DataFrame(df.isna().sum()).rename(columns={0:'isna'}).reset_index()\ndf_isna_perc = df_notna.merge(df_isna, on='index', how='inner')\ndf_isna_perc['isna_percentage'] = df_isna_perc['isna']\/(df_isna_perc['isna']+df_isna_perc['notna'])*100\ndf_isna_perc.sort_values('isna_percentage',ascending=False).head()","3744ae98":"# Check missing value location in the data\nplt.figure(figsize=(10, 3))\n_ = sns.heatmap(df.isnull().T, cbar=False) #transpose the data so its easier to see","a904e588":"def plot_dataset_trendline(df, title, label='value'):\n    plt.figure(figsize=(15, 5))\n    # plot dataset\n    sns.lineplot(x=df.index, y=df.value, label=label)\n\n    # plot trendline (have to be clean from missing data)\n    x = range(0, len(df))\n    z = np.polyfit(x, y=df.value, deg=1)  \n    p = np.poly1d(z)\n    plt.plot(df.index, p(x), c=\"r\", ls='-')\n\n    plt.title(title)\n    plt.xlabel('Time')\n    plt.ylabel('Sea Level (mm)')\n    print(f'Total Sea level rise: {round(p(len(df)) - p(0), 2)} mm')","12332c2d":"# make a new dataset for plotting (because we need to drop some missing values)\ndf_plot = df.dropna()\ndf_plot['year'] = df_plot['time'].dt.year\ndf_plot = df_plot.set_index(['time'])\ndf_plot = df_plot.rename(columns={'sea_surface_height_above_reference_level': 'value'})\n\n# plot all of the dataset from 1921 - 2014\nplot_dataset_trendline(df_plot, 'Sea Level in Charleston, USA (1921 - 2014)')","952053d4":"# plot latest 20 year of the dataset\nplot_dataset_trendline(df_plot[df_plot['year'] > 1994], 'Sea Level in Charleston, USA (1995 - 2014)')","c03ad407":"from statsmodels.tsa.seasonal import seasonal_decompose","59573615":"df_plot2 = df_plot.resample('M').mean()\ndf_plot2.reset_index(inplace= True)\ndf_plot2['time'] = df_plot2['time'].dt.to_period('M')\ndf_plot2 = df_plot2.set_index('time')\ndf_plot2.interpolate(inplace = True)\ndf_plot2.index = df_plot2.index.to_timestamp()\ndf_plot2.head()","254bb399":"plt.rc(\"figure\", figsize=(15,8))\nresult = seasonal_decompose(df_plot2['value'], model= 'additive')\nresult.plot()\nplt.show()","2c7363ee":"# subsetting data from 1995 until 2014\ndf['year'] = df['time'].dt.year\ndf_m95 = df[df['year'] > 1994]\n\ndf_m95 = df_m95.reset_index()\ndf_m95.drop(columns=['index', 'year'], inplace=True)\n\n# make a copy as a safety measure for later process\ndf_try = df_m95.copy()\ndf_try.tail(2)","e01e4495":"df_try = df_try.set_index(['time'])\ndf_try = df_try.rename(columns={'sea_surface_height_above_reference_level': 'value'})\n\n# plot the dataset that we're gonna use\nplot_dataset_trendline(df_try, title='Sea Level in Charleston, USA (1995 - 2014)')","524b9511":"# generate day, month, and week of year feature\ndf_features = (df_try\n                .assign(day = df_try.index.day)\n                .assign(month = df_try.index.month)\n                .assign(week_of_year = df_try.index.week)\n                .assign(year = df_try.index.year)\n              )","1647b2d4":"def onehot_encode_pd(df, cols):\n    for col in cols:\n        dummies = pd.get_dummies(df[col], prefix=col)\n        df = pd.concat([df, dummies], axis=1) #.drop(columns=col)\n    return df\n\n# one-hot encoding for categorical value from datetime feature\ndf_features = onehot_encode_pd(df_features, ['day', 'month', 'week_of_year', 'year'])\ndf_features['year_2015'], df_features['year_2016'] = 0, 0 # for predicting future event purpose","589eba79":"def generate_cyclical_features(df, col_name, period, start_num=0):\n    kwargs = {\n        f'sin_{col_name}' : lambda x: np.sin(2*np.pi*(df[col_name]-start_num)\/period),\n        f'cos_{col_name}' : lambda x: np.cos(2*np.pi*(df[col_name]-start_num)\/period)    \n             }\n    return df.assign(**kwargs).drop(columns=[col_name])\n\ndf_features = generate_cyclical_features(df_features, 'day', 31, 1)\ndf_features = generate_cyclical_features(df_features, 'month', 12, 1)\ndf_features = generate_cyclical_features(df_features, 'week_of_year', 52, 0)\n\ndf_features.tail(3)","a1c2fdc6":"from sklearn.model_selection import train_test_split\n\n# function for splitting target dan predictor variable\ndef feature_label_split(df, target_col):\n    y = df[[target_col]]\n    X = df.drop(columns=[target_col])\n    return X, y\n\n# function for splitting data to train, test, and validation data\ndef train_val_test_split(df, target_col, test_ratio):\n    val_ratio = test_ratio \/ (1 - test_ratio)\n    X, y = feature_label_split(df, target_col)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, shuffle=False)\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_ratio, shuffle=False)\n    return X_train, X_val, X_test, y_train, y_val, y_test\n\nX_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(df_features, 'value', 0.2)","92f7554c":"from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler\n\n# function to pick scaler \ndef get_scaler(scaler):\n    scalers = {\n        \"minmax\": MinMaxScaler,\n        \"standard\": StandardScaler,\n        \"maxabs\": MaxAbsScaler,\n        \"robust\": RobustScaler,\n    }\n    return scalers.get(scaler.lower())()","cab1ff04":"scaler = get_scaler('minmax')\n\n# fit and apply scaler to predictor variable\nX_train_arr = scaler.fit_transform(X_train)\nX_val_arr = scaler.transform(X_val)\nX_test_arr = scaler.transform(X_test)\n\n# fit and apply scaler to target variable\ny_train_arr = scaler.fit_transform(y_train)\ny_val_arr = scaler.transform(y_val)\ny_test_arr = scaler.transform(y_test)","7437b38c":"from torch.utils.data import TensorDataset, DataLoader\n\nbatch_size = 32\n\n# convert data shape to tensor (multi-dimensional matrix containing elements of a single data type)\ntrain_features = torch.Tensor(X_train_arr)\ntrain_targets = torch.Tensor(y_train_arr)\nval_features = torch.Tensor(X_val_arr)\nval_targets = torch.Tensor(y_val_arr)\ntest_features = torch.Tensor(X_test_arr)\ntest_targets = torch.Tensor(y_test_arr)\n\n# wrapping the tensors above as Dataset\ntrain = TensorDataset(train_features, train_targets)\nval = TensorDataset(val_features, val_targets)\ntest = TensorDataset(test_features, test_targets)\n\n# convert data to Pytorch DataLoader (collating data samples into batches)\ntrain_loader = DataLoader(train, batch_size=batch_size, shuffle=False, drop_last=True)\nval_loader = DataLoader(val, batch_size=batch_size, shuffle=False, drop_last=True)\ntest_loader = DataLoader(test, batch_size=batch_size, shuffle=False, drop_last=True)","a7855e14":"import torch.nn as nn\n\nclass LSTMModel(nn.Module):\n    \"\"\"LSTMModel class extends nn.Module class and works as a constructor for LSTMs.\n\n       LSTMModel class initiates a LSTM module based on PyTorch's nn.Module class.\n       It has only two methods, namely init() and forward(). While the init()\n       method initiates the model with the given input parameters, the forward()\n       method defines how the forward propagation needs to be calculated.\n       Since PyTorch automatically defines back propagation, there is no need\n       to define back propagation method.\n\n       --Attributes--\n           hidden_dim: int \n               The number of nodes in each layer\n           layer_dim: int\n               The number of layers in the network\n           lstm: nn.LSTM\n               The LSTM model constructed with the input parameters.\n           fc: nn.Linear\n               The fully connected layer to convert the final state of LSTMs to our desired output shape.\n\n    \"\"\"\n    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n        \"\"\"The __init__ method that initiates a LSTM instance.\n\n        --Arguments--\n            input_dim: int\n                The number of nodes in the input layer\n            hidden_dim: int\n                The number of nodes in each layer\n            layer_dim: int\n                The number of layers in the network\n            output_dim: int\n                The number of nodes in the output layer\n            dropout_prob: float\n                The probability of nodes being dropped out\n\n        \"\"\"\n        super(LSTMModel, self).__init__()\n\n        # Defining the number of layers and the nodes in each layer\n        self.hidden_dim = hidden_dim\n        self.layer_dim = layer_dim\n\n        # LSTM layers\n        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob)\n\n        # Fully connected layer\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        \"\"\"The forward method takes input tensor x and does forward propagation\n\n        --Arguments--\n            x: torch.Tensor \n                The input tensor of the shape (batch size, sequence length, input_dim)\n\n        --Returns--\n            out: torch.Tensor \n                The output tensor of the shape (batch size, output_dim)\n\n        \"\"\"\n        # Initializing hidden state for first input with zeros\n        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n\n        # Initializing cell state for first input with zeros\n        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n\n        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n        # If we don't, we'll backprop all the way to the start even after going through another batch\n        # Forward propagation by passing in the input, hidden state, and cell state into the model\n        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n\n        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n        # so that it can fit into the fully connected layer\n        out = out[:, -1, :]\n\n        # Convert the final state to our desired output shape (batch_size, output_dim)\n        out = self.fc(out)\n\n        return out","e45f67fc":"class Optimization:\n    \"\"\"\n    Optimization is a helper class that takes model, loss function, optimizer function\n    learning scheduler (optional), early stopping (optional) as inputs. In return, it\n    provides a framework to train and validate the models, and to predict future values\n    based on the models.\n\n    --Attributes--\n        model: \n            Model class created for the type of RNN\n        loss_fn: torch.nn.modules.Loss\n            Loss function to calculate the losses\n        optimizer: torch.optim.Optimizer \n            Optimizer function to optimize the loss function\n        train_losses: list[float]\n            The loss values from the training\n        val_losses: list[float]\n            The loss values from the validation\n    \"\"\"\n    def __init__(self, model, loss_fn, optimizer):\n        self.model = model\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n        self.train_losses = []\n        self.val_losses = []\n        \n    def train_step(self, x, y):\n        \"\"\"\n        Given the features (x) and the target values (y) tensors, the method completes\n        one step of the training. First, it activates the train mode to enable back prop.\n        After generating predicted values (yhat) by doing forward propagation, it calculates\n        the losses by using the loss function. Then, it computes the gradients by doing\n        back propagation and updates the weights by calling step() function.\n\n        --Arguments--\n            x: torch.Tensor\n                Tensor for features to train one step\n            y: torch.Tensor\n                Tensor for target values to calculate losses\n\n        \"\"\"\n        # Sets model to train mode\n        self.model.train()\n\n        # Makes predictions\n        yhat = self.model(x)\n\n        # Computes loss\n        loss = self.loss_fn(y, yhat)\n\n        # Computes gradients\n        loss.backward()\n\n        # Updates parameters and zeroes gradients\n        self.optimizer.step()\n        self.optimizer.zero_grad()\n\n        # Returns the loss\n        return loss.item()\n\n    def train(self, train_loader, val_loader, batch_size=64, n_epochs=50, n_features=1):\n        \"\"\"\n        The method takes DataLoaders for training and validation datasets, batch size for\n        mini-batch training, number of epochs to train, and number of features as inputs.\n        Then, it carries out the training by iteratively calling the method train_step for\n        n_epochs times. Finally, it saves the model in a designated file path.\n\n        --Arguments--\n            train_loader: torch.utils.data.DataLoader\n                DataLoader that stores training data\n            val_loader: torch.utils.data.DataLoader\n                DataLoader that stores validation data\n            batch_size: int\n                Batch size for mini-batch training\n            n_epochs: int \n                Number of epochs, i.e., train steps, to train\n            n_features: int\n                Number of feature columns\n\n        \"\"\"\n        model_path = f'model_lstm'\n        \n        for epoch in range(1, n_epochs + 1):\n            # mini-batch training iteration of training datasets\n            batch_losses = []\n            for x_batch, y_batch in train_loader:\n                x_batch = x_batch.view([batch_size, -1, n_features]).to(device)\n                y_batch = y_batch.to(device)\n                loss = self.train_step(x_batch, y_batch)\n                batch_losses.append(loss)\n                # update training loss value\n            training_loss = np.mean(batch_losses)\n            self.train_losses.append(training_loss)\n            \n            with torch.no_grad():\n                # mini-batch training iteration of validation datasets\n                batch_val_losses = []\n                validation = []\n                validation_values = []\n                for x_val, y_val in val_loader:\n                    x_val = x_val.view([batch_size, -1, n_features]).to(device)\n                    y_val = y_val.to(device)\n                    self.model.eval()\n                    yhat = self.model(x_val)\n                    val_loss = self.loss_fn(y_val, yhat).item()\n                    batch_val_losses.append(val_loss)\n                    #\n                    validation.append(yhat.to(device).detach().numpy())\n                    validation_values.append(y_val.to(device).detach().numpy())\n                # update validation loss value\n                validation_loss = np.mean(batch_val_losses)\n                self.val_losses.append(validation_loss)\n            \n            # print loss value per epoch period\n            if (epoch <= 10) | (epoch % 10 == 0):\n                print(\n                    f\"[{epoch}\/{n_epochs}] Training loss: {training_loss:.4f}\\t Validation loss: {validation_loss:.4f}\"\n                )\n\n        torch.save(self.model.state_dict(), model_path)\n        return validation, validation_values\n\n    def evaluate(self, test_loader, batch_size=1, n_features=1):\n        \"\"\"\n        The method takes DataLoaders for the test dataset, batch size for mini-batch testing,\n        and number of features as inputs. Similar to the model validation, it iteratively\n        predicts the target values and calculates losses. Then, it returns two lists that\n        hold the predictions and the actual values.\n\n        Note:\n            This method assumes that the prediction from the previous step is available at\n            the time of the prediction, and only does one-step prediction into the future.\n\n        --Arguments--\n            test_loader: torch.utils.data.DataLoader \n                DataLoader that stores test data\n            batch_size: int\n                Batch size for mini-batch training\n            n_features: int\n                Number of feature columns\n\n        --Returns--\n            predictions: list[float]\n                The values predicted by the model\n            values: list[float]\n                The actual values in the test set.\n\n        \"\"\"\n        # mini-batch testing to evaluate data from test dataset\n        with torch.no_grad():\n            predictions = []\n            values = []\n            for x_test, y_test in test_loader:\n                x_test = x_test.view([batch_size, -1, n_features]).to(device)\n                y_test = y_test.to(device)\n                self.model.eval()\n                yhat = self.model(x_test)\n                # save model prediction result to list\n                predictions.append(yhat.to(device).detach().numpy())\n                values.append(y_test.to(device).detach().numpy())\n\n        return predictions, values\n\n    def predict(self, future_loader, batch_size=1, n_features=1):\n        \"\"\"\n        The method takes DataLoaders for the predicting future dataset, batch size for mini-batch testing,\n        and number of features as inputs. \n\n        --Arguments--\n            test_loader: torch.utils.data.DataLoader \n                DataLoader that stores test data\n            batch_size: int\n                Batch size for mini-batch training\n            n_features: int\n                Number of feature columns\n\n        --Returns--\n            predictions: list[float]\n                The values predicted by the model\n\n        \"\"\"\n        # mini-batch testing to predict data from future dataset\n        with torch.no_grad():\n            predictions = []\n            for x_test in test_loader:\n                x_test = x_test.view([batch_size, -1, n_features]).to(device)\n                self.model.eval()\n                yhat = self.model(x_test)\n                # save model prediction result to list\n                predictions.append(yhat.to(device).detach().numpy())\n\n        return predictions\n    \n    def plot_losses(self):\n        \"\"\"\n        The method plots the calculated loss values for training and validation\n        \"\"\"\n        plt.figure(figsize=[8, 5])\n        plt.plot(self.train_losses, label=\"Training loss\")\n        plt.plot(self.val_losses, label=\"Validation loss\")\n        plt.legend()\n        plt.title(\"Losses\")\n        plt.show()\n        plt.close()","4bc51e3f":"# LSTM config\ninput_dim = len(X_train.columns)\noutput_dim = 1 \nhidden_dim = 64 \nlayer_dim = 4 \nbatch_size = batch_size\ndropout = 0.05\n# training and evaluate config\nn_epochs = 30\n# weight optimization config\nlearning_rate = 1e-3\nweight_decay = 1e-6\n\n# bundle config in dictionary\nmodel_params = {'input_dim': input_dim,\n                'hidden_dim' : hidden_dim,\n                'layer_dim' : layer_dim,\n                'output_dim' : output_dim,\n                'dropout_prob' : dropout}","c38d5660":"import torch.optim as optim\n\nmodel = LSTMModel(**model_params)\n\n# set criterion to calculate loss gradient\nloss_fn = nn.MSELoss(reduction=\"mean\")\n# set model optimizer (process of adjusting model parameters to reduce model error in each training step)\noptimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n\n# training model\nopt = Optimization(model=model, loss_fn=loss_fn, optimizer=optimizer)\nvalidation, validation_values = opt.train(\n    train_loader, \n    val_loader, \n    batch_size=batch_size, \n    n_epochs=n_epochs, \n    n_features=input_dim,\n)\nopt.plot_losses()\n\n# evaluate model based on model from training dataset\npredictions, values = opt.evaluate(\n    test_loader,\n    batch_size=batch_size,\n    n_features=input_dim\n)","b72a2e5a":"# inverse the scale from model result\ndef inverse_transform(scaler, df, columns):\n    for col in columns:\n        df[col] = scaler.inverse_transform(df[col])\n    return df\n\n# change format of multi-dimensional tensors to one-dimensional vectors (flatten)\ndef format_predictions(predictions, values, df_test, scaler):\n    vals = np.concatenate(values, axis=0).ravel()\n    preds = np.concatenate(predictions, axis=0).ravel()\n    df_result = pd.DataFrame(data={\"value\": vals, \"prediction\": preds}, index=df_test.head(len(vals)).index)\n    df_result = df_result.sort_index()\n    df_result = inverse_transform(scaler, df_result, [[\"value\", \"prediction\"]])\n    return df_result","8e84b522":"df_val = format_predictions(validation, validation_values, X_val, scaler)\ndf_result = format_predictions(predictions, values, X_test, scaler)\ndf_result.head()","09046dde":"from sklearn.metrics import mean_absolute_error, mean_squared_error #, mean_absolute_percentage_error\n\ndef calculate_metrics(df):\n    result_metrics = {'mae' : mean_absolute_error(df.value, df.prediction),\n                      'rmse' : np.sqrt(mean_squared_error(df.value, df.prediction)),}\n#                       'mape' : mean_absolute_percentage_error(df.value, df.prediction)}\n    \n    print(\"Mean Absolute Error:       \", result_metrics[\"mae\"])\n    print(\"Root Mean Squared Error:   \", result_metrics[\"rmse\"])\n#     print(\"MAPE Score:                \", result_metrics[\"mape\"])\n    \n    return result_metrics\n\nresult_metrics = calculate_metrics(df_result)","ad53ef62":"def plot_predictions(df_result):\n    plt.figure(figsize=(15,5))\n    value = sns.lineplot(x = df_result.index, y=df_result.value, color='k', alpha = 0.3, label = 'values')\n    prediction = sns.lineplot(x = df_result.index, y=df_result.prediction, color='g', alpha = 0.8, label = 'prediction')\n    \n    plt.title(\"Predictions vs Actual Values of Sea Level\")\n    plt.xlabel('Time')\n    plt.ylabel('Value') \n    \nplot_predictions(df_result)","77784eb8":"def plot_all(df_result, df_train, df_val):\n    plt.figure(figsize=(15,5))\n    observation = sns.lineplot(x = df_train.index, y=df_train.value, color='k', alpha = 0.3, label = 'observation')\n    validation = sns.lineplot(x = df_val.index, y=df_val.prediction, color='b', alpha = 0.8, label = 'validation')\n    prediction = sns.lineplot(x = df_result.index, y=df_result.prediction, color='g', alpha = 0.8, label = 'prediction')\n    \n    plt.title(\"Observation, Validation, and Prediction Values of Sea Level\")\n    plt.xlabel('Time')\n    plt.ylabel('Value') \n    \nplot_all(df_result, df_features, df_val)","02e5bc5b":"def plot_dataset_trendline_after_model(df, title):\n    plt.figure(figsize=(15, 5))\n    # plot dataset\n    sns.lineplot(x=df.index, y=df.value, color = 'k', alpha = 0.3, label='values')\n    sns.lineplot(x=df.index, y=df.prediction, color='g', alpha= 0.5, label='prediction')\n\n    # plot trendline of observation value\n    x = range(0, len(df))\n    z = np.polyfit(x, y=df.value, deg=1)  \n    p = np.poly1d(z)\n    plt.plot(df.index, p(x), c=\"k\", ls='-')\n    print(f'Sea level rise (values) from 2011-2014: {round(p(len(df)) - p(0), 2)} mm')\n    \n    # plot trendline of prediction (have to be clean from missing data)\n    x = range(0, len(df))\n    z = np.polyfit(x, y=df.prediction, deg=1)  \n    p = np.poly1d(z)\n    plt.plot(df.index, p(x), c=\"g\", ls='-')\n    print(f'Sea level rise (prediction) from 2011-2014: {round(p(len(df)) - p(0), 2)} mm')\n    \n    plt.title(title)\n    plt.xlabel('Time')\n    plt.ylabel('Sea Level (mm)')\n    \nplot_dataset_trendline_after_model(df_result, 'Sea Level Rise on Charleston')","ea51bcfc":"def give_year_name(df, start, end, name):\n    i = start\n    while i <= end:\n        df[f'{name}_{i}'] = 0\n        i += 1\n    \n    return df","229eed6b":"from datetime import date\n\nsdate = date(2015,1,1)   # define start date\nedate = date(2016,12,31)   # define end date\n\n# make a dataframe with range of sdate and edate\ndf_future = pd.DataFrame(pd.date_range(sdate, edate, freq='d'))\ndf_future.columns = ['time']\ndf_future['value'] = 0 # it's just here because the function need it as an input\ndf_future.set_index('time', inplace=True)\n\n# make features with datetime object\ndf_future = (df_future\n             .assign(day = df_future.index.day)\n             .assign(month = df_future.index.month)\n             .assign(week_of_year = df_future.index.week)\n             .assign(year = df_future.index.year)\n            )\n\n# one hot encoding the features\ndf_future = onehot_encode_pd(df_future, ['day', 'month', 'week_of_year'])\ndf_future = give_year_name(df_future, 1995, 2014, 'year')\ndf_future = onehot_encode_pd(df_future, ['year'])\n\ndf_future = generate_cyclical_features(df_future, 'day', 31, 1)\ndf_future = generate_cyclical_features(df_future, 'month', 12, 1)\ndf_future = generate_cyclical_features(df_future, 'week_of_year', 52, 0)","beb7231c":"# split the data that we going to predict\nX_fut, y_fut = feature_label_split(df_future, 'value')\n\n#scaling\nscaler = get_scaler('minmax')\nX_train_arr = scaler.fit_transform(X_train)\nX_fut_arr = scaler.transform(X_fut)\n# X_fut_arr = np.array(X_fut)\ny_train_arr = scaler.fit_transform(y_train)\ny_fut_arr = scaler.transform(y_fut)\n\n# convert to tensor\nX_fut_tens = torch.Tensor(X_fut_arr)\ny_fut_tens = torch.Tensor(y_fut_arr)\n\n# convert to Dataset object (pytorch)\nfuture = TensorDataset(X_fut_tens, y_fut_tens)\nfuture_loader = DataLoader(future, batch_size=batch_size, shuffle=False, drop_last=True)","6ba2b854":"# predict using evaluate function\nfut_predictions, fut_values = opt.evaluate(\n    future_loader,\n    batch_size=batch_size,\n    n_features=input_dim\n)\n\n# format the prediction result\ndf_future_predict = format_predictions(fut_predictions, fut_values, X_fut, scaler)","5a653c8e":"def plot_future(df_result, df_train, df_val, df_future):\n    plot_all(df_result, df_train, df_val)\n    future = sns.lineplot(x = df_future.index, y=df_future.prediction, color='r', alpha = 0.8, label = 'future')\n    \nplot_future(df_result, df_features, df_val, df_future_predict)","11677e4e":"## Choosing the data\nFrom the inspection that I have done, there are some sequential missing values in 1952, 1972, and 1991 which is quite a number for each year. Because our data has seasonal composition because of the sky objects phenomenon, it makes the missing data in the middle of dataset can be easily interpolated, but when the data missing is too much the pattern changed and makes some value invalid. Other than that, the model don't calculate physical phenomenon above and only predict based on the pattern made by the data. Because of this reason I choose to drop the data before 1995.","98695d15":"# Making predictions\n\nBefore we get into training model and making predictions, we will make a framework. In this case we will make a helper class that hold training, validation and evaluation methods that we will use later. ","ed138a8b":"## Generate Cyclical Features\nSea level is a product of tidal phenomenon and because the earth, moon, and sun trajectory has effect to tidal, it has a cyclical which happens daily, monthly, and in 18.6 year period. This cyclical features will make our models learn that 31 and 1 in day is as close as 2 and 3. And because the cycle can be interpreted with sine graph, we will make get the sine and cosine from our date components.","37051c70":"## Training the model\nThe model we are going to use is LSTM with AdamW as optimizer that you can explore more [here](https:\/\/pytorch.org\/docs\/stable\/optim.html#algorithms). The training process starts when we call our helper class it does one training process and it continues the training loop with train function.","82146c28":"# Conclusions and Point to Improve","cce93063":"## Applying scale transformation\nScaling is useful because it speeds up the learning by making the model easier to update weight. While the features doesn't need scaling, we use the scaler to make it's shape become numpy array so we have the same input when we processing them to be DataLoader later.","6c22dd41":"From the result above we can see that the model shows the daily and seasonal pattern but doesn't really shows the down pattern from our sea level (the valley on wave patterns) but it shows the increase in trend of sea level rise. Although if we compare the difference on sea level rise from prediction is almost half from our test dataset, but it catch the increase trend. We can try to improve the model by breakdown our seasonal component because our dataset shows a multiseasonal condition, then we try another approach to process our data. \n\nFrom our prediction we can see the increase in trendline, this is the same as the real condition of sea level in Charleston. The climate change is real and increase in sea level rise is happening even right now. The value might be small if we seen it in a short-term but from almost a century of data in Charleston it shows about 30 cm of rise. ","123701a2":"# Exploratory Data Analysis\nThe data in our first line are units of each columns of our data and because we don't need them we will remove it. We also don't need the location and measuring tools specification so we are going to drop them.","7e0ae303":"# References\n1. Kuguoglu, Kaan (2021). Building RNN, LSTM, and GRU for time series using PyTorch. Retrieved from: https:\/\/towardsdatascience.com\/building-rnn-lstm-and-gru-for-time-series-using-pytorch-a46e5b094e7b\n2. Olah, Chris (2015). Understanding LSTM Networks. Retrieved from: https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/\n3. Masri, Fikhri, et. al. Forecasting of Sea Level Time Series using Deep Learning RNN, LSTM, and BiLSTM, Case Study in Jakarta Bay, Indonesia. e-Proceeding of Engineering : Vol.7, No.2 Agustus 2020, Page 8544-8551","3b34ada9":"# Predict Future Sea Level\nAfter we have the model, we are going to predict with the data from 2015 - 2017. We will need a new dataframe containing time column only and use the same process when we prepare our data before.","49e22254":"# Splitting the data into test, validation, and train sets\nWe need to separate our 20 years data as a train, validation, and test dataset for next processing. Here we will use train_test_spplit function from sklearn to separate the data. There is a parameter in this function that will be different from when we use it on normal machine learning problems, which is shuffle. This parameter has a True value as a default and it makes the function takes random rows from our dataframe when separates the data. If we set this parameter as False, the function is going to separate our dataframe sequentially. This is going to be needed when we have a time series forecasting problem.","a719a448":"# Long Short-Term Memory\u00a0(LSTM) Architecture\n\nTraditional neural networks start from scratch every time they are given a task. RNN addresses this shortcoming by looping the information from one step of the network to the next, allowing information to persist within the network. This makes RNN can solve various problems involving sequential data or in this case time-series forecasting. But RNN has a shortcoming, simple RNN can connect previous information to the current one, where the temporal gap between the relevant past information and the current one is small. As that gap grows, RNN become less capable of learning the long-term dependencies, also known as the vanishing gradient problem. This is why we are going to use LSTM or Long Short-Term Memory. <br>\n\n![LSTM3-chain.png](attachment:dc3531d0-3a6a-46ab-87c3-6a6efb826fc1.png)  <br>\nLSTM is able to learn long-term dependencies. The difference between LSTM with normal RNN is that it has the cell state in addition of the hidden state which carries relevant information from earlier steps to later steps. The new information are going to be removed from this cell state via input and forget gates. The picture above shown a whoole process of LSTM and the steps are going to be explained below. \n<br> <br>\nStep 1: Forget information on forget gate <br> \n![LSTM3-focus-f.png](attachment:69b732a0-9e79-4889-bc56-84670dbcbf32.png)  <br>\nThe first process happened on LSTM is a forget gate. At this part LSTM choose what information that it going to remove or throw from the cell state. The function that represented by that lowercase sigma symbol (usually called sigmoid function) are going to look at the input from $h_{t-1}$ and $x_{t}$ and outputs a number between 0 and 1. 0 represents forget all of this information and 1 represents keep all the information.\n<br> <br>\nStep 2: Selecting information for cell state <br> \n![LSTM3-focus-i.png](attachment:dc2cb7cc-cbe0-4b71-9190-ce5a7c152bde.png) <br>\nThe second step will be selecting relevant information for our cell state. There are two process here, a sigmoid ($\\sigma$) layer (or input gate layer) that stores our updated value $i_{t}$ and tanh layer that makes a new value of $\\tilde{C_{t}}$. These two will be merged later to update our cell state. \n<br> <br>\nStep 3: Cell state processing <br> \n![LSTM3-focus-C.png](attachment:ce6587d1-73ae-4246-92a1-5134c213516b.png) <br>\nThe third step will be updating our old cell state. On this step, LSTM is going to multiply the old cell state ($\\tilde{C_{t-1}}$) with $f_{t}$ or the things that should be forgotten or not. Then it add the result with $i_{t}*\\tilde{C_{t}}$. This will be the new cell state that already forgot the information that doesn't really needed. This cell state will be continued to our next LSTM process.\n<br> <br>\nStep 4: Output gate <br> \n![LSTM3-focus-o.png](attachment:c29f6470-621b-4922-bab3-4957362b1a13.png) <br>\nThe last step is selecting values for our output. This output will be from our cell state that calculated with tanh function and then multiplied by the result of our input that has been processed by sigmoid function. Then this value will be our output and continued again as the input for our next LSTM process. \n\nYou can read the full explanation by Chris Olah [here](https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/). Or if you are into the mathematics inside you can read from the pytorch documentation [here](https:\/\/pytorch.org\/docs\/1.9.1\/generated\/torch.nn.LSTM.html).","ca4df52d":"## Formatting the results\n\nOur validation and evaluation mini-batch method makes our result in batches so we need to reshape the multi-dimensional tensors to one-dimensional vector or flatten them. But before that, because we use scaling to standardized the inputs, we have to revert the result to normal condition. As the shape back to normal dataframe, we can calculate error metrics and plot them.","45387dc8":"### Visualizing the data\nTo help use interpret our data we will visualize it as a line chart. We will also make a trendline that helps us know the increase of the sea level on our data period. But we will have to make a new dataframe without our missing values to get the trendline.","02339f67":"### Check Missing Value\nMissing values has important effect to our data, they will ruin the pattern especially if the gap between two values that missing is large. So before we continue processing our data we will look into our missing values.","8247083a":"## Loading the data into DataLoaders\nRNN architecture makes us need to splits our data as mini-batch and the DataLoader class by Pytorch does that for us. But before we use the DataLoader, we have to make our data as Dataset (not technically a dataset but a name of object on Pytorch) which is a dataset object to load data from. Since we make our data shape as tensors we use TensorDataset (dataset class wrapping tensors) to make that Pytorch Dataset. Also because sklearn scalers have Numpy arrays as output, we need to convert the data into Torch tensors before we load them to TensorDatasets.","88bb77dc":"## Calculating error metrics\nBefore visualizing the prediction of our test data, we are going to inspect our prediction error. The metrics that we are going to use are MAE, RMSE, and MAPE.","8b67becb":"## Set Configurations of Parameters\nThe configuration belows are parameter values that we are going to use on our model. Our neuron showed by hidden_dim parameters. Dropout is a regularization method where input and recurrent connections to LSTM units are probabilistically excluded from activation and weight updates while training a network, which reduce overfitting and improving model performance.","4840e29b":"# Sea Level Rise Prediction with Pytorch\n\nClimate change and its effect to sea condition is a hot topic on environmental condition in the world. While the sea level increase is not visible by our eyes, the impact can be felt by people who live near the sea. The impact of sea level rise get multiplicated especially if coupled with the occurence of land subsidence. While the land subsidence happens because of groundwater consumption, sea level rise happens because of temperature increase on our atmosphere. \n\nRecently the study of deep learning has become popular for time series forecasting especially Recurrent Neural Network (RNN) based model. With RNN based model like LSTM, we can predict sea level rise for some times in the future that cannot be predicted by conventional method like tidal harmonic analysis. The tidal harmonic analysis predict sea level based on tidal components and sea level rise happens because of climate change which is a non-tidal components. \n\nIn this project I will work with a sea level dataset from a certain point in Charleston, South Carolina, USA and predicting its raise on a certain time with RNN method, LSTM. I pick the data from Charleston because the data has almost a century in term of length and it happen that the location also encounter land subsidence condition. You can read the condition [here](https:\/\/weather.com\/news\/climate\/news\/2020-09-23-charleston-flooding-sinking-sea-level-rise-climate-change).","c33f09d3":"# Import libraries and Dataset","41e7d64b":"## Generating date\/time predictors\n\nSo the dataset is ready, the next step is to generate feature columns to transform our univariate dataset into a multivariate dataset. We are going to make new features using our datetime features.","53d4c19b":"### Seasonal Decomposition","136855a8":"As we can see on the picture above, our data has a seasonal composition which forms a sinusoidal wave pattern. The nature of tidal phenomenon (sea level) is happened because of gravitational force of moon, sun, and other sky objects that have huge mass. The gravitational effect of those objects makes a sinusoidal pattern like one on seasonal composition above.This seasonal pattern usually have another pattern that moves up and down in a period of 18.6 years. In theory, this condition happened because of astronomical cycle condition of moon and sun position that reached their nearest position to earth. You can read more about the topic [here](https:\/\/noc.ac.uk\/news\/highest-tides-186-years). \n\nThe trend pattern should have been following this 18.6 years cycle patter but on the trend showed above, it keep increasing. This trend happened because of the increase of our atmosphere temperature or people said as climate change.","d516488f":"## Visualizing the predictions\nThen we compare our prediction with our observation values. There will be three plot that showing the comparison of our test data only, the plot that showing all of our data, including validation and test data prediction, and the different in trendline on our test data plot.","07349c6e":"## Helper Class for training\n\nTraining data using neural networks is a repetitive process, looping between forward and back-propagation. To help us with that we will make a helper class named Optimization which help us combine the looping steps. This helper class contain one training step and then training loop that will be called each epoch. The weight of network will be updated on each training step. This process is important to minimize the loss function. Each epoch has two stages that use different dataset, training and validation dataset. The loss in train() function get updated on each epoch, while the loss in eval() function only there to compare the performance of our training model with validation one on the plot loss function.","96c20b40":"The graph above shows the sea level in Charleston from 1921 to 2014 and we can see that there is an increase in our trendline from start to the end. If we take the value on our end of trendline then deduct it with the value on the start of trendline, we get the value of 300 mm increase in almost a decade of sea level data. The first graph shows all of our data but we cannot see the pattern of our yearly data, so we will subset our latest 20 years of data and from that we can see that our data has a seasonal yearly pattern that forms a sine graph. This 20 years of sea level plot is shown below.","e94098da":"The visualization above helps us locate the missing values on our data, then we can inspect each location with indexing or subsetting to see how much of our data that missing. As I inspect the data, the missing values happened on 1921, 1952, 1966, 1972, and 1990 until 1992. "}}