{"cell_type":{"0f8fb941":"code","d1ec3f38":"code","bc23cef0":"code","6422cd38":"code","7afd9506":"code","802bcef0":"code","b41ca8f6":"code","05342dd0":"code","b0ad8482":"code","186425da":"code","b15f70bf":"code","eb2e7db5":"code","f75e4819":"code","69c58ef9":"code","a0a05e41":"code","c37838b0":"code","448e1e78":"code","dc7e67c0":"code","ae9cf968":"code","ad09375f":"code","639e1b0f":"code","06825586":"code","8d4ddadf":"code","83e76fea":"code","86ea4946":"code","6d891262":"code","9a7122ca":"code","de24aa72":"code","99383eca":"code","dca4d066":"markdown","c3212401":"markdown","51e175fa":"markdown","9acc6a6f":"markdown"},"source":{"0f8fb941":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d1ec3f38":"%config Completer.use_jedi=False","bc23cef0":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats \nimport matplotlib.pyplot as plt","6422cd38":"data = pd.read_csv(\"..\/input\/loan-data\/loan_data.csv\")\ndata.head()","7afd9506":"plt.figure(figsize=(12, 8))\nsns.heatmap(data.corr())","802bcef0":"data.corr()['not.fully.paid']","b41ca8f6":"df = pd.get_dummies(data, columns=['purpose'], drop_first=True)\ndf.head()","05342dd0":"from pandas_profiling import ProfileReport","b0ad8482":"ProfileReport(df)","186425da":"df.corr()","b15f70bf":"df.corr()['not.fully.paid']","eb2e7db5":"#Dropping 2 cols having more zeroes\n\ndf.drop(['inq.last.6mths','pub.rec'],axis=1)","f75e4819":"#Dropping 2 cols to avoid multi collinearity\n\ndf.drop(['fico','credit.policy'],axis=1)","69c58ef9":"x=df.drop(['not.fully.paid'],axis=1)\ny=df['not.fully.paid']","a0a05e41":"x.head()","c37838b0":"y.head()","448e1e78":"#split the data\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y)","dc7e67c0":"#standardise the data\nfrom sklearn.preprocessing import StandardScaler","ae9cf968":"stdSc=StandardScaler()\nstdSc.fit(x_train)","ad09375f":"x_train_std=stdSc.transform(x_train)\nx_test_std=stdSc.transform(x_test)","639e1b0f":"# Build Network\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation","06825586":"model=Sequential()\n#hidden layer\nmodel.add(Dense(20,activation='relu'))\nmodel.add(Dense(20,activation='relu'))\n#output layer\nmodel.add(Dense(1,activation='sigmoid')) # for binary classification we use SIGMOID only as activation at output\n\nmodel.compile(loss=\"binary_crossentropy\",optimizer='adam',metrics=['accuracy'])","8d4ddadf":"pip install tensorflow","83e76fea":"pip install livelossplot","86ea4946":"from livelossplot.inputs.tf_keras import PlotLossesCallback","6d891262":"model.fit(x_train_std,y_train,epochs=20,callbacks=[PlotLossesCallback()],validation_data=(x_test_std,y_test))","9a7122ca":"model.summary()","de24aa72":"print(x_train.shape)\nprint(y_train.shape)","99383eca":"print(x_test.shape)\nprint(y_test.shape)","dca4d066":"#Our Target is \"not.fully.paid\", From the below EDA from Pandas profiling, we could see that there is problem of multi collinearity. \n#From the above correlation matrix, we can also understand that target is more correlated positively with \n#'int.rate-0.16' and 'inq.last.6mths - 0.15'.\n\n#But they are inturn highly correlated with other columns \n#-> int.rate & credit.policy\n#-> inq.last.6mths & fico","c3212401":"# Problem Statement:  \n\nCreate a model that predicts whether or not a loan will be default using the historical data.\n\nFor companies like Lending Club correctly predicting whether or not a loan will be a default is very important. In this project, using the historical data from 2007 to 2015, you have to build a deep learning model to predict the chance of default for future loans. ","51e175fa":"#**Insights:**\n#1. Our Target is \"not.fully.paid\", From the below EDA from Pandas profiling, we could see that there is problem of multi collinearity. \n#From the above correlation matrix, we can also understand that target is more correlated positively with \n#'int.rate-0.16' and 'inq.last.6mths - 0.15'.\n\n#But they are inturn highly correlated with other columns \n#-> int.rate & fico \n#-> inq.last.6mths & credit policy\n\n#2. The fields inq.last.6mths has 88.3 % zeroes and pub.rec has 94.2% zeroes. SO, they can be dropped off from our modelling","9acc6a6f":"# So here we can see the accuracy as almost 84%"}}