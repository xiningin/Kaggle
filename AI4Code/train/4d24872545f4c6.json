{"cell_type":{"1e7d8217":"code","148ba013":"code","8be63f56":"code","baa8e68e":"code","0463cf1c":"code","7f662873":"code","7e6487b2":"code","74ee5a4c":"code","19feee58":"code","0b33a92b":"code","94b1a1bf":"code","5c654ceb":"code","083f4061":"code","58bcb710":"code","3f1b0680":"code","9b41c54b":"code","4f892b26":"code","3a7993af":"code","620839c3":"code","3b9327b6":"code","4eaf4f63":"code","6584b2c9":"code","8415a48a":"code","f7b56f9d":"code","d9e4e06e":"code","6c70110d":"code","2ea45b45":"code","4476b404":"code","0c7eb16f":"code","90992d3b":"code","8bea3e66":"code","ebfed49e":"code","10266937":"code","311a9297":"code","60ca27ce":"code","f66b14c3":"code","47836528":"code","f161d7c7":"code","51147aec":"code","1556eaa4":"code","150eccca":"code","f460081e":"code","89d5c12d":"code","a692f4ce":"code","316565e8":"code","df1023b5":"code","f3831159":"code","174dc7df":"code","9eced2d3":"code","55dc2787":"markdown","398dabff":"markdown","a08359ad":"markdown","bb894b7f":"markdown","03c284a8":"markdown","7b53750a":"markdown","1e28206e":"markdown","46720b3e":"markdown","8a00348b":"markdown","c1b9ab52":"markdown","6e2e5374":"markdown","7f3f8dc7":"markdown","05615c3b":"markdown","3b1ed983":"markdown","bf71581b":"markdown","81d15ffe":"markdown","c78639e6":"markdown","e0386f99":"markdown","42bfba30":"markdown","3c97011d":"markdown","b8bc2bae":"markdown","b3b497da":"markdown","50fbcfa2":"markdown","7909c284":"markdown","64d39ac6":"markdown","5cc490f2":"markdown","9130abb2":"markdown","b05b498f":"markdown","fd8d707a":"markdown","88688393":"markdown","231c68b9":"markdown","33d434c2":"markdown","b1aea9c9":"markdown"},"source":{"1e7d8217":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","148ba013":"titanic = pd.read_csv('..\/input\/titanic\/train.csv')\ntitanic.head(5)","8be63f56":"titanic.info()","baa8e68e":"titanic.describe()","0463cf1c":"titanic.columns","7f662873":"Survival = titanic.groupby(['Survived'])['PassengerId'].count().reset_index()\ndisplay(Survival)\nsns.barplot(Survival['Survived'], Survival['PassengerId'])\nfor i in Survival['Survived']:\n    plt.text(i, Survival['PassengerId'][i] - 35,Survival['PassengerId'][i], ha='center')\nplt.xticks([0, 1], ['No', 'Yes'])\nplt.show()","7e6487b2":"plt.figure(figsize=(30,5))\nsns.countplot(titanic['Age'], hue=titanic['Survived'])\nplt.xticks(rotation=90)\nplt.show()","74ee5a4c":"list1 = ['Sex','Pclass','SibSp','Parch','Embarked']\n\nfig, ax = plt.subplots(1,len(list1), figsize=(5*len(list1),5))\nfor l in range(0,len(list1)):\n    sns.countplot(titanic[list1[l]], hue=titanic['Survived'], ax=ax[l])\n    ax[l].set_ylabel('')\n    ax[0].set_ylabel('Jumlah Penumpang')\n    #plt.text(l, Survival['PassengerId'] - 35, Survival['PassengerId'], ha='center')","19feee58":"list2 = ['Age','Fare']\nfig, axes = plt.subplots(1,1, figsize=(10,5))\nfor i in range(0, len(list2)):\n    plt.subplot(2, np.ceil(len(list2)\/2), i+1)\n    sns.boxplot(titanic[list2[i]], orient='h')\n    plt.tight_layout()","0b33a92b":"titanic.info()","94b1a1bf":"titanic['Sex_label'] = titanic['Sex'].astype('category').cat.codes\ntitanic[['Sex','Sex_label']].sample(3)","5c654ceb":"onehots = pd.get_dummies(titanic['Embarked'], prefix='Embarked')\ntitanic = titanic.join(onehots)","083f4061":"titanic.sample(3)","58bcb710":"print('Missing value =','\\n',titanic.isna().sum())\nprint('-'*50)\nprint('Duplicated data =',titanic.duplicated() .sum())","3f1b0680":"titanic = titanic.dropna(subset=['Age','Embarked'])\ntitanic.info()","9b41c54b":"# Split Feature Vector and Label\nX = titanic[['Pclass', \n            'Sex_label',\n            'Age',\n            'SibSp',\n            'Parch',\n            'Fare',\n            'Embarked_C',\n            'Embarked_Q',\n            'Embarked_S']] \n# X = df.drop(['Fraud_Decoded'],axis=1) # untuk menggunakan semua feature kecuali target\ny = titanic['Survived'] # target\n\n#Splitting the data into Train and Test\nfrom sklearn.model_selection import train_test_split \nX_train, X_test,y_train,y_test = train_test_split(X,\n                                                y,\n                                                test_size = 0.3,\n                                                random_state = 789)","4f892b26":"display(X_train.info())\nprint('-'*100)\ndisplay(y_train.count())","3a7993af":"X_test.info()\nprint('-'*100)\ny_test.count()","620839c3":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(random_state=789)\nlogreg.fit(X_train, y_train)","3b9327b6":"y_predicted = logreg.predict(X_test)\ny_predicted","4eaf4f63":"y_predicted_proba = logreg.predict_proba(X_test)\ny_predicted_proba[:10]","6584b2c9":"from sklearn.metrics import classification_report, confusion_matrix\nprint('\\nconfustion matrix') # generate the confusion matrix\nprint(confusion_matrix(y_test, y_predicted))\n\nfrom sklearn.metrics import accuracy_score\nprint('\\naccuracy')\nprint(accuracy_score(y_test, y_predicted))\n\nfrom sklearn.metrics import classification_report\nprint('\\nclassification report')\nprint(classification_report(y_test, y_predicted)) # generate the precision, recall, f-1 score, num","8415a48a":"import numpy as np\n\nTHRESHOLD = 0.58\npredsfinal = np.where(logreg.predict_proba(X_test)[:,1] > THRESHOLD, 1, 0)\nprint(classification_report(y_test, predsfinal)) # generate the precision, recall, f-1 score, num","f7b56f9d":"from sklearn import metrics\nfpr, tpr, thresholds = metrics.roc_curve(np.array(y_test), np.array(y_predicted_proba[:,1]), pos_label=1) # pos_label: The label of the positive class. ada 2, 0 dan 1\n\nmetrics.auc(fpr, tpr)\n\npd.DataFrame({'thresholds':thresholds,'tpr':tpr,'fpr':fpr})\n\nplt.subplots(figsize=(10, 6))\nplt.plot(fpr, tpr, 'o-', label=\"ROC curve\")\nplt.plot(np.linspace(0,1,10), np.linspace(0,1,10), label=\"diagonal\")\nfor x, y, txt in zip(fpr, tpr, thresholds):\n    plt.annotate(np.round(txt,2), (x, y-0.04))\nplt.legend(loc=\"upper left\")\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")","d9e4e06e":"from sklearn.metrics import roc_curve, auc\nfpr, tpr, thresholds = roc_curve(y_test, predsfinal, pos_label=1) # pos_label: Survived\nprint('Nilai Area Under ROC Curve:', auc(fpr, tpr))\nprint(\"train Accuracy : \",logreg.score(X_train,y_train))\nprint(\"test Accuracy : \",logreg.score(X_test,y_test))","6c70110d":"from sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier(n_neighbors = 30)\nneigh.fit(X_train,y_train)\n\n\ny_predicted = neigh.predict(X_test)\n\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint('\\nconfustion matrix') # generate the confusion matrix\nprint(confusion_matrix(y_test, y_predicted))\n\nfrom sklearn.metrics import accuracy_score\nprint('\\naccuracy')\nprint(accuracy_score(y_test, y_predicted))\n\nfrom sklearn.metrics import classification_report\nprint('\\nclassification report')\nprint(classification_report(y_test, y_predicted)) # generate the precision, recall, f-1 score, num","2ea45b45":"from sklearn.metrics import roc_curve, auc\nfpr, tpr, thresholds = roc_curve(y_test, y_predicted, pos_label=1) # pos_label: Survived\nprint('Nilai Area Under ROC Curve:', auc(fpr, tpr))\nprint(\"train Accuracy : \",neigh.score(X_train,y_train))\nprint(\"test Accuracy : \",neigh.score(X_test,y_test))","4476b404":"from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier()\ntree.fit(X_train,y_train)\n\n\ny_predicted = tree.predict(X_test)\n\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint('\\nconfustion matrix') # generate the confusion matrix\nprint(confusion_matrix(y_test, y_predicted))\nprint('-'*111)\n\nfrom sklearn.metrics import accuracy_score\nprint('\\naccuracy')\nprint(accuracy_score(y_test, y_predicted))\nprint('-'*111)\n\nfrom sklearn.metrics import classification_report\nprint('\\nclassification report')\nprint(classification_report(y_test, y_predicted)) # generate the precision, recall, f-1 score, num\nprint('-'*111)","0c7eb16f":"from sklearn.metrics import roc_curve, auc\nfpr, tpr, thresholds = roc_curve(y_test, y_predicted, pos_label=1) # pos_label: Survived\nprint('Nilai Area Under ROC Curve:', auc(fpr, tpr))\nprint(\"train Accuracy : \",tree.score(X_train,y_train))\nprint(\"test Accuracy : \",tree.score(X_test,y_test))","90992d3b":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators= 100, max_depth=10, \n                            max_features = 'sqrt',\n                            min_samples_split = 8,\n                            min_samples_leaf = 7,\n                            bootstrap = True,\n                            n_jobs = -1,\n                            random_state = 42)\n\n\nrf.fit(X_train, y_train)\n\ny_predicted = rf.predict(X_test)\n\n\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score\nprint('\\nconfustion matrix') # generate the confusion matrix\nprint(confusion_matrix(y_test, y_predicted))\n\nprint('\\naccuracy')\nprint(accuracy_score(y_test, y_predicted))\nprint('\\nprecision')\nprint(precision_score(y_test, y_predicted))\n\n\nprint('\\nclassification report')\nprint(classification_report(y_test, y_predicted)) # generate the precision, recall, f-1 score","8bea3e66":"from sklearn.metrics import roc_curve, auc\nfpr, tpr, thresholds = roc_curve(y_test, y_predicted, pos_label=1) # pos_label: Survived\nprint('Nilai Area Under ROC Curve:', auc(fpr, tpr))\nprint(\"train Accuracy : \",rf.score(X_train,y_train))\nprint(\"test Accuracy : \",rf.score(X_test,y_test))","ebfed49e":"# Split Feature Vector and Label\nX = titanic[['Pclass', \n            'Sex_label',\n            'Age',\n            'SibSp',\n            'Parch',\n            'Fare',\n            'Embarked_C',\n            'Embarked_Q',\n            'Embarked_S']] \n# X = df.drop(['Fraud_Decoded'],axis=1) # untuk menggunakan semua feature kecuali target\ny = titanic['Survived'] # target\n\n#Splitting the data into Train and Test\nfrom sklearn.model_selection import train_test_split \nX_train, X_test,y_train,y_test = train_test_split(X,\n                                                y,\n                                                test_size = 0.3,\n                                                random_state = 789)","10266937":"from sklearn.ensemble import AdaBoostClassifier\nclf = AdaBoostClassifier()\nclf.fit(X, y)\n\ny_predicted = clf.predict(X_test)\n\n\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score\nprint('\\nconfustion matrix') # generate the confusion matrix\nprint(confusion_matrix(y_test, y_predicted))\n\nprint('\\naccuracy')\nprint(accuracy_score(y_test, y_predicted))\nprint('\\nprecision')\nprint(precision_score(y_test, y_predicted))\n\n\nprint('\\nclassification report')\nprint(classification_report(y_test, y_predicted)) # generate the precision, recall, f-1 score","311a9297":"from sklearn.metrics import roc_curve, auc\nfpr, tpr, thresholds = roc_curve(y_test, y_predicted, pos_label=1) # pos_label: Survived\nprint('Nilai Area Under ROC Curve:', auc(fpr, tpr))\nprint(\"train Accuracy : \",clf.score(X_train,y_train))\nprint(\"test Accuracy : \",clf.score(X_test,y_test))","60ca27ce":"titanictest = pd.read_csv('..\/input\/titanic\/test.csv')\ntitanictest.info()","f66b14c3":"titanictest['Sex_label'] = titanictest['Sex'].astype('category').cat.codes\nonehots = pd.get_dummies(titanictest['Embarked'], prefix='Embarked')\ntitanictest = titanictest.join(onehots)\n#titanictest = titanictest.dropna(subset=['Age','Embarked'])\ntitanictest = titanictest[['Pclass', \n            'Sex_label',\n            'Age',\n            'SibSp',\n            'Parch',\n            'Fare',\n            'Embarked_C',\n            'Embarked_Q',\n            'Embarked_S']]                          \ntitanictest.info()","47836528":"titanictest.isna() .sum()","f161d7c7":"titanictest['Age'].describe()","51147aec":"titanictest['Age'] = titanictest['Age'].fillna(30)\ntitanictest.isna() .sum()","1556eaa4":"titanictest['Fare'].describe()","150eccca":"titanictest['Fare'] = titanictest['Fare'].fillna(35)\ntitanictest.isna() .sum()","f460081e":"titanictest = titanictest[['Pclass', \n            'Sex_label',\n            'Age',\n            'SibSp',\n            'Parch',\n            'Fare',\n            'Embarked_C',\n            'Embarked_Q',\n            'Embarked_S']]                          \ntitanictest.info()","89d5c12d":"# Split Feature Vector and Label\nX = titanic[['Pclass', \n            'Sex_label',\n            'Age',\n            'SibSp',\n            'Parch',\n            'Fare',\n            'Embarked_C',\n            'Embarked_Q',\n            'Embarked_S']] \ny = titanic['Survived'] # target\n\n#Splitting the data into Train and Test\nfrom sklearn.model_selection import train_test_split \nX_train, X_test,y_train,y_test = train_test_split(X,\n                                                y,\n                                                test_size = 0.3,\n                                                random_state = 789)","a692f4ce":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(random_state=789)\nlogreg.fit(X_train, y_train)\n\ny_predicted = logreg.predict(titanictest)\n\nTHRESHOLD = 0.58\ny_predicted = np.where(logreg.predict_proba(titanictest)[:,1] > THRESHOLD, 1, 0)","316565e8":"pd.DataFrame(y_predicted).rename(columns = {0 : 'Survived'})","df1023b5":"dfkagglesubmission = pd.read_csv('..\/input\/titanic\/test.csv')\ndfid = dfkagglesubmission[['PassengerId']]\n\ndfkagglesubmission = pd.concat([dfid, pd.DataFrame(y_predicted).rename(columns = {0 : 'Survived'})], axis=1)\n\ndfkagglesubmission","f3831159":"dfkagglesubmission['Survived'].value_counts()","174dc7df":"dfkagglesubmission","9eced2d3":"dfkagglesubmission.to_csv('Titanic_submission.csv',index=False)","55dc2787":"Feature Ready to use:\n\nPclass OK\nSex_label OK\nAge OK\nSibSp OK\nParch OK\nFare OK \nEmbarked OK","398dabff":"## Missing and Duplicated Data","a08359ad":"tree = DecisionTreeClassifier()\ntree.fit(X_train,y_train)\n\ny_predicted = tree.predict(titanictest)","bb894b7f":"## Random Forest","03c284a8":"## Label Encoding\n* Sex","7b53750a":"## One Hot Encode\n* Embarkation","1e28206e":"## AdaBoost","46720b3e":"# MODELLING","8a00348b":"## Decision Tree","c1b9ab52":"neigh = KNeighborsClassifier(n_neighbors = 30)\nneigh.fit(X_train,y_train)\n\ny_predicted = neigh.predict(titanictest)","6e2e5374":"Feature to use:\nPclass OK\nSex -> label encod\nAge (NaN -> drop dulu biar gampang) besok coba imputasi diklasifikasi based on Mr\/Master\/Miss\/etc. average age\nSibSp OK\nParch OK\nFare OK -> outliers biarin dulu dah\nEmbarked -> one hot encode, dropna dulu 2 values.\n\nOutliers?? Fare\nStandarisasi Normalisasi??? all\nClass Imbalance?? Survival","7f3f8dc7":"# EDA","05615c3b":"### Tune Hyperparameter with Random Search","3b1ed983":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators= 100, max_depth=10, \n                            max_features = 'sqrt',\n                            min_samples_split = 8,\n                            min_samples_leaf = 7,\n                            bootstrap = True,\n                            n_jobs = -1,\n                            random_state = 42)\n\n\nrf.fit(X_train, y_train)\n\ny_predicted = rf.predict(titanictest)\ny_predicted","bf71581b":"## Logistic Regression","81d15ffe":"### Tune Hyperparameter with RandomSearch","c78639e6":"## AdaBoost","e0386f99":"### Evaluation","42bfba30":"## kNN","3c97011d":"## RF","b8bc2bae":"### Drop the missing Age and Embarked","b3b497da":"# Pre-Processing","50fbcfa2":"## Split Train Test Data","7909c284":"## kNN","64d39ac6":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform\nimport numpy as np\n\n#List Hyperparameters yang akan diuji\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 20)] # Number of trees in random forest\nmax_features = ['auto', 'sqrt', 'log2'] # Number of features to consider at every split\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)] # Maximum number of levels in tree\nmin_samples_split = [int(x) for x in np.linspace(start = 2, stop = 10, num = 5)] # Minimum number of samples required to split a node\nmin_samples_leaf = [int(x) for x in np.linspace(start = 1, stop = 10, num = 5)] # Minimum number of samples required at each leaf node\nbootstrap = [True, False] # Method of selecting samples for training each tree\nn_jobs = [-1]\n#Menjadikan ke dalam bentuk dictionary\nhyperparameters = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap,\n               'n_jobs':n_jobs}\n\n# Init random forest dengan randomsearch, cross validation = 5\nrf = RandomForestClassifier(random_state=42)\nclf = RandomizedSearchCV(rf, hyperparameters, cv=5, random_state=42, scoring='precision')\n\n#Fitting Model\nbest_model = clf.fit(X_train,y_train)\n\n#Nilai hyperparameters terbaik\nprint('Best n_estimators:', best_model.best_estimator_.get_params()['n_estimators'])\nprint('Best max_depth:', best_model.best_estimator_.get_params()['max_depth'])\nprint('Best max_features:', best_model.best_estimator_.get_params()['max_features'])\nprint('Best min_samples_split:', best_model.best_estimator_.get_params()['min_samples_split'])\nprint('Best min_samples_leaf:', best_model.best_estimator_.get_params()['min_samples_leaf'])\nprint('Best bootstrap:', best_model.best_estimator_.get_params()['bootstrap'])\nprint('Best n_jobs:', best_model.best_estimator_.get_params()['n_jobs'])\n\n\n#Prediksi menggunakan model baru\ny_predicted = best_model.predict(X_test)#Check performa dari model\nprint(classification_report(y_test, y_predicted))\n# roc_auc_score(y_test, y_pred)","5cc490f2":"## Logistic","9130abb2":"## etc etc","b05b498f":"# Predicting The TEST Dataset","fd8d707a":"## Survival Rate","88688393":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform\n\n#List Hyperparameters yang akan diuji\npenalty = ['l2']\nC = [0.0001, 0.001, 0.002] # Inverse of regularization strength; smaller values specify stronger regularization.\n\n#Menjadikan ke dalam bentuk dictionary\nhyperparameters = dict(penalty=penalty, C=C)\n\n# Init Logres dengan Gridsearch, cross validation = 5\nlogreg = LogisticRegression(random_state=789)\nclf = RandomizedSearchCV(logreg, hyperparameters, cv=5, random_state=789)\n\n#Fitting Model\nbest_model = clf.fit(X_train, y_train)\n\n#Nilai hyperparameters terbaik\nprint('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\nprint('Best C:', best_model.best_estimator_.get_params()['C'])\n\n#Prediksi menggunakan model baru\ny_pred = best_model.predict(X_test)#Check performa dari model\nprint(classification_report(y_test, y_pred))\n# roc_auc_score(y_test, y_pred)","231c68b9":"## Decision Tree","33d434c2":"clf = AdaBoostClassifier()\nclf.fit(X, y)\n\ny_predicted = clf.predict(titanictest)","b1aea9c9":"### Evaluation"}}