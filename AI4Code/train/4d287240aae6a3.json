{"cell_type":{"e0ec09db":"code","f22554d0":"code","286bea0c":"code","ac67110e":"code","d071c846":"code","270e5cca":"code","ef0c2404":"code","9506bfff":"code","c3c50093":"code","3cc91331":"code","3cc1c558":"code","ab2cbaff":"code","5cf8d894":"code","49168455":"code","e743cc42":"code","7f643066":"code","11b21049":"code","54c13df5":"code","d8ef7b9d":"code","3bef2f56":"code","dc9938f7":"code","5c156361":"code","78ea7644":"code","b038eda8":"code","6a1952c0":"code","f556f1c0":"code","f674ba33":"code","16a04f51":"code","efbd0fbe":"code","62e44dcf":"code","39d81577":"code","4866bb98":"code","7f310d67":"code","c22f7c2a":"code","c02ccc9f":"code","3221dcb2":"code","a6863a5f":"code","1def39dd":"code","d5f86294":"code","edc00c42":"code","3b73250d":"markdown","d3527917":"markdown","020b5c2b":"markdown","e54ee082":"markdown","918a6038":"markdown","51790408":"markdown","e3a16d88":"markdown","8d4c2e32":"markdown","864e8c9b":"markdown","a2efc193":"markdown","3c01f60a":"markdown","df1e3277":"markdown","3ea8376a":"markdown","931c38f7":"markdown","17b6ef9b":"markdown","6e44393c":"markdown","0f042138":"markdown","61de50f7":"markdown","67f7512e":"markdown","4a7cac03":"markdown","21327ae2":"markdown","f57e80e7":"markdown","125734e8":"markdown","c4eb3ef9":"markdown","a03cd459":"markdown","14827c37":"markdown","da8586a2":"markdown"},"source":{"e0ec09db":"! wget https:\/\/repo.anaconda.com\/miniconda\/Miniconda3-py37_4.8.2-Linux-x86_64.sh\n! chmod +x Miniconda3-py37_4.8.2-Linux-x86_64.sh\n! bash .\/Miniconda3-py37_4.8.2-Linux-x86_64.sh -b -f -p \/usr\/local\n! conda install -c rdkit rdkit -y\nimport sys \nsys.path.append('\/usr\/local\/lib\/python3.7\/site-packages\/')\nimport warnings\nwarnings.filterwarnings('ignore')","f22554d0":"! wget https:\/\/pubs.acs.org\/doi\/suppl\/10.1021\/ci034243x\/suppl_file\/ci034243xsi20040112_053635.txt\n! wget https:\/\/raw.githubusercontent.com\/dataprofessor\/data\/master\/delaney.csv","286bea0c":"import pandas as pd\nimport numpy as np\ndata=pd.read_csv('delaney.csv')\ndata.head()","ac67110e":"from rdkit import Chem\nmol_lst=[]\nfor i in data.SMILES:\n    mol=Chem.MolFromSmiles(i)\n    mol_lst.append(mol)\nlen(mol_lst)","d071c846":"from rdkit.Chem import Descriptors\nfrom rdkit.ML.Descriptors import MoleculeDescriptors","270e5cca":"desc_lst=[i[0] for i in Descriptors._descList]\ndescriptor=MoleculeDescriptors.MolecularDescriptorCalculator(desc_lst)\ndescrs = []\nfor i in range(len(mol_lst)):\n    descrs.append(descriptor.CalcDescriptors(mol_lst[i]))","ef0c2404":"molDes=pd.DataFrame(descrs,columns=desc_lst)\nmolDes.head(20)","9506bfff":"print(molDes.shape)\nmolDes.columns","c3c50093":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\nplt.style.use(\"fivethirtyeight\")\nmolDes.describe().style.background_gradient(cmap = \"Reds\")","3cc91331":"corr_mat = molDes.corr()\ncorr_mat","3cc1c558":"corr_feature = []\nfor i in range(len(corr_mat.columns)):\n    for j in range(i):\n        if abs(corr_mat.iloc[i,j] > 0.9): #abs is absolute\n            corr_feature.append(corr_mat.columns[i])\ncorr_feature = list(set(corr_feature))\nprint(corr_feature)\nlen(corr_feature)","ab2cbaff":"molDes.drop(columns = corr_feature,axis = 1, inplace = True)\nprint(molDes.shape)\nmolDes.head(10)","5cf8d894":"from sklearn.model_selection import train_test_split\nfrom sklearn.tree import ExtraTreeRegressor\nfrom sklearn.metrics import roc_auc_score, mean_squared_error","49168455":"x=molDes\ny=data['measured log(solubility:mol\/L)'] #Target\nmodel = ExtraTreeRegressor()\nmodel.fit(x,y)","e743cc42":"feat_importances = pd.Series(model.feature_importances_, index = x.columns)\nfeat_importances=feat_importances.sort_values(ascending=False)\nImp_features=feat_importances.head(20)\nprint(type(Imp_features))\nImp_features","7f643066":"molDes_feat = pd.DataFrame(Imp_features)\nmolDes_feat.index","11b21049":"mol_feat = molDes[['MolLogP', 'Kappa2', 'PEOE_VSA6', 'MinPartialCharge',\n       'NumAromaticRings', 'fr_bicyclic', 'BCUT2D_MRLOW', 'MolWt',\n       'SlogP_VSA4', 'SMR_VSA10', 'FractionCSP3', 'EState_VSA5', 'fr_Al_OH',\n       'EState_VSA8', 'BertzCT', 'Ipc', 'FpDensityMorgan1', 'SMR_VSA7',\n       'BCUT2D_LOGPLOW', 'SlogP_VSA2']]","54c13df5":"#Plotting the distribution of the features.\nmol_feat.hist(bins=50,figsize=(16, 20));","d8ef7b9d":"plt.figure(figsize=(16,60))\ni=1\ntarget=data['measured log(solubility:mol\/L)']\nfor feature in mol_feat:\n    plt.subplot(15,3,i)\n    sns.lineplot(x=mol_feat[feature],y=target)\n    plt.xlabel(feature)\n    plt.ylabel('mol\/L')\n    i+=1","3bef2f56":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.model_selection import train_test_split\nlin_reg=LinearRegression()\nX=mol_feat\nY=data['measured log(solubility:mol\/L)']\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)","dc9938f7":"linear=lin_reg.fit(X_train,Y_train)\nprint(\"Training score: {} \\nValidation score: {} \".format(round(linear.score(X_train, Y_train),3),round(linear.score(X_test,Y_test),3)))","5c156361":"from sklearn.model_selection import GridSearchCV\nGridSearchCV.get_params(LinearRegression)","78ea7644":"lin_params={'fit_intercept':[True],'normalize':[True]}\nlin=GridSearchCV(LinearRegression(),param_grid=lin_params).fit(X_train, Y_train).best_estimator_\nlinear_regression=lin.fit(X_train,Y_train)\nprint(\"Training score: {} \\nValidation score: {} \".format(round(linear_regression.score(X_train, Y_train),3),round(linear_regression.score(X_test,Y_test),3)))","b038eda8":"Y_lin_pred_train = linear_regression.predict(X_train)\nY_lin_pred_test=linear_regression.predict(X_test)\nprint('Mean squared error (MSE): %.2f'\n      % mean_squared_error(Y_test, Y_lin_pred_test))\nprint('Coefficient of determination (R^2): %.2f'\n      % r2_score(Y_test, Y_lin_pred_test))","6a1952c0":"#Converting the target variable into binary form\nlst = []\nfor i in data['measured log(solubility:mol\/L)']:\n    #print(i)\n    if i>0:\n        lst.append(1)\n    else:\n        lst.append(0)\ndata['sol_class']=lst","f556f1c0":"#Using logistic Regression\nfrom sklearn import linear_model\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nlog_reg=linear_model.LogisticRegression()\nX=mol_feat\nY=data['sol_class']\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\nlog_reg=log_reg.fit(X_train,Y_train)\nprint(\"Training score: {} \\nValidation score: {} \".format(round(log_reg.score(X_train, Y_train),3),round(log_reg.score(X_test,Y_test),3)))","f674ba33":"y_log_pred=log_reg.predict(X_test)\nprint(classification_report(Y_test,y_log_pred))","16a04f51":"from sklearn.metrics import confusion_matrix\ncm=confusion_matrix(Y_test,y_log_pred)\nprint(cm)","efbd0fbe":"from sklearn.linear_model import RidgeCV, Ridge\nX=mol_feat\nY=data['measured log(solubility:mol\/L)']\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n","62e44dcf":"r = RidgeCV(alphas=[0.01,1.0,5.0,10.0],normalize=True,cv=5)\nridge=r.fit(X_train,Y_train)\nridge.score(X_train,Y_train)","39d81577":"from sklearn.model_selection import GridSearchCV\nridge_params = {'alpha':[0.01,0.02],'normalize':[True,False],'solver':['auto']}\nr_cv=GridSearchCV(Ridge(),param_grid=ridge_params,cv=5).fit(X_train, Y_train).best_estimator_\nr_cv.fit(X_train,Y_train)\n","4866bb98":"print(\"Training score: {} \\nValidation score: {} \".format(round(r_cv.score(X_train, Y_train),3),round(r_cv.score(X_test,Y_test),3)))","7f310d67":"y_pred_ridge=r_cv.predict(X_test)\nprint(\"The r2_score and the mse are {} and {} respectively\".format(round(r2_score(Y_test, y_pred_ridge),3),round(mean_squared_error(Y_test,y_pred_ridge),3)))","c22f7c2a":"from sklearn.linear_model import Lasso\nX=mol_feat\nY=data['measured log(solubility:mol\/L)']\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\nlasso_params={'alpha':[0,0.005,0.01],'positive':[True,False],'fit_intercept':[True]}\nlasso=GridSearchCV(Lasso(),param_grid=lasso_params).fit(X_train, Y_train).best_estimator_\nlasso.fit(X_train,Y_train)\n","c02ccc9f":"print(\"Training score: {} \\nValidation score: {} \".format(round(lasso.score(X_train, Y_train),3),round(lasso.score(X_test,Y_test),3)))","3221dcb2":"y_pred_lasso=lasso.predict(X_test)\nprint(\"The r2_score and the mse are {} and {} respectively\".format(round(r2_score(Y_test, y_pred_lasso),3),\n                                                                   round(mean_squared_error(Y_test,y_pred_lasso),3)))","a6863a5f":"from sklearn.ensemble import RandomForestRegressor\nX=mol_feat\nY=data['measured log(solubility:mol\/L)']\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n","1def39dd":"X=mol_feat\nY=data['measured log(solubility:mol\/L)']\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)\nrf_params={'n_estimators':[105,110,115,120],'max_depth':[30,35,40],'max_features':['auto', 'sqrt', 'log2'],'oob_score':[True]}\nrf=GridSearchCV(RandomForestRegressor(),param_grid=rf_params).fit(X_train, Y_train).best_estimator_\nrf=rf.fit(X_train,Y_train)\n","d5f86294":"print(\"Training score: {} \\nValidation score: {} \".format(round(rf.score(X_train, Y_train),3),round(rf.score(X_test,Y_test),3)))","edc00c42":"y_pred_rf=rf.predict(X_test)\nprint(\"The r2_score and the mse are {} and {} respectively\".format(round(r2_score(Y_test, y_pred_rf),3),round(mean_squared_error(Y_test,y_pred_rf),3)))","3b73250d":"**From this we can see that the MolLogP is the best feature**","d3527917":"***Logistic Regression Model***","020b5c2b":"**Downloading the dataset**","e54ee082":"*rdkit is an opensource cheminformatics package*","918a6038":"**Kindly upvote if you find this notebook helpful**","51790408":"# Lasso Regression","e3a16d88":"**This is the best fit for the model**","8d4c2e32":"**Since the score is too good, no need for parameter tuning**","864e8c9b":"**Logistic Regression can be used only when the dependent (Target) variable is of binary form ( 0 or 1)**","a2efc193":"# Logistic Regression","3c01f60a":"**Parameter Tuning using GridSearchCV**","df1e3277":"# Ridge Regression","3ea8376a":"*To see the relationship between the target and features*","931c38f7":"**Model Scores**","17b6ef9b":"From the above regression models, logistic regression has the best model score, but it can tell only whether the molecule is\nsoluble or not (assuming that the molecules are soluble, if they have positive mol\/L and are insoluble if they are negative).\nBut for prediction of the solubility, the best model is given by the random forest regressor which had good model score and optimal r2 and mse scores.","6e44393c":"# Feature selection","0f042138":"# Linear Regression","61de50f7":"**Hyperparameter Tuning of the linear regression model**","67f7512e":"# Random Forest Regression","4a7cac03":"# Calculation of Descriptors","21327ae2":"**Using ExtraTreeRegressor to find the best features**","f57e80e7":"**Random Forest Model Tuning Using GridSearchCV**","125734e8":"**Dataset**","c4eb3ef9":"**Removing the highly correlated columns**","a03cd459":"**Plotting the features, with respect to the target value**","14827c37":"**Converting the SMILES object to mol object**","da8586a2":"# Installing rdkit"}}