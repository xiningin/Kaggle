{"cell_type":{"59e80cb7":"code","e3e28906":"code","cf980859":"code","8bd606a1":"code","73761fb1":"code","7fe6fa95":"code","9014c6ba":"code","616cbfdd":"code","64a09994":"code","b27475e0":"code","da9f393c":"code","87dfb40a":"code","9df004c7":"code","46ce7cd5":"code","2eaff046":"code","c7507f3a":"code","379cd68e":"code","8c3b01a4":"code","b70bfd85":"code","b0cfa90f":"code","9856e66f":"code","2cca1bb7":"code","d60b8b41":"code","7789d930":"code","031de820":"code","5b48cac3":"code","029e372b":"code","93e0f09b":"code","a5867237":"code","7dd664b0":"code","604af9ae":"code","56dd381e":"code","b51702ad":"code","99af4c6f":"code","d21d0912":"code","0d1c9ae3":"code","3b57ba72":"code","926b82b2":"code","e87a6e17":"code","37ec8dd2":"code","d6cba4a8":"code","f12be28b":"code","92d12a8d":"code","112b1515":"code","ba6a22c9":"code","b6598b79":"code","1de055a3":"code","a39eddfb":"code","cfa990d2":"code","41e988d5":"code","ff3792bd":"code","b0803577":"code","7a34ab64":"code","57389e30":"code","12a56e25":"code","fa0edd32":"code","b92f8589":"code","b6854b01":"code","c5e7bc78":"code","145be171":"code","8f54ab5f":"code","9a3919d0":"code","697954d1":"code","56513456":"code","3690012c":"code","5e20f512":"code","0726a8ae":"code","4e8dfb38":"code","320298b2":"code","0870006f":"code","fd2c1f0a":"code","3ebd63c7":"code","e9867b53":"code","a273c217":"code","a494500e":"code","182c6308":"code","aec670e7":"code","caf5d2f0":"code","d8714062":"code","3bd050af":"code","daaf605b":"code","a2ddec0b":"code","941a6e52":"code","1e7d0ff1":"code","ae830508":"code","f43b5dfd":"code","b3ca66c8":"code","d558f624":"code","2c7f6c28":"code","a953e6be":"code","ff7ad059":"code","944a5ab1":"code","a27fd3e5":"code","8bcc729e":"code","73fdc777":"code","2dbded31":"code","91cf181a":"code","7811fd55":"code","0fa15485":"code","9e0853f9":"code","5a5f9fe9":"markdown","8c6f3791":"markdown","bcde5ad8":"markdown","dd1f2112":"markdown","0f1854b0":"markdown","85a01b40":"markdown","556145e9":"markdown","d947a039":"markdown","f03fdbd3":"markdown","9f798f5d":"markdown","0cd2c75a":"markdown","b863227d":"markdown","8dfb0b4f":"markdown","938836c7":"markdown","d2b4e678":"markdown","4fb80761":"markdown","19f4ee66":"markdown","f02bcbb6":"markdown","7629427b":"markdown","eb776667":"markdown","281f6168":"markdown","a386efd6":"markdown","91dd86f5":"markdown","2212c372":"markdown","9074bcf2":"markdown","d65f46e0":"markdown","d2a8db34":"markdown","27c07331":"markdown","2d61b4b8":"markdown","e0425fad":"markdown","66fb9255":"markdown","67674905":"markdown","627d7fed":"markdown","e9e30f41":"markdown","3123141d":"markdown","3a32041b":"markdown","423a5ed4":"markdown","e95df5f6":"markdown","7c183ea0":"markdown","2b92c92c":"markdown","2fd3277f":"markdown","7472fdbf":"markdown","bee19e5c":"markdown","5e2f1b6b":"markdown","58ecc721":"markdown","6f4ae50b":"markdown","888c06a5":"markdown"},"source":{"59e80cb7":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\nimport missingno as msno\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.impute import KNNImputer\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom lightgbm import LGBMRegressor","e3e28906":"#Read the Data\ndf=pd.read_csv(\"\/kaggle\/input\/hitters-baseball-data\/Hitters.csv\")","cf980859":"data=df.copy()\ndf.head()","8bd606a1":"def data_understanding(df):\n    print('############shape##############')\n    print(df.shape)\n    print('############types##############')\n    print(df.dtypes)\n    print('############head###############')\n    print(df.head())\n    print('############info###############')\n    print(df.info())\n    print('############nunique###############')\n    print(df.nunique())","73761fb1":"# There are 322 observations and int-float-object types of features in this data set\ndata_understanding(df)","7fe6fa95":"print(\"Num of Object Variables:\", df.select_dtypes(object).shape[1])\nprint(\"Num of Integer Variables:\", df.select_dtypes(\"integer\").shape[1])\nprint(\"Num of Float Variables:\", df.select_dtypes(\"float\").shape[1])","9014c6ba":"df[\"League\"].value_counts()","616cbfdd":"df[\"League\"].value_counts().plot.barh()","64a09994":"df[\"NewLeague\"].value_counts()","b27475e0":"df[\"NewLeague\"].value_counts().plot.barh()","da9f393c":"df[\"Division\"].value_counts()","87dfb40a":"df[\"Division\"].value_counts().plot.barh()","9df004c7":"sns.distplot(df['Salary'])","46ce7cd5":"#If the missing values don't come from Salary(target feature), i would have thought to assign mean according to these results.\n# Because, there seems to be a relation between categoric variables and Salary values for example there is an important differences between being E Division and W Devision.\nprint(\"New League= A\" ,df[df[\"NewLeague\"]==\"A\"].agg({\"Salary\":\"mean\"}))\nprint(\"New League= N\" ,df[df[\"NewLeague\"]==\"N\"].agg({\"Salary\":\"mean\"}))\nprint(\"League= A\" ,df[df[\"League\"]==\"A\"].agg({\"Salary\":\"mean\"}))\nprint(\"League= N\" ,df[df[\"League\"]==\"N\"].agg({\"Salary\":\"mean\"}))\nprint(\"Division= E\" ,df[df[\"Division\"]==\"E\"].agg({\"Salary\":\"mean\"}))\nprint(\"Division= W\" ,df[df[\"Division\"]==\"W\"].agg({\"Salary\":\"mean\"}))","2eaff046":"#There are 59 null values in Hitters data set\ndf.isnull().sum().sum()","c7507f3a":"# All these NA values comes from \"Salary\" feature\ndf.isnull().sum()","379cd68e":"df[df.Salary.isnull()==True].head()","8c3b01a4":"msno.bar(df)","b70bfd85":"#Statistical view for all features\ndf.describe().T","b0cfa90f":"# Descriptive Analysis\ndf.describe([0.05,0.25,0.50,0.75,0.95,0.99]).T","9856e66f":"sns.boxplot(x = df[\"Salary\"])\nplt.show()","2cca1bb7":"def outlier_thresholds(dataframe, col_name, q1=0.05, q3=0.95):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit","d60b8b41":"#When the quarters of 1% and quartiles of 99% were examined first, no outlier was found.\nlower, upper=outlier_thresholds(df, 'Salary', q1=0.01, q3=0.99)\nprint(df[(df['Salary']<lower) | (df['Salary']>upper)].shape[0])","7789d930":"#Then, when the quarters of 25% and quarters of 75% were examined, an outlier was found.\n#Conclusion: Observation analysis against the dependent variable is applied according to quartiles of 25 and 75. \n#Business sector information may remain untouched.\nlower, upper=outlier_thresholds(df, 'Salary', q1=0.25, q3=0.75)\nprint(df[(df['Salary']<lower) | (df['Salary']>upper)].shape[0])","031de820":"#Later, when quarters of 5% and quarters of 95% were examined, no outlier was found.\nlower, upper=outlier_thresholds(df, 'Salary')\nprint(df[(df['Salary']<lower) | (df['Salary']>upper)].shape[0])","5b48cac3":"# numerical variables\ndef numeric_cols(df):\n    numeric_cols = [col for col in df.columns if df[col].dtypes != \"O\"]\n    return numeric_cols","029e372b":"#Here, how many outlier observations in all variables in quartiles of 25 and 75 are accessed.\nfor col in numeric_cols(df):\n    lower, upper=outlier_thresholds(df, col, 0.25, 0.75)\n    count=df[(df[col]<lower) | (df[col]>upper)].shape[0]\n    if count!=0:\n        print(col, 'yes')\n        print(count)\n    else:\n        print(col, 'no')","93e0f09b":"def replace_with_thresholds(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if low_limit > 0:\n        dataframe.loc[(dataframe[col_name] < low_limit), col_name] = low_limit\n        dataframe.loc[(dataframe[col_name] > up_limit), col_name] = up_limit\n    else:\n        dataframe.loc[(dataframe[col_name] > up_limit), col_name] = up_limit\n        \n    return dataframe","a5867237":"df=replace_with_thresholds(df, 'Salary')","7dd664b0":"sns.boxplot(df['Salary'])","604af9ae":"for i in numeric_cols(df):\n\n    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 4))\n    sns.histplot(df[i], bins=10, ax=axes[0])\n    axes[0].set_title(i)\n    \n    sns.boxplot(df[i], ax=axes[1])\n    axes[1].set_title(i)\n   \n    sns.kdeplot(df[i], ax=axes[2])\n    axes[2].set_title(i)\n    plt.show()","56dd381e":"# correlation analysis\ndf.corr()","b51702ad":"def correlation(df, size=[20, 15]):\n    f, ax = plt.subplots(figsize= [20,15])\n    sns.heatmap(df.corr(), annot=True, fmt=\".2f\", ax=ax, cmap = \"magma\" )\n    ax.set_title(\"Correlation Matrix\", fontsize=20)\n    plt.show()","99af4c6f":"correlation(df)","d21d0912":"# Correlation analysis of numerical variables was performed.\ndef find_corr(df, num_col_names, limit=0.55):\n    high_corrs={}\n    for col in num_col_names:\n        if col=='Salary':\n            pass\n        else:\n            corr=df[[col, 'Salary']].corr().loc[col, 'Salary']\n            print(col, corr)\n            if abs(corr)>limit:\n                high_corrs[col]=corr\n    return high_corrs","0d1c9ae3":"high_corrs = find_corr(df, numeric_cols(df))","3b57ba72":"#Two variables with high correlation.\nprint(high_corrs)","926b82b2":"sns.scatterplot(x= df['CRuns'], y=df.Salary)","e87a6e17":"sns.scatterplot(x= df['CRBI'], y=df.Salary)","37ec8dd2":"def lof_scores(df):\n    clf=LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n    clf.fit_predict(df)\n    df_scores=clf.negative_outlier_factor_\n    sns.boxplot(df_scores)\n    plt.show()\n    return df_scores\n    \ndef lof(df, df_scores, threshold):\n    not_outlier = df_scores >threshold\n    value= df[df_scores == threshold]\n    outliers = df[~not_outlier] \n    res=outliers.to_records(index=False)\n    res[:] = value.to_records(index = False)\n    not_outlier_df = df[not_outlier]\n    outliers = pd.DataFrame(res, index = df[~not_outlier].index)\n    df_res = pd.concat([not_outlier_df, outliers], ignore_index = True)\n    return df_res","d6cba4a8":"#New variables were created with the most appropriate variables according to their proportions.\n#The data set includes the data obtained by the players in 1986 and throughout their careers and how many years of experience they have. \n#We add the annual averages of these data and the ratio of the 1986 data to the overall performance.\ndef new_var(df):\n    df['AtBat_new'] = df['AtBat'] \/ df['CAtBat']\n    df['Hits_new'] = df['Hits'] \/ df['CHits']\n    df['HmRun_new'] = (df['HmRun'] \/ df['CHmRun']).fillna(0)\n    df['Runs_new'] = df['Runs'] \/ df['CRuns']\n    df['RBI_new'] = (df['RBI'] \/ df['CRBI']).fillna(0)\n    df['Walks_new'] = (df['Walks'] \/ df['CWalks']).fillna(0)\n\n    df[\"CAtBat_rate\"] = df[\"CAtBat\"] \/ df[\"Years\"]\n    df[\"CHits_rate\"] = df[\"CHits\"] \/ df[\"Years\"]\n    df[\"CHmRun_rate\"] = df[\"CHmRun\"] \/ df[\"Years\"]\n    df[\"Cruns_rate\"] = df[\"CRuns\"] \/ df[\"Years\"]\n    df[\"CRBI_rate\"] = df[\"CRBI\"] \/ df[\"Years\"]\n    df[\"CWalks_rate\"] = df[\"CWalks\"] \/ df[\"Years\"]\n    \n    return df","f12be28b":"def minmax_scaler(dataframe, col_names, feature_range=(0,1)):\n    minmax_scaler = MinMaxScaler(feature_range=feature_range)\n    col_names=[col for col in col_names if col !=\"Salary\"]\n    dataframe[col_names] = minmax_scaler.fit_transform(dataframe[col_names])\n    return dataframe","92d12a8d":"#drop NA values\ndf1=df.dropna()\ndf1.shape","112b1515":"df1=minmax_scaler(df1, numeric_cols(df1))","ba6a22c9":"df1.isnull().sum().sum()","b6598b79":"# Variables with 2 categories\ndef var_two_cat(df):    \n    bins_cols=[col for col in df.columns if df[col].dtype=='O' and df[col].nunique()==2]\n    return bins_cols","1de055a3":"print(var_two_cat(df1))","a39eddfb":"def label_encoder(df, bins_cols):\n    for col in bins_cols:\n        le=LabelEncoder()\n        df[col]=le.fit_transform(df[col])\n    return df","cfa990d2":"df1=label_encoder(df1, var_two_cat(df1))","41e988d5":"df1.name='df1'\ndf1.head()","ff3792bd":"#This is second option and method is fill NA values with mean\ndf2=df.copy()","b0803577":"df2['New_Year'] = pd.cut(x=df2['Years'], bins=[0, 3, 6, 10, 15, 19, 24], ).astype(\"O\")\n\ndf2['New_Year'].value_counts().plot.barh()","7a34ab64":"df2.isnull().sum().sum()","57389e30":"msno.bar(df2)","12a56e25":"df2['Salary']=df2['Salary'].fillna(df2.groupby(['New_Year', \"League\", 'Division'])['Salary'].transform('mean'))","fa0edd32":"df2.isnull().sum().sum()","b92f8589":"df2.head()","b6854b01":"df2.drop('New_Year', axis=1, inplace=True)","c5e7bc78":"df2=minmax_scaler(df2, numeric_cols(df2))","145be171":"df2=label_encoder(df2, var_two_cat(df2))","8f54ab5f":"df2.head()","9a3919d0":"df2.head()\ndf2.name='df2'","697954d1":"df3=df.copy()","56513456":"df3=minmax_scaler(df3, numeric_cols(df3))","3690012c":"df3=label_encoder(df3, var_two_cat(df3))","5e20f512":"df3.head()","0726a8ae":"# We fill in the missing observations with the KNN algorithm and create the dataset named 'df_knn_imp':\ndef knn_imputer(df, n):\n    imputer = KNNImputer(n_neighbors = n)\n    df_filled = imputer.fit_transform(df)\n    df_knn_imp = pd.DataFrame(df_filled,columns = df.columns)\n    return df_knn_imp","4e8dfb38":"df3=knn_imputer(df3, 4)","320298b2":"df3.isnull().sum().sum()","0870006f":"df3.name='df3'\ndf3.head()","fd2c1f0a":"#Filling Missing Data with KNN and Suppressing Outliers to create 'df4'\ndf4=df.copy()","3ebd63c7":"df4.head()","e9867b53":"df4=minmax_scaler(df4, numeric_cols(df4))","a273c217":"df4=label_encoder(df4, var_two_cat(df4))","a494500e":"df4.head()","182c6308":"df4=knn_imputer(df4, 4)","aec670e7":"array=np.sort(lof_scores(df4))\nprint(array)","caf5d2f0":"sns.boxplot(array[array>array[16]])","d8714062":"df_scores=lof_scores(df4)\ndf4=lof(df4, df_scores, np.sort(df_scores)[16])","3bd050af":"df4.isnull().sum().sum()","daaf605b":"df4.name='df4'\ndf4.head()","a2ddec0b":"df5=df.copy()","941a6e52":"df5=new_var(df5)","1e7d0ff1":"df5=label_encoder(df5, var_two_cat(df5))","ae830508":"df5=knn_imputer(df5, 4)","f43b5dfd":"df_scores=lof_scores(df5)\nprint(np.sort(df_scores))","b3ca66c8":"df5=lof(df5, df_scores, np.sort(df_scores)[13])","d558f624":"df5=minmax_scaler(df5, numeric_cols(df5))","2c7f6c28":"df5.isnull().sum().sum()","a953e6be":"df5.name='df5'\ndf5.head()","ff7ad059":"def reg_model(df, Y, algo, test_size=0.20):\n    X=df.drop(Y, axis=1)\n    Y=df[[Y]]\n    X_train, X_test, Y_train, Y_test=train_test_split(X, Y, test_size=test_size, random_state=42)\n    model=algo.fit(X_train, Y_train)\n    Y_train_pred=model.predict(X_train)\n    train_rmse=np.sqrt(mean_squared_error(Y_train, Y_train_pred))\n    print(df.name)\n    print(type(model).__name__)\n    print(\"Train RMSE: {}\".format(train_rmse))\n    \n    Y_test_pred=model.predict(X_test)\n    test_rmse=np.sqrt(mean_squared_error(Y_test, Y_test_pred))\n    print(\"Test RMSE: {}\".format(test_rmse))\n    print('###################################')\n    return (df.name, type(model).__name__, train_rmse, test_rmse)","944a5ab1":"models=[LinearRegression(), Ridge(), Lasso(), ElasticNet(), LGBMRegressor()]\ndataframes=[df1, df2, df3, df4, df5]\nresults={'frame':[], 'model':[], 'train_rmse':[], 'test_rmse':[]}","a27fd3e5":"for frame in dataframes:\n    for model in models:\n        res=reg_model(frame, 'Salary', model)\n        results['frame'].append(res[0])\n        results['model'].append(res[1])\n        results['train_rmse'].append(res[2])\n        results['test_rmse'].append(res[3])","8bcc729e":"results=pd.DataFrame(results)\nresults","73fdc777":"for frame in dataframes:\n    sns.barplot(x= 'test_rmse', y = 'model', data=results[results['frame']==frame.name])\n    plt.title(frame.name)\n    plt.xlabel('Errors')\n    plt.ylabel('Models')\n    plt.show()","2dbded31":"def model_tuning(df, Y, algo_cv, algo, grid, test_size=0.20, cv=10):\n    X=df.drop(Y, axis=1)\n    Y=df[[Y]]\n    X_train, X_test, Y_train, Y_test=train_test_split(X, Y, random_state=42, test_size=test_size)\n    if type(algo()).__name__=='LGBMRegressor':\n        model=algo()\n        model_cv=algo_cv(model, grid, cv=10, n_jobs=-1, verbose=2)\n        model_cv.fit(X_train, Y_train)\n        model_tuned=LGBMRegressor(learning_rate=model_cv.best_params_['learning_rate'],\n                         max_depth=model_cv.best_params_['max_depth'],\n                         n_estimators=model_cv.best_params_['n_estimators'],\n                         boosting_type=model_cv.best_params_['boosting_type'],\n                         colsample_bytree=model_cv.best_params_['colsample_bytree'])\n    else:   \n        model_cv=algo_cv(alphas=grid, cv=cv)\n        model_cv.fit(X_train, Y_train)\n        model_tuned=algo(alpha=model_cv.alpha_)\n    model_tuned.fit(X_train, Y_train)\n    print(df.name)\n    print(type(model_tuned).__name__)\n    Y_train_pred=model_tuned.predict(X_train)\n    train_rmse=np.sqrt(mean_squared_error(Y_train, Y_train_pred))\n    print(\"Train RMSE:{}\".format(train_rmse))\n    Y_test_pred=model_tuned.predict(X_test)\n    test_rmse=np.sqrt(mean_squared_error(Y_test, Y_test_pred))\n    print(\"Test RMSE:{}\".format(test_rmse))\n    print('#####################')\n    return (df.name, type(model_tuned).__name__, train_rmse, test_rmse)","91cf181a":"models={Ridge: RidgeCV, Lasso:LassoCV, ElasticNet:ElasticNetCV, LGBMRegressor: GridSearchCV}\nresults_tuned={'frame':[], 'model':[], 'train_rmse':[], 'test_rmse':[]}\nlgbm_grid={\n    'colsample_bytree':[0.4, 0.5, 0.6, 0.9, 1],\n    'boosting_type':['dart'],\n    'learning_rate':[0.01, 0.1, 0.5, 0.2],\n    'num_leaves':[30, 31, 32],\n           'n_estimators':[20, 40, 100, 200, 150, 90, 110],\n           'max_depth':[1, 2, 3, 4, 5, 6, 7, 8]}\nalphas = [0.1,0.01, 0.005, 0.05, 0.001,0.2,0.3,0.5,0.8,0.9]","7811fd55":"for frame in dataframes:\n    for model in models:\n        if type(model()).__name__=='LGBMRegressor':\n            res=model_tuning(frame, 'Salary', models[model], model, lgbm_grid)\n        else:\n            res=model_tuning(frame, 'Salary', models[model], model, alphas)\n        results_tuned['frame'].append(res[0])\n        results_tuned['model'].append(res[1])\n        results_tuned['train_rmse'].append(res[2])\n        results_tuned['test_rmse'].append(res[3])","0fa15485":"results_tuned=pd.DataFrame(results_tuned)\nresults_tuned","9e0853f9":"for frame in dataframes:\n    sns.barplot(x= 'test_rmse', y = 'model', data=results_tuned[results_tuned['frame']==frame.name])\n    plt.title(frame.name)\n    plt.xlabel('Errors')\n    plt.ylabel('Models')\n    plt.show()","5a5f9fe9":"The aim of this study is to set up different models for the Hitters data set and minimize error scores in 4 data sets that have undergone different preprocessing.","8c6f3791":"For Df4: NaN values \u200b\u200bwere filled with KNN. Outliers detected on the LOF side were suppressed. Dummy variables are created. The x variables are normalized.","bcde5ad8":"# Data Preprocessing","dd1f2112":"Types of variables in data set were examined.","0f1854b0":"# Outliers","85a01b40":"# Fifth option","556145e9":"# Reporting","d947a039":"# Second option","f03fdbd3":"The 5 data frames mentioned above were trained with models.(Linear Regression, Ridge, Lasso and Light GBM)","9f798f5d":"# First option","0cd2c75a":"df5 -> the best rmse -> Lasso 212.8","b863227d":"df4 -> the best rmse -> LightGBM 153.2","8dfb0b4f":"df1 -> the best rmse -> LightGBM 353.8","938836c7":"2) With Exploratory Data Analysis:","d2b4e678":"For Df2: the year column is categorized and the column \"new_year\" is created. NaN values \u200b\u200bwere filled by looking at the averages of \"Salary\" in age, league and department variables. Dummy variables were created. The x variables are normalized.","4fb80761":"# 4. Tuning","19f4ee66":"For Df5: New variables were added, NaN values \u200b\u200bwere filled with KNN, outliers detected on the LOF side were suppressed.","f02bcbb6":"1) Hitters Data Set was read.\n","7629427b":"# Third option","eb776667":"# Fourth option","281f6168":"We will create different data sets for different scenarios that we will apply for salary estimation.","a386efd6":"For Df1: NaN values \u200b\u200bdeleted, Outliers detected by LOF and fall. Dummy variables are created. The x variables are normalized.","91dd86f5":"3) In Data PreProcessing:","2212c372":"For Df3: NaN values \u200b\u200bwere filled with KNN. Dummy variables were created. The x variables are normalized.","9074bcf2":"Descriptive statistics of the data set were examined.","d65f46e0":"df3 -> the best rmse -> LightGBM 284.8","d2a8db34":"# 1. Data Understanding","27c07331":"# MODEL TUNING","2d61b4b8":"# 2. Data Preprocessing","e0425fad":"4) During the Model Building phase:","66fb9255":"# New variables","67674905":"# Libraries","627d7fed":"# Model","e9e30f41":"# Min-Max Scaler","3123141d":"# DATA UNDERSTANDING","3a32041b":"# Local Outlier Factor","423a5ed4":"# 5. Conclusion","e95df5f6":"Overall, lower error was obtained with Light GBM in 4 data frames. The best result was obtained with 141.1 with Light GBM trained with df4.","7c183ea0":"# 3. Modeling","2b92c92c":"The size information of the data set has been accessed.","2fd3277f":"Structural information of the dataset was checked.","7472fdbf":"When the model created as a result of Light GBM Hyperparameter optimization was applied to the df4 Data Frame, the lowest RMSE was obtained. (141.1)","bee19e5c":"Trying different values \u200b\u200bfor hyper parameters, it was tried to find the best model.","5e2f1b6b":"Using the Linear, Ridge, Lasso, ElasticNet and Light GBM machine learning models, RMSE values representing the difference between actual values and predicted values were calculated. Later, hyperparameter optimizations were applied for Ridge, Lasso and ElasticNet to further reduce the error value.","58ecc721":"df2 -> the best rmse -> LightGBM 295.3","6f4ae50b":"The number of missing observations from which variable in the data set was accessed. It was observed that there were 59 missing observations only in \"Salary\" which was dependent variable.\n","888c06a5":"The studies conducted are as follows:"}}