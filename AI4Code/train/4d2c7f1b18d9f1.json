{"cell_type":{"d55a040e":"code","6d083fa9":"code","86fbd04a":"code","c0820f9e":"code","f61b0dbb":"code","c4c8fd0f":"code","bad17a0d":"code","08f77e18":"code","4332946e":"code","3e6247c5":"code","dedbd8ec":"code","46cc1a96":"code","262e69c1":"code","449ff64b":"code","5d403e62":"code","8bb7ea56":"code","6ffe1791":"code","b2585227":"code","85b37620":"code","3a6a0f16":"code","04cb637c":"code","6775fd81":"code","4d65701e":"code","831281c8":"code","99fa2c07":"code","73425896":"code","dd28242e":"code","b3b9b8c9":"code","50924d98":"code","881fc9b6":"code","26fc0222":"markdown","aa15290b":"markdown","564ec847":"markdown","f426a638":"markdown","e21ee43b":"markdown","4b28cd19":"markdown","488aa94a":"markdown","d03e62c9":"markdown","885891c5":"markdown","6722c967":"markdown","1b0932dc":"markdown","eed8d2ec":"markdown","041efb77":"markdown","464a181b":"markdown","29f73fd4":"markdown","3cf8d721":"markdown","a7720c5f":"markdown","17535d5a":"markdown","ab8ae922":"markdown","f796c504":"markdown","9a2facd1":"markdown","7365f01c":"markdown","a9a2ee5f":"markdown","cd52fd90":"markdown","a5763eb1":"markdown","77f2c3a6":"markdown"},"source":{"d55a040e":"%matplotlib inline","6d083fa9":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(action=\"ignore\") # Filters out all warnings\n#warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")","86fbd04a":"files=os.listdir(\"..\/input\")","c0820f9e":"for f in files:\n    print(f)","f61b0dbb":"df1=pd.read_csv(\"..\/input\/\"+files[0])\ndf2=pd.read_csv(\"..\/input\/\"+files[1])\n# Using both data files\ndf = df1.append(df2)","c4c8fd0f":"df.head()","bad17a0d":"df.isna().sum() \n# Since all are 0, means the data contains no Null values so no explicit handling required","08f77e18":"df.describe()\n#df.columns","4332946e":"df['Serial No.']=range(1,901)\ndf.index=range(0,900)\ndf=df.rename(index=str,columns={\"Chance of Admit \":\"Chance of Admit\",\"LOR \":\"LOR\"})\ndf.head(10)","3e6247c5":"#pd.plotting.scatter_matrix(df,alpha=0.2,figsize=(30,30),diagonal='kde')\n#plt.show()\npp = sns.pairplot(data=df,y_vars=['Chance of Admit'],x_vars=['GRE Score', 'TOEFL Score', 'University Rating','SOP','LOR','CGPA','Research'])","dedbd8ec":"df.hist(bins=3,figsize=(10,10))\nplt.show()","46cc1a96":"corr_matrix = df.corr()\ncorr_matrix[\"Chance of Admit\"].sort_values(ascending=False)","262e69c1":"X=df.drop(columns=['Serial No.','Chance of Admit'])\ny=df[['Chance of Admit']]","449ff64b":"X.head()","5d403e62":"from sklearn.model_selection import train_test_split","8bb7ea56":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n\nprint(\"Shape of x_train :\", X_train.shape)\nprint(\"Shape of x_test :\", X_test.shape)\nprint(\"Shape of y_train :\", y_train.shape)\nprint(\"Shape of y_test :\", y_test.shape)","6ffe1791":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score","b2585227":"linreg_model = LinearRegression()\nlinreg_model.fit(X_train, y_train)\n\ny_pred = linreg_model.predict(X_test)\n\nprint(\"---------------------------------------\")\nprint('Coefficients for independent variables:', dict(zip(X.columns,linreg_model.coef_[0])))\nprint(\"---------------------------------------\")\nprint('Intercept:', linreg_model.intercept_)\nprint('Slope:' ,linreg_model.coef_)\nprint(\"---------------------------------------\")","85b37620":"mse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\n\n# Another detailed way to calculate RMSE below:-\n\n# mse2 = np.sum((linreg_pred - y_test)**2)\n\n# # root mean squared error\n# # m is the number of training examples\n# rmse = np.sqrt(mse2\/len(X_test))\n\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Root Mean Squared Error : \",rmse)\nprint(\"R-Square : \", r2)","3a6a0f16":"lr_df = pd.DataFrame({'Actual':np.array(y_test)[:,0],'Predicted':y_pred[:,0]})","04cb637c":"sns.regplot(lr_df['Actual'],lr_df['Predicted'],fit_reg=True,color='red')\nplt.show()\nplt.close()","6775fd81":"from sklearn.model_selection import cross_val_score\n# cv variable tells in how many parts do we need to divide the data into a stratified manner.\nscores = cross_val_score(linreg_model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10)\nrmse_scores = np.sqrt(-scores)\nprint(\"RMSE scores :\", rmse_scores)\nprint(\"Average RMSE value after cross validation :\", np.mean(rmse_scores))","4d65701e":"from sklearn.tree import DecisionTreeRegressor","831281c8":"tree_model = DecisionTreeRegressor(random_state=1)\ntree_model.fit(X_train, y_train)\n\ny_pred=tree_model.predict(X_test)\ntree_model","99fa2c07":"tree_mse = mean_squared_error(y_test, y_pred)\ntree_rmse = np.sqrt(tree_mse)\ntree_r2 = r2_score(y_test, y_pred)\nprint(\"Root Mean Squared Error : \",tree_rmse)\nprint(\"R-Square :\", tree_r2)","73425896":"from sklearn.model_selection import cross_val_score\n# cv variable tells in how many parts do we need to divide the data into a stratified manner.\nscores = cross_val_score(tree_model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10)\nrmse_scores = np.sqrt(-scores)\nprint(\"RMSE scores :\", rmse_scores)\nprint(\"Average RMSE value after cross validation :\", np.mean(rmse_scores))","dd28242e":"from sklearn.ensemble import RandomForestRegressor","b3b9b8c9":"forest_model = RandomForestRegressor(n_estimators=10, random_state=2)\nforest_model.fit(X_train, y_train)\n\ny_pred=forest_model.predict(X_test)\n\nforest_model","50924d98":"forest_mse = mean_squared_error(y_test, y_pred)\nforest_rmse = np.sqrt(forest_mse)\nforest_r2 = r2_score(y_test, y_pred)\nprint(\"Root Mean Squared Error : \",forest_rmse)\nprint(\"R-Square :\", forest_r2)","881fc9b6":"from sklearn.model_selection import cross_val_score\n# cv variable tells in how many parts do we need to divide the data into a stratified manner.\nscores = cross_val_score(forest_model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10)\nrmse_scores = np.sqrt(-scores)\nprint(\"RMSE scores :\", rmse_scores)\nprint(\"Average RMSE value after cross validation :\", np.mean(rmse_scores))","26fc0222":"## Relation between the variables\n\nUsing a scatter matrix to look at how the variables in the data are related to each other.\n#### Insight\nScatter Matrix clearly shows a positive slope and direct relation between Chance of Admit and other relevant dependent variables.","aa15290b":"### Checking whether we have Null values in data","564ec847":"### Building the model and Predicting on Test Set","f426a638":"### Visual Representation of R square\nR\u00b2 score or the coefficient of determination explains how much the total variance of the dependent variable can be reduced by using the least square regression.","e21ee43b":"### Evaluation of Model","4b28cd19":"## Correlation between Variables","488aa94a":"## Distribution of variables in the data\n\nLooking at the distribution of variables in the data to see whether certain values in excess don't affect the model too much.\nWe can actually use a 400 values each from Research column but in the current model I am passing by this intervention.","d03e62c9":"# Predictive Models","885891c5":"### Evaluation of Model\nThe Random Forest model has a higher R-square than the Decision Tree model and a lower RMSE. This means that this model perfoms better than DT model but there is also a possibility of overfitting. Hence, we should cross-validate on this data for all the ML models created for predicition.","6722c967":"### Evaluation of Model\nThe decision tree model has a higher R-square than the Linear Regression model and a lower RMSE. This means that this model perfoms better than LR but there is also a possibility of overfitting. Hence, we should cross-validate on this data for all the ML models created for prediction.","1b0932dc":"## Looks like all variables are important variables for predicting Admit chances\nHence, taking all columns in Independent variables(predictors) and Chance of Admit as Dependent variable(to be predicted).","eed8d2ec":"### Issue with Serial No. and Index Column and Column Name\n\nAlthough, Serial No. column is not required but an issue that I noticed was that while merging I didn't take care of Serial numbers and Index columnn because of which duplicate Serial numbers and indices present. So, updating the Serial numbers column and Index column.","041efb77":"## Random Forest Regression","464a181b":"## Reading data files","29f73fd4":"### Splitting Data into Testing and Training Set","3cf8d721":"### Building the model and predicting on test set","a7720c5f":"## Linear Regression","17535d5a":"## Fine Tuning Model\n### Cross Validation","ab8ae922":"## Reviewing data","f796c504":"## Fine Tuning Model\n### Cross Validation ","9a2facd1":"### Conclusion: All models perform well but based on the RMSE values the performance of Random Forest Regressor is best and Linear Regression is lowest.","7365f01c":"## Fine Tuning Model\n### Cross Validation\n#### It basically means training the data on a part of the training set and testing\/evaluating it on the other part of the training set itself.\nThere is a possibility that we may be overfitting our model. Hence, just to verify it is better to cross validate data on your training set itself. This is usually done when we have divided are dataset into 3 main sets: Training data, Test data and Validation Set. Usually we don't touch the test set until we are ready to launch a model we are confident about, so we need to use part of the training set for training, and part for model validation. ","a9a2ee5f":"### Building the model and predicting on test set","cd52fd90":"#### Looking at how predicted values look like against observed\/actual values","a5763eb1":"## Decision Tree Regression","77f2c3a6":"# Exploratory Data Analysis (EDA)"}}