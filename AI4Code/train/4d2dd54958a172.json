{"cell_type":{"359bd18c":"code","9707a982":"code","cd043e00":"code","0e3c8cee":"code","8bf9e791":"code","30eaead5":"code","7b8b67d6":"code","cdb18293":"code","1396fd3a":"code","7557f05f":"code","067e0735":"code","5522f39d":"code","701e0b63":"code","37fb3716":"code","a969067c":"code","da470dfa":"code","333aed22":"code","6de5039f":"code","9f8b9c5d":"code","75d60c05":"code","d82ab049":"code","3a0f243c":"code","8a66ccf9":"code","9abf6ab5":"code","4a52adf2":"code","cd5caf4b":"code","c073f0cc":"code","5c22b913":"code","a911322f":"code","fd8d59c8":"code","2095aaf0":"code","6b6f6127":"code","0ab327af":"code","44b08a48":"code","aba996bf":"code","0d8a1130":"code","40ed2921":"code","27df1c99":"code","9a88d02a":"code","024baec2":"code","bca64bf6":"code","6064bdc8":"code","e4c8df22":"markdown","bfc12894":"markdown","463f687a":"markdown","2f366fe6":"markdown","1541a2ce":"markdown","8ceade98":"markdown","ef2acbaa":"markdown","5e9cdf1d":"markdown","561853fb":"markdown","84607afa":"markdown","2e2aaa91":"markdown","71c5697c":"markdown","f69a47a3":"markdown","8f6c859f":"markdown","28de8aa9":"markdown","42f5ce83":"markdown","5ecdf025":"markdown","30d02f0c":"markdown","f8d365ec":"markdown","a5085f7f":"markdown","f247fc19":"markdown","4160486a":"markdown","7e3c0174":"markdown","1f47e35e":"markdown","b2ca0e57":"markdown","809f2fcd":"markdown","1e354fee":"markdown"},"source":{"359bd18c":"import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\nfrom imblearn.over_sampling import SMOTE \nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import classification_report, confusion_matrix, plot_roc_curve, plot_precision_recall_curve","9707a982":"src = pd.read_csv(\"..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\")","cd043e00":"src.head()","0e3c8cee":"src.dtypes","8bf9e791":"src.isnull().mean()","30eaead5":"src.shape","7b8b67d6":"src[\"DEATH_EVENT\"].value_counts()","cdb18293":"src[\"DEATH_EVENT\"].value_counts().plot(kind=\"bar\")\nplt.title(\"Class Distribution\");","1396fd3a":"# Storing categorical and numerical features names in different Series\ncat_features = [\"anaemia\",\"diabetes\",\"high_blood_pressure\",\"sex\",\"smoking\",\"DEATH_EVENT\"]\nnum_features = pd.Series(src.columns)\nnum_features = num_features[~num_features.isin(cat_features)]","7557f05f":"for i in cat_features:\n    ct = pd.crosstab(columns=src[i],index=src[\"DEATH_EVENT\"])\n    stat, p, dof, expected = chi2_contingency(ct) \n    print(f\"\\n{'-'*len(f'CROSSTAB BETWEEN {i.upper()} & DEATH_EVENT')}\")\n    print(f\"CROSSTAB BETWEEN {i.upper()} & DEATH_EVENT\")\n    print(f\"{'-'*len(f'CROSSTAB BETWEEN {i.upper()} & DEATH_EVENT')}\")\n    print(ct)\n    print(f\"\\nH0: THERE IS NO RELATIONSHIP BETWEEN DEATH_EVENT & {i.upper()}\\nH1: THERE IS RELATIONSHIP BETWEEN DEATH_EVENT & {i.upper()}\")\n    print(f\"\\nP-VALUE: {np.round(p,2)}\")\n    print(\"REJECT H0\" if p<0.05 else \"FAILED TO REJECT H0\")","067e0735":"r = c = 0\nfig,ax = plt.subplots(3,2,figsize=(14,12))\nfor n,i in enumerate(cat_features[:-1]):\n    ct = pd.crosstab(columns=src[i],index=src[\"DEATH_EVENT\"],normalize=\"columns\")\n    ct.T.plot(kind=\"bar\",stacked=True,color=[\"green\",\"red\"],ax=ax[r,c])\n    ax[r,c].set_ylabel(\"% of observations\")\n    c+=1\n    if (n+1)%2==0:\n        r+=1\n        c=0\nax[r,c].axis(\"off\")\nplt.show()","5522f39d":"r = c = 0\nfig,ax = plt.subplots(4,2,figsize=(14,25))\nfor n,i in enumerate(num_features):\n    sns.boxplot(x=\"DEATH_EVENT\",y=i,data=src,ax=ax[r,c])\n    ax[r,c].set_title(i.upper()+\" by \"+\"DEATH_EVENT\")\n    c+=1\n    if (n+1)%2==0:\n        r+=1\n        c=0\nax[r,c].axis(\"off\")\nplt.show()","701e0b63":"fig = plt.figure(figsize=(8,6))\nsns.heatmap(src[num_features].corr(),annot=True,fmt=\".2f\",mask=np.triu(src[num_features].corr()),cbar=False)\nplt.show()","37fb3716":"src[num_features].hist(figsize=(14,14))\nplt.show();","a969067c":"X = src.iloc[:,:-1]\ny = src.iloc[:,-1]","da470dfa":"rf = RandomForestClassifier(n_estimators=5000,random_state=11)\nrf.fit(X,y)\nfeat_imp = pd.DataFrame(rf.feature_importances_)\nfeat_imp.index = pd.Series(src.iloc[:,:-1].columns)\nfeat_imp = (feat_imp*100).copy().sort_values(by=0,ascending=False)\nfeat_imp = feat_imp.reset_index()\nfeat_imp.columns = [\"Feature\",\"Importance_score\"]\n\nfig = plt.figure(figsize=(6,10))\nsns.scatterplot(data=feat_imp,x=5,y=np.linspace(100,0,12),size=\"Importance_score\",sizes=(200,2000),legend=False)\nfor i,feat,imp in zip(np.linspace(100,0,12),feat_imp[\"Feature\"],feat_imp[\"Importance_score\"]):\n    plt.text(x=5.05,y=i-1,s=feat)\n    plt.text(x=4.89,y=i-1,s=np.round(imp,2))\nplt.axis(\"off\")\nplt.title(\"Feature Importance\")\nplt.show()","333aed22":"for var in np.arange(feat_imp.shape[0],6,-1):\n    X_new = X[feat_imp.iloc[:var,0]].copy()\n    X_train, X_test, y_train,y_test = train_test_split(X_new,y,test_size=0.2,random_state=11)\n    final_rf = RandomForestClassifier(random_state=11)\n    gscv = GridSearchCV(estimator=final_rf,param_grid={\n        \"n_estimators\":[100,500,1000,5000],\n        \"criterion\":[\"gini\",\"entropy\"]\n    },cv=5,n_jobs=-1,scoring=\"f1_weighted\")\n\n    gscv.fit(X_train,y_train)\n    print(str(var)+\" variables:  \"+str(gscv.best_estimator_)+\"  F1 score: \"+str(gscv.best_score_))","6de5039f":"X_new = X[feat_imp.iloc[:8,0]].copy()\nX_train, X_test, y_train,y_test = train_test_split(X_new,y,test_size=0.2,random_state=11)\nfinal_rf = RandomForestClassifier(random_state=11)\ngscv = GridSearchCV(estimator=final_rf,param_grid={\n    \"n_estimators\":[100,500,1000,5000],\n    \"criterion\":[\"gini\",\"entropy\"]\n},cv=5,n_jobs=-1,scoring=\"f1_weighted\")\n\ngscv.fit(X_train,y_train)\nFINAL_MODEL_NO_SMOTE = gscv.best_estimator_","9f8b9c5d":"FINAL_MODEL_NO_SMOTE.score(X_train,y_train)","75d60c05":"train_pred = FINAL_MODEL_NO_SMOTE.predict(X_train)\nprint(classification_report(y_train,train_pred))","d82ab049":"FINAL_MODEL_NO_SMOTE.score(X_test,y_test)","3a0f243c":"X_new = X.copy()\nX_train, X_test, y_train,y_test = train_test_split(X_new,y,test_size=0.2,random_state=11)\nsmote = SMOTE(random_state = 11) \nX_train_smote, y_train_smote = smote.fit_sample(X_train, y_train)\n\n\nX_train.insert(12,\"category\",\"NO_SMOTE\")\nX_train_smote.insert(12,\"category\",\"SMOTE\")\n\n\nfinal_X = X_train.append(X_train_smote).copy()\nfinal_X.drop_duplicates(subset=list(final_X.columns[:-1]),inplace=True)\nfinal_cat = final_X[\"category\"]\nfinal_X.drop(columns=\"category\",inplace=True)\npca = PCA(n_components=2)\nfinal_X = pd.DataFrame(pca.fit_transform(final_X))\nfinal_X[\"category\"] = list(final_cat)\nfinal_X = final_X.loc[(final_X[0]<=200000) & (final_X[1]<=2000),:].copy()\nsns.relplot(data=final_X,x=0,y=1,hue=\"category\",alpha=0.6,s=100,height=6,aspect=1.5)\nplt.title(\"Synthetic Examples Generated Using SMOTE\");","8a66ccf9":"for var in np.arange(feat_imp.shape[0],6,-1):\n    X_new = X[feat_imp.iloc[:var,0]].copy()\n    X_train, X_test, y_train,y_test = train_test_split(X_new,y,test_size=0.2,random_state=11)\n    smote = SMOTE(random_state = 11) \n    X_train_smote, y_train_smote = smote.fit_sample(X_train, y_train)\n    final_rf = RandomForestClassifier(random_state=11)\n    \n    \n    gscv = GridSearchCV(estimator=final_rf,param_grid={\n        \"n_estimators\":[100,500,1000,5000],\n        \"criterion\":[\"gini\",\"entropy\"]\n    },cv=5,n_jobs=-1,scoring=\"f1_weighted\")\n\n    gscv.fit(X_train_smote,y_train_smote)\n    print(str(var)+\" variables:  \"+str(gscv.best_estimator_)+\"  F1 score: \"+str(gscv.best_score_))","9abf6ab5":"X_new = X[feat_imp.iloc[:8,0]].copy()\nX_train, X_test, y_train,y_test = train_test_split(X_new,y,test_size=0.2,random_state=11)\nsmote = SMOTE(random_state = 11) \nX_train_smote, y_train_smote = smote.fit_sample(X_train, y_train)\nfinal_rf = RandomForestClassifier(random_state=11)\ngscv = GridSearchCV(estimator=final_rf,param_grid={\n    \"n_estimators\":[100,500,1000,5000],\n    \"criterion\":[\"gini\",\"entropy\"]\n},cv=5,n_jobs=-1,scoring=\"f1_weighted\")\n\ngscv.fit(X_train_smote,y_train_smote)\nFINAL_MODEL = gscv.best_estimator_","4a52adf2":"FINAL_MODEL.score(X_train_smote,y_train_smote)","cd5caf4b":"train_pred = FINAL_MODEL.predict(X_train_smote)\nprint(classification_report(y_train_smote,train_pred))","c073f0cc":"FINAL_MODEL.score(X_test,y_test) #Test set score","5c22b913":"pred = FINAL_MODEL.predict(X_test)","a911322f":"print(classification_report(y_test,pred))","fd8d59c8":"train_size,train_acc,test_acc = learning_curve(FINAL_MODEL, X_train_smote,y_train_smote,cv=5)\nlearn_df = pd.DataFrame({\"Train_size\":train_size,\"Train_Accuracy\":train_acc.mean(axis=1),\"Test_Accuracy\":test_acc.mean(axis=1)}).melt(id_vars=\"Train_size\")\nsns.lineplot(x=\"Train_size\",y=\"value\",data=learn_df,hue=\"variable\")\nplt.title(\"Learning Curve\")\nplt.ylabel(\"Accuracy\");","2095aaf0":"sns.heatmap(confusion_matrix(y_test,pred),annot=True)\nplt.ylabel(\"Actual\")\nplt.xlabel(\"Prediction\");","6b6f6127":"X_new = X[feat_imp.iloc[:8,0]].copy()\nX_train, X_test, y_train,y_test = train_test_split(X_new,y,test_size=0.2,random_state=11)\nsmote = SMOTE(random_state = 11) \nX_train_smote, y_train_smote = smote.fit_sample(X_train, y_train)\nfinal_rf = RandomForestClassifier(random_state=11)\ngscv = GridSearchCV(estimator=final_rf,param_grid={\n    \"n_estimators\":[5000,7000],\n    \"criterion\":[\"gini\",\"entropy\"],\n    \"max_depth\":[3,5,7],\n    \"min_samples_split\":[80,100],\n    \"min_samples_leaf\":[40,50],\n},cv=5,n_jobs=-1,scoring=\"f1_weighted\")\n\ngscv.fit(X_train_smote,y_train_smote)\nFINAL_MODEL = gscv.best_estimator_","0ab327af":"FINAL_MODEL","44b08a48":"gscv.best_score_","aba996bf":"FINAL_MODEL.score(X_train_smote,y_train_smote)","0d8a1130":"train_pred = FINAL_MODEL.predict(X_train_smote)\nprint(classification_report(y_train_smote,train_pred))","40ed2921":"FINAL_MODEL.score(X_test,y_test) #Test set score","27df1c99":"pred = FINAL_MODEL.predict(X_test)","9a88d02a":"print(classification_report(y_test,pred))","024baec2":"plot_roc_curve(FINAL_MODEL, X_test, y_test)\nplt.show()","bca64bf6":"plot_precision_recall_curve(FINAL_MODEL, X_test, y_test)\nplt.show()","6064bdc8":"sns.heatmap(confusion_matrix(y_test,pred),annot=True)\nplt.ylabel(\"Actual\")\nplt.xlabel(\"Prediction\");","e4c8df22":"The model built without SMOTE has a score of 1 on training set and shows the model is overfitting the training data. Further hyperparameter tuning like max_depth may prevent the trees from overfitting.","bfc12894":"<h3>The model is overfit to the train set despite<\/h3>\n\n<h3>\n<ol>\n    <li>Using Ensembling<\/li>\n    <li>Reducing the number of features<\/li>\n    <\/ol>\n    \nHence, overfitting might be because of a very small training set used. As seen from the learning curve, adding additional examples to the training set will improve the model performance on unseen data. There has been an improvement in the CV score and test set score after adding synthetic examples using SMOTE. Further hyperparameter tuning may also be explored to prevent the classifier from overfitting.  ","463f687a":"Once we are done with finding the feature importance, we need to identify a threshold for feature importance. It is clear that we can choose 7.222778 can be chosen as threshold. But I want to find a threshold by building a model by eliminating one least important feature at a time till the feature serum_sodium. Then select a set of features with best score.\n\nSince the class is moderately imbalanced, F1 score would be an appropriate metric instead of accuracy.\n\n<h2>Training the model without SMOTE<\/h2>\n\n<h3>First we train the model without using SMOTE to get an idea of the score and also find the threshold for feature importance scores","2f366fe6":"<h3>Training set score is close to cross validation and test set score, showing that the model is not overfit to the training set.","1541a2ce":"Crosstabs\/contingency tables are one of the best ways to see how categorical variables are distributed among each other.\n\nChi-square test is a statistical technique to test a relationship betwen two categorical variables. The chi-square tests below shows no categorical variable has a relationship with the target variable.","8ceade98":"RandomForestClaasifier is an ensemble machine learning algorithm which uses a technique called bootstrap aggregation (aka bagging). Bootstrapping is a statistical technique of drawing multiple random samples of size 'k' with replacement. Aggregation refers to combining the prediction of multiple models.\n\nRandom Forests work by drawing multiple samples from the training set and build multiple models and average their predictions to give a final prediction.","ef2acbaa":"<a id=\"oof\"><\/a><h1>Overcoming Overfitting<\/h1>\n\n<h3>As discussed above, adding training examples can prevent overfitting, but we have no option to add more training examples. Hence, we need to do something else. We'll discuss few ways of overcoming overfitting a model to smaller datasets<\/h3>\n\n<h3>Using a simpler model on small training sets is one of the ways to prevent overfitting. But we're to learn how to overcome overfitting RandomForest to smaller datasets. Tuning few hyperparameters helps us overcome overfitting","5e9cdf1d":"<a id=\"rf\"><\/a><h1>Training RandomForestClassifier & Hyperparameter Tuning With GridSearchCV<\/h1>","561853fb":"<h3>The class\/label is moderately imbalanced, with 68% of the classes pointing to 0. If I build a baseline model without need of any training, say y_hat = src[\"DEATH_EVENT\"].mode(), I'd get an accuracy of 68%.<\/h3>\n\n\n<h3>A class is said to be imbalanced if a single category dominates another catrgory. In this case '0' appears 203 times while '1' appears just 96 times.<\/h3>\n\n<h3>To overcome the problem of imbalanced class, we'll try SMOTE which will be discussed later.<\/h3>","84607afa":"<a id=\"lc\"><\/a><h1>Identifying Overfitting With Learning Curve<\/h1>\n<h3>Learning curve helps us identify bias and variance and also helps us confirm if adding more training data will improve the performance on unseen data or not","2e2aaa91":"The predictions of the final model with 8 variables and SMOTE","71c5697c":"The test set scores and the CV scores of model trained on data with SMOTE applied are better compared to the model trained without SMOTE. So, we'll consider the model trained on data with SMOTE applied as the final model.","f69a47a3":"<h3>The above bar plots doesn't show a strong relationship among categorical variables and target variables as seen in the chi-square tests<\/h3>\n<h3>Boxplots are a one of the best ways to view relationships between numerical and categorical variables. Hence, we'll use them to find the relationships amomg numerical and target variables.<\/h3>\n","8f6c859f":"As seen above, SMOTE has brought an improvement in the model score.\n\nThe best model is the one with top 8 variables\n\nBelow we build the final model with 8 variables","28de8aa9":"The heatmap below shows no relationship among input features","42f5ce83":"<a id=\"eda\"><\/a><h1>EDA<\/h1>","5ecdf025":"The training set score is again 1. Further hyperparameter tuning like max_depth may prevent the trees from overfitting.","30d02f0c":"<h3>Now we see the model is not overfit to the training set. This is because:<\/h3>\n    \n<h4><ol><li>We selected a large number for n_estimators, which grows more trees, hence preventing overfitting.<\/li>\n<li>We prevented the tree to grow by choosing a low number for 'max_depth'.<\/li>\n<li>We selected large numbers for 'max_samples_split' and 'max_samples_leaf', which ensures the leaf has a good number of samples.","f8d365ec":"<h3>SMOTE stands for 'Synthetic Minority Oversampling Technique' is an oversampling technique of the minority category. SMOTE generates synthetic samples to brings the count of the categories in the training target variable to the same number.<\/h3> Below is a plot to show synthetic samples\/examples generated using SMOTE in comparison with original samples\/examples","a5085f7f":"Below we see the percentage of missing values per column. The dataset has no missing values","f247fc19":"<h2>Metrics of the final model","4160486a":"<a id=\"feature_selection\"><\/a><h1>Feature Selection<\/h1>\n\n<h3>Finding feature importance with RandomForestClassifier.<\/h3>\n\n'time' is the most influential variable as it can be seen from the earlier boxplot.\n\n'serum_creatinine', 'ejection_fraction', 'age', 'creatinine_phosphokinase', 'platelets', 'serum_sodium' are looking important in the specified order. All these numerical features and as we saw from the chi-square tests earlier, the categorical features are not so important.","7e3c0174":"<h1>THIS NOTEBOOK COVERS<\/h1>\n    \n<h3><ol>\n<li><a href=\"#eda\">EDA<\/a><\/li>\n<li><a href=\"#feature_selection\">Feature Selection<\/a><\/li>\n<li><a href=\"#smote\">Handling Imbalanced Class Problem With SMOTE<\/a><\/li>\n<li><a href=\"#rf\">Training RandomForestClassifier & Hyperparameter Tuning With GridSearchCV<\/a><\/li>\n<li><a href=\"#lc\">Identifying Overfitting With Learning Curve<\/a><\/li>\n<li><a href=\"#oof\">Overcoming Overfitting<\/a><\/li>    ","1f47e35e":"Model with top 8 variables has the best score. Hence the threshold is 1.375402. We'll include all the features with importance above the threshold. So let's build a model with the 8 variables","b2ca0e57":"<h3>As seen in the ROC curve above, this model has a good True Positive Rate (TPR) which is important for a model used for medical diagnosis","809f2fcd":"<h3>As expected the model is overfit to the training set as the test set score is 0.86 and the CV score is 0.84 vs the training set score of 1.","1e354fee":"<a id=\"smote\"><\/a><h1>Handling Imbalanced Class Problem With SMOTE<\/h1>"}}