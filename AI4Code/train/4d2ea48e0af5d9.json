{"cell_type":{"0220f7a0":"code","1a5958c3":"code","b6b60a0d":"code","98c2c7ab":"code","9e53abde":"code","dd9f42cd":"code","d9748b02":"code","5f7e5af6":"code","75e9dd39":"code","4a2a7f3c":"code","7a481635":"code","aacc2054":"code","16a03506":"code","cfa4d0e8":"code","8b3aeffc":"code","9b4ba673":"code","72cf40ec":"code","89024b5a":"markdown"},"source":{"0220f7a0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1a5958c3":"import math\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","b6b60a0d":"train = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')","98c2c7ab":"# Pavel's change\n# make_csv_dataset only seems to work with label_name being the first column??\ntrain = train.reindex(columns = train.columns[::-1])\ntest  = test.reindex(columns = test.columns[::-1])","9e53abde":"train.drop('id', axis=1, inplace=True)\ntest.drop('id', axis=1, inplace=True)","dd9f42cd":"random_selection = np.random.rand(len(train.index)) <= 0.85\ntrain_data = train[random_selection]\nvalid_data = train[~random_selection]","d9748b02":"train_data_file = \"train_data.csv\"\nvalid_data_file = \"valid_data.csv\"\ntest_data_file = \"test_data.csv\"\n\ntrain_data.to_csv(train_data_file, index=False, header=train_data.columns)\nvalid_data.to_csv(valid_data_file, index=False, header=train_data.columns)\ntest.to_csv(test_data_file, index=False, header=False)","5f7e5af6":"# Target feature name.\nTARGET_FEATURE_NAME = 'target'\n# Weight column name.\nWEIGHT_COLUMN_NAME = \"instance_weight\"\n# Numeric feature names.\nNUMERIC_FEATURE_NAMES = train.drop(['target'], axis=1).columns\n\n# All features names.\nFEATURE_NAMES = NUMERIC_FEATURE_NAMES\n# Feature default values.\nCOLUMN_DEFAULTS = [[0.0]]","75e9dd39":"from tensorflow.keras.layers import StringLookup\n\n\ndef process(features, target):\n    weight = features.pop(WEIGHT_COLUMN_NAME)\n    return features, target, weight\n\n\ndef get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):\n\n    dataset = tf.data.experimental.make_csv_dataset(\n        csv_file_path,\n        batch_size=batch_size,\n        column_names=train_data.columns,\n        column_defaults=COLUMN_DEFAULTS,\n        label_name=TARGET_FEATURE_NAME,\n        num_epochs=1,\n        header=True,\n        shuffle=shuffle,\n    ).map(process)\n\n    return dataset","4a2a7f3c":"def create_model_inputs():\n    inputs = {}\n    for feature_name in FEATURE_NAMES:\n        if feature_name in NUMERIC_FEATURE_NAMES:\n            inputs[feature_name] = layers.Input(\n                name=feature_name, shape=(), dtype=tf.float32\n            )\n        else:\n            inputs[feature_name] = layers.Input(\n                name=feature_name, shape=(), dtype=tf.string\n            )\n    return inputs","7a481635":"def encode_inputs(inputs, encoding_size):\n    encoded_features = []\n    for feature_name in inputs:\n       # Project the numeric feature to encoding_size using linear transformation.\n        encoded_feature = tf.expand_dims(inputs[feature_name], -1)\n        encoded_feature = layers.Dense(units=encoding_size)(encoded_feature)\n        encoded_features.append(encoded_feature)\n    return encoded_features","aacc2054":"class GatedLinearUnit(layers.Layer):\n    def __init__(self, units):\n        super(GatedLinearUnit, self).__init__()\n        self.linear = layers.Dense(units)\n        self.sigmoid = layers.Dense(units, activation=\"sigmoid\")\n\n    def call(self, inputs):\n        return self.linear(inputs) * self.sigmoid(inputs)","16a03506":"class GatedResidualNetwork(layers.Layer):\n    def __init__(self, units, dropout_rate):\n        super(GatedResidualNetwork, self).__init__()\n        self.units = units\n        self.elu_dense = layers.Dense(units, activation=\"elu\")\n        self.linear_dense = layers.Dense(units)\n        self.dropout = layers.Dropout(dropout_rate)\n        self.gated_linear_unit = GatedLinearUnit(units)\n        self.layer_norm = layers.LayerNormalization()\n        self.project = layers.Dense(units)\n\n    def call(self, inputs):\n        x = self.elu_dense(inputs)\n        x = self.linear_dense(x)\n        x = self.dropout(x)\n        if inputs.shape[-1] != self.units:\n            inputs = self.project(inputs)\n        x = inputs + self.gated_linear_unit(x)\n        x = self.layer_norm(x)\n        return x","cfa4d0e8":"class VariableSelection(layers.Layer):\n    def __init__(self, num_features, units, dropout_rate):\n        super(VariableSelection, self).__init__()\n        self.grns = list()\n        # Create a GRN for each feature independently\n        for idx in range(num_features):\n            grn = GatedResidualNetwork(units, dropout_rate)\n            self.grns.append(grn)\n        # Create a GRN for the concatenation of all the features\n        self.grn_concat = GatedResidualNetwork(units, dropout_rate)\n        self.softmax = layers.Dense(units=num_features, activation=\"softmax\")\n\n    def call(self, inputs):\n        v = layers.concatenate(inputs)\n        v = self.grn_concat(v)\n        v = tf.expand_dims(self.softmax(v), axis=-1)\n\n        x = []\n        for idx, input in enumerate(inputs):\n            x.append(self.grns[idx](input))\n        x = tf.stack(x, axis=1)\n\n        outputs = tf.squeeze(tf.matmul(v, x, transpose_a=True), axis=1)\n        return outputs","8b3aeffc":"def create_model(encoding_size):\n    inputs = create_model_inputs()\n    feature_list = encode_inputs(inputs, encoding_size)\n    num_features = len(feature_list)\n\n    features = VariableSelection(num_features, encoding_size, dropout_rate)(\n        feature_list\n    )\n\n    outputs = layers.Dense(units=1, activation=\"sigmoid\")(features)\n    model = keras.Model(inputs=inputs, outputs=outputs)\n    return model","9b4ba673":"train_data_file","72cf40ec":"learning_rate = 0.001\ndropout_rate = 0.15\nbatch_size = 265\nnum_epochs = 20\nencoding_size = 16\n\nmodel = create_model(encoding_size)\nmodel.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=1e-3),\n        metrics=['AUC']\n    )\n\n# Create an early stopping callback.\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\", patience=5, restore_best_weights=True\n)\n\nprint(\"Start training the model...\")\ntrain_dataset = get_dataset_from_csv(\n    train_data_file, shuffle=True, batch_size=batch_size\n)\nvalid_dataset = get_dataset_from_csv(valid_data_file, batch_size=batch_size)\nmodel.fit(\n    train_dataset,\n    epochs=num_epochs,\n    validation_data=valid_dataset,\n    callbacks=[early_stopping],\n)\nprint(\"Model training finished.\")\n\nprint(\"Evaluating model performance...\")\n# test_dataset = get_dataset_from_csv(test_data_file, batch_size=batch_size)\n# _, accuracy = model.evaluate(test_dataset)\n# print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")","89024b5a":"Copied from [Nikhil Khetan](https:\/\/www.kaggle.com\/nikhilkhetan), debugging"}}