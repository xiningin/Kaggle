{"cell_type":{"b33da757":"code","615f06c1":"code","101f0c51":"code","a90b8521":"code","6d33c1e9":"code","63fc9fca":"code","75148a87":"code","13e0b98d":"code","4d237bcb":"code","003a1a60":"code","efa86f05":"code","e2941b74":"code","65ccb4bd":"code","76688ace":"code","c6721fd0":"code","531d0854":"code","806a2f56":"code","665a2886":"code","f46657c9":"code","61347498":"code","c4a64b0e":"code","71e7b873":"code","d8350b88":"code","f0434919":"code","54df6d80":"code","2c548124":"code","94f60df3":"code","79dd186e":"code","47bb8c0e":"code","bb322a60":"code","ec1ef5a3":"code","0c61d021":"code","138884e2":"code","d841d806":"code","e6f8309c":"code","755d2d96":"code","2bd08ef9":"code","78287e0c":"code","20b91370":"code","7e756b6e":"code","090d02ce":"code","c15bbb77":"code","2cb54126":"code","5052882d":"code","f522d147":"code","e068f63f":"code","0c017ea1":"code","37940f21":"code","22c6afec":"code","440442e9":"code","49cf004a":"code","fc85b472":"code","e60a3317":"markdown","15de44cf":"markdown","0c27e0f3":"markdown","05d2c19e":"markdown","917839f5":"markdown","cfe2b7f6":"markdown","23ebf2d5":"markdown","86dd34fc":"markdown","b6c7439a":"markdown","32064d20":"markdown","b1a16beb":"markdown","441acb14":"markdown","f2d1b94d":"markdown","72e70a1c":"markdown","293d8e1e":"markdown","6585ad14":"markdown","de95fbc5":"markdown","e087011d":"markdown","1e28acfc":"markdown","498663f0":"markdown","e06dc87f":"markdown","f83570ff":"markdown","25a1ca68":"markdown","1d8efe44":"markdown","e82908c5":"markdown","8985c5e3":"markdown","09c78e3e":"markdown","a6f84e7e":"markdown","6bba9957":"markdown","47f447b2":"markdown","cf4ab7ca":"markdown","e72065f6":"markdown","7201a200":"markdown","dea17027":"markdown","119de484":"markdown","b1d10033":"markdown","a2b8241b":"markdown","e6a2864f":"markdown","c3c382ad":"markdown","7ef60aa8":"markdown","22716474":"markdown","bf1cebf5":"markdown","64253278":"markdown","f3ec2577":"markdown","3644bf3c":"markdown","b65e53b4":"markdown","09d1526d":"markdown","f0c723eb":"markdown","03ef4491":"markdown","4c28c827":"markdown","e527ac88":"markdown","55be45c0":"markdown","28923f80":"markdown","5f3e8e43":"markdown","7ebe163d":"markdown","ea47aa5e":"markdown","e136b0c0":"markdown","78b55376":"markdown","847193da":"markdown","1922c5fa":"markdown","4c2c97a0":"markdown","db75303d":"markdown","097dbf83":"markdown","e10f90ee":"markdown","9fd23447":"markdown","d21d265d":"markdown","f22b4068":"markdown","50d1cd6e":"markdown","f7930502":"markdown","0625484f":"markdown","3d09f8bd":"markdown","47e8014c":"markdown","019f498c":"markdown","601df480":"markdown","9ca92a5f":"markdown","131daf08":"markdown","9d08e168":"markdown","45483f2e":"markdown","45575dc5":"markdown","ee46471b":"markdown","9277be86":"markdown","c419b84e":"markdown","b141392e":"markdown","49d3d08d":"markdown","0353eb00":"markdown"},"source":{"b33da757":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nplt.style.use('bmh')\nimport warnings\nwarnings.filterwarnings('ignore')","615f06c1":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\ndfdict = dict()\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        name = Path(os.path.join(dirname, filename)).stem\n        dfdict[name] = pd.read_csv(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","101f0c51":"with pd.option_context('display.max_rows', 6, 'display.max_columns', None):  # show all df columns\n    for name, df_ in dfdict.items():\n        display(name)\n        display(df_.shape)\n        display(df_.head(6))","a90b8521":"for name, df_ in dfdict.items():\n    display(name)\n    display(df_.info())","6d33c1e9":"# Remove columns with almost no values: listings: bathrooms, neighbourhood_group_cleansed, calendar_updated, license\ndfdict['listings'].drop(columns=['bathrooms', 'neighbourhood_group_cleansed', 'calendar_updated', 'license'], inplace=True)\n# Remove irrelevant columns\ndfdict['listings'].drop(columns=['listing_url', 'picture_url', 'host_url', 'host_name', 'host_thumbnail_url', 'host_picture_url'], inplace=True)\ndfdict['listings'].drop(columns=['minimum_minimum_nights', 'maximum_minimum_nights', 'minimum_maximum_nights', 'maximum_maximum_nights', \n                                 'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm'], inplace=True)","63fc9fca":"print(dfdict['calendar']['date'].head())\nprint(dfdict['reviews']['date'].head())","75148a87":"dfdict['calendar']['date'] = pd.to_datetime(dfdict['calendar']['date'])\ndfdict['reviews']['date'] = pd.to_datetime(dfdict['reviews']['date'])\ndfdict['listings']['last_scraped'] = pd.to_datetime(dfdict['listings']['last_scraped'])\ndfdict['listings']['host_since'] = pd.to_datetime(dfdict['listings']['host_since'])\ndfdict['listings']['calendar_last_scraped'] = pd.to_datetime(dfdict['listings']['calendar_last_scraped'])\ndfdict['listings']['first_review'] = pd.to_datetime(dfdict['listings']['first_review'])\ndfdict['listings']['last_review'] = pd.to_datetime(dfdict['listings']['last_review'])","13e0b98d":"def convert_price(df_column):\n    return df_column.str.replace('$', '', regex = 'true').str.replace(',', '', regex = 'true').astype(float)","4d237bcb":"dfdict['listings']['price'] = convert_price(dfdict['listings']['price'])\ndfdict['calendar']['price'] = convert_price(dfdict['calendar']['price'])\ndfdict['calendar']['adjusted_price'] = convert_price(dfdict['calendar']['adjusted_price'])","003a1a60":"def convert_boolean(df_column):\n    return df_column.replace({'f': 0, 't': 1}).astype('boolean')","efa86f05":"# Convert t\/f fields to boolean\n# calendar dataframe : available\n# listings dataframe : (host_is_superhost, host_has_profile_pic, host_identity_verified, calendar_updated, has_availability, instant_bookable)\ndfdict['calendar']['available'] = convert_boolean(dfdict['calendar']['available'])\ndfdict['listings']['host_is_superhost'] = convert_boolean(dfdict['listings']['host_is_superhost'])\ndfdict['listings']['host_has_profile_pic'] = convert_boolean(dfdict['listings']['host_has_profile_pic'])\ndfdict['listings']['host_identity_verified'] = convert_boolean(dfdict['listings']['host_identity_verified'])","e2941b74":"dfdict['listings']['has_availability'] = convert_boolean(dfdict['listings']['has_availability'])\ndfdict['listings']['instant_bookable'] = convert_boolean(dfdict['listings']['instant_bookable'])","65ccb4bd":"# Convert 'host_acceptance_rate', 'host_response_rate', removing the %\ndfdict['listings']['host_acceptance_rate'] = dfdict['listings']['host_acceptance_rate'].str.replace('%', '', regex = 'true').str.replace(',', '', regex = 'true').astype(float)\ndfdict['listings']['host_response_rate'] = dfdict['listings']['host_response_rate'].str.replace('%', '', regex = 'true').str.replace(',', '', regex = 'true').astype(float)","76688ace":"# Convert % fields to float: listings.host_response_rate, listings.host_acceptance_rate\ndisplay(dfdict['calendar'].info())\n# Convert floats to int: listings.bathrooms, listings.bedrooms, listings.beds\ndisplay(dfdict['listings'].info())","c6721fd0":"for name, df_ in dfdict.items():\n    display(name, df_.index, '------------------------')","531d0854":"print(len(dfdict['calendar'].index))\nprint(dfdict['calendar'].groupby(['listing_id', 'date'])['available'].transform('nunique')) #count(distinct)\nprint(dfdict['calendar'].groupby(['listing_id', 'date'])['available'].count())","806a2f56":"# Calendar appears to have listing_id and date as index\ndfdict['calendar'].set_index(['listing_id', 'date'], inplace=True)","665a2886":"print('[Listings] number of records: ', len(dfdict['listings'].index))\nprint('[Listings] Unique id values: ', len(dfdict['listings']['id'].unique()))\nprint('[Reviews] Listings: number of records: ', len(dfdict['reviews'].index))\nprint('[Reviews] Unique id values: ', len(dfdict['reviews']['id'].unique()))","f46657c9":"dfdict['listings'].set_index('id', inplace=True)\ndfdict['reviews'].set_index('id', inplace=True)","61347498":"for name, df_ in dfdict.items():\n    display(name)\n    display(df_.isnull().sum())  # isna() does the same thing","c4a64b0e":"print(dfdict['listings']['price'].describe(percentiles=[0.25, 0.5, 0.75, 0.9, 0.95, 0.98, 0.99, 0.997]))","71e7b873":"# Let's calculate the percentage of each job status category.\ndisplay(dfdict['listings'].neighbourhood_cleansed.value_counts(normalize=True))\n\n# plot the bar graph of percentage job categories\nplt.figure(figsize = (12, 6))\ndfdict['listings'].neighbourhood_cleansed.value_counts(normalize=True).plot.barh()\nplt.show()","d8350b88":"print('bedrooms: ', dfdict['listings']['bedrooms'].unique())\nprint('beds: ', dfdict['listings']['beds'].unique())\nprint('review_scores_rating', dfdict['listings']['review_scores_rating'].unique())","f0434919":"dfdict['listings']['property_type'].unique()","54df6d80":"# calculate the percentage of each property type.\ndisplay(dfdict['listings']['property_type'].value_counts(normalize=True))\n\n# plot the pie chart of property categories\nplt.figure(figsize = (20, 12))\ndfdict['listings']['property_type'].value_counts(normalize=True).plot.pie(autopct='%1.0f%%', pctdistance=1.1, labeldistance=1.2, rotatelabels=True)\nplt.show()","2c548124":"for name, df_ in dfdict.items():\n    display(name)\n    display(df_.describe())","94f60df3":"df_num = dfdict['listings'].select_dtypes(include = ['float64', 'int64'])\ndf_num.head()","79dd186e":"params = {'axes.titlesize':'8', 'xtick.labelsize':'12', 'ytick.labelsize':'12'}\nplt.rcParams.update(params)\ndf_num.drop(columns=['scrape_id', 'host_id']).hist(figsize=(20, 20), bins=50, xlabelsize=8, ylabelsize=8, ); # ; avoid having the matplotlib verbose informations","47bb8c0e":"print(dfdict['listings']['price'].describe(percentiles=[0.25, 0.5, 0.75, 0.9, 0.95, 0.98, 0.99, 0.997]))\nplt.figure(figsize=(9, 8))\nsns.distplot(dfdict['listings']['price'], color='g', bins=100, hist_kws={'alpha': 0.4});","bb322a60":"plt.figure(figsize=(9, 8))\nsns.distplot(dfdict['listings'][dfdict['listings']['price'] <= 1250]['price'], color='g', bins=100, hist_kws={'alpha': 0.4});","ec1ef5a3":"plt.figure(figsize=(10,8))\nplt.title('Price distribution for properties that cost < GBP 1200')\nsns.boxplot(y='price', x='room_type', data = dfdict['listings'][dfdict['listings']['price'] < 1200])","0c61d021":"plt.figure(figsize=(10,8))  # 'review_scores_cleanliness', 'review_scores_rating'\nplt.title('Review scores distribution for London Airbnb properties.')\nsns.boxplot(y='review_scores_rating', x='room_type', data = dfdict['listings'][['room_type', 'review_scores_rating']])","138884e2":"corr = df_num.corr()\nplt.figure(figsize=(12, 10))\n\nsns.heatmap(corr[(corr >= 0.1) | (corr <= -0.1)], \n            cmap='viridis', vmax=1.0, vmin=-1.0, linewidths=0.1,\n            annot=True, annot_kws={\"size\": 8}, square=True);","d841d806":"df_not_num = dfdict['listings'].select_dtypes(include = ['O'])\nprint('There are {} non numerical features including:\\n{}'.format(len(df_not_num.columns), df_not_num.columns.tolist()))\ndf_not_num.head(4)","e6f8309c":"# 'host_location' is a pretty large column, with the host's full address! We'll not consider it.\n# 'host_neighbourhood' has way too many values, let's ignore it.\n# 'host_verifications' and 'amenities' are multi-value column. \n# TODO Include 'host_acceptance_rate', 'host_response_rate'\ndf_not_num = df_not_num[['neighbourhood_cleansed', 'property_type', 'room_type', 'host_response_time', 'bathrooms_text']]","755d2d96":"ncols = 1\n\nfig, axes = plt.subplots(round(len(df_not_num.columns) \/ ncols), ncols, figsize=(20, 40))\n#plt.xticks(rotation=90)\n\nfor i, ax in enumerate(fig.axes):\n    if i < len(df_not_num.columns):\n        ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=45)\n        ax.set_xticklabels(ax.get_xticklabels(), rotation = 90, ha=\"right\")\n        sns.countplot(x=df_not_num.columns[i], alpha=0.7, data=df_not_num, ax=ax)\n\nfig.tight_layout()","2bd08ef9":"# plot the scatter plot of neighborhood and price variable in data\nplt.figure(figsize=(6, 5))\nplt.scatter(dfdict['listings'].review_scores_cleanliness, dfdict['listings'].price)\nplt.xticks(rotation=90)\nplt.title('review_scores_cleanliness x property price', fontsize =20)\nplt.show()\n\n# plot the scatter plot of review_scores_rating and price variable in data\ndfdict['listings'].plot.scatter(x=\"review_scores_rating\",y=\"price\")\nplt.title('review_scores_rating x property price', fontsize =20)\nplt.show()","78287e0c":"# plot the pair plot of beds, price and property review scores in dataframe\nplt.figure(figsize = (10, 5))\nsns.pairplot(data = dfdict['listings'], vars=['beds', 'price', 'review_scores_cleanliness', 'review_scores_rating', \n                                              'review_scores_location', 'review_scores_value', 'host_acceptance_rate'])\nplt.show()","20b91370":"# Creating a matrix using beds, accommodates, price and some review scores as rows and columns\nxpto = dfdict['listings'][['beds','accommodates','price', 'review_scores_cleanliness', 'review_scores_rating',\n                          'review_scores_location', 'review_scores_value', 'host_acceptance_rate']].corr()\n\n# plot the correlation matrix of these columns in data dataframe\nsns.heatmap(xpto, annot=True, cmap = 'Reds')\nplt.show()","7e756b6e":"df_num.corr()['price']","090d02ce":"df_num_corr = df_num.corr()['price'][:-1] # -1 because it is the latest row in df_num\ngolden_features_list = df_num_corr[abs(df_num_corr) > 0.1].sort_values(ascending=False)\nprint(\"There are {} correlated columns with respect to 'price':\\n{}\".format(len(golden_features_list), golden_features_list))","c15bbb77":"# groupby the listings df to find the mean of the property price according to the city neighborhood.\ndfdict['listings'].groupby('neighbourhood_cleansed')['price'].mean()","2cb54126":"# groupby the listings df to find the median of the property price according to the city neighborhood.\ndfdict['listings'].groupby('neighbourhood_cleansed')['price'].median()","5052882d":"plt.figure(figsize = (12, 6))\ndfdict['listings'].groupby('neighbourhood_cleansed')['price'].mean().plot.bar()\nplt.show()","f522d147":"# plot the box plot of price according to neighborhood, after removing outliers where price > 300 GBP\nplt.figure(figsize = (20, 12))\ndf_filtered = dfdict['listings'][dfdict['listings'].price <= 300]\nsns.boxplot(df_filtered.price, df_filtered.neighbourhood_cleansed, orient=\"h\", palette=\"Set2\")\nplt.show()","e068f63f":"dfdict['calendar']","0c017ea1":"print('There are', dfdict['calendar'].reset_index()['date'].nunique(), 'days and', \n      dfdict['calendar'].reset_index()['listing_id'].nunique(), 'unique listings on calendar df.')","37940f21":"dfdict['calendar']['occupied'] = dfdict['calendar']['available'].astype(float) * 100.0","22c6afec":"avg_daily_price = dfdict['calendar'].reset_index().groupby('date').mean()\n#print(avg_daily_price)\n# Plotting the Graph\nplt.figure(figsize=(10, 5))\nprice_plot_by_day = avg_daily_price['price'].plot(title='Average property prices')\nprice_plot_by_day.set_xlabel('Date')\nprice_plot_by_day.set_ylabel('Property price')","440442e9":"avg_daily_occupancy = dfdict['calendar'].reset_index().groupby('date').mean()\nprint(avg_daily_occupancy)\n# Plotting the Graph\nplt.figure(figsize=(10, 5))\noccupancy_plot_by_day = avg_daily_occupancy['occupied'].plot(title='Average property occupancy (%)')\noccupancy_plot_by_day.set_xlabel('Date')\noccupancy_plot_by_day.set_ylabel('Property Occupancy')","49cf004a":"first_10_neihborhoods = dfdict['listings']['neighbourhood_cleansed'].unique()[0:10]\nprint('first_10_neihborhoods: ', first_10_neihborhoods)\ndf_filtered = dfdict['listings'][dfdict['listings']['price'] < 400]\ndf_filtered = df_filtered[df_filtered['neighbourhood_cleansed'].isin(first_10_neihborhoods)]\ng = sns.catplot(y=\"neighbourhood_cleansed\", x=\"price\", hue=\"room_type\", kind=\"bar\", data=df_filtered, height=8.27, aspect=11.7\/8.27)\ng.set_xticklabels(rotation=90)","fc85b472":"result = pd.pivot_table(data=dfdict['listings'], index='neighbourhood_cleansed', columns='bedrooms',values='price')\nprint('Pivot table:\\n', result)\n\n# create heat map of neighbourhood vs price vs availability_rate\nplt.figure(figsize = (12, 6))\nax = plt.axes()\nsns.heatmap(result, annot=False, cmap = 'RdYlGn', center=0.117, ax=ax)\nax.set_title('Property price x neighborhood x number of beds')\nplt.show()","e60a3317":"We need to convert the following columns from object to datetime64: `calendar.date, reviews.date, listings: host_since, calendar_last_scraped, first_review, last_review.`","15de44cf":"**Note: I gave up joining the `listing` and `calendar` dataframes, because of OutOfMemory errors in Kaggle Jupyter Kernel :-(**","0c27e0f3":"The property listings refer to periods starting at Feb 2021 until Feb 2022.","05d2c19e":"Correlation is very weak for these variables (`c < 0.2`).","917839f5":"#### 2.5.1. Calendar dataframe","cfe2b7f6":"### TODO Other ideas: heat maps (using latitude and longitude) and tag clouds (using textual information from property listings, reviews, etc.).","23ebf2d5":"#### 4.2.1. a) Scatter plot\n\nLet\u2019s take the columns \u2018price\u2019, \u2018review_scores_cleanliness\u2019 and 'review_scores_rating' from our dataset and see what we can infer by plotting to scatter plot.","86dd34fc":"By the above analysis, we can infer that the data set has a large number of Entire apartments, followed by Private rooms in apartment and then by Private rooms in houses and Entire houses. Also, among the properties with very small percentage, we have 'Shared room in bus', 'Shared room in hotel', 'Room in minsu', 'Shared room in tent' and 'Earth house'. ","b6c7439a":"## London neighborhoods\n\n![London_neighborhoods](https:\/\/assets.londonist.com\/uploads\/2016\/08\/i730\/absolute_profit_from_airbnb_london.jpg)","32064d20":"## 1. Data import","b1a16beb":"As expected, the average price of Entire home\/apt is greater than Private rooms. Shared rooms are the cheapest properties.","441acb14":"Impressive. Sometimes a hotel room can be more expensive than an entire home\/apt! For example, this happens in Hounslow and Richmond upon Thames.","f2d1b94d":"#### Average price by day","72e70a1c":"## ","293d8e1e":"### 3.3. Handling Outliers\n\nLet's take a look at some outliers, regarding property price.\n\nOuliers can be handled by dropping the records or imputing with the values or leaving them as is, if it makes more sense.","6585ad14":"### 4.2.3. Categorical \u2014 Categorical Analysis\n\nLet's see how the different categories like neighborhood, property_type, etc., are associated with each other. ","de95fbc5":"#### 4.1.3. a) Visualize Numerical Data Distributions \u2014 Histogram Plot of all features","e087011d":"# Exploratory Data Analysis - Airbnb London Dataset\n\nPortions of this EDA were based on the following articles:\n* https:\/\/towardsdatascience.com\/exploratory-data-analysis-eda-python-87178e35b14\n* https:\/\/www.kaggle.com\/ekami66\/detailed-exploratory-data-analysis-with-python\n* https:\/\/levelup.gitconnected.com\/a-complete-exploratory-data-analysis-with-python-45a57f5ef4c9\n* https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic","1e28acfc":"We can see that the majority of the review scores is above 80, regardless of the room type.","498663f0":"Other fields that need to be converted: \n\n% => host_response_rate, host_acceptance_rate;\n\nint => accommodates, bathrooms, bedrooms, beds, minimum_nights, maximum_nights, availability_30, availability_60, availability_90, availability_365, number_of_reviews, number_of_reviews_ltm, number_of_reviews_l30d, calculated_host_listings_count, calculated_host_listings_count_entire_homes, calculated_host_listings_count_private_rooms, calculated_host_listings_count_shared_rooms);\n\n? => license => we do not know its datatype and it does not contain enough data.","e06dc87f":"#### 4.2.2. b) Plot the bar graph of neighborhood x average value of price","f83570ff":"#### 4.1.1. Categorical Unordered Univariate Analysis\n\nAn unordered variable is a categorical variable that has no defined order. If we take our data as an example, the neighbourhood_cleansed column in the dataset is divided into many sub-categories like ..., etc. There is no weight or measure given to any value in the \u2018neighbourhood_cleansed\u2019 column.\nNow, let\u2019s analyze the neighbourhood_cleansed category by using plots. Since neighbourhood_cleansed is a category, we will plot the bar plot.","25a1ca68":"#### 4.1.3. Numerical features\n\nIf the column or variable is numerical, then we\u2019ll analyze it by calculating its mean, median, standard deviation, etc. We can get those values by using the describe function.","1d8efe44":"#### 4.1.3. b) Visualize Price Distributions \u2014 Seaborn Histogram\n\nLet's take a look at how the property price is distributed.","e82908c5":"#### 4.1.2. Categorical Ordered Univariate Analysis\n\nOrdered variables are those variables that have a natural rank of order. Some examples of categorical ordered variables from our dataset are:\n\nproperty_type: Entire cottage, Campsite, Shared Room, ...\n\nBedrooms, beds: 1, 2, 3, ...\n\nNow, let\u2019s analyze the property_type from the dataset. Since we\u2019ve already seen a bar plot, let\u2019s see how a Pie Chart looks like.","8985c5e3":"By the above graph, we can infer that the property price is, on average, higher for more central locations, like City of London and Westminster. ","09c78e3e":"There is not a clear correlation between property rating and the price. \n\nBut... In the second graph, we can see that high-priced properties (`> 5000 GBP`) have only high review-scores ratings (`above 80`). ","a6f84e7e":"### 3.1. Check for null values","6bba9957":"Let's look for candidate indices for each dataframe.","47f447b2":"## 4. Feature analysis","cf4ab7ca":"#### 2.5.2. Listings and Reviews dataframes\n\nThese two dataframes have a predefined `id` field.","e72065f6":"#### 4.2.1. c) Correlation Matrix\n\nSince we cannot use more than two variables as x-axis and y-axis in Scatter and Pair Plots, it is difficult to see the relation between three numerical variables in a single graph. In those cases, we\u2019ll use the correlation matrix.","7201a200":"#### Average availability by day","dea17027":"Based on the Heatmap above, we can infer that more expensive properties usually have more beds available. Additionally, on more expensive neighborhoods, such as City of London, Westminster and Islington, smaller properties have high prices too. ","119de484":"df = dfdict['calendar'].join(dfdict['listings'], on='listing_id', rsuffix='_listing')","b1d10033":"#### 4.1.3. e) Calculate and Visualize Correlations \u2014 Seaborn Heat Map\n\n Let's see if some variables are linked between each other and then try to explain their relation with common sense.","a2b8241b":"### 2.5. Check current indices","e6a2864f":"We have now converted all important columns from `object` to their appropriate datatype, except for columns containing large texts or arrays of values. Let's take a look at the index of each dataframe.","c3c382ad":"It is difficult to find a trend between 2 different variables in the above graph.","7ef60aa8":"As seen above, `['listing_id', 'date']` is a good index for calendar table, i.e., they have a one-to-one correspondance with each line. Let's set this index.","22716474":"## 4.3. Multivariate Analysis\n\nIf we analyze data by taking more than two variables\/columns into consideration from a dataset, it is known as Multivariate Analysis.\nLet\u2019s see how \u2018neighborhood_cleansed\u2019, \u2018bedrooms\u2019, and \u2018price\u2019 vary with each other.\nWe\u2019ll create a pivot table with the three columns and after that, we\u2019ll create a heatmap.","bf1cebf5":"Now let's try to find which features are strongly correlated with `price`. We'll reuse our df_num dataset (created in 4.1.3.a) to do so.","64253278":"#### 4.2.2. d) Time series plot of price and availability\n\nLet's use the `calendar` dataframe to analyze the evolution of price and availability through time.\n\nThere are 27 million lines on the calendar dataframe. Let's see how many different dates we have.","f3ec2577":"### 2.3. Analyzing the date column on calendar and reviews dataframes","3644bf3c":"#### But we need to remove columns with large text or sentences! E.g., name, description, neighborhood_overview, host_about.","b65e53b4":"As we can see, when we plot the Box Plot, it paints a very different picture compared to mean and median. `City of London`, for instance, has the greatest minimum property price, and, together with `Kensignton and Chelsea` and `Westminster`, has the highest prices considering the IQR [25%-75&].","09d1526d":"There is a lot of price difference between the neighborhoods. `City of London` has an average price of 258 pounds, while `Bexley` costs on average 59 pounds. This gives us a price difference of 199 pounds!\n\nLet\u2019s calculate the median,","f0c723eb":"Let's define a new 0\/1 column to show availability.","03ef4491":"#### What types of property do we have?","4c28c827":"### 2.1. Check data types\nLet\u2019s see how Pandas determined the types of each column when loading them","e527ac88":"Property prices have their lowest values on the start of the dataset (Mar, Apr, May), and they start to rise until July (summer season). From July on, they remain stable. Finally price hit a peak near Christmas and New Year's eve. \n\nThere is also a strange valley in Fev 2022...","55be45c0":"#### Average % Property Occupancy for 2021 (i.e. the % of properties already booked, by day) oscillates between 27% and almost 50%, depending on the month.","28923f80":"## 3. Data Cleaning\n\nThe next step in the process of EDA is Data Cleaning. It is very important to get rid of the irregularities and clean the data after sourcing it into our system.\nIrregularities are of different types of data.\n\n* Missing Values\n* Incorrect Format\n* Incorrect Headers\n* Anomalies\/Outliers","5f3e8e43":"#### Both Listings and Reviews dataframes can have column 'id' as index, since it has unique values\n\nLet's set `id` as index for these 2 dataframes.","7ebe163d":"df.head(6)\nsns.lineplot(x=\"date\", y=\"occupied\",\n             hue=\"\", style=\"event\",\n             data=avg_daily_price)","ea47aa5e":"#### Observe that the dataset contains 76534 listed properties and over 1 million reviews. There are also 27 million lines on the calendar dataframe.","e136b0c0":"#### Review scores\n\nLet's see the distribution of review scores, grouped by room type.","78b55376":"Features such as `beds` and `bedrooms` seem to share a similar distribution to the one we have with `price`.\n\nRegarding `review_scores_rating`, remark that the majority of scores are concentrated above 75%.","847193da":"The median appears to be less sensitive to outliers. Now, we have: `City of London` with a median price of 119 pounds and `Bexley` with a median cost of 40 pounds. The median indicates a price difference of 79 pounds, against an average price difference of 199 pounds.","1922c5fa":"By the above bar plot, we can infer that the data set contains more number of Westminster bnb's compared to other neighborhoods. Other neighborhoods are also frequent, such as Tower Hamlets, Hackney, Kensignton and Chelsea, Camden and Islington.","4c2c97a0":"We have already analyzed neighborhood frequency in a previous graph.\n\nWe can see that the majority of properties are of the following types: `Private room in apartment`, `Entire apartment`, `Entire townhouse`, `Entire house`, `Private room in townhouse`, `Entire condominium` and `Entire serviced apartment`.\n\nWhen it comes to `room_type`, the vast majority are `private rooms` and `entire home\/apartments`.\n\nNormally, hosts give a response to their guests within an hour (more than 16000 hosts lie in this category). Less than 4000 hosts take more than a day\/a few days or more to answer. ","db75303d":"### 4.1. Univariate Analysis\n\nIf we analyze data over a single variable\/column from a dataset, it is known as Univariate Analysis.","097dbf83":" Let's look at their distribution.","e10f90ee":"We can see that the dataset date range is [2010-08-18, 2011-10-09]","9fd23447":"#### Property prices\n\nFirst, let's take a look at the property price distribution.\n\nSince there are some price outliers above the 1200 pounds range, we will filter them out when doing the box plots.","d21d265d":"#### 4.2.2. c) Boxplot of the price in function of the neighborhood\n\nBesides median and average values, it is important to observe the interquantile ranges (25%-75%), minimum and maximum values. The box plot gives us this information visually.","f22b4068":"Let's get all the types of our data from our dataset and take only the numerical ones.","50d1cd6e":"### 4.2.2. Numeric - Categorical Analysis\n\nAnalyzing the one numeric variable and one categorical variable from a dataset is known as numeric-categorical analysis. We analyze them mainly using mean, median, and box plots.\n\nLet\u2019s take price and neighborhood columns from our dataset.\nFirst check for mean value using groupby.","f7930502":"## 2. Analyzing columns\n\nLet's perform an initial analysis of columns, including data types, removal of irrelevant columns, or columns with too much missing values, and finally we'll setup an index for each dataframe.","0625484f":"With this information we can see that the prices are skewed right and some outliers lie above ~1250. Notice that the percentile 99.7% corresponds to this price of 1250 GBP.\n\nLet's regenerate the graph after removing these outliers (`price > 1250 GBP`).","3d09f8bd":"## About London\n\nSource: Wikipedia\n\n![London](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/e\/e4\/Palace_of_Westminster_from_the_dome_on_Methodist_Central_Hall_%28cropped%29.jpg\/1000px-Palace_of_Westminster_from_the_dome_on_Methodist_Central_Hall_%28cropped%29.jpg)\n\nLondon is the capital and largest city of England and the United Kingdom. The city stands on the River Thames in the south-east of England, at the head of its 50-mile (80 km) estuary leading to the North Sea. London has been a major settlement for two millennia, and was originally called Londinium, which was founded by the Romans. The City of London, London's ancient core and financial centre\u2014an area of just 1.12 square miles (2.9 km2) and colloquially known as the Square Mile\u2014retains boundaries that closely follow its medieval limits. The adjacent City of Westminster has for centuries been the location of much of the national government. Thirty-one additional boroughs north and south of the river also comprise modern London. The London region is governed by the mayor of London and the London Assembly.\n\nLondon is one of the world's most important global cities. It exerts a considerable impact upon the arts, commerce, education, entertainment, fashion, finance, healthcare, media, professional services, research and development, tourism and transportation. It is one of the largest financial centres in the world and in 2019, London had the second highest number of ultra high-net-worth individuals in Europe, after Paris. And in 2020, London had the second-highest number of billionaires of any city in Europe, after Moscow. London's universities form the largest concentration of higher education institutes in Europe, and London is home to highly ranked institutions such as Imperial College London in natural and applied sciences, the London School of Economics and social sciences, as well as the comprehensive University College London. In 2012, London became the first city to have hosted three modern Summer Olympic Games.\n\nLondon has a diverse range of people and cultures, and more than 300 languages are spoken in the region. Its estimated mid-2018 municipal population (corresponding to Greater London) was roughly 9 million, which made it the third-most populous city in Europe. London accounts for 13.4% of the U.K. population. Greater London Built-up Area is the fourth-most populous in Europe, after Istanbul, Moscow, and Paris, with 9,787,426 inhabitants at the 2011 census. The London metropolitan area is the third-most populous in Europe, after Istanbul and the Moscow Metropolitan Area, with 14,040,163 inhabitants in 2016.\n\nLondon contains four World Heritage Sites: the Tower of London; Kew Gardens; the site comprising the Palace of Westminster, Westminster Abbey, and St Margaret's Church; and the historic settlement in Greenwich where the Royal Observatory, Greenwich defines the Prime Meridian (0\u00b0 longitude) and Greenwich Mean Time. Other landmarks include Buckingham Palace, the London Eye, Piccadilly Circus, St Paul's Cathedral, Tower Bridge, Trafalgar Square and The Shard. London has numerous museums, galleries, libraries and sporting events. These include the British Museum, National Gallery, Natural History Museum, Tate Modern, British Library and West End theatres. The London Underground is the oldest underground railway network in the world.","47e8014c":"## 4.2. Bivariate Analysis\n\nIf we analyze data by taking two variables\/columns into consideration from a dataset, it is known as Bivariate Analysis.","019f498c":"As we can see, on the `calendar` dataframe, 4 columns contain missing values. We'll simply ignore the NaN values when plotting the graphs and analyzing the data.\n\nOn the `listings` dataframe, we'll ignore pure textual columns (with large sentences\/text), such as: `name, description, neighborhood_overview, host_about, host_neighborhood and neighborhood`. \n\nFor now, we'll ignore the `reviews` dataframe, since it only contains textual (reviewer's comments) data.\n\nLet\u2019s see how to handle the other missing values:  \n\nhost_response_time                              43221\n\nhost_response_rate                              43221\n\nhost_acceptance_rate                            34703\n\nhost_is_superhost                                  46\n\nbathrooms_text                                    181\n\nbedrooms                                         4838\n\nbeds                                             1219\n\nfirst_review                                    22194\n\nlast_review                                     22194\n\nreview_scores_rating                            23937\n\nreview_scores_accuracy                          23999\n\nreview_scores_cleanliness                       23990\n\nreview_scores_checkin                           24046\n\nreview_scores_communication                     23997\n\nreview_scores_location                          24045\n\nreview_scores_value                             24046\n\nreviews_per_month                               22194\n\nWe can handle missing values by dropping the missing records or by imputing the values. Or we can simply choose to ignore them at this moment. **That's exactly what we are going to do.**","601df480":"#### 4.1.4.a) Visualize Categorical Data Distributions \u2014 Histogram Plot of all features\n\nWe'll now visualize the non-numerical features.","9ca92a5f":"### 2.2. Dropping irrelevant information or with too much missing values \n\nFrom these informations above we can already see that some features won't be relevant in our exploratory analysis as there are too much missing values (such as `license` and `bathrooms`). Plus there is so much features to analyse that it may be better to concentrate on the ones which can give us real insights.","131daf08":"Only a few features seem to be correlated with each other. For example, \n\n* availability 30\/60\/90\/365\n* bedrooms\/beds\/accomodates\n* review scores rating\/accuracy\/cleanliness\/checkin\/communication\/value: the correlation between these columns indicates that, in a lot of cases, if a host has a good score (guest evaluation), s\/he also has good scores related to cleanliness of the room\/property, good communication with the guest and value, which makes sense.\n\nThe rest of the variables have very low correlation with each other. Against intuition, correlation between property price and review scores is very, very low...","9d08e168":"#### 4.2.1. b) Pair Plot\n\nNow, let\u2019s plot Pair Plots for some other numerical columns. We\u2019ll use the seaborn library for plotting Pair Plots.","45483f2e":"99.7% of all properties cost less than 1250 GBP. We could consider values above that as outliers.\n\nBesides, 90% of all properties cost less than 200 GBP.","45575dc5":"#### 4.1.3. d) Box plot of 'price', 'review_scores_cleanliness' and 'review_scores_rating'\n","ee46471b":"### Let's look at the shape of the data","9277be86":"The main file is `listings`. Taking a quick look at its data, we can observe 76534 property listings on the London Airbnb dataset, which provides 74 columns of information, such as listing and picture URL's, the date when the listing was scraped from Airbnb's website, name and description of the property and a textual overview of its neighborhood, data about the host (id, url, name, location, short-bio, if the host was verified by Airbnb and some statistics about his\/her response time). There are also columns containing geographical data about the property, such as latitude and longitude, its neighborhood, the property and room type (room, flat, etc.), how many persons it accommodates, how many beds, bedroom and bathrooms it has, room amenities, and, of course, information about price. The final group of columns regards several statistics about availability, guest reviews, and property rating (number of stars?).\n\n","c419b84e":"#### 4.2.2. a) Comparing mean and median values of price","b141392e":"### 2.4. Converting other columns","49d3d08d":"#### 4.2.3. a) Category plot of neighborhood, room type and price in dataframe `listings`","0353eb00":"### 4.2.1. Numeric-Numeric Analysis\n\nAnalyzing the two numeric variables from a dataset is known as numeric-numeric analysis. We can analyze it in three different ways.\n\n* Scatter Plot\n* Pair Plot\n* Correlation Matrix"}}