{"cell_type":{"a83fcbb7":"code","647bc21c":"code","6017f162":"code","da86567f":"code","30c3cd34":"code","a7db32d5":"code","3182a816":"code","a414b6e8":"code","d5bceb4a":"code","e46f5e0a":"code","4ac19684":"code","135cc13d":"code","2a9393d9":"code","dd03bf15":"code","f2a5cf51":"code","ac22aaf0":"code","c9f99345":"code","72b7f410":"code","564808ac":"code","155a16dd":"code","ae4344f7":"code","e29b40c1":"code","36bde022":"code","55f522e4":"code","f77235f6":"code","d080bd80":"code","bfaa3695":"code","6155533f":"code","00dda87f":"code","5c3540f1":"code","ebfe1607":"code","99869807":"markdown","f1f3e4a1":"markdown","8273f07e":"markdown","42afac44":"markdown","7fd9adae":"markdown","c0bee8b7":"markdown","cd5afe5f":"markdown","acc2ff1c":"markdown","9dd8f3ae":"markdown","9a2c45f3":"markdown","166217ee":"markdown","f3041af2":"markdown","8a9c4f34":"markdown","346f096a":"markdown","263b3b43":"markdown","be56d39d":"markdown","7613360b":"markdown","95a557a0":"markdown","e95d23d8":"markdown","ef3c73ee":"markdown","0a5daf96":"markdown","0a6d04f6":"markdown","7b7cc5f4":"markdown","bd2ecb35":"markdown","9e9002e9":"markdown","bc2e247e":"markdown","1f562682":"markdown","6cede6f6":"markdown","5944273b":"markdown","83cb12d0":"markdown","5f36f517":"markdown","dfea83c7":"markdown","e854313a":"markdown","44d95815":"markdown","d0550663":"markdown","798b053b":"markdown","30eef099":"markdown","05875cf8":"markdown","88431967":"markdown","bd337cc7":"markdown","6f3945cc":"markdown","c5dd3e66":"markdown","7cdc847e":"markdown","3f8b9b7f":"markdown","239475c5":"markdown"},"source":{"a83fcbb7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns #for plotting\nfrom sklearn.ensemble import RandomForestClassifier #for the model\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz #plot tree\nfrom sklearn.metrics import roc_curve, auc #for model evaluation\nfrom sklearn.metrics import classification_report #for model evaluation\nfrom sklearn.metrics import confusion_matrix #for model evaluation\nfrom sklearn.model_selection import train_test_split #for data splitting\nimport eli5 #for purmutation importance\nfrom eli5.sklearn import PermutationImportance\nimport shap #for SHAP values\nfrom pdpbox import pdp, info_plots #for partial plots\nnp.random.seed(123) #ensure reproducibility\n\npd.options.mode.chained_assignment = None  #hide any pandas warnings","647bc21c":"dt = pd.read_csv(\"..\/input\/heart.csv\")","6017f162":"dt.tail(10)","da86567f":"x=dt[['age','trestbps']]","30c3cd34":"y=dt.target","a7db32d5":"sns.pairplot(dt,hue='target')","3182a816":"dt.corr()","a414b6e8":"sns.heatmap(dt.corr())","d5bceb4a":"sns.scatterplot(data=dt, x=\"age\", y=\"trestbps\", hue=\"target\")","e46f5e0a":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nscaler.fit_transform(x)\nscaler.transform(x)","4ac19684":"X_train, X_test, y_train, y_test = train_test_split(dt.drop('target', 1), dt['target'], test_size = .2, random_state=10) #split the data","135cc13d":"model = RandomForestClassifier(max_depth=5)\nmodel.fit(X_train, y_train)","2a9393d9":"estimator = model.estimators_[1]\nfeature_names = [i for i in X_train.columns]\n\ny_train_str = y_train.astype('str')\ny_train_str[y_train_str == '0'] = 'no disease'\ny_train_str[y_train_str == '1'] = 'disease'\ny_train_str = y_train_str.values","dd03bf15":"#code from https:\/\/towardsdatascience.com\/how-to-visualize-a-decision-tree-from-a-random-forest-in-python-using-scikit-learn-38ad2d75f21c\n\nexport_graphviz(estimator, out_file='tree.dot', \n                feature_names = feature_names,\n                class_names = y_train_str,\n                rounded = True, proportion = True, \n                label='root',\n                precision = 2, filled = True)\n\nfrom subprocess import call\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n\nfrom IPython.display import Image\nImage(filename = 'tree.png')","f2a5cf51":"y_predict = model.predict(X_test)\ny_pred_quant = model.predict_proba(X_test)[:, 1]\ny_pred_bin = model.predict(X_test)","ac22aaf0":"confusion_matrix = confusion_matrix(y_test, y_pred_bin)\nconfusion_matrix","c9f99345":"total=sum(sum(confusion_matrix))\n\nsensitivity = confusion_matrix[0,0]\/(confusion_matrix[0,0]+confusion_matrix[1,0])\nprint('Sensitivity : ', sensitivity )\n\nspecificity = confusion_matrix[1,1]\/(confusion_matrix[1,1]+confusion_matrix[0,1])\nprint('Specificity : ', specificity)","72b7f410":"fpr, tpr, thresholds = roc_curve(y_test, y_pred_quant)\n\nfig, ax = plt.subplots()\nax.plot(fpr, tpr)\nax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for diabetes classifier')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)","564808ac":"auc(fpr, tpr)","155a16dd":"perm = PermutationImportance(model, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","ae4344f7":"base_features = dt.columns.values.tolist()\nbase_features.remove('target')\n\nfeat_name = 'num_major_vessels'\npdp_dist = pdp.pdp_isolate(model=model, dataset=X_test, model_features=base_features, feature=feat_name)\n\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()","e29b40c1":"feat_name = 'age'\npdp_dist = pdp.pdp_isolate(model=model, dataset=X_test, model_features=base_features, feature=feat_name)\n\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()","36bde022":"feat_name = 'st_depression'\npdp_dist = pdp.pdp_isolate(model=model, dataset=X_test, model_features=base_features, feature=feat_name)\n\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()","55f522e4":"inter1  =  pdp.pdp_interact(model=model, dataset=X_test, model_features=base_features, features=['st_slope_upsloping', 'st_depression'])\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=['st_slope_upsloping', 'st_depression'], plot_type='contour')\nplt.show()\n\ninter1  =  pdp.pdp_interact(model=model, dataset=X_test, model_features=base_features, features=['st_slope_flat', 'st_depression'])\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=['st_slope_flat', 'st_depression'], plot_type='contour')\nplt.show()","f77235f6":"explainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_test)\n\nshap.summary_plot(shap_values[1], X_test, plot_type=\"bar\")","d080bd80":"shap.summary_plot(shap_values[1], X_test)","bfaa3695":"def heart_disease_risk_factors(model, patient):\n\n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer.shap_values(patient)\n    shap.initjs()\n    return shap.force_plot(explainer.expected_value[1], shap_values[1], patient)","6155533f":"data_for_prediction = X_test.iloc[1,:].astype(float)\nheart_disease_risk_factors(model, data_for_prediction)","00dda87f":"data_for_prediction = X_test.iloc[3,:].astype(float)\nheart_disease_risk_factors(model, data_for_prediction)","5c3540f1":"ax2 = fig.add_subplot(224)\nshap.dependence_plot('num_major_vessels', shap_values[1], X_test, interaction_index=\"st_depression\")","ebfe1607":"shap_values = explainer.shap_values(X_train.iloc[:50])\nshap.force_plot(explainer.expected_value[1], shap_values[1], X_test.iloc[:50])","99869807":"This gives us on explainability tool. However, I can't glance at this and get a quick sense of the most important features. We'll revisit those later. Next, let's evaluate the model,","f1f3e4a1":"Check the data types,","8273f07e":"\\begin{align}\nSpecificity = \\frac{True\\:Negatives}{True\\:Negatives + False\\:Positives}\n\\end{align}","42afac44":"Interestingly, this variable also shows a reduction in probability the higher it goes. What exactly is this? A search on Google brought me to the following description by Anthony L. Komaroff, MD, an internal medicine specialist [5](https:\/\/www.sharecare.com\/health\/circulatory-system-health\/what-st-segment-electrocardiogram-ecg) .... *\"An electrocardiogram (ECG) measures the heart's electrical activity. The waves that appear on it are labeled P, QRS, and T. Each corresponds to a different part of the heartbeat. The **ST segment** represents the heart's electrical activity immediately after the right and left ventricles have contracted, pumping blood to the lungs and the rest of the body. Following this big effort, ventricular muscle cells relax and get ready for the next contraction. During this period, little or no electricity is flowing, so the ST segment is even with the baseline or sometimes slightly above it. The faster the heart is beating during an ECG, the shorter all of the waves become. **The shape and direction of the ST segment are far more important than its length. Upward or downward shifts can represent decreased blood flow to the heart from a variety of causes, including heart attack, spasms in one or more coronary arteries (Prinzmetal's angina), infection of the lining of the heart (pericarditis) or the heart muscle itself (myocarditis), an excess of potassium in the bloodstream, a heart rhythm problem, or a blood clot in the lungs (pulmonary embolism).\"***\n\n<img style=\"float: left;\" src=\"https:\/\/www.cvphysiology.com\/uploads\/images\/CAD012%20ST%20elevation.png\" width=\"350px\"\/>    [6](https:\/\/www.cvphysiology.com\/CAD\/CAD012)","7fd9adae":"Let's take a look,","c0bee8b7":"# The Data","cd5afe5f":"It looks like a low depression is bad in both cases. Odd.\n\nLet's see what the SHAP values tell us. These work by showing the influence of the values of every variable in a single row, compared to their baseline values (learn more [here](https:\/\/www.kaggle.com\/dansbecker\/shap-values)).","acc2ff1c":"<a id='section2'><\/a>","9dd8f3ae":"# The Model\n\nThe next part fits a random forest model to the data,","9a2c45f3":"I'm also going to change the values of the categorical variables, to improve the interpretation later on,","166217ee":"\\begin{align}\nSensitivity = \\frac{True\\:Positives}{True\\:Positives + False\\:Negatives}\n\\end{align}","f3041af2":"OK, so it's working well.","8a9c4f34":"<a id='section1'><\/a>","346f096a":"It's a clean, easy to understand set of data. However, the meaning of some of the column headers are not obvious. Here's what they mean,\n\n- **age**: The person's age in years\n- **sex**: The person's sex (1 = male, 0 = female)\n- **cp:** The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\n- **trestbps:** The person's resting blood pressure (mm Hg on admission to the hospital)\n- **chol:** The person's cholesterol measurement in mg\/dl\n- **fbs:** The person's fasting blood sugar (> 120 mg\/dl, 1 = true; 0 = false) \n- **restecg:** Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n- **thalach:** The person's maximum heart rate achieved\n- **exang:** Exercise induced angina (1 = yes; 0 = no)\n- **oldpeak:** ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more [here](https:\/\/litfl.com\/st-segment-ecg-library\/))\n- **slope:** the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n- **ca:** The number of major vessels (0-3)\n- **thal:** A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\n- **target:** Heart disease (0 = no, 1 = yes)\n\nTo avoid [HARKing](https:\/\/journals.sagepub.com\/doi\/abs\/10.1207\/s15327957pspr0203_4) (or Hypothesizing After the Results are Known) I'm going to take a look at online guides on how heart disease is diagnosed, and look up some of the terms above.\n\n**Diagnosis**: The diagnosis of heart disease is done on a combination of clinical signs and test results. The types of tests run will be chosen on the basis of what the physician thinks is going on [1](https:\/\/www.mayoclinic.org\/diseases-conditions\/heart-disease\/diagnosis-treatment\/drc-20353124), ranging from electrocardiograms and cardiac computerized tomography (CT) scans, to blood tests and exercise stress tests [2](https:\/\/www.heartfoundation.org.au\/your-heart\/living-with-heart-disease\/medical-tests).\n\nLooking at information of heart disease risk factors led me to the following: **high cholesterol, high blood pressure, diabetes, weight, family history and smoking** [3](https:\/\/www.bhf.org.uk\/informationsupport\/risk-factors). According to another source [4](https:\/\/www.heart.org\/en\/health-topics\/heart-attack\/understand-your-risks-to-prevent-a-heart-attack), the major factors that can't be changed are: **increasing age, male gender and heredity**. Note that **thalassemia**, one of the variables in this dataset, is heredity. Major factors that can be modified are: **Smoking, high cholesterol, high blood pressure, physical inactivity, and being overweight and having diabetes**. Other factors include **stress, alcohol and poor diet\/nutrition**.\n\nI can see no reference to the 'number of major vessels', but given that the definition of heart disease is **\"...what happens when your heart's blood supply is blocked or interrupted by a build-up of fatty substances in the coronary arteries\"**, it seems logical the *more* major vessels is a good thing, and therefore will reduce the probability of heart disease.\n\nGiven the above, I would hypothesis that, if the model has some predictive ability, we'll see these factors standing out as the most important.","263b3b43":"Next, let's pick out individual patients and see how the different variables are affecting their outcomes,","be56d39d":"The number of major vessels is at the top. Let's use a summary plot of the SHAP values,","7613360b":"We can plot the consequent decision tree, to see what it's doing,","95a557a0":"# Conclusion\n\nThis dataset is old and small by today's standards. However, it's allowed us to create a simple model and then use various machine learning explainability tools and techniques to peek inside. At the start, I hypothesised, using (Googled) domain knowledge that factors such as cholesterol and age would be major factors in the model. This dataset didn't show that. Instead, the number of major factors and aspects of ECG results dominated. I actually feel like I've learnt a thing or two about heart disease!\n\nI suspect this sort of approach will become increasingly important as machine learning has a greater and greater role in health care.","e95d23d8":"<a id='section4'><\/a>","ef3c73ee":"That seems reasonable. Let's also check with a **[Receiver Operator Curve (ROC)](https:\/\/en.wikipedia.org\/wiki\/Receiver_operating_characteristic)**,","0a5daf96":"Next, load the data,","0a6d04f6":"That's a bit odd. The higher the age, the lower the chance of heart disease? Althought the blue confidence regions show that this might not be true (the red baseline is within the blue zone).\n\nWhat about the 'st_depression',","7b7cc5f4":"Assess the fit with a confusion matrix,","bd2ecb35":"You can see the stark effect on the number of major vessels, but there doesn't seem to be a lot to take from the colour (st_depression).\n\nThe final plot, for me, is one of the most effective. It shows the predictions and influencing factors for many (in this case 50) patients, all together. It's also interactive, which is great. Hover over to see *why* each person ended up either red (prediction of disease) or blue (prediction of no disease),","9e9002e9":"For the categorical varibles, we need to create dummy variables. I'm also going to drop the first category of each. For example, rather than having 'male' and 'female', we'll have 'male' with values of 0 or 1 (1 being male, and 0 therefore being female).","bc2e247e":"<a id='section3'><\/a>","1f562682":"So, it looks like the most important factors in terms of permutation is a thalessemia result of 'reversable defect'. The high importance of 'max heart rate achieved' type makes sense, as this is the immediate, subjective state of the patient at the time of examination (as opposed to, say, age, which is a much more general factor).\n\nLet's take a closer look at the number of major vessles using a **Partial Dependence Plot** (learn more [here](https:\/\/www.kaggle.com\/dansbecker\/partial-plots)). These plots vary a single variable in a single row across a range of values and see what effect it has on the outcome. It does this for several rows and plots the average effect. Let's take a look at the 'num_major_vessels' variable, which was at the top of the permutation importance list,","6cede6f6":"# Introduction\n\nOf all the applications of machine-learning, diagnosing any serious disease using a black box is always going to be a hard sell. If the output from a model is the particular course of treatment (potentially with side-effects), or surgery, or the *absence* of treatment, people are going to want to know **why**.\n\nThis dataset gives a number of variables along with a target condition of having or not having heart disease. Below, the data is first used in a simple random forest model, and then the model is investigated using ML explainability tools and techniques.\n\nLearn more in Dan Becker's course in Kaggle Learn [here](https:\/\/www.kaggle.com\/learn\/machine-learning-explainability)\n\nFirst, load the appropriate libraries,","5944273b":"For this person, their prediction is 70% (compared to a baseline of 58.4%). Not working in their favour are things like having no major vessels, a flat st_slope, and *not* a reversible thalassemia defect.\n\nWe can also plot something called 'SHAP dependence contribution plots' (learn more [here](https:\/\/www.kaggle.com\/dansbecker\/advanced-uses-of-shap-values)), which are pretty self-explanatory in the context of SHAP values,","83cb12d0":"So, we can see that as the number of major blood vessels *increases*, the probability of heart disease *decreases*. That makes sense, as it means more blood can get to the heart.\n\nWhat about the 'age',","5f36f517":"The number of major vessels division is pretty clear, and it's saying that low values are bad (blue on the right). The thalassemia 'reversable defect' division is very clear (yes = red = good, no = blue = bad).\n\nYou can see some clear separation in many of the other variables. Exercise induced angina has a clear separation, although not as expected, as 'no' (blue) *increases* the probability. Another clear one is the st_slope. It looks like when it's flat, that's a bad sign (red on the right).\n\nIt's also odd is that the men (red) have a *reduced* chance of heart disease in this model. Why is this? Domain knowledge tells us that men have a greater chance.","dfea83c7":"Another common metric is the **Area Under the Curve**, or **AUC**. This is a convenient way to capture the performance of a model in a single number, although it's not without certain issues. As a rule of thumb, an AUC can be classed as follows,\n\n- 0.90 - 1.00 = excellent\n- 0.80 - 0.90 = good\n- 0.70 - 0.80 = fair\n- 0.60 - 0.70 = poor\n- 0.50 - 0.60 = fail\n\nLet's see what the above ROC gives us,","e854313a":"# The Explanation\n\nNow let's see what the model gives us from the ML explainability tools.\n\n**Permutation importance** is the first tool for understanding a machine-learning model, and involves shuffling individual variables in the validation data (after a model has been fit), and seeing the effect on accuracy. Learn more [here](https:\/\/www.kaggle.com\/dansbecker\/permutation-importance).\n\nLet's take a look,\n","44d95815":"<img style=\"float: left;\" src=\"https:\/\/guardian.ng\/wp-content\/uploads\/2016\/08\/Heart-diseases.jpg\" width=\"350px\"\/>","d0550663":"# Contents\n\n1. [Introduction ](#section1)\n2. [The Data](#section2)\n3. [The Model](#section3)\n4. [The Explanation](#section4)\n5. [Conclusion](#section5)","798b053b":"Some of those aren't quite right. The code below changes them into categorical variables,","30eef099":"<a id='section5'><\/a>","05875cf8":"# Diagnosing Heart Disease\n## Using ML Explainability Tools and Techniques","88431967":"Now let's see,","bd337cc7":"So, this variable, which is described as 'ST depression induced by exercise relative to rest', seems to suggest the higher the value the higher the probability of heart disease, but the plot above shows the opposite. Perhaps it's not just the depression amount that's important, but the interaction with the slope type? Let's check with a 2D PDP,","6f3945cc":"For this person, their prediction is 36% (compared to a baseline of 58.4%). Many things are working in their favour, including having a major vessel, a reversible thalassemia defect, and *not* having a flat st_slope.\n\nLet's check another,","c5dd3e66":"Looking good. Now, on to the model.","7cdc847e":"Let's change the column names to be a bit clearer,","3f8b9b7f":"Diagnostic tests are often sold, marketed, cited and used with **sensitivity** and **specificity** as the headline metrics. Sensitivity and specificity are defined as,","239475c5":"Let's see what this model is giving,"}}