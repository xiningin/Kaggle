{"cell_type":{"f1484c0b":"code","ace21298":"code","216cc6b5":"code","89d67c0b":"code","7393aa02":"code","e2bba719":"code","03b13c0a":"code","5d04c031":"code","d56ab3c4":"code","4dab5821":"code","33ad29c5":"code","0df4235f":"code","35396cf0":"code","dcc90a81":"code","016af7f8":"code","ab93d853":"code","29efa7d2":"code","07a2768d":"code","4688d6fb":"code","1fa9381f":"code","1e527fcf":"code","52d4a2fd":"code","cc8b5ef3":"code","dbb916a5":"markdown","4cf15e68":"markdown","29bbae42":"markdown","5a0fb64d":"markdown","bda1fc3a":"markdown","fa800ffa":"markdown","e6b93a85":"markdown","f5927b3f":"markdown","310c3bfe":"markdown","d3b29f23":"markdown","49ea2298":"markdown","3c328b22":"markdown"},"source":{"f1484c0b":"import numpy as np \nimport pandas as pd\nimport plotly as py\nimport plotly.graph_objs as go\nimport plotly.express as px\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected = True)\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\nfrom sklearn.model_selection import StratifiedKFold, KFold, LeaveOneGroupOut\n\nimport optuna\n\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\n\npd.set_option('display.max_columns', None)\n#########################################################\ntrain = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/test.csv')\nss = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')","ace21298":"train.head(3)","216cc6b5":"train.info()","89d67c0b":"print(f'NA values in train df: {sum(train.isna().sum())}')\nprint(f'NA values in test df: {sum(test.isna().sum())}')","7393aa02":"for i in [train, test]:\n    i.drop('id', axis = 1, inplace = True)","e2bba719":"train.describe()","03b13c0a":"test.describe()","5d04c031":"fig = plt.figure(figsize = (15, 60))\nfor i in range(len(train.columns.tolist()[:100])):\n    plt.subplot(20,5,i+1)\n    sns.set_style(\"white\")\n    plt.title(train.columns.tolist()[:100][i], size = 12, fontname = 'monospace')\n    a = sns.kdeplot(train[train.columns.tolist()[:100][i]], color = '#34675c', shade = True, alpha = 0.9, linewidth = 1.5, edgecolor = 'black')\n    plt.ylabel('')\n    plt.xlabel('')\n    plt.xticks(fontname = 'monospace')\n    plt.yticks([])\n    for j in ['right', 'left', 'top']:\n        a.spines[j].set_visible(False)\n        a.spines['bottom'].set_linewidth(1.2)\n        \nfig.tight_layout(h_pad = 3)\n\nplt.show()","d56ab3c4":"matrix = np.triu(train.corr())\nplt.figure(figsize = (15, 12))\nsns.heatmap(train.corr(), annot = False, cmap = 'Greens', mask = matrix, vmin = -0.03, vmax = 0.03, linewidths = 0.1, linecolor = 'white', cbar = True)\nplt.xticks(size = 8, fontname = 'monospace')\nplt.yticks(size = 8, fontname = 'monospace')\nplt.figtext(0.77, 0.8, '''All 100 features and the target variable\nhave a very small\ncorrelation''', fontsize = 20, fontname = 'monospace', ha = 'right', color = '#34675c')\nplt.show()","4dab5821":"plt.figure(figsize = (14, 7))\nsns.set_style(\"white\")\nplt.title('Distribution of loss (target)', size = 25, y = 1.03, fontname = 'monospace', color = '#34675c')\nplt.grid(color = 'gray', linestyle = ':', axis = 'x', alpha = 0.8, zorder = 0,  dashes = (1,7))\na = sns.kdeplot(train['loss'], color = '#34675c', shade = True, alpha = 0.9, linewidth = 1.5, edgecolor = 'black')\nplt.ylabel('')\nplt.xlabel('')\nplt.xticks(fontname = 'monospace')\nplt.yticks([])\nfor j in ['right', 'left', 'top']:\n    a.spines[j].set_visible(False)\n    a.spines['bottom'].set_linewidth(1.2)","33ad29c5":"X = train.drop('loss', axis = 1)\ny = train['loss']\n\nsc = StandardScaler()\nX[X.columns.tolist()] = sc.fit_transform(X[X.columns.tolist()])\ntest[test.columns.tolist()] = sc.fit_transform(test[test.columns.tolist()])\n\nX.head(3)","0df4235f":"def objective(trial, data = X, target = y):\n    params = {\n        'depth': trial.suggest_int('depth', 2, 6),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n        'iterations': trial.suggest_int('iterations', 500, 5000),\n        'max_bin': trial.suggest_int('max_bin', 1, 300),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 300),\n        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.0001, 1.0, log = True),\n        'subsample': trial.suggest_float('subsample', 0.1, 0.8),\n        'grow_policy': trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide']),\n        'leaf_estimation_method': trial.suggest_categorical('leaf_estimation_method', ['Newton', 'Gradient']),\n        'random_seed': 228,\n        'loss_function': 'RMSE',\n        'eval_metric': 'RMSE',\n        'bootstrap_type': 'Bernoulli',\n        'task_type': 'GPU'\n    }\n    \n    model = CatBoostRegressor(**params)\n    scores = []\n    k = KFold(n_splits = 5, random_state = 228, shuffle = True)\n    for i, (trn_idx, val_idx) in enumerate(k.split(X)):\n        \n        X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n        model.fit(X_train, y_train, eval_set = [(X_val, y_val)], early_stopping_rounds = 150, verbose = False, use_best_model = True)\n        \n        tr_preds = model.predict(X_train)\n        tr_score = np.sqrt(mean_squared_error(y_train, tr_preds))\n        \n        val_preds = model.predict(X_val)\n        val_score = np.sqrt(mean_squared_error(y_val, val_preds))\n\n        scores.append((tr_score, val_score))\n        \n        print(f\"Fold {i+1} | RMSE: {val_score}\")\n        \n        \n    scores = pd.DataFrame(scores, columns = ['train score', 'validation score'])\n    \n    return scores['validation score'].mean()\n\nstudy = optuna.create_study(direction = 'minimize')\nstudy.optimize(objective, n_trials = 100)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint('Best value:', study.best_value)","35396cf0":"# Mean RMSE on 5 folds - 7.8448\nparamsCB = {'depth': 5, \n            'learning_rate': 0.04278413956100119, \n            'iterations': 2116, \n            'max_bin': 238, \n            'min_data_in_leaf': 251, \n            'l2_leaf_reg': 0.004676455789227335, \n            'subsample': 0.3773307810571105, \n            'grow_policy': 'Depthwise', \n            'leaf_estimation_method': 'Newton',\n            'random_seed': 228,\n            'loss_function': 'RMSE',\n            'eval_metric': 'RMSE',\n            'bootstrap_type': 'Bernoulli',\n            'task_type': 'GPU'}","dcc90a81":"folds = KFold(n_splits = 5, random_state = 228, shuffle = True)\n\npredictions = np.zeros(len(test))\n\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X)):\n    \n    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n    model = CatBoostRegressor(**paramsCB)\n   \n    model.fit(X_train, y_train, eval_set = [(X_val, y_val)], verbose = False, early_stopping_rounds = 150, use_best_model = True)\n    \n    predictions += model.predict(test) \/ folds.n_splits \n    \nss['loss'] = predictions","016af7f8":"ss.to_csv('cb.csv', index = False)","ab93d853":"def objective(trial, data = X, target = y):\n\n    params = {\n        'max_depth': trial.suggest_int('max_depth', 2, 8),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n        'n_estimators': trial.suggest_int('n_estimators', 500, 10000),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 200),\n        'gamma': trial.suggest_float('gamma', 0.0001, 1.0, log = True),\n        'alpha': trial.suggest_float('alpha', 0.0001, 10.0, log = True),\n        'lambda': trial.suggest_float('lambda', 0.0001, 10.0, log = True),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.8),\n        'subsample': trial.suggest_float('subsample', 0.1, 0.8),\n        'tree_method': 'gpu_hist',\n        'booster': 'gbtree',\n        'random_state': 228,\n        'use_label_encoder': False,\n        'eval_metric': 'rmse'\n    }\n    \n    model = XGBRegressor(**params)\n    scores = []\n    k = KFold(n_splits = 5, random_state = 228, shuffle = True)\n    for i, (trn_idx, val_idx) in enumerate(k.split(X)):\n        \n        X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n        model.fit(X_train, y_train, eval_set = [(X_val, y_val)], early_stopping_rounds = 150, verbose = False)\n        \n        tr_preds = model.predict(X_train)\n        tr_score = np.sqrt(mean_squared_error(y_train, tr_preds))\n        \n        val_preds = model.predict(X_val)\n        val_score = np.sqrt(mean_squared_error(y_val, val_preds))\n\n        scores.append((tr_score, val_score))\n        \n        print(f\"Fold {i+1} | RMSE: {val_score}\")\n        \n        \n    scores = pd.DataFrame(scores, columns = ['train score', 'validation score'])\n    \n    return scores['validation score'].mean()\n\nstudy = optuna.create_study(direction = 'minimize')\nstudy.optimize(objective, n_trials = 100)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint('Best value:', study.best_value)","29efa7d2":"# Mean RMSE on 5 folds - 7.8462\nparamsXGB = {'max_depth': 2, \n             'learning_rate': 0.010707053059983151, \n             'n_estimators': 9688, \n             'min_child_weight': 168, \n             'gamma': 0.0006130691869231192, \n             'alpha': 0.0015540336440723174, \n             'lambda': 0.012133281664909838, \n             'colsample_bytree': 0.5945187331960007, \n             'subsample': 0.3432887319679862,\n             'tree_method': 'gpu_hist',\n             'booster': 'gbtree',\n             'random_state': 228,\n             'use_label_encoder': False,\n             'eval_metric': 'rmse'}","07a2768d":"predictions = np.zeros(len(test))\n\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X)):\n    \n    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n    model = XGBRegressor(**paramsXGB)\n   \n    model.fit(X_train, y_train, eval_set = [(X_val, y_val)], verbose = False, early_stopping_rounds = 150)\n    \n    predictions += model.predict(test) \/ folds.n_splits \n    \nss['loss'] = predictions","4688d6fb":"ss.to_csv('xgb.csv', index = False)","1fa9381f":"def objective(trial, data = X, target = y):\n\n    params = {\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 10.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 10, 500),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'max_depth': trial.suggest_int('max_depth', 2, 8),\n        'n_estimators': trial.suggest_int('n_estimators', 500, 10000),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.6),\n        'cat_smooth' : trial.suggest_int('cat_smooth', 10, 100),\n        'cat_l2': trial.suggest_int('cat_l2', 1, 20),\n        'min_data_per_group': trial.suggest_int('min_data_per_group', 1, 200),\n        'device_type': 'gpu',\n        'boosting_type': 'gbdt',\n        'random_state': 228,\n        'metric': 'rmse'\n    }\n    \n    model = LGBMRegressor(**params)\n    scores = []\n    k = KFold(n_splits = 5, random_state = 228, shuffle = True)\n    for i, (trn_idx, val_idx) in enumerate(k.split(X)):\n        \n        X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n        model.fit(X_train, y_train, eval_set = [(X_val, y_val)], early_stopping_rounds = 150, verbose = False)\n        \n        tr_preds = model.predict(X_train)\n        tr_score = np.sqrt(mean_squared_error(y_train, tr_preds))\n        \n        val_preds = model.predict(X_val)\n        val_score = np.sqrt(mean_squared_error(y_val, val_preds))\n\n        scores.append((tr_score, val_score))\n        \n        print(f\"Fold {i+1} | RMSE: {val_score}\")\n        \n        \n    scores = pd.DataFrame(scores, columns = ['train score', 'validation score'])\n    \n    return scores['validation score'].mean()\n\nstudy = optuna.create_study(direction = 'minimize')\nstudy.optimize(objective, n_trials = 100)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint('Best value:', study.best_value)","1e527fcf":"# Mean RMSE on 5 folds - 7.8405\nparamsLGBM = {'reg_alpha': 8.682795832798263, \n              'reg_lambda': 8.688528314713478, \n              'num_leaves': 35, \n              'min_child_samples': 5, \n              'max_depth': 8, \n              'n_estimators': 4461, \n              'learning_rate': 0.010109446255049337, \n              'colsample_bytree': 0.104662962036166, \n              'cat_smooth': 56, \n              'cat_l2': 13, \n              'min_data_per_group': 5,\n              'device_type': 'gpu',\n              'boosting_type': 'gbdt',\n              'random_state': 228,\n              'metric': 'rmse'\n              }","52d4a2fd":"predictions = np.zeros(len(test))\n\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X)):\n    \n    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n    model = LGBMRegressor(**paramsLGBM)\n   \n    model.fit(X_train, y_train, eval_set = [(X_val, y_val)], verbose = False, early_stopping_rounds = 150)\n    \n    predictions += model.predict(test) \/ folds.n_splits \n    \nss['loss'] = predictions","cc8b5ef3":"ss.to_csv('lgbm.csv', index = False)","dbb916a5":"**The distributions of all 100 features are almost the same.**","4cf15e68":"# LGBM","29bbae42":"**Interesting. When I first started studying machine learning, I often read about how powerful XGB is, but for the second competition in a row, XGB shows the worst result.**","5a0fb64d":"# CatBoost","bda1fc3a":"**Beautiful result - 7.88000**","fa800ffa":"# XGB","e6b93a85":"# Preproceesing for modeling","f5927b3f":"# EDA","310c3bfe":"# Conclusion","d3b29f23":"**Result - 7.88071**","49ea2298":"# Basic information","3c328b22":"**Result - 7.88625**"}}