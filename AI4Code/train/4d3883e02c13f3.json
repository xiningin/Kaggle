{"cell_type":{"a748bc99":"code","81ae22d4":"code","d18c4628":"code","9c2607ab":"code","df907362":"code","997a9d8a":"code","b8cf28b1":"code","59e54d88":"code","2c32104c":"code","7cabfd8a":"code","5f3511f4":"code","c238b9d2":"code","d5dbf30a":"code","2f946a16":"code","8428484c":"code","17f00906":"code","9a484075":"code","ca021ebc":"code","ddfa575f":"code","2c5d195a":"code","2de30d69":"code","2e3aed1d":"code","5e54118e":"code","ce5b74eb":"code","40fc17ef":"code","3d627a99":"code","8d3d93d4":"code","32583015":"markdown","5dd9895f":"markdown","0776a703":"markdown","5d9c12da":"markdown","14b375a5":"markdown","fa96570c":"markdown","765df29e":"markdown","b2258c30":"markdown","13cb62a5":"markdown","378c41f6":"markdown","6da4d03f":"markdown","4c71c5a3":"markdown","12cf24bc":"markdown","2993b383":"markdown","d4df2555":"markdown","b11cfdc5":"markdown","a37f7715":"markdown","1ec8727a":"markdown","d40bbd05":"markdown","4a2330ee":"markdown","1fa5817f":"markdown","4eee9091":"markdown","480943f8":"markdown","0977364d":"markdown","492be825":"markdown","04f5744d":"markdown","3124f1f5":"markdown","8fc1c64a":"markdown"},"source":{"a748bc99":"import pandas as pd\nraw = pd.read_csv('..\/input\/language-identification-datasst\/dataset.csv')\nraw","81ae22d4":"# Languages\nlanguages = set(raw['language'])\nprint('Languages', languages)\nprint('========')\n# Examples of multiple langs taken from heads and tails\nprint('Swedish & English:', raw['Text'][1])\nprint('Thai & English:', raw['Text'][2])\nprint('Chinese & English:', raw['Text'][21998])","d18c4628":"import numpy as np\nfrom sklearn.model_selection import train_test_split\n\nX=raw['Text']\ny=raw['language']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(len(X_train))\nprint(len(X_test))\nprint(len(y_train))\nprint(len(y_test))","9c2607ab":"# Extract Unigrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nunigramVectorizer = CountVectorizer(analyzer='char', ngram_range=(1,1))\nX_unigram_train_raw = unigramVectorizer.fit_transform(X_train)\nX_unigram_test_raw = unigramVectorizer.transform(X_test)\n\n\nunigramFeatures = unigramVectorizer.get_feature_names()\n\nprint('Number of unigrams in training set:', len(unigramFeatures))","df907362":"# Aggregate Unigrams per language\ndef train_lang_dict(X_raw_counts, y_train):\n    lang_dict = {}\n    for i in range(len(y_train)):\n        lang = y_train[i]\n        v = np.array(X_raw_counts[i])\n        if not lang in lang_dict:\n            lang_dict[lang] = v\n        else:\n            lang_dict[lang] += v\n            \n    # to relative\n    for lang in lang_dict:\n        v = lang_dict[lang]\n        lang_dict[lang] = v \/ np.sum(v)\n        \n    return lang_dict\n\nlanguage_dict_unigram = train_lang_dict(X_unigram_train_raw.toarray(), y_train.values)\n\n# Collect relevant chars per language\ndef getRelevantCharsPerLanguage(features, language_dict, significance=1e-5):\n    relevantCharsPerLanguage = {}\n    for lang in languages:\n        chars = []\n        relevantCharsPerLanguage[lang] = chars\n        v = language_dict[lang]\n        for i in range(len(v)):\n            if v[i] > significance:\n                chars.append(features[i])\n    return relevantCharsPerLanguage\n\nrelevantCharsPerLanguage = getRelevantCharsPerLanguage(unigramFeatures, language_dict_unigram)\n    \n# Print number of unigrams per language\nfor lang in languages:    \n    print(lang, len(relevantCharsPerLanguage[lang]))","997a9d8a":"# get most common chars for a few European languages\neuropeanLanguages = ['Portugese', 'Spanish', 'Latin', 'English', 'Dutch', 'Swedish']\nrelevantChars_OnePercent = getRelevantCharsPerLanguage(unigramFeatures, language_dict_unigram, 1e-2)\n\n# collect and sort chars\neuropeanCharacters = []\nfor lang in europeanLanguages:\n    europeanCharacters += relevantChars_OnePercent[lang]\neuropeanCharacters = list(set(europeanCharacters))\neuropeanCharacters.sort()\n\n# build data\nindices = [unigramFeatures.index(f) for f in europeanCharacters]\ndata = []\nfor lang in europeanLanguages:\n    data.append(language_dict_unigram[lang][indices])\n\n#build dataframe\ndf = pd.DataFrame(np.array(data).T, columns=europeanLanguages, index=europeanCharacters)\ndf.index.name = 'Characters'\ndf.columns.name = 'Languages'\n\n# plot heatmap\nimport seaborn as sn\nimport matplotlib.pyplot as plt\nsn.set(font_scale=0.8) # for label size\nsn.set(rc={'figure.figsize':(10, 10)})\nsn.heatmap(df, cmap=\"Greens\", annot=True, annot_kws={\"size\": 12}, fmt='.0%')# font size\nplt.show()","b8cf28b1":"# number of bigrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nbigramVectorizer = CountVectorizer(analyzer='char', ngram_range=(2,2))\nX_bigram_raw = bigramVectorizer.fit_transform(X_train)\nbigramFeatures = bigramVectorizer.get_feature_names()\nprint('Number of bigrams', len(bigramFeatures))","59e54d88":"# top bigrams (>1%) for Spanish, Italian (Latin), English, Dutch, Chinese, Japanese, Korean\nlanguage_dict_bigram = train_lang_dict(X_bigram_raw.toarray(), y_train.values)\nrelevantCharsPerLanguage = getRelevantCharsPerLanguage(bigramFeatures, language_dict_bigram, significance=1e-2)\nprint('Spanish', relevantCharsPerLanguage['Spanish'])\nprint('Italian (Latin)', relevantCharsPerLanguage['Latin'])\nprint('English', relevantCharsPerLanguage['English'])\nprint('Dutch', relevantCharsPerLanguage['Dutch'])\nprint('Chinese', relevantCharsPerLanguage['Chinese'])\nprint('Japanese', relevantCharsPerLanguage['Japanese'])","2c32104c":"# Uni- & Bi-Gram Mixture CountVectorizer for top 1% features\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ntop1PrecentMixtureVectorizer = CountVectorizer(analyzer='char', ngram_range=(1,2), min_df=1e-2)\nX_top1Percent_train_raw = top1PrecentMixtureVectorizer.fit_transform(X_train)\nX_top1Percent_test_raw = top1PrecentMixtureVectorizer.transform(X_test)\n\nlanguage_dict_top1Percent = train_lang_dict(X_top1Percent_train_raw.toarray(), y_train.values)\n\ntop1PercentFeatures = top1PrecentMixtureVectorizer.get_feature_names()\nprint('Length of features', len(top1PercentFeatures))\nprint('')\n\n#Unique features per language\nrelevantChars_Top1Percent = getRelevantCharsPerLanguage(top1PercentFeatures, language_dict_top1Percent, 1e-5)\nfor lang in relevantChars_Top1Percent:\n    print(\"{}: {}\".format(lang, len(relevantChars_Top1Percent[lang])))","7cabfd8a":"def getRelevantGramsPerLanguage(features, language_dict, top=50):\n    relevantGramsPerLanguage = {}\n    for lang in languages:\n        chars = []\n        relevantGramsPerLanguage[lang] = chars\n        v = language_dict[lang]\n        sortIndex = (-v).argsort()[:top]\n        for i in range(len(sortIndex)):\n            chars.append(features[sortIndex[i]])\n    return relevantGramsPerLanguage\n\ntop50PerLanguage_dict = getRelevantGramsPerLanguage(top1PercentFeatures, language_dict_top1Percent)\n\n# top50\nallTop50 = []\nfor lang in top50PerLanguage_dict:\n    allTop50 += set(top50PerLanguage_dict[lang])\n\ntop50 = list(set(allTop50))\n    \nprint('All items:', len(allTop50))\nprint('Unique items:', len(top50))","5f3511f4":"# getRelevantColumnIndices\ndef getRelevantColumnIndices(allFeatures, selectedFeatures):\n    relevantColumns = []\n    for feature in selectedFeatures:\n        relevantColumns = np.append(relevantColumns, np.where(allFeatures==feature))\n    return relevantColumns.astype(int)\n\nrelevantColumnIndices = getRelevantColumnIndices(np.array(top1PercentFeatures), top50)\n\n\nX_top50_train_raw = np.array(X_top1Percent_train_raw.toarray()[:,relevantColumnIndices])\nX_top50_test_raw = X_top1Percent_test_raw.toarray()[:,relevantColumnIndices] \n\nprint('train shape', X_top50_train_raw.shape)\nprint('test shape', X_top50_test_raw.shape)","c238b9d2":"# Define some functions for our purpose\n\nfrom sklearn.preprocessing import normalize\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import confusion_matrix, f1_score\nimport seaborn as sn\nimport matplotlib.pyplot as plt\nimport scipy\n\n# Utils for conversion of different sources into numpy array\ndef toNumpyArray(data):\n    data_type = type(data)\n    if data_type == np.ndarray:\n        return data\n    elif data_type == list:\n        return np.array(data_type)\n    elif data_type == scipy.sparse.csr.csr_matrix:\n        return data.toarray()\n    print(data_type)\n    return None\n\n\ndef normalizeData(train, test):\n    train_result = normalize(train, norm='l2', axis=1, copy=True, return_norm=False)\n    test_result = normalize(test, norm='l2', axis=1, copy=True, return_norm=False)\n    return train_result, test_result\n\ndef applyNaiveBayes(X_train, y_train, X_test):\n    trainArray = toNumpyArray(X_train)\n    testArray = toNumpyArray(X_test)\n    \n    clf = MultinomialNB()\n    clf.fit(trainArray, y_train)\n    y_predict = clf.predict(testArray)\n    return y_predict\n\ndef plot_F_Scores(y_test, y_predict):\n    f1_micro = f1_score(y_test, y_predict, average='micro')\n    f1_macro = f1_score(y_test, y_predict, average='macro')\n    f1_weighted = f1_score(y_test, y_predict, average='weighted')\n    print(\"F1: {} (micro), {} (macro), {} (weighted)\".format(f1_micro, f1_macro, f1_weighted))\n\ndef plot_Confusion_Matrix(y_test, y_predict, color=\"Blues\"):\n    allLabels = list(set(list(y_test) + list(y_predict)))\n    allLabels.sort()\n    confusionMatrix = confusion_matrix(y_test, y_predict, labels=allLabels)\n    unqiueLabel = np.unique(allLabels)\n    df_cm = pd.DataFrame(confusionMatrix, columns=unqiueLabel, index=unqiueLabel)\n    df_cm.index.name = 'Actual'\n    df_cm.columns.name = 'Predicted'\n\n    sn.set(font_scale=0.8) # for label size\n    sn.set(rc={'figure.figsize':(15, 15)})\n    sn.heatmap(df_cm, cmap=color, annot=True, annot_kws={\"size\": 12}, fmt='g')# font size\n    plt.show()\n\n","d5dbf30a":"# Unigrams\nX_unigram_train, X_unigram_test = normalizeData(X_unigram_train_raw, X_unigram_test_raw)\ny_predict_nb_unigram = applyNaiveBayes(X_unigram_train, y_train, X_unigram_test)\nplot_F_Scores(y_test, y_predict_nb_unigram)\nplot_Confusion_Matrix(y_test, y_predict_nb_unigram, \"Oranges\")","2f946a16":"# Top 1%\nX_top1Percent_train, X_top1Percent_test = normalizeData(X_top1Percent_train_raw, X_top1Percent_test_raw)\ny_predict_nb_top1Percent = applyNaiveBayes(X_top1Percent_train, y_train, X_top1Percent_test)\nplot_F_Scores(y_test, y_predict_nb_top1Percent)\nplot_Confusion_Matrix(y_test, y_predict_nb_top1Percent, \"Reds\")","8428484c":"# Top 50\nX_top50_train, X_top50_test = normalizeData(X_top50_train_raw, X_top50_test_raw)\ny_predict_nb_top50 = applyNaiveBayes(X_top50_train, y_train, X_top50_test)\nplot_F_Scores(y_test, y_predict_nb_top50)\nplot_Confusion_Matrix(y_test, y_predict_nb_top50, \"Greens\")","17f00906":"from sklearn.neighbors import KNeighborsClassifier\n\ndef applyNearestNeighbour(X_train, y_train, X_test):\n    trainArray = toNumpyArray(X_train)\n    testArray = toNumpyArray(X_test)\n    \n    clf = KNeighborsClassifier()\n    clf.fit(trainArray, y_train)\n    y_predict = clf.predict(testArray)\n    return y_predict\n\n\n## Unigrams\n#y_predict_knn_unigram = applyNearestNeighbour(X_unigram_train, y_train, X_unigram_test)\n#plot_F_Scores(y_test, y_predict_knn_unigram)\n#plot_Confusion_Matrix(y_test, y_predict_knn_unigram, \"Purples\")\n\n# Top 50\ny_predict_knn_top50 = applyNearestNeighbour(X_top50_train, y_train, X_top50_test)\nplot_F_Scores(y_test, y_predict_knn_top50)\nplot_Confusion_Matrix(y_test, y_predict_knn_top50, \"Blues\")","9a484075":"# Train lang dict\ndef toRelative(X_test):\n    return [v \/ np.sum(v) for v in X_test]\n\nlanguage_dict_top50 = train_lang_dict(X_top50_train, y_train.values)\nX_top50_test_rel = toRelative(X_top50_test)","ca021ebc":"def ols_predict(language_dict, X_test):\n    def calcSquareDifference(p, q):\n        return np.sum((p-q)**2)\n    \n    def ols(language_dict, v, langs):\n        olsVector = np.array([calcSquareDifference([language_dict[l]], v) for l in langs])\n        index = np.argmin(olsVector)\n        return langs[index]\n    \n    langs = [l for l in language_dict]\n    return [ols(language_dict, v, langs) for v in X_test]\n\n\nols_predictions = ols_predict(language_dict_top50, X_top50_test_rel)\n\nplot_F_Scores(y_test, ols_predictions)\nplot_Confusion_Matrix(y_test, ols_predictions, 'cool')","ddfa575f":"# Kullback Leibler Divergence\nfrom math import log\n\n\ndef kl_predict(language_dict, X_test):\n\n    def kl_divergence(p, q):\n        p_ = np.array(p) + 1e-200\n        q_ = np.array(q) + 1e-200\n        n = len(p)\n        return np.sum([p_[i] * log(p_[i]\/ (q_[i] )) for i in range(n)])\n    \n    def predict(language_dict, v, langs):\n        divs = [kl_divergence(language_dict[l], v) for l in langs]\n        index = np.argmin(divs)\n        return langs[index]\n    \n    langs = [l for l in language_dict]\n    return [predict(language_dict, v, langs) for v in X_test]\n\n\nkl_predictions = kl_predict(language_dict_top50, X_top50_test_rel)\n\nplot_F_Scores(y_test, kl_predictions)\nplot_Confusion_Matrix(y_test, kl_predictions, 'plasma')","2c5d195a":"# Calculate average number of chars per Text\navgChars = np.mean(raw.apply(lambda x : len(x['Text']), axis=1))\nprint('avgChars', avgChars)","2de30d69":"def ks_predict(language_dict, X_test, n, c_alpha=1.628):\n    def calcMaxAbsDifference(p, q):\n        return np.max(np.abs((p-q)))\n    \n    def scaleAlpha(n, m, c_alpha):\n        factor = ((n + m) \/ n \/ m) ** 0.5\n        return factor * c_alpha\n    \n    def ks(language_dict, v, langs):\n        ksVector = np.array([calcMaxAbsDifference([language_dict[l]], v) for l in langs])\n        index = np.argmin(ksVector)\n        m = len(v)\n        scaledAlpha = scaleAlpha(n, m, c_alpha)\n        if ksVector[index] <= scaledAlpha:\n            return langs[index]\n        else:\n            return '_N\/A_'\n    \n    langs = [l for l in language_dict]\n    return [ks(language_dict, v, langs) for v in X_test]\n\n\nks_predictions = ks_predict(language_dict_top50, X_top50_test_rel, 356*800)\nprint('None-Predicitons:', (np.array(ks_predictions)=='_N\/A_').sum())\nplot_F_Scores(y_test, ks_predictions)\nplot_Confusion_Matrix(y_test, ks_predictions, 'summer')","2e3aed1d":"! pip install langdetect\n! pip install iso-639\n\nfrom langdetect import detect\nfrom iso639 import languages\n\ndef proofLanguage(text):\n    twoLetterCode = detect(text)[:2] #consolidate Chinese \n    if twoLetterCode == 'it': # Italian -> Latin\n        return 'Latin'\n    elif twoLetterCode == 'pt': # Portuguese -> Portugese\n        return 'Portugese'\n    else:\n        lang = languages.get(alpha2=twoLetterCode)\n        langName = lang.name\n        if not langName==None:\n            return langName\n        return twoLetterCode\n\ny_test_proof = [proofLanguage(t) for t in X_test]","5e54118e":"plot_F_Scores(y_test, y_test_proof)\nplot_Confusion_Matrix(y_test, y_test_proof, 'cool')","ce5b74eb":"def plotTopErrors(y_predict, top=5):\n    ys = y_test.values\n    Xs = X_test.values\n    errorCount = 0\n    \n    for i in range(len(ys)):\n        if not ys[i]==y_predict[i]:\n            errorCount += 1\n            print(\"#{}: Expected: {}, Predicted: {}\".format(errorCount, ys[i], y_predict[i]))\n            print(\"Text:\", Xs[i])\n            print(\"=================================================\")\n        if errorCount >= top:\n            break","40fc17ef":"plotTopErrors(y_predict_nb_top50, top=5)","3d627a99":"plotTopErrors(y_predict_nb_unigram, top=5)","8d3d93d4":"plotTopErrors(ks_predictions, top=5)","32583015":"A score of 92% for F1 (weighted) is not too bad. Anyhow it is not sufficient and we will do better.\n\nAnd as suggested, it is pretty hard to distinguish between languages of the same familiy.\n\nHere our major problem is to distinguish between Dutch, English and Swedish.\n\n## Top 1% Mixture\n\nIn the case of top 1% 1- & 2-Grams we are working on 3,079 features.","5dd9895f":"Well, if you inspect the data yourself, you will find plenty more examples containing two languages.\n\nNow, let's do, what we always need to do when applying ML somewhere: *Split the data into train and test.*","0776a703":"Alternatively, we could also use a **mixture** of Uni-Grams and Bi-Grams, restricted on the most frequently used ones.\n\n## Mixture of Uni-Gram & Bi-Grams\nWhen we restrict ourselves to a limited number of features, it is important, that we will capture details for each language. Since Chinese consists of >3,000 of different symbols, the probability of the most frequently used Chinese Uni-Grams might be below the top 1000 used Bi-Grams of the other languages.\n\nIn the following, I will try some arbitrarily chosen Mixtures. They are straight-forward ideas, based on intuition only. \n\nFor a more analytic approach, one should use a dimension reduction technique or something similar.\n\n### Mixture Uni- & Bi-Grams (using the top 1%)\nSo, first try, take Uni- & Bi-Grams occurring at least in 1% of all cases.","5d9c12da":"Now, lets start with the error inspection.\n\n### Naive Bayes on Mix Top 50","14b375a5":"As we can see in the above overview, following languages are using a lot of unique symbols:\n- Chinese:  3,249\n- Japanese: 2,054\n- Korean:   1,407\n\nI.e. we can easily identify these languages by using Uni-Grams.\n\nBut:\n- All other languages are using **much fewer symbols**. \n- And most of the other languages **share common symbols**.\n\nWell, the Uni-Gram-approach might also be usefull, if languages share *common symbols*, like the [Latin symbols](https:\/\/en.wikipedia.org\/wiki\/Latin_alphabet) which are used in a lot of [Indo-European](https:\/\/en.wikipedia.org\/wiki\/Indo-European_languages) languages including e.g. [Romance](https:\/\/en.wikipedia.org\/wiki\/Romance_languages) (Italian, Romanian, Spanish, ...) and [Germanic](https:\/\/en.wikipedia.org\/wiki\/Germanic_languages) (English, German, Dutch, ...). And in a similar way also for [Kyrillic letters](https:\/\/en.wikipedia.org\/wiki\/Cyrillic_alphabets) used for Russian, Serbian, ... \n\nIn this cases we need to use the distribution of the characters, which will allow us to distinguish between the root languages. So we will be able to distinguish between a language based on Romance (e.g. Italian) or a Germanic (e.g. Dutch).\n\nUnfortunately, using **Uni-Grams** will not be sufficient in all cases. We might face some issues, when we try to distinguish between languages having the same root, like for the Romance languages: Italian, Spanish, French, Protuguese, Catalan, ... Or for the Germanic languages: English, Dutch, Swedish, ...\n\nSo, let's have a look on some European languages, sharing Latin symbols","fa96570c":"**So, when dealing with the top-50-approach on these 22 languages, we will effectively use 564 features only.**\n\nNow, let's build the data set for the models, based on our 564 features.","765df29e":"Well, there are errors on both sides. But as far as I can see, the majority of the predicted values is correct. \n\nSo far I would suggest, that the real F1 is better than 97%. \n\nAnyhow, I need to proof my suggestion in the future.\n\n### Naive Bayes on Uni-Grams","b2258c30":"Well, it seems that a lot of the predicted values are wrong for KS.","13cb62a5":"## Comparison Ranking by F1 weighted\n1. 0.9751510056992273 **NB** on **Mix Top 50**\n1. 0.9746025107937131 **NB** on **Mix Top 1%**\n1. 0.9723506120636130 **kNN** on **Mix Top 50**\n1. 0.9645429434449447 **KL** on **Mix Top 50**\n1. 0.9495364489332023 **OLS** on **Mix Top 50**\n1. 0.9201996483569281 **NB** on **Unigrams**\n1. 0.8789520584453827 **langdetect**\n1. 0.8025836015759978 **KS** on **Mix Top 50**","378c41f6":"Before we start to use the features in our models, let's finally discuss the feature selection.\n\n## n-Gram Discussion\n\nLet's recap the above:\n- using Uni-Grams is sufficient for syllable languages (Chinese, Japanese, ...)\n- but Uni-Grams do not help when inspecting European languages - here we need at least some Bi-Grams\n- using all Bi-Grams will blow up the memory required \n- the same is true for **Tri-Grams** - so, we will do *no* further investigation on that here\n\nConclusion: **From a theoretical perspective, it is most efficient to use a Mixture of the most common Uni-Grams and Bi-Grams**.\n\n***\n\n# Naive Bayes\nNow, we can apply the Naive Bayes on our different feature sets:\n- Unigram (`X_unigram_train_raw`)\n- Mixture Top 1% (`X_top1Percent_train_raw`)\n- Mixture Top 50 (`X_top50_train_raw`)\n\nFirst of all, we need to define some utility functions for our evaluation.","6da4d03f":"## Error Analysis\nLets have a brief look on the error details for some selected scenarios.\n\nFirst we define a helper method `plotTopErrors`:","4c71c5a3":"F1 (weighted) of 97.46% is much better. Again we have some issues with European languages here.\n\nUnfortunately, we are still working on a lot of features.\n\nLet's see, if the Mixture of the Top 50 will perform better.\n\n## Top 50 Mixture\nWe are using **564** features only.","12cf24bc":"## Uni-Grams\n\nRemember, we are working on **6,816** features in the case of Uni-Grams.","2993b383":"Using Uni-Grams, does result in the initially expected errors. \n\nWell, it is very hard to distinguish between similar languages. \n\n### Kolmogorov Smirnov on Mix Top 50","d4df2555":"Looking at the data above shows, that it is very hard to distinguish between the European languages. Especially between the languages based on Romance.\n\nAnyhow, it is also very hard to distinguish between English and Italian (labeled *Latin* in the table above), since there are only a few narrow differences in the distribution.\n\nWe will inspect the performance (in terms of the F1) score in the later chapters.\n\n********\n\n## Bi-Gram\nThe next larger piece of information are **Bi-Grams**. Unfortunately, using Bi-Grams might already blow up the required resources. So we should restrict ourselves here, to use only those, \noccurring most frequently. \n\nIf we restrict ourselves to the most frequently n-Grams only, we will face another challenge. For languages containing a large amount of symbols, all combinations of symbols do not occure very often (e.g. Chinese contains >3,000 symbols). This seems to be a common pattern for languages based on syllables.\n","b11cfdc5":"# Preface\nFirst of all, **thanks a lot for this fantastic dataset!**\n\nIt is very interesting, as it contains a lot of texts. Most of the rows consist of *two languages* - a **primary** one for the major part and a **secondary** one used for some words only. \n\nAccording to this observation, we already have a lot of noise in our data (i.e. all words in the secondary language). \n\nA part from that, we are *not* dealing with long texts here, all texts are relatively short.\n\nSo, we are not working on a plain vanilla problem using the 10 best text books available for each of the investigated languages, but on a *realistic scenario* using kind of real world data. \n\n# Pupose\nThis notebook, should show alternative ways, how to detect the language. \n\nThe focus lies on the identification of efficient algorithms (in terms of memory requirements) by exploiting the knowledge about languages and their structures.\n\n# Agenda\n1. Get familiar with the data\n2. Language specifics *aka Uni- vs. Bi-Grams*\n3. Naive Bayes\n4. k Nearest Neighbours\n5. Sum of squared differences\n6. Kulback Leibler Divergence\n7. Kolmogorov Smirnov Test\n8. Analysis & Comparison of F1-Scores\n\n# Get familiar with the data","a37f7715":"In addition to the problems observed for OLS and KL we are facing a lot of more problems here.\n\n\n# Comparison of results\n\nBefore we finally compare all results, we start with the implementation of a benchmark using an existing library. And have a quick look on the mismatched data.\n\n\n## Benchmark library `langdetect`\n\nUnfortunately, we need to add some fix-up code:\n- the [output](https:\/\/pypi.org\/project\/langdetect\/) of `langdetect` is in iso-639 format (except for Chinese).\n- Italian is called Latin in our dataset\n- Portuguese is named Portugese in our dataset","1ec8727a":"Well, the result for F1 (weighted) of 97.52% is somehow better, but not significantly. \n\nAnyway we are using much fewer features. Therefore I will prefer this approach.\n\n## Comparison\nFor Naive Bayes we achieve the following scores for F1 (weighted):\n- Unigram: 0.9201996483569281 (using 6,816 features)\n- Top 1% Mixture: 0.9746025107937131 (3,079 features)\n- Top 50 Mixture: 0.9751510056992273 (564 features)\n\nNow let's see, how k-Nearest-Neighbours performs.\n\n# k Nearest Neighbour\nFirst, we need to define the apply method and to discuss the size of k.\n\nThe default for k=5. Let's stick to that default.\n\nI will run kNN on the Top-50-feature-set only. (If you like to run it on the Uni-Grams please uncomment below.)","d40bbd05":"`language_dict_top50` contains the relative distribution per language for the Top-50-feature-set. ($p_l(i)$ for all languages $l$)\n\n`X_top50_test_rel` contains (distribution-like) relative values for the test data, based on the Top-50-feature-set. ($q(i)$ for all samples)\n\n# (Ordinary) least squares\nIdentify the language by taking the minimum of the sum of the squared differnces per feature (=character\/symbol), i.e.\n\n$$ \\delta_l = \\sum _i (p_l(i)-q(i))^2 $$\n$$ l^* = argmin_l(\\delta_l) $$","4a2330ee":"It seems, that OLS results in more differences to the labelled data than Naive Bayes and k-Nearest-Neighbour.\n\nDifferences are:\n- Latin -> predicted as English (seems to be a common error also for Naive Bayes and kNN) \n- Urdu -> predicted as Arabic\n- Chinese -> English\n- Spanish -> Portuguese\n\n\n# Kullback Leibler Divergence\nTaking the smallest value of [Kullback Leibler Divergence](https:\/\/en.wikipedia.org\/wiki\/Kullback%E2%80%93Leibler_divergence) as a measure for the difference between distributions, given by:\n$$D_\\text{KL}(p \\parallel q) = \\sum_{x\\in\\mathcal{X}} p(x) \\log\\left(\\frac{p(x)}{q(x)}\\right)$$","1fa5817f":"# Language specifics\n\nTo be memory efficient, we need to restrict ourselves to a minimum number of features, which is still sufficient to distinguish between the languages.\n\n## Excurs on Cryptographie\nWhen thinking about a solution, the first thing, which came into my mind, was the lecture on Crypto-Analysis. Breaking a [Vigen\u00e8re-](https:\/\/en.wikipedia.org\/wiki\/Vigen%C3%A8re_cipher) or [Ceasar-Cipher](https:\/\/en.wikipedia.org\/wiki\/Caesar_cipher) is performed by exploiting the statistical distribution of characters using [Frequency analysis](https:\/\/en.wikipedia.org\/wiki\/Frequency_analysis) on Uni-Grams, Bi-Grams or even Tri-Grams. \n\nTherefore I feel confident, that using one of these n-Grams ($n \\in {1,2,3}$) will be sufficient here.\n\n## Uni-Gram\n\nThe smallest pieces of information are single characters, i.e. **Uni-Grams**. \n\nUsing Uni-Grams allows us to identify *all* languages, which consist of *unique symbols*. \nE.g. this approach is sufficient to identify Chinese, Korean, Japanese, ... \n\nLet's inspect the unigrams:","4eee9091":"Well, we are using a lot of features per language, which might be already a good solution. \n\n*But chosing 3,079 features is already a lot*. And therefore the calculation is still expensive.\n\nBy the way: It is already possible to start the calculation based on 3,079 features. And the delivered results seem to be sufficient. \n\nSo, how can we do better?\n\n### Take top 50 (Uni- & Bi-Grams) per language\nWell, we can restrict ourselves to the top 50 Uni- & Bi-Grams per language. \n\nThis will lead to max `22 * 50 = 1100` features. \n\n(*Remark:* Due to simplification, I will reuse `top1PrecentMixtureVectorizer` here, since it provides at least >500 features per language)","480943f8":"# Future Work & Next Steps\n- Rerun analysis on cleaned data\n- Use quantitative method to identify features, e.g. PCA\n- Improve results by Hyperparameter-Optimzation (at least run some Auto-ML)\n- Try to identify, if there are two languages. And try to predict the secondary language","0977364d":"But for the languages consisting on a few letters only, using Bi-Grams is very helpfull. Since the most frequently used Bi-Grams differ a lot.\n\nWhereas for Japanese and Chinese we can *not* find any Bi-Grams occuring at least in 1% of the cases.","492be825":"The results of kNN are almost equal to those of the Naive Bayes approach.\n\nIn a future notebook, I might investigate some Auto-ML or hyperparameter optimization using the classical scikit algorithms.\n\nBut, right now I am more interested in some analytical algorithms.\n\n\n# Analytical Algorithms\n\n*Is it possible to use an classical analytical algorithm to predict the language?*\n\nWell, this should work, since we are working on something very similar to a distribution.\n\nIf we would restrict ourselves to a fixed n (for the n-Grams) only, it would be a real distribution.\n\nBut for the Mixture case, this does not hold. E.g. the occurence of 'e' and 'n' and 'en' will not be independent.\n\nAnyhow, I will convert all inputs into a relative form, so that we do not run into technical problems.\n\nLets inspect, if the following algorithms work here:\n- Least-Squares\n- Kulback-Leibler Divergence\n- Kolmogorov-Smirnov-Test\n\n\n\n## Assumptions\n- We are working on discrete distributions\n- $p_l(i)$ is the probability function of language $l$ for character\/symbol $i$\n- $q(i)$ is the probability function of the current sample \n","04f5744d":"In addition to the observed errors for OLS, the KL approach has some more errors on:\n- Pushto vs. Persian\n- Persian vs. Urdu\n- Latin vs. French\n\n# Kolmogorov Smirnov Test\n\nUse the two-sample-case of the [Kolmogorov Smirnov Test](https:\/\/en.wikipedia.org\/wiki\/Kolmogorov%E2%80%93Smirnov_test#Discrete_and_mixed_null_distribution) to check, if a given sample is written in a specific language.\n\nThe general formula is given by: \n\n$$ D_{n,m}=\\sup_x |F_{1,n}(x)-F_{2,m}(x)| $$\n\nWe then reject the Null hypothesis ($H_0$) at level $\\alpha$ if \n$$ D_{n,m}>c(\\alpha)\\sqrt{\\frac{n + m}{n\\cdot m}}. $$\n\nWhere \n- $F_1$ and $F_2$ are two samples \n- consisting of $m$ respective $n$ samples \n\n* Applied in our case:\n\n$$ D^l_{n,m}=\\sup_i |P(i)-Q(i)| $$\n\nfor each language $l$.\n\n\nWe have in total 1,000 texts per language (20% = 800 for training). So, $ n \\approx 800 * AvgChars $\n\n$\\alpha=0.01 \\implies c(\\alpha)=1.628 $\n\nSince we might reject the Null hypothesis for all languages, we could also cover the case, that the sample is written in a completely different language.\n\nIn the case when $H_0$ holds for more languages, we will use the language $l$ with the smallest value for $D^l$.\n\nIn other words: *We will choose the language, where the maximum of all absolute differences is the smallest. Except the Null hypothesis does not hold.*","3124f1f5":"So, we have **6816** Uni-Grams.\n\nBut how are they distributed across the languages?","8fc1c64a":"Obviously the benchmark library has some large differences in the prediction:\n- Pushto and Persian are both predicted to be Persian.\n- Some Italian (here Latin) is classified as Catalan, English, French, Romanian, ...\n- Chinese is also predicted as Korean.\n- Texts labelled to be in Urdu are classified to be Arabic.\n\n\n***\n\nWell, the benchmark seems to be beaten by far. But this does not reflect the reality. \n\nLet us have a look on the occured differences.\n\n"}}