{"cell_type":{"9e5ececa":"code","b2b3d0cc":"code","e5b5c906":"code","3e6af163":"code","828298b5":"code","51c81942":"code","b9133bbb":"code","fc40605d":"code","6a265878":"code","e3c5d406":"code","f43c60ac":"code","a3d36d50":"code","cc1a0266":"code","bd0715b7":"code","d5a2e271":"code","bf694cde":"markdown","eb0339ca":"markdown","de9d1283":"markdown","0b3ce729":"markdown","dd6fcc7a":"markdown","50d90e89":"markdown","e6793ea0":"markdown","a8325bda":"markdown","7a238a9a":"markdown","f36b4eb0":"markdown","b48857f7":"markdown","2a9761f9":"markdown","cb7dae7f":"markdown","400b87be":"markdown","71019bd7":"markdown","c448c138":"markdown","9c747efd":"markdown","d12f44c2":"markdown","89c65dc5":"markdown","356740c4":"markdown","95668042":"markdown","23dfdfb7":"markdown","28a949fd":"markdown","c038f0e2":"markdown","dcc7c9dd":"markdown","49b853e5":"markdown"},"source":{"9e5ececa":"# General Import\n\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport time\nfrom datetime import datetime\nimport datetime\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport folium\nfrom folium import plugins\nimport warnings\nimport seaborn as sns\nplt.style.use('ggplot')\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\n# TSA from Statsmodels\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa.api as smt\n\nfrom sklearn.metrics import mean_squared_error\n\ndef fxn():\n    warnings.warn(\"deprecated\", DeprecationWarning)\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    fxn()\n    \n#### Load the data\n\n# df1 = pd.read_csv('\/kaggle\/input\/eda-sensors\/df1.csv')\n# df2 = pd.read_csv('\/kaggle\/input\/eda-sensors\/df2.csv')\ndf3 = pd.read_csv('\/kaggle\/input\/eda-sensors\/df3.csv')\ndf4 = pd.read_csv('\/kaggle\/input\/eda-sensors\/df4.csv')\ndf5 = pd.read_csv('\/kaggle\/input\/eda-sensors\/df5.csv')\n\n\n#### Resample the data\n\n# lets create time series from weather \ntimeSeries = df5.loc[:, [\"datetime\",\"liv\"]]\ntimeSeries['datetime'] = pd.to_datetime(timeSeries['datetime'] )\nts = timeSeries.set_index('datetime').resample('1H').max().reset_index()\n","b2b3d0cc":"from pandas import read_csv\nfrom matplotlib import pyplot\n# adfuller library \nfrom statsmodels.tsa.stattools import adfuller\n#  kpss library\nfrom statsmodels.tsa.stattools import kpss\n\ndef summary_statistics(series):\n    X = series.values\n    split = round(len(X) \/ 2)\n    X1, X2 = X[0:split], X[split:]\n    mean1, mean2 = X1.mean(), X2.mean()\n    var1, var2 = X1.var(), X2.var()\n    print('Summary Statistics')\n    print('mean1=%f, mean2=%f' % (mean1, mean2))\n    print('variance1=%f, variance2=%f' % (var1, var2))\n    print('')\n\n\n# check_adfuller\ndef check_adfuller(series):\n    # Dickey-Fuller test\n    print ('Results of adfuller Test:')\n    result = adfuller(series, autolag='AIC')\n    print('Test statistic: ' , result[0])\n    print('p-value: '  ,result[1])\n    print('Critical Values:' ,result[4])\n    print('')\n    \n\n#define KPSS\ndef check_kpss(series):\n    print ('Results of KPSS Test:')\n    result = kpss(series, regression='c', nlags='auto')\n    print('Test statistic: ' , result[0])\n    print('p-value: '  ,result[1])\n    print('Critical Values:' ,result[3])\n    print('')\n\n\n# check_mean_std\ndef check_mean_std(series):\n    #Rolling statistics\n    TS = series\n    TS['rollmean'] = TS.liv.rolling(12).mean()\n    TS['rollstd'] = TS.liv.rolling(12).std()\n\n    # Create traces\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x = TS['datetime'], y=TS['liv'], name='Original'))\n    fig.add_trace(go.Scatter(x = TS['datetime'], y=TS['rollmean'], name='Rollling Mean'))\n    fig.add_trace(go.Scatter(x = TS['datetime'], y=TS['rollstd'], name='Rolling Std'))\n    fig.update_layout(title='Check Stationarity with Rolling Mean and Rolling Std ',xaxis_title='Datetime')\n    fig.show()\n    TS.drop(['rollmean','rollstd'],axis=1,inplace=True)\n\n# Examine the patterns of ACF and PACF (along with the time series plot and histogram)\n\ndef tsplot(ts, lags=None, title='', figsize=(14, 8)):\n    '''Examine the patterns of ACF and PACF, along with the time series plot and histogram.\n    '''\n    y = ts.liv\n    fig = plt.figure(figsize=figsize)\n    layout = (2, 2)\n    ts_ax   = plt.subplot2grid(layout, (0, 0))\n    hist_ax = plt.subplot2grid(layout, (0, 1))\n    acf_ax  = plt.subplot2grid(layout, (1, 0))\n    pacf_ax = plt.subplot2grid(layout, (1, 1))\n    \n    y.plot(ax=ts_ax)\n    ts_ax.set_title(title)\n    y.plot(ax=hist_ax, kind='hist', bins=25)\n    hist_ax.set_title('Histogram')\n    smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n    smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n    [ax.set_xlim(0) for ax in [acf_ax, pacf_ax]]\n    sns.despine()\n    fig.tight_layout()\n    \n    summary_statistics(y)\n    check_adfuller(y)\n    check_kpss(y)\n    check_mean_std(ts)\n    \n    return ts_ax, acf_ax, pacf_ax\n","e5b5c906":"tsplot(ts)","3e6af163":"ts_diff_first = timeSeries.set_index('datetime').resample('1H').max().reset_index()\n# 1st order differencing\nts_diff_first.liv = ts_diff_first.liv.diff()\nts_diff_first.dropna(inplace=True)\n# visualization\ntsplot(ts_diff_first)","828298b5":"ts_diff_seas = timeSeries.set_index('datetime').resample('1H').max().reset_index()\n# Seasonal differencing\nts_diff_seas.liv = ts_diff_seas.liv.diff(24)\nts_diff_seas.dropna(inplace=True)\n# visualization\ntsplot(ts_diff_seas)","51c81942":"TS = ts.liv","b9133bbb":"TRAIN = TS[:-24]\nTEST = TS[-24:]","fc40605d":"pred_constant = [TRAIN.values[-1] for _ in range(24)]\n\n# MSE\nmse = mean_squared_error(TEST.values, pred_constant)\nprint('MSE')\nprint(mse)\n\n# plot\nx = [i for i in range(2150,len(TS))]\n\nplt.plot(x[:-len(TEST)], TRAIN[2150:], color='green')\nplt.plot(x[-len(TEST):], TEST.values, color='red', label ='target')\nplt.plot(x[-len(TEST):], pred_constant, color='blue', label ='prediction')\nplt.legend()","6a265878":"pred_seasonal = TRAIN.values[-24:]\n\n# MSE\nmse = mean_squared_error(TEST.values, pred_seasonal)\nmse = round(mse, 4)\n\nprint('MSE')\nprint(mse)\n\n# plot\nx = [i for i in range(2150,len(TS))]\n\nplt.plot(x[:-len(TEST)], TRAIN[2150:], color='green')\nplt.plot(x[-len(TEST):], TEST.values, color='red', label ='target')\nplt.plot(x[-len(TEST):], pred_seasonal, color='blue', label ='prediction')\nplt.legend()","e3c5d406":"# create a n-th order differenced series\ndef difference(dataset, interval=1):\n    diff = list()\n    for i in range(interval, len(dataset)):\n        value = dataset[i] - dataset[i - interval]\n        diff.append(value)\n    return diff\n\n \n# invert differenced forecast\ndef inverse_difference(last_ob, value):\n    return value + last_ob","f43c60ac":"v_0 = TS[:24]\ndiff_24 = difference(TS,24)\ntrain = diff_24[:-24]\ntest = diff_24[-24:] #predictions last day","a3d36d50":"from statsmodels.tsa.holtwinters import ExponentialSmoothing\n\n# fit model\nmodel_es = ExponentialSmoothing(train, seasonal=\"add\", seasonal_periods=24)\nmodel_es_fit = model_es.fit()\n\n# forecast\npred_es = model_es_fit.forecast(24)\n\n# stack with train dataset\ndiff_predicted_es = np.hstack((train, pred_es))\ndiff_target = np.hstack((train,test))\n\n\n# total invert differencing\npredicted_es =  np.hstack((v_0,[inverse_difference(TRAIN[i], diff_predicted_es[i]) for i in range(len(diff_predicted_es))]))\ntarget =  np.hstack((v_0,[inverse_difference(TRAIN[i], diff_target[i]) for i in range(len(diff_target))]))\n\ny_pred = predicted_es[-24:]\ny_true = target[-24:]\n\n# MSE\nmse = mean_squared_error(y_true, y_pred)\nmse = round(mse, 4)\n\nprint('MSE')\nprint(mse)\n\n\n# plot\nx = [i for i in range(2150,len(TS))]\n\nplt.plot(x[:-len(TEST)], TRAIN[2150:], color='green')\nplt.plot(x[-len(TEST):], y_true, color='red', label ='target')\nplt.plot(x[-len(TEST):], y_pred, color='blue', label ='prediction')\nplt.legend()","cc1a0266":"pyplot.figure()\nsmt.graphics.plot_acf(ts_diff_seas.liv)\nsmt.graphics.plot_pacf(ts_diff_seas.liv)\npyplot.show()","bd0715b7":"from statsmodels.tsa.arima_model import ARIMA\n\n# fit model\nmodel_arima = ARIMA(train, order=(6,0,3))\nmodel_arima_fit = model_arima.fit(disp=0)\n\n# forecast\npred_arima = model_arima_fit.forecast(steps=24)[0]\n\n# stack\ndiff_predicted_arima = np.hstack((train, pred_arima))\n\n#invert\npredicted_arima =  np.hstack((v_0,[inverse_difference(TRAIN[i], diff_predicted_arima[i]) for i in range(len(diff_predicted_arima))]))\n\n\ny_pred_arima = predicted_arima[-24:]\n\n# MSE\nmse = mean_squared_error(y_true, y_pred_arima)\nmse = round(mse, 4)\n\nprint('MSE')\nprint(mse)\n\n\n# plot\nx = [i for i in range(2150,len(TS))]\n\nplt.plot(x[:-len(TEST)], TRAIN[2150:], color='green')\nplt.plot(x[-len(TEST):], y_true, color='red', label ='target')\nplt.plot(x[-len(TEST):], y_pred_arima, color='blue', label ='prediction')\nplt.legend()","d5a2e271":"# Lets reframe the previous ARIMA case. The same can be done with other models.\n\n# fit model\nmodel_arima = ARIMA(train, order=(6,0,3))\nmodel_arima_fit = model_arima.fit(disp=0)\n\n\n# at the end of the loop I will get 24 forecasts (recursive method)\nlist_forecasts = []\n\nfor i in range(24):\n    \n    # one-step-ahead prediction\n    pred_arima_one_step = model_arima_fit.forecast(steps=1)[0]\n    \n    # append prediction to the list of forecasts\n    list_forecasts.append(pred_arima_one_step[0])\n    \n    # append the prediction to the training set\n    train.append(pred_arima_one_step[0])\n    \n    # refit the model with the updated training set (old set + )\n    model_arima = ARIMA(train, order=(6,0,3))\n    model_arima_fit = model_arima.fit(disp=0)\n    \n\n# stack train and predictions\npred_arima = np.array(list_forecasts)\ndiff_predicted_arima = np.hstack((train[:-24], pred_arima))\n\n#invert\npredicted_arima =  np.hstack((v_0,[inverse_difference(TRAIN[i], diff_predicted_arima[i]) for i in range(len(diff_predicted_arima))]))\n\ny_pred_arima = predicted_arima[-24:]\n\n# MSE\nmse = mean_squared_error(y_true, y_pred_arima)\nmse = round(mse, 4)\n\nprint('MSE')\nprint(mse)\n\n# plot\nx = [i for i in range(2150,len(TS))]\n\nplt.plot(x[:-len(TEST)], TRAIN[2150:], color='green')\nplt.plot(x[-len(TEST):], y_true, color='red', label ='target')\nplt.plot(x[-len(TEST):], y_pred_arima, color='blue', label ='prediction')\nplt.legend()","bf694cde":"### Why Stationarity is important in Time Series?","eb0339ca":"When a time series is stationary, it can be easier to model. Statistical modeling methods assume or require the time series to be stationary to be effective.","de9d1283":"### Differencing ","0b3ce729":"### make data stationary","dd6fcc7a":"## Time series Analysis","50d90e89":"#### Seasonal Differencing ","e6793ea0":"### Exponential Smoothing","a8325bda":"# ! IMPORTANT !","7a238a9a":"We can check stationarity using the following methods:\n* Summary Statistics: You can split your time series into two (or more) partitions and compare the mean and variance of each group. If they differ and the difference is statistically significant, the time series is likely non-stationary.\n* Plotting Rolling Statistics: We have a window lets say window size is 12 and then we find rolling mean and variance to check stationary.\n* Dickey-Fuller Test: The test results comprise of a Test Statistic and some Critical Values for difference confidence levels. If the test statistic is less than the critical value, we can say that time series is stationary.\n* KPSS Test: it is another test for checking the stationarity of a time series (slightly less popular than the Dickey Fuller test). The null and alternate hypothesis for the KPSS test are opposite that of the ADF test, which often creates confusion.\n* Autocorrelation (ACF) and partial autocorrelation (PACF) plots: these are plots that graphically summarize the strength of a relationship with an observation in a time series with observations at prior time steps.","f36b4eb0":"## Do we get the same result if we implement recursive forecasting explicitly?\n","b48857f7":"* The ACF shows a significant lag for 6 hours.\n* The PACF shows a significant lag for perhaps 2-3 hours\n\nA good starting point for the p and q values is 6 or 3.","2a9761f9":"# Now, forecast the water level for next 24 hours.\n* Apply Seasonal Differencing to make the time series stationary\n* Train and Test Splitting of time series\n* Choose a Method, Train and Test it\n* Invert the difference back to the original level\n* Compute the MSE metric\n\n#### the following traditional forecasting models are applied:\n* Constant Naive model\n* Seasonal Naive Model\n* ES - Exponential Smoothing\n* ARIMA - Autoregressive Integrated Moving Average Model\n\n#### --> For ARIMA look at ACF and PACF for quick models parameters configuration","cb7dae7f":"### Seasonal Naive Model","400b87be":"### Make TimeSeries Stationary\n* Differencing\n* Power based Transformations","71019bd7":"### ARIMA","c448c138":"# Traditional Time series Forecasting","9c747efd":"#### Check TimeSeries Stationarity\n\n\n* Look at Plots: You can review a time series plot of your data and visually check if there are any obvious trends or seasonality.\n\n* Summary Statistics: You can review the summary statistics for your data for seasons or random partitions and check for obvious or significant differences.\n\n* Statistical Tests: You can use statistical tests to check if the expectations of stationarity are met or have been violated.\n","d12f44c2":"Remember that ARIMA (or any other of the previous models) only perform recursive forecasting, not direct forecasting. \n\nFrom [Stackoverflow](https:\/\/stackoverflow.com\/questions\/53064545\/statsmodels-implementing-a-direct-and-recursive-multi-step-forecasting-strategy\/53073801?fbclid=IwAR02vCIse-N8M77pXDrkNTBjZ95QgZIc1TG-aaygd8EZPd5hYGd27ptsYds#53073801), in statsmodels, (or in R auto.arima()), when you set a value for h > 1, it simply performs a recursive forecast to get there.\n","89c65dc5":"#### First-Order Differencing","356740c4":"#### Configure ARIMA\n\nThe parameters (p,d,q) of the ARIMA model are defined as follows:\n\n* --> p: The number of lag observations included in the model, also called the lag order.\n* --> d: The number of times that the raw observations are differenced, also called the degree of differencing.\n* --> q: The size of the moving average window, also called the order of moving average.\n\n        ** Note: since the time series has been already differenciated it is set equal to 0.\n\nTo select the lag values for the Autoregression (AR) and Moving Average (MA) parameters, p and q respectively, we review Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots.","95668042":"### Constant Naive Model","23dfdfb7":"### Train & Test Splitting","28a949fd":"Data transformations (detrending, demeaning, differencing, ARMA structure ,power, constancy of variance considerations, elimination of pulses\/level shifts\/seasonal pulses\/local time trends ) are used to convert an observed series to a white noise series\/process . The parameters of this white-noise process ( the errors from these suitable transformations) i.e. the mean , variance and covarince for all lags should be constant for all sub-intervals of time.","c038f0e2":"### Stationary Time Series Definition\n\nTime series are stationary if they do not have trend or seasonal effects. Summary statistics calculated on the time series are consistent over time, like the mean or the variance of the observations.","dcc7c9dd":"### But ...\n\nStatistical time series methods and even modern machine learning methods will benefit from the clearer signal in the data.\n\nBut\u2026\n\nWe turn to machine learning methods when the classical methods fail. When we want more or better results. We cannot know how to best model unknown nonlinear relationships in time series data and some methods may result in better performance when working with non-stationary observations or some mixture of stationary and non-stationary views of the problem.\n\nThe suggestion here is to treat properties of a time series being stationary or not as another source of information that can be used in feature engineering and feature selection on your time series problem when using machine learning methods.","49b853e5":"## Recursive approaches will be further analyze in the coming notebook. "}}