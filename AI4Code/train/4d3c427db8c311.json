{"cell_type":{"ef265ad6":"code","7e3169c0":"code","d1f2118d":"code","0f60891a":"code","c7ef4321":"code","e774d7cb":"code","61fb69fd":"code","dfb4d748":"code","7803ceb3":"code","86d44d25":"code","960c264e":"code","68de0fe6":"code","e4ab7b79":"code","3d9111bf":"code","50dd6ae5":"code","901dac67":"code","f00f43f3":"code","1c539ebb":"code","05a48078":"code","7ef44a8c":"code","c0e0ca74":"code","1bd1593a":"code","1d62451f":"code","b13c20dd":"markdown","3b339e22":"markdown","d27e1fa1":"markdown","8004f17a":"markdown","47fb17fe":"markdown","55d76d27":"markdown","d8ea7100":"markdown","b3a7f57c":"markdown","012d9966":"markdown","55aaac25":"markdown","6a1b8794":"markdown","14498c36":"markdown","26e65bfa":"markdown","e03e430a":"markdown","20e67997":"markdown","07c9561d":"markdown","88abb7a8":"markdown"},"source":{"ef265ad6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7e3169c0":"import json\n\nstScript = {} \nwith open('\/kaggle\/input\/start-trek-scripts\/all_series_lines.json') as f: \n    stScript = json.load(f)","d1f2118d":"from collections import defaultdict\n\n#(episodeList:dict<episodeName:string,dict<character_name:string, lines: string[]>>) => dict<characterName: lines: string[]>\ndef GetCharacterLines(episodeList):\n    characterLines = defaultdict(list)\n    for episode, characterLineDict in episodeList.items():\n        for characterName, lines in characterLineDict.items():\n            characterLines[characterName] += lines\n    return characterLines\n\ndef MergeAll(seriesDict):\n    combined = defaultdict(list)\n    for series, episodes in seriesDict.items():\n        seriesCharLines = GetCharacterLines(episodes)\n        for charName, lines in seriesCharLines.items():\n            combined[charName] += lines\n    return combined\n    \nallSeriesLines = MergeAll(stScript)","0f60891a":"import matplotlib.pyplot as plt\n\nlinesPerChar = {charName: len(lines) for charName, lines in allSeriesLines.items()}\nlines = [lineCount for charName, lineCount in linesPerChar.items()]\nprint(len(lines))\nplt.title(\"Number of Characters with N Lines\")\nplt.xlabel(\"Number of lines\")\nplt.ylabel(\"Number of characters\")\nplt.hist(lines, bins=100)","c7ef4321":"minLength = 10000\nfilteredCharacterLineCounts = {charName: nLines for charName, nLines in linesPerChar.items() if nLines > minLength}\nprint(len(filteredCharacterLineCounts.items()))\nprint(filteredCharacterLineCounts)\nplt.title(\"Number of characters with more than {0} Lines\".format(minLength))\nplt.xlabel(\"Number of lines\")\nplt.ylabel(\"Number of characters\")\nplt.hist([nLines for _, nLines in filteredCharacterLineCounts.items()])\n\nmainCharLineDict = {charName: lines for charName, lines in allSeriesLines.items() if charName in filteredCharacterLineCounts}","e774d7cb":"#dict<class:string, lines:string[]> => dict<class, dict<string, dict<string, int>>\ndef GetClassWordCounts(classLineDict):\n    classWordCountDict = defaultdict(lambda: defaultdict(int))\n    for className, lines in classLineDict.items():\n        for line in lines:\n            for token in line.split():\n                classWordCountDict[className][token] += 1\n    return classWordCountDict\n\n#(wordCounts: dict<string, int>, vocabularySet: set<string>, kFactor) => dict<string, float>\ndef AddKSmoothing(wordCounts, kFactor):\n    wordCounts[\" unknown\"] = 0\n    totalWordCount = sum([count for word, count in wordCounts.items()])\n    totalVocabularySize = len(wordCounts)\n    denominator = totalWordCount + kFactor * totalVocabularySize\n    wordProbDict = {word: (count + kFactor)\/ denominator for word, count in wordCounts.items()}\n    return wordProbDict\n\n#(linesDictionary:dict<string,string[]>) => set<string>\ndef GetUniqueWords(linesDictionary):\n    words = set()\n    for _, lines in linesDictionary.items():\n        for line in lines:\n            [words.add(word) for word in line.split()]\n    return words\n\n#Multinomial Naive Bayes model\n#(classLineDict: dict<string, string[]>, smoothingFactor:float) => (dict<character_name:string, dict<word:string, prob:float>>, priors:dict<string, float>, vocabulary:set<string>)\ndef CreateNaiveBayesModel(classLineDict, smoothingFactor):\n    nTotalLines = sum([len(lines) for char, lines in classLineDict.items()])  \n    classPriorDict = {charName: (len(lines) \/ nTotalLines) for charName, lines in classLineDict.items()}\n    classTotalWordCount = GetClassWordCounts(classLineDict) #count the number of times each word has been said by a character\n    vocabulary = GetUniqueWords(classLineDict)\n    classProbDicts = {charName: AddKSmoothing(wordCounts, smoothingFactor) for charName, wordCounts in classTotalWordCount.items()}\n    return (classProbDicts, classPriorDict,  vocabulary)\n\nmodel = CreateNaiveBayesModel(mainCharLineDict, 0.01)","61fb69fd":"#(line:string, bayesModel: dict<character_name:string, dict<word:string, prob:float>>) => dict<characterName:string, evaluated_ln_probability:float>\n#returns the natural logarithm of the probability instead (less -ve means higher prob.)\ndef EvaluateLine(line, bayesModel):\n    classProbDicts, classPriors, vocab  = bayesModel\n    tokens = [token if token in vocab else \" unknown\" for token in line.split()]\n    results = {}\n    for charName, wordProbabilities in classProbDicts.items():\n        prior = classPriors[charName]\n        t = [token if token in wordProbabilities else \" unknown\" for token in tokens]\n        lnProb = np.log(prior) + np.sum(np.log([wordProbabilities[token] for token in t]))\n        results[charName] = lnProb\n    return results\n\ndef GetNthBestMatches(line, bayesModel, n):\n    probDict = EvaluateLine(line, model)\n    matchOrder = sorted(probDict.items(), reverse=True, key=lambda p: p[1])\n    return matchOrder[:min(len(matchOrder), n)]\n\ntestLine = \"Ok, hear me out. So it's about this guy named Rick\"\n\nprint(GetNthBestMatches(testLine, model, 5))","dfb4d748":"from sklearn.model_selection import train_test_split\n\n#Model evaulation\n\n#(classLineDict: dict<string, string[]>, testFrac:float) => (training:dict<string, string[]>, test:dict<string, string[]>)\ndef SplitDictIntoTrainingAndTestSets(classLineDicts, testFrac, minLinesPerChar):\n    trainingDict = {}\n    testDict = {}\n    omittedClasses = []\n    for charName, lines in classLineDicts.items():\n        if len(lines) < minLinesPerChar:\n            omittedClasses.append(charName)\n            continue\n        splits = train_test_split(lines, test_size=testFrac)\n        trainingDict[charName] = splits[0]\n        testDict[charName] = splits[1]\n    print(\"Following classes were dropped (did not meet min. sample size of {1}): {0}\".format(omittedClasses, minLinesPerChar))\n    print(\"Remaining clases: {0}\".format([className for className, _ in trainingDict.items()]))\n    return (trainingDict, testDict)\n\ndef EvaluateMultiNodalModel(bayesModel, testDataDict):\n    actualLabels = []\n    predictedLabels = []\n    for charName, lines in testDataDict.items():\n        actualLabels += [charName] * len(lines)\n        for line in lines:\n            resultDict = EvaluateLine(line, bayesModel)\n            bestMatch = max(resultDict.items(), key=lambda p:p[1])\n            predictedLabels.append(bestMatch[0])\n    return (actualLabels, predictedLabels)\n\ntrainingDict, testDict = SplitDictIntoTrainingAndTestSets(mainCharLineDict, 0.2, 7500)","7803ceb3":"multiNodalModel = CreateNaiveBayesModel(trainingDict, 0.01)\nactual, predicted = EvaluateMultiNodalModel(multiNodalModel, testDict)","86d44d25":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n    \nprint(confusion_matrix(actual, predicted))\nprint(accuracy_score(actual, predicted))","960c264e":"import string\n#decapitalization\n#(classLineDict: dict<string, string[]>) => classLineDict: dict<string, string[]>\ndef decapitalizeAll(classLineDict):\n    trans = str.maketrans(\"\",\"\", string.punctuation)\n    return {charName:[line.lower().translate(trans) for line in lines] for charName, lines in classLineDict.items()}\n\ntrainingDecapitalized = decapitalizeAll(trainingDict)\ntestDecapitalized = decapitalizeAll(testDict)\n\n\nmultiNodalModelDecap = CreateNaiveBayesModel(trainingDecapitalized, 0.01)\nactualDecap, predictedDecap = EvaluateMultiNodalModel(multiNodalModelDecap, testDecapitalized)\nprint(\"Accuracy (Post De-capitalization & Punctuation removal): {0}$%\".format(accuracy_score(actualDecap, predictedDecap)))","68de0fe6":"import spacy\nnlp = spacy.load('en_core_web_sm', disable = ['ner', 'parser'])\n\n#lemmatization\n#(classLineDict: dict<string, string[]>) => classLineDict: dict<string, string[]>\ndef lemmatizeAll(classLineDict):\n    return {charName: [\" \".join([token.lemma_ for token in nlp(line)]) for line in lines] for charName, lines in classLineDict.items()}\n\ntrainingLemmatized = lemmatizeAll(trainingDecapitalized)\ntestLemmatized = lemmatizeAll(testDecapitalized)","e4ab7b79":"multiNodalModelLemmatized = CreateNaiveBayesModel(trainingLemmatized, 0.01)\nactualLemmatized, predictedLemmatized = EvaluateMultiNodalModel(multiNodalModelLemmatized, testLemmatized)\nprint(\"Accuracy (Post Lemmatization): {0}%\".format(accuracy_score(actualLemmatized, predictedLemmatized)))","3d9111bf":"import math\n\n#TF-IDF\n#(classLineDict: dict<string, string[]>)\ndef CreateTFIDFNaiveBayesModel(classLineDict, kSmoothingFactor):\n    #(void) => dictionary<string, int>\n    def GetDocumentFrequency(): # for IDF calculation\n        docFreq = defaultdict(int)\n        for _, lines in classLineDict.items():\n            for line in lines:\n                lineWordSet = set(line.split())\n                for word in lineWordSet:\n                    docFreq[word] += 1\n        return docFreq\n    \n    #(tokens:string[]) => (token-tf:dict<string, float>, token-freq:dict<string, int>)           \n    def TF(tokens):\n        nTokens = len(tokens)\n        tokenFreq = defaultdict(int)\n        for token in tokens:\n            tokenFreq[token] += 1\n        tfDict = {tokenName: (freq\/nTokens) for tokenName,freq in tokenFreq.items()}\n        return (tfDict, tokenFreq)  \n    \n    def GetWordProbabilities(lines, IDF):\n        #Stores the the summation of the (word freq * TF-IDF) for each word\n        TF_IDFScores = defaultdict(float)\n        for line in lines: # for one document\n            tokens = line.split()\n            tokenTfDict, tokenFreq = TF(tokens) #Varies on a per-document basis\n            for token, freq in tokenFreq.items():\n                line_tf_idf = freq * tokenTfDict[token] * tokenFreq[token]\n                TF_IDFScores[token] += line_tf_idf\n        \n        minValue = min(TF_IDFScores.values())\n        compensatedSmoothingFactor = kSmoothingFactor * minValue\n        TF_IDFScores[\" unknown\"] += 0 \n        \n        #Apply smoothing factor\n        for token in TF_IDFScores:\n            TF_IDFScores[token] += compensatedSmoothingFactor\n        \n        nUniqueTokens = len(TF_IDFScores)\n        denominator = sum([val for tokenName, val in TF_IDFScores.items()]) + nUniqueTokens * kSmoothingFactor\n        wordProbDict = {tokenName: score \/ denominator  for tokenName, score in TF_IDFScores.items()}\n        return wordProbDict \n    \n    nTotalLines = sum([len(lines) for _, lines in classLineDict.items()])\n    DF = GetDocumentFrequency()\n    DF[\" unknown\"] = 0\n    IDFDict =  {word: math.log(nTotalLines \/ (freq + 1)) for word, freq in DF.items()}\n    classPriorDict = {charName: (len(lines) \/ nTotalLines) for charName, lines in classLineDict.items()}\n    classProbDicts = {charName: GetWordProbabilities(lines, IDFDict) for charName, lines in classLineDict.items()}\n    vocabulary = GetUniqueWords(classLineDict)\n    \n    return (classProbDicts, classPriorDict,  vocabulary)","50dd6ae5":"tdidfModel = CreateTFIDFNaiveBayesModel(trainingDecapitalized, 0.01)\nactualTDIDF, predictedTDIDF = EvaluateMultiNodalModel(tdidfModel, testDecapitalized)\nprint(\"Accuracy:{0}\".format(accuracy_score(actualTDIDF, predictedTDIDF)))\nprint(confusion_matrix(actualTDIDF, predictedTDIDF, labels=[charName for charName, _ in trainingDict.items()]))","901dac67":"#Complementary Naive Bayes\n#(classLineDict: dict<string, string[]>) => dict<string, dict<word,int>>\ndef GetComplementaryWordCounts(classLineDict):\n    totalWordCounts = defaultdict(int)\n    for _, lines in classLineDict.items():\n        for line in lines:\n            for word in line.split():\n                totalWordCounts[word] += 1\n                \n    classWordCounts = GetClassWordCounts(classLineDict)\n    complementaryWordCounts = {charName: \n                               {word: \n                                    totalCount - (wordCountDict[word] if word in wordCountDict else 0) for word, totalCount in totalWordCounts.items()} \n                               for charName, wordCountDict in classWordCounts.items()}\n    return complementaryWordCounts\n    \n#(classLineDict: dict<string, string[]>, smoothingFactor:float) => (dict<character_name:string, dict<word:string, prob:float>>, priors:dict<string, float>, vocabulary:set<string>)\ndef GenComplementaryNaiveBayesModel(classLineDict, smoothingFactor):\n    nTotalLines = sum([len(lines) for char, lines in classLineDict.items()])  \n    complementaryClassPriorDict = {charName: 1.0 - (len(lines) \/ nTotalLines) for charName, lines in classLineDict.items()}\n    \n    complementaryClassWordCounts = GetComplementaryWordCounts(classLineDict)\n    \n    vocabulary = GetUniqueWords(classLineDict)\n    vocabulary.add(\" unknown\") #unknown token\n    \n    complementaryClassProbDicts = {charName: AddKSmoothing(wordCounts, smoothingFactor) for charName, wordCounts in complementaryClassWordCounts.items()}\n    return (complementaryClassProbDicts, complementaryClassPriorDict, vocabulary)\n\ndef EvaluateLineUsingComplementaryBayes(line, complementaryBayesModel):\n    compClassProbDicts, compClassPriors, vocab  = complementaryBayesModel\n    tokens = [token if token in vocab else \" unknown\" for token in line.split()]\n    results = {}\n    for charName, compWordProbabilies in compClassProbDicts.items():\n        prior = compClassPriors[charName]\n        invProb = np.sum([math.log(prior * compWordProbabilies[token]) for token in tokens])\n        results[charName] = invProb\n    return results\n\ndef GetNthBestMatchesUsingComplementaryBayes(line, complementaryBayesModel, nMatches):\n    compProbDict = EvaluateLineUsingComplementaryBayes(line, complementaryBayesModel)\n    lowestInvProbFirst = sorted(list(compProbDict.items()), key=lambda p:p[1],reverse=False) #argmin rather than argmax, since the complementary prob is calculated\n    return lowestInvProbFirst[:min(nMatches, len(lowestInvProbFirst))]\n\ncomplementaryBayesModel = GenComplementaryNaiveBayesModel(trainingDecapitalized, 0.01)\nprint(GetNthBestMatchesUsingComplementaryBayes(testLine, complementaryBayesModel, 5))","f00f43f3":"def EvaluateComplementaryModel(bayesModel, testDataDict):\n    actualLabels = []\n    predictedLabels = []\n    for charName, lines in testDataDict.items():\n        actualLabels += [charName] * len(lines)\n        for line in lines:\n            resultDict = EvaluateLineUsingComplementaryBayes(line, bayesModel)\n            bestMatch = min(resultDict.items(), key=lambda p:p[1])\n            predictedLabels.append(bestMatch[0])\n    return (actualLabels, predictedLabels)\n\n\ncomplementaryBayesModel = GenComplementaryNaiveBayesModel(trainingDecapitalized, 0.01)\nactualCompl, predictedCompl = EvaluateComplementaryModel(complementaryBayesModel, testDecapitalized)\nprint(\"Accuracy:{0}\".format(accuracy_score(actualCompl, predictedCompl)))\nprint(confusion_matrix(actualCompl, predictedCompl))","1c539ebb":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import ComplementNB\n\ndef dict2LabelsAndLines(classLineDict):\n    labels = []\n    lines = []\n    for charName, charLines in classLineDict.items():\n        labels += [charName] * len(charLines)\n        lines += charLines\n    return (labels, lines)\n        \ntrainLabels, trainLines = dict2LabelsAndLines(trainingDict)\ntestLabels, testLines = dict2LabelsAndLines(testDict)\n\nvec = CountVectorizer();\ntrainVec = vec.fit_transform(trainLines).toarray()\ntestVec = vec.transform(testLines).toarray()\n\nspNB = MultinomialNB(alpha=0.01)\nspNB.fit(trainVec, trainLabels);\n\nprint(\"Accuracy:\")\nspNB.score(testVec, testLabels)","05a48078":"spCB = ComplementNB(alpha=0.01)\nspCB.fit(trainVec, trainLabels);\n\nprint(\"Accuracy:\")\nspCB.score(testVec, testLabels)","7ef44a8c":"#(tokens:string[], nGramCount:int) => tuple<string...>[]\ndef Tokens2NGrams(tokens, nGramCount):\n    allNGrams = []\n    nTokens = len(tokens)\n    for nGramLength in range(1, nGramCount + 1):\n        nGrams = [tuple(tokens[index: index + nGramLength]) for index in range(nTokens - nGramLength + 1)]\n        allNGrams += nGrams\n    return allNGrams\n\n#(lines:string[], nGramCount:int) => dict<tuple<string...>, int>\ndef GetClassNGramCount(lines, nGramCount):\n    nGramCountDict = defaultdict(int)\n    for line in lines:\n        tokens = line.split()\n        for nGram in Tokens2NGrams(tokens, nGramCount):\n            nGramCountDict[nGram] += 1\n    return nGramCountDict\n\ndef CreateNGramNaiveBayesModel(classLineDict, nGramCount, smoothingFactor):\n    nTotalLines = sum([len(lines) for char, lines in classLineDict.items()])  \n    classPriorDict = {charName: (len(lines) \/ nTotalLines) for charName, lines in classLineDict.items()}\n    classNGramCounts = {charName: GetClassNGramCount(lines, nGramCount) for charName, lines in classLineDict.items()}\n    classProbDicts = {charName: AddKSmoothing(nGramCounts, smoothingFactor) for charName, nGramCounts in classNGramCounts.items()}\n    return (classProbDicts, classPriorDict, nGramCount)\n\ndef EvaluateNGramLine(line, nGramBayesModel):\n    classProbDicts, classPriors, nGramCount = nGramBayesModel\n    nGrams = Tokens2NGrams(line.split(), nGramCount)\n    results = {}\n    for charName, wordProbabilities in classProbDicts.items():\n        prior = classPriors[charName]\n        t = [token if token in wordProbabilities else \" unknown\" for token in nGrams]\n        lnProb = np.log(prior) + np.sum(np.log([wordProbabilities[token] for token in t]))\n        results[charName] = lnProb\n    return results\n\ndef NGramGetNthBestMatches(line, nGramBayesModel, n):\n    probDict = EvaluateNGramLine(line, nGramBayesModel)\n    matchOrder = sorted(probDict.items(), reverse=True, key=lambda p: p[1])\n    return matchOrder[:min(len(matchOrder), n)]\n\ndef EvaluateNGramModel(nGramBayesModel, testDataDict):\n    actualLabels = []\n    predictedLabels = []\n    for charName, lines in testDataDict.items():\n        actualLabels += [charName] * len(lines)\n        for line in lines:\n            resultDict = EvaluateNGramLine(line, nGramBayesModel)\n            bestMatch = max(resultDict.items(), key=lambda p:p[1])\n            predictedLabels.append(bestMatch[0])\n    return (actualLabels, predictedLabels)\n\n\nTokens2NGrams(\"I turned myself into a pickle, Morty! Boom!\".split(), 2)","c0e0ca74":"nGramBayesModel = CreateNGramNaiveBayesModel(trainingDecapitalized, 2, 0.01)\n\nnGramActual, nGramPredictions = EvaluateNGramModel(nGramBayesModel, testDecapitalized)\nprint(\"Accuracy (Bigram Multinomial Naive Bayes):{0}\".format(accuracy_score(nGramActual, nGramPredictions)))\nprint(confusion_matrix(nGramActual, nGramPredictions, labels=[charName for charName, _ in trainingDict.items()]))","1bd1593a":"rick = \"\"\"\nCome on, flip the pickle, Morty. You're not gonna regret it. The payoff is huge. \nI turned myself into a pickle, Morty! Boom! Big reveal: I'm a pickle. \nWhat do you think about that? I turned myself into a pickle! \n\"\"\"\n\nprint(\"Most likely class ordered first:\")\nprint(GetNthBestMatches(rick, multiNodalModel, 3))","1d62451f":"copypasta = \"\"\"\nTo be fair, you have to have a very high IQ to understand Rick and Morty. The humour is extremely subtle, \nand without a solid grasp of theoretical physics most of the jokes will go over a typical viewer's head. \nThere's also Rick's nihilistic outlook, which is deftly woven into his characterisation - his personal philosophy draws heavily from Narodnaya Volya literature, \nfor instance. The fans understand this stuff; they have the intellectual capacity to truly appreciate the depths of these jokes, \nto realise that they're not just funny - they say something deep about LIFE. As a consequence people who dislike Rick & Morty truly ARE idiots - \nof course they wouldn't appreciate, for instance, the humour in Rick's existential catchphrase \"Wubba Lubba Dub Dub,\" \nwhich itself is a cryptic reference to Turgenev's Russian epic Fathers and Sons. \nI'm smirking right now just imagining one of those addlepated simpletons scratching their heads in confusion as Dan Harmon's genius wit unfolds itself on their television screens. \nWhat fools.. how I pity them. \n\nAnd yes, by the way, i DO have a Rick & Morty tattoo. And no, you cannot see it.\nIt's for the ladies' eyes only - \nand even then they have to demonstrate that they're within 5 IQ points of my own (preferably lower) beforehand. Nothin personnel kid \n\"\"\"\n\nprint(\"Most likely class ordered first:\")\nprint(GetNthBestMatches(copypasta, multiNodalModel, 3))","b13c20dd":"## Complement Naive Bayes.\n\nRather than check for the probability that $word_i$ has occurred within class $C_k$, Complement Naive Bayes instead models the probability of $word_i$ occurring in classes outside of $C_k$ (hence the name *complement* Bayes). During the decision phase, rather than check for the maximum likelihood of a sentence belong to class $C_k$, it instead chooses the class that gives the lowest probability of the word occuring outside of it (choosing a minimum instead). \n\nThe model thus becomes:\n\n$$p(\\hat{C_k}|w) = p(x|\\hat{C_k})p(\\hat{C_k})$$\n$$$$\n\nWhere\n\n* $\\hat{C_k}$ is the complement of class $C_k$ (Everything outside of $C_k$)\n\nThus, it follows that:\n\n$$p(\\hat{C_k}|\\{w_1, w_2, w_3, w_{...}\\}) = \\prod_{i}{\\frac{N_w|_{\\hat{c}}}{N_{total}|_{\\hat{c}}}} \\times \\frac{DN_{\\hat{c}}}{DN_{total}}$$\n\nWhere\n* $N_w|_{\\hat{c}}$ is the number times $\\text{word}_w$ occurs outside of class $C_k$\n* $N_{total}|_{\\hat{c}}$ is the total number of words outside of class $C_k$\n* $DN_{\\hat{c}}$ is the number of documents(lines) outside of class $C_k$\n* $DN_{total}$ is the total number of documents\n\nClass prediction is found via the following expression:\n\n$$\\text{predicted class C}_k = \\text{arg}\\min_{k} \\left( p(\\hat{C_k}|\\{w_1, w_2, w_3, w_{...}\\}) \\right)$$","3b339e22":"* What about the most likely of the 3 to spout the infamous copypasta?","d27e1fa1":"**-Results-**\n\nTraining accuracy was nudged up by a single meagre percentage point, in other words, this barely helps (for this Corpus)","8004f17a":"**-Results-**\n\nIncluding the Bigrams to the Multinomial Classifer only gives an minor bump in accuracy (~1%) compared to the standard Multinomial classifer.","47fb17fe":"**-Results-**\n\nCompared to the standard Multinomial Bayes model, test accuracy is still roughly the same (~66\/67%)","55d76d27":"**-Results-**\n\nUsing a basic multinomial model, a test accuracy of ~67% can be obtained. The effects of applying the following 3 different types of data-preprocessing will be investigated using the standard Multinomial Naive Bayes Model.\n* Decapitalization & Removal of punctation\n* Lemmatization\n* TF-IDF","d8ea7100":"# De-capitalization & Punctuation Removal\n\nWhile the capitalization of some words indicate the presence of nouns, it may also simply occur due to words occuring at the start of sentences. The current classifier treats capitalized words as completely different entities, decapitalization will remove this distinction. Punctuation found at the end of words may also cause the string splits to produce seemingly different words to the classifer due to the presence of punctuation marks at the end of some words. Punctuation will be removed to avoid this.\n\n","b3a7f57c":"**-Results-**\n\nBarely an improvement over decapitization & punctuation removal. Clearly, lemmatization and decapitalization did little to boost model accuracy. Time for something more involved.","012d9966":"**-Results-**\n\nUnfortunately, TF-IDF somehow made classification accuracy worse, time to test other Naive Bayes models to find potentially better classifiers.","55aaac25":"# Cross-Validation of implementations with SciPy\n\nJust to check the correctness of the from-scratch implementations, the same training set will be run on SciPy's Multinomial (*MultinomialNB*) and Complement Bayes (*ComplementNB*) classifier. Test accuracies reveal values in the same ballpark (~ 67%).","6a1b8794":"# N-Gram extension to Multinomial Naive Bayes\n\nIn the prior 2 implementations of Naive Bayes, the probability of each word occuring in a sentence is treated to be completely independent of the rest of the words within the same sentence. In reality, concepts such as context, grammar & topic almost certainly guarantee certain words to occur more frequently together (thus violating the independent probabilty assumption). It is possible to inject the increased likelihood of words occuring given the presence of prior words by counting N-Grams (phrases of *N* adjacent words) in addition to isolated words. \n\nThe same equation is used to calculate the probabilty of a line occuring in a given class:\n\n$$p(C_k|\\{w_1, w_2, w_3, w_{...}\\}) = \\prod_{i}{\\frac{N_w|_c}{N_{total}|c}} \\times \\frac{DN_c}{DN_{total}}$$\n\nHowever, wheras $w_i$ used to consist only of single words (i.e. {\"Quick\", \"Brown\", \"Fox\"}), the N-Gram model now counts phrases as well, causing $w_i$ to morph into {\"Quick\", \"Brown\", \"Fox\", \"Quick Brown\", \"Brown Fox\"} (For a Bigram case).   ","14498c36":"# Classification Demostration\n\nLike any other classifier, the Naive Bayes Model could be used to perform some meaningful work, such as Email Spam Detection\/ Author detection for un-attributed documents.\n\nHowever, this demostration centres around something far more trivial.\n* Among Captain Picard, Kirk & Janeway, whose speech patterns are statistically most similar to Morty from Rick & Morty?","26e65bfa":"# TF-IDF\n**Term Frequency, Inverse Document Frequency**\n\nWithin Text Classification, not all words are created equal. Compared to specialized terminology (i.e. hydraulic, flora, viscosity), common stop words (is, the, a, etc.) serve as far poorer signals of distinctive indications which help differentiate  classes. TF-IDF attempts to model this by penalizing the weightage of commonly occuring words within the entire corpus (the entire Scipt), and boosting words that occur multiple times within a single document (A single character line).\n\nMany variations of TF-IDF exist for the TF (Term-Frequency) and IDF (Inverse Document Frequency) portions, but the following definition will be employed in this case.\n\n**Term Frequency (TF) \\[Within a single document (line in this case) \\]**\n* The total number of occurrences of $word_n$ in $document_i$ divided by the total number of words within $document_i$. \n$$TF \\text{(Term Frequency)|}_{word_i} = \\frac{\\text{Number of times word}_{i}\\text{ occurs in document}_{d} }{\\text{Total number of words in document}_{d} }$$\n\n**Inverse Document Frequency (IDF)**\n*  The log of the total number of documents divided by the number of documents containing word<sub>n<\/sub>. An additional term (1) will be added to the denominator to avoid divide-by-zero errors. The more frequently a word appears across all of the documents, the lower this value becomes, thus giving less weight to words that are generally more frequently used (i.e. stop words such as \"is\", \"when\", etc.)\n$$IDF \\text(Inverse Document Frequency)|_{word_i} = \\log(\\frac{\\text{Total number of documents}}{1 + \\text{Number of documents containing word}_{i}})$$\n\nThe two terms are multiplied together to give a measure of how important the word is within the corpus. (Higher values indicate greater weightage)\n$$TF\\text{-}IDF = \\text{TF} \\times \\text{IDF}$$\n\nInstead of raw word counts during the conditional probability calculation for each class, the raw word counts will now be scaled by the TF-IDF of that word. \n\n$$P(\\text{word}_i|\\text{class}_{c}) = \\frac{K_{compensated} + \\sum_d{(F_{word_i|d} \\times \\text{TF-IDF(word}_i\\text{|d)})}}{\\sum_{word_i}{(K_{compensated} + \\sum_d{(F_{word_i|d} \\times \\text{TF-IDF(word}_i\\text{|d)})})} }$$\nWhere:\n* $c$ represents class C (Star-Trek character)\n* $d$ represents a document (Script line)\n* $K_{compensated}$ is the compensated smoothing factor\n* $word_i$ represents the i-th word within the corpus\n\n**Remarks on smoothing-factor $K_{compensated}$**\n\nNote that in this case, the smoothing factor will be different on a per class basis. In other scenarioes without TF-IDF weighting applied, the smallest possible word weight in a class before smoothing is applied would be 1 (the raw word count). TF-IDF causes the minimum weight to vary from class to class (since TF-IDF is calculated on a per-class basis). As such, when using TD-IDF, the ratio between smoothing factor K and the smallest word weight varies across classes. \n\nTo remove this variance, the smoothing factor when using TF-IDF will thus be defined as:\n\n$$K_{compensated} =  K \\times \\arg\\min_i \\left( \\text{TF-IDF(word}_i\\text{)} \\right)|_{C}$$\n\nIn other words, the compensated smoothing factor for a class will be the normal smoothing factor $K$ multiplied by smallest TF-IDF weight within that class","e03e430a":"# Lemmatization\n\nWhile different grammatical contexts dictate the use of different word forms (pickle, pickling, pickled), sometimes the base meaning of the word is more important, with different grammatical forms only serving to add noise to the dataset. Lemmatization reduces these grammatical variants into a single word (i.e. pickle, pickling, pickled $\\rightarrow$ pickle)","20e67997":"# Data Preparation\n\nLines from every episode across every series will be combined into a single corpus.\nMost of the 2761 characters have barely any lines, and thus are unsuited to be used as output classes in the Bayes Classifier.\n\nOnly the top 3 characters (Picard, Kirk & Janeway) with the most lines will be used to create the training set to maintain a reasonable test classification accuracy (~ above 65%).\nExpanding the number of output classes drastically reduces test accuracy (~ 50% for 5 characters).","07c9561d":"# Introduction\n\nThis notebook will explore some variations on Naive Bayes and their impact on text classification accuracy.\nLines from the 3 most talkative (Picard, Kirk & Janeway) Star-Trek characters across all series and episodes will be used as the training set to the Bayes Classifiers.\nGiven a line (input), the bayes classifier will predict the most likely character who said it (output).\n\nThe following variations on Naive Bayes will be explored:\n1. Multinomial Naive Bayes (base case)\n   *    Effect of removing punctation + decapitalization\n   *    Effect of Lemmatization\n   *    Effect of applying TF-IDF (Term Frequency, Inverse Document Frequency)\n2. Complement Naive Bayes\n3. N-Gram Multinomial Naive Bayes\n\nAll 3 classifiers will be written from scratch for illustrative purposes. The Multinomial(1) & Complementary(2) variations will be cross-validated against SciPy's corresponding implementation to check for correctness. \n\n***Spoiler: None of the variations + data-preprocessing steps make a significant impact on classification accuracy***","88abb7a8":"# Multinomial Naive Bayes\n\nAccording to Baye's Theorem, the probabilty of class $C_k$ occuring given features $w$ is expressed as:\n\n$$p(C_k|w) = \\frac{p(x|C_k)p(C_k)}{P(w)}$$\n\nThe probabilty of the features occuring is ignored since it is indepedent of class $C_k$ and features $w$ are given. Hence it is treated as a constant and ignored, giving:\n\n$$p(C_k|w) = p(x|C_k)p(C_k)$$\n\nFor a given dataset, $p(C_k)$ can simply be computed as the number of instances of class $C_k$ divided by the total number of instances. In this case, it would be the number of lines by $\\text{character}_k$ divided by the total number of lines by all characters in the series.\n\nThe probability of feature $x$ occuring in class $C_k$ is computed in a similar fashion - as the number of times $x$ occurs in class $C_k$ divided by the total number of instances in the entire class. For this case, it would be the number of times a particular $\\text{word}_i$ has been spoken by a character divided by the total number of words the character has spoken throughout the entire series.\n\nSentences are a series of words. By making a fundamental assumption within the Naive Bayes model - that each feature is independent of each other - the probability of a string of words being spoken by $\\text{character}_k$ can be taken as the product of the probabilities of each word being spoken independently.\n\nHence,\n\n$$p(\\{w_1, w_2, w_3, w_{...}\\}|C_k) = \\prod_{i}{p(w_i|C_k)}$$\n$$=\\prod_{i}{\\frac{N_w|_c}{N_{total}|c}}$$\n\nWhere:\n\n* $N_w|_c$ is the number of times $\\text{word}_w$ is said by $\\text{character}_k$\n\n* $N_{total}|_c$ is the total number of words spoken by $\\text{character}_k$\n\nAs such, the probability of class $C_k$ occuring given word vector ${w_1, w_2, w_3, w_{...}}$ is:\n\n$$p(C_k|\\{w_1, w_2, w_3, w_{...}\\}) = \\prod_{i}{\\frac{N_w|_c}{N_{total}|c}} \\times \\frac{DN_c}{DN_{total}}$$\n\nWhere:\n\n* $DN_c$ is the number of lines spoken by $\\text{character}_k$\n\n* $DN_{total}$ is the total number of lines spoken by all characters\n\nTo use this as a predictive model, we simply compute the above probabilities from a dataset and choose the class that gives the highest likelihood of occuring given features $\\{w_1, w_2, w_3, w_{...}\\}$.\n\nHence,\n\n$$\\text{predicted class C}_k = \\text{arg}\\max_{k} (\\prod_{i}{\\frac{N_w|_{C_k}}{N_{total}|C_k}} \\times \\frac{DN_{C_k}}{DN_{total}})$$"}}