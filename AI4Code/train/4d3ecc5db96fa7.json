{"cell_type":{"ee40a956":"code","dfc6a8b2":"code","aee10b7e":"code","cefa9a1a":"code","7e92ad1f":"code","84ef130e":"code","f3b7cee5":"code","0f7e158a":"code","e201e246":"code","6ac25b2b":"code","a18aef77":"code","b3c6b8dc":"code","cc3177a3":"code","f413f438":"code","ad04258a":"code","a367afd6":"code","0f324419":"code","2caa0592":"code","cc082e9f":"code","0a343fee":"code","db2e21ec":"code","751c0cc5":"code","28925ee5":"code","466392dd":"code","4e0c39be":"code","dd3a086b":"code","7ba13c03":"code","31e2179a":"code","75b0ae7d":"code","de4ac87f":"code","3e8144e4":"code","23858e33":"code","7519bf85":"code","6ca83f4f":"code","affc0909":"code","8e54b477":"code","ce954677":"code","d573d2dc":"code","674078cf":"markdown","d5109213":"markdown","04b1b7dd":"markdown","ca465a93":"markdown","4c30880a":"markdown","6af6e21b":"markdown","9de3ac2e":"markdown","3c72c313":"markdown","54da2a6f":"markdown","11274654":"markdown","cb0e070c":"markdown","ef094bfe":"markdown","9ce033a0":"markdown","c6f19ece":"markdown","589664d6":"markdown","c6583563":"markdown","e8925850":"markdown","98f3eeca":"markdown","9703490a":"markdown","ac98979c":"markdown","5e9f0317":"markdown","b577e580":"markdown","28694776":"markdown","a90d35d9":"markdown","77cdd145":"markdown","bca232d9":"markdown","ee8e1700":"markdown","7734c1c6":"markdown","337c24c6":"markdown","798f582a":"markdown","6ceaa257":"markdown","71cbabf8":"markdown","e135e675":"markdown","38b888f1":"markdown","65856b98":"markdown","ca2c0c89":"markdown","cb561176":"markdown","75599162":"markdown","848331c4":"markdown"},"source":{"ee40a956":"! pip install scikit-learn==0.21\n! pip install hypopt","dfc6a8b2":"import sys\nimport json\nimport warnings\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport seaborn as sns\nimport tensorflow as tf\nimport numpy as np\nimport tensorflow_probability as tfp\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom hypopt import GridSearch\nfrom matplotlib import rc \nimport math\n\nplt.rcParams.update({'font.size': 5})\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\n\nrc('figure', **{'dpi': 200})\npd.set_option('display.max_columns', None)  ","aee10b7e":"X_dtype = {\n    'ID'                   : int,\n    'YEAR'                 : int,  \n    'MONTH'                : int,  \n    'DAY'                  : int,  \n    'DAY_OF_WEEK'          : int,  \n    'AIRLINE'              : str, \n    'FLIGHT_NUMBER'        : int,  \n    'TAIL_NUMBER'          : str, \n    'ORIGIN_AIRPORT'       : str, \n    'DESTINATION_AIRPORT'  : str, \n    'SCHEDULED_DEPARTURE'  : float,  \n    'DEPARTURE_TIME'       : float, \n    'DEPARTURE_DELAY'      : float,\n    'TAXI_OUT'             : float, \n    'WHEELS_OFF'           : float,\n    'SCHEDULED_TIME'       : float,\n    'AIR_TIME'             : float,\n    'DISTANCE'             : int,\n    'SCHEDULED_ARRIVAL'    : float,\n    'DIVERTED'             : int,  \n    'CANCELLED'            : int,  \n    'CANCELLATION_REASON'  : str\n}\n\ny_dtype = {\n    'ID'                   : int,\n    \"ARRIVAL_DELAY\"        : float\n}\n\nX_train_df = pd.read_csv(\"\/kaggle\/input\/eurecom-aml-2021-challenge-1\/data\/train_features.csv\", dtype=X_dtype)\ny_train_df = pd.read_csv(\"\/kaggle\/input\/eurecom-aml-2021-challenge-1\/data\/train_targets.csv\", dtype=y_dtype)","cefa9a1a":"airports = pd.read_csv('\/kaggle\/input\/eurecom-aml-2021-challenge-1\/data\/airports.csv',\n                       usecols=['IATA_CODE', 'AIRPORT', 'LATITUDE', 'LONGITUDE']).rename({'AIRPORT': 'NAME'}, axis='columns')\n\nairlines = pd.read_csv('\/kaggle\/input\/eurecom-aml-2021-challenge-1\/data\/airlines.csv')\n\n","7e92ad1f":"num_column_before_pre=[\n    'ID',\n    'YEAR',\n    'MONTH', \n    'DAY',  \n    'DAY_OF_WEEK',\n    'DEPARTURE_DELAY',\n    'SCHEDULED_TIME',\n    'AIR_TIME',\n    'DISTANCE',\n    'DIVERTED',  \n    'CANCELLED',\n    'ARRIVAL_DELAY',\n    'ORIGIN_LATITUDE',\n    'DESTINATION_LATITUDE',\n    'ORIGIN_LONGITUDE',\n    'DESTINATION_LONGITUDE',\n    'TIME_TO_GO',\n    'ARRIVAL_DELAY'\n\n]\n\nnum_column_after_pre=[\n    'ID',\n    'YEAR',\n    'MONTH', \n    'DAY',  \n    'DAY_OF_WEEK',\n    'DEPARTURE_DELAY',\n    'SCHEDULED_TIME',\n    'AIR_TIME',\n    'DISTANCE',\n    'ORIGIN_LATITUDE',\n    'DESTINATION_LATITUDE',\n    'ORIGIN_LONGITUDE',\n    'DESTINATION_LONGITUDE',\n    'ORIGIN_AIRPORT',\n    'DESTINATION_AIRPORT', \n    'SCHEDULED_DEPARTURE',  \n    'DEPARTURE_TIME',\n    'TAXI_OUT', \n    'WHEELS_OFF',\n    'SCHEDULED_ARRIVAL',\n    'FLIGHT_NUMBER',\n    'AIRLINE', \n    'TAIL_NUMBER',\n    'TIME_TO_GO',\n    'ARRIVAL_DELAY'\n\n]\n\nnum_column_after_pre = list( dict.fromkeys(num_column_after_pre) )\n\nstr_column_before_pre=[\n    'DAY_OF_WEEK',  \n    'AIRLINE', \n    'FLIGHT_NUMBER',  \n    'TAIL_NUMBER', \n    'ORIGIN_AIRPORT', \n    'DESTINATION_AIRPORT', \n    'SCHEDULED_DEPARTURE',  \n    'DEPARTURE_TIME',\n    'TAXI_OUT', \n    'WHEELS_OFF',\n    'SCHEDULED_ARRIVAL',\n    'CANCELLATION_REASON',\n    'DATE'\n]\n\nstr_column_after_pre=[\n    'ORIGIN_AIRPORT', \n    'DESTINATION_AIRPORT', \n    'CANCELLATION_REASON',\n    'DATE'\n]\n\n","84ef130e":"df= X_train_df.merge(airports[['LATITUDE', 'LONGITUDE','IATA_CODE']], how='left', left_on='ORIGIN_AIRPORT', right_on='IATA_CODE')\ndf = df.rename(columns={'LATITUDE': 'ORIGIN_LATITUDE', 'LONGITUDE':'ORIGIN_LONGITUDE'})\ndf = df.drop(['IATA_CODE'], axis=1)\n\n\ndf= df.merge(airports[['LATITUDE', 'LONGITUDE','IATA_CODE']], how='left', left_on='DESTINATION_AIRPORT', right_on='IATA_CODE')\ndf = df.drop(['IATA_CODE'], axis=1)\ndf = df.rename(columns={'LATITUDE': 'DESTINATION_LATITUDE', 'LONGITUDE':'DESTINATION_LONGITUDE'})\n\ndf = pd.merge(df, y_train_df, on='ID')\n\n","f3b7cee5":"#La formule du savoir\n\ndf['TIME_TO_GO']=df['TAXI_OUT']+df['DEPARTURE_DELAY']+df['AIR_TIME']-df['SCHEDULED_TIME']\ndf.TIME_TO_GO\n","0f7e158a":"def reformat(df_t):\n    ints =[\n    'DAY_OF_WEEK',  \n    'FLIGHT_NUMBER'\n    ]\n    \n    time=[\n    'SCHEDULED_DEPARTURE',  \n    'DEPARTURE_TIME',\n    'WHEELS_OFF',\n    'SCHEDULED_ARRIVAL'\n    ]\n    \n    strings =[\n    'AIRLINE', \n    'TAIL_NUMBER', \n    'ORIGIN_AIRPORT', \n    'DESTINATION_AIRPORT'\n        \n    ]\n\n    \n    df_t[ints]=df_t[ints].astype(int)\n    \n    for i in time:\n        df_t[i]=df_t[i].apply(lambda x: round(int(x\/100)+(x\/100 - int(x\/100))\/0.6, 4))\n        \n    df_t[strings]=df_t[strings].apply(lambda x: x.factorize()[0])\n\n    \n    return df_t\n    ","e201e246":"df_pre=df.copy()    \ndf_pre=reformat(df_pre)[num_column_after_pre]","6ac25b2b":"df_pre.head()","a18aef77":"#BASIC QUERIES\n\na=df_pre.ORIGIN_AIRPORT.drop_duplicates().count()\nb=df_pre.DESTINATION_AIRPORT.drop_duplicates().count()\nc=df_pre.TAIL_NUMBER.drop_duplicates().count()\nd=df_pre[df_pre.SCHEDULED_DEPARTURE>18.0].ID.count()\n\nprint(\"Number of unique airports (origin\/destination): \"+str(a) +\" and \"+str(b) )\nprint(\"Number of unique carriers : \"+str(c))\nprint(\"Number of planes schedule to take off later than 6 pm :\" +str(d))\n\n","b3c6b8dc":"#Statistic on flight volume\nplt.rcParams.update({'font.size': 5})\n\nplt.figure()\nfig, axes = plt.subplots(nrows=2, ncols=2)\nfig.set_size_inches(8,8)\n\ndf_month = df_pre.groupby('MONTH').ID.count()\ndf_month.plot(ax=axes[0,0], kind=\"bar\", title=\"Flights per months\")\n\ndf_day = df_pre.groupby('DAY_OF_WEEK').ID.count()\ndf_day.plot(ax=axes[0,1], kind=\"bar\",title=\"Flights per day of the week\")\n\ndf_carrier = df.groupby('TAIL_NUMBER').ID.count().sort_values(ascending=False).head(20)\ndf_carrier.plot(ax=axes[1,0], kind=\"bar\", title=\"20 busiest carries\", rot=90)\n\ndf1=df.groupby('DESTINATION_AIRPORT').ID.count()\ndf2=df.groupby('ORIGIN_AIRPORT').ID.count()\ndf_airport = pd.concat([df1,df2],axis =1).sum(axis=1).sort_values(ascending=False).head(20)\ndf_airport.plot(ax=axes[1,1], kind=\"bar\", title=\"20 busiest airports\")\n\nplt.tight_layout()\n","cc3177a3":"a=sns.pairplot(df_pre, y_vars=['ARRIVAL_DELAY'], x_vars=num_column_after_pre[:5], height=3, aspect=1)\nb=sns.pairplot(df_pre, y_vars=['ARRIVAL_DELAY'], x_vars=num_column_after_pre[5:10], height=3, aspect=1)\nc=sns.pairplot(df_pre, y_vars=['ARRIVAL_DELAY'], x_vars=num_column_after_pre[10:15], height=3, aspect=1)\nd=sns.pairplot(df_pre, y_vars=['ARRIVAL_DELAY'], x_vars=num_column_after_pre[15:20], height=3, aspect=1)\ne=sns.pairplot(df_pre, y_vars=['ARRIVAL_DELAY'], x_vars=num_column_after_pre[20:], height=3, aspect=1)\n","f413f438":"corrmat = np.abs(df_pre, axis=1).corr()\nmask = np.zeros_like(corrmat)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(12,8))\n    ax = sns.heatmap(corrmat, mask=mask, square=True, cmap=\"YlGnBu\")","ad04258a":"n = 8\ncols = corrmat.nlargest(n, 'ARRIVAL_DELAY')['ARRIVAL_DELAY'].index # List of highly correlated colums with 'SalePrice'\ncm = np.corrcoef(df_pre[cols].values.T)\nmask = np.zeros_like(cm)\nmask[np.triu_indices_from(cm)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(12, 9))\n    ax = sns.heatmap(cm, \n                     mask=mask, \n                     cbar=True, \n                     annot=True, \n                     square=True, \n                     fmt='.2f', \n                     annot_kws={'size': 10}, \n                     yticklabels=cols.values, \n                     xticklabels=cols.values,\n                     cmap=\"YlGnBu\"\n                    )\n    plt.show()","a367afd6":"#airline\n\ndf_airline=df.groupby('AIRLINE').ARRIVAL_DELAY.mean()\ndf_airline_flight=df.groupby('AIRLINE').ID.count()\ndf_tot = pd.concat([df_airline,df_airline_flight],axis =1)\n\nax1 = df_tot.ID.plot(kind='bar', alpha=0.7)\nax2 = df_tot.ARRIVAL_DELAY.plot(kind='bar',secondary_y=True, color='r', alpha=0.5)\n\nax1.set_ylim(-0.19e6, 1.5e6)\nax2.set_ylim(-2, 16)\n\n#df_airline_flight.plot(kind='bar', secondary_y=True, style='green')","0f324419":"\nax = sns.stripplot(y=\"AIRLINE\", x=\"DEPARTURE_DELAY\", size = 4, \n                    data=df, linewidth = 0.5,  jitter=True)","2caa0592":"airline_list=df.AIRLINE.drop_duplicates().tolist()\n\nplt.figure()\nfig, axes = plt.subplots(nrows=7, ncols=2)\nfig.set_size_inches(8,8)\ni=0\n\nfor x in airline_list:\n    a,b=int(i\/2),i%2\n    df[df['AIRLINE']==x]['DEPARTURE_DELAY'].hist(ax=axes[a,b], \n             bins=60, range = (15,180), density=1, legend=x)\n    i+=1\n    \nplt.tight_layout()\n\n    ","cc082e9f":"#geography\nimport geopandas as gpd\nfrom geopandas import GeoDataFrame\nfrom shapely.geometry import Point\n\ndf_meanO = df_pre.groupby(['ORIGIN_AIRPORT', 'ORIGIN_LATITUDE', 'ORIGIN_LONGITUDE']).ARRIVAL_DELAY.mean()\ndf_meanD = df_pre.groupby(['DESTINATION_AIRPORT', 'DESTINATION_LATITUDE', 'DESTINATION_LONGITUDE']).ARRIVAL_DELAY.mean()\n\n\ngeometryO, geometryD=[],[]\n\nfor i in range(df_meanO.size):\n    x,y= df_meanO.index[i][2], df_meanO.index[i][1]\n    xy = (x,y)\n    geometryO.append(Point(xy))#[Point(xy) for x in zip(df_geo_dep['LONGITUDE'], df_geo_dep['LATITUDE'])]\ngdfO = GeoDataFrame(df_meanO, geometry=geometryO)\n\nfor i in range(df_meanD.size):\n    x,y= df_meanD.index[i][2], df_meanD.index[i][1]\n    xy = (x,y)\n    geometryD.append(Point(xy))#[Point(xy) for x in zip(df_geo_dep['LONGITUDE'], df_geo_dep['LATITUDE'])]\ngdfD = GeoDataFrame(df_meanD, geometry=geometryD)\n\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\nusa = world.iloc[[4]]\ngdfO.plot(ax=usa.plot(figsize=(10, 6)),column= 'ARRIVAL_DELAY' , legend=True, vmax=40, vmin=-20,markersize=35,cmap='OrRd')\n\ngdfD.plot(ax=usa.plot(figsize=(10, 6)),column= 'ARRIVAL_DELAY' , legend=True, vmax=40, vmin=-20,markersize=35,cmap='OrRd')\n","0a343fee":"df_busiestO=df.groupby('ORIGIN_AIRPORT').ID.count().sort_values(ascending=False).head(14)\nsO=df_busiestO.index.tolist()\ndf_newO = df[df['ORIGIN_AIRPORT'].isin(sO)]\n\ndf_busiestD=df.groupby('DESTINATION_AIRPORT').ID.count().sort_values(ascending=False).head(14)\nsD=df_busiestD.index.tolist()\ndf_newD = df[df['DESTINATION_AIRPORT'].isin(sD)]\n\ndf_newO=df_newO[['ORIGIN_AIRPORT', 'DEPARTURE_DELAY']]\ndf_newO['BIN']='ORIGIN'\ndf_newO=df_newO.rename(columns={'ORIGIN_AIRPORT':'AIRPORT'})\ndf_newO.head()\n\ndf_newD=df_newD[['DESTINATION_AIRPORT', 'DEPARTURE_DELAY']]\ndf_newD['BIN']='DESTINATION'\ndf_newD=df_newD.rename(columns={'DESTINATION_AIRPORT':'AIRPORT'})\ndf_newD.head()\n\ndf_concat=pd.concat([df_newD, df_newO]).reset_index(drop=True)","db2e21ec":"colors = ['firebrick', 'gold', 'lightcoral', 'aquamarine', 'c', 'yellowgreen', 'grey',\n          'seagreen', 'tomato', 'violet', 'wheat', 'chartreuse', 'lightskyblue', 'royalblue']\n\nax1=sns.stripplot(y='AIRPORT', x=\"DEPARTURE_DELAY\", size = 4, hue='BIN', \n                    data=df_concat, linewidth = 0.5,  jitter=True, dodge=True, color=colors, palette = colors)","751c0cc5":"# First of all, you should split the data into a training set and a validation set\nX_train_df, X_val_df, y_train_df, y_val_df = train_test_split(\n    df_pre, y_train_df, random_state=1, test_size=0.2)","28925ee5":"# Here we use 7 features as follows\nfeature_names = [ 'TIME_TO_GO', 'AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT']\ntarget_name = ['ARRIVAL_DELAY']\n\nX_train_df = X_train_df[feature_names]\ny_train_df = y_train_df[target_name]\n\nX_val_df = X_val_df[feature_names]\ny_val_df = y_val_df[target_name]","466392dd":"# Filling missing values by the mean along each column.\n# These statistics should be estimated by using the training set.\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer.fit(X_train_df)\n\nX_train = imputer.transform(X_train_df)\nX_val = imputer.transform(X_val_df)","4e0c39be":"# Standardize the features by removing the mean and scaling to unit variance\n# Similarly to the preivous step, the statistics used for standardization\n# should be computed across the training set only\nX_scaler = StandardScaler()\nX_scaler.fit(X_train)\n\nX_train = X_scaler.transform(X_train)\nX_val = X_scaler.transform(X_val)","dd3a086b":"# We should also standardize the targets.\ny_scaler = StandardScaler()\ny_scaler.fit(y_train_df)\n\ny_train = y_scaler.transform(y_train_df)\ny_val = y_scaler.transform(y_val_df)","7ba13c03":"def preprocess_data(df, feature_names, imputer, scaler):\n    \"\"\"Preprocess data.\n\n    Parameters\n    ----------\n    df: pandas DataFrame.\n        The input data.\n    feature_names: list of strings.\n        The names of selected features.\n    imputer: sklearn.impute.SimpleImputer\n        The imputation transformer for completing missing values.\n    scaler: sklearn.preprocessing.StandardScaler.\n        The scaler used to normalize the features.\n\n    Returns\n    -------\n    X: numpy array.\n        The preprocessed data.\n    \"\"\"\n    \n    df['TIME_TO_GO']=df['TAXI_OUT']+df['DEPARTURE_DELAY']+df['AIR_TIME']-df['SCHEDULED_TIME']\n    \n    ints =[\n    'DAY_OF_WEEK',  \n    'FLIGHT_NUMBER'\n    ]\n    \n    time=[\n    'SCHEDULED_DEPARTURE',  \n    'DEPARTURE_TIME',\n    'WHEELS_OFF',\n    'SCHEDULED_ARRIVAL'\n    ]\n    \n    strings =[\n    'AIRLINE', \n    'TAIL_NUMBER', \n    'ORIGIN_AIRPORT', \n    'DESTINATION_AIRPORT'\n        \n    ]\n\n    \n    df[ints]=df[ints].astype(int)\n    \n    for i in time:\n        df[i]=df[i].apply(lambda x: round(int(x\/100)+(x\/100 - int(x\/100))\/0.6, 4))\n        \n    df[strings]=df[strings].apply(lambda x: x.factorize()[0])\n    \n    \n    \n    # Select features\n    X_df = df[feature_names]\n\n\n    # Impute missing values\n    X = imputer.transform(X_df)\n    \n    # Normalize features\n    X = scaler.transform(X)\n\n    return X","31e2179a":"from sklearn import neighbors, datasets, preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.svm import SVR\nimport xgboost as xgb","75b0ae7d":"def make_prediction(X, model, scaler):\n    \"\"\"Makes predictions given a preprocessed dataset.\n\n    Parameters\n    ----------\n    X: numpy array.\n        The input data, which already is pre-processed.\n    model: an hypopt or sklearn model.\n        The trained model used for making predictions.\n    scaler: sklearn.preprocessing.StandardScaler.\n        The scaler used to normalize the targets.\n\n    Returns\n    -------\n    y_pred: numpy array.\n        The unnormalized predictions.\n    \"\"\"\n    y_pred = scaler.inverse_transform(model.predict(X))\n    return y_pred","de4ac87f":"knn = neighbors.KNeighborsRegressor(n_neighbors=6)\nknn.fit(X_train, y_train)\ny_train_pred = make_prediction(X_train, knn, y_scaler)\ntrain_rmse = np.sqrt(mean_squared_error(y_train_pred, y_train_df.values))\nprint(\"Training KNN RMSE: {:.5f}\".format(float(train_rmse)))\nprint(\"-------------------------\")\ny_val_pred = make_prediction(X_val, knn, y_scaler)\nval_rmse = np.sqrt(mean_squared_error(y_val_pred, y_val_df.values))\nprint(\"Validation KNN RMSE: {:.5f}\".format(float(val_rmse)))\nprint(\"--------------------------------\")","3e8144e4":"#num_neighb = []\n#results = []\n#for k in range(1,10):\n#    knn = neighbors.KNeighborsRegressor(n_neighbors=k)\n#    knn.fit(X_train, y_train)\n#    y_train_pred = make_prediction(X_train, knn, y_scaler)\n#    train_rmse = np.sqrt(mean_squared_error(y_train_pred, y_train_df.values))\n#    print(\"--------------\"+str(k)+\"---------------\")\n#    print(\"Training KNN RMSE: {:.5f}\".format(float(train_rmse)))\n#    print(\"-------------------------\")\n#    y_val_pred = make_prediction(X_val, knn, y_scaler)\n#    val_rmse = np.sqrt(mean_squared_error(y_val_pred, y_val_df.values))\n #   print(\"Validation KNN RMSE: {:.5f}\".format(float(val_rmse)))\n #   print(\"-------------------------\")\n#    num_neighb.append(k)\n #   results.append([train_rmse, val_rmse])","23858e33":"#plt.plot(num_neighb, results)","7519bf85":"rfr = RandomForestRegressor(max_depth=9)\nrfr.fit(X_train,y_train)\n\ny_train_pred = make_prediction(X_train, rfr, y_scaler)\ntrain_rmse = np.sqrt(mean_squared_error(y_train_pred, y_train_df.values))\nprint(\"Training Random Forest RMSE: {:.5f}\".format(float(train_rmse)))\nprint(\"-------------------------\")\ny_val_pred = make_prediction(X_val, rfr, y_scaler)\nval_rmse = np.sqrt(mean_squared_error(y_val_pred, y_val_df.values))\nprint(\"Validation Random Forest RMSE: {:.5f}\".format(float(val_rmse)))\nprint(\"-------------------------\")\n","6ca83f4f":"#rfr = RandomForestRegressor(max_depth=k)\n#rfr.fit(X_train,y_train)\n#y_train_pred = make_prediction(X_train, rfr, y_scaler)\n#train_rmse = np.sqrt(mean_squared_error(y_train_pred, y_train_df.values))\n#print(\"Training Random Forest RMSE: {:.5f}\".format(float(train_rmse)))\n#print(\"-------------------------\")\n#y_val_pred = make_prediction(X_val, rfr, y_scaler)\n#val_rmse = np.sqrt(mean_squared_error(y_val_pred, y_val_df.values))\n#print(\"Validation Random Forest RMSE: {:.5f}\".format(float(val_rmse)))\n#num_depthsRFR.append(k)\n#resultsRFR.append([train_rmse, val_rmse])\n\n#X = [k for k in range(len(y_val_pred))]","affc0909":"#plt.plot(X[0:200],y_val_pred[0:200],y_val_df.values[0:200])","8e54b477":"values = rfr.feature_importances_\nplt.bar(feature_names, values)\nvalues","ce954677":"sns.jointplot(x=y_val_pred,y=y_val_df['ARRIVAL_DELAY'], kind='reg')","d573d2dc":"import os\n\nos.system('jupyter nbconvert --to html yourNotebook.ipynb')","674078cf":"### Correlation Analysis\n\nNow, let's move on to some analysis of our own, shall we?\nFirst let's plot the arrival delay with regards to all features of the pre-processed dataframe. Double click on the output to zoom-in.","d5109213":"Here  we only use the 4 features deemed interesting.","04b1b7dd":"Let's reorganize that matrix only selecting the 8 most correlated features and sorting them.","ca465a93":"#### The K Nearest Neighbours Regressor\n\u200b\nThe first implementation which worked well was the K Nearest Neighbours Regressor of scikit-learn. There were some advantages in terms of time and of parameter to use because this is a free parameter model.The advantage is that the model is really quick to set up since there are no particular parameters to calculate. The time become a bit longer in terms of prediction because even if there are no \"training\", it needs to search the \"nearest neighbors\" in all the dataset which takes time with a huge dataset. Another drawback at first was the error which was satisfying and better than the previous regression (around 21) but not enough at all. But then with the choice of fetaures, we've came up with some good results ( around 6 of training and validation RMSE). But we were thinking that, for a KNN it doesn't make huge sense to ask which features are the better or most useful since there are no weights, no training. That is why we oriented towards an other regression which was our first assumption of model : a Random Forest Regressor","4c30880a":"There is seemingly no correlation between the size of the airline and the average delay. Therefore, we cannot simply assume that the biggest, the worst.\n\nLet's plot the delay with reards to each airlines. ","6af6e21b":"The *preprocess_data()* function has been modified by adding to it the *format()* and by creating the additional column *TIME_TO_GO*.","9de3ac2e":"## Data analysis\n### Overview\n\nFirst we explore the data by answering the questions. \n\n- Basic queries:\n  - How many unique origin airports?\n  - How many unique destination airports?\n  - How many carriers?\n  - How many flights that have a scheduled departure time later than 18h00?\n\n\n- Statistics on flight volume: this kind of statistics are helpful to reason about delays. Indeed, it is plausible to assume that \"*the more flights in an airport, the higher the probability of delay*\".\n  - How many flights in each month of the year?\n  - Is there any relationship between the number of flights and the days of week?\n  - How many flights in different days of months and in different hours of days?\n  - Which are the top 20 busiest airports (this depends on inbound and outbound traffic)?\n  - Which are the top 20 busiest carriers?\n\n","3c72c313":"The graph bellow allows us to see that most of the datas are around a little delay, but also that there is a sort of bias around these little delay. Indeed we find back the shape of a similar plot with TIME_TO_GO in the data analysis. This comfort us in our feeling and result that TIME_TO_GO is a real useful feature.","54da2a6f":"#### Airlines\n\nAs the correlation matrix could not help with airlines, let's compare them by hand. First intuition : the busiest the airline, the bigger it is. A big company needs to manage a bigger crew, a bigger fleet and more passengers + belongings. As crew and passenger lateness account for a great portition of delay, it is a reasonable assumption. \n\nHence let's plot the average delay (red) and size(blue) of each airlines.","11274654":"### Imports","cb0e070c":"**To conclude, we shall consider the 4 following features for training purpose:**\n- TIME_TO_GO (~DEPARTURE_DELAY)\n- AIRLINES\n- ORIGIN_AIRPORT\n- DESTINATION_AIRPORT\n","ef094bfe":"#### Airports\n\nFirst, let's plot the average delay on a map for each airport. We need to do it for both departure and arrival airports.\n","9ce033a0":"The result is similar to the one we got for the airlines. There seems to be better airports than others when it comes to flight delay.\n\n","c6f19ece":"These queries give us a sense of what we are dealing with and help us understand the dataset better.","589664d6":"This representation gives a feeling on the dispersion of data, and shows very different distributions for each airline. It puts in perspective the relative homogeneity that appeared in the average delay plot. Indeed, we see that while all mean delays are around 10 minutes, this low value is a consequence of the fact that a majority of flights take off on time. However, we see that occasionally, we can face really large delays that can reach a few tens of hours.\n\nNow that we're done with the qualitative analysis, let's dive in the quantitative one. The goal is to estimate the distribution of the delay for each airline in order to rank them. To do so, let's plot the histograms and fit an their contours.","c6583563":"Then, let's create a reformating function. It takes a dataframe as parameter and spits out the processed one. It converts some columns back to integers, reformat time as usable floats and encode strings to unique IDs. Converting strings to numerical values will be helpful when plotting the correlation matrix later on.","e8925850":"# Challenge 1\n# Airplane delay: Analysis and Prediction\n\nNicolas SERVOT, Paul MICHEL\n\nGroupe 3\n\n","98f3eeca":"Below is a very basic example of pre-processing steps.","9703490a":"## Data Pre-processing\n\nThe previous step should give you a better understanding of which pre-processing is required for the data.\nThis may include:\n\n- Normalising and standardising the given data;\n- Removing outliers;\n- Carrying out feature selection, possibly using metrics derived from information theory;\n- Handling missing information in the dataset;\n- Augmenting the dataset with external information;\n- Combining existing features.","ac98979c":"## Model Selection\n\u200b\nFor the model selection we had to pass through different steps in order to come up with our last final result : the Random Forest Regressor. The random forest was already a good choice since the beginning but we wanted to go step by step.\n","5e9f0317":"Loading the airport, airline and target csvs in dataframes","b577e580":"Then, let's join the longitude and latitude from airport to the main dataframe.","28694776":"### Random Forest Analysis","a90d35d9":"Each distribution is different. In all 14 cases, the delay follows an inverserve exponential distribution with different parameters. Long tail indicates that an airline is more likely to have significant delays, which is undesirable. Therefore longtail airports can be classified as worst than others. \n\nTherefore, it is an important feature to consider for the training of the model.\n\n","77cdd145":"![179235421_132633502180969_6053571332077337894_n.png](attachment:478cd8a8-7f3a-44c1-8b6b-d18380507fd0.png)","bca232d9":"#### The Random Forest Regressor\n\u200b\nNow the model is effective, there is also a part of logic to use it for this case since we have features which drive in one branch or another of a decision tree. The model seems to be appropriate, then we have tested it with the initial set features provided and the result is way better (around 10). We think that this model should be the more appropriate since, after thinking about Neural Network and Deep Neural Network we think that the training would be too long for the dataset (and we have tested it for the NN and the DNN), so in terms of computing power and time the Random Forest Regressor is a good choice. By using it with our choice of features, we have discovered some really good results but the training error and validation error were not the same : it can be a problem of overfitting. Then we have decided to work with the parameters to avoid this overfitting problem. ","ee8e1700":"As we did for the airlines, we now need to go deeper for each departure airports. However, we need to make a selection as there are 628 of them. Let's choose the 14 busiest ones, and plot the delay distribution.","7734c1c6":"## Parameter Optimisation \n\nIrrespective of your choice, it is highly likely that your model will have one or more parameters that require tuning.\nThere are several techniques for carrying out such a procedure, including cross-validation, Bayesian optimisation, and several others.\nAs before, an analysis into which parameter tuning technique best suits your model is expected before proceeding with the optimisation of your model.","337c24c6":"### Loading data","798f582a":"*df_pre* is the pre-processed data ready for data analysis.","6ceaa257":"With a correlation of nearly 1, *TIME_TO_GO* is a feature that needs to be considered for the training of the model. This feature is an improved version of *DEPARTURE_DELAY* as it also takes into account the taxi and air times.","71cbabf8":"\u200b\n#### A word on Neural Network ..\n\u200b\nThe Multi Layer Perceptron regressor is a Neural Network but we know that with a huge dataset like we have, this is way to slow for the training, so in terms of timing we didn't choose this model. Indeed we have tested it at several times and the training was too long so we stopped it and took a look at the results but the error was too high, because the training not ended.\n\u200b\n","e135e675":"We wanted to see how the KNN model could work if we optimize the only parameter for this model : the number of neighbors. Below is the plot for this search of optimization.\n\nNOTE : the plot is a screenshot resulting of the commented code below. It is commented as it takes about 2 hours to run.","38b888f1":"That gives us several hints on the correlation between some features and the target.\n- DEPARTRE_DELAY is seems strongly correlated to ARRIVAL_DELAY\n- TIME_TO_GO is looking even more promissing. This was expected is its value is computed using TAXI_OUT, AIR_TIME and DEPARTURE_DELAY.\n\nHowever, we cannot conclude anything about numerically encoded string values such as the TAIL_NUMBER or the AIRLINE. These features will be looked at later on.\n\n\nLet's go more into detail and plot the correlation matrix.","65856b98":"We can see with the graph above that the prediction seems to fit quite well the original distribution. Indeed the high delays are respected but the part for low delay are more discutable. Indeed we think that it is difficult to predict perfectly a delay of for example 10 minutes because it is more sensible to perturbations. \n\nBelow is an interesting graph showing the important of the features during the training of the Random Forest. We can see that TIME_TO_GO which is the more useful is a logic feature since it is directly linked to the Departure delay.","ca2c0c89":"## Data Preparation\n\nSelecting useful data from the target features, the airport and the airline dataframes. \nFirst, let's define the columns needed for the data reformatting.\n","cb561176":"![179669943_189322399698252_875678966591621494_n.png](attachment:f4c4c1d9-3717-4da8-92e9-94c1aadd52ff.png)","75599162":"Let's create another column to get the actual time elapsed during the flight, minus the taxi-in phase. We'll call it 'TIME_TO_GO'.","848331c4":"NOTE : the plot is a screenshot resulting of the commented code below. It is commented as it takes about 1h30 to run."}}