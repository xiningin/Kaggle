{"cell_type":{"eb7a4a33":"code","6be70132":"code","4846ac38":"code","9acd8ac5":"code","9bf99500":"code","440c0375":"code","02a31928":"code","bc829952":"code","117206e2":"code","dd61d01e":"code","e030103c":"code","bca5c8bd":"code","39b716fb":"code","f852692f":"code","eb57d47f":"code","a152808c":"code","83c21303":"code","58f76210":"code","65008fb5":"code","f5b561c2":"code","1f8bcee2":"code","fe28d6d7":"code","9075cb54":"code","8c6bd04d":"markdown","0d7100f6":"markdown","5a3bfff0":"markdown","d8167b11":"markdown","88ca6814":"markdown","05e556a5":"markdown","bea82f8d":"markdown","c36027e2":"markdown","0ea718b5":"markdown","675102e2":"markdown","01c3f59c":"markdown","509591a7":"markdown","9bc05348":"markdown","354e033e":"markdown","c6e45145":"markdown","780344ef":"markdown"},"source":{"eb7a4a33":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6be70132":"#Importing all the required modules\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nimport calendar\nimport matplotlib.dates as mdates","4846ac38":"#Reading the data\ncal_data = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')\nprices = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv')\nsales = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')","9acd8ac5":"print(cal_data.shape)\nprint(prices.shape)\nprint(sales.shape)","9bf99500":"#Viewing the first five rows of sales data\nsales.head() ","440c0375":"print('There are {0} items '.format(len(sales['item_id'].unique())))\nprint('There are {0} depts'.format(len(sales['dept_id'].unique())))\nprint('There are {0} categories'.format(len(sales['cat_id'].unique())))\nprint('There are {0} stores'.format(len(sales['store_id'].unique())))\nprint('There are {0} states'.format(len(sales['state_id'].unique())))","02a31928":"#Copying the sales dataframe so that modifications can be made and the original dataframe be kept intact\nsales_df = sales.copy()","bc829952":"date_list = [d.strftime('%Y-%m-%d') for d in pd.date_range(start = '2011-01-29', end = '2016-04-24')]","117206e2":"#Renaming days to dates\nsales_df.rename(columns=dict(zip(sales_df.columns[6:], date_list)),inplace=True)\nsales_df.head()","dd61d01e":"#Aggregating by mean the sales by department\ndept_mean = sales_df.groupby(['dept_id']).mean().T\ndept_mean.index = pd.to_datetime(dept_mean.index)\n\n#Aggregating by mean the sales by categories\ncat_mean = sales_df.groupby(['cat_id']).mean().T\ncat_mean.index = pd.to_datetime(cat_mean.index)\n\n#Aggregating by mean the sales by stores\nstore_mean = sales_df.groupby(['store_id']).mean().T\nstore_mean.index = pd.to_datetime(store_mean.index)\n\n#Aggregating by mean the sales by states\nstate_mean = sales_df.groupby(['state_id']).mean().T\nstate_mean.index = pd.to_datetime(state_mean.index)\n","e030103c":"#Function for creating plots\ndef create_plots(df,freq):\n    fig, ax = plt.subplots()\n    for i in df.columns:\n        df_plot = df[i].resample(freq).sum()\n        df_plot.plot(ax=ax)\n        fig.set_figheight(7)\n        fig.set_figwidth(15)\n    plt.grid(True)\n    ax.legend(df.columns,loc='best')","bca5c8bd":"#Plotting the mean data\ncreate_plots(dept_mean,'m')\ncreate_plots(cat_mean,'m')\ncreate_plots(store_mean,'m')\ncreate_plots(state_mean,'m')","39b716fb":"#To plot data in a particular date range\nfig, ax = plt.subplots(figsize=(15,5))\nstate_mean.plot(xlim=['2012-01-01','2014-01-01'],ax=ax,rot=90)\nplt.grid(True)\nplt.xlabel('Sales by State')\n# set ticks every week\nax.xaxis.set_major_locator(mdates.MonthLocator())\n# #set major ticks format\nax.xaxis.set_major_formatter(mdates.DateFormatter('%d %b'))","f852692f":"cal_data.head(31)","eb57d47f":"print(cal_data['event_name_1'].notnull().sum())\nprint(cal_data['event_name_2'].notnull().sum())","a152808c":"print(len(cal_data['event_name_1'].unique()))\nprint(len(cal_data['event_type_1'].unique()))","83c21303":"prices.head()","58f76210":"prices['sell_price'].hist(bins=50)\nplt.xlim(0,25)","65008fb5":"#Checking the price range of each department\nprices[(prices['item_id'].str.startswith('FOODS_1'))]['sell_price'].hist()\nplt.xlabel('FOODS_1')\nplt.show()\nprices[(prices['item_id'].str.startswith('FOODS_2'))]['sell_price'].hist()\nplt.xlabel('FOODS_2')\nplt.show()\nprices[(prices['item_id'].str.startswith('FOODS_3'))]['sell_price'].hist()\nplt.xlabel('FOODS_3')\nplt.show()\nprices[(prices['item_id'].str.startswith('HOUSEHOLD_1'))]['sell_price'].hist()\nplt.xlabel('HOUSEHOLD_1')\nplt.show()\nprices[(prices['item_id'].str.startswith('HOUSEHOLD_2'))]['sell_price'].hist()\nplt.xlabel('HOUSEHOLD_2')\nplt.show()\nprices[(prices['item_id'].str.startswith('HOBBIES_1'))]['sell_price'].hist()\nplt.xlabel('HOBBIES_1')\nplt.show()\nprices[(prices['item_id'].str.startswith('HOBBIES_2'))]['sell_price'].hist()\nplt.xlabel('HOBBIES_2')\nplt.show()","f5b561c2":"#Get the average selling price of each item\navg_price = prices.groupby(['item_id'])['sell_price'].mean()\n#Merge it with sales data\nmerged = pd.merge(sales_df,avg_price, right_index=True, left_on='item_id')\n#Group the merged that by id \nid_grouped = merged.groupby(['id']).sum()\n#Sum by days to get total quantity\nid_grouped['Total_Qty'] = id_grouped.sum(axis=1)\n#Get the total amount sold by multiplying the total quantity and selling price\nid_grouped['Amount_Sold'] = id_grouped['Total_Qty'] * id_grouped['sell_price']\n#Remove duplicate columns to merge data with sales\ncols_to_use = id_grouped.columns.difference(sales_df.columns)\n#Store the final df in new_sales\nnew_sales = pd.merge(sales_df,id_grouped[cols_to_use], right_index=True, left_on='id')","1f8bcee2":"new_sales.groupby(['dept_id','store_id'])['Total_Qty'].agg('mean').unstack().plot(kind='bar',figsize=(15,7))\nplt.title('Mean Quantity Sold by Department in each store')","fe28d6d7":"new_sales.groupby(['dept_id','store_id'])['Total_Qty'].agg('mean').unstack().T.plot(kind='bar',figsize=(15,7))\nplt.title('Mean Quantity Sold by Each Store of each Department')","9075cb54":"WI_2 = sales_df[(sales_df['store_id'] == 'WI_2')]\ndept_WI2 = WI_2.groupby(['dept_id']).sum().T\ndept_WI2.index = pd.to_datetime(dept_WI2.index)\ndept_WI2.head()\n\nCA_2 = sales_df[(sales_df['store_id'] == 'CA_2')]\ndept_CA2 = CA_2.groupby(['dept_id']).sum().T\ndept_CA2.index = pd.to_datetime(dept_CA2.index)\ndept_CA2.head()\n\nfig, ax = plt.subplots(figsize=(15,5))\ndept_CA2.plot(xlim=['2015-01-01','2016-01-01'],ax=ax,rot=90)\nplt.grid(True)\nplt.xlabel('Sales by Category')\n# set ticks every week\nax.xaxis.set_major_locator(mdates.MonthLocator())\n#set major ticks format\nax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\nplt.title('CA Plot 2015-16 ')\nplt.show()\n\nfig, ax = plt.subplots(figsize=(15,5))\ndept_WI2.plot(xlim=['2012-01-01','2013-01-01'],ax=ax,rot=90)\nplt.grid(True)\nplt.xlabel('Sales by Category')\n# set ticks every week\nax.xaxis.set_major_locator(mdates.MonthLocator())\n#set major ticks format\nax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\nplt.title('WI Plot 2012-13 ')\nplt.show()","8c6bd04d":"#### There are 162 rows where event_name_1 is not null and only 5 rows where event_name_2 is not null","0d7100f6":"#### We can see that the sales fall to zero just before january, this is because walmart is closed on christmas","5a3bfff0":"## Calendar data EDA done, starting EDA on prices data","d8167b11":"## Ending the EDA of sales, starting the EDA of <i>calender<\/i> data\n","88ca6814":"## Starting the EDA with Sales Data","05e556a5":"#### Plotted WI and CA Plot in different period as saw anomaly earlier, we can see that there's a change in level of data, interesting part is that both are around June and in both FOODS_3 and FOOS_2 have increasing sales ","bea82f8d":"#### Each entry in sales data corresponds to sales quantity of each item in a store across all days<br>\n#### For eg: the first row coreesponds to item 1 of hobbies_1 deparment in state california in store 1<br>\n#### We also saw that sales data has 1919 columns, among which 6 columns are different type of ids and other 1913 columns are days from d_1 to d_1913. The sales date starts from 29-Jan-2011 and ends on 24-April-2016 ","c36027e2":"#### There are total of 31 unique events which belong to 5 unique types, and as we saw before 162 rows, the data given is of 5 years, so these events occur every year","0ea718b5":"#### We can see that FOODS_3 sell the highest values of sales  \n#### Foods sell the most\n#### CA_3 sell the most, WI_2 started off low, but it had a sudden increase in level in ending of 1st quarter in 2012\n#### WI_3 went downhill at the begining of 2013, CA_2 had a decreasing trend throughout 2014 but increasing trend in 2015","675102e2":"#### Sell prices lie between 0 and 25, where most of them lies between 0 and 10","01c3f59c":"#### We can see that SNAP follows a certain pattern in 3 respective states, In CA, SNAP is allowed on first ten days, TX follows pattern 101-011, WI follows pattern 011","509591a7":"#### This data contains the details regarding events on each day and it also shows on which days SNAP purchases are allowed\n","9bc05348":"> Business Problem\nUse hierarchical sales data from Walmart, the world\u2019s largest company by revenue, to forecast daily sales for the next 28 days.\nThe data, covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events. Together, this robust dataset can be used to improve forecasting accuracy.","354e033e":"The United States federal government provides a nutrition assistance benefit called the Supplement Nutrition Assistance Program (SNAP).  SNAP provides low income families and individuals with an Electronic Benefits Transfer debit card to purchase food products.  In many states, the monetary benefits are dispersed to people across 10 days of the month and on each of these days 1\/10 of the people will receive the benefit on their card.  ","c6e45145":"#### Calender data has 1969 rows with 14 columns, prices has 68,41,121 rows with 4 columns and sales has 30490 rows with 1919 columns","780344ef":"#### Here we have viewed the price range of all departments"}}