{"cell_type":{"4a67ba1d":"code","0e8b23d9":"code","4ff1c7e9":"code","793b3c55":"code","13f60f78":"code","d7e3ee4b":"code","64982fd6":"code","6e7d270c":"code","e6f475a9":"code","43ea37d5":"code","b7a78b14":"code","c24d17a3":"code","994be25e":"code","85ea3395":"code","2ce54829":"code","ec15a00d":"code","c4f0156c":"code","69dc1958":"code","aa5bc840":"code","98d35e43":"code","9f4c61b5":"code","68e07ca3":"code","2649f7f0":"code","3425d31e":"code","306333ee":"code","1115f9b1":"code","d343139e":"code","45300ffa":"code","2b8c5c22":"code","bc1092fa":"code","12e19398":"code","d957a148":"code","37f72b72":"code","0b857d72":"code","5b843dda":"code","c9292d89":"code","764b1535":"code","c91f24c9":"code","a3dd052c":"code","62df0677":"code","d07846bf":"code","ed80993f":"code","3e3f6848":"code","4c1cb5c8":"code","6d389ea6":"markdown","824c6a6d":"markdown","58b1c999":"markdown","19a0e098":"markdown","c5a0cb43":"markdown","6a8d5446":"markdown","bd8b7aca":"markdown","91c0289d":"markdown","5adb94ab":"markdown","b49043b0":"markdown","f5d63ca6":"markdown","0e041175":"markdown","622436e0":"markdown","0624f134":"markdown","29f7c5a0":"markdown","85c6cdc2":"markdown","b45bda5d":"markdown","2a6ad827":"markdown","78b0a6eb":"markdown"},"source":{"4a67ba1d":"import os, logging, gc\nfrom time import time\nimport pandas as pd\nimport numpy as np\nimport time\n\npd.set_option(\"display.max_columns\", 50)\n\nimport warnings\n\nwarnings.filterwarnings(action='ignore')\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.pylab import rcParams \n\n%matplotlib inline\nrcParams['figure.figsize'] = 15, 8\n\nseed = 515\nnp.random.seed(seed)","0e8b23d9":"def missing_values_table(df):\n    #\n    # Function to explore how many missing values (NaN) in the dataframe against its size\n    # Args:\n    #   df: the input dataframe for analysis\n    # \n    # Return:\n    #   mis_val_table_ren_columns: dataframe table contains the name of columns with missing data, # of missing values and % of missing against total\n    #\n    mis_val = df.isnull().sum()\n    mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n    mis_val_table_ren_columns = mis_val_table.rename(columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n    mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1] != 0].sort_values('% of Total Values', ascending=False).round(1)\n    print(\"Your selected dataframe has \" + str(df.shape[1]) + \" columns and rows of \" + str(df.shape[0]) + \".\\n\" \"There are \" + str(mis_val_table_ren_columns.shape[0]) + \" columns that have missing values.\\n\")\n    return mis_val_table_ren_columns\n\n\ndef read_data(filename, nrows=10):\n    #\n    # Function to read the csv file onto the panda dataframe\n    # Args:\n    #   filename: The name of csv file\n    #   nrows: number of rows to be read. Default is 10 rows. None will read all rows\n    #\n    # Return:\n    #  df: panda dataframe containing the data from csv file\n    #\n    if(os.path.isfile(filename)):\n        print(\"\\nReading file:: {}\\n\".format(filename))\n        df = pd.read_csv(filename, sep = ',', nrows = nrows)\n        df.columns = [x.lower() for x in df.columns]\n        print(\"\\n=======================================================================\")\n        print(\"Sample records: \\n\", df.head(2))\n        print(\"\\n=======================================================================\")\n        print(\"The data type: \\n\", df.columns.to_series().groupby(df.dtypes).groups)\n        print(\"\\n=======================================================================\")\n        print(\"Checking missing data (NaN): \\n\", missing_values_table(df))\n        \n    else:\n        logging.warning(\"File is not existed\")\n        df = None\n        \n    return df\n\n\ndef one_way_tab (df, col):\n    #\n    # Function to compute one way table\n    # Args:\n    #   df: pandas dataframe\n    #   col: column name to tabulate\n    #\n    # Return:\n    #   df: the tabulate pandas of the outcome\n    #\n    sns.countplot(x = col, data = df)\n    plt.show();\n    df = pd.crosstab(index = df[col], columns = \"count\")\n    df['percent'] = df\/df.sum() * 100\n    return df\n\n","4ff1c7e9":"data_file = \"..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\ndf = read_data(data_file, nrows = None)","793b3c55":"display(df.head(5))","13f60f78":"one_way_tab(df, 'churn')","d7e3ee4b":"df[df.duplicated(['customerid'], keep=False)]","64982fd6":"df['totalcharges'] = df['totalcharges'].replace(r'\\s+', np.nan, regex=True)\ndf['totalcharges'] = pd.to_numeric(df['totalcharges'])","6e7d270c":"missing_values_table(df)","e6f475a9":"df[df.totalcharges.isnull()]","43ea37d5":"df.loc[df.totalcharges.isnull(), 'totalcharges'] = 0\nsns.distplot(df.totalcharges)\nplt.show();","b7a78b14":"def display_plot(df, col_to_exclude, object_mode = True):\n    \"\"\" \n     This function plots the count or distribution of each column in the dataframe based on specified inputs\n     @Args\n       df: pandas dataframe\n       col_to_exclude: specific column to exclude from the plot, used for excluded key \n       object_mode: whether to plot on object data types or not (default: True)\n       \n     Return\n       No object returned but visualized plot will return based on specified inputs\n    \"\"\"\n    n = 0\n    this = []\n    \n    if object_mode:\n        nrows = 4\n        ncols = 4\n        width = 20\n        height = 20\n    \n    else:\n        nrows = 2\n        ncols = 2\n        width = 14\n        height = 10\n    \n    \n    for column in df.columns:\n        if object_mode:\n            if (df[column].dtypes == 'O') & (column != col_to_exclude):\n                this.append(column)\n                \n                \n        else:\n            if (df[column].dtypes != 'O'):\n                this.append(column)\n     \n    \n    fig, ax = plt.subplots(nrows, ncols, sharex=False, sharey=False, figsize=(width, height))\n    for row in range(nrows):\n        for col in range(ncols):\n            if object_mode:\n                sns.countplot(df[this[n]], ax=ax[row][col])\n                \n            else:\n                sns.distplot(df[this[n]], ax = ax[row][col])\n            \n            ax[row,col].set_title(\"Column name: {}\".format(this[n]))\n            ax[row, col].set_xlabel(\"\")\n            ax[row, col].set_ylabel(\"\")\n            n += 1\n\n    plt.show();\n    return None\n","c24d17a3":"display_plot(df, 'customerid', object_mode = True)","994be25e":"display_plot(df, 'customerid', object_mode = False)","85ea3395":"pd.crosstab(index = df[\"phoneservice\"], columns = df[\"multiplelines\"])","2ce54829":"pd.crosstab(index = df[\"internetservice\"], columns = df[\"streamingtv\"])","ec15a00d":"def convert_no_service (df):\n    col_to_transform = []\n    for col in df.columns:\n        if (df[col].dtype == 'O') & (col != 'customerid'):\n            if len(df[df[col].str.contains(\"No\")][col].unique()) > 1:\n                col_to_transform.append(col)\n    \n    print(\"Total column(s) to transform: {}\".format(col_to_transform))\n    for col in col_to_transform:\n        df.loc[df[col].str.contains(\"No\"), col] = 'No'\n        \n    return df\n","c4f0156c":"df = convert_no_service(df)","69dc1958":"display_plot(df, 'customerid', object_mode = True)","aa5bc840":"df.gender = df.gender.map(dict(Male=1, Female=0))\ndisplay(df.gender.value_counts())","98d35e43":"def encode_yes_no (df, columns_to_encode):\n    for col in columns_to_encode:\n        df[col] = df[col].map(dict(Yes = 1, No = 0))\n        \n    return df\n","9f4c61b5":"encode_columns = []\nfor col in df.columns:\n    keep = np.sort(df[col].unique(), axis = None)\n    \n    if (\"Yes\" in keep) & (\"No\" in keep):\n        encode_columns.append(col)\n\ndel keep\nprint(\"Encode Columns Yes\/No: {}\".format(encode_columns))\n        \n    ","68e07ca3":"df = encode_yes_no(df, encode_columns)\ndisplay(df.head(5))","2649f7f0":"df = pd.get_dummies(df, columns = ['internetservice', 'contract', 'paymentmethod'], prefix = ['ISP', 'contract', 'payment'])\ndisplay(df.head(5))","3425d31e":"df2 = df.drop('customerid', axis = 1, inplace = False)\ndf2.columns = df2.columns.str.replace(\" \", \"_\")","306333ee":"df2.corr()['churn'].sort_values(ascending=False)","1115f9b1":"corr = df2.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nf, ax = plt.subplots(figsize=(16, 10))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.6, cbar_kws={\"shrink\": .5})\nplt.show();","d343139e":"from sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix, roc_curve\nfrom sklearn.metrics import precision_recall_curve, average_precision_score, precision_recall_fscore_support\nimport pickle\nimport scikitplot as skplt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.metrics import make_scorer\n","45300ffa":"X = df2.drop('churn', axis = 1, inplace = False)\ny = df2['churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = seed)\nprint(\"Training target distribution:\\n{}\".format(y_train.value_counts()))\nprint(\"\\nTesting target distribution:\\n{}\".format(y_test.value_counts()))\n","2b8c5c22":"def xgb_f1(y, t):\n    #\n    # Function to evaluate the prediction based on F1 score, this will be used as evaluation metric when training xgboost model\n    # Args:\n    #   y: label\n    #   t: predicted\n    #\n    # Return:\n    #   f1: F1 score of the actual and predicted\n    #\n    t = t.get_label()\n    y_bin = [1. if y_cont > 0.5 else 0. for y_cont in y]   # change the prob to class output\n    return 'f1', f1_score(t, y_bin)\n\n\ndef plot_evaluation_metric (y_true, y_prob):\n    #\n    # Function to plot the evaluation metric (cumulative gain, lift chart, precision and recall) on the screen\n    # Args:\n    #   y_true: array of y true label\n    #   y_prob: array of y predicted probability (outcome of predict_proba() function)\n    #\n    # Return:\n    #   None\n    #\n    skplt.metrics.plot_cumulative_gain(y_true, y_prob)\n    plt.show();\n    skplt.metrics.plot_precision_recall(y_true, y_prob)\n    plt.show();\n    skplt.metrics.plot_lift_curve(y_true, y_prob)\n    plt.show();\n    return \n\n\ndef print_evaluation_metric (y_true, y_pred):\n    #\n    # Function to print out the model evaluation metrics\n    # Args:\n    #   y_true: array of y true label\n    #   y_pred: array of y predicted class\n    #\n    # Return:\n    #   None\n    #\n    precision, recall, fscore, support = precision_recall_fscore_support(y_true, y_pred)\n    print(\"Precision: {}\".format(precision))\n    print(\"Recall: {}\".format(recall))\n    print(\"F-score: {}\".format(fscore))\n    print(\"Support: {}\".format(support))\n    return \n\n\ndef get_confusion_matrix (y_true, y_pred, save=0, filename=\"this.csv\"):\n    #\n    # Function to print out the confusion matrix on screen as well as print to csv file, if enabled\n    # Args:\n    #   y_true: array of y true label\n    #   y_pred: array of y prediction\n    #   save: to enable the write to csv file (default = 0)\n    #   filename: the name of the file to be saved (default = this.csv)\n    #\n    # Return:\n    #   None\n    #\n    from sklearn.metrics import confusion_matrix\n    get_ipython().magic('matplotlib inline')\n    cm = pd.DataFrame(confusion_matrix(y_true, y_pred),\n                      columns = ['Predicted False', 'Predicted True'],\n                      index = ['Actual False', 'Actual True']\n                      )\n    display(cm)\n    if(save):\n        cm.to_csv(filename, index = True)\n    \n    return \n\n\ndef my_plot_roc_curve (y_true, y_prob, filename=\"img.png\", dpi = 200):\n    #\n    # Function to plot the ROC curve by computing fpr and tpr as well as save the plot to file\n    # Args:\n    #   y_true: array of y true label\n    #   y_prob: the output of y probability prediction (outcome for predict_proba() function)\n    #   filename: the name of the file to be saved\n    #   dpi: the resolution of the figure\n    # Return:\n    #   None\n    #\n    fpr, tpr, threshold = roc_curve(y_true, y_prob[:, 1])\n    fig = plt.gcf()\n    fig.set_size_inches(10, 8)\n    plt.title(\"Receiver Operating Characteristic (ROC)\")\n    plt.plot(fpr, tpr, 'b')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()\n    fig.savefig(filename, dpi = dpi)\n    return\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    import itertools\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    return\n\n","bc1092fa":"classifiers = [\n    KNeighborsClassifier(n_jobs = 4),\n    RandomForestClassifier(n_jobs = 4),\n    XGBClassifier(n_jobs = 4)\n]\n\n# iterate over classifiers\nfor item in classifiers:\n    classifier_name = ((str(item)[:(str(item).find(\"(\"))]))\n    print (classifier_name)\n    \n    # Create classifier, train it and test it.\n    clf = item\n    clf.fit(X_train, y_train)\n    pred = clf.predict(X_test)\n    score = clf.score(X_test, y_test)\n    print (\"Score: \", round(score,3),\"\\nF1 score: \", round(f1_score(y_test, pred), 3), \"\\n- - - - - \", \"\\n\")\n    ","12e19398":"param_grid = {\n    'silent': [False],\n    'max_depth': [2, 3, 4, 5],\n    'learning_rate': [0.001, 0.01, 0.1, 0.15],\n    'subsample': [0.7, 0.8, 0.9, 1.0],\n    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n    'colsample_bylevel': [0.7, 0.8, 0.9, 1.0],\n    'min_child_weight': [0.5, 1.0, 3.0],\n    'gamma': [0, 0.25, 0.5, 1.0],\n    'reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0, 100.0],\n    'n_estimators': [50, 100, 150],\n    'scale_pos_weight': [1, 1.5, 2],\n    'max_delta_step': [1, 2, 3]\n}\n\nclf = XGBClassifier(objective = 'binary:logistic')","d957a148":"fit_params = {'eval_metric': 'logloss',\n              'early_stopping_rounds': 10,\n              'eval_set': [(X_test, y_test)]}\n\nrs_clf = RandomizedSearchCV(clf, param_grid, n_iter=50,\n                            n_jobs=4, verbose=2, cv=5,\n                            fit_params=fit_params,\n                            scoring= 'f1_macro', refit=True, random_state=seed)\n\n\nprint(\"Randomized search..\")\nsearch_time_start = time.time()\nrs_clf.fit(X_train, y_train)\nprint(\"Randomized search time:\", time.time() - search_time_start)\n\nbest_score = rs_clf.best_score_\nbest_params = rs_clf.best_params_\nprint(\"Best score: {}\".format(best_score))\nprint(\"Best params: \")\nfor param_name in sorted(best_params.keys()):\n    print('%s: %r' % (param_name, best_params[param_name]))","37f72b72":"best_xgb = XGBClassifier(objective = 'binary:logistic',\n                         colsample_bylevel = 0.7,\n                         colsample_bytree = 0.8,\n                         gamma = 1,\n                         learning_rate = 0.15,\n                         max_delta_step = 3,\n                         max_depth = 4,\n                         min_child_weight = 1,\n                         n_estimators = 50,\n                         reg_lambda = 10,\n                         scale_pos_weight = 1.5,\n                         subsample = 0.9,\n                         silent = False,\n                         n_jobs = 4\n                        )\n\nbest_xgb.fit(X_train, y_train, eval_metric = xgb_f1, eval_set = [(X_train, y_train), (X_test, y_test)], \n             early_stopping_rounds = 20)","0b857d72":"xgb.plot_importance(best_xgb, max_num_features = 15)\nplt.show();","5b843dda":"y_pred = best_xgb.predict(X_test)\ny_prob = best_xgb.predict_proba(X_test)\nprint_evaluation_metric(y_test, y_pred)\nget_confusion_matrix (y_test, y_pred, save=0, filename=\"this.csv\")\nmy_plot_roc_curve (y_test, y_prob, filename=\"ROC.png\", dpi = 200)\nplot_evaluation_metric (y_test, y_prob)","c9292d89":"from sklearn.metrics import classification_report\nev = classification_report(y_test, y_pred, target_names = ['Not Churn', 'Churn'])\nprint(ev)","764b1535":"from xgboost import plot_tree\nimport graphviz\n\nplot_tree(best_xgb, num_trees = 0)\nfig = plt.gcf()\nfig.set_size_inches(300, 100)\nfig.savefig('tree.png')","c91f24c9":"y_all_prob = best_xgb.predict_proba(X)\ndf['churn_prob'] = y_all_prob[:, 1]\nsns.distplot(df['churn_prob'])\nplt.show();","a3dd052c":"df[['customerid', 'churn', 'churn_prob']].head(10)","62df0677":"import shap\nshap.initjs()\n\nexplainer = shap.TreeExplainer(best_xgb)\nshap_values = explainer.shap_values(X_train)","d07846bf":"shap.force_plot(explainer.expected_value, shap_values, X_train)","ed80993f":"shap.summary_plot(shap_values, X_train)\nshap.summary_plot(shap_values, X_train, plot_type=\"bar\")","3e3f6848":"shap.dependence_plot(\"ISP_Fiber_optic\", shap_values, X_train, interaction_index=\"monthlycharges\")","4c1cb5c8":"shap.force_plot(explainer.expected_value, shap_values[0,:], X_test.iloc[0,:], link=\"logit\")","6d389ea6":"#### 4. Application\n\nNormally in business setting, we will use the prediction score, rather than the class prediction. This score is used to associate with the subscriber profiles, and can be used to adjust for campaign targeting.","824c6a6d":"When we train the model, we can get the SHAP values, rotate the data and then stack them, hence we can see the explanations for the entire dataset and we can see which features influence the output (**Note**: The plot is interactive).","58b1c999":"#### 2. Data Sensemaking\nAfter the data loaded, we can see that there is no missing data, however, **totalcharges** feature has data types as *object*. Based on the sample data, this should be numeric data.","19a0e098":"We will start to transform the data for building the predictive model in the next section.\n\n- Encode Yes \/ No into 1 \/ 0, respectively\n- Encode Male \/ Female into 1\/0, respectively (for **gender** feature)\n- Create dummy variables for **internet service, contract, and payment method** features.","c5a0cb43":"Is there any duplicated Customer ID in the dataframe? Based on the below result, there is no duplicated **customer id** in the data.","6a8d5446":"Based on the value of the services the subscribers subscribed to, there are **yes**, **no**, and **no phone \/ internet service**. These are somewhat related to primary products. Examples are illustrated through *panda crosstab* function below:\n\n1. **Phone service (Primary) and Multiple lines (Secondary)**\n \n - If the subscribers have phone service, they may have multiple lines (yes or no). \n - But if the subscribers don't have phone service, the subscribers will never have multiple lines.\n \n \n2. **Internet Service (Primary) and other services, let's say streaming TV (secondary)**\n\n - If the subscribers have Internet services (either DSL or Fiber optic), the subscribers may opt to have other services related to Internet (i.e. streaming TV, device protection).\n - But if the subscribers don't have the Internet services, this secondary service will not be available for the subscribers.\n \n\nWith this conclusion, I opt to transform the feature value of **No Phone \/ Internet service** to be the same **No** because it can be used another features (hence, **phone service** and **internet service** column) to explain.","bd8b7aca":"By exploring the missing total charges column, we can see that all of them has **tenure** of 0. Hence, this is the newbie subscriber (as tenure represents in a month unit).\n\nThis is the case depends on each operator in each country, because sometimes they will pro-rate based on day and some will wait until specific days (i.e. 15 days after usage) and charge the full amount.\n\nHowever, we are given ***tenure = 0***, we cannot correctly decipher the values. Hence, we will fix this value to become 0.","91c0289d":"#### Univariate Analysis\n\nLet's look at each feature (except **customerid**) to see the unique values and the distribution of the features.\n\nI have created a function to quickly plot the data points based on its type by separating **object** and **non-object** from each other.","5adb94ab":"As we start to develop the model, let's quickly drop **customerid** column and assign it to new dataframe.","b49043b0":"#### 3. Machine Learning \n\nPrior to model build, let's quickly observe the correlation of the data we have.\n\n- Based on this, looks like there are positive correlation between churn and those Month-to-month contracts.\n   - Assumption is that the month-to-month contract doesn't require a commitment from the subscribers, hence they can easily churn out (stop using the services)\n- The second and third variables with positive correlation are **Fiber Optic ISP** and **monthly charges**.","f5d63ca6":"Furthermore, we can use **dependence_plot** to understand the effects of the single feature to the model output. Since SHAP values represent a feature's responsibility for a change in the model output, the plot below represents the change in predicted churn as the subscribers have **ISP - Fiber Optic** product. \n\nVertical dispersion at a single value of ISP Fiber Optic represents the interaction effects with another features, in this plot, I assign **monthly charges** for coloring. Based on this plot, we can see that the Fiber Optic subscribers who have higher monthly charges are likely to churn more than those who don't have Fiber optic.","0e041175":"## Telecom Churn: XGBoost (Extreme Gradient Boosting)\n\n\n\n\nData was downloaded from [IBM Sample Data](https:\/\/www.ibm.com\/communities\/analytics\/watson-analytics-blog\/guide-to-sample-datasets\/).","622436e0":"#### Appendix: Explain XGBoost model\n\nI will add **SHAP** package to help visualization and explain the ***XGBoost Classifier***. \n\nFirst, we will need to use *TreeExplainer* to the XGBoost model object.","0624f134":"#### Build the classifiers\n\nLet's try (very) basic classifiers, using only default setting of each classifier's algorithm. I will try on 3 different algorithms:\n\n1. K-NN (K-Nearest Neighbors) classifier\n2. Random Forest Classifier\n3. XGBoosting Classifier\n\nThen I will print out the accuracy score and F1 score of each classifier.","29f7c5a0":"Based on the above finding, **total charges** column should not be an object. Let's start by looking at this column.","85c6cdc2":"#### 1. Define UDFs\n\nIn this section, I will setup some local utilities to load the data and find the missing features. The data will be loaded and checked the null values as well.","b45bda5d":"Let's see the data after transformation.","2a6ad827":"For each observation, this represents one subscriber, with his \/ her details, product holding and other services subscription.\n\nIn the end, we want to predict whether the subscribers become **churn** or not. Let's quickly look at the distribution of the target variable.","78b0a6eb":"We can display how each feature influenced the output of the model. SHAP values sum to the difference between the expected output of the model and the current output for the observation. Note that for the Tree SHAP implementation, the margin output of the model is explained, not the transformed output (i.e. output from **predict** function).\n\nThis means the units of the SHAP values for this model are log odd rations. Large positive values mean a subscriber is likely to churn.\n\nBelow can be used in place of Feature importances plot. "}}