{"cell_type":{"3580ce0e":"code","6890d281":"code","d891404d":"code","295262b8":"code","39cc1cbd":"code","d5ac9486":"code","3678cad3":"code","73637281":"code","2a6916f3":"code","d9fd4f7c":"code","9ec32d5d":"code","175a9c02":"code","2dec7c0e":"code","9afc291b":"code","0ed7f2cb":"code","f92f1691":"code","cc468d81":"code","a88e816d":"code","4db86522":"code","a34487bc":"code","7abae213":"code","4b314458":"code","34789cd0":"code","5ccdb6b5":"code","f9dc996e":"code","0b1128fd":"code","1c89cda4":"code","4a57fa33":"code","5eb12d86":"code","6624d08a":"code","a2ede373":"code","fea961ea":"code","853aea39":"code","a96d425d":"code","c7e90fd8":"code","df568e60":"code","741c290d":"code","55ee8012":"code","67545c45":"code","8048c899":"markdown","f8087a1c":"markdown","60ccac72":"markdown","529188d2":"markdown","ac57f088":"markdown","a029ee36":"markdown","4877b40f":"markdown","7529be39":"markdown","b1969540":"markdown","e5b2fb46":"markdown","afc68e18":"markdown","c3315827":"markdown","b3ec7083":"markdown"},"source":{"3580ce0e":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6890d281":"import pandas as pd\nimport numpy as np\nimport scipy\nfrom scipy import stats\nimport seaborn as sns\nfrom warnings import filterwarnings\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, GridSearchCV","d891404d":"filterwarnings(action='ignore')","295262b8":"train_df = pd.read_csv('\/kaggle\/input\/santander-value-prediction-challenge\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/santander-value-prediction-challenge\/test.csv')","39cc1cbd":"train_df","d5ac9486":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))\n\nsns.distplot(train_df['target'], fit=stats.norm, ax=ax[0])\nax[0].set_title('Before Normalization')\n\n# use log1p instead of log to get the normalized value for even tiny values -> 0\ntrain_df['target'] = np.log1p(train_df['target'])\nax[1].set_title('After Normalization')\nsns.distplot(train_df['target'], fit=stats.norm, ax=ax[1])\nplt.show()","3678cad3":"# check for duplicated rows\nprint('num of duplicated rows in train set: ', train_df.duplicated().sum())\n# drop ID because there is no need to keep it\ntrain_df.drop(['ID'], axis=1, inplace=True)\n\n# check for duplicated rows\nprint('num of duplicated rows in test set: ', test_df.duplicated().sum())\n# drop ID because there is no need to keep it\nids = test_df['ID']\ntest_df.drop(['ID'], axis=1, inplace=True)","73637281":"# check for null values\nprint('num of null values in train set', train_df.isnull().sum().sum())\n\n# check for null values\nprint('num of null values in test set', test_df.isnull().sum().sum())","2a6916f3":"# check if there're any feature with 0 variance(they give us no information)\n\nzero_var_train = []\nfor col in train_df.columns:\n    if train_df[col].var() == 0:\n        zero_var_train.append(col)\n\nprint('num of columns with zero variance in the train set: ', len(zero_var_train))\n\nzero_var_test = []\nfor col in test_df.columns:\n    if test_df[col].var() == 0:\n        zero_var_test.append(col)\n\nprint('num of columns with zero variance in the test set: ', len(zero_var_test))","d9fd4f7c":"# check for duplicate columns\n\ndef duplicate_columns(df):\n    dups = []\n    columns = df.columns\n\n    for i in range(len(columns)):\n        col1 = df.iloc[:, i]\n        for j in range(i + 1, len(columns)):\n            col2 = df.iloc[:, j]\n            # break early if dtypes aren't the same (helps deal with\n            # categorical dtypes)\n            if col1.dtype is not col2.dtype:\n                break\n            # otherwise compare values\n            if col1.equals(col2):\n                dups.append(columns[i])\n                break\n    return dups\n\n\ntrain_dups = duplicate_columns(train_df)\nprint('num of duplicated cols in the train set: ', len(train_dups))\n\n# test_dups = duplicate_columns(test_df)\n# print('num of duplicated cols in the test set: ', len(test_dups))","9ec32d5d":"# dropping useless features\n\nuseless_features = list(set(zero_var_train + train_dups))\n\ntrain_df = train_df.drop(useless_features, axis=1)\ntest_df = test_df.drop(useless_features, axis=1)","175a9c02":"# adding some statistical features to boost the model\n\nfor df in [train_df, test_df]:\n    df['max'] = df.max(axis=1)\n    df['min'] = df.min(axis=1)\n    df['mean'] = df.mean(axis=1)\n    df['non_zero_sum'] = (df != 0).sum(axis=1)\n    df['zero_sum'] = (df == 0).sum(axis=1)\n    df['sum'] = df.sum(axis=1)\n    df['variance'] = df.var(axis=1)\n    df['median'] = df.median(axis=1)\n    df['mode'] = df.mode(axis=1)\n    df['log_sum'] = np.log1p(df['sum'])\n    df['log_non_zero'] = np.log1p(df['non_zero_sum'])\n    df['log_zero'] = np.log1p(df['zero_sum'])\n    df['log_mean'] = np.log1p(df['mean'])\n    df['log_max'] = np.log1p(df['max'])\n    df['log_min'] = np.log1p(df['min'])\n    df['log_variance'] = np.log1p(df['variance'])\n    df['log_mode'] = np.log1p(df['mode'])\n    df['log_median'] = np.log1p(df['median'])","2dec7c0e":"pearson_selection = train_df.corr().nlargest(20, 'target')['target'].index","9afc291b":"plt.figure(figsize=(20, 15))\nsns.heatmap(train_df[pearson_selection].corr(), cmap='Greys', annot=True)","0ed7f2cb":"spearman_selection = train_df.corr(\n    method='spearman').nlargest(20, 'target')['target'].index","f92f1691":"plt.figure(figsize=(20, 15))\nsns.heatmap(train_df[spearman_selection].corr(), cmap='Greys', annot=True)","cc468d81":"X = train_df.drop(['target'], axis=1)\ny = train_df['target']","a88e816d":"X_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.2, random_state=42)","4db86522":"from xgboost import XGBRegressor\nimport shap\n\nxgb = XGBRegressor(verbose=False).fit(X_train, y_train)","a34487bc":"explainer = shap.Explainer(xgb)\nshap_values = explainer(X_train)\n\nshap.plots.beeswarm(shap_values)","7abae213":"X_importance = X_val\n\nexplainer = shap.TreeExplainer(xgb)\nshap_values = explainer.shap_values(X_importance)\n\nshap_sum = np.abs(shap_values).mean(axis=0)\nimportance_df = pd.DataFrame(\n    [X_importance.columns.tolist(), shap_sum.tolist()]).T\nimportance_df.columns = ['column_name', 'shap_importance']\nimportance_df = importance_df.sort_values('shap_importance', ascending=False)","4b314458":"shap_selected_features_xgb = list(\n    importance_df[importance_df['shap_importance'] != 0]['column_name'])","34789cd0":"len(shap_selected_features_xgb)","5ccdb6b5":"from catboost import CatBoostRegressor\n\ncat = CatBoostRegressor(random_state=42, verbose=False).fit(X_train, y_train)\nexplainer = shap.Explainer(cat)\nshap_values = explainer(X_train)\n\nexplainer = shap.TreeExplainer(cat)\nshap_values = explainer.shap_values(X_importance)\n\nshap_sum = np.abs(shap_values).mean(axis=0)\nimportance_df = pd.DataFrame(\n    [X_importance.columns.tolist(), shap_sum.tolist()]).T\nimportance_df.columns = ['column_name', 'shap_importance']\nimportance_df = importance_df.sort_values('shap_importance', ascending=False)","f9dc996e":"explainer = shap.Explainer(cat)\nshap_values = explainer(X_train)\n\nshap.plots.beeswarm(shap_values)","0b1128fd":"shap_selected_features_cat = list(\n    importance_df[importance_df['shap_importance'] != 0]['column_name'])","1c89cda4":"len(shap_selected_features_cat)","4a57fa33":"from lightgbm import LGBMRegressor\n\nX_importance = X_val\nlgbm = LGBMRegressor(random_state=42).fit(X_train, y_train)\nexplainer = shap.Explainer(lgbm)\nshap_values = explainer(X_train)\n\nexplainer = shap.TreeExplainer(lgbm)\nshap_values = explainer.shap_values(X_importance)\n\nshap_sum = np.abs(shap_values).mean(axis=0)\nimportance_df = pd.DataFrame(\n    [X_importance.columns.tolist(), shap_sum.tolist()]).T\nimportance_df.columns = ['column_name', 'shap_importance']\nimportance_df = importance_df.sort_values('shap_importance', ascending=False)","5eb12d86":"explainer = shap.Explainer(lgbm)\nshap_values = explainer(X_train)\n\nshap.plots.beeswarm(shap_values)","6624d08a":"shap_selected_features_lgbm = list(\n    importance_df[importance_df['shap_importance'] != 0]['column_name'])","a2ede373":"len(shap_selected_features_lgbm)","fea961ea":"shap_selection = common_elements = np.intersect1d(\n    shap_selected_features_xgb, shap_selected_features_cat)\nshap_selection = np.intersect1d(shap_selection, shap_selected_features_lgbm)","853aea39":"len(shap_selection)","a96d425d":"from sklearn.ensemble import RandomForestRegressor, VotingRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_log_error, mean_squared_error","c7e90fd8":"X_train = X_train[shap_selection]\nX_val = X_val[shap_selection]\ntest_df = test_df[shap_selection]","df568e60":"# xgboost\nxgb = XGBRegressor(colsample_bytree=0.055, colsample_bylevel=0.5,\n                   gamma=1.5, learning_rate=0.02, max_depth=32,\n                   objective='reg:linear', booster='gbtree',\n                   min_child_weight=57, n_estimators=1000, reg_alpha=0,\n                             reg_lambda=0, eval_metric='rmse', subsample=0.7,\n                   silent=1, n_jobs=-1, early_stopping_rounds=14,\n                   random_state=42, nthread=-1)\n\n# randomforest\nrf = RandomForestRegressor(random_state=42)\n\n# catboost\ncb = CatBoostRegressor(random_state=42, verbose=False)\n\n# lightgbm\nlgbm = LGBMRegressor(objective='regression', num_leaves=144,\n                     learning_rate=0.005, n_estimators=720, max_depth=13,\n                     metric='rmse', is_training_metric=True,\n                     max_bin=55, bagging_fraction=0.8, verbose=-1,\n                     bagging_freq=5, feature_fraction=0.9, random_state=42)\n\n\n# defining ensemble\nensemble_regressor = VotingRegressor(\n    [('rf', rf), ('xgb', xgb), ('cb', cb), ('lgbm', lgbm)])\n\n# training each model\nfor reg in (rf, xgb, cb, lgbm, ensemble_regressor):\n    reg.fit(X_train, y_train)\n    y_pred = reg.predict(X_val)\n    print(reg.__class__.__name__, mean_squared_error(y_val, y_pred))","741c290d":"prediction = np.expm1(ensemble_regressor.predict(test_df))","55ee8012":"sub = pd.DataFrame({'ID': ids, 'target': prediction})","67545c45":"sub.to_csv('sub.csv', index=False)","8048c899":"### Normalizing the target","f8087a1c":"## Feature importance using SHAP","60ccac72":"### XGBoost","529188d2":"## Training model","ac57f088":"## Train and Validation split","a029ee36":"### LightGBM","4877b40f":"## EDA","7529be39":"### CatBoost","b1969540":"## Correlation","e5b2fb46":"## Adding some statistical features","afc68e18":"### Pearson","c3315827":"## Predicting test set ","b3ec7083":"### Spearman"}}