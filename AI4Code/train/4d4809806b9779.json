{"cell_type":{"bdd18966":"code","c886c9ff":"code","16c010c6":"code","fb589f27":"code","1af10821":"code","4f1597e9":"code","a5d96103":"code","f90326aa":"code","6fc491b7":"code","b3c7c976":"code","e3b4530e":"code","1b48b063":"code","28e26604":"code","e0879cab":"code","c57a8329":"markdown","5e2982f7":"markdown","50fdabf0":"markdown","ebbf0f70":"markdown","6bbcf03e":"markdown","f4528455":"markdown","837859e9":"markdown","ad8ee5c1":"markdown","dda6488f":"markdown","b90a7a58":"markdown","28a3fd84":"markdown","ff707134":"markdown","1f3a49b7":"markdown","0ee270e3":"markdown","881ff34d":"markdown","d1178440":"markdown"},"source":{"bdd18966":"# Importing pandas\nimport pandas as pd\nimport os\nos.listdir('..\/input')\n# Importing training data set\nX_train=pd.read_csv('..\/input\/loan_prediction-1\/X_train.csv')\nY_train=pd.read_csv('..\/input\/loan_prediction-1\/Y_train.csv')\n# Importing testing data set\nX_test=pd.read_csv('..\/input\/loan_prediction-1\/X_test.csv')\nY_test=pd.read_csv('..\/input\/loan_prediction-1\/Y_test.csv')\nprint (X_train.head())","c886c9ff":"import matplotlib.pyplot as plt\nX_train[X_train.dtypes[(X_train.dtypes==\"float64\")|(X_train.dtypes==\"int64\")]\n                        .index.values].hist(figsize=[11,11])","16c010c6":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train[['ApplicantIncome', 'CoapplicantIncome','LoanAmount', \n                   'Loan_Amount_Term', 'Credit_History']],Y_train)\n# Checking the performance of our model on the testing data set\nfrom sklearn.metrics import accuracy_score\naccuracy_score(Y_test,knn.predict(X_test[['ApplicantIncome', 'CoapplicantIncome',\n                             'LoanAmount', 'Loan_Amount_Term', 'Credit_History']]))","fb589f27":" Y_train.Target.value_counts()\/Y_train.Target.count()","1af10821":"Y_test.Target.value_counts()\/Y_test.Target.count()","4f1597e9":"# Importing MinMaxScaler and initializing it\nfrom sklearn.preprocessing import MinMaxScaler\nmin_max=MinMaxScaler()\n# Scaling down both train and test data set\nX_train_minmax=min_max.fit_transform(X_train[['ApplicantIncome', 'CoapplicantIncome',\n                'LoanAmount', 'Loan_Amount_Term', 'Credit_History']])\nX_test_minmax=min_max.fit_transform(X_test[['ApplicantIncome', 'CoapplicantIncome',\n                'LoanAmount', 'Loan_Amount_Term', 'Credit_History']])","a5d96103":"# Fitting k-NN on our scaled data set\nknn=KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train_minmax,Y_train)\n# Checking the model's accuracy\naccuracy_score(Y_test,knn.predict(X_test_minmax))","f90326aa":"from sklearn.preprocessing import scale\nX_train_scale=scale(X_train[['ApplicantIncome', 'CoapplicantIncome',\n                'LoanAmount', 'Loan_Amount_Term', 'Credit_History']])\nX_test_scale=scale(X_test[['ApplicantIncome', 'CoapplicantIncome',\n               'LoanAmount', 'Loan_Amount_Term', 'Credit_History']])\n# Fitting logistic regression on our standardized data set\nfrom sklearn.linear_model import LogisticRegression\nlog=LogisticRegression(penalty='l2',C=.01)\nlog.fit(X_train_scale,Y_train)\n# Checking the model's accuracy\naccuracy_score(Y_test,log.predict(X_test_scale))","6fc491b7":"# Fitting a logistic regression model on whole data\nlog=LogisticRegression(penalty='l2',C=.01)\nlog.fit(X_train,Y_train)\n# Checking the model's accuracy\naccuracy_score(Y_test,log.predict(X_test))","b3c7c976":"# Importing LabelEncoder and initializing it\nfrom sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\n# Iterating over all the common columns in train and test\nfor col in X_test.columns.values:\n       # Encoding only categorical variables\n        if X_test[col].dtypes=='object':\n         data = (X_train[col].append(X_test[col]))\n         le.fit(data.values)\n         X_train[col] = le.transform(X_train[col])\n         X_test[col] = le.transform(X_test[col])","e3b4530e":"# Standardizing the features\nX_train_scale=scale(X_train)\nX_test_scale=scale(X_test)\n# Fitting the logistic regression model\nlog=LogisticRegression(penalty='l2',C=.01)\nlog.fit(X_train_scale,Y_train)\n# Checking the models accuracy\naccuracy_score(Y_test,log.predict(X_test_scale))","1b48b063":"# We are using scaled variable as we saw in previous section that \n# scaling will effect the algo with l1 or l2 reguralizer\nX_train_scale=scale(X_train)\nX_test_scale=scale(X_test)\n# Fitting a logistic regression model\nlog=LogisticRegression(penalty='l2',C=1)\nlog.fit(X_train_scale,Y_train)\n# Checking the model's accuracy\naccuracy_score(Y_test,log.predict(X_test_scale))","28e26604":"from sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder(sparse=False)\nX_train_1=X_train\nX_test_1=X_test\ncolumns=['Gender', 'Married', 'Dependents', 'Education','Self_Employed',\n          'Credit_History', 'Property_Area']\nfor col in columns:\n       # creating an exhaustive list of all possible categorical values\n        data=X_train[[col]].append(X_test[[col]])\n        enc.fit(data)\n       # Fitting One Hot Encoding on train data\n        temp=enc.transform(X_train[[col]])\n       # Changing the encoded features into a data frame with new column names\n        temp=pd.DataFrame(temp,columns=[(col+\"_\"+str(i)) for i in data[col]\n            .value_counts().index])\n       # In side by side concatenation index values should be same\n       # Setting the index values similar to the X_train data frame\n        temp=temp.set_index(X_train.index.values)\n       # adding the new One Hot Encoded varibales to the train data frame\n        X_train_1=pd.concat([X_train_1,temp],axis=1)\n       # fitting One Hot Encoding on test data\n        temp = enc.transform(X_test[[col]])\n       # changing it into data frame and adding column names\n        temp=pd.DataFrame(temp,columns=[(col+\"_\"+str(i)) for i in data[col]\n            .value_counts().index])\n       # Setting the index for proper concatenation\n        temp=temp.set_index(X_test.index.values)\n       # adding the new One Hot Encoded varibales to test data frame\n        X_test_1=pd.concat([X_test_1,temp],axis=1)","e0879cab":"#Standardizing the data set\nX_train_scale=scale(X_train_1)\nX_test_scale=scale(X_test_1)\n# Fitting a logistic regression model\nlog=LogisticRegression(penalty='l2',C=1)\nlog.fit(X_train_scale,Y_train)\n# Checking the model's accuracy\naccuracy_score(Y_test,log.predict(X_test_scale))","c57a8329":"**Label Encoding**\n\nIn previous sections, we did the pre-processing for continuous numeric features. But, our data set has other features too such as Gender, Married, Dependents, Self_Employed and Education. All these categorical features have string values. For example, Gender has two levels either Male or Female. Lets feed the features in our logistic regression model.","5e2982f7":"we got an accuracy of 63% just by guessing, What is the meaning of this, getting better accuracy than our prediction model ?\n\nThis might be happening because of some insignificant variable with larger range will be dominating the objective function. We can remove this problem by scaling down all the features to a same range. sklearn provides a tool MinMaxScaler that will scale down all the features between 0 and 1.","50fdabf0":"There are 70% of approved loans, since there are more number of approved loans we will generate a prediction where all the loans are approved and lets go ahead and check the accuracy of our prediction","ebbf0f70":"**Feature Scaling**\nFeature scaling is the method to limit the range of variables so that they can be compared on common grounds. It is performed on continuous variables. Lets plot the distribution of all the continuous variables in  the data set.","6bbcf03e":"This article primarily focuses on data pre-processing techniques in python. Learning algorithms have affinity towards certain data types on which they perform incredibly well. They are also known to give reckless predictions with unscaled or unstandardized features. Algorithm like XGBoost, specifically requires dummy encoded data while algorithm like decision tree doesn\u2019t seem to care at all (sometimes)!\n\nIn simple words, pre-processing refers to the transformations applied to your data before feeding it to the algorithm. In python, scikit-learn library has a pre-built functionality under sklearn.preprocessing. There are many more options for pre-processing which we\u2019ll explore.\n\nAfter finishing this article, you will be equipped with the basic techniques of data pre-processing and their in-depth understanding.","f4528455":"Here, again we got the maximum accuracy as 0.75 that we have gotten so far. In this case, logistic regression regularization(C) parameter 1 where as earlier we used C=0.01.\n\n \n\n**End Notes**\n\n\nThe aim of this article is to familiarize you with the basic data pre-processing techniques and have a deeper understanding of the situations of where to apply those techniques.\n\nThese methods work because of the underlying assumptions of the algorithms. This is by no means an exhaustive list of the methods. I\u2019d encourage you to experiment with these methods since they can be heavily modified according to the problem at hand.\n\n\nDid you like reading this article ? Do you follow a different approach \/ package \/ library to perform these talks. I\u2019d love to interact with you in comments.","837859e9":"Now, lets apply logistic regression model on one-hot encoded data.","ad8ee5c1":"We got around 61% of correct prediction which is not bad but in real world practices will this be enough ? Can we deploy this model in real world problem? To answer this question lets take a look at distribution of Loan_Status in train data set.","dda6488f":"We got an error saying that it cannot convert string to float. So, what\u2019s actually happening here is learners like logistic regression, distance based methods such as kNN, support vector machines, tree based methods etc. in sklearn needs numeric arrays. Features having string values cannot be handled by these learners.\n\nSklearn provides a very efficient tool for encoding the levels of a categorical features into numeric values. LabelEncoder encode labels with value between 0 and n_classes-1.\n\nLets encode all the categorical features.","b90a7a58":"We again reached to our maximum score that was attained using kNN after scaling. This means standardizing the data when using a estimator having l1 or l2 regularization helps us to increase the accuracy of the prediction model. Other learners like kNN with euclidean distance measure, k-means, SVM, perceptron, neural networks, linear discriminant analysis, principal component analysis may perform better with standardized data.\n\nThough, I suggest you to understand your data and what kind of algorithm you are going to apply on it; over the time you will be able to judge weather to standardize your data or not.\n\nNote : Choosing between scaling and standardizing is a confusing choice, you have to dive deeper in your data and learner that you are going to use to reach the decision. For starters, you can try both the methods and check cross validation score for making a choice.","28a3fd84":"Now that we are done with label encoding, lets now run a logistic regression model on the data set with both categorical and continuous features.","ff707134":"Now, that we are done with scaling, lets apply kNN on our scaled data and check its accuracy.","1f3a49b7":"After understanding these plots, we infer that ApplicantIncome and CoapplicantIncome are in similar range (0-50000$) where as LoanAmount is in thousands and it ranges from 0 to 600$. The story for Loan_Amount_Term is completely different from other variables because its unit is months as opposed to other variables where the unit is dollars.\n\nIf we try to apply distance based methods such as kNN on these features, feature with the largest range will dominate the outcome results and we\u2019ll obtain less accurate predictions. We can overcome this trouble using feature scaling. Let\u2019s do it practically.\n\nLets try out kNN on our data set to see how well it will perform.","0ee270e3":"Great !! Our accuracy has increased from 61% to 75%. This means that some of the features with larger range were dominating the prediction outcome in the domain of distance based methods(kNN).\n\nIt should be kept in mind while performing distance based methods we must attempt to scale the data, so that the feature with lesser significance might not end up dominating the objective function due to its larger range. In addition, features having different unit should also be scaled thus providing each feature equal initial weightage and at the end we will have a better prediction model.\n\n\n\n**Feature Standardization**\n\nIn the previous section, we worked on the Loan_Prediction data set and fitted a kNN learner on the data set. After scaling down the data, we have got an accuracy of 75% which is very considerably good. I tried the same exercise on Logistic Regression and I got the following result :\n\nBefore Scaling : 61%\n\nAfter Scaling : 63%\n\nThe accuracy we got after scaling is close to the prediction which we made by guessing, which is not a very impressive achievement. So, what is happening here? Why hasn\u2019t the accuracy increased by a satisfactory amount as it increased in kNN?\n\nResources : Go through this article on Logistic Regression for better understanding.\n\nHere is the answer:\n\nIn logistic regression, each feature is assigned a weight or coefficient (Wi). If there is a feature with relatively large range and it is insignificant in the objective function then logistic regression will itself assign a very low value to its co-efficient, thus neutralizing the dominant effect of that particular feature, whereas distance based method such as kNN does not have this inbuilt strategy, thus it requires scaling.\n\nAren\u2019t we forgetting something ? Our logistic model is still predicting with an accuracy almost closer to a guess.\n\nNow, I\u2019ll be introducing a new concept here called standardization. Many machine learning algorithms in sklearn requires standardized data which means having zero mean and unit variance.\n\nStandardization (or Z-score normalization) is the process where the features are rescaled so that they\u2019ll have the properties of a standard normal distribution with \u03bc=0 and \u03c3=1, where \u03bc is the mean (average) and \u03c3 is the standard deviation from the mean. Standard scores (also called z scores) of the samples.\n\nFeatures having larger order of variance would dominate on the objective function as it happened in the previous section with the feature having large range. As we saw in the Exercise 1 that without any preprocessing on the data the accuracy was 61%, lets standardize our data apply logistic regression on that. Sklearn provides scale to standardize the data.\n\n","881ff34d":"Its working now. But, the accuracy is still the same as we got with logistic regression after standardization from numeric features. This means categorical features we added are not very significant in our objective function.\n\n\n**One-Hot Encoding**\n\n\nOne-Hot Encoding transforms each categorical feature with n possible values into n binary features, with only one active.\n\nMost of the ML algorithms either learn a single weight for each feature or it computes distance between the samples. Algorithms like linear models (such as logistic regression) belongs to the first category.\n\nLets take a look at an example from loan_prediction data set. Feature Dependents have 4 possible values 0,1,2 and 3+ which are then encoded without loss of generality to 0,1,2 and 3.\n\nWe, then have a weight \u201cW\u201d assigned for this feature in a linear classifier,which will make a decision based on the constraints W*Dependents + K > 0 or eqivalently  W*Dependents < K.\n\nLet f(w)= W*Dependents\n\nPossible values that can be attained by the equation are 0, W, 2W and 3W. A problem with this equation is that the weight \u201cW\u201d cannot make decision based on four choices. It can reach to a decision in following ways:\n\nAll leads to the same decision (all of them <K or vice versa)\n3:1 division of the levels (Decision boundary at f(w)>2W)\n2:2 division of the levels (Decision boundary at f(w)>W)\nHere we can see that we are loosing many different possible decisions such as the case where \u201c0\u201d and \u201c2W\u201d should be given same label and \u201c3W\u201d and \u201cW\u201d are odd one out.\n\nThis problem can be solved by One-Hot-Encoding as it effectively changes the dimensionality of the feature \u201cDependents\u201d from one to four, thus every value in the feature \u201cDependents\u201d will have their own weights. Updated equation for the decison would be f'(w) < K.\n\nwhere,  f'(w) = W1*D_0 + W2*D_1 + W3*D_2 + W4*D_3\nAll four new variable has boolean values (0 or 1).\n\nThe same thing happens with distance based methods such as kNN. Without encoding, distance between \u201c0\u201d and \u201c1\u201d values of Dependents is 1 whereas distance between \u201c0\u201d and \u201c3+\u201d will be 3, which is not desirable as both the distances should be similar. After encoding, the values will be new features (sequence of columns is 0,1,2,3+) : [1,0,0,0] and [0,0,0,1] (initially we were finding distance between \u201c0\u201d and \u201c3+\u201d), now the distance would be \u221a2.\n\nFor tree based methods, same situation (more than two values in a feature) might effect the outcome to extent but if methods like random forests are deep enough, it can handle the categorical variables without one-hot encoding.\n\nNow, lets take look at the implementation of one-hot encoding with various algorithms.\n\nLets create a logistic regression model for classification without one-hot encoding.","d1178440":"All our categorical features are encoded. You can look at your updated data set using X_train.head(). We are going to take a look at Gender frequency distribution before and after the encoding."}}