{"cell_type":{"2ae51bbf":"code","84624445":"code","4504864a":"code","5d4de6cc":"code","7e1ff341":"code","adc41683":"code","aee5906c":"code","1dbcada2":"code","739debbd":"code","088c5154":"code","d8381faa":"code","09787e19":"code","253a9ddb":"code","61922c43":"code","af95d49d":"code","3ed4f844":"code","f86ded5e":"code","f040a349":"code","6ca392df":"code","6dda0086":"code","fa9172de":"code","85499f6f":"code","c6bc48ef":"code","88e221d2":"code","0c59555e":"code","ce566034":"code","78a6b154":"code","112f851e":"code","0a98cc6e":"code","ec9f7125":"code","23a53adc":"code","51398a1e":"code","49d2bac6":"code","9e064528":"code","717e2148":"code","e197803c":"code","37bee657":"code","b1660dbd":"code","f525658a":"code","b53eb965":"code","e091cb1c":"code","d932b7be":"code","bb67dc6c":"code","171fee46":"markdown","86834673":"markdown","b764b5f4":"markdown","b4ca2a3b":"markdown","9c2df05b":"markdown","efcfc69a":"markdown","d3f6e267":"markdown","4ad01cfe":"markdown","45f7532f":"markdown","44540d16":"markdown","bb2a6ae3":"markdown","a2e3fddf":"markdown","9fbcc63e":"markdown","fa609003":"markdown","09d06896":"markdown"},"source":{"2ae51bbf":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (8, 8)\nplt.rcParams[\"figure.dpi\"] = 150\nplt.rcParams[\"font.size\"] = 14\nplt.rcParams['font.family'] = ['sans-serif']\nplt.rcParams['font.sans-serif'] = ['DejaVu Sans']\nplt.style.use('ggplot')\nsns.set_style(\"whitegrid\", {'axes.grid': False})","84624445":"import pandas as pd\nimport numpy as np\nfrom skimage.color import label2rgb, gray2rgb\nfrom skimage.transform import resize\nfrom skimage.io import imread\nfrom pathlib import Path\nfrom tqdm import tqdm\ntqdm.pandas()","4504864a":"img_dir = Path('..') \/ 'input' \/ 'pulmonary-chest-xray-abnormalities'\nimg_df = pd.DataFrame(dict(path = list(img_dir.glob('**\/*.*'))))\nimg_df['img_id'] = img_df['path'].map(lambda x: x.stem)\nimg_df['folder'] = img_df['path'].map(lambda x: x.parent.stem)\nimg_df['group'] = img_df['path'].map(lambda x: x.parent.parent.stem)\nimg_df = img_df[img_df['path'].map(lambda x: x.suffix[1:].lower() in {'png', 'jpg'})]\nimg_df","5d4de6cc":"IMG_SIZE = (1024, 1024)\ndef read_seg_map(in_row):\n    \"\"\"Read segmentation maps as images\"\"\"\n    right_img = imread(in_row['rightMask'], as_gray=True)\n    left_img = imread(in_row['leftMask'], as_gray=True)\n    comb_img = np.clip(255.0*(right_img+left_img), 0, 255).astype('uint8')\n    rs_img = resize(comb_img, IMG_SIZE)\n    return np.expand_dims(rs_img, -1)\ncolorize = lambda x: (gray2rgb(x)*255).clip(0, 255).astype('uint8')[:, :, :3]","7e1ff341":"img_pairs_df = img_df.\\\n    pivot_table(\n        columns='folder', \n        index='img_id', \n        values='path', \n        aggfunc='first').\\\n    dropna().\\\n    sample(50, random_state=2019)\nimg_pairs_df['segmap'] = img_pairs_df.progress_apply(read_seg_map, axis=1)\nimg_pairs_df['rgb_image'] = img_pairs_df['CXR_png'].progress_map(lambda x: \n                                                                 colorize(\n                                                                     resize(\n                                                                         imread(x, as_gray=True),\n                                                                         IMG_SIZE\n                                                                     ) \n                                                                 )\n                                                                )\nprint(img_pairs_df.shape[0])\nimg_pairs_df.sample(1)","adc41683":"def show_row(n_axs, c_row, channel_wise=True):\n    (ax1, ax2, ax3) = n_axs\n    ax1.imshow(c_row['rgb_image'].squeeze())\n    ax1.axis('off')\n    \n    segmap = c_row['segmap']\n    col_map = (segmap[:, :, 0]>0.5).astype('int')\n    ax2.imshow(col_map)\n    ax2.axis('off')\n    \n    ax3.imshow(label2rgb(image=c_row['rgb_image'].squeeze(), label=col_map.astype('int'), bg_label=0))\n    ax3.axis('off')\n    \nfig, m_axs = plt.subplots(3, min(len(img_pairs_df), 3), figsize=(15, 5))\n\nfor (c_idx, c_row), n_axs in zip(img_pairs_df.iterrows(), m_axs.T):\n    show_row(n_axs, c_row)","aee5906c":"from sklearn.model_selection import train_test_split\ntrain_df, valid_df = train_test_split(img_pairs_df, test_size=0.2, random_state=2019)\ntrain_df = train_df.copy()\nvalid_df = valid_df.copy() # make the datasets independent of the input","1dbcada2":"from keras import layers, models\nfrom keras.utils.vis_utils import model_to_dot\nfrom IPython.display import SVG\nimport keras.backend as K\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    A simple DICE implementation without any weighting\n    \"\"\"\n    y_t = K.batch_flatten(y_true)\n    y_p = K.batch_flatten(y_pred)\n    return 2.0 * K.sum(y_t * y_p) \/ (K.sum(y_t) + K.sum(y_p) + K.epsilon())\ndef dice_loss(y_true, y_pred):\n    \"\"\"\n    A simple inverted dice to use as a loss function\n    \"\"\"\n    return 1 - dice_score(y_true, y_pred)","739debbd":"from albumentations import (\n    PadIfNeeded,\n    HorizontalFlip,\n    VerticalFlip,    \n    CenterCrop,    \n    Crop,\n    Compose,\n    Transpose,\n    RandomRotate90,\n    ElasticTransform,\n    GridDistortion, \n    OpticalDistortion,\n    RandomSizedCrop,\n    OneOf,\n    CLAHE,\n    RandomBrightnessContrast,    \n    RandomGamma,\n    HueSaturationValue,\n    MedianBlur, \n    MotionBlur,\n    Blur,\n    RandomFog,\n    Rotate\n)","088c5154":"print(img_pairs_df['segmap'].iloc[0].max())\ntile_size = (256, 256)\naug = Compose([\n    Rotate(limit=15, p=0.5),\n    OneOf([\n        ElasticTransform(p=0.5, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n        GridDistortion(p=0.5),\n        OpticalDistortion(p=1, distort_limit=2, shift_limit=0.5)                  \n        ], p=0.8),\n    RandomFog(p=0.1),\n    CLAHE(p=0.1),\n    OneOf([\n        RandomBrightnessContrast(p=0.5),    \n        RandomGamma(p=0.5),\n        HueSaturationValue(hue_shift_limit=10, sat_shift_limit=10, val_shift_limit=0, p=0.5),\n        HueSaturationValue(hue_shift_limit=30, sat_shift_limit=10, val_shift_limit=0, p=0.25),\n    ]),\n    Blur(p=0.2),\n    RandomSizedCrop(min_max_height=(128, 512), width=tile_size[0], height=tile_size[1])\n])","d8381faa":"aug_df = pd.concat([\n    train_df.sample(n=16, replace=True, random_state=i).\\\n        apply(\n            lambda x: pd.Series(\n                aug(image=x['rgb_image'], \n                    mask=(x['segmap']>0.5).astype('uint8')\n                   )\n            ), 1) \n    for i in tqdm(range(16))], ignore_index=True).\\\n    rename(columns={'image': 'rgb_image', 'mask': 'segmap'})","09787e19":"sample_aug_df = aug_df.sample(12)\nfig, m_axs = plt.subplots(3, len(sample_aug_df), figsize=(15, 5))\nfor (c_idx, c_row), n_axs in zip(sample_aug_df.iterrows(), m_axs.T):\n    show_row(n_axs, c_row)","253a9ddb":"X_train = np.stack(aug_df['rgb_image'], 0)\ny_train = np.stack(aug_df['segmap'], 0)\nprint(X_train.shape, y_train.shape)","61922c43":"X_valid = np.stack(valid_df['rgb_image'], 0)\ny_valid = np.stack(valid_df['segmap'], 0)\nprint(X_valid.shape, y_valid.shape)","af95d49d":"from IPython.display import clear_output\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\ndef get_callbacks(in_model):\n    weight_path=\"{}_weights.best.hdf5\".format(in_model.name)\n\n    checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n                                 save_best_only=True, mode='min', save_weights_only = True)\n\n    reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=3, verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.0001)\n    early = EarlyStopping(monitor=\"val_loss\", \n                          mode=\"min\", \n                          patience=15) # probably needs to be more patient, but kaggle time is limited\n    return [checkpoint, early, reduceLROnPlat], weight_path\n\ndef fit_model(in_model, epochs=50, batch_size=16, loss_func='binary_crossentropy'):\n    in_model.compile(loss=loss_func, metrics=['binary_accuracy', dice_score, 'mae'], optimizer='adam')\n    callback_list, weight_path = get_callbacks(in_model)\n    out_results = in_model.fit(X_train, y_train, \n                                   epochs=epochs,\n                                   batch_size=batch_size,\n                                   validation_data=(X_valid, y_valid), \n                                   callbacks=callback_list)\n    in_model.load_weights(weight_path)\n    in_model.save(weight_path.replace('_weights', '_model'))\n    clear_output()\n    v_keys = [k for k in out_results.history.keys() if 'val_{}'.format(k) in out_results.history.keys()]\n    fig, m_axs = plt.subplots(1, len(v_keys), figsize=(16, 4))\n    for c_key, c_ax in zip(v_keys, m_axs):\n        c_ax.plot(out_results.history[c_key], 'r-', label='Training')\n        val_vec = out_results.history['val_{}'.format(c_key)]\n        c_ax.plot(val_vec, 'b-', label='Validation (Best: {:2.2%})'.format(np.nanmin(val_vec) if 'loss' in c_key else np.nanmax(val_vec)))\n        c_ax.set_title(c_key)\n        c_ax.legend()\n    fig.savefig(weight_path.replace('.hdf5', '.png'))\n    return out_results","3ed4f844":"def show_training(in_model):\n    sample_aug_df['predictions'] = [x for x in in_model.predict(np.stack(sample_aug_df['rgb_image'], 0))]\n    fig, m_axs = plt.subplots(5, len(sample_aug_df), figsize=(15, 10))\n    m_axs[1, 0].set_title('Ground-Truth')\n    m_axs[3, 0].set_title('Prediction')\n    for (c_idx, c_row), (ax1, ax2, ax3, ax4, ax5) in zip(sample_aug_df.iterrows(), \n                                     m_axs.T):\n        show_row((ax1, ax2, ax3), c_row)\n        show_row((ax1, ax4, ax5), {'rgb_image': c_row['rgb_image'], 'segmap': c_row['predictions']})\ndef show_validation(in_model):\n    valid_df['predictions'] = [x for x in in_model.predict(np.stack(valid_df['rgb_image'], 0))]\n    fig, m_axs = plt.subplots(5, len(valid_df), figsize=(8, 12))\n    m_axs[1, 0].set_title('Ground-Truth')\n    m_axs[3, 0].set_title('Prediction')\n    for (c_idx, c_row), (ax1, ax2, ax3, ax4, ax5) in zip(valid_df.iterrows(), \n                                     m_axs.T):\n        show_row((ax1, ax2, ax3), c_row)\n        show_row((ax1, ax4, ax5), {'rgb_image': c_row['rgb_image'], 'segmap': c_row['predictions']})\n        ","f86ded5e":"from keras.models import Model\nfrom keras.layers import BatchNormalization, Conv2D, Conv2DTranspose, MaxPooling2D, Dropout, UpSampling2D, Input, concatenate\nfrom keras import layers\n\ndef conv2d_block(\n    inputs, \n    use_batch_norm=True, \n    dropout=0.3, \n    filters=16, \n    kernel_size=(3,3), \n    activation='leakyrelu', \n    kernel_initializer='he_normal', \n    double_layer=True,\n    padding='same'):\n    c = inputs\n    for _ in range(2 if double_layer else 1):\n        c = Conv2D(filters, kernel_size, activation='linear', kernel_initializer=kernel_initializer, padding=padding, use_bias=not use_batch_norm) (inputs)\n        if use_batch_norm:\n            c = BatchNormalization()(c)\n        if dropout > 0.0:\n            c = Dropout(dropout)(c)\n        if activation.lower().startswith('leaky'):\n            c = layers.LeakyReLU(0.1)(c)\n        else:\n            c = layers.Activation(activation)(c)\n    return c\n\ndef basic_unet(\n    input_shape,\n    num_classes=1,\n    dropout=0.0, \n    filters=64,\n    num_layers=4,\n    use_deconv=False,\n    crop_output=True,\n    output_name='OutMask',\n    output_activation='sigmoid'): # 'sigmoid' or 'softmax'\n    \"\"\"taken from https:\/\/github.com\/karolzak\/keras-unet\"\"\"\n    # Build U-Net model\n    inputs = Input(input_shape)\n    x = BatchNormalization()(inputs)   \n\n    down_layers = []\n    for l in range(num_layers):\n        x = conv2d_block(inputs=x, filters=filters, use_batch_norm=False, dropout=0.0, padding='same')\n        down_layers.append(x)\n        x = MaxPooling2D((2, 2), strides=2) (x)\n        filters = filters*2 # double the number of filters with each layer\n\n    x = Dropout(dropout)(x)\n    x = conv2d_block(inputs=x, filters=filters, use_batch_norm=False, dropout=0.0, padding='same')\n\n    for conv in reversed(down_layers):\n        filters \/\/= 2 # decreasing number of filters with each layer \n        if use_deconv:\n            x = Conv2DTranspose(filters, (2, 2), strides=(2, 2), padding='same') (x)\n        else:\n            x = UpSampling2D((2, 2))(x)\n        \n        x = concatenate([x, conv])\n        x = conv2d_block(inputs=x, filters=filters, use_batch_norm=False, dropout=0.0, padding='same')\n    \n    outputs = Conv2D(num_classes, (1, 1), \n                     activation=output_activation, \n                     name=output_name if not crop_output else 'Pre_{}'.format(output_name)) (x)    \n    if crop_output:\n        lay_17_crop = layers.Cropping2D((2**num_layers, 2**num_layers))(outputs)\n        outputs = layers.ZeroPadding2D((2**num_layers, 2**num_layers), name=output_name)(lay_17_crop)\n\n    model = Model(inputs=[inputs], outputs=[outputs], name='VanillaUNET')\n    return model","f040a349":"simple_unet = basic_unet((None, None, 3), num_classes=1, num_layers=4, dropout=0.1, filters=32)","6ca392df":"dot_mod = model_to_dot(simple_unet, show_shapes=True, show_layer_names=False)\ndot_mod.set_rankdir('UD')\nSVG(dot_mod.create_svg())","6dda0086":"fit_model(simple_unet, epochs=50, loss_func='binary_crossentropy', batch_size=16)","fa9172de":"plt.imshow(simple_unet.predict(np.random.uniform(0, 255, size=(1, 128, 128, 3)))[0, :, :, 0])","85499f6f":"show_training(simple_unet)","c6bc48ef":"show_validation(simple_unet)","88e221d2":"pre_unet = basic_unet((None, None, 3), num_classes=1, num_layers=2, filters=8)\npre_unet.name = \"FirstGuess\"\npre_unet.summary()","0c59555e":"def recursive_unet_block(\n    input_shape,\n    num_classes=1,\n    dropout=0.0, \n    filters=16,\n    num_layers=4,\n    use_deconv=False,\n    crop_output=True,\n    resnet_style=False):\n    \"\"\"a unet model that can be run multiple times\"\"\"\n    # Build U-Net model\n    inputs = Input(input_shape, name='InImage')\n    bn_img = BatchNormalization()(inputs)\n    last_mask = Input(input_shape[:2]+(num_classes,), name='InMask')\n    down_layers = [[], []]\n    \n    c_pair = [bn_img, last_mask]\n    for l in range(num_layers):\n        for i, x in enumerate(c_pair):\n            if (l>0) and (i==0): # combine the mask and input channels\n                x = concatenate(c_pair)\n            \n            x = conv2d_block(inputs=x, filters=filters, use_batch_norm=False, dropout=0.0, padding='same')\n            down_layers[i].append(x)\n            x = MaxPooling2D((2, 2), strides=2)(x)\n            c_pair[i] = x\n        \n        filters = filters*2 # double the number of filters with each layer\n    \n    x = concatenate(c_pair)\n    x = Dropout(dropout)(x)\n    x = conv2d_block(inputs=x, filters=filters, use_batch_norm=False, dropout=0.0, padding='same')\n\n    for conv_img, conv_mask in zip(reversed(down_layers[0]), reversed(down_layers[1])):\n        filters \/\/= 2 # decreasing number of filters with each layer \n        if use_deconv:\n            x = Conv2DTranspose(filters, (2, 2), strides=(2, 2), padding='same') (x)\n        else:\n            x = UpSampling2D((2, 2))(x)\n        \n        x = concatenate([x, conv_img, conv_mask])\n        x = conv2d_block(inputs=x, filters=filters, use_batch_norm=False, dropout=0.0, padding='same')\n    \n    outputs = Conv2D(num_classes, (1, 1), activation='tanh' if resnet_style else 'sigmoid') (x)   \n    \n    if resnet_style:\n        outputs = layers.add([last_mask, outputs]) # resnet style\n    \n    if crop_output:\n        lay_17_crop = layers.Cropping2D((2**num_layers, 2**num_layers))(outputs)\n        outputs = layers.ZeroPadding2D((2**num_layers, 2**num_layers), name='OutRecursive')(lay_17_crop)\n    \n    model = Model(inputs=[inputs, last_mask], outputs=[outputs], name='RecursiveBlock')\n    return model","ce566034":"rec_block = recursive_unet_block(input_shape=(None, None, 3), num_classes=1, num_layers=4, filters=16)","78a6b154":"dot_mod = model_to_dot(rec_block, show_shapes=True, show_layer_names=False)\ndot_mod.set_rankdir('UD')\ndot_mod.write_svg('rec_block.svg')\nSVG(dot_mod.create_svg())","112f851e":"def build_rec_unet(recursive_steps):\n    in_img = Input((None, None, 3))\n    first_seg = pre_unet(in_img)\n    out_seg_list = [first_seg]\n    for i in range(recursive_steps):\n        noisy_seg = layers.GaussianNoise(0.1)(out_seg_list[-1]) # jitter things a bit\n        out_seg_list.append(rec_block([in_img, noisy_seg]))\n    for i in range(len(out_seg_list)):\n        # name the outputs sensibly\n        out_seg_list[i] = layers.Lambda(lambda x: x, name='Iter{:02}'.format(i))(out_seg_list[i])\n    return Model(inputs=[in_img], outputs=out_seg_list, name='RecUNET')\nrec_unet = build_rec_unet(4)","0a98cc6e":"dot_mod = model_to_dot(rec_unet, show_shapes=True, show_layer_names=True)\ndot_mod.set_rankdir('LR')\ndot_mod.write_svg('rec_unet.svg')\nSVG(dot_mod.create_svg())","ec9f7125":"rec_unet.summary()","23a53adc":"from keras import losses\ndef fit_rec_model(in_model, \n                  epochs=50, \n                  batch_size=8,\n                 loss_weight_min=0.1):\n    rec_steps = len(in_model.outputs)\n    # stagger losses from being mostly dice to being mostly BCE\n    loss_funcs = {\n        'Iter{:02d}'.format(i): lambda x,y: (1-k)*dice_loss(x,y)+k*losses.binary_crossentropy(x,y) \n        for i,k in enumerate(np.linspace(loss_weight_min, 1-loss_weight_min, rec_steps))\n    }\n    all_results = []\n    # progressively grow the training\n    for c_step in range(rec_steps):\n        base_weights = np.zeros(rec_steps)\n        base_weights[:c_step] = 1\n        base_weights[c_step] = 3\n        loss_weights = base_weights\/np.sum(base_weights)\n\n        in_model.compile(loss=loss_funcs, \n                         metrics=['binary_accuracy', dice_score, 'mae'], \n                         optimizer='adam', \n                         loss_weights=loss_weights.tolist())\n\n        callback_list, weight_path = get_callbacks(in_model)\n\n        out_results = in_model.fit(X_train, [y_train]*(rec_steps), \n                                       epochs=epochs,\n                                       batch_size=batch_size,\n                                       validation_data=(X_valid, [y_valid]*(rec_steps)), \n                                       callbacks=callback_list)\n        all_results += [out_results]\n        in_model.load_weights(weight_path)\n    \n        clear_output()\n        v_keys = [k for k in out_results.history.keys() if 'val_{}'.format(k) in out_results.history.keys()]\n        fig, m_axs = plt.subplots(1, len(v_keys), figsize=(4*len(v_keys), 4))\n        for c_key, c_ax in zip(v_keys, m_axs):\n            c_ax.plot(out_results.history[c_key], 'r-', label='Training')\n            val_vec = out_results.history['val_{}'.format(c_key)]\n            c_ax.plot(val_vec, 'b-', label='Validation (Best: {:2.2%})'.format(np.nanmin(val_vec) if 'loss' in c_key else np.nanmax(val_vec)))\n            c_ax.set_title(c_key)\n            c_ax.legend()\n        fig.savefig(weight_path.replace('.hdf5', '.png'))\n\n        in_model.save(weight_path.replace('_weights', '_model'))\n    out_df = pd.concat([pd.DataFrame(keras_history.history).\\\n                            assign(train_block=i).\\\n                            reset_index().\\\n                            rename(columns={'index': 'inner_epoch'})\n                        for i, keras_history in enumerate(all_results)],\n                      ignore_index=True).\\\n        reset_index().\\\n        rename(columns={'index': 'epoch'})\n    return out_df","51398a1e":"rec_fit_df = fit_rec_model(rec_unet, epochs=50)\nrec_fit_df","49d2bac6":"full_fit_df = pd.melt(rec_fit_df, id_vars=['train_block','epoch', 'inner_epoch']).query('variable!=\"lr\"')\nfull_fit_df['split'] = full_fit_df['variable'].map(lambda x: 'validation' if x.startswith('val_') else 'training')\nfull_fit_df['clean_variable'] = full_fit_df['variable'].map(lambda x: x.replace('val_', ''))\nfull_fit_df['model'] = full_fit_df['clean_variable'].map(lambda x: x.split('_')[0] if '_' in x else 'all')\nfull_fit_df['variable'] = full_fit_df['clean_variable'].map(lambda x: '_'.join(x.split('_')[1:]) if '_' in x else x)\nfull_fit_df.to_csv('rec_values.csv', index=False)\nfull_fit_df.head(5)","9e064528":"sns.catplot(data=full_fit_df.query('variable!=\"loss\"'), \n            x='epoch', \n            y='value', \n            col='split', \n            hue='model', \n            row='variable', \n            kind='point', \n            sharey='row')","717e2148":"sns.catplot(data=full_fit_df.query('variable!=\"loss\"'), \n            x='model', \n            y='value', \n            col='split', \n            hue='epoch', \n            row='variable', \n            kind='point', \n            sharey='row')","e197803c":"full_fit_df.query('variable==\"binary_accuracy\"').query('split==\"validation\"').pivot_table(columns='model', values='value', index=['epoch'])","37bee657":"in_vec = Input((None, None, 3))\nmulti_out = rec_unet(in_vec)\nsimple_rec_unet = Model(inputs=[in_vec], \n                        outputs=[multi_out[-1]])","b1660dbd":"show_training(simple_rec_unet)","f525658a":"show_validation(simple_rec_unet)","b53eb965":"show_training(pre_unet)","e091cb1c":"def show_multistep(in_df, samples=2, steps=3):\n    fig, m_axs = plt.subplots((2+steps)*2+1, samples, figsize=(6*samples, 4*((2+steps)*2+1)))\n    m_axs[1, 0].set_title('Ground-Truth')\n    \n    for (c_idx, c_row), n_axs in zip(in_df.sample(samples, random_state=0).iterrows(), \n                                     m_axs.T):\n        ax1 = n_axs[0]\n        show_row([ax1]+n_axs[1:3].tolist(), c_row)\n        in_img = np.expand_dims(c_row['rgb_image'], 0)\n        last_seg = pre_unet.predict(in_img)\n        show_row([ax1]+n_axs[3:5].tolist(), {'rgb_image': c_row['rgb_image'], 'segmap': last_seg[0]})\n        m_axs[3, 0].set_title('Simple')\n        for i in range(2, 2+steps):\n            last_seg = rec_block.predict([in_img, last_seg])\n            j = 2*i+1\n            show_row([ax1]+n_axs[j:j+2].tolist(), {'rgb_image': c_row['rgb_image'], 'segmap': last_seg[0]})\n            \n            m_axs[j, 0].set_title('Rec#{}'.format(i-2))\n            m_axs[j+1, 0].set_title('Rec#{}'.format(i-2))","d932b7be":"show_multistep(sample_aug_df, samples=4, steps=4)","bb67dc6c":"show_multistep(valid_df, steps=4)","171fee46":"# Setup and Loading","86834673":"# Vanilla U-Net","b764b5f4":"# Simple U-Net","b4ca2a3b":"# Recursive Model\nThe recursive model starts with a simple U-Net to get the initial segmentation and then ","9c2df05b":"# Overview\n## Model\nThe model choice is a recursive U-Net where the output segmentation is fed into the same model again as a _seed_ segmentation for making the next prediction. The idea is that maybe the model can get iteratively better by knowing what the last model produced. The parameter update is quite complicated since with 4 loops it is going through the same model 4 times (a bit RNN-like), but it might work?","efcfc69a":"### Make a simple, single input, single output model","d3f6e267":"## Deep Learning Section","4ad01cfe":"# Create the static datasets","45f7532f":"### Glue the beast together","44540d16":"### Progressively Growing Training\nHere we train the model one output at a time starting with the simple first layer and then progressively adding outputs and de-weighting the earlier stages. The idea should be to get a more stable result than trying the train the whole mess at once.","bb2a6ae3":"# Build and Train Models","a2e3fddf":"## Result Code","9fbcc63e":"## Show other components","fa609003":"## Setup the Augmentation","09d06896":"## Training Code"}}