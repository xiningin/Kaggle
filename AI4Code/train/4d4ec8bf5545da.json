{"cell_type":{"0d5379ea":"code","e93d2993":"code","5c3a5ad3":"markdown","d20fb604":"markdown","e89d0c88":"markdown","a426ce77":"markdown"},"source":{"0d5379ea":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n# Any results you write to the current directory are saved as output.","e93d2993":"import matplotlib.pyplot as plt\nfrom numpy import *\n\n# y = mx + b\n# m is slope, b is y-intercept\ndef compute_error_for_line_given_points(b, m, points):\n    totalError = 0\n    for i in range(0, len(points)):\n        x = points[i, 0]\n        y = points[i, 1]\n        totalError += (y - (m * x + b)) ** 2\n    return totalError \/ float(len(points))\n\ndef step_gradient(b_current, m_current, points, learningRate):\n    b_gradient = 0\n    m_gradient = 0\n    N = float(len(points))\n    for i in range(0, len(points)):\n        x = points[i, 0]\n        y = points[i, 1]\n        b_gradient += -(2\/N) * (y - ((m_current * x) + b_current))\n        m_gradient += -(2\/N) * x * (y - ((m_current * x) + b_current))\n    new_b = b_current - (learningRate * b_gradient)\n    new_m = m_current - (learningRate * m_gradient)\n    return [new_b, new_m]\n\ndef gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations):\n    b = starting_b\n    m = starting_m\n    for i in range(num_iterations):\n        b, m = step_gradient(b, m, array(points), learning_rate)\n    return [b, m]\n\ndef run():\n    points = genfromtxt(\"..\/input\/linear_demo.csv\", delimiter=\",\")\n    learning_rate = 0.0001\n    initial_b = 0 # initial y-intercept guess\n    initial_m = 0 # initial slope guess\n    num_iterations = 1000\n    print(\"Starting gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_m, compute_error_for_line_given_points(initial_b, initial_m, points)))\n    print (\"Running...\")\n    [b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations)\n    #print (\"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, b, m, compute_error_for_line_given_points(b, m, points)))\n    print (f' after iteration={num_iterations}  b={b} m={m}', \"error=\",compute_error_for_line_given_points(b, m, points)) \n    \n    x,y = genfromtxt(\"..\/input\/linear_demo.csv\",unpack=True, delimiter=\",\")\n    plt.scatter(x,y)\n    new_y=m*x+b\n    plt.plot(x, new_y, '-b')\n    plt.show()\nif __name__ == '__main__':\n    run()","5c3a5ad3":"> Implementing the alogrithm on small dataset which is linear. this is just to demonstrate gradient descent.\n","d20fb604":"By applying gradeint descent finding the optimal values of m and  below is the code.","e89d0c88":"**Optimization** is a big part of machine learning. Almost every machine learning algorithm has\nan optimization algorithm at it's core.So this is simple optimization\nalgorithm that you can use with any machine learning algorithm.\n\n**Intuition for Gradient Descent**\nThink of a large bowl like what you would eat serial out of or store fruit in. This bowl is a plot\nof the cost function (f). A random position on the surface of the bowl is the cost of the current\nvalues of the coeffecients (cost). The bottom of the bowl is the cost of the best set of coefficients,\nthe minimum of the function.\nThe goal is to continue to try defferent values for the coeffecients, evaluate their cost and\nselect new coefficients that have a slightly better (lower) cost. Repeating this process enough\ntimes will lead to the bottom of the bowl and you will know the values of the coefficients that\nresult in the minimum cost.\n\n**Gradient Descent**\nGradient descent is an optimization algorithm used to find the values of parameters (coefficients)\nof a function (f) that minimizes a cost function (cost). Gradient descent is best used when the\nparameters cannot be calculated analytically (e.g. using linear algebra) and must be searched\nfor by an optimization algorithm.\n\n> Gradient descent is the most popular optimization strategy in deep learning, in particular an implementation of it called backpropagation. We are using gradient descent as our optimization strategy for linear regression.\n\n**Gradient Descent Procedure**\nThe procedure starts of with initial values for the coeffcients for the function.These could be 0.0 or a small random value.\ncoeffcient = 0:0 \nThe cost of the coeffcients is evaluated by plugging them into the function and calculating\nthe cost.\n cost = f(coeffcient)\n \n cost = evaluate(f(coeffcient))\nThe derivative of the cost is calculated. The derivative is a concept from calculus and refers\nto the slope of the function at a given point. We need to know the slope so that we know the\ndirection (sign) to move the coeffcient values in order to get a lower cost on the next iteration.\n\ndelta = derivative(cost) \nNow that we know from the derivative which direction is downhill, we can now update the\ncoeffcient values. A learning rate parameter (alpha) must be specifed that controls how much\nthe coeffcients can change on each update.\n\ncoeffcient = coeffcient - (alpha * delta)\nhis process is repeated until the cost of the coeffcients (cost) is 0.0 or no further improve-\nments in cost can be achieved.","a426ce77":"Here are some helpful links:\n\nGradient descent visualization\nhttps:\/\/raw.githubusercontent.com\/mattnedrich\/GradientDescentExample\/master\/gradient_descent_example.gif\n\nSum of squared distances formula (to calculate our error)\nhttps:\/\/spin.atomicobject.com\/wp-content\/uploads\/linear_regression_error1.png\n\nPartial derivative with respect to b and m (to perform gradient descent)\nhttps:\/\/spin.atomicobject.com\/wp-content\/uploads\/linear_regression_gradient1.png"}}