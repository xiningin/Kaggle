{"cell_type":{"dea2943f":"code","2d8c560e":"code","98b816cb":"code","d0dbc5c8":"code","defc2d2b":"code","7a9647e2":"code","1759bff1":"code","7e3472ac":"code","d4c3f49a":"code","00644476":"code","7070e456":"code","250e58df":"code","9d78aac0":"code","a262ca3d":"code","3e8240e9":"code","9a10a793":"code","b17efbbe":"code","b9377529":"code","5f863e57":"code","9cde3019":"code","0e9bbe1a":"code","0e017e93":"code","66c0725f":"code","92a47e5e":"code","68865f73":"code","b4a77380":"code","d5a14ab4":"code","5e150350":"code","857654d2":"code","b45cd7b6":"code","716239b2":"code","eaee71c9":"code","96997ea3":"markdown","c48c8ee0":"markdown","28d2d17f":"markdown","d9462641":"markdown","47fbdbaa":"markdown","d0708b6c":"markdown","1335b63f":"markdown","51217473":"markdown","27df5666":"markdown","e1e193fa":"markdown","74b933b2":"markdown","b141a46f":"markdown","109cf8eb":"markdown","cb9ddad5":"markdown","0848fc67":"markdown","211d2ffa":"markdown","1a9beb71":"markdown","5c52b4ac":"markdown","03ace480":"markdown","9331dc06":"markdown","eee4bd04":"markdown","f81b7fe0":"markdown","bfe5353f":"markdown","28a6a15e":"markdown","973a06de":"markdown","270f4e19":"markdown","396a6f9f":"markdown","0210ba1c":"markdown","a42bb9a0":"markdown","c6966893":"markdown","fa870fd9":"markdown","0fc7ddcd":"markdown","b6cef354":"markdown","9299724e":"markdown","899657af":"markdown","98bf6e11":"markdown","8bc9b89f":"markdown","bea8a2a5":"markdown","459a4b22":"markdown","e8a08659":"markdown","13f9e2f5":"markdown","08aa1d44":"markdown","eeea0cb0":"markdown","c088daef":"markdown","aa08cfc2":"markdown","bf1e6572":"markdown","64150940":"markdown","55a4fe6d":"markdown"},"source":{"dea2943f":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression,LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectKBest,SelectPercentile,f_classif,f_regression,mutual_info_regression,mutual_info_classif,SelectFromModel,RFE\n\npd.set_option('max_columns',100)\npd.set_option('max_rows',900)\n\npd.set_option('max_colwidth',200)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","2d8c560e":"df_heart = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\n\ndf_heart.head()","98b816cb":"df_car = pd.read_csv('..\/input\/car-price-prediction\/CarPrice_Assignment.csv', usecols = ['price', 'wheelbase', 'carlength', 'carwidth', 'carheight','curbweight', 'enginesize', 'boreratio', 'stroke', 'horsepower','peakrpm', 'citympg', 'highwaympg'])\ndf_car.head()","d0dbc5c8":"from sklearn.feature_selection import VarianceThreshold\n\nX= df_heart.drop('DEATH_EVENT', axis=1)\ny= df_heart['DEATH_EVENT']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.30)\n\n\nvth = VarianceThreshold(threshold=0)  # as deafult threshold=0\nvth.fit(X_train)\nX_train_vth = X_train.iloc[:, vth.get_support()]\n\npd.DataFrame( {'Feature': X_train.columns,'Variance': vth.variances_,}).sort_values('Variance', ascending=True)\n","defc2d2b":"\nX= df_car.drop('price', axis=1)\ny= df_car['price']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.30)\n\n\nvth = VarianceThreshold(threshold=0)  # as deafult threshold=0\nvth.fit(X_train)\nX_train_vth = X_train.iloc[:, vth.get_support()]\n\npd.DataFrame( {'Feature': X_train.columns,'Variance': vth.variances_,}).sort_values('Variance', ascending=True)\n","7a9647e2":"\nX= df_heart.drop('DEATH_EVENT', axis=1)\ny= df_heart['DEATH_EVENT']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.30)\n\nKbest_classif = SelectKBest(score_func=f_classif, k=6)\nKbest_classif.fit(X_train, y_train)\n\n# what are scores for the features\nfor i in range(len(Kbest_classif.scores_)):\n    print(f'Feature {i} : {round(Kbest_classif.scores_[i],3)}')\n\nprint()\n\nplt.bar([X_train.columns[i] for i in range(len(Kbest_classif.scores_))], Kbest_classif.scores_)\nplt.xticks(rotation=90)\nplt.rcParams[\"figure.figsize\"] = (8,6)\nplt.show()\n\n","1759bff1":"# transform training set\nX_train_classif = Kbest_classif.transform(X_train)\nprint(\"X_train.shape: {}\".format(X_train.shape))\nprint()\nprint(\"X_train_selected.shape: {}\".format(X_train_classif.shape))\nprint()\n# transform test data\nX_test_classif = Kbest_classif.transform(X_test)","7e3472ac":"\nlor = LogisticRegression(solver='liblinear', random_state=0)\nlor.fit(X_train, y_train)\n\nprint(f'Score with all features: {round(lor.score(X_test, y_test),4)}')\n\nlor.fit(X_train_classif, y_train)\n\nprint(f'Score with only selected features: {round(lor.score(X_test_classif, y_test),4)}')","d4c3f49a":"\nX= df_car.drop('price', axis=1)\ny= df_car['price']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.30)\n\nKbest_reg = SelectKBest(score_func=f_regression, k=6)\nKbest_reg.fit(X_train, y_train)\n\n\n\n# what are scores for the features\nfor i in range(len(Kbest_reg.scores_)):\n    print(f'Feature {i} : {round(Kbest_reg.scores_[i],3)}')\n\nprint()\n\n\n# plot the scores\nplt.bar([X_train.columns[i] for i in range(len(Kbest_reg.scores_))], Kbest_reg.scores_)\nplt.xticks(rotation=90)\nplt.rcParams[\"figure.figsize\"] = (8,6)\nplt.show()\n\n","00644476":"# transform training set\nX_train_reg = Kbest_reg.transform(X_train)\nprint(\"X_train.shape: {}\".format(X_train.shape))\nprint()\nprint(\"X_train_selected.shape: {}\".format(X_train_reg.shape))\nprint()\n# transform test data\nX_test_reg = Kbest_reg.transform(X_test)","7070e456":"\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\nprint(f'Score with all features: {round(lr.score(X_test, y_test),4)}')\n\nlr.fit(X_train_reg, y_train)\n\nprint(f'Score with only selected features: {round(lr.score(X_test_reg, y_test),4)}')","250e58df":"\nX= df_heart.drop('DEATH_EVENT', axis=1)\ny= df_heart['DEATH_EVENT']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.30)\n\ninfogain_classif = SelectKBest(score_func=mutual_info_classif, k=6)\n\ninfogain_classif.fit(X_train, y_train)\n\n\n\n# what are scores for the features\nfor i in range(len(infogain_classif.scores_)):\n    print(f'Feature {i} : {round(infogain_classif.scores_[i],3)}')\n\n\nprint()\n\n# plot the scores\nplt.bar([X_train.columns[i] for i in range(len(infogain_classif.scores_))], infogain_classif.scores_)\nplt.xticks(rotation=90)\nplt.rcParams[\"figure.figsize\"] = (8,6)\nplt.show()\n\n","9d78aac0":"# transform training set\nX_train_info_classif = infogain_classif.transform(X_train)\n\nprint(\"X_train.shape: {}\".format(X_train.shape))\nprint()\n\nprint(\"X_train_selected.shape: {}\".format(X_train_info_classif.shape))\nprint()\n\n# transform test data\nX_test_info_classif = infogain_classif.transform(X_test)","a262ca3d":"\nlor = LogisticRegression(solver='liblinear', random_state=0)\nlor.fit(X_train, y_train)\n\nprint(f'Score with all features: {round(lor.score(X_test, y_test),4)}')\n\nlor.fit(X_train_info_classif, y_train)\n\nprint(f'Score with only selected features: {round(lor.score(X_test_info_classif, y_test),4)}')","3e8240e9":"\nX= df_car.drop('price', axis=1)\ny= df_car['price']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.30)\n\ninfogain_reg = SelectKBest(score_func=mutual_info_regression, k=6)\n\ninfogain_reg.fit(X_train, y_train)\n\n\n\n# what are scores for the features\nfor i in range(len(infogain_reg.scores_)):\n    print('Feature %d: %f' % (i, infogain_reg.scores_[i]))\n\n\n# plot the scores\nplt.bar([X_train.columns[i] for i in range(len(infogain_reg.scores_))], infogain_reg.scores_)\nplt.xticks(rotation=90)\nplt.rcParams[\"figure.figsize\"] = (8,6)\nplt.show()\n\n","9a10a793":"# transform training set\nX_train_reg = infogain_reg.transform(X_train)\nprint(\"X_train.shape: {}\".format(X_train.shape))\nprint()\nprint(\"X_train_reg.shape: {}\".format(X_train_reg.shape))\nprint()\n# transform test data\nX_test_reg = infogain_reg.transform(X_test)","b17efbbe":"\nlr = LinearRegression()\nlr.fit(X_train, y_train)\nprint(f'Score with all features: {round(lr.score(X_test, y_test),4)}')\nlr.fit(X_train_reg, y_train)\nprint(f'Score with only selected features: {round(lr.score(X_test_reg, y_test),4)}')","b9377529":"X= df_heart.drop('DEATH_EVENT', axis=1)\ny= df_heart['DEATH_EVENT']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.30)\n\npercentile = SelectPercentile(percentile=50)\npercentile.fit(X_train, y_train)","5f863e57":"percentile.get_support()","9cde3019":"X_train.columns","0e9bbe1a":"# transform training set\nX_train_percentile = percentile.transform(X_train)\nprint(\"X_train.shape: {}\".format(X_train.shape))\n\nprint()\n\nprint(\"X_train_selected.shape: {}\".format(X_train_percentile.shape))\nprint()\n\n# transform test data\nX_test_percentile = percentile.transform(X_test)","0e017e93":"\nlor = LogisticRegression(solver='liblinear', random_state=0)\nlor.fit(X_train, y_train)\n\nprint(f'Score with all features: {round(lor.score(X_test, y_test),4)}')\n\nlor.fit(X_train_percentile, y_train)\n\nprint(f'Score with only selected features: {round(lor.score(X_test_percentile, y_test),4)}')","66c0725f":"model_based_feature = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42),threshold=\"median\")\n\nX= df_heart.drop('DEATH_EVENT', axis=1)\ny= df_heart['DEATH_EVENT']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.30)\n\n\nmodel_based_feature.fit(X_train, y_train)","92a47e5e":"model_based_feature.get_support()","68865f73":"X_train.columns","b4a77380":"# transform training set\nX_train_mbf = model_based_feature.transform(X_train)\nprint(f'X_train.shape: {X_train.shape}')\n\nprint()\n\nprint(f'X_train_selected.shape: {X_train_mbf.shape}')\nprint()\n\n# transform test data\nX_test_mbf = model_based_feature.transform(X_test)","d5a14ab4":"lor = LogisticRegression()\nlor.fit(X_train_mbf, y_train)\n\nprint(f'Score with only selected features: {round(lor.score(X_test_mbf, y_test),4)}')\n","5e150350":"rfe_features = RFE(RandomForestClassifier(n_estimators=100, random_state=42),n_features_to_select=6)\n\nX= df_heart.drop('DEATH_EVENT', axis=1)\ny= df_heart['DEATH_EVENT']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.30)\n\n\nrfe_features.fit(X_train, y_train)","857654d2":"rfe_features.get_support()","b45cd7b6":"X_train.columns","716239b2":"# transform training set\n\nX_train_rfe= rfe_features.transform(X_train)\n\nprint(f'X_train.shape: {X_train.shape}')\n\nprint()\n\nprint(f'X_train_selected.shape: {X_train_rfe.shape}')\nprint()\n\n# transform test data\nX_test_rfe= rfe_features.transform(X_test)","eaee71c9":"\nlor = LogisticRegression()\nlor.fit(X_train_mbf, y_train)\n\nprint(f'Score with only selected features: {round(lor.score(X_test_rfe, y_test),4)}')\n","96997ea3":"![](https:\/\/assets-global.website-files.com\/5debb9b4f88fbc3f702d579e\/60ecb081507f4559c84381f5_feature-selection-graphic.png)","c48c8ee0":"<a id=\"0\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>What is Feature Selection ?<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n","28d2d17f":"- We can see that **'age','ejection_fraction','serum_creatinine', 'serum_sodium', 'sex', 'time'** are selected by percentile.","d9462641":"### Heart Failure Clinical Records Dataset","47fbdbaa":"<a id=\"11\"><\/a>\n<font color=\"darkblue\" size=+1.5><b>Conclusion & Which Method is the Best?<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","d0708b6c":"<a id=\"1\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>Data<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","1335b63f":"- In this example, by using only 6 features of the dataset, we got better score than by using whole features.\n- We can suspect that maybe some of the features are uninformative and not providing much about the target variable.","51217473":"- Bu using only 6 of the features we couldn't get the better score. \n- Maybe other features on the dataset are informative about the target, we shoudl include them into our model.","27df5666":"- In this example, by using only 6 features of the dataset, we got better score than by using whole features.\n- We can suspect that maybe some of the features are uninformative and not providing much about the target variable.","e1e193fa":"Let's see the three  main approaches of feature selection in the supervised learning.","74b933b2":"- In this example, by using only 6 features of the dataset, we got better score than by using whole features.\n- We can suspect that maybe some of the features are uninformative and not providing much about the target variable.","b141a46f":"- Traditionaly, feature selection aims to improve accuracy and interpretability  \n- We can add these aims also algorithmic training time and reducing memory footprint.\n- Since we are dealing a large samples and thousands and thousand features, feature selection is one of the choice in our hands.\n- No single data-reduction method can be best suited for all applications. \n- Available knowledge on the application is the one of the most important things which feature selection is based on.\n- At the end there is no one best method to offer.\n- We need to find out best features for our problem by using different methods and algorithms with the systematic experimentation.\n- Even though in a real life cases, mostly we can not get large gains by applying feature selection, I can assure that it is valuable tool to have in your toolbox.\n","109cf8eb":"- None of our features have a zero variance, for that reason we didn't remove any of our features.","cb9ddad5":"Image Credit: https:\/\/miro.medium.com\/","0848fc67":"Variance threshold allows us to set a minimum threshold for an accepted variance in each feature. As a default it removes all zero-variance features (same value in all samples).","211d2ffa":"- We can see that **'age',''creatinine_phosphokinase',''ejection_fraction',''platelets',''serum_creatinine', 'time'** are selected by model.","1a9beb71":"#### **By the way, when you like the topic, you can show it by supporting** \ud83d\udc4d\n\n####  **Feel free to leave a comment in the notebook**. \n\n#### All the best \ud83e\udd18","5c52b4ac":"<a id=\"12\"><\/a>\n<font color=\"darkblue\" size=+1.5><b>References & Further Reading<\/b><\/font>\n\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n\n\n[Machine Learning - Beginner &Intermediate-Friendly BOOKS](https:\/\/www.kaggle.com\/general\/255972)","03ace480":"- Iterative feature selection uses different models which have different subsets of future variables.\n- One method, start wiith no features and add features one by one until the criteria defined by the user.\n- Other method, which we use in our example, RFE (recursive feature elimination),\n- RFE starts with the all the features and builds the model and eliminates the least important feature from the model.\n- Just to remember that, since iterative-wrapper method builts several models, it is computationaly expensive to use.","9331dc06":"- **Enjoy** \ud83e\udd18","eee4bd04":"Even though there are unsupervised feature selection techniques, in this study we will focus on the supervised feature selection techniques.","f81b7fe0":"<a id=\"3\"><\/a>\n<font color=\"lightseagreen\" size=+1><b>Removing features with low variance - Variance Threshold<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n","bfe5353f":"![](https:\/\/www.researchgate.net\/profile\/Enis-Karaarslan\/publication\/337591149\/figure\/fig2\/AS:830089595990017@1574920190654\/The-main-feature-selection-methods-for-machine-learning.png)","28a6a15e":"<a id=\"4\"><\/a>\n<font color=\"lightseagreen\" size=+1><b>Select KBest for Classification Problems<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","973a06de":"- Based on the SelectKBest, ['age','ejection_fraction','serum_creatinine', 'serum_sodium','sex','time'] are selected.","270f4e19":"<a id=\"10\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>Iterative Feature Selection- Wrapper<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","396a6f9f":"- With the better feature selection, we also gained some improvements here.\n- By using model based feature, even with base model Logistic Regression we get better result.","0210ba1c":"- Our main aim to find out statistically significant \/ meaningful relationship between features and target.\n- In this part, we will look at the different methods under the filter methods.\n    - Removing features with low variance - Variance Threshold\n    - KBest models for both classification and regression problems\n    - Information gain for both classification and regression problems\n    - Select percentile","a42bb9a0":"image credit: https:\/\/www.omnisci.com\/technical-glossary\/feature-selection","c6966893":"<a id=\"9\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>Model-Based Feature Selection - Embedded<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","fa870fd9":"> In machine learning and statistics, feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for several reasons:\n\n> * simplification of models to make them easier to interpret by researchers\/users,\n> * shorter training times,\n> * to avoid the curse of dimensionality,\n> * improve data's compatibility with a learning model class,\n> * encode inherent symmetries present in the input space.\n\nReference: https:\/\/en.wikipedia.org\/wiki\/Feature_selection","0fc7ddcd":"![](https:\/\/miro.medium.com\/max\/1400\/1*FUZS9K4JPqzfXDcC83BQTw.png)","b6cef354":"<a id=\"5\"><\/a>\n<font color=\"lightseagreen\" size=+1><b>Select KBest for Regression Problems<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","9299724e":"<a id=\"6\"><\/a>\n<font color=\"lightseagreen\" size=+1><b>Information Gain - Classification Problems<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","899657af":"Image Credit: https:\/\/www.researchgate.net\/profile\/Enis-Karaarslan\/publication\/337591149","98bf6e11":"<a id=\"2\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>Univariate Statistics - Filter Methods<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","8bc9b89f":"<a id=\"7\"><\/a>\n<font color=\"lightseagreen\" size=+1><b>Information Gain- Regression Problems<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","bea8a2a5":"#### Hi all.  \ud83d\ude4b\n\n#### We continue our **Beginner-Intermediate Friendly Machine Learning series**, which would help anyone who wants to learn or refresh the basics of ML.\n\n#### What we have covered: \n\n#### [Beginner Friendly Detailed Explained EDAs \u2013 For anyone at the beginnings of DS\/ML journey](https:\/\/www.kaggle.com\/general\/253911#1393015) \u2714\ufe0f\n\n#### [BIAS & VARIANCE TRADEOFF](https:\/\/www.kaggle.com\/kaanboke\/ml-basics-bias-variance-tradeoff) \u2714\ufe0f\n\n#### [LINEAR ALGORITHMS](https:\/\/www.kaggle.com\/kaanboke\/ml-basics-linear-algorithms)  \u2714\ufe0f\n\n#### [NONLINEAR ALGORITHMS](https:\/\/www.kaggle.com\/kaanboke\/nonlinear-algorithms)  \u2714\ufe0f\n\n#### [The Most Used Methods to Deal with MISSING VALUES](https:\/\/www.kaggle.com\/kaanboke\/the-most-used-methods-to-deal-with-missing-values)  \u2714\ufe0f\n\n#### [Beginner Friendly End to End ML Project- Classification with Imbalanced Data](https:\/\/www.kaggle.com\/kaanboke\/beginner-friendly-end-to-end-ml-project-enjoy)  \u2714\ufe0f\n\n#### [How to Prevent the Data Leakage ?](https:\/\/www.kaggle.com\/kaanboke\/how-to-prevent-the-data-leakage) \u2714\ufe0f\n\n#### [The Most Common EVALUATION METRICS- A Gentle Intro](https:\/\/www.kaggle.com\/kaanboke\/the-most-common-evaluation-metrics-a-gentle-intro) \u2714\ufe0f\n\n\n#### In this notebook we will  cover one of the important concepts of the Data Science Journey : **Feature Selection**\n#### Enjoy \ud83e\udd18","459a4b22":"- In this study I'll use two dataset. One for classification problems and other one for the prediction problems.","e8a08659":"![](https:\/\/media.giphy.com\/media\/u1PvrqJWlY4OPSxDJ5\/giphy-downsized-large.gif?cid=ecf05e47oo0ggho7d7xjvqnbs372rv1apu2eubitqk0ctt9d&rid=giphy-downsized-large.gif&ct=g)","13f9e2f5":"- With the better feature selection, we also gained some improvements here.\n- By using model based feature, even with base model Logistic Regression we get better result.","08aa1d44":"- We can see that **'age',''creatinine_phosphokinase',''ejection_fraction',''platelets',''serum_creatinine', 'time'** are selected by iterative future selection.","eeea0cb0":"### Car Price Dataset","c088daef":"- Let's see the differences with the whole features and the selected 6 features by using Logistic Regression as a base model.","aa08cfc2":"- Model based feature selection uses ML algortihms to determine the importance of the feature\n\n- After then, it keeps the only important ones.\n- We will use Random Forest Classifier to select the features\n- We will continue to use Logistic Regression as our base model.","bf1e6572":"- In the filter model the selection of features is done as a preprocessing activity.\n- We haven't tried to optimize the performance. \n- By using filter model,we select a subset of features that maximizes  model function.","64150940":"<a id=\"toc\"><\/a>\n\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Table of Contents<\/h3>\n    \n* [What is Feature Selection](#0)   \n* [Data](#1)\n* [Univariate Statistics-Filter Methods](#2)\n    * [Removing features with low variance - Variance Threshold](#3)\n    * [Select KBest for Classification Problems](#4)\n    * [Select KBest for Regression Problems](#5)\n    * [Information Gain - Classification Problems](#6)\n    * [Information Gain- Regression Problems](#7)\n    * [Select Percentile](#8)\n\n\n* [Model-Based Feature Selection - Embedded](#9)\n* [Iterative Feature Selection - Wrapper](#10)\n* [Conclusion](#11)\n* [References & Further Reading](#12)\n","55a4fe6d":"<a id=\"8\"><\/a>\n<font color=\"lightseagreen\" size=+1><b>Select Percentile<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>"}}