{"cell_type":{"c8842b08":"code","a3207e10":"code","a142f14d":"code","51f56306":"code","c20ad3cb":"code","2a1305e8":"code","31ea93e7":"code","4fa271e6":"code","e09453f7":"code","1c02efbe":"code","7b5ec9e6":"code","2233dfbe":"code","41759255":"code","3dac3086":"code","91c93525":"code","324c2420":"code","07d38b11":"code","5d1e7606":"code","93a25709":"markdown","a46152ec":"markdown","0254925c":"markdown","c91cd56e":"markdown","a865a4f3":"markdown","4a102f50":"markdown","dfae8daa":"markdown","6571b93c":"markdown","4a867677":"markdown"},"source":{"c8842b08":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy.stats import uniform, skew\nfrom scipy.special import boxcox1p, inv_boxcox1p\n\nfrom sklearn import set_config\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, ShuffleSplit, learning_curve, cross_val_score, KFold\nfrom sklearn.inspection import permutation_importance\n\nfrom sklearn.ensemble import RandomForestRegressor, StackingRegressor\nfrom sklearn.linear_model import LinearRegression, SGDRegressor, Ridge, Lasso\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\n\nfrom sklearn.svm import SVR\nfrom lightgbm import LGBMRegressor\n\nfrom category_encoders.target_encoder import TargetEncoder\n\nfrom sklearn import metrics\nimport json","a3207e10":"def display_estimator_parameters_for_pipeline(ppl):\n    '''\n    Display a formatted list of all the parameters and their values of the estimator of a pipeline.\n\n    Parameters:\n    ---\n    ppl: The pipeline of interest. The estimator must be the last step in the pipeline.\n    '''\n    name_model = ppl.steps[-1][0] + \"__\"\n    print(\"Name: %s\" % (ppl.steps[-1][0]))\n    print(\"Estimator: %s\" % (ppl.steps[-1][1]))    \n    print(\"------------------------------------------------\")\n    pars = ppl.get_params()\n    modelpars = {x: pars[x] for x in pars if name_model in x}\n    for (key, val) in modelpars.items():\n        print(\"%-24s: %-20s\" % (key.replace(name_model, \"\"), str(val)))","a142f14d":"def examine_learning_curve(ppl, X, y, n_splits=5, modelname=\"\"):\n    '''\n    Display the learning curves for a pipeline. The top panel shows the training errors and cross-validation errors as a function of the training sizes (subsample of 10% to 80% of the data). The bottom panel shows the time cost.\n\n    Parameters:\n    ---\n    ppl: Pipeline or model. \n    X: Features of the training data\n    y: Targets of the training data\n    n_splits: Number of train\/cross-validation pairs to resample\n    '''\n\n    fig, axs = plt.subplots(1, 2, num='learning_curve', figsize=(10,5))\n    cv = ShuffleSplit(n_splits=n_splits, test_size=0.2, random_state=12)\n    lc_sizes, lc_train, lc_cv, lc_time, _ = \\\n        learning_curve(ppl, X, y, cv=cv, return_times=True, \\\n                       train_sizes=np.linspace(0.1, 1.0, 9),\n                       scoring = 'neg_root_mean_squared_error')\n    # score vs. size\n    ax = axs.flatten()[0]\n    x, y, dy = lc_sizes, lc_train.mean(axis=1), lc_train.std(axis=1)\n    ax.plot(x, y, \"bo-\", label=\"training\")\n    ax.fill_between(x, y-dy, y+dy, alpha=0.2, color='blue')\n    x, y, dy = lc_sizes, lc_cv.mean(axis=1), lc_cv.std(axis=1)\n    ax.plot(x, y, \"ro-\", label=\"cross-validation\")\n    ax.fill_between(x, y-dy, y+dy, alpha=0.2, color='red')\n    ax.legend(loc=\"lower right\")\n    ax.set_ylabel(\"RMSE\")\n    #ax.set_title(\"Learning Curve (%s)\" % (ppl.steps[-1][0]))\n    ax.set_title(\"Learning Curve (%s)\" % (modelname))\n    ax.grid()\n    # time vs. size\n    ax = axs.flatten()[1]\n    x, y, dy = lc_sizes, lc_time.mean(axis=1), lc_time.std(axis=1)\n    ax.plot(x, y, \"bo-\", label=\"Time\")\n    ax.set_ylabel(\"Time cost (sec)\")\n    ax.set_xlabel(\"Training Size\")\n    ax.legend(loc=\"lower right\")\n    ax.grid()\n    plt.show()\n","51f56306":"print(\"Loading data from train.csv.\")\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\nm = train.shape[0]\n\n# outliers = train[train['GrLivArea'] > 4000].index\n# outliers.union(train[train['GarageArea'] > 1200].index)\noutliers = train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index\n\ntrain = train.drop(outliers)","c20ad3cb":"def preprocessing(raw, rank_mapping=None):\n    processed = raw.copy()\n\n    # Drop features that will not be used in modeling\n    # Combines features that would be dropped:\n    drop_features = [\"Id\",\"Utilities\",\"Street\"]\n    drop_features += ['Alley', 'PoolQC', 'Fence', 'MiscFeature'] # sparse features\n    #drop_features += ['MoSold', 'YrSold', 'SaleType', 'SaleCondition']\n\n    for var in drop_features:\n        if(var in processed):\n            # print(\"Drop feature: \", var)\n            processed = processed.drop(var, axis=1)\n\n    #Hard code the ordinal features using a .json file\n    with open('..\/input\/ordinal-features-mapping\/ordinal.json',) as f:\n        ordinal_feature_encoder = json.load(f)\n    processed = processed.replace(ordinal_feature_encoder)\n    ord_features = pd.Index(ordinal_feature_encoder.keys()).join(processed.columns, how='inner')\n\n    # MSSubClass is a nominal (categorical) feature but is encoded with numerics\n    for col in ['MSSubClass', 'MoSold', 'SaleType', 'SaleCondition']:\n        processed[col] = processed[col].astype('object')\n\n    vars_fill_zero = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'MasVnrArea', 'GarageArea', 'BsmtFullBath', 'BsmtHalfBath', 'GarageCars']\n\n    for var in vars_fill_zero:\n        processed[var] = processed[var].fillna(0)\n    for var in ord_features:\n        processed[var] = processed[var].fillna(0)\n        \n    # Target encoding using ranked median \n    # Be aware of encoding the test set this way.\n    if(rank_mapping is None):\n        rank_mapping = dict()\n        vars_te = ['Neighborhood', 'MSSubClass', 'Exterior1st', 'Exterior2nd', 'Condition1', 'SaleType', 'Condition2', 'RoofMatl']\n        for var in vars_te:\n            sorted_idx = processed.groupby(var)['SalePrice'].median().sort_values().argsort()\n            processed[var] = processed[var].map(sorted_idx)\n            rank_mapping[var] = sorted_idx\n    else: # test set\n        for var, sorted_idx in rank_mapping.items():\n            processed[var] = processed[var].map(sorted_idx)\n    \n    #processed['TotalSF'] = processed['TotalBsmtSF'] + processed['1stFlrSF'] + processed['2ndFlrSF']\n\n    num_features = processed.select_dtypes(['int', 'float']).columns\n    vars_skewed = []\n    #Apply boxcox transformation to all skewed features\n    print(\"Skewness of variables: \")\n    print(\"----------------\")    \n    for col in num_features:\n        if(col == 'SalePrice'): continue\n        if(abs(skew(processed[col])) > 1.0):\n            vars_skewed.append(col)\n            print(\"%-24s %f\" % (col, skew(processed[col])))\n            processed[col] = boxcox1p(processed[col], 0.15)\n    if 'SalePrice' in processed.columns:\n        processed['SalePrice'] = np.log1p(processed['SalePrice'])\n    \n\n    return processed, rank_mapping","2a1305e8":"train_derived, rank_mapping = preprocessing(train)\n\n# Check whether or not the rank encoding is correct\n# train_derived.groupby('Neighborhood')['SalePrice'].median()","31ea93e7":"# Use pipelines for the rest of the features.\n\nX = train_derived.drop('SalePrice', axis=1)\nytrain = train_derived['SalePrice']\n\nnum_features = X.select_dtypes(['int', 'float']).columns\ncat_features = X.select_dtypes(['object']).columns\n\nppl_num = Pipeline(steps = \\\n    [('imp_num', SimpleImputer(strategy = 'median')), ('scaler', RobustScaler())])    \n# or StandardScaler\nppl_cat = Pipeline(steps = \\\n    [('imp_cat', SimpleImputer(strategy='most_frequent')), ('ohe', OneHotEncoder(handle_unknown='ignore'))])\nct = ColumnTransformer(transformers= \\\n    [('numeric', ppl_num, num_features), \\\n     ('categorical', ppl_cat, cat_features)] \\\n)\n\nct.fit(X)\nXtrain = ct.transform(X)","4fa271e6":"def train_model_lasso_parameters_search(X_train, y_train):\n    model_lasso = Lasso(max_iter=100000)\n    #grid_params = {'alpha':[1.e-5, 3.e-5, 0.0001, 0.0003, 0.001, 0.003, 0.1, 0.3]}\n    grid_params = {'alpha':np.linspace(0.0001, 0.001, 50)}\n    reg_lasso = GridSearchCV(model_lasso, grid_params, \n                             cv=5, verbose=3, n_jobs=-1, \n                             scoring = 'neg_root_mean_squared_error',\n                             return_train_score=True) # r2\n    model = reg_lasso.fit(X_train, y_train)\n    gcv_matrix = pd.DataFrame(model.cv_results_)\n    gcv_bestpar = model.best_params_\n    gcv_scores = gcv_matrix.groupby('param_alpha')[['mean_test_score', 'std_test_score']].mean()\n    print(gcv_matrix.groupby('param_alpha')[['mean_fit_time', 'mean_train_score', 'mean_test_score']].mean())\n    print()        \n    print(\"Best Parameters:\")\n    print(\"--------------------------------\")\n    for (key, val) in gcv_bestpar.items():\n        print(\"%-24s: %-20s\" % (key, str(val)))\n    print()\n    print(\"Best Fit Coefficients:\")\n    print(\"--------------------------------\")\n    #reg = reg_lasso.best_estimator_.named_steps['lasso']\n    reg = reg_lasso.best_estimator_\n    print(\"Mean: %f\" % (abs(reg.coef_).mean()))\n    print(\"Std : %f\" % (abs(reg.coef_).std()))\n    print()\n    #print(\"Best Score (Hold-out): %7.5f\" % (reg_lasso.score(X_test, y_test)))\n    fig, ax = plt.subplots(1, 1, figsize=(8,8))\n    x, y, dy = gcv_scores.index, gcv_scores['mean_test_score'], gcv_scores['std_test_score']\n    ax.plot(x, y, \"ro-\", label=\"cross-validation\")\n    ax.fill_between(x, y-dy, y+dy, alpha=0.2, color='red')\n    ax.legend(loc=\"lower right\")\n    ax.set_ylabel(\"RMSE scores\")\n    ax.grid()\n    return reg_lasso.best_estimator_, gcv_matrix\n\nmodel_lasso, gcv_matrix = train_model_lasso_parameters_search(Xtrain, ytrain)\n","e09453f7":"examine_learning_curve(model_lasso, Xtrain, ytrain, modelname=\"Lasso\")","1c02efbe":"kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\nmodel_ridge = Ridge(alpha=10.0)\nmodel_lasso = Lasso(alpha=0.00052, max_iter=1.e5)\nmodel_rf = RandomForestRegressor(n_estimators=400, max_features='sqrt', max_depth=100, min_samples_split=2, min_samples_leaf=1) # Random Forest Decision Tree\nmodel_svr = SVR(C=20, epsilon=0.008, gamma=0.0003)\n#model_lgbm = LGBMRegressor(verbose=0, objective='regression', num_leaves=6, max_depth=8, learning_rate=0.1, n_estimators=300, min_data_in_leaf=32, min_child_weight=0.4, random_state=42)\nmodel_xgb = XGBRegressor(n_estimators=800, min_child_weight=3, max_depth=3, gamma=0, learning_rate=0.08, subsample=0.7, reg_lambda=0.1, reg_alpha=0.0, colsample_bytree=1.0) # Extreme Gradient Boosting Decision Tree\nmodel_stack = StackingRegressor(estimators=[('ridge', model_ridge), \\\n                                            ('lasso', model_lasso),\\\n                                            ('svr', model_svr),\\\n                                            #('lgbm', model_lgbm),\\\n                                            ('rf', model_rf),\\\n                                            ('xgb', model_xgb)],\\\n                                cv = kfolds)","7b5ec9e6":"def cv_rmse(model, X, y, modelname=\"\"):\n    rmse = np.sqrt(-cross_val_score(model, X, y,\n                                    scoring=\"neg_mean_squared_error\",\n                                    cv=8))\n    print (\"%-15s: %f %f\" % (modelname, rmse.mean(), rmse.std()))","2233dfbe":"cv_rmse(model_lasso, Xtrain, ytrain, \"Lasso\")\ncv_rmse(model_ridge, Xtrain, ytrain, \"Ridge\")\ncv_rmse(model_svr, Xtrain, ytrain, \"SVR\")\ncv_rmse(model_rf, Xtrain, ytrain, \"Random Forest\")\ncv_rmse(model_xgb, Xtrain, ytrain, \"XGBoost\")\n#cv_rmse(model_lgbm, Xtrain, ytrain, \"Light GBM\")","41759255":"# Slow, so use a different cell here\ncv_rmse(model_stack, Xtrain, ytrain, \"Stack\")","3dac3086":"model_xgb.fit(Xtrain, ytrain)\nmodel_lasso.fit(Xtrain, ytrain)\nmodel_ridge.fit(Xtrain, ytrain)\nmodel_rf.fit(Xtrain, ytrain)\nmodel_svr.fit(Xtrain, ytrain)\n#model_lgbm.fit(Xtrain, ytrain)\nmodel_stack.fit(Xtrain, ytrain)","91c93525":"examine_learning_curve(model_ridge, Xtrain, ytrain, modelname=\"Ridge\")\nexamine_learning_curve(model_svr, Xtrain, ytrain, modelname=\"SVR\")\nexamine_learning_curve(model_xgb, Xtrain, ytrain, modelname=\"XGBoost\")","324c2420":"def predict():\n    test = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n    testId = test['Id']\n    Xtest = test.copy()\n    Xtest, _ = preprocessing(Xtest, rank_mapping)\n    \n    Xtest = ct.transform(Xtest)\n    pred_ridge = np.exp(model_ridge.predict(Xtest)) - 1.0\n    pred_lasso = np.exp(model_lasso.predict(Xtest)) - 1.0\n    pred_xgb = np.exp(model_xgb.predict(Xtest)) - 1.0\n    pred_stack = np.exp(model_stack.predict(Xtest)) - 1.0\n    pred_svr = np.exp(model_svr.predict(Xtest)) - 1.0\n    pred_rf = np.exp(model_rf.predict(Xtest)) - 1.0\n    #pred_lgbm = np.exp(model_lgbm.predict(Xtest)) - 1.0                \n\n    pred = 0.55 * pred_stack + \\\n        0.1 * pred_ridge + \\\n        0.1 * pred_lasso + \\\n        0.1 * pred_xgb + \\\n        0.1 * pred_svr + \\\n        0.05 * pred_rf\n\n    out = pd.DataFrame({'Id':testId, 'SalePrice':pred})\n    \n    out.to_csv(\".\/submission.csv\", index=False)\n    return out\n","07d38b11":"out = predict()\nprint(out)\n# outcomp = pd.read_csv(\".\/output\/submission_best_12.388.csv\")\n# fig = plt.figure(figsize=(6,6))\n# plt.plot(out['SalePrice'], outcomp['SalePrice'], \"b.\")\n# ax = plt.gca()\n# ax.set_xlim(0., 1.e6)\n# ax.set_ylim(0., 1.e6)\n# ax.plot([0., 1.e6], [0.0, 1.e6], \"k--\")","5d1e7606":"import matplotlib as mpl\nprint(out[out['SalePrice']> 600000])\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n#print(test.columns)\nbad = test[test['Id'] == 2550]\ntrain.corrwith(train['SalePrice'], method='spearman').sort_values(ascending=False)\nfig, axs = plt.subplots(3, 3, figsize=(12, 12))\nfeatures = ['OverallQual', 'GrLivArea', 'GarageArea', \\\n            'YearBuilt', 'FullBath', 'TotalBsmtSF', \\\n            'YearRemodAdd', 'LotFrontage', 'TotRmsAbvGrd']\naxs = axs.flatten()\ny = train['SalePrice']\nfor i, var in enumerate(features):\n    axs[i].plot(train[var], y, \"b.\")\n    axs[i].plot(bad[var], 200000., \"rx\", markersize=12)\n    axs[i].plot(test[var], out['SalePrice'], \"k+\")\n    axs[i].set_xlabel(var)\n    ax = axs[i]\n    ticks_loc = ax.get_yticks().tolist()\n    ax.yaxis.set_major_locator(mpl.ticker.FixedLocator(ticks_loc))\n    ylabels = ['{:,.0f}'.format(y\/1000) + 'k' for y in ticks_loc]\n    ax.set_yticklabels(ylabels)\n    if(i % 3 == 0): ax.set_ylabel(\"SalePrice\")\n    \nplt.subplots_adjust(hspace=0.25)","93a25709":"## Prediction","a46152ec":"Utility function: display a formatted list of parameters from the estimator of a pipeline.","0254925c":"## House Price II: Modeling\n\n### Charlie Huang\n\n***\n\nThis is the second module of my study on the popular Kaggle project: *House Prices - Advanced Regression Techniques*. The first module (https:\/\/www.kaggle.com\/shuiyaohuang\/house-price-i-comprehensive-eda) focused on explorative data analysis using numpy\/pandas and data visualization using matplotlib\/seaborn. This module focuses on training machine learning models and optimizing model predictions on the separate test set.\n\nAfter many hours of experimentation I pushed the leaderboard rank from 50% to top 20% with more careful data engineering and better models. I also found that **one single outlier in the test data is responsible for a 0.007 difference in the leaderboard score** (see the end of the notebook for more details). Taking care of this outlier alone would move my rank up to top 4%. However, I don't take as a genuine improvement since my models are not capable of figuring this out by themselves.\n\nThere are two metrics to evaluate a model:\n1. Cross-validation score, e.g., kFolds, from the training data only\n2. The leaderboard score.\nAs mentioned earlier, the leaderboard score may fluctuate drastically due to a single outlier and this is hard to control. Therefore, both metrics would be considered in evaluating the goodness of a model.\n\nHere is a summary of data engineering procedures that may or may not affect the final score:\n- **Outliers**\n  \n  I found that having slightly different criteria for outliers leads to **large fluctuations** (up to 0.01 in my experiments) in the final score. Removing more outliers often results in much better cross-validation scores using only the training data, but seems to only harm the submission score. I threw out 2 outliers but later realized that the model failed to predict for one single outlier (Id = 2550) in the test set.\n\n\n- Transform **skewed features**  \n  \n  A **clear improvement** on both the cross-validation and the test set. Using boxcox1p is slightly better than log1p.\n\n\n- **Feature engineering**\n  - Add new features, e.g, totalSF, House age, log\/poly features, etc.  \n\n- **Missing values**\n  - numerical: mean, median, 0\n  - categorical: 0, most_frequent, 'none'\n\n- **Feature encoding and scaling**\n  - Numerical features: StandardScaler, RobustScaler\n  - Ordinal features: OneHot, ordinal encoder\n  - Target encoding for some nominal features, e.g., 'Neighborhood'\n\n  I didn't find any of these choices leads to a significant improvement in the final score. The strongest predictors such as OverallQual, GrLivArea, ... are numerical and have no\/little missing values. \n\n\n- **Hyperparameter tuning**\n\n  **Important for linear regressors**. Did not help that much for XGBoost within a reason range.\n\n\n- **Stacking** multiple regressors\n\n  **Quite helpful**. A stacked regressor with both linear and tree-based models improves model predictions on both the cross-validation sets and also the test set. The model fitting is much slower. Using the default RidgeCV regressor as the primary model seems to work pretty well, though not significantly better than other choices like XGBoost. \n\n\n- **Blend** the results and final corrections on the predicted value \n\n  Blending results from different regressors leads to some improvements in the leaderboard score, but this is likely an artifact from the outlier: my experiments seem to show that a tree-based model, e.g., random forest regressor, predicts a lower value for the outlier, which is actually closer to the true value. So even though the RF performs less well on the cross-validation sets, blending it in will help with the leaderboard score. ","c91cd56e":"## Learning Curves\n\nThe tree based models overfit the data while the linear models perform well.","a865a4f3":"## Evaluate Model\n\nNow evaluate the cross validation scores of these models. The linear models perform pretty well. \n\n**Stacking multiple regressors results in a significant improvement in the cross-validation score.**\n\n\nNote: This can be **very slow**, especially for the stacked regressor","4a102f50":"## A Bad Outlier\n\nThere's this one outlier in the **test** data: Id = 2550\n\nIt gets an OverallQual of 10 and a huge size so most of the time my model predicts a very high price.\nHowever, when I made multiple submissions with different values for this outlier (while keeping the rest the same) the leaderboard score changed drastically:\n\n| SalePrice | Leaderboard Score |\n|-----------|-------------------|\n|948458     |0.12388            |\n|800000     |0.12241            |\n|600000     |0.12025            |\n|300000     |0.11690            |\n|100000     |0.11729            |\n\nIt appears that this house should have a pretty low price ~200000, in spite of its quality and size measures, if my understanding on how leaderboard scores are calculated is correct.\n\nHowever, it should be noted that I removed two outliers from the training set that are quite similar to this outlier in the test set. If I included those two outliers, the score will likely be less affected by this outlier although it might lower the model accuracy in general.\n\nSome findings related to this:\n\n- I found that tree-based models tend to predict lower values than linear models for this outlier. This makes it hard to really say which models are better for this dataset solely from the leaderboard score. Cross-validation scores from the training set might be a better choice for evaluating models.\n- Some models use leaky information from the test set in data engineering. This likely will improve the leaderboard score because of the outliers.\n- Many top submissions used arbitrary coefficients for blending models but it is unclear whether or not most improvements from blending came from a better prediction of this outlier.\n- I spent quite some time trying to figure out this anomaly. I think it might be a good habit to always check if the train and test sets come from a same distribution.\n\nHere are scatter plots showing the training data (blue), test data (black) and the outlier (red) with y fixed to 200000.","dfae8daa":"## Feature Engineering\n\nSame process should be applied to both the training and the test data, but separately.\n\nHere is a summary of the pre-processing steps:\n1. Perform log(1+x) transformation on the target, boxcox1p on other skewed features.\n2. Drop several features from both the training and test set\n  - 'Id': non-informative\n  - 'Utilities', 'Street': single-valued\n  - 'PoolQC', 'Alley', 'Fence', 'MiscFeature': Very sparse with more than 80% data missing.\n3. Several features, e.g., 'MSSubClass', is falsely interpreted as numerical. Treat it as categorical.\n4. Hard encode all ordinal features using input from a separate .json file.\n5. Encodes several nominal features, e.g., 'Neighborhood', by ranking the categories according the median target values.\n6. Fill missing values with 0 for some features (mostly area measures)\n7. Transform other numerical features with a median imputer and a robust scaler\n8. Transform other categorical features with OneHot Encoder ignoring missing values.\n\nDetails can be found in the first part of the notebook:\n\nhttps:\/\/www.kaggle.com\/shuiyaohuang\/house-price-i-comprehensive-eda","6571b93c":"## Tuning Hyperparameters\n### Example: Lasso\n\nI used a list of 6 independent models for fitting the data. I didn't extensively tune the parameters so the tuning process is omitted here. Nevertheless, here is an example for tuning the Lasso regressor: ","4a867677":"## Fit the Models"}}