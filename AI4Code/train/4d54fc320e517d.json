{"cell_type":{"f1ec52e5":"code","c3e3461f":"code","4447fd1a":"code","b4f1dacb":"code","1d988f55":"code","8cdacaa9":"code","c699119f":"code","ab2323d7":"code","1c96b088":"code","c89b6713":"code","0ed6daf7":"code","8f444fac":"code","d3f6f92f":"code","a1825af0":"code","78cd6833":"code","a6c4d87d":"code","e0aa5621":"code","1a23af3f":"code","4364123a":"code","45c71965":"code","26109cef":"code","d2d41e20":"code","245686c5":"code","711cb077":"code","fa300b86":"markdown","5f38cb73":"markdown","c08c33d5":"markdown","9f0fd311":"markdown","0fb5e51c":"markdown","ba807c2d":"markdown","942ac67d":"markdown","bd44df14":"markdown","f5cd33d7":"markdown"},"source":{"f1ec52e5":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom scipy import signal\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential, Model, load_model\nfrom tensorflow.keras.layers import Input, Conv2D, AveragePooling2D, MaxPooling2D, Dense, Dropout, Activation, Flatten\nfrom tensorflow.keras.layers import BatchNormalization\n\nimport glob\nimport os","c3e3461f":"import tensorflow as tf\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n","4447fd1a":"df_example = pd.read_csv(\"..\/input\/predict-volcanic-eruptions-ingv-oe\/train\/2068207140.csv\")\ndf_example.head()\n\ndata_columns = list(df_example.columns)\n\nprint('Index Dataframe Shape: {}'.format(df_example.shape))\nprint('Column Headers:\\n')\nprint(data_columns)\ndf_example","b4f1dacb":"train_df = pd.read_csv(\"..\/input\/predict-volcanic-eruptions-ingv-oe\/train.csv\")\ntrain_df.head()","1d988f55":"train_list = []\ntrain_labels = []\ntrain_dir = '..\/input\/predict-volcanic-eruptions-ingv-oe\/train'\n\nfor index, row in train_df.iterrows():\n    segment_id = str(row['segment_id'])\n    fname = os.path.join(train_dir,segment_id+'.csv')\n\n    segment_label = row['time_to_eruption']\n    \n    train_list.append(fname)\n    train_labels.append(segment_label)\n    \nprint(\"Length of training list: {}\".format(len(train_list)))\nprint(\"Length of training labels: {}\".format(len(train_labels)))","8cdacaa9":"def process_path(segment_path):\n    input_df = pd.read_csv(segment_path)\n    input_df = input_df.fillna(0.0)\n    \n    fname = os.path.split(segment_path)[1]\n    segment_id = os.path.splitext(fname)[0]\n    \n    spec_array = {}\n        \n    for col in input_df.columns:\n        f,t,Sxx = signal.spectrogram(input_df[col],100,window=('tukey',.25),nperseg=256,nfft=256,mode='psd',noverlap=3)\n        spec_array[col] = Sxx\n    \n    segment_data = np.stack((list(spec_array.values())),axis=2)\n    \n    return segment_data","c699119f":"X = np.array([process_path(file_name) for file_name in train_list[0:5]])\nX.shape","ab2323d7":"y = df_example['sensor_2'].to_numpy()\nf,t,Sxx = signal.spectrogram(y,100,window=('tukey',.25),nperseg=256,nfft=256,mode='psd',noverlap=3)","1c96b088":"print(\"Length of f: {}\".format(len(f)))\nprint(\"Length of t: {}\".format(len(t)))\nprint(\"Shape of Sxx: {}\".format(Sxx.shape))\nSxx.dtype","c89b6713":"plt.pcolormesh(t, f, Sxx,shading='gouraud',vmax=10)\n\nplt.ylabel('Frequency [Hz]')\n\nplt.xlabel('Time [sec]')\n\nplt.show()","0ed6daf7":"fig, axs = plt.subplots(nrows=5, ncols=2)\nfig.set_size_inches(20,10)\nfig.subplots_adjust(hspace=0.5)\n\nfor col,ax in zip(data_columns, axs.flatten()):\n    y = df_example[col]\n    f,t,Sxx = signal.spectrogram(y,100,window=('tukey',.25),nperseg=256,nfft=256,mode='psd',noverlap=3)\n    ax.pcolormesh(t,f,Sxx,shading='auto',vmax=100)\n    ax.set_title(col)","8f444fac":"class SpectoGenerator(tf.keras.utils.Sequence):\n    \n    def __init__(self, filenames, labels, batch_size,to_predict=False):\n        self.filenames = filenames\n        self.labels = labels\n        self.batch_size = batch_size\n        self.to_predict = to_predict\n        \n    def __len__(self):\n        return (np.ceil(len(self.filenames) \/ float(self.batch_size))).astype(np.int)\n    \n    def __getitem__(self, index):  \n        train_ID_tmp = self.filenames[index * self.batch_size : (index+1) * self.batch_size]\n        \n        X = np.array([self._process_path(file_name) for file_name in train_ID_tmp])\n        \n        if self.to_predict:\n            return X\n        else:      \n            y = np.array(self.labels[index * self.batch_size : (index+1) * self.batch_size])\n            return X,y\n        \n    def _process_path(self, segment_path):\n        input_df = pd.read_csv(segment_path)\n        input_df = input_df.fillna(0.0)\n\n        fname = os.path.split(segment_path)[1]\n        segment_id = os.path.splitext(fname)[0]\n\n        spec_array = {}\n\n        for col in input_df.columns:\n            f,t,Sxx = signal.spectrogram(input_df[col],100,window=('tukey',.25),nperseg=256,nfft=256,mode='psd',noverlap=3)\n            spec_array[col] = Sxx\n\n        segment_data = np.stack((list(spec_array.values())),axis=2)\n\n        return segment_data","d3f6f92f":"training_generator = SpectoGenerator(train_list,train_labels,32)","a1825af0":"model = Sequential()\n\nmodel.add(Conv2D(filters=16, kernel_size = (2,8), activation='relu',input_shape=(129, 237, 10)))\nmodel.add(BatchNormalization(axis=3))\nmodel.add(MaxPooling2D(pool_size=(1,4)))\n\nmodel.add(Conv2D(filters=32, kernel_size = (8,2), dilation_rate=2,activation='relu'))\nmodel.add(BatchNormalization(axis=3))\nmodel.add(MaxPooling2D(pool_size=(2,1)))\n\nmodel.add(Conv2D(filters=32, kernel_size = (2,8), dilation_rate=3, activation='relu'))\nmodel.add(BatchNormalization(axis=3))\nmodel.add(MaxPooling2D(pool_size=(1,2)))\n\nmodel.add(Conv2D(filters=32, kernel_size = (8,2), dilation_rate=4,activation='relu'))\nmodel.add(BatchNormalization(axis=3))\nmodel.add(MaxPooling2D(pool_size=(2,1)))\n\nmodel.add(Flatten())\nmodel.add(Dense(1,activation=\"relu\"))\n\nmodel.summary()","78cd6833":"def scheduler(epoch, lr):\n  if epoch < 15:\n    return lr\n  else:\n    return lr * tf.math.exp(-0.1*epoch)\n\nscheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\nearlystop = tf.keras.callbacks.EarlyStopping(monitor='mae',min_delta=50000,patience=3)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\nmodel.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=optimizer, metrics=['mae'])","a6c4d87d":"history = model.fit(training_generator,epochs=40,steps_per_epoch=int(4431\/\/32),verbose=1,\n                   callbacks=[scheduler,earlystop])","e0aa5621":"model.save('cnn_model_40epochs_XXX.h5')","1a23af3f":"mae = history.history['mae']\nloss = history.history['loss']\n\nepochs = range(len(mae))\n\n#plt.plot(epochs, loss, 'r', label='loss')\nplt.plot(epochs, mae, 'b', label='MAE')\nplt.title('Training and validation accuracy')\nplt.legend(loc=0)\nplt.figure()\n\n\nplt.show()","4364123a":"test_df = pd.read_csv('..\/sample_submission.csv')\ntest_df","45c71965":"test_list = []\ntest_labels = [...]\ntest_dir = '..\/test\/'\n\nfor index, row in test_df.iterrows():\n    segment_id = str(row['segment_id'])\n    fname = os.path.join(test_dir,segment_id+'.csv')\n\n    segment_label = row['time_to_eruption']\n    \n    test_list.append(fname)\n    \nprint(\"Length of training list: {}\".format(len(train_list)))","26109cef":"test_generator = SpectoGenerator(test_list,test_labels,64,to_predict=True)\npred = model.predict(test_generator)\npred.shape","d2d41e20":"df_submit = test_df.copy()\ndf_submit['time_to_eruption'] = abs(pred)\ndf_submit.head(10)","245686c5":"print(\"Minimum event time is: {}\".format(df_submit['time_to_eruption'].min()))\nprint(\"Maximum event time is: {}\".format(df_submit['time_to_eruption'].max()))","711cb077":"df_submit.to_csv('.\/submission.csv',index=False)","fa300b86":"# Create list of training file paths\n\nWe'll deconstruct the train_df to do this so the sequence and label match","5f38cb73":"We'll use the dataset API to read the hardisk files as necessary. Previously I ran out of memory to simply loop through. Once  the files are imported, we can perform a map function on the tensor to alter the sensor columns and 60001 readings to manageable spectograms.","c08c33d5":"# Predict Test Values","9f0fd311":"# Creation of Custom Generator\n\nThe generator is defined as a class and incorporates the above methods. Files are read and processes in batches thus making it memory friendly.","0fb5e51c":"## Create Test Data Generator\n\nSame generator can be used for creating a test class. I created a boolean `to_predict` which will return test set parameters. This can then be passed into the model for prediction.","ba807c2d":"# Create List of Test Path Files","942ac67d":"# Overview\n\nThis approach will use spectogram and create images of each sensor. Each file will be imported and mapped to create a 130x237 image per sensor. These images will be stacked in the third axis so each `time_to_eruption` will be represented by a 130x237x10 tensor. This data won't fit into memory so I created a custom generator to read batches of files at a time. \n\nThe objective of sharing this notebook is to highligh Tensorflows ability to create a custom generator that reads data from hard disk versus into memory.","bd44df14":"# File Read to Spectogram\n\nThis are the basic steps developed and incorporated into the custom generator.","f5cd33d7":"# Create Model"}}