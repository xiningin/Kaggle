{"cell_type":{"b8d2bf51":"code","c7fad37f":"code","66194ed9":"code","40c027df":"code","1066a7d6":"code","48506411":"code","e9df3a6e":"code","a7fe012e":"code","86dde612":"code","0dc1c98a":"code","3b5245cd":"code","d0d572e3":"code","5d729620":"code","3e506c12":"code","a363c359":"code","76eb3234":"code","5ee35927":"code","61bee5c3":"code","4a3bcde2":"code","7a7f21b6":"code","060119ac":"code","954bee7d":"code","ed5100b9":"code","f53ea1f7":"code","b0d4054b":"code","7890e62e":"code","48c7fe3d":"code","206b6525":"code","b215d162":"code","8a7a696f":"code","e05939e0":"code","112bfab4":"code","ea866940":"code","2f054a23":"markdown","05ec5ad0":"markdown","1d42cf6c":"markdown","474ce8a2":"markdown","42092cbb":"markdown"},"source":{"b8d2bf51":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf \nimport tensorflow.keras as keras\nimport matplotlib.pyplot as plt","c7fad37f":"dataset_path = \"..\/input\/nyt-comments\/CommentsApril2017.csv\"","66194ed9":"dataset = pd.read_csv(dataset_path)","40c027df":"dataset.shape","1066a7d6":"# to prevent run out of memory, I only select part of dataset\ndataset = dataset[:1000]","48506411":"dataset.head(10)","e9df3a6e":"sentences = dataset[\"commentBody\"].values","a7fe012e":"sentences[0]","86dde612":"sentences[1]","0dc1c98a":"# convert all words to lowercase\nfor idx, sentence in enumerate(sentences):\n    sentences[idx] = sentence.lower()","3b5245cd":"# fit all sentences on tokenizer\ntokenizer = keras.preprocessing.text.Tokenizer()\ntokenizer.fit_on_texts(sentences)","d0d572e3":"# word index of tokenizer\ntokenizer.word_index","5d729620":"# number of total words\ntotal_word = len(tokenizer.word_index)+1\nprint(\"Total number of word: \", total_word)","3e506c12":"# convert sentences to sequences\nsequences = tokenizer.texts_to_sequences(sentences)","a363c359":"# prepare training sequences\ntraining_sequences = []\n\nfor seq in sequences:\n    for i in range(2, len(seq)):\n        training_sequences.append(seq[:i])\n        \ntraining_sequences = np.array(training_sequences)","76eb3234":"print(\"Length of training_sequences: \", len(training_sequences))","5ee35927":"# take a look on training_sequences\nprint(\"The first sequence in training sequences: \", training_sequences[0])\nprint(\"The second sequence in training sequences: \", training_sequences[1])","61bee5c3":"# pad all sequences to make them same length\nlongest_len = max([len(l) for l in training_sequences])\ntraining_sequences = keras.preprocessing.sequence.pad_sequences(sequences=training_sequences,\n                                           maxlen=longest_len,\n                                           padding=\"pre\")","4a3bcde2":"# prepare x_train and y_train\nx_train = training_sequences[:, :-1]\ny_train = training_sequences[:, -1]","7a7f21b6":"y_train = keras.utils.to_categorical(y=y_train, num_classes=total_word)","060119ac":"print(\"Shape of training_sequences: \", training_sequences.shape)\nprint(\"Shape of x_train: \", x_train.shape)\nprint(\"Shape of y_train: \", y_train.shape)","954bee7d":"# model architechture\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Embedding(input_dim=total_word,\n                                 output_dim=64,\n                                 input_length=longest_len))\nmodel.add(keras.layers.Bidirectional(keras.layers.LSTM(256, return_sequences=True)))\nmodel.add(keras.layers.Bidirectional(keras.layers.LSTM(128)))\nmodel.add(keras.layers.Dense(units=64, activation=\"relu\"))\nmodel.add(keras.layers.Dense(units=total_word, activation=\"softmax\"))","ed5100b9":"model.summary()","f53ea1f7":"# load model weight: model was trained to get the accuracy of 0.95\ntry:\n    model.load_weights(\"..\/input\/model-weight-generate-text-with-rnn\/best_model_weight.h5\")\nexcept:\n    print(\"ERROR\")","b0d4054b":"# compile model\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=0.0001), metrics=[\"acc\"])","7890e62e":"# define custom callback for training\nclass CustomCallback(keras.callbacks.Callback):\n    \n    def on_epoch_end(self, epoch, logs):\n        if(logs[\"acc\"] >= 0.95):\n            self.model.stop_training = True\n\ncustome_callback = CustomCallback()\ncheckpoint = keras.callbacks.ModelCheckpoint(filepath=\"best_model.h5\",\n                                             monitor=\"acc\",\n                                             verbose=1,\n                                             save_best_only=True,\n                                             save_weights_only=True,\n                                             mode=\"auto\",\n                                             save_freq=\"epoch\")","48c7fe3d":"# train model\nhistory = model.fit(x=x_train,\n                    y=y_train,\n                    batch_size=32,\n                    epochs=500,\n                    callbacks=[custome_callback, checkpoint])","206b6525":"first_word = \"You\"\n\ngenerated_sentence = [first_word]\nnum_word_to_generate = 25","b215d162":"generated_sentence","8a7a696f":"tokenizer.texts_to_sequences(generated_sentence)","e05939e0":"# create a dict to map idx to word\nidx2word = {idx:word for word, idx in tokenizer.word_index.items()}\nidx2word","112bfab4":"for i in range(num_word_to_generate):\n    \n    x = tokenizer.texts_to_sequences(generated_sentence)\n    \n    if len(x[0]) > longest_len:\n        x[0] = x[0][-1 * longest_len:]\n    else:\n        x = keras.preprocessing.sequence.pad_sequences(sequences=x,\n                                                   maxlen=longest_len,\n                                                   padding=\"pre\")\n    x = np.array(x)\n    y = model.predict(x)[0]\n    idx = np.argmax(y)\n    \n    generated_word = idx2word[idx]\n    \n    generated_sentence[0] += \" \" + generated_word","ea866940":"generated_sentence","2f054a23":"## Prepare Dataset for Training","05ec5ad0":"## Load & Explore Dataset","1d42cf6c":"## Import Package","474ce8a2":"## Create Text with Trained Model","42092cbb":"## Define Model"}}