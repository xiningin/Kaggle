{"cell_type":{"f6b3478b":"code","f3e06b34":"code","f5c489d8":"code","7a5fd4db":"code","861f1865":"code","2b4a5f93":"code","05ca8799":"code","9aecfab9":"code","acfdb46b":"code","cc1b5772":"code","f9507b8e":"code","7e76fde9":"code","42aa84d5":"code","f2ed0210":"code","50daa77c":"code","4e9b97d8":"code","4a7cdb8c":"code","0815a529":"code","aff0a6fa":"code","c2fc140c":"code","b978bcd8":"code","b0473fba":"code","d5ffd1df":"code","a3c80748":"code","2829a3c2":"code","12580449":"code","aa18a129":"code","6f1c93e1":"code","8af0033d":"code","06f36c54":"code","8ef31312":"code","b9d1569b":"code","bae7c775":"code","04d43f66":"code","caf8856f":"code","ccd081dc":"code","971687ca":"code","96576393":"code","7c55d977":"code","98d4e8aa":"code","4dde90dc":"code","1e55affb":"code","32d241f9":"code","c3f41120":"markdown","a2041680":"markdown","40f5394f":"markdown","05fcbdef":"markdown","1e763cd9":"markdown","5bc82e24":"markdown","b6043fef":"markdown","8fcfc433":"markdown","aba3e45c":"markdown","c83e5948":"markdown","20102a25":"markdown","4396ba6f":"markdown","c9da6b02":"markdown","7ec4fbd9":"markdown","f5337100":"markdown"},"source":{"f6b3478b":"import numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nimport warnings\nwarnings.filterwarnings('ignore')","f3e06b34":"train = pd.read_csv('..\/input\/train-data\/train_data.csv')\ntrain.shape","f5c489d8":"train.head()","7a5fd4db":"w = train.columns[0:54]\nwords = []\n\n#dividir a palavra pelo caracter \"_\" e usar somente o ultimo termo apos a divisao\nfor i in w:\n    x = i.split(\"_\")[2]\n    words.append(x)\n\n#renomear as colunas do data set\nfor i in range(len(train.columns[0:54])):\n    train = train.rename(columns = {train.columns[i]: words[i]})\ntrain.head()","861f1865":"freq_words = train.groupby('ham')[train.columns[0:48]].sum()\n\nspam_freq_words = freq_words.T.sort_values(by = 0, ascending = False)[0]\nnot_spam_freq_words = freq_words.T.sort_values(by = 0, ascending = False)[1]\n\nplt.figure(figsize = (17, 8))\nplt.bar(spam_freq_words.index, spam_freq_words, color = 'lightblue')\nplt.bar(not_spam_freq_words.index, not_spam_freq_words, color = 'lightgreen', bottom = spam_freq_words)\n\n_ = plt.xticks(rotation = 60)\n_ = plt.title('Frequencia de palavras', fontsize = 20)\n_ = plt.legend(labels = [\"Spam\", \"Not Spam\"], fontsize = 20)","2b4a5f93":"freq_chars = train.groupby('ham')[train.columns[48:54]].sum()\n\nspam_freq_chars = freq_chars.T.sort_values(by = 0, ascending = False)[0]\nnot_spam_freq_chars = freq_chars.T.sort_values(by = 0, ascending = False)[1]\n\nplt.figure(figsize = (10, 8))\nplt.bar(spam_freq_chars.index, spam_freq_chars, color = 'lightblue')\nplt.bar(not_spam_freq_chars.index, not_spam_freq_chars, color = \"lightgreen\", bottom = spam_freq_chars)\n\n_ = plt.title(\"Frequencia de caracteres\", fontsize = 20)\n_ = plt.legend(labels = [\"Spam\", \"Not Spam\"], fontsize = 20)","05ca8799":"l = train.groupby('ham')[train.columns[54:57]].sum()\nl_spam = l.T.sort_values(by = 0, ascending = False)[0]\nl_not_spam = l.T.sort_values(by = 0, ascending = False)[1]\n\nplt.figure(figsize = (10,8))\nplt.bar(l_spam.index, l_spam, color = 'lightblue')\nplt.bar(l_not_spam.index, l_not_spam, color = 'lightgreen', bottom = l_spam)\n\n_ = plt.title('Spam email - Outros atributos', fontsize = 20)\n_ = plt.legend(labels = [\"Spam\", \"Not Spam\"], fontsize = 20)","9aecfab9":"x = pd.concat([train.iloc[:, 0:57], train['ham']], axis = 1).corr().iloc[57].sort_values()\nplt.figure(figsize = (17, 8))\nplt.bar(x.index[:-1], x[:-1], color = 'lightblue')\n_ = plt.xticks(rotation = 'vertical')\n_ = plt.title(\"Correla\u00e7\u00e3o entre Nao Spam e Atributos\")","acfdb46b":"#Seprando os dados em teste e treino\nY = train[[\"ham\"]]\nX = train.iloc[:,0:57]\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20)\n\n#Normaliza\u00e7\u00e3o dos dados\nscaler = preprocessing.StandardScaler().fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","cc1b5772":"#Escolhendo o n\u00famero de vizinhos para o algoritmo KNN\nscores_mean = {}\nfor i in range(1,15):\n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn.fit(X_train_scaled, Y_train)\n    scores_scaled = cross_val_score(knn, X_train_scaled, Y_train, cv=10)\n    scores_mean[i] = scores_scaled.mean()\n    \npd.DataFrame(pd.Series(scores_mean)).sort_values(by = 0, ascending = False).T","f9507b8e":"#Treinando o algoritmo KNN\nknn_1 = KNeighborsClassifier(n_neighbors = 11)\nknn_1.fit(X_train_scaled, Y_train)\n\n#Usando o algoritmo nos dados de teste\nY_predict_knn_1 = knn_1.predict(X_test_scaled)\nfbeta_score(Y_test, Y_predict_knn_1, beta=3)","7e76fde9":"#Identificando as vari\u00e1veis que possuem correlacao com o alvo > 0.3\nhigh_corr_var = []\nx = train.corr()[abs(train.corr()) > 0.3].iloc[57].isna()\nfor i in range(0,59):\n    if x.iloc[i] == False: high_corr_var.append(x.index[i])\nhigh_corr_var = high_corr_var[:-1]\nhigh_corr_var","42aa84d5":"#Usando uma nova variavel de dados apenas com as colunas detectadas com alta correla\u00e7\u00e7ao\nX_train_corr = X_train[high_corr_var]\nX_test_corr = X_test[high_corr_var]","f2ed0210":"#Novamente, escolhendo o n\u00famero de vizinhos para o algoritmo KNN\nscores_mean = {}\nfor i in range(1,100):\n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn.fit(X_train_corr, Y_train)\n    scores_scaled = cross_val_score(knn, X_train_corr, Y_train, cv=10)\n    scores_mean[i] = scores_scaled.mean()\n    \npd.DataFrame(pd.Series(scores_mean)).sort_values(by = 0, ascending = False).T","50daa77c":"#Treinando o algoritmo KNN\nk = pd.DataFrame(pd.Series(scores_mean)).sort_values(by = 0, ascending = False).index[0]\nknn_2 = KNeighborsClassifier(n_neighbors = k)\nknn_2.fit(X_train_corr, Y_train)\n\n#Usando o algoritmo nos dados de teste\nY_predict_knn_2 = knn_2.predict(X_test_corr)\nfbeta_score(Y_test, Y_predict_knn_2, beta=3)","4e9b97d8":"train = pd.read_csv('..\/input\/train-data\/train_data.csv')\nw = train.columns[0:54]\nwords = []\n\n#dividir a palavra pelo caracter \"_\" e usar somente o ultimo termo apos a divisao\nfor i in w:\n    x = i.split(\"_\")[2]\n    words.append(x)\n\n#renomear as colunas do data set\nfor i in range(len(train.columns[0:54])):\n    train = train.rename(columns = {train.columns[i]: words[i]})","4a7cdb8c":"for i in range(0,58):\n    #Se o valor observado for menor que a media, substitui-se por 0\n    train.iloc[:, i] [train.iloc[:, i] < train.iloc[:, i].mean()] = 0\n    #Se for maior, por 1\n    train.iloc[:, i] [train.iloc[:, i] >= train.iloc[:, i].mean()] = 1","0815a529":"#Seprando os dados em teste e treino\nY = train[[\"ham\"]]\nX = train.iloc[:,0:57]\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20)\n\n#Normaliza\u00e7\u00e3o dos dados\nscaler = preprocessing.StandardScaler().fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","aff0a6fa":"#Novamente, escolhendo o n\u00famero de vizinhos para o algoritmo KNN\nscores_mean = {}\nfor i in range(1,100):\n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn.fit(X_train_corr, Y_train)\n    scores_scaled = cross_val_score(knn, X_train_corr, Y_train, cv=10)\n    scores_mean[i] = scores_scaled.mean()\n    \npd.DataFrame(pd.Series(scores_mean)).sort_values(by = 0, ascending = False).T","c2fc140c":"#Treinando o algoritmo KNN\nk = pd.DataFrame(pd.Series(scores_mean)).sort_values(by = 0, ascending = False).index[0]\nknn_3 = KNeighborsClassifier(n_neighbors = k)\nknn_3.fit(X_train_corr, Y_train)\n\n#Usando o algoritmo nos dados de teste\nY_predict_knn_3 = knn_3.predict(X_test_corr)\nfbeta_score(Y_test, Y_predict_knn_3, beta=3)","b978bcd8":"train = pd.read_csv('..\/input\/train-data\/train_data.csv')\nw = train.columns[0:54]\nwords = []\n\n#dividir a palavra pelo caracter \"_\" e usar somente o ultimo termo apos a divisao\nfor i in w:\n    x = i.split(\"_\")[2]\n    words.append(x)\n\n#renomear as colunas do data set\nfor i in range(len(train.columns[0:54])):\n    train = train.rename(columns = {train.columns[i]: words[i]})","b0473fba":"for i in range(0,58):\n    #Se o valor observado for menor que a media, substitui-se por 0\n    train.iloc[:, i] [train.iloc[:, i] < train.iloc[:, i].mean()] = 0\n    #Se for maior, por 1\n    train.iloc[:, i] [train.iloc[:, i] >= train.iloc[:, i].mean()] = 1","d5ffd1df":"#Seprando os dados em teste e treino\nY = train[[\"ham\"]]\nX = train.iloc[:,0:57]\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20)\n\n#Normaliza\u00e7\u00e3o dos dados\nscaler = preprocessing.StandardScaler().fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","a3c80748":"#Identificando as vari\u00e1veis que possuem correlacao com o alvo > 0.4\nhigh_corr_var = []\nx = train.corr()[abs(train.corr()) > 0.4].iloc[57].isna()\nfor i in range(0,59):\n    if x.iloc[i] == False: high_corr_var.append(x.index[i])\nhigh_corr_var = high_corr_var[:-1]\nhigh_corr_var","2829a3c2":"#Usando uma nova variavel de dados apenas com as colunas detectadas com alta correla\u00e7\u00e7ao\nX_train_corr = X_train[high_corr_var]\nX_test_corr = X_test[high_corr_var]","12580449":"#Novamente, escolhendo o n\u00famero de vizinhos para o algoritmo KNN\nscores_mean = {}\nfor i in range(1,100):\n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn.fit(X_train_corr, Y_train)\n    scores_scaled = cross_val_score(knn, X_train_corr, Y_train, cv=10)\n    scores_mean[i] = scores_scaled.mean()\n\npd.DataFrame(pd.Series(scores_mean)).sort_values(by = 0, ascending = False).T","aa18a129":"#Treinando o algoritmo KNN\nk = pd.DataFrame(pd.Series(scores_mean)).sort_values(by = 0, ascending = False).index[0]\nknn_4 = KNeighborsClassifier(n_neighbors = k)\nknn_4.fit(X_train_corr, Y_train)\n\n#Usando o algoritmo nos dados de teste\nY_predict_knn_4 = knn.predict(X_test_corr)\nfbeta_score(Y_test, Y_predict_knn_4, beta=3)","6f1c93e1":"train = pd.read_csv('..\/input\/train-data\/train_data.csv')\nw = train.columns[0:54]\nwords = []\n\n#dividir a palavra pelo caracter \"_\" e usar somente o ultimo termo apos a divisao\nfor i in w:\n    x = i.split(\"_\")[2]\n    words.append(x)\n\n#renomear as colunas do data set\nfor i in range(len(train.columns[0:54])):\n    train = train.rename(columns = {train.columns[i]: words[i]})","8af0033d":"#Seprando os dados em teste e treino\nY = train[[\"ham\"]]\nX = train.iloc[:,0:57]\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20)\n\n#Normaliza\u00e7\u00e3o dos dados\nscaler = preprocessing.StandardScaler().fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","06f36c54":"#Treinando o modelo\nnb_1 = MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\nnb_1.fit(X_train, Y_train)\n\n#Testando o modelo\nY_predict_nb_1 = nb_1.predict(X_test)\nfbeta_score(Y_test, Y_predict_nb_1, beta=3)","8ef31312":"train = pd.read_csv('..\/input\/train-data\/train_data.csv')\nw = train.columns[0:54]\nwords = []\n\n#dividir a palavra pelo caracter \"_\" e usar somente o ultimo termo apos a divisao\nfor i in w:\n    x = i.split(\"_\")[2]\n    words.append(x)\n\n#renomear as colunas do data set\nfor i in range(len(train.columns[0:54])):\n    train = train.rename(columns = {train.columns[i]: words[i]})","b9d1569b":"#Identificando as vari\u00e1veis que possuem correlacao com o alvo > 0.3\nhigh_corr_var = []\nx = train.corr()[abs(train.corr()) > 0.3].iloc[57].isna()\nfor i in range(0,59):\n    if x.iloc[i] == False: high_corr_var.append(x.index[i])\nhigh_corr_var = high_corr_var[:-1]\nhigh_corr_var","bae7c775":"#Usando uma nova variavel de dados apenas com as colunas detectadas com alta correla\u00e7\u00e7ao\nX_train_corr = X_train[high_corr_var]\nX_test_corr = X_test[high_corr_var]","04d43f66":"#Treinando o modelo\nnb_2 = MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\nnb_2.fit(X_train_corr, Y_train)\n\n#Testando o modelo\nY_predict_nb_2 = nb_2.predict(X_test_corr)\nfbeta_score(Y_test, Y_predict_nb_2, beta=3)","caf8856f":"train = pd.read_csv('..\/input\/train-data\/train_data.csv')\nw = train.columns[0:54]\nwords = []\n\n#dividir a palavra pelo caracter \"_\" e usar somente o ultimo termo apos a divisao\nfor i in w:\n    x = i.split(\"_\")[2]\n    words.append(x)\n\n#renomear as colunas do data set\nfor i in range(len(train.columns[0:54])):\n    train = train.rename(columns = {train.columns[i]: words[i]})","ccd081dc":"for i in range(0,58):\n    #Se o valor observado for menor que a media, substitui-se por 0\n    train.iloc[:, i] [train.iloc[:, i] < train.iloc[:, i].mean()] = 0\n    #Se for maior, por 1\n    train.iloc[:, i] [train.iloc[:, i] >= train.iloc[:, i].mean()] = 1","971687ca":"#Treinando o modelo\nnb_3 = MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\nnb_3.fit(X_train, Y_train)\n\n#Testando o modelo\nY_predict_nb_3 = nb_3.predict(X_test)\nfbeta_score(Y_test, Y_predict_nb_3, beta=3)","96576393":"train = pd.read_csv('..\/input\/train-data\/train_data.csv')\nw = train.columns[0:54]\nwords = []\n\n#dividir a palavra pelo caracter \"_\" e usar somente o ultimo termo apos a divisao\nfor i in w:\n    x = i.split(\"_\")[2]\n    words.append(x)\n\n#renomear as colunas do data set\nfor i in range(len(train.columns[0:54])):\n    train = train.rename(columns = {train.columns[i]: words[i]})","7c55d977":"for i in range(0,58):\n    #Se o valor observado for menor que a media, substitui-se por 0\n    train.iloc[:, i] [train.iloc[:, i] < train.iloc[:, i].mean()] = 0\n    #Se for maior, por 1\n    train.iloc[:, i] [train.iloc[:, i] >= train.iloc[:, i].mean()] = 1","98d4e8aa":"#Seprando os dados em teste e treino\nY = train[[\"ham\"]]\nX = train.iloc[:,0:57]\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20)\n\n#Normaliza\u00e7\u00e3o dos dados\nscaler = preprocessing.StandardScaler().fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","4dde90dc":"#Identificando as vari\u00e1veis que possuem correlacao com o alvo > 0.4\nhigh_corr_var = []\nx = train.corr()[abs(train.corr()) > 0.4].iloc[57].isna()\nfor i in range(0,59):\n    if x.iloc[i] == False: high_corr_var.append(x.index[i])\nhigh_corr_var = high_corr_var[:-1]\nhigh_corr_var","1e55affb":"#Usando uma nova variavel de dados apenas com as colunas detectadas com alta correla\u00e7\u00e7ao\nX_train_corr = X_train[high_corr_var]\nX_test_corr = X_test[high_corr_var]","32d241f9":"#Treinando o modelo\nnb_4 = MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\nnb_4.fit(X_train_corr, Y_train)\n\n#Testando o modelo\nY_predict_nb_4 = nb_4.predict(X_test_corr)\nfbeta_score(Y_test, Y_predict_nb_4, beta=3)","c3f41120":"<font color=blue>**Estrat\u00e9gia 3: Alterar os dados para maior ou menor que a m\u00e9dia do atributo**<\/font>","a2041680":"<font color=blue>Renomendo as colunas para facilitar a EDA<\/font>","40f5394f":"<font color=blue>**Correla\u00e7\u00e3o entre emails Spam e os outros atrbutos**<\/font>","05fcbdef":"<ul>\n    <li>O <b>algoritmo KNN<\/b> parece ser mais <b>robusto<\/b> que o Naive Bayes em relacao ao processo de feature engineering, pois o resultado da predi\u00e7\u00e3o nao foi alterado substancialmente ap\u00f3s tentativas de alterar os atributos. <\/li>\n    <li>Notamos que a <b>correla\u00e7\u00e3o<\/b> entre os atributos e a vari\u00e1vel target aumentou ap\u00f3s usarmos a <b>transforma\u00e7\u00e3o para valores bin\u00e1rios<\/b> (maior ou menor que a m\u00e9dia). <\/li>\n    <li>Apenas usar a transforma\u00e7\u00e3o para valores bin\u00e1rios <b>N\u00c3O<\/b> foi suficiente no caso do Naive Bayes. <\/li> \n    <li>Usar <b>somente<\/b> as vari\u00e1veis mais correlacionadas melhorou o resultado no caso do Naive Bayes. <\/li>\n<\/ul>\n    \n\n","1e763cd9":"## <font color=Red>**K-Nearest Neighbours**<\/font>\n","5bc82e24":"<font color=blue>**Estrat\u00e9gia 2: Usar apenas os atributos com maior correla\u00e7\u00e3o com a vari\u00e1vel alvo**<\/font>","b6043fef":"<font color=blue>**Estrat\u00e9gia 4: Alterar os dados para maior ou menor que a m\u00e9dia do atributo *E*  usar as vari\u00e1veis mais correlacionadas com a variavel alvo**<\/font>","8fcfc433":"<font color=blue>**Estrat\u00e9gia 2: Usar apenas os atributos com maior correla\u00e7\u00e3o com a vari\u00e1vel alvo**<\/font>","aba3e45c":"## <font color=red>**Naive Bayes**<\/font>","c83e5948":"## <font color=red>**Algumas conclus\u00f5es**<\/font>","20102a25":"<font color=blue>**Estrat\u00e9gia 1: Usando todos os dados originais**<\/font>","4396ba6f":"<font color=blue>**Estrat\u00e9gia 1: Usar todos os dados fornecidos**<\/font>\n","c9da6b02":"<font color=blue>**Estrat\u00e9gia 4: Alterar os dados para maior ou menor que a m\u00e9dia do atributo *E*  usar as vari\u00e1veis mais correlacionadas com a variavel alvo**<\/font>","7ec4fbd9":"# <font color=green>**PMR3508 Aprendizado de M\u00e1quina e Reconhecimento de Padr\u00f5es**<\/font> \n## <font color=green>**Trabalho 2 - Detec\u00e7\u00e3o de Spam**<\/font> \n***","f5337100":"<font color=blue>**Estrat\u00e9gia 3: Alterar os dados para maior ou menor que a m\u00e9dia do atributo**<\/font>"}}