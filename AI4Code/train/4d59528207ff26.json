{"cell_type":{"b0267a54":"code","fe1650ce":"code","9e0cb933":"code","553ee625":"code","02bff040":"code","1d44bcde":"code","e494defd":"code","044b2816":"code","3d80ee0c":"code","37bb0379":"code","3dc78b43":"code","63b1eba6":"code","3ff1c575":"code","be5d8687":"code","da01a579":"code","593ce99f":"code","34849eac":"code","f0a11afd":"code","cf24dd69":"code","e24bc0a9":"code","f20c77b2":"code","fd744a2e":"code","809a9e17":"code","2913409a":"code","67a51c68":"code","bbdbb2f4":"code","0b6f6993":"code","e199204f":"code","c7e6dacb":"code","0a68d22e":"code","65d9c431":"code","fdfff590":"code","2b03ad15":"code","48181b6c":"code","9f42c5f3":"code","e5f85b8c":"code","8d56fa6d":"code","08d33c6e":"code","8f8ea8a1":"code","1266d8be":"code","5d9e243f":"code","308ca1b9":"code","48cef22b":"code","7a872e93":"code","cca1f18a":"code","8883acdf":"code","460a05a7":"code","81d0def9":"code","7327df52":"code","8613d150":"code","b001a76e":"code","e1ad8a57":"code","5256ef95":"code","b77250e5":"code","b508d0ba":"code","783ec4dc":"code","8d5e343e":"code","b078e215":"code","30771193":"code","07c6a488":"code","449ef8b8":"code","15b1cbcb":"code","9f931c28":"code","b7b2af7c":"code","45542f64":"markdown","8d9cc7ed":"markdown","9a0143b5":"markdown","83f1cce7":"markdown","f9aab2b8":"markdown","2b2f9da2":"markdown","9d8f8f08":"markdown","4df96443":"markdown","7b42e4fe":"markdown","b0d03e3b":"markdown","f30cdd20":"markdown","7cdfb8df":"markdown","bd0da810":"markdown","efc838cc":"markdown","465788b5":"markdown","9433a9ee":"markdown","c9c2dcad":"markdown","61962205":"markdown","ec321590":"markdown","5f4c5a30":"markdown","3ededaab":"markdown","9efcfddd":"markdown","9b9e87a0":"markdown","50121479":"markdown","9ec71f51":"markdown","acf2f810":"markdown","a7b41212":"markdown","5b16b610":"markdown","aa810e02":"markdown","de5cbf34":"markdown","b143926b":"markdown","09a10a2a":"markdown","02f2dc39":"markdown","1ae8b228":"markdown","67372220":"markdown","86824273":"markdown","2153239e":"markdown","a1730789":"markdown","0948f0f1":"markdown","95f153ec":"markdown","0fc222cd":"markdown","ce28733a":"markdown","339467b9":"markdown","8989e9b6":"markdown","0805073d":"markdown","cb5a0bf8":"markdown","df775eea":"markdown","47bf845b":"markdown","37559492":"markdown","5421c28f":"markdown","4371102c":"markdown","7f4064b3":"markdown","8eb6931e":"markdown","2dfc4bf7":"markdown","da68ed9f":"markdown","d258b199":"markdown","e6c1a59c":"markdown","b7792f6d":"markdown","e8a90a85":"markdown","b43c988e":"markdown","b7a21058":"markdown","d64e4f16":"markdown","d59742a8":"markdown","6f7dc9f9":"markdown","c6c8f032":"markdown","9954dee5":"markdown","ce5fceaf":"markdown"},"source":{"b0267a54":"import numpy as np \nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import LassoCV\nfrom sklearn import metrics \nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', None)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","fe1650ce":"home = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","9e0cb933":"home.head()","553ee625":"test.head()","02bff040":"# Save Id for submission\ntest_Id = test['Id']\n\nhome.drop('Id', axis=1, inplace=True)\ntest.drop('Id', axis=1, inplace=True)\n\n","1d44bcde":"print(\"Home Data Dimensions: {}\".format(home.shape))\nprint(\"Test Data Dimensions: {}\".format(test.shape))","e494defd":"print(\"Skewness: %f\" % home['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % home['SalePrice'].kurt())\n\nfigure = plt.figure(figsize=(18,10))\nplt.subplot(1,2,1)\nsns.distplot(home['SalePrice'] , fit=norm);\n(mu, sigma) = norm.fit(home['SalePrice'])\nplt.ylabel('Frequency')\nplt.title('SalePrice Distribution')\n\nplt.subplot(1,2,2)\nstats.probplot(home['SalePrice'], plot=plt)\nplt.show()","044b2816":"home['SalePrice'] = np.log1p(home['SalePrice'])","3d80ee0c":"# Print out Skewness and Kurtosis\nprint(\"Skewness: %f\" % home['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % home['SalePrice'].kurt())\n\nfigure = plt.figure(figsize=(18,10))\nplt.subplot(1,2,1)\nsns.distplot(home['SalePrice'] , fit=norm);\n(mu, sigma) = norm.fit(home['SalePrice'])\nplt.ylabel('Frequency')\nplt.title('SalePrice Distribution')\n\nplt.subplot(1,2,2)\nstats.probplot(home['SalePrice'], plot=plt)\nplt.show()","37bb0379":"numerical_features = home.select_dtypes(exclude='object').drop(['SalePrice'], axis=1)","3dc78b43":"fig = plt.figure(figsize=(16,20))\n\nfor i in range(len(numerical_features.columns)):\n    fig.add_subplot(9, 4, i+1)\n    sns.boxplot(y=numerical_features.iloc[:,i])\n\nplt.tight_layout()\nplt.show()","63b1eba6":"fig = plt.figure(figsize=(12,18))\nfor i in range(len(numerical_features.columns)):\n    fig.add_subplot(9, 4, i+1)\n    sns.scatterplot(numerical_features.iloc[:, i],home['SalePrice'])\nplt.tight_layout()\nplt.show()","3ff1c575":"# Credit to https:\/\/www.kaggle.com\/amiiiney\/price-prediction-regularization-stacking\n\nfig = plt.figure(figsize=(15,15))\nax1 = plt.subplot2grid((3,2),(0,0))\nplt.scatter(x=home['GrLivArea'], y=home['SalePrice'], color=('yellowgreen'), alpha=0.5)\nplt.axvline(x=4600, color='r', linestyle='-')\nplt.title('Ground living Area- Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(0,1))\nplt.scatter(x=home['TotalBsmtSF'], y=home['SalePrice'], color=('red'),alpha=0.5)\nplt.axvline(x=5900, color='r', linestyle='-')\nplt.title('Basement Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(1,0))\nplt.scatter(x=home['1stFlrSF'], y=home['SalePrice'], color=('deepskyblue'),alpha=0.5)\nplt.axvline(x=4000, color='r', linestyle='-')\nplt.title('First floor Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(1,1))\nplt.scatter(x=home['MasVnrArea'], y=home['SalePrice'], color=('gold'),alpha=0.9)\nplt.axvline(x=1500, color='r', linestyle='-')\nplt.title('Masonry veneer Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(2,0))\nplt.scatter(x=home['GarageArea'], y=home['SalePrice'], color=('orchid'),alpha=0.5)\nplt.axvline(x=1230, color='r', linestyle='-')\nplt.title('Garage Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(2,1))\nplt.scatter(x=home['TotRmsAbvGrd'], y=home['SalePrice'], color=('tan'),alpha=0.9)\nplt.axvline(x=13, color='r', linestyle='-')\nplt.title('TotRmsAbvGrd - Price scatter plot', fontsize=15, weight='bold' )\nplt.show()","be5d8687":"# Gathered these positions and outliers from previous notebooks\n\npos = [1298,523, 297]\nhome.drop(home.index[pos], inplace=True)","da01a579":"home.head()","593ce99f":"home.shape","34849eac":"# Save target value for later\ny = home.SalePrice.values\n\n# In order to make imputing easier, we combine train and test data\nhome.drop(['SalePrice'], axis=1, inplace=True)\ndataset = pd.concat((home, test)).reset_index(drop=True)","f0a11afd":"dataset.head()","cf24dd69":"dataset.shape","e24bc0a9":"na_percent = (dataset.isnull().sum()\/len(dataset))[(dataset.isnull().sum()\/len(dataset))>0].sort_values(ascending=False)\n\nmissing_data = pd.DataFrame({'Missing Percentage':na_percent*100})\nmissing_data","f20c77b2":"na = (dataset.isnull().sum() \/ len(dataset)) * 100\nna = na.drop(na[na == 0].index).sort_values(ascending=False)\n\nf, ax = plt.subplots(figsize=(15,12))\nsns.barplot(x=na.index, y=na)\nplt.xticks(rotation='90')\nplt.xlabel('Features', fontsize=15)\nplt.title('Percentage Missing', fontsize=15)","fd744a2e":"dataset[na.index].dtypes","809a9e17":"for col in ('FireplaceQu', 'Fence', 'Alley', 'MiscFeature', 'PoolQC', 'MSSubClass'):\n    dataset[col] = dataset[col].fillna('None')","2913409a":"for col in ('Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'Functional', 'MSZoning', 'SaleType', 'Utilities'):\n    dataset[col] = dataset[col].fillna(dataset[col].mode()[0])","67a51c68":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    dataset[col] = dataset[col].fillna(0)\n    \nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    dataset[col] = dataset[col].fillna('None')","bbdbb2f4":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    dataset[col] = dataset[col].fillna(0)\n\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    dataset[col] = dataset[col].fillna('None')","0b6f6993":"dataset.isnull().sum()[dataset.isnull().sum() > 0].sort_values(ascending=False)","e199204f":"dataset['MasVnrType'] = dataset['MasVnrType'].fillna('None')\ndataset['MasVnrArea'] = dataset['MasVnrArea'].fillna(0)","c7e6dacb":"# LotFrontage is correlated to the 'Neighborhood' feature because the LotFrontage for nearby houses will be really similar, so we fill in missing values by the median based off of Neighborhood\ndataset[\"LotFrontage\"] = dataset.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))","0a68d22e":"na_percent = (dataset.isnull().sum()\/len(dataset))[(dataset.isnull().sum()\/len(dataset))>0].sort_values(ascending=False)\n\nmissing_data = pd.DataFrame({'Missing Percentage':na_percent})\nmissing_data","65d9c431":"list(dataset.select_dtypes(exclude='object').columns)","fdfff590":"# Features 'MSSubClass', 'YrSold', 'OverallCond' and 'MoSold' are supposed to be categorical features so change their type\ndataset['MSSubClass'] = dataset['MSSubClass'].apply(str)\ndataset['YrSold'] = dataset['YrSold'].apply(str)\ndataset['MoSold'] = dataset['MoSold'].apply(str)\ndataset['OverallCond'] = dataset['OverallCond'].astype(str)\n\n# Features 'LotArea' and 'MasVnrArea' are supposed to be numerical features so change their type\ndataset['LotArea'] = dataset['LotArea'].astype(np.int64)\ndataset['MasVnrArea'] = dataset['MasVnrArea'].astype(np.int64)","2b03ad15":"figure, (ax1, ax2, ax3,ax4) = plt.subplots(nrows=1, ncols=4)\nfigure.set_size_inches(20,10)\n_ = sns.regplot(home['TotalBsmtSF'], y, ax=ax1)\n_ = sns.regplot(home['1stFlrSF'], y, ax=ax2)\n_ = sns.regplot(home['2ndFlrSF'], y, ax=ax3)\n_ = sns.regplot(home['TotalBsmtSF'] + home['2ndFlrSF']+home['1stFlrSF'], y, ax=ax4)","48181b6c":"dataset['TotalSF']=dataset['TotalBsmtSF']  + dataset['1stFlrSF'] + dataset['2ndFlrSF']","9f42c5f3":"figure, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2)\nfigure.set_size_inches(14,10)\n_ = sns.barplot(home['BsmtFullBath'], y, ax=ax1)\n_ = sns.barplot(home['FullBath'], y, ax=ax2)\n_ = sns.barplot(home['BsmtHalfBath'], y, ax=ax3)\n_ = sns.barplot(home['BsmtFullBath'] + home['FullBath'] + home['BsmtHalfBath'] + home['HalfBath'], y, ax=ax4)","e5f85b8c":"dataset['TotalBath']=dataset['BsmtFullBath'] + dataset['FullBath'] + (0.5*dataset['BsmtHalfBath']) + (0.5*dataset['HalfBath'])","8d56fa6d":"figure, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3)\nfigure.set_size_inches(18,8)\n_ = sns.regplot(home['YearBuilt'], y, ax=ax1)\n_ = sns.regplot(home['YearRemodAdd'],y, ax=ax2)\n_ = sns.regplot((home['YearBuilt']+home['YearRemodAdd'])\/2, y, ax=ax3)","08d33c6e":"dataset['YrBltAndRemod']=dataset['YearBuilt']+dataset['YearRemodAdd']","8f8ea8a1":"figure, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(nrows=2, ncols=3)\nfigure.set_size_inches(20,10)\n_ = sns.regplot(home['OpenPorchSF'], y, ax=ax1)\n_ = sns.regplot(home['3SsnPorch'], y, ax=ax2)\n_ = sns.regplot(home['EnclosedPorch'], y, ax=ax3)\n_ = sns.regplot(home['ScreenPorch'], y, ax=ax4)\n_ = sns.regplot(home['WoodDeckSF'], y, ax=ax5)\n_ = sns.regplot((home['OpenPorchSF']+home['3SsnPorch']+home['EnclosedPorch']+home['ScreenPorch']+home['WoodDeckSF']), y, ax=ax6)","1266d8be":"dataset['Porch_SF'] = (dataset['OpenPorchSF'] + dataset['3SsnPorch'] + dataset['EnclosedPorch'] + dataset['ScreenPorch'] + dataset['WoodDeckSF'])","5d9e243f":"dataset['Has2ndfloor'] = dataset['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ndataset['HasBsmt'] = dataset['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ndataset['HasFirePlace'] =dataset['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\ndataset['Has2ndFlr']=dataset['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ndataset['HasPool']=dataset['PoolArea'].apply(lambda x: 1 if x > 0 else 0)","308ca1b9":"from sklearn.preprocessing import LabelEncoder\ncategorical_col = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n\nfor col in categorical_col:\n    label = LabelEncoder() \n    label.fit(list(dataset[col].values)) \n    dataset[col] = label.transform(list(dataset[col].values))\n\nprint('Shape all_data: {}'.format(dataset.shape))","48cef22b":"num_features = dataset.dtypes[dataset.dtypes != 'object'].index\nskewed_features = dataset[num_features].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' :skewed_features})\nskewness.head(15)","7a872e93":"numerical_dataset = dataset.select_dtypes(exclude='object')\n\nfor i in range(len(numerical_dataset.columns)):\n    f, ax = plt.subplots(figsize=(7, 4))\n    fig = sns.distplot(numerical_dataset.iloc[:,i].dropna(), rug=True, hist=False, label='UW', kde_kws={'bw':0.1})\n    plt.xlabel(numerical_dataset.columns[i])","cca1f18a":"skewness = skewed_features[abs(skewed_features) > 0.75]\nskewed_features = skewness.index\n\nlam = 0.15\nfor i in skewed_features:\n    dataset[i] = boxcox1p(dataset[i], lam)","8883acdf":"from datetime import datetime\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error , make_scorer\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.kernel_ridge import KernelRidge\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.linear_model import LinearRegression\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor","460a05a7":"# Hot-Encode Categorical features\ndataset = pd.get_dummies(dataset) \n\n# Splitting dataset back into X and test data\nX = dataset[:len(home)]\ntest = dataset[len(home):]\n\nX.shape","81d0def9":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=0)","7327df52":"# Indicate number of folds for cross validation\nkfolds = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# Parameters for models\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [0.00005, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]","8613d150":"# Lasso Model\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas = alphas2, random_state = 42, cv=kfolds))\n\n# Printing Lasso Score with Cross-Validation\nlasso_score = cross_val_score(lasso, X, y, cv=kfolds, scoring='neg_mean_squared_error')\nlasso_rmse = np.sqrt(-lasso_score.mean())\nprint(\"LASSO RMSE: \", lasso_rmse)\nprint(\"LASSO STD: \", lasso_score.std())","b001a76e":"# Training Model for later\nlasso.fit(X_train, y_train)","e1ad8a57":"ridge = make_pipeline(RobustScaler(), RidgeCV(alphas = alphas_alt, cv=kfolds))\nridge_score = cross_val_score(ridge, X, y, cv=kfolds, scoring='neg_mean_squared_error')\nridge_rmse =  np.sqrt(-ridge_score.mean())\n# Printing out Ridge Score and STD\nprint(\"RIDGE RMSE: \", ridge_rmse)\nprint(\"RIDGE STD: \", ridge_score.std())","5256ef95":"# Training Model for later\nridge.fit(X_train, y_train)","b77250e5":"elasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))\nelastic_score = cross_val_score(elasticnet, X, y, cv=kfolds, scoring='neg_mean_squared_error')\nelastic_rmse =  np.sqrt(-elastic_score.mean())\n\n# Printing out ElasticNet Score and STD\nprint(\"ELASTICNET RMSE: \", elastic_rmse)\nprint(\"ELASTICNET STD: \", elastic_score.std())","b508d0ba":"# Training Model for later\nelasticnet.fit(X_train, y_train)","783ec4dc":"lightgbm = make_pipeline(RobustScaler(),\n                        LGBMRegressor(objective='regression',num_leaves=5,\n                                      learning_rate=0.05, n_estimators=720,\n                                      max_bin = 55, bagging_fraction = 0.8,\n                                      bagging_freq = 5, feature_fraction = 0.2319,\n                                      feature_fraction_seed=9, bagging_seed=9,\n                                      min_data_in_leaf =6, \n                                      min_sum_hessian_in_leaf = 11))\n\n# Printing out LightGBM Score and STD\nlightgbm_score = cross_val_score(lightgbm, X, y, cv=kfolds, scoring='neg_mean_squared_error')\nlightgbm_rmse = np.sqrt(-lightgbm_score.mean())\nprint(\"LIGHTGBM RMSE: \", lightgbm_rmse)\nprint(\"LIGHTGBM STD: \", lightgbm_score.std())","8d5e343e":"# Training Model for later\nlightgbm.fit(X_train, y_train)","b078e215":"xgboost = make_pipeline(RobustScaler(),\n                        XGBRegressor(learning_rate =0.01, n_estimators=3460, \n                                     max_depth=3,min_child_weight=0 ,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,nthread=4,\n                                     scale_pos_weight=1,seed=27, \n                                     reg_alpha=0.00006))\n\n# Printing out XGBOOST Score and STD\nxgboost_score = cross_val_score(xgboost, X, y, cv=kfolds, scoring='neg_mean_squared_error')\nxgboost_rmse = np.sqrt(-xgboost_score.mean())\nprint(\"XGBOOST RMSE: \", xgboost_rmse)\nprint(\"XGBOOST STD: \", xgboost_score.std())","30771193":"# Training Model for later\nxgboost.fit(X_train, y_train)","07c6a488":"results = pd.DataFrame({\n    'Model':['Lasso',\n            'Ridge',\n            'ElasticNet',\n            'LightGBM',\n            'XGBOOST',\n            ],\n    'Score':[lasso_rmse,\n             ridge_rmse,\n             elastic_rmse,\n             lightgbm_rmse,\n             xgboost_rmse,\n             \n            ]})\n\nsorted_result = results.sort_values(by='Score', ascending=True).reset_index(drop=True)\nsorted_result","449ef8b8":"f, ax = plt.subplots(figsize=(14,8))\nplt.xticks(rotation='90')\nsns.barplot(x=sorted_result['Model'], y=sorted_result['Score'])\nplt.xlabel('Model', fontsize=15)\nplt.ylabel('Performance', fontsize=15)\nplt.ylim(0.10, 0.12)\nplt.title('RMSE', fontsize=15)","15b1cbcb":"# Predict every model\nlasso_pred = lasso.predict(test)\nridge_pred = ridge.predict(test)\nelasticnet_pred = elasticnet.predict(test)\nlightgbm_pred = lightgbm.predict(test)\nxgboost_pred = xgboost.predict(test)\n","9f931c28":"# Combine predictions into final predictions\nfinal_predictions = np.expm1((0.3*elasticnet_pred) + (0.3*lasso_pred) + (0.2*ridge_pred) + \n               (0.1*xgboost_pred) + (0.1*lightgbm_pred))","b7b2af7c":"submission = pd.DataFrame()\nsubmission['Id'] = test_Id\nsubmission['SalePrice'] = final_predictions\nsubmission.to_csv('house_pricing_submission.csv',index=False)","45542f64":"### 8.3 Models\nSetting up Models, Printing out the RMSE and STD (Standard Deviation) of each model, Training each model.\n\n### Models Used:\n- LassoCV\n- Ridge\n- ElasticNet\n- LightGBM\n- XGBOOST\n- CATBOOST\n- STACKED (Combined)","8d9cc7ed":"### 5.3 Imputing Certain features with their Mode\nImputing some features with their mode","9a0143b5":"### 8.4 Viewing Model Performance\nView Model Performance through a DataFrame and a barplot","83f1cce7":"### 6.3 Create 'TotalBath' Feature\nVisualize 'BsmtFullBath', 'FullBath', 'BsmtHalfBath' and them together, Create 'TotalBath' for total number of bathrooms using those features","f9aab2b8":"### 4.2 Bivariate Analysis\nBivariate Analysis is a procedure but with two variables (hence the name 'Bi'), not one. This is our second step to detecting outliers.","2b2f9da2":"### Lasso","9d8f8f08":"### Kurtosis Formula:\n![](https:\/\/rula-tech.com\/uploads\/images\/Kurtosis\/kurtosis_formula.png)","4df96443":"### 5.5 Imputing Basement-related features\nImputing Basement-Related Features which are numerical with 0 and 'None' for categorical","7b42e4fe":"<font size=\"+3\" color=\"green\"><b>4 - Outliers<\/b><\/font><br><a id=\"4\"><\/a>\n\n### Section Contents:\n- 4.1 Univariate Analysis\n- 4.2 Bivariate Analysis\n- 4.3 Removing Outliers\n\n### Additional Information about Outliers: \n### - https:\/\/www.kaggle.com\/getting-started\/147428\n\n### - https:\/\/www.kaggle.com\/getting-started\/150064","b0d03e3b":"### 8.1 Importing Libraries for Modelling\nFor modelling we must import these libraries","f30cdd20":"- A Garage-Related Feature with a missing value usually indicates there is no Garage","7cdfb8df":"### 4.3 Removing Outliers\nAfter looking at the Visualizations of each numerical feature, we use the 1.5 IQR Rule in order to detect and remove outliers ","bd0da810":"### Skew Formula:\n![](https:\/\/i0.wp.com\/datalabbd.com\/wp-content\/uploads\/2019\/05\/3b.png?w=520&ssl=1)","efc838cc":"### 7.1 Checking Skew\nCreate a new variable containing the dataset of only numerical features","465788b5":"### 5.7 Imputing LotFrontage\nImputing LotFrontage with median based on 'Neighborhood'","9433a9ee":"## To show your support PLEASE UPVOTE \ud83d\udc4d, Thank You! Also if you have any Suggestions\/Questions please comment them down below!","c9c2dcad":"### ElasticNet","61962205":"<font size=\"+3\" color=\"green\"><b>2 - Reading and Inspecting Data<\/b><\/font><br><a id=\"2\"><\/a>\n### Section Contents: \n\n- 2.1.  Reading Data\n\n- 2.2.  Inspecting Data\n","ec321590":"### 5.6 Imputing MasVnr-Related Features\nImputing 'MasVnrType' and 'MasVnrArea'","5f4c5a30":"- Creating a Dataframe listing every feature with missing values","3ededaab":"### 5.2 Imputing Certain features with 'None'\nImputing some features with 'None'","9efcfddd":"### 6.4 Create 'YrBuiltAndRemod' Feature\nVisualize 'YearBuilt', 'YearRemodAdd', and them together, Create 'YrBltAndRemod' for Year Built and Year remodel combined using those features","9b9e87a0":"### 5.8 Checking for any Extra Missing Values\nChecking for any more missing values (Ignore SalePrice because that's our target variable)","50121479":"### 2.2 Inspecting Data\nChecking our Data, Saving then dropping Id column, Checking Dataset's Dimensions","9ec71f51":"### Standard Deviation:\n![](https:\/\/cdn.kastatic.org\/googleusercontent\/N8xzWFc6eo0XBHEXZjz1SwvLSnPezvhTRF1P17kdjUG_tnJivGKkyCxbwVe4MZ0-USOxIZBohgcjdi8e7Z4Hswcqfw)","acf2f810":"### 6.5 Create 'PorchSF' Feature\nVisualize 'OpenPorchSF', '3SsnPorch', 'EnclosedPorch', 'ScreenPorch', 'WoodDeckSF' and them together, Create 'Porch_SF' for total porch surface using those features","a7b41212":"<a id=\"1\"><\/a> <br>\n<font size=\"+3\" color=\"green\"><b>1 - Importing Libraries<\/b><\/font><br><a id=\"1\"><\/a>\n<br>\nImporting Necessary Libraries ","5b16b610":"<font size=\"+3\" color=\"green\"><b>8 - Modelling<\/b><\/font><br><a id=\"8\"><\/a>\n### Section Contents:\n- 8.1 Importing Libraries for Modelling\n- 8.2 Preparing Data for Modelling\n- 8.3 Models\n- 8.4 Viewing Model Performance\n- 8.5 Stacking","aa810e02":"- The reason we fill in these features with their mode (most common value) is because these features are mandatory in a house, meaning that if these values are missing it has to be because of the data, not because of the fact that the house is missing the feature.\n\nEx: For the 'Electrical' feature we fill in the most common value because every house has electricity, therefore it wouldn't make sense for there to be any missing values.","de5cbf34":"\n\n# Introduction\nHi,\nI created a notebook as a guide for this Advanced House Pricing Competition. This competition is relatively similar to its predecessor except for a different output and slightly different procedure. I have a notebook for the regular House Pricing Competition if you want to check it out. This notebook will continuously be updated as I still have a lot to learn from this competition and in general. \n\nThis kernel was very helpful to me:\nhttps:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\n\nIf you have any questions or comments please leave them in the comment section! Also if you found this helpful please leave an UPVOTE, thank you!\n\n<font size=\"+3\" color=\"green\"><b>Useful Pandas CheatSheet<\/b><\/font><br>\n### -> https:\/\/www.kaggle.com\/getting-started\/146910\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Table of Contents:<\/h1>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#1\" role=\"tab\" aria-controls=\"profile\">1. Importing Libraries<span class=\"badge badge-primary badge-pill\">1<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#2\" role=\"tab\" aria-controls=\"messages\">2. Reading and Inspecting Data<span class=\"badge badge-primary badge-pill\">2<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#3\" role=\"tab\" aria-controls=\"settings\">3. SalePrice: Skew and Kurtosis Analysis<span class=\"badge badge-primary badge-pill\">3<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#4\" role=\"tab\" aria-controls=\"settings\">4. Outliers<span class=\"badge badge-primary badge-pill\">4<\/span><\/a> \n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#5\" role=\"tab\" aria-controls=\"settings\">5. Missing Values<span class=\"badge badge-primary badge-pill\">5<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#6\" role=\"tab\" aria-controls=\"settings\">6. Feature Engineering<span class=\"badge badge-primary badge-pill\">6<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#7\" role=\"tab\" aria-controls=\"settings\">7. High Skew Features<span class=\"badge badge-primary badge-pill\">7<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#8\" role=\"tab\" aria-controls=\"settings\">8. Modelling<span class=\"badge badge-primary badge-pill\">8<\/span><\/a>  \n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#9\" role=\"tab\" aria-controls=\"settings\">9. Submission<span class=\"badge badge-primary badge-pill\">9<\/span><\/a>  \n<\/div>\n\n# Version Logs:\n* Version 1: RELEASED\n\n* Version 2:\n    * ['TotalSF'] feature now includes '1stFlrSF' in Feature Engineering section\n    * Dataframe with Percentage of Missing values is multiplied by 100\n    * Removed accidently created duplicate of 'HasBsmt'\n* Version 3 **[CURRENT]**:\n    * Dropped pre-used outliers from other notebooks\n    * Added more visualization for outliers","b143926b":"### 5.4 Imputing Garage-related features\nImputing Garage-Related Features which are numerical with 0 and 'None' for categorical","09a10a2a":"### 8.2 Preparing Data for Modelling\nPreparing data so we can use it for modelling, Splitting data using train_test_split","02f2dc39":"### 6.7 Label Encoding\nOur dataset cannot run with categorical columns so we must Label Encode these columns in order to make them numerical","1ae8b228":"![](https:\/\/miro.medium.com\/max\/1110\/1*mshlbZAnP4uZubMNSuWWOA.jpeg)\n\n### Reasons a right-skewed predictive variable is bad:\n- Mean greater than mode\n- Median greater than mode\n- Mean is greater than median\n\n### This eventually can affect the performance of our modelling process, so we will decide to log-transform it in the next step.","67372220":"**Stacking:** At this point we basically trained and predicted each model so we can combine its predictions into a 'final_predictions' variable for submission","86824273":"<font size=\"+3\" color=\"green\"><b>3 - SalePrice: Skew and Kurtosis Analysis<\/b><\/font><br><a id=\"3\"><\/a>\n### Section Contents:\n- 3.1 Original SalePrice Visualization\n- 3.2 Log-Transforming SalePrice\n- 3.3 Log-Transformed SalePrice Visualization\n\n### Additional Information about Statistics:\n### - https:\/\/www.kaggle.com\/getting-started\/149618","2153239e":"### 6.1 Changing Types\nChanging the types of some of the features","a1730789":"### 3.2 Log-Transforming SalePrice\nLog-Transforming SalePrice with np.log1p","0948f0f1":"### 3.1 Original SalePrice Visualization\nPrint out skew and Kurtosis of SalePrice, Visualize SalePrice with distplot and probplot","95f153ec":"### 8.5 Stacking\nPredict every model, then combine every prediction into a final predictions used for submission","0fc222cd":"### 2.1 Reading Data\nReading in train and test data","ce28733a":"Splitting data into train and validation data using variables X and y","339467b9":"![](https:\/\/i.stack.imgur.com\/giq8G.png)","8989e9b6":"<font size=\"+3\" color=\"green\"><b>7 - High Skew Features<\/b><\/font><br><a id=\"7\"><\/a>\n\n\n### Section Contents:\n- 7.1 Checking Skew\n- 7.2 Skew Visualization\n- 7.3 Box-Cox Transformation","0805073d":"### Random Example:\nThis shows the distribution of a dataset before and after log-transformation\n\n![](https:\/\/miro.medium.com\/max\/810\/1*4fagRPrzuHquTAKe35OhdA.png)","cb5a0bf8":"### 7.2 Skew Visualization\nVisualize each numerical feature with distplot","df775eea":"### 5.1 Checking for Missing Values\nCombining train and test data in order to make imputing missing values easier, locating missing values","47bf845b":"### 6.2 Create 'TotalSF' Feature\nVisualize 'TotalBsmtSF' and '2ndFlrSF' and them together, Create 'TotalSF' for total surface area with those features","37559492":"- Creating a Visualization of every feature with missing values","5421c28f":"### Using the 1.5 IQR Rule: \n![](https:\/\/cdn.kastatic.org\/googleusercontent\/8bSRVB7q_zWxFliXcZVQSBDtip3sMGRkkHGLVzvflS3goQZZhmhrSD9u1cSduXh-9DJ9sSjCqVyozwQ_FwJNkptC)\n\n### It states that a data point is an outlier if:\n- It is below the First Quadrant (Q1) subtracted by (1.5 x IQR)\n- It is above the Third Quadrant (Q3) added by (1.5 x IQR)","4371102c":"### 3.3 Log-Transformed SalePrice Visualization\nPrint out skew and Kurtosis after log-transformation, Visualize SalePrice after Log-transformation\n\n![](https:\/\/cdn-images-1.medium.com\/max\/800\/1*hxVvqttoCSkUT2_R1zA0Tg.gif)","7f4064b3":"### 7.3 Box-Cox Transformation\nPerform Box-Cox Transformation on Skewed Features","8eb6931e":"<font size=\"+3\" color=\"green\"><b>6 - Feature Engineering<\/b><\/font><br><a id=\"6\"><\/a>\n\n### Section Contents:\n- 6.1 Changing Types\n- 6.2 Create 'TotalSF' Feature\n- 6.3 Create 'TotalBath' Feature\n- 6.4 Create 'YrBuiltAndRemod' Feature\n- 6.5 Create 'PorchSF' Feature\n- 6.6 Creating Extra Features\n- 6.7 Label Encoding","2dfc4bf7":"<font size=\"+3.7\" color=\"green\"><b><center>House Pricing Predictions Guide<\/center><\/b><\/font><br>\n\n","da68ed9f":"- We can see that our dataset shrunk because some of the rows with outliers were removed","d258b199":"<font size=\"+3\" color=\"green\"><b>9 - Submission<\/b><\/font><br><a id=\"9\"><\/a>\n<br>\nSubmitting to Competition using test_Id which stores the test data's Id and the final predictions ","e6c1a59c":"- Checking each feature's type to know which value to impute later","b7792f6d":"### XGBOOST","e8a90a85":"<font size=\"+3\" color=\"green\"><b>5 - Missing Values<\/b><\/font><br><a id=\"5\"><\/a>\n\n### Section Contents:\n- 5.1 Checking for Missing Values\n- 5.2 Imputing Certain features with 'None'\n- 5.3 Imputing Certain features with their Mode\n- 5.4 Imputing Garage-related features\n- 5.5 Imputing Basement-related features\n- 5.6 Imputing MasVnr-Related Features\n- 5.7 Imputing LotFrontage\n- 5.8 Checking for any Extra Missing Values","b43c988e":"### 6.6 Creating Extra Features\nCreating useful extra features in order to strongly distinct data\n\nEx: 'Has2ndfloor' feature below indicates whether there is a 2ndfloor or not","b7a21058":"- A Basement-Related Feature with a missing value usually indicates there is no Basement","d64e4f16":"### Ridge","d59742a8":"- There are two features related to MasVnr that have missing values: 'MasVnrType' and 'MasVnrArea'. \n- 'MasVnrType' is a categorical feature that we need to impute with 'None'\n- 'MasVnrArea' is a categorical feature that we need to impute with 'MasVnrType'","6f7dc9f9":"- Any of the below features with a missing value likely indicates that it doesn't exist\n\nEx: FireplaceQu doesn't exist, therefore it probably means that there was no Fireplace for that house","c6c8f032":"### 4.1 Univariate Analysis\nIn the name 'Univariate Analysis' 'Uni' means one, and 'variate' means variable, meaning that Univariate analysis is analysis of one feature. This procedure basically tells us the distribution of each feature and information about its mean, median, and mode. This is our first step to detecting outliers.","9954dee5":"### Parameters\n'kfolds' is used for cross validation, the list type variables are for the models","ce5fceaf":"### LightGBM"}}