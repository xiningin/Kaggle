{"cell_type":{"7836f09c":"code","90021fcc":"code","9e47ea41":"code","c022dbad":"code","d59c49a8":"code","5268b166":"markdown","620d64ad":"markdown","ac62f214":"markdown"},"source":{"7836f09c":"!pip install feature-engine","90021fcc":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom feature_engine.selection import DropDuplicateFeatures\nfrom feature_engine.selection import DropCorrelatedFeatures\nfrom sklearn import preprocessing\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nimport time\nfrom imblearn.over_sampling import RandomOverSampler\n\nrng = np.random.RandomState(42)\ntrain_path = '..\/input\/santander-customer-satisfaction\/train.csv'\ntest_path = '..\/input\/santander-customer-satisfaction\/test.csv'","9e47ea41":"data_train = pd.read_csv(train_path, sep=',').drop(columns=['ID'])\ndata_train = data_train.drop_duplicates()\ndata_train = data_train.drop_duplicates(\n    subset=data_train.columns[:-1], keep=False)\ndata_train['var3'] = data_train['var3'].replace(-999999, 2)\n\nX_train, X_test, y_train, y_test = train_test_split(data_train.drop('TARGET', axis=1), data_train.TARGET, train_size=0.8,\n                                                    stratify=data_train.TARGET, random_state=rng)\n\ncorrelated = DropCorrelatedFeatures(\n    variables=None, method='pearson', threshold=0.9)\ncorrelated.fit(X_train)\nX_train = correlated.transform(X_train)\nX_test = correlated.transform(X_test)\nprint('delete columns with high corr: done!')\n\nvar = VarianceThreshold(threshold=.9 * (1 - .9))\nvar.fit(X_train)\nvar.transform(X_train)\nX_train = X_train[X_train.columns[var.get_support()]]\nvar.transform(X_test)\nX_test = X_test[X_test.columns[var.get_support()]]\nprint('delete columns with low var: done!')\n\nX_train_clean = X_train.copy()\nX_test_clean = X_test.copy()\n\n# standardlized\nss = StandardScaler()\nss.fit(X_train)\nX_train = ss.transform(X_train)\nX_test = ss.transform(X_test)","c022dbad":"# select 60 features\nselector_fc = SelectKBest(score_func=f_classif, k=60)\nselector_fc.fit(X_train, y_train)\nmask_selected = selector_fc.get_support()\n# Saving the selected columns in a list\nselected_col = X_train_clean.columns[mask_selected]\n\n\n# data for final training\nX_train_k = X_train_clean[selected_col]\ny_train_k = y_train\n\ndf_test = pd.read_csv(test_path)\nid_test = df_test['ID']\nx_test = df_test[selected_col]","d59c49a8":"d_train = xgb.DMatrix(X_train_k, label=y_train_k)\nwatchlist = [(d_train, 'train')]\nparams = {}\n\nparams['objective'] = 'binary:logistic'\nparams['alpha'] = 1e-2\nparams['booster'] = 'gbtree'\nparams['eval_metric'] = 'auc'\nparams['eta'] = 0.03\nparams['max_depth'] = 4\nparams['subsample'] = 0.65\nparams['colsample_bytree'] = 0.65\nparams['verbose'] = 2\nparams['maximise'] = False\nparams['min_child_weight'] = 3\nparams['scale_pos_weight'] = 24\n\nclf = xgb.train(params, d_train, 250, watchlist)\nd_test = xgb.DMatrix(x_test)\ny_pred = clf.predict(d_test)\nsubmission = pd.DataFrame({\"ID\": id_test, \"TARGET\": y_pred})\nsubmission.to_csv(\"submissionhy-pure-code.csv\", index=False)","5268b166":"XGB","620d64ad":"preprocessing","ac62f214":"SelectKBest"}}