{"cell_type":{"a8f98b92":"code","c893f692":"code","f21597d1":"code","38a55f1a":"code","49f62dfd":"code","5fa5b928":"code","725b9792":"code","b8e1e8e1":"code","a5dd90aa":"code","321b5351":"code","ff57e514":"code","ad7dd172":"code","b85c3f93":"code","ddee695b":"code","215f02ce":"code","e526adb7":"code","e7dbfcc6":"code","3169b78a":"code","8846f1a5":"code","54d47a1a":"code","e94f81fe":"code","5ac885f7":"code","f0a6f0ea":"code","3383a262":"code","b835c4a9":"code","032c572a":"code","6d5bd34b":"code","e419b3b3":"code","5405e0b6":"code","ddea5bc7":"code","0ae70d81":"code","30ca3386":"code","85c2b972":"code","75f38d53":"code","31ffdf9e":"code","2faafd89":"code","c936f24c":"code","df985d01":"code","075eee0d":"code","9b6813de":"code","d642eae8":"code","b35a01d9":"code","2576c2f5":"code","71b902b7":"code","b613997a":"code","b22a305d":"code","fa6c5a8f":"code","c1197986":"markdown","69ab368b":"markdown","03fde2e4":"markdown","a4007bb3":"markdown","3de39b5a":"markdown","3654196b":"markdown","ff7c3ba5":"markdown","57378a2a":"markdown","f1ac970a":"markdown","2d357deb":"markdown","17270598":"markdown","f6fd265a":"markdown","be20b720":"markdown","5723befc":"markdown","44234934":"markdown","63d652f3":"markdown","967e54f1":"markdown","6878c5d3":"markdown","407048b3":"markdown","c0eee87e":"markdown","76a54925":"markdown","536d254f":"markdown","16f948c3":"markdown","1467cc03":"markdown","5c19983b":"markdown","d89e69ef":"markdown","2741b785":"markdown","9d859454":"markdown","d8ffa433":"markdown","33aa14bb":"markdown","5fb55df8":"markdown","72b98de6":"markdown","61906987":"markdown","9892b8ba":"markdown","35da5a30":"markdown","f3859a8b":"markdown","650c9925":"markdown","01aec40e":"markdown","4e4e6f76":"markdown","ce81196d":"markdown","bdffb2e0":"markdown","b5dc649a":"markdown","3c502192":"markdown","d1ff36ef":"markdown","e51fffc3":"markdown","3e0e8628":"markdown","62013e6c":"markdown","d14f0bcc":"markdown"},"source":{"a8f98b92":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c893f692":"#################################\n########## LIBRARIES ############\n#################################\nimport glob\nimport json\nimport pandas as pd\nfrom tabulate import tabulate\nimport numpy as np\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas_profiling as pp\nimport scipy.stats as stats\nfrom scipy.stats import norm\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew, norm\nfrom sklearn.preprocessing import LabelEncoder as le\nfrom sklearn.preprocessing import StandardScaler as scale\nfrom sklearn import preprocessing\nimport warnings\n\n################################\n####### DISPLAY SETTINGS #######\n################################\npd.pandas.set_option('display.max_rows', None)\npd.pandas.set_option('display.max_columns', None)\nwarnings.simplefilter(action='ignore', category=FutureWarning)\npd.options.mode.chained_assignment = None  # default='warn'\nnp.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n\n\n################################\n####### DATA DIRECTORY #########\n################################\n# Create list of directories and file paths\ndir_ = '..\/input\/dogracing2days'\nsubdir_ = os.listdir(dir_)\nrace_dir = glob.glob(dir_ + \"*\/*\/*.json\")\nresults_dir = glob.glob(dir_ + \"*\/*\/results\/*.json\")\nscan_dir = glob.glob(dir_ + \"*\/*\/scan\/*.json\")\n\n\n################################\n#### CONSTRUCT LIST OF DF'S ####\n################################\n# Append a list of dataframes for the scan data\nscan_dfs = [] # list to store the DFs\nfor file in scan_dir:\n    data = pd.io.json.read_json(file) \n    scan_dfs.append(data) \n    \n# Append a list of dataframes for the results data\nresults_dfs = [] \nfor file in results_dir:\n    data = pd.io.json.read_json(file) \n    results_dfs.append(data) \n\n# Append a list of dataframes for the race data\nrace_dfs = [] \nfor file in race_dir:\n    data = pd.io.json.read_json(file) \n    race_dfs.append(data) ","f21597d1":"################################\n###### UNNESTING THE DATA ######\n################################\n# Unnest data from results\/*.json\ndf = pd.concat(results_dfs)\ntop = df.drop(['success', 'url'], axis=1)\ndoggo_json = pd.json_normalize(top['doggo_json'])\nprice_data = pd.json_normalize(doggo_json['price_data.prices'])\npricing = pd.json_normalize(price_data[1])\ndoggo_json = pd.concat([pricing, doggo_json], axis=1)\ndoggo_json = doggo_json[['name', 'winPrice', 'placePrice','race_result.result', 'race_result.place']]\n#doggo_json.reset_index(drop=True, inplace=True)\ntop.reset_index(drop=True, inplace=True)\nresults_df = pd.concat([top, doggo_json], axis=1)\nresults_df['win_odds'] = results_df['winPrice']\nresults_df['place_odds'] = results_df['placePrice']\nresults_df['placing'] = results_df['race_result.place'].fillna(0)\nresults_df['result'] = results_df['race_result.result'].fillna('L')\nresults_df['result'] = np.where(results_df['placing'] == 4.0, 'L', results_df['result'])\nresults_df = results_df[results_df.result != 'V']\nresults_df['winner'] = pd.to_numeric(results_df['result'].replace({'W':1.0, 'P':0.0, 'L':0.0}))\nresults_df['place'] = pd.to_numeric(results_df['result'].replace({'W':1.0, 'P':1.0, 'L':0.0}))\n#Drop what we don't need\nresults_df = results_df.drop(['doggo_json','race_result.place', 'race_result.result', 'placePrice', 'winPrice', 'placing', 'result'], axis=1)\nsearchfor = [\"Any of dogs\", \"Either dog number\", \"The Field to beat\", \"Odds\", \"Evens\", \"Inside\", \"Outside\"]\nresults_df = results_df[~results_df.name.str.contains('|'.join(searchfor))]\nresults_df.reset_index(drop=True, inplace=True)\n\n\n# Unnest data from scan\/*.json\ndf = pd.concat(scan_dfs)\ntop = df.drop(['success', 'url'], axis=1)\ndoggo_json = pd.json_normalize(top['doggo_json'])\nprice_data = pd.json_normalize(doggo_json['price_data.prices'])\nprice_data = pd.json_normalize(price_data[0])\nprice_data.reset_index(drop=True, inplace=True)\ntop.reset_index(drop=True, inplace=True)\ndoggo_json.reset_index(drop=True, inplace=True)\nscan_df = pd.concat([top, doggo_json], axis=1)\nscan_df = scan_df.drop(['doggo_json', 'price_data.prices'], axis=1)\n\n\n# Unnest data from \/race.json\n#df = race_dfs[0]\ndf = pd.concat(race_dfs)\nraces = df.explode('Races')\nraces = races.reset_index(col_level=0)\nracesRaces = pd.DataFrame(dict([(k,pd.Series(v)) for k,v in races['Races'].items()]))\nracesRaces = racesRaces.T\nraces_data = pd.concat([races, racesRaces], axis=1)\nraces_data = races_data.drop(['index','Races', 'Link'], axis=1)\n#races_data = races_data.drop(['index', 'Link'], axis=1)\nraces_data_dropped = races_data\nraces_data_dropped['distance'] = races_data_dropped['Description'].str[-4:]\nraces_data_dropped['distance'] = [x.strip()[:-1] for x in races_data_dropped['distance']]\nraces_data_dropped['distance'] = pd.to_numeric(races_data_dropped['distance'])\nraces_data_dropped['race_number'] = races_data_dropped.Description.apply(lambda x: x.split('R')[1].split(' ')[0].strip())\nraces_data_dropped = races_data_dropped.drop(['Description', 'Country'], axis=1)\nraces_data_dropped['race_id'] = pd.to_numeric(races_data_dropped['Id'])\nrace_df = races_data_dropped.drop(['Id'], axis=1)","38a55f1a":"race_df.head(3)","49f62dfd":"scan_df.head(3)","5fa5b928":"results_df.head(3)","725b9792":"################################\n######### RESULTS DF ###########\n################################\n# Recording our data types as lists to index later\nresults = results_df\nresults_categorical = [col for col in results.columns if results[col].dtypes=='O']\nresults_numerical = [col for col in results.columns if results[col].dtypes!='O']\nresults_numerical_null = [col for col in results.columns if results[col].isnull().any() and results[col].dtypes!='O']\nresults_categorical_null = [col for col in results.columns if results[col].isnull().any() and results[col].dtypes=='O']\n\n################################\n########### RACE DF ############\n################################\nrace = race_df\nrace_categorical = [col for col in race.columns if race[col].dtypes=='O']\nrace_numerical = [col for col in race.columns if race[col].dtypes!='O']\nrace_numerical_null = [col for col in race.columns if race[col].isnull().any() and race[col].dtypes!='O']\nrace_categorical_null = [col for col in race.columns if race[col].isnull().any() and race[col].dtypes=='O']\n\n################################\n########### SCAN DF ############\n################################\nscan = scan_df\nscan_categorical = [col for col in scan.columns if scan[col].dtypes=='O']\nscan_numerical = [col for col in scan.columns if scan[col].dtypes!='O']\nscan_numerical_null = [col for col in scan.columns if scan[col].isnull().any() and scan[col].dtypes!='O']\nscan_categorical_null = [col for col in scan.columns if scan[col].isnull().any() and scan[col].dtypes=='O']\n\n#  Tabulated summary of datatypes and shape\nsummary_data = [\n    ['ResultsDF',results.shape,'Categorial', len(results_categorical), len(results_categorical_null), results_categorical_null],\n    ['','','Numerical', len(results_numerical), len(results_numerical_null), results_numerical_null],\n    ['ScanDF',scan.shape,'Categorial', len(scan_categorical), len(scan_categorical_null), scan_categorical_null],\n    ['','','Numerical', len(scan_numerical), len(scan_numerical_null), scan_numerical_null],\n    ['RaceDF',race.shape,'Categorial', len(race_categorical), len(race_categorical_null), race_categorical_null],\n    ['','','Numerical', len(race_numerical), len(race_numerical_null), race_numerical_null]]\n\nsummary_table = pd.DataFrame(summary_data, columns = ['Dataframe','Shape','Data Type', 'Total Columns','No. Columns w\/ Missing Values', 'Columns w\/ Missing Values'])\nsummary_table","b8e1e8e1":"################################\n########## PLOT NANS ###########\n################################\nnan_results = pd.DataFrame(results.isna().sum().sort_values(ascending=False), columns = ['NaN_sum'])\nnan_results['%NaN'] = (nan_results['NaN_sum']\/len(results))*100\n\nnan_scan = pd.DataFrame(scan.isna().sum().sort_values(ascending=False), columns = ['NaN_sum'])\nnan_scan['%NaN'] = (nan_scan['NaN_sum']\/len(scan))*100\n\nnan_races = pd.DataFrame(races.isna().sum().sort_values(ascending=False), columns = ['NaN_sum'])\nnan_races['%NaN'] = (nan_races['NaN_sum']\/len(races))*100\n\nfig, ax = plt.subplots(nrows=1, ncols=3, figsize=(30,8))\nsns.barplot(x = nan_results.index, y = nan_results['%NaN'], ax=ax[0], order=nan_results.iloc[:7].index).set_title('Feature NaN% in RESULTS set')\nsns.barplot(x = nan_scan.index, y = nan_scan['%NaN'], ax=ax[1], order=nan_scan.iloc[:7].index).set_title('Feature NaN% in SCANS set')\nsns.barplot(x = nan_races.index, y = nan_races['%NaN'], ax=ax[2], order=nan_races.iloc[:7].index).set_title('Feature NaN% in RACES set')\nplt.show()","a5dd90aa":"################################\n########### CLEANING ###########\n################################\nscan = scan_df\n\n# Drop row from scan df if name == 'vacant box'\nsearchfor = [\"Vacant Box\",\"Any of dogs\", \"Either dog number\", \"The Field to beat\", \"Odds\", \"Evens\", \"Inside\", \"Outside\"]\nscan = scan[~scan.name.str.contains('|'.join(searchfor))]\n\n# Number\nscan['number'] = scan['race_data.runnerNumber']\n\n# Draw\nscan['draw'] = scan['race_data.drawNumber']\n\n# Distance \n#scan['Distance'] = races['Distance']\n\n# Sex\nscan['sex'] = scan['breedInfo.age'].str.extract(pat='(Dog|Bitch)',expand=False).fillna('NaN')\nscan['sex'] = pd.to_numeric(scan['sex'].str[0:5].replace({'Dog':'0', 'Bitch':'1'}))\n\n# Weight\nscan['weight'] = scan['keyStats.Weight'].fillna('0')\nscan['weight'] = pd.to_numeric(scan['weight'].apply(lambda x: x.split('kg')[0].strip()))\nscan['weight'] = scan['weight'].replace({0:scan['weight'].median()})\n\n# Age\nscan['age'] = scan['breedInfo.age'].fillna('0')\nscan['age'] = pd.to_numeric(scan['age'].apply(lambda x: x.split('yo')[0].strip()))\nscan['age'] = scan['age'].replace({0:scan['age'].median()})\n\n# Last Race Result\nscan['shortForm'] = scan['shortForm'].str[0:1].replace({'X':'0', 'D':'0', 'F':'0'})\nscan['last_result'] = pd.to_numeric(scan['shortForm'].str[0:1])\n\n# Win\/place percentage\nscan['keyStats.Win %'] = scan['keyStats.Win %'].fillna('0')\nscan['keyStats.Place %'] = scan['keyStats.Place %'].fillna('0')\nscan['career_win_perc'] = pd.to_numeric(scan['keyStats.Win %'].apply(lambda x: x.split('%')[0].strip()))\nscan['career_place_perc'] = pd.to_numeric(scan['keyStats.Place %'].apply(lambda x: x.split('%')[0].strip()))\n\n# CareerWins and CareerPlacings\nscan['CareerAppearances'] = scan['keyStats.Career'].apply(lambda x: x.split(':')[0].strip())\nscan['CareerPerformance'] = scan['keyStats.Career'].apply(lambda x: x.split(':')[-1].strip())\nscan['CareerPerformance'] = scan['CareerPerformance'].str.replace('-','')\nscan['career_wins'] = pd.to_numeric(scan['CareerPerformance'].str[0:1])\nscan['CareerSeconds'] = pd.to_numeric(scan['CareerPerformance'].str[1:2])\nscan['CareerThirds'] = pd.to_numeric(scan['CareerPerformance'].str[2:3])\nscan['career_placings'] = scan.loc[:,['CareerSeconds','CareerThirds']].sum(axis=1)\n\n# TrkWins and TrkPlacings\nscan['TrkAppearances'] = scan['keyStats.Trk\/Dist'].apply(lambda x: x.split(':')[0].strip())\nscan['TrkPerformance'] = scan['keyStats.Trk\/Dist'].apply(lambda x: x.split(':')[-1].strip())\nscan['TrkPerformance'] = scan['TrkPerformance'].str.replace('-','')\nscan['track_wins'] = pd.to_numeric(scan['TrkPerformance'].str[0:1])\nscan['TrkSeconds'] = pd.to_numeric(scan['TrkPerformance'].str[1:2])\nscan['TrkThirds'] = pd.to_numeric(scan['TrkPerformance'].str[2:3])\nscan['track_placings'] = scan.loc[:,['TrkSeconds','TrkThirds']].sum(axis=1)\nscan['track_best'] = scan['keyStats.Best Time'].str[0:4]\nscan['track_best'] = pd.to_numeric(scan['track_best'].replace(['NBT','FSH', 'FSTD'],[0,0,0],inplace=False))\n        \n    \n################################\n####### DROPPING COLUMNS #######\n################################\n# Merge results data with scan data\ndf = pd.merge(results_df, scan, on=[\"race_id\", \"name\"])\ndf = pd.merge(df, race_df, on=[\"race_id\"])  \n\n# Search columns for keyphases then drop them\ndf = df.drop(['marketId', 'id', 'active'], axis = 1)\nkeyphrases = ['keyStats', 'race_data', 'breedInfo', 'price_data', 'race_data', 'shortForm', 'Trk', 'Career', 'race_result']\nfor keyphrase in keyphrases:\n    df = df.drop(df.filter(like=keyphrase).columns, 1)\n\n# Drop columns without Odds as they are an integral part of the baselining\ndf = df[df['win_odds'].notna()] \n\n# Check we have no NaN's ('Empty DataFrame' = no NaN values)\nprint(df[df.isna().any(axis=1)].head()) \n\n# Re-order the features \ncol_to_keep = ['race_id', 'distance', 'draw', 'win_odds', 'place_odds', 'number', 'sex', 'weight', 'age', 'last_result', 'career_win_perc', 'career_place_perc', 'career_wins', 'career_placings', 'track_wins', 'track_placings', 'track_best', 'winner', 'place']\ndf = df[col_to_keep]\n\n#df['winner'] = df['winner'].fillna('0')\n#runs.loc[runs['race_id'] == 6184851]\n#df[df.isna().any(axis=1)].head()\n#scan_df[scan_df.isna().any(axis=1)].head(50)","321b5351":"df.head(15)","ff57e514":"################################\n########## FUNCTIONS ###########\n################################\ndef cramers_v(x, y):\n    confusion_matrix = pd.crosstab(x,y)\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2\/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))\/(n-1))\n    rcorr = r-((r-1)**2)\/(n-1)\n    kcorr = k-((k-1)**2)\/(n-1)\n    return np.sqrt(phi2corr\/min((kcorr-1),(rcorr-1)))","ad7dd172":"################################\n######### DISTRIBUTION #########\n################################\ndf_num = df.select_dtypes(include = ['float64', 'int64'])\ndf_num.hist(figsize=(30, 20), bins=50, xlabelsize=8, ylabelsize=8);\n\n#list(set(df.columns.tolist()))\n#column_list=df.columns.tolist()","b85c3f93":"################################\n########### PACKAGES ###########\n################################\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport scipy.stats as ss\n\n\n################################\n####### PANDAS PROFILING #######\n################################\n# Creating pandas.df.profileReports\ndf_profile = pp.ProfileReport(df, minimal=False)\n#df_profile\n\n\n################################\n########## CRAMERS V  ##########\n################################\n# Creating pandas.df.profileReports\n# column_list=df.columns.tolist()\n# for i in column_list:\n#    print(cramers_v(df['winner'],df[i]))\n\n\n################################\n###### CORRELATION MATRIX ######\n################################\n# Spearman is more appropriate for measurements taken from ordinal scales \nf, ax = plt.subplots(figsize=(30, 25))\nmat = df.corr('spearman')\nmask = np.triu(np.ones_like(mat, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(mat, mask=mask, cmap=cmap, vmax=1, center=0, annot = True,\n            square=True, linewidths=.5)\nplt.show()\n\n# Correlation with output variable\ncor = df.corr('spearman')\ncor_target = abs(cor[\"win_odds\"])\ncor_value = 0.05\n\n# Moderately correlated features\ncorrelated_features = cor_target[cor_target>=cor_value]\ncorrelated_features = correlated_features.sort_values(ascending=False).to_frame('win_odds')\ncorrelated_features.T\n","ddee695b":"################################\n#### VISUALISING CORRELATION ###\n################################\n# lorem ipsummmmmmm\nindex = ['career_win_perc', 'career_place_perc']\n\nfor i in index:\n    plt.figure(figsize = (16,7))\n    sns.regplot(data=df, x=i, y='win_odds', scatter_kws={'alpha':0.1})\n    plt.title(i + ' vs win_odds', fontsize = 12)\n    plt.legend(['$Spearman=$ {:.2f}'.format(cor_target[i])], loc = 'best')\n    plt.show()","215f02ce":"################################\n######### PLOTTING WIN% ########\n################################\ndef plot_win_percentage(df, col, target='winner'):\n    tmp1 = df[[col, target]].groupby([col, target]).size().reset_index(name='wins')\n    tmp2 = pd.DataFrame({'win_pct' : tmp1.groupby([col])['wins'].apply(lambda x: round(x \/ x.sum(), 2))})\n    tmp3 = pd.concat([tmp1, tmp2], axis=1)\n    tmp3 = tmp3[tmp3[target] == 1]\n    return tmp3\n\nage_temp = plot_win_percentage(df, 'age')\ndraw_temp = plot_win_percentage(df, 'draw')\nlast_result_temp = plot_win_percentage(df, 'last_result')\n\nfig, (ax1, ax2, ax3) = plt.subplots(1,3,  figsize=(30,6))\nfig.suptitle('Win% of nominal features', fontsize=24)\nax1.title.set_text('Age')\nax2.title.set_text('Draw')\nax3.title.set_text('Last result')\nax1.plot(age_temp['age'].astype(int), age_temp['win_pct'])\nax2.plot(draw_temp['draw'].astype(int), draw_temp['win_pct'])\nax3.plot(last_result_temp['last_result'].astype(int), last_result_temp['win_pct'])\nfig.supylabel('Win%', fontsize=20)\nplt.subplots_adjust(left=0.05, top=0.85)\n\n#df.groupby(df[\"winner\"])['win_odds'].median()","e526adb7":"################################\n########### PACKAGES ###########\n################################\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport numpy as np\nimport math\n\n\n################################\n##### DEFINING DF FEATURES #####\n################################\n# For the baseline models we don't require any features.  These are more to explore how the data works and how our modelling results will compare.\ncol_to_keep = ['race_id', 'number', 'draw', 'win_odds', 'place_odds', 'winner', 'place']\ndata = df[col_to_keep]\n\n\n################################\n######### PIVOT TABLE ##########\n################################\ndata_pivot = data.pivot(index='race_id',columns='number').reset_index()","e7dbfcc6":"data_pivot.head(4)","3169b78a":"################################\n### NUMBER OF DOGS IN A RACE ###\n################################\n# Create a list of lists to store race IDs under the number of dogs in each race\nrace_id_by_number_of_dogs = [[] for _ in range(0,10)]\n\n################################\n########## FUNCTIONS ###########\n################################\ndef number_of_dogs(data, race_id):\n    \"Returns the number of dogs in the specified race\"\n    X = data[data['race_id']==race_id]['winner']\n    return len(X.columns[(X.isna() == False).iloc[0]])\n\n\n# Loop through all race IDs & append it to the matching nod list (number of dogs) \nfor race_id in data_pivot.race_id.unique():\n    nod = number_of_dogs(data_pivot, race_id)\n    race_id_by_number_of_dogs[nod].append(race_id) \n\n# Create a new list with the counts of race IDs per race size\nnod_per_race_size = [len(list) for list in race_id_by_number_of_dogs]\n\n\n################################\n######## PLOT DOGS\/RACE ########\n################################\nx = [str(i) for i in range(0,10)]\nsize = (15, 8)\nfig = plt.figure(figsize=(12,5))\nsns.barplot(x, nod_per_race_size)\nplt.title(\"Dogs per race\", fontsize = 20)\nplt.ylabel('Count', fontsize=16)\nplt.xlabel('No. of dogs', fontsize=16)\nplt.show()","8846f1a5":"################################\n####### WINNER SELECTIONS ######\n################################\n# https:\/\/github.com\/codeworks-data\/mvp-horse-racing-prediction\/blob\/master\/winner\/winner_functions.py\n# Create a numpy array with a list of winners (one for each race_id)\ndef return_winner(data, race_id):\n    \"Returns the dog number integer for the winner\"\n    X = data[data['race_id']==race_id]['winner']==1\n    return X.columns[(X == True).iloc[0]][0]\nactual_winner = np.array([return_winner(data_pivot, race_id) for race_id in data_pivot.race_id])\n\n#We create this list with all race_id and this will alow us to match indice of the list with the actual race_id\nmatch_race_id_from_indices = data_pivot.race_id.to_list()\n\n################################\n########## FUNCTIONS ###########\n################################\ndef return_win_odds(data,race_id,number):\n    \"Returns the win_odds for a dog number using race_id\"\n    return data[data['race_id']==race_id]['win_odds'].iloc[0][number]\n\ndef return_random_winner_selection(data, race_id):\n    \"Generate a random winner selection seed (+1 is to ensure seed is always the same)\"\n    random.seed(race_id+1)\n    return random.sample(list_dogs(data,race_id), 1)[0]\n\ndef list_dogs(data,race_id):\n    \"Returns a list of each dog's number that finished the race\"\n    X = data[data['race_id']==race_id]['winner']\n    return X.columns[(X.isna() == False).iloc[0]].to_list()\n\n\ndef return_favourite(data, race_id):\n    \"Returns the dog number of the favourite\"\n    X = data[data['race_id']==race_id]['win_odds']\n    sorted_list = sorted(data[data['race_id']==race_id]['win_odds'].iloc[0].to_list())\n    J =[x for x in sorted_list if math.isnan(x) == False]\n    return X.columns[(X == J[0]).iloc[0]][0]\n\n\ndef return_secondfavourite(data, race_id):\n    \"Returns the dog number of the second favourite\"\n    X = data[data['race_id']==race_id]['win_odds']\n    sorted_list = sorted(data[data['race_id']==race_id]['win_odds'].iloc[0].to_list())\n    J =[x for x in sorted_list if math.isnan(x) == False]\n    return X.columns[(X == J[1]).iloc[0]][0]\n\n\ndef return_thirdfavourite(data, race_id):\n    \"Returns the dog number of the third favourite\"\n    X = data[data['race_id']==race_id]['win_odds']\n    sorted_list = sorted(data[data['race_id']==race_id]['win_odds'].iloc[0].to_list())\n    J =[x for x in sorted_list if math.isnan(x) == False]\n    return X.columns[(X == J[2]).iloc[0]][0]\n\n\ndef return_draw(data, race_id, draw_place):\n    \"Returns the dog that is placed in the specified draw\"\n    X = data[data['race_id']==race_id]['draw']\n    sorted_list = sorted(data[data['race_id']==race_id]['draw'].iloc[0].to_list())\n    J =[x for x in sorted_list if math.isnan(x) == False]\n    return X.columns[(X == J[draw_place-1]).iloc[0]][0] \n\n\n# Used to create a summary table of prediction performances\ndef balance_calculation(prediction_type, prediction_name):\n    \"Create a list item that contains summary information of the performance for that model\"\n    \"Retrieve array of predictions (prediction_type) and check if the winner matches prediction\"\n    \"If true, returns the win_odds of the index for the winning dog and updates balance\"\n    \"Since we are only using $1 bets: balance represents the total income we recieve, and the expenditure can be represented by len(data_pivot) == $1 * the number of races\"\n    is_same = (prediction_type == actual_winner)\n    indices = np.where( is_same == True)\n    balance = 0.0\n    for indice in indices[0]:\n        balance = balance + return_win_odds(data_pivot,match_race_id_from_indices[indice],actual_winner[indice])\n    list_item = [prediction_name, balance - len(data_pivot), is_same.sum(), is_same.sum()\/len(data_pivot), len(data_pivot)]\n    return list_item\n\n\ndef compute_to_df(predicted, actual, model_name):\n    \"Generate a Df for information pertaining to graphing the performance of the selections\"\n    model_name = pd.DataFrame(zip(predicted,actual), columns=['predicted','actual'])\n    model_name['win_odds'] = [return_win_odds(data_pivot,match_race_id_from_indices[indice],actual_winner[indice]) for indice in range(len(data_pivot))]\n    model_name['profit'] = np.where(model_name['predicted']== model_name['actual'], model_name['win_odds']-1, -1)\n    model_name['cumul'] = model_name['profit'].cumsum()\n    model_name['cumul_100'] = model_name['cumul'] + 100\n    return model_name\n\n\ndef table_metrics(i):\n    \"Takes items in balance_list and rounds up the Winning%\"\n    i = i.copy()\n    i[3] = str(round(i[3]*100, 1)) + ' %'\n    return i\n\n\n################################\n####### WINNER SELECTIONS ######\n################################\n# Create a list to append model balances to\nbalance_list = []\n\n# Random selection vs actual winner\nrandom_prediction = np.array([return_random_winner_selection(data_pivot, race_id) for race_id in data_pivot.race_id])\nbalance_random_prediction = balance_calculation(random_prediction, 'random_prediction')\nbalance_list.append(balance_random_prediction)\ndf_random_prediction = compute_to_df(random_prediction,actual_winner,'random_prediction')\n\n# First favourite vs actual winner\nfirst_favourite = np.array([return_favourite(data_pivot, race_id) for race_id in data_pivot.race_id])\nbalance_first_favourite = balance_calculation(first_favourite, 'first_favourite')\nbalance_list.append(balance_first_favourite)\ndf_first_favourite = compute_to_df(first_favourite,actual_winner,'first_favourite')\n\n# Second favourite vs actual winner\nsecond_favourite = np.array([return_secondfavourite(data_pivot, race_id) for race_id in data_pivot.race_id])\nbalance_second_favourite = balance_calculation(second_favourite, 'second_favourite')\nbalance_list.append(balance_second_favourite)\ndf_second_favourite = compute_to_df(second_favourite,actual_winner,'second_favourite')\n\n# Second favourite vs actual winner\nthird_favourite = np.array([return_thirdfavourite(data_pivot, race_id) for race_id in data_pivot.race_id])\nbalance_third_favourite = balance_calculation(third_favourite, 'third_favourite')\nbalance_list.append(balance_third_favourite)\ndf_third_favourite = compute_to_df(third_favourite,actual_winner,'third_favourite')\n\n# Specific draw (1) vs actual winner\ndraw_number = 1\ndraw_number_one = np.array([return_draw(data_pivot, race_id, draw_number) for race_id in data_pivot.race_id])\nbalance_draw_number_one = balance_calculation(draw_number_one, 'draw_number_one')\nbalance_list.append(balance_draw_number_one)\ndf_draw_number_one = compute_to_df(draw_number_one,actual_winner,'draw_number_one')\n\n# Specific draw (2) vs actual winner\ndraw_number = 2\ndraw_number_two = np.array([return_draw(data_pivot, race_id, draw_number) for race_id in data_pivot.race_id])\nbalance_draw_number_two = balance_calculation(draw_number_two, 'draw_number_two')\nbalance_list.append(balance_draw_number_two)\ndf_draw_number_two = compute_to_df(draw_number_two,actual_winner,'draw_number_two')\n\n\n################################\n####### PLOT PERFORMANCE #######\n################################\nfig, (ax1, ax2, ax3, ax4, ax5, ax6) = plt.subplots(1,6,  figsize=(30,6))\nfig.suptitle('Outcomes of winner betting', fontsize=24)\nax1.title.set_text('Random selection')\nax2.title.set_text('First favourite')\nax3.title.set_text('Second favourite')\nax4.title.set_text('Third favourite')\nax5.title.set_text('Draw 1')\nax6.title.set_text('Draw 2')\nax1.plot(df_random_prediction.index, df_random_prediction['cumul_100'])\nax2.plot(df_first_favourite.index, df_first_favourite['cumul_100'])\nax3.plot(df_second_favourite.index, df_second_favourite['cumul_100'])\nax4.plot(df_third_favourite.index, df_third_favourite['cumul_100'])\nax5.plot(df_draw_number_one.index, df_draw_number_one['cumul_100'])\nax6.plot(df_draw_number_two.index, df_draw_number_two['cumul_100'])\nfig.supxlabel('Number of races', fontsize=20)\nfig.supylabel('Balance', fontsize=20)\nplt.subplots_adjust(left=0.05, bottom=0.2, top=0.8)\n\n\n################################\n###### TABULATED SUMMARY #######\n################################\nupdated_balance_list = [table_metrics(i) for i in balance_list]\npd.DataFrame(updated_balance_list,columns=['Prediction model', 'Profit ($)','No. of winning bets', 'Winning%', 'Total bets placed'])\n","54d47a1a":"################################\n######## RANDOM PLACES #########\n################################\n# We create this list with all race_id and this will alow us to match indice of the list with the actual race_id\nmatch_race_id_from_indices = data_pivot.race_id.to_list()\n\n\n################################\n########## FUNCTIONS ###########\n################################\ndef return_placings(data,race_id, num):\n    \"Return a list with num horse if they are placed 1st, 2nd, and 3th\"\n    \"Do not respect the order\"\n    X = data[data['race_id']==race_id]['place']\n    return X.columns[(X == True).iloc[0]].to_list()[:num] \n\n\n#Function to determine if there are 2 or 3 placed horses in each race\ndef number_of_placings(data_pivot, race_id):\n    \"If the number of dogs in each race is >= 8, then return 3\"\n    \"If the number of dogs in each race is <= 7, then return 2\"\n    nod = number_of_dogs(data_pivot, race_id)\n    if nod >= 8:\n        return 3\n    else:\n        return 2\n\n\ndef number_of_dogs(data, race_id):\n    \"Returns the number of dogs in the specified race\"\n    X = data[data['race_id']==race_id]['winner']\n    return len(X.columns[(X.isna() == False).iloc[0]])\n\n    \ndef get_place_odds(data,race_id,number):\n    \"Return the place_odds for a race_id and a number\"\n    return data[data['race_id']==race_id]['place_odds'].iloc[0][number]\n\n\ndef get_best_place_odds(data, race_id, num):\n    \"Return a list of num horses which are favorties to get placed\"\n    \"Thoses horses are selected because the have the lowest place_odds\"\n    X = data[data['race_id']==race_id]['place_odds']\n    sorted_list = sorted(data[data['race_id']==race_id]['place_odds'].iloc[0].to_list()) \n    S =[x for x in sorted_list if math.isnan(x) == False]\n    return X.columns[(X.isin(S[:num])).iloc[0]].to_list()[:num] \n\n\ndef get_favourite_by_placed(data, race_id):\n    X = data[data['race_id']==race_id]['place_odds']\n    sorted_list = sorted(data[data['race_id']==race_id]['place_odds'].iloc[0].to_list()) \n    S =[x for x in sorted_list if math.isnan(x) == False]\n    return X.columns[(X.isin(S[0])).iloc[0]].to_list()[0] \n\n\n\ndef get_worst_place_odds(data, race_id, num):\n    \"Return a list of num horses which are favorties to get placed\"\n    X = data[data['race_id']==race_id]['place_odds']\n    sorted_list = sorted(data[data['race_id']==race_id]['place_odds'].iloc[0].to_list()) \n    S =[x for x in sorted_list if math.isnan(x) == False]\n    return X.columns[(X.isin(S[:-1])).iloc[0]].to_list()[:num] \n    \n\ndef get_random_place(data, race_id, num):\n    \"Return a random list of num horses\"\n    \"We use a seed to have always the same random prediction\"\n    random.seed(race_id)\n    return random.sample(list_dogs(data,race_id), num)\n\n\ndef return_favourite_place(data, race_id):\n    \"Returns the dog number of the favourite\"\n    X = data[data['race_id']==race_id]['place_odds']\n    sorted_list = sorted(data[data['race_id']==race_id]['place_odds'].iloc[0].to_list())\n    J =[x for x in sorted_list if math.isnan(x) == False]\n    return X.columns[(X == J[0]).iloc[0]][0]\n\n\ndef list_to_odds(x):\n    'return the amount of money we win according to prediction, real values and place_odds'\n    tot = 0\n    list_predicted = x.predicted\n    list_actual = x.actual\n    place_odds_actual = x.place_odds_actual\n    for i in range(len(list_actual)):\n        if list_actual[i] in list_predicted:\n            tot = tot + place_odds_actual[i] - 1\n        else:\n            tot = tot - 1\n    return tot\n\n\ndef get_place_draw(data, race_id, num):\n    \"Return a list of horse closest to the draw\"\n    \"Meaning with the lower draw_place\"\n    X = data[data['race_id']==race_id]['draw']\n    sorted_list = sorted(data[data['race_id']==race_id]['draw'].iloc[0].to_list())\n    S =[x for x in sorted_list if math.isnan(x) == False]\n    return X.columns[(X.isin(S[:num])).iloc[0]].to_list() #si jamais pas de valeur correspondante, on prendre ceux en fonction de la plus proche de la corde\n\n\ndef compute_placed_to_df(predicted, actual, model_name):\n    \"Generate a Df for information pertaining to graphing the performance of the selections\"\n    L = []\n    for i in range(len(actual)):\n        L.append([get_place_odds(data_pivot,match_race_id_from_indices[i],actual[i][j]) for j in range(len(actual[i]))])\n    model_name = pd.DataFrame(zip(predicted,actual), columns=['predicted','actual'])   \n    model_name['place_odds_actual'] = L\n    model_name['profit'] = model_name.apply(list_to_odds,axis=1)\n    model_name['cumul'] = model_name['profit'].cumsum()\n    model_name['cumul_100'] = model_name['cumul'] + 100\n    return model_name\n\n\ndef balance_calculation_placed(prediction_type, prediction_name):\n    \"Since we are only using $1 bets: balance represents the total income we recieve, and the expenditure can be represented by len(data_pivot) == $1 * the number of races\"\n    total_bets = 0\n    sucessful_bets = 0\n    balance = 0\n    for indice in range(len(actual_placed)):\n        for number in prediction_type[indice]:\n            if number in actual_placed[indice]:  #if the dog is in the prediction and the actual placed list\n                sucessful_bets = sucessful_bets + 1\n                balance = balance + get_place_odds(data_pivot,match_race_id_from_indices[indice],number)\n            total_bets = total_bets + 1\n    list_item = [prediction_name, balance - total_bets, sucessful_bets, sucessful_bets\/total_bets, total_bets]\n    return list_item\n\n\n# Used to create a summary table of prediction performances\ndef balance_calculation_single_place(prediction_type, prediction_name):\n    \"Create a list item that contains summary information of the performance for that model\"\n    \"Retrieve array of predictions (prediction_type) and check if the winner matches prediction\"\n    \"If true, returns the win_odds of the index for the winning dog and updates balance\"\n    \"Since we are only using $1 bets: balance represents the total income we recieve, and the expenditure can be represented by len(data_pivot) == $1 * the number of races\"\n    is_same = (prediction_type == actual_winner)\n    indices = np.where( is_same == True)\n    balance = 0.0\n    for indice in indices[0]:\n        balance = balance + get_place_odds(data_pivot,match_race_id_from_indices[indice],actual_winner[indice])\n    list_item = [prediction_name, balance - len(data_pivot), is_same.sum(), is_same.sum()\/len(data_pivot), len(data_pivot)]\n    return list_item\n\n\n\n################################\n###### PLACING SELECTIONS ######\n################################\n# Create a numpy array with a list of winners (one for each race_id)\nactual_placed = np.array([return_placings(data_pivot, race_id, number_of_placings(data_pivot, race_id)) for race_id in data_pivot.race_id])\n\n# Create a list to append model balances to\nbalance_list_placed = []\n\n# Take favourite for place\nfavourite_for_place = np.array([return_favourite_place(data_pivot, race_id) for race_id in data_pivot.race_id])\nbalance_favourite_for_place = balance_calculation_single_place(favourite_for_place, 'favourite_for_place')\nbalance_list_placed.append(balance_favourite_for_place)\ndf_favourite_for_place = compute_to_df(favourite_for_place, actual_placed,'favourite_for_place')\n\n# Random assortment of place betting vs actual placings\nrandom_place_prediction = np.array([get_random_place(data_pivot, race_id, number_of_placings(data_pivot, race_id)) for race_id in data_pivot.race_id])\nbalance_random_place_prediction = balance_calculation_placed(random_place_prediction, 'random_place_prediction')\nbalance_list_placed.append(balance_random_place_prediction)\ndf_random_place_prediction = compute_placed_to_df(random_place_prediction, actual_placed,'random_place_prediction')\n\n# Take the top favourites by place odds (2 or 3) vs actual placings\ntop_n_odds_place = np.array([get_best_place_odds(data_pivot, race_id, number_of_placings(data_pivot, race_id)) for race_id in data_pivot.race_id])\nbalance_top_n_odds_place = balance_calculation_placed(top_n_odds_place, 'top_n_odds_place')\nbalance_list_placed.append(balance_top_n_odds_place)\ndf_top_n_odds_place = compute_placed_to_df(top_n_odds_place, actual_placed,'top_n_odds_place')\n\n# Take the first 3 dogs by draw position\ntop_n_draws_place = np.array([get_place_draw(data_pivot, race_id, number_of_placings(data_pivot, race_id)) for race_id in data_pivot.race_id])\nbalance_top_n_draws_place = balance_calculation_placed(top_n_draws_place, 'top_n_draws_place')\nbalance_list_placed.append(balance_top_n_draws_place)\ndf_top_n_draws_place = compute_placed_to_df(top_n_draws_place, actual_placed,'top_n_draws_place')\n\n# Least favourite for place betting vs actual placings\nworst_n_odds_place = np.array([get_worst_place_odds(data_pivot, race_id, number_of_placings(data_pivot, race_id)) for race_id in data_pivot.race_id])\nbalance_worst_n_odds_place = balance_calculation_placed(worst_n_odds_place, 'worst_n_odds_place')\nbalance_list_placed.append(balance_worst_n_odds_place)\ndf_worst_n_odds_place = compute_placed_to_df(worst_n_odds_place, actual_placed,'worst_n_odds_place')\n\n\n\n################################\n####### PLOT PERFORMANCE #######\n################################\nfig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1,5,  figsize=(30,6))\nfig.suptitle('Outcomes of place betting', fontsize=24)\nax1.title.set_text('df_worst_n_odds_place')\nax2.title.set_text('df_random_place_prediction')\nax3.title.set_text('df_top_n_odds_place')\nax4.title.set_text('df_top_n_draws_place')\nax5.title.set_text('df_favourite_for_place')\nax1.plot(df_worst_n_odds_place.index, df_worst_n_odds_place['cumul_100'])\nax2.plot(df_random_place_prediction.index, df_random_place_prediction['cumul_100'])\nax3.plot(df_top_n_odds_place.index, df_top_n_odds_place['cumul_100'])\nax4.plot(df_top_n_draws_place.index, df_top_n_draws_place['cumul_100'])\nax5.plot(df_favourite_for_place.index, df_favourite_for_place['cumul_100'])\nfig.supxlabel('Number of races', fontsize=20)\nfig.supylabel('Balance', fontsize=20)\nplt.subplots_adjust(left=0.05, bottom=0.2, top=0.8)\n\n\n\n################################\n###### TABULATED SUMMARY #######\n################################\nupdated_balance_list_placed = [table_metrics(i) for i in balance_list_placed]\npd.DataFrame(updated_balance_list_placed,columns=['Prediction model', 'Profit ($)','No. of winning bets', 'Winning%', 'Total bets placed'])","e94f81fe":"################################\n########### PACKAGES ###########\n################################\n## Metrics \nfrom sklearn import metrics\nfrom sklearn.metrics import (roc_curve, roc_auc_score)\nfrom sklearn.metrics import (confusion_matrix, accuracy_score, classification_report)\nfrom sklearn.metrics import (f1_score, precision_score, recall_score)\n\n## StatsModels & SkLearn\nimport statsmodels.api as sm\nimport statsmodels.tools \nfrom sklearn import (svm, linear_model, preprocessing)\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import (LogisticRegression, LinearRegression)\nfrom sklearn.model_selection import (train_test_split, GridSearchCV, cross_val_score)\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier)\n\n\n################################\n########## FUNCTIONS ###########\n################################\n\ndef apr(y_real, y_pred):\n    \"Since accuracy is largely skewed by large numbers of True Negatives, we're going to want to focus on Precision and the F1 Score as comparison to compare False Negative and False Positive.\"\n    print('Accuracy: {:.4f}'.format(accuracy_score(y_real, y_pred)))\n    print('Precision: {:.4f}'.format(precision_score(y_real, y_pred)))\n    print('Recall: {:.4f}'.format(recall_score(y_real, y_pred)))\n    print('F1: {:.4f}'.format(f1_score(y_real, y_pred)))\n    return\n\n\ndef return_confusion_matrix(y_test, y_pred):\n    \"Use y_test and y_pred to calculate null accuracy and produce a confusion matrix\"\n    # Since we are only predicting 1\/8 winners, our null accuracy should be around 0.875\n    null_accuracy = (((len(y_test))-(y_test.loc[y_test == 1].sum()))\/(len(y_test)))\n    # Confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n    TP = cm[0,0]\n    TN = cm[1,1]\n    FP = cm[0,1]\n    FN = cm[1,0]\n    cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n    sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\n    print('\\nNull accuracy score: {0:0.4f}'. format(null_accuracy))\n    print('\\nFalse Positives(Type I error) = ', cm[0,1])\n    print('\\nFalse Negatives(Type II error) =', cm[1,0], '\\n')\n    return \n\n\ndef xy_split(df):\n    \"X,Y test train splitting while using groups (race_ids)\"\n    # Packages\n    from sklearn.model_selection import GroupShuffleSplit\n    \n    # Create a list for the feature columns and remove the response variables from it\n    feature_columns=[]\n    feature_columns = list(df.columns)\n    feature_columns.remove('winner')\n    feature_columns.remove('place')\n                                   \n    # GroupShuffleSplit\n    df = df.sort_values('race_id')\n    train_test_split_percentage = 0.8\n    uniques = df[\"race_id\"].unique()\n    sep = int(len(uniques) * train_test_split_percentage)\n    shuffled_data = df.sample(frac=1).reset_index(drop=True) #shuffles data\n    train_ids, test_ids = uniques[:sep], uniques[sep:] #seperates data at percentage point, then reassigns by race_id to test\/train\n    train_set, test_set = shuffled_data[shuffled_data.race_id.isin(train_ids)], shuffled_data[shuffled_data.race_id.isin(test_ids)]\n    train_set=train_set.sort_values('race_id').reset_index(drop=True)\n    test_set=test_set.sort_values('race_id').reset_index(drop=True)\n    \n    # Create our X and y variables\n    X_train = train_set[feature_columns]\n    y_train = train_set['winner']\n    X_test = test_set[feature_columns]\n    y_test = test_set['winner']\n    \n    # Store index values for future reference\n    X_test_race_id = X_test['race_id']\n    X_train_race_id = X_train['race_id']\n    X_test.drop(['race_id'], axis=1, inplace=True)\n    X_train.drop(['race_id'], axis=1, inplace=True)\n    \n    feature_columns = list(X_train.columns)\n    \n    #We slipt each dataset between features and labels\n    print(f\"Shape of original data: \", df.shape)\n    print(f\"feature columns: \", feature_columns)\n    print(f\"Shape of the X_train: \", X_train.shape)\n    print(f\"Shape of the y_train: \", y_train.shape)\n    print(f\"Shape of the X_test: \", X_test.shape)\n    print(f\"Shape of the y_test: \", y_test.shape)\n\n    return X_train, y_train, X_test, y_test, X_train_race_id, X_test_race_id\n\n\n\n\ndef add_constant(data):\n    data = data.copy()\n    data = sm.add_constant(data)\n    data.drop(inplace = True, columns = ['race_id'])\n    return data\n\n\ndef produce_confusion(positive_label, negative_label, cut_off, dfx, dfy, y_pred_name):\n    #Set pred to 0 or 1 depending on whether it's higher than the cut_off point.\n    dfx['pred_binary'] = np.where(dfx['prediction'] > cut_off , 1, 0)\n    cm = confusion_matrix(dfy, dfx['pred_binary'])\n    ax= plt.subplot()\n    sns.heatmap(cm, annot=True, ax=ax, fmt='g'); \n    ax.set_xlabel('Predicted labels');ax.set_ylabel('Real labels'); \n    ax.set_title('Confusion Matrix'); \n    ax.xaxis.set_ticklabels([negative_label, positive_label])\n    ax.yaxis.set_ticklabels([negative_label, positive_label]);\n    print('Test accuracy = ', accuracy_score(dfy, dfx['pred_binary']))\n    return accuracy_score(dfy, dfx['pred_binary'])\n\n\n\ndef predict_single_winner(model, cut_off):\n    \"........\"\n    X_test_results = X_test.copy()\n    X_test_results['y_prob'] = model.predict_proba(X_test_tree)[:,1]\n    X_test_results['y_pred'] = (model.predict_proba(X_test_tree)[:,1] >= cut_off).astype('int')\n    X_test_results['race_id'] = X_test_race_id\n    X_test_results['y_real'] = y_test.astype('int')\n    #X_test_results['race_id'] = X_test_race_id\n    #X_test_results = X_test_results.sort_index()\n    \n    # Reorder columns & assign a new feature 'One-winner'\n    X_test_results['One_winner'] = 0\n    \n    # sort by y_prob, then drop raceID duplicates to create a new DF of only the best probable dogs for each race\n    highest_prob = X_test_results\n    highest_prob = highest_prob.sort_values('y_prob').drop_duplicates('race_id', keep='last')\n    highest_prob['One_winner'] = 1\n    highest_prob = highest_prob.reset_index(drop=True) \n    \n    # Determine if any of the predictions is too low probability\n    low_probables = 0\n    confidence_value = cut_off\n    for i in range(len(highest_prob)):\n        if (highest_prob['y_prob'][i]<=confidence_value) == True:\n            highest_prob['One_winner'].iloc[i] = 0\n            low_probables = low_probables + 1\n            \n    # Print APR, number of removed predictions (confidence too low)     \n    print(f'{low_probables} predictions were too low of confidence')\n    print()\n    apr(X_test_results['y_pred'], X_test_results['y_real'])\n    print()\n    return highest_prob\n\n\ndef simulate_win_betting(prediction_df):\n    \"Return the mean for the set according to the min win_odds of each race\"\n    win_amount = 0\n    val = 0\n    for i in range(len(prediction_df)):\n        if (prediction_df['One_winner'][i]==1 and prediction_df['y_real'][i]==1) == True:\n            val = prediction_df[\"win_odds\"].iloc[i]\n            print(f'win ${val-1}')\n            win_amount = win_amount + val-1\n        elif (prediction_df['One_winner'][i]==1 and prediction_df['y_real'][i]==0) == True :\n            print('lose $1')\n            win_amount = win_amount-1\n    print()\n    win_amount = '$'+str(round(win_amount, 2))  \n    print(f'PROFIT: {win_amount}')\n    return \n\n\ndef caluclate_importance(columns, model):\n    \"The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance\"\n    \"Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See sklearn.inspection.permutation_importance as an alternative\"\n    \"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance\"\n    importance = list(zip(columns, list(model.feature_importances_)))\n    importance.sort(key=lambda x:x[1], reverse=True)\n    pd_importance = pd.DataFrame(data=importance, columns=[\"Feature\", \"Importance%\"])\n    return pd_importance","5ac885f7":"################################\n########### PACKAGES ###########\n################################\nimport pandas as pd\nimport numpy as np\nfrom lightgbm import plot_importance\nfrom lightgbm import LGBMClassifier\nimport sklearn.preprocessing as preprocessing\nimport sklearn.model_selection as model_selection\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot\nimport tensorflow as tf\nimport keras\nimport joblib\nfrom time import time\nfrom sklearn.model_selection import GroupShuffleSplit\n\n\n################################\n########## DATA SPLIT ##########\n################################\n# Split the data into train and test sets (80\/20 using xy_split function)\nX_train, y_train, X_test, y_test, X_train_race_id, X_test_race_id = xy_split(df)\n\n# filter out low importance columns \ntree_cols = [\n    'number', \n    'draw', \n    'sex', \n    'weight', \n    'age', \n    'last_result', \n    'career_win_perc', \n    'career_place_perc', \n    'career_wins', \n    'career_placings', \n    'track_wins', \n    'track_placings', \n    'track_best', \n    'win_odds', \n    'place_odds'\n]\n\nX_train = X_train[tree_cols]\nX_test = X_test[tree_cols]\n\n# Use gridsearch to generate a reccommended model\ntree_model = GridSearchCV(estimator = DecisionTreeClassifier(),\n                   param_grid = {'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n                                'min_samples_split': [2, 3, 5, 10, 15, 20],\n                                'min_samples_leaf': [2, 3, 4, 5, 6, 7, 8,9,10]},\n                    cv = 10,\n                    refit = True,\n                    verbose = 1,\n                    scoring = 'f1')\n\n\ntree_model.fit(X_train, y_train)\ntree_model.best_params_\n","f0a6f0ea":"################################\n########## RF MODEL ############\n################################\n# train a rf using gridsearch params\ntree = DecisionTreeClassifier(random_state=1, max_depth=4, min_samples_leaf=2, min_samples_split=20)\ntree.fit(X_train, y_train)\n\nprint('\\n\\033[1mAccuracy scores:\\033[0m')\nprint('Training set score: {:.4f}'.format(tree.score(X_train, y_train)))\nprint('Test set score: {:.4f}\\n'.format(tree.score(X_test, y_test)))\n\n# Use importance% to cut back cols\ncaluclate_importance(tree_cols, tree)","3383a262":"################################\n############ Y_PRED ############\n################################\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html\n# Although it doesn't mean to much at this stage, C=0.01 had the best accuracy score so we'll use that for y_pred calculation.  \nX_test_results = X_test.copy()\ny_pred_prob = tree.predict_proba(X_test)[0:] #predict lose\/win probability (0\/1)\ny_pred_prob_df = pd.DataFrame(data=y_pred_prob, columns=['prob_lose', 'prob_win'])\nX_test_results['y_prob'] = y_pred_prob_df['prob_win']\nX_test_results['race_id'] = X_test_race_id\n\n# Here, we set all y_pred == 0 to start.  Then we generate a new Df of the highest y_prob dogs of each race\n# by sorting by prob_win, then dropping raceID duplicates, we can set a single y_pred using the model's probability outputs - then we can update our X_test_results data with these values\nX_test_results['y_pred'] = 0\nhighest_prob = X_test_results\nhighest_prob = highest_prob.sort_values('y_prob').drop_duplicates('race_id', keep='last')\nhighest_prob['y_pred'] = 1\nX_test_results.update(highest_prob)\nX_test_results.tail(3)\n\n#X_test_results.loc[X_test_results['race_id'] == 6187112]\n#df.loc[df['race_id'] == 6187128.0]","b835c4a9":"################################\n###### APR & CORR. MATRIX ######\n################################\n# Now that we have only one y_pred per race, let's check APR\ntarget_names = ['lose', 'win']\ny_pred_test = X_test_results['y_pred']\nprint(classification_report(y_test, y_pred_test, target_names=target_names))\n\n# Confusion matrix\n# https:\/\/medium.com\/@dtuk81\/confusion-matrix-visualization-fc31e3f30fea\ncf_matrix = confusion_matrix(y_test, y_pred_test)\nrf_cf_matrix = cf_matrix\ngroup_names = ['True Neg','False Pos','False Neg','True Pos']\ngroup_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten()\/np.sum(cf_matrix)]\nlabels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')","032c572a":"rf_y_test = y_test\nrf_X_test = X_test_results\nrf_X_test['y_true'] = rf_y_test\nrf_X_test.head(3)","6d5bd34b":"################################\n########### PACKAGES ###########\n################################\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport lightgbm as lgb\nfrom lightgbm import plot_importance\nfrom lightgbm import LGBMClassifier\n\n\n################################\n########## DATA SPLIT ##########\n################################\n# Split the data into train and test sets (80\/20 using xy_split function)\nX_train, y_train, X_test, y_test, X_train_race_id, X_test_race_id = xy_split(df)\n\n\n################################\n######### GNB MODELS ###########\n################################\n# train a Gaussian Naive Bayes classifier on the training set\n# solver = .....\nclf = lgb.LGBMClassifier()\nclf.fit(X_train, y_train)\n\nprint('\\n\\033[1mAccuracy scores:\\033[0m')\nprint('Training set score: {:.4f}'.format(clf.score(X_train, y_train)))\nprint('Test set score: {:.4f}'.format(clf.score(X_test, y_test)))","e419b3b3":"################################\n############ Y_PRED ############\n################################\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html\n# Although it doesn't mean to much at this stage, C=0.01 had the best accuracy score so we'll use that for y_pred calculation.  \nX_test_results = X_test.copy()\ny_pred_prob = clf.predict_proba(X_test)[0:] #predict lose\/win probability (0\/1)\ny_pred_prob_df = pd.DataFrame(data=y_pred_prob, columns=['prob_lose', 'prob_win'])\nX_test_results['y_prob'] = y_pred_prob_df['prob_win']\nX_test_results['race_id'] = X_test_race_id\n\n# Here, we set all y_pred == 0 to start.  Then we generate a new Df of the highest y_prob dogs of each race\n# by sorting by prob_win, then dropping raceID duplicates, we can set a single y_pred using the model's probability outputs - then we can update our X_test_results data with these values\nX_test_results['y_pred'] = 0\nhighest_prob = X_test_results\nhighest_prob = highest_prob.sort_values('y_prob').drop_duplicates('race_id', keep='last')\nhighest_prob['y_pred'] = 1\nX_test_results.update(highest_prob)\n\n#X_test_results.loc[X_test_results['race_id'] == 6187112]\n#df.loc[df['race_id'] == 6187128.0]","5405e0b6":"################################\n###### APR & CORR. MATRIX ######\n################################\n# Now that we have only one y_pred per race, let's check APR\ntarget_names = ['lose', 'win']\ny_pred_test = X_test_results['y_pred']\nprint(classification_report(y_test, y_pred_test, target_names=target_names))\n\n# Confusion matrix\n# https:\/\/medium.com\/@dtuk81\/confusion-matrix-visualization-fc31e3f30fea\ncf_matrix = confusion_matrix(y_test, y_pred_test)\nlgbm_cf_matrix = cf_matrix\ngroup_names = ['True Neg','False Pos','False Neg','True Pos']\ngroup_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten()\/np.sum(cf_matrix)]\nlabels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')","ddea5bc7":"lgbm_y_test = y_test\nlgbm_X_test = X_test_results\nlgbm_X_test['y_true'] = lgbm_y_test\nlgbm_X_test.head(3)","0ae70d81":"################################\n########### PACKAGES ###########\n################################\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.naive_bayes import GaussianNB\nimport time\n\n\n################################\n########## DATA SPLIT ##########\n################################\n# Split the data into train and test sets (80\/20 using xy_split function)\nX_train, y_train, X_test, y_test, X_train_race_id, X_test_race_id = xy_split(df)\n\n\n################################\n######### GNB MODELS ###########\n################################\n# train a Gaussian Naive Bayes classifier on the training set\n# solver = .....\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\nprint('\\n\\033[1mAccuracy scores:\\033[0m')\nprint('Training set score: {:.4f}'.format(gnb.score(X_train, y_train)))\nprint('Test set score: {:.4f}'.format(gnb.score(X_test, y_test)))","30ca3386":"################################\n############ Y_PRED ############\n################################\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html\n# Although it doesn't mean to much at this stage, C=0.01 had the best accuracy score so we'll use that for y_pred calculation.  \nX_test_results = X_test.copy()\ny_pred_prob = gnb.predict_proba(X_test)[0:] #predict lose\/win probability (0\/1)\ny_pred_prob_df = pd.DataFrame(data=y_pred_prob, columns=['prob_lose', 'prob_win'])\nX_test_results['y_prob'] = y_pred_prob_df['prob_win']\nX_test_results['race_id'] = X_test_race_id\n\n# Here, we set all y_pred == 0 to start.  Then we generate a new Df of the highest y_prob dogs of each race\n# by sorting by prob_win, then dropping raceID duplicates, we can set a single y_pred using the model's probability outputs - then we can update our X_test_results data with these values\nX_test_results['y_pred'] = 0\nhighest_prob = X_test_results\nhighest_prob = highest_prob.sort_values('y_prob').drop_duplicates('race_id', keep='last')\nhighest_prob['y_pred'] = 1\nX_test_results.update(highest_prob)\n\n#X_test_results.loc[X_test_results['race_id'] == 6187112]\n#df.loc[df['race_id'] == 6187128.0]","85c2b972":"################################\n###### APR & CORR. MATRIX ######\n################################\n# Now that we have only one y_pred per race, let's check APR\ntarget_names = ['lose', 'win']\ny_pred_test = X_test_results['y_pred']\nprint(classification_report(y_test, y_pred_test, target_names=target_names))\n\n# Confusion matrix\n# https:\/\/medium.com\/@dtuk81\/confusion-matrix-visualization-fc31e3f30fea\ncf_matrix = confusion_matrix(y_test, y_pred_test)\ngnb_cf_matrix = cf_matrix\ngroup_names = ['True Neg','False Pos','False Neg','True Pos']\ngroup_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten()\/np.sum(cf_matrix)]\nlabels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')","75f38d53":"gnb_y_test = y_test\ngnb_X_test = X_test_results\ngnb_X_test['y_true'] = gnb_y_test\ngnb_X_test.head(3)","31ffdf9e":"################################\n########## FUNCTIONS ###########\n################################\n# https:\/\/www.kaggle.com\/prashant111\/logistic-regression-classifier-tutorial\ndef run_logit(dfx, dfy, feature_cols):\n    \"https:\/\/www.statsmodels.org\/dev\/generated\/statsmodels.discrete.discrete_model.Logit.html\"\n    y = dfy\n    X = dfx[feature_cols]\n    model = sm.Logit(y, X).fit()\n    return model\n\n\ndef metrics(dfx, dfy, model, cut_off, feature_cols):\n    params = model.params\n    model.summary()\n    dfx['prediction'] = model.predict(dfx[feature_cols])\n    accuracy = produce_confusion('winner', 'Loser', cut_off, dfx, dfy, 'winner')\n    dfx['pred_bins'] = pd.cut(dfx['prediction'], np.arange(0.05,0.95,0.05))\n    dfx = pd.concat([dfx, dfy], axis =1, join=\"inner\" )\n    summary = dfx.groupby('pred_bins').agg({'winner':'mean', 'prediction':['mean','count']})\n    summary = summary.reset_index()\n    summary.columns = ['pred_bins','%_winner','avg_prediction','count']\n\n    #start AUC\n    logit_roc_auc = roc_auc_score(dfx['winner'], dfx['prediction'])\n    fpr, tpr, thresholds = roc_curve(dfx['winner'], dfx['prediction'])\n    plt.figure()\n    plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    return None","2faafd89":"################################\n########### PACKAGES ###########\n################################\nimport pandas as pd\nimport numpy as np\nfrom lightgbm import plot_importance\nfrom lightgbm import LGBMClassifier\nimport sklearn.preprocessing as preprocessing\nimport sklearn.model_selection as model_selection\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot\nimport tensorflow as tf\nimport keras\nimport joblib\nfrom time import time\nfrom sklearn.model_selection import GroupShuffleSplit\nimport warnings\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\nwarnings.simplefilter('ignore', ConvergenceWarning)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n\n################################\n########## DATA SPLIT ##########\n################################\n# Split the data into train and test sets (80\/20 using xy_split function)\nX_train, y_train, X_test, y_test, X_train_race_id, X_test_race_id = xy_split(df)\n\n\n################################\n########## LR MODELS ###########\n################################\n# Cfloat = Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization - default=1.0\n# LogisticRegression with C=1\nlogreg = LogisticRegression(solver='liblinear', random_state=1)\nlogreg.fit(X_train, y_train)\nprint('\\n\\033[1mC=1\\033[0m')\nprint('Training set score: {:.4f}'.format(logreg.score(X_train, y_train)))\nprint('Test set score: {:.4f}'.format(logreg.score(X_test, y_test)))\n\n# LogisticRegression with C=100\nlogreg100 = LogisticRegression(C=100, solver='liblinear', random_state=1)\nlogreg100.fit(X_train, y_train)\nprint('\\n\\033[1mC=100\\033[0m')\nprint('Training set score: {:.4f}'.format(logreg100.score(X_train, y_train)))\nprint('Test set score: {:.4f}'.format(logreg100.score(X_test, y_test)))\n\n# LogisticRegression with C=0.01\nlogreg001 = LogisticRegression(C=0.01, solver='liblinear', random_state=1) \nlogreg001.fit(X_train, y_train)\nprint('\\n\\033[1mC=0.01\\033[0m')\nprint('Training set score: {:.4f}'.format(logreg001.score(X_train, y_train)))\nprint('Test set score: {:.4f}'.format(logreg001.score(X_test, y_test)))","c936f24c":"################################\n############ Y_PRED ############\n################################\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html\n# Although it doesn't mean to much at this stage, C=0.01 had the best accuracy score so we'll use that for y_pred calculation.  \nX_test_results = X_test.copy()\ny_pred_prob = logreg001.predict_proba(X_test)[0:] #predict lose\/win probability (0\/1)\ny_pred_prob_df = pd.DataFrame(data=y_pred_prob, columns=['prob_lose', 'prob_win'])\nX_test_results['y_prob'] = y_pred_prob_df['prob_win']\nX_test_results['race_id'] = X_test_race_id\n\n# Here, we set all y_pred == 0 to start.  Then we generate a new Df of the highest y_prob dogs of each race\n# by sorting by prob_win, then dropping raceID duplicates, we can set a single y_pred using the model's probability outputs - then we can update our X_test_results data with these values\nX_test_results['y_pred'] = 0\nhighest_prob = X_test_results\nhighest_prob = highest_prob.sort_values('y_prob').drop_duplicates('race_id', keep='last')\nhighest_prob['y_pred'] = 1\nX_test_results.update(highest_prob)\nX_test_results.tail(3)\n\n#X_test_results.loc[X_test_results['race_id'] == 6187112]\n#df.loc[df['race_id'] == 6187112]","df985d01":"################################\n###### APR & CORR. MATRIX ######\n################################\n# Now that we have only one y_pred per race, let's check APR\ntarget_names = ['lose', 'win']\ny_pred_test = X_test_results['y_pred']\nprint(classification_report(y_test, y_pred_test, target_names=target_names))\n\n# Confusion matrix\n# https:\/\/medium.com\/@dtuk81\/confusion-matrix-visualization-fc31e3f30fea\ncf_matrix = confusion_matrix(y_test, y_pred_test)\nlr_cf_matrix = cf_matrix\ngroup_names = ['True Neg','False Pos','False Neg','True Pos']\ngroup_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten()\/np.sum(cf_matrix)]\nlabels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')","075eee0d":"lr_y_test = y_test\nlr_X_test = X_test_results\nlr_X_test['y_true'] = lr_y_test\nlr_X_test.head(3)","9b6813de":"column_list = df.columns.tolist()","d642eae8":"################################\n######## NEURAL NETWORK ########\n################################\nimport sklearn.model_selection as model_selection\nimport tensorflow as tf\nss = preprocessing.StandardScaler()\n\n\n################################\n########### DF PIVOT ###########\n################################\n# Draw 10 represents the NaN values from Draw currently  - might be worth just wiping these sets of races since Draw likely plays a large role. If there isn't a winner for that row, then it shouldn't negatively impact the model\ncols = ['race_id', 'distance', 'draw', 'win_odds', 'place_odds', 'number', 'sex', 'weight', 'age', 'last_result', 'career_win_perc', 'career_place_perc', 'career_wins', 'career_placings', 'track_wins', 'track_placings', 'track_best', 'winner']\ndata_pivot = df[cols]\ndata_pivot.reset_index(drop=True, inplace=True)\ndata_pivot = data_pivot.pivot_table(index='race_id',columns='draw')\ndata_pivot = data_pivot.fillna(0) # There are quite a lot of NaN values due to race size variation - fill NaN with 0.\n\n\n################################\n####### TRAIN\/TEST SPLIT #######\n################################\nX = data_pivot[data_pivot.columns[:-10]] \nX = pd.DataFrame(ss.fit_transform(X),columns = X.columns)\ny = data_pivot[data_pivot.columns[-10:]].applymap(lambda x: 1.0 if 0.5 < x < 1.5 else 0.0) \n\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=1) #train_size\nprint(X_train.shape,y_train.shape,X_test.shape,y_test.shape)\n## df.loc[df['race_id'] == 6184851]\n\nprint(X.shape)\nprint(y.shape)","b35a01d9":"################################\n######### KERAS MODEL ##########\n################################\n# optimize\/losses = https:\/\/keras.io\/api\/optimizers\/ \n# https:\/\/towardsdatascience.com\/use-machine-learning-to-predict-horse-racing-4f1111fb6ced\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(96, activation='relu', input_shape=(150,)),\n    tf.keras.layers.Dense(10, activation='softmax')])\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n    loss=tf.keras.losses.CategoricalCrossentropy(\n        from_logits=False,\n        label_smoothing=0,\n        axis=-1,\n        reduction=\"auto\", \n        name=\"categorical_crossentropy\"),\n    metrics=[tf.keras.metrics.Precision(name='precision')])\n\nprint(model.summary())","2576c2f5":"################################\n######### KERAS MODEL ##########\n################################\ndataset = tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values))\ntrain_dataset = dataset.shuffle(len(X_train)).batch(500)\ndataset = tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values))\nvalidation_dataset = dataset.shuffle(len(X_test)).batch(500)\n\nprint(\"Start training..\\n\")\nhistory = model.fit(train_dataset, epochs=200, validation_data=validation_dataset, verbose=0)\nprint(\"Done.\")\n\nprecision = history.history['precision']\nval_precision = history.history['val_precision']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(precision) + 1)\n\nplt.plot(epochs, precision, 'b', label='Training precision')\nplt.plot(epochs, val_precision, 'r', label='Validation precision')\nplt.title('Training and validation precision')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","71b902b7":"################################\n####### MODEL ACCURACIES #######\n################################\ndef unravel_cf(matrix, model_title):\n    \"Unravels cf pieces to compare across a df\"\n    tn, fp, fn, tp = matrix.ravel()\n    df = pd.DataFrame(np.array([[model_title, tn, fp, fn, tp]]), columns=['Model type','True Neg', 'False Pos.', 'False Neg.', 'True Pos.'])\n    return df\n    \n\nlr_cf = unravel_cf(lr_cf_matrix, 'Linear Regression')\ngnb_cf = unravel_cf(gnb_cf_matrix, 'Naive Bayes')\nlgbm_cf = unravel_cf(lgbm_cf_matrix, 'LightGBM')\nrf_cf = unravel_cf(rf_cf_matrix, 'Random Forest')\n\ncf_frames = [lr_cf, gnb_cf, lgbm_cf, rf_cf]\nconfusion_df = pd.concat(cf_frames).reset_index(drop=True)  \n#results['Profit($)']=results['Profit($)'].astype(float).round(2)\n#results['Winning%']=results['Winning%'].astype(float).round(2)\nconfusion_df.head()","b613997a":"################################\n##### BETTING SIMULATIONS ######\n################################\n# https:\/\/www.kaggle.com\/prashant111\/logistic-regression-classifier-tutorial\ndef compute_model_simulation(data, model_title):\n    \"\"\n    col_to_keep = ['race_id', 'win_odds', 'y_true', 'y_pred']\n    data = data[col_to_keep]\n    total_bets = 0\n    successful_bets = 0\n    balance = 0\n    rows = data.index.tolist()\n\n    for i in rows:\n        if (data[\"y_pred\"].iloc[i]==1 and data[\"y_true\"].iloc[i]==1) == True:\n            successful_bets = successful_bets + 1\n            win_odds = data[\"win_odds\"].iloc[i]\n            balance = balance + win_odds - 1\n            total_bets = total_bets + 1\n        elif (data[\"y_pred\"].iloc[i]==1 and data[\"y_true\"].iloc[i]==0) == True:        \n            total_bets = total_bets + 1\n    winning_percentage = (successful_bets\/total_bets)\n    df = pd.DataFrame(np.array([[model_title, successful_bets, total_bets, winning_percentage, balance]]), columns=['Model type','No. of winning bets', 'Total bets placed', 'Winning%', 'Profit($)'])\n    return df\n\nlr_df = compute_model_simulation(lr_X_test, 'Linear Regression')\ngnb_df = compute_model_simulation(gnb_X_test, 'Naive Bayes')\nlgbm_df = compute_model_simulation(lgbm_X_test, 'LightGBM')\nrf_df = compute_model_simulation(rf_X_test, 'Random Forest')\n\nframes = [lr_df, gnb_df, lgbm_df, rf_df]\nresults = pd.concat(frames).reset_index(drop=True)  \nresults['Profit($)']=results['Profit($)'].astype(float).round(2)\nresults['Winning%']=results['Winning%'].astype(float).round(2)\nresults.head()","b22a305d":"# #########################################################################\n# #############################  I G N O R E  #############################\n# #########################################################################\n\n\n\n\n# ################################\n# ###### CORRELATION MATRIX ######\n# ################################\n\n# # Correlation Matrix\n# f, ax = plt.subplots(figsize=(30, 25))\n# mat = runs.corr('pearson')\n# mask = np.triu(np.ones_like(mat, dtype=bool))\n# cmap = sns.diverging_palette(230, 20, as_cmap=True)\n# sns.heatmap(mat, mask=mask, cmap=cmap, vmax=1, center=0, annot = True,\n#             square=True, linewidths=.5)\n# plt.show()\n\n# # Correlation with output variable\n# cor = runs.corr()\n# cor_target = abs(cor[\"CarWinPerc\"])\n# cor_value = 0.01\n\n# # Moderately correlated features\n# correlated_features = cor_target[cor_target>=cor_value]\n# correlated_features = correlated_features.sort_values(ascending=False).to_frame('correlation')\n# correlated_features.T\n\n\n# ################################\n# #### VISUALISING CORRELATION ###\n# ################################\n# # lorem ipsummmmmmm\n# index = correlated_features.index.tolist()\n\n# for i in index:\n#     plt.figure(figsize = (16,7))\n#     sns.regplot(data=runs, x=i, y='RaceResult', scatter_kws={'alpha':0.2})\n#     plt.title(i + 'vs RaceResult', fontsize = 12)\n#     plt.legend(['$Pearson=$ {:.2f}'.format(cor_target[i])], loc = 'best')\n#     plt.show()\n    \n    \n    \n    \n# #  SCAN - NaN counts and % for each column  \n# nan = pd.DataFrame(scan.isna().sum(), columns = ['Count'])\n# nan['%NaN'] = (nan['Count']\/len(scan))*100\n# nan = nan[nan['Count'] > 0]\n# nan = nan.sort_values(by = ['Count'], ascending=False)\n# nan = nan.T\n# nan\n\n\n\n\n\n# ################################\n# ####### NAN COMPARISON #########\n# ################################\n# #  Dataframe copy\n# runs = runs #### pd.merge(scan_df, results_df, on=[\"race_id\"])\n# races = races #### pd.merge(scan_df, results_df, on=[\"race_id\"])\n\n# #  Plotting how the NAN spread looks across our data sets\n# nan_runs = pd.DataFrame(runs.isna().sum().sort_values(ascending=False), columns = ['NaN_sum'])\n# nan_runs['Perc(%)'] = (nan_runs['NaN_sum']\/len(runs))*100\n\n# nan_races = pd.DataFrame(races.isna().sum().sort_values(ascending=False), columns = ['NaN_sum'])\n# nan_races['%NaN'] = (nan_races['NaN_sum']\/len(races))*100\n\n# fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20,8))\n# sns.barplot(x = nan_races.index, y = nan_races['%NaN'], ax=ax[1], order=nan_races.iloc[:7].index).set_title('Feature NaN% in races set')\n# sns.barplot(x = nan_runs.index, y = nan_runs['Perc(%)'], ax=ax[0], order=nan_runs.iloc[:7].index).set_title('Feature NaN% in runs set')\n# plt.show()\n\n\n\n# ################################\n# ########## NAN TABLE ###########\n# ################################\n# #  Dataframe copy\n# combined = runs #### pd.merge(scan_df, results_df, on=[\"race_id\"])\n\n# #  NaN counts and % for each column  \n# nan = pd.DataFrame(combined.isna().sum(), columns = ['Count'])\n# nan['%NaN'] = (nan['Count']\/len(combined))*100\n# nan['%Cor'] = correlated_features\n# nan = nan[nan['Count'] > 0]\n# nan = nan.sort_values(by = ['Count'], ascending=False)\n# nan = nan.T\n# nan\n\n","fa6c5a8f":"# # create a second data frame with the rest of the data by removing the duplicate indexes of highest_prob rows\n# duplicate_indexes = highest_prob.index.values\n# desired_indices = [i for i in wins_modelled.index if i not in duplicate_indexes]\n# lowest_prob = wins_modelled.iloc[desired_indices]\n\n# # Create a fresh data frame with no duplicates that only picks one winner for each race using y_prob\n# one_winner = pd.concat([lowest_prob, highest_prob], axis =0, join='inner').sort_values('race_id')\n# #one_winner = one_winner[~one_winner.index.duplicated(keep='first')]\n\n# # Now cut down this data (which is everything at this stage) to only take the columns of our testing split (X_test_index).  \n# test_one_winner = one_winner[one_winner.index.isin(X_test_index)]  # This will give us all the data from the test set\n# test_one_winner = test_one_winner.loc[test_one_winner['One_winner'] == 1] # Then trim back the df above to only show where a winner was predicted\n\n# #apr(one_winner['One_winner'], one_winner['y_real'])\n# #highest_prob.loc[highest_prob['race_id'] == 6187109]\n\n\n# one_winner_pivot = one_winner.pivot(index='race_id',columns='number').reset_index()\n\n# def return_rf_winner(data, race_id):\n#     \"Returns the dog number of the rf winner prediction\"\n#     X = data[data['race_id']==race_id]['One_winner']\n    \n#     sorted_list = (data[data['race_id']==race_id]['One_winner']==1).to_list()\n#     J =[x for x in sorted_list if math.isnan(x) == False]\n#     return X.columns[(X == J[0]).iloc[0]][0]\n\n\n# rf_winner = np.array([return_rf_winner(one_winner_pivot, race_id) for race_id in one_winner_pivot.race_id])\n\n\n# win_amount=0\n# if (one_winner['One_winner'][i]==1 and one_winner['y_real'][i]==1) == True:\n#     val = 0\n#     print('yes')\n#     val = one_winner.win_odds[:][one_winner['y_pred'][0]+i].tolist()\n#     win_amount = win_amount + val\n# elif (one_winner['One_winner'][i]==1 and one_winner['y_real'][i]==0) == True :\n#     val = 0\n#     print('no')\n#     win_amount = win_amount-1\n\n# print(val)  \n# print(win_amount)\n","c1197986":"# 6.0 Testing Algorithms\n * A list of functions required for the next few alogrithms\n * Testing a variety of models for performance without K-folds\n * Cross validating (K-Fold)\n * Evaluation metrics (MSE \/ R-Square)","69ab368b":" ***","03fde2e4":"# 5.0 Feature Engineering\n","a4007bb3":" ***","3de39b5a":"\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana;\">\n\n    \n##   \ud83d\udcda  **To-do List...**\n\n####  \n    \n#### \ud83d\udccc **Data changes**\n* **race-times:** see if we can scrape race times - it will provide the option to use regression models and open a few more doors in terms of probability ranking\n\n####  \n    \n  \n#### \ud83d\udccc **Feature engineering**\n* **Modelling features:** since the market takes fully into account the features it provides by default, a model we build will likely struggle to beat the market.  If we engineer features that show high win predictability that the market does not fully compensate for - we might be able to squeeze a lot more precision out of our predictions. \n    \n####   \n   \n#### \ud83d\udccc **Model building**\n* **SVM classifier:** The goal of the SVM algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future.\n* **XGB \/ LightGBM classifier:** since every successive decision tree is built on the errors of the previous trees, gradient boosted classifiers might work best here without race data\n* **Learning-to-rank:** !!!!!!\n      \n####  \n    \n#### \ud83d\udccc **Functions**\n* **Feature importance:** we need a feature importance function to determine the weight each model gives a feature in it's calculation.  We can use this in determining what models to continue using\/drop\n* **Value betting:** a value bet is a bet where you expect to have more equity than your opponent.  With enough data we could try develop a function to predict odds\/probability then weigh the return value of the market against that.  We'd have to explore trends in odds, and almost create an entirely different model for odds prediction","3654196b":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana;\">\n\n    \n##   \ud83d\udcda  **Data selection...**\n\n* race_id \n* distance\n* number \n* draw  \n* sex  \n* weight  \n* age  \n* last_result  \n* career_win_perc  \n* career_place_perc  \n* career_wins  \n* career_placings  \n* TrackWins  \n* track_placings  \n* track_best  \n* win_odds  \n* place_odds  \n* winner \n* place","ff7c3ba5":"# 2.0 Preparing the problem\n \n* Data types\n* NaN occurance\n* Cleaning the Dfs\n* Concatting","57378a2a":"***","f1ac970a":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana;\">\n\n##   \ud83d\udcda  **Combing our vote classifier and utilising a profitability strategy...**\n \n#####  \n    \n#### \ud83d\udccc **Value betting**\nOne way to turn a profit betting on sport is to bet on matches where the bookmakers' odds reflect a probability that is less to the actual probability of that outcome occurring AKA value betting strategy. Value betting is the ability to identify an edge against the market\/bookmakers. If a bet is priced for less than it\u2019s worth, then it offers value.\n    \n#####  \n    ","2d357deb":"## 6.6 Keras Sequential\nA Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.","17270598":"***","f6fd265a":"## 6.1 Random Forest\n","be20b720":"***","5723befc":"***","44234934":"## 8.0 Profitability Strategy \/ Accuracy Improvement\n * Creating a value betting strategy\n * Tuning algorithm parameters and hyperparameters for better results","63d652f3":"\n <\/br>\n \n   \n <\/br>\n \n   \n <\/br>\n \n   \n <\/br>\n \n   \n <\/br>\n \n   \n <\/br>\n \n   \n <\/br>\n \n   \n <\/br>\n \n   \n <\/br>\n \n   \n <\/br>\n \n   \n <\/br>\n \n   \n <\/br>\n \n   \n <\/br>\n \n   \n <\/br>\n \n   \n <\/br>\n \n   \n <\/br>\n \n   \n <\/br>\n \n   \n <\/br>\n \n   \n <\/br>\n \n   \n <\/br>\n \n   \n <\/br>\n \n   \n <\/br>\n \n   ","967e54f1":"# 1.0 Loading...\n \n\n##### JSON dump\n * Appending the api output documents\n \n##### Cleaning Schema\n * Cleaning schema to keep only the multiindex components\n\n    \n##### Races Data\n * Data on the each race, including venue, distance, race number and track & weather conditions.\n\n##### Results Data\n * Each line describes the features of one doggo run and their info post-race.\n \n##### Scans Data\n * Each line describes the features of one doggo run and their info pre-race. \n  \n","6878c5d3":"\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana;\">\n\n###   \ud83d\udcda  **Winner betting...**\n\n* **random:** select a random dog and compare it to the actual winner\n* **win_odds:** select the dog with the lowest win_odds \n* **best draw:**  select the dog with the best draw","407048b3":"# 7.0 Evaluating Algorithms","c0eee87e":"***","76a54925":"***","536d254f":"***","16f948c3":"***","1467cc03":"## Project Outline\n\n### 1. Loading...\n * Cleaning the schema\n * JSON Dump\n * Load dataset\n    \n\n### 2. Preparing the problem\n * NaN occurance\n * Cleaning the Dfs\n * Concatting\n * Pivot tables\n\n\n### 3. Exploratory Data Analysis (EDA)\n * Descriptive statistics\n * Data visualizations\n    \n\n### 4. Baseline Simulations \/ Feature Selection\n * Exploring a variety of baseline betting simulations\n * Establish useful features to include in the algorithms\n    \n\n\n### 5. Feature Engineering\n * Explore a mix of custom variables that demonstrate significance\n    \n\n\n### 6. Testing Algorithms\n * Instantiate a mix of regression, classification & LTR models\n * Evaluation metrics (Type I & Type II errors, precision\/F1)\n\n\n### 7. Evaluating Algorithms\n * Voting system (or shortlisting)\n * Betting simulation \n\n\n### 8. Profitability Strategy \/ Accuracy Improvement\n * Creating a value betting strategy\n * Tuning algorithm parameters and hyperparameters for better results\n\n\n### 9.  Finalising a Model\n * Validating the model on unseen race dataset.\n    \n\n### 10.  References\n * A list of reference material used throughout this project\n   \n","5c19983b":"***","d89e69ef":"\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana;\">\n    \n##   \ud83d\udcda  **Glossary of Terms...**\n* **Rating:** Based on their previous time and win\/place ratio\n* **raceser:** A person who is registered to races, kennel, nominate and race a greyhound.\n* **Owner:** A person\/s registered by GRV for the purpose of owning a registered greyhound eligible to be nominated for a race or event.\n* **Rating:** Takes into account current form and consistency\n* **Sire:** A stud dog that is the registered father of a specific greyhound.\n* **Dam:** A Brood Matron that is the registered mother of a specific greyhound.\n* **Last 6:** The dogs finishing position for its last six races.\n* **Win%:** Percentage of races this dog has winner.\n* **Place%:** Percentage of races this dog has placed.\n* **Career:** Number of career starts, and the amount of win, second, and third places.\n* **Prize Money:** The amount of money the runner has earned in winnings shown in AUD.\n* **Track\/Dist:** Number of starts on today's races track, at this distance, and the amount of wins, second place, or third placings at this same track and distance.\n* **Best Time:** No best time (NBT), First start here (FSH)\n* **Whelped:** Date of Birth\n* **Weight:** Weight of greyhound in kilos\n","2741b785":"## 6.3 Naive Bayes Classifier\nNa\u00efve Bayes classification is based on applying Bayes\u2019 theorem with strong independence assumption between the features.\n* Gaussian NB: It should be used for features in decimal form. GNB assumes features to follow a normal distribution.\n\n* MultiNomial NB: It should be used for the features with discrete values like word count 1,2,3...\n\n* Bernoulli NB: It should be used for features with binary or boolean values like True\/False or 0\/1.","9d859454":"\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana;\">\n\n    \n##   \ud83d\udcda  **Custom features...**\n* **Modelling new features:** since the market takes fully into account the features it provides by default, a model we build will likely struggle to beat the market.  If we engineer features that show high win predictability that the market does not fully compensate for - we might be able to squeeze a lot more precision out of our predictions. \n \n","d8ffa433":"\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana;\">\n\n    \n##   \ud83d\udcda  **Place betting...**\n* **random list:** select a list at random and compare it to actual placings\n* **by place_odds:** select the top 2\/3 doggs by place odds and compare it to actual placings\n* **by draw:** select the top 2\/3 doggs by draw and compare it to actual placings\n    \n#####  \n    \n#### \ud83d\udccc **A place bet is placed on a dog to finish the race in:**\n\n* first, second or third position, if there are 8 runners or more in the race;\n\n* if there are less than 7 runners a place bet will be on first and second place;\n    \n* if there are fewer than 5 runners there will be no place bet offered.\n    \n\n\n","33aa14bb":"## 6.5 K-Nearest Neighbors Classifier\nThe K-Nearest Neighbors (KNN) algorithm is a generalization of the nearest neighbor (NN) algorithm. The core idea of the KNN algorithm is similar to that of the nearest neighbor algorithm, which is classified by finding categories similar to the unknown samples. However, in the NN algorithm, only one sample is used for decision. When the classification is too absolute, the classification effect is poor. To overcome the defect, the KNN algorithm uses K neighboring samples to jointly decide the categories of unknown samples.\nhttps:\/\/www.kaggle.com\/yyzz1010\/predict-the-winning-horse-100-on-small-test-data\/notebook#MODELING","5fb55df8":"## 6.2 LightGBM\nLightGBM is a fast, distributed, high performance gradient boosting framework based on decision tree algorithms, used for ranking, classification \n","72b98de6":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana;\">\n\n##   \ud83d\udcda  **Pivot table...**\n    \n\n    \n#### \ud83d\udccc **Pivotting our data:**\nNow that we have addressed cleaning of the data - we can begin pivoting each race's run data. By pivoting - we aggregate the dog's run data into one row of each race (basically 1 row of race data, will contain a column number for each dog inside it).  This will be used in a number of functions that require race\/dog numbers math.\n\n\n#### \ud83d\udccc **Changes after the pivot:**\nIt's safe to assume that a fair few races may not contain 8 dogs, so to keep our data uniform, we'll just fill NaNs with 0.\n\n","61906987":"***","9892b8ba":"# 4.0 Baseline Models & Feature Selection\n\nHere we're creating a number of baseline models for reference with future ML models","35da5a30":"*** ","f3859a8b":"# 9.0 ....\n\n* lorem ipsum","650c9925":"\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana;\">\n\n    \n##   \ud83d\udcda  **Betting selection types...**\n\n#### \ud83d\udccc **Winner betting**\n* **random:** select a random dog and compare it to the actual winner\n* **win_odds:** select the dog with the lowest win_odds \n* **best draw:**  select the dog with the best draw\n\n    \n#### \ud83d\udccc **Place betting**\n* **random list:** select a list at random and compare it to actual placings\n* **by place_odds:** select the top 2\/3 doggs by place odds and compare it to actual placings\n* **by draw:** select the top 2\/3 doggs by draw and compare it to actual placings\n","01aec40e":"<h1 style=\"font-family:verdana;\"> <center> \ud83d\udcda Machine learning and the doggos<\/center> <\/h1>","4e4e6f76":"***","ce81196d":"***","bdffb2e0":"***","b5dc649a":"\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana;\">\n\n    \n##   \ud83d\udcda  **Performance evaluations...**\n* **Model scores:** compare confusion matricies\n* **Voting system:** we can set weights to a voting system using f1\n* **Betting simulation:** run the previous models through a round of betting simulation to see how they compare on the test set\n \n","3c502192":"***","d1ff36ef":"   ***","e51fffc3":"    \n## Doggo Racing in Australia\n\n\n### Project Overview \n\nThe main objective of this project is to demonstrate the analysis and model building process for greyhound racing predictions in Australia & NZ. \n","3e0e8628":"# 3.0 Exploratory Data Analysis (EDA)\n\nHere we're identifying how features correlate with eachother and the response variables, as well as which features may need engineering, scaling or transformations: \n* Descriptive statistics\n\n* Data visualizations","62013e6c":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana;\">\n\n##   \ud83d\udcda  **Algorithm selection...**\n \n#####  \n    \n#### \ud83d\udccc **Regression**\nWe try to learn a function  f(x)  given feature  x  to predict a real-valued  y\u2208\u211d .\n\n#####  \n    \n#### \ud83d\udccc **Classification**\nWe try to learn a function  f(x)  given feature  x  to predict a set of discrete integer label  y\u2208{1,2,...,N}  with  N -classes.\n\n#####  \n    \n#### \ud83d\udccc **Learning to Rank**\nWe try to learn a function  f(q,D) , given a query  q  and a relevant list of items  D , to predict the order (ranking) of all items within list.  The three major approaches to LTR are known as pointwise, pairwise, and listwise.\n\n##### \n    \n#### \ud83d\udccc **Learning approaches**\n* **Pointwise** approaches look at a single document at a time using classification or regression to discover the best ranking for individual results.\n\n* **Pairwise** approaches look at two documents together. They also use classification or regression \u2014 to decide which of the pair ranks higher.\n\n* **Listwise** approaches decide on the optimal ordering of an entire list of documents. Ground truth lists are identified, and the machine uses that data to rank its list. Listwise approaches use probability models to minimize the ordering error., They can get quite complex compared to the pointwise or pairwise approaches.","d14f0bcc":"## 6.4 Logistic Regression\n"}}