{"cell_type":{"444b4313":"code","d7f387d1":"code","ab2b182e":"code","470d1dca":"code","09202b0b":"code","0d18bad9":"code","a1427bce":"code","b08cc441":"code","5bfdda7f":"code","16e02e1e":"code","0a3943fa":"code","3d29ae5d":"code","b4cad6f4":"code","2e8bec9d":"code","a4951893":"code","9e87891a":"code","199f958b":"code","9683fffd":"code","d62ba92f":"code","bab3d5da":"code","33c77733":"code","c5f5ff1b":"code","d6f0004d":"code","f1540528":"code","fabca06b":"code","7cb94423":"code","647bee12":"code","d2938c43":"code","b2a2774e":"code","2f51ad9c":"code","96c0ac4d":"code","66cd256b":"code","ef079c5b":"code","15ddc10f":"code","d354976f":"code","0874452e":"code","1d80e5f1":"code","5ee73833":"code","cfb29206":"code","8d4fe991":"code","b3d054fa":"code","8977dc87":"code","5664326a":"code","0208954e":"code","8db4db7e":"code","6c6d386a":"code","42291b8f":"code","3a6a3bd5":"code","90019966":"code","ebfc48ca":"code","8f4b7942":"code","9e110844":"code","94952374":"code","bfb14d40":"code","b076d286":"code","8a27e539":"code","cd7155c5":"code","86b92af4":"markdown","dff1603a":"markdown","be5c5ff1":"markdown","b90606ae":"markdown","8cb9de86":"markdown"},"source":{"444b4313":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d7f387d1":"%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\n\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\nfrom scipy.special import boxcox1p\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.base import BaseEstimator,TransformerMixin\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score","ab2b182e":"df = pd.read_csv('..\/input\/xAPI-Edu-Data.csv')","470d1dca":"df.head()","09202b0b":"df.tail()","0d18bad9":"def DescriptiveStatistics(df):\n    print(\"No of rwos and columns information:\",df.shape)\n    print(\"\")\n    print(\"---\"*20)\n    print(\"\")\n    print(\"Columns:\")\n    print(\"\")\n    print(df.columns.values)\n    print(\"---\"*20)\n    print(\"\")\n    print(df.info())\n    print(\"---\"*20)\n    print(\"\")\n    print(df.describe())","a1427bce":"DescriptiveStatistics(df)","b08cc441":"def CheckMissingInfo(df):\n    print(df.isnull().sum())\n    print(\"---\"*20)\n    print(\"\")\n    df_na = (df.isnull().sum() \/ len(df)) * 100\n    df_na = df_na.drop(df_na[df_na == 0].index).sort_values(ascending=False)[:30]\n    missing_data = pd.DataFrame({'Missing Ratio' :df_na})\n    print(missing_data)","5bfdda7f":"CheckMissingInfo(df)","16e02e1e":"def GetColumnCount(df):\n    int_columns = [col for col in df.columns if(df[col].dtype != \"object\")]\n    print(\"No of integer type columns:\",len(int_columns))\n    print(int_columns)\n    print(\"\")\n    obj_columns = [col for col in df.columns if(df[col].dtype == \"object\")]\n    print(\"No of object type columns:\",len(obj_columns))\n    print(obj_columns)\n    return int_columns,obj_columns","0a3943fa":"int_columns,obj_columns = GetColumnCount(df)","3d29ae5d":"def GetCountPlots(df,obj_columns):\n    for col in obj_columns:\n        if(len(df[col].value_counts()) < 5):\n            plt.figure(figsize=(5,5))\n        else:\n            plt.figure(figsize=(12,6))\n        print(sns.countplot(x=col, data=df, palette=\"muted\"))\n        plt.show()","b4cad6f4":"GetCountPlots(df,obj_columns)","2e8bec9d":"def GetCardinality(df,obj_columns):\n    for col in obj_columns:\n        print(\"{0} :: {1}\".format(col,len(df[col].value_counts())))\n        \n        print(df[col].value_counts())\n        print(\"\")","a4951893":"GetCardinality(df,obj_columns)","9e87891a":"pd.crosstab(df['Class'],df['Topic'])","199f958b":"def GetCountPlots_with_hue(df,obj_columns,col_hue):\n    for col in obj_columns:\n        if(len(df[col].value_counts()) < 5):\n            plt.figure(figsize=(5,5))\n        else:\n            plt.figure(figsize=(12,6))\n        #print(sns.countplot(x=col, data=df, palette=\"muted\"))\n        sns.countplot(x=col,data = df, hue=col_hue,palette='bright')\n        plt.show()","9683fffd":"GetCountPlots_with_hue(df,obj_columns,'Class')","d62ba92f":"def GetBoxPlots(df,x_col):\n    for col in int_columns:\n        plt.figure(figsize=(5,5))\n        boxplot1 = sns.boxplot(x=x_col, y=col, data=df)\n        boxplot1 = sns.swarmplot(x=x_col, y=col, data=df, color=\".15\")\n        plt.show()","bab3d5da":"GetBoxPlots(df,'Class')","33c77733":"df['Failed'] = np.where(df['Class']=='L',1,0)","c5f5ff1b":"df['AbsBoolean'] = df['StudentAbsenceDays']\ndf['AbsBoolean'] = np.where(df['AbsBoolean'] == 'Under-7',0,1)\ndf['AbsBoolean'].groupby(df['Topic']).mean()","d6f0004d":"df.head()","f1540528":"df.info()","fabca06b":"def NumaricVariablesDistributions(df):\n    int_columns=df.columns[df.dtypes==int]\n    plt.figure(figsize=(10,7))\n    for i, column in enumerate(int_columns):\n        plt.subplot(3,2, i+1)\n        sns.distplot(df[column], label=column, bins=10, fit=norm)\n        plt.ylabel('Density');","7cb94423":"NumaricVariablesDistributions(df)","647bee12":"def ApplyBoxcoxTransformation(df,columns):\n    plt.figure(figsize=(10,7))\n    for i, column in enumerate(columns):\n        plt.subplot(2,2, i+1)\n        df[column]=boxcox1p(df[column], 0.3)\n        sns.distplot(df[column], label=column, bins=10, fit=norm)\n        plt.ylabel('Density')","d2938c43":"ApplyBoxcoxTransformation(df,['raisedhands', 'VisITedResources', 'AnnouncementsView', 'Discussion'])","b2a2774e":"df['raisedhands_bin']=np.where(df.raisedhands>df.raisedhands.mean(),1,0)\ndf['VisITedResources_bin']=np.where(df.VisITedResources>df.VisITedResources.mean(),1,0)","2f51ad9c":"GetBoxPlots(df,'Class')","96c0ac4d":"plt.figure(figsize=(12,8))\nsns.heatmap(df.corr(), annot=True, fmt='.1g', cmap='RdBu');","66cd256b":"sns.pairplot(df);","ef079c5b":"print('Percent of students\\' nationality - Kuwait or Jordan: {}'.format(\n            round(100*df.NationalITy.isin(['KW','Jordan']).sum()\/df.shape[0],2)))","15ddc10f":"target=df['Class']\ndf=df.drop('Class', axis=1)","d354976f":"df.head()","0874452e":"#Create new feature - type of topic (technical, language, other)\nTopic_types={'Math':'technic', 'IT':'technic','Science':'technic','Biology':'technic',\n 'Chemistry':'technic', 'Geology':'technic', 'Arabic':'language', 'English':'language',\n 'Spanish':'language','French':'language', 'Quran':'other' ,'History':'other'}\ndf['Topic_type']=df.Topic.map(Topic_types)","1d80e5f1":"df.head()","5ee73833":"int_columns,obj_columns = GetColumnCount(df)","cfb29206":"def ApplyScaling(df):\n    for column in ['raisedhands', 'VisITedResources', 'AnnouncementsView', 'Discussion']:\n        SS=StandardScaler().fit(df[[column]])\n        df[[column]]=SS.transform(df[[column]])","8d4fe991":"ApplyScaling(df)","b3d054fa":"df.head()","8977dc87":"int_columns,obj_columns = GetColumnCount(df)","5664326a":"def LabelEncoding(df):\n    for column in obj_columns:\n        #Binarize and LabelEncode\n        if ((df[column].value_counts().shape[0]==2) | (column=='StageID') | (column=='GradeID')):\n            le=LabelEncoder().fit(df[column])\n            df[column]=le.transform(df[column])\n    ","0208954e":"LabelEncoding(df)","8db4db7e":"df.head()","6c6d386a":"#One-hot encoding\ndf=pd.get_dummies(df)","42291b8f":"df.head()","3a6a3bd5":"df.columns","90019966":"from sklearn.metrics import make_scorer, accuracy_score,roc_auc_score,confusion_matrix, classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline","ebfc48ca":"x_train, x_test, y_train, y_test = train_test_split(df, target, test_size=0.20, random_state=42)","8f4b7942":"#using cross_val_score\nlogis = LogisticRegression()\nsvm = SVC()\nknn = KNeighborsClassifier()\ndTmodel = DecisionTreeClassifier()\nrForest = RandomForestClassifier()\ngrBoosting = GradientBoostingClassifier()\n    \nscores = cross_val_score(logis,x_train,y_train,cv=5)\nprint(\"Accuracy for logistic regresion: mean: {0:.2f} 2sd: {1:.2f}\".format(scores.mean(),scores.std() * 2))\nprint(\"Scores::\",scores)\nprint(\"\\n\")\n\nscores2 = cross_val_score(svm,x_train,y_train,cv=5)\nprint(\"Accuracy for SVM: mean: {0:.2f} 2sd: {1:.2f}\".format(scores2.mean(),scores2.std() * 2))\nprint(\"Scores::\",scores)\nprint(\"\\n\")\n\nscores3 = cross_val_score(knn,x_train,y_train,cv=5)\nprint(\"Accuracy for KNN: mean: {0:.2f} 2sd: {1:.2f}\".format(scores3.mean(),scores3.std() * 2))\nprint(\"Scores::\",scores)\nprint(\"\\n\")\n\nscores4 = cross_val_score(dTmodel,x_train,y_train,cv=5)\nprint(\"Accuracy for Decision Tree: mean: {0:.2f} 2sd: {1:.2f}\".format(scores4.mean(),scores4.std() * 2))\nprint(\"Scores::\",scores4)\nprint(\"\\n\")\n\nscores5 = cross_val_score(rForest,x_train,y_train,cv=5)\nprint(\"Accuracy for Random Forest: mean: {0:.2f} 2sd: {1:.2f}\".format(scores5.mean(),scores5.std() * 2))\nprint(\"Scores::\",scores5)\nprint(\"\\n\")\n\nscores6 = cross_val_score(grBoosting,x_train,y_train,cv=5)\nprint(\"Accuracy for Gradient Boosting: mean: {0:.2f} 2sd: {1:.2f}\".format(scores6.mean(),scores6.std() * 2))\nprint(\"Scores::\",scores6)\nprint(\"\\n\")","9e110844":"from sklearn.metrics import roc_auc_score\n\ndef roc_auc_score_multiclass(actual_class, pred_class, average = \"macro\"):\n\n  #creating a set of all the unique classes using the actual class list\n  unique_class = set(actual_class)\n  roc_auc_dict = {}\n  for per_class in unique_class:\n    #creating a list of all the classes except the current class \n    other_class = [x for x in unique_class if x != per_class]\n\n    #marking the current class as 1 and all other classes as 0\n    new_actual_class = [0 if x in other_class else 1 for x in actual_class]\n    new_pred_class = [0 if x in other_class else 1 for x in pred_class]\n\n    #using the sklearn metrics method to calculate the roc_auc_score\n    roc_auc = roc_auc_score(new_actual_class, new_pred_class, average = average)\n    roc_auc_dict[per_class] = roc_auc\n\n  return roc_auc_dict","94952374":"def modelling(model,model_name):\n    print(model)\n    print(\"\\n\")\n    model.fit(x_train, y_train)\n    preds=model.predict(x_test)\n    preds_proba=model.predict_proba(x_test)\n    print('Accuracy = {}'.format(100*round(accuracy_score(y_test,preds),2)))\n    print(classification_report(y_test, preds))\n    \n    print(\"\\n\")\n    print(model_name)\n    lr_roc_auc_multiclass = roc_auc_score_multiclass(y_test, preds)\n    print(\"AUC Score for each lable\")\n    print(lr_roc_auc_multiclass)\n    print(\"\\n\")\n    plt.figure(figsize=(7,5))\n    sns.heatmap(confusion_matrix(y_test,preds), annot=True, vmax=50)\n    plt.show()","bfb14d40":"modelling(LogisticRegression(),\"Logistic Regression\")","b076d286":"modelling(GradientBoostingClassifier(),\"Gradient Boosting\")","8a27e539":"# Grid search cross validation\n\ngrid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\nlogreg=LogisticRegression()\nlogreg_cv=GridSearchCV(logreg,grid,cv=10)\nlogreg_cv.fit(x_train,y_train)\n\nprint(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\nprint(\"accuracy :\",logreg_cv.best_score_)","cd7155c5":"modelling(LogisticRegression(C=1.0,penalty='l1'),\"Logistic Regression tuned\")","86b92af4":"# Hyper parameter tuning","dff1603a":"- Features doesn't have gaussian (normal) distribution.\n- As ML algorithms deal better with values, which are normally distributed, we need to transfrom them closer that view. BoxCox transformation will help us with it","be5c5ff1":"- It appears that no one failed Geology while students in IT, Chemistry, and Math had the highest probability of failing.\n\n- The boxplot analysis indicates that those who did well were more active in class, and the worst performers were the least active.\n\n- It is clear that the lowest performers rarely visited the course resources. The swarmplot shapes also indicates that the highest and lowest performers had the most consistent habits with respect to viewing the course resources. \n\n- It also appears that less students from all groups viewed course announcements, but there is still a clear pattern with viewing course announcements and how well the student performed.\n\n- The biggest visual trend can be seen in how frequently the student was absent. Over 90% of the students who did poorly were absent more than seven times, while almost none of the students who did well were absent more than seven times.","b90606ae":"Let's look at correlation between these features:\n- VisitedResources, RaisedHands and AnnouncementViews have medium correlation (0.5-0.7)","8cb9de86":"- However raisedhands and visitedresourses have double gaussian distribution. We can create new binary features for them, where 1 is when values more than its' average, and 0 - less. Then we can look how these features will improve model"}}