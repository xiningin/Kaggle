{"cell_type":{"60fa3a1d":"code","1010e86b":"code","d263e4ae":"code","bb3162ce":"code","883869c5":"code","f3deb5c9":"code","82eacc2c":"code","e2fd3138":"code","2d9063fb":"code","4882a4c0":"code","c92e6112":"code","768e9dd7":"code","d4302d17":"code","9f56f1f1":"code","065ea2b8":"code","ef2158b5":"code","adc87893":"code","58184fb2":"code","1d1b55b7":"code","5f9f8eac":"code","ec41cffb":"code","9157cd1b":"code","f2ba2a06":"code","203daf09":"code","0704a78a":"code","0dce9ec1":"code","c7c74ed3":"code","32bf3690":"code","26dcbf5a":"code","e0776026":"code","38a82df1":"code","dc73e2a7":"code","859ad542":"code","f159bc5e":"code","fd79f2f0":"code","7c7cb847":"code","496a8667":"code","f63c0a4a":"code","a0448172":"code","2b8a0987":"code","7a02dc8c":"code","474ecdc4":"code","da082d9c":"code","3cdfa665":"code","c1e7a372":"code","0d14f803":"code","53047c89":"code","04ef6463":"code","42c8f647":"code","e59c9619":"code","f89945b1":"code","2aeb4b4b":"code","b6832e4e":"code","e363c5c0":"code","3637f9b7":"code","84029e3b":"code","eff83080":"code","937c2a76":"code","06781e5c":"code","ed23b662":"code","f334c31c":"code","fda69339":"code","b5d7c795":"code","ee02d278":"code","64445f2a":"code","b5a7c727":"code","01ad4e54":"code","dd13408c":"code","6ca703a4":"code","48168362":"code","379e34e0":"code","a8b3e558":"code","17dadefa":"code","1fd7904c":"code","3e26be43":"code","d0d210a8":"code","c3cf4856":"code","ae2ffcc2":"code","9ce6046d":"code","c11eff93":"code","5a9d9151":"code","3bfec667":"code","cc8f4929":"code","3da8e3a1":"code","c7747e4e":"code","2854b93d":"code","4f87d222":"code","6704c1a2":"code","91ac5420":"code","73ff32fe":"code","c7ed4f37":"code","d32b3864":"code","fc8dfc88":"code","6e037717":"code","3526011a":"code","a57beeb2":"code","f97921ce":"code","a215624d":"code","01879f09":"code","f69f2b85":"code","d1884422":"code","d11e785c":"code","8282f652":"code","823b2b3d":"code","9847718f":"code","e3e0ee4d":"code","1a602872":"code","bfdcf1fe":"code","81895660":"code","169cef54":"code","fce42b69":"code","bc4c01e6":"code","d5d96c98":"code","99e45290":"code","cde8f9fa":"code","de7e993a":"code","4cc3a222":"code","b846648f":"code","4435ea1b":"code","180ddcf1":"code","e24108bb":"code","3e91e1e4":"code","fc28a4d0":"code","4ddca198":"code","9fa4db91":"code","5e694990":"code","e9ea79a7":"code","31069d5e":"code","7db23415":"code","52815b6a":"code","153a0986":"code","ab9313b9":"code","b2b7dc3f":"code","e066f15c":"code","d37f2180":"code","6bc2fc88":"code","408dc22a":"code","51800c5a":"code","4e046f32":"code","af3d9b73":"code","f0555c58":"code","82c5ba53":"code","0f51d096":"code","68baeb0c":"code","9fbc2430":"code","6fd4484b":"code","3f278c32":"markdown","c8b95dae":"markdown","d26426c3":"markdown","34693d71":"markdown","c568268e":"markdown","96832e47":"markdown","21428983":"markdown","8135aa22":"markdown","fce3c9ac":"markdown","621cc3bf":"markdown","ed2ef778":"markdown","7583f325":"markdown","03372e79":"markdown","5fc62dce":"markdown","d97266ed":"markdown","bef692f7":"markdown"},"source":{"60fa3a1d":"import pandas as pd\nimport numpy as np\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","1010e86b":"plt.rcParams[\"figure.figsize\"] = (10, 6) # (w, h)","d263e4ae":"ls ..\/input\/","bb3162ce":"data = pd.read_csv('..\/input\/train.csv', parse_dates = [0]) # convert to datetime object ","883869c5":"data.shape","f3deb5c9":"data.columns","82eacc2c":"# renaming columns to non pandas \"reserved\" object attributes (to avoid mistakes \/ confusion)\ndata.rename(columns={'datetime':'logged',\n                          'count':'cnt'}, \n                 inplace=True)","e2fd3138":"# ordinal value which represents different levels of precipitation. rename to more appropriate name \ndata.rename(columns=\n            {'weather':'precipitation'          \n            },\n            inplace=True)","2d9063fb":"data.head(20)","4882a4c0":"data.describe()","c92e6112":"#check for any missing values\ndata.isnull().values.any()","768e9dd7":"# nice!","d4302d17":"#however, windspeed has many zeros. will check that out later ","9f56f1f1":"# making sure a holiday is not a working day\nlen(data[((data.holiday & data.workingday) == 1)])","065ea2b8":"# todo: still there might be correlation between holiday and workingday. will check this out ","ef2158b5":"data.info()","adc87893":"'''\nlogged : datetime \nholiday : categorical\nworkingday : categorical\nprecipitation : ordinal . represents the \"level\" of precipitation\nseason : ordinal . one season is \"greater\" than another\ntemp : numerical\natemp : numerical\nhumidity : numerical\nwindspeed : numerical\ncasual : numerical\nregistered :numerical\ncnt : numerical\n'''","58184fb2":"data.info()","1d1b55b7":"dataOrig = data.copy() # deep copy\n#data = dataOrig.copy()","5f9f8eac":"colsToDrop = []","ec41cffb":"# unravel date time data and add them as as features \ndata['hour'] = data.logged.dt.hour # hour in day\ndata['day'] = data.logged.dt.day # day in month\ndata['dayofweek'] = data.logged.dt.dayofweek # day of week \ndata['month'] = data.logged.dt.month # month in year\ndata['year'] = data.logged.dt.year # year within the two year timespan ","9157cd1b":"# drop unraveled datetime object\ncolsToDrop.append('logged')","f2ba2a06":"data.head(20)","203daf09":"data.hist(figsize = (25,22), bins=25);","0704a78a":"for col in data.columns:\n    print(data[col].value_counts().head(20))\n    print()","0dce9ec1":"sns.boxplot(data['cnt']).set_title('Cnt distribution')","c7c74ed3":"# cnt has many outlier datapoints beyond the outer quartile ","32bf3690":"sns.boxplot(data['season'],data['cnt']).set_title('Cnt distribution across season')","26dcbf5a":"# season 1 (spring) has a significant drop in count ","e0776026":"sns.boxplot(data['precipitation'],data['cnt']).set_title('Cnt distribution across precipitation')","38a82df1":"# amount of rentals drop as it gets more rainy . ","dc73e2a7":"sns.boxplot(data['hour'],data['cnt']).set_title('Cnt distribution across hour')","859ad542":"# peak hours are during commuting hours: between 7-8 in the morning, and 17-18 in evening","f159bc5e":"dateCols = ['hour','day','dayofweek','month','year']\nnonNumCols = ['season','holiday', 'workingday', 'precipitation']\ncontinuousCols = ['temp','atemp','humidity','windspeed']\ntargetCols = ['casual','registered','cnt']","fd79f2f0":"for col in continuousCols:\n    sns.boxplot(col, data=data,orient='v') \n    plt.show()","7c7cb847":"'''\ntemp :  distributed normal\natemp : distributed normal\nwindspeed : has many outliers \nhumidity : few outliers \n'''","496a8667":"# windspeed count plot ","f63c0a4a":"sns.countplot(data['windspeed']) ","a0448172":"'''\nthere are many zero values\n\nlogically, if I were renting a bike, the wind levels won't matter so much (as long there are not tornado \/ hurricane winds)\nand it wont really wouldn't be part of my decision to rent a boke \n\nwill need to check for correlation to decide if this feature is kept or not \n\n'''\n","2b8a0987":"# bar plot with sum estimator \nfor col in dateCols + nonNumCols:\n    sns.barplot(x=col, y='cnt', data=data, estimator = sum)\n    plt.show()","7a02dc8c":"# bar plot with mean estimator\n# with mean can relatively compare between classes despite high imbalances\n\nfor col in dateCols + nonNumCols:\n    sns.barplot(x=col, y='cnt', data=data) #estimator = mean (default param)\n    plt.show()","474ecdc4":"'''\n1. summer months had the highest amount of rentals  \n2. cnt varies well with: hour, month, year, season, precipitation levels \n\n\ntodo: check if day in month is correlated to day in week \n'''","da082d9c":"fig=plt.figure()\nfig.show()\nax=fig.add_subplot(111)\nax.set_title('Weekday - Time of Day')\n\ndays = {0: 'sunday', 1:'monday', 2:'tuesday',3:'wednesday',4:'thursday',5:'friday',6:'saturday'}\n\nfor day in range(7): \n    datetimeGroup = data[data['dayofweek'] == day].groupby(['hour']).mean().reset_index() # reset index turns index back into col after groupby turned it into an index \n    ax.plot(datetimeGroup.hour,datetimeGroup.cnt, label = days[day])\n\nax.legend(loc=2)\nfig.show()","3cdfa665":"'''\nwe now see that the high usage during commuting hours is not during friday+saturday\n\nfriday and saturday had high usages during the morning-afternoon (10-16 oclock)\n'''","c1e7a372":"fig=plt.figure()\nfig.show()\nax=fig.add_subplot(111)\nax.set_title('Season - Time of Day')\n\nseasons = {0: 'spring', 1:'summer', 2:'fall',3:'winter'}\n\n\nfor season in range(4): \n    datetimeGroup = data[data['season'] == season].groupby(['hour']).mean().reset_index() # reset index turns index back into col after groupby turned it into an index \n    ax.plot(datetimeGroup.hour,datetimeGroup.cnt, label = seasons[season])\n\nax.legend(loc=2)\nfig.show()","0d14f803":"'''\nwe see the same  amounts of rental peaks during those certain hours of the day, just a less amount of rentals in the summer season\n'''","53047c89":"fig=plt.figure()\nfig.show()\nax=fig.add_subplot(111)\nax.set_title('Customer Type - Time of Day')\n\ncustomers = {0: 'casual', 1:'registered'}\n\nfor customer in range(len(customers)): \n    datetimeGroup = data.groupby(['hour']).mean().reset_index() # reset index turns index back into col after groupby turned it into an index \n    ax.plot(datetimeGroup.hour,datetimeGroup[customers[customer]], label = customers[customer])\n\nax.legend(loc=2)\nfig.show()","04ef6463":"# registered users alone are the ones who ride during the peak commuting hours, not casual users\n# specifally, the casual users ride on the off hours of the peak users.\n# could be that the registered users are registered since they use this only to commute, as the more ideal tiem to ride\n# seems to be the middle of the day\n\n#also, most riding is done during the second peak (evening)","42c8f647":"# 24 hour window ","e59c9619":"data['cnt'].rolling(24).mean().plot()\nplt.show()","f89945b1":"# can see the spikes which are the changes during the different days of the week as discussed . ","2aeb4b4b":"#zooming out ","b6832e4e":"# one month window","e363c5c0":"daysPerMonth = len(data.day.unique())","3637f9b7":"data['cnt'].rolling(24*daysPerMonth).mean().plot()\nplt.show()","84029e3b":"#each season (each 3 months)\n# https:\/\/www.google.com\/search?q=how+many+months+a+season&oq=how+many+months+a+season","eff83080":"#there are 19 days a month","937c2a76":"data['cnt'].rolling(24*daysPerMonth*3).mean().plot()\nplt.show()","06781e5c":"# as seen from the daily, monthly, and seasonal rolling windows, the count grows over each season of time","ed23b662":"# since season already correlates to the unraveled datetime features, no need to keep as a feature, it will already be incorporated ","f334c31c":"corrMatrix = data.corr(method='spearman')\n\nfig, ax = plt.subplots(figsize=(12,8))\nsns.heatmap(corrMatrix,square = False,annot =True,cmap='Spectral', ax=ax)\n\nplt.show()","fda69339":"# pearson\ncorrMatrix = data.corr(method='pearson')\nfig, ax = plt.subplots(figsize=(12,8))\nsns.heatmap(corrMatrix,square = False,annot =True,cmap='Spectral', ax=ax)\n\nplt.show()","b5d7c795":"# windspeed has a very low correlation with cnt\ncolsToDrop.append('windspeed')","ee02d278":"#logically there is a high correlation between month and season ","64445f2a":"colsToDrop.append('season')","b5a7c727":"# hifh corr between cnt, registered and casual. makes sense since they are leakage variables! ","01ad4e54":"(data['cnt'] - data['registered']).equals(data['casual'])","dd13408c":"#bingo!\n#  drop registered and casual\ncolsToDrop.append('registered')\ncolsToDrop.append('casual')","6ca703a4":"# temperature (both temp and atemp) has a corr with cnt. not very high  but still significant enough","48168362":"# humidiy has a neg corr with count. not very high  but still significant enough","379e34e0":"# temp is also hightly correlated with atemp","a8b3e558":"tempDiff = (data['atemp'] - data['temp'])","17dadefa":"print('mean: ' + str(tempDiff.mean()))\nprint('median: ' + str(tempDiff.median()))\nprint('mode: ' + str(tempDiff.mode()[0]))","1fd7904c":"tempDiff.hist(bins=50)","3e26be43":"tempDiff.plot()","d0d210a8":"tempDiff.plot(kind='box')","c3cf4856":"data['atemp'].max()","ae2ffcc2":"data['temp'].max()","9ce6046d":"# temp diff does have a few outliers due to atemp having a few spikes, yet temp and atemp are highly correleated and very close in value. so will simply just drop one of them\n# decided to drop atemp due to the outliers  ","c11eff93":"colsToDrop.append('atemp')","5a9d9151":"#highly neg correlated with day of week and workingday\n# working day: whether the day is neither a weekend nor holiday. that is already derived from day of week and holiday\ncolsToDrop.append('workingday')","3bfec667":"colsToDrop","cc8f4929":"data.columns","3da8e3a1":"#todo: if have time, explore methods to remove corrlelated dimensions such as pca, knn+pca, lasso, tsne...","c7747e4e":"# setting the target variable: given an order, predict the amount of rentals there will be that day","2854b93d":"data.head()","4f87d222":"years = data.logged.dt.year.unique()\nmonths = data.logged.dt.month.unique()\ndays = data.logged.dt.day.unique()\n","6704c1a2":"years","91ac5420":"months","73ff32fe":"days","c7ed4f37":"data.head(10)","d32b3864":"#dataOrig = data.copy()","fc8dfc88":"# alrady accomplished this with the datagrouping \/ aggregation \n\n'''\ndata[\"cntDay\"] = np.nan\n\nfor y in years:\n    for m in months:\n        for d in days:\n            selection = (data['year'] == y) & (data['month'] == m) & (data['day'] == d)\n            data.loc[selection, 'cntDay'] = (data[ selection ])['cnt'].sum()\n  '''          ","6e037717":"#making sure data is sorted properly \ndata.sort_index(ascending=True).equals(data)","3526011a":"dataset = data.copy()","a57beeb2":"dataset.head(2)","f97921ce":"dataset.columns","a215624d":"colsToDrop","01879f09":"dataset = dataset.drop(colsToDrop,axis=1)","f69f2b85":"dataset.columns","d1884422":"dataset = dataset.groupby(['year','month','day']).agg({'precipitation':'mean','temp':'mean','holiday':'mean','humidity':'mean','cnt':'sum'}).reset_index()","d11e785c":"dataset.head()","8282f652":"dataset['precipitation'] = round(dataset['precipitation'])\ndataset['humidity'] = round(dataset['humidity'])\ndataset['temp'] = round(dataset['temp'],2)","823b2b3d":"dataset.head(10)","9847718f":"dataset['tomorrowCnt'] = dataset['cnt'].shift(-1)","e3e0ee4d":"dataset.head(10)","1a602872":"#making sure the only thing na is the last col due to the upshift ","bfdcf1fe":"dataset.isna().sum() #","81895660":"dataset = dataset.dropna()","169cef54":"dataset.isna().sum() #","fce42b69":"dataset.head(20)","bc4c01e6":"target = dataset.tomorrowCnt","d5d96c98":"tomorrowCntMean = target.mean()","99e45290":"tomorrowCntMean","cde8f9fa":"dataset = dataset.drop(['cnt','tomorrowCnt'],axis=1)","de7e993a":"dataset.columns","4cc3a222":"dataset.head()","b846648f":"from sklearn import preprocessing\nmin_max_scaler = preprocessing.MinMaxScaler()","4435ea1b":"dataBeforeNorm = dataset.copy()","180ddcf1":"#dataset = dataBeforeNorm.copy()","e24108bb":"#commenting out since got better results not scaling \n\n#dataset = pd.DataFrame(min_max_scaler.fit_transform(dataset), columns=dataset.columns, index=dataset.index)\n#dataset.head()  ","3e91e1e4":"for col in dataset.columns:\n    print(dataset[col].value_counts().head())\n    print()","fc28a4d0":"from sklearn.model_selection import train_test_split","4ddca198":"import xgboost as xgb","9fa4db91":"# we should split this into train, validation, test but i understood from the problem to only split it into train and test ","5e694990":"#For training, use 90% of the data and 10% for test\nX_train, X_test, y_train, y_test  = train_test_split(dataset, target, test_size=0.1, random_state=26)\ndm_train = xgb.DMatrix(X_train, label=y_train)\ndm_test = xgb.DMatrix(X_test, label=y_test)","e9ea79a7":"'''\ndef evalerror(preds, dtrain):\n    labels = dtrain.get_label()\n    assert len(preds) == len(labels)\n    #labels = labels.tolist()\n    #preds = preds.tolist()\n\n    return 'error', mean_squared_error(preds)\/tomorrowCntMean > 1\n'''","31069d5e":"clParams = {}\nclParams['objective'] = 'reg:linear'\n#clParams['eval_metric'] = 'mae' # 'error'\n\nclParams['eta'] = 0.1\nclParams['max_depth'] = 8\n\nwatchlist = [(dm_train, 'train'), (dm_test, 'test')]\n\n#cl = xgb.train(clParams, dm_train, 10000, watchlist, feval = evalerror, early_stopping_rounds=500, maximize=False, verbose_eval=10)\ncl = xgb.train(clParams, dm_train, 10000,  watchlist, early_stopping_rounds=500, maximize=False, verbose_eval=10)","7db23415":"xgb.plot_importance(cl)","52815b6a":"# Prediction on test ","153a0986":"def MeasurePerf(preds,y_test):\n    \n    predsMse = mean_squared_error(y_test,preds)\n    meanPreds = np.full(len(preds),tomorrowCntMean)\n    meanMse = mean_squared_error(y_test,meanPreds)\n    \n    res = predsMse \/ meanMse\n    print('predMse\/meanMse: '+ str(res))\n    \n    if (res>=1): # if it equals 1, why train a whole model when u can use average \n        print('\\tbetter to use average')\n    else:\n        print('\\tgood prediction')\n        \n    \n    ","ab9313b9":"def CalculateCompanyGain(ordered,actual): \n    \n    extraOrders =  ordered.sum() - actual.sum()\n    print(\"extra orders: \" + str(extraOrders))\n    \n    gain = actual.sum()*15 - ordered.sum()*10  + (extraOrders<0)*extraOrders*100 # supplier charges 10 per vehicle and registered pays 15\n\n    gain = round(gain)\n    print('gain: ' + str(gain))    \n    return gain","b2b7dc3f":"from sklearn.metrics import mean_squared_error","e066f15c":"preds = cl.predict(dm_test)","d37f2180":"MeasurePerf(preds,y_test)","6bc2fc88":"# company gains with forcasted \ngainsWithForcasted = CalculateCompanyGain(preds,y_test)","408dc22a":"# company gains with mean\ngainsWithAverage = CalculateCompanyGain(np.full(len(preds),tomorrowCntMean),y_test)","51800c5a":"# model doesn't seem to put enough of a penalty on ordering less than required. this is bad since it's more expensive to lose\n# a customer then to order extra. the loss function is root mean squared error which substracts the difference between them and squares it\n# this gives it equal penalties to under and over ordering. ","4e046f32":"# did we make more money with forcased or average?\n\ngainsWithForcasted - gainsWithAverage > 0","af3d9b73":"# sample test ","f0555c58":"sampleX = X_train.head(1)\nsampleY = y_train.head(1)","82c5ba53":"dm_sample = xgb.DMatrix(sampleX, label=sampleY)","0f51d096":"samplePreds = cl.predict(dm_sample)","68baeb0c":"MeasurePerf(samplePreds,sampleY)","9fbc2430":"'''\n\nI would have some threshold defined which marks when we are running low on bikes.\n\nOnce that threshold is hit, if a registered customer orders, i will supply him. \n\nHowever if a casual user orders, I would need to check the time in the day.\nIf it's past the second commuting peak, I will grant the casual customer his order.\nIf it's before the second commuting peak, I will deny the casual customer's order, since I am expecting a large amount of orders\nfrom the registered customers during the next peak. \n(Although the casual users pay more money and their peak time is between the two commuting peaks,\nbased of the graphs it seems the high volumes of the registered users during their peak hours can generate a higher profit.)\n\nOptionally, more tresholds can be set defining the policy of before the first commuting peak or in between peaks\n'''","6fd4484b":"\n'''\n\nI would use a custom loss function instead of mse. This was trained on rmse which \ngives an loss of an equal balance to more bikes predicted or less bikes predicted\n\nInstead, we need to give a higher penalty loss when less than the needed amount of bikes are perdicted, \n(since being short of bikes causes the company to suffer an expensive fee of losing customers),\nand will penalize less for ordering too many bikes\n\nI'd try a loss of: (actual.sum()*15 - forcasted.sum()*10  + (extraOrders<0)*extraOrders*100 )^2\nit is squared so that it will be convex function and have a min \n\n\n'''","3f278c32":"##  Data Analysis","c8b95dae":"### Classifier","d26426c3":"#### checking for improvements in count over time","34693d71":"# A: Max capacity to set for each day","c568268e":"#### exploring the correlations","96832e47":"#### check for outliers ","21428983":"# Feature Reduction","8135aa22":"#### Cnt Along Time","fce3c9ac":"# B: how to act an each hour","621cc3bf":"### normalize data with min max ","ed2ef778":"# C: improvements:","7583f325":"## Getting to know the data","03372e79":"# Prediction","5fc62dce":"### multicollinearity","d97266ed":"#### frequency counts for each feature","bef692f7":"#### view distribution of continuous features using boxplot"}}