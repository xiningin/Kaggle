{"cell_type":{"59f00b56":"code","c78e9d0d":"code","d82fa529":"code","1b16eecf":"code","713d884e":"code","9e7f8dcd":"code","5755f4de":"code","8a5b7cef":"code","d5014103":"code","7195be16":"code","b7e49745":"code","a9a1d765":"code","bcf3ea9b":"code","0c06af62":"code","40c68169":"code","1517f846":"code","8bf5aa43":"code","8091af55":"code","ea7c31b4":"code","807be6dc":"code","bc129940":"code","f04cb384":"code","5e9c6e87":"code","37bf75b7":"code","45787471":"code","436f7388":"code","33da5791":"code","0eb80cd5":"code","2c30683a":"code","de272246":"code","5a447ad1":"code","84e9a9e2":"code","d34e9637":"code","505c547e":"code","16fde100":"markdown","635daff3":"markdown","394bbdf0":"markdown","0d843ed7":"markdown","966267d6":"markdown","2ab8ec7c":"markdown","f804022e":"markdown","bdb72669":"markdown","fdb69860":"markdown","3f948ff8":"markdown","9e6b1a61":"markdown","b6667118":"markdown","98d57946":"markdown","88d181eb":"markdown","65df9d3e":"markdown","bde38b3d":"markdown","36010e0f":"markdown","ff0b8ad4":"markdown","dd3998bb":"markdown","20d0662a":"markdown","4acfa11d":"markdown","0a2d7486":"markdown","31e02a4b":"markdown"},"source":{"59f00b56":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nimport torch\nimport torchvision\nfrom PIL import Image","c78e9d0d":"imagesFilepath = \"..\/input\/flickr8k\/Flickr_Data\/Flickr_Data\/Images\"\nlatent_representation_filepath = \"\/kaggle\/working\/flickr8k_latent.csv\"\nuseCuda = torch.cuda.is_available()\n# Load the googlenet pre-trained neural network\nencoder = torchvision.models.googlenet(pretrained=True)\nencoder.eval()\n# Replace the last linear layer (called 'fc') with a dummy identity layer.\n# By doing this, we can output the image latent representation\n# without messing with the layers naming convention\nencoder.fc = torch.nn.Identity()\nnumber_of_latent_variables = 1024 # Can be found by examining googlenet architecture\nif useCuda:\n    encoder = encoder.cuda()","d82fa529":"preprocess = torchvision.transforms.Compose([\n            torchvision.transforms.Resize(256),\n            torchvision.transforms.CenterCrop(224),\n            torchvision.transforms.ToTensor(),\n            torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        ","1b16eecf":"# Create a list of jpg image filepaths\nfound_jpg_filepaths = []\ndirectory_contents = os.listdir(imagesFilepath)\nfor content in directory_contents:\n    filepath = os.path.join(imagesFilepath, content)\n    if filepath.endswith('.jpg'):\n        found_jpg_filepaths.append(filepath)","713d884e":"if not os.path.isfile(latent_representation_filepath): # No need to run it more than once\n    with open(latent_representation_filepath, 'w+') as output_file:\n        # Write the file header\n        output_file.write('filepath')\n        for variableNdx in range(number_of_latent_variables):\n            output_file.write(\",v{}\".format(variableNdx))\n        output_file.write(\"\\n\")\n\n        # Loop through the images\n        for index, image_filepath in enumerate(found_jpg_filepaths):\n            output_file.write(image_filepath)\n            image = Image.open(image_filepath)\n            inputTsr = preprocess(image).unsqueeze(0) # Preprocess and add a dummy mini-batch dimension\n            if useCuda:\n                inputTsr = inputTsr.cuda()\n            with torch.no_grad():\n                latentTsr = encoder(inputTsr)[0] # Run a forward pass through the encoder and get rid of the dummy mini-batch dimension\n                for valueNdx in range(latentTsr.shape[-1]):\n                    output_file.write(\",{}\".format(latentTsr[valueNdx].item()))\n                output_file.write(\"\\n\")\n            if index % 300 == 0:\n                print (\"{}\/{}\".format(index, len(found_jpg_filepaths)), end=\" \", flush=True)\n","9e7f8dcd":"latent_representationDf = pd.read_csv(latent_representation_filepath)","5755f4de":"descriptions_filepath = '..\/input\/flickr8k\/Flickr_Data\/Flickr_Data\/Flickr_TextData\/Flickr8k.token.txt'\n!head $descriptions_filepath\nsample_filename = os.path.basename(found_jpg_filepaths[0])","8a5b7cef":"sample_image_filename = '1000268201_693b08cb0e.jpg'\nimport IPython.display\nIPython.display.Image(os.path.join(imagesFilepath, sample_image_filename))","d5014103":"training_images_filepath = '..\/input\/flickr8k\/Flickr_Data\/Flickr_Data\/Flickr_TextData\/Flickr_8k.trainImages.txt'\nvalidation_images_filepath = '..\/input\/flickr8k\/Flickr_Data\/Flickr_Data\/Flickr_TextData\/Flickr_8k.devImages.txt'\n!head $training_images_filepath","7195be16":"import en_core_web_sm\nimport re\nimport string\n\nnlp = en_core_web_sm.load()\n\ndef Tokenize(text, nlp):\n    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')  # Remove punctuation and numbers\n    nopunct = regex.sub(\" \", text.lower())\n    tokens = [token.text for token in nlp.tokenizer(nopunct) if not token.text.isspace()]\n    return tokens\n\ndef TrainDescriptionTokens(descriptions_filepath, train_images_list, nlp):\n    number_of_train_lines = 0\n    number_of_not_train_lines = 0\n    train_token_to_occurrences_dict = {}\n    with open(descriptions_filepath, 'r') as descriptions_file:\n        for line in descriptions_file:\n            line = line.strip()\n            sharp_index = line.find('#')\n            if sharp_index == -1:\n                raise ValueError(\"TrainDescriptionTokens(): Could not find '#' in line '{}'\".format(line))\n            image_filename = line[:sharp_index]\n            description = line[sharp_index + 3: ]\n            if image_filename in train_images_list:\n                # Tokenize the description\n                tokens = Tokenize(description, nlp)\n                for token in tokens:\n                    if token in train_token_to_occurrences_dict:\n                        train_token_to_occurrences_dict[token] += 1\n                    else:\n                        train_token_to_occurrences_dict[token] = 1\n                number_of_train_lines += 1\n            else:\n                number_of_not_train_lines += 1\n    print(\"TrainDescriptionTokens(): number_of_train_lines = {}; number_of_not_train_lines = {}\".format(number_of_train_lines, number_of_not_train_lines))\n    return train_token_to_occurrences_dict\n\nwith open(training_images_filepath, 'r') as train_images_file:\n    train_images_list = [line.strip() for line in train_images_file]\ntrain_token_to_occurrences_dict = TrainDescriptionTokens(descriptions_filepath,\n                                                             train_images_list, nlp)","b7e49745":"print(\"Before filtering the single occurrences, len(train_token_to_occurrences_dict) = {}\".format(len(train_token_to_occurrences_dict)))\nsingle_occurrence_words = []\nfor word, occurrences in train_token_to_occurrences_dict.items():\n    if occurrences < 2:\n        single_occurrence_words.append(word)\nfor word in single_occurrence_words:\n    train_token_to_occurrences_dict.pop(word)\nprint(\"After filtering the single occurrences, len(train_token_to_occurrences_dict) = {}\".format(len(train_token_to_occurrences_dict)))","a9a1d765":"vocabulary_filepath = '\/kaggle\/working\/vocabulary.csv'\nsorted_tokens = sorted(train_token_to_occurrences_dict.items(),\n                           key=lambda x: x[1], reverse=True) # Cf. https:\/\/careerkarma.com\/blog\/python-sort-a-dictionary-by-value\/\nsorted_tokens = [('ENDOFSEQ', 0), ('UNKNOWN', 0), ('NOTSET', 0)] + sorted_tokens\n\nwith open(vocabulary_filepath, 'w+') as output_file:\n    output_file.write(\"index,word,frequency\\n\")\n    for index, token in enumerate(sorted_tokens):\n        output_file.write(\"{},{},{}\\n\".format(index, token[0], token[1]))","bcf3ea9b":"def LoadVocabulary(vocabularyFilepath):\n    word_to_index_dict = {}\n    index_to_word_dict = {}\n    vocabDf = pd.read_csv(vocabularyFilepath)\n    for i, row in vocabDf.iterrows():\n        index = row['index']\n        word = row['word']\n        word_to_index_dict[word] = index\n        index_to_word_dict[index] = word\n    return word_to_index_dict, index_to_word_dict\n\nword_to_index_dict, index_to_word_dict = LoadVocabulary(vocabulary_filepath)\nprint (\"word_to_index_dict['apple'] = {}\".format(word_to_index_dict['apple']))\n\ndef ConvertTokensListToIndices(tokens, word_to_index_dict, maximum_length):\n    indices = [word_to_index_dict['NOTSET']] * maximum_length\n    for tokenNdx, token in enumerate(tokens):\n        index = word_to_index_dict.get(token, word_to_index_dict['UNKNOWN']) # If the word is not in the dictionary, fall back to 'UNKOWN'\n        indices[tokenNdx] = index\n    if len(tokens) < maximum_length:\n        indices[len(tokens)] = word_to_index_dict['ENDOFSEQ']\n    return indices\n\npretokenized_descriptions_filepath = '\/kaggle\/working\/tokenized_descriptions.csv'\ndescription_maximum_length = 40\nwith open(pretokenized_descriptions_filepath, 'w+') as outputFile:\n    # Write the header\n    outputFile.write(\"image\")\n    for wordNdx in range(description_maximum_length):\n        outputFile.write(\",w{}\".format(wordNdx))\n    outputFile.write(\"\\n\")\n\n    # Loop through the lines of the descriptions file\n    with open(descriptions_filepath, 'r') as descriptionsFile:\n        for line in descriptionsFile:\n            line = line.strip()\n            sharp_index = line.find('#')\n            if sharp_index == -1:\n                raise ValueError(\"Could not find '#' in line '{}'\".format(line))\n            image_filename = line[:sharp_index]\n            description = line[sharp_index + 3:]\n            # Tokenize the description\n            tokens = Tokenize(description, nlp)\n\n            # Convert the list of tokens to a list of indices\n            indices = ConvertTokensListToIndices(tokens,\n                                                 word_to_index_dict,\n                                                 description_maximum_length)\n            outputFile.write(image_filename)\n            for indexNdx in range(len(indices)):\n                outputFile.write(\",{}\".format(indices[indexNdx]))\n            outputFile.write(\"\\n\")","0c06af62":"tokenized_descriptionsDf = pd.read_csv(pretokenized_descriptions_filepath)\ntokenized_descriptionsDf.head()","40c68169":"training_description_indices = []\nfor t in tokenized_descriptionsDf.itertuples():\n    filename = t[1]\n    if filename in train_images_list:\n        training_description_indices.append(list(t[2:]))\nprint (\"training_description_indices[0:5] = {}\".format(training_description_indices[0:5]))","1517f846":"from torch.utils.data import Dataset, DataLoader\nimport random\n\nclass ContextToWordDataset(Dataset):\n    def __init__(self,\n                 training_descriptions_indices,\n                 index_to_word_dict,\n                 word_to_index_dict,\n                 contextLength):\n        self.training_descriptions_indices = training_descriptions_indices\n        self.index_to_word_dict = index_to_word_dict\n        self.word_to_index_dict = word_to_index_dict\n        self.contextLength = contextLength\n\n    def __len__(self):\n        return len(self.training_descriptions_indices)\n\n    def __getitem__(self, idx):\n        description_indices = self.training_descriptions_indices[idx]\n        # Randomly select a target word\n        last_acceptable_center_index = len(description_indices) - 1\n        if self.word_to_index_dict['ENDOFSEQ'] in description_indices:\n            for position, index in enumerate(description_indices):\n                if index == self.word_to_index_dict['ENDOFSEQ'] in description_indices:\n                    last_acceptable_center_index = position        \n        targetNdx = random.choice(range(last_acceptable_center_index + 1))\n        # Create a Long tensor with dim (2 * context_length)\n        description_indicesTsr = torch.ones((2 * self.contextLength)).long() * self.word_to_index_dict['NOTSET']\n\n        runningNdx = targetNdx - int(self.contextLength)\n        counter = 0\n        while counter < 2 * self.contextLength:\n            if runningNdx != targetNdx:\n                if runningNdx >= 0 and runningNdx < len(description_indices):\n                    description_indicesTsr[counter] = description_indices[runningNdx]\n                counter += 1\n            runningNdx += 1\n        return (description_indicesTsr, description_indices[targetNdx])\n    \ntrain_dataset = ContextToWordDataset(training_description_indices,\n                 index_to_word_dict,\n                 word_to_index_dict,\n                 contextLength=3)","8bf5aa43":"sample_data_0 = train_dataset[0]\nsample_words_0 = [index_to_word_dict[i] for i in sample_data_0[0].tolist()]\ncenter_word_0 = index_to_word_dict[sample_data_0[1]]\nprint (\"sample_words_0 = {}; center_word_0 = {}\".format(sample_words_0, center_word_0))\nsample_data_1 = train_dataset[1]\nsample_words_1 = [index_to_word_dict[i] for i in sample_data_1[0].tolist()]\ncenter_word_1 = index_to_word_dict[sample_data_1[1]]\nprint (\"sample_words_1 = {}; center_word_1 = {}\".format(sample_words_1, center_word_1))","8091af55":"class CenterWordPredictor(torch.nn.Module):\n    def __init__(self, vocabulary_size, embedding_dimension):\n        super(CenterWordPredictor, self).__init__()\n        self.embedding = torch.nn.Embedding(vocabulary_size, embedding_dimension)\n        self.decoderLinear = torch.nn.Linear(embedding_dimension, vocabulary_size)\n\n    def forward(self, contextTsr):\n        # contextTsr.shape = (N, context_length), contextTsr.dtype = torch.int64\n        embedding = self.embedding(contextTsr)  # (N, context_length, embedding_dimension)\n        # Average over context words: (N, context_length, embedding_dimension) -> (N, embedding_dimension)\n        embedding = torch.mean(embedding, dim=1)\n\n        # Decoding\n        outputTsr = self.decoderLinear(embedding)\n        return outputTsr\n    \nembedding_dimension = 128\nword_embedder = CenterWordPredictor(len(word_to_index_dict), embedding_dimension)\nif useCuda:\n    word_embedder = word_embedder.cuda()","ea7c31b4":"word_embedder_parameters = filter(lambda p: p.requires_grad, word_embedder.parameters())\noptimizer = torch.optim.Adam(word_embedder_parameters, lr=0.0001)\nlossFcn = torch.nn.CrossEntropyLoss()\ntrain_dataLoader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\nfor epoch in range(1, 1000):\n    word_embedder.train()\n    loss_sum = 0.0\n    number_of_batches = 0\n    for (description_indicesTsr, target_center_word_ndx) in train_dataLoader:\n        if number_of_batches % 20 == 1:\n            print (\".\", end=\"\", flush=True)\n        if useCuda:\n            description_indicesTsr = description_indicesTsr.cuda()\n            target_center_word_ndx = target_center_word_ndx.cuda()\n        predicted_center_word_ndx = word_embedder(description_indicesTsr)\n        optimizer.zero_grad()\n        loss = lossFcn(predicted_center_word_ndx, target_center_word_ndx)\n        loss.backward()\n        optimizer.step()\n        loss_sum += loss.item()\n        number_of_batches += 1\n    train_loss = loss_sum\/number_of_batches\n    print (\"\\nepoch {}: train_loss = {}\".format(epoch, train_loss))","807be6dc":"words2vec_dictionary_filepath = '\/kaggle\/working\/words2vec.csv'\nwith open(words2vec_dictionary_filepath, 'w+') as word2vecFile:\n        # Write the header\n        word2vecFile.write(\"word\")\n        for embeddingNdx in range(embedding_dimension):\n            word2vecFile.write(\",e{}\".format(embeddingNdx))\n        word2vecFile.write(\"\\n\")\n\n        for index, word in index_to_word_dict.items():\n            wordEmbeddingList = word_embedder.embedding.weight[index].tolist()\n            word2vecFile.write(word)\n            for coef in wordEmbeddingList:\n                word2vecFile.write(\",{}\".format(str(coef)))\n            word2vecFile.write(\"\\n\")","bc129940":"word2vecDf = pd.read_csv(words2vec_dictionary_filepath)\nword_to_embedding_dict = {word2vecDf.iloc[i]['word'] : word2vecDf.loc[i, 'e0': 'e{}'.format(embedding_dimension - 1)].tolist()\n                              for i in range(len(word2vecDf))}","f04cb384":"print (\"word_to_embedding_dict['dog'] = {}\".format(word_to_embedding_dict['dog']))\nprint()\nprint (\"word_to_embedding_dict['dance'] = {}\".format(word_to_embedding_dict['dance']))","5e9c6e87":"sample_image_filename = '99679241_adc853a5c0.jpg'\nIPython.display.Image(os.path.join(imagesFilepath, sample_image_filename))","37bf75b7":"class CaptionGenerationDataset(Dataset):\n    def __init__(self,\n                 image_filenames,\n                 image_filename_to_latent_variables,\n                 image_filename_to_tokenized_descriptions_list,\n                 endOfSeqIndex,\n                 notSetIndex,\n                 vocabulary_size,\n                 index_to_word_dict,\n                 word_to_embedding_dict\n                 ):\n        self.image_filenames = image_filenames\n        self.image_filename_to_latent_variables = image_filename_to_latent_variables\n        self.image_filename_to_tokenized_descriptions_list = image_filename_to_tokenized_descriptions_list\n        self.endOfSeqIndex = endOfSeqIndex\n        self.notSetIndex = notSetIndex\n        self.vocabulary_size = vocabulary_size\n        self.index_to_word_dict = index_to_word_dict\n        self.word_to_embedding_dict = word_to_embedding_dict\n\n    def __len__(self):\n        return len(self.image_filenames)\n\n    def __getitem__(self, idx):\n        if idx >= len(self.image_filenames):\n            raise IndexError(\"CaptionGenerationDataset.__getitem__(): Index {} is greater than the number of images ({})\".format(idx, len(self.image_filenames)))\n        filename = self.image_filenames[idx]\n        latent_variables = self.image_filename_to_latent_variables[filename]\n        # Build a tensor\n        latent_variablesTsr = torch.zeros(len(latent_variables))\n        for i in range(len(latent_variables)):\n            latent_variablesTsr[i] = latent_variables[i]\n        # Randomly choose one of the descriptions\n        description = random.choice(self.image_filename_to_tokenized_descriptions_list[filename])\n        lastIndex = self.IndexOfEndOfSeq(description)\n        chopIndex = random.randint(0, lastIndex) - 1 # The last index of the chopped description\n        choppedDescription = [self.notSetIndex] * len(description)\n        if chopIndex >= 0:\n            for i in range(chopIndex + 1):\n                choppedDescription[i] = description[i]\n\n        embedding_dim = len(self.word_to_embedding_dict[ self.index_to_word_dict[0] ]) # Length of the embedding of the 1st word\n        embeddedChoppedDescriptionTsr = torch.zeros((len(description), embedding_dim))\n        for wordPosn in range(len(choppedDescription)):\n            embedding = self.word_to_embedding_dict[ self.index_to_word_dict[choppedDescription[wordPosn]]]\n            embeddedChoppedDescriptionTsr[wordPosn] = torch.tensor(embedding)\n\n        next_word_index = description[chopIndex + 1]\n\n        return ( (latent_variablesTsr, embeddedChoppedDescriptionTsr), next_word_index)\n\n    def IndexOfEndOfSeq(self, description_list):\n        foundIndex = -1\n        for candidateNdx in range(len(description_list)):\n            if description_list[candidateNdx] == self.endOfSeqIndex:\n                foundIndex = candidateNdx\n                break\n        if foundIndex == -1:\n            return len(description_list) - 1\n        else:\n            return foundIndex\n        \ndef ImageFilenameToLatentVariables(latent_varDf):\n    image_filename_to_latent_variables = {}\n    for i, row in latent_varDf.iterrows():\n        filepath = latent_varDf.iloc[i][0]\n        filename = os.path.basename(filepath)\n        latent_variables = list(latent_varDf.iloc[i][1:])\n        image_filename_to_latent_variables[filename] = latent_variables\n    return image_filename_to_latent_variables\n\nimage_filename_to_latent_variables = ImageFilenameToLatentVariables(latent_representationDf)\n\ndef ImageFilenameToTokenizedDescriptionsList(descriptionsDF):\n    image_filename_to_tokenized_description = {}\n    for i, row in descriptionsDF.iterrows():\n        filename = descriptionsDF.iloc[i][0]\n        tokenized_description = list(descriptionsDF.iloc[i][1:])\n        if filename in image_filename_to_tokenized_description:\n            image_filename_to_tokenized_description[filename].append(tokenized_description)\n        else:\n            image_filename_to_tokenized_description[filename] = [tokenized_description]\n    return image_filename_to_tokenized_description\n\nimage_filename_to_tokenized_descriptions_list = ImageFilenameToTokenizedDescriptionsList(\ntokenized_descriptionsDf)\n\nlstm_train_dataset = CaptionGenerationDataset(\n        train_images_list,\n        image_filename_to_latent_variables,\n        image_filename_to_tokenized_descriptions_list,\n        word_to_index_dict['ENDOFSEQ'],\n        word_to_index_dict['NOTSET'],\n        len(word_to_index_dict),\n        index_to_word_dict,\n        word_to_embedding_dict\n    )\n","45787471":"with open(validation_images_filepath, 'r') as valid_images_file:\n    validation_images_list = [line.strip() for line in valid_images_file]\nlstm_validation_dataset = CaptionGenerationDataset(\n        validation_images_list,\n        image_filename_to_latent_variables,\n        image_filename_to_tokenized_descriptions_list,\n        word_to_index_dict['ENDOFSEQ'],\n        word_to_index_dict['NOTSET'],\n        len(word_to_index_dict),\n        index_to_word_dict,\n        word_to_embedding_dict\n    )","436f7388":"class LSTM_fixed_embedding(torch.nn.Module):\n    def __init__(self, embedding_dim, lstm_hidden_dim,\n                 num_lstm_layers, image_latent_dim,\n                 vocab_size,\n                 dropoutProportion=0.5):\n        super(LSTM_fixed_embedding, self).__init__()\n        self.embedding_dim = embedding_dim\n        self.lstm = torch.nn.LSTM(embedding_dim, lstm_hidden_dim, num_lstm_layers,\n                                  batch_first=True)\n        self.dropout = torch.nn.Dropout(dropoutProportion)\n        self.linear = torch.nn.Linear(lstm_hidden_dim + image_latent_dim, vocab_size)\n\n\n    def forward(self, image_latentTsr, embeddedChoppedDescriptionTsr):\n        # image_latentTsr.shape = (N, image_latent_dim)\n        # embeddedChoppedDescriptionTsr.shape = (N, sequence_length, embedding_dim)\n        aggregated_h, (ht, ct) = self.lstm(embeddedChoppedDescriptionTsr)\n        # ht.shape = (num_lstm_layers, N, lstm_hidden_dim)\n        # ht[-1].shape = (N, lstm_hidden_dim)\n        concat_latent = torch.cat( (torch.nn.functional.normalize(ht[-1]),\n                                    torch.nn.functional.normalize(image_latentTsr)), dim=1)\n        # concat_latent.shape = (N, image_latent_dim + lstm_hidden_dim)\n        outputTsr = self.linear(self.dropout(concat_latent))\n        # outputTsr.shape = (N, vocab_size)\n        return outputTsr\n\n    def Caption(self, latentVariablesTsr, maximumLength,\n                         word_to_embedding_dict, index_to_word_dict,\n                         endOfSeqIndex, useCuda):\n        notSetEmbedding = word_to_embedding_dict['NOTSET']\n        embeddedChoppedDescriptionTsr = torch.zeros((maximumLength, self.embedding_dim))\n        for i in range(maximumLength):\n            embeddedChoppedDescriptionTsr[i] = torch.tensor(notSetEmbedding)\n        endOfSeqIsFound = False\n        runningNdx = 0\n        caption_words = []\n        while not endOfSeqIsFound and runningNdx < maximumLength:\n            if useCuda:\n                latentVariablesTsr = latentVariablesTsr.cuda()\n                embeddedChoppedDescriptionTsr = embeddedChoppedDescriptionTsr.cuda()\n            outputTsr = self.forward(latentVariablesTsr.unsqueeze(0), embeddedChoppedDescriptionTsr.unsqueeze(0))\n            next_word_index = torch.argmax(outputTsr[0]).item()\n            caption_words.append(index_to_word_dict[next_word_index])\n            next_word = index_to_word_dict[next_word_index]\n            embeddedChoppedDescriptionTsr[runningNdx] = torch.tensor(word_to_embedding_dict[next_word])\n            runningNdx += 1\n            if next_word_index == endOfSeqIndex:\n                endOfSeqIsFound = True\n        return caption_words\n\nlstm_hidden_dimension = 32\nlstm_number_of_layers = 2\ndropoutProportion = 0.5\nlstm_model = LSTM_fixed_embedding(\n        embedding_dim=embedding_dimension,\n        lstm_hidden_dim=lstm_hidden_dimension,\n        num_lstm_layers=lstm_number_of_layers,\n        image_latent_dim=number_of_latent_variables,\n        vocab_size=len(word_to_index_dict),\n        dropoutProportion=dropoutProportion\n    )\n\nif useCuda:\n    lstm_model = lstm_model.cuda()","33da5791":"def TestSample(index, validation_images_list, image_filename_to_latent_variables, model,\n               index_to_word_dict,\n               word_to_embedding_dict,\n               sequence_length=40,\n               endOfSeqIndex=0,\n               useCuda=True):\n    sample_filename = validation_images_list[index]\n    sample_latentVariablesTsr = torch.FloatTensor(image_filename_to_latent_variables[sample_filename])\n    if useCuda:\n        sample_latentVariablesTsr = sample_latentVariablesTsr.cuda()\n    sample_words = model.Caption(\n        latentVariablesTsr=sample_latentVariablesTsr,\n        maximumLength=sequence_length,\n        word_to_embedding_dict=word_to_embedding_dict,\n        index_to_word_dict=index_to_word_dict,\n        endOfSeqIndex=endOfSeqIndex,\n        useCuda=useCuda\n    )\n    return sample_words\n\nvalidation_sample_0_Ndx = 0\nvalidation_sample_100_Ndx = 100\nIPython.display.Image(os.path.join(imagesFilepath, validation_images_list[validation_sample_0_Ndx]))","0eb80cd5":"IPython.display.Image(os.path.join(imagesFilepath, validation_images_list[validation_sample_100_Ndx]))","2c30683a":"import sys\n\nparameters = filter(lambda p: p.requires_grad, lstm_model.parameters())\noptimizer = torch.optim.Adam(parameters, lr=0.0003)\nlossFcn = torch.nn.CrossEntropyLoss()\ntrain_dataLoader = DataLoader(lstm_train_dataset, batch_size=16, shuffle=True)\nvalidation_dataLoader = DataLoader(lstm_validation_dataset, batch_size=lstm_validation_dataset.__len__())\nbest_model_filepath = '\/kaggle\/working\/lstm.pth'\n\nlowestValidationLoss = sys.float_info.max\nfor epoch in range(1, 500 + 1):\n    lstm_model.train()\n    loss_sum = 0.0\n    numberOfBatches = 0\n    for ( (latent_variablesTsr, chopped_descriptionTsr), target_next_word) in train_dataLoader:\n        if numberOfBatches % 4 == 1:\n            print (\".\", end=\"\", flush=True)\n        if useCuda:\n            latent_variablesTsr = latent_variablesTsr.cuda()\n            chopped_descriptionTsr = chopped_descriptionTsr.cuda()\n            target_next_word = target_next_word.cuda()\n        predicted_next_word = lstm_model(latent_variablesTsr, chopped_descriptionTsr)\n        optimizer.zero_grad()\n        loss = lossFcn(predicted_next_word, target_next_word)\n        loss.backward()\n        optimizer.step()\n        loss_sum += loss.item()\n        numberOfBatches += 1\n    train_loss = loss_sum\/numberOfBatches\n    print (\"\\nepoch {}: train_loss = {}\".format(epoch, train_loss))\n\n    # Validation\n    lstm_model.eval()\n    sample_0_words = TestSample(validation_sample_0_Ndx, validation_images_list, image_filename_to_latent_variables, lstm_model,\n       index_to_word_dict,\n       word_to_embedding_dict,\n       sequence_length=40,\n       endOfSeqIndex=0,\n       useCuda=useCuda)\n    print (\"sample_0_words = {}\".format(sample_0_words))\n    sample_100_words = TestSample(validation_sample_100_Ndx, validation_images_list, image_filename_to_latent_variables, lstm_model,\n       index_to_word_dict,\n       word_to_embedding_dict,\n       sequence_length=40,\n       endOfSeqIndex=0,\n       useCuda=useCuda)\n    print (\"sample_100_words = {}\".format(sample_100_words))\n    \n    for ((validation_latent_variablesTsr, validation_chopped_descriptionTsr), validation_target_next_word) in validation_dataLoader:\n        if useCuda:\n            validation_latent_variablesTsr = validation_latent_variablesTsr.cuda()\n            validation_chopped_descriptionTsr = validation_chopped_descriptionTsr.cuda()\n            validation_target_next_word = validation_target_next_word.cuda()\n        validation_predicted_next_word = lstm_model(validation_latent_variablesTsr, validation_chopped_descriptionTsr)\n        validation_loss = lossFcn(validation_predicted_next_word, validation_target_next_word).item()\n    print (\"validation_loss = {}\".format(validation_loss))\n\n    if validation_loss < lowestValidationLoss:\n        lowestValidationLoss = validation_loss\n        torch.save(lstm_model.state_dict(), best_model_filepath)","de272246":"test_images_filepath = '..\/input\/flickr8k\/Flickr_Data\/Flickr_Data\/Flickr_TextData\/Flickr_8k.testImages.txt'\nwith open(test_images_filepath, 'r') as test_images_file:\n    test_images_list = [line.strip() for line in test_images_file]\nprint (\"len(test_images_list) = {}\".format(len(test_images_list)))","5a447ad1":"# Load the model that gave the lowest validation loss\nlstm_model.load_state_dict(torch.load(best_model_filepath))\n# Randomly select some test images\ntest_image_sample_indices = random.choices(range(len(test_images_list)), k=3)\nlstm_model.eval()\ntest_sample_filepaths = []\ntest_sample_captions = []\nfor test_image_sample_index in test_image_sample_indices:\n    sample_words = TestSample(test_image_sample_index, test_images_list, image_filename_to_latent_variables, lstm_model,\n       index_to_word_dict,\n       word_to_embedding_dict,\n       sequence_length=40,\n       endOfSeqIndex=0,\n       useCuda=useCuda) \n    caption = ' '.join(sample_words)\n    test_sample_filepaths.append(os.path.join(imagesFilepath, test_images_list[test_image_sample_index]))\n    test_sample_captions.append(caption)","84e9a9e2":"print (test_sample_captions[0])\nIPython.display.Image(test_sample_filepaths[0])","d34e9637":"print (test_sample_captions[1])\nIPython.display.Image(test_sample_filepaths[1])","505c547e":"print (test_sample_captions[2])\nIPython.display.Image(test_sample_filepaths[2])","16fde100":"# Image Captioning with PyTorch LSTM\n\nWe'll be using a PyTorch implementation of an *LSTM* (Long Short Term Memory) neural network to create this image captioning model. The image input will be preprocessed with one of torchvision's pretrained model and the partial caption (truncated at mid-sentence) will be preprocessed with the *word2vec* algorithm.","635daff3":"The word2vec (CBOW) is a self-supervised learning algorithm that aims to predict a center word, given the context words, i.e. the few words before and after the center word, in no particular order (hence the *bag of words* part in the CBOW name). For example, from the sentence 'A red apple is falling from the tree', we could sample the context bag of words ('from', 'is', 'the', 'apple') and the center word 'falling', assuming a context length of 2. \nLet's create a PyTorch Dataset that will automatically generate context words and center word from a list of sentences.","394bbdf0":"We now have our descriptions in the form of a list of integers. If their length is less than the set maximum length (40), they will end with the special index 0 meaning 'ENDOFSEQ', followed by a padding of the special index 2 meaning 'NOTSET'. If a word is not in the vocabulary (i.e. the word is one of those that we threw away because they appeared only once), it will be replaced by the special index 1, meaning 'UNKNOWN'.\nNote that each image filename will appear in 5 lines, since each image has 5 different descriptions.","0d843ed7":"## Test samples\nThe final step is to test with images from the test set. Those were not seen during training, neither in the training loop nor in the computation of the validation loss.","966267d6":"We can check a few embedding vectors.","2ab8ec7c":"These two files give a list of filenames, partitioning the dataset into training and validations sets.\nLet's build a dictionary of each word in the training set to their corresponding number of occurrences. The following function Tokenize() will convert a description such as 'A child eats an apple .' into a list of lowercase tokens such as ['a', 'child', 'eats', 'an', 'apple'].","f804022e":"We'll define a simple LSTM model.","bdb72669":"## Convert images to latent variables, with a pre-trained CNN\nThe image analysis part of the caption generator will use a pre-trained convolutional neural network. Since the pre-trained models available through torchvision were trained on ImageNet, a large database of natural scene images, we will assume that these models learned to detect the presence of the objects and the actions that we can find in the flickr8k dataset. If it is indeed the case, the latent representation of the images will contain sufficient signals to reconstruct a depicting sentence. \nLet's start by converting all the images in the dataset into their corresponding latent representation, which we'll write to file.","fdb69860":"## Training an LSTM neural network\nWe have all the ingredients at hand to train a simple LSTM that will take as inputs the latent representation of an image and the beginning of a caption that describes this image, and that will produce as an output the next word in the caption. \nFor example, let's say we have the following image:","3f948ff8":"We can see that the file '...\/Flickr8k.token.txt' contains, on each line, the filename of an image appended with '#X' (X being a number between 0 and 4, presumably representing various human annotators who wrote the description), followed by the description.\nThere is another pair of files that we'll need to split the training images and the validation images (referred to as 'dev' by the dataset).","9e6b1a61":"Let's say we have already predicted the beginning of the caption: *'a large bird stands in the water on the'*. The task of the LSTM, given the image latent representation and the beginning of the caption, will be to output the most likely following word. In this case, it would be *beach*.\nLet's create a Dataset class.","b6667118":"## Create a word embedding with the word2vec (Continuous Bag of Words) algorithm\nEncoding the words with an arbitrary index from a vocabulary dictionary is easy, but it comes with a cost. To translate this encoding into a tensor form would yield to a one-hot encoding, where each word would be represented by a sparse vector of length V (the vocabulary size) having a single entry set to 1, all the other entries being 0. Such a representation doesn't exploit the semantic of the words.  For example, verbs such as *to walk* and *to run* can often be swapped in a sentence. This will slightly affect the meaning of the sentence, but it won't destroy the sentence syntax, nor create confusing gibberish. It would be advantageous to have an encoding that represents words as numerical vectors that transpose word meaning similarities into vector similarities. This is achieved by a word embedding called [**word2vec**](https:\/\/en.wikipedia.org\/wiki\/Word2vec). We'll implement the Continuous Bag of Words (CBOW) variant.","98d57946":"We could remove almost 3000 words that appeared only once in the vocabulary.\nLet's sort the words in reverse order of frequency and write our vocabulary to file. We'll also need three special words that we'll put at the beginning of the vocabulary.","88d181eb":"And now the LSTM training loop!\nAs a sanity check, we'll monitor the caption generated for two validation images. This should give us a qualitative idea of what the LSTM model is learning.","65df9d3e":"We'll have to be patient... Before we get an actual meaningful sentence, the training loop will go through hundreds of epochs.\nIf automatic caption generation was easy, we wouldn't be doing it!","bde38b3d":"## Pre-tokenize the descriptions\nIn order to accelerate the training process, we'll convert the descriptions into their tokenized version, using the index attributed to each word that we saved in the vocabulary file.","36010e0f":"We can now train the word embedder.","ff0b8ad4":"We now need to create a simple PyTorch model that predicts the center word from the context words. The ordering of the context words is shuffled by an averaging operation.","dd3998bb":"## Create the dataset vocabulary\nThe set of words used in the training dataset will be the vocabulary that our caption generator will be allowed to use. \nLet's check the dataset captions structure.","20d0662a":"The word embedding is the weights matrix stored in the encoder embedding layer. Let's write that to disk.","4acfa11d":"The raw dictionary that was just created contains more than 7000 words, but some of them only appear once. These are either very rare words or possibly misspelled words. For these reasons, we want to get rid of the words that appear only once in the dataset.","0a2d7486":"The images need to be preprocessed the same way as they were when the googlenet creators trained it. We can find this information in [torchvision's documentation](https:\/\/pytorch.org\/docs\/stable\/torchvision\/models.html).","31e02a4b":"We can now loop through all the images in the dataset, preprocess them, then pass them through googlenet to get their latent representation. The image filepath and the 1024 latent variables are written in a csv file with 1025 columns."}}