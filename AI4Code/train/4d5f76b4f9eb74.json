{"cell_type":{"31edc690":"code","06126625":"code","3f3f2c1c":"code","a7e6fc7e":"code","729dfba3":"code","47ea9c62":"code","17876be3":"code","1775adb1":"code","5e267bb3":"code","4d8412a2":"code","99e98356":"code","4e7725e2":"code","a568200a":"code","c6b7818e":"code","a78ebf09":"code","f63a520e":"code","7661b4ae":"code","f140b4d5":"code","410de9a5":"code","d78a397b":"code","ce8d1eb3":"code","0316857c":"code","35ea99d1":"code","657165af":"code","a6bbef6b":"code","fe03077b":"code","0ef8b088":"code","accf7f37":"code","a48b13e4":"code","6bb90973":"code","fbf95d9b":"code","23f06aca":"code","4e2c79c2":"code","ad2e8bca":"code","574f3b06":"code","c1479b16":"code","ea60c949":"code","c4353b40":"markdown","2411273d":"markdown","8c816cd6":"markdown","460efce9":"markdown","ca24f17d":"markdown","5e6f7ae2":"markdown","2ab80658":"markdown","c11d2854":"markdown","426e0242":"markdown","9817c306":"markdown","d0f1266d":"markdown","d7eb8668":"markdown","4830f0f2":"markdown","46d91e83":"markdown","bc7442c8":"markdown","66ca0c8d":"markdown","6760af82":"markdown","701e4fde":"markdown","b0c2b315":"markdown","54ff2ead":"markdown","4f4c4235":"markdown","56c4f1e2":"markdown","181ce1b0":"markdown","46d3ef5d":"markdown","a60b3cc3":"markdown","59fd1b60":"markdown","b6d700d1":"markdown","c9a9173f":"markdown","c29208d4":"markdown","179146a3":"markdown","9dacd997":"markdown","2c01b773":"markdown","b686a901":"markdown","839f150f":"markdown","5a560bc5":"markdown"},"source":{"31edc690":"import pandas as pd\nimport numpy as np\n\n# What do we say to python warnings? NOT TODAY\nimport warnings\nwarnings.filterwarnings('ignore')","06126625":"def select_features(df, target, th):\n    \"\"\"\n    Select features.\n    \"\"\"\n    # Select rows with our target value\n    proc_df = df[df[target].isna() == False]\n    \n    # Remove useless columns\n    to_drop = [col for col in proc_df.columns if ((\"(F)\" in col) or (\"(fpm)\" in col) or (\"Thermal\" in col) or (\"Air movement\" in col)) and (col != target)] + [\"Database\", \"Publication (Citation)\", \"Data contributor\"]\n    proc_df = proc_df.drop(to_drop, axis=1)\n    \n    # Remove columns with a lot of missing values\n    # Get columns with less than 30% of data missing\n    s = (proc_df.isna().sum() \/ len(proc_df) * 100 < th)\n    to_keep = list(s[s].index)\n    proc_df = proc_df[to_keep]\n    \n    return proc_df","3f3f2c1c":"# Load original data\ndf_raw = pd.read_csv(\"\/kaggle\/input\/ashrae-global-thermal-comfort-database-ii\/ashrae_db2.01.csv\", low_memory=False)\n# Select features\ndf = select_features(df_raw, \"Thermal sensation acceptability\", 25)","a7e6fc7e":"df.head()","729dfba3":"df.info()","47ea9c62":"# Get list of categorical variables\ns = (df.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","17876be3":"for col in object_cols:\n    unique_cat = df[col].unique()\n    print(f\"{len(unique_cat)} categories in {col}: {unique_cat}\\n\")","1775adb1":"df.drop(\"Climate\", axis=1, inplace=True)","5e267bb3":"def group_koppen_categories(category):\n    if \"A\" in category:\n        return \"A\"\n    elif \"B\" in category:\n        return \"B\"\n    elif \"C\" in category:\n        return \"C\"\n    elif \"D\" in category:\n        return \"D\"\n    elif \"E\" in category:\n        return \"E\"\n    else:\n        return None","4d8412a2":"df[\"Koppen climate classification\"] = [group_koppen_categories(category) for category in df[\"Koppen climate classification\"]]","99e98356":"from sklearn.impute import SimpleImputer","4e7725e2":"# Get list of remaining categorical variables\ns = (df.dtypes == 'object')\nobject_cols = list(s[s].index)\n\ndf.groupby(\"City\")[object_cols].agg(pd.Series.mode)","a568200a":"# This is the mode of the column \"Cooling startegy_building level\"\ndf[\"Cooling startegy_building level\"].mode()","c6b7818e":"# And we replace for city Harbin\ndf.loc[df.City == \"Harbin\", \"Cooling startegy_building level\"] = \"Naturally Ventilated\"","a78ebf09":"df.isna().sum() \/ len(df) * 100","f63a520e":"df[df.City.isna()]","7661b4ae":"# This is the mode of the column \"City\" for Malaysia\ndf.loc[df.Country == \"Malaysia\", \"City\"].mode()","f140b4d5":"# And we replace\ndf.loc[df.Country == \"Malaysia\", \"City\"] = \"Kota Kinabalu\"","410de9a5":"df.isna().sum() \/ len(df) * 100","d78a397b":"# Get list of categorical variables with missing values\ncols = [\"Season\", \"Cooling startegy_building level\"]\n\n# Input mode for each city\nfor city in df.City.unique():\n    print(city)\n    # Filter data of selected city\n    temp = df.loc[df.City == city, cols]\n    \n    # Create imputer\n    imputer = SimpleImputer(strategy='most_frequent')\n    \n    # Input missing values with mode\n    imputed = pd.DataFrame(imputer.fit_transform(temp))\n    \n    # Rename columns and index\n    imputed.columns = temp.columns\n    imputed.index = temp.index\n    \n    # Replace in dataframe\n    df.loc[df.City == city, cols] = imputed","ce8d1eb3":"df.isna().sum() \/ len(df) * 100","0316857c":"from sklearn.preprocessing import OneHotEncoder","35ea99d1":"# Get list of categorical variables\ns = (df.dtypes == 'object')\ncols = list(s[s].index)\nprint(cols)\n\n# One Hot Encoder\nfor col in cols:\n   \n    # Create encoder\n    OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\n    # Transform\n    OH_cols = pd.DataFrame(OH_encoder.fit_transform(df[[col]]))\n    # Get categories names and rename\n    names = OH_encoder.categories_\n    OH_cols.columns = names\n    OH_cols.index = df.index\n\n    # Add encoded columns\n    df[list(names[0])] = OH_cols\n\n# Drop un-encoded column\ndf.drop(cols, axis=1, inplace=True)","657165af":"df.head()","a6bbef6b":"df.info()","fe03077b":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, classification_report","0ef8b088":"# Replace the missing values\ndf = df.replace(np.nan, 0)","accf7f37":"# balance\ndf.value_counts(\"Thermal sensation acceptability\")","a48b13e4":"df1 = df[df[\"Thermal sensation acceptability\"] == 1].sample(14045, random_state=55)\ndf0 = df[df[\"Thermal sensation acceptability\"] == 0] \ndfb = pd.concat([df0,df1])\n\ndfb.value_counts(\"Thermal sensation acceptability\")","6bb90973":"def get_balance_dataset_index(df, target):\n    \"\"\"\n    df: the dataset to balance\n    target: the name of the target column\n    \"\"\"\n    \n    # Get count for each category\n    value_counts = df.value_counts(target).to_dict()\n    \n    # List comprehension to find the key with the minimum value (count)\n    min_category = [k for k,v in value_counts.items() if v == min(value_counts.values())][0]\n    min_count = [v for k,v in value_counts.items() if v == min(value_counts.values())][0]\n    \n    # For each category in your target\n    dfs = []\n    for key in value_counts:\n        if key == min_category:\n            df1 = df[df[target] == min_category]\n        else:\n            df1 = df[df[target] == key].sample(min_count, random_state=55)\n        dfs.append(df1)\n    \n    dfb = pd.concat(dfs)\n    print(f\"Your balance dataset: {dfb.value_counts(target).to_dict()}\")\n    \n    return dfb.index","fbf95d9b":"X = dfb.drop(\"Thermal sensation acceptability\", axis=1).reset_index(drop=True)\ny = dfb[\"Thermal sensation acceptability\"].reset_index(drop=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=55)","23f06aca":"from sklearn.model_selection import cross_val_score\n\n# Define the model\nmy_model = XGBClassifier(n_estimators=50, learning_rate=0.1, n_jobs=4, random_state=55, \n                         objective=\"binary:logistic\", eval_metric=\"auc\", use_label_encoder=False)\n\n# Perform cross validation with 5 folds\nprint(\"Training...\")\nscores = cross_val_score(my_model, \n                          X_train, y_train,\n                          cv=5,\n                          scoring='roc_auc')\nprint(\"...done.\")","4e2c79c2":"print(f\"Scores: {scores}\")\nprint(f\"Mean scores: {np.mean(scores)}\")","ad2e8bca":"my_model = XGBClassifier(n_estimators=50, learning_rate=0.1, n_jobs=4, random_state=55, \n                         objective=\"binary:logistic\", eval_metric=\"auc\", use_label_encoder=False)\n\n# Train\nprint(\"Training...\")\nmy_model.fit(X_train, y_train, verbose=False)\nprint(\"...done\")","574f3b06":"# And we predict\nprediction = pd.DataFrame({\"y_pred\": my_model.predict(X_test), \"y_real\": y_test})\nroc_auc_score(prediction.y_real, prediction.y_pred)","c1479b16":"print(classification_report(prediction.y_real, prediction.y_pred))","ea60c949":"def group_koppen_categories(category):\n    if \"A\" in category:\n        return \"A\"\n    elif \"B\" in category:\n        return \"B\"\n    elif \"C\" in category:\n        return \"C\"\n    elif \"D\" in category:\n        return \"D\"\n    elif \"E\" in category:\n        return \"E\"\n    else:\n        return None\n    \ndef handle_categorical_features(df):\n\n    from sklearn.impute import SimpleImputer\n    from sklearn.preprocessing import OneHotEncoder\n    \n    # Drop climate\n    df.drop(\"Climate\", axis=1, inplace=True)\n    # Group koppen climate\n    df[\"Koppen climate classification\"] = [group_koppen_categories(category) for category in df[\"Koppen climate classification\"]]\n    # Fill missing cooling strategy in city Harbin\n    df.loc[df.City == \"Harbin\", \"Cooling startegy_building level\"] = \"Naturally Ventilated\"\n    # Fill missing city in Malaysia\n    df.loc[df.Country == \"Malaysia\", \"City\"] = \"Kota Kinabalu\"\n    \n    # Input mode by city in Season and Cooling strategy\n    # Get list of categorical variables with missing values\n    cols = [\"Season\", \"Cooling startegy_building level\"]\n    # Input mode for each city\n    for city in df.City.unique():\n        # Filter data of selected city\n        temp = df.loc[df.City == city, cols]\n        # Create imputer\n        imputer = SimpleImputer(strategy='most_frequent')\n        # Input missing values with mode\n        imputed = pd.DataFrame(imputer.fit_transform(temp))\n        # Rename columns and index\n        imputed.columns = temp.columns\n        imputed.index = temp.index\n        # Replace in dataframe\n        df.loc[df.City == city, cols] = imputed\n        \n    # Encode\n    # Get list of categorical variables\n    s = (df.dtypes == 'object')\n    cols = list(s[s].index)\n    # One Hot Encoder\n    for col in cols:\n        # Create encoder\n        OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n        # Transform\n        OH_cols = pd.DataFrame(OH_encoder.fit_transform(df[[col]]))\n        # Get categories names and rename\n        names = OH_encoder.categories_\n        OH_cols.columns = names\n        OH_cols.index = df.index\n        # Add encoded columns\n        df[list(names[0])] = OH_cols\n\n    # Drop un-encoded column\n    df.drop(cols, axis=1, inplace=True)\n    \n    return df\n\n    \ndef get_balance_dataset_index(df, target):\n    \"\"\"\n    df: the dataset to balance\n    target: the name of the target column\n    \"\"\"\n    \n    # Get count for each category\n    value_counts = df.value_counts(target).to_dict()\n    \n    # List comprehension to find the key with the minimum value (count)\n    min_category = [k for k,v in value_counts.items() if v == min(value_counts.values())][0]\n    min_count = [v for k,v in value_counts.items() if v == min(value_counts.values())][0]\n    \n    # For each category in your target\n    dfs = []\n    for key in value_counts:\n        if key == min_category:\n            df1 = df[df[target] == min_category]\n        else:\n            df1 = df[df[target] == key].sample(min_count, random_state=55)\n        dfs.append(df1)\n    \n    dfb = pd.concat(dfs)\n    print(f\"Your balance dataset: {dfb.value_counts(target).to_dict()}\")\n    \n    return dfb.index","c4353b40":"Ok, is highly unbalanced. We can balance it, but that will means loosing data. It's a compromise that sometimes has to be made, you can balance your dataset (if you have enough data) or you can try to solve it by optimizing your model. Here we will go the easy way, balance the data set:","2411273d":"Now let's predict and check the score","8c816cd6":"## Balance dataset\nIn general, classification models work better when they are balanced among the categories; if most of the data belongs to one category the model will predict mostly that category to get a better score. Let's inspect our data:","460efce9":"Now we will use the method `cross_val_score` from SciKitLearn library. This technique splits the train dataset into diferent folds to train an predict like this ([image source](https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html#cross-validation)):\n\n![cross-val](https:\/\/scikit-learn.org\/stable\/_images\/grid_search_cross_validation.png)\n\nIn each split a different part of the data is used to train and to predict. This way we can estimate our metric avoiding the chance of getting a super-good (or super bad) split.\n\nWe are going to use a simple XGBoost classifier without optimizing parameters (this is a huge part of machine learning, but we are not focusing on that in these notebooks) and we are going to use 5 folds.","ca24f17d":"# Explore categories\nCategorical features are usually of type object (i.e., a string). Let's get a list of them:","5e6f7ae2":"Let's check out how it's going:","2ab80658":"Instead of encoding them one by one we are using a loop to do it automatically, rename them and add them to our data set. This is kind of similar to the loop we used for imputing the missing values in the previous section.","c11d2854":"| Notebook           | Categorical features | Missing values in categorical | Missing values in numerical | Feature engineering | ROC-AUC score |\n|--------------------|----------------------|-------------------------------|-----------------------------|---------------------|---------------|\n| Preprocessing pt.2 | One Hot Encoding     | Mode input                    | 0 input                     | -                   |0.6591         |","426e0242":"All the missings in `City` are from the same country. We are going to input the most common city from Malaysia. We can follow the exact same processas before:","9817c306":"A little bit lower than our cross-validation score but not that bad considering is the first try. We can also get the classification report, that shows us some more metrics:","d0f1266d":"Notice that we have a problem with the city Harbin and the feature `Cooling startegy_building level`: is always missing. Here we can drop that city information or input the mode by column. We are going to input the mode by column.\n\nIn cases like this, where are all missing, we can just replace the values:","d7eb8668":"# Encoding\nNow is time of the encoding. We are going to use the [package from SciKitLearn `OneHotEncoder`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html). This technique creates boolean columns, one per category in our feature.","4830f0f2":"Now we are ready, no more missing in City, our grouping column. We have to input the mode by city in `Season` and `Cooling startegy_building level`, the categorical features that have missing values. Here we will use `SimpleImputer`. We are going to use a for loop to encode all the cities automatically. Here is the code with comments in each step:","46d91e83":"We could also write a function to do it automatically amd get the indexes in order to use always the same part of the data set in the following notebooks.","bc7442c8":"# Notebook goal\nThe goal of this notebook is to handle the categorical features. This will include:\n\n- Select the ones we want to use in the model\n- Fill the missing values\n- Try to reduce the number of categories in some of them\n- Encode them, creating one boolean column per category\n\nHere will be shown only one of the many possibilities in machine learning, feel free to experiment and play around with the data on your own. We will be focusing on the techniques and not on the model performance.","66ca0c8d":"# (Optional) One function to do it all\nThis part is completely optional. The only purpose of this function is to be used in the following notebooks.","6760af82":"First thing that can be noticed here is that we have two redundante columns: `Koppen climate classification` and `Climate`. If you check the [definition](https:\/\/en.wikipedia.org\/wiki\/K%C3%B6ppen_climate_classification) you'll see that every code has a description of the climate. We are going to drop `Climate` and work only with the code (you can keep both of them and play around, there are not strict rules here, this is just one of the infinite possibilities). Say goodbye to the climate column for now:","701e4fde":"# Fill missing values\nBefore encoding our categorical features we have to input the missing values (otherwise the encoder won't work). A good practice for this is to input the [mode](https:\/\/en.wikipedia.org\/wiki\/Mode_(statistics)), which is the most common value. We could input the mode by column, but a more realistic method would be to input the mode by a selected group. Here, we can input the **mode by city**. Let's try it out.","b0c2b315":"## Train and predict\nLet's train a model with the whole training data set.","54ff2ead":"Here we have the dataset we obtained in our previous notebook. Next step is to explore the categorical features and transform them to work in our model.","4f4c4235":"## Split and cross validation\nWe are going to use [the method `train_test_split`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html), which randomly split the data based on the test size specified. We are setting also a `random_state` which is a seed for that randomnization that allows to reproduce the process and obtain the same results when running it again. The test data is sometimes called \"holdout set\", because is not used during the training period, it just wait there to be predicted; this way you make sure that your model is predicting completely new data and won't overfit.","56c4f1e2":"Let's check out the scores!","181ce1b0":"# Load data\nThe functions used here were defined in previous notebooks, they replicate the process followed there to use always the same dataset, you can ignore them.","46d3ef5d":"## Cardinalty\nWe are going to inspect them one by one to check out how many categories we have. We are going to create one column per category when encoding, we don't want a super high cardinality. A simple `for` loop can do the job:","a60b3cc3":"Now we can encode our categorical features!","59fd1b60":"Looks like there are not a lot of them, BUT remember that we have to create one column per categoy inside those features, and that can scalate quickly. Let's inspect the cardinality.","b6d700d1":"We have some missings in `City`, our grouping category, let's explore  them first.","c9a9173f":"This is what we would input in each case:","c29208d4":"We haven't yet handle missing values, so, for now, we are **replacing them with 0**. Which is not a great practice, but we'll handle them properly in the next notebook.","179146a3":"# Introduction\n\nThis is a serie of notebooks thar should be visited in order, they are all linked in the table of content. In this notebook we are going to handle categorical features and run a classification model.\n\n### Content table\n- [Preprocessing pt. 1: data transformation & EDA](https:\/\/www.kaggle.com\/ponybiam\/classification-preprocessing-pt-1)\n- **Preprocessing pt. 2: encoding categorical variables** (you are here)\n    - [Load data](#Load-data)\n    - [Explore categories](#Group-categories)\n        - [Cardinality](#Cardinality)\n        - [Fill missing values](#Fill-missing-values)\n    - [Encoding](#Encoding)\n    - [Model](#Model)\n        - [Balance dataset](#Balance-dataset)\n        - [Split and cross validation](#Split-and-cross-validation)\n        - [Train and predict](#Train-and-predict)\n- [Preprocessing pt. 3: handling numerical features](https:\/\/www.kaggle.com\/ponybiam\/classification-preprocessing-pt-3) \n- [Feature engineering pt. 1: simple features](https:\/\/www.kaggle.com\/ponybiam\/classification-feature-engineering-pt-1)\n- [Feature engineering pt. 2: clustering & PCA](https:\/\/www.kaggle.com\/ponybiam\/classification-feature-engineering-pt-2)\n- [Feature engineering pt. 3: target encoding](https:\/\/www.kaggle.com\/ponybiam\/classification-feature-engineering-pt-3)","9dacd997":"Continuing with the [Koppen climate classification](https:\/\/en.wikipedia.org\/wiki\/K%C3%B6ppen_climate_classification), the first letter of the code is the climate group and the following letters are a subgroup. Here is an oportunity to reduce the final ammount of columns, let's work only with the main group. We can create a function to do this:","2c01b773":"Not that bad! Now is time to train the model with all the training data (in general, more data means a better generalization) and predict our test data set.","b686a901":"And we use it wth list comprehension to replace in our dataset:","839f150f":"more than 70 columns! and we started with 15. Keep in mind that for a much bigger data set this could be a problem: adding a column translate on more memory usage.","5a560bc5":"# Model\nAnd finally, the model. We are going to use XGBoost and the metric `roc_auc_score`.\n\nWe won't dive deep into optimizing the model parameters, but you can [check them out here](https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#module-xgboost.sklearn) and play around with them.\n\nAs metric we are going to use [ROC-AUC score](https:\/\/en.wikipedia.org\/wiki\/Receiver_operating_characteristic); this metrics is the area under the curve (AUC) of True Positive Rate (TPR, aka: sensitivity or recall) vs. False Positive Rate (FPR, aka: a false alarm) at various threshold settings. In a classification model what we want is a really high TPR and a really low FPR, perfect case would be a score of 1.\n\n![ROC-curve-score](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/1\/13\/Roc_curve.svg\/800px-Roc_curve.svg.png)\n\nWe will also inspect the [classification report](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.classification_report.html), which informs the following metrics:\n\n- **precision**: defined as all the positive cases (class 1) divided by all the cases classified as positive $\\frac{TP}{TP+FP}$ where TP is \"True Positive\" and FP is \"False Positive\".\n- **recall**: also known as sensitivity or True Positive Rate, defined as all the positive cases divided by all the cases that were really positive $\\frac{TP}{TP+FN}$ where FN is \"False Negative.\n- **f1-score**: is the harmonic mean of precision and sensitivity $\\frac{2TP}{2TP+FP+FN}$\n- **accuracy**: defined as all that was correctly classified divided by all the cases $\\frac{TP+TN}{TP+TN+FP+FN}$ were TN is \"True Negative\"."}}