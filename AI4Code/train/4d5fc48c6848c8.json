{"cell_type":{"bc879783":"code","abc5a7bb":"code","087b710b":"code","c42ea6f7":"code","9f8c63c5":"code","6744d8bd":"code","023211a7":"code","f6c6bcbc":"code","28231c4b":"code","c9f685a7":"code","5cfc4362":"code","88403431":"code","1479e135":"code","f9de7c84":"code","bb7b3df6":"code","a36e6019":"code","0af1337f":"markdown","9ec3c815":"markdown","4514b2d0":"markdown"},"source":{"bc879783":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport torchvision.transforms as transforms\nimport torchvision.models as models\n\nimport copy","abc5a7bb":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","087b710b":"# desired size of the output image\nimsize = 512 if torch.cuda.is_available() else 128  # use small size if no gpu\n\nloader = transforms.Compose([\n    transforms.Resize(imsize),  # scale imported image\n    transforms.ToTensor()])  # transform it into a torch tensor\n\n\ndef image_loader(image_name):\n    image = Image.open(image_name)\n    # fake batch dimension required to fit network's input dimensions\n    image = loader(image).unsqueeze(0)\n    return image.to(device, torch.float)\n\n\nstyle_img = image_loader(\"..\/input\/two-images\/picasso.jpg\")\ncontent_img = image_loader(\"..\/input\/two-images\/dancing.jpg\")\n\nassert style_img.size() == content_img.size(), \\\n    \"we need to import style and content images of the same size\"","c42ea6f7":"class ContentLoss(nn.Module):\n\n    def __init__(self, target,):\n        super(ContentLoss, self).__init__()\n        # we 'detach' the target content from the tree used\n        # to dynamically compute the gradient: this is a stated value,\n        # not a variable. Otherwise the forward method of the criterion\n        # will throw an error.\n        self.target = target.detach()\n\n    def forward(self, input):\n        self.loss = F.mse_loss(input, self.target)\n        return input","9f8c63c5":"def gram_matrix(input):\n    a, b, c, d = input.size()  # a=batch size(=1)\n    # b=number of feature maps\n    # (c,d)=dimensions of a f. map (N=c*d)\n\n    features = input.view(a * b, c * d)  # resise F_XL into \\hat F_XL\n\n    G = torch.mm(features, features.t())  # compute the gram product\n\n    # we 'normalize' the values of the gram matrix\n    # by dividing by the number of element in each feature maps.\n    return G.div(a * b * c * d)","6744d8bd":"class StyleLoss(nn.Module):\n\n    def __init__(self, target_feature):\n        super(StyleLoss, self).__init__()\n        self.target = gram_matrix(target_feature).detach()\n\n    def forward(self, input):\n        G = gram_matrix(input)\n        self.loss = F.mse_loss(G, self.target)\n        return input","023211a7":"cnn = models.vgg19(pretrained=True).features.to(device).eval()","f6c6bcbc":"cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\ncnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n\n# create a module to normalize input image so we can easily put it in a\n# nn.Sequential\nclass Normalization(nn.Module):\n    def __init__(self, mean, std):\n        super(Normalization, self).__init__()\n        # .view the mean and std to make them [C x 1 x 1] so that they can\n        # directly work with image Tensor of shape [B x C x H x W].\n        # B is batch size. C is number of channels. H is height and W is width.\n        self.mean = torch.tensor(mean).view(-1, 1, 1)\n        self.std = torch.tensor(std).view(-1, 1, 1)\n\n    def forward(self, img):\n        # normalize img\n        return (img - self.mean) \/ self.std","28231c4b":"content_layers_default = ['conv_4']\nstyle_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n\ndef get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n                               style_img, content_img,\n                               content_layers=content_layers_default,\n                               style_layers=style_layers_default):\n    # At runtime, CNN is a pretrained VGG19 CNN network.\n    cnn = copy.deepcopy(cnn)\n\n    # Ship the Normalization module to CUDA. The mean and standard deviation values\n    # used for normalization on the originating training dataset are well-known\n    # values, specified manually as `cnn_normalization_mean` and `cnn_normalization_std`\n    # in the code block above.\n    #\n    # In this demo, the Normalization module is applied just-in-time as a functional\n    # layer of the network (first layer).\n    normalization = Normalization(normalization_mean, normalization_std).to(device)\n    model = nn.Sequential(normalization)\n\n    content_losses = []\n    style_losses = []\n\n    # This next code block does the bulk of the work building the new model.\n    #\n    # Recall that .children() iterates over model layers that are direct children\n    # of the model, and not children-of-children, in case the model contains\n    # submodel (e.g. layers that are themselves nn.Module modules) definitions.\n    #\n    # Recall that cnn is the VGG19 pretrain convolutional model.\n    i = 0  # increment every time we see a conv\n    for layer in cnn.children():\n        # The first layer simply puts names to things and replaces ReLU inplace\n        # (which is optimized) with ReLU reallocated. This is a small optimization\n        # being removed, and hence a small performance penalty, necessitated by\n        # ContentLoss and StyleLoss not working well when inplace=True.\n        if isinstance(layer, nn.Conv2d):\n            i += 1\n            name = 'conv_{}'.format(i)\n        elif isinstance(layer, nn.ReLU):\n            name = 'relu_{}'.format(i)\n            layer = nn.ReLU(inplace=False)\n        elif isinstance(layer, nn.MaxPool2d):\n            name = 'pool_{}'.format(i)\n        elif isinstance(layer, nn.BatchNorm2d):\n            name = 'bn_{}'.format(i)\n        else:\n            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n\n        # add_module is a setter that is pretty much a setattr equivalent, used for\n        # registering the layer with PyTorch.\n        model.add_module(name, layer)\n\n        # The next pair of boolean checks ship the layer from cnn to the output model, with\n        # a content-dependent adjustment.\n        #\n        # Content loss is defined on just one layer in the network, conv_4. Style loss is\n        # defined on every convolutional layer in the network, conv_1...conv_4. conv_4 has\n        # both style loss and content loss defined on it.\n        #\n        # In both cases, the output vector of the model thus far is called on with .detach\n        # applied to it. .detach is call which tells PyTorch that the given layer output\n        # is not subject to gradient tracking and backpropogation, e.g. it is a functional\n        # or frozen layer. Basically the same thing as setting .no_grad to True.\n        #\n        # The loss of choice (Content or Style) is then applied to the layer. The loss is\n        # calculated by taking the difference between the corresponding target image as it\n        # appears after passing thus far into the network (model(content_img).detach()) and\n        # the current input image (at forward time).\n        if name in content_layers:\n            # model(content_img) executes *the entire model we have built so far* on the\n            # input image. detach keeps the operations involved out of the computational\n            # graph, e.g. no backpropogation.\n            target = model(content_img).detach()\n            content_loss = ContentLoss(target)\n            model.add_module(\"content_loss_{}\".format(i), content_loss)\n            content_losses.append(content_loss)\n\n        if name in style_layers:\n            target_feature = model(style_img).detach()\n            style_loss = StyleLoss(target_feature)\n            model.add_module(\"style_loss_{}\".format(i), style_loss)\n            style_losses.append(style_loss)\n\n    # Trim off the layers after the last content and style losses\n    for i in range(len(model) - 1, -1, -1):\n        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n            break\n\n    model = model[:(i + 1)]\n\n    return model, style_losses, content_losses","c9f685a7":"input_img = content_img.clone()","5cfc4362":"# The convolutional layers in the resulting model are the trainable ones.\n#\n# The loss layers are pass-through. They propogate their input forward,\n# whilst calculating a loss against target. The loss against target is\n# what is used to perform backpropogation.\n#\n# This is an unusual organization, and means that ContentLoss et al are\n# not true loss modules. Wouldn't it make more sense to run them at\n# forward time? E.g.:\n# https:\/\/discuss.pytorch.org\/t\/how-to-calculate-loss-related-to-intermediate-feed-forward-results\/10278\/2?u=residentmario\ntemp_model, _, _ = get_style_model_and_losses(\n    cnn, cnn_normalization_mean, cnn_normalization_std,\n    style_img, content_img\n)\ntemp_model","88403431":"[n for (n, pg) in list(temp_model.named_parameters()) if pg.requires_grad]","1479e135":"# The optimizer operates on *the input image* instead of on any intermediate weights.\n# We need to run requires_grad_() here to set requires_grad=True.\n#\n# Since the optimizer is only being run on the input image, no training occurs on\n# intermediate weights.\ndef get_input_optimizer(input_img):\n    optimizer = optim.LBFGS([input_img.requires_grad_()])\n    return optimizer","f9de7c84":"def run_style_transfer(cnn, normalization_mean, normalization_std,\n                       content_img, style_img, input_img, num_steps=1000,\n                       style_weight=1000000, content_weight=1):\n    \"\"\"Run the style transfer.\"\"\"\n    print('Building the style transfer model.')\n    model, style_losses, content_losses = get_style_model_and_losses(cnn,\n        normalization_mean, normalization_std, style_img, content_img)\n    optimizer = get_input_optimizer(input_img)\n\n    print('Optimizing..')\n    # TODO: refactor out this [0] list nonsense\n    run = [0]\n    while run[0] <= num_steps:\n\n        def closure():\n            # The input image can stray out of the (0, 1) range as a\n            # result of our operations, so we have to clamp it so that\n            # the result makes sense.\n            input_img.data.clamp_(0, 1)\n\n            optimizer.zero_grad()\n            model(input_img)\n            style_score = 0\n            content_score = 0\n\n            # Construct the cumulative loss function and backpropogate.\n            for sl in style_losses:\n                style_score += sl.loss\n            for cl in content_losses:\n                content_score += cl.loss\n            style_score *= style_weight\n            content_score *= content_weight\n            loss = style_score + content_score\n            loss.backward()\n\n            run[0] += 1\n            if run[0] % 50 == 0:\n                print(\"run {}:\".format(run))\n                print('Style Loss : {:4f} Content Loss: {:4f}'.format(\n                    style_score.item(), content_score.item()))\n                print()\n\n            return style_score + content_score\n\n        optimizer.step(closure)\n\n    # a last correction...\n    input_img.data.clamp_(0, 1)\n\n    return input_img","bb7b3df6":"output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n                            content_img, style_img, input_img)","a36e6019":"unloader = transforms.ToPILImage()\n\ndef imshow(tensor, title=None):\n    image = tensor.cpu().clone()  # we clone the tensor to not do changes on it\n    image = image.squeeze(0)      # remove the fake batch dimension\n    image = unloader(image)\n    plt.imshow(image)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001) # pause a bit so that plots are updated\n\nimshow(output)","0af1337f":"LBFGS is the optimizer suggested by the original author of this implementation [here](https:\/\/discuss.pytorch.org\/t\/pytorch-tutorial-for-neural-transfert-of-artistic-style\/336).","9ec3c815":"# PyTorch GAN Style Transfer Worked Example\n\nThis notebook is a run of the model presented in the PyTorch tutorial section [Neural Transfer Using PyTorch](https:\/\/pytorch.org\/tutorials\/advanced\/neural_style_tutorial.html).\n\nFirst we reproduce this model, then we dissect it to understand how it works.\n\n## High-level description\n\nThis demo showcases building a generative adversarial network, or GAN, for performing style transfer.","4514b2d0":"How were the layers at which the losses were attached chosen? This is unknown to me.\n\nAs for the definitions of the losses themselves, the original document has good descriptions:\n* https:\/\/pytorch.org\/tutorials\/advanced\/neural_style_tutorial.html#content-loss\n* https:\/\/pytorch.org\/tutorials\/advanced\/neural_style_tutorial.html#style-loss"}}