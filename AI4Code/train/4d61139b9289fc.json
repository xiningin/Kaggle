{"cell_type":{"cfea346c":"code","9790a4be":"code","e7e95f61":"code","671012f7":"code","6f1462fb":"code","ed5f6ce1":"code","4ee84e1f":"code","fdc0a378":"code","50c6c83b":"code","947cc6be":"code","4f6b9b46":"code","bf381378":"code","5e7825b4":"code","1ab0be77":"code","bd482133":"code","09ba285c":"code","bbfd0952":"code","6ded200c":"code","eb66210f":"code","5b0d4997":"code","06617bd5":"markdown","60c188aa":"markdown","ef2fa4c4":"markdown","35ef3ea0":"markdown","2456637a":"markdown","cf22ea13":"markdown"},"source":{"cfea346c":"!pip install faraway","9790a4be":"import numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport plotly.graph_objects as go\nimport faraway.datasets.seatpos\nimport seaborn as sns\nimport matplotlib.pyplot as plt","e7e95f61":"# Gerando um dataset sint\u00e9tico de vari\u00e1veis para explicitar a Regress\u00e3o Linear em Python.\n\nn_amostras = 100\nX = np.array([np.random.normal(2, 40, n_amostras), np.random.normal(5, 5, n_amostras)]).T\ne = np.random.normal(0, 50, n_amostras)\n\ny = [(2*i[0]) + (0.5*i[1]) + 4 for i in X] + e\ny_real = [(2*i[0]) + (0.5*i[1]) + 4 for i in X]","671012f7":"#Usando o m\u00e9todo Linear Regression do sklearn, para realizar a Regress\u00e3o Linear no dataset sint\u00e9tico gerado anteriormente.\n\nreg = LinearRegression().fit(X, y_real)\n\nbeta = np.append(reg.coef_, reg.intercept_)\nbeta","6f1462fb":"# Calculamos o Erro Quadr\u00e1tico M\u00e9dio da predi\u00e7\u00e3o realizada por meio do m\u00e9todo 'mean_squared_error' do sklearn.\nmean_squared_error(y_real, reg.predict(X))","ed5f6ce1":"# Realizaremos um plot 3D para a regress\u00e3o linear ser melhor visualizada.\n\nfig = go.Figure(data = go.Scatter3d(x=X[:,0], y=X[:,1], z=y, mode='markers'))\n\nx_plot,y_plot = np.meshgrid(X[:,0], X[:,1])\n\nz = beta[0]*x_plot + beta[1]*y_plot + beta[2]\n\n\nfig.add_trace(\n    go.Surface(\n        x=tuple(x_plot),\n        y=tuple(y_plot),\n        z=tuple(z),\n        colorscale=[[0, 'rgb(44, 160, 44)'], [1, 'rgb(44, 160, 44)']]\n    )\n)\n\n\nfig.show()","4ee84e1f":"# Usando o m\u00e9todo linalg.svd do numpy na matriz de features constru\u00edda anteriormente.\nU,s_values,V = np.linalg.svd(X)\n\nS = np.zeros((U.shape[0],V.shape[1]))\nnp.fill_diagonal(S, s_values)\n\ns_values","fdc0a378":"X_reconstructed = U@S@V","50c6c83b":"# Vemos que o SVD foi capaz de reconstruir completamente a matriz X, de maneira que o valor da norma da subtra\u00e7\u00e3o de\n#X original pelo X reconstruido pelo SVD \u00e9 meramente erro num\u00e9rico da linguagem Python.\nnp.linalg.norm(X - X_reconstructed)","947cc6be":"#Realizando uma redu\u00e7\u00e3o de dimensionalidade de X, com 2 valores singulares, estamos jogando X para a dimens\u00e3o 2.\nX_red_dim = U[:,:2]@S[:2,:2]@V[:2, :]","4f6b9b46":"# Vemos que desta vez a norma n\u00e3o \u00e9 mais 0, uma vez que a redu\u00e7\u00e3o de dimensionalidade foi realizada e 2 valores singulares\n#n\u00e3o s\u00e3o capazes de reconstruir X completamente.\n\nnp.linalg.norm(X - X_red_dim)","bf381378":"# Carregando o dataset de assentos em avi\u00f5es do dispon\u00edvel em https:\/\/github.com\/julianfaraway\/faraway.\n\nseatpos = faraway.datasets.seatpos.load()\nseatpos.head()","5e7825b4":"# Matriz de correla\u00e7\u00f5es entre as vari\u00e1veis do dataset.\n\nseatpos.corr()","1ab0be77":"# Dividindo as vari\u00e1veis entre preditoras e a vari\u00e1vel que ser\u00e1 predita. Em terminologia de Data Science s\u00e3o as vari\u00e1veis\n#feature e a vari\u00e1vel target.\n\nX = seatpos.loc[:, seatpos.columns != 'hipcenter'].values\ny = seatpos.hipcenter.values","bd482133":"# Usaremos o m\u00e9todo StandardScaler para padronizar as vari\u00e1veis (subtrair a m\u00e9dia e dividir pelo desvio padr\u00e3o).\n\nscaler = StandardScaler()\nX = scaler.fit_transform(X)","09ba285c":"# Podemos observar os valores singulares na matriz de features usando o m\u00e9todo linalg.svd do numpy.\nU,s_values,V = np.linalg.svd(X)\n\nS = np.zeros((U.shape[0],V.shape[1]))\nnp.fill_diagonal(S, s_values)\n\ns_values","bbfd0952":"# Calculamos o quanto cada valor singular \u00e9 respons\u00e1vel pela variabilidade na matriz de features.\nvariancia_explicada = s_values\/np.sum(s_values)\nvariancia_explicada","6ded200c":"# Al\u00e9m da vari\u00e2ncia explicada pelos valores singulares, calculamos essa vari\u00e2ncia cumulativa.\n\nvariancia_explicada_cumulativa = np.cumsum(variancia_explicada)\nvariancia_explicada_cumulativa","eb66210f":"# Abaixo usamos o m\u00e9todo k-fold para treinar o nosso modelo, o m\u00e9todo k-fold divide o dataset em k (entrada do algoritmo)\n#partes iguais, onde 1 dessas k partes \u00e9 separada para teste e as outras k-1 separadas para teste.\n#O algoritmo vai criar k ambientes diferentes de treino e teste, em cada ambiente de treino e teste uma parte diferente\n#ser\u00e1 selecionada para treino.\n# No exemplo abaixo temos dividimos o dataset em 20 partes iguais e, portanto vamos trein\u00e1-lo e test\u00e1-lo 20 vezes, al\u00e9m\n#disso, o Kfold ser\u00e1 aplicado \u00e0 cada n\u00famero de valores singulares na matriz de features, como temos 8 valores singulares,\n#realizaremos 8 kfolds para o teste.\n\nkf = KFold(n_splits=20, shuffle=True, random_state=5)\nreg = LinearRegression()\n\nMSE_train = []\nMSE_test = []\nfor i in range(len(s_values)):\n    X_t = U[:,:i+1]@S[:i+1,:i+1]@V[:i+1, :]\n    MSE_train_t = []\n    MSE_test_t = []\n    \n    for train_index, test_index in kf.split(X_t):\n        reg.fit(X_t[train_index], y[train_index])\n        y_pred_train = reg.predict(X_t[train_index])\n        y_pred_test = reg.predict(X_t[test_index])\n        MSE_train_t.append(mean_squared_error(y[train_index], y_pred_train))\n        MSE_test_t.append(mean_squared_error(y[test_index], y_pred_test))\n    \n    MSE_train.append(np.mean(MSE_train_t))\n    MSE_test.append(np.mean(MSE_test_t))\n    ","5b0d4997":"#Podemos ver um plot do erro quadr\u00e1tico m\u00e9dio para cada n\u00famero de valores singulares.\n\nsns.set()\nplt.figure(figsize=(14,7))\nsns.lineplot(y=MSE_train, x=np.arange(1, 9, 1), label='Erro quadr\u00e1tico m\u00e9dio no dataset de treino')\nsns.lineplot(y=MSE_test, x=np.arange(1, 9, 1), label='Erro quadr\u00e1tico m\u00e9dio no dataset de teste')\nplt.xticks(ticks=np.arange(1, 9, 1))\nplt.xlabel(\"Quantidade de valores singulares\")\nplt.ylabel(\"Erro quadr\u00e1tico m\u00e9dio da previs\u00e3o gerada pela Regress\u00e3o Linear\")\nplt.show()","06617bd5":"# <center> O modelo Linear<\/center>\n<p>Para usar uma reta que determine a rela\u00e7\u00e3o entre duas vari\u00e1veis, assumimos que a rela\u00e7\u00e3o entre as vari\u00e1veis \u00e9 a seguinte:<\/p>\n<br>\n\\begin{equation}\nY = X\\beta + e\n\\end{equation}\n<p>Assumindo que $N$ \u00e9 o n\u00famero de observa\u00e7\u00f5es que temos das vari\u00e1veis, $Y$ e $e$ s\u00e3o vetores de $N$ elementos, X \u00e9 uma matriz de tamanho $N \\times p$, onde p \u00e9 o n\u00famero de vari\u00e1veis usadas para predizer $Y$, al\u00e9m disso $\\beta$ \u00e9 um vetor de $p$ elementos. <\/p>\nAssumindo $p=3$, teriamos a seguinte fun\u00e7\u00e3o de regress\u00e3o:\n<br>\n\\begin{equation}\ny_i = x_{i1}\\beta_{1} + x_{i2}\\beta_{2} + x_{i3}\\beta_{3} + e_i\n\\end{equation}\n<br>\nGeralmente, em uma regress\u00e3o, assumimos uma das vari\u00e1veis como $x_j = 1$, ou seja, a real equa\u00e7\u00e3o para $y_i$ quando \ntemos $p=3$:\n<br>\n<br>\n\\begin{equation}\ny_i = x_{i1}\\beta_{1} + x_{i2}\\beta_{2} + \\beta_{3} + e_i\n\\end{equation}\n<br>\nO coeficiente $\\beta_3$ \u00e9 chamado de termo independente.","60c188aa":"# Uso da Decomposi\u00e7\u00e3o em Valores Singulares na Regress\u00e3o Linear","ef2fa4c4":"A partir do gr\u00e1fico acima, podemos ver que o erro quadr\u00e1tico m\u00e9dio atinge o seu menor valor com 2 valores singulares.<br>\nRealizar a redu\u00e7\u00e3o de dimensionalidade pelo SVD no nosso dataset associou um vi\u00e9s \u00e0 nossa previs\u00e3o, uma vez que agora diminuimos o n\u00famero de vari\u00e1veis e o nosso modelo tem menos poder de previs\u00e3o.<br>\nNo entanto, o menor n\u00famero de vari\u00e1veis usadas para predizer o <i>target<\/i> confere uma menor vari\u00e2ncia nos resultados da <b>Regress\u00e3o Linear<\/b>, o que confere um menor <b>Erro Quadr\u00e1tico M\u00e9dio<\/b>, apesar do modelo enviesado.","35ef3ea0":"# O dilema do vi\u00e9s e vari\u00e2ncia\n\nO dilema do vi\u00e9s vari\u00e2ncia \u00e9 um problema inerente \u00e0 qualquer problema de aprendizado supervisionado.\n<br> Tal dilema surge associado \u00e0 complexidade do modelo escolhido, no caso da regress\u00e3o linear a complexidade se refere ao n\u00famero de coeficiente usados no modelo, ou o n\u00famero de vari\u00e1veis que estamos usando para predizer o <i>target<\/i>.\n\n<img src=\"https:\/\/www.ncbi.nlm.nih.gov\/books\/NBK543534\/bin\/463627_1_En_8_Fig3_HTML.jpg\" alt=\"Drawing\" style=\"width: 600px;\"\/>\n\nA ideia \u00e9 que modelos de predi\u00e7\u00e3o mais complexos tem a capacidade de aprender melhor o seu conjunto de dados, no entanto, s\u00e3o pouco generaliz\u00e1veis, ou seja, \u00e9 conferida uma grande vari\u00e2ncia \u00e0 estima\u00e7\u00e3o das componentes que compoem aquele modelo.<br>\nQuando um modelo de aprendizado de m\u00e1quina aprende o conjunto de dados de treino melhor do que deveria, perdendo a capacidade de generaliza\u00e7\u00e3o, dizemos que cocorreu um <b>overfitting<\/b>.<br>\nAl\u00e9m disso, modelos menos complexos possuem menos potencial de aprendizado, no entanto s\u00e3o muito generaliz\u00e1veis, e por isso, por muitas vezes conseguem predizer situa\u00e7\u00f5es ainda n\u00e3o vistas melhor do que modelos mais complexos. O problema com essa generaliza\u00e7\u00e3o \u00e9 que por muitas vezes, tais modelos n\u00e3o s\u00e3o capacidades de reconhecer suficientemente os padr\u00f5es dentro de conjunto de dados.<br>\nQuando um modelo de aprendizado de m\u00e1quina n\u00e3o \u00e9 capaz de aprender o conjunto de dados de treino, por conta de uma baixo n\u00edvel de complexidade do modelo escolhido, dizemos que cocorreu um <b>underfitting<\/b>.<br>","2456637a":"# <center> PCR - Principal Components Regression <\/center>\n<p>O PCR (Regress\u00e3o por Componentes Principais, em portugu\u00eas), \u00e9 a ideia de substituir o $X$ na f\u00f3rmula original da regress\u00e3o linear pelo seu SVD. Assim, veremos que temos alguns benef\u00edcios em certos casos.<\/p>\n<br>\n\\begin{equation}\nY = X\\beta + e\n\\end{equation}\n\\begin{equation}\nY = U\\Sigma V^T\\beta + e\n\\end{equation}\nIntroduzindo uma matriz $\\alpha_{r \\times 1}$\n\\begin{equation}\n\\alpha = \\Sigma V^T\\beta\n\\end{equation}    \nA nova equa\u00e7\u00e3o da regress\u00e3o linear \u00e9:\n\\begin{equation}\nY = U\\alpha + e\n\\end{equation}    \n\nA partir de tais resultados \u00e9 poss\u00edvel chegar em uma express\u00e3o para a vari\u00e2ncia de $\\beta$:<br>\nCome\u00e7amos por escrever $\\hat{\\beta}$ em fun\u00e7\u00e3o de $\\hat{\\alpha}$ usando a decomposi\u00e7\u00e3o SVD de $X$:\n\\begin{equation}\n\\hat{\\beta} = V\\Sigma^{-1}\\hat{\\alpha}\n\\end{equation}\nOu seja, temos para uma entrada $i$ qualquer de $\\beta$:\n\\begin{equation}\n\\hat{\\beta_i} = \\sum_{m=0}^{p}\\frac{V_{im}}{\\Sigma^{-1}_{m}}\\hat{\\alpha_m}\n\\end{equation}\nOu seja, para $Var[\\hat{\\beta_i}]$:\n\\begin{equation}\nVar[\\hat{\\beta_i}] = Var[\\sum_{m=0}^{p}\\frac{V_{im}}{\\Sigma^{-1}_{m}}\\hat{\\alpha_m}]\n\\end{equation}\n\\begin{equation}\n= E\\left[\\left(\\sum_{m=0}^{p}\\frac{V_{im}}{\\Sigma^{-1}_{m}}\\hat{\\alpha_m}\\right)^2\\right] - E\\left[\\sum_{m=0}^{p}\\frac{V_{im}}{\\Sigma^{-1}_{m}}\\hat{\\alpha_m}\\right]^2 \n\\end{equation}\n\\begin{equation}\n= E\\left[\\sum_{m=0}^{p}\\sum_{n=0}^{p}\\frac{V_{im}}{\\Sigma^{-1}_{m}}\\hat{\\alpha_m}\\frac{V_{in}}{\\Sigma^{-1}_{n}}\\hat{\\alpha_n}\\right] - \\left(\\sum_{m=0}^{p}E\\left[\\frac{V_{im}}{\\Sigma^{-1}_{m}}\\hat{\\alpha_m}\\right]\\right)^2 \n\\end{equation}\n\\begin{equation}\n= \\sum_{m=0}^{p}\\sum_{n=0}^{p}E\\left[\\frac{V_{im}}{\\Sigma^{-1}_{m}}\\hat{\\alpha_m}\\frac{V_{in}}{\\Sigma^{-1}_{n}}\\hat{\\alpha_n}\\right] - \\sum_{m=0}^{p}\\sum_{n=o}^{p}E\\left[\\frac{V_{im}}{\\Sigma^{-1}_{m}}\\hat{\\alpha_m}\\right]E\\left[\\frac{V_{in}}{\\Sigma^{-1}_{n}}\\hat{\\alpha_n}\\right]\n\\end{equation}\n\\begin{equation}\n= \\sum_{m=0}^{p}\\sum_{n=0}^{p} \\left(\\frac{V_{im}}{\\Sigma^{-1}_{m}}\\frac{V_{in}}{\\Sigma^{-1}_{n}}E\\left[\\hat{\\alpha_m}\\hat{\\alpha_n}\\right] - \\frac{V_{im}}{\\Sigma^{-1}_{m}}\\frac{V_{in}}{\\Sigma^{-1}_{n}}E[\\hat{\\alpha_m}]E[\\hat{\\alpha_n}] \\right)\n\\end{equation}\n\\begin{equation}\n= \\sum_{m=0}^{p}\\sum_{n=0}^{p} \\frac{V_{im}}{\\Sigma^{-1}_{m}}\\frac{V_{in}}{\\Sigma^{-1}_{n}}\\left(E\\left[\\hat{\\alpha_m}\\hat{\\alpha_n}\\right] - E[\\hat{\\alpha_m}]E[\\hat{\\alpha_n}] \\right)\n\\end{equation}\n\\begin{equation}\n= \\sum_{m=0}^{p}\\sum_{n=0}^{p}\\frac{V_{im}}{\\Sigma^{-1}_{m}}\\frac{V_{in}}{\\Sigma^{-1}_{n}} Cov(\\hat{\\alpha_m},\\hat{\\alpha_n})\n\\end{equation}\nDesse resultado, \u00e9 poss\u00edvel perceber que os valores singulares na diagonal da matriz $\\Sigma$, s\u00e3o inversamentes proporcionais \u00e0 vari\u00e2ncia de $\\beta$, ou seja, valores singulares muito pequenos far\u00e3o a componente $\\left(\\frac{V_{im}}{\\Sigma^{-1}_{m}}\\frac{V_{in}}{\\Sigma^{-1}_{n}}\\right )$ dar um resultado muito grande, atribuindo maior incerteza ao resultado de $\\beta_i$.","cf22ea13":"# <center> A decomposi\u00e7\u00e3o em Valores Singulares<\/center>\n<p>Dada uma matriz $X_{N \\times p}$, \u00e9 poss\u00edvel decompor-la da seguinte forma: <\/p>\n<br>\n\\begin{equation}\nX_{N \\times p} = U_{N \\times r}\\Sigma_{r \\times r}V^T_{r \\times p}\n\\end{equation}\n<p>Onde r \u00e9 o posto da matriz $X_{N \\times p}$, $U_{N \\times r}$ e $V_{r \\times p}$ s\u00e3o matrizes ortogonais<b>*<\/b>, e $\\Sigma_{r \\times r}$ \u00e9 uma matriz diagonal.<\/p>\n<p>\u00c0 essa decomposi\u00e7\u00e3o \u00e9 dado o nome de <b>SVD<\/b>, ou <b>Decomposi\u00e7\u00e3o em Valores Singulares<\/b>.<\/p>\n<p>Na <b>decomposi\u00e7\u00e3o em Valores Singulares<\/b>, devemos uma especial aten\u00e7\u00e3o \u00e0 matriz $\\Sigma$, que \u00e9 da seguinte forma:<\/p>\n\\begin{equation}\n\\Sigma = \\begin{bmatrix}\n                \\sigma_1 & 0 & 0 & ... & 0 & 0\\\\\n                 0 & \\sigma_2 & 0 & ... & 0 & 0\\\\\n                 0 & 0 & \\sigma_3 & ... & 0 & 0\\\\\n                 ... & ... & ... & ... & ... & ...\\\\\n                 0 & 0 & 0 & ... & \\sigma_{r-1} & 0\\\\\n                 0 & 0 & 0 & ... & 0 & \\sigma_r\\\\\n            \\end{bmatrix}\n\\end{equation}\n<p>O valores na diagonal de $\\Sigma$, s\u00e3o chamados de <b>Valores Singulares<\/b> de $X$.<\/p>\n\n---\n<p>Al\u00e9m disso, vamos definir a matrix $\\Sigma_{d \\times d}$, com d $\\leq$ r, como sendo:<\/p>\n\\begin{equation}\n\\Sigma_{d \\times d} = \\begin{bmatrix}\n                \\sigma_1 & 0 & 0 & ... & 0 & 0 & ... & 0 & 0\\\\\n                 0 & \\sigma_2 & 0 & ... & 0 & 0 & ... & 0 & 0\\\\\n                 0 & 0 & \\sigma_3 & ... & 0 & 0 & ... & 0 & 0\\\\\n                 ... & ... & ... & ... & ... & ... & ... & ... & ...\\\\\n                 ... & ... & ... & ... & \\sigma_{d - 1} & 0 & ... & ... & ...\\\\\n                 ... & ... & ... & ... & ... & \\sigma_{d} & ... & 0 & 0\\\\\n                 ... & ... & ... & ... & ... & ... & ... & ... & ...\\\\\n                 0 & 0 & 0 & ... & & 0 & 0 & ... & 0 & 0\\\\\n                 0 & 0 & 0 & ... & & 0 & 0 & ... & 0 & 0\\\\\n            \\end{bmatrix}\n\\end{equation}\n<p>Ou seja, \u00e9 a mesma matriz, s\u00f3 que substituindo todos os valores singulares maiores que $d$ por 0.<\/p>\n\n---\n\n<p>O SVD \u00e9 comumente usado para realizar a tarefa de <b>Redu\u00e7\u00e3o de Dimensionalidade<\/b>, no contexto de Ci\u00eancia de Dados, Redu\u00e7\u00e3o de Dimensionalidade \u00e9, basicamente, reduzir o n\u00famero de colunas em um dataset.<\/p>\n<p>O SVD \u00e9 de especial interesse nesse contexto, por conta do que \u00e9 conhecido como <b>Teorema de Eckart-Young<\/b>, o teorema diz que dado uma dimens\u00e3o $d$, a matriz $\\tilde{X}$, definida abaixo como: <\/p>\n\n\\begin{equation}\n\\tilde{X}_{N \\times p} = U_{N \\times r}\\Sigma_{d \\times d}V^T_{r \\times p}\n\\end{equation}\n\n<p>A matriz $\\tilde{X}$, tem como propriedade $posto(\\tilde{X}) = d$, e, al\u00e9m disso, temos que $\\tilde{X}$ \u00e9 a matriz de posto $d$, cuja $||X - \\tilde{X}||^2$ \u00e9 m\u00ednima.<\/p>\n<p>Ou seja, $\\tilde{X}$ \u00e9 a matriz de posto $d$ que melhor aproxima linearmente $X$.<\/p>\n\n---\n\n<p><b>*<\/b>: Se uma matriz $A$ \u00e9 ortogonal, ent\u00e3o:<\/p>\n<ul>\n<li>\n$\ndet(A) =  \\pm 1\n$\n<\/li>\n<li>\n$\nA^{-1} = A^T\n$\n<\/li>\n<\/ul>\n<p>O determinante de matrizes ortogonais ($\\pm 1$), indicam que matrizes ortogonais n\u00e3o alargam nem encurtam o espa\u00e7o vetorial, mais especificamente, as transforma\u00e7\u00f5es produzidas por matrizes ortogonais s\u00e3o exclusivamente rota\u00e7\u00f5es.<\/p>\n<p>Olhemos com cuidado para a seguinte matriz, $U_{N \\times r}\\Sigma_{d \\times d}$, podemos ver que ela \u00e9 uma matriz com $N$ linhas e $d$ colunas, tendo como imagem um hiperplano de dimens\u00e3o $d$.<\/p>\n<p>Observe agora, que a matriz $\\tilde{X}$, definida por $U_{N \\times r}\\Sigma_{d \\times d}V^T_{r \\times p}$, tem como imagem uma hiperplano de dimens\u00e3o $d$ em $\\mathbb{R}^r$.<\/p>\n<p>Na verdade, $V^T$ est\u00e1 levando a imagem da matriz $U_{N \\times r}\\Sigma_{d \\times d}$ para $\\mathbb{R}^r$, sem mudar a dist\u00e2ncia entre os pontos.<\/p>\n<p>Dessa perspectiva, podemos interpretar a matriz $U_{N \\times r}\\Sigma_{d \\times d}$, como uma aproxima\u00e7\u00e3o de dimens\u00e3o $d$ para $X$, originalmente de dimens\u00e3o r.<\/p>\n\n---\n\n<p>\u00c0 este processo damos o nome de <b>Redu\u00e7\u00e3o de Dimensionalidade<\/b>.<\/p>"}}