{"cell_type":{"9fbbb12f":"code","221743da":"code","5cfefa3d":"code","fc0a54e0":"code","6aa7dbed":"code","5eb485b1":"code","4f103a94":"code","346497d0":"code","ac620ee6":"code","004ca8fe":"code","c380450c":"code","a6412e82":"code","869d1513":"code","dc40affd":"code","a5a64cf6":"code","ff11f19e":"code","73b61809":"code","50efe258":"code","b4f9da06":"code","2368b257":"markdown","895debe4":"markdown","ccdcee7e":"markdown","fb212854":"markdown","14cccc7b":"markdown","6655eac0":"markdown","b57fe94f":"markdown"},"source":{"9fbbb12f":"import numpy as np\nimport pandas as pd\nimport time\nimport h5py\nimport copy\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom torchvision import models\nimport matplotlib.pyplot as plt\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2","221743da":"class SignsDataset(Dataset):\n    def __init__(self, path, sub, transform=None):\n        self.transform = transform\n        self.sub = sub\n        subtype = {'train' : 'Signs_Data_Training.h5',\n                   'test' : 'Signs_Data_Testing.h5'}\n        self.dataset = h5py.File(path + subtype[sub], \"r\")\n        self.classes = np.array(self.dataset[\"list_classes\"][:])\n\n    def __getitem__(self, idx):\n        image = self.dataset[self.sub + \"_set_x\"][idx]\n        label = self.dataset[self.sub + \"_set_y\"][idx]\n        \n        if(self.transform):\n            image = self.transform(image=image)[\"image\"]\n        \n        label = torch.tensor(label).type(torch.LongTensor)\n        return image, label\n\n    def __len__(self):\n        return self.dataset[self.sub + \"_set_x\"].shape[0]","5cfefa3d":"transform = A.Compose(\n    [\n        A.Resize(height=299, width=299),\n        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n        A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2(),\n    ]\n)","fc0a54e0":"path = '..\/input\/signs-detection-dataset\/'\ntrain_ds = SignsDataset(path, sub='train', transform=transform)\ntest_ds = SignsDataset(path, sub='test', transform=transform)","6aa7dbed":"image, label = train_ds[0]\nplt.imshow(image.permute(1, 2, 0))","5eb485b1":"# device cpu or gpu\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\nmodel_names = [\"resnet\", \"alexnet\", \"vgg\", \"squeezenet\", \"densenet\", \"inception\"]\n\n# Number of classes in the dataset\nnum_classes = len(train_ds.classes)\n\n# Batch size for training (change depending on how much memory you have)\nbatch_size = 8\n\n# Number of epochs to train for\nnum_epochs = 25\n\n# Flag for feature extracting. When False, we finetune the whole model,\n#   when True we only update the reshaped layer params\nfeature_extract = True","4f103a94":"def set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False","346497d0":"train_dataloader = DataLoader(train_ds, batch_size=batch_size,\n                              shuffle=True, num_workers=1)\n\ntest_dataloader = DataLoader(test_ds, batch_size=batch_size,\n                              shuffle=True, num_workers=1)\n\ndataloaders = { 'train' : train_dataloader,\n                'val' : test_dataloader }","ac620ee6":"def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n    # Initialize these variables which will be set in this if statement. Each of these\n    #   variables is model specific.\n    model_ft = None\n    input_size = 0\n\n    if model_name == \"resnet\":\n        \"\"\" Resnet18\n        \"\"\"\n        model_ft = models.resnet18(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n        input_size = 224\n\n    elif model_name == \"alexnet\":\n        \"\"\" Alexnet\n        \"\"\"\n        model_ft = models.alexnet(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier[6].in_features\n        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n        input_size = 224\n\n    elif model_name == \"vgg\":\n        \"\"\" VGG11_bn\n        \"\"\"\n        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier[6].in_features\n        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n        input_size = 224\n\n    elif model_name == \"squeezenet\":\n        \"\"\" Squeezenet\n        \"\"\"\n        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n        model_ft.num_classes = num_classes\n        input_size = 224\n\n    elif model_name == \"densenet\":\n        \"\"\" Densenet\n        \"\"\"\n        model_ft = models.densenet121(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier.in_features\n        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n        input_size = 224\n\n    elif model_name == \"inception\":\n        \"\"\" Inception v3\n        Be careful, expects (299,299) sized images and has auxiliary output\n        \"\"\"\n        model_ft = models.inception_v3(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        # Handle the auxilary net\n        num_ftrs = model_ft.AuxLogits.fc.in_features\n        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n        # Handle the primary net\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n        input_size = 299\n\n    else:\n        print(\"Invalid model name, exiting...\")\n        exit()\n\n    return model_ft, input_size\n\n# Initialize the model for this run\nmodel_fts = []\nfor model_name in model_names:\n    model_ft, _ = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n    model_fts.append(model_ft)","004ca8fe":"def train_model(model_name, model, dataloaders, criterion, optimizer, \n                num_epochs=25, printing=False, is_inception=False):\n    since = time.time()\n\n    val_acc_history = []\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    # Get model outputs and calculate loss\n                    # Special case for inception because in training it has an auxiliary output. In train\n                    #   mode we calculate the loss by summing the final output and the auxiliary output\n                    #   but in testing we only consider the final output.\n                    if is_inception and phase == 'train':\n                        # From https:\/\/discuss.pytorch.org\/t\/how-to-optimize-inception-model-with-auxiliary-classifiers\/7958\n                        outputs, aux_outputs = model(inputs)\n                        loss1 = criterion(outputs, labels)\n                        loss2 = criterion(aux_outputs, labels)\n                        loss = loss1 + 0.4*loss2\n                    else:\n                        outputs = model(inputs)\n                        loss = criterion(outputs, labels)\n\n                    _, preds = torch.max(outputs, 1)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss \/ len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() \/ len(dataloaders[phase].dataset)\n\n            if(printing):\n                print('Epoch:{}, Type:{}, Loss: {:.4f}, Acc: {:.4f}'\n                       .format(epoch, phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n            if phase == 'val':\n                val_acc_history.append(epoch_acc)\n\n    time_elapsed = time.time() - since\n    print('Model {}'.format(model_name))\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n    print()\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model, val_acc_history, best_acc","c380450c":"models_fts_ = []\nfor i, model_ft in enumerate(model_fts):\n    # Send the model to GPU\n    model_ft = model_ft.to(device)\n\n    # Gather the parameters to be optimized\/updated in this run. If we are\n    #  finetuning we will be updating all parameters. However, if we are\n    #  doing feature extract method, we will only update the parameters\n    #  that we have just initialized, i.e. the parameters with requires_grad\n    #  is True.\n    params_to_update = model_ft.parameters()\n    print(\"Params to learn for model {}:\"\n             .format(model_names[i]))\n    if feature_extract:\n        params_to_update = []\n        for name,param in model_ft.named_parameters():\n            if param.requires_grad == True:\n                params_to_update.append(param)\n                print(\"\\t\",name)\n    else:\n        for name,param in model_ft.named_parameters():\n            if param.requires_grad == True:\n                print(\"\\t\",name)\n    models_fts_.append(model_ft)\n\n# Observe that all parameters are being optimized\noptimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)","a6412e82":"# Setup the loss fxn\ncriterion = nn.CrossEntropyLoss()\n\n# Train and evaluate\nmodel_fts, hists, best_accs = [], [], []\nfor i, model_ft in enumerate(models_fts_):\n    model_ft, hist, best_acc = train_model(model_names[i], model_ft, dataloaders, criterion, \n                                 optimizer_ft, num_epochs=num_epochs, is_inception=(model_names[i]=='inception'))\n    model_fts.append(model_ft)\n    hists.append(hist)\n    best_accs.append(best_acc)","869d1513":"fig, axs = plt.subplots(3, 2, figsize=(10,15))\naxs[0, 0].plot(np.arange(len(hists[0])), hists[0], 'tab:blue')\naxs[0, 0].set_title(\"Accuracy \" + model_names[0], color='blue')\n\naxs[0, 1].plot(np.arange(len(hists[0])), hists[1], 'tab:red')\naxs[0, 1].set_title(\"Accuracy \" + model_names[1], color='red')\n\naxs[1, 0].plot(np.arange(len(hists[0])), hists[2], 'tab:orange')\naxs[1, 0].set_title(\"Accuracy \" + model_names[2], color='orange')\n\naxs[1, 1].plot(np.arange(len(hists[0])), hists[3], 'tab:green')\naxs[1, 1].set_title(\"Accuracy \" + model_names[3], color='green')\n\naxs[2, 0].plot(np.arange(len(hists[0])), hists[4], 'tab:purple')\naxs[2, 0].set_title(\"Accuracy \" + model_names[4], color='purple')\n\naxs[2, 1].plot(np.arange(len(hists[0])), hists[5], 'tab:brown')\naxs[2, 1].set_title(\"Accuracy \" + model_names[5], color='brown')","dc40affd":"def get_hist_weights(best_accs):\n    total_accs = sum(best_accs)\n    values = [h[-1].item() * total_accs * best_accs[i]\n                  for i,h in enumerate(hists)]\n    return values","a5a64cf6":"# use the best accuracies\n# to multiply the values by the percentage\n# without training\nget_hist_weights(best_accs)","ff11f19e":"class Ensemble(nn.Module):\n    \n    def __init__(self, models, weights, batch_size, num_classes):\n        super(Ensemble, self).__init__()\n        self.models = models\n        self.weights = weights\n        self.num_classes = num_classes\n        self.batch_size = batch_size\n        \n    def forward(self, x):\n        out = torch.zeros((self.batch_size, self.num_classes)\n                         ).type(torch.cuda.FloatTensor)\n        for i, model in enumerate(self.models):\n            out += model(x) * self.weights[i]\n           \n        return torch.softmax(out, dim=1)","73b61809":"ensemble = Ensemble(model_fts, get_hist_weights(best_accs), batch_size, num_classes)","50efe258":"def get_accuracy(model, validation_data, batch_size):\n    running_corrects = 0\n\n    for inputs, labels in validation_data:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        outputs = model(inputs)\n        _, preds = torch.max(outputs, 1)\n\n        running_corrects += torch.sum(preds == labels.data)\n\n    total = len(dataloaders['val']) * batch_size\n    total_accuracy = running_corrects \/ total\n    \n    return total_accuracy","b4f9da06":"acc = get_accuracy(ensemble, dataloaders['val'], batch_size)\nprint('Ensemble Accuracy:{:1.3f}'.format(acc))","2368b257":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:black; background:white; border:1px dashed;\" role=\"tab\" aria-controls=\"home\"><center>Train<\/center><\/h3>","895debe4":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:black; background:white; border:1px dashed;\" role=\"tab\" aria-controls=\"home\"><center>Analysis<\/center><\/h3>","ccdcee7e":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:black; background:white; border:1px dashed;\" role=\"tab\" aria-controls=\"home\"><center>Ensemble<\/center><\/h3>","fb212854":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:black; background:white; border:1px dashed;\" role=\"tab\" aria-controls=\"home\"><center>Models<\/center><\/h3>","14cccc7b":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:black; background:white; border:1px dashed;\" role=\"tab\" aria-controls=\"home\"><center>Dataset<\/center><\/h3>","6655eac0":"<pre style=\"border: 1px dashed;\">\n<div style=\"margin-left: 35%;\">\n$$$$$$$$$$$$$$$$$$$$$$$$$\n$$$$$$$$$$$$$$$$$$$$$$$$$\n$$$$$'`$$$$$$$$$$$$$'`$$$\n$$$$$$  $$$$$$$$$$$  $$$$\n$$$$$$$  '$\/ `\/ `$' .$$$$\n$$$$$$$$. i  i  \/! .$$$$$\n$$$$$$$$$.--'--'   $$$$$$\n$$^^$$$$$'        J$$$$$$\n$$$   ~\"\"   `.   .$$$$$$$\n$$$$$e,      ;  .$$$$$$$$\n$$$$$$$$$$$.'   $$$$$$$$$\n$$$$$$$$$$$$.    $$$$$$$$\n$$$$$$$$$$$$$     $$$$$$$\n\n<b>Signs Detection<\/b>\n      Transfer Learning\n            by Alin Cijov\n<\/div>\n<\/pre>","b57fe94f":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:black; background:white; border:1px dashed;\" role=\"tab\" aria-controls=\"home\"><center>Parameters<\/center><\/h3>"}}