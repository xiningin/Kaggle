{"cell_type":{"99b40a5a":"code","f92a6bf3":"code","5922819c":"code","60191ded":"code","ca8a5e77":"code","ce3c1454":"code","10e1c063":"code","fa534db9":"code","eacb9689":"code","88a62a28":"code","ca579edf":"code","842a8388":"code","594dd38b":"code","2f7a5b84":"code","a412ef96":"code","39f29f0f":"code","914563d3":"code","9a42327a":"code","4b3b564f":"code","f89adde2":"code","e99cdb67":"code","45e32ab2":"code","db402bb7":"code","2f8e4f1b":"code","1e4fadcb":"code","e1de3f89":"code","f85059d4":"code","8e877de0":"code","edb50482":"code","72252405":"code","996c2fd0":"code","e542e2ef":"code","7bef40e1":"code","56965258":"code","1243f82a":"code","b97bb4ad":"code","2a997169":"code","c7f8db7d":"code","9700653a":"code","f3af2c7e":"code","56c97cb8":"code","3d6f1b85":"code","29cecccc":"code","944cd0ae":"code","fbe50a7a":"code","fa2d1275":"code","8fe8b7b0":"code","bf1b594d":"markdown","9d5c9a58":"markdown","080a68b2":"markdown","9968abdd":"markdown","fd5fd7a4":"markdown","c3e48942":"markdown","79e6b5cf":"markdown","4df0e386":"markdown","8d919d81":"markdown","226af546":"markdown","09b637d4":"markdown","5a7aa417":"markdown","cce94a35":"markdown","f612e514":"markdown","304f6931":"markdown","96395fb1":"markdown","afa245b7":"markdown","510268c7":"markdown"},"source":{"99b40a5a":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport dask.bag as db\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.data\nfrom tqdm import tqdm, tqdm_notebook\n\nimport os\nimport random\nimport subprocess\nimport sys\nimport time","f92a6bf3":"import platform\nprint(f'Python version: {platform.python_version()}')\nprint(f'PyTorch version: {torch.__version__}')","5922819c":"# This notebook runs on GPU\nassert torch.cuda.is_available()\n\nDEVICE = torch.device('cuda')\nNUM_GPUS = torch.cuda.device_count()\nassert NUM_GPUS > 0","60191ded":"# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\nimport logging\nlogging.basicConfig(level=logging.INFO)","ca8a5e77":"import argparse\n\ndef define_args(str_list):\n    '''\n    A lite version of args at https:\/\/github.com\/huggingface\/pytorch-pretrained-BERT\/blob\/master\/examples\/run_classifier.py#L565\n    \n    The following flags are set to constant values implicitly thus they're removed from args:\n      * local_rank=-1\n      * fp16=True\n      * cache_dir=''\n      \n    '''\n    parser = argparse.ArgumentParser()\n\n    ## Required parameters\n    parser.add_argument(\"--data_dir\",\n                        default=None,\n                        type=str,\n                        required=True,\n                        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n    parser.add_argument(\"--bert_model\", default=None, type=str, required=True,\n                        help=\"Bert pre-trained model name selected in the list: bert-base-uncased, \"\n                        \"bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, \"\n                        \"bert-base-multilingual-cased, bert-base-chinese.\")\n    parser.add_argument(\"--max_seq_length\",\n                        default=None,\n                        type=int,\n                        required=True,\n                        help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n                             \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n                             \"than this will be padded.\")\n\n    ## Other parameters\n    parser.add_argument(\"--do_lower_case\",\n                        action='store_true',\n                        help=\"Set this flag if you are using an uncased model.\")\n    parser.add_argument(\"--train_batch_size\",\n                        default=32,\n                        type=int,\n                        help=\"Total batch size for training.\")\n    parser.add_argument(\"--eval_batch_size\",\n                        default=8,\n                        type=int,\n                        help=\"Total batch size for eval.\")\n    parser.add_argument(\"--learning_rate\",\n                        default=5e-5,\n                        type=float,\n                        help=\"The initial learning rate for Adam.\")\n    parser.add_argument(\"--begin_epoch\",\n                        default=0,\n                        type=int,\n                        help=\"The begin training epoch, starts from 0.\")\n    parser.add_argument(\"--end_epoch\",\n                        default=1,\n                        type=int,\n                        help=\"The end training epoch, excluded.\")\n    parser.add_argument(\"--warmup_proportion\",\n                        default=0.1,\n                        type=float,\n                        help=\"Proportion of training to perform linear learning rate warmup for. \"\n                             \"E.g., 0.1 = 10%% of training.\")\n    parser.add_argument('--seed',\n                        type=int,\n                        default=42,\n                        help=\"random seed for initialization\")\n    parser.add_argument('--gradient_accumulation_steps',\n                        type=int,\n                        default=1,\n                        help=\"Number of updates steps to accumulate before performing a backward\/update pass.\")\n    parser.add_argument('--fp16',\n                        action='store_true',\n                        help=\"Whether to use 16-bit float precision instead of 32-bit\")\n    parser.add_argument('--loss_scale',\n                        type=float, default=0,\n                        help=\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"\n                             \"0 (default value): dynamic loss scaling.\\n\"\n                             \"Positive power of 2: static loss scaling value.\\n\")\n    parser.add_argument('--verbose', '-v', action='count')\n\n    args = parser.parse_args(str_list)\n\n    args.do_lower_case = 'uncased' in args.bert_model\n    # see dataset https:\/\/www.kaggle.com\/soulmachine\/bert-fine-tuned-for-jigsaw\n    args.output_dir = f'..\/input\/bert-fine-tuned-for-jigsaw\/jigsaw-{args.bert_model}-len-{args.max_seq_length}-{\"fp16\" if args.fp16 else \"fp32\"}'\n\n    return args","ce3c1454":"args = define_args([\n    '--data_dir', '..\/input\/jigsaw-unintended-bias-in-toxicity-classification',\n    '--bert_model', 'bert-base-uncased',\n    '--max_seq_length', '220',\n    '--fp16',\n    '--learning_rate', '2e-5',\n    '--begin_epoch', '0',\n    '--end_epoch', '4',\n    '-v',\n])\nargs","10e1c063":"# If the last checkpoint exists, skip training\nIS_TRAINING = not os.path.exists(f'{args.output_dir}\/epoch-{args.end_epoch-1}')\nIS_TRAINING","fa534db9":"def check_args(args):\n    assert args.begin_epoch < args.end_epoch\n    if args.begin_epoch > 0:\n        assert os.path.exists(f'{args.output_dir}\/epoch-{args.begin_epoch-1}')\n    if IS_TRAINING:\n        for i in range(args.begin_epoch, args.end_epoch):\n            assert not os.path.exists(f'{args.output_dir}\/epoch-{i}')\n    else:\n        assert os.path.exists(f'{args.output_dir}\/epoch-{args.end_epoch-1}')","eacb9689":"check_args(args)","88a62a28":"random.seed(args.seed)\nnp.random.seed(args.seed)\ntorch.manual_seed(args.seed)\ntorch.cuda.manual_seed(args.seed)\ntorch.cuda.manual_seed_all(args.seed)\n\ntorch.backends.cudnn.deterministic = True","ca579edf":"def install_apex():\n    try:\n        import apex\n    except ModuleNotFoundError:\n        print('Installing NVIDIA Apex')\n        if 'KAGGLE_URL_BASE' in os.environ:  # kaggle kernel\n            APEX_SRC = '..\/input\/nvidia-apex\/apex-master\/apex-master'\n            assert os.path.exists(APEX_SRC)\n            print(subprocess.check_output(\n                f'{sys.executable} -m pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" {APEX_SRC}',\n                shell=True).decode('utf-8'))\n        else:\n            APEX_SRC = '..\/input\/apex'\n            if not os.path.exists(APEX_SRC):\n                os.makedirs(APEX_SRC)\n                print(subprocess.check_output(f'git clone https:\/\/github.com\/NVIDIA\/apex {APEX_SRC}', shell=True).decode('utf-8'))\n            else:\n                print(f'{APEX_SRC} already exists')\n            print(subprocess.check_output(\n                f'sudo {sys.executable} -m pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" {APEX_SRC}',\n                shell=True).decode('utf-8'))\n        import apex\n        print('Installed apex successfully')","842a8388":"if IS_TRAINING and args.fp16:\n    install_apex()","594dd38b":"try:\n    from pytorch_pretrained_bert import BertTokenizer, BertModel\nexcept ModuleNotFoundError:\n    print('Installing Install pytorch-pretrained-bert ...')\n    if 'KAGGLE_URL_BASE' in os.environ:  # kaggle kernel\n        bert_lib = '..\/input\/pytorchpretrainedbert\/pytorch-pretrained-bert-master\/pytorch-pretrained-BERT-master'\n        assert os.path.exists(bert_lib)\n        sys.path.insert(0, bert_lib)\n    else:\n        print(subprocess.check_output('sudo -u jupyter conda install -y -c conda-forge pytorch-pretrained-bert', shell=True).decode('utf-8'))\n    print('Installed pytorch-pretrained-bert successfully')","2f7a5b84":"from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE, WEIGHTS_NAME, CONFIG_NAME\nfrom pytorch_pretrained_bert.modeling import BertModel, BertForSequenceClassification, BertConfig, BertForMaskedLM\nfrom pytorch_pretrained_bert.tokenization import BertTokenizer\nfrom pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule","a412ef96":"y_columns=['target']","39f29f0f":"if 'KAGGLE_URL_BASE' in os.environ:  # kaggle kernel\n    MODELS_ROOT_DIR = '..\/input\/pretrained-bert-models-for-pytorch'\n    VOCAB_FILE = f'{MODELS_ROOT_DIR}\/{args.bert_model}-vocab.txt'\n    tokenizer = BertTokenizer.from_pretrained(VOCAB_FILE, do_lower_case=args.do_lower_case, cache_dir=None)\nelse:\n    tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)","914563d3":"def convert_lines(lines, max_seq_length, tokenizer):\n    '''\n      Converting the lines to BERT format.\n      \n      Copied from https:\/\/www.kaggle.com\/httpwwwfszyc\/bert-in-keras-taming\n    '''\n    max_seq_length -= 2  # CLS, SEP\n    all_tokens = []\n    longer = 0\n    for text in tqdm_notebook(lines):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    print(f'longer: {longer}')\n    return np.array(all_tokens)","9a42327a":"def convert_lines_parallel(i):\n    total_lines = len(X_train)\n    num_lines_per_thread = total_lines \/\/ os.cpu_count() + 1\n    lines = X_train[i * num_lines_per_thread : (i+1) * num_lines_per_thread]\n    return convert_lines(lines, args.max_seq_length, tokenizer)","4b3b564f":"def preprocess_data(df, has_label=True):\n    # Make sure all comment_text values are strings\n    df['comment_text'] = df['comment_text'].astype(str).fillna(\"DUMMY_VALUE\")\n    if has_label:\n        # convert target to 0,1\n        df['target']=(df['target']>=0.5).astype(float)","f89adde2":"if IS_TRAINING:\n    train_df = pd.read_csv(os.path.join(args.data_dir, \"train.csv\"))#.sample(num_to_load+valid_size,random_state=args.seed)\n    preprocess_data(train_df)\n    \n    X_train = train_df[\"comment_text\"]\n    X_train = np.vstack(db.from_sequence(list(range(os.cpu_count()))).map(convert_lines_parallel).compute())\n    Y_train = train_df[y_columns].values\n\n    train_dataset = torch.utils.data.TensorDataset(torch.tensor(X_train, dtype=torch.long), torch.tensor(Y_train,dtype=torch.float))\n    num_train_optimization_steps = int(\n        len(train_dataset) \/ args.train_batch_size \/ args.gradient_accumulation_steps) * (args.end_epoch-args.begin_epoch)\n    \n    assert Y_train.shape[1] == 1\n    print(X_train.shape)\n    print(Y_train.shape)\n    print(X_train.dtype)\n    print(Y_train.dtype)","e99cdb67":"NUM_VALID_SAMPLES = 100000\n\nvalid_df = pd.read_csv(os.path.join(args.data_dir, \"train.csv\")).sample(NUM_VALID_SAMPLES, random_state=args.seed)\npreprocess_data(valid_df)\n\nX_valid = convert_lines(valid_df['comment_text'], args.max_seq_length, tokenizer)\nY_valid = valid_df[y_columns].values\nvalid_dataset = torch.utils.data.TensorDataset(torch.tensor(X_valid, dtype=torch.long))\n\nassert Y_valid.shape[1] == 1\nprint(X_valid.shape)\nprint(Y_valid.shape)\nprint(X_valid.dtype)\nprint(Y_valid.dtype)","45e32ab2":"test_df = pd.read_csv(os.path.join(args.data_dir, \"test.csv\"))\npreprocess_data(test_df, has_label=False)","db402bb7":"X_test = convert_lines(test_df[\"comment_text\"], args.max_seq_length, tokenizer)","2f8e4f1b":"print(X_test.shape)\nprint(X_test.dtype)","1e4fadcb":"test_dataset = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))","e1de3f89":"def prepare_model(model):\n    '''\n      See https:\/\/github.com\/huggingface\/pytorch-pretrained-BERT\/blob\/master\/examples\/run_classifier.py\n    '''\n#     if args.fp16:\n#         # Users should not manually cast their model or data to .half()\n#         # see https:\/\/nvidia.github.io\/apex\/amp.html\n#         model.half()\n    model.zero_grad()\n    model.to(DEVICE)\n    if NUM_GPUS > 1:\n        model = torch.nn.DataParallel(model)\n    return model","f85059d4":"def load_model():\n    if args.begin_epoch == 0:  # load BERT model\n        print('Load BERT model')\n        if 'KAGGLE_URL_BASE' in os.environ:  # kaggle kernel\n            MODELS_ROOT_DIR = '..\/input\/pretrained-bert-models-for-pytorch'\n            MODEL_PATH = f'{MODELS_ROOT_DIR}\/{args.bert_model}'\n            model = BertForSequenceClassification.from_pretrained(MODEL_PATH, cache_dir=None, num_labels=len(y_columns))\n        else:\n            model = BertForSequenceClassification.from_pretrained(args.bert_model, cache_dir=None, num_labels=len(y_columns))\n    else:  # args.begin_epoch > 0\n        print('Load previous checkpoint')\n        model_dir = f'{args.output_dir}\/epoch-{args.begin_epoch-1}'\n        assert os.path.exists(model_dir)\n        model = BertForSequenceClassification.from_pretrained(model_dir, num_labels=len(y_columns), cache_dir=None)\n\n    return model","8e877de0":"if IS_TRAINING:\n    model = prepare_model(load_model())","edb50482":"def prepare_optimizer(model):\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\n    if False:  # args.fp16\n        try:\n            from apex.optimizers import FP16_Optimizer\n            from apex.optimizers import FusedAdam\n        except ImportError:\n            raise ImportError(\"Please install apex from https:\/\/www.github.com\/nvidia\/apex to use distributed and fp16 training.\")\n\n        optimizer = FusedAdam(optimizer_grouped_parameters,\n                              lr=args.learning_rate,\n                              bias_correction=False,\n                              max_grad_norm=1.0)\n        if args.loss_scale == 0:\n            optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n        else:\n            optimizer = FP16_Optimizer(optimizer, static_loss_scale=args.loss_scale)\n    else:\n        optimizer = BertAdam(optimizer_grouped_parameters,\n                             lr=args.learning_rate,\n                             warmup=args.warmup_proportion,\n                             t_total=num_train_optimization_steps)\n    return optimizer","72252405":"if IS_TRAINING:\n    optimizer = prepare_optimizer(model)","996c2fd0":"def save_model(model, tokenizer, output_dir):\n    '''\n      Save a trained model and configuration.\n    '''\n    if os.path.exists(output_dir) and os.listdir(output_dir):\n        raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(output_dir))\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # Save a trained model and configuration\n    model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n\n    # If we save using the predefined names, we can load using `from_pretrained`\n    output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n    output_config_file = os.path.join(output_dir, CONFIG_NAME)\n\n    torch.save(model_to_save.state_dict(), output_model_file)\n    model_to_save.config.to_json_file(output_config_file)\n    tokenizer.save_vocabulary(output_dir)","e542e2ef":"def train(model, optimizer, train_dataset):\n    if args.fp16:\n        # Allow Amp to perform casts as required by the opt_level\n        model, optimizer = apex.amp.initialize(model, optimizer, opt_level=\"O1\")\n        warmup_linear = WarmupLinearSchedule(warmup=args.warmup_proportion,\n                                             t_total=num_train_optimization_steps)\n    global_step = 0\n\n    model=model.train()\n\n    start_time = time.time()\n    outer_tq = tqdm_notebook(range(args.begin_epoch, args.end_epoch))\n    for epoch in outer_tq:\n        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.train_batch_size, shuffle=True)\n        avg_loss = 0.\n        avg_accuracy = 0.\n        lossf=None\n        epoch_start_time = time.time()\n\n        #for step, batch in enumerate(tqdm(train_dataloader, desc= f'Iteration {epoch}')):\n        #    batch = tuple(t.to(DEVICE) for t in batch)\n\n        inner_tq = tqdm_notebook(enumerate(train_loader), total=len(train_loader),leave=False, desc= f'Iteration {epoch}')\n        for step, (x_batch, y_batch) in inner_tq:\n            optimizer.zero_grad()\n            y_pred = model(x_batch.to(DEVICE), attention_mask=(x_batch>0).to(DEVICE), labels=None)\n            loss =  F.binary_cross_entropy_with_logits(y_pred, y_batch.to(DEVICE))\n\n            if NUM_GPUS > 1:\n                loss = loss.mean() # mean() to average on multi-gpu.\n            if args.gradient_accumulation_steps > 1:\n                loss = loss \/ args.gradient_accumulation_steps\n\n            if args.fp16:\n                # optimizer.backward(loss)\n                with apex.amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                optimizer.step()\n                optimizer.zero_grad()\n                global_step += 1\n\n            if lossf:\n                lossf = 0.98*lossf+0.02*loss.item()\n            else:\n                lossf = loss.item()\n            inner_tq.set_postfix(loss = lossf)\n            avg_loss += loss.item() \/ len(train_loader)\n            avg_accuracy += torch.mean(((torch.sigmoid(y_pred[:,0])>0.5) == (y_batch[:,0]>0.5).to(DEVICE)).to(torch.float) ).item()\/len(train_loader)\n        outer_tq.set_postfix(avg_loss=avg_loss,avg_accuracy=avg_accuracy)\n        save_model(model, tokenizer, f'{args.output_dir}\/epoch-{epoch}')\n        epoch_end_time = time.time()\n        print(f'Iteration {step} time elapsed {int(epoch_end_time-epoch_start_time)}s')\n\n    end_time = time.time()\n    print(f'Time elapsed {int(end_time-start_time)}s')","7bef40e1":"if not os.path.exists(f'{args.output_dir}\/epoch-{args.end_epoch-1}'):\n    train(model, optimizer, train_dataset)","56965258":"def load_eval_model(model_dir):\n    # Load a trained model and vocabulary that you have fine-tuned\n    model = BertForSequenceClassification.from_pretrained(model_dir, num_labels=len(y_columns), cache_dir=None)\n    tokenizer = BertTokenizer.from_pretrained(model_dir, do_lower_case=args.do_lower_case, cache_dir=None)\n    model.to(DEVICE)\n    model.eval()\n    for param in model.parameters():\n        param.requires_grad = False\n    return model","1243f82a":"def predict(model, valid_dataset):\n    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=args.eval_batch_size, shuffle=False)\n    batch_size=args.eval_batch_size\n    valid_preds = np.zeros((len(valid_dataset)))\n    \n    for step, (x_batch, ) in tqdm_notebook(enumerate(valid_loader), total=len(valid_loader)):\n        y_pred = model(x_batch.to(DEVICE), attention_mask=(x_batch>0).to(DEVICE), labels=None)\n        valid_preds[step*batch_size:(step+1)*batch_size]=y_pred[:,0].detach().cpu().squeeze().numpy()\n    return valid_preds","b97bb4ad":"model = load_eval_model(f'{args.output_dir}\/epoch-{args.end_epoch-1}')","2a997169":"valid_preds = predict(model, valid_dataset)","c7f8db7d":"# From baseline kernel\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score\n\ndef calculate_overall_auc(df, model_name):\n    true_labels = df[TOXICITY_COLUMN]>0.5\n    predicted_labels = df[model_name]\n    return metrics.roc_auc_score(true_labels, predicted_labels)\n\ndef power_mean(series, p):\n    total = sum(np.power(series, p))\n    return np.power(total \/ len(series), 1 \/ p)\n\ndef get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n    bias_score = np.average([\n        power_mean(bias_df[SUBGROUP_AUC], POWER),\n        power_mean(bias_df[BPSN_AUC], POWER),\n        power_mean(bias_df[BNSP_AUC], POWER)\n    ])\n    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n\n\n\nSUBGROUP_AUC = 'subgroup_auc'\nBPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\nBNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n\ndef compute_auc(y_true, y_pred):\n    try:\n        return metrics.roc_auc_score(y_true, y_pred)\n    except ValueError:\n        return np.nan\n\ndef compute_subgroup_auc(df, subgroup, label, model_name):\n    subgroup_examples = df[df[subgroup]>0.5]\n    return compute_auc((subgroup_examples[label]>0.5), subgroup_examples[model_name])\n\ndef compute_bpsn_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n    subgroup_negative_examples = df[(df[subgroup]>0.5) & (df[label]<=0.5)]\n    non_subgroup_positive_examples = df[(df[subgroup]<=0.5) & (df[label]>0.5)]\n    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n    return compute_auc(examples[label]>0.5, examples[model_name])\n\ndef compute_bnsp_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n    subgroup_positive_examples = df[(df[subgroup]>0.5) & (df[label]>0.5)]\n    non_subgroup_negative_examples = df[(df[subgroup]<=0.5) & (df[label]<=0.5)]\n    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n    return compute_auc(examples[label]>0.5, examples[model_name])\n\ndef compute_bias_metrics_for_model(dataset,\n                                   subgroups,\n                                   model,\n                                   label_col,\n                                   include_asegs=False):\n    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n    records = []\n    for subgroup in subgroups:\n        record = {\n            'subgroup': subgroup,\n            'subgroup_size': len(dataset[dataset[subgroup]>0.5])\n        }\n        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n        records.append(record)\n    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)","9700653a":"MODEL_NAME = 'model1'\n# List all identities\nidentity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n\nvalid_df[MODEL_NAME]=torch.sigmoid(torch.tensor(valid_preds)).numpy()\nTOXICITY_COLUMN = 'target'\nbias_metrics_df = compute_bias_metrics_for_model(valid_df, identity_columns, MODEL_NAME, 'target')","f3af2c7e":"bias_metrics_df","56c97cb8":"get_final_metric(bias_metrics_df, calculate_overall_auc(valid_df, MODEL_NAME))","3d6f1b85":"y_test = predict(model, test_dataset)","29cecccc":"y_test.shape","944cd0ae":"test_pred = torch.sigmoid(torch.tensor(y_test)).numpy().ravel()","fbe50a7a":"test_pred.shape","fa2d1275":"submission = pd.DataFrame.from_dict({\n    'id': test_df['id'],\n    'prediction': test_pred\n})\nsubmission.to_csv('submission.csv', index=False)","8fe8b7b0":"submission.head()","bf1b594d":"### 3.1 Training Dataset","9d5c9a58":"## 3. Loading Data","080a68b2":"## 4. Training","9968abdd":"### 4.3 Start Training","fd5fd7a4":"### 2.1 Install Apex","c3e48942":"## 2. Install requirements","79e6b5cf":"### 3.2 Validation Dataset","4df0e386":"# Jigsaw Unintended Bias in Toxicity Classification","8d919d81":"**Define flags**:","226af546":"Make Pytorch **deterministic**:","09b637d4":"## 6. Inference","5a7aa417":"## 5. Validation","cce94a35":"**Tokenizer**:","f612e514":"### 4.1 Load BERT model","304f6931":"### 2.2 Install pytorch-pretrained-bert","96395fb1":"## 1. Initialize Environment","afa245b7":"### 3.3 Test Dataset","510268c7":"### 4.2 Optimizer"}}