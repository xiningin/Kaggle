{"cell_type":{"92bd5f4a":"code","bb1c182d":"code","c630df6e":"code","cd998b5f":"code","d83ad951":"code","5407dce5":"code","677af1f6":"code","59b89c88":"code","2fe4a1f8":"code","c498766f":"code","0ae835a7":"code","1bd34d53":"code","ba97dcec":"code","ff2d3a6a":"code","523ae7f9":"code","5ac583c0":"code","f13975e1":"code","510ebbd6":"code","1cb85328":"code","d80a9767":"markdown","f8566b23":"markdown","2be754d9":"markdown"},"source":{"92bd5f4a":"import sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","bb1c182d":"import warnings\nwarnings.filterwarnings(\"ignore\")","c630df6e":"import numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nimport tensorflow.keras.backend as K\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\nfrom tensorflow.keras.losses import BinaryCrossentropy\nimport tensorflow_addons as tfa\n\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import log_loss\nfrom sklearn import preprocessing\n\nfrom sklearn.decomposition import PCA\n\nfrom tqdm.notebook import tqdm\n\nimport math","cd998b5f":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\n\ndata = train_features.append(test_features)\n\nss = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","d83ad951":"train = train_features.copy()\ntarget = train_targets.copy()\ntest = test_features.copy()\n\ntarget = target[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntarget.drop(['sig_id'], axis=1, inplace=True)\n\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntrain.drop(['sig_id', 'cp_type'], axis=1, inplace=True)\n\ntest.drop(['sig_id', 'cp_type'], axis=1, inplace=True)","5407dce5":"def preprocess(df):\n    df = df.copy()\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    df.loc[:, 'cp_time'] = df.loc[:, 'cp_time'].map({24: 0, 48: 1, 72: 2})    \n    #df = pd.get_dummies(df, columns=['cp_time','cp_dose'])\n    return df\n\ntrain = preprocess(train)\ntest = preprocess(test)\ndata = train.append(test)\n#del train_targets['sig_id']","677af1f6":"g_cols = [col for col in train_features.columns if col.startswith('g-')]\nc_cols = [col for col in train_features.columns if col.startswith('c-')]","59b89c88":"for col in (g_cols + c_cols):\n\n    transformer = QuantileTransformer(n_quantiles=250, random_state=321, output_distribution=\"normal\")\n    vec_len = len(data[col].values)\n    vec_len_train = len(train[col].values)\n    vec_len_test = len(test[col].values)\n    \n    raw_vec = data[col].values.reshape(vec_len, 1)\n    raw_vec_train = train[col].values.reshape(vec_len_train, 1)\n    raw_vec_test = test[col].values.reshape(vec_len_test, 1)\n    \n    transformer.fit(raw_vec)\n\n    train[col] = transformer.transform(raw_vec_train).reshape(1, vec_len_train)[0]\n    test[col] = transformer.transform(raw_vec_test).reshape(1, vec_len_test)[0]","2fe4a1f8":"somthing_rate = 5e-4\nP_MIN = somthing_rate\nP_MAX = 1 - P_MIN\n\ndef loss_fn(yt, yp):\n    yp = np.clip(yp, P_MIN, P_MAX)\n    return log_loss(yt, yp, labels=[0,1])\n    ","c498766f":"def create_model(num_columns, actv='relu'):\n    model = tf.keras.Sequential([tf.keras.layers.Input(num_columns)])\n                \n    #model.add(tf.keras.layers.BatchNormalization())\n    #model.add(tf.keras.layers.Dropout(0.3))\n    #model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(2048, activation=actv)))\n    \n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Dropout(0.2))\n    model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(1024, activation=actv)))\n    \n    if actv == 'elu':\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.AlphaDropout(0.25))\n        model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(512, kernel_initializer='lecun_normal', activation='selu')))\n    else:\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.Dropout(0.25))\n        model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(512, activation=actv))) \n\n    #============ Final Layer =================\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Dropout(0.3))\n    model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(206, activation=\"sigmoid\")))\n    \n    model.compile(optimizer=tfa.optimizers.AdamW(lr = 1e-3, weight_decay = 1e-5, clipvalue = 900), \n                  loss=BinaryCrossentropy(label_smoothing=somthing_rate),\n                  )\n    return model","0ae835a7":"# Use All feats as top feats\ntop_feats = [i for i in range(train.shape[1])]\nprint(\"Top feats length:\",len(top_feats))","1bd34d53":"mod = create_model(len(top_feats))\nmod.summary()","ba97dcec":"def metric(y_true, y_pred):\n    metrics = []\n    for _target in train_targets.columns:\n        metrics.append(loss_fn(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float)))\n    return np.mean(metrics)","ff2d3a6a":"N_STARTS = 16\nS_STARTS = int(N_STARTS\/2) \n#seeds = np.random.randint(0, 100, size=N_STARTS)\ntrain_targets = target\n\nres_relu = train_targets.copy()\nres_elu = train_targets.copy()\nres_relu.loc[:, train_targets.columns] = 0\nres_elu.loc[:, train_targets.columns] = 0\n\nss_relu = ss.copy()\nss_elu = ss.copy()\nss_relu.loc[:, train_targets.columns] = 0\nss_elu.loc[:, train_targets.columns] = 0\n\n#ss.loc[:, train_targets.columns] = 0\nss_dict = {}\n\nhistorys = dict()\n\ntf.random.set_seed(42)\nfor seed in range(N_STARTS):\n    for n, (tr, te) in enumerate(MultilabelStratifiedKFold(n_splits=5, random_state=seed, shuffle=True).split(train_targets, train_targets)):\n        print(f\"======{train_targets.values[tr].shape}========{train_targets.values[te].shape}=====\")\n        \n        if seed < S_STARTS: # every actv. will train for 7 times seed.\n            print(f'Seed: {seed} => Fold: {n} ==> (RELU MODEL)')\n            model = create_model(len(top_feats), actv='relu')\n        else:\n            print(f'Seed: {seed} => Fold: {n} ==> (ELU MODEL)')\n            model = create_model(len(top_feats), actv='elu')\n\n        \n        checkpoint_path = f'repeat:{seed}_Fold:{n}.hdf5'\n        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.2, min_lr=1e-6, patience=4, verbose=1, mode='min')\n        cb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 1, save_best_only = True,\n                                     save_weights_only = True, mode = 'min')\n        early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", restore_best_weights=True, patience= 10, verbose = 1)\n        \n        history = model.fit(train.values[tr][:, top_feats],\n                  train_targets.values[tr],\n                  validation_data=(train.values[te][:, top_feats], train_targets.values[te]),\n                  epochs=60, batch_size=128,\n                  callbacks=[reduce_lr_loss, cb_checkpt, early], verbose=2\n                 )\n        \n        historys[f'history_{seed+1}'] = history\n        print(\"Model History Saved.\")\n        \n        model.load_weights(checkpoint_path)\n        \n        test_predict = model.predict(test.values[:, top_feats])\n        val_predict = model.predict(train.values[te][:, top_feats])\n\n        if seed < S_STARTS: \n            ss_relu.loc[:, train_targets.columns] += test_predict\n            res_relu.loc[te, train_targets.columns] += val_predict\n        else:\n            ss_elu.loc[:, train_targets.columns] += test_predict\n            res_elu.loc[te, train_targets.columns] += val_predict\n            \n        print(f'OOF Metric For SEED {seed} => FOLD {n} : {metric(train_targets.loc[te, train_targets.columns], pd.DataFrame(val_predict, columns=train_targets.columns))}')\n        print('+-' * 10)\n        \nss_relu.loc[:, train_targets.columns] \/= ((n+1) * S_STARTS)\nres_relu.loc[:, train_targets.columns] \/= S_STARTS\n\nss_elu.loc[:, train_targets.columns] \/= ((n+1) * S_STARTS)\nres_elu.loc[:, train_targets.columns] \/= S_STARTS","523ae7f9":"# Show Model loss in plots\n\nfor k,v in historys.items():\n    loss = []\n    val_loss = []\n    loss.append(v.history['loss'][:40])\n    val_loss.append(v.history['val_loss'][:40])\n    \nimport matplotlib.pyplot as plt\nplt.figure(figsize = (15, 6))\nplt.plot(np.mean(loss, axis=0))\nplt.plot(np.mean(val_loss, axis=0))\nplt.yscale('log')\nplt.yticks(ticks=[1,1e-1,1e-2])\nplt.xlabel('Epochs')\nplt.ylabel('Average Logloss')\nplt.legend(['Training','Validation'])","5ac583c0":"print(f'OOF Metric (relu): {metric(train_targets, res_relu)}')\nprint(f'OOF Metric (elu): {metric(train_targets, res_elu)}')","f13975e1":"ss_relu.to_csv('submission_relu.csv', index=False)\nss_elu.to_csv('submission_elu.csv', index=False)","510ebbd6":"target_cols = list(ss_relu.columns[1:])\npreds = [ss_relu, ss_elu]\navr_sub = pd.DataFrame()\navr_sub['sig_id'] = ss_relu['sig_id']\n\nfor column in target_cols:\n    column_data = []\n    for i in range(len(preds)):\n        column_data.append(preds[i][column])\n    avr_sub[column] = np.mean(column_data, axis=0)\n    \n\npreds.append(avr_sub)    \navr_sub.head()","1cb85328":"avr_sub.to_csv('submission.csv', index=False)","d80a9767":"Kernel still under modification.. **<span style='color:red'>Feedbacks<\/span>** is also very much appreciated.\nPls **<span style='color:red'>UPVOTE<\/span>**, if you find it useful. \n","f8566b23":"<center><h2 style='color:red'>MoA | Keras Multilabel Classifier NN | Starter <\/h2><\/center><hr>","2be754d9":"## What is new in this Kernel?\n1. Process all 'cp' columns.\n2. training is done from scratch. Controlled using (ReduceLROnPlateau, EarlyStopping, LearningRateScheduler)\n3. Add WeightNormalization.\n4. Model checkpoints (from https:\/\/www.kaggle.com\/ravy101\/drug-moa-tf-keras-starter kernel)\n5. Used Adamw as optimizer with initial LR and **weight decay**\n6. Used 7-MultilabelStratifiedKFold, 7 seeds averaging.\n7. Put zeros for `ctl_vehicle` predictions.\n\n**Updates:**<br>\n**V3:** (0.01888) public score.<br>\n**V4:** Add **elu** activation to Dense layer<br>\n**V7:** Use differnet model architecture with 100 epochs<br>\n**V9:** New baseline model<br>\n**V10:** V9 + different data process.<br>\n**V11:** Ensemble 2 layer and 3 layer model results.<br>\n**V12:** Use 2 baseline models with different activation functions.<br>\n**V16:** Aplly RankGauss (from https:\/\/www.kaggle.com\/kushal1506\/moa-pytorch-0-01859-rankgauss-pca-nn\/ kernel)<br>\n\n\n<hr><h4>Pls <span style='color:red'>UPVOTE<\/span>, if you find it useful. Feedbacks is also very much appreciated.<h4>"}}