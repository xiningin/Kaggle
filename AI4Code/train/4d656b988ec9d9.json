{"cell_type":{"565a3096":"code","7cb283b8":"code","1bf5ec80":"code","cde0d757":"code","3610ca37":"code","3e65d181":"code","360fd670":"code","a6fdfd88":"code","d19f3776":"code","dc29cbb2":"code","f6c60119":"code","f4a1cf9a":"code","3f514991":"code","0d4a15e2":"markdown","191f0806":"markdown","a20f5bc2":"markdown","a07fe3b6":"markdown","3f0b3074":"markdown","1a945dd4":"markdown","6ece8af9":"markdown","8986a39a":"markdown","c0a3de02":"markdown","f1927175":"markdown","0bed72dd":"markdown","41a8c1f7":"markdown","91b73a02":"markdown","9d3d8963":"markdown","cb67ab73":"markdown","7619058d":"markdown","d1d15ad0":"markdown"},"source":{"565a3096":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import boxcox\nfrom collections import Counter\nfrom xgboost import XGBRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import SGDRegressor, Ridge\nfrom sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7cb283b8":"df = pd.read_csv('..\/input\/spotify-dataset-19212020-160k-tracks\/data.csv')\ndf = df.drop('id', axis=1)","1bf5ec80":"df.head()","cde0d757":"artists = []\nfor i in df['artists']:\n    for j in i.split(\", \"):\n        artist = j.replace('[', '').replace(']', '')\n        if artist[0] == \"'\":\n            artist = artist[1:]\n        if artist[-1] == \"'\":\n            artist = artist[:-1]\n        artists.append(artist)","3610ca37":"fig, ax = plt.subplots(1, 1, figsize=(15, 7))\ncount = pd.Series(Counter(artists)).sort_values(ascending=False)[:100]\nbars = sns.barplot(count.keys(), count, palette='twilight')\nfor index in range(0, len(bars.patches), 2):\n    bar = bars.patches[index]\n    bars.annotate(format(bar.get_height(), '.0f'), (bar.get_x()+bar.get_width()\/2., \n                                                    bar.get_height()), ha='center',\n                 va='bottom', size=8)\n    \nplt.title('Most common artists')\nplt.xlabel('Artists')\nplt.ylabel('Number of artists')\nplt.xticks(rotation=90, size=8)\nplt.show()","3e65d181":"groups = df[df['year']>=1950].groupby(['year']).mean()\nfig, axes1 = plt.subplots(4, 3, figsize=(17, 25))\nplt.subplots_adjust(hspace=0.3, wspace=0.25)\nfig, axes2 = plt.subplots(1, 2, figsize=(9, 4))\nplt.subplots_adjust(wspace=0.25)\n\nfor row in axes1:\n    for ax in row:\n        col = groups.columns[list(axes1.flatten()).index(ax)]\n        ax.plot(groups.index, groups[col])\n        ax.set_title(col + ' over years', size=15)\n        ax.set_xlabel('year')\n        ax.set_ylabel(col)\n        \nfor ax in axes2:\n    col = groups.columns[list(axes2.flatten()).index(ax)+12]\n    ax.plot(groups.index, groups[col])\n    ax.set_title(col + ' over years', size=12)\n    ax.set_xlabel('year')\n    ax.set_ylabel(col)\n\nplt.show()","360fd670":"fig, ax = plt.subplots(1, 1, figsize=(12, 7))\nsns.heatmap(df.corr(), mask=np.triu(df.corr()), annot=True)\nplt.title('Correlation of features', size=20)\nplt.show()","a6fdfd88":"features_list = [['energy', 'loudness'], ['year', 'popularity']]\norders = [5, 1]\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\nfor ax in axes:\n    index = list(axes).index(ax)\n    features = features_list[index]\n    x = features[0]\n    y = features[1]\n    \n    sns.regplot(df[x], df[y], line_kws={'color':'black'}, \n                order=orders[index], ax=ax)\n    ax.set_xlabel(x)\n    ax.set_ylabel(y)\n    ax.set_title(x + \" and \" + y)\n\nplt.suptitle('Correlation', size=20)\nplt.show()","d19f3776":"i = 0\nfor col in df.drop(['artists', 'name', 'explicit', 'mode', 'release_date'], axis=1).columns:\n    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n    fig.tight_layout(h_pad=3)\n    i += 1\n    \n    if i%3==0:\n        colour1='lightgreen'\n        colour2='blue'\n    elif i%2==0:\n        colour1='pink'\n        colour2='green'\n    else:\n        colour1='skyblue'\n        colour2='red'\n    \n    if col=='loudness':\n        log = (df[col]+61).transform(np.log)\n        box_cox = pd.Series(boxcox(df[col]+61)[0])\n    else:\n        log = (df[col]+1).transform(np.log)\n        box_cox = pd.Series(boxcox(df[col]+1)[0])\n    \n    df[col].hist(ax=axes[0][0], color=colour1)\n    log.hist(ax=axes[0][1], color=colour1)\n    box_cox.hist(ax=axes[0][2], color=colour1)\n    \n    for transform in [[df[col], axes[0][0]], [log, axes[0][1]], [box_cox, axes[0][2]]]:\n        ax = transform[1]\n        deciles = pd.Series(transform[0]).quantile([.1, .2, .3, .4, .5, .6, .7, .8, .9])\n        for pos in np.array(deciles).reshape(1, -1)[0]:\n            handle = ax.axvline(pos, color='darkblue', linewidth=1)\n        ax.legend([handle], ['deciles'])\n    \n    sns.boxplot(df[col], orient=\"h\", ax=axes[1][0], color=colour2)\n    sns.boxplot(log, orient=\"h\", ax=axes[1][1], color=colour2)\n    sns.boxplot(box_cox, orient=\"h\", ax=axes[1][2], color=colour2)\n    \n    axes[1][0].set_xlabel('normal', size=15)\n    axes[1][1].set_xlabel('log transform', size=15)\n    axes[1][2].set_xlabel('box cox', size=15)\n    axes[0][1].set_title(col, size=25, pad=15)\n    \n    plt.show()","dc29cbb2":"fig, axes = plt.subplots(4, 3, figsize=(15, 20))\nplt.subplots_adjust(hspace=0.3, wspace=0.27)\ncolours = ['skyblue', 'pink', 'lightgreen']*4\n\nfor row in axes:\n    for ax in row:\n        index = list(axes.flatten()).index(ax)\n        col = df.drop(['artists', 'name', 'explicit', 'mode', 'release_date', 'year'], \n                      axis=1).columns[index]\n        feature = df[col]\n        count = Counter(np.digitize(feature, np.arange(feature.min(), feature.max(),\n                                                            (feature.max()-feature.min())\/20)))\n        ax.bar(count.keys(), count.values(), color=colours[index])\n        ax.set_title(col + ' bin')\n        ax.set_xlabel('bins')\n        ax.set_ylabel(col)\n\nplt.show()","f6c60119":"for col in df[df.drop(['artists', 'name', 'explicit', 'mode', 'release_date'], \n                      axis=1).columns[3:12]]:\n    if col=='loudness':\n        df[col] = pd.Series(boxcox(df[col]+61)[0])\n    else:\n        df[col] = pd.Series(boxcox(df[col]+1)[0])","f4a1cf9a":"X = df[['acousticness', 'danceability', 'duration_ms', 'energy', 'explicit', \n        'instrumentalness', 'key', 'liveness', 'loudness', 'mode', 'popularity', 'speechiness',\n        'tempo', 'valence']]\ny = df['year']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","3f514991":"model = XGBRegressor()\nmodel.fit(X_train, y_train)\n\nprint('XGBoost')\nprint('Model score:    ', model.score(X_test, y_test))\nprint('Cross val score:', cross_val_score(model, X_test, y_test).mean())","0d4a15e2":"# Transformations\nAdditionally, we will now use the log transformation and box cox methods to transform our data so that its distribution resembles more of a bell shape.","191f0806":"The \"X\" is the features in our dataset except \"year\", while the \"y\" will be assigned to \"year\". Afterwards, we split the data further off into train and test sets.","a20f5bc2":"Similarly, the loudness has climbed over the years from -16 to -8. Additionally, music seems to have gotten a lot more explicit in the 2000s, due to it having a value of 0.0 in the 50s while in the 2000s it reached around 0.17. The valence (happiness) of the songs has been on a rollercoaster, with the 70s and 80s producing the happiest songs, while the 2010s having a sudden drop.","a07fe3b6":"# Predicting the data\nWe finish off by creating an XGBRegressor model, training it on the train set and achieving a pleasing accuracy around 75%.","3f0b3074":"# Creating X and y\nThe final part of this notebook will be predicting the data. Our goal will be to predict the \"year\" feature using every other numerical variable.","1a945dd4":"# Features over time","6ece8af9":"# Spotify Visualisation and Prediction\nWelcome to my Spotify notebook, where today we will be visualising this dataset and creating a regressor to predict which year the songs were made.","8986a39a":"<img src=\"https:\/\/miro.medium.com\/max\/593\/1*pfmeGgGM5sxmLBQ5IQfQew.png\" width=\"500px\"\/>","c0a3de02":"# Binning\nNow we will bin the data by compressing the unique amount of values per feature into twenty. The distribution per variable is shown below through histograms.","f1927175":"We visualise this by going through every variable, plotting a histogram of the distribution and a boxplot below it.","0bed72dd":"Next, we plot out some line graphs which show the average values per year in the features. Here we see some interesting observations. First of all, we can conclude that the **acousticness** has plummeted over the years; going from around 90% in the 50s to 25% in the last few years. Also, the **danceability** has had a steady increase over time, with it being slightly lower than 50% in the 50s to roughly 60% in recent years.","41a8c1f7":"<img src=\"https:\/\/th.bing.com\/th\/id\/OIP.2mczJR1u19WDWERsDFrgNwHaFb?pid=Api&rs=1\" width=\"200px\"\/>\n<img src=\"https:\/\/www.dailydot.com\/wp-content\/uploads\/d62\/72\/b41e5005a226e2ed.png\" width=\"200px\"\/>","91b73a02":"# Heatmap\nThe subsequent heatmap shows us that the most correlated features are energy and loudness (0.78), followed by energy & year and danceability & valence (0.54).","9d3d8963":"We have chosen \"energy and loudness\" and \"year and popularity\". As seen with **\"energy and loudness\"**, there is a slow and steady increase in the trend of our features. With the **\"years and popularity\"**, there is a lot more outliers and the data is more messy, however we can still make some solid conclusions from the plot. Along with scattering the data points, we have also created a line which (somewhat) accurately plots the trend of our samples.","cb67ab73":"## Thank you for reading this notebook.\n## If you enjoyed this notebook and found it helpful, please give it an upvote and provide feedback, as it would help me make more of these.","7619058d":"# Most common artists\nThe first visualisation that we will be showing is a bar chart which shows the 100 most common music artists. As seen below, the most common artist in our dataset is Francisco Canaro with 2,228 songs. Some famous performers on our list include Chopin, Beethoven, Mozart, Sinatra, Johhny Cash, Bob Dylan, and The Beatles.","d1d15ad0":"# Scatterplot\nFurthermore, we will now use a scatterplot to take a look at two of the most correlated pairs of features."}}