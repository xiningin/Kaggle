{"cell_type":{"946f1b8e":"code","8fbf2622":"code","9ce04169":"code","4fabdd79":"code","858b0ce3":"code","fe567da8":"code","d971e2c4":"code","56a936cb":"code","bca64cc5":"code","5b9a6324":"code","08c252b8":"code","3348c6ce":"code","6e3ef8f6":"code","df4b4571":"code","cfcacd90":"code","94267e50":"code","cb49d415":"code","ca407d9a":"code","5f357632":"code","79fce40d":"code","30761b7f":"code","62d42bb4":"code","a3804205":"code","c39004da":"code","a2f46fb0":"code","ec0e0012":"code","dbc0272f":"code","b6df55ad":"code","1997f43b":"code","9801977a":"code","b8030ad6":"code","bfb92a9e":"code","360542c9":"code","b1883321":"code","dc45a0c3":"code","eb626a5d":"code","d7cc5445":"code","778f10e4":"code","44a0a557":"code","83d57b3c":"code","d6a21ab3":"code","4102cc9e":"code","34e1a9d3":"code","8fc6508a":"code","952d4319":"code","3f98b55c":"code","736f0cba":"code","89cf240c":"code","c1c56d1f":"code","1125af1c":"code","d4f8ebd6":"code","a250e1dd":"code","95083de7":"code","d379a61e":"code","2fdf6d3a":"code","275583df":"code","334ca700":"code","5cafd95b":"code","aa765a7e":"code","ebd1b511":"code","2e27cadf":"code","5088b514":"code","e4f598bf":"code","923d3950":"code","4563973c":"code","cf740ba6":"code","8e660be2":"code","cc8cb414":"code","9d3c8fa2":"code","78b4048c":"markdown","ea3f55f3":"markdown","4f5688f8":"markdown","25776dbe":"markdown","b1a28742":"markdown","2de6c927":"markdown","2085a3e6":"markdown","c1141f56":"markdown","442bbfd8":"markdown","7a898653":"markdown","72d84cbb":"markdown","58e7fc7b":"markdown","8c77da1b":"markdown","d6723679":"markdown","980acd11":"markdown","a505c5e6":"markdown","64a5fbec":"markdown","9022908c":"markdown","79d3fc89":"markdown","1e4b4a84":"markdown","8b43c3b4":"markdown","ea82029c":"markdown","38b70a80":"markdown","59e177e0":"markdown","83db7c3f":"markdown","45780bd1":"markdown","be10ca18":"markdown","e94b5593":"markdown","af95db98":"markdown"},"source":{"946f1b8e":"#!pip install --quiet optuna\n!pip install --q GPUtil","8fbf2622":"import warnings\nimport os\nimport gc\nimport random\nimport glob\nimport optuna\nimport time\nimport torch","9ce04169":"import pandas               as pd\nimport numpy                as np\nimport matplotlib.pyplot    as plt \nimport seaborn              as sns\nimport joblib               as jb","4fabdd79":"from sklearn                       import metrics\nfrom sklearn.model_selection       import train_test_split, KFold, StratifiedKFold \nfrom sklearn.preprocessing         import QuantileTransformer,  KBinsDiscretizer\nfrom sklearn.preprocessing         import PowerTransformer, StandardScaler, MinMaxScaler\nfrom sklearn.pipeline              import make_pipeline\nfrom sklearn.compose               import make_column_transformer","858b0ce3":"from optuna.samplers               import TPESampler\nfrom optuna.visualization          import plot_edf\nfrom optuna.visualization          import plot_optimization_history\nfrom optuna.visualization          import plot_parallel_coordinate\nfrom optuna.visualization          import plot_param_importances\nfrom optuna.visualization          import plot_slice\nfrom optuna.visualization          import plot_intermediate_values\nfrom optuna.visualization          import plot_contour\nfrom optuna.pruners                import MedianPruner","fe567da8":"from GPUtil                        import showUtilization as gpu_usage\nfrom psutil                        import virtual_memory\nfrom datetime                      import datetime","d971e2c4":"import tensorflow as tf","56a936cb":"from tensorflow                    import keras\nfrom tensorflow.keras              import layers\nfrom tensorflow.keras              import callbacks","bca64cc5":"def jupyter_setting():\n    \n    %matplotlib inline\n     \n    pd.options.display.max_columns = None\n    \n    optuna.logging.set_verbosity(optuna.logging.WARNING)\n      \n    warnings.filterwarnings(action='ignore')\n    warnings.simplefilter('ignore')\n    warnings.filterwarnings('ignore')\n    warnings.filterwarnings('ignore', category=DeprecationWarning)\n    warnings.filterwarnings('ignore', category=FutureWarning)\n    warnings.filterwarnings('ignore', category=RuntimeWarning)\n    warnings.filterwarnings('ignore', category=UserWarning)\n    #pd.set_option('display.max_rows', 150)\n    pd.set_option('display.max_columns', 500)\n    pd.set_option('display.max_colwidth', None)\n\n    icecream = [\"#00008b\", \"#960018\",\"#008b00\", \"#00468b\", \"#8b4500\", \"#582c00\"]\n    #sns.palplot(sns.color_palette(icecream))\n    \n    return icecream\n\nicecream = jupyter_setting()","5b9a6324":"#@title\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.rcParams['font.size'] = 12\n    plt.title('Precision Recall vs threshold')\n    plt.xlabel('Threshold')\n    plt.legend(loc=\"lower left\")\n    \n    plt.grid(True)","08c252b8":"def plot_precision_vs_recall(precisions, recalls):\n    plt.plot(recalls[:-1], precisions[:-1], \"b-\", label=\"Precision\")\n    \n    plt.rcParams['font.size'] = 12\n    plt.title('Precision vs recall')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    # plt.legend(loc=\"lower left\")\n    \n    plt.grid(True)","3348c6ce":"def plot_roc_curve(fpr, tpr, label=None):\n    fig, ax = plt.subplots()\n    ax.plot(fpr, tpr, \"r-\", label=label)\n    ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n    plt.rcParams['font.size'] = 12\n    plt.title('ROC curve for TPS 09')\n    plt.xlabel('False Positive Rate (1 - Specificity)')\n    plt.ylabel('True Positive Rate (Sensitivity)')\n    plt.legend(loc=\"lower right\")\n    plt.grid(True)","6e3ef8f6":"def graf_corr(df):\n    \n    df = df.corr().round(5)\n\n    # M\u00e1scara para ocultar a parte superior direita do gr\u00e1fico, pois \u00e9 uma duplicata\n    mask = np.zeros_like(df)\n    mask[np.triu_indices_from(mask)] = True\n\n    # Making a plot\n    plt.figure(figsize=(16,16))\n    ax = sns.heatmap(df, annot=True, mask=mask, cmap=\"RdBu\", annot_kws={\"weight\": \"bold\", \"fontsize\":13})\n\n    ax.set_title(\"Mapa de calor de correla\u00e7\u00e3o das vari\u00e1vel\", fontsize=17)\n\n    plt.setp(ax.get_xticklabels(), \n             rotation      = 90, \n             ha            = \"right\",\n             rotation_mode = \"anchor\", \n             weight        = \"normal\")\n\n    plt.setp(ax.get_yticklabels(), \n             weight        = \"normal\",\n             rotation_mode = \"anchor\", \n             rotation      = 0, \n             ha            = \"right\");","df4b4571":"def correlation(dataset, threshold):\n\n    col_corr    = set()  # Conjunto de todos os nomes de colunas correlacionadas\n    corr_matrix = dataset.corr()\n    \n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) >= threshold: # estamos interessados no valor coeficiente absoluto\n                colname = corr_matrix.columns[i]        # obtendo o nome da coluna\n                col_corr.add(colname)\n    \n    return col_corr","cfcacd90":"def free_gpu_cache():\n    \n    # https:\/\/www.kaggle.com\/getting-started\/140636\n    #print(\"Initial GPU Usage\")\n    #gpu_usage()                             \n\n    #cuda.select_device(0)\n    #cuda.close()\n    #cuda.select_device(0)   \n    \n    gc.collect()\n    torch.cuda.empty_cache()","94267e50":"def seedAll(seed):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    #random.seed(seed)    \n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\nseed = 12342\nseedAll(seed)","cb49d415":"gpu_info = !nvidia-smi\ngpu_info = '\\n'.join(gpu_info)\n\nif gpu_info.find('failed') >= 0:\n  print('Not connected to a GPU')\nelse:\n  print(gpu_info)","ca407d9a":"ram_gb = virtual_memory().total \/ 1e9\n\nprint('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n\nif ram_gb < 20:\n  print('Not using a high-RAM runtime')\nelse:\n  print('You are using a high-RAM runtime!')","5f357632":"!mkdir img\n!mkdir Data\n!mkdir Data\/pkl\n!mkdir Data\/submission\n!mkdir Data\/submission\/tunning\n\n!mkdir model\n!mkdir model\/mdl\n!mkdir model\/preds\n!mkdir model\/optuna\n!mkdir model\/preds\/param\n            \n!mkdir model\/preds\/test\n!mkdir model\/preds\/test\/n1\n!mkdir model\/preds\/test\/n2\n!mkdir model\/preds\/test\/n3\n\n!mkdir model\/preds\/train\n!mkdir model\/preds\/train\/n1\n!mkdir model\/preds\/train\/n2\n!mkdir model\/preds\/train\/n3\n!mkdir model\/preds\/param","79fce40d":"path = '..\/input\/tps11002\/'\n#path = ''","30761b7f":"df3_train     = jb.load(path + 'df2_nb_02_train.pkl.z')\ndf3_test      = jb.load(path + 'df2_nb_02_test.pkl.z')\ndf_submission = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')\n\ndf3_train.shape, df3_test.shape, df_submission.shape","62d42bb4":"df3_train.head()","a3804205":"df3_test.head()","c39004da":"X      = df3_train.drop(['target'], axis=1)    \ny      = df3_train[\"target\"].copy()\nX_test = df3_test\n\ndel df3_train , df3_test\n\nfree_gpu_cache()   ","a2f46fb0":"feature_int      = X_test.filter(regex=r'f[0-9]').columns.to_list()\nfeature_cluster  = X_test.filter(regex=r'fe_clu').columns.to_list()\nfeature_static   = X_test.filter(regex=r'fe_[m-s]').columns.to_list()\nfeature_pipeline = make_pipeline(StandardScaler(),MinMaxScaler(feature_range=(0, 1)))\n\npreprocessor = make_column_transformer((feature_pipeline, feature_int), \n                                       (feature_pipeline, feature_cluster), \n                                       (feature_pipeline, feature_static),\n                                       n_jobs=-1\n                                      )\n\n#xx = preprocessor.fit_transform(X.head(300))\n#pd.DataFrame(xx).head()","ec0e0012":"def load_model(learning_rate_, activation_, input_shape_, dropout_=0):\n    \n    early_stopping = callbacks.EarlyStopping(patience             = 20,\n                                             min_delta            = 0,\n                                             monitor              = 'val_loss',\n                                             restore_best_weights = True,\n                                             verbose              = 0,\n                                             mode                 = 'min', \n                                             baseline             = None,)\n    \n    plateau = callbacks.ReduceLROnPlateau(monitor  = 'val_loss', \n                                          factor   = 0.2, \n                                          patience = 7, \n                                          verbose  = 0,\n                                          mode     = 'min')\n    \n    # -----------------------------------------------------------------\n    # Model \n    # https:\/\/machinelearningmastery.com\/choose-an-activation-function-for-deep-learning\/\n    # https:\/\/www.kaggle.com\/ryanholbrook\/stochastic-gradient-descent\n    # https:\/\/www.kaggle.com\/sfktrkl\/tps-nov-2021-nn-2 \n    # https:\/\/www.kaggle.com\/javiervallejos\/simple-nn-with-good-results-tps-nov-21\n    \n\n    model = keras.Sequential([layers.Dense(128, activation = activation_, input_shape = [input_shape_]),      \n                              layers.Dense(64, activation = activation_), \n                              #layers.BatchNormalization(),\n                              layers.Dropout(dropout_),\n                              layers.Dense(32, activation =activation_),\n                              layers.Dense(16, activation =activation_),\n                              layers.Dense(1, activation='sigmoid'),\n                              ])\n    \n    # -----------------------------------------------------------------\n\n    model.compile(optimizer = keras.optimizers.Adam(learning_rate=learning_rate_),\n                  loss      = 'binary_crossentropy',\n                  metrics   = ['AUC'],\n                  )\n    \n    return model, early_stopping, plateau","dbc0272f":"path=''\n\nclass TunningModels():\n    \n    def __init__(self, name_model_, X_trn_, y_trn_, X_ts_, feature_=None, seed_=12359, path_=''):\n        \n        super(TunningModels,self).__init__() \n\n        self.name_clf         = name_model_\n        self.X_trn            = X_trn_\n        self.y_trn            = y_trn_\n        self.X_ts             = X_ts_         \n        self.feature          = feature_\n        self.seed             = seed_\n        self.path             = path_\n        \n    def diff(t_a, t_b):\n        from dateutil.relativedelta import relativedelta\n        t_diff = relativedelta(t_b, t_a)  # later\/end time comes first!\n        return '{h}h {m}m {s}s'.format(h=t_diff.hours, m=t_diff.minutes, s=t_diff.seconds)\n        \n    def delete_files(namefile):\n\n        path = ['model\/train', 'model\/test', 'model\/valid', 'model\/params', 'model\/score',\n                'model\/test_f', 'model\/cv_model', 'model\/preds', 'model\/optuna', \n                'model\/preds\/train', 'model\/preds\/test', 'model\/preds\/test\/n1', \n                'model\/preds\/test\/n2', 'model\/preds\/test\/n3', 'model\/preds\/train\/n1', \n                'model\/preds\/train\/n2', 'model\/preds\/train\/n3','model\/preds\/param', \n                'Data\/submission\/tunning', 'Data\/submission'\n                \n               ]\n\n        for path_ in path:\n            for raiz, diretorios, arquivos in os.walk(path_):\n                for arquivo in arquivos:\n                    if arquivo.startswith(namefile):\n                        os.remove(os.path.join(raiz, arquivo))\n        \n    def save_data_model(model_, model_name_, path_, y_pred_train_prob_, y_pred_test_prob_,\n                     score_, seed_, level_='1', target_='target'):\n\n        level_ = 'n'+ level_ + '\/'\n\n        if score_>.5:          \n\n            path_name_param = path_ + 'model\/preds\/param\/' + model_name_.format(score_, seed_)\n            path_name_train = path_ + 'model\/preds\/train\/' + level_ + model_name_.format(score_, seed_)\n            path_name_test  = path_ + 'model\/preds\/test\/'  + level_ + model_name_.format(score_, seed_)    \n            path_name_model = path_ + 'model\/mdl\/'         + model_name_.format(score_, seed_)    \n\n            jb.dump(y_pred_train_prob_, path_name_train)\n            jb.dump(y_pred_test_prob_, path_name_test)\n            #jb.dump(model_, path_name_model)\n            #jb.dump(pd.DataFrame([model_.get_params()]), path_name_param)   \n\n            if score_>.5:\n                # Gerar o arquivo de submiss\u00e3o \n                df_submission[target_] = y_pred_test_prob_\n                name_file_sub =  path_ + 'Data\/submission\/tunning\/' + model_name_.format(score_, seed_) + '.csv'\n                df_submission.to_csv(name_file_sub, index = False)\n\n    def preprocessor(X_trn_, X_val_, X_tst_): \n        sc_1 = StandardScaler()\n        sc_2 = MinMaxScaler(feature_range=(0, 1))\n\n        X_trn = sc_1.fit_transform(X_trn_)\n        X_val = sc_1.transform(X_val_)\n        X_tst = sc_1.transform(X_tst_)\n\n        X_trn = sc_2.fit_transform(X_trn_)\n        X_val = sc_2.transform(X_val_)\n        X_tst = sc_2.transform(X_tst_)\n\n        return X_trn, X_val, X_tst\n    \n    def df_return_preds_tunning(model_name=None, level=1, target_='target'): \n    \n        if level==1: \n            level_ = 'n1'\n        else: \n            if level==2:\n                level_ = 'n2'\n            else: \n                level_ = 'n3'\n        \n        paths = ['model\/preds\/test\/n1', 'model\/preds\/train\/' + level_ ]    \n\n        if model_name==None: \n            model_name=''\n            \n        for i, path in enumerate(paths): \n\n            name_file_pkl     = glob.glob(path + '\/'+ model_name + '*.pkl.z')\n            dic_preds_mdl_pkl = dict()\n\n            for p_name in name_file_pkl:    \n                y_model_pkl_name_col  = p_name.replace(path +'\/', '').replace('.pkl.z','') \n                y_model_pkl           = jb.load(p_name)   \n\n                dic_preds_mdl_pkl[y_model_pkl_name_col] = y_model_pkl\n\n            if i==0: \n                X_test_pred_nivel_1 = pd.DataFrame(dic_preds_mdl_pkl)\n            else: \n                X_train_pred_nivel_1 = pd.DataFrame(dic_preds_mdl_pkl)\n\n        X_train_pred_nivel_1[target_] = y\n        \n        return X_train_pred_nivel_1, X_test_pred_nivel_1\n    \n    def cross_valid(model_name_, X_train_, y_train_, X_test_, epochs_, learning_rate_, batch_size_, \n                    activation_, fold_, target_, path_, level_,  print_result_, seed_,\n                    dropout_=0, feature_=None, save_predict_=True):\n\n        if feature_!=None: \n            X_train_ = X_train_[feature_]\n            X_test_  = X_test_[feature_]\n                    \n        #--------------------------------------------------------  \n        # Scorpo de vari\u00e1veis\n        #--------------------------------------------------------\n\n        time_pred_start    = datetime.now()\n        clf_name           = 'Rede Neural TensorFlow'\n        preds_valid_f      = {}\n        preds_test         = []\n        total_auc          = []\n        f_scores           = []\n        auc_mean           = []\n        f1_mean            = []\n        lloss_mean         = []\n        preds_test_prob    = 0    \n        df_score_history   = pd.DataFrame()\n        df_train_pred_fold = pd.DataFrame()\n        random             = str(np.random.rand(1)[0]).replace('.','')\n        model_name_        = model_name_ + '_score_{:2.5f}_{}_' + random + '.pkl.z'\n        pri_result         = 95\n\n        #--------------------------------------------------------  \n        # In\u00edcio do process de varilida\u00e7\u00e3o\n        #--------------------------------------------------------\n        is_dropout=''\n\n        if dropout_>0: \n            is_dropout='*'\n\n        if print_result_:\n            msg = 'Training model: {} - seed {} - Epoch: {} - lr: {} {}'\n            print('='*pri_result)            \n            print(msg.format(clf_name, seed_, epochs_, learning_rate_, is_dropout))\n            print('='*pri_result)\n\n        kf = StratifiedKFold(n_splits=fold_, random_state=seed, shuffle=True)\n\n        for fold,(idx_train, idx_val) in enumerate(kf.split(X_train_, y_train_, groups=y_train_)):\n\n            time_fold_start = datetime.now()\n\n            #--------------------------------------------------------  \n            # Sele\u00e7\u00e3o dos dados\n            #--------------------------------------------------------\n            X_trn, X_val = X_train_.iloc[idx_train], X_train_.iloc[idx_val]\n            y_trn, y_val = y_train_.iloc[idx_train], y_train_.iloc[idx_val]\n            index_valid  = X_val.index.tolist() \n\n            #--------------------------------------------------------  \n            # Processamento\n            #--------------------------------------------------------\n            X_trn = preprocessor.fit_transform(X_trn)\n            X_val = preprocessor.transform(X_val)    \n            X_tst = preprocessor.transform(X_test_.copy())               \n            #X_trn, X_val, X_tst= TunningModels.preprocessor(X_trn, X_val, X_test_.copy())\n            \n            #--------------------------------------------------------  \n            # Modelo\n            #--------------------------------------------------------\n            model, early_stopping, plateau = load_model(learning_rate_,\n                                                        activation_, \n                                                        X_trn.shape[1], \n                                                        dropout_\n                                                        )\n\n            history = model.fit(X_trn, \n                                y_trn,\n                                validation_data       = (X_val, y_val),\n                                batch_size            = batch_size_, \n                                epochs                = epochs_,\n                                callbacks             = [early_stopping, plateau],\n                                validation_batch_size = len(y_val), ##\n                                shuffle               = True,\n                                verbose               = 0\n                                )\n            \n            #--------------------------------------------------------  \n            # oof\n            #--------------------------------------------------------\n            preds_valid = model.predict(X_val).reshape(1,-1)[0] \n            y_pred      = (preds_valid>.5).astype(int)\n\n            #--------------------------------------------------------  \n            # Obtenha os valores m\u00e9dios de cada fold para a previs\u00e3o\n            #--------------------------------------------------------\n            #preds_test_prob.append(model.predict(X_test_).reshape(1,-1)[0])\n            preds_test_prob += model.predict(X_tst).reshape(1,-1)[0].clip(0,1e10) \/ fold_\n            # test_predictions_nn += model.predict([tt]).reshape(1,-1)[0].clip(0,1e10)\/n_folds  \n            \n            #--------------------------------------------------------  \n            # Salvar score de treinamento em um DataFrame \n            #--------------------------------------------------------\n            df_scores_fold          = pd.DataFrame(history.history)\n            df_scores_fold['folds'] = fold     \n            df_score_history        = pd.concat([df_score_history, df_scores_fold], axis=0)\n\n            #--------------------------------------------------------  \n            # Concatenar valida\u00e7\u00e3o e predi\u00e7\u00e3o\n            #--------------------------------------------------------        \n            df_val_pred_fold = pd.DataFrame({'fold'     : fold+1,\n                                             'index'    : index_valid, \n                                             'pred_val' : preds_valid, \n                                             'target'   : y_val})\n\n            df_train_pred_fold = pd.concat([df_train_pred_fold, df_val_pred_fold], axis=0)\n\n            #--------------------------------------------------------  \n            # M\u00e9tricas \n            #--------------------------------------------------------\n            auc   = metrics.roc_auc_score(y_val, y_pred)\n            f1    = metrics.f1_score(y_val, y_pred)\n            lloss = metrics.log_loss(y_val, preds_valid)   \n\n            auc_mean.append(auc)   \n            f1_mean.append(f1)    \n            lloss_mean.append(lloss) \n\n            msg = 'Fold: {} - AUC: {:2.5f} - F1-score: {:2.5f} - L.Loss: {:2.5f} - {}'\n\n            #--------------------------------------------------------  \n            # Print resultado Fold\n            #--------------------------------------------------------\n            time_fold_start_end = TunningModels.diff(time_fold_start, datetime.now())\n            print(msg.format(fold+1, auc, f1, lloss, time_fold_start_end))\n\n        #del X_trn, y_trn, X_val, y_val\n\n        df_train_pred_fold.sort_values(\"index\", axis=0, ascending=True, inplace=True)\n\n        #--------------------------------------------------------  \n        # Salvar predi\u00e7\u00e3o em disco\n        #--------------------------------------------------------\n        X_train_prob      = df_train_pred_fold['pred_val'].to_list()\n        score             = np.mean(auc_mean)\n        y_pred_test_prob_ = preds_test_prob # np.mean(preds_test_prob, axis=0)\n\n        if save_predict_:\n            TunningModels.save_data_model(model_             = model, \n                                          model_name_        = model_name_, \n                                          path_              = path_, \n                                          y_pred_train_prob_ = X_train_prob, \n                                          y_pred_test_prob_  = y_pred_test_prob_, \n                                          score_             = score, \n                                          seed_              = seed_, \n                                          level_             = level_, \n                                          target_            = target_\n                                          )  \n\n        #--------------------------------------------------------  \n        # Print m\u00e9dia dos Folds\n        #--------------------------------------------------------\n        time_pred_end = TunningModels.diff(time_pred_start, datetime.now())\n\n        if print_result_:\n            msg = '[Mean Fold]  AUC: {:.5f}(Std:{:.5f}) - F1: {:.5f} - L. Loss: {:.5f}  {}'        \n            print('-'*pri_result)            \n            print(msg.format(np.mean(auc_mean),np.std(auc_mean) , np.mean(f1_mean), np.mean(lloss_mean), time_pred_end))\n            print('='*pri_result)\n            print()\n\n        return model, score, y_pred_test_prob_, df_score_history, df_train_pred_fold\n    \n    def nn(self, trial):\n        \n        params = {'epochs'        : trial.suggest_int('epochs', 600, 1200), \n                  'learning_rate' : trial.suggest_float ('learning_rate', 0.0001, 0.0006 ), \n                  'batch_size'    : trial.suggest_int('batch_size', 2048, 4096),\n                  'activation'    : trial.suggest_categorical('activation', ['swish', 'relu']),     \n                  'dropout'       : trial.suggest_float('dropout', .0, .5)        \n                 }\n        \n        #if params.get('dropout') < .2:\n         #   params.update({'dropout': trial.suggest_float('dropout', .0)})\n                    \n        _, score, _, _, _ = TunningModels.cross_valid(model_name_    = self.name_clf,\n                                                      X_train_       = self.X_trn,\n                                                      y_train_       = self.y_trn,\n                                                      X_test_        = self.X_ts,\n                                                      epochs_        = params.get('epochs'),\n                                                      learning_rate_ = params.get('learning_rate'),\n                                                      batch_size_    = params.get('batch_size'),\n                                                      activation_    = params.get('activation'),\n                                                      fold_          = 5,\n                                                      target_        ='target',\n                                                      path_          = self.path,\n                                                      level_         = '1',\n                                                      save_predict_  = True,\n                                                      print_result_  = True,\n                                                      seed_          = self.seed, \n                                                      dropout_       = params.get('dropout')\n                                                      )        \n        \n        print('param = {}'.format(params))\n        print()\n\n        return score    ","b6df55ad":"my_seed = seed\ndef seedAll(seed):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \nseedAll(my_seed)","1997f43b":"# Some parameters to config  https:\/\/www.kaggle.com\/adityasharma01\/simple-nn-tps-nov-21\nEPOCHS        = 1000\nBATCH_SIZE    = 2048 \nACTIVATION    = 'swish'\nLEARNING_RATE = 0.001\nFOLDS         = 5\nname_model_clf = 'rnn_' \nname_model     = name_model_clf + '01' \n\nfeature_int      = X_test.filter(regex=r'f[0-9]').columns.to_list()\nfeature_cluster  = X_test.filter(regex=r'fe_clu').columns.to_list()\nfeature_static   = X_test.filter(regex=r'fe_[m-s]').columns.to_list()\nfeature_pipeline = make_pipeline(StandardScaler(),MinMaxScaler(feature_range=(0, 1)))\n\npreprocessor = make_column_transformer((feature_pipeline, feature_int), \n                                       (feature_pipeline, feature_cluster), \n                                       (feature_pipeline, feature_static),\n                                       n_jobs=-1\n                                      )\n\nmodel, score, y_pred_test, df_score_history, _ = TunningModels.cross_valid(model_name_    = name_model,\n                                                                           X_train_       = X,\n                                                                           y_train_       = y,\n                                                                           X_test_        = X_test,\n                                                                           feature_       = None,\n                                                                           epochs_        = EPOCHS,\n                                                                           learning_rate_ = LEARNING_RATE,\n                                                                           batch_size_    = BATCH_SIZE,\n                                                                           activation_    = ACTIVATION,\n                                                                           fold_          = FOLDS,\n                                                                           target_        ='target',\n                                                                           path_          = path,\n                                                                           level_         = '1',\n                                                                           save_predict_  = True,\n                                                                           print_result_  = True,\n                                                                           seed_          = seed\n                                                                             )  ","9801977a":"!ls Data\/submission\/tunning\/*rnn_01_*","b8030ad6":"for fold in range(df_score_history['folds'].nunique()):\n\n    history_f = df_score_history[df_score_history['folds'] == fold]\n    \n    lloss_val = history_f['val_loss'].mean()\n    subtitle  = '\\n Fold: {} - Loss Validation: {:2.5f} \\n'.format(fold+1, lloss_val)\n    \n    fig, ax = plt.subplots(1, 2, tight_layout=True, figsize=(12,5))\n    fig.suptitle(subtitle, fontsize=14)\n        \n    plt.subplot(1,2,1)\n    plt.plot(history_f.loc[:, ['loss', 'val_loss']], label= ['loss', 'val_loss'])\n    plt.legend(fontsize=10)\n    plt.grid()\n    \n    #print(\"Validation Loss: {:0.4f}\".format(history_f['val_loss'].min()));\n    \n    plt.subplot(1,2,2)  \n    plt.plot(history_f.loc[:, ['auc', 'val_auc']],label= ['auc', 'val_auc'])\n    plt.legend(fontsize=10)\n    plt.grid()","bfb92a9e":"%%time\n\nn_trials_  = 15\nscore_     = 0\nname_model = 'rnn_02_tunning'\n\nTunningModels.delete_files(name_model)\n\n# Inicialize a classe do modelo de otimiza\u00e7\u00e3o\nmodelOpt = TunningModels(name_model_     = name_model, \n                         X_trn_          = X,\n                         y_trn_          = y,\n                         X_ts_           = X_test,                                     \n                         feature_        = None, \n                         seed_           = seed, \n                         path_           = path\n                        )\n \nstudy = optuna.create_study(direction = 'maximize',\n                            sampler   = optuna.samplers.TPESampler(seed=seed),\n                            pruner    = optuna.pruners.MedianPruner(n_warmup_steps=10),\n                            study_name= 'nn_tuning'\n                            ) \n\nstudy.optimize(modelOpt.nn, n_trials=n_trials_)\n\nscore  = study.best_value \nparams = study.best_params \n\nif score > score_ : \n    score_      = score\n    seed_best   = seed\n    score_best  = score \n    params_best = params\n    \nprint()\nprint('-'*110)\nprint('Best score: {:2.5f}'.format(score_best))\nprint('Seed      : {}'.format(seed_best))\nprint('Parameters:\\n\\n{}'.format(params_best))\nprint()","360542c9":"!ls Data\/submission\/tunning\/*rnn_02_tunning*","b1883321":"###################################################################################################\n# Plot functions\n# --------------\n# Visualize the optimization history. See :func:`~optuna.visualization.plot_optimization_history` for the details.\nplot_optimization_history(study)","dc45a0c3":"#plot_intermediate_values(study)\n###################################################################################################\n# Visualize the learning curves of the trials. See :func:`~optuna.visualization.plot_intermediate_values` for the details.\n#plot_intermediate_values(study)","eb626a5d":"###################################################################################################\n# Visualize high-dimensional parameter relationships. See :func:`~optuna.visualization.plot_parallel_coordinate` for the details.\nplot_parallel_coordinate(study)","d7cc5445":"###################################################################################################\n# Select parameters to visualize.\nplot_parallel_coordinate(study, params=[\"batch_size\", \"epochs\", 'dropout'])","778f10e4":"###################################################################################################\n# Visualize hyperparameter relationships. See :func:`~optuna.visualization.plot_contour` for the details.\n#plot_contour(study)","44a0a557":"###################################################################################################\n# Select parameters to visualize.\nplot_contour(study, params=[\"batch_size\", \"epochs\"])","83d57b3c":"###################################################################################################\n# Visualize individual hyperparameters as slice plot. See :func:`~optuna.visualization.plot_slice` for the details.\nplot_slice(study)","d6a21ab3":"###################################################################################################\n# Select parameters to visualize.\nplot_slice(study, params=[\"batch_size\", \"epochs\"])","4102cc9e":"###################################################################################################\n# Visualize parameter importances. See :func:`~optuna.visualization.plot_param_importances` for the details.\n#plot_param_importances(study)","34e1a9d3":"###################################################################################################\n# Learn which hyperparameters are affecting the trial duration with hyperparameter importance.\n# optuna.visualization.plot_param_importances( study, target=lambda t: t.duration.total_seconds(), target_name=\"duration\")","8fc6508a":"###################################################################################################\n# Visualize empirical distribution function. See :func:`~optuna.visualization.plot_edf` for the details.\nplot_edf(study)","952d4319":"X_train, X_valid, y_train, y_valid = train_test_split(X, \n                                                      y, \n                                                      test_size    = 0.2,\n                                                      shuffle      = True, \n                                                      stratify     = y,\n                                                      random_state = 12359)\n\nX_train.shape, y_train.shape, X_valid.shape, y_valid.shape ","3f98b55c":"%%time\n\nEPOCHS        = params_best.get('epochs')\nBATCH_SIZE    = params_best.get('batch_size')\nACTIVATION    = params_best.get('activation')\nLEARNING_RATE = params_best.get('learning_rate')\nFOLDS         = 5\nname_model_clf = 'rnn_' \nname_model     = name_model_clf + '03tunning' \n\nmodel, score, y_pred_test, df_score_history, _ = TunningModels.cross_valid(model_name_     = name_model,\n                                                                            X_train_       = X_train,\n                                                                            y_train_       = y_train,\n                                                                            X_test_        = X_test,\n                                                                            feature_       = None,\n                                                                            epochs_        = EPOCHS,\n                                                                            learning_rate_ = LEARNING_RATE,\n                                                                            batch_size_    = BATCH_SIZE,\n                                                                            activation_    = ACTIVATION,\n                                                                            fold_          = FOLDS,\n                                                                            target_        ='target',\n                                                                            path_          = path,\n                                                                            level_         = '1',\n                                                                            save_predict_  = False,\n                                                                            print_result_  = True,\n                                                                            seed_          = seed\n                                                                            )  ","736f0cba":"for fold in range(df_score_history['folds'].nunique()):\n\n    history_f = df_score_history[df_score_history['folds'] == fold]\n    \n    lloss_val = history_f['val_loss'].mean()\n    subtitle  = '\\n Fold: {} - Loss Validation: {:2.5f} \\n'.format(fold+1, lloss_val)\n    \n    fig, ax = plt.subplots(1, 2, tight_layout=True, figsize=(12,5))\n    fig.suptitle(subtitle, fontsize=14)\n        \n    plt.subplot(1,2,1)\n    plt.plot(history_f.loc[:, ['loss', 'val_loss']], label= ['loss', 'val_loss'])\n    plt.legend(fontsize=10)\n    plt.grid()\n        \n    plt.subplot(1,2,2)  \n    plt.plot(history_f.loc[:, ['auc', 'val_auc']],label= ['auc', 'val_auc'])\n    plt.legend(fontsize=10)\n    plt.grid()","89cf240c":"X_valid_scaler = preprocessor.fit_transform(X_valid)\ny_hat_002 = model.predict(X_valid_scaler).reshape(1,-1)[0] ","c1c56d1f":"fpr, tpr, thresholds = metrics.roc_curve(y_valid, y_hat_002)\nplot_roc_curve(fpr, tpr, label=\"RNN\")\nplt.show()","1125af1c":"threshold = .5\nf1_002  = metrics.f1_score (y_valid, (y_hat_002 > threshold))\nauc_002 = metrics.roc_auc_score(y_valid, y_hat_002 )\nprint(metrics.classification_report(y_valid, (y_hat_002 > threshold).astype(int) ))\nprint('')\nprint('F1-score: {:2.5f}'.format(f1_002))\nprint('AUC     : {:2.5f}'.format(auc_002))","d4f8ebd6":"%%time \n\nSEED_          = [42, 59,100, 200, 1000, 1500, 2020, 2021, 5000, 10000, 7000]\ndf_seed        = pd.DataFrame()\nseed_best      = seed_best\n\nEPOCHS         = params_best.get('epochs')\nBATCH_SIZE     = params_best.get('batch_size')\nACTIVATION     = params_best.get('activation')\nLEARNING_RATE  = params_best.get('learning_rate')\nFOLDS          = 5\nname_model_clf = 'rnn_' \nname_model     = name_model_clf + '004_div_seed' \n\nTunningModels.delete_files(name_model)\n\nfor i, seed_ in  enumerate (SEED_):     \n        \n    seedAll(seed_)\n    \n    model, score, y_pred_test, df_score_history, _ = TunningModels.cross_valid(model_name_     = name_model,\n                                                                                X_train_       = X,\n                                                                                y_train_       = y,\n                                                                                X_test_        = X_test,\n                                                                                feature_       = None,\n                                                                                epochs_        = EPOCHS,\n                                                                                learning_rate_ = LEARNING_RATE,\n                                                                                batch_size_    = BATCH_SIZE,\n                                                                                activation_    = ACTIVATION,\n                                                                                fold_          = FOLDS,\n                                                                                target_        ='target',\n                                                                                path_          = path,\n                                                                                level_         = '1',\n                                                                                save_predict_  = True,\n                                                                                print_result_  = True,\n                                                                                seed_          = seed_\n                                                                                )  \n                \n    if score > score_best:         \n        seed_best  = seed_\n        score_best = score\n\n    df_seed['seed_' + str(seed_)] = y_pred_test \n   \nprint('Seed best: {}'.format(seed_best))\nprint('Score    : {:2.5f}'.format(score_best))","a250e1dd":" df_seed.head()","95083de7":"submission = pd.DataFrame({'id': df_submission.id, 'target': df_seed.mean(axis=1)})\nsubmission.to_csv(path + 'Data\/submission\/tunning\/rnn_005_div_seed_mean.csv', index=False)","d379a61e":"!ls Data\/submission\/tunning\/*rnn_005_div*","2fdf6d3a":"df_bach_size   = pd.DataFrame()\nseed_best      = seed_best\nbach_size_best = score_best\n\nEPOCHS         = params_best.get('epochs')\nBATCH_SIZE     = 2048\nACTIVATION     = params_best.get('activation')\nLEARNING_RATE  = params_best.get('learning_rate')\nFOLDS          = 5\nname_model_clf = 'rnn_' \nname_model     = name_model_clf + '006_div_bach_size' \n\nTunningModels.delete_files(name_model)\n\nfor i, seed_ in  enumerate (range(5)):     \n        \n    seedAll(seed_best)\n         \n    model, score, y_pred_test, df_score_history, _ = TunningModels.cross_valid(model_name_    = name_model,\n                                                                               X_train_       = X,\n                                                                               y_train_       = y,\n                                                                               X_test_        = X_test,\n                                                                               feature_       = None,\n                                                                               epochs_        = EPOCHS,\n                                                                               learning_rate_ = LEARNING_RATE,\n                                                                               batch_size_    = BATCH_SIZE,\n                                                                               activation_    = ACTIVATION,\n                                                                               fold_          = FOLDS,\n                                                                               target_        ='target',\n                                                                               path_          = path,\n                                                                               level_         = '1',\n                                                                               save_predict_  = True,\n                                                                               print_result_  = True,\n                                                                               seed_          = seed_best\n                                                                                )  \n    \n    if score > score_best:                \n        score_best     = score\n        bach_size_best = BATCH_SIZE\n\n    df_bach_size['bach_size_' + str(BATCH_SIZE)] = y_pred_test \n    \n    BATCH_SIZE += 1024\n\nprint('Seed best: {}'.format(seed_best))\nprint('Score    : {:2.5f}'.format(score_best))\nprint('Bach size: {}'.format(bach_size_best))","275583df":"df_bach_size.head() ","334ca700":"submission = pd.DataFrame({'id': df_submission.id, 'target': df_bach_size.mean(axis=1)})\nsubmission.to_csv(path + 'Data\/submission\/tunning\/rnn_007_bach_size_meand.csv', index=False)","5cafd95b":"!ls Data\/submission\/tunning\/*004_div_bach_size*","aa765a7e":"params_best","ebd1b511":"%%time \ndf_train_rnn, df_test_rnn = TunningModels.df_return_preds_tunning('rnn', target_ ='target') # \nprint(df_train_rnn.shape, df_test_rnn.shape)\nprint()","2e27cadf":"df_train_rnn.head()","5088b514":"df_test_rnn.head()","e4f598bf":"jb.dump(df_train_rnn,  \"Data\/pkl\/df_train_rnn.pkl.z\")\njb.dump(df_test_rnn,  \"Data\/pkl\/df_test_rnn.pkl.z\")","923d3950":"!ls Data\/pkl\/*rnn*","4563973c":"df_test_rnn.mean(axis=1).describe()","cf740ba6":"y_pred_mean = df_test_rnn.mean(axis=1)\nsubmission = pd.DataFrame({'id': df_submission.id, 'target': y_pred_mean })\nsubmission.to_csv(path + 'Data\/submission\/tunning\/rnn_008_st_mean.csv', index=False) ","8e660be2":"graf_corr(df_train_rnn)","cc8cb414":"corr_features = correlation(df_train_rnn, 0.9)\nlen(set(corr_features))","9d3c8fa2":"#df_train_lgbm.drop(labels=corr_features, axis=1, inplace=True)\n\ngraf_corr(df_train_rnn) ","78b4048c":"### 1.4.2. Mem\u00f3ria","ea3f55f3":"#### 1.2.5.1. Recuparar dataset","4f5688f8":"#### 1.2.5.4. Correla\u00e7\u00e3o","25776dbe":"#### 1.2.3.1. Analise do Modelo","b1a28742":"# <div class=\"alert alert-success\"> 3.  TUNNING <\/div>","2de6c927":"#### 1.2.4.3. FEATURE","2085a3e6":"#### 1.2.5.3. Gerar submission \nVamos gerar uma submission com a media das previss\u00f5es, para termos uma ideia de como estamos. ","c1141f56":"## 1.2. Bibliotecas","442bbfd8":"## 1.3. Fun\u00e7\u00f5es","7a898653":"# <div class=\"alert alert-success\">  1. IMPORTA\u00c7\u00d5ES <\/div>","72d84cbb":"#### 1.2.4.1. Bath Size","58e7fc7b":"#### 1.2.5.2. Descritiva","8c77da1b":"### 3.1.1. Pipeline de transforma\u00e7\u00f5es","d6723679":"### 1.2.3. Modelo Final\nAgora que temos os melhores parametros, vamos treinar uma modelo com esse parametros e fazer algumas an\u00e1lises. ","980acd11":"Temos muitas previs\u00f5es autocorrelacionadas, vamos fazer a exclus\u00e3o de algumas.","a505c5e6":"#### 1.2.4.1. SEED\nNesta etapa vamos utilizar os melhores parametros, que encontramos na tunagem acima, com `seed` diferentes. ","64a5fbec":"#### 1.2.3.1.1. Curva Roc","9022908c":"## 3.1. Classe Tunning ","79d3fc89":"## 1.5. Carregar Dados","1e4b4a84":"### 1.2.5. Ensable ","8b43c3b4":"<h1 div class='alert alert-success'><center> Tunning Hyperparameters Rede Neural\n\n <\/center><\/h1>\n\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/26480\/logos\/header.png?t=2021-04-09-00-57-05)","ea82029c":"### An\u00e1lise ","38b70a80":"### 3.1.2. Tunning Rede Neural","59e177e0":"### 1.4.1. Informa\u00e7\u00f5es","83db7c3f":"## 1.1. Instala\u00e7\u00f5es","45780bd1":"### 1.2.4. Divercidade","be10ca18":"<div class=\"alert alert-info\" role=\"alert\"> \n    \n**`NOTA:`** <br>\n \n\n<\/div>\n","e94b5593":"# <div class=\"alert alert-success\"> 2. Split Train\/Test <\/div>","af95db98":"## 1.4. GPU"}}