{"cell_type":{"a8d5c87c":"code","6d881a77":"code","e1f960aa":"code","dcc09d95":"code","f9388842":"code","a4721319":"code","1ba6c0f6":"code","4123c5c9":"code","e602bf0e":"code","efb0ef2b":"code","a33c9f75":"code","b1345c8a":"code","293222a8":"code","0d38bcae":"code","0f5b3c6c":"code","293b7181":"code","78b3a25d":"code","346bc459":"code","478952d6":"code","bfae73e9":"code","e5fb07c2":"code","77974511":"code","273ed00e":"code","5e5b200c":"code","f458f4d6":"code","ccd84255":"code","50073708":"code","536463e7":"code","b0901e5a":"code","1f69c220":"code","bcd40d51":"code","b9bd77b5":"code","d7dc2372":"code","3b1f75df":"code","fd6ddb79":"code","5cbbc489":"code","2f8b92bd":"code","f2ae42b2":"code","5e68d227":"code","50d28b01":"code","43358b9c":"code","9f6b7107":"markdown","17d94dca":"markdown","b23f79fc":"markdown","a9509a1d":"markdown","d2ab915f":"markdown","11f5dd44":"markdown","851d5f2f":"markdown","d5a054cd":"markdown","2276c6a8":"markdown","b9d6ea46":"markdown","77f5a8b6":"markdown","0ea1dbcc":"markdown","c1b937fe":"markdown","37016650":"markdown","c863a04e":"markdown","1b3a943f":"markdown","7436b5f7":"markdown","d13de1ea":"markdown","2ea033c0":"markdown","e65a466f":"markdown","5a1214b3":"markdown","2caade6a":"markdown","4c274d1f":"markdown","e2ff8bca":"markdown","9f367c0a":"markdown","5735f70a":"markdown","72d252dc":"markdown","18b4c063":"markdown","04ecd3f0":"markdown","3c93f070":"markdown","9dd8d97b":"markdown","74d5dc5c":"markdown","1f7dfba0":"markdown","abf9eb24":"markdown","23b4da87":"markdown","eb9e509f":"markdown","a9f1e173":"markdown","8ea74dc3":"markdown","94da6c9c":"markdown","ed2924ad":"markdown"},"source":{"a8d5c87c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6d881a77":"import numpy             as np\nimport pandas            as pd\nimport matplotlib.pyplot as plt\nimport seaborn           as sns\nimport plotly.graph_objs as go\nimport plotly.express    as px \nimport nltk\nimport re\nimport string\n\nfrom scipy.stats import norm\nfrom wordcloud   import WordCloud, STOPWORDS\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom wordcloud import WordCloud,STOPWORDS\nstopwords = set(STOPWORDS)\n\nfrom textblob import TextBlob\nimport re\nfrom collections import Counter\n\nfrom sklearn.metrics import classification_report,accuracy_score,confusion_matrix\nfrom IPython.display import Markdown as md\n","e1f960aa":"data=pd.read_csv(\"..\/input\/covid19-tweets\/covid19_tweets.csv\")","dcc09d95":"data.head()","f9388842":"#Dtypes of features\ndata.info()","a4721319":"#Description of the dataset\ndata.describe()","1ba6c0f6":"#Number of rows and columns in the dataset\nprint(\"There are {} rows and {} columns in the dataset.\".format(data.shape[0],data.shape[1]))","4123c5c9":"text = \",\".join(review for review in data.text if 'COVID' not in review and 'https' not in review and 'Covid' not in review)\nwordcloud = WordCloud(max_words=200, colormap='Set3',background_color=\"black\").generate(text)\nplt.figure(figsize=(15,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.figure(1,figsize=(12, 12))\nplt.title('Prevalent words in tweets',fontsize=19)\nplt.show()","e602bf0e":"plt.figure(figsize=(10,12))\nsns.barplot(data[\"user_location\"].value_counts().values[0:10],\n            data[\"user_location\"].value_counts().index[0:10]);\nplt.title(\"Top 10 Countries with maximum tweets\",fontsize=14)\nplt.xlabel(\"Number of tweets\",fontsize=14)\nplt.ylabel(\"Country\",fontsize=14)\nplt.show()","efb0ef2b":"plt.figure(figsize=(17, 5))\nsns.heatmap(data.isnull(), cbar=True, cmap='Paired_r')\nplt.xlabel(\"Column_Name\", size=14, weight=\"bold\")\nplt.title(\"Places of missing values in column\",fontweight=\"bold\",size=14)\nplt.show()","a33c9f75":"def unique_values_funct(data_frame):\n    unique_dataframe = pd.DataFrame()\n    unique_dataframe['Features'] = data_frame.columns\n    uniques = []\n    for col in data_frame.columns:\n        u = data_frame[col].nunique()\n        uniques.append(u)\n    unique_dataframe['Uniques'] = uniques\n    return unique_dataframe\n\nudf = unique_values_funct(data)\n\nf, ax = plt.subplots(1,1, figsize=(10,5))\nsns.barplot(x=udf['Features'], y=udf['Uniques'], alpha=0.8)\nplt.title('Bar plot for unique values in each column', fontsize=14)\nplt.ylabel('Unique values', fontsize=14)\nplt.xlabel('Columns', fontsize=14)\nplt.xticks(rotation=90)\nplt.show()","b1345c8a":"data[\"num of words in text\"] = data[\"text\"].apply(lambda x: len(x))\nplt.figure(figsize=(10,7))\nsns.kdeplot(data[\"num of words in text\"])\nplt.title(\"Distribution of words in text column\")\nplt.xlabel(\"Number of words\")\nplt.show()","293222a8":"username_count = data['user_name'].value_counts().reset_index().rename(columns={\n    'user_name':'tweet_count','index':'user_name'})\n\nplt.figure(figsize=(8, 10))\nsns.barplot(y='user_name',x='tweet_count',data=username_count.head(20))\ny=username_count['tweet_count'].head(20)\nfor index, value in enumerate(y):\n    plt.text(value, index, str(value),fontsize=12)\nplt.title('Users with maximum tweets',weight='bold', size=13)\nplt.ylabel('Username', size=12, weight='bold')\nplt.xlabel('TweetCount', size=12, weight='bold')\nplt.show()","0d38bcae":"plt.figure(figsize=(5, 5))\nsns.countplot(x =\"user_verified\",data=data, palette=\"Set2\")\nplt.title(\"Verified user accounts or not ?\")\nplt.xticks([False,True],['Unverified','Verified'])\nplt.show()","0f5b3c6c":"plt.figure(figsize=(15,5))\nsrc = data['source'].value_counts().sort_values(ascending=False)\nsource = src.head(10)\nsource.plot.bar(color=['red', 'green', 'blue', 'black','cyan','pink','purple','violet','yellow','orange'])\nplt.title('Platform with maximum number of tweets',size=13)\nplt.xlabel('User Platform',size=13)\nplt.ylabel('Tweet Count',size=13)\nplt.show()","293b7181":"pla = data['source'][data['user_location'] == 'India'].value_counts().sort_values(ascending=False)\nexplode = (0, 0.1, 0, 0,0.01) \nplt.figure(figsize=(8,8))\npla[0:5].plot(kind = 'pie', title = '5 Most Tweet Sources used in India', autopct='%1.1f%%',shadow=True,explode = explode)\nplt.show()","78b3a25d":"pla = data['source'][data['user_location'] == 'United States'].value_counts().sort_values(ascending=False)\nexplode = (0, 0.1, 0, 0,0.01) \nplt.figure(figsize=(8,8))\npla[0:5].plot(kind = 'pie', title = '5 Most Tweet Sources used in USA', autopct='%1.1f%%',shadow=True,explode = explode)\nplt.show()","346bc459":"pla = data['source'][data['user_location'] == 'Switzerland'].value_counts().sort_values(ascending=False)\nexplode = (0, 0.1, 0, 0,0.01) \nplt.figure(figsize=(8,8))\npla[0:5].plot(kind = 'pie', title = '5 Most Tweet Sources used in Switzerland', autopct='%1.1f%%',shadow=True,explode = explode)\nplt.show()","478952d6":"pla = data['source'][data['user_location'] == 'Australia'].value_counts().sort_values(ascending=False)\nexplode = (0, 0.1, 0, 0,0.01) \nplt.figure(figsize=(8,8))\npla[0:5].plot(kind = 'pie', title = '5 Most Tweet Sources used in Australia', autopct='%1.1f%%',shadow=True,explode = explode)\nplt.show()","bfae73e9":"pla = data['source'][data['user_location'] == 'United Kingdom'].value_counts().sort_values(ascending=False)\nexplode = (0, 0.1, 0, 0,0.01) \nplt.figure(figsize=(8,8))\npla[0:5].plot(kind = 'pie', title = '5 Most Tweet Sources used in United Kingdom', autopct='%1.1f%%',shadow=True,explode = explode)\nplt.show()","e5fb07c2":"top_tags=data['hashtags'].value_counts().sort_values(ascending=False)\nplt.figure(figsize=(8,8))\nexplode = (0, 0.1, 0, 0,0.01) \n\ntop_tags[0:5].plot(kind = 'pie',title = 'Top 5 hashtags',autopct='%1.1f%%',shadow=True,explode = explode)","77974511":"data[\"date\"] = pd.to_datetime(data[\"date\"])\ndata[\"Month\"] = data[\"date\"].apply(lambda x : x.month)\ndata[\"day\"] = data[\"date\"].apply(lambda x : x.dayofweek)\ndmap = {0:'Mon',1:'Tue',2:'Wed',3:'Thu',4:'Fri',5:'Sat',6:'Sun'}\ndata[\"day\"] = data[\"day\"].map(dmap)\nplt.title(\"Day with maximun tweets\")\nsns.countplot(data[\"day\"])","273ed00e":"senti_df = pd.read_csv('\/kaggle\/input\/twitterdata\/finalSentimentdata2.csv')","5e5b200c":"senti_df","f458f4d6":"senti_df.info()","ccd84255":"senti_df['sentiment'].nunique","50073708":"senti_df.describe()","536463e7":"plt.figure(figsize=(10, 5))\nsns.heatmap(senti_df.isnull(), cbar=True, cmap='Paired_r')\nplt.xlabel(\"Column_Name\", size=14, weight=\"bold\")\nplt.title(\"Places of missing values in column\",fontweight=\"bold\",size=14)\nplt.show()","b0901e5a":"from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\n\npunc=string.punctuation\nstop_words = set(stopwords.words('english'))\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\ndef clean_text(text):\n    \n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    \n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    \n    #Removing stopwords\n    text=\" \".join([word for word in str(text).split() if word not in stop_words])\n    \n    #Stemming\n    text = \" \".join([stemmer.stem(word) for word in text.split()])\n    \n    #Lemmatization\n    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n    \n    return text\n\nsenti_df['text'] = senti_df['text'].apply(lambda x: clean_text(x))","1f69c220":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nsenti_df['text']=senti_df['text'].apply(lambda x: remove_emoji(x))","bcd40d51":"senti_df.head()","b9bd77b5":"\"\"\"\n!pip install textblob\n\nfrom textblob import TextBlob\n\ndef correct_bytextblob(sent):\n    return str(TextBlob(sent).correct())\n\nsenti_df['text'] = senti_df['text'].apply(lambda x: correct_bytextblob(x))\n\nsenti_df.to_csv('clean_tweets.csv',index=False)\n\"\"\"","d7dc2372":"from sklearn.model_selection import train_test_split\n\ntrain,valid = train_test_split(senti_df,test_size = 0.2,random_state=0,stratify = senti_df.sentiment.values) #stratification means that the train_test_split method returns training and test subsets that have the same proportions of class labels as the input dataset.\nprint(\"train shape : \", train.shape)\nprint(\"valid shape : \", valid.shape)","3b1f75df":"from sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\nstop = list(stopwords.words('english'))\nvectorizer = CountVectorizer(decode_error = 'replace',stop_words = stop)\n\nX_train = vectorizer.fit_transform(train.text.values)\nX_valid = vectorizer.transform(valid.text.values)\n\ny_train = train.sentiment.values\ny_valid = valid.sentiment.values\n\nprint(\"X_train.shape : \", X_train.shape)\nprint(\"X_train.shape : \", X_valid.shape)\nprint(\"y_train.shape : \", y_train.shape)\nprint(\"y_valid.shape : \", y_valid.shape)","fd6ddb79":"from sklearn.naive_bayes import MultinomialNB\n\nnaiveByes_clf = MultinomialNB()\n\nnaiveByes_clf.fit(X_train,y_train)\n\nNB_prediction = naiveByes_clf.predict(X_valid)\nNB_accuracy = accuracy_score(y_valid,NB_prediction)\nprint(\"training accuracy Score    : \",naiveByes_clf.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",NB_accuracy )\nprint(classification_report(NB_prediction,y_valid))","5cbbc489":"from sklearn.linear_model import SGDClassifier\n\nsgd_clf = SGDClassifier(loss = 'hinge', penalty = 'l2', random_state=0)\n\nsgd_clf.fit(X_train,y_train)\n\nsgd_prediction = sgd_clf.predict(X_valid)\nsgd_accuracy = accuracy_score(y_valid,sgd_prediction)\nprint(\"Training accuracy Score    : \",sgd_clf.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",sgd_accuracy )\nprint(classification_report(sgd_prediction,y_valid))","2f8b92bd":"from sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier()\n\nrf_clf.fit(X_train,y_train)\n\nrf_prediction = rf_clf.predict(X_valid)\nrf_accuracy = accuracy_score(y_valid,rf_prediction)\nprint(\"Training accuracy Score    : \",rf_clf.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",rf_accuracy )\nprint(classification_report(rf_prediction,y_valid))","f2ae42b2":"#takes huge amount of time to execute\nimport xgboost as xgb\n\nxgboost_clf = xgb.XGBClassifier()\n\nxgboost_clf.fit(X_train, y_train)\n\nxgb_prediction = xgboost_clf.predict(X_valid)\nxgb_accuracy = accuracy_score(y_valid,xgb_prediction)\nprint(\"Training accuracy Score    : \",xgboost_clf.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",xgb_accuracy )\nprint(classification_report(xgb_prediction,y_valid))","5e68d227":"from sklearn.svm import SVC\n\nsvc = SVC()\n\nsvc.fit(X_train, y_train)\n\nsvc_prediction = svc.predict(X_valid)\nsvc_accuracy = accuracy_score(y_valid,svc_prediction)\nprint(\"Training accuracy Score    : \",svc.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",svc_accuracy )\nprint(classification_report(svc_prediction,y_valid))","50d28b01":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\n\nlogreg.fit(X_train, y_train)\n\nlogreg_prediction = logreg.predict(X_valid)\nlogreg_accuracy = accuracy_score(y_valid,logreg_prediction)\nprint(\"Training accuracy Score    : \",logreg.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",logreg_accuracy )\nprint(classification_report(logreg_prediction,y_valid))","43358b9c":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', \n              'Stochastic Gradient Decent', 'XGBoost'],\n    'Test accuracy': [svc_accuracy, logreg_accuracy, \n              rf_accuracy, NB_accuracy, \n              sgd_accuracy, xgb_accuracy,]})\n\nmodels.sort_values(by='Test accuracy', ascending=False)","9f6b7107":"### 6. Users with maximum tweets(Top 20)","17d94dca":"#### 8.4> 5 Most Tweet Sources used in Australia","b23f79fc":"All the model test accuracy in descending order","a9509a1d":"## Visualization","d2ab915f":"I've checked vectorizing using TF-IDF technique but its not performing well so going ahead with CountVectorizer","11f5dd44":"### 2. Number of Tweets by location(Top 10)","851d5f2f":"#### 8.3> 5 Most Tweet Sources used in Switzerland","d5a054cd":"### Naive Bayes","2276c6a8":"## Sentiment Analysis on Covid19 Tweets","b9d6ea46":"### 9. Plot top 5 hashtags","77f5a8b6":"As we can see we dont have any missing\/NaN value in our dataset.","0ea1dbcc":"## Splitting the dataset","c1b937fe":"### 10. Day with most number of tweets","37016650":"Downloaded tweets with the hashtag #covid19\n","c863a04e":"### 7. Plot verified users account","1b3a943f":"## ML model building","7436b5f7":"#### Extreme Gradient Boosting","d13de1ea":"## Data Preprocessing\n","2ea033c0":"Since the classes are almost balanced, we dont need to perform any sampling technique.","e65a466f":"### 5. Distribution of words in text column","5a1214b3":"# This notebook contains EDA,visualization and sentiment analysis on Covid-19 tweets","2caade6a":"### Random Forest","4c274d1f":"To peform sentiment analysis we need labeled dataset. The data can be downloaded from here: https:\/\/www.kaggle.com\/surajkum1198\/twitterdata","e2ff8bca":"### Correct spelling of incorrect words","9f367c0a":"**Let's look at the dataset we have :**\n\nuser_name - contains the user name of that person.\n\nuser_location - contains the user's loaction from where he\/she has tweeted.\n\nuser_description - It contains the user description on Tweeter\n\nuser_created - It contains the user id created time and date.\n\nuser_followers - It conatins the followers of users\n\nuser_friends - It contains the user's friends on Tweeter\n\nuser_favourites - It conatins user's favourites on Tweeter.\n\nuser_verified - User is verified or not ( True \/ False )\n\ndate - Date of Tweet\n\ntext - Text of Tweet he\/she has tweeted.\n\nhashtags - how many hashtags his\/her tweet have.\n\nsource - It contains the source of that.\n\nis_retweet - Any retweets it have or not ( True \/ False ).","5735f70a":"### 8. Plot platform with maximum number of tweets","72d252dc":"### 3. Heatmap representation of missing values","18b4c063":"### 4. Bar plot of unique values in each column","04ecd3f0":"## Load all the required libraries","3c93f070":"#### 8.2> 5 Most Tweet Sources used in USA","9dd8d97b":"Removing emojis","74d5dc5c":"If you are using high configuration machine then only perform this step of correcting incorrect words.","1f7dfba0":"Now let us preprocess text using some NLP tchniques like:\n\n1. converting to lowercase\n2. remove text in square brackets,\n3. remove links,\n4. remove punctuation\n5. remove words containing numbers\n6. Removing Punctuation\n7. Removing stopwords\n8. Stemming\n9. Lemmatization","abf9eb24":"#### 8.5> 5 Most Tweet Sources used in United Kingdom","23b4da87":"#### 8.1> 5 Most Tweet Sources used in India","eb9e509f":"### Logistic Regression","a9f1e173":"### Stochastic Gradient Descent","8ea74dc3":"### 1. Word cloud","94da6c9c":"## Vectorizing","ed2924ad":"### Support Vector Machine "}}