{"cell_type":{"9ae814bc":"code","f1cb13f1":"code","7065ca9f":"code","8eebe2fe":"code","2c25d0b7":"code","f6f7d415":"code","0e958219":"code","eaa02ade":"code","7018c4af":"code","c1044717":"code","24f2c892":"code","58051d35":"code","c4da451a":"code","9c9d48e2":"code","501714c1":"code","19b34870":"code","18ac7db1":"code","70ada87b":"code","ceedc326":"code","35c1f348":"code","5d194bdc":"code","572f08c3":"code","2e6fade3":"code","1ab49da3":"code","5e1b2ed7":"code","6c938966":"code","b3ec40f4":"code","734a1781":"markdown","d74dae79":"markdown","ba562291":"markdown","25fedf44":"markdown","1eb4709f":"markdown","35921b70":"markdown","3552d5ed":"markdown","72d4e9cf":"markdown","d5cd0a41":"markdown","e3d72648":"markdown","bad38df5":"markdown","58e17796":"markdown","6830878d":"markdown","e05a33dd":"markdown","85f20bce":"markdown","849e6a4c":"markdown","7b4a67d2":"markdown","4e6c7876":"markdown"},"source":{"9ae814bc":"import os\nfrom typing import Tuple, List, Sequence, Callable\n\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\n\nfrom torch import nn, Tensor\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\n\n!pip install -U git+https:\/\/github.com\/albu\/albumentations\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport torch.nn.functional as F\nfrom sklearn.model_selection import train_test_split\n\nimport time","f1cb13f1":"from google.colab import drive\ndrive.mount('\/content\/drive\/')","7065ca9f":"# share drive \ub97c \uc0ac\uc6a9\ud560 \uacbd\uc6b0, \/content\/drive\/Shared drives\nos.chdir('\/content\/drive\/MyDrive\/statml_competition\/')","8eebe2fe":"if torch.cuda.is_available():\n  device = torch.device('cuda:0')\nelse:\n  device = torch.device('cpu')\n\nprint('using device:', device)","2c25d0b7":"device = \"cuda:0\"\ndtype = torch.float\nltype = torch.long # entropy","f6f7d415":"# fake 1, real 0 \ub85c \uc0ac\uc6a9\ntrain_df = pd.read_csv('.\/face_image\/face_images.csv')\ntrain_df.head()","0e958219":"print(train_df.shape)","eaa02ade":"print(train_df['fake'].value_counts())\n\ntrain_df['fake'].value_counts().plot.bar()\nplt.ylabel('count')\nplt.title('Distribution of true\/fake')\nplt.show()","7018c4af":"real_df = train_df[train_df['real']==1]\n\nfig = plt.figure(figsize=(10,10))\nfor i in range(25):\n    plt.subplot(5, 5, i+1)\n    real_path = real_df.iloc[i,]['path']\n    image = cv2.imread(real_path, cv2.COLOR_BGR2RGB)\n    plt.imshow(image[...,::-1]\/255.0)\n    plt.suptitle(\"Real faces\",fontsize=20)\n    plt.axis('off')\nplt.show()","c1044717":"fake_df = train_df[train_df['fake']==1]\n\nfig = plt.figure(figsize=(10,10))\nfor i in range(25):\n    plt.subplot(5, 5, i+1)\n    fake_path = fake_df.iloc[i,]['path']\n    image = cv2.imread(fake_path, cv2.COLOR_BGR2RGB)\n    plt.imshow(image[...,::-1]\/255.0)\n    plt.suptitle(\"Fake faces\",fontsize=20)\n    plt.axis('off')\nplt.show()","24f2c892":"train_df.head()","58051d35":"class FaceDataset(Dataset):\n  def __init__(self, image_label, transforms) :\n    self.df = image_label\n    self.transforms = transforms\n        \n  def __len__(self) -> int:\n    return self.df.shape[0]\n\n  def __getitem__(self, index: int) -> Tuple[Tensor]:\n    assert index <= len(self), 'index range error' \n      \n    image_dir = self.df.iloc[index, ]['path']\n    image_id = self.df.iloc[index, ]['fake'].astype(np.int64)\n    \n    image =  cv2.imread(image_dir, cv2.COLOR_BGR2RGB)\n    target = torch.as_tensor(image_id, dtype=torch.long)\n\n    if self.transforms is not None :\n      image = self.transforms(image=image)['image']\n\n    image = image\/255.0\n    \n    return image, target\n\nclass TestDataset(Dataset):\n  def __init__(self, image, transforms) :\n    self.image = image\n    self.transforms = transforms\n        \n  def __len__(self) -> int:\n    return len(self.image)\n\n  def __getitem__(self, index: int) -> Tuple[Tensor]:\n    assert index <= len(self), 'index range error' \n    \n    image_name = self.image[index]\n    image_dir = '.\/face_image\/test\/' + image_name\n\n    image =  cv2.imread(image_dir, cv2.COLOR_BGR2RGB)\n    \n    if self.transforms is not None :\n      image = self.transforms(image=image)['image']\n    image = image\/255.0\n\n    return image_name, image","c4da451a":"transforms_tr = A.Compose([\n    A.Resize(128, 128),\n    ToTensorV2(), ])\n\ntransforms_val = A.Compose([\n    A.Resize(128, 128),\n    ToTensorV2(), ])","9c9d48e2":"train, valid = train_test_split(train_df, test_size=0.1)\n\nprint(f'Train Set dim : (%d, %d)' % (train.shape))\nprint(f'Valid Set dim : (%d, %d)' % (valid.shape))","501714c1":"tr_dataset = FaceDataset(image_label=train, transforms=transforms_tr)\nval_dataset = FaceDataset(image_label=valid, transforms=transforms_val)\n\ntrain_loader = DataLoader(tr_dataset, batch_size=64, shuffle=True, num_workers=2)\nvalid_loader = DataLoader(val_dataset, batch_size=64, shuffle=True, num_workers=2)","19b34870":"print(\"train set size :\",len(tr_dataset))\nprint(\"valid set size :\",len(val_dataset))","18ac7db1":"class CNN(torch.nn.Module):\n  def __init__(self):\n    super(CNN, self).__init__()\n    self.keep = 0.5\n\n    self.conv = nn.Sequential(\n        nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1),\n        nn.BatchNorm2d(8),\n        nn.ReLU(),\n        nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n        nn.BatchNorm2d(16),\n        nn.ReLU())\n\n    self.pool_conv1 = nn.Sequential(\n        nn.MaxPool2d(kernel_size=2, stride=2),\n        nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n        nn.BatchNorm2d(32),\n        nn.ReLU())\n    \n    self.pool_conv2 = nn.Sequential(\n        nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n        nn.Conv2d(32, 16, kernel_size=3, stride=1, padding=1),\n        nn.BatchNorm2d(16),\n        nn.ReLU())\n    \n    self.flatten = nn.Flatten()\n    self.fc_layer = nn.Sequential(\n            nn.Linear(33*33*16, 625, bias=True),\n            nn.ReLU(),\n            nn.Dropout(p=1 - self.keep))\n    \n    self.output = torch.nn.Linear(625, 2, bias=True)\n\n  def forward(self, x):\n    # input : 128x128 @ 3\n    out = self.conv(x)\n    # 128x128 @ 16\n    out = self.pool_conv1(out)\n    # 64x64 @ 32\n    out = self.pool_conv2(out)\n    # 33x33 @ 16\n    out = self.flatten(out)\n    # 33*33*16\n    out = self.fc_layer(out)\n    # 625\n    scores = self.output(out)\n    # output : 2\n    return scores\n\nmodel = CNN()","70ada87b":"model","ceedc326":"model.to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\nlr_sched = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)","35c1f348":"num_epochs = 30\n\nvalid_early_stop = 0\nvalid_best_loss = float('inf')\nEARLY_STOPPING_EPOCH = 5\nsince = time.time()\n\nfinal_train_loss = []\nfinal_train_acc = []\nfinal_valid_loss = []\nfinal_valid_acc = []\n\nmodel.train()\nfor e in range(num_epochs) :\n  print(f' ====================== epoch %d ======================' % (e+1) )\n  train_loss_list = []\n  train_acc_list = []\n\n  # train\n  for i, (images, targets) in enumerate(train_loader) : \n    optimizer.zero_grad()\n    \n    images = images.to(device, dtype)\n    targets = targets.to(device, ltype)\n  \n    scores = model(images)\n    _, preds = scores.max(dim=1)\n\n    loss = F.cross_entropy(scores, targets)\n    loss.backward()\n    optimizer.step()\n    \n    correct = sum(targets == preds).cpu()\n    acc=(correct\/64 * 100)\n\n    train_loss_list.append(loss)\n    train_acc_list.append(acc)\n\n    if i % 20 == 0 :\n      print(f'Iteration %3.d | Train Loss  %.4f | Classifier Accuracy %2.2f' % (i, loss, acc))\n\n  train_mean_loss = np.mean(train_loss_list, dtype=\"float64\")\n  train_mean_acc = np.mean(train_acc_list, dtype=\"float64\")\n\n  final_train_loss.append(train_mean_loss)\n  final_train_acc.append(train_mean_acc)\n  \n  epoch_time = time.time() - since\n  since = time.time()\n\n  print('')\n  print(f'[Summary] Elapsed time : %.0f m %.0f s' % (epoch_time \/\/ 60, epoch_time % 60))\n  print(f'Train Loss Mean %.4f | Accuracy %2.2f ' % (train_mean_loss, train_mean_acc) )\n\n  # validation \n  valid_loss_list = []\n  valid_acc_list = []\n  for i, (images, targets) in enumerate(valid_loader) : \n    optimizer.zero_grad()\n    images = images.to(device=device, dtype=dtype)\n    targets = targets.to(device=device, dtype=ltype)\n\n    with torch.no_grad():\n      scores = model(images)\n      loss = F.cross_entropy(scores, targets)\n      _, preds = scores.max(dim=1)\n    \n    correct = sum(targets == preds).cpu()\n    acc=(correct\/64 * 100)\n\n    valid_loss_list.append(loss)\n    valid_acc_list.append(acc)\n \n  val_mean_loss = np.mean(valid_loss_list, dtype=\"float64\")\n  val_mean_acc = np.mean(valid_acc_list, dtype=\"float64\")\n\n  final_valid_loss.append(val_mean_loss)\n  final_valid_acc.append(val_mean_acc)\n\n  print(f'Valid Loss Mean %.4f | Accuracy %2.2f ' % (val_mean_loss, val_mean_acc) )\n  print('')\n\n  if val_mean_loss < valid_best_loss:\n    valid_best_loss = val_mean_loss\n    valid_early_stop = 0\n    # new best model save (valid \uae30\uc900)\n    best_model = model\n    path = '.\/model\/'\n    torch.save(best_model.state_dict(), f'{path}model{val_mean_acc:2.2f}_epoch_{e}.pth')\n\n  else:\n    # early stopping    \n    valid_early_stop += 1\n    if valid_early_stop >= EARLY_STOPPING_EPOCH:\n      print(\"EARLY STOPPING!!\")\n      break\n\n  lr_sched.step()","5d194bdc":"fig, (ax1, ax2) = plt.subplots(2, sharex=True)\nax1.plot(final_train_loss)\nax1.plot(final_valid_loss)\nax1.legend(['train', 'valid'])\nax1.set_title('Loss')\n\nax2.plot(final_train_acc)\nax2.plot(final_valid_acc)\nax2.legend(['train', 'valid'])\nax2.set_title('Accuracy')","572f08c3":"test_dataset = TestDataset(submission['image'], transforms_val)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)","2e6fade3":"submission = pd.read_csv(\".\/face_image\/submission.csv\")\nsubmission.head()","1ab49da3":"predictions = []\nfiles = []\n\nwith torch.no_grad():\n  for img_names, images in test_loader:\n    images = images.to(device=device, dtype=dtype)\n    scores = best_model(images)\n    _, preds = scores.max(dim=1)\n    \n    files.extend(img_names)\n    predictions.extend(preds.squeeze(0).detach().cpu().numpy())","5e1b2ed7":"print(files)\nprint(predictions)","6c938966":"df = pd.DataFrame(columns=submission.columns)\ndf['image'] = files\ndf['label'] = predictions\ndf.head(30)","b3ec40f4":"df.to_csv(\".\/submission0428.csv\", index=False)","734a1781":"*   learning rate scheduler : \ud559\uc2b5 \uc18d\ub3c4\ub97c \uc904\uc5ec \ud559\uc2b5 \uc815\ub3c4\ub97c \uc870\uc815\ud560 \uc218 \uc788\ub2e4. \ucf54\ub4dc\uc5d0\uc11c \uc0ac\uc6a9\ud55c StepLR \uc740 \uc9c0\uc815\ud55c \uc5d0\ud3ed \uc218(step_size) \ub9c8\ub2e4 gamma \ub9cc\ud07c learning rate \ub97c \uac10\uc18c\uc2dc\ud0a8\ub2e4. \n*   Ealry stopping : \ub354 \uc774\uc0c1\uc758 \ud559\uc2b5\uc744 \ud1b5\ud55c \uc131\ub2a5 \ud5a5\uc0c1\uc774 \uc5c6\ub294 \uacbd\uc6b0 \ud559\uc2b5\uc744 \uba48\ucd94\uac8c \ud55c\ub2e4. \uc131\ub2a5 \ud5a5\uc0c1\uc740 validation set \uc5d0\uc11c\uc758 loss \ub85c \ud655\uc778\ud55c\ub2e4. \n\n","d74dae79":"## \ub370\uc774\ud130\uc14b \ubd84\ud560","ba562291":"## \ud559\uc2b5","25fedf44":"### (3) Setting","1eb4709f":"# \uac04\ub2e8\ud55c CNN model \ud559\uc2b5\n## \uc219\uba85\uc5ec\uc790\ub300\ud559\uad50 \ud1b5\uacc4\ud559\uacfc \ub178\ud61c\ub9bc\n(\ubcf8 \ucf54\ub4dc\ub294 \uc81c\ucd9c\uc744 \uc704\ud55c \ucd5c\uc18c\ud55c\uc758 baseline\uc785\ub2c8\ub2e4. \uad00\ub828 \uc9c8\ubb38\uc740 TA\uc2dc\uac04\uc744 \ud65c\uc6a9\ud558\uc2dc\uae30 \ubc14\ub78d\ub2c8\ub2e4.)","35921b70":"### (2) Google Drive Colab \uc5f0\uacb0","3552d5ed":"## \ucee4\uc2a4\ud140 \ub370\uc774\ud130 \uc815\uc758","72d4e9cf":"## 1. EDA","d5cd0a41":"## \uc774\ubbf8\uc9c0 \uc5b4\uadf8\uba58\ud14c\uc774\uc158\n( \ucc38\uace0 : https:\/\/github.com\/albumentations-team\/albumentations )","e3d72648":"## \ucd94\ub860","bad38df5":"### (2) \uaddc\uce59 \uc18c\uac1c\n * \ud3c9\uac00 : classification accuracy\n * \ud559\uc2b5\uc2dc \uc678\ubd80 \ub370\uc774\ud130 \ubc0f \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130 \uc0ac\uc6a9\uc744 \uae08\uc9c0\ud569\ub2c8\ub2e4. (ImageNet \ub4f1\uc758 pretrained weight\ub3c4 \uc0ac\uc6a9 \ubd88\uac00)\n * \ucef4\ud53c\ud2f0\uc158 \uc885\ub8cc \ud6c4 \ucf54\ub4dc \uacf5\uc720\uac00 \uc774\ub8e8\uc5b4\uc9c0\uba70, \uacf5\uc720\ub41c \ucf54\ub4dc\ub294 \ucd5c\uc885 \uacb0\uacfc\ub97c \ubcf5\uc6d0\ud560 \uc218 \uc788\uc5b4\uc57c \ud569\ub2c8\ub2e4.\n * \ud558\ub8e8 \uc81c\ucd9c \uac00\ub2a5 \ud69f\uc218\ub294 3\ud68c\uc774\uba70, \ucd5c\uc885 \uc81c\ucd9c \ud30c\uc77c\uc744 \uc81c\ucd9c\ud588\ub358 \ud30c\uc77c \uc911 2\uac1c \uc911\uc5d0\uc11c \ubbf8\ub9ac \uc120\ud0dd\ud574\uc57c \ud569\ub2c8\ub2e4.\n * \ud0c0 \ud300\uacfc\uc758 \uc815\ubcf4 \uacf5\uc720\ub294 \uc2ac\ub799 \ub4f1\uc758 \uacf5\uac1c\uc801\uc778 \ucee4\ubba4\ub2c8\ud2f0\ub97c \uc774\uc6a9\ud558\uc2dc\uae30 \ubc14\ub78d\ub2c8\ub2e4.\n","58e17796":"## CNN \ubaa8\ub378","6830878d":" \ud30c\uc774\ud1a0\uce58\uc5d0\uc11c\ub294 **Dataset**\uacfc **DataLoader**\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. \ud30c\uc774\ud1a0\uce58\uc758 Dataset \uc744 DataLoader \uc5d0 \uc804\ub2ec\ud558\uc5ec \ubbf8\ub2c8 \ubc30\uce58 \ud559\uc2b5 (batch_size), \ub370\uc774\ud130 \uc154\ud50c(shuffle=TRUE\/FALSE), \ubcd1\ub82c \ucc98\ub9ac(num_workers, colab \uc5d0\uc11c\ub294 2\uae4c\uc9c0\ub9cc \uac00\ub2a5)\ub97c \uc218\ud589\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. Dataset\uc744 \uc0c1\uc18d\ubc1b\uc740 CustomDataset \uc744 \uc815\uc758\ud558\uace0, \uc774\ub97c DataLoader\uc5d0 \uc804\ub2ec\ud558\uc5ec \uc0ac\uc6a9\ud55c\ub2e4.\n\n```\nclass CustomDataset(torch.utils.data.Dataset): \n  def __init__(self):\n\n  def __len__(self):\n\n  def __getitem__(self, idx): \n```\n\n * \\_\\_len\\_\\_ : \ub370\uc774\ud130\uc14b\uc758 \uae38\uc774 \uc989 n \ub9ac\ud134\n * \\_\\_getitem\\_\\_ : idx \ubc88\uc9f8 \ub370\uc774\ud130 \ub9ac\ud134\n\n(\ucc38\uace0 : https:\/\/wikidocs.net\/57165 )","e05a33dd":"### (1) Import packages","85f20bce":"## 0. Introduction","849e6a4c":"## 1. Preparation","7b4a67d2":"### (1) \ub370\uc774\ud130\uc14b \uc18c\uac1c\n\n\ub525\ud398\uc774\ud06c(deepfake, \ub525 \ub7ec\ub2dd(deep learning)\uacfc \uac00\uc9dc(fake)\uc758 \ud63c\uc131\uc5b4) : \uc778\uacf5 \uc9c0\ub2a5\uc744 \uae30\ubc18\uc73c\ub85c \ud55c \uc778\uac04 \uc774\ubbf8\uc9c0 \ud569\uc131 \uae30\uc220 \\\\\n\ubaa9\uc801 : \ubcc0\uc870\ub41c \uc774\ubbf8\uc9c0\ub97c \ud0d0\uc9c0 \ubc0f \uac80\ucd9c\ud558\ub294 AI \uae30\uc220 \uac1c\ubc1c\n\n( \ucd9c\ucc98 )\n * Thread for real faces dataset: https:\/\/www.kaggle.com\/c\/deepfake-detection-challenge\/discussion\/122786 \n * 1 Million Fake faces: https:\/\/www.kaggle.com\/c\/deepfake-detection-challenge\/discussion\/121173\n","4e6c7876":"## \ucd94\uac00 \uc2dc\ub3c4\n * \uc774\ubbf8\uc9c0 \uc5b4\uadf8\uba58\ud14c\uc774\uc158\n * \ubaa8\ub378 \uc218\uc815\n * Optimizer \ud639\uc740 \ud30c\ub77c\ubbf8\ud130 \ubcc0\uacbd\n * 5-CV \ud639\uc740 \uc559\uc0c1\ube14 \ubaa8\ub378"}}