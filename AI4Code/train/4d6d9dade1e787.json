{"cell_type":{"55488089":"code","4fee0c73":"code","ba3e27ff":"code","56dddfeb":"code","3944ba0a":"code","a0dff5bf":"code","c2bc36dc":"code","dfaea96a":"code","2801224e":"code","8ffa8ce7":"code","490d460f":"code","6c92bf73":"code","f333c726":"code","d484ab9c":"code","155b472c":"code","9bb736cb":"code","d5cf1ad0":"code","1cbf6040":"code","dab72bf6":"code","327bc214":"code","e298e758":"code","6a15576b":"code","1e5daa0a":"code","2ba4aa7d":"code","4a846489":"code","5fd1eee0":"code","2d575fee":"code","3356bd9c":"code","969f709d":"code","4de5a8ee":"code","81cc1eba":"code","d8ea3932":"code","b3f81499":"code","91cdae51":"code","a8e628b6":"code","117a7273":"code","eac74131":"code","49ca2d77":"code","f384c8b4":"code","9f300f02":"code","03e8759a":"code","d9e4781e":"code","d5f9df49":"code","92569267":"code","c05f928f":"code","883b6a8d":"code","caf2cf54":"code","92b5043f":"markdown","1929ac04":"markdown","7a3fe2d2":"markdown","b4d301e1":"markdown","05d6f367":"markdown","a89bd83d":"markdown","ee0c0995":"markdown","2725673e":"markdown","50183148":"markdown","f1de5863":"markdown","bd1b808e":"markdown","f6d197e2":"markdown","92ed099a":"markdown","61d8c768":"markdown","549249a4":"markdown","f6d92e73":"markdown","be521242":"markdown","e9596d53":"markdown","8d7ebb20":"markdown","cf7d4d7b":"markdown","6634850a":"markdown","4894a3bd":"markdown","d5ebcf87":"markdown","ce48dbe0":"markdown","ca9f0789":"markdown","f03f36d5":"markdown","1c0dbb51":"markdown","5a0c3bca":"markdown"},"source":{"55488089":"# !pip install pandas_profiling","4fee0c73":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.pyplot import figure\nfrom sklearn.utils import shuffle\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nimport time\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ba3e27ff":"df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf2 = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\npd.options.display.max_columns = None\ndf.head()","56dddfeb":"import pandas_profiling\n\ndf.profile_report()","3944ba0a":"df2.profile_report()","a0dff5bf":"df3 = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\ndf3.head()","c2bc36dc":"percent_missing = df.isnull().sum() * 100 \/ len(df)\nmissing_values = pd.DataFrame({'percent_missing': percent_missing})\nmissing_values.sort_values(by ='percent_missing' , ascending=False)","dfaea96a":"percent_missing = df2.isnull().sum() * 100 \/ len(df2)\nmissing_values = pd.DataFrame({'percent_missing': percent_missing})\nmissing_values.sort_values(by ='percent_missing' , ascending=False)","2801224e":"sns.set(style=\"ticks\")\nf = sns.countplot(x=\"Survived\", data=df, palette=\"bwr\")\nplt.show()","8ffa8ce7":"df['Survived'].value_counts()","490d460f":"df = df.fillna(df.mode().iloc[0])\ndf2 = df2.fillna(df2.mode().iloc[0])\n\npercent_missing = df.isnull().sum() * 100 \/ len(df)\nmissing_values = pd.DataFrame({'percent_missing': percent_missing})\nmissing_values.sort_values(by ='percent_missing' , ascending=False)","6c92bf73":"X = df.drop(['Cabin', 'Name', 'PassengerId', 'Survived', 'Ticket'], axis = 1)\nY = df['Survived']\nTest_Data = df2.drop(['Cabin', 'Name', 'PassengerId', 'Ticket'], axis = 1)","f333c726":"X = pd.get_dummies(X, prefix_sep='_')\nTest_Data = pd.get_dummies(Test_Data, prefix_sep='_')\nX.head()","d484ab9c":"Test_Data['Embarked_0'] = 0\n# 'Embarked_0',\nTest_Data = Test_Data[['Pclass', 'Age','SibSp','Parch','Fare','Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S']]\nTest_Data.head()","155b472c":"Y = LabelEncoder().fit_transform(Y)\n#np.set_printoptions(threshold=np.inf)\nY","9bb736cb":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\n\nX2 = StandardScaler().fit_transform(X)\nTest_Data = StandardScaler().fit_transform(Test_Data)\n\nX_Train, X_Test, Y_Train, Y_Test = train_test_split(X2, Y, test_size = 0.20, random_state = 101)","d5cf1ad0":"start = time.process_time()\ntrainedmodel = LogisticRegression().fit(X_Train,Y_Train)\nprint(time.process_time() - start)\npredictions =trainedmodel.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictions))\nprint(classification_report(Y_Test,predictions))","1cbf6040":"start = time.process_time()\ntrainedsvm = svm.LinearSVC().fit(X_Train, Y_Train)\nprint(time.process_time() - start)\npredictionsvm = trainedsvm.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictionsvm))\nprint(classification_report(Y_Test,predictionsvm))","dab72bf6":"start = time.process_time()\ntrainedtree = tree.DecisionTreeClassifier().fit(X_Train, Y_Train)\nprint(time.process_time() - start)\npredictionstree = trainedtree.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictionstree))\nprint(classification_report(Y_Test,predictionstree))","327bc214":"start = time.process_time()\ntrainedforest = RandomForestClassifier(n_estimators=700).fit(X_Train,Y_Train)\nprint(time.process_time() - start)\npredictionforest = trainedforest.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictionforest))\nprint(classification_report(Y_Test,predictionforest))","e298e758":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=2,svd_solver='full')\nX_pca = pca.fit_transform(X)\n\nX_reduced, X_test_reduced, Y_reduced, Y_test_reduced = train_test_split(X_pca, Y, test_size = 0.30, random_state = 101)\n\nstart = time.process_time()\ntrainedforest = RandomForestClassifier(n_estimators=700).fit(X_reduced,Y_reduced)\nprint(time.process_time() - start)\npredictionforest = trainedforest.predict(X_test_reduced)\nprint(confusion_matrix(Y_test_reduced,predictionforest))\nprint(classification_report(Y_test_reduced,predictionforest))","6a15576b":"from sklearn.ensemble import VotingClassifier\n\nmodel1 = svm.LinearSVC()\nmodel2 = tree.DecisionTreeClassifier()\nmodel3 = RandomForestClassifier()\nmodel = VotingClassifier(estimators=[('svm', model1), ('dt', model2), ('rf', model3)], voting='hard')\nmodel.fit(X_Train,Y_Train)\nmodel.score(X_Test,Y_Test)","1e5daa0a":"model1 = LogisticRegression()\nmodel2 = tree.DecisionTreeClassifier()\nmodel3 = RandomForestClassifier()\n\nmodel1.fit(X_Train,Y_Train)\nmodel2.fit(X_Train,Y_Train)\nmodel3.fit(X_Train,Y_Train)\n\npred1 = model1.predict_proba(X_Test)[:,1]\npred2 = model2.predict_proba(X_Test)[:,1]\npred3 = model3.predict_proba(X_Test)[:,1]\n\nfinalpred = (pred1+pred2+pred3)\/3\n\npreds = np.where(finalpred > 0.5, 1, 0)\n\nprint(confusion_matrix(Y_Test, preds))\nprint(classification_report(Y_Test, preds))","2ba4aa7d":"model1 = LogisticRegression()\nmodel2 = tree.DecisionTreeClassifier()\nmodel3 = RandomForestClassifier()\n\nmodel1.fit(X_Train,Y_Train)\nmodel2.fit(X_Train,Y_Train)\nmodel3.fit(X_Train,Y_Train)\n\npred1 = model1.predict_proba(X_Test)[:,1]\npred2 = model2.predict_proba(X_Test)[:,1]\npred3 = model3.predict_proba(X_Test)[:,1]\n\nfinalpred=(pred1*0.2+pred2*0.4+pred3*0.4)\n\npreds = np.where(finalpred > 0.5, 1, 0)\n\nprint(confusion_matrix(Y_Test, preds))\nprint(classification_report(Y_Test, preds))","4a846489":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom vecstack import stacking\nfrom sklearn.metrics import accuracy_score","5fd1eee0":"from sklearn.model_selection import RandomizedSearchCV\n\nrandom_search = {'criterion': ['entropy', 'gini'],\n               'max_depth': list(np.linspace(10, 1200, 10, dtype = int)) + [None],\n               'max_features': ['auto', 'sqrt','log2', None],\n               'min_samples_leaf': [4, 6, 8, 12],\n               'min_samples_split': [5, 7, 10, 14],\n               'n_estimators': list(np.linspace(151, 1200, 10, dtype = int))}\n\nclf = RandomForestClassifier()\nmodel = RandomizedSearchCV(estimator = clf, param_distributions = random_search, n_iter = 80, \n                               cv = 4, verbose= 5, random_state= 101, n_jobs = -1)\nmodel.fit(X,Y)\n\nmodel.best_params_","2d575fee":"random_search = {'criterion': ['entropy', 'gini'],\n               'max_depth': list(np.linspace(10, 1200, 10, dtype = int)),\n               'max_features': ['auto', 'sqrt','log2', None],\n               'min_samples_leaf': [4, 6, 8, 12],\n               'min_samples_split': [5, 7, 10, 14],\n               'n_estimators': list(np.linspace(151, 1200, 10, dtype = int))}\n\nclf = XGBClassifier()\nmodel = RandomizedSearchCV(estimator = clf, param_distributions = random_search, n_iter = 80, \n                               cv = 4, verbose= 5, random_state= 101, n_jobs = -1)\nmodel.fit(X,Y)\n\nmodel.best_params_","3356bd9c":"random_search = {'dual': [False],\n                'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n               'max_iter': list(np.linspace(10, 50000, 100, dtype = int))}\n\nclf = LogisticRegression()\nmodel = RandomizedSearchCV(estimator = clf, param_distributions = random_search, n_iter = 80, \n                               cv = 4, verbose= 5, random_state= 101, n_jobs = -1)\nmodel.fit(X,Y)\n\nmodel.best_params_","969f709d":"models = [\n    LogisticRegression(solver= 'sag', max_iter = 15024, dual= False),\n        \n    RandomForestClassifier(n_estimators = 1200, min_samples_split = 5, min_samples_leaf = 4, max_features = None, max_depth = 1200, criterion = 'entropy'),\n        \n    XGBClassifier(n_estimators = 151, min_samples_split = 10, min_samples_leaf = 6, \n                  max_features = 'auto', max_depth = 10, criterion = 'gini')\n]","4de5a8ee":"S_train, S_test = stacking(models,                   \n                           X_Train, Y_Train, Test_Data,   \n                           regression=False, \n     \n                           mode='oof_pred_bag', \n       \n                           needs_proba=False,\n         \n                           save_dir=None, \n            \n                           metric=accuracy_score, \n    \n                           n_folds= 4, \n                 \n                           stratified=True,\n            \n                           shuffle=True,  \n            \n                           random_state=0,    \n         \n                           verbose=2)","81cc1eba":"model = XGBClassifier(n_estimators = 151, min_samples_split = 10, min_samples_leaf = 6, \n                  max_features = 'auto', max_depth = 10, criterion = 'gini')\n    \nmodel = model.fit(S_train, Y_Train)\nY_Pred = model.predict(S_test)\n#print('Final prediction score: [%.3f]' % accuracy_score(Y_Test, Y_Pred))","d8ea3932":"from sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlxtend.classifier import StackingClassifier\nimport numpy as np\nimport warnings\n\nwarnings.simplefilter('ignore')\n\nclf1 = KNeighborsClassifier(n_neighbors=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\nlr = LogisticRegression()\nsclf = StackingClassifier(classifiers=[clf1, clf2, clf3], \n                          use_probas=True,\n                          average_probas=False,\n                          meta_classifier=lr)\n\nprint('3-fold cross validation:\\n')\n\nfor clf, label in zip([clf1, clf2, clf3, sclf], \n                      ['KNN', \n                       'Random Forest', \n                       'Naive Bayes',\n                       'StackingClassifier']):\n\n    scores = model_selection.cross_val_score(clf, X, Y, \n                                              cv=3, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+\/- %0.2f) [%s]\" \n          % (scores.mean(), scores.std(), label))","b3f81499":"sclf.fit(X_Train, Y_Train)\nY_Pred = sclf.predict(X_Test)\nsclf.score(X_Test,Y_Test)","91cdae51":"import matplotlib.pyplot as plt\nfrom mlxtend.plotting import plot_decision_regions\nimport matplotlib.gridspec as gridspec\nimport itertools\n\ngs = gridspec.GridSpec(2, 2)\n\nfig = plt.figure(figsize=(10,8))\n\nfor clf, lab, grd in zip([clf1, clf2, clf3, sclf], \n                         ['KNN', \n                          'Random Forest', \n                          'Naive Bayes',\n                          'StackingClassifier'],\n                          itertools.product([0, 1], repeat=2)):\n\n    clf.fit(X_pca, Y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X_pca, y=Y, clf=clf)\n    print(clf.score(X_pca,Y))\n    plt.title(lab)","a8e628b6":"# stack_pred = sclf.predict(Test_Data)","117a7273":"X_Train2, X_Val, Y_Train2, Y_Val = train_test_split(X_Train, Y_Train, test_size = 0.30, random_state = 101)\n\nmodel1 = tree.DecisionTreeClassifier()\nmodel1.fit(X_Train2,Y_Train2)\nval_pred1=model1.predict(X_Val)\ntest_pred1=model1.predict(X_Test)\nval_pred1=pd.DataFrame(val_pred1)\ntest_pred1=pd.DataFrame(test_pred1)\n\nmodel2 = LogisticRegression()\nmodel2.fit(X_Train2,Y_Train2)\nval_pred2=model2.predict(X_Val)\ntest_pred2=model2.predict(X_Test)\nval_pred2=pd.DataFrame(val_pred2)\ntest_pred2=pd.DataFrame(test_pred2)\n\ndf_val=pd.concat([pd.DataFrame(X_Val), val_pred1,val_pred2],axis=1)\ndf_test=pd.concat([pd.DataFrame(X_Test), test_pred1,test_pred2],axis=1)\n\nmodel = LogisticRegression()\nmodel.fit(df_val,Y_Val)\nmodel.score(df_test,Y_Test)","eac74131":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn import tree\n\nmodel = BaggingClassifier(tree.DecisionTreeClassifier(random_state=1))\nmodel.fit(X_Train,Y_Train)\nmodel.score(X_Test,Y_Test)","49ca2d77":"from sklearn.ensemble import AdaBoostClassifier\nmodel = AdaBoostClassifier()\nmodel.fit(X_Train,Y_Train)\nmodel.score(X_Test,Y_Test)","f384c8b4":"from sklearn.ensemble import GradientBoostingClassifier\nmodel= GradientBoostingClassifier(learning_rate=0.01,random_state=1)\nmodel.fit(X_Train,Y_Train)\nmodel.score(X_Test,Y_Test)","9f300f02":"import xgboost as xgb\nmodel=xgb.XGBClassifier(random_state=1,learning_rate=0.01)\nmodel.fit(X_Train,Y_Train)\nmodel.score(X_Test,Y_Test)","03e8759a":"import lightgbm as lgb\ntrain_data=lgb.Dataset(X_Train,Y_Train)\n#define parameters\nparams = {'learning_rate':0.2}\nmodel= lgb.train(params, train_data, 100) \ny_pred=model.predict(X_Test)\n\npreds = np.where(y_pred > 0.5, 1, 0)\n    \nprint(confusion_matrix(Y_Test, preds))\nprint(classification_report(Y_Test, preds))","d9e4781e":"# from catboost import CatBoostClassifier\n\n# model=CatBoostClassifier(iterations=100)\n# #categorical_features_indices = np.where(df.dtypes != np.float)[0]\n# model.fit(X_Train,Y_Train,eval_set=(X_Train,Y_Train))\n# model.score(X_Test,Y_Test)","d5f9df49":"# predictionforest = model.predict(Test_Data).astype(int)\n# #predictionforest = np.where(predictionforest > 0.5, 1, 0)\n# predictionforest","92569267":"from sklearn.metrics import accuracy_score\nfrom keras.models import load_model\nfrom keras.utils import to_categorical\nfrom keras.utils import plot_model\nfrom keras.models import Model\nfrom keras.models import Sequential\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers.merge import concatenate\nfrom numpy import argmax\n\n# fit model on dataset\ndef fit_model(trainX, trainy):\n    # define model\n    model = Sequential()\n    model.add(Dense(14, input_dim= 10, activation='relu'))\n    model.add(Dense(50))\n    model.add(Dense(10, activation='relu'))\n    model.add(Dense(2, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    model.fit(trainX, trainy, epochs=500, verbose=0)\n    return model\n\n# define stacked model from multiple member input models\ndef define_stacked_model(members):\n    # update all layers in all models to not be trainable\n    for i in range(len(members)):\n        model = members[i]\n        for layer in model.layers:\n            # make not trainable\n            layer.trainable = False\n            # rename to avoid 'unique layer name' issue\n            layer.name = 'ensemble_' + str(i+1) + '_' + layer.name\n    # define multi-headed input\n    ensemble_visible = [model.input for model in members]\n    # concatenate merge output from each model\n    ensemble_outputs = [model.output for model in members]\n    merge = concatenate(ensemble_outputs)\n    hidden = Dense(10, activation='relu')(merge)\n    output = Dense(2, activation='softmax')(hidden)\n    model = Model(inputs=ensemble_visible, outputs=output)\n    # plot graph of ensemble\n    plot_model(model, show_shapes=True, to_file='model_graph.png')\n    # compile\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n# fit a stacked model\ndef fit_stacked_model(model, inputX, inputy):\n    # prepare input data\n    X = [inputX for _ in range(len(model.input))]\n    # fit model\n    model.fit(X, inputy, epochs=300, verbose=0)\n\n# make a prediction with a stacked model\ndef predict_stacked_model(model, inputX):\n    # prepare input data\n    X = [inputX for _ in range(len(model.input))]\n    # make prediction\n    return model.predict(X, verbose=0)\n\nBinY_Train = to_categorical(Y_Train)\n\nn_members = 5\nmembers = []\nfor i in range(n_members):\n    members.append(fit_model(X_Train, BinY_Train))\n\nprint('Created %d models' % len(members))\n# define ensemble model\nstacked_model = define_stacked_model(members)","c05f928f":"BinY_Test = to_categorical(Y_Test)\n# fit stacked model on test dataset\nfit_stacked_model(stacked_model, X_Test, BinY_Test)\n# make predictions and evaluate\nyhat = predict_stacked_model(stacked_model, X_Test)\nyhat = argmax(yhat, axis=1)\nacc = accuracy_score(Y_Test, yhat)\nprint('Stacked Test Accuracy: %.3f' % acc)","883b6a8d":"n_members = 7\nmembers = []\nfor i in range(n_members):\n    members.append(fit_model(X_Train, BinY_Train))\n\nprint('Created %d models' % len(members))\n# define ensemble model\nstacked_model = define_stacked_model(members)\n# fit stacked model on test dataset\nfit_stacked_model(stacked_model, X_Test, BinY_Test)\n# make predictions and evaluate\nyhat = predict_stacked_model(stacked_model, Test_Data)\nyhat = argmax(yhat, axis=1)","caf2cf54":"submission = pd.DataFrame({\n        \"PassengerId\": df2[\"PassengerId\"],\n        \"Survived\": yhat\n    })\nsubmission.to_csv('titanic.csv', index=False)\nsubmission.head()","92b5043f":"#### MaxVoting","1929ac04":"MaxVoting Submission: 0.74162%","7a3fe2d2":"Ensemble Learning Summary:\n- Basic Ensemble Techniques\n    1. Max Voting\n    2. Averaging\n    3. Weighted Average\n- Advanced Ensemble Techniques\n    1. Stacking\n    2. Blending\n    3. Bagging\n    4. Boosting\n- Algorithms based on Bagging and Boosting\n    1. Bagging meta-estimator\n    2. Random Forest\n    3. AdaBoost\n    4. GBM\n    5. XGB\n    6. Light GBM\n    7. CatBoost","b4d301e1":"## Ensemble Learning","05d6f367":"This notebook was created in order to learn and test some of the possible Ensemble Learning techniques. In order to realise this notebook, the following tutorials have been followed: \n- https:\/\/www.analyticsvidhya.com\/blog\/2018\/06\/comprehensive-guide-for-ensemble-models\/\n- https:\/\/towardsdatascience.com\/automate-stacking-in-python-fc3e7834772e\n- https:\/\/machinelearningmastery.com\/stacking-ensemble-for-deep-learning-neural-networks\/\n- http:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/StackingClassifier\/","a89bd83d":"#### Light GBM","ee0c0995":"## Machine Learning","2725673e":"There are three main techniques of Ensemble Learning:\n- Bagging (bootstrapping and aggregating) to decrease the model\u2019s variance\n- Boosting to decreasing the model\u2019s bias\n- Stacking to increasing the predictive force of the classifier","50183148":"#### Stacking","f1de5863":"#### XGBoost","bd1b808e":"#### Averaging","f6d197e2":"# Titanic Ensemble Learning","92ed099a":"Submission score: 0.75598%","61d8c768":"Submission score: 0.72727%","549249a4":"#### Catboost","f6d92e73":"#### AdaBoost","be521242":"#### Deep Learning Stacking","e9596d53":"#### Bagging Meta Estimator","8d7ebb20":"#### Weighted Average","cf7d4d7b":"#### Stacking (using Mlxtend)","6634850a":"Submission score: 0.74641%","4894a3bd":"Baseline submission using a Random Forest Classifier was: 74%","d5ebcf87":"#### Gradient Boosting (GBM)","ce48dbe0":"Submission score: 77%","ca9f0789":"## Preprocessing","f03f36d5":"### Algorithms based on Bagging and Boosting","1c0dbb51":"MaxVoting Submission: 0.71%","5a0c3bca":"#### Blending"}}