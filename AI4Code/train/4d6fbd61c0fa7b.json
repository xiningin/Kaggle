{"cell_type":{"cd586dcf":"code","bc20a331":"code","6f7fe60a":"code","a632c3cc":"code","7e580c2a":"code","9934d066":"code","cf688c1f":"code","f743aa48":"code","0a736ee1":"code","5220611d":"code","74e2adc2":"code","83b41804":"code","631998e4":"code","011c65dd":"code","5cdef776":"code","5a09963b":"code","8df620b9":"code","84fc650f":"code","33fb52ac":"code","c71623da":"code","286295b8":"code","b2926bef":"code","60b33b4e":"code","cbb662d4":"code","e6bcd06d":"code","4f7b7b8b":"code","c08600d9":"code","1a7596c8":"code","c0be54f0":"code","88225b62":"code","2d61108c":"code","e6b35e5b":"code","e22ac99f":"code","77483678":"code","eee41d1f":"code","3ea6a013":"code","8e11be15":"code","2ce17d0d":"code","75fe2f67":"code","f4dd356c":"code","8f04aa51":"code","75c9c550":"code","8b0e8c61":"code","6ead04a6":"code","7c71992b":"code","94d3cf0f":"code","2fcf4821":"code","7d91cd36":"code","f1bd14ae":"code","14e3ce4f":"code","22e010eb":"code","31b2801a":"code","5622d8c4":"code","6f773fa7":"code","aef287a1":"markdown","2d4fffb1":"markdown","899fab56":"markdown","6e2a0d98":"markdown","f80d714f":"markdown","10600f19":"markdown","6dccd547":"markdown","309f648e":"markdown","ca2f389f":"markdown"},"source":{"cd586dcf":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, scale\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn import neighbors\nfrom sklearn.svm import SVR\n\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\")","bc20a331":"df = pd.read_csv(\"..\/input\/hitters-baseball-data\/Hitters.csv\")\ndf = df.dropna()\n# Transforming categorical data to dummy data\ndms = pd.get_dummies(df[[\"League\", \"Division\", \"NewLeague\"]])\ny = df[\"Salary\"]\nX_ = df.drop([\"Salary\", \"League\", \"Division\", \"NewLeague\"], axis= 1).astype(\"float64\")\nX = pd.concat([X_, dms[[\"League_N\", \"Division_W\", \"NewLeague_N\"]]], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X,\ny,\ntest_size = 0.2,\nrandom_state =42)","6f7fe60a":"X_train.head()","a632c3cc":"knn_model = KNeighborsRegressor().fit(X_train, y_train)","7e580c2a":"# Automatically the model assign k value as 5\n# We will check the k value optimisation later\nknn_model.n_neighbors","9934d066":"knn_model.metric\n# dir(knn_model)","cf688c1f":"knn_model.predict(X_test)[0:5]","f743aa48":"# Prediction\ny_pred = knn_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","0a736ee1":"# Model Tuning\nRMSE = []\n\nfor k in range(0,10):\n    k = k+1\n    knn_model = KNeighborsRegressor(n_neighbors=k).fit(X_train, y_train)\n    y_pred = knn_model.predict(X_test)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    RMSE.append(rmse)\n    print(\"k=\", k, \"RMSE:\", rmse, \"\\n\")","5220611d":"# GridSearchCV\nknn_params = {\"n_neighbors\": np.arange(1,30,1)}\nknn = KNeighborsRegressor()\nknn_cv_model = GridSearchCV(knn, knn_params, cv=10).fit(X_train, y_train)","74e2adc2":"knn_cv_model.best_params_","83b41804":"# Tuned Final Model\nknn_tuned = KNeighborsRegressor(n_neighbors= knn_cv_model.best_params_[\"n_neighbors\"]).fit(X_train, y_train)\n# Prediction\ny_pred = knn_tuned.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","631998e4":"# Model and Prediction\n# We can change the svr kernel and we can try \"rbf\"\nsvr_model = SVR(\"linear\").fit(X_train, y_train)\nsvr_model","011c65dd":"svr_model.predict(X_train)[0:5]","5cdef776":"svr_model.predict(X_test)[0:5]","5a09963b":"svr_model.intercept_","8df620b9":"svr_model.coef_","84fc650f":"# Test\ny_pred = svr_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","33fb52ac":"# Model Tuning\nsvr_model = SVR(\"linear\")\nsvr_model","c71623da":"# Defining penalty coef\nsvr_params = {\"C\": [0.1, 0.5, 1, 3]}\nsvr_cv_model = GridSearchCV(svr_model, svr_params , cv=5, verbose= 2, n_jobs= -1).fit(X_train, y_train)\nsvr_cv_model.best_params_","286295b8":"svr_tuned = SVR(\"linear\", C = 0.1).fit(X_train, y_train)\ny_pred = svr_tuned.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","b2926bef":"df = pd.read_csv(\"..\/input\/hitters-baseball-data\/Hitters.csv\")\ndf = df.dropna()\n# Transforming categorical data to dummy data\ndms = pd.get_dummies(df[[\"League\", \"Division\", \"NewLeague\"]])\ny = df[\"Salary\"]\nX_ = df.drop([\"Salary\", \"League\", \"Division\", \"NewLeague\"], axis= 1).astype(\"float64\")\nX = pd.concat([X_, dms[[\"League_N\", \"Division_W\", \"NewLeague_N\"]]], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X,\ny,\ntest_size = 0.2,\nrandom_state =42)","60b33b4e":"# Model & Prediction\nX_train = pd.DataFrame(X_train[\"Hits\"])\nX_test = pd.DataFrame(X_test[\"Hits\"])","cbb662d4":"cart_model = DecisionTreeRegressor(max_leaf_nodes=3)\ncart_model.fit(X_train,y_train)","e6bcd06d":"X_grid = np.arange(min(np.array(X_train)), max(np.array(X_train)), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\n\nplt.scatter(X_train, y_train, color=\"red\")\nplt.plot(X_grid, cart_model.predict(X_grid), color = \"blue\")\n\nplt.title(\"CART Regression Tree\")\nplt.xlabel(\"Number of Hits\")\nplt.ylabel(\"Salary\")","4f7b7b8b":"# One independet value prediction with Hits variable\ny_pred = cart_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","c08600d9":"# Many Independent value\ndf = pd.read_csv(\"..\/input\/hitters-baseball-data\/Hitters.csv\")\ndf = df.dropna()\n# Transforming categorical data to dummy data\ndms = pd.get_dummies(df[[\"League\", \"Division\", \"NewLeague\"]])\ny = df[\"Salary\"]\nX_ = df.drop([\"Salary\", \"League\", \"Division\", \"NewLeague\"], axis= 1).astype(\"float64\")\nX = pd.concat([X_, dms[[\"League_N\", \"Division_W\", \"NewLeague_N\"]]], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X,\ny,\ntest_size = 0.2,\nrandom_state =42)\n\ncart_model_2 = DecisionTreeRegressor(max_leaf_nodes=3).fit(X_train, y_train)\ny_pred = cart_model_2.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","1a7596c8":"# ?cart_model","c0be54f0":"cart_params = {\"max_depth\": [2,3,4,5,10,20],\n                   \"min_samples_split\": [2,10,5,30,50,10]}\n\ncart_model = DecisionTreeRegressor()\ncart_cv_model = GridSearchCV(cart_model, cart_params, cv=10).fit(X_train, y_train)","88225b62":"cart_cv_model.best_params_","2d61108c":"# Final Model\ncart_model_tuned = DecisionTreeRegressor(max_depth=5, min_samples_split=50).fit(X_train, y_train)\ny_pred = cart_cv_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","e6b35e5b":"# Model & Prediction\n\ndf = pd.read_csv(\"..\/input\/hitters-baseball-data\/Hitters.csv\")\ndf = df.dropna()\n# Transforming categorical data to dummy data\ndms = pd.get_dummies(df[[\"League\", \"Division\", \"NewLeague\"]])\ny = df[\"Salary\"]\nX_ = df.drop([\"Salary\", \"League\", \"Division\", \"NewLeague\"], axis= 1).astype(\"float64\")\nX = pd.concat([X_, dms[[\"League_N\", \"Division_W\", \"NewLeague_N\"]]], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X,\ny,\ntest_size = 0.2,\nrandom_state =42)","e22ac99f":"# ?rf_model\nrf_model = RandomForestRegressor(random_state=42).fit(X_train, y_train)\ny_pred = rf_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","77483678":"# Model Tuning\n\nrf_params = {\"max_depth\": [5,10], \n                 \"max_features\": [5,10], \n                 \"n_estimators\": [200,500], \n                 \"min_samples_split\": [2,10,100]}\nrf_cv_model = GridSearchCV(rf_model, rf_params, cv =5, verbose=2, n_jobs=-1).fit(X_train, y_train)","eee41d1f":"rf_cv_model.best_params_","3ea6a013":"# Final Model\nrf_model = RandomForestRegressor(random_state=42, max_depth=8, max_features=2, min_samples_split=2, n_estimators=200)\nrf_tuned = rf_model.fit(X_train, y_train)\ny_pred = rf_tuned.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","8e11be15":"# Lets Check Hitters Dataset Feature Importance\nimportance = pd.DataFrame({\"Importance\": rf_tuned.feature_importances_*100},\n                            index = X_train.columns)\n\nimportance.sort_values(by = \"Importance\", axis= 0, ascending=True).plot(kind=\"barh\")\nplt.xlabel(\"Feature Importance\")\nplt.gca().legend = None","2ce17d0d":"# Model & Prediction\n\ndf = pd.read_csv(\"..\/input\/hitters-baseball-data\/Hitters.csv\")\ndf = df.dropna()\n# Transforming categorical data to dummy data\ndms = pd.get_dummies(df[[\"League\", \"Division\", \"NewLeague\"]])\ny = df[\"Salary\"]\nX_ = df.drop([\"Salary\", \"League\", \"Division\", \"NewLeague\"], axis= 1).astype(\"float64\")\nX = pd.concat([X_, dms[[\"League_N\", \"Division_W\", \"NewLeague_N\"]]], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X,\ny,\ntest_size = 0.2,\nrandom_state =42)","75fe2f67":"gbm_model = GradientBoostingRegressor().fit(X_train, y_train)\n# ?gbm_model for learning the parameters\ny_pred = gbm_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","f4dd356c":"# Model Tuning\ngbm_params = {\"learning_rate\": [0.001, 0.01], \n              \"max_depth\": [3,5,8], \n              \"n_estimators\": [100,200,500], \n              \"subsample\": [1,0.5,0.8], \n              \"loss\": [\"ls\", \"lad\", \"quantile\"] }\n\ngbm_cv_model = GridSearchCV(gbm_model, gbm_params, \n                            cv=5, verbose=2, \n                            n_jobs=-1).fit(X_train, y_train)","8f04aa51":"gbm_cv_model.best_params_","75c9c550":"gbm_tuned_model = GradientBoostingRegressor(learning_rate=0.1, loss= \"lad\", max_depth=5, n_estimators=200, subsample=1).fit(X_train, y_train)\ny_pred = gbm_tuned_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","8b0e8c61":"# Feature Importance\nimportance = pd.DataFrame({\"Importance\": gbm_tuned_model.feature_importances_*100},\n                            index = X_train.columns)\n\nimportance.sort_values(by = \"Importance\", axis= 0, ascending=True).plot(kind=\"barh\", color = \"orange\")\nplt.xlabel(\"Feature Importance\")\nplt.gca().legend = None","6ead04a6":"# Model & Prediction\n\ndf = pd.read_csv(\"..\/input\/hitters-baseball-data\/Hitters.csv\")\ndf = df.dropna()\n# Transforming categorical data to dummy data\ndms = pd.get_dummies(df[[\"League\", \"Division\", \"NewLeague\"]])\ny = df[\"Salary\"]\nX_ = df.drop([\"Salary\", \"League\", \"Division\", \"NewLeague\"], axis= 1).astype(\"float64\")\nX = pd.concat([X_, dms[[\"League_N\", \"Division_W\", \"NewLeague_N\"]]], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X,\ny,\ntest_size = 0.2,\nrandom_state =42)","7c71992b":"!pip install xgboost","94d3cf0f":"import xgboost\nfrom xgboost import XGBRegressor\n\nxgb = XGBRegressor().fit(X_train, y_train)\ny_pred = xgb.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","2fcf4821":"xgb","7d91cd36":"xgb_params = {\"learning_rate\": [0.1,0.01,0.5],\n\"max_depth\": [2,3,4,5,8],\n\"n_estimators\": [100,200,500,1000],\n\"colsample_bytree\": [0.4,0.7,1]}","f1bd14ae":"xgb_cv_model = GridSearchCV(xgb, xgb_params, verbose=2, cv=10, n_jobs=-1).fit(X_train, y_train)\nxgb_cv_model.best_params_","14e3ce4f":"# Model & Prediction\n\ndf = pd.read_csv(\"..\/input\/hitters-baseball-data\/Hitters.csv\")\ndf = df.dropna()\n# Transforming categorical data to dummy data\ndms = pd.get_dummies(df[[\"League\", \"Division\", \"NewLeague\"]])\ny = df[\"Salary\"]\nX_ = df.drop([\"Salary\", \"League\", \"Division\", \"NewLeague\"], axis= 1).astype(\"float64\")\nX = pd.concat([X_, dms[[\"League_N\", \"Division_W\", \"NewLeague_N\"]]], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X,\ny,\ntest_size = 0.2,\nrandom_state =42)","22e010eb":"!pip install lightgbm","31b2801a":"from lightgbm import LGBMRegressor\nlgb = LGBMRegressor().fit(X_train, y_train)\nlgb.get_params\ny_pred = lgb.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","5622d8c4":"# Model Tuning\nlgb_params = {\"learning_rate\": [0.1, 0.01, 0.5, 1],\n\"n_estimators\": [100,200,500],\n\"max_depth\": [2,3,5,10]}\n\nlgb_cv_model = GridSearchCV(lgb, lgb_params, \n                            cv=5, verbose=2, \n                            n_jobs= -1).fit(X_train, y_train)\nlgb_cv_model.best_params_","6f773fa7":"lgbm_tuned_model = LGBMRegressor(learning_rate=0.01, \n                                 max_depth=5, \n                                 n_estimators=500).fit(X_train, y_train)\n\ny_pred = lgbm_tuned_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","aef287a1":"# 2) Support Vector Regression\n\nThe objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N \u2014 the number of features) that distinctly classifies the data points.\n\nHyperplanes are decision boundaries that help classify the data points. Data points falling on either side of the hyperplane can be attributed to different classes. Also, the dimension of the hyperplane depends upon the number of features. If the number of input features is 2, then the hyperplane is just a line. If the number of input features is 3, then the hyperplane becomes a two-dimensional plane. It becomes difficult to imagine when the number of features exceeds 3.\n\nSupport vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier. Deleting the support vectors will change the position of the hyperplane. These are the points that help us build our SVM.\n\n\n![image.png](https:\/\/miro.medium.com\/max\/600\/0*0o8xIA4k3gXUDCFU.png)\n\n**Source:**\n\n[Towards Data Science](https:\/\/towardsdatascience.com\/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47)","2d4fffb1":"# 4) Random Forest Algorithm\n\nRandom forest, like its name implies, consists of a large number of individual decision trees that operate as an ensemble. __Each individual tree__ in the random forest spits out a class prediction and the class with the most votes becomes our model\u2019s prediction (see figure below).\n\n![Random Forest](https:\/\/static.javatpoint.com\/tutorial\/machine-learning\/images\/random-forest-algorithm2.png)\n\nThe fundamental concept behind random forest is a simple but powerful one \u2014 the wisdom of crowds. In data science speak, the reason that the random forest model works so well is:\n\n- A large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models.\n\nThe low correlation between models is the key. Just like how investments with low correlations (like stocks and bonds) come together to form a portfolio that is greater than the sum of its parts, uncorrelated models can produce ensemble predictions that are more accurate than any of the individual predictions. The reason for this wonderful effect is that the trees protect each other from their individual errors (as long as they don\u2019t constantly all err in the same direction). While some trees may be wrong, many other trees will be right, so as a group the trees are able to move in the correct direction. So the prerequisites for random forest to perform well are:\n\n- There needs to be some actual signal in our features so that models built using those features do better than random guessing.\n\n- The predictions (and therefore the errors) made by the individual trees need to have low correlations with each other.\n\n__What is Out-of-Bag Error ?__\n\nOut-of-bag (OOB) error, also called out-of-bag estimate, is a method of measuring the prediction error of random forests, boosted decision trees, and other machine learning models utilizing bootstrap aggregating (bagging). \n__Bagging uses subsampling with replacement to create training samples for the model to learn from__. OOB error is the mean prediction error on each training sample xi, using only the trees that did not have xi in their bootstrap sample.\n\n\n![OOB](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/7\/77\/OOB_Error_Example.png)\n\nBootstrap aggregating allows one to define an out-of-bag estimate of the prediction performance improvement by evaluating predictions on those observations that were not used in the building of the next base learner.\n\n**Source:**\n\n[Towards Data Science](https:\/\/towardsdatascience.com\/understanding-random-forest-58381e0602d2)\n\n[Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Out-of-bag_error)","899fab56":"## Gradient Boosting Machines\n\n__What is Gradient Boosting?__\n\nLet\u2019s start by understanding Boosting! __Boosting is a method of converting weak learners into strong learners.__ In boosting, each new tree is a fit on a modified version of the original data set. __The gradient boosting algorithm (gbm) can be most easily explained by first introducing the AdaBoost Algorithm.__\n\nThe AdaBoost Algorithm begins by training a decision tree in which each observation is assigned an equal weight. After evaluating the first tree, we increase the weights of those observations that are difficult to classify and lower the weights for those that are easy to classify. \n\nThe second tree is therefore grown on this weighted data. Here, the idea is to improve upon the predictions of the first tree. Our new model is therefore Tree 1 + Tree 2. We then compute the classification error from this new 2-tree ensemble model and grow a third tree to predict the revised residuals. We repeat this process for a specified number of iterations. Subsequent trees help us to classify observations that are not well classified by the previous trees. Predictions of the final ensemble model is therefore the weighted sum of the predictions made by the previous tree models.\n\n![GBM](http:\/\/explained.ai\/gradient-boosting\/images\/golf-MSE.png)\n\nGradient Boosting trains many models in a gradual, additive and sequential manner. __The major difference between AdaBoost and Gradient Boosting Algorithm is how the two algorithms identify the shortcomings of weak learners (eg. decision trees).__\n\n__While the AdaBoost model identifies the shortcomings by using high weight data points, gradient boosting performs the same by using gradients in the loss function__ (y=ax+b+e , e needs a special mention as it is the error term). \n\nThe loss function is a measure indicating how good are model\u2019s coefficients are at fitting the underlying data. A logical understanding of loss function would depend on what we are trying to optimise. For example, if we are trying to predict the sales prices by using a regression, then the loss function would be based off the error between true and predicted house prices. Similarly, if our goal is to classify credit defaults, then the loss function would be a measure of how good our predictive model is at classifying bad loans. One of the biggest motivations of using gradient boosting is that it allows one to optimise a user specified cost function, instead of a loss function that usually offers less control and does not essentially correspond with real world applications.\n\n![Gradient boosting](https:\/\/media.springernature.com\/original\/springer-static\/image\/chp%3A10.1007%2F978-3-030-34482-5_25\/MediaObjects\/482246_1_En_25_Fig2_HTML.png)\n\n\n**Source:**\n\n[Towards Data Science](https:\/\/towardsdatascience.com\/understanding-gradient-boosting-machines-9be756fe76ab)\n\n[Akira AI](https:\/\/www.akira.ai\/glossary\/gradient-boosting\/)","6e2a0d98":"# 1) K-Nearest Neighbors Algorithm\n\nA supervised machine learning algorithm (as opposed to an unsupervised machine learning algorithm) is one that relies on labeled input data to learn a function that produces an appropriate output when given new unlabeled data.\n\nKNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label (in the case of classification) or averages the labels (in the case of regression).\n\n\n![image.png](https:\/\/miro.medium.com\/max\/1222\/1*wW8O-0xVQUFhBGexx2B6hg.png)\n\n**Source:**\n\n[Towards Data Science](https:\/\/towardsdatascience.com\/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761#:~:text=KNN%20works%20by%20finding%20the,in%20the%20case%20of%20regression)","f80d714f":"## Feature Importance\n\nFeature importance refers to techniques that assign a score to input features based on __how useful they are at predicting a target variable.__\n\nThere are many types and sources of feature importance scores, although popular examples include statistical correlation scores, coefficients calculated as part of linear models, decision trees, and permutation importance scores.\n\n![Feature Importance](https:\/\/scikit-learn.org\/0.24\/_images\/sphx_glr_plot_permutation_importance_001.png)\n\nFeature importance scores play an important role in a predictive modeling project, including providing insight into the data, insight into the model, and the basis for dimensionality reduction and feature selection that can improve the efficiency and effectiveness of a predictive model on the problem.\n\nSource:\n\n[Machine Learning Mastery](https:\/\/machinelearningmastery.com\/calculate-feature-importance-with-python\/#:~:text=Feature%20importance%20refers%20to%20techniques,at%20predicting%20a%20target%20variable.&text=The%20role%20of%20feature%20importance%20in%20a%20predictive%20modeling%20problem.)","10600f19":"# Evolution of Tree-Based Algorithms and XGBoost Algorithm\n\nXGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. In prediction problems involving unstructured data (images, text, etc.) artificial neural networks tend to outperform all other algorithms or frameworks. However, when it comes to small-to-medium structured\/tabular data, decision tree based algorithms are considered best-in-class right now. \n\n![XGBoost](https:\/\/miro.medium.com\/proxy\/1*QJZ6W-Pck_W7RlIDwUIN9Q.jpeg)\n\nThe algorithm differentiates itself in the following ways:\n\n- A wide range of applications: Can be used to solve regression, classification, ranking, and user-defined prediction problems.\n\n- Portability: Runs smoothly on Windows, Linux, and OS X.\n\n- Languages: Supports all major programming languages including C++, Python, R, Java, Scala, and Julia.\n\n- Cloud Integration: Supports AWS, Azure, and Yarn clusters and works well with Flink, Spark, and other ecosystems.\n\nEach step of the evolution of tree-based algorithms can be viewed as below.\n\n- **Decision Tree:** Every hiring manager has a set of criteria such as education level, number of years of experience, interview performance. A decision tree is analogous to a hiring manager interviewing candidates based on his or her own criteria.\n\n- **Bagging:** Now imagine instead of a single interviewer, now there is an interview panel where each interviewer has a vote. Bagging or bootstrap aggregating involves combining inputs from all interviewers for the final decision through a democratic voting process.\n\n- **Random Forest:** It is a bagging-based algorithm with a key difference wherein only a subset of features is selected at random. In other words, every interviewer will only test the interviewee on certain randomly selected qualifications (e.g. a technical interview for testing programming skills and a behavioral interview for evaluating non-technical skills).\n\n- **Boosting:** This is an alternative approach where each interviewer alters the evaluation criteria based on feedback from the previous interviewer. This \u2018boosts\u2019 the efficiency of the interview process by deploying a more dynamic evaluation process.\n\n- **Gradient Boosting:** A special case of boosting where errors are minimized by gradient descent algorithm e.g. the strategy consulting firms leverage by using case interviews to weed out less qualified candidates.\n\n- **XGBoost:** Think of XGBoost as gradient boosting on \u2018steroids\u2019 (well it is called \u2018Extreme Gradient Boosting\u2019 for a reason!). It is a perfect combination of software and hardware optimization techniques to yield superior results using less computing resources in the shortest amount of time.\n\n**Why does XGBoost perform so well?**\n\n**System Optimization:**\n\n\n- **Parallelization:** XGBoost approaches the process of sequential tree building using parallelized implementation. This is possible due to the interchangeable nature of loops used for building base learners; the outer loop that enumerates the leaf nodes of a tree, and the second inner loop that calculates the features. This nesting of loops limits parallelization because without completing the inner loop (more computationally demanding of the two), the outer loop cannot be started. Therefore, to improve run time, the order of loops is interchanged using initialization through a global scan of all instances and sorting using parallel threads. This switch improves algorithmic performance by offsetting any parallelization overheads in computation.\n\n\n- **Tree Pruning:** The stopping criterion for tree splitting within GBM framework is greedy in nature and depends on the negative loss criterion at the point of split. XGBoost uses \u2018max_depth\u2019 parameter as specified instead of criterion first, and starts pruning trees backward. This \u2018depth-first\u2019 approach improves computational performance significantly.\n\n- **Hardware Optimization:** This algorithm has been designed to make efficient use of hardware resources. This is accomplished by cache awareness by allocating internal buffers in each thread to store gradient statistics. Further enhancements such as \u2018out-of-core\u2019 computing optimize available disk space while handling big data-frames that do not fit into memory.\n\n**Algorithmic Enhancements:**\n\n- **Regularization:** It penalizes more complex models through both LASSO (L1) and Ridge (L2) regularization to prevent overfitting.\n\n- **Sparsity Awareness:** XGBoost naturally admits sparse features for inputs by automatically \u2018learning\u2019 best missing value depending on training loss and handles different types of sparsity patterns in the data more efficiently.\n\n- **Weighted Quantile Sketch:** XGBoost employs the distributed weighted Quantile Sketch algorithm to effectively find the optimal split points among weighted datasets.\n\n- **Cross-validation:** The algorithm comes with built-in cross-validation method at each iteration, taking away the need to explicitly program this search and to specify the exact number of boosting iterations required in a single run.\n\n![XGBoost_2](https:\/\/miro.medium.com\/proxy\/1*FLshv-wVDfu-i54OqvZdHg.png)\n\n\n**Source:**\n\n[Towards Data Science](https:\/\/towardsdatascience.com\/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d#:~:text=XGBoost%20is%20a%20decision%2Dtree,all%20other%20algorithms%20or%20frameworks.)","6dccd547":"# Light GBM\n\nLight GBM is a gradient boosting framework that uses tree based learning algorithm.\n\n**How it differs from other tree based algorithm?**\n\nLight GBM grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithm grows level-wise. It will choose the leaf with max delta loss to grow. When growing the same leaf, Leaf-wise algorithm can reduce more loss than a level-wise algorithm.\n\nBelow diagrams explain the implementation of LightGBM and other boosting algorithms.\n\nLight GBM & Other Boosting Algorithms\n\n![Light GBM](https:\/\/www.yildirimmehmet.com\/wp-content\/uploads\/2021\/03\/13.png)\n\nThe size of data is increasing day by day and it is becoming difficult for traditional data science algorithms to give faster results. Light GBM is prefixed as \u2018Light\u2019 because of its high speed. Light GBM can handle the large size of data and takes lower memory to run. Another reason of why Light GBM is popular is because it focuses on accuracy of results. LGBM also supports GPU learning and thus data scientists are widely using LGBM for data science application development.\n\n\n**Can we use Light GBM everywhere?**\n\nNo, it is not advisable to use LGBM on small datasets. Light GBM is sensitive to overfitting and can easily overfit small data. Their is no threshold on the number of rows but my experience suggests me to use it only for data with 10,000+ rows\n\n**Source:**\n\n[Light GBM Documentation](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Installation-Guide.html#macos)\n\n[Medium](https:\/\/medium.com\/@pushkarmandot\/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc)","309f648e":"# 3) CART (Classification andd Regresson Tree)\n\nA decision tree is a largely used non-parametric effective machine learning modeling technique for regression and classification problems. To find solutions a decision tree makes a sequential, hierarchical decision about the outcomes variable based on the predictor data.\nThe decision tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a decision tree with nodes and leaf nodes.\nThe Understanding Level of Decision Tree algorithm is so easy as compared to the classification algorithm.\nIn the Decision tree algorithm, we solve our problem in a tree regression.\n\n- **Each internal node of the tree corresponds to an attribute.**\n- **Each leaf node corresponds to a Class Label.**\n\nIn the decision tree for predicting a class label for a record, we start from the root of the tree. We compare the value of the root attribute with the record\u2019s attribute on the basis of comparison. We follow the branch corresponding to that value & jump to the next node. We continue comparing our record\u2019s attribute value with other internal nodes of the tree until we reach a leaf node.\n\n![Decision Tree](https:\/\/ec.europa.eu\/eurostat\/statistics-explained\/images\/f\/fb\/Decision_tree.PNG)\n\n**Source:**\n\n[Medium](https:\/\/medium.com\/machine-learning-researcher\/decision-tree-algorithm-in-machine-learning-248fb7de819e)","ca2f389f":"# Machine Learning - Non-Linear Regression Models - Supervised Learning\n\n- K Nearest Neighbors\n\n- Support Vector \n\n- CART (Classification and Regression)\n\n- Random Forest\n\n- Gradient Boosting Machines\n\n- XGBoost\n\n- Light GBM\n\n- CatBoost (Categorical Boosting)"}}