{"cell_type":{"f1911991":"code","c6644cbc":"code","238d0ee1":"code","7e0a5785":"code","4adb0148":"code","45203fa0":"code","a81f915d":"code","2ebf038c":"code","06d9dd16":"code","56851a4c":"code","58b4494d":"code","17607f43":"code","073391f5":"markdown","b507f01f":"markdown","d10e41ee":"markdown","75f66135":"markdown","d8c6fef3":"markdown"},"source":{"f1911991":"import pip._internal as pip\npip.main(['install', '--upgrade', 'numpy==1.17.2'])\nimport numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import cross_val_predict, cross_val_score, GridSearchCV\nfrom sklearn.utils.multiclass import unique_labels\n\nimport time\nimport pickle\n\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm_notebook\n\nfrom lwoku import get_accuracy, add_features","c6644cbc":"# Read training and test files\nX_train = pd.read_csv('..\/input\/learn-together\/train.csv', index_col='Id', engine='python')\nX_test = pd.read_csv('..\/input\/learn-together\/test.csv', index_col='Id', engine='python')\n\n# Define the dependent variable\ny_train = X_train['Cover_Type'].copy()\n\n# Define a training set\nX_train = X_train.drop(['Cover_Type'], axis='columns')","238d0ee1":"# Add new features\nprint('     Number of features (before adding): {}'.format(len(X_train.columns)))\nX_train = add_features(X_train)\nX_test = add_features(X_test)\nprint('     Number of features (after adding): {}'.format(len(X_train.columns)))","7e0a5785":"print('  -- Drop features')\nprint('    -- Drop columns with few values (or almost)')\nfeatures_to_drop = ['Soil_Type7', 'Soil_Type8', 'Soil_Type15', 'Soil_Type27', 'Soil_Type37']\nX_train.drop(features_to_drop, axis='columns', inplace=True)\nX_test.drop(features_to_drop, axis='columns', inplace=True)\nprint('    -- Drop columns with low importance')\nfeatures_to_drop = ['Soil_Type5', 'Slope', 'Soil_Type39', 'Soil_Type18', 'Soil_Type20', 'Soil_Type35', 'Soil_Type11',\n                    'Soil_Type31']\nX_train.drop(features_to_drop, axis='columns', inplace=True)\nX_test.drop(features_to_drop, axis='columns', inplace=True)\nprint('     Number of features (after drop): {}'.format(len(X_train.columns)))","4adb0148":"with open('..\/input\/tactic-03-hyperparameter-optimization-lightgbm\/clf.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nlg_clf = clf.best_estimator_\nlg_clf","45203fa0":"from lwoku import get_accuracy#, add_features\naccuracy_all_features = get_accuracy(lg_clf, X_train, y_train)\nprint('Accuracy: {:.4f} %'.format(accuracy_all_features * 100))","a81f915d":"model_fitted = lg_clf.fit(X_train, y_train)\n    \nimportances = pd.DataFrame({'Features': X_train.columns, \n                            'Importances': model_fitted.feature_importances_})\n    \nimportances.sort_values(by=['Importances'], axis='index', ascending=False, inplace=True)\n\nfig = plt.figure(figsize=(18, 6))\nplt.bar(importances['Features'], importances['Importances'])\nplt.xticks(rotation='vertical')\nplt.show()","2ebf038c":"results = pd.DataFrame()\nfeatures = []\nfeatures_positive = []\nfeatures_negative = []\naccuracy_last = 0\n\nfor feature in tqdm_notebook(importances['Features']):\n    features.append(feature)\n    accuracy = get_accuracy(lg_clf, X_train[features], y_train)\n    if accuracy > accuracy_last:\n        features_positive.append(feature)\n    else:\n        features_negative.append(feature)\n    accuracy_last = accuracy\n    print('{}: {:.4f} %'.format(feature, accuracy * 100))\n    results = results.append({\n        'Accuracy': accuracy,\n        'Feature': feature,        \n        'Features': features.copy(),\n        'Features positive': features_positive.copy()\n    }, ignore_index = True)\n\nresults.to_csv('results.csv', index = True)\nresults","06d9dd16":"iteration = results['Accuracy'].idxmax()\nfeatures_largest_accuracy = results['Features'][iteration]\nprint('Best result in iteration #{}'.format(iteration))\nprint('{} features: {}'.format(len(features_largest_accuracy), features_largest_accuracy))\nprint('{} features positive: {}'.format(len(features_positive), features_positive))\nprint('{} features negative: {}'.format(len(features_negative), features_negative))","56851a4c":"print('N# features in largest accuracy: {} and in positive: {}'.format(len(features_largest_accuracy), len(features_positive)))\n\nprint(list(set(features_largest_accuracy) - set(features_positive)))\n\nprint(list(set(features_positive) - set(features_largest_accuracy)))","58b4494d":"def submit(features, name):\n    \n    print(name)\n    accuracy = get_accuracy(lg_clf, X_train[features], y_train)\n    print('Accuracy: {:.2f} %'.format(accuracy * 100))\n    print('Improvement: {:.2f} %\\n'.format((accuracy - accuracy_all_features)* 100))\n\n    # Predict\n    lg_clf.fit(X_train[features], y_train)\n    y_test_pred = pd.Series(lg_clf.predict(X_test[features]))\n\n    # Submit\n    output = pd.DataFrame({'ID': X_test.index,\n                           'Cover_Type': y_test_pred})\n\n    output.to_csv('submission_' + name + '.csv', index=False)","17607f43":"submit(features_largest_accuracy, 'largest_accuracy')\n\n# submit(results['Features'][iteration - 1], 'largest_accuracy_m1')\n\n# submit(results['Features'][iteration + 1], 'largest_accuracy_p1')\n\nsubmit(features_positive, 'positive')\n\n# submit(features_positive[:-1], 'positive_m1')\n\n# submit(features_positive + [features_negative[0]], 'positive_p1')","073391f5":"# Introduction\n\nThe aim of this notebook is to select the features that maximizes the score,\nfollowing its importance.\n\nThe models are fitted and predicted with the optimized parameters and new features of the notebook ([Tactic 06. Feature engineering](https:\/\/www.kaggle.com\/juanmah\/tactic-06-feature-engineering\/)).\n\nThe results are collected at [Tactic 99. Summary](https:\/\/www.kaggle.com\/juanmah\/tactic-99-summary).","b507f01f":"# LightGBM\n\nExamine the full hyperparameter optimization details for this model in the following notebook: [Tactic 03. Hyperparameter optimization. LightGBM](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-lightgbm)","d10e41ee":"# Prepare data","75f66135":"## Submit","d8c6fef3":"## Differences in selected features"}}