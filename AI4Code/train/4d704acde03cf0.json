{"cell_type":{"c9b9a4f0":"code","fdea70f0":"code","9e65c203":"code","3707869c":"code","e0c2f4f4":"code","32304adb":"code","908300d3":"code","f7fac331":"code","50a8d24f":"code","9a88a634":"code","d8a8fb73":"code","8bc85f54":"code","d7b3322c":"code","56d0bbe6":"code","86a9a1ab":"code","226163f2":"code","e3cb568b":"code","84e73ebe":"code","53c777d1":"code","73fbdbfd":"code","601d575a":"code","c40189b5":"code","45cfa83c":"code","cabc2511":"code","851af010":"code","81b8491e":"code","41f7abd3":"code","1af994a3":"code","fcc21a22":"code","a0e937fa":"code","6c87428f":"code","eefb8e8f":"code","d7273596":"code","6d674ff2":"code","8890c3c1":"code","22811cf9":"code","1236b0c0":"code","2d82efaf":"code","b516bd26":"code","8859fd23":"code","57a9358c":"code","85b83128":"code","e689bca5":"code","f3a9758f":"code","54413c83":"code","4fc67909":"code","c7336374":"code","3ce214ab":"code","2e73f8d0":"code","0aa798ef":"code","7ed9ff34":"code","abecfab6":"code","60bf2442":"code","98b91ff9":"code","183ff4a8":"code","6b3788b9":"code","a2859687":"code","1d361f89":"code","d9ff5216":"code","5850ac08":"code","0acca06d":"code","6b82c8c8":"code","db75a532":"code","8246543f":"code","93067f7e":"code","81e5b60c":"code","20150c76":"code","66e403ba":"code","0e170d87":"code","81bb7c3a":"code","65c0d2f2":"code","f43700e9":"code","8c841bcf":"code","8634c787":"code","75e9d6ee":"code","77dfbea5":"code","248e121e":"code","9fa8cf40":"code","993ada60":"code","849b12d0":"code","0589f999":"code","9a16236a":"code","19942eda":"code","4a4b2ba6":"code","23f63320":"code","9ec76cd8":"code","401c3824":"code","6f730197":"code","34ccac9f":"code","db72dc4b":"code","61d7e66d":"code","111da32a":"code","b11f6bc3":"code","12e752c1":"code","f6ddffc9":"code","f4fb8f2e":"code","e1120ec1":"code","e58d72f6":"code","b25e19e4":"code","c35b2f63":"code","7b39d7d7":"code","437433ae":"code","f078d1ce":"code","3f0155cb":"code","80fbed8c":"code","b6199d3e":"code","fd5e1f07":"code","53470df3":"code","8adc505e":"code","c2333bef":"code","06c8e722":"code","b38a3a92":"code","44befce2":"code","0c472e5e":"markdown","6419b62d":"markdown","2aec4258":"markdown","9deaaf42":"markdown","95767a91":"markdown","996509f0":"markdown","6d9b92a6":"markdown","71c619da":"markdown","4914c270":"markdown","688808fe":"markdown","9793d931":"markdown","ee119c42":"markdown","7481a1cf":"markdown","67962c52":"markdown","85a2ef6c":"markdown","3a2aef5f":"markdown","1836f072":"markdown","384073aa":"markdown","a8f0791b":"markdown","a6da00f3":"markdown","041be689":"markdown","9aa62767":"markdown","dcb4ab29":"markdown","e1fdaa8b":"markdown","d7654d91":"markdown","edec4abe":"markdown","76016c48":"markdown","f3d3d4dd":"markdown","19fba22f":"markdown","9c1e8f83":"markdown","b3c5543c":"markdown","6463ab1d":"markdown","43b62ac1":"markdown","f5d36ead":"markdown","324613e9":"markdown","f2ae7982":"markdown","bbfbedb8":"markdown","0073ab92":"markdown","bb809a99":"markdown","a93e2ea0":"markdown","ad022599":"markdown","5b7ed873":"markdown","51d03237":"markdown","d0feaffb":"markdown","89a1095a":"markdown","cc7c8ea5":"markdown","ea68ab53":"markdown","3b069a20":"markdown","1ebd627f":"markdown","090cc14c":"markdown","61a76257":"markdown"},"source":{"c9b9a4f0":"from PIL import Image\nImage.open('..\/input\/images\/diabet.jpg')","fdea70f0":"import numpy as np\nimport pandas as pd\n\n# Visualization\nimport missingno as msno\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\n# Metrics\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score,f1_score,recall_score,mean_squared_error, r2_score, roc_auc_score, roc_curve, classification_report\nfrom sklearn.metrics import classification_report\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import precision_recall_fscore_support\n\n# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n# !pip install catboost\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier","9e65c203":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","3707869c":"data = pd.read_csv(\"..\/input\/diabetesdataset\/diabetic_data.csv\")\ndef display_all(data):\n    with pd.option_context(\"display.max_row\", 100, \"display.max_columns\", 100):\n        display(data)\ndisplay_all(data.head())","e0c2f4f4":"data.shape","32304adb":"data.info()","908300d3":"data.describe().T","f7fac331":"IDs_mapping = pd.read_csv(\"..\/input\/diabetesdataset\/IDs_mapping.csv\")\n        \ndisplay_all(IDs_mapping.head(67))","50a8d24f":"data.readmitted = [1 if each=='<30' else 0 for each in data.readmitted]","9a88a634":"fig, ax =plt.subplots(nrows=1,ncols=2, figsize=(12,5))\nlabels=['0','1']\nsns.countplot(x=data.readmitted, data=data, palette=\"pastel\",ax=ax[0], edgecolor=\".3\")\ndata.readmitted.value_counts().plot.pie(autopct=\"%1.2f%%\", ax=ax[1], colors=['#66a3ff','#facc99'], \n                                        labels=labels, explode = (0, 0.05), startangle=120,\n                                        textprops={'fontsize': 12, 'color':'#0a0a00'})\nplt.show()","d8a8fb73":"data.replace('?', np.nan , inplace=True)","8bc85f54":"msno.matrix(data)\nplt.show()","d7b3322c":"msno.bar(data,sort='descending',color='#66a9bc')\nplt.show()","56d0bbe6":"def Missing_Values(data):\n    variable_name = []\n    total_value = []\n    total_missing_value = []\n    missing_value_rate = []\n    unique_value_list = []\n    total_unique_value = []\n    data_type = []\n    \n    for col in data.columns:\n        variable_name.append(col)\n        data_type.append(data[col].dtype)\n        total_value.append(data[col].shape[0])\n        total_missing_value.append(data[col].isnull().sum())\n        missing_value_rate.append(round(data[col].isnull().sum()\/data[col].shape[0],4))\n        unique_value_list.append(data[col].unique())\n        total_unique_value.append(len(data[col].unique()))\n        \n    missing_data=pd.DataFrame({\"Variable\":variable_name,\\\n                               \"#_Total_Value\":total_value,\\\n                               \"#_Total_Missing_Value\":total_missing_value,\\\n                               \"%_Missing_Value_Rate\":missing_value_rate,\\\n                               \"Data_Type\":data_type,\"Unique_Value\":unique_value_list,\\\n                               \"Total_Unique_Value\":total_unique_value\n                              })\n    \n    missing_data = missing_data.set_index(\"Variable\")\n    return missing_data.sort_values(\"#_Total_Missing_Value\",ascending=False)","86a9a1ab":"data_info = Missing_Values(data)\ndata_info","226163f2":"data_dictionary = pd.read_csv('..\/input\/diabetesdataset\/var.csv', sep=';')\ndata_dictionary = data_dictionary.set_index(\"variable_name\")\ndata_dictionary.head()","e3cb568b":"data_info['Variable_Structure'] = np.array(data_dictionary[\"Variable_Structure\"])\ndata_info","84e73ebe":"drop_list = ['examide' , 'citoglipton', 'weight','encounter_id','patient_nbr','payer_code','medical_specialty']  \ndata.drop(drop_list,axis=1, inplace=True)\ndata_info.drop(drop_list, axis=0,inplace=True)","53c777d1":"numerical_columns = list(data_info.loc[(data_info.loc[:,\"Variable_Structure\"]==\"numeric\")].index)\nlen(numerical_columns), numerical_columns","73fbdbfd":"categorical_columns = list(data_info.loc[(data_info.loc[:,\"Variable_Structure\"]==\"nominal\")].index)\nlen(categorical_columns), categorical_columns","601d575a":"def boxplot_for_outlier(df,columns):\n    count = 0\n    fig, ax =plt.subplots(nrows=2,ncols=4, figsize=(16,8))\n    for i in range(2):\n        for j in range(4):\n            sns.boxplot(x = df[columns[count]], palette=\"Wistia\",ax=ax[i][j])  # palette = rocket, Wistia\n            count = count+1","c40189b5":"boxplot_for_outlier(data,numerical_columns)","45cfa83c":"f,ax = plt.subplots(figsize=(8, 6))\nsns.heatmap(data[numerical_columns].corr(), annot=True, linewidths=0.5,linecolor=\"black\", fmt= '.2f',ax=ax,cmap=\"coolwarm\")\nplt.show()","cabc2511":"data.gender.replace('Unknown\/Invalid', np.nan , inplace=True)\ndata.dropna(subset=['gender'], how='all', inplace = True)","851af010":"data.gender.value_counts()","81b8491e":"fig, ax =plt.subplots(nrows=1,ncols=2, figsize=(12,5))\nlabels=['Female','Male']\nsns.countplot(x=data.gender, data=data, palette=\"pastel\",ax=ax[0], edgecolor=\".3\")\ndata.gender.value_counts().plot.pie(autopct=\"%1.2f%%\", ax=ax[1], colors=['#66a3ff','#facc99'], \n                                        labels=labels, explode = (0, 0.05), startangle=120,\n                                        textprops={'fontsize': 12, 'color':'#0a0a00'})\nplt.show()","41f7abd3":"visual_list = ['gender','age','race']\nfig, ax =plt.subplots(nrows=1,ncols=3,figsize=(24,8))\ncount =0\nfor i in visual_list:\n    sns.countplot(data[i], hue=data.readmitted, palette='YlOrBr', ax=ax[count]);\n    count = count+1","1af994a3":"data.groupby(by = \"insulin\").readmitted.mean()","fcc21a22":"sns.countplot(x=\"insulin\", hue=\"readmitted\", data=data, palette=\"YlOrBr\")\nplt.show()","a0e937fa":"age_list = list(data.age.unique())\nsns.catplot(x=\"insulin\", hue=\"age\", data=data, kind=\"count\", height=6, aspect=2, palette=\"gnuplot\");","6c87428f":"data[\"race\"].fillna(data[\"race\"].mode()[0], inplace = True)","eefb8e8f":"data[\"race\"].isnull().sum()","d7273596":"data = data.loc[~data.discharge_disposition_id.isin([11,13,14,19,20,21])]","6d674ff2":"diag_list = ['diag_1','diag_2','diag_3']\n\nfor col in diag_list:\n    data[col].fillna('NaN', inplace=True)","8890c3c1":"import re\nimport numpy as np\ndef transformFunc(value):\n    value = re.sub(\"V[0-9]*\", \"0\", value) # V \n    value = re.sub(\"E[0-9]*\", \"0\", value) # E \n    value = re.sub('NaN', \"-1\", value) # Nan \n    return value\n\ndef transformCategory(value):\n    if value>=390 and value<=459 or value==785:\n        category = 'Circulatory'\n    elif value>=460 and value<=519 or value==786:\n        category = 'Respiratory'\n    elif value>=520 and value<=579 or value==787:\n        category = 'Digestive'\n    elif value==250:\n        category = 'Diabetes'\n    elif value>=800 and value<=999:\n        category = 'Injury'          \n    elif value>=710 and value<=739:\n        category = 'Musculoskeletal'   \n    elif value>=580 and value<=629 or value==788:\n        category = 'Genitourinary'\n    elif value>=140 and value<=239 :\n        category = 'Neoplasms'\n    elif value==-1:\n        category = 'NAN'\n    else :\n        category = 'Other'\n\n    return category\n\ndef diag_transform(value):\n    if value==250:\n        category = 1\n    else :\n        category = 0\n        \n    return category","22811cf9":"for col in diag_list:\n    data[col] = data[col].apply(transformFunc)\n    data[col] = data[col].astype(float)","1236b0c0":"for col in diag_list:\n    data[col] = data[col].apply(transformCategory)","2d82efaf":"fig, ax =plt.subplots(nrows=3,ncols=1,figsize=(15,12))\ncount =0\nfor i in diag_list:\n    sns.countplot(data[i], hue=data.readmitted, palette='hot', ax=ax[count]);\n    count = count+1","b516bd26":"from sklearn.neighbors import LocalOutlierFactor\nclf = LocalOutlierFactor(n_neighbors = 2 , contamination = 0.1)\nclf.fit_predict(data[numerical_columns])","8859fd23":"df_scores = clf.negative_outlier_factor_\ndf_scores[0:30]","57a9358c":"np.sort(df_scores)[0:30]","85b83128":"threshold_value = np.sort(df_scores)[2]","e689bca5":"outlier_tf = df_scores > threshold_value\noutlier_tf","f3a9758f":"new_df = data[df_scores > threshold_value]","54413c83":"data[df_scores < threshold_value]","4fc67909":"# Custom encoding for the 21 Drug Features\nkeys = ['metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', 'glipizide', 'glyburide', 'pioglitazone',\n        'rosiglitazone', 'acarbose', 'miglitol', 'insulin', 'glyburide-metformin', 'tolazamide', 'metformin-pioglitazone',\n        'metformin-rosiglitazone', 'glimepiride-pioglitazone', 'glipizide-metformin', 'troglitazone', 'tolbutamide', 'acetohexamide']\n\nfor col in keys:\n    data[col] = data[col].replace(['No','Steady','Up','Down'],[0,1,1,1])\n    data[col] = data[col].astype(int)","c7336374":"# A1Cresult and max_glu_serum\ndata['A1Cresult'] = data['A1Cresult'].replace(['>7','>8','Norm','None'],[1,1,0,-99])\ndata['max_glu_serum'] = data['max_glu_serum'].replace(['>200','>300','Norm','None'],[1,1,0,-99])","3ce214ab":"# One hot Encoding Race and Id's \none_hot_data = pd.get_dummies(data, columns=['race'], prefix=[\"enc\"])\n\ncolumns_ids = ['admission_type_id', 'discharge_disposition_id', 'admission_source_id']\n\none_hot_data[columns_ids] = one_hot_data[columns_ids].astype('str')\none_hot_data = pd.get_dummies(one_hot_data, columns=columns_ids)","2e73f8d0":"df = one_hot_data.copy()\nX = df.drop(columns=\"readmitted\", axis=1)\nY = df.readmitted","0aa798ef":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.20, random_state = 42)","7ed9ff34":"ordinal_enc = OrdinalEncoder()\nX_train.age = ordinal_enc.fit_transform(X_train.age.values.reshape(-1, 1))\nX_test.age = ordinal_enc.transform(X_test.age.values.reshape(-1, 1))","abecfab6":"for col in diag_list:\n    label_enc = LabelEncoder()\n    X_train[col] = label_enc.fit_transform(X_train[col])\n    X_test[col] = label_enc.fit_transform(X_test[col]) ","60bf2442":"binary = ['change', 'diabetesMed', 'gender']\n\nfrom category_encoders import BinaryEncoder\nbinary_enc = BinaryEncoder(cols=binary)\nbinary_enc.fit_transform(X_train)\nX_train = binary_enc.fit_transform(X_train)\nX_test = binary_enc.transform(X_test)","98b91ff9":"from sklearn.utils import resample\n\nX = pd.concat([X_train, y_train], axis=1)\n\nnot_readmitted = X[X.readmitted==0]\nreadmitted = X[X.readmitted==1]\n\nnot_readmitted_sampled = resample(not_readmitted,\n                                replace = False, \n                                n_samples = len(readmitted),\n                                random_state = 42)\n\ndownsampled = pd.concat([not_readmitted_sampled, readmitted])\n\ndownsampled.readmitted.value_counts()","183ff4a8":"y_train = downsampled.readmitted\nX_train = downsampled.drop('readmitted', axis=1)","6b3788b9":"X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.25, random_state = 42)","a2859687":"from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score,f1_score\nfrom sklearn.metrics import confusion_matrix as cm","1d361f89":"def calc_specificity(y_actual, y_pred, thresh):\n    # calculates specificity\n    return sum((y_pred < thresh) & (y_actual == 0)) \/sum(y_actual ==0)\n\ndef print_report(y_actual, y_pred, thresh):\n    \n    auc = roc_auc_score(y_actual, y_pred)\n    accuracy = accuracy_score(y_actual, (y_pred > thresh))\n    recall = recall_score(y_actual, (y_pred > thresh))\n    precision = precision_score(y_actual, (y_pred > thresh))\n    fscore = f1_score(y_actual,(y_pred > thresh) )\n    specificity = calc_specificity(y_actual, y_pred, thresh)\n    print('AUC:%.3f'%auc)\n    print('accuracy:%.3f'%accuracy)\n    print('recall:%.3f'%recall)\n    print('precision:%.3f'%precision)\n    print('fscore:%.3f'%fscore)\n    print('specificity:%.3f'%specificity)\n    print(' ')\n    return auc, accuracy, recall, precision,fscore, specificity","d9ff5216":"thresh = 0.5","5850ac08":"log_model = LogisticRegression(solver = \"liblinear\",class_weight=\"balanced\",random_state = 42).fit(X_train, y_train)","0acca06d":"y_train_preds = log_model.predict_proba(X_train)[:,1]\ny_val_preds = log_model.predict_proba(X_val)[:,1]\n\nprint(\"Logistic Regression\")\nprint('Training:')\nlr_train_auc, lr_train_accuracy, lr_train_recall, \\\n    lr_train_precision, lr_train_fscore, lr_train_specificity = print_report(y_train,y_train_preds, thresh)\nprint('Validation:')\nlr_val_auc, lr_val_accuracy, lr_val_recall, \\\n    lr_val_precision,lr_val_fscore, lr_val_specificity = print_report(y_val,y_val_preds, thresh)","6b82c8c8":"# Confusion Matrix\n\npredictions = log_model.predict(X_train)\ntrain_score = round(accuracy_score(y_train, predictions), 3)\ncm_train = cm(y_train, predictions)\n\npredictions = log_model.predict(X_val)\nval_score = round(accuracy_score(y_val, predictions), 3)\ncm_val = cm(y_val, predictions)\n\nfig, (ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize=(15,5)) \nsns.heatmap(cm_train, annot=True, fmt=\".0f\",ax=ax1)\nax1.set_xlabel('Predicted Values')\nax1.set_ylabel('Actual Values')\nax1.set_title('Train Accuracy Score: {0}'.format(train_score), size = 15)\nsns.heatmap(cm_val, annot=True, fmt=\".0f\",ax=ax2)\nax2.set_xlabel('Predicted Values')\nax2.set_ylabel('Actual Values')\nax2.set_title('Validation Accuracy Score: {0}'.format(val_score), size = 15)\nplt.show()","db75a532":"random_forest_model = RandomForestClassifier(random_state=42, n_jobs=-1, n_estimators=100, max_depth=3)\nrandom_forest_model.fit(X_train, y_train) ","8246543f":"y_train_preds = random_forest_model.predict_proba(X_train)[:,1]\ny_val_preds = random_forest_model.predict_proba(X_val)[:,1]\n\nprint(\"Random Forest\")\nprint('Training:')\nrf_train_auc, rf_train_accuracy, rf_train_recall, rf_train_precision,rf_train_fscore, rf_train_specificity =print_report(y_train,y_train_preds, thresh)\nprint('Validation:')\nrf_val_auc, rf_val_accuracy, rf_val_recall, rf_val_precision,rf_val_fscore, rf_val_specificity = print_report(y_val,y_val_preds, thresh)","93067f7e":"# Confusion Matrix\n\npredictions = random_forest_model.predict(X_train)\ntrain_score = round(accuracy_score(y_train, predictions), 3)\ncm_train = cm(y_train, predictions)\n\npredictions = random_forest_model.predict(X_val)\nval_score = round(accuracy_score(y_val, predictions), 3)\ncm_val = cm(y_val, predictions)\n\nfig, (ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize=(15,5)) \nsns.heatmap(cm_train, annot=True, fmt=\".0f\",ax=ax1)\nax1.set_xlabel('Predicted Values')\nax1.set_ylabel('Actual Values')\nax1.set_title('Train Accuracy Score: {0}'.format(train_score), size = 15)\nsns.heatmap(cm_val, annot=True, fmt=\".0f\",ax=ax2)\nax2.set_xlabel('Predicted Values')\nax2.set_ylabel('Actual Values')\nax2.set_title('Validation Accuracy Score: {0}'.format(val_score), size = 15)\nplt.show()","81e5b60c":"gradient_model = GradientBoostingClassifier(random_state=42)\ngradient_model.fit(X_train, y_train)","20150c76":"y_train_preds = gradient_model.predict_proba(X_train)[:,1]\ny_val_preds = gradient_model.predict_proba(X_val)[:,1]\n\nprint(\"Gradient Boosing\")\nprint('Training:')\ngbc_train_auc, gbc_train_accuracy, gbc_train_recall, gbc_train_precision,gbc_train_fscore, gbc_train_specificity = print_report(y_train,y_train_preds, thresh)\nprint('Validation:')\ngbc_val_auc, gbc_val_accuracy, gbc_val_recall, gbc_val_precision, gbc_val_fscore, gbc_val_specificity = print_report(y_val,y_val_preds, thresh)","66e403ba":"# Confusion Matrix\n\npredictions = gradient_model.predict(X_train)\ntrain_score = round(accuracy_score(y_train, predictions), 3)\ncm_train = cm(y_train, predictions)\n\npredictions = gradient_model.predict(X_val)\nval_score = round(accuracy_score(y_val, predictions), 3)\ncm_val = cm(y_val, predictions)\n\nfig, (ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize=(15,5)) \nsns.heatmap(cm_train, annot=True, fmt=\".0f\",ax=ax1)\nax1.set_xlabel('Predicted Values')\nax1.set_ylabel('Actual Values')\nax1.set_title('Train Accuracy Score: {0}'.format(train_score), size = 15)\nsns.heatmap(cm_val, annot=True, fmt=\".0f\",ax=ax2)\nax2.set_xlabel('Predicted Values')\nax2.set_ylabel('Actual Values')\nax2.set_title('Validation Accuracy Score: {0}'.format(val_score), size = 15)\nplt.show()","0e170d87":"xgb_model = XGBClassifier(random_state=42, n_jobs=-1,max_depth=3)\nxgb_model.fit(X_train, y_train)","81bb7c3a":"y_train_preds = xgb_model.predict_proba(X_train)[:,1]\ny_val_preds = xgb_model.predict_proba(X_val)[:,1]\n\nprint(\"XGBOOST\")\ny_train_preds = gradient_model.predict_proba(X_train)[:,1]\ny_val_preds = gradient_model.predict_proba(X_val)[:,1]\n\nprint(\"Gradient Boosing\")\nprint('Training:')\nxgb_train_auc, xgb_train_accuracy, xgb_train_recall, xgb_train_precision, xgb_train_fscore, xgb_train_specificity = print_report(y_train,y_train_preds, thresh)\nprint('Validation:')\nxgb_val_auc, xgb_val_accuracy, xgb_val_recall, xgb_val_precision,xgb_val_fscore, xgb_val_specificity = print_report(y_val,y_val_preds, thresh)","65c0d2f2":"# Confusion Matrix\n\npredictions = xgb_model.predict(X_train)\ntrain_score = round(accuracy_score(y_train, predictions), 3)\ncm_train = cm(y_train, predictions)\n\npredictions = xgb_model.predict(X_val)\nval_score = round(accuracy_score(y_val, predictions), 3)\ncm_val = cm(y_val, predictions)\n\nfig, (ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize=(15,5)) \nsns.heatmap(cm_train, annot=True, fmt=\".0f\",ax=ax1)\nax1.set_xlabel('Predicted Values')\nax1.set_ylabel('Actual Values')\nax1.set_title('Train Accuracy Score: {0}'.format(train_score), size = 15)\nsns.heatmap(cm_val, annot=True, fmt=\".0f\",ax=ax2)\nax2.set_xlabel('Predicted Values')\nax2.set_ylabel('Actual Values')\nax2.set_title('Validation Accuracy Score: {0}'.format(val_score), size = 15)\nplt.show()","f43700e9":"from lightgbm import LGBMClassifier\nlgbm_model = LGBMClassifier(random_state = 42,max_depth=3)\nlgbm_model.fit(X_train, y_train)","8c841bcf":"y_train_preds = lgbm_model.predict_proba(X_train)[:,1]\ny_val_preds = lgbm_model.predict_proba(X_val)[:,1]\n\nprint(\"LGBM\")\nprint('Training:')\nlgbm_train_auc, lgbm_train_accuracy,lgbm_train_recall, lgbm_train_precision,lgbm_train_fscore,lgbm_train_specificity = print_report(y_train,y_train_preds, thresh)\nprint('Validation:')\nlgbm_val_auc, lgbm_val_accuracy, lgbm_val_recall, lgbm_val_precision,lgbm_val_fscore,lgbm_val_specificity = print_report(y_val,y_val_preds, thresh)","8634c787":"# Confusion Matrix\n\npredictions = lgbm_model.predict(X_train)\ntrain_score = round(accuracy_score(y_train, predictions), 3)\ncm_train = cm(y_train, predictions)\n\npredictions = lgbm_model.predict(X_val)\nval_score = round(accuracy_score(y_val, predictions), 3)\ncm_val = cm(y_val, predictions)\n\nfig, (ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize=(15,5)) \nsns.heatmap(cm_train, annot=True, fmt=\".0f\",ax=ax1)\nax1.set_xlabel('Predicted Values')\nax1.set_ylabel('Actual Values')\nax1.set_title('Train Accuracy Score: {0}'.format(train_score), size = 15)\nsns.heatmap(cm_val, annot=True, fmt=\".0f\",ax=ax2)\nax2.set_xlabel('Predicted Values')\nax2.set_ylabel('Actual Values')\nax2.set_title('Validation Accuracy Score: {0}'.format(val_score), size = 15)\nplt.show()","75e9d6ee":"cat_model = CatBoostClassifier(random_state = 42, max_depth=3)\ncat_model.fit(X_train, y_train,verbose=False)","77dfbea5":"y_train_preds = cat_model.predict_proba(X_train)[:,1]\ny_val_preds = cat_model.predict_proba(X_val)[:,1]\n\nprint(\"CATBOOST\")\nprint('Training:')\ncatb_train_auc, catb_train_accuracy,catb_train_recall, catb_train_precision,catb_train_fscore,catb_train_specificity = print_report(y_train,y_train_preds, thresh)\nprint('Validation:')\ncatb_val_auc,catb_val_accuracy, catb_val_recall, catb_val_precision,catb_val_fscore,catb_val_specificity = print_report(y_val,y_val_preds, thresh)","248e121e":"# Confusion Matrix\n\npredictions = cat_model.predict(X_train)\ntrain_score = round(accuracy_score(y_train, predictions), 3)\ncm_train = cm(y_train, predictions)\n\npredictions = cat_model.predict(X_val)\nval_score = round(accuracy_score(y_val, predictions), 3)\ncm_val = cm(y_val, predictions)\n\nfig, (ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize=(15,5)) \nsns.heatmap(cm_train, annot=True, fmt=\".0f\",ax=ax1)\nax1.set_xlabel('Predicted Values')\nax1.set_ylabel('Actual Values')\nax1.set_title('Train Accuracy Score: {0}'.format(train_score), size = 15)\nsns.heatmap(cm_val, annot=True, fmt=\".0f\",ax=ax2)\nax2.set_xlabel('Predicted Values')\nax2.set_ylabel('Actual Values')\nax2.set_title('Validation Accuracy Score: {0}'.format(val_score), size = 15)\nplt.show()","9fa8cf40":"base_models_results = pd.DataFrame({'classifier':['LOJ','LOJ','RF','RF','GBM','GBM','XGB','XGB','LGBM','LGBM','CATB','CATB'],\n                           'data_set':['train','val']*6,\n                          'auc':[lr_train_auc, lr_val_auc,rf_train_auc,rf_val_auc,gbc_train_auc,gbc_val_auc,xgb_train_auc,xgb_val_auc,lgbm_train_auc,lgbm_val_auc,catb_train_auc,catb_val_auc,],\n                          'accuracy':[lr_train_accuracy, lr_val_accuracy,rf_train_accuracy,rf_val_accuracy,gbc_train_accuracy,gbc_val_accuracy,xgb_train_accuracy,xgb_val_accuracy,lgbm_train_accuracy,lgbm_val_accuracy,catb_train_accuracy,catb_val_accuracy,],\n                          'recall':[lr_train_recall, lr_val_recall,rf_train_recall,rf_val_recall,gbc_train_recall,gbc_val_recall,xgb_train_recall,xgb_val_recall,lgbm_train_recall,lgbm_val_recall,catb_train_recall,catb_val_recall,],\n                          'precision':[lr_train_precision, lr_val_precision,rf_train_precision,rf_val_precision,gbc_train_precision,gbc_val_precision,xgb_train_precision,xgb_val_precision,lgbm_train_precision,lgbm_val_precision,catb_train_precision,catb_val_precision,],\n                          'fscore':[lr_train_fscore, lr_val_fscore,rf_train_fscore,rf_val_fscore,gbc_train_fscore,gbc_val_fscore,xgb_train_fscore,xgb_val_fscore,lgbm_train_fscore,lgbm_val_fscore,catb_train_fscore,catb_val_fscore,],\n                          'specificity':[lr_train_specificity, lr_val_specificity,rf_train_specificity,rf_val_specificity,gbc_train_specificity,gbc_val_specificity,xgb_train_specificity,xgb_val_specificity,lgbm_train_specificity,lgbm_val_specificity,catb_train_specificity,catb_val_specificity,]})","993ada60":"base_models_results","849b12d0":"fig, ax = plt.subplots(figsize=(10,6)) \nax = sns.barplot(x=\"classifier\", y=\"auc\", hue=\"data_set\", data=base_models_results)\nax.set_xlabel('Classifier',fontsize = 15)\nax.set_ylabel('AUC', fontsize = 15)\nax.tick_params(labelsize=15)\n\n# Put the legend out of the figure\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., fontsize = 15)\nplt.show()","0589f999":"import lightgbm as lgb\n\nplt.rcParams[\"figure.figsize\"] = (18, 10)\nlgb.plot_importance(lgbm_model)\n\nfeature_imp = pd.Series(lgbm_model.feature_importances_, index = X_train.columns)\nbest_features = feature_imp.nlargest(25)","9a16236a":"best_features.index","19942eda":"X_train_importance = X_train[best_features.index]\nX_val_importance = X_val[best_features.index]\nX_test_importance = X_test[best_features.index]","4a4b2ba6":"rf = RandomForestClassifier()","23f63320":"rf_params = {\"max_depth\": [2,5,8],\n             \"n_estimators\": [100,200,500,700],\n             \"max_features\": [3,5,8],\n             \"min_samples_split\":[2,5,10]}","9ec76cd8":"rf_cv_model = GridSearchCV(rf, rf_params, cv=3, n_jobs=-1, verbose=2).fit(X_train_importance, y_train)","401c3824":"rf_cv_model.best_params_","6f730197":"rf_tuned =RandomForestClassifier(max_depth=5,\n                                 max_features=5,\n                                 min_samples_split=5,\n                                 n_estimators=500).fit(X_train_importance, y_train)","34ccac9f":"y_train_preds = random_forest_model.predict_proba(X_train)[:,1]\ny_val_preds = random_forest_model.predict_proba(X_val)[:,1]\ny_test_preds = random_forest_model.predict_proba(X_test)[:,1]\n\nprint('Baseline Random Forest')\nrf_train_auc_base = roc_auc_score(y_train, y_train_preds)\nrf_val_auc_base = roc_auc_score(y_val, y_val_preds)\nrf_test_auc_base = roc_auc_score(y_test, y_test_preds)\n\nprint('Training AUC:%.3f'%(rf_train_auc_base))\nprint('Validation AUC:%.3f'%(rf_val_auc_base))\nprint('Test AUC:%.3f'%(rf_test_auc_base))\n\nprint('Optimized Random Forest')\ny_train_preds_random = rf_tuned.predict_proba(X_train_importance)[:,1]\ny_val_preds_random = rf_tuned.predict_proba(X_val_importance)[:,1]\ny_test_preds_random = rf_tuned.predict_proba(X_test_importance)[:,1]\n\nrf_train_auc = roc_auc_score(y_train, y_train_preds_random)\nrf_val_auc = roc_auc_score(y_val, y_val_preds_random)\nrf_test_auc = roc_auc_score(y_test, y_test_preds_random)\n\nprint('Training AUC:%.3f'%(rf_train_auc))\nprint('Validation AUC:%.3f'%(rf_val_auc))\nprint('Test AUC:%.3f'%(rf_test_auc))","db72dc4b":"lgbm=LGBMClassifier()","61d7e66d":"lgbm_params = {\"learning_rate\":[0.01,0.1,0.05],\n              \"n_estimators\": [100,200,500],\n               \"subsample\":[0.1,0.2],\n              \"max_depth\":[2,3,5,8]}","111da32a":"lgbm_cv_model=GridSearchCV(lgbm,lgbm_params,cv=3,n_jobs=-1,verbose=2).fit(X_train_importance,y_train)","b11f6bc3":"lgbm_cv_model.best_params_","12e752c1":"lgbm_tuned=LGBMClassifier(learning_rate=0.1,max_depth=2,n_estimators=200,subsample= 0.1).fit(X_train_importance, y_train)","f6ddffc9":"y_train_preds = lgbm_model.predict_proba(X_train)[:,1]\ny_val_preds = lgbm_model.predict_proba(X_val)[:,1]\ny_test_preds = lgbm_model.predict_proba(X_test)[:,1]\n\nprint('Baseline LGBM')\nlgbm_train_auc_base = roc_auc_score(y_train, y_train_preds)\nlgbm_val_auc_base = roc_auc_score(y_val, y_val_preds)\nlgbm_test_auc_base = roc_auc_score(y_test, y_test_preds)\n\nprint('Training AUC:%.3f'%(lgbm_train_auc_base))\nprint('Validation AUC:%.3f'%(lgbm_val_auc_base))\nprint('Test AUC:%.3f'%(lgbm_test_auc_base))\n\nprint('Optimized LGBM')\ny_train_preds_lgbm = lgbm_tuned.predict_proba(X_train_importance)[:,1]\ny_val_preds_lgbm = lgbm_tuned.predict_proba(X_val_importance)[:,1]\ny_test_preds_lgbm = lgbm_tuned.predict_proba(X_test_importance)[:,1]\n\nlgbm_train_auc = roc_auc_score(y_train, y_train_preds_lgbm)\nlgbm_val_auc = roc_auc_score(y_val, y_val_preds_lgbm)\nlgbm_test_auc = roc_auc_score(y_test, y_test_preds_lgbm)\n\nprint('Training AUC:%.3f'%(lgbm_train_auc))\nprint('Validation AUC:%.3f'%(lgbm_val_auc))\nprint('Test AUC:%.3f'%(lgbm_test_auc))","f4fb8f2e":"catb=CatBoostClassifier()","e1120ec1":"catb_params={\"iterations\":[200,500,1000],\n            \"learning_rate\":[0.05,0.1],\n            \"depth\":[4,5,8]}","e58d72f6":"catb_cv_model=GridSearchCV(catb,catb_params, cv=3, n_jobs=-1,  verbose=2).fit(X_train_importance,y_train)","b25e19e4":"catb_cv_model.best_params_","c35b2f63":"catb_tuned =CatBoostClassifier(depth=5,iterations=200,learning_rate=0.05).fit(X_train_importance, y_train)","7b39d7d7":"y_train_preds = cat_model.predict_proba(X_train)[:,1]\ny_valid_preds = cat_model.predict_proba(X_val)[:,1]\ny_test_preds = cat_model.predict_proba(X_test)[:,1]\n\nprint('Baseline CATBOOST')\ncatb_train_auc_base = roc_auc_score(y_train, y_train_preds)\ncatb_val_auc_base = roc_auc_score(y_val, y_val_preds)\ncatb_test_auc_base = roc_auc_score(y_test, y_test_preds)\n\nprint('Training AUC:%.3f'%(catb_train_auc_base))\nprint('Validation AUC:%.3f'%(catb_val_auc_base))\nprint('Test AUC:%.3f'%(catb_test_auc_base))\n\nprint('Optimized CATBOOST')\ny_train_preds_catb = catb_tuned.predict_proba(X_train_importance)[:,1]\ny_val_preds_catb = catb_tuned.predict_proba(X_val_importance)[:,1]\ny_test_preds_catb = catb_tuned.predict_proba(X_test_importance)[:,1]\n\ncatb_train_auc = roc_auc_score(y_train, y_train_preds_catb)\ncatb_val_auc = roc_auc_score(y_val, y_val_preds_catb)\ncatb_test_auc = roc_auc_score(y_test, y_test_preds_catb)\n\nprint('Training AUC:%.3f'%(catb_train_auc))\nprint('Validation AUC:%.3f'%(catb_val_auc))\nprint('Test AUC:%.3f'%(catb_test_auc))","437433ae":"data_results = pd.DataFrame({'classifier':['RF','RF','LGBM','LGBM','CATB','CATB'],\n                           'data_set':['base','optimized']*3,\n                          'auc':[rf_val_auc_base,rf_val_auc,\n                                 lgbm_val_auc_base,lgbm_val_auc,\n                                 catb_val_auc_base,catb_val_auc,],\n                          })","f078d1ce":"data_results","3f0155cb":"ax = sns.barplot(x=\"classifier\", y=\"auc\", hue=\"data_set\", data=data_results)\nax.set_xlabel('Classifier',fontsize = 15)\nax.set_ylabel('AUC', fontsize = 15)\nax.tick_params(labelsize=15)\n# Put the legend out of the figure\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., fontsize = 15)\n\nplt.show()","80fbed8c":"classifiers = [ rf_tuned,\n                lgbm_tuned,\n                catb_tuned]\n\n# Define a result table as a DataFrame\nresult_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])\n\n# Train the models and record the results\nfor cls in classifiers:\n    yproba = cls.predict_proba(X_test_importance)[::,1]\n    \n    fpr, tpr, _ = roc_curve(y_test,  yproba)\n    auc = roc_auc_score(y_test, yproba)\n    \n    result_table = result_table.append({'classifiers':cls.__class__.__name__,\n                                        'fpr':fpr, \n                                        'tpr':tpr, \n                                        'auc':auc}, ignore_index=True)\n\n# Set name of the classifiers as index labels\nresult_table.set_index('classifiers', inplace=True)\nresult_table.sort_values('auc',ascending=False,inplace=True)","b6199d3e":"fig = plt.figure(figsize=(10,6))\n\nfor i in result_table.index:\n    plt.plot(result_table.loc[i]['fpr'], \n             result_table.loc[i]['tpr'], \n             label=\"{}, AUC={:.3f}\".format(i, result_table.loc[i]['auc']))\n    \nplt.plot([0,1], [0,1], color='black', linestyle='--')\n\nplt.xticks(np.arange(0.0, 1.1, step=0.1))\nplt.xlabel(\"False Positive Rate\", fontsize=14)\n\nplt.yticks(np.arange(0.0, 1.1, step=0.1))\nplt.ylabel(\"True Positive Rate\", fontsize=14)\n\nplt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\nplt.legend(prop={'size':10}, loc='lower right')\n\nplt.show()","fd5e1f07":"def test_scores(y_actual, y_pred, thresh):\n    \n    auc = roc_auc_score(y_actual, y_pred)\n    accuracy = accuracy_score(y_actual, (y_pred > thresh))\n    recall = recall_score(y_actual, (y_pred > thresh))\n    \n    return auc, accuracy, recall\n\n\nclassifiers = [ rf_tuned,\n                lgbm_tuned,\n                catb_tuned]\n\n# Define a result table as a DataFrame\ntest_result = pd.DataFrame(columns=['classifiers', 'accuracy','recall','auc'])\n\n# Train the models and record the results\nfor cls in classifiers:\n    y_test_preds = cls.predict_proba(X_test_importance)[:,1]\n    \n    test_auc, test_accuracy, test_recall = test_scores(y_test,y_test_preds, 0.5) # thresh = 0.5\n    \n    test_result = test_result.append({'classifiers':cls.__class__.__name__,\n                                        'accuracy':test_accuracy, \n                                        'recall':test_recall, \n                                        'auc':test_auc}, ignore_index=True)\n\n\n# Set name of the classifiers as index labels\ntest_result.set_index('classifiers', inplace=True)\ntest_result.sort_values('auc',ascending=False,inplace=True)","53470df3":"test_result","8adc505e":"import plotly.graph_objs as go\n\ntrace1=go.Bar(\n                x=test_result.index,\n                y=test_result.accuracy,\n                name=\"Accuracy\",\n                marker= dict(color = 'rgba(100, 20, 30, 0.7)',\n                            line=dict(color='rgb(0,0,0)',width=1.9)),\n                text=round(test_result.accuracy,3),textposition='auto')\ntrace2=go.Bar(\n                x=test_result.index,\n                y=test_result.recall,\n                name=\"Recall\",\n                marker=dict(color = 'rgba(56, 140, 200, 0.7)',\n                           line=dict(color='rgb(0,0,0)',width=1.9)),\n                text=round(test_result.recall,3),textposition='auto')\ntrace3=go.Bar(\n                x=test_result.index,\n                y=test_result.auc,\n                name=\"AUC\",\n                marker=dict(color = 'rgba(120, 180, 20, 0.7)',\n                           line=dict(color='rgb(0,0,0)',width=1.9)),\n                text=round(test_result.auc,3),textposition='auto')\n\nedit_df=[trace1,trace2,trace3]\nlayout = { 'barmode':'group',\n           'title_text':'Accuracy, Recall and AUC Plot Readmitted' }\n\nfig= go.Figure(data=edit_df,layout=layout)\n#plt.savefig('graph.png')\nfig.show()","c2333bef":"best_model = catb_tuned\n\ny_train_preds = best_model.predict_proba(X_train_importance)[:,1]\ny_valid_preds = best_model.predict_proba(X_val_importance)[:,1]\ny_test_preds = best_model.predict_proba(X_test_importance)[:,1]","06c8e722":"thresh = 0.5\n\nprint('Training:')\ntrain_auc, train_accuracy, train_recall, train_precision, train_fscore, train_specificity = print_report(y_train,y_train_preds, thresh)\nprint('Validation:')\nval_auc, val_accuracy, val_recall, val_precision, val_fscore,val_specificity = print_report(y_val,y_val_preds, thresh)\nprint('Test:')\ntest_auc, test_accuracy, test_recall, test_precision, test_fscore, test_specificity = print_report(y_test,y_test_preds, thresh)","b38a3a92":"from sklearn.metrics import roc_curve \n\nfpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_train_preds)\nauc_train = roc_auc_score(y_train, y_train_preds)\n\nfpr_val, tpr_val, thresholds_val = roc_curve(y_val, y_val_preds)\nauc_val = roc_auc_score(y_val, y_val_preds)\n\nfpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_test_preds)\nauc_test = roc_auc_score(y_test, y_test_preds)\n\nfig, ax = plt.subplots(figsize=(10,6)) \nplt.plot(fpr_train, tpr_train, 'r-',label ='Train AUC:%.3f'%auc_train)\nplt.plot(fpr_val, tpr_val, 'b-',label ='Valid AUC:%.3f'%auc_val)\nplt.plot(fpr_test, tpr_test, 'g-',label ='Test AUC:%.3f'%auc_test)\nplt.plot([0,1],[0,1],'k--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.show()","44befce2":"from PIL import Image\nImage.open('..\/input\/poster\/diabetes_classification_analysis_results_POSTER.jpeg')","0c472e5e":"# 8. Prediction Result","6419b62d":"Diabetes, which is at the forefront of diseases of the age, is a disease that plays a leading role in the formation of many deadly diseases and is very common all over the world.\n\nIt is important to know whether a patient can be readmitted in a hospital. In this project, we tried predict whether diabetes patients will return to the hospital or not by using machine learning algorithms.","2aec4258":"## Random Forest Classifier","9deaaf42":"## Variable Description","95767a91":"# 2. V\u0130SUAL\u0130ZAT\u0130ON","996509f0":"## XGBOOST Classifier","6d9b92a6":"## Train-Validation Splitting","71c619da":"## Hyperparameter tuning results","4914c270":"# Hospital Readmissions Prediction","688808fe":"# 5. Modelling","9793d931":"## Model Selection: Baseline Models","ee119c42":"### Visualization of the insulin variable according to the age variable:","7481a1cf":"# 3. Feature Engineering","67962c52":"## CATBOOST Classifier Model Tuning","85a2ef6c":"## Light-GBM Classifier","3a2aef5f":"## Target Distribution","1836f072":"## Random Forest Classifier Model Tuning","384073aa":"### Distribution of Diag_1, Diag_2 and Diag_3 Variables by Target Variable:","a8f0791b":"## Roc-Auc Comparison of Models","a6da00f3":"# 7. Hyperparameter tuning","041be689":"## Determination of numerical columns:","9aa62767":"## Logistic Regression","dcb4ab29":"## Gradient Boosting Classifier","e1fdaa8b":"# 1. Exploratory Data Analysis\n\n## Importing Libraries","d7654d91":"## Loading Data","edec4abe":"## CATBOOST Classifier","76016c48":"# 6. Feature Importance","f3d3d4dd":"## Representation of missing values, unique values, etc.","19fba22f":"### Gender, Age and Race Visualization","9c1e8f83":"## Encoding","b3c5543c":"## Determination of categorical columns:","6463ab1d":"## Local Outlier Factor","43b62ac1":"### Examination and visualization of the effect of the target variable on insulin variable","f5d36ead":"Columns that would not give information were removed","324613e9":"# Model Selection: Best Classifier","f2ae7982":"Encounter ID: Unique identifier of an encounter\n    \nPatient number: Unique identifier of a patient\n    \nRace Values: Caucasian, Asian, African American, Hispanic, and other\n    \nGender Values: male, female, and unknown\/invalid\n    \nAge: Grouped in 10-year intervals: 0, 10), 10, 20), \u2026, 90, 100)\n        \nWeight: Weight in pounds\n    \nAdmission type: Integer identifier corresponding to 9 distinct values, for example, emergency, urgent, elective, newborn, and not available\n    \nDischarge disposition: Integer identifier corresponding to 29 distinct values, for example, discharged to home, expired, and not available\n\nAdmission source: Integer identifier corresponding to 21 distinct values, for example, physician referral, emergency room, and transfer from a hospital\n\nTime in hospital: Integer number of days between admission and discharge\n\nPayer code : Integer identifier corresponding to 23 distinct values, for example, Blue Cross\/Blue Shield, Medicare, and self-pay Medical\n\nMedical specialty: Integer identifier of a specialty of the admitting physician, corresponding to 84 distinct values, for example, cardiology, internal medicine, family\/general practice, and surgeon\n\nNumber of lab procedures: Number of lab tests performed during the encounter\n\nNumber of procedures: Numeric Number of procedures (other than lab tests) performed during the encounter\n\nNumber of medications: Number of distinct generic names administered during the encounter\n\nNumber of outpatient visits: Number of outpatient visits of the patient in the year preceding the encounter\n\nNumber of emergency visits: Number of emergency visits of the patient in the year preceding the encounter\n\nNumber of inpatient visits: Number of inpatient visits of the patient in the year preceding the encounter\n\nDiagnosis 1: The primary diagnosis (coded as first three digits of ICD9); 848 distinct values\n\nDiagnosis 2: Secondary diagnosis (coded as first three digits of ICD9); 923 distinct values\n\nDiagnosis 3: Additional secondary diagnosis (coded as first three digits of ICD9); 954 distinct values\n\nNumber of diagnoses : Number of diagnoses entered to the system 0%\n\nGlucose serum test : result Indicates the range of the result or if the test was not taken. Values: \u201c>200,\u201d \u201c>300,\u201d \u201cnormal,\u201d and \u201cnone\u201d if not measured\n\nA1c test result : Indicates the range of the result or if the test was not taken. Values: \u201c>8\u201d if the result was greater than 8%, \u201c>7\u201d if the result was greater than 7% but less than 8%, \u201cnormal\u201d if the result was less than 7%, and \u201cnone\u201d if not measured.\n\nChange of medications : Indicates if there was a change in diabetic medications (either dosage or generic name). Values: \u201cchange\u201d and \u201cno change\u201d\n\nDiabetes medications : Indicates if there was any diabetic medication prescribed. Values: \u201cyes\u201d and \u201cno\u201d 24 features for medications For the generic names: metformin, repaglinide, nateglinide, chlorpropamide, glimepiride, acetohexamide, glipizide, glyburide, tolbutamide, pioglitazone, rosiglitazone, acarbose, miglitol, troglitazone, tolazamide, examide, sitagliptin, insulin, glyburide-metformin, glipizide-metformin, glimepiride- pioglitazone, metformin-rosiglitazone, and metformin- pioglitazone, the feature indicates whether the drug was prescribed or there was a change in the dosage. Values: \u201cup\u201d if the dosage was increased during the encounter, \u201cdown\u201d if the dosage was decreased, \u201csteady\u201d if the dosage did not change, and \u201cno\u201d if the drug was not prescribed\n\nReadmitted: Days to inpatient readmission. Values: \u201c<30\u201d if the patient was readmitted in less than 30 days, \u201c>30\u201d if the patient was readmitted in more than 30 days, and \u201cNo\u201d for no record of readmission","bbfbedb8":"## Missing Value Filling","0073ab92":"## Analyze results baseline models","bb809a99":"# Content\n\nThe data set represents 10 years (1999-2008) of clinical care at 130 US hospitals and integrated delivery networks.\n\nThe following steps were followed in this project;\n<ol> \n    <li>Exploratory Data Analysis<\/li>\n    <li>Visualization<\/li>\n    <li>Feature Engineering<\/li>\n        <ul>     \n         <li> Missing Value Handling<\/li>\n         <li> Outlier Handling<\/li>\n         <li> Encoding<\/li>\n        <\/ul>\n    <li>Splitting Train-Validation-Test<\/li>\n    <li>Modelling<\/li>\n        <ul>     \n         <li> Logistic Regression<\/li>\n         <li> Random Forest Classifier<\/li>\n         <li> GradientBoosting Classifier<\/li>\n         <li> XGboost Classifier<\/li>\n         <li> Light-GBM Classifier<\/li>\n         <li> CatBoost Classifier<\/li>\n        <\/ul>\n    <li>Feature Importance<\/li>\n    <li>Hyperparameter Tuning<\/li>\n    <li>Predict results<\/li>\n<\/ol> ","a93e2ea0":"## Gender Distribution","ad022599":"## Feature \u0130mportance with Light-GBM Classifier","5b7ed873":"## Types of data measurement scales","51d03237":"## Ligth-GBM Classifier Model Tuning","d0feaffb":"## Information about Missing values","89a1095a":"## Model Selection","cc7c8ea5":"### Resampling techniques \u2014 Undersample majority class","ea68ab53":"Target content changed to 1-0\n\nThe outcome we are looking at is whether the patient gets readmitted to the hospital within 30 days or not.\n\nThe variable actually has <30, >30 and No Readmission categories. To reduce our problem to a binary classification, we combined the readmission after 30 days and no readmission into a single category:","3b069a20":"### Diagnostics 1-2-3 Transform","1ebd627f":"## Outlier Visualization With BoxPlot:","090cc14c":"[presentation]!(http:\/\/prezi.com\/view\/TJ6WPvqUV5962zIIeEnV\/)","61a76257":"# 4. Train-Test Splitting"}}