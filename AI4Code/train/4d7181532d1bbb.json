{"cell_type":{"f96eefb4":"code","b8da904a":"code","d5bde2de":"code","33be970f":"code","213e93cb":"code","e5575d4c":"code","bbb07489":"code","b7c114ba":"code","36a06f27":"code","00d92c24":"code","1c6c1bc8":"code","adcee9bb":"code","558796ac":"code","41f653f4":"code","af29a4de":"code","a0c45250":"code","af97666a":"code","d0b7f3e6":"code","5e92f703":"code","df363209":"code","ca359cea":"code","754b8552":"code","f5035328":"code","44fe86b0":"code","74f6a621":"code","37177c1d":"code","f704ba99":"code","8e424710":"code","a99399f9":"code","c1c9e026":"code","0c36dc66":"code","6af47591":"code","05b578ce":"code","0cff5caa":"code","68f999ab":"code","4c8b151c":"code","074bdf84":"code","d6b714cf":"code","f45ca4ac":"code","a81ac9aa":"code","fc8ed02e":"code","eaa7a768":"code","df653e82":"code","e5013792":"code","fc65200b":"code","257608a5":"markdown","dfc806ad":"markdown","b1789da3":"markdown","e050103e":"markdown","f3ac1100":"markdown","934714b5":"markdown","7f549857":"markdown","bc3e7466":"markdown","c858ef3e":"markdown","27c066cf":"markdown","90981f3b":"markdown","323096f0":"markdown","34d46899":"markdown","bc7c44a4":"markdown","48612729":"markdown","e0222047":"markdown","2787eec5":"markdown","3a856620":"markdown"},"source":{"f96eefb4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix , classification_report\nfrom sklearn.metrics import roc_curve , auc\nfrom sklearn.svm import SVC\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n#import os\n#print(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b8da904a":"# Load Data\ndf = pd.read_csv('..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv')","d5bde2de":"df.head()","33be970f":"df.info()","213e93cb":"df.SeniorCitizen.unique()","e5575d4c":"#Convert to Categorical variable\ndf.SeniorCitizen= df.SeniorCitizen.apply(lambda x : 'No' if x == 0 else 'Yes')\n","bbb07489":"#Check Type after conversion\ndf.SeniorCitizen.unique()","b7c114ba":"df['TotalCharges_new']= pd.to_numeric(df.TotalCharges,errors='coerce_numeric')","36a06f27":"#Check NULL values after the conversion\ndf.loc[pd.isna(df.TotalCharges_new),'TotalCharges']","00d92c24":"#Fill 11 Missing values from the original column\nTotalCharges_Missing=[488,753,936,1082,1340,3331,3826,4380,5218,6670,6754]\ndf.loc[pd.isnull(df.TotalCharges_new),'TotalCharges_new']=TotalCharges_Missing\n","1c6c1bc8":"#We are good to replace old columns with the new numerical column\ndf.TotalCharges=df.TotalCharges_new\ndf.drop(['customerID','TotalCharges_new'],axis=1,inplace=True)\ndf.info()","adcee9bb":"df.dtypes=='object'\ncategorical_var=[i for i in df.columns if df[i].dtypes=='object']\nfor z in categorical_var:\n    print(df[z].name,':',df[z].unique())","558796ac":"Dual_features= ['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']\nfor i in Dual_features:\n    df[i]=df[i].apply(lambda x: 'No' if x=='No internet service' else x)\n#Remove No Phones Service that equivilent to No for MultipleLines\ndf.MultipleLines=df.MultipleLines.apply(lambda x: 'No' if x=='No phone service' else x)\n","41f653f4":"#Check levels or all Categorical Variables\nfor z in [i for i in df.columns if df[i].dtypes=='object']:\n    print(df[z].name,':',df[z].unique())","af29a4de":"continues_var=[i for i in df.columns if df[i].dtypes !='object']\nfig , ax = plt.subplots(1,3,figsize=(15,5))\nfor i , x in enumerate(continues_var):\n    ax[i].hist(df[x][df.Churn=='No'],label='Churn=0',bins=30)\n    ax[i].hist(df[x][df.Churn=='Yes'],label='Churn=1',bins=30)\n    ax[i].set(xlabel=x,ylabel='count')\n    ax[i].legend()","a0c45250":"fig , ax = plt.subplots(1,3,figsize=(15,5))\nfor i , xi in enumerate(continues_var):\n    sns.boxplot(x=df.Churn,y=df[xi],ax=ax[i],hue=df.gender)\n    ax[i].set(xlabel='Churn',ylabel=xi)\n    ax[i].legend()\n","af97666a":"#Remove Churn Variable for Analysis\ncategorical_var_NoChurn= categorical_var[:-1]","d0b7f3e6":"#Count Plot all Categorical Variables with Hue Churn\nfig , ax = plt.subplots(4,4,figsize=(20,20))\nfor axi , var in zip(ax.flat,categorical_var_NoChurn):\n    sns.countplot(x=df.Churn,hue=df[var],ax=axi)","5e92f703":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder=LabelEncoder()\nfor x in [i for i in df.columns if len(df[i].unique())==2]:\n    print(x, df[x].unique())\n    df[x]= label_encoder.fit_transform(df[x])","df363209":"#Check Variables after Encoding\n[[x, df[x].unique()] for x in [i for i in df.columns if len(df[i].unique())<10]]","ca359cea":"#Encode Variables with more than 2 Classes\ndf= pd.get_dummies(df, columns= [i for i in df.columns if df[i].dtypes=='object'],drop_first=True)\n  ","754b8552":"#Check Variables after Encoding\n[[x, df[x].unique()] for x in [i for i in df.columns if len(df[i].unique())<10]]","f5035328":"#Create Features DataFrame\nX=df.drop('Churn',axis=1)\n#Create Target Series\ny=df['Churn']\n#Split Data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)","44fe86b0":"#Scale Data\nsc= StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_train=pd.DataFrame(X_train,columns=X.columns)\nX_test=sc.transform(X_test)","74f6a621":"#Check Data after Scaling\nX_train.head()","37177c1d":"#Apply RandomForest Algorethm\nrandom_classifier= RandomForestClassifier()\nrandom_classifier.fit(X_train,y_train)","f704ba99":"y_pred= random_classifier.predict(X_test)","8e424710":"#Classification Report\nprint(classification_report(y_test,y_pred))","a99399f9":"#Confusion Matrix\nmat = confusion_matrix(y_test, y_pred)\nsns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n          xticklabels=['No','Yes'],\n          yticklabels=['No','Yes'] )\nplt.xlabel('true label')\nplt.ylabel('predicted label')","c1c9e026":"#get features Importances\nxx= pd.Series(random_classifier.feature_importances_,index=X.columns)\nxx.sort_values(ascending=False)","0c36dc66":"y_pred_proba=random_classifier.predict_proba(X_test)[:,1]","6af47591":"fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\nroc_auc=auc(fpr,tpr)\n#Now Draw ROC using fpr , tpr\nplt.plot([0, 1], [0, 1], 'k--',label='Random')\nplt.plot(fpr,tpr,label='ROC curve (area = %0.2f)' %roc_auc)\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('Random Forest ROC curve')\nplt.legend(loc='best')\n","05b578ce":"svm_classifier= SVC(probability=True)\nsvm_classifier.fit(X_train,y_train)","0cff5caa":"#Predict\ny_pred_svm= svm_classifier.predict(X_test)\n#Classification Report\nprint(classification_report(y_test,y_pred_svm))","68f999ab":"#Confusion Matrix\nmat_svm = confusion_matrix(y_test, y_pred_svm)\nsns.heatmap(mat_svm.T, square=True, annot=True, fmt='d', cbar=False,\n          xticklabels=['No','Yes'],\n          yticklabels=['No','Yes'] )\nplt.xlabel('true label')\nplt.ylabel('predicted label')","4c8b151c":"y_pred_svm_proba=svm_classifier.predict_proba(X_test)[:,1]\n#ROC Curve\nfpr_svm, tpr_svm, _svm = roc_curve(y_test, y_pred_svm_proba)\nroc_auc=auc(fpr_svm,tpr_svm)\n#Now Draw ROC using fpr , tpr\nplt.plot([0, 1], [0, 1], 'k--',label='Random')\nplt.plot(fpr,tpr,label='ROC curve (area = %0.2f)' %roc_auc)\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('SVM ROC curve')\nplt.legend(loc='best')\n","074bdf84":"#Initiate ANN Classifier\nann_classifier= Sequential()\nX.shape","d6b714cf":"#Adding Hidden Layer1\nann_classifier.add(Dense(12,activation='relu',kernel_initializer='uniform',input_dim=23))\n#Adding Hidden Layer2\nann_classifier.add(Dense(12,activation='relu',kernel_initializer='uniform'))\n#Adding output Layer\nann_classifier.add(Dense(1,activation='sigmoid',kernel_initializer='uniform'))\n#Compile them Model\nann_classifier.compile(optimizer='adam',loss='binary_crossentropy', metrics = ['accuracy'])","f45ca4ac":"ann_classifier.summary()","a81ac9aa":"%time ann_classifier.fit(X_train,y_train,batch_size=10,epochs=100)","fc8ed02e":"#Get Prediction Proba\ny_pred_ann_proba= ann_classifier.predict(X_test)","eaa7a768":"#Convert Prediction to Int\ny_pred_ann= (y_pred_ann_proba>.5).astype('int')","df653e82":"#Priint Classification Report\nprint(classification_report(y_test,y_pred_ann))","e5013792":"#Confusion Matrix\nmat_ann = confusion_matrix(y_test, y_pred_ann)\nsns.heatmap(mat_ann.T, square=True, annot=True, fmt='d', cbar=False,\n          xticklabels=['No','Yes'],\n          yticklabels=['No','Yes'] )\nplt.xlabel('true label')\nplt.ylabel('predicted label')","fc65200b":"#Roc Curve\nfpr_ann,tpr_ann,_ann=roc_curve(y_test,y_pred_ann_proba)\nroc_auc=auc(fpr_ann,tpr_ann)\n#Now Draw ROC using fpr , tpr\nplt.plot([0, 1], [0, 1], 'k--',label='Random')\nplt.plot(fpr,tpr,label='ROC curve (area = %0.2f)' %roc_auc)\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc='best')","257608a5":"## SVM Model\n\nNow will run SVM model and compare","dfc806ad":"Another Observation that TotalCharges is continues variables and comes as object. Will convert to numeric format","b1789da3":"We need to use this list in next tuning of the model\n\nFinally will draw ROC curve for the model","e050103e":"# Applying ML Models\n\n## Random Forest Model\n\nWill start by train Random forest model using default parameters and all variables and get initial results","f3ac1100":"# Objectives\n\nI am trying to build a churn model using three ML algorithms (Random Forest, SVM , and ANN). I am building as initial trial all models with default parameters without any tuning or Cross Validation. Will Check which is the best model for next steps of tuning\n","934714b5":"SVM results is a little better than Random Forest. But not a huge improvement\n\nFinally will draw ROC Curve for this model\n","7f549857":"## Categorical Variables Encoding\n\nFor logistics variables(2 classes) will encode using Label Encoder , For Variables has more than 2 classes will use get_dummies function ","bc3e7466":"# Data Analysis and Visualizations\n\nFirst will analyze continues variables against Churn variable","c858ef3e":"We can see a real impact of all continues variables on Churn specially Tenue(Loyal Customers Stay)\n\nWill Check now Box Plot for more explorations","27c066cf":"## ANN Model\n\nLastly, will build Artificial Neural Network (ANN) Model which theoretically should bring best result.\nWill build 2 Hidden Layers with 12 Nodes , Using Variate Function , and Output layer with one Node using Sigmoid Function. Will not run Cross Validation as a first run and will use Adam as optimizer with 100 epochs\n","90981f3b":"There are some variables has value 'No Internet Service' that equivalent to 'No'. Will merge both values","323096f0":"We can see a big improvement with ANN comparing with other models.\n\n## Next Step\n\nI will review deeply all variables, and start tune ANN models for better results\n\n\nI hope this Kernel is useful. Happy to receive your comments, questions, and advises\n\n","34d46899":"We observe SeniorCitizen Should be categorical variables, but comes as int64. Will convert it back to categorical","bc7c44a4":"Now will check all categorical variables levels","48612729":"Now it is more clear the impact of Continues Variables on Churn , We can see minimal impact of Gender\n\nNow will convert to check regarding Categorical Variables","e0222047":"- We cannot see a real Impact of gender\n- Seniors are less loyalty\n- Partners are more loyal\n- Dependents are more loyal\n- Customers does not have multiplelines are more loyal\n- Customer are not happy with Optical Fiber and Leaving with rate of other internet services\n- Customers with month-to-month contract are more willing to leave than people with contracts\n- Paperless customers are more willing to leave that paper billing\n- Customer pay using electronic check is more willing to leave\n\nI Can conclude that mostly customers are suffering from the services , and specially advances customers who are using paperless billing and electronic payment. Some variables has no real impact of Churn but as a first trial for the model i will include all variables, should remove variables in the tuning phase","2787eec5":"Result are not bad as a start. Recall, and Precision of Churn='Yes' is not that good. We need to check features importance for the next tuning","3a856620":"Variables Looks good now and we are ready for data splitting and scaling\n\n# Data Scaling and Splitting"}}