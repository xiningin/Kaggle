{"cell_type":{"8b366b45":"code","8ba64bea":"code","fefbeadc":"code","0313bc57":"code","7eb29ee7":"code","c64ac799":"code","e0f8681d":"code","99cab229":"code","5c961160":"code","0f1b9ac4":"code","0d167c47":"code","9cd1c81b":"code","325a2e5e":"code","c135fe1e":"code","2b7d9cd0":"code","26fdd04c":"code","f1f1de21":"code","18282981":"code","0ed0e684":"code","f106672f":"code","ffe3fd4a":"code","3fdef08f":"code","cba64ab1":"code","d2aa477f":"code","8682b357":"markdown","255d055b":"markdown","3869a9d6":"markdown","4701f601":"markdown","bb0cfa5f":"markdown","8285f5f0":"markdown","b3c18066":"markdown","480de533":"markdown","43ee69d0":"markdown","be7cecad":"markdown","5e3ffdf1":"markdown","10dc2191":"markdown","9da63d77":"markdown","6200397f":"markdown","16a4aead":"markdown","dc404c54":"markdown","32ff6419":"markdown"},"source":{"8b366b45":"!pip install --upgrade tf-agents\n#!pip install  tf-agents\n!pip install GPUtil\n!pip install gym[atari]\n!pip install pyvirtualdisplay\nfrom tensorflow import keras\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport os,sys,humanize,psutil,GPUtil\n\nprint(\"CPU RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ))\nGPUs = GPUtil.getGPUs()\nfor i, gpu in enumerate(GPUs):\n    print('GPU {:d} ... Mem Free: {:.0f}MB \/ {:.0f}MB | Utilization {:3.0f}%'.format(i, gpu.memoryFree, gpu.memoryTotal, gpu.memoryUtil*100))\n\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\nmpl.rc('animation', html='jshtml')\n\ndef update_scene(num, frames, patch):\n    patch.set_data(frames[num])\n    return patch,\n\ndef plot_animation(frames, repeat=False, interval=40):\n    fig = plt.figure()\n    patch = plt.imshow(frames[0])\n    plt.axis('off')\n    anim = animation.FuncAnimation(\n        fig, update_scene, fargs=(frames, patch),\n        frames=len(frames), repeat=repeat, interval=interval)\n    plt.close()\n    return anim\nkeras.backend.clear_session()\n","8ba64bea":"from tf_agents.environments import  suite_gym\nfrom tf_agents.environments.atari_preprocessing import AtariPreprocessing\nfrom tf_agents.environments.atari_wrappers import FrameStack4","fefbeadc":"#Carregando - River raid\nenv = suite_gym.load(environment_name=\"RiverraidNoFrameskip-v4\",max_episode_steps=27000, gym_env_wrappers=[AtariPreprocessing,FrameStack4])","0313bc57":"env.reset()\nimg = env.render(mode=\"rgb_array\")\nplt.figure(figsize=(4, 6))\nplt.imshow(img)\nplt.axis(\"off\")\nplt.show()","7eb29ee7":"print('A\u00e7\u00f5es dispon\u00edveis:\\n{}\\r\\n'.format(env.gym.get_action_meanings()))\nprint('Observa\u00e7\u00e3o:\\n{}'.format(env.observation_spec()))","c64ac799":"#Aqui est\u00e1 a lista de wrappers dispon\u00edveis:\nimport tf_agents.environments.wrappers\n\nfor name in dir(tf_agents.environments.wrappers):\n    obj = getattr(tf_agents.environments.wrappers, name)\n    if hasattr(obj, \"__base__\") and issubclass(obj, tf_agents.environments.wrappers.PyEnvironmentBaseWrapper):\n        print(\"{:27s} {}\".format(name, obj.__doc__.split(\"\\n\")[0]))","e0f8681d":"#Vamos executar alguns comandos para ver o que est\u00e1 acontecendo dentro do enviroment.\nenv.reset()\ntime_step = env.step(np.array(1)) # FIRE\ntime_step = env.step(np.array(3)) # RIGHT\ntime_step = env.step(np.array(8)) # DOWNRIGHT\n\nobservation = time_step.observation.astype(np.float32)\n#Como existem apenas 3 canais de cores, voc\u00ea n\u00e3o pode exibir 4 frames.\nimage = observation[..., :3]\nimage = np.clip(image \/ 150, 0, 1)\nplt.imshow(image)\nplt.axis(\"off\")","99cab229":"from tf_agents.environments.tf_py_environment import TFPyEnvironment\ntf_env = TFPyEnvironment(env)","5c961160":"from tf_agents.networks.q_network import QNetwork\n#converter as observa\u00e7\u00f5es em float 32, os normalizando.. (valores  0.0 a 1.0) \npreprocessing_layer = keras.layers.Lambda( lambda obs: tf.cast(obs, np.float32) \/ 255.)\n\n#Tr\u00eas redes neurais convolucionais simples, 64 filtros, seus respectiveis kernels e strides.\n#conv_layer_params=[(32, (8, 8), 4), (64, (4, 4), 2), (64, (3, 3), 1)]\nconv_layer_params=[(32, (8, 8), 4) , (64, (4, 4), 2)]#, (64, (3, 3), 1), (1024, (7, 7), 1)]\n#Uma dense layer com 512 por uma cama de sair de 4 unidades\nfc_layer_params=(1024,)\n\nq_network = QNetwork(tf_env.observation_spec(), tf_env.action_spec()\n                     ,preprocessing_layers=preprocessing_layer\n                     ,conv_layer_params=conv_layer_params\n                     ,fc_layer_params=fc_layer_params)\nq_network.summary","0f1b9ac4":"from tf_agents.agents.dqn.dqn_agent import DqnAgent\n\ntrain_step = tf.Variable(0)\nupdate_period = 4 \n#optimizer = keras.optimizers.Adam(lr=2.5e-4, rho=0.95, momentum=0.1,epsilon=0.00001, centered=True)\noptimizer = tf.compat.v1.train.RMSPropOptimizer(learning_rate=2.5e-4, decay=0.95, momentum=0.0,\n                                     epsilon=0.00001, centered=True)\n#optimizer = keras.optimizers.Adam(lr=0.00005,epsilon=0.00001)\nepsilon_fn = keras.optimizers.schedules.PolynomialDecay(\n    initial_learning_rate=0.8,\n    decay_steps=250000 \/\/ update_period, \n    end_learning_rate=0.01)\n\nagent = DqnAgent(tf_env.time_step_spec(),\n                 tf_env.action_spec(),\n                 q_network=q_network,\n                 optimizer=optimizer,\n                 target_update_period=2000, \n                 #A fun\u00e7\u00e3o de perda  deve retornar um erro por instancia, por isso definimos  reduction=\"none\" \n                 td_errors_loss_fn=keras.losses.Huber(reduction=\"none\"),\n                 gamma=0.89, # discount factor\n                 train_step_counter=train_step,\n                 epsilon_greedy=lambda: epsilon_fn(train_step))\nagent.initialize()","0d167c47":"from tf_agents.replay_buffers import tf_uniform_replay_buffer\n\n#data_spec \u00e9 a especifica\u00e7\u00e3o dos dados que ser\u00e3o salvos no buffer. \n#batch_size \u00e8 o numero de trajeroias que devem ser adicionadas a cada etapa.\n#max_length \u00e9 o tamanho m\u00e1ximo de reprodu\u00e7\u00e3o. (Paper DQN2015: cuidado com o devorador de RAM)\n\nreplay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer( data_spec=agent.collect_data_spec,    batch_size=tf_env.batch_size, max_length=300000)\n\nreplay_buffer_observer = replay_buffer.add_batch","9cd1c81b":"from tf_agents.metrics import tf_metrics\nfrom tf_agents.eval.metric_utils import log_metrics\nimport logging\ntrain_metrics = [\n    tf_metrics.NumberOfEpisodes(),\n    tf_metrics.EnvironmentSteps(),\n    tf_metrics.AverageReturnMetric(),\n    tf_metrics.AverageEpisodeLengthMetric(),\n]\nlogging.getLogger().setLevel(logging.INFO)\nlog_metrics(train_metrics)","325a2e5e":"from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n\ncollect_driver = DynamicStepDriver(\n    tf_env,\n    agent.collect_policy,\n    observers=[replay_buffer_observer] + train_metrics,\n    num_steps=update_period) \ncollect_driver","c135fe1e":"from tf_agents.policies.random_tf_policy import RandomTFPolicy\n\nclass ShowProgress:\n    def __init__(self, total):\n        self.counter = 0\n        self.total = total\n    def __call__(self, trajectory):\n        if not trajectory.is_boundary():\n            self.counter += 1\n        if self.counter % 100 == 0:\n            print(\"\\r{}\/{}\".format(self.counter, self.total), end=\"\")\n\ninitial_collect_policy = RandomTFPolicy(tf_env.time_step_spec(),\n                                        tf_env.action_spec())\ninit_driver = DynamicStepDriver(\n    tf_env,\n    initial_collect_policy,\n    observers=[replay_buffer.add_batch, ShowProgress(20000)],\n    num_steps=20000)\nfinal_time_step, final_policy_state = init_driver.run()","2b7d9cd0":"#Exemplo de trajetoria final de um episodio\ntrajectories, buffer_info = replay_buffer.get_next(sample_batch_size=2, num_steps=17)\ntrajectories, buffer_info, trajectories._fields","26fdd04c":"from tf_agents.trajectories.trajectory import to_transition\n\ntime_steps, action_steps, next_time_steps = to_transition(trajectories)\ntime_steps.observation.shape,trajectories.step_type.numpy()\n\n\nplt.figure(figsize=(10, 6.8))\nfor row in range(2):\n    for col in range(3):\n        plt.subplot(2, 3, row * 3 + col + 1)\n        obs = trajectories.observation[row, col].numpy().astype(np.float32)\n        img = obs[..., :3]\n        current_frame_delta = np.maximum(obs[..., 3] - obs[..., :3].mean(axis=-1), 0.)\n        img[..., 0] += current_frame_delta\n        img[..., 2] += current_frame_delta\n        img = np.clip(img \/ 150, 0, 1)\n        plt.imshow(img)\n        plt.axis(\"off\")\nplt.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0, wspace=0.02)\nplt.show()","f1f1de21":"dataset = replay_buffer.as_dataset(\n    sample_batch_size=64,\n    num_steps=17,\n    num_parallel_calls=3\n).prefetch(3)\niterator = iter(dataset)\ntrajectories, buffer_info = next(iterator)\nplt.figure(figsize=(10, 6.8))\nfor row in range(2):\n    for col in range(3):\n        plt.subplot(2, 3, row * 3 + col + 1)\n        obs = trajectories.observation[row, col].numpy().astype(np.float32)\n        img = obs[..., :3]\n        current_frame_delta = np.maximum(obs[..., 3] - obs[..., :3].mean(axis=-1), 0.)\n        img[..., 0] += current_frame_delta\n        img[..., 2] += current_frame_delta\n        img = np.clip(img \/ 150, 0, 1)\n        plt.imshow(img)\n        plt.axis(\"off\")\nplt.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0, wspace=0.02)\nplt.show()\n","18282981":"dataset = replay_buffer.as_dataset(\n    sample_batch_size=64,\n    num_steps=2,\n    num_parallel_calls=3).prefetch(3)","0ed0e684":"#Converter fun\u00e7\u00f5es.\nfrom tf_agents.utils.common import function\n\ncollect_driver.run = function(collect_driver.run)\nagent.train = function(agent.train)","f106672f":"\ndef train_agent(n_iterations):\n    time_step = None\n    policy_state = agent.collect_policy.get_initial_state(tf_env.batch_size)\n    iterator = iter(dataset)\n    for iteration in range(n_iterations):\n        time_step, policy_state = collect_driver.run(time_step, policy_state)\n        trajectories, buffer_info = next(iterator)\n        train_loss = agent.train(trajectories)\n        print(\"\\r{} loss: {:.5f}\".format(\n            iteration, train_loss.loss.numpy()), end=\"\")\n        if iteration % 1000 == 0:\n            log_metrics(train_metrics)","ffe3fd4a":"# o valor ideal para n_iterations  \u00e9 1.000.000 \ntrain_agent(n_iterations=900000)","3fdef08f":"frames = []\ndef save_frames(trajectory):\n    global frames\n    frames.append(tf_env.pyenv.envs[0].render(mode=\"rgb_array\"))\n\nprev_lives = tf_env.pyenv.envs[0].ale.lives()\ndef reset_and_fire_on_life_lost(trajectory):\n    global prev_lives\n    lives = tf_env.pyenv.envs[0].ale.lives()\n    if prev_lives != lives:\n        tf_env.reset()\n        tf_env.pyenv.envs[0].step(np.array(tf_env.pyenv.envs[0].action_space.sample()))\n        prev_lives = lives\n\nwatch_driver = DynamicStepDriver(\n    tf_env,\n    agent.policy,\n    observers=[save_frames, reset_and_fire_on_life_lost, ShowProgress(10000)],\n    num_steps=10000)\nfinal_time_step, final_policy_state = watch_driver.run()\n\nplot_animation(frames)\n","cba64ab1":"#Criando um Gif\nimport PIL\n\nimage_path = os.path.join(\"view1.gif\")\nframe_images = [PIL.Image.fromarray(frame) for frame in frames[:150]]\nframe_images[0].save(image_path, format='GIF',\n                     append_images=frame_images[1:],\n                     save_all=True,\n                     duration=300,\n                     loop=0)","d2aa477f":"%%html\n<img src=\"view1.gif\" \/>","8682b357":"# Replay Buffer and the Corresponding Observer\nO TF-Agents libary disponibiliza algumas implementa\u00e7\u00f5es de buffer de reprodu\u00e7\u00e3o no pacote tf_agents.replay_buffers.\n\n**max_length:** Valor ideia \u00e9 1000000\n","255d055b":"# Visualization","3869a9d6":"# Collect Driver\nUm driver \u00e9 um objseto que explora um enviroment usando policy, ele colecta as experiencias de cada etapa de treinamento e as transmite aos observadores.","4701f601":"# DQN Agent\n[DQN paper ](https:\/\/web.stanford.edu\/class\/psych209\/Readings\/MnihEtAlHassibis15NatureControlDeepRL.pdf)  ","bb0cfa5f":"Para agruparmoss o ambiente, usamos o TFPyEnviroment","8285f5f0":"# Training Loop","b3c18066":"# Reinforcement Learning \/ Tensorflow - TF_Agents \n\nO Aprendizado por Refor\u00e7o (RL) \u00e9 um dos campos do Aprendizado de M\u00e1quina  mais antigos. Existe desde a d\u00e9cada de 1950, produzindo muitas aplica\u00e7\u00f5es interessantes ao longo dos anos. \n<br \/>\n<img src='https:\/\/es.mathworks.com\/help\/\/\/reinforcement-learning\/ug\/reinforcement_learning_diagram.png' width='300' \/>\n\n*\"O aprendizado por refor\u00e7o difere do aprendizado supervisionado por n\u00e3o precisar de apresenta\u00e7\u00e3o de pares de entrada \/ sa\u00edda rotulados e por n\u00e3o precisar que a\u00e7\u00f5es sub\u00f3timas sejam explicitamente corrigidas. Em vez disso, o foco est\u00e1 em encontrar um equil\u00edbrio entre explora\u00e7\u00e3o (de territ\u00f3rio desconhecido) e explora\u00e7\u00e3o (de conhecimento atual).\"* [wikipedia](https:\/\/en.wikipedia.org\/wiki\/Reinforcement_learning)\n\n","480de533":"# Environment Wrappers and Atari Preprocessing\n\nO TF-Agents disponibliza um wapper para gym para executarmos jogos do atary.\nEle usa por tra\u00e1s o ambiente do OpenAI Gym.\nPara enviroment Atari, algums padr\u00f5es de pr\u00e9-processamento devem ser aplicados por tanto, TF-Agents fornece o wrapper AtariPreprocessing.\nO FrameStack4 retorna pilhas de quadro a quadro.","43ee69d0":"<img src=\"https:\/\/i.pinimg.com\/originals\/41\/7d\/33\/417d33d3f0aa7be41e9495f845930209.gif\" \/>","be7cecad":"Exibindo ambiente","5e3ffdf1":"# Referencies\n[Hands\u2013On Machine Learning with Scikit\u2013Learn and TensorFlow 2](https:\/\/www.amazon.com.br\/Hands-Machine-Learning-Scikit-Learn-TensorFlow\/dp\/1492032646)\n\n[Agents is a library for reinforcement learning in TensorFlow.\n](https:\/\/www.tensorflow.org\/agents)\n\n[Introduction to TF-Agents : A library for Reinforcement Learning in TensorFlow](https:\/\/towardsdatascience.com\/introduction-to-tf-agents-a-library-for-reinforcement-learning-in-tensorflow-68ab9add6ad6)\n\n[Train a Deep Q Network with TF-Agents](https:\/\/www.tensorflow.org\/agents\/tutorials\/1_dqn_tutorial)\n\n\n\n","10dc2191":"# Environment Wrappers\n\n","9da63d77":"# Training Metrics\nUsando as v\u00e1rias metricas do pacote  tf_agents.metrics.\n\n","6200397f":"# Environment Specifications\nO TF-Agents fornces as especifica\u00e7\u00f5es das observa\u00e7\u00f5es, a\u00e7\u00f5es e etapas incluindo seus respectivos shapes.","16a4aead":"# DQN\nO TF-Agents, fornece alguns pacotes de redes.\nNesse pacote, as imagens s\u00e3o armazenadas usando bytes de 0 a 255 para usar menos RAM.\n\nO QNetwork usa um observation_spec como entrada e gera um Q-Value de a\u00e7\u00e3o.\n\n","dc404c54":"# Setup...\n**Aprendendo a jogar River Raid** <br \/>\n<img src='https:\/\/s2.glbimg.com\/bQhuS5w10e3MAFxzNtnBm_jJNVA=\/695x0\/s.glbimg.com\/po\/tt2\/f\/original\/2016\/02\/26\/river-raid-atari-2600-8.jpg' width='300' \/>\n<br \/>\n*\"River Raid foi eleito o melhor jogo do ano por diversas revistas. Em 1983 pela InfoWorld, foi chamado de o \u201cvideojogo\u201d mais desafiador.[2] Em 1984, a revista The Desert News pontuou: \u201cO mais jog\u00e1vel e divertido game de guerra\u201d.[3] Neste mesmo ano, o jogo recebeu o pr\u00eamio de \"melhor game de a\u00e7\u00e3o do ano\"1984\"[4]:42 e um certificado de m\u00e9rito na categoria \"1984 Best Computer Action Game\" no 5o Arkie Awards\"* [Wikipedia](https:\/\/pt.wikipedia.org\/wiki\/River_Raid)","32ff6419":"# Dataset\nconverter os dados do buffer em um dataset para nosso treinamento"}}