{"cell_type":{"b82375f9":"code","a150e48c":"code","290dd641":"code","aca6295a":"code","c3715682":"code","423b291a":"code","8abc3e9c":"code","ad4b2717":"code","f542b4f5":"code","0c32ca7c":"code","3e883483":"code","1f295bf9":"code","ea658c68":"code","634b22ce":"code","d8f4fa18":"code","eef07d9a":"code","38f8d3f5":"code","f3359ddd":"code","2abb09bc":"code","4b417cc4":"code","eb8a7f7b":"code","97a79df8":"code","b2f4d4de":"markdown","6ca88f3e":"markdown","eb9f1fcb":"markdown","adb807d2":"markdown","309d3cd5":"markdown","b1b59ac8":"markdown","41e66d44":"markdown"},"source":{"b82375f9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a150e48c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\nfrom textblob import TextBlob\nfrom  sklearn.feature_extraction.text import CountVectorizer\nimport pdb \nfrom nltk.stem import *\nfrom nltk.corpus import stopwords\nimport re\nimport string\nfrom collections import Counter\nfrom gensim import matutils, models\nimport scipy.sparse\nfrom sklearn.feature_extraction import text \nfrom wordcloud import WordCloud\n","290dd641":"data = pd.read_csv(\"..\/input\/kuc-hackathon-winter-2018\/drugsComTest_raw.csv\")\ndata0 = data.copy() #Duplicate the data\ndata0.head(10)","aca6295a":"data0.shape","c3715682":"def firstCleaning(data): \n    \n    \"\"\"\n    creates a dataframe with a summarised version of the data \n    by combining ALL the reviews for ALL customers for A \n    particular drugName under A particular medical condition.\n                 \n                 \n\n    Parameters\n    ----------\n    data : dataframe\n    \n    output>>dataframe[drugName,condition,chunk(combination of reviews),usefulTo,averageRating]\n    -------\n    \n    \"\"\"\n    variable = list(data.condition.unique())\n    #dummydata = np.zeros((len(variable),))\n    conditionList =[]\n    drugnameList=[]\n    chunkList=[]\n    ratingList=[]\n    usefulCountList=[]\n    #iterating through a condition\n    for i in range(len(variable)):        \n        conditionVariable= data0.loc[data0['condition']==variable[i]]\n        #iterating through each drug in the condition\n        for t in list(conditionVariable.drugName.unique()):\n            drugname = conditionVariable.loc[conditionVariable['drugName']==t]\n            rating = data.rating[(data.drugName==t) & (data.condition ==variable[i])].mean()\n            usefulcount = data.usefulCount[(data.drugName==t) & (data.condition ==variable[i])].sum()\n            #appending each review of each drug under one condition into one big chunk of text\n            chunk = ''.join(drugname.review)\n            #Storing my data into a list and then a dictionary for my dataframe\n            conditionList.append(variable[i])\n            drugnameList.append(t)\n            chunkList.append(chunk)\n            ratingList.append(round(rating,2))\n            usefulCountList.append(usefulcount)\n    final_dict ={\"Condition\":conditionList,\"drugName\":drugnameList,\"chunk\":chunkList,\"averageRating\":ratingList,\"usefulTo\":usefulCountList}\n    final = pd.DataFrame(final_dict)\n    return(final)","423b291a":"def secondCleaning(data):\n    \n    \"\"\"\n    \n\n    Parameters\n    ----------\n    data : works on the chunk aspect of the dataframe and \n          converts all text to lower cases, removes string \n          formatting and all punctuation stored\n          input>>dataframe\n    Returns\n    -------\n    output>>series[chunk]\n    Note: this should be applied on the data\n\n    \"\"\"\n    clean = data.chunk\n    clean = clean.str.lower()\n    ##pdb.set_trace()\n    clean = clean.str.replace('\\[.,*?\\]', '')\n    clean = clean.str.replace('\\w*\\d\\w*', '')\n    clean = clean.str.replace('[%s]' % re.escape(string.punctuation), '')\n    clean = clean.str.replace('\\n','')\n    clean = clean.str.replace('[\u2018\u2019\u201c\u201d\u2026]', '')\n    return clean","8abc3e9c":"#Just stemmer or lemmatizer is good for the job i implemented both to confirm if there was any significant difference between both so any should do just fine \n\nstemmer = PorterStemmer()    \ndef word_stemmer(text):\n    \"\"\" \n    Parameters\n    ----------\n    text : built on the stemmer function to cut words into short \n             forms which is not necceasarily an english word \n             input>>series[chunk]\n             \n\n    Returns\n    -------\n    stem_text : output>>series[chunk] less words taken out\n\n    \"\"\"\n    stem_text = \"\".join([stemmer.stem(i) for i in text])\n    return stem_text \n\nlemmatizer = WordNetLemmatizer()\ndef word_lemmatizer(text):\n    \"\"\"\n    \n\n    Parameters\n    ----------\n    text : built on the lemmatizer function to cut words into short \n             forms which is an english word but has tradeoff which involves \n             computational expenses\n             input>>series[chunk]\n             \n\n    Returns\n    -------\n    lem_text : series[chunk] less words taken out\n\n    \"\"\"\n    lem_text = \"\".join([lemmatizer.lemmatize(i) for i in text])\n    return lem_text\n","ad4b2717":"\ndef remove_stopwords(text):\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words\n","f542b4f5":"def finalCleaning(data,stopword):\n    \"\"\"\n    \n\n    Parameters\n    ----------\n    data : this converts the chunk data from the corpus \n    format to the document matrix format(spreading\n    out the words into columns across the observation)\n    input>>dataframe\n                \n    \n\n    Returns\n    -------\n    data_dtm : dataframe[each word as a column](with multiIndexing)\n\n        DESCRIPTION.\n\n    \"\"\"\n    cv = CountVectorizer(stop_words=stopword)\n    #temp=[\"0\"] * len(data.chunk)\n    \n    #for i in range(len(data.chunk)):\n    data_cv = cv.fit_transform(data.chunk)\n        #temp[i]= data_cv\n        \n    data_dtm =pd.DataFrame(data_cv.toarray(),columns=cv.get_feature_names())\n    data_info = data[['Condition','drugName']]\n    #data_merged = pd.concat([data_info,test],axis=1)\n    \n    data_dtm.index = pd.MultiIndex.from_frame(data_info)\n    return data_dtm\n","0c32ca7c":"#Applying and visualizing the first cleaned data\ncombined_data = firstCleaning(data0)\ncombined_data.head(10)\n","3e883483":"#bottom 10 rows\ncombined_data.tail(10)","1f295bf9":"#found a wrongly inputed condition and took it out -  671 entries removed\ncombined_data = combined_data[~combined_data['Condition'].str.contains(\"users found this\")] \ncombined_data = combined_data[~combined_data['Condition'].str.contains(\"Not Listed \/ Othe\")] \n#quick look at the data\ncombined_data.head()\ncombined_data.tail(10)","ea658c68":"combined_data.chunk= secondCleaning(combined_data)\n#both lemmatizer and stemmer had almost same impact on cleaning\n#process so running either of any would do just fine\ncombined_data.chunk = combined_data.chunk.apply(lambda x: word_stemmer(x))\n#combined_data.chunk = combined_data.chunk.apply(lambda x: word_lemmatizer(x))\n","634b22ce":"#it adds new_stop_words(gotten from looking at the data) to stop words list\nnew_stop_words =['im','like', 'just','ive', 'forwardyesterday', 'says','mei'\n ,'forwardthis', 'forwas','forwards', 'forwhat',\n 'forwhen', 'forwardmy','forwhile','did', 'took','wont','uti',\n'forwish','forworked','forwardmethadone', '\u0456t','didnt','let','aa','aaa','aaaaa',\n'zzzzzzzzthe','zzz','zzzzz', 'zzzzzzzzif','aaaaand', 'aaaaargmy', 'aaaand','aaahhi', 'aaand',\n 'aaddthis', 'aafter', 'aai', 'aam', 'aamp', 'aampb', 'aampd',\n 'aampe', 'aampee','aana', 'aap','aampeon', 'zzzzap','aampethe','abacavirlamivudinenevirapine',\n 'aaps','aarp','aatd','ab','abbvie','taking']\nstop_words = text.ENGLISH_STOP_WORDS.union(new_stop_words)\ntest = finalCleaning(combined_data,stop_words) \ntest = test.transpose()","d8f4fa18":"\n#######################################\n#########Exploratory Analysis##########\n#######################################\n\ndef topwords(data,n):\n    \"\"\"\n    \n\n    Parameters\n    ----------\n    data : the document based matrix of the chunk data\n    n : int\n       Number of words to retrieve \n\n    Returns\n    -------\n    top_dict : list \n        top words in the document based metrix per drugName.\n\n    \"\"\"\n    top_dict = {}\n    for c in data.columns:\n        top = data[c].sort_values(ascending=False).head(n)\n        top_dict[c]= list(zip(top.index,top.values))\n\n    return top_dict\n\n","eef07d9a":"top_word = topwords(test,30)\n\n#printing top 15 words for  (condition,drugname)\ni =0\nfor drugName, top_words in top_word.items():\n    if i !=14: #to visualize a hand few of the result \n        print(drugName)\n        print(', '.join([word for word, count in top_words[0:14]]))\n        print('---')\n        i+=1\n#this would be easily visualized using  word cloud later on\n    \n","38f8d3f5":"# pull out the top 30 words for each condition\nwords = []\n\nfor Condition in test.columns:\n    top = [word for (word, count) in top_word[drugName]]\n    \n    for t in top:\n        words.append(t)\n\n","f3359ddd":"#This displays the number of unique rows (condition and drugname)  where each word was used.\nCounter(words).most_common()\n\n[(word,count) for word, count in Counter(words).most_common()]\n\n#So i can easily tell that skin,condition,dose and nauseous was used in relation to 5200 medication.\n","2abb09bc":"#Wordcloud can be used to visualize how customer perceive the effectiveness\n#of a medication for each medication\nwc = WordCloud(stopwords=stop_words,\n                       background_color='white',\n                       colormap='Dark2',\n                       max_font_size=300,\n                       max_words=30,\n                       random_state=42)","4b417cc4":"#You can change the condition and drugname on the next cell and run the \n#next cell(cell 21) to see the word cloud run this code to see the condition and drugname \ncombined_data[['Condition','drugName']]\n#Use this to randomly get a condition and drugname\nsearch =combined_data.loc[np.random.randint(0,combined_data.shape[0]),['Condition','drugName']]\n","eb8a7f7b":"#Please the result from the last cell can be inputed here\n#sorry for the lazy programming lol\ncondition =search[0]\ndrugname=search[1]","97a79df8":"print(\"Report on {0} for {1}\".format(drugname,condition))\nwcdata = combined_data.chunk[(combined_data.Condition == condition) & (combined_data.drugName==drugname)]\nwc_topword= top_word[(condition,drugname)]\nwc.generate(str(wc_topword))\nwc.to_file('wc.jpeg')\nplt.show\nplt.show('wc.jpeg')\nimg = mpimg.imread('wc.jpeg')\nimgplot = plt.imshow(img)\nplt.show()\nwc_topword= top_word[(condition,drugname)]\nwc.generate(str(wc_topword))\n\n\n'''\nYou can easily see a high correlation between  (Number of people \nthat found it useful and average rating) and positivity or negativity of \nwords found in the word cloud\n'''\n#Note that max rating is a 10.\nrealdata = data[['drugName', 'condition','rating','usefulCount']]\nrealdata = realdata[data.condition == condition]\n\nalldrugs = combined_data[(combined_data.Condition == condition)]\nrequesteddrug = combined_data[(combined_data.Condition ==condition) & (combined_data.drugName == drugname)]\nrelateddrugs =  combined_data[(combined_data.Condition ==condition) & (combined_data.drugName != drugname)]\nmax_average_rating = list(alldrugs.drugName[alldrugs.averageRating == alldrugs.averageRating.max()])[0]\nmax_usefulTo =list(alldrugs.drugName[alldrugs.usefulTo == alldrugs.usefulTo.max()])[0]\n#col1 = (alldrugs[\"usefulTo\"]\/100).astype(str)\n#col2 = alldrugs[\"averageRating\"].astype(str)\nprint(\"Average Rating of  : {0}\\n\".format(list(requesteddrug.averageRating))+'\\n\\n'+\n\"Found Useful To(persons) : {0}\\n\".format(list(requesteddrug.usefulTo))+'\\n\\n'+\n\"List of other drugs also for {1}: {0}  \\n\".format(list(relateddrugs.drugName.unique()),condition)+'\\n\\n'+\n\"Best medication  based on Average rating : {0}\\n\".format(max_average_rating)+'\\n\\n'+\n\"best Medication by number of person(s) that found it useful: {0}\\n\".format(max_usefulTo))","b2f4d4de":"exploring the data to see how much cleaning is needed before work starts proper and showcasing the first 10 records","6ca88f3e":"wordcreator('ADHD','Dexmethylphenidate')","eb9f1fcb":"print(subtitle[1])\nprint(message[0])\nprint(message[1])        \nif (relateddrugs.empty !=True):\n    print(\"--------------------------------------------------\")\n    print(subtitle[2])\n    print(\"--------------------------------------------------\")\n    print(message[2])\n    print(\"--------------------------------------------------\")\n    print(subtitle[3])\n    print(\"--------------------------------------------------\")\n    unique_related_drug = list(relateddrugs.drugName.unique())        \n    for drug in unique_related_drug:\n        t= top_word[(condition,drug)]\n        wc.generate(str(t))\n        wc.to_file('fig.jpeg')\n        plt.imshow('fig.jpeg')\n        print(\"figure: \"+drug)","adb807d2":"**Data Preprocessing**:\n1.  As can be observed above, each review is a seperate row and so the idea is to create a corpus of data unique per medication and condition. For example all 100 reviews for the condition \"headaches\" where \"aspirin\" was used would be grouped into one row containing the big chunk of review, average of all ratings and total number to have found it useful.","309d3cd5":"Here we come to an end or rather a new begining of sharing my world of NLP  (FYI: IN the nearest future i would be updating this visualization aspect of this code with better TSNE plots to give more insight to the data). \n\nI do hope you find the research fascinating and should incase you have more questions feel free to reach out. \ud83d\ude43\ud83d\ude01","b1b59ac8":"Importing all neccesary libraries for project\n","41e66d44":"    \"\"\"This was infused into my main project while i was  submitting and it was basically meant to compile my information and do comparison on all the medications for a condition all into a word document, feel free to explore\"\"\"\ndef wordcreator(issue,medication):\n    try:\n        wc = WordCloud(stopwords=stop_words,\n                       background_color='white',\n                       colormap='Dark2',\n                       max_font_size=300,\n                       max_words=30,\n                       random_state=42)\n        condition =issue\n        drugname =medication\n        #original dataset that has not been summarised\n        realdata = data[['drugName', 'condition','rating','usefulCount']]\n        realdata = realdata[data.condition == condition]\n        ratingplot = sns.boxplot(x='drugName',y='rating',hue='drugName',data=realdata).set_title('Boxplot of Medications')\n        \n        alldrugs = combined_data[(combined_data.Condition == condition)]\n        requesteddrug = combined_data[(combined_data.Condition ==condition) & (combined_data.drugName == drugname)]\n        relateddrugs =  combined_data[(combined_data.Condition ==condition) & (combined_data.drugName != drugname)]\n        max_average_rating = list(alldrugs.drugName[alldrugs.averageRating == alldrugs.averageRating.max()])[0]\n        max_usefulTo =list(alldrugs.drugName[alldrugs.usefulTo == alldrugs.usefulTo.max()])[0]\n        col1 = (alldrugs[\"usefulTo\"]\/100).astype(str)\n        col2 = alldrugs[\"averageRating\"].astype(str)\n        \n        alldrugs['Rank'] = (col2+col1).rank(method='dense', ascending=False).astype(int)\n        sorted_rank =alldrugs.sort_values('Rank')\n        sorted_rank = sorted_rank[['drugName','averageRating','usefulTo','Rank']]\n        alldrugs.drop(['Rank'],axis=1)\n        \n        \n                \n        title = \"Report on {0} for {1}\".format(drugname,condition)\n        subtitle =['Word Cloud on {0}'.format(drugname),\n                   'Statistics Summary on {0}'.format(drugname),\n                   'Related Medication Others Used for {0}'.format(condition),\n                   'Word Cloud for other related Medications for {0}'.format(condition),\n                   'Summary Statistics of other related medications',\n                   'Best Medication for {0} according to reviews'.format(condition)]\n        message = [\"Average Rating of  : {0}\".format(list(requesteddrug.averageRating)), \n                   \"Found Useful To(persons) : {0}\".format(list(requesteddrug.usefulTo)),\n                   \"List of other drugs: {0}\".format(list(relateddrugs.drugName.unique())),\n                   #\"Best Medication for {0} is {1}\".format(condition,sorted_rank[0]),\n                   \"Best medication  based on Average rating : {0}\".format(max_average_rating),\n                   \"best Medication by number of person(s) that found it useful: {0}\".format(max_usefulTo)]\n        print(\"**************************************************\")\n        print(title )\n        print(\"**************************************************\")\n        print(subtitle[0])\n        print(\"--------------------------------------------------\")\n        wcdata = combined_data.chunk[(combined_data.Condition == condition) & (combined_data.drugName==drugname)]\n        wc_topword= top_word[(condition,drugname)]\n        wc.generate(str(wc_topword))\n        wc.to_file('wc.jpeg')\n        plt.show\n        plt.show('wc.jpeg')\n        img = mpimg.imread('wc.jpeg')\n        imgplot = plt.imshow(img)\n        plt.show()\n        print(\"--------------------------------------------------\")\n        print(subtitle[1])\n        print(\"--------------------------------------------------\")\n        print(message[0])\n        print(message[1])        \n        if (relateddrugs.empty !=True):\n            print(\"--------------------------------------------------\")\n            print(subtitle[2])\n            print(\"--------------------------------------------------\")\n            print(message[2])\n            print(\"--------------------------------------------------\")\n            print(subtitle[3])\n            print(\"--------------------------------------------------\")\n            unique_related_drug = list(relateddrugs.drugName.unique())        \n            for drug in unique_related_drug:\n                t= top_word[(condition,drug)]\n                wc.generate(str(t))\n                wc.to_file('fig.jpeg')\n                plt.imshow('fig.jpeg')\n                print(\"figure: \"+drug)\n        \n            print(\"--------------------------------------------------\")\n            print(subtitle[4])\n            print(\"--------------------------------------------------\")\n            print(ratingplot)\n            print(\"--------------------------------------------------\")            \n            print(subtitle[5])\n            print(\"--------------------------------------------------\")\n            print(message[3])\n            print(message[4])\n            \n                   \n    except:\n        print(\"Something went wrong, {0} or {1} were not found in the dataset\".format(condition,drugname))\n        \n"}}