{"cell_type":{"c520783a":"code","522bccb1":"code","cdd84f12":"code","44da32c6":"code","b3d04c3e":"code","235eae08":"code","d3233d8e":"code","d8244a21":"code","fe9a808a":"code","cc4cc248":"code","f11cae92":"code","c23c301f":"code","d88c9f4c":"code","8a0b5d6f":"code","bd84d392":"code","26f8ce71":"code","0552531c":"code","0089b1aa":"code","7c67b6b6":"code","5d4384b8":"code","bcf9d49e":"code","14791c09":"code","9309e0bb":"code","83a150a8":"code","34134ba2":"code","5f2fe1c8":"code","e6fcb3a0":"code","47f3c723":"code","fd073d19":"code","358a27e4":"code","62df99f8":"code","a433dbc9":"code","5c739e17":"code","7de8a271":"code","e33e123a":"code","e641689b":"code","2de6fb79":"code","938c39e3":"code","4459cd3a":"code","9ba7ae49":"code","ebc51802":"code","e954d666":"code","90312fc3":"code","fbccfa13":"code","be8afa45":"markdown","410c4361":"markdown","bd7534c4":"markdown","ea6f3814":"markdown","4536a666":"markdown","a9549153":"markdown","32ff897c":"markdown","7fceaec6":"markdown","5b05cc2c":"markdown","bc8a3ce0":"markdown","6f5c3e72":"markdown","61b967bd":"markdown","e2e04ff8":"markdown"},"source":{"c520783a":"!git clone https:\/\/github.com\/siddharthchaini\/gpvae-raw.git","522bccb1":"!mv -v .\/gpvae-raw\/* .\/","cdd84f12":"!rm -rf gpvae-raw\n!rm gpvaeraw.ipynb","44da32c6":"# !wget https:\/\/www.dropbox.com\/s\/651d86winb4cy9n\/physionet.npz?dl=1 -O physionet.npz\n\n!wget https:\/\/raw.githubusercontent.com\/siddharthchaini\/centering-perfect-sims-resspect\/master\/np_arrays\/objlist.npy -q\n!wget https:\/\/raw.githubusercontent.com\/siddharthchaini\/centering-perfect-sims-resspect\/master\/np_arrays\/y.npy -q\n!wget https:\/\/raw.githubusercontent.com\/siddharthchaini\/centering-perfect-sims-resspect\/master\/np_arrays\/X.npy -q\n!wget https:\/\/raw.githubusercontent.com\/siddharthchaini\/centering-perfect-sims-resspect\/master\/np_arrays\/X_miss_asplasticc.npy -q\n!wget https:\/\/raw.githubusercontent.com\/siddharthchaini\/centering-perfect-sims-resspect\/master\/np_arrays\/X_miss_50.npy -q","b3d04c3e":"import sys\nimport os\nimport time\nfrom datetime import datetime\nimport numpy as np\nimport matplotlib\nmatplotlib.use(\"Agg\")\nfrom matplotlib import pyplot as plt\nimport tensorflow as tf\n\ntf.compat.v1.enable_eager_execution()\n\nfrom sklearn.metrics import average_precision_score, roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","235eae08":"from lib.models import *","d3233d8e":"latent_dim = 5 # 'Dimensionality of the latent space'\nencoder_sizes = [128, 128] # 'Layer sizes of the encoder'\ndecoder_sizes = [256, 256] # 'Layer sizes of the decoder'\nwindow_size = 24 # 'Window size for the inference CNN: Ignored if model_type is not gp-vae'\nsigma = 1.005 # 'Sigma value for the GP prior: Ignored if model_type is not gp-vae'\nlength_scale = 7.0 # 'Length scale value for the GP prior: Ignored if model_type is not gp-vae'\nbeta = 0.2 # 'Factor to weigh the KL term (similar to beta-VAE'\nnum_epochs = 40 # 'Number of training epochs'\n\n# Flags with common default values for all three datasets\nlearning_rate = 1e-3 # 'Learning rate for training'\ngradient_clip = 1e4 # 'Maximum global gradient norm for the gradient clipping during training'\nnum_steps = 0 # 'Number of training steps: If non-zero it overwrites num_epochs'\nprint_interval = 0 # 'Interval for printing the loss and saving the model during training'\nexp_name = \"reproduce_physionet\" # 'Name of the experiment'\nbasedir = \"models\" # 'Directory where the models should be stored'\ndata_dir = \"\" # 'Directory from where the data should be read in'\ndata_type = 'physionet' # ['hmnist', 'physionet', 'sprites'], 'Type of data to be trained on'\nseed = 1337 # 'Seed for the random number generator'\nmodel_type = 'gp-vae' # ['vae', 'hi-vae', 'gp-vae'], 'Type of model to be trained'\ncnn_kernel_size = 3 # 'Kernel size for the CNN preprocessor'\ncnn_sizes = [256] # 'Number of filters for the layers of the CNN preprocessor'\ntesting = True # 'Use the actual test set for testing'\nbanded_covar = True # 'Use a banded covariance matrix instead of a diagonal one for the output of the inference network: Ignored if model_type is not gp-vae'\nbatch_size = 64 # 'Batch size for training'\n\nM = 1 # 'Number of samples for ELBO estimation'\nK = 1 # 'Number of importance sampling weights'\n\nkernel = 'cauchy' # ['rbf', 'diffusion', 'matern', 'cauchy'], 'Kernel to be used for the GP prior: Ignored if model_type is not (mgp-vae'\nkernel_scales = 1 # 'Number of different length scales sigma for the GP prior: Ignored if model_type is not gp-vae'","d8244a21":"np.random.seed(seed)\ntf.compat.v1.set_random_seed(seed)\nprint(\"Testing: \", testing, f\"\\t Seed: {seed}\")","fe9a808a":"encoder_sizes = [int(size) for size in encoder_sizes]\ndecoder_sizes = [int(size) for size in decoder_sizes]\n\nif 0 in encoder_sizes:\n    encoder_sizes.remove(0)\nif 0 in decoder_sizes:\n    decoder_sizes.remove(0)","cc4cc248":"# Make up full exp name\ntimestamp = datetime.now().strftime(\"%y%m%d\")\nfull_exp_name = \"{}_{}\".format(timestamp, exp_name)\noutdir = os.path.join(basedir, full_exp_name)\nif not os.path.exists(outdir): os.mkdir(outdir)\ncheckpoint_prefix = os.path.join(outdir, \"ckpt\")\nprint(\"Full exp name: \", full_exp_name)","f11cae92":"data_type","c23c301f":"if data_type == \"hmnist\":\n    data_dir = \"data\/hmnist\/hmnist_mnar.npz\"\n    data_dim = 784\n    time_length = 10\n    num_classes = 10\n    decoder = BernoulliDecoder\n    img_shape = (28, 28, 1)\n    val_split = 50000\nelif data_type == \"physionet\":\n#     if data_dir == \"\":\n#         data_dir = \"physionet.npz\"\n#     data_dim = 35\n#     time_length = 48\n#     num_classes = 2\n#     decoder = GaussianDecoder\n    data_dim = 200 # CHANGED\n    time_length = 6 # CHANGED\n    num_classes = 5\n    decoder = GaussianDecoder\nelif data_type == \"sprites\":\n    if data_dir == \"\":\n        data_dir = \"data\/sprites\/sprites.npz\"\n    data_dim = 12288\n    time_length = 8\n    decoder = GaussianDecoder\n    img_shape = (64, 64, 3)\n    val_split = 8000\nelse:\n    raise ValueError(\"Data type must be one of ['hmnist', 'physionet', 'sprites']\")","d88c9f4c":"# data = np.load(data_dir)","8a0b5d6f":"import pandas as pd\nx_train_full = np.load(\".\/X.npy\").astype(\"float32\")\nx_train_miss = np.load(\".\/X_miss_asplasticc.npy\").astype(\"float32\") # Try .\/X_miss_50.npy.npy\n\nx_train_full = np.moveaxis(x_train_full, -1, 1) # CHANGED\nx_train_miss = np.moveaxis(x_train_miss, -1, 1) # CHANGED\n\nm_train_miss = (x_train_miss == 0).astype(\"float32\")\ny = np.load(\".\/y.npy\")\ny, label_strings = pd.factorize(y,sort=True)\ny = tf.keras.utils.to_categorical(y)\nprint(label_strings)\nobjlist = np.load(\".\/objlist.npy\")","bd84d392":"from sklearn.model_selection import train_test_split\n# Train Test Split\nzipX = list(zip(x_train_full, x_train_miss, m_train_miss))\nzipy = list(zip(y, objlist))\n\nzipX_train, zipX_val, zipy_train, zipy_val = train_test_split(zipX, zipy,\n                                                              test_size = 0.1,\n                                                              random_state=1337)\n\nx_train_full, x_train_miss, m_train_miss = zip(*zipX_train)\nx_val_full, x_val_miss, m_val_miss = zip(*zipX_val)\n\ny_train, objlist_train = zip(*zipy_train)\ny_val, objlist_val = zip(*zipy_val)\n\nx_train_full = np.array(x_train_full).astype(\"float32\")\nx_train_miss = np.array(x_train_miss).astype(\"float32\")\nm_train_miss = np.array(m_train_miss).astype(\"float32\")\nm_train_artificial = m_train_miss.copy()\ny_train = np.array(y_train).astype(\"float32\")\nobjlist_train = np.array(objlist_train).astype(\"float32\")\n\n\nx_val_full = np.array(x_val_full).astype(\"float32\")\nx_val_miss = np.array(x_val_miss).astype(\"float32\")\nm_val_miss = np.array(m_val_miss).astype(\"float32\")\nm_val_artificial = m_val_miss.copy()\ny_val = np.array(y_val).astype(\"float32\")\nobjlist_val = np.array(objlist_val).astype(\"float32\")","26f8ce71":"# if data_type in ['hmnist', 'physionet']:\n#     y_train = data['y_train']","0552531c":"# if testing:\n#     if data_type in ['hmnist', 'sprites']:\n#         x_val_full = data['x_test_full']\n#         x_val_miss = data['x_test_miss']\n#         m_val_miss = data['m_test_miss']\n#     if data_type == 'hmnist':\n#         y_val = data['y_test']\n#     elif data_type == 'physionet':\n#         x_val_full = data['x_train_full']\n#         x_val_miss = data['x_train_miss']\n#         m_val_miss = data['m_train_miss']\n#         y_val = data['y_train']\n#         m_val_artificial = data[\"m_train_artificial\"]\n# elif data_type in ['hmnist', 'sprites']:\n#     x_val_full = x_train_full[val_split:]\n#     x_val_miss = x_train_miss[val_split:]\n#     m_val_miss = m_train_miss[val_split:]\n#     if data_type == 'hmnist':\n#         y_val = y_train[val_split:]\n#     x_train_full = x_train_full[:val_split]\n#     x_train_miss = x_train_miss[:val_split]\n#     m_train_miss = m_train_miss[:val_split]\n#     y_train = y_train[:val_split]\n# elif data_type == 'physionet':\n#     x_val_full = data[\"x_val_full\"]  # full for artificial missings\n#     x_val_miss = data[\"x_val_miss\"]\n#     m_val_miss = data[\"m_val_miss\"]\n#     m_val_artificial = data[\"m_val_artificial\"]\n#     y_val = data[\"y_val\"]\n# else:\n#     raise ValueError(\"Data type must be one of ['hmnist', 'physionet', 'sprites']\")","0089b1aa":"tf_x_train_miss = tf.data.Dataset.from_tensor_slices((x_train_miss, m_train_miss))\\\n                                 .shuffle(len(x_train_miss)).batch(batch_size).repeat()\ntf_x_val_miss = tf.data.Dataset.from_tensor_slices((x_val_miss, m_val_miss)).batch(batch_size).repeat()\ntf_x_val_miss = tf.compat.v1.data.make_one_shot_iterator(tf_x_val_miss)","7c67b6b6":"# Build Conv2D preprocessor for image data\nif data_type in ['hmnist', 'sprites']:\n    print(\"Using CNN preprocessor\")\n    image_preprocessor = ImagePreprocessor(img_shape, cnn_sizes, cnn_kernel_size)\nelif data_type == 'physionet':\n    image_preprocessor = None\nelse:\n    raise ValueError(\"Data type must be one of ['hmnist', 'physionet', 'sprites']\")","5d4384b8":"if model_type == \"vae\":\n    model = VAE(latent_dim=latent_dim, data_dim=data_dim, time_length=time_length,\n                encoder_sizes=encoder_sizes, encoder=DiagonalEncoder,\n                decoder_sizes=decoder_sizes, decoder=decoder,\n                image_preprocessor=image_preprocessor, window_size=window_size,\n                beta=beta, M=M, K=K)\nelif model_type == \"hi-vae\":\n    model = HI_VAE(latent_dim=latent_dim, data_dim=data_dim, time_length=time_length,\n                   encoder_sizes=encoder_sizes, encoder=DiagonalEncoder,\n                   decoder_sizes=decoder_sizes, decoder=decoder,\n                   image_preprocessor=image_preprocessor, window_size=window_size,\n                   beta=beta, M=M, K=K)\nelif model_type == \"gp-vae\":\n    encoder = BandedJointEncoder if banded_covar else JointEncoder\n    model = GP_VAE(latent_dim=latent_dim, data_dim=data_dim, time_length=time_length,\n                   encoder_sizes=encoder_sizes, encoder=encoder,\n                   decoder_sizes=decoder_sizes, decoder=decoder,\n                   kernel=kernel, sigma=sigma,\n                   length_scale=length_scale, kernel_scales = kernel_scales,\n                   image_preprocessor=image_preprocessor, window_size=window_size,\n                   beta=beta, M=M, K=K, data_type=data_type)\nelse:\n    raise ValueError(\"Model type must be one of ['vae', 'hi-vae', 'gp-vae']\")","bcf9d49e":"print(\"GPU support: \", tf.test.is_gpu_available())","14791c09":"print(\"Training...\")\n_ = tf.compat.v1.train.get_or_create_global_step()\ntrainable_vars = model.get_trainable_vars()\noptimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n\nprint(\"Encoder: \", model.encoder.net.summary())\nprint(\"Decoder: \", model.decoder.net.summary())","9309e0bb":"if model.preprocessor is not None:\n    print(\"Preprocessor: \", model.preprocessor.net.summary())\n    saver = tf.compat.v1.train.Checkpoint(optimizer=optimizer, encoder=model.encoder.net,\n                                          decoder=model.decoder.net, preprocessor=model.preprocessor.net,\n                                          optimizer_step=tf.compat.v1.train.get_or_create_global_step())\nelse:\n    saver = tf.compat.v1.train.Checkpoint(optimizer=optimizer, encoder=model.encoder.net, decoder=model.decoder.net,\n                                          optimizer_step=tf.compat.v1.train.get_or_create_global_step())","83a150a8":"summary_writer = tf.compat.v2.summary.create_file_writer(logdir=outdir, flush_millis=10000)","34134ba2":"if num_steps == 0:\n    num_steps = num_epochs * len(x_train_miss) \/\/ batch_size\nelse:\n    num_steps = num_steps","5f2fe1c8":"if print_interval == 0:\n    print_interval = num_steps \/\/ num_epochs","e6fcb3a0":"losses_train = []\nlosses_val = []","47f3c723":"t0 = time.time()","fd073d19":"with summary_writer.as_default(), tf.compat.v2.summary.record_if(True):\n    for i, (x_seq, m_seq) in enumerate(tf_x_train_miss.take(num_steps)):\n        try:\n            with tf.GradientTape() as tape:\n                tape.watch(trainable_vars)\n                loss = model.compute_loss(x_seq, m_mask=m_seq)\n                losses_train.append(loss.numpy())\n            grads = tape.gradient(loss, trainable_vars)\n            grads = [np.nan_to_num(grad) for grad in grads]\n            grads, global_norm = tf.clip_by_global_norm(grads, gradient_clip)\n            optimizer.apply_gradients(zip(grads, trainable_vars),\n                                      global_step=tf.compat.v1.train.get_or_create_global_step())\n\n            # Print intermediate results\n            if i % print_interval == 0:\n                print(\"================================================\")\n                print(\"Learning rate: {} | Global gradient norm: {:.2f}\".format(optimizer._lr, global_norm))\n                print(\"Step {}) Time = {:2f}\".format(i, time.time() - t0))\n                loss, nll, kl = model.compute_loss(x_seq, m_mask=m_seq, return_parts=True)\n                print(\"Train loss = {:.3f} | NLL = {:.3f} | KL = {:.3f}\".format(loss, nll, kl))\n\n                saver.save(checkpoint_prefix)\n                tf.compat.v2.summary.scalar(name=\"loss_train\", data=loss, step=tf.compat.v1.train.get_or_create_global_step())\n                tf.compat.v2.summary.scalar(name=\"kl_train\", data=kl, step=tf.compat.v1.train.get_or_create_global_step())\n                tf.compat.v2.summary.scalar(name=\"nll_train\", data=nll, step=tf.compat.v1.train.get_or_create_global_step())\n\n                # Validation loss\n                x_val_batch, m_val_batch = tf_x_val_miss.get_next()\n                val_loss, val_nll, val_kl = model.compute_loss(x_val_batch, m_mask=m_val_batch, return_parts=True)\n                losses_val.append(val_loss.numpy())\n                print(\"Validation loss = {:.3f} | NLL = {:.3f} | KL = {:.3f}\".format(val_loss, val_nll, val_kl))\n\n                tf.compat.v2.summary.scalar(name=\"loss_val\", data=val_loss, step=tf.compat.v1.train.get_or_create_global_step())\n                tf.compat.v2.summary.scalar(name=\"kl_val\", data=val_kl, step=tf.compat.v1.train.get_or_create_global_step())\n                tf.compat.v2.summary.scalar(name=\"nll_val\", data=val_nll, step=tf.compat.v1.train.get_or_create_global_step())\n\n                if data_type in [\"hmnist\", \"sprites\"]:\n                    # Draw reconstructed images\n                    x_hat = model.decode(model.encode(x_seq).sample()).mean()\n                    tf.compat.v2.summary.image(name=\"input_train\", data=tf.reshape(x_seq, [-1]+list(img_shape)), step=tf.compat.v1.train.get_or_create_global_step())\n                    tf.compat.v2.summary.image(name=\"reconstruction_train\", data=tf.reshape(x_hat, [-1]+list(img_shape)), step=tf.compat.v1.train.get_or_create_global_step())\n                elif data_type == 'physionet':\n                    # Eval MSE and AUROC on entire val set\n                    x_val_miss_batches = np.array_split(x_val_miss, batch_size, axis=0)\n                    x_val_full_batches = np.array_split(x_val_full, batch_size, axis=0)\n                    m_val_artificial_batches = np.array_split(m_val_artificial, batch_size, axis=0)\n                    get_val_batches = lambda: zip(x_val_miss_batches, x_val_full_batches, m_val_artificial_batches)\n\n                    n_missings = m_val_artificial.sum()\n                    mse_miss = np.sum([model.compute_mse(x, y=y, m_mask=m).numpy()\n                                       for x, y, m in get_val_batches()]) \/ n_missings\n\n                    x_val_imputed = np.vstack([model.decode(model.encode(x_batch).mean()).mean().numpy()\n                                               for x_batch in x_val_miss_batches])\n                    x_val_imputed[m_val_miss == 0] = x_val_miss[m_val_miss == 0]  # impute gt observed values\n\n                    x_val_imputed = x_val_imputed.reshape([-1, time_length * data_dim])\n                    val_split = len(x_val_imputed) \/\/ 2 # WHY!?\n                    cls_model = LogisticRegression(solver='liblinear', tol=1e-10, max_iter=10000)\n                    cls_model.fit(x_val_imputed[:val_split], y_val.argmax(axis=1)[:val_split])\n                    probs = cls_model.predict_proba(x_val_imputed[val_split:])[:, 1]\n                    auroc = roc_auc_score(y_val[val_split:], probs.reshape(-1,1),multi_class='ovo')\n                    print(\"MSE miss: {:.4f} | AUROC: {:.4f}\".format(mse_miss, auroc))\n\n                    # Update learning rate (used only for physionet with decay=0.5)\n                    if i > 0 and i % (10*print_interval) == 0:\n                        optimizer._lr = max(0.5 * optimizer._lr, 0.1 * learning_rate)\n                t0 = time.time()\n        except KeyboardInterrupt as e:\n            print(\"KeyboardInterrupt\")\n            saver.save(checkpoint_prefix)\n#             if debug:\n#                 import ipdb\n#                 ipdb.set_trace()\n            break\n","358a27e4":"# Split data on batches\nx_val_miss_batches = np.array_split(x_val_miss, batch_size, axis=0)\nx_val_full_batches = np.array_split(x_val_full, batch_size, axis=0)","62df99f8":"if data_type == 'physionet':\n    m_val_batches = np.array_split(m_val_artificial, batch_size, axis=0)\nelse:\n    m_val_batches = np.array_split(m_val_miss, batch_size, axis=0)","a433dbc9":"get_val_batches = lambda: zip(x_val_miss_batches, x_val_full_batches, m_val_batches)","5c739e17":"# Compute NLL and MSE on missing values\nn_missings = m_val_artificial.sum() if data_type == 'physionet' else m_val_miss.sum()\nnll_miss = np.sum([model.compute_nll(x, y=y, m_mask=m).numpy()\n                   for x, y, m in get_val_batches()]) \/ n_missings\nmse_miss = np.sum([model.compute_mse(x, y=y, m_mask=m, binary=data_type==\"hmnist\").numpy()\n                   for x, y, m in get_val_batches()]) \/ n_missings","7de8a271":"print(\"NLL miss: {:.4f}\".format(nll_miss))\nprint(\"MSE miss: {:.4f}\".format(mse_miss))","e33e123a":"# Save imputed values\nz_mean = [model.encode(x_batch).mean().numpy() for x_batch in x_val_miss_batches]\nnp.save(os.path.join(outdir, \"z_mean\"), np.vstack(z_mean))\nx_val_imputed = np.vstack([model.decode(z_batch).mean().numpy() for z_batch in z_mean])\nnp.save(os.path.join(outdir, \"imputed_no_gt\"), x_val_imputed)","e641689b":"# impute gt observed values\nx_val_imputed[m_val_miss == 0] = x_val_miss[m_val_miss == 0]\nnp.save(os.path.join(outdir, \"imputed\"), x_val_imputed)","2de6fb79":"if data_type == \"hmnist\":\n    # AUROC evaluation using Logistic Regression\n    x_val_imputed = np.round(x_val_imputed)\n    x_val_imputed = x_val_imputed.reshape([-1, time_length * data_dim])\n\n    cls_model = LogisticRegression(solver='lbfgs', multi_class='multinomial', tol=1e-10, max_iter=10000)\n    val_split = len(x_val_imputed) \/\/ 2\n\n    cls_model.fit(x_val_imputed[:val_split], y_val[:val_split])\n    probs = cls_model.predict_proba(x_val_imputed[val_split:])\n\n    auprc = average_precision_score(np.eye(num_classes)[y_val[val_split:]], probs)\n    auroc = roc_auc_score(np.eye(num_classes)[y_val[val_split:]], probs)\n    print(\"AUROC: {:.4f}\".format(auroc))\n    print(\"AUPRC: {:.4f}\".format(auprc))\n\nelif data_type == \"sprites\":\n    auroc, auprc = 0, 0\n\nelif data_type == \"physionet\":\n    # Uncomment to preserve some z_samples and their reconstructions\n    # for i in range(5):\n    #     z_sample = [model.encode(x_batch).sample().numpy() for x_batch in x_val_miss_batches]\n    #     np.save(os.path.join(outdir, \"z_sample_{}\".format(i)), np.vstack(z_sample))\n    #     x_val_imputed_sample = np.vstack([model.decode(z_batch).mean().numpy() for z_batch in z_sample])\n    #     np.save(os.path.join(outdir, \"imputed_sample_{}_no_gt\".format(i)), x_val_imputed_sample)\n    #     x_val_imputed_sample[m_val_miss == 0] = x_val_miss[m_val_miss == 0]\n    #     np.save(os.path.join(outdir, \"imputed_sample_{}\".format(i)), x_val_imputed_sample)\n\n    # AUROC evaluation using Logistic Regression\n    x_val_imputed = x_val_imputed.reshape([-1, time_length * data_dim])\n    val_split = len(x_val_imputed) \/\/ 2\n    cls_model = LogisticRegression(solver='liblinear', tol=1e-10, max_iter=10000)\n    cls_model.fit(x_val_imputed[:val_split], y_val.argmax(axis=1)[:val_split])\n    probs = cls_model.predict_proba(x_val_imputed[val_split:])[:, 1]\n    auprc = average_precision_score(y_val[val_split:], probs.reshape(-1,1))\n    auroc = roc_auc_score(y_val[val_split:], probs.reshape(-1,1),multi_class='ovo')\n\n    print(\"AUROC: {:.4f}\".format(auroc))\n    print(\"AUPRC: {:.4f}\".format(auprc))","938c39e3":"# Visualize reconstructions\nif data_type in [\"hmnist\", \"sprites\"]:\n    img_index = 0\n    if data_type == \"hmnist\":\n        img_shape = (28, 28)\n        cmap = \"gray\"\n    elif data_type == \"sprites\":\n        img_shape = (64, 64, 3)\n        cmap = None\n\n    fig, axes = plt.subplots(nrows=3, ncols=x_val_miss.shape[1], figsize=(2*x_val_miss.shape[1], 6))\n\n    x_hat = model.decode(model.encode(x_val_miss[img_index: img_index+1]).mean()).mean().numpy()\n    seqs = [x_val_miss[img_index:img_index+1], x_hat, x_val_full[img_index:img_index+1]]\n\n    for axs, seq in zip(axes, seqs):\n        for ax, img in zip(axs, seq[0]):\n            ax.imshow(img.reshape(img_shape), cmap=cmap)\n            ax.axis('off')\n\n    suptitle = model_type + f\" reconstruction, NLL missing = {mse_miss}\"\n    fig.suptitle(suptitle, size=18)\n    fig.savefig(os.path.join(outdir, data_type + \"_reconstruction.pdf\"))\n\nresults_all = [seed, model_type, data_type, kernel, beta, latent_dim,\n               num_epochs, batch_size, learning_rate, window_size,\n               kernel_scales, sigma, length_scale,\n               len(encoder_sizes), encoder_sizes[0] if len(encoder_sizes) > 0 else 0,\n               len(decoder_sizes), decoder_sizes[0] if len(decoder_sizes) > 0 else 0,\n               cnn_kernel_size, cnn_sizes,\n               nll_miss, mse_miss, losses_train[-1], losses_val[-1], auprc, auroc, testing, data_dir]\n\nwith open(os.path.join(outdir, \"results.tsv\"), \"w\") as outfile:\n    outfile.write(\"seed\\tmodel\\tdata\\tkernel\\tbeta\\tz_size\\tnum_epochs\"\n                  \"\\tbatch_size\\tlearning_rate\\twindow_size\\tkernel_scales\\t\"\n                  \"sigma\\tlength_scale\\tencoder_depth\\tencoder_width\\t\"\n                  \"decoder_depth\\tdecoder_width\\tcnn_kernel_size\\t\"\n                  \"cnn_sizes\\tNLL\\tMSE\\tlast_train_loss\\tlast_val_loss\\tAUPRC\\tAUROC\\ttesting\\tdata_dir\\n\")\n    outfile.write(\"\\t\".join(map(str, results_all)))\n\nwith open(os.path.join(outdir, \"training_curve.tsv\"), \"w\") as outfile:\n    outfile.write(\"\\t\".join(map(str, losses_train)))\n    outfile.write(\"\\n\")\n    outfile.write(\"\\t\".join(map(str, losses_val)))\n\nprint(\"Training finished.\")","4459cd3a":"z = model.encode(x_val_miss).mean().numpy()","9ba7ae49":"print(z.shape)","ebc51802":"np.save(\"z.npy\",z)\nnp.save(\"objlist_val.npy\",objlist_val)\nnp.save(\"y_val.npy\",y_val)","e954d666":"rfX = z.reshape(z.shape[0],-1)\nrfy = label_strings[y_val.argmax(1)]","90312fc3":"def get_metrics(y_pred, y_test, to_print=True):\n    correct_labels = np.where(y_pred==y_test)[0]\n    accuracy = metrics.accuracy_score(y_test, y_pred)\n    precision = metrics.precision_score(y_test, y_pred,average='macro')\n    recall = metrics.recall_score(y_test, y_pred,average='macro')\n    f1score = metrics.f1_score(y_test, y_pred,average='macro')\n    # rocscore = metrics.roc_auc_score(y_test, y_pred,average='micro',multi_class=\"ovo\")\n    confusion_matrix = metrics.confusion_matrix(y_test, y_pred)  \n    classification_report = metrics.classification_report(y_test, y_pred)\n\n    if to_print:\n        print(\"Identified {} correct labels out of {} labels\".format(len(correct_labels), y_test.shape[0]))\n        print(\"Accuracy:\",accuracy)\n        print(\"Precision:\",precision)\n        print(\"Recall:\",recall)\n        print(\"F1 Score:\",f1score)\n        # print(\"ROC AUC Score:\",rocscore)\n        print(\"Confusion Matrix:\\n\", confusion_matrix)\n        print(\"Classification_Report:\\n\", classification_report)","fbccfa13":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn import metrics\nrfc = RandomForestClassifier(n_estimators=100,random_state=42)\npreds = cross_val_predict(rfc, rfX, rfy, cv=10)\nget_metrics(preds,rfy)","be8afa45":"### Evaluation","410c4361":"### Define data specific parameters","bd7534c4":"### Training","ea6f3814":"### Training preparation","4536a666":"### Flags","a9549153":"### Download data","32ff897c":"### Check RFC on Latent Space","7fceaec6":"### Prep","5b05cc2c":"### Load data","bc8a3ce0":"- https:\/\/www.kaggle.com\/siddharthchaini\/gp-vae-intro\n- https:\/\/github.com\/siddharthchaini\/GP-VAE\/blob\/master\/train.py","6f5c3e72":"### Build Model","61b967bd":"### Imports","e2e04ff8":"## Kaggle stuff"}}