{"cell_type":{"e3ccada1":"code","31e06e00":"code","8fd6857c":"code","25a334e9":"code","3c45e3e1":"code","c4834cd1":"code","6131ad4c":"code","a6ba1038":"code","f96ebd03":"code","bc3490bc":"code","284d40c9":"code","a64141e5":"code","ecd01466":"code","dd833666":"code","024a34c8":"code","84273420":"code","6d9ab95a":"code","6ca218d9":"code","3b087305":"code","ebba4b65":"markdown","ec8dfc16":"markdown","d86f7cb0":"markdown","73992b43":"markdown","c139361e":"markdown","6d76c801":"markdown","75e85afc":"markdown","eb065192":"markdown","af223296":"markdown","57cac63a":"markdown","3e0d84ea":"markdown"},"source":{"e3ccada1":"import numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')","31e06e00":"# Load data\ndf = pd.read_csv('..\/input\/predictingwages\/predicting_wages.csv')","8fd6857c":"# See variables in the dataset\ndf.head()","25a334e9":"# Dimensions of the dataset\ndf.shape","3c45e3e1":"# Table of means of each var\ndf.mean()","c4834cd1":"####################  Linear and Quadratic specifications ##############################\n\n# Wage linear regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nY = df['wage'] # target variable\nX = df[['female' , 'sc', 'cg', 'mw' , 'so' , 'we' , 'exp1' , 'exp2' , 'exp3']] #regressors\nmodel = LinearRegression()\n# Run Linear Specification\nresults = model.fit(X,Y) # train the model\nprint(\"Intercept\",results.intercept_) # beta_0","6131ad4c":"pd.DataFrame(results.coef_.reshape(1,-1),columns=X.columns)","a6ba1038":"# compute MSE and R^2\ny_pred = results.predict(X) # predictions on same data\nprint(\"R-squared\",metrics.r2_score(Y,y_pred)) # r-squared\nMSE_adj2 =  (3835\/(3835-10)) * np.mean(np.square(Y-y_pred))\nprint(\"MSE adj:\",MSE_adj2) #MSE","f96ebd03":"adj_rsquared = 1 - ((1-0.095488)*(3835-1)\/(3835-9-1))\nprint(\"Adj Rsquared:\",adj_rsquared)# adjusted r-squared","bc3490bc":"# Linear regression: Quadratic specification\nfrom sklearn.preprocessing import PolynomialFeatures\nX.drop('female',axis = 1,inplace = True)\npoly = PolynomialFeatures(interaction_only=True,include_bias=False)\nX_poly = poly.fit_transform(X) # creating polynomial features with degree = 2(Quadratic)\nX_poly = pd.DataFrame(X_poly,columns= poly.get_feature_names(X.columns))\nX_poly['female'] = df['female']\nX['female'] = df['female']\nresults = model.fit(X_poly,Y) # train the model\nprint(\"Intercept\",results.intercept_) # beta_0","284d40c9":"pd.DataFrame(results.coef_.reshape(1,-1),columns=X_poly.columns) # printing the co-efficients in a tabular format","a64141e5":"# compute MSE and R^2\ny_pred = results.predict(X_poly)\nprint(\"R-squared\",metrics.r2_score(Y,y_pred))\nMSE_adj2 =  (3835\/(3835-33)) * np.mean(np.square(Y-y_pred))\nprint(\"MSE adj:\",MSE_adj2) #MSE","ecd01466":"adj_rsquared = 1 - ((1-0.10397)*(3835-1)\/(3835-32-1))\nadj_rsquared","dd833666":"####################  Linear and Quadratic specifications with Sample Splitting ##############################\nfrom sklearn.model_selection import train_test_split\n# Line### Basic Modelar specification\nX_train = X.iloc[:1918,:]\nX_test = X.iloc[1918:,:]\nY_train = Y.iloc[:1918]\nY_test = Y.iloc[1918:]\n# X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.5,random_state=101) # splitting data into train and test\nresults = model.fit(X_train,Y_train)\nprint(\"Intercept\",results.intercept_)","024a34c8":"pd.DataFrame(results.coef_.reshape(1,-1),columns=X.columns)","84273420":"y_pred = results.predict(X_test) # predit y values on test data\nprint(\"R-squared\",metrics.r2_score(Y_test,y_pred)) # r-squared\nprint(\"MSE\",metrics.mean_squared_error(Y_test,y_pred))# MSE","6d9ab95a":"# Linear regression: Quadratic specification\nX_train = X_poly.iloc[:1918,:]\nX_test = X_poly.iloc[1918:,:]\nY_train = Y.iloc[:1918]\nY_test = Y.iloc[1918:]\n# X_train,X_test,Y_train,Y_test = train_test_split(X_poly,Y,test_size=0.5,random_state=101)\nresults = model.fit(X_train,Y_train)\nprint(\"Intercept\",results.intercept_)","6ca218d9":"pd.DataFrame(results.coef_.reshape(1,-1),columns=X_poly.columns)","3b087305":"y_pred = results.predict(X_test)\nprint(\"R-squared\",metrics.r2_score(Y_test,y_pred))\nprint(\"MSE\",metrics.mean_squared_error(Y_test,y_pred))","ebba4b65":"|| p | R-squared_test | MSE_test |\n| --- | --- | --- | --- |\n| basic reg | 10 | 0.1027 | 154.584 |\n| flexi reg | 33 | 0.1046 | 154.260 |\n\nHere we report results for one random split of the data in 2 halves, and see that the flexible rule works just slightly better.\n\nNote that these numbers vary across different data splits, so we can average results over several datasplits.\n\nBy looking at results for several splits, we conclude that the basic and flexible model perform about the same.","ec8dfc16":"### Thanks for being till here. Do not forget to drop an upvote if you liked the notebook!","d86f7cb0":"### Basic Model","73992b43":"*The above table shows some descriptive statistics... From the table we see that average wage is about 15 dollars per hour... 42% of workers are women... average experience is 13 years;...38% are college graduates... 32% have done some college work, and 30% hold only high school diploma. You can also see geographical distribution of workers across major geographical regions of the states.*","c139361e":"### Basic Model on splitted data","6d76c801":"Given that p\/n is quite small here, the sample linear regression should approximate the population linear regression quite well.","75e85afc":"|| p | R-squared_sample | R-squared_adj | MSE_adj |\n| --- | --- | --- | --- | --- |\n| basic reg | 10 | 0.0954 | 0.0933 | 165.680 |\n| flexi reg | 33 | 0.1039 | 0.0964 | 165.118 |\n\nWe conclude that the performance of the basic and flexible model are about the same, with the flexible model being just slightly better (slightly higher R2 lower MSE).","eb065192":"### Flexible model on splitted data","af223296":"We considered two predictive models, basic and flexible.\n\nIn the basic model, regressors X consist of the female indicator D and other controls W , which include a constant, experience, experience squared, experience cubed, education and regional indicators. The basic model has 10 regressors in total.\n\nIn the flexible model, regressors consist of all regressors in the basic model PLUS their two-way interactions or products. An example of a regressor created by a 2-way interaction is the experience variable times the indicator of having a college degree; another example is the indicator of having a high-school diploma times the indicator of working in the \u201dnorth-east\u201d region. The flexible model has 33 regressors.","57cac63a":"**PREDICTING WAGES**\n\nIn this segment, we will examine a real example, where we will predict workers\u2019 wages using a linear combination of workers\u2019 characteristics, and we will assess the predictive performance of our prediction rules using the mean squared error and r-squared as well as out-of-sample MSE and r-sqaured.\n\nOur data comes from the March Supplement of the U.S. Current Population Survey, year 2012. We focus on the single (never married) workers with education levels equal to high-school, some college, or college graduates. The sample size is approx 4,000.\n\nOur outcome variable Y is hourly wage, and our X \u2019s are various characteristics of workers such as gender, experience, education, and geographical indicators.\n\nThe dataset contains the following variables:\n\nwage : weekly wage\n\n\nfemale : female dummy\n\ncg : college Graduate Dummy\n\nsc : some college dummy\n\nhsg : High School graduate dummy\n\nmw : midwest dummy\n\nso : south dummy\n\n\nwe : west dummy\n\nne : northeast dummy\n\nexp1 : experience(year)\n\nexp2 : experience squared\n\nexp3 : experinence cubed","3e0d84ea":"### Flexible Model"}}