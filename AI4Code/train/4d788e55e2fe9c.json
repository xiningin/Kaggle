{"cell_type":{"f4dc0e9a":"code","156f57a2":"code","dff52679":"code","d12e4e42":"code","6d58d74d":"code","e8350b1a":"code","cd661135":"code","1bd187c0":"code","7e61e892":"code","cf344bd2":"code","0f5fa2d4":"code","b217a2c5":"code","5ecd01e8":"code","9f058144":"code","bb173123":"code","403f0268":"code","9a0e3cd2":"code","2ccc20c4":"code","b19035ca":"code","c4f94b89":"code","5233f139":"code","14f93028":"markdown","91b33ff9":"markdown","387d3b7d":"markdown","52d41fb0":"markdown","6ba75e10":"markdown","fc65c31d":"markdown","6148f06f":"markdown","4f60f34c":"markdown","5a10f942":"markdown","009d60e1":"markdown"},"source":{"f4dc0e9a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/Ob (e.g. pd.read_csv)\nimport matplotlib.pylab as plt\nimport seaborn as sns\nplt.style.use('ggplot')\ncolor_pal = [x['color'] for x in plt.rcParams['axes.prop_cycle']]","156f57a2":"import time\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom lightgbm import LGBMRegressor\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.simplefilter('ignore', UserWarning)\n\nimport gc\ngc.enable()","dff52679":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-feb-2021\/train.csv')\ntest  = pd.read_csv('\/kaggle\/input\/tabular-playground-series-feb-2021\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/tabular-playground-series-feb-2021\/sample_submission.csv')","d12e4e42":"train.head()","6d58d74d":"categorical_feats = [\n    f for f in train.columns if train[f].dtype == 'object'\n]\n\ncategorical_feats\nfor f_ in categorical_feats:\n    train[f_], _ = pd.factorize(train[f_])\n    # Set feature type as categorical\n    train[f_] = train[f_].astype('category')","e8350b1a":"def get_feature_importances(data, shuffle, seed=None):\n    # Gather real features\n    train_features = [f for f in data if f not in ['target', 'id']]\n    # Go over fold and keep track of CV score (train and valid) and feature importances\n    \n    # Shuffle target if required\n    y = data['target'].copy()\n    if shuffle:\n        # Here you could as well use a binomial distribution\n        y = data['target'].copy().sample(frac=1.0)\n    \n    # Fit LightGBM in RF mode, yes it's quicker than sklearn RandomForest\n    dtrain = lgb.Dataset(data[train_features], y, free_raw_data=False, silent=True)\n    lgb_params = {\n        'objective': 'regression',\n        'boosting_type': 'rf',\n        'subsample': 0.623,\n        'colsample_bytree': 0.7,\n        'num_leaves': 127,\n        'max_depth': 8,\n        'seed': seed,\n        'bagging_freq': 1,\n        'n_jobs': 4\n    }\n    \n    # Fit the model\n    clf = lgb.train(params=lgb_params, train_set=dtrain, num_boost_round=200, categorical_feature=categorical_feats)\n\n    # Get feature importances\n    imp_df = pd.DataFrame()\n    imp_df[\"feature\"] = list(train_features)\n    imp_df[\"importance_gain\"] = clf.feature_importance(importance_type='gain')\n    imp_df[\"importance_split\"] = clf.feature_importance(importance_type='split')    \n    \n    return imp_df","cd661135":"# Seed the unexpected randomness of this world\nnp.random.seed(123)\n# Get the actual importance, i.e. without shuffling\nactual_imp_df = get_feature_importances(data=train, shuffle=False)","1bd187c0":"actual_imp_df.head()","7e61e892":"%%time\nnull_imp_df = pd.DataFrame()\nnb_runs = 80\nimport time\nstart = time.time()\ndsp = ''\nfor i in range(nb_runs):\n    # Get current run importances\n    imp_df = get_feature_importances(data=train, shuffle=True)\n    imp_df['run'] = i + 1 \n    # Concat the latest importances with the old ones\n    null_imp_df = pd.concat([null_imp_df, imp_df], axis=0)\n    # Erase previous message\n    for l in range(len(dsp)):\n        print('\\b', end='', flush=True)\n    # Display current run and time used\n    spent = (time.time() - start) \/ 60\n    dsp = 'Done with %4d of %4d (Spent %5.1f min)' % (i + 1, nb_runs, spent)\n    print(dsp, end='', flush=True)","cf344bd2":"null_imp_df.head()","0f5fa2d4":"def display_distributions(actual_imp_df_, null_imp_df_, feature_):\n    plt.figure(figsize=(13, 6))\n    gs = gridspec.GridSpec(1, 2)\n    # Plot Split importances\n    ax = plt.subplot(gs[0, 0])\n    a = ax.hist(null_imp_df_.loc[null_imp_df_['feature'] == feature_, 'importance_split'].values, label='Null importances')\n    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_['feature'] == feature_, 'importance_split'].mean(), \n               ymin=0, ymax=np.max(a[0]), color='r',linewidth=10, label='Real Target')\n    ax.legend()\n    ax.set_title('Split Importance of %s' % feature_.upper(), fontweight='bold')\n    plt.xlabel('Null Importance (split) Distribution for %s ' % feature_.upper())\n    # Plot Gain importances\n    ax = plt.subplot(gs[0, 1])\n    a = ax.hist(null_imp_df_.loc[null_imp_df_['feature'] == feature_, 'importance_gain'].values, label='Null importances')\n    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_['feature'] == feature_, 'importance_gain'].mean(), \n               ymin=0, ymax=np.max(a[0]), color='r',linewidth=10, label='Real Target')\n    ax.legend()\n    ax.set_title('Gain Importance of %s' % feature_.upper(), fontweight='bold')\n    plt.xlabel('Null Importance (gain) Distribution for %s ' % feature_.upper())","b217a2c5":"train.head(1)","5ecd01e8":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='cat0')","9f058144":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='cat1')","bb173123":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='cat2')","403f0268":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='cat3')","9a0e3cd2":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='cat4')","2ccc20c4":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='cont0')","b19035ca":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='cont1')","c4f94b89":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='cont13')","5233f139":"%%time\nfeature_scores = []\nfor _f in actual_imp_df['feature'].unique():\n    f_null_imps_gain = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_gain'].values\n    f_act_imps_gain = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_gain'].mean()\n    gain_score = np.log(1e-10 + f_act_imps_gain \/ (1 + np.percentile(f_null_imps_gain, 75)))  # Avoid didvide by zero\n    f_null_imps_split = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_split'].values\n    f_act_imps_split = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_split'].mean()\n    split_score = np.log(1e-10 + f_act_imps_split \/ (1 + np.percentile(f_null_imps_split, 75)))  # Avoid didvide by zero\n    feature_scores.append((_f, split_score, gain_score))\n\nscores_df = pd.DataFrame(feature_scores, columns=['feature', 'split_score', 'gain_score'])\n\nplt.figure(figsize=(16, 16))\ngs = gridspec.GridSpec(1, 2)\n# Plot Split importances\nax = plt.subplot(gs[0, 0])\nsns.barplot(x='split_score', y='feature', data=scores_df.sort_values('split_score', ascending=False).iloc[0:70], ax=ax)\nax.set_title('Feature scores wrt split importances', fontweight='bold', fontsize=14)\n# Plot Gain importances\nax = plt.subplot(gs[0, 1])\nsns.barplot(x='gain_score', y='feature', data=scores_df.sort_values('gain_score', ascending=False).iloc[0:70], ax=ax)\nax.set_title('Feature scores wrt gain importances', fontweight='bold', fontsize=14)\nplt.tight_layout()","14f93028":"Null Importance","91b33ff9":"##### Create a scoring function\n- Scoring function uses LightGBM in RandomForest mode fitted on the full dataset","387d3b7d":"### Feature Importances","52d41fb0":"#### Read data","6ba75e10":"### Overview Data","fc65c31d":"#### if you like my kernel please consider upvoting it\n\n##### Remember the upvote button is next to the fork button, and it's free too! ;)","6148f06f":"![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/24673\/logos\/header.png?t=2021-01-02-00-34-25)\n\n\n# Feature selecture using target permutation\n\nFeature selection process using target permutation tests actual importance significance against the distribution of feature importances when fitted to noise (shuffled target). The notebook implements the following steps :\nCreate the null importances distributions : these are created fitting the model over several runs on a shuffled version of the target. This shows how the model can make sense of a feature irrespective of the target.\n\nFit the model on the original target and gather the feature importances. This gives us a benchmark whose significance can be tested against the Null Importances Distribution for each feature test the actual importance:\n\nCompute the probabability of the actual importance wrt the null distribution. I will use a very simple estimation using occurences while the article proposes to fit known distribution to the gathered data. In fact here I'll compute 1 - the proba so that things are in the right order.\n\nSimply compare the actual importance to the mean and max of the null importances. This will give sort of a feature importance that allows to see major features in the dataset. Indeed the previous method may give us lots of ones. Inspired by:\n\nLook: https:\/\/www.kaggle.com\/ogrellier\/feature-selection-with-null-importances (upvote this !) Not only useful but also valuable\n\n\n\nThe notebook uses a procedure described in this article.\n\nFeature selection process using target permutation tests actual importance significance against the distribution of feature importances when fitted to noise (shuffled target).\n\n## The notebook implements the following steps :\n\nCreate the null importances distributions : these are created fitting the model over several runs on a shuffled version of the target. This shows how the model can make sense of a feature irrespective of the target.\n\nFit the model on the original target and gather the feature importances. This gives us a benchmark whose significance can be tested against the Null - Importances Distribution\n\nfor each feature test the actual importance:\n\n- 1 - Compute the probabability of the actual importance wrt the null distribution. I will use a very simple estimation using occurences while the article proposes to fit known distribution to the gathered data. In fact here I'll compute 1 - the proba so that things are in the right order.\n\n- 2 - Simply compare the actual importance to the mean and max of the null importances. This will give sort of a feature importance that allows to see major features in the dataset. Indeed the previous method may give us lots of ones.","4f60f34c":"Actual Importance","5a10f942":"# Display distribution examples\nA few plots are better than any words","009d60e1":"Build Null Importances distribution"}}