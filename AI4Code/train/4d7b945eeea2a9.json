{"cell_type":{"e9638166":"code","82e7b503":"code","59dd14ed":"code","36abc4b6":"code","95a62682":"code","7cab6eb1":"code","fc00d905":"code","25820bd9":"code","3e7e777d":"code","0e9c33d3":"code","8125237f":"code","481f5ed3":"code","b75df702":"code","ec645ae4":"code","92dd9087":"code","57b6b615":"code","23e2c96b":"code","41146216":"code","97e2b31e":"code","17d176d0":"code","4418b0e7":"code","1bc30a7e":"code","9fa16a66":"code","1c06da5a":"code","33672d1d":"code","b22160f5":"code","8f6c5b19":"markdown","60ad812b":"markdown","2d1b075c":"markdown","eec5c9e7":"markdown","7838796c":"markdown","52eba552":"markdown","0763b54e":"markdown","e59d22fd":"markdown","939302c2":"markdown","7f5ff755":"markdown","e8ea429c":"markdown","53c18492":"markdown","f29ac560":"markdown","a53fddbc":"markdown","e799cf74":"markdown"},"source":{"e9638166":"from tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import Input, LSTM, Dense\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras import optimizers\nimport tensorflow as tf\n\nimport pickle\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import shuffle\n\nseed = 0","82e7b503":"!pip install xlrd==1.2","59dd14ed":"df = pd.read_excel('\/kaggle\/input\/air-quality-time-series-data-uci\/AirQualityUCI.xlsx')\ndf = df.rename(columns={'Date': 'date', \n                        'Time': 'time',\n                        'CO(GT)': 'co_gt',\n                        'NMHC(GT)': 'nmhc_gt',\n                        'C6H6(GT)': 'c6h6_gt',\n                        'NOx(GT)': 'nox_gt',\n                        'NO2(GT)': 'no2_gt',\n                        'PT08.S1(CO)': 'co_pt',\n                        'PT08.S2(NMHC)': 'nmhc_pt',\n                        'PT08.S3(NOx)': 'nox_pt',\n                        'PT08.S4(NO2)': 'no2_pt',\n                        'PT08.S5(O3)': 'o3_pt',\n                        'T': 't',\n                        'RH': 'rh',\n                        'AH': 'ah',\n                       })\ndf['full_timestamp'] = (df['date'].astype(str) + ' ' + df['time'].astype(str)).map(pd.Timestamp) \ndf = df.sort_values('full_timestamp', ascending=True).reset_index(drop=True)\ndf","36abc4b6":"print('\u00bfLa diferencia de tiempo entre dos mediciones es siempre de una hora?:', \n      (df.full_timestamp.diff().iloc[1:] == pd.Timedelta(hours=1)).all())\nprint('\u00bfHay valores nulos?', pd.isna(df).any().any())","95a62682":"# Fijamos como \u00edndice la fecha de cada muestra.\ndf = df.set_index('full_timestamp')","7cab6eb1":"# Vamos a ver la proporci\u00f3n de mediciones faltantes de cada variable\ndisplay((df.drop(columns=['date', 'time']) == -200).mean())\n# y la proporci\u00f3n de horas en las que falta al menos una medici\u00f3n\nprint('Proporci\u00f3n de horas en las que falta al menos una medici\u00f3n:',\n      (df.drop(columns=['date', 'time']) == -200).any(axis=1).mean())\nprint('Proporci\u00f3n de horas en las que faltan todas las mediciones:',\n      (df.drop(columns=['date', 'time']) == -200).all(axis=1).mean())","fc00d905":"# Veamos ahora la proporci\u00f3n de horas en las que falta alguna medici\u00f3n distinta de nmhc_gt, \n# ya que es la que menos aparece:\nprint('Proporci\u00f3n de horas en las que falta al menos una medici\u00f3n (no nmhc_gt):',\n      (df.drop(columns=['date', 'time', 'nmhc_gt']) == -200).any(axis=1).mean())","25820bd9":"# Para facilitar el filtrado de los valores faltantes vamos a sustituirlos por nan.\ndf = df.replace(to_replace=-200, value=np.nan)","3e7e777d":"for col in df.columns.drop(['date', 'time']):\n    fig, ax = plt.subplots(figsize=(10, 10))\n    df[df.notna()][col].plot(linewidth=1, label=col)\n    plt.scatter(df[df.isna()].index, np.ones(df[df.isna()].shape[0])*(-200), c='r', s=0.5, label='NaN')\n    plt.title('Evoluci\u00f3n temporal de ({}) y valores NaN'.format(col))\n    plt.ylabel(col)\n    plt.legend()\n    plt.show()","0e9c33d3":"for col in df.columns.drop(['date', 'time']):\n    fig, ax = plt.subplots(figsize=(10, 10))\n    plt.plot(range(24), \n             [df[df[col].isna() & df.index.map(lambda x: x.hour == h)].shape[0] for h in range(24)])\n    plt.title('Valores NaN por hora del d\u00eda ({})'.format(col))\n    plt.xticks(ticks=range(24))\n    plt.ylabel(col)\n    plt.show()","8125237f":"for col in df.columns.drop(['date', 'time']):\n    fig, ax = plt.subplots(figsize=(10, 10))\n    plt.plot(range(12), \n             [df[df[col].isna() & df.index.map(lambda x: x.month == m)].shape[0] for m in range(12)])\n    plt.title('Valores NaN por mes ({})'.format(col))\n    plt.xticks(ticks=range(12))\n    plt.ylabel(col)\n    plt.show()","481f5ed3":"for col in df.columns.drop(['date', 'time']):\n    s = df.isna().cumsum().diff()[col] \n    mask = s.ne(s.shift())\n\n    ids = s[mask].to_numpy()\n    counts = s.groupby(mask.cumsum()).cumcount().add(1).groupby(mask.cumsum()).max().to_numpy()\n\n    ids[ids == 'nan'] = np.nan\n    ser_out = pd.Series(counts, index=ids, name='counts')\n    ser_out = ser_out[ser_out.index==1]\n    \n    fig, ax = plt.subplots()\n    plt.hist(ser_out)\n    plt.title('Histograma de valores NaN seguidos ({})'.format(col))\n    plt.show()","b75df702":"correlations = df.drop(columns=['date', 'time']).corr()\ncorrelations","ec645ae4":"df = df.drop(columns=['t', 'ah', 'rh'])\ncorrelations = df.corr()\ncorrelations_map = {col: correlations[col].sort_values()[1:].index.tolist() for col in correlations.columns}\ncorrelations_map","92dd9087":"def manhattan_distance(p1, p2, q1, q2):\n    \"\"\"\n    Distancia manhattan entre dos puntos con dos dimensiones cada uno.\n    \"\"\"\n    return np.abs(p1-q1) + np.abs(p2-q2)","57b6b615":"# df_to_fill va a ser el dataframe que vayamos rellenando, para no utilizar como inputs \n# del algoritmo de generaci\u00f3n valores creados.\ndf_to_fill = df.copy()\n# Creamos una copia normalizada para obtener una medida de la distancia normalizada.\ndf_normalized = df.drop(columns=['time', 'date']).copy()\nmax_norm, min_norm = df_normalized.max(), df_normalized.min()\ndf_normalized = (df_normalized - min_norm) \/ (max_norm - min_norm)\n\nfor index, row in df_normalized.iterrows():\n    for col in df_normalized.columns:\n        if pd.isna(row[col]):\n            filling_vars = [v for v in correlations_map[col] if pd.notna(row[v])][:2]\n            if len(filling_vars) == 2:\n                p = row[filling_vars[0]], row[filling_vars[1]]\n                aux_df = df_normalized.loc[:index][:-1]\n                aux_df = aux_df.loc[pd.notna(aux_df[col])][filling_vars + [col]]\n                aux_df['distance'] = aux_df.apply(lambda row: manhattan_distance(row[filling_vars[0]], \n                                                                                 row[filling_vars[1]], \n                                                                                 p[0], p[1]), \n                                                  axis=1)\n                new_value = aux_df.nsmallest(3, 'distance')[col].mean()\n                df_to_fill.at[index, col] = new_value * (max_norm[col] - min_norm[col]) + min_norm[col]\n            else:\n                # En caso de no existir ning\u00fan valor a partir del cual rellenar, tomaremos el inmediato m\u00e1s cercano.\n                df_to_fill.at[index, col] = df_to_fill.loc[:index].iloc[-2][col]","23e2c96b":"# df_to_fill.to_csv('df_to_fill.csv', index=False)\n# df_to_fill = pd.read_csv('df_to_fill.csv')","41146216":"# Longitud de la serie previa a la predicci\u00f3n y de la serie posterior.\nprevious_series_length = 48\nposterior_series_length = 24\n\n# Hacemos un split temporal dejando el 60% para train, un 20% para validaci\u00f3n y otro 20% para test.\nnum_samples = df_to_fill.shape[0]\ntrain_df = df_to_fill.iloc[:int(num_samples*0.6)]\ntrain_df['hour'] = train_df.index.map(lambda x: x.hour)\ntrain_df = train_df.reset_index(drop=True)\nval_df = df_to_fill.iloc[int(num_samples*0.6):int(num_samples*0.8)]\nval_df['hour'] = val_df.index.map(lambda x: x.hour)\nval_df = val_df.reset_index(drop=True)\ntest_df = df_to_fill.iloc[int(num_samples*0.8):]\ntest_df['hour'] = test_df.index.map(lambda x: x.hour)\ntest_df = test_df.reset_index(drop=True)\n\n# Definimos las columnas de variables predictoras y de targets\nfeature_columns = ['hour', 'co_pt', 'nmhc_pt', 'nox_pt', 'no2_pt', 'o3_pt']\ntarget_columns = ['no2_gt', 'nox_gt', 'nmhc_gt', 'co_gt', 'c6h6_gt']\n\nX_train = train_df[feature_columns + target_columns].values\ny_train = train_df[target_columns].values\nX_val = val_df[feature_columns + target_columns].values\ny_val = val_df[target_columns].values\nX_test = test_df[feature_columns + target_columns].values\ny_test = test_df[target_columns].values\n\n# Escalamos los datos y guardamos el normalizador para usarlo en las futuras predicciones.\ndata_scaler = StandardScaler()\ntarget_scaler = StandardScaler()\n\nX_train = data_scaler.fit_transform(X_train)\nX_val = data_scaler.transform(X_val)\nX_test = data_scaler.transform(X_test)\n\ny_train = target_scaler.fit_transform(y_train)\ny_val = target_scaler.transform(y_val)\ny_test = target_scaler.transform(y_test)\n\nwith open('data_scaler.pkl', 'wb') as f:\n    pickle.dump(data_scaler, f)\nwith open('target_scaler.pkl', 'wb') as f:\n    pickle.dump(target_scaler, f)","97e2b31e":"def build_timeseries(arr, length=48):\n    \"\"\"\n    Devuelve series temporales de un array ordenado temporalmente.\n    \"\"\"\n    shape = arr.shape\n    ts_arr = np.zeros((shape[0]-length+1, length, shape[1]))\n    for n in range(shape[0]-length+1):\n        ts_arr[n] = arr[n:length+n]\n    return ts_arr","17d176d0":"X_train = build_timeseries(X_train[:-posterior_series_length], length=previous_series_length)\nX_val = build_timeseries(X_val[:-posterior_series_length], length=previous_series_length)\nX_test = build_timeseries(X_test[:-posterior_series_length], length=previous_series_length)\n\ny_train = build_timeseries(y_train[previous_series_length:], length=posterior_series_length)\ny_val = build_timeseries(y_val[previous_series_length:], length=posterior_series_length)\ny_test = build_timeseries(y_test[previous_series_length:], length=posterior_series_length)","4418b0e7":"X_train, y_train = shuffle(X_train, y_train, random_state=seed)\nX_val, y_val = shuffle(X_val, y_val, random_state=seed)\nX_test, y_test = shuffle(X_test, y_test, random_state=seed)","1bc30a7e":"def train_seq2seq_hps(latent_dim, optimizer, dropout, recurrent_dropout, lr, epochs=1000, batch_size=64):\n    tf.random.set_seed(seed)\n    \n    encoder_inputs = Input(shape=(None, 11))\n    encoder = LSTM(latent_dim, dropout=dropout, recurrent_dropout=recurrent_dropout, return_state=True)\n    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n    encoder_states = [state_h, state_c]\n\n    decoder_inputs = Input(shape=(None, 5))\n    decoder_lstm = LSTM(latent_dim, dropout=dropout, recurrent_dropout=recurrent_dropout, \n                        return_sequences=True, return_state=True)\n    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n                                         initial_state=encoder_states)\n    decoder_dense = Dense(5)\n    decoder_outputs = decoder_dense(decoder_outputs)\n\n    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\n    early_stopping = EarlyStopping(patience=5, min_delta=0.001, restore_best_weights=True)\n    opt = optimizer(lr=lr)\n    model.compile(optimizer=opt, loss='mse')\n    model.fit([X_train, y_train[:,:-1]], y_train[:,1:],\n              batch_size=batch_size,\n              epochs=epochs,\n              validation_data=([X_val, y_val[:,:-1]], y_val[:,1:]),\n              callbacks=[early_stopping])\n\n    encoder_model = Model(encoder_inputs, encoder_states)\n\n    decoder_state_input_h = Input(shape=(latent_dim,))\n    decoder_state_input_c = Input(shape=(latent_dim,))\n    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n    decoder_outputs, state_h, state_c = decoder_lstm(\n        decoder_inputs, initial_state=decoder_states_inputs)\n    decoder_states = [state_h, state_c]\n    decoder_outputs = decoder_dense(decoder_outputs)\n    decoder_model = Model(\n        [decoder_inputs] + decoder_states_inputs,\n        [decoder_outputs] + decoder_states)\n    \n    encoder_model.save('encoder_{}_{}_{}_{}_{}.h5'.format(latent_dim, \n                                                          opt.__class__.__name__,\n                                                          int(dropout*10),\n                                                          int(recurrent_dropout*10),\n                                                          int(lr*100)))\n    decoder_model.save('decoder_{}_{}_{}_{}_{}.h5'.format(latent_dim, \n                                                          opt.__class__.__name__,\n                                                          int(dropout*10),\n                                                          int(recurrent_dropout*10),\n                                                          int(lr*100)))\n    \n    best_epoch = np.argmin(model.history.history['val_loss'])\n    dic = {metric: model.history.history[metric][best_epoch] for metric in ['loss', 'val_loss']}\n    dic['latent_dim'] = latent_dim\n    dic['optimizer'] = opt.__class__.__name__\n    dic['dropout'] = dropout\n    dic['recurrent_dropout'] = recurrent_dropout\n    dic['lr'] = lr\n    \n    return dic\n\n\nhps_list = [{'latent_dim': latent_dim, \n             'optimizer': optimizer, 'dropout': dropout, 'recurrent_dropout': recurrent_dropout, 'lr': lr} \n            for latent_dim in [10, 50, 100]\n            for optimizer in [optimizers.Adam, optimizers.RMSprop, optimizers.Adadelta]\n            for dropout in [0.0, 0.3]\n            for recurrent_dropout in [0.0, 0.3]\n            for lr in [0.01, 0.1]]\n\nhps_list = shuffle(hps_list)\nresults = []\nfor hps in hps_list:\n    print(hps)\n    results.append(train_seq2seq_hps(**hps))\n    df_results = pd.DataFrame(results)\n    df_results.to_csv('results.csv', index=False)\n    \ndef decode_sequence(inputs, encoder_model, decoder_model):\n    \"\"\"\n    Recibe un modelo encoder y uno decoder y genera toda la serie temporal a partir de los pasos previos.\n    \"\"\"\n    states_value = encoder_model.predict(inputs)\n    target_seq = inputs[:,-1:,-5:]\n    decoded_sentence = np.zeros((1,posterior_series_length,5))\n    \n    for i in range(posterior_series_length):\n        output_tokens, h, c = decoder_model.predict(\n            [target_seq] + states_value)\n        decoded_sentence[0,i] = output_tokens[0,0] \n        target_seq = output_tokens\n        states_value = [h, c]\n\n    return decoded_sentence\n","9fa16a66":"# Cargamos el dataframe de resultados.\ndf_results = pd.read_csv('results.csv')\n\ndf_results['val_loss'] = df_results.apply(lambda row: row['val_loss'][row['best_epoch']], axis=1)\ndf_results['loss'] = df_results.apply(lambda row: row['loss'][row['best_epoch']], axis=1)\n\ndf_results = df_results.sort_values('val_loss', ascending=True)","1c06da5a":"# Cargamos el decoder y el encoder del mejor modelo:\nbest_comb = df_results.iloc[0]\nencoder_model = load_model('encoder_{}_{}_{}_{}_{}.h5'.format(best_comb.latent_dim,\n                                                             best_comb.optimizer,\n                                                             int(best_comb.dropout*10),\n                                                             int(best_comb.recurrent_dropout*10),\n                                                             int(best_comb.lr*100)))\ndecoder_model = load_model('decoder_{}_{}_{}_{}_{}.h5'.format(best_comb.latent_dim,\n                                                             best_comb.optimizer,\n                                                             int(best_comb.dropout*10),\n                                                             int(best_comb.recurrent_dropout*10),\n                                                             int(best_comb.lr*100)))\nencoder_model.save('best_model_encoder.h5')\ndecoder_model.save('best_model_decoder.h5')","33672d1d":"fig, ax = plt.subplots(nrows=5, ncols=5, figsize=(25,25))\nfor n in range(5):\n    previous_ts = data_scaler.inverse_transform(X_test[n])[:,6:]\n    future_ts = target_scaler.inverse_transform(y_test[n])\n    total_ts = np.concatenate([previous_ts, future_ts])\n    pred = target_scaler.inverse_transform(decode_sequence(X_test[n:n+1], encoder_model, decoder_model))[0]\n    for i in range(5):\n        ax[n, i].plot(range(72), total_ts[:,i], label=['Ground Truth'])\n        ax[n, i].vlines(48, min(total_ts[:,i].min(), pred[:,i].min()), max(total_ts[:,i].max(), pred[:,i].max()))\n        ax[n, i].plot(range(48, 72), pred[:,i], color='r', label=['Predicted'])\n        ax[n, i].set_title(target_columns[i])\n        ax[n, i].set_ylabel(target_columns[i])\n        ax[n, i].set_xlabel('timestep')\n        ax[n, i].set_xticks(range(0, 72, 12))\n        \nhandles, labels = ax[0,0].get_legend_handles_labels()\nfig.legend(handles, list(map(lambda x: x[2:-2], labels)), loc='upper center', prop={'size': 20})\nplt.show()","b22160f5":"# Vamos a generar un df de prueba para lanzar el script.\ndf = pd.read_excel('AirQualityUCI.xlsx')\ntest_df = df.fillna(0)[:48]\ntest_df.to_csv('test_df.csv', index=False)","8f6c5b19":"Nuestro objetivo va a ser predecir las variables de ground truth (no2_gt, nox_gt, nmhc_gt, co_gt, c6h6_gt) de las pr\u00f3ximas 24 horas en base al hist\u00f3rico previo (vamos a limitarnos a las 48 horas anteriores).","60ad812b":"La forma de rellenar datos va a ser:\n- Para cada medici\u00f3n en un momento t de una variable v que sea NaN tomaremos, de entre los datos anteriores temporalmente a t, las dos variables disponibles (en ese momento t) con m\u00e1s correlaci\u00f3n, v1 y v2, con v.\n- Tomaremos las tres mediciones m\u00e1s cercanas a los valores v1 y v2.\n- El valor asignado a la variable v ser\u00e1 la media de esas tres mediciones.","2d1b075c":"Parece que la densidad de valores NaN es muy alta en todas las variables (no se aprecia ninguna discontinuidad en la l\u00ednea roja). Veamos si se aprecia algo por horas y por meses.","eec5c9e7":"### Modelado","7838796c":"Parece que va a ser posible hacer la sustituci\u00f3n por este m\u00e9todo. Para evitar meter una cantidad excesivo de ruido a los datos, vamos a eliminar las variables t, rh y ah, ya que guardan bastante menos correlaci\u00f3n con las dem\u00e1s. \n\nPara cada variable, vamos a quedarnos en cada caso con las otras dos variables con una correlaci\u00f3n m\u00e1s fuerte, para usarlas para rellenar datos.","52eba552":"### Conclusi\u00f3n","0763b54e":"Para esta tarea vamos a construir un modelo Seq2Seq basado en redes neuronales de tipo LSTM, especializadas en procesar datos temporales. Esta arquitectura nos va a permitir hacer predicciones de un paso temporal bas\u00e1ndonos en las de los pasos previos que ya hemos ido prediciendo.","e59d22fd":"Vamos a visualizar algunos casos concretos de test para terminar. Lo ideal ser\u00eda hacerlo en un tramo temporal en el que los datos est\u00e9n completos. Sin embargo, no es posible porque la variable nmhc_gt no existe en todo el conjunto de test.  As\u00ed que vamo a hacer las predicciones directamente en el dataset con los datos rellenados. \n\nOtra posibilidad hubiera sido eliminarla por completo del dataset.","939302c2":"Vamos a ver si existe alg\u00fan patr\u00f3n claro en los valores que faltan para ver si es posible rellenarlos de alguna forma.","7f5ff755":"Dado que hay muchos casos en los que la secuencia de NaNs es demasiado larga como para asignar a un tiempo t el primer valor anterior a ese tiempo t en el que se ten\u00eda la medici\u00f3n, por lo que vamos a probar a rellenar valores de ciertas variables en base a otras que compartan una correlaci\u00f3n fuerte.\n\nVamos a ver la tabla de correlaciones:","e8ea429c":"Vemos que no hay valores nan. Sin embargo, en la descripci\u00f3n del dataset aparece indicado -200 como valor por defecto para los valores faltantes.","53c18492":"El resultado parece ser bastante bueno a simple vista, adem\u00e1s de coherente, pues todas las variables evolucionan de forma similar. Faltar\u00eda hacer el c\u00e1lculo del error por cada timestep, que probablemente vaya increment\u00e1ndose ligeramente a lo largo de la serie predicha.\n\nDebido a falta de tiempo, no se ha podido realizar una b\u00fasqueda de hiperpar\u00e1metros m\u00e1s exhaustiva, lo que hubiera dado unos resultados m\u00e1s precisos.\n\nTambi\u00e9n quedar\u00eda pendiente comparar el rendimiento del seq2seq con otros modelos, as\u00ed como probar otros m\u00e9todos de relleno de valores NaN.","f29ac560":"Una de las primeras dudas que se pueden plantear es si, efectivamente, hay una medici\u00f3n por hora y si ninguna de esas mediciones tiene valores nulos:","a53fddbc":"### Exploraci\u00f3n de datos","e799cf74":"Parece haber alg\u00fan patr\u00f3n similar en varias variables, pero no parece ser algo claro que nos pueda ayudar a rellenar los datos. La pregunta que habr\u00eda que hacerse ahora es c\u00f3mo de grandes son los lapsos temporales en los que no hay ning\u00fan valor, para ver si tiene sentido utilizar los valores previos de una variable para rellenar los posteriores.\n\nVamos a calcular las longitudes de las secuencias de NaN que se encuentran seguidas en los datos."}}