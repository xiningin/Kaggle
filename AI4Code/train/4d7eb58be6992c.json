{"cell_type":{"145c1ade":"code","a3d96f42":"code","ec4a8bbd":"code","8c5c8628":"code","b4bf88a0":"code","12632f13":"code","da74fd15":"code","1ed5ca5e":"code","b4e661ea":"code","e77af645":"code","9672a105":"code","d94c5c1e":"code","82faf178":"code","08fc8791":"code","25055aaa":"code","754d63c0":"code","ff324a0d":"markdown","43248ec6":"markdown","fa7d1779":"markdown","e3e89bc7":"markdown","5e322cd8":"markdown","bac68d94":"markdown","9ab09e93":"markdown","cfcb221f":"markdown","11725178":"markdown","2bb62972":"markdown","b72b8b57":"markdown"},"source":{"145c1ade":"# Load Libraries\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np","a3d96f42":"# Define Work Space\nwork_dir = os.curdir\ndata_dir = os.path.join(work_dir,'datasets')\nlog_dir = os.path.join(work_dir,'logs')","ec4a8bbd":"def LoadData(): # Load the Data and convert them to tensors\n    (X_train,y_train),(X_test,y_test) = tf.keras.datasets.mnist.load_data(path='mnist.npz')\n    X_train = tf.constant(X_train,dtype='float32')\n    X_test = tf.constant(X_test,dtype='float32')\n    y_train = tf.constant(y_train,dtype='float32')\n    y_test = tf.constant(y_test,dtype='float32')\n    return ((X_train,y_train),(X_test,y_test))","8c5c8628":"(X_train,y_train),(X_test,y_test) = LoadData()","b4bf88a0":"# Sample & Visualize Images\nfig,ax = plt.subplots(nrows=2,ncols=5,figsize=(12,4),dpi=80)\nfor i in range(2):\n    for j in range(5):\n        dig = i*5 + j\n        bool_list = (y_train == dig)\n        idx = np.random.randint(low=0,high=1000)\n        X_sample = X_train[bool_list][idx]\n        y_sample = y_train[bool_list][idx]\n        ax[i][j].imshow(np.array(X_sample),cmap='Greys')\n        ax[i][j].title.set_text(dig)\n        ax[i][j].tick_params(axis='x',which='both',bottom=False,top=False,labelbottom=False)\n        ax[i][j].tick_params(axis='y',which='both',left=False,right=False,labelleft=False)","12632f13":"# Custom Implementation of Normalization Layer\nclass MyNormalizationLayer(tf.keras.layers.Layer):\n    def __init__(self,epsilon=0.0001,**kwargs):\n        super().__init__(**kwargs)\n        self.epsilon = epsilon\n        \n    def build(self,batch_input_shape):\n        self.alpha = tf.Variable(np.ones(shape=batch_input_shape[-1:]),dtype='float32')\n        self.beta = tf.Variable(np.zeros(shape=batch_input_shape[-1:]),dtype='float32')\n    \n    def call(self,inputs):\n        if len(inputs.shape) == 1:\n            mu,var = tf.nn.moments(inputs, axes=0, keepdims=True)\n        else:\n            mu,var = tf.nn.moments(inputs, axes=1, keepdims=True)\n        std = tf.math.sqrt(var)\n        return self.alpha*(inputs-mu)\/(std + self.epsilon) + self.beta\n    \n    def compute_output_shape(self,batch_input_shape):\n        return batch_input_shape\n    \n    def get_config():\n        base_config =super().get_config()\n        return {**base_config,'epsilon':self.epsilon}","da74fd15":"def my_glorot_normal(shape,dtype=tf.float32,seed=42): # Custom Implementation of Kernel Initialization function\n    stddev = tf.sqrt(2. \/ (shape[0] + shape[1]))\n    return tf.random.normal(shape=shape,stddev=stddev,seed=seed)","1ed5ca5e":"fig,ax = plt.subplots(figsize=(6,6))\nkernel_init_weights = my_glorot_normal((100,1))\nsns.kdeplot(kernel_init_weights.numpy().reshape(1,-1)[0],shade=True)","b4e661ea":"def my_relu(z): # Custom Relu Activation Function\n    return tf.cast(tf.nn.leaky_relu(z,0.2),tf.float32)\n\ndef my_softmax(z): # Custom Softmax Implementation\n    return tf.math.exp(z)\/tf.math.reduce_sum(tf.math.exp(z))","e77af645":"fig,ax = plt.subplots(ncols=2,figsize=(16,5))\nx = tf.range(-100,100,dtype=tf.float32)\nsns.lineplot(x=x,y=my_relu(x),ax=ax[0])\nax[0].grid()\nax[0].title.set_text('My Relu Function')\nx = tf.range(-1,1,delta=0.01,dtype=tf.float32)\nsns.lineplot(x=x,y=my_softmax(x),ax=ax[1])\nax[1].grid()\nax[1].title.set_text('My Softmax Function')","9672a105":"class MyDenseLayer(keras.layers.Layer): # Custom Dense Layer Implementation\n    def __init__(self,units, activation=None, **kwargs):\n        super().__init__(**kwargs)\n        self.units=units\n        self.activation = activation\n    def build(self, batch_input_shape):\n        self.kernel = self.add_weight(\n                                name='kernel',shape=[batch_input_shape[-1],self.units],\n                                initializer=my_glorot_normal)\n        self.bias = self.add_weight(name='bias', shape=[self.units], initializer='zeros')\n        \n    def call(self, X):\n        return self.activation(X @ self.kernel + self.bias)\n    \n    def compute_output_shape(self, batch_input_shape):\n        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n    \n    def get_config(self):\n        base_config =super().get_config()\n        return {**base_config,'units':self.units,'activation':keras.activations.serialize(self.activation)}","d94c5c1e":"class CustomModel(tf.keras.Model):\n    \n    def __init__(self,n_layers,n_units,output_dim,**kwargs):\n        super().__init__(**kwargs)\n        self.n_layers = n_layers\n        self.n_units = n_units\n        self.output_dim = output_dim\n        self.flat_layer = tf.keras.layers.Flatten()\n        self.dense_layer1 = MyDenseLayer(n_units,activation=my_relu)\n        self.norm_layer = MyNormalizationLayer()\n        self.dense_layer = MyDenseLayer(n_units,activation=my_relu)\n        self.out_layer = MyDenseLayer(self.output_dim,activation=my_softmax)\n        \n    def call(self,inputs):\n        z = self.flat_layer(inputs)\n        z = self.dense_layer1(z)\n        for _ in  range(1 + self.n_layers):\n            z = self.norm_layer(z)\n            z = self.dense_layer(z)\n        return self.out_layer(z)","82faf178":"# Reset Backemd and set Random Seed to make results reproduceable and comparable\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)","08fc8791":"model = CustomModel(10,128,10)\nmodel.compile(optimizer='nadam',loss='sparse_categorical_crossentropy',metrics=['acc'])\n#model.build((32,28,28))\n#model.summary()","25055aaa":"history = model.fit(X_train,y_train,batch_size=32,epochs=1,validation_split=0.2)","754d63c0":"# Evaluate the Model\nmodel.evaluate(X_test,y_test)","ff324a0d":"<h1><i>Stay Hungry, Stay Foolish <\/i><\/h1>","43248ec6":"### 3. Whats Yet to be Done\n\n<b><ol>\n    <li>Building Custome Layer to Implement Flattening<\/li>\n    <li>Build Custom Implementation of Loss 'Ndam'<\/li>\n    <li>Build Custom Implementation of Optimizer 'Sparse Categorical Cross Entropy'<\/li>\n    <li>Build Custom Implementations of All major Layer types, CNN, LSTM, RNN, etc.<\/li>\n<\/ol><\/b>","fa7d1779":"### 1. So whats the deal here?\n\n<b><i>I am learning how  to build custom models from scratch using lower level API's of TensorFlow.<\/i><\/b>\n\n<i>So I will try to implementing every aspect of a simple Multi Layer Perceptron (MLP) using custom build functions and classes.<\/i>","e3e89bc7":"### 2. Why Do It?\n\n<b><ul>\n    <li>Cause at Industrial Level,you need to implement custom tools, for unique problems<\/li>\n    <li>Learning to code custom models, leads to learning custom GPU optimizations, and thats awesome!<\/li>\n    <li>Other way is too easy, and too boring<\/li>","5e322cd8":"## Custom Normalization Layer","bac68d94":"<b><i>Hola!! Welcome Back to my notebook, if you are new here, leave a comment as shout out.\n    <\/i><\/b>","9ab09e93":"# Custom Model Development Using TensorFlow","cfcb221f":"## Custom Kernel Initializers","11725178":"### 2. Whats Already Completed\n\n<b><ol>\n    <li>Data loading, processing into Tensors<\/li>\n    <li>Sampling & Visualizing Images<\/li>\n    <li>Designing & Implementing Normalization Class<\/li>\n    <li>Define Custom Kernel Initializer<\/li>\n    <li>Define Custom Activation Functions<\/li>\n    <li>Designing & Implementing Custom Dense Layer<\/li>\n    <li>Build Custom Model, with number of layers & units as hyper parameters<\/li>\n    <\/ol><\/b>","2bb62972":"## Custom Activation Functions","b72b8b57":"### What to Expect in Future\n\n<b><I>Just Like a model, there is always scope for improvements for us humans<\/I><\/b>"}}