{"cell_type":{"cbfccd79":"code","b77698c2":"code","ff166931":"code","3b99c96d":"code","0daa78a1":"code","dcc1e584":"code","9658ab26":"code","050d318f":"code","80d78aa7":"code","963e395a":"code","703008a9":"code","bd49a319":"code","41d5cf26":"code","271d2251":"code","34259e9d":"code","69be9c6d":"code","a51a60d5":"code","2c93ec74":"code","7a3debad":"code","04a2f08b":"code","255c4377":"code","07082ff2":"code","33b56d94":"code","28b13831":"code","1239d325":"code","89d32986":"code","2a554d25":"code","004a5b01":"code","cc675aef":"code","2a999792":"code","af894149":"code","ec15e009":"code","c348722c":"code","3909dfa8":"code","35b0ed04":"code","c7c7f971":"code","74432ae0":"code","608cd08b":"code","c5e4ea97":"code","a5ba319f":"code","81b15efb":"code","c5abe8b0":"code","d06022a7":"code","bc4a7964":"code","ea06457a":"code","76e8e235":"code","c7d9179f":"code","af0c0853":"markdown","c618ef4b":"markdown","2dabfa38":"markdown","3b9eb749":"markdown","98756c98":"markdown","803c35fc":"markdown","4fe2303b":"markdown","1cc24491":"markdown","2284cb5c":"markdown","bd554cce":"markdown","7e0c16d9":"markdown","89657fd2":"markdown","7197107c":"markdown","d23e38b7":"markdown","90cb0504":"markdown","4a3612ee":"markdown","01ed6e9b":"markdown","3898f244":"markdown","79f5ed36":"markdown","a198503d":"markdown","51f51bb6":"markdown","cae15adc":"markdown"},"source":{"cbfccd79":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b77698c2":"from sklearn.preprocessing import LabelEncoder,LabelBinarizer\nimport lightgbm as lgb\nimport catboost as ct\nimport sklearn\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.model_selection import KFold,RepeatedStratifiedKFold,RandomizedSearchCV,GridSearchCV,cross_val_score\nfrom sklearn.feature_extraction.text import TfidfTransformer,CountVectorizer,ENGLISH_STOP_WORDS\nimport nltk\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer","ff166931":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, plot_confusion_matrix, plot_precision_recall_curve","3b99c96d":"dataset=pd.read_csv('..\/input\/60k-stack-overflow-questions-with-quality-rate\/train.csv')","0daa78a1":"dataset.head()","dcc1e584":"draft_dataset=dataset","9658ab26":"dataset.sort_values('CreationDate',inplace=True)","050d318f":"lb=LabelEncoder()\nnew_data=lb.fit_transform(dataset.CreationDate)\n#dataset['DateCatCOl']=new_data","80d78aa7":"dataset.head()","963e395a":"dataset.drop(['Id','CreationDate'],axis=1,inplace=True)\ndataset.head()","703008a9":"dataset.Y.value_counts().to_dict()","bd49a319":"dataset['Y']=dataset.Y.map({'LQ_CLOSE': 0, 'LQ_EDIT': 1, 'HQ': 2})","41d5cf26":"dataset","271d2251":"import re\ndef clean_tags(T):\n    T=T.lower()\n    text=re.sub(r'<','',T)\n    text=re.sub(r'>',' ',text)\n    return text\n\ndataset['Tags']=dataset['Tags'].map(clean_tags)\ndataset.head()","34259e9d":"dataset.Tags.value_counts()[:10]","69be9c6d":"count_v=CountVectorizer()\ntags_vecorized=count_v.fit_transform(dataset.Tags)","a51a60d5":"dataset.drop('Tags',axis=1,inplace=True)","2c93ec74":"dataset.head()","7a3debad":"import os\ndef clean_body(x):\n    x=x.lower()\n    x=re.sub(r'[^(a-zA-Z)\\s]','', x)\n    return x\n\ndataset['Body']=dataset.Body.map(clean_body)","04a2f08b":"dataset.head()","255c4377":"dataset['CombineTextandBody']=dataset['Title']+' '+dataset['Body']\ndataset.head()","07082ff2":"dataset.drop(['Title','Body'],axis=1,inplace=True)\ndataset.head()","33b56d94":"label=dataset.pop('Y')\ndataset.head()","28b13831":"train_x,test_x,train_y,test_y=train_test_split(dataset,label,test_size=0.15,random_state=42)","1239d325":"train_x.shape,test_x.shape","89d32986":"train_x.head()","2a554d25":"tfidf=TfidfVectorizer()\ntransform_text_train=tfidf.fit_transform(train_x.CombineTextandBody)\ntransform_text_test=tfidf.transform(test_x.CombineTextandBody)","004a5b01":"transform_text_train.shape","cc675aef":"rskf=RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)","2a999792":"lr_classifier = LogisticRegression(C=1.)\nlr_classifier.fit(transform_text_train, train_y)","af894149":"transform_text_test.shape,test_y.shape","ec15e009":"print(f\"Validation Accuracy of Logsitic Regression Classifier is: {(lr_classifier.score(transform_text_test, test_y))*100:.2f}%\")","c348722c":"score=cross_val_score(lr_classifier,transform_text_train, train_y,cv=3,n_jobs=-1)","3909dfa8":"score","35b0ed04":"xg_classifier = XGBClassifier(n_estimators=500,n_jobs=-1,random_state=42)\nxg_classifier.fit(transform_text_train, train_y)","c7c7f971":"print(f\"Validation Accuracy of XGBoost Clf. is: {(xg_classifier.score(transform_text_test, test_y))*100:.2f}%\")","74432ae0":"nb_classifier = MultinomialNB()\nnb_classifier.fit(transform_text_train, train_y)","608cd08b":"\nprint(f\"Validation Accuracy of Naive Bayes Classifier is: {(nb_classifier.score(transform_text_test, test_y))*100:.2f}%\")","c5e4ea97":"lgb_model=lgb.LGBMClassifier()\nlgb_model.fit(transform_text_train, train_y)","a5ba319f":"\nprint(f\"Validation Accuracy of lgb_model Classifier is: {(lgb_model.score(transform_text_test, test_y))*100:.2f}%\")","81b15efb":"from scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\nparam_test ={'num_leaves': sp_randint(6, 50), \n             'min_child_samples': sp_randint(100, 500), \n             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n             'subsample': sp_uniform(loc=0.2, scale=0.8), \n             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100],\n            'learning_rate':[0.1,0.01,0.05,0.001,0.005,0.03,0.003,0.006,0.08]}","c5abe8b0":"lgb_model.get_params()","d06022a7":"RS=RandomizedSearchCV(\n    estimator=lgb_model, param_distributions=param_test,\n    cv=3,\n    refit=True,\n    random_state=42,\n    verbose=True)","bc4a7964":"RS.fit(transform_text_train, train_y)","ea06457a":"RS.best_estimator_,RS.best_params_,RS.best_score_","76e8e235":"parameter_list={\n    'C':[0.10,0.6,0,3.0,4.0,5.,6.,9.,0.11,0.12,0.15,0.14,0.20],\n    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n}\n\nlr_classifier_2 = LogisticRegression(n_jobs=-1,random_state=42)\n\nlog_tune=RandomizedSearchCV(\n    estimator=lr_classifier_2, param_distributions=parameter_list,\n    cv=3,\n    refit=True,\n    random_state=42,\n    verbose=True)\n\nlog_tune.fit(transform_text_train, train_y)","c7d9179f":"log_tune.best_estimator_,log_tune.best_params_,log_tune.best_score_","af0c0853":"## created some folds ","c618ef4b":"## Vectorizing the Data\nLet's vectorize the data so it's in the numerical format","2dabfa38":"Print the accuracy score of the naive bayes classifier","3b9eb749":"Let's join the title and the body of the text data so that we can use both of them in our classification","98756c98":"## Introduction and Imports\nn this notebook, I will be using only Machine Learning methods to get decent prediction scores. There are much better and sophisticated ways (like RNN, GRU, Fine-tuning BERT, etc) but you have seen them on a lot of notebook already.\n\nThe main aim of this notebook is to just show how quickly and easily you can do Text Classification using Basic Machine Learning Methods, rather than spend waiting 1 hour for a model to train!\n\nIf you like this notebook, please make sure to give an upvote, it helps a lot and motivates me to make much more good-quality content\n\nIf you don't like my work, please leave a comment on what can I do to make it better!","803c35fc":"# 2. XGBoost\nFinally, let's use the XGBoost Classifier and then we'll compare all the different classifiers so far","4fe2303b":"\n\n![image](https:\/\/miro.medium.com\/max\/1200\/0*UEtwA2ask7vQYW06.png)","1cc24491":"## Splitting the Data\nLet's now split the dataset into training and validation sets","2284cb5c":"cross-validation score of logistic classifier ","bd554cce":"    x=x.lower()\n    x=re.sub(r'<p>',\" \",x)\n    x=re.sub(r'[^(a-zA-Z)\\s]','', x)\n    x=x.strip(os.linesep)\n    x=re.sub(r'[\\n\\r]+', '', x)\n    x=x.strip()","7e0c16d9":"# 1. Logistic Regression\n\nLet's first start with our good old, Logistic Regression!","89657fd2":"<p style=\"color:red\">If you like this notebook, please make sure to give an upvote, it helps a lot and motivates me to make much more good-quality content<\/p>\n<p style=\"color:blue\">If you don't like my work, please leave a comment on what can I do to make it better!<\/p>\n<hr>\n<h3 style=\"color:aqua\">Edits:<\/h3>\n<ul>\n<li style=\"color:green\">All Classifiers now classify for all 3 categories and not just 2. Good Validation Accuracy is maintained.<\/li>\n<\/ul>","7197107c":"**Note: since we got very good result with logistic regression we make it simple beacuase light GBM takes lot of time to train and find optimum result. hence we can compromise with some accuracy and avoid some complexity we can go with logistic Regression.**\n\nTip:- From my experiance most of the time we generally go for complex model but we should always start with some basic model if they dont work then we should go for some complex models.","d23e38b7":"# 4. Light GBM Model","90cb0504":"Print the accuracy score of the XG boost classifier","4a3612ee":"Print the accuracy score of the lgb_model classifier","01ed6e9b":"# Data Preprocessing and Some EDA","3898f244":"### best parameters and best score of LIght GBM","79f5ed36":"# 3. Multinomial Naive Bayes\nLet's now switch to the naive the bayes, the NAIVE BAYES!","a198503d":"## Modelling\nLet's start with different non-deep learning approaches for this task.","51f51bb6":"## Best Parameter and best score of logistic Regression ","cae15adc":"## Hyper parameter tuning of light GBM "}}