{"cell_type":{"ba972b9d":"code","0b132f4b":"code","d14daa6f":"code","22f855fa":"code","46714bdf":"code","69fb5035":"code","210aefe0":"code","e817a452":"code","fef657b3":"code","50fc2b49":"code","d9f8949d":"code","ca70fb0a":"code","c2d1972d":"code","14265acc":"code","8410007f":"code","4e42dc1a":"code","aa3c4550":"code","ce2c16e9":"code","cd613f98":"code","7c90a985":"code","865acfee":"code","0e81836e":"code","615cc3ac":"code","b585ce33":"code","01927ba2":"code","c03b519c":"code","f4933322":"code","c552704b":"code","3af4d611":"code","bae23885":"code","eb836467":"code","6979940e":"code","f150aac2":"code","298e9124":"code","cefb4bd2":"code","8f9c97ba":"code","496c5b4e":"code","5e4292d9":"code","5d431d5b":"code","d02769d2":"code","48fc4909":"code","de0026c2":"code","66247ec8":"code","27cfc5de":"code","54662ab5":"code","b4eb234e":"code","70ff9c57":"code","715403f3":"code","1a7638ea":"code","718046f1":"code","df4be766":"code","596a8360":"code","a3a7baf7":"code","38ab87b1":"code","a1e0c04a":"code","17cb6c52":"code","ad292fb4":"code","5a582c35":"code","3c88db51":"code","a11d672f":"code","9022d620":"code","4681582d":"code","3063e767":"code","183396a6":"code","06418366":"code","bea7db54":"code","fd9a7f31":"code","1149867f":"code","f59d3c10":"code","d157e5f9":"code","0ede63cb":"code","e1175d45":"code","c39187ff":"code","7b84d067":"code","a3f0711f":"code","14a0f683":"code","5a656349":"code","c33968c4":"code","5c82aa34":"code","fd49a85b":"code","5ded3c0d":"code","32894ddb":"code","62ac8cc3":"code","598b65b7":"code","7787189c":"code","2bd0c62c":"code","710d7d11":"code","8fb9a212":"code","863f9fe0":"code","ba83ec8b":"code","297dcb01":"markdown","89a9b3a6":"markdown","03f69078":"markdown","5ae7fbb0":"markdown","c1db38d3":"markdown","a20c5d24":"markdown","ab6f38eb":"markdown","d2a01006":"markdown","b4177983":"markdown","398839d0":"markdown","bd90f9df":"markdown","dbe3411e":"markdown","0bd36e5b":"markdown","016ac5d0":"markdown","fa76e744":"markdown","fac05240":"markdown","25d28969":"markdown","5bfa1644":"markdown","77ea6651":"markdown","b645cf2c":"markdown","8b7c248b":"markdown","1fb6c4cd":"markdown","eb81174e":"markdown","e65db13d":"markdown","d4093637":"markdown","38b74ee1":"markdown","09024b07":"markdown","b77c6ca0":"markdown","07ec2964":"markdown","808916c0":"markdown","fd77c947":"markdown","4102049a":"markdown","456f5280":"markdown","ab111765":"markdown","9d10dcb5":"markdown","56db64e3":"markdown","7128047e":"markdown","3cc49ee9":"markdown","4b5d9605":"markdown","e75cb060":"markdown","d5e60c5c":"markdown","e5e5df7d":"markdown","1b02ac7a":"markdown","1fada319":"markdown","1048bdcd":"markdown","6de8811b":"markdown","212bae90":"markdown","4c4dd7c6":"markdown","d23ea0ce":"markdown","9fbdf955":"markdown","00fc76c4":"markdown"},"source":{"ba972b9d":"#import modules:\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Open the data file and convert to a Data Frame\n#Here I also separated randomly selected 20% of the data for later validation\n\ndata = pd.read_csv('..\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv')\n#randomize the data\ndata.sample(frac=1)\ndf=data.copy()\ndf.shape","0b132f4b":"df.head().iloc[:,0:8]","d14daa6f":"df.head().iloc[:,8:]","22f855fa":"#fill missing values for last review and reviews per month with 0\ndf[[\"last_review\", \"reviews_per_month\"]] = df[[\"last_review\", \"reviews_per_month\"]].fillna(0)\n\n#if there is no host name or listing name fill in None\ndf[[\"name\", \"host_name\"]] = df[[\"name\", \"host_name\"]].fillna(\"None\")\n\n#Drop rows were price of the listing is 0. We are not intersted in \"free\" \n#listings as they are most likely an error.\nfree = len(df[df.price == 0])\ndf = df[df.price != 0].copy()\n\n#Print initial insights:\nprint(\"The initial dataset contained \" + str(free)+ \" listings with price of 0 USD, that had been removed\")\nprint(\"There are \" + str(len(df[\"id\"].unique()))+\" listings\")\nprint(\"There are \"+str(len(df.host_id.unique()))\n      +\" unique and indentifiable \"+ \"hosts.\")\nprint(\"There are \"+str(len(df[df[\"host_name\"]==\"None\"]))\n      +\" unindentifiable \"+ \"hosts.\")\nprint(\"Dataframe shape: \"+str(df.shape))","46714bdf":"(len(df[df[\"host_id\"]==30985759]) == df[df[\"id\"]==36485609][\"calculated_host_listings_count\"]).tolist()","69fb5035":"df[(df[\"calculated_host_listings_count\"]>1)][[\"host_id\",\"calculated_host_listings_count\"]].sort_values(by=['host_id']).head(10)","210aefe0":"df_old=df.copy()\ndf = df[df[\"minimum_nights\"] <=31].copy()\nremoved_listings = len(df_old)-len(df)\n\nfig = plt.figure(figsize=(14,3))\nax1 = fig.add_subplot(1, 2, 1)\nax2 = fig.add_subplot(1, 2, 2)\n\n\nax1.hist(df_old.minimum_nights, bins=100, log=True)\nax1.set_ylabel(\"Frequency\")\nax1.set_title(\"No limit on minimum nights\")\n\nax2.hist(df.minimum_nights, bins=31, log=True)\nax2.set_ylabel(\"Frequency\")\nax2.set_title(\"Maximum 31 minimum nights\")\n\nplt.show()\n\nprint(\"As a result of imposing minimum nights limit, \" + str(removed_listings)+\" listings were removed.\")","e817a452":"df.isnull().sum()","fef657b3":"df.describe().iloc[:,0:8]","50fc2b49":"df.describe().iloc[:,8:]","d9f8949d":"df.dtypes","ca70fb0a":"#separate out numerical variables\na=pd.DataFrame(df.dtypes.copy())\nb= a[a[0] != 'object'].reset_index()\n#drop id and host id:\nnumeric_vars=b[\"index\"].tolist()[2:]\n\nfig = plt.figure(figsize=(14,14))\nax1 = fig.add_subplot(3, 3, 1)\nax2 = fig.add_subplot(3, 3, 2)\nax3 = fig.add_subplot(3, 3, 3)\nax4 = fig.add_subplot(3, 3, 4)\nax5 = fig.add_subplot(3, 3, 5)\nax6 = fig.add_subplot(3, 3, 6)\nax7 = fig.add_subplot(3, 3, 7)\nax8 = fig.add_subplot(3, 3, 8)\n\nax1.hist(df[numeric_vars[0]], bins=30)\nax1.set_ylabel(\"Frequency\")\nax1.set_title(numeric_vars[0])\n\nax2.hist(df[numeric_vars[1]], bins=30)\nax2.set_ylabel(\"Frequency\")\nax2.set_title(numeric_vars[1])\n\nax3.hist((df[numeric_vars[2]]), bins=30)\nax3.set_ylabel(\"Frequency\")\nax3.set_title('price')\n\nax4.hist(df[numeric_vars[3]], bins=31)\nax4.set_ylabel(\"Frequency\")\nax4.set_title(numeric_vars[3])\n\nax5.hist(df[numeric_vars[4]], bins=30)\nax5.set_ylabel(\"Frequency\")\nax5.set_title(\"number of reviews\")\n\nax6.hist(df[numeric_vars[5]], bins=30)\nax6.set_ylabel(\"Frequency\")\nax6.set_title(\"last review\")\n\nax7.hist(df[numeric_vars[6]], bins=30)\nax7.set_ylabel(\"Frequency\")\nax7.set_title(numeric_vars[6])\n\nax8.hist(df[numeric_vars[7]])\nax8.set_ylabel(\"Frequency\")\nax8.set_title(numeric_vars[7])\nplt.show()","c2d1972d":"numeric_vars","14265acc":"fig = plt.figure(figsize=(14,14))\nax1 = fig.add_subplot(3, 3, 1)\nax2 = fig.add_subplot(3, 3, 2)\nax3 = fig.add_subplot(3, 3, 3)\nax4 = fig.add_subplot(3, 3, 4)\nax5 = fig.add_subplot(3, 3, 5)\nax6 = fig.add_subplot(3, 3, 6)\nax7 = fig.add_subplot(3, 3, 7)\nax8 = fig.add_subplot(3, 3, 8)\n\nax1.hist(df[numeric_vars[0]], bins=30)\nax1.set_ylabel(\"Frequency\")\nax1.set_title(numeric_vars[0])\n\nax2.hist(df[numeric_vars[1]], bins=30)\nax2.set_ylabel(\"Frequency\")\nax2.set_title(numeric_vars[1])\n\nax3.hist(np.log((df[numeric_vars[2]])), bins=30)\nax3.set_ylabel(\"Frequency\")\nax3.set_title('log(price)')\n\nax4.hist(np.log((df[numeric_vars[3]])), bins=31)\nax4.set_ylabel(\"Frequency\")\nax4.set_title(\"log(minimum nights + 1)\")\n\nax5.hist(np.log(df[numeric_vars[4]]+1), bins=30)\nax5.set_ylabel(\"Frequency\")\nax5.set_title(\"log(number of reviews + 1)\")\n\nax6.hist(np.log(df[numeric_vars[5]]+1), bins=30)\nax6.set_ylabel(\"Frequency\")\nax6.set_title(\"log(last review + 1)\")\n\nax7.hist(np.log(df[numeric_vars[6]]+1), bins=30)\nax7.set_ylabel(\"Frequency\")\nax7.set_title(\"log(calculated host listing count) + 1)\")\n\nax8.hist(np.log(df[numeric_vars[7]]+1), bins=30)\nax8.set_ylabel(\"Frequency\")\nax8.set_title(\"log(availability 365 + 1)\")\n\nplt.show()","8410007f":"for num in numeric_vars[3:]:\n    df[\"log_(\"+num+\" +1)\"] = np.log(df[num]+1)\ndf[\"log_price\"] = np.log(df.price)\ndf=df.drop(columns = numeric_vars[2:]).copy()","4e42dc1a":"df.columns.tolist()","aa3c4550":"df.shape","ce2c16e9":"numeric_vars = df.columns.tolist()[6:8]+df.columns.tolist()[10:]","cd613f98":"numeric_vars","7c90a985":"import seaborn as sns\nx=df[numeric_vars].apply(lambda x: np.log(np.abs(x+1))).corr(method='pearson')\nsns.heatmap(x, annot=True)\nplt.show()","865acfee":"#separate out numerical variables\na=pd.DataFrame(df.dtypes.copy())\nb= a[a[0] == 'object'].reset_index()\n#drop id and host id:\nnon_num=b[\"index\"].tolist()\nprint(non_num)","0e81836e":"y = df.latitude\nx = df.longitude\np = df.log_price\nplt.figure(figsize=(16,9))\nplt.scatter(x,y,c=p,cmap='viridis')\nplt.colorbar()\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\nplt.title(\"Distribution of listing prices\")\nplt.show()","615cc3ac":"grouped = df.groupby(\"neighbourhood\")\nprice_grouped = grouped[\"log_price\"]\nprice = price_grouped.agg([np.mean,np.median,np.max, np.std]).sort_values(\"mean\")\n\n\nfig = plt.figure(figsize=(14,4))\nax1 = fig.add_subplot(1, 3, 1)\nax2 = fig.add_subplot(1, 3, 2)\nax3 = fig.add_subplot(1, 3, 3)\n\nax1.barh(price.index,price[\"mean\"])\nax1.set_yticklabels([])\nax1.set_ylabel(\"Neighborhood\")\nax1.set_xlabel(\"Mean Price\")\nax1.set_title(\"Mean Listing Price per Neighborhood, Sorted\")\nax1.set_xlim(3,7)\n\nax2.barh(price.index,price[\"median\"])\nax2.set_yticklabels([])\nax2.set_ylabel(\"Neighborhood\")\nax2.set_xlabel(\"Median Price\")\nax2.set_title(\"Median Listing Price per Neighborhood\")\nax2.set_xlim(3,7)\n\nax3.barh(price.index,price[\"std\"])\nax3.set_yticklabels([])\nax3.set_ylabel(\"Neighborhood\")\nax3.set_xlabel(\"Standard Deviation of Price\")\nax3.set_title(\"StDev of Listing Prices per Neighborhood\")\nplt.show()","b585ce33":"#One hot encoding\ndf = pd.concat([df, pd.get_dummies(df[\"neighbourhood\"], drop_first=False)], axis=1)\n#save neighborhoods into a list for further analysis:\nneighborhoods = df.neighbourhood.values.tolist()\nboroughs = df.neighbourhood_group.unique().tolist()\n#drop the neighbourhood column from the database\ndf.drop(['neighbourhood'],axis=1, inplace=True)","01927ba2":"df.shape","c03b519c":"grouped = df.groupby(\"room_type\")\nroom_type_price_grouped = grouped[\"log_price\"]\nroom_type_price = room_type_price_grouped.agg([np.mean,np.median,np.max, np.std]).sort_values(\"mean\")\nroom_type_price","f4933322":"sns.boxplot(x=\"room_type\",y=\"log_price\", data=df)\nplt.show()","c552704b":"def removal_of_outliers(df,room_t, nhood, distance):\n    '''Function removes outliers that are above 3rd quartile and below 1st quartile'''\n    '''The exact cutoff distance above and below can be adjusted'''\n\n    new_piece = df[(df[\"room_type\"]==room_t)&(df[\"neighbourhood_group\"]==nhood)][\"log_price\"]\n    #defining quartiles and interquartile range\n    q1 = new_piece.quantile(0.25)\n    q3 = new_piece.quantile(0.75)\n    IQR=q3-q1\n\n    trimmed = df[(df.room_type==room_t)&(df[\"neighbourhood_group\"]==nhood) &(df.log_price>(q1-distance*IQR))&(df.log_price<(q3+distance*IQR))]\n    return trimmed\n\n#apply the function\ndf_private = pd.DataFrame()\nfor neighborhood in boroughs:\n    a = removal_of_outliers(df, \"Private room\",neighborhood,3)\n    df_private = df_private.append(a)\n\ndf_shared = pd.DataFrame()\nfor neighborhood in boroughs:\n    a = removal_of_outliers(df, \"Shared room\",neighborhood,3)\n    df_shared = df_shared.append(a)\n    \ndf_apt = pd.DataFrame()\nfor neighborhood in boroughs:\n    a = removal_of_outliers(df, \"Entire home\/apt\",neighborhood,3)\n    df_apt = df_apt.append(a)\n    \n# Create new dataframe to absorb newly produced data    \ndf_old=df.copy()    \ndf = pd.DataFrame()\ndf = df.append([df_private,df_shared,df_apt])\n\n#plot the results\nfig = plt.figure(figsize=(14,4))\nax1 = fig.add_subplot(1, 2, 1)\nax2 = fig.add_subplot(1, 2, 2)\n\nax1.hist(df_old.log_price)\nax1.set_xlim(2,7)\nax1.set_ylabel(\"Frequency\")\nax1.set_xlabel(\"Log Price\")\nax1.set_title(\"Original price distribution\")\n\nax2.hist(df.log_price)\nax2.set_xlim(2,7)\nax2.set_ylabel(\"Frequency\")\nax2.set_xlabel(\"Log Price\")\nax2.set_title(\"Price distribution after removal of extreme outliers\")\nplt.show()\n\nprint(\"As a result of oulier removal \" + str(df_old.shape[0]-df.shape[0]) + \" rows of data were removed.\")","3af4d611":"df.shape","bae23885":"grouped = df.groupby(\"room_type\")\nroom_type_price_grouped = grouped[\"log_price\"]\nroom_type_price = room_type_price_grouped.agg([np.mean,np.median,np.max, np.std]).sort_values(\"mean\")\nroom_type_price","eb836467":"#convert room types to dummies\ndf = pd.concat([df, pd.get_dummies(df[\"room_type\"], drop_first=False)], axis=1)\ndf.drop(['room_type'],axis=1, inplace=True)","6979940e":"df.shape","f150aac2":"y = df[(df[\"SoHo\"]==1) & (df[\"Private room\"]==1)].latitude\nx = df[(df[\"SoHo\"]==1) & (df[\"Private room\"]==1)].longitude\np = df[(df[\"SoHo\"]==1) & (df[\"Private room\"]==1)].log_price\nplt.scatter(x,y,c=p,cmap='viridis')\nplt.xlim(-74.01,-73.995)\nplt.ylim(40.718,40.73)\nplt.colorbar()\nplt.show()","298e9124":"import datetime as dt\n#convert object to datetime:\ndf[\"last_review\"] = pd.to_datetime(df[\"last_review\"])\n#Check the latest review date in the datebase:\nprint(df[\"last_review\"].max())","cefb4bd2":"df.shape","8f9c97ba":"df[\"last_review\"]=df[\"last_review\"].apply(lambda x: dt.datetime(2019,7,8)-x)\ndf[\"last_review\"]=df[\"last_review\"].dt.days.astype(\"int\").replace(18085, 1900)\nplt.hist(df[\"last_review\"], bins=100)\nplt.ylabel(\"Frequency\")\nplt.xlabel(\"Days since last review\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram of days since last review\")\nplt.show()","496c5b4e":"def date_replacement(date):\n    if date <=3:\n        return \"Last_review_last_three_day\"\n    elif date <= 7:\n        return \"Last_review_last_week\"\n    elif date <= 30:\n        return \"Last_review_last_month\"\n    elif date <= 183:\n        return \"Last_review_last_half_year\"\n    elif date <= 365:\n        return \"Last_review_last year\"\n    elif date <= 1825:\n        return \"Last_review_last_5_years\"\n    else:\n        return \"Last_review_never\" \n\n    \ndf[\"last_review\"]=df[\"last_review\"].apply(lambda x: date_replacement(x))\nsns.boxplot(x=\"last_review\", y=df.log_price, data=df)\nplt.show()","5e4292d9":"grouped = df.groupby(\"last_review\")\nlast_review_price_grouped = grouped[\"log_price\"]\nlast_review_price = last_review_price_grouped.agg([np.mean,np.median,np.max, np.std]).sort_values(\"mean\")\nlast_review_price","5d431d5b":"#convert last review to dummies\ndf = pd.concat([df, pd.get_dummies(df[\"last_review\"], drop_first=False)], axis=1)\ndf.drop([\"last_review\"],axis=1, inplace=True)","d02769d2":"#import necessary libraries\nimport nltk\nimport os\nimport nltk.corpus\nfrom nltk import ne_chunk\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nimport string\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('maxent_ne_chunker')\nnltk.download('words')","48fc4909":"#initiate stopwords\na = set(stopwords.words('english'))\n#obtain text\ntext = df[\"name\"].iloc[10]\n#tokenize text\ntext1 = word_tokenize(text.lower())\n#create a list free of stopwords\nno_stopwords = [x for x in text1 if x not in a]\n#lemmatize the words\nlemmatizer = WordNetLemmatizer() \nlemmatized = [lemmatizer.lemmatize(x) for x in no_stopwords]","de0026c2":"def unique_words1(dwelling):\n\n    apt = df[df[dwelling]==1][\"name\"]\n    a = set(stopwords.words('english'))\n    words = []\n    # append each to a list\n    for lis in range(0, len(apt)):\n        listing = apt.reset_index().iloc[lis,1]\n        #tokenize text\n        text1 = word_tokenize(listing.lower())\n        #create a list free of stopwords\n        no_stopwords = [x for x in text1 if x not in a]\n        #lemmatize the words\n        lemmatized = [lemmatizer.lemmatize(x) for x in no_stopwords]\n        no_punctuation = [x.translate(str.maketrans('','',string.punctuation)) for x in lemmatized]\n        no_digits = [x.translate(str.maketrans('','',\"0123456789\")) for x in no_punctuation ]\n        for item in no_digits:\n            words.append(item)\n\n\n    #create a dictionary\n    unique={}\n    for word in words:\n        if word in unique:\n            unique[word] +=1\n        else:\n            unique[word] = 1\n\n    #sort the dictionary\n    a=[]\n    b=[]\n\n    for key, value in unique.items():\n        a.append(key)\n        b.append(value)\n\n    aa=pd.Series(a)\n    bb=pd.Series(b)    \n\n    comb=pd.concat([aa,bb],axis=1).sort_values(by=1, ascending=False).copy()\n\n    return comb\n\n#apply the function\nprivate = unique_words1(\"Private room\")\nhome = unique_words1(\"Entire home\/apt\")\nshared = unique_words1(\"Shared room\")\n\nwords_private = private.iloc[1:,1]\nwords_home = home.iloc[1:,1] \nwords_shared = shared.iloc[1:,1] \n\n#plot the results\nplt.plot(words_shared.reset_index()[1], label=\"shared\")\nplt.plot(words_private.reset_index()[1], label =\"private\")\nplt.plot(words_home.reset_index()[1], label=\"Entire home\/apt\")\nplt.xlim(0,200)\nplt.ylabel(\"WordFrequency\")\nplt.xlabel(\"Word position on the list\")\nplt.legend()\nplt.show()\n","66247ec8":"home_new = home.reset_index().iloc[1:50,1:3].copy()\nprivate_new = private.reset_index().iloc[1:50,1:3].copy()\nshared_new = shared.reset_index().iloc[1:50,1:3].copy()\n\nall_words = pd.concat([home_new, private_new, shared_new], axis=1, sort=False)\nall_words","27cfc5de":"#see how many listing there are for each type of room:\nprint(\"Numer of shared room listings: \"+str(len(df[df[\"Shared room\"]==1])))\nprint(\"Numer of private room listings: \"+str(len(df[df[\"Private room\"]==1])))\nprint(\"Numer of entire home\/apt listings: \"+str(len(df[df[\"Entire home\/apt\"]==1])))","54662ab5":"#Create a list of the most popular words common for all room types:\nmost_popular_words = home_new.iloc[:,0].tolist()+private_new.iloc[:,0].tolist()+shared_new.iloc[:,0].tolist()\nmost_popular = pd.Series(most_popular_words)\npopular_descriptors=most_popular.unique().tolist()","b4eb234e":"def unique_words2(name, word):\n    '''This function takes individual name and looks for a matching word in it'''\n    a = set(stopwords.words('english'))\n    #tokenize the name\n    text1 = word_tokenize(str(name).lower())\n    #create a list free of stopwords\n    no_stopwords = [x for x in text1 if x not in a]\n    #lemmatize the words\n    lemmatized = [lemmatizer.lemmatize(x) for x in no_stopwords]\n    no_punctuation = [x.translate(str.maketrans('','',string.punctuation)) for x in lemmatized]\n    no_digits = [x.translate(str.maketrans('','',\"0123456789\")) for x in no_punctuation ]\n    counter = 0\n    for item in no_digits:\n        if str(item) == str(word):\n            counter += 1\n        else:\n            continue\n\n    if counter != 0:\n        return 1\n    else:\n        return 0\n    \n#Apply the function \nfor item in popular_descriptors:\n    df[item]= df[\"name\"].apply(lambda x: unique_words2(x,item))","70ff9c57":"#convert last review to dummies\ndf = pd.concat([df, pd.get_dummies(df['neighbourhood_group'], drop_first=False)], axis=1)\ndf.drop(['neighbourhood_group'],axis=1, inplace=True)","715403f3":"#drop unnecessary columns\ndf = df.drop(['id','name','host_id','host_name'], axis=1).copy()\n#copy for later\ndf2 = df.copy()\ndf.shape","1a7638ea":"len(popular_descriptors)","718046f1":"def plot_by_word(word):\n    '''creates a plot of price for listings matching given word'''\n    y = df[(df[word]==1)].latitude\n    x = df[(df[word]==1)].longitude\n    p = df[(df[word]==1)].log_price\n    plt.figure(figsize=(16,9))\n    plt.scatter(x,y,c=p,cmap='viridis')\n    plt.xlabel\n    plt.colorbar()\n    plt.xlabel(\"Longitude\")\n    plt.ylabel(\"Latitude\")\n    plt.title(\"Word 'Luxury' in the name of the listing\\nColormap indicates price\")\n    plt.show()\n    \nplot_by_word(\"Manhattan\")","df4be766":"#import modules:\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\n\nfrom sklearn.tree            import DecisionTreeRegressor\nfrom sklearn.neural_network  import MLPRegressor\nfrom sklearn.linear_model    import LinearRegression\nfrom sklearn.ensemble        import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics         import mean_squared_error\nfrom sklearn.metrics         import r2_score","596a8360":"target = df['log_price'].copy()\n#drop unnecessary columns\ndf = df.drop(['log_price'], axis=1).copy()\n#strip the target column from input columns and put it in front\ndf = pd.concat([target, df], axis=1).copy()\n#select input variable columns\nnums = df.iloc[:,1:]","a3a7baf7":"#first few rows of the dataframe:\ndf.head()","38ab87b1":"#dataframe shape\nprint(df.shape)","a1e0c04a":"#column names in the dataframe\ndf.columns.tolist()","17cb6c52":"y= target\nx = nums\nX_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.20, random_state=1)","ad292fb4":"rmse_dt=[]\ndt = DecisionTreeRegressor()\nkf = KFold(5, shuffle = True, random_state=1)\nmse = cross_val_score(dt ,x,y, scoring = \"neg_mean_squared_error\", cv=kf) \nrmse = np.sqrt(np.absolute(mse))\navg_rmse = np.sum(rmse)\/len(rmse)\nrmse_dt.append(avg_rmse)\nprint(\"Root mean square error: \" +str(round(rmse_dt[0],2)))","5a582c35":"rmse_xg = []\ndata_dmatrix = xgb.DMatrix(data=x,label=y)\nparams = {\n              'colsample_bytree': 0.9,\n              'learning_rate': 0.1,\n              'max_depth': 1, \n              'alpha': 10}\ncv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=5, num_boost_round=300,\n                        early_stopping_rounds=10, metrics=\"rmse\", as_pandas=True, \n                        seed=123)\n    \nrmse_xg.append(cv_results[\"test-rmse-mean\"].tolist()[-1])\nprint(\"Root mean square error: \" +str(round(rmse_xg[0],2)))","3c88db51":"rmse_rf=[]\nrf=RandomForestRegressor(n_estimators = 100, random_state=1,  min_samples_leaf=2)\nkf = KFold(5, shuffle = True, random_state=1)\nmse = cross_val_score(rf ,x,y, scoring = \"neg_mean_squared_error\", cv=kf) \nrmse = np.sqrt(np.absolute(mse))\navg_rmse = np.sum(rmse)\/len(rmse)\nrmse_rf.append(avg_rmse)\nprint(rmse_rf)","a11d672f":"rmse_nndf=[]\nmlp = MLPRegressor(activation='relu', max_iter=1000)\nkf = KFold(5, shuffle = True, random_state=1)\nmse = cross_val_score(mlp ,x,y, scoring = \"neg_mean_squared_error\", cv=kf) \nrmse = np.sqrt(np.absolute(mse))\navg_rmse = np.sum(rmse)\/len(rmse)\nrmse_nndf.append(avg_rmse)\nprint(rmse_nndf)","9022d620":"dt = pd.Series(rmse_dt, name =\"Decision Tree\")\nrand = pd.Series(rmse_rf, name =\"Random Forest\")\nxgb = pd.Series(rmse_xg, name =\"XG Boost\")\nnn = pd.Series(rmse_nndf, name=\"Neural Network\")\npd.concat([dt,rand,xgb,nn],axis=1)","4681582d":"#optimizing number of estimators\ntrain_results = []\ntest_results = []\nn_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200]\nfor estimator in n_estimators:\n    rf = RandomForestRegressor(n_estimators=estimator, n_jobs=-1, random_state=1)\n    rf.fit(X_train, y_train)\n    train_pred = rf.predict(X_train)\n    rmse = round(np.sqrt(mean_squared_error(y_train, train_pred)),2)\n    train_results.append(rmse)\n    y_pred = rf.predict(X_test)\n    rmse = round(np.sqrt(mean_squared_error(y_test, y_pred)),2)\n    test_results.append(rmse)\n    \nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(n_estimators, train_results, 'b', label='Train RMSE')\nline2, = plt.plot(n_estimators, test_results, 'r', label='Test RMSE')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('RMSE')\nplt.xlabel('n_estimators')\nplt.show()","3063e767":"#optimizing max_features\ntrain_results = []\ntest_results = []\nmax_features = ['auto','sqrt','log2']\nfor feature in max_features:\n    rf = RandomForestRegressor(max_features=feature, n_estimators=100, n_jobs=-1, random_state=1)\n    rf.fit(X_train, y_train)\n    train_pred = rf.predict(X_train)\n    rmse = round(np.sqrt(mean_squared_error(y_train, train_pred)),2)\n    train_results.append(rmse)\n    y_pred = rf.predict(X_test)\n    rmse = round(np.sqrt(mean_squared_error(y_test, y_pred)),2)\n    test_results.append(rmse)\n    \nplt.bar(max_features,test_results)\nplt.bar(max_features,train_results)\nplt.ylabel('RMSE')\nplt.xlabel('max_features, test, train')\nplt.show()","183396a6":"#optimizing min_sample_leaf\ntrain_results = []\ntest_results = []\nmin_samples_leaf = [1,2,10,50,70,100]\nfor leaf in min_samples_leaf:\n    rf = RandomForestRegressor(min_samples_leaf = leaf, max_features='auto', n_estimators=100, n_jobs=-1, random_state=1)\n    rf.fit(X_train, y_train)\n    train_pred = rf.predict(X_train)\n    rmse = round(np.sqrt(mean_squared_error(y_train, train_pred)),3)\n    train_results.append(rmse)\n    y_pred = rf.predict(X_test)\n    rmse = round(np.sqrt(mean_squared_error(y_test, y_pred)),3)\n    test_results.append(rmse)\n    \nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(min_samples_leaf, train_results, 'b', label='Train RMSE')\nline2, = plt.plot(min_samples_leaf, test_results, 'r', label='Test RMSE')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('RMSE')\nplt.xlabel('min_sample_leaf')\nplt.show()","06418366":"test_results ","bea7db54":"#apply the hyperparameter optmized model:\nrf=RandomForestRegressor(n_estimators = 300, max_features = 'auto', min_samples_leaf=2, random_state=1)\nrf.fit(X_train,y_train)\npredicted = rf.predict(X_test)","fd9a7f31":"#plot the results of the model:\nplt.figure(figsize=(8,4.5))\nplt.scatter(y_test,predicted, label=\"Model Results\")\nplt.plot([2,7],[2,7], color=\"red\", label = \"Equality Line\")\nplt.title(\"Predictions for test portion of the dataset\")\nplt.xlim(2,7)\nplt.ylim(2,7)\nplt.legend()\nplt.ylabel(\"Predicted log_price\")\nplt.xlabel(\"Actual log_price\")\nplt.show()","1149867f":"print(\"Model accuracy measures for withheld data:\\nR2: \"+str(round(r2_score(y_test,predicted),2)))\nprint(\"Root mean square error: \"+str(round(np.sqrt(mean_squared_error(y_test,predicted)),3)))","f59d3c10":"#derive important features\nfeature_importances = pd.DataFrame(rf.feature_importances_,\n                                   index = X_train.columns,\n                                    columns=['importance']).sort_values('importance', ascending=False)\n\nprint(\"Number of important features: \"+str(feature_importances[feature_importances[\"importance\"]!=0].shape[0]))\nprint(\"\\nTop fifteen features by importance:\")\nfeature_importances[feature_importances[\"importance\"]!=0].head(15)","d157e5f9":"predicted_ = rf.predict(X_test)\npred = pd.DataFrame({'Predicted log_price':predicted_,'log_price':y_test})\ndf_with_predictions = pd.concat([X_test, pred], axis=1).copy()\ndf_with_predictions[\"price\"]=df_with_predictions[\"log_price\"].apply(lambda x: np.exp(x))\ndf_with_predictions[\"predicted_price\"]=df_with_predictions[\"Predicted log_price\"].apply(lambda x: round(np.exp(x),1))","0ede63cb":"prices=df_with_predictions.sort_values(by=\"price\").reset_index()\nprices[\"error\"]=np.abs(prices.price-prices.predicted_price)\/prices.price*100","e1175d45":"plt.figure(figsize=(15,4.5))\nplt.plot(prices[\"predicted_price\"], label=\"Predicted Price\")\nplt.plot(prices[\"price\"], label = \"Actual Price\")\nplt.title(\"Price prediction vs. actual price for listings in the test dataset sorted\")\nplt.xlim(0,10000)\nplt.ylim(0,800)\nplt.legend()\nplt.ylabel(\"Price USD\")\nplt.xlabel(\"Lisitng\")\nplt.show()","c39187ff":"plt.figure(figsize=(15,4.5))\nplt.plot(prices[\"error\"], label=\"Price Error\")\nplt.title(\"Absolute price error % sorted\")\nplt.xlim(0,10000)\nplt.ylim(0,400)\nplt.legend()\nplt.ylabel(\"Price Error (%)\")\nplt.xlabel(\"Lisitng\")\nplt.show()","7b84d067":"small_error = prices[prices.error<20].copy()\ny = small_error[\"latitude\"]\nx = small_error[\"longitude\"]\np = small_error.error\nplt.figure(figsize=(16,9))\nplt.scatter(x,y,c=p,cmap='viridis')\nplt.colorbar()\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\nplt.title(\"Distribution of errors\")\nplt.show()","a3f0711f":"print(prices[prices.Manhattan==1][\"error\"].mean())\nprint(prices[prices.Brooklyn==1][\"error\"].mean())\nprint(prices[prices.Bronx==1][\"error\"].mean())\nprint(prices[prices.Queens==1][\"error\"].mean())\nprint(prices[prices[\"Staten Island\"]==1][\"error\"].mean())","14a0f683":"#define random forest function with kfold cross-validation\ndef random_forest(df):\n    target = df['log_price'].copy()\n    #select input variable columns\n    nums = df.iloc[:,1:]\n\n    #split the data into test and train\n    y= target\n    x = nums\n    X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.20, random_state=1)\n\n    rmse_rf=[]\n    rf=RandomForestRegressor(n_estimators = 300, max_features = 'auto', min_samples_leaf=1, random_state=1)\n    kf = KFold(5, shuffle = True, random_state=1)\n    mse = cross_val_score(rf ,x,y, scoring = \"neg_mean_squared_error\", cv=kf) \n    rmse = np.sqrt(np.absolute(mse))\n    avg_rmse = np.sum(rmse)\/len(rmse)\n    rmse_rf.append(avg_rmse)\n    return rmse_rf","5a656349":"#separate datasets\nprivate = df[df[\"Private room\"]==1].copy()\nshared = df[df[\"Shared room\"]==1].copy()\nhomes = df[df[\"Entire home\/apt\"]==1].copy()\n\nprivate_rmse = random_forest(private)\nshared_rmse = random_forest(shared)\nhome_rmse = random_forest(homes)\n\nprint(\"\\nShared RMSE: \"+str(round(shared_rmse[0],3)))\nprint(\"Private RMSE: \"+str(round(private_rmse[0],3)))\nprint(\"Home RMSE: \"+str(round(home_rmse[0],3)))","c33968c4":"print(private.log_price.std())\nprint(homes.log_price.std())","5c82aa34":"#separate datasets\nmanhattan = df[(df[\"Manhattan\"]==1)].copy()\nbrooklyn = df[(df[\"Brooklyn\"]==1)].copy()\nqueens = df[(df[\"Queens\"]==1)].copy()\nbronx = df[(df[\"Bronx\"]==1)].copy()\nstaten_island = df[(df[\"Staten Island\"]==1)].copy()\n\nmanhattan_rmse = random_forest(manhattan)\nbrooklyn_rmse = random_forest(brooklyn)\nqueens_rmse = random_forest(queens)\nbronx_rmse = random_forest(bronx)\nstaten_island_rmse = random_forest(staten_island)\n\nprint(\"\\nManhattan RMSE: \"+str(round(manhattan_rmse[0],3)))\nprint(\"Brooklyn RMSE: \"+str(round(brooklyn_rmse[0],3)))\nprint(\"Queens RMSE: \"+str(round(queens_rmse[0],3)))\nprint(\"Bronx RMSE: \"+str(round(bronx_rmse[0],3)))\nprint(\"Staten Island RMSE: \"+str(round(staten_island_rmse[0],3)))","fd49a85b":"print(\"Number of listings in Manhattan: \"+str(len(manhattan)))\nprint(\"Number of listings in Brooklyn: \"+str(len(brooklyn)))\nprint(\"Number of listings in Queens: \"+str(len(queens)))\nprint(\"Number of listings in Bronx: \"+str(len(bronx)))\nprint(\"Number of listings in Staten Island: \"+str(len(staten_island)))","5ded3c0d":"plt.scatter(x=[0.384,0.369,0.369,0.419,0.458],y=[21192,19801,5592,1071,370])\nplt.xlabel(\"RMSE\")\nplt.ylabel(\"Number of listings in a borough\")\nplt.title(\"Listing number vs. Model RMSE\")\nplt.show()","32894ddb":"#separate datasets\nmanhattan = df[(df[\"Manhattan\"]==1)&(df[\"Private room\"]==1)].copy()\nbrooklyn = df[(df[\"Brooklyn\"]==1)&(df[\"Private room\"]==1)].copy()\nqueens = df[(df[\"Queens\"]==1)&(df[\"Private room\"]==1)].copy()\nbronx = df[(df[\"Bronx\"]==1)&(df[\"Private room\"]==1)].copy()\nstaten_island = df[(df[\"Staten Island\"]==1)&(df[\"Private room\"]==1)].copy()\n\nmanhattan_rmse = random_forest(manhattan)\nbrooklyn_rmse = random_forest(brooklyn)\nqueens_rmse = random_forest(queens)\nbronx_rmse = random_forest(bronx)\nstaten_island_rmse = random_forest(staten_island)\n\nprint(\"\\nManhattan RMSE: \"+str(round(manhattan_rmse[0],3)))\nprint(\"Brooklyn RMSE: \"+str(round(brooklyn_rmse[0],3)))\nprint(\"Queens RMSE: \"+str(round(queens_rmse[0],3)))\nprint(\"Bronx RMSE: \"+str(round(bronx_rmse[0],3)))\nprint(\"Staten Island RMSE: \"+str(round(staten_island_rmse[0],3)))","62ac8cc3":"\ntarget = df2['log_price'].copy()\n#drop unnecessary columns\ndf = df2.drop(['log_price'], axis=1).copy()\n#strip the target column from input columns and put it in front\ndf = pd.concat([target, df], axis=1).copy()\n#select input variable columns\nnums = df.iloc[:,1:]","598b65b7":"# RMSE for the global model considering Queens and Private room only\nqns_priv_price=df_with_predictions[(df_with_predictions[\"Queens\"]==1)&(df_with_predictions[\"Private room\"]==1)].log_price\nqns_priv_predprice=df_with_predictions[(df_with_predictions[\"Queens\"]==1)&(df_with_predictions[\"Private room\"]==1)][\"Predicted log_price\"]\nrmse_global = round(np.sqrt(mean_squared_error(qns_priv_price, qns_priv_predprice)),3)\nprint(rmse_global)","7787189c":"#separating the data to make it specific to the borough of Queens and the listing type\ndf=df[(df[\"Queens\"]==1)&(df[\"Private room\"]==1)].copy()\n\ntarget = df['log_price'].copy()\n#drop unnecessary columns\ndf = df.drop(['log_price'], axis=1).copy()\n#strip the target column from input columns and put it in front\ndf = pd.concat([target, df], axis=1).copy()\n#select input variable columns\nnums = df.iloc[:,1:]\n\ny= target\nx = nums\nX_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.20, random_state=1)\n\nrf=RandomForestRegressor(n_estimators = 300, max_features = 'auto', min_samples_leaf=2, random_state=1)\nrf.fit(X_train,y_train)\npredicted = rf.predict(X_test)","2bd0c62c":"round(np.sqrt(mean_squared_error(y_test, predicted)),3)","710d7d11":"def final_model(borough, room_type):\n    '''Build a function specifc to a borough and room_type'''\n    #read the cleaned data from a file:\n    df = df2.copy()\n    #filter the data\n    df=df[(df[borough]==1)&(df[room_type]==1)].copy()    \n    target = df['log_price'].copy()\n    #drop unnecessary columns\n    df = df.drop(['log_price'], axis=1).copy()    \n    #strip the target column from input columns and put it in front\n    df = pd.concat([target, df], axis=1).copy()\n    #select input variable columns\n    nums = df.iloc[:,1:]\n    \n    y= target\n    x = nums\n    X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.20, random_state=1)\n\n    rf=RandomForestRegressor(n_estimators = 300, max_features = 'auto', min_samples_leaf=2, random_state=1)\n    rf.fit(X_train,y_train)\n    predicted = rf.predict(X_test)\n    y_test = y_test.values.tolist()\n    predicted = predicted.tolist()\n    return y_test, predicted ","8fb9a212":"boroughs = [\"Manhattan\", \"Brooklyn\", \"Bronx\",\"Queens\",\"Staten Island\"]\nlistings= [\"Private room\",\"Shared room\",\"Entire home\/apt\"]\n\nactual=[]\npredicted=[]\nfor borough in boroughs:\n    for listing in listings:\n        a,b = final_model(borough, listing)\n        actual +=a\n        predicted +=b\n        \nround(np.sqrt(mean_squared_error(actual, predicted)),3)","863f9fe0":"#plot the results of the model:\nplt.figure(figsize=(8,4.5))\nplt.scatter(actual, predicted, label=\"Model Results\")\nplt.plot([2,7],[2,7], color=\"red\", label = \"Equality Line\")\nplt.title(\"Predictions for test portion of the dataset\")\nplt.xlim(2,7)\nplt.ylim(2,7)\nplt.legend()\nplt.ylabel(\"Predicted log_price\")\nplt.xlabel(\"Actual log_price\")\nplt.show()","ba83ec8b":"print(\"Model accuracy measures for withheld data:\\nR2: \"+str(round(r2_score(actual,predicted),3)))\nprint(\"Root mean square error: \"+str(round(np.sqrt(mean_squared_error(actual,predicted)),3)))","297dcb01":"Looking at the distributions, clearly the following are heavily right skewed:\n- price\n- minimum_nights\n- number of reviews\n- last review\n- calculated_host_lising_count\n\nOne way to reduce the skeweness is to logarithmically transform the distributions:","89a9b3a6":"### XG Boost - kfold","03f69078":"Here see if the global model and deliver the same performance as the specialized model for the same set of data. For instance only Manhattand and only private room.","5ae7fbb0":"#### Let's start by looking at the distributions:","c1db38d3":"Run individual models and combine the predictions","a20c5d24":"### Impact of Borough on the model's accuracy","ab6f38eb":"These belong to:\n\n#### Host descriptors:\n- __host_name:__ name of the host\n\n#### Listing descriptors:\n- __name:__ name of the listing\n- __room_type:__ listing space type\n\n#### Review descriptors:\n- __last_review:__ latest review\n\n#### Location descriptors:\n- __neighbourhood_group:__ location\n- __neighbourhood:__ area\n","d2a01006":"### Comments:\n\nOn the low end of the actual log_price the results tend to cluster above the line, while on the high end they tend to cluster below the line. \n\nThis has consequences of underpredicting high prices and overpredicting low prices.\n\nNext, let's take a look at what features are important in the model:","b4177983":"We already know that location is an important price determinant. Hence let's dig a bit deeper to see how mean prices vary by neighborhood","398839d0":"The most important features that factor into the price of a listing are:\n\n- listing type (if it is a home\/apartment)\n- location, which is very intuitive considering that in real estate location is often a decided factor for price\n- availability and review related factors\n- certain listing descriptor words indicating the character or location of a listing\n\n### Further visualization of results\n\nThe three graphs below illustrate the pricing in USD:\n- first graph illustrates actual and predicted price for the test dataset in order of growing price\n- second graph illustrates associated % error\n- finally the third graph visualizes the distribution of errors with respect to latitude and longitude","bd90f9df":"Time since last review does not show any clear trend on the sale of entire dataset. Yet it may be important to keep the categorical values included for later machine learning experiments.","dbe3411e":"Looking at the left-hand side histogram of minimum nights required to rent the property, there appears to be many listings targeted to rent for a minimum period of over 31 days.\n\nAs mentioned previously, in order to narrow the scope of this analysis I will focus on a potential tourist market, assuming that tourists will not want to stay longer than 31 days. Hence, I will remove any listings with required minimum nights greater than 31.\n\nThe updated distribution is plotted in the right-hand side histogram. It appears that the distribution is bimodal. There are two peaks of minimum nights. First peak maximizes at 1 night, while the other at 30 nights. This indicates, that the limit of 31 days that I set, captures renters interested in at least a month long rental period.","0bd36e5b":"The graph above inticates that the model provides the best performance if the minimum samples on a leaf is at least 1\n\nBased on the hyperparameter optimization the best hyperparameter conditions are:\n- n_estimator = 100\n- max_features = 'auto'\n- min_samples_leaf = 2\n\nNext, apply these parameters to see how the overall model performs:","016ac5d0":"## Experiment with several ML approaches","fa76e744":"### Decision Tree Regression - kfold","fac05240":"Based on current findings it is worth to one-hot encode the neighborhood variable:","25d28969":" ### Proceed to Machine Learning\n\nWith data cleaning and feature engineering complete, it is time now to apply machine learning algorithms to develop an approprie price prediction model and derive futher insights from the data.","5bfa1644":"- Clearly, the model overfits since the RMSE for train dataset is much lower then the test dataset regardless of the number of estimators\n- For the best model performance on unseen data, a minimum of 100 estimators appears to be sufficient","77ea6651":"At this point we can also take a closer look into number based variables to see if any of them should be turned into categorical variables:","b645cf2c":"## Data Cleaning\n\nBefore applying any machine learning algorithms I will prepare the dataset by turning any potentially useful data into ML accessible format.","8b7c248b":"#### Further analysis","1fb6c4cd":"There is no readily apparent trend of recency of reviews with respect to room price visible. Further analysis may reveal more insight.","eb81174e":"#### The next obvious variable to evaluate is Room type. This variable provides information about relative privacy that comes with the listing as well as its size","e65db13d":"With one hot encoding, feature engineering and natural language processing the shape of the dataframe grew substantially. \n- Several numeric columns have been log transformed\n- Individual neighborhoods have been one hot encoded along with boroughs and categorized times since the last review\n- Lastly columns were created to document use of popular words in the listing name\n\n\nNext, let's use the entire dataset and feed it into some of the most common regression models to see what sort of root mean square error we get:","d4093637":"### Taking care of dataset missing values\nReplace missing values\n\n- listings without reviews have missing values for last_review and reviews_per_month. For these lisitngs the missing values will be replaced by 0\n\n- Some listings are missing a name or the host name is missing. These will be replaced with \"None\"","38b74ee1":"# Predicting Price of Airbnb Listings in NYC - Part 1\n\n\n\nThe following notebook showcases my analysis of AirBnB listings dataset originally posted on Kaggle by Dgomonov. The listings were scrapped on July 8th 2019 and are specific to NYC, NY.\n\nLink to the dataset: https:\/\/www.kaggle.com\/dgomonov\/new-york-city-airbnb-open-data\n\nThe objective of the analysis is to:\n\n- estimate listing price based on provided information\n- derive additional useful and interesting insights\n\n\nPart 1 deals with taking the existing dataset, performing data cleaning, feature engineering and running preliminary analysis. The product of this part is a data file for Machnie Learning analysis.\n\nPart 2 - The Machine Learning part of the project applies machine learning algorithms to predict price of lisings based on various input variables. ","09024b07":"Let's first review the final dataframe that will be used for the analysis:","b77c6ca0":"Based on the Queens\/Private bedroom example it appears to be more beneficial to train several models specific to the listing type and location (such as Borough).\n\nMaking a model specific improved RMSE from 0.334 to 0.309 or by 7.5%.\n\nLet's see if taking the approach of using a model specifc to borough and listing type can improve the overall RMSE for the entire data set.","07ec2964":"####  Last review is a data when last review has been posted. Perhaps the most effective way of dealing with this variable is to\n- covert it to number of days since last review counting down from the data the data was scraped off the web\n- categorize","808916c0":"- Private room dataset yielded the best model accuracy followed by home and shared. \n- The difference in accuracy between shared and the other two datasets is most likely derived from fewer datapoints available to train the shared model, resulting in lower accuracy.\n- It is clear that the model predicts with better accuracy for private listings than for the home\/apartment listings. It is likely driven by larger spread of prices within the home\/apartments listings relative to private listings. Look at standard deviations for the two population samples below:","fd77c947":"### Neural Network  - kfold","4102049a":"Below is a statistical summary of the columns in the dataframe. We can conclude that:\n- there are 38708 rows of data being considered\n- all listings are between 40.499 and 40.913 latitude and -74.244 and -73.712 longitude squarely fitting into NYC's geographic location\n- the listing price ranges between 10USD and 10,000USD with mean price of 149USD and standard deviation of 219USD suggesting a very broad price range distribution\n- the minimum nights limit ranges betweeen 1 and 31 as determined above. It averages at 5.65, but as we have seen it is not normally distributed\n- on average the listings are available 110 days per year. It would be interesting to understand if the listings are available in short intervals (such as weekends) or long intervals (several months at a time). Perhaphs this can be understood by looking at the correlation between minimum nights and availability.","456f5280":"Transform the variables","ab111765":"There is no apparent geographic concentration of error visible in the graph above.\n\n\nThe following are mean price errors for each borough in USD.","9d10dcb5":"#### Next let's turn the attention to non numerical variables","56db64e3":"#### Next, lets shift focus to the minimum nights \n\nHere I am plotting a histogram of minimum nights of rental required in the listing:","7128047e":"### Extreme Outliers\n- The data most likely includes extreme outliers, which will be difficult to model. \n- The following code will remove extreme outliers by borough.\n- An outlier is defined as 3 x IQR below 25th quantile and above 75th quantile\n- The code will also treat each room type differently to avoid data bias","3cc49ee9":"#### Natural Language Processing","4b5d9605":"inital analysis indicates that there are 11,492 more listings than hosts (identifiable and unidentifiable). This means that some hosts may list several properties. Let's verify that.\n\n\nEach listing contains \"Calculated_host_listings_count\", which is a count of total listing by a specific host in the provided data. The logic below derives value of the calculated host listings count for a specific listing (36485609 in this example) and checks if it is equal to the total number of listings by that host (whose host ID is 30985759) in the dataset\n\nThe comparison yields true, meaning that \"calculated_host_listings_count\"  for the specific host indicated gives an accurate number of listings posted by the same host. Properties could then easily related by the host ID.","e75cb060":"The last review in the database dates to July 8th 2019, which will be used as time zero for analysis:","d5e60c5c":"At this point there are no null values in the dataframe:","e5e5df7d":"### Following are the dataset columns:\n\nI grouped the columns into column categories for enhanced understanding of the dataset\n\n#### Host descriptors:\n- __host_id:__ host ID\n- __host_name:__ name of the host\n- __calculated_host_listings_count:__ amount of listing per host\n\n#### Listing descriptors:\n- __id:__ listing ID\n- __name:__ name of the listing\n- __room_type:__ listing space type\n- __minimum_nights:__ amount of nights minimum\n- __availability_365:__ number of days when listing is available for booking\n- __price:__ price in dollars\n\n#### Review descriptors:\n- __number_of_reviews:__ number of reviews\n- __last_review:__ latest review\n- __reviews_per_month:__ number of reviews per month\n\n#### Location descriptors:\n- __neighbourhood_group:__ location\n- __neighbourhood:__ area\n- __latitude:__ latitude coordinates\n- __longitude:__ longitude coordinates\n\n\n### General approach\n\nPrior to diving into the data it is worth to perform a thought experiment in which we ask ourselves what factors may be driving the lisiting prices for the given dataset, considering that the listings were most likely scrapped on July 8th 2019 (Monday), right after the Independence Day in New York City. \n\nI will also limit my analysis to lisitngs that are confined to 31 minimum nights at most, in order to exclude long term rentals. Therefore, my analysis will target short term renters, most likley vistors to the city, majority of whom may be tourists.\n\nHere are a few items I brainstormed:\n- neighborhood\n- proximity to landmark objects\n- proximity to public transportation (subways mostly)\n- size of the dwelling (shared v. private v. apartment)\n- prior reviews\n- quality of the listing (ie. is the room pretty\/tidy etc)\n\n#### With that let's dive into data!\n\nThis is a snapshot of our data:","1b02ac7a":"### Random Forest Regression - kfold","1fada319":"### Impact of Private v. Shared v. Home on the model's accuracy\n\nLet's look at model efficiency differences when considering different listing types (shared, private, apartment):","1048bdcd":"Replace with the following categories for simplification:\n- last month\n- last 6 months\n- last year\n- last 5 years\n- never","6de8811b":"While the approach of building individual models worked well for certain cases, overall the RMSE of 0.378 was higher than the one achieved using the entire dataset (0.372)\n\n### In summary:\n\n- Random Forest regression model provided best accuracy for prediction of listing price based on variables generated from the initial data\n- the model as is tends to underpredict listings priced relatively high\n- the model tends to underpredict listings priced relatively low\n- the model importances can be used to further understand what drives the price of an Airbnb listing in NYC\n- RMSE are given based on log_price values","212bae90":"Note that logarithmic data transformation was used to smooth out the distributions. \n\n#### Let's see if any correlations between variables start to emerge:","4c4dd7c6":"#### Observations on price:\n- price seems to be positively correlated with longitude meaning that one can expect higher prices as position in NYC moves West. This is expected because Manhattan, which is the most expensive borough of the city, is located on the west side of the city\n- latitude seems to have lesser effect on the price. However, there is a slight indication of higher prices located in the northern parts of the city\n- price is also positively correlated with: increasing availability, the fact that the property is rented by a host who lists other properties, and increasing number of minimum nights\n- price is negatively correlated with number of reviews and reviews per month, indicating that it is possible that the prior reviews could depress the prices to some extent\n\n#### Other interesting observations:\n- calculated host lisitng count is positively correlated with minimum nights and availability_365 indicating that hosts who list more than one property may be more strategic rather than opportunisitic about their rentals. That may attempt to maximize the amount of time a single renter stays at their property to minimize turnover cost. They also tend to maximize the amount of time the property is being rented.","d23ea0ce":"### Comments:\n\nRandom Forest model yields the lowest RMSE, followed by Neural Netork, XGBoost and Decision Tree\n\nSince random forest yielded the best performance, let's keep optimizing it:","9fbdf955":"The scatter plot above visualizes the geographical distribution of listings along with the relative pricing (increasing with brightening color).\n\nHigh prices appear to be concentrated around Manhattan starting with neighborhoods around Central Park going south, as well as around portions of Brooklyn and Queens close to Manhattan.\n\nLet's group the data by neighborhood, deriving mean pricing for each neighborhood:","00fc76c4":"There are also large differences in model performance from borough to borough. The graph above illustrates that as the number of listings in a given borough decreases, the model accuracy decreases as well. The only exception is Queens where in spite of a realtively small amount of listings, the model delivered a very good performance.\n\nAs a next step let's check model accuracy for each borough considering only private rooms."}}