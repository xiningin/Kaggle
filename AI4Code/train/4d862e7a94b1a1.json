{"cell_type":{"cdb9d04d":"code","91554f88":"code","fd8d938b":"code","01aa8b0c":"code","1710cad3":"code","920cbc13":"code","cf9f1e3d":"code","8e2ad8c7":"code","ed60777c":"code","e50022d3":"code","35066b70":"code","d429cb34":"code","9cce353f":"code","42c5d44b":"code","0c7d5a50":"code","26bdb706":"code","8112174f":"code","abb11c67":"code","bcdf7cd9":"code","2253babe":"code","2c35eee8":"code","b14c6d87":"code","b4afdd78":"code","11e9017e":"code","1f5036a3":"code","f907fd07":"code","10f12646":"code","904102b5":"code","510c3adc":"code","61ff3093":"code","b7994b09":"code","8f29b821":"code","7bcbe67f":"code","ff7a251e":"code","ed62b1e6":"code","04e77169":"code","10babd8a":"code","53312a75":"code","32b6ba2c":"code","5e760d1b":"code","6ff9364f":"code","bfbb6392":"code","515f26ae":"code","4036a81f":"code","13242bc5":"code","6cc0b9fd":"code","b6f02590":"code","ba99a51b":"code","38b2f905":"code","0c180dd1":"code","30508058":"code","338d1f8e":"code","24487319":"code","dfd4182e":"code","02f1012c":"code","ea1fed1c":"code","8bdc36f7":"code","13d9f058":"code","aeeb175e":"code","ffbf2295":"code","405ce2aa":"code","1ef9b2e8":"code","b757e8b3":"code","f14799ab":"code","aa026a88":"code","7fdeafa3":"code","a972ff0a":"code","3ecc2a3b":"code","2633887f":"code","a049832b":"markdown","b400a7c9":"markdown","4114b325":"markdown","62eb1a95":"markdown","4323021a":"markdown","f0724bb6":"markdown","7d55f705":"markdown","18eeeb79":"markdown","874b2787":"markdown","9299bd75":"markdown","ddfe1bb6":"markdown","e2620dad":"markdown","04221774":"markdown","0d0ee311":"markdown","00ec3d73":"markdown","2b94c5fb":"markdown","2ad77ba1":"markdown","ac818e62":"markdown","9bb382bb":"markdown","71c8fc76":"markdown","e74852fb":"markdown","7ae5b2d5":"markdown","406c6206":"markdown","7d93ab04":"markdown","4dfa4a0a":"markdown","f097f4d6":"markdown","3cd0394d":"markdown","3f88ea28":"markdown","8806e81c":"markdown","baf38a72":"markdown"},"source":{"cdb9d04d":"import pandas as pd\nimport numpy as np\n\nfrom itertools import product\nimport gc\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom multiprocessing import Pool\n\nimport lightgbm as lgb","91554f88":"from matplotlib import style\nstyle.use('seaborn')\n\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)","fd8d938b":"train = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\ntest = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv').set_index('ID')\nitems = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\nshops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')\ncategories = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')","01aa8b0c":"print(train.shape)\ntrain.head(10)","1710cad3":"train.drop_duplicates(inplace=True, ignore_index=True)","920cbc13":"sns.boxplot(x=train['item_cnt_day'])\nplt.show()","cf9f1e3d":"sns.boxplot(x=train['item_price'])\nplt.show()","8e2ad8c7":"train.loc[train['item_cnt_day'].argmax()]","ed60777c":"items[items['item_id'] == 11373]","e50022d3":"sns.distplot(train[train['item_id']==11373]['item_cnt_day'].values)\nplt.show()","35066b70":"train[train['item_cnt_day'] == 1000]","d429cb34":"items[items['item_id'] == 20949]","9cce353f":"sns.distplot(train[train['item_id']==20949]['item_cnt_day'].values)\nplt.show()","42c5d44b":"train.iloc[train['item_price'].argmax()]","0c7d5a50":"items[items['item_id'] == 6066]","26bdb706":"test[test['item_id'] == 6066]","8112174f":"train.iloc[train['item_price'].argmin()]","abb11c67":"train.loc[train['item_price'].argmin(), 'item_price'] = train[train['item_id'] == 2973].item_price.mean()","bcdf7cd9":"train = train[train['item_cnt_day'] <= 1000]","2253babe":"train = train[train['item_price'] < 300000]","2c35eee8":"cols = ['date_block_num', 'shop_id', 'item_id']","b14c6d87":"shops","b4afdd78":"train.loc[train['shop_id'] == 0, 'shop_id'] = 57\ntest.loc[test['shop_id'] == 0, 'shop_id'] = 57\ntrain.loc[train['shop_id'] == 1, 'shop_id'] = 58\ntest.loc[test['shop_id'] == 1, 'shop_id'] = 58\ntrain.loc[train['shop_id'] == 10, 'shop_id'] = 11\ntest.loc[test['shop_id'] == 10, 'shop_id'] = 11\ntrain.loc[train['shop_id'] == 40, 'shop_id'] = 39\ntest.loc[test['shop_id'] == 40, 'shop_id'] = 39","11e9017e":"%%time\ndata = []\nfor block in range(34):\n    tmp = train[train['date_block_num'] == block]\n    data.append(np.array(list(product([block], tmp['shop_id'].unique(), tmp['item_id'].unique())), dtype='int16'))\n\ndel tmp\n\ndata = pd.DataFrame(data=np.vstack(data), columns=cols)","1f5036a3":"data['date_block_num'] = data['date_block_num'].astype('int8')\ndata['shop_id'] = data['shop_id'].astype('int8')\ndata.dtypes","f907fd07":"data.sort_values(cols, inplace=True)","10f12646":"group = train.groupby(cols).agg({'item_cnt_day': 'sum'})\ngroup.columns = ['target']\ngroup.reset_index(inplace=True)\n\ndata = data.merge(group, how='left', on=cols)\n\ndata['target'] = data['target'].fillna(0).clip(0, 20).astype('float16')","904102b5":"data.to_hdf('data.hdf5', 'df')","510c3adc":"data = pd.read_hdf('data.hdf5', 'df')\ndata","61ff3093":"shops","b7994b09":"shops['city_name'] = shops['shop_name'].apply(lambda x: x.split(' ')[0])\nshops.replace('!\u042f\u043a\u0443\u0442\u0441\u043a', '\u042f\u043a\u0443\u0442\u0441\u043a', inplace=True)\nshops['city_name'], _ = pd.factorize(shops['city_name'])\nshops['city_name'] = shops['city_name'].astype('int8')\n\nshops.head(10)","8f29b821":"categories['category_general_name'] = categories['item_category_name'].apply(lambda x: x.split(' ')[0])\ncategories['category_general_name'], _ = pd.factorize(categories['category_general_name'])\ncategories['category_general_name'] = categories['category_general_name'].astype('int8')\n\ncategories.head()","7bcbe67f":"test['date_block_num'] = 34\ntest['shop_id'] = test['shop_id'].astype('int8')\ntest['date_block_num'] = test['date_block_num'].astype('int8')\ntest['item_id'] = test['item_id'].astype('int16')","ff7a251e":"data = pd.concat([data, test], ignore_index=True).fillna(-1)\ndata","ed62b1e6":"data = data.merge(shops[['shop_id', 'city_name']], how='left', on='shop_id')","04e77169":"a = pd.merge(items[['item_id', 'item_category_id']], categories[['item_category_id','category_general_name']], how='left', on='item_category_id')\ndata = data.merge(a, how='left', on='item_id')\ndel a","10babd8a":"data['item_category_id'] = data['item_category_id'].astype('int8')","53312a75":"data['month'] = data['date_block_num'] % 12 + 1","32b6ba2c":"def lag_generator(df, col, lags):\n    tmp = df[['date_block_num', 'shop_id', 'item_id', col]]\n    for lag in lags:\n        a = tmp.copy()\n        a['date_block_num'] += lag\n        a.columns = ['date_block_num', 'shop_id', 'item_id', f'{col}_lag_{lag}']\n        df = df.merge(a, how='left', on=['date_block_num', 'shop_id', 'item_id'])\n    return df","5e760d1b":"%%time\nlags = [1, 2, 3]\ndata = lag_generator(data, 'target', lags)\n\ndata.fillna(-1, inplace=True)","6ff9364f":"%%time\ngroup = data.groupby(['date_block_num', 'shop_id']).agg({'target': 'mean'})\ngroup.columns = ['target_mean_date_shop']\ngroup.reset_index(inplace=True)\n\ndata = data.merge(group, how='left', on=['date_block_num', 'shop_id']).fillna(-1)\n\ndata = lag_generator(data, 'target_mean_date_shop', [1])\ndata.drop(columns='target_mean_date_shop', inplace=True)\n\ndata.fillna(-1, inplace=True)","bfbb6392":"%%time\ngroup = data.groupby(['date_block_num', 'item_id']).agg({'target': 'mean'})\ngroup.columns = ['target_mean_date_item']\ngroup.reset_index(inplace=True)\n\ndata = data.merge(group, how='left', on=['date_block_num', 'item_id']).fillna(-1)\n\ndata = lag_generator(data, 'target_mean_date_item', [1, 2, 3])\ndata.drop(columns='target_mean_date_item', inplace=True)\n\ndata.fillna(-1, inplace=True)","515f26ae":"%%time\ngroup = data.groupby(['date_block_num', 'item_category_id']).agg({'target': 'mean'})\ngroup.columns = ['target_mean_date_category']\ngroup.reset_index(inplace=True)\n\ndata = data.merge(group, how='left', on=['date_block_num', 'item_category_id']).fillna(-1)\n\ndata = lag_generator(data, 'target_mean_date_category', [1])\ndata.drop(columns='target_mean_date_category', inplace=True)\n\ndata.fillna(-1, inplace=True)","4036a81f":"%%time\ngroup = data.groupby(['date_block_num', 'city_name']).agg({'target': 'mean'})\ngroup.columns = ['target_mean_date_city']\ngroup.reset_index(inplace=True)\n\ndata = data.merge(group, how='left', on=['date_block_num', 'city_name']).fillna(-1)\n\ndata = lag_generator(data, 'target_mean_date_city', [1])\ndata.drop(columns='target_mean_date_city', inplace=True)\n\ndata.fillna(-1, inplace=True)","13242bc5":"%%time\ngroup = data.groupby(['date_block_num', 'category_general_name']).agg({'target': 'mean'})\ngroup.columns = ['target_mean_date_gencategory']\ngroup.reset_index(inplace=True)\n\ndata = data.merge(group, how='left', on=['date_block_num', 'category_general_name']).fillna(-1)\n\ndata = lag_generator(data, 'target_mean_date_gencategory', [1])\ndata.drop(columns='target_mean_date_gencategory', inplace=True)\n\ndata.fillna(-1, inplace=True)","6cc0b9fd":"data.to_hdf('data1.hdf5', 'df')","b6f02590":"data = pd.read_hdf('data1.hdf5', 'df')\ndata","ba99a51b":"group = train.groupby('item_id').agg({'item_price': 'mean'})\ngroup['item_price'] = group['item_price'].astype('float32')\ngroup.columns = ['item_mean_price']\ngroup.reset_index(inplace=True)\n\ndata = data.merge(group, how='left', on='item_id')","38b2f905":"%%time\ngroup = train.groupby(['shop_id', 'item_id'], sort=False)['date_block_num'].unique()\n\ngroup.name = 'last_sales'","0c180dd1":"data = data.merge(group.reset_index(), how='left', on=['shop_id', 'item_id'])","30508058":"def find_prev_sel(arr):\n    try:\n        date_block = arr[0]\n        last_sale = arr[1]\n        return last_sale[last_sale < date_block].max()\n    except:\n        return np.nan","338d1f8e":"%%time\npool = Pool(2)\n\ndata['last_sale'] = pool.map(find_prev_sel, data[['date_block_num', 'last_sales']].values)\n\npool.close()\npool.join()","24487319":"group = train.groupby(['item_id', 'date_block_num'], as_index=False, sort=False).agg({'item_price': 'mean'})\ngroup.columns = ['item_id', 'last_sale', 'item_date_mean_price_prev_sale']\n\ndata = data.merge(group, how='left', on=['item_id', 'last_sale'])","dfd4182e":"data['delta_item_prev_price'] = data['item_mean_price'] - data['item_date_mean_price_prev_sale']\ndata['prev_sold_delta'] = data['date_block_num'] - data['last_sale']","02f1012c":"data.drop(columns=['last_sale', 'item_mean_price', 'item_date_mean_price_prev_sale', 'last_sales'], inplace=True)","ea1fed1c":"data.fillna(-1, inplace=True)","8bdc36f7":"data['prev_sold_delta'] = data['prev_sold_delta'].astype('int8')\ndata['delta_item_prev_price'] = data['delta_item_prev_price'].astype('float32')","13d9f058":"train['revenue'] = train['item_price'] * train['item_cnt_day']\ngroup = train.groupby(['date_block_num', 'shop_id']).agg({'revenue': 'sum'})\ngroup.columns = ['revenue_lag_1']\ngroup.reset_index(inplace=True)\ngroup['date_block_num'] += 1\n\ndata = data.merge(group, how='left', on=['date_block_num', 'shop_id'])\n\ndata.fillna(-1, inplace=True)","aeeb175e":"data.to_hdf('data2.hdf5', 'df')","ffbf2295":"data = pd.read_hdf('data2.hdf5', 'df')\ndata","405ce2aa":"del train\ndel items\ndel test\ndel shops\ndel categories\ndel group\n\ngc.collect()","1ef9b2e8":"data = data[data['date_block_num'] >= 3]","b757e8b3":"train_data = lgb.Dataset(data[data['date_block_num'] < 33].drop(columns=['date_block_num', 'target']), label=data[data['date_block_num'] < 33].target.values, categorical_feature=['shop_id', 'item_id', 'city_name', 'item_category_id', 'category_general_name', 'month', 'prev_sold_delta'])\nval_data = lgb.Dataset(data[data['date_block_num'] == 33].drop(columns=['date_block_num', 'target']), label=data[data['date_block_num'] == 33].target.values, categorical_feature=['shop_id', 'item_id', 'city_name', 'item_category_id', 'category_general_name', 'month', 'prev_sold_delta'], reference=train_data)\ntest_data = data[data['date_block_num'] == 34].drop(columns=['date_block_num', 'target'])","f14799ab":"%%time\nparams = {'metric': 'rmse',\n          'learning_rate': 0.01,\n          'max_depth': 13,\n          'num_leaves': 1673,\n          'random_state': 42,\n          'num_iterations': 500,\n          'early_stopping_round': 12,\n          'num_threads': 2\n         }\n\nmodel = lgb.train(params, train_data, valid_sets=[val_data, train_data])\n\n#model = lgb.train(params, train_data, valid_sets=train_data)","aa026a88":"lgb.plot_importance(model)","7fdeafa3":"preds = model.predict(test_data)","a972ff0a":"submission = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')","3ecc2a3b":"submission['item_cnt_month'] = preds\nsubmission","2633887f":"submission.to_csv('best_lgb.csv', index=False)","a049832b":"Before we generate train set, we can see that some shop names seems very similar. So let's fix shop_id values it  train and test data.","b400a7c9":"## Generating mean encoded features and adding lag","4114b325":"## LightGBM Model","62eb1a95":"# MERGE SHOPS AND CATEGORIES WITH TRAIN DF","4323021a":"The name product is a package with some print. Item_cnt_day 1000 seems too big even for it, but I think it really possible to sell 1000 per day, so I'll keep it.","f0724bb6":"But at firts take a look at shops info","7d55f705":"It's seems like an outlier. Item name is smth like \"Delivery to post office\" and we can see that item_cnt_day - 2169 is really too big value for it. So we will delete it.","18eeeb79":"Calculate time from last sale","874b2787":"# TO HDF FILE\nVery basic version of dataset is ready. Saving it to hdf file for fast backup.","9299bd75":"## Creating training seet","ddfe1bb6":"### Month feature","e2620dad":"## Adding test data to train data","04221774":"At first, let's drop duplicates","0d0ee311":"## Modules import, reading data","00ec3d73":"Train\/val","2b94c5fb":"## Some explorations of other files","2ad77ba1":"Plot features 'item_cnt_day' and 'item_price' to see their distributions and find outliers","ac818e62":"### Checkpoint","9bb382bb":"### SHOP REVENUE LAG","71c8fc76":"## Data investigation","e74852fb":"### Deleting outliers","7ae5b2d5":"Generate training dataframe","406c6206":"Deleting garbage","7d93ab04":"Also let's extract the first words from categories names","4dfa4a0a":"Let's inspect rows with item_price 300,000 and -1 and rows with item_cnt_day 1000 and 2000+","f097f4d6":"## The next step is downloading data2.hdf file and training other models on it","3cd0394d":"So this item is really costs so much, because it's some kind of corporative software. But we don't have this item in test set, so we can delete it form train set.","3f88ea28":"Let's extract city names from shop names","8806e81c":"Fill -1 price with mean item price","baf38a72":"Always downcast datatypes if possible"}}