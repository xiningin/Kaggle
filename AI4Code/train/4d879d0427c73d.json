{"cell_type":{"1aa9af4d":"code","d747c2d8":"code","e784bb3c":"code","a780db1a":"code","16d90236":"code","ba043cde":"code","90bd8e1e":"code","fadbd412":"code","dcdaf6a1":"code","7af4d859":"code","b6dfccf1":"code","6760baf1":"code","150718a2":"code","6e42eacc":"code","f72f3536":"code","80044a1e":"code","d52e45d4":"code","c7c6fff4":"code","2b538305":"code","5aac4f0e":"code","b36731d6":"code","85567a79":"code","cae7a28e":"code","065e1246":"code","690d1350":"markdown","77f920a8":"markdown","a5c1bea2":"markdown","fffaed89":"markdown","411249e5":"markdown","2c98e458":"markdown","ea0b5f63":"markdown","de16289f":"markdown","d95b2bda":"markdown","a614cabb":"markdown","1f328609":"markdown","987bcbb0":"markdown","c93f8f54":"markdown","a1bd663b":"markdown","e43244f3":"markdown","6cdb16aa":"markdown","a04ec2dc":"markdown","1c0109f9":"markdown","a99bd45d":"markdown","457e5081":"markdown","2d91223c":"markdown","be176d36":"markdown","8205002d":"markdown","8dcd2a65":"markdown"},"source":{"1aa9af4d":"import pandas as pd\nimport numpy as np\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\n%matplotlib inline","d747c2d8":"# Below two commands are used to fetch data from CSV file.\ntrain_dataset_raw = pd.read_csv('..\/input\/train.csv')\ntest_dataset = pd.read_csv('..\/input\/test.csv')\n\n# We created a duplicate copy of train dataset to manipulate it. \ntrain_dataset = train_dataset_raw.copy(deep = True)\n\n# Here the train and test dataset are stored in a list so that we dont have to manipulate both one by one\ndata_cleaner = [train_dataset, test_dataset]\n\n# Displays the overall details about the train dataset.\ntrain_dataset_raw.describe(include = 'all')","e784bb3c":"print(train_dataset.isnull().sum())\nprint ('-'*10)\nprint (test_dataset.isnull().sum())","a780db1a":"# This loop will manipulate both train and test dataset.\nfor dataset in data_cleaner:    \n    #complete missing age values with median\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n\n    #complete embarked with mode\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n\n    #complete missing fare with median\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)","16d90236":"###CREATE: Feature Engineering for train and test\/validation dataset\nfor dataset in data_cleaner:    \n        \n    #creates a new column 'CabinBool' which replaces the null values with '0' and who has cabin with '1'.\n    dataset['CabinBool'] = (dataset['Cabin'].notnull().astype('int'))\n    \n    #Discrete variables - creates a family size with no. of siblings , spouse , parents or children.\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n\n    dataset['IsAlone'] = 1 #initialize to yes\/1 is alone\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no\/0 if family size is greater than 1\n\n    #quick and dirty code split title from name: http:\/\/www.pythonforbeginners.com\/dictionary\/python-split\n    #This will store the title of each person from its 'Name'.\n    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n\n\n    #Continuous variable bins; qcut vs cut: https:\/\/stackoverflow.com\/questions\/30211923\/what-is-the-difference-between-pandas-qcut-and-pandas-cut\n    #Fare Bins\/Buckets using qcut or frequency bins: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.qcut.html\n    #This will create a Fare Slab according to no. provided. In our case we want '4' Fare Slab.\n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n\n    \n    #Age Bins\/Buckets using cut or value bins: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.cut.html\n    #Same goes with here, we will have 6 Age categories.\n    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 6)","ba043cde":"# Check our training data after transformation.\ntrain_dataset.sample(10)\nprint(train_dataset['Title'].value_counts())","90bd8e1e":"#cleanup rare title names\n#print(data1['Title'].value_counts())\nstat_min = 10 #while small is arbitrary, we'll use the common minimum in statistics: http:\/\/nicholasjjackson.com\/2012\/03\/08\/sample-size-is-10-a-magic-number\/\ntitle_names = (train_dataset['Title'].value_counts() < stat_min) #this will create a true false series with title name as index\n\n#apply and lambda functions are quick and dirty code to find and replace with fewer lines of code: https:\/\/community.modeanalytics.com\/python\/tutorial\/pandas-groupby-and-python-lambda-functions\/\ntrain_dataset['Title'] = train_dataset['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\nprint(train_dataset['Title'].value_counts())\nprint(\"-\"*10)\ntitle_namest = (test_dataset['Title'].value_counts() < stat_min)\ntest_dataset['Title'] = test_dataset['Title'].apply(lambda x: 'Misc' if title_namest.loc[x] == True else x)","fadbd412":"label = LabelEncoder()\nfor dataset in data_cleaner:\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n    \nprint(train_dataset.columns)\ntest_dataset.columns","dcdaf6a1":"sns.heatmap(train_dataset.corr(),annot=True,cmap='RdYlGn',linewidths=0.2) #data.corr()-->correlation matrix\nfig=plt.gcf()\nfig.set_size_inches(10,8)\nplt.show()","7af4d859":"features_col=['Pclass','Sex','Embarked','IsAlone','Title','AgeBin_Code','FamilySize','FareBin_Code','CabinBool']\ntrain_ds = train_dataset[features_col]\ntest_ds = test_dataset[features_col]\ntrain_label = train_dataset['Survived']\nprint(train_ds.columns)\ntrain_ds.head()","b6dfccf1":"one_hot_encoded_training_predictors = pd.get_dummies(train_ds)\none_hot_encoded_testing_predictors = pd.get_dummies(test_ds)","6760baf1":"sns.heatmap(one_hot_encoded_training_predictors.corr(),annot=True,cmap='RdYlGn',linewidths=0.2) #data.corr()-->correlation matrix\nfig=plt.gcf()\nfig.set_size_inches(10,8)\nplt.show()","150718a2":"# Remove the correlated feature to reduce redundancy in model.\ncorln_col=['Title_Miss','Sex_male']\none_hot_encoded_training_predictors = one_hot_encoded_training_predictors.drop(corln_col,axis=1)\none_hot_encoded_testing_predictors = one_hot_encoded_testing_predictors.drop(corln_col,axis=1)","6e42eacc":"# Import all the required packages.\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn import svm #support vector Machine\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix","f72f3536":"from sklearn.model_selection import train_test_split\n\ntrain_X, test_X, train_y, test_y = train_test_split(one_hot_encoded_training_predictors, train_label, test_size=0.20)\n","80044a1e":"svc_clf = SVC(kernel='rbf',C=1,gamma=0.1) #we can try different parameters\nsvc_clf.fit(train_X, train_y)\npred_svc = svc_clf.predict(test_X)\nacc_svc = accuracy_score(test_y, pred_svc)\n\nprint(acc_svc)","d52e45d4":"linsvc_clf = LinearSVC()\nlinsvc_clf.fit(train_X, train_y)\npred_linsvc = linsvc_clf.predict(test_X)\nacc_linsvc = accuracy_score(test_y, pred_linsvc)\n\nprint(acc_linsvc)","c7c6fff4":"logreg_clf = LogisticRegression()\nlogreg_clf.fit(train_X, train_y)\npred_logreg = logreg_clf.predict(test_X)\nacc_logreg = accuracy_score(test_y, pred_logreg)\n\nprint(acc_logreg)","2b538305":"knn_clf = KNeighborsClassifier()\nknn_clf.fit(train_X, train_y)\npred_knn = knn_clf.predict(test_X)\nacc_knn = accuracy_score(test_y, pred_knn)\n\nprint(acc_knn)\n","5aac4f0e":"gnb_clf = GaussianNB()\ngnb_clf.fit(train_X, train_y)\npred_gnb = gnb_clf.predict(test_X)\nacc_gnb = accuracy_score(test_y, pred_gnb)\n\nprint(acc_gnb)","b36731d6":"#Compare all model performance.\nmodel_performance = pd.DataFrame({\n    'Model': ['Radial SVM', 'Linear SVM', \n              'Logistic Regression', 'K Nearest Neighbors', 'Gaussian Naive Bayes'],\n    'Accuracy': [acc_svc, acc_linsvc, \n              acc_logreg, acc_knn, acc_gnb]\n})\n\nmodel_performance.sort_values(by='Accuracy', ascending=False)","85567a79":"from sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nxyz=[]\naccuracy=[]\nstd=[]\nclassifiers=['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree','Naive Bayes','Random Forest']\nmodels=[svm.SVC(kernel='linear',gamma='auto'),svm.SVC(kernel='rbf',gamma='auto'),LogisticRegression(),KNeighborsClassifier(n_neighbors=9),DecisionTreeClassifier(),GaussianNB(),RandomForestClassifier(n_estimators=100)]\nfor i in models:\n    model = i\n    cv_result = cross_val_score(model, train_X, train_y, cv = kfold,scoring = \"accuracy\")\n    cv_result=cv_result\n    xyz.append(cv_result.mean())\n    std.append(cv_result.std())\n    accuracy.append(cv_result)\nnew_models_dataframe2=pd.DataFrame({'CV Mean':xyz,'Std':std},index=classifiers)       \nnew_models_dataframe2","cae7a28e":"sub_clf = RandomForestClassifier(n_estimators=100)\nsub_clf.fit(one_hot_encoded_training_predictors, train_label)\nsubmission_predictions = sub_clf.predict(one_hot_encoded_testing_predictors)","065e1246":"submission = pd.DataFrame({\n        \"PassengerId\": test_dataset['PassengerId'],\n        \"Survived\": submission_predictions\n    })\n\nsubmission.to_csv(\"titanic.csv\", index=False)","690d1350":"**I hope this kernal may help you in any way. Please support me if this helps you by giving UPVOTE.**","77f920a8":"# Titanic Competition Made Simple","a5c1bea2":"**This will make a group of those titles which are less than 10 as 'Misc' column.**","fffaed89":"# One Hot Encoding\nOne hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction.\n\nSuppose we have Sex classes as 'male' and 'female'. One hot encoding will create two columns 'Sex_male' and 'Sex_female' and store the values as binary.","411249e5":"## Submission\nSubmit the csv file in the specified format.","2c98e458":"## Gaussian Naive Bayes","ea0b5f63":"# Loading the dataset\nDataset are in CSV format, we will use Pandas library to fetch the data CSV file and manipulate it.","de16289f":"**Comparing all model performance.**","d95b2bda":"# Cross Validation\nCross validation is a technique to limit the problem of overfitting. It assess how well the model will generalize to independent data set in the training phase.\nHere we will be using two validation techniques :\n1. **train_test_split**\n2. **K-fold Cross Validation**\n\n## train_test_split()\nThe data we use is usually split into training data and test data. The training set contains a known output and the model learns on this data in order to be generalized to other data later on. We have the test dataset (or subset) in order to test our model\u2019s prediction on this subset.\n\nThe test_size=0.20 inside the function indicates the percentage of the data that should be held over for testing. It\u2019s usually around 80\/20 or 70\/30.","a614cabb":"## Predict the testing dataset\nChoose any model which gives you the best results","1f328609":"## K- fold Cross Validation\n\n1.  The K-Fold Cross Validation works by first dividing the dataset into k-subsets.\n2.   Let's say we divide the dataset into (k=5) parts. We reserve 1 part for testing and train the algorithm over the 4 parts.\n3.   We continue the process by changing the testing part in each iteration and training the algorithm over the other parts. The accuracies and errors are then averaged to get a average accuracy of the algorithm.\n\nThis is called K-Fold Cross Validation.","987bcbb0":"# Cleaning of Data\nAs you can see the above dataset has many null  values, we will modify the dataset to give meaningful dataset to our model.","c93f8f54":"# Correlation between features\n **Non-graphical method** : This correlation matrix is to understand the strength between variables. Correlation varies between -1 and +1.\n\n* -1: Perfect negative linear correlation\n\n* +1: Perfect positive linear correlation\n\n* 0: No correlation\n\nGenerally, if the correlation between the two independent variables are high (>= 0.8) then we drop one independent variable otherwise it may lead to **multi collinearity problem** as both of them contains almost the same information.\n\nSo do you think we should use both of them as one of them is redundant. While making or training models, we should try to eliminate redundant features as it reduces training time and many such advantages.","a1bd663b":"## Label Encoding\nIt is used to transform non-numerical labels to numerical labels (or nominal categorical variables). Numerical labels are always between 0 and n_classes-1. \n\nBelow code will create two column 'AgeBin_Code' and 'FareBin_Code' and convert the bins( ex: AgeBin has {1-16},{16-24} ) to numeric value and label them according to the bins.","e43244f3":"Most Correlating features are **(Sex_male,Title_Mr) , (Sex_female,Title_Miss)**\nSo , chances are they might have **redundant data**. Its better to remove one of the feature.","6cdb16aa":"**Check Correlation after One hot encoding.**","a04ec2dc":"## Logistic Regression","1c0109f9":"# Importing library files","a99bd45d":"# Predictive Modeling\nSo now we will predict the whether the Passenger will survive or not using some great Classification Algorithms.Following are the algorithms I will use to make the model:\n\n1. Logistic Regression\n\n2. Support Vector Machines(Linear and radial)\n\n3. K-Nearest Neighbours\n\n4. Naive Bayes\n","457e5081":"## K-Nearest Neighbours(KNN)","2d91223c":"## Radial Support Vector Machines(rbf-SVM)","be176d36":"**Here we will select the features we want to include in our model.**","8205002d":"## Linear Support Vector Machines","8dcd2a65":"## Feature Engineering\nHere we will try to make new features by analysing the existing features.\nThis will increase our model efficiency and performance."}}