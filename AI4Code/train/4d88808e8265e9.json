{"cell_type":{"f5e5abab":"code","922f9ce5":"code","eab2b077":"code","e686d6db":"code","c4c9aac7":"code","f42349d2":"code","f11ac797":"code","f6346d4d":"code","69372ef2":"code","2c0ec075":"code","0315b2f8":"code","c82beb7f":"code","6c2c0e50":"code","b19f19ea":"code","10bd60cc":"code","a3322fc8":"code","2875d1c7":"markdown","7475d3ce":"markdown"},"source":{"f5e5abab":"#imports\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, Flatten, MaxPool2D, Dropout, Dense\nfrom keras.utils.np_utils import to_categorical\nfrom kerastuner import RandomSearch\nfrom kerastuner.engine.hyperparameters import HyperParameters\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport cv2","922f9ce5":"dataset = []\nlabel = []","eab2b077":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        label.append(dirname[-1])\n        dataset.append(os.path.join(dirname, filename))","e686d6db":"X = []\nfor item in dataset:\n    X.append(cv2.imread(item))\nX = np.array(X)","c4c9aac7":"plt.imshow(X[0])","f42349d2":"label = to_categorical(label,num_classes = 6)","f11ac797":"X = X\/255.00\nX_train,X_val,y_train,y_val = train_test_split(X,label,test_size = 0.1,random_state =  42)","f6346d4d":"def build_model(hp):\n\n    model = Sequential([Conv2D(filters = hp.Int('conv_1_filter', min_value=32, max_value=128, step=16),padding = 'same',activation = 'relu',input_shape = (100,100,3),kernel_size = hp.Choice('conv_1_kernel', values = [3,5])),\n                       Conv2D(filters = hp.Int('conv_2_filter', min_value=32, max_value=64, step=16),padding = 'same', activation = 'relu', kernel_size = hp.Choice('conv_2_kernel', values = [3,5])),\n                       MaxPool2D(pool_size = (2,2)),\n                       Dropout(0.25),\n                      Conv2D(filters = hp.Int('Conv_3_filter', min_value = 32,max_value = 256,step = 16),padding = 'same',activation = 'relu',kernel_size = hp.Choice('Conv_3_kernel',values = [3,5])),\n                      Conv2D(filters = hp.Int('Conv_4_filter', min_value = 32,max_value = 256,step = 16),padding = 'same', activation = 'relu', kernel_size = hp.Choice('conv_4_kernel', values = [3,5])),\n                      MaxPool2D(pool_size = (2,2)),\n                      Dropout(0.25),\n                      Flatten(),\n                      Dropout(0.5),\n                      Dense(6,activation = 'softmax')])\n  \n\n    model.compile(optimizer = keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3])),loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    \n    return model","69372ef2":"tuner_search=RandomSearch(build_model,\n                          objective='val_accuracy',\n                          max_trials=5,directory='output',project_name=\"Mnist Fashion\")","2c0ec075":"tuner_search.search(X,label,epochs=3,validation_split=0.1)","0315b2f8":"model =tuner_search.get_best_models(num_models=1)[0]","c82beb7f":"model.summary()","6c2c0e50":"datagen = ImageDataGenerator()\n\ndatagen.fit(X_train)\n\ncallback = ReduceLROnPlateau(factor = 0.5,\n                            min_lr = 0.0001,\n                            paitence = 3,\n                            monitor = 'val_accuracy',\n                            verbose = 1)","b19f19ea":"model.fit_generator(datagen.flow(X_train,y_train,batch_size = 80),validation_data = (X_val,y_val), epochs = 10,callbacks = [callback])","10bd60cc":"from sklearn.metrics import confusion_matrix\nimport itertools","a3322fc8":"plt.figure(figsize=  [10,10])\nsns.set_style('white')\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Predict the values from the validation dataset\nY_pred = model.predict(X_val)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(y_val,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(6)) ","2875d1c7":"# This is the thing which I want to make you aware of\n\n* You can select best parameters through it.","7475d3ce":"# Keras tuner is love \n\n* When you start in deep learning one common question arise that How I'm going to select the parameters for model for eg. filter size, stride,etc\n\n* By keras tuner you can leave this tension and easily build your architecture parameters will be taken care by it.\n\n* When you are working on relatively small problem than only your'e going to build your own architecture and in that case you will be having 5-8 layers max and for this small model or more specifically in these type of problems you will be using keras tuner.\n\n* It just randomly train model for smaller epochs on random variables and suggest you best parameters and then yu can use those in your model building.\n\n* the dataset is very good and if your'e here you will pretty familiar with deep learning so code will be self-explanatory for you.\n\n* Please Don't forget to upvote it.\n\n* \"upvote karoge tabhi to bhai ki notebook chalegi bro\""}}