{"cell_type":{"0cea52af":"code","04898df3":"code","493b3143":"code","7e68a15e":"code","c9094cb1":"code","26ed1f3e":"code","3b7a19cd":"code","b041925d":"code","6e652d6e":"code","a9ee52ec":"code","5e0bc82d":"code","0b23e337":"code","74061053":"code","72e87c14":"code","96598c98":"code","fba6320f":"code","480de8fe":"code","919c6bd6":"code","33321b0e":"code","1787c4ea":"code","1c2d2cba":"code","819ee282":"code","8446dba4":"code","51f9fed2":"code","a238ade5":"code","4e308cbc":"code","d2a5fdd4":"code","7a26afca":"code","e4c95d67":"code","e2f9d2f0":"code","974a510b":"code","53b32a2d":"code","e372a99a":"code","63f0f1ba":"code","cdfcd4aa":"code","80c15a89":"code","c7d76fe7":"code","84bb8180":"code","43f8cdec":"code","889ede5c":"code","1cde066f":"code","c1a7ec77":"code","7e76b359":"code","c2c9c48d":"code","b51ad6c6":"code","e83ab331":"code","44b05887":"code","0d05f923":"code","6558c148":"code","a4e2e8f5":"code","fa10efe7":"code","e5512a37":"code","1653d502":"code","23aa3a5c":"code","83873a50":"code","34113e03":"code","df57863f":"code","5e71c992":"code","6c3c0ab0":"code","37bf7a98":"code","78b0776e":"code","10d84933":"code","8736d4b5":"code","f2e74d89":"code","7fe0f17f":"code","01cdce12":"code","748f504f":"code","8cae7808":"markdown","5c1418f7":"markdown","5bc7d93d":"markdown","0a2a54b4":"markdown","2986cbe6":"markdown","3c245c3c":"markdown","3f36de10":"markdown","7812c8ca":"markdown","6a859d21":"markdown","5747ab26":"markdown","1c067033":"markdown","87e22b61":"markdown","2495bad6":"markdown","94b3825d":"markdown","e4f78cc5":"markdown","2a76c00e":"markdown","5b47051e":"markdown","99746772":"markdown","17a5fc89":"markdown","b2eee8c2":"markdown","a67855d5":"markdown","a99b79ed":"markdown","e45359a5":"markdown","ff247ad8":"markdown","f9b080a2":"markdown","d319767f":"markdown","20543ab0":"markdown","0c4804d4":"markdown","1170367f":"markdown","b882f13a":"markdown","d1c243c4":"markdown","352ae4ab":"markdown","77b689bc":"markdown","4e0627db":"markdown","eb5ea45d":"markdown","ce10b298":"markdown","289e2626":"markdown","de011bc7":"markdown","0bd36adc":"markdown","d2b9c429":"markdown","356d5406":"markdown","fa9e7dce":"markdown","c49dd8d2":"markdown","daf23531":"markdown","0f29cab3":"markdown","d168d83f":"markdown","0b934023":"markdown","5461189c":"markdown","73097e99":"markdown","fff289e9":"markdown","926601dd":"markdown","7a79cdb7":"markdown","f8d490fe":"markdown","1e27eb6d":"markdown","e5dd821f":"markdown","2f246b4d":"markdown","630d7c32":"markdown","4affda0c":"markdown","42db2328":"markdown","5938a5e0":"markdown","21a22edd":"markdown","d0c3ffa4":"markdown","73240664":"markdown","f5046ce2":"markdown","5fd7a607":"markdown"},"source":{"0cea52af":"# use to visualize missing value\n!pip install missingno","04898df3":"# use for hyper parameter tuning\n!pip install optuna","493b3143":"# use to choose best algorithms for our dataset \n!pip install lazypredict==0.2.7","7e68a15e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\n## Display all the columns of the dataframe\npd.pandas.set_option('display.max_columns',None)\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew # for some statistics\nimport warnings # to ignore warning\nfrom sklearn.preprocessing import RobustScaler, PowerTransformer, LabelEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\n\nimport lazypredict\nfrom lazypredict.Supervised import LazyRegressor\nimport optuna\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet, LassoCV, RidgeCV\n\nfrom sklearn.ensemble import StackingRegressor, RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.svm import SVR\nimport xgboost as xgb\nimport lightgbm as lgb\nimport joblib\n\nprint(\"Library Imported!!\")","c9094cb1":"# load train and test dataset\ntrain_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n\n# combined train and test datasets\ncombined_df = pd.concat([train_df,test_df],axis=0)","26ed1f3e":"train_df.head()","3b7a19cd":"train_df.shape","b041925d":"train_df.info()","6e652d6e":"train_dtype = train_df.dtypes\ntrain_dtype.value_counts()","a9ee52ec":"train_df.isnull().sum().sort_values(ascending = False).head(20)","5e0bc82d":"msno.matrix(train_df)","0b23e337":"msno.heatmap(train_df)","74061053":"test_df.head()","72e87c14":"test_dtype = test_df.dtypes\ntest_dtype.value_counts()","96598c98":"test_df.info()","fba6320f":"test_df.shape","480de8fe":"test_df.isnull().sum().sort_values(ascending = False).head(20)","919c6bd6":"msno.matrix(test_df)","33321b0e":"msno.heatmap(test_df)","1787c4ea":"# as 'SalePrice' Column is not available in test dataset. So we'll delete it.\ntrn_dtype = train_dtype.drop('SalePrice')\ntrn_dtype.compare(test_dtype)","1c2d2cba":"test_df[\"TotalBsmtSF\"].head()","819ee282":"null_train = train_df.isnull().sum()\nnull_test = test_df.isnull().sum()\nnull_train = null_train.drop('SalePrice')\nnull_comp_df = null_train.compare(null_test).sort_values(['self'],ascending = [False])\nnull_comp_df  ","8446dba4":"numerical_features = [col for col in train_df.columns if train_df[col].dtypes != 'O']\ndiscrete_features = [col for col in numerical_features if len(train_df[col].unique()) < 25 and col not in ['Id']]\ncontinuous_features = [feature for feature in numerical_features if feature not in discrete_features+['Id']]\ncategorical_features = [col for col in train_df.columns if train_df[col].dtype == 'O']\n\nprint(\"Total Number of Numerical Columns : \",len(numerical_features))\nprint(\"Number of discrete features : \",len(discrete_features))\nprint(\"No of continuous features are : \", len(continuous_features))\nprint(\"Number of discrete features : \",len(categorical_features))","51f9fed2":"combined_df[\"Label\"] = \"test\"\ncombined_df[\"Label\"][:1460] = \"train\"","a238ade5":"f, axes = plt.subplots(3,6 , figsize=(30, 10), sharex=False)\nfor i, feature in enumerate(discrete_features):\n    sns.histplot(data=combined_df, x = feature, hue=\"Label\",ax=axes[i%3, i\/\/3]) ","4e308cbc":"f, axes = plt.subplots(4,6 , figsize=(30, 15), sharex=False)\nfor i, feature in enumerate(continuous_features):\n    sns.histplot(data=combined_df, x = feature, hue=\"Label\",ax=axes[i%4, i\/\/4]) ","d2a5fdd4":"f, axes = plt.subplots(7,6 , figsize=(30, 30), sharex=False)\nfor i, feature in enumerate(numerical_features):\n    sns.scatterplot(data=combined_df, x = feature, y= \"SalePrice\",ax=axes[i%7, i\/\/7])","7a26afca":"f, axes = plt.subplots(7,7 , figsize=(30, 30), sharex=False)\nfor i, feature in enumerate(categorical_features):\n    sns.countplot(data = combined_df, x = feature, hue=\"Label\",ax=axes[i%7, i\/\/7])","e4c95d67":"f, axes = plt.subplots(7,7 , figsize=(30, 30), sharex=False)\nfor i, feature in enumerate(categorical_features):\n    sort_list = sorted(combined_df.groupby(feature)['SalePrice'].median().items(), key= lambda x:x[1], reverse = True)\n    order_list = [x[0] for x in sort_list ]\n    sns.boxplot(data = combined_df, x = feature, y = 'SalePrice', order=order_list, ax=axes[i%7, i\/\/7])\nplt.show()","e2f9d2f0":"# check the normal distribution of columns having null values by filling with the mean value\nnull_features_numerical = [col for col in combined_df.columns if combined_df[col].isnull().sum() > 0 and col not in categorical_features]\nplt.figure(figsize=(30,20))\nsns.set()\n\nwarnings.simplefilter(\"ignore\")\nfor i,var in enumerate(null_features_numerical):\n  plt.subplot(4,3,i+1)\n  sns.distplot(combined_df[var],bins=20,kde_kws={'linewidth':3,'color':'red'},label=\"original\")\n  sns.distplot(combined_df[var],bins=20,kde_kws={'linewidth':2,'color':'yellow'},label=\"mean\")","974a510b":"plt.figure(figsize=(30,20))\nsns.set()\nwarnings.simplefilter(\"ignore\")\nfor i,var in enumerate(null_features_numerical):\n  plt.subplot(4,3,i+1)\n  sns.distplot(combined_df[var],bins=20,kde_kws={'linewidth':3,'color':'red'},label=\"original\")\n  sns.distplot(combined_df[var],bins=20,kde_kws={'linewidth':2,'color':'yellow'},label=\"median\")","53b32a2d":"# ---------------- do -----------------------","e372a99a":"# ---------------- do -----------------------","63f0f1ba":"# variables which contain year information\nyear_feature = [col for col in combined_df.columns if 'Yr' in col or 'Year' in col]\nyear_feature","cdfcd4aa":"combined_df.groupby('YrSold')['SalePrice'].median().plot()\nplt.xlabel('Year Sold')\nplt.ylabel('House Price')\nplt.title('House price vs YearSold')","80c15a89":"for fet in year_feature:\n  if fet != 'YrSold':\n    hs = combined_df.copy()\n    hs[fet] = hs['YrSold'] - hs[fet]\n    plt.scatter(hs[fet],hs['SalePrice'])\n    plt.xlabel(fet)\n    plt.ylabel('SalePrice')\n    plt.show()","c7d76fe7":"training_corr = train_df.corr(method='spearman')\nplt.figure(figsize=(20,10))\nsns.heatmap(training_corr, cmap=\"YlGnBu\", linewidths=.5)","84bb8180":"drop_columns = [\"Id\", \"Alley\", \"Fence\", \"LotFrontage\", \"FireplaceQu\", \"PoolArea\", \"LowQualFinSF\", \"3SsnPorch\", \"MiscVal\", 'RoofMatl','Street','Condition2','Utilities','Heating','Label']\n#  Drop columns\nprint(\"Number of columns before dropping : \",len(combined_df.columns))\nprint(\"Number of dropping columns : \",len(drop_columns))\ncombined_df.drop(columns=drop_columns, inplace=True, errors='ignore')\nprint(\"Number of columns after dropping : \",len(combined_df.columns))\n","43f8cdec":"## Temporal Variables (Date Time Variables)\n\nfor feature in ['YearBuilt','YearRemodAdd','GarageYrBlt']:\n\n    combined_df[feature]=combined_df['YrSold']-combined_df[feature]\n\ncombined_df[['YearBuilt','YearRemodAdd','GarageYrBlt']].head()","889ede5c":"for col in null_features_numerical:\n  if col not in drop_columns:    \n    # combined_df[col] = combined_df[col].fillna(combined_df[col].mean())\n    combined_df[col] = combined_df[col].fillna(0.0)","1cde066f":"null_features_categorical = [col for col in combined_df.columns if combined_df[col].isnull().sum() > 0 and col in categorical_features]\ncat_feature_mode = [\"SaleType\", \"Exterior1st\", \"Exterior2nd\", \"KitchenQual\", \"Electrical\", \"Functional\"]\n\nfor col in null_features_categorical:\n  if col != 'MSZoning' and col not in cat_feature_mode:\n    combined_df[col] = combined_df[col].fillna('NA')\n  else:\n    combined_df[col] = combined_df[col].fillna(combined_df[col].mode()[0])\n","c1a7ec77":"# Convert \"numerical\" feature to categorical\nconvert_list = ['MSSubClass']\nfor col in convert_list:\n  combined_df[col] = combined_df[col].astype('str')","7e76b359":"numeric_feats = combined_df.dtypes[combined_df.dtypes != 'object'].index\n# get the features except object types\n\n# check the skew of all numerical features\nskewed_feats = combined_df[numeric_feats].apply(lambda x : skew(x.dropna())).sort_values(ascending = False)\nprint('\\n Skew in numberical features: \\n')\nskewness_df = pd.DataFrame({'Skew' : skewed_feats})\nprint(skewness_df.head(10))","c2c9c48d":"# Apply PowerTransformer to columns\nlog_list = ['BsmtUnfSF', 'LotArea', '1stFlrSF', 'GrLivArea', 'TotalBsmtSF', 'GarageArea']\n# log_list = ['LotArea', 'KitchenAbvGr', 'BsmtFinSF2', 'EnclosedPorch', 'ScreenPorch', 'BsmtHalfBath', 'MasVnrArea', 'OpenPorchSF']\n# log_list = skewness_df[abs(skewness_df) > 1].dropna().index\n\n\nfor col in log_list:\n    power = PowerTransformer(method='yeo-johnson', standardize=True)\n    combined_df[[col]] = power.fit_transform(combined_df[[col]]) # fit with combined_data to avoid overfitting with training data?\n\nprint('Number of skewed numerical features got transform : ', len(log_list))","b51ad6c6":"# Regroup features\nregroup_dict = {\n#     'LotConfig': ['FR2','FR3'],\n#     'LandSlope':['Mod','Sev'],\n#     'BldgType':['2FmCon','Duplex'],\n#     'RoofStyle':['Mansard','Flat','Gambrel'],\n#     'Electrical':['FuseF','FuseP','FuseA','Mix'],\n#     'SaleCondition':['Abnorml','AdjLand','Alloca','Family'],\n#     'BsmtExposure':['Min','Av'],\n#     'Functional':['Min1','Maj1','Min2','Mod','Maj2','Sev'],\n#     'LotShape':['IR2','IR3'],\n    'HeatingQC':['Fa','Po'],\n    # 'FireplaceQu':['Fa','Po'],\n    'GarageQual':['Fa','Po'],\n    'GarageCond':['Fa','Po'],\n}\n \n\nfor col, regroup_value in regroup_dict.items():\n    mask = combined_df[col].isin(regroup_value)\n    combined_df[col][mask] = 'Other'","e83ab331":"# print('Shape combined_df before LabelEncoder : {}'.format(combined_df.shape))\n\n# labelencoder_cols = ['FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n#         'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n#         'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n#         'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n#         'YrSold', 'MoSold']\n\n# # process columns, apply LabelEncoder to categorical features \n# for c in labelencoder_cols:\n#   if c in combined_df.columns:\n#       lbl = LabelEncoder()\n#       lbl.fit(list(combined_df[c].values))\n#       combined_df[c] = lbl.transform(list(combined_df[c].values))\n    \n# # shape\n# print('Shape combined_df after LabelEncoder : {}'.format(combined_df.shape))","44b05887":"# Generate one-hot dummy columns\ncombined_df = pd.get_dummies(combined_df).reset_index(drop=True)\n","0d05f923":"new_train_data = combined_df.iloc[:len(train_df), :]\nnew_test_data = combined_df.iloc[len(train_df):, :]\nX_train = new_train_data.drop('SalePrice', axis=1)\ny_train = np.log1p(new_train_data['SalePrice'].values.ravel())\nX_test = new_test_data.drop('SalePrice', axis=1)","6558c148":"\npre_precessing_pipeline = make_pipeline(RobustScaler(), \n                                        # VarianceThreshold(0.001),\n                                       )\n\nX_train = pre_precessing_pipeline.fit_transform(X_train)\nX_test = pre_precessing_pipeline.transform(X_test)\n\nprint(X_train.shape)\nprint(X_test.shape)","a4e2e8f5":"x_train1,x_test1,y_train1,y_test1=train_test_split(X_train,y_train,test_size=0.25)\n\nreg= LazyRegressor(verbose=0,ignore_warnings=True,custom_metric=None)\ntrain,test=reg.fit(x_train1,x_test1,y_train1,y_test1)\ntest","fa10efe7":"RANDOM_SEED = 23\n\n# 10-fold CV\nkfolds = KFold(n_splits=10, shuffle=True, random_state=RANDOM_SEED)","e5512a37":"def tune(objective):\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=100)\n\n    params = study.best_params\n    best_score = study.best_value\n    print(f\"Best score: {best_score} \\nOptimized parameters: {params}\")\n    return params","1653d502":"def ridge_objective(trial):\n\n    _alpha = trial.suggest_float(\"alpha\", 0.1, 20)\n\n    ridge = Ridge(alpha=_alpha, random_state=RANDOM_SEED)\n\n    score = cross_val_score(\n        ridge,X_train,y_train, cv=kfolds, scoring=\"neg_root_mean_squared_error\"\n    ).mean()\n    return score\n\n\n\n# Best score: -0.13586760243668033 \nridge_params = {'alpha': 19.997759851201025}","23aa3a5c":"ridge = Ridge(**ridge_params, random_state=RANDOM_SEED)\nridge.fit(X_train,y_train)","83873a50":"def lasso_objective(trial):\n\n    _alpha = trial.suggest_float(\"alpha\", 0.0001, 1)\n\n    lasso = Lasso(alpha=_alpha, random_state=RANDOM_SEED)\n\n    score = cross_val_score(\n        lasso,X_train,y_train, cv=kfolds, scoring=\"neg_root_mean_squared_error\"\n    ).mean()\n    return score\n\n\n\n\n# Best score: -0.13319435700230317 \nlasso_params = {'alpha': 0.0006224224345371836}","34113e03":"lasso = Lasso(**lasso_params, random_state=RANDOM_SEED)\nlasso.fit(X_train,y_train)","df57863f":"def gbr_objective(trial):\n    _n_estimators = trial.suggest_int(\"n_estimators\", 50, 2000)\n    _learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 1)\n    _max_depth = trial.suggest_int(\"max_depth\", 1, 20)\n    _min_samp_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n    _min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 2, 20)\n    _max_features = trial.suggest_int(\"max_features\", 10, 50)\n\n    gbr = GradientBoostingRegressor(\n        n_estimators=_n_estimators,\n        learning_rate=_learning_rate,\n        max_depth=_max_depth, \n        max_features=_max_features,\n        min_samples_leaf=_min_samples_leaf,\n        min_samples_split=_min_samp_split,\n        \n        random_state=RANDOM_SEED,\n    )\n\n    score = cross_val_score(\n        gbr, X_train,y_train, cv=kfolds, scoring=\"neg_root_mean_squared_error\"\n    ).mean()\n    return score\n\n\n\n\n\n# Best score: -0.12736574760627803 \ngbr_params = {'n_estimators': 1831, 'learning_rate': 0.01325036780847096, 'max_depth': 3, 'min_samples_split': 17, 'min_samples_leaf': 2, 'max_features': 29}","5e71c992":"gbr = GradientBoostingRegressor(random_state=RANDOM_SEED, **gbr_params)\ngbr.fit(X_train,y_train)","6c3c0ab0":"def xgb_objective(trial):\n    _n_estimators = trial.suggest_int(\"n_estimators\", 50, 2000)\n    _max_depth = trial.suggest_int(\"max_depth\", 1, 20)\n    _learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 1)\n    _gamma = trial.suggest_float(\"gamma\", 0.01, 1)\n    _min_child_weight = trial.suggest_float(\"min_child_weight\", 0.1, 10)\n    _subsample = trial.suggest_float('subsample', 0.01, 1)\n    _reg_alpha = trial.suggest_float('reg_alpha', 0.01, 10)\n    _reg_lambda = trial.suggest_float('reg_lambda', 0.01, 10)\n\n    \n    xgbr = xgb.XGBRegressor(\n        n_estimators=_n_estimators,\n        max_depth=_max_depth, \n        learning_rate=_learning_rate,\n        gamma=_gamma,\n        min_child_weight=_min_child_weight,\n        subsample=_subsample,\n        reg_alpha=_reg_alpha,\n        reg_lambda=_reg_lambda,\n        random_state=RANDOM_SEED,\n    )\n    \n\n    score = cross_val_score(\n        xgbr, X_train,y_train, cv=kfolds, scoring=\"neg_root_mean_squared_error\"\n    ).mean()\n    return score\n\n\n\n\n\nxgb_params = {'n_estimators': 847, 'max_depth': 7, 'learning_rate': 0.07412279963454066, 'gamma': 0.01048697764796929, 'min_child_weight': 5.861571837417184, 'subsample': 0.7719639391828977, 'reg_alpha': 2.231609305115769, 'reg_lambda': 3.428674606766844}\n#  . Best is trial 34 with value: -0.13193488071216425.","37bf7a98":"xgbr = xgb.XGBRegressor(random_state=RANDOM_SEED, **xgb_params)\nxgbr.fit(X_train,y_train)","78b0776e":"import lightgbm as lgb\n\ndef lgb_objective(trial):\n    _num_leaves = trial.suggest_int(\"num_leaves\", 50, 100)\n    _max_depth = trial.suggest_int(\"max_depth\", 1, 20)\n    _learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 1)\n    _n_estimators = trial.suggest_int(\"n_estimators\", 50, 2000)\n    _min_child_weight = trial.suggest_float(\"min_child_weight\", 0.1, 10)\n    _reg_alpha = trial.suggest_float('reg_alpha', 0.01, 10)\n    _reg_lambda = trial.suggest_float('reg_lambda', 0.01, 10)\n    _subsample = trial.suggest_float('subsample', 0.01, 1)\n\n\n    \n    lgbr = lgb.LGBMRegressor(objective='regression',\n                             num_leaves=_num_leaves,\n                             max_depth=_max_depth,\n                             learning_rate=_learning_rate,\n                             n_estimators=_n_estimators,\n                             min_child_weight=_min_child_weight,\n                             subsample=_subsample,\n                             reg_alpha=_reg_alpha,\n                             reg_lambda=_reg_lambda,\n                             random_state=RANDOM_SEED,\n    )\n    \n\n    score = cross_val_score(\n        lgbr, X_train,y_train, cv=kfolds, scoring=\"neg_root_mean_squared_error\"\n    ).mean()\n    return score\n\n# Best score: -0.12497294451988177 \n# lgb_params = tune(lgb_objective)\nlgb_params = {'num_leaves': 81, 'max_depth': 2, 'learning_rate': 0.05943111506493225, 'n_estimators': 1668, 'min_child_weight': 4.6721695700874015, 'reg_alpha': 0.33400189583009254, 'reg_lambda': 1.4457484337302167, 'subsample': 0.42380175866399206}\n\n\n# Best score: -0.012014396001532427 \n# lgb_params = {'num_leaves': 84, 'max_depth': 15, 'learning_rate': 0.3765620685374334, 'n_estimators': 1363, 'min_child_weight': 2.933698765978165, 'reg_alpha': 0.025700686948561362, 'reg_lambda': 9.02451400894547, 'subsample': 0.9947557511368282}","10d84933":"lgbr = lgb.LGBMRegressor(objective='regression', random_state=RANDOM_SEED, **lgb_params)\nlgbr.fit(X_train,y_train)","8736d4b5":"# stack models\nstack = StackingRegressor(\n    estimators=[\n        ('ridge', ridge),\n        ('lasso', lasso),\n        ('gradientboostingregressor', gbr),\n        ('xgb', xgbr),\n        ('lgb', lgbr),\n        # ('svr', svr), # Not using this for now as its score is significantly worse than the others\n    ],\n    cv=kfolds)\nstack.fit(X_train,y_train)","f2e74d89":"# # joblib.dump(stack, \"prediction_model.pkl\")\n# model=joblib.load(\"prediction_model.pkl\")\n# model","7fe0f17f":"# def cv_rmse(model):\n#     rmse = -cross_val_score(model, X_train,y_train,\n#                             scoring=\"neg_root_mean_squared_error\",\n#                             cv=kfolds)\n#     return (rmse)\n","01cdce12":"# def compare_models():\n#     models = {\n#         'Ridge': ridge,\n#         'Lasso': lasso,\n#         'Gradient Boosting': gbr,\n#         'XGBoost': xgbr,\n#         'LightGBM': lgbr,\n#         'Stacking': stack, \n#         # 'SVR': svr, # TODO: Investigate why SVR got such a bad result\n#     }\n\n#     scores = pd.DataFrame(columns=['score', 'model'])\n\n#     for name, model in models.items():\n#         score = cv_rmse(model)\n#         print(\"{:s} score: {:.4f} ({:.4f})\\n\".format(name, score.mean(), score.std()))\n#         df = pd.Series(score, name='score').to_frame()\n#         df['model'] = name\n#         scores = scores.append(df)\n\n#     plt.figure(figsize=(20,10))\n#     sns.boxplot(data = scores, x = 'model', y = 'score')\n#     plt.show()\n    \n# compare_models()","748f504f":"print('Predict submission')\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\n\nsubmission.iloc[:,1] = np.expm1(stack.predict(X_test))\n\nsubmission.to_csv('submission_file.csv', index=False)","8cae7808":"# <center>**House Prices - Advanced Regression Techniques**<\/center>","5c1418f7":"### 5.1. Drop Columns\nHere we'll drop columns like\n- ID\n- Column having more missing value\n- Column dominated by 0\/null or single value \n","5bc7d93d":"### 4.1.2. Data Shape - Train Data","0a2a54b4":"### 4.3.3.1. Distribution Comparison - Discrete","2986cbe6":"### 5.7. Encoding Categorical Features\n\n### 5.7.1. LabelEncoder","3c245c3c":"### 4.2.3. Data Information - Test Data","3f36de10":"### 5.6. Regroup Features","7812c8ca":"### 4.2.6. Visualize missing value using **Misingno** - Test Data","6a859d21":"### 4.1.5. Null Value - Train Data","5747ab26":"### 4.2. Test Data Exploration\n\n### 4.2.1. First 5 rows - Test Data","1c067033":"### 6.2. Hyperparameter Tuning using Optuna","87e22b61":"# **Introduction**\n### **Objective:** \nThe objective of the project is to perform advance regression techniques to predict the house price in Boston.\n### **Data Description:**\n- train.csv - the training set\n- test.csv - the test set\n- data_description.txt - full description of each column, originally prepared by Dean De Cock but lightly edited to match the column names used here\n- sample_submission.csv - a benchmark submission from a linear regression on year and month of sale, lot square footage, and number of bedrooms\n\n### **Table of Content:**\n1. Fetch Dataset\n2. Install & Import Libraries\n3. Load Datasets\n4. Exploratory Data Analysis\n5. Feature Engineering\n6. Model Development\n7. Find Prediction\n\n\n\n","2495bad6":"### 5.4. Convert Numerical feature to Categorical","94b3825d":"Here we can see which algorithms give best accuracy in less time.\n- **Gradient Boosting Regressor**\n- **XGBRegressor**\n- **LGBMRegressor**\n- **Lasso**\n- **Ridge**","e4f78cc5":"# **4. Exploratory Data Analysis**\n### 4.1. Train Data Exploration\n\nFor both train and test dataset, We'll explore following things\n\n- First 5 rows\n- Data shape\n- Data information\n- Data types\n- Null value","2a76c00e":"### 4.4.2. Fill Median Value","5b47051e":"### 6.6. XGBRegressor ","99746772":"Above distribution shows that:\n- The distribution of train and test data are similar for most continous features.","17a5fc89":"- Here We can see some columns have inconsistent data types i.e int64 & float64. It's not a problem.","b2eee8c2":"### 4.1.6. Visualize missing value using **Misingno** - Train Data","a67855d5":"### 6.1. Find best algorithms using LazyPredict","a99b79ed":"### 6.5. Gradient Boosting Regressor","e45359a5":"- Here we can see that columns like **\"Alley\", \"Fence\", \"LotFrontage\", \"FireplaceQu\"**  have maximum number of null value. So we will consider to drop these columns.","ff247ad8":"### 4.6. Data Correlation","f9b080a2":"# **7. Find Prediction**","d319767f":"### 4.3. Train & Test Data Comparison\n\nHere we'll compare below things between train and test dataset.\n- Data Type\n- Null values\n- Data Distribution","20543ab0":"### 4.3.3.3. Linearity Check\nHere we'll see the linearity between all features and the target variable.","0c4804d4":"### 4.4.3 Find Suitable value for missing values - Categorical ","1170367f":"#**5. Feature Engineering**","b882f13a":"### 4.1.4. Data Type - Train Data","d1c243c4":"### 4.1.3. Data Information - Train Data","352ae4ab":"### 6.4. Lasso Regression","77b689bc":"# **3. Load Datasets**","4e0627db":"### 5.7.2. Get-Dummies","eb5ea45d":"### 5.3.1. Fill Missing Values - Numerical Feature","ce10b298":"### 4.4. Find Suitable value for missing values - Numerical \n\n### 4.4.1. Fill Mean Value","289e2626":"- The missingno correlation heatmap measures nullity correlation: how strongly the presence or absence of one variable affects the presence of another.","de011bc7":"### 5.3.2. Fill Missing Values - Categorical Feature","0bd36adc":"Here, we could see that sale prices for 'Fa' & 'Po' in 'HeatingQC', 'FireplaceQu', 'GarageQual' and 'GarageCond' are similar, so we can combine these items.","d2b9c429":"### 6.8. StackingRegressor","356d5406":"### 6.3. Ridge Regression","fa9e7dce":"Above distribution shows that:\n\n- The distribution of train and test data are similar for most categorical features.\n- Some features have dominant items, we can combine some minor items into a group otherwise we can drop these columns.\n- Ex: **'RoofMatl','Street','Condition2','Utilities','Heating'** (These columns should be dropped)\n- Ex: 'Fa' & 'Po' in 'HeatingQC', 'FireplaceQu', 'GarageQual' and 'GarageCond'\n\nNow let's conform that the items we want to combine has similar prices(SalePrices value).\n","c49dd8d2":"Above distribution shows that:\n- Some features can be reclassified as 'Categorical', such as **'MSSubClass'**.\n- Some features are dominated by 0\/null **(eg:PoolArea, LowQualFinSF, 3SsnPorch, MiscVal )**, thus we can consider to drop.","daf23531":"### 6.7. LGBMRegressor","0f29cab3":"### 4.2.4. Data Shape - Test Data","d168d83f":"### 4.3.2. Null Value Comparison","0b934023":"We notice that some features are not linear towards target feature.\n\n- 'SalePrice' VS.'BsmtUnfSF',\n- 'SalePrice' VS.'TotalBsmtSF',\n- 'SalePrice' VS.'GarageArea',\n- 'SalePrice' VS.'LotArea',\n- 'SalePrice' VS.'LotFrontage',\n- 'SalePrice' VS.'GrLivArea',\n- 'SalePrice' VS.'1stFlrSF',\n\n","5461189c":"### 4.3.3. Distribution Comparison","73097e99":"### 4.1.1. First 5 records","fff289e9":"## If you find this notebook useful,don't forget to **\"UPVOTE\"**\ud83d\udc4f\n\n## Follow me on [github](https:\/\/github.com\/sidharth178),i used to upload good data science projects.","926601dd":"Check is there any relation betwwn **\"Year Sold\"** and **\"Sales price\"**","7a79cdb7":"Here we'll see how the temporal variables(Year features) affect to House Price","f8d490fe":"### 5.5. Apply PowerTransformer to columns\n- We saw in distribution of continuous features that some features are not linear towards target feature. So we need to transform this. \n- Lets check the skewness of all distributions\n","1e27eb6d":"# **2. Install & Import Libraries**","e5dd821f":"### 4.5. Temporal Variable Analysis","2f246b4d":"### 4.2.5. Null Data - Test Data","630d7c32":"### 4.3.1. Data Type Comparison","4affda0c":"### 4.3.3.4. Distribution Comparison - Categorical ","42db2328":"- From the above visualization we saw that mean and median value both maintain the same destribution. So we can choose one of them to fill the missing values.","5938a5e0":"### 6.9. Save the Model","21a22edd":"### 4.3.3.2. Distribution Comparison - Continuous","d0c3ffa4":"# **6. Model Development**","73240664":"### 4.2.2. Data Type - Test Data","f5046ce2":"# **1. Fetch datasets from kaggle**","5fd7a607":"### 5.2. Temporal Variable Change"}}