{"cell_type":{"9348e7bc":"code","6f2b40ce":"code","4ee27583":"code","1063fd5a":"code","d109d067":"code","453772f1":"code","b658606b":"code","73621e64":"code","07324bff":"code","657bc9cc":"code","29e02bb3":"code","a42a65be":"code","6b15568e":"code","bf0516b9":"code","67425f8d":"code","e21a8f00":"markdown","4c8f3b4e":"markdown","1cd94ad5":"markdown","20d68042":"markdown","909ee172":"markdown","2590176e":"markdown"},"source":{"9348e7bc":"%%bash\napt-get install -y xarchiver || true\nSAVED_MODEL_PATH=\"\/kaggle\/input\/tse2020-roberta-pytorch-multi-tpu-10-skfd-d\"\nNEW_MODEL_PATH=\"\/kaggle\/working\"\nfor model_file in $(ls $SAVED_MODEL_PATH\/*.pth)\ndo\n    just_filename=$(basename \"${model_file%.*}\")\n    if [[ ! -e \"$NEW_MODEL_PATH\/$just_filename.pth\" ]]; then\n        echo \"Copying $model_file to $NEW_MODEL_PATH\"\n        cp $model_file \/tmp\n        cd \/tmp\/\n        mkdir -p $just_filename\n        echo 3 > $just_filename\/version\n        echo \"\"\n        zip -u  $just_filename.pth $just_filename\/version\n        echo \"Moving model file $just_filename.pth to $NEW_MODEL_PATH\"\n        mv $just_filename.pth $NEW_MODEL_PATH\/$just_filename.pth\n    fi\n    echo \"(After) Contents of '$just_filename\/version' in the model file '$model_file'\"\n    unzip -p $NEW_MODEL_PATH\/$(basename $model_file) $just_filename\/version\ndone","6f2b40ce":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport string\nimport re\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport random\nimport torch \nfrom torch import nn\nfrom torch.nn import functional as F\nimport torch.optim as optim\nfrom sklearn.model_selection import StratifiedKFold\nimport tokenizers\nimport transformers\nfrom transformers import RobertaModel, RobertaConfig","4ee27583":"accelerator_device = \"gpu\"\ndef print_to_console(string_to_print, end='\\n', flush=False):\n    if accelerator_device == \"tpu\":\n        xm.master_print(string_to_print, flush=flush) \n    else:\n        print(string_to_print, end=end, flush=flush)","1063fd5a":"MAX_LEN = 128\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE = 16\nEPOCHS = 5\nROBERTA_PATH = \"\/kaggle\/input\/roberta-base\"\nTOKENIZER = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=f\"{ROBERTA_PATH}\/vocab.json\", \n    merges_file=f\"{ROBERTA_PATH}\/merges.txt\", \n    lowercase=True,\n    add_prefix_space=True\n)\n# previous version, now we are copying and moving the files to working folder: SAVED_MODEL_PATH = \"\/kaggle\/input\/tse2020-roberta-pytorch-multi-tpu-10-skfd-d\"\nSAVED_MODEL_PATH=\"\/kaggle\/working\/\"\nNUM_OF_SAVED_MODELS = 10\ncpu_count = os.cpu_count()","d109d067":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n\nseed = 42\nseed_everything(seed)","453772f1":"class TweetDataset(torch.utils.data.Dataset):\n    def __init__(self, df, max_len=96):\n        self.df = df\n        self.max_len = max_len\n        self.labeled = 'selected_text' in df\n        self.tokenizer = tokenizers.ByteLevelBPETokenizer(\n            vocab_file='..\/input\/roberta-base\/vocab.json', \n            merges_file='..\/input\/roberta-base\/merges.txt', \n            lowercase=True,\n            add_prefix_space=True)\n\n    def __getitem__(self, index):\n        data = {}\n        row = self.df.iloc[index]\n        \n        ids, masks, tweet, offsets = self.get_input_data(row)\n        data['ids'] = ids\n        data['masks'] = masks\n        data['tweet'] = tweet\n        data['offsets'] = offsets\n        \n        if self.labeled:\n            start_idx, end_idx = self.get_target_idx(row, tweet, offsets)\n            data['start_idx'] = start_idx\n            data['end_idx'] = end_idx\n        \n        return data\n\n    def __len__(self):\n        return len(self.df)\n    \n    def get_input_data(self, row):\n        tweet = \" \" + \" \".join(row.text.lower().split())\n        encoding = self.tokenizer.encode(tweet)\n        sentiment_id = self.tokenizer.encode(row.sentiment).ids\n        ids = [0] + sentiment_id + [2, 2] + encoding.ids + [2]\n        offsets = [(0, 0)] * 4 + encoding.offsets + [(0, 0)]\n                \n        pad_len = self.max_len - len(ids)\n        if pad_len > 0:\n            ids += [1] * pad_len\n            offsets += [(0, 0)] * pad_len\n        \n        ids = torch.tensor(ids)\n        masks = torch.where(ids != 1, torch.tensor(1), torch.tensor(0))\n        offsets = torch.tensor(offsets)\n        \n        return ids, masks, tweet, offsets\n        \n    def get_target_idx(self, row, tweet, offsets):\n        selected_text = \" \" +  \" \".join(row.selected_text.lower().split())\n\n        len_st = len(selected_text) - 1\n        idx0 = None\n        idx1 = None\n\n        for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n            if \" \" + tweet[ind: ind+len_st] == selected_text:\n                idx0 = ind\n                idx1 = ind + len_st - 1\n                break\n\n        char_targets = [0] * len(tweet)\n        if idx0 != None and idx1 != None:\n            for ct in range(idx0, idx1 + 1):\n                char_targets[ct] = 1\n\n        target_idx = []\n        for j, (offset1, offset2) in enumerate(offsets):\n            if sum(char_targets[offset1: offset2]) > 0:\n                target_idx.append(j)\n\n        start_idx = target_idx[0]\n        end_idx = target_idx[-1]\n        \n        return start_idx, end_idx\n        \ndef get_train_val_loaders(df, train_idx, val_idx, batch_size=8):\n    train_df = df.iloc[train_idx]\n    val_df = df.iloc[val_idx]\n\n    train_loader = torch.utils.data.DataLoader(\n        TweetDataset(train_df), \n        batch_size=batch_size, \n        shuffle=True, \n        num_workers=2,\n        drop_last=True)\n\n    val_loader = torch.utils.data.DataLoader(\n        TweetDataset(val_df), \n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=2)\n\n    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n\n    return dataloaders_dict\n\ndef get_test_loader(df, batch_size=32):\n    loader = torch.utils.data.DataLoader(\n        TweetDataset(df), \n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=2)    \n    return loader","b658606b":"class TweetModel(nn.Module):\n    def __init__(self):\n        super(TweetModel, self).__init__()\n        \n        config = RobertaConfig.from_pretrained(\n            f'{ROBERTA_PATH}\/config.json', output_hidden_states=True)    \n        config.output_hidden_states = True\n        self.roberta = RobertaModel.from_pretrained(\n            f'{ROBERTA_PATH}\/pytorch_model.bin', config=config)\n\n        self.dropout = nn.Dropout(0.5)\n        self.fc = nn.Linear(config.hidden_size, 2)\n        nn.init.normal_(self.fc.weight, std=0.02)\n        nn.init.normal_(self.fc.bias, 0)\n\n    def forward(self, input_ids, attention_mask):\n        _, _, hs = self.roberta(input_ids, attention_mask)\n         \n        x = torch.stack([hs[-1], hs[-2], hs[-3]])\n        x = torch.mean(x, 0)\n        x = self.dropout(x)\n        x = self.fc(x)\n        start_logits, end_logits = x.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n                \n        return start_logits, end_logits","73621e64":"def get_selected_text(text, start_idx, end_idx, offsets):\n    selected_text = \"\"\n    for ix in range(start_idx, end_idx + 1):\n        selected_text += text[offsets[ix][0]: offsets[ix][1]]\n        if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\n            selected_text += \" \"\n    return selected_text\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\ndef compute_jaccard_score(text, start_idx, end_idx, start_logits, end_logits, offsets):\n    start_pred = np.argmax(start_logits)\n    end_pred = np.argmax(end_logits)\n    if start_pred > end_pred:\n        pred = text\n    else:\n        pred = get_selected_text(text, start_pred, end_pred, offsets)\n        \n    true = get_selected_text(text, start_idx, end_idx, offsets)\n    \n    return jaccard(true, pred)","07324bff":"%env JOBLIB_TEMP_FOLDER=\/tmp\n%env JOBLIB_START_METHOD=\"forkserver\"  ### commented out helped, usually its set to stop Parallel from hanging or going idle\n%env TMPDIR=\/tmp","657bc9cc":"device = torch.device(\"cuda\")\nmodel_config = transformers.RobertaConfig.from_pretrained(ROBERTA_PATH)\nmodel_config.output_hidden_states = True","29e02bb3":"%%time\nmodels = {}\n\nfor index in range(NUM_OF_SAVED_MODELS):\n    model_filename=f\"{SAVED_MODEL_PATH}\/roberta_fold{index}.pth\"\n    print(f\"Loading model {index} from {model_filename}\")\n    models[index] = TweetModel()\n    models[index].to(device)\n    models[index].load_state_dict(torch.load(model_filename))\n    models[index].eval()","a42a65be":"%%time\ntest_df = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv')\ntest_df['text'] = test_df['text'].astype(str)\ntest_loader = get_test_loader(test_df)\npredictions = []\n\nfor data in test_loader:\n    print('Reading test data via the test loader...')\n    ids = data['ids'].cuda()\n    masks = data['masks'].cuda()\n    tweet = data['tweet']\n    offsets = data['offsets'].numpy()\n\n    start_logits = []\n    end_logits = []\n    for model in models:\n        print(f'Processing model {model}...')\n        with torch.no_grad():\n            output = models[model](ids, masks)\n            start_logits.append(torch.softmax(output[0], dim=1).cpu().detach().numpy())\n            end_logits.append(torch.softmax(output[1], dim=1).cpu().detach().numpy())\n    \n    print('Preparing predictions...')\n    start_logits = np.mean(start_logits, axis=0)\n    end_logits = np.mean(end_logits, axis=0)\n    for i in range(len(ids)):    \n        start_pred = np.argmax(start_logits[i])\n        end_pred = np.argmax(end_logits[i])\n        if start_pred > end_pred:\n            pred = tweet[i]\n        else:\n            pred = get_selected_text(tweet[i], start_pred, end_pred, offsets[i])\n        predictions.append(pred)","6b15568e":"sample = pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/test.csv\")\nsample.loc[:, 'selected_text'] = predictions\nsample[['textID','selected_text']].to_csv(\"submission.csv\", index=False)","bf0516b9":"sample.head()","67425f8d":"!rm -fr *.pth","e21a8f00":"## Full credits to the origin author [@shoheiazuma](https:\/\/kaggle.com\/shoheiazuma) of the original notebook. But also big thanks to [@abhishek](https:\/\/www.kaggle.com\/abhishek) for his example notebooks and videos on how to build and run models on TPUs and multiple TPUs.\n\nI did some tidying and reorganisation of the code to learn more about how to switch code between CPU\/GPU\/TPU. There can be more improvements as we go along please join me in simplifying the process of writing and running code on CPUs, GPUs and TPUs. \n\nPlease feel free to answer there as well as comment below.\n\n#### Forked from https:\/\/www.kaggle.com\/shoheiazuma\/tweet-sentiment-roberta-pytorch\n\n### This is the inference version of the notebook, the [training version can be found here](https:\/\/www.kaggle.com\/neomatrix369\/tse2020-roberta-pytorch-multi-tpu-10-skfd-1-2).","4c8f3b4e":"# Evaluation Function","1cd94ad5":"# Data Loader","20d68042":"# Model","909ee172":"# Seed","2590176e":"### Loading TPU models on GPU instances, after manually downgrading the model version from 4 to 3\n\n#### Thanks to [@msmelguizo](https:\/\/www.kaggle.com\/msmelguizo) for suggesting the solution, see this [discussion](https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/discussion\/157695#886125). And thanks to [@cpmpml](https:\/\/www.kaggle.com\/cpmpml) for leading me to the below solution."}}