{"cell_type":{"e96626ef":"code","7b06d46a":"code","7127f015":"code","f29d46ac":"code","30159999":"code","7b5f53e2":"code","fdf93024":"code","788f30a4":"code","ae7693af":"code","df9c0ad4":"code","d4c49c1f":"code","64e075a0":"code","40976dce":"code","b85cce1d":"code","c2531f09":"code","7fb302c9":"code","220dd610":"code","4d3afb0b":"code","5646be6d":"code","83af3d40":"code","35cd2c85":"code","022e2590":"code","2190be3a":"code","eff00c27":"code","2985c02e":"code","f7a211bc":"code","9e6c08aa":"code","4c4e6694":"code","1367c29e":"code","d1adeb56":"code","30751a1c":"code","30501905":"code","efd6a4be":"code","44b937d1":"code","3e74223b":"code","dfcff093":"code","99891f07":"code","b81cbbd6":"code","e8726cee":"code","ad7482e4":"code","a50c3c70":"code","0625f83b":"code","2b7aa76e":"code","d38fa235":"code","a44894f5":"code","119c76c2":"code","d67b467b":"code","e0063a60":"code","13a044e7":"code","af4d07d9":"code","c71f542c":"code","add405d3":"code","f3628146":"code","bf40aa67":"code","02ccca3b":"code","4e732ad7":"code","9a408de5":"code","e1964db3":"code","f42dcbd2":"code","c5b4fe02":"code","a71427e1":"markdown","f07631d2":"markdown","b0c3faac":"markdown","57ae8407":"markdown","0fab3da9":"markdown","73d5691a":"markdown","482cb695":"markdown","2bcc16fb":"markdown","ced6cd60":"markdown","e0d9c2fd":"markdown","d5d033ba":"markdown","1a849fc2":"markdown","a920e89d":"markdown","cd85c1ad":"markdown","68384f0f":"markdown","e94e25a1":"markdown","dfc421ab":"markdown","e990bbf4":"markdown","20fb9c60":"markdown","e4f0963e":"markdown","61ab90cc":"markdown","8a2c0264":"markdown","cb01a1e3":"markdown","de6bf568":"markdown","04899e12":"markdown","0cd80f7b":"markdown","8a4bc0d2":"markdown","c6c1f0a1":"markdown","2d6c3fa0":"markdown","3c13f6ed":"markdown"},"source":{"e96626ef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7b06d46a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.io as pio\nimport missingno as msno\nfrom scipy import stats\nimport plotly.express as ex\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots","7127f015":"# We read the dataset and know the columns\ndf=pd.read_csv('..\/input\/new-analyst\/new_Data_Analyst.csv')\ndf.sample(5)","f29d46ac":"df.info()","30159999":"#Number of the rows and columns\ndf.shape","7b5f53e2":"new = df[\"Job_Title\"].str.split(\",\", n = 1, expand = True) \n# making separate first name column from new data frame \ndf[\"Job\"]= new[0] \ndf.drop(columns =[\"Job_Title\"], inplace = True) \ndf.head(3)","fdf93024":"# We want to change the number of the columns\ntitles=list(df.columns)\ntitles","788f30a4":"titles[0],titles[1],titles[2],titles[3],titles[4],titles[5],titles[6],titles[7],titles[8],titles[9],titles[10],titles[11],titles[12],titles[13],titles[14]=titles[14],titles[10],titles[11],titles[1],titles[2],titles[0],titles[3],titles[4],titles[12],titles[13],titles[5],titles[6],titles[7],titles[8],titles[9]\ndf=df[titles]\ndf","ae7693af":"median=df['Min_size'].median()\ndf['Min_size'].fillna(median,inplace=True)\nmedian=df['Max_size'].median()\ndf['Max_size'].fillna(median,inplace=True)\ndf.isnull().sum()","df9c0ad4":"cat_cols=df.select_dtypes(include='object')\nnum_cols=df.select_dtypes(exclude='object')","d4c49c1f":"cat_cols.describe()","64e075a0":"num_cols.describe()","40976dce":"target=df[df['Easy_Apply']=='True']\nnon_target=df[df['Easy_Apply']=='False']","b85cce1d":"#we use this part for analyzing the data based on response\ntarget_cols=['Easy_Apply']\ncat_cols=df.nunique()[df.nunique()<10].keys().tolist()\ncat_cols=[x for x in cat_cols if x not in target_cols]\nnum_col=[x for x in df.columns if x not in cat_cols+ target_cols]","c2531f09":"plt.figure(figsize=(10,5))\nsns.heatmap(df.isnull());","7fb302c9":"# New York has is the most popular city to find a job \ndf['Location'].value_counts(normalize=True)*100","220dd610":"#New York has the highest amount for finding jobs \ndf['Headquarters'].value_counts(normalize=True)*100","4d3afb0b":"# It has the most Vacancy in this dataset\ndf['Sector'].value_counts(normalize=True)*100","5646be6d":"# As you see, most rule of the apply for jobs are not easy \ndf['Easy_Apply'].value_counts(normalize=True)*100","83af3d40":"# It shows that the Rating for easy apply and non easy are different and for easy apply is 3.79 and for non easy is 3.13\ndf.groupby ('Easy_Apply').mean().sort_values('Rating',ascending=True)","35cd2c85":"df.groupby(['Min_size','Max_size']).mean().sort_values('Rating',ascending=True)","022e2590":"df.groupby('Easy_Apply')['Estimated_max_salary'].mean()","2190be3a":"df.groupby('Sector')['Min_size'].mean().sort_values(ascending=False)","eff00c27":"# We use pivot to distribute the sector and and divide into easy apply and their ratings and it shows that the range of the rating is between 3 and 4 except the Unknown sector\npd.pivot_table(df,index=['Sector','Easy_Apply'],values='Rating')","2985c02e":"df['Sector'].unique()\nSector_Easy_Apply=pd.crosstab(df['Sector'],df['Easy_Apply'])\nSector_Easy_Apply","f7a211bc":"df['Easy_Apply'].unique()\nEasy_Apply_Size=pd.crosstab(df['Easy_Apply'],df['Max_size'])\nEasy_Apply_Size","9e6c08aa":"df_grouped_sum=df.groupby('Sector',as_index=False)['Estimated_min_salary'].agg('sum').rename(columns={'Estimated_min_salary':'Estimated_min_salary_Sum'})\ndf_grouped_cnt=df.groupby('Sector',as_index=False)['Estimated_min_salary'].agg('count').rename(columns={'Estimated_min_salary':'Estimated_min_salary_Count'})\n# we want to merge these 2 lines\ndf_grouped_salary=df_grouped_sum.merge(df_grouped_cnt, left_on='Sector',right_on='Sector',how='inner')\n\n#Calculate the average salary\ndf_grouped_salary.loc[:,' Average of min salary']=df_grouped_salary['Estimated_min_salary_Sum']\/df_grouped_salary['Estimated_min_salary_Count']\n\n# The final result\ndf_grouped_salary.sort_values('Estimated_min_salary_Sum',ascending=False)","4c4e6694":"df_grouped_sum=df.groupby('Easy_Apply',as_index=False)['Rating'].agg('sum').rename(columns={'Rating':'Rating_Sum'})\ndf_grouped_cnt=df.groupby('Easy_Apply',as_index=False)['Rating'].agg('count').rename(columns={'Rating':'Rating_Count'})\n# merge the line of the codes\ndf_grouped_average=df_grouped_sum.merge(df_grouped_cnt,left_on='Easy_Apply',right_on='Easy_Apply',how='inner')\n\n# calculate the merge part\ndf_grouped_average.loc[:,'Average of the rating']=df_grouped_average['Rating_Sum']\/df_grouped_average['Rating_Count']\n\ndf_grouped_average.sort_values('Rating_Sum',ascending=False)","1367c29e":"#It is necessary to establish the presence of duplicates. If they are found, delete them and check if they are all deleted.\ndf.duplicated().sum()","d1adeb56":"df = df.drop_duplicates().reset_index(drop=True)\ndf.duplicated().sum()\ndf.head(5)","30751a1c":"def number_of_job(df,Job,Headquarters):\n    number_list=df[(df['Job']==Job)& (df['Headquarters']== Headquarters)]\n    number_list_count=number_list['Sector'].count()\n    return number_list_count          ","30501905":"# the numebr of the headquarters which exist in NY \nnumber_of_job(df,'Data Analyst',\"New York, NY\")","efd6a4be":"# We have only two vacancies in Paris\nnumber_of_job(df,\"Data Analyst\",\"Washington, DC\")","44b937d1":"number_of_job(df,'Data Analyst','Seattle, WA')","3e74223b":"number_of_job(df,'Data Analyst','Chicago, IL')","dfcff093":"# It has the highest demading and sector among rest of the sectors\nplt.rcParams['figure.figsize']=(10,5)\ndf['Sector'].value_counts().sort_values(ascending=False).head(10).plot.bar(color='Aqua')\nplt.xlabel('Sector')\nplt.ylabel('count')\nplt.xticks(rotation=50)\nplt.show()","99891f07":"# As you see, the most portion of the easy apply is False and it means that is not easy to apply for findng jobs.\nplt.rcParams['figure.figsize']=(10,5)\ndf['Easy_Apply'].value_counts().sort_values(ascending=False).plot.pie(y='Easy_Apply',autopct=\"%0.1f%%\")\nplt.axis('off')\nplt.show()","b81cbbd6":"# in this figure, the non profit organization has the highest amount and most of these are not easy apply\nfig,ax=plt.subplots(figsize=(10,6))\nsns.countplot(df['ownership\\t'],hue=df['Easy_Apply'],ax=ax)\nplt.xlabel('Ownership')\nplt.ylabel('Easy_Apply')\nplt.xticks(rotation=50)\nplt.show()","e8726cee":"fig,ax=plt.subplots(figsize=(35,5))\nsns.countplot(df['Sector'],hue=df['Max_size'],ax=ax)\nplt.xlabel('Sectors')\nplt.ylabel('Max_size')\nplt.xticks(rotation=50)\nplt.show()","ad7482e4":"plt.figure(figsize=(16,9))\nsns.set_palette(sns.color_palette(\"Paired\"))\nax = sns.barplot(x='Max_size', y='ownership\\t', data=df, orient='h')\nax.axes.set_title(\"The max size of ownership \",fontsize=20)\nax.set_xlabel(\"Max size\")\nax.set_ylabel(\"Distribute the ownership\")\nsns.color_palette(\"Set2\")\nax.tick_params(labelsize=20)","a50c3c70":"plt.figure(figsize=(16,9))\nsns.set_palette(sns.color_palette(\"Paired\"))\nax = sns.barplot(x='Min_size', y='Sector', data=df, orient='h')\nax.axes.set_title(\"The max size of sectors \",fontsize=20)\nax.set_xlabel(\"Min size\")\nax.set_ylabel(\"Distribute the sectors\")\nsns.color_palette(\"Set2\")\nax.tick_params(labelsize=10)","0625f83b":"sns.violinplot(x='Easy_Apply',y='Estimated_max_salary',data=df, color='yellow')\nplt.figure('Find out the easy apply with the estimated max salary ')\nplt.show()","2b7aa76e":"sns.boxplot(x='Rating',y='Revenue',data=df)\nplt.title=('compare the ratings with the annual revenue of the companies')\nplt.show()","d38fa235":"sns.scatterplot(x='Founded\\t',y='Sector',data=df,color='pink')\nplt.xlabel('which years the Sectors were founded')\nplt.ylabel('Distribute the sectors')\nplt.xticks(rotation=50)\nplt.show()","a44894f5":"df.sort_values(by='Estimated_max_salary',ascending=False)\n\n\nplt.figure(figsize=(20,15))\nsns.barplot(x='Sector',y='Estimated_max_salary',data=df,palette=\"rocket_r\")\nplt.xlabel('Kind of sector')\nplt.ylabel('range of Estimated max salary')\nplt.xticks(rotation=60)\nplt.show()","119c76c2":"sns.jointplot(x='Estimated_max_salary',y='Rating',data=df, color='pink',kind='kde')\nplt.xlabel('Estimated max salary')\nplt.ylabel('Rating')\nplt.xticks(rotation=50)\nplt.figure(figsize=(10,6))\nplt.show()","d67b467b":"# It shows that most of the companies and sectors and industries were founded betweem years 1980 to 2019\ndf['Founded\\t'].hist(bins=30)\nplt.xlabel('Founded of the companies')\nplt.ylabel('count')\nplt.show()","e0063a60":"sns.catplot(x='Easy_Apply',y='Estimated_max_salary',data=df,kind='box')\nplt.figure(figsize=(10,6))\nplt.show()","13a044e7":"pm = df[['Estimated_max_salary', 'Sector']].groupby(['Sector']).agg([sum])\n\nsns.set_palette('Spectral')\nplt.figure(figsize=(10,5))\nplt.pie(pm['Estimated_max_salary']['sum'], labels = pm.index, explode = (0, 1.5, 0, 1.3, 1, 0, 0, 0.5, 0, 1.5, 0, 1.3, 1, 0, 0,1.5, 0, 0.5, 0, 1.3, 1, 0, 0, 1.5,0.5 ), \n        shadow = True, autopct = '%1.1f%%')\nplt.show()","af4d07d9":"df.sample(5)","c71f542c":"# preprocessing\nfrom  sklearn.preprocessing import scale\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n\n#classifier \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Performance metrics \nfrom sklearn import preprocessing \nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import cross_val_score","add405d3":"df.drop(columns=['ownership\\t','Founded\\t','Competitors\\t'],inplace=True)\ndf.head(4)","f3628146":"le=preprocessing.LabelEncoder()\ndf.Job=le.fit_transform(df.Job)\ndf.Location=le.fit_transform(df.Location)\ndf.Headquarters=le.fit_transform(df.Headquarters)\ndf.Industry=le.fit_transform(df.Industry)\ndf.Sector=le.fit_transform(df.Sector)\ndf.Revenue=le.fit_transform(df.Revenue)\ndf.Revenue=le.fit_transform(df.Revenue)\ndf.Easy_Apply=le.fit_transform(df.Easy_Apply)\ndf.sample(5)","bf40aa67":"x=df.drop(['Estimated_min_salary','Estimated_max_salary','Rating','Min_size','Max_size','Easy_Apply'],axis=1)\ny=df['Easy_Apply'].values","02ccca3b":"x.head(5)","4e732ad7":"# k-NN classifier\n\n# Split into training and test set\nx_train , x_test , y_train , y_test=train_test_split(x , y , test_size=0.3 , random_state=1234, stratify= y )\n\n# Create a k-NN classifier with 7 neighbors\nknn = KNeighborsClassifier(n_neighbors=7)\n\n# Fit the classifier to the training data\nknn.fit(x_train, y_train)\n\n# Print the accuracy\nprint(knn.score(x_test, y_test))","9a408de5":"# we want to know the performance of the KNeighbors for testing and training result\n\nneighbors = np.arange(1, 12)\ntrain_accuracy = np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\nfor i, k in enumerate(neighbors):\n    # Train and test the model\n    knn = KNeighborsClassifier(n_neighbors=k)\n\n    # train the model\n    knn.fit(x_train, y_train)\n         \n    # compute the accuracy\n    train_accuracy[i] = knn.score(x_train, y_train)\n        \n    # compute the test\n    test_accuracy[i]=knn.score(x_test, y_test)\n    \n    \n    # result\nplt.plot(neighbors , test_accuracy , label='Test_Acuracy')\nplt.plot(neighbors, train_accuracy , label='Train_Accuracy')\nplt.legend()\nplt.xlabel('number of neighbors')\nplt.ylabel('Accuracy')\nplt.show()\n# You can see the differences the number of the neighbors and also train and test of the KNeighbors and amount of the acuracy","e1964db3":"print(x_train.shape,y_train.shape, x_test.shape,y_test.shape)","f42dcbd2":"models=[]\nmodels.append(('LR',LogisticRegression(random_state=12345)))\nmodels.append(('NN',MLPClassifier(random_state=12345)))\nmodels.append(('RF',RandomForestClassifier(random_state=12345)))\nmodels.append(('DT',DecisionTreeClassifier(random_state=12345)))\nmodels.append(('svm',SVC(random_state=12345)))\nmodels.append(('KN', KNeighborsClassifier()))\n\nresult=[]\nname=[]","c5b4fe02":"# As you see, these are the result of making model( Random Forest has the highest percent among another )\nfor name, model in models:\n    model.fit(x_train,y_train)\n    predictions=model.predict(x_test)\n    accuracy=accuracy_score(y_test, predictions)\n    msg = \"%s: (%f)\" % (name, accuracy)\n    print(msg)\n    print(classification_report(y_test, predictions))\n    print(confusion_matrix(y_test, predictions))","a71427e1":"**this is the very important part of the sector with their minimum size which Insurannce has the highest number(1623) and travel and tourist has the lowest number(51).**","f07631d2":"* We use the boxplot for from which rating to we get the most revenue for companies and it shows that from rating -1 to almost 4 non_aalicable has the highest portion.\n* After this, from rating 3.5 to 5, the revenue is almost between 50 to 100 million $.","b0c3faac":"> We distribute the sectors with their years which founded and as you see, most of the sectors were founded between 1950 to 2018 and only Finance(1700) and education(before 1700) were founded.","57ae8407":"**in the down , we use crosstab to find out which sector has the higehst number of vacancy with easy apply and Business service and IT  sector are in demanding.**","0fab3da9":"**we want to find the first top 3 job title.**","73d5691a":"# 2- Data analysis","482cb695":"**We want to use groupby to find out the average , count and sum of the Rting based on the sector**","2bcc16fb":"**As you see, the dataset is so messy.there are some ways to clean and sort it.One of this is SQLITE or SQL and second is clean on kaggle and I clean on sqlite.**","ced6cd60":"*This graph shows that the Estimated max salary with tall sectors.This graph shows that the sectors like Entertainment and art,Real state and Biotech has the higehst estimated max salary which is about 100000 Dollor and Restaurants and bars has the highest max salary which is about 65000 Dollor.*","e0d9c2fd":"* Divide the target with easy apply which one is easy(True) and which one is hard to apply(False)","d5d033ba":"*On the top, we want to find put the distribuation of the rating with estimated max salary and as you see, the salary between 75 to 100 has the highest rating between 3.5 to 4.5*","1a849fc2":"*We use catplot and we compare the Easy apply and estimated max salary.As you see, we have true and false for easy apply and the estimated max salary for easy apply is from 80 to 110 and it is more than non easy apply* ","a920e89d":"**This figure shows that the Hospital has the highest amount of the max size(alomost 3500),then Business segment is the seecond with almost 3400 numbers and lowest is self employed which has the lowest max size of the compamy with less than 10.**","cd85c1ad":"**In this figure, the Private companies has the highest amount and most of these are not easy apply(almost 1200 records) and the lowest one is for finance.the only sector which has the highest amount of easy apply is for private companies with almost 100 records.**","68384f0f":"**We distributed the sectors by the max size of the companies,As you see, The Unknown has the highest portion and the min size for Unknown sector is over 200(an orange one) \nAfter this, the Business services  has the highest amount(the max size is 51 and the amount of it is 150 records)**\n","e94e25a1":"# 1-Data exploration and preprocessing","dfc421ab":"**As you see in the top, the estimated max salary for easy apply is 96000 and for non easy is 89**","e990bbf4":"**This is the categorical part of the dataset which the max of the size of the company is 10000 and for estimated_max salary is 190000$ and the max of the rating is 5**","20fb9c60":"# We want to know that the Data Analyst in some main  state is so demanding.","e4f0963e":"**In the below, the highest number for easy apply is 5000 members **","61ab90cc":"# We want to find out what is the classification report and confusion matrix and when we use it:","8a2c0264":"* Percision: measure the model performance on measuring the count of true positives in correct manner out of all positive predictions made.\n\n* Recall score :is used to measure the model performance in terms of measuring the count of true positives in correct manner out of all the actual positive values.\n\n* F1-score :is harmonic mean of precision and recall score and is used as a metrics in the scenarios where choosing either of precision or recall score can result in compromise in terms of model giving high false positives and false negatives respectively.\n\n* Macro average gives each prediction similar weight while calculating loss but there might be case when your data might be imbalanced and you want to give importance to some prediction more (based on their proportion), there you use 'weighted' averag","cb01a1e3":"**we clean the missing values and ass you see, there is no missing values and it is clean.**","de6bf568":"**We divide the data into categorical and numerical**","04899e12":"**As you see,for easy apply we have 2 options, true or false and it shows that for non easy apply the estimated max salary is between 55k to 110k but easy apply is over 130k.**","0cd80f7b":"*On the top, we we distribute the sectors with percent of the portion for each of them and it shows that the highest one is IT sector which has 26.1% and then Business service has 23.4%*","8a4bc0d2":"**On the top, we find out the max size for each industry part.As you see, inssurance has the highest amount of the min size and afer this, manufacturing and Biotechnology is the secod and third for the highest amount of minimunm od the size and the lowest min size is for trave and tourism.**","c6c1f0a1":"**This is the categorical part of the dataset which Data analys is the demanding job and with the highest salary.The most headquarters are exist in NY  and IT has the highest sector and we can say that applying is not easy in this data.**","2d6c3fa0":"**As you see upside, the easy apply divide to false and easy one, the average of the rating for easy one is 3.79 and for non_easy is 3.13**","3c13f6ed":"**This is the one of the most interesting part of the code.the max average of the min salary is for It industry and The second is the min average of the min salary is for travel agency\n**"}}