{"cell_type":{"f61e8017":"code","94c010f6":"code","02951bdf":"code","1633db67":"code","b499a3e2":"code","dc3a2b9e":"code","88842c2e":"code","2d822b3a":"code","4f4c54ed":"code","1cd25ed2":"code","b2097bbc":"code","96fc99d6":"code","0a65a8f9":"code","b0dcd4e8":"code","b6f78942":"code","a4a93c33":"code","8cf2e5fe":"code","c80e5271":"code","ddad9b8b":"code","99c8abb1":"code","a7e79bcc":"code","dcfe6371":"code","41d2ea04":"code","21ae1ecb":"code","5a633d3f":"code","e0527bf6":"code","7f8712d8":"code","37c61890":"code","9367dae2":"code","15989e1b":"code","ee0bc9bd":"code","c389b66e":"code","e39ea6ea":"code","c776485e":"code","192f1399":"code","1d43c40b":"code","a7b1278e":"code","d82943ac":"code","0804febb":"code","ecd5f66f":"code","3f4b7b21":"code","5ff96492":"code","3c4b3706":"code","567328f3":"code","69674924":"code","25f6bc7a":"code","00d4d0db":"code","d51d917e":"code","0c98b481":"markdown","95cc6325":"markdown","d8f0c9e2":"markdown","bdc70ae0":"markdown","93cda3b1":"markdown","9081a25a":"markdown","85c10acd":"markdown","fa575729":"markdown","dc695f84":"markdown","22d26e6b":"markdown","d6923ac2":"markdown","bb0cfcc2":"markdown","57133f95":"markdown","754ceb76":"markdown"},"source":{"f61e8017":"\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.utils import resample\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sn\nimport warnings\nwarnings.filterwarnings(action='ignore')\n%matplotlib inline\n","94c010f6":"\ndataset = pd.read_csv('..\/input\/weather-dataset-rattle-package\/weatherAUS.csv')\n","02951bdf":"\ndataset.head()\n","1633db67":"\ndataset.info()\n","b499a3e2":"\ndataset.describe()\n","dc3a2b9e":"\ndataset = dataset.drop(['Date', 'RISK_MM'], axis=1)\n","88842c2e":"\ntarget = 'RainTomorrow'\n","2d822b3a":"\ncol = dataset.columns       # .columns gives columns names in data\nprint(col)\n","4f4c54ed":"\nfeatures = col[:-1]\n","1cd25ed2":"\ndataset[target].value_counts()\n","b2097bbc":"\nsns.countplot(x=target, data=dataset, palette=\"bwr\")\nplt.show()\n\n","96fc99d6":"\ncountRain= len(dataset[dataset[target] == 'Yes'])\ncountNotRain = len(dataset[dataset[target] == 'No'])\nprint(\"Rain Tomorrow: {:.2f}%\".format((countRain \/ (len(dataset[target]))*100)))\nprint(\"Not rain tomorrow: {:.2f}%\".format((countNotRain \/ (len(dataset[target]))*100)))\n","0a65a8f9":"\ndataset['RainToday'].replace({'No': 0, 'Yes': 1},inplace = True)\ndataset['RainTomorrow'].replace({'No': 0, 'Yes': 1},inplace = True)\n","b0dcd4e8":"\ndataset[features].hist(figsize=(12,12), bins=20)\nplt.show()\n","b6f78942":"\n# Next let's check the relationship between parameters of blue team features\ng = sns.PairGrid(data=dataset, vars=['MinTemp', 'MaxTemp', 'Rainfall',\n                                     'Evaporation', 'Sunshine','WindGustSpeed'\n                                     ], hue=target, size=3, palette='Set1')\ng.map_diag(plt.hist)\ng.map_offdiag(plt.scatter)\ng.add_legend()\n","a4a93c33":"\ndata_map = dataset[[target]]\ndata_map[features] = dataset[features]\nplt.figure(figsize=(15,15))\nsns.heatmap(data_map.corr(), annot=True, square=True, cmap='coolwarm')\nplt.show()\n","8cf2e5fe":"\nlist_cor = pd.DataFrame(dataset.corr().unstack().sort_values().drop_duplicates())\nlist_cor.columns = ['correlation_index']\nlist_cor[(list_cor['correlation_index'] > 0.9) | (list_cor['correlation_index'] < -0.9)]\n","c80e5271":"\ntotal = dataset[features].isnull().sum().sort_values(ascending = False)\npercent = (dataset[features].isnull().sum()\/dataset[features].isnull().count()*100).sort_values(ascending = False)\nmissing  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing\n\n","ddad9b8b":"\nduplicated_data = dataset.duplicated()\ndataset[duplicated_data]\n","99c8abb1":"\ndataset.drop_duplicates(keep = False, inplace = True)\n","a7e79bcc":"\nduplicated_data= dataset.duplicated()\ndataset[duplicated_data]\n","dcfe6371":"\ndef max_value(df3, variable, top):\n    return np.where(df3[variable]>top, top, df3[variable])\n","41d2ea04":"\n# find outliers for Rainfall variable\n\nIQR = dataset.Rainfall.quantile(0.75) - dataset.Rainfall.quantile(0.25)\nLower_fence = dataset.Rainfall.quantile(0.25) - (IQR * 3)\nUpper_fence = dataset.Rainfall.quantile(0.75) + (IQR * 3)\nprint('Rainfall outliers are values < {lowerboundary} or > {upperboundary}'.format(\n    lowerboundary=Lower_fence, upperboundary=Upper_fence))\n","21ae1ecb":"\n# find outliers for Evaporation variable\n\nIQR = dataset.Evaporation.quantile(0.75) - dataset.Evaporation.quantile(0.25)\nLower_fence = dataset.Evaporation.quantile(0.25) - (IQR * 3)\nUpper_fence = dataset.Evaporation.quantile(0.75) + (IQR * 3)\nprint('Evaporation outliers are values < {lowerboundary} or > {upperboundary}'.format(\n    lowerboundary=Lower_fence, upperboundary=Upper_fence))\n","5a633d3f":"\n# find outliers for WindSpeed9am variable\n\nIQR = dataset.WindSpeed9am.quantile(0.75) - dataset.WindSpeed9am.quantile(0.25)\nLower_fence = dataset.WindSpeed9am.quantile(0.25) - (IQR * 3)\nUpper_fence = dataset.WindSpeed9am.quantile(0.75) + (IQR * 3)\nprint('WindSpeed9am outliers are values < {lowerboundary} or > {upperboundary}'.format(\n    lowerboundary=Lower_fence, upperboundary=Upper_fence))\n","e0527bf6":"\n# find outliers for WindSpeed3pm variable\n\nIQR = dataset.WindSpeed3pm.quantile(0.75) - dataset.WindSpeed3pm.quantile(0.25)\nLower_fence = dataset.WindSpeed3pm.quantile(0.25) - (IQR * 3)\nUpper_fence = dataset.WindSpeed3pm.quantile(0.75) + (IQR * 3)\nprint('WindSpeed3pm outliers are values < {lowerboundary} or > {upperboundary}'.format(\n    lowerboundary=Lower_fence, upperboundary=Upper_fence))\n","7f8712d8":"\ndataset['Rainfall'] = max_value(dataset, 'Rainfall', 3.2)\ndataset['Evaporation'] = max_value(dataset, 'Evaporation', 21.8)\ndataset['WindSpeed9am'] = max_value(dataset, 'WindSpeed9am', 55)\ndataset['WindSpeed3pm'] = max_value(dataset, 'WindSpeed3pm', 57)\n","37c61890":"no = dataset[dataset.RainTomorrow == 0]\nyes = dataset[dataset.RainTomorrow == 1]\nyes_oversampled = resample(yes, replace=True, n_samples=len(no), random_state=123)\noversampled = pd.concat([no, yes_oversampled])\n\nfig = plt.figure(figsize = (8,5))\noversampled.RainTomorrow.value_counts(normalize = True).plot(kind='bar', color= ['skyblue','navy'], alpha = 0.9, rot=0)\nplt.title('RainTomorrow Indicator No(0) and Yes(1) after Oversampling (Balanced Dataset)')\nplt.show()\n","9367dae2":"\ny = oversampled[target]\nX = oversampled.drop([target], axis=1)\n","15989e1b":"\ncategorical_columns  = [col for col in X.columns if X[col].dtypes == 'O']\nnumerical_columns = list(set(X.columns) - set(categorical_columns) )\ncategorical_columns\n","ee0bc9bd":"\nX = pd.concat([X[numerical_columns],\n                     pd.get_dummies(X.Location, prefix='Location'),\n                     pd.get_dummies(X.WindGustDir, prefix='WindGustDir'),\n                     pd.get_dummies(X.WindDir9am, prefix='WindDir9am'),\n                     pd.get_dummies(X.WindDir3pm, prefix='WindDir3pm')], axis=1)\n\n\nX.head()","c389b66e":"\nnumerical_columns = list(X._get_numeric_data().columns)\ncategorical_columns = list(set(X.columns) - set(numerical_columns))\ncategorical_columns\n","e39ea6ea":"\nnumerical_pipeline = Pipeline([\n        ('data_filler', SimpleImputer(strategy=\"median\")),\n        ('std_scaler', MinMaxScaler()),\n    ])\n","c776485e":"\ntransformer = ColumnTransformer([\n    (\"numerical\", numerical_pipeline, numerical_columns)\n])\n","192f1399":"X, X_validation, y, y_validation = train_test_split(X, y, test_size = 0.3, random_state = 0)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n","1d43c40b":"\ny_train.value_counts()\n","a7b1278e":"\ny_test.value_counts()","d82943ac":"\ny_validation.value_counts()","0804febb":"\ndef plot_matrix(y_test, y_pred):\n    data = confusion_matrix(y_test, y_pred)\n    df_cm = pd.DataFrame(data, columns=np.unique(y_test), index = np.unique(y_test))\n    df_cm.index.name = 'Actual'\n    df_cm.columns.name = 'Predicted'\n    plt.figure(figsize = (5,3))\n    sn.set(font_scale=1.4) #for label size\n    sn.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 10})# font size\n","ecd5f66f":"\ndef train_ensemble_models(X, y):\n    clf1 = DecisionTreeClassifier(random_state=0)\n    #clf2 = RandomForestClassifier(random_state=0)\n    clf3 = xgb.XGBClassifier()\n\n    for clf, label in zip([clf1, clf3], ['DecisionTreeClassifier', 'XGBClassifier']):\n        execute_pipeline(clf, X, y, label)\n","3f4b7b21":"\ndef execute_pipeline(clf, X, y, title):\n    pipe = Pipeline([\n        ('transformer',transformer),\n        ('reduce_dim', 'passthrough'),\n        ('classify', clf)\n    ])\n\n    N_FEATURES_OPTIONS = [16, 64]\n\n    param_grid = [\n          {\n            'reduce_dim': [PCA()],\n            'reduce_dim__n_components': N_FEATURES_OPTIONS\n        },\n        {\n            'reduce_dim': [SelectKBest()],\n            'reduce_dim__k': N_FEATURES_OPTIONS\n        },\n    ]\n    reducer_labels = ['PCA', 'KBest']\n\n    grid = GridSearchCV(pipe,  param_grid=param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1, return_train_score=True)\n    grid.fit(X, y)\n\n    mean_train_scores = np.array(grid.cv_results_['mean_train_score'])\n    mean_scores = np.array(grid.cv_results_['mean_test_score'])\n    mean_scores = mean_scores.reshape(2, len(N_FEATURES_OPTIONS))\n    bar_offsets = (np.arange(len(N_FEATURES_OPTIONS)) * (len(reducer_labels) + 1) + .5)\n\n    plt.figure()\n    COLORS = 'bgrcmyk'\n    for i, (label, reducer_scores) in enumerate(zip(reducer_labels, mean_scores)):\n        plt.bar(bar_offsets + i, mean_train_scores[i], label='{} train'.format(label),alpha=.7)\n        plt.bar(bar_offsets + i, reducer_scores, label='{} test'.format(label), color=COLORS[i])\n\n    plt.title(title)\n    plt.xlabel('Number of features')\n    plt.xticks(bar_offsets + len(reducer_labels) \/ 2, N_FEATURES_OPTIONS)\n    plt.ylabel('Classification accuracy')\n    plt.ylim((0, 1))\n    plt.legend(bbox_to_anchor=(0,1), loc=\"upper right\", bbox_transform=plt.gcf().transFigure)\n    plt.show()\n","5ff96492":"\ndef train_best_model(transformer, clf, parameters, k_best, X_train, y_train, X_test, y_test, X_validation, y_validation):\n\n    pipeline = Pipeline([\n        ('transformer', transformer),\n        ('reduce_dim', SelectKBest(k=k_best)),\n        ('clf', clf)])\n    clf = GridSearchCV(pipeline,\n                        parameters,\n                        scoring='accuracy',\n                        cv=5)\n    clf.fit(X_train, y_train)\n    print(clf.best_params_)\n    model = clf.best_estimator_.named_steps['clf']\n\n    reduction = SelectKBest(k=k_best)\n\n    X_train_transformer = transformer.fit_transform(X_train)\n    X_test_transformer = transformer.transform(X_validation)\n\n    X_train_reduction_transformer = reduction.fit_transform(X_train_transformer, y_train)\n    X_test_reduction_transformer = reduction.transform(X_test_transformer)\n\n    model.fit(X_train_reduction_transformer, y_train)\n    y_predict = model.predict(X_test_reduction_transformer)\n\n    print(classification_report(y_predict, y_validation))\n    plot_matrix(y_validation, y_predict)\n","3c4b3706":"\ngrid_result = train_ensemble_models(X_train, y_train)\n","567328f3":"\ndef plot_best_model(model, k_best, parameters):\n    train_best_model(transformer, model, parameters, k_best,\n                     X_train, y_train,\n                     X_test, y_test,\n                     X_validation, y_validation)\n","69674924":"\nparams_dt = {'clf__max_depth': [5, 16],\n             'clf__max_features': ['sqrt']}\n\nmodel_dt = DecisionTreeClassifier(random_state=42)\n\n\nparams_rf = {'clf__max_depth': [5, 16],\n             'clf__min_samples_leaf': [1,5],\n             'clf__min_samples_split': [2,5],\n             'clf__n_estimators':[10, 100]}\n\nmodel_rf = RandomForestClassifier(random_state=42)\n\n\nparams_xgb ={'clf__n_estimators': [10, 100],\n            'clf__max_depth': [8,16]}\n\nmodel_xgb = xgb.XGBClassifier()\n\n","25f6bc7a":"\nplot_best_model(model_dt, 16, params_dt)\n","00d4d0db":"\nplot_best_model(model_rf, 16, params_rf)\n","d51d917e":"\nplot_best_model(model_xgb, 16, params_xgb)","0c98b481":"\n**Decision Tree Classifier**: 81.00%\n\n**Random Forest Classifier**: 87.00%\n\n**XGB Classifier**: 90.00%\n","95cc6325":"\n## outliers in numerical variables\n","d8f0c9e2":"\n#### Missing data\n","bdc70ae0":"\n## Data Exploration\n","93cda3b1":"\n### Context\nPredict whether or not it will rain tomorrow by training a binary\nclassification model on target RainTomorrow\n\n### Content\nThis dataset contains daily weather observations from numerous Australian weather stations.\n\nThe target variable RainTomorrow means: Did it rain the next day? Yes or No.\n\nNote: You should exclude the variable Risk-MM when training a binary\nclassification model. Not excluding it will leak the answers to your\n model and reduce its predictability.\n\n### Acknowledgements\nObservations were drawn from numerous weather stations.\nThe daily observations are available from http:\/\/www.bom.gov.au\/climate\/data.\nCopyright Commonwealth of Australia 2010, Bureau of Meteorology.\n","9081a25a":"\n## Models\n","85c10acd":"\n# Rain in Australia\n","fa575729":"\n## Read dataset\n","dc695f84":"\n## Data Preprocessing\n","22d26e6b":"\n\nIt was noticed that the dataset has a lot of missing data and has a difference\nbetween classes, for this reason it is necessary to do a work of feature engineering.\nIn conclusion, we understand that of the 3 models analyzed, XGBClassifier with 90% accuracy\nand acceptable recalls was the one that had the best results, signaling that it was not overfit.\nFor future work, I would use more techniques with ROC or\nother models in the literature such as logistic regrecisson and neural networks","d6923ac2":"\n## Conclusion\n","bb0cfcc2":"\n## Clean Dataset\n","57133f95":"\n### Handling Class Imbalance\n","754ceb76":"\n## Data Analysis\n"}}