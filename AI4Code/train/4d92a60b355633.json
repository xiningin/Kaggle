{"cell_type":{"0c948daa":"code","64d2795f":"code","94ca2d9f":"code","988a3fc3":"code","02c6ab4f":"code","f423f363":"code","5357cd0e":"code","02526d8f":"code","f40eeb35":"code","0e0dfc9f":"code","7e268bca":"code","9591e25c":"code","09bb5180":"code","db17ca67":"code","0550e731":"code","ea2914a6":"code","ebd4cc83":"code","9739c9db":"code","5a1f7db2":"code","8a669d88":"code","ac4fe743":"code","2910f8bb":"markdown","77365689":"markdown","35d2df82":"markdown","91e09642":"markdown","bcffcc4d":"markdown","36649b9e":"markdown","def10f9b":"markdown"},"source":{"0c948daa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","64d2795f":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","94ca2d9f":"# Let's load the data\n\niris_df = pd.read_csv('..\/input\/iris\/Iris.csv')\niris_df.head()","988a3fc3":"iris_df.shape","02c6ab4f":"iris_df.info()","f423f363":"# we can drop column id\niris_df = iris_df.drop('Id',axis=1)","5357cd0e":"iris_df.hist(figsize=(8, 6), bins=50);","02526d8f":"sns.heatmap(iris_df.corr(), annot = True)","f40eeb35":"sns.countplot(iris_df.Species)","0e0dfc9f":"from sklearn.preprocessing import LabelEncoder\nenc = LabelEncoder()\niris_df.loc[:, ['Species']] = iris_df.loc[:, ['Species']].apply(enc.fit_transform)\nmapping = dict(zip(enc.classes_, range(len(enc.classes_))))\nmapping","7e268bca":"# split data into train and test\nfrom sklearn.model_selection import train_test_split\n\ny = iris_df.pop('Species')\nX = iris_df\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 1)","9591e25c":"# Rescale dataset columns to the range 0-1\nfrom sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\ny_train = y_train.values.reshape(-1,1)\ny_train.shape","09bb5180":"from keras.models import Sequential\nfrom keras.layers import Dense\n\n# Neural Network\nmodel = Sequential()\nmodel.add(Dense(16, input_dim=4, activation='relu'))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(3, activation='softmax'))\n# Compile model\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n","db17ca67":"model.summary()","0550e731":"history = model.fit(X_train, y_train, validation_split=0.33, epochs=150, verbose=2)","ea2914a6":"# Learnable parameters i.e. weights and bias after training\n\nprint(\"Weights for 1st layer\", model.get_weights()[0],'\\n')\nprint(\"Bias for 1st layer\", model.get_weights()[1],'\\n')\nprint(\"Weights for 2st layer\", model.get_weights()[2],'\\n')\nprint(\"Bias for 2st layer\", model.get_weights()[3],'\\n')\nprint(\"Weights for output layer\", model.get_weights()[4],'\\n')\nprint(\"Bias for output layer\", model.get_weights()[5],'\\n')","ebd4cc83":"# train vs test accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","9739c9db":"# train vs test loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","5a1f7db2":"eva = model.evaluate(X_test, y_test)","8a669d88":"print('On test data : Accuracy is ', eva[0],' and loss is ',eva[1])","ac4fe743":"from sklearn.metrics import classification_report\n\ny_pred = model.predict(X_test, batch_size=64, verbose=1)\ny_pred_bool = np.argmax(y_pred, axis=1)\n\nprint(classification_report(y_test, y_pred_bool))","2910f8bb":"### Activation function\n\nChoosing Activation function for output layer :\n- If problem is a `regression` problem, use `linear activation` function\n- If problem is a `classification` problem, use `sigmoid` for `binary classification` and `softmax` for `multiclass classification` problem\n\nChooding Activation Function for input layer :\n- If Network type is `MultiLayer Perceptron`, use `ReLu` activation\n- If Network type is `Convolutional neural network`, use `LeakyRelu` or `ReLu` activation\n- If Network type is `Recurrent neural network`, use `sigmoid` or `tanh` activation\n\nIn this target variable `Species` have three values - `Iris-setosa`, `Iris-versicolor` and `Iris-virginica`. i.e. the output layer will have 3 neurons. It is a multiclass classification problem.\n\nActivation function used are ReLu, Softmax.\n\n1) `Relu (Rectilinear unit)` : The ReLU function is non-linear activation function. Advantage of using the ReLU function is that it does not activate all the neurons at the same time. Relu function - output = x for x>=0 and 0 otherwise. ReLU function is used only in the hidden layers\n\n2) `Softmax` : This results in values between 0 and 1 for each of the outputs which all sum up to 1. The softmax function - e^x \/ sum(e^x)","77365689":"# Data Preparation","35d2df82":"## Analysis","91e09642":"# Neural Network","bcffcc4d":"Here, we can see that petalLength and sepalLength has high correlation where as petallength and sepalwidth have negative relation.","36649b9e":"# Data Understanding and Exploration","def10f9b":"Here, we can see that we have balance dataset"}}