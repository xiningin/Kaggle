{"cell_type":{"79f18c1e":"code","d81decb5":"code","a6185813":"code","6501077c":"code","3c7ea1c9":"code","5cd0ea10":"code","19c4a77e":"code","8ff0a563":"code","adf1f285":"code","2e843c91":"code","cc8e4bee":"code","12a47ad0":"code","2a6c3019":"code","140ac9aa":"code","18bf20f8":"code","bf79b596":"code","b3640950":"code","9ccf9df6":"code","d4afe2e1":"code","4eb010f9":"code","77954813":"code","d95c7ac4":"code","a14f7a91":"code","15e48e01":"code","b4f60f29":"code","d6885536":"markdown","f3d16e17":"markdown","63500f4d":"markdown","215bdb04":"markdown","c182317e":"markdown","d19a3696":"markdown","c8aaa2ba":"markdown","60225b11":"markdown","ec392015":"markdown","2b7a1fde":"markdown","a66cc522":"markdown","422662fd":"markdown"},"source":{"79f18c1e":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom PIL import Image\nfrom skimage import color\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.use('agg')\n%matplotlib inline","d81decb5":"SEED = 42","a6185813":"DIR_PATH = '..\/input\/intel-image-classification\/seg_train\/seg_train\/'","6501077c":"%%time\n\nimg_paths = []\nfor r, d, f in os.walk(DIR_PATH):\n    for file in f:\n        img_paths.append(os.path.join(r, file))\n        \nnp.random.shuffle(img_paths)","3c7ea1c9":"len(img_paths)","5cd0ea10":"%%time\n\nnum_samples = 10000\n\nimages_col = np.zeros((num_samples, 144, 144, 2))\nimages_gray = np.zeros((num_samples, 144, 144, 1))\n\ncount = 0\n\nfor i, img_path in tqdm(enumerate(img_paths)):\n    img = np.asarray(Image.open(img_path))\n    # Few images are of diferent size. For consistency we only take images of size 50x50\n    if img.shape == (150, 150, 3):\n        # Resizing the image to 144x44 size better compatibility while downscaling and upscaling in CNNs\n        img = tf.image.resize(img, [144,144]) \/ 225.\n        # Converting the imae to lab colour space as it is better suited to image colourisation tasks\n        img = color.rgb2lab(img)\n        # Normalizing the image in lab colour space\n        img = (img + [0, 128, 128]) \/ [100, 255, 255]\n        images_gray[count,:,:,:] = tf.expand_dims(img[:,:,0], axis=-1)\n        images_col[count,:,:,:] = img[:,:,1:]\n        \n        count +=1\n        if count == num_samples:\n            break","19c4a77e":"print(images_gray.shape)\nprint(images_col.shape)","8ff0a563":"plt.figure(figsize=(24,24))\nfor i in range(0, 12):\n    plt.subplot(6, 6, 3*i+1)\n    plt.imshow(images_gray[i], cmap='gray')\n    plt.title(f'Input {i+1} L')\n    plt.axis('off')\n    plt.subplot(6, 6, 3*i+2)\n    plt.imshow(images_col[i][:,:,0], cmap='RdYlGn_r')\n    plt.title(f'Output {i+1} A')\n    plt.subplot(6, 6, 3*i+3)\n    plt.imshow(images_col[i][:,:,1], cmap='YlGnBu_r')\n    plt.title(f'Output {i+1} B')\n    plt.axis('off')\nplt.show()","adf1f285":"def get_rgb(gray, col):\n    assert (gray.shape == (144,144,1)) and (col.shape == (144,144, 2))\n    # Stacking rgb and lab\n    rgb_img = np.dstack([gray, col])\n    # Denormalizing\n    rgb_img = rgb_img*[100, 255, 255] - [0,128,128]\n    # Converting back to rgb\n    rgb_img = color.lab2rgb(rgb_img)\n    return rgb_img","2e843c91":"plt.imshow(get_rgb(images_gray[0], images_col[0]))\nplt.axis('off')","cc8e4bee":"INPUT_DIM = (144, 144, 1)\nBATCH_SIZE = 128\nEPOCHS = 100\nLOSS = \"mse\"\nMETRICS = [\"accuracy\"]\nOPTIMIZER = 'adam'","12a47ad0":"train_datagen = tf.keras.preprocessing.image.ImageDataGenerator().flow(\n    x=images_gray, y=images_col,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    seed=SEED,\n)","2a6c3019":"from keras.models import Sequential, Model\nfrom keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization, MaxPooling2D, BatchNormalization, UpSampling2D\nfrom keras import models\nfrom keras import layers","140ac9aa":"checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    monitor=\"loss\",\n    mode='min',\n    save_best_only=True,\n    save_weights_only=True,\n    filepath=\".\/modelcheck\"\n)\n\nlr_plateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='loss', factor=0.2, patience=10, verbose=1, min_lr=0.001\n)\n\nmodel_callbacks = [checkpoint, lr_plateau]","18bf20f8":"def Colorize():\n    \n    # Encoder\n    encoder_input = Input(shape=INPUT_DIM)\n    encoder_output = Conv2D(64, (3,3), activation='relu', padding='same', strides=2)(encoder_input)\n    encoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(encoder_output)\n    encoder_output = Conv2D(128, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n    encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n    encoder_output = Conv2D(256, (3,3), activation='relu', padding='valid', strides=3)(encoder_output)\n    encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n    encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n    encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n    \n    # Decoder\n    decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(encoder_output)\n    decoder_output = UpSampling2D((3,3))(decoder_output)\n    decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\n    decoder_output = UpSampling2D((2, 2))(decoder_output)\n    decoder_output = Conv2D(32, (3,3), activation='relu', padding='same')(decoder_output)\n    decoder_output = Conv2D(16, (3,3), activation='relu', padding='same')(decoder_output)\n    decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\n    decoder_output = UpSampling2D((2, 2))(decoder_output)\n\n    model = Model(inputs=encoder_input, outputs=decoder_output)\n    return model\n\nmodel = Colorize()\nmodel.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS)\nmodel.summary()","bf79b596":"history = model.fit(\n    train_datagen,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks=model_callbacks,\n)","b3640950":"loss = history.history[\"loss\"]\nacc = history.history[\"accuracy\"]\n\nn = range(len(loss))\n\nplt.plot(n, loss)\nplt.plot(n, acc)\nplt.legend(['loss', 'accuracy'])\nplt.xlabel('Epochs')\nplt.title('Accuracy and Loss Graph')\nplt.show()","9ccf9df6":"TEST_DIR_PATH = '..\/input\/intel-image-classification\/seg_test\/seg_test\/'","d4afe2e1":"num_test = 10","4eb010f9":"%%time\n\ntest_paths = []\n\nfor r, d, f in os.walk(TEST_DIR_PATH):\n    for file in f:\n        test_paths.append(os.path.join(r, file))\n        \nnp.random.shuffle(test_paths)\n\ntest_paths = test_paths[:num_test]","77954813":"len(test_paths)","d95c7ac4":"%%time\n\ntest_col = np.zeros((num_test, 144, 144, 2))\ntest_gray = np.zeros((num_test, 144, 144, 1))\n\ncount = 0\n\nfor i, test_path in tqdm(enumerate(test_paths)):\n    img = np.asarray(Image.open(test_path))\n\n    # Resizing the image to 144x44 size better compatibility while downscaling and upscaling in CNNs\n    img = tf.image.resize(img, [144,144]) \/ 225.\n    # Converting the imae to lab colour space as it is better suited to image colourisation tasks\n    img = color.rgb2lab(img)\n    # Normalizing the image in lab colour space\n    img = (img + [0, 128, 128]) \/ [100, 255, 255]\n    test_gray[count,:,:,:] = tf.expand_dims(img[:,:,0], axis=-1)\n    test_col[count,:,:,:] = img[:,:,1:]\n        \n    count +=1\n    \n    if count == num_test:\n        break","a14f7a91":"print(test_gray.shape)\nprint(test_col.shape)","15e48e01":"preds = model.predict(test_gray)","b4f60f29":"plt.figure(figsize=(36,30))\nfor i in range(0, 10):\n    plt.subplot(5, 6, 3*i+1)\n    plt.imshow(test_gray[i], cmap='gray')\n    plt.title(f'Input {i+1}')\n    plt.axis('off')\n    plt.subplot(5, 6, 3*i+2)\n    plt.imshow(get_rgb(test_gray[i], preds[i]))\n    plt.title(f'Predicted {i+1}')\n    plt.subplot(5, 6, 3*i+3)\n    plt.imshow(get_rgb(test_gray[i], test_col[i]))\n    plt.title(f'Actual {i+1}')\n    plt.axis('off')\nplt.show()","d6885536":"## Defining the model","f3d16e17":"## Importing and visualising the dataset","63500f4d":"### Creating the model","215bdb04":"### Function to convert from lab back to rgb","c182317e":"### Hyperparameters","d19a3696":"# Self Supervised Learning - Image Colourisation","c8aaa2ba":"## Testing on unseen images","60225b11":"### Creting the Data Generator","ec392015":"## Imports","2b7a1fde":"## Objectives\n\nThe objectives of this notebook is to use Self Supervised Learning to implement Image Colourisation.","a66cc522":"## References\n\n1. [Image Colorization Using Autoencoders and Resnet](https:\/\/www.kaggle.com\/valkling\/image-colorization-using-autoencoders-and-resnet\/)\n2. [Auto Colorization of Black and White Images using Machine Learning \u201cAuto-encoders\u201d technique](https:\/\/becominghuman.ai\/auto-colorization-of-black-and-white-images-using-machine-learning-auto-encoders-technique-a213b47f7339)\n1. [Colorizing B&W Photos with Neural Networks](https:\/\/blog.floydhub.com\/colorizing-b-w-photos-with-neural-networks\/)\n4. [Computer Vision 101: Working with Color Images in Python](https:\/\/towardsdatascience.com\/computer-vision-101-working-with-color-images-in-python-7b57381a8a54)","422662fd":"## Authors\n\n- [Tanmay Ambadkar](https:\/\/kaggle.com\/tanmayambadkar) - 201861018\n- [Mayank Bazari](https:\/\/www.kaggle.com\/mayankbazari) - 201851065\n- [Pushkar Patel](https:\/\/kaggle.com\/thepushkarp) - 201851094"}}