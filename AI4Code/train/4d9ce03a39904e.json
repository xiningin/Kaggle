{"cell_type":{"1ed6fb80":"code","b58eb3d4":"code","b6f4ba46":"code","ab44331f":"code","c8fe9190":"code","45c9f7b4":"code","cee154a0":"code","a980e417":"code","56bf3d25":"code","7d1e64a8":"code","ef08c884":"code","ae3e6952":"code","78150545":"code","beccf6eb":"code","10a08067":"code","f7254915":"code","dad561fe":"code","4e8039c6":"code","724e360c":"code","208c230b":"code","5d50d85b":"code","140745c1":"code","a690c058":"code","efd59a50":"code","cc8f6d57":"code","a20c2f9b":"code","4dde2762":"code","8d1a9621":"code","097ee74c":"code","2cea95c2":"code","ba98f2c4":"code","1e7935d4":"code","109e23d6":"code","f2e79759":"code","376b8eb7":"code","dd6824ea":"code","0541ab2c":"code","543be6a7":"code","76da0855":"code","746d2539":"code","59ff6eb7":"code","88336e64":"code","62e4169b":"code","dbf8c254":"code","f9b8370c":"code","efc983bb":"code","ac989c46":"code","01847e9d":"code","d62a5646":"markdown","40a93b2f":"markdown","ad61a1bd":"markdown","ea7d5842":"markdown","b7580a57":"markdown","ec5020de":"markdown","6225485b":"markdown","2956358c":"markdown","17605b7f":"markdown","256502f1":"markdown","0964ebfc":"markdown","0bce4b51":"markdown"},"source":{"1ed6fb80":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom numba import jit","b58eb3d4":"@jit\ndef eval_gini(y_true, y_prob):\n    \"\"\"\n    Original author CPMP : https:\/\/www.kaggle.com\/cpmpml\n    In kernel : https:\/\/www.kaggle.com\/cpmpml\/extremely-fast-gini-computation\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    ntrue = 0\n    gini = 0\n    delta = 0\n    n = len(y_true)\n    for i in range(n-1, -1, -1):\n        y_i = y_true[i]\n        ntrue += y_i\n        gini += y_i * delta\n        delta += 1 - y_i\n    gini = 1 - 2 * gini \/ (ntrue * (n - ntrue))\n    return gini","b6f4ba46":"dfTreino = pd.read_csv('..\/input\/train.csv')\ndfTeste = pd.read_csv('..\/input\/test.csv')","ab44331f":"# Podemos usar a coluna \"id\" como o \u00edndice dos Datasets, sem nenhum prejuizo\ndfTreino.set_index('id', inplace=True)\ndfTeste.set_index('id', inplace=True)","c8fe9190":"headNumber = 5\nprint(f'Dataset de treino - Primeiras {headNumber} linhas')\ndisplay(dfTreino.head(headNumber))\n\nprint(f'Dataset de teste - Primeiras {headNumber} linhas')\ndisplay(dfTeste.head(headNumber))","45c9f7b4":"print('Dataset de treino - Estatistica descritiva')\ndisplay(dfTreino.describe())\n\nprint('Dataset de teste - Estatistica descritiva')\ndisplay(dfTeste.describe())","cee154a0":"print('Dataset de treino - Sum\u00e1rio das Features')\nprint(dfTreino.info())\nprint('---')\nprint('Dataset de treino - Sum\u00e1rio das Features')\nprint(dfTeste.info())","a980e417":"print(f'Dataset de treino tem {dfTreino.shape[0]} linhas por {dfTreino.shape[1]} colunas ({dfTreino.shape[0] * dfTreino.shape[1]} celulas)')\nprint(f'Dataset de teste tem {dfTeste.shape[0]} linhas por {dfTeste.shape[1]} colunas ({dfTeste.shape[0] * dfTeste.shape[1]} celulas)')","56bf3d25":"nonUsed, used = dfTreino.groupby('target').size()\nprint(f'Das {dfTreino.shape[0]} entradas no dataset, {nonUsed} foram de casos onde n\u00e3o foi acionado o seguro e {used} foram caso onde houve acionamento')\nprint(f'Temos assim {round((used\/nonUsed) * 100,6)}% de ocorrencias em que o resultado (1 ou \"houve acionamento\") desejamos prever')","7d1e64a8":"print(f'Antes - Treino tem {dfTreino.shape[0]} linhas por {dfTreino.shape[1]} colunas ({dfTreino.shape[0] * dfTreino.shape[1]} celulas)')\ndfTreino.drop_duplicates()\nprint(f'Depois - Treino tem {dfTreino.shape[0]} linhas por {dfTreino.shape[1]} colunas ({dfTreino.shape[0] * dfTreino.shape[1]} celulas)')\nprint('---')\nprint(f'Antes - Teste tem {dfTeste.shape[0]} linhas por {dfTeste.shape[1]} colunas ({dfTeste.shape[0] * dfTeste.shape[1]} celulas)')\ndfTeste.drop_duplicates()\nprint(f'Depois - Teste tem {dfTeste.shape[0]} linhas por {dfTeste.shape[1]} colunas ({dfTeste.shape[0] * dfTeste.shape[1]} celulas)')","ef08c884":"def generateMetadata(dfInput):\n    data = []\n    for f in dfInput.columns:\n        # definindo o uso (entre r\u00f3tulo, id e atributos)\n        if f == 'target':\n            role = 'target' # r\u00f3tulo\n        elif f == 'id':\n            role = 'id'\n        else:\n            role = 'input' # atributos\n\n        # definindo o tipo do dado\n        if 'bin' in f or f == 'target':\n            level = 'binary'\n        elif 'cat' in f or f == 'id':\n            level = 'nominal'\n        elif dfInput[f].dtype == float or dfInput[f].dtype == np.float64:\n            level = 'interval'\n        elif dfInput[f].dtype == int or dfInput[f].dtype == np.int64:\n            level = 'ordinal'\n            \n        # mantem keep como verdadeiro pra tudo, exceto id\n        keep = True\n        if f == 'id':\n            keep = False\n\n        # cria o tipo de dado\n        dtype = dfInput[f].dtype\n\n        # cria dicion\u00e1rio de metadados\n        f_dict = {\n            'varname': f,\n            'role': role,\n            'level': level,\n            'keep': keep,\n            'dtype': dtype\n        }\n        data.append(f_dict)\n\n    meta = pd.DataFrame(data, columns=['varname', 'role', 'level', 'keep', 'dtype'])\n    meta.set_index('varname', inplace=True)\n    \n    return meta","ae3e6952":"meta_train = generateMetadata(dfTreino)\nmeta_test = generateMetadata(dfTeste)","78150545":"display(meta_train)\ndisplay(meta_test)","beccf6eb":"print('Metadados categoricos da base de treino')\nprint(meta_train[(meta_train.level == 'nominal') & (meta_train.keep)].index)\nprint('---')\nprint('Metadados categoricos da base de teste')\nprint(meta_test[(meta_test.level == 'nominal') & (meta_test.keep)].index)","10a08067":"print('Tipos e quantidade de features do dataset de treino')\ndisplay(pd.DataFrame({'count' : meta_train.groupby(['role', 'level'])['role'].size()}).reset_index())\n\nprint('Tipos e quantidade de features do dataset de teste')\ndisplay(pd.DataFrame({'count' : meta_test.groupby(['role', 'level'])['role'].size()}).reset_index())","f7254915":"def getMissingAttributes(dfInput):\n    atributos_missing = []\n    return_missing = []\n\n    for f in dfInput.columns:\n        missings = dfInput[dfInput[f] == -1][f].count()\n        if missings > 0:\n            atributos_missing.append(f)\n            missings_perc = missings\/dfInput.shape[0]\n            \n            return_missing.append([f, missings, missings_perc])\n\n            print('Atributo {} tem {} amostras ({:.2%}) com valores faltantes'.format(f, missings, missings_perc))\n            \n\n    print('No total, h\u00e1 {} atributos com valores faltantes'.format(len(atributos_missing)))\n    \n    return pd.DataFrame(return_missing).rename(index=str, columns={0: \"column_name\", 1: \"column_nulls\", 2: \"column_percentage\"})","dad561fe":"missing_Train = getMissingAttributes(dfTreino)\ndisplay(missing_Train)","4e8039c6":"missing_Test = getMissingAttributes(dfTeste)\ndisplay(missing_Test)","724e360c":"# limiar de remo\u00e7\u00e3o - 42.5% de nulos\nremove_threshold = 0.425","208c230b":"columns_to_remove = np.array(missing_Train.column_name[(missing_Train.column_percentage >= remove_threshold)])","5d50d85b":"# removendo as colunas que tem muitos valores faltantes\ndfTreino = dfTreino.drop(columns_to_remove, axis=1)\ndfTeste = dfTeste.drop(columns_to_remove, axis=1)\n\n# atualiza os metadados para ter como refer\u00eancia\nmeta_train.loc[(columns_to_remove),'keep'] = False  \nmeta_test.loc[(columns_to_remove),'keep'] = False\n\n# remove do frame de colunas com falta de dados as colunas que foram dropadas\nmissing_Train.drop(missing_Train[(np.isin(missing_Train.column_name, columns_to_remove))].index)","140745c1":"# Usa ou moda ou m\u00e9dia para preencher os valores \"vazios\" que nosso dataset contem, baseado nos metadados do mesmo\ndef fillNullNumbers(dfInput, dfMetadata, dfMissing, missing_default, label):\n\n    from sklearn.impute import SimpleImputer\n\n    media_imp = SimpleImputer(missing_values=missing_default, strategy='mean')\n    moda_imp = SimpleImputer(missing_values=missing_default, strategy='most_frequent')\n\n    for index,row in dfMissing.iterrows():\n        columnName = row['column_name']\n        columnType = dfMetadata.level[(dfMetadata.index == columnName)][0]\n\n        if (columnType == 'interval'):\n            imputerToUse = media_imp\n            imputerString = 'media_imp'\n        elif (columnType == 'ordinal'):\n            imputerToUse = moda_imp\n            imputerString = 'moda_imp'\n        else:\n            imputerToUse = None\n            imputerString = None\n\n        if (imputerToUse != None):\n            dfInput[columnName] = imputerToUse.fit_transform(dfInput[[columnName]]).ravel()\n            print(f\"{label} - Preenchida coluna {columnName}, cujo tipo \u00e9 {columnType}, usando o Imputer {imputerString}\")\n\n    return dfInput","a690c058":"dfTreino = fillNullNumbers(dfTreino, meta_train, missing_Train, -1, 'Treino')\nprint('---')\ndfTeste = fillNullNumbers(dfTeste, meta_test, missing_Train, -1, 'Teste')","efd59a50":"def performOneHotEncoding(dfTrain, dfTest, meta_generic, dist_limit):\n    v = meta_generic[(meta_generic.level == 'nominal') & (meta_generic.keep)].index\n    display(v)\n    for f in v:\n        dist_values = dfTrain[f].value_counts().shape[0]\n        print('Atributo {} tem {} valores distintos'.format(f, dist_values))\n        if (dist_values > dist_limit):\n            print('Atributo {} tem mais de {} valores distintos e por isso ser\u00e1 ignorado'.format(f, dist_limit))\n            dfTrain.drop([f], axis=1)\n            v = v.drop([f])\n        \n    print('Antes do one-hot encoding tinha-se {} atributos'.format(dfTrain.shape[1]))\n    dfTrain = pd.get_dummies(dfTrain, columns=v, drop_first=True)\n    print('Depois do one-hot encoding tem-se {} atributos'.format(dfTrain.shape[1]))\n\n    dfTest = pd.get_dummies(dfTest, columns=v, drop_first=True)\n    missing_cols = set( dfTrain.columns ) - set( dfTest.columns )\n    for c in missing_cols:\n        dfTest[c] = 0\n\n    dfTrain, dfTest = dfTrain.align(dfTest, axis=1)\n    \n    return dfTrain, dfTest","cc8f6d57":"dfTreino, dfTeste = performOneHotEncoding(dfTreino, dfTeste, meta_train, 200)","a20c2f9b":"from sklearn.preprocessing import MinMaxScaler\n\nmin_max_scaler = MinMaxScaler()\n\ndfTreino[dfTreino.columns] = min_max_scaler.fit_transform(dfTreino[dfTreino.columns])\ndfTeste[dfTeste.columns] = min_max_scaler.fit_transform(dfTeste[dfTeste.columns])","4dde2762":"dfTeste.drop(['target'], axis=1, inplace=True)","8d1a9621":"# Models\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\n\n# Feature Selection\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, ShuffleSplit\n\n# Auxiliary Scores\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score","097ee74c":"def showDistribution(val_classes):\n    nonUsed, used = pd.DataFrame(val_classes).groupby('target').size()\n    print('---')\n    print(f'Das {pd.DataFrame(val_classes).shape[0]} entradas no dataset, {nonUsed} foram de casos onde n\u00e3o foi acionado o seguro e {used} foram caso onde houve acionamento')\n    print(f'Temos assim {round((used\/len(val_classes)) * 100,6)}% de ocorrencias em que o resultado (1 ou \"houve acionamento\") desejamos prever')\n    print('---')","2cea95c2":"def logisticRegression(X_Train, y_Train, X_Val, y_Val):\n\n    model = LogisticRegression(solver='lbfgs')\n\n    model.fit(X_Train, y_Train)\n\n    y_pred_class = model.predict(X_Val)\n    y_pred_proba = model.predict_proba(X_Val)\n\n    recall = recall_score(y_Val, y_pred_class)\n    accuracy = accuracy_score(y_Val, y_pred_class)\n    logloss = log_loss(y_Val, y_pred_proba)\n    precision =  precision_score(y_Val, y_pred_class)\n    f1 = f1_score(y_Val, y_pred_class)\n    gini = eval_gini(y_Val, y_pred_class)\n\n    print(f'Baseline - Regress\u00e3o Logistica')\n    print('---')\n    print(f'Acur\u00e1cia: {round(accuracy, 6)}%')\n    print(f'Recall: {round(recall, 6)}%')\n    print(f'Precis\u00e3o: {round(precision, 6)}%')\n    print(f'Log Loss: {round(logloss, 6)}')\n    print(f'F1 Score: {round(f1, 6)}')\n    print(f'Gini: {round(gini, 6)}')\n\n    print('---')\n    print('Matriz de Confus\u00e3o')\n    display(pd.DataFrame(confusion_matrix(y_Val, y_pred_class)))\n    print('---')\n    \n    return model, 'Baseline - Regress\u00e3o Logistica'","ba98f2c4":"# n_estimators=20, learning_rate = 0.5, max_features=2, max_depth = 70, random_state = 0 - Recall: 0.057621%\n\ndef xGBClassifier(X_Train, y_Train, X_Val, y_Val, modelName, modelParams):\n\n    if (modelParams == None):\n        clf = XGBClassifier()\n    else:\n        clf = XGBClassifier(**modelParams)  \n        modelName = modelName + ' - Parameters: ' + str(modelParams)\n    \n    clf.fit(X_Train, y_Train)\n\n    y_pred_class = clf.predict(X_Val)\n    y_pred_proba = clf.predict_proba(X_Val)\n\n    recall = recall_score(y_Val, y_pred_class)\n    accuracy = accuracy_score(y_Val, y_pred_class)\n    logloss = log_loss(y_Val, y_pred_proba)\n    gini = eval_gini(y_Val, y_pred_class)\n    precision =  precision_score(y_Val, y_pred_class)\n    f1 = f1_score(y_Val, y_pred_class)\n\n    print(modelName)\n    print('---')\n    print(f'Acur\u00e1cia: {round(accuracy, 6)}%')\n    print(f'Recall: {round(recall, 6)}%')\n    print(f'Precis\u00e3o: {round(precision, 6)}%')\n    print(f'Log Loss: {round(logloss, 6)}')\n    print(f'F1 Score: {round(f1, 6)}')\n    print(f'Gini: {round(gini, 6)}')\n\n    print('---')\n    print('Matriz de Confus\u00e3o')\n    display(pd.DataFrame(confusion_matrix(y_Val, y_pred_class)))\n    print('---')\n    \n    return clf, modelName","1e7935d4":"def decisionTreeClassifier(X_Train, y_Train, X_Val, y_Val):\n\n    clf = DecisionTreeClassifier()\n\n    clf.fit(X_Train, y_Train)\n\n    y_pred_class = clf.predict(X_Val)\n    y_pred_proba = clf.predict_proba(X_Val)\n\n    recall = recall_score(y_Val, y_pred_class)\n    accuracy = accuracy_score(y_Val, y_pred_class)\n    gini = eval_gini(y_Val, y_pred_class)\n    logloss = log_loss(y_Val, y_pred_proba)\n    precision =  precision_score(y_Val, y_pred_class)\n    f1 = f1_score(y_Val, y_pred_class)\n\n    print(f'Decision Tree - Default Parameters')\n    print('---')\n    print(f'Acur\u00e1cia: {round(accuracy, 6)}%')\n    print(f'Recall: {round(recall, 6)}%')\n    print(f'Precis\u00e3o: {round(precision, 6)}%')\n    print(f'Log Loss: {round(logloss, 6)}')\n    print(f'F1 Score: {round(f1, 6)}')\n    print(f'Gini: {round(gini, 6)}')\n\n    print('---')\n    print('Matriz de Confus\u00e3o')\n    display(pd.DataFrame(confusion_matrix(y_Val, y_pred_class)))\n    print('---')\n    \n    return clf, f'Decision Tree - Default Parameters'","109e23d6":"def gridSearchKNN(X_Train, y_Train, X_Val, y_Val, k_range):\n    clf=KNeighborsClassifier()\n    param_grid=dict(n_neighbors=k_range)\n    scores = ['f1']\n    for sc in scores:\n        grid=GridSearchCV(clf,param_grid,cv=4,scoring=sc,n_jobs=-1)\n        print(\"K-Nearest Neighbors - Tuning hyper-parameters for %s\" % sc)\n        \n        grid.fit(X_Train,y_Train)\n        \n        print(grid.best_params_)\n        print(np.round(grid.best_score_,3))\n        \n        y_pred_class = grid.predict(X_Val)\n        y_pred_proba = grid.predict_proba(X_Val)\n\n        recall = recall_score(y_Val, y_pred_class)\n        accuracy = accuracy_score(y_Val, y_pred_class)\n        gini = eval_gini(y_Val, y_pred_class)\n        logloss = log_loss(y_Val, y_pred_proba)\n        precision =  precision_score(y_Val, y_pred_class)\n        f1 = f1_score(y_Val, y_pred_class)\n\n        print(f'KNN with recall-maxing hyperparameters - {grid.best_params_}')\n        print('---')\n        print(f'Acur\u00e1cia: {round(accuracy, 6)}%')\n        print(f'Recall: {round(recall, 6)}%')\n        print(f'Precis\u00e3o: {round(precision, 6)}%')\n        print(f'Log Loss: {round(logloss, 6)}')\n        print(f'F1 Score: {round(f1, 6)}')\n        print(f'Gini: {round(gini, 6)}')\n\n        print('---')\n        print('Matriz de Confus\u00e3o')\n        display(pd.DataFrame(confusion_matrix(y_Val, y_pred_class)))\n        print('---')\n        \n        return grid, f'KNN with recall-maxing hyperparameters - {grid.best_params_}'","f2e79759":"def gridSearchSVC(X_Train, y_Train, X_Val, y_Val):\n    svc=SVC()\n    param_grid = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4, 1e-5],'C': [1, 10, 100, 1000]},\n                  {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n    scores = ['f1']\n    for sc in scores:\n        grid=GridSearchCV(svc,param_grid,cv=4,scoring=sc,n_jobs=-1)\n        \n        print(\"Support Vector Classifier - Tuning hyper-parameters for %s\" % sc)\n        \n        grid.fit(X_Train,y_Train)\n        print(grid.best_params_)\n        print(np.round(grid.best_score_,3))\n        \n        y_pred_class = grid.predict(X_Val)\n\n        recall = recall_score(y_Val, y_pred_class)\n        accuracy = accuracy_score(y_Val, y_pred_class)\n        gini = eval_gini(y_Val, y_pred_class)\n        precision =  precision_score(y_Val, y_pred_class)\n        f1 = f1_score(y_Val, y_pred_class)\n\n        print(f'SVC with recall-maxing hyperparameters - {grid.best_params_}')\n        print('---')\n        print(f'Acur\u00e1cia: {round(accuracy, 6)}%')\n        print(f'Recall: {round(recall, 6)}%')\n        print(f'Precis\u00e3o: {round(precision, 6)}%')\n        print(f'F1 Score: {round(f1, 6)}')\n        print(f'Gini: {round(gini, 6)}')\n\n        print('---')\n        print('Matriz de Confus\u00e3o')\n        display(pd.DataFrame(confusion_matrix(y_Val, y_pred_class)))\n        print('---')\n        \n        return grid, f'SVC with recall-maxing hyperparameters - {grid.best_params_}'","376b8eb7":"def gridSearchXGB(X_Train, y_Train, X_Val, y_Val, score):\n    xgb=XGBClassifier(random_state = 0)\n    param_grid = [{'n_estimators': [100, 200, 300, 400], 'learning_rate': [0.1, 0.25, 0.5, 0.75],'max_depth': [25, 50, 75, 100], 'gamma': [0, 3, 6, 9]}]\n    scores = [score]\n    for sc in scores:\n        grid=GridSearchCV(xgb,param_grid,cv=2,scoring=sc,n_jobs=-1)\n        \n        print(\"XGBoost - Tuning hyper-parameters for %s\" % sc)\n        \n        grid.fit(X_Train,y_Train)\n        print(grid.best_params_)\n        print(np.round(grid.best_score_,3))\n        \n        y_pred_class = grid.predict(X_Val)\n\n        recall = recall_score(y_Val, y_pred_class)\n        accuracy = accuracy_score(y_Val, y_pred_class)\n        gini = eval_gini(y_Val, y_pred_class)\n        precision =  precision_score(y_Val, y_pred_class)\n        f1 = f1_score(y_Val, y_pred_class)\n\n        print(f'XGBoost with {sc}-maxing hyperparameters - {grid.best_params_}')\n        print('---')\n        print(f'Acur\u00e1cia: {round(accuracy, 6)}%')\n        print(f'Recall: {round(recall, 6)}%')\n        print(f'Precis\u00e3o: {round(precision, 6)}%')\n        print(f'F1 Score: {round(f1, 6)}')\n        print(f'Gini: {round(gini, 6)}')\n\n        print('---')\n        print('Matriz de Confus\u00e3o')\n        display(pd.DataFrame(confusion_matrix(y_Val, y_pred_class)))\n        print('---')\n        \n        return grid, f'XGBoost with {sc}-maxing hyperparameters - {grid.best_params_}'","dd6824ea":"def predictTestDataset(X_Test, y_Test, clfModel, clfName):\n    y_pred_class = clfModel.predict(X_Test)\n    y_pred_proba = clfModel.predict_proba(X_Test)\n\n    recall = recall_score(y_Test, y_pred_class)\n    accuracy = accuracy_score(y_Test, y_pred_class)\n    gini = eval_gini(y_Test, y_pred_class)\n    logloss = log_loss(y_Test, y_pred_proba)\n    precision =  precision_score(y_Test, y_pred_class)\n    f1 = f1_score(y_Test, y_pred_class)\n\n    print(clfName)\n    print('---')\n    print(f'Acur\u00e1cia: {round(accuracy, 6)}%')\n    print(f'Recall: {round(recall, 6)}%')\n    print(f'Precis\u00e3o: {round(precision, 6)}%')\n    print(f'Log Loss: {round(logloss, 6)}')\n    print(f'F1 Score: {round(f1, 6)}')\n    print(f'Gini: {round(gini, 6)}')\n\n    print('---')\n    print('Matriz de Confus\u00e3o')\n    display(pd.DataFrame(confusion_matrix(y_Test, y_pred_class)))\n    print('---')","0541ab2c":"def predictContestDataset(X_Test, clfModel, clfName):\n    \n    print(clfName)\n    print('---')\n    \n    y_pred_class = clfModel.predict(X_Test)\n    y_pred_proba = clfModel.predict_proba(X_Test)\n    \n    pd_prediction = pd.DataFrame(y_pred_class)\n    pd_prediction.columns = ['target']\n    showDistribution(pd_prediction)\n\n    return y_pred_class, y_pred_proba","543be6a7":"print(dfTreino.shape)\nprint(dfTeste.shape)","76da0855":"sample_size = 10000\ninactive_sample_size = int(sample_size * 1)\n\nactivated_indices = dfTreino[dfTreino.target == 1].index\nactivated = dfTreino.loc[np.random.choice(activated_indices, sample_size, replace=False)]\n\ninactive_indices = dfTreino[dfTreino.target == 0].index\ninactive = dfTreino.loc[np.random.choice(inactive_indices, inactive_sample_size, replace=False)]\n\nsubsampled = pd.concat([activated, inactive])\n\nsubsampled.sort_index(inplace=True)","746d2539":"X = subsampled.drop(['target'], axis=1)\ny = subsampled['target']","59ff6eb7":"X_supersampled = dfTreino.drop(X.index).drop(['target'], axis=1)\ny_supersampled = dfTreino.drop(X.index)['target']","88336e64":"print(X_supersampled.shape)\nprint(y_supersampled.shape)","62e4169b":"print(X.shape)\nprint(y.shape)","dbf8c254":"showDistribution(y)","f9b8370c":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.30, random_state=42, stratify=y)\n\nshowDistribution(y_train)\nlogRegModel, logRegName = logisticRegression(X_train, y_train, X_val, y_val)\nxgbPureModel, xgbPureName = xGBClassifier(X_train, y_train, X_val, y_val, 'XGBoost - Base',None)\n#xgbPresetModel, xgbPresetName = xGBClassifier(X_train, y_train, X_val, y_val, 'XGBoost - Preset', {'n_estimator':400, 'learning_rate' : 0.5,'random_state' : 0,'max_depth':70,'objective':\"binary:logistic\",'subsample':.8,'min_child_weig':6,'colsample_bytr':.8,'scale_pos_weight':1.6, 'gamma':10, 'reg_alph':8, 'reg_lambda':1})\nxgbHyperParametrizedModel, xgbHyperParametrizedName = xGBClassifier(X_train, y_train, X_val, y_val, 'XGBoost - Hyperparametrized',{'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 2, 'min_child_weight': 16, 'n_estimators': 100, 'subsample': 1.0})\n#decTreeModel, decTreeName = decisionTreeClassifier(X_train, y_train, X_val, y_val)\n#knnModel, knnName = gridSearchKNN(X_train, y_train, X_val, y_val, list(range(1,20)))\n#svcModel, svcName = gridSearchSVC(X_train, y_train, X_val, y_val)\n\nshowDistribution(y_supersampled)\npredictTestDataset(X_supersampled, y_supersampled, logRegModel, logRegName)\npredictTestDataset(X_supersampled, y_supersampled, xgbPureModel, xgbPureName)\n#predictTestDataset(X_supersampled, y_supersampled, xgbPresetModel, xgbPresetName)\npredictTestDataset(X_supersampled, y_supersampled, xgbHyperParametrizedModel, xgbHyperParametrizedName)\n#predictTestDataset(X_supersampled, y_supersampled, decTreeModel, decTreeName)\n#predictTestDataset(X_supersampled, y_supersampled, knnModel, knnName)\n#predictTestDataset(X_supersampled, y_supersampled, svcModel, svcName)","efc983bb":"#showDistribution(y_train)\n#xgbGridSearchModel, xgbGridSearchName = gridSearchXGB(X_train, y_train, X_val, y_val, 'f1')\n\n#showDistribution(y_supersampled)\n#predictTestDataset(X_supersampled, y_supersampled, xgbGridSearchModel, xgbGridSearchName)","ac989c46":"contest_prediction, contest_prediction_probability = predictContestDataset(dfTeste, xgbHyperParametrizedModel, xgbHyperParametrizedName)","01847e9d":"sample    = pd.read_csv('..\/input\/sample_submission.csv', low_memory=False)\nsample.target = contest_prediction_probability\nsample.target = 1 - sample.target\nsample.to_csv(\"submission.csv\", float_format='%.6f', index=False)","d62a5646":"## AAM \/ Machine Learning - Atividade 02 - Competi\u00e7\u00e3o Porto Seguro\n\n**Equipe**:\n\n*Ciro Mora* - RA: 111310\n\n*Jos\u00e9 Diniz* - RA: 183134\n\n*Renan Renger* - RA: 183148\n\n*Roberto Rodrigues* -  RA: 060235\n\n**Projeto**\n\nPredi\u00e7\u00e3o de acionamento ou n\u00e3o de sinistro de seguro baseado em dataset liberado pela competi\u00e7\u00e3o.\n\n**Observa\u00e7\u00e3o**\n\nBoa parte dos coment\u00e1rio do notebook s\u00e3o proveniente de um notebook base fornecido em sala com a descri\u00e7\u00e3o da atividade ser\u00e3o mantidos por comodidade.\n","40a93b2f":"## Depois de todo pr\u00e9-processamento...\n\n\u00c9 hora de verificar se tanto treino como teste t\u00eam o mesmo tamanho\/formato, e aplicar um modelo de classifica\u00e7\u00e3o j\u00e1 que esse \u00e9 um problema desse tipo. Vale lembrar que o tamanho do treino e teste pode variar quando voc\u00ea estiver participando de outras competi\u00e7\u00f5es ou explorando outros conjuntos de dados.\n\nIsso porque na maioria das competi\u00e7\u00f5es n\u00e3o se tem o *target* do test. Estima-se uma resposta e submete ao Kaggle, por exemplo, para que ele verifique qual foi o resultado final. Ent\u00e3o esse tamanho pode variar em 1 entre treino e teste. No nosso caso, como todos os dados v\u00eam de uma mesma fonte para experimentos, \u00e9 esperado que tenham a mesma quantidade de atributos ou colunas.","ad61a1bd":"## Observa\u00e7\u00e3o\nCom base nas analises acima, podemos perceber algumas coisas:\n\n1 -  Os valores est\u00e3o desnormalizados, variando tanto de tipo (num\u00e9ricos discretos e continuos, categoricos e bin\u00e1rios);\n\n2 - Faltando dados em ambos os datasets (marcados como -1 nos datasets);\n\n3 - **Temos um enorme desbalan\u00e7o no que tange a ocorrencias cujo valor \u00e9 desejado (\"houve acionamento\") vs ocorrencias sem acionamento**\n\n---","ea7d5842":"## Pr\u00e9-processamento \u00e9 provavelmente a parte mais importante de ci\u00eancia dos dados\n\nTer dados representativos sem atributos faltantes \u00e9 provavelmente o pote de ouro em ci\u00eancia dos dados. \u00c9 muito incomum que os dados do mundo real n\u00e3o apresentem anomalias seja da pr\u00f3pria natureza ou sejam anomalias introduzidas no processo de medi\u00e7\u00e3o e registro da observa\u00e7\u00e3o (amostra).\n\nEsse notebook \u00e9 voltado para como tratar dados mais complexos e transformar todas as informa\u00e7\u00f5es em n\u00fameros que fa\u00e7am sentido para que o modelo seja capaz de tra\u00e7ar a rela\u00e7\u00e3o entre atributos e classes. A seguir \u00e9 oferecida uma pequena parcela de um conjunto de dados da empresa Porto Seguro, no qual uma competi\u00e7\u00e3o foi aberta e os competidores foram desafiados a criar um modelo para prever se uma ap\u00f3lice teria um sinistro registrado ou n\u00e3o, indicando o uso do servi\u00e7o.\n\nAlgumas caracter\u00edsticas sobre o nome das features:\n- O nome dos atributos indica o grupo ao qual pertence (ind, reg, car);\n- Os prefixos bin e cat indicam atributos bin\u00e1rios e categ\u00f3ricos, respectivamente;\n- Atributos sem os prefixos citados podem ser ordinais ou cont\u00ednuos;\n- Atributos com -1 indicam dado faltante (missing); e\n- A coluna 'target' indica se houve sinistro para ap\u00f3lice ou n\u00e3o.","b7580a57":"## Valores faltantes\n\nConforme j\u00e1 mencionado, os valores faltantes s\u00e3o indicados por -1, ent\u00e3o \u00e9 importante saber quais colunas t\u00eam valores faltantes e em qual propor\u00e7\u00e3o.","ec5020de":"Para visualizar o atributo e todos seus metadados, basta mostrar a vari\u00e1vel meta:","6225485b":"Os atributos categ\u00f3ricos podem ser mantidos porque o n\u00famero de valores faltantes n\u00e3o \u00e9 expressivo. Inclusive, a estrat\u00e9gia de preenchimento dos **atributos categ\u00f3ricos** \u00e9 sempre mais complexa. Esses atributos **n\u00e3o se beneficiam de medidas estat\u00edsticas** como moda e m\u00e9dia, portanto essas medidas n\u00e3o servem para preench\u00ea-los de forma sint\u00e9tica.\n\n---\n\n## One-hot encoding (ou dummy variables)\n\nDepois de ter tratado os dados faltantes, \u00e9 importante que os dados ordinais tenham representa\u00e7\u00e3o apropriada para o problema tratado. Se o dado n\u00e3o tem dist\u00e2ncia ou rankamento entre eles, cada valor de um atributo deve ser representado por um conjunto de atributos de mesma dist\u00e2ncia. *(Verificar slides desse encontro para que isso fique mais claro)*\n\nOs dados que precisam ser separados em mais dimens\u00f5es j\u00e1 foram identificados como nominais no pr\u00e9-processamento. \u00c9 importante verificar se esses dados t\u00eam grande variedade de valores ou n\u00e3o, e aplicar essa separa\u00e7\u00e3o apenas se for vi\u00e1vel. Por exemplo, se um determinado atributo tem 300 valores, isso geraria 300 colunas novas. Isso s\u00f3 se justificaria se fosse uma base realmente grande e se houvesse uma correla\u00e7\u00e3o muito alta entre essa variedade de valores e a classe.","2956358c":"Com essa estrutura de metadados, fica f\u00e1cil consultar quais colunas quer se manter e que s\u00e3o nominais, por exemplo:","17605b7f":"Da mesma forma, seria poss\u00edvel contar os atributos por tipo de uso e dado:","256502f1":"Vamos optar por manter todos atributos e, portanto, gerar o conjunto de atributos que os mant\u00eam \u00e0 mesma dist\u00e2ncia:","0964ebfc":"Duas estrat\u00e9gias podem ser optadas aqui: simplesmente remover o atributo ou tentar preench\u00ea-lo de forma sint\u00e9tica. Preencher de forma sint\u00e9tica pode gerar uma falsa distribui\u00e7\u00e3o quando o n\u00famero de atributos faltantes \u00e9 muito alto. Quando este for o caso, \u00e9 sempre seguro optar por remover o atributo inteiro. Tamb\u00e9m \u00e9 importante lembrar que a estrat\u00e9gia de preenchimento deve ser coerente com o tipo de dado, por exemplo: **dados ordinais n\u00e3o devem ser preenchidos com m\u00e9dia, nem dados cont\u00ednuos com moda.**","0bce4b51":"Ao trabalhar com as colunas (atributos), \u00e9 interessante ter uma organiza\u00e7\u00e3o de que tipo de dado determinado atributo \u00e9, e para quais prop\u00f3sitos determinado atributo pode ser usado. Nesse sentido, seguindo o trabalho de https:\/\/www.kaggle.com\/bertcarremans\/data-preparation-exploration, vamos criar metadados para esse conjunto."}}