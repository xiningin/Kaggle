{"cell_type":{"9d5b7298":"code","fd8f339c":"code","286e602a":"code","480a9074":"code","079b0ad5":"code","00dcac08":"code","7db8bfdd":"code","30f75423":"code","e71aa431":"code","28b12355":"code","ec24e0e6":"code","0e0b165e":"code","396f5cf6":"code","3d982344":"code","34172662":"code","2fef1e6a":"code","fd8156e8":"code","9627520f":"code","2a7987bb":"code","11afc2f6":"code","d339c76e":"code","9ac43662":"code","95b2468d":"code","0530ed9e":"code","26564f55":"code","0429b54e":"code","6a5f736b":"code","48c8e1c3":"code","189475ea":"code","db11a186":"code","3884e08f":"code","68eff9c1":"code","ef10da27":"markdown","beb5cbc5":"markdown","5376effa":"markdown","743167c8":"markdown","0ea3148f":"markdown","07550207":"markdown","9d664572":"markdown","d9964b02":"markdown","1d5c4b99":"markdown","6a286e7d":"markdown","251cd2d4":"markdown","59d116c4":"markdown","7f0431b0":"markdown","18a8842f":"markdown","b580ac55":"markdown","42f19bc7":"markdown","5ef7b4f3":"markdown","56464c50":"markdown","ae244e5a":"markdown"},"source":{"9d5b7298":"#importing libraries\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","fd8f339c":"#importing the dataset\n\n\ndf = pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')\ndf.head()","286e602a":"#Let us get some basic insight on our columns\n#and understand their properties and datatypes\ndf.info()","480a9074":"df.describe()","079b0ad5":"df.info()","00dcac08":"#We will drop the column \"Unnamed: 32\", because it has 0 non-null values.\n#Also, we can see that no other attributes have any missing values\n\n\ndf.drop(['Unnamed: 32'], axis=1, inplace=True)\ndf.head()","7db8bfdd":"#We will also need to convert our categorical coumns, into numerical by using\n#hot encoding on the dataset\n\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\n\n\nfor column in df.columns:\n  if df[column].dtype == np.int64 or df[column].dtype == np.float64:\n    continue\n  df[column] = LabelEncoder().fit_transform(df[column])\n\n\ndf.head()","30f75423":"#Let's check the ratio of Benign to Malignant cancer\n\n\nplt.figure(figsize=(13,6))\ndf.diagnosis.value_counts().plot.pie(autopct=\"%.1f%%\")\nplt.title(\"Diagnosis Ratio\", fontsize = 20)\nplt.legend(['Benign','Malignant'])","e71aa431":"#A heatmap is used to graphically represent the correlation between the attibutes in our dataset\n#We will plot a heatmap to check for the highly correlated columns\n\nplt.figure(figsize=(25,20))\nsns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\")","28b12355":"#There are many attributes with correalation under less than 0.5.\n#Let us do further analysis on these columns\n\n\nhigh_corr_data = df.corr()\nhigh_corr_columns = high_corr_data.index[abs(high_corr_data['diagnosis'])>=0.5]\nhigh_corr_columns","ec24e0e6":"#Plotting a heatmap of these high correlated values\n\nplt.figure(figsize=(16,8))\nsns.heatmap(df[high_corr_columns].corr(), annot=True, cmap=\"coolwarm\")","0e0b165e":"#Let us check the difference between the means values of attributes of the two types of cancer by using the\n#distplot feature.\n\nmean_col = ['radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']\n\nfor col in mean_col:\n    sns.displot(df, x=col, hue=\"diagnosis\", kind=\"kde\", multiple=\"stack\")","396f5cf6":"#importing libraries\nfrom sklearn.model_selection import train_test_split \n\n#Splitting dependent and independent columns\nx = df.drop(columns = 'diagnosis')\ny = df['diagnosis']","3d982344":"#Splitting data into training and test sets\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=0)","34172662":"#Let us first import the model from the sklearn module\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report , confusion_matrix , accuracy_score","2fef1e6a":"model_logistic = LogisticRegression()\nmodel_logistic.fit(x_train, y_train)\nprint('Logistic regression accuracy: {:.4f}'.format(accuracy_score(y_test, model_logistic.predict(x_test))))","fd8156e8":"confusionmatrix = confusion_matrix(y_test, model_logistic.predict(x_test))\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(3, 3))\nax.matshow(confusionmatrix, cmap=plt.cm.Blues, alpha=0.3)\nfor i in range(confusionmatrix.shape[0]):\n    for j in range(confusionmatrix.shape[1]):\n        ax.text(x=j, y=i,s=confusionmatrix[i, j], va='center', ha='center', size='xx-large')\n \nplt.xlabel('Predictions', fontsize=18)\nplt.ylabel('Actuals', fontsize=18)\nplt.title('Confusion Matrix', fontsize=18)","9627520f":"model_randomforest = RandomForestClassifier()\nmodel_randomforest.fit(x_train, y_train)\nprint('Random Forest accuracy: {:.4f}'.format(accuracy_score(y_test, model_randomforest.predict(x_test))))","2a7987bb":"confusionmatrix = confusion_matrix(y_test, model_randomforest.predict(x_test))\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(3, 3))\nax.matshow(confusionmatrix, cmap=plt.cm.Blues, alpha=0.3)\nfor i in range(confusionmatrix.shape[0]):\n    for j in range(confusionmatrix.shape[1]):\n        ax.text(x=j, y=i,s=confusionmatrix[i, j], va='center', ha='center', size='xx-large')\n \nplt.xlabel('Predictions', fontsize=18)\nplt.ylabel('Actuals', fontsize=18)\nplt.title('Confusion Matrix', fontsize=18)","11afc2f6":"model_knnclassfier = KNeighborsClassifier()\nmodel_knnclassfier.fit(x_train, y_train)\nprint('KNeighborsClassifier accuracy: {:.4f}'.format(accuracy_score(y_test, model_knnclassfier.predict(x_test))))","d339c76e":"confusionmatrix = confusion_matrix(y_test, model_knnclassfier.predict(x_test))\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(3, 3))\nax.matshow(confusionmatrix, cmap=plt.cm.Blues, alpha=0.3)\nfor i in range(confusionmatrix.shape[0]):\n    for j in range(confusionmatrix.shape[1]):\n        ax.text(x=j, y=i,s=confusionmatrix[i, j], va='center', ha='center', size='xx-large')\n \nplt.xlabel('Predictions', fontsize=18)\nplt.ylabel('Actuals', fontsize=18)\nplt.title('Confusion Matrix', fontsize=18)","9ac43662":"model_xgb = XGBClassifier()\nmodel_xgb.fit(x_train, y_train)\nprint('XGBoostClassifier accuracy: {:.4f}'.format(accuracy_score(y_test, model_xgb.predict(x_test))))","95b2468d":"confusionmatrix = confusion_matrix(y_test, model_xgb.predict(x_test))\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(3, 3))\nax.matshow(confusionmatrix, cmap=plt.cm.Blues, alpha=0.3)\nfor i in range(confusionmatrix.shape[0]):\n    for j in range(confusionmatrix.shape[1]):\n        ax.text(x=j, y=i,s=confusionmatrix[i, j], va='center', ha='center', size='xx-large')\n \nplt.xlabel('Predictions', fontsize=18)\nplt.ylabel('Actuals', fontsize=18)\nplt.title('Confusion Matrix', fontsize=18)","0530ed9e":"model_svm = SVC()\nmodel_svm.fit(x_train, y_train)\nprint('SVM accuracy: {:.4f}'.format(accuracy_score(y_test, model_svm.predict(x_test))))","26564f55":"confusionmatrix = confusion_matrix(y_test, model_svm.predict(x_test))\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(3, 3))\nax.matshow(confusionmatrix, cmap=plt.cm.Blues, alpha=0.3)\nfor i in range(confusionmatrix.shape[0]):\n    for j in range(confusionmatrix.shape[1]):\n        ax.text(x=j, y=i,s=confusionmatrix[i, j], va='center', ha='center', size='xx-large')\n \nplt.xlabel('Predictions', fontsize=18)\nplt.ylabel('Actuals', fontsize=18)\nplt.title('Confusion Matrix', fontsize=18)","0429b54e":"model_selection_dict = {\"Logistic Regression\" : accuracy_score(y_test, model_logistic.predict(x_test)),\n                           \"Random Forest Classifier\" : accuracy_score(y_test, model_randomforest.predict(x_test)),\n                               \"XGBoost Classifier\" :accuracy_score(y_test, model_xgb.predict(x_test)),\n                                    \"KNN Classifier\":accuracy_score(y_test, model_knnclassfier.predict(x_test)),\n                                        \"SVM\": accuracy_score(y_test, model_svm.predict(x_test))\n                       }\n\npd.DataFrame(model_selection_dict.items(), columns=['Model','Accuracy Score'])","6a5f736b":"print(classification_report(y_test, model_xgb.predict(x_test)))","48c8e1c3":"predictedvalues= pd.DataFrame({'Actual': y_test, 'Predicted': model_xgb.predict(x_test)})\npredictedvalues","189475ea":"from sklearn.model_selection import GridSearchCV\n\nparam_test1 = {\n 'max_depth':range(3,10,2),\n 'min_child_weight':range(1,6,2)\n}\ngsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n\ngsearch1.fit(x,y)\n\n\nprint(\"Tuned XGBoost Parameters: {}\".format(gsearch1.best_params_))\nprint(\"Best score is {}\".format(gsearch1.best_score_))","db11a186":"confusionmatrix = confusion_matrix(y_test, gsearch1.predict(x_test))\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(3, 3))\nax.matshow(confusionmatrix, cmap=plt.cm.Blues, alpha=0.3)\nfor i in range(confusionmatrix.shape[0]):\n    for j in range(confusionmatrix.shape[1]):\n        ax.text(x=j, y=i,s=confusionmatrix[i, j], va='center', ha='center', size='xx-large')\n \nplt.xlabel('Predictions', fontsize=18)\nplt.ylabel('Actuals', fontsize=18)\nplt.title('Confusion Matrix', fontsize=18)","3884e08f":"print(classification_report(y_test, gsearch1.predict(x_test)))","68eff9c1":"predicted_tuned_values= pd.DataFrame({'Actual': y_test, 'Predicted': model_xgb.predict(x_test)})\npredicted_tuned_values.to_csv(\"final_predictions_with_tuning.csv\", index=False)\npredicted_tuned_values","ef10da27":"## Dataset Description\nSource : https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+%28Diagnostic%29\n\nDataset Description : Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.  They describe characteristics of the cell nuclei present in the image. A few of the images can be found at http:\/\/www.cs.wisc.edu\/~street\/images\/\n\nThe number of instances present in the model are 569, with 32 attributes.\n\n\n\n## Attribute Information\nID number\nDiagnosis (M = malignant, B = benign)\n\nTen real-valued features are computed for each cell nucleus:\n\n    1) radius (mean of distances from center to points on the perimeter)\n\t2) texture (standard deviation of gray-scale values)\n\t3) perimeter\n\t4) area\n\t5) smoothness (local variation in radius lengths)\n\t6) compactness (perimeter^2 \/ area - 1.0)\n\t7) concavity (severity of concave portions of the contour)\n\t8) concave points (number of concave portions of the contour)\n\t9) symmetry \n\t10)fractal dimension (\"coastline approximation\" - 1)\n\n\n## Task\nPredict the type of breast cancer","beb5cbc5":"A 0.99 precision is not bad! But there is still some scope of improvement. We might be able to get a slightly\nbetter result if we had tuned our model. We will do that ahead","5376effa":"Let us analyse our XGBoost model, by checking its precision, recall value and f1 score. test We do this by printing a classification report between the y_test values which we had separated from the dataset containing the actual answer to the test set variables, and the model predictions of the x_test set which was untrained and the class variable was unknown to it","743167c8":"#### Testing for XGBoostClassifier","0ea3148f":"A 100% precision. So now our model is well trained with the dataset in hand. It might produce some errors in\nreal world datasets, but this was just a beginner project and everything built can always be improved further","07550207":"#### Testing for K Neighbours Classifier","9d664572":"# Breast Cancer Detection","d9964b02":"### XGBoost Classifier","1d5c4b99":"#### Testing for Logistic Regression","6a286e7d":"### Model Selection","251cd2d4":"From the above pie chart, we can see that 62.7% of our entries have Benign Type Cancer and 37.3% have Malignant Cancer. ","59d116c4":"### EDA and Pre-Processing\n\nIn this section of our notebook, we perform some exploratory data analysis on our dataframe to get a general idea of what our dataframe consists of and to manipulate it if required.","7f0431b0":"After tuning our hyper parameters, we can see our accuracy has gone up to 99.45%, which is roughly 1%\nhigher than our previous prediction score. To analyse our tuned model further we will plot the confusion matrix and print a classification report","18a8842f":"From our above plots, it that the radius mean,texture mean, perimeter mean, area mean, smoothness mean,\ncompactness mean, concavity mean, concave points mean, symmetry mean and the fractal dimenion mean is \nsignificantly varying in the different types of tumors.","b580ac55":"Now that we have sampled our data and performed our basic analysism we will move on to testing our dataset for best the model","42f19bc7":"#### Testing for SVM","5ef7b4f3":"#### Testing for Random Forest Classifier","56464c50":"In our model seletion analysis we can see that the XGBoostClassifier Model has the maximum score of 98.25% accuracy. Let us furhter analyse this model before confirming our predictions.","ae244e5a":"Since we have trained our model with 98.25% accuracy, we might not need hyper parameter tuning but let us check it out just in case it helps our case"}}