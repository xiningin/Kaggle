{"cell_type":{"9a2907ad":"code","26c73014":"code","efaf693f":"code","34d84757":"code","8b341ebf":"code","999b8441":"code","88297a59":"code","7aa4a937":"code","559f6762":"code","11ca38e3":"code","f3327700":"code","11e28c5c":"code","b80d42d0":"code","8f55814c":"code","55881571":"code","599a7cb0":"code","a20fd070":"code","f5676f82":"code","bb9b9e0f":"code","4ad4bef0":"code","8bb126dc":"code","ded5462c":"code","adaa24c7":"code","bc084ab9":"code","f3549efa":"code","7b057e68":"code","3cf10f67":"code","75c319ad":"code","60e0d5cf":"code","63454eb1":"code","0109bf77":"code","3cc69e4e":"code","2b65ca33":"code","0b0fbfe7":"code","32e5d441":"code","e112c3ba":"code","d0d88536":"code","645f1baf":"code","0a992639":"code","e51cf461":"code","6f6b9a40":"code","12c5aa93":"code","a5a66a7f":"code","b6aeb4d4":"code","fe34b267":"code","9dc30370":"code","f349ca08":"code","dbeb02b2":"code","37f5cc58":"code","ff06de55":"code","cc54dcf2":"code","d645d209":"code","d9cf499a":"code","c891d41d":"code","b2baa582":"code","38f627b1":"code","eeaa9f05":"code","a24a848c":"code","6aa1dad5":"code","9a7293d3":"code","504932d0":"code","1e9977ab":"code","081eb154":"code","0b23f698":"code","18dde436":"code","a0680c92":"code","051cd967":"code","4b07f44e":"code","ce6df920":"code","64905075":"code","c960b54b":"code","fb711f98":"code","ff53887c":"code","1ea96279":"code","b525cd80":"code","94732a84":"code","44de73b2":"code","232df290":"code","a207ec97":"code","39c9ff24":"code","444dacd4":"code","b0777150":"code","12aa95a6":"code","a3cc3a74":"code","62e52131":"code","d94a3a86":"code","057c4282":"code","874b7d7e":"code","c6073e33":"code","72807a09":"code","56e9d695":"code","c4e86a4c":"code","569774a2":"code","d68d2ef5":"code","78ca7896":"code","a4095386":"code","acc936ba":"code","87401aeb":"code","7b973b22":"code","a63fcf44":"code","0bde192b":"code","6e6db36f":"code","87bafd1a":"code","2b97d56a":"code","0be5c3ef":"code","22fb1867":"code","df38c0d7":"code","e86973fb":"code","efb67ebd":"code","af8d1bf5":"code","9712632c":"code","be965e89":"code","cccb842f":"code","69825762":"code","7f036022":"code","59b9efb8":"code","1e20e8c5":"code","074a33fa":"code","48e82902":"code","5ced2d0b":"code","b307da0f":"code","43ee4cee":"code","4b6f6a2c":"code","01120772":"code","cb92f686":"code","62c364a5":"code","7a3018b2":"code","b609974d":"code","aca5b7ca":"code","d75424f5":"code","0fa99bc2":"code","b04e170b":"code","ccde2d92":"code","e37a4a06":"code","00a230f0":"code","00fb7b1a":"code","79f00147":"code","36234a3e":"code","34c6735f":"code","6d0f9f6a":"code","9d01b890":"code","a1c621cc":"code","127ae2ab":"code","bb96d210":"code","16a92b31":"code","b7951461":"code","55434e75":"code","cc5bc587":"code","682a2a69":"code","b6d2fa71":"code","dabed234":"code","64735e30":"code","de001df2":"code","0a6cf1b0":"code","81a6d99e":"code","6945f9fa":"code","0e503a1a":"code","7cc652cd":"code","0b96137f":"code","bd14a907":"code","37a7c404":"code","96911056":"code","94530e0a":"code","2f3db7bd":"code","87ad9c58":"code","5ea59941":"code","9343c930":"code","9915c3e4":"code","e40a517e":"code","81321792":"code","13c4240f":"code","2624028b":"code","297900e7":"code","4edc0722":"code","bd94b56d":"code","03dabd31":"code","f7cd3e1b":"code","8a782ebc":"code","f4a0b0d2":"code","7457224f":"code","fc811b22":"code","1a06cc98":"code","03368e7c":"code","43d208a4":"code","6bb5dfb8":"code","95863b9f":"code","b6a3b2ca":"code","4552a6b9":"code","30338ae8":"code","fa871c0c":"code","89b3b86f":"code","f5d50faa":"code","5dab4fb9":"code","73f003b6":"code","8d234055":"code","5a462008":"code","9b70ff74":"code","591e2d49":"markdown","bff84a75":"markdown","2c62450a":"markdown","12951c68":"markdown","aef98c67":"markdown","27bc0125":"markdown","9e594e00":"markdown","0b72eafe":"markdown","ec6719c0":"markdown","dcc60168":"markdown","b936b900":"markdown","f38c4101":"markdown","4cafd3f0":"markdown","63180c52":"markdown","5b1cae7d":"markdown","1def9b3d":"markdown","18a88fbe":"markdown","e1975d25":"markdown","11ac3c6a":"markdown","544f2a41":"markdown","fc0a1b27":"markdown","2161d1c2":"markdown","1d17e6a2":"markdown","3c6df10e":"markdown","b21743da":"markdown","74fb836b":"markdown","6acd36ee":"markdown","45012e42":"markdown","212605b8":"markdown","bc5199c8":"markdown","3713d932":"markdown","fb1de7d5":"markdown","a62eb0be":"markdown","c9bb7972":"markdown","7175d7c2":"markdown","03e615e0":"markdown","d12b9aa2":"markdown","d4d3831d":"markdown","4385e73b":"markdown","1c6e6bc7":"markdown","da42e689":"markdown","d8446847":"markdown","3d34354e":"markdown","ad3a43a3":"markdown","88233c23":"markdown","5e7c413c":"markdown","b1ee5164":"markdown","1c68d783":"markdown","8328f629":"markdown","c190e762":"markdown","e5138fa0":"markdown","b49a1d04":"markdown","382c964d":"markdown","dcb5eec3":"markdown","d26238bb":"markdown","1e23d8f2":"markdown","70d5fab6":"markdown","f5bb38b4":"markdown","2361e1fc":"markdown","e363c9bb":"markdown","85b3ae54":"markdown","c1bef59d":"markdown","591eba5d":"markdown","6cd94a00":"markdown","ee9c1ad0":"markdown","60b2511d":"markdown","a1df2a3e":"markdown","fc43fcaf":"markdown","00f09211":"markdown","8be4c510":"markdown","281ab139":"markdown","ed22d029":"markdown","a450e139":"markdown","6a8e01fd":"markdown","1a8fdd42":"markdown","2d0c9b05":"markdown","7c4effb6":"markdown","5955deb9":"markdown","b89b5449":"markdown","ba4c55a8":"markdown","bc6d7e91":"markdown","3c89d8e2":"markdown","073aecd7":"markdown","2b84999f":"markdown","e72ae81b":"markdown","8afda19a":"markdown","05844032":"markdown","0dc633a2":"markdown","c431cbda":"markdown","3215052b":"markdown","e2671147":"markdown","a14dc247":"markdown","98f26808":"markdown","8202ff7d":"markdown","02bcdf16":"markdown","4c19e9a8":"markdown","ef687d8d":"markdown","a41c4357":"markdown","601cc23c":"markdown","c2a989c1":"markdown","44d810bf":"markdown","758620df":"markdown","eeab00f5":"markdown","2de67341":"markdown","e9a1003e":"markdown","05cf8b59":"markdown","4b521fcf":"markdown","2d2493c5":"markdown","94906a4e":"markdown","405796ad":"markdown","4096d43a":"markdown","6c888017":"markdown","48dce786":"markdown","1521d77a":"markdown","49aad0e7":"markdown","bc6ee10e":"markdown","3d13f0ec":"markdown"},"source":{"9a2907ad":"import pandas as pd\nimport numpy as np\nimport multiprocessing\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport gc\nfrom time import time\nimport datetime\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, TimeSeriesSplit, train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nimport graphviz\nwarnings.simplefilter('ignore')\nsns.set()\n%matplotlib inline","26c73014":"files = ['..\/input\/test_identity.csv', \n         '..\/input\/test_transaction.csv',\n         '..\/input\/train_identity.csv',\n         '..\/input\/train_transaction.csv',\n         '..\/input\/sample_submission.csv']","efaf693f":"%%time\ndef load_data(file):\n    return pd.read_csv(file)\n\nwith multiprocessing.Pool() as pool:\n    test_id, test_tr, train_id, train_tr, sub = pool.map(load_data, files)","34d84757":"train = pd.merge(train_tr, train_id, on='TransactionID', how='left')\ntest = pd.merge(test_tr, test_id, on='TransactionID', how='left')\n\ndel test_id, test_tr, train_id, train_tr\ngc.collect();","8b341ebf":"def plot_numerical(feature):\n    \"\"\"\n    Plot some information about a numerical feature for both train and test set.\n    Args:\n        feature (str): name of the column in DataFrame\n    \"\"\"\n    fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(16, 18))\n    sns.kdeplot(train[feature], ax=axes[0][0], label='Train');\n    sns.kdeplot(test[feature], ax=axes[0][0], label='Test');\n\n    sns.kdeplot(train[train['isFraud']==0][feature], ax=axes[0][1], label='isFraud 0')\n    sns.kdeplot(train[train['isFraud']==1][feature], ax=axes[0][1], label='isFraud 1')\n\n    test[feature].index += len(train)\n    axes[1][0].plot(train[feature], '.', label='Train');\n    axes[1][0].plot(test[feature], '.', label='Test');\n    axes[1][0].set_xlabel('row index');\n    axes[1][0].legend()\n    test[feature].index -= len(train)\n\n    axes[1][1].plot(train[train['isFraud']==0][feature], '.', label='isFraud 0');\n    axes[1][1].plot(train[train['isFraud']==1][feature], '.', label='isFraud 1');\n    axes[1][1].set_xlabel('row index');\n    axes[1][1].legend()\n\n    pd.DataFrame({'train': [train[feature].isnull().sum()], 'test': [test[feature].isnull().sum()]}).plot(kind='bar', rot=0, ax=axes[2][0]);\n    pd.DataFrame({'isFraud 0': [train[(train['isFraud']==0) & (train[feature].isnull())][feature].shape[0]],\n                  'isFraud 1': [train[(train['isFraud']==1) & (train[feature].isnull())][feature].shape[0]]}).plot(kind='bar', rot=0, ax=axes[2][1]);\n\n    fig.suptitle(feature, fontsize=18);\n    axes[0][0].set_title('Train\/Test KDE distribution');\n    axes[0][1].set_title('Target value KDE distribution');\n    axes[1][0].set_title('Index versus value: Train\/Test distribution');\n    axes[1][1].set_title('Index versus value: Target distribution');\n    axes[2][0].set_title('Number of NaNs');\n    axes[2][1].set_title('Target value distribution among NaN values');\n    \n# This code is stolen from Chris Deotte. \ndef relax_data(df_train, df_test, col):\n    cv1 = pd.DataFrame(df_train[col].value_counts().reset_index().rename({col:'train'},axis=1))\n    cv2 = pd.DataFrame(df_test[col].value_counts().reset_index().rename({col:'test'},axis=1))\n    cv3 = pd.merge(cv1,cv2,on='index',how='outer')\n    factor = len(df_test)\/len(df_train)\n    cv3['train'].fillna(0,inplace=True)\n    cv3['test'].fillna(0,inplace=True)\n    cv3['remove'] = False\n    cv3['remove'] = cv3['remove'] | (cv3['train'] < len(df_train)\/10000)\n    cv3['remove'] = cv3['remove'] | (factor*cv3['train'] < cv3['test']\/3)\n    cv3['remove'] = cv3['remove'] | (factor*cv3['train'] > 3*cv3['test'])\n    cv3['new'] = cv3.apply(lambda x: x['index'] if x['remove']==False else 0,axis=1)\n    cv3['new'],_ = cv3['new'].factorize(sort=True)\n    cv3.set_index('index',inplace=True)\n    cc = cv3['new'].to_dict()\n    df_train[col] = df_train[col].map(cc)\n    df_test[col] = df_test[col].map(cc)\n    return df_train, df_test\n\ndef plot_categorical(train: pd.DataFrame, test: pd.DataFrame, feature: str, target: str, values: int=5):\n    \"\"\"\n    Plotting distribution for the selected amount of most frequent values between train and test\n    along with distibution of target\n    Args:\n        train (pandas.DataFrame): training set\n        test (pandas.DataFrame): testing set\n        feature (str): name of the feature\n        target (str): name of the target feature\n        values (int): amount of most frequest values to look at\n    \"\"\"\n    df_train = pd.DataFrame(data={feature: train[feature], 'isTest': 0})\n    df_test = pd.DataFrame(data={feature: test[feature], 'isTest': 1})\n    df = pd.concat([df_train, df_test], ignore_index=True)\n    df = df[df[feature].isin(df[feature].value_counts(dropna=False).head(values).index)]\n    train = train[train[feature].isin(train[feature].value_counts(dropna=False).head(values).index)]\n    fig, axes = plt.subplots(2, 1, figsize=(14, 12))\n    sns.countplot(data=df.fillna('NaN'), x=feature, hue='isTest', ax=axes[0]);\n    sns.countplot(data=train[[feature, target]].fillna('NaN'), x=feature, hue=target, ax=axes[1]);\n    axes[0].set_title('Train \/ Test distibution of {} most frequent values'.format(values));\n    axes[1].set_title('Train distibution by {} of {} most frequent values'.format(target, values));\n    axes[0].legend(['Train', 'Test']);","999b8441":"startdate = datetime.datetime.strptime('2017-12-01', '%Y-%m-%d')\ntrain['TransactionDT'] = train['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\ntest['TransactionDT'] = test['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))","88297a59":"fig, axes = plt.subplots(1, 1, figsize=(16, 6))\ntrain.set_index('TransactionDT').resample('D').mean()['isFraud'].plot(ax=axes).set_ylabel('isFraud mean', fontsize=14);\naxes.set_title('Mean of isFraud by day', fontsize=16);","7aa4a937":"fig, axes = plt.subplots(1, 1, figsize=(16, 6))\ntrain['TransactionDT'].dt.floor('d').value_counts().sort_index().plot(ax=axes).set_xlabel('Date', fontsize=14);\ntest['TransactionDT'].dt.floor('d').value_counts().sort_index().plot(ax=axes).set_ylabel('Number of training examples', fontsize=14);\naxes.set_title('Number of training examples by day', fontsize=16);\naxes.legend(['Train', 'Test']);","559f6762":"fig, ax1 = plt.subplots(figsize=(16, 6))\ntrain.set_index('TransactionDT').resample('D').mean()['isFraud'].plot(ax=ax1, color='blue')\nax1.tick_params(axis='y', labelcolor='blue')\nax1.set_ylabel('isFraud mean', color='blue', fontsize=14)\nax2 = ax1.twinx()\ntrain['TransactionDT'].dt.floor('d').value_counts().sort_index().plot(ax=ax2, color='tab:orange');\nax2.tick_params(axis='y', labelcolor='tab:orange');\nax2.set_ylabel('Number of training examples', color='tab:orange', fontsize=14);\nax2.grid(False)","11ca38e3":"y = train['isFraud']\nX = pd.DataFrame()\nX['card1'] = train['card1']\nX['card1_count'] = train['card1'].map(pd.concat([train['card1'], test['card1']], ignore_index=True).value_counts(dropna=False))","f3327700":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=47, stratify=y)\nclf = DecisionTreeClassifier(max_leaf_nodes=4)\nclf.fit(X_train, y_train)\nprint('ROC AUC score:', roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1]))","11e28c5c":"tree_graph = tree.export_graphviz(clf, out_file=None, max_depth = 10,\n    impurity = False, feature_names = X.columns, class_names = ['0', '1'],\n    rounded = True, filled= True )\ngraphviz.Source(tree_graph)","b80d42d0":"plt.figure(figsize=(14, 6))\nsns.kdeplot(X[y==1]['card1'], label='isFraud 1');\nsns.kdeplot(X[y==0]['card1'], label='isFraud 0');\nplt.plot([10881.5, 10881.5], [0.0000, 0.0001], sns.xkcd_rgb[\"black\"], lw=2);\nplt.plot([8750.0, 8750.0], [0.0000, 0.0001], sns.xkcd_rgb[\"red\"], lw=2);","8f55814c":"params = {'objective': 'binary', \"boosting_type\": \"gbdt\", \"subsample\": 1, \"bagging_seed\": 11, \"metric\": 'auc', 'random_state': 47}\nX_train, X_test, y_train, y_test = train_test_split(X['card1'], y, test_size=0.33, random_state=47, stratify=y)\nclf = lgb.LGBMClassifier(**params)\nclf.fit(X_train.values.reshape(-1, 1), y_train)\nprint('ROC AUC score', roc_auc_score(y_test, clf.predict_proba(X_test.values.reshape(-1, 1))[:, 1]))","55881571":"plt.figure(figsize=(12, 6))\nx = clf.predict_proba(X['card1'].sort_values().unique().reshape(-1, 1))[:, 1]\nx = pd.Series(x, index=X['card1'].sort_values().unique())\nsns.heatmap(x.to_frame(), cmap='RdBu_r', center=0.0);\nplt.xticks([]);","599a7cb0":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=47, stratify=y)\nclf = lgb.LGBMClassifier(**params)\nclf.fit(X_train, y_train)\nprint('ROC AUC score:', roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1]))","a20fd070":"plot_numerical('card1')","f5676f82":"def covariate_shift(feature):\n    df_card1_train = pd.DataFrame(data={feature: train[feature], 'isTest': 0})\n    df_card1_test = pd.DataFrame(data={feature: test[feature], 'isTest': 1})\n\n    # Creating a single dataframe\n    df = pd.concat([df_card1_train, df_card1_test], ignore_index=True)\n    \n    # Encoding if feature is categorical\n    if str(df[feature].dtype) in ['object', 'category']:\n        df[feature] = LabelEncoder().fit_transform(df[feature].astype(str))\n    \n    # Splitting it to a training and testing set\n    X_train, X_test, y_train, y_test = train_test_split(df[feature], df['isTest'], test_size=0.33, random_state=47, stratify=df['isTest'])\n\n    clf = lgb.LGBMClassifier(**params, num_boost_round=500)\n    clf.fit(X_train.values.reshape(-1, 1), y_train)\n    roc_auc =  roc_auc_score(y_test, clf.predict_proba(X_test.values.reshape(-1, 1))[:, 1])\n\n    del df, X_train, y_train, X_test, y_test\n    gc.collect();\n    \n    return roc_auc","bb9b9e0f":"print('Covariate Shift ROC AUC score:', covariate_shift('card1'))","4ad4bef0":"plot_categorical(train, test, 'ProductCD', 'isFraud')","8bb126dc":"print('Covariate shift ROC AUC:', covariate_shift('ProductCD'))","ded5462c":"y = train['isFraud']\nX = pd.DataFrame()\nX['card2'] = train['card2']\nX['card2_count'] = train['card2'].map(pd.concat([train['card2'], test['card2']], ignore_index=True).value_counts(dropna=False))\n\nresult_df = pd.DataFrame()\n\nfor i in X['card2'].sort_values().unique():\n    x = pd.DataFrame()\n    x['card2'] = [i] * X['card2_count'].nunique()\n    x['card2_count'] = X['card2_count'].sort_values().unique()\n    \n    result_df = pd.concat([result_df, x], axis=0)","adaa24c7":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=47, stratify=y)\nclf = lgb.LGBMClassifier(**params)\nclf.fit(X_train, y_train)\nprint('ROC AUC score:', roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1]))\n\npreds = clf.predict_proba(result_df)[:, 1]\npreds = preds.reshape(X['card2'].nunique(dropna=False), X['card2_count'].nunique(dropna=False))\npreds = pd.DataFrame(preds, index=X['card2'].sort_values().unique(), columns=X['card2_count'].sort_values().unique())\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 6))\nsns.heatmap(preds, cmap='RdBu_r', center=0.0);\nax.set_ylabel('card2');\nax.set_xlabel('card2_count');\nax.set_title('card2 \/ card2_count interaction');","bc084ab9":"test_X = pd.DataFrame()\ntest_X['card2'] = test['card2']\ntest_X['card2_count'] = test['card2'].map(pd.concat([train['card2'], test['card2']], ignore_index=True).value_counts(dropna=False))","f3549efa":"plt.figure(figsize=(12, 6))\nax = plt.axes()\nsc = plt.scatter(y=result_df['card2'], x=result_df['card2_count'], c=clf.predict_proba(result_df)[:, 1], cmap='RdBu_r');\nax.set_ylabel('card2');\nax.set_xlabel('card2_count');\nax.set_title('card2 \/ card2_count interaction');\nplt.colorbar(sc);\nplt.scatter(y=test_X['card2'], x=test_X['card2_count'], marker='x', c='white', alpha=0.5);","7b057e68":"plot_numerical('card2')","3cf10f67":"print('Covariate shift ROC AUC:', covariate_shift('card2'))","75c319ad":"plot_numerical('card3')","60e0d5cf":"print('Covariate shift ROC AUC:', covariate_shift('card3'))","63454eb1":"df_train = pd.DataFrame(data={'card4': train['card4'], 'isTest': 0})\ndf_test = pd.DataFrame(data={'card4': test['card4'], 'isTest': 1})\ndf = pd.concat([df_train, df_test], ignore_index=True)\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\nsns.countplot(data=df.fillna('NaN'), x='card4', hue='isTest', ax=axes[0]);\nsns.countplot(data=train[['card4', 'isFraud']].fillna('NaN'), x='card4', hue='isFraud', ax=axes[1]);\naxes[0].set_title('Train \/ Test distibution');\naxes[1].set_title('Train distibution by isFraud');\naxes[0].legend(['Train', 'Test']);","0109bf77":"print('Covariate shift ROC AUC:', covariate_shift('card4'))","3cc69e4e":"plot_numerical('card5')","2b65ca33":"print('Covariate shift ROC AUC:', covariate_shift('card5'))","0b0fbfe7":"df_train = pd.DataFrame(data={'card6': train['card6'], 'isTest': 0})\ndf_test = pd.DataFrame(data={'card6': test['card6'], 'isTest': 1})\ndf = pd.concat([df_train, df_test], ignore_index=True)\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\nsns.countplot(data=df.fillna('NaN'), x='card6', hue='isTest', ax=axes[0]);\nsns.countplot(data=train[['card6', 'isFraud']].fillna('NaN'), x='card6', hue='isFraud', ax=axes[1]);\naxes[0].set_title('Train \/ Test distibution');\naxes[1].set_title('Train distibution by isFraud');\naxes[0].legend(['Train', 'Test']);","32e5d441":"print('Covariate shift ROC AUC:', covariate_shift('card6'))","e112c3ba":"y = train['isFraud']\nX = pd.DataFrame()\nX['addr1'] = train['addr1']\nX['addr1_count'] = train['addr1'].map(pd.concat([train['addr1'], test['addr1']], ignore_index=True).value_counts(dropna=False))\nX['addr1'].fillna(0, inplace=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X['addr1'], y, test_size=0.33, random_state=47)\nclf = DecisionTreeClassifier(max_leaf_nodes=4)\nclf.fit(X_train.values.reshape(-1, 1), y_train)\nprint('ROC AUC score:', roc_auc_score(y_test, clf.predict_proba(X_test.values.reshape(-1, 1))[:, 1]))","d0d88536":"tree_graph = tree.export_graphviz(clf, out_file=None, max_depth = 10,\n    impurity = False, feature_names = ['addr1'], class_names = ['0', '1'],\n    rounded = True, filled= True )\ngraphviz.Source(tree_graph)","645f1baf":"plt.figure(figsize=(14, 6))\nsns.kdeplot(X[y==1]['addr1'], label='isFraud 1');\nsns.kdeplot(X[y==0]['addr1'], label='isFraud 0');\nplt.plot([50.0, 50.0], [0.0000, 0.008], sns.xkcd_rgb[\"black\"], lw=2);","0a992639":"params = {'objective': 'binary', \"boosting_type\": \"gbdt\", \"subsample\": 1, \"bagging_seed\": 11, \"metric\": 'auc', 'random_state': 47}\nX_train, X_test, y_train, y_test = train_test_split(X['addr1'], y, test_size=0.33, random_state=47, stratify=y)\nclf = lgb.LGBMClassifier(**params)\nclf.fit(X_train.values.reshape(-1, 1), y_train)\nprint('ROC AUC score:', roc_auc_score(y_test, clf.predict_proba(X_test.values.reshape(-1, 1))[:, 1]))","e51cf461":"plt.figure(figsize=(12, 6))\nx = clf.predict_proba(X['addr1'].sort_values().unique().reshape(-1, 1))[:, 1]\nx = pd.Series(x, index=X['addr1'].sort_values().unique())\nsns.heatmap(x.to_frame(), cmap='RdBu_r', center=0.0);\nplt.xticks([]);","6f6b9a40":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=47, stratify=y)\nclf = lgb.LGBMClassifier(**params)\nclf.fit(X_train, y_train)\nprint('ROC AUC score:', roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1]))","12c5aa93":"result_df = pd.DataFrame()\n\nfor i in X['addr1'].sort_values().unique():\n    x = pd.DataFrame()\n    x['addr1'] = [i] * X['addr1_count'].nunique()\n    x['addr1_count'] = X['addr1_count'].sort_values().unique()\n    \n    result_df = pd.concat([result_df, x], axis=0)","a5a66a7f":"preds = clf.predict_proba(result_df)[:, 1]\npreds = preds.reshape(X['addr1'].nunique(), X['addr1_count'].nunique())\npreds = pd.DataFrame(preds, index=X['addr1'].sort_values().unique(), columns=X['addr1_count'].sort_values().unique())\n\nplt.figure(figsize=(12, 6))\nsns.heatmap(preds, cmap='RdBu_r', center=0.0);","b6aeb4d4":"plot_numerical('addr1')","fe34b267":"print('Covariate shift ROC AUC score:', covariate_shift('addr1'))","9dc30370":"X = pd.DataFrame()\nX['addr1'] = train['addr1']\nX['card1'] = train['card1']\ny = train['isFraud']\nX['addr1'].fillna(0, inplace=True)\n\nX['addr1_card1'] = X['addr1'].astype(str) + '_' + X['card1'].astype(str)\nX['addr1_card1'] = LabelEncoder().fit_transform(X['addr1_card1'])","f349ca08":"X_train, X_test, y_train, y_test = train_test_split(X[['addr1', 'card1']], y, test_size=0.33, random_state=47, stratify=y)\nclf = lgb.LGBMClassifier(**params)\nclf.fit(X_train, y_train)\nprint('ROC AUC score:', roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1]))","dbeb02b2":"X_train, X_test, y_train, y_test = train_test_split(X[['addr1', 'card1', 'addr1_card1']], y, test_size=0.33, random_state=47, stratify=y)\nclf1 = lgb.LGBMClassifier(**params)\nclf1.fit(X_train, y_train)\nprint('ROC AUC score:', roc_auc_score(y_test, clf1.predict_proba(X_test)[:, 1]))","37f5cc58":"result_df = pd.DataFrame()\n\nfor i in tqdm_notebook(X['addr1'].sort_values().unique()):\n    x = pd.DataFrame()\n    x['addr1'] = [i] * X['card1'].nunique()\n    x['card1'] = X['card1'].sort_values().unique()\n    \n    result_df = pd.concat([result_df, x], axis=0)","ff06de55":"preds = clf.predict_proba(result_df)[:, 1]\npreds = preds.reshape(X['addr1'].nunique(), X['card1'].nunique())\npreds = pd.DataFrame(preds, index=X['addr1'].sort_values().unique(), columns=X['card1'].sort_values().unique())\nplt.figure(figsize=(12, 6))\nsns.heatmap(preds, cmap='RdBu_r', center=0.0);","cc54dcf2":"X['card1_count'] = train['card1'].map(pd.concat([train['card1'], test['card1']], ignore_index=True).value_counts(dropna=False))\nX['addr1_count'] = train['addr1'].map(pd.concat([train['addr1'], test['addr1']], ignore_index=True).value_counts(dropna=False))","d645d209":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=47, stratify=y)\nclf = lgb.LGBMClassifier(**params)\nclf.fit(X_train, y_train)\nprint('ROC AUC score:', roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1]))","d9cf499a":"train['nulls'] = train.isnull().sum(axis=1)\ntest['nulls'] = test.isnull().sum(axis=1)\nplot_numerical('nulls')","c891d41d":"print('Covariant shift ROC AUC:', covariate_shift('nulls'))","b2baa582":"plot_numerical('TransactionAmt')","38f627b1":"fig, axes = plt.subplots(1,1,figsize=(16, 6))\naxes.set_title('Moving average of TransactionAmt', fontsize=16);\ntrain[['TransactionDT', 'TransactionAmt']].set_index('TransactionDT').rolling(10000).mean().plot(ax=axes);\ntest[['TransactionDT', 'TransactionAmt']].set_index('TransactionDT').rolling(10000).mean().plot(ax=axes);\naxes.legend(['Train', 'Test']);","eeaa9f05":"fig, axes = plt.subplots(1, 1, figsize=(16, 6))\ntrain.set_index('TransactionDT').resample('D').mean()['TransactionAmt'].plot(ax=axes).set_ylabel('TransactionAmt mean', fontsize=14);\ntest.set_index('TransactionDT').resample('D').mean()['TransactionAmt'].plot(ax=axes).set_ylabel('TransactionAmt mean', fontsize=14);\naxes.set_title('Mean of TransactionAmt by day', fontsize=16);","a24a848c":"fig, ax1 = plt.subplots(figsize=(16, 6))\ntrain.set_index('TransactionDT').resample('D').mean()['isFraud'].plot(ax=ax1, color='blue')\nax1.tick_params(axis='y', labelcolor='blue')\nax1.set_ylabel('isFraud mean by day', color='blue', fontsize=14)\nax2 = ax1.twinx()\ntrain.set_index('TransactionDT').resample('D').mean()['TransactionAmt'].plot(ax=ax2, color='tab:orange')\nax2.tick_params(axis='y', labelcolor='tab:orange');\nax2.set_ylabel('TransactionAmt mean by day', color='tab:orange', fontsize=14);\nax2.grid(False)","6aa1dad5":"train['TransactionAmt_decimal'] = ((train['TransactionAmt'] - train['TransactionAmt'].astype(int)) * 1000).astype(int)\ntest['TransactionAmt_decimal'] = ((test['TransactionAmt'] - test['TransactionAmt'].astype(int)) * 1000).astype(int)\nplot_numerical('TransactionAmt_decimal')","9a7293d3":"fig, axes = plt.subplots(1, 1, figsize=(16, 6))\ntrain.set_index('TransactionDT').resample('D').mean()['TransactionAmt_decimal'].plot(ax=axes).set_ylabel('TransactionAmt_decimal mean', fontsize=14);\ntest.set_index('TransactionDT').resample('D').mean()['TransactionAmt_decimal'].plot(ax=axes).set_ylabel('TransactionAmt_decimal mean', fontsize=14);\naxes.set_title('Mean of TransactionAmt_decimal by day', fontsize=16);","504932d0":"fig, ax1 = plt.subplots(figsize=(16, 6))\ntrain.set_index('TransactionDT').resample('D').mean()['isFraud'].plot(ax=ax1, color='blue')\nax1.tick_params(axis='y', labelcolor='blue')\nax1.set_ylabel('isFraud mean by day', color='blue', fontsize=14)\nax2 = ax1.twinx()\ntrain.set_index('TransactionDT').resample('D').mean()['TransactionAmt_decimal'].plot(ax=ax2, color='tab:orange')\nax2.tick_params(axis='y', labelcolor='tab:orange');\nax2.set_ylabel('TransactionAmt_decimal mean by day', color='tab:orange', fontsize=14);\nax2.grid(False)","1e9977ab":"train['TransactionAmt_decimal_lenght'] = train['TransactionAmt'].astype(str).str.split('.', expand=True)[1].str.len()\ntest['TransactionAmt_decimal_lenght'] = test['TransactionAmt'].astype(str).str.split('.', expand=True)[1].str.len()","081eb154":"df_train = pd.DataFrame(data={'TransactionAmt_decimal_lenght': train['TransactionAmt_decimal_lenght'], 'isTest': 0})\ndf_test = pd.DataFrame(data={'TransactionAmt_decimal_lenght': test['TransactionAmt_decimal_lenght'], 'isTest': 1})\ndf = pd.concat([df_train, df_test], ignore_index=True)\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\nsns.countplot(data=df.fillna('NaN'), x='TransactionAmt_decimal_lenght', hue='isTest', ax=axes[0]);\nsns.countplot(data=train[['TransactionAmt_decimal_lenght', 'isFraud']].fillna('NaN'), x='TransactionAmt_decimal_lenght', hue='isFraud', ax=axes[1]);\naxes[0].set_title('Train \/ Test distibution');\naxes[1].set_title('Train distibution by isFraud');\naxes[0].legend(['Train', 'Test']);","0b23f698":"print('Covariant shift ROC AUC:', covariate_shift('TransactionAmt'))","18dde436":"print('Covariant shift ROC AUC:', covariate_shift('TransactionAmt_decimal'))","a0680c92":"print('Covariant shift ROC AUC:', covariate_shift('TransactionAmt_decimal_lenght'))","051cd967":"plot_numerical('V1')","4b07f44e":"print('Covariate shift:', covariate_shift('V1'))","ce6df920":"plot_numerical('V2')","64905075":"print('Covariate shift:', covariate_shift('V2'))","c960b54b":"plot_numerical('V3')","fb711f98":"print('Covariate shift:', covariate_shift('V3'))","ff53887c":"plot_numerical('V4')","1ea96279":"print('Covariate shift:', covariate_shift('V4'))","b525cd80":"plot_numerical('V5')","94732a84":"print('Covariate shift:', covariate_shift('V5'))","44de73b2":"plot_numerical('V6')","232df290":"print('Covariate shift:', covariate_shift('V6'))","a207ec97":"plot_numerical('V7')","39c9ff24":"print('Covariate shift:', covariate_shift('V7'))","444dacd4":"plot_numerical('V258')","b0777150":"print('Covariate shift:', covariate_shift('V258'))","12aa95a6":"train, test = relax_data(train, test, 'V258')\nplot_numerical('V258')","a3cc3a74":"plot_numerical('V294')","62e52131":"print('Covariate shift:', covariate_shift('V294'))","d94a3a86":"train, test = relax_data(train, test, 'V294')\nplot_numerical('V294')","057c4282":"plot_numerical('C1')","874b7d7e":"print('Covariate shift:', covariate_shift('C1'))","c6073e33":"train, test = relax_data(train, test, 'C1')\nplot_numerical('C1')","72807a09":"plot_numerical('C2')","56e9d695":"print('Covariate shift:', covariate_shift('C2'))","c4e86a4c":"train, test = relax_data(train, test, 'C2')\nplot_numerical('C2')","569774a2":"print('Covariate shift after data relaxation:', covariate_shift('C2'))","d68d2ef5":"plot_numerical('C3')","78ca7896":"print('Covariate shift:', covariate_shift('C3'))","a4095386":"train, test = relax_data(train, test, 'C3')\nplot_numerical('C3')","acc936ba":"print('Covariate shift after data relaxation:', covariate_shift('C3'))","87401aeb":"plot_numerical('C4')","7b973b22":"print('Covariate shift:', covariate_shift('C4'))","a63fcf44":"train, test = relax_data(train, test, 'C4')\nplot_numerical('C4')","0bde192b":"print('Covariate shift after data relaxation:', covariate_shift('C4'))","6e6db36f":"plot_numerical('C5')","87bafd1a":"print('Covariate shift:', covariate_shift('C5'))","2b97d56a":"train, test = relax_data(train, test, 'C5')\nplot_numerical('C5')","0be5c3ef":"print('Covariate shift after data relaxation:', covariate_shift('C5'))","22fb1867":"plot_numerical('C6')","df38c0d7":"print('Covariate shift:', covariate_shift('C6'))","e86973fb":"train, test = relax_data(train, test, 'C6')\nplot_numerical('C6')","efb67ebd":"print('Covariate shift after data relaxation:', covariate_shift('C6'))","af8d1bf5":"plot_numerical('C7')","9712632c":"print('Covariate shift:', covariate_shift('C7'))","be965e89":"train, test = relax_data(train, test, 'C7')\nplot_numerical('C7')","cccb842f":"print('Covariate shift after data relaxation:', covariate_shift('C7'))","69825762":"plot_numerical('C8')","7f036022":"print('Covariate shift:', covariate_shift('C8'))","59b9efb8":"train, test = relax_data(train, test, 'C8')\nplot_numerical('C8')","1e20e8c5":"print('Covariate shift after data relaxation:', covariate_shift('C8'))","074a33fa":"plot_numerical('C9')","48e82902":"print('Covariate shift:', covariate_shift('C9'))","5ced2d0b":"train, test = relax_data(train, test, 'C9')\nplot_numerical('C9')","b307da0f":"print('Covariate shift after data relaxation:', covariate_shift('C9'))","43ee4cee":"plot_numerical('C10')","4b6f6a2c":"print('Covariate shift:', covariate_shift('C10'))","01120772":"train, test = relax_data(train, test, 'C10')\nplot_numerical('C10')","cb92f686":"print('Covariate shift after data relaxation:', covariate_shift('C10'))","62c364a5":"plot_numerical('C11')","7a3018b2":"print('Covariate shift:', covariate_shift('C11'))","b609974d":"train, test = relax_data(train, test, 'C11')\nplot_numerical('C11')","aca5b7ca":"print('Covariate shift after data relaxation:', covariate_shift('C11'))","d75424f5":"plot_numerical('C12')","0fa99bc2":"print('Covariate shift:', covariate_shift('C12'))","b04e170b":"train, test = relax_data(train, test, 'C12')\nplot_numerical('C12')","ccde2d92":"print('Covariate shift after data relaxation:', covariate_shift('C12'))","e37a4a06":"plot_numerical('C13')","00a230f0":"print('Covariate shift:', covariate_shift('C13'))","00fb7b1a":"train, test = relax_data(train, test, 'C13')\nplot_numerical('C13')","79f00147":"print('Covariate shift after data relaxation:', covariate_shift('C13'))","36234a3e":"plot_numerical('C14')","34c6735f":"print('Covariate shift:', covariate_shift('C14'))","6d0f9f6a":"train, test = relax_data(train, test, 'C14')\nplot_numerical('C14')","9d01b890":"print('Covariate shift after data relaxation:', covariate_shift('C14'))","a1c621cc":"plot_numerical('D1')","127ae2ab":"print('Covariate shift:', covariate_shift('D1'))","bb96d210":"plot_numerical('D2')","16a92b31":"print('Covariate shift:', covariate_shift('D2'))","b7951461":"plot_numerical('D3')","55434e75":"print('Covariate shift:', covariate_shift('D3'))","cc5bc587":"plot_numerical('D4')","682a2a69":"print('Covariate shift:', covariate_shift('D4'))","b6d2fa71":"plot_numerical('D5')","dabed234":"print('Covariate shift:', covariate_shift('D5'))","64735e30":"plot_numerical('D6')","de001df2":"print('Covariate shift:', covariate_shift('D6'))","0a6cf1b0":"plot_numerical('D7')","81a6d99e":"print('Covariate shift:', covariate_shift('D7'))","6945f9fa":"plot_numerical('D8')","0e503a1a":"print('Covariate shift:', covariate_shift('D8'))","7cc652cd":"plot_numerical('D9')","0b96137f":"print('Covariate shift:', covariate_shift('D9'))","bd14a907":"plot_numerical('D10')","37a7c404":"print('Covariate shift:', covariate_shift('D10'))","96911056":"plot_numerical('D11')","94530e0a":"print('Covariate shift:', covariate_shift('D11'))","2f3db7bd":"plot_numerical('D12')","87ad9c58":"print('Covariate shift:', covariate_shift('D12'))","5ea59941":"plot_numerical('D13')","9343c930":"print('Covariate shift:', covariate_shift('D13'))","9915c3e4":"plot_numerical('D14')","e40a517e":"print('Covariate shift:', covariate_shift('D14'))","81321792":"plot_numerical('D15')","13c4240f":"print('Covariate shift:', covariate_shift('D15'))","2624028b":"plot_numerical('id_01')","297900e7":"plot_categorical(train, test, 'id_01', 'isFraud', 10)","4edc0722":"plot_numerical('id_02')","bd94b56d":"plot_categorical(train, test, 'id_03', 'isFraud', 10)","03dabd31":"plot_categorical(train, test, 'id_04', 'isFraud', 10)","f7cd3e1b":"plot_categorical(train, test, 'id_05', 'isFraud', 10)","8a782ebc":"plot_categorical(train, test, 'id_06', 'isFraud', 10)","f4a0b0d2":"plot_categorical(train, test, 'id_07', 'isFraud', 10)","7457224f":"plot_numerical('id_08')","fc811b22":"plot_categorical(train, test, 'id_08', 'isFraud', 10)","1a06cc98":"plot_categorical(train, test, 'id_09', 'isFraud', 10)","03368e7c":"plot_numerical('id_10')","43d208a4":"plot_categorical(train, test, 'id_10', 'isFraud', 10)","6bb5dfb8":"plot_numerical('id_11')","95863b9f":"plot_categorical(train, test, 'id_12', 'isFraud', 3)","b6a3b2ca":"plot_categorical(train, test, 'id_13', 'isFraud', 10)","4552a6b9":"plot_categorical(train, test, 'id_14', 'isFraud', 10)","30338ae8":"plot_categorical(train, test, 'id_15', 'isFraud', 4)","fa871c0c":"plot_categorical(train, test, 'id_16', 'isFraud', 3)","89b3b86f":"plot_numerical('id_17')","f5d50faa":"plot_categorical(train, test, 'id_17', 'isFraud', 10)","5dab4fb9":"plot_categorical(train, test, 'id_31', 'isFraud', 6)","73f003b6":"print('Covariate shift:', covariate_shift('id_31'))","8d234055":"train, test = relax_data(train, test, 'id_31')\nplot_categorical(train, test, 'id_31', 'isFraud', 6)","5a462008":"plot_numerical('id_31')","9b70ff74":"print('Covariate shift after data relaxation:', covariate_shift('id_31'))","591e2d49":"## id_31 after data relaxation","bff84a75":"# V7","2c62450a":"# id_04","12951c68":"# id_12","aef98c67":"# C7","27bc0125":"## C11 after data relaxation","9e594e00":"# ProductCD","0b72eafe":"# id_13","ec6719c0":"# V4","dcc60168":"# D6","b936b900":"# V294","f38c4101":"This is a heatmap with a probability of isFraud=1 for every unique value in the **card1** feature.\n\nThis picture reminds me an opening from a Total Recall movie. ","4cafd3f0":"# card4","63180c52":"# card2","5b1cae7d":"# D10","1def9b3d":"## C7 after data relaxation","18a88fbe":"## C3 after data relaxation","e1975d25":"Again training a gradient boosting model with only one feature.","11ac3c6a":"# id_07","544f2a41":"<a id=\"6\"><\/a>\n# TransactionAmt and it's decimal part\n\nFirst let's take a look at TransactionAmt feature and them I will create a new one - it's decimal part, which is a very popular way of creating a new features.","fc0a1b27":"# C2","2161d1c2":"![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F1696976%2F7153f1242daa586d6849c83242c3fe40%2F35267aee89a7552caf082b6bb0039aa5-full.png?generation=1564585074348507&alt=media)","1d17e6a2":"# id_08","3c6df10e":"Plotting this variable gives us such information as:\n* distribution in train and test set is almost equal.\n* distribution between target values differs, which make this feature so valuable\n* this feature doesn't have any NaNs","b21743da":"# id_10","74fb836b":"Now lets add a second feature - count encoded **card1** values.","6acd36ee":"# D1","45012e42":"Decimal part of transaction amount.","212605b8":"Moving average for TransactionAmt over time.","bc5199c8":"# C3","3713d932":"Finally adding count features, so all in all we have 5 features","fb1de7d5":"# C12 after data relaxation","a62eb0be":"ROC AUC score is close to 0.5\n\nThis feature also does not have any shift between train and test set.","c9bb7972":"But lets take a little step back and train a boosting model on only one original feature card1","7175d7c2":"# D11","03e615e0":"## C6 after data relaxation","d12b9aa2":"## C9 after data relaxation","d4d3831d":"Predictions heatmap.","4385e73b":"# D3","1c6e6bc7":"# id_16","da42e689":"# C8","d8446847":"Covariate shift for all 3 features.","3d34354e":"<a id=\"3\"><\/a>\n# card1 to addr1 interaction\n\nNext I am going to create a new feature out of this two features interaction and train on the result.","ad3a43a3":"The first split is by the values less than or equal to 10881.5 (black line) and the second one is 8750.0 (red line) and a tree does not use a count feature at all.","88233c23":"## C2 after data relaxation.","5e7c413c":"<a id=\"5\"><\/a>\n# New feature: number of NaN's\nWe have plenty of NaN's in this dataset and they can have a significant effect so why don't we use them?\nI am adding a new column to the dateset, which will contain a number of NaN for each row. So if a row (a single training example) contain, say, 10 NaNs, a new feature's value for this row will be 10.","b1ee5164":"# C12","1c68d783":"# D14","8328f629":"# card5","c190e762":"# id_31","e5138fa0":"# id_03","b49a1d04":"## C10 after data relaxation","382c964d":"# D15","dcb5eec3":"# C4","d26238bb":"# C9","1e23d8f2":"# C6","70d5fab6":"# V3","f5bb38b4":"# id_09","2361e1fc":"And now WITH interaction","e363c9bb":"# V2","85b3ae54":"# D8","c1bef59d":"# id_06","591eba5d":"# V258","6cd94a00":"# D12","ee9c1ad0":"# Transaction DT\nAccording to the official description 'TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).' I see people in some kernels assume that a start date is a 1 of December 2017, but to be honest the exact start date is not that important. \n\nSo lets transform TransactionDT into a datetime.","60b2511d":"# V5","a1df2a3e":"# id_11","fc43fcaf":"# D5","00f09211":"# D9","8be4c510":"# id_02","281ab139":"# id_14","ed22d029":"# addr1 ","a450e139":"# C14","6a8e01fd":"Holdout score has significantly increased. Lets create another heatmap and see why. \n\nThere are some darker spots in some intersections of the variable **card1** values and it's count encoded values. This is the reason of the holdout score improvement.\n\n*The image is pre-rendered since rendering takes some significant amount of time*","1a8fdd42":"# C10","2d0c9b05":"# id_05","7c4effb6":"Loading all datasets using multiprocessing. This speads up a process a bit.","5955deb9":"<a id=\"4\"><\/a>\nThis is where I want to introduce a little trick to you, called data relaxation. So what is it? In order to understand it take a look at the plot above. See the distibution difference between train and test set at a certain point? Gradient boosting algorithm doesn't know what to do with a data it has never seen so it will not approximate it well. And what we do by relaxing data is we are removing all the values from the train set that appears in it 3 times more often than in a test set and vice versa, also cleaning all the data that appears in train and test set only couple of times.\n\n## V258 after data relaxation","b89b5449":"# V6","ba4c55a8":"## V294 after data relaxation","bc6d7e91":"Making a count feature for card2 to perform the same experiment as with card1. First the heatmap for all possible interactions of card2 feature and it's count.","3c89d8e2":"We can see that this feature might be useful, but also keep in mind that covatiate shift is almost 0.7, which tells us that the distribution between train and test set has some difference.","073aecd7":"# D2","2b84999f":"<a id=\"1\"><\/a>\n# card1\nI have decided to start from one of the most important features of this dataset according to LightGBM feature_importance. And **card1** is one of those features.\n\nWhat I did is I've created a separate dataset with only this feature in it and also I added one more feature to this new dataset, which is an original feature's frequency (count) encoding. Why I did this? Well, you can reference [Santander Customer Transaction Prediction](https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction) competition, where this kind of encoding really boosted a score up. \n\nI'll make some visualizations (shoutout to [Chris Deotte](https:\/\/www.kaggle.com\/cdeotte)) to show you why that works and might work in this case as well.","e72ae81b":"A relationship between mean of TransactionAmt_decimal by day and a mean of isFraud by day.","8afda19a":"A relationship between mean of TransactionAmt by day and a mean of isFraud by day.","05844032":"# id_15","0dc633a2":"Another feature with a relatively high importance is **addr1**. According to the name of the feature we can assume that it contains some kind of users address, but in an encoded way. Also this time a feature have some missing values. We are going to fill them with 0.","c431cbda":"In this kernel I will do my EDA on the dataset, make some visualizations, try to find any insights and create some new features.\n\nJoin me, it promises to be a thrilling adventure.\n\nSome tricks being used:\n* [card1 count encoding](#1)\n* [Covariate Shift](#2)\n* [features interaction](#3)\n* [data relaxation](#4)\n\nNew engineered features:\n* [Number of NaNs](#5)\n* [TransactionAmt and it's decimal part](#6)","3215052b":"## C14 after data relaxation","e2671147":"# C5","a14dc247":"# id_17","98f26808":"Predictions heatmap of the two features interaction.","8202ff7d":"# D7","02bcdf16":"# C1","4c19e9a8":"And a scatter plot with a \"decision boundary\" of the model. White 'X' marks represents a test set examples.","ef687d8d":"So if we train a simple decision tree, using this two features we have an AUC slightly higher that 0.5. Let's see why by plotting this tree as a graph","a41c4357":"# C13 after data relaxation","601cc23c":"# card3","c2a989c1":"Distribution is the same, amount of NaN's is the same. Some difference in target value distribution. \n\nNext checking Covariate Shift for addr1.","44d810bf":"# D13","758620df":"ROC AUC score is close to 0.5, this means that this feature almost does not have any shift between train and test and is definitely worth keeping it.","eeab00f5":"First training a model only using this two features, without their interaction.","2de67341":"## C8 after data relaxation","e9a1003e":"And now combining both mean of isFraud by day and number of training examples by day into a single plot.","05cf8b59":"# card6","4b521fcf":"# C11","2d2493c5":"<a id=\"2\"><\/a>\nLets check a Covariate Shift of the feature. This means that we will try to distinguish whether a values correspond to a training set or to a testing set.","94906a4e":"## C1 after data relaxation","405796ad":"So far we are doing exactly the same thing that we have been doing for the previous variable.","4096d43a":"# V1","6c888017":"# D4","48dce786":"# C13","1521d77a":"## C4 after data relaxation","49aad0e7":"# id_01","bc6ee10e":"Lenght of the decimal part of transaction amount. What does it mean? Well, if lenght is 1 or 2 signs it is totaly understandable - it might be cents. But what is wrong with a decimal part's lenght being 3 and more sings? Maybe it is due to a currency convertion?","3d13f0ec":"## C5 after data relaxation"}}