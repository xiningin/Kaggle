{"cell_type":{"5cf0e2ab":"code","f4ba7ce1":"code","8caa1414":"code","2b0dda37":"code","c3fa4d6d":"code","9693b17d":"code","1541bc8e":"code","fb0b167f":"code","2982b742":"code","7635d952":"code","f5ff2d3c":"code","bd631539":"code","9c1f09fe":"code","01a3d056":"code","6d8ed734":"code","9be1331d":"code","8bf56f84":"code","0bf5dd6d":"code","393c638a":"code","ad517838":"code","568e781b":"code","2c10cdb0":"code","d6dae667":"code","5bc79e80":"code","3e477053":"code","691d5a83":"code","53b27215":"code","88fb6223":"code","6b822ddf":"markdown","51b2b74e":"markdown","b71adb76":"markdown","11e58d02":"markdown","c5659901":"markdown"},"source":{"5cf0e2ab":"# Load the data\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load the train and test files\ntrain = pd.read_csv(\"..\/input\/fakenewskdd2020\/train.csv\", sep='\\t', encoding='utf-8')\ntest = pd.read_csv(\"..\/input\/fakenewskdd2020\/test.csv\", sep='\\t', encoding='utf-8')","f4ba7ce1":"train.head()","8caa1414":"import nltk #Import NLTK ---> Natural Language Toolkit\nnltk.download('punkt')\nnltk.download('stopwords')","2b0dda37":"train['text'].loc[0]","c3fa4d6d":"train.text.loc[ : 5].values.tolist()[0]","9693b17d":"from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# create a function to tokenize the data\ndef preprocess_data(data):\n  \n  # 1. Tokenization\n  tk = RegexpTokenizer('\\s+', gaps = True)\n  text_data = [] # List for storing the tokenized data\n  for values in data.text:\n    tokenized_data = tk.tokenize(values) # Tokenize the news\n    text_data.append(tokenized_data) # append the tokenized data\n\n  # 2. Stopword Removal\n\n  # Extract the stopwords\n  sw = stopwords.words('english')\n  clean_data = [] # List for storing the clean text\n  # Remove the stopwords using stopwords\n  for data in text_data:\n    clean_text = [words.lower() for words in data if words.lower() not in sw]\n    clean_data.append(clean_text) # Appned the clean_text in the clean_data list\n  \n  # 3. Stemming\n\n  # Create a stemmer object\n  ps = PorterStemmer()\n  stemmed_data = [] # List for storing the stemmed data\n  for data in clean_data:\n    stemmed_text = [ps.stem(words) for words in data] # Stem the words\n    stemmed_data.append(stemmed_text) # Append the stemmed text\n  \n\n  # 4. tfidf vectorizer --> Term Frequency Inverse Document Frequency\n\n  '''TF-IDF stands for Term Frequency Inverse Document Frequency of records. \n     It can be defined as the calculation of how relevant a word in a series or corpus is to a text. \n     The meaning increases proportionally to the number of times in the text a word appears but is compensated \n     by the word frequency in the corpus (data-set).'''\n\n  '''Term Frequency: In document d, the frequency represents the number of instances of a given word t. \n     Therefore, we can see that it becomes more relevant when a word appears in the text, which is rational. \n     Since the ordering of terms is not significant, we can use a vector to describe the text in the bag of term models. \n     For each specific term in the paper, there is an entry with the value being the term frequency.'''\n\n     # tf(t,d) = count of t in d \/ number of words in d\n\n  '''Document Frequency: This tests the meaning of the text, which is very similar to TF, in the whole corpus collection. \n     The only difference is that in document d, TF is the frequency counter for a term t, while df is the number of occurrences \n     in the document set N of the term t. In other words, the number of papers in which the word is present is DF.'''\n\n     # df(t) = occurrence of t in documents\n\n  '''Inverse Document Frequency: Mainly, it tests how relevant the word is. \n     The key aim of the search is to locate the appropriate records that fit the demand. \n     Since tf considers all terms equally significant, it is therefore not only possible to use the term frequencies \n     to measure the weight of the term in the paper. First, find the document frequency of a term t by counting the \n     number of documents containing the term.'''\n\n  ''' df(t) = N(t)\n      where\n      df(t) = Document frequency of a term t\n      N(t) = Number of documents containing the term t'''\n\n      # Take the log, idf(t) = log(N\/ df(t))\n      # tf-idf(t, d) = tf(t, d) * idf(t)\n  \n  # Flatten the stemmed data\n\n  updated_data = []\n  for data in stemmed_data:\n    updated_data.append(\" \".join(data))\n\n  # TFID Vector object\n  tfidf = TfidfVectorizer()\n  tfidf_matrix = tfidf.fit_transform(updated_data)\n\n  return tfidf_matrix","1541bc8e":"# Call the above function on the merged data\ntrain_len = train.shape[0]\nmerged_data = pd.concat((train.drop('label', axis=1), test.drop('id', axis=1)), axis=0).reset_index().drop('index', axis=1)","fb0b167f":"# preprocess the merged data\npreprocessed_data = preprocess_data(merged_data)","2982b742":"train_data = preprocessed_data[ : train_len]\ntest_data = preprocessed_data[train_len : ]","7635d952":"# Model selection\nfrom sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(train_data, train.label, test_size=0.2, random_state = 42)","f5ff2d3c":"# Metrics\nfrom sklearn.metrics import accuracy_score\n\n# model\ndef compute_metrics(data, y_true, model_obj, model):\n\n  # Make predictions\n  y_pred = model_obj.predict(data)\n\n  # Compute accuracy\n  acc = accuracy_score(y_true = y_true, y_pred = y_pred)\n\n  # Make DataFrame\n  metrics = pd.DataFrame(data = np.array([acc]), index=[model], columns=['Accuracy Score'])\n  return metrics","bd631539":"# 1. LogisticRegression\nfrom sklearn.linear_model import LogisticRegressionCV\n\n# Model object\nlr_reg = LogisticRegressionCV(Cs=20, cv=3, random_state=42)\n\n# fit the model\nlr_reg.fit(X_train, y_train)","9c1f09fe":"# Compute the Logistic Regression Metrics\nlr_metrics =  compute_metrics(X_test, y_test, lr_reg, 'LogisticRegression')","01a3d056":"lr_metrics_train =  compute_metrics(X_train, y_train, lr_reg, 'LogisticRegression')","6d8ed734":"lr_metrics","9be1331d":"#2. Naive Bayes\nfrom sklearn.naive_bayes import MultinomialNB\n\n# Model Object\nmnb = MultinomialNB(alpha=0.0)\n\n# Fit the object\nmnb.fit(X_train, y_train)","8bf56f84":"# Compute metrics\nmnb_metrics = compute_metrics(X_test, y_test, mnb, 'Naive Bayes')\nmnb_metrics","0bf5dd6d":"# 3. DecisionTree\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Model Object\ndt_clf = DecisionTreeClassifier()\n\n# Fit the object\ndt_clf.fit(X_train, y_train)","393c638a":"dt_metrics = compute_metrics(X_test, y_test, dt_clf, \"DecisionTree\")","ad517838":"dt_metrics","568e781b":"from xgboost import XGBClassifier\n\n# XGB model\nxgb_model = XGBClassifier(n_estimators=200)\n\nxgb_model.fit(X_train, y_train)","2c10cdb0":"xgb_metrics = compute_metrics(X_test, y_test, xgb_model, 'XGBClassifier')","d6dae667":"# Concatenate all the metrics\nmodel_metrics = pd.concat((lr_metrics, mnb_metrics, dt_metrics, xgb_metrics), axis=0).sort_values(by='Accuracy Score', ascending=False)","5bc79e80":"model_metrics","3e477053":"# Make predictions --> XGBoost\npredictions = xgb_model.predict(test_data)\n# Submissions\ntest_ID = test.id\nsubmissions = pd.DataFrame({'id' : test_ID, 'label' : predictions})","691d5a83":"submissions.head()","53b27215":"submissions.to_csv(\".\/predictions_4.csv\", index=False) # Convert the submissions to .csv","88fb6223":"# make predictions--> Logistic Regression\npredictions2 = lr_reg.predict(test_data)\n# Submissions\ntest_ID = test.id\nsubmissions2 = pd.DataFrame({'id' : test_ID, 'label' : predictions2})\nsubmissions2.to_csv(\".\/predictions_5.csv\", index=False) # Convert the submissions to .csv","6b822ddf":"# load the data","51b2b74e":"# Bag of Words Pipeline","b71adb76":"# Developing Model","11e58d02":"**Logistic Regression Prediction**","c5659901":"**Making Predictions**\n\n**XGB Predictions**"}}