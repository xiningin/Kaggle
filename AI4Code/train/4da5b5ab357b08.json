{"cell_type":{"e9eee924":"code","67a88181":"code","4a7fbf53":"code","d3cedfdf":"code","a81397c6":"code","a58c2e63":"code","4072f7b6":"code","e773e5a0":"code","1ad27447":"code","18ef6e4a":"code","f09cd85d":"code","73ede9f9":"code","c775106c":"code","3166298e":"code","0fa60a1d":"code","7e455610":"code","b8192340":"code","c0df5c97":"code","e50aa93c":"code","fb535a42":"markdown","09bcfd24":"markdown","f93dad41":"markdown","53f2b9af":"markdown","69f9d8e9":"markdown","597c6da9":"markdown","a1ababe1":"markdown","e1490e61":"markdown","c4a07b72":"markdown","fc8ccffc":"markdown"},"source":{"e9eee924":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import LinearSVC, SVC\nfrom time import perf_counter\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\nfrom IPython.display import Markdown, display\ndef printmd(string):\n    # Print with Markdowns    \n    display(Markdown(string))\n    \ndf = pd.read_csv('..\/input\/spam-mails-dataset\/spam_ham_dataset.csv')","67a88181":"# Display the first rows\ndf.head(5)","4a7fbf53":"print(f'The dataset as a total of {df.shape[0]} E-Mails, which are categorized is ham and spam')","d3cedfdf":"df['label'].value_counts().plot.bar(color = [\"g\",\"r\"])\nplt.title('Total number of ham and spam in the dataset')\nplt.show()","a81397c6":"from nltk.tokenize import RegexpTokenizer\n\ndef clean_str(string, reg = RegexpTokenizer(r'[a-z]+')):\n    # Clean a string with RegexpTokenizer\n    string = string.lower()\n    tokens = reg.tokenize(string)\n    return \" \".join(tokens)\n\nprint('Before cleaning:')\ndf['text'][0]\n\n","a58c2e63":"print('After cleaning:')\nclean_str(df['text'][0])","4072f7b6":"# Create a new column with the cleaned messages\ndf['text_clean'] = df['text'].apply(lambda string: clean_str(string))\n\n# Display the result\ndf.head()","e773e5a0":"# Convert a collection of text documents to a matrix of token counts\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\nX = cv.fit_transform(df.text_clean)\n\n# Get the categories\ny = df.label","1ad27447":"# Split arrays or matrices into random train and test subsets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","18ef6e4a":"models = {\n    \"Random Forest\": {\"model\":RandomForestClassifier(), \"perf\":0},\n    \"Gradient Boosting\": {\"model\":GradientBoostingClassifier(), \"perf\":0},\n    \"XGBoost\": {\"model\":XGBClassifier(eval_metric='mlogloss'), \"perf\":0},\n    \"MultinomialNB\": {\"model\":MultinomialNB(), \"perf\":0},\n    \"Logistic Regr.\": {\"model\":LogisticRegression(), \"perf\":0},\n    \"KNN\": {\"model\":KNeighborsClassifier(), \"perf\":0},\n    \"Decision Tree\": {\"model\":DecisionTreeClassifier(), \"perf\":0},\n    \"SVM (Linear)\": {\"model\":LinearSVC(), \"perf\":0},\n    \"SVM (RBF)\": {\"model\":SVC(), \"perf\":0}\n}\n\nfor name, model in models.items():\n    start = perf_counter()\n    model['model'].fit(X_train, y_train)\n    duration = perf_counter() - start\n    duration = round(duration,2)\n    model[\"perf\"] = duration\n    print(f\"{name:20} trained in {duration} sec\")","f09cd85d":"models_acc = []\nfor name, model in models.items():\n    models_acc.append([name, model[\"model\"].score(X_test, y_test),model[\"perf\"]])","73ede9f9":"df_acc = pd.DataFrame(models_acc)\ndf_acc.columns = ['Model', 'Accuracy w\/o scaling', 'Training time (sec)']\ndf_acc.sort_values(by = 'Accuracy w\/o scaling', ascending = False, inplace=True)\ndf_acc.reset_index(drop = True, inplace=True)\ndf_acc","c775106c":"plt.figure(figsize = (15,5))\nsns.barplot(x = 'Model', y = 'Accuracy w\/o scaling', data = df_acc)\nplt.title('Accuracy on the test set\\n(the Y-Axis is between 0.8 and 1.0)', fontsize = 15)\nplt.ylim(0.8,1)\nplt.show()","3166298e":"plt.figure(figsize = (15,5))\nsns.barplot(x = 'Model', y = 'Training time (sec)', data = df_acc)\nplt.title('Training time for each model in sec', fontsize = 15)\nplt.ylim(0,20)\nplt.show()","0fa60a1d":"# Find the best hyperparameter with GridSearchCV\n# Exhaustive search over specified parameter values for an estimator.\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import GridSearchCV\nparameters = {\"alpha\": [0.2,1,2,5,10], \"fit_prior\": [True, False]}\n\ngrid = GridSearchCV(MultinomialNB(), param_grid=parameters)\ngrid.fit(X_train,y_train)\n\n# Create a DataFrame with the best Hyperparameters\npd.DataFrame(grid.cv_results_)[['params','mean_test_score']]\\\n                               .sort_values(by=\"mean_test_score\", ascending=False)","7e455610":"# Display the best hyperparameters\ngrid.best_params_","b8192340":"# Create the model with the best hyperparameters\nfrom sklearn.naive_bayes import MultinomialNB\nalpha, fit_prior = grid.best_params_['alpha'], grid.best_params_['fit_prior']\nmodel = MultinomialNB(alpha = alpha)\n\nmodel.fit(X_train,y_train)\ny_pred = model.predict(X_test)\n\nfrom sklearn.metrics import classification_report, accuracy_score\nprintmd(f'## Accuracy: {round(accuracy_score(y_test,y_pred),3)*100}%\\n')","c0df5c97":"print(classification_report(y_test,y_pred))","e50aa93c":"def display_result(df, number=2):\n    for i in range(number):\n        msg = df['text_clean'].iloc[i]\n        label = df[\"label\"].iloc[i]\n        msg_vec = cv.transform([msg])\n        pred_label = model.predict(msg_vec)\n        printmd(f\"**Real: {label}, Predicted: {pred_label[0]}**\")\n        printmd(f\"**E-Mail:** {msg}\")\n        printmd(\"_______________________________________________________________\")\n    \ndf_spam = df[df['label'] == 'spam']\ndf_ham = df[df['label'] == 'ham']\ndisplay_result(df_spam)\ndisplay_result(df_ham)","fb535a42":"# 3. Hyperparameter selection for MultinomialNB<a class=\"anchor\" id=\"3\"><\/a>","09bcfd24":"# 1. Loading, data explorating and preprocessing<a class=\"anchor\" id=\"1\"><\/a>","f93dad41":"The raw E-Mails have a lot of characters beside of the ones of the alphabet, which might provoke issues later. Therefore, the E-Mails will be cleaned. Remember that it is a basic model, which won't take count of punctuation.","53f2b9af":"# MultinomialNB: Lightweight, fast and powerfull!\n\n![simple-powerfull](https:\/\/i.imgur.com\/bLOdU7Q.png)","69f9d8e9":"# Table of contents\n\n[<h3>1. Loading, data explorating and preprocessing<\/h3>](#1)\n\n[<h3>2. Model comparison<\/h3>](#2)\n\n[<h3>3. Hyperparameter selection for MultinomialNB<\/h3>](#3)\n\n[<h3>4. The MultinomialNB Model<\/h3>](#4)\n\n[<h3>5. Example of predictions<\/h3>](#5)","597c6da9":"# 5. Example of predictions<a class=\"anchor\" id=\"5\"><\/a>","a1ababe1":"# 4. The MultinomialNB Model<a class=\"anchor\" id=\"4\"><\/a>","e1490e61":"XGBoosting gives the best result on the test set. Nevertheless, MultinomialNB is nearly as good, but is very fast (0.2 sec vs 5.1 sec). In the following part, we'll choose the MultinomialNB and try various hyperparameters to optimize it.","c4a07b72":"# Spam Classifier - Model comparison (accuracy > 97%)\n\n![emails](https:\/\/i.imgur.com\/5mT9Beb.png)\n\n ","fc8ccffc":"# 2. Model comparison<a class=\"anchor\" id=\"2\"><\/a>"}}