{"cell_type":{"d33257b6":"code","08611cdc":"code","8ed6deb0":"code","1163ffb9":"code","0c3585df":"code","a8b6ce25":"code","40165395":"code","9631ac13":"code","8e0b555c":"code","715e1008":"code","668af0ce":"markdown","88eb8c27":"markdown","0e719ba3":"markdown","18f0bd7a":"markdown","b8bdf0fc":"markdown","512f9923":"markdown","d155cee1":"markdown","4b4618c6":"markdown","bc244f4f":"markdown","de901433":"markdown","78e51fad":"markdown","72fbbeee":"markdown","747ce3fb":"markdown","aa13fc7b":"markdown","13d47eb3":"markdown","c953b045":"markdown","4a3318a6":"markdown","4fb7af93":"markdown","72509130":"markdown","3bf057e4":"markdown","ee000860":"markdown","2acad502":"markdown"},"source":{"d33257b6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\ntrain_data_dict = {\n        'Title': ['Mrs', 'Master', 'Mrs', 'Master', 'Mr', 'Mr'],\n        'Sex': ['F', 'M', 'F', 'F', 'M', 'M'],\n        'Survived' : [1, 1, 1, 0, 1, 0]\n        }\n\ntrain_data = pd.DataFrame(train_data_dict, columns = ['Title', 'Sex', 'Survived'])\n\ntrain_data.head()","08611cdc":"from math import log\nfrom math import log2\n\n#Entropy of System\nentropy_system = -(4\/5 * log2(4\/5) + 1\/5 * log2(1\/5))\n\n#Entropy of Splitting by Sex Feature\nentropy_female = -(2\/3 * log2(2\/3) + 1\/3 * log2(1\/3))\nentropy_male = -(2\/2 * log2(2\/2))\n\nentropy_sex = 3\/5 * entropy_female + 2\/5 * entropy_male\n\n#Entropy of Splitting by Title Feature\nentropy_mrs = -(2\/2 * log2(2\/2))\nentropy_master = -(1\/2 * log2(1\/2) + 1\/2 * log2(1\/2))\nentropy_mr = -(1\/1 * log2(1\/1))\n\nentropy_title = 2\/5 * entropy_mrs + 2\/5 * entropy_master + 1\/5 * entropy_mr\n\nprint('Entropy of System: %f' % (entropy_system))\nprint('Entropy Sex: %f' % (entropy_sex))\nprint('Entropy Title: %f' % (entropy_title))","8ed6deb0":"information_gain_sex = entropy_system - entropy_sex\ninformation_gain_title = entropy_system - entropy_title\n\nprint('Information Gain of Sex: %f' % (information_gain_sex))\nprint('Information Gain of Title: %f' % (information_gain_title))","1163ffb9":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","0c3585df":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic-preprocessed-train-data\/titanic_train_preprocessed.csv\")\ntrain_data.columns\n\ndefault_titanic_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ny = default_titanic_data['Survived']","a8b6ce25":"features = [\"PassengerId\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Cabin\", \"Embarked\", \"Family_Size\", \"Travelled_Together\",\n            \"Title\", \"Marry_Status\"]\nX = pd.get_dummies(train_data[features])","40165395":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)","9631ac13":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)\n\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","8e0b555c":"from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\nfrom sklearn.model_selection import GridSearchCV\nimport numpy as np\nnp.random.seed(0)\nimport random as rn\nrn.seed(0)\n\nparams = {\n    \"criterion\":(\"gini\", \"entropy\"), \n    \"splitter\":(\"best\", \"random\"), \n    \"max_depth\":(list(range(1, 20))), \n    \"min_samples_split\":[ 2, 3, 4, 6, 8, 10], \n    \"min_samples_leaf\":list(range(1, 20)), \n}\n\nclf = DecisionTreeClassifier(random_state=0)\n\ntree_cv = GridSearchCV(clf, params, scoring=\"accuracy\", n_jobs=-1, verbose=1, cv=3)\ntree_cv.fit(X_train, y_train)\nbest_params = tree_cv.best_params_\nprint(f\"Best paramters: {best_params})\")\n\nclf = DecisionTreeClassifier(**best_params)\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)","715e1008":"print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))","668af0ce":"# 1.1. Gini Index\n\npj is the probability that class j will occur. It is calculated for each class and the sum of the squares of the results is subtracted. \n\nThe Gini value gets a result between 0 and 1. \n\nValues close to 0 provide better separation in the decision tree. \n\nWe choose minimum Gini valued feature for node.\n\nGini Index formula;\n\n![gini.png](attachment:gini.png)","88eb8c27":"As shown, entropy of Sex feature is high rather than entropy of Title feature. We can say Sex feature is more uncertain than Title feature. \n\nSince the Title feature is more certain, it can divide the data more precisely.","0e719ba3":"**One Hot Encoding for categorical features**","18f0bd7a":"**Example:**\nLets create example train data set for predict Titanic Survived according to  passenger's Sex and Title features.","b8bdf0fc":"**Information gain is the measure of how much dataset uncertainty decreases after data set division.**\n\nWe can formulate as;\n\n![info_gain_formula.png](attachment:info_gain_formula.png)","512f9923":"**Training & Prediction**\n\nUsing GridSearchCV method we calculate best parameters for decision tree.\n\n* criterion\n\n> The function to measure the quality of a split. Supported criteria are \u201cgini\u201d for the Gini impurity and \u201centropy\u201d for the information gain.\n\n* splitter\n\n> The strategy used to choose the split at each node. Supported strategies are \u201cbest\u201d to choose the best split and \u201crandom\u201d to choose the best random split.\n\n* max_depth\n\n> The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n\n* min_samples_split\n\n> The minimum number of samples required to split an internal node\n\n* min_samples_leaf\n\n> The minimum number of samples required to be at a leaf node","d155cee1":"**Find Best Splits**\n\nWe want the choose node which describe our dataset as much as possible. This assumption also applies to interval nodes.\n\nSome methods for selecting a node are as follows;\n- Gini Index\n- Information Gain","4b4618c6":"**Normalization with Standard Scaler**","bc244f4f":"![decision_tree_nodes.jpeg](attachment:decision_tree_nodes.jpeg)","de901433":"Lets calculate;\n*  Entropy of entire system (Survived)\n* If we choose Sex feature for root node of decision tree, so calculate entropy of Sex feature\n* If we choose Title feature for root node of decision tree, so calculate entropy of Title feature","78e51fad":"![gini_example.jpeg](attachment:gini_example.jpeg)","72fbbeee":"Run as python script;","747ce3fb":"# 1.2. Information Gain\n\nInformation Gain, like Gini Index, is used for train decision trees. \n\nInformation Gain, measures the reduction in entropy by splitting a dataset according to selected feature.\n\n**What is Entropy?**\n\nEntropy is the measure of the uncertainty of the data set. \n\nCalculate with this formula; \n\n![entropy_formula.png](attachment:entropy_formula.png)\n\n**pi** is the probability of randomly taking an element of class i (the split of the dataset made by class i)\n","aa13fc7b":"# 1. Decision Tree Theoretical Description\n\nDecision trees is one of the tree-based algorithms used in Classification and Regression problems. It can be used in complex data sets.\n\nDecision trees contains three part as nodes;\n* root nodes\n> The first cells of decision trees are called root node. \n* interval nodes\n> Interval nodes found in the below of the root node and other interval nodes. Each observation is classified using nodes. As the number of nodes increases, the complexity of the model increases.\n* leaf nodes\n> Leaf node is found in bottom of the decision tree. Leaf nodes give us the result.\n\nEach observation is classified as \"Yes\" or \"No\" depending on the node condition.","13d47eb3":"# 1.3. Overfitting and Prunning\n\n**Overfitting**\n\nIf there is an outlier in the data set, the classification algorithm can learn the outliers in the training data set so algorithm get unsuccessful results in test dataset because same outlier not in the test dataset. This situation is referred to as overfitting in learning.\n\nThere is two approach for solve overfitting problem as;\n\n* Preprunning\n* Postprunning\n\n**Preprunning**\n\nPrepruning is stoping the growth of decision tree on an early stage. We can stop the growth by using some constraints like max_depth , min_samples etc.\n\n* max_depth: maximum depth of decision tree\n* min_sample: The minimum number of samples required to split an internal node\n\n\n**Postprunning**\n\nIn the postprunning, after the tree is created, some nodes are deleted from the tree by verifying with the validation dataset.","c953b045":"# 2. Decision Tree Application with using Titanic Dataset","4a3318a6":"**Decision Tree Algorithm**\n\nDecision trees building continues until it finds the purest class distribution. If class distrubition is not pure, tree continue to split data with features (forming nodes).\n\n![decision_tree_algorithm.png](attachment:decision_tree_algorithm.png)","4fb7af93":"# Decision Tree Tutorial\n\nThis tutorial include two parts as;\n\n1.  Decision Tree Theoretical Description    \n        1.1  Gini Index\n        1.2. Information Gain\n        1.3  Overfitting and Prunning\n    \n    \n2.   Decision Tree Application with using Titanic Dataset","72509130":"I had preprocessed for the titanic data set in my previous [my previous notebook](https:\/\/www.kaggle.com\/fethiye\/titanic-predict-survival-using-ensemble). You can see preprocessing step from this notebook.\n\nI will continue with the preprocessed data set called titanic_train_preprocessed.csv.","3bf057e4":"![entropi_example.png](attachment:entropi_example.png)","ee000860":"**Split data for train and test**","2acad502":"**Gini Calculation Example**\n\nIn titanic dataset if we want to calculate Gini of Sex feature according to Survived passenger possibilty."}}