{"cell_type":{"6c043716":"code","b405d636":"code","42e0d368":"code","270d2b18":"code","22a93458":"code","7b00a28e":"code","06e6adcd":"code","8d9e410e":"code","03bbd51b":"code","414518de":"code","96c19cb3":"code","758acc96":"code","a0a9abdc":"code","c9d36644":"code","41e981c5":"code","985dc2a9":"code","f3299906":"code","98985840":"code","4b0e57b2":"code","92cc3b6d":"code","3a617187":"code","20887007":"code","194dad38":"code","a617f797":"code","3018a6ce":"code","6edfb0bf":"code","5d60b9b7":"code","4acdc473":"code","5f88fe0b":"code","4904de62":"code","bd6bf09f":"code","3cc0301b":"code","12960d36":"code","eb707778":"code","51aec746":"code","10a4ef16":"code","984f9067":"code","c0db9c62":"code","652dd154":"code","69b7cc3e":"code","823eb0b1":"code","7e019cd2":"code","9f8838bb":"code","b6a9970a":"code","493e3261":"code","65ecb9f8":"code","d5a8f7b3":"code","6b4c5afc":"code","d0031ef3":"code","9b3f9bb4":"code","606c9b38":"code","ae203e5f":"code","d76e06e4":"code","e735148f":"code","45dd1e5c":"code","00790091":"markdown","8aca77be":"markdown","4ce2befa":"markdown","78d31ee2":"markdown","53db5507":"markdown","d553561c":"markdown","712f9b6a":"markdown","2858520c":"markdown","f7fb9f67":"markdown","a0055a7e":"markdown","2f9d7f72":"markdown","5af57ccf":"markdown","1768741c":"markdown","cbf99d17":"markdown"},"source":{"6c043716":"import math\nimport numpy as np\nimport pandas as pd\nimport holoviews as hv\nfrom holoviews import opts\nfrom bokeh.io import output_file, save, show\nimport scipy\nimport scipy.special\nfrom scipy.stats import boltzmann\nfrom scipy.special import gamma\nimport scipy.stats as stats\nhv.extension('bokeh')","b405d636":"def histogram(hist, x, pmf, cdf, label):\n    pmf = hv.Curve((x, pmf), label='PMF')\n    cdf = hv.Curve((x, cdf), label='CDF')\n    return (hv.Histogram(hist, vdims='P(r)').opts(fill_color=\"gray\") * pmf * cdf).relabel(label)","42e0d368":"label = \"Bernoulli Distribution (p=0.6)\"\np = 0.6\n\nmeasured = stats.bernoulli.rvs(size=100,p=0.6)\nhist = np.histogram(measured,density=True, bins=40)\n\nx = np.linspace(0, 2, 100)\npmf = stats.bernoulli.pmf(x, p, loc=0)\ncdf = stats.bernoulli.cdf(x, p)\nbern = histogram(hist, x, pmf, cdf, label)","270d2b18":"bern.opts(width = 800, height = 700 , show_grid=True)","22a93458":"def hist(p):\n    data = stats.bernoulli.rvs(size=100,p=0.6)\n    frequencies, edges = np.histogram(data, 40)\n    return hv.Histogram((edges, frequencies))\n\nhmap = hv.DynamicMap(hist, kdims=['p'])\nhmap.redim.range(p = (0.1,1)).opts(width = 700, height = 600 , show_grid=True).relabel('Bernoulli Histogram')","7b00a28e":"def pmf(p):\n    xs = np.linspace(0,2,10)\n    ys = [stats.bernoulli.pmf(x, p, loc=0) for x in xs]\n    return hv.Curve((xs, ys))\n\nhmap1 = hv.DynamicMap(pmf, kdims=['p'])\nhmap1.redim.range(p = (0.1,1)).opts(width = 700, height = 600 , show_grid=True).relabel('Bernoulli PMF')","06e6adcd":"def cdf(p):\n    xs = np.linspace(0, 2, 1000)\n    ys = [stats.bernoulli.cdf(x, p, loc=0) for x in xs]\n    return hv.Curve((xs, ys))\n\nhmap1 = hv.DynamicMap(cdf, kdims=['p'])\nhmap1.redim.range(p = (0.1,1)).opts(width = 700, height = 600 , show_grid=True).relabel('Bernoulli CDF')","8d9e410e":"label = \"Binomial Distribution (n = 25, p=0.6)\"\nn , p = 25, 0.6\n\nmeasured = stats.binom.rvs(n, p, size=1000)\nhist = np.histogram(measured,density=True, bins=40)\n\nx = np.linspace(0, 25, 1000)\npmf = stats.binom.pmf(x, n, p, loc=0)\ncdf = stats.binom.cdf(x, n, p, loc=0)\nbinom = histogram(hist, x, pmf, cdf, label)\n\nbinom.opts(width = 800, height = 700 , show_grid=True)","03bbd51b":"def hist(n, p):\n    data = stats.binom.rvs(n, p, size=1000)\n    frequencies, edges = np.histogram(data, 40)\n    return hv.Histogram((edges, frequencies))\n\nhmap = hv.DynamicMap(hist, kdims=['n', 'p'])\nhmap.redim.range(n= (20, 50),p = (0.1,1)).opts(width = 700, height = 600 , show_grid=True).relabel('Binomial Histogram')","414518de":"def pmf(n,p):\n    xs = np.arange(0,20)\n    ys = [stats.binom.pmf(x,n, p, loc=0) for x in xs]\n    return hv.Curve((xs, ys))\n\nhmap1 = hv.DynamicMap(pmf, kdims=['n','p'])\nhmap1.redim.range(n = (5,50),p = (0.1,1)).opts(width = 700, height = 600 , show_grid=True).relabel('Binom PMF')","96c19cb3":"def cdf(n,p):\n    xs = np.linspace(0, 20, 1000)\n    ys = [stats.binom.cdf(x,n, p) for x in xs]\n    return hv.Curve((xs, ys))\n\nhmap1 = hv.DynamicMap(cdf, kdims=['n','p'])\nhmap1.redim.range(n = (5,50),p = (0.1,1)).opts(width = 700, height = 600 , show_grid=True).relabel('Binom CDF')","758acc96":"label = \"Boltzman Distribution (lambda, N = 1.1, 20)\"\nlambda_, N = 1.1, 20\n\nmeasured = boltzmann.rvs(lambda_, N, size=1000)\nhist = np.histogram(measured,density=True, bins=40)\n\nx = np.linspace(0, 5, 1000)\npmf = stats.boltzmann.pmf(x, lambda_, N, loc=0)\ncdf = stats.boltzmann.cdf(x, lambda_, N, loc=0)\nbol = histogram(hist, x, pmf, cdf, label)\n\nbol.opts(width = 800, height = 700 , show_grid=True)","a0a9abdc":"def hist(lambda_, N):\n    data = boltzmann.rvs(lambda_, N, size=1000)\n    frequencies, edges = np.histogram(data, 40)\n    return hv.Histogram((edges, frequencies))\n\nhmap = hv.DynamicMap(hist, kdims=['lambda_', 'N'])\nhmap.redim.range(lambda_ = (0.5,2), N = (20, 100)).opts(width = 700, height = 600 , show_grid=True).relabel('Bolzmann Histogram')","c9d36644":"def pmf(lambda_, N):\n    xs = np.arange(0,20)\n    ys = [stats.boltzmann.pmf(x,lambda_, N, loc=0) for x in xs]\n    return hv.Curve((xs, ys))\n\nhmap1 = hv.DynamicMap(pmf, kdims=['lambda_', 'N'])\nhmap1.redim.range(lambda_ = (0.5,2), N = (20, 100)).opts(width = 700, height = 600 , show_grid=True).relabel('Bolzmann PMF')","41e981c5":"def cdf(lambda_, N):\n    xs = np.arange(0,20)\n    ys = [stats.boltzmann.cdf(x,lambda_, N, loc=0) for x in xs]\n    return hv.Curve((xs, ys))\n\nhmap1 = hv.DynamicMap(cdf, kdims=['lambda_', 'N'])\nhmap1.redim.range(lambda_ = (0.5,2), N = (20, 100)).opts(width = 700, height = 600 , show_grid=True).relabel('Bolzmann CDF')","985dc2a9":"label = \"Discrete Laplace Distribution (a = 0.8)\"\na = 0.8\n\nmeasured = stats.dlaplace.rvs(a, size=1000)\nhist = np.histogram(measured,density=True, bins=40)\n\nx = np.linspace(0, 5, 1000)\npmf = stats.dlaplace.pmf(x, a, loc=0)\ncdf = stats.dlaplace.cdf(x, a, loc=0)\ndlap = histogram(hist, x, pmf, cdf, label)\n\ndlap.opts(width = 800, height = 700 , show_grid=True)","f3299906":"def hist(a):\n    data = stats.dlaplace.rvs(lambda_, N, size=1000)\n    frequencies, edges = np.histogram(data, 40)\n    return hv.Histogram((edges, frequencies))\n\nhmap = hv.DynamicMap(hist, kdims=['a'])\nhmap.redim.range(a = (0.1,1.5)).opts(width = 700, height = 600 , show_grid=True).relabel('Discrete Laplace Histogram')","98985840":"def pmf(a):\n    xs = np.arange(0,30)\n    ys = [stats.dlaplace.pmf(x,a, loc=0) for x in xs]\n    return hv.Curve((xs, ys))\n\nhmap1 = hv.DynamicMap(pmf, kdims=['a'])\nhmap1.redim.range(a = (0.1,2)).opts(width = 700, height = 600 , show_grid=True).relabel('Descrete Laplace PMF')","4b0e57b2":"def cdf(a):\n    xs = np.arange(0,30)\n    ys = [stats.dlaplace.cdf(x,a, loc=0) for x in xs]\n    return hv.Curve((xs, ys))\n\nhmap1 = hv.DynamicMap(cdf, kdims=['a'])\nhmap1.redim.range(a = (0.1,2)).opts(width = 700, height = 600 , show_grid=True).relabel('Descrete Laplace CDF')","92cc3b6d":"label = \"Geometric Distribution (p = 0.6)\"\np = 0.6\n\nmeasured = stats.geom.rvs(p, size=1000)\nhist = np.histogram(measured,density=True, bins=40)\n\nx = np.linspace(0, 5, 1000)\npmf = stats.geom.pmf(x, p, loc=0)\ncdf = stats.geom.cdf(x, p, loc=0)\ngeo = histogram(hist, x, pmf, cdf, label)\n\ngeo.opts(width = 800, height = 700 , show_grid=True)","3a617187":"def hist(p):\n    data = stats.geom.rvs(p, size=1000)\n    frequencies, edges = np.histogram(data, 40)\n    return hv.Histogram((edges, frequencies))\n\nhmap = hv.DynamicMap(hist, kdims=['p'])\nhmap.redim.range(p = (0.1,1)).opts(width = 700, height = 600 , show_grid=True).relabel('Geometric Histogram')","20887007":"def pmf(p):\n    xs = np.arange(0,30)\n    ys = [stats.geom.pmf(x,p, loc=0) for x in xs]\n    return hv.Curve((xs, ys))\n\nhmap1 = hv.DynamicMap(pmf, kdims=['p'])\nhmap1.redim.range(p = (0.1,1)).opts(width = 700, height = 600 , show_grid=True).relabel('Geometric PMF')","194dad38":"def cdf(p):\n    xs = np.arange(0,30)\n    ys = [stats.geom.cdf(x,p, loc=0) for x in xs]\n    return hv.Curve((xs, ys))\n\nhmap1 = hv.DynamicMap(cdf, kdims=['p'])\nhmap1.redim.range(p = (0.1,1)).opts(width = 700, height = 600 , show_grid=True).relabel('Geometric CDF')","a617f797":"label = \"Hypergeometric Distribution (M = 50, n = 9, N = 21)\"\n[M, n, N] = [50, 9, 21]\n\nmeasured = stats.hypergeom.rvs(M, n, N, size=1000)\nhist = np.histogram(measured,density=True, bins=40)\n\nx = np.linspace(0, 10, 1000)\npmf = stats.hypergeom.pmf(x, M, n, N)\ncdf = stats.hypergeom.cdf(x, M, n, N)\nhyp = histogram(hist, x, pmf, cdf, label)\n\nhyp.opts(width = 800, height = 700 , show_grid=True)","3018a6ce":"def hist(M, n, N):\n    data = stats.hypergeom.rvs(M, n, N, size=1000)\n    frequencies, edges = np.histogram(data, 40)\n    return hv.Histogram((edges, frequencies))\n\nhmap = hv.DynamicMap(hist, kdims=['M', 'n', 'N'])\nhmap.redim.range(M = (30,100), n = (5, 20), N = (5, 30)).opts(width = 700, height = 600 , show_grid=True).relabel('Hypergeometric Histogram')","6edfb0bf":"def pmf(M, n, N):\n    xs = np.arange(0,10)\n    ys = [stats.hypergeom.pmf(x, M, n, N) for x in xs]\n    return hv.Curve((xs, ys))\n\nhmap1 = hv.DynamicMap(pmf, kdims=['M', 'n', 'N'])\nhmap1.redim.range(M = (30,100), n = (5, 20), N = (5, 30)).opts(width = 700, height = 600 , show_grid=True).relabel('Hypergeometric PMF')","5d60b9b7":"def cdf(M, n, N):\n    xs = np.arange(0,5)\n    ys = [stats.hypergeom.cdf(x, M, n, N) for x in xs]\n    return hv.Curve((xs, ys))\n\nhmap1 = hv.DynamicMap(cdf, kdims=['M', 'n', 'N'])\nhmap1.redim.range(M = (30,100), n = (5, 20), N = (5, 30)).opts(width = 700, height = 600 , show_grid=True).relabel('Hypergeometric CDF')","4acdc473":"label = \"Log Series Distribution (p = 0.6)\"\np = 0.6\n\nmeasured = stats.logser.rvs(p, size=1000)\nhist = np.histogram(measured,density=True, bins=40)\n\nx = np.linspace(0, 10, 1000)\npmf = stats.logser.pmf(x, p)\ncdf = stats.logser.cdf(x, p)\nlog = histogram(hist, x, pmf, cdf, label)\n\nlog.opts(width = 800, height = 700 , show_grid=True)","5f88fe0b":"def hist(p):\n    data = stats.logser.rvs(p, size=1000)\n    frequencies, edges = np.histogram(data, 40)\n    return hv.Histogram((edges, frequencies))\n\nhmap = hv.DynamicMap(hist, kdims=['p'])\nhmap.redim.range(p = (0.4,1)).opts(width = 700, height = 600 , show_grid=True).relabel('Log-Series Histogram')","4904de62":"def pmf(p):\n    xs = np.arange(0,10)\n    ys = [stats.logser.pmf(x,p) for x in xs]\n    return hv.Curve((xs, ys))\n\nhmap1 = hv.DynamicMap(pmf, kdims=['p'])\nhmap1.redim.range(p = (0.1,1)).opts(width = 700, height = 600 , show_grid=True).relabel('Log-series PMF')","bd6bf09f":"def cdf(p):\n    xs = np.arange(0,10)\n    ys = [stats.logser.cdf(x,p) for x in xs]\n    return hv.Curve((xs, ys))\n\nhmap1 = hv.DynamicMap(cdf, kdims=['p'])\nhmap1.redim.range(p = (0.1,1)).opts(width = 700, height = 600 , show_grid=True).relabel('Log-series CDF')","3cc0301b":"label = \"Negative Binomial Distribution (n = 20, p = 0.6)\"\nn, p = 20, 0.6\n\nmeasured = stats.nbinom.rvs(n, p, size=1000)\nhist = np.histogram(measured,density=True, bins=40)\n\nx = np.linspace(0, 30, 1000)\npmf = stats.nbinom.pmf(x,n, p)\ncdf = stats.nbinom.cdf(x,n, p)\nnb = histogram(hist, x, pmf, cdf, label)\n\nnb.opts(width = 800, height = 700 , show_grid=True)","12960d36":"def hist(n, p):\n    data = stats.nbinom.rvs(n, p, size=1000)\n    frequencies, edges = np.histogram(data, 40)\n    return hv.Histogram((edges, frequencies))\n\nhmap = hv.DynamicMap(hist, kdims=['n','p'])\nhmap.redim.range(n = (5, 30), p = (0.1,1)).opts(width = 700, height = 600 , show_grid=True).relabel('Negative Binomial Histogram')","eb707778":"def pmf(n, p):\n    xs = np.arange(0,100)\n    ys = [stats.nbinom.pmf(x,n,p) for x in xs]\n    return hv.Curve((xs, ys))\n\nhmap1 = hv.DynamicMap(pmf, kdims=['n','p'])\nhmap1.redim.range(n = (5, 15), p = (0.1,1)).opts(width = 700, height = 600 , show_grid=True).relabel('Negative Binomial PMF')","51aec746":"def cdf(n, p):\n    xs = np.arange(0,100)\n    ys = [stats.nbinom.cdf(x,n,p) for x in xs]\n    return hv.Curve((xs, ys))\n\nhmap1 = hv.DynamicMap(pmf, kdims=['n','p'])\nhmap1.redim.range(n = (5, 15), p = (0.1,1)).opts(width = 700, height = 600 , show_grid=True).relabel('Negative Binomial CDF')","10a4ef16":"label = \"Planck Discrete Exponential Distribution (lam = 0.6)\"\nlam = 0.6\n\nmeasured = stats.planck.rvs(lam, size=1000)\nhist = np.histogram(measured,density=True, bins=40)\n\nx = np.linspace(0, 10, 1000)\npmf = stats.planck.pmf(x,lam)\ncdf = stats.planck.cdf(x,lam)\nplanck = histogram(hist, x, pmf, cdf, label)\n\nplanck.opts(width = 800, height = 700 , show_grid=True)","984f9067":"def hist(lam):\n    data = stats.planck.rvs(lam, size=1000)\n    frequencies, edges = np.histogram(data, 40)\n    return hv.Histogram((edges, frequencies))\n\nhmap = hv.DynamicMap(hist, kdims=['lam'])\nhmap.redim.range(lam = (0.1, 1)).opts(width = 700, height = 600 , show_grid=True).relabel('Planck Discrete Exponential Histogram')","c0db9c62":"def pmf(lam):\n    xs = np.arange(0,100)\n    ys = [stats.planck.pmf(x,lam) for x in xs]\n    return hv.Curve((xs, ys))\n\nhmap1 = hv.DynamicMap(pmf, kdims=['lam'])\nhmap1.redim.range(lam = (0.1, 1)).opts(width = 700, height = 600 , show_grid=True).relabel('Planck Discrete Exponential PMF')","652dd154":"def cdf(lam):\n    xs = np.arange(0,100)\n    ys = [stats.planck.cdf(x,lam) for x in xs]\n    return hv.Curve((xs, ys))\n\nhmap1 = hv.DynamicMap(cdf, kdims=['lam'])\nhmap1.redim.range(lam = (0.1, 1)).opts(width = 700, height = 600 , show_grid=True).relabel('Planck Discrete Exponential CDF')","69b7cc3e":"label = \"Poisson Distribution (mu = 0.6)\"\nmu = 0.6\n\nmeasured = stats.poisson.rvs(mu, size=1000)\nhist = np.histogram(measured,density=True, bins=40)\n\nx = np.linspace(0, 5, 1000)\npmf = stats.poisson.pmf(x,mu)\ncdf = stats.poisson.cdf(x, mu)\npois = histogram(hist, x, pmf, cdf, label)\n\npois.opts(width = 800, height = 700 , show_grid=True)","823eb0b1":"def hist(mu):\n    data = stats.poisson.rvs(mu, size=1000)\n    frequencies, edges = np.histogram(data, 40)\n    return hv.Histogram((edges, frequencies))\n\nhmap = hv.DynamicMap(hist, kdims=['mu'])\nhmap.redim.range(mu = (1, 10)).opts(width = 700, height = 600 , show_grid=True).relabel('Poisson Histogram')","7e019cd2":"def pmf(mu):\n    xs = np.arange(0,7)\n    ys = [stats.poisson.pmf(x,mu) for x in xs]\n    return hv.Curve((xs, ys))\n\nhmap1 = hv.DynamicMap(pmf, kdims=['mu'])\nhmap1.redim.range(mu = (0.1, 10)).opts(width = 700, height = 600 , show_grid=True).relabel('Poisson PMF')","9f8838bb":"def cdf(mu):\n    xs = np.arange(0,7)\n    ys = [stats.poisson.cdf(x,mu) for x in xs]\n    return hv.Curve((xs, ys))\n\nhmap1 = hv.DynamicMap(cdf, kdims=['mu'])\nhmap1.redim.range(mu = (0.1, 10)).opts(width = 700, height = 600 , show_grid=True).relabel('Poisson CDF')","b6a9970a":"label = \"Uniform Distribution (low, high = 5, 10)\"\nlow, high = 5, 10\n\nmeasured = stats.randint.rvs(low, high, size=1000)\nhist = np.histogram(measured,density=True, bins=40)\n\nx = np.linspace(0, 10, 1000)\npmf = stats.randint.pmf(x, low, high)\ncdf = stats.randint.cdf(x, low, high)\nuni = histogram(hist, x, pmf, cdf, label)\n\nuni.opts(width = 800, height = 700 , show_grid=True)","493e3261":"def hist(low, high):\n    data = stats.randint.rvs(low, high, size=1000)\n    frequencies, edges = np.histogram(data, 40)\n    return hv.Histogram((edges, frequencies))\n\nhmap = hv.DynamicMap(hist, kdims=['low', 'high'])\nhmap.redim.range(low = (1, 10),high = (11, 20)).opts(width = 700, height = 600 , show_grid=True).relabel('Uniform Histogram')","65ecb9f8":"def pmf(low, high):\n    xs = np.arange(0,100)\n    ys = [stats.randint.pmf(x, low, high) for x in xs]\n    return hv.Curve((xs, ys))\n\nhmap1 = hv.DynamicMap(pmf, kdims=['low', 'high'])\nhmap1.redim.range(low = (1, 49),high = (50, 100)).opts(width = 700, height = 600 , show_grid=True).relabel('Uniform PMF')","d5a8f7b3":"def cdf(low, high):\n    xs = np.arange(0,100)\n    ys = [stats.randint.cdf(x, low, high) for x in xs]\n    return hv.Curve((xs, ys))\n\nhmap1 = hv.DynamicMap(cdf, kdims=['low', 'high'])\nhmap1.redim.range(low = (1, 49),high = (50, 100)).opts(width = 700, height = 600 , show_grid=True).relabel('Uniform CDF')","6b4c5afc":"label = \"Skellam Distribution (mu1, mu2 = 15, 8)\"\nmu1, mu2 = 15, 8\n\nmeasured = stats.skellam.rvs(mu1, mu2, size=1000)\nhist = np.histogram(measured,density=True, bins=40)\n\nx = np.linspace(0, 10, 1000)\npmf = stats.skellam.pmf(x, mu1, mu2)\ncdf = stats.skellam.cdf(x, mu1, mu2)\nuni = histogram(hist, x, pmf, cdf, label)\n\nuni.opts(width = 800, height = 700 , show_grid=True)","d0031ef3":"def hist(mu1, mu2):\n    data = stats.randint.rvs(low, high, size=1000)\n    frequencies, edges = np.histogram(data, 40)\n    return hv.Histogram((edges, frequencies))\n\nhmap = hv.DynamicMap(hist, kdims=['mu1', 'mu2'])\nhmap.redim.range(mu1 = (1, 20),mu2 = (1, 20)).opts(width = 700, height = 600 , show_grid=True).relabel('Skellam Histogram')","9b3f9bb4":"def pmf(mu1, mu2):\n    xs = np.arange(0,10)\n    ys = [stats.skellam.pmf(x, mu1, mu2) for x in xs]\n    return hv.Curve((xs, ys))\n\nhmap1 = hv.DynamicMap(pmf, kdims=['mu1', 'mu2'])\nhmap1.redim.range(mu1 = (1, 20),mu2 = (1, 20)).opts(width = 700, height = 600 , show_grid=True).relabel('Skellam PMF')","606c9b38":"def cdf(mu1, mu2):\n    xs = np.arange(0,10)\n    ys = [stats.skellam.cdf(x, mu1, mu2) for x in xs]\n    return hv.Curve((xs, ys))\n\nhmap1 = hv.DynamicMap(cdf, kdims=['mu1', 'mu2'])\nhmap1.redim.range(mu1 = (1, 20),mu2 = (1, 20)).opts(width = 700, height = 600 , show_grid=True).relabel('Skellam CDF')","ae203e5f":"label = \"Zipf Distribution (a = 4)\"\na = 4\n\nmeasured = stats.zipf.rvs(a, size=1000)\nhist = np.histogram(measured,density=True, bins=40)\n\nx = np.linspace(0, 4, 1000)\npmf = stats.zipf.pmf(x,a)\ncdf = stats.zipf.cdf(x,a)\nzipf = histogram(hist, x, pmf, cdf, label)\n\nzipf.opts(width = 800, height = 700 , show_grid=True)","d76e06e4":"def hist(a):\n    data = stats.zipf.rvs(a, size=1000)\n    frequencies, edges = np.histogram(data, 40)\n    return hv.Histogram((edges, frequencies))\n\nhmap = hv.DynamicMap(hist, kdims=['a'])\nhmap.redim.range(a = (2, 20)).opts(width = 700, height = 600 , show_grid=True).relabel('Zipf Histogram')","e735148f":"def pmf(a):\n    xs = np.arange(0,10)\n    ys = [stats.zipf.pmf(x, a) for x in xs]\n    return hv.Curve((xs, ys))\n\nhmap1 = hv.DynamicMap(pmf, kdims=['a'])\nhmap1.redim.range(a = (2, 20)).opts(width = 700, height = 600 , show_grid=True).relabel('Zipf PMF')","45dd1e5c":"def cdf(a):\n    xs = np.arange(0,100)\n    ys = [stats.zipf.cdf(x, a) for x in xs]\n    return hv.Curve((xs, ys))\n\nhmap1 = hv.DynamicMap(cdf, kdims=['a'])\nhmap1.redim.range(a = (2, 20)).opts(width = 700, height = 600 , show_grid=True).relabel('Zipf CDF')","00790091":"## Log-Series Distribution\nFor more information: http:\/\/mathworld.wolfram.com\/Log-SeriesDistribution.html","8aca77be":"## Zipf Distribution\nZipf's law  is an empirical law formulated using mathematical statistics that refers to the fact that many types of data studied in the physical and social sciences can be approximated with a Zipfian distribution, one of a family of related discrete power law probability distributions. Zipf distribution is related to the zeta distribution, but is not identical. \nFor example, Zipf's law states that given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table. Thus the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word, etc.: the rank-frequency distribution is an inverse relation. For example, in the Brown Corpus of American English text, the word the is the most frequently occurring word, and by itself accounts for nearly 7% of all word occurrences (69,971 out of slightly over 1 million). True to Zipf's Law, the second-place word of accounts for slightly over 3.5% of words (36,411 occurrences), followed by and (28,852). Only 135 vocabulary items are needed to account for half the Brown Corpus. \n<br>\nThe law is named after the American linguist George Kingsley Zipf (1902\u20131950), who popularized it and sought to explain it (Zipf 1935, 1949), though he did not claim to have originated it. The French stenographer Jean-Baptiste Estoup (1868\u20131950) appears to have noticed the regularity before Zipf. It was also noted in 1913 by German physicist Felix Auerbach[4] (1856\u20131933). \n<br>\nFor more information check: http:\/\/mathworld.wolfram.com\/ZipfDistribution.html","4ce2befa":"## Bernoulli Distribution \nIn probability theory and statistics, the Bernoulli distribution, named after Swiss mathematician Jacob Bernoulli, is the discrete probability distribution of a random variable which takes the value 1 with probability p and the value 0 with probability  q=1-p, q=1-p that is, the probability distribution of any single experiment that asks a yes\u2013no question; the question results in a boolean-valued outcome, a single bit of information whose value is success\/yes\/true\/one with probability p and failure\/no\/false\/zero with probability q. It can be used to represent a (possibly biased) coin toss where 1 and 0 would represent \"heads\" and \"tails\" (or vice versa), respectively, and p would be the probability of the coin landing on heads or tails, respectively. In particular, unfair coins would have p different then 1\/2.\n\nThe Bernoulli distribution is a special case of the binomial distribution where a single trial is conducted (so n would be 1 for such a binomial distribution). It is also a special case of the two-point distribution, for which the possible outcomes need not be 0 and 1.\n<br>\nFor more information on the topic check http:\/\/mathworld.wolfram.com\/BernoulliDistribution.html","78d31ee2":"## Poisson Distribution\nn probability theory and statistics, the Poisson distribution, named after French mathematician Sim\u00e9on Denis Poisson, is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant rate and independently of the time since the last event. The Poisson distribution can also be used for the number of events in other specified intervals such as distance, area or volume.\n\nFor instance, an individual keeping track of the amount of mail they receive each day may notice that they receive an average number of 4 letters per day. If receiving any particular piece of mail does not affect the arrival times of future pieces of mail, i.e., if pieces of mail from a wide range of sources arrive independently of one another, then a reasonable assumption is that the number of pieces of mail received in a day obeys a Poisson distribution. Other examples that may follow a Poisson distribution include the number of phone calls received by a call center per hour and the number of decay events per second from a radioactive source.\n<br>\nApplications of the Poisson distribution can be found in many fields related to counting:\n<br>\nTelecommunication example: telephone calls arriving in a system.\n<br>\nAstronomy example: photons arriving at a telescope.\n<br>\nChemistry example: the molar mass distribution of a living polymerization\n<br>\nBiology example: the number of mutations on a strand of DNA per unit length.\n<br>\nManagement example: customers arriving at a counter or call centre.\n<br>\nFinance and insurance example: number of losses or claims occurring in a given period of time.\n<br>\nEarthquake seismology example: an asymptotic Poisson model of seismic risk for large earthquakes.\n<br>\nRadioactivity example: number of decays in a given time interval in a radioactive sample.\n<br>\nThe Poisson distribution arises in connection with Poisson processes. It applies to various phenomena of discrete properties (that is, those that may happen 0, 1, 2, 3, ... times during a given period of time or in a given area) whenever the probability of the phenomenon happening is constant in time or space. Examples of events that may be modelled as a Poisson distribution include:\n<br>\n\nThe number of soldiers killed by horse-kicks each year in each corps in the Prussian cavalry. This example was used in a book by Ladislaus Bortkiewicz (1868\u20131931).\n<br>\nThe number of yeast cells used when brewing Guinness beer. This example was used by William Sealy Gosset (1876\u20131937).\n<br>\nThe number of phone calls arriving at a call centre within a minute. This example was described by A.K. Erlang (1878 \u2013 1929).\n<br>\nInternet traffic.\n<br>\nThe number of goals in sports involving two competing teams.\n<br>\nThe number of deaths per year in a given age group.\n<br>\nThe number of jumps in a stock price in a given time interval.\n<br>\nUnder an assumption of homogeneity, the number of times a web server is accessed per minute.\n<br>\nThe number of mutations in a given stretch of DNA after a certain amount of radiation.\n<br>\nThe proportion of cells that will be infected at a given multiplicity of infection.\n<br>\nThe number of bacteria in a certain amount of liquid.\n<br>\nThe arrival of photons on a pixel circuit at a given illumination and over a given time period.\n<br>\nThe targeting of V-1 flying bombs on London during World War II investigated by R. D. Clarke in 1946.\nGallagher in 1976 showed that the counts of prime numbers in short intervals obey a Poisson distribution provided a certain version of an unproved conjecture of Hardy and Littlewood is true.\n<br>\nFor more information check: http:\/\/mathworld.wolfram.com\/PoissonDistribution.html","53db5507":"## Discrete Uniform Distribtion\nIn probability theory and statistics, the discrete uniform distribution is a symmetric probability distribution whereby a finite number of values are equally likely to be observed; every one of n values has equal probability 1\/n. Another way of saying \"discrete uniform distribution\" would be \"a known, finite number of outcomes equally likely to happen\". \nA simple example of the discrete uniform distribution is throwing a fair die. The possible values are 1, 2, 3, 4, 5, 6, and each time the dice is thrown the probability of a given score is 1\/6. If two dice are thrown and their values added, the resulting distribution is no longer uniform since not all sums have equal probability.\n<br>\nThe family of uniform distributions (with one or both bounds unknown) has a finite-dimensional sufficient statistic, namely the triple of the sample maximum, sample minimum, and sample size, but is not an exponential family of distributions, since the support varies with the parameters. For families whose support does not depend on the parameters, the Pitman\u2013Koopman\u2013Darmois theorem states that only exponential families have a sufficient statistic whose dimension is bounded as sample size increases. The uniform distribution is thus a simple example showing the limit of this theorem. \n<br>\nFor more information check: http:\/\/mathworld.wolfram.com\/DiscreteUniformDistribution.html","d553561c":"## Boltzmann (Truncated Discrete Exponential)\nIn statistical mechanics and mathematics, a Boltzmann distribution (also called Gibbs distribution[1]) is a probability distribution or probability measure that gives the probability that a system will be in a certain state as a function of that state's energy and the temperature of the system. It is also a frequency distribution of particles in a system.\n<br>\nThe Boltzmann distribution appears in statistical mechanics when considering isolated (or nearly-isolated) systems of fixed composition that are in thermal equilibrium (equilibrium with respect to energy exchange). The most general case is the probability distribution for the canonical ensemble, but also some special cases (derivable from the canonical ensemble) also show the Boltzmann distribution in different aspects:\n\nCanonical ensemble (general case)\nThe canonical ensemble gives the probabilities of the various possible states of a closed system of fixed volume, in thermal equilibrium with a heat bath. The canonical ensemble is a probability distribution with the Boltzmann form.\nStatistical frequencies of subsystems' states (in a non-interacting collection)\nWhen the system of interest is a collection of many non-interacting copies of a smaller subsystem, it is sometimes useful to find the statistical frequency of a given subsystem state, among the collection. The canonical ensemble has the property of separability when applied to such a collection: as long as the non-interacting subsystems have fixed composition, then each subsystem's state is independent of the others and is also characterized by a canonical ensemble. As a result, the expected statistical frequency distribution of subsystem states has the Boltzmann form.\nMaxwell\u2013Boltzmann statistics of classical gases (systems of non-interacting particles)\nIn particle systems, many particles share the same space and regularly change places with each other; the single-particle state space they occupy is a shared space. Maxwell\u2013Boltzmann statistics give the expected number of particles found in a given single-particle state, in a classical gas of non-interacting particles at equilibrium. This expected number distribution has the Boltzmann form.\nAlthough these cases have strong similarities, it is helpful to distinguish them as they generalize in different ways when the crucial assumptions are changed:\n\nWhen a system is in thermodynamic equilibrium with respect to both energy exchange and particle exchange, the requirement of fixed composition is relaxed and a grand canonical ensemble is obtained rather than canonical ensemble. On the other hand, if both composition and energy are fixed, then a microcanonical ensemble applies instead.\nIf the subsystems within a collection do interact with each other, then the expected frequencies of subsystem states no longer follow a Boltzmann distribution, and even may not have an analytical solution.[9] The canonical ensemble can however still be applied to the collective states of the entire system considered as a whole, provided the entire system is isolated and in thermal equilibrium.\nWith quantum gases of non-interacting particles in equilibrium, the number of particles found in a given single-particle state does not follow Maxwell\u2013Boltzmann statistics, and there is no simple closed form expression for quantum gases in the canonical ensemble. In the grand canonical ensemble the state-filling statistics of quantum gases are described by Fermi\u2013Dirac statistics or Bose\u2013Einstein statistics, depending on whether the particles are fermions or bosons respectively.\n<br>\n\nIn more general mathematical settings, the Boltzmann distribution is also known as the Gibbs measure. In statistics and machine learning it is called a log-linear model. In deep learning, the Boltzmann distribution is used in the sampling distribution of stochastic neural networks such as the Boltzmann machine and Restricted Boltzmann machine.\n<br>\nThe Boltzmann distribution can be introduced to allocate permits in emissions trading.The new allocation method using the Boltzmann distribution can describe the most probable, natural, and unbiased distribution of emissions permits among multiple countries. Simple and versatile, this new method holds potential for many economic and environmental applications.\n\nThe Boltzmann distribution has the same form as the multinomial logit model. As a discrete choice model, this is very well known in economics since Daniel McFadden made the connection to random utility maximization.\n\n<br>\nFor more information check: https:\/\/en.wikipedia.org\/wiki\/Boltzmann_distribution#cite_note-landau-1","712f9b6a":"## Discrete Laplace Distribution\nFor more information check: https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.dlaplace.html#scipy.stats.dlaplace","2858520c":"## Skellam distribution\nThe Skellam distribution is the discrete probability distribution of the difference \nN1 \u2212 N2  of two statistically independent random variables N1 and N2, each Poisson-distributed with respective expected values \u03bc1 and \u03bc2. It is useful in describing the statistics of the difference of two images with simple photon noise, as well as describing the point spread distribution in sports where all scored points are equal, such as baseball, hockey and soccer. \nThe distribution is also applicable to a special case of the difference of dependent Poisson random variables, but just the obvious case where the two variables have a common additive random contribution which is cancelled by the differencing.\n<br>\nFor more information check: https:\/\/en.wikipedia.org\/wiki\/Skellam_distribution","f7fb9f67":"## Negative Binomial Distribution\nIn probability theory and statistics, the negative binomial distribution is a discrete probability distribution of the number of successes in a sequence of independent and identically distributed Bernoulli trials before a specified (non-random) number of failures (denoted r) occurs. For example, if we define a 1 as failure, all non-1s as successes, and we throw a die repeatedly until 1 appears the third time (r = three failures), then the probability distribution of the number of non-1s that appeared will be a negative binomial distribution.\n\nThe Pascal distribution (after Blaise Pascal) and Polya distribution (for George P\u00f3lya) are special cases of the negative binomial distribution. A convention among engineers, climatologists, and others is to use \"negative binomial\" or \"Pascal\" for the case of an integer-valued stopping-time parameter r, and use \"Polya\" for the real-valued case.\n\nFor occurrences of \"contagious\" discrete events, like tornado outbreaks, the Polya distributions can be used to give more accurate models than the Poisson distribution by allowing the mean and variance to be different, unlike the Poisson. \"Contagious\" events have positively correlated occurrences causing a larger variance than if the occurrences were independent, due to a positive covariance term.\n\nFor more information: http:\/\/mathworld.wolfram.com\/NegativeBinomialDistribution.html","a0055a7e":"## Planck Discrete Exponential Distribution\nFor more information check : https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.planck.html#scipy.stats.planck","2f9d7f72":"## Geometric Distribution\nIn probability theory and statistics, the geometric distribution is either of two discrete probability distributions:\n\nThe probability distribution of the number X of Bernoulli trials needed to get one success, supported on the set { 1, 2, 3, ... }\nThe probability distribution of the number Y = X \u2212 1 of failures before the first success, supported on the set { 0, 1, 2, 3, ... }\nWhich of these one calls \"the\" geometric distribution is a matter of convention and convenience.\n\nThese two different geometric distributions should not be confused with each other. Often, the name shifted geometric distribution is adopted for the former one (distribution of the number X); however, to avoid ambiguity, it is considered wise to indicate which is intended, by mentioning the support explicitly.\n\nThe geometric distribution gives the probability that the first occurrence of success requires k independent trials, each with success probability p.\n<br>\n\nFor more information check:\nhttp:\/\/mathworld.wolfram.com\/GeometricDistribution.html","5af57ccf":"## Table of contents\n1. Bernoulli Distribution\n2. Binomial Distribution\n3. Boltzmann (Truncated Discrete Exponential)\n4. Discrete Laplace Distribution\n5. Geometric Distribution\n6. Hypergeometric Distribution\n7. Log-Series Distribution\n6. Negative Binomial Distribution\n7. Planck Discrete Exponential Distribution\n8. Poisson Distribution\n9. Discrete Uniform Distribtion\n10. Skellam distribution\n11. Zipf Distribution","1768741c":"## Binomial Distribution\nIn probability theory and statistics, the binomial distribution with parameters n and p is the discrete probability distribution of the number of successes in a sequence of n independent experiments, each asking a yes\u2013no question, and each with its own boolean-valued outcome: success\/yes\/true\/one (with probability p) or failure\/no\/false\/zero (with probability q = 1 \u2212 p). A single success\/failure experiment is also called a Bernoulli trial or Bernoulli experiment and a sequence of outcomes is called a Bernoulli process; for a single trial, i.e., n = 1, the binomial distribution is a Bernoulli distribution. The binomial distribution is the basis for the popular binomial test of statistical significance.\n\nThe binomial distribution is frequently used to model the number of successes in a sample of size n drawn with replacement from a population of size N. If the sampling is carried out without replacement, the draws are not independent and so the resulting distribution is a hypergeometric distribution, not a binomial one. However, for N much larger than n, the binomial distribution remains a good approximation, and is widely used.\n<br>\nFor more information on the topic check http:\/\/mathworld.wolfram.com\/BinomialDistribution.html","cbf99d17":"## Hypergeometric Distribution\nIn probability theory and statistics, the hypergeometric distribution is a discrete probability distribution that describes the probability of k successes (random draws for which the object drawn has a specified feature) in n draws, without replacement, from a finite population of size  N that contains exactly K objects with that feature, wherein each draw is either a success or a failure. In contrast, the binomial distribution describes the probability of k successes in n draws with replacement.\n\nIn statistics, the hypergeometric test uses the hypergeometric distribution to calculate the statistical significance of having drawn a specific k successes (out of n total draws) from the aforementioned population. The test is often used to identify which sub-populations are over- or under-represented in a sample. This test has a wide range of applications. For example, a marketing group could use the test to understand their customer base by testing a set of known customers for over-representation of various demographic subgroups (e.g., women, people under 30).\n<br>\nThe classical application of the hypergeometric distribution is sampling without replacement. Think of an urn with two types of marbles, red ones and green ones. Define drawing a green marble as a success and drawing a red marble as a failure (analogous to the binomial distribution). If the variable N describes the number of all marbles in the urn (see contingency table below) and K describes the number of green marbles, then N \u2212 K corresponds to the number of red marbles. In this example, X is the random variable whose outcome is k, the number of green marbles actually drawn in the experiment.\n<br>\nElection audits typically test a sample of machine-counted precincts to see if recounts by hand or machine match the original counts. Mismatches result in either a report or a larger recount. The sampling rates are usually defined by law, not statistical design, so for a legally defined sample size n, what is the probability of missing a problem which is present in K precincts, such as a hack or bug? This is the probability that k = 0. Bugs are often obscure, and a hacker can minimize detection by affecting only a few precincts, which will still affect close elections, so a plausible scenario is for K to be on the order of 5% of N. Audits typically cover 1% to 10% of precincts (often 3%), so they have a high chance of missing a problem.\n<br>\nIn hold'em poker players make the best hand they can combining the two cards in their hand with the 5 cards (community cards) eventually turned up on the table. The deck has 52 and there are 13 of each suit. For this example assume a player has 2 clubs in the hand and there are 3 cards showing on the table, 2 of which are also clubs. The player would like to know the probability of one of the next 2 cards to be shown being a club to complete the flush.\n(Note that the probability calculated in this example assumes no information is known about the cards in the other players' hands; however, experienced poker players may consider how the other players place their bets (check, call, raise, or fold) in considering the probability for each scenario. Strictly speaking, the approach to calculating success probabilities outlined here is accurate in a scenario where there is just one player at the table; in a multiplayer game this probability might be adjusted somewhat based on the betting play of the opponents.)\n\nThere are 4 clubs showing so there are 9 still unseen. There are 5 cards showing (2 in the hand and 3 on the table) so there are 52-5=47 still unseen.\n\nThe probability that one of the next two cards turned is a club can be calculated using hypergeometric with  k=1,n=2,K=9 and N=47. (about 31.6%)\n\nThe probability that both of the next two cards turned are clubs can be calculated using hypergeometric with  k=2,n=2,K=9 and N=47. (about 3.3%)\n\nThe probability that neither of the next two cards turned are clubs can be calculated using hypergeometric with  k=0,n=2,K=9 and  N=47. (about 65.0%)\n<br>\nFor more information check: http:\/\/mathworld.wolfram.com\/HypergeometricDistribution.html"}}