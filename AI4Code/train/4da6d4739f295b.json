{"cell_type":{"b5001a4f":"code","3444e9eb":"code","09e2a344":"code","e4eebd9e":"code","15d66e0e":"code","9dbd8da3":"code","70856ddd":"code","c00673dc":"code","5eb24021":"code","ee98f815":"code","db110cb4":"code","9ee3ab68":"code","96327637":"code","87646a13":"code","b80997a6":"code","66345d17":"code","4e5dc678":"code","43c191dd":"code","f28a0ad1":"code","d9a9818d":"code","5e446298":"code","b4f3c6f7":"code","4960ed10":"code","97da0e92":"code","467826c2":"code","38fbf43c":"code","db625f69":"code","d09f9ac4":"code","9d7b20c4":"code","dc2da30b":"code","e8c53002":"code","5a9a2e3c":"code","8071ddac":"code","838f68b5":"code","44003407":"code","564d8ea8":"code","1be78e30":"code","52c10847":"code","313b40c2":"code","c68e4be8":"code","8a52fbfa":"markdown","9adfd88b":"markdown","cc243103":"markdown","aa46c274":"markdown","dbed388b":"markdown","a7924bd0":"markdown","4414f5a1":"markdown","17ae9241":"markdown","c4bcf534":"markdown","816feb44":"markdown","bbb5be37":"markdown","cd15b016":"markdown","1630d9e1":"markdown","b0de7476":"markdown","56cded0a":"markdown","f32379dd":"markdown","03f7bd2f":"markdown","a2042fe1":"markdown","a7c3e9f7":"markdown"},"source":{"b5001a4f":"%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\nimport pandas as pd","3444e9eb":"np.random.seed(23)\ndef data_gen(n_samples):\n    x=np.sort(np.random.uniform(0,1,n_samples))\n    N=np.random.normal(0,1,n_samples)\n    y=np.sin(2*np.pi*x)+0.1*N\n    data=pd.DataFrame()\n    data['X']=x\n    data['Y']=y\n    return data\ndata= data_gen(n_samples=20)","09e2a344":"def train_test_split(data):\n    train_data=data.sample(frac=0.5,random_state=23)\n    test_data=data.drop(train_data.index).sample(frac=1.0)\n    x_train=np.array(train_data['X']).reshape(-1,1)\n    y_train=np.array(train_data['Y']).reshape(-1,1)\n    x_test=np.array(test_data['X']).reshape(-1,1)\n    y_test=np.array(test_data['Y']).reshape(-1,1)\n    return x_train,y_train,x_test,y_test\nx_train,y_train,x_test,y_test=train_test_split(data)\n#print(x_train)","e4eebd9e":"# Degree 0 means y=b i.e there is only bias \nw=0\n\ndef forward_0(x):\n    return w\n\ndef loss(y,y_pred):\n    loss=np.sqrt(np.mean((y_pred-y)**2))\n    return loss\n                 \ndef gradient_0(x, y,y_pred):  \n    m = x.shape[0]\n    return (2\/m)*np.sum((y_pred - y)) # gradient of loss w.r.t  bias\n\n# Training loop\nlosses0=[]\nfor epoch in range(100):\n    losses=[]\n    y_pred=forward_0(x_train)\n    grad = gradient_0(x_train, y_train,y_pred)\n    w= w -0.01 * grad\n    y_pred=forward_0(x_train)\n    l = loss(y_train,y_pred)\n    losses.append(l)\n    losses0.append(l)\n    if(epoch%10==0 or epoch==99):\n        print(\"progress:\", epoch, \"w=\", w, \"loss=\", np.mean(losses))","15d66e0e":"#Predictions\ny_pred=forward_0(data['X'].array.reshape(-1,1))","9dbd8da3":"fig = plt.figure(figsize=(8,6))\nplt.plot(data['X'], data['Y'], 'yo-')\nplt.axhline(y_pred, color='r')\nplt.legend([\"Data\", \"Degree=0\"])\nplt.xlabel('X - Input')\nplt.ylabel('y - target \/ true')\nplt.title('Polynomial Regression')\nplt.show()","70856ddd":"weights_dict={}\nwl=[w]\nwl = [\"%.4f\" % member for member in wl]\nweights_dict['Degree:0']=wl\nprint(weights_dict)","c00673dc":"w=0\nb=0\n\ndef forward_1(x):\n    return np.dot(x,w)+b\n\ndef loss(y,y_pred):\n    loss=np.sqrt(np.mean((y_pred-y)**2))\n    return loss\n                 \ndef gradient_1(x,y,y_pred):  \n    m = x.shape[0]\n    # gradient w.r.t weights\n    dw= (2\/m)*np.dot(x.T,(y_pred - y)) \n    #gradient w.r.t bias\n    db= (2\/m)*np.sum((y_pred - y))\n    return dw,db\n\n# Training loop\nlosses1=[]\nfor epoch in range(100):\n    losses=[]\n    y_pred=forward_1(x_train)\n    dw,db = gradient_1(x_train, y_train,y_pred)\n    w= w -0.01 * dw\n    b=b-0.01*db\n    y_pred=forward_1(x_train)\n    l = loss(y_train,y_pred)\n    losses.append(l)\n    losses1.append(l)\n    if(epoch%10==0):\n        print(\"progress:\", epoch, \"loss=\", np.mean(losses))","5eb24021":"y_pred=forward_1(data['X'].array.reshape(-1,1))","ee98f815":"fig = plt.figure(figsize=(8,6))\nplt.plot(data['X'], data['Y'], 'yo-')\nplt.plot(data['X'],y_pred, 'ro-')\nplt.legend([\"Data\", \"Degree=1\"])\nplt.xlabel('X - Input')\nplt.ylabel('y - target \/ true')\nplt.title('Polynomial Regression')\nplt.show()","db110cb4":"#Function to add bias to our weights list and formatting it\ndef make_weights_list(w,b):\n    W=[]\n    w=w.tolist()\n    for i in w:\n        W.append(i.pop())\n    W.insert(0,b) # the first entry is the bias\n    W=[\"%.4f\" % x for x in W]\n    return W\n","9ee3ab68":"W=make_weights_list(w,b)\nweights_dict['Degree:1']=W\n#print(weights_dict)","96327637":"def transform(x, degree):\n    t = x.copy()\n    \n    # Appending columns of higher degrees to X.\n    for i in range(2,degree+1):\n        x= np.append(x, t**i,axis=1)\n            \n    return x","87646a13":"def forward_n(x,w,b):\n    return np.dot(x,w)+b\n\ndef loss(y,y_pred):\n    loss=np.sqrt(np.mean((y_pred-y)**2))\n    return loss\n                 \ndef gradient_n(x, y,y_pred):  \n    m = x.shape[0]\n    dw= (2\/m)*np.dot(x.T,(y_pred - y)) \n    db= (2\/m)*np.sum((y_pred - y))\n    return dw,db","b80997a6":"def train(x,y,epochs,lr,degree,x_test,y_test,display=True):\n    x_new=transform(x,degree)\n    n=x_new.shape[1]\n    weights=np.zeros((n,1))\n    bias=0\n\n\n    # Training loop\n    loss_history=[]\n    x_test_new=transform(x_test,degree)\n    test_loss=[]\n    for epoch in range(epochs):\n        losses=[]\n        y_pred=forward_n(x_new,weights,bias)\n        dw,db = gradient_n(x_new, y,y_pred)\n        weights= weights -lr * dw\n        bias=bias-lr*db\n        y_pred=forward_n(x_new,weights,bias)\n        l = loss(y,y_pred)\n        losses.append(l)\n        loss_history.append(l)\n        #calculate and store our test loss\n        y_pred2=forward_n(x_test_new,weights,bias)\n        l2=loss(y_test,y_pred2)\n        test_loss.append(l2)\n        if((epoch%10==0 or epoch==epochs-1) and display==True):\n            print(\"progress:\", epoch, \"loss=\", np.mean(losses))\n    return weights,bias,loss_history,test_loss","66345d17":"def predict(x,w,b,degree):\n    x=transform(x,degree)\n    return np.dot(x,w)+b","4e5dc678":"def plot_fit_data(X,Y,x,pred,degree,size=(8,6)):\n    fig = plt.figure(figsize=size)\n    plt.plot(X, Y, 'yo-')\n    plt.plot(x,pred, 'ro-')\n    plt.legend([\"Data\", \"Degree of fit={}\".format(degree)])\n    plt.xlabel('X')\n    plt.ylabel('y')\n    plt.title('Polynomial Regression')\n    plt.show()","43c191dd":"#function that print train and test loss vs epochs\ndef plot_losses(train_loss,test_loss):\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n    axes[0].plot(train_loss,'-x')\n    axes[0].set_xlabel('epoch')\n    axes[0].set_ylabel('Train loss')\n    axes[0].set_title(' Train Loss vs. No. of epochs')\n\n    axes[1].plot(test_loss,'-x')\n    axes[1].set_xlabel('epoch')\n    axes[1].set_ylabel('Test loss')\n    axes[1].set_title('Test losss vs. No. of epochs')\n    plt.show()","f28a0ad1":"degree=3\nweight_3,bias_3,train_losses_3,test_losses_3=train(x_train,y_train,200,0.01,degree,x_test,y_test)","d9a9818d":"preds= predict(data['X'].array.reshape(-1,1),weight_3,bias_3,degree)\n\nplot_fit_data(data['X'],data['Y'],data['X'],preds,degree)","5e446298":"plot_losses(train_losses_3,test_losses_3)","b4f3c6f7":"W=make_weights_list(weight_3,bias_3)\nweights_dict['Degree:3']=W","4960ed10":"degree=5\nweight_5,bias_5,train_losses_5,test_losses_5=train(x_train,y_train,200,0.01,degree,x_test,y_test)","97da0e92":"preds= predict(data['X'].array.reshape(-1,1),weight_5,bias_5,degree)\n\nplot_fit_data(data['X'],data['Y'],data['X'],preds,degree)","467826c2":"plot_losses(train_losses_5,test_losses_5)","38fbf43c":"W=make_weights_list(weight_5,bias_5)\nweights_dict['Degree:5']=W","db625f69":"degree=9\nweight_9,bias_9,train_losses_9,test_losses_9=train(x_train,y_train,200,0.01,degree,x_test,y_test)","d09f9ac4":"preds= predict(data['X'].array.reshape(-1,1),weight_9,bias_9,degree)\n\nplot_fit_data(data['X'],data['Y'],data['X'],preds,degree)","9d7b20c4":"plot_losses(train_losses_9,test_losses_9)","dc2da30b":"W=make_weights_list(weight_9,bias_9)\nweights_dict['Degree:9']=W\n","e8c53002":"from tabulate import tabulate\nprint(tabulate(weights_dict,headers='keys',tablefmt='fancy_grid',showindex=True))","5a9a2e3c":"more_data=data_gen(n_samples=150)\nxtrain,ytrain,xtest,ytest=train_test_split(more_data)","8071ddac":"degree=9\nmweight_9,mbias_9,mtrain_losses_9,mtest_losses_9=train(xtrain,ytrain,500,0.01,degree,xtest,ytest,display=False)","838f68b5":"preds= predict(more_data['X'].array.reshape(-1,1),mweight_9,mbias_9,degree)\n\nplot_fit_data(more_data['X'],more_data['Y'],more_data['X'],preds,degree)","44003407":"plot_losses(mtrain_losses_9,mtest_losses_9)","564d8ea8":"def gradient_reg(x, y,y_pred,L,w):  \n    m = x.shape[0]\n    dw= (2\/m)*np.dot(x.T,(y_pred - y))+L*np.sum(abs(w)) \n    db= (2\/m)*np.sum((y_pred - y))\n    return dw,db\n\ndef new_train(x,y,epochs,lr,L1_lambda,degree,x_test,y_test,display=True):\n    x_new=transform(x,degree)\n    n=x_new.shape[1]\n    weights=np.zeros((n,1))\n    bias=0\n\n\n    # Training loop\n    loss_history=[]\n    x_test_new=transform(x_test,degree)\n    test_loss=[]\n    for epoch in range(epochs):\n        losses=[]\n        y_pred=forward_n(x_new,weights,bias)\n        dw,db = gradient_reg(x_new, y,y_pred,L1_lambda,w)\n        weights= weights -lr * dw\n        bias=bias-lr*db\n        y_pred=forward_n(x_new,weights,bias)\n        l = loss(y,y_pred)\n        losses.append(l)\n        loss_history.append(l)\n        y_pred2=forward_n(x_test_new,weights,bias)\n        l2=loss(y_test,y_pred2)\n        test_loss.append(l2)\n        if((epoch%10==0 or epoch==epochs-1) and display==True):\n            print(\"progress:\", epoch, \"loss=\", np.mean(losses))\n    return weights,bias,loss_history,test_loss","1be78e30":"rdata=data_gen(n_samples=500)\nxtrain,ytrain,xtest,ytest=train_test_split(rdata)","52c10847":"degree=75\nL=1\nreg_weight,rbias,rtrain_losses,rtest_losses=new_train(xtrain,ytrain,500,0.01,L,degree,xtest,ytest,display=False)\npreds= predict(rdata['X'].array.reshape(-1,1),reg_weight,rbias,degree)\n\nfig = plt.figure(figsize=(8,6))\nplt.plot(rdata['X'], rdata['Y'], 'yo-')\nplt.plot(rdata['X'],preds, 'ro-')\nplt.legend([\"Data\", \"Degree of fit={}\".format(degree)])\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title(\"Lambda={}\".format(L))\nplt.show()","313b40c2":"degree=75\nL=0.1\nreg_weight,rbias,rtrain_losses,rtest_losses=new_train(xtrain,ytrain,500,0.01,L,degree,xtest,ytest,display=False)\npreds= predict(rdata['X'].array.reshape(-1,1),reg_weight,rbias,degree)\n\nfig = plt.figure(figsize=(8,6))\nplt.plot(rdata['X'], rdata['Y'], 'yo-')\nplt.plot(rdata['X'],preds, 'ro-')\nplt.legend([\"Data\", \"Degree of fit={}\".format(degree)])\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title(\"Lambda={}\".format(L))\nplt.show()","c68e4be8":"degree=75\nL=0.0001\nreg_weight,rbias,rtrain_losses,rtest_losses=new_train(xtrain,ytrain,500,0.01,L,degree,xtest,ytest,display=False)\npreds= predict(rdata['X'].array.reshape(-1,1),reg_weight,rbias,degree)\n\nfig = plt.figure(figsize=(8,6))\nplt.plot(rdata['X'], rdata['Y'], 'yo-')\nplt.plot(rdata['X'],preds, 'ro-')\nplt.legend([\"Data\", \"Degree of fit={}\".format(degree)])\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title(\"Lambda={}\".format(L))\nplt.show()","8a52fbfa":"# **Degree 0**\n#### Before we start building our model let's first talk about the loss function ,here we will be using root mean square error as our loss function. It differs in MSE just that the root means the growth in error wont look very explosive.\n![image.png](attachment:7e3fa50b-0d10-4a1b-9be1-c3b3dfc5219c.png) [source](https:\/\/towardsdatascience.com\/gradient-descent-from-scratch-e8b75fa986cc)","9adfd88b":"# **Regularization**\n### There is also another way to adjust overfitting called regularization. Regularization basically means penalizing our model when it learns too specific patterns ,in the case of regression, we mean trying to stop weights from learning too many changes in the data i.e *smoothing* our function. One method is adding a term to our gradient when doing gradient descent called L1 regularization term, which means sum of absolute values of weights.\n![image.png](attachment:67392a7e-5bb2-4450-925b-f278282e6fd8.png) [source](https:\/\/androidkt.com\/how-to-add-l1-l2-regularization-in-pytorch-loss-function\/)\n#### Here Lambda is a *penalty* term just like our learning rate it controls the L1 norm. Let's implement this for our code by first writing a modified gradient and trianing function and then try different Lambda values.","cc243103":"#### In this post we will be trying to implement polynomial regression using gradient descent from scratch and also look at overfitting and underfitting issues. Let's first generate some data. We shall start with some small dataset.\n# **Data generation**","aa46c274":"#### Now that we are done with writing the training and helper functions, lets move on to training","dbed388b":"# **Add more samples**\n#### Let's increase the dataset size i.e data samples and check if the model overfits or not.","a7924bd0":"## Degree 9","4414f5a1":"#### Here we see that though training loss is decreasing the test loss starts increasing after a point , this means that our model with degree 9 is overfitting to the data.Let's save the weights and display them before moving on.","17ae9241":"# **References**\n#### Completely coded everything from scratch by myself,to revise concepts used this blog [post](https:\/\/towardsdatascience.com\/polynomial-regression-in-python-b69ab7df6105)","c4bcf534":"### As we can see above the degree 0 and 1 dont perform well i.e it can be said that it's underfitting. Let's try increasing the degree and move to higher order polynomial functions.","816feb44":"#### Similarly for the bias we get partial derivative as,\n\n![image.png](attachment:e7378324-7f8c-405a-92a7-457c5bbea5c4.png) [source](https:\/\/towardsdatascience.com\/gradient-descent-from-scratch-e8b75fa986cc)","bbb5be37":"#### Here the test loss continues to decrease , what does this signify?. We can safely say that higher degrees might overfit on smaller data and  may perform well if there are large number of data samples. Thus one way to avoid overfitting in regression could be using lower degree polynomial equation or increasing data samples. ","cd15b016":"### Let's create a dictionary to store our model weights","1630d9e1":"#### Let's plot our model vs data","b0de7476":"# **Degree 1**","56cded0a":"## Degree 5","f32379dd":"## Degree 3","03f7bd2f":"#### Now that we have defined our loss function, let's talk about the gradients of the loss we require for finding our model weights and bias. Calculating the partial derivative with respect to m (our weights) gives us,\n\n![image.png](attachment:dba9c5f5-51f8-4092-b73c-1ba206c40e26.png) [source](https:\/\/towardsdatascience.com\/gradient-descent-from-scratch-e8b75fa986cc)","a2042fe1":"# **Higher order polynomials**\n#### For a uni-variate higher polynomial regression we transform our input ,say we are performing a degree 2 regression model so we add `X^2` feature to our input matrix if degree  3 we add `X^2` `X^3` so on.","a7c3e9f7":"#### Now let's write our helper functions for training and write a function for implementing model training"}}