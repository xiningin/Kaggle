{"cell_type":{"fa54ce08":"code","d80c2b45":"code","fd8923c6":"code","115be7bf":"code","696ce170":"code","6bfd31f2":"code","b08cf38e":"code","1297d886":"code","7c3d11d7":"code","1bf2af2f":"code","9e4a9d24":"code","9252cfcd":"code","3af03b90":"code","1771d57b":"code","eee43d1d":"code","ae001464":"code","e5f68618":"code","127231ce":"code","261db4ae":"code","d9a6e079":"code","b797008c":"code","e1ebc02e":"code","afd3f742":"code","aa0f231b":"code","42837bac":"code","0fd90ec7":"code","ab13f6de":"code","c590df7f":"code","445a823f":"code","59a35f4f":"code","17504b34":"code","e4dc0894":"code","957c08d1":"code","abd9060d":"code","4323558e":"code","1e186488":"code","3c3ae927":"code","a7b67ef0":"code","52cb099b":"code","5f7a2da4":"code","ba8926bd":"code","eeb0ee31":"code","89c7a921":"markdown","b8d579b9":"markdown","dc60abb7":"markdown","d9765931":"markdown","75f2e189":"markdown","f8789d52":"markdown","42939548":"markdown","d69ddfb6":"markdown","e3a53767":"markdown","a727a71c":"markdown","54d870f4":"markdown","eee4ab61":"markdown","9cf755e7":"markdown","f8bed4b6":"markdown","269b8f78":"markdown","886fae9a":"markdown","6c61fbfb":"markdown","bf38e9cc":"markdown","39b250a9":"markdown","4d3dcd1f":"markdown","e52b1c53":"markdown","03783721":"markdown","eeda1361":"markdown","3fbc98c3":"markdown","4dace505":"markdown","d6ab35af":"markdown","0aa002be":"markdown","3562a4b8":"markdown","883afa5d":"markdown","1a36f788":"markdown","404d4418":"markdown","419f92bc":"markdown","43450f85":"markdown","aaa33dd1":"markdown","c74d3c7a":"markdown","2bbe991a":"markdown","cb5b471d":"markdown","70f111ba":"markdown","0a3f7646":"markdown","fe8207e8":"markdown","b92d0a1a":"markdown","ee345a78":"markdown","60e0b934":"markdown","39763e88":"markdown"},"source":{"fa54ce08":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n#Jupyter notebook tricks\n#https:\/\/www.dataquest.io\/blog\/jupyter-notebook-tips-tricks-shortcuts\/\n\n#https:\/\/ipython.readthedocs.io\/en\/stable\/config\/extensions\/autoreload.html\n#The below 2 lines will automatically reload any changed modules before executing any line of code.\n#load_ext is an IPython magic command\n#More about magic commands:\n#1)https:\/\/ipython.org\/ipython-doc\/3\/interactive\/tutorial.html\n#2)https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/01.03-magic-commands.html\n#3)https:\/\/ipython.org\/ipython-doc\/3\/interactive\/reference.html - good explanation of magic commands\n#autoreload is a IPython extension to automatically reload modules.\n!pip install git+https:\/\/github.com\/fastai\/fastai@2e1ccb58121dc648751e2109fc0fbf6925aa8887\n!apt update && apt install -y libsm6 libxext6\n%load_ext autoreload\n%autoreload 2\n\n#The below line is used to plot charts inline in the notebook, instead of having the charts displayed in a seperate window.\n%matplotlib inline\n\nimport pandas as pd\nimport numpy as np\n\n#Import the necessary libraries\nfrom pandas_summary import DataFrameSummary\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom IPython.display import display\nfrom sklearn import metrics\nfrom fastai.imports  import *\nfrom fastai.structured  import *\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d80c2b45":"#We are executing a shell command here. You can do that by using the ! character before the command as\n#shown here. You can execute any command you want here by preceding the command with the ! character\n#Format: !<command to execute>\nPATH = \"..\/input\/\"\n!ls {PATH}","fd8923c6":"#Read the contents of train.csv into a dataframe using the Pandas library\ndf_raw = pd.read_csv(f'{PATH}TrainAndValid.csv',low_memory=False,parse_dates=[\"saledate\"])","115be7bf":"#Lets look at the top 5 rows using the Pandas DataFrame head() method\ndf_raw.head()","696ce170":"#The info method is useful to get a quick description of the data(# of columns, #of rows,datatypes of each column )\ndf_raw.info()","6bfd31f2":"def display_all(df):\n    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n        display(df)","b08cf38e":"display_all(df_raw.tail().T)","1297d886":"display_all(df_raw.describe(include='all').T)","7c3d11d7":"df_raw.SalePrice = np.log(df_raw.SalePrice)","1bf2af2f":"#n_jobs=-1 indicates that the algorithm should use parallelism as part of its fit\/predict phases. With n_jobs=-1\n#scikit-learn will use all the CPU's available.\n#For more information on this parameter, you can take a look at: http:\/\/scikit-learn.org\/stable\/glossary.html#term-n-jobs\n#m = RandomForestRegressor(n_jobs=-1)\n# The following code is supposed to fail due to string values in the input data\n#m.fit(df_raw.drop('SalePrice',axis=1),df_raw.SalePrice)","9e4a9d24":"add_datepart(df_raw,'saledate')\ndf_raw.saleYear.head()\n","9252cfcd":"#train_cats will not change the way the dataframe looks but behind the scenes it assign numbers to each\n#of the categories.\ntrain_cats(df_raw)\ndf_raw.UsageBand.cat.categories\n","3af03b90":"#Check the columns in the dataframe\ndf_raw.columns\n","1771d57b":"#There is a kind of categorical variable called \u201cordinal\u201d. An ordinal categorical variable has some kind of order (e.g. \u201cLow\u201d < \u201cMedium\u201d < \u201cHigh\u201d). \n#Random forests are not terribly sensitive for that fact, but it is worth noting.\n\ndf_raw.UsageBand.cat.set_categories(['High','Medium','Low'],ordered=True,inplace=True)","eee43d1d":"df_raw.UsageBand = df_raw.UsageBand.cat.codes","ae001464":"#https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.isnull.html\ndisplay_all(df_raw.isnull().sum().sort_index()\/len(df_raw))","e5f68618":"os.makedirs('tmp',exist_ok=True)\ndf_raw.to_feather('tmp\/bulldozers-raw')","127231ce":"df_raw = pd.read_feather('tmp\/bulldozers-raw')","261db4ae":"df, y, nas = proc_df(df_raw,'SalePrice')","d9a6e079":"m=RandomForestRegressor(n_jobs=-1)\nm.fit(df,y)\nm.score(df,y)","b797008c":"#With DataFrame, slicing inside of [] slices the rows. This is provided largely as a convenience since it is such a common operation.\n#https:\/\/pandas.pydata.org\/pandas-docs\/stable\/indexing.html\n\ndef split_vals(a,n): return a[:n].copy(), a[n:].copy()\n\nn_valid = 12000  # same as Kaggle's test set size\nn_trn = len(df)-n_valid\nraw_train, raw_valid = split_vals(df_raw, n_trn)\nX_train, X_valid = split_vals(df, n_trn)\ny_train, y_valid = split_vals(y, n_trn)\n\nX_train.shape, y_train.shape, X_valid.shape","e1ebc02e":"def rmse(x,y): return math.sqrt(((x - y)**2).mean())\n\ndef print_score(m):\n    res = [rmse(m.predict(X_train),y_train), rmse(m.predict(X_valid),y_valid),\n          m.score(X_train,y_train), m.score(X_valid,y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)\n\n","afd3f742":"m = RandomForestRegressor(n_jobs=-1)\n%time (m.fit(X_train,y_train))\nprint_score(m)","aa0f231b":"df_trn, y_trn, nas = proc_df(df_raw,'SalePrice', subset=30000, na_dict=nas)\nX_train, _ = split_vals(df_trn,20000)\ny_train, _ = split_vals(y_trn,20000)\n\nm = RandomForestRegressor(n_jobs=-1)\n%time (m.fit(X_train,y_train))\nprint_score(m)","42837bac":"m = RandomForestRegressor(n_estimators=1, max_depth=3, bootstrap=False, n_jobs=-1)\nm.fit(X_train,y_train)\nprint_score(m)","0fd90ec7":"draw_tree(m.estimators_[0], df_trn, precision=3)","ab13f6de":"#Here we have removed the depth parameter to see if that makes a difference\n#As you can see, the R2 is better than the earlier R2. However, its still not up to the mark.\nm = RandomForestRegressor(n_estimators=1, bootstrap=False, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","c590df7f":"m = RandomForestRegressor(n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","445a823f":"#Good explanation of slicing multi dimensional numpy arrays can be found in the book: Python for Data Analysis by Wes McKinney. Refer Chapter 4\npreds = np.stack([t.predict(X_valid) for t in m.estimators_]) \npreds[:,0],np.mean(preds[:,0]),y_valid[0]","59a35f4f":"plt.plot([metrics.r2_score(y_valid, np.mean(preds[:i+1], axis=0)) for i in range(10)]);\n\n","17504b34":"m = RandomForestRegressor(n_estimators=20, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","e4dc0894":"m = RandomForestRegressor(n_estimators=40, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","957c08d1":"m = RandomForestRegressor(n_estimators=80, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","abd9060d":"#Possible explanation as to why the oob score is better here than the R2 score of the validation set, whereas Jeremy says that it should generally be lower\n#https:\/\/forums.fast.ai\/t\/oob-then-and-now-2017-11-vs-2018-10\/23913\n#The below OOB score also proves that the validation set time difference is making a difference here. The OOB score was calculated on data points in the same time range\n#and we got a higher OOB score. This proves that the time difference in the validation set is what is making the difference when compared to the training data.\n\nm = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","4323558e":"df_trn, y_trn, nas = proc_df(df_raw, 'SalePrice')\nX_train, X_valid = split_vals(df_trn, n_trn)\ny_train, y_valid = split_vals(y_trn, n_trn)","1e186488":"set_rf_samples(20000)","3c3ae927":"m = RandomForestRegressor(n_jobs=-1, oob_score=True)\n%time m.fit(X_train, y_train)\nprint_score(m)","a7b67ef0":"m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","52cb099b":"reset_rf_samples()","5f7a2da4":"m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\n%time m.fit(X_train, y_train)\nprint_score(m)","ba8926bd":"m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, \n                          n_jobs=-1, oob_score=True) \n%time m.fit(X_train, y_train) \nprint_score(m)","eeb0ee31":"m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3,max_features=0.5, n_jobs=-1, oob_score=True) \nm.fit(X_train, y_train)\nprint_score(m)","89c7a921":"We can also increase the amount of variation amongst the trees by not only use a sample of rows for each tree, but to also using a sample of columns for each split. We do this by specifying max_features, which is the proportion of features to randomly select from at each split.","b8d579b9":"**Pre-processing**","dc60abb7":"**Since each additional tree allows the model to see more data, this approach can make additional trees more useful.**","d9765931":"**Bagging**\nTo learn about bagging in random forests, let's start with our basic model again.","75f2e189":"This will take the same amount of time to run as before, **but every tree has an access to the entire dataset. **","f8789d52":"**The basic idea is this**: rather than limit the total amount of data that our model can access, let's instead limit it to a different random subset per tree. **That way, given enough trees, the model can still see all the data, but for each individual tree it'll be just as fast as if we had cut down our dataset as before**.","42939548":"Earlier, we took 30,000 rows and created all the models which used a different subset of that 30,000 rows. Why not take a totally different subset of 30,000 each time? In other words, let\u2019s leave the entire 389,125 records as is, and if we want to make things faster, pick a different subset of 30,000 each time. So rather than bootstrapping the entire set of rows, just randomly sample a subset of the data\n\nIt turns out that one of the easiest ways to avoid over-fitting is also one of the best ways to speed up analysis: subsampling. Let's return to using our full dataset, so that we can demonstrate the impact of this technique.","d69ddfb6":"It's important to note what metric is being used for a project. Generally, selecting the metric(s) is an important part of the project setup. However, in this case Kaggle tells us what metric to use: RMSLE (root mean squared log error) between the actual and predicted auction prices. Therefore we take the log of the prices, so that RMSE will give us what we need.","e3a53767":"Normally, pandas will continue displaying the text categories, while treating them as numerical data internally. Optionally, we can replace the text categories with numbers, which will make this variable non-categorical, like so:.","a727a71c":"**Subsampling**","54d870f4":"**max_feature**","eee4ab61":"As you see, adding more trees do not help much. It will not get worse but it will stop improving things much. **This is the first hyper parameter to learn to set\u200a\u2014\u200aa number of estimators**. **A method of setting is, as many as you have time to fit and that actually seems to be helping.** Adding more trees slows it down, but with less trees you can still get the same insights. **So when Jeremy builds most of his models, he starts with 20 or 30 trees and at the end of the project or at the end of the day\u2019s work, he will use 1000 trees and run it over night.**","9cf755e7":"Here is a plot of R\u00b2 values given first i trees. As we add more trees, R\u00b2 improves. But it seems as though it has flattened out.","f8bed4b6":"**The biggest tip:**Most people run all of their models on all of the data all of the time using their best possible parameters which is just pointless. If you are trying to find out which feature is important and how they are related to each other, having that 4th decimal place of accuracy is not going to change any of your insights at all. Do most of your models on a large enough sample size that your accuracy is reasonable (within a reasonable distance of the best accuracy you can get) and taking a small number of seconds to train so that you can interactively do your analysis.","269b8f78":"**min_sample**","886fae9a":"We're still not quite done - for instance we have lots of missing values, which we can't pass directly to a random forest.\nThe below  will add a number of empty values for each series, we sort them by the index (pandas.Series.sort_index), and divide by a number of dataset.","6c61fbfb":"We revert to using a full bootstrap sample in order to show the impact of other over-fitting avoidance methods.","bf38e9cc":"**Tree building parameters** - Lets explore some more parameters","39b250a9":"**Reducing over-fitting**","4d3dcd1f":"As you can see above, we have a lot of missing data as well. ","e52b1c53":"Sometimes your dataset will be small and you will not want to pull out a validation set because doing so means you now do not have enough data to build a good model. However, random forests have a very clever trick called out-of-bag (OOB) error which can handle this (and more!)\n\n\n\nIs our validation set worse than our training set because we're over-fitting, or because the validation set is for a different time period, or a bit of both? With the existing information we've shown, we can't tell. However, random forests have a very clever trick called out-of-bag (OOB) error which can handle this (and more!)\nThe idea is to calculate error on the training set, but only include the trees in the calculation of a row's error where that row was not included in training that tree. This allows us to see whether the model is over-fitting, without needing a separate validation set.\nThis also has the benefit of allowing us to see whether our model generalizes, even if we only have a small amount of data so want to avoid separating some out to create a validation set.\nThis is as simple as adding one more parameter to our model constructor. We print the OOB error last in our print_score function below.\n","03783721":"By default when you read a dataframe, as the number of columns increases,  there is a chance that not all the data gets displayed fully in the table and instead the display is truncated. Hence,  the below function display_all, ensures that all the data is displayed in its full form. We make use of the options listed at: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/options.html. Also, please observe that the columns are printed row wise and the rows are printed column wise. We are doing this as there are huge number of columns to print.","eeda1361":"**Initial modelling using Random Forest algorithm**","3fbc98c3":"Lets create a bigger tree to see how it fares","4dace505":"1. min_sample_leaf=3 : Stop training the tree further when a leaf node has 3 or less samples (before we were going all the way down to 1). This means there will be one or two less levels of decision being made which means there are half the number of actual decision criteria we have to train (i.e. faster training time).\n2. For each tree, rather than just taking one point, we are taking the average of at least three points that we would expect the each tree to generalize better. But each tree is going to be slightly less powerful on its own.\n3.The numbers that work well are 1, 3, 5, 10, 25, but it is relative to your overall dataset size.\n","d6ab35af":"1. low_memory=False  - More of the file is read so that the types are inferred correctly\n2. parse_dates=[\"saledate\"] - Results in the \"saledate\" column being read as a seperate date column","0aa002be":"This dataset contains a mix of continuous and categorical variables.\nThe following method extracts particular date fields from a complete datetime for the purpose of constructing categoricals. You should always consider this feature extraction step when working with date-time. Without expanding your date-time into these additional fields, you can't capture any trend\/cyclical behavior as a function of time at any of these granularities.\n","3562a4b8":"* max_features=0.5 : The idea is that the **less correlated your trees are** with each other, the better. Imagine you had one column that was so much better than all of the other columns of being predictive that every single tree you built always started with that column. But there might be some interaction of variables where that interaction is more important than the individual column. So if every tree always splits on the same thing the first time, you will not get much variation in those trees.\n* In addition to taking a subset of rows, at every single split point, take a different subset of columns.\n* For row sampling, each new tree is based on a random set of rows, for column sampling, every individual binary split, we choose from a different subset of columns.\n* 0.5 means randomly choose a half of them. There are special values you can use such as sqrt or log2\n* Good values to use are 1, 0.5, log2, or sqrt\n\n\n","883afa5d":"After running add_datepart, it added many numerical columns and removed saledate column. This is not quite enough to get passed the error we saw earlier as we still have other columns that contain string values. Pandas has a concept of a category data type, but by default it would not turn anything into a category for you. Fast.ai provides a function called **train_cats** which creates categorical variables for everything that is a String. **Behind the scenes, it creates a column that is an integer and it is going to store a mapping from the integers to the strings**. train_cats is called \u201ctrain\u201d because it is training data specific. It is important that validation and test sets will use the same category mappings (in other words, if you used 1 for \u201chigh\u201d for a training dataset, then 1 should also be for \u201chigh\u201d in validation and test datasets). **For validation and test dataset, use apply_cats instead.**","1a36f788":"To make these trees better, we will build a forest, which consists of various trees. We will use a technique called **bagging**, to build the forest. Bagging should make the model more generalizable.","404d4418":"Everything in scikit-learn has the following form:\n-Instantiate an object of the machine learning model.\n-Fit the data using the model. As part of the fit operation, the algorithm tries to learn the relationship between the independent variables and the dependent variables. In our case, everything except the SalePrice are the independent variables and the value which will be predicted(in our case: SalePrice) is the dependent variable.\n-axis=1 means remove columns","419f92bc":"Another way to reduce over-fitting is to grow our trees less deeply. We do this by specifying (with min_samples_leaf) that we require some minimum number of rows in every leaf node. This has two benefits:\n\n* 1.     There are less decision rules for each leaf node; simpler models should generalize better\n* 2.     The predictions are made by averaging more rows in the leaf node, resulting in less volatility\n\n","43450f85":"Building a single tree","aaa33dd1":"Reading CSV took about 10 seconds, and processing took another 10 seconds, so if we do not want to wait again, it is a good idea to save them. Here we will save it in a feather format. What this is going to do is to save it to disk in exactly the same basic format that it is in RAM. This is by far the fastest way to save something, and also to read it back. Feather format is becoming standard in not only Pandas but in Java, Apache Spark, etc.","c74d3c7a":"We now have something we can pass to a random forest!","2bbe991a":"In the future we can simply read it from this fast format.","cb5b471d":"Here OOB is higher than validation set. This is because our validation set is a different time period whereas OOB samples are random. It is much harder to predict a different time period.","70f111ba":"Co-efficient of determination -  When you have a negative co-efficient of determination, it means that your model has performed worse than the naive model that predicts the mean for all observations.\nA great article which explains the co-efficient of determination: **https:\/\/ragrawal.wordpress.com\/2017\/05\/06\/intuition-behind-r2-and-other-regression-evaluation-metrics\/#comment-7387**\n\n\n","0a3f7646":"Machine learning algorithms work only with numbers. Hence, as part of data pre-processing, we try to\nconvert any string values into numbers.\n\nHere are some of the information we can extract from date\u200a\u2014\u200ayear, month, quarter, day of month, day of week, week of year, is it a holiday? weekend? was it raining? was there a sport event that day? It really depends on what you are doing. If you are predicting soda sales in SoMa, you would probably want to know if there was a San Francisco Giants ball game that day. **What is in a date is one of the most important piece of feature engineering you can do** and no machine learning algorithm can tell you whether the Giants were playing that day and that it was important. So this is where you need to do feature engineering.\n\nThe **add_datepart** method extracts particular date fields from a complete datetime for the purpose of constructing categoricals. You should always consider this feature extraction step when working with date-time. Without expanding your date-time into these additional fields, you can\u2019t capture any trend\/cyclical behavior as a function of time at any of these granularities.\n","fe8207e8":"We will replace categories with their numeric codes, handle missing continuous values, and split the dependent variable into a separate variable.","b92d0a1a":"Numpy lets us treat arrays, matrices, vectors, high dimensional tensors as if they are Python variables","ee345a78":"As per Prof. Jeremy Howard, if there is any operation that takes more than 10 seconds, then it becomes extremely difficult to work with that data interactively. Hence, one of the approaches that is taken is to work with a subset of the data. Once we decide upon the hyperparameters and are done with the feature engineering on this subset of the data, we then run the model on the entire dataset which takes much more time than it took on the subset of the data. We do this process in the below cell. Make sure that the validation set remains the same.","60e0b934":"The shape of the above curve suggests that adding more trees isn't going to help us much. Let's check. (Compare this to our original model on a sample)","39763e88":"**Random Forests**\nHaving a validation set if one of the most important steps in building a machine learning model. No one in the industry does this but as per Prof. Jeremy Howard this is a very very important step.\nNow, lets try our model again with a training set and a validation set.\n  "}}