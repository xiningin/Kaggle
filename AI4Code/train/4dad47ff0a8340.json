{"cell_type":{"361ad5c1":"code","4104a295":"code","38c693d3":"code","8bf8e43e":"code","0a8025ae":"code","f9f5b9cd":"code","85a6d2f3":"code","db3b3279":"code","6be0c907":"code","28fa3c1c":"code","06099cf3":"code","c8e3916a":"code","fec2f63e":"code","6f551ff7":"code","2784f380":"code","41ae20fe":"code","30d79a9e":"code","2b71080e":"code","aa31c515":"code","54aab9c0":"code","04c86853":"code","eb4be556":"code","31b65d1b":"code","1f228ad9":"code","ed72ca8c":"code","0ca0ad08":"code","22797c95":"code","aaeac1dc":"code","7e4ac056":"code","dabab983":"markdown","de847c79":"markdown","2966d37b":"markdown","a60e0a86":"markdown","2b482683":"markdown","e9877e89":"markdown","6c9e20e7":"markdown","d45eeaa2":"markdown","cd39d2a3":"markdown","012f3734":"markdown","ba66ebd1":"markdown","6403fdfb":"markdown","538f3e09":"markdown","ee2b859e":"markdown","e3cf6762":"markdown","e00ec07e":"markdown","38fde966":"markdown","a4945147":"markdown","0f393004":"markdown","9c0ec072":"markdown","a5122ecf":"markdown","284bd7c4":"markdown","a5f83a19":"markdown","0c5386bd":"markdown","94bd5126":"markdown","89be7d02":"markdown","36b3b921":"markdown","af808e0e":"markdown","3ce54080":"markdown","7e3ae3df":"markdown","6c342916":"markdown","fffc3c26":"markdown","08ed2dab":"markdown","7f1259e2":"markdown","f6604e7e":"markdown"},"source":{"361ad5c1":"import pandas as pd \nimport numpy as np \nimport random\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn import preprocessing","4104a295":"df = pd.read_csv(\"..\/input\/wine-quality-dataset\/WineQT.csv\")","38c693d3":"df.head()","8bf8e43e":"df.info()","0a8025ae":"df.describe()","f9f5b9cd":"df.isnull().sum()","85a6d2f3":"y = df[\"quality\"]\nX = df.loc[:, df.columns!='quality']","db3b3279":"X = X.drop(\"Id\", axis = 1)","6be0c907":"X.head()","28fa3c1c":"y.head()","06099cf3":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","c8e3916a":"def make_mi_scores(X, y):\n    mi_scores = mutual_info_regression(X, y, discrete_features=False)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(X_train, y_train)","fec2f63e":"def plot_utility_scores(scores):\n    y = scores.sort_values(ascending=True)\n    width = np.arange(len(y))\n    ticks = list(y.index)\n    plt.barh(width, y)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n\n\nplt.figure(dpi=100, figsize=(8, 5))\nplt.xlabel(\"Score\")\nplt.ylabel(\"Feature\")\nplot_utility_scores(mi_scores)\n\n","6f551ff7":"drop_cols = [\"free sulfur dioxide\",\"residual sugar\",\"pH\",\"fixed acidity\",\"density\"]\nX_train = X_train.drop(drop_cols,axis=1)\nX_test = X_test.drop(drop_cols,axis=1)","2784f380":"X_train.columns, X_test.columns","41ae20fe":"min_max_scaler = preprocessing.MinMaxScaler()\nX_train =  min_max_scaler.fit_transform(X_train)","30d79a9e":"X_test = min_max_scaler.transform(X_test)","2b71080e":"print(f'Training data:\\n{X_train[0:5]}\\nTest data:\\n{X_test[0:5]}')","aa31c515":"y_train = y_train.astype('float32').to_numpy().astype(np.float32)\ny_test = y_test.astype('float32').to_numpy().astype(np.float32)","54aab9c0":"def data_iter(batch_size, features, targets):\n    num_examples = len(features)\n    indices = list(range(num_examples))\n    # The examples are read at random, in no particular order\n    random.shuffle(indices)\n    for i in range(0, num_examples, batch_size):\n        batch_indices = np.array(indices[i:min(i+batch_size,num_examples)])\n        yield features[batch_indices],targets[batch_indices]","04c86853":"batch_size = 10\nfor X, y in data_iter(batch_size, X_train, y_train):\n    print(X,'\\n',X.shape[0], '\\n', y)\n    break","eb4be556":"def linear_regression(X, w, b): #@save\n    \"\"\"The linear regression model.\"\"\"\n    return np.dot(X, w) + b","31b65d1b":"def initialize_params():\n    w = np.random.normal(0, 0.1, size=(X_train.shape[1], 1)).astype(np.float32)\n    b = np.zeros(1).astype(np.float32)\n    return w,b","1f228ad9":"def mean_squared_loss(y_hat, y): #@save\n    \"\"\"Squared loss.\"\"\"\n    return ((1\/2)*(y_hat - y.reshape(y_hat.shape))**2).mean()","ed72ca8c":"def mbgd(X, params, batch_size, lr=0.005): #@save\n    y_hat = linear_regression(X, w, b)\n    error = (y_hat - y.reshape(y_hat.shape))\n    w_derivative = (np.dot(X.T,error)).sum()\n    params[0] = params[0] - ((lr\/batch_size)*w_derivative)\n    params[1] = params[1] - ((lr\/batch_size)*error.sum())\n    return params[0],params[1]","0ca0ad08":"lr = 0.01\nnum_epochs = 20\nbatch_size = 10","22797c95":"w,b =  initialize_params()\nepoch_list = []\ncost_list = []\nloss_array = np.empty((0, 2), np.float32)\nfor epoch in range(num_epochs):\n    for X, y in data_iter(batch_size, X_train, y_train):\n             w,b = mbgd(X, [w,b], X.shape[0], lr)\n    train_cost = mean_squared_loss(linear_regression(X_train, w, b), y_train)\n    epoch_list.append(epoch+1)\n    cost_list.append(train_cost)\n    print(f'learning rate = {lr}, epoch = {epoch + 1}, training cost = {float(train_cost):f}')\n","aaeac1dc":"plt.plot(epoch_list,cost_list)\nplt.xlabel(\"No. of epochs\")\nplt.ylabel(\"Training cost\")\nplt.xlim(0,21)\nplt.show()","7e4ac056":"y_test_pred = linear_regression(X_test,w,b)\ny_train_pred = linear_regression(X_train,w,b)\ntest_MSE  = mean_squared_loss(y_test_pred,y_test).mean()\ntrain_MSE = mean_squared_loss(y_train_pred,y_train).mean()\nprint(f'Test error is {test_MSE}\\nTrain error is {train_MSE}')","dabab983":"The **MinMaxScaler()** automatically transforms our data to numpy array. So *X_train* and *X_test* are now numpy arrays. But we have to manually convert *y_train* and *y_test* to numpy arrays.","de847c79":"# 2. Loading & Analyzing Data\nWe load the **Wine Quality Dataset** and store it in a pandas dataframe.","2966d37b":"# 8. Initializing Parameters\nThe values for weight parameters are initialized taking values from the normal distribution with a mean of 0 and standard deviation of 0.1.\n<br> The bias is initialized with 0.","a60e0a86":"We then split our dataset into train and test set where 80% of our data is for model training and 20% is used for making predictions and validating our model.","2b482683":"Now we check if there are any missing values or null values in our data for each column respectively.","e9877e89":"# 4. Feature Selection","6c9e20e7":"# 7. Defining Model: Linear Regression","d45eeaa2":"Next we look at some descriptive statistics of our dataset such as the count, mean, median, standard deviation, the quartiles, maximum and minimum using the function **describe()**.","cd39d2a3":"# 6. Defining a Data Iterator\nWe define a generator function data_iter() that returns us a minibatch of data. Each minibatch consists of a tuple of features\nand targets.","012f3734":"# 9. Defining Cost Function\nThe cost function is used to calculate the error in our predicted value. We will use the mean squared error as our cost function. The cost function **J** is defined such:\n$$ J  = {1 \\over 2m} \\sum\\limits_{i=1}^{m} ({\\hat{y}-y^{(i)})}^2  $$\nwhere, $$\\hat{y} = w*X^{(i)}+b $$\nwhich is linear regression itself.","ba66ebd1":"We use **MinMaxScaler()** for normalizing our data. We fit the scaler to our trainig data and transform it accordingly.","6403fdfb":"Linear regression is a linear approach to modelling the relationship between one dependent variable and one or more independent variables using a straight line.\nThe straight line is represented such: $$y = w*X+b$$\n\nwhere,\n<br>w = weight\n<br>b = bias\n<br>X = feature matrix(independent variables)\n<br>y = target vector(dependent variable)\n\n\n","538f3e09":"Now let us take a peek into our feature matrix ***X***.","ee2b859e":"We transoform our test data using the scaler which we fitted to our training data.","e3cf6762":"# 3. Splitting Feature & Target\n\nSince we are to make predictions for the quality of wine, the column ***quality*** is to be considered the ***target*** vector, we denote it by ***y***. We make a feature matrix ***X*** by dropping the ***target*** column from our dataset.","e00ec07e":"# 11. Training Loop\n\nWe initialize the value for learning rate to 0.01 and set the number of epochs to 20. It means that our training loop will run 10 times. We also set the batch_size to 10.\n","38fde966":"We also don't need the column ***Id*** so we can drop it as well from our feature matrix ***X***.","a4945147":"Now we can see that we have a total of six features remaining.","0f393004":"Next we shall take a peek at our data","9c0ec072":"# 10. Optimization Algorithm\nOur goal is to minimize the cost as much as possible in order to find the best fit line. For that we need to define an algorithm for minimizing our cost function. Gradient descent is an optimization algorithm used to minimize some function(cost function in our case) by iteratively moving in the direction of steepest descent as defined by the negative of the gradient.  We define a version of gradient descent that takes a minibatch of data samples for optimizing the cost function. This is known as *Minibatch Gradient Descent*.\n\n<br> The steps include:\n1. For a minibatch of data make predictions:\n$$\\hat{y} = w*X+b $$\n2. Calculate the error of prediction:\n$$error = \\hat{y}-y $$\n3. Calculate the derivative for weight:\n$${d(J) \\over dw} = {1 \\over |B|} \\sum\\limits_{i=1}^{|B|} X*error $$\n<br>\n&ensp;&ensp;&ensp; where |B| = Batch Size (Since we are calculating loss for a minibatch of data).\n<br>\n<br>\n4. Calculate the derivative for bias:\n$${d(J) \\over db} = {1 \\over |B|} \\sum\\limits_{i=1}^{|B|} error $$\n<br>\n&ensp;&ensp;&ensp; where |B| = Batch Size (Since we are calculating loss for a minibatch of data).\n<br>\n<br>\n5. We update the parameters w and b:\n$$ w = w - lr * {d(J) \\over dw}  $$\n<br>\n$$ b = b - lr * {d(J) \\over db}  $$\n<br>\n&ensp;&ensp;&ensp; where lr = learning rate.\n\n<br>\n\nBelow we implement the Minibatch Gradient Descent algorithm:","a5122ecf":"Let us read and print the first small batch of data examples.","284bd7c4":"We calculate the ***Mutual information*** scores of our features with respect to the target. Mutual information measures the similarity between each of the features and the target variable.","a5f83a19":"With this you're good to go with performing linear regression on data sets!\n<br>Reviews and edits are always welcome. \n<br>Let's share and grow :)","0c5386bd":"We can see from the above plot that alcohol has the highest similarity with wine quality. \n<br> We will drop the features that have a mutual information score less than 0.04 as these features aren't much similar to our target vector and are less significant in prediction.","94bd5126":"So there are no missing values in our data","89be7d02":"# 1. Importing Libraries\n\nWe import:-\n* ***pandas*** for loading and analyzing our data.\n* ***numpy*** for fitting a model to our data and for making predictions\n* ***random*** for randomly shuffling our data\n* ***matplotlib.pyplot*** for data visualization\n* ***train_test_split*** for splitting our train and test set\n* ***mutual_info_regression*** for feature selection\n* ***preprocessing*** for applying normalization","36b3b921":"# 12. Evaluating Accuracy\nSince we are working with regression, we shall use the mean squared error(MSE) for evaluating our model. We predict the target vector for the given test data(X_test) and calculate the mean squared error between the predicted values(y_test_pred) and the actual target values of the test data(y_test). We also calculate the mean squared error(MSE) for the training data.\n","af808e0e":"We can plot and see how the cost is minimized and converges to a minima.","3ce54080":"# 5. Scaling & Normalizing Data","7e3ae3df":"Next we take a look at some certain information(index, data type, column names, non-null values and memory usage) of our dataset using the function **info()**.","6c342916":"We follow these steps in our training loop:\n1. Select a random minibatch of data equal to the given batch_size (in our case 10). This can be done by the data_iter generator we created a while back.\n2. Update the parameters(weights and bias) using the optimization algorithm *Minibatch Gradient Descent*.\n3. After the whole dataset has been exhausted once, calculate the cost over the whole training set.\n4. Continue this process until the max number of epochs has been reached.","fffc3c26":"We also take a peek into our target vector ***y***.","08ed2dab":"Now all the values of our train and test features should be in the range 0-1.","7f1259e2":"As we run the iteration, we obtain distinct minibatches successively until the entire dataset has\nbeen exhausted.","f6604e7e":"# Introduction\nIn this notebook we will learn:-\n* How to make simple data analysis.\n* How to select features which are more similar to our target.\n* How to code linear regression from scratch using minibatch gradient descent.\n* How to fit the model and make predictions using it.\n\nWe will use the \"wine-quality-dataset\" in this notebook.\n\nThis notebook is divided into several sections which are:\n1. **Importing Libraries**\n2. **Loading & Analyzing Data**\n3. **Splitting Feature & Target**\n4. **Feature Selection**\n5. **Scaling & Normalizing Data**\n6. **Defining a Data Iterator**\n7. **Defining Model: Linear Regression**\n8. **Initializing Parameters**\n9. **Defining Cost Function**\n10. **Optimization Algorithm**\n11. **Training Loop**\n12. **Evaluating Accuracy**"}}