{"cell_type":{"4d105551":"code","9f0b2ab9":"code","23abbb4c":"code","388518e8":"code","b7c903db":"code","05242e9d":"code","7c1be0b6":"code","8c8c5454":"code","f7cb64cc":"code","d93eff23":"code","193ccb4e":"code","866de217":"code","d4c55818":"code","4f5e7fe3":"code","161d668f":"code","83592f8a":"code","caa5b628":"code","8a4baddb":"code","fe4f01fe":"code","56b466bd":"code","b6b6a547":"code","fb41e6c2":"code","cb8c9701":"code","f02d3431":"markdown","db81f7a6":"markdown","9b8b7734":"markdown","b4f06063":"markdown","dfedb273":"markdown","6ec4e209":"markdown","cda5a1c2":"markdown","e673b2ac":"markdown","c9235fd9":"markdown"},"source":{"4d105551":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline  \n\nimport time\nstart_time = time.time()\nfrom sklearn.model_selection import train_test_split\nimport sys, os, re, csv, codecs, numpy as np, pandas as pd\nnp.random.seed(32)\nos.environ[\"OMP_NUM_THREADS\"] = \"4\"\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\nfrom keras.engine import InputSpec, Layer\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom keras.layers import GRU, BatchNormalization, Conv1D, MaxPooling1D\n\nimport logging\nfrom sklearn.metrics import roc_auc_score\nfrom keras.callbacks import Callback\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","9f0b2ab9":"train = pd.read_csv('..\/input\/quora-insincere-questions-classification\/train.csv').fillna(' ')\ntest = pd.read_csv('..\/input\/quora-insincere-questions-classification\/test.csv').fillna(' ')\ntest_qid = test['qid']\ntrain_qid = train['qid']\ntrain_target = train['target'].values\n\ntrain_text = train['question_text']\ntest_text = test['question_text']\n\nall_text = pd.concat([train_text, test_text])\n\n","23abbb4c":"embedding_path = \"..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec\"","388518e8":"embed_size = 300\nmax_features = 130000\nmax_len = 220\n\nlist_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ntrain_text = train_text.str.lower()\ntest_text = test_text.str.lower()\nall_text = all_text.str.lower()","b7c903db":"tk = Tokenizer(num_words = max_features, lower = True)\ntk.fit_on_texts(all_text)\nall_text = tk.texts_to_sequences(all_text)\ntrain_text = tk.texts_to_sequences(train_text)\ntest_text = tk.texts_to_sequences(test_text)","05242e9d":"train_pad_sequences = pad_sequences(train_text, maxlen = max_len)\ntest_pad_sequences = pad_sequences(test_text, maxlen = max_len)\nall_pad_sequences = pad_sequences(all_text, maxlen = max_len)\n","7c1be0b6":"def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))","8c8c5454":"word_index = tk.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.zeros((nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","f7cb64cc":"model = load_model(\"..\/input\/bi-gru-lstm-cnn-poolings-fasttext\/best_model.hdf5\")\n","d93eff23":"train_pred = model.predict(train_pad_sequences, batch_size = 1024, verbose = 1)\ntest_pred = model.predict(test_pad_sequences, batch_size = 1024, verbose = 1)\n#all_pred = model.predict(all_pad_sequences, batch_size = 1024, verbose = 1)","193ccb4e":"train_pred.max()","866de217":"test_pred.max()","d4c55818":"toxic_predictions_train = pd.DataFrame(columns=list_classes, data=train_pred)\ntoxic_predictions_test = pd.DataFrame(columns=list_classes, data=test_pred)\ntoxic_predictions_train['question_text'] = train['question_text'].values\ntoxic_predictions_test['question_text'] = test['question_text'].values\ntoxic_predictions_train['qid'] = train_qid\ntoxic_predictions_test['qid'] = test_qid","4f5e7fe3":"toxic_predictions_train.head()","161d668f":"toxic_predictions_test.head()","83592f8a":"toxic_predictions_train[list_classes].describe()","caa5b628":"toxic_predictions_test[list_classes].describe()","8a4baddb":"print(toxic_predictions_train.sort_values(by=['toxic'], ascending=False)['question_text'].head(10).values)","fe4f01fe":"print(toxic_predictions_train.sort_values(by=['severe_toxic'], ascending=False)['question_text'].head(10).values)","56b466bd":"print(toxic_predictions_train.sort_values(by=['obscene'], ascending=False)['question_text'].head(10).values)","b6b6a547":"print(toxic_predictions_train.sort_values(by=['threat'], ascending=False)['question_text'].head(10).values)","fb41e6c2":"print(toxic_predictions_train.sort_values(by=['insult'], ascending=False)['question_text'].head(10).values)","cb8c9701":"print(toxic_predictions_train.sort_values(by=['identity_hate'], ascending=False)['question_text'].head(10).values)","f02d3431":"Now we'll load the Quora datasets:","db81f7a6":"In other words, at nearly 1.0 probability the model seems pretty confident about the \"toxicity\" of some of the tweets.\n\nNow let's put the predictions into a dataframe, so we can have a better view of them and how they relate to the actual tweets.","9b8b7734":"One of the more underappreciated aspects of Kaggle competitions is that we can \"repurpose\" them for all sorts of different tasks that go beyond the socope of the original context. Not all of the models that we build are equally generalizable, but some can be used for a wide variety of purposes. \n\nIn this kernel I'd like to see how do models built on [Toxic Comment Classification Challenge](https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge\/) perform on non-competition \"real world\" data. Here I will just use one model that was built inside of a [kernel](https:\/\/www.kaggle.com\/tunguz\/bi-gru-lstm-cnn-poolings-fasttext). The kernel scores in the 0.984x AUC range. It's a respectable score, but well below the top solutions that scored in the 0.988x range. I have used this approach to find out [how toxic are Hillary Clinton and Donald Trump tweets](https:\/\/www.kaggle.com\/tunguz\/how-toxic-are-hillary-and-trump-tweets\/), with some interesting insights. \n\nFirst, let's load up the Python libraries.","b4f06063":"The worst 'toxic' train questions:","dfedb273":"In other words, the model seems not to work too well. The fact that soem of these are marked almost 1.0 on probablity scale shows the limitations of this model. It is very likely that Quora employs a very good set of toxicity-detection tools on their site, so the kinds of questions we get in this competition will most likley be already heavily vetted for inapropriate content. ","6ec4e209":"We will embed words from these tweets into a word-vector space using one of the previously trained word embeddings. Here we use a 300-dimensional vector space that comes curtesy of FastText. Unfortunately, this embedding is not available for the Quora compatition, but as we are using this kernel just for the educational purposes, that will be fine. We will also limit the length of text to 220 words. This is an overkill for questions, but for general purpose it is rather small text length. The original was aimed at much longer text sizes, and this was a reasonable length for those purposes. The best embedding that we used in Toxic limited length to 900 words.","cda5a1c2":"Let's see what's the maximum probability for this model:","e673b2ac":"We also need to pad the tweets that are less than 220 words, which is essentially all of them.","c9235fd9":"In order for our pretrained models to work, we need to transform the text here into the appropriate vectorized format."}}