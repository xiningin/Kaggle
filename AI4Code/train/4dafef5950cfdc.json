{"cell_type":{"d1d53587":"code","5ff62d82":"code","e8b7a6ba":"code","c2077e87":"code","37ffd74c":"code","db80f5ba":"code","60387536":"code","ab8e30d8":"code","c28c54e4":"code","ef2d7a21":"code","6d787aa3":"code","4042f24e":"code","58c8d448":"code","628cd4f5":"code","46bf72aa":"code","e5bbefaf":"code","43a69bfb":"code","8254c9b9":"code","4df62ba8":"code","3d8e5aa8":"code","5f2d09b6":"code","c6707dbb":"markdown","34223746":"markdown","4e6b2822":"markdown","3d9e6389":"markdown","622c346b":"markdown","f1f57166":"markdown","f84fdaa6":"markdown","714377ae":"markdown","a4a6cf6f":"markdown","c00a81b9":"markdown","71072fff":"markdown","7282a940":"markdown","3b3eb7d4":"markdown","74dd7bdd":"markdown","790cc75b":"markdown","ee6e380f":"markdown","7c9140bc":"markdown","471c3fc1":"markdown","4ca452f8":"markdown","b9cd8e88":"markdown","11b436c8":"markdown","587b896e":"markdown","e4ab1d80":"markdown","926dd982":"markdown","c3e2a43c":"markdown","b39fe2ed":"markdown","b8a63f03":"markdown"},"source":{"d1d53587":"!pip install tensorflow-addons\n!pip install adabelief-tf\n!git clone https:\/\/github.com\/salvaba94\/G2Net.git g2net\n!git clone https:\/\/github.com\/google\/automl.git g2net\/src\/automl","5ff62d82":"import sys\nimport os\nimport warnings\nimport copy\nimport pandas as pd\nimport numpy as np\nimport multiprocessing as mp\nimport tensorflow as tf\nimport seaborn as sns\nfrom tensorflow.data.experimental import AUTOTUNE\nfrom tensorflow.python.client import device_lib\nfrom tensorflow.keras.layers import Input, Dense, Flatten\nfrom tensorflow.keras.layers.experimental.preprocessing import Resizing\nfrom adabelief_tf import AdaBeliefOptimizer\nfrom kaggle_datasets import KaggleDatasets\nfrom datetime import datetime\nfrom pathlib import Path, os\nfrom functools import partial\nfrom scipy import signal\nfrom scipy import interpolate\nfrom typing import Tuple, Union, Mapping\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","e8b7a6ba":"class Config:\n### General data #############################################################\n    N_SAMPLES, N_DETECT = 4096, 3\n\n### Training #################################################################\n    MODEL_TRAIN = True\n    MODEL_SAVE_NAME = \"Model_Ref.h5\"\n    MODEL_PRELOAD = False\n    MODEL_PRELOAD_NAME = \"Model_Ref.h5\"\n    HISTORY_NAME = \"history_train.csv\"\n\n    MODEL_PATH = Path(\"models\")\n    CKPT_PATH = Path(\"checkpoints\")\n\n    SPLIT = 0.98\n    SEED_SPLIT = 21\n    BATCH_SIZE = 128\n    BATCH_SIZE_TEST = 32\n    EPOCHS = 15\n    LEARNING_RATE = 0.001\n    \n### Prediction ################################################################\n    MODEL_PREDICT = True\n    PREDICTIONS_NAME = \"submission.csv\"\n\n\n### Model ####################################################################\n    TUKEY_SHAPE = 0.25\n    TRAINABLE_TUKEY = False\n\n    DEGREE_FILT = 3\n    F_BAND_FILT = (20., 500.)\n    TRAINABLE_FILT = True\n\n    SAMPLE_RATE = 2048\n    F_BAND_SPEC = (20., 500.)\n    HOP_LENGTH = 64\n    BINS_PER_OCTAVE = 12\n    WINDOW_CQT = \"hann\"\n    TRAINABLE_CQT = True\n    \n    IMAGE_SIZE = 384\n\n    P_PERM = 0.1\n\n    P_MASK = 0.1\n    N_MAX_MASK = 2\n    W_MASK = (IMAGE_SIZE \/\/ 64, IMAGE_SIZE \/\/ 6)\n    \n    MODEL_ID = \"efficientnetv2-s\"\n    MODEL_ID_WEIGHTS = MODEL_ID\n\n### Plotting #################################################################\n    PLOT_EXAMPLE = True\n    PLOT_TEST = False","c2077e87":"MODEL_BUCK = r\"gs:\/\/kds-983b9dcac5a159ffccd004a1e8ad343f6e393ff7c8bb558702180a3c\"\nTEST_BUCK = r\"gs:\/\/kds-066250e6a4aca3761258af1c2dd80a17a7120892c82cd1d77b48327d\"\nTRAIN_BUCK = r\"gs:\/\/kds-54baab6758c40d2c47d569e9b26af8a2be4be98b47494a5b5a5aeb3d\"","37ffd74c":"if MODEL_BUCK is None:\n    MODEL_BUCK = KaggleDatasets().get_gcs_path(\"automl-efficientnetv2-ckpt\")\nprint(MODEL_BUCK)","db80f5ba":"if TEST_BUCK is None:\n    TEST_BUCK = KaggleDatasets().get_gcs_path(\"g2net-float32-test\")\nprint(TEST_BUCK)","60387536":"if TRAIN_BUCK is None:\n    TRAIN_BUCK = KaggleDatasets().get_gcs_path(\"g2net-float32-train\")\nprint(TRAIN_BUCK)","ab8e30d8":"OV_PATH = Path(\"\/kaggle\", \"working\")\nINP_PATH = Path(\"\/kaggle\", \"input\")\nSRC_PATH = str(Path(OV_PATH, \"g2net\", \"src\"))\nCKT_PATH = MODEL_BUCK + \"\/\" + Config.MODEL_ID_WEIGHTS\nTRAIN_PATH = TRAIN_BUCK + \"\/train\"\nTEST_PATH = TEST_BUCK + \"\/test\"\n\nAUTOML_PATH = str(Path(SRC_PATH, \"automl\"))\nEFFNETV2_PATH = str(Path(AUTOML_PATH, \"efficientnetv2\"))\n\nif SRC_PATH not in sys.path:\n    sys.path.append(SRC_PATH)\n    sys.path.append(AUTOML_PATH)\n    sys.path.append(EFFNETV2_PATH)\n    \nfrom utilities import PlottingUtilities, GeneralUtilities\nfrom ingest import TFRDatasetCreator, NPYDatasetCreator, DatasetGeneratorTF\nfrom preprocess import SpectralMask, TimeMask, FreqMask, PermuteChannel\nfrom preprocess import TukeyWinLayer, BandpassLayer, WindowingLayer\nfrom preprocess import CQTLayer \nfrom train import RocLoss, Acceleration, CosineAnnealingRestarts\n\nfrom automl.efficientnetv2 import effnetv2_model\nfrom automl.efficientnetv2 import hparams\nfrom automl.efficientnetv2 import effnetv2_configs\nfrom automl.efficientnetv2 import utils","c28c54e4":"# Create strategy and define data types for data and tensorflow models\nstrategy, device = Acceleration.get_acceleration()\ndtype = tf.float32 # Do not modify since TF Records are in tf.float32","ef2d7a21":"def build_model(\n            input_shape: Tuple[int, int],\n            window_shape: float = 0.2,\n            trainable_window: bool = False,\n            sample_rate: float = 2048., \n            degree_filt: int = 8,\n            f_band_filt: Tuple[float, float] = (20., 500.),\n            trainable_filt: bool = False,\n            hop_length: int = 64,\n            f_band_spec: Tuple[float, float] = (20., 500.),\n            bins_per_octave: int = 12,\n            window_cqt: str = \"hann\",\n            perc_range: float = 0.01, \n            trainable_cqt: bool = False,\n            resize_shape: Tuple[int, int] = (128, 128),\n            p_perm: float = 0.1,\n            p_mask: float = 0.1,\n            n_max_mask_t: int = 2,\n            w_mask_t: Tuple[int, int] = (12, 25),\n            n_max_mask_f: int = 2,\n            w_mask_f: Tuple[int, int] = (12, 25),\n            dtype: type = tf.float32,\n            strategy: str = \"GPU\",\n            effnet_id: str = \"efficientnetv2-b0\",\n            weights: str = \"imagenet\",\n            learning_rate: float = 0.001\n        ) -> tf.keras.Model:\n        \"\"\"\n        Function to build and compile the model.\n        \n        Parameters\n        ----------\n        input_shape : Tuple[int, int], \n            Shape of the input to the model without accounting for batch size.\n        window_shape : float, optional\n            Shape parameter of the Tukey temporal window. The default is 0.2.\n        trainable_window : bool, optional\n            Whether the Tukey temporal window should be trained or not. \n            The default is False.\n        sample_rate : float, optional\n            The sampling rate for the input time series. The default is 2048.\n        degree_filt : int, optional\n            Degree of the bandpass filter. The default is 8.\n        f_band_filt : Tuple[float, float], optional\n            The frequency band for the bandpass filter [Hz]. The default \n            is (20, 500).\n        trainable_filt : bool, optional\n            Whether the bandpass filter should be trained or not. \n            The default is False.\n        hop_length : int, optional\n            The hop (or stride) size for the CQT layer. The default is 512.\n        f_band_spec : Tuple[float, float], optional\n            The frequency for the lowest (f_min) and highest (f_max) CQT bins [Hz]. \n            The default is (20, 500).\n        bins_per_octave : int, optional\n            Number of bins per octave for the CQT layer. The default is 12.\n        window_cqt : str, optional\n            The windowing function for CQT. The default is \"hann\".\n        perc_range : float, optional\n            Extra range to apply to tracked spectrogram output maximum to \n            leave a safe margin for non-seen examples. The default is 0.05.\n        trainable_cqt : bool, optional\n            Whether the cqt layer should be trained or not. If transfer learning \n            is applied, the recommendation is to freeze this layer during the \n            first epochs and activate its training afterwards. The default is False.\n        resize_shape : Tuple[int, int], optional\n            Spectrogram resize shape without including batch size and channels. \n            The default is (128, 128).\n        p_perm : float, optional\n            Probability of performing a channel permutation for regularisation.\n            The default is 0.1.\n        p_mask : foat, optional\n            Probability of performing spectral mask for regularisation. The \n            default is 0.1.\n        n_max_mask_t : int, optional\n            Maximum number of masks in time dimension. The default is 2.\n        w_mask_t : Tuple[int, int], optional\n            Minimum and maximum width of masking bands in the time dimension. \n            The default is (12, 25).\n        n_max_mask_f : int, optional\n            Maximum number of masks in frequency dimension. The default is 2.\n        w_mask_f : Tuple[int, int], optional\n            Minimum and maximum width of masking bands in the frequency dimension. \n            The default is (12, 25).\n        dtype : type, optional\n            Data type of the model layer parameters. The default is tf.float32.\n        strategy : str, optional\n            In use strategy. It is mainly used to switch to layers compatible \n            with XLA when using TPU. The default is \"GPU\".\n            Available options are:\n                - \"TPU\"\n                - \"GPU\"\n                - \"CPU\"\n        effnet_id : str, optional\n            Id of the efficientnet backend model to use. The default is \"efficientnetv2-b2\".\n            Available options are:\n                - \"efficientnetv2-s\"\n                - \"efficientnetv2-m\"\n                - \"efficientnetv2-l\"\n                - \"efficientnetv2-xl\"\n                - \"efficientnetv2-b0\"\n                - \"efficientnetv2-b1\"\n                - \"efficientnetv2-b2\"\n                - \"efficientnetv2-b3\"\n                - \"efficientnet-b0\"\n                - \"efficientnet-b1\"\n                - \"efficientnet-b2\"\n                - \"efficientnet-b3\"\n                - \"efficientnet-b4\"\n                - \"efficientnet-b5\"\n                - \"efficientnet-b6\"\n                - \"efficientnet-b7\"\n                - \"efficientnet-b8\"\n                - \"efficientnet-l2\"\n        weights : str, optional\n            Whether to use weights from pre-trained models or not. The default \n            is \"imagenet\". Available options are:\n                - \"imagenet\"\n                - \"imagenet21k\"\n                - \"imagenet21k-ft1k\"\n                - \"jft\"\n        learning_rate : float, optional\n            Learning rate for the optimizer. The default is 0.001.\n        \"\"\"\n\n        inp = Input(shape = input_shape, dtype = dtype, name = \"input\")\n\n        if strategy == \"TPU\":\n            window = WindowingLayer(window = (\"tukey\", window_shape),\n                                    window_len = input_shape[0],\n                                    trainable = trainable_window, name = \"window\")\n        else:\n            window = TukeyWinLayer(initial_alpha = window_shape, \n                                   trainable = trainable_window, name = \"window\")\n\n        bandpass = BandpassLayer(sample_rate = sample_rate, degree = degree_filt, \n                                      f_band = f_band_filt, n_samples = input_shape[0], \n                                      trainable = trainable_filt, name = \"bandpass\")\n        \n        tpu = True if strategy == \"TPU\" else False\n        cqt = CQTLayer(sample_rate = sample_rate, hop_length = hop_length, \n                       f_band = f_band_spec, bins_per_octave = bins_per_octave,\n                       window = window_cqt, trainable = trainable_cqt, \n                       perc_range = perc_range, tpu = tpu, name = \"cqt\")\n\n        resize = Resizing(resize_shape[0], resize_shape[1], name = \"resize\")\n        permute = PermuteChannel(p = p_perm, name = \"permute\")\n        \n        if strategy == \"TPU\":\n            mask_t = TimeMask(p = p_mask, w_mask = w_mask_t, name = \"mask_t\")\n            mask_f = FreqMask(p = p_mask, w_mask = w_mask_f, name = \"mask_f\")\n        else:\n            mask = SpectralMask(p = p_mask, n_max_mask_t = n_max_mask_t,\n                                w_mask_t = w_mask_t, n_max_mask_f = n_max_mask_f,\n                                w_mask_f = w_mask_f, name = \"mask\")\n\n        flatten = Flatten(name = \"flatten\")\n        dense = Dense(units = 1, activation = \"sigmoid\", name = \"dense\")\n\n        effnet_config = copy.deepcopy(hparams.base_config)\n        effnet_config.override(effnetv2_configs.get_model_config(effnet_id))\n        if strategy == \"TPU\" and not effnet_config.model.bn_type:\n            effnet_config.model.bn_type = \"tpu_bn\"\n\n        effnet = effnetv2_model.get_model(model_name = effnet_id,\n            model_config = effnet_config.model, include_top = False, \n            weights = weights)\n\n        x = inp\n\n        y = window(x)\n        y = bandpass(y)\n        y = cqt(y)\n        y = resize(y)\n        y = permute(y)\n\n        if strategy == \"TPU\":\n            for _ in range(n_max_mask_t):\n                y = mask_t(y)\n            for _ in range(n_max_mask_f):\n                y = mask_f(y)\n        else:\n            y = mask(y)\n\n        y = effnet(y)\n        y = flatten(y)\n        y = dense(y)\n        model = tf.keras.Model(inputs = [x], outputs = [y])\n        \n        optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n        # optimizer = AdaBeliefOptimizer(learning_rate = learning_rate, \n        #                                amsgrad = False, print_change_log = False)\n        loss = tf.keras.losses.BinaryCrossentropy()\n        metric = tf.keras.metrics.AUC()\n        model.compile(optimizer = optimizer, loss = loss, metrics = [metric])\n        \n        return model","6d787aa3":"# Prepare original dataframes \nsub_file = INP_PATH.joinpath(\"g2net-float32-test\", \"sample_submission.csv\")\ntrain_labels_file = INP_PATH.joinpath(\"g2net-float32-train\", \"training_labels.csv\")\n\ntrain_df_ori = pd.read_csv(train_labels_file)\ntest_df_ori = pd.read_csv(sub_file)\n\ntrain_df = pd.DataFrame([Path(x).stem for x in tf.io.gfile.glob(TRAIN_PATH + \"\/*.tfrec\")], columns = [\"id\"])\ntrain_df = train_df.sample(frac = 1, random_state = Config.SEED_SPLIT).reset_index(drop = True)\ntest_df = pd.DataFrame([Path(x).stem for x in tf.io.gfile.glob(TEST_PATH + \"\/*.tfrec\")], columns = [\"id\"])\n\nn_split = np.int32(train_df.shape[0] * Config.SPLIT)\ntraining_df = train_df.loc[:n_split - 1, :]\nvalidation_df = train_df.loc[n_split:, :]\n        \ntraining_gen = DatasetGeneratorTF(training_df, TRAIN_PATH, batch_size = Config.BATCH_SIZE, dtype = dtype)\nvalidation_gen = DatasetGeneratorTF(validation_df, TRAIN_PATH, batch_size = Config.BATCH_SIZE, dtype = dtype)\ntest_gen = DatasetGeneratorTF(test_df, TEST_PATH, batch_size = Config.BATCH_SIZE_TEST, dtype = dtype)\n    \ntraining_ds = training_gen.get_dataset(buffer_size = 2048)\nvalidation_ds = validation_gen.get_dataset(buffer_size = 2048)\ntest_ds = test_gen.get_dataset(shuffle = False, repeat = False, target = False)\n\n# Estimate number of steps per train, validation and test sets\nns_training = np.int32(train_df_ori.shape[0] * Config.SPLIT)\nns_validation = train_df_ori.shape[0] - ns_training\nns_test = test_df_ori.shape[0]\nspe_training = np.int32(np.ceil(ns_training \/ Config.BATCH_SIZE))\nspe_validation = np.int32(np.ceil(ns_validation \/ Config.BATCH_SIZE))\nspe_test = np.int32(np.ceil(ns_test \/ Config.BATCH_SIZE_TEST))\n\ntest_ds_id = test_ds.map(lambda data, identity: tf.strings.unicode_encode(\n    identity, \"UTF-8\"))\ntest_ds_id = test_ds_id.unbatch()\ntest_ids = next(iter(test_ds_id.batch(test_df_ori.shape[0]))).numpy().astype(\"U\")\ntest_ds = test_ds.map(lambda data, identity: data)","4042f24e":"# Create model, compile and display summary within the scope of the \n# distribution strategy\ntf.keras.backend.clear_session()\nwith strategy.scope():\n    model = build_model(input_shape = (Config.N_SAMPLES, Config.N_DETECT), \n                        window_shape = Config.TUKEY_SHAPE,\n                        trainable_window = Config.TRAINABLE_TUKEY, \n                        sample_rate = Config.SAMPLE_RATE,\n                        degree_filt = Config.DEGREE_FILT, \n                        f_band_filt = Config.F_BAND_FILT,\n                        trainable_filt = Config.TRAINABLE_FILT, \n                        hop_length = Config.HOP_LENGTH,\n                        f_band_spec = Config.F_BAND_SPEC, \n                        bins_per_octave = Config.BINS_PER_OCTAVE,\n                        window_cqt = Config.WINDOW_CQT, \n                        resize_shape = (Config.IMAGE_SIZE, Config.IMAGE_SIZE),\n                        p_perm = Config.P_PERM, p_mask = Config.P_MASK,\n                        n_max_mask_t = Config.N_MAX_MASK, \n                        w_mask_t = Config.W_MASK, \n                        n_max_mask_f = Config.N_MAX_MASK,\n                        w_mask_f = Config.W_MASK, \n                        strategy = device, weights = CKT_PATH, \n                        effnet_id = Config.MODEL_ID, \n                        learning_rate = Config.LEARNING_RATE)\n\nmodel.summary()","58c8d448":"if Config.MODEL_PRELOAD:\n    pretrained_model = Config.MODEL_PATH.joinpath(Config.MODEL_PRELOAD_NAME)\n    local = tf.train.CheckpointOptions(experimental_io_device = \"\/job:localhost\")\n    if tf.io.gfile.isdir(pretrained_model):\n        pretrained_model = tf.train.latest_checkpoint(pretrained_model)\n    model.load_weights(pretrained_model, options = local)","628cd4f5":"# Train model with training and validation sets with checkpoints and control \n# over training validation loss plateaus\nif Config.MODEL_TRAIN:\n    lr_callback = tf.keras.callbacks.ReduceLROnPlateau(patience = 1, monitor = \"val_loss\", cooldown = 0, verbose = 1)\n    \n    local = tf.train.CheckpointOptions(experimental_io_device='\/job:localhost')\n    check_callback = tf.keras.callbacks.ModelCheckpoint(\n            filepath = Config.CKPT_PATH.joinpath(\"ckpt-{epoch:d}\"),\n            save_weights_only = True,\n            monitor = \"val_auc\",\n            mode = \"max\",\n            save_best_only = False,\n            options = local)\n    \n    train_history = model.fit(training_ds, epochs = Config.EPOCHS, batch_size = Config.BATCH_SIZE, \n                              validation_data = validation_ds, steps_per_epoch = spe_training,\n                              validation_steps = spe_validation, callbacks = [lr_callback, check_callback])\n\n    #best_model_path = tf.train.latest_checkpoint(Config.CKPT_PATH)\n    #model.load_weights(best_model_path, options = local)\n\n    Config.MODEL_PATH.mkdir(parents = True, exist_ok = True)\n    model.save_weights(Config.MODEL_PATH.joinpath(Config.MODEL_SAVE_NAME), options = local)\n\n    train_hist_df = pd.DataFrame(train_history.history)\n    train_hist_df.to_csv(Config.HISTORY_NAME, index = False)","46bf72aa":"# Predict on test set and save to submission file\nif Config.MODEL_PREDICT:\n    preds_test = model.predict(test_ds, batch_size = Config.BATCH_SIZE_TEST, \n                               steps = spe_test, verbose = 1)\n\n    sub_df = pd.DataFrame({\n        \"id\": test_ids,\n        \"target\": preds_test.flatten()\n    })\n\n    sub_df = sub_df.sort_values(\"id\").reset_index(drop = True)\n        \n    sub_df.to_csv(Config.PREDICTIONS_NAME, index = False)\n\n    plt.style.use(\"seaborn\")\n    print()\n    print(\"Test dataset output distribution\")\n    sns.displot(sub_df, x = \"target\", kind = \"kde\")","e5bbefaf":"if Config.PLOT_EXAMPLE:\n\n    permute = model.get_layer(\"permute\")\n    window = model.get_layer(\"window\")\n    bandpass = model.get_layer(\"bandpass\")\n    cqt = model.get_layer(\"cqt\")\n    resize = model.get_layer(\"resize\")\n    permute = model.get_layer(\"permute\")\n    mask_t = model.get_layer(\"mask_t\")\n    mask_f = model.get_layer(\"mask_f\")\n    effnet = model.get_layer(Config.MODEL_ID)\n    flatten = model.get_layer(\"flatten\")\n    dense = model.get_layer(\"dense\")\n        \n    if Config.PLOT_TEST:\n        for data, _ in test_ds.take(1):\n            x_ref = data.numpy()[0, ...][np.newaxis, ...]\n    else:\n        for data, _ in training_ds.take(1):\n            x_ref = data.numpy()[0, ...][np.newaxis, ...]\n\n    x = np.squeeze(x_ref)\n    y = x_ref\n        \n    y = window(y)\n    y_win = np.squeeze(y.numpy())\n    y = bandpass(y)\n    y_band = np.squeeze(y.numpy())\n    y = cqt(y, training = False)\n    y = resize(y)\n    y_spec = np.squeeze(y.numpy())\n    y = permute(y, training = True)\n\n    for _ in range(Config.N_MAX_MASK):\n        y = mask_t(y, training = True)\n    for _ in range(Config.N_MAX_MASK):\n        y = mask_f(y, training = True)\n\n    y_masked = np.squeeze(y.numpy())\n    y = effnet(y)\n    y = flatten(y)\n    y = dense(y)\n    y_dense = np.squeeze(y.numpy())","43a69bfb":"if Config.PLOT_EXAMPLE:\n    print(\"Standardised signals\")\n    PlottingUtilities.plot_wave(x)","8254c9b9":"if Config.PLOT_EXAMPLE:\n    print(\"Tappered signals\")\n    PlottingUtilities.plot_wave(y_win)","4df62ba8":"if Config.PLOT_EXAMPLE:\n    print(\"Filtered signals\")\n    PlottingUtilities.plot_wave(y_band)","3d8e5aa8":"if Config.PLOT_EXAMPLE:\n    print(\"CQT spectrogram\")\n    PlottingUtilities.plot_spectrogram(y_spec)","5f2d09b6":"if Config.PLOT_EXAMPLE:\n    print(\"Masked CQT spectrogram\")\n    PlottingUtilities.plot_spectrogram(y_masked)","c6707dbb":"\nNow it is high time to start with the Python specifics. Run the following cell to import the necessary libraries for the code to work.","34223746":"## 2. Configuration","4e6b2822":"Not only a network of Gravitational Waves, Geophysics and Machine Learning experts, G2Net was also released as a Kaggle competition. I'm pretty sure that you have heard about the discovery of Gravitational Waves (GW), signals from colliding binary black holes, back in 2015. If not, you can refresh your memory with [The Sound of Two Black Holes Colliding](https:\/\/www.youtube.com\/watch?v=QyDcTbR-kEA). \n\nThe aim of this competition is to detect GW signals from the mergers of binary black holes. Specifically, the participant was expected to build a model to analyse synthetic GW time-series data from a network of Earth-based detectors (LIGO Hanford, LIGO Livingston and Virgo). The [Data](https:\/\/www.kaggle.com\/c\/g2net-gravitational-wave-detection\/data) was simulated with a sampling rate of 2048 Hz. Each of the time-series originated by the corresponding detector comprises a channel (three in total).\n\nIn the context of this notebook, data has been already standardised (with training set mean and standard deviation), transposed (to ease channels last format) and saved to TensorFlow Records format. Such data can be found in my Kaggle profile, [Training Dataset](https:\/\/www.kaggle.com\/salbeal94\/g2net-float32-train) and [Test Dataset](https:\/\/www.kaggle.com\/salbeal94\/g2net-float32-test). Since it is a classification task, the output is the black hole merger occurence probability. As per the evaluation\/validation metric, it is the AUC score.","3d9e6389":"\nThe best G2Net single model was obtained with trainable Tukey window and bandpass layers, a non-trainable CQT, a resize size of 384, an EfficientNet v2 S backbone and training in a Tesla V100 (16GB) GPU with a batch size of 32 (learning rates in the range 0.0003 to 0.00001. The score was boosted with an averaging ensemble of several models with different settings. \n\nThe ROC & Roll team encourages you to play with the code, propose improvements and even introduce new layers that might be of use!","622c346b":"### 1.1 Problem Description","f1f57166":"Now let's try to break down the model into its constituent parts and plot the intermediate data for a single example. You might see that, depending on the example, visually identifying a merger chirp becomes difficult or even impossible mainly due other low and high frequency sources of noise (in this case simulated).","f84fdaa6":"Now we are ready to build the model within the strategy scope (TPU), preload model weights from local storage (weights other than those provided by AutoML) and train it.","714377ae":"### 2.2 Code Configuration ","a4a6cf6f":"Run the next cell to tune the paths and add G2Net source code and AutoML EfficientNet model paths to Python path.","c00a81b9":"## 1. Introduction","71072fff":"To make use of TPUs, data ingested by the model should be stored in Google Cloud Storage (GCS) buckets. Fortunately, Kaggle is so integrated with Google that public datasets get uploaded to Google Cloud. The corresponding addresses should be checked from time to time as they are periodically changed. To know their current bucket addresses one should just turn to ```None``` the strings below and run the following:","7282a940":"### 2.1 Environment Configuration","3b3eb7d4":"The following cell automatically configures the TensorFlow strategy and device (relevant for TPU) and sets the data type to use.","74dd7bdd":"At this point, it only remains to prepare the datasets with the classes defined above before proceding with training. Of note is that a small proportion of the training dataset is stripped and used for validation purposes. Since each TF Record contains several examples (each with ID, time-domain signals and label if any), one might lose track of the number of examples in training and test sets. Actually, it needs to be known to figure out the number of steps per epoch. Such information is contained in two CSV from the training and test input folders.","790cc75b":"### 3.1 Custom Model","ee6e380f":"![G2Net Model](https:\/\/github.com\/salvaba94\/G2Net\/blob\/main\/img\/Model.png?raw=true \"G2Net Model\")","7c9140bc":"### 3.2 Model Training","471c3fc1":"Before starting with implementation-specific details, let's configure some aspects of the environment: \n\n- Make sure the Kaggle environment type is set to TPU going to ```Accelerator \u2192 TPU vX-Y```\n- Install any library that might be missing from the defaults.\n\n","4ca452f8":"### 1.2 Implemented Model","b9cd8e88":"### 3.3 Predicting with the Model","11b436c8":"Once you are happy with the model, it is time to make predictions on the test set and explore a bit what is actually doing. First, try to obtain the predictions for the full test dataset and plot the output distribution.","587b896e":"After many iterations, the model used for the competition ended up being a 2D Convolutional Neural Network (CNN) preceded by a series of time-series processing techniques. Even the preprocessing has been here implemented as part of the model to stay loyal to the end-to-end philosophy. Such a model contains a series of building blocks several of them presented as trainable Tensorflow Keras layers. These are described as follows:\n- **Tukey Window (trainable\/non-trainable)**: Introduces a tappering effect that forces the signal amplitude to decay until having zero values at the ends. It is applied to avoid artefacts stemming from discontinuities when taking Fourier transforms or similar.\n\n- **Bandpass Filter (trainable\/non-trainable)**: Applies a filter with the frequency response of a Butterworth filter. The idea is to filter out or attenuate frequencies that have nothing to do with the merger.\n\n- **Constant-Q Transform (trainable\/non-trainable)**: Transforms the time-domain signal into the time-frequency domain. In other words, it converts a time-domain signal to a spectrogram. Particularly, the PyTorch CQT1992v2 implementation from [nnAudio](https:\/\/github.com\/KinWaiCheuk\/nnAudio) has been taken as reference. It has been re-implemented in TensorFlow for being one of the most cost-effective solutions offered by the library. As an additional functionality, the layer keeps track of maximum spectrogram magnitudes and normalises its values to a preset range for the sake of stability. The output spectrogram is later resized with bilinear interpolation to adapt it to the downstream CNN recommended input sizes.\n\n- **Channel Permutation (non-trainable)**: Randomly decides whether to apply a stochastic permutation of the channels. The aim is to make the prediction a bit less independent of the detector it comes from, pressumably acting as a regularisation layer.\n\n- **Spectral Masking (non-trainable)**: Randomly decides whether to stochastically mask certain time or frequency bands. Similar to the permutation, the idea is for this layer to act as a regularising operation.\n\n- **Convolutional Neural Network (trainable\/non-trainable)**: Used as backbone to extract the relevant features to be ingested by a single fully-connected neuron with sigmoid activation (after flattening). Given its performance-complexity trade-off in the ImageNet dataset, the EfficientNet family from [AutoML](https:\/\/github.com\/google\/automl) was selected as a more than appropriate model for this purpose.","e4ab1d80":"Set the configuration variables necessary for the code to run. Here is where you should come if you would like to play with the model. Don't worry if some of the variables are not clear enough at this point. Their utility can be derived as these are used all along the code.\n","926dd982":"The building blocks of the model are already implemented in the cloned G2Net source code. Run the following piece of code to define a function that builds and compiles the model. Pay attention to the fact that the model is compiled in the scope of the function to avoid problems with TPU.","c3e2a43c":"## 3. Source Code","b39fe2ed":"\n# G2Net Playground TPU\n\n","b8a63f03":"This notebook has been created to serve as a guided tour to training G2Net models by using Kaggle TPU. It makes use of the code in the project-like tree uploaded to my [GitHub](https:\/\/github.com\/salvaba94\/G2Net), a code that rocketed the team (ROC & Roll) to top 8% and a bronze medal in its first competition. With all that said, let's ROC it! (Hope you get the joke)"}}