{"cell_type":{"f45709e9":"code","635d3bf5":"code","a4c79011":"code","0c5ae734":"code","56a757d8":"code","adcbd9a6":"code","505682f4":"code","23bf84f1":"code","c314db6f":"code","9336537b":"code","c10e47c2":"code","f78d06c8":"code","a3a6c492":"code","5d022353":"code","56ecacfc":"code","b1bf5c72":"code","e66b377b":"code","619ce374":"code","819122d0":"code","fa8eadec":"code","fb369b56":"code","7350f7ca":"code","087d5abb":"code","9540a0d8":"code","6176fea6":"code","3b996b8d":"code","7cbdd72e":"code","52a98ff6":"code","cab2f52c":"code","a9f9723e":"code","e3cbfb52":"code","6ad39110":"code","5b89db05":"code","9bb72a7c":"code","074edb20":"code","f923a3ce":"code","7e43fd7f":"code","527cfcc6":"code","7de86ddf":"code","2cd60e7c":"code","b36bc84a":"code","00f71663":"code","979c39bf":"code","b5039065":"code","8c09689b":"code","1d4cc844":"code","7cc855e3":"code","bf368473":"code","92e4b2e4":"code","06e0a16f":"code","46376e60":"code","89ef653c":"code","e512c9bf":"code","8045e5eb":"code","f6e9f78e":"code","005af570":"code","2a52d804":"code","d9573fe1":"markdown","d05f60c3":"markdown","a506a546":"markdown","6f56f752":"markdown","ed6554e8":"markdown","0d5c5226":"markdown","724cc2e2":"markdown","892827f8":"markdown","ed37b2cb":"markdown","e739cee3":"markdown","58072acb":"markdown","fc28a42d":"markdown","951db572":"markdown","870e9393":"markdown","768044c8":"markdown","f635947e":"markdown","99e188f4":"markdown","9267b5ab":"markdown","e838fbf1":"markdown","e3132c56":"markdown","5a26ce25":"markdown","ca1577c9":"markdown","8f9985d1":"markdown","02d5cdd0":"markdown","4e9a2764":"markdown","93aa0f51":"markdown","c71d9f60":"markdown"},"source":{"f45709e9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","635d3bf5":"# Importando os dados\ntrain = pd.read_csv('\/kaggle\/input\/big-mart-sales-prediction-datasets\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/big-mart-sales-prediction-datasets\/test.csv')\ntest2 = test.copy()\ntrain.shape, test.shape","a4c79011":"# Tipos:\ntrain.info()","0c5ae734":"# Valores \u00fanicos de cada coluna:\ntrain.nunique()","56a757d8":"# Quais s\u00e3o os valores nulos:\ntrain.isna().sum()","adcbd9a6":"test.isna().sum()","505682f4":"#Imputando valores das notas a partir da m\u00e9dia\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n\nimputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\nimputer.fit(test[['Item_Weight']])\ntest['Item_Weight'] = imputer.transform(test[['Item_Weight']]).ravel()","23bf84f1":"# Criando um dicion\u00e1rio para mapear todos os registros em 'low' ou 'regular':\nitem_fat = {'Low Fat':'low', 'Regular':'regular', 'LF':'low', 'reg':'regular','low fat':'low'}\n\ntrain['Item_Fat_Content'] = train['Item_Fat_Content'].map(item_fat)\ntest['Item_Fat_Content'] = test['Item_Fat_Content'].map(item_fat)\n\n# Verificando\ntrain['Item_Fat_Content'].unique()","c314db6f":"# Item_Fat_Content:\ntrain['Item_Fat_Content'].unique()","9336537b":"# Verificando Outlet_Size\ntrain['Outlet_Size'].unique()","c10e47c2":"# Verificando Outlet_Location_Type\ntrain['Outlet_Location_Type'].unique()","f78d06c8":"# Verificando Outlet_Type\ntrain['Outlet_Type'].unique()","a3a6c492":"# Verificando Item_Type\ntrain['Item_Type'].unique()","5d022353":"# Verificando Outlet_Identifier\ntrain['Outlet_Identifier'].unique()","56ecacfc":"# Quantos valores \u00fanicos de Item_Identifier\ntrain['Item_Identifier'].nunique()","b1bf5c72":"# Tratando 'Item_Weight' e 'Outlet_Size':\n\n# Vamos usar a m\u00e9dia para imputar valores em Item_Weight\ntrain['Item_Weight'].fillna(train['Item_Weight'].mean(), inplace=True)\n\n# Vamos usar a moda para imputar valores em Outlet_Size\ntrain['Outlet_Size'].fillna(train['Outlet_Size'].mode()[0], inplace=True)","e66b377b":"# Vari\u00e1veis categ\u00f3ricas:\ntrain.dtypes","619ce374":"test.columns","819122d0":"test.dtypes","fa8eadec":"# Codificando as colunas categ\u00f3ricas usando one hot encoding:\ncat_cols = ['Item_Fat_Content', 'Item_Type', 'Outlet_Identifier', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']\n\ntrain = pd.get_dummies(train, columns=cat_cols)\ntest = pd.get_dummies(test, columns=cat_cols)\n\ntrain.shape","fb369b56":"# Resultado final:\ntrain.head()","7350f7ca":"# Verificando as colunas\ntrain.info()","087d5abb":"# Separando o dataframe\nfrom sklearn.model_selection import train_test_split\n\ntrain, valid = train_test_split(train, random_state=42)\n\ntrain.shape, valid.shape","9540a0d8":"# Obtendo as colunas para treinamento\nfeatures = [c for c in train.columns if c not in ['Item_Identifier', 'Item_Outlet_Sales']]\n\nfeatures","6176fea6":"# RandomForest:\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf_padrao = RandomForestRegressor(n_estimators=200, n_jobs=-1, random_state=42)\n\nrf_padrao.fit(train[features], train['Item_Outlet_Sales'])","3b996b8d":"# Predi\u00e7\u00e3o:\npreds_rf = rf_padrao.predict(valid[features])\npreds_rf","7cbdd72e":"# Calculando a m\u00e9trica:\nfrom sklearn.metrics import mean_squared_error\n\nrmse_rf = mean_squared_error(valid['Item_Outlet_Sales'], preds_rf, squared=False)\n\nrmse_rf","52a98ff6":"# GBR:\nfrom sklearn.ensemble import GradientBoostingRegressor\n\ngbr_padrao = GradientBoostingRegressor(n_estimators=200, random_state=42)\n\ngbr_padrao.fit(train[features], train['Item_Outlet_Sales'])\n\npreds_gbr = gbr_padrao.predict(valid[features])\n\nrmse_gbr = mean_squared_error(valid['Item_Outlet_Sales'], preds_gbr, squared=False)\n\nrmse_gbr","cab2f52c":"# AdaBoost:\nfrom sklearn.ensemble import AdaBoostRegressor\n\nada_padrao = AdaBoostRegressor(n_estimators=200, random_state=42)\n\nada_padrao.fit(train[features], train['Item_Outlet_Sales'])\n\npreds_ada = ada_padrao.predict(valid[features])\n\nrmse_ada = mean_squared_error(valid['Item_Outlet_Sales'], preds_ada, squared=False)\n\nrmse_ada","a9f9723e":"# Regress\u00e3o Linear:\n\nfrom sklearn.linear_model import LinearRegression\n\nlr_padrao = LinearRegression()\n\nlr_padrao.fit(train[features], train['Item_Outlet_Sales'])\n\npreds_lr = lr_padrao.predict(valid[features])\n\nrmse_lr = mean_squared_error(valid['Item_Outlet_Sales'], preds_lr, squared=False)\n\nrmse_lr","e3cbfb52":"# BaggingRegressor:\n\nfrom sklearn.ensemble import  BaggingRegressor \n\nbgr_padrao = BaggingRegressor()\n\nbgr_padrao.fit(train[features], train['Item_Outlet_Sales'])\n\npreds_bgr = bgr_padrao.predict(valid[features])\n\nrmse_bgr = mean_squared_error(valid['Item_Outlet_Sales'], preds_bgr, squared=False)\n\nrmse_bgr","6ad39110":"# VotingRegressor:\nfrom sklearn.ensemble import VotingRegressor\n\nestimators = [('rf_padrao', rf_padrao),('gbr_padrao',gbr_padrao),('ada_padrao', ada_padrao),('lr_padrao', lr_padrao),('bgr_padrao', bgr_padrao)]\n\nensemble1 = VotingRegressor(estimators=estimators, n_jobs=-1)\n\nensemble1.fit(train[features], train['Item_Outlet_Sales'])\n\npreds_ens = ensemble1.predict(valid[features])\n\nrmse_ens = mean_squared_error(valid['Item_Outlet_Sales'], preds_ens, squared=False)\n\nrmse_ens","5b89db05":"# Modelo 1:\npreds_rf = rf_padrao.predict(test[features])\npreds_rf","9bb72a7c":"# Modelo 2:\npreds_gbr = gbr_padrao.predict(test[features])\npreds_gbr","074edb20":"# Modelo 3:\npreds_ada = ada_padrao.predict(test[features])\npreds_ada","f923a3ce":"# Modelo 4:\npreds_lr = lr_padrao.predict(test[features])\npreds_lr","7e43fd7f":"# Modelo 5:\npreds_bgr = bgr_padrao.predict(test[features])\npreds_bgr","527cfcc6":"# Voting Regressor:\npreds_ens = ensemble1.predict(test[features])\npreds_ens","7de86ddf":"# Modelo 1:\ndf_rf = test2[['Item_Identifier','Outlet_Identifier']].copy()\ndf_rf['Item_Outlet_Sales'] = preds_rf\ndf_rf.to_csv('predict_rf.csv')\ndf_rf.head()","2cd60e7c":"# Modelo 2:\ndf_gbr = test2[['Item_Identifier','Outlet_Identifier']].copy()\ndf_gbr['Item_Outlet_Sales'] = preds_gbr\ndf_gbr.to_csv('predict_gbr.csv')\ndf_gbr.head()","b36bc84a":"# Modelo 3:\ndf_ada = test2[['Item_Identifier','Outlet_Identifier']].copy()\ndf_ada['Item_Outlet_Sales'] = preds_ada\ndf_ada.to_csv('predict_ada.csv')\ndf_ada.head()","00f71663":"# Modelo 4:\ndf_lr= test2[['Item_Identifier','Outlet_Identifier']].copy()\ndf_lr['Item_Outlet_Sales'] = preds_lr\ndf_lr.to_csv('predict_lr.csv')\ndf_lr.head()","979c39bf":"# Modelo 5:\ndf_bgr= test2[['Item_Identifier','Outlet_Identifier']].copy()\ndf_bgr['Item_Outlet_Sales'] = preds_bgr\ndf_bgr.to_csv('predict_bgr.csv')\ndf_bgr.head()","b5039065":"# Voting Regressor:\n\ndf_ens = test2[['Item_Identifier','Outlet_Identifier']].copy()\ndf_ens['Item_Outlet_Sales'] = preds_ens\ndf_ens = df_ens.set_index('Item_Identifier')\ndf_ens.to_csv('predict_ens.csv')\ndf_ens.head()","8c09689b":"!pip install pycaret==2.3.3","1d4cc844":"from pycaret.regression import *","7cc855e3":"setup1 = setup(data = train, \n               target = 'Item_Outlet_Sales',\n               ignore_features=['Item_Identifier'],\n               normalize = True,\n               silent = True)","bf368473":"models()","92e4b2e4":"# Modelo\nlr = create_model('lr')\ntuned_lr = tune_model(lr, optimize = 'MSE')\nevaluate_model(tuned_lr)","06e0a16f":"# Predi\u00e7\u00e3o\npred_lr_automl = predict_model(tuned_lr, data = test)","46376e60":"# Exportando para csv\ndf_lr_automl = test2[['Item_Identifier','Outlet_Identifier']].copy()\ndf_lr_automl['Item_Outlet_Sales'] = pred_lr_automl['Label'].abs()\ndf_lr_automl.to_csv('predict_lr_automl.csv',index=False)\ndf_lr_automl.head()","89ef653c":"# Modelo\nlasso = create_model('lasso')\ntuned_lasso = tune_model(lasso, optimize = 'MSE')\nevaluate_model(tuned_lasso)","e512c9bf":"# Predi\u00e7\u00e3o\npred_lasso_automl = predict_model(tuned_lasso, data = test)","8045e5eb":"# Exportando para csv\ndf_lasso_automl = test2[['Item_Identifier','Outlet_Identifier']].copy()\ndf_lasso_automl['Item_Outlet_Sales'] = pred_lasso_automl['Label'].abs()\ndf_lasso_automl.to_csv('predict_lasso_automl.csv',index=False)\ndf_lasso_automl.head()","f6e9f78e":"# Modelo\nridge = create_model('ridge')\ntuned_ridge = tune_model(ridge, optimize = 'MSE')\nevaluate_model(tuned_ridge)","005af570":"# Predi\u00e7\u00e3o\npred_ridge_automl = predict_model(tuned_ridge, data = test)","2a52d804":"# Exportando para csv\ndf_ridge_automl = test2[['Item_Identifier','Outlet_Identifier']].copy()\ndf_ridge_automl['Item_Outlet_Sales'] = pred_ridge_automl['Label'].abs()\ndf_ridge_automl.to_csv('predict_ridge_automl.csv',index=False)\ndf_ridge_automl.head()","d9573fe1":"# Ridge Regression em AutoML\nridge\tRidge Regression\tsklearn.linear_model._ridge.Ridge","d05f60c3":" **Resultado:**\n Os valores nulos foram tratados atrav\u00e9s da m\u00e9dia e da moda, as colunas categ\u00f3ricas foram codificadas com o one hot encoding e o item fat foi ajustado em regular e low.","a506a546":"O modelo 5 se trata de Bagging regressor, \u00e9 um metaestimador de conjunto que ajusta cada um dos regressores de base em subconjuntos aleat\u00f3rios do conjunto de dados original e, em seguida, agrega suas previs\u00f5es individuais (por vota\u00e7\u00e3o ou por m\u00e9dia) para formar uma previs\u00e3o final. Esse metaestimador pode normalmente ser usado como uma forma de reduzir a vari\u00e2ncia de um estimador de caixa preta (por exemplo, uma \u00e1rvore de decis\u00e3o), introduzindo a aleatoriza\u00e7\u00e3o em seu procedimento de constru\u00e7\u00e3o e, em seguida, fazendo um conjunto a partir dela.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.BaggingRegressor.html","6f56f752":"O modelo 1 se trata de um random forest, que \u00e9 um metaestimador que ajusta v\u00e1rias \u00e1rvores de decis\u00e3o de classifica\u00e7\u00e3o em v\u00e1rias subamostras do conjunto de dados e usa a m\u00e9dia para melhorar a precis\u00e3o preditiva e o sobreajuste de controle.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html","ed6554e8":"**Quais os problemas \u00e1 serem resolvidos?**\n* A vari\u00e1vel Item_Weight tem 1463 valores nulos e a Outlet_Size tem 2410 valores nulos;\n* Item_Fat_Content tem valores em excesso;\n* As vari\u00e1veis categ\u00f3ricas precisam ser codificadas;","0d5c5226":"# Linear Regression em AutoML\n\nlr\tLinear Regression\tsklearn.linear_model._base.LinearRegression","724cc2e2":"O modelo 3 se trata de um Ada Boost, que \u00e9 um metaestimador que come\u00e7a ajustando um regressor no conjunto de dados original e, em seguida, ajusta c\u00f3pias adicionais do regressor no mesmo conjunto de dados, mas onde os pesos das inst\u00e2ncias s\u00e3o ajustados de acordo com o erro da previs\u00e3o atual. Como tal, os regressores subsequentes se concentram mais nos casos dif\u00edceis.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostRegressor.html","892827f8":"# **3 - Escolher e testar 5 modelos:**","ed37b2cb":"# **1 - An\u00e1lise explorat\u00f3ria:**","e739cee3":"# Lasso Regression em AutoML\nlasso\tLasso Regression\tsklearn.linear_model._coordinate_descent.Lasso","58072acb":"# **P1**","fc28a42d":"O modelo 4 se trata de um LinearRegression, ele ajusta um modelo linear com coeficientes w = (w1, ..., wp) para minimizar a soma residual dos quadrados entre os alvos observados no conjunto de dados e os alvos previstos pela aproxima\u00e7\u00e3o linear.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LinearRegression.html","951db572":"# **Modelo 5**","870e9393":"# **AutoML:**\nEscolher 3 algoritmos\/software de AutoML e executar as previs\u00f5es usando esses modelos\n\nhttps:\/\/www.kaggle.com\/andreshg\/automl-libraries-comparison , https:\/\/python.plainenglish.io\/do-you-find-machine-learning-boring-try-out-these-10-automl-solutions-f907ec5c20a8 , https:\/\/www.kaggle.com\/marcosvafg\/iesb-cia035-aula-11-pycaret-automl ,https:\/\/medium.com\/algoanalytics\/automatic-labelling-of-text-for-nlp-5270e70a2f5f","768044c8":"## Modelo 1","f635947e":"Utilizando a biblioteca **pycaret** de **AutoML**","99e188f4":"# **P2**","9267b5ab":"# Modelo 2","e838fbf1":"Um regressor de vota\u00e7\u00e3o \u00e9 um metaestimador de conjunto que se ajusta a v\u00e1rios regressores de base, cada um no conjunto de dados inteiro. Em seguida, ele calcula a m\u00e9dia das previs\u00f5es individuais para formar uma previs\u00e3o final.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.VotingRegressor.html","e3132c56":"[AVALIA\u00c7\u00c3O FINAL] Trabalho Final de Machine Learning\n\nPessoal,\n\nNesse trabalho final iremos trabalhar os conceitos dos algoritmos\/softwares de Automated Machine Learning, que vimos nas \u00faltimas aulas. Para isso, vamos fazer diversas previs\u00f5es para os dados da Competi\u00e7\u00e3o Big Mart Sales Prediction, a mesma que usamos no \u00faltimo trabalho.\n\nPara participar da competi\u00e7\u00e3o e poder submeter seus dados, voc\u00eas j\u00e1 sabem que devem se registrar no site - https:\/\/datahack.analyticsvidhya.com\/contest\/practice-problem-big-mart-sales-iii\n\nComo entrega para o trabalho voc\u00eas devem:\n - Criar um novo notebook no Kaggle usando como base o \u00faltimo trabalho entregue\n - Verificar a an\u00e1lise j\u00e1 realizada nos dados da base e melhorar os c\u00f3digos\/an\u00e1lises\n - Manter os modelos j\u00e1 criados mas buscar melhorar ainda mais as previs\u00f5es e voltar a submeter cada uma delas em separado\n - Registrar o resultado de cada submiss\u00e3o no pr\u00f3prio notebook\n - Manter a escolha dos modelos e usar o VotingRegressor, submeter novamente as previs\u00f5es e registrar o resultado no notebook\n - Escolher 3 algoritmos\/software de AutoML, executar as previs\u00f5es usando esses modelos, fazer as submiss\u00f5es para a competi\u00e7\u00e3o e \n\nregistrar o resultado no notebook\n\n\nAl\u00e9m da aula que j\u00e1 fizemos sobre PyCaret, esse notebook do Kaggle explica bem como aplicar as bibliotecas e softwares de AutoML -> https:\/\/www.kaggle.com\/andreshg\/automl-libraries-comparison\n \nO link para o notebook criado no Kaggle deve ser enviado para meu e-mail (marcosvafg@gmail) at\u00e9 a quarta-feira, dia 01\/12. N\u00e3o esquecer de deixar p\u00fablico o notebook do Kaggle.\n\n\nBons estudos e bom trabalho\nProf. Marcos Guimar\u00e3es","5a26ce25":"# **Modelo 4**","ca1577c9":"# **5 - VotingRegressor:**","8f9985d1":"# Modelo 3","02d5cdd0":"O modelo 2 se trata de um Gradient Boosting Regressor, que constr\u00f3i um modelo aditivo de forma avan\u00e7ada no est\u00e1gio; permite a otimiza\u00e7\u00e3o de fun\u00e7\u00f5es de perda diferenci\u00e1veis \u200b\u200barbitr\u00e1rias. Em cada est\u00e1gio, uma \u00e1rvore de regress\u00e3o \u00e9 ajustada no gradiente negativo da fun\u00e7\u00e3o de perda fornecida.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingRegressor.html","4e9a2764":"# **6 - Dados de teste:**","93aa0f51":"# **7 - Resultados:**\nCriando dataframes com os resultados, que seram passados para CSV e depois submetidos","c71d9f60":"# **2 - Tratamento dos dados:**"}}