{"cell_type":{"7f0a90e8":"code","bf8593b1":"code","f7a5c42e":"code","4366c4ba":"code","a9ef1daf":"code","60594649":"code","2fe77945":"code","63add46f":"code","0742c661":"code","22d6708e":"code","789c23c7":"code","ec69f63a":"code","5538b4ea":"code","1bffdfcf":"code","5e660103":"code","11bf4da1":"code","a50a5f73":"code","fa058353":"code","411c6f82":"code","e4bc4a53":"code","deb79894":"code","ec8734b3":"code","0d14f07e":"code","fd3eb41b":"code","69ff6ed0":"code","294447ea":"code","a954cd83":"code","0acaed0f":"code","26ed32ee":"code","94305791":"code","2a7ae247":"code","12ae079e":"code","a2fff98f":"markdown","b000b851":"markdown","f9953e7b":"markdown","ecd977f0":"markdown","8aac1fb6":"markdown","61f9b9de":"markdown","d05a96a2":"markdown","caa91ad7":"markdown","4a52ff48":"markdown","ce73ccad":"markdown","eda0bf00":"markdown","34b1969d":"markdown","65f63eff":"markdown","b8f377df":"markdown","718618c8":"markdown"},"source":{"7f0a90e8":"# Load packages and dataset\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","bf8593b1":"%matplotlib inline\nsns.set_style('darkgrid')","f7a5c42e":"df = pd.read_csv('..\/input\/league-of-legends-diamond-ranked-games-10-min\/high_diamond_ranked_10min.csv')\ndf.head()","4366c4ba":"# check missing values and data type\ndf.info()","a9ef1daf":"df_clean = df.copy()","60594649":"# Drop some unecessary columns. e.g. blueFirstblood\/redfirst blood blueEliteMonster\/redEliteMonster blueDeath\/redKills etc are repeated\n# Based on personal experience with the game, mimion yield gold+experience, we can drop minion kill too\ncols = ['gameId', 'redFirstBlood', 'redKills', 'redEliteMonsters', 'redDragons','redTotalMinionsKilled',\n       'redTotalJungleMinionsKilled', 'redGoldDiff', 'redExperienceDiff', 'redCSPerMin', 'redGoldPerMin', 'redHeralds',\n       'blueGoldDiff', 'blueExperienceDiff', 'blueCSPerMin', 'blueGoldPerMin', 'blueTotalMinionsKilled']\ndf_clean = df_clean.drop(cols, axis = 1)","2fe77945":"df_clean.info()","63add46f":"# Next let's check the relationship between parameters of blue team features\ng = sns.PairGrid(data=df_clean, vars=['blueKills', 'blueAssists', 'blueWardsPlaced', 'blueTotalGold'], hue='blueWins', size=3, palette='Set1')\ng.map_diag(plt.hist)\ng.map_offdiag(plt.scatter)\ng.add_legend();","0742c661":"# We can see that a lot of the features are highly correlated, let's get the correlation matrix\nplt.figure(figsize=(16, 12))\nsns.heatmap(df_clean.drop('blueWins', axis=1).corr(), cmap='YlGnBu', annot=True, fmt='.2f', vmin=0);","22d6708e":"# Based on the correlation matrix, let's clean the dataset a little bit more to avoid colinearity\ncols = ['blueAvgLevel', 'redWardsPlaced', 'redWardsDestroyed', 'redDeaths', 'redAssists', 'redTowersDestroyed',\n       'redTotalExperience', 'redTotalGold', 'redAvgLevel']\ndf_clean = df_clean.drop(cols, axis=1)","789c23c7":"# Next let's drop the columns has little correlation with bluewins\ncorr_list = df_clean[df_clean.columns[1:]].apply(lambda x: x.corr(df_clean['blueWins']))\ncols = []\nfor col in corr_list.index:\n    if (corr_list[col]>0.2 or corr_list[col]<-0.2):\n        cols.append(col)\ncols","ec69f63a":"df_clean = df_clean[cols]\ndf_clean.head()","5538b4ea":"df_clean.hist(alpha = 0.7, figsize=(12,10), bins=5);","1bffdfcf":"# train test split scale the set\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nX = df_clean\ny = df['blueWins']\nscaler = MinMaxScaler()\nscaler.fit(X)\nX = scaler.transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","5e660103":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\n# fit the model\nclf_nb = GaussianNB()\nclf_nb.fit(X_train, y_train)\n\npred_nb = clf_nb.predict(X_test)\n\n# get the accuracy score\nacc_nb = accuracy_score(pred_nb, y_test)\nprint(acc_nb)","11bf4da1":"# fit the decision tree model\nfrom sklearn import tree\nfrom sklearn.model_selection import GridSearchCV\n\ntree = tree.DecisionTreeClassifier()\n\n# search the best params\ngrid = {'min_samples_split': [5, 10, 20, 50, 100]},\n\nclf_tree = GridSearchCV(tree, grid, cv=5)\nclf_tree.fit(X_train, y_train)\n\npred_tree = clf_tree.predict(X_test)\n\n# get the accuracy score\nacc_tree = accuracy_score(pred_tree, y_test)\nprint(acc_tree)","a50a5f73":"# fit the model\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\n\n# search the best params\ngrid = {'n_estimators':[100,200,300,400,500], 'max_depth': [2, 5, 10]}\n\nclf_rf = GridSearchCV(rf, grid, cv=5)\nclf_rf.fit(X_train, y_train)\n\npred_rf = clf_rf.predict(X_test)\n# get the accuracy score\nacc_rf = accuracy_score(pred_rf, y_test)\nprint(acc_rf)","fa058353":"# fit logistic regression model\nfrom sklearn.linear_model import LogisticRegression\nlm = LogisticRegression()\nlm.fit(X_train, y_train)\n\n# get accuracy score\npred_lm = lm.predict(X_test)\nacc_lm = accuracy_score(pred_lm, y_test)\nprint(acc_lm)","411c6f82":"# fit the model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier() \n\n# search the best params\ngrid = {\"n_neighbors\":np.arange(1,100)}\nclf_knn = GridSearchCV(knn, grid, cv=5)\nclf_knn.fit(X_train,y_train) \n\n# get accuracy score\npred_knn = clf_knn.predict(X_test) \nacc_knn = accuracy_score(pred_knn, y_test)\nprint(acc_knn)","e4bc4a53":"data_dict = {'Naive Bayes': [acc_nb], 'DT': [acc_tree], 'Random Forest': [acc_rf], 'Logistic Regression': [acc_lm], 'K_nearest Neighbors': [acc_knn]}\ndf_c = pd.DataFrame.from_dict(data_dict, orient='index', columns=['Accuracy Score'])\nprint(df_c)","deb79894":"# recall and precision\nfrom sklearn.metrics import recall_score, precision_score\n\n# params for lm \nrecall_lm = recall_score(pred_lm, y_test, average = None)\nprecision_lm = precision_score(pred_lm, y_test, average = None)\nprint('precision score for naive bayes: {}\\n recall score for naive bayes:{}'.format(precision_lm, recall_lm))","ec8734b3":"# params for rf\nrecall_rf = recall_score(pred_rf, y_test, average = None)\nprecision_rf = precision_score(pred_rf, y_test, average = None)\nprint('precision score for naive bayes: {}\\n recall score for naive bayes:{}'.format(precision_rf, recall_rf))","0d14f07e":"df_clean.columns","fd3eb41b":"lm.coef_","69ff6ed0":"np.exp(lm.coef_)","294447ea":"coef_data = np.concatenate((lm.coef_, np.exp(lm.coef_)),axis=0)\ncoef_df = pd.DataFrame(data=coef_data, columns=df_clean.columns).T.reset_index().rename(columns={'index': 'Var', 0: 'coef', 1: 'oddRatio'})\ncoef_df.sort_values(by='coef', ascending=False)","a954cd83":"# try to visualize the results using PCA\nX = df_clean\ny = df['blueWins']\n\n# PCA is affected by scale, scale the dataset first\nfrom sklearn import preprocessing \n# Standardizing the features\nX = preprocessing.StandardScaler().fit_transform(X)","0acaed0f":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\ncomponents = pca.fit_transform(X)\nprint(pca.explained_variance_ratio_)","26ed32ee":"# create visulization df\ndf_vis = pd.DataFrame(data = components, columns = ['pc1', 'pc2'])\ndf_vis = pd.concat([df_vis, df['blueWins']], axis = 1)\nX = df_vis[['pc1', 'pc2']]\ny = df_vis['blueWins']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","94305791":"# refit the pca data\nlm.fit(X_train, y_train)","2a7ae247":"# visualize function\nfrom matplotlib.colors import ListedColormap\ndef DecisionBoundary(clf):\n    X = df_vis[['pc1', 'pc2']]\n    y = df_vis['blueWins']\n    \n    h = .02  # step size in the mesh\n\n    # Create color maps\n    cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])\n    cmap_bold = ListedColormap(['#FF0000', '#0000FF'])\n    \n    #Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    x_min, x_max = X.iloc[:, 0].min() - 1, X.iloc[:, 0].max() + 1\n    y_min, y_max = X.iloc[:, 1].min() - 1, X.iloc[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    plt.figure(figsize=(8, 8))\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n    \n    # Plot also the training points\n    plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, cmap=cmap_bold)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.show()","12ae079e":"DecisionBoundary(lm)","a2fff98f":"From the accuracy score, we can see that logistic regression and random forests predict the best. Next let's take a closer look at the recall and precision of these two methods.","b000b851":"## Naive Bayes","f9953e7b":"### Set UP","ecd977f0":"## Decision Tree","8aac1fb6":"# Model Selection","61f9b9de":"![](https:\/\/cdn.images.express.co.uk\/img\/dynamic\/143\/590x\/League-of-Legends-servers-down-1259147.webp?r=1585327760984)","d05a96a2":"We can see a lot of co-linearity between variables","caa91ad7":"# EDA","4a52ff48":"## Random Forests","ce73ccad":"# Conclusion","eda0bf00":"## K-nearest neighbours","34b1969d":"### PCA","65f63eff":"# Introduction\nLeague of Legends (LoL) is a multiplayer online battle arena video game developed and published by Riot Games for Microsoft Windows and macOS. In League of Legends, players assume the role of a \"champion\" with unique abilities and battle against a team of other player- or computer-controlled champions. The goal is usually to destroy the opposing team's \"Nexus\", a structure that lies at the heart of a base protected by defensive structures, although other distinct game modes exist as well with varying objectives, rules, and maps. Each League of Legends match is discrete, with all champions starting off relatively weak but increasing in strength by accumulating items and experience over the course of the game.\nThis dataset contains the first 10min. stats of approx. 10k ranked games (SOLO QUEUE) from a high ELO (DIAMOND I to MASTER). Players have roughly the same level. There are 19 features per team (38 in total) collected after 10min in-game. This includes kills, deaths, gold, experience, level\u2026 It's up to you to do some feature engineering to get more insights. The column blueWins is the target value (the value we are trying to predict). A value of 1 means the blue team has won.\nThis notebook is aim to predict which features are more correlated with winning. ","b8f377df":"In general, I probabily will choose the logistic regressaion after tweat a little bit.","718618c8":"## Logistic Regression"}}