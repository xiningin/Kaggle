{"cell_type":{"206df58d":"code","93c489ab":"code","fa278ec6":"code","5436e2ff":"code","810c8cf5":"code","225e03e1":"code","4774d183":"code","f850c4f9":"code","c1cd914f":"code","5ade8fc0":"code","41bb7acb":"code","b31b03b5":"code","0a919a2c":"code","182ef11e":"code","e330860a":"code","a8cf8c2c":"code","23ad63fc":"code","518437f3":"code","c84b1214":"code","34604cf6":"code","99b368f4":"code","289d50f3":"code","501e29f6":"code","fdac51d4":"code","1796e654":"markdown","0ad6fa8c":"markdown","bb2ebc0e":"markdown","cf0fea44":"markdown"},"source":{"206df58d":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np \nimport pandas as pd \nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, PowerTransformer, MinMaxScaler, FunctionTransformer\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nimport category_encoders as ce\nimport re\nimport string as s\nimport os\nplt.style.use('ggplot')\nsns.set(palette='RdBu', context='notebook', style='darkgrid')\n%matplotlib inline\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","93c489ab":"cars=pd.read_csv('\/kaggle\/input\/automobile-dataset\/Automobile_data.csv')\ncars.head()","fa278ec6":"cars.info()","5436e2ff":"cars.describe()","810c8cf5":"cars.describe(exclude=['int', 'float'])","225e03e1":"to_num=['normalized-losses', 'bore','stroke','horsepower','peak-rpm','price']\ncars[to_num]=cars[to_num].replace('?','0')\ncars[['horsepower','peak-rpm','price', 'normalized-losses']]=cars[['horsepower','peak-rpm','price','normalized-losses']].astype('float')\ncars[['bore','stroke']]=cars[['bore','stroke']].astype('float')\ncars[to_num]=cars[to_num].replace(0, np.nan)\ncars.dropna(subset=['price'], inplace=True)\ncars['num-of-doors']=cars['num-of-doors'].replace('?', 'four')","4774d183":"fig=plt.figure(figsize=(20, 8))\ncat_cols=['make','fuel-type','aspiration','num-of-doors','body-style','drive-wheels','engine-location','engine-type','num-of-cylinders','fuel-system']\nfor i,j in zip(cars[cat_cols].columns, range(10)):\n    ax=fig.add_subplot(2,5,j+1)\n    c2=cars[i].value_counts()\n    ax.bar(c2.index, c2)\n    ax.set_title('bar plot of {}'.format(i))\n    ax.set_xlabel(i)\n    ax.set_xticklabels(c2.index,rotation=90)\nplt.subplots_adjust(hspace=0.7)\nplt.show()","f850c4f9":"fig=plt.figure(figsize=(25, 8))\ncat_cols=['make','fuel-type','aspiration','num-of-doors','body-style','drive-wheels','engine-location','engine-type','num-of-cylinders','fuel-system']\nfor i,j in zip(cars[cat_cols].columns, range(10)):\n    ax=fig.add_subplot(2,5,j+1)\n    c1=pd.pivot_table(index=i, values='price', data=cars, aggfunc='mean')\n    ax.plot(c1.index, c1['price'])\n    ax.scatter(c1.index, c1['price'])\n    ax.set_title('mean price for each {}'.format(i))\n    ax.set_xlabel(i)\n    ax.set_xticklabels(c1.index,rotation=90)\nplt.subplots_adjust(hspace=0.8)\nplt.show()","c1cd914f":"sns.pairplot(cars, diag_kind='hist')\nplt.show()","5ade8fc0":"corr_df=cars.corr()\nmask=np.triu(np.ones_like(corr_df, dtype=bool))\nplt.figure(figsize=(10,5))\nsns.heatmap(corr_df,\n            annot=True,\n            mask=mask,\n            fmt='.2f',\n            linewidth=1)\nplt.show()","41bb7acb":"#detecting outliers\nfor i in cars.columns:\n    if cars[i].dtype!='object':\n        print(i)\n        mu=cars[i].mean()\n        st=cars[i].std()\n        cut_off=st * 3\n        lower, upper=mu-cut_off, mu+cut_off\n        print('number of outliers',len(cars[(cars[i]>upper)&(cars[i]<lower)]))\n        print('\\n')\n        ","b31b03b5":"cols_to_drop=['city-mpg', 'engine-location']\ncars.drop(cols_to_drop, axis=1, inplace=True)\ncars_1=cars.copy()","0a919a2c":"#x=x.drop('symboling', axis=1)","182ef11e":"all_num_cols=[i for i in cars_1.columns if cars_1[i].dtype!='object']\nfor i in all_num_cols:\n    cars_1[i]=cars_1[i].fillna(cars_1[i].mean())","e330860a":"code_cols=[i for i in cars_1.columns if (cars_1[i].dtype=='object')and(cars_1[i].nunique()==2)]\nohe_cols=[i for i in cars_1.columns if (cars_1[i].dtype=='object')and(cars_1[i].nunique()>2)]\nall_cat_cols=[i for i in cars_1.columns if cars_1[i].dtype=='object']\nst_num_cols=[i for i in cars_1.drop(['compression-ratio', 'price'], axis=1).columns if cars_1[i].dtype!='object']\nlog_num_cols='compression-ratio'","a8cf8c2c":"'''\ndef to_dummies(df, col):\n    c=pd.get_dummies(df[col], prefix=col)\n    df=df.drop(col, axis=1)\n    df=pd.concat([df, c], axis=1)\n    return df\ndef to_codes(df, col):\n    df[col]=pd.Categorical(df[col]).codes\n    return df\nfor i in code_cols:\n    x=to_codes(x, i)\nfor i in ohe_cols:\n    x=to_dummies(x,i)\n'''","23ad63fc":"def cat_boost(df, cols, target):\n    cb=ce.CatBoostEncoder(cols=cols)\n    cb.fit(df[cols], df[target])\n    c=cb.transform(df[cols]).add_suffix('_cb')\n    df=df.drop(cols,axis=1)\n    df=pd.concat([df, c], axis=1)\n    return df\ncars_1=cat_boost(cars_1, all_cat_cols, 'price')","518437f3":"def standard_scaler(df, col):\n    df[col]=(df[col]-df[col].mean())\/df[col].std()\n    return df\nfor i in st_num_cols:\n    cars_1=standard_scaler(cars_1, i)","c84b1214":"def log_transformer(df, col):\n    df[col]=np.log(df[col])\n    return df\ncars_1=log_transformer(cars_1, 'compression-ratio')","34604cf6":"x=cars_1.drop('price', axis=1)\ny=cars_1['price']","99b368f4":"x_train, x_test, y_train, y_test=train_test_split(x,\n                                                  y,\n                                                  test_size=0.2,\n                                                  random_state=42)","289d50f3":"model=[\n    {\n        'name':'lasso regression',\n        'estimator':Lasso(),\n        'hyperparameters':{\n            'alpha':np.arange(0.01, 1, 0.02)\n        }\n    },\n    {\n        'name':'decision Tree',\n        'estimator':DecisionTreeRegressor(),\n        'hyperparameters':{\n            'max_depth':[2,3,4,5,6,7],\n            'criterion':['mse', 'friedman_mse', 'mae'],\n            'splitter':['best', 'random'],\n            'max_features':['auto', 'sqrt', 'log2']\n        }\n    },\n    {\n        'name':'Random Forest',\n        'estimator':RandomForestRegressor(),\n        'hyperparameters':{\n            'n_estimators':[2,3,4,5,6],\n            'max_depth':[2,3,4,5,6,7],\n            'max_features':['auto', 'sqrt', 'log2']\n        }\n    },\n    {\n        'name':'Extreme Gradient Boosting',\n        'estimator':XGBRegressor(),\n        'hyperparameters':{\n            'n_etimators':[10,20,30,40,50],\n            'max_depth':[2,4,6,8],\n            'subsample':[0.3, 0.5, 0.7, 1],\n            'learning_rate':np.arange(0.01, 0.1, 0.01)\n        }\n    }\n]\nfor i in model:\n    print(i['name'])\n    gs=GridSearchCV(i['estimator'], param_grid=i['hyperparameters'], cv=3, n_jobs=-1, scoring='r2')\n    gs.fit(x_train, y_train)\n    print('best score: ', gs.best_score_)\n    print('best parameters ; ', gs.best_params_)\n    print('best model: ', gs.best_estimator_)\n    print('\\n')","501e29f6":"xgb=XGBRegressor(learning_rate= 0.09, max_depth= 2, n_etimators= 10, subsample= 1)\nxgb.fit(x_train, y_train)\npreds=xgb.predict(x_test)\npreds[:15]","fdac51d4":"pd.DataFrame({\n    'real_values':y_test,\n    'predicted_values':preds\n})","1796e654":"<h1>model Selection<\/h1>","0ad6fa8c":"<h1>Feature Engineering & Preprocessing<\/h1>","bb2ebc0e":"<h1>EDA<\/h1>","cf0fea44":"<h1>Data Cleaning<\/h1>"}}