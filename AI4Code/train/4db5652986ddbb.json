{"cell_type":{"7ce1baca":"code","3b5ddf34":"code","6519a64b":"code","37840278":"code","074ebeda":"code","21930681":"code","9a8d8e76":"code","ed7030ce":"code","692cda6c":"code","528f007a":"code","82b1e687":"code","6e0ff7de":"code","e09a0c8b":"code","5aed30b0":"code","b55e6708":"code","a36fac34":"code","754a3b6b":"code","6ecf590b":"code","863353bf":"code","a6d6dee2":"code","bda0f097":"code","1d5b8f8d":"code","1f44ff75":"code","dc4ab1f6":"code","b6de6445":"code","468bcfba":"code","b36eccc6":"code","4ece6782":"code","6ea54dd8":"code","692c1c34":"code","7472d941":"code","96fa5230":"code","c9747e57":"code","5844d38c":"code","2872ed4c":"code","2ffe86f0":"code","49996b0e":"code","61063b76":"code","be416dc6":"code","f691cb7f":"code","3cdc0b53":"code","a435ef41":"code","1699cbd4":"code","de87eed4":"code","de03255b":"code","30b8b860":"code","e9b4c4ca":"code","45b45efb":"code","3cae3880":"code","38925a1d":"code","3e0edd8a":"code","6bf66225":"code","b5e8f92b":"code","24ed2592":"code","75608ca0":"code","a1b86922":"code","eb94035a":"code","dfd5eaa5":"code","e947b722":"code","7fe901cb":"code","6ce25189":"code","f91ddc90":"markdown","95429af3":"markdown","56150a5b":"markdown","0d4a3c8d":"markdown","96c29b65":"markdown","25a408a9":"markdown","d15670b7":"markdown","f8c76429":"markdown","88869e19":"markdown","17dc6aed":"markdown","647cd4f9":"markdown","d6ea9eab":"markdown","fb7c37d0":"markdown","def73589":"markdown","c9a10d31":"markdown","74a99bf9":"markdown","876e4bf1":"markdown","6323e8a8":"markdown"},"source":{"7ce1baca":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3b5ddf34":"import os\nimport pandas as pd\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedShuffleSplit, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import reciprocal, expon\nfrom sklearn.utils.fixes import loguniform\nfrom sklearn.ensemble import RandomForestClassifier","6519a64b":"train_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')","37840278":"train_data.head()","074ebeda":"print(f\"Dimension of the Training Dataset is {train_data.shape}\")\nprint(f\"Dimension of the Testing Dataset is {test_data.shape}\")","21930681":"train_data.info()","9a8d8e76":"def print_the_number_missing_data(data):\n    # Lets check the number of duplicated rows\n    print(\"=\"*30)\n    print(f'Number of Duplicate Rows {data[data.duplicated()].shape[0]}')\n\n    #Let's check the percentage of missing values for the application dataset.\n    total_missing = data.isnull().sum()\n    percent = round((100*(total_missing\/data.isnull().count())),2)\n\n    #Making a table for both and get the top 20 Columns\n\n    missing_value_app_data = pd.concat([total_missing, percent], axis =1, keys= ['Total_missing', 'percent'])\n    missing_value_app_data.sort_values(by='Total_missing',ascending=False,inplace=True)\n    print(\"=\"*30)\n    print(\"Number and % of Missing Value\")\n    print(missing_value_app_data)","ed7030ce":"print_the_number_missing_data(train_data)","692cda6c":"train_data.describe()","528f007a":"def percent_value_counts(df, feature):\n    \"\"\"\n    This will take in a dataframe and a column and \n    finds the percentage of the value counts\n    \"\"\"\n    percent = pd.DataFrame(round(df[feature].value_counts(dropna=False, normalize=True)*100,2))\n    total = pd.DataFrame(df[feature].value_counts(dropna = False))\n    total.columns = [\"Total\"]\n    percent.columns = ['Percent']\n    return pd.concat([percent, total], axis = 1)    ","82b1e687":"# Let's check that the target is indeed 0 or 1\npercent_value_counts(train_data, \"Survived\")","6e0ff7de":"percent_value_counts(train_data, \"Pclass\")","e09a0c8b":"percent_value_counts(train_data, \"Sex\")","5aed30b0":"percent_value_counts(train_data, \"Embarked\")","b55e6708":"# This will do the basic feature selection based on the number of features selected \n# This is a custom transformer & Scikit-Learn provides many useful transformers but in this case we would be needing a custom transformer\n# Transformer works seamlessly with Scikit-Learn functionalities(such as pipelines), and since Scikit-Learn relies on duck typing (not inheritance),\n# all we need is to create a class and implement three methods: fit()(returning self), transform(), and fit_transform().\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attributes_names):\n        self.attributes_names = attributes_names\n    def fit(self, X, y = None):\n        return self\n    def transform(self, X):\n        return X[self.attributes_names]\n\n# Scikit-Learn provides the Pipeline class to help with such sequences of transformations.\n# pipeline for the numerical attributes\nnum_pipeline = Pipeline([\n        (\"select_numeric\", DataFrameSelector([\"Age\", \"SibSp\", \"Parch\", \"Fare\"])),\n        (\"imputer\", SimpleImputer(strategy=\"median\")),\n    ])\n\n# Select the most frequent impute it using the same\n# Why can't we just simply use the Simple Impurter in categorical column ? it doesn't work on the caregorical columns\n# Hence the workaround is this approach\nclass MostFrequentImputer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y = None):\n        self.most_frequent = pd.Series([X[elem].value_counts().index[0] for elem in X],\n                                      index = X.columns)\n        return self\n    def transform(self, X, y = None):\n        return X.fillna(self.most_frequent)\n\n# Categroical Column selection pipeline\ncat_pipeline = Pipeline([\n    (\"select_cat\", DataFrameSelector([\"Pclass\", \"Sex\", \"Embarked\"])),\n    (\"imputer\", MostFrequentImputer()),\n    (\"cat_encoder\", OneHotEncoder(sparse=False))\n])\n\n# Lets put both the numerical and categorical pipelines together\npreprocess_pipeline = FeatureUnion(transformer_list=[\n    (\"num_pipeline\", num_pipeline),\n    (\"cat_pipeline\", cat_pipeline)\n])","a36fac34":"# Lets see how the numeric pipeline is working\nnum_pipeline.fit_transform(train_data)","754a3b6b":"# Lets see how the categorical columns pipeline is working\ncat_pipeline.fit_transform(train_data)","6ecf590b":"preprocess_pipeline.fit_transform(train_data)","863353bf":"# Lets try with ColumnTransformer as well\nnum_pipeline_mod = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"median\"))\n])\n\ncol_pipeline_mod = Pipeline([\n    (\"imputer\", MostFrequentImputer()),\n    (\"cat_encoder\", OneHotEncoder(sparse=False))\n])\n\nnum_list = ['Age', 'SibSp', 'Parch', 'Fare']\ncolumn_list = [\"Pclass\", \"Sex\", \"Embarked\"]\n\npreprocess_pipeline_mod = ColumnTransformer([\n    ('num', num_pipeline_mod, num_list),\n    ('cat', col_pipeline_mod, column_list)\n])","a6d6dee2":"preprocess_pipeline_mod.fit_transform(train_data)","bda0f097":"# Lets compare the Column Transformer Results with the FeatureUnion results\n(preprocess_pipeline.fit_transform(train_data) == preprocess_pipeline_mod.fit_transform(train_data) ).all()\n# And they are same","1d5b8f8d":"# Preprocessing pipeline that takes the raw data and \n# outputs numerical input features that we can feed to any ML model\n\n# Lets prepare the training data\nX_train = preprocess_pipeline_mod.fit_transform(train_data.drop(columns='Survived'))\ny_train = train_data.Survived","1f44ff75":"# Ready to train a classifier. Let's start with an SVC:\nsvm_clf = SVC(gamma=\"auto\")\nsvm_clf.fit(X_train, y_train)\n\nsvm_scores = cross_val_score(svm_clf, X_train, y_train, cv=10, scoring='accuracy')\nsvm_scores.mean()\n\n# 73% accuracy, clearly better than random chance, but it's not a great score.\n# Lets try to find the ideal hyperparameter that can boost up the performance","dc4ab1f6":"# How to come an ideal combination of Hypeparameters? Answer is GridSearch CV\n# All you need to do is tell it which hyperparameters you want it to experiment with, and what values to\n# try out, and it will evaluate all the possible combinations of hyperparameter values,\n# using cross-validation.\nparam_list = [\n    {'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n                     'C': [100, 1000, 10000], 'shrinking' : [True, False], 'decision_function_shape' : ['ovo', 'ovr']}]\n\nsvm_clf = SVC(random_state=42, verbose=True)\ngrid_search = GridSearchCV(svm_clf, param_list, cv = 3, verbose = 1, scoring='accuracy', return_train_score=True, n_jobs= -1)\ngrid_search.fit(X_train, y_train)","b6de6445":"grid_search.best_estimator_","468bcfba":"grid_search.best_score_","b36eccc6":"def plot_perfomance_of_cross_validation(arr_list_scores, label):\n    plt.figure(figsize=(8, 4))\n    plt.boxplot(arr_list_scores)\n    plt.plot([1]*len(arr_list_scores), arr_list_scores, \".\")\n    plt.ylabel(\"Accuracy\", fontsize=14)\n    plt.xlabel(label, fontsize = 14)\n    plt.show()\n\ncvscores = grid_search.cv_results_\narr_list_svm = []\nfor mean_score, params in zip(cvscores[\"mean_test_score\"], cvscores[\"params\"]):\n    print(round(mean_score,2), params)\n    arr_list_svm.append(round(mean_score,2))\n\nplot_perfomance_of_cross_validation(arr_list_svm, \"SVM with GridSearchCV\")","4ece6782":"# The grid search approach is fine when you are exploring relatively few combinations\n# but when the hyperparameter search space is large, it is often preferable to use RandomizedSearchCV.\n# Instead of trying out all possible combinations,it evaluates a given number of random combinations by selecting a random\n# value for each hyperparameter at every iteration.\n\n# Advantages - \n# If you let the randomized search run for, say, 1,000 iterations, this approach will\n# explore 1,000 different values for each hyperparameter (instead of just a few values\n# per hyperparameter with the grid search approach).\n# You have more control over the computing budget you want to allocate to hyperparameter\n# search, simply by setting the number of iterations.\n\nparam_list = {\n    'kernel':['rbf'],\n    'C': loguniform(1e0, 1e3),\n    'gamma': expon(scale=1.0),\n    'shrinking' : [True, False],\n    'decision_function_shape' : ['ovo', 'ovr']\n}\n\nsvm_clf = SVC(random_state=42, verbose=True)\nrnd_search = RandomizedSearchCV(svm_clf, param_distributions=param_list,\n                                cv=5, scoring='accuracy',n_iter= 50,\n                               verbose=1, random_state=42, n_jobs= -1, return_train_score=True)\nrnd_search.fit(X_train, y_train)","6ea54dd8":"rnd_search.best_estimator_","692c1c34":"rnd_search.best_score_","7472d941":"cvscores = rnd_search.cv_results_\narr_list_svm_rndm = []\nfor mean_score, params in zip(cvscores[\"mean_test_score\"], cvscores[\"params\"]):\n    print(round(mean_score,2), params)\n    arr_list_svm_rndm.append(round(mean_score,2))\n\nplot_perfomance_of_cross_validation(arr_list_svm_rndm, \"SVM with Randomized\")","96fa5230":"forest_clf = RandomForestClassifier(n_estimators=10, random_state=42)\nforest_scores = cross_val_score(forest_clf, X_train, y_train, cv = 10, scoring= 'accuracy')\nforest_scores.mean()","c9747e57":"# Grid search CV\nn_estimators = range(500, 2000, 400 );\nmax_depth = range(7,12);\ncriterions = ['gini', 'entropy'];\ncv = StratifiedShuffleSplit(n_splits=5, test_size=.30, random_state=15)\n\nparameters = {'n_estimators':n_estimators,\n              'max_depth':max_depth,\n              'criterion': criterions\n              }\n\nrandom_forest_grid = GridSearchCV(estimator=RandomForestClassifier(max_features='auto', n_jobs = -1, random_state = 42),\n                                 param_grid=parameters,\n                                 cv=cv,\n                                 n_jobs = -1, \n                                 verbose= 1, \n                                 return_train_score =True, \n                                 scoring = 'accuracy')\nrandom_forest_grid.fit(X_train, y_train)","5844d38c":"random_forest_grid.best_estimator_","2872ed4c":"random_forest_grid.best_score_","2ffe86f0":"cvscores = random_forest_grid.cv_results_\narr_list_rndc_grid = []\nfor mean_score, params in zip(cvscores[\"mean_test_score\"], cvscores[\"params\"]):\n    arr_list_rndc_grid.append(round(mean_score,2))\n\nplot_perfomance_of_cross_validation(arr_list_rndc_grid, \"Random Forest Classifier with Grid Search CV\")","49996b0e":"# Randomized Search CV\n\nn_estimators = range(500, 2000, 400 );\nmax_depth = range(7,12);\ncriterions = ['gini', 'entropy'];\nn_estimators = [50, 70, 100, 150]\ncv = StratifiedShuffleSplit(n_splits= 10, test_size=.30, random_state=15)\n\nparameters = {'n_estimators':n_estimators,\n              'max_depth':max_depth,\n              'criterion': criterions,\n              'n_estimators': n_estimators,\n              'bootstrap':[True, False],\n              'min_samples_leaf': range(1,5),\n              'warm_start':[True, False],\n              'class_weight': [\"balanced\", \"balanced_subsample\", None]\n             }\n\nrandom_forest_grid = RandomizedSearchCV(estimator=RandomForestClassifier(max_features='auto', n_jobs = -1, random_state = 42),\n                                 param_distributions=parameters,\n                                 cv=cv,\n                                 n_jobs = -1, \n                                 verbose= 1, \n                                 n_iter= 100,\n                                 scoring= 'accuracy',\n                                 random_state= 42,\n                                 return_train_score =True)\nrandom_forest_grid.fit(X_train, y_train)","61063b76":"random_forest_grid.best_score_","be416dc6":"random_forest_grid.best_estimator_","f691cb7f":"cvscores = random_forest_grid.cv_results_\narr_list_rndc_rndm = []\nfor mean_score, params in zip(cvscores[\"mean_test_score\"], cvscores[\"params\"]):\n    arr_list_rndc_rndm.append(round(mean_score,2))\n\nplot_perfomance_of_cross_validation(arr_list_rndc_rndm, \"Random Forest Classifier with Randomized Search CV\")","3cdc0b53":"plt.figure(figsize=(15, 4))\nplt.plot([1]*len(arr_list_svm), arr_list_svm, \".\")\nplt.plot([2]*len(arr_list_svm_rndm), arr_list_svm_rndm, \".\")\nplt.plot([3]*len(arr_list_rndc_grid), arr_list_rndc_grid, \".\")\nplt.plot([4]*len(arr_list_rndc_rndm), arr_list_rndc_rndm, \".\")\nplt.boxplot([arr_list_svm, arr_list_svm_rndm,arr_list_rndc_grid, arr_list_rndc_rndm ], labels=(\"SVM Grid Search\",\"SVM Randomized Search\", \"Random Forest Grid Search\", \"Random Forest Randomized Search\"))\nplt.ylabel(\"Accuracy\", fontsize=14)\nplt.ylim(0.6, 0.85)\nplt.show()","a435ef41":"# Lets build the final Pipeline that will have some data preprocessing steps and the final model as well\nprepare_select_predict_pipeline = Pipeline([\n    ('preparation', preprocess_pipeline_mod ),\n    ('model', RandomForestClassifier(criterion='entropy', max_depth=9, n_estimators=50,\n                       n_jobs=-1, random_state=42)  )\n])\n\nprepare_select_predict_pipeline.fit(train_data.drop(columns='Survived'), y_train )","1699cbd4":"# This basically tells how to use the final trained model for prediction \n# So the whole model building steps gets consized in some few steps\nrndm_index = np.random.randint(0, len(train_data))\ndata_output = prepare_select_predict_pipeline.predict(train_data.drop(columns='Survived').iloc[[rndm_index]]).tolist()\nprint(f\"Index choosen is: {rndm_index} and the predicted o\/p is: {data_output}, and the actual output should be: {y_train.iloc[[rndm_index]].values}\")","de87eed4":"# Lets create a Custom Transformer that would add some columns\n# But prior to that lets see if some columns can be combined or not\n\ntrain_data['Total_Members'] = train_data.SibSp + train_data.Parch\ntrain_data[[\"Total_Members\", \"Survived\"]].groupby(['Total_Members']).mean()\n\n# As it can be seen below lower as the number of members more is the chances of survival","de03255b":"# if the Age Column can also be transformed into some interesting Categorical Columns\ntrain_data[\"AgeBucket\"] = train_data[\"Age\"] \/\/ 15 * 15\ntrain_data[[\"AgeBucket\", \"Survived\"]].groupby(['AgeBucket']).mean()\n\n# As it can also be seen below age do have a lot of importance like infants\/childs have higher chances of survival\n# Followed by older people and then the middle aged people","30b8b860":"# Lets revert it back to the previous state\ntrain_data = pd.read_csv('..\/input\/titanic\/train.csv')","e9b4c4ca":"X_train = train_data.copy()","45b45efb":"# Lets create a transformer that would do the job of creating these new features & removing the existing ones\nclass CreateNewFeatureAndRemoveSome(BaseEstimator, TransformerMixin):\n    def __init__(self, age_bucket = 15):\n        self.age_bucket = age_bucket\n    def fit(self, X, y = None):\n        return self\n    def transform(self, X):\n        X['Total_Members'] =  X['SibSp'] + X['Parch']\n        X['AgeBucket'] = X['Age'] \/\/ self.age_bucket * self.age_bucket\n        X['Total_Members'] = X['Total_Members'].astype('object')\n        X['AgeBucket'] = X['AgeBucket'].astype('object')\n        return X.drop(columns = ['SibSp','Parch','Age'])\n\nattr_adder = CreateNewFeatureAndRemoveSome()\ntrain_data_extra_attr = attr_adder.transform(X_train.copy())\ntrain_data_extra_attr.head()","3cae3880":"# Lets try to incorporate the new Transformer CreateNewFeatureAndRemoveSome into the existing pipeline\n\nnum_pipeline_mod = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"median\"))\n])\n\ncol_pipeline_mod = Pipeline([\n    (\"imputer\", MostFrequentImputer()),\n    (\"cat_encoder\", OneHotEncoder(sparse=False))\n])\n\nfeature_adder = Pipeline([\n    (\"add_features\",CreateNewFeatureAndRemoveSome(15)),\n    (\"treat_as_categorical\",col_pipeline_mod )\n])\n\nfeature_selection_list = ['SibSp', 'Parch', 'Age']\nnum_list = ['Fare']\ncolumn_list = [\"Pclass\", \"Sex\", \"Embarked\"]\n\npreprocess_pipeline_mod = ColumnTransformer([\n    ('num', num_pipeline_mod, num_list),\n    ('cat', col_pipeline_mod, column_list),\n    ('feature_adder', feature_adder, feature_selection_list )\n])\n\n\nX_train_mod = preprocess_pipeline_mod.fit_transform(X_train.drop(columns='Survived').copy())\nX_train_mod.shape","38925a1d":"# As it can be seen below these are the categories that got transfored into one hot encoded vectors\n# As we can see above we have 24 number of features and below it can be seen what are their values\nprocessed_cat = preprocess_pipeline_mod.named_transformers_[\"cat\"][\"cat_encoder\"].categories_\n\ncat_one_hot_encoded_column = num_list.copy()\nattributes = []\nfor index, elem in enumerate(column_list):\n    attributes += list(map(lambda x: f'{elem}_{x}', processed_cat[index]))\n\ncat_one_hot_encoded_column += attributes\n\n# As it can be seen below these are new added categories that got transfered into one hot encoded vectors\nadded_new_features = preprocess_pipeline_mod.named_transformers_[\"feature_adder\"][\"treat_as_categorical\"][\"cat_encoder\"].categories_\nadded_new_features\nattributes = []\nfor index, elem in enumerate([\"Total_Members\", \"AgeBucket\"]):\n    attributes += list(map(lambda x: f'{elem}_{x}', added_new_features[index]))\ncat_one_hot_encoded_column += attributes\ncat_one_hot_encoded_column","3e0edd8a":"n_estimators = range(10, 2000, 50 );\nmax_depth = range(1,12);\ncriterions = ['gini', 'entropy'];\nn_estimators = [50, 70, 100, 150]\ncv = StratifiedShuffleSplit(n_splits= 10, test_size=.30, random_state=15)\n\nparameters = {'n_estimators':n_estimators,\n              'max_depth':max_depth,\n              'criterion': criterions,\n              'n_estimators': n_estimators,\n              'bootstrap':[True, False],\n              'min_samples_leaf': range(1,5),\n              'warm_start':[True, False],\n              'class_weight': [\"balanced\", \"balanced_subsample\", None]\n             }\n\nrandom_forest_grid = RandomizedSearchCV(estimator=RandomForestClassifier(max_features='auto', n_jobs = -1, random_state = 42),\n                                 param_distributions=parameters,\n                                 cv=cv,\n                                 n_jobs = -1, \n                                 verbose= 1, \n                                 n_iter= 50,\n                                 scoring= 'accuracy',\n                                 random_state= 42,\n                                 return_train_score =True)\n\nrandom_forest_grid.fit(X_train_mod, y_train)","6bf66225":"random_forest_grid.best_estimator_","b5e8f92b":"random_forest_grid.best_score_","24ed2592":"# As it can be seen below there are lots of features available but all of them is not so important \n# So lets get the top features and using that lets try to build the models\nfeature_importance = random_forest_grid.best_estimator_.feature_importances_\nsorted(zip(feature_importance, cat_one_hot_encoded_column), reverse=True)","75608ca0":"# Lets create a transformer that will select top features based on feature importance\ndef indices_of_top_k(arr,k):\n    return np.sort(np.argpartition(np.array(arr),-k)[-k:])\n\nclass TopFeatureSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, feature_importances, k):\n        self.feature_importances = feature_importances\n        self.k = k\n    def fit(self, X, y = None):\n        self.feature_indices_ = indices_of_top_k(self.feature_importances, self.k)\n        return self\n    def transform(self, X):\n        return X[:, self.feature_indices_]","a1b86922":"top_k_feature_indices = indices_of_top_k(feature_importance, 5)\nnp.array(cat_one_hot_encoded_column)[top_k_feature_indices]","eb94035a":"# This will be the Final Pipeline\n# This will take the raw data, do Featue Engineering, Then Select Top Feature and Finally apply the learning algorithm\n# So this code is quite consise and its very readable as well\nk = 5\n\nfinal_outcome_pipeline = Pipeline([\n    (\"preprocess_pipeline_mod\", preprocess_pipeline_mod),\n    (\"feature_selection\", TopFeatureSelector(feature_importance, k)),\n    (\"random_forest_classifier\", RandomForestClassifier(criterion='entropy', max_depth=9, n_estimators=50, n_jobs=-1, random_state=42))\n])\n\nfinal_outcome_pipeline.fit(X_train.drop(columns='Survived').copy(), y_train)","dfd5eaa5":"final_outcome_pipeline","e947b722":"final_outcome_pipeline.predict(X_train.drop(columns='Survived').iloc[:4])","7fe901cb":"y_train.iloc[:4]","6ce25189":"# Now it can be seen as it as well how easy it is to apply on the test data\ntest_data['Survived'] = final_outcome_pipeline.predict(test_data)\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': test_data.Survived})\noutput.to_csv('my_submission.csv', index=False)\n\nprint(\"your submission was successfully saved!\")","f91ddc90":"* Only 38% **Survived**. That's close enough to 40%, so accuracy metrics will be a reasonable metric to evaluate our model.\n* The mean **Fare** was \u00a332.20, which does not seem so expensive (but it was probably a lot of money back then).\n* The mean **Age** was less than 30 years old.","95429af3":"### Lets Try with SVM","56150a5b":"**Conclusion**\n- When the learning algorithm was choosen as **SVM**\n    - When it was mainly on the default set of parameter the mean score came out as **0.7329588014981274**\n    - After Applying Grid Search, the best score was found out at **0.7991021324354658**\n    - After applying Randomized Search CV, the best score came out as **0.795744146632352**\n\n\n- When the learning algorithm was choosen as **Random Forest Classifier**\n    - When it was mainly on the default set of parameter the mean score came out as **0.8081647940074905**, scores improved a lot even in the base model\n    - After Applying Grid Search, the best score was found out at **0.8305970149253732**\n    - After applying Randomized Search CV, the best score came out as **0.839179104477612**","0d4a3c8d":"### Lets do some Feature Engineering","96c29b65":"The Embarked attribute tells us where the passenger embarked: C=Cherbourg, Q=Queenstown, S=Southampton, and most of the people onboarded from Southampton","25a408a9":"Most of the people are in 3rd Class and followed by first class and also the second class","d15670b7":"**Conclusion**\n- As it can be seen only the top 3 features are contributing more than 10%, so lets take the top 5 features and build the model","f8c76429":"As it can be seen above around 65% of people are male and rest 35% of people are female","88869e19":"**Note**: the code below uses a mix of `Pipeline`, `FeatureUnion` and a custom `DataFrameSelector` to preprocess some columns differently. \nWill also try with a preferable option to use a `ColumnTransformer`.","17dc6aed":"### Initial Pipeline Creation & Custom Transformer","647cd4f9":"It improved by 6%, but lets do some more research how to boost the hyperparameter, as in the above param grid its quite less number of combinations","d6ea9eab":"Now let's take a quick look at all the categorical attributes:","fb7c37d0":"The **Age**, **Cabin** and **Embarked** attributes are sometimes null (less than 891 non-null), especially the **Cabin** (77% are null). Will ignore **Cabin** for now and focus on the rest. The **Age** attribute has about 20% null values, for now lets replacing null values with the median age seems reasonable.\n\nLet's take a look at the numerical attributes:","def73589":"### Lets try with Random Forest Classifier","c9a10d31":"So it can be seen above new columns got created and also the old ones which were not needed were also removed as well","74a99bf9":"This notebook is mainly about describing the different ways of writing a custom transformer and a pipeline & this notebook will demonstrate the different ways of doing cross validation\n\n[N.B] - This notebook is not about achieving an ideal accuracy this is about how to write an efficient and reusable ML code\n\n\n### Importing Libraries","876e4bf1":"The attributes have the following meaning:\n* **Survived**: that's the target, 0 means the passenger did not survive, while 1 means he\/she survived.\n* **Pclass**: passenger class, 1 = 1st, 2 = 2nd, 3 = 3rd\n* **Name**, **Sex**, **Age**: self-explanatory\n* **SibSp**: how many siblings & spouses of the passenger aboard the Titanic.\n* **Parch**: how many children & parents of the passenger aboard the Titanic.\n* **Ticket**: ticket id\n* **Fare**: price paid (in pounds)\n* **Cabin**: passenger's cabin number\n* **Embarked**: where the passenger embarked the Titanic, C = Cherbourg, Q = Queenstown, S = Southampton","6323e8a8":"### Importing Data & Data Exploration"}}