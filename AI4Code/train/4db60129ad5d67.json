{"cell_type":{"59dae46c":"code","84caf1ab":"code","9f6b72f8":"code","f426820a":"code","5359e9cd":"code","56126b18":"code","2d6afbcf":"code","6048067a":"code","26228512":"code","8eddf8c7":"code","3ebc69db":"code","3dae450b":"code","1e2b1d8d":"code","44bed287":"code","b7851b31":"code","fb2dd9ac":"code","c8b9fc73":"code","f8bc6836":"code","8b45916a":"code","5276d77a":"code","8a66832a":"code","f1da9a7b":"code","79b23ca9":"code","27e13ef1":"code","152302b0":"markdown","118a76c3":"markdown","bd5b0b18":"markdown","292cf9dd":"markdown","c82f4cf3":"markdown","d3fb0843":"markdown","d35abe11":"markdown","b124fbee":"markdown"},"source":{"59dae46c":"### Load the required packages in the required format\nimport pandas as pd\nimport os\nimport warnings\n\n%matplotlib notebook\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pickle\nplt.style.use('ggplot')\nimport warnings\nwarnings.filterwarnings(\"ignore\")","84caf1ab":"### Read the datasets from the given locations and do following\npath = \"..\/input\/spotify-playlist\/\"\nfilename = \"spotify_dataset.csv\"\n### While Loading datasets we say error_bad_lines = false which drops rows with errors \n### As it is experimental project and we have huge datasets, dropping 100-200 Bad rows will not impact any results\nprint (\"Reading the data\")\nspotify_data = pd.read_csv(os.path.join(path,filename),escapechar= '.',error_bad_lines = False,warn_bad_lines=False)\nprint (\"Read Succesful with shape {}\".format(spotify_data.shape))\n### Columns names were not very clean give them manual names\nspotify_data.columns = ['user_id','artistname','trackname','playlistname']","9f6b72f8":"print (\"Some General statistics about data are as follows:\",spotify_data.info())\nprint (\"Lets look at the summary stats about the data :\",spotify_data.describe(include ='object'))\nprint (\"The number of rows in the datasets are as follows :\",spotify_data.shape[0])\nprint (\" The columns in the data are as follows :\",spotify_data.columns)","f426820a":"### Lets first look at the distribution of number of playlist by user_id\nspotify_user_summary = spotify_data.groupby(['user_id'])[\"playlistname\"].nunique().reset_index()\n### Just for better visualisation remove very high playlist\nfig, ax = plt.subplots(2, figsize=(6,8))\nsns.distplot(spotify_user_summary['playlistname'], hist=True,ax = ax[0])\nax[0].set_title(\"Number of playlist per user\")\nspotify_user_summary = spotify_user_summary[spotify_user_summary['playlistname'] <= 50]\nsns.distplot(spotify_user_summary['playlistname'], hist=True,ax=ax[1])\nax[1].set_title(\"Number of playlist per user (where <=50)\")\nplt.show()","5359e9cd":"### Lets first look at the distribution of number of playlist by user_id\nspotify_user_summary = spotify_data.groupby(['user_id'])[\"artistname\"].nunique().reset_index()\n\n### Just for better visualisation remove very high playlist\nfig, ax = plt.subplots(2, figsize=(6,8))\nsns.distplot(spotify_user_summary[\"artistname\"], hist=True,ax = ax[0])\nax[0].set_title(\"Number of artist per user\")\nspotify_user_summary = spotify_user_summary[spotify_user_summary[\"artistname\"] <= 500]\nsns.distplot(spotify_user_summary[\"artistname\"], hist=True,ax=ax[1])\nax[1].set_title(\"Number of artist per user (where <=500)\")\nplt.show()","56126b18":"### Insight 2\n###### Most of the users listen to less number of artist as compared higher number of artist\n###### Also We see a few users which may have very high number of playlists ","2d6afbcf":"### Lets first look at the distribution of number of playlist by user_id\nspotify_user_summary = spotify_data.groupby(['user_id'])[\"trackname\"].nunique().reset_index()\n\n### Just for better visualisation remove very high playlist\nfig, ax = plt.subplots(2, figsize=(6,8))\nsns.distplot(spotify_user_summary[\"trackname\"], hist=True,ax = ax[0])\nax[0].set_title(\"Number of tracks per user\")\nspotify_user_summary = spotify_user_summary[spotify_user_summary[\"trackname\"] <= 500]\nsns.distplot(spotify_user_summary[\"trackname\"], hist=True,ax=ax[1])\nax[1].set_title(\"Number of tracks per user (where <=500)\")\nplt.show()","6048067a":"### Lets first look at the distribution of number of playlist by user_id\nspotify_user_summary = spotify_data.groupby(['user_id'])[\"trackname\",\"artistname\"].nunique().reset_index()\nspotify_user_summary = spotify_user_summary[spotify_user_summary['artistname']>0]\n\nspotify_user_summary['Track_to_artist_ratio'] = spotify_user_summary['trackname'] \/ spotify_user_summary['artistname']\nspotify_user_summary.dropna(inplace = True)\nspotify_user_summary['Track_to_artist_ratio'] = spotify_user_summary['Track_to_artist_ratio'].astype(int)\n\n### Just for better visualisation remove very high playlist\nfig, ax = plt.subplots(2, figsize=(6,8))\nsns.distplot(spotify_user_summary[\"Track_to_artist_ratio\"], hist=True,ax = ax[0])\nax[0].set_title(\"AVerage tracks per artist\")\nspotify_user_summary = spotify_user_summary[spotify_user_summary[\"Track_to_artist_ratio\"] <= 40]\nsns.distplot(spotify_user_summary[\"Track_to_artist_ratio\"], hist=True,ax=ax[1])\nax[1].set_title(\"Number of tracks per artist (where <=40)\")\nplt.show()","26228512":"%matplotlib inline\n### Lets have a look at most common playlist\n## We could have aslo created a word cloud\nspotify_data['playlistname'].value_counts()[3:53].plot(kind= 'bar',title=\"Most Common Playlist names\",figsize = (16,6))","8eddf8c7":"%matplotlib inline\n### Lets have a look at most common playlist\n## We could have aslo created a word cloud\nspotify_data['artistname'].value_counts()[0:51].plot(kind= 'bar',title=\"Top 50 Most Popular Artist \",figsize = (15,6))","3ebc69db":"%matplotlib inline\n### Lets have a look at most common playlist\n## We could have aslo created a word cloud\nspotify_data['trackname'].value_counts()[0:51].plot(kind= 'bar',title=\"Top 50 Most Popular Songs  \",figsize = (15,6))","3dae450b":"### Now lets define function which creates a dictionary and convert songs names to dictionary\n\ntracklist = spotify_data['trackname'].unique()","1e2b1d8d":"spotify_data.dropna(inplace = True)","44bed287":"### Create a function which takes a dataset name and column name \n\ndef create_dict(dataset, column):\n    ''' Takes two input from user column name and dataset name and return dictionary with hash map '''\n    unique_list = dataset[column].unique()\n    out_dict = {}\n    out_dict1 = {}\n    \n    for j,i in enumerate(unique_list):\n        out_dict[i.lower()] = str(j)\n        out_dict1[str(j)] = i.lower()\n        \n    print (\"Number of distinct in vocab is :\",j)\n    return (out_dict,out_dict1)","b7851b31":"### call the dict functions on track names and artistname\ntrack_map, track_map_comp= create_dict(spotify_data,'trackname')\nartist_map,artist_map_comp = create_dict(spotify_data,'artistname')","fb2dd9ac":"with open('track_map_dict.pickle','wb') as track_file:\n    pickle.dump(track_map,track_file)\nwith open('track_map_comp_dict.pickle','wb') as track_file_comp:\n    pickle.dump(track_map_comp,track_file_comp)","c8b9fc73":"with open('artist_map_dict.pickle','wb') as artist_file:\n    pickle.dump(artist_map,artist_file)\nwith open('artist_map_comp_dict.pickle','wb') as artist_file_comp:\n    pickle.dump(artist_map_comp,artist_file_comp)","f8bc6836":"### Lets shuffle the data first\nprint (\"Shape of data before sampling is:\", spotify_data.shape)\nspotify_data.sample(frac = 1,  random_state = 10000).reset_index(drop=True)\nprint (\"Shape of data after sampling is :\", spotify_data.shape)","8b45916a":"### Load the pickle files stored for song to numeric \nwith open('track_map_dict.pickle','rb') as dict1:\n    track_dict= pickle.load( dict1)\nprint (\"Track dict has {} observations\".format(len(track_dict)))\n#### Load the prcikle file for artist to numeric\nwith open('artist_map_dict.pickle','rb') as dict2:\n    artist_dict = pickle.load(dict2)\nprint (\"Track dict has {} observations\".format(len(artist_dict)))","5276d77a":"### Now we will use this mapping to convert names to numeric\nprint (\"Data before mapping dict :\", spotify_data.head(5))\nspotify_data['trackname'] = spotify_data['trackname'].str.lower().map(track_dict)\nspotify_data['artistname'] = spotify_data['artistname'].str.lower().map(artist_dict)\nprint (\"Data after mapping dict :\")\nprint (spotify_data.head(5))","8a66832a":"### We want to create a list of songs in zip file \ndef zip_list(x):\n    return ([str(z) for z in x])","f1da9a7b":"spotify_summary = spotify_data.groupby(['user_id','playlistname'])['trackname'].apply(zip_list).reset_index()","79b23ca9":"print (\" Distinct playlist after summarizing the data is :\",spotify_summary.shape[0])\nprint (\" The data looks like this :\")\nprint (spotify_summary.head(5))","27e13ef1":"### We will Dump this data in the pickle file and work in it later\nwith open(\"spotify_summary.pickle\",'wb') as pick_data:\n    pickle.dump(spotify_summary,pick_data)\n    print (\"The dataset is pickled at \",os.getcwd())","152302b0":"### Data Processing For Word2vec models \n##### 1. Convert each artist name & song name to numeric using the dictionary first\n##### 2. Roll up the data at User_id and Playlist level, and store songss in a playlist as list. Before doing that we will like to shuffle the datasets\n##### 3. Train a word 2 vector model, and see how it works ","118a76c3":"### Insight 3 \n##### Majority of customers at an average listen a few songs from different artists\n##### But we do have custmers who listen song a lot of songs from artists","bd5b0b18":"### Objective : In this notebook we will play around with the spotify datasets and do the following things\n                \n    1. Do EDA on data to understand the size of the data\n    2. To reduce memory usage take each distinct Song and give it a numeric value \n    3. Convert each playlist of the user into sequential data by randomly grouping them based playlist name\n    4. Hypothesis is simple that each song inplaylist can be played randomly","292cf9dd":"### We created two dicts as will first need to convert songs to numeric mapping and after we have trained the model we will return numeric to song mapping","c82f4cf3":"#### Spotify playlists dataset\n\n\nThis dataset is based on the subset of users in the #nowplaying dataset who publish their #nowplaying tweets via Spotify. In principle, the dataset holds users, their playlists and the tracks contained in these playlists. \n\nThe csv-file holding the dataset contains the following columns: \n\"user_id\", \"artistname\", \"trackname\", \"playlistname\"\n, where\n- user_id is a hash of the user's Spotify user name\n- artistname is the name of the artist\n- trackname is the title of the track and\n- playlistname is the name of the playlist that contains this track.\n\nThe separator used is , each entry is enclosed by double quotes and the escape character used is \\.\n\n\n\nA description of the generation of the dataset and the dataset itself can be found in the following paper:\n\nPichl, Martin; Zangerle, Eva; Specht, G\u00fcnther: \"Towards a Context-Aware Music Recommendation Approach: What is Hidden in the Playlist Name?\" in 15th IEEE International Conference on Data Mining Workshops (ICDM 2015), pp. 1360-1365, IEEE, Atlantic City, 2015.","d3fb0843":"### Insight 1 \n###### Above Plot shows that data is skewed toward smaller number of playlist. Which makes logical sense.\n###### Also We see a few users which may have very high number of playlists","d35abe11":"#### Lets group by data based on the user id and playlist and zip it as a list","b124fbee":"### Lets look at the few stats about the data"}}