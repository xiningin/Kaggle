{"cell_type":{"d0b5e5e0":"code","f1d0ad9a":"code","03a50917":"code","221f1620":"code","1765cba7":"code","f9b98162":"code","564a45f0":"code","f858e5e6":"code","a5d47326":"code","9d035d23":"code","379da753":"code","b9e12c9f":"code","6350864a":"code","c2c5c39d":"code","dc92aefa":"code","26f93f27":"code","c2ed95a1":"code","1e5977de":"code","4aa09169":"code","ef7bf771":"code","da9975b9":"markdown","eea3ff93":"markdown","8925e58d":"markdown","1d4124d5":"markdown","0e46180b":"markdown","d50105e6":"markdown","54b4a176":"markdown","e5690968":"markdown","263c37c2":"markdown","83a8075d":"markdown","b404bed8":"markdown","4ac12971":"markdown","247d18a4":"markdown","849f4cef":"markdown","50539b73":"markdown","b6cc7033":"markdown","68d8897b":"markdown","7bc2506c":"markdown","a484bdbc":"markdown"},"source":{"d0b5e5e0":"!conda install gdcm -c conda-forge -y\n!pip install pycocotools numpy opencv-python tqdm tensorboard tensorboardX pyyaml webcolors matplotlib","f1d0ad9a":"!git clone https:\/\/github.com\/zylo117\/Yet-Another-EfficientDet-Pytorch\n\nimport os\nos.chdir(\"Yet-Another-EfficientDet-Pytorch\")","03a50917":"# load checkpoint\n! mkdir weights\n! wget https:\/\/github.com\/zylo117\/Yet-Another-EfficientDet-Pytorch\/releases\/download\/1.0\/efficientdet-d0.pth -O weights\/efficientdet-d0.pth","221f1620":"siim_yml = '''\nproject_name: siim  # also the folder name of the dataset that under data_path folder\ntrain_set: train\nval_set: val\nnum_gpus: 1\n\n# mean and std in RGB order, actually this part should remain unchanged as long as your dataset is similar to coco.\nmean: [ 0.485, 0.456, 0.406 ]\nstd: [ 0.229, 0.224, 0.225 ]\n\n# this anchor is adapted to the dataset\nanchors_scales: '[2 ** 0, 2 ** (1.0 \/ 3.0), 2 ** (2.0 \/ 3.0)]'\nanchors_ratios: '[(1.0, 1.0), (1.3, 0.8), (1.9, 0.5)]'\n\nobj_list: ['typical', 'indeterminate', 'atypical']\n'''\nwith open('projects\/siim.yml', 'w') as f:\n    f.write(siim_yml)","1765cba7":"import os\nfrom PIL import Image\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport numpy as np\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\n\nimport torch\n\ndef read_xray(path, voi_lut=False, fix_monochrome=True):\n    # Original from: https:\/\/www.kaggle.com\/raddar\/convert-dicom-to-np-array-the-correct-way\n    dicom = pydicom.read_file(path)\n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to\n    # \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n\n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n\n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    data = (data * 255).astype(np.uint8)\n\n    return data\n\n\ndef resize(array, size, keep_ratio=False, resample=Image.LANCZOS):\n    # Original from: https:\/\/www.kaggle.com\/xhlulu\/vinbigdata-process-and-resize-to-image\n    im = Image.fromarray(array)\n\n    if keep_ratio:\n        im.thumbnail((size, size), resample)\n    else:\n        im = im.resize((size, size), resample)\n\n    return im","f9b98162":"from glob import glob\nINPUT_PATH = \"\/kaggle\/input\/siim-covid19-detection\/\"\n\nfor split in [\"test\", \"train\"]:\n    save_dir = f\"datasets\/siim\/{split}\/\"\n\n    os.makedirs(save_dir, exist_ok=True)\n\n    for path in tqdm(glob(INPUT_PATH + split + '\/*\/*\/*')):\n        # set keep_ratio=True to have original aspect ratio\n        xray = read_xray(path)\n        im = resize(xray, size=256)\n        im.save(os.path.join(save_dir, path.split('\/')[-1][:-3]+'jpg'))","564a45f0":"import os\nimport pandas as pd\nfrom glob import glob\nimport pydicom","f858e5e6":"train_study = pd.read_csv(INPUT_PATH + 'train_study_level.csv')\ntrain_image = pd.read_csv(INPUT_PATH + 'train_image_level.csv')","a5d47326":"train_study.head()","9d035d23":"train_study = train_study.rename(columns = {\n    'Negative for Pneumonia': 'Negative', 'Typical Appearance': 'Typical',\n    'Indeterminate Appearance': 'Indeterminate', 'Atypical Appearance': 'Atypical'},\n                                       inplace = False)\ntrain_study['StudyInstanceUID'] = train_study['id'].str[:-6]\ntrain_study.drop(columns=['id'], inplace=True)\ntrain_study.head()","379da753":"train_image.head()","b9e12c9f":"train_image = train_image.merge(train_study, on='StudyInstanceUID')\ntrain_image['id'] = train_image['id'].str[:-6]\ntrain_image.head()","6350864a":"id_path = []\nfor i in glob('\/kaggle\/input\/siim-covid19-detection\/train\/*\/*\/*'):\n    id_path.append((i, i.split('\/')[-1][:-4]))\nid_path = pd.DataFrame(id_path, columns=['path', 'id'])\ntrain_image = train_image.merge(id_path, on='id')\ntrain_image.head()","c2c5c39d":"train_image.iloc[0]['boxes']","dc92aefa":"train_image.iloc[0]['label']","26f93f27":"pydicom.read_file(train_image.iloc[0]['path']).pixel_array.shape","c2ed95a1":"xy = []\nfor i, data in train_image.iterrows():\n    xy.append(pydicom.read_file(data['path']).pixel_array.shape)\ntrain_image[['xcell','ycell']] = xy\ntrain_image.to_csv('datasets\/train_image.csv', index=None)\ntrain_image.head()","1e5977de":"import random\nimport os\nimport shutil\nimport pandas as pd\nimport json\n\nrandom.seed(481)\nSRC_PATH = 'datasets\/siim\/train\/'\nTRG_PATH = 'datasets\/siim\/'\ntrain_list = os.listdir(SRC_PATH)\nrandom.shuffle(train_list)\n\nimport shutil\nos.makedirs(TRG_PATH+'val', exist_ok=True)\nfor path in train_list[int(len(train_list)*0.8):]:\n    shutil.move(SRC_PATH + path, TRG_PATH + 'val\/' + path)","4aa09169":"def anno(sets='train'):\n    image_id = pd.DataFrame(os.listdir(TRG_PATH + sets))[0].str[:-4].values.tolist()\n    annotation = {}\n    annotation['type'] = 'instances'\n    annotation['categories'] = []\n    annotation['images'] = []\n    annotation['annotations'] = []\n    annotation['categories'].append({'supercategory': 'none', 'id': 1, 'name': 'typical'})\n    annotation['categories'].append({'supercategory': 'none', 'id': 2, 'name': 'indeterminate'})\n    annotation['categories'].append({'supercategory': 'none', 'id': 3, 'name': 'atypical'})\n    for i, data in train_image[train_image.id.isin(image_id)].iterrows():\n        dic = {}\n        dic['file_name'] = data['id']+'.jpg'\n        dic['height'] = 256\n        dic['width'] = 256\n        dic['id'] = data.name + 1\n        annotation['images'].append(dic)\n        cnt = 1\n\n    for i, data in train_image.iterrows():\n        if type(data['boxes']) == float: # nan\n            continue\n        # split box string\n        boxes = json.loads(data['boxes'].replace('\\'', '\\\"'))\n        \n        # reverse x,y cell count\n        ycell, xcell = data['xcell'], data['ycell']\n        \n        # category\n        t, i, a = data['Typical'], data['Indeterminate'], data['Atypical']\n        if t==1:\n            category = 1\n        elif i==1:\n            category = 2\n        elif a == 1:\n            category = 3\n        \n        # add boxes\n        for j in boxes:\n            dic = {}\n            dic['area'] = (j['width']*256)\/\/xcell * (j['height']*256)\/\/ycell\n            dic['iscrowd'] = 0\n            dic['image_id'] = data.name + 1\n            dic['bbox'] = [(j['x']*256)\/\/xcell, (j['y']*256)\/\/ycell,\n                        (j['width']*256)\/\/xcell, (j['height']*256)\/\/ycell]\n            dic['category_id'] = category\n            dic['id'] = cnt\n            dic['ignore'] = 0\n            dic['segmentation'] = []\n            cnt += 1\n            annotation['annotations'].append(dic)\n            \n    # save annotation json files\n    with open(f'{TRG_PATH}annotations\/instances_{sets}.json', 'w') as f:\n        json.dump(annotation, f)","ef7bf771":"os.makedirs(TRG_PATH + 'annotations', exist_ok=True)\nanno('train')\nanno('val')","da9975b9":"Make the validation set.","eea3ff93":"Clone EfficientDet repository. In this repo, we can training after preprocessing **pre-trained weights**, **yml file**, **image files(not dcm)**, **annotation files**.","8925e58d":"This dataframe's id is too long, and merge train_study dataframe.","1d4124d5":"# Finished!","0e46180b":"We can know the image size in the same way as above.","d50105e6":"We need categories, images, annotations(box). Let's make this file.","54b4a176":"label column includes x1, y1, x2, y2.\n\n<img src=\"https:\/\/aihub-storage.s3.ap-northeast-2.amazonaws.com\/file\/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-07-08_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_8.54.13.png\" style=\"width:500px\">","e5690968":"The whole preprocessing process is complete. I will train on the next notebook with the files from here. Please wait for the next notebook. Thank you for read my notebook!","263c37c2":"# Annotation Files","83a8075d":"# Split train to train&val","b404bed8":"# Environment","4ac12971":"# Base Setting","247d18a4":"Add path of image.","849f4cef":"Let's check what the csv file looks like.","50539b73":"# Preprocessing","b6cc7033":"id is too long, and column name too. Let's shorten the name.","68d8897b":"boxes column includes x, y, width, height.\n\n<img src=\"https:\/\/aihub-storage.s3.ap-northeast-2.amazonaws.com\/file\/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-07-08_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_8.51.15.png\" style=\"width:500px\">","7bc2506c":"# EfficientDet\n\nARXIV [https:\/\/arxiv.org\/pdf\/1911.09070.pdf](https:\/\/arxiv.org\/pdf\/1911.09070.pdf)  \nGithub [https:\/\/github.com\/zylo117\/Yet-Another-EfficientDet-Pytorch](https:\/\/github.com\/zylo117\/Yet-Another-EfficientDet-Pytorch)  \n\nEfficientDet employs EfficientNet as the backbone network, BiFPN as the feature network, and shared class\/box prediction network. Both BiFPN layers and class\/box net layers are repeated multiple times based on different resource constraints.\n\n[Object Detection SOTA model](https:\/\/paperswithcode.com\/sota\/object-detection-on-coco)  \nThis page shows object detection models' score on COCO test-dev. I focus AP50 score, because this competitions' metric is AP50.    \n1. DyHead (Based Swin-L) : 78.5\n2. DetectoRS (Based ResNeXt) : 74.2\n3. YOLOv4-P7 (Based Scaled-YOLO) : 73.3\n4. EfficientDet-D7 (Based EfficientNet) : 72.4\n5. YOLOv4-608 (Based YOLO) : 65.7\n\n\n<figure>\n<img src=\"https:\/\/blog.roboflow.com\/content\/images\/2020\/06\/yolov5-performance.png\" style=\"width:700px\">\n    <figcaption>EfficientDet is better model than YOLOv5 on AP.<\/figcaption>\n<\/figure>\n\n\nEveryone used model based YOLOv4 or YOLOv5, but this model isn't SOTA model. I'll try EfficientDet first and then Scaled-YOLOv4, DyHead.  \n  \nThree notebooks summarize how to use this model.\n1. [Preprocessing](https:\/\/www.kaggle.com\/adldotori\/efficientdet-preprocessing-better-than-yolov5\/)\n2. [Training](https:\/\/www.kaggle.com\/adldotori\/efficientdet-training-better-than-yolov5\/)\n3. [Inference](https:\/\/www.kaggle.com\/adldotori\/efficientdet-inference-better-than-yolov5\/) - 2days later open!\n\nThis notebook is first notebook which includes how to preprocess the data.  \nLet's start!  \n\nThis picture shows the rough structure of efficientdet.\n![image](https:\/\/aihub-storage.s3.ap-northeast-2.amazonaws.com\/file\/efficientdet.png)","a484bdbc":"# Change to 256x256px Image"}}