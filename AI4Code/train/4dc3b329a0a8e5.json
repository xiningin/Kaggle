{"cell_type":{"37d91fc7":"code","e855d6e4":"code","1b049dea":"code","e886759c":"code","0ecf0eb6":"code","53b019f6":"code","9791f6b5":"code","4d6b0f7a":"code","d9203e38":"code","76d1256b":"code","640caa59":"markdown","341a296d":"markdown","ff20302d":"markdown","33b4d5e0":"markdown","be39f526":"markdown"},"source":{"37d91fc7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e855d6e4":"import numpy as np\nimport pandas as pd\nimport re\nimport nltk\nimport spacy\nimport string\nimport pprint\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize \nfrom nltk.probability import FreqDist","1b049dea":"data = pd.read_csv('..\/input\/artikeldatasetalwi\/artikel_alwi.csv')","e886759c":"df = data [['Bidang']]\ndf['Bidang'] = df['Bidang'].astype(str)\n\ndf = data [['Judul']]\ndf['Judul'] = df['Judul'].astype(str)\n\ndf = data [['Isi']]\ndf['Isi'] = df['Isi'].astype(str)\n\ndata.head()","0ecf0eb6":"df = pd.DataFrame(data[['Bidang','Judul','Isi']])\ndf","53b019f6":"df[\"Case_Folding_Bidang\"] = df['Bidang'].str.lower()\n\ndf[\"Case_Folding_Judul\"] = df['Judul'].str.lower()\n\ndf[\"Case_Folding_Isi\"] = df['Isi'].str.lower()\n\ndf = pd.DataFrame(df[['Case_Folding_Bidang', 'Case_Folding_Judul', 'Case_Folding_Isi']])\n\ndf","9791f6b5":"def remove_df_special(text):\n    text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n    text = text.encode('ascii', 'replace').decode('ascii')\n    text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\\/\\\/\\S+)\",\" \", text).split())\n    return text.replace(\"http:\/\/\", \" \").replace(\"https:\/\/\", \" \")\n                \ndf['Case_Folding_Isi'] = df['Case_Folding_Isi'].apply(remove_df_special)\n\ndef remove_df_number(text):\n    return  re.sub(r\"\\d+\", \"\", text)\n\ndf['Case_Folding_Isi'] = df['Case_Folding_Isi'].apply(remove_df_number)\n\ndef remove_df_punctuation(text):\n    return text.translate(str.maketrans(\"\",\"\",string.punctuation))\n\ndf['Case_Folding_Isi'] = df['Case_Folding_Isi'].apply(remove_df_punctuation)\n\ndef remove_df_whitespace(text):\n    return text.strip()\n\ndf['Case_Folding_Isi'] = df['Case_Folding_Isi'].apply(remove_df_whitespace)\n\ndef remove_df_whitespace_multiple(text):\n    return re.sub('\\s+',' ',text)\n\ndf['Case_Folding_Isi'] = df['Case_Folding_Isi'].apply(remove_df_whitespace_multiple)\n\ndef remove_df_singl_char(text):\n    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n\ndf['Case_Folding_Isi'] = df['Case_Folding_Isi'].apply(remove_df_singl_char)\n\ndef word_df_tokenize(text):\n    return word_tokenize(text)\n\ndf['df_tokenizing'] = df['Case_Folding_Isi'].apply(word_df_tokenize)\n\nprint('Tokenizing Isi Artikel : \\n') \nprint(df['df_tokenizing'].head())","4d6b0f7a":"def freqDist_wrapper(text):\n    return FreqDist(text)\n\ndf['df_jumlah_token'] = df['df_tokenizing'].apply(freqDist_wrapper)\n\nprint('Jumlah Token di Isi Artikel : \\n') \nprint(df['df_jumlah_token'].head().apply(lambda x : x.most_common()))","d9203e38":"rex_stopwords = stopwords.words('indonesian')\n\nrex_stopwords.extend([\"mn\", \"dg\", \"jg\", \"jga\", \"ny\", \"d\", 'kli', \n                       'masasi', 'kalo', 'biar', 'iya', 'bikin', \n                       'gak', 'ga', 'krn', 'bilang', 'nih', 'sih', \n                       'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', \n                       'jd', 'jgn', 'sdh', 'nya', 'n', 't', \n                       'nyg', 'hehe', 'pen', 'u', 'aja', 'loh', 'nan',\n                       '&amp', 'yah'])\n\nrex_stopwords = set(rex_stopwords)\n\ndef df_stopwords_removal(words):\n    return [word for word in words if word not in rex_stopwords]\n\ndf['df_stopword'] = df['df_tokenizing'].apply(df_stopwords_removal) \n\n\nprint('Stopword Isi Artikel : \\n') \nprint(df['df_stopword'].head())","76d1256b":"df.to_csv(\"Text Preprosessing - Muhammad Alwi.csv\")","640caa59":"# **TOKENIZE ~~~~~~~~** ","341a296d":"*melakukan perhitungan token yang ada di isi artikel*","ff20302d":"# **CASE FOLDING ~~~~~~~~~**","33b4d5e0":"# **EXPORT TO CSV ~~~~~~~~~**","be39f526":"# **STOPWORDS ~~~~~~~~~**"}}