{"cell_type":{"c401be3e":"code","35f5a4ec":"code","e3a928b5":"code","717325ad":"code","35f1ba58":"code","d8a06baf":"code","abe2482b":"code","51e73e43":"code","00b584eb":"code","207f6291":"code","05a368cc":"code","d1d9450e":"code","db3fa9ca":"code","b973d2ec":"code","c609e3ff":"code","6b2e1c05":"code","bf83ef95":"code","65a003d5":"code","e89576d7":"code","27bd9a46":"markdown"},"source":{"c401be3e":"import numpy as np\nimport cv2 \nimport matplotlib.pyplot as plt\nimport os","35f5a4ec":"#Reading data. Before running this code, the dataset should be added to the notebook using the Kaggle functionality (or by any other means). In this block we read the first image\n#and make sure everything works\npath = '..\/input\/cityscapes-image-pairs\/cityscapes_data\/train\/'\nflist = os.listdir(path)\nimg0 = cv2.imread(path+flist[0])\nplt.imshow(img0)\nprint(np.shape(img0))\nprint(len(flist))","e3a928b5":"#reading the actual images and forming them into the training dataset\nszy,szx,_ = np.shape(img0)\nN_ex = 1500\nN_bias = 0\nx_train = np.zeros((N_ex,szy,int(szx\/2),3))\ny_train = np.zeros((N_ex,szy,int(szx\/2),3))\nk = 0;\n\nfor f in flist[N_bias:N_bias+N_ex]:\n    x_train[k] = cv2.imread(path+f)[:,:256]\/256\n    y_train[k] = cv2.imread(path+f)[:,256:]\/256\n    k = k+1","717325ad":"plt.figure(figsize = (15,15))\nplt.subplot(1,2,1)\nplt.imshow(x_train[1])\nplt.subplot(1,2,2)\nplt.imshow(y_train[1])","35f1ba58":"#same for the validation data\npath = '..\/input\/cityscapes-image-pairs\/cityscapes_data\/val\/'\nflist = os.listdir(path)\nimg0 = cv2.imread(path+flist[0])\nN_val = 100\n\nszy,szx,_ = np.shape(img0)\nx_val = np.zeros((N_val,szy,int(szx\/2),3))\ny_val = np.zeros((N_val,szy,int(szx\/2),3))\nk = 0;\n\nfor f in flist[0:N_val]:\n    x_val[k] = cv2.imread(path+f)[:,:256]\/256\n    y_val[k] = cv2.imread(path+f)[:,256:]\/256\n    k = k+1","d8a06baf":"#u-net architecture\nimport tensorflow as tf\nimport keras\n\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers import Input, Dense, Dropout, Activation, Flatten, Convolution2D, MaxPooling2D, UpSampling2D, Concatenate\nfrom keras.layers.convolutional import Conv2D\nfrom keras.models import Sequential, Model, model_from_json, load_model\nfrom keras.regularizers import l2\n\n\n# define conv_factory: batch normalization + ReLU + Conv2D + Dropout (optional)\ndef conv_factory(x, concat_axis, nb_filter,\n                 dropout_rate=None, weight_decay=1E-4):\n    x = BatchNormalization(axis=concat_axis,\n                           gamma_regularizer=l2(weight_decay),\n                           beta_regularizer=l2(weight_decay))(x)\n    x = Activation('relu')(x)\n    x = Conv2D(nb_filter, (5, 5), dilation_rate=(2, 2),\n               kernel_initializer=\"he_uniform\",\n               padding=\"same\",\n               kernel_regularizer=l2(weight_decay))(x)\n    if dropout_rate:\n        x = Dropout(dropout_rate)(x)\n  \n    return x\n\n\n# define dense block: a nb_layers stack of conv_factory layers merged together\ndef denseblock(x, concat_axis, nb_layers, growth_rate, dropout_rate=None, weight_decay=1E-4):\n    list_feat = [x]\n    for i in range(nb_layers):\n        x = conv_factory(x, concat_axis, growth_rate,dropout_rate, weight_decay)\n        list_feat.append(x)\n    x = Concatenate(axis=concat_axis)(list_feat)\n\n    return x\n\n\n# define model U-net modified with dense block\ndef u_net():\n    dr = 0.5\n    nr = 2\n    mod_inputs = Input((256,256,3))\n    print(\"inputs shape:\", mod_inputs.shape) #input layer\n\n    conv1 = Conv2D(64\/nr, 3, activation='relu', padding='same', kernel_initializer='he_normal')(mod_inputs)\n    print(\"conv1 shape:\", conv1.shape)\n    db1 = denseblock(x=conv1, concat_axis=3, nb_layers=4, growth_rate=16, dropout_rate=dr)\n    print(\"db1 shape:\", db1.shape)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(db1)\n    print(\"pool1 shape:\", pool1.shape)\n\n    conv2 = Conv2D(128\/nr, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool1)\n    print(\"conv2 shape:\", conv2.shape)\n    db2 = denseblock(x=conv2, concat_axis=3, nb_layers=4, growth_rate=16, dropout_rate=dr)\n    print(\"db2 shape:\", db2.shape)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(db2)\n    print(\"pool2 shape:\", pool2.shape)\n\n    conv3 = Conv2D(256\/nr, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n    print(\"conv3 shape:\", conv3.shape)\n    db3 = denseblock(x=conv3, concat_axis=3, nb_layers=4, growth_rate=16, dropout_rate=dr)\n    print(\"db3 shape:\", db3.shape)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(db3)\n    print(\"pool3 shape:\", pool3.shape)\n\n    conv4 = Conv2D(512\/nr, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool3)\n    print(\"conv4 shape:\", conv4.shape)\n    db4 = denseblock(x=conv4, concat_axis=3, nb_layers=4, growth_rate=16, dropout_rate=dr)\n    print(\"db4 shape:\", db4.shape)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(db4)\n    print(\"pool4 shape:\", pool4.shape)\n#################this is the bottleneck######################################\n    conv5 = Conv2D(1024\/nr, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n    print(\"conv5 shape:\", conv5.shape)\n    db5 = denseblock(x=conv5, concat_axis=3, nb_layers=3, growth_rate=16, dropout_rate=dr)\n    print(\"db5 shape:\", db5.shape)\n    up5 = Conv2D(512\/nr, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(db5))\n    print(\"up5 shape:\", up5.shape)\n    merge5 = Concatenate(axis=3)([ BatchNormalization()(db4), BatchNormalization()( up5)]) #skip connection db4 to up5\n    print(\"merge5 shape:\", merge5.shape)\n\n    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge5)\n    print(\"conv6 shape:\", conv6.shape)\n    db6 = denseblock(x=conv6, concat_axis=3, nb_layers=3, growth_rate=16, dropout_rate=dr)\n    print(\"db5 shape:\", db6.shape)\n    up6 = Conv2D(256\/nr, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(db6))\n    print(\"up6 shape:\", up6.shape)\n    merge6 = Concatenate(axis=3)([BatchNormalization()(db3), BatchNormalization()(up6)]) #skip connection db3 to up6\n    print(\"merge6 shape:\", merge6.shape)\n\n    conv7 = Conv2D(256\/nr, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge6)\n    print(\"conv7 shape:\", conv7.shape)\n    db7 = denseblock(x=conv7, concat_axis=3, nb_layers=2, growth_rate=16, dropout_rate=dr)\n    print(\"db7 shape:\", db7.shape)\n    up7 = Conv2D(128\/nr, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n        UpSampling2D(size=(2, 2))(db7))\n    print(\"up7 shape:\", up7.shape)\n    merge7 = Concatenate(axis=3)([BatchNormalization()(db2), BatchNormalization()(up7)]) #skip connection db2 to up7\n    print(\"merge7 shape:\", merge7.shape)\n\n    conv8 = Conv2D(128\/nr, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge7)\n    print(\"conv8 shape:\", conv8.shape)\n    db8 = denseblock(x=conv8, concat_axis=3, nb_layers=2, growth_rate=16, dropout_rate=dr)\n    print(\"db8 shape:\", db8.shape)\n    up8 = Conv2D(64\/nr, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n        UpSampling2D(size=(2, 2))(db8))\n    print(\"up8 shape:\", up8.shape)\n    merge8 = Concatenate(axis=3)([BatchNormalization()(db1), BatchNormalization()(up8)]) #skip connection db1 to up8\n    print(\"merge8 shape:\", merge8.shape)\n\n    conv9 = Conv2D(64\/nr, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge8)\n    print(\"conv9 shape:\", conv9.shape)\n    db9 = denseblock(x=conv9, concat_axis=3, nb_layers=2, growth_rate=16, dropout_rate=dr)\n    print(\"db9 shape:\", db9.shape)\n    conv10 = Conv2D(32\/nr, 3, activation='relu', padding='same', kernel_initializer='he_normal')(db9) # final node layer\n    print(\"conv10 shape:\", conv10.shape)\n    conv11 = Conv2D(3, 1, activation='sigmoid')(conv10)  #output layer matched in size with the input\n    print(\"conv11 shape:\", conv11.shape)\n\n    model = Model(inputs=mod_inputs, outputs=conv11) \n    model.compile(optimizer='adam', loss = 'MSE')\n    \n    return model","abe2482b":"model = u_net()","51e73e43":"model.summary()","00b584eb":" hist = model.fit(x_train, y_train, epochs=20, shuffle = True, batch_size= 10, validation_data=(x_val, y_val))","207f6291":" pp = model.predict(x_val[0:20,:,:,:])","05a368cc":"#show the result\nni = 10\nfor k in range(ni):\n\n    plt.figure(figsize=(10,30))\n    plt.subplot(ni,3,1+k*3)\n    plt.imshow(x_val[k])\n    plt.subplot(ni,3,2+k*3)\n    plt.imshow(y_val[k])\n    plt.subplot(ni,3,3+k*3)\n    plt.imshow(pp[k])\n","d1d9450e":"#the same model, but without the skip connections\ndef u_net_wihtout_skips():\n    dr = 0.5\n    nr = 2\n    mod_inputs = Input((256,256,3))\n    print(\"inputs shape:\", mod_inputs.shape)\n\n    conv1 = Conv2D(64\/nr, 3, activation='relu', padding='same', kernel_initializer='he_normal')(mod_inputs)\n    print(\"conv1 shape:\", conv1.shape)\n    db1 = denseblock(x=conv1, concat_axis=3, nb_layers=4, growth_rate=16, dropout_rate=dr)\n    print(\"db1 shape:\", db1.shape)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(db1)\n    print(\"pool1 shape:\", pool1.shape)\n\n    conv2 = Conv2D(128\/nr, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool1)\n    print(\"conv2 shape:\", conv2.shape)\n    db2 = denseblock(x=conv2, concat_axis=3, nb_layers=4, growth_rate=16, dropout_rate=dr)\n    print(\"db2 shape:\", db2.shape)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(db2)\n    print(\"pool2 shape:\", pool2.shape)\n\n    conv3 = Conv2D(256\/nr, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n    print(\"conv3 shape:\", conv3.shape)\n    db3 = denseblock(x=conv3, concat_axis=3, nb_layers=4, growth_rate=16, dropout_rate=dr)\n    print(\"db3 shape:\", db3.shape)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(db3)\n    print(\"pool3 shape:\", pool3.shape)\n\n    conv4 = Conv2D(512\/nr, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool3)\n    print(\"conv4 shape:\", conv4.shape)\n    db4 = denseblock(x=conv4, concat_axis=3, nb_layers=4, growth_rate=16, dropout_rate=dr)\n    print(\"db4 shape:\", db4.shape)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(db4)\n    print(\"pool4 shape:\", pool4.shape)\n###############################################################################################################\n    conv5 = Conv2D(1024\/nr, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n    print(\"conv5 shape:\", conv5.shape)\n    db5 = denseblock(x=conv5, concat_axis=3, nb_layers=3, growth_rate=16, dropout_rate=dr)\n    print(\"db5 shape:\", db5.shape)\n    up5 = Conv2D(512\/nr, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(db5))\n    print(\"up5 shape:\", up5.shape)\n    #merge5 = Concatenate(axis=3)([ BatchNormalization()(db4), BatchNormalization()( up5)])\n    #print(\"merge5 shape:\", merge5.shape)\n\n    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(up5) #was merge5\n    print(\"conv6 shape:\", conv6.shape)\n    db6 = denseblock(x=conv6, concat_axis=3, nb_layers=3, growth_rate=16, dropout_rate=dr)\n    print(\"db5 shape:\", db6.shape)\n    up6 = Conv2D(256\/nr, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(db6))\n    print(\"up6 shape:\", up6.shape)\n    #merge6 = Concatenate(axis=3)([BatchNormalization()(db3), BatchNormalization()(up6)])\n    #print(\"merge6 shape:\", merge6.shape)\n\n    conv7 = Conv2D(256\/nr, 3, activation='relu', padding='same', kernel_initializer='he_normal')(up6)#was merge6\n    print(\"conv7 shape:\", conv7.shape)\n    db7 = denseblock(x=conv7, concat_axis=3, nb_layers=2, growth_rate=16, dropout_rate=dr)\n    print(\"db7 shape:\", db7.shape)\n    up7 = Conv2D(128\/nr, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n        UpSampling2D(size=(2, 2))(db7))\n    print(\"up7 shape:\", up7.shape)\n    #merge7 = Concatenate(axis=3)([BatchNormalization()(db2), BatchNormalization()(up7)])\n    #print(\"merge7 shape:\", merge7.shape)\n\n    conv8 = Conv2D(128\/nr, 3, activation='relu', padding='same', kernel_initializer='he_normal')(up7) #was merge7\n    print(\"conv8 shape:\", conv8.shape)\n    db8 = denseblock(x=conv8, concat_axis=3, nb_layers=2, growth_rate=16, dropout_rate=dr)\n    print(\"db8 shape:\", db8.shape)\n    up8 = Conv2D(64\/nr, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n        UpSampling2D(size=(2, 2))(db8))\n    print(\"up8 shape:\", up8.shape)\n    #merge8 = Concatenate(axis=3)([BatchNormalization()(db1), BatchNormalization()(up8)])\n    #print(\"merge8 shape:\", merge8.shape)\n\n    conv9 = Conv2D(64\/nr, 3, activation='relu', padding='same', kernel_initializer='he_normal')(up8)#was merge8\n    print(\"conv9 shape:\", conv9.shape)\n    db9 = denseblock(x=conv9, concat_axis=3, nb_layers=2, growth_rate=16, dropout_rate=dr)\n    print(\"db9 shape:\", db9.shape)\n    conv10 = Conv2D(32\/nr, 3, activation='relu', padding='same', kernel_initializer='he_normal')(db9)\n    print(\"conv10 shape:\", conv10.shape)\n    conv11 = Conv2D(3, 1, activation='sigmoid')(conv10)\n    print(\"conv11 shape:\", conv11.shape)\n\n    model = Model(inputs=mod_inputs, outputs=conv11)\n    model.compile(optimizer='adam', loss = 'MSE')\n    \n    return model","db3fa9ca":"model_nc = u_net_wihtout_skips()","b973d2ec":"model_nc.summary()","c609e3ff":"his_nc = model_nc.fit(x_train, y_train, epochs=20, shuffle = True, batch_size= 10, validation_data=(x_val, y_val))","6b2e1c05":" pp_nc = model_nc.predict(x_val[0:20,:,:,:])","bf83ef95":"ni = 10\nfor k in range(ni):\n\n    plt.figure(figsize=(10,30))\n    plt.subplot(ni,4,1+k*4)\n    plt.imshow(x_val[k])\n    plt.subplot(ni,4,2+k*4)\n    plt.imshow(y_val[k])\n    plt.subplot(ni,4,3+k*4)\n    plt.imshow(pp_nc[k]) #without skips\n    plt.subplot(ni,4,4+k*4)\n    plt.imshow(pp[k]) #with skips\n","65a003d5":"#training mse\nplt.plot(hist.history['loss'])\nplt.plot(his_nc.history['loss'])","e89576d7":"#validation mse\nplt.plot(hist.history['val_loss'])\nplt.plot(his_nc.history['val_loss'])","27bd9a46":"**U-net ANN**\n\nIn this notebook we train a complicated network with the help of the skip connections like in Res-Net. \nThe purpose of the network is image segmentation, i.e. identification of particular shapes in the images. We use the https:\/\/www.cityscapes-dataset.com\/ dataset, uploaded to Kaggle https:\/\/www.kaggle.com\/dansbecker\/cityscapes-image-pairs. This dataset has 2975 training images files and 500 validation image files. Each image file is 256x512 pixels, and each file is a composite with the original photo on the left half of the image, alongside the labeled image (output of semantic segmentation) on the right half."}}