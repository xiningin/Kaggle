{"cell_type":{"d7f5ad45":"code","93b4622f":"code","66ff6707":"code","1af8167c":"code","4482d2f6":"code","6745439c":"code","ac0bc69c":"code","256f31d8":"code","b6e11239":"code","631a06ea":"code","63f89de1":"code","4f2a2e35":"code","77bf69ea":"code","b54a65c0":"code","ac35b2e5":"code","5b7d1637":"code","0d08f330":"code","a6b22a76":"code","71c9dd2e":"code","6dd88383":"code","21fe5516":"code","c7dc5eba":"code","70d77475":"code","fc0763a2":"code","8d2e2df7":"code","c922a023":"code","df536873":"code","af61ac2d":"code","468e292c":"code","77592d39":"code","844ec56f":"code","f1d9fb87":"code","ed3f1e23":"code","d94556b7":"code","a2c47fe4":"code","4faecd77":"code","678248ef":"code","b0ed1c6a":"code","387be099":"code","c005dfb8":"code","62bd12cc":"code","94c64674":"code","8a9cab7b":"code","66277f39":"code","f3848c94":"code","ab8277da":"code","9adf4071":"code","591fc5ef":"code","6a03f17a":"code","4f227592":"code","2a74e902":"code","747ee0f1":"code","73b21059":"code","20a13e64":"markdown","13625183":"markdown","76444a9b":"markdown","c90e8bf0":"markdown","78da1b30":"markdown","8e97b0d9":"markdown","2ff23c4b":"markdown","afced88f":"markdown","97424990":"markdown","a792b414":"markdown","fcc95c66":"markdown","9b06254d":"markdown","f757fd6f":"markdown","40fde38f":"markdown","1b46195e":"markdown","c3f1d8fa":"markdown","cf07b4dc":"markdown","ae8575d0":"markdown","744fe4f2":"markdown","00971d00":"markdown","00e6a771":"markdown","b62c0a99":"markdown","dd8653d5":"markdown","7f0d0672":"markdown","a2a9eafa":"markdown","821e38e9":"markdown","14a28eaf":"markdown","f033ac02":"markdown","b001116c":"markdown","086702ca":"markdown","b908bb38":"markdown","50b12cbe":"markdown","cd6dfc08":"markdown","92e6dab5":"markdown"},"source":{"d7f5ad45":"# Common libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Restrict minor warnings\nimport warnings\nwarnings.filterwarnings('ignore')","93b4622f":"# Import test and train data\ndf_train = pd.read_csv('..\/input\/train.csv')\ndf_Test = pd.read_csv('..\/input\/test.csv')\ndf_test = df_Test","66ff6707":"# First 5 data points\ndf_train.head()","1af8167c":"# Datatypes of the attributes\ndf_train.dtypes","4482d2f6":"pd.set_option('display.max_columns', None) # we need to see all the columns\ndf_train.describe()","6745439c":"# From both train and test data\ndf_train = df_train.drop(['Soil_Type7', 'Soil_Type15'], axis = 1)\ndf_test = df_test.drop(['Soil_Type7', 'Soil_Type15'], axis = 1)\n\n# Also drop 'Id'\ndf_train = df_train.iloc[:,1:]\ndf_test = df_test.iloc[:,1:]","ac0bc69c":"size = 10\ncorrmat = df_train.iloc[:,:size].corr()\nf, ax = plt.subplots(figsize = (10,8))\nsns.heatmap(corrmat,vmax=0.8,square=True);","256f31d8":"data = df_train.iloc[:,:size]\n\n# Get name of the columns\ncols = data.columns\n\n# Calculate the pearson correlation coefficients for all combinations\ndata_corr = data.corr()\n\n# Threshold ( only highly correlated ones matter)\nthreshold = 0.5\ncorr_list = []","b6e11239":"data_corr","631a06ea":"# Sorting out the highly correlated values\nfor i in range(0, size):\n    for j in range(i+1, size):\n        if data_corr.iloc[i,j]>= threshold and data_corr.iloc[i,j]<1\\\n        or data_corr.iloc[i,j] <0 and data_corr.iloc[i,j]<=-threshold:\n            corr_list.append([data_corr.iloc[i,j],i,j])\n        ","63f89de1":"# Sorting the values\ns_corr_list = sorted(corr_list,key= lambda x: -abs(x[0]))\n\n# print the higher values\nfor v,i,j in s_corr_list:\n    print(\"%s and %s = %.2f\" % (cols[i], cols[j], v))","4f2a2e35":"df_train.iloc[:,:10].skew()","77bf69ea":"# Pair wise scatter plot with hue being 'Cover_Type'\nfor v,i,j in s_corr_list:\n    sns.pairplot(data = df_train, hue='Cover_Type', size= 6, x_vars=cols[i], y_vars=cols[j])\n    plt.show()\n    ","b54a65c0":"# A violin plot is a hybrid of a box plot and a kernel density plot, which shows peaks in the data.\ncols = df_train.columns\nsize = len(cols) - 1 # We don't need the target attribute\n# x-axis has target attributes to distinguish between classes\nx = cols[size]\ny = cols[0:size]\n\nfor i in range(0, size):\n    sns.violinplot(data=df_train, x=x, y=y[i])\n    plt.show()","ac35b2e5":"df_train.Wilderness_Area2.value_counts()","5b7d1637":"### Group one-hot encoded variables of a category into one single variable\ncols = df_train.columns\nr,c = df_train.shape\n\n# Create a new dataframe with r rows, one column for each encoded category, and target in the end\nnew_data = pd.DataFrame(index= np.arange(0,r), columns=['Wilderness_Area', 'Soil_Type', 'Cover_Type'])\n\n# Make an entry in data for each r for category_id, target_value\nfor i in range(0,r):\n    p = 0;\n    q = 0;\n    # Category1_range\n    for j in range(10,14):\n        if (df_train.iloc[i,j] == 1):\n            p = j-9 # category_class\n            break\n    # Category2_range\n    for k in range(14,54):\n        if (df_train.iloc[i,k] == 1):\n            q = k-13 # category_class\n            break\n    # Make an entry in data for each r\n    new_data.iloc[i] = [p,q,df_train.iloc[i, c-1]]\n    \n# plot for category1\nsns.countplot(x = 'Wilderness_Area', hue = 'Cover_Type', data = new_data)\nplt.show()\n\n# Plot for category2\nplt.rc(\"figure\", figsize = (25,10))\nsns.countplot(x='Soil_Type', hue = 'Cover_Type', data= new_data)\nplt.show()","0d08f330":"# Checking the value count for different soil_types\nfor i in range(10, df_train.shape[1]-1):\n    j = df_train.columns[i]\n    print (df_train[j].value_counts())","a6b22a76":"# Let's drop them\ndf_train = df_train.drop(['Soil_Type8', 'Soil_Type25'], axis=1)\ndf_test = df_test.drop(['Soil_Type8', 'Soil_Type25'], axis=1)\ndf_train1 = df_train # To be used for algos like SVM where we need normalization and StandardScaler\ndf_test1 = df_test # To be used under normalization and StandardScaler","71c9dd2e":"# Checking for data transformation (take only non-categorical values)\ndf_train.iloc[:,:10].skew()","6dd88383":"#Horizontal_Distance_To_Hydrology\nfrom scipy import stats\nplt.figure(figsize=(8,6))\nsns.distplot(df_train1['Horizontal_Distance_To_Hydrology'], fit = stats.norm)\nfig = plt.figure(figsize=(8,6))\nres = stats.probplot(df_train1['Horizontal_Distance_To_Hydrology'], plot=plt)","21fe5516":"df_train1['Horizontal_Distance_To_Hydrology'] = np.sqrt(df_train1['Horizontal_Distance_To_Hydrology'])","c7dc5eba":"# Plot again after sqrt transformation\nplt.figure(figsize=(8,6))\nsns.distplot(df_train1['Horizontal_Distance_To_Hydrology'], fit = stats.norm)\nfig = plt.figure(figsize=(8,6))\nres = stats.probplot(df_train1['Horizontal_Distance_To_Hydrology'], plot=plt)","70d77475":"#Vertical_Distance_To_Hydrology\nplt.figure(figsize=(8,6))\nsns.distplot(df_train1['Vertical_Distance_To_Hydrology'], fit = stats.norm)\nfig = plt.figure(figsize=(8,6))\nres = stats.probplot(df_train1['Vertical_Distance_To_Hydrology'], plot=plt)","fc0763a2":"#Horizontal_Distance_To_Roadways\nplt.figure(figsize=(8,6))\nsns.distplot(df_train1['Horizontal_Distance_To_Roadways'], fit=stats.norm)\nfig = plt.figure(figsize=(8,6))\nres = stats.probplot(df_train1['Horizontal_Distance_To_Roadways'], plot=plt)","8d2e2df7":"df_train1['Horizontal_Distance_To_Roadways'] = np.sqrt(df_train1['Horizontal_Distance_To_Roadways'])","c922a023":"# Plot again after sqrt transformation\nplt.figure(figsize=(8,6))\nsns.distplot(df_train1['Horizontal_Distance_To_Roadways'], fit = stats.norm)\nfig = plt.figure(figsize=(8,6))\nres = stats.probplot(df_train1['Horizontal_Distance_To_Roadways'], plot=plt)","df536873":"#Hillshade_9am\nfig = plt.figure(figsize=(8,6))\nsns.distplot(df_train1['Hillshade_9am'],fit=stats.norm)\nfig = plt.figure(figsize=(8,6))\nres = stats.probplot(df_train1['Hillshade_9am'],plot=plt)","af61ac2d":"df_train1['Hillshade_9am'] = np.square(df_train1['Hillshade_9am'])","468e292c":"# Plot again after square transformation\nplt.figure(figsize=(8,6))\nsns.distplot(df_train1['Hillshade_9am'], fit = stats.norm)\nfig = plt.figure(figsize=(8,6))\nres = stats.probplot(df_train1['Hillshade_9am'], plot=plt)","77592d39":"# Hillshade_Noon\nfig = plt.figure(figsize=(8,6))\nsns.distplot(df_train1['Hillshade_Noon'],fit=stats.norm)\nfig = plt.figure(figsize=(8,6))\nres = stats.probplot(df_train1['Hillshade_Noon'],plot=plt)","844ec56f":"df_train1['Hillshade_Noon'] = np.square(df_train1['Hillshade_Noon'])","f1d9fb87":"# Plot again after square transformation\nfig = plt.figure(figsize=(8,6))\nsns.distplot(df_train1['Hillshade_Noon'],fit=stats.norm)\nfig = plt.figure(figsize=(8,6))\nres = stats.probplot(df_train1['Hillshade_Noon'],plot=plt)","ed3f1e23":"# Horizontal_Distance_To_Fire_Points\nplt.figure(figsize=(8,6))\nsns.distplot(df_train1['Horizontal_Distance_To_Fire_Points'], fit=stats.norm)\nplt.figure(figsize=(8,6))\nres = stats.probplot(df_train1['Horizontal_Distance_To_Fire_Points'],plot=plt)","d94556b7":"df_train1['Horizontal_Distance_To_Fire_Points'] = np.sqrt(df_train1['Horizontal_Distance_To_Fire_Points'])","a2c47fe4":"# Plot again after sqrt transformation\nplt.figure(figsize=(8,6))\nsns.distplot(df_train1['Horizontal_Distance_To_Fire_Points'], fit=stats.norm)\nplt.figure(figsize=(8,6))\nres = stats.probplot(df_train1['Horizontal_Distance_To_Fire_Points'],plot=plt)","4faecd77":"# To be used in case of algorithms like SVM\ndf_test1[['Horizontal_Distance_To_Hydrology','Horizontal_Distance_To_Fire_Points'\\\n        ,'Horizontal_Distance_To_Roadways']] = np.sqrt(df_test1[['Horizontal_Distance_To_Hydrology',\\\n        'Horizontal_Distance_To_Fire_Points','Horizontal_Distance_To_Roadways']])","678248ef":"# To be used in case of algorithms like SVM\ndf_test1[['Hillshade_9am','Hillshade_Noon']] = np.square(df_test1[['Hillshade_9am','Hillshade_Noon']])","b0ed1c6a":"from sklearn.preprocessing import StandardScaler","387be099":"# Taking only non-categorical values\nSize = 10\nX_temp = df_train.iloc[:,:Size]\nX_test_temp = df_test.iloc[:,:Size]\nX_temp1 = df_train1.iloc[:,:Size]\nX_test_temp1 = df_test1.iloc[:,:Size]\n\nX_temp1 = StandardScaler().fit_transform(X_temp1)\nX_test_temp1 = StandardScaler().fit_transform(X_test_temp1)","c005dfb8":"r,c = df_train.shape\nX_train = np.concatenate((X_temp,df_train.iloc[:,Size:c-1]),axis=1)\nX_train1 = np.concatenate((X_temp1, df_train1.iloc[:,Size:c-1]), axis=1) # to be used for SVM\ny_train = df_train.Cover_Type.values","62bd12cc":"from sklearn import svm\nfrom sklearn.model_selection import train_test_split\n#from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV","94c64674":"# Setting parameters\nx_data, x_test_data, y_data, y_test_data = train_test_split(X_train1,y_train,test_size=0.2, random_state=123)\nsvm_para = [{'kernel':['rbf'],'C': [1,10,100,100]}]","8a9cab7b":"#classifier = GridSearchCV(svm.SVC(),svm_para,cv=3,verbose=2)\n#classifier.fit(x_data,y_data)\n#classifier.best_params_\n#classifier.grid_scores_","66277f39":"# Parameters optimized using the code in above cell\nC_opt = 10 # reasonable option\nclf = svm.SVC(C=C_opt,kernel='rbf')\nclf.fit(X_train1,y_train)","f3848c94":"clf.score(X_train1,y_train)","ab8277da":"# y_pred = clf.predict(X_test1)","9adf4071":"from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.metrics import classification_report\n\n# setting parameters\nx_data, x_test_data, y_data, y_test_data = train_test_split(X_train,y_train,test_size= 0.3, random_state=0)\netc_para = [{'n_estimators':[20,30,100], 'max_depth':[5,10,15], 'max_features':[0.1,0.2,0.3]}] \n# Default number of features is sqrt(n)\n# Default number of min_samples_leaf is 1","591fc5ef":"ETC = GridSearchCV(ExtraTreesClassifier(),param_grid=etc_para, cv=10, n_jobs=-1)\nETC.fit(x_data, y_data)\nETC.best_params_\nETC.grid_scores_","6a03f17a":"print ('Best accuracy obtained: {}'.format(ETC.best_score_))\nprint ('Parameters:')\nfor key, value in ETC.best_params_.items():\n    print('\\t{}:{}'.format(key,value))","4f227592":"# Classification Report\nY_pred = ETC.predict(x_test_data)\ntarget = ['class1', 'class2','class3','class4','class5','class6','class7' ]\nprint (classification_report(y_test_data, Y_pred, target_names=target))","2a74e902":"from sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\ndef plot_learning_curve(model,title, X, y,n_jobs = 1, ylim = None, cv = None,train_sizes = np.linspace(0.1, 1, 5)):\n    \n    # Figrue parameters\n    plt.figure(figsize=(10,8))\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel('Training Examples')\n    plt.ylabel('Score')\n    \n    train_sizes, train_score, test_score = learning_curve(model, X, y, cv = cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    \n    # Calculate mean and std\n    train_score_mean = np.mean(train_score, axis=1)\n    train_score_std = np.std(train_score, axis=1)\n    test_score_mean = np.mean(test_score, axis=1)\n    test_score_std = np.std(test_score, axis=1)\n    \n    plt.grid()\n    plt.fill_between(train_sizes, train_score_mean - train_score_std, train_score_mean + train_score_std,\\\n                    alpha = 0.1, color = 'r')\n    plt.fill_between(train_sizes, test_score_mean - test_score_std, test_score_mean + test_score_std,\\\n                    alpha = 0.1, color = 'g')\n    \n    plt.plot(train_sizes, train_score_mean, 'o-', color=\"r\", label=\"Training score\")\n    plt.plot(train_sizes, test_score_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n    \n    plt.legend(loc = \"best\")\n    return plt","747ee0f1":"# 'max_features': 0.3, 'n_estimators': 100, 'max_depth': 15, 'min_samples_leaf: 1'\netc = ExtraTreesClassifier(bootstrap=True, oob_score=True, n_estimators=100, max_depth=10, max_features=0.3, \\\n                           min_samples_leaf=1)\n\netc.fit(X_train, y_train)\n# yy_pred = etc.predict(X_test)\netc.score(X_train, y_train)","73b21059":"# Plotting learning curve\ntitle = 'Learning Curve (ExtraTreeClassifier)'\n# cross validation with 50 iterations to have a smoother curve\ncv = ShuffleSplit(n_splits=50, test_size=0.2, random_state=0)\nmodel = etc\nplot_learning_curve(model,title,X_train, y_train, n_jobs=-1,ylim=None,cv=cv)\nplt.show()","20a13e64":"## Correlation matrix (heatmap)\nCorrelation requires continuous data. Hence, ignore Wilderness_Area and Soil_Type as they are binary values","13625183":"No categorical data. All are numerical","76444a9b":"## Data Visualisation","c90e8bf0":"It shows positive skewness (log or squared transformations will be a good option)","78da1b30":"##PART II\n<https:\/\/www.kaggle.com\/nitin007\/forest-cover-type-prediction\/forest-cover-type-prediction-complete-part-ii\/> ","8e97b0d9":"- Horizontal and vertical distance to hydrology seems to have a linear relation\n- Hillside and Aspect seems to have a sigmoid relation given by: $$\\frac { 1 }{ 1\\quad +\\quad { e }^{ -x } } $$","2ff23c4b":"## Normality\n(Needed only for few ML algorithms like SVM)","afced88f":"## Inferences\n- Count is 15120 for each column, so no data point is missing.\n- Soil type 7 and 15 are constant(each value is zero), so they can be removed.\n- Wilderness_Area and Soil_Type are one hot encoded. Hence, they could be converted back for some analysis.\n- Scales are not the same for all. Hence, rescaling and standardisation may be necessary for some algos.","97424990":"Shows positive skewness","a792b414":"## ExtraTreesClassifier","fcc95c66":"It shows Cover_Type 1 and 2 are difficult to predict","9b06254d":"## ML algorithms","f757fd6f":"## PART I (Being a newbie would love to have your suggestions on improving it)\nLink to PART II <https:\/\/www.kaggle.com\/nitin007\/forest-cover-type-prediction\/forest-cover-type-prediction-complete-part-ii\/>","40fde38f":"I also performed log transformation but squared one gives better result","1b46195e":"## Learning Curve\nExtraTreesClassifier","c3f1d8fa":"Improvement clearly visible","cf07b4dc":"Shows positive skewness","ae8575d0":"'rbf' or radial basis function is the Gaussian kernel","744fe4f2":"Presence of skewness can easily be noticed","00971d00":"Reasonable improvement noticed","00e6a771":"Too many zero values means attributes like it shows class distinction","b62c0a99":"## Skewness","dd8653d5":"## Data Preparation\n## Delete rows or impute values in case of missing\n## Check for data transformation\n## Some of the soil_types is present in very few Cover_Types","7f0d0672":"Data transformation needed in: 'Horizontal n vertical distance', 'Hillshade_9am & noon'","a2a9eafa":"Reasonable improvement seen","821e38e9":"## Removing Soil_type 7 & 15","14a28eaf":"Negative skewness present","f033ac02":"- Wilderness_Area4 has lot of presence of cover_type 4, good class distinction\n- SoilType 1-6,9-13,15, 20-22, 27-31,35,36-38 offer lot of class distinction as counts for some are very high","b001116c":"Shows positive skewness","086702ca":"Shows negative skewness","b908bb38":"- Elevation has a seperate distribution for each class, hence an important attribute for prediction\n- Aspect plot contains couple of normal distribution for several classes\n- Horizontal distance to hydrology and roadways is quite similar\n- Hillshade 9am and 12pm displays left skew (long tail towards left)\n- Wilderness_Area3 gives no class distinction. As values are not present, others give some scope to distinguish\n- Soil_Type, 1,5,8,9,12,14,18-22, 25-30 and 35-40 offer class distinction as values are not present for many classes","50b12cbe":"## Correlation values","cd6dfc08":"## Train & Test Data","92e6dab5":"## Support vector Machines"}}