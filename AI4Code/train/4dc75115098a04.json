{"cell_type":{"7fb66da5":"code","a94c459e":"code","192e232b":"code","fe93e6c9":"code","5969a81a":"code","bb926c56":"code","ea1a52dc":"code","d8384872":"code","a2895b45":"code","d81f511f":"code","a1611f10":"code","23e5cd65":"code","a52cec4e":"code","2e21c647":"code","ccc219a2":"code","470d6441":"code","96396bd6":"code","f8aeee71":"code","504350d4":"code","c5c6036e":"code","38bb9e05":"code","79fb41a3":"code","e08e88bc":"code","f299e528":"code","82b6e574":"code","ee95de92":"code","103fa0d9":"code","304dfea6":"code","a57229c2":"code","f43b957e":"code","ce9502d1":"code","5a63ce1d":"code","3efb8f91":"code","823bff44":"code","32bd0e7c":"code","8e048b0f":"code","03ffcf2e":"markdown","33afed97":"markdown","2945f718":"markdown","4f78c0bb":"markdown","324e185e":"markdown","0c2028cc":"markdown","7c82d700":"markdown","c8ab1007":"markdown","104fbdca":"markdown","27c3a163":"markdown","2ccb7266":"markdown"},"source":{"7fb66da5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a94c459e":"# Libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_score, recall_score\nfrom sklearn.ensemble import ExtraTreesClassifier\nimport warnings\nwarnings.filterwarnings('ignore')","192e232b":"pip install openpyxl","fe93e6c9":"data = pd.read_excel(\"..\/input\/covid19\/dataset.xlsx\", engine=\"openpyxl\")","5969a81a":"data.info()\ndata.describe()\ndata.head()","bb926c56":"data.columns = [x.lower().strip().replace(' ','_') for x in data.columns]","ea1a52dc":"def miss_data(x):\n    total = x.isnull().sum()\n    percent = (x.isnull().sum()\/x.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(x[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","d8384872":"miss_data(data)","a2895b45":"for x in data.columns:\n    if data[x].dtype=='float16' or  data[x].dtype=='float32' or  data[x].dtype=='float64':\n        data[x].fillna(data[x].mean())\n\ndata = data.fillna(-999)\n\nfor y in data.columns:\n    if data[y].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(data[y].values))\n        data[y] = lbl.transform(list(data[y].values))","d81f511f":"threshold = 0.92\n\ncorr_matrix = data.corr().abs()\ncorr_matrix.head()","a1611f10":"upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nupper.head()","23e5cd65":"to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n\nprint('There are %d columns to remove.' % (len(to_drop)))\ndataset = data.drop(columns = to_drop)\nprint('Data shape: ', data.shape)\nprint('Size of the data', data.shape)","a52cec4e":"data_missing = (data.isnull().sum() \/ len(data)).sort_values(ascending = False)\ndata_missing.head()","2e21c647":"data_missing_ = data_missing.index[data_missing > 0.85]\nall_missing = list(set(data_missing_))","ccc219a2":"dataset = dataset.drop(columns = all_missing)","470d6441":"dataset.info()","96396bd6":"cols = [x for x in dataset.columns if x not in ['patient_id','sars-cov-2_exam_result', 'patient_addmited_to_regular_ward_(1=yes,_0=no)', 'patient_addmited_to_semi-intensive_unit_(1=yes,_0=no)', 'patient_addmited_to_intensive_care_unit_(1=yes,_0=no)']]","f8aeee71":"new_df = dataset[cols]","504350d4":"new_df","c5c6036e":"X = new_df\ny = dataset['sars-cov-2_exam_result']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state=101)","38bb9e05":"model = ExtraTreesClassifier()\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()","79fb41a3":"feat_head = feat_importances.head(10)\nfeat_head.index","e08e88bc":"X = new_df[feat_head.index]\ny = dataset['sars-cov-2_exam_result']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 101)","f299e528":"accuracy_lst =[]\n\ndef model_assess(model, name='Default'):\n    model.fit(X_train, y_train)\n    prds = model.predict(X_test)\n    model_acc = accuracy_score(y_test, prds)\n    accuracy_lst.append(100*model_acc)\n    print('---', name, '---', '\\n',\n          confusion_matrix(y_test, prds), '\\n',\n          'Accuracy:', (accuracy_score(y_test, prds)), '\\n',\n          'Classification Report:', (classification_report(y_test, prds)))","82b6e574":"# Logistic Regression\nlg = LogisticRegression()\nmodel_assess(lg, 'Logistic Regression')\n\n# Decision Tree\ntree = DecisionTreeClassifier()\nmodel_assess(tree, 'Decission Trees')\n\n# Random Forest\nrforest = RandomForestClassifier(n_estimators=1000, max_depth=10, random_state=0)\nmodel_assess(rforest, 'Random Forest')\n\n# SVM\nsvm = SVC()\nmodel_assess(svm, 'SVM')\n\n# KNN\nknn = KNeighborsClassifier(n_neighbors=19)\nmodel_assess(knn, name='KNN')\n\n# XGBOOST\nxgb = XGBClassifier(n_estimators=1000, learning_rate=0.05)\nmodel_assess(xgb, 'XGBoost')\n\n# Neural Network\nnn = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(150, 10), random_state=1)\nmodel_assess(nn, 'Neural Nets')","ee95de92":"model_list = ['Logistic Regression', 'DT', 'Random Forest', 'SVM', 'KNearestNeighbours', 'XGBOOST', 'NN']","103fa0d9":"plt.rcParams['figure.figsize']=20,8\nsns.set_style('darkgrid')\nax = sns.barplot(x=model_list, y = accuracy_lst, palette = \"coolwarm\", saturation =2.0)\nplt.xlabel('Classifier Models', fontsize = 20 )\nplt.ylabel('% of Accuracy', fontsize = 20)\nplt.title('Accuracy of different Classifier Models', fontsize = 20)\nplt.xticks(fontsize = 12, horizontalalignment = 'center', rotation = 8)\nplt.yticks(fontsize = 12)\nfor i in ax.patches:\n    width, height = i.get_width(), i.get_height()\n    x, y = i.get_xy() \n    ax.annotate(f'{round(height,2)}%', (x + width\/2, y + height*1.02), ha='center', fontsize = 'x-large')\nplt.show()","304dfea6":"cross_acc = []\n\nca_lg = cross_val_score(lg, X_train, y_train, scoring='accuracy')\nca_lg = ca_lg.mean()\ncross_acc.append(100*ca_lg)\n\nca_tree = cross_val_score(tree, X_train, y_train, scoring='accuracy')\nca_tree = ca_tree.mean()\ncross_acc.append(100*ca_tree)\n\nca_rforest = cross_val_score(rforest, X_train, y_train, scoring='accuracy')\nca_rforest = ca_rforest.mean()\ncross_acc.append(100*ca_rforest)\n\nca_svm = cross_val_score(svm, X_train, y_train, scoring='accuracy')\nca_svm = ca_svm.mean()\ncross_acc.append(100*ca_svm)\n\nca_knn = cross_val_score(knn, X_train, y_train, scoring='accuracy')\nca_knn = ca_knn.mean()\ncross_acc.append(100*ca_knn)\n\nca_xgb = cross_val_score(xgb, X_train, y_train, scoring='accuracy')\nca_xgb = ca_xgb.mean()\ncross_acc.append(100*ca_xgb)","a57229c2":"ca_nn = cross_val_score(nn, X_train, y_train, scoring='accuracy')\nca_nn = ca_nn.mean()\ncross_acc.append(100*ca_nn)","f43b957e":"plt.rcParams['figure.figsize']=20,8\nsns.set_style('darkgrid')\nax = sns.barplot(x=model_list, y=cross_acc, palette = \"rocket\", saturation =2.0)\nplt.xlabel('Classifier Models', fontsize = 20 )\nplt.ylabel('Cross validation Accuracy', fontsize = 20)\nplt.title('Accuracy of different Classifier Models', fontsize = 20)\nplt.xticks(fontsize = 12, horizontalalignment = 'center', rotation = 8)\nplt.yticks(fontsize = 12)\nfor i in ax.patches:\n    width, height = i.get_width(), i.get_height()\n    x, y = i.get_xy() \n    ax.annotate(f'{round(height,2)}%', (x + width\/2, y + height*1.02), ha='center', fontsize = 'x-large')\nplt.show()","ce9502d1":"covid_positive = dataset[dataset['sars-cov-2_exam_result'] == 1]","5a63ce1d":"admission = []  \n\ndef multiclass_target(row):\n    check = 0\n    check += 1 if (row['patient_addmited_to_regular_ward_(1=yes,_0=no)'] == 1) else 0\n    check += 2 if (row['patient_addmited_to_semi-intensive_unit_(1=yes,_0=no)'] == 1) else 0\n    check += 3 if (row['patient_addmited_to_intensive_care_unit_(1=yes,_0=no)'] == 1) else 0\n    row['target'] = check\n    return row\n\ndata_adm = covid_positive.apply(multiclass_target, axis=1)\ndata_adm","3efb8f91":"X = data_adm[feat_head.index]\ny = data_adm['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)","823bff44":"acc_lst = []\n\ndef model_assess(model, name='Default'):\n    model.fit(X_train, y_train)\n    prds = model.predict(X_test)\n    model_acc = accuracy_score(y_test, prds)\n    acc_lst.append(100*model_acc)\n    print('---', name, '---', '\\n',\n          confusion_matrix(y_test, prds), '\\n',\n          'Accuracy:', (accuracy_score(y_test, prds)), '\\n',\n          'Classification Report:', (classification_report(y_test, prds)))","32bd0e7c":"# Logistic Regression\nlg = LogisticRegression()\nmodel_assess(lg, 'Logistic Regression')\n\n# Decision Tree\ntree = DecisionTreeClassifier()\nmodel_assess(tree, 'Decission Trees')\n\n# Random Forest\nrforest = RandomForestClassifier(n_estimators=1000, max_depth=10, random_state=0)\nmodel_assess(rforest, 'Random Forest')\n\n# SVM\nsvm = SVC()\nmodel_assess(svm, 'SVM')\n\n# KNN\nknn = KNeighborsClassifier(n_neighbors=19)\nmodel_assess(knn, name='KNN')\n\n# XGBOOST\nxgb = XGBClassifier(n_estimators=1000, learning_rate=0.05)\nmodel_assess(xgb, 'XGBoost')\n\n# Neural Network\nnn = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(150, 10), random_state=1)\nmodel_assess(nn, 'Neural Nets')","8e048b0f":"plt.rcParams['figure.figsize']=20,8\nsns.set_style('darkgrid')\nax = sns.barplot(x=model_list, y = acc_lst, palette = \"coolwarm\", saturation =2.0)\nplt.xlabel('Classifier Models', fontsize = 20 )\nplt.ylabel('% of Accuracy', fontsize = 20)\nplt.title('Accuracy of different Classifier Models', fontsize = 20)\nplt.xticks(fontsize = 12, horizontalalignment = 'center', rotation = 8)\nplt.yticks(fontsize = 12)\nfor i in ax.patches:\n    width, height = i.get_width(), i.get_height()\n    x, y = i.get_xy() \n    ax.annotate(f'{round(height,2)}%', (x + width\/2, y + height*1.02), ha='center', fontsize = 'x-large')\nplt.show()","03ffcf2e":"Import Libraries","33afed97":"ML Models for Ward Prediction!","2945f718":"Getting to know the data!","4f78c0bb":"Data Split","324e185e":"Load data","0c2028cc":"Feature Engineering!","7c82d700":"Feature Importance using Extra Trees Classifier","c8ab1007":"# Covid 19 Prediction, Covid 19 patient's admission to the ward using Supervised Machine Learning.","104fbdca":"**Logistic Regression, SVM & KNN were the top performers in predicting Covid 19 with an accuracy of 90.55%. While predicting Covid 19 patients admission to the ward Logistic Regression, RF, XGBoost were the best performers. This dataset is imbalanced with only 10% people having Covid 19. The performance of these models could be better in a more balanced dataset.**\n\n**Cheers!**","27c3a163":"ML Models","2ccb7266":"Admission to ward (Covid-19 Patients) "}}