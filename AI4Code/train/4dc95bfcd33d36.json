{"cell_type":{"4316e91a":"code","b2a85b67":"code","6c1deb38":"code","9ecb8aaa":"code","087fe6e4":"code","89238b8b":"code","c074be20":"code","793eae5a":"code","466dbbcd":"code","4f82713a":"code","2bf280bc":"code","f4a2f31c":"code","4f80d736":"code","dd9599e1":"code","b5b0bd55":"code","cc74a009":"code","54e44387":"code","a8a4d8a4":"code","29a5371f":"code","cd796379":"code","2d89d171":"code","506ee6f6":"code","2c2cc151":"code","c2424db9":"code","48d9a543":"code","d0d680ce":"code","915379c4":"code","85227e05":"code","683710ed":"code","0cb52172":"code","e14fa2ca":"code","5efb3902":"code","30a91eaa":"code","55c4dbe7":"code","0cf9b271":"code","91e73af6":"code","8fcdc3b1":"code","5f9bb3e2":"code","109b306a":"code","8d2ef489":"markdown","549f4cab":"markdown","4b0eea8c":"markdown","5d6762d4":"markdown","4a801be9":"markdown","cf2a2c14":"markdown","eddd5569":"markdown","f6585b4e":"markdown","b6a7fe93":"markdown","f345fff6":"markdown","a8c76628":"markdown","e32100fb":"markdown","1259193d":"markdown","e88b0e39":"markdown","42dc6e1e":"markdown","8db7e1e6":"markdown","bbcf3775":"markdown","f801a9b3":"markdown","c33e4c91":"markdown","f6580140":"markdown","6f82f024":"markdown","c6eb7912":"markdown","0dd57565":"markdown","f582be3a":"markdown","bb951694":"markdown","bbed3dec":"markdown","107baea4":"markdown","23d45920":"markdown","ca9a0056":"markdown","47043619":"markdown","171d779f":"markdown","2db69a07":"markdown","08eff5aa":"markdown","d7110db6":"markdown","6cc77fe9":"markdown","6b6f49cc":"markdown","9862d125":"markdown","d5609cf8":"markdown","fc595030":"markdown","383899b6":"markdown","3477b0df":"markdown","07159e37":"markdown","5c01c614":"markdown","61945407":"markdown","80015892":"markdown","5fa6050c":"markdown","f157174e":"markdown","f23ccbdf":"markdown","b5e4da37":"markdown"},"source":{"4316e91a":"import numpy as np \nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report","b2a85b67":"df = pd.read_csv('\/kaggle\/input\/mobile-price-classification\/train.csv')","6c1deb38":"df.head()","9ecb8aaa":"print(f'Shape of dataframe: {df.shape}')","087fe6e4":"df.info()","89238b8b":"df.describe()","c074be20":"df.isna().sum()","793eae5a":"print(df.duplicated().any())","466dbbcd":"df['price_range'].value_counts()","4f82713a":"corr = df.corr()\n\nnp.fill_diagonal(corr.values, 0)\n\ncorr.replace(0, np.nan, inplace=True)\nplt.show()\ncorr","2bf280bc":"plt.figure(figsize=(20,10))\nsns.heatmap(corr, annot=True, cmap='Blues')","f4a2f31c":"corr.unstack().sort_values(kind='quicksort', na_position='first').drop_duplicates(keep='last')","4f80d736":"corr.abs()['price_range'].sort_values(ascending=False)","dd9599e1":"sns.displot(df, x='ram')","b5b0bd55":"sns.lmplot(x='ram', y='price_range', data=df, line_kws={'color': 'purple'})\nplt.yticks([0, 1, 2, 3])\nplt.xlabel('Ram')\nplt.ylabel('Price Range')\nplt.show()","cc74a009":"sns.boxplot(x='price_range', y='battery_power', data=df)\nplt.xlabel('Price Range')\nplt.ylabel('Battery Power')\nplt.title('Battery Power\\'s correlation to Price Range', weight='bold')\nplt.show()","54e44387":"four_g = df['four_g'].value_counts()\nplt.title('Percentage of Mobiles with 4G', weight='bold')\nlabels_4g = ['4G', 'No 4G']\nfour_g.plot.pie(autopct=\"%.1f%%\", labels=labels_4g)\nplt.show()","a8a4d8a4":"n_cores = df['n_cores'].value_counts()\nplt.title('Number of cores in mobile phones\\n\\n', weight='bold')\nn_cores.plot.pie(autopct=\"%.1f%%\", radius=1.5)\nplt.show()","29a5371f":"import plotly.express as px\nfig = px.scatter_3d(df.head(1000), x='ram', y='battery_power', z='px_width', color='price_range')\nfig.show()","cd796379":"X = df.drop('price_range', axis=1)\ny = df['price_range']","2d89d171":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33, random_state=100)","506ee6f6":"models = {'KNN': KNeighborsClassifier(),\n         'Linear Regression': LinearRegression(),\n         'Random Forest': RandomForestClassifier()}\n         \ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n    np.random.seed(42)\n    \n    model_scores = {}\n    \n    for name, model in models.items():\n        model.fit(X_train, y_train)\n        \n        model_scores[name] = model.score(X_test, y_test)\n    return model_scores","2c2cc151":"model_scores = fit_and_score(models=models, \n                             X_train=X_train,\n                            X_test=X_test,\n                            y_train=y_train,\n                            y_test=y_test)\nmodel_scores","c2424db9":"model_comp = pd.DataFrame(model_scores, index=['accuracy'])\nmodel_comp.T.plot.bar(); # .T accesses the attributes of an object (in this case, the scores)","48d9a543":"train_scores = []\n\ntest_scores = []\n\nneighbors = range(1, 21)\n\nknn = KNeighborsClassifier()\n\nfor i in neighbors:\n    knn.set_params(n_neighbors = i)\n    \n    knn.fit(X_train, y_train)\n    \n    train_scores.append(knn.score(X_train, y_train))\n    \n    test_scores.append(knn.score(X_test, y_test))","d0d680ce":"plt.plot(neighbors, train_scores, label=\"Train Scores\")\nplt.plot(neighbors, test_scores, label=\"Test Scores\")\nplt.xticks(np.arange(1, 21, 1))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model score\")\nplt.legend()\n\nprint(f\"Maximum KNN score on the test data: {max(test_scores)*100:.2f}%\")","915379c4":"knn = KNeighborsClassifier(n_neighbors=13)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nprint(f'KNN Model Score: {knn.score(X_test, y_test) * 100}%')","85227e05":"# Random Forest hyperparemeters (from sklearn documentation as well)\n\nrf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n           \"max_depth\": [None, 3, 5, 10],\n           \"min_samples_split\": np.arange(2, 20, 2),\n           \"min_samples_leaf\": np.arange(1, 20, 2)}","683710ed":"rs_rf = RandomizedSearchCV(RandomForestClassifier(),\n                           param_distributions=rf_grid,\n                           cv=5,\n                           n_iter=20,\n                           verbose=True)\n\nrs_rf.fit(X_train, y_train);\n\nrs_rf.best_params_","0cb52172":"rs_rf.score(X_test, y_test)","e14fa2ca":"rf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n           \"max_depth\": [None, 3, 5, 10],\n           \"min_samples_split\": np.arange(2, 20, 2),\n           \"min_samples_leaf\": np.arange(1, 20, 2)}","5efb3902":"gs_rf = RandomizedSearchCV(RandomForestClassifier(),\n                           param_distributions=rf_grid,\n                           cv=5,\n                           n_iter=20,\n                           verbose=True)\n\ngs_rf.fit(X_train, y_train);\n\ngs_rf.best_params_","30a91eaa":"gs_rf.score(X_test, y_test)","55c4dbe7":"xgb = XGBClassifier(eval_metric='logloss', use_label_encoder=False)\n\nxgb.fit(X_train, y_train)\nxgb_pred = xgb.predict(X_test)\nxgb.score(X_test, y_test)","0cf9b271":"params_xgb = {'n_estimators': [50,100,250,400,600,800,1000], \n    'learning_rate': [0.2,0.5,0.8,1]}\n    \nrs_xgb =  RandomizedSearchCV(xgb, param_distributions=params_xgb, cv=5)\nrs_xgb.fit(X_train, y_train)\nxgb_pred_2 = rs_xgb.predict(X_test)\nrs_xgb.score(X_test, y_test)","91e73af6":"print(confusion_matrix(y_test, y_pred))","8fcdc3b1":"import seaborn as sns\nsns.set(font_scale=1.5) # Increase font size\n\ndef plot_conf_mat(y_test, y_preds):\n    \n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n                     annot=True, # Annotate the boxes\n                     cbar=False,\n                    fmt='g', # no scientific notation\n                    cmap='Blues')\n    \n    plt.xlabel(\"true label\", weight='bold')\n    plt.ylabel(\"predicted label\", weight='bold')\n    \nplot_conf_mat(y_test, y_pred)","5f9bb3e2":"print(classification_report(y_test, y_pred))","109b306a":"print(f'Cross Validation Scores: ' + str(cross_val_score(knn, X, y, cv=5)))\n\nprint(f'Cross Validation Score (Mean): ' + str(np.mean(cross_val_score(knn, X, y, cv=5))))","8d2ef489":"Above, we see how ram, battery power, and px height all contribute to a mobile phone's price classification","549f4cab":"## Modeling (Part 2)\n- XGBoost","4b0eea8c":"The confusion matrix above shows the breakdown of how our model correctly and incorrectly classified mobile phones price's","5d6762d4":"**Train-Test-Split**","4a801be9":"# Mobile Price Classification\n\n## **Main Question:** Which features are most important in predicting a Mobile Phone's Price?\n\n\n### Objective:\n1. Perform **Exploratory Data Analysis** on this dataset in order to extract insights from the data\n2. **Predict** the price_range of a mobile phone using feature(s) - independent variable(s)","cf2a2c14":"Looking at the graph above, n_neighbors = 13 seems to be the best choice. Now, we will apply this!","eddd5569":"**Data Dictionary -> below**","f6585b4e":"You might be thinking let's call it a day! Well before we do that, we are going to tune our Random Forest Model to ensure that we are using the best model!","b6a7fe93":"## Final Model Evaluation\n\nFinally, let's evaluate our model using some other metrics:\n- Confusion Matrix\n- Classification Report \n    - precision\n    - recall\n    - f1-score\n    - support \n    - accuracy \n    - macro avg\n    - weighted avg","f345fff6":"#### Import necessary modules","a8c76628":"# **Data Visualization: Analyzing the Relationship Between Variables**","e32100fb":"The classification report shows that beyond accuracy, our model performs very well!","1259193d":"**Data Cleaning**","e88b0e39":"#### Hyperparameter tuning: KNeighborsClassifier","42dc6e1e":"# **Machine Learning: Prediction**","8db7e1e6":"**Correlation Matrix**","bbcf3775":"## Modeling (Part 1)\n\n1. K-Nearest Neighbors\n2. Linear Regression\n3. RandomForest","f801a9b3":"All 3 of our models perform very well!\n\nNext, we're going to tune the hyperparemters of our KNN and Random Forest models. Unfortunately, however, Linear Regression has a few hyperparameters which don't affect its overall score, and therefore, our final final score for our Linear Regression model is the score above.\n","c33e4c91":"Correlation between variables visualized with sns.heatmap","f6580140":"Very high correlation between \"price_range\" and \"ram\" -- this means that we should use the ram variable in predicting the price range of a mobile phone when doing our Machine Learning prediction","6f82f024":"We will predict the price_range of a mobile using all features in the dataframe (excluding price_range, of course)","c6eb7912":"##### Tuning Random Forest Classifier using GridSearchCV","0dd57565":"**Data Familiarization**","f582be3a":"### **Key Variables Visualizations**","bb951694":"#### Hyperparameter tuning: XGBoost","bbed3dec":"# Conclusion","107baea4":"Well it looks like even after tuning our Random Forest Model, our KNN model still beats it! It was worth the effort though to ensure that are using the best model with the best hyperparemeters possible. But before a call it a day, let's try using an XGBoost model to see if it outperforms our KNN model.","23d45920":"The plot aboves shows the high correlation between ram and price range. It shows the general pattern: as ram increases, mobile's price increases","ca9a0056":"### Classification Report","47043619":"500 mobile phones in each of the following categories: low cost, medium cost, high cost, and very high cost","171d779f":"Display highest correlations between price_range and the other features in our dataset","2db69a07":"### Model Comparison","08eff5aa":"First, to address our main question the most important features in predicting a mobile phone's price are ram, battery power, and pixel width! We figured this out by using a correlation matrix, specifically looking at the most highly correlated variables to price range.\n\nIn the next part of our notebook, we used machine learning to predict mobile phones price's using all of the features in our dataset. We saw that the best performing model was KNN -- outperforming Linear Regression, Random Forest, and even XGBoost. We were even able to improve our KNN model's score by tuning its hyperparameters (n_neighbors). Later on, we evaluated our KNN model using other metrics (besides accuracy) and saw that it performed very well by those metrics, as well. The fact that KNN was the best performing model alludes to the idea that sometimes the more complicated models might not be the best model for a given dataset. ","d7110db6":"The highest correlations to our target variable (price_range) are:\n- ram\n- battery_power\n- px_width\n- px_height","6cc77fe9":"You may be thinking \"Why are we using Linear Regression (a regression algorithm) on a classification problem?\" Well, because our y variable will either be 0, 1, 2, or 3 our model treats it as if it's a regression problem and runs. Moreover, it performs well because it evaluates our 4 categories (above) as quantitative variables (not qualitative) and uses linear regression to find the optimal price range for each observation, which is then classified into either 0, 1, 2, or 3.","6b6f49cc":"***","9862d125":"# Exploratory Data Analysis (EDA)","d5609cf8":"0 NaN values in the dataframe","fc595030":"#### Cross Validation","383899b6":"Next, we'll use plotly to visualize the 3 most highly correlated variables to price_range","3477b0df":"#### Hyperparemeter tuning: Random Forest Model","07159e37":"##### Tuning Random Forest Classifier using RandomizedSearchCV","5c01c614":"## Thank you for reading my notebook. Please upvote it, and leave comments -- it would be greatly appreciated!","61945407":"Looking at the results of the cross validation, we can be sure that even if we performed the split on our data differently, we'd still get similar (if not even strong) results","80015892":"### Confusion Matrix","5fa6050c":"Display highest correlations between all of our variables ","f157174e":"Even after tuning our XGBoost model's hyperparameter, it still does not perform as well as our KNN model (it even performs worse than our Linear Regression model). Now, we can be sure that we've selected the best model. Finally, we'll evaluate our best performing model (KNN) using other metrics!","f23ccbdf":"0 duplicated values in the dataframe","b5e4da37":"- ID: ID\n- battery_power: Total energy a battery can store in one time measured in mAh\n- blue: Has bluetooth or not\n- clock_speed: speed at which microprocessor executes instructions\n- dual_sim: Has dual sim support or not\n- fc: Front Camera mega pixels\n- four_g: Has 4G or not\n- int_memory: Internal Memory in Gigabytes\n- m_dep: Mobile Depth in cm\n- mobile_wt: Weight of mobile phone\n- n_cores: Number of cores of processor\n- pc: Primary Camera mega pixels\n- px_height: Pixel Resolution Height\n- px_width: Pixel Resolution Width\n- ram: Random Access Memory in Megabytes\n- sc_h: Screen Height of mobile in cm\n- sc_w: Screen Width of mobile in cm\n- talk_time: longest time that a single battery charge will last when you are\n- three_g: Has 3G or not\n- touch_screen: Has touch screen or not\n- wifi: Has wifi or not\n\nTarget variable:\n- price_range: This is the target variable with value of \n0(low cost), \n1(medium cost), \n2(high cost) and \n3(very high cost)."}}