{"cell_type":{"2e5735d8":"code","2c69f0de":"code","54b81563":"code","94d70d40":"code","6b1fe4bf":"code","99832ce2":"code","67bd9fc7":"code","e9a62307":"code","9868c4f1":"code","6de96da0":"code","afcd4741":"code","37edba2d":"code","6eec58b4":"code","7dab3c77":"code","8cc3aae7":"code","3343d850":"code","41d82bd9":"markdown","c2c5bfd6":"markdown","fc4524a4":"markdown","6e4df6e7":"markdown","47e6a5e3":"markdown","f910cdb3":"markdown","ae8540ea":"markdown","b096dd21":"markdown"},"source":{"2e5735d8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom numbers import Real\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import preprocessing\nfrom sklearn import ensemble\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn import decomposition\nfrom sklearn import preprocessing\nfrom functools import partial\nimport optuna\nfrom skopt import space\nfrom skopt import gp_minimize \nfrom hyperopt import hp, fmin, tpe, Trials\nfrom hyperopt.pyll.base import scope\nfrom lightgbm import LGBMClassifier\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2c69f0de":"df_train = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\nX_test = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')","54b81563":"df_train.head()","94d70d40":"df_train.shape","6b1fe4bf":"df_train.target.hist()","99832ce2":"df_train.isnull().sum().sum()","67bd9fc7":"X_test.isnull().sum().sum()","e9a62307":"print(df_train.shape)\nprint(X_test.shape)","9868c4f1":"y = df_train['target']\ndf_train.pop('target')\ndf_train.pop('id')\nX_test.pop('id')\nX=df_train\n\ndel df_train\n","6de96da0":"print(X.shape)\nprint(X_test.shape)","afcd4741":"X.head()","37edba2d":"\"\"\"def optimize(params, x, y):\n    \n    model = LGBMClassifier(**params)\n    kf = model_selection.StratifiedKFold(n_splits=5)\n    accuracies = []\n    for idx in kf.split(X=x, y = y):\n        train_idx, test_idx = idx[0], idx[1]\n      \n        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n\n        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n\n       \n        model.fit(X_train, y_train, eval_set = [(X_test, y_test)], \n                  early_stopping_rounds = 300, verbose = False)\n        \n        preds = model.predict_proba(X_test)[:,1]\n        fold_acc = metrics.roc_auc_score(y_test, preds)\n        accuracies.append(fold_acc)\n        \n    return -1.0*np.mean(accuracies)\"\"\"","6eec58b4":"\"\"\"param_space = {\n        \"max_depth\": scope.int(hp.quniform(\"max_depth\", 3, 18, 1)),\n        \"n_estimators\": scope.int(hp.quniform(\"n_estimators\", 100, 600, 1)),\n        'min_child_weight' : scope.int(hp.quniform('min_child_weight', 0, 10, 1)),\n        'reg_alpha' : scope.int(hp.quniform('reg_alpha', 40,180,1)),\n        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n        \n        'random_state':42,\n    }\n        \noptimization_function = partial(optimize,  x = X, y = y)\n\ntrials = Trials()\n\nresult = fmin(\n        fn = optimization_function,\n        space = param_space,\n        algo = tpe.suggest, \n        max_evals = 15,\n        trials = trials, \n    )\nprint(result)\"\"\"","7dab3c77":"params_lgbm =  {\n        'boosting_type': 'gbdt',\n        \"max_depth\": 5,\n        \"n_estimators\": 538,\n        'min_child_weight' : 10,\n        'reg_alpha' : 127.0,\n        'reg_lambda' : 0.544,\n        'colsample_bytree' : 0.5815,\n        'random_state' : 42,\n        'n_jobs': -1,\n        'metric': 'AUC',\n        'verbosity': -1,\n    }\nfolds = model_selection.StratifiedKFold(n_splits = 5, random_state = 42, shuffle = True)\ny_pred = np.zeros(len(X_test))\nscores = []\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n    \n    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n    model = LGBMClassifier(**params_lgbm)\n   \n    model.fit(X_train, y_train, eval_set = [(X_val, y_val)], verbose = False, early_stopping_rounds = 300)\n    \n    final_preds = model.predict_proba(X_val)[:,1]\n    fold_score = metrics.roc_auc_score(y_val, final_preds)\n    scores.append(fold_score)\n    y_pred += model.predict_proba(X_test)[:,1] \/ folds.n_splits \n\nprint(scores)","8cc3aae7":"sample_submission.head()","3343d850":"sample_submission['target'] = y_pred\nsample_submission.to_csv('Submission.csv',index = False)","41d82bd9":"## Hyperparameter Tuning begins here\n\n### Important note\nThe following cell can be uncommented to run the hyperparameter tunning process which uses hyperopt method","c2c5bfd6":"### Results obtained from Hyperparameter Tuning using hyperopt with accuracy 0.0.72736 are given as under:\n{'colsample_bytree': 0.5815313640855496, 'max_depth': 5.0, 'min_child_weight': 10.0, 'n_estimators': 538.0, 'reg_alpha': 127.0, 'reg_lambda': 0.5449746986993232}","fc4524a4":"### Final Submission","6e4df6e7":"From the above histogram of the target, it is known that the it is a classification problem","47e6a5e3":"## Importing the main libraries","f910cdb3":"It is also known that there no null values in training data and from the data there seems no categorical values","ae8540ea":"It is also known that there no null values in test data and from the data there seems no categorical values","b096dd21":"## Dividing dependent and independent variables and adding new features\nIt can be seen that columns like \"Id\" are unique, hence wont contribute for our predictions. Therefore, these must be removed from both training and testing datasets."}}