{"cell_type":{"146d1cb9":"code","558e2b50":"code","25af0a9b":"code","87dd24ad":"code","415c35d8":"code","fce78b0f":"code","448919b7":"code","8782004c":"code","c8b6ec50":"code","2529f7ff":"code","b4bb8be3":"code","ed1ff762":"code","a2b2d1e0":"code","0cfaa5f6":"code","3a96ce6a":"code","33a801c9":"code","46a5adfc":"code","efc8fff5":"code","045c4406":"code","adb9c520":"code","299905b5":"code","f43acc2e":"code","e1b0207f":"code","ebbdf526":"code","fd7edb9b":"code","adc5d9ae":"code","2ee89ec8":"code","ee9a720a":"code","4c16db3a":"code","e12baebe":"code","28c1ce97":"code","6c29dda8":"code","ad30cc37":"code","4af73da5":"code","151c2138":"code","d8996af8":"code","7ce8200f":"code","0202ccec":"code","8c598469":"code","2fbb5e4f":"code","c721451c":"code","b66c2000":"code","f230f29c":"code","ba742448":"code","6d6b1c56":"code","522f1d21":"code","c5cf921c":"code","371ebc01":"code","8401a589":"code","42d55758":"code","2bdfa019":"code","04d952c8":"code","cb6b4270":"code","c8ecc06d":"code","334f4476":"code","3c9bf1ec":"code","26a5ed3c":"code","de1e6837":"code","4bf56e73":"code","0d7cd30b":"code","a9de8370":"code","044dc11a":"code","566f373b":"code","30d82a7f":"code","3afb9fac":"code","77c6de49":"code","879f43af":"code","ab1112a4":"code","a802845a":"markdown","76354ded":"markdown","bbe6027a":"markdown","d052a994":"markdown","2e7b084b":"markdown","7cd1a751":"markdown","2ff572f6":"markdown","c1890740":"markdown","c8fcb471":"markdown","a3bcbcae":"markdown","6f7791a7":"markdown","afc5f821":"markdown","88857a9c":"markdown","919c82fc":"markdown","0275033d":"markdown","f0da42b4":"markdown","8ecb27b9":"markdown","5c3a26f5":"markdown","3bd9eb05":"markdown","2e394890":"markdown","fc0af1cd":"markdown","19e6f60c":"markdown","ed08a098":"markdown","a5779e83":"markdown","f3946836":"markdown","4eca2bdd":"markdown","f03abe3e":"markdown","6dd19e77":"markdown","f6583f23":"markdown","851b455b":"markdown","8095b6ce":"markdown","73e8ad02":"markdown","f7c12178":"markdown","b39f5752":"markdown","72c83c00":"markdown","c448eb93":"markdown","8dcb13fd":"markdown","c1f6f8ce":"markdown","dd2cb684":"markdown","e26e5f8c":"markdown","fe104c8f":"markdown","b22a2d1f":"markdown","dc8d5099":"markdown"},"source":{"146d1cb9":"# For data processing and analysis\nimport numpy as np \nimport pandas as pd \n\n# For data visualization\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nsns.set(style='darkgrid')\nimport plotly.graph_objs as go\nimport plotly.offline as py\n\n# For preprocessing dataset\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n# For model building\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Recursive Feature Elimination with Cross-Validation (RFECV)\nfrom sklearn.feature_selection import RFECV\n# GridSeachCV\nfrom sklearn.model_selection import GridSearchCV\n\n# For model evaluation\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import plot_roc_curve, roc_curve\nfrom sklearn.metrics import confusion_matrix","558e2b50":"# Load Dataset as CSV file\nPATH = '..\/input\/breast-cancer-wisconsin-data\/data.csv'\ndf = pd.read_csv(PATH)","25af0a9b":"df.head()","87dd24ad":"# Preview the dataset\ndf.head()","415c35d8":"df.shape","fce78b0f":"# Variable identification\ndf.info()","448919b7":"# Check the summary dtatistics\ndf.describe()","8782004c":"df.isnull().sum()","c8b6ec50":"df['Unnamed: 32'].unique()","2529f7ff":"df.head()","b4bb8be3":"def drop_and_encode_features(data):\n    \"\"\"\n    - Drop 'Unnamed: 32' column\n    - Encode 'diagnosis' to numerical variable (M = 1, B = 0)\n    \"\"\"\n    # Drop 'Unnamed: 32' column\n    data.drop(columns=['id', 'Unnamed: 32'], inplace=True)\n    \n    # Encode the label\n    label = LabelEncoder()\n    data['diagnosis'] = label.fit_transform(data['diagnosis'])\n    return data","ed1ff762":"def get_proportion_of_diagnosis(data):\n    \"\"\"\n    Visualize the ratio of 'Malignant' and 'Benign' in 'diagnosis' using pie chart\n    \"\"\"\n    result = data['diagnosis'].value_counts()\n    values = [result['M'], result['B']]\n    labels = ['Malignant', 'Benign']\n    trace = go.Pie(labels=labels, values=values)\n    py.iplot([trace])\n    \n\ndf = df.copy()\nget_proportion_of_diagnosis(df);","a2b2d1e0":"# Implement the function allowing 'diagnosis' to be encoded as numerical variables\ndf = drop_and_encode_features(df)\ndf.info()","0cfaa5f6":"# Split dataset into each _mean, _se, and _worst.\nmean_features = df.loc[:, df.columns.str.contains('_mean')]\nse_features = df.loc[:, df.columns.str.contains('_se')]\nworst_features = df.loc[:, df.columns.str.contains('_worst')]","3a96ce6a":"def plot_distributions(features_df):\n    \"\"\"\n    Represent the data distribution and check distribution-skewness for each feature\n    \"\"\"\n    fig, ax = plt.subplots(10, 2, figsize=(15, 20))\n    for feature in features_df:\n        idx = features_df.columns.get_loc(feature)\n        \n        # distribution\n        sns.distplot(features_df[feature], bins=20, \n                     label='skewness: %.2f'%(features_df[feature].skew()),\n                     ax = ax[idx,0])\n        \n        # boxplot\n        sns.boxplot(features_df[feature], ax=ax[idx,1])    \n    plt.tight_layout()\n    plt.show()\n","33a801c9":"plot_distributions(mean_features);","46a5adfc":"plot_distributions(se_features);","efc8fff5":"plot_distributions(worst_features);","045c4406":"def feature_dist_diagnosis(data, features):\n    \"\"\"\n    Distribution (Malignant vs. Benign)    \n    \"\"\"\n    fig, ax = plt.subplots(10, 2, figsize=(15, 25))\n    for feature in features:\n        idx = features.columns.get_loc(feature)\n        # distribution\n        sns.distplot(data[data['diagnosis']==1][feature], label='Malignant', color='red', bins=20, ax = ax[idx,0])\n        sns.distplot(data[data['diagnosis']==0][feature], label='Benign', color='green', bins=20, ax = ax[idx,0])\n        plt.legend(loc='upper right')\n        \n        # boxplot\n        sns.boxplot(x=feature, y='diagnosis', hue='diagnosis', orient='h', data=data, ax = ax[idx,1])\n        plt.legend(loc='best')\n        plt.tight_layout()\n    plt.show()","adb9c520":"feature_dist_diagnosis(df, mean_features);","299905b5":"feature_dist_diagnosis(df, se_features);","f43acc2e":"feature_dist_diagnosis(df, worst_features);","e1b0207f":"def correlation_heatmap(data):\n    plt.figure(figsize=(18,12))\n    corr = data.corr(method='pearson')\n    cmap = sns.diverging_palette(220,10,as_cmap=True)\n    mask = np.zeros_like(corr, dtype=bool)\n    mask[np.triu_indices_from(mask)] = True\n    \n    sns.heatmap(corr, cmap=cmap, annot=True, fmt='.2f', linewidth=0.2, mask=mask)                 \n    plt.title('Heatmap displaying the relationship between the features of the data',\n              fontsize=20)\n    plt.tight_layout()\n    plt.show()\n    \n\ncorrelation_heatmap(df);","ebbdf526":"def get_positive_corr_coefficient(data):\n    positive_corr_df = data.corr(method='pearson')\n    positive_corr_df = positive_corr_df.mask(np.tril(np.ones(positive_corr_df.shape)).astype(np.bool))\n    positive_corr_df = positive_corr_df[abs(positive_corr_df) > 0.8].stack().reset_index()\n    return positive_corr_df\n\n\npositive_corr_df = get_positive_corr_coefficient(df)\npositive_corr_df.head(50)","fd7edb9b":"positive_corr_features = positive_corr_df['level_0'].unique()\npositive_corr_features","adc5d9ae":"def split_corr_feature_df(positive_corr_features, positive_corr_df):\n    split_corr_features_df = []\n    for feature in positive_corr_features:\n        corr_feature_df = positive_corr_df[positive_corr_df['level_0'] == feature].sort_values(by=[0], ascending=False).reset_index(drop=True)\n        split_corr_features_df.append(corr_feature_df)\n    return split_corr_features_df\n\n\nsplit_corr_feature_df = split_corr_feature_df(positive_corr_features, positive_corr_df)","2ee89ec8":"def scatterplot_features_positive_corr(positive_corr_features, split_corr_feature_df, df):\n    \n    for feature_dependent in range(len(positive_corr_features)):\n        fig = plt.figure(figsize=(15, 8))\n        for idx, item in enumerate(split_corr_feature_df[feature_dependent]['level_0']):\n            plt.subplot(2, 4, idx+1)\n            y = split_corr_feature_df[feature_dependent]['level_1'][idx]\n            sns.scatterplot(x=item, y=y, hue='diagnosis', data=df)\n            plt.legend(loc='best')\n            plt.tight_layout()\n        print(split_corr_feature_df[feature_dependent])\n        plt.show()\n    \n    \nscatterplot_features_positive_corr(positive_corr_features, split_corr_feature_df, df);","ee9a720a":"def get_negative_corr_coefficient(data):\n    negative_corr_df = data.corr(method='pearson')\n    negative_corr_df = negative_corr_df.mask(np.tril(np.ones(negative_corr_df.shape)).astype(np.bool))\n    negative_corr_df = negative_corr_df[abs(negative_corr_df) < -0.8].stack().reset_index()\n    return negative_corr_df\n\n\nnegative_corr_df = get_negative_corr_coefficient(df)\nnegative_corr_df.head()","4c16db3a":"def get_data_normalized(data):\n    \"\"\"\n    Data normalization\n    \"\"\"\n    y = data['diagnosis']\n    X = data.drop('diagnosis', axis=1)\n    X = (X - X.mean()) \/ X.std()\n    return X, y\n\n\ndef get_train_test_data_split(data):\n    \"\"\"\n    Train-test split (Train : Test = 80% : 20%)\n    \"\"\"\n    X, y = get_data_normalized(data);\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n    print('Train Shape:', X_train.shape)\n    print('Test Shape:', X_test.shape)\n    return X_train, X_test, y_train, y_test\n\n\n\nX_train, X_test, y_train, y_test = get_train_test_data_split(df)","e12baebe":"# Base Model: Logistic Regression  \nlr = LogisticRegression()\nlr.fit(X_train, y_train)\nprint(f'Used Parameters in the Default Logistic Regression: \\n')\nprint(f'{lr.get_params()}')\n\ny_pred_lr = lr.predict(X_test)","28c1ce97":"# Defining evaluation function: MAPE\n# Get accuracy and MAPE score of model\nclass ModelResult:\n    def __init__(self, model_name: str, accuracy: float, mape: float):\n        self.accuracy = accuracy\n        self.mape = mape\n        self.model_name = model_name\n\n        \ndef get_accuracy_model(model_name, y_test, y_predict) -> ModelResult:\n    errors = abs(y_test - y_predict)\n    mape = 100 * np.mean(errors \/ y_test)\n    accuracy = 100 - mape\n\n    mape = float('{:.2}'.format(mape))\n    accuracy = float('{:.2}'.format(accuracy))\n\n    print(f'{model_name} Model Performance')\n    print(f'Average Error: {mape} %')\n    print(f'Accuracy: {accuracy} %')\n        \n    return ModelResult(model_name, accuracy, mape)\n","6c29dda8":"# Accuracy in Logistic Regression base model\nlr_results = get_accuracy_model('LR', y_test, y_pred_lr)\n\n# Create a list of each model result\nall_model_results = []\nall_model_results.append(lr_results)","ad30cc37":"def conf_matrix(y_test, y_predict):\n    \"\"\"\n    Plot a confusion matrix of the model\n    \"\"\"\n    plt.figure(figsize=(7, 5))\n    sns.heatmap(confusion_matrix(y_test, y_predict), \n                annot=True, fmt='d', cbar_kws={'shrink': .5})\n    plt.xlabel('Predicted', fontsize=15)\n    plt.ylabel('Actual', fontsize=15)\n    plt.show()\n","4af73da5":"# Confusion Matrix of Logistic Regression base model\nconf_matrix(y_test, y_pred_lr);","151c2138":"def plot_roc_auc(model, X_test, y_test, label):\n    y_score = model.predict_proba(X_test)[:,1]\n    fpr, tpr, thresholds = roc_curve(y_test, y_score)\n    roc_auc = roc_auc_score(y_test, y_score)\n   \n    plt.figure()\n    plt.plot(fpr, tpr, label=label % roc_auc)\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic')\n    plt.legend(loc='lower right')\n    plt.savefig('Log_ROC')\n    plt.show()\n    ","d8996af8":"# ROC and AUC in Logistic Regression base model\nplot_roc_auc(lr, X_test, y_test, label='Logistic Regression (area=%0.2f)');","7ce8200f":"# Base Model: Random Forest\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)\nprint(f'Used Parameters in the Default Random Forest: \\n')\nprint(f'{rf.get_params()}')\n\ny_pred_rf = rf.predict(X_test)","0202ccec":"# Accuracy in Random Forest base model\nrf_results = get_accuracy_model('RF', y_test, y_pred_rf)\n\n# Add RF model results on all_model_results list\nall_model_results.append(rf_results)","8c598469":"# Confusion Matrix of Random Forest base model\nconf_matrix(y_test, y_pred_rf);","2fbb5e4f":"# ROC and AUC in Random Forest base model\nplot_roc_auc(rf, X_test, y_test, label='Random Forest (area=%0.2f)');","c721451c":"# Use RFECV and get the best feature daraframe for a model\ndef get_best_features_by_RFECV(model, X_train, y_train, data):\n    # Use RFECV to pick the best features\n    selector = RFECV(estimator=model, step=1, \n                     cv=5, scoring='accuracy')\n    selector = selector.fit(X_train, y_train)\n    \n    print(f'Optimal number of features: {selector.n_features_}')\n    print()\n    print(f'Best features: \\n')\n    print(f'{X_train.columns[selector.support_]}')\n    \n    # Reframe the feature dataset\n    selected_features = selector.get_support(1)\n    X_selected_features = data[data.columns[selected_features]]\n    return X_selected_features\n","b66c2000":"# Get the best feature dataset in Logistic Regression\nX_selected_lr = get_best_features_by_RFECV(lr, X_train, y_train, df)","f230f29c":"X_selected_lr.head()","ba742448":"X_selected_lr.info()","6d6b1c56":"def normalize_best_features_for_model(data, selected_data):\n    \"\"\"\n    Split dataset and standardize the dataset\n    \"\"\"\n    y = data['diagnosis']\n    X = selected_data\n    X = (X - X.mean()) \/ X.std()\n    return X, y\n\n\n\ndef get_best_dataset_split(data, selected_data):\n    \"\"\"\n    Train-test split ratio (Train : Test = 80% : 20%)\n    \"\"\"\n    X, y = normalize_best_features_for_model(data, selected_data)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n    print(f'Train Shape: {X_train.shape}')\n    print(f'Test Shape: {X_test.shape}')\n    return X_train, X_test, y_train, y_test","522f1d21":"# Get the selected dataset into train and test in Logistic Regression\nX_train_lr, X_test_lr, y_train_lr, y_test_lr = get_best_dataset_split(df, X_selected_lr)","c5cf921c":"# Logistic Regression with RFECV (LR-RFECV)\nlr_RFECV = LogisticRegression()\nlr_RFECV.fit(X_train_lr, y_train_lr)\ny_pred_lr_RFECV = lr_RFECV.predict(X_test_lr)\n\nprint(f'Used Parameters in LR-RFECV : \\n')\nprint(f'{lr_RFECV.get_params()}')","371ebc01":"# Accuracy in LR-RFECV\nlr_RFECV_results = get_accuracy_model('LR-RFECV', y_test_lr, y_pred_lr_RFECV)\n\n# Add LR-RFECV model results on all_model_results list\nall_model_results.append(lr_RFECV_results)","8401a589":"# Confusion Matrix of LR-RFECV\nconf_matrix(y_test_lr, y_pred_lr_RFECV);","42d55758":"# ROC and AUC in LR-RFECV\nplot_roc_curve(lr_RFECV, X_test_lr, y_test_lr);","2bdfa019":"# Get the best feature dataset in Random Forest\nX_selected_rf = get_best_features_by_RFECV(rf, X_train, y_train, df)","04d952c8":"X_selected_rf.head()","cb6b4270":"X_selected_rf.info()","c8ecc06d":"# Get the selected dataset into train and test in Random Forest\nX_train_rf, X_test_rf, y_train_rf, y_test_rf = get_best_dataset_split(df, X_selected_rf)","334f4476":"# Random Forest with RFECV (RF-RFECV)\nrf_RFECV = RandomForestClassifier()\nrf_RFECV.fit(X_train_rf, y_train_rf)\ny_pred_rf_RFECV = rf_RFECV.predict(X_test_rf)\n\nprint(f'Used Parameters in RF-RFECV: \\n')\nprint(f'{rf_RFECV.get_params()}')\n","3c9bf1ec":"# Accuracy in LR-RFECV\nrf_RFECV_results = get_accuracy_model('RF-RFECV', y_test_lr, y_pred_lr_RFECV)\n\n# Add LR-RFECV model results on all_model_results list\nall_model_results.append(rf_RFECV_results)","26a5ed3c":"# Confusion Matrix of RF-RFECV\nconf_matrix(y_test_rf, y_pred_rf_RFECV);","de1e6837":"# ROC and AUC in RF-RFECV\nplot_roc_curve(rf_RFECV, X_test_rf, y_test_rf);","4bf56e73":"# Define Grid Search CV function\ndef get_model_with_best_hyperparameters(model, param_grid, X_train, y_train):\n    grid_search = GridSearchCV(model, param_grid=param_grid, cv=5)\n    grid_search.fit(X_train, y_train)\n    print(f'Tuned hyperparameters: \\n')\n    print(f'{grid_search.best_params_}')\n    print()\n    \n    best_grid_search = grid_search.best_estimator_\n    print(f'Best Estimator: \\n')\n    print(f'{best_grid_search}')\n  \n    return best_grid_search\n","0d7cd30b":"# Create the parameter grid for LR-RFECV\nparam_grid_lr = {'penalty':['l1', 'l2'], \n                 'C': [100, 10, 1.0, 0.1, 0.01],\n                 'solver': ['liblinear']}\n\n\nlr_RFECV_gs = get_model_with_best_hyperparameters(lr_RFECV, param_grid_lr, X_train_lr, y_train_lr)\ny_pred_lr_RFECV_gs = lr_RFECV_gs.predict(X_test_lr)","a9de8370":"# Accuracy in LR-RFECV-gs model \nlr_RFECV_gs_results = get_accuracy_model('LR-RFECV-gs', y_test_lr, y_pred_lr_RFECV)\n\n# Add LR-RFECV-gs model results on all_model_results list\nall_model_results.append(lr_RFECV_gs_results)","044dc11a":"# Confusion Matrix of LR-RFECV-gs\nconf_matrix(y_test_lr, y_pred_lr_RFECV_gs);","566f373b":"# ROC and AUC in LR-RFECV-gs\nplot_roc_curve(lr_RFECV_gs, X_test_lr, y_test_lr);","30d82a7f":"# Create the parameter grid for RF-RFECV\nparam_rf = {'bootstrap': [True], \n            'max_depth': [5, 10, None], \n            'max_features': ['auto', 'log2'], \n            'n_estimators': [5, 6, 7, 8, 9, 10, 11, 12, 13, 15]}\n\n\nrf_RFECV_gs = get_model_with_best_hyperparameters(rf_RFECV, param_rf, X_train_rf, y_train_rf)\ny_pred_rf_RFECV_gs = rf_RFECV_gs.predict(X_test_rf)","3afb9fac":"# Accuracy in RF-RFECV-gs model \nrf_RFECV_gs_results = get_accuracy_model('RF-RFECV-gs', y_test_lr, y_pred_lr_RFECV)\n\n# Add RF-RFECV-gs model results on all_model_results list\nall_model_results.append(rf_RFECV_gs_results)","77c6de49":"# Confusion Matrix of RF_RFECV_gs\nconf_matrix(y_test_rf, y_pred_rf_RFECV_gs);","879f43af":"# ROC and AUC in RF_RFECV_gs\nplot_roc_curve(rf_RFECV_gs, X_test_rf, y_test_rf);","ab1112a4":"# Create a dataframe for comparing the models\nmodel_name = [model_name.model_name for model_name in all_model_results]\naccuracy = [accuracy.accuracy for accuracy in all_model_results]\nerror = [error.mape for error in all_model_results]\ncomparison = {'accuracy': accuracy, 'error': error}\n\ncomparison_df = pd.DataFrame(data=comparison, index=model_name)\ncomparison_df","a802845a":"**Mean**","76354ded":"**Worst**","bbe6027a":"## Logistic Regression with RFECV (LR-RFECV)  ","d052a994":"### RF-RFECV with GridSearchCV (RF-RFECV-gs)","2e7b084b":"### Dara Preprocessing for Random Forest with RFECV model (RF-RFECV)","7cd1a751":"# 7. Comparison  \nNow, we can compare all model performance. ","2ff572f6":"# 6. Model Building  \n## 6-1. Base Models  \nFirst, we will build Logistic Regression and Random Forest Classifier models with default parameters.\n### Logistic Regression (LR)","c1890740":"**Standard Error (SE)**  \nStandard error (SE) is the standard deviation of the means. It quantifies the variation in the means from multiple sets of measurements.","c8fcb471":"## Random Forest Classifier with RFECV (RF-RFECV)  ","a3bcbcae":"# 6-2. Recursive Feature Elimination with Cross-validation (RFECV)  \n[Recursive Feature Elimination with Cross-validation (RFECV)](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFECV.html) is a combination method of Recursive Feature Elimination (RFE) and Cross-validation technique. The RFE chooses the best set of features in data. We initially impliment with all the features. As it goes through every step or iteration, the less important features are eliminated. In RFECV, Cross-Validation is a resampling method by using different proportion of the dataset for training or testing a model on different iterations.","6f7791a7":"* Dataset comprises of 569 observations and 31 characteristics.\n* 30 independent variables and a dependent variable.","afc5f821":"* [**Positive correlation**](https:\/\/www.simplypsychology.org\/correlation.html) shows that both variables change in the same direction. One variable increases while the other variable increases, or one variable decreases as the other decreases.\n\n* [**Neutral correlation**](https:\/\/www.simplypsychology.org\/correlation.html) means no relationship in the change of variables.  \n\n* [**Negative correlation**](https:\/\/www.simplypsychology.org\/correlation.html) shows that variables change in opposite directions. One variable increases as the other decreases.  \n\n\n### Positive Correlation  \nLet's check the features with stong positive correlations.\nWe will extract the significant values which are larger than +0.8.","88857a9c":"**Worst**","919c82fc":"### Data Preprocessing for Logistic Regression with RFECV model (LR-RFECV)","0275033d":"### 4-2. Multivariate Analysis  \n[**Multivariate Analysis**](https:\/\/www.statisticshowto.com\/probability-and-statistics\/multivariate-analysis\/) is a statistical procedure for simultaneously analyzing data involving more than one variables or features, that how they are related to each other. \n\n### Distributions of the Variables\/Features According to ```diagnosis```  ","f0da42b4":"### Thanks for reading. I'll keep updating.\n### If you have any advice, please leave your comment.  \n### I appriciate your comment, upvote, and advice!  ","8ecb27b9":"* In this dataset, there is no negative correlation between features, but we can understand that the higher values of features have risks of **<span style=\"color:red\">Malignant<\/span>** (cancerous) in Breast Cancer.","5c3a26f5":"### Distributions of the Variables\/Features  \nWe'll check the shape, variability, and center (or median) of a statistical dataset of features for each mean, standard error, and worst.","3bd9eb05":"### Table of Contents \n1. **[Import Libraries and Load Dataset](#1.-Import-Libraries-and-Load-Dataset)**  \n2. **[Data Description](#2.-Data-Description)**  \n3. **[Data Preprocessing](#3.-Data-Preprocessing)**  \n    * Data Cleaning  \n    * Data Reduction  \n    * Feature Encoding  \n    \n    \n4. **[Exploratory Data Analysis](#4.-Exploratory-Data-Analysis)**  \n   4-1. **[Univariate Analysis](#4-1.-Univariate-Analysis)**  \n    * Proportion of ```diagnosis``` (Malignant vs Benign)  \n    * Distributions of the Variables\/Features  \n    \n   4-2. **[Multivariate Analysis](#4-2.-Multivariate-Analysis)** \n    * Distributions of the Variables\/Features According to ```diagnosis``` \n    * Pearson's Correlation Coefficient  \n        * Correlation Heatmap  \n    * Positive Correlation  \n        * Scatterplot  \n    * Negative Correlation  \n        \n        \n5. **[Data Preprocessing 2](#5.-Data-Preprocessing-2)**  \n    * Data Transformation  \n        * Normalization  \n        * Train-Test Split  \n        \n        \n6. **[Model Building](#6.-Model-Building)**  \n   6-1. **[Base Models](#6-1.-Base-Models)**  \n      * Logistic Regression (LR)  \n      * Random Forest Classifier (RF)  \n      \n   6-2. **[Recursive Feature Elimination with Cross-validation (RFECV)](#6-2.-Recursive-Feature-Elimination-with-Cross-validation-(RFECV))**  \n      * Logistic Regression with RFECV (LR-RFECV)  \n      * Random Forest Classifier with RFECV (RF-RFECV)  \n      \n   6-3. **[GridSearchCV for Models with RFECV](#6-3.-GridSearchCV-for-Models-with-RFECV)**       \n      * LR-RFECV with GridSearchCV (LR-RFECV-gs)  \n      * RF-RFECV with GridSearchCV (RF-RFECV-gs)  \n\n\n7. **[Comparison](#7.-Comparison)**  \n","2e394890":"# 2. Data Description  \n### Attribute Infomation:\n1. ```id```: ID number  \n2. ```diagnosis```: \n  - ```M```: Malignant (cancerous) = 1  \n  - ```B```: Benign (non-cancerous) = 0  \n\n10 real-valued features are computed for each cell nucleus:  \n\n3. ```radius```: mean of distances from center to points on the perimeter  \n4. ```texture```: standard deviation of gray-scale values  \n5. ```perimeter```:  \n6. ```area```:  \n7. ```smoothness```: local variation in radius lengths  \n8. ```compactness```: perimeter^2\/area-1.0  \n9. ```concavity```: severity of concave portions of the contour  \n10. ```concave points```: number of concave portions of the contour  \n11. ```symmetry```:  \n12. ```fractal dimension```: \"coastline approximation\"-1  \n\nThe **mean**, **standard error (SE)** and **worst** or largest (mean of the three\nlargest values) of these features were computed for each image,\nresulting in 30 features. For instance, field 3 is Mean Radius, field\n13 is Radius SE, field 23 is Worst Radius.","fc0af1cd":"* In those feature - feature positive correlations above, as both dependent and independent features get higher scores, the ```diagnosis``` result tends to be **<span style=\"color:red\">Malignant<\/span>** (cancerous).","19e6f60c":"In [Logistic Regression](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html), the default **solever** parameter is ***'lbdgs'***. It handles multiclass problems and is good for large datasets, but it doesn't support **penalty** parameter ***'l1'***.  \nIn our problem, we choose ***liblinear*** in **solever** parameter because the size of dataset is small and the label is binary.","ed08a098":"Now, we have the best features from RFECV for Logistic Regression. We will  normalize and split them into train and test dataset.","a5779e83":"* **Mean** value ","f3946836":"# 1. Import Libraries and Load Dataset  ","4eca2bdd":"* The pie chart above shows the retio of two classes, **<span style=\"color:blue\">Benign<\/span>** is 62.7% (357 cases) and **<span style=\"color:red\">Malignant<\/span>** is 37.7% (212 cases). \n* We will not deal with the dataset as an imbalanced data because the two classes in ```diagnosis``` are not hugely imbalance.","f03abe3e":"* ```diagnosis``` is now converted to int type.","6dd19e77":"### Correlation Heatmap   \nIn our [correlation heatmap](https:\/\/www.geeksforgeeks.org\/how-to-create-a-seaborn-correlation-heatmap-in-python\/#:~:text=A%20correlation%20heatmap%20is%20a,second%20dimension%20as%20a%20column.), Red color represents a positive correlation and blue color shows a negative correlation.","f6583f23":"* Those 44 pairs of variables above are significantly positive correlations.","851b455b":"* Data has only float and integer values.\n* ```diagnosis``` is object type in the given dataset.","8095b6ce":"### Negative Correlation  \nIn this dataset, there is no features with stong negative correlations as you can see below.","73e8ad02":"# 5. Data Preprocessing 2  \n### Data Transformation  ","f7c12178":"# 3. Data Preprocessing\n### Data Cleaning  \nLet's check a missing value.","b39f5752":"**Mean**  ","72c83c00":"# Breast Cancer Prediction  \n\nBreast cancer is the most common cancer in women. According to [World Health Organization (WHO)](https:\/\/www.who.int\/news-room\/fact-sheets\/detail\/breast-cancer#:~:text=In%202020%2C%20there%20were%202.3,the%20world's%20most%20prevalent%20cancer.), in 2020, there were about 2.3 million women were diagnosed with breast cancer and 685,000 deaths globally.  \nDiagnosis of breast cancer is performed when an abnormal lump is found from imaging tests such as [X-ray](https:\/\/www.myvmc.com\/investigations\/x-ray-general-radiology-and-fluoroscopy\/), [ultrasound](https:\/\/healthengine.com.au\/info\/ultrasound-machine-picture-ultrasonography), or [mammography](https:\/\/www.myvmc.com\/investigations\/mammography-breast-imaging\/) and tissue sampling (biopsy).  \n[Fine needle aspiration (FNA)](https:\/\/www.myvmc.com\/investigations\/fine-needle-aspiration-biopsy-fna\/) is a type of biopsy procedure. A thin needle is inserted into an area of abnormal tissue or fluid from a cyst or solid mass. The samples taken are examined under a microscope to descide whether the cells are malignant (cancerous) or benign (non-cancerous).  \n\n\nNow, we have the dataset which is computed from a digitized image of [fine needle aspiration (FNA)](https:\/\/www.worcsacute.nhs.uk\/pathology\/pathology-fine-needle-aspiration) of a breast mass. It consists of 33 attributes and 569 subjects.  \nData from Kaggle Dataset: [Breast Cancer Wisconsin (Diagnostic) Data set](https:\/\/www.kaggle.com\/uciml\/breast-cancer-wisconsin-data)  \nAlso can be found on [UCI Machine Learning Repository](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+%28Diagnostic%29)\n\n\n### Objective  \nWe will try to predict breast cancer from the dataset, a digitized image of a fine needle aspiration (FNA) of a breast mass.\n   * The target is ```diagnosis``` which consists of a categorical variable, ```M```(Malignant), and ```B```(Benign).  \n\n\n### Techniques  \nWe will built the binary classifications of Logistic Regression (LR) and Random Forest Classifier (RF). We also perform Recursive Feature Elimination with Cross-validation (RFECV) for both models, Logistic Regression with Recursive Feature Elimination with Cross-validation (LR-RFECV) and Random Forest Classifier with Recursive Feature Elimination with Cross-validation (RF-RFECV) to select the best subset of features. After RFECV technique, we will implement GridSearchCV(gs) for both LR-RFECV and RF-RFECV to choose thier best hyperparameters.","c448eb93":"### Scatterplot  \nWe'll show the strongly positive correlations between features by using scatter plot in order from the highest covariance value. ","8dcb13fd":"### Data Reduction\nWe will remove ```Unnamed: 32```, includes a missing value,'NaN'. This column will be removed for training a model to understand general patterns.  \n\n### Feature Encoding  \nNow, we have a categorical label, ```diagnosis```. We will encode the categorical value into numberical one to transform a non-numerical label to a numerical label.","c1f6f8ce":"# 4. Exploratory Data Analysis\n**Exploratory Data Analysis (EDA)** is a critical process of visualizing, summarizing and interpreting the dataset to allows us to discover certain insights, patterns and statistical measure.\n\n### 4-1. Univariate Analysis  \n[**Univariate Analysis**](https:\/\/deepai.org\/machine-learning-glossary-and-terms\/univariate-analysis) is the simplest form of data analysis. It doesn't deal with causes or relationships. This technique allows us to provide a summary and associated patterns of dataset.  \n\n\n### Proportion of ```diagnosis``` (Malignant vs Benign)  \nExplaining the frequency of **<span style=\"color:blue\">Benign<\/span>** (non-cancerous) and **<span style=\"color:red\">Malignant<\/span>** (cancerous) in ```diagnosis```. ","dd2cb684":"### Pearson's Correlation Coefficient\n[**Pearson's correlation coefficient**](https:\/\/www.investopedia.com\/terms\/c\/correlationcoefficient.asp) is the most common test statistics that measures the strength and direction of a linear relationship between two continuous variables. \n* The closer to +1.0 a value is, the more positive the relationship between the variables is. On the other hand, the closer to -1.0 a value is, the more negative it is.\n* Values less than +0.8 or larger than -0.8 are not significant relationships.","e26e5f8c":"### Random Forest (RF)","fe104c8f":"# 6-3. GridSearchCV for Models with RFECV  \n[GridSearchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html) is \nIt allows you to select the best hyperparameters and fit models on the training dataset.\n\n### LR-RFECV with GridSearchCV (LR-RFECV-gs)  ","b22a2d1f":"**Standard Error (SE)**","dc8d5099":"We are going to use [```LabelEncoder```](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html) for encoding the target value, ```diagnosis```."}}