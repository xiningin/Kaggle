{"cell_type":{"3275a4e3":"code","bcde46e3":"code","637e142b":"code","da73ae02":"code","15d3994b":"code","bc67fdde":"code","6a64f4d2":"code","707a9237":"code","843c51e6":"code","c5e27aa2":"code","10a4adf5":"code","24998ee3":"code","f997e96e":"code","fc3ec36a":"code","286d8d41":"code","d3f392fe":"code","a0b53caa":"code","3345a7bd":"code","7dc8f5f8":"code","1c6d7eb5":"code","617f7fb4":"code","b3c6718f":"code","48aa1d31":"code","73294066":"code","ee496af1":"code","4fdcbd1c":"code","6cb118df":"code","7f8fd4ce":"code","47ce4738":"code","1bf589ab":"code","69fc7110":"code","e53303d4":"code","cfb7dd49":"code","5eef12ff":"code","22469e35":"code","7cd27288":"code","a8a2a524":"code","9d7b2375":"code","73299017":"code","0cdd7242":"code","2a84dc86":"code","fc145cd2":"code","ac90ae30":"code","b2763d69":"code","e2e7fae8":"code","c8d78313":"code","8fea114d":"code","02aa6c8f":"code","6c57ea40":"code","0797a277":"code","16b9ee41":"code","f988c3a9":"code","ea2fd39f":"code","baecbce3":"code","f8e85269":"code","d4277290":"code","203288dc":"code","14ee961f":"code","47ff6de9":"code","50b22a7b":"code","047cff6d":"code","2507f64c":"code","e59e7eb5":"code","2256cfef":"code","dfc00d33":"code","3c9fb7cb":"code","de38a14b":"code","71dadda3":"code","569888aa":"code","427f5240":"code","33ba05fd":"markdown","07d0ced3":"markdown","58c69280":"markdown"},"source":{"3275a4e3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        url = os.path.join(dirname, filename)\n        print(url)\n\n# Any results you write to the current directory are saved as output.","bcde46e3":"import os\nimport sys\nimport requests\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import accuracy_score, classification_report\n","637e142b":"# I Just need the Close\n# url='\/kaggle\/input\/dataset-tree\/Tree Training Dataset.csv'\n","da73ae02":"def read_data(url):\n\n    data = pd.read_csv(url)\n    \n    # sort the values by symbol and then date\n#     data.sort_values(by = ['symbol','datetime'], inplace = True)\n    cols = ['Date','Price','Open','High','Low','Vol.','Change']\n    data = data[cols]\n    \n#     top_row = pd.DataFrame({'Date':['June 22, 2020'], 'Price':[0], 'Open':[0], 'High':[0],'Low':[0],'Vol.':[0],'Change':[0]})\n#     top_row = pd.DataFrame({'Date':['June 22, 2020']})\n    \n#     # Concat with old DataFrame and reset the Index.\n#     data = pd.concat([top_row, data]).reset_index(drop = True)\n\n    send_data = data.loc[:,:].values\n    \n    return cols, data, send_data\n    ","15d3994b":"def add_change_in_price(data):\n    # calculate the change in price\n    data['change'] = data['Price'].diff()\n    return data","bc67fdde":"cols, price_data, array_data = read_data(url)\n# price_data = add_change_in_price(price_data)\nprint('Features are:', cols)\nprice_data = price_data.iloc[::-1]\nprint(price_data)","6a64f4d2":"print(price_data.shape)\nfor i in range (len(cols)):\n    print(type(price_data[cols[i]][5]))","707a9237":"def calculate_rsi(data):\n    # Calculate the 14 day RSI\n    n = 14\n\n    # First make a copy of the data frame twice\n    up_df, down_df = data[['Change']].copy(), data[['Change']].copy()\n\n    # # For up days, if the change is less than 0 set to 0.\n    up_df.loc['Change'] = up_df.loc[(up_df['Change'] < 0), 'Change'] = 0\n\n    # # For down days, if the change is greater than 0 set to 0.\n    down_df.loc['Change'] = down_df.loc[(down_df['Change'] > 0), 'Change'] = 0\n\n    # # We need change in price to be absolute.\n    down_df['Change'] = down_df['Change'].abs()\n\n    # # Calculate the EWMA (Exponential Weighted Moving Average), meaning older values are given less weight compared to newer values.\n    ewma_up = up_df['Change'].transform(lambda x: x.ewm(span = n).mean())\n    ewma_down = down_df['Change'].transform(lambda x: x.ewm(span = n).mean())\n\n    # # Calculate the Relative Strength\n    relative_strength = ewma_up \/ ewma_down\n\n    # # Calculate the Relative Strength Index\n    relative_strength_index = 100.0 - (100.0 \/ (1.0 + relative_strength))\n\n    # # Add the info to the data frame.\n    data['down_days'] = down_df['Change']\n    data['up_days'] = up_df['Change']\n    data['RSI'] = relative_strength_index\n\n    # print(price_data.shape)\n\n    # price_data\n    return data","843c51e6":"price_data = calculate_rsi(price_data)\n\n# Display the head.\nprice_data.head(30)","c5e27aa2":"def calculate_stos(data):\n    # Calculate the Stochastic Oscillator\n    n = 14\n\n    # Make a copy of the high and low column.\n    low_14, high_14 = data['Low'].copy(), data['High'].copy()\n\n    # low_14 = low_14.apply(pd.to_numeric, errors='coerce')\n    # high_14 = high_14.apply(pd.to_numeric, errors='coerce')\n\n    # # # Group by symbol, then apply the rolling function and grab the Min and Max.\n    # low_14 = low_14.transform(lambda x: x.rolling(window = n).min())\n    # high_14 = high_14.transform(lambda x: x.rolling(window = n).max())\n    high_14 = high_14.rolling(n).max()\n    low_14 = low_14.rolling(n).min()\n\n    # # # # Calculate the Stochastic Oscillator.\n    k_percent = 100 * ((data['Price'] - low_14) \/ (high_14 - low_14))\n\n    # # # Add the info to the data frame.\n    data['low_14'] = low_14\n    data['high_14'] = high_14\n    data['k_percent'] = k_percent\n    return data\n","10a4adf5":"price_data = calculate_stos(price_data)\n# Display the head.\nprice_data.tail(1)","24998ee3":"def calculate_william_r(data):\n    # Calculate the Williams %R\n    n = 14\n\n    # Make a copy of the high and low column.\n    # low_14, high_14 = price_data['Low'].copy(), price_data['High'].copy()\n\n    # # Group by symbol, then apply the rolling function and grab the Min and Max.\n    # low_14 = low_14.transform(lambda x: x.rolling(window = n).min())\n    # high_14 = high_14.groupby('symbol')['high'].transform(lambda x: x.rolling(window = n).max())\n    low_14, high_14 = data['Low'].copy(), data['High'].copy()\n\n    high_14 = high_14.rolling(n).max()\n    low_14 = low_14.rolling(n).min()\n\n    # # Calculate William %R indicator.\n    r_percent = ((high_14 - data['Price']) \/ (high_14 - low_14)) * - 100\n\n    # # Add the info to the data frame.\n    data['r_percent'] = r_percent\n    return data","f997e96e":"price_data = calculate_william_r(price_data)\n# Display the head.\nprice_data.head(14)","fc3ec36a":"def calculate_macd(data):\n    # Calculate the MACD\n    ema_26 = data['Price'].transform(lambda x: x.ewm(span = 26).mean())\n    ema_12 = data['Price'].transform(lambda x: x.ewm(span = 12).mean())\n    macd = ema_12 - ema_26\n    print(price_data.shape)\n    # Calculate the EMA\n    ema_9_macd = macd.ewm(span = 9).mean()\n\n    # # Store the data in the data frame.\n    data['MACD'] = macd\n    data['MACD_EMA'] = ema_9_macd\n    \n    return data","286d8d41":"price_data = calculate_macd(price_data)\n# Print the head.\nprice_data.head(30)","d3f392fe":"def calculate_price_roc(data):\n    # Calculate the Price Rate of Change\n    n = 9\n\n    # Calculate the Rate of Change in the Price, and store it in the Data Frame.\n    data['Price_Rate_Of_Change'] = data['Price'].transform(lambda x: x.pct_change(periods = n))\n    \n    return data\n    ","a0b53caa":"price_data = calculate_price_roc(price_data)\n# Print the first 30 rows\nprice_data.head(33)","3345a7bd":"# Create a column we wish to predict\ndef create_prediction(data):\n    # Group by the `Symbol` column, then grab the `Close` column.\n    price_groups = data['Price']\n\n    # Apply the lambda function which will return -1.0 for down, 1.0 for up and 0.0 for no change.\n    price_groups = price_groups.transform(lambda x : np.sign(x.diff()))\n\n    # add the data to the main dataframe.\n    data['Prediction'] = price_groups\n\n    # for simplicity in later sections I'm going to make a change to our prediction column. To keep this as a binary classifier I'll change flat days and consider them up days.\n    data.loc[data['Prediction'] == 0.0] = 1.0\n    \n    return data\n\n# OPTIONAL CODE: Dump the data frame to a CSV file to examine the data yourself.\n# price_data.to_csv('final_metrics.csv')","7dc8f5f8":"price_data = create_prediction(price_data)\n# print the head\nprice_data.tail(10)\n","1c6d7eb5":"# We need to remove all rows that have an NaN value.\nprint('Before NaN Drop we have {} rows and {} columns'.format(price_data.shape[0], price_data.shape[1]))\n\n# Any row that has a `NaN` value will be dropped.\nprice_data = price_data.dropna()\n\n# Display how much we have left now.\nprint('After NaN Drop we have {} rows and {} columns'.format(price_data.shape[0], price_data.shape[1]))\n\n# Print the head.\nprice_data.head()","617f7fb4":"features = ['RSI','low_14','high_14','k_percent','r_percent','MACD','MACD_EMA','Price_Rate_Of_Change']\ncollection = ['Date','RSI','low_14','high_14','k_percent','r_percent','MACD','MACD_EMA','Price_Rate_Of_Change']\ntarget = ['Prediction']\nx_collect = price_data[collection]\nx_data = price_data[features]\ny_data = price_data[target]\nx_collect\n","b3c6718f":"price_data.shape","48aa1d31":"top_row = pd.DataFrame({'Date':['June 22, 2020'],'RSI':[0],'low_14':[0], 'high_14':[0],'k_percent':[0],\n                        'r_percent':[0],'MACD':[0],'MACD_EMA':[0],'Price_Rate_Of_Change':[0]})\n    \nprice_data1 = price_data\nprice_data1 = price_data1.iloc[::-1]\n\n# Concat with old DataFrame and reset the Index.\ndf = price_data1[collection]\n\n# df.drop(df.tail(1).index,inplace=True)\ndf = pd.concat([top_row, df])\n# df = df.iloc[::-1]\nprint(df.shape)\ndf","73294066":"df = df.iloc[::-1]","ee496af1":"df","4fdcbd1c":"df = df[features]\nmy = df.loc[:,:].values\nmy\ndg = pd.DataFrame(my)\n# dg = dg.iloc[::-1]\ndg","6cb118df":"dg\n# dg = dg.reindex(index=dg.index[::-1])\n# dg = dg.iloc[::-1]\n\n# dg.loc[278:278,:]\n# price_data.loc[1:1,:]","7f8fd4ce":"price_data","47ce4738":"dg = dg.iloc[::-1]\ndg.head(5)","1bf589ab":"price_data1 = price_data\nprice_data1 = price_data1.iloc[::-1]\n\n# price_data1[features] = dg\n\nprice_data1.head(5)","69fc7110":"# 52.182112\t1671.7\t1761.0\t66.517357\t-33.482643\t3.448006\t3.101512\t","e53303d4":"x_data","cfb7dd49":"y_data","5eef12ff":"plot_data = pd.DataFrame(x_data)\nplot_data[\"Pre\"] = y_data\nsns.pairplot(plot_data, hue='Pre', palette='OrRd')","22469e35":"def split_data(x1, y1):\n    # Split X and y into X_\n    X_train, X_test, y_train, y_test = train_test_split(x1, y1, random_state = 0)\n    return X_train, X_test, y_train, y_test\n","7cd27288":"def apply_randomforest(x, y):\n    # Create a Random Forest Classifier\n    rand_frst_clf = RandomForestClassifier(n_estimators = 200, oob_score = True, criterion = \"gini\", random_state = 0)\n    \n    X_train, X_test, y_train, y_test = split_data(x, y)\n    # Fit the data to the model\n    rand_frst_clf.fit(X_train, y_train)\n\n    # Make predictions\n    y_pred = rand_frst_clf.predict(X_test)\n    \n    correct = accuracy_score(y_test, y_pred, normalize = True) * 100.0\n    print('Correct Prediction (%): ', correct)\n    \n    return correct, y_pred","a8a2a524":"pred_score, y_pred = apply_randomforest(x_data, y_data)\ny_pred","9d7b2375":"count = np.zeros(price_data.shape[0])\n# add=0\nfor i in range (price_data.shape[0]):\n    count[i] = i\n# count","73299017":"x_array = x_data.loc[:,:].values\n\nfor i in range (x_array.shape[1]):\n    plt.scatter(x_array[:,i], count)\n# plt.scatter(y_data, count)","0cdd7242":"from sklearn.decomposition import PCA\npca_ml1 = PCA(n_components=4)\npca_fit = pca_ml1.fit_transform(x_data)\n\npca_df = pd.DataFrame(data = pca_fit\n             , columns = ['pc1', 'pc2', 'pc3', 'pc4'])\npca_df.head(5)","2a84dc86":"sns.pairplot(pca_df)\nplt.show()","fc145cd2":"pca_df[\"Pre\"] = y_data\nsns.pairplot(pca_df, hue='Pre', palette='OrRd')\n","ac90ae30":"count.shape\npca_fit[:,1].shape","b2763d69":"rf_pca_pred_score, rf_pca_pred = apply_randomforest(pca_df, y_data)","e2e7fae8":"def apply_knn(x, y):\n    from sklearn.neighbors import KNeighborsClassifier\n    model = KNeighborsClassifier(n_neighbors=3)\n    \n    X_train, X_test, y_train, y_test = split_data(x, y)\n    # Train the model using the training sets\n    \n    model.fit(X_train,y_train)\n\n    #Predict Output\n    pred = model.predict(X_test)\n    correct = accuracy_score(y_test, pred, normalize = True) * 100.0\n    print('Correct Prediction (%): ', correct)\n    return correct, pred","c8d78313":"knn_pred_score, pred = apply_knn(x_data, y_data)","8fea114d":"def apply_NB(x, y):\n    from sklearn.naive_bayes import GaussianNB\n    \n    model = GaussianNB()\n    \n    X_train, X_test, y_train, y_test = split_data(x, y)\n    \n    # Train the model using the training sets\n    model.fit(X_train,y_train)\n\n    #Predict Output\n    pred = model.predict(X_test)\n    correct = accuracy_score(y_test, pred, normalize = True) * 100.0\n    print('Correct Prediction (%): ', correct)\n    return correct, pred","02aa6c8f":"NB_pred_score, pred = apply_NB(x_data, y_data)","6c57ea40":"def apply_lda(x, y):\n#     from sklearn.lda import LDA\n    \n#     model = LDA()\n    \n    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n    model = LinearDiscriminantAnalysis()\n\n    X_train, X_test, y_train, y_test = split_data(x, y)\n    \n    print(X_train.shape)\n    print(y_train.shape)\n    \n    # Train the model using the training sets\n    model.fit(X_train,y_train.values.ravel())\n#     model.fit(X_train,y_train)\n#     model.fit(X_train)\n    lda_fit = model.transform(X_train)\n    lda_fit=0\n    \n    #Predict Output\n    pred = model.predict(X_test)\n    correct = accuracy_score(y_test, pred, normalize = True) * 100.0\n    print('Correct Prediction (%): ', correct)\n\n    \n    return correct, lda_fit, pred","0797a277":"lda_pred_score, lda_fit, lda_pred = apply_lda(x_data, y_data)","16b9ee41":"def apply_LG(x, y):\n    \n    from sklearn.linear_model import LinearRegression\n\n    model = LinearRegression()\n\n    X_train, X_test, y_train, y_test = split_data(x, y)\n    \n    # Train the model using the training sets\n    model.fit(X_train,y_train)\n#     LG_fit = model.fit(X_train,y_train).transform(X_train)\n    \n    #Predict Output\n    pred = model.predict(X_test)\n#     correct = accuracy_score(y_test, pred, normalize = True) * 100.0\n#     print('Correct Prediction (%): ', correct)\n#     return correct, pred\n    return y_test, pred","f988c3a9":"# LG_pred_score, LG_pred = apply_LG(x_data, y_data)\ny_te, LG_pred = apply_LG(x_data, y_data)\nLG_pred\n# correct = accuracy_score(y_test, pred, normalize = True) * 100.0\n#     print('Correct Prediction (%): ', correct)\n","ea2fd39f":"def apply_svm(x, y):\n    from sklearn.pipeline import make_pipeline\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.svm import SVC\n\n    X_train, X_test, y_train, y_test = split_data(x, y)\n    \n    # Train the model using the training sets\n#     model = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n    model = SVC()\n    model.fit(X_train, y_train)\n    \n    #Predict Output\n    pred = model.predict(X_test)\n    correct = accuracy_score(y_test, pred, normalize = True) * 100.0\n    print('Correct Prediction (%): ', correct)\n    return correct, pred","baecbce3":"svm_pred_score, svm_pred = apply_svm(x_data, y_data)\nsvm_pred_score","f8e85269":"def apply_decisiontree(x, y):\n    from sklearn import tree\n    \n    X_train, X_test, y_train, y_test = split_data(x, y)\n    \n    # Train the model using the training sets\n    model = tree.DecisionTreeClassifier()\n    model.fit(X_train, y_train)\n    tree.plot_tree(model)\n    \n    #Predict Output\n    pred = model.predict(X_test)\n    correct = accuracy_score(y_test, pred, normalize = True) * 100.0\n    print('Correct Prediction (%): ', correct)\n    return correct, pred","d4277290":"dt_pred_score, dt_pred = apply_decisiontree(x_data, y_data)\ndt_pred_score","203288dc":"plot_data = price_data.tail(50)\nplot_data","14ee961f":"def apply_randomforest_test(x, y):\n    # Create a Random Forest Classifier\n    rand_frst_clf = RandomForestClassifier(n_estimators = 200, oob_score = False, criterion = \"gini\", random_state = 0)\n    \n    X_train, X_test, y_train, y_test = split_data(x, y)\n    # Fit the data to the model\n    rand_frst_clf.fit(X_train, y_train)\n\n    # Make predictions\n    pred = rand_frst_clf.predict(X_test)\n    \n    correct = accuracy_score(y_test, pred, normalize = True) * 100.0\n    print('Correct Prediction (%): ', correct)\n    \n    return correct, pred\n","47ff6de9":"y11 = price_data.tail(1)[features]\ny11","50b22a7b":"price_data.tail(2)\ny11 = x_data.loc[2:2,:]\n# rsi=52.182112 low_14=1671.7 high_14=1761.0 k_percent=66.517357 r_percent=-33.482643 MACD=3.448006 MACD_EMA=3.101512 Price_roc=0.018595\ny11","047cff6d":"price_data.head(15)","2507f64c":"z_score, z_pred = apply_randomforest_test(x_data, y_data)\nz_score","e59e7eb5":"count_lda_pred = np.zeros(lda_pred.shape)\nfor i in range (lda_pred.shape[0]):\n    count_lda_pred[i] = i \nplt.scatter(lda_pred, count_lda_pred)","2256cfef":"plot_data = pd.DataFrame(x_data)\nplot_data['Prediction'] = y_data","dfc00d33":"\n# sns.relplot(x='Change', y='Prediction',  data=price_data)\n# sns.catplot(x='Date', y='Prediction',  data=plot_data)\nsns.regplot(x='RSI', y='Prediction',  data=plot_data)\nsns.regplot(x='r_percent', y='Prediction',  data=plot_data)\n","3c9fb7cb":"g = sns.FacetGrid(price_data, hue=\"Prediction\", hue_kws={\"marker\": [\"^\", \"v\"]})\n# g = sns.FacetGrid(plot_data, col=\"Prediction\", row=\"Date\")\ng.map(plt.scatter, \"RSI\", \"r_percent\", alpha=.7)\ng.add_legend();","de38a14b":"sns.pairplot(plot_data, hue='Prediction', palette='OrRd')","71dadda3":"g = sns.FacetGrid(plot_data, col=\"Prediction\", height=4, aspect=.5)\ng.map(sns.barplot, \"RSI\", \"r_percent\");","569888aa":"x_data.loc[2:2,:]\n# rsi=52.182112 low_14=1671.7 high_14=1761.0 k_percent=66.517357 r_percent=-33.482643 MACD=3.448006 MACD_EMA=3.101512 Price_roc=0.018595","427f5240":"price_data.tail()","33ba05fd":"Applying PCA","07d0ced3":"Applying Decision Tree","58c69280":"The correlogram is an array of scatterplots, for each pair of principal components. The dimension of this array of graphs is obviously equal to the number of elements in the dataframe.\n\nAlong the diagonal Seaborn plots by default the histogram of the relevant variable, in our case the distribution of values of the principal components.\n\nTo learn some more about the data, let\u2019s use the additional information about the labels associated with the spectra"}}