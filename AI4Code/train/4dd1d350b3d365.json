{"cell_type":{"abecc58a":"code","bc51992e":"code","fbef53d5":"code","35cc60d7":"code","9387bb0a":"code","f73baebb":"code","e6de60b4":"code","3e12543b":"code","c35416b6":"markdown","9ce21827":"markdown","6f70b4e6":"markdown"},"source":{"abecc58a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bc51992e":"df=pd.read_csv('\/kaggle\/input\/mushroom-classification\/mushrooms.csv')","fbef53d5":"df\n#df.info()\n#train.columns.tolist()","35cc60d7":"target = df[\"class\"]\ntrain = df.drop('class',axis=1)\nfrom  sklearn.preprocessing import LabelEncoder\nly=LabelEncoder()\n\n\ntarget=ly.fit_transform(target)\ntarget=pd.DataFrame(target,columns=['class'])\n#target\n\n#train.apply(LabelEncoder().fit_transform)\n\n\nfor i in train:\n    x=ly.fit_transform(train[i])\n    y=pd.DataFrame(x,columns=[i])\n    #train[i] = np.where(y[i] == 0, y[i],train[i])\n    #train[i]\n    train = pd.concat([train, y], join = 'outer', axis = 1) \n    \n    \n#train=ly.fit_transform(train)\n#train=pd.DataFrame(train,columns=['cap-shape',\ntrain=train.select_dtypes(exclude=['object'])\ntrain\n","9387bb0a":"# importing utility modules\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\n\n# importing machine learning models for prediction\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm\n\n\n# importing voting classifer\nfrom sklearn.ensemble import VotingClassifier\n\n\n\n\n# Splitting between train data into training and validation dataset\nX_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.20)\n\n# initializing all the model objects with default parameters\nmodel_1 =tree.DecisionTreeClassifier(random_state=0)\nmodel_2 = svm.SVC(random_state = 1)\nmodel_3 = RandomForestClassifier(n_estimators = 1000, random_state = 1)\n\n# Making the final model using voting classifier\nfinal_model = VotingClassifier(estimators=[('lr', model_1), ('svm', model_2), ('rf', model_3)], voting='hard')\n\n# training all the model on the train dataset\nfinal_model.fit(X_train, y_train)\n\n# predicting the output on the test dataset\npred_final = final_model.predict(X_test)\n\n# printing log loss between actual and predicted value\nprint(log_loss(y_test, pred_final))\n","f73baebb":"# importing utility modul# importing utility modules\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# importing machine learning models for prediction\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# importing voting classifer\nfrom sklearn.ensemble import VotingClassifier\n\n\n\n\n# Splitting between train data into training and validation dataset\nX_train, X_test, y_train, y_test = train_test_split(\n    train, target, test_size=0.20)\n  \n# initializing the boosting module with default parameters\nmodel = GradientBoostingRegressor()\n  \n# training the model on the train dataset\nmodel.fit(X_train, y_train)\n  \n# predicting the output on the test dataset\npred_final = model.predict(X_test)\n  \n# printing the root mean squared error between real value and predicted value\nprint(mean_squared_error(y_test, pred_final))\n \n","e6de60b4":"#Although there ensemble is not needed because classification -random_forest_classifier\n#alone giving good accuracy","3e12543b":"rf = RandomForestClassifier(n_estimators = 1000, random_state = 1)\nrf.fit(X_train, y_train)\n\nrf.score(X_test,y_test)","c35416b6":"# Boosting","9ce21827":"#in regression\nthings in mind\nclean() ------either delet null rows or col\n        ------or fill it by clean func()\n\nscalling of num columns    (min max\/standard)\nencoding of cat columns    (ordinal----dummy or one hot encoder\n                            numeral----label encoder)\n\nafter that\nduring train test split   do stratify(ratio mantain)\n\n\n\nin cat\nx should be in numerical   ---------- encoding\n\n","6f70b4e6":"# Voting"}}