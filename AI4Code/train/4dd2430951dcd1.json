{"cell_type":{"37358a7b":"code","7ecf15c4":"code","9aa19a22":"code","6e3ffb49":"code","1307a4b2":"code","6076acda":"code","5cb2b066":"code","8c493204":"code","ba8b6494":"code","4bab9b40":"code","b92e1e6b":"code","a76f3df9":"code","e9c523a6":"code","0b379718":"code","ac4a82ee":"code","82d82b70":"code","ba86bfda":"code","ad4e9ae0":"code","a69d8561":"code","950c9241":"code","cf491e27":"code","b1c81ba0":"code","9043d1d6":"code","7ac60119":"code","2f500e27":"code","2d8ae2ab":"code","6a85c2e0":"code","ec0ad653":"code","0388d981":"code","2f4e0326":"code","8f0438f7":"code","e713e030":"code","12d1a42f":"code","515b945e":"code","7c4fc89b":"markdown","da029f8e":"markdown","2321816e":"markdown","697d6139":"markdown"},"source":{"37358a7b":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np","7ecf15c4":"(trainX,trainY),(testX,testY) = tf.keras.datasets.fashion_mnist.load_data()","9aa19a22":"print(trainX.shape)\nprint(trainY.shape)\nprint(testX.shape)\nprint(testY.shape)","6e3ffb49":"np.set_printoptions(linewidth = 200)","1307a4b2":"trainX[5]","6076acda":"import matplotlib.pyplot as plt\nplt.imshow(trainX[5000])","5cb2b066":"#Preprocess data\ntrainX = trainX\/255.0\ntestX = testX\/255.0","8c493204":"trainY1 = tf.keras.utils.to_categorical(trainY, num_classes=10)\ntestY1 = tf.keras.utils.to_categorical(testY, num_classes=10)","ba8b6494":"# Model Building with Dense 100 and Dense 10\ntf.keras.backend.clear_session()\n\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Flatten(input_shape=(28,28,)))\nmodel.add(tf.keras.layers.Dense(100, activation='relu'))\nmodel.add(tf.keras.layers.Dense(10, activation='softmax'))","4bab9b40":"model.summary()","b92e1e6b":"#sgd_optimzer = tf.keras.optimizers.SGD(learning_rate = 0.03)\n\nmodel.compile(optimizer='adam', loss = 'categorical_crossentropy', metrics=['accuracy'])","a76f3df9":"# fitting the model\nhistory = model.fit(trainX,trainY1, epochs = 10, validation_data=(testX,testY1), batch_size = 512)","e9c523a6":"#Model Evaluation\ntest_loss, test_acc= model.evaluate(testX,testY1,verbose=2)","0b379718":"#modifiedthe model to Dense 300 and Dense 10\nmodel2 = tf.keras.Sequential()\nmodel2.add(tf.keras.layers.Flatten(input_shape=(28,28,)))\nmodel2.add(tf.keras.layers.Dense(300, activation='relu'))\nmodel2.add(tf.keras.layers.Dense(10, activation='softmax'))","ac4a82ee":"model2.summary()","82d82b70":"model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","ba86bfda":"model2.fit(trainX, trainY1, epochs=10, batch_size=512, validation_data=(testX,testY1))","ad4e9ae0":"#MOdel Prediction\npredictions = model2.predict(testX)\npredictions[0]","a69d8561":"#plotting accuracy\nplt.plot(history.history['accuracy'])\n#plt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","950c9241":"# plotting loss\n#plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","cf491e27":"from sklearn import metrics\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.metrics import classification_report\n\npredy = [np.argmax(i) for i in predictions]\ntesty2 = [np.argmax(i) for i in testY1]\n\nmatrix =metrics.confusion_matrix(testy2,predy)\nmatrix","b1c81ba0":"df = pd.DataFrame(matrix)\ndf","9043d1d6":"summ = []\nsc = 0\nfor i in range(df.shape[0]):\n    for j in range(df.shape[1]):\n        if i!=j:\n            sc = sc + df[i][j]\n    summ.append(sc)\n    sc = 0\n    \n# Number of correct and incorrect predictions in each class\ndf2 = pd.DataFrame()\ndf2['Class'] = df.columns\ndf2['Num. of incorrect predictions'] = summ\ndf2['Num of correct predictions'] = pd.Series(np.diag(df))","7ac60119":"df2","2f500e27":"print(classification_report(testy2,predy))","2d8ae2ab":"report = classification_report(testy2, predy, output_dict=True)","6a85c2e0":"#Precision, recall and F1 score as data frame for each class\ndf3 = pd.DataFrame(report).transpose()\ndf3","ec0ad653":"df3 = df3.iloc[0:10,:]\ndf4 = pd.concat([df2.reset_index(drop = True),df3.reset_index(drop = True)], axis=1)\ndf4","0388d981":"# Model Building with Dense 100 and Dense 10\ntf.keras.backend.clear_session()\n\nmodel3 = tf.keras.Sequential()\nmodel3.add(tf.keras.layers.Flatten(input_shape=(28,28,)))\nmodel3.add(tf.keras.layers.Dense(100, activation='relu'))\nmodel3.add(tf.keras.layers.Dense(10, activation='softmax'))\n\nmodel3.compile(loss='sparse_categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n\nmodel3.summary()","2f4e0326":"history1 = model3.fit(trainX,trainY, batch_size=512, epochs=10, validation_data=(testX,testY))","8f0438f7":"from sklearn.metrics import confusion_matrix, classification_report \n\npreds = np.argmax(model3.predict(testX), axis=-1)\nconf_matrix = confusion_matrix(testY, preds)\nprint(conf_matrix)","e713e030":"conf_matrixtf = tf.math.confusion_matrix(testY, preds)\nconf_matrixtf","12d1a42f":"correct_classifications = np.diag(conf_matrix) \nnp.fill_diagonal(conf_matrix,0)\nmisclassifications = conf_matrix.sum(axis = 1)\ndf = pd.DataFrame({'class_name': range(0,10), 'misclassifications': misclassifications})\ndf","515b945e":"print(classification_report(testY, preds))","7c4fc89b":"<center><h1 style=\"color:purple\"><b>Fashion MNIST Dataset: Simple Neural Network<\/b><\/h1><\/center>","da029f8e":"This is my first Neural Network Code on Kaggle. Here I am trying to show how categorical_crossentropy and sparse_categorical_crossenrtropy differs. Constructive Criticisms are welcome.\n\nBoth categorical cross entropy and sparse categorical cross-entropy have the same loss function. The only difference between the two is on how truth labels are defined.\n- Categorical cross-entropy is used when true labels are one-hot encoded, for example, we have the following true values for 3-class classification problem [1,0,0], [0,1,0] and [0,0,1].\n- In sparse categorical cross-entropy , truth labels are integer encoded, for example, [1], [2] and [3] for 3-class problem.\n\nTo read more about loss functions, click [here](https:\/\/towardsdatascience.com\/cross-entropy-loss-function-f38c4ec8643e)","2321816e":"## With Sparse Categorical CrossEntropy","697d6139":"## With Categorical CrossEntropy"}}