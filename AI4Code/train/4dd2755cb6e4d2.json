{"cell_type":{"847a0dab":"code","2b017649":"code","4bfefae6":"code","52cd5844":"code","261bcc1c":"code","aaf947e2":"code","d9fccfee":"code","e1866b82":"code","64f26ed8":"code","20655df2":"code","0e680ffe":"code","1c9c86b3":"code","a1319fdf":"code","bd6cb8bf":"code","f6ef349a":"code","6fab0549":"code","c36bef61":"code","d5694972":"code","3c7ffa7e":"code","b386cfa9":"code","8aed1de9":"code","23287192":"code","bd0b2626":"code","34c708cb":"code","6e3b130e":"code","0a8f52d6":"code","0850886f":"markdown","6ea571d5":"markdown","daf45874":"markdown","c3103489":"markdown","f0d68b8f":"markdown","4e7f4839":"markdown","0b65727f":"markdown","a48ed1fa":"markdown","70f6f633":"markdown","1534aefc":"markdown","759fdca4":"markdown","7e401361":"markdown","42e5b507":"markdown","c4161ebb":"markdown","5990ff9a":"markdown","2089f893":"markdown","7fa95528":"markdown"},"source":{"847a0dab":"import keras\nimport numpy as np\nimport os\nimport matplotlib.pylab as plt\nfrom imgaug import augmenters\nfrom keras.layers import Dense, Input, Conv2D, LSTM, MaxPool2D, UpSampling2D, Flatten\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical\nfrom numpy import argmax, array_equal\nimport matplotlib.pyplot as plt\nfrom keras.models import Model\nfrom imgaug import augmenters","2b017649":"# from keras.datasets import mnist\n\ndef load_mnist_dataset(path):\n    with np.load(path) as f:\n        x_train, y_train = f['x_train'], f['y_train']\n        x_test, y_test = f['x_test'], f['y_test']\n        return (x_train, y_train), (x_test, y_test)\n    \n(train_data, train_labels), (test_data, test_labels) = load_mnist_dataset('..\/input\/mnist.npz')\ntrain_x, val_x = train_test_split(train_data, test_size=0.2)\n\ntrain_x = train_x\/255.\nval_x = val_x\/255.\ntrain_x = train_x.reshape(-1, 28, 28, 1)\nval_x = val_x.reshape(-1, 28, 28, 1)\n","4bfefae6":"\ngaussian_noise = augmenters.GaussianBlur(sigma=(0, 3.0))\nsequential_object = augmenters.Sequential([gaussian_noise])\n\ntrain_x_noisy = sequential_object.augment_images(train_x * 255) \/ 255\nval_x_noisy = sequential_object.augment_images(val_x * 255) \/ 255","52cd5844":"f, ax = plt.subplots(1,10)\nfor i in range(0,10):\n    ax[i].imshow(train_x[i].reshape(28, 28))\nplt.show()","261bcc1c":"f, ax = plt.subplots(1,10)\nfor i in range(0,10):\n    ax[i].imshow(train_x_noisy[i].reshape(28, 28))\nplt.show()","aaf947e2":"input_layer = Input(shape=(28, 28, 1))\n\nencoded_layer1 = Conv2D(64, (3, 3), activation='relu', padding='same')(input_layer)\nencoded_layer1 = MaxPool2D( (2, 2), padding='same')(encoded_layer1)\nencoded_layer2 = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded_layer1)\nencoded_layer2 = MaxPool2D( (2, 2), padding='same')(encoded_layer2)\nencoded_layer3 = Conv2D(16, (3, 3), activation='relu', padding='same')(encoded_layer2)\nencoded_layer3 = MaxPool2D( (2, 2), padding='same')(encoded_layer3)\n\ndecoded_layer1 = Conv2D(16, (3, 3), activation='relu', padding='same')(encoded_layer3)\ndecoded_layer1 = UpSampling2D((2, 2))(decoded_layer1)\ndecoded_layer2 = Conv2D(32, (3, 3), activation='relu', padding='same')(decoded_layer1)\ndecoded_layer2 = UpSampling2D((2, 2))(decoded_layer2)\ndecoded_layer3 = Conv2D(64, (3, 3), activation='relu')(decoded_layer2)\ndecoded_layer3 = UpSampling2D((2, 2))(decoded_layer3)\noutput_layer   = Conv2D(1, (3, 3), padding='same')(decoded_layer3)\n\nmodel = Model(input_layer, output_layer)\nmodel.compile(optimizer='adam', loss='mse')","d9fccfee":"model_train = model.fit(train_x_noisy, train_x, epochs=80, batch_size=2048, validation_data=(val_x_noisy, val_x))","e1866b82":"loss = model_train.history['loss']\nval_loss = model_train.history['val_loss']\nepochs = range(80)\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training loss and Validation loss')\nplt.legend()\nplt.show()","64f26ed8":"model.save_weights('model_noisy_autoencoder.h5')","20655df2":"f, ax = plt.subplots(1,10)\nfor i in range(0,10):\n    ax[i].imshow(val_x_noisy[i].reshape(28, 28))\nplt.show()","0e680ffe":"preds = model.predict(val_x_noisy[:10])\nf, ax = plt.subplots(1,10)\nfor i in range(0,10):\n    ax[i].imshow(preds[i].reshape(28, 28))\nplt.show()","1c9c86b3":"train_labels_categorical = to_categorical(train_labels)","a1319fdf":"train_x,val_x,train_label,val_label = train_test_split(train_data,train_labels_categorical,test_size=0.2,random_state=13)\n\n# train_data = train_data.reshape(-1, 28,28, 1)\ntrain_x = train_x.reshape(-1, 28, 28, 1)\nval_x = val_x.reshape(-1, 28, 28, 1)\n\ntrain_x.shape,val_x.shape,train_label.shape,val_label.shape\nnum_classes = 10","bd6cb8bf":"def encoder(input_img):\n    encoded_layer1 = Conv2D(64, (3, 3), activation='relu', padding='same')(input_layer)\n    encoded_layer1 = MaxPool2D( (2, 2), padding='same')(encoded_layer1)\n    encoded_layer2 = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded_layer1)\n    encoded_layer2 = MaxPool2D( (2, 2), padding='same')(encoded_layer2)\n    encoded_layer3 = Conv2D(16, (3, 3), activation='relu', padding='same')(encoded_layer2)\n    encoded_layer3 = MaxPool2D( (2, 2), padding='same')(encoded_layer3)\n    return encoded_layer3","f6ef349a":"def fully_connected(encoder):\n    flat_layer = Flatten()(encoder)\n    dense_layer = Dense(128, activation='relu')(flat_layer)\n    out = Dense(num_classes, activation='softmax')(dense_layer)\n    return out\n","6fab0549":"model_classification = Model(input_layer,fully_connected(encoder(input_layer)))","c36bef61":"for l1,l2 in zip(model_classification.layers[:7],model.layers[0:7]):\n    l1.set_weights(l2.get_weights())\n","d5694972":"for layer in model_classification.layers[0:7]:\n    layer.trainable = False\n    \nmodel_classification.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])","3c7ffa7e":"classify_train = model_classification.fit(train_x, train_label, batch_size=64,epochs=80,verbose=1,validation_data=(val_x, val_label))","b386cfa9":"model_classification.save_weights('classification_encoder_model.h5')","8aed1de9":"for layer in model_classification.layers[0:7]:\n    layer.trainable = True\n    \nmodel_classification.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])","23287192":"classify_train = model_classification.fit(train_x, train_label, batch_size=64,epochs=80,verbose=1,validation_data=(val_x, val_label))","bd0b2626":"model_classification.save_weights('classification_full_model.h5')","34c708cb":"test_data = test_data.reshape(-1, 28, 28, 1)\ntest_label_predicted_classes = model_classification.predict(test_data)\ntest_label_predicted_classes = np.argmax(np.round(test_label_predicted_classes),axis=1)\n","6e3b130e":"correct_cases = np.where(test_label_predicted_classes==test_labels)[0]\nprint(\"Found %d correct labels\", len(correct_cases))\nfor i, correct_cases in enumerate(correct_cases[:3]):\n    plt.subplot(3,3,i+1)\n    plt.imshow(test_data[correct_cases].reshape(28,28))\n    plt.title(\"Predicted {}, Class {}\".format(test_label_predicted_classes[correct_cases], test_labels[correct_cases]))\n","0a8f52d6":"incorrect_cases = np.where(test_label_predicted_classes!=test_labels)[0]\nprint(\"Found %d incorrect labels\" % len(incorrect_cases))\nfor i, incorrect_cases in enumerate(incorrect_cases[:3]):\n    plt.subplot(3,3,i+1)\n    plt.imshow(test_data[incorrect_cases].reshape(28,28))\n    plt.title(\"Predicted {}, Class {}\".format(test_label_predicted_classes[incorrect_cases], test_labels[incorrect_cases]))","0850886f":"**Classification using trained encoder part of autoencoder**","6ea571d5":"Traning and Validation loss ","daf45874":"Visualize added noise","c3103489":"**Sample Results**","f0d68b8f":"**Define Model**","4e7f4839":"**Train complete model**","0b65727f":"Save Model weights","a48ed1fa":"Predict using trained model","70f6f633":"## AutoEncoder Model","1534aefc":"## Setup","759fdca4":"Preprocess","7e401361":"**Predict Class Labels**","42e5b507":"Adding Gaussian Noise","c4161ebb":"Convert labels to one hot encoding","5990ff9a":"**Train with initial enocder layers as non-trainable**","2089f893":"Set initial layer's weight","7fa95528":"Loading MNIST dataset.(Due to some issue, need to download mnist.npz from https:\/\/s3.amazonaws.com\/img-datasets\/mnist.npz)"}}