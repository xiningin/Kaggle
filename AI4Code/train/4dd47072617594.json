{"cell_type":{"f74beddc":"code","f5b99437":"code","46906962":"code","a0183221":"code","50cbb27d":"code","eb64c5da":"code","512a3919":"code","4b312cc8":"code","b8445cca":"code","33b4426b":"code","553b5b4b":"code","d9b87494":"code","145f2efe":"code","a411069a":"code","fbba6c85":"code","51fde567":"code","e3306e69":"code","afe577ef":"code","dbb63766":"code","fd3987ee":"code","f8fbd00a":"code","a57d637d":"code","d7125f19":"code","8dcd1616":"code","fcc0ac0b":"code","616cc9c8":"code","914889a4":"code","fb3e3a87":"code","08065094":"code","a92b6218":"code","06c5cb32":"code","0b601ba7":"code","086452ff":"code","22134b3d":"code","0099f998":"code","363eb42b":"code","ca03c937":"code","88ce76b0":"code","67a26ea6":"code","10ffc3de":"code","603190fe":"code","75ed5018":"code","308749d1":"markdown","3f2dcf76":"markdown","f0f14b65":"markdown","083517dd":"markdown","10f51c79":"markdown","163a732f":"markdown","24905f92":"markdown","379555bb":"markdown","3160f7e9":"markdown","37124287":"markdown","1b7f0c56":"markdown"},"source":{"f74beddc":"# Installlation required\n# !pip install textblob\n# !pip install wordcloud\n# !pip install nltk\n\nfrom warnings import filterwarnings\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom nltk.corpus import stopwords\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, cross_validate\nfrom sklearn.preprocessing import LabelEncoder\nfrom textblob import Word, TextBlob\nfrom wordcloud import WordCloud\n\n# For appearance settings\nfilterwarnings('ignore')\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 200)\npd.set_option('display.float_format', lambda x: '%.2f' % x)","f5b99437":"# Helpers\ndef check_df(dataframe):\n    print(f\"\"\"\n        ##################### Shape #####################\\n\\n\\t{dataframe.shape}\\n\\n\n        ##################### Types #####################\\n\\n{dataframe.dtypes}\\n\\n\n        ##################### Head #####################\\n\\n{dataframe.head(3)}\\n\\n\n        ##################### NA #####################\\n\\n{dataframe.isnull().sum()}\\n\\n\n        ##################### Quantiles #####################\\n\\n{dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T}\\n\\n\"\"\")","46906962":"df = pd.read_csv(\"..\/input\/amazon-reviews\/amazon_reviews.csv\", sep=\",\")\ncheck_df(df)","a0183221":"# Normalizing Case Folding\ndf['reviewText'] = df['reviewText'].str.lower()\n\n# Punctuations\ndf['reviewText'] = df['reviewText'].str.replace('[^\\w\\s]', '')\n\n# Numbers\ndf['reviewText'] = df['reviewText'].str.replace('\\d', '')","50cbb27d":"# Stopwords\n\n# nltk.download('stopwords')\nsw = stopwords.words('english')\ndf['reviewText'] = df['reviewText'].apply(lambda x: \" \".join(x for x in str(x).split() if x not in sw))","eb64c5da":"# Rarewords\n\ndrops = pd.Series(' '.join(df['reviewText']).split()).value_counts()[-1000:]\ndf['reviewText'] = df['reviewText'].apply(lambda x: \" \".join(x for x in x.split() if x not in drops))","512a3919":"# Tokenization\n\n# nltk.download(\"punkt\")\ndf[\"reviewText\"].apply(lambda x: TextBlob(x).words).head()","4b312cc8":"# Lemmatization\n\n# nltk.download('wordnet')\ndf['reviewText'] = df['reviewText'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\ndf['reviewText'].head(10)","b8445cca":"# Terim Frekanslar\u0131n\u0131n Hesaplanmas\u0131\n\ntf = df[\"reviewText\"].apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis=0).reset_index()\n\ntf.columns = [\"words\", \"tf\"]\ntf.head()","33b4426b":"print(f\"\"\"Shape of the tf: {tf.shape}\nUnique words of dataframe: {tf[\"words\"].nunique()}\"\"\")","553b5b4b":"tf[\"tf\"].describe([0.05, 0.10, 0.25, 0.50, 0.75, 0.80, 0.90, 0.95, 0.99]).T","d9b87494":"# Barplot\n\ntf[tf[\"tf\"] > 500].plot.bar(x=\"words\", y=\"tf\")\nplt.show()","145f2efe":"# Wordcloud\n\ntext = \" \".join(i for i in df.reviewText)\nwordcloud = WordCloud().generate(text)\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","a411069a":"# We can use also white background \nwordcloud = WordCloud(max_font_size=50,\n                      max_words=100,\n                      background_color=\"white\").generate(text)\nplt.figure()\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()\n\n# When you want to save this image, you can use to_file method.\n# wordcloud.to_file(\"wordcloud.png\")","fbba6c85":"# NLTK already has a built-in, pretrained sentiment analyzer\n# called VADER (Valence Aware Dictionary and Sentiment Reasoner).\n\ndf.head()","51fde567":"# nltk.download('vader_lexicon')\nsia = SentimentIntensityAnalyzer()","e3306e69":"sia.polarity_scores(\"The film was awesome\")","afe577ef":"sia.polarity_scores(\"I liked this music but it is not good as the other one\")","dbb63766":"# make all review upper\ndf[\"reviewText\"].apply(lambda x: x.upper())","fd3987ee":"# Calculating the scores for 10 of review\ndf[\"reviewText\"][0:10].apply(lambda x: sia.polarity_scores(x))","f8fbd00a":"# selecting compound values \ndf[\"reviewText\"][0:10].apply(lambda x: sia.polarity_scores(x)[\"compound\"])","a57d637d":"# assigning compound values as a polarity_score\ndf[\"polarity_score\"] = df[\"reviewText\"].apply(lambda x: sia.polarity_scores(x)[\"compound\"])\ndf.head()","d7125f19":"# Feature Engineering\n\n# Target\ndf[\"reviewText\"][0:10].apply(lambda x: \"pos\" if sia.polarity_scores(x)[\"compound\"] > 0 else \"neg\")","8dcd1616":"# assigning neg and pos values as a sentiment_label\ndf[\"sentiment_label\"] = df[\"reviewText\"].apply(lambda x: \"pos\" if sia.polarity_scores(x)[\"compound\"] > 0 else \"neg\")\ndf.head(10)","fcc0ac0b":"# I checked that is there any unbalanced distribution in sentiment_label\ndf[\"sentiment_label\"].value_counts()","616cc9c8":"# I checked also mean of the sentiment_label\ndf.groupby(\"sentiment_label\")[\"overall\"].mean()","914889a4":"# encoding process for target variable\ndf[\"sentiment_label\"] = LabelEncoder().fit_transform(df[\"sentiment_label\"])","fb3e3a87":"# Determining of X and y for model\nX = df[\"reviewText\"]\ny = df[\"sentiment_label\"]","08065094":"vectorizer = CountVectorizer()\nX_count = vectorizer.fit_transform(X)\n\nvectorizer.get_feature_names()[10:15]\nX_count.toarray()[10:15]","a92b6218":"tf_idf_word_vectorizer = TfidfVectorizer()\nX_tf_idf_word = tf_idf_word_vectorizer.fit_transform(X)","06c5cb32":"# Logistic Regression\n\nlog_model = LogisticRegression().fit(X_tf_idf_word, y)\n\ncross_val_score(log_model,\n                X_tf_idf_word,\n                y, scoring=\"accuracy\",\n                cv=5).mean()","0b601ba7":"new_review = pd.Series(\"this product is great\")\nnew_review = CountVectorizer().fit(X).transform(new_review)\nlog_model.predict(new_review)","086452ff":"new_review1 = pd.Series(\"look at that shit very bad\")\nnew_review1 = CountVectorizer().fit(X).transform(new_review1)\nlog_model.predict(new_review1)","22134b3d":"new_review2 = pd.Series(\"it was good but I am sure that it fits me\")\nnew_review2 = CountVectorizer().fit(X).transform(new_review2)\nlog_model.predict(new_review2)","0099f998":"# output for the orijinal review\nrandom_review = pd.Series(df[\"reviewText\"].sample(1).values)\nrandom_review","363eb42b":"transform = CountVectorizer().fit(X).transform(random_review)\nlog_model.predict(transform)","ca03c937":"# Count Vectors\nrf_model = RandomForestClassifier().fit(X_count, y)\ncross_val_score(rf_model, X_count, y, cv=5, n_jobs=-1).mean()","88ce76b0":"# TF-IDF Word-Level\nrf_model = RandomForestClassifier().fit(X_tf_idf_word, y)\ncross_val_score(rf_model, X_tf_idf_word, y, cv=5, n_jobs=-1).mean()\n\n# TF-IDF N-GRAM\n#rf_model = RandomForestClassifier().fit(X_tf_idf_ngram, y)\n#cross_val_score(rf_model, X_tf_idf_ngram, y, cv=5, n_jobs=-1).mean()","67a26ea6":"# Hyperparameter Optimization\n\nrf_model = RandomForestClassifier(random_state=17)\n\nrf_params = {\"max_depth\": [5, 8, None],\n             \"max_features\": [5, 7, \"auto\"],\n             \"min_samples_split\": [2, 5, 8, 20],\n             \"n_estimators\": [100, 200, 500]}\n\nrf_best_grid = GridSearchCV(rf_model,\n                            rf_params,\n                            cv=5,\n                            n_jobs=-1,\n                            verbose=True).fit(X_count, y)\n\nrf_best_grid.best_params_\n\nrf_final = rf_model.set_params(**rf_best_grid.best_params_, random_state=17).fit(X_count, y)\n\ncv_results = cross_validate(rf_final, X_count, y, cv=3, scoring=[\"accuracy\", \"f1\", \"roc_auc\"])","10ffc3de":"rf_best_grid.best_params_","603190fe":"rf_final = rf_model.set_params(**rf_best_grid.best_params_, random_state=17).fit(X_count, y)","75ed5018":"cross_validate(rf_final, X_count, y, cv=3, scoring=[\"accuracy\", \"f1\", \"roc_auc\"])","308749d1":"# 5. Modeling","3f2dcf76":"**Thank you for taking the time.**","f0f14b65":"# 3. Sentiment Analysis","083517dd":"# A short introduction to NLP","10f51c79":"# 4. Sentiment Modeling","163a732f":"# 2. Text Visualization","24905f92":"# Random Forests","379555bb":"In this kernel I explore some fundamental NLP concepts and show how they can be implemented using the increasingly popular nltk package in Python. This post is for the absolute NLP beginner, but knowledge of Python is assumed.\n\n> **The two significant libraries used in NLP are NLTK and spaCy. There are substantial differences between them, which are as follows:**\n\n- NLTK provides a plethora of algorithms to choose from for a particular problem which is boon for a researcher but a bane for a developer. Whereas, spaCy keeps the best algorithm for a problem in its toolkit and keep it updated as state of the art improves.\n- NLTK supports various languages whereas spaCy have statistical models for 7 languages (English, German, Spanish, French, Portuguese, Italian, and Dutch). It also supports named entities for multi language.\n- NLTK is a string processing library. It takes strings as input and returns strings or lists of strings as output. Whereas, spaCy uses object-oriented approach. When we parse a text, spaCy returns document object whose words and sentences are objects themselves.\n- spaCy has support for word vectors whereas **NLTK does not**.\n- As spaCy uses the latest and best algorithms, its performance is usually good as compared to NLTK. As we can see below, in word tokenization and POS-tagging spaCy performs better, but in sentence tokenization, NLTK outperforms spaCy. Its poor performance in sentence tokenization is a result of differing approaches: NLTK attempts to split the text into sentences. In contrast, spaCy constructs a syntactic tree for each sentence, a more robust method that yields much more information about the text.\n\nI used NLTK for this kernel. You can find these steps in below.\n> 1. Text Preprocessing\n> 2. Text Visualization\n> 3. Sentiment Analysis\n> 4. Feature Engineering\n> 5. Sentiment Modeling","3160f7e9":"# 1. Text Preprocessing","37124287":"![img.png](https:\/\/www.emerald.com\/insight\/proxy\/img?link=\/resource\/id\/urn:emeraldgroup.com:asset:id:article:10_1016_j_aci_2019_11_003\/urn:emeraldgroup.com:asset:id:binary:ACI-j.aci.2019.11.003002.tif)","1b7f0c56":"# TF-IDF"}}