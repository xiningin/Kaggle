{"cell_type":{"dbf3f58d":"code","1ef5f8f3":"code","35da08ed":"code","9b948597":"code","9c80c0af":"code","188ef394":"code","f38c45f9":"code","6da6f29e":"code","b2d3392a":"code","d43808b0":"code","53612f82":"code","a66b14c4":"markdown","2814d416":"markdown","9b87c546":"markdown","f5184bf3":"markdown","425cd620":"markdown","bdb8ce32":"markdown","7b37f87d":"markdown","758f02eb":"markdown"},"source":{"dbf3f58d":"import re\nimport random\nimport numpy as np\nimport pandas as pd\n\nimport spacy\nnlp = spacy.load('en_core_web_sm')\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\n# Keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, Dropout\nfrom keras.callbacks import LambdaCallback, ModelCheckpoint\nfrom keras.utils.vis_utils import plot_model\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action='ignore', category=FutureWarning)","1ef5f8f3":"quotes_df = pd.read_csv('..\/input\/quotes-500k\/quotes.csv')\nquotes_df.columns = ['text', 'author', 'tags']              # I'm more comfortable with these names\nquotes_df.head(5)","35da08ed":"# Create word and character count\nquotes_df['word_count'] = quotes_df['text'].apply(lambda x: len(re.split('[?!.,:;-_ ]', str(x))))\nquotes_df['char_count'] = quotes_df['text'].apply(lambda x: len(str(x)))\n\nfig, ax = plt.subplots(1, 2, figsize = (20, 5))\n\nax[0].hist(quotes_df['word_count'], color = 'g')\nax[0].set_title('Words Count Distribution')\n\nax[1].hist(quotes_df['char_count'], color = 'r')\nax[1].set_title('Characters Count Distribution')\n\nplt.show()","9b948597":"# Select top 20 authors with most quotes count in the dataset\ntop_authors_with_most_quotes = quotes_df.groupby(\n    by = 'author',\n    as_index = False\n).count().sort_values(\n    by = 'text',\n    ascending = False\n).iloc[:20]\n\nfig = plt.figure(figsize = (20, 5))\nplt.bar(top_authors_with_most_quotes['author'], top_authors_with_most_quotes['text'], color = 'b')\nplt.xticks(rotation = 45)\nplt.show()","9c80c0af":"# Select top author\nauthor = top_authors_with_most_quotes['author'].iloc[0]\nprint('Selected author:', author)\n\nauthor_quotes_df = quotes_df.loc[quotes_df['author'] == author]\nauthor_quotes_df.head()","188ef394":"sample_size = int(len(author_quotes_df) * 0.3)\nselected_author_quotes_df = author_quotes_df[:sample_size].copy()\n\n# Conver to lowercase\nselected_author_quotes_df['text'] = selected_author_quotes_df['text'].apply(lambda x: x.lower())\n\n# Join all quotes as one\ntext = ' '.join(selected_author_quotes_df['text'].values)","f38c45f9":"window_size = 30\nwindow_step = 1\ntexts, next_tokens = [], []\n\nfor i in range(0, len(text) - window_size, window_step):\n    texts.append(text[i: i + window_size])            # Select n-1 characters as input\n    next_tokens.append(text[i + window_size])         # Select the n-th character as output\n\nprint('First 10 items of texts:')\ntexts[:10]","6da6f29e":"tokenizer = Tokenizer(\n    char_level = True,\n    lower = True\n)\n\n# Fit sentences to construct a our vocabulary and dictionary\ntokenizer.fit_on_texts(texts)\nvocabulary_size = len(tokenizer.word_index)\n\ndef encode(texts):\n    ''' Convert to sequences, reshape for LSTM and normalize '''\n    X = np.array(tokenizer.texts_to_sequences(texts))\n    X = np.reshape(X, (len(X), window_size, 1))\n    \n    return X \/ float(vocabulary_size)\n\nX = encode(texts)\n\n# One-Hot-Encoding\ny = np.array(tokenizer.texts_to_matrix(next_tokens))","b2d3392a":"# Model architecture\nmodel = Sequential()\n# model.add(Embedding(input_dim = vocabulary_size, output_dim = 10, input_length = window_size))\nmodel.add(LSTM(units = 256, input_shape = (X.shape[1], X.shape[2])))\n# model.add(Dropout(0.2))\n# model.add(LSTM(128))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units = y.shape[1], activation = 'softmax'))\n\n# Compile and plot the model\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop')\nplot_model(\n    model,\n    show_shapes = True,\n    show_dtype = True,\n    show_layer_names = True,\n    expand_nested = True,\n)","d43808b0":"checkpoint = ModelCheckpoint(\n    filepath = 'model.hdf5',\n    monitor = 'loss', \n    verbose = 2,\n    save_best_only = True,\n)\n\nmodel.fit(\n    X, y,\n    batch_size = 128,\n    epochs = 20,\n    verbose = 2,\n    callbacks = [checkpoint]\n)","53612f82":"model.load_weights('.\/model.hdf5')\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop')\n\nrand_index = random.randint(0, len(texts))\npattern = texts[rand_index]\noutput = pattern\n\nfor i in range(100):\n    \n    # Encode pattern\n    x = encode([pattern])\n    \n    pred = model.predict(x)\n    pred_index = np.argmax(pred)\n    pred_char = tokenizer.sequences_to_texts([[pred_index]])[0]\n    \n    pattern = pattern[1:] + str(pred_char)\n    output += str(pred_char)\n\nprint(output)","a66b14c4":"# Quotes-500k | Quote Generation","2814d416":"Good news is, most of our quotes have less than 200 words.\n### Selecting our beloved author\nSince we have a huge amount of data that's hard to train on and that we want the most reasonable quotes to be used in our model, let's select the top 20 authors with most number of quotes..","9b87c546":"### Creating two new features\nLet's start of by creating two features for counting *words* and *characters*. This gives us an understanding of how long quotes generally are.","f5184bf3":"Looks like our hypothesis wasn't wrong after all. Comparing to some other quotes in the same dataset, I would say it's more structured and less noisy. (The word casing is alright, they quotes aren't very long and are OK)\n### Selecting our text data\nSince `Debasish Mridha` has more than 6000 quotes, let's take a portion of them (20~30% is good enough).","425cd620":"### Creating our training data\nWe have two parameters `window_len` which is the number of characters we select for each training sequence and `window_step` which is the number of characters it jumps between our sequences.","bdb8ce32":"Looks like `Debasish Mridha` is our author of choice since it has more than 6500 quotes. My hypothesis is that the more quotes an author has, the more sence the quotes would have (Maybe more structure? and less noisy?)\nThis is not a solid statement, but it's a starting point.\n### Selecting our beloved author's quotes","7b37f87d":"### Modeling with LSTM\nThe choice of architecture used for the model can vary, but I'll stick with something simple for now.","758f02eb":"### Time to Encode!\nSince our model is an idiot, we need to pass numbers to it instead of characters. How we encode them is a vital question to answer! We should decided whether we want a Word-Based or a Character-Based text generator by specifying the `char_level` parameter of the *Tokenizer* object.\n\nAlso not that we encoded our sentences and the leading token two different ways:\n- Our sentences are encoded as sequences, meaning they are mapped to our vocabulary\n- Our leading token (word or character) is *One-Hot-Encoded*, this is because models last layer will output a set of probablies that them we use to find the most relevant prediction."}}