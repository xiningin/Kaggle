{"cell_type":{"23e97fbf":"code","5d7e42da":"code","412f7c12":"code","4b108cf7":"code","c0b02a14":"code","e496b0e1":"code","a2fc79d6":"code","f6cc40d6":"code","5db253e9":"code","9372f40d":"code","2821bef3":"code","873d7bd5":"code","5bdffe62":"code","2e81b024":"code","1a9d9d21":"code","197dcf14":"code","6d9dc3a7":"code","12118830":"code","6809e950":"code","120bde73":"code","d228df30":"code","97446e5d":"code","bc0cd575":"code","f2e779dd":"code","de44a9ff":"code","0b82c664":"code","a3945195":"code","3634fa2d":"code","4f0c1acf":"code","1fec7dcf":"code","26bfb700":"code","4fa605a6":"code","1cc272ad":"code","a0ab7314":"code","6d44731d":"code","30909a47":"code","af1d249e":"code","8409e43a":"code","229fd354":"code","3ce2525b":"code","830f48de":"code","7802cee5":"code","f4be9c2f":"code","14bdb44e":"code","d2cd6a72":"markdown","c761917f":"markdown","8188c01e":"markdown","bba6877f":"markdown","3bedfd39":"markdown","a8ad3e15":"markdown","0a4d8d1e":"markdown","2bbc4dc0":"markdown","7e72eabc":"markdown","d106274e":"markdown","4ad48d37":"markdown","5a2595dd":"markdown"},"source":{"23e97fbf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport warnings\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker\nimport seaborn as sns\n\n\n# for feature importance study\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom pdpbox import pdp\nimport shap\n\n# ML\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn import metrics\nimport os\nimport gc\n\n# Reproducability\ndef set_seed(seed = 0):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    print('*** --- Set seed \"%i\" --- ***' %seed)\n    \n# setting up options\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('float_format', '{:f}'.format)\nwarnings.filterwarnings('ignore')\n\n\n# matplotlib setting\nmpl.rcParams['figure.dpi'] = 200\nmpl.rcParams['axes.spines.top'] = False\nmpl.rcParams['axes.spines.right'] = False\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5d7e42da":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv')","412f7c12":"print(\"The shape of the train dataset is : \", train.shape)\nprint()\nprint(\"The shape of the test dataset is : \", test.shape)\nprint()","4b108cf7":"print(\"The first 2 rows of the train dataset are \")\ntrain.head(2)","c0b02a14":"print(\"The first 2 rows of the test dataset are \")\ntest.head(2)","e496b0e1":"train.columns","a2fc79d6":"print('Total missing values in the training data' , sum(train.isna().sum()))","f6cc40d6":"train_df = train\ntest_df = test\n\nmissing_train_df = pd.DataFrame(train_df.isna().sum())\nmissing_train_df = missing_train_df.drop(['id', 'claim']).reset_index()\nmissing_train_df.columns = ['feature', 'missing value count']\n\nmissing_train_percent_df = missing_train_df.copy()\nmissing_train_percent_df['missing value percentage'] = (missing_train_df['missing value count']\/train_df.shape[0])*100\n\nmissing_test_df = pd.DataFrame(test_df.isna().sum())\nmissing_test_df = missing_test_df.drop(['id']).reset_index()\nmissing_test_df.columns = ['feature', 'missing value count']\n\nmissing_test_percent_df = missing_test_df.copy()\nmissing_test_percent_df['missing value percentage'] = (missing_test_df['missing value count']\/test_df.shape[0])*100","5db253e9":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.rcParams[\"axes.labelsize\"] = 10\nsns.set_theme(style=\"whitegrid\")\nf, ax = plt.subplots(2,2,figsize=(15,35))\nax0_sns = sns.barplot(y=\"feature\", x=\"missing value count\", data=missing_train_df, color=\"#00CDFF\",orient='h',ax=ax[0][0]).set_title('Missing values in TRAIN data')\n# ax0_sns.set_xlabel(\"Missing Values\",fontsize=3, weight='bold')\nax1_sns = sns.barplot(y=\"feature\", x=\"missing value count\", data=missing_test_df, color=\"#00CDFF\",orient='h',ax=ax[0][1]).set_title('Missing values in TEST data')\n# ax1_sns.set_xlabel(\"Missing Values\",fontsize=3, weight='bold')\nax2_sns = sns.barplot(y=\"feature\", x=\"missing value percentage\", data=missing_train_percent_df, color=\"#00CDFF\",orient='h',ax=ax[1][0]).set_title('Missing values % in TRAIN data')\n# ax2_sns.set_xlabel(\"Missing Values Percentage\",fontsize=3, weight='bold')\nax3_sns = sns.barplot(y=\"feature\", x=\"missing value percentage\", data=missing_test_percent_df, color=\"#00CDFF\",orient='h',ax=ax[1][1]).set_title('Missing values % in TEST data')\n# ax3_sns.set_xlabel(\"Missing Values Percentage\",fontsize=3, weight='bold')","9372f40d":"# train_data missing values\nnull_values_train = []\nfor col in train.columns:\n    c = train[col].isna().sum()\n    pc = np.round((100 * (c)\/len(train)), 2)            \n    dict1 ={\n        'Features' : col,\n        'null_train (count)': c,\n        'null_trian (%)': '{}%'.format(pc)\n    }\n    null_values_train.append(dict1)\nDF1 = pd.DataFrame(null_values_train, index=None).sort_values(by='null_train (count)',ascending=False)\n\n\n# test_data missing values\nnull_values_test = []\nfor col in test.columns:\n    c = test[col].isna().sum()\n    pc = np.round((100 * (c)\/len(test)), 2)            \n    dict2 ={\n        'Features' : col,\n        'null_test (count)': c,\n        'null_test (%)': '{}%'.format(pc)\n    }\n    null_values_test.append(dict2)\nDF2 = pd.DataFrame(null_values_test, index=None).sort_values(by='null_test (count)',ascending=False)\n\n\ndf = pd.concat([DF1, DF2], axis=1)\ndf#.head()\n\nimport plotly.figure_factory as ff\nimport plotly.graph_objects as go\nfig = go.Figure(data=[go.Scatter(x=DF1['Features'],\n                             y=DF1[\"null_trian (%)\"], mode= 'markers',                             \n                             name='Train', marker_color='#0004FF'),        \n\n                go.Scatter(x=DF2['Features'],\n                             y=DF2[\"null_test (%)\"], mode= 'markers',\n                             name='Test', marker_color='#00CDFF')])\nfig.update_traces(marker_line_color='black', marker_line_width=1.5, opacity=1)\nfig.update_layout(title_text='Null Values In Each Feature (%)', \n                  #template='plotly_dark',\n                  paper_bgcolor='#FFFFFF',\n                  plot_bgcolor='#FFFFFF',\n                  width=750, height=500,\n                  xaxis_title='Features', yaxis_title='Count',\n                  titlefont={'color':'black', 'size': 24, 'family': 'San-Serif'})\nfig.show()","2821bef3":"claim_df = pd.DataFrame(train_df['claim'].value_counts()).reset_index()\nclaim_df.columns = ['claim', 'count']\n\nclaim_percent_df = pd.DataFrame((train_df['claim'].value_counts()\/train_df.shape[0])*100).reset_index()\nclaim_percent_df.columns = ['claim', 'count']","873d7bd5":"plt.rcParams[\"axes.labelsize\"] = 3\nsns.set_theme(style=\"whitegrid\")\nf, ax = plt.subplots(1,2,figsize=(15,5))\nsns.barplot(x=\"claim\", y=\"count\", data=claim_df, color=\"#00CDFF\",orient='v',ax=ax[0]).set_title('Target Distribution TRAIN data')\nsns.barplot(x=\"claim\", y=\"count\", data=claim_percent_df, color=\"#00CDFF\",orient='v',ax=ax[1]).set_title('Target Distribution % TRAIN data')","5bdffe62":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(10, 10), facecolor='#FFFFFF')\ngs = fig.add_gridspec(5, 5)\ngs.update(wspace=0.3, hspace=0.3)\nbackground_color = \"#FFFFFF\"\nrun_no = 0\nfor row in range(0, 5):\n    for col in range(0, 5):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1  \n\nfeatures = list(train_df.columns[1:26])\n\n\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=train_df[col], zorder=2, alpha=1, linewidth=1, color='#1700FF')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=test_df[col], zorder=2, alpha=1, linewidth=1, color='#00CDFF')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nplt.show()","2e81b024":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(10, 10), facecolor='#FFFFFF')\ngs = fig.add_gridspec(5, 5)\ngs.update(wspace=0.3, hspace=0.3)\nbackground_color = \"#FFFFFF\"\nrun_no = 0\nfor row in range(0, 5):\n    for col in range(0, 5):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1  \n\nfeatures = list(train_df.columns[26:51])\n\n\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=train_df[col], zorder=2, alpha=1, linewidth=1, color='#1700FF')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=test_df[col], zorder=2, alpha=1, linewidth=1, color='#00CDFF')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nplt.show()","1a9d9d21":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(10, 10), facecolor='#FFFFFF')\ngs = fig.add_gridspec(5, 5)\ngs.update(wspace=0.3, hspace=0.3)\nbackground_color = \"#FFFFFF\"\nrun_no = 0\nfor row in range(0, 5):\n    for col in range(0, 5):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1  \n\nfeatures = list(train_df.columns[51:76])\n\n\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=train_df[col], zorder=2, alpha=1, linewidth=1, color='#1700FF')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=test_df[col], zorder=2, alpha=1, linewidth=1, color='#00CDFF')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nplt.show()","197dcf14":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(10, 10), facecolor='#FFFFFF')\ngs = fig.add_gridspec(5, 5)\ngs.update(wspace=0.3, hspace=0.3)\nbackground_color = \"#FFFFFF\"\nrun_no = 0\nfor row in range(0, 5):\n    for col in range(0, 5):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1  \n\nfeatures = list(train_df.columns[76:101])\n\n\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=train_df[col], zorder=2, alpha=1, linewidth=1, color='#1700FF')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=test_df[col], zorder=2, alpha=1, linewidth=1, color='#00CDFF')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nplt.show()","6d9dc3a7":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(10, 10), facecolor='#FFFFFF')\ngs = fig.add_gridspec(5, 5)\ngs.update(wspace=0.3, hspace=0.3)\nbackground_color = \"#FFFFFF\"\nrun_no = 0\nfor row in range(0, 4):\n    for col in range(0, 5):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1  \n\nfeatures = list(train_df.columns[101:119])\n\n\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=train_df[col], zorder=2, alpha=1, linewidth=1, color='#1700FF')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=test_df[col], zorder=2, alpha=1, linewidth=1, color='#00CDFF')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\nax18.remove()\nax19.remove()\n\nplt.show()","12118830":"train = pd.read_csv(r'\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv', index_col='id')\ncorr = train.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nf, ax = plt.subplots(figsize=(16, 16), facecolor='#EAECEE')\ncmap = sns.color_palette(\"Blues\", as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=0.05, vmin=-0.05, center=0, annot=False,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": 0.75})\n\nax.set_title('Correlation heatmap', fontsize=24, y= 1.05)\ncolorbar = ax.collections[0].colorbar\ncolorbar.set_ticks([-0.75, 0, 0.75])\n\ntrain = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv', \n                    index_col = 0)\ntest = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv', \n                   index_col = 0)\nss = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')","6809e950":"train = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv', \n                    index_col = 0)\ntest = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv', \n                   index_col = 0)\nss = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\nclaim = train.claim\n\ntrain.drop(['claim'], axis = 1, inplace = True)\ncolumn_name = train.columns\n\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\ntrain = imputer.fit_transform(train)\ntest = imputer.transform(test)","120bde73":"def model_imp_viz(model, columns, bias = 0.01):\n    imp = pd.DataFrame({'importance': model.feature_importances_,\n                        'features': columns}).sort_values('importance', \n                                                          ascending = False)\n    fig, ax = plt.subplots(figsize = (10, 40))\n    plt.title('Feature importances', size = 15, fontweight = 'bold', fontfamily = 'serif')\n\n    sns.barplot(x = imp.importance, y = imp.features, edgecolor = 'black',\n                palette = reversed(sns.color_palette(\"Blues\", len(imp.features))))\n\n    for i in ['top', 'right']:\n            ax.spines[i].set_visible(None)\n\n    rects = ax.patches\n    labels = imp.importance\n    for rect, label in zip(rects, labels):\n        x_value = rect.get_width() + bias\n        y_value = rect.get_y() + rect.get_height() \/ 2\n\n        ax.text(x_value, y_value, round(label, 4), fontsize = 9, color = 'black',\n                 ha = 'center', va = 'center')\n    ax.set_xlabel('Importance', fontweight = 'bold', fontfamily = 'serif')\n    ax.set_ylabel('Features', fontweight = 'bold', fontfamily = 'serif')\n    plt.show()","d228df30":"# Create data sets for training (80%) and validation (20%)\nX_train, X_valid, y_train, y_valid = train_test_split(train, claim, \n                                                      test_size = 0.2,\n                                                      random_state = 42)","97446e5d":"from sklearn.model_selection import train_test_split, KFold, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn import metrics\nimport os\nimport gc\n# The basic model\nparams = {'random_state': 0,\n          'predictor': 'gpu_predictor',\n          'tree_method': 'gpu_hist',\n          'eval_metric': 'auc'}\n\nmodel = XGBClassifier(**params)\n\nmodel.fit(X_train, y_train, verbose = False)\n\npreds = model.predict_proba(X_valid)[:, 1]\nfpr, tpr, thresholds = metrics.roc_curve(y_valid, preds)\nprint('Valid AUC: ', metrics.auc(fpr, tpr))","bc0cd575":"metrics.plot_confusion_matrix(model, X_valid, y_valid,\n                              cmap = 'Blues')\nplt.title('Confusion matrix')\nplt.grid(False)\nplt.show()","f2e779dd":"model_imp_viz(model, column_name.values, bias = 0.0005)","de44a9ff":"explainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_valid)\n\nshap.summary_plot(shap_values, X_valid)","0b82c664":"# The basic model\nparams = {'n_estimators': 5000,\n          'max_depth': 2,\n          'colsample_bytree': 0.30,\n          'learning_rate': 0.09,\n          'reg_lambda': 18,\n          'reg_alpha': 18,\n          'random_state': 2021,\n          'predictor': 'gpu_predictor',\n          'tree_method': 'gpu_hist',\n          'eval_metric': 'auc'}\n\nmodel = XGBClassifier(**params)\n\nmodel.fit(X_train, y_train, verbose = False)\n\npreds = model.predict_proba(X_valid)[:, 1]\nfpr, tpr, thresholds = metrics.roc_curve(y_valid, preds)\nprint('Valid AUC: ', metrics.auc(fpr, tpr))","a3945195":"metrics.plot_confusion_matrix(model, X_valid, y_valid,\n                              cmap = 'Blues')\nplt.title('Confusion matrix')\nplt.grid(False)\nplt.show()","3634fa2d":"model_imp_viz(model, column_name.values, bias = 0.0005)","4f0c1acf":"explainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_valid)\n\nshap.summary_plot(shap_values, X_valid)","1fec7dcf":"FOLDS = 10\nss.claim = np.zeros(len(ss.claim))\nmetric = []\nkfold = KFold(n_splits = FOLDS, random_state = 2021, shuffle = True)\ni = 1\nfor train_idx, test_idx in kfold.split(train):\n    X_train, y_train = train[train_idx, :], claim[train_idx]\n    X_test, y_test = train[test_idx, :], claim[test_idx]\n\n    model = XGBClassifier(**params)\n    model.fit(X_train, y_train)\n    \n    preds = model.predict_proba(X_test)[:, 1]\n    fpr, tpr, thresholds = metrics.roc_curve(y_test, preds)\n    AUC = metrics.auc(fpr, tpr)    \n    print('[FOLD #{}] Validation AUC: {:.5f}'.format(i, AUC))\n\n    ss.claim += model.predict_proba(test)[:, 1] \/ FOLDS\n    metric.append(AUC)\n    i += 1\nprint('*'*50)\nprint('[ALL FOLDS] Mean Validation AUC: {:.5f}'.format(np.mean(metric)))","26bfb700":"ss.to_csv('submission.csv', index = False)\nss.head()","4fa605a6":"import pandas as pd\nimport numpy as np\nimport random\nimport time\nimport os\nimport gc\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\n\nimport lightgbm as lgb\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.simplefilter('ignore')","1cc272ad":"N_SPLITS = 5\nN_ESTIMATORS = 20000\nEARLY_STOPPING_ROUNDS = 200\nVERBOSE = 1000\nSEED = 42","a0ab7314":"import random\nimport os\nimport numpy  as np\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\nseed_everything(SEED)","6d44731d":"INPUT = \"..\/input\/tabular-playground-series-sep-2021\/\"\n\ntrain = pd.read_csv(INPUT + \"train.csv\")\ntest = pd.read_csv(INPUT + \"test.csv\")\nsubmission = pd.read_csv(INPUT + \"sample_solution.csv\")","30909a47":"features = [col for col in test.columns if 'f' in col]\nTARGET = 'claim'\n\ntarget = train[TARGET].copy()","af1d249e":"train['n_missing'] = train[features].isna().sum(axis=1)\ntest['n_missing'] = test[features].isna().sum(axis=1)\n\ntrain['std'] = train[features].std(axis=1)\ntest['std'] = test[features].std(axis=1)\n\nfeatures += ['n_missing', 'std']\nn_missing = train['n_missing'].copy()","8409e43a":"train[features] = train[features].fillna(train[features].mean())\ntest[features] = test[features].fillna(test[features].mean())","229fd354":"train[features] = train[features].fillna(train[features].mean())\ntest[features] = test[features].fillna(test[features].mean())","3ce2525b":"lgb_params = {\n    'objective': 'binary',\n    'n_estimators': N_ESTIMATORS,\n    'random_state': SEED,\n    'learning_rate': 5e-3,\n    'subsample': 0.6,\n    'subsample_freq': 1,\n    'colsample_bytree': 0.4,\n    'reg_alpha': 10.0,\n    'reg_lambda': 1e-1,\n    'min_child_weight': 256,\n    'min_child_samples': 20,\n    'importance_type': 'gain',\n}","830f48de":"lgb_oof = np.zeros(train.shape[0])\nlgb_pred = np.zeros(test.shape[0])\nlgb_importances = pd.DataFrame()\n\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X=train, y=n_missing)):\n    print(f\"===== fold {fold} =====\")\n    X_train = train[features].iloc[trn_idx]\n    y_train = target.iloc[trn_idx]\n    X_valid = train[features].iloc[val_idx]\n    y_valid = target.iloc[val_idx]\n    X_test = test[features]\n    \n    start = time.time()\n    model = lgb.LGBMClassifier(**lgb_params)\n    model.fit(\n        X_train, \n        y_train,\n        eval_set=[(X_valid, y_valid)],\n        eval_metric='auc',\n        early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n        verbose=VERBOSE,\n    )\n    \n    fi_tmp = pd.DataFrame()\n    fi_tmp['feature'] = model.feature_name_\n    fi_tmp['importance'] = model.feature_importances_\n    fi_tmp['fold'] = fold\n    fi_tmp['seed'] = SEED\n    lgb_importances = lgb_importances.append(fi_tmp)\n\n    lgb_oof[val_idx] = model.predict_proba(X_valid)[:, -1]\n    lgb_pred += model.predict_proba(X_test)[:, -1] \/ N_SPLITS\n\n    elapsed = time.time() - start\n    auc = roc_auc_score(y_valid, lgb_oof[val_idx])\n    print(f\"fold {fold} - lgb auc: {auc:.6f}, elapsed time: {elapsed:.2f}sec\\n\")\n\nprint(f\"oof lgb roc = {roc_auc_score(target, lgb_oof)}\")\n\nnp.save(\"lgb_oof.npy\", lgb_oof)\nnp.save(\"lgb_pred.npy\", lgb_pred)","7802cee5":"ss['lgb_TARGET'] = lgb_pred\n\n\nss.to_csv('submission_1.csv', index = False)\nss.head()\n\nss['final'] = (ss['lgb_TARGET']+ss['claim'])\/2\nss['claim1'] = ss['claim']\nss['claim'] = ss['final']","f4be9c2f":"ss1 = ss[['id','claim']]\nss1.to_csv('sub_2.csv',index=False)","14bdb44e":"%%html\n<marquee style='width: 90% ;height:70%; color: #799EFF ;'>\n    <b> Do UPVOTE if you like my work, I will be adding some more content to this kernel (Baseline and SHAP) :) <\/b><\/marquee>","d2cd6a72":"# LightGBM","c761917f":"### Observations:\n- *claim* column is the target variable\n- We have 118 columns + 1 target column + 1 id column for train \n- We have 118 columns + 1 id column for test\n- We have a total data of around 1M for the train data and around 500k for the test data\n- Total missing values in the training data 1820782","8188c01e":"# Target Distribution ","bba6877f":"# Highlevel Data Overview ","3bedfd39":"## Competition Overview \nThe goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset.\n\nThese competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.\n\n## Competition Objective\nThe original dataset deals with predicting whether a claim will be made on an insurance policy.\n\n## Evalualtion Metric\nSubmissions are evaluated on area under the **ROC curve** between the predicted probability and the observed target.\n\n## Competition Timeline\n- Start Date - September 1, 2021\n- Entry deadline - Same as the Final Submission Deadline\n- Team Merger deadline - Same as the Final Submission Deadline\n- Final submission deadline - September 30, 2021\n\n\n## Points to note:\n\n- This competition does not award ranking points\n- This competition does not count towards tiers","a8ad3e15":"# Improved Model - XGB","0a4d8d1e":"Credits to the codes that have helped me make this notebook:\n\n- [Notebook by Jaupula](https:\/\/www.kaggle.com\/jarupula\/tps-sep-getting-started)\n- [Notebook by Sharlto Cope](https:\/\/www.kaggle.com\/dwin183287\/tps-september-2021-eda)\n- [Notebook by des](https:\/\/www.kaggle.com\/desalegngeb\/sept-2021-tps-eda-model)\n\n\nSome of my other works\n- [How did Covid-19 impact Digital Learning - EDA](https:\/\/www.kaggle.com\/udbhavpangotra\/how-did-covid-19-impact-digital-learning-eda)\n- [EDA + Optuna: An attempt at a clean notebook](https:\/\/www.kaggle.com\/udbhavpangotra\/eda-optuna-an-attempt-at-a-clean-notebook)\n- [Heart Attacks! Extensive EDA and visualizations :)](https:\/\/www.kaggle.com\/udbhavpangotra\/heart-attacks-extensive-eda-and-visualizations)\n- [CommonLit Readibility Prize Extensive EDA + Model](https:\/\/www.kaggle.com\/udbhavpangotra\/commonlit-readibility-prize-extensive-eda-model)","2bbc4dc0":"# Missing Values","7e72eabc":"# Correlation Heatmap","d106274e":"Handling the missing values can be tricky, some places to check out for learning it :\n- [A Guide to Handling Missing values in Python](https:\/\/www.kaggle.com\/parulpandey\/a-guide-to-handling-missing-values-in-python)\n- [How to Handle Missing Data with Python](https:\/\/machinelearningmastery.com\/handle-missing-data-python\/)\n- [Handling Missing Data](https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/03.04-missing-values.html)","4ad48d37":"# k-folds","5a2595dd":"# Model Baseline - XGBoost"}}