{"cell_type":{"6c1bc2fd":"code","36d44f38":"code","fa4dacca":"code","69ddde9f":"code","91c0f20c":"code","bdcb5f06":"code","e67e27d0":"code","cfb90380":"code","eb467458":"code","1133f6c4":"code","60f7bab4":"code","07251999":"code","b9f61a9a":"code","248658ef":"code","04ff2cbd":"code","e3c8fbfb":"code","6db57c11":"code","e3ed9e83":"code","46b416a0":"code","8e7cbe63":"code","5b34a5bf":"code","383b700e":"code","40950999":"code","ba44691b":"code","94e0c5f5":"code","9589d216":"code","33e999fb":"code","29611e7f":"code","e84c1cbd":"code","71332319":"code","f22eb775":"code","e4fbf5ae":"code","f13cde53":"code","92673987":"code","cc68de06":"code","f9f0ca44":"code","f7042ec3":"code","b2aa3723":"code","ba3519a3":"code","c99f767c":"code","fbf89828":"code","f6b76be7":"code","3293d1b7":"code","06022a1e":"code","efe39f29":"code","a924dc30":"code","6421ecca":"markdown","eda6d8d8":"markdown","06a0c031":"markdown","975ea815":"markdown","bdbcce51":"markdown","3270d1b4":"markdown","f238c1be":"markdown","5969b942":"markdown","a56738f4":"markdown","89e47a56":"markdown","c69261cc":"markdown","ec7961e0":"markdown","462c686c":"markdown","dd334a95":"markdown","dca24b70":"markdown","6cd5428d":"markdown","533c7f9e":"markdown","e0a945d6":"markdown","523c5d1d":"markdown","6d3916ff":"markdown","187a7fa3":"markdown","7355c97e":"markdown"},"source":{"6c1bc2fd":"import pandas as pd\ntrain_transact = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv', index_col='TransactionID')\ntrain_id = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv', index_col='TransactionID')\ntest_transact = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv', index_col='TransactionID')\ntest_id = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv', index_col='TransactionID')","36d44f38":"train = train_transact.merge(train_id,how='left',left_index=True,right_index=True)\ntest = test_transact.merge(test_id,how='left',left_index=True,right_index=True)","fa4dacca":"train.info()\ntest.info()","69ddde9f":"# Freeing up some space\ndel train_transact, train_id, test_transact, test_id","91c0f20c":"# Taken from https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\nimport numpy as np\ndef reduce_mem_usage(df):\n   \n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","bdcb5f06":"reduce_mem_usage(train)","e67e27d0":"reduce_mem_usage(test)","cfb90380":"import matplotlib.pyplot as plt\nfig = plt.figure(figsize = (8,5))\ntrain.isFraud.value_counts(normalize = True).plot(kind='bar', color= ['skyblue','red'], alpha = 0.8)\nplt.title('Fraud and Non-Fraud (Imbalanced Dataset)')\nplt.show()","eb467458":"# Undersampling majority to remove Class Imbalance\nimport pandas as pds\nfrom sklearn.utils import resample\n\nfraud = train[train.isFraud==1]\nnot_fraud = train[train.isFraud==0]\n\nnot_fraud_undsamp = resample(not_fraud,replace = False,n_samples = len(fraud),random_state = 42)\nundsamp_train = pds.concat([not_fraud_undsamp, fraud])\n\nfig = plt.figure(figsize = (8,5))\nundsamp_train.isFraud.value_counts(normalize = True).plot(kind='bar', color= ['skyblue','red'], alpha = 0.8)\nplt.title('Fraud and Non-Fraud after Undersampling (Balanced Dataset)')\nplt.show()","1133f6c4":"# Drop features having >40% missing values in Train Dataset\n\npct_null = undsamp_train.isnull().sum() \/ len(undsamp_train)\nmissing_features = pct_null[pct_null > 0.40].index\nundsamp_train.drop(missing_features, axis=1, inplace=True)\nundsamp_train.info()","60f7bab4":"del train, fraud, not_fraud #free space","07251999":"undsamp_train.select_dtypes(include=['category']).columns","b9f61a9a":"# Impute categorical var with Mode\nundsamp_train['ProductCD'] = undsamp_train['ProductCD'].fillna(undsamp_train['ProductCD'].mode()[0])\nundsamp_train['card4'] = undsamp_train['card4'].fillna(undsamp_train['card4'].mode()[0])\nundsamp_train['card6'] = undsamp_train['card6'].fillna(undsamp_train['card6'].mode()[0])\nundsamp_train['P_emaildomain'] = undsamp_train['P_emaildomain'].fillna(undsamp_train['P_emaildomain'].mode()[0])\nundsamp_train['M4'] = undsamp_train['M4'].fillna(undsamp_train['M4'].mode()[0])","248658ef":"# Convert categorical features to continuous features with Label Encoding\nfrom sklearn.preprocessing import LabelEncoder\nlencoders = {}\nfor col in undsamp_train.select_dtypes(include=['category']).columns:\n    lencoders[col] = LabelEncoder()\n    undsamp_train[col] = lencoders[col].fit_transform(undsamp_train[col])","04ff2cbd":"# Multiple Imputation by Chained Equations\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\ntrain_MiceImputed = undsamp_train.copy(deep=True) \nmice_imputer = IterativeImputer()\ntrain_MiceImputed.iloc[:, :] = mice_imputer.fit_transform(undsamp_train)","e3c8fbfb":"train_MiceImputed.info()","6db57c11":"# Feature Selection using Extra Trees Classifier (max_features='log2')\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\nx_t = train_MiceImputed.drop('isFraud', axis=1)\ny_t = train_MiceImputed['isFraud']\nclf_1 = SelectFromModel(ExtraTreesClassifier(max_features='log2'))\nclf_1.fit(x_t, y_t)\nselect_feats_1 = x_t.columns[(clf_1.get_support())]\nprint(select_feats_1)\n\n# Feature Selection using Extra Trees Classifier (max_features = 'auto')\n\nclf_2 = SelectFromModel(ExtraTreesClassifier(max_features='auto'))\nclf_2.fit(x_t, y_t)\nselect_feats_2 = x_t.columns[(clf_2.get_support())]\nprint(select_feats_2)","e3ed9e83":"# Feature Selection using Random Forest (max_features='log2')\n\nfrom sklearn.ensemble import RandomForestClassifier\nclf_3 = SelectFromModel(RandomForestClassifier(max_features='log2'))\nclf_3.fit(x_t, y_t)\nselect_feats_3 = x_t.columns[(clf_3.get_support())]\nprint(select_feats_3)\n\n# Feature Selection using Random Forest (max_features 'auto') \n\nclf_4 = SelectFromModel(RandomForestClassifier(max_features='auto'))\nclf_4.fit(x_t, y_t)\nselect_feats_4 = x_t.columns[(clf_4.get_support())]\nprint(select_feats_4)","46b416a0":"# Combine all selected features\nx_train = train_MiceImputed[['TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2','card3', 'card4', 'card5', \n                             'card6','addr1', 'addr2', 'P_emaildomain', 'C1', 'C2', 'C6', 'C7','C8', 'C9','C10','C11',\n                             'C12', 'C13', 'C14','D1', 'D4', 'D10', 'D15','M4','V12', 'V13', 'V15', 'V22','V29', 'V30',\n                             'V34', 'V35','V36', 'V37','V38','V42', 'V43', 'V44','V45', 'V47', 'V48','V49','V50', 'V51', \n                             'V52', 'V53', 'V54','V57','V69', 'V70','V71', 'V74','V75', 'V76', 'V78', 'V81','V82','V83', \n                             'V84', 'V85', 'V86','V87', 'V90', 'V91', 'V92','V93', 'V94', 'V102', 'V127', 'V128', 'V133', \n                             'V280','V282','V283', 'V285', 'V294','V295', 'V306', 'V307', 'V308', 'V310', 'V312', 'V313',\n                             'V314','V315', 'V317', 'V318']]","8e7cbe63":"y_train = train_MiceImputed['isFraud']","5b34a5bf":"skewness_of_features=[]\nfor col in x_train:\n        skewness_of_features.append(x_train[col].skew())\nprint(skewness_of_features)","383b700e":"skewed_vars = x_train[['TransactionAmt','C2','C6','C7','C8','C9','C10','C11','C12','C13','C14','V29','V37','V38','V44',\n                       'V45','V47','V52','V53','V69','V75','V78','V81','V82','V86','V87','V91','V102','V127','V128','V133',\n                       'V280','V283','V285','V294','V295','V306','V307','V308','V310','V312','V313','V314','V315','V317',\n                       'V318']]","40950999":"import numpy\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom pyearth import Earth\n\n# Fit an Earth model (Multi Variate Adaptive Regression Splines)\n# Residual Sum of Squares (RSS), Generalized Cross Validation (GCV), No. of Subsets of MARS Model Terms \ncriteria = ('rss', 'gcv', 'nb_subsets')  \nmodel = Earth(max_degree=3,\n              max_terms=10,\n              minspan_alpha=.5,\n              feature_importance_type=criteria,\n              verbose=True)\nmodel.fit(skewed_vars, y_train)\nrf = RandomForestRegressor()\nrf.fit(skewed_vars, y_train)\nprint(model.trace())\nprint(model.summary())\nprint(model.summary_feature_importances(sort_by='gcv'))\n\n# Plot the feature importances\nimportances = model.feature_importances_\nimportances['random_forest'] = rf.feature_importances_\ncriteria = criteria + ('random_forest',)\nidx = 1\n\nfig = plt.figure(figsize=(30, 12))\nlabels = [i for i in range(len(skewed_vars.columns))]\nfor crit in criteria:\n    plt.subplot(2, 2, idx)\n    plt.bar(numpy.arange(len(labels)),\n            importances[crit],\n            align='center',\n            color='blue')\n    plt.xticks(numpy.arange(len(labels)), labels)\n    plt.title(crit)\n    plt.ylabel('importances')\n    idx += 1\ntitle = 'Feature Importance Plots using Earth Package'\nfig.suptitle(title, fontsize=\"18\")\nplt.show()","ba44691b":"x_train['TransactionAmt']=np.log(x_train['TransactionAmt'])","94e0c5f5":"# Check skewness again after log-transormation\nprint(x_train['TransactionAmt'].skew())","9589d216":"# Keeping only the important features\nx_train_work = x_train[['TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2','card3', 'card4', 'card5', \n                        'card6','addr1', 'addr2', 'P_emaildomain', 'D1','D4', 'D10', 'D15','M4','C2','C6','C7','C8',\n                        'C9','C10','C11','C12','C13','C14','V12', 'V13', 'V15', 'V22','V29','V30','V34', 'V35','V36',\n                        'V37','V38','V42', 'V43','V44','V45','V47','V48','V49', 'V50', 'V51', 'V52','V53','V54','V57',\n                        'V69','V70','V71', 'V74','V75','V76','V78','V83', 'V84', 'V85', 'V87','V90','V91','V92','V93',\n                        'V94', 'V282','V294','V308','V315','V318']]","33e999fb":"# Check correlation matrix for the selected important features\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nmask = np.zeros_like(x_train_work.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nplt.figure(figsize=(20,20))\nsns.heatmap(x_train_work.corr(), mask = mask, vmin = -1, annot = False, cmap = 'RdYlGn')","29611e7f":"import lightgbm as lgb\nfrom hyperopt import hp, tpe, fmin\nfrom sklearn.model_selection import cross_val_score\n\nvalgrid = {'n_estimators':hp.quniform('n_estimators', 1000, 5000, 50),\n         'subsample_for_bin':hp.uniform('subsample_for_bin',10,300000),\n         'learning_rate':hp.uniform('learning_rate', 0.00001, 0.03),\n         'max_depth':hp.quniform('max_depth', 3,8,1),\n         'num_leaves':hp.quniform('num_leaves', 7,256,1),\n         'subsample':hp.uniform('subsample', 0.60, 0.95),\n         'colsample_bytree':hp.uniform('colsample_bytree', 0.60, 0.95),\n         'min_child_samples':hp.quniform('min_child_samples', 1, 500,1),\n         'min_child_weight':hp.uniform('min_child_weight', 0.60, 0.95),\n         'min_split_gain':hp.uniform('min_split_gain', 0.60, 0.95),  \n         'reg_lambda':hp.uniform('reg_lambda', 1, 25)\n         #'reg_alpha':hp.uniform('reg_alpha', 1, 25)  \n        }\n\ndef objective(params):\n    params = {'n_estimators': int(params['n_estimators']),\n              'subsample_for_bin': int(params['subsample_for_bin']),\n              'learning_rate': params['learning_rate'],\n              'max_depth': int(params['max_depth']),\n              'num_leaves': int(params['num_leaves']),\n              'subsample': params['subsample'],\n              'colsample_bytree': params['colsample_bytree'],\n              'min_child_samples': int(params['min_child_samples']),\n              'min_child_weight': params['min_child_weight'],\n              'min_split_gain': params['min_split_gain'],\n              'reg_lambda': params['reg_lambda']}\n              #'reg_alpha': params['reg_alpha']}\n    \n    lgb_a = lgb.LGBMClassifier(**params)\n    score = cross_val_score(lgb_a, x_train_work, y_train, cv=5, n_jobs=-1).mean()\n    return score\n\nbestP = fmin(fn= objective, space= valgrid, max_evals=20, rstate=np.random.RandomState(123), algo=tpe.suggest)","e84c1cbd":"print(bestP)","71332319":"# Missing value analysis for the selected feature set in Test Data\ntotal = test[['TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2','card3', 'card4', 'card5', \n                        'card6','addr1', 'addr2', 'P_emaildomain', 'D1','D4', 'D10', 'D15','M4','C2','C6','C7','C8',\n                        'C9','C10','C11','C12','C13','C14','V12', 'V13', 'V15', 'V22','V29','V30','V34', 'V35','V36',\n                        'V37','V38','V42', 'V43','V44','V45','V47','V48','V49', 'V50', 'V51', 'V52','V53','V54','V57',\n                        'V69','V70','V71', 'V74','V75','V76','V78','V83', 'V84', 'V85', 'V87','V90','V91','V92','V93',\n                        'V94', 'V282','V294','V308','V315','V318']].isnull().sum().sort_values(ascending=False)\n\npercent = (test.isnull().sum()\/test.isnull().count()).sort_values(ascending=False)\nmissing_test = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_test.head(25)","f22eb775":"# Impute categorical vars with Mode\ntest['ProductCD']=test['ProductCD'].fillna(test['ProductCD'].mode()[0])\ntest['card4']=test['card4'].fillna(test['card4'].mode()[0])\ntest['card6']=test['card6'].fillna(test['card6'].mode()[0])\ntest['P_emaildomain']=test['P_emaildomain'].fillna(test['P_emaildomain'].mode()[0])\ntest['M4']=test['M4'].fillna(test['M4'].mode()[0])","e4fbf5ae":"# Convert categorical features to continuous features with Label Encoding\nfrom sklearn.preprocessing import LabelEncoder\nlencoders_te = {}\nfor col in test[['ProductCD','card4','card6','P_emaildomain','M4']]:\n    lencoders_te[col] = LabelEncoder()\n    test[col] = lencoders_te[col].fit_transform(test[col])  ","f13cde53":"test_working = test[['TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2','card3', 'card4', 'card5', \n                        'card6','addr1', 'addr2', 'P_emaildomain', 'D1','D4', 'D10', 'D15','M4','C2','C6','C7','C8',\n                        'C9','C10','C11','C12','C13','C14','V12', 'V13', 'V15', 'V22','V29','V30','V34', 'V35','V36',\n                        'V37','V38','V42', 'V43','V44','V45','V47','V48','V49', 'V50', 'V51', 'V52','V53','V54','V57',\n                        'V69','V70','V71', 'V74','V75','V76','V78','V83', 'V84', 'V85', 'V87','V90','V91','V92','V93',\n                        'V94', 'V282','V294','V308','V315','V318']]\ndel test #free space","92673987":"# MICE Imputation for test data set\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\ntest_MiceImputed = test_working.copy(deep=True) \nmice_imputer_te = IterativeImputer()\ntest_MiceImputed.iloc[:, :] = mice_imputer_te.fit_transform(test_working)","cc68de06":"test_MiceImputed.head()","f9f0ca44":"test_MiceImputed['TransactionAmt']=np.log(test_MiceImputed['TransactionAmt'])","f7042ec3":"test_working_final = test_MiceImputed[['TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2','card3', 'card4', 'card5', \n                        'card6','addr1', 'addr2', 'P_emaildomain', 'D1','D4', 'D10', 'D15','M4','C2','C6','C7','C8',\n                        'C9','C10','C11','C12','C13','C14','V12', 'V13', 'V15', 'V22','V29','V30','V34', 'V35','V36',\n                        'V37','V38','V42', 'V43','V44','V45','V47','V48','V49', 'V50', 'V51', 'V52','V53','V54','V57',\n                        'V69','V70','V71', 'V74','V75','V76','V78','V83', 'V84', 'V85', 'V87','V90','V91','V92','V93',\n                        'V94', 'V282','V294','V308','V315','V318']]\ndel test_MiceImputed #free space","b2aa3723":"# Prediction using Light GBM with Best Hyperparameters \nimport pandas as pd_out\nimport lightgbm \nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics \nimport numpy as np\n\nclf_final = lightgbm.LGBMClassifier(n_estimators = int(bestP['n_estimators']),\n                                    subsample_for_bin = int(bestP['subsample_for_bin']),\n                                    learning_rate = bestP['learning_rate'],\n                                    max_depth = int(bestP['max_depth']),\n                                    num_leaves = int(bestP['num_leaves']),\n                                    subsample = bestP['subsample'],\n                                    colsample_bytree = bestP['colsample_bytree'],\n                                    min_child_samples = int(bestP['min_child_samples']),\n                                    min_child_weight = bestP['min_child_weight'],\n                                    min_split_gain = bestP['min_split_gain'],\n                                    reg_lambda = bestP['reg_lambda'],\n                                    #reg_alpha = bestP['reg_alpha'], \n                                    random_state = 123)\n\nX_train, X_test, Y_train, Y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=123, stratify=y_train)\nclf_final.fit(x_train_work,y_train)\ny_pred = clf_final.predict(test_working_final)\n#print(accuracy_score(Y_test,y_pred))","ba3519a3":"y_prob = clf_final.predict_proba(x_train_work)[:,-1] # Positive class prediction probabilities  \ny_thresh = np.where(y_prob > 0.5, 1, 0) # Threshold the probabilities to give class predictions\nclf_final.score(x_train_work, y_thresh)","c99f767c":"predicted_proba = clf_final.predict_proba(x_train_work)","fbf89828":"from sklearn.metrics import roc_curve, auc\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, predicted_proba[:,-1]) \nroc_auc = auc(false_positive_rate, true_positive_rate)\nprint(roc_auc) # Checking area under ROC Curve","f6b76be7":"# Plotting ROC Curve\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,10))\nplt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],linestyle='--')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","3293d1b7":"# Detailed Classification Report\nfrom sklearn.metrics import classification_report\nprint(classification_report(clf_final.predict(x_train_work),y_train,digits=4))","06022a1e":"from sklearn import metrics\nconfusion_matrix=metrics.confusion_matrix(clf_final.predict(x_train_work),y_train)\nprint(confusion_matrix)","efe39f29":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(clf_final, x_train_work, y_train,cmap=plt.cm.Blues, normalize = 'all')","a924dc30":"submission = pd_out.read_csv('\/kaggle\/input\/ieee-fraud-detection\/sample_submission.csv')\nsubmission['isFraud'] = y_pred\nsubmission.head()\nsubmission.to_csv('IEEEfraud_submission.csv', index=False)\nprint(\"Submission successful\")","6421ecca":"There are very high correlation among the following group of features: \n* C6, C7, C8\n* C10, C11, C12, C13, C14\n* V90, V91\n* V92, V93\n* V308, V318\nBut the correlations are not equal to 1 in any of these cases. So we are not discarding any feature on the ground of multicollinearity.","eda6d8d8":"# **Step 4:**\nWe will first discard the features having more than 40% missing values. For remaining features, we will impute categorical features with mode (followed by label encoding to convert them into continuous variables). Afterwards, we will impute all continuous variables using **MICE (Multiple Imputation by Chained Equations)**.","06a0c031":"We observe that the size of both train and test datasets are considerably large (1.9+ GB and 1.7+ GB respectively). So we need to take care of memory allocation and will have to keep on deleting redundant (non-required) datasets time-to-time while running the script.","975ea815":"Visibly, the sizes of datasets have been reduced from 1.9 GB to around 500 MB, and from 1.7 GB to 480 MB respectively. The compression function has worked very fine. ","bdbcce51":"# **Step 3:**\nNow we will proceed to check whether the training data set is imbalanced. If the number of instances having fraud label positive is very low in number (rare event), then we need to balance the data first. This step is highly recommended to judge the sensitivity of a classifier to the training data set. ","3270d1b4":"We will spot the highly skewed variables from selected features now and then we will analyse their feature importance further using **\"Earth Package\"**.","f238c1be":"The training data set is clearly imbalanced having only about 2% of fraudulent transactions and rest 98% legit transactions respectively. We will follow **\"Undersampling Strategy\"** to balance the data set. Since the data set is very large, we avoided using oversampling.","5969b942":"The above one is our final working data set for training.","a56738f4":"# **Step 7:**\nWe will use Light GBM model for our binary classification. **\"Hyperopt\"** library has been used to tune the LGBM hyper-parameters and to select the best set of hyper-parameters into a separate dataframe for future use.","89e47a56":"After undersampling majority, the data set has now become perfectly balanced having 50% fraud and 50% legit transaction instances.","c69261cc":"# **Step 5:**\nWe are now done with data pre-processing, and now, we will move forward carrying on with **Feature Selection** step. Here, we have used Extra Trees Classifier and Random Forest Classifier for selecting the most relevant features having high relative feature importance w.r.t. the target variable 'isFraud'. Finally, we will merge the resultant features and will use that as our workable training data set. ","ec7961e0":"Though we have combined all important features, we will further check if these selected features are very heavily skewed or not. If heavily skewed, they will deviate from normality and might require log-transformation to reduce their skewness.","462c686c":"**Log-Transformation :** It is interesting to note that we can apply log-transformation to 'TransactionAmt' only (among these considerable set of highly skewed variables). We cannot apply log-transformation on 'C14','V52','V87','V91','V308','V315' since they have many zero values. Converting them into log will give raise to \"inf\" values. 'TransactionAmt' does not have any zero value.","dd334a95":"Before submitting our final result, we will do some analysis of key metrics generated for the final LGBM classifier model we applied on the test dataset. These key metrics include:\n* Calculation of Area under ROC Curve\n* Plotting ROC Curve (False Positive Rate vs True Positive Rate)\n* A detailed classification report with Precision, Recall, F1-Score information\n* Plotting Confusion Matrix indicating TPs, TNs, FPs, FNs\n\n**Note:-** **\"F1-Score\"** is a better metric compared to **\"Accuracy\"** for this kind of problems where the original data set is imbalanced (in case of rare events).","dca24b70":"Please observe that the skewness has now got reduced.","6cd5428d":"# **Step 9:**\nFinally, we will apply Light GBM classifier model having the best set of hyper-parameters on test data to get final prediction whether a transaction in test data set is fraudulent or legit.","533c7f9e":"# **Step 10:**\nExport binary classification result to an excel file and complete submission. ","e0a945d6":"# **Step 8:**\nNow, we have to repeat the similar steps for test data set as well to form the working data set for testing.","523c5d1d":"# **Step 2: ** \nWe need to compress the working data sets (train and test) so that we do not encounter any interruption due to shortage of memory while working with the large data sets.","6d3916ff":"* Visibly, from first three plots, it is evident that 0th (TransactionAmt), 17th (V52), 25th (V87), 26th (V91), 38th (V308) indexed variables are having high feature importance. Whereas, last plot shows that 0th, 11th (C14), 17th, 25th, 44th (V315) indexed variables are having high feature importance. So we need to consider them. \n\n* Also, features V29, V53, V47, V45, V44, V38, V37 are ranked higher than C14 in the exhaustive list. They can be considered. Features V75, C13, C12, C11, C10, C9, C8, C7, C6, C2, V69, V318, V78, V294 are ranked higher than V315 in the exhaustive list. They can be considered as well.","187a7fa3":"# **Step 6:**\nNow we will check feature importance of these highly skewed selected variables using **Earth Package**. The earth package implements variable importance based on 3 criteria: **(1)** **RSS (Residual Sum of Squares)**; **(2)** **GCV (Generalized Cross Validation)** and **(3)** **Number of subset models where a variable occurs (nb_subsets)**. If considering all criteria, any highly skewed variable shows high feature importance, we will consider that variable into our working model.   ","7355c97e":"# **Step 1:** \nMerging transaction data set with identification data set to form complete (workable) data sets"}}