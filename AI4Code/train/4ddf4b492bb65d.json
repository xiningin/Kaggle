{"cell_type":{"aa40547c":"code","7c559c5e":"code","f252cc6b":"markdown"},"source":{"aa40547c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","7c559c5e":"import requests\nfrom bs4 import BeautifulSoup\nfrom nltk.tokenize import sent_tokenize,word_tokenize\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nfrom nltk.probability import FreqDist\nfrom heapq import nlargest\nfrom collections import defaultdict\narticleURL=\"https:\/\/arstechnica.com\/cars\/2018\/10\/honda-will-use-gms-self-driving-technology-invest-2-75-billion\/\"\ndef articleCheck(articleURL): \n    response = requests.get(articleURL)\n    response.encoding = 'utf-8'\n    data = response.text\n    soup = BeautifulSoup(data)\n    if soup.find('article') is None:\n        print (\"No valid article tag!\")\n    else :\n        return soup.find('article')\nvalidArticle = articleCheck(articleURL)\ndef summarizeArticle(validArticle):\n    text = ' '.join(map(lambda p: p.text, soup.find_all('article')))\n    text.encode('ascii', 'ignore')\n    text = text.replace('\\n', ' ').replace('\\r', '').replace('  ','').replace('\\' s','\\'s')\n    sents = sent_tokenize(text)\n    word_sent = word_tokenize(text.lower())\n    _stopwords = set(stopwords.words('english') + list(punctuation))\n    word_sent=[word for word in word_sent if word not in _stopwords]\n    freq = FreqDist(word_sent)\n    nlargest(10, freq, key=freq.get)\n    ranking = defaultdict(int)\n    for i,sent in enumerate(sents):\n        for w in word_tokenize(sent.lower()):\n            if w in freq:\n                ranking[i] += freq[w]\n    sents_idx = nlargest(4, ranking, key=ranking.get)\n    print([sents[j] for j in sorted(sents_idx)])\nsummarizeArticle(validArticle)\n#Regardless of what I do, the library is extracting \"'s\" as a word. I tried removing preceding spaces to\n#connect it to whatever word it belongs with but it will not stop counting it as a word. As far as the \n#metrics go, there is no way to compare it to anything because the scikit metrics library requires\n#an \"answer\"\n","f252cc6b":"# TL;DR - Automated Gist\n## Find the most important words\n### Word Importance = Word Frequency\n## Compute a significance score for sentences based on words they contain\n### Significant score = Sum(Word Importance)\n## Pick the top most significant sentences\n\n* Retrieve Text\n* Preprocess Text\n* Extract Sentences\n\n#### Source: PluralSight - Natural Langauge Processing"}}