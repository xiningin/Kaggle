{"cell_type":{"35d4f8c0":"code","247d7c8d":"code","9951c41b":"code","32259261":"code","c2953005":"code","eb08a668":"code","95bb0f96":"code","ae381d1b":"code","7531666d":"code","47985363":"code","ffb9ae82":"code","3ad52339":"code","59a3a420":"code","322d5b64":"code","248bf890":"code","a3c7693a":"code","610df6fc":"code","70e412f0":"code","a06732ad":"code","478dde7c":"code","fe1ea531":"code","279e0287":"code","4981120b":"code","e3e2692d":"code","48b97fc5":"code","41447219":"code","6fb53768":"code","46d412a3":"code","19e7d7b9":"code","a2b60dd3":"code","2e8b1f0e":"code","6c84822c":"code","dd686c51":"code","22933c9c":"code","1eb6609e":"code","fad0230d":"code","ac4a8078":"code","61f3f53f":"code","4a0ff4df":"code","386560ff":"code","812b51d8":"code","0bbe9a1a":"code","7b74afb8":"code","492c4272":"code","df2c84b9":"code","72c8e576":"code","d42eba9d":"markdown","442a26b4":"markdown","cee48b4c":"markdown","aacf95fb":"markdown","37d25763":"markdown","c5e4a6c4":"markdown","fcc3aaf7":"markdown","8ea84b9f":"markdown","7a2967cb":"markdown","9d8051d8":"markdown"},"source":{"35d4f8c0":"import pandas as pd\nimport numpy as np\nimport datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import Image\nimport math\nfrom scipy.stats import kendalltau\n\nfrom IPython.display import clear_output\nimport timeit\n\nimport warnings\nwarnings.filterwarnings('ignore')","247d7c8d":"#kills = pd.read_csv('C:\\\\Users\\\\Phil\\\\Documents\\\\LoL Model\\\\kills.csv')\n#matchinfo = pd.read_csv('C:\\\\Users\\\\Phil\\\\Documents\\\\LoL Model\\\\matchinfo.csv')\n#monsters = pd.read_csv('C:\\\\Users\\\\Phil\\\\Documents\\\\LoL Model\\\\monsters.csv')\n#structures = pd.read_csv('C:\\\\Users\\\\Phil\\\\Documents\\\\LoL Model\\\\structures.csv')\n\nkills = pd.read_csv('..\/input\/kills.csv')\nmatchinfo = pd.read_csv('..\/input\/matchinfo.csv')\nmonsters = pd.read_csv('..\/input\/monsters.csv')\nstructures = pd.read_csv('..\/input\/structures.csv')\n","9951c41b":"#gold = pd.read_csv('C:\\\\Users\\\\Phil\\\\Documents\\\\LoL Model\\\\gold.csv')\ngold = pd.read_csv('..\/input\/gold.csv')","32259261":"gold = gold[gold['Type']==\"golddiff\"]\ngold.head()","c2953005":"# Add ID column based on last 16 digits in match address for simpler matching\n\nmatchinfo['id'] = matchinfo['Address'].astype(str).str[-16:]\nkills['id'] = kills['Address'].astype(str).str[-16:]\nmonsters['id'] = monsters['Address'].astype(str).str[-16:]\nstructures['id'] = structures['Address'].astype(str).str[-16:]\ngold['id'] = gold['Address'].astype(str).str[-16:]\n","eb08a668":"# Dragon became multiple types in patch v6.9 (http:\/\/leagueoflegends.wikia.com\/wiki\/V6.9) \n# so we remove and games before this change occured and only use games with the new dragon system\n\nold_dragon_id = monsters[ monsters['Type']==\"DRAGON\"]['id'].unique()\nold_dragon_id\n\nmonsters = monsters[ ~monsters['id'].isin(old_dragon_id)]\nmonsters = monsters.reset_index()\n\nmatchinfo = matchinfo[ ~matchinfo['id'].isin(old_dragon_id)]\nmatchinfo = matchinfo.reset_index()\n\nkills = kills[ ~kills['id'].isin(old_dragon_id)]\nkills = kills.reset_index()\n\nstructures = structures[ ~structures['id'].isin(old_dragon_id)]\nstructures = structures.reset_index()\n\ngold = gold[ ~gold['id'].isin(old_dragon_id)]\ngold = gold.reset_index()\n\ngold.head(3)","95bb0f96":"#Transpose Gold table, columns become matches and rows become minutes\n\ngold_T = gold.iloc[:,3:-1].transpose()\ngold_T.head(3)","ae381d1b":"gold2 = pd.DataFrame()\n\nstart = timeit.default_timer()\nfor r in range(0,len(gold)):\n    clear_output(wait=True)\n    \n    # Select each match column, drop any na rows and find the match id from original gold table\n    gold_row = gold_T.iloc[:,r]\n    gold_row = gold_row.dropna()\n    gold_row_id = gold['id'][r]\n    \n    # Append into table so that each match and event is stacked on top of one another    \n    gold2 = gold2.append(pd.DataFrame({'id':gold_row_id,'GoldDiff':gold_row}))\n    \n    \n    stop = timeit.default_timer()\n   \n    if (r\/len(gold)*100) < 5  :\n        expected_time = \"Calculating...\"\n        \n    else:\n        time_perc = timeit.default_timer()\n        expected_time = np.round( ( (time_perc-start)\/60 \/ (r\/len(gold)) ),2)\n        \n  \n        \n    print(\"Current progress:\",np.round(r\/len(gold) *100, 2),\"%\")        \n    print(\"Current run time:\",np.round((stop - start)\/60,2),\"minutes\")\n    print(\"Expected Run Time:\",expected_time,\"minutes\")\n    \n","7531666d":"gold3 = gold2[['id','GoldDiff']]\ngold3.head(3)","47985363":"### Create minute column with index, convert from 'min_1' to just the number\ngold3['Minute'] = gold3.index.to_series()\ngold3['Minute'] = np.where(gold3['Minute'].str[-2]==\"_\", gold3['Minute'].str[-1],gold3['Minute'].str[-2:])\ngold3['Minute'] = gold3['Minute'].astype(int)\ngold3 = gold3.reset_index()\ngold3 = gold3.sort_values(by=['id','Minute'])\n\ngold3.head(3)\n","ffb9ae82":"# Gold difference from data is relative to blue team's perspective,\n# therefore we reverse this by simply multiplying amount by -1\ngold3['GoldDiff'] = gold3['GoldDiff']*-1\n\ngold3.head(10)","3ad52339":"matchinfo.head(3)","59a3a420":"gold4 = gold3\n\nmatchinfo2 = matchinfo[['id','rResult','gamelength']]\nmatchinfo2['gamlength'] = matchinfo2['gamelength'] + 1\nmatchinfo2['index'] = 'min_'+matchinfo2['gamelength'].astype(str)\nmatchinfo2['rResult2'] =  np.where(matchinfo2['rResult']==1,999999,-999999)\nmatchinfo2 = matchinfo2[['index','id','rResult2','gamelength']]\nmatchinfo2.columns = ['index','id','GoldDiff','Minute']\n\n\ngold4 = gold4.append(matchinfo2)\ngold4.tail()","322d5b64":"kills = kills[ kills['Time']>0]\n\nkills['Minute'] = kills['Time'].astype(int)\n\nkills['Team'] = np.where( kills['Team']==\"rKills\",\"Red\",\"Blue\")\nkills.head(3)","248bf890":"# For the Kills table, we need decided to group by the minute in which the kills took place and averaged \n# the time of the kills which we use later for the order of events\n\nf = {'Time':['mean','count']}\n\nkillsGrouped = kills.groupby( ['id','Team','Minute'] ).agg(f).reset_index()\nkillsGrouped.columns = ['id','Team','Minute','Time Avg','Count']\nkillsGrouped = killsGrouped.sort_values(by=['id','Minute'])\nkillsGrouped.head(3)","a3c7693a":"structures = structures[ structures['Time']>0]\n\nstructures['Minute'] = structures['Time'].astype(int)\nstructures['Team'] = np.where(structures['Team']==\"bTowers\",\"Blue\",\n                        np.where(structures['Team']==\"binhibs\",\"Blue\",\"Red\"))\nstructures2 = structures.sort_values(by=['id','Minute'])\n\nstructures2 = structures2[['id','Team','Time','Minute','Type']]\nstructures2.head(3)","610df6fc":"monsters['Type2'] = np.where( monsters['Type']==\"FIRE_DRAGON\", \"DRAGON\",\n                    np.where( monsters['Type']==\"EARTH_DRAGON\",\"DRAGON\",\n                    np.where( monsters['Type']==\"WATER_DRAGON\",\"DRAGON\",       \n                    np.where( monsters['Type']==\"AIR_DRAGON\",\"DRAGON\",   \n                             monsters['Type']))))\n\nmonsters = monsters[ monsters['Time']>0]\n\nmonsters['Minute'] = monsters['Time'].astype(int)\n\nmonsters['Team'] = np.where( monsters['Team']==\"bDragons\",\"Blue\",\n                   np.where( monsters['Team']==\"bHeralds\",\"Blue\",\n                   np.where( monsters['Team']==\"bBarons\", \"Blue\", \n                           \"Red\")))\n\nmonsters = monsters[['id','Team','Time','Minute','Type2']]\nmonsters.columns = ['id','Team','Time','Minute','Type']\nmonsters.head(3)","70e412f0":"GoldstackedData = gold4.merge(killsGrouped, how='left',on=['id','Minute'])\n \nmonsters_structures_stacked = structures2.append(monsters[['id','Team','Minute','Time','Type']])\n\nGoldstackedData2 = GoldstackedData.merge(monsters_structures_stacked, how='left',on=['id','Minute'])\n\nGoldstackedData2 = GoldstackedData2.sort_values(by=['id','Minute'])\nGoldstackedData2.head(30)","a06732ad":"GoldstackedData3 = GoldstackedData2\nGoldstackedData3['Time2'] = GoldstackedData3['Time'].fillna(GoldstackedData3['Time Avg']).fillna(GoldstackedData3['Minute'])\nGoldstackedData3['Team'] = GoldstackedData3['Team_x'].fillna(GoldstackedData3['Team_y'])\nGoldstackedData3 = GoldstackedData3.sort_values(by=['id','Time2'])\n\nGoldstackedData3['EventNum'] = GoldstackedData3.groupby('id').cumcount()+1\n\nGoldstackedData3 = GoldstackedData3[['id','EventNum','Team','Minute','Time2','GoldDiff','Count','Type']]\n\nGoldstackedData3.columns = ['id','EventNum','Team','Minute','Time','GoldDiff','KillCount','Struct\/Monster']\n\nGoldstackedData3.head(30)","478dde7c":"GoldstackedData3[GoldstackedData3['GoldDiff']==999999].head(3)","fe1ea531":"# We then add an 'Event' column to merge the columns into one, where kills are now\n# simple labelled as 'KILLS'\n\nGoldstackedData3['Event'] = np.where(GoldstackedData3['KillCount']>0,\"KILLS\",None)\nGoldstackedData3['Event'] = GoldstackedData3['Event'].fillna(GoldstackedData3['Struct\/Monster'])\n\nGoldstackedData3['Event'] = GoldstackedData3['Event'].fillna(\"NONE\")\n\nGoldstackedData3['GoldDiff2'] = np.where( GoldstackedData3['GoldDiff']== 999999,\"WIN\",\n                                np.where( GoldstackedData3['GoldDiff']==-999999, 'LOSS',\n                                         \n    \n                                np.where((GoldstackedData3['GoldDiff']<1000) & (GoldstackedData3['GoldDiff']>-1000),\n                                        \"EVEN\",\n                                np.where( (GoldstackedData3['GoldDiff']>=1000) & (GoldstackedData3['GoldDiff']<2500),\n                                         \"SLIGHTLY_AHEAD\",\n                                np.where( (GoldstackedData3['GoldDiff']>=2500) & (GoldstackedData3['GoldDiff']<5000),\n                                         \"AHEAD\",\n                                np.where( (GoldstackedData3['GoldDiff']>=5000),\n                                         \"VERY_AHEAD\",\n                                         \n                                np.where( (GoldstackedData3['GoldDiff']<=-1000) & (GoldstackedData3['GoldDiff']>-2500),\n                                         \"SLIGHTLY_BEHIND\",\n                                np.where( (GoldstackedData3['GoldDiff']<=-2500) & (GoldstackedData3['GoldDiff']>-5000),\n                                         \"BEHIND\",\n                                np.where( (GoldstackedData3['GoldDiff']<=-5000),\n                                         \"VERY_BEHIND\",\"ERROR\"\n                                        \n                                        )))))))))\n\nGoldstackedData3.head(3)","279e0287":"GoldstackedData3[GoldstackedData3['GoldDiff2']==\"ERROR\"]","4981120b":"GoldstackedData3[GoldstackedData3['Team']=='Blue'].head(10)","e3e2692d":"GoldstackedData3['Next_Min'] = GoldstackedData3['Minute']+1\n\n\nGoldstackedData4 = GoldstackedData3.merge(gold4[['id','Minute','GoldDiff']],how='left',left_on=['id','Next_Min'],\n                                         right_on=['id','Minute'])\n\nGoldstackedData4.head(10)","48b97fc5":"GoldstackedData4[ GoldstackedData4['GoldDiff_y']== -999999].head(3)","41447219":"GoldstackedData4['GoldDiff2_Next'] =  np.where( GoldstackedData4['GoldDiff_y']== 999999,\"WIN\",\n                                np.where( GoldstackedData4['GoldDiff_y']==-999999, 'LOSS',\n                                         \n    \n                                np.where((GoldstackedData4['GoldDiff_y']<1000) & (GoldstackedData4['GoldDiff_y']>-1000),\n                                        \"EVEN\",\n                                np.where( (GoldstackedData4['GoldDiff_y']>=1000) & (GoldstackedData4['GoldDiff_y']<2500),\n                                         \"SLIGHTLY_AHEAD\",\n                                np.where( (GoldstackedData4['GoldDiff_y']>=2500) & (GoldstackedData4['GoldDiff_y']<5000),\n                                         \"AHEAD\",\n                                np.where( (GoldstackedData4['GoldDiff_y']>=5000),\n                                         \"VERY_AHEAD\",\n                                         \n                                np.where( (GoldstackedData4['GoldDiff_y']<=-1000) & (GoldstackedData4['GoldDiff_y']>-2500),\n                                         \"SLIGHTLY_BEHIND\",\n                                np.where( (GoldstackedData4['GoldDiff_y']<=-2500) & (GoldstackedData4['GoldDiff_y']>-5000),\n                                         \"BEHIND\",\n                                np.where( (GoldstackedData4['GoldDiff_y']<=-5000),\n                                         \"VERY_BEHIND\",\"ERROR\"\n                                        \n                                        )))))))))\nGoldstackedData4 = GoldstackedData4[['id','EventNum','Team','Minute_x','Time','Event','GoldDiff2','GoldDiff2_Next']]\nGoldstackedData4.columns = ['id','EventNum','Team','Minute','Time','Event','GoldDiff2','GoldDiff2_Next']\n\nGoldstackedData4['Event'] = np.where( GoldstackedData4['Team']==\"Red\", \"+\"+GoldstackedData4['Event'],\n                                np.where(GoldstackedData4['Team']==\"Blue\", \"-\"+GoldstackedData4['Event'], \n                                         GoldstackedData4['Event']))\n\n#GoldstackedData4.head(10)","6fb53768":"# Errors are caused due to game ending in minute and then there is no 'next_min' info for this game but our method expects there to be\nGoldstackedData4 = GoldstackedData4[GoldstackedData4['GoldDiff2_Next']!=\"ERROR\"]\nGoldstackedData4[GoldstackedData4['GoldDiff2_Next']==\"ERROR\"]","46d412a3":"GoldstackedDataFINAL = GoldstackedData4\nGoldstackedDataFINAL['Min_State_Action_End'] = ((GoldstackedDataFINAL['Minute'].astype(str)) + \"_\"\n                                       + (GoldstackedDataFINAL['GoldDiff2'].astype(str)) + \"_\"\n                                       + (GoldstackedDataFINAL['Event'].astype(str)) + \"_\"  \n                                       + (GoldstackedDataFINAL['GoldDiff2_Next'].astype(str))\n                                      )\n\nGoldstackedDataFINAL['MSAE'] = ((GoldstackedDataFINAL['Minute'].astype(str)) + \"_\"\n                                       + (GoldstackedDataFINAL['GoldDiff2'].astype(str)) + \"_\"\n                                       + (GoldstackedDataFINAL['Event'].astype(str)) + \"_\"  \n                                       + (GoldstackedDataFINAL['GoldDiff2_Next'].astype(str))\n                                      )\n\nGoldstackedDataFINAL.head()","19e7d7b9":"goldMDP = GoldstackedDataFINAL[['Minute','GoldDiff2','Event','GoldDiff2_Next']]\ngoldMDP.columns = ['Minute','State','Action','End']\ngoldMDP['Counter'] = 1\ngoldMDP.head()","a2b60dd3":"goldMDP[goldMDP['End']=='ERROR'].head(3)","2e8b1f0e":"goldMDP2 = goldMDP.groupby(['Minute','State','Action','End']).count().reset_index()\ngoldMDP2['Prob'] = goldMDP2['Counter']\/(goldMDP2['Counter'].sum())\ngoldMDP2.head()","6c84822c":"goldMDP3 = goldMDP.groupby(['Minute','State','Action']).count().reset_index()\ngoldMDP3['Prob'] = goldMDP3['Counter']\/(goldMDP3['Counter'].sum())\ngoldMDP3.head()","dd686c51":"goldMDP4 = goldMDP2.merge(goldMDP3[['Minute','State','Action','Prob']], how='left',on=['Minute','State','Action'] )\ngoldMDP4.head(20)","22933c9c":"goldMDP4['GivenProb'] = goldMDP4['Prob_x']\/goldMDP4['Prob_y']\ngoldMDP4 = goldMDP4.sort_values('GivenProb',ascending=False)\ngoldMDP4['Next_Minute'] = goldMDP4['Minute']+1\ngoldMDP4[(goldMDP4['State']!=goldMDP4['End'])&(goldMDP4['Counter']>10)&(goldMDP4['State']!=\"WIN\")&(goldMDP4['State']!=\"LOSS\")].head(10)","1eb6609e":"def MCModelv4(data, alpha, gamma, epsilon, reward, StartState, StartMin, StartAction, num_episodes, Max_Mins):\n    \n    # Initiatise variables appropiately\n    \n    data['V'] = 0\n    data_output = data\n    \n    outcomes = pd.DataFrame()\n    episode_return = pd.DataFrame()\n    actions_output = pd.DataFrame()\n    V_output = pd.DataFrame()\n    \n    \n    Actionist = [\n       'NONE',\n       'KILLS', 'OUTER_TURRET', 'DRAGON', 'RIFT_HERALD', 'BARON_NASHOR',\n       'INNER_TURRET', 'BASE_TURRET', 'INHIBITOR', 'NEXUS_TURRET',\n       'ELDER_DRAGON']\n    \n    \n    for e in range(0,num_episodes):\n        action = []\n        \n        current_min = StartMin\n        current_state = StartState\n        \n        \n        \n        data_e1 = data\n    \n    \n        actions = pd.DataFrame()\n\n        for a in range(0,100):\n            \n            action_table = pd.DataFrame()\n       \n            # Break condition if game ends or gets to a large number of mins \n            if (current_state==\"WIN\") | (current_state==\"LOSS\") | (current_min==Max_Mins):\n                continue\n            else:\n                if a==0:\n                    data_e1=data_e1\n                   \n                elif (len(individual_actions_count[individual_actions_count['Action']==\"+RIFT_HERALD\"])==1):\n                    data_e1_e1 = data_e1[(data_e1['Action']!='+RIFT_HERALD')|(data_e1['Action']!='-RIFT_HERALD')]\n                    \n                elif (len(individual_actions_count[individual_actions_count['Action']==\"-RIFT_HERALD\"])==1):\n                    data_e1 = data_e1[(data_e1['Action']!='+RIFT_HERALD')|(data_e1['Action']!='-RIFT_HERALD')]\n                \n                elif (len(individual_actions_count[individual_actions_count['Action']==\"+OUTER_TURRET\"])==3):\n                    data_e1 = data_e1[data_e1['Action']!='+OUTER_TURRET']\n                elif (len(individual_actions_count[individual_actions_count['Action']==\"-OUTER_TURRET\"])==3):\n                    data_e1 = data_e1[data_e1['Action']!='-OUTER_TURRET']\n                    \n                elif (len(individual_actions_count[individual_actions_count['Action']==\"+INNER_TURRET\"])==3):\n                    data_e1 = data_e1[data_e1['Action']!='+INNER_TURRET']\n                elif (len(individual_actions_count[individual_actions_count['Action']==\"-INNER_TURRET\"])==3):\n                    data_e1 = data_e1[data_e1['Action']!='-INNER_TURRET']\n                    \n                elif (len(individual_actions_count[individual_actions_count['Action']==\"+BASE_TURRET\"])==3):\n                    data_e1 = data_e1[data_e1['Action']!='+BASE_TURRET']\n                elif (len(individual_actions_count[individual_actions_count['Action']==\"-BASE_TURRET\"])==3):\n                    data_e1 = data_e1[data_e1['Action']!='-BASE_TURRET']\n                    \n                elif (len(individual_actions_count[individual_actions_count['Action']==\"+INHIBITOR\"])==3):\n                    data_e1 = data_e1[data_e1['Action']!='+INHIBITOR']\n                elif (len(individual_actions_count[individual_actions_count['Action']==\"-INHIBITOR\"])==3):\n                    data_e1 = data_e1[data_e1['Action']!='-INHIBITOR']\n                elif (len(individual_actions_count[individual_actions_count['Action']==\"+NEXUS_TURRET\"])==2):\n                    data_e1 = data_e1[data_e1['Action']!='+NEXUS_TURRET']\n                elif (len(individual_actions_count[individual_actions_count['Action']==\"-NEXUS_TURRET\"])==2):\n                    data_e1 = data_e1[data_e1['Action']!='-NEXUS_TURRET']\n                \n                       \n                else:\n                    data_e1 = data_e1\n                    \n                # Break condition if we do not have enough data    \n                if len(data_e1[(data_e1['Minute']==current_min)&(data_e1['State']==current_state)])==0:\n                    continue\n                else:             \n\n                    if (a>0) | (StartAction is None):\n                        random_action = data_e1[(data_e1['Minute']==current_min)&(data_e1['State']==current_state)].sample()\n                        random_action = random_action.reset_index()\n                        current_action = random_action['Action'][0]\n                    else:\n                        current_action =  StartAction\n\n\n                    data_e = data_e1[(data_e1['Minute']==current_min)&(data_e1['State']==current_state)&(data_e1['Action']==current_action)]\n\n                    data_e = data_e[data_e['GivenProb']>0]\n\n\n\n\n\n                    data_e = data_e.sort_values('GivenProb')\n                    data_e['CumProb'] = data_e['GivenProb'].cumsum()\n                    data_e['CumProb'] = np.round(data_e['CumProb'],4)\n\n\n                    rng = np.round(np.random.random()*data_e['CumProb'].max(),4)\n                    action_table = data_e[ data_e['CumProb'] >= rng]\n                    action_table = action_table[ action_table['CumProb'] == action_table['CumProb'].min()]\n                    action_table = action_table.reset_index()\n\n\n                    action = current_action\n                    next_state = action_table['End'][0]\n                    next_min = current_min+1\n\n\n                    if next_state == \"WIN\":\n                        step_reward = 10*(gamma**a)\n                    elif next_state == \"LOSS\":\n                        step_reward = -10*(gamma**a)\n                    else:\n                        step_reward = -0.005*(gamma**a)\n\n                    action_table['StepReward'] = step_reward\n\n\n                    action_table['Episode'] = e\n                    action_table['Action_Num'] = a\n\n                    current_action = action\n                    current_min = next_min\n                    current_state = next_state\n\n\n                    actions = actions.append(action_table)\n\n                    individual_actions_count = actions\n\n\n        actions_output = actions_output.append(actions)\n                \n        episode_return = actions['StepReward'].sum()\n\n                \n        actions['Return']= episode_return\n                \n        data_output = data_output.merge(actions[['Minute','State','Action','End','Return']], how='left',on =['Minute','State','Action','End'])\n        data_output['Return'] = data_output['Return'].fillna(0)    \n             \n        data_output['V'] = data_output['V'] + alpha*(data_output['Return']-data_output['V'])\n        data_output = data_output.drop('Return', 1)\n        \n        V_outputs = pd.DataFrame({'Episode':[e],'V_total':[data_output['V'].sum()]})\n        V_output = V_output.append(V_outputs)\n        \n                \n        if current_state==\"WIN\":\n            outcome = \"WIN\"\n        elif current_state==\"LOSS\":\n            outcome = \"LOSS\"\n        else:\n            outcome = \"INCOMPLETE\"\n        outcome = pd.DataFrame({'Epsiode':[e],'Outcome':[outcome]})\n        outcomes = outcomes.append(outcome)\n\n        \n   \n\n\n    return(outcomes,actions_output,data_output,V_output)\n    ","fad0230d":"alpha = 0.1\ngamma = 0.9\nnum_episodes = 100\nepsilon = 0.1\n\n\ngoldMDP4['Reward'] = 0\nreward = goldMDP4['Reward']\n\nStartMin = 15\nStartState = 'EVEN'\nStartAction = None\ndata = goldMDP4\n\nMax_Mins = 50\nstart_time = timeit.default_timer()\n\n\nMdl4 = MCModelv4(data=data, alpha = alpha, gamma=gamma, epsilon = epsilon, reward = reward,\n                StartMin = StartMin, StartState=StartState,StartAction=StartAction, \n                num_episodes = num_episodes, Max_Mins = Max_Mins)\n\nelapsed = timeit.default_timer() - start_time\n\nprint(\"Time taken to run model:\",np.round(elapsed\/60,2),\"mins\")\nprint(\"Avg Time taken per episode:\", np.round(elapsed\/num_episodes,2),\"secs\")","ac4a8078":"Mdl4[1].head(10)","61f3f53f":"final_output = Mdl4[2]\n\nfinal_output2 = final_output[(final_output['Minute']==StartMin)&(final_output['State']==StartState)].sort_values('V',ascending=False).head(10)\nfinal_output2","4a0ff4df":"sum_V_episodes = Mdl4[3]\n\nplt.plot(sum_V_episodes['Episode'],sum_V_episodes['V_total'])\nplt.xlabel(\"Epsiode\")\nplt.ylabel(\"Sum of V\")\nplt.title(\"Cumulative V by Episode\")\nplt.show()","386560ff":"from IPython.display import clear_output","812b51d8":"def MCModelv5(data, alpha, gamma, epsilon, reward, StartState, StartMin, StartAction, num_episodes, Max_Mins):\n    \n    # Initiatise variables appropiately\n    \n    data['V'] = 0\n    data_output = data\n    \n    outcomes = pd.DataFrame()\n    episode_return = pd.DataFrame()\n    actions_output = pd.DataFrame()\n    V_output = pd.DataFrame()\n    \n    \n    Actionist = [\n       'NONE',\n       'KILLS', 'OUTER_TURRET', 'DRAGON', 'RIFT_HERALD', 'BARON_NASHOR',\n       'INNER_TURRET', 'BASE_TURRET', 'INHIBITOR', 'NEXUS_TURRET',\n       'ELDER_DRAGON']\n        \n    for e in range(0,num_episodes):\n        clear_output(wait=True)\n        \n        action = []\n        \n        current_min = StartMin\n        current_state = StartState\n        \n        \n        \n        data_e1 = data\n    \n    \n        actions = pd.DataFrame()\n\n        for a in range(0,100):\n            \n            action_table = pd.DataFrame()\n       \n            # Break condition if game ends or gets to a large number of mins \n            if (current_state==\"WIN\") | (current_state==\"LOSS\") | (current_min==Max_Mins):\n                continue\n            else:\n                if a==0:\n                    data_e1=data_e1\n                   \n                elif (len(individual_actions_count[individual_actions_count['Action']==\"+RIFT_HERALD\"])==1):\n                    data_e1_e1 = data_e1[(data_e1['Action']!='+RIFT_HERALD')|(data_e1['Action']!='-RIFT_HERALD')]\n                    \n                elif (len(individual_actions_count[individual_actions_count['Action']==\"-RIFT_HERALD\"])==1):\n                    data_e1 = data_e1[(data_e1['Action']!='+RIFT_HERALD')|(data_e1['Action']!='-RIFT_HERALD')]\n                \n                elif (len(individual_actions_count[individual_actions_count['Action']==\"+OUTER_TURRET\"])==3):\n                    data_e1 = data_e1[data_e1['Action']!='+OUTER_TURRET']\n                elif (len(individual_actions_count[individual_actions_count['Action']==\"-OUTER_TURRET\"])==3):\n                    data_e1 = data_e1[data_e1['Action']!='-OUTER_TURRET']\n                    \n                elif (len(individual_actions_count[individual_actions_count['Action']==\"+INNER_TURRET\"])==3):\n                    data_e1 = data_e1[data_e1['Action']!='+INNER_TURRET']\n                elif (len(individual_actions_count[individual_actions_count['Action']==\"-INNER_TURRET\"])==3):\n                    data_e1 = data_e1[data_e1['Action']!='-INNER_TURRET']\n                    \n                elif (len(individual_actions_count[individual_actions_count['Action']==\"+BASE_TURRET\"])==3):\n                    data_e1 = data_e1[data_e1['Action']!='+BASE_TURRET']\n                elif (len(individual_actions_count[individual_actions_count['Action']==\"-BASE_TURRET\"])==3):\n                    data_e1 = data_e1[data_e1['Action']!='-BASE_TURRET']\n                    \n                elif (len(individual_actions_count[individual_actions_count['Action']==\"+INHIBITOR\"])==3):\n                    data_e1 = data_e1[data_e1['Action']!='+INHIBITOR']\n                elif (len(individual_actions_count[individual_actions_count['Action']==\"-INHIBITOR\"])==3):\n                    data_e1 = data_e1[data_e1['Action']!='-INHIBITOR']\n                elif (len(individual_actions_count[individual_actions_count['Action']==\"+NEXUS_TURRET\"])==2):\n                    data_e1 = data_e1[data_e1['Action']!='+NEXUS_TURRET']\n                elif (len(individual_actions_count[individual_actions_count['Action']==\"-NEXUS_TURRET\"])==2):\n                    data_e1 = data_e1[data_e1['Action']!='-NEXUS_TURRET']\n                \n                       \n                else:\n                    data_e1 = data_e1\n                    \n                # Break condition if we do not have enough data    \n                if len(data_e1[(data_e1['Minute']==current_min)&(data_e1['State']==current_state)])==0:\n                    continue\n                else:             \n\n                    \n                    # Greedy Selection:\n                    # If this is our first action and start action is non, select greedily. \n                    # Else, if first actions is given in our input then we use this as our start action. \n                    # Else for other actions, if it is the first episode then we have no knowledge so randomly select actions\n                    # Else for other actions, we randomly select actions a percentage of the time based on our epsilon and greedily (max V) for the rest \n                    \n                    \n                    if   (a==0) & (StartAction is None):\n                        random_action = data_e1[(data_e1['Minute']==current_min)&(data_e1['State']==current_state)].sample()\n                        random_action = random_action.reset_index()\n                        current_action = random_action['Action'][0]\n                    elif (a==0):\n                        current_action =  StartAction\n                    \n                    elif (e==0) & (a>0):\n                        random_action = data_e1[(data_e1['Minute']==current_min)&(data_e1['State']==current_state)].sample()\n                        random_action = random_action.reset_index()\n                        current_action = random_action['Action'][0]\n                    \n                    elif (e>0) & (a>0):\n                        epsilon = epsilon\n                        greedy_rng = np.round(np.random.random(),2)\n                        if (greedy_rng<=epsilon):\n                            random_action = data_e1[(data_e1['Minute']==current_min)&(data_e1['State']==current_state)].sample()\n                            random_action = random_action.reset_index()\n                            current_action = random_action['Action'][0]\n                        else:\n                            greedy_action = (\n                            \n                                data_e1[(data_e1['Minute']==current_min)&(data_e1['State']==current_state)][\n                                    \n                                    data_e1[(data_e1['Minute']==current_min)&(data_e1['State']==current_state)]['V']==data_e1[(data_e1['Minute']==current_min)&(data_e1['State']==current_state)]['V'].max()\n                                \n                                ])\n                                \n                            greedy_action = greedy_action.reset_index()\n                            current_action = greedy_action['Action'][0]\n                            \n                  \n                    \n                        \n                   \n                            \n                        \n                        \n                        \n                    \n\n\n                    data_e = data_e1[(data_e1['Minute']==current_min)&(data_e1['State']==current_state)&(data_e1['Action']==current_action)]\n\n                    data_e = data_e[data_e['GivenProb']>0]\n\n\n\n\n\n                    data_e = data_e.sort_values('GivenProb')\n                    data_e['CumProb'] = data_e['GivenProb'].cumsum()\n                    data_e['CumProb'] = np.round(data_e['CumProb'],4)\n\n\n                    rng = np.round(np.random.random()*data_e['CumProb'].max(),4)\n                    action_table = data_e[ data_e['CumProb'] >= rng]\n                    action_table = action_table[ action_table['CumProb'] == action_table['CumProb'].min()]\n                    action_table = action_table.reset_index()\n\n\n                    action = current_action\n                    next_state = action_table['End'][0]\n                    next_min = current_min+1\n\n\n                    if next_state == \"WIN\":\n                        step_reward = 10*(gamma**a)\n                    elif next_state == \"LOSS\":\n                        step_reward = -10*(gamma**a)\n                    else:\n                        step_reward = -0.005*(gamma**a)\n\n                    action_table['StepReward'] = step_reward\n\n\n                    action_table['Episode'] = e\n                    action_table['Action_Num'] = a\n\n                    current_action = action\n                    current_min = next_min\n                    current_state = next_state\n\n\n                    actions = actions.append(action_table)\n\n                    individual_actions_count = actions\n                    \n        print(\"Current progress:\", np.round((e\/num_episodes)*100,2),\"%\")\n\n        actions_output = actions_output.append(actions)\n                \n        episode_return = actions['StepReward'].sum()\n\n                \n        actions['Return']= episode_return\n                \n        data_output = data_output.merge(actions[['Minute','State','Action','End','Return']], how='left',on =['Minute','State','Action','End'])\n        data_output['Return'] = data_output['Return'].fillna(0)    \n             \n            \n        data_output['V'] = np.where(data_output['Return']==0,data_output['V'],data_output['V'] + alpha*(data_output['Return']-data_output['V']))\n        \n        data_output = data_output.drop('Return', 1)\n\n        \n        for actions in data_output[(data_output['Minute']==StartMin)&(data_output['State']==StartState)]['Action'].unique():\n            V_outputs = pd.DataFrame({'Index':[str(e)+'_'+str(actions)],'Episode':e,'StartMin':StartMin,'StartState':StartState,'Action':actions,\n                                      'V':data_output[(data_output['Minute']==StartMin)&(data_output['State']==StartState)&(data_output['Action']==actions)]['V'].sum()\n                                     })\n            V_output = V_output.append(V_outputs)\n        \n        if current_state==\"WIN\":\n            outcome = \"WIN\"\n        elif current_state==\"LOSS\":\n            outcome = \"LOSS\"\n        else:\n            outcome = \"INCOMPLETE\"\n        outcome = pd.DataFrame({'Epsiode':[e],'Outcome':[outcome]})\n        outcomes = outcomes.append(outcome)\n\n        \n   \n\n\n    return(outcomes,actions_output,data_output,V_output)\n    ","0bbe9a1a":"alpha = 0.3\ngamma = 0.9\nnum_episodes = 1000\nepsilon = 0.2\n\n\ngoldMDP4['Reward'] = 0\nreward = goldMDP4['Reward']\n\nStartMin = 15\nStartState = 'EVEN'\nStartAction = None\ndata = goldMDP4\n\nMax_Mins = 50\nstart_time = timeit.default_timer()\n\n\nMdl5 = MCModelv5(data=data, alpha = alpha, gamma=gamma, epsilon = epsilon, reward = reward,\n                StartMin = StartMin, StartState=StartState,StartAction=StartAction, \n                num_episodes = num_episodes, Max_Mins = Max_Mins)\n\nelapsed = timeit.default_timer() - start_time\n\nprint(\"Time taken to run model:\",np.round(elapsed\/60,2),\"mins\")\nprint(\"Avg Time taken per episode:\", np.round(elapsed\/num_episodes,2),\"secs\")","7b74afb8":"Mdl5[3].sort_values(['Episode','Action']).head(10)","492c4272":"V_episodes = Mdl5[3]\n\nplt.figure(figsize=(20,10))\n\nfor actions in V_episodes['Action'].unique():\n    plot_data = V_episodes[V_episodes['Action']==actions]\n    plt.plot(plot_data['Episode'],plot_data['V'])\nplt.xlabel(\"Epsiode\")\nplt.ylabel(\"V\")\nplt.title(\"V for each Action by Episode\")\n#plt.show()\n","df2c84b9":"final_output = Mdl5[2]\n\n\nfinal_output2 = final_output[(final_output['Minute']==StartMin)&(final_output['State']==StartState)]\nfinal_output3 = final_output2.groupby(['Minute','State','Action']).sum().sort_values('V',ascending=False).reset_index()\nfinal_output3[['Minute','State','Action','V']]","72c8e576":"single_action1 = final_output3['Action'][0]\nsingle_action2 = final_output3['Action'][len(final_output3)-1]\n\nplot_data1 = V_episodes[(V_episodes['Action']==single_action1)]\nplot_data2 = V_episodes[(V_episodes['Action']==single_action2)]\n\nplt.plot(plot_data1['Episode'],plot_data1['V'], label = single_action1)\nplt.plot(plot_data2['Episode'],plot_data2['V'], label = single_action2)\nplt.xlabel(\"Epsiode\")\nplt.ylabel(\"V\")\nplt.legend()\nplt.title(\"V by Episode for the Best\/Worst Actions given the Current State\")\nplt.show()","d42eba9d":"In theory, we are now ready to run the model for as many episodes as possible so that the results will eventually converge to an optimal suggestion. However, it seems that we will need a lot of epsiodes with this current method to get anything close to convergence.\n\n","442a26b4":"### Import Packages and Data","cee48b4c":"### Introducing Gold Difference to Redesign the Markov Decision Process","aacf95fb":"We have also changed our sum of V output to only be for the possible actions from our start state as these are the only ones we are currently concerned with.","37d25763":"### Motivations and Objectives\nLeague of Legends is a team oriented video game where on two team teams (with 5 players in each) compete for objectives and kills. Gaining an advantage enables the players to become stronger (obtain better items and level up faster) than their opponents and, as their advantage increases, the likelihood of winning the game also increases. We therefore have a sequence of events dependent on previous events that lead to one team destroying the other\u2019s base and winning the game. \n\nSequences like this being modelled statistically is nothing new; for years now researchers have considered how this is applied in sports, such as basketball (https:\/\/arxiv.org\/pdf\/1507.01816.pdf), where a sequence of passing, dribbling and foul plays lead to a team obtaining or losing points. The aim of research such as this one mentioned is to provide more detailed insight beyond a simple box score (number of points or kill gained by player in basketball or video games respectively) and consider how teams perform when modelled as a sequence of events connected in time. \n\nModelling the events in this way is even more important in games such as League of Legends as taking objectives and kills lead towards both an item and level advantage. For example, a player obtaining the first kill of the game nets them gold that can be used to purchase more powerful items. With this item they are then strong enough to obtain more kills and so on until they can lead their team to a win. Facilitating a lead like this is often referred to as \u2018snowballing\u2019 as the players cumulatively gain advantages but often games are not this one sided and objects and team plays are more important. \n\n#### The aim of this is project is simple; can we calculate the next best event given what has occurred previously in the game so that the likelihood of eventually leading to a win increases based on real match statistics?\n\nHowever, there are many factors that lead to a player\u2019s decision making in a game that cannot be easily measured. No how matter how much data collected, the amount of information a player can capture is beyond any that a computer can detect (at least for now!). For example, players may be over or underperforming in this game or may simply have a preference for the way they play (often defined by the types of characters they play). Some players will naturally be more aggressive and look for kills while others will play passively and push for objectives instead.\nTherefore, we further develop our model to allow the player to adjust the recommended play on their preferences.\n","c5e4a6c4":"# AI in Video Games: Improving Decision Making in League of Legends using Real Match Statistics and Personal Preferences\n\n## Part 2: Redesigning Markov Decision Process with Gold Difference and Improving Model","fcc3aaf7":"## Reinforcement Learning AI Model\n\n\nNow that we have our data modelled as an MDP, we can apply Reinforcement Learning. In short, this applied a model that simulates thousands of games and learns how good or bad each decision is towards reaching a win given the team\u2019s current position. \n\nWhat makes this AI is its ability to learn from its own trial and error experience. It starts with zero knowledge about the game but, as it is rewarded for reaching a win and punished for reaching a loss, it begins to recognise and remember which decisions are better than others. Our first models start with no knowledge but I later demonstrate the impact initial information about decisions can be fed into the model to represent a person\u2019s preferences.  \n\nSo how is the model learning? In short, we use Monte Carlo learning whereby each episode is a simulation of a game based on our MDP probabilities and depending on the outcome for the team, our return will vary (+1 terminal reward for win and -1 terminal reward for loss). The value of each action taken in this episode is then updated accordingly based on whether the outcome was a win or loss. \n\nIn Monte Carlo learning, we have a parameter 'gamma' that discounts the rewards and will give a higher value to immediate steps than later one. In our model, this can be understood by the fact that as we reach later stages of the games, the decisions we make will have a much larger impact on the final outcome than those made in the first few minutes. For example, losing a team fight in minute 50 is much more likely to lead to a loss than losing a team fight in the first 5 minutes.\n\nWe can re-apply our model from part 1 with some minor adjustments for our new MDP.\n","8ea84b9f":"### Model Improvements\n\nThere are many ways we can improve this, one solution would be to greatly simplify the problem by breaking the game into segments. Often these segments are referred to as early, mid and late game and would be we would only need to consider fewer steps to reach an end goal. In this case it would not be to lead to a win but rather aiming to be at an advantage and the end of, say, 10 minute intervals. \n\nAnother solution would be to use a model that learns quicker instead of the Monte Carlo method used. This often includes Q-learning or SARSA.\n\nWe will not consider doing this here as they would require a lot of work to re-adjust the code. Instead, we will improve the rate the model learns by the way it selects its actions. Currently, we can either define the first action or it will choose randomly which means it is. However, after this first action all subsequent actions are also chosen randomly which is causing the number of episodes needed to exponentially increase for convergence to occur.\n\nTherefore, we will introduce a basic action selection method for these known as greedy selection. This means we select the best action the majority of the time therefore continually testing the success rate of the actions it thinks are best. It will also randomly select actions some of the time so that it can still explore states and doesnt get caught in a local maximum and not the optimal sulution. \n\nAlso parameters play a key part in how quickly the output will converge, none moreso than our alpha parameter. Although a small alpha will be more accurate, a larger alpha value will learn and therefore subsequently converge faster. \n\nWe will adjust our code to utilise both greedy selection and a reasonably large alpha as well as running for as many episodes as possible. More episodes means the runtime will be longer but because, at least at this stage, we are not attemping to have this running in real time there is no issue with it running for a while to find an optimal suggestion. \n\nIt takes approximately 10 mins to run the model for 1,000 episodes, we have also included a tracking system for the run progress which shows the current percentage (based on which episode) the loop is in. This is particuarly useful if running for anything more than a few minutes.\n\nIt was at this stage, I also notice that my method for applying the update rule was overriding previous knowledge if the action wasnt used in the current episode and so all results were converging back to zero after each episode. I fixed this by making it so if the state + action wasnt used in the episode, then it simply remians the same and will show in our results as being the flat parts of the lines. \n\nLastly, we output the value of each of our available actions given our start state in each episode so we can track the learning process for these and how, over many episodes, our optimal action is decided.\n","7a2967cb":"### Part 2 Conclusion\n\nWe have now fixed our issue highlighted in part one and have a MDP that takes into account cumulative success\/failures in a match by defining our current\/next states by a gold advtantage\/disadvantage.\n\nWe have also made a number of improvements to our model but there are many aspects that could be further improved. These will be discussed further in our next part where we will introduce personal preferences to influence the model output.","9d8051d8":"So we now have the values of each state action pair, we can use this to select the single next best step as being the action with the highest value given the current state."}}