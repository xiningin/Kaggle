{"cell_type":{"c5bf6f85":"code","9cc7ff0e":"code","3e285ba0":"code","2c9ce606":"code","cb31b76e":"code","aa9ea351":"code","e404f373":"code","a0edd73f":"code","915178e5":"code","df08ba88":"code","049c4094":"code","e00014f6":"code","bf20f727":"code","1858cfbd":"code","1490cb25":"code","d3561f1a":"code","50fe236e":"code","6130bb1c":"code","0825bfab":"code","6a4b8337":"code","27dc88d1":"code","f07dcfc7":"code","70142078":"code","f456ced0":"code","5a229119":"code","cf6d8f1f":"code","aaf82ee3":"code","633709e6":"code","f05ad8c4":"code","49129fda":"code","892db590":"code","652dd5ad":"code","b143fe97":"code","8c21e526":"code","e7ac938a":"code","82801116":"code","f348f530":"code","700d8f32":"code","9d263ea8":"code","80e038d9":"code","eb41124f":"code","0e7be801":"code","4fa3c280":"code","5614af2b":"code","b454290b":"code","b84f675c":"code","7f9a7a47":"code","d3069cf8":"code","d134bb53":"code","baaca3a8":"code","0c0e6301":"code","bb7ad19b":"markdown","8ac2b27a":"markdown","0fe00ff0":"markdown","ed412d28":"markdown","172e393d":"markdown","b8b192fc":"markdown","1a22a1e9":"markdown","2bea202d":"markdown","f83d2b44":"markdown","a918acd5":"markdown","751770e4":"markdown","99dd1a4a":"markdown","e8ef4c48":"markdown","07a93b18":"markdown","70f726af":"markdown","25e658e3":"markdown","802f35a8":"markdown","b5029947":"markdown","51da064f":"markdown","7dd6dac4":"markdown","9612db9a":"markdown","ffcf300b":"markdown","95f2d3fb":"markdown","c161d4f0":"markdown","803329e4":"markdown","ca10588e":"markdown"},"source":{"c5bf6f85":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9cc7ff0e":"# Packages \/ libraries\nimport matplotlib\nmatplotlib.rcParams['backend'] = 'module:\/\/ipykernel.pylab.backend_inline'\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, explained_variance_score, confusion_matrix, accuracy_score, classification_report, log_loss\nfrom math import sqrt\n#%matplotlib inline\n# To install sklearn type \"pip install numpy scipy scikit-learn\" to the anaconda termi\n# To change scientific numbers to float\nnp.set_printoptions(formatter={'float_kind':'{:f}'.format})\n# Increases the size of sns plots\nsns.set(rc={'figure.figsize':(12,10)})\n# import sys\n# !conda list Check the packages installed","3e285ba0":"df=pd.read_csv(\"..\/input\/hr-analytics\/HR_comma_sep.csv\")","2c9ce606":"df","cb31b76e":"df.describe()","aa9ea351":"df.shape","e404f373":"df.info()","a0edd73f":"df.isnull().sum()","915178e5":"df[\"Department\"].unique()","df08ba88":"df[\"salary\"].unique()","049c4094":"df.groupby('left').mean()","e00014f6":"pd.crosstab(df.salary,df.left).plot(kind='bar')","bf20f727":"pd.crosstab(df.Department,df.left).plot(kind='bar')","1858cfbd":"new_data=pd.get_dummies(df, columns = ['salary','Department'])\nnew_data","1490cb25":"print(df.shape)\nprint(new_data.shape)","d3561f1a":"# Split the data into X & y\n\nX = new_data.drop([\"left\"],axis=\"columns\")\nprint(X.shape)\n\ny=new_data.left\nprint(y.shape)\n\n# Making sure y as integer\ny = y.astype(int)","50fe236e":"dt = DecisionTreeClassifier(random_state=15, criterion = 'entropy', max_depth = 10)\ndt.fit(X,y)","6130bb1c":"# Running Feature Importance\nfi_col = []\nfi = []\n\nfor i,column in enumerate(new_data.drop([\"left\"],axis=\"columns\")):\n    print('The feature importance for {} is : {}'.format(column, dt.feature_importances_[i]))\n    \n    fi_col.append(column)\n    fi.append(dt.feature_importances_[i])","0825bfab":"# Creating a Dataframe\n# zip two list\n\nfi_df = zip(fi_col, fi)\nfi_df = pd.DataFrame(fi_df, columns = ['Feature','Feature Importance'])\nfi_df","6a4b8337":"# Ordering the data\nfi_df = fi_df.sort_values('Feature Importance', ascending = False).reset_index()\n\nfi_df\n","27dc88d1":"# Creating columns to keep\ncolumns_to_keep = fi_df['Feature'][0:13]\n\ncolumns_to_keep","f07dcfc7":"print(new_data.shape)\nprint(new_data[columns_to_keep].shape)\n","70142078":"X = new_data[columns_to_keep].values\nX","f456ced0":"y = new_data.left\ny = y.astype(int)\ny\n\nprint(X.shape)\nprint(y.shape)","5a229119":"# Hold-out validation\n\n# first one\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, test_size=0.2, random_state=15)\n\n# Second one(From Training)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, train_size = 0.9, test_size=0.1, random_state=15)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(X_valid.shape)\n\nprint(y_train.shape)\nprint(y_test.shape)\nprint(y_valid.shape)","cf6d8f1f":"# Investigating the distribution  of all y so that we can know is it balanced data or not\n\nax = sns.countplot(x =y_train, palette = \"Set3\")\n","aaf82ee3":"ax = sns.countplot(x =y_test, palette = \"Set3\")","633709e6":"ax = sns.countplot(x =y_valid, palette = \"Set3\")","f05ad8c4":"# Training my model\n\nlog_reg = LogisticRegression(random_state=10, solver = 'lbfgs')\n\nlog_reg.fit(X_train, y_train)","49129fda":"# Methods we can use in Logistic\n\n# predict - Predict class labels for samples in X\nlog_reg.predict(X_train)\ny_pred = log_reg.predict(X_train)\ny_pred","892db590":"# predict_proba - Probability estimates\npred_proba = log_reg.predict_proba(X_train)\npred_proba","652dd5ad":"# coef_ - Coefficient of the features in the decision function\nlog_reg.coef_\n\n# score- Returns the mean accuracy on the given test data and labels - below","b143fe97":"# Accuracy on Train\nprint(\"The Training Accuracy is: \", log_reg.score(X_train, y_train))\n\n# Accuracy on Test\nprint(\"The Testing Accuracy is: \", log_reg.score(X_test, y_test))","8c21e526":"# Classification Report\nprint(classification_report(y_train, y_pred))","e7ac938a":"# Confusion Matrix function\n\ndef plot_confusion_matrix(cm, classes=None, title='Confusion matrix'):\n    \"\"\"Plots a confusion matrix.\"\"\"\n    if classes is not None:\n        sns.heatmap(cm, cmap=\"YlGnBu\", xticklabels=classes, yticklabels=classes, vmin=0., vmax=1., annot=True, annot_kws={'size':50})\n    else:\n        sns.heatmap(cm, vmin=0., vmax=1.)\n    plt.title(title)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","82801116":"# Visualizing cm\n\ncm = confusion_matrix(y_train, y_pred)\ncm","f348f530":"cm.sum(axis=1)","700d8f32":"cm_norm = cm \/ cm.sum(axis=1).reshape(-1,1)\ncm_norm","9d263ea8":"# What are the classes\nlog_reg.classes_","80e038d9":"plot_confusion_matrix(cm_norm, classes = log_reg.classes_, title='Confusion matrix')","eb41124f":"# Calculating False Positives (FP), False Negatives (FN), True Positives (TP) & True Negatives (TN)\n\nFP = cm.sum(axis=0) - np.diag(cm)\nFN = cm.sum(axis=1) - np.diag(cm)\nTP = np.diag(cm)\nTN = cm.sum() - (FP + FN + TP)\n\n# Sensitivity, hit rate, recall, or true positive rate\nTPR = TP \/ (TP + FN)\nprint(\"The True Positive Rate is:\", TPR)\n\n# Precision or positive predictive value\nPPV = TP \/ (TP + FP)\nprint(\"The Precision is:\", PPV)\n\n# False positive rate or False alarm rate\nFPR = FP \/ (FP + TN)\nprint(\"The False positive rate is:\", FPR)\n\n\n# False negative rate or Miss Rate\nFNR = FN \/ (FN + TP)\nprint(\"The False Negative Rate is: \", FNR)\n\n\n\n##Total averages :\nprint(\"\")\nprint(\"The average TPR is:\", TPR.sum()\/2)\nprint(\"The average Precision is:\", PPV.sum()\/2)\nprint(\"The average False positive rate is:\", FPR.sum()\/2)\nprint(\"The average False Negative Rate is:\", FNR.sum()\/2)","0e7be801":"# Running Log loss on training\nprint(\"The Log Loss on Training is: \", log_loss(y_train, pred_proba))\n\n# Running Log loss on testing\npred_proba_test = log_reg.predict_proba(X_test)\nprint(\"The Log Loss on Testing Dataset is: \", log_loss(y_test, pred_proba_test))","4fa3c280":"np.geomspace(1e-5, 1e5, num=20)","5614af2b":"# Creating a range for C values\nnp.geomspace(1e-5, 1e5, num=20)\n\n# ploting it\nplt.plot(np.geomspace(1e-5, 1e5, num=20)) #  uniformly distributed in log space\nplt.plot(np.linspace(1e-5, 1e5, num=20)) # uniformly distributed in linear space, instead of log space","b454290b":"# Looping over the parameters\n\nC_List = np.geomspace(1e-5, 1e5, num=20)\nCA = []\nLogarithmic_Loss = []\n\nfor c in C_List:\n    log_reg2 = LogisticRegression(random_state=10, solver = 'lbfgs', C=c)\n    log_reg2.fit(X_train, y_train)\n    score = log_reg2.score(X_test, y_test)\n    CA.append(score)\n    print(\"The CA of C parameter {} is {}:\".format(c, score))\n    pred_proba_t = log_reg2.predict_proba(X_test)\n    log_loss2 = log_loss(y_test, pred_proba_t)\n    Logarithmic_Loss.append(log_loss2)\n    print(\"The Logg Loss of C parameter {} is {}:\".format(c, log_loss2))\n    print(\"\")","b84f675c":"# putting the outcomes in a Table\n\n# reshaping\nCA2 = np.array(CA).reshape(20,)\nLogarithmic_Loss2 = np.array(Logarithmic_Loss).reshape(20,)\n\n# zip\noutcomes = zip(C_List, CA2, Logarithmic_Loss2)\n\n#df\ndf_outcomes = pd.DataFrame(outcomes, columns = [\"C_List\", 'CA2','Logarithmic_Loss2'])\n\n#print\ndf_outcomes\n\n# Ordering the data (sort_values)\ndf_outcomes.sort_values(\"Logarithmic_Loss2\", ascending = True).reset_index()","7f9a7a47":"# Another way we can do it\n\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=3, random_state=0, shuffle=True)\n\n# Logistic Reg CV\nLog_reg3 = LogisticRegressionCV(random_state=15, Cs = C_List, solver ='lbfgs')\nLog_reg3.fit(X_train, y_train)\n\npred_proba_t = Log_reg3.predict_proba(X_test)\nlog_loss3 = log_loss(y_test, pred_proba_t)\n","d3069cf8":"print(\"The CA is:\", Log_reg3.score(X_test, y_test))\n\nprint(\"The Logistic Loss is: \", log_loss3)\n\nprint(\"The optimal C parameter is: \", Log_reg3.C_)","d134bb53":"# Maybe we have a different metric we want to track\n\n# Looping over the parameters\n\nC_List = np.geomspace(1e-5, 1e5, num=20)\nCA = []\nLogarithmic_Loss = []\n\nfor c in C_List:\n    log_reg2 = LogisticRegression(random_state=10, solver = 'lbfgs', C=c)\n    log_reg2.fit(X_train, y_train)\n    score = log_reg2.score(X_test, y_test)\n    CA.append(score)\n    print(\"The CA of C parameter {} is {}:\".format(c, score))\n    pred_proba_t = log_reg2.predict_proba(X_test)\n    log_loss2 = log_loss(y_test, pred_proba_t)\n    Logarithmic_Loss.append(log_loss2)\n    print(\"The Logg Loss of C parameter {} is {}:\".format(c, log_loss2))\n    print(\"\")\n    \n    y_pred = log_reg2.predict(X_train)\n    cm = confusion_matrix(y_train, y_pred)\n    cm_norm = cm \/ cm.sum(axis=1).reshape(-1,1)\n    plot_confusion_matrix(cm_norm, classes = log_reg.classes_, title='Confusion matrix')\n    plt.show()","baaca3a8":"# Training a Dummy Classifier\n\nfrom sklearn.dummy import DummyClassifier\n\ndummy_clf = DummyClassifier(strategy=\"most_frequent\")\ndummy_clf.fit(X_train, y_train)\nscore = dummy_clf.score(X_test, y_test)\n\npred_proba_t = dummy_clf.predict_proba(X_test)\nlog_loss2 = log_loss(y_test, pred_proba_t)\n\nprint(\"Testing Acc:\", score)\nprint(\"Log Loss:\", log_loss2)\n","0c0e6301":"# Final Model \n\nlog_reg3 = LogisticRegression(random_state=10, solver = 'lbfgs', C=784.759970)\nlog_reg3.fit(X_train, y_train)\nscore = log_reg3.score(X_valid, y_valid)\n\npred_proba_t = log_reg3.predict_proba(X_valid)\nlog_loss2 = log_loss(y_valid, pred_proba_t)\n\nprint(\"Testing Acc:\", score)\nprint(\"Log Loss:\", log_loss2)","bb7ad19b":"***0 means they retained, 1 means they left***\n\nfrom above data we can point out that\n1. If the **satisfication level** is *low* ,employee left.\n2. If the **average monthly hours** is High, they left.\n3. **Promotion_last_5years** : if employee doesn't get any promotion they left\n","8ac2b27a":"It doesn't look a balanced data","0fe00ff0":"# **Evaluating Model**","ed412d28":"# **Running Model**","172e393d":"**From the chart we can see that, most of the employee with high salary are retaining.**","b8b192fc":"## **Run a Tree-based estimators**","1a22a1e9":"# Making categorical variables into numeric representation","2bea202d":"**Impact of Department**","f83d2b44":"**False Positive rate is quite high**","a918acd5":"# **Hold out Validation**","751770e4":"# **Data Explotary Analysis**","99dd1a4a":"From above chart we are not sure how much impact on department","e8ef4c48":"**Notes: We test our data with totally unseen data ( Xvalid). & we have seen that log loss fell down**","07a93b18":" **Average  Numbers for column**","70f726af":"- We have predicted actual 0->93%, where 7.3% 0 we predicted as 1 \n\n- 0.68% predicted as 0 where actual is 1 and 32% is predicted correctly as 1","25e658e3":"# **Importing Libraries**","802f35a8":"## **Impact of salary**","b5029947":"# Feature Selection:\n\n- Steps of Running Feature Importance\n- Split the data into X & y\n- Run a Tree-based estimators (i.e. decision trees & random forests)\n- Run Feature Importance","51da064f":"**Notes:**\n\nWe calculate probability bcz we want to label class to be 0 or 1\n\nProbability>0.5 = 1\nProbability<0.5 = 0 First row, 0.63>0.5 so class is 0. check y_pred, it's 0","7dd6dac4":"**Now we have to calculate coeifficient**","9612db9a":"# Logarithmic loss - or Log Loss - or cross-entropy loss","ffcf300b":"**We want to choose lowest logarithmic and highest classification accuracy**","95f2d3fb":"**For training & testing both Log Loss same**","c161d4f0":"# Hyper Parameter Tuning","803329e4":"- We will loop over parameter C (Inverse of regularization strength).\n- Inverse of regularization strength helps to avoid overfitting - it penalizes large values of your parameters\n- It also helps to find Global Minimum by moving to better \"solutions\" from local minimum to global minimum\n- The values of C to search should be n-equally-spaced values in log space ranging from 1e-5 to 1e5","ca10588e":"**Notes: We want to import them as a dataframe and select the highest importance feature**"}}