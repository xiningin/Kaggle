{"cell_type":{"471ee66d":"code","85f5c15f":"code","f2b86490":"code","c5ecbb8c":"code","8802df60":"code","b42fd3cd":"code","1e2f1a93":"code","25c94078":"code","0810b1e8":"code","59fbe76d":"code","c67091bf":"code","77b3723b":"code","11c31bca":"code","b45b6126":"code","1e51afd4":"code","3fc6bc29":"code","5c562266":"code","730cfe0a":"code","84e7aff5":"code","fe016d08":"code","66d9cb46":"code","319c654b":"code","313b8125":"code","0ecfef75":"code","269cb08c":"code","340cd174":"markdown","632e8223":"markdown","402c6962":"markdown"},"source":{"471ee66d":"import numpy as np \nimport pandas as pd \nimport bz2\nimport gc\nimport chardet\nimport re\nimport os\nprint(os.listdir(\"..\/input\"))","85f5c15f":"from keras.models import Model, Sequential\nfrom keras.layers import Dense, Embedding, Input, Conv1D, GlobalMaxPool1D, Dropout, concatenate, Layer, InputSpec, CuDNNLSTM\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import backend as K\nfrom keras import activations, initializers, regularizers, constraints\nfrom keras.utils.conv_utils import conv_output_length\nfrom keras.regularizers import l2\nfrom keras.constraints import maxnorm","f2b86490":"train_file = bz2.BZ2File('..\/input\/amazonreviews\/train.ft.txt.bz2')\ntest_file = bz2.BZ2File('..\/input\/amazonreviews\/test.ft.txt.bz2')","c5ecbb8c":"train_file_lines = train_file.readlines()\ntest_file_lines = test_file.readlines()","8802df60":"del train_file, test_file","b42fd3cd":"train_file_lines = [x.decode('utf-8') for x in train_file_lines]\ntest_file_lines = [x.decode('utf-8') for x in test_file_lines]","1e2f1a93":"train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file_lines]\ntrain_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file_lines]\n\nfor i in range(len(train_sentences)):\n    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n    \ntest_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file_lines]\ntest_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file_lines]\n\nfor i in range(len(test_sentences)):\n    test_sentences[i] = re.sub('\\d','0',test_sentences[i])\n                                                       \nfor i in range(len(train_sentences)):\n    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n        \nfor i in range(len(test_sentences)):\n    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])","25c94078":"del train_file_lines, test_file_lines","0810b1e8":"gc.collect()","59fbe76d":"max_features = 20000\nmaxlen = 100","c67091bf":"tokenizer = text.Tokenizer(num_words=max_features)","77b3723b":"tokenizer.fit_on_texts(train_sentences)","11c31bca":"tokenized_train = tokenizer.texts_to_sequences(train_sentences)\nX_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)","b45b6126":"X_train[0]","1e51afd4":"tokenized_test = tokenizer.texts_to_sequences(test_sentences)\nX_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)","3fc6bc29":"EMBEDDING_FILE = '..\/input\/glovetwitter100d\/glove.twitter.27B.100d.txt'","5c562266":"def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))","730cfe0a":"all_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\n#change below line if computing normal stats is too slow\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size)) #embedding_matrix = np.zeros((nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","84e7aff5":"del tokenized_test, tokenized_train, tokenizer, train_sentences, test_sentences, word_index, embeddings_index, all_embs, nb_words\ngc.collect()","fe016d08":"batch_size = 2048\nepochs = 7\nembed_size = 100","66d9cb46":"gc.collect()","319c654b":"def cudnnlstm_model(conv_layers = 2, max_dilation_rate = 3):\n    inp = Input(shape=(maxlen, ))\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=True)(inp)\n    x = Dropout(0.25)(x)\n    x = Conv1D(2*embed_size, kernel_size = 3)(x)\n    prefilt = Conv1D(2*embed_size, kernel_size = 3)(x)\n    x = prefilt\n    for strides in [1, 1, 2]:\n        x = Conv1D(128*2**(strides), strides = strides, kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6), kernel_size=3, kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10))(x)\n    x_f = CuDNNLSTM(512, kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6), kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10))(x)  \n    x_b = CuDNNLSTM(512, kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6), kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10))(x)\n    x = concatenate([x_f, x_b])\n    x = Dropout(0.5)(x)\n    x = Dense(64, activation=\"relu\")(x)\n    x = Dropout(0.1)(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['binary_accuracy'])\n\n    return model\n\ncudnnlstm_model = cudnnlstm_model()\ncudnnlstm_model.summary()","313b8125":"weight_path=\"early_weights.hdf5\"\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nearly_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\ncallbacks = [checkpoint, early_stopping]","0ecfef75":"cudnnlstm_model.fit(X_train, train_labels, batch_size=batch_size, epochs=epochs, shuffle = True, validation_split=0.20, callbacks=callbacks)","269cb08c":"cudnnlstm_model.load_weights(weight_path)\nscore, acc = cudnnlstm_model.evaluate(X_test, test_labels, batch_size=batch_size)\nprint('Test score:', score)\nprint('Test accuracy:', acc)","340cd174":"# Convert from raw binary strings to strings that can be parsed","632e8223":"# Create Lists containing Train & Test sentences","402c6962":"# Read Train & Test Files"}}