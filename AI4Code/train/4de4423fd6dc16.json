{"cell_type":{"a939b975":"code","efebcdd0":"code","e368d180":"code","1011466a":"code","c3f6c6f0":"code","e7ceee2a":"code","15e1d335":"code","80ab9d58":"code","fa67ba7a":"code","161ad385":"code","4cb48ecb":"code","d9ff54a8":"code","8058457e":"code","c42323d9":"code","347afeb2":"code","eab9b273":"code","08a029f9":"code","393d8d52":"code","51803746":"code","01414924":"code","f19af78d":"code","3d849d50":"code","4f585176":"code","5ebb523f":"code","12455141":"code","8d508501":"code","18cfb776":"code","1f0fc565":"code","e667be9b":"code","7a821a29":"code","e6677145":"code","40d37b94":"code","268308de":"code","b72558b8":"code","ef18da48":"code","b7fca7d1":"code","3e1b794e":"code","fa1e1e3e":"code","885531a6":"markdown","fed84ee5":"markdown","729e5c95":"markdown","7a0a5fdf":"markdown","e39d8321":"markdown","0b879f5f":"markdown","1d5b4299":"markdown","a8bbf465":"markdown","2e4f26db":"markdown","636c0845":"markdown","30ac12d3":"markdown","4fdcaf07":"markdown","8be4bece":"markdown","937fd5de":"markdown","3b6d1324":"markdown","0bb7c5ca":"markdown","c2b1542a":"markdown","7c316497":"markdown","b7706f83":"markdown"},"source":{"a939b975":"# First, let's import the necessary stuff and load our training and test data\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport scipy\nsns.set()\nimport matplotlib.pyplot as plt\n%matplotlib inline","efebcdd0":"# suppressing all warnings for readability (I wouldn't do this unless I'm really really sure...)\nimport warnings\nwarnings.filterwarnings(\"ignore\")","e368d180":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","1011466a":"train.info()","c3f6c6f0":"# firstly, we do not need the ID column in both the train and test datasets so we'll go ahead and drop them\ntrain.drop('Id', axis = 1, inplace = True)\ntest_ids = test['Id']\ntest.drop('Id', axis = 1, inplace = True)","e7ceee2a":"# after studying the dataset, it is clear that there are quite a few outliers in several of the predictors\n# but given that we only have 1460 rows to play with, it doesn't make sense to remove all of them\n# So, for now, I am only removing the outliers from the 'ground living area' predictor\n\nfig, axes = plt.subplots(nrows = 1, ncols = 2)\nsns.scatterplot(x = train.GrLivArea, y = train.SalePrice, data = train, ax = axes[0])\naxes[0].set_xlim(0, 4000)\naxes[0].set_ylim(0, 650000)\naxes[0].set_title(\"Before dropping outliers\")\ntrain = train[~(np.abs(train.GrLivArea - train.GrLivArea.mean()) > (3.5 * train.GrLivArea.std()))]\nprint(f\"Updated shape of the dataset: {train.shape}\")\nsns.scatterplot(x = train.GrLivArea, y = train.SalePrice, data = train, ax = axes[1], color = \"brown\")\naxes[1].set_xlim(0, 4000)\naxes[1].set_ylim(0, 650000)\naxes[1].set_title(\"After dropping outliers\")\nplt.tight_layout()","15e1d335":"# let's study the distribution of the response\nsns.distplot(train.SalePrice)\n\n# qq-plot\nfig = plt.figure()\nres = scipy.stats.probplot(train['SalePrice'], plot = plt)\nplt.show()","80ab9d58":"# we see that the response data is skewed\n# it would be preferable to have normality in our data, so let's do that first\n# for this purpose, a natural log of the data seems to be doing the trick\n\ntrain['SalePrice'] = np.log(train['SalePrice'])\nfig = plt.figure()\nres = scipy.stats.probplot(train['SalePrice'], plot = plt)\nplt.show()","fa67ba7a":"# to aid is better in this process, we would be combining the train and test data\n# this would prove beneficial in some cases. For instance, when we are imputing missing data\n\nntrain = len(train)\ny = train.SalePrice\ncombined = pd.concat([train, test], ignore_index = True)\ncombined.drop(columns='SalePrice', inplace=True)\nprint(f\"Shape of combined data: {combined.shape}\")","161ad385":"# let's analyze the missing data now\n(combined.isna().sum().sort_values(ascending = False) \/ len(combined)) * 100","4cb48ecb":"# we see that PoolQC, MiscFeature, Alley, Fence, FireplaceQu have a substantial number of missing values\n# it doesn't make sense to try to impute\/fill them as it'd be an inaccurate representation anyways\n# so let's go ahead and drop them\n\ncombined.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu'], inplace = True, axis = 1)\nprint(f\"Update shape of the combined dataset: {combined.shape}\")","d9ff54a8":"# for lot frontage, since we can roughly expect streets in a neighbourhood to have similar lot areas\n# we can impute the missing values based on this grouping\n\ncombined.LotFrontage = combined.groupby('Neighborhood').LotFrontage.apply(lambda x: x.fillna(x.median()))","8058457e":"# the following will be replaced with \"None\"\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'MSSubClass', 'MasVnrType', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    combined[col] = combined[col].fillna('None')","c42323d9":"# the following will be replaced with a zero\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea'):\n    combined[col] = combined[col].fillna(0)","347afeb2":"# the following categorical predictors will be replaced by the mode (most frequently occuring class)\ncombined['MSZoning'] = combined['MSZoning'].fillna(combined['MSZoning'].mode()[0])\ncombined['Electrical'] = combined['Electrical'].fillna(combined['Electrical'].mode()[0])\ncombined['KitchenQual'] = combined['KitchenQual'].fillna(combined['KitchenQual'].mode()[0])\ncombined['Exterior1st'] = combined['Exterior1st'].fillna(combined['Exterior1st'].mode()[0])\ncombined['Exterior2nd'] = combined['Exterior2nd'].fillna(combined['Exterior2nd'].mode()[0])\ncombined['SaleType'] = combined['SaleType'].fillna(combined['SaleType'].mode()[0])","eab9b273":"# for function, the data description says NA means 'typical'\ncombined[\"Functional\"] = combined[\"Functional\"].fillna(\"Typical\")\n\n# and finally, for Utilities, we might as well get rid of it as almost all of the entries have the same value for it\n# so it won't really aid us in the modeling process\ncombined.drop('Utilities', axis = 1, inplace = True)\nprint(f\"The final dimensions of the combined data {combined.shape}\")","08a029f9":"# let's now transform some of the numerical variables that are really categorical\ncombined['MSSubClass'] = combined['MSSubClass'].apply(str)\ncombined['OverallCond'] = combined['OverallCond'].astype(str)","393d8d52":"# given how important the total sq. footage is in determining a house price, we are creating a TotalSF predictor\ncombined['TotalSF'] = combined['TotalBsmtSF'] + combined['1stFlrSF'] + combined['2ndFlrSF']","51803746":"# let's check for skewness in our numerical data\nnumeric_feats = combined.dtypes[combined.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = combined[numeric_feats].apply(lambda x: x.skew()).sort_values(ascending = False)\nskewness = pd.DataFrame({'Skew': skewed_feats})\nskewness","01414924":"# with the above info, let's perform a box-cox transformation to bring data as close as possible to a gaussian distribution\nfrom scipy.special import boxcox1p\n\nskewness = skewness[abs(skewness) > 0.75]\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    combined[feat] = boxcox1p(combined[feat], lam)","f19af78d":"# finally, let's assign dummy variables to our categorical data (algorithms like linear regression need this)\ncombined = pd.get_dummies(combined)\ncombined.shape","3d849d50":"# let's split our datasets back to training and testing\ntrain = combined[:ntrain]\ntest = combined[ntrain:]","4f585176":"# so we have the predictors in 'train' and the response in 'y'\n# since we've already taken log of the response, it would suffice if we just calculated the RMSE\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\ny.reset_index(drop = True, inplace = True)\n\ndef cv_rmsle(model):\n    cv_score_array = np.sqrt(-cross_val_score(model, train, y, cv = 5, scoring = \"neg_mean_squared_error\"))\n    return cv_score_array.mean()","5ebb523f":"from sklearn.linear_model import LinearRegression\n\nlm_rmsle = cv_rmsle(LinearRegression())\nprint(f\"RMSLE for Linear Regression: [{lm_rmsle}]\")","12455141":"# for LASSO, we first need to determine the value of alpha\n# we will use LassoCV for that purpose\n\nfrom sklearn.linear_model import Lasso, LassoCV\n\nlassocv_model = LassoCV(cv = 5, random_state = 1)\nlassocv_model.fit(train, y)\nbest_alpha = lassocv_model.alpha_\n\nlasso_model = make_pipeline(RobustScaler(), Lasso(alpha = best_alpha, random_state = 1))\nlasso_rmsle = cv_rmsle(lasso_model)\nprint(f\"RMSLE for LASSO (L1 Regularization): [{lasso_rmsle}]\")","8d508501":"from sklearn.linear_model import RidgeCV, Ridge\n\nridgecv_model = make_pipeline(RobustScaler(), RidgeCV(alphas = np.logspace(-10, 10, 100)))\nridge_rmsle = cv_rmsle(ridgecv_model)\nprint(f\"RMSLE for Ridge Regression (L2 Regularization): [{ridge_rmsle}]\")","18cfb776":"from sklearn.linear_model import ElasticNet, ElasticNetCV\n\nenetcv_model = ElasticNetCV(l1_ratio = np.arange(0.1, 1, 0.1), cv = 5, random_state = 1)\nenetcv_model.fit(train, y)\nbest_l1_ratio = enetcv_model.l1_ratio_\nbest_alpha = enetcv_model.alpha_\n\nenet_model = make_pipeline(RobustScaler(), ElasticNet(alpha = best_alpha, l1_ratio = best_l1_ratio, random_state = 1))\nenet_rmsle = cv_rmsle(enet_model)\nprint(f\"RMSLE for Elastic Net (L1 and L2 Regularization): [{enet_rmsle}]\")","1f0fc565":"from sklearn.ensemble import GradientBoostingRegressor\n\n# after much trial and error, I arrived at the hyperparameters used in gradient boosting\ngboost_model = GradientBoostingRegressor(loss = 'huber', learning_rate = 0.1, n_estimators = 3000, max_depth = 1, random_state = 1)\ngboost_rmsle = cv_rmsle(gboost_model)\nprint(f\"RMSLE for Gradient Boosting: [{gboost_rmsle}]\")","e667be9b":"from sklearn.ensemble import AdaBoostRegressor\n\n# after much trial and error, I arrived at the hyperparameters used in adaptive boosting\nadaboost_lasso_model = AdaBoostRegressor(lasso_model, n_estimators = 50, learning_rate = 0.001, random_state = 1)\nadaboost_lasso_rmsle = cv_rmsle(adaboost_lasso_model)\nprint(f\"RMSLE for ADA Boosting (with LASSO): [{adaboost_lasso_rmsle}]\")","7a821a29":"from sklearn.ensemble import AdaBoostRegressor\n\n# after much trial and error, I arrived at the hyperparameters used in adaptive boosting\nadaboost_enet_model = AdaBoostRegressor(enet_model, n_estimators = 50, learning_rate = 0.001, random_state = 1)\nadaboost_enet_rmsle = cv_rmsle(adaboost_enet_model)\nprint(f\"RMSLE for ADA Boosting (with Elastic Net): [{adaboost_enet_rmsle}]\")","e6677145":"from sklearn.ensemble import RandomForestRegressor\n\nrforest_rmsle = cv_rmsle(RandomForestRegressor(random_state = 1))\nprint(f\"RMSLE for Random Forests: [{rforest_rmsle}]\")","40d37b94":"from sklearn.ensemble import ExtraTreesRegressor\n\netrees_rmsle = cv_rmsle(ExtraTreesRegressor(random_state = 1))\nprint(f\"RMSLE for extremely randomized trees: [{etrees_rmsle}]\")","268308de":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\n\npca = make_pipeline(RobustScaler(), PCA(n_components = 3, random_state = 1))\npca.fit(train.transpose())\nprint(f\"Proportion of variance explained by the components: {pca.steps[1][1].explained_variance_ratio_}\")\n\n# we are using 3 components in this case\np_comps = pca.steps[1][1].components_.transpose()\n\npca_lm_rmsle = np.sqrt(-cross_val_score(LinearRegression(), p_comps, y, cv = 5, scoring = \"neg_mean_squared_error\")).mean()\nprint(f\"RMSLE for Linear Regression after PCA reduction: [{pca_lm_rmsle}]\")","b72558b8":"from sklearn.model_selection import cross_val_predict\n\n# we also need to define a function that returns cross-validated predictions for the data\ndef cv_pred(model):\n    return cross_val_predict(model, train, y, cv = 5)","ef18da48":"adaboost_enet_pred = cv_pred(adaboost_enet_model)\nadaboost_lasso_pred = cv_pred(adaboost_lasso_model)\nlasso_pred = cv_pred(lasso_model)","b7fca7d1":"# now let's take the exponential of the responses to bring them back to their original scale\nadaboost_enet_pred = np.exp(adaboost_enet_pred)\nadaboost_lasso_pred = np.exp(adaboost_lasso_pred)\nlasso_pred = np.exp(lasso_pred)","3e1b794e":"from sklearn.metrics import mean_squared_log_error\ny_actual = np.exp(y)\n\nrmsle_summary = pd.DataFrame(columns = ['ADA_ENet', 'ADA_Lasso', 'Lasso', 'RMSLE'])\nrmsle_summary = rmsle_summary.append({'ADA_ENet':1, 'ADA_Lasso':0, 'Lasso':0, 'RMSLE':adaboost_enet_rmsle}, ignore_index = True)\nrmsle_summary = rmsle_summary.append({'ADA_ENet':0, 'ADA_Lasso':1, 'Lasso':0, 'RMSLE':adaboost_lasso_rmsle}, ignore_index = True)\nrmsle_summary = rmsle_summary.append({'ADA_ENet':0, 'ADA_Lasso':0, 'Lasso':1, 'RMSLE':lasso_rmsle}, ignore_index = True)\n\nfor i in np.arange(0.1, 0.9, 0.1).tolist():\n    for j in np.arange(0.1, 1 - i, 0.1).tolist():\n            final_preds = round(i, 1)*adaboost_enet_pred + round(j, 1)*adaboost_lasso_pred + round(1 - (i + j), 1)*lasso_pred\n            rmsle = np.sqrt(mean_squared_log_error(y_actual, final_preds))\n            rmsle_summary = rmsle_summary.append({'ADA_ENet':round(i, 1), 'ADA_Lasso':round(j, 1), 'Lasso':round(1 - (i + j), 1), 'RMSLE':rmsle}, ignore_index = True)\n            \nprint(rmsle_summary)","fa1e1e3e":"adaboost_lasso_model.fit(train, y_actual)\nsubmission_preds = adaboost_lasso_model.predict(test)\n\nresults = pd.DataFrame({'Id':test_ids, 'SalePrice':submission_preds})\nresults.to_csv(\"submission.csv\", index = False)","885531a6":"### PCA\n\n### Let's see if reducing dimensionality has a positive effect on the predictions","fed84ee5":"So PCA reduction didn't really help our case. So we are now done with all our models so it's time to have some fun.\n\nLet's try combining some of these models and see if we get better results","729e5c95":"### Ridge Regression","7a0a5fdf":"### Moving on to ensemble methods..\n\n### Gradient Boosting","e39d8321":"### Adaptive Boosting (with Elastic Net as the estimator)","0b879f5f":"This is one of the first few Kaggle competitions I've participated in so it's quite special to me as it helped kick off my data science journey. I've taken inspiration from the following awesome notebook(s):-  \n\n- [Stacked Regressions to predict House Prices](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\/notebook) by **Serigne**","1d5b4299":"### Linear Regression","a8bbf465":"Ahh... all that effort and we still see that ADA boosting with LASSO gives us the best results!\n\n### Submission time!","2e4f26db":"### Adaptive Boosting (with LASSO as the estimator)","636c0845":"## Exploratory Data Analysis","30ac12d3":"## Now, we can finally start modeling","4fdcaf07":"## Aaanddd, that's a goodbye to missing values!\n\n### Now, let's do further feature engineering","8be4bece":"We will try out various combinations of our top 3 models which are:-\n1. Adaptive boosting with LASSO (`adaboost_lasso_model`)\n2. Adaptive boosting with Elastic Net (`adaboost_enet_model`)\n3. LASSO (`lasso_model`)","937fd5de":"### LASSO","3b6d1324":"### Extremely randomized trees","0bb7c5ca":"### Elastic Net","c2b1542a":"## Now, we'll analyze the response variable","7c316497":"## Time for feature engineering","b7706f83":"### Random Forests"}}