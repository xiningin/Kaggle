{"cell_type":{"e586d279":"code","4194faac":"code","dc47c3d9":"code","d2122dd3":"code","892aa9d6":"code","95e068f7":"code","7abc764b":"code","7e318f6c":"code","bd88f682":"code","769f7f24":"code","22327c1e":"code","58b2c770":"code","38af9f15":"code","09502825":"code","3df10c2f":"code","f274d7c3":"code","681955de":"code","ffbf5ebd":"code","9038e803":"code","06412a02":"code","8e840285":"code","282b0236":"code","393676ea":"code","5807173d":"code","00c456ac":"code","15ef5221":"code","f31b31cc":"code","b0df0408":"code","da094705":"code","456e07d7":"code","eee29d2e":"code","2543931e":"code","dbe297f2":"code","24939501":"code","8b6f9d92":"code","6e1ef2b5":"code","ff3f889a":"code","ec624f6e":"code","0f96d639":"code","48e066ee":"code","f7589d18":"code","a067a950":"code","ced0264d":"code","b21218c1":"code","a6125544":"code","89f08a8a":"code","f310cf1f":"code","7ed3413a":"code","36de498b":"code","af8a6e2b":"code","7109c41e":"code","9de04499":"code","ec80b0b8":"code","54e93710":"code","e05d2b02":"code","8ad4143e":"code","85c02db1":"code","e9be625b":"code","c59a0232":"code","e39b1325":"code","e8bfc3a2":"code","c688388a":"code","c32723fb":"code","858a83f8":"code","3d9af240":"code","900fa4f6":"code","c7ae13e2":"code","3a01e875":"code","e43f20fb":"code","871752a6":"code","12ac48d7":"code","c1cdbdc3":"code","74d78580":"code","b7ce7a5a":"code","502e16f0":"code","b007bb21":"code","9be73743":"code","1994ca85":"code","aac3c468":"code","60af7c8f":"code","f7e924b6":"code","225f54ae":"code","3ac426aa":"code","9e66d718":"code","b81ae227":"code","95213870":"code","bdc1e4f1":"code","7a069ae0":"code","a2e5ae22":"code","8ac039b5":"code","528909a3":"code","557d1d1f":"code","fcd7d9a6":"code","efaa8ee9":"code","0a30ea69":"code","261d0717":"code","976c1387":"code","deaada74":"code","d453967c":"code","931becf3":"code","6c87b512":"code","0f2670c7":"code","99ec1554":"code","755ad37d":"code","cc0cc8db":"code","ed0bdf6f":"code","74c7f709":"markdown","a09ef9ae":"markdown","458c2f28":"markdown","099c2e55":"markdown","68b74932":"markdown","ea1d309f":"markdown","501afef1":"markdown","05fed585":"markdown","e5af582f":"markdown","2a422bb0":"markdown","496ec1ee":"markdown","ca2a7826":"markdown","1eb58a91":"markdown","8b5fc9ac":"markdown","961bb32b":"markdown","64b99efb":"markdown","faeb116c":"markdown","5350671a":"markdown","568d49ab":"markdown","36bbe653":"markdown","5a3e52d5":"markdown","e6275a4c":"markdown","3efe67da":"markdown","7494e2c5":"markdown","e05f3fec":"markdown","c4a5e641":"markdown","d5fc3ead":"markdown","65c78cc3":"markdown","40a25186":"markdown","5f934c81":"markdown","a5b75670":"markdown","0db7a1ac":"markdown","d5ca754d":"markdown","6c63bae6":"markdown","3e0f49ed":"markdown","340411e0":"markdown","e2342758":"markdown","7331b18b":"markdown","23203853":"markdown","df31d537":"markdown","e2d9a1c0":"markdown","780f9c2c":"markdown","33d8e220":"markdown","3c036966":"markdown","d2e314c1":"markdown","c8c75723":"markdown","a8952736":"markdown","9939f7f4":"markdown","524a5b22":"markdown","40e51276":"markdown","f0c18dee":"markdown","ed6598b3":"markdown","52548d3b":"markdown","8e3e8052":"markdown","0a1db9c4":"markdown","5d0ff28b":"markdown","3e826565":"markdown","d86e4be4":"markdown","cd91bb6a":"markdown","181478bb":"markdown","a5ec703a":"markdown","b8c3b203":"markdown","10e81d25":"markdown","dcb95efb":"markdown","37219fef":"markdown","bc3da39b":"markdown","2f8bab1b":"markdown","df8a0354":"markdown","02269c1e":"markdown","f6fed02c":"markdown","80cfec80":"markdown","a9e5b2c6":"markdown"},"source":{"e586d279":"'''Importing Data Manipulattion Modules'''\nimport numpy as np\nimport pandas as pd\nfrom pandas.core.dtypes.dtypes import CategoricalDtype\nfrom scipy.stats import norm, skew\nfrom scipy.special import boxcox1p\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x))\npd.set_option(\"display.max_columns\", 81)\npd.set_option(\"display.max_rows\", 101)\npd.set_option(\"display.max_colwidth\", 100)\n\n'''Seaborn and Matplotlib Visualization'''\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('bmh')                    \nsns.set_style({'axes.grid':False}) \nsns.set_style('darkgrid')\n%matplotlib inline\n\n'''Validation'''\nfrom sklearn.model_selection import KFold, cross_val_score\n\n'''Ignore deprecation and future, and user warnings.'''\nimport warnings as wrn\nwrn.filterwarnings('ignore', category = DeprecationWarning) \nwrn.filterwarnings('ignore', category = FutureWarning) \nwrn.filterwarnings('ignore', category = UserWarning) ","4194faac":"'''Check the files'''\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\/house-prices-advanced-regression-techniques\/\"]).decode(\"utf8\"))","dc47c3d9":"'''Read in train and test data from csv files'''\ntrain_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","d2122dd3":"''' check the numbers of samples and features '''\nprint(\"The train data size is: {}\".format(train_df.shape))\nprint(\"The test data size is:  {}\".format(test_df.shape))","892aa9d6":"'''Train and test data at a glance'''\ntrain_df.head(5)","95e068f7":"test_df.head(5)","7abc764b":"'''Check the coloumn names. The column SalePrice is the target here'''\ntrain_df.columns.values","7e318f6c":"'''Save the Id column'''\ntrain_Id = train_df['Id']\ntest_Id = test_df['Id']","bd88f682":"'''Merge the train and test data'''\nall_df = pd.concat((train_df, test_df)).reset_index(drop=True)\nall_df.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"The all data size is : {}\".format(all_df.shape))","769f7f24":"'''KFold for cross validation'''\nkf = KFold(n_splits=5, shuffle=True, random_state=2)\n\n'''Define the validation function'''\ndef rmsle_cv(model, X, y, cv=kf):\n    rmsle = np.sqrt(\n        -cross_val_score(\n            model,\n            X, y,\n            scoring=\"neg_mean_squared_log_error\",\n            cv=cv,\n        )\n    )\n    return(rmsle)","22327c1e":"'''Count the missing data'''\nna_count_all_df = all_df.isnull().sum()\nna_count_all_df = na_count_all_df.drop(na_count_all_df[na_count_all_df == 0].index)\nna_ratio_all_df = na_count_all_df \/ len(all_df)\nmissing_data = pd.DataFrame({\n    'All Missing Ratio': na_ratio_all_df, \n    'All Missing Count': na_count_all_df, \n    'Train Missing Count': train_df[na_count_all_df.index].isnull().sum(),\n    'Test Missing Count': test_df[na_count_all_df.index].isnull().sum(),\n    'All Data Type': all_df[na_count_all_df.index].dtypes,\n    'Train Data Type': train_df[na_count_all_df.index].dtypes,\n    'Test Data Type': test_df[na_count_all_df.index].dtypes,\n}).sort_values('All Missing Count', ascending=False)\nmissing_data.head(100)","58b2c770":"all_df['PoolQC'] = all_df['PoolQC'].fillna('None')","38af9f15":"all_df['MiscFeature'] = all_df['MiscFeature'].fillna('None')","09502825":"all_df['Alley'] = all_df['Alley'].fillna('None')","3df10c2f":"all_df['Fence'] = all_df['Fence'].fillna('None')","f274d7c3":"all_df['FireplaceQu'] = all_df['FireplaceQu'].fillna('None')","681955de":"# Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_df['LotFrontage'] = all_df.groupby('Neighborhood')['LotFrontage'].transform(\n    lambda x: x.fillna(x.median()))","ffbf5ebd":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_df[col] = all_df[col].fillna('None')","9038e803":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_df[col] = all_df[col].fillna(0)","06412a02":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_df[col] = all_df[col].fillna(0)","8e840285":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_df[col] = all_df[col].fillna('None')","282b0236":"all_df['MasVnrType'] = all_df['MasVnrType'].fillna('None')\nall_df['MasVnrArea'] = all_df['MasVnrArea'].fillna(0)","393676ea":"all_df['MSZoning'] = all_df['MSZoning'].fillna(all_df['MSZoning'].mode()[0])","5807173d":"all_df['Utilities'] = all_df['Utilities'].fillna(all_df['Utilities'].mode()[0])","00c456ac":"all_df['Functional'] = all_df['Functional'].fillna('Typ')","15ef5221":"all_df['Electrical'] = all_df['Electrical'].fillna(all_df['Electrical'].mode()[0])","f31b31cc":"all_df['KitchenQual'] = all_df['KitchenQual'].fillna(all_df['KitchenQual'].mode()[0])","b0df0408":"all_df['Exterior1st'] = all_df['Exterior1st'].fillna(all_df['Exterior1st'].mode()[0])\nall_df['Exterior2nd'] = all_df['Exterior2nd'].fillna(all_df['Exterior2nd'].mode()[0])","da094705":"all_df['SaleType'] = all_df['SaleType'].fillna(all_df['SaleType'].mode()[0])","456e07d7":"all_df['MSSubClass'] = all_df['MSSubClass'].fillna('None')","eee29d2e":"'''There remains no missing data'''\n(all_df.isnull().sum() == 0).all()","2543931e":"'''Transform data type fload64 to int64'''\nfor col in all_df.select_dtypes(include=['float64']).columns:\n    all_df[col] = all_df[col].astype('int64')","dbe297f2":"'''Transform data type object to category'''\nfor col in all_df.select_dtypes(include=['object']).columns:\n    all_df[col] = all_df[col].astype('category')","24939501":"'''Transform specific variables'''\n# MSSubClass=The building class\nall_df['MSSubClass'] = all_df['MSSubClass'].astype('category')\n\n# Year and month sold are transformed into categorical features.\nall_df['YrSold'] = all_df['YrSold'].astype('category')\nall_df['MoSold'] = all_df['MoSold'].astype('category')","8b6f9d92":"'''Splitting the data back to train and test'''\ntrain_df = pd.merge(all_df, train_df[['Id', 'SalePrice']], how='inner', on='Id')\ntest_df = all_df[all_df.Id.isin(test_Id)]","6e1ef2b5":"'''Comprehend data types'''\nfrom collections import defaultdict\nd = defaultdict(int)\n\ndf = pd.DataFrame()\n\nfor col, dtype in zip(all_df.columns, all_df.dtypes.values):\n    description = ''\n    if type(dtype) == CategoricalDtype:\n        description = dtype.categories\n        dtype = 'category'\n    df = df.append({'column': col, 'dtype': dtype, 'description': description}, ignore_index=True)\n    d[dtype] += 1\nprint(d)\ndf.set_index('column')[['dtype', 'description']]","ff3f889a":"'''Check the categorical variables'''\nnum_uniq = all_df.select_dtypes(include=['category']).nunique().sort_values()\nindex = num_uniq.index\nnum_uniq","ec624f6e":"all_df.loc[:, index].head(5)","0f96d639":"'''Check the numerical variables'''\nnum_uniq = all_df.select_dtypes(include=['int64']).nunique().sort_values()\nindex = num_uniq.index\nnum_uniq","48e066ee":"all_df.loc[:, index].head(5)","f7589d18":"'''Plot histogram of numerical variables to validate pandas intuition.'''\ndef draw_histograms(df, variables, n_rows, n_cols, size):\n    fig=plt.figure()\n    for i, var_name in enumerate(variables):\n        ax=fig.add_subplot(n_rows, n_cols, i+1)\n        df[var_name].hist(bins=40, ax=ax, color='skyblue', alpha=0.8, figsize=size)\n        ax.set_title(var_name, fontsize=43)\n        ax.tick_params(axis='both', which='major', labelsize=35)\n        ax.tick_params(axis='both', which='minor', labelsize=35)\n        ax.set_xlabel('')\n    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n    plt.show()\n    \n# draw_histograms(all_df, index, 17, 2, (40, 200))","a067a950":"'''Descriptive statistics summary'''\ntrain_df['SalePrice'].describe()","ced0264d":"'''Plot histgram'''\nplt.figure(figsize=(16, 8))\nsns.distplot(train_df['SalePrice']);","b21218c1":"'''Skewness and kurtosis'''\nprint(\"Skewness: %f\" % train_df['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train_df['SalePrice'].kurt())","a6125544":"'''saleprice correlation matrix'''\nplt.subplots(figsize=(20, 16))\nk = 80 #number of variables for heatmap\ncorrmat = train_df.corr()\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\n\ncm = np.corrcoef(train_df[cols].values.T)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True,\n                 fmt='.2f', annot_kws={'size': 10}, cmap='Blues',\n                 yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","89f08a8a":"'''Check feature inportance by applying LightGBM'''\nimport lightgbm as lgb\nmodel_lgb = lgb.LGBMRegressor(num_leaves=10,\n                              max_depth=5,\n                              learning_rate=0.1,\n                              random_state=2)\nmodel_lgb.fit(train_df.drop(['Id', 'SalePrice'], axis=1), train_df.SalePrice)","f310cf1f":"rmsle_cv(model_lgb, train_df.drop('SalePrice', axis=1), train_df.SalePrice)","7ed3413a":"df = pd.DataFrame(model_lgb.feature_importances_,\n             index=train_df.drop(['Id', 'SalePrice'], axis=1).columns,\n             columns=['importance']).sort_values('importance', ascending=False)\ndf[df.importance > 10]","36de498b":"'''Analyze relationship between important variables and SalePrice'''\nimp_index = pd.Index([\n    'GrLivArea',\n    'TotalBsmtSF',\n    'LotArea',\n    'Neighborhood',\n    'OverallQual',\n    'OverallCond',\n    'GarageCars',\n    'MasVnrArea',\n    'YearBuilt',\n    'KitchenQual',\n    'YearRemodAdd',\n    'SalePrice',\n    ])","af8a6e2b":"train_df.loc[:, imp_index].dtypes","7109c41e":"'''Important variable\\'s cardinality'''\ntrain_df.loc[:, imp_index].nunique()","9de04499":"'''Plot histogram of important variables'''\ndraw_histograms(train_df, \n                train_df.loc[:, imp_index].select_dtypes(include=['int64']).columns,\n                5, 2, (40, 50))","ec80b0b8":"'''Plot appropriate graphs to comprehend the relationship'''\n# GrLiveArea\nplt.figure(figsize=(16, 8))\nsns.scatterplot(x='GrLivArea', y=\"SalePrice\", data=train_df, palette='Blues_d')","54e93710":"# TotalBsmtSF\nplt.figure(figsize=(16, 8))\nsns.scatterplot(x='TotalBsmtSF', y=\"SalePrice\", data=train_df, palette='Blues_d')","e05d2b02":"# LotArea\nplt.figure(figsize=(16, 8))\nsns.scatterplot(x='LotArea', y=\"SalePrice\", data=train_df, palette='Blues_d')","8ad4143e":"# Neighborhood\nmedian_order = train_df.groupby('Neighborhood')['SalePrice'].median().sort_values().index\nplt.figure(figsize=(16, 8))\nsns.boxplot(x='Neighborhood', y=\"SalePrice\", data=train_df, palette='Blues_d',\n            order=median_order)","85c02db1":"# OverallQual\nplt.figure(figsize=(16, 8))\nsns.boxplot(x='OverallQual', y=\"SalePrice\", data=train_df, palette='Blues_d')","e9be625b":"# OverallCond\nplt.figure(figsize=(16, 8))\nsns.boxplot(x='OverallCond', y=\"SalePrice\", data=train_df, palette='Blues_d')","c59a0232":"# GarageCars\nplt.figure(figsize=(16, 8))\nsns.boxplot(x='GarageCars', y=\"SalePrice\", data=train_df, palette='Blues_d')","e39b1325":"# MasVnrArea\nplt.figure(figsize=(16, 8))\nsns.scatterplot(x='MasVnrArea', y=\"SalePrice\", data=train_df, palette='Blues_d')","e8bfc3a2":"# KitchenQual\nplt.figure(figsize=(16, 8))\nsns.boxplot(x='KitchenQual', y=\"SalePrice\", data=train_df, palette='Blues_d',\n           order=['Fa', 'TA', 'Gd', 'Ex'])","c688388a":"# YearBuilt\nplt.figure(figsize=(16, 8))\nsns.scatterplot(x='YearBuilt', y=\"SalePrice\", data=train_df, palette='Blues_d')","c32723fb":"# YearRemodAdd\nplt.figure(figsize=(16, 8))\nsns.scatterplot(x='YearRemodAdd', y=\"SalePrice\", data=train_df, palette='Blues_d')","858a83f8":"c_train_df = train_df.copy()\nc_test_df = test_df.copy()\nc_all_df = all_df.copy()","3d9af240":"'''Define imp_index again'''\nimp_index = pd.Index([\n    'GrLivArea',\n    'TotalBsmtSF',\n    'LotArea',\n    'Neighborhood',\n    'OverallQual',\n    'OverallCond',\n    'GarageCars',\n    'MasVnrArea',\n    'YearBuilt',\n    'KitchenQual',\n    'YearRemodAdd',\n    'SalePrice',\n    ])","900fa4f6":"'''Extract doubtful data by watching important variable\\'s graphs'''\ndoubtful_train_df = train_df[\n    (train_df.SalePrice > 700000) \\\n    | ((train_df.GrLivArea > 4000) & (train_df.SalePrice < 200000))\n    | ((train_df.LotArea > 100000))\n    ]\ndoubtful_train_df.loc[:, imp_index].sort_values('SalePrice', ascending=False)","c7ae13e2":"'''We decide removing 2 outliers'''\nc_all_df = c_all_df.drop(\n    c_all_df[c_all_df.Id.isin(\n        train_df[\n            (train_df.SalePrice > 700000) \\\n            | ((train_df.GrLivArea > 4000) & (train_df.SalePrice < 200000))\n        ].Id)].index\n)","3a01e875":"c_all_df['YearsSinceBuilt'] = c_all_df.YrSold.astype('int64') - c_all_df.YearBuilt\nc_all_df.loc[c_all_df.YearsSinceBuilt <= 0, 'YearsSinceBuilt'] = 0","e43f20fb":"c_all_df['YearsSinceRemod'] = c_all_df.YrSold.astype('int64') - c_all_df.YearRemodAdd\n# If YearRemodAdd is '1950', no remodeling\nc_all_df.loc[c_all_df.YearRemodAdd == 1950, 'YearsSinceRemod'] = \\\n    c_all_df.loc[c_all_df.YearRemodAdd == 1950, 'YearsSinceBuilt']\nc_all_df.loc[c_all_df.YearsSinceRemod <= 0, 'YearsSinceRemod'] = 0","871752a6":"c_all_df['TotalSF'] = c_all_df['TotalBsmtSF'] + c_all_df['1stFlrSF'] + c_all_df['2ndFlrSF']","12ac48d7":"c_all_df['TotalBathrooms'] = \\\n    c_all_df['FullBath'] + (0.5 * c_all_df['HalfBath']) + \\\n    c_all_df['BsmtFullBath'] + (0.5 * c_all_df['BsmtHalfBath'])","c1cdbdc3":"c_all_df['TotalPorchSf'] = \\\n    c_all_df['OpenPorchSF'] + c_all_df['3SsnPorch'] + \\\n    c_all_df['EnclosedPorch'] + c_all_df['ScreenPorch'] + c_all_df['WoodDeckSF']","74d78580":"# median_order = train_df.groupby('Neighborhood')['SalePrice'].median().sort_values().index\n# c_all_df['RankNeighborhood'] = c_all_df.Neighborhood.replace(to_replace=median_order, value=range(len(median_order)))","b7ce7a5a":"# c_all_df['YrBltAndRemod']=c_all_df['YearBuilt']+c_all_df['YearRemodAdd']","502e16f0":"# c_all_df['TotalSqrFootage'] = \\\n#     c_all_df['BsmtFinSF1'] + c_all_df['BsmtFinSF2'] + \\\n#     c_all_df['1stFlrSF'] + c_all_df['2ndFlrSF']","b007bb21":"# c_all_df['haspool'] = c_all_df['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n# c_all_df['has2ndfloor'] = c_all_df['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n# c_all_df['hasgarage'] = c_all_df['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n# c_all_df['hasbsmt'] = c_all_df['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n# c_all_df['hasfireplace'] = c_all_df['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","9be73743":"# c_all_df.loc[c_all_df.YearsSinceBuilt == c_all_df.YearsSinceRemod, 'PresenceOfRemodel'] = 0\n# c_all_df.loc[c_all_df.YearsSinceBuilt != c_all_df.YearsSinceRemod, 'PresenceOfRemodel'] = 1\n# c_all_df.PresenceOfRemodel = c_all_df.PresenceOfRemodel.astype('int64')","1994ca85":"c_all_df = c_all_df.drop(columns=['Street', 'PoolQC', 'Utilities'],axis=1)","aac3c468":"\"\"\"Label encoding of ordinal variable\"\"\"\nc_all_df.LotShape.replace(to_replace=['IR3', 'IR2', 'IR1', 'Reg'], value=[0, 1, 2, 3], inplace=True)\nc_all_df.LandContour.replace(to_replace=['Low', 'Bnk', 'HLS', 'Lvl'], value=[0, 1, 2, 3], inplace=True)\n# c_all_df.Utilities.replace(to_replace=['NoSeWa', 'AllPub'], value=[0, 1], inplace=True)\nc_all_df.LandSlope.replace(to_replace=['Sev', 'Mod', 'Gtl'], value=[0, 1, 2], inplace=True)\nc_all_df.ExterQual.replace(to_replace=['Fa', 'TA', 'Gd', 'Ex'], value=[0, 1, 2, 3], inplace=True)\nc_all_df.ExterCond.replace(to_replace=['Po', 'Fa', 'TA', 'Gd', 'Ex'], value=[0, 1, 2, 3, 4], inplace=True)\nc_all_df.BsmtQual.replace(to_replace=['None', 'Fa', 'TA', 'Gd', 'Ex'], value=[0, 1, 2, 3, 4], inplace=True)\nc_all_df.BsmtCond.replace(to_replace=['None', 'Po', 'Fa', 'TA', 'Gd'], value=[0, 1, 2, 3, 4], inplace=True)\nc_all_df.BsmtExposure.replace(to_replace=['None', 'No', 'Mn', 'Av', 'Gd'], value=[0, 1, 2, 3, 4], inplace=True)\nc_all_df.BsmtFinType1.replace(to_replace=['None', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'], value=[0, 1, 2, 3, 4, 5, 6], inplace=True)\nc_all_df.BsmtFinType2.replace(to_replace=['None', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'], value=[0, 1, 2, 3, 4, 5, 6], inplace=True)\nc_all_df.HeatingQC.replace(to_replace=['Po', 'Fa', 'TA', 'Gd', 'Ex'], value=[0, 1, 2, 3, 4], inplace=True)\nc_all_df.Electrical.replace(to_replace=['Mix', 'FuseP', 'FuseF', 'FuseA', 'SBrkr'], value=[0, 1, 2, 3, 4], inplace=True)\nc_all_df.KitchenQual.replace(to_replace=['Fa', 'TA', 'Gd', 'Ex'], value=[0, 1, 2, 3], inplace=True)\nc_all_df.Functional.replace(to_replace=['Sev', 'Maj2', 'Maj1', 'Mod', 'Min2', 'Min1', 'Typ'], value=[0, 1, 2, 3, 4, 5, 6], inplace=True)\nc_all_df.FireplaceQu.replace(to_replace=['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'], value=[0, 1, 2, 3, 4, 5], inplace=True)\nc_all_df.GarageFinish.replace(to_replace=['None', 'Unf', 'RFn', 'Fin'], value=[0, 1, 2, 3], inplace=True)\nc_all_df.GarageQual.replace(to_replace=['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'], value=[0, 1, 2, 3, 4, 5], inplace=True)\nc_all_df.GarageCond.replace(to_replace=['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'], value=[0, 1, 2, 3, 4, 5], inplace=True)\nc_all_df.PavedDrive.replace(to_replace=['N', 'P', 'Y'], value=[0, 1, 2], inplace=True)\n# c_all_df.PoolQC.replace(to_replace=['None', 'Fa', 'Gd', 'Ex'], value=[0, 1, 2, 3], inplace=True)\nc_all_df.Fence.replace(to_replace=['None', 'MnWw', 'GdWo', 'MnPrv', 'GdPrv'], value=[0, 1, 2, 3, 4], inplace=True)","60af7c8f":"c_all_df.MSSubClass = c_all_df.MSSubClass.astype('int64')\nc_all_df.loc[c_all_df['MSSubClass']==60, 'MSSubClass'] = 0\nc_all_df.loc[(c_all_df['MSSubClass']==20)|(c_all_df['MSSubClass']==120), 'MSSubClass'] = 1\nc_all_df.loc[c_all_df['MSSubClass']==75, 'MSSubClass'] = 2\nc_all_df.loc[(c_all_df['MSSubClass']==40)|(c_all_df['MSSubClass']==70)|(c_all_df['MSSubClass']==80), 'MSSubClass'] = 3\nc_all_df.loc[(c_all_df['MSSubClass']==50)|(c_all_df['MSSubClass']==85)|(c_all_df['MSSubClass']==90)|(c_all_df['MSSubClass']==160)|(c_all_df['MSSubClass']==190), 'MSSubClass'] = 4\nc_all_df.loc[(c_all_df['MSSubClass']==30)|(c_all_df['MSSubClass']==45)|(c_all_df['MSSubClass']==180), 'MSSubClass'] = 5\nc_all_df.loc[(c_all_df['MSSubClass']==150), 'MSSubClass'] = 6","f7e924b6":"c_all_df.GarageCars.replace(to_replace=[0, 1, 2, 3, 4, 5], value=[0, 1, 2, 3, 3, 3], inplace=True)","225f54ae":"skewness = c_all_df.select_dtypes(include=['int64']).apply(lambda x: skew(x))\nskew_index = skewness[abs(skewness) >= 0.75].index\nskewness[skew_index].sort_values(ascending=False)","3ac426aa":"'''BoxCox Transform'''\nlam = 0.15\nfor column in skew_index:\n    c_all_df[column] = boxcox1p(c_all_df[column], lam)","9e66d718":"c_all_df.shape","b81ae227":"'''There remains no missing data'''\n(all_df.isnull().sum() == 0).all()","95213870":"'''Splitting the data back to train and test'''\nc_train_df = pd.merge(c_all_df, train_df[['Id', 'SalePrice']], how='inner', on='Id')\nc_test_df = c_all_df[c_all_df.Id.isin(test_Id)]","bdc1e4f1":"'''Importing Modeling Interested Modules'''\nfrom sklearn.base import BaseEstimator\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, ElasticNetCV\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n### http:\/\/rasbt.github.io\/mlxtend\/user_guide\/regressor\/StackingCVRegressor\/\nfrom mlxtend.regressor import StackingCVRegressor","7a069ae0":"'''Adjust dataframe for modeling'''\nc_train_X = c_train_df.drop(['Id', 'SalePrice'], axis=1)\nc_train_y = c_train_df.SalePrice\nc_test_X = c_test_df.drop('Id', axis=1)\n\n'''Transform categorical features to dummy variables'''\nd_c_train_X = pd.get_dummies(c_train_X)\nd_c_test_X = pd.get_dummies(c_test_X)","a2e5ae22":"'''We should use the log transform of the target value'''\nclass MyEstimator(BaseEstimator):\n    def __init__(self, model):\n        self.model = model\n        \n    def fit(self, X, y):\n        self.model.fit(X, np.log1p(y))\n        return self \n\n    def predict(self, X):\n        return np.expm1(self.model.predict(X))","8ac039b5":"'''Define evaluation function for Convienience'''\ndef evaluation_model(model, train_X, train_y, test_X):\n    cv = rmsle_cv(model, train_X, train_y)\n    cv_mean = np.round(cv.mean(), 5)\n    cv_std = np.round(cv.std(), 5)\n    sample_prediction = model.predict(test_X.loc[:3, :])\n    return {'cv_mean': cv_mean, 'cv_std': cv_std, 'sample_prediction': sample_prediction}","528909a3":"'''Define Hyperparameters Tuning Function'''\ndef tune_hyperparameters(model, param_grid, train_X, train_y):\n    grid = GridSearchCV(\n        model, param_grid, \n        scoring='neg_mean_squared_log_error',\n        cv=5, n_jobs=-1,\n    )\n    grid.fit(train_X, train_y)\n    best_params = grid.best_params_ \n    best_score = np.round(np.sqrt(-1 * grid.best_score_), 5)\n    return best_params, best_score","557d1d1f":"model = make_pipeline(\n    RobustScaler(),\n    LinearRegression(),\n)\nlr_model = MyEstimator(model)\nlr_model.fit(d_c_train_X, c_train_y)\nlr_eval = evaluation_model(lr_model, d_c_train_X, c_train_y, d_c_test_X)\nprint(lr_eval)","fcd7d9a6":"model = make_pipeline(\n    RobustScaler(),\n    LassoCV(\n        alphas=(0.0001, 0.0005, 0.001, 0.01, 0.05, 0.1, 0.3, 1, 3, 5, 10),\n    ),\n)\nlasso_cv_model = MyEstimator(model)\nlasso_cv_model.fit(d_c_train_X, c_train_y)\nlasso_cv_eval = evaluation_model(lasso_cv_model, d_c_train_X, c_train_y, d_c_test_X)\nprint(lasso_cv_eval)\n\nopt_alpha = lasso_cv_model.model.steps[1][1].alpha_\nprint(f'\\nopt_alpha: {opt_alpha}')","efaa8ee9":"model = make_pipeline(\n    RobustScaler(),\n    RidgeCV(\n        alphas=(0.0001, 0.0005, 0.001, 0.01, 0.05, 0.1, 0.3, 1, 3, 5, 10),\n    ),\n)\nridge_cv_model = MyEstimator(model)\nridge_cv_model.fit(d_c_train_X, c_train_y)\nridge_cv_eval = evaluation_model(ridge_cv_model, d_c_train_X, c_train_y, d_c_test_X)\nprint(ridge_cv_eval)\n\nopt_alpha = ridge_cv_model.model.steps[1][1].alpha_\nprint(f'\\nopt_alpha: {opt_alpha}')","0a30ea69":"model = make_pipeline(\n    RobustScaler(),\n    ElasticNetCV(\n        alphas=(0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007), \n        l1_ratio=(0.8, 0.85, 0.9, 0.95, 0.99, 1),\n    ),\n)\nelsnt_cv_model = MyEstimator(model)\nelsnt_cv_model.fit(d_c_train_X, c_train_y)\nelsnt_cv_eval = evaluation_model(elsnt_cv_model, d_c_train_X, c_train_y, d_c_test_X)\nprint(elsnt_cv_eval)\n\nopt_alpha = elsnt_cv_model.model.steps[1][1].alpha_\nopt_l1_ratio = elsnt_cv_model.model.steps[1][1].l1_ratio_\nprint(f'\\nopt_alpha: {opt_alpha} opt_l1_ratio: {opt_l1_ratio}')","261d0717":"### build basemodel\nmodel = make_pipeline(\n    RobustScaler(),\n    SVR(),\n)\nsvr_model = MyEstimator(model)\n\n### optimize hyperparameters\nparam_grid = {'model__svr__C': [1, 10, 20],\n              'model__svr__epsilon': [0.001, 0.01, 0.1],\n              'model__svr__gamma': [0.0001, 0.001, 0.01]}\nbest_params, best_score = \\\n    tune_hyperparameters(svr_model, param_grid, d_c_train_X, c_train_y)\n\n### fit using best_params\nsvr_model.set_params(**best_params)\nsvr_model.fit(d_c_train_X, c_train_y)\nsvr_eval = evaluation_model(svr_model, d_c_train_X, c_train_y, d_c_test_X)\nprint(svr_eval)\n\nprint(f'\\ngrid best_params: {best_params}')","976c1387":"### build basemodel\nmodel = make_pipeline(\n    RobustScaler(),\n    KernelRidge(),\n)\nkr_model = MyEstimator(model)\n\n### optimize hyperparameters\nparam_grid = {'model__kernelridge__alpha': [0.01, 0.1, 0.5, 1],\n              'model__kernelridge__kernel': ['linear', 'polynomial'],\n              'model__kernelridge__degree': [1, 1.5, 2, 3],\n              'model__kernelridge__coef0': [3, 4, 5]}\nbest_params, best_score = \\\n    tune_hyperparameters(kr_model, param_grid, d_c_train_X, c_train_y)\n\n### fit using best_params\nkr_model.set_params(**best_params)\nkr_model.fit(d_c_train_X, c_train_y)\nkr_eval = evaluation_model(kr_model, d_c_train_X, c_train_y, d_c_test_X)\nprint(kr_eval)\n\nprint(f'\\ngrid best_params: {best_params}')","deaada74":"### build basemodel\nmodel = make_pipeline(\n    RobustScaler(),\n    KNeighborsRegressor(),\n)\nknn_model = MyEstimator(model)\n\n### optimize hyperparameters\nparam_grid = {'model__kneighborsregressor__n_neighbors': [3, 5, 7]}\nbest_params, best_score = \\\n    tune_hyperparameters(knn_model, param_grid, d_c_train_X, c_train_y)\n\n### fit using best_params\nknn_model.set_params(**best_params)\nknn_model.fit(d_c_train_X, c_train_y)\nknn_eval = evaluation_model(knn_model, d_c_train_X, c_train_y, d_c_test_X)\nprint(knn_eval)\n\nprint(f'\\ngrid best_params: {best_params}')","d453967c":"### build basemodel\nmodel = ExtraTreesRegressor(n_jobs=-1, n_estimators=100)\net_model = MyEstimator(model)\n\n### optimize hyperparameters\nparam_grid = {'model__max_depth': [1, 3, 5]}\nbest_params, best_score = \\\n    tune_hyperparameters(et_model, param_grid, d_c_train_X, c_train_y)\n\n### fit using best_params\net_model.set_params(**best_params)\net_model.fit(d_c_train_X, c_train_y)\net_eval = evaluation_model(et_model, d_c_train_X, c_train_y, d_c_test_X)\nprint(et_eval)\n\nprint(f'\\ngrid best_params: {best_params}')","931becf3":"model = XGBRegressor(learning_rate=0.01, n_estimators=3000,\n                     max_depth=3, \n                     nthread=-1,\n                     objective='reg:squarederror',\n                     min_child_weight=0,\n                     gamma=0, subsample=0.7,\n                     colsample_bytree=0.7,\n                     scale_pos_weight=1,\n                     reg_alpha=0.00006)\nxgb_model = MyEstimator(model)\nxgb_model.fit(d_c_train_X, c_train_y)\nxgb_eval = evaluation_model(xgb_model, d_c_train_X, c_train_y, d_c_test_X)\nprint(xgb_eval)","6c87b512":"model = LGBMRegressor(learning_rate=0.01, n_estimators=3000,\n                      num_leaves=5,\n                      objective='regression',\n                      max_bin=55, bagging_fraction=0.8,\n                      bagging_freq=5, feature_fraction=0.2319,\n                      feature_fraction_seed=9, bagging_seed=9,\n                      min_data_in_leaf=6, min_sum_hessian_in_leaf=11)\nlgb_model = MyEstimator(model)\nlgb_model.fit(d_c_train_X, c_train_y)\nlgb_eval = evaluation_model(lgb_model, d_c_train_X, c_train_y, d_c_test_X)\nprint(lgb_eval)","0f2670c7":"models = [\n    ('lr_model', lr_eval),\n    ('lasso_cv_model', lasso_cv_eval),\n    ('ridge_cv_model', ridge_cv_eval),\n    ('elsnt_cv_model', elsnt_cv_eval),\n    ('svr_model', svr_eval),\n    ('kr_model', kr_eval),\n    ('knn_model', knn_eval),\n    ('et_model', et_eval),\n    ('xgb_model', xgb_eval),\n    ('lgb_model', lgb_eval),\n]\nfor mname, meval in models:\n    print(f'[{mname}]')\n    print(meval)","99ec1554":"stack_cv_model = StackingCVRegressor(\n    regressors=(ridge_cv_model, lasso_cv_model, elsnt_cv_model,\n                svr_model, kr_model, xgb_model, lgb_model),\n    meta_regressor=lgb_model,\n    cv=5,\n    n_jobs=-1,\n    use_features_in_secondary=True)\nstack_cv_model.fit(np.array(d_c_train_X), np.array(c_train_y))\nstack_cv_eval = evaluation_model(stack_cv_model, np.array(d_c_train_X), np.array(c_train_y), np.array(d_c_test_X))\nprint(stack_cv_eval)","755ad37d":"'''Blending Models'''\ndef blend_models_predict(X):\n    return ((0.2 * lasso_cv_model.predict(X)) + \\\n            (0.1 * ridge_cv_model.predict(X)) + \\\n            (0.2 * elsnt_cv_model.predict(X)) + \\\n            (0.1 * svr_model.predict(X)) + \\\n            (0.1 * kr_model.predict(X)) + \\\n            (0.1 * xgb_model.predict(X)) + \\\n            (0.1 * lgb_model.predict(X)) + \\\n            (0.1 * stack_cv_model.predict(np.array(X))))","cc0cc8db":"def output_submission_file(test_X, filename='submission.csv'):\n    prediction = blend_models_predict(test_X)\n    df = pd.DataFrame({'Id': test_Id.values, 'SalePrice': prediction})\n    print(f'{df.shape}')\n    print(f'{df.head(5)}')\n    df.to_csv(filename, index=False)\n    df.to_csv(filename + '.gz', index=False, compression='gzip')","ed0bdf6f":"'''Submission'''\noutput_submission_file(d_c_test_X)","74c7f709":"## 1.3 Defining Evaluation <a id=\"1.3\"><\/a>","a09ef9ae":"- **Alley** : data description says NA means \"no alley access\"","458c2f28":"# 3. Exploratory Data Analysis <a id=\"3\"><\/a>","099c2e55":"- **PoolQC** : data description says NA means \"No  Pool\". That make sense, given the huge ratio of missing value (+99%) and majority of houses have no Pool at all in general. ","68b74932":"## 3.2 Analyzing SalePrice <a id=\"3.2\"><\/a>","ea1d309f":"## 3.1 Take a glance at all variables <a id=\"3.1\"><\/a>","501afef1":"## 2.3 Splitting the Data Back <a id=\"2.3\"><\/a>","05fed585":"### RidgeCV","e5af582f":"- **Functional** : data description says NA means typical","2a422bb0":"### TotalSF","496ec1ee":"# 2 Adjusting Data <a id=\"2\"><\/a>","ca2a7826":"- **BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath** : missing values are likely zero for having no basement","1eb58a91":"### YearsSinceRemod","8b5fc9ac":"# Table of Contents\n\n* [1. Importing Packages and Collecting Data, Defining Evaluation](#1)\n  * [1.1 Importing Packages](#1.1)\n  * [1.2 Collecting Data](#1.2)\n  * [1.3 Defining Evaluation Metric](#1.3)\n* [2. Adjusting Data](#2)\n  * [2.1 Imputing Missing Data](#2.1)\n  * [2.2 Transforming Data Type](#2.2)\n  * [2.3 Splitting the Data Back](#2.3)\n* [3. Exploratory Data Analysis](#3)\n  * [3.1 Take a glance at all variables](#3.1)\n  * [3.2 Analyzing SalePrice](#3.2)\n* [4. Data Preprocessing](#4)\n  * [4.1 Saving DataFrame](#4.1)\n  * [4.2 Outliers](#4.2)\n  * [4.3 Adding New Features](#4.3)\n  * [4.4 Droping Features](#4.4)\n  * [4.5 Transforming Features](#4.5)\n  * [4.6 Splitting the Data Back](#4.6)\n* [5. Model Building and Evaluation](#5)\n  * [5.1 Importing Packages](#5.1)\n  * [5.2 Preparation before Building Models](#5.2)\n  * [5.3 Building Models](#5.3)\n  * [5.4 Submission](#5.4)","961bb32b":"## 4.6 Splitting the Data Back <a id=\"4.6\"><\/a>","64b99efb":"## 5.3 Building Models <a id=\"5.3\"><\/a>","faeb116c":"- **LotFrontage** : Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can **fill in missing values by the median LotFrontage of the neighborhood**.","5350671a":"### XGBoost","568d49ab":"- **BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2** : For all these categorical basement-related features, NaN means that there is no  basement.\n","36bbe653":"### TotalPorchSf","5a3e52d5":"## 2.1 Imputing Missing Data <a id=\"2.1\"><\/a>","e6275a4c":"- **MSZoning (The general zoning classification)** :  'RL' is by far  the most common value.  So we can fill in missing values with 'RL'\n","3efe67da":"# About the Kernel","7494e2c5":"- **MiscFeature** : data description says NA means \"no misc feature\"\n","e05f3fec":"- **SaleType** : Fill in again with most frequent which is \"WD\"","c4a5e641":"- **Utilities** : 'AllPub' is by far the most common value. So we can fill in missing values with 'AllPub'\n","d5fc3ead":"### has equipment","65c78cc3":"- **GarageYrBlt, GarageArea and GarageCars** : Replacing missing data with 0 (Since No garage = no cars in such garage.)\n","40a25186":"### LightGBM","5f934c81":"**The following variables seem to play an important role in this problem**\n\n* GrLivArea\n* TotalBsmtSF (correlated with 1stFlrSF and 2ndFlrSF, BsmtFinSF1)\n* LotArea\n* Neighborhood\n* OverallQual\n* OverallCond\n* GarageCars (correlated with GarageArea)\n* MasVnrArea\n* YearBuilt\n* KitchenQual\n* YearRemodAdd","a5b75670":"### Summary","0db7a1ac":"- **GarageType, GarageFinish, GarageQual and GarageCond** : Replacing missing data with None","d5ca754d":"## 1.2 Collecting Data <a id=\"1.2\"><\/a>","6c63bae6":"### Stacking","3e0f49ed":"- **MSSubClass** : Na most likely means No building class. We can replace missing values with None\n","340411e0":"## 4.1 Saving DataFrame <a id=\"4.1\"><\/a>","e2342758":"## 5.2 Preparation before Building Models <a id=\"5.2\"><\/a>","7331b18b":"- **MasVnrArea and MasVnrType** : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type. \n","23203853":"- **Exterior1st and Exterior2nd** : Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string\n","df31d537":"### TotalBathroom","e2d9a1c0":"## 4.5 Transforming Features <a id=\"4.5\"><\/a>","780f9c2c":"# 4. Data Preprocessing <a id=\"4\"><\/a>","33d8e220":"# 5. Model Building and Evaluation <a id=\"5\"><\/a>","3c036966":"### Label encoding","d2e314c1":"### TotalSqrFootage","c8c75723":"### ExtraTreesRegressor","a8952736":"### ElasticNetCV","9939f7f4":"## 4.4 Droping Features <a id=\"4.4\"><\/a>","524a5b22":"## 4.2 Outliers <a id=\"4.2\"><\/a>","40e51276":"### YearsSinceBuilt","f0c18dee":"### SVR","ed6598b3":"## 5.1 Importing Packages <a id=\"5.1\"><\/a>","52548d3b":"### MSSubClass","8e3e8052":"# 1. Importing Packages and Collecting Data, Defining Evaluation <a id=\"1\"><\/a>","0a1db9c4":"### YrBltAndRemod","5d0ff28b":"## 4.3 Adding New Features <a id=\"4.3\"><\/a>","3e826565":"### GarageCars","d86e4be4":"### skewed features","cd91bb6a":"## 1.1 Importing Packages <a id=\"1.1\"><\/a>","181478bb":"## 2.2 Transforming Data Type <a id=\"2.2\"><\/a>","a5ec703a":"## 5.4 Submission <a id=\"5.4\"><\/a>","b8c3b203":"- **Fence** : data description says NA means \"no fence\"","10e81d25":"- **Electrical** : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.\n","dcb95efb":"### LinearRegression","37219fef":"### LassoCV","bc3da39b":"- **KitchenQual**: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent)  for the missing value in KitchenQual.\n","2f8bab1b":"### KNN","df8a0354":"In this kernel, we first imputed missing data carefully before Exploratory Data Analysis. \n\nIn EDA, we marked some features down as important ones by LightGBM\u2019s feature importance.\n\nAlso, we added some new features like \nYearsSinceBuilt that improved score.\n\nFinally, we built many models from Linear Regression to LightGBM. We also built the stacking model.","02269c1e":"### RankNeighborhood","f6fed02c":"### KernelRidge","80cfec80":"- **FireplaceQu** : data description says NA means \"no fireplace\"","a9e5b2c6":"### PresenceOfRemodel"}}