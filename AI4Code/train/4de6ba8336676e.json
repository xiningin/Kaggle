{"cell_type":{"6714c675":"code","6ad8436d":"code","64f25b16":"code","e0ddbf1a":"code","0eda2cb5":"code","64ad06fc":"code","cd605330":"code","ed5c20b0":"code","accd2d22":"code","ec4154e3":"code","226373ff":"code","ae4e607b":"code","d7884f3b":"code","ad7438ea":"code","404bd6d1":"code","9ef5ff30":"code","b9130b90":"code","975d3b5f":"code","14542590":"code","4326dba0":"code","99e0eaa6":"code","362daec2":"code","7962657d":"code","be9e665b":"code","537f6381":"code","49adf5f6":"code","276a8ca5":"code","611a7630":"code","da296044":"code","a842244d":"code","a961768b":"code","fab50e61":"code","b5edf389":"code","50f85d22":"code","2c9065bf":"code","2ee17908":"code","8fcee98f":"code","f5d5815a":"code","f0273cd6":"code","3740af64":"code","81d3154f":"code","c04cf12c":"code","c24589fc":"code","d87252f0":"code","6024a9ab":"code","2a4cc669":"code","c3a13e35":"code","96238709":"code","9efebdfb":"code","1484d37d":"code","3742d852":"code","b316c2d5":"code","7c30e9ad":"code","785e2cba":"markdown","2ae7c97b":"markdown","ccbc0d2f":"markdown","cfb20416":"markdown","14e43a28":"markdown","04b277e4":"markdown"},"source":{"6714c675":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6ad8436d":"%matplotlib inline\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom datetime import timedelta\nimport datetime as dt\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [16, 10]\nimport seaborn as sns\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import MiniBatchKMeans\nimport warnings\nwarnings.filterwarnings('ignore')","64f25b16":"import zipfile\nwith zipfile.ZipFile('..\/input\/nyc-taxi-trip-duration\/train.zip', 'r') as zip_ref:\n    zip_ref.extractall('..\/new\/nyc-taxi-trip-duration')","e0ddbf1a":"with zipfile.ZipFile('..\/input\/nyc-taxi-trip-duration\/test.zip', 'r') as zip_ref:\n    zip_ref.extractall('..\/new\/nyc-taxi-trip-duration')\nwith zipfile.ZipFile('..\/input\/nyc-taxi-trip-duration\/sample_submission.zip', 'r') as zip_ref:\n    zip_ref.extractall('..\/new\/nyc-taxi-trip-duration')","0eda2cb5":"np.random.seed(1987)\nN = 100000 # number of sample rows in plots\nt0 = dt.datetime.now()\ntrain = pd.read_csv('..\/new\/nyc-taxi-trip-duration\/train.csv')\ntest = pd.read_csv('..\/new\/nyc-taxi-trip-duration\/test.csv')\nsample_submission = pd.read_csv('..\/new\/nyc-taxi-trip-duration\/sample_submission.csv')","64ad06fc":"test.head()","cd605330":"print('Id is unique') if train['id'].nunique()==train.shape[0]  else print('oops')\nprint('Train and test set are distinct') if len(np.intersect1d(train['id'].values,test['id'].values))==0 else print('oops')\nprint('No missing values') if train.count().min()==train.shape[0] and test.count().min()==test.shape[0] else print('oops')\nprint('The store_and_fwd_flag has only two values {}.'.format(str(set(train.store_and_fwd_flag.unique()) | set(test.store_and_fwd_flag.unique()))))","ed5c20b0":"train.head()","accd2d22":"train['pickup_datetime']=pd.to_datetime(train['pickup_datetime'])\ntest['pickup_datetime'] = pd.to_datetime(test['pickup_datetime'])\ntrain['pickup_date']=train['pickup_datetime'].dt.date\ntest['pickup_date']=test['pickup_datetime'].dt.date\ntrain['dropoff_datetime'] = pd.to_datetime(train.dropoff_datetime)\ntrain['store_and_fwd_flag']=1*(train['store_and_fwd_flag']=='Y')\ntest['store_and_fwd_flag']=1*(test['store_and_fwd_flag']=='Y')\ntrain['check_trip_duration'] = (train['dropoff_datetime'] - train['pickup_datetime']).map(lambda x: x.total_seconds())\n","ec4154e3":"duration_difference=train[np.abs(train['check_trip_duration'].values  - train['trip_duration'].values)>1]\nprint('Trip_duration and datetimes are ok.') if len(duration_difference[['pickup_datetime', 'dropoff_datetime', 'trip_duration', 'check_trip_duration']]) == 0 else print('Ooops.')","226373ff":"train['trip_duration'].max() \/\/ 3600","ae4e607b":"#We can see that the max trip_duration is ~ 1000 hours. Fortunately the evaluation metric is RMSLE and not RMSE . \n#Outliers will cause less trouble. We could logtransform our target label and use RMSE during training.\ntrain['log_trip_duration'] = np.log(train['trip_duration'].values + 1)\nplt.hist(train['log_trip_duration'].values, bins=100)\nplt.xlabel('log(trip_duration)')\nplt.ylabel('number of train records')\nplt.show()","d7884f3b":"plt.plot(train.groupby('pickup_date').count()[['id']], 'o-', label='train')\nplt.plot(test.groupby('pickup_date').count()[['id']], 'o-', label='test')\nplt.title('Train and test period complete overlap.')\nplt.legend(loc=0)\nplt.ylabel('number of records')\nplt.show()","ad7438ea":"city_long_border = (-74.03, -73.75)\ncity_lat_border = (40.63, 40.85)\nfig, ax = plt.subplots(ncols=2, sharex=True, sharey=True)\nax[0].scatter(train['pickup_longitude'].values[:N], train['pickup_latitude'].values[:N],\n              color='blue', s=1, label='train', alpha=0.1)\nax[1].scatter(test['pickup_longitude'].values[:N], test['pickup_latitude'].values[:N],\n              color='green', s=1, label='test', alpha=0.1)\nfig.suptitle('Train and test area complete overlap.')\nax[0].legend(loc=0)\nax[0].set_ylabel('latitude')\nax[0].set_xlabel('longitude')\nax[1].set_xlabel('longitude')\nax[1].legend(loc=0)\nplt.ylim(city_lat_border)\nplt.xlim(city_long_border)\nplt.show()","404bd6d1":"coords = np.vstack((train[['pickup_latitude', 'pickup_longitude']].values,\n                    train[['dropoff_latitude', 'dropoff_longitude']].values,\n                    test[['pickup_latitude', 'pickup_longitude']].values,\n                    test[['dropoff_latitude', 'dropoff_longitude']].values))","9ef5ff30":"pca = PCA().fit(coords)","b9130b90":"#We use PCA to transform longitude and latitude coordinates. In this case it is not about dimension reduction since we transform 2D-> 2D. \n#The rotation could help for decision tree splits.\ntrain['pickup_pca0'] = pca.transform(train[['pickup_latitude', 'pickup_longitude']])[:, 0]\ntrain['pickup_pca1'] = pca.transform(train[['pickup_latitude', 'pickup_longitude']])[:, 1]\ntrain['dropoff_pca0'] = pca.transform(train[['dropoff_latitude', 'dropoff_longitude']])[:, 0]\ntrain['dropoff_pca1'] = pca.transform(train[['dropoff_latitude', 'dropoff_longitude']])[:, 1]\ntest['pickup_pca0'] = pca.transform(test[['pickup_latitude', 'pickup_longitude']])[:, 0]\ntest['pickup_pca1'] = pca.transform(test[['pickup_latitude', 'pickup_longitude']])[:, 1]\ntest['dropoff_pca0'] = pca.transform(test[['dropoff_latitude', 'dropoff_longitude']])[:, 0]\ntest['dropoff_pca1'] = pca.transform(test[['dropoff_latitude', 'dropoff_longitude']])[:, 1]\n","975d3b5f":"fig, ax = plt.subplots(ncols=2)\nax[0].scatter(train['pickup_longitude'].values[:N], train['pickup_latitude'].values[:N],\n              color='blue', s=1, alpha=0.1)\nax[1].scatter(train['pickup_pca0'].values[:N], train['pickup_pca1'].values[:N],\n              color='green', s=1, alpha=0.1)\nfig.suptitle('Pickup lat long coords and PCA transformed coords.')\nax[0].set_ylabel('latitude')\nax[0].set_xlabel('longitude')\nax[1].set_xlabel('pca0')\nax[1].set_ylabel('pca1')\nax[0].set_xlim(city_long_border)\nax[0].set_ylim(city_lat_border)\npca_borders = pca.transform([[x, y] for x in city_lat_border for y in city_long_border])\nax[1].set_xlim(pca_borders[:, 0].min(), pca_borders[:, 0].max())\nax[1].set_ylim(pca_borders[:, 1].min(), pca_borders[:, 1].max())\nplt.show()","14542590":"def haversine_array(lat1, lng1, lat2, lng2):\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    AVG_EARTH_RADIUS = 6371  # in km\n    lat = lat2 - lat1\n    lng = lng2 - lng1\n    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n    return h\n\ndef dummy_manhattan_distance(lat1, lng1, lat2, lng2):\n    a = haversine_array(lat1, lng1, lat1, lng2)\n    b = haversine_array(lat1, lng1, lat2, lng1)\n    return a + b\n\ndef bearing_array(lat1, lng1, lat2, lng2):\n    AVG_EARTH_RADIUS = 6371  # in km\n    lng_delta_rad = np.radians(lng2 - lng1)\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    y = np.sin(lng_delta_rad) * np.cos(lat2)\n    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)\n    return np.degrees(np.arctan2(y, x))\n","4326dba0":"train.loc[:, 'distance_haversine'] = haversine_array(train['pickup_latitude'].values, train['pickup_longitude'].values, train['dropoff_latitude'].values, train['dropoff_longitude'].values)","99e0eaa6":"train.head()","362daec2":"train['distance_haversine'] = haversine_array(train['pickup_latitude'].values, train['pickup_longitude'].values, train['dropoff_latitude'].values, train['dropoff_longitude'].values)\ntrain.loc[:, 'distance_dummy_manhattan'] = dummy_manhattan_distance(train['pickup_latitude'].values, train['pickup_longitude'].values, train['dropoff_latitude'].values, train['dropoff_longitude'].values)\ntrain.loc[:, 'direction'] = bearing_array(train['pickup_latitude'].values, train['pickup_longitude'].values, train['dropoff_latitude'].values, train['dropoff_longitude'].values)\ntrain.loc[:, 'pca_manhattan'] = np.abs(train['dropoff_pca1'] - train['pickup_pca1']) + np.abs(train['dropoff_pca0'] - train['pickup_pca0'])\n\ntest.loc[:, 'distance_haversine'] = haversine_array(test['pickup_latitude'].values, test['pickup_longitude'].values, test['dropoff_latitude'].values, test['dropoff_longitude'].values)\ntest.loc[:, 'distance_dummy_manhattan'] = dummy_manhattan_distance(test['pickup_latitude'].values, test['pickup_longitude'].values, test['dropoff_latitude'].values, test['dropoff_longitude'].values)\ntest.loc[:, 'direction'] = bearing_array(test['pickup_latitude'].values, test['pickup_longitude'].values, test['dropoff_latitude'].values, test['dropoff_longitude'].values)\ntest.loc[:, 'pca_manhattan'] = np.abs(test['dropoff_pca1'] - test['pickup_pca1']) + np.abs(test['dropoff_pca0'] - test['pickup_pca0'])\n\ntrain.loc[:, 'center_latitude'] = (train['pickup_latitude'].values + train['dropoff_latitude'].values) \/ 2\ntrain.loc[:, 'center_longitude'] = (train['pickup_longitude'].values + train['dropoff_longitude'].values) \/ 2\ntest.loc[:, 'center_latitude'] = (test['pickup_latitude'].values + test['dropoff_latitude'].values) \/ 2\ntest.loc[:, 'center_longitude'] = (test['pickup_longitude'].values + test['dropoff_longitude'].values) \/ 2","7962657d":"train.loc[:, 'pickup_weekday'] = train['pickup_datetime'].dt.weekday\ntrain.loc[:, 'pickup_hour_weekofyear'] = train['pickup_datetime'].dt.weekofyear\ntrain.loc[:, 'pickup_hour'] = train['pickup_datetime'].dt.hour\ntrain.loc[:, 'pickup_minute'] = train['pickup_datetime'].dt.minute\ntrain.loc[:, 'pickup_dt'] = (train['pickup_datetime'] - train['pickup_datetime'].min()).dt.total_seconds()\ntrain.loc[:, 'pickup_week_hour'] = train['pickup_weekday'] * 24 + train['pickup_hour']\n\ntest.loc[:, 'pickup_weekday'] = test['pickup_datetime'].dt.weekday\ntest.loc[:, 'pickup_hour_weekofyear'] = test['pickup_datetime'].dt.weekofyear\ntest.loc[:, 'pickup_hour'] = test['pickup_datetime'].dt.hour\ntest.loc[:, 'pickup_minute'] = test['pickup_datetime'].dt.minute\ntest.loc[:, 'pickup_dt'] = (test['pickup_datetime'] - train['pickup_datetime'].min()).dt.total_seconds()\ntest.loc[:, 'pickup_week_hour'] = test['pickup_weekday'] * 24 + test['pickup_hour']","be9e665b":"train.loc[:, 'avg_speed_h'] = 1000 * train['distance_haversine'] \/ train['trip_duration']\ntrain.loc[:, 'avg_speed_m'] = 1000 * train['distance_dummy_manhattan'] \/ train['trip_duration']\nfig, ax = plt.subplots(ncols=3, sharey=True)\nax[0].plot(train.groupby('pickup_hour').mean()['avg_speed_h'], 'bo-', lw=2, alpha=0.7)\nax[1].plot(train.groupby('pickup_weekday').mean()['avg_speed_h'], 'go-', lw=2, alpha=0.7)\nax[2].plot(train.groupby('pickup_week_hour').mean()['avg_speed_h'], 'ro-', lw=2, alpha=0.7)\nax[0].set_xlabel('hour')\nax[1].set_xlabel('weekday')\nax[2].set_xlabel('weekhour')\nax[0].set_ylabel('average speed')\nfig.suptitle('Rush hour average traffic speed')\nplt.show()","537f6381":"train.loc[:, 'pickup_lat_bin'] = np.round(train['pickup_latitude'], 3)\ntrain.loc[:, 'pickup_long_bin'] = np.round(train['pickup_longitude'], 3)\n# Average speed for regions\ngby_cols = ['pickup_lat_bin', 'pickup_long_bin']\ncoord_speed = train.groupby(gby_cols).mean()[['avg_speed_h']].reset_index()\ncoord_count = train.groupby(gby_cols).count()[['id']].reset_index()\ncoord_stats = pd.merge(coord_speed, coord_count, on=gby_cols)\ncoord_stats = coord_stats[coord_stats['id'] > 100]","49adf5f6":"fig, ax = plt.subplots(ncols=1, nrows=1)\nax.scatter(train.pickup_longitude.values[:N], train.pickup_latitude.values[:N],\n           color='black', s=1, alpha=0.5)\nax.scatter(coord_stats.pickup_long_bin.values, coord_stats.pickup_lat_bin.values,\n           c=coord_stats.avg_speed_h.values,\n           cmap='RdYlGn', s=20, alpha=0.5, vmin=1, vmax=8)\nax.set_xlim(city_long_border)\nax.set_ylim(city_lat_border)\nax.set_xlabel('Longitude')\nax.set_ylabel('Latitude')\nplt.title('Average speed')\nplt.show()","276a8ca5":"train.loc[:, 'pickup_lat_bin'] = np.round(train['pickup_latitude'], 2)\ntrain.loc[:, 'pickup_long_bin'] = np.round(train['pickup_longitude'], 2)\ntrain.loc[:, 'center_lat_bin'] = np.round(train['center_latitude'], 2)\ntrain.loc[:, 'center_long_bin'] = np.round(train['center_longitude'], 2)\ntrain.loc[:, 'pickup_dt_bin'] = (train['pickup_dt'] \/\/ (3 * 3600))\ntest.loc[:, 'pickup_lat_bin'] = np.round(test['pickup_latitude'], 2)\ntest.loc[:, 'pickup_long_bin'] = np.round(test['pickup_longitude'], 2)\ntest.loc[:, 'center_lat_bin'] = np.round(test['center_latitude'], 2)\ntest.loc[:, 'center_long_bin'] = np.round(test['center_longitude'], 2)\ntest.loc[:, 'pickup_dt_bin'] = (test['pickup_dt'] \/\/ (3 * 3600))","611a7630":"sample_ind = np.random.permutation(len(coords))[:500000]\nkmeans = MiniBatchKMeans(n_clusters=100, batch_size=10000).fit(coords[sample_ind])","da296044":"train.loc[:, 'pickup_cluster'] = kmeans.predict(train[['pickup_latitude', 'pickup_longitude']])\ntrain.loc[:, 'dropoff_cluster'] = kmeans.predict(train[['dropoff_latitude', 'dropoff_longitude']])\ntest.loc[:, 'pickup_cluster'] = kmeans.predict(test[['pickup_latitude', 'pickup_longitude']])\ntest.loc[:, 'dropoff_cluster'] = kmeans.predict(test[['dropoff_latitude', 'dropoff_longitude']])\nt1 = dt.datetime.now()\nprint('Time till clustering: %i seconds' % (t1 - t0).seconds)","a842244d":"fig, ax = plt.subplots(ncols=1, nrows=1)\nax.scatter(train.pickup_longitude.values[:N], train.pickup_latitude.values[:N], s=10, lw=0,\n           c=train.pickup_cluster[:N].values, cmap='tab20', alpha=0.2)\nax.set_xlim(city_long_border)\nax.set_ylim(city_lat_border)\nax.set_xlabel('Longitude')\nax.set_ylabel('Latitude')\nplt.show()","a961768b":"for gby_col in ['pickup_hour', 'pickup_date', 'pickup_dt_bin',\n               'pickup_week_hour', 'pickup_cluster', 'dropoff_cluster']:\n    gby = train.groupby(gby_col).mean()[['avg_speed_h', 'avg_speed_m', 'log_trip_duration']]\n    gby.columns = ['%s_gby_%s' % (col, gby_col) for col in gby.columns]\n    train = pd.merge(train, gby, how='left', left_on=gby_col, right_index=True)\n    test = pd.merge(test, gby, how='left', left_on=gby_col, right_index=True)","fab50e61":"for gby_cols in [['center_lat_bin', 'center_long_bin'],\n                 ['pickup_hour', 'center_lat_bin', 'center_long_bin'],\n                 ['pickup_hour', 'pickup_cluster'],  ['pickup_hour', 'dropoff_cluster'],\n                 ['pickup_cluster', 'dropoff_cluster']]:\n    coord_speed = train.groupby(gby_cols).mean()[['avg_speed_h']].reset_index()\n    coord_count = train.groupby(gby_cols).count()[['id']].reset_index()\n    coord_stats = pd.merge(coord_speed, coord_count, on=gby_cols)\n    coord_stats = coord_stats[coord_stats['id'] > 100]\n    coord_stats.columns = gby_cols + ['avg_speed_h_%s' % '_'.join(gby_cols), 'cnt_%s' %  '_'.join(gby_cols)]\n    train = pd.merge(train, coord_stats, how='left', on=gby_cols)\n    test = pd.merge(test, coord_stats, how='left', on=gby_cols)","b5edf389":"group_freq = '60min'\ndf_all = pd.concat((train, test))[['id', 'pickup_datetime', 'pickup_cluster', 'dropoff_cluster']]","50f85d22":"train.loc[:, 'pickup_datetime_group'] = train['pickup_datetime'].dt.round(group_freq)\ntest.loc[:, 'pickup_datetime_group'] = test['pickup_datetime'].dt.round(group_freq)","2c9065bf":"# Count trips over 60min\ndf_counts=df_all.set_index('pickup_datetime')[['id']].sort_index()\ndf_counts['count_60min'] = df_counts.isnull().rolling(group_freq).count()['id']","2ee17908":"train = train.merge(df_counts, on='id', how='left')\ntest = test.merge(df_counts, on='id', how='left')\n","8fcee98f":"df_all.set_index('pickup_datetime').groupby([pd.Grouper(freq=group_freq),'dropoff_cluster'])","f5d5815a":"# Count how many trips are going to each cluster over time\ndropoff_counts=df_all.set_index('pickup_datetime').groupby([pd.Grouper(freq=group_freq), 'dropoff_cluster']).agg({'id':'count'}).reset_index().set_index('pickup_datetime').groupby('dropoff_cluster').rolling('240min').mean().drop('dropoff_cluster', axis=1).reset_index().set_index('pickup_datetime').shift(freq='-120min').reset_index().rename(columns={'pickup_datetime': 'pickup_datetime_group', 'id': 'dropoff_cluster_count'})\n","f0273cd6":"train['dropoff_cluster_count'] = train[['pickup_datetime_group', 'dropoff_cluster']].merge(dropoff_counts, on=['pickup_datetime_group', 'dropoff_cluster'], how='left')['dropoff_cluster_count'].fillna(0)\ntest['dropoff_cluster_count'] = test[['pickup_datetime_group', 'dropoff_cluster']].merge(dropoff_counts, on=['pickup_datetime_group', 'dropoff_cluster'], how='left')['dropoff_cluster_count'].fillna(0)","3740af64":"# Count how many trips are going from each cluster over time\ndf_all = pd.concat((train, test))[['id', 'pickup_datetime', 'pickup_cluster', 'dropoff_cluster']]\npickup_counts = df_all.set_index('pickup_datetime').groupby([pd.Grouper(freq=group_freq), 'pickup_cluster']).agg({'id': 'count'}).reset_index().set_index('pickup_datetime').groupby('pickup_cluster').rolling('240min').mean().drop('pickup_cluster', axis=1).reset_index().set_index('pickup_datetime').shift(freq='-120min').reset_index().rename(columns={'pickup_datetime': 'pickup_datetime_group', 'id': 'pickup_cluster_count'})\n","81d3154f":"train['pickup_cluster_count'] = train[['pickup_datetime_group', 'pickup_cluster']].merge(pickup_counts, on=['pickup_datetime_group', 'pickup_cluster'], how='left')['pickup_cluster_count'].fillna(0)\ntest['pickup_cluster_count'] = test[['pickup_datetime_group', 'pickup_cluster']].merge(pickup_counts, on=['pickup_datetime_group', 'pickup_cluster'], how='left')['pickup_cluster_count'].fillna(0)","c04cf12c":"fr1 = pd.read_csv('..\/input\/new-york-city-taxi-with-osrm\/fastest_routes_train_part_1.csv',\n                  usecols=['id', 'total_distance', 'total_travel_time',  'number_of_steps'])\nfr2 = pd.read_csv('..\/input\/new-york-city-taxi-with-osrm\/fastest_routes_train_part_2.csv',\n                  usecols=['id', 'total_distance', 'total_travel_time', 'number_of_steps'])\ntest_street_info = pd.read_csv('..\/input\/new-york-city-taxi-with-osrm\/fastest_routes_test.csv',\n                               usecols=['id', 'total_distance', 'total_travel_time', 'number_of_steps'])","c24589fc":"train_street_info = pd.concat((fr1, fr2))\ntrain = train.merge(train_street_info, how='left', on='id')\ntest = test.merge(test_street_info, how='left', on='id')\ntrain_street_info.head()","d87252f0":"features_names=list(train.columns)\nprint(np.setdiff1d(train.columns,test.columns))\ndo_not_use_for_training=['id','log_trip_duration','pickup_datetime','dropoff_datetime', 'trip_duration', 'check_trip_duration','pickup_date', 'avg_speed_h', 'avg_speed_m','pickup_lat_bin', 'pickup_long_bin',\n'center_lat_bin', 'center_long_bin','pickup_dt_bin', 'pickup_datetime_group']\nfeature_names = [f for f in train.columns if f not in do_not_use_for_training]\n# print(feature_names)\nprint('We have %i features.' % len(feature_names))\ntrain[feature_names].count()\ny = np.log(train['trip_duration'].values + 1)\n\nt1 = dt.datetime.now()\nprint('Feature extraction time: %i seconds' % (t1 - t0).seconds)","6024a9ab":"feature_stats=pd.DataFrame({'feature':feature_names})\nfeature_stats.loc[:,'train_mean']=np.nanmean(train[feature_names].values, axis=0).round(4)\nfeature_stats.loc[:, 'test_mean'] = np.nanmean(test[feature_names].values, axis=0).round(4)\nfeature_stats.loc[:, 'train_std'] = np.nanstd(train[feature_names].values, axis=0).round(4)\nfeature_stats.loc[:, 'test_std'] = np.nanstd(test[feature_names].values, axis=0).round(4)\nfeature_stats.loc[:, 'train_nan'] = np.mean(np.isnan(train[feature_names].values), axis=0).round(3)\nfeature_stats.loc[:, 'test_nan'] = np.mean(np.isnan(test[feature_names].values), axis=0).round(3)","2a4cc669":"feature_stats.loc[:, 'train_test_mean_diff'] = np.abs(feature_stats['train_mean'] - feature_stats['test_mean']) \/ np.abs(feature_stats['train_std'] + feature_stats['test_std'])  * 2\nfeature_stats.loc[:, 'train_test_nan_diff'] = np.abs(feature_stats['train_nan'] - feature_stats['test_nan'])\nfeature_stats = feature_stats.sort_values(by='train_test_mean_diff')\nfeature_stats[['feature', 'train_test_mean_diff']].tail()\n","c3a13e35":"feature_stats = feature_stats.sort_values(by='train_test_nan_diff')\nfeature_stats[['feature', 'train_nan', 'test_nan', 'train_test_nan_diff']].tail()","96238709":"Xtr, Xv, ytr, yv = train_test_split(train[feature_names].values, y, test_size=0.2, random_state=1987)\ndtrain = xgb.DMatrix(Xtr, label=ytr)\ndvalid = xgb.DMatrix(Xv, label=yv)\ndtest = xgb.DMatrix(test[feature_names].values)\nwatchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n\n","9efebdfb":"xgb_pars = {'min_child_weight': 50, 'eta': 0.3, 'colsample_bytree': 0.3, 'max_depth': 10,\n            'subsample': 0.8, 'lambda': 1., 'nthread': 4, 'booster' : 'gbtree', 'silent': 1,\n            'eval_metric': 'rmse', 'objective': 'reg:linear'}\ncat_pars={'iterations':50,'learning_rate':0.1,\n                        'max_depth':3,\n                        'loss_function':'RMSE',\n                        'l2_leaf_reg': 9}\n                        \n                       \nmodel = xgb.train(xgb_pars, dtrain, 60, watchlist, early_stopping_rounds=50,\n                  maximize=False, verbose_eval=10)\n\n\n\n\n","1484d37d":"from catboost import CatBoostRegressor\n\n\nmodel = CatBoostRegressor(loss_function = \"RMSE\", eval_metric = \"RMSE\", metric_period = 100, iterations=2000,\n                        use_best_model = True,\n                        random_strength = 0.5,\n                        learning_rate=0.2,\n                        depth=7,\n                        random_seed = 93,\n                        l2_leaf_reg = 0.1,\n                        verbose=False,\n                        logging_level = None)\nmodel.fit( Xtr, ytr, cat_features=None, eval_set=(Xv,yv))","3742d852":"predictions1=model.predict(test[feature_names])\ntest['trip_duration'] = np.exp(predictions1) - 1","b316c2d5":"\ntest[['id', 'trip_duration']].to_csv('catboost1.csv', index=False)\n","7c30e9ad":"model.fit( dtrain, ltrain, use_best_model=True, eval_set=(dval, lval), silent=True, plot=True )","785e2cba":"**Distance**\n\nLet's calculate the distance (km) between pickup and dropoff points. Currently Haversine is used, geopy has another heuristics (vincenty() or great_circle()) if you prefer. The cabs are not flying and we are in New York so we could check the Manhattan (L1) distance too :)\n\npd.DataFrame.apply() would be too slow so the haversine function is rewritten to handle arrays. We extraxt the middle of the path as a feature as well.","2ae7c97b":"**Temporal and geospatial aggregation**\n\nAdd a few average traffic speed features. Note that if the train\/test split would be time based then we could not use as much temporal features. In this competition we do not need to predict the future.","ccbc0d2f":"* id - a unique identifier for each trip\n* vendor_id - a code indicating the provider associated with the trip record\n* pickup_datetime - date and time when the meter was engaged\n* dropoff_datetime - date and time when the meter was disengaged\n* passenger_count - the number of passengers in the vehicle (driver entered value)\n* pickup_longitude - the longitude where the meter was engaged\n* pickup_latitude - the latitude where the meter was engaged\n* dropoff_longitude - the longitude where the meter was disengaged\n* dropoff_latitude - the latitude where the meter was disengaged\n* store_and_fwd_flag - This flag indicates whether the trip record was held in vehicle memory before sending to the vendor because the vehicle did not have a connection to the server (Y=store and forward; N=not a store and forward trip)\n* trip_duration - duration of the trip in seconds","cfb20416":"**OSRM Features**\n\nWe had only rough distance estimates in the previous versions. Now we use better fastest route distance estimates between pickup and dropoff.","14e43a28":"**SPEED**","04b277e4":"Even the top mean difference is less than 1% of the standard deviation. We have a few missing values but the missing rates are the same. Fortunately xgboost can handle missing values."}}