{"cell_type":{"099118e4":"code","301baaf3":"code","1e87de96":"code","d06eafe1":"code","c922e089":"code","d99f12bf":"code","08995c59":"code","adb1942b":"code","fb901f8c":"code","48179813":"code","d3a950ff":"code","49e5242c":"code","12b6d932":"code","0f9e26c8":"code","b0baab25":"code","37b66052":"code","8a3a3668":"code","af0648f4":"code","ac24d469":"code","dd37d274":"code","1666d72e":"code","0b3e37c4":"code","d9ef90f2":"code","c03d961f":"code","c3210730":"code","686ded10":"code","130a4d31":"code","ab8fdd30":"code","692d1916":"code","52a6408b":"code","b5a55df3":"code","3dc18a48":"code","b52e52c6":"code","679f78ec":"code","66e478e9":"code","1f8fac97":"code","792988ac":"code","e9f42417":"code","5f399717":"code","ce45393a":"code","12c325c4":"code","51c7f5a0":"code","accc1f9f":"code","e8b5340d":"code","f66b7474":"code","402a0f0e":"code","f5494dc8":"code","b74d1f07":"code","e6e05b62":"code","3c218b8c":"code","28093001":"code","f52f55cd":"code","5c6c4de3":"code","0436c102":"code","4981a3f6":"code","48b5aef4":"code","0c51678b":"code","1423e007":"code","d354d77f":"code","0c1014b4":"code","a2c6a4e3":"code","3b5e563b":"code","1910f06e":"code","d7c9cc7d":"code","582343e8":"code","0b9648dc":"code","92f99ea5":"code","b34f112e":"code","9cd58555":"code","6a0a1130":"code","effa8717":"code","fc3fef92":"code","79cc8bcb":"code","5705f99e":"code","9fbb3d35":"code","70d13a07":"code","e44e9d55":"code","970e8434":"code","4bebc60f":"code","3ec8682d":"code","a0d4df85":"code","9e579924":"code","7ac6bbde":"code","cd44ed73":"code","ecddbd61":"code","c7aae780":"code","68762e39":"code","64304ead":"code","2a87bf0d":"code","21028839":"code","5d96dc0d":"code","e4ef8e36":"code","da33fba3":"code","4676a84d":"code","9929e667":"code","03553bae":"code","21f763e2":"code","b60da8be":"code","98d8b070":"code","176a971d":"code","5af51238":"code","57debb1e":"code","d9007def":"code","8f744476":"code","b5042fdc":"code","92aa69af":"code","b7f69528":"code","ea88412c":"code","413754eb":"code","cc009382":"code","8870c140":"code","0c515019":"code","7fb463dc":"code","08fe55ee":"code","a4c67340":"code","edc3ff4d":"code","df359dfa":"code","0fce6be7":"code","aa8ecc7a":"code","b4ccef3d":"code","97fdc9cf":"code","774e96e6":"code","23c0d09d":"code","09a778a7":"code","38d437dc":"code","f1b62c9b":"code","03c28300":"code","9d3b0d23":"code","839f15e7":"code","ba35c70b":"code","7423328c":"code","375d263c":"code","8473ffdf":"code","15f7d498":"markdown","f3a9652b":"markdown","7343a391":"markdown","68ee0ae4":"markdown","87e95acb":"markdown","26fedb3d":"markdown","69fcae8b":"markdown","c2c1cf2e":"markdown","0341ade1":"markdown","eeeb35f7":"markdown","7635abfa":"markdown","659e8417":"markdown","0a65315a":"markdown","3b505466":"markdown","33d99791":"markdown","308be039":"markdown","c88e6586":"markdown","977c244e":"markdown","917546ab":"markdown","1bb255d5":"markdown","e8d6487d":"markdown"},"source":{"099118e4":"# Train and test datset was given to work as a project on the poverty level prediction. Source : Kaggle\n\n#Steps to be be followed.\n#a. Explore data and perform data visualization\n#b. Fill in missing values (NULL values) either using mean or median (if the attribute is numeric) or most-frequently occurring value if the attribute is 'object' or categorical.\n#b. Perform feature engineering, may be using some selected features and only from numeric features.\n#c. Scale numeric features, AND IF REQUIRED, perform One HOT Encoding of categorical features\n#d. IF number of features is very large, please do not forget to do PCA.\n#e. Select some estimators for your work.\n\n\n#Modelling tried below\n#       GradientBoostingClassifier\n#        RandomForestClassifier\n#        KNeighborsClassifier\n#        XGBoost\n#        LightGBM\n        \n#        followed by perform tuning using Bayesian Optimization after each modelling.\n\n\n","301baaf3":"%reset -f","1e87de96":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","d06eafe1":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import  OneHotEncoder as ohe\nfrom sklearn.preprocessing import StandardScaler as ss\nfrom sklearn.compose import ColumnTransformer as ct\nfrom sklearn.impute import SimpleImputer\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\n#from sklearn.model_selection import train_test_split","c922e089":"from sklearn.model_selection import train_test_split","d99f12bf":"from sklearn.ensemble import RandomForestClassifier as rf\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier as gbm\nfrom xgboost.sklearn import XGBClassifier\nimport lightgbm as lgb\n","08995c59":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import auc, roc_curve\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import average_precision_score\nimport sklearn.metrics as metrics\nfrom xgboost import plot_importance\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix","adb1942b":"from bayes_opt import BayesianOptimization\nfrom skopt import BayesSearchCV\nfrom eli5.sklearn import PermutationImportance","fb901f8c":"import os\nimport time\nimport gc\nimport random\nfrom scipy.stats import uniform\nimport warnings","48179813":"#path=\"E:\\\\bigdata\\\\costa rica assignment\\\\costa-rican-household-poverty-prediction\\\\\"\n#file_name=\"train.csv\"\n#file_path_name =path+file_name\n#print(file_path_name)\n#train=pd.read_csv(file_path_name)\ntrain=pd.read_csv(\"..\/input\/train.csv\")\nprint('executing')                  \ntrain.info()","d3a950ff":"train.head()","49e5242c":"train.shape","12b6d932":"#path=\"E:\\\\bigdata\\\\costa rica assignment\\\\costa-rican-household-poverty-prediction\\\\\"\n#file_name=\"test.csv\"\n#file_path_name =path+file_name\n#print(file_path_name)\n#test=pd.read_csv(file_path_name)\ntest=pd.read_csv(\"..\/input\/test.csv\")\nprint('executing') \ntest.info()","0f9e26c8":"test.head()","b0baab25":"test.shape","37b66052":"ids=test['Id']","8a3a3668":"sns.countplot(\"Target\", data = train)","af0648f4":" sns.countplot(x=\"v2a1\",hue=\"Target\",data=train)","ac24d469":"sns.countplot(x=\"r4t3\",hue=\"Target\",data=train)  #Total persons in the household","dd37d274":"pd.value_counts(train['Target']).plot.bar()\nplt.title('total persons in household histogram')\nplt.xlabel('r4t3')\nplt.ylabel('count')\ntrain['Target'].value_counts()","1666d72e":"sns.countplot(x=\"tamhog\",hue=\"Target\",data=train) #tamhog, size of the household","0b3e37c4":"sns.countplot(x=\"tamviv\",hue=\"Target\",data=train)#tamviv, number of persons living in the household","d9ef90f2":"train.select_dtypes('object').head()","c03d961f":"yes_no_map = {'no':0,'yes':1}\ntrain['dependency'] = train['dependency'].replace(yes_no_map).astype(np.float32)\ntrain['edjefe'] = train['edjefe'].replace(yes_no_map).astype(np.float32)\ntrain['edjefa'] = train['edjefa'].replace(yes_no_map).astype(np.float32)","c3210730":"test.select_dtypes('object').head()","686ded10":"yes_no_map = {'no':0,'yes':1}\ntest['dependency'] = test['dependency'].replace(yes_no_map).astype(np.float32)\ntest['edjefe'] = test['edjefe'].replace(yes_no_map).astype(np.float32)\ntest['edjefa'] = test['edjefa'].replace(yes_no_map).astype(np.float32)","130a4d31":"train[[\"dependency\",\"edjefe\",\"edjefa\"]].describe()","ab8fdd30":"test[[\"dependency\",\"edjefe\",\"edjefa\"]].describe()","692d1916":"train.select_dtypes('float64').head()","52a6408b":"missing = pd.DataFrame(train.isnull().sum()).rename(columns = {0: 'total'})\nmissing.sort_values('total', ascending = False).head(10)","b5a55df3":"test.select_dtypes('float64').head()","3dc18a48":"missingT = pd.DataFrame(test.isnull().sum()).rename(columns = {0: 'total'})\nmissingT.sort_values('total', ascending = False).head(10)","b52e52c6":"train['rez_esc'] = train['rez_esc'].fillna(0)\ntrain['v18q1'] = train['v18q1'].fillna(0)\ntrain['v2a1'] = train['v2a1'].fillna(0)\ntrain['SQBmeaned'] = train['SQBmeaned'].fillna(0)\ntrain['meaneduc'] = train['meaneduc'].fillna(0)","679f78ec":"test['rez_esc'] = test['rez_esc'].fillna(0)\ntest['v18q1'] = test['v18q1'].fillna(0)\ntest['v2a1'] = test['v2a1'].fillna(0)\ntest['SQBmeaned'] = test['SQBmeaned'].fillna(0)\ntest['meaneduc'] = test['meaneduc'].fillna(0)","66e478e9":"missing = pd.DataFrame(train.isnull().sum()).rename(columns = {0: 'total'})\nmissing.sort_values('total', ascending = False).head(10)","1f8fac97":"missingT = pd.DataFrame(test.isnull().sum()).rename(columns = {0: 'total'})\nmissingT.sort_values('total', ascending = False).head(10)","792988ac":"train.drop(['Id','idhogar'], inplace = True, axis =1)\ntest.drop(['Id','idhogar'], inplace = True, axis =1)","e9f42417":"train.shape\n","5f399717":"test.shape","ce45393a":"y = train.iloc[:,140]\ny.unique()\n\n","12c325c4":"X = train.iloc[:,1:141]\nX.shape","51c7f5a0":"my_imputer = SimpleImputer()\nX = my_imputer.fit_transform(X)\nscale = ss()\nX = scale.fit_transform(X)","accc1f9f":"X.shape","e8b5340d":"my_imputer = SimpleImputer()\ntest = my_imputer.fit_transform(test)\nscale = ss()\ntest = scale.fit_transform(test)\n","f66b7474":"X_train, X_test, y_train, y_test = train_test_split(\n                                                    X,\n                                                    y,\n                                                    test_size = 0.3,\n                                                    random_state=1)\n","402a0f0e":"from sklearn.ensemble import RandomForestClassifier as rf","f5494dc8":"modelrf = rf()","b74d1f07":"start = time.time()\nmodelrf = modelrf.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","e6e05b62":"classes = modelrf.predict(X_test)","3c218b8c":"(classes == y_test).sum()\/y_test.size","28093001":"f1 = f1_score(y_test, classes, average='macro')\nf1","f52f55cd":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    rf(\n       n_jobs = 2         # No need to tune this parameter value\n      ),\n\n    # 2.12 Specify estimator parameters that you would like to change\/tune\n    {\n        'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        'criterion': ['gini', 'entropy'],     # Specify categorical parameters as here\n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    # 2.13\n    n_iter=32,            # How many points to sample\n    cv = 3                # Number of cross-validation folds\n)","5c6c4de3":"bayes_cv_tuner.fit(X_train, y_train)","0436c102":"bayes_cv_tuner.best_params_","4981a3f6":"modelrfTuned=rf(criterion=\"entropy\",\n               max_depth=77,\n               max_features=64,\n               min_weight_fraction_leaf=0.0,\n               n_estimators=500)","48b5aef4":"start = time.time()\nmodelrfTuned = modelrfTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","0c51678b":"yrf=modelrfTuned.predict(X_test)","1423e007":"yrf","d354d77f":"yrftest=modelrfTuned.predict(test)","0c1014b4":"yrftest","a2c6a4e3":"bayes_cv_tuner.best_score_","3b5e563b":"bayes_cv_tuner.score(X_test, y_test)","1910f06e":"bayes_cv_tuner.cv_results_['params']","d7c9cc7d":"from sklearn.neighbors import KNeighborsClassifier\nmodelneigh = KNeighborsClassifier(n_neighbors=4)","582343e8":"start = time.time()\nmodelneigh = modelneigh.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","0b9648dc":"classes = modelneigh.predict(X_test)\n\nclasses","92f99ea5":"(classes == y_test).sum()\/y_test.size","b34f112e":"f1 = f1_score(y_test, classes, average='macro')\nf1","9cd58555":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    KNeighborsClassifier(\n       n_neighbors=4         # No need to tune this parameter value\n      ),\n    {\"metric\": [\"euclidean\", \"cityblock\"]},\n    n_iter=32,            # How many points to sample\n    cv = 2            # Number of cross-validation folds\n   )","6a0a1130":"bayes_cv_tuner.fit(X_train, y_train)","effa8717":"bayes_cv_tuner.best_params_","fc3fef92":"modelneighTuned = KNeighborsClassifier(n_neighbors=4,\n               metric=\"cityblock\")","79cc8bcb":"start = time.time()\nmodelneighTuned = modelneighTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","5705f99e":"yneigh=modelneighTuned.predict(X_test)","9fbb3d35":"yneightest=modelneighTuned.predict(test)","70d13a07":"bayes_cv_tuner.best_score_","e44e9d55":"bayes_cv_tuner.score(X_test, y_test)","970e8434":"bayes_cv_tuner.cv_results_['params']","4bebc60f":"from sklearn.ensemble import GradientBoostingClassifier as gbm\nmodelgbm=gbm()","3ec8682d":"start = time.time()\nmodelgbm = modelgbm.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","a0d4df85":"classes = modelgbm.predict(X_test)\n\nclasses","9e579924":"(classes == y_test).sum()\/y_test.size ","7ac6bbde":"f1 = f1_score(y_test, classes, average='macro')\nf1","cd44ed73":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    gbm(\n               # No need to tune this parameter value\n      ),\n\n    # 2.12 Specify estimator parameters that you would like to change\/tune\n    {\n        'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        \n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    # 2.13\n    n_iter=32,            # How many points to sample\n    cv = 2                # Number of cross-validation folds\n)","ecddbd61":"bayes_cv_tuner.fit(X_train, y_train)","c7aae780":"bayes_cv_tuner.best_params_","68762e39":"modelgbmTuned=gbm(\n               max_depth=84,\n               max_features=11,\n               min_weight_fraction_leaf=0.04840,\n               n_estimators=489)","64304ead":"start = time.time()\nmodelgbmTuned = modelgbmTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","2a87bf0d":"ygbm=modelgbmTuned.predict(X_test)","21028839":"ygbmtest=modelgbmTuned.predict(test)","5d96dc0d":"bayes_cv_tuner.best_score_","e4ef8e36":"bayes_cv_tuner.score(X_test, y_test)","da33fba3":"bayes_cv_tuner.cv_results_['params']","4676a84d":"from xgboost.sklearn import XGBClassifier\nmodelxgb=XGBClassifier()","9929e667":"start = time.time()\nmodelxgb = modelxgb.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","03553bae":"classes = modelxgb.predict(X_test)\n\nclasses","21f763e2":"(classes == y_test).sum()\/y_test.size ","b60da8be":"f1 = f1_score(y_test, classes, average='macro')\nf1","98d8b070":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    XGBClassifier(\n       n_jobs = 2         # No need to tune this parameter value\n      ),\n\n    # 2.12 Specify estimator parameters that you would like to change\/tune\n    {\n        'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        'criterion': ['gini', 'entropy'],     # Specify categorical parameters as here\n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    # 2.13\n    n_iter=32,            # How many points to sample\n    cv = 3                # Number of cross-validation folds\n)","176a971d":"bayes_cv_tuner.fit(X_train, y_train)","5af51238":"bayes_cv_tuner.best_params_","57debb1e":"modelxgbTuned=XGBClassifier(criterion=\"gini\",\n               max_depth=4,\n               max_features=15,\n               min_weight_fraction_leaf=0.05997,\n               n_estimators=499)","d9007def":"start = time.time()\nmodelxgbTuned = modelxgbTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","8f744476":"yxgb=modelxgbTuned.predict(X_test)\n","b5042fdc":"yxgbtest=modelxgbTuned.predict(test)","92aa69af":"bayes_cv_tuner.best_score_","b7f69528":"bayes_cv_tuner.score(X_test, y_test)","ea88412c":"bayes_cv_tuner.cv_results_['params']","413754eb":"import lightgbm as lgb\nmodellgb = lgb.LGBMClassifier(max_depth=-1, learning_rate=0.1, objective='multiclass',\n                             random_state=None, silent=True, metric='None', \n                             n_jobs=4, n_estimators=5000, class_weight='balanced',\n                             colsample_bytree =  0.93, min_child_samples = 95, num_leaves = 14, subsample = 0.96)","cc009382":"start = time.time()\nmodellgb = modellgb.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","8870c140":"classes = modellgb.predict(X_test)\n\nclasses","0c515019":"(classes == y_test).sum()\/y_test.size ","7fb463dc":"f1 = f1_score(y_test, classes, average='macro')\nf1","08fe55ee":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    lgb.LGBMClassifier(\n       n_jobs = 2         # No need to tune this parameter value\n      ),\n\n    # 2.12 Specify estimator parameters that you would like to change\/tune\n    {\n        'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        'criterion': ['gini', 'entropy'],     # Specify categorical parameters as here\n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    # 2.13\n    n_iter=32,            # How many points to sample\n    cv = 3                # Number of cross-validation folds\n)","a4c67340":"bayes_cv_tuner.fit(X_train, y_train)","edc3ff4d":"bayes_cv_tuner.best_params_","df359dfa":"modellgbTuned = lgb.LGBMClassifier(criterion=\"gini\",\n               max_depth=5,\n               max_features=53,\n               min_weight_fraction_leaf=0.01674,\n               n_estimators=499)","0fce6be7":"start = time.time()\nmodellgbTuned = modellgbTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","aa8ecc7a":"ylgb=modellgbTuned.predict(X_test)","b4ccef3d":"ylgbtest=modellgbTuned.predict(test)","97fdc9cf":"bayes_cv_tuner.best_score_","774e96e6":"bayes_cv_tuner.score(X_test, y_test)","23c0d09d":"bayes_cv_tuner.cv_results_['params']","09a778a7":"#                                                     Summary\n#                                 ACCURACY                   f1            ACCURACY\n#                            with default parameters       score          with  parameters tuned with Bayesian                                                                                             Optimization \n#    RandomForestClassifier         0.96                    0.93              1.0\n#    KNeighborsClassifier           0.81                    0.70              0.77 \n#    GradientBoostingClassifier     1.0                     1.0               1.0 \n#    XGBoost                        1.0                     1.0               1.0\n#    LightGBM                       1.0                     1.0               1.0","38d437dc":"NewTrain = pd.DataFrame()\nNewTrain['yrf'] = yrf.tolist()\nNewTrain['yneigh'] = yneigh.tolist()\nNewTrain['ygbm'] = ygbm.tolist()\nNewTrain['yxgb'] = yxgb.tolist()\nNewTrain['ylgb'] = ylgb.tolist()\n\nNewTrain.head(5), NewTrain.shape","f1b62c9b":"NewTest = pd.DataFrame()\nNewTest['yrf'] = yrftest.tolist()\nNewTest['yneigh'] = yneightest.tolist()\nNewTest['ygbm'] = ygbmtest.tolist()\nNewTest['yxgb'] = yxgbtest.tolist()\nNewTest['ylgb'] = ylgbtest.tolist()\n\nNewTest.head(5), NewTest.shape","03c28300":"NewModel=rf(criterion=\"entropy\",\n               max_depth=77,\n               max_features=5,\n               min_weight_fraction_leaf=0.0,\n               n_estimators=500)","9d3b0d23":"start = time.time()\nNewModel = NewModel.fit(NewTrain, y_test)\nend = time.time()\n(end-start)\/60","839f15e7":"ypredict=NewModel.predict(NewTest)","ba35c70b":"ylgbtest","7423328c":"submit=pd.DataFrame({'Id': ids, 'Target': ylgbtest})\nsubmit.head(5)","375d263c":"submit.to_csv('submit.csv', index=False)","8473ffdf":"#submit.to_csv('E:\/bigdata\/costa rica assignment\/Nagendrafinalsubmit.csv', index=False)","15f7d498":"# Modelling with Light Gradient Booster","f3a9652b":"# Costa Rican Household Poverty Level Prediction","7343a391":"Drop columns in train data","68ee0ae4":"scaling and PCA","87e95acb":"now we will convert these columns rez_esc,v18q1,v2a1,SQBmeaned andcmeaneduc to zero","26fedb3d":"target and predictors","69fcae8b":"now convert all null \"NaN\" to  0","c2c1cf2e":"# Plotting using train data","0341ade1":"Performing tuning using Bayesian Optimization.","eeeb35f7":"# Begin Data Modelling using Train Test Split","7635abfa":"# Modelling with KNeighborsClassifier","659e8417":"# Modelling with XGBClassifier","0a65315a":" Performing tuning using Bayesian Optimization.","3b505466":"Identify the Object data types to convert \"yes\" and \"no\" to 1 and 0","33d99791":"Performing tuning using Bayesian Optimization.","308be039":"# Modelling using Random Forest","c88e6586":"# Modelling with GradientBoostingClassifier","977c244e":"Performing tuning using Bayesian Optimization","917546ab":"view float64 data type variables","1bb255d5":"Performing tuning using Bayesian Optimization.","e8d6487d":"double check if all the above columns are converted"}}