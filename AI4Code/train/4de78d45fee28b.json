{"cell_type":{"bddca186":"code","50c602be":"code","7e71c393":"code","56382031":"code","6a887e4a":"code","cf927e97":"code","76720956":"code","e706757a":"code","e8781e94":"code","c0f6e74a":"code","48403bac":"code","998e1aba":"code","7056953a":"code","77610c86":"code","33b711f5":"code","19979889":"code","1d6b430f":"code","d48b9d3d":"code","3f0aaa15":"code","12c61f93":"code","e165d0ac":"code","8e538793":"code","363ac60e":"code","840feda3":"code","e4df3f55":"code","1f792b05":"code","d433f7b2":"code","bc7d2dac":"code","4a9f7236":"code","e6949640":"code","dfeae636":"code","a3ac03f8":"code","51f1e340":"code","2382888c":"code","35e856d3":"code","daa1003e":"code","33b5f11d":"code","c4be79b2":"code","83810f5f":"code","cc09f8be":"code","19a44c59":"code","cb18fad9":"code","e0cbab6d":"code","a9d4909e":"code","cc5cb692":"code","cbd843fb":"code","744b943e":"code","2d7c3d0f":"code","77e57b95":"code","2cc22e4f":"code","8ef802aa":"code","de458583":"markdown","b77d6797":"markdown","30608b94":"markdown","399197e6":"markdown","14ab8ff8":"markdown","b7944715":"markdown","d27b661d":"markdown","c092e7f0":"markdown","69e3224e":"markdown","710e46f7":"markdown","99caa42f":"markdown","f9583d43":"markdown","1cbc24c0":"markdown","4b066b49":"markdown","2ea23324":"markdown"},"source":{"bddca186":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","50c602be":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nimport seaborn as sns\nimport cufflinks as cf\nimport plotly.express as px\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected = True)\ncf.go_offline() \n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error","7e71c393":"#Importing data\ndf = pd.read_csv(\"\/kaggle\/input\/real-or-fake-fake-jobposting-prediction\/fake_job_postings.csv\")\ndf.head()\n","56382031":"df.isnull().values.any()","6a887e4a":"df.isnull().sum()","cf927e97":"df.shape","76720956":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\ndef Labelencoder_feature(x):\n    le=LabelEncoder()\n    x=le.fit_transform(x)\n    return x\ndf=df.apply(Labelencoder_feature)\ndf.head()","e706757a":"feature=df.drop(columns=['fraudulent'])\nlabel=df['fraudulent']","e8781e94":"from scipy.stats import pearsonr\n\ncorre=pd.DataFrame()\n\nfor i in feature.columns:\n    corre[i]= pearsonr(label, feature[i])\n    \n    \ncorre","c0f6e74a":"corre1=corre.T\ncorre1\ncorre1","48403bac":"corre1=corre.T\ncorre1\ncoore2= corre1.iloc[:,0].sort_values(ascending=False)\ncoore2\ncoore2= corre1.iloc[:,0].sort_values(ascending=False)\ncoore2","998e1aba":"feature=df.drop(columns=['fraudulent'])\nlabel=df['fraudulent']\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import accuracy_score\nTransf_pca = PCA(n_components= 10)\ndatanew = Transf_pca.fit_transform(feature)\nprint(datanew)\ndatanew.shape","7056953a":"label=pd.DataFrame(label)\nlabel.shape\nnewbase=np.concatenate((datanew,label),axis=1)\nprint(newbase)\nnewbase.shape\nnewbase=pd.DataFrame(newbase)\nprint(newbase.columns)","77610c86":"dataset_selected1=newbase.loc[newbase[10].isin([1])]\ndataset_selected0=newbase.loc[newbase[10].isin([0])]\n\n\n#on va diviser dataset_selected1 en 2 partie data1 et label1\nlabel1=dataset_selected1[10]\ndata1=dataset_selected1.drop([10],axis=1)\n\n#on va diviser dataset_selected0 en 2 partie data0 et label0\nlabel0=dataset_selected0[10]\n#remplacer 0 par -1\nlabel0=label0-1\ndata0=dataset_selected0.drop([10],axis=1)","33b711f5":"from sklearn.model_selection import train_test_split\nx_train1,x_test1,y_train1,y_test1=train_test_split(data1,label1,test_size=0.33,random_state=0)\nfrom sklearn import svm\nmodel=svm.OneClassSVM(kernel='rbf',nu=1,gamma=0.00001)\nimport time\ndebut=time.time()\nmodel.fit(x_train1)\nfin=time.time()-debut","19979889":"import numpy as np\ndata_tesst=np.concatenate((x_test1,data0),axis=0)\nlabel_tesst=np.concatenate((y_test1,label0),axis=0)\npred=model.predict(data_tesst)","1d6b430f":"from sklearn.metrics import accuracy_score\nACC=accuracy_score(label_tesst,pred)*100\nprint('the accurency score is :')\nprint(ACC)","d48b9d3d":"from sklearn.metrics import classification_report\nprint(classification_report(label_tesst,pred))","3f0aaa15":"print(newbase)\nlabel=newbase[10]\ndata=newbase.drop([10],axis=1)\n","12c61f93":"label.value_counts()","e165d0ac":"from imblearn.over_sampling import RandomOverSampler\nimport imblearn\nprint(imblearn.__version__)\n\n# define oversampling strategy\noversample = RandomOverSampler(sampling_strategy='minority')\n\n# define oversampling strategy\noversample = RandomOverSampler(sampling_strategy=1.0)\n\n# fit and apply the transform\nX_over, y_over = oversample.fit_resample(feature, label)\n#compter combien de 1 et de 0 dans dataset\nlabel=y_over\ndata=X_over\nlabel =pd.DataFrame(label)\nlabel.value_counts()","8e538793":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\ndtree = DecisionTreeClassifier()\nx_train,x_test,y_train,y_test = train_test_split(data,label,test_size = 0.3,random_state = 0)\nimport time\ndebut=time.time()\ndtree.fit(x_train,y_train)\nfin=time.time()-debut\nprediction = dtree.predict(x_test)","363ac60e":"from sklearn.metrics import accuracy_score\nACC=accuracy_score(y_test,prediction)*100\nprint('With decision tree accuracy is: ',ACC) # accuracy","840feda3":"#Importing data\ndf = pd.read_csv(\"\/kaggle\/input\/real-or-fake-fake-jobposting-prediction\/fake_job_postings.csv\")\ndf.head()","e4df3f55":"import warnings\nwarnings.filterwarnings('ignore')\n\n\n\nfrom wordcloud import WordCloud , ImageColorGenerator\n\nfrom nltk.corpus import stopwords\nfrom textblob import TextBlob","1f792b05":"df=df[['description', 'fraudulent']]\n\ndf_real=df.loc[df['fraudulent']==0]\ndf_fake=df.loc[df['fraudulent']==1]","d433f7b2":"df_fake.shape","bc7d2dac":"df['description'] = df['description'].astype('str')\ndf_fake['description'] = df_fake['description'].astype('str')","4a9f7236":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\ntarget_text = \" \".join(df['description'])\nwordcloud = WordCloud(width=1400, height=700).generate(text=target_text)\nplt.figure(figsize=(30,18))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","e6949640":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\ntarget_text = \" \".join(df_fake['description'])\nwordcloud = WordCloud(width=1400, height=700).generate(text=target_text)\nplt.figure(figsize=(30,18))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","dfeae636":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\ntarget_text = \" \".join(df_real['description'])\nwordcloud = WordCloud(width=1400, height=700).generate(text=target_text)\nplt.figure(figsize=(30,18))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","a3ac03f8":"from wordcloud import WordCloud  \n\n#for text processing\nimport string\nimport re\nimport nltk\nfrom textblob import TextBlob\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\n#for feature selection\nfrom sklearn import decomposition\n\n#for model building\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report","51f1e340":"def clean_data(text):\n    text = text.lower()  # convert all the text into lowercase\n    text = text.strip()  #remove starting and trailing whitespaces\n    special_char_reg = '([a-zA-Z0-9]+)' + '[!\"#$%&\\'()*+,-.\/:;<=>?@\\\\^_`{|}~]' + '([a-zA-Z0-9]+)'\n    text = re.sub(special_char_reg, ' ', text)\n    text = re.sub(r'\\s+', ' ', text) #remove all line formattings\n    text = re.sub(r'\\d+', '', text) #remove digits\n    text = re.sub(r'\\w*\\d\\w*', '', text)\n    text = re.sub(r\"\\w+\u2026|\u2026\", \"\", text)  # Remove ellipsis (and last word)\n    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n    text = ''.join(c for c in text if c not in string.punctuation)   #remove pecial symbols from job titles\n    return text","2382888c":"df_des=df['description']\ndf_des = df_des.apply(lambda x : clean_data(x))\ndf_des.head()","35e856d3":"from nltk import WordNetLemmatizer\ndef lemma(text):\n    word_list = nltk.word_tokenize(text) #tokenize beofre lemmatization\n    lemma_output = ' '.join(WordNetLemmatizer().lemmatize(word) for word in word_list)\n    return lemma_output\ndf_des_lemma = df_des.apply(lambda x : lemma(x))\n","daa1003e":"\nfrom nltk import word_tokenize\ndef remove_stopwords_and_tokenize(text):\n    the_stopwords = set(stopwords.words(\"english\"))\n    tokens = word_tokenize(text)  # tokenize \n    tokens = [t for t in tokens if not t in the_stopwords]  \n    tokens = [t for t in tokens if len(t) > 1] \n    return tokens","33b5f11d":"df_des_token = df_des_lemma.apply(lambda x : remove_stopwords_and_tokenize(x))\ndf_des_token.head()","c4be79b2":"#Tokenization using count vectorizer\ncount_vect = CountVectorizer(ngram_range=(1,1))\ntoken = count_vect.fit_transform(df_des_lemma)\ntoken","83810f5f":"print('Total number of tokens\/words in all the tweets - ', len(count_vect.get_feature_names()))","cc09f8be":"from gensim.corpora import Dictionary\n\nid2word = Dictionary(df_des_token)\n\ntexts =df_des_token\n\ncorpus = [id2word.doc2bow(text) for text in texts]\n\ncorpus[:1]","19a44c59":"from gensim.models import LdaModel\nlda_model = LdaModel(corpus=corpus,\n                    id2word=id2word,\n                    num_topics=20, \n                    random_state=100,\n                    update_every=1,\n                    chunksize=100,\n                    passes=10,\n                    alpha='auto',\n                    per_word_topics=True)","cb18fad9":"import pyLDAvis\nimport pyLDAvis.gensim_models as gensimvis\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Visualize the topics\npyLDAvis.enable_notebook()\nvis = gensimvis.prepare(lda_model, corpus, id2word)\nvis","e0cbab6d":"df.isnull().sum()","a9d4909e":"df['fraudulent'].value_counts()","cc5cb692":"X =df['description']\ny=df['fraudulent']\n\n\nmy_tfidf = TfidfVectorizer(stop_words='english', max_df=0.7)\n\n# fit the vectorizer and transform X_train into a tf-idf matrix,\n# then use the same vectorizer to transform X_test\ntfidf_data = my_tfidf.fit_transform(X)","cbd843fb":"from imblearn.over_sampling import RandomOverSampler\nimport imblearn\nprint(imblearn.__version__)\n\n# define oversampling strategy\noversample = RandomOverSampler(sampling_strategy='minority')\n\n# define oversampling strategy\noversample = RandomOverSampler(sampling_strategy=1.0)\n\n\n# fit and apply the transform\nX_over, y_over = oversample.fit_resample(tfidf_data, y)\n#compter combien de 1 et de 0 dans dataset\nlabel=y_over\ndata=X_over\nlabel =pd.DataFrame(label)\nlabel.value_counts()","744b943e":"X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.25)","2d7c3d0f":"from sklearn.ensemble import RandomForestClassifier\npa_rf = RandomForestClassifier()\npa_rf.fit(X_train, y_train)","77e57b95":"y_pred = pa_rf.predict(X_test)\nprint('ROC-AUC Score for train dataset : ' , metrics.roc_auc_score(y_test, y_pred))\nprint('ROC-AUC Score for validation dataset : ' , metrics.roc_auc_score(y_test, y_pred))","2cc22e4f":"print('Accuracy Score for train dataset : ' , metrics.accuracy_score(y_test, y_pred))\nprint('Accuracy Score for test dataset : ' , metrics.accuracy_score(y_test, y_pred))","8ef802aa":"conn_cm_test = metrics.confusion_matrix(y_test, y_pred, [1,0])\nsns.heatmap(conn_cm_test, fmt= '.2f', annot=True)","de458583":"# finding the Correlation of features and target","b77d6797":"predict fake jobs after using PCA to reduce dimension","30608b94":"Check if we have Imbalanced Data","399197e6":"extreme imbalanced data\n\nUsing RandomOversampling to deal with imabalanced data","14ab8ff8":"check the NaN Values","b7944715":"we can use sklearn.preprocessing.LabelEncoder to convert all categorial features to numeric and also consider Nan-Values as a separate category","d27b661d":"separating fake and real jobs and do a prediction using OneClass SVM for fake jobs","c092e7f0":"# LDA Model","69e3224e":"Applying Decision Tree Classifier to predict fakeor real posting jobs","710e46f7":"# Forecasting","99caa42f":"# RandomForestClassifier","f9583d43":"# Sentimental analysis","1cbc24c0":"# Reducing the dimension of data","4b066b49":"Too Many Nan-Values in data. cannot delete all of it","2ea23324":"As we can see, the first 9 features have larger correlation in compare with other features"}}