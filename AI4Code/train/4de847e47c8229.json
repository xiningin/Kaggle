{"cell_type":{"7f343657":"code","bf202230":"code","c9265bad":"code","329e070f":"code","1f4dd2bf":"code","08d299b9":"code","3106f383":"code","de25de02":"code","f2f9c59b":"code","46cfb0d1":"code","e9b8b51c":"code","8989ec99":"code","03648f19":"code","0213f1ee":"code","90d9ce90":"code","671b975d":"code","667e3a92":"code","eea007fd":"code","692b352f":"code","c180a50e":"code","0d213403":"code","b7791353":"code","d0411aef":"code","527f044b":"code","9195c4da":"code","343088c9":"code","f29fb25e":"code","e0bbb810":"markdown","730c8246":"markdown","ea4b866a":"markdown","a014711c":"markdown","a89608b8":"markdown","26da751b":"markdown"},"source":{"7f343657":"pip install sentence-transformers","bf202230":"import torch\nimport pandas as pd\nfrom sentence_transformers import evaluation,losses, SentenceTransformer, util, models, InputExample\nfrom torch.utils.data import DataLoader","c9265bad":"import urllib.request\nimport zipfile\n\nurl = \"https:\/\/github.com\/sdadas\/polish-roberta\/releases\/download\/models-transformers-v3.4.0\/roberta_base_transformers.zip\"\nextract_dir = \"\/kaggle\/roberta_base\"\n\nzip_path, _ = urllib.request.urlretrieve(url)\nwith zipfile.ZipFile(zip_path, \"r\") as f:\n    f.extractall(extract_dir)","329e070f":"word_embedding_model = models.Transformer('\/kaggle\/roberta_base', max_seq_length=512)\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\nmodel = SentenceTransformer(modules=[word_embedding_model, pooling_model])","1f4dd2bf":"\nsentences1 = 'Pi\u0142ka no\u017cna z wieloma graj\u0105cymi facetami'\nsentences2 = 'Jacy\u015b m\u0119\u017cczy\u017ani graj\u0105 w futbol'\nsentences3 = 'Kobiety id\u0105 do fryzjera'\n\n#Compute embedding for both lists\n%time embeddings1 = model.encode(sentences1, convert_to_tensor=True)\nembeddings2 = model.encode(sentences2, convert_to_tensor=True)\nembeddings3 = model.encode(sentences3, convert_to_tensor=True)\n\n#Compute cosine-similarits                       \n%time print(util.pytorch_cos_sim(embeddings1, embeddings2).item())\nprint(util.pytorch_cos_sim(embeddings3, embeddings2).item()) \nprint(util.pytorch_cos_sim(embeddings1, embeddings3).item()) ","08d299b9":"# model.evaluate(evaluator)\n# 0.7986272749202373","3106f383":"import urllib.request\nimport os\n\nurltrain = 'http:\/\/git.nlp.ipipan.waw.pl\/Scwad\/SCWAD-CDSCorpus\/raw\/master\/CDSCorpus\/CDS_train.csv'\nurltest = 'http:\/\/git.nlp.ipipan.waw.pl\/Scwad\/SCWAD-CDSCorpus\/raw\/master\/CDSCorpus\/CDS_test.csv'\n\nif not os.path.isdir('\/kaggle\/working\/data\/'):\n        os.makedirs('\/kaggle\/working\/data\/')\n        \nurllib.request.urlretrieve(urltrain, '\/kaggle\/working\/data\/CDS_train.csv')\nurllib.request.urlretrieve(urltest, '\/kaggle\/working\/data\/CDS_test.csv')\n\ncorpus = pd.read_csv('\/kaggle\/working\/data\/CDS_train.csv',sep='\\t',error_bad_lines=False, encoding='utf-8')# ,nrows=1000  \ncorpus_test = pd.read_csv('\/kaggle\/working\/data\/CDS_test.csv',sep='\\t',error_bad_lines=False, encoding='utf-8')# ,nrows=1000  \n","de25de02":"corpus['relatedness_score'] = corpus['relatedness_score'].div(5)\ncorpus_test['relatedness_score'] = corpus_test['relatedness_score'].div(5)\n\ncorpus.head()","f2f9c59b":"s1=[]\ns2=[]\nsc=[]\n\nfor index, row in corpus_test.iterrows():\n    s1.append(row['sentence_A'])\n    s2.append(row['sentence_B'])\n    sc.append(row['relatedness_score'])\n\nevaluator = evaluation.EmbeddingSimilarityEvaluator(s1, s2, sc)# then change to corpus_test data\n","46cfb0d1":"train_examples = []\ntest_examples = []\ns3 = []\nfor index, row in corpus.iterrows():\n    train_examples.append(InputExample(texts=[row['sentence_A'], row['sentence_B']], label=row['relatedness_score'])) \n    s3.append(row['sentence_A'])\n    s3.append(row['sentence_B'])","e9b8b51c":"train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=64)\ntrain_loss = losses.CosineSimilarityLoss(model)\nmodel.fit(train_objectives=[(train_dataloader, train_loss)],\n          epochs=5, warmup_steps=100, evaluator=evaluator, evaluation_steps=124)\n","8989ec99":"model.best_score #20epochs 0.9317 roberta-base\n# model.evaluate(evaluator) #  0.9310\n","03648f19":"#Compute embedding for both lists\nembeddings1 = model.encode(sentences1, convert_to_tensor=True)\nembeddings2 = model.encode(sentences2, convert_to_tensor=True)\nembeddings3 = model.encode(sentences3, convert_to_tensor=True)\n\n#Compute cosine-similarits \nprint(util.pytorch_cos_sim(embeddings1, embeddings2).item())\nprint(util.pytorch_cos_sim(embeddings3, embeddings2).item()) \nprint(util.pytorch_cos_sim(embeddings1, embeddings3).item()) ","0213f1ee":"import urllib.request\nimport zipfile\n\nurl = \"https:\/\/github.com\/sdadas\/polish-roberta\/releases\/download\/models-transformers-v3.4.0\/roberta_large_transformers.zip\"\nextract_dir = \"\/kaggle\/roberta_large\"\n\nzip_path, _ = urllib.request.urlretrieve(url)\nwith zipfile.ZipFile(zip_path, \"r\") as f:\n    f.extractall(extract_dir)","90d9ce90":"word_embedding_model2 = models.Transformer('\/kaggle\/roberta_large', max_seq_length=512)\npooling_model2 = models.Pooling(word_embedding_model2.get_word_embedding_dimension())\nmodel2 = SentenceTransformer(modules=[word_embedding_model2, pooling_model2])","671b975d":"train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=8)\ntrain_loss = losses.CosineSimilarityLoss(model2)\nmodel2.fit(train_objectives=[(train_dataloader, train_loss)],\n          epochs=2, warmup_steps=500, evaluator=evaluator, evaluation_steps=1000)","667e3a92":"model2.best_score #2epochs 0.9342 roberta-large\n# model2.evaluate(evaluator) #  0.9342","eea007fd":"#Compute embedding for both lists\n%time embeddings1 = model2.encode(sentences1, convert_to_tensor=True)\nembeddings2 = model2.encode(sentences2, convert_to_tensor=True)\nembeddings3 = model2.encode(sentences3, convert_to_tensor=True)\n\n#Compute cosine-similarits \n%time print(util.pytorch_cos_sim(embeddings1, embeddings2).item())\nprint(util.pytorch_cos_sim(embeddings3, embeddings2).item()) \nprint(util.pytorch_cos_sim(embeddings1, embeddings3).item()) ","692b352f":"largecorpus = corpus_test['sentence_A'].unique()\nparaphrases = util.paraphrase_mining(model, largecorpus)\n\ndf = pd.DataFrame.from_records(paraphrases)\ndf[1] = [largecorpus[idx] for idx in df[1]]\ndf[2] = [largecorpus[idx] for idx in df[2]]","c180a50e":"df","0d213403":"largecorpus2 = corpus_test['sentence_A'].unique()\nparaphrases2 = util.paraphrase_mining(model2, largecorpus2)\n\ndf2 = pd.DataFrame.from_records(paraphrases2)\ndf2[1] = [largecorpus2[idx] for idx in df2[1]]\ndf2[2] = [largecorpus2[idx] for idx in df2[2]]","b7791353":"df2","d0411aef":"pip install spacy","527f044b":"pip install spacy_sentence_bert","9195c4da":"!python -m spacy download pl_core_news_lg","343088c9":"import spacy\n\nnlp = spacy.load(\"pl_core_news_lg\")\n\n%time s1 = nlp(sentences1)\ns2 = nlp(sentences2)\ns3 = nlp(sentences3)\n# sentence similarity\n%time print(s1.similarity(s2)) \nprint(s3.similarity(s2))\nprint(s1.similarity(s3)) ","f29fb25e":"import spacy_sentence_bert\n\nnlp = spacy_sentence_bert.load_model(\"xx_distiluse_base_multilingual_cased_v2\")\n\n%time s1 = nlp(sentences1)\ns2 = nlp(sentences2)\ns3 = nlp(sentences3)\n# sentence similarity\n%time print(s1.similarity(s2)) \nprint(s3.similarity(s2))\nprint(s1.similarity(s3)) \n\n# senTab = [s1, s2, s3]\n\n# print(s1.vector.shape)\n# print(s1[3].vector.shape)\n# print(s1[2:4].vector.shape)\n\n# for i in senTab:\n#     print(f'{i} | {s1} = {i.similarity(s1)}')\n#     print(f'{i} | {s2} = {i.similarity(s2)}')\n#     print(f'{i} | {s3} = {i.similarity(s3)}')","e0bbb810":"Polish CDSCorpus consists of 10K Polish sentence pairs which are human-annotated for semantic relatedness and entailment. The dataset may be used for the evaluation of compositional distributional semantics models of Polish. ","730c8246":"# 1.Pretrained Roberta_base pl cosine-similarity","ea4b866a":"Roberta_large gives better semantic sentence understanding than roberta_base","a014711c":"# Paraphrase Mining\nfinding texts with similar meaning for large colections of sentences 10000+","a89608b8":"# Spacy, Spacy transformers","26da751b":"# Robera_large pl"}}