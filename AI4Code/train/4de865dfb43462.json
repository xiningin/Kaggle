{"cell_type":{"89de4576":"code","7081e449":"code","42537fcb":"code","b63f38db":"code","07739200":"code","04f6c6fd":"code","7f6ac376":"code","bff44321":"code","cedd6f5a":"code","89b1b80a":"code","edf65953":"code","78d692a0":"code","c44cd945":"code","9f5842d5":"code","85469e2f":"code","e773ba3e":"code","1250aa61":"code","ac840388":"code","bb7dc0ca":"code","ab9b81b6":"code","8e883325":"code","52877529":"code","d3314d3c":"code","a0578583":"code","d04d713f":"code","4d67f8e5":"code","90417db8":"code","2041f0f1":"code","a48d08b9":"code","08ea31b6":"code","cb95fb86":"code","ab1b582e":"code","699b61d9":"code","e0db45f9":"code","5bb52ccf":"code","22acaa82":"code","9fcb42f5":"code","f07b26ad":"code","b754799b":"code","9d08d44c":"code","c8c96c49":"code","6a8e516c":"code","e2710699":"code","9419d769":"code","d6349ccd":"code","e58f7e76":"code","26645a88":"code","e3fef66e":"code","b77f5f75":"code","96864c74":"code","9c223719":"code","bdaee5c3":"code","307471c0":"code","9dd75d7e":"code","f103ed9b":"code","3a30587d":"code","a583cddf":"code","633e96cc":"code","6b1d63d2":"code","02339e19":"code","f617ed8f":"code","b2b4fd63":"markdown","af31a747":"markdown","728586f8":"markdown","a9e5de48":"markdown","1fc1e579":"markdown","c47c6509":"markdown","cce111fd":"markdown","af657c57":"markdown","15b4c430":"markdown","4d335bd5":"markdown","5d91320b":"markdown","e9a639c5":"markdown","e3d2b310":"markdown","9082cfd7":"markdown","64e8a089":"markdown","a40a5586":"markdown","5a3c0716":"markdown","6ac787b9":"markdown","8835947d":"markdown","086f358d":"markdown","42cf26ca":"markdown","1e77d648":"markdown","733286d6":"markdown","04e78a24":"markdown","3ffbd2bb":"markdown","e5b77841":"markdown","4e9ce315":"markdown","a104d683":"markdown","44ca31d5":"markdown","35048fcf":"markdown","2d802a35":"markdown","1121abef":"markdown","42415ebc":"markdown","4cbcd2fd":"markdown","d81dcab0":"markdown","442af3b0":"markdown","f72ee4ca":"markdown","fc1ba5c2":"markdown","8717dc21":"markdown","bfe692ad":"markdown","d798788d":"markdown","2641a56b":"markdown","bf294d69":"markdown","a172ca91":"markdown","8b6486b6":"markdown","aac32b8b":"markdown","81a6fc5a":"markdown","dc42875f":"markdown","8b17f1da":"markdown","9475b8da":"markdown","2b533cfd":"markdown","7cfe504f":"markdown","de7a072c":"markdown","0ea843ef":"markdown","d5d3a716":"markdown","7d3b8d56":"markdown","77bebeea":"markdown","d4519abb":"markdown","9c968a46":"markdown","448ba715":"markdown","dbbf3c9b":"markdown","3698c3e1":"markdown","874b8f19":"markdown","0ac886a2":"markdown","00f06b78":"markdown","f818b09c":"markdown","5d095089":"markdown","ef922983":"markdown","057bc681":"markdown","1edfc987":"markdown","3fd22f07":"markdown","9ff02b3c":"markdown","c53ce7d8":"markdown","1583cdd1":"markdown","328a834f":"markdown","03e44c1d":"markdown","4e77147c":"markdown","3ca8a475":"markdown","068df895":"markdown","d28620fe":"markdown","6753702d":"markdown","b30c07c9":"markdown","924c9fa1":"markdown"},"source":{"89de4576":"# Pandas & Numpy\nimport pandas as pd\nimport numpy as np\nimport pandas_profiling\n\n# Data Visualisation\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Machine learning models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nmpl.style.use(['ggplot']) ","7081e449":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest= pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\nprint('Data loaded !')","42537fcb":"print(train.shape)\ntrain.head()","b63f38db":"print(test.shape)\ntest.head()","07739200":"train.profile_report()","04f6c6fd":"test.profile_report()","7f6ac376":"train.info()","bff44321":"train.describe()","cedd6f5a":"train[\"Survived\"].value_counts().plot(kind='pie',\n                                      figsize=(16,8),\n                                      autopct='%1.1f',\n                                      startangle=90,\n                                      colors=['lightcoral', 'lightgreen'],\n                                      labels=None,\n                                      explode=[0.07, 0])\nplt.title('Survival percentage')\nplt.axis('equal')\nplt.legend(labels=['Did not survive', 'Survived'], loc='upper left')\nplt.show()","89b1b80a":"train[\"Sex\"].value_counts().plot(kind='pie',\n                                 figsize=(16,8),\n                                 autopct='%1.1f',\n                                 startangle=90,\n                                 colors=['lightblue', 'pink'],\n                                 labels=None,\n                                 explode=[0.07, 0])\nplt.title('Male\/Female percentage')\nplt.axis('equal')\nplt.legend(labels=['Male', 'Female'], loc='upper left')\nplt.show()","edf65953":"data_sex_survived = train[['Sex','Survived']]\n\nd1 = data_sex_survived[data_sex_survived[\"Survived\"] == 1].groupby(\"Sex\").count()\nd2 = data_sex_survived[data_sex_survived[\"Survived\"] == 0].groupby(\"Sex\").count().rename(columns={'Survived':'Did not Survive'})\n\nsex_survived_count = d1.merge(d2, left_on='Sex',right_on='Sex')\nsex_survived_count = sex_survived_count.div(sex_survived_count.sum(axis=1), axis=0)\n\nsex_survived_count.plot(kind='bar',\n                        figsize=(16,8),\n                       color=['lightgreen','lightcoral'])\n\nplt.title('Survival rate based on Sex',size=19)\n\nplt.show()","78d692a0":"train[\"Pclass\"].value_counts().plot(kind='pie',\n                                    figsize=(16,8),\n                                    autopct='%1.1f',\n                                    startangle=90,\n                                    colors=['lightcoral','maroon','mistyrose'],\n                                    labels=None)\n\nplt.title('Ticket classes proportions within passengers')\nplt.axis('equal')\nplt.legend(labels=['Class 3', 'Class 1', 'Class 2'], loc='upper left', fontsize=14)\nplt.show()","c44cd945":"data_pclass_survived = train[['Pclass','Survived']]\n\nd1 = data_pclass_survived[data_sex_survived[\"Survived\"] == 1].groupby(\"Pclass\").count()\nd2 = data_pclass_survived[data_sex_survived[\"Survived\"] == 0].groupby(\"Pclass\").count().rename(columns={'Survived':'Did not Survive'})\npclass_survived_count = d1.merge(d2, left_on='Pclass',right_on='Pclass')\n\npclass_survived_count = pclass_survived_count.div(pclass_survived_count.sum(axis=1), axis=0)\n\npclass_survived_count.plot(kind='bar',\n                           figsize=(16,8),\n                           color=['lightgreen','lightcoral'])\n\nplt.title('Survival rate based on classes',size=19)\n\nplt.show()","9f5842d5":"train['SibSp'].value_counts().plot(kind='bar',\n                                   figsize=(16,8),\n                                   color='rosybrown')\n\nplt.title('Number of passengers based on number of siblings \/ spouses',size=19)\nplt.xlabel('Number of siblings')\nplt.ylabel('Number of passengers')\nplt.show()","85469e2f":"data_SibSp_survived = train[['SibSp','Survived']]\n\nd1 = data_SibSp_survived[data_sex_survived[\"Survived\"] == 1].groupby(\"SibSp\").count()\nd2 = data_SibSp_survived[data_sex_survived[\"Survived\"] == 0].groupby(\"SibSp\").count().rename(columns={'Survived':'Did not Survive'})\n\nSibSp_survived_count = d1.merge(d2, left_on='SibSp',right_on='SibSp')\nSibSp_survived_count = SibSp_survived_count.div(SibSp_survived_count.sum(axis=1), axis=0)\n\nSibSp_survived_count.plot(kind='bar',\n                          figsize=(16,8),\n                          color=['lightgreen','lightcoral'])\n\nplt.title('Survival rate based on number of siblings \/ spouses',size=19)\n\nplt.show()","e773ba3e":"train['Parch'].value_counts().plot(kind='bar',\n                                   figsize=(16,8),\n                                   color='rosybrown')\n\nplt.title('Number of parents \/ children',size=19)\n\nplt.show()","1250aa61":"data_Parch_survived = train[['Parch','Survived']]\n\nd1 = data_Parch_survived [data_sex_survived[\"Survived\"] == 1].groupby(\"Parch\").count()\nd2 = data_Parch_survived [data_sex_survived[\"Survived\"] == 0].groupby(\"Parch\").count().rename(columns={'Survived':'Did not Survive'})\n\nParch_survived_count = d1.merge(d2, left_on='Parch',right_on='Parch')\nParch_survived_count = Parch_survived_count.div(Parch_survived_count.sum(axis=1), axis=0)\n\nParch_survived_count.plot(kind='bar',\n                          figsize=(16,8),\n                          color=['lightgreen','lightcoral'])\n\nplt.title('Survival rate based on number of parents \/ children',size=19)\n\nplt.show()","ac840388":"train[\"Embarked\"].value_counts()","bb7dc0ca":"train[\"Embarked\"].value_counts().plot(kind='pie',\n                                    figsize=(16,8),\n                                    autopct='%1.1f',\n                                    startangle=90,\n                                    colors=['lightcoral','maroon','mistyrose'],\n                                    labels=None)\n\nplt.title('Port of Embarkation proportions within passengers')\nplt.axis('equal')\nplt.legend(labels=['Southampton', 'Cherbourg', 'Queenstown'], loc='upper left', fontsize=14)\nplt.show()","ab9b81b6":"data_embarked_survived = train[['Embarked','Survived']]\n\nd1 = data_embarked_survived[data_embarked_survived[\"Survived\"] == 1].groupby(\"Embarked\").count()\nd2 = data_embarked_survived[data_embarked_survived[\"Survived\"] == 0].groupby(\"Embarked\").count().rename(columns={'Survived':'Did not Survive'})\ndata_embarked_survived_count = d1.merge(d2, left_on='Embarked',right_on='Embarked')\n\ndata_embarked_survived_count = data_embarked_survived_count.div(data_embarked_survived_count.sum(axis=1), axis=0)\n\ndata_embarked_survived_count.plot(kind='bar',\n                           figsize=(16,8),\n                           color=['lightgreen','lightcoral'])\n\nplt.title('Survival rate based on port of embarkation',size=19)\n\nplt.show()","8e883325":"train['Fare'].plot(kind='hist',\n                  figsize=(16,8),\n                  color='rosybrown')\n\nplt.title('Passenger fare distribution',size=19)\n\nplt.show()","52877529":"data_fare_survived = train[['Fare','Survived']][train[\"Survived\"]==1]\ndata_fare_survived_grouped = data_fare_survived.groupby('Fare').count()\n\ndata_fare_dsurvived = train[['Fare','Survived']][train[\"Survived\"]==0]\ndata_fare_dsurvived_grouped = data_fare_dsurvived.groupby('Fare').count().rename(columns={'Survived':'Did not Survive'})\n\nax = data_fare_survived_grouped.plot(kind='area',\n                                    figsize=(16,8),\n                                    stacked=False,\n                                    color='lightgreen')\n\ndata_fare_dsurvived_grouped.plot(kind='area',\n                                figsize=(16,8),\n                                stacked=False,\n                                color='lightcoral',\n                                ax=ax)\n\nplt.title('Number of passengers - Surived\/Did not Survive - by Fare')\nplt.ylabel('Number of passengers')\n\n\nplt.show()","d3314d3c":"pd.qcut(train['Fare'],4).value_counts()","a0578583":"data_fare_survived = train[['Fare', 'Survived']]\ndata_fare_survived['farecat'] = pd.qcut(data_fare_survived['Fare'],4, labels=['1st','2nd','3nd','4th'])\ndata_farecat_survived = data_fare_survived[['farecat','Survived']]\n\nd1 = data_farecat_survived[data_farecat_survived[\"Survived\"] == 1].groupby(\"farecat\").count()\nd2 = data_farecat_survived[data_farecat_survived[\"Survived\"] == 0].groupby(\"farecat\").count().rename(columns={'Survived':'Did not Survive'})\ndata_farecat_survived_count = d1.merge(d2, left_on='farecat',right_on='farecat')\n\ndata_farecat_survived_count = data_farecat_survived_count.div(data_farecat_survived_count.sum(axis=1), axis=0)\n\ndata_farecat_survived_count.plot(kind='bar',\n                           figsize=(16,8),\n                           color=['lightgreen','lightcoral'])\n\nplt.title('Survival rate based on port of farecat',size=19)\n\nplt.show()","d04d713f":"train['Age'].plot(kind='hist',\n                  figsize=(16,8),\n                  color='rosybrown')\n\nplt.title('Age distribution',size=20)\n\nplt.show()","4d67f8e5":"data_age_survived = train[['Age','Survived']][train[\"Survived\"]==1]\ndata_age_survived_grouped = data_age_survived.groupby('Age').count()\n\ndata_age_dsurvived = train[['Age','Survived']][train[\"Survived\"]==0]\ndata_age_dsurvived_grouped = data_age_dsurvived.groupby('Age').count().rename(columns={'Survived':'Did not Survive'})\n\nax = data_age_survived_grouped.plot(kind='area',\n                                    figsize=(16,8),\n                                    stacked=False,\n                                    color='lightgreen')\n\ndata_age_dsurvived_grouped.plot(kind='area',\n                                figsize=(16,8),\n                                stacked=False,\n                                color='lightcoral',\n                                ax=ax)\n\nplt.title('Number of passengers - Surived\/Did not Survive - by Age', size=20)\nplt.ylabel('Number of passengers')\n\n\nplt.show()","90417db8":"female = train[train['Sex']=='female']\nmale = train[train['Sex']=='male']\n\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(16, 6))\n\n\nax = sns.distplot(female[female['Survived']==1].Age.dropna(),\n                  bins=20,\n                  label = 'Survived',\n                  ax = axes[0],\n                  kde =False)\n\nax = sns.distplot(female[female['Survived']==0].Age.dropna(),\n                  bins=20,\n                  label = 'Did not survived',\n                  ax = axes[0],\n                  kde =False)\nax.legend()\nax.set_title('Female',size=20)\n\nax = sns.distplot(male[male['Survived']==1].Age.dropna(),\n                  bins=20,\n                  label = 'Survived',\n                  ax = axes[1],\n                  kde = False)\n\nax = sns.distplot(male[male['Survived']==0].Age.dropna(),\n                  bins=20,\n                  label = 'Did not survived',\n                  ax = axes[1],\n                  kde = False)\nax.legend()\n_ = ax.set_title('Male',size=20)","2041f0f1":"data_relatives_survived = train[['Survived']]\n\ndata_relatives_survived['Relatives'] = train['Parch'] + train['SibSp']\n\n\ng = sns.factorplot(x='Relatives',\n                   y='Survived',\n                   data=data_relatives_survived,\n                   aspect = 2.5,\n                   color='rosybrown')\n\n_ = g.axes.flatten()[0].set_title('Survival rate based on relatives', size=20)","a48d08b9":"train_copy = train.copy()\ntrain_copy['Cabin'].fillna('U00', inplace=True)\n\ntrain_copy['Deck'] = train_copy['Cabin'].astype(str).str[0]","08ea31b6":"data_deck_survived = train_copy[['Deck','Survived']]\n\nd1 = data_deck_survived[data_deck_survived[\"Survived\"] == 1].groupby(\"Deck\").count()\nd2 = data_deck_survived[data_deck_survived[\"Survived\"] == 0].groupby(\"Deck\").count().rename(columns={'Survived':'Did not Survive'})\ndata_deck_survived_count = d1.merge(d2, left_on='Deck',right_on='Deck')\n\ndata_deck_survived_count = data_deck_survived_count.div(data_deck_survived_count.sum(axis=1), axis=0)\n\ndata_deck_survived_count.plot(kind='bar',\n                           figsize=(16,8),\n                           color=['lightgreen','lightcoral'])\n\nplt.title('Survival rate based on port of deck',size=19)\n\nplt.show()","cb95fb86":"for df in [train, test] :\n    df.drop(['PassengerId','Ticket'],axis=1,inplace=True)","ab1b582e":"train.head()","699b61d9":"for df in [train, test] :\n    df['Title'] = df.Name.str.extract('([A-Za-z]+)\\.')","e0db45f9":"train['Title'].value_counts()","5bb52ccf":"for df in [train, test] :\n    df['Title'].replace(['Dona','Lady','Mme','Countess','Ms'],'Mrs',inplace=True)\n    df['Title'].replace(['Mlle'],'Miss',inplace=True)\n    df['Title'].replace(['Sir','Capt','Don','Jonkheer','Col','Major','Rev','Dr','Major'],'Mr',inplace=True)\n    df.drop(['Name'], axis=1,inplace=True)\n    ","22acaa82":"train['Title'].value_counts()","9fcb42f5":"train.head()","f07b26ad":"test.head()","b754799b":"for df in [train, test] :\n    df['RelativesGroup'] = 0\n    df['Relatives'] = df['Parch'] + df['SibSp']\n    df.loc[(df['Relatives'] > 0) & (df['Relatives'] <= 3), 'RelativesGroup'] = 1\n    df.loc[(df['Relatives'] >= 4), 'RelativesGroup'] = 2\n    df.drop(['Relatives','Parch','SibSp'], axis=1,inplace=True)\n    df['RelativesGroup'] = df['RelativesGroup'].astype('category')","9d08d44c":"train.isnull().sum()","c8c96c49":"test.isnull().sum()","6a8e516c":"for df in [train,test]:\n    df['Age'] = df.groupby('Title')['Age'].apply(lambda x: x.fillna(x.mean()))\n    ","e2710699":"common = train['Embarked'].describe()['top']\ntrain['Embarked'].fillna(common,inplace=True)","9419d769":"mean = test['Fare'].mean()\ntest['Fare'].fillna(mean,inplace=True)","d6349ccd":"for df in [train,test] :\n    df['DeckUnkown'] = 1\n    df.loc[(df['Cabin'].isnull()), 'DeckUnkown'] = 0\n    df.drop(['Cabin'], axis=1,inplace=True)","e58f7e76":"train[['Age','Fare']].describe()","26645a88":"scaler = MinMaxScaler()\nfor df in [train,test]:\n    df[['Age','Fare']] = scaler.fit_transform(df[['Age','Fare']])","e3fef66e":"train.head()","b77f5f75":"test.head()","96864c74":"for df in [train, test]:\n    df['Sex'] = (df['Sex'] == \"male\").astype(int)","9c223719":"for df in [train, test]:\n    df['Pclass'] = df['Pclass'].astype('category')","bdaee5c3":"train = pd.get_dummies(train)\ntest = pd.get_dummies(test)","307471c0":"print(train.shape)\ntrain.head()","9dd75d7e":"print(test.shape)\ntest.head()","f103ed9b":"X = train.drop(['Survived'], axis = 1)\ny = train['Survived']","3a30587d":"rfr = RandomForestClassifier(random_state=42)\n\nrfr.fit(X, y)\n\nrfr_pred = rfr.predict(X)\n\nrfr_acc = accuracy_score(y, rfr_pred)\n\nprint('Accuracy on the whole train set: ',round(rfr_acc*100,2,),'%')","a583cddf":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state=42)","633e96cc":"rfr = RandomForestClassifier(random_state=42)\n\nrfr.fit(X_train, y_train)\n\nrfr_pred_train = rfr.predict(X)\nrfr_pred_test = rfr.predict(X_test)\n\nrfr_acc_train = accuracy_score(y, rfr_pred_train)\nrfr_acc_test = accuracy_score(y_test, rfr_pred_test)\n\nprint('Accuracy on the train set: ',round(rfr_acc_train*100,2,),'%')\nprint('Accuracy on the test set: \\t',round(rfr_acc_test*100,2,),'%')","6b1d63d2":"# param_grid = { \"criterion\" : [\"gini\", \"entropy\"],\n#               \"min_samples_leaf\" : [1, 5, 10, 25, 50, 70],\n#               \"min_samples_split\" : [2, 4, 10, 12, 16, 18, 25, 35],\n#               \"n_estimators\": [100, 400, 700, 1000, 1500]}\n\n# from sklearn.model_selection import GridSearchCV, cross_val_score\n\n# rf = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\n\n# clf = GridSearchCV(estimator=rf, param_grid=param_grid, n_jobs=-1,verbose=10)\n# clf.fit(X_train, y_train)\n\n# clf.best_params_","02339e19":"rfr = RandomForestClassifier(criterion = \"entropy\", \n                              min_samples_leaf = 5,\n                              min_samples_split = 12,\n                              n_estimators=400,\n                              random_state=42,\n                              n_jobs=-1)\nrfr.fit(X_train, y_train)\n\nrfr_pred_train = rfr.predict(X_train)\nrfr_pred_test = rfr.predict(X_test)\n\nrfr_ac_train = accuracy_score(y_train, rfr_pred_train)\nrfr_ac_test = accuracy_score(y_test, rfr_pred_test)\n\nprint('Accuracy on the train set: ',round(rfr_ac_train*100,2,),'%')\nprint('Accuracy on the test set: \\t',round(rfr_ac_test*100,2,),'%')","f617ed8f":"rfr.fit(X, y)\n\nrfr_pred = rfr.predict(test)\n\n# test_df = pd.read_csv(file_path_test)\n\n# submission = pd.DataFrame({\"PassengerId\": test_df[\"PassengerId\"],\n#                            \"Survived\": rfr_pred})\n\n# submission.to_csv('submissionfinal', index=False)","b2b4fd63":"That's pretty good, our model is correcly fitting the data but it is possible that it's overfitting it.  \n\nLet's hold 15% of our data as a test set and see how well our model performs :","af31a747":"## Age & Sex :","728586f8":"# Data Cleaning & Pre-processing - Feature Ingineering","a9e5de48":"## SibSp","1fc1e579":"According to this graph, we can assume that children were saved first, and that a lot of passengers between 20 and 30 did not survive.","c47c6509":"# Introduction","cce111fd":"Let's see now how the number of parents \/ chilidren affects our target value :","af657c57":"Some warnings need to catch our attention:  \n\n* **Ticket:** High cardinality, too much distinct values. This is the ticket number that we will ignore because it will have no impact on the survival rate of the passengers.  \n\n* **Name:** High cardinality, too mouch distinct values. We will ignore this warning because this feature will be very useful in order to characterize the passengers according to the titles present in their names, but also in order to deal with the missing age data.  \n\n* **Cabin:** Missing values 77.1%.At first, I just wanted to drop this feature considering the large propotion of missing data, but I found out by reading other contributions that we can still manage to extract some valuable information from it.  \n\n* **Age:** Missing values 19.9%, we will ignore this warning because it is an important feature as we will show right after. In addition, we will be able to intelligently replace this missing data.","15b4c430":"We will convert Sex into a binary feature 1 = male, 0 = female :","4d335bd5":"I'm going to split my training set into a train set and a test set. I will train my model on the train set and hyperparameter tunning will be done by crossvalidation within the training set. The test set will allow us to evaluate our model, if it performs well, then the model is not overfitting the training set.","5d91320b":"## Missing values","e9a639c5":"### Fare (test)","e3d2b310":"## Pclass","9082cfd7":"**Well, we pretty much went around all the variables individually regarding our target variable, let's see now what can we get from observing two variables regarding our target variable.**","64e8a089":"# Exploratory Data Analysis","a40a5586":"We will us this feature to extract **Titles** which will help us replace missing age values and also create a new feature which can be valuable to our analysis.","5a3c0716":"Let's see how the number of passengers is distributed according to the number of siblings \/ spouses :","6ac787b9":"## Name","8835947d":"**Well I think we have a pretty good overview and understanding of our data. Now let's make it more valuable and turn it into consumable data !**","086f358d":"Well, here is some intersting things to observe :  \n* Class 1 : Approximately, two thirds of the the passengers had the chance to survive.\n* Class 2 : The survival rate is nearly the same.\n* Class 3 : Three quarters of the passengers did not survive.  \n\nThe Ticket Class has definitely an influence on the survival rate of the passengers.","42cf26ca":"**Top 8%!** (**Top 6%** in reality if we rank equal scores as one rank).\n\nWhat an adventure aboard the Titanic we had ! We started by basic data exploration and visualization in order to familiarize ourselves with different features. We did some cleaning data and feature engineering in order to provide our model as much as information as possible. Finally we chose RandomForests as a model, train it and tunned its parameters.\n\nI preferred not to analyze the results (confusion matrix, ROC curves) because we don't have acces to the test set results. Maybe be we could have done it on the training set but let's just say that my adventure with the Titanic ends here :D\n\nSome ideas and approaches are not the figment of my imagination. I worked on this project since quite a time now, trying things and being inspired by what was done by others competitors.\n\nAnyway I am very proud of this result in my first Kaggle competition. I hope you enjoyed reading this. Do not hesitate to share with me your thoughts and feedback in comments, feel free to ask any question, and do not hesitate to upvote if you found this notebook interesting.\n\nTo the future !\n\n***Amnay Kanane***","1e77d648":"### Cabin","733286d6":"Let's see now how the number of siblings \/ spouses affects our target value :","04e78a24":"As mentionned before, we will use the Title feature to replace the missing values in Age by the mean of each Title category :","3ffbd2bb":"Since we have now an overall view and awareness of some warnings, we are going to look closer at each of the features.","e5b77841":"I'll spare you the training of countless models, I would like to focus on RandomForests which give fairly good results.\n\n**Random Forest** is a supervised learning algorithm. The \"forest\" is an ensemble of Decision Trees, most of the time trained with the \u201cbagging\u201d method. The general idea of the bagging method is that a combination of learning models increases the overall result.","4e9ce315":"Once we have the optimal hyperparameters, let's see how our model performs :","a104d683":"Well here it is, we previously observed that women had a better chance of survival, we note here that the protocol was well applied : children also had better chance of survival.","44ca31d5":"This feature has only one missing values, we will just replace it by the mean :","35048fcf":"## Sex","2d802a35":"What we can observe is :\n* Approximately 50% of passengers embarked from Cherbourg survived.\n* 60% \/ 40% for passengers embarked from QueenStown.\n* 65% \/ 35% for passengers embarked from Southampton.\n\nThe Titanic's itinary was Southampton -> Cherbourg -> QueenStown.\n\nWell I don't think we can make a strong asumption here, we'll see how this feature may show us when put with another variable such as Sex or Age.","1121abef":"Let's observe the age distribution :","42415ebc":"Now that we have our hyper-parameters, I think it's possible to train our model on the whole train set. As we feed it with more data, this could (should?) Improve the performance of the model.","4cbcd2fd":"Surprisingly, we can still draw some assumptions from this feature :\n* Passengers whose deck we could not identify have a low chance of survival.\n* Passengers in some decks have a very high survival rate (D,E).  \n\nWe will therefore still use this feature and hope it helps us getting better results !","d81dcab0":"Let's see first how our model performs on the whole train data :","442af3b0":"Now let's see how the Pclass feature affects our target variable :","f72ee4ca":"### Methodology :","fc1ba5c2":"## SibSp + Parch","8717dc21":"# Imports & Loading data","bfe692ad":"What we can assume according to this observation is :\n* Passengers with neither siblings nor spouses were less likely to survive.\n* Approximately 50% of passengers with 1 or 2 sibllings \/ spouses survived.\n* Survival rate goes down among passengers with 3 or 4 siblings \/ spouses.\n    ","d798788d":"## Fare","2641a56b":"# Conclusion","bf294d69":"## Scaling","a172ca91":"Let's start by dropping features that we won't need, so that we can see more clearly :","8b6486b6":"**Well, our data is clean and ready to be used !**","aac32b8b":"What we can assume according to this observation is :\n* Passengers with neither parents nor chilidren were less likely to survive.\n* Approximately 50% of passengers with 1 or 2 chilidren \/ spouses survived.\n* We can see a relatively high survival rate withtin passengers with 3 parents \/ children\n* Survival rate goes down among passengers with 4 parents \/ children.","81a6fc5a":"We can assume here that : \n* Passengers with 1 to 3 relatives on board had better chance to survive.\n* The survival rate of passenger with less than 1 or more than 3 relatives is more lower.","dc42875f":"For some for Machine Learning algorithms, it can be important to limit our input features within a defined range, we will use a MinMax Normalization :","8b17f1da":"Let's see now how decks affects our target value :","9475b8da":"As we saw earlier, we will juste mark passengers whose deck we could not identify, and make it a binary variable as follow :","2b533cfd":"We can observe here more clearly that the more we move towards categories of high value tickets, the more the survival rate increases, we will keep in mind this categorization in order to use it later in the a later section.","7cfe504f":"Let's explore this feature even if it contains a lot of missing data and see what we can extract from it as information.  \n\nA cabin identifier looks like \u2018C85\u2019 and the letter refers to the deck. We will replace the missing data with the cabin identifier 'U00' as Unknown and extract the first letter from all the identifiers.","de7a072c":"The two numerical features we have Age & Fare have different ranges :","0ea843ef":"## Age","d5d3a716":"### Embarked (train)","7d3b8d56":"## Cabin","77bebeea":"Let's see how the number of passengers is distributed according to the number of parents \/ children :","d4519abb":"Well, we have now a better performance on the test set, as small as it is, but believe me, it's worth it. This means that we have improved the model's ability to genralize to unkown data !","9c968a46":"Well we have here too much different titles, let's group them more significantly. I googled every title and tried to put the rare titles into the other much known titles :","448ba715":"## Survived","dbbf3c9b":"# Submission","3698c3e1":"Well, **38.4%** of the passengers survived this disaster.","874b8f19":"We saw earlier that the number of relatives had its influence on the surival rate, we assume that:\n* Passengers with 1 to 3 relatives on board had better chance to survive.\n* The survival rate of passenger with less than 1 or more than 3 relatives is more lower.  \n\nlet's create that new feature to take it into account and group it into 3 groups :\n* 0 : the passenger is alone\n* 1 : the passenger has between 1 and 3 relatives on board\n* 3 : the passenger has more than 3 relatives on board\n","0ac886a2":"**Remark about SibSp and Parch :**  \n\nThese two features have similar characteristics, we can think of combining them (summing) in order to be able to characterize each passenger by the number of members of his family present on board the Titanic. We will see that in a later section.","00f06b78":"Through this kernel, I will present my humble participation in this competition. I'll put into practice the different concepts that I have seen since I started learning about data science and machine learning.\n\nI will use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nHere is a description of the variables that we will handle :\n\n| Variable  |    Definition   |  Key |\n|----------|:-------------:|------:|\n| survived |  Survival |0 = No, 1 = Yes|\n| name |  Full name ||\n| passengerid |  Unique id of each passenger ||\n| pclass |  Ticket class |1 = 1st, 2 = 2nd, 3 = 3rd|\n| sex |  Sex ||\n| age |  Age in years ||\n| sibsp\t| # of siblings \/ spouses aboard the Titanic ||\n| parch |  # of parents \/ children aboard the Titanic ||\n| ticket |  Ticket number ||\n| fare |  Passenger fare ||\n| cabin |  Cabin number ||\n| embarked |  Port of Embarkation |C = Cherbourg, Q = Queenstown, S = Southampton|\n\n\n","f818b09c":"Let's see the proportion of each class to which passengers belong :","5d095089":"## OneHotEncoding","ef922983":"This feature has only 2 missing values, we will just replace them by the most common :","057bc681":"We can see here that the fare as its influence on the survival rate, as fare increases, survivals increases. Let's try a more general approach by grouping passengers into fare categories. We will use the \"qcut\" function to discretize this variable into equal-sized buckets to avoid some categories to have a lot more values than the others.  \n\nHere you can see how this method is used, we are going to label those categories right after :","1edfc987":"## Parch","3fd22f07":"This is our target\/dependent variable. It is binary coded : 1 = Survived, 0 = Did not survive.  \nLet's plot the survival percentage of the passengers :","9ff02b3c":"We have seen that these two features lead us to (more or less) the same assumptions. However, it could be wiser to add them up in order to characterize each passenger by the number of relatives on board and see how it affects the survival rate :","c53ce7d8":"That's very interesting, we can see here that females had better chance to survive, therefore \"Sex\" is a valuable feature for our analysis.","1583cdd1":"<h1 align=center><font size = 8>Titanic Competition<\/font><\/h1>\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/f\/fd\/RMS_Titanic_3.jpg\/1024px-RMS_Titanic_3.jpg\" style=\"width: 650px;\"\/>","328a834f":"### Age","03e44c1d":"We will use the pandas profile_report tool to get a first overview of the data :","4e77147c":"We can read on Wipedia : \"A disproportionate number of men were left aboard because of a \"women and children first\" protocol for loading lifeboats.\" Let's visualize the results of this protocol :","3ca8a475":"We know now the human sex ratio with the passengers, let's see how this is related with our target variable :","068df895":"## Relatives = Sibsp + Parch","d28620fe":"# Building and training the ML Model","6753702d":"## Embarked","b30c07c9":"Indeed, our model overfits the data, the performance dropped drastically in the test set: the model has difficulty in generalizing to unkown data.  \n\nCross validation is a device used to defend the model from overfitting, it fights tendencies in the direction of complexity.\n\nWe will try to reduce this overfitting by tunning the hyperparameters of the model and hope it will perform well.  \n\nTo do that, we will use grid search on the train set, it will perform cross-validations to do the hyperparameter search.","924c9fa1":"Let's see now how age influence the survival rate :"}}