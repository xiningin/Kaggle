{"cell_type":{"b571bb90":"code","67632410":"code","31ceb164":"code","b3f50a69":"code","870107c5":"code","bc93f66f":"code","d588fdca":"code","89ed2a15":"code","2068796d":"code","43d98798":"code","2899d491":"code","29dca4f8":"code","222ce37d":"code","0c28a571":"code","a00b00d6":"code","10c000aa":"code","6fc57f0b":"code","1daa197e":"code","7f955b49":"code","8e0bc752":"code","ae1dab87":"code","5faf3b22":"code","c4686c24":"code","396a35ad":"code","1e0cb156":"code","6b87ed83":"code","42083cab":"code","7935a0f3":"code","e13a7135":"code","cabd6241":"code","e6f3e4e0":"code","b920b92b":"code","c7bb5766":"code","e8344832":"code","e70a4c3d":"code","c662b256":"markdown","5d3300c4":"markdown","b4001e2c":"markdown","f9562d00":"markdown","28230116":"markdown","7249b639":"markdown","37e1924f":"markdown","1e44aab7":"markdown","7a3aef78":"markdown","0ebe4b8d":"markdown","45e22660":"markdown","6998c95a":"markdown","acd330a5":"markdown","dec4a8c0":"markdown","d9bfc29f":"markdown","cd172622":"markdown","8ad5b8a4":"markdown","77267f5d":"markdown","16304b7b":"markdown","1aae447e":"markdown","e1a6076d":"markdown","23898184":"markdown","2cceb772":"markdown","9637b5f7":"markdown","34b3c463":"markdown","5f5eb786":"markdown","b251cbae":"markdown","d8797ef0":"markdown","2b092e77":"markdown","08b84f6e":"markdown","3c198398":"markdown","775f9da3":"markdown","43a24ea6":"markdown","69347e49":"markdown","f29aca93":"markdown","663b5b09":"markdown","e1dd3e51":"markdown","8a03543f":"markdown","c1310cb5":"markdown","0c6bbb93":"markdown","6c8e1049":"markdown","92d4d6c3":"markdown"},"source":{"b571bb90":"import pandas as pd\nimport numpy as np\nimport plotly.express as px\n\nimport matplotlib.pyplot as plt\n\ntrain = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv', dtype = {'stock_id': np.int32, 'time_id': np.int32, 'target': np.float64})\ntrain.head()","67632410":"book_example = pd.read_parquet('..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/stock_id=0')\ntrade_example =  pd.read_parquet('..\/input\/optiver-realized-volatility-prediction\/trade_train.parquet\/stock_id=0')\nstock_id = '0'\nbook_example = book_example[book_example['time_id']==5]\nbook_example.loc[:,'stock_id'] = stock_id\ntrade_example = trade_example[trade_example['time_id']==5]\ntrade_example.loc[:,'stock_id'] = stock_id\n\n\nbook_example['wap'] = (book_example['bid_price1'] * book_example['ask_size1'] +\n                                book_example['ask_price1'] * book_example['bid_size1']) \/ (\n                                       book_example['bid_size1']+ book_example['ask_size1'])\n\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff()\n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\nbook_example.loc[:,'log_return'] = log_return(book_example['wap'])\nbook_example = book_example[~book_example['log_return'].isnull()]","31ceb164":"plt.plot(book_example['seconds_in_bucket'],book_example['wap'])","b3f50a69":"plt.plot(book_example['seconds_in_bucket'],book_example['log_return']**2)","870107c5":"x = np.append(book_example['seconds_in_bucket'].values,1200)\ny = np.append((book_example['log_return']**2).values,0)\nz = np.mean(book_example['log_return']**2)\n\nplt.hlines(z,xmin=0,xmax=600,color='red')\nplt.plot(x,y)\nplt.text(900, 0.6*1e-6,'???', ha='center', va='center', size = 40)","bc93f66f":"print(f'Number of unique stocks is {train.stock_id.nunique()}')\nprint(f'Number of unique time_id is {train.time_id.nunique()}')","d588fdca":"import matplotlib.pyplot as plt\nimport numpy as np\n\nn_stock = np.max(train.stock_id)\nn_time = np.max(train.time_id)\n\nmat_plot = np.zeros((n_stock, n_time))\n\ns_id = train.stock_id.values\nt_id = train.time_id.values\n\nfor k in range(train.shape[0]):\n    i = s_id[k]-1\n    j = t_id[k]-1\n    mat_plot[i,j] = 1\n  \nplt.figure(figsize=(12,8))\nplt.matshow(mat_plot, fignum=1, aspect='auto')","89ed2a15":"plt.hist(train.target, bins = 1000)\nplt.show()","2068796d":"hist = np.histogram(train.target, bins=1000)\nidx = np.argmax(hist[0])\n\nprint(f'Target mean is {np.mean(train.target)}')\nprint(f'Target median is {np.median(train.target)}')\nprint(f'Target mode is {hist[1][idx]}')","43d98798":"n_stock = np.max(train.stock_id)\nn_time = np.max(train.time_id)\n\nmat_plot = np.zeros((n_stock, n_time))\n\ns_id = train.stock_id.values\nt_id = train.time_id.values\nt = train.target.values\n\nfor k in range(train.shape[0]):\n    i = s_id[k]-1\n    j = t_id[k]-1\n    mat_plot[i,j] = t[k]\n  \nplt.figure(figsize=(12,8))\nplt.matshow(mat_plot, fignum=1, aspect='auto', vmax=0.0075)","2899d491":"book_example = pd.read_parquet('..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/stock_id=0')\ntrade_example =  pd.read_parquet('..\/input\/optiver-realized-volatility-prediction\/trade_train.parquet\/stock_id=0')\nstock_id = '0'\nbook_example = book_example[book_example['time_id']==5]\nbook_example.loc[:,'stock_id'] = stock_id\ntrade_example = trade_example[trade_example['time_id']==5]\ntrade_example.loc[:,'stock_id'] = stock_id\n\n\nbook_example['wap'] = (book_example['bid_price1'] * book_example['ask_size1'] +\n                                book_example['ask_price1'] * book_example['bid_size1']) \/ (\n                                       book_example['bid_size1']+ book_example['ask_size1'])\n\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff()\n\nbook_example.loc[:,'log_return'] = log_return(book_example['wap'])\nbook_example = book_example[~book_example['log_return'].isnull()]\n\n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\nrealized_vol = realized_volatility(book_example['log_return'])\nprint(f'Realized volatility for stock_id 0 on time_id 5 is {realized_vol}')\n\nimport os\nfrom sklearn.metrics import r2_score\nimport glob\nlist_order_book_file_train = glob.glob('\/kaggle\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/*')\n\ndef realized_volatility_per_time_id(file_path, prediction_column_name):\n    df_book_data = pd.read_parquet(file_path)\n    df_book_data['wap'] =(df_book_data['bid_price1'] * df_book_data['ask_size1']+df_book_data['ask_price1'] * df_book_data['bid_size1'])  \/ (\n                                      df_book_data['bid_size1']+ df_book_data[\n                                  'ask_size1'])\n    df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n    df_book_data = df_book_data[~df_book_data['log_return'].isnull()]\n    df_realized_vol_per_stock =  pd.DataFrame(df_book_data.groupby(['time_id'])['log_return'].agg(realized_volatility)).reset_index()\n    df_realized_vol_per_stock = df_realized_vol_per_stock.rename(columns = {'log_return':prediction_column_name})\n    stock_id = file_path.split('=')[1]\n    df_realized_vol_per_stock['row_id'] = df_realized_vol_per_stock['time_id'].apply(lambda x:f'{stock_id}-{x}')\n    return df_realized_vol_per_stock[['row_id',prediction_column_name]]\n\ndef past_realized_volatility_per_stock(list_file,prediction_column_name):\n    df_past_realized = pd.DataFrame()\n    for file in list_file:\n        df_past_realized = pd.concat([df_past_realized,\n                                     realized_volatility_per_time_id(file,prediction_column_name)])\n    return df_past_realized\ndf_past_realized_train = past_realized_volatility_per_stock(list_file=list_order_book_file_train,\n                                                           prediction_column_name='pred')\n\ntrain['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\ntrain = train[['row_id','target']]\ndf_joined = train.merge(df_past_realized_train[['row_id','pred']], on = ['row_id'], how = 'left')\n\nfrom sklearn.metrics import r2_score\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n\nR2 = round(r2_score(y_true = df_joined['target'], y_pred = df_joined['pred']),3)\nRMSPE = round(rmspe(y_true = df_joined['target'], y_pred = df_joined['pred']),3)\nprint(f'Performance of the naive prediction: R2 score: {R2}, RMSPE: {RMSPE}')","29dca4f8":"df_joined['se'] = np.square((df_joined['target'] - df_joined['pred']) \/ df_joined['target'])\ndf_joined['stock_id'] = df_joined['row_id'].str.partition('-')[0].astype('int')\ndf_joined['time_id'] = df_joined['row_id'].str.partition('-')[2].astype('int')","222ce37d":"test_df = df_joined[df_joined.stock_id == 0]\n\nplt.plot(test_df.target,test_df.pred, 'o', label='realisations')\n\nm, b = np.polyfit(test_df.target,test_df.pred, 1)\n\nplt.plot(test_df.target, test_df.target, label='first secant')\nplt.plot(test_df.target, m*test_df.target+b, label='linear regression')\nplt.legend()","0c28a571":"plt.hist(df_joined['se'], bins = 1000)\nplt.show()","a00b00d6":"plt.hist(np.log(df_joined['se']), bins = 1000)\nplt.show()","10c000aa":"##### n_stock = np.max(df_joined.stock_id)\nn_time = np.max(df_joined.time_id)\n\nmat_plot = np.zeros((n_stock, n_time))\n\ns_id = df_joined.stock_id.values\nt_id = df_joined.time_id.values\nt = df_joined['se'].values\n\nfor k in range(train.shape[0]):\n    i = s_id[k]-1\n    j = t_id[k]-1\n    mat_plot[i,j] = t[k]\n  \nplt.figure(figsize=(12,8))\nplt.matshow(mat_plot, fignum=1, aspect='auto',vmax=10*np.median(df_joined['se']))","6fc57f0b":"stock_0 = df_joined[df_joined.stock_id == 0]\nstock_1 = df_joined[df_joined.stock_id == 1]\n\nplt.scatter(stock_0.target,stock_1.target)","1daa197e":"from sklearn.preprocessing import quantile_transform\nqt_target_0 = quantile_transform(stock_0.target.values.reshape(-1, 1), n_quantiles=100)\nqt_target_1 = quantile_transform(stock_1.target.values.reshape(-1, 1), n_quantiles=100)\n\nplt.scatter(qt_target_0,qt_target_1)","7f955b49":"stock_0 = df_joined[df_joined.stock_id == 0]\n\nfrom sklearn.preprocessing import quantile_transform\nqt_target_0 = quantile_transform(stock_0.target.values.reshape(-1, 1), n_quantiles=100)\nqt_pred_0 = quantile_transform(stock_0.pred.values.reshape(-1, 1), n_quantiles=100)\n\nplt.scatter(qt_target_0,qt_pred_0)","8e0bc752":"df_joined['se'].nlargest(n=10)","ae1dab87":"df_joined.iloc[df_joined['se'].nlargest(n=10).index,]","5faf3b22":"test_df = df_joined[df_joined.stock_id == 31]\n\nplt.plot(test_df.target,test_df.pred, 'o', label='realisations')\n\nm, b = np.polyfit(test_df.target,test_df.pred, 1)\n\nplt.plot(test_df.target, test_df.target, label='first secant')\nplt.plot(test_df.target, m*test_df.target+b, label='linear regression')\nplt.legend()","c4686c24":"test_df = df_joined[df_joined.stock_id == 31]\n\ntest_df2 = test_df.loc[test_df['se'].nlargest(n=50).index,]\n\nplt.plot(test_df2.target,test_df2.pred, 'o', label='realisations')\n\nm, b = np.polyfit(test_df.target,test_df.pred, 1)\n\nplt.plot(test_df.target, test_df.target, label='first secant')\nplt.plot(test_df.target, m*test_df.target+b, label='linear regression')\nplt.legend()","396a35ad":"dfp = df_joined.pivot('time_id','stock_id')\nmat_corr = dfp.target.corr()\n\nplt.figure(figsize=(8,8))\nplt.matshow(mat_corr, fignum=1, aspect='auto')","1e0cb156":"import scipy\nimport scipy.cluster.hierarchy as sch\n\nX = mat_corr\nd = sch.distance.pdist(X)   # vector of ('55' choose 2) pairwise distances\nL = sch.linkage(d, method='complete')\nind = sch.fcluster(L, 0.5*d.max(), 'distance')\ncolumns = [dfp.target.columns.tolist()[i] for i in list((np.argsort(ind)))]\n\nplt.figure(figsize=(8,8))\nplt.matshow(dfp.target[columns].corr(), fignum=1, aspect='auto')","6b87ed83":"from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\nfrom scipy.spatial.distance import squareform\n\nplt.figure(figsize=(12,6))\ndissimilarity = 1 - abs(mat_corr)\nZ = linkage(squareform(dissimilarity), 'complete')\n\ndendrogram(Z, labels=dfp.target.columns, orientation='top', \n           leaf_rotation=90);","42083cab":"dfp = df_joined.pivot('stock_id','time_id')\nmat_corr = dfp.target.corr()\n\nplt.figure(figsize=(8,8))\nplt.matshow(mat_corr, fignum=1, aspect='auto')","7935a0f3":"X = mat_corr\nd = sch.distance.pdist(X)\nL = sch.linkage(d, method='complete')\nind = sch.fcluster(L, 0.5*d.max(), 'distance')\ncolumns = [dfp.se.columns.tolist()[i] for i in list((np.argsort(ind)))]\n\nplt.figure(figsize=(8,8))\nplt.matshow(dfp.se[columns].corr(), fignum=1, aspect='auto')","e13a7135":"plt.figure(figsize=(12,5))\ndissimilarity = 1 - abs(mat_corr)\nZ = linkage(squareform(dissimilarity), 'complete')\n\ndendrogram(Z, labels=dfp.target.columns, orientation='top', \n           leaf_rotation=90);","cabd6241":"dfp = df_joined.pivot('time_id','stock_id')\nmat_corr = dfp.se.corr()\n\nplt.figure(figsize=(8,8))\nplt.matshow(mat_corr, fignum=1, aspect='auto')","e6f3e4e0":"X = mat_corr\nd = sch.distance.pdist(X)   # vector of ('55' choose 2) pairwise distances\nL = sch.linkage(d, method='complete')\nind = sch.fcluster(L, 0.5*d.max(), 'distance')\ncolumns = [dfp.se.columns.tolist()[i] for i in list((np.argsort(ind)))]\n\nplt.figure(figsize=(8,8))\nplt.matshow(dfp.se[columns].corr(), fignum=1, aspect='auto')","b920b92b":"plt.figure(figsize=(12,5))\ndissimilarity = 1 - abs(mat_corr)\nZ = linkage(squareform(dissimilarity), 'complete')\n\ndendrogram(Z, labels=dfp.target.columns, orientation='top', \n           leaf_rotation=90);","c7bb5766":"dfp = df_joined.pivot('stock_id','time_id')\nmat_corr = dfp.se.corr()\n\nplt.figure(figsize=(8,8))\nplt.matshow(mat_corr, fignum=1, aspect='auto')","e8344832":"X = mat_corr\nd = sch.distance.pdist(X)\nL = sch.linkage(d, method='complete')\nind = sch.fcluster(L, 0.5*d.max(), 'distance')\ncolumns = [dfp.se.columns.tolist()[i] for i in list((np.argsort(ind)))]\n\nplt.figure(figsize=(8,8))\nplt.matshow(dfp.se[columns].corr(), fignum=1, aspect='auto')","e70a4c3d":"plt.figure(figsize=(12,5))\ndissimilarity = 1 - abs(mat_corr)\nZ = linkage(squareform(dissimilarity), 'complete')\n\ndendrogram(Z, labels=dfp.target.columns, orientation='top', \n           leaf_rotation=90);","c662b256":"<a id='Codependance'><\/a>\n# Codependance","5d3300c4":"In a quantile-quantile approach:","b4001e2c":"Looking further into error we notice that a big contributor is stock 31. We might investigate outliers with higher contribution to the error.","f9562d00":"There appears to be 2 outliers, other than that the conclusion seems to be that there is enough correlations so that it should be taken into account. ","28230116":"<a id='Stock_time_repartition'><\/a>\n# Repartition of Stock x Time","7249b639":"We start with a simple exploration of stock x time repartition to look for any exploitable pattern. ","37e1924f":"It's hard not to see time pattern in targets ... are we sure the time are shuffled ?","1e44aab7":"We observe a somewhat skewed distribution of the target. Given the square in the objective function dealing with those outliers mihgt be one of our main goals.","7a3aef78":"I don't know why id are merged. More practical to demerge them :","0ebe4b8d":"From the correlation matrix there seems to be 2\/3 clusters that appear on the dendogram. It is ot clear how to exploit that.","45e22660":"<a id='Correlation_Matrices_target'><\/a>\n# Correlation Matrices - Clustering - target\n\nIt's still unclear to me if stocks can be considered independtly or not. So I tried to look a bit into it, first with correlation, then with hierarchical clustering.","6998c95a":"<a id='Target_Distribution'><\/a>\n# Target distribution","acd330a5":"The same can be done with base error prediction. This might help understand where the difficulty of the competition lie.","dec4a8c0":"<a id='Base_prediction_error'><\/a>\n# Base prediction - error\n\nHere we study the base prediction error to get some hindsigt on the data.","d9bfc29f":"There is a lot of dependance going on. This might be what this competition is all about. Dependence between two stock targets: ","cd172622":"Another approach of clustering might be trough time clustering. That is burst, of correlation might appears on a given date, due to a significant external event. ","8ad5b8a4":"<a id='Stock_correlation_target'><\/a>\n## Stock correlation - Clustering","77267f5d":"Seems better, right ?","16304b7b":"We then look at the repartition of errors :","1aae447e":"The time clustering appears more interesting here than the previous one as some clusters seems to appears more distinctively in the correlation matrix. However this might be due to sample that were chosen closely, not necessarily to something that can be exploited in the future.","e1a6076d":"The idea of this notebook was to perform a basic exploration of the train data. This base exploration of the target lead me to consider the squared error with the base prediciton and observe some correlation between stock and day, then to some basic hierarchical clustering. As our main data doesn't contain much information other than prices, learning and using those high level correlations might be at the core of the competition. Along the way I also find hindsight about the data and the target \/ error (and a weird outlier). If you find the notebook to be usefull \/ interesting feel free to upvote, this will keep me motivated to share more hindsights trough the competition.\n\n**Version 2 :** I updated the notebook to better reflect my understanding of the competition.\n\n**Version 3 :** I updated the notebook to add a codependance \u00a7 as I feel like this is what this competition is all about.","23898184":"A simple linear regression by stock might be a good first step into modelling. ","2cceb772":"There doesn't seems to be anything exploitable here. I even think times are shuffled so that it would be difficult to use anything based on those.","9637b5f7":"It's not evident what is the problem here as the linear correlation seems to hold. However this might be due to the definition of  the objective. The error being relative means that biggest contributions my lie near the origin.","34b3c463":"<a id='Stock_correlation_se'><\/a>\n## Stock correlation - Clustering","5f5eb786":"<a id='Correlation_squared_error'><\/a>\n# Correlation - Clustering - squared error","b251cbae":"- [Objective of the competition](#Objective)\n- [Stock and Time repartition](#Stock_time_repartition)\n- [Target Distribution](#Target_Distribution)\n- [Optiver Base Function](#Optiver_base_functions)\n- [Custom Objective](#RMSPE)\n- [Base Prediction Error](#Base_prediction_error)\n- [Codependance](#Codependance)\n- [Weird Outlier](#Weird_Outlier)\n- [Correlation Matrices - Clustering - Target](#Correlation_Matrices_target)\n- [Correlation Matrices - Clustering - Error](#Correlation_squared_error)","d8797ef0":"The question is what happen next ?","2b092e77":"Quantile dependance between realized volatility and prediction :","08b84f6e":"<a id='RMSPE'><\/a>\n# Custom objective - RMPSE\n\nIt's important to not we are not optimising the an usual regression loss. We need to optimise the RMPSE. That is the usual RMSE corrceted by the target to make it a relative error:\n\n$$ \\sqrt{\\frac{1}{n}\\sum^{n}_{i=1}((y_i-\\hat{y_i})\/y_i)^2}$$\n\nAs it happen, for simple models this is equivalent to weighting the RMSE error by : \n\n$$ \\frac{1}{{y^2}_i} $$\n\nFor a complete discussion see here : \nhttps:\/\/www.kaggle.com\/c\/optiver-realized-volatility-prediction\/discussion\/250324\n\nFor a simple exemple (a linear regression) getting a good score using this basic idea see here :\nhttps:\/\/www.kaggle.com\/lucasmorin\/realised-vol-weighted-regression-baseline","3c198398":"Next the volatility :","775f9da3":"<a id='Weird_Outlier'><\/a>\n# Weird Outlier - Stock 31","43a24ea6":"Past seems is used as a benchmark. We might want to try other benchmarks suchs as the median or the mode, the more populous values, not the mean that can be influenced by outliers. (Or may be it might be better to get more influence from outliers, I don't know yet).","69347e49":"Copulas anyone ?","f29aca93":"<a id='Optiver_base_functions'><\/a>\n# Optiver exemple - Base functions","663b5b09":"<a id='Objective'><\/a>\n# Objective of the competition :\n\nWe have a bunch of time series of 10 minutes length of differents stocks over different time periods. The goal of this competition is to predict the volatility of the stock over the next ten minutes, not the price. Let's have a look at one of the price time serie.","e1dd3e51":"# Exploration of the train data and stock \/ day clustering","8a03543f":"<a id='Time_correlation_se'><\/a>\n## Time correlation - Clustering","c1310cb5":"Correlation is less clear. Still, some lowly correlated clusters seems to appear.","0c6bbb93":"Errors seems correlated trough stocks. We might have periods with high variations of volatility trough the stock market. Again we can't say much about correlation trough time.","6c8e1049":"My favorite graph so far. This is not a bug. You can see the data around 0. This mean we have high contributor to the error. Let's try with a log.","92d4d6c3":"<a id='Time_correlation_target'><\/a>\n## Time correlation - Clustering"}}