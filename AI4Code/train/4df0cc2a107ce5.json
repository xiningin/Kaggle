{"cell_type":{"9f0cfb00":"code","b2b8db74":"code","f1849ec5":"code","915fe59c":"code","75a6a4f8":"code","c931f705":"code","5deb7450":"code","7cf47d96":"code","e329d82a":"code","413585d5":"code","f1fc383f":"code","b681c53f":"code","424adc1b":"code","dbbfaaa3":"code","c57fa257":"code","2502bbc6":"code","d903f0d0":"code","5f963282":"code","dcb51a88":"code","7874debb":"code","ab1cadfc":"code","5e0cff4d":"code","8b998870":"code","a6b6b741":"code","723a8b8d":"code","8a39a3af":"code","bab5c4cf":"code","d272b326":"code","9551fc07":"code","a25931a7":"code","ab7450a9":"code","3f2d6865":"code","b6c6230e":"code","8a9a6a10":"code","640a0e90":"code","1aaa2731":"code","ef7f6911":"code","6b5dde86":"code","b9eb8a89":"code","8fe2c831":"code","f29dbe01":"code","6fc15c6e":"code","4e4fc020":"code","1653df9d":"code","461b4f46":"code","608c57f0":"code","f9d561ea":"markdown","e5c4b5a6":"markdown","d2813bd7":"markdown","a50c4880":"markdown","0b9e73ef":"markdown","e73b8f79":"markdown","7d71d643":"markdown","338b1161":"markdown","a12484b8":"markdown","caf711d8":"markdown","9d77194a":"markdown","d65eef64":"markdown","6725aa0f":"markdown","a940c8bb":"markdown","371ef7af":"markdown"},"source":{"9f0cfb00":"import numpy as np, pandas as pd, seaborn as sns, matplotlib.pyplot as plt\nimport warnings, time, gc\nfrom plotly import tools\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\n\ninit_notebook_mode(connected = True)\ncolor = sns.color_palette(\"Set2\")\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n\nfrom kaggle.competitions import twosigmanews\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\n\nenv = twosigmanews.make_env()","b2b8db74":"market_train, news_train = env.get_training_data()","f1849ec5":"del market_train; gc.collect()","915fe59c":"news_train.head()","75a6a4f8":"news_train[\"headline_len\"] = news_train[\"headline\"].apply(lambda x: len(x))","c931f705":"news_train[\"headline_len\"].hist(figsize = (15, 5), bins = 100)\nplt.show()","5deb7450":"max_len = max(news_train[\"headline_len\"])\ntemp = news_train[news_train[\"headline_len\"] == max_len][\"headline\"]\nfor t in temp:\n    print(t)\n    print()","7cf47d96":"temp = news_train[news_train[\"headline_len\"] == 0]\ntemp.head()","e329d82a":"# Drop rows with empty headlines\nnews_train.drop(news_train[news_train[\"headline_len\"] == 0].index, \n                inplace = True)\nnews_train.head()","413585d5":"from wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\nfrom nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer \nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\nfrom nltk.corpus import stopwords\nfrom string import punctuation\n\nimport re\nfrom functools import reduce\n\nstop_words = set(stopwords.words(\"english\"))","f1fc383f":"#from:https:\/\/github.com\/RenatoBMLR\/nlpPy\/tree\/master\/src\n\nclass TextDataset():\n\n    def __init__(self, df, lang = 'english'):\n\n        self.data = df\n\n        self.tokenizer = TweetTokenizer()\n        self.stop_words = set(stopwords.words(lang))\n        self.lemmatizer = WordNetLemmatizer()\n        self.ps = PorterStemmer()\n        \n    def _get_tokens(self, words):    \n        return [word.lower() for word in words.split()]\n    \n    def _removeStopwords(self, words):\n        # Removing all the stopwords\n        return [word for word in words if word not in self.stop_words]\n\n    def _removePonctuation(self, words):\n        return re.sub(r'[^\\w\\s]', '', words)\n\n    def _lemmatizing(self, words):\n        #Lemmatizing\n        return [self.lemmatizer.lemmatize(word) for word in words]\n\n    def _stemming(self, words):\n        #Stemming\n        return [self.ps.stem(word) for word in words]\n\n\n    def process_data(self, col = 'content', remove_pontuation=True, remove_stopw = True, lemmalize = False, stem = False):\n\n        self.data = self.data.drop_duplicates(subset=col, keep=\"last\")\n        \n        proc_col = col\n        if remove_pontuation:\n            proc_col = col + '_data'\n            self.data[proc_col] = self.data[col].apply(lambda x: self._removePonctuation(x) )\n        \n        # get tokens of the sentence\n        self.data[proc_col] = self.data[proc_col].apply(lambda x: self._get_tokens(x))\n        if remove_stopw:\n            self.data[proc_col] = self.data[proc_col].apply(lambda x: self._removeStopwords(x)) \n        if lemmalize:\n            self.data[proc_col] = self.data[proc_col].apply(lambda x: self._lemmatizing(x) )\n        if stem:\n            self.data[proc_col] = self.data[proc_col].apply(lambda x: self._stemming(x))\n\n        self.data['nb_words'] = self.data[proc_col].apply(lambda x: len(x))\n        self.proc_col = proc_col\n        \n    def __len__(self):\n        return len(self.data)","b681c53f":"partial_news = news_train[news_train[\"time\"] >= \"2016-12-01\"]","424adc1b":"text_tokens = TextDataset(partial_news)","dbbfaaa3":"text_tokens.process_data(col = \"headline\")","c57fa257":"text_tokens.data[\"headline_data\"].head()","2502bbc6":"partial_news[\"tokens\"] = text_tokens.data[\"headline_data\"]","d903f0d0":"partial_news.head()","5f963282":"partial_news.fillna(method = \"bfill\", inplace = True)","dcb51a88":"tf_idf_vec = TfidfVectorizer(min_df = 3, \n                             max_features = 100000, \n                             analyzer = \"word\",\n                             ngram_range = (1, 2),\n                             stop_words = \"english\")","7874debb":"tf_idf = tf_idf_vec.fit_transform(list(partial_news[\"tokens\"].map(lambda tokens: \" \".join(tokens))))","ab1cadfc":"tfidf_df = dict(zip(tf_idf_vec.get_feature_names(), tf_idf_vec.idf_))\ntfidf_df = pd.DataFrame(columns = [\"tfidf\"]).from_dict(dict(tfidf_df), orient = \"index\")\ntfidf_df.columns = [\"tfidf\"]","5e0cff4d":"tfidf_df.tfidf.hist(bins = 25, figsize = (15, 5))\nplt.show()","8b998870":"tfidf_df.sort_values(by = [\"tfidf\"], ascending = True).head(10)","a6b6b741":"tfidf_df.sort_values(by=[\"tfidf\"], ascending = False).head(10)","723a8b8d":"tf_idf.shape","8a39a3af":"from sklearn.decomposition import TruncatedSVD\n\nsvd = TruncatedSVD(n_components = 30, random_state = 32)\nsvd_tfidf = svd.fit_transform(tf_idf)\n\nsvd_tfidf.shape","bab5c4cf":"from sklearn.manifold import TSNE\n\ntsne_model = TSNE(n_components = 2, verbose = 1, random_state = 32, n_iter = 500)\ntsne_tfidf = tsne_model.fit_transform(svd_tfidf)","d272b326":"tsne_tfidf.shape","9551fc07":"import bokeh.plotting as bp\nfrom bokeh.models import HoverTool, BoxSelectTool\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.plotting import figure, show, output_notebook, reset_output\nfrom bokeh.palettes import d3\nimport bokeh.models as bmo\nfrom bokeh.io import save, output_file","a25931a7":"tsne_tfidf_df = pd.DataFrame(tsne_tfidf)\ntsne_tfidf_df.columns = [\"x\", \"y\"]\ntsne_tfidf_df[\"asset_name\"] = partial_news[\"assetName\"].values\ntsne_tfidf_df[\"headline\"] = partial_news[\"headline\"].values","ab7450a9":"tsne_tfidf_df.head()","3f2d6865":"output_notebook()\nplot_tfidf = bp.figure(plot_width = 700, plot_height = 600, \n                       title = \"tf-idf clustering of stock market news\",\n                       tools = \"pan, wheel_zoom, box_zoom, reset, hover, previewsave\",\n                       x_axis_type = None, y_axis_type = None, min_border = 1)\n\n# palette = d3[\"Category10\"][len(tsne_tfidf_df[\"asset_name\"].unique())]\n# color_map = bmo.CategoricalColorMapper(factors = tsne_tfidf_df[\"asset_name\"].map(str).unique(), \n#                                        palette = palette)\n\nplot_tfidf.scatter(x = \"x\", y = \"y\", \n#                    color = {\"field\": \"asset_name\", \"transform\": color_map}, \n#                    legend = \"asset_name\",\n                   source = tsne_tfidf_df,\n                   alpha = 0.7)\nhover = plot_tfidf.select(dict(type = HoverTool))\nhover.tooltips = {\"headline\": \"@headline\", \"asset_name\": \"@asset_name\"}\n\nshow(plot_tfidf)","b6c6230e":"from sklearn.cluster import MiniBatchKMeans\n\nkmeans_model = MiniBatchKMeans(n_clusters = 50, # don't have time to find the best number\n                               init = \"k-means++\",\n                               n_init =  1,\n                               init_size = 1000, \n                               batch_size = 1000, \n                               verbose = 0, \n                               max_iter = 1000)","8a9a6a10":"kmeans = kmeans_model.fit(tf_idf)\nkmeans_clusters = kmeans.predict(tf_idf)\nkmeans_distances = kmeans.transform(tf_idf)","640a0e90":"tsne_kmeans = tsne_model.fit_transform(kmeans_distances)","1aaa2731":"tsne_kmeans_df = pd.DataFrame(tsne_kmeans)\ntsne_kmeans_df.columns = [\"x\", \"y\"]\ntsne_kmeans_df[\"cluster\"] = kmeans_clusters\ntsne_kmeans_df[\"asset_name\"] = partial_news[\"assetName\"].values\ntsne_kmeans_df[\"headline\"] = partial_news[\"headline\"].values","ef7f6911":"colormap = np.array([\"#6d8dca\", \"#69de53\", \"#723bca\", \"#c3e14c\", \"#c84dc9\", \"#68af4e\", \"#6e6cd5\", \"#e3be38\", \n                     \"#4e2d7c\", \"#5fdfa8\", \"#d34690\", \"#3f6d31\", \"#d44427\", \"#7fcdd8\", \"#cb4053\", \"#5e9981\",\n                     \"#803a62\", \"#9b9e39\", \"#c88cca\", \"#e1c37b\", \"#34223b\", \"#bdd8a3\", \"#6e3326\", \"#cfbdce\", \n                     \"#d07d3c\", \"#52697d\", \"#194196\", \"#d27c88\", \"#36422b\", \"#b68f79\", \"#00ffff\", \"#33ff33\",\n                     \"#ffff99\", \"#99ff33\", \"#ff6666\", \"#666600\", \"#99004c\", \"#808080\", \"#a80a0a\", \"#a4924c\",\n                     \"#4a8e92\", \"#92734a\", \"#7d4097\", \"#4b4097\", \"#c0c0c0\", \"#409794\", \"#1a709b\", \"#a7dcf6\",\n                     \"#b1a7f6\", \"#eea7f6\"])","6b5dde86":"plot_kmeans = bp.figure(plot_width = 700, plot_height = 600, \n                       title = \"k-means clustering of stock market news\",\n                       tools = \"pan, wheel_zoom, box_zoom, reset, hover, previewsave\",\n                       x_axis_type = None, y_axis_type = None, min_border = 1)\n\nsource = ColumnDataSource(data = dict(x = tsne_kmeans_df[\"x\"], y = tsne_kmeans_df[\"y\"],\n                                      color = colormap[kmeans_clusters],\n                                      headline = tsne_kmeans_df[\"headline\"],\n                                      asset_name = tsne_kmeans_df[\"asset_name\"],\n                                      cluster = tsne_kmeans_df[\"cluster\"]))\n\nplot_kmeans.scatter(x = \"x\", y = \"y\", color = \"color\", source = source)\nhover = plot_kmeans.select(dict(type = HoverTool))\nhover.tooltips = {\"headline\": \"@headline\", \"asset_name\": \"@asset_name\", \"cluster\": \"@cluster\"}\nshow(plot_kmeans)","b9eb8a89":"cv = CountVectorizer(min_df = 2,\n                     max_features = 100000,\n                     analyzer = \"word\",\n                     ngram_range = (1, 2),\n                     stop_words = \"english\")","8fe2c831":"count_vectors = cv.fit_transform(partial_news[\"headline\"])","f29dbe01":"lda_model = LatentDirichletAllocation(n_components = 20, \n                                      # we choose a small n_components for time convenient\n                                      learning_method = \"online\",\n                                      max_iter = 20,\n                                      random_state = 32)","6fc15c6e":"news_topics = lda_model.fit_transform(count_vectors)","4e4fc020":"n_top_words = 10\ntopic_summaries = []\ntopic_word = lda_model.components_\nvocab = cv.get_feature_names()\n\nfor i, topic_dist in enumerate(topic_word):\n    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n    topic_summaries.append(\" \".join(topic_words))\n    print(\"Topic {}: {}\".format(i, \" | \".join(topic_words)))","1653df9d":"tsne_lda = tsne_model.fit_transform(news_topics)","461b4f46":"news_topics = np.matrix(news_topics)\ndoc_topics = news_topics\/news_topics.sum(axis = 1)\n\nlda_keys = []\nfor i, tweet in enumerate(partial_news[\"headline\"]):\n    lda_keys += [doc_topics[i].argmax()]\n    \ntsne_lda_df = pd.DataFrame(tsne_lda, columns = [\"x\", \"y\"])\ntsne_lda_df[\"headline\"] = partial_news[\"headline\"].values\ntsne_lda_df[\"asset_name\"] = partial_news[\"assetName\"].values\ntsne_lda_df[\"topics\"] = lda_keys\ntsne_lda_df[\"topics\"] = tsne_lda_df[\"topics\"].map(int)","608c57f0":"plot_lda = bp.figure(plot_width = 700, plot_height = 600, \n                    title = \"LDA topics of stock market news\",\n                    tools = \"pan, wheel_zoom, box_zoom, reset, hover, previewsave\",\n                    x_axis_type = None, y_axis_type = None, min_border = 1)\n\nsource = ColumnDataSource(data = dict(x = tsne_lda_df[\"x\"], y = tsne_lda_df[\"y\"],\n                         color = colormap[lda_keys],\n                         headline = tsne_lda_df[\"headline\"],\n                         asset_name = tsne_lda_df[\"asset_name\"],\n                         topics = tsne_lda_df[\"topics\"]))\n\nplot_lda.scatter(x = \"x\", y = \"y\", color = \"color\", source = source)\nhover = plot_lda.select(dict(type = HoverTool))\nhover.tooltips = {\"headline\": \"@headline\", \"asset_name\": \"@asset_name\", \"topics\": \"@topics\"}\nshow(plot_lda)","f9d561ea":"* About $30\\%$ headlines' length are over $100$ \n* A little have zero length. This implies that there are empty headlines.","e5c4b5a6":"### Latent Dirichlet Allocation (LDA)\n>  LDA, a generative probabilistic model for collections of discrete data such as text corpra. LDA is a three-level hierachical Baysian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an inifinte mixture over an underlying set of topic probabilities. \n\nUnlike k-means clustering, LDA is capable to illustrate documents with multible topics","d2813bd7":"### Visualize the distribution of the tf-idf scores ","a50c4880":"## Tf-Idf\n\n> tf-idf stands for term frequencey-inverse document frequency. It's a numerical statistic intended to reflect how important a word is to a document or a corpus (i.e a collection of documents).","0b9e73ef":"As we see from above, the tf-idf documents contain over 50,000 features. In order to visualize the document, we need to reduce the dimension to 2 or 3. To achive this goal, we do the following steps:\n1. Apply Singular Value Decomposition (SVD) to reduce the dimension to a much lower dimension (for example, 50. Due to the limit of kernel, we do 30 in this case)\n2. Apply t-SNE to reduce the dimention to 2    ","e73b8f79":"Examples of more meaningful words:","7d71d643":"## K-Means\nK-means cluster is an another technique to group data by their *similarity*. In this case, we use Euclidean distance to calculate the *similarity*.","338b1161":"**Due to the limit of RAM, we will limit the analysis to news of the last month in 2016 (aka, 2016\/12\/01-2016\/12\/31)**","a12484b8":"As we can from above, we now have 2 features after applying t-SNE.\nTo visualize t-SNE, we use *Bokeh*","caf711d8":"Examples of less meaningful words:","9d77194a":"This is an extention of this [notebook](https:\/\/www.kaggle.com\/konohayui\/two-sigma-market-news-data-eda)\n\nThe analysis in this notebook follows [How to mine newsfeed data and extract interactive insights in Python](https:\/\/ahmedbesbes.com\/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html).","d65eef64":"## t-Stochastic Neighbor Embedding (t-SNE)\n\nt-SNE is an non-linear visualizing technique that visualizes high dimentional data by mapping datapoints into 2 or 3-dimention. \n\nAdvantages using t-SNE:\n1. It is able to convert a high dimentional data set into a matrix of pairwise similarities. \n2. It can well capture most of the local structure of high-dimention data while revealing the globle structure.","6725aa0f":"Some tokens are filled with **NA** because of duplicates and the ones been kept are on the last; thus, we fill those **NA** from below.","a940c8bb":"### To Be Continued...","371ef7af":"### An Overview on News Data"}}