{"cell_type":{"b77c2802":"code","1c779fe2":"code","0d3301f8":"code","187b3096":"code","e89f62fd":"code","89c42e18":"code","a144c96d":"code","687e4c7b":"code","8dbb5e73":"code","618d5292":"code","acae3189":"code","dbdae2d1":"code","5e65e559":"code","a921bc9a":"code","2af8b92a":"code","18b44206":"code","fe636de0":"code","1c7c716e":"code","4195aa82":"code","3fbf4921":"code","50b09633":"code","613bb53b":"code","aee151f2":"code","8f593e40":"code","09afc4f0":"code","aabecda9":"code","c3a61f64":"code","2b3d9125":"code","85107d02":"code","8f7c7a8c":"code","5dea8bcb":"code","d7e7ac86":"code","428c8f61":"code","f0e01fe8":"code","fc9e9482":"code","839f0ba3":"code","4ea6925e":"markdown","a99c3a25":"markdown","a495b348":"markdown","c7b87898":"markdown","33e61348":"markdown","3d0a74e4":"markdown","0938c8e0":"markdown","d131a937":"markdown","f36f4d1e":"markdown","79be646c":"markdown","df6f4cb1":"markdown"},"source":{"b77c2802":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1c779fe2":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import LinearSVC","0d3301f8":"data=pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')","187b3096":"data.head()","e89f62fd":"corr=data.corr()\nsns.heatmap(corr)","89c42e18":"# Target variable = Death Event","a144c96d":"# We have 12 features in total let's find out the 8 best features for our model","687e4c7b":"X=data.drop(columns=['DEATH_EVENT'])\ny=data['DEATH_EVENT']","8dbb5e73":"# Code to select the 8 best features for the dataset with the help of chi2\nbestfeatures = SelectKBest(score_func=chi2, k=8)\nfit = bestfeatures.fit(X,y)","618d5292":"dfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)","acae3189":"featureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(8,'Score')) ","dbdae2d1":"data.drop(columns=['diabetes','sex','smoking','anaemia'],axis=1,inplace=True)","5e65e559":"X=data.drop(columns=['DEATH_EVENT']).values","a921bc9a":"y=data['DEATH_EVENT'].values","2af8b92a":"scaler = StandardScaler()","18b44206":"X=scaler.fit_transform(X)","fe636de0":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)","1c7c716e":"lg=LogisticRegression()","4195aa82":"lg.fit(X_train,y_train)","3fbf4921":"y_hat=lg.predict(X_test)","50b09633":"print('The accuracy score of logistic regression is : ',accuracy_score(y_hat,y_test))","613bb53b":"# Let's do 10 k folds validation set and check the accuracy score\nfor i in range(2,11):\n    cv=LogisticRegressionCV(cv=i,random_state=True,scoring='accuracy').fit(X,y)\n    print(cv.score(X,y))","aee151f2":"# we see that the score is higher when the k folds value is equal to 2,3,4....\n# we will take as 2 in that condition","8f593e40":"cv=LogisticRegressionCV(cv=2,random_state=True,scoring='accuracy').fit(X_train,y_train)\ny_hat_cv=cv.predict(X_test)\nprint('The accuracy score with the CV is :',accuracy_score(y_hat_cv,y_test))","09afc4f0":"dt=DecisionTreeClassifier()","aabecda9":"dt.fit(X_train,y_train)","c3a61f64":"y_hat_dt=dt.predict(X_test)","2b3d9125":"print('The accuracy score of the decision tree classifier is: ',accuracy_score(y_hat_dt,y_test))","85107d02":"# Decision Tree classifier less score than the Logistic Regression","8f7c7a8c":"# let's take 10 nearest neighbors and have a look at the score","5dea8bcb":"wss=[]\nfor i in range(1,11):\n    kn=KNeighborsClassifier(n_neighbors=i)\n    kn.fit(X_train,y_train)\n    y_hat_kn=kn.predict(X_test)\n    wss.append(accuracy_score(y_hat_kn,y_test))\n    ","d7e7ac86":"plt.plot([int(x) for x in range(1,11)],wss)","428c8f61":"# We can see the highest accuracy at n=7","f0e01fe8":"kn=KNeighborsClassifier(n_neighbors=7)\nkn.fit(X_train,y_train)\ny_hat_kn=kn.predict(X_test)\nprint('The accuracy of the K-Nearest Model is : ',accuracy_score(y_hat_kn,y_test))","fc9e9482":"sv=LinearSVC(max_iter=10000)\nsv.fit(X_train,y_train)\ny_hat_sv=sv.predict(X_test)\nprint('The accuracy score using the SVM model is : ',accuracy_score(y_hat_sv,y_test))","839f0ba3":"# Well the model has the same accuracy as the Logistic regression model","4ea6925e":"from this we can see that only first 6 ranked features matters the most but let's carry the model with 8 for now","a99c3a25":"**Let's try support vector machine**","a495b348":"The accuracy increased from the simple logistic regression","c7b87898":"Let's do some feature selection","33e61348":"**We need to standardize the data now**","3d0a74e4":"**Let's start by adding models now**","0938c8e0":"**Logistic Regression**","d131a937":"**Let's make a test train model**","f36f4d1e":"Let's Do K Nearest Neighbors","79be646c":"**Let's try logistic regression with cross validation now **","df6f4cb1":"**Let's Try out decision tree forest here**"}}