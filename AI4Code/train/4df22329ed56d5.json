{"cell_type":{"9d206d8d":"code","c6be5bcc":"code","73a3ce8e":"code","7f9e4ab8":"code","ba6f5501":"code","949925a4":"code","da583f95":"code","1f35247d":"code","00a93deb":"code","777f52e0":"code","bb7cae09":"code","a2d6865e":"code","5b877e1f":"code","90c4862d":"code","b77bfa98":"code","769ff14e":"code","f1314f2e":"code","aad0d3c7":"code","25da9a8a":"code","4ffab9d8":"code","6a9043e4":"code","1f96ec76":"code","370f5fd5":"code","1ee0008f":"code","acdb2d59":"code","c53d5fd9":"code","47fa5f95":"code","f609b432":"code","e2c9fca6":"code","f694ec03":"code","7b000798":"code","0bf6335a":"code","7e9bbd3a":"markdown","379f3434":"markdown","e8ad7d76":"markdown","ad3a7d50":"markdown","61ad37c3":"markdown","5338074d":"markdown"},"source":{"9d206d8d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c6be5bcc":"# Load data using pandas\ndata = pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')\ndata.head()","73a3ce8e":"data.shape         ","7f9e4ab8":"data.describe().T    # It describes the dataset statistically","ba6f5501":" data.info()   #Basic info about dataset","949925a4":"data.isnull().sum()             # check null values","da583f95":"for i in data.columns:\n    sns.distplot(data[i])\n    plt.xlabel(i)\n    plt.ylabel('Count')\n    plt.show()","1f35247d":"sns.pairplot(data)","00a93deb":"plt.figure(figsize=(12,10))\nsns.heatmap(data.corr(),annot=True)     # It shows us the correlation between features using heatmap","777f52e0":"# Divide the dataset into dependent and independent Features\nX = data.drop(['target'],axis=1)\ny = data['target']","bb7cae09":"#import libraries and split dataset\nfrom sklearn.model_selection import train_test_split,cross_val_score,RandomizedSearchCV,GridSearchCV\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\nX_train","a2d6865e":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","5b877e1f":"models =[]\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\n\n#Decision Tree\n\ntr = DecisionTreeClassifier(criterion = 'entropy',random_state=1)\ntr.fit(X_train,y_train)\ntr_pred=tr.predict(X_test)\nprint('DecisionTreeClassifier: ',accuracy_score(y_test,tr_pred)*100)\nmodels.append(['DecisionTreeClassifier',accuracy_score(y_test,tr_pred)*100])\n","90c4862d":"#RandomForest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=50, random_state=42)\nrf.fit(X_train,y_train)\nrf_pred=rf.predict(X_test)\nmodels.append(['RandomForestClassifier: ',accuracy_score(y_test,rf_pred)*100])","b77bfa98":"# K nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\nkn = KNeighborsClassifier(n_neighbors=6)\nkn.fit(X_train,y_train)\nkn_pred=kn.predict(X_test)\nmodels.append(['KNeighborsClassifier: ',accuracy_score(y_test,kn_pred)*100])","769ff14e":"#XGB Classifier\n\nfrom xgboost import XGBClassifier\n\nxgb = XGBClassifier()\n\nxgb.fit(X_train,y_train)\nxgb_pred = xgb.predict(X_test)\nmodels.append(['XGBClassifier: ',accuracy_score(y_test,xgb_pred)*100])","f1314f2e":"#CatBoostClassifier\n\nfrom catboost import CatBoostClassifier\n\ncb = CatBoostClassifier(iterations=50, \n    learning_rate=0.01,depth = 3)\ncb.fit(X_train,y_train)\ncb_pred = cb.predict(X_test)\nmodels.append(['CatBoostClassifier: ',accuracy_score(y_test,cb_pred)*100])","aad0d3c7":"#GaussianNB\nfrom sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB()\nnb.fit(X_train,y_train)\nnb_pred = nb.predict(X_test)\nmodels.append(['GaussianNB: ',accuracy_score(y_test,nb_pred)*100])","25da9a8a":"#ExtraTreesClassifier\n\nfrom sklearn.ensemble import ExtraTreesClassifier\n\net = ExtraTreesClassifier()\net.fit(X_train,y_train)\net_pred = et.predict(X_test)\nmodels.append(['ExtraTreeClassifier: ',accuracy_score(y_test,et_pred)*100])","4ffab9d8":"#AdaboostClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\nab = AdaBoostClassifier()\nab.fit(X_train,y_train)\nab_pred = ab.predict(X_test)\nmodels.append(['AdaBoostClassifier: ',accuracy_score(y_test,ab_pred)*100])","6a9043e4":"#LightGBM\nfrom lightgbm import LGBMClassifier\n\nlgbm = LGBMClassifier(learning_rate=0.01,n_estimators=40)\nlgbm.fit(X_train,y_train)\nlgbm_pred = lgbm.predict(X_test)\nmodels.append(['LGBMClassifier: ',accuracy_score(y_test,lgbm_pred)*100])","1f96ec76":"# ANN\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Activation,Dropout,Dense,Flatten\n\nimport warnings","370f5fd5":"model = Sequential()\nmodel.add(Dense(128,activation='relu',input_dim=13))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(128,activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(1,activation='sigmoid'))\n\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n","1ee0008f":"model.fit(X_train,y_train,batch_size=50,epochs=90,validation_data=(X_test,y_test))","acdb2d59":"models.append(['ANN_Accuracy: ','61.0000000'])","c53d5fd9":"\n# Parameter for hyperparameter tuning of XGBOOSTClassifier\nparams={\n    \"learning_rate\" :[0.05,0.10,0.15,0.20,0.25,0.30],\n    \"max_depth\" : [3,4,5,6,7,8,10,12,15],\n    \"min_child_weight\": [1,3,5,7],\n    \"gamma\": [0.0,0.1,0.2,0.3,0.4,0.5],\n    \"colsample_bytree\" :[0.3,0.4,0.5,0.7,0.9]\n}\n\nrandom_search = RandomizedSearchCV(xgb,param_distributions=params,n_iter=5,scoring='roc_auc',cv=5,verbose=3,n_jobs=-1)\n\nrandom_search.fit(X,y)\n\nrandom_search.best_estimator_","47fa5f95":"classifier= XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.5, gamma=0.2, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.1, max_delta_step=0, max_depth=8,\n              min_child_weight=5, monotone_constraints='()',\n              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)\n\nscore = cross_val_score(classifier,X,y,cv=5)\n\nmodels.append(['Tunned Xgboost: ', score.mean()*100])","f609b432":"# hyperparameter tuning of LGBMClassifier\nparams={\n    \"learning_rate\" :[0.05,0.10,0.15,0.20,0.25,0.30],\n    \"max_depth\" : [3,4,5,6,7,8,10,12,15],\n    \"min_child_weight\": [1,3,5,7],\n    \"reg_alpha\": [0.0,0.1,0.2,0.3,0.4,0.5],\n    \"reg_lambda\": [0.0,0.1,0.2,0.3,0.4,0.5],\n    \"colsample_bytree\" :[0.3,0.4,0.5,0.7,0.9]\n}\n\nlgbm_class = LGBMClassifier()\nrandom_search_lgbm = RandomizedSearchCV(lgbm_class,param_distributions=params,n_iter=5,scoring='roc_auc',cv=5,verbose=3,n_jobs=-1)\nrandom_search_lgbm.fit(X,y)","e2c9fca6":"random_search_lgbm.best_estimator_\n","f694ec03":"lgbm_classifiers = LGBMClassifier(colsample_bytree=0.3, learning_rate=0.05, max_depth=7,\n               min_child_weight=3, reg_alpha=0.5, reg_lambda=0.5)\n\nscore = cross_val_score(lgbm_classifiers,X,y,cv=5)","7b000798":"models.append(['Tuned LGBMClassifier',score.mean()*100])\n","0bf6335a":"df = pd.DataFrame(models,columns=['Models','Scores'])\ndf","7e9bbd3a":"## MULTIVARIATE ","379f3434":"# EDA","e8ad7d76":"## Models\n- DecisionTreeClassifier\n- RandomForestClassifier\n- KNeighborsClassifier\n- XGBClassifier\n- CatBoostClassifier\n- GaussianNB\n- ExtraTreesClassifier\n- AdaBoostClassifier\n- LightGBMClassifier\n- Tuned Xgboostclassifier\n- Artificial Neural Network\n- Tuned LGBMClassifier\n","ad3a7d50":"Here we can see there are two models having heighest score Catboost and LGBMClassifier.So we try hyperparameter tuning using LGBMClassifier.","61ad37c3":"Hurray!!!\nKNeighborsClassifier won with score of 90.1639","5338074d":"## Univariate Analysis"}}