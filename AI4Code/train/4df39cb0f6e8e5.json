{"cell_type":{"03dca023":"code","ba4a10e4":"code","38240d16":"code","eca06d97":"code","ca0fd412":"code","5f0c6585":"code","adce552b":"code","91ad767e":"code","c0fcfc1c":"code","a0f26ef7":"code","d0d71e16":"code","bf72d5a2":"code","5f53ca37":"code","ae17109f":"code","754f424a":"code","8b2a9fc1":"code","feca98ae":"code","8182d154":"code","0885888d":"code","1b82b6d3":"code","cac8e7fb":"code","67b054d3":"code","c1f02846":"code","060f4096":"code","22020098":"code","f853b44b":"code","d4ee4280":"code","eed45ada":"code","bd4d62ef":"code","ede03d7b":"markdown"},"source":{"03dca023":"import os\nbase_path=\"mycomputer\\\\Probabilistic Graphical Models\\\\Lab Exam\\\\Lab Exam\"\n\nos.chdir(base_path)\n\n\nfrom pdfminer.pdfparser import PDFParser\nfrom pdfminer.pdfdocument import PDFDocument\nfrom pdfminer.pdfpage import PDFPage\nfrom pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\nfrom pdfminer.converter import PDFPageAggregator\nfrom pdfminer.layout import LAParams, LTTextBox, LTTextLine\n\n\n# Importing necessary library\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport os\nimport nltk.corpus\n# importing word_tokenize from nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk import ne_chunk\n\nimport nltk\n\n# uncomment and run when using first time\n#nltk.download('stopwords')\n#nltk.download('averaged_perceptron_tagger')\n#nltk.download('words')\n#nltk.download('maxent_ne_chunker')\n\n\n","ba4a10e4":"my_file = os.path.join(base_path + \"\/\" + \"panchatantra-tales_complete.pdf\")\nfp = open(my_file, 'rb')\nparser = PDFParser(fp)\ndoc = PDFDocument(parser)\n#parser.set_document(doc)\n#doc.set_parser(parser)\n#doc.initialize('')\nrsrcmgr = PDFResourceManager()\nlaparams = LAParams()\nlaparams.char_margin = 1.0\nlaparams.word_margin = 1.0\ndevice = PDFPageAggregator(rsrcmgr, laparams=laparams)\ninterpreter = PDFPageInterpreter(rsrcmgr, device)\n\nextracted_text = ''\n\nfor page in PDFPage.create_pages(doc):\n        interpreter.process_page(page)\n        layout = device.get_result()\n        for lt_obj in layout:\n            if isinstance(lt_obj, LTTextBox) or isinstance(lt_obj, LTTextLine):\n                extracted_text += lt_obj.get_text()\n                \n#print(extracted_text)","38240d16":"start =extracted_text.find(\"1.The Monkey And The Wedge\")\nend=extracted_text.find(\"2.The Jackal And The Drum\")\n\nprint(start, end)","eca06d97":"story= extracted_text[start:end]\nprint(story[0:500])\n#printing limited characters","ca0fd412":"# Passing the string text into word tokenize for breaking the sentences\nword_tokens = word_tokenize(story)\n#print(word_tokens)","5f0c6585":"# finding the frequency distinct in the tokens\n# Importing FreqDist library from nltk and passing token into FreqDist\nfdist = FreqDist(word_tokens)\nfdist","adce552b":"# To find the frequency of top 10 words\nfdist1 = fdist.most_common(10)\nfdist1","91ad767e":"from nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nstop_words = set(stopwords.words('english'))\n\nfiltered_sentence = [w for w in word_tokens if not w in stop_words] \n  \nfiltered_sentence = [] \n  \nfor w in word_tokens: \n    if w not in stop_words: \n        filtered_sentence.append(w) \n  \nprint(word_tokens[0:30]) \nprint(filtered_sentence[0:30]) \n","c0fcfc1c":"#POS Tagging\nfor tex in word_tokens:\n    pos_tag=nltk.pos_tag([tex])\n    #print(pos_tag)\n\n## for brevity, printing only limited number of POS_tag as below, otherwise upper loop prints all POS\nfor tex in word_tokens[0:20]:\n    pos_tag=nltk.pos_tag([tex])\n    print(pos_tag)","a0f26ef7":"from nltk import CFG\ngroucho_grammar = CFG.fromstring(\"\"\"\nS -> NP VP\nPP -> P NP\nNP -> Det N | Det N PP | 'I'\nVP -> V NP | VP PP\nDet -> 'an' | 'my'\nN -> 'elephant' | 'pajamas'\nV -> 'shot'\nP -> 'in'\n\"\"\")","d0d71e16":"# ref: http:\/\/www.nltk.org\/book_1ed\/ch08.html\n\nsent = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']\nparser = nltk.ChartParser(groucho_grammar)\ntrees = parser.parse(sent)\nfor tree in trees:\n    print (tree)","bf72d5a2":"## part C begins here","5f53ca37":"#conda install -c conda-forge spacy=2.2.1\n# conda install -c conda-forge spacy-model-en_core_web_sm\n#https:\/\/www.analyticsvidhya.com\/blog\/2019\/10\/how-to-build-knowledge-graph-text-using-spacy\/\nimport spacy\nprint(spacy.__version__)\n\n### extracting dependency parsing because POS tagging is not sufficient many times\nnlp = spacy.load('en_core_web_sm')\n\n#Example\ndoc = nlp(\"The 22-year-old recently won ATP Challenger tournament.\")\n\nfor tok in doc:\n  print(tok.text, \"...\", tok.dep_)","ae17109f":"import re\nimport pandas as pd\nimport bs4\nimport requests\nimport spacy\nfrom spacy import displacy\nnlp = spacy.load('en_core_web_sm')\n\nfrom spacy.matcher import Matcher \nfrom spacy.tokens import Span \n\nimport networkx as nx\n\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\npd.set_option('display.max_colwidth', 200)\n%matplotlib inline\n","754f424a":"doc=nlp(story)\n# for bravity, printing only limited output\nfor tok in doc[0:20]:\n  print(tok.text, \"...\", tok.dep_)","8b2a9fc1":"#The main idea is to go through a sentence and extract the subject and the object as and when they are encountered so that we can\n# have nodes and edges for the graph\n\ndef get_entities(sent):\n  ## chunk 1\n  ent1 = \"\"\n  ent2 = \"\"\n\n  prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n  prv_tok_text = \"\"   # previous token in the sentence\n\n  prefix = \"\"\n  modifier = \"\"\n\n  #############################################################\n  \n  for tok in nlp(sent):\n    ## chunk 2\n    # if token is a punctuation mark then move on to the next token\n    if tok.dep_ != \"punct\":\n      # check: token is a compound word or not\n      if tok.dep_ == \"compound\":\n        prefix = tok.text\n        # if the previous word was also a 'compound' then add the current word to it\n        if prv_tok_dep == \"compound\":\n          prefix = prv_tok_text + \" \"+ tok.text\n      \n      # check: token is a modifier or not\n      if tok.dep_.endswith(\"mod\") == True:\n        modifier = tok.text\n        # if the previous word was also a 'compound' then add the current word to it\n        if prv_tok_dep == \"compound\":\n          modifier = prv_tok_text + \" \"+ tok.text\n      \n      ## chunk 3\n      if tok.dep_.find(\"subj\") == True:\n        ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n        prefix = \"\"\n        modifier = \"\"\n        prv_tok_dep = \"\"\n        prv_tok_text = \"\"      \n\n      ## chunk 4\n      if tok.dep_.find(\"obj\") == True:\n        ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n        \n      ## chunk 5  \n      # update variables\n      prv_tok_dep = tok.dep_\n      prv_tok_text = tok.text\n  #############################################################\n\n  return [ent1.strip(), ent2.strip()]\n","feca98ae":"from nltk import sent_tokenize\n\nsentence=sent_tokenize(story)\nprint (sentence[0])","8182d154":"entity_pairs = []\n\nfor i in tqdm(sentence):\n    entity_pairs.append(get_entities(i))","0885888d":"entity_pairs[10:20]","1b82b6d3":"#Our hypothesis is that the predicate is actually the main verb in a sentence.\n#The function below is capable of capturing such predicates from the sentences.\n#The pattern defined in the function tries to find the ROOT word or the main verb in the sentence. \n#Once the ROOT is identified, then the pattern checks whether it is followed by a preposition (\u2018prep\u2019) \n#or an agent word. If yes, then it is added to the ROOT word.\n\n\n# some patterns and their meaning\n\n#GDP --> nsubj --> NOUN\n#in --> prep --> ADP\n#developing --> amod --> VERB\n#countries --> pobj --> NOUN\n#such --> amod --> ADJ\n#as --> prep --> ADP\n#Vietnam --> pobj --> PROPN\n#will --> aux --> VERB\n#continue --> ROOT --> VERB\n#growing --> xcomp --> VERB\n#at --> prep --> ADP\n#a --> det --> DET\n#high --> amod --> ADJ\n#rate --> pobj --> NOUN\n#. --> punct --> PUNCT\n\ndef get_relation(sent):\n\n  doc = nlp(sent)\n\n  # Matcher class object \n  matcher = Matcher(nlp.vocab)\n\n  #define the pattern \n  pattern = [{'DEP':'ROOT'}, \n            {'DEP':'prep','OP':\"?\"},\n            {'DEP':'agent','OP':\"?\"},  \n            {'POS':'ADJ','OP':\"?\"},\n            {\"POS\": \"ADV\", \"OP\": \"*\"},\n            {'DEP':'amod', 'OP':\"?\"}] \n\n  matcher.add(\"matching_1\", None, pattern) \n\n  matches = matcher(doc)\n  k = len(matches) - 1\n\n  span = doc[matches[k][1]:matches[k][2]] \n\n  return(span.text)","cac8e7fb":"relations = [get_relation(i) for i in tqdm(sentence)]","67b054d3":"print(relations)","c1f02846":"#most frequent relations or predicates that we have just extracted:\npd.Series(relations).value_counts()[:10]","060f4096":"total= len(relations)\ntotal\n### joint probability table\nprob_table = pd.Series(relations).value_counts()[:10]\/ total\nprint(prob_table)","22020098":"#we create a dataframe of entities and predicates:\n\n# extract subject\nsource = [i[0] for i in entity_pairs]\n\n# extract object\ntarget = [i[1] for i in entity_pairs]\n\nkg_df = pd.DataFrame({'source':source, 'target':target, 'edge':relations})","f853b44b":"#print(source[1:10])\n#print(target[1:10])\n\nprint(kg_df)","d4ee4280":"# create a directed-graph from a dataframe\nG=nx.from_pandas_edgelist(kg_df, \"source\", \"target\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())","eed45ada":"plt.figure(figsize=(12,12))\n\npos = nx.spring_layout(G)\nnx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos = pos)\nplt.show()","bd4d62ef":"G=nx.from_pandas_edgelist(kg_df[kg_df['edge']==\"told\"], \"source\", \"target\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())\n\nplt.figure(figsize=(12,12))\npos = nx.spring_layout(G, k=0.5) # k regulates the distance between nodes\nnx.draw(G, with_labels=True, node_color='skyblue', node_size=2500, edge_cmap=plt.cm.Blues, pos = pos)\nplt.show()","ede03d7b":"# The End"}}