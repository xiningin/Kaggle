{"cell_type":{"753d4f30":"code","5ff82a91":"code","a2f1ba4b":"code","895d9401":"code","213254f9":"code","4b66f1dd":"code","fd0e7c0b":"code","744ed3f4":"code","4a0cca4d":"code","cd30152c":"markdown","248a8c07":"markdown","66a35278":"markdown","2bde3d1b":"markdown","3d17ae69":"markdown","7bc691ee":"markdown","817830e2":"markdown","5eef9876":"markdown","3ac3f597":"markdown","b167a329":"markdown","74640cf1":"markdown","7a7f9fda":"markdown","56099918":"markdown","1f12a588":"markdown"},"source":{"753d4f30":"import pandas as pd, numpy as np, os, gc, matplotlib.pyplot as plt, seaborn as sb, re, warnings, calendar, sys\nfrom numpy import arange\n%matplotlib inline\nwarnings.filterwarnings('ignore'); np.set_printoptions(suppress=True); pd.options.mode.chained_assignment = None\npd.set_option('display.float_format', lambda x: '%.3f' % x)\nglobal directory; directory = '..\/input'\n\ndef files(): return os.listdir(directory)\n\ndef read_clean(data):\n    data.columns = [str(x.lower().strip().replace(' ','_')) for x in data.columns]\n    seen = {}; columns = []; i = 0\n    for i,x in enumerate(data.columns):\n        if x in seen: columns.append(x+'_{}'.format(i))\n        else: columns.append(x)\n        seen[x] = None\n        \n    for x in data.columns[data.count()\/len(data) < 0.0001]: del data[x];\n    gc.collect();\n    try: data = data.replace({'':np.nan,' ':np.nan});\n    except: pass;\n    \n    if len(data) < 10000: l = len(data);\n    else: l = 10000;\n    sample = data.sample(l);size = len(sample);\n    \n    for x in sample.columns:\n        ints = pd.to_numeric(sample[x], downcast = 'integer', errors = 'coerce')\n        if ints.count()\/size > 0.97:\n            minimum = ints.min()\n            if minimum > 0: data[x] = pd.to_numeric(data[x], downcast = 'unsigned', errors = 'coerce')\n            else: data[x] = pd.to_numeric(data[x], downcast = 'integer', errors = 'coerce')\n        else:\n            floats = pd.to_numeric(sample[x], downcast = 'float', errors = 'coerce')\n            if floats.count()\/size > 0.97: data[x] = pd.to_numeric(data[x], downcast = 'float', errors = 'coerce')\n            else:\n                dates = pd.to_datetime(sample[x], errors = 'coerce')\n                if dates.count()\/size > 0.97: data[x] = pd.to_datetime(data[x], errors = 'coerce')\n    return data.reset_index(drop = True)\n\ndef read(x):\n    '''Kaggle Reading in CSV files.\n    Just type read('file.csv'), and you'll get back a Table.'''\n    \n    file = '{}\/{}'.format(directory,x)\n    try:     data = pd.read_csv(file)\n    except:  data = pd.read_csv(file, encoding = 'latin-1')\n    return read_clean(data)\n\ndata = read('ramen-ratings.csv').sample(1000)\n\ndef tally(column, minimum = 0, top = None, graph = False, percent = False, multiple = False, lowercase = False, min_count = 1):\n    '''Provides a tally count of all values in a COLUMN.\n        1. minimum  =  (>0)          Least count of item to show.\n        2. top      =  (-1,>0)       Only show top N objects\n        3. graph    =  (False,True)  Show bar graph of results\n        4. percent  =  (False,>0)    Instead of showing counts, show percentages of total count\n        \n       multiple = False\/True.\n       If True, counts and tallies objects in list of lists (Count Vectorizer)\n       \n       lowercase = True \/ False.\n       If True, lowers all text firsrt. So A == a\n       \n       min_count >= 1\n       If a column sum for tag has less than min_count, discard whole column\n    '''\n    if multiple == False:\n        counts = column.value_counts().astype('uint')\n        counts = counts[counts >= minimum][:top]\n        counts = pd.DataFrame(counts).reset_index()\n        counts.columns = [column.name, 'tally']\n        if percent: \n            counts['tally'] \/= counts['tally'].sum()\/100\n            counts['tally'] = counts['tally']\n        if graph:\n            C = counts[::-1]\n            C.plot.barh(x = column.name, y = 'tally', legend = False); plt.show();\n        return counts\n    else:\n        from sklearn.feature_extraction.text import CountVectorizer\n        if type(column.iloc[0]) != list: column = column.apply(lambda x: [x])\n        counter = CountVectorizer(lowercase = lowercase, tokenizer = lambda x: x, dtype = np.uint32, min_df = min_count)\n        counter.fit(column)\n        counts = pd.DataFrame(counter.transform(column).toarray())\n        counts.columns = [column.name+'_('+str(x)+')' for x in counter.get_feature_names()]\n        return counts\n    \n    \ndef describe(data):\n    '''Provides an overview of your data\n        1. dtype    =  Column type\n        2. missing% =  % of the column that is missing\n        3. nunique  =  Number of unique values in column\n        4. top3     =  Top 3 most occuring items\n        5. min      =  Minimum value. If not a number column, then empty\n        6. mean     =  Average value. If not a number column, then empty\n        7. median   =  Middle value. So sort all numbers, and get middle. If not a number column, then empty\n        8. max      =  Maximum value. If not a number column, then empty\n        9. sample   =  Random 2 elements\n        10. name    =  Column Name\n    '''\n    dtypes = dtype(data)\n    length = len(data)\n    missing = ((length - data.count())\/length*100)\n    \n    N = [];    most3 = []\n    for dt,col in zip(dtypes,data.columns):\n        if dt != 'datetime':\n            U = data[col].value_counts()\n            N.append(len(U))\n            if U.values[0] > 1: most3.append(U.index[:3].tolist())\n            else: most3.append([]);\n        else: N.append(0); most3.append([]);\n            \n    df = pd.concat([dtypes, missing], 1)\n    df.columns = ['dtype','missing%']\n    df['nunique'] = N; df['top3'] = most3\n    \n    numbers = list(data.columns[df['dtype'].isin(('uint','int','float'))])\n    df['min'] = data.min()\n    df['mean'] = data[numbers].mean()\n    df['median'] = data[numbers].median()\n    df['max'] = data.max()\n    df['sample'] = data.apply(lambda x : x.sample(2).values.tolist())\n    df['name'] = list(data.columns)\n    return df.sort_values(['missing%', 'nunique', 'dtype'], ascending = [False, False, True]).reset_index(drop = True)\n\n\ndef Checker(x):\n    if type(x) is pd.DataFrame: return 0\n    elif type(x) is pd.Series: return 1\n    else: return -1\n\ndef columns(data): return list(data.columns)\ndef rows(data): return list(data.index)\ndef index(data): return list(data.index)\ndef head(data, n = 10): return data.head(n)\ndef tail(data, n = 10): return data.tail(n)\ndef sample(data, n = 10): return data.sample(n)\n\ndef dtype(data):\n    what = Checker(data)\n    if what == 0:\n        dtypes = data.dtypes.astype('str')\n        dtypes = dtypes.str.split(r'\\d').str[0]\n    else:\n        dtypes = str(data.dtypes)\n        dtypes = re.split(r'\\d', dtypes)[0]\n    return dtypes\n\ndef mean(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].mean()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.mean()\n        else: return np.nan\n    else:\n        try:     return np.nanmean(data)\n        except:  return np.nan\n        \ndef std(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].std()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.std()\n        else: return np.nan\n    else:\n        try:     return np.nanstd(data)\n        except:  return np.nan\n        \ndef var(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].var()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.var()\n        else: return np.nan\n    else:\n        try:     return np.nanvar(data)\n        except:  return np.nan\n        \ndef log(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        x = np.log(data[numbers])\n        x[np.isinf(x)] = np.nan\n        return pd.Series(x)\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt:\n            x = np.log(data)\n            x[np.isinf(x)] = np.nan\n            return x\n        else: return np.nan\n    else:\n        try:\n            x = np.log(data)\n            x[np.isinf(x)] = np.nan\n            return x\n        except:  return np.nan\n        \ndef median(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].median()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.median()\n        else: return np.nan\n    else:\n        try:     return np.nanmedian(data)\n        except:  return np.nan\n        \ndef minimum(data):\n    what = Checker(data)\n    if what == 0:      return data.min()\n    elif what == 1:    return data.min()\n    else:              return np.min(data)\n        \ndef maximum(data):\n    what = Checker(data)\n    if what == 0:      return data.max()\n    elif what == 1:    return data.max()\n    else:              return np.max(data)\n    \ndef missing(data):\n    what = Checker(data)\n    if what >= 0:      return pd.isnull(data)\n    else:              return np.isnan(data)\n    \ndef count(data):\n    what = Checker(data)\n    if what >= 0:      return data.count()\n    else:              return len(data)\n    \ndef nunique(data):\n    what = Checker(data)\n    if what >= 0:      return data.nunique()\n    else:              return len(np.unique(data))\n    \ndef unique(data):\n    if type(data) is pd.DataFrame:\n        uniques = []\n        for x in data.columns:\n            uniques.append(data[x].unique())\n        df = pd.Series(uniques)\n        df.index = data.columns\n        return df\n    elif type(data) is pd.Series: return data.unique()\n    else:              return np.unique(data)\n    \ndef total(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].sum()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.sum()\n        else: return np.nan\n    else:\n        try:     return np.nansum(data)\n        except:  return np.nan\n\ndef hcat(*columns):\n    cols = []\n    for c in columns:\n        if c is None: continue;\n        if type(c) in (list, tuple): \n            for i in c:\n                if type(i) not in (pd.DataFrame, pd.Series): cols.append(pd.Series(i))\n                else: cols.append(i)\n        elif type(c) not in (pd.DataFrame, pd.Series): cols.append(pd.Series(c))\n        else: cols.append(c)\n    return pd.concat(cols, 1)\n\ndef vcat(*columns):\n    cols = []\n    for c in columns:\n        if c is None: continue;\n        if type(c) in (list, tuple): \n            for i in c:\n                if type(i) not in (pd.DataFrame, pd.Series): cols.append(pd.Series(i))\n                else: cols.append(i)\n        elif type(c) not in (pd.DataFrame, pd.Series): cols.append(pd.Series(c))\n        else: cols.append(c)\n    return pd.concat(cols, 0)\n    \ndef tabulate(*columns, method = 'count'):\n    '''Splits columns into chunks, and counts the occurences in each group.\n        Remember - tabulate works on the LAST column passed.\n        Options:\n            1. count            = Pure Count in group\n            2. count_percent    = Percentage of Count in group\n            3. mean             = Mean in group\n            4. median           = Median in group\n            5. max              = Max in group\n            6. min              = Min in group\n            7. sum_percent      = Percentage of Sum in group\n        Eg:\n            Apple | 1\n            ---------\n            Orange| 3\n            ---------\n            Apple | 2\n            ---------\n        Becomes:\n            Apple | 1 | 1\n            -------------\n                  | 2 | 1\n            -------------\n            Orange| 3 | 1\n        \n        NOTE --------\n            method can be a list of multiple options.\n    '''\n    if type(method) in (list, tuple):\n        xs = []\n        for x in method:\n            g = tabulate(*columns, method = x)\n            xs.append(g)\n        xs = hcat(xs)\n        xs = xs.T.drop_duplicates().T\n        return read_clean(xs)        \n    else:\n        def percent(series):\n            counts = series.count()\n            return counts.sum()\n\n        data = hcat(*columns)\n        columns = data.columns.tolist()\n\n        if method in ('count', 'count_percent'):\n            groups = data.groupby(data.columns.tolist()).apply(lambda x: x[data.columns[-1]].count())\n\n            if method == 'count_percent':\n                groups = groups.reset_index()\n                groups.columns = list(groups.columns[:-1])+['Group_Count']\n                right = data.groupby(columns[:-1]).count().reset_index()\n                right.columns = list(right.columns[:-1])+['Group_Sum']\n\n                groups = pd.merge(left = groups, right = right, left_on = columns[:-1], right_on = columns[:-1])\n                groups['Percent%'] = groups['Group_Count']\/groups['Group_Sum']*100\n                groups = groups[columns+['Percent%']]\n                return groups\n\n        elif method == 'mean': groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].mean())\n        elif method == 'median': groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].median())\n        elif method == 'max': groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].max())\n        elif method == 'min': groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].min())\n        elif method == 'sum_percent':\n            groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].sum()).reset_index()\n            groups.columns = list(groups.columns[:-1])+['Group_Count']\n            right = data.groupby(columns[:-1]).sum().reset_index()\n            right.columns = list(right.columns[:-1])+['Group_Sum']\n\n            groups = pd.merge(left = groups, right = right, left_on = columns[:-1], right_on = columns[:-1])\n            groups['Sum%'] = groups['Group_Count']\/groups['Group_Sum']*100\n            groups = groups[cols+['Sum%']]\n            return groups\n        else:\n            print('Method does not exist. Please choose count, count_percent, mean, median, max, min, sum_percent.'); return None;\n        #except: print('Method = {}'.format(method)+' cannot work on Object, Non-Numerical data. Choose count.'); return None;\n\n        groups = pd.DataFrame(groups)\n        groups.columns = [method]\n        groups.reset_index(inplace = True)\n        return groups\n\n\ndef sort(data, by = None, how = 'ascending', inplace = False):\n    ''' how can be 'ascending' or 'descending' or 'a' or 'd'\n    It can also be a list for each sorted column.\n    '''\n    replacer = {'ascending':True,'a':True,'descending':False,'d':False}\n    if by is None and type(data) is pd.Series:\n        try:    x = replacer[how]\n        except: print(\"how can be 'ascending' or 'descending' or 'a' or 'd'\"); return None;\n        return data.sort_values(ascending = x, inplace = inplace)\n    elif type(how) is not list:\n        try:    how = replacer[how]\n        except: print(\"how can be 'ascending' or 'descending' or 'a' or 'd'\"); return None;\n    else:\n        for x in how: \n            try:    x = replacer[x]\n            except: print(\"how can be 'ascending' or 'descending' or 'a' or 'd'\"); return None;\n    return data.sort_values(by, ascending = how, inplace = inplace)\n\ndef keep(data, what, inplace = False):\n    '''Keeps data in a column if it's wanted.\n    Everything else is filled with NANs'''\n    if type(what) not in (list,tuple,np.array,np.ndarray): what = [what]\n    need = data.isin(what)\n    if inplace: \n        df = data\n        df.loc[~need] = np.nan\n    else: \n        df = data.copy()\n        df.loc[~need] = np.nan\n        return df\n\ndef remove(data, what, inplace = False):\n    '''Deletes data in a column if it's not wanted.\n    Everything else is filled with NANs'''\n    if type(what) not in (list,tuple): what = [what]\n    need = data.isin(what)\n    if inplace: \n        df = data\n        df.loc[need] = np.nan\n    else: \n        df = data.copy()\n        df.loc[need] = np.nan\n        return df\n    \n    \ndef ternary(data, condition, true, false = np.nan, inplace = False):\n    '''C style ternary operator on column.\n    Condition executes on column, and if true, is filled with some value.\n    If false, then replaced with other value. Default false is NAN.'''\n    try:\n        execute = 'data {}'.format(condition)\n        series = eval(execute)\n        try: series = series.map({True:true, False:false})\n        except: series = series.replace({True:true, False:false})\n        return series\n    except: print('Ternary accepts conditions where strings must be enclosed.\\nSo == USD not allowed. == \"USD\" allowed.'); return False;\n\n    \ndef locate(data, column):\n    '''Use ternary to get result and then filter with notnull'''\n    if dtype(column) == 'bool': return data.loc[column]\n    return data.loc[column.notnull()]\n    \ndef query(data, column = None, condition = None):\n    '''Querying data based on conditions'''\n    def Q(data, column, condition):\n        if column is not None:\n            if type(condition) in (np.array, np.ndarray, list, tuple, set):\n                cond = keep(data[column], tuple(condition))\n                cond = (cond.notnull())\n            else: cond = ternary(data[column], condition, True, False)\n            return data.loc[cond]\n        else:\n            if type(condition) in (np.array, np.ndarray, list, tuple, set):\n                cond = keep(data, tuple(condition))\n            else: cond = ternary(data, condition, True, False)\n            return data.loc[cond]\n    try:\n        return Q(data, column, condition)\n    except:\n        condition = condition.replace('=','==')\n        return Q(data, column, condition)\n        \ndef keep_top(x, n = 5):\n    '''Keeps top n (after tallying) in a column'''\n    df = keep(x, tally(x)[x.name][:n].values)\n    return df\n\ndef keep_bot(x, n = 5):\n    '''Keeps bottom n (after tallying) in a column'''\n    df = keep(x, tally(x)[x.name][:-n].values)\n    return df\n    \n    \ndef plot(x, y = None, colour = None, column = None, data = None, size = 5, top = 10, wrap = 4, \n         subset = 5000, method = 'mean', quantile = True, bins = 10,\n         style = 'lineplot', logx = False, logy = False, logc = False, power = 1):\n    '''Plotting function using seaborn and matplotlib\n        Options:\n        x, y, colour, column, subset, style, method\n        \n        Plot styles:\n            1. boxplot\n            2. barplot\n            3. tallyplot (counting number of appearances)\n            4. violinplot (boxplot just fancier)\n            5. lineplot (mean line plot)\n            6. histogram\n            7. scatterplot (X, Y must be numeric --> dates will be converted)\n            8. bivariate (X, Y must be numeric --> dates will be converted)\n            9. heatmap (X, Y will be converted into categorical automatically --> bins)\n            10. regplot (X, Y must be numeric --> dates will be converted)\n    '''\n    if type(x) in (np.array,np.ndarray): x = pd.Series(x); x.name = 'x';\n    if type(y) in (np.array,np.ndarray): y = pd.Series(y); y.name = 'y';\n    if type(column) in (np.array,np.ndarray): column = pd.Series(column); column.name = 'column';\n    if type(colour) in (np.array,np.ndarray): colour = pd.Series(colour); colour.name = 'colour';\n        \n    if type(x) == pd.Series: \n        data = pd.DataFrame(x); x = x.name\n        if type(x) is not str:\n            data.columns = [str(x)]\n            x = str(x)\n    if method == 'mean': estimator = np.nanmean\n    elif method == 'median': estimator = np.nanmedian\n    elif method == 'min': estimator = np.min\n    elif method == 'max': estimator = np.max\n    else: print('Wrong method. Allowed = mean, median, min, max'); return False;\n    #----------------------------------------------------------\n    sb.set(rc={'figure.figsize':(size*1.75,size)})\n    dtypes = {'x':None,'y':None,'c':None,'col':None}\n    names = {'x':None,'y':None,'c':None,'col':None}\n    xlim = None\n    #----------------------------------------------------------\n    if data is not None:\n        if type(x) is str: x = data[x];\n        if type(y) is str: y = data[y]; \n        if type(colour) is str: colour = data[colour]; \n        if type(column) is str: column = data[column]; \n    if type(x) is str: print('Please specify data.'); return False;\n    #----------------------------------------------------------\n    if x is not None:\n        dtypes['x'] = dtype(x); names['x'] = x.name\n        if dtypes['x'] == 'object': x = keep_top(x, n = top)\n        elif dtypes['x'] == 'datetime': x = date_number(x)\n        if logx and dtype(x) != 'object': x = log(x)\n    if y is not None: \n        dtypes['y'] = dtype(y); names['y'] = y.name\n        if dtypes['y'] == 'object': y = keep_top(y, n = top)\n        elif dtypes['y'] == 'datetime': y = date_number(y)\n        if logy and dtype(y) != 'object': y = log(y)\n    if colour is not None:\n        dtypes['c'] = dtype(colour); names['c'] = colour.name\n        if dtypes['c'] == 'object': colour = keep_top(colour, n = top)\n        elif dtypes['c'] == 'datetime': colour = date_number(colour)\n        if logc and dtype(colour) != 'object': colour = log(colour)\n    if column is not None:\n        dtypes['col'] = dtype(column); names['col'] = column.name\n        if dtypes['col'] == 'object': column = keep_top(column, n = top)\n        elif dtypes['col'] == 'datetime': column = date_number(column)\n    #----------------------------------------------------------\n    df = hcat(x, y, colour, column)\n    if subset > len(df): subset = len(df)\n    df = sample(df, subset)\n    #----------------------------------------------------------\n    if column is not None:\n        if dtype(df[names['col']]) not in ('object', 'uint',' int') and nunique(df[names['col']]) > top: \n            if quantile: df[names['col']] = cut(df[names['col']], bins = bins, method = 'quantile')\n            else: df[names['col']] = cut(df[names['col']], bins = bins, method = 'range')\n    \n    try: df.sort_values(names['y'], inplace = True);\n    except: pass;\n    #----------------------------------------------------------\n    replace = {'boxplot':'box', 'barplot':'bar', 'tallyplot':'count', 'violinplot':'violin', \n               'lineplot': 'point', 'histogram':'lv'}\n    \n    if style == 'histogram' and y is None:\n        plot = sb.distplot(df[names['x']].loc[df[names['x']].notnull()], bins = bins)\n    elif style == 'lineplot' and y is None:\n        plot = plt.plot(df[names['x']]);\n        plt.show(); return;\n    elif style == 'barplot' and y is None:\n        plot = df.sort_values(names['x']).plot.bar();\n        plt.show(); return;\n    elif style in replace.keys():\n        if dtype(df[names['x']]) not in ('object', 'uint',' int') and nunique(df[names['x']]) > top: \n            if quantile: df[names['x']] = cut(df[names['x']], bins = bins, method = 'quantile')\n            else: df[names['x']] = cut(df[names['x']], bins = bins, method = 'range')\n        \n        if names['col'] is not None:\n            plot = sb.factorplot(x = names['x'], y = names['y'], hue = names['c'], data = df, kind = replace[style], col = names['col'],\n                             n_boot = 1, size = size, estimator = estimator, col_wrap = wrap)\n        else:\n            plot = sb.factorplot(x = names['x'], y = names['y'], hue = names['c'], data = df, kind = replace[style], col = names['col'],\n                             n_boot = 1, size = size, estimator = estimator)\n            \n        for ax in plot.axes.flatten(): \n            for tick in ax.get_xticklabels(): \n                tick.set(rotation=90)\n    \n    elif style == 'heatmap':\n        if dtype(df[names['x']]) != 'object'and nunique(df[names['x']]) > top:\n            if quantile: df[names['x']] = cut(df[names['x']], bins = bins, method = 'quantile')\n            else: df[names['x']] = cut(df[names['x']], bins = bins, method = 'range')\n                \n        if dtype(df[names['y']]) != 'object'and nunique(df[names['y']]) > top:\n            if quantile: df[names['y']] = cut(df[names['y']], bins = bins, method = 'quantile')\n            else: df[names['y']] = cut(df[names['y']], bins = bins, method = 'range')     \n\n        df = tabulate(df[names['x']], df[names['y']]).pivot(index = names['x'], columns = names['y'], values = 'count')\n        plot = sb.heatmap(df, cmap=\"YlGnBu\")\n\n        \n    elif dtype(df[names['x']]) == 'object' or dtype(df[names['y']]) == 'object':\n            print('{} can only take X = number and Y = number.'.format(style)); return False;\n        \n    elif style  in ('regplot', 'scatterplot'):\n        if column is None: col_wrap = None\n        else: col_wrap = wrap\n        if style == 'regplot': reg = True\n        else: reg = False\n        \n        plot = sb.lmplot(x = names['x'], y = names['y'], hue = names['c'], data = df, col = names['col'],\n                             n_boot = 2, size = size, ci = None, scatter_kws={\"s\": 50,'alpha':0.5},\n                        col_wrap = col_wrap, truncate = True, fit_reg = reg, order = power)\n        plot.set_xticklabels(rotation=90)\n        \n    elif style == 'bivariate':\n        plot = sb.jointplot(x = names['x'], y = names['y'], data = df, dropna = True, size = size, kind = 'reg',\n                           scatter_kws={\"s\": 50,'alpha':0.5}, space = 0)\n    plt.show()","5ff82a91":"head(data, 5)","a2f1ba4b":"tally(data['style'])","895d9401":"plot(data = data, x = 'stars', style = 'histogram')","213254f9":"plot(data = data, x = 'brand', y = 'stars', style = 'boxplot')","4b66f1dd":"tally(data['country'], percent = True, graph = True, top = 10)","fd0e7c0b":"query(data = data, column = 'brand', condition = '==\"Ve Wong\"')","744ed3f4":"sorts = sort(data['stars'], how = 'descending')  ### OR sort(data = data, by = 'stars', how = 'descending')\ntail(sorts, 12)","4a0cca4d":"stars = query(data, 'stars', '>3')\nresult = tabulate(stars['country'], stars['style'])\nplot(data = result, x = 'country', y = 'count', colour = 'style')","cd30152c":"<h2> Welcome to MARK5826 Assignment 1 SOLUTIONS!<\/h2>\n\n<h3> Thanks for taking this first time ever offered course! I hope you'll love it! <\/h3>\n\nLecturer In Charge: Junbum Kwon;Teaching Assistant: Daniel Han-Chen\n\n<h2>ASSIGNMENT 1<\/h2>\n\n**This assignment is worth 7 marks out of a total of 20 marks for assignments. (Ass2 is 13 marks).**\n\nThe stuff you learnt in week 1 and week 2 will be enough to complete Assignment 1 entirely.\n\nIt's due in 2 weeks time on Saturday week 4 10pm. We need you to submit a URL \/ LINK to your assignment notebook on Kaggle.\n\nIT MUST BE **PUBLIC**. Also, download the Kaggle file, and upload it to Moodle by 10pm Saturday week 4.\n\nHelper functions will be provided. It's a cut down version of the massive code we used in week 1 and onwards.\n\nJust a note. **Assignment 1 will be used as a scaling factor for the Group Assignment. The method for scaling will be discussed once the group assingmenThis is to counteract students who do not contribute in group assignments.**\n\n<h1> To run a cell press CTRL+ENTER <\/h1>\n\n <h1> DUE: Saturday Week 4 - 10pm via Moodle  | Worth 7 marks (so 7% of final course mark)<\/h1>\n\n\n<h2>PROCEDURE<\/h2>\n\n(You can click the links below)\n\n1.[The Data and Task Description and Aims](#Content)\n\n2.[Submission Procedure and Plagarism](#Submit)\n\n[<h1>3. HELPER CODE --> MUST RUN (CTRL + ENTER)<\/h1>](#Help)\n\n4.[Question 1 (0.5mark)](#Q1)\n\n5.[Question 2 (0.5mark)](#Q2)\n\n6.[Question 3 (1mark)](#Q3)\n\n7.[Question 4 (1mark)](#Q4)\n\n8.[Question 5 (1mark)](#Q5)\n\n9.[Question 6 (1mark)](#Q6)\n\n10.[Question 7 (2mark)](#Q7)\n\n11.[Submission](#Sub)\n","248a8c07":"<a id='Content'><\/a>\n<h1> 1. The Data and Task Description and Aims <\/h1>\n\nWe are using **Ramen (Instant Noodles) Reviews**.\n\nThis assignment has 7 Questions. Each is worth different marks each. You must get all 7 Questions correct to get full marks.\n\nReading in the data is already done for you, so no need to do that.\n\nYou can use **ANY CODE. BUT --> NO NEW PACKAGES can be used (only pandas, numpy, seaborn, matplotlib, os, re).** In other words --> **there should be no IMPORT statement in your code.**","66a35278":"<a id='Q4'><\/a>\n<h1> 7. Question 4 (1mark) <\/h1>\n<h2> TASK: Using TALLY on COUNTRY, find the PERCENT each country holds with a GRAPH set to TRUE. Show only the TOP 10 countries.<\/h2>","2bde3d1b":"<a id='Q1'><\/a>\n<h1> 4. Question 1 (0.5mark) <\/h1>\n\n<h1> You can access the RAMEN data using data. Below, I have shown the HEAD of the data (first 5 rows) <\/h1>","3d17ae69":"<h2> TASK: TALLY up how many of each STYLE is in the column STYLE. <h2>\n\nFor example, if [apple, apple, orange] --> apple = 2, orange = 1","7bc691ee":"<a id='Q2'><\/a>\n<h1> 5. Question 2 (0.5mark) <\/h1>\n\n<h2> TASK: Plot a histogram on stars. <h2>","817830e2":"<a id='Q5'><\/a>\n<h1> 8. Question 5 (1mark) <\/h1>\n<h2> TASK: QUERY only BRAND = Ve Wong<\/h2>","5eef9876":"<a id='Sub'><\/a>\n<h1> 11. SUBMISSION <\/h1>\n\nGood work on finishing the Assignment!\n\nBy **Saturday week 4 10pm, you must DOWNLOAD this notebook and submit to MOODLE. AND, you must post the link of this notebook as well.**\n\n**You need to make your notebook PUBLIC, or share it with DANIEL, JAMES or JUNBUM.**\n\n**IF you are confused, please email us or talk to us**","3ac3f597":"<a id='Q6'><\/a>\n<h1> 9. Question 6 (1mark) <\/h1>\n<h2> TASK: SORT STARS by DESCENDING order. Then, show the LAST 12 rows. <\/h2>","b167a329":"<a id='Q3'><\/a>\n<h1> 6. Question 3 (1mark) <\/h1>\n<h2> TASK: Plot a BOXPLOT where Xaxis = BRAND, Yaxis = STARS.<\/h2>","74640cf1":"<a id='Submit'><\/a>\n<h1> 2. Submission Procedure and Plagarism <\/h1>\n\n**Plagarism** or copying from other students or mates is not allowed.\n\nYou can ask your friends for advice, but they must not do your assignment. If you have questions, please talk to me.\n\nIn order to **counteract cheating**, I have made ALL datasets DIFFERENT fro each of you. The **ramen** data will NOT be the same, so outputs will look **different** from each other.","7a7f9fda":"<a id='Help'><\/a>\n<h1> 3. HELPER CODE --> MUST RUN (CTRL + ENTER) <\/h1>\n\n**The below code MUST be run for this notebook to work. Do not change below, as in marking, I'm going to use the original code I gave you.**\n\n<h1> After running the code below, SKIP to QUESTION 1: <\/h1>\n\n[<h1>Question 1 (0.5mark)<\/h1>](#Q1)","56099918":"<a id='Q7'><\/a>\n<h1> 10. Question 7 (2mark) <\/h1>\n<h1> TASK1: QUERY only STARS > 3. Data will be used for TASK2<\/h2>\n<h2> TASK2: Plot a LINEPLOT Xaxis = COUNTRY and Yaxis = count(STYLE) per COUNTRY and COLOUR = STYLE using TABULATE.<\/h2>","1f12a588":"####### END OF FILE"}}