{"cell_type":{"c3ce9b8a":"code","6c1c298b":"code","efe6bf36":"code","52a135b6":"code","5d3744a7":"code","140c8418":"code","b0f88499":"code","55067585":"code","8687960d":"code","171320bb":"code","65c24de3":"code","eb31e934":"code","0923204c":"code","99afdf6a":"code","bf2f5291":"code","b8996917":"code","d4914151":"code","0e1f6381":"code","96ebe539":"code","9e9e4b7b":"code","f5594b31":"code","34a4f8c0":"code","8f10daa9":"code","0387c358":"code","b7da4350":"code","b7b6e5d7":"code","3ea8a38f":"code","5ecbd595":"code","3086e371":"code","7b30b16c":"code","ad550752":"code","78ca4620":"code","84fd1e06":"code","f9b27907":"code","ae7f6ea7":"code","6eaa1fe5":"code","e7deda3a":"code","5174b0ac":"code","e92e8dee":"code","79d2c70e":"code","19b6ab39":"code","92790625":"code","f37a92ee":"code","fd018e13":"code","3c938924":"code","49000767":"code","be87404e":"code","60da4ab6":"code","c14de20f":"code","23981de7":"code","ec39013a":"code","d26c720e":"code","1b035ac9":"code","189bf01d":"code","c7a30215":"code","7e3b52b5":"code","0f776955":"code","f7929b98":"code","30537d46":"code","e8f718ef":"code","be00f5ce":"code","80d31f00":"code","f09d4f94":"code","50ed3c6c":"code","596de87a":"code","70e0e23d":"code","4016cb30":"code","bf08c713":"code","b109785f":"markdown","8d43a221":"markdown","58156f99":"markdown","cecc4042":"markdown","9bc9bf06":"markdown","e7f9b4ab":"markdown","65cc19bf":"markdown","7a88ac9a":"markdown","d1819549":"markdown","2657ed83":"markdown","3bc54504":"markdown","580605b6":"markdown","838dfba3":"markdown","4a16a2c4":"markdown","16c17369":"markdown","0d43ce2e":"markdown","6466dc80":"markdown","4e3ec495":"markdown","48e652dd":"markdown","fd3615b3":"markdown","91879abe":"markdown","eebc0687":"markdown","e892b252":"markdown","6ebc9254":"markdown","ae5028e0":"markdown"},"source":{"c3ce9b8a":"!pip install simple-colors","6c1c298b":"import pandas as pd\nimport numpy as np\nimport datatable as dt\nimport optuna\n\nfrom simple_colors import *\nfrom termcolor import colored\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import normaltest\nfrom scipy import stats\n\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\n# Supress Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","efe6bf36":"#Setting up options\n\npd.set_option(\"display.max_rows\", None)\npd.set_option(\"display.max_columns\", None)\npd.options.display.float_format = \"{:,.3f}\".format","52a135b6":"# Load the data\n\ntrain_eda = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\")\ntest_eda = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\")\n# sample_solution = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")\n\ntrain = \"..\/input\/tabular-playground-series-sep-2021\/train.csv\"\ntest = \"..\/input\/tabular-playground-series-sep-2021\/test.csv\"\nsample_solution = \"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\"","5d3744a7":"train = dt.fread(train).to_pandas()\ntest = dt.fread(test).to_pandas()","140c8418":"train.claim = train.claim.astype('int16')","b0f88499":"train_memory = train.memory_usage().sum() \/ 1024**2\nprint('Memory usage of original training set (in MB): {}'.format(train_memory))\n\ndef reduce_memory(df):\n    for col in df.columns:\n        if str(df[col].dtypes)[:5] == 'float':\n            low = df[col].min()\n            high = df[col].max()\n            if((low > np.finfo(np.float16).min) and (high < np.finfo(np.float16).max)):\n                df[col] = df[col].astype('float16')\n            elif((low > np.finfo(np.float32).min) and (high < np.finfo(np.float).max)):\n                df[col] = df[col].astype('float32')\n    return df\n\nreduce_memory(train)\ntrain_memory_reduced = train.memory_usage().sum() \/ 1024**2\nprint('Memory usage of reduced training set (in MB): {}'.format(train_memory_reduced))","55067585":"test_memory = test.memory_usage().sum() \/ 1024**2\nprint('Memory usage of original test set(in MB): {}'.format(test_memory))\n\nreduce_memory(test)\ntest_memory_reduced = test.memory_usage().sum() \/ 1024**2\nprint('Memory usage of reduced test set(in MB): {}'.format(test_memory_reduced))","8687960d":"train.replace([np.inf, -np.inf], np.nan, inplace=True)\ntest.replace([np.inf, -np.inf], np.nan, inplace=True)","171320bb":"def data_desc(df):\n    \n    \"\"\"\n    This function helps us with simple data analysis.\n    We may explore the common information about the dataset, missing values, features distribution and duplicated rows\n    \"\"\"\n    \n    # applying info() method\n    print('*******************')\n    print(cyan('General information of this dataset', 'bold'))\n    print('*******************\\n')\n    print(df.info())\n    \n    print('\\n*******************')\n    print(cyan('Number of rows and columns', 'bold'))\n    print('*******************\\n')\n    print(\"Number of rows:\", colored(df.shape[0], 'green', attrs=['bold']))\n    print(\"Number of columns:\", colored(df.shape[1], 'green', attrs=['bold']))\n    \n    # missing values\n    print('\\n*******************')\n    print(cyan('Missing value checking', 'bold'))\n    print('*******************\\n')\n    if df.isna().sum().sum() == 0:\n        print(colored('There are no missing values', 'green'))\n        print('*******************')\n    else:\n        print(colored('Missing value detected!', 'green', attrs=['bold']))\n        print(\"\\nTotal number of missing values:\", colored(sum(df.isna().sum()), 'green', attrs=['bold']))\n        \n        print('\\n*******************')\n        print(cyan('Missing values of features', 'bold'))\n        print('*******************\\n')\n        display(df.isna().sum().sort_values(ascending = False).to_frame().rename({0:'Counts'}, axis = 1).T.style.background_gradient('Purples', axis = None))\n        print('\\n*******************')\n        print(cyan('Percentage of missing values of features', 'bold'))\n        print('*******************\\n')\n        display(round((df.isnull().sum() \/ (len(df.index)) * 100) , 3).sort_values(ascending = False).to_frame().rename({0:'%'}, axis = 1).T.style.background_gradient('PuBuGn', axis = None))\n\n    # describe() for numerical features\n    cont_feats = [col for col in df.columns if df[col].dtype != object and col not in ('id', 'claim')]\n    print('\\n*******************')\n    print(cyan('Numerical columns', 'bold'))\n    print('*******************\\n')\n    print(\"Total numerical features:\", colored(len(cont_feats), 'green', attrs=['bold']))\n    df = df[df.columns.difference(['id', 'claim'], sort = False)]\n    display(df.describe())\n    \n    # Checking for duplicated rows -if any-\n    if df.duplicated().sum() == 0:\n        print('\\n*******************')\n        print(colored('There are no duplicates!', 'green', attrs=['bold']))\n        print('*******************')\n    else:\n        print('\\n*******************')\n        print(colored('Duplicates found!', 'green', attrs=['bold']))\n        print('*******************')\n        display(df[df.duplicated()])\n\n    print('\\n*******************')\n    print(cyan('Preview of the data - Top 10 rows', 'bold'))\n    print('*******************\\n')\n    display(df.head(10))\n    print('*******************\\n')\n    \n    print('\\n*******************')\n    print(cyan('End of the report', 'bold'))","65c24de3":"data_desc(train)","eb31e934":"data_desc(test)","0923204c":"numerical_features = [x for x in train.columns if x.startswith(\"f\")]","99afdf6a":"plt.figure(figsize=(10, 7))\nax = sns.countplot(y=train[\"claim\"], palette='muted', zorder=3, linewidth=5, orient='h', saturation=1, alpha=1)\nax.set_title('Distribution of Target', fontname = 'Times New Roman', fontsize = 30, color = '#8c49e7', x = 0.5, y = 1.05)\nbackground_color = \"#8c49e7\"\nsns.set_palette(['#ffd514']*120)\n\nfor a in ax.patches:\n    value = f'Amount and percentage of values: {a.get_width():,.0f} | {(a.get_width()\/train.shape[0]):,.3%}'\n    x = a.get_x() + a.get_width() \/ 2 - 220000\n    y = a.get_y() + a.get_height() \/ 2 \n    ax.text(x, y, value, ha='left', va='center', fontsize=18, \n            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round4', linewidth=0.7))\n\n\n# ax.margins(-0.12, -0.12)\nax.grid(axis=\"x\")\n\nsns.despine(right=True)\nsns.despine(offset=15, trim=True)","bf2f5291":"def missing_data_percentage_plot(data, titleText):\n    \n    data = data[data.columns.difference(['id', 'claim'], sort = False)]\n    \n    missing_data_percentage = (data.isnull().sum() \/ (len(data.index)) * 100).to_frame().reset_index().rename({0:'%'}, axis = 1)\n\n\n    v0 = sns.color_palette(palette = \"mako\").as_hex()[3]\n    fig = plt.figure(figsize=(40, 40))\n    ax = sns.barplot(missing_data_percentage['%'], missing_data_percentage['index'], color=v0, saturation=.75, zorder=3, linewidth=5, orient='h', alpha=1)\n    ax.set_ylabel(\"Numerical features\", fontsize=18, labelpad=15)\n    ax.set_xlabel(\"Percentage of missing values (%)\", fontsize=18, labelpad=15)\n    ax.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    ax.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    plt.xticks(fontsize = 10)\n    plt.yticks(fontsize = 10)\n    ax.tick_params(axis=\"x\", rotation=90, labelsize=12)\n#     ax.margins(-0.05, -0.02)\n    plt.title(titleText, fontsize=32, pad=15, fontweight = 'bold');\n    \n    for a in ax.patches:\n        value = \"%.3f%%\" % a.get_width()\n        x = a.get_width() + 0.005\n        y = a.get_y() + a.get_height() \/ 2\n        ax.text(x, y, value, ha='left', va='center', fontsize=10, \n                bbox=dict(facecolor='none', edgecolor='black', boxstyle='round4', linewidth=1.5))","b8996917":"missing_data_percentage_plot(train, 'Percetage of Missing Values - Train Dataset')","d4914151":"missing_data_percentage_plot(test, 'Percetage of Missing Values - Test Dataset')","0e1f6381":"def missing_data_number_plot(data, titleText):\n    \n    data = data[data.columns.difference(['id', 'claim'], sort = False)]\n    \n    missing_data_number = data.isnull().sum().to_frame().reset_index().rename({0:'%'}, axis = 1)\n\n    v0 = sns.color_palette(palette = \"mako\").as_hex()[3]\n    fig = plt.figure(figsize=(40, 40))\n    ax = sns.barplot(missing_data_number['%'], missing_data_number['index'], color=v0, saturation=.75, zorder=3, linewidth=5, orient='h', alpha=1)\n    ax.set_ylabel(\"Numerical features\", fontsize=18, labelpad=15)\n    ax.set_xlabel(\"Percentage of missing values (%)\", fontsize=18, labelpad=15)\n    ax.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    ax.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    plt.xticks(fontsize = 10)\n    plt.yticks(fontsize = 10)\n    ax.tick_params(axis=\"x\", rotation=90, labelsize=12)\n#     ax.margins(-0.05, -0.02)\n    plt.title(titleText, fontsize=32, pad=15, fontweight = 'bold');\n    \n    for a in ax.patches:\n        value = a.get_width()\n        x = a.get_width() + 40\n        y = a.get_y() + a.get_height() \/ 2\n        ax.text(x, y, value, ha='left', va='center', fontsize=10, \n                bbox=dict(facecolor='none', edgecolor='black', boxstyle='round4', linewidth=1.5))","96ebe539":"missing_data_number_plot(train, 'Number of Missing Value per Feature')","9e9e4b7b":"missing_data_number_plot(test, 'Number of Missing Value per Feature')","f5594b31":"def missing_value_distribution(data, titleText):\n    \n    data[\"number_of_null\"] = data.isnull().sum(axis=1)\n\n    counts = data.groupby(\"number_of_null\")[\"claim\"].count().to_dict()\n    null_data = {\"{} Null Value(s) Per Row\".format(k) : v for k, v in counts.items() if k < 8}\n    null_data[\"8 or More Null Values Per Row\"] = sum([v for k, v in enumerate(counts.values()) if k > 7])\n\n    pie, ax = plt.subplots(1, 1, figsize=[20, 12])\n    plt.title(titleText, fontsize=32, fontweight='bold')\n    pie.set_facecolor('#e4e4e4')\n    plt.pie(x=null_data.values(), autopct=\"%.3f%%\", explode=[0.05]*len(null_data.keys()), labels=null_data.keys(), pctdistance=0.8, shadow=True, labeldistance = 1.025, wedgeprops = {'linewidth': 1})\n    plt.tight_layout()\n    \n    #Percentage of Null Values Per Row (Train Data)","34a4f8c0":"missing_value_distribution(train, 'Percentage of Null Values Per Row (Train Data)')","8f10daa9":"missing_value_distribution(train, 'Percentage of Null Values Per Row (Test Data)')","0387c358":"def box_plot(data, features, titleText, hue=None):\n\n    L = len(features)\n    nrow = int(np.ceil(L\/4))\n    ncol = 7\n    remove_last = (nrow * ncol) - L\n\n    fig, axs = plt.subplots(nrow, ncol, figsize=(30, 100))\n    fig.tight_layout()\n    fig.set_facecolor('#e4e4e4')\n\n    while remove_last > 0:\n      axs.flat[-remove_last].set_visible(False)\n      remove_last -= 1\n\n    fig.subplots_adjust(top = 0.97)\n    plt.subplots_adjust(hspace = 0.5)\n    i = 1\n    for feature in features:\n        plt.subplot(nrow, ncol, i)\n        v0 = sns.color_palette(palette = \"crest\").as_hex()[3]\n        ax = sns.boxplot(data[feature], color=v0, saturation=.75)  \n        ax = ax.legend(loc = \"best\")    \n        plt.xlabel(feature, fontsize=18, fontweight = 'bold')\n        plt.ylabel('Values', fontsize=18, fontweight = 'bold')\n        i += 1\n\n    plt.suptitle(titleText, fontsize = 32, fontweight = 'bold', color = 'navy')\n    plt.show()","b7da4350":"box_plot(train, numerical_features, 'Box Plot of Numerical Columns of Train Dataset')","b7b6e5d7":"box_plot(test, numerical_features, 'Box Plot of Numerical Columns of Test Dataset')","3ea8a38f":"def kde_plot(data, features, titleText, hue=None):\n\n    L = len(features)\n    nrow = int(np.ceil(L\/4))\n    ncol = 7\n    remove_last = (nrow * ncol) - L\n\n    fig, axs = plt.subplots(nrow, ncol, figsize=(30, 100))\n    fig.tight_layout()\n    fig.set_facecolor('#e4e4e4')\n\n    while remove_last > 0:\n      axs.flat[-remove_last].set_visible(False)\n      remove_last -= 1\n\n    fig.subplots_adjust(top = 0.97)\n    plt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9,  \n                    wspace=0.4, \n                    hspace=0.4)\n    i = 1\n    for feature in features:\n        plt.subplot(nrow, ncol, i)\n        ax = sns.kdeplot(data[feature], color=\"m\", shade=True, label=\"%.3f\"%(data[feature].skew()))  \n        ax = ax.legend(loc = \"best\")    \n        plt.xlabel(feature, fontsize=18, fontweight = 'bold')\n        plt.ylabel('Density', fontsize=18, fontweight = 'bold')\n        i += 1\n\n    plt.suptitle(titleText, fontsize = 32, fontweight = 'bold', color = 'navy')\n    \n    plt.show()","5ecbd595":"train_frac = train_eda.sample(frac = 0.1).reset_index(drop = True)\n\nkde_plot(train_frac, numerical_features, titleText = 'KDE Plot of Numerical Features of Train Dataset', hue = None)","3086e371":"test_frac = test_eda.sample(frac = 0.1).reset_index(drop = True)\n\nkde_plot(test_frac, numerical_features, titleText = 'KDE Plot of Numerical Features of Test Dataset', hue = None)","7b30b16c":"def correlation_matrix(data):\n\n    fig, ax = plt.subplots(1, 1, figsize=(25, 10))\n    plt.title('Pearson Correlation Matrix', fontweight='bold', fontsize=25)\n    fig.set_facecolor('#d0d0d0') \n    corr = data.drop('id', axis=1).corr()\n\n    # Mask to hide upper-right part of plot as it is a duplicate\n    mask = np.triu(np.ones_like(corr, dtype = bool))\n    sns.heatmap(corr, annot = False, center = 0, cmap = 'jet', mask = mask, linewidths=.5)\n    ax.set_xticklabels(ax.get_xticklabels(), fontfamily='sans', rotation=90, fontsize=12)\n    ax.set_yticklabels(ax.get_yticklabels(), fontfamily='sans', rotation = 0, fontsize=12)\n    plt.tight_layout()\n    plt.show()","ad550752":"correlation_matrix(train)","78ca4620":"correlation_matrix(test)","84fd1e06":"def hist_plot(data, features, titleText, hue=None):\n\n    L = len(features)\n    nrow = int(np.ceil(L\/4))\n    ncol = 7\n    remove_last = (nrow * ncol) - L\n\n    fig, axs = plt.subplots(nrow, ncol, figsize=(30, 100))\n    fig.tight_layout()\n    fig.set_facecolor('#e4e4e4')\n\n    while remove_last > 0:\n      axs.flat[-remove_last].set_visible(False)\n      remove_last -= 1\n\n    fig.subplots_adjust(top = 0.97)\n    plt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9,  \n                    wspace=0.4, \n                    hspace=0.4)\n    i = 1\n    for feature in features:\n        plt.subplot(nrow, ncol, i)\n        ax = sns.histplot(data[feature], edgecolor=\"black\", color=\"darkseagreen\", alpha=0.7)  \n        ax = ax.legend(loc = \"best\")    \n        plt.xlabel(feature, fontsize=18, fontweight = 'bold')\n        plt.ylabel('Frequency', fontsize=18, fontweight = 'bold')\n        i += 1\n\n    plt.suptitle(titleText, fontsize = 32, fontweight = 'bold', color = 'navy')\n    plt.show()","f9b27907":"hist_plot(train_frac, numerical_features, titleText = 'Histogram of Numerical Features of Train Dataset', hue = None)","ae7f6ea7":"hist_plot(test_frac, numerical_features, titleText = 'Histogram of Numerical Features of Test Dataset', hue = None)","6eaa1fe5":"def qqplot(data, features, titleText, hue=None):\n\n    L = len(features)\n    nrow = int(np.ceil(L\/4))\n    ncol = 7\n    remove_last = (nrow * ncol) - L\n\n    fig, axs = plt.subplots(nrow, ncol, figsize=(30, 100))\n    fig.tight_layout()\n    fig.set_facecolor('#e4e4e4')\n\n    while remove_last > 0:\n      axs.flat[-remove_last].set_visible(False)\n      remove_last -= 1\n\n    fig.subplots_adjust(top = 0.97)\n    plt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9,  \n                    wspace=0.4, \n                    hspace=0.4)\n        \n    i = 1\n    for feature in features:\n        plt.subplot(nrow, ncol, i)   \n        stats.probplot(data[feature],plot=plt)\n        plt.title('\\nQ-Q Plot')\n        plt.xlabel(feature, fontsize=18, fontweight = 'bold')\n        plt.ylabel('Sample Quantile', fontsize=18, fontweight = 'bold')\n        i += 1\n\n    plt.suptitle(titleText, fontsize = 32, fontweight = 'bold', color = 'navy')\n    plt.show()","e7deda3a":"qqplot(train_frac, numerical_features, 'Q-Q Plot of Numerical Features of Train Dataset', hue=None)","5174b0ac":"qqplot(test_frac, numerical_features, 'Q-Q Plot of Numerical Features of Test Dataset', hue=None)","e92e8dee":"# D'Agostino and Pearson's Test\n\ndef normality_check(data):\n  for i in numerical_features:\n    # normality test\n    stat, p = normaltest(data[[i]])\n    print('Statistics=%.3f, p=%.3f' % (stat, p))\n    # interpret results\n    alpha = 1e-2\n    if p > alpha:\n        print(f'{i} looks Gaussian (fail to reject H0)\\n')\n    else:\n        print(f'{i} does not look Gaussian (reject H0)\\n')","79d2c70e":"normality_check(train)","19b6ab39":"normality_check(test)","92790625":"def detect_outliers(x, c = 1.5):\n    \"\"\"\n    Function to detect outliers.\n    \"\"\"\n    q1, q3 = np.percentile(x, [25,75])\n    iqr = (q3 - q1)\n    lob = q1 - (iqr * c)\n    uob = q3 + (iqr * c)\n\n    # Generate outliers\n\n    indicies = np.where((x > uob) | (x < lob))\n\n    return indicies","f37a92ee":"# Detect all Outliers \noutliers = detect_outliers(train['claim'])\nprint(\"Total Outliers count for claim : \", len(outliers[0]))\n\nprint(\"\\nShape before removing outliers : \",train.shape)\n\n# Remove outliers\n#train.drop(outliers[0],inplace=True, errors = 'ignore')\nprint(\"Shape after removing outliers : \",train.shape)","fd018e13":"train_iqr = pd.DataFrame()\ntrain_iqr.reindex(columns=[*train_iqr.columns.tolist(), \"-3 IQR\", \"-1.5 IQR\", \"1.5 IQR\", \"3 IQR\"], fill_value = 0)","3c938924":"from scipy.stats import iqr\n\ndata = []\n\nk = 0\ncolumns = [\"-3 IQR\", \"-1.5 IQR\", \"1.5 IQR\", \"3 IQR\"]\n\nfor i in numerical_features:\n\n    q1 = train[i].quantile(0.25)\n    q3 = train[i].quantile(0.75)\n    \n    iqr = (q3 - q1)\n    lob_1 = q1 - (iqr * 1.5)\n    uob_1 = q3 + (iqr * 1.5)\n    lob_3 = q1 - (iqr * 3)\n    uob_3 = q3 + (iqr * 3)\n    \n    number_uob_1 = f'{round(sum(train[numerical_features[k]] > uob_1) \/ len(train[numerical_features[k]]), 5):,.3%}'\n    number_lob_1 = f'{round(sum(train[numerical_features[k]] < lob_1) \/ len(train[numerical_features[k]]), 5):,.3%}'\n    number_uob_3 = f'{round(sum(train[numerical_features[k]] > uob_3) \/ len(train[numerical_features[k]]), 5):,.3%}'\n    number_lob_3 = f'{round(sum(train[numerical_features[k]] < lob_3) \/ len(train[numerical_features[k]]), 5):,.3%}'\n\n    values = [number_lob_3, number_lob_1, number_uob_1, number_uob_3]\n    zipped = zip(columns, values)\n    a_dictionary = dict(zipped)\n    print(a_dictionary)\n    data.append(a_dictionary)\n    \n    k = k + 1","49000767":"train_iqr = train_iqr.append(data, True)\ntrain_iqr.set_axis([numerical_features], axis='index')","be87404e":"def colour(value):\n\n    if float(value.strip('%')) > 10:\n      color = 'red'\n    elif float(value.strip('%')) > 5:\n        color = 'darkorange'   \n    else:\n      color = 'green'\n\n    return 'color: %s' % color\n\n# train_iqr = train_iqr.set_axis([numerical_features], axis='index')\ntrain_iqr = train_iqr.style.applymap(colour)","60da4ab6":"train_iqr","c14de20f":"def create_stratified_folds_for_classification(df, n_splits = 10):\n\n    \"\"\"\n    @param data_df: training data to split in Stratified K Folds for a continous target value\n    @param n_splits: number of splits\n    @return: the training data with a column with kfold id\n    \"\"\"\n\n    df['StratifiedKFold'] = -1\n\n    # randomize the data\n    df = df.sample(frac=1).reset_index(drop=True)\n\n    # calculate the optimal number of bins based on log2(df.shape[0])\n    df_test = []\n    k = 0\n    df_ = df.select_dtypes(include='number')\n    df_ = df_.drop(['id', 'claim', 'StratifiedKFold'], axis=1)\n\n    while k <= len(df_.columns)-1:\n      q1 = df_.iloc[:,k].quantile(0.25)\n      q3 = df_.iloc[:,k].quantile(0.75)\n      iqr = q3 - q1\n      bin_width = (2 * iqr) \/ (len(df_) ** (1 \/ 3))\n      bin_count = int(np.ceil((df_.iloc[:,k].max() - df_.iloc[:,k].min()) \/ bin_width))\n      df_test.append(bin_count)\n      mean_bin = np.ceil(sum(df_test) \/ len(df_test))\n      k = k + 1\n    print(f\"Num bins: {mean_bin}\")\n\n    # bins value will be the equivalent of class value of target feature used by StratifiedKFold to distribute evenly the classed over each fold\n    df.loc[:, \"bins\"] = pd.cut(pd.to_numeric(df['claim'], downcast=\"signed\"), bins=int(mean_bin), labels=False)\n    kf = model_selection.StratifiedKFold(n_splits=n_splits, shuffle = True, random_state = 606)\n    \n    # set the fold id as a new column in the df data\n    for fold, (df_indicies, valid_indicies) in enumerate(kf.split(X=df, y=df.bins.values)):\n        df.loc[valid_indicies, \"StratifiedKFold\"] = fold\n    \n    # drop the bins column (no longer needed)\n    df = df.drop(\"bins\", axis=1)\n    \n    return df","23981de7":"train = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\")\nn_splits = 10\ntrain = create_stratified_folds_for_classification(train, n_splits)","ec39013a":"train.to_csv(\"train_folds(10).csv\", index=False)","d26c720e":"train.StratifiedKFold.value_counts()","1b035ac9":"plt.figure(figsize=(25,12))\nplt.title(\"Distribution of claim values (StratifiedKFolds with bins)\")\nfor k in range(0,n_splits):\n    df = train.loc[train.StratifiedKFold==k]\n    sns.distplot(df['claim'], kde=True, hist=False, bins=12, label=k)\nplt.legend(); plt.show()","189bf01d":"## LGBM parameter tuning\n\ntrain = pd.read_csv(\"train_folds(10).csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\")\nsample_solution = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")\n\n##In line with the information obtained from the train_iqr data set, it was observed that some features contain significant outlier values. To manage this, Q1 and Q3 quantiles were adapted while scaling some features. \n\nnumerical_features = ['f1','f4','f5','f6','f7','f8','f9','f11','f12','f14','f15','f17','f18','f19','f21','f22','f24','f25','f27','f28','f29','f30','f31','f34','f35','f37','f39','f40','f42','f43','f44','f45','f47','f48','f49','f50','f52','f54','f56','f57','f59','f60','f61','f63','f64','f65','f67','f68','f70','f71','f72','f75','f76','f79','f80','f81','f82','f84','f85','f86','f87','f88','f89','f90','f93','f95','f97','f98','f100','f101','f102','f104','f105','f106','f107','f108','f109','f110','f111','f113','f117','f118']\nnumerical_features_1 = [\"f2\", \"f13\", \"f23\", \"f58\", \"f66\", \"f91\"]\nnumerical_features_2 = [\"f55\", \"f94\"]\nnumerical_features_3 = [\"f36\"]\nnumerical_features_4 = [\"f46\"]\nnumerical_features_5 = [\"f33\", \"f62\", \"f78\"]\nnumerical_features_6 = [\"f3\", \"f10\", \"f16\", \"f20\", \"f32\", \"f38\", \"f41\", \"f51\", \"f53\", \"f69\", \"f73\", \"f77\", \"f83\", \"f92\", \"f96\", \"f103\", \"f114\", \"f115\", \"f116\"]\nnumerical_features_7 = [\"f26\", \"f99\"]\nnumerical_features_8 = [\"f74\", \"f112\"]\n\ntrain[numerical_features] = RobustScaler(quantile_range=(25, 75)).fit_transform(train[numerical_features])\ntest[numerical_features] = RobustScaler(quantile_range=(25, 75)).fit_transform(test[numerical_features])\n\ntrain[numerical_features_1] = RobustScaler(quantile_range=(15, 75)).fit_transform(train[numerical_features_1])\ntest[numerical_features_1] = RobustScaler(quantile_range=(15, 75)).fit_transform(test[numerical_features_1])\n\ntrain[numerical_features_2] = RobustScaler(quantile_range=(20, 75)).fit_transform(train[numerical_features_2])\ntest[numerical_features_2] = RobustScaler(quantile_range=(20, 75)).fit_transform(test[numerical_features_2])\n\ntrain[numerical_features_3] = RobustScaler(quantile_range=(20, 90)).fit_transform(train[numerical_features_3])\ntest[numerical_features_3] = RobustScaler(quantile_range=(20, 90)).fit_transform(test[numerical_features_3])\n\ntrain[numerical_features_4] = RobustScaler(quantile_range=(20, 80)).fit_transform(train[numerical_features_4])\ntest[numerical_features_4] = RobustScaler(quantile_range=(20, 80)).fit_transform(test[numerical_features_4])\n\ntrain[numerical_features_5] = RobustScaler(quantile_range=(25, 80)).fit_transform(train[numerical_features_5])\ntest[numerical_features_5] = RobustScaler(quantile_range=(25, 80)).fit_transform(test[numerical_features_5])\n\ntrain[numerical_features_6] = RobustScaler(quantile_range=(25, 85)).fit_transform(train[numerical_features_6])\ntest[numerical_features_6] = RobustScaler(quantile_range=(25, 85)).fit_transform(test[numerical_features_6])\n\ntrain[numerical_features_7] = RobustScaler(quantile_range=(25, 90)).fit_transform(train[numerical_features_7])\ntest[numerical_features_7] = RobustScaler(quantile_range=(25, 90)).fit_transform(test[numerical_features_7])\n\ntrain[numerical_features_8] = RobustScaler(quantile_range=(25, 100)).fit_transform(train[numerical_features_8])\ntest[numerical_features_8] = RobustScaler(quantile_range=(25, 100)).fit_transform(test[numerical_features_8])\n\nnumerical_features = [c for c in train.columns if c.startswith(\"f\")]\ntest = test[numerical_features]","c7a30215":"def opt_lgbm(trial):\n    fold = 6\n\n    params = {\n        'objective': 'binary',\n        'metric': 'auc',\n        'verbosity': 0,\n        'boosting_type': 'gbdt',\n        'device' : 'gpu',\n        'gpu_platform_id': 0,\n        'gpu_device_id': 0,\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n        'learning_rate' : trial.suggest_float('learning_rate', 1e-2, 0.30, log=True),\n        'reg_lambda' : trial.suggest_loguniform('reg_lambda', 1e-8, 100.0),\n        'num_leaves' : trial.suggest_int('num_leaves', 25, 250),\n        'reg_alpha' : trial.suggest_loguniform('reg_alpha', 1e-8, 100.0),\n        'subsample' : trial.suggest_float('subsample', 0.1, 1.0),\n        'colsample_bytree' : trial.suggest_float('colsample_bytree', 0.05, 1.0),\n        'min_child_samples' : trial.suggest_int('min_child_samples', 10, 250),\n        'cat_smooth' : trial.suggest_float('cat_smooth', 10, 150),\n        'min_data_per_group' : trial.suggest_int('min_data_per_group', 10, 250),\n        'cat_l2' : trial.suggest_float('cat_l2', 1e-2, 10),\n        'bagging_freq' : trial.suggest_int('bagging_freq', 1, 10),\n        'bagging_fraction' : trial.suggest_float('bagging_fraction', 1e-2, 1),\n        'max_depth' : trial.suggest_int('max_depth', 1, 100)\n    }\n\n    xtrain = train[train.StratifiedKFold != fold].reset_index(drop=True)\n    xvalid = train[train.StratifiedKFold == fold].reset_index(drop=True)\n\n    ytrain = xtrain.claim\n    yvalid = xvalid.claim\n\n    xtrain = xtrain[numerical_features]\n    xvalid = xvalid[numerical_features]\n\n    model = LGBMClassifier(**params)\n\n    model.fit(xtrain, ytrain, early_stopping_rounds=200, eval_set=[(xvalid, yvalid)], verbose = False)\n    preds_valid = model.predict_proba(xvalid)[: , 1]\n    score = roc_auc_score(yvalid, preds_valid)\n    return score","7e3b52b5":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(opt_lgbm, n_trials=50)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial of LGBM:', study.best_trial.params)","0f776955":"study.trials_dataframe()","f7929b98":"lgbm_params = study.best_params\nlgbm_params['num_iteration'] = 10000\nlgbm_params['n_jobs'] = -1\nlgbm_params['early_stopping_round'] = 200","30537d46":"## LightGBM Classifier Model\n\ntrain = pd.read_csv(\"train_folds(10).csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\")\nsample_solution = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")\n\nnumerical_features = [c for c in train.columns if c.startswith(\"f\")]\ntest = test[numerical_features]\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\n\nfor fold in range(len(train['StratifiedKFold'].unique().tolist())):\n    xtrain =  train[train.StratifiedKFold != fold].reset_index(drop=True)\n    xvalid = train[train.StratifiedKFold == fold].reset_index(drop=True)\n    xtest = test.copy()\n\n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.claim\n    yvalid = xvalid.claim\n    \n    xtrain = xtrain[numerical_features]\n    xvalid = xvalid[numerical_features]\n\n    model = LGBMClassifier(**lgbm_params)\n    model.fit(xtrain, ytrain, early_stopping_rounds = 200, eval_set=[(xvalid, yvalid)], eval_metric = 'auc', verbose = False)\n    preds_valid = model.predict_proba(xvalid)[: , 1]\n    test_preds = model.predict_proba(xtest)[: , 1]\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    score = roc_auc_score(yvalid, preds_valid)\n    print(fold, score)\n    scores.append(score)\n\nprint(np.mean(scores), np.std(scores))\n\n\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_1\"]\nfinal_valid_predictions.to_csv(\"train_pred_1.csv\", index=False)\n\nsample_solution.claim = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_solution.columns = [\"id\", \"pred_1\"]\nsample_solution.to_csv(\"test_pred_1.csv\", index=False)","e8f718ef":"## XGB Classifier parameter tuning\n\ntrain = pd.read_csv(\"train_folds(10).csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\")\nsample_solution = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")\n\n##In line with the information obtained from the train_iqr data set, it was observed that some features contain significant outlier values. To manage this, Q1 and Q3 quantiles were adapted while scaling some features. \n\nnumerical_features = ['f1', 'f4','f5','f6','f7','f8','f9','f11','f12','f14','f15','f17','f18','f19','f21','f22','f24','f25','f27','f28','f29','f30','f31','f34','f35','f37','f39','f40','f42','f43','f44','f45','f47','f48','f49','f50','f52','f54','f56','f57','f59','f60','f61','f63','f64','f65','f67','f68','f70','f71','f72','f75','f76','f79','f80','f81','f82','f84','f85','f86','f87','f88','f89','f90','f93','f95','f97','f98','f100','f101','f102','f104','f105','f106','f107','f108','f109','f110','f111','f113','f117','f118']\nnumerical_features_1 = [\"f2\", \"f13\", \"f23\", \"f58\", \"f66\", \"f91\"]\nnumerical_features_2 = [\"f55\", \"f94\"]\nnumerical_features_3 = [\"f36\"]\nnumerical_features_4 = [\"f46\"]\nnumerical_features_5 = [\"f33\", \"f62\", \"f78\"]\nnumerical_features_6 = [\"f3\", \"f10\", \"f16\", \"f20\", \"f32\", \"f38\", \"f41\", \"f51\", \"f53\", \"f69\", \"f73\", \"f77\", \"f83\", \"f92\", \"f96\", \"f103\", \"f114\", \"f115\", \"f116\"]\nnumerical_features_7 = [\"f26\", \"f99\"]\nnumerical_features_8 = [\"f74\", \"f112\"]\n\ntrain[numerical_features] = RobustScaler(quantile_range=(25, 75)).fit_transform(train[numerical_features])\ntest[numerical_features] = RobustScaler(quantile_range=(25, 75)).fit_transform(test[numerical_features])\n\ntrain[numerical_features_1] = RobustScaler(quantile_range=(15, 75)).fit_transform(train[numerical_features_1])\ntest[numerical_features_1] = RobustScaler(quantile_range=(15, 75)).fit_transform(test[numerical_features_1])\n\ntrain[numerical_features_2] = RobustScaler(quantile_range=(20, 75)).fit_transform(train[numerical_features_2])\ntest[numerical_features_2] = RobustScaler(quantile_range=(20, 75)).fit_transform(test[numerical_features_2])\n\ntrain[numerical_features_3] = RobustScaler(quantile_range=(20, 90)).fit_transform(train[numerical_features_3])\ntest[numerical_features_3] = RobustScaler(quantile_range=(20, 90)).fit_transform(test[numerical_features_3])\n\ntrain[numerical_features_4] = RobustScaler(quantile_range=(20, 80)).fit_transform(train[numerical_features_4])\ntest[numerical_features_4] = RobustScaler(quantile_range=(20, 80)).fit_transform(test[numerical_features_4])\n\ntrain[numerical_features_5] = RobustScaler(quantile_range=(25, 80)).fit_transform(train[numerical_features_5])\ntest[numerical_features_5] = RobustScaler(quantile_range=(25, 80)).fit_transform(test[numerical_features_5])\n\ntrain[numerical_features_6] = RobustScaler(quantile_range=(25, 85)).fit_transform(train[numerical_features_6])\ntest[numerical_features_6] = RobustScaler(quantile_range=(25, 85)).fit_transform(test[numerical_features_6])\n\ntrain[numerical_features_7] = RobustScaler(quantile_range=(25, 90)).fit_transform(train[numerical_features_7])\ntest[numerical_features_7] = RobustScaler(quantile_range=(25, 90)).fit_transform(test[numerical_features_7])\n\ntrain[numerical_features_8] = RobustScaler(quantile_range=(25, 100)).fit_transform(train[numerical_features_8])\ntest[numerical_features_8] = RobustScaler(quantile_range=(25, 100)).fit_transform(test[numerical_features_8])\n\nnumerical_features = [c for c in train.columns if c.startswith(\"f\")]\ntest = test[numerical_features]","be00f5ce":"def opt_xgb(trial):\n    fold = 1\n\n    params = {\n        'objective': 'binary:logistic',\n        'eval_metric' : 'auc',\n        'random_state': 606,\n        'tree_method' : 'gpu_hist',\n        'booster' : 'gbtree',\n        'gpu_id' : 1,\n        'predictor' : 'gpu_predictor',\n        'lambda': trial.suggest_loguniform('lambda', 1e-4, 10),\n        'alpha': trial.suggest_loguniform('alpha', 1e-4, 10),\n        'max_depth': trial.suggest_int('max_depth', 1, 15),\n        'learning_rate': trial.suggest_float('learning_rate', 1e-2, 0.30, log=True),\n        #'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 100.0),\n        #'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 100.0),\n        'gamma': trial.suggest_float('gamma', 0, 1),\n        'subsample': trial.suggest_float(\"subsample\", 1e-1, 1.0),\n        'min_child_weight': trial.suggest_int('min_child_weight', 50, 400),\n        'colsample_bytree' : trial.suggest_float(\"colsample_bytree\", 1e-1, 1.0),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1000)\n    }\n\n    xtrain = train[train.StratifiedKFold != fold].reset_index(drop=True)\n    xvalid = train[train.StratifiedKFold == fold].reset_index(drop=True)\n\n    ytrain = xtrain.claim\n    yvalid = xvalid.claim\n\n    xtrain = xtrain[numerical_features]\n    xvalid = xvalid[numerical_features]\n\n    model = XGBClassifier(**params)\n    model.fit(xtrain, ytrain, early_stopping_rounds = 200, eval_set=[(xvalid, yvalid)], eval_metric = 'auc', verbose = False)\n    preds_valid = model.predict_proba(xvalid)[: , 1]\n    score = roc_auc_score(yvalid, preds_valid)\n    return score","80d31f00":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(opt_xgb, n_trials=50)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial of XGB Classifier:', study.best_trial.params)","f09d4f94":"study.trials_dataframe()","50ed3c6c":"xgb_params = study.best_params\nxgb_params['tree_method'] = \"gpu_hist\"\nxgb_params['gpu_id'] = 0\nxgb_params['use_label_encoder'] = False\nxgb_params['predictor'] = \"gpu_predictor\"\nxgb_params['n_estimators'] = 10000","596de87a":"## XGB Classifier Model\n\ntrain = pd.read_csv(\"train_folds(10).csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\")\nsample_solution = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")\n\nnumerical_features = [c for c in train.columns if c.startswith(\"f\")]\ntest = test[numerical_features]\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\n\nfor fold in range(len(train['StratifiedKFold'].unique().tolist())):\n    xtrain =  train[train.StratifiedKFold != fold].reset_index(drop=True)\n    xvalid = train[train.StratifiedKFold == fold].reset_index(drop=True)\n    xtest = test.copy()\n\n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.claim\n    yvalid = xvalid.claim\n    \n    xtrain = xtrain[numerical_features]\n    xvalid = xvalid[numerical_features]\n\n    model = XGBClassifier(**xgb_params)\n    model.fit(xtrain, ytrain, early_stopping_rounds = 200, eval_set=[(xvalid, yvalid)], eval_metric = 'auc', verbose = False)\n    preds_valid = model.predict_proba(xvalid)[: , 1]\n    test_preds = model.predict_proba(xtest)[: , 1]\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    score = roc_auc_score(yvalid, preds_valid)\n    print(fold, score)\n    scores.append(score)\n\nprint(np.mean(scores), np.std(scores))\n\n\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_2\"]\nfinal_valid_predictions.to_csv(\"train_pred_2.csv\", index=False)\n\nsample_solution.claim = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_solution.columns = [\"id\", \"pred_2\"]\nsample_solution.to_csv(\"test_pred_2.csv\", index=False)","70e0e23d":"train = pd.read_csv(\"train_folds(10).csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\")\nsample_solution = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")\n\ntrain1 = pd.read_csv(\"train_pred_1.csv\")\ntrain2 = pd.read_csv(\"train_pred_2.csv\")\n\ntest1 = pd.read_csv(\"test_pred_1.csv\")\ntest2 = pd.read_csv(\"test_pred_2.csv\")\n\ntrain = train.merge(train1, on=\"id\", how=\"left\")\ntrain = train.merge(train2, on=\"id\", how=\"left\")\n\ntest = test.merge(test1, on=\"id\", how=\"left\")\ntest = test.merge(test2, on=\"id\", how=\"left\")","4016cb30":"useful_features = [\"pred_1\", \"pred_2\"]\ntest = test[useful_features]\n\npoly = preprocessing.PolynomialFeatures(degree = 2, interaction_only = False, include_bias = False)\ntrain_poly = poly.fit_transform(train[useful_features])\ntest_poly = poly.fit_transform(test[useful_features])\n\ndf_train_poly = pd.DataFrame(train_poly, columns=[f\"POLY_{i}\" for i in range(train_poly.shape[1])])\ndf_test_poly = pd.DataFrame(test_poly, columns=[f\"POLY_{i}\" for i in range(test_poly.shape[1])])\n\ntrain = pd.concat([train, df_train_poly], axis=1)\ntest = pd.concat([test, df_test_poly], axis=1)\n\nuseful_features = [col for col in test.columns]\n\nfinal_predictions = []\nscores = []\n\nfor fold in range(10):\n    xtrain =  train[train.StratifiedKFold != fold].reset_index(drop=True)\n    xvalid = train[train.StratifiedKFold == fold].reset_index(drop=True)\n    xtest = test.copy()\n\n    ytrain = xtrain.claim\n    yvalid = xvalid.claim\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    model = LGBMClassifier(**lgbm_params)\n    model.fit(xtrain, ytrain, early_stopping_rounds = 200, eval_set=[(xvalid, yvalid)], eval_metric = 'auc')\n    \n    preds_valid = model.predict_proba(xvalid)[: , 1]\n    test_preds = model.predict_proba(xtest)[: , 1]\n    final_predictions.append(test_preds)\n    score = roc_auc_score(yvalid, preds_valid)\n    print(fold, score)\n    scores.append(score)\n\nprint(np.mean(scores), np.std(scores))","bf08c713":"sample_solution.claim = np.mean(np.column_stack(final_predictions), axis=1)\nsample_solution.to_csv(\"submission.csv\", index=False)","b109785f":"However, considering the large number of outliers and also the possibility of losing rows from the train data set tremendously negatively impacting the outcome of the model, no drops were made. ","8d43a221":"There is not a single feature that fits the normal distribution in either data set. ","58156f99":"Unlike many notebooks, the percentage of mild and extreme outputs relative to the total number of lines is taken into account for each feature. The reason for this is that when using the scalar (StandardScaler, MinMaxScaler, RobustScaler etc.), the Q1 and Q3 values can be given manually in order to better manage the outliers. ","cecc4042":"[back to top](#table-of-contents)\n<a id=\"model_blending\"><\/a>\n# <p style=\"background-color:BlueViolet; font-family:newtimeroman; font-size:150%; text-align:center\">4. Model Blending<\/p>","9bc9bf06":"<a id=\"numerical_variables\"><\/a>\n## <p style=\"background-color:MediumPurple; font-family:newtimeroman; font-size:120%; text-align:center\">2.1. Numerical Variables<\/p>","e7f9b4ab":"## <p style=\"background-color:BlueViolet; font-family:newtimeroman; margin-bottom:2px; font-size:32px; color: white; text-align:center\">Table of Content<\/p>  \n\n<a id=\"table-of-contents\"><\/a>\n1. [Preperation](#preperation)\n    * 1.1. [Loading Packages and Importing Libraries](#load_packages_import_libraries)\n    * 1.2. [Data Description](#data_description)\n2. [Exploratory Data Analysis (EDA)](#eda)\n    * 2.1. [Numerical Variables](#numerical_variables)\n    * 2.2. [Normality Check and Outlier Detection](#norm_check_outlier_detect)\n3. [Feature Engineering and Modeling](#feat_eng_model)\n    * 3.1. [LightGBM Model](#lgbm_model)\n    * 3.2. [XGB Model](#xgb_model)\n4. [Model Blending](#model_blending)","65cc19bf":"After some preprocessing, we can tune XGB parameters and train model.","7a88ac9a":"[back to top](#table-of-contents)\n<a id=\"eda\"><\/a>\n# <p style=\"background-color:BlueViolet; font-family:newtimeroman; font-size:150%; text-align:center\">2. Exploratory Data Analysis (EDA)<\/p>\n\nAll numerical variables will be explored in this section.","d1819549":"The percentages of missing data in both train and test data sets are very similar. ","2657ed83":"Since KDE plots are processed in a long time, plots were created on 10% of the data sets. Supporting the box chart, it can be seen from this chart that there are various outliers. ","3bc54504":"There is no significant correlation between variables in train dataset.","580605b6":"After some preprocessing, we can tune LGBM parameters and train model.","838dfba3":"Such a cell was added to avoid error while generating the histogram and kde plots. Otherwise, kde plot of some features are not created and the histogram plot gives an error for some reason. ","4a16a2c4":"The Q-Q plot with clues to the normal distribution also shows tremendously that the data is not normally distributed. ","16c17369":"We will begin by using two trained model to blend predictions, which we will save to a CSV file.","0d43ce2e":"There is also no significant correlation between variables in test dataset.","6466dc80":"[back to top](#table-of-contents)\n<a id=\"preperation\"><\/a>\n# <p style=\"background-color:BlueViolet; font-family:newtimeroman; font-size:150%; text-align:center\">1. Preperation<\/p>\n\n\n<a id=\"load_packages_import_libraries\"><\/a>\n## <p style=\"background-color:MediumPurple; font-family:newtimeroman; font-size:120%; text-align:center\">1.1. Loading Packages and Importing Libraries<\/p>\n\nLoading packages and importing some helpful libraries.","4e3ec495":"<a id=\"lgbm_model\"><\/a>\n## <p style=\"background-color:MediumPurple; font-family:newtimeroman; font-size:120%; text-align:center\">3.1. LGBM Model<\/p>","48e652dd":"It is very obvious that some features contain significant amount of outlier value. This situation must be handled. ","fd3615b3":"<a id=\"norm_check_outlier_detect\"><\/a>\n## <p style=\"background-color:MediumPurple; font-family:newtimeroman; font-size:120%; text-align:center\">2.2. Normality Check and Outlier Detection<\/p>","91879abe":"<a id=\"xgb_model\"><\/a>\n## <p style=\"background-color:MediumPurple; font-family:newtimeroman; font-size:120%; text-align:center\">3.2. XGB Model<\/p>","eebc0687":"Creating 10-fold using Stratified KFold method.","e892b252":"<a id=\"data_description\"><\/a>\n## <p style=\"background-color:MediumPurple; font-family:newtimeroman; font-size:120%; text-align:center\">1.2. Data Description<\/p>\n\nFirst of all, some setting up options were made. It is aimed to show all rows and columns in order to improve the view, especially while giving the definitions of the data sets. Next, I will load the train, test and sample_solution data sets and display train and test data sets.","6ebc9254":"[back to top](#table-of-contents)\n<a id=\"feat_eng_model\"><\/a>\n# <p style=\"background-color:BlueViolet; font-family:newtimeroman; font-size:150%; text-align:center\">3. Feature Engineering and Modeling<\/p>","ae5028e0":"The logic in the KDE plots is also executed in the histogram plots."}}