{"cell_type":{"b92f731c":"code","0663b6a6":"code","bfdd555f":"code","bb735cd9":"code","3c4b0477":"code","0b7556c1":"code","39ea168c":"code","c1cd386f":"code","1fb3cc7d":"code","2a37d69b":"code","fe2ad49a":"code","1007fac8":"code","443c8827":"code","82b1c239":"code","252ca12e":"code","92ffde74":"code","b89b2fe4":"code","d7d8fbac":"code","3e42253e":"code","d3cd3268":"code","f111f0ee":"code","40784d39":"code","1b4010f7":"code","faa64d80":"code","6c181d8e":"code","d93f1273":"code","0dc932bc":"code","0c0549b1":"code","14be0382":"code","0a848909":"code","be8a5f80":"code","ecd9fec6":"code","29b8a4ca":"code","fc912bac":"code","98783b21":"code","4b79d627":"code","320db40f":"code","68c447bc":"code","6a971bf8":"code","70ca24fd":"code","d00b4700":"code","c1cd6192":"code","fd5ac17e":"code","a08d2884":"code","8b129b32":"code","cf7974b6":"code","c3cbb6a1":"code","2afdd015":"code","9758969c":"code","9cb224fb":"code","ebc758c6":"code","1f141aad":"code","9a2bb171":"code","892d2e9b":"code","e44d3354":"code","83825e74":"code","ce53dcdf":"code","73f6b70e":"code","2923dcd4":"code","f3a68b4b":"code","d0d50b45":"code","a5a7d054":"code","5c4ecb58":"code","2ec83fbc":"code","39738dd4":"code","f9590570":"code","583b33b7":"code","6e216f43":"code","a8cb8107":"code","f65738db":"code","25026bda":"code","49268fbd":"code","ba3dfc79":"code","5f975860":"code","9f597fcf":"markdown","68763578":"markdown","f70ad3ec":"markdown","91ae153a":"markdown","840978f7":"markdown","5b42bb59":"markdown","02d177c8":"markdown","71492a4f":"markdown","d9bb1c67":"markdown","968dc41a":"markdown","b03feb32":"markdown","3bee9086":"markdown","6f6aeb89":"markdown","ac99ba00":"markdown","f155796a":"markdown","a67e00c8":"markdown","30ad9c29":"markdown","39a937b9":"markdown","648b3ee3":"markdown","2945c46d":"markdown","7c279aa1":"markdown","aa72a419":"markdown","ace8a56f":"markdown","2813867e":"markdown","5a89c2ec":"markdown","5cfec5f5":"markdown","caecd56f":"markdown","215d79f4":"markdown","75cec1b6":"markdown","325f6407":"markdown","1a5271e1":"markdown","05ef573c":"markdown","1d02757d":"markdown","49299aed":"markdown","19ee84f9":"markdown","9031ea2d":"markdown","e2d76411":"markdown","81d102ae":"markdown","524cf89a":"markdown","405249f9":"markdown","c1b570a2":"markdown","2d4b20d1":"markdown","a5e9775c":"markdown","3b08a7ab":"markdown","1a8b875e":"markdown","f4470d51":"markdown","c509f5aa":"markdown","fabd657a":"markdown","d7d1fbab":"markdown","cf9e30ab":"markdown","f6824fed":"markdown","df882189":"markdown"},"source":{"b92f731c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly.offline as py\npy.init_notebook_mode(connected = True)\nimport plotly.graph_objs as go\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nimport plotly.figure_factory as ff\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score,recall_score\nimport warnings\nwarnings.filterwarnings(\"ignore\")","0663b6a6":"carprice = pd.read_csv('..\/input\/car-price-prediction\/CarPrice_Assignment.csv')","bfdd555f":"carprice.head()","bb735cd9":"print('\\033[1mRows :\\033[0m' , carprice.shape[0])\nprint('\\033[1m\\nColumns :\\033[0m', carprice.shape[1])\nprint('\\033[1m\\nFeatures :\\033[0m', carprice.columns.tolist())\nprint('\\033[1m\\nNull Values :\\033[0m',carprice.isnull().sum().values.sum())","3c4b0477":"carprice.info()","0b7556c1":"carprice.describe()","39ea168c":"import scipy.stats as stats","c1cd386f":"plt.figure(figsize=(8,8))\nfor i in carprice.columns:\n    \n    if carprice[i].dtype=='O':\n            sns.boxplot(x=carprice[i], y=carprice['price'],data=carprice)\n            plt.show()\n            ","1fb3cc7d":"carprice.columns","2a37d69b":"carprice['CarName']","fe2ad49a":"x = carprice['CarName'].str.split(\" \", expand=True)","1007fac8":"carprice['Company'] = x[0].values","443c8827":"carprice['Company'].value_counts()","82b1c239":"carprice['Company'] = carprice['Company'].replace({'toyouta': 'Toyota','vw':'Volkswagen','vokswagen':'Volkswagen',\n                                                      'maxda':'Mazda','porcshce':'Porsche'})","252ca12e":"carprice['Company'] = carprice['Company'].str.title()","92ffde74":"plt.figure(figsize=(10,8))\nsns.boxplot(carprice['Company'], y=carprice['price'], data=carprice)\nplt.xticks(rotation=60)\nplt.show()","b89b2fe4":"carprice.columns","d7d8fbac":"df =carprice.copy()","3e42253e":"correlation = carprice.corr()","d3cd3268":"plt.figure(figsize=(15,8))\nsns.heatmap(abs(correlation), annot=True, cmap='coolwarm')","f111f0ee":"correlation.price","40784d39":"carprice.drop(columns =['car_ID','carheight', 'stroke', 'compressionratio','peakrpm', 'CarName', 'doornumber'],inplace=True)","1b4010f7":"labeel_for_DS = carprice['price'].copy() #Will use it in the Random Classifier","faa64d80":"from scipy.stats import zscore\nnumeric_cols = carprice.select_dtypes(include=[np.number]).columns\ncarprice[numeric_cols] = carprice[numeric_cols].apply(zscore)","6c181d8e":"carprice.head()","d93f1273":"carprice.info()","0dc932bc":"for i in carprice.columns:\n    if carprice[i].dtype =='O':\n        print(i+' : ',carprice[i].unique())","0c0549b1":"carprice.fueltype = carprice.fueltype.map({'gas': 1,'diesel':0})","14be0382":"carprice.aspiration = carprice.aspiration.map({'std':1, 'turbo':0})","0a848909":"carprice.enginelocation = carprice.enginelocation.map({'front':1,'rear':0})","be8a5f80":"carprice.head()","ecd9fec6":"for i in carprice.columns:\n    if carprice[i].dtype == 'O' and carprice[i].nunique() >2:\n        #print(i, i+'_Dummies')\n        carprice.Dummies =pd.get_dummies(carprice[i])\n        carprice = pd.concat([carprice, carprice.Dummies], axis=1)\n        carprice.drop(columns=[i], inplace=True)\n        ","29b8a4ca":"carprice.head()","fc912bac":"df2=carprice.copy()","98783b21":"# Target Variable\ny = carprice['price']\n\n#Features\nx= carprice.drop(columns=['price'])","4b79d627":"# First converting the Features into Dictionary\nx = x.to_dict(orient='records')\n\n#Importing vectorizer to convert Dictionary to array\nfrom sklearn.feature_extraction import DictVectorizer\nvec = DictVectorizer()\nx = vec.fit_transform(x).toarray()\n\n#converting our target variable into array\ny = np.asarray(y)\n","320db40f":"xtrain, xtest, ytrain, ytest = train_test_split(x,y, test_size=0.2, random_state=42)","68c447bc":"print(\"xtrain shape : \", xtrain.shape,\" :: xtest shape  : \", xtest.shape,\" :: ytrain shape : \", ytrain.shape,\" :: ytest shape  : \", ytest.shape) ","6a971bf8":"regressor = LinearRegression()","70ca24fd":"# Model Training\nregressor.fit(xtrain, ytrain)\n\n#Model Prediction\ny_pred_linear = regressor.predict(xtest) ","d00b4700":"plt.style.use('ggplot')\nplt.scatter(ytest, y_pred_linear, c = 'blue') \nplt.xlabel(\"Expected\") \nplt.ylabel(\"Predicted value\") \nplt.title(\"True value vs predicted value : Linear Regression\") \nplt.show() ","c1cd6192":"print('\\033[1mMean Squared Error is:\\033[0m', metrics.mean_squared_error(ytest, y_pred_linear))  \nprint('\\033[1mMean Absolute Error is:\\033[0m', metrics.mean_absolute_error(ytest, y_pred_linear))  \nprint('\\033[1mRoot Mean Squared Error is:\\033[0m', np.sqrt(metrics.mean_squared_error(ytest, y_pred_linear)))","fd5ac17e":"regressor.coef_","a08d2884":"df = pd.DataFrame({'Actual': ytest.flatten(), 'Predicted': y_pred_linear.flatten()})","8b129b32":"df.head(10)","cf7974b6":"df = df.head(25)\ndf.plot(kind='bar',figsize=(10,5))\nplt.grid(which='major', linestyle=':', linewidth='0.99', color='black')\nplt.xlabel(\"No. of Records\")\nplt.ylabel(\"Values\")\nplt.show()","c3cbb6a1":"from sklearn.ensemble import RandomForestRegressor","2afdd015":"#Model Training\nrfr = RandomForestRegressor(n_estimators =1000, random_state=42)\nrfr.fit(xtrain,ytrain);\n\n#Model Prediction\ny_pred_rfr = rfr.predict(xtest)","9758969c":"print('\\033[1mMean Absolute Error:\\033[0m', metrics.mean_absolute_error(ytest,y_pred_rfr))\nprint('\\033[1mMean Squared Error:\\033[0m', metrics.mean_squared_error(ytest, y_pred_rfr))\nprint('\\033[1mRoot Mean Square Error:\\033[0m', np.sqrt(metrics.mean_squared_error(ytest, y_pred_rfr)))","9cb224fb":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV","ebc758c6":"ridge = Ridge()\nparameters = {'alpha': [1e-15,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1,5,10,20,30,40,45,50,55,60,100]}\nridge_regressor = GridSearchCV(ridge, parameters, scoring='neg_mean_squared_error', cv=5)\nridge_regressor.fit(xtrain,ytrain);","1f141aad":"print(\"The best fit alpha value is found out to be :\" ,ridge_regressor.best_params_)\nprint(\"\\nUsing \",ridge_regressor.best_params_, \" the negative mean squared error is: \", ridge_regressor.best_score_)","9a2bb171":"#Model Prediction\ny_pred_ridge = ridge_regressor.predict(xtest)","892d2e9b":"print(\"\\033[1mUSING ALPHA =1\")\nprint(\"\\nMean Sqaured Error for Ridge Regression is : \\033[0m\", metrics.mean_squared_error(ytest, y_pred_ridge))","e44d3354":"plt.figure(figsize=(8,5))\nplt.plot(y_pred_ridge)\nplt.plot(ytest)\nplt.legend([\"Predicted\",\"Actual\"])\nplt.show()","83825e74":"from sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV","ce53dcdf":"lasso = Lasso()\nparamaters = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1e-1,1,5,10,20,30,40,45,50,55,60,100]}\nlasso_regressor = GridSearchCV(lasso, parameters, scoring='neg_mean_squared_error', cv=3)\nlasso_regressor.fit(xtrain, ytrain);","73f6b70e":"print(\"The best fit alpha value is found out to be :\" ,lasso_regressor.best_params_)\nprint(\"\\nUsing \",lasso_regressor.best_params_, \" the negative mean squared error is: \", lasso_regressor.best_score_)","2923dcd4":"#Prediction using Lasso\ny_pred_lasso = lasso_regressor.predict(xtest)","f3a68b4b":"print(\"\\033[1mUSING ALPHA =0.001\")\nprint(\"\\nMean Sqaured Error for Lasso is : \\033[0m\", metrics.mean_squared_error(ytest, y_pred_lasso))","d0d50b45":"plt.figure(figsize=(8,5))\nplt.plot(y_pred_lasso)\nplt.plot(ytest)\nplt.legend([\"Predicted\",\"Actual\"])\nplt.show()","a5a7d054":"from sklearn.linear_model import ElasticNet","5c4ecb58":"elastic = ElasticNet()\nparamaters = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1e-1,1,5,10,20,30,40,45,50,55,60,100]}\nelastic_regressor = GridSearchCV(elastic, parameters, scoring='neg_mean_squared_error',cv=5)\nelastic_regressor.fit(xtrain, ytrain);","2ec83fbc":"print(\"The best fit alpha value is found out to be :\" ,elastic_regressor.best_params_)\nprint(\"\\nUsing \",elastic_regressor.best_params_, \" the negative mean squared error is: \", elastic_regressor.best_score_)","39738dd4":"y_pred_elastic = elastic_regressor.predict(xtest)","f9590570":"print(\"\\033[1mUSING ALPHA =0.001\")\nprint(\"\\nMean Sqaured Error for Elastic Net Regression is : \\033[0m\", metrics.mean_squared_error(ytest, y_pred_elastic))","583b33b7":"plt.figure(figsize=(8,5))\nplt.plot(y_pred_elastic)\nplt.plot(ytest)\nplt.legend([\"Predicted\",\"Actual\"])\nplt.show()","6e216f43":"# Categorizing Target variable as High for income >11000 and Low for  <11000\ny2 =np.where(labeel_for_DS>11000, 'High',\"Low\") \n\n#Manual Encoding of Target Varibale\ny2 = np.where(y2=='High',1,0)","a8cb8107":"xtrain2,xtest2,ytrain2,ytest2 = train_test_split(x,y2,test_size=0.3, random_state=42 )","f65738db":"print(xtrain2.shape,xtest2.shape,ytrain2.shape,ytest2.shape)","25026bda":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nmodel = rfc.fit(xtrain2, ytrain2)\ny_pred_rfc = rfc.predict(xtest2)","49268fbd":"print (\"\\n \\033[1m Classification report : \\033[0m\\n\",classification_report(ytest2, y_pred_rfc))\nprint (\"\\n \\033[1m Accuracy : \\033[0m\\n\",metrics.accuracy_score(ytest2, y_pred_rfc))","ba3dfc79":"plt.style.use('ggplot')\ncf_matrix = confusion_matrix(y_pred_rfc, ytest2)\nx_y_labels = ['High','Low']\nsns.heatmap(cf_matrix.T, square=True, annot=True, xticklabels=x_y_labels, yticklabels=x_y_labels)\nplt.xlabel('Predicted label')\nplt.ylabel('Actual label');","5f975860":"def model_report(model,training_x,testing_x,training_y,testing_y,name) :\n    model.fit(training_x,training_y)\n    predictions  = model.predict(testing_x)\n    #accuracy     = accuracy_score(testing_y,predictions)\n    mean_sq_err  = metrics.mean_squared_error(testing_y,predictions)\n    mean_abs_err = metrics.mean_absolute_error(testing_y,predictions)\n    Rmean_sq_err = np.sqrt(metrics.mean_squared_error(testing_y,predictions) )\n        \n    df = pd.DataFrame({\"Model\"                  : [name],\n                       \"Mean Square Error\"      : [mean_sq_err],\n                       \"Mean Absolute Error\"    : [mean_abs_err],\n                       \"Root Mean Square Error\" : [Rmean_sq_err],\n                       \n                      })\n    return df\n\nmodel1 = model_report(regressor,xtrain,xtest,ytrain,ytest,\n                      \"Linear Regression\")\n\nmodel2 = model_report(ridge_regressor,xtrain,xtest,ytrain,ytest,\n                      \"Ridge Regression\")\n\nmodel3 = model_report(lasso_regressor,xtrain,xtest,ytrain,ytest,\n                      \"Lasso Regression\")\n\nmodel4 = model_report(elastic_regressor,xtrain,xtest,ytrain,ytest,\n                      \"Elastic Net Regression\")\n\nmodel5 = model_report(rfr,xtrain,xtest,ytrain,ytest,\n                      \"Random Forest Regressor\")\n\nmodel_performances = pd.concat([model1,model2,model3,model4,model5],axis = 0).reset_index()\nmodel_performances = model_performances.drop(columns = \"index\",axis =1)\n\ntable  = ff.create_table(np.round(model_performances,4))\n\npy.iplot(table)","9f597fcf":"## 4. Bivariate Analysis (for Feature Selection)","68763578":"### 8.1 Transforming Features and Target Variables into Arrays","f70ad3ec":"### 8.4.1 Fitting Random Forest Regressor","91ae153a":"For categorical features that have two unique values, we will  manually encode them.","840978f7":"#### Actual vs Prediction","5b42bb59":"### 8.6.1 Applying Lasso Regression","02d177c8":"Looking at the values we can see spelling mistakes in the company names as well as car model names. We will therefore split the column into company name and car model name.","71492a4f":"## 8.6 Lasso Regression","d9bb1c67":"#### 8.3.2.1 Visualization of Actual vs Predicted Values","968dc41a":"#### Classification Report","b03feb32":"## 6. Normalization of Numerical Features","3bee9086":"# 1. Importing Libraries","6f6aeb89":"## 8.3 Linear Regression Model","ac99ba00":"## 8.8 Random Forest Classifier","f155796a":"### 5.1. Pearson Correlation","a67e00c8":"### 8.2 Train Test Split","30ad9c29":"## 7. Feature Encoding","39a937b9":"# <center><u> CarPrice Data Set<\/u><\/center>","648b3ee3":"## 5. Feature Selection","2945c46d":"## 8.5 Ridge Regression","7c279aa1":"## <center>---**---End---**---<\/center>","aa72a419":"#### Actual vs Predicted","ace8a56f":"From the above box plots we can see that there is nout much difference in price when the door numbers are two or four. Therefore assuming a very low correlation we will drop the door number column. For the car name column we cant clearly see the labels but the plot is some what depicting a high correlation with price. \n\nTo visualize this better lets split the column and also rename the values in it.","2813867e":"<b> Before splitting the data its worth mentioning here that we havent removed the outliers. I think the outliers here may represent a real picture for eg prices of some cars may in real world be too high. Therefore I am of the opinion that removing outliers in the data set at hand would be not a wise thing to do.<\/b>","5a89c2ec":"## 8.4 Random Forest Regressor","5cfec5f5":"CRIME DATA SET NOTE BOOK STRUCTURE\n\n1.\tImporting Libraries\n2.\tData Loading\n3.\tData Description\n\n4.\tBivariate Analysis (for Feature Selection)\n    4.1.\tDistribution of Each Numerical Attribute against price\n\n5.\tFeature Selection\n    5.1.\tPearson Correlation\n\n6.\tNormalization of Numerical Features\n\n7.\tFeature Encoding\n    7.1.\tManual Encoding\n    7.2.\tOne Hot Encoding\n\n8.\tMachine Learning\n    8.1.\tTransforming Features and Target Variables into Arrays\n    8.2.\tTrain Test Split\n    8.3.\tLinear Regression Model\n        8.3.1.\tFitting Linear Regression Model\n        8.3.2.\tLinear Regression Model Evaluation\n        8.3.2.1.\tVisualization of Actual vs Predicted\n    \n    8.4.\tRandom Forest Regressor\n        8.4.1.\tFitting Random Forest Regressor\n        8.4.2.\tRandom Forest Regressor Evaluation\n    \n    8.5.\tRidge Regression\n        8.5.1.\tApplying Ridge Regression\n        8.5.2.\tRidge Regressor Evaluation\n    \n    8.6.\tLasso Regression\n        8.6.1.\tApplying Lasso Regression\n        8.6.2.\tLasso Regression Evaluation\n    \n    8.7.\tElastic Net Regression\n        8.7.1.\tApplying Elastic Net Regression\n        8.7.2.\tElastic Net Regressor Evaluation\n    \n    8.8.\tRandom Forest Classifier\n        8.8.1.\tBuilding Classifier for Random Forest\n        8.8.2.\tTrain Test Split\n        8.8.3.\tFitting Random Forest Classifier\n        8.8.4.\tRandom Forest Classifier Evaluation\n    \n    8.9. Model Metrics Comparision\n","caecd56f":"### 4.1. Distribution of Each Numerical Attribute against Price","215d79f4":"For Categorical variables having more than two unique values we will hot encode those features","75cec1b6":"#### Confusion Matrix","325f6407":"#### Actual vs Predicted Graph","1a5271e1":"## 2. Data Loading","05ef573c":"### 8.8.1 Building Classifer for Random Forest","1d02757d":"We need to convert both x and y into arrays before applying the Linear Regression Model","49299aed":"## 8.7. Elastic Net Regression ","19ee84f9":"## 8.9. Model Metrics Evaluation","9031ea2d":"## 8. Machine Learning","e2d76411":"## 3. Data Description","81d102ae":"### 8.5.1 Applying Ridge Regression","524cf89a":"### 8.4.2 Random Forest Regressor Evaluation","405249f9":"### 7.2. One Hot Encoding","c1b570a2":"### 8.3.1 Fitting Linear Regression Model","2d4b20d1":"### 8.5.2 Ridge Regressor Evaluation","a5e9775c":"### 8.8.3 Applying Random Forest Classifier","3b08a7ab":"From th above visulizations we can clearly see how widely the prices vary from company to company. We therefore can use this feature to train our model and predict price based on company names rather than using car name (models).","1a8b875e":"### 8.8.4 Random Forest Classifier Evaluation:","f4470d51":"### 8.7.1. Applying Elastic Net Regression","c509f5aa":"###  8.8.2 Train Test Split","fabd657a":"From the correlation matrix we found out that carheight, stroke, compressionratio and peakrpm have no noticable affect on the price of the cars therefore, we will drop these columns. car_ID column is also irrelevant for the prediction of car price. From the box plot visualizations above we saw carNames and doornumber attribute can also be dropped.","d7d1fbab":"### 8.7.2. ElasticNet Regressor Evaluation","cf9e30ab":"### 7.1. Manual Encoding","f6824fed":"### 8.3.2 Linear Regression Evaluation","df882189":"### 8.6.2 Lasso Regressor Evaluation"}}