{"cell_type":{"2c03e168":"code","b94ee6f1":"code","c333d01a":"code","4e022c12":"code","db7b284b":"code","da7d4bbc":"code","5f92c10d":"code","ec4f5a67":"code","5785fa12":"code","2ef61ff2":"code","b9d6223f":"code","987aef2e":"code","45137492":"code","2f957880":"code","9b25cf92":"code","dd043abb":"code","4c18ab04":"code","82f779f6":"code","1a1eb868":"code","954d3273":"code","eb1ccd71":"code","4efb22a7":"code","1631f3e6":"code","9fd7739e":"code","923aa6a3":"code","5c95733f":"code","91731163":"code","5f9dd575":"markdown","b5dde6da":"markdown"},"source":{"2c03e168":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport datetime\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b94ee6f1":"# Let's do Transformer :)\n\n# read EU wind data\ndata_path_eu = \"\/kaggle\/input\/30-years-of-european-wind-generation\/\"\n# TS is based on smaller regime, but EMHIRESPV is for countries\n# data is for each hour\n\nTS = pd.read_csv(data_path_eu+ \"TS.CF.N2.30yr.csv\")\n\nEMHIRESPV_TSh_CF_Country_19862015 = pd.read_csv(data_path_eu+\"EMHIRESPV_TSh_CF_Country_19862015.csv\")\ndate_array = [datetime.datetime(1986, 1, 1) + datetime.timedelta(hours=i) for i in range(TS.shape[0])]\nTS[\"time_stamp\"] = date_array\nEMHIRESPV_TSh_CF_Country_19862015[\"time_stamp\"] = date_array\nos.listdir(data_path_eu)","c333d01a":"plot_path=\".\/\"\nimport matplotlib\nfrom matplotlib import colors as mcolors\nimport matplotlib.pyplot as plt\ncolor_array = list(mcolors.CSS4_COLORS.keys())\ndf = EMHIRESPV_TSh_CF_Country_19862015\n\nnames_array = list(df.keys()[:29])\n\n\n## group by hours\n# A rough visualization of the data\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\ndf['hours'] = df['time_stamp'].dt.hour\n\n\n\nfrom matplotlib.pylab import rc\nfont = {'family': 'normal','weight': 'bold',\n        'size': 25}\n\nmatplotlib.rc('font', **font)\nrc('axes', linewidth=3)\n\n\nplt.subplot(1,1,1)\n\n\n\nfor i in range(len(names_array)):\n    #print(\"Doing %d\"%i)\n    plt.plot(df.groupby('hours').mean().index,df.groupby('hours').mean()[names_array[i]],color_array[i],label=names_array[i],linewidth=2,markersize=5)\n    \n\n\n\n\nplt.xlabel(\"Hour\")\nplt.ylabel(r\"${\\rm value}$\")\nplt.suptitle(\"Mean Usage grouped by hour\")\n\nfig = matplotlib.pyplot.gcf()\n\nfig.set_size_inches(20,20)\nplt.legend(fontsize=25,handlelength=5,ncol=3)\n#save_path = plot_path + \"Data_EU_group_by_hour\" + \".png\"\n\n#fig.savefig(save_path, dpi=200)\n\n\n\n\n\n\n","4e022c12":"from matplotlib import colors as mcolors\ncolor_array = list(mcolors.CSS4_COLORS.keys())\n\ndf = EMHIRESPV_TSh_CF_Country_19862015\n\nnames_array = list(df.keys()[:29])\n\n\n## group by hours\n# A rough visualization of the data\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\ndf['hours'] = df['time_stamp'].dt.hour\ndf['month'] = df['time_stamp'].dt.month\n\n\n\nfrom matplotlib.pylab import rc\nfont = {'family': 'normal','weight': 'bold',\n        'size': 25}\n\nmatplotlib.rc('font', **font)\nrc('axes', linewidth=3)\n\n\nplt.subplot(1,1,1)\n\n\n\nfor i in range(len(names_array)):\n    #print(\"Doing %d\"%i)\n    plt.plot(df.groupby('month').mean().index,df.groupby('month').mean()[names_array[i]],color_array[i],label=names_array[i],linewidth=2,markersize=5)\n    \n\n\n\n\nplt.xlabel(\"Month\")\nplt.ylabel(r\"${\\rm value}$\")\nplt.suptitle(\"Mean Usage grouped by month\")\n\nfig = matplotlib.pyplot.gcf()\n\nfig.set_size_inches(20,20)\nplt.legend(fontsize=25,handlelength=5,ncol=3)\nplt.show()\n","db7b284b":"# Let's find out the peroid inside:\n# PLOT VS DAY\ndf[\"day_delta\"] = np.arange(0,df.shape[0],1)\/\/24\n\n\n\n\n\nfrom matplotlib.pylab import rc\nfont = {'family': 'normal','weight': 'bold',\n        'size': 25}\n\nmatplotlib.rc('font', **font)\nrc('axes', linewidth=3)\n\n\nfor i in range(3):\n    plt.subplot(3,1,1+i)\n\n    plt.plot(df.groupby(\"day_delta\").mean()[names_array[i]],\"b\",label=names_array[i])\n    plt.legend()\n\n\n\n\nplt.xlabel(\"Day\")\nplt.ylabel(r\"${\\rm value}$\")\nplt.suptitle(\"Value vs day\")\n\nfig = matplotlib.pyplot.gcf()\n\nfig.set_size_inches(35,30)\n\nplt.show()","da7d4bbc":"# PLOT VS Month\ndf[\"week_delta\"] = np.arange(0,df.shape[0],1)\/\/(24*7)\n\n\n\n\n\nfrom matplotlib.pylab import rc\nfont = {'family': 'normal','weight': 'bold',\n        'size': 25}\n\nmatplotlib.rc('font', **font)\nrc('axes', linewidth=3)\n\n\nfor i in range(3):\n    plt.subplot(3,1,1+i)\n\n    plt.plot(df.groupby(\"week_delta\").mean()[names_array[i]],\"b\",label=names_array[i])\n    plt.legend()\n\n\n\n\nplt.xlabel(\"Week\")\nplt.ylabel(r\"${\\rm value}$\")\nplt.suptitle(\"Value vs Week\")\n\nfig = matplotlib.pyplot.gcf()\n\nfig.set_size_inches(35,30)\n\nplt.show()","5f92c10d":"# PLOT VS Month\ndf[\"month_delta\"] = np.arange(0,df.shape[0],1)\/\/(24*30)\n\n\nfrom matplotlib.pylab import rc\nfont = {'family': 'normal','weight': 'bold',\n        'size': 25}\n\nmatplotlib.rc('font', **font)\nrc('axes', linewidth=3)\n\n\nfor i in range(3):\n    plt.subplot(3,1,1+i)\n\n    plt.plot(df.groupby(\"month_delta\").mean()[names_array[i]],\"b\",label=names_array[i])\n    plt.legend()\n\n\n\n\nplt.xlabel(\"Month\")\nplt.ylabel(r\"${\\rm value}$\")\nplt.suptitle(\"Value vs Month\")\n\nfig = matplotlib.pyplot.gcf()\n\nfig.set_size_inches(35,30)\nplt.show()","ec4f5a67":"# PLOT VS year\ndf[\"year_delta\"] = np.arange(0,df.shape[0],1)\/\/(24*365)\n\n\nfrom matplotlib.pylab import rc\nfont = {'family': 'normal','weight': 'bold',\n        'size': 25}\n\nmatplotlib.rc('font', **font)\nrc('axes', linewidth=3)\n\n\nfor i in range(3):\n    plt.subplot(3,1,1+i)\n\n    plt.plot(df.groupby(\"year_delta\").mean()[names_array[i]],\"b\",label=names_array[i])\n    plt.legend()\n\n\n\n\nplt.xlabel(\"Year\")\nplt.ylabel(r\"${\\rm value}$\")\nplt.suptitle(\"Value vs Year\")\n\nfig = matplotlib.pyplot.gcf()\n\nfig.set_size_inches(35,30)\n\nplt.show()","5785fa12":"# Next step: predict future trend using Transformer.","2ef61ff2":"import tensorflow as tf\nimport os\n\nfrom sklearn import preprocessing\n\nfrom sklearn.model_selection import train_test_split \n\n","b9d6223f":"# scale dot attention:\ndef scaled_dot_product_attention(q, k, v, mask):\n    matmul_qk = tf.matmul(q, k, transpose_b=True)\n    # Dimension of k\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    scaled_attention_logits = matmul_qk \/ tf.math.sqrt(dk)\n    if mask is not None:\n        scaled_attention_logits += (mask * -1e9)  \n    # calculate attention weight:\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n    output = tf.matmul(attention_weights, v)\n    return output, attention_weights\n\n# Multi-head Attention:\n# This is what we use\nclass MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads):\n        \n        # Always use Super to inheriatte and avoid extra code.\n        assert d_model%num_heads==0\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n        # sanity check:\n        assert d_model % self.num_heads == 0\n        self.depth = d_model \/\/ self.num_heads\n        # Q K W:\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n        \n        self.dense = tf.keras.layers.Dense(d_model)\n    def split_heads(self, x, batch_size):\n        # Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n    def call(self, v, k, q, mask):\n        batch_size = tf.shape(q)[0]\n        q = self.wq(q)  # (batch_size, seq_len, d_model)\n        k = self.wk(k)  # (batch_size, seq_len, d_model)\n        v = self.wv(v)  # (batch_size, seq_len, d_model)\n        \n        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n        \n        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n        \n        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n        # https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/transpose : perm\n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n        concat_attention = tf.reshape(scaled_attention, \n                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n        return output, attention_weights\n    \n    \n        \n        \n\n","987aef2e":"## Encoder decoder for Time series:\n\n# pointwise feed forward network\ndef point_wise_feed_forward_network(d_model, dff):\n    # Two FC layers:\n    return tf.keras.Sequential([\n      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n  ])\n\n# Change embedding since it's not int anymore:\nclass EmbeddingLayer(tf.keras.layers.Layer):\n    def __init__(self,embedding_size):\n        super(EmbeddingLayer,self).__init__()\n        self.embedding_size=embedding_size\n\n    def build(self,input_shape):\n        with tf.name_scope('embedding'):\n            self.shared_weights=self.add_weight(name='weights',\n                                                shape=[input_shape[-1],self.embedding_size],\n                                                initializer=tf.random_normal_initializer(mean=0.,\n                                                                                         stddev=self.embedding_size ** -0.5))\n        super(EmbeddingLayer,self).build(input_shape)\n\n\n    def call(self,x):\n        y=tf.einsum('bsf,fk->bsk',x,self.shared_weights)\n        return y\n    \n\nclass EncoderLayer(tf.keras.layers.Layer):\n    # Here we use a 0.1 dropout rate as default\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(EncoderLayer, self).__init__()\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n        \n    def call(self, x, training, mask):\n        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n        attn_output = self.dropout1(attn_output, training=training)\n        \n        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        \n        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n    \n        return out2\nsample_encoder_layer = EncoderLayer(512, 8, 2048)\n\nsample_encoder_layer_output = sample_encoder_layer(tf.random.uniform((64, 43, 512)), False, None)\n\nprint(sample_encoder_layer_output.shape)  # (batch_size, input_seq_len, d_model)\n\nclass DecoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(DecoderLayer, self).__init__()\n\n        self.mha1 = MultiHeadAttention(d_model, num_heads)\n        self.mha2 = MultiHeadAttention(d_model, num_heads)\n\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n        self.dropout3 = tf.keras.layers.Dropout(rate)\n        \n    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n        # enc_output.shape == (batch_size, input_seq_len, d_model)\n\n        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n        attn1 = self.dropout1(attn1, training=training)\n        out1 = self.layernorm1(attn1 + x)\n\n        attn2, attn_weights_block2 = self.mha2(\n            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n        attn2 = self.dropout2(attn2, training=training)\n        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n\n        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n        ffn_output = self.dropout3(ffn_output, training=training)\n        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n\n        return out3, attn_weights_block1, attn_weights_block2\n\n","45137492":"def get_angles(pos, i, d_model):\n    angle_rates = 1 \/ np.power(10000, (2 * (i\/\/2)) \/ np.float32(d_model))\n    return pos * angle_rates\n\ndef positional_encoding(position, d_model):\n    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n                              np.arange(d_model)[np.newaxis, :],\n                              d_model)\n\n    # apply sin to even indices in the array; 2i\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n\n    # apply cos to odd indices in the array; 2i+1\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n    pos_encoding = angle_rads[np.newaxis, ...]\n    return tf.cast(pos_encoding, dtype=tf.float32)\n\nclass Encoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n               maximum_position_encoding, rate=0.1):\n        super(Encoder, self).__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n        self.pos_encoding = positional_encoding(maximum_position_encoding, \n                                                self.d_model)\n\n\n        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n                           for _ in range(num_layers)]\n\n        self.dropout = tf.keras.layers.Dropout(rate)\n        \n    def call(self, x, training, mask):\n        seq_len = tf.shape(x)[1]\n    \n        # adding embedding and position encoding.\n        #print(\"Check\",x.shape)\n        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n        #x = tf.keras.layers.Dense(self.d_model)(x)\n        #print(\"check 2\",x.shape)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n        #print(\"check 3\",x.shape)\n\n        x = self.dropout(x, training=training)\n        #print(\"check 4\",x.shape)\n\n        for i in range(self.num_layers):\n            x = self.enc_layers[i](x, training, mask)\n        return x  # (batch_size, input_seq_len, d_model)\n    \nclass Decoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n               maximum_position_encoding, rate=0.1):\n        super(Decoder, self).__init__()\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n\n        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n                           for _ in range(num_layers)]\n        self.dropout = tf.keras.layers.Dropout(rate)\n        \n    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n        seq_len = tf.shape(x)[1]\n        attention_weights = {}\n\n        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n        #x = tf.keras.layers.Dense(self.d_model)(x)\n        \n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n\n        x = self.dropout(x, training=training)\n        for i in range(self.num_layers):\n            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n                                                 look_ahead_mask, padding_mask)\n\n            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n    \n        # x.shape == (batch_size, target_seq_len, d_model)\n        return x, attention_weights\n    ","2f957880":"class Transformer(tf.keras.Model):\n    def __init__(self, num_layers, d_model, num_heads, dff, input_seq_size, \n               output_seq_size, input_delta_t, output_delta_t, rate=0.1):\n        super(Transformer, self).__init__()\n\n        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n                               input_seq_size, input_delta_t, rate)\n\n        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n                               output_seq_size, output_delta_t, rate)\n\n        #self.final_layer = tf.keras.layers.Dense(output_seq_size)\n        self.final_layer = tf.keras.layers.Dense(1)\n\n\n    def call(self, inp, tar, training, enc_padding_mask, \n           look_ahead_mask, dec_padding_mask):\n        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n        #print(\"check encoder size\",enc_output.shape)\n\n    \n        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n        dec_output, attention_weights = self.decoder(\n            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n        \n        #print(\"check decoder size\",dec_output.shape)\n\n        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n\n        return final_output, attention_weights\n\n","9b25cf92":"# sanity check:\n# We encoder the float32 input to input_seq_size\/output_seq_size integers\n# The output is a sliding time table for different time scale prediction:\n# Eg: you need to make sure your prediction delta_t<output delta_t and input data delta_t < input_delta_t\n# For GTX 1060 we can set batch=16 and use 4X batch size for Tesla P40\n\nbatch = 8\n\nsample_transformer = Transformer(\n    num_layers=2, d_model=512, num_heads=8, dff=2048, \n    input_seq_size=1000, output_seq_size=1000, \n    input_delta_t=1440, output_delta_t=240)\n\n# input: batch+sequence length\n# biggest length for in\/out put is pe_input,  pe_target\ntemp_input = tf.random.uniform((batch, 720), dtype=tf.int64, minval=0, maxval=1000)\ntemp_target = tf.random.uniform((batch, 3), dtype=tf.int64, minval=0, maxval=1000)\n\n#temp_input = tf.cast(temp_input,dtype=tf.float32)\n#temp_target = tf.cast(temp_target,dtype=tf.float32)\n\nfn_out, attention = sample_transformer(temp_input, temp_target, training=False, \n                               enc_padding_mask=None, \n                               look_ahead_mask=None,\n                               dec_padding_mask=None)\n\nprint(\"final output size\",fn_out.shape)  # (batch_size, tar_seq_len, target_vocab_size)","dd043abb":"# prepare data: fow now I only use 1D data, but it can be extended to multiple channel:\n# Load data:names_array\n\n# Let's use uk for example\ntemp = df[\"UK\"]\n# Normalize to 0-1000\n\ntemp = (temp-min(temp))\/(max(temp)-min(temp))\n\nlower, upper = 0, 999\ntemp = [lower + (upper - lower) * x for x in temp]\ntemp = np.array(temp,dtype=int)\n\n## This value is not always recommended since we should use around 7:1 in slidind window Eg: 168 H for 24 H. It's just a demon\ndelta_t = 168\ndelta_t_out = 2\n\n\nX = np.zeros((temp.shape[0]-delta_t-delta_t_out,delta_t,1),dtype=int)\n\nfor i in range(delta_t_out):\n    if i==0:\n        y = temp[delta_t:-delta_t_out]\n    else:\n        y = np.c_[y,temp[delta_t+i:-(delta_t_out-i)]]\n    \n    \nfor i in range(y.shape[0]):\n    if i%50000==0:\n        print(\"Prepare data %.2f percent\"%(100*i\/len(y)))\n    X[i,:,:] = np.atleast_2d(temp[i:i+delta_t]).T\n\n\ntrain_dataset_TS = tf.data.Dataset.from_tensor_slices((X,y))","4c18ab04":"## Optimizor:\nimport matplotlib.pyplot as plt\n\nd_model=512\n\nclass CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=4000):\n        super(CustomSchedule, self).__init__()\n    \n        self.d_model = d_model\n        self.d_model = tf.cast(self.d_model, tf.float32)\n\n        self.warmup_steps = warmup_steps\n        \n    def __call__(self, step):\n        arg1 = tf.math.rsqrt(step)\n        arg2 = step * (self.warmup_steps ** -1.5)\n\n        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n    \nlearning_rate = CustomSchedule(d_model)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n                                     epsilon=1e-9)\n# Learning rate curve:\ntemp_learning_rate_schedule = CustomSchedule(d_model)\n\nplt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\nplt.ylabel(\"Learning Rate\")\nplt.xlabel(\"Train Step\")\nfig = matplotlib.pyplot.gcf()\n\nfig.set_size_inches(16,16)\nplt.show()","82f779f6":"# Loss function:\n# loss and metric\n\n# For now I use sparse-cross entropy. But MAE may make more sense here:\n\nloss_object = tf.keras.losses.MeanSquaredError(reduction='none')\n\n\ndef loss_function(real, pred):\n    #mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n\n    #mask = tf.cast(mask, dtype=loss_.dtype)\n    #loss_ *= mask\n  \n    return tf.reduce_sum(loss_)\/tf.cast(len(loss_),dtype=tf.float32)\n\n\n\n\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\n\n\ntrain_accuracy = tf.keras.metrics.MeanSquaredError(name='mean_squared_error',dtype=tf.float32)\n\n\n\n# Optional\n#train_accuracy = tf.keras.metrics.MeanSquaredError(name='train_MSE')\n\n\n\n\n","1a1eb868":"def create_padding_mask(seq):\n    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n  \n    # add extra dimensions to add the padding\n    # to the attention logits.\n    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n\ndef create_look_ahead_mask(size):\n    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n    return mask  # (seq_len, seq_len)\n\ndef create_masks(inp, tar):\n    # Encoder padding mask\n    enc_padding_mask = create_padding_mask(inp)\n\n    # Used in the 2nd attention block in the decoder.\n    # This padding mask is used to mask the encoder outputs.\n    dec_padding_mask = create_padding_mask(inp)\n\n    # Used in the 1st attention block in the decoder.\n    # It is used to pad and mask future tokens in the input received by \n    # the decoder.\n    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n    dec_target_padding_mask = create_padding_mask(tar)\n    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n\n      \n    return enc_padding_mask, combined_mask, dec_padding_mask","954d3273":"\n\n\n\nbatch = 8\n\ntransformer = Transformer(\n    num_layers=2, d_model=512, num_heads=8, dff=2048, \n    input_seq_size=1000, output_seq_size=1000, \n    input_delta_t=1440, output_delta_t=240)\n\n\n\n# save file: optional\ncheckpoint_path = \".\/checkpoints\/train_TS_EU\"\n\nckpt = tf.train.Checkpoint(transformer=transformer,\n                           optimizer=optimizer)\n\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n\n# if a checkpoint exists, restore the latest checkpoint.\nif ckpt_manager.latest_checkpoint:\n    ckpt.restore(ckpt_manager.latest_checkpoint)\n    print ('Latest checkpoint restored!!')\n\n\n\ntrain_step_signature = [\n        tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n        tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n    ]\n\n@tf.function(input_signature=train_step_signature)\n\ndef train_step(inp, tar):\n        \n    tar_inp = tar\n    tar_real = tar\n\n    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n    with tf.GradientTape() as tape:\n        # No mask for now\n        enc_padding_mask, combined_mask, dec_padding_mask = None,None,None\n        predictions, _ = transformer(inp, tar_inp, True, enc_padding_mask, combined_mask, dec_padding_mask)\n        predictions = predictions[:,:,0]\n        loss = loss_function(tar_real, predictions)\n        ## Optional: Add MSE error term. Since the number in SCCE doesn't make sense. Add MSE to punish far away dots like 0 and 999\n        #predictions_id = tf.argmax(predictions, axis=-1)\n        #loss+=float(tf.reduce_sum(tf.keras.losses.MSE(tar,predictions_id))\/(10000*batch))\n        #value = float(tf.reduce_sum(tf.keras.losses.MSE(tar,predictions_id))\/(1*batch))\n        # Avoid gradient exploding\n        \"\"\"\n        if not loss>0:\n            value=float(100000)\n        loss+=value\n        \n        \"\"\"\n        \n        \n        # Or we can only use MSE loss.\n        \n    gradients = tape.gradient(loss, transformer.trainable_variables)    \n    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n\n    train_loss(loss)\n    train_accuracy(tar_real, predictions)\n\n\n\n\n\n\n\n\n\n","eb1ccd71":"\"\"\"\n# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n\n\n\"\"\"\n\n\n\n\n#Train and save:\n\nimport time\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)\n\nEPOCHS = 10\ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train,y_train))\nbatch=128\n\nN = len(y_train)\n\nfor epoch in range(EPOCHS):\n    start = time.time()\n  \n    train_loss.reset_states()\n    train_accuracy.reset_states()\n    for i in range(N\/\/batch):\n        inp, tar=X_train[batch*i:min(batch*i+batch,N),:,0],y_train[batch*i:min(batch*i+batch,N)]\n        tar = np.atleast_2d(tar)\n        lo = train_step(inp, tar)\n        if i%500==0:\n            print(\"Doing %d (%d) batch in epoch %d \"%(i,N\/\/batch,epoch))\n            print(\"MSE\",train_accuracy.result())\n    \n  ","4efb22a7":"# testing:\nN_test = len(y_test)\n\n\nfor i in range(N_test\/\/batch):\n    if i%200==0:\n            print(\"Doing %d (%d)\"%(i,N_test\/\/batch))\n    \n    inp, tar=X_test[batch*i:min(batch*i+batch,N),:,0],y_test[batch*i:min(batch*i+batch,N)]\n    tar = tar\n    tar_inp = tar\n    tar_real = tar\n\n    #enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n    \n    predictions, attention_weights = transformer(inp, \n                                                 tar,\n                                                 False,\n                                                 None,None,None)\n    \n    if i==0:\n        y_pred_all = tf.cast(predictions[:,:,0], dtype=tf.float32)\n    else:\n        y_pred_all = np.r_[y_pred_all,tf.cast(predictions[:,:,0], dtype=tf.float32)]\n\n\n    \ny_pred_all = np.array(y_pred_all)\n\nprint(\"Train+Test all set!\")","1631f3e6":"y_pred_1d = y_pred_all[np.arange(0,y_pred_all.shape[0],y_pred_all.shape[1]),:]\nnp.save(\"y_pred_all.npy\",y_pred_all)","9fd7739e":"# plot:\ny_pred_1d = y_pred_all[np.arange(0,y_pred_all.shape[0],y_pred_all.shape[1]),:]\ny_test = y_test[:y_pred_all.shape[0]]\nimport matplotlib\nfrom matplotlib.pylab import rc\n\n\nfont = {'family': 'normal','weight': 'bold',\n        'size': 25}\n\nmatplotlib.rc('font', **font)\nrc('axes', linewidth=3)\n\nplt.plot(y_test[:1000,0],\"k\",label=\"Data\")\nplt.plot(y_pred_1d.ravel()[:1000],\"r\",label=\"Prediction-Transformer\")\ndiff = y_test[:1000,0]-y_pred_1d.ravel()[:1000]\nplt.plot(diff,\"b\",label=\"Difference\")\n\nplt.xlabel(\"Time\")\nplt.ylabel(r\"CIF\")\nplt.suptitle(\"Value vs day\")\n\nfig = matplotlib.pyplot.gcf()\nplt.legend()\n\nfig.set_size_inches(35,12)\n\n","923aa6a3":"print(\"All set\")","5c95733f":"# Anomaly:\n# Detect anomaly:\n# We want the model to detect anomaly ahead of the anomaly happening, so we shift half-pixel :)\n# plot:\ny_pred_1d = y_pred_all[np.arange(0,y_pred_all.shape[0],y_pred_all.shape[1]),:]\ny_test = y_test[:y_pred_all.shape[0]]\n\ny_pred_1d = y_pred_1d[1:]\ny_test = y_test[0:-1]\nimport matplotlib\nfrom matplotlib.pylab import rc\n\n\nfont = {'family': 'normal','weight': 'bold',\n        'size': 25}\n\nmatplotlib.rc('font', **font)\nrc('axes', linewidth=3)\n\ny1 = y_test[:1000,0]\ny2 = y_pred_1d.ravel()[:1000]\ndiff = y1-y2\nmask = abs(diff)>np.nanpercentile(abs(diff),97)\n\nplt.plot(y1,\"b\",label=\"Data\")\nplt.plot(np.arange(0,len(y2),1)[mask],y1[mask],\"ro\",label=\"Anomaly\")\n\n\n\nplt.xlabel(\"Time\")\nplt.ylabel(r\"CIF\")\nplt.suptitle(\"Value vs day\")\n\nfig = matplotlib.pyplot.gcf()\nplt.legend()\n\nfig.set_size_inches(35,12)\n\n\n","91731163":"# plot attention:\n# Let's plot the attention from model:\n# Only input one week data\nX_test_temp = tf.cast(X_test[:8,:,0],dtype=tf.float32)\ny_test_temp = tf.cast(y_test[:8,:],dtype=tf.float32)\n\n\n_, attention_temp = sample_transformer(X_test_temp, y_test_temp, training=False, \n                               enc_padding_mask=None, \n                               look_ahead_mask=None,\n                               dec_padding_mask=None)\n\n\nkeys_list = list(attention_temp)\n## There should be 8 period in each sub-plot since we input 8 one-week data :) Just a sanity check\n\nfrom matplotlib.pylab import rc\nfont = {'family': 'normal','weight': 'bold',\n        'size': 25}\n\nmatplotlib.rc('font', **font)\nrc('axes', linewidth=3)\n\ncolor_array = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\nfor i in range(4):\n    plt.subplot(4,1,1+i)\n\n    plt.plot(np.array(attention_temp[keys_list[i]]).ravel(),color = color_array[i],label=keys_list[i])\n    \n\n\n    plt.xlabel(\"Hour\")\n    plt.ylabel(r\"Attention weights\")\n    plt.legend()\nplt.suptitle(\"Attention weightsfrom each layer\")\n\nfig = matplotlib.pyplot.gcf()\n\nfig.set_size_inches(35,40)\n\n\n","5f9dd575":"# Now Let's apply the Transformer in single dimension Time series prediction\n\nReference: https:\/\/www.tensorflow.org\/tutorials\/text\/transformer","b5dde6da":"# Transformer in Time Series dataset:\n\n# Classifier-Regressor Transformer :)\nJason's toy Transformer model for predicting future trend in EU wind\n\nThe front is a classifier, since any NN model has the highest efficiency at classifier mode rather than regressor mode.\nUse interpolation to convert input into integer, since we can interpolate them into a lot of ints, we expect a very low loss in precision\nRegressor for output\n\nEU dataset: https:\/\/www.kaggle.com\/sohier\/30-years-of-european-wind-generation\n\nThanks for this link: https:\/\/www.tensorflow.org\/tutorials\/text\/transformer"}}