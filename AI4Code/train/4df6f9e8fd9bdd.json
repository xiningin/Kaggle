{"cell_type":{"5adef0fa":"code","bff3b049":"code","9c065fb8":"code","dd57b6e2":"code","09e8a0a8":"code","f95b628e":"code","fa47cda3":"code","5cb04d7e":"code","fbe7f099":"code","25aab7e6":"code","24c3d478":"code","e1d00eaa":"code","f592edc2":"code","76c2d7fb":"markdown","38d9dda1":"markdown","ea57b8ed":"markdown"},"source":{"5adef0fa":"import fastai\nfrom fastai.vision import *\nimport os\nimport cv2\n\nwork_dir = Path('\/kaggle\/working\/')\npath = Path('..\/input\/')\n\ntrain = path\/'train_images\/train_images'\ntest =  path\/'leaderboard_test_data\/leaderboard_test_data'\nholdout = path\/'leaderboard_holdout_data\/leaderboard_holdout_data'\nsample_sub = path\/'SampleSubmission.csv'\nlabels = path\/'traininglabels.csv'\n\ndf_sample = pd.read_csv(sample_sub)","bff3b049":"df_sample = pd.read_csv(sample_sub)","9c065fb8":"df = pd.read_csv(labels)\ndf.head()","dd57b6e2":"test_names = [f for f in test.iterdir()]\nholdout_names = [f for f in holdout.iterdir()]\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split","09e8a0a8":"!pip install pretrainedmodels","f95b628e":"#these are the 2 pretrainedmodels we used , for the purpose of the example we only used  se_resnext101_32x4d\nimport pretrainedmodels\n\n#def model_f(pretrained=True, **kwargs):\n#    return pretrainedmodels.senet154(num_classes=1000,pretrained='imagenet')\n\ndef model_f(pretrained=True,**kwargs):\n    return pretrainedmodels.se_resnext101_32x4d(num_classes=1000,pretrained='imagenet')","fa47cda3":"folds = StratifiedKFold(n_splits=11, random_state=2019)\n\ny = df['has_oilpalm'].values\n\nFOLD=5 # just one fold for the kaggle kernel\n\nfor fold_, (trn_, val_) in enumerate(folds.split(df, y)):\n    if fold_==FOLD:\n      train_x, trn_y = df.loc[trn_], y[trn_]\n      val_x, val_y   = df.loc[val_], y[val_]\n      filename = \"resnext101_32x4d_tfms_\" + str(fold_)\n      filename_sub = filename+\"_sub.csv\"\n\n      print(\"fold : \", fold_, train_x.shape,val_x.shape,trn_y.mean(),val_y.mean())\n\n      src = (ImageItemList.from_df(df, path, folder=train)\n          .split_by_idxs(train_x.index, val_x.index)\n          .label_from_df('has_oilpalm')\n          .add_test(test_names+holdout_names))\n\n      data =  (src.transform(get_transforms(), size=256)\n             .databunch(bs=16)\n             .normalize(imagenet_stats))\n\n      learn = create_cnn(data=data,arch= model_f,cut=-2, \n                   metrics=[accuracy], \n                   model_dir='\/kaggle\/working\/models')\n  \n      lr = 1e-2\n      learn.fit_one_cycle(5, lr)\n      lr = 1e-3\n      learn.fit_one_cycle(5, lr)\n\n      learn.unfreeze()\n      learn.fit_one_cycle(4, slice(1e-5, 1e-4))\n      learn.save(filename)\n      p,t = learn.TTA()\n \n      p = to_np(p); p.shape\n      sub = val_x\n      #We only recover the probs of having palmoil (column 1)\n      sub['preds'] =  p[:,1]\n      sub.to_csv(\"val_\"+filename_sub, index=False)\n\n      p,t = learn.TTA(ds_type=DatasetType.Test)\n      p = to_np(p); p.shape\n      ids = np.array([f.name for f in (test_names+holdout_names)]);ids.shape\n      #We only recover the probs of having palmoil (column 1)\n      sub = pd.DataFrame(np.stack([ids, p[:,1]], axis=1), columns=df_sample.columns)\n      sub.to_csv(filename_sub, index=False)\n      \n      # lets take a second look at the confusion matrix. See if how much we improved.\n      interp = ClassificationInterpretation.from_learner(learn)\n      interp.plot_confusion_matrix(title='Confusion matrix')\n  ","5cb04d7e":"# this come from this kernel https:\/\/www.kaggle.com\/qitvision\/a-complete-ml-pipeline-fast-ai\nfrom random import randint\n\ndef plot_overview(interp:ClassificationInterpretation, classes=['Negative','Positive']):\n    # top losses will return all validation losses and indexes sorted by the largest first\n    tl_val,tl_idx = interp.top_losses()\n    #classes = interp.data.classes\n    fig, ax = plt.subplots(3,6, figsize=(24,12))\n    fig.suptitle('Predicted \/ Actual \/ Loss \/ Probability',fontsize=20)\n    # Random\n    for i in range(6):\n        random_index = randint(0,len(tl_idx))\n        idx = tl_idx[random_index]\n        im,cl = interp.data.dl(DatasetType.Valid).dataset[idx]\n        im = image2np(im.data)\n        cl = int(cl)\n        ax[0,i].imshow(im)\n        ax[0,i].set_xticks([])\n        ax[0,i].set_yticks([])\n        ax[0,i].set_title(f'{classes[interp.pred_class[idx]]} \/ {classes[cl]} \/ {interp.losses[idx]:.2f} \/ {interp.probs[idx][cl]:.2f}')\n    ax[0,0].set_ylabel('Random samples', fontsize=16, rotation=0, labelpad=80)\n    # Most incorrect or top losses\n    for i in range(6):\n        idx = tl_idx[i]\n        im,cl = interp.data.dl(DatasetType.Valid).dataset[idx]\n        cl = int(cl)\n        im = image2np(im.data)\n        ax[1,i].imshow(im)\n        ax[1,i].set_xticks([])\n        ax[1,i].set_yticks([])\n        ax[1,i].set_title(f'{classes[interp.pred_class[idx]]} \/ {classes[cl]} \/ {interp.losses[idx]:.2f} \/ {interp.probs[idx][cl]:.2f}')\n    ax[1,0].set_ylabel('Most incorrect\\nsamples', fontsize=16, rotation=0, labelpad=80)\n    # Most correct or least losses\n    for i in range(6):\n        idx = tl_idx[len(tl_idx) - i - 1]\n        im,cl = interp.data.dl(DatasetType.Valid).dataset[idx]\n        cl = int(cl)\n        im = image2np(im.data)\n        ax[2,i].imshow(im)\n        ax[2,i].set_xticks([])\n        ax[2,i].set_yticks([])\n        ax[2,i].set_title(f'{classes[interp.pred_class[idx]]} \/ {classes[cl]} \/ {interp.losses[idx]:.2f} \/ {interp.probs[idx][cl]:.2f}')\n    ax[2,0].set_ylabel('Most correct\\nsamples', fontsize=16, rotation=0, labelpad=80)","fbe7f099":"plot_overview(interp, ['Negative','Positive'])","25aab7e6":"from fastai.callbacks.hooks import *\n# hook into forward pass\ndef hooked_backward(m, oneBatch, cat):\n    # we hook into the convolutional part = m[0] of the model\n    with hook_output(m[0]) as hook_a: \n        with hook_output(m[0], grad=True) as hook_g:\n            preds = m(oneBatch)\n            preds[0,int(cat)].backward()\n    return hook_a,hook_g","24c3d478":"# We can create a utility function for getting a validation image with an activation map\ndef getHeatmap(val_index):\n    \"\"\"Returns the validation set image and the activation map\"\"\"\n    # this gets the model\n    m = learn.model.eval()\n    tensorImg,cl = data.valid_ds[val_index]\n    # create a batch from the one image\n    oneBatch,_ = data.one_item(tensorImg)\n    oneBatch_im = vision.Image(data.denorm(oneBatch)[0])\n    # convert batch tensor image to grayscale image with opencv\n    cvIm = cv2.cvtColor(image2np(oneBatch_im.data), cv2.COLOR_RGB2GRAY)\n    # attach hooks\n    hook_a,hook_g = hooked_backward(m, oneBatch, cl)\n    # get convolutional activations and average from channels\n    acts = hook_a.stored[0].cpu()\n    #avg_acts = acts.mean(0)\n\n    # Grad-CAM\n    grad = hook_g.stored[0][0].cpu()\n    grad_chan = grad.mean(1).mean(1)\n    grad.shape,grad_chan.shape\n    mult = (acts*grad_chan[...,None,None]).mean(0)\n    return mult, cvIm","e1d00eaa":"# Then, modify our plotting func a bit\ndef plot_heatmap_overview(interp:ClassificationInterpretation, classes=['Negative','Positive']):\n    # top losses will return all validation losses and indexes sorted by the largest first\n    tl_val,tl_idx = interp.top_losses()\n    #classes = interp.data.classes\n    fig, ax = plt.subplots(3,6, figsize=(24,12))\n    fig.suptitle('Grad-CAM\\nPredicted \/ Actual \/ Loss \/ Probability',fontsize=20)\n    # Random\n    for i in range(6):\n        random_index = randint(0,len(tl_idx))\n        idx = tl_idx[random_index]\n        act, im = getHeatmap(idx)\n        H,W = im.shape\n        _,cl = interp.data.dl(DatasetType.Valid).dataset[idx]\n        cl = int(cl)\n        ax[0,i].imshow(im)\n        ax[0,i].imshow(im, cmap=plt.cm.gray)\n        ax[0,i].imshow(act, alpha=0.5, extent=(0,H,W,0),\n              interpolation='bilinear', cmap='inferno')\n        ax[0,i].set_xticks([])\n        ax[0,i].set_yticks([])\n        ax[0,i].set_title(f'{classes[interp.pred_class[idx]]} \/ {classes[cl]} \/ {interp.losses[idx]:.2f} \/ {interp.probs[idx][cl]:.2f}')\n    ax[0,0].set_ylabel('Random samples', fontsize=16, rotation=0, labelpad=80)\n    # Most incorrect or top losses\n    for i in range(6):\n        idx = tl_idx[i]\n        act, im = getHeatmap(idx)\n        H,W = im.shape\n        _,cl = interp.data.dl(DatasetType.Valid).dataset[idx]\n        cl = int(cl)\n        ax[1,i].imshow(im)\n        ax[1,i].imshow(im, cmap=plt.cm.gray)\n        ax[1,i].imshow(act, alpha=0.5, extent=(0,H,W,0),\n              interpolation='bilinear', cmap='inferno')\n        ax[1,i].set_xticks([])\n        ax[1,i].set_yticks([])\n        ax[1,i].set_title(f'{classes[interp.pred_class[idx]]} \/ {classes[cl]} \/ {interp.losses[idx]:.2f} \/ {interp.probs[idx][cl]:.2f}')\n    ax[1,0].set_ylabel('Most incorrect\\nsamples', fontsize=16, rotation=0, labelpad=80)\n    # Most correct or least losses\n    for i in range(6):\n        idx = tl_idx[len(tl_idx) - i - 1]\n        act, im = getHeatmap(idx)\n        H,W = im.shape\n        _,cl = interp.data.dl(DatasetType.Valid).dataset[idx]\n        cl = int(cl)\n        ax[2,i].imshow(im)\n        ax[2,i].imshow(im, cmap=plt.cm.gray)\n        ax[2,i].imshow(act, alpha=0.5, extent=(0,H,W,0),\n              interpolation='bilinear', cmap='inferno')\n        ax[2,i].set_xticks([])\n        ax[2,i].set_yticks([])\n        ax[2,i].set_title(f'{classes[interp.pred_class[idx]]} \/ {classes[cl]} \/ {interp.losses[idx]:.2f} \/ {interp.probs[idx][cl]:.2f}')\n    ax[2,0].set_ylabel('Most correct\\nsamples', fontsize=16, rotation=0, labelpad=80)","f592edc2":"plot_heatmap_overview(interp, ['Negative','Positive'])","76c2d7fb":"fast.ai can be enrich with pretrained models, you can find a good list of them here : https:\/\/github.com\/Cadene\/pretrained-models.pytorch\n\nwe tested several of them and final end up using se_resnext101_32x4d and senet154","38d9dda1":"\nGradient-weighted Class Activation Mapping (Grad-CAM)\n\nGrad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\n\nThis method produces a coarse localization map highlighting the areas that the model considers important for the classification decision. The visual explanation gives transparency to the model making it easier to notice if it has learned the wrong things\n","ea57b8ed":"we used stratified kfold and use a blend with median at the end between all results."}}